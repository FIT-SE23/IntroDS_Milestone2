%----------------------------------------------------------------------
%%% SETUP
%----------------------------------------------------------------------
% !TEX root = ./Main.tex


%----------------------------------------------------------------------
%%% NOTATION
%----------------------------------------------------------------------
\subsection{Notation}

In the rest of our paper,
$\pspace$ will denote a $\nCoords$-dimensional real space with norm $\norm{\cdot}$
and
$\points$ will be a closed convex subset thereof.
We will also write
$\dpoints \defeq \dspace$ for the dual of $\pspace$,
$\braket{\dpoint}{\point}$ for the canonical pairing between $\dpoint\in\dpoints$ and $\point\in\pspace$,
and
$\dnorm{\dpoint} \defeq \max \setdef{\braket{\dpoint}{\point}}{\norm{\point}\leq 1}$ for the induced dual norm on $\dpoints$.
In addition, if $\tfun\from\pspace\to\R\cup\{\infty\}$ is an extended-real-valued convex function on $\pspace$, we will write
$\dom\tfun = \setdef{\point\in\pspace}{\tfun(\point) < \infty}$ for its effective domain,
$\subd\tfun(\point) = \setdef{\dpoint\in\dpoints}{\tfun(\pointalt) \geq \tfun(\point) + \braket{\dpoint}{\pointalt - \point} \; \text{for all $\pointalt\in\pspace$}}$ for the subdifferential of $\tfun$ at $\point\in\pspace$,
and
$\dom\subd\tfun = \setdef{\point\in\pspace}{\subd\tfun(\point) \neq \varnothing}$ for the domain of subdifferentiability of $\tfun$.
%Unless explicitly stated otherwise, all the functions we consider will have $\dom\tfun = \points$.
Finally, we will make frequent use of Landau's asymptotic notation, writing in particular
\begin{enumerate*}
[(\itshape i\hspace*{.5pt}\upshape)]
\item
$f(\run) = \bigoh(g(\run))$ and $g(\run)=\Omega(f(\run))$ when $\limsup_{\run\to\infty} f(t)/g(t) < \infty$;
\item
$f(\run) = \Theta(g(\run))$ when $f(\run) = \bigoh(g(\run))$ and $f(\run) = \Omega(g(\run))$;
\item
$f(\run) = o(g(\run))$ when $\limsup_{\run\to\infty} f(t)/g(t) = 0$;
and
\item
$f(\run) \sim g(\run)$ when $\lim_{\run\to\infty} f(\run) / g(\run) = 1$.
\end{enumerate*}


%----------------------------------------------------------------------
%%% STATEMENT
%----------------------------------------------------------------------
\subsection{Problem statement and basic assumptions}

As we mentioned in the introduction, we will focus throughout on solving \aclp{VI} of the form:
\begin{equation}
%\label{eq:VI}
\tag{\ref*{eq:VI}}
\text{Find $\sol\in\points$ such that}
	\;\;
	\braket{\vecfield(\sol)}{\point - \sol}
	\geq 0
	\;\;
	\text{for all $\point\in\points$}
\end{equation}
where $\vecfield\from\points\to\dpoints$ is a single-valued operator, which we call the problem's \emph{defining vector field},
and for which we make the following blanket assumption:
%Throughout the sequel, we will make the following regularity assumption for the \emph{defining vector field} $\vecfield\from\points\to\dpoints$ of \eqref{eq:VI}:

\begin{assumption}
[Lipschitz continuity]
\label{asm:Lipschitz}
$\vecfield$ is \emph{$\lips$-Lipschitz continuous}, \ie
\begin{equation}
\label{eq:Lipschitz}
\tag{LC}
\dnorm{\vecfield(\pointalt) - \vecfield(\point)}
	\leq \lips \norm{\pointalt - \point}
	\quad
	\text{for all $\point,\pointalt\in\points$}.
\end{equation}
\end{assumption}

For concreteness, we provide below two archetypal examples of \ac{VI} problems of this type:


%%% Min problems
%----------------------------------------------------------------------
\begin{example}
[Function minimization]
Consider the minimization problem
\begin{equation}
\label{eq:opt}
\tag{Opt}
\min_{\point\in\points} \obj(\point)
%\begin{aligned}
%\textrm{minimize}
%	&\quad
%	\obj(\point)
%	\\
%\textrm{subject to}
%	&\quad
%	\point\in\points
%\end{aligned}
\end{equation}
with $\obj\from\points\to\R$ assumed smooth.
Then, letting $\vecfield(\point) = \subsel\obj(\point)$, the solutions of \eqref{eq:VI} are precisely the \ac{KKT} points of \eqref{eq:opt} \citep{FP03}.
\endenv
\end{example}


%%% Min-max problems
%----------------------------------------------------------------------
\begin{example}
[Saddle-point problems]
A \emph{saddle-point} \textendash\ or \emph{min-max} \textendash\ problem can be stated in normal form as
\begin{equation}
\label{eq:minmax}
\tag{SP}
\min_{\minvar\in\minvars} \max_{\maxvar\in\maxvars} \, \minmax(\minvar,\maxvar)
\end{equation}
where $\minvars\subseteq\R^{\nCoords_{1}}$ and $\maxvars\subseteq\R^{\nCoords_{2}}$ are convex and closed, and the problem's objective function $\minmax\from\minvars \times \maxvars \to \R$ is again assumed to be smooth.
In the game-theoretic interpretation of the problem,
$\minvar$ is controlled by a player seeking to minimize $\minmax(\cdot,\maxvar)$,
whereas $\maxvar$ is controlled by a player seeking to maximize $\minmax(\minvar,\cdot)$.
Accordingly, solving \eqref{eq:minmax} consists of finding a \acdef{NE}, \ie an action profile $(\minsol,\maxsol) \in \points \defeq \minvars \times \maxvars$ such that
%\begin{equation}
%\label{eq:NE}
%\tag{NE}
\(
\minmax(\minsol,\maxvar)
	\leq \minmax(\minsol,\maxsol)
	\leq \minmax(\minvar,\maxsol)
\)
%	\quad
%	\text{for all $\minvar\in\minvars$, $\maxvar\in\maxvars$}.
%\end{equation}
for all $\minvar\in\minvars$, $\maxvar\in\maxvars$.
If $\minmax$ is not convex-concave, \aclp{NE} may fail to exist, in which case
%\aclp{NE} may not exist if $\minmax$ is not convex-concave \citep{LRS19}.
%In this case,
one typically looks for \acdef{FOS} points of $\minmax$, \ie action profiles $(\minsol,\maxsol) \in \minvars \times \maxvars$ such that $\minsol$ is a \ac{KKT} point of $\minmax(\cdot,\maxsol)$ and, respectively, $\maxsol$ is a \ac{KKT} point of $-\minmax(\minsol,\cdot)$.
In this case, if we set $\point = (\minvar,\maxvar)$
%$\points = \points_{1}\times\points_{2}$
and $\vecfield = (\subsel_{\minvar}\minmax,-\subsel_{\maxvar}\minmax)$, it is straightforward to check that the solutions of \eqref{eq:VI} are precisely the \acl{FOS} points of $\minmax$.
\endenv
\end{example}

The above examples show that not all solutions of \eqref{eq:VI} are desirable:
for example, in the case of \eqref{eq:opt}, such a solution could be a local \emph{maximum} of $\obj$.
For this reason, we will concentrate on solutions $\sol$ of \eqref{eq:VI} that satisfy the following sufficiency condition:

\begin{assumption}
[Second-order sufficiency]
\label{asm:strong}
There exists
a convex neighborhood $\basin$ of $\sol$ in $\points$
and
a positive constant $\strong > 0$
such that
\begin{equation}
\label{eq:strong}
\tag{SOS}
\braket{\vecfield(\point) - \vecfield(\sol)}{\point - \sol}
	\geq \strong \norm{\point - \sol}^{2}
	\quad
	\text{for all $\point\in\basin$}.
\end{equation}
\end{assumption}

In general, \Cref{asm:strong} guarantees that $\sol$ is the unique solution of \eqref{eq:VI} in $\basin$.
%\footnote{Indeed, any other solution $\test\neq\sol$ of \eqref{eq:VI} would satisfy $0 \geq \braket{\vecfield(\test)}{\test - \sol} \geq \strong \norm{\test - \sol}^{2} > 0$, a contradiction.}
In particular, in the setting of \eqref{eq:opt}, \cref{asm:strong} implies that $\obj$ grows (at least) quadratically along every ray emanating from $\sol$, \ie $\obj(\point) - \obj(\sol) \geq \braket{\subsel\obj(\sol)}{\point - \sol} + (\strong/2) \norm{\point - \sol}^{2} = \Omega(\norm{\point-\sol}^{2})$ for all $\point\in\basin$ (though, of course, this does not mean that $\obj$ is strongly convex in $\basin$).
%indeed, for all $\point\in\nhd$, we have
%\begin{align}
%\obj(\point) - \obj(\sol)
%	&= \int_{0}^{1} \braket{\subsel\obj(\sol + \time(\point - \sol))}{\point - \sol} \dd\time
%	\notag\\
%	&\geq \int_{0}^{1}
%		\frac{\braket{\subsel\obj(\sol)}{\sol + \time(\point - \sol) - \sol} + \strong \norm{\sol + \time(\point-\sol) - \sol}^{2}}{\time}
%		\dd\time
%	\notag\\
%	&= \braket{\subsel\obj(\sol)}{\point - \sol}
%		+ \frac{\strong}{2} \norm{\point - \sol}^{2}
%	= \Omega(\norm{\point-\sol}^{2}).
%\end{align}
Analogously, for \eqref{eq:minmax}, \cref{asm:strong} gives $\minmax(\minvar,\maxsol) = \Omega(\norm{\minvar - \minsol}^{2})$ and $\minmax(\minsol,\maxvar) = - \Omega(\norm{\maxvar - \maxsol}^{2})$, so $\sol$ is a local \acl{NE} of $\minmax$.
In view of the above, we will focus throughout on solutions satisfying \eqref{eq:strong}.


%----------------------------------------------------------------------
%%% METHOD
%----------------------------------------------------------------------
\subsection{\acl{BP} methods}
\label{subsec:BP_methods}

The algorithmic framework that we will consider is a general class of \acl{BP} methods that we collectively refer to as the \acli{AMP} template.
The principal ingredient of these methods is that of a \emph{Bregman regularizer} \textendash\ or \acdef{DGF} \textendash\ which we define below as follows:

\begin{definition}
[Bregman regularizers and related notions]
\label{def:Bregman}
%An extended-real-valued function $\hreg\from\pspace\to\R\cup\{\infty\}$
A proper, \acl{lsc}, strictly convex function $\hreg\from\pspace\to\R\cup\{\infty\}$ is said to be a \emph{Bregman regularizer} on $\points$ if
\begin{enumerate}
%\item
%$\hreg$ is proper, \ac{lsc} and convex.

\item
$\hreg$ is supported on $\points$, \ie $\dom\hreg = \points$.
\item
The subdifferential of $\hreg$ admits a \emph{continuous selection}, \ie there exists a continuous mapping $\subsel\hreg\from\dom\subd\hreg\to\dpoints$ such that $\subsel\hreg(\point) \in \subd\hreg(\point)$ for all $\point\in\dom\subd\hreg$.

\item
$\hreg$ is $1$-strongly convex relative to $\norm{\cdot}$, \ie for all $\point\in\dom\subd\hreg, \pointalt\in\dom\hreg$, we have
\begin{equation}
\label{eq:hstr}
\hreg(\pointalt)
	\geq \hreg(\point)
		+ \braket{\subsel\hreg(\point)}{\pointalt - \point}
		+ \tfrac{1}{2} \norm{\pointalt - \point}^{2}.
\end{equation}
\end{enumerate}
\noindent
For posterity, the set $\proxdom \defeq \dom\subd\hreg$ will be referred to as the \emph{prox-domain} of $\hreg$.
%by standard results in convex analysis, $\intpoints \subseteq \proxdom \subseteq \points$.
We also define the \emph{Bregman divergence} of $\hreg$ as
\begin{alignat}{2}
\label{eq:Breg}
\breg(\base,\point)
	&= \hreg(\base)
		- \hreg(\point)
		- \braket{\subsel\hreg(\point)}{\base - \point}
	&\qquad
	&\text{for all $\point\in\proxdom$, $\base\in\points$}
\shortintertext{and the induced \emph{Bregman proximal mapping} as}
\label{eq:prox}
\proxof{\point}{\dvec}
	&= \argmin_{\pointalt\in\points} \{ \braket{\dvec}{\point - \pointalt} + \breg(\pointalt,\point) \}
	&\qquad
	&\text{for all $\point\in\proxdom$, $\dvec\in\dpoints$}.
\end{alignat}
\end{definition}

\begin{example}
A staple choice for $\hreg$ is the Euclidean regularizer $\hreg(\point) = \frac{1}{2}\twonorm{\point}^{2}$.
This choice gives
\begin{equation}
\label{eq:prox-Eucl}
\breg(\base,\point)
	= \tfrac{1}{2} \norm{\base - \point}^{2}
	\quad
	\text{and}
	\quad
\proxof{\point}{\dvec}
	= \Eucl_{\points}(\point + \dvec),
\end{equation}
with $\Eucl_{\points}(\point) \defeq \argmin_{\pointalt\in\points} \norm{\pointalt - \point}$ denoting the Euclidean projector on $\points$.
\endenv
\end{example}

Further examples of Bregman regularizers are given in \cref{sec:examples}, where we also take an in-depth look into their properties.
For now, given a Bregman regularizer on $\points$, we proceed to define the \acdef{AMP} template via the generic recursion
\begin{equation}
\label{eq:AMP}
\tag{AMP}
\begin{aligned}
\lead
	= \proxof{\curr}{-\curr[\step]\curr[\signal]}
%	\\
	\qquad
\next
	= \proxof{\curr}{-\curr[\step]\lead[\signal]}
\end{aligned}
\end{equation}
%In the above,
where
\begin{enumerate}
%[\itshape i\hspace*{1pt}\upshape)]
\item
$\run=\running$ denotes the method's iteration counter.
\item
$\curr[\step] > 0$ is a (non-increasing) step-size sequence.
%and
\item
$\curr[\signal]$ and $\lead[\signal]$ are sequences of ``oracle signals'' whose precise definition we discuss below.
\end{enumerate}
In terms of vocabulary (and for reasons that will also become clear in the sequel), the iterates $\curr$, $\run=\running$, will be referred to as the ``\emph{base states}'' of the method, while the ``half-iterates'' $\lead$, $\run=\running$, will be referred to as the method's ``\emph{leading states}''.
Finally, in terms of initialization, we will take for convenience $\init = \state_{1/2}$.


Now, to link \eqref{eq:AMP} to the problem under study, we will assume throughout that the sequence of oracle signals $\lead[\signal]$ is generated by querying $\vecfield$ at $\lead$, \ie
\begin{equation}
\label{eq:signal-lead}
\lead[\signal]
	= \vecfield(\lead)
	\quad
	\text{for all $\run=\running$}
\end{equation}
In words, \eqref{eq:AMP} generates a new base state $\next$ by taking a Bregman proximal step from $\curr$ with oracle information from the leading state $\lead$.
By contrast, $\lead$ can be generated in a number of different ways, depending on the definition of $\curr[\signal]$.
%Each corresponding to a specific instantiation of \eqref{eq:AMP}.
We present three prototypical examples below:
\medskip

\begin{enumerate}
\addtolength{\itemsep}{\medskipamount}
\item
The \acdef{MP} update:
\begin{alignat}{2}
\label{eq:MP}
\tag{MP}
\curr[\signal]
	&= \vecfield(\curr)
	&\qquad
	&\text{for all $\run=\running$}
\intertext{%
The motivation behind \eqref{eq:MP} is that the algorithm tries to make more informed steps towards a solution of \eqref{eq:VI} by ``anticipating'' the change of $\vecfield$ via a second oracle query.
%This is the basic incarnation and namesake of the ``\acl{MP}'' template;
In the context of the Euclidean regularizer \eqref{eq:prox-Eucl}, the recursion \eqref{eq:MP} is known as the \acdef{EG} algorithm, and was originally proposed by \citet{Kor76};
for the bona fide Bregman version of the algorithm (and namesake of the method), see \citet{Nem04} and \citet{JNT11}.
%
\item
The \acdef{MD} update is defined as}
\label{eq:MD}
\tag{MD}
\curr[\signal]
	&= 0
	&\qquad
	&\text{for all $\run=\running$}
\intertext{%
In this case, the method foregoes any look-ahead efforts and proceeds in a series of Bregman proximal steps $\next \gets \proxof{\curr}{-\curr[\step]\vecfield(\curr)}$.
This method has a long history dating back to \citet{NY83};
for an appetizer, we refer the reader to \cite{BecTeb03,NJLS09,Teb18,ZMBB+20} and references therein.
%
\item
The \acdef{OMD} update:}
\label{eq:OMD}
\tag{OMD}
\curr[\signal]
	&= \vecfield(\beforelead)
	&\qquad
	&\text{for all $\run=\running$}
\end{alignat}
The idea of this update is to lighten the per-iteration complexity of \eqref{eq:MP} by making only a \emph{single} query to $\vecfield$:
this is done by approximating $\vecfield(\lead)$ with the previously available oracle signal, \ie taking $\lead[\signal] \gets \vecfield(\beforelead)$.
This ``oracle reuse'' idea dates back to \citet{Pop80}, and it has been subsequently popularized in machine learning and other fields by \cite{CYLM+12}, \cite{RS13-NIPS}, and many others;
for an overview, see \cite{HIMM19} and references therein.
\end{enumerate}

The three algorithms described above are the most widely studied Bregman methods in the literature, so we will use them as running examples throughout.
More generally, we will make the following assumption for the input signal $\curr[\signal]$ generated at the method's base state.

\begin{assumption}
\label{asm:signal-base}
For all $\run=\running$, the oracle signal $\curr[\signal]$ is of the form:
\begin{equation}
\label{eq:signal-base}
\curr[\signal]
	= \coef[a] \vecfield(\curr)
		+ \coef[b] \vecfield(\beforelead)
\end{equation}
for some $\coef[a],\coef[b] \in [0,1]$ with
$\coef[a] + \coef[b] \leq 1$
and
$\coef[a] + \coef[b] = 1$ if $\coef[b]>0$.
\end{assumption}

In the language of \cref{asm:signal-base}, the three examples of \eqref{eq:AMP} above can be recovered as follows:
\begin{itemize}
\item
For \eqref{eq:MP}:
	\tabto{6em}
	$\coef[a] = 1$, $\coef[b] = 0$.
\item
For \eqref{eq:MD}:
	\tabto{6em}
	$\coef[a] = 0$, $\coef[b] = 0$.
\item
For \eqref{eq:OMD}:
	\tabto{6em}
	$\coef[a] = 0$, $\coef[b] = 1$.
\end{itemize}
More general input sequences can also be considered \textendash\ \eg to cover for sequential or ``$k$-to-$1$'' update rules that are sometimes used in min-max problems \citep{HMC21} \textendash\ but this would complicate the notation and the resulting rates, so \cref{asm:signal-base} will suffice for our purposes.
We also note in passing that the requirement ``$\coef[a] + \coef[b] = 1$ if $\coef[b] > 0$'' is only introduced to ease notation and does not lead to a loss in generality:
if $\coef[b] > 0$, we can always rescale the method's step-size by $\coef[a] + \coef[b]$ so that the condition $\coef[a] + \coef[b] = 1$ is satisfied automatically.

For future use, we close this section with some basic properties of the Bregman divergence and the induced proximal mapping:
%\textendash\ not the least of which is the fact that the update rule \eqref{eq:AMP} is well-posed:

\begin{restatable}{lemma}{mirror}
\label{lem:mirror}
Let $\hreg$ be a Bregman regularizer on $\points$ and let $\subsel\hreg$ be a continuous selection of $\subd\hreg$.
Then, for all $\point\in\proxdom$, $\new\in\points$ and $\dvec\in\dpoints$, we have:
\begin{subequations}
\begin{flalign}
\quad
	a)\;\;
	&\subd\hreg(\point)
	= \subsel\hreg(\point)
		+ \pcone(\point)
	&&
	\\
\quad
	b)\;\;
	&\new
	= \proxof{\point}{\dvec}
	\iff
\subsel\hreg(\point) + \dvec
	\in \subd\hreg(\new)
	\iff
\subsel\hreg(\new) - \subsel\hreg(\point)
	\in \dvec - \pcone(\new)
	&&
\end{flalign}
\end{subequations}
where $\pcone(\point) = \setdef{\dvec\in\dpoints}{\braket{\dvec}{\pointalt - \point} \leq 0 \; \text{for all $\pointalt\in\points$}}$ denotes the polar cone to $\points$ at $\point$.
In particular, \eqref{eq:AMP} is well-posed:
$\new = \proxof{\point}{\dvec}$ implies that $\new\in\proxdom$.
\end{restatable}

\begin{restatable}[$3$-point identity]{lemma}{threepoint}
\label{lem:threepoint}
%Let $\hreg$ be a Bregman regularizer on $\points$.
For all $\base\in\points$ and all $\point,\new\in\proxdom$, we have:
\begin{equation}
\label{eq:threepoint}
\breg(\base,\new)
	= \breg(\base,\point)
		+ \breg(\point,\new)
		+ \braket{\subsel\hreg(\new) - \subsel\hreg(\point)}{\point - \base}
\end{equation}
\end{restatable}

\begin{restatable}[Non-expansiveness]{lemma}{proxlip}
\label{lem:proxlip}
For all $\point\in\proxdom$ and all $\dvec,\new[\dvec]\in\dpoints$ we have:
\begin{equation}
	\norm{\proxof{\point}{\new[\dvec]} - \proxof{\point}{\dvec}}
	\leq \dnorm{\new[\dvec] - \dvec}
\end{equation}
\end{restatable}

These properties are fairly well known, so we omit their proofs;
for a detailed treatment, we defer the interested reader to \citet{BecTeb03,JNT11}, and \citet{MZ19}.
