%----------------------------------------------------------------------
%% SETUP
%----------------------------------------------------------------------
% !TEX root = ../Main.tex


%----------------------------------------------------------------------
%%% NOTATION
%----------------------------------------------------------------------
%\subsection{Notation}
%In the rest of our paper,
%$\pspace$ will denote a $\nCoords$-dimensional real space with norm $\norm{\cdot}$
%and
%$\points$ will be a closed convex subset thereof.
%We will also write
%$\dpoints \defeq \dspace$ for the dual of $\pspace$,
%$\braket{\dpoint}{\point}$ for the canonical pairing between $\dpoint\in\dpoints$ and $\point\in\pspace$,
%and
%$\dnorm{\dpoint} \defeq \max \setdef{\braket{\dpoint}{\point}}{\norm{\point}\leq 1}$ for the induced dual norm on $\dpoints$.
%In addition, if $\tfun\from\pspace\to\R\cup\{\infty\}$ is an extended-real-valued convex function on $\pspace$, we will write
%$\dom\tfun = \setdef{\point\in\pspace}{\tfun(\point) < \infty}$ for its effective domain,
%$\subd\tfun(\point) = \setdef{\dpoint\in\dpoints}{\tfun(\pointalt) \geq \tfun(\point) + \braket{\dpoint}{\pointalt - \point} \; \text{for all $\pointalt\in\pspace$}}$ for the subdifferential of $\tfun$ at $\point\in\pspace$,
%and
%$\dom\subd\tfun = \setdef{\point\in\pspace}{\subd\tfun(\point) \neq \varnothing}$ for the domain of subdifferentiability of $\tfun$.
%Unless explicitly stated otherwise, all the functions we consider will have $\dom\tfun = \points$.
%Finally, we will make frequent use of Landau's asymptotic notation, writing in particular
%\begin{enumerate*}
%[(\itshape i\hspace*{.5pt}\upshape)]
%\item
%$f(\run) = \bigoh(g(\run))$ and $g(\run)=\Omega(f(\run))$ when $\limsup_{\run\to\infty} f(t)/g(t) < \infty$;
%\item
%$f(\run) = \Theta(g(\run))$ when $f(\run) = \bigoh(g(\run))$ and $f(\run) = \Omega(g(\run))$;
%\item
%$f(\run) = o(g(\run))$ when $\limsup_{\run\to\infty} f(t)/g(t) = 0$;
%and
%\item
%$f(\run) \sim g(\run)$ when $\lim_{\run\to\infty} f(\run) / g(\run) = 1$.
%\end{enumerate*}


To fix notation and terminology, in the rest of our paper
$\pspace$ will denote an $\nCoords$-dimensional real space with norm $\norm{\cdot}$
and
$\points$ will be a closed convex subset thereof.
We will also write
$\dpoints \defeq \dspace$ for the dual of $\pspace$,
$\braket{\dpoint}{\point}$ for the canonical pairing between $\dpoint\in\dpoints$ and $\point\in\pspace$,
and
$\dnorm{\dpoint} \defeq \max \setdef{\braket{\dpoint}{\point}}{\norm{\point}\leq 1}$ for the induced dual norm on $\dpoints$.


%----------------------------------------------------------------------
%% ASSUMPTIONS
%----------------------------------------------------------------------
\subsection{Blanket assumptions}

Throughout the sequel, we will make the following assumptions for the defining vector field $\vecfield\from\points\to\dpoints$ of \eqref{eq:VI} and the solution $\sol\in\points$ under study:

\begin{assumption}
[Lipschitz continuity]
\label{asm:Lipschitz}
The vector field $\vecfield$ is \emph{$\lips$-Lipschitz continuous}, \ie
\begin{equation}
\label{eq:Lipschitz}
\tag{LC}
\dnorm{\vecfield(\pointalt) - \vecfield(\point)}
	\leq \lips \norm{\pointalt - \point}
	\quad
	\text{for all $\point,\pointalt\in\points$}.
\end{equation}
\end{assumption}

\begin{assumption}
[Second-order sufficiency]
\label{asm:strong}
There exists
a convex neighborhood $\basin$ of $\sol$ in $\points$
and
a positive constant $\strong > 0$
such that
\begin{equation}
\label{eq:strong}
\tag{SOS}
\braket{\vecfield(\point) - \vecfield(\sol)}{\point - \sol}
	\geq \strong \norm{\point - \sol}^{2}
	\quad
	\text{for all $\point\in\basin$}.
\end{equation}
\end{assumption}

In general, \Cref{asm:strong} guarantees that $\sol$ is the unique solution of \eqref{eq:VI} in $\basin$;
we illustrate this in two special cases of interest:
%\footnote{Indeed, any other solution $\test\neq\sol$ of \eqref{eq:VI} would satisfy $0 \geq \braket{\vecfield(\test)}{\test - \sol} \geq \strong \norm{\test - \sol}^{2} > 0$, a contradiction.}
\PM{$\obj$ and $\minmax$ were not defined (they were orphaned after the min / min-max examples were removed), so I expanded this a bit.
Also, fixed an error:
it is the \emph{differences} of the value function that grow quadratically, not the value function per se (the various expressions were missing $\minmax(\minsol,\maxsol)$).}
\begin{enumerate}
\item
\emph{Minimization problems:}
suppose that $\vecfield = \nabla\obj$ for some Lipschitz smooth objective function $\obj$ on $\points$.
Then, \cref{asm:strong} implies that $\obj$ grows (at least) quadratically along every ray emanating from $\sol$, \ie $\obj(\point) - \obj(\sol) \geq \braket{\subsel\obj(\sol)}{\point - \sol} + (\strong/2) \norm{\point - \sol}^{2} = \Omega(\norm{\point-\sol}^{2})$ for all $\point\in\basin$, \ie $\sol$ is an isolated local minimizer of $\obj$.
\item
\emph{Min-max problems:}
%in obvious notation,
suppose that $\points$ factorizes as $\points = \minvars\times\maxvars$ for suitable factor sets $\minvars$, and $\maxvars$,
let $\minmax\from\points\to\R$ be a smooth function on $\points$,
and
write $\vecfield = (\nabla_{\minvar}\minmax,-\nabla_{\maxvar}\minmax)$ for the min-max gradient of $\minmax$ (with respect to $\minvar\in\minvars$ and $\maxvar\in\maxvars$ respectively).
Then, any solution $\sol = (\minsol,\maxsol)$ of \eqref{eq:VI} that satisfies \cref{asm:strong} enjoys the local growth bounds $\minmax(\minvar,\maxsol) - \minmax(\minsol,\maxsol) = \Omega(\norm{\minvar - \minsol}^{2})$ and $\minmax(\minsol,\maxsol) - \minmax(\minsol,\maxvar) = \Omega(\norm{\maxvar - \maxsol}^{2})$, implying in turn that $\sol$ is an isolated, hyperbolic saddle-point of $\minmax$.
\end{enumerate}
More examples satisfying \eqref{eq:strong} include strict \aclp{NE} in finite games \cite{FT91}, deterministic Nash policies in (generic) stochastic games \cite{SV15}, etc.
Overall, \cref{asm:Lipschitz,asm:strong} apply to a very wide range of problems, so we will treat them as blanket assumptions throughout.


%in the case of minimization problems ($\vecfield = \nabla\obj$ for some Lipschitz smooth function $\), \cref{asm:strong} implies that the objective function $\obj$ grows (at least) quadratically along every ray emanating from $\sol$, \ie $\obj(\point) - \obj(\sol) \geq \braket{\subsel\obj(\sol)}{\point - \sol} + (\strong/2) \norm{\point - \sol}^{2} = \Omega(\norm{\point-\sol}^{2})$ for all $\point\in\basin$.
%%(though, of course, this does not mean that $\obj$ is strongly convex in $\basin$).
%%indeed, for all $\point\in\nhd$, we have
%%\begin{align}
%%\obj(\point) - \obj(\sol)
%%	&= \int_{0}^{1} \braket{\subsel\obj(\sol + \time(\point - \sol))}{\point - \sol} \dd\time
%%	\notag\\
%%	&\geq \int_{0}^{1}
%%		\frac{\braket{\subsel\obj(\sol)}{\sol + \time(\point - \sol) - \sol} + \strong \norm{\sol + \time(\point-\sol) - \sol}^{2}}{\time}
%%		\dd\time
%%	\notag\\
%%	&= \braket{\subsel\obj(\sol)}{\point - \sol}
%%		+ \frac{\strong}{2} \norm{\point - \sol}^{2}
%%	= \Omega(\norm{\point-\sol}^{2}).
%%\end{align}
%Analogously, for saddle-point problems, \cref{asm:strong} implies that the min-max objective function $\minmax\from\minvars \times \maxvars \to \R$ satisfies $\minmax(\minvar,\maxsol) = \Omega(\norm{\minvar - \minsol}^{2})$ and $\minmax(\minsol,\maxvar) = - \Omega(\norm{\maxvar - \maxsol}^{2})$, so $\sol$ is a local \acl{NE} of $\minmax$.
%In view of the above, we %will 
%focus throughout on solutions satisfying \eqref{eq:strong}.

%%% Min problems
%%----------------------------------------------------------------------
%\begin{example}
%[Function minimization]
%Consider the minimization problem
%\begin{equation}
%\label{eq:opt}
%\tag{Opt}
%\min_{\point\in\points} \obj(\point)
%%\begin{aligned}
%%\textrm{minimize}
%%	&\quad
%%	\obj(\point)
%%	\\
%%\textrm{subject to}
%%	&\quad
%%	\point\in\points
%%\end{aligned}
%\end{equation}
%with $\obj\from\points\to\R$ assumed smooth.
%Then, letting $\vecfield(\point) = \subsel\obj(\point)$, the solutions of \eqref{eq:VI} are precisely the \ac{KKT} points of \eqref{eq:opt} \citep{FP03}.
%\endenv
%\end{example}
%
%
%%% Min-max problems
%%----------------------------------------------------------------------
%\begin{example}
%[Saddle-point problems]
%A \emph{saddle-point} \textendash\ or \emph{min-max} \textendash\ problem can be stated in normal form as
%\begin{equation}
%\label{eq:minmax}
%\tag{SP}
%\min_{\minvar\in\minvars} \max_{\maxvar\in\maxvars} \, \minmax(\minvar,\maxvar)
%\end{equation}
%where $\minvars\subseteq\R^{\nCoords_{1}}$ and $\maxvars\subseteq\R^{\nCoords_{2}}$ are convex and closed, and the problem's objective function $\minmax\from\minvars \times \maxvars \to \R$ is again assumed to be smooth.
%In the game-theoretic interpretation of the problem,
%$\minvar$ is controlled by a player seeking to minimize $\minmax(\cdot,\maxvar)$,
%whereas $\maxvar$ is controlled by a player seeking to maximize $\minmax(\minvar,\cdot)$.
%Accordingly, solving \eqref{eq:minmax} consists of finding a \acdef{NE}, \ie an action profile $(\minsol,\maxsol) \in \points \defeq \minvars \times \maxvars$ such that
%%\begin{equation}
%%\label{eq:NE}
%%\tag{NE}
%\(
%\minmax(\minsol,\maxvar)
%	\leq \minmax(\minsol,\maxsol)
%	\leq \minmax(\minvar,\maxsol)
%\)
%%	\quad
%%	\text{for all $\minvar\in\minvars$, $\maxvar\in\maxvars$}.
%%\end{equation}
%for all $\minvar\in\minvars$, $\maxvar\in\maxvars$.
%If $\minmax$ is not convex-concave, \aclp{NE} may fail to exist, in which case
%%\aclp{NE} may not exist if $\minmax$ is not convex-concave \citep{LRS19}.
%%In this case,
%one typically looks for \acdef{FOS} points of $\minmax$, \ie action profiles $(\minsol,\maxsol) \in \minvars \times \maxvars$ such that $\minsol$ is a \ac{KKT} point of $\minmax(\cdot,\maxsol)$ and, respectively, $\maxsol$ is a \ac{KKT} point of $-\minmax(\minsol,\cdot)$.
%In this case, if we set $\point = (\minvar,\maxvar)$
%%$\points = \points_{1}\times\points_{2}$
%and $\vecfield = (\subsel_{\minvar}\minmax,-\subsel_{\maxvar}\minmax)$, it is straightforward to check that the solutions of \eqref{eq:VI} are precisely the \acl{FOS} points of $\minmax$.
%\endenv
%\end{example}



%----------------------------------------------------------------------
%% METHOD
%----------------------------------------------------------------------
\subsection{\acl{BP} methods}
\label{subsec:BP_methods}

As we discussed in the introduction, the main algorithmic template that we will examine for solving \eqref{eq:VI} is a general class of first-order algorithms known as \acdefp{BPM}.
The defining ingredient of this class is the notion of \emph{Bregman regularizer}, which is defined below as follows:

\begin{definition}
[Bregman regularizers and related notions]
\label{def:Bregman}
%An extended-real-valued function $\hreg\from\pspace\to\R\cup\{\infty\}$
A proper, \acl{lsc}, strictly convex function $\hreg\from\pspace\to\R\cup\{\infty\}$ is %said to be 
a \emph{Bregman regularizer} on $\points$ if
\begin{enumerate}
%\item
%$\hreg$ is proper, \ac{lsc} and convex.

\item
$\hreg$ is supported on $\points$, \ie $\dom\hreg = \points$.
\item
The subdifferential of $\hreg$ admits a \emph{continuous selection}, \ie there exists a continuous mapping $\subsel\hreg\from\dom\subd\hreg\to\dpoints$ such that $\subsel\hreg(\point) \in \subd\hreg(\point)$ for all $\point\in\dom\subd\hreg$.

\item
$\hreg$ is $1$-strongly convex relative to $\norm{\cdot}$, \ie for all $\point\in\dom\subd\hreg, \pointalt\in\dom\hreg$, we have
\begin{equation}
\label{eq:hstr}
\hreg(\pointalt)
	\geq \hreg(\point)
		+ \braket{\subsel\hreg(\point)}{\pointalt - \point}
		+ \tfrac{1}{2} \norm{\pointalt - \point}^{2}.
\end{equation}
\end{enumerate}
\noindent
The set $\proxdom \defeq \dom\subd\hreg$ will be referred to as the \emph{prox-domain} of $\hreg$.
In addition, we also define the \emph{Bregman divergence} of $\hreg$ as
\begin{alignat}{2}
\label{eq:Breg}
\breg(\base,\point)
	&= \hreg(\base)
		- \hreg(\point)
		- \braket{\subsel\hreg(\point)}{\base - \point}
	&\qquad
	&\text{for all $\point\in\proxdom$, $\base\in\points$}
\intertext{and the induced \emph{prox-mapping} as}
\label{eq:prox}
\proxof{\point}{\dvec}
	&= \argmin\nolimits_{\pointalt\in\points} \{ \braket{\dvec}{\point - \pointalt} + \breg(\pointalt,\point) \}
	&\qquad
	&\text{for all $\point\in\proxdom$, $\dvec\in\dpoints$}.
\end{alignat}
\end{definition}


%\begin{example}
%A staple choice for $\hreg$ is the Euclidean regularizer $\hreg(\point) = \frac{1}{2}\twonorm{\point}^{2}$.
%This choice gives
%\begin{equation}
%\label{eq:prox-Eucl}
%\breg(\base,\point)
%	= \tfrac{1}{2} \norm{\base - \point}^{2}
%	\quad
%	\text{and}
%	\quad
%\proxof{\point}{\dvec}
%	= \Eucl_{\points}(\point + \dvec),
%\end{equation}
%with $\Eucl_{\points}(\point) \defeq \argmin_{\pointalt\in\points} \norm{\pointalt - \point}$ denoting the Euclidean projector on $\points$.
%\endenv
%\end{example}

Examples of Bregman regularizers are given in \cref{sec:examples}, where we also take an in-depth look at their properties.
For now, given a Bregman regularizer on $\points$, the general class of \acdefp{BPM} that we will consider is defined via the generic recursion
\begin{equation}
\label{eq:BPM}
\tag{BPM}
\begin{aligned}
\lead
	= \proxof{\curr}{-\curr[\step]\curr[\signal]}
%	\\
	\qquad
\next
	= \proxof{\curr}{-\curr[\step]\lead[\signal]}
\end{aligned}
\end{equation}
%In the above,
where
\begin{enumerate*}
[(\itshape i\hspace*{1pt}\upshape)]
\item
$\run=\running$ denotes the method's iteration counter;
\item
$\curr[\step] > 0$ is a (non-increasing) step-size sequence;
%and
\item
$\curr[\signal]$ and $\lead[\signal]$ are sequences of ``oracle signals'' that we discuss in detail below.
\end{enumerate*}
In terms of vocabulary, %(and for reasons that will also become clear in the sequel), 
the iterates $\curr$, $\run=\running$, will be referred to as the ``\emph{base states}'' of the method, while the ``half-iterates'' $\lead$, $\run=\running$, will be referred to as the method's ``\emph{leading states}''.
Finally, in terms of initialization, we will take for convenience $\init = \state_{1/2}$.

Now, regarding the sequence of oracle signals $\curr[\signal]$ and $\lead[\signal]$ defining \eqref{eq:BPM}, we will assume throughout that
\begin{equation}
\label{eq:signal-lead}
\lead[\signal]
	= \vecfield(\lead)
	\qquad
	\text{for all $\run=\running$}
\end{equation}
\ie \eqref{eq:BPM} generates a new base state $\next$ by taking a Bregman proximal step from $\curr$ with oracle input from the leading state $\lead$.
By contrast, the leading state itself can be generated in a number of different ways from $\curr$, depending on the definition of $\curr[\signal]$:
%We %will 
%make the following assumption for the input signal $\curr[\signal]$ generated at the method's base state.

\begin{assumption}
\label{asm:signal-base}
For all $\run=\running$, the oracle signal $\curr[\signal]$ is of the form:
\begin{equation}
\label{eq:signal-base}
\curr[\signal]
	= \coef[a] \vecfield(\curr)
		+ \coef[b] \vecfield(\beforelead)
\end{equation}
for some $\coef[a],\coef[b] \in [0,1]$ with
$\coef[a] + \coef[b] \leq 1$
and
$\coef[a] + \coef[b] = 1$ if $\coef[b]>0$.\footnote{Note that the requirement ``$\coef[a] + \coef[b] = 1$ if $\coef[b] > 0$'' is only intended to ease notation and does not lead to a loss in generality:
if $\coef[b] > 0$, we can always rescale $\curr[\step]$ by $\coef[a] + \coef[b]$ so the condition $\coef[a] + \coef[b] = 1$ is satisfied automatically.}
\end{assumption}

For concreteness, we illustrate below three archetypal Bregman methods that serve as the backbone of the above framework:

\begin{enumerate}
\item
\emph{\Acl{MD}:}
following \cite{NY83,BecTeb03,NJLS09}, the \acl{MD} algorithm proceeds recursively as $\new = \proxof{\point}{-\step\vecfield(\point)}$, so it can be recovered from \eqref{eq:BPM} by taking
\begin{alignat}{3}
\label{eq:MD}
\tag{MD}
\coef[a] = 0,\,
	\coef[b] = 0
	&\quad
	\text{or, equivalently}
	&\quad
\curr[\signal]
	&= 0
	&\quad
	\text{for all $\run=\running$}
%\intertext{%
%In this case, the method foregoes any look-ahead efforts and proceeds in a series of Bregman proximal steps $\next \gets \proxof{\curr}{-\curr[\step]\vecfield(\curr)}$.
%This method has a long history dating back to \citet{NY83};
%for an appetizer, we refer the reader to \cite{BecTeb03,NJLS09,Teb18,ZMBB+20} and references therein.
%
\intertext{%
\item
\emph{\Acl{MP}:}
following \cite{Nem04,JNT11}, the \acl{MP} algorithm corresponds to the choice}
\label{eq:MP}
\tag{MP}
\coef[a] = 1,\,
	\coef[b] = 0
	&\quad
	\text{or, equivalently}
	&\quad
\curr[\signal]
	&= \vecfield(\curr)
	&\quad
	\text{for all $\run=\running$}
%\intertext{%
%The motivation behind \eqref{eq:MP} is that the algorithm tries to make more informed steps towards a solution of \eqref{eq:VI} by ``anticipating'' the change of $\vecfield$ via a second oracle query.
%%This is the basic incarnation and namesake of the ``\acl{MP}'' template;
%In the context of the Euclidean regularizer, %\eqref{eq:prox-Eucl}, 
%the recursion \eqref{eq:MP} is known as the \acdef{EG} algorithm, and was originally proposed by \citet{Kor76};
%for the bona fide Bregman version of the algorithm (and namesake of the method), see \citet{Nem04} and \citet{JNT11}.
%
\intertext{%
\item
\emph{\Acl{OMD}:}
originally due to \cite{Pop80} (in the Euclidean case) and \cite{CYLM+12,RS13-NIPS} (for the general case), the \acl{OMD} algorithm is obtained by setting}
\label{eq:OMD}
\tag{OMD}
\coef[a] = 0,\,
	\coef[b] = 1
	&\quad
	\text{or, equivalently}
	&\quad
\curr[\signal]
	&= \vecfield(\beforelead)
	&\quad
	\text{for all $\run=\running$}
\end{alignat}
%The idea of this update is to lighten the per-iteration complexity of \eqref{eq:MP} by making only a \emph{single} query to $\vecfield$:
%this is done by approximating $\vecfield(\lead)$ with the previously available oracle signal, \ie taking $\lead[\signal] \gets \vecfield(\beforelead)$.
%This ``oracle reuse'' idea dates back to \citet{Pop80}, and it has been subsequently popularized in machine learning and other fields by \cite{CYLM+12}, \cite{RS13-NIPS}, and many others;
%for an overview, see \cite{HIMM19} and references therein.
\end{enumerate}
These three algorithms are the most widely studied Bregman methods in the literature, so we will use them as running examples throughout.

%More general input sequences can also be considered \textendash\ \eg to cover for sequential or ``$k$-to-$1$'' update rules that are sometimes used in min-max problems \citep{HMC21} \textendash\ but this would complicate the notation and the resulting rates, so \cref{asm:signal-base} will suffice for our purposes.
%We also note in passing that the requirement ``$\coef[a] + \coef[b] = 1$ if $\coef[b] > 0$'' is only introduced to ease notation and does not lead to a loss in generality:
%if $\coef[b] > 0$, we can always rescale the method's step-size by $\coef[a] + \coef[b]$ so that the condition $\coef[a] + \coef[b] = 1$ is satisfied automatically.

%For future use, we close this section with some basic properties of the Bregman divergence and the induced proximal mapping:
%%\textendash\ not the least of which is the fact that the update rule \eqref{eq:BPM} is well-posed:

%\begin{restatable}{lemma}{mirror}
%\label{lem:mirror}
%Let $\hreg$ be a Bregman regularizer on $\points$ and let $\subsel\hreg$ be a continuous selection of $\subd\hreg$.
%Then, for all $\point\in\proxdom$, $\new\in\points$ and $\dvec\in\dpoints$, we have:
%\begin{subequations}
%\begin{flalign}
%\quad
%	a)\;\;
%	&\subd\hreg(\point)
%	= \subsel\hreg(\point)
%		+ \pcone(\point)
%	&&
%	\\
%\quad
%	b)\;\;
%	&\new
%	= \proxof{\point}{\dvec}
%	\iff
%\subsel\hreg(\point) + \dvec
%	\in \subd\hreg(\new)
%	\iff
%\subsel\hreg(\new) - \subsel\hreg(\point)
%	\in \dvec - \pcone(\new)
%	&&
%\end{flalign}
%\end{subequations}
%where $\pcone(\point) = \setdef{\dvec\in\dpoints}{\braket{\dvec}{\pointalt - \point} \leq 0 \; \text{for all $\pointalt\in\points$}}$ denotes the polar cone to $\points$ at $\point$.
%In particular, \eqref{eq:BPM} is well-posed:
%$\new = \proxof{\point}{\dvec}$ implies that $\new\in\proxdom$.
%\end{restatable}

%\begin{restatable}[$3$-point identity]{lemma}{threepoint}
%\label{lem:threepoint}
%%Let $\hreg$ be a Bregman regularizer on $\points$.
%For all $\base\in\points$ and all $\point,\new\in\proxdom$, we have:
%\begin{equation}
%\label{eq:threepoint}
%\breg(\base,\new)
%	= \breg(\base,\point)
%		+ \breg(\point,\new)
%		+ \braket{\subsel\hreg(\new) - \subsel\hreg(\point)}{\point - \base}
%\end{equation}
%\end{restatable}

%\begin{restatable}[Non-expansiveness]{lemma}{proxlip}
%\label{lem:proxlip}
%For all $\point\in\proxdom$ and all $\dvec,\new[\dvec]\in\dpoints$ we have:
%\begin{equation}
%	\norm{\proxof{\point}{\new[\dvec]} - \proxof{\point}{\dvec}}
%	\leq \dnorm{\new[\dvec] - \dvec}
%\end{equation}
%\end{restatable}

%These properties are fairly well known, so we omit their proofs;
%for a detailed treatment, we defer the interested reader to \citet{BecTeb03,JNT11}, and \citet{MZ19}.