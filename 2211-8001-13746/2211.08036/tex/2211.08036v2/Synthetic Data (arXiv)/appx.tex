\section{``Indistinguishable'' distributions}
\label{appx:indistinguishable}

      We adopt the definition of \emph{total variation distance} used in \cite{Romano2005}:
      \begin{definition}(Total Variation Distance (TV))
      \label{def:tvd}
          \eq{
              \TV(P_0, P_1) &= \half\norm{P_1-P_0}_1 = \half\int |p_1-p_0|d\mu
          }
          where $p_i$ is the density of $P_i$ with respect to any measure $\mu$ dominating both $P_0$ and $P_1$, i.e. $p_i =
          \frac{dP_i}{d\mu}$.
      \end{definition}
      Using this definition, \(\TV(P_0,P_1)\in [0,1]\) for any measures \(P_0,P_1\). Note that we use notation such that \(\TV(P_0,P_1)= \TV(p_0,p_1)\).

    The two proofs below draw heavily from the proof of Theorem 13.1.1 from \cite{Romano2005}:

\begin{proof}[Proof that the decision process of section \ref{sec:indistinguishable} achieves the minimum possible error rate]
\label{proof:min_err_rate}
% If we consider the situation outlined in \ref{sec:indistinguishable} with exact-GP \(P_0\), approximate-GP \(P_1\) with prior probabilities \(\Pr(P_0)=\Pr(P_1)=\half\) and a sample \(y\) from one of them, then we can define our Bayesian decision rule as choosing \(P_0\) when \(\Pr(P_0\given y) \geq \Pr(P_1\given y)\). In this setup this is equivalent to \(\Pr(y\given P_0)\geq \Pr(y\given P_1)\) (and \(p_0(y)\geq p_1(y)\)). 
% We can associate with this a decision function \(\phi : \mathbb{R}^n\rightarrow \{0,1\}\).
% \[
%     \phi(x) = \begin{cases}
%         1 \, \mbox{if } p_0(x) < p_1(x) \\
%         0 \, \mbox{if } p_0(x) \geq p_1(x)
%     \end{cases}
% \]

    A general decision process for our problem is represented by a function \(\phi : \mathbb{R}^n\rightarrow \{0,1\}\) whereby, if we are presented with a sample vector \(y\ \in \mathbb{R}^n\), we select model \(P_{\phi(y)}\). Under this decision process
    \(\Pr(\mbox{error}) = \Pr(\mbox{select }P_1\given P_0)\Pr(P_0) + \Pr(\mbox{select }P_0\given P_1)\Pr(P_1)\). With an assumed uniform prior on models, we have
    \eq{
        2\Pr(\mbox{error}) &= \Pr(\mbox{select }P_1\given P_0) + \Pr(\mbox{select }P_0\given P_1) \\ 
        &= \int\phi(x)p_0d\mu(x) + \int(1-\phi(x))p_1d\mu(x) \\
        &= 1 + \int \phi(x)(p_0(x)-p_1(x))d\mu(x) \\
        &= 1 + \int_{R_+}\phi(x)f(x)d\mu(x) + \int_{R_-}\phi(x)f(x)d\mu(x) + \int_{R_0}\phi(x)f(x)d\mu(x)
    }
    where \(f(x)=p_0(x)-p_1(x)\), \(R_+=\{x \in \mathbb{R}^n : f(x) > 0\}\), \(R_-=\{x \in \mathbb{R}^n : f(x) < 0\}\) and \(R_0=\{x \in \mathbb{R}^n : f(x)=0\}\). From this it follows that setting \(\phi^\star\) to be 0 on \(R_+\), 1 on \(R_-\) and either 1 or 0 on \(R_0\) minimises the probability of error in the decision process. This choice of \(\phi\) precisely agrees with the decision process described in \cite{Romano2005}.

\end{proof}

\begin{proof}[Proof of \ref{lem:indistinguishable}]
    From the above proof we see that the probability of error achieved by the optimal decision process is \(p'=\half\left[1+\int_{R_-}(p_0(x)-p_1(x))d\mu(x)\right]\). If we interchange the roles of \(p_0\) and \(p_1\) we obtain the alternative representation \(p''=\half\left[1 + \int_{R_+}(p_1(x)-p_0(x))d\mu(x)\right]\). Since the probability of error, \(p\), satisfies \(p=p'=p''\) we have \(p=\half[p'+p'']\) which can be rearranged to give
    \eq{
    p &= \half\left[1 - \half\int\abs{p_1(x)-p_0(x)}d\mu(x)\right] = \half - \frac{1}{2}\TV(P_0,P_1).
    }
    Hence if we can set \(\TV(P_0,P_1)\leq 2\eps\) we obtain a maximin error rate of at least \(\half-\eps\) as claimed.
\end{proof}

\begin{remark}
        Note that we can consider the \emph{worst} decision rule, \(\phi^\dagger\), to do the exact opposite of \(\phi^\star\) and obtain that for any \(\phi\): \(\half - \frac{1}{2}\tau \leq p \leq \half + \frac{1}{2}\tau\) where \(\tau = \TV(P_0,P_1)\). \(p=\half\) only when \(\tau=0\). Additionally, a best-case rate of 0 (under \(\phi^\star\)) is achieved when \(P_0\) and \(P_1\) are maximally different in the sense that \(\tau=1\). In this regime we correspondingly obtain the worst-case rate (under \(\phi^\dagger\)) of 1.
\end{remark}

%%%%%%%%%%%%%%%%%%%%%%%

% Defining the sum of the errors by \(S\) and decomposing our approximate distribution into a perturbed version of the true, i.e. \(P_1=(1-\varepsilon)P_0 + \varepsilon Q\) we can see that
% \eq{
% S &= \int_{\mathbb{1}(p_0<p_1)}dP_0(x) + \int_{\mathbb{1}(p_1\leq p_0)}dP_1(x) \\
% &= \int_{\mathbb{1}(p_0<p_1)}p_0(x)d\mu(x) + \int_{\mathbb{1}(p_1\leq p_0)}((1-\varepsilon)p_0(x) + \varepsilon q(x))d\mu(x) \\
% &= 1 + \varepsilon\int_{\mathbb{1}(p_1\leq p_0)}(q(x) - p_0(x))d\mu(x) \\
% &= 1 - \frac{\varepsilon}{2}\int\abs{q(x)-p_0(x)}d\mu(x)
% }
% Clearly then \(1-\frac{\varepsilon}{2} \leq S \leq 1\), but since \(P_1-P_0 = \varepsilon(Q - P_0)\) then \(\varepsilon = \frac{\norm{P_1-P_0}}{\norm{Q-P_0}}\).

% To obtain the lower bound on \(S\), \(\norm{Q-P_0}_1=1\) and hence \(\varepsilon = \norm{P_1-P_0}_1=\TV(P_0,P_1)\).

% Finally,
% \eq{
%     \Pr(\mbox{error}) &\geq \half\left[1 - \half\TV(P_0,P_1)\right] \\
%     &\geq \half - \eps
% }
% provided \(\TV(P_0,P_1)\leq 4\eps\).


% \subsection{Hypothesis Testing}
% \label{appx:hyp}
%     We can follow a similar line of reasoning to make a hypothesis-test based statement about the indistinguishability of \(P_0\) and \(P_1\) based again around the TV. \cite{Romano2005} provides details that relate the probabilities of type I and II errors to the TV; we reproduce a small quantity of that work here to outline the argument.
      
%      Define a hypothesis test $\phi : \mc{X} \rightarrow \{0,1\}$ and the sum of the probabilities of type I and type II errors for this test as
%     \eq{
%         S(\phi) &= \int_{\mc{X}}\phi(x)dP_0(x) + \int_{\mc{X}}(1-\phi(x))dP_1(x)
%     }
%     and an optimal test $\phi^*$ such that $S(\phi^*) =\inf_\phi S(\phi)$.
%     Using Theorem 13.1.1 from \cite{Romano2005} we see that this optimal test must satisfy that
%     \eq{
%         \phi^*(x) = \begin{cases}
%             1 \, \textrm{if } p_1(x) > p_0(x) \\
%             0 \, \textrm{if } p_1(x) < p_0(x) \\
%         \end{cases}
%       }
%       and so
%       \eq{
%           S(\phi^*) &= 1 - \half\int_{\mc{X}}|p_1(x) - p_0(x)|\mu(dx) \\
%           &= 1- \half\TV(P_0,P_1)
%       }
%     Hence,
%     \eq{
%           \beta(\phi^\star) &= 1-\half\TV(P_0,P_1) - \alpha(\phi^\star)
%       }
%       Thus if we can bound the TV from above by \(\eps\) we will find a lower bound on the probability of failing to distinguish the approximate distribution from the exact. Finally, from the definition of an optimal test we can then see that for \emph{any} test,
%       \eq{
%         \beta(\phi) \geq 1-\half\TV(P_0,P_1) - \alpha(\phi).
%       }
%       The conclusion then, is that with a probability at least \(1-\nicefrac{\eps}{2}-\alpha\) for some fixed \(\alpha \in [0,1]\) we will fail to distinguish between a draw from the approximate-GP from the exact-GP.

\subsection{Bounding the total variation distance}

      We could try to directly bound the total variation distance, for example, by using the work in \cite{Devroye2018}.
      However we make use of Pinsker's inequality \[\TV(P_0,P_1) \leq \sqrt{\half\KL(P_0,P_1)}\label{pinsker}\tag{\(\dagger\)}\] and bound the \(\KL\) instead.
      
      Rather than bound the KL-divergence between the marginal distributions (\ref{def:marg_kl}), we seek to bound the \emph{conditional} KL (\ref{def:cond_kl}), since for Gaussian distributions this collapses to the 2-norm of the difference of the sample vectors, \(f,\hat{f}\) (\ref{subsec:gauss}). Details justifying this are provided in \ref{subsec:cond_kl}.    
      
    %         \begin{remark}
    %         The implication of this hypothesis-test formulation is that any procedure on a \emph{valid}\footnote{i.e. satisfying our conditions} synthetic dataset, in particular evaluations of GP techniques, will behave identically to data from the true model, since if this were not the case this procedure could be used as a hypothesis test to distinguish the two. 
    %   \end{remark}

      We define the \emph{marginal} KL divergence as the usual KL divergence, to clearly distinguish from the
      \emph{conditional} KL we define below. Note that we assume \(\KL(Q,P)=\KL(q,p)\) where \(q,p\) are the densities of \(Q,P\) respectively w.r.t some dominating measure \(\mu\).
      \begin{definition}[Marginal Kullback-Leibler Divergence]
          \label{def:marg_kl}
          \eq{
              \KL(q,p) &= \int q(y)\log \frac{q(y)}{p(y)}dy. 
          }
      \end{definition}

      \subsection{Conditional $\KL$}
      \label{subsec:cond_kl}
          First, note that $f$ and $\hat{f}$ are correlated RVs that actually depend on an underlying RV $u\sim
              \normdist{0}{I_n}$. We then condition both the approximate and true distributions ($q$ and $p$) on $u$ rather than
          $f$ and $\hat{f}$. In particular, $f=K^\half u$, $\hat{f}=\hat{K}^\half u$.
          \begin{definition}[Conditional Kullback-Leibler Divergence]
              \label{def:cond_kl}
              \eq{
                  \KL(q,p\given u) &= \int q(y\given u)\log \frac{q(y\given u)}{p(y\given u)}dy. 
              }
          \end{definition}
          
        \begin{lemma}
          \label{lem:cond_kl}
              Using \ref{def:cond_kl},
              \eq{\E_u[\KL(q,p\given u)] &\geq \KL(q,p)}
         \end{lemma}

          \begin{proof}[Proof of \ref{lem:cond_kl}]
              \eq{
                  \E_u[\KL(q,p\given u)] &= \int \pi(u)q(y\given u)\log \frac{q(y\given u)}{p(y\given u)}dydu \\
                  &= \int q(y,u)(\log q(y\given u) - \log p(y\given u))dydu \\
                  &= \int q(y,u)\left[\log\frac{q(y,u)}{\pi(u)} - \log\frac{p(y,u)}{\pi(u)}\right]dydu \\
                  &= \underbrace{\int q(y,u)\log\frac{q(y,u)}{p(y,u)}dydu}_{\KL(q(y,u),p(y,u))} \\
                  &=\int q(u\given y)q(y)\left[\log\frac{q(y)}{p(y)} + \log\frac{q(u\given y)}{p(u\given y)}\right]dydu\\
                  &= \underbrace{\int q(y)\log\frac{q(y)}{p(y)}dy}_{\KL(q,p)} + \int q(y)\underbrace{\int q(u\given
                      y)\log\frac{q(u\given y)}{p(u\given y)}du}_{\KL(q(u\given y), p(u\given y))}dy \\
                  &= \KL(q,p) + \underbrace{q(y')\KL(q(u\given y),p(u\given y))}_{\geq 0}.
              }
              where we have used Tonelli's theorem to reorder the integral, the non-negativity of the KL divergence and subsequently the Mean Value Theorem to complete the proof.
          \end{proof}
        %   The significance of this result is that now if we can find an upper bound for the conditional KL, then since
        %   the expectation of a constant is a constant, we will have simultaneously bounded the marginal KL as we
        %   originally set out to do.

      \subsection{Gaussians}
      \label{subsec:gauss}
          Since we are specifically interested in samples from GPs, we are fortunate enough to be dealing with Gaussian
          distributions and, as such, we can write down a closed form for the KL-divergence between two Gaussian measures.

          For the marginal case, with $p(y) = \normdist[y]{0}{K_\xi}$ and $q(y) =
              \normdist[y]{0}{\hat{K}_\xi}$, we have
          \eq{
              \KL(q,p) &= \half\left\{\Tr K_\xi^{-1}\hat{K}_\xi - n
              + \log|K_\xi| - \log|\hat{K}_\xi|\right\}.
          }
          
          We now define \(E=\hat{K}_\xi - K_\xi\) and set $\Delta
         = K_\xi^{-1}E$. Note that $E$ is symmetric but not p.s.d. So we have:
        
        \begin{lemma}
         \label{lem:kl_frob}
         Using the definitions above,
         \eq{
         \KL(q,p) &\leq \frac{\norm{E}_F^2}{4\sigma_\xi^4}. 
         }
        \end{lemma}
        
        \begin{proof} 
        \eq{ 
         \KL(q,p) &= \half\left\{\Tr\Delta - \log|I+\Delta|\right\} \\
         &\leq \frac{1}{4}\Tr\Delta^2 \\
         &\leq \frac{1}{4}\norm{K_\xi^{-1}}_2^2\norm{E}_F^2 \\
         &= \frac{1}{4}\lambda_n(K_\xi)^{-2}\norm{E}_F^2 \\
         &\leq \frac{\norm{E}_F^2}{4\sigma_\xi^4} } 
        where we have used that 
        \eq{ \log|I+\Delta| &=
         \sum_i\log(1+\lambda_i(\Delta)) \geq \sum_i\lambda_i(\Delta) - \half\lambda_i(\Delta)^2 \\
         &\geq \Tr\Delta - \half\Tr\Delta^2,
     }
     and 
     \eq{
     \Tr\Delta^2 &= \Tr[(K_\xi^{-\half}EK_\xi^{-\half})^2] \\
     &= \Tr[(\Lambda_\xi^{-\half}U^TEU\Lambda_\xi^{-\half})^2] \\
     &\leq \lambda_1(K_\xi^{-\half})^2\Tr[EU\Lambda_\xi^{-1}U^TE] \\
     &= \lambda_1(K_\xi^{-1})\Tr[\Lambda_\xi^{-1}U^TE^2U] \\
     &\leq \lambda_1(K_\xi^{-1})^2\Tr(E^2) \\
     &= \norm{K_\xi^{-1}}_2^2\norm{E}_F^2
     }
     with the eigendecomposition \(K_\xi=U\Lambda_\xi U^T\).
     
    %  von Neumann's trace inequality:
    %  \eq{
    %  \abs{\Tr(AB)} &\leq \sum_{i=1}^n\sigma_i(A)\sigma_i(B)
    %  }
    %  for matrices \(A,B \in \mathbb{C}^{n\times n}\) and
    %  Ostrowski's theorem:
    %  \eq{
    %  \lambda_k(X^*AX) &= \theta_k\lambda_k(A)
    %  }
    %  where \(\lambda_n(X^*X)\leq\theta_k\leq\lambda_1(X^*X)\) for an Hermitian matrix \(A\) and non-singular matrix \(X\).
    %  These can be combined to show that
    %  \eq{
    %  \Tr\Delta^2 &\leq \abs{\Tr\Delta^2} = \abs{\Tr(K_\xi^{-1}EK_\xi^{-1}E)} \\
    %  &\leq \sum_{i=1}^n \sigma_i(K_\xi^{-1})\sigma_i(EK_\xi^{-1}E) \quad \textrm{(von Neumann)}\\
    %  &\leq \sigma_1(K_\xi^{-1})\sum_{i=1}^n\sigma_i(EK_\xi^{-1}E) \\
    %  &= \lambda_1(K_\xi^{-1})\sum_{i=1}^n\abs{\lambda_i(K_\xi^{-\half}E^2K_\xi^{-\half})} \quad \textrm{(Using symmetry of \(EK_\xi^{-1}E\))}\\
    %  &\leq \lambda_1(K_\xi^{-1})^2\sum_{i=1}^n\lambda_i(E)^2  \quad \textrm{(Ostrowski)}\\
    %  &= \norm{K_\xi^{-1}}_2^2\norm{E}_F^2.
    %  }
     
     \end{proof}

          For the conditional distributions $p(y\given f) = \normdist[y]{f}{(1-\eta)\sigma_\xi^2I_n}$ and
          $q(y\given \hat{f})=\normdist[y]{\hat{f}}{(1-\eta)\sigma_\xi^2 I_n}$, we get
        \eq{
            \label{eq:cond_kl_gauss}
            \KL(q,p\given f,\hat{f}) &= \half\left[\Tr I_n + (\hat{f}-f)^T\sigma_\xi^{-2}(1-\eta)^{-1}I_n(\hat{f}-f) - n
                \right] \\
            &= \frac{1}{2(1-\eta)\sigma_\xi^2}\norm{\hat{f}-f}_2^2 \tag{*}.} 
            
            We can then apply lemma \ref{lem:cond_kl} to demonstrate that, if we can find an
        upper bound on the 2-norm between true and approximate function evaluations on some $x$ data, we will be
        able to correspondingly bound the KL-divergence between the marginal distributions of the noise-corrupted
        samples and finally the TV via the inequality \eqref{pinsker}.

 
      %=========================================================
\section{Random Fourier Features}
\label{appx:rff}
\begin{algorithm}[H]
Let \(M\) denote memory usage for each line. \\
Define a rule to set a random seed for each \(j\) in \(1..D\) to ensure the same vector \(\omega_j\) is used for each row of \(X\). \\
\For{$i:n$}{
\begin{algorithmic}
    \State Sample \((x_i^{(1)},\dots,x_i^{(d)}) \sim \mathbb{P}_X\) \Comment{\(M=\order{d}\)}
    \State \(\hat{f}_i \gets 0\) 
    \end{algorithmic}
    \For{$j:D$}{
    \begin{algorithmic}
            \State Sample \((\omega_j^{(1)},\dots, \omega_j^{(d)}) \sim \mathbb{P}_\Omega\) \Comment{\(\order{1} \leq M \leq \order{d^2}\)}
            \State Compute \(z_j(x_i) = g(x_i^T\omega_j)\) \Comment{\(M=\order{1}\)}
            \State Sample \(w_j \sim \normdist{0}{1}\) \Comment{\(M=\order{1}\)}
            \State \(\hat{f}_i \gets \hat{f}_i + z_j(x_i)w_j\) \Comment{\(M=\order{1}\)}
    \end{algorithmic}
    }
}
\caption{Memory-efficient procedure to generate RFF samples.}
\label{alg:rff_sampling}
\end{algorithm}
Algorithm \ref{alg:rff_sampling} represents an extreme example of how we can trade-off sequential time complexity in the RFF procedure to produce an exceptionally memory-efficient method of at worst \(\order{d^2}\) (in the most general case where a \(d\times d\) matrix is required to sample from \(\mathbb{P}_\Omega\)). We would not recommend this algorithm for a practical implementation, but rather as an illustration of how far the principle can be pushed: massively distributing work amongst many processors and opting to write each sample to disk to avoid storing the full length \(n\) vector output. 

\label{appx:rff_proof}

    \begin{proof}[Proof of \ref{lem:rff}]
         Define the matrix of differences $E_{ij} = z(x_i)^Tz(x_j)-k(x_i,x_j)$.
         \eq{
         |E_{ij}|<\varepsilon/n &\implies \underbrace{\sum_{ij}E_{ij}^2}_{=\norm{E}_F^2}<\varepsilon^2\implies \norm{E}_F < \varepsilon
         }
         which implies
         \eq{
             \Pr\left(\abs{E_{ij}}<\frac{\varepsilon}{n} \quad \forall i,j\right) &= \Pr\left(\abs{E_{ij}}^2<\frac{\varepsilon^2}{n^2} \quad
             \forall i,j\right) \\
             &\leq \Pr\left(\sum_{ij}\abs{E_{ij}}^2<\sum_{ij}\frac{\varepsilon^2}{n^2}\right) \\
             &= \Pr\left(\sum_{ij}\abs{E_{ij}}^2< \varepsilon^2\right). }

         At a particular pair of locations $(x,x')$ which correspond to an element of the kernel matrix, we can use the
         bounds on the (vector) functions $z_j(x) = \sqrt{\frac{2}{D}}(\sin(\omega_j^Tx),\cos(\omega_j^Tx))^T$ (as suggested in \cite{Sutherland2015}) and Hoeffding's inequality to get a
         probabilistic tail bound on the error of an element of the approximate kernel matrix: \eq{ S_{D/2} &= \sum_{i=1}^{D/2}
             z_i(x)^Tz_i(x') - k(x,x') \\
             -2/D &\leq z_i(x)^Tz_i(x') \leq 2/D \\
             p = \Pr\left(|S_{D/2}|\geq \frac{\varepsilon}{n}\right) &\leq
             2\exp\left(\frac{-2\varepsilon^2}{n^2\sum_{i=1}^{D/2}(2/D--2/D)^2}\right) \\
             &= 2\exp(-D\varepsilon^2/4n^2) .}

         Now we apply the union bound, assuming that the locations are relatively uncorrelated. Note that $E$ is symmetric
         and we can ensure the diagonal elements are 0 so that we only need to bound $\half n(n-1)$ elements: \eq{ q =
             \Pr\left(\bigcup_{ij}\left\{\abs{E_{ij}}\geq \frac{\varepsilon}{n}\right\}\right) &\leq p\cdot \half n(n-1) =
             n(n-1)\exp(-D\varepsilon^2/4n^2) \\
             &\leq n^2\exp\left(-\frac{D\varepsilon^2}{4n^2}\right)\\
             \Pr\left(\abs{E_{ij}}<\frac{\varepsilon}{n} \quad \forall i,j\right) &= 1-q. }

         If we now state that we wish to choose a number of RFF $D$
         s.t. all elements of the error matrix are less than $\varepsilon/n$ with probability $1-\delta$ then we can
         rearrange the final expression above with $q=\delta$ to find:
         \eq{ D &\geq
             8\log\left(\frac{n}{\sqrt{\delta}}\right)\frac{n^2}{\varepsilon^2} \\
             &\implies \norm{E}_F^2 < \varepsilon^2 \\
             &\implies \KL <\frac{\varepsilon^2}{4\sigma_\xi^4} \quad (\ref{lem:kl_frob})\\
             &\implies \TV < \frac{\varepsilon}{\sqrt{8}\sigma_\xi^2} \quad \eqref{pinsker}.
         }
         To complete the proof we set \(\eps=\sqrt{8}\sigma_\xi^2\varepsilon\) so that \(\TV<\eps\).
     \end{proof}

\section{Contour Integral Quadrature}
\label{appx:ciq_summary}

\subsection{Summary of the CIQ method}
Here we give a brief summary of the CIQ method, as discussed in \cite{Davies2005}, \cite{Hale2008} and \cite{Pleiss2020}. We neglect much of the intricacies, which can be found in the aforementioned sources.
The CIQ method relies on a numerical quadrature approximation of the matrix version of Cauchy's integral theorem, for some square matrix \(A\):
\[f(A) = \frac{1}{2\pi\it{i}}\int_\Gamma f(z)(zI-A)^{-1}dz. \]
As usual in complex analysis \(\Gamma\) is a closed anticlockwise contour over which \(f\) is analytic. 

In our case, we want to use \(f(z)=z^\half\) so that
\[ A^\half = \frac{1}{2\pi\it{i}}\int_\Gamma z^\half(zI-A)^{-1}dz.\]
Section 4 of \cite{Hale2008} pays particular attention to this case and makes a change of variables \(w=z^\half\) to find
\eq{
A^\half &= \frac{A}{\pi\it{i}}\int_{\Gamma_w}(w^2I-A)^{-1}dw\\
&= \frac{\it{i}A}{\pi}\int_{-\it{i}\infty}^{\it{i}\infty}(w^2I-A)^{-1}dw
}

This expression is then approximated using a trapezoid rule with \(Q\) quadrature points:
\[\hat{A}_Q^\half = \frac{-2K'm^\half A}{\pi Q}\sum_{q=1}^Q(w(t_q)^2I-A)^{-1}\mathrm{cn}(t_q)\mathrm{dn}(t_q)
\]
where \(\mathrm{cn},\mathrm{dn}\) are Jacobi elliptic functions in standard notation.

To compute matrix-vector products, as we intend to, note that Cauchy's integral formula can be adapted straightforwardly to
\[ f(A)b = \frac{1}{2\pi\it{i}}\int_\Gamma f(z)(zI-A)^{-1}b dz.\]

Although these expressions are sufficient to calculate the desired approximations, we refer the reader to \cite{Pleiss2020} for significantly more detail on the practicalities of an efficient implementation.

 \label{appx:ciq_proof}

 \subsection{Proof of bounds for CIQ parameters (\ref{lem:ciq_bounds})}
 \begin{proof}

     From \cite{Pleiss2020} we get the following expression for the error when using CIQ to approximately sample from a
     multivariate normal:

     \numeq{
     \label{eq:epsJ}
         \varepsilon_J = \norm{a_J-K^\half u}_2 &\leq
         \underbrace{\order{\exp\left(-\frac{2Q\pi^2}{\log\kappa+3}\right)}}_{\varepsilon_Q} +
         \underbrace{\frac{2Q\log(5\sqrt{\kappa})\kappa\sqrt{\lambda_n}}{\pi}\left(\frac{\sqrt{\kappa}-1}{\sqrt{\kappa}+1}\right)^{J-1}}_{B(Q,J)}\norm{u}_2. 
     }
     Clearly the most important factor in the number of iterations, $J$, required for the algorithm to satisfy some
     prespecified degree of accuracy $\varepsilon_J$ for a sample of size $n$ depends on the condition number
     $\kappa$ of the kernel matrix. Thus we need a bound on the condition number, which we can find by the following
     argument.

     Using the spectral condition number $\kappa = \frac{\lambda_1}{\lambda_n}$, where we have ordered the
     eigenvalues such that $\lambda_1 \geq \lambda_2 \geq \dots \lambda_n$, we first bound the largest eigenvalue by
     noting that for a kernel $K$ with kernel-scale $\sigma_f^2=1$, $\Tr K = n$ and where $\Tr K = \sum_{i=1}^n\lambda_i$, we
     see that $\lambda_1 \leq n$. For a kernel with added uncorrelated noise of the form $K_{\eta\xi}=K+\eta\sigma_\xi^2 I_n$, we
     similarly see that $\lambda_1(K_{\eta\xi}) \leq n + \eta\sigma_\xi^2$. We include this latter case as it also allows us to bound
     the minimum eigenvalue as $\lambda_n(K_{\eta\xi}) \geq \eta\sigma_\xi^2$ and so
     \eq{
         \kappa(K_{\eta\xi}) &\leq \frac{n}{\eta\sigma_\xi^2} + 1.
     }

     Now, starting from the required bound on the TV-distance we can obtain lower bounds for the CIQ fidelity parameters \(Q,J\) to ensure that \(Q\) is such that \(\varepsilon_Q < \delta_Q\) for some free parameter \(\delta_Q\) and \(\TV(q,p)\leq\eps\):
     
     \eq{
     \TV(q,p) &\leq \sqrt{\half\KL(q,p)} \leq \frac{1}{\sqrt{2}}\sqrt{\E_u\left[\KL(q,p\given u)\right]} \\
     &= \half\sqrt{\E_u\left[\frac{1}{(1-\eta)\sigma_\xi^2}\norm{f-\hat{f}}^2_2\right]} \\
     &\leq \frac{1}{2\sqrt{1-\eta}\sigma_\xi}\sqrt{\E_u\left[\varepsilon_Q + B(Q,J)\norm{u}_2^2\right]} \\
     &= \frac{1}{2\sqrt{1-\eta}\sigma_\xi}\sqrt{\varepsilon_Q^2 + B(Q,J)^2\E_u\norm{u}_2^2 + \varepsilon_QB(Q,J)\E_u\norm{u}_2} \\
     &\leq \frac{1}{2\sqrt{1-\eta}\sigma_\xi}(\delta_Q + \sqrt{n}B(Q,J)) \leq \half\eps.
     }
     
     Where we have used Pinsker's inequality \eqref{pinsker}, lemma \ref{lem:cond_kl}, \eqref{eq:cond_kl_gauss}, \eqref{eq:epsJ} and that $u\sim \normdist{0}{I_n}$ and hence $\norm{u}_2\sim\chi_n$ (note not \(\chi^2\)). Thus we can explicitly find the mean (and variance) (\cite{chidist}) and hence a bound on \(\E_u\norm{u}_2\). Here we will assume $n$ is large and use the asymptotic expansion in \cite{Laforgia2012} to simplify the result:
        \eq{
             \E[\norm{u}_2] &= \sqrt{2}\frac{\Gamma(\half(n+1))}{\Gamma(n/2)} \\
             &= \sqrt{n}(1-\frac{1}{4n} + \order{n^{-2}}) \leq \sqrt{n}. 
            %  \Var(\norm{u}_2) &= \E\norm{u}_2^2 - (\E\norm{u}_2)^2 \\
            %  &= n - n(1-\frac{1}{2n} + \order{n^{-2}}) = \half - \order{n^{-1}} \leq \half
         }
         
         From here we can write 
         \numeq{
         \frac{\eps\sigma_\xi\sqrt{1-\eta}-\delta_Q}{\sqrt{n}} &\geq B(Q,J) 
         \label{eq:BQJ}
         }
         and rearrange for \(J\) (once we have bounded \(Q\) in terms of our free parameter \(\delta_Q\)). Note that \eqref{eq:BQJ} determines an upper bound on \(\delta_Q\) to guarantee that the LHS is positive.
         
         In the next sections we make extensive use of the following relationships (where \(\kappa\leq 1+\zeta\) with \(\zeta \gg 1\)):
             \numeq{
             \label{eq:sqrtkappa}
             \sqrt{\kappa} \leq \sqrt{1+\zeta} = \sqrt{\zeta}(1+\zeta^{-1})^\half \leq \sqrt{\zeta} + \frac{1}{2\sqrt{\zeta}} 
             }
             and
             \numeq{
             \label{eq:logkappa}
              \log\kappa = \log\zeta + \log(1+\zeta^{-1}) \leq \log\zeta + \zeta^{-1}
             }
             to see that
             \numeq{
             \label{eq:sqrtkappalogkappa}
             \sqrt{\kappa}\log\kappa\leq \sqrt{\zeta}\log\zeta + \frac{1}{2\sqrt{\zeta}}\log\zeta + \frac{1}{\sqrt{\zeta}} + \frac{1}{2\zeta^{3/2}}.
             }
             Similarly,
             \eq{
             \log\log\frac{x}{y} &= \log(\log x - \log y) = \log\log x + \log\left(1-\frac{\log y}{\log x}\right) \\
             &=\log\log x -\frac{\log y}{\log x} + \order{(\log x)^{-2}}
             }
             giving
             \numeq{\label{eq:loglogxy1}
             \log\log\frac{x}{y} &= \log\log x + \order{(\log x)^{-1}}
             }
             provided \(x \gg y\), and
             \numeq{\label{eq:loglogxy2}
             \log(\log x+y) &= \log\log x +\order{(\log x)^{-1}}
             }
             if additionally \(y < \log x\).

     \subsubsection{Bounding the number of Quadrature Points}
         To ensure \(\varepsilon_Q\) does not exceed \(\delta_Q\) we will constrain \(Q\) as follows:
         \numeq{
         \label{eq:Qkappa}
             Q &\geq \left(\log\kappa + 3\right)(-\log\delta_Q)\frac{1}{2\pi^2}.
        }
        Using \eqref{eq:logkappa} with \(\zeta=\frac{n}{\eta\sigma_\xi^2}\) we can achieve a sufficient \(Q\) by requiring that
        \numeq{
             Q &\geq \frac{1}{2\pi^2}\left(\log\frac{n}{\eta\sigma_\xi^2} + 3 + \order{n^{-1}}\right)(-\log\delta_Q)
             \label{eq:Q}.
         }

     \subsubsection{Bounding the number of msMINRES Iterations}
         Taking \eqref{eq:epsJ} and \eqref{eq:BQJ} we rearrange in terms of $J$ to find
         \numeq{
         \label{eq:J1}
             J &\geq 1 + \frac{1}{\log(\sqrt{\kappa}-1) - \log(\sqrt{\kappa}+1)}\log\left\{\frac{\pi(\eps\sigma_\xi\sqrt{1-\eta} -
                 \delta_Q)}{2Q\sqrt{\lambda_n}\kappa\sqrt{n}(\log(5\sqrt{\kappa}))}\right\}.
         }
         We start by simplifying the prefactor (making use of Taylor expansions):
         \eq{
             \log(\sqrt{\kappa}-1) - \log(\sqrt{\kappa}+1) &= \log(1-1/\sqrt{\kappa}) - \log(1+1/\sqrt{\kappa}) \\
             &= -\frac{2}{\sqrt{\kappa}} - \order{\kappa^{-3/2}} \\
            %  &\leq -\frac{2}{\sqrt{\kappa}} \\
            %  (\log(\sqrt{\kappa}-1) - \log(\sqrt{\kappa}+1))^{-1} &\geq -\frac{\sqrt{\kappa}}{2} \\
             (\log(\sqrt{\kappa}-1) - \log(\sqrt{\kappa}+1))^{-1} &= -\frac{\sqrt{\kappa}}{2}\left(1-\order{\kappa^{-1}}\right). 
            %  &\geq -\frac{\sqrt{n}}{2\sqrt{\eta}\sigma_\xi}
         }
         
         Before we substitute our bound for the condition number, we first give a more general expression. To obtain it we define the RHS of \eqref{eq:Qkappa} to be \(\bar{Q}\) and that \(\log\bar{Q} \leq \log\log\kappa + \log(-\log\delta_Q)+\order{(\log\kappa)^{-1}}\) using \eqref{eq:loglogxy2}. Hence,
         \numeq{
            J &\geq 1 + \frac{\sqrt{\kappa}}{2}\left[\log(\kappa\sigma_\xi\sqrt{n}) + 2\log\log\kappa - \log(\eps\sigma_\xi\sqrt{1-\eta}-\delta_Q) + C\right]
            \label{eq:Jkappa}
         }
         where \(C\) is a pseudo-constant that contains constants, decaying functions of \(\kappa\) and similarly `negligible' terms (such as \(\log(-\log\delta_Q)\) and the small error term \(\abs{\sqrt{\lambda_n}-\sqrt{\eta}\sigma_\xi}\)). Note that the RHS here is larger than the RHS of \eqref{eq:J1} so that it is a more conservative bound.
         
         Now using our bounds for \(\kappa\), \(\sqrt{\kappa}\) \eqref{eq:sqrtkappa} and \(\log\kappa\) \eqref{eq:logkappa} (with \(\zeta=\frac{n}{\eta\sigma_\xi^2}\)) we see that (denoting \(\tilde{J}\) as the RHS of \eqref{eq:Jkappa})
         \eq{
         \tilde{J} &\leq 1 + \frac{\sqrt{n}}{2\sqrt{\eta}\sigma_\xi}\left\{\log\left(\left(\frac{n}{\eta\sigma_\xi^2}+1\right)\sigma_\xi\sqrt{n}\right) + 2\log\log\left(\frac{n}{\eta\sigma_\xi^2}+1\right) -\log(\pi(\eps\sigma_\xi\sqrt{1-\eta}-\delta_Q)) + C' \right\} \\
         &\leq 1 + \frac{\sqrt{n}}{2\sqrt{\eta}\sigma_\xi}\left\{\log\left(\frac{n^{3/2}}{\eta\sigma_\xi}\right) + 2\log\log\frac{n}{\eta\sigma_\xi^2} -\log(\pi(\eps\sigma_\xi\sqrt{1-\eta}-\delta_Q)) + C''\right\} \\
         &\leq 1 + \frac{\sqrt{n}}{2\sqrt{\eta}\sigma_\xi}\left\{\log n^{3/2} -\log(\pi(\eps\sigma_\xi\sqrt{1-\eta}-\delta_Q)) + 2\log\log n + C'''\right\} \\
         &= \ordtilde{\frac{\sqrt{n}}{\sqrt{\eta}\sigma_\xi}\log\frac{n}{\sigma_\xi(\eps\sigma_\xi\sqrt{1-\eta}-\delta_Q)}}
         }
         where we have again used \eqref{eq:logkappa} and \eqref{eq:loglogxy1} and absorbed the constant and decaying terms into the sequence of `constants' \(C',C'',C'''\).
         
         Having upper bounded \(\tilde{J}\), if we now use this as a lower bound for \(J\) then we have a sufficient condition to satisfy our TV requirement, i.e.
         \numeq{
          J & \geq \ordtilde{\frac{\sqrt{n}}{\sqrt{\eta}\sigma_\xi}\log\frac{n}{\sigma_\xi(\eps\sigma_\xi\sqrt{1-\eta}-\delta_Q)}}
         }
         where we use \(\ordtilde{\cdot}\) to mean ignoring \(\log\log\) terms.
         
        %  {\color{gray}
        %  \eq{
        %  J &\geq 1 + \frac{\sqrt{n}}{2\sqrt{\eta}\sigma_\xi}\left\{\log\left[2Q\sigma_\xi\log\left(\frac{5\sqrt{n}}{\sqrt{\eta}\sigma_\xi}\right)\left(\frac{n^{3/2}}{\eta\sigma_\xi^2} + \sqrt{n}\right) + \order{1}\right] - \log(\pi(\eps\sigma_\xi\sqrt{1-\eta}-\delta_Q)) \right\} \\
        %  &\geq 1 + \frac{3\sqrt{n}}{4\sqrt{\eta}\sigma_\xi}\left\{\log n - \frac{2}{3}\log(\eta\sigma_\xi\pi(\eps\sigma_\xi\sqrt{1-\eta}-\delta_Q)) + \frac{2}{3}\log\frac{\alpha}{2\pi^2} + \frac{4}{3}\log\log\frac{n}{\eta\sigma_\xi^2} + \order{\log n^{-1}} \right\} \\
        %  &= \ordtilde{\frac{\sqrt{n}}{\sqrt{\eta}\sigma_\xi}\log\frac{n}{\sigma_\xi(\eps\sigma_\xi\sqrt{1-\eta}-\delta_Q)}}
        %  }
        %  where we use \(\ordtilde{\cdot}\) to mean ignoring \(\log\log\) terms.
        %  }

\end{proof}

% \begin{remark}
% Now, choosing $b=10$ and setting $\sigma_\xi^2=10^{-\beta}$, $n=10^m$ for $m,\beta>0$, then we see that we need
%          \eq{
%              Q &\geq (m+\beta + 3)\frac{\alpha}{2\pi^2}
%          }
%          which means that for plausible situations where $\beta = 3$, and $m\geq 4$ then
%          \eq{
%              Q &\geq \log n
%          }
%          will suffice, provided we require at most $\alpha \leq 6$.
         
%          We also note that from \ref{lem:ciq_bounds} that we need \(\delta_Q < \varepsilon_J\) and in practice we aim to ensure that \(\delta_Q\) is a full order of magnitude less than \(\varepsilon_J\). 
%         %  Fortunately, \ref{sf:Qmin} shows that we can manage this relatively easily for reasonable parameter values.
         
% \end{remark}

     \subsubsection{Preconditioning}

         \begin{proof}[Proof of \ref{lem:nystrom_precond}]
             If we take the rank-$k$ Nystr\"{o}m approximation ($\tilde{K}$) as our preconditioner, with cost $\order{Nk^2}$, then Corollary 4.10 of \cite{Shabat2019} shows:
             \numeq{
             \label{eq:cond1}
                 \tilde{\kappa} = \cond[(\tilde{K} + \eta\sigma_\xi^2 I)^{-1}(K+\eta\sigma_\xi^2 I)] &\leq 1 + \frac{2\lambda_{k+1}(K)\sqrt{4k(n-k)+1}}{\eta\sigma_\xi^2}.
             }
             (Henceforth we use \(\lambda_k\) to denote \(\lambda_k(K)\).) To satisfy our cost requirement we set $k=\lfloor{\sqrt{n}}\rfloor$ and since 
             \eq{
             (4\sqrt{n}(n-\sqrt{n}) + 1)^\half &= (1+4n^{3/2}(1-1/\sqrt{n}))^\half\\
             &= 2n^{3/4}(1-1/\sqrt{n})^\half\left(1+\frac{1}{8}n^{-3/2}(1-1/\sqrt{n})^{-1} + \order{n^{-3}}\right) \\
             &=2n^{3/4}\left(1-\frac{1}{2\sqrt{n}} - \frac{1}{8n} - \frac{1}{16n^{3/2}} + \order{n^{-2}}\right)\left(1+\frac{1}{8n^{3/2}} +
             \order{n^{-2}}\right) \\
            %  &= 2n^{3/4} - n^{1/4} - o(1)\\%\order{n^{-1/4}} \\
             &\leq 2n^{3/4}
             }
             we have
             \numeq{
             \label{eq:cond2}
             \tilde{\kappa} &\leq 1+\frac{4\lambda_{k+1}}{\eta\sigma_\xi^2}n^{3/4}
             }
             which we write as \(\tilde{\kappa} \leq 1+\zeta\), as before, but now with \(\zeta=\frac{4\lambda_{k+1}}{\eta\sigma_\xi^2}n^{3/4}\).

             With this value of \(\zeta\) we again make use of \eqref{eq:sqrtkappa}, \eqref{eq:logkappa} and \eqref{eq:sqrtkappalogkappa} to show that
             \numeq{
             \label{eq:cond3}
                \sqrt{\tilde{\kappa}}\log\tilde{\kappa} &\leq \frac{2\sqrt{\lambda_{k+1}}}{\sqrt{\eta}\sigma_\xi}n^{3/8}\log\left(\frac{4\lambda_{k+1}}{\eta\sigma_\xi^2}n^{3/4}\right) + \order{n^{-3/8}\log n} .
             }
             
             Similarly, we have
             \numeq{
             \label{eq:cond4}
             \sqrt{\tilde{\kappa}} &\leq \sqrt{\zeta} + \half\zeta^{-\half} = \frac{2\sqrt{\lambda_{k+1}}}{\sqrt{\eta}\sigma_\xi}n^{3/8} + \order{n^{-3/8}}.
             }
             
             We finish the proof by rewriting \eqref{eq:Jkappa} in the form
             \eq{
             J &\geq 1 + \frac{\sqrt{\tilde{\kappa}}}{2}\left(\log(\tilde{\kappa}\sqrt{n}\sigma_\xi) - \log(\eps\sigma_\xi\sqrt{1-\eta}-\delta_Q) + \tilde{C}\right)
             }
             and inserting \eqref{eq:cond3} (updating \(\tilde{C}\rightarrow\tilde{C}'\) to absorb additional approximately negligible terms).
         \end{proof}
         

        %  \begin{proof}[Proof of \ref{lem:gauss_precond}]\label{proof:gauss_precond}
        %      Using \ref{lem:nystrom_precond} and the expression for eigenvalues of a Gaussian kernel we can see that
        %      \eq{
        %          \tilde{\kappa} &\leq 1 + \frac{4}{\sigma_\xi^2}\order{n^{5/4}\e^{-\sqrt{n}C}}
        %          %  \\&< 1 + \frac{4}{C\beta}\order{n^{3/4}}
        %      }
        %      For large enough \(n\) such that the exponential term dominates, we now have (using the same notation but different definition of \(\zeta\) to above) \eq{
        %      \sqrt{\tilde{\kappa}}\log\tilde{\kappa} &\leq \zeta + \half\zeta^2 \\
        %      &= \frac{4}{\sigma_\xi^2}\order{n^{5/4}\e^{-\sqrt{n}C}}
        %      }
        %      where we have dropped terms that decay faster; this includes errors introduced when utilising the asymptotic expression for the Gaussian kernel eigenvalues.
        %      Now we can substitute this into our expression for $J$.
        %  \end{proof}
         
         \begin{proof}[Proof of \ref{lem:general_precond}]\label{proof:general_precond}
             \cite{Belkin2018} shows us that for sufficiently smooth radial kernels, the \(k^{th}\) matrix eigenvalue is given by (e.g. Gaussian, Cauchy) \eq{
             \lambda_k &\lesssim n\sqrt{\varphi}c_2\e^{-c_1k^{1/d}}
             }
             for $c_1,c_2>0$, $x \in \mathbb{R}^d$, $\varphi=\sup_{x\in\Omega}k(x,x)$.
             For us, $\varphi=\sigma_f^2$ and we set $k=\floor{\sqrt{n}}$.
             
             We can use this to see
             \eq{
             \sqrt{\lambda_{k+1}}n^{3/8} &\leq \sqrt{\lambda_k}n^{3/8} \leq \sqrt{c_2\sigma_f}n^{7/8}\e^{-\frac{c_1}{2}n^{1/d}} \leq \sqrt{c_2\sigma_f}n^{7/8}
             }
             and hence use this with lemma \ref{lem:nystrom_precond} (for \ref{itm:modn})
             \eq{
             J &\geq 1 + \frac{\sqrt{c_2\sigma_f}}{\sqrt{\eta}\sigma_\xi}n^{7/8}\left(\frac{5}{4}\log n - \log(\eps\sigma_\xi\sqrt{1-\eta}-\delta_Q) + \tilde{C}'\right) \\
             &= 1 + \order{n^{7/8}\log n}.
             }

             For modest \(n\) such that \(0<\frac{3}{8}\log n - \frac{c_1}{2}n^\frac{1}{d} < 1\) (\ref{itm:bign}),
             \eq{
             \exp\left[\frac{7}{8}\log n - \frac{c_1}{2}n^\frac{1}{d}\right] &\leq 1 + \frac{7}{8}\log n 
             }
             which we combine with \ref{lem:nystrom_precond} to obtain 
             \eq{
             J &\geq 1 + \frac{\sqrt{c_2\sigma_f}}{\sqrt{\eta}\sigma_\xi}\left[\frac{31}{32}(\log n)^2 + \frac{5}{4}\log n - \log(\eps\sigma_\xi\sqrt{1-\eta} - \delta_Q) + \tilde{C}''\right] \\
             &= \order{(\log n)^2}
             }
             where we have again absorbed small constant, decaying and \(\log\log\) terms into \(\tilde{C}''\).
             
             At sufficiently large \(n\) (\ref{itm:asympn}), we can write
             \eq{
             \sqrt{\lambda_k}n^{3/8}\log n &\leq \sqrt{c_2\sigma_f}\exp\left[\frac{7}{8}\log n + \log\log n - \frac{c_1}{2}n^\frac{1}{d}\right] \\
             & \leq \sqrt{c_2\sigma_f}
             }
             since for all finite $d$ the $n^\frac{1}{d}$ term grows faster in $n$ than $\log n$ (and definitely \(\log\log n\)), the exponent will become negative at large $n$. This gives us
             \[
                J \geq 1 + \order{1}.
             \]
         \end{proof}
         
        \begin{remark}
         Note that the constant terms hidden in the \(\order{\cdot}\) notation are largely consistent for all \(\gamma\) and generally less than \(\order{\sigma_\xi^{-1}}\).
         \end{remark}
        
        \begin{remark}
                For the special case of the RBF kernel we can exploit the precise expressions for the eigenvalues to obtain more specific bounds on \(J\), but we skip these details here.
        \end{remark}

\section{Conjugate Gradients (CG)}
\label{appx:cg}
In addition to the sampling methods described in this paper we would also like to acknowledge that the conjugate gradients algorithm can also be adapted to facilitate approximate sampling as, for example, in \cite{Parker2012}. Since the computational complexity of such a procedure is in general \(\order{n^2k}\) where \(k\) is the number of iterations employed by the conjugate gradient algorithm, in order for this to be competitive, we require \(k\leq\order{\sqrt{n}\log n}\). Within the time-frame of this paper, we have been unable to investigate this option fully, but we believe that in cases where the Gram matrix can be well-approximated by a low-rank matrix (e.g. at large lengthscales), CG sampling is likely to be a promising approach.

% \pagebreak
\section{Summary of algorithms}
\label{appx:results}
    \begin{table}[h!]
        \centering
        \begin{tabular}{lll}
        \hline
        \textbf{Method} & \textbf{Time} & \textbf{Space}\\
           \hline
                Cholesky    &   \(\order{n^3}\) & \(\order{n^2}\) \\
                RFF         &   \(\order{n^3\log n}\) & \(\order{n}\)\\
                % pmRFF        &   \(\order{d^3n^2\log n}\)    &   \(\order{nd}\) \\
                CIQ         &   \(\order{n^{5/2}\log n}\) & \(\order{n\log n}\) \\
                PCIQ         &   \(\order{n^{2.375}\log n}\) & \(\order{n\log n}\) \\
            \hline
        \end{tabular}
        \caption{Time and space complexity of competing methods of generating draws from a GP. P=with preconditioning. \label{appx:table}}
    \end{table} 
    
In addition to the algorithms in table \ref{appx:table} we point out that the RFF method is highly parallelisable and the `wall-clock' time could be significantly reduced from that listed, with a large enough supply of processors; but it is beyond the scope of this paper to delve into the details of such an implementation. Finally, the PCIQ entry does not include the gains observed as \(n\) becomes very large (as outlined in \ref{lem:general_precond}). 

\section{Implementation}
\label{appx:implementation}
    We implemented the RFF sampling procedure in NumPy and made use of the GPyTorch\footnote{\url{github.com/cornellius-gp/gpytorch/}} (\cite{Gardner2018}) library to facilitate CIQ. We wrapped a SciPy implementation of interpolative decomposition to integrate with GPyTorch for preconditioning with the CIQ method. These will be made available on our GitHub (\url{github.com/ant-stephenson/gpsampler}) and can be installed as a Python library.
    
    To run our experiments we made use of the HPC system BluePebble at the University of Bristol, using nodes with 12 CPUs with 15GB of memory each and allocating a maximum of 200 hours per experiment. This was sufficient for our purposes and this much parallel compute was only necessary to facillitate the running of 1000 repeats per experiment.

\section{Experiments}
\label{appx:experiments}
    We present results from some empirical experiments running hypothesis tests in section \ref{sec:experiments}. We chose to use hypothesis testing rather than directly implement the Bayesian decision procedure to avoid considerable coding effort and compute resource whilst still demonstrating our main points.
    
    A more complete description of the method used is the following:
    
    \begin{algorithm}[H]
    \KwIn{A set of adjustable experiment parameters \(\theta\); \(N\) the number of repeat experiments per parameter set.}
    \KwOut{Hypothesis test rejection rate, \(r\).}
    \begin{algorithmic}
         \State \(r\gets 0\) 
    \end{algorithmic}
         \For{$i:N$}{
            \begin{algorithmic}[1]
                 \State Generate sample \(\hat{y}\) of length \(n\) using method \(M(\theta)\). 
                 \State Whiten the sample using a Cholesky decomposition of the true kernel matrix, i.e. \(\hat{z}=L^{-1}\hat{y}\) for \(K_\xi=LL^T\).
                 \State Run a (CramÃ©r-von Mises) hypothesis test to determine whether the whitened sample \(\hat{z}\) is consistent with an i.i.d draw from a standard normal distribution. Record the test result \(t \in \{0,1\}\) at a predetermined significance level \(\alpha\).
                 \State \(r \gets r + t\)
            \end{algorithmic}
            }
            \begin{algorithmic}
            \State \(r \gets r/N\)
            \end{algorithmic}
        
         \caption{Procedure used to test sample quality.}
         \label{alg:hyp_test}
     \end{algorithm}
     
     In addition to figure \ref{fig:results}, figure \ref{fig:results_more} shows the results of further experiments run at larger lengthscales to assess how performance of the CIQ method degrades as condition number becomes more extreme. We note that apart from an increase in variance (which is expected), the results appear to be quite stable, oscillating around the blue line and mostly contained between the green lines. The blue line represents the rejection rate we expect under the null hypothesis (that the distributions are the same) and is thus the rate we expect to achieve at convergence. The green lines are given by the 95\% confidence intervals for a large-sample of Bernoulli trials at the converged rate.
    \begin{figure}
        \centering
        \subfloat[]{\label{appx_main:b}\includegraphics[clip,scale=0.3]{\detokenize{logreject-logD_byN_ciq_1_2703849_rescaled_3}}}\\
        \centering
        \subfloat[]{\label{appx_main:c}\includegraphics[scale=0.3]{\detokenize{logreject-logD_byN_ciq_1_2686645_rescaled_2}}}
        \caption{Rejection rate convergence with size of fidelity parameter as before, with additional plots at more extreme lengthscales.}
        \label{fig:results_more}
\end{figure}
    
    % On the results themselves we would like to note that, although the experiments are broadly what we expected to find, the fact that PCIQ appears to converge more slowly in the \(l=0.1\) case than the \(l=1.0\) case seems surprising, since in the former the kernel matrix should be closer to the identity and have a smaller condition number. In running the experiments we made extensive use of the GPyTorch machinery, pushing it beyond its intended use. As a result we wish to acknowledge the possibility that our potential {\it misuse} could be responsible for spurious results.
    
    On the results themselves, we note that the PCIQ method appears to converge more slowly for \(l=0.1\) than for \(l=1.0\), an initially surprising result. At first consideration, we expect the kernel matrix for the former to be closer to the identity and thus have a smaller condition number. This is true, but conversely, the effectiveness of the preconditioning step is hindered by the fact that at small lengthscales the matrix will also be full rank and thus to well-approximate the inverse we are likely to need a higher rank approximation. To test this hypothesis we ran a simulation of our (rank-\(\sqrt{n}\)) preconditioner acting on a series of random kernel matrices with varying lengthscales. Figure \ref{fig:precond_effect} shows the results from this simulation from which can be seen an apparent peak in the vicinity of \(l=0.1\), in particular for the \(n \in \{2000,5000\}\) cases, which replicates our previous findings. In fact, for the precise setup in question and an RBF kernel with \(d=2\), it can be shown that 0.1 is close to the worst case lengthscale.
    
        \begin{figure}
        \centering
        \includegraphics[scale=0.5]{\detokenize{precon_effect_w_lengthscale_reg1em2.pdf}}
        \caption{Preconditioner effectiveness as a function of lengthscale. Different colour lines represent different sample sizes. For this simulation we used parameter values consistent with our previous experiments: \((d, \sigma_\xi^2,\sigma_f^2)=(2, 0.001,1.0)\). The (first) black dashed line is at \(l=0.1\) and the second dot-dash line is at \(l=1.0\). \(\hat{P}\) represents the preconditioner approximation to the kernel matrix inverse.}
        \label{fig:precond_effect}
\end{figure}
    

\section{Further work}
\label{appx:further}
As noted in \ref{sec:results}, we believe the bounds on at least the RFF method can be improved upon. Additionally, more extensive experiments over a wider hyperparameter space and more general kernel functions, as well as utilising the Bayesian decision process outlined in the paper, would provide more thorough support to the arguments. We also acknowledge the vast amount of literature dedicated to efficient implementations of various linear algebra routines (e.g. SVD and Cholesky) that could be utilised for our purposes, albeit with considerable effort to derive similar theoretical guarantees.

In running the experiments we made extensive use of the GPyTorch machinery, pushing it beyond its intended use; we wish to acknowledge that our application of CIQ is a `misuse' of the GPyTorch implementation as it was originally designed. As a result, we believe it would be beneficial to adjust it to be more compatible with this application, in order to facilitate further adoption of synthetic data for GP evaluation.
% As observed in the previous section, our application of CIQ is a `misuse' of the GPyTorch implementation as it was originally designed. As a result, we believe it would be beneficial to adjust it to be more compatible with this application, in order to facilitate further adoption of synthetic data for GP evaluation.
