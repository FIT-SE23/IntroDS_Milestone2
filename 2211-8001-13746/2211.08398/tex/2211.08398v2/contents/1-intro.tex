\vspace{-0.1cm}
\section{Introduction}
\label{sec:intro}

Recently, bird's-eye-view (BEV) based multi-camera perception frameworks have greatly narrowed the performance gap with LiDAR based methods for 3D object detection tasks~\cite{bevdepth,bevformer,detr3d,fcos3d}.
For example, compared with state-of-the-art LiDAR methods, some recent works have obtained NDS scores within a 10\% margin~\cite{bevdepth,li2022bevstereo}.

Such vision-centric BEV frameworks usually involve two stages: single view feature extraction using backbone networks (convnets~\cite{liu2022convnet} or transformers~\cite{transformer}), and information fusion across multiple camera views and multiple timestamps using transformers~\cite{bevformer,liu2022petr,liu2022petrv2} or the lift-splat-shoot paradigm~\cite{huang2021bevdet,huang2022bevdet4d,bevdepth}. Once a spatial-temporal coherent feature representation is obtained in the unified BEV space, 3D object detection and semantic segmentation~\cite{bevformer,huang2021bevdet,bevdepth} can be done on the BEV feature map with high accuracy. 

\begin{figure}[t!]
    \centering
    \includegraphics[width=7.25cm]{performance_overview.png}
    \vspace{-0.2cm}
    \caption{Experimental results on nuScenes. The area of circles indicate the number of parameters. Compared with BEVFormer-ResNeXt, our method achieves 0.72 NDS improvements, 2.74$\times$ acceleration and 4.97$\times$ compression. BEVFormer-ResNext\&R50 indicate teacher-2 and student-2 in Table~\ref{tab:stu_tea}, respectively.}
    \label{fig:performance}
    \vspace{-0.3cm}
\end{figure}


However, such performance improvements are achieved with a hefty computation overhead. For instance, 
the 120M parameters in BEVDet~\cite{huang2021bevdet} require more than 4 TFlops computation, which is almost 20$\times$ larger and 10$\times$ slower than CenterPoint~\cite{yin2021center}, a state-of-the-art LiDAR-based 3D detector.
Practical applications such as self-driving vehicles, usually have limited  computation budget but rather strict latency and accuracy requirements. Deployment of such visual BEV models onto edge devices requires a delicate balancing between low computation cost and high detection accuracy.
Compared with neural network pruning~\cite{deepcompression} and weight quantization~\cite{INQ,actication_quantization}, knowledge distillation (KD)~\cite{distill_hinton,model_compression} is more suited for striking such a balance.

Knowledge distillation is an effective model training technique that improves the performance of a lightweight student model by transferring the knowledge from a pre-trained but over-parameterized teacher model~\cite{model_compression,distill_hinton}.  At deployment time, only the lightweight student model is used.
While knowledge distillation has demonstrated great success in various 2D computer vision tasks, such as classification~\cite{selfdistillation}, object detection~\cite{kd_detection1,kd_detection2,detectiondistillation}, semantic segmentation~\cite{structured_kd,he2019knowledge}, and image generation~\cite{spkd_gan,omgd,distill_portable_gan,wkd}, 
% Following its success in image classification~\cite{selfdistillation}, abundant researchers have applied knowledge distillation to other vision tasks such as object detection~\cite{kd_detection1,kd_detection2,detectiondistillation}, semantic segmentation~\cite{structured_kd,he2019knowledge} and image generation~\cite{spkd_gan,omgd,distill_portable_gan,wkd} and achieved satisfactory performance. 
% However, most previous methods only focus on image-based vision tasks 
the application of knowledge distillation on 3D computer vision, especially the camera-based multi-view 3D detection, has not been well-studied.
However, it is also brought to our attention that simply applying traditional knowledge distillation methods to 3D vision tasks usually leads to limited performance gains.

To address the aforementioned problems, this paper proposes a novel knowledge distillation framework for visual BEV detection models. We start with analyzing the challenges in the multi-view 3D detection task and then propose the corresponding solution as follows:
\vspace{-0.4cm}
\paragraph{Information fusion from multiple positions:} 
In multi-view 3D detection, the detector takes input from multiple cameras across different timestamps to identify objects.
Hence, the student should be able to learn not only the information from single images but also how to fuse and leverage the information from multiple spatial/temporal positions.
To tackle this challenge, we propose spatial-temporal distillation, which improves student performance by allowing it to learn the semantic correspondence between inputs in different spatial (\emph{i.e.,} view) and temporal positions from their teachers.
Moreover, we also propose BEV response distillation, which aims to distill teacher response to different positions/pillars in the BEV feature map, which contains high level information on object localization.


\vspace{-0.4cm}
\paragraph{Discrepancies between the inputs:} The state-of-the-art BEV 3D detectors usually employ a DETR-like architecture, which utilizes self-attention and cross-attention layers for information fusion~\cite{detr,detr3d}. Different from traditional convolutional detectors, the input information of DETR-style detectors contains not only images but also trainable queries and positional encodings. 
Without explicit constraints, student and teacher models could have learned different positional encodings and queries after training. Knowledge distillation will be hindered by such discrepancies~\cite{beyer2022knowledge}.
To address this problem, we propose a weight-inheriting scheme which fixes the positional encodings and BEV queries of the student model to the corresponding values in the teacher detector. In this way, the student detector will benefit from the pre-trained weights of the teacher detector directly.
Surprisingly, we find that even without applying any knowledge distillation losses, simply using the weight-inheriting scheme can significantly improve the performance of knowledge distillation for this task.

Without loss of generality, we conduct extensive experiments on the nuScenes dataset~\cite{caesar2020nuscenes} using a representative and state-of-the-art BEVFormer model architecture~\cite{bevformer}. 
On average, 2.16 mAP and 2.27 NDS improvements can be observed across three different student-teacher settings, demonstrating the effectiveness of our proposed knowledge distillation framework. Compared with multiple baseline methods~\cite{distill_hinton,attentiondistillation,kd_comprehensive,relation_detection,relational_kd2,kd_variational,detectiondistillation,DBLP:conf/cvpr/Guo00W0X021}, our method outperforms them all by a large margin.% We provide detailed ablation analyses and visualization 

In summary, our contributions include:
%Extensive experiments on nuScene with BEVFormer, a typical and state-of-the-art multi-view 3D detector, have demonstrated the effectiveness of our method. On average, 2.15 mAP and 2.27 NDS improvements can be observed in three different student-teacher settings, which outperforms eight kinds of previous knowledge distillation methods by a large margin. In the discussion section, detailed ablation studies have been conducted to demonstrate the effectiveness of different modules in our methods. Besides, sufficient visualization results have been provided to understand the performance of our methods. To sum up, the contribution of this paper can be summarized as follows.
%\begin{itemize}
%  \setlength\itemsep{1em}
%\item We propose a novel spatial-temporal distillation scheme which enables the student detector to learn teacher knowledge on how to fuse information from different camera views and timestamps. 
%\vspace{-0.3cm}
%\item BEV response distillation is proposed to distill teacher response to different BEV pillars, which transfers teacher knowledge on object localization to the student.
%\vspace{-0.3cm}
%\item We identify the problem of inconsistent inputs in knowledge distillation on DETR-style detectors and propose a weight-inheriting scheme to solve it.
%\vspace{-0.3cm}
%\item Extensive experiments on nuScenes demonstrate the effectiveness of our method. On average, \textbf{2.16} mAP and \textbf{2.27} NDS improvements can be obtained compared with the student without knowledge distillation. Source code and models will be released to benefit the community.
%\end{itemize}

\noindent
(\textbf{1}) We propose a novel spatial-temporal distillation scheme which enables the student detector to learn teacher knowledge on how to fuse information from different camera views and timestamps. 

\noindent
(\textbf{2}) BEV response distillation is proposed to distill teacher response to different BEV pillars, which transfers teacher knowledge on object localization to the student.

\noindent
(\textbf{3}) We identify the problem of inconsistent inputs in knowledge distillation on DETR-style detectors and propose a weight-inheriting scheme to solve it.

\noindent
(\textbf{4}) Extensive experiments on nuScenes demonstrate the effectiveness of our method. On average, \textbf{2.16} mAP and \textbf{2.27} NDS improvements can be obtained compared with the student without knowledge distillation. Source code and models will be released to benefit the community.
% \end{itemize}

\begin{figure*}
    \centering
    \includegraphics[width=0.95\linewidth]{main_fig-eps-converted-to.pdf}
    \vspace{-0.25cm}
    \caption{The overall framework and details of our method. 
    (a) The proposed knowledge distillation methods mainly include weight-inheriting, spatial-temporal distillation, and BEV response distillation. Weight-inheriting fixes the parameters of BEV queries and positional encoding to their value in the pre-trained teacher detector during the whole training period to guarantee that students and teachers have the same inputs. (b) Spatial-temporal distillation aims to improve student performance on information fusion of images from multiple views and timestamps by transferring teacher knowledge in the attention weights in the temporal self-attention and spatial cross-attention layers. (c) BEV response distillation first computes the response of different positions in BEV map and then distills it to the students.
    }
    \vspace{-0.4cm}
    \label{fig:main_figure}
\end{figure*}