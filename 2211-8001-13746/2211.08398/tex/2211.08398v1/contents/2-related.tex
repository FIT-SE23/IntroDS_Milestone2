\vspace{-0.1cm}
\section{Related Work}
\label{sec:related_work}
\subsection{Knowledge Distillation}
Knowledge distillation, which aims to transfer knowledge from a cumbersome teacher model to a lightweight student model, has become one of the most popular model training techniques in both model compression and model performance boosting~\cite{distill_hinton,model_compression}. 
Knowledge distillation is originally proposed to train the student model to mimic the output probability distribution from teachers for image classification tasks~\cite{distill_hinton,deepmutuallearning}.
In addition to model outputs, intermediate features~\cite{fitnets,kd_crd}, attentions~\cite{attentiondistillation}, as well as relations~\cite{relational_kd,relational_kd2,fsp_kd,kd_cc}, all have been utilized for knowledge distillation.
%Then, abundant attempts have been made to distill teacher knowledge from the features~\cite{fitnets,kd_crd}, attention~\cite{attentiondistillation}, relation~
%\cite{relational_kd,relational_kd2,fsp_kd,kd_cc} perspective, 
Nowadays, knowledge distillation has successfully swept a wide range of applications, such as object detection~\cite{detectiondistillation,kd_detection4,kd_detection3}, semantic segmentation~\cite{structured_kd,he2019knowledge}, image generation~\cite{wkd}, self-supervised learning~\cite{kd_self1,kd_self2}, vision model pretraining~\cite{simclr,simclr2,moco}, language models~\cite{kd_bert1,kd_bert2}, data augmentation~\cite{labelrefine}, model robustness~\cite{kd_defense,auxiliarytraining} and so on.

\vspace{-0.4cm}
\paragraph{KD for 3D Detection}
Following the success of knowledge distillation on 2D tasks, a few works have been proposed to apply knowledge distillation to 3D object detection tasks.
For example, in the point cloud detection domain,
Zhang~\emph{et al.\@}~\cite{zhang2022pointdistiller} distills local information extracted by dynamic graph convolutions,
Cho~\emph{et al.\@}~\cite{cho2022itkd} distills the knowledge compressed by an AutoEncoder,
Yang~\emph{et~al.\@}~\cite{yang2022towards} propose to perform knowledge distillation only on the positions with high teacher classification responses,
%PointDistiller to compress point cloud-based detectors by distilling the local information extracted by dynamic graph convolution~\cite{zhang2022pointdistiller}. 
%Cho~\emph{et al.\@} proposed to distill the knowledge compressed by an AutoEncoder for point clouds-based 3D detection~\cite{cho2022itkd}.
%Yang~\emph{et~al.\@} proposed to perform knowledge distillation only on the positions with high teacher classification response~\cite{yang2022towards}.
Hou~\emph{et~al.\@}~\cite{hou2022point} propose to compress 3D segmentation by distilling the knowledge in both points and voxels,
Huang~\emph{et al.\@}~\cite{huang2022label} proposed label-guided auxiliary training which generates pseudo teacher features with label information.
In the vision based 3D detection domain,
Chong~\emph{et al.\@}~\cite{chong2022monodistill} improves the efficiency of monocular 3D detection with relation-based knowledge distillation.
%Besides, another popular research trend is to employ teachers and students in different modalities. For instance, 
Guo~\emph{et~al.\@} propose to learn stereo-based students from a LiDAR-based teacher~\cite{guo2021liga}. Sautier~\emph{et~al.\@} propose image-to-LiDAR self-supervised distillation which leverages the information of an image-based detector to improve the performance of 3D detector~\cite{sautier2022image}. 
Despite much success in these prior works, applying knowledge distillation to the modern multi-camera BEV detection paradigm has been rarely explored. We hope our attempts can inspire future research in this domain.
%Despite their success, unfortunately, specific knowledge distillation methods for BEV-based multi-view 3D detection has not been studied.
%Unfortunately, specific knowledge distillation methods for BEV-based multi-view 3D detection are still empty.


\vspace{-0.1cm}
\subsection{Camera-based 3D Detection}
Detecting 3D objects from images is one of the most challenging problems in 3D computer vision. 
With the help of modern deep learning techniques, much progress has been made for monocular 3D object detection~\cite{3d_object_1,3d_object_2,3d_object_3,fcos3d}.
However, monocular 3D object detection often suffer from truncation and occlusion problems, which are quite challenging without the help of additional sensors.
Detecting 3D objects using multi-cameras in the Birds'-Eye-View (BEV) thus have become quite popular recently.
% Mousavian~\emph{et al.\@} propose to detect 3D objects from a single image by first predicting the 2D bounding box and then estimating its 3D attributes~\cite{3d_object_2}.
% Xu~\emph{et al.\@} introduce a multi-level fusion scheme that uses a stand-alone module for disparity estimation~\cite{3d_object_3}.
% Wang~\emph{et al.\@} extend FCOS to monocular 3D object detection by transforming the 7-DoF 3D targets to the image domain~\cite{fcos3d}.
%Another rising topic in camera-based 3D detection is to predict 3D objects in Birds'-Eye-View (BEV) from multi-view images.
Huang~\emph{et~al.\@} propose BEVDet which encodes the feature of single views and then projects them to BEV space~\cite{huang2022bevdet4d,huang2021bevdet}.
Wang~\emph{et al.} propose Detr3D which manipulates predictions directly in 3D space by using 3D object queries to index the features of multi-view images~\cite{detr3d}.
PETR and PETRv2 are proposed to produce 3D position-aware features by encoding the position information of 3D coordinates into image features~\cite{liu2022petr,liu2022petrv2}.
%Then, PETRv2 is further introduced to leverage the temporal information by extending 3D positional embedding to temporal modeling~\cite{}.
Li~\emph{et al.} propose BEVFormer, which employs spatial cross-attention and temporal self-attention to merge the information of features in different spatial and temporal positions~\cite{bevformer}.
Then, BEVDepth is proposed to perform explicit depth supervision with encoded intrinsic and extrinsic parameters~\cite{bevdepth}.
Observing the fact that the optimal temporal difference between views varies significantly for different pixels and depths,
Park~\emph{et~al.} propose SOLOFusion which employs both short-term and long-term temporal stereo for depth estimation~\cite{park2022time}.
Li~\emph{et al.} propose BEVStereo which dynamically selects the scale of matching candidates to reduce the computation overhead~\cite{li2022bevstereo}. 
Following such trend, we focus on designing effective knowledge distillation strategies to push the envelope of such multi-camera BEV object detection paradigm.