\section{Conclusion}
\label{sec:conclusion}
Most advanced multi-view 3D detectors suffer from low inference efficiency, which has limited their applications in edge devices. To address this problem, we propose a series of knowledge distillation methods to achieve model compression, which includes (1) spatial-temporal distillation which allows the student to learn how to fuse information from different timestamps and views (2) BEV response distillation which enables the student to learn the localization-aware knowledge, and (3) a weight-inheriting scheme which fixes the BEV queries and positional encoding to guarantee that students and teachers have the same inputs. Extensive comparison experiments with 8 previous methods and sufficient ablation studies demonstrate the significant performance of our method in three different student-teacher settings. On average, 2.16 mAP and 2.27 NDS improvements can be observed on the nuScenes dataset. Besides, detailed ablation studies and visualization have demonstrated the performance of our method from different perspectives. We hope that this paper may promote more research on efficient multi-view 3D detection.

\vspace{-0.4cm}
\paragraph{Limitations}
We tailor our proposed framework for transformer-based multi-camera BEV detection models.
Adaptations must be made to accommodate the popular lift-splat-shoot style BEV detection models\cite{huang2021bevdet,huang2022bevdet4d,bevdepth}.
%The method we proposed is limited by the architecture of BEV model, which is more suitable for transformer-based network rather than those 2D-3D paradigms based on predicted depth. More details please refer to supplementary materials.














