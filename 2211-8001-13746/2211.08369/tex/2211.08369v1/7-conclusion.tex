\section{Conclusion}
\label{sec:conclusion}

We analysed two prototypical models from different families in \textsc{jwa} and \textsc{dbert} with the goal of finding out the cause of low agreement between saliency method intepretations.
We first took a closer look at the previously used rank-order correlation metric and demonstrated that it is prone to exhibiting a high difference in agreement for small changes in importance scores. As an alternative, we proposed a linear correlation metric which is robust to small importance perturbations and demonstrated that it exhibits consistently higher agreement scores.
Taking a step further, we applied two regularization techniques, \textsc{tying} and \textsc{conicity}, originally aimed at increasing faithfulness of attention explanations, with the hypothesis that the issue underpinning disagreements and unfaithfulness is the same -- representation entanglement in the hidden space.
We showed that regularization consistently and significantly improves agreement scores across all models and datasets with minimal penalty for classification performance.
Having demonstrated that it is possible to improve upon the low agreement scores, we attempted to offer intuition on which instance categories saliency methods agree the least and show that surprisingly, \textit{easy-to-learn} instances are \textit{hard-to-agree} on.
Lastly, we offered insights into how the representation space morphs when regularization is applied and linked these findings with dataset cartography categories, paving way for further work on understanding what properties of neural models affect interpretability.
