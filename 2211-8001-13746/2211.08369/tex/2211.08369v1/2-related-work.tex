
\section{Background and Related Work}
\label{sec:rw}

Explainability methods come in different flavors determined by the method of computing feature importance scores.
Saliency methods perform \textit{post-hoc} analysis of the trained black-box model by either leveraging gradient information \cite{denil2014extraction,sundararajan2017axiomatic}, modifying the backpropagation rules \cite{bach2015pixel,shrikumar2017learning}, or training a shallow interpretable model to locally approximate behavior of the black-box model \cite{ribeiro2016should}, all with the goal of assigning scalar saliency scores to input features.
Alternatively, if the analysed model is capable of generating text, one can resort to self-rationalization by prompting the trained model to generate an explanation for its decision \cite{marasovic2021few}.
In contrast to \textit{post-hoc} explanations, \textit{inherently interpretable} models produce explanations as part of their decision process, either by masking a proportion of input tokens and then performing prediction based on the remaining \textit{rationale} \cite{lei-etal-2016-rationalizing,bastings2019interpretable,jain2020learning} or jointly performing prediction and rationale generation in cases where datasets with annotated rationales are available \cite{camburu2018snli}.
For some time, the attention mechanism \cite{bahdanau2014neural} has also been considered as inherently interpretable. However, the jury is still out on whether such explanations can be considered faithful  \cite{jain2019attention,wiegreffe2019attention,tutek2020staying,bastings2020elephant}.



\textit{Faithfulness} is one of the most important desideratum of explanation methods \cite{jacovi-goldberg-2020-towards} -- faithful explanations are those that are true to the inner decision making process of the model.
Approaches to evaluating faithfulness rely on measuring how confidence of the model changes when inputs are perturbed \cite{kindermans2019reliability} or completely dropped from the model \cite{li2016understanding,serrano2019attention}.
However, perturbations to input often result in corrupted instances that fall off the data manifold \cite{hooker2019benchmark} and also appear nonsensical to humans \cite{feng2018pathologies} -- raising questions whether such evaluation is valid.
Another option is to leverage the \textit{evaluation-by-agreement} \cite{jain2019attention} paradigm, which states that an interpretability method should be highly agreeable with other methods in order to be considered faithful.
However, since empirical evidence has shown that saliency methods exhibit poor agreement between their explanations \cite{neely2021order}, \citet{atanasova2020diagnostic} recommend practitioners consider alternative methods for evaluating the quality of interpretability methods, such as diagnostic tests.