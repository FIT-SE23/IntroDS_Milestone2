
\section{Introduction}
\label{sec:intro}

Following the meteoric rise of popularity of neural NLP models during the neural revolution, they have found practical usage across a plethora of domains and tasks.
However, in a number of high-stakes domains such as law \citep{kehl2017algorithms}, finance \citep{grath2018interpretable}, and medicine \citep{caruana2015intelligible}, the opacity of deep learning methods needs to be addressed.
In the area of explainable artificial intelligence (XAI), one of major recent efforts is to unveil the neural black box and produce explanations for the end-user.
There are various approaches to rationalizing model predictions, such as using the attention mechanism \cite{bahdanau2014neural}, saliency methods \cite{denil2014extraction,bach2015pixel,ribeiro2016should,lundberg2016unexpected,shrikumar2017learning,sundararajan2017axiomatic}, rationale generation by-design \cite{lei-etal-2016-rationalizing,bastings2019interpretable,jain2020learning}, or self-rationalizing models \cite{marasovic2021few}.
These methods have to simultaneously satisfy numerous desiderata to have practical application in high-stakes scenarios: they have to be \textit{faithful} -- an accurate representation of the inner reasoning process of the model, and \textit{plausible} -- convincing to human stakeholders.

When evaluating faithfulness of using attention as explanations, \citet{jain2019attention} have shown that attention importance scores do not correlate well with gradient-based measures of feature importance.
The authors state that although gradient-based measures of feature importance should not be taken as ground truth, one would still expect importance measures to be highly agreeable, bringing forth the \textit{agrement-as-evaluation} paradigm \cite{abnar2020quantifying,meister2021sparse}.
While imperfect agreement is something one could expect as interpretability methods differ in their formulation and it is reasonable to observe differences in importance scores, subsequent work has shown that saliency methods exibit low agreement scores even when applied to the \textit{same model instance} \cite{neely2021order}.
Since a single trained model instance can only have a single feature importance ranking for its decision, disagreement of saliency methods implies that at least one, if not all methods, do not produce faithful explanations -- placing doubt on their practical relevance.

It has been hypothesized that unfaithfulness of attention is caused by input entanglement in the hidden space \cite{jain2019attention}.
This claim has later been experimentally confirmed through results showing that regularization techniques targeted to reduce entanglement significantly improve faithfulness of attention-based explanations \cite{mohankumar2020towards,tutek2020staying}.
While entanglement in the hidden space is clearly a problem in case of attention, where attention weights directly pertain to hidden states, we hypothesize that representation entanglement could also cause similar issues for saliency methods -- which might not be able to adequately disentangle importance when propagating toward the inputs.

In our work, we first take a closer look at whether rank-correlation is an appropriate method for evaluating agreement and confirm that, as hypothesized in previous work, small differences in values of saliency scores significantly affect agreement scores.
We argue that a linear correlation method such as Pearson-r is a better motivated choice, since the exact ranking order of features is not as crucial for agreement as the relative importance values, which are adequately captured by Pearson-r.
We hypothesize that the cause of saliency method disagreements is rooted in representation entanglement and experimentally show that agreement can be significantly improved by regularization techniques such as tying \cite{tutek2020staying} and conicity \cite{mohankumar2020towards}.
The fact that regularization methods, which were originally aimed at improving faithfulness of attention, also improve agreement between saliency methods suggests that the two problems have the same underlying cause.
Taking the analysis deeper, we apply techniques from dataset cartography \cite{swayamdipta2020dataset} and show that, surprisingly, instances that are easy for models to learn have lower agreement when compared to ambiguous instances.
We further analyze how local curvature of the representation space morphs when regularization techniques are applied, paving way for further analysis of (dis)agreements between interpretability methods. 



