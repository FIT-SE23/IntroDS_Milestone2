\begin{abstract}
A popular approach to unveiling the black box of neural NLP models is to leverage saliency methods, which assign scalar importance scores to each input component.
A common practice for evaluating whether an interpretability method is \textit{faithful} and \textit{plausible} has been to use evaluation-by-agreement -- multiple methods agreeing on an explanation increases its credibility. %
However, recent work has found that even saliency methods have weak rank correlations and advocated for the use of alternative diagnostic methods. %
In our work, we demonstrate that rank correlation is not a good fit for evaluating agreement and argue that Pearson-$r$ is a better suited alternative. %
We show that regularization techniques that increase faithfulness of attention explanations also increase agreement between saliency methods. %
Through connecting our findings to instance categories based on training dynamics we show that, surprisingly, easy-to-learn instances exhibit low agreement in saliency method explanations.
\end{abstract}