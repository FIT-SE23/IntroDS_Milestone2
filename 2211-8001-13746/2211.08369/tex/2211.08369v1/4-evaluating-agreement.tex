\section{Improving Agreement}
\label{sect:impr-agreement}
In this section we present two modifications of the existing \textit{evaluation-by-agreement} procedure: (1) substituting rank-correlation with a linear correlation measure, which is more robust to rank changes caused by small differences in importance weights, and (2) regularizing the models with the goal of reducing entanglement in the hidden space, and as a consequence, improving agreement. 

\subsection{Choice of Correlation Metric}
\label{sub:corr-metric}
Previous work \cite{jain2019attention,neely2021order} has evaluated agreement between two explainability methods by using rank-correlation as measured by Kendall-$\tau$ \cite{kendall1938new}.
Although Kendall-$\tau$ is generally more robust than Spearman's rank correlation, i.e., it has smaller gross-error sensitivity \cite{croux2010influence}, we still face difficulties when using Kendall-$\tau$ for evaluating agreement.
As \citet{jain2019attention} also note, perturbations in ranks assigned to tokens in the tail of the saliency distribution have a large influence on the agreement score.
In addition, rankings are also unstable when saliency scores for the most relevant tokens are close to one another.
In \Cref{fig:kendall_problem}, we illustrate the problem with using rank correlation on a toy example of explaining sentiment classification.
While saliency scores attributed to tokens differ slightly, the differences in rank order are significant, lowering agreement according to Kendall-$\tau$ due to the discretization of raw saliency scores when converted into ranks.
We believe that a better approach to approximating agreement is to use a linear correlation metric such as Pearson's $r$, as it evaluates whether both saliency methods assign similar importance scores to the same tokens -- which is a more robust setup if we assume small amounts of noise in importance attribution between different methods.
\begin{table}[t!]
\begin{subtable}{.48\textwidth}
\centering
\small
\begin{tabular}{lrrrrrrr}
\toprule
& & \multicolumn{2}{c}{D-SHAP} & \multicolumn{2}{c}{G-SHAP} & \multicolumn{2}{c}{Int-Grad} \\
\cmidrule(lr){3-4} \cmidrule(lr){5-6} \cmidrule(lr){7-8}
& & $k_\tau$ & $p_r$ & $k_\tau$ & $p_r$ & $k_\tau$ & $p_r$ \\
\midrule
\multirow{4}{*}{\rotatebox[origin=c]{90}{DeepLIFT}}
& SUBJ & $\mathbf{1.}$ & $\mathbf{1.}$ & $.31$ & $\mathbf{.45}$ & $.43$ & $\mathbf{.64}$ \\
& SST & $\mathbf{1.}$ & $\mathbf{1.}$ & $.30$ & $\mathbf{.47}$ & $.35$  & $\mathbf{.54}$ \\
& TREC & $\mathbf{1.}$ & $\mathbf{1.}$ & $.12$ & $\mathbf{.31}$ & $.15$ & $\mathbf{.33}$ \\
& IMDB & $\mathbf{1.}$  & $\mathbf{1.}$ & $.29$  & $\mathbf{.59}$ & $.28$ & $\mathbf{.60}$ \\
\midrule
\multirow{4}{*}{\rotatebox[origin=c]{90}{D-SHAP}}
& SUBJ & & & $.31$ & $\mathbf{.45}$ & $.43$ & $\mathbf{.64}$ \\
& SST & & & $.30$ & $\mathbf{.47}$ & $.35$ & $\mathbf{.54}$ \\
& TREC & & & $.12$ & $\mathbf{.31}$ & $.15$ & $\mathbf{.33}$ \\
& IMDB & & & $.29$ & $\mathbf{.60}$ & $.28$ & $\mathbf{.60}$ \\
\midrule
\multirow{4}{*}{\rotatebox[origin=c]{90}{G-SHAP}}
& SUBJ & & & & & $.62$ & $\mathbf{.78}$ \\
& SST & & & & & $.70$ & $\mathbf{.87}$ \\
& TREC & & & & & $.66$ & $\mathbf{.85}$ \\
& IMDB & & & & & $.68$ & $\mathbf{.94}$ \\
\bottomrule
\end{tabular}
\subcaption{\textsc{jwa}}
\end{subtable}


\bigskip

\begin{subtable}{.48\textwidth}
\centering
\small
\begin{tabular}{lrrrrrrr}
\toprule
& & \multicolumn{2}{c}{D-SHAP} & \multicolumn{2}{c}{G-SHAP} & \multicolumn{2}{c}{Int-Grad} \\
\cmidrule(lr){3-4} \cmidrule(lr){5-6} \cmidrule(lr){7-8}
& & $k_\tau$ & $p_r$ & $k_\tau$ & $p_r$ & $k_\tau$ & $p_r$ \\
\midrule
\multirow{4}{*}{\rotatebox[origin=c]{90}{DeepLIFT}}
& SUBJ & $.24$ & $\mathbf{.44}$ & $.10$ & $\mathbf{.19}$ & $.12$ & $\mathbf{.21}$ \\
& SST & $.19$ & $\mathbf{.34}$ & $.09$ & $\mathbf{.17}$ & $.10$  & $\mathbf{.20}$ \\
& TREC & $.16$ & $\mathbf{.30}$ & $.12$ & $\mathbf{.25}$ & $.12$ & $\mathbf{.26}$ \\
& IMDB & $.28$  & $\mathbf{.51}$ & $.11$  & $\mathbf{.24}$ & $.13$ & $\mathbf{.27}$ \\
\midrule
\multirow{4}{*}{\rotatebox[origin=c]{90}{D-SHAP}}
& SUBJ & & & $.11$ & $\mathbf{.22}$ & $.13$ & $\mathbf{.24}$ \\
& SST & & & $.10$ & $\mathbf{.19}$ & $.11$ & $\mathbf{.23}$ \\
& TREC & & & $.13$ & $\mathbf{.28}$ & $.14$ & $\mathbf{.30}$ \\
& IMDB & & & $.12$ & $\mathbf{.26}$ & $.14$ & $\mathbf{.30}$ \\
\midrule
\multirow{4}{*}{\rotatebox[origin=c]{90}{G-SHAP}}
& SUBJ & & & & & $.36$ & $\mathbf{.58}$ \\ & SST & & & & & $.31$ & $\mathbf{.54}$ \\
& TREC & & & & & $.42$ & $\mathbf{.71}$ \\ & IMDB & & & & & $.29$ & $\mathbf{.55}$ \\
\bottomrule
\end{tabular}
\subcaption{\textsc{dbert}}
\end{subtable}


\caption{Agreement between pairs of saliency methods in terms of Kendall-$\tau$ ($k_\tau$) and Pearson-$r$ ($p_r$) for \textsc{base} variants of (a) \textsc{jwa} and (b) \textsc{dbert}. We average the agreement over five runs with different seeds. D-SHAP and G-SHAP denote Deep-SHAP and Grad-SHAP, respectively. The values in bold indicate which agreement is higher between the two metrics.}
\label{tab:agreement}

\end{table}




We now evaluate how Pearson's $r$ ($p_r$) compares to Kendall-$\tau$ ($k_\tau$) when evaluating agreement. 
In \Cref{tab:agreement} we compare agreement scores produced by $p_r$ and $k_\tau$ across all datasets for \textsc{jwa} and \textsc{dbert}, respectively.
Across all datasets and models, we observe consistently higher agreement values for $p_r$, which we take as indication that part of the reason for low agreement scores are minor differences between saliency scores on tokens with approximately same relative importance produced different rankings, which were harshly penalized by $k_\tau$.
We argue that the higher correlation scores reported by the Pearson correlation coefficient are a better estimate for agreement between saliency scores and we advocate for its use rather than rank correlation.


\begin{figure}[t!]
\centering
\includegraphics[width=\linewidth]{img/kendall2.pdf}
\caption{A toy example of sentiment classification illustrating the problems with Kendall-$\tau$. The corresponding agreement on the shown example is $k_\tau = -.43$ and $p_r = .99$. Token opacity indicates higher saliency score, i.e., token relevance, written out at the top of the token box. Each of the two explainability methods A and B outputs its saliency scores. The value of Kendall-$\tau$ is much lower than Pearson's correlation, because the irrelevant tokens are perturbed, despite the fact that the tokens are correctly partitioned into more important and less important ones.}
\label{fig:kendall_problem}
\end{figure}


\subsection{Regularizing Models}
\label{sub:reg-models}


\begin{table}
\small
\centering
\begin{tabular}{lrrrrrrr}
\toprule
& & \multicolumn{3}{c}{$\tau_k$} & \multicolumn{3}{c}{$r_p$} \\
\cmidrule(lr){3-5} \cmidrule(lr){6-8} \cmidrule(lr){7-8}
& & B & C & T & B & C & T \\
\midrule
\multirow{4}{*}{\rotatebox[origin=c]{90}{\textsc{jwa}}}
& SUBJ & $.52$ & $.48$ & $\mathbf{.65}$\nospacetext{$^\dagger$} & $.66$ & $.70$ & $\mathbf{.88}$\nospacetext{$^\dagger$}  \\
& SST & $.50$ & $.67$ & $\mathbf{.68}$ & $.65$ & $\mathbf{.90}$\nospacetext{$^\dagger$} & $.86$ \\
& TREC & $.37$ & $\mathbf{.77}$\nospacetext{$^\dagger$} & $.68$ & $.52$  & $\mathbf{.98}$\nospacetext{$^\dagger$} & $.93$ \\
& IMDB & $.47$ & $.52$ & $\mathbf{.60}$\nospacetext{$^\dagger$} & $.72$  & $.64$ & $\mathbf{.80}$\nospacetext{$^\dagger$} \\
\midrule
\multirow{4}{*}{\rotatebox[origin=c]{90}{\textsc{dbert}}}
& SUBJ & $.18$ & $.28$ & $\mathbf{.36}$\nospacetext{$^\dagger$} & $.31$ & $.48$ & $\mathbf{.57}$\nospacetext{$^\dagger$} \\
& SST & $.15$ & $.15$ & $\mathbf{.33}$\nospacetext{$^\dagger$} & $.28$ & $.27$ & $\mathbf{.60}$\nospacetext{$^\dagger$} \\
& TREC & $.18$ & $.17$ & $\mathbf{.28}$\nospacetext{$^\dagger$} & $.35$  & $.34$ & $\mathbf{.53}$\nospacetext{$^\dagger$} \\
& IMDB & $.18$ & $.20$ & $\mathbf{.24}$\nospacetext{$^\dagger$} & $.36$ & $.42$ & $\mathbf{.51}$\nospacetext{$^\dagger$} \\
\bottomrule
\end{tabular}
\caption{Average agreement over all pairs of saliency methods. We report agreement in terms of Kendall-$\tau$ ($k_\tau$) and Pearson-$r$ ($r_p$) from the last epoch for three model variants: B -- base, C -- conicity, and T -- tying. \textbf{Bold} numbers indicate highest agreement among the three model flavors. Results are averages over $5$ runs with different seeds. We ran one-sided Wilcoxon signed-rank tests to check for statistical significance. Agreement values significantly higher ($p < .05$) than the other variants in two individual Wilcoxon tests are marked with a $\dagger$. We adjusted the $p$-values for family-wise error rate due to multiple tests using the Holm-Bonferroni method.}
\label{tab:avg_agr}
\end{table}

\begin{table}[t!]
\small
\centering
\begin{tabular}{lrrrr}
\toprule
& & Base & Conicity & Tying \\
\midrule
\multirow{4}{*}{\rotatebox[origin=c]{90}{\textsc{dbert}}}
& SUBJ & $.93_{.01}$ & $.90_{.02}$ & $.93_{.00}$ \\
& SST & $.83_{.00}$ & $.83_{.01}$ & $.82_{.01}$ \\
& TREC & $.92_{.01}$ & $.92_{.01}$ & $.91_{.01}$ \\
& IMDB & $.86_{.01}$ & $.86_{.01}$ & $.88_{.00}$ \\
\midrule
\multirow{4}{*}{\rotatebox[origin=c]{90}{\textsc{jwa}}}
& SUBJ & $.92_{.00}$ & $.90_{.00}$ & $.89_{.00}$ \\
& SST & $.78_{.04}$ & $.76_{.02}$ & $.78_{.02}$ \\
& TREC & $.89_{.02}$ & $.86_{.01}$ & $.89_{.01}$ \\
& IMDB & $.89_{.00}$ & $.88_{.00}$ & $.86_{.00}$ \\
\bottomrule
\end{tabular}
\caption{$F_1$ scores on \textit{test} sets across datasets for \textsc{dbert} and \textsc{jwa}. We report the test results on epochs in which the model had the best performance on the validation set. Columns correspond to the base and regularized models. Numbers in subscript denote standard deviation on $5$ runs with different seeds.}
\label{tab:f1_scores}
\end{table}

Our next goal is to improve agreement between saliency methods through intervention in the training procedure, namely by applying regularization aimed at promoting disentanglement in the hidden space. 
In \Cref{tab:avg_agr} we report correlation scores on the test splits of all datasets for regularized models (\textsc{conicity}, \textsc{tying}) and their unregularized variants (\textsc{base}).
We notice that both regularization techniques have a positive effect on agreement across both correlation metrics, indicating that regularization techniques alleviate a deeper issue that also affects interpretability of attention weights. 
In \Cref{tab:f1_scores} we report $F_1$ scores on the test set for the regularized and unregularized models with best performance on the validation split.
We observe that in general, regularized models perform comparably well to unregularized ones on downstream tasks, indicating that the improvement in agreement does not come at a cost for downstream performance.
When selecting regularized models, we choose ones with the strongest regularization scale hyperparameter that does not perform more than $3$ $F_1$ points worse on the validation set compared to the unregularized model (cf.~details in \Cref{subsec:A_hs}). %
