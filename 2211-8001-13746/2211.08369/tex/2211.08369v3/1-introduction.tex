
\section{Introduction}
\label{sec:intro}

% Intro: NN popularity
Following the meteoric rise of the popularity of neural NLP models during the neural revolution, they have found practical usage across a plethora of domains and tasks.
% Problem of trust
However, in a number of high-stakes domains such as law \citep{kehl2017algorithms}, finance \citep{grath2018interpretable}, and medicine \citep{caruana2015intelligible}, the opacity of deep learning methods needs to be addressed.
% Goals of XAI
In the area of explainable artificial intelligence (XAI), one of the major recent efforts is to unveil the neural black box and produce explanations for the end-user.
% Explanation methods
There are various approaches to rationalizing model predictions, such as using the attention mechanism \cite{bahdanau2014neural}, saliency methods \cite{denil2014extraction,bach2015pixel,ribeiro2016should,lundberg2016unexpected,shrikumar2017learning,sundararajan2017axiomatic}, rationale generation by-design \cite{lei-etal-2016-rationalizing,bastings2019interpretable,jain2020learning}, or self-rationalizing models \cite{marasovic2021few}.
% Faithfulness + plausibility
These methods have to simultaneously satisfy numerous desiderata to have practical application in high-stakes scenarios: they have to be \textit{faithful} -- an accurate representation of the inner reasoning process of the model, and \textit{plausible} -- convincing to human stakeholders.

% Introduce agreement as evaluation
When evaluating faithfulness in using attention as explanations, \citet{jain2019attention} have shown that attention importance scores do not correlate well with gradient-based measures of feature importance.
The authors state that although gradient-based measures of feature importance should not be taken as ground truth, one would still expect importance measures to be highly agreeable, bringing forth the \textit{agrement-as-evaluation} paradigm \cite{abnar2020quantifying,meister2021sparse}.
% Saliency methods don't disagree well between themselves
While the imperfect agreement is something one could expect as interpretability methods differ in their formulation, and it is reasonable to observe differences in importance scores, subsequent work has shown that saliency methods exhibit low agreement scores even when applied to the \textit{same model instance} \cite{neely2021order}.
Since a single trained model instance can only have a single feature importance ranking for its decision, disagreement of saliency methods implies that at least one, if not all methods, do not produce faithful explanations -- placing doubt on their practical relevance.
It has been hypothesized that unfaithfulness of attention is caused by input entanglement in the hidden space \cite{jain2019attention}.
This claim has later been experimentally verified through results showing that regularization techniques targeted to reduce entanglement significantly improve the faithfulness of attention-based explanations \cite{mohankumar2020towards,tutek2020staying}.
While entanglement in the hidden space is clearly a problem in the case of attention explanations, where attention weights directly pertain to hidden states, we also hypothesize that representation entanglement could cause similar issues for gradient- and propagation-based explainability methods -- which might not be able to adequately disentangle importance when propagating toward the inputs.

% Rank correlation
In our work, we first take a closer look at whether the rank correlation is an appropriate method for evaluating agreement and confirm that, as hypothesized in previous work, small differences in values of saliency scores significantly affect agreement scores.
We argue that a linear correlation method such as Pearson-$r$ is a better-motivated choice since the exact ranking order of features is not as crucial for agreement as the relative importance values, which Pearson-$r$ adequately captures.
% Improving agreement through regularization
We hypothesize that the cause of saliency method disagreements is rooted in representation entanglement and experimentally show that agreement can be significantly improved by regularization techniques such as tying \cite{tutek2020staying} and conicity \cite{mohankumar2020towards}.
The fact that regularization methods, which were originally aimed at improving faithfulness of attention, also improve agreement between saliency methods suggests that the two problems have the same underlying cause.
Taking the analysis deeper, we apply techniques from dataset cartography \cite{swayamdipta2020dataset} and show that, surprisingly, the explanations of easy-to-learn instances exhibit a lower agreement than of ambiguous instances.
We further analyze how local curvature of the representation space morphs when regularization techniques are applied, paving the way for further analysis of (dis)agreements between interpretability methods. 