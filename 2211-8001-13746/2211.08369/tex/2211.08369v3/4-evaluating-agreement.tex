\section{Improving Agreement}
\label{sect:impr-agreement}
In this section, we present two modifications of the existing \textit{evaluation-by-agreement} procedure: (1) substituting rank-correlation with a linear correlation measure, which is more robust to rank changes caused by small differences in importance weights, and (2) regularizing the models with the goal of reducing entanglement in the hidden space, and as a consequence, improving agreement. 

\subsection{Choice of Correlation Metric}
\label{sub:corr-metric}
% Prev work uses K-T
Previous work \cite{jain2019attention,neely2021order} has evaluated the agreement between two explainability methods by using rank-correlation as measured by Kendall-$\tau$ \cite{kendall1938new}.
% Why we consider Kendall Tau as bad
Although Kendall-$\tau$ is generally more robust than Spearman's rank correlation, i.e., it has smaller gross-error sensitivity \cite{croux2010influence}, we still face difficulties when using Kendall-$\tau$ for evaluating agreement.
% saliency methods often assign high relevance to observations in
As \citet{jain2019attention} also note, perturbations in ranks assigned to tokens in the tail of the saliency distribution have a large influence on the agreement score.
% the tails of the distribution that have a large influence relative to their rank values, leading to underestimation of agreement when using rank correlation.
In addition, rankings are also unstable when saliency scores for the most relevant tokens are close to one another.
% Thus, Kendall-$\tau$ can be low even when the same set of most relevant tokens is identified, but with discordant saliency scores. 
In \Cref{fig:kendall_problem}, we illustrate the deficiencies of using rank correlation on a toy example of explaining sentiment classification.
While saliency scores attributed to tokens differ slightly, the differences in rank order are significant, lowering agreement according to Kendall-$\tau$ due to the discretization of raw saliency scores when converted into ranks.
% Alternative (TODO: JSD, distr. metric)
We believe that a better approach to approximating agreement is to use a linear correlation metric such as Pearson's $r$, as it evaluates whether both saliency methods assign similar importance scores to the same tokens -- which is a more robust setup if we assume small amounts of noise in importance attribution between different methods.
%
% checks whether both saliency methods assign approximately same importance values to the same tokens and bypasses the ranking problems which might originate from small differences in token importance.
% \todo{Check usage of divergence metrics}
% Some work has explored leveraging divergence metrics to compute similarity between saliency score distributions, such as Jensen-Shannon divergence \todo{add ref; argument against using jsd (we obtain const values)}. 
% Reporting results that show pearson > Kendall tau
%
\begin{table}[t!]
\begin{subtable}{.48\textwidth}
\centering
\small
\begin{tabular}{lrrrrrrr}
\toprule
& & \multicolumn{2}{c}{D-SHAP} & \multicolumn{2}{c}{G-SHAP} & \multicolumn{2}{c}{Int-Grad} \\
\cmidrule(lr){3-4} \cmidrule(lr){5-6} \cmidrule(lr){7-8}
& & $k_\tau$ & $p_r$ & $k_\tau$ & $p_r$ & $k_\tau$ & $p_r$ \\
\midrule
\multirow{4}{*}{\rotatebox[origin=c]{90}{DeepLIFT}}
& SUBJ & $\mathbf{1.}$ & $\mathbf{1.}$ & $.31$ & $\mathbf{.45}$ & $.43$ & $\mathbf{.64}$ \\
& SST & $\mathbf{1.}$ & $\mathbf{1.}$ & $.30$ & $\mathbf{.47}$ & $.35$  & $\mathbf{.54}$ \\
& TREC & $\mathbf{1.}$ & $\mathbf{1.}$ & $.12$ & $\mathbf{.31}$ & $.15$ & $\mathbf{.33}$ \\
& IMDB & $\mathbf{1.}$  & $\mathbf{1.}$ & $.29$  & $\mathbf{.59}$ & $.28$ & $\mathbf{.60}$ \\
\midrule
\multirow{4}{*}{\rotatebox[origin=c]{90}{D-SHAP}}
& SUBJ & & & $.31$ & $\mathbf{.45}$ & $.43$ & $\mathbf{.64}$ \\
& SST & & & $.30$ & $\mathbf{.47}$ & $.35$ & $\mathbf{.54}$ \\
& TREC & & & $.12$ & $\mathbf{.31}$ & $.15$ & $\mathbf{.33}$ \\
& IMDB & & & $.29$ & $\mathbf{.60}$ & $.28$ & $\mathbf{.60}$ \\
\midrule
\multirow{4}{*}{\rotatebox[origin=c]{90}{G-SHAP}}
& SUBJ & & & & & $.62$ & $\mathbf{.78}$ \\
& SST & & & & & $.70$ & $\mathbf{.87}$ \\
& TREC & & & & & $.66$ & $\mathbf{.85}$ \\
& IMDB & & & & & $.68$ & $\mathbf{.94}$ \\
\bottomrule
\end{tabular}
\subcaption{\textsc{jwa}}
\end{subtable}


\bigskip

\begin{subtable}{.48\textwidth}
\centering
\small
\begin{tabular}{lrrrrrrr}
\toprule
& & \multicolumn{2}{c}{D-SHAP} & \multicolumn{2}{c}{G-SHAP} & \multicolumn{2}{c}{Int-Grad} \\
\cmidrule(lr){3-4} \cmidrule(lr){5-6} \cmidrule(lr){7-8}
& & $k_\tau$ & $p_r$ & $k_\tau$ & $p_r$ & $k_\tau$ & $p_r$ \\
\midrule
\multirow{4}{*}{\rotatebox[origin=c]{90}{DeepLIFT}}
& SUBJ & $.24$ & $\mathbf{.44}$ & $.10$ & $\mathbf{.19}$ & $.12$ & $\mathbf{.21}$ \\
& SST & $.19$ & $\mathbf{.34}$ & $.09$ & $\mathbf{.17}$ & $.10$  & $\mathbf{.20}$ \\
& TREC & $.16$ & $\mathbf{.30}$ & $.12$ & $\mathbf{.25}$ & $.12$ & $\mathbf{.26}$ \\
& IMDB & $.28$  & $\mathbf{.51}$ & $.11$  & $\mathbf{.24}$ & $.13$ & $\mathbf{.27}$ \\
\midrule
\multirow{4}{*}{\rotatebox[origin=c]{90}{D-SHAP}}
& SUBJ & & & $.11$ & $\mathbf{.22}$ & $.13$ & $\mathbf{.24}$ \\
& SST & & & $.10$ & $\mathbf{.19}$ & $.11$ & $\mathbf{.23}$ \\
& TREC & & & $.13$ & $\mathbf{.28}$ & $.14$ & $\mathbf{.30}$ \\
& IMDB & & & $.12$ & $\mathbf{.26}$ & $.14$ & $\mathbf{.30}$ \\
\midrule
\multirow{4}{*}{\rotatebox[origin=c]{90}{G-SHAP}}
& SUBJ & & & & & $.36$ & $\mathbf{.58}$ \\ & SST & & & & & $.31$ & $\mathbf{.54}$ \\
& TREC & & & & & $.42$ & $\mathbf{.71}$ \\ & IMDB & & & & & $.29$ & $\mathbf{.55}$ \\
\bottomrule
\end{tabular}
\subcaption{\textsc{dbert}}
\end{subtable}


\caption{Agreement between pairs of saliency methods in terms of Kendall-$\tau$ ($k_\tau$) and Pearson-$r$ ($p_r$) for \textsc{base} variants of (a) \textsc{jwa} and (b) \textsc{dbert}. We average the agreement over five runs with different seeds. D-SHAP and G-SHAP denote Deep-SHAP and Grad-SHAP, respectively. The values in bold indicate which agreement is higher between the two metrics.}
\label{tab:agreement}

\end{table}

% \begin{table*}[t!]
% \centering
% \small
% \begin{tabular}{lrrrrrrr}
% \toprule
% & & \multicolumn{2}{c}{Deep-SHAP} & \multicolumn{2}{c}{Grad-SHAP} & \multicolumn{2}{c}{Int-Grad} \\
% \cmidrule(lr){3-4} \cmidrule(lr){5-6} \cmidrule(lr){7-8}
% & & $k_\tau$ & $p_r$ & $k_\tau$ & $p_r$ & $k_\tau$ & $p_r$ \\
% \midrule
% \multirow{4}{*}{\rotatebox[origin=c]{90}{DeepLIFT}}
% & SUBJ & $\mathbf{1.00_{.00}}$ & $\mathbf{1.00_{.00}}$ & $.31_{.02}$ & $\mathbf{.45_{.02}}$ & $.43_{.01}$ & $\mathbf{.64_{.03}}$ \\
% & SST & $\mathbf{1.00_{.00}}$ & $\mathbf{1.00_{.00}}$ & $.30_{.04}$ & $\mathbf{.47_{.05}}$ & $.35_{.05}$  & $\mathbf{.54_{.05}}$ \\
% & TREC & $\mathbf{1.00_{.00}}$ & $\mathbf{1.00_{.00}}$ & $.12_{.03}$ & $\mathbf{.31_{.02}}$ & $.15_{.04}$ & $\mathbf{.33_{.02}}$ \\
% & IMDB & $\mathbf{1.00_{.00}}$  & $\mathbf{1.00_{.00}}$ & $.29_{.04}$  & $\mathbf{.59_{.07}}$ & $.28_{.04}$ & $\mathbf{.60_{.07}}$ \\
% \midrule
% \multirow{4}{*}{\rotatebox[origin=c]{90}{Deep-SHAP}}
% & SUBJ & & & $.31_{.02}$ & $\mathbf{.45_{.02}}$ & $.43_{.01}$ & $\mathbf{.64_{.03}}$ \\
% & SST & & & $.30_{.04}$ & $\mathbf{.47_{.05}}$ & $.35_{.05}$ & $\mathbf{.54_{.05}}$ \\
% & TREC & & & $.12_{.13}$ & $\mathbf{.31_{.02}}$ & $.15_{.14}$ & $\mathbf{.33_{.02}}$ \\
% & IMDB & & & $.29_{.04}$ & $\mathbf{.60_{.07}}$ & $.28_{.04}$ & $\mathbf{.60_{.07}}$ \\
% \midrule
% \multirow{4}{*}{\rotatebox[origin=c]{90}{Grad-SHAP}}
% & SUBJ & & & & & $.62_{.02}$ & $\mathbf{.78_{.02}}$ \\
% & SST & & & & & $.70_{.04}$ & $\mathbf{.87_{.03}}$ \\
% & TREC & & & & & $.66_{.04}$ & $\mathbf{.85_{.04}}$ \\
% & IMDB & & & & & $.68_{.05}$ & $\mathbf{.94_{.02}}$ \\
% \bottomrule
% \multicolumn{8}{@{}c}{(a) \textsc{jwa}}\\
% \end{tabular}

% \bigskip
% \centering
% \small
% \begin{tabular}{lrrrrrrr}
% \toprule
% & & \multicolumn{2}{c}{Deep-SHAP} & \multicolumn{2}{c}{Grad-SHAP} & \multicolumn{2}{c}{Int-Grad} \\
% \cmidrule(lr){3-4} \cmidrule(lr){5-6} \cmidrule(lr){7-8}
% & & $k_\tau$ & $p_r$ & $k_\tau$ & $p_r$ & $k_\tau$ & $p_r$ \\
% \midrule
% \multirow{4}{*}{\rotatebox[origin=c]{90}{DeepLIFT}}
% & SUBJ & $.24_{.01}$ & $\mathbf{.44_{.02}}$ & $.10_{.02}$ & $\mathbf{.19_{.03}}$ & $.12_{.02}$ & $\mathbf{.21_{.03}}$ \\
% & SST & $.19_{.01}$ & $\mathbf{.34_{.02}}$ & $.09_{.02}$ & $\mathbf{.17_{.03}}$ & $.10_{.03}$  & $\mathbf{.20_{.04}}$ \\
% & TREC & $.16_{.01}$ & $\mathbf{.30_{.04}}$ & $.12_{.02}$ & $\mathbf{.25_{.04}}$ & $.12_{.02}$ & $\mathbf{.26_{.04}}$ \\
% & IMDB & $.28_{.01}$  & $\mathbf{.51_{.02}}$ & $.11_{.01}$  & $\mathbf{.24_{.02}}$ & $.13_{.01}$ & $\mathbf{.27_{.02}}$ \\
% \midrule
% \multirow{4}{*}{\rotatebox[origin=c]{90}{Deep-SHAP}}
% & SUBJ & & & $.11_{.02}$ & $\mathbf{.22_{.03}}$ & $.13_{.02}$ & $\mathbf{.24_{.04}}$ \\
% & SST & & & $.10_{.02}$ & $\mathbf{.19_{.03}}$ & $.11_{.04}$ & $\mathbf{.23_{.05}}$ \\
% & TREC & & & $.13_{.02}$ & $\mathbf{.28_{.04}}$ & $.14_{.03}$ & $\mathbf{.30_{.05}}$ \\
% & IMDB & & & $.12_{.01}$ & $\mathbf{.26_{.02}}$ & $.14_{.02}$ & $\mathbf{.30_{.02}}$ \\
% \midrule
% \multirow{4}{*}{\rotatebox[origin=c]{90}{Grad-SHAP}}
% & SUBJ & & & & & $.36_{.02}$ & $\mathbf{.58_{.02}}$ \\ & SST & & & & & $.31_{.02}$ & $\mathbf{.54_{.03}}$ \\
% & TREC & & & & & $.42_{.03}$ & $\mathbf{.71_{.04}}$ \\ & IMDB & & & & & $.29_{.01}$ & $\mathbf{.55_{.02}}$ \\
% \bottomrule
% \multicolumn{8}{@{}c}{(a) \textsc{dbert}}\\
% \end{tabular}

% \caption{Agreement between pairs of saliency methods in terms of Kendall-$\tau$ ($k_\tau$) and Pearson-$r$ ($p_r$) for (a) \textsc{jwa} and (b) \textsc{dbert}. We average the agreement over $5$ runs with different seeds. The numbers in subscript denote the standard deviation and values in bold indicate which agreement is higher between the two metrics.}
% \label{tab:agreement}
% \end{table*}
%

We now evaluate how Pearson's $r$ ($p_r$) compares to Kendall-$\tau$ ($k_\tau$) when evaluating agreement. 
In \Cref{tab:agreement} we compare agreement scores produced by $p_r$ and $k_\tau$ across all datasets for \textsc{jwa} and \textsc{dbert}, respectively. Across all datasets and models, we observe consistently higher agreement values for $p_r$. We take this as an indication that the minor differences between saliency scores on tokens with approximately the same relative importance produced different rankings which were harshly penalized by $k_\tau$. We argue that the higher correlation scores reported by the Pearson correlation coefficient are a better estimate for agreement between saliency scores, and we advocate for its use rather than rank correlation.


\begin{figure}[t!]
\centering
\includegraphics[width=\linewidth]{img/kendall2.pdf}
\caption{A toy example of sentiment classification illustrating the problems with Kendall-$\tau$. The corresponding agreement on the shown example is $k_\tau = -.43$ and $p_r = .99$. Token opacity indicates higher saliency score, i.e., token relevance, written out at the top of the token box. Each of the two explainability methods A and B outputs its saliency scores. The value of Kendall-$\tau$ is much lower than Pearson's correlation, because the irrelevant tokens are perturbed, despite the fact that the tokens are correctly partitioned into more important and less important ones.}
\label{fig:kendall_problem}
\end{figure}


\subsection{Regularizing Models}
\label{sub:reg-models}


\begin{table}
\small
\centering
\begin{tabular}{lrrrrrrr}
\toprule
& & \multicolumn{3}{c}{$\tau_k$} & \multicolumn{3}{c}{$r_p$} \\
\cmidrule(lr){3-5} \cmidrule(lr){6-8} \cmidrule(lr){7-8}
& & B & C & T & B & C & T \\
\midrule
\multirow{4}{*}{\rotatebox[origin=c]{90}{\textsc{jwa}}}
& SUBJ & $.52$ & $.48$ & $\mathbf{.65}$\nospacetext{$^\dagger$} & $.66$ & $.70$ & $\mathbf{.88}$\nospacetext{$^\dagger$}  \\
& SST & $.50$ & $.67$ & $\mathbf{.68}$ & $.65$ & $\mathbf{.90}$\nospacetext{$^\dagger$} & $.86$ \\
& TREC & $.37$ & $\mathbf{.77}$\nospacetext{$^\dagger$} & $.68$ & $.52$  & $\mathbf{.98}$\nospacetext{$^\dagger$} & $.93$ \\
& IMDB & $.47$ & $.52$ & $\mathbf{.60}$\nospacetext{$^\dagger$} & $.72$  & $.64$ & $\mathbf{.80}$\nospacetext{$^\dagger$} \\
\midrule
\multirow{4}{*}{\rotatebox[origin=c]{90}{\textsc{dbert}}}
& SUBJ & $.18$ & $.28$ & $\mathbf{.36}$\nospacetext{$^\dagger$} & $.31$ & $.48$ & $\mathbf{.57}$\nospacetext{$^\dagger$} \\
& SST & $.15$ & $.15$ & $\mathbf{.33}$\nospacetext{$^\dagger$} & $.28$ & $.27$ & $\mathbf{.60}$\nospacetext{$^\dagger$} \\
& TREC & $.18$ & $.17$ & $\mathbf{.28}$\nospacetext{$^\dagger$} & $.35$  & $.34$ & $\mathbf{.53}$\nospacetext{$^\dagger$} \\
& IMDB & $.18$ & $.20$ & $\mathbf{.24}$\nospacetext{$^\dagger$} & $.36$ & $.42$ & $\mathbf{.51}$\nospacetext{$^\dagger$} \\
\bottomrule
\end{tabular}
\caption{Average agreement over all pairs of saliency methods. We report agreement in terms of Kendall-$\tau$ ($k_\tau$) and Pearson-$r$ ($r_p$) from the last epoch for three model variants: B -- base, C -- conicity, and T -- tying. \textbf{Bold} numbers indicate highest agreement among the three model flavors. Results are averages over $5$ runs with different seeds. We ran one-sided Wilcoxon signed-rank tests to check for statistical significance. Agreement values significantly higher ($p < .05$) than the other variants in two individual Wilcoxon tests are marked with a $\dagger$. We adjusted the $p$-values for family-wise error rate due to multiple tests using the Holm-Bonferroni method.}
\label{tab:avg_agr}
\end{table}

\begin{table}[t!]
\small
\centering
\begin{tabular}{lrrrr}
\toprule
& & Base & Conicity & Tying \\
\midrule
\multirow{4}{*}{\rotatebox[origin=c]{90}{\textsc{dbert}}}
& SUBJ & $.93_{.01}$ & $.90_{.02}$ & $.93_{.00}$ \\
& SST & $.83_{.00}$ & $.83_{.01}$ & $.82_{.01}$ \\
& TREC & $.92_{.01}$ & $.92_{.01}$ & $.91_{.01}$ \\
& IMDB & $.86_{.01}$ & $.86_{.01}$ & $.88_{.00}$ \\
\midrule
\multirow{4}{*}{\rotatebox[origin=c]{90}{\textsc{jwa}}}
& SUBJ & $.92_{.00}$ & $.90_{.00}$ & $.89_{.00}$ \\
& SST & $.78_{.04}$ & $.76_{.02}$ & $.78_{.02}$ \\
& TREC & $.89_{.02}$ & $.86_{.01}$ & $.89_{.01}$ \\
& IMDB & $.89_{.00}$ & $.88_{.00}$ & $.86_{.00}$ \\
\bottomrule
\end{tabular}
\caption{$F_1$ scores on \textit{test} sets across datasets for \textsc{dbert} and \textsc{jwa}. We report the test results on epochs in which the model had the best performance on the validation set. Columns correspond to the base and regularized models. Numbers in subscript denote standard deviation on $5$ runs with different seeds.}
\label{tab:f1_scores}
\end{table}

% \todo{Perhaps move to a later section as it introduces regularization}
Our next goal is to improve agreement between saliency methods through intervention in the training procedure, namely by applying regularization to promote disentanglement in the hidden space. 
% Although the $p_r$ scores are higher than $k_\tau$, we are still not satisfied with them -- remember, the agreement scores are computed for the same model instance.
%% Legacy -- figures moved to appendix
% In \Cref{fig:corr-jwa-subj,fig:corr-jwa-sst,fig:corr-jwa-trec,fig:corr-jwa-imdb,fig:corr-dbert-subj,fig:corr-dbert-sst,fig:corr-dbert-trec,fig:corr-dbert-imdb} we report correlation scores on the test splits of all datasets across training epochs for regularized models (\textsc{conicity}, \textsc{tying}) when compared to their unregularized, \textsc{base} variants.
In \Cref{tab:avg_agr} we report correlation scores on the test splits of all datasets for regularized models (\textsc{conicity}, \textsc{tying}) and their unregularized variants (\textsc{base}).
We notice that both regularization techniques have a positive effect on agreement across both correlation metrics, indicating that regularization techniques alleviate a deeper issue that also affects the interpretability of attention weights. 
% despite the fact that regularization was originally aimed towards increasing faithfulness of interpretability through inspecting attention weights.
% One might wonder whether the regularization techniques have an adverse effect on model performance?
In \Cref{tab:f1_scores} we report $F_1$ scores on the test set for the regularized and unregularized models with the best performance on the validation split.
We observe that regularized models generally perform comparably well to unregularized ones on downstream tasks, indicating that the improvement in the agreement does not come at a cost for downstream performance.
When selecting regularized models, we choose ones with the strongest regularization scale hyperparameter that performs within $3$ $F_1$ points on the validation set compared to the unregularized model (cf.~details in \Cref{subsec:A_hs}). % \josip{worse or better}
% In the following sections, we will attempt to analyse the reason why regularization techniques increase agreement and on which sets of data samples.
%
