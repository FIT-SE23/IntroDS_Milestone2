\section*{Limitations}
\label{sec:limitations}

Our work has a number of important limitations that affect the conclusions we can draw from it.
% No ground truth
First and foremost, evaluating the faithfulness of model interpretations is problematic as we do not have ground truth annotations for token importances.
Thus, when applying the \textit{agreement-as-evaluation} paradigm, we implicitly assume that most saliency methods are close to the truth -- an assumption that we cannot verify.
However, every method of evaluating faithfulness has its own downsides. Token and representation erasure runs the risk of drawing conclusions from corrupted inputs that fall off the data manifold.
We argue that while \textit{agreement-as-evaluation} is far from an ideal way of evaluating faithfulness, it still increases credibility when used along with other techniques.
% Model limitations

Secondly, our work is limited both with respect to the datasets and models considered.
Specifically, we only evaluate one Transformer-based model from the masked language modeling family, and it is entirely possible that the findings do not generalize to models pre-trained on different tasks.
Also, we only consider single sequence classification datasets -- mainly due to the fact that the issues with the faithfulness of attention were most prevalent in those setups, which we assumed would be the same for agreement due to the same hypothesized underlying issue.
We believe that tasks that require retention of token-level information in hidden states, such as sequence labeling and machine translation, would exhibit higher agreement overall, even without intervention through regularization. We leave this analysis for future work.