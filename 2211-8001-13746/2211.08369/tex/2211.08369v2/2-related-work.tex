
\section{Background and Related Work}
\label{sec:rw}

% Introduce explainability method categories
Explainability methods come in different flavors determined by the method of computing feature importance scores.
Saliency methods perform \textit{post-hoc} analysis of the trained black-box model by either leveraging gradient information \cite{denil2014extraction,sundararajan2017axiomatic}, modifying the backpropagation rules \cite{bach2015pixel,shrikumar2017learning}, or training a shallow interpretable model to locally approximate behavior of the black-box model \cite{ribeiro2016should}, all with the goal of assigning scalar saliency scores to input features.
Alternatively, if the analyzed model is capable of generating text, one can resort to self-rationalization by prompting the trained model to generate an explanation for its decision \cite{marasovic2021few}.
In contrast to \textit{post-hoc} explanations, \textit{inherently interpretable} models produce explanations as part of their decision process, either by masking a proportion of input tokens and then performing prediction based on the remaining \textit{rationale} \cite{lei-etal-2016-rationalizing,bastings2019interpretable,jain2020learning}, or jointly performing prediction and rationale generation in cases where datasets with annotated rationales are available \cite{camburu2018snli}.
For some time, the attention mechanism \cite{bahdanau2014neural} has also been considered inherently interpretable. However, the jury is still out on whether such explanations can be considered faithful  \cite{jain2019attention,wiegreffe2019attention,tutek2020staying,bastings2020elephant}.

% One approach towards explaining model decisions is to perform \textit{post-hoc} analysis of the trained black-box model.
% Grad based
% Withing this family, gradient-based methods \cite{denil2014extraction,sundararajan2017axiomatic} leverage gradient information present in the model to compute feature importance.
% Propagation based
% Similarly, propagataion-based methods \cite{bach2015pixel,shrikumar2017learning} modify the backpropagation rules of a network to bypass issues caused by e.g. saturated nonlinearities.
% 
% Methods such as LIME \cite{ribeiro2016should} leverage a simpler, inherently interpretable model which is trained to approximate local behavior of the black-box model around each instance, while work in self-rationalization prompts the trained model to generate an explanation for its decision for a specific instance \cite{marasovic2021few}.
% Inherently interpretable models
% The alternative to post-hoc analysis is to design \textit{inherently interpretable} models which produce explanations as part of the decision process.
%
% An example of this family are models that mask a proportion of input tokens and then perform prediction based on the remainder of the text, which is treated as the \textit{rationale} \cite{lei-etal-2016-rationalizing,bastings2019interpretable,jain2020learning}.
% Similarly, in cases where datasets with annotated rationales are available, one can train models to jointly perform prediction and rationale generation \cite{camburu2018snli}.

% \subsection{Faithfulness of Explanations} -> samo novi paragraf

\textit{Faithfulness} is one of the most important desiderata of explanation methods \cite{jacovi-goldberg-2020-towards} -- faithful explanations are those that are true to the inner decision-making process of the model.
Approaches to evaluating faithfulness rely on measuring how the confidence of the model changes when inputs are perturbed \cite{kindermans2019reliability} or completely dropped from the model \cite{li2016understanding,serrano2019attention}.
However, perturbations to input often result in corrupted instances that fall off the data manifold and appear nonsensical to humans \cite{feng2018pathologies} or fail to identify all salient tokens properly \cite[ROAR;][]{hooker2019benchmark} -- raising questions about the validity of perturbation-based evaluation.
Recursive ROAR \cite{madsen2021evaluating} alleviates the issues of its predecessor at the cost of requiring many prohibitively expensive retraining steps, further motivating us to seek efficient solutions which do not require retraining the model multiple times.
% One instance of a trained model can only have one true ranking of feature importance for a single instance, where the job of the explanation method is to uncover this ranking.
% If we embrace the idea that saliency methods are correct \textit{to an extent}, 
Another option is to leverage the \textit{evaluation-by-agreement} \cite{jain2019attention} paradigm, which states that an interpretability method should be highly agreeable with other methods to be considered faithful.
% should
However, since empirical evidence has shown that saliency methods exhibit poor agreement between their explanations \cite{neely2021order}, \citet{atanasova2020diagnostic} recommend practitioners consider alternative methods for evaluating the quality of interpretability methods, such as diagnostic tests.
Finally, methods such as data staining \cite{sippy2020data} and lexical shortcuts \cite{bastings2021will} artificially introduce tokens that act as triggers for certain classes -- creating a ground truth for faithfulness which can be used as a comparison.
Nevertheless, such methods have a certain drawback in that they only offer the ground truth importance of a few artificially inserted tokens, but offer no insight regarding the relative importance of the remainder of the input.
Each of the aforementioned methods for estimating faithfulness of interpretability methods has its drawbacks \cite{jacovi-goldberg-2020-towards}, and we argue each should be taken in conjunction with others to increase the credibility of their collective verdict. 
%\martin{Trebalo bi motivirati zasto ne koristimo ove metode. Zasad mi je jedina ideja to da sve analizirane metode u nasem clanku, po shortcutima, pronalaze ground truth u tom umjetnom settingu, pa nema potrebe ih dodatno provjeravati?} 