%% 
%% Copyright 2007-2020 Elsevier Ltd
%% 
%% This file is part of the 'Elsarticle Bundle'.
%% ---------------------------------------------
%% 
%% It may be distributed under the conditions of the LaTeX Project Public
%% License, either version 1.2 of this license or (at your option) any
%% later version.  The latest version of this license is in
%%    http://www.latex-project.org/lppl.txt
%% and version 1.2 or later is part of all distributions of LaTeX
%% version 1999/12/01 or later.
%% 
%% The list of all files belonging to the 'Elsarticle Bundle' is
%% given in the file `manifest.txt'.
%% 

%% Template article for Elsevier's document class `elsarticle'
%% with numbered style bibliographic references
%% SP 2008/03/01
%%
%% 
%%
%% $Id: elsarticle-template-num.tex 190 2020-11-23 11:12:32Z rishi $
%%
%%
\documentclass[preprint,authoryear]{elsarticle}
% \documentclass[review,authoryear]{elsarticle}

%% Use the option review to obtain double line spacing
%% \documentclass[authoryear,preprint,review,12pt]{elsarticle}

%% Use the options 1p,twocolumn; 3p; 3p,twocolumn; 5p; or 5p,twocolumn
%% for a journal layout:
%% \documentclass[final,1p,times]{elsarticle}
%% \documentclass[final,1p,times,twocolumn]{elsarticle}
%% \documentclass[final,3p,times]{elsarticle}
%% \documentclass[final,3p,times,twocolumn]{elsarticle}
%% \documentclass[final,5p,times]{elsarticle}
%% \documentclass[final,5p,times,twocolumn]{elsarticle}

%% For including figures, graphicx.sty has been loaded in
%% elsarticle.cls. If you prefer to use the old commands
%% please give \usepackage{epsfig}

%% The amssymb package provides various useful mathematical symbols
\usepackage{amssymb}
\usepackage{booktabs}
\usepackage{subcaption}
\usepackage{longtable}
\usepackage{natbib}
\usepackage{hyperref}
\usepackage{amsmath}
\usepackage{lscape}
\usepackage[T1]{fontenc}
\usepackage{amsfonts} 
\usepackage{placeins}
\usepackage{multirow}
%% The amsthm package provides extended theorem environments
%% \usepackage{amsthm}

%% The lineno packages adds line numbers. Start line numbering with
%% \begin{linenumbers}, end it with \end{linenumbers}. Or switch it on
%% for the whole article with \linenumbers.
%% \usepackage{lineno}

\journal{Expert Systems with Applications}
\usepackage{xcolor}
\newcommand{\dorien}[1]{\textcolor{red}{#1}}
\newcommand{\subsubsubsection}[1]{\paragraph{#1}\mbox{}\\}
\setcounter{secnumdepth}{4}
\setcounter{tocdepth}{4}
\usepackage{subcaption}
\usepackage{pdflscape}


\begin{document}

\let\WriteBookmarks\relax
\def\floatpagepagefraction{1}
\def\textpagefraction{.001}

\begin{frontmatter}

%% Title, authors and addresses

%% use the tnoteref command within \title for footnotes;
%% use the tnotetext command for theassociated footnote;
%% use the fnref command within \author or \address for footnotes;
%% use the fntext command for theassociated footnote;
%% use the corref command within \author for corresponding author footnotes;
%% use the cortext command for theassociated footnote;
%% use the ead command for the email address,
%% and the form \ead[url] for the home page:
%% \title{Title\tnoteref{label1}}
%% \tnotetext[label1]{}
%% \author{Name\corref{cor1}\fnref{label2}}
%% \ead{email address}
%% \ead[url]{home page}
%% \fntext[label2]{}
%% \cortext[cor1]{}
%% \affiliation{organization={},
%%             addressline={},
%%             city={},
%%             postcode={},
%%             state={},
%%             country={}}
%% \fntext[label3]{}

\title{Forecasting Bitcoin volatility spikes from whale transactions and CryptoQuant data using Synthesizer Transformer models }

%% use optional labels to link authors explicitly to addresses:
%% \author[label1,label2]{}
%% \affiliation[label1]{organization={},
%%             addressline={},
%%             city={},
%%             postcode={},
%%             state={},
%%             country={}}
%%
%% \affiliation[label2]{organization={},
%%             addressline={},
%%             city={},
%%             postcode={},
%%             state={},
%%             country={}}


     % \author{Anonymous for peer review}     
\author[label1]{Dorien Herremans\footnote[1]{The authors contributed equally (co-first authors).}}
\ead{dorien_herremans@sutd.edu.sg}
\author[label1]{Kah Wee Low\footnotemark[1]}
\ead{lowkahwee1995@hotmail.com}

\affiliation[label1]{organization={SUTD},%Department and Organization
            addressline={Somapah Road 8}, 
            city={Singapore},
            postcode={487372}, 
            state={Singapore},
            country={SG}}

% \fntext[label4]{}
% \fntext[label4]{These authors contributed equally.}


% \affiliation{organization={SUTD},%Department and Organization
%             addressline={}, 
%             city={},
%             postcode={}, 
%             state={},
%             country={}}
            


\begin{abstract}


The cryptocurrency market is highly volatile compared to traditional financial markets. Hence, forecasting its volatility is crucial for risk management. In this paper, we investigate CryptoQuant data (e.g. on-chain analytics, exchange and miner data) and whale-alert tweets, and explore their relationship to Bitcoin's next-day volatility, with a focus on extreme volatility spikes. We propose a deep learning Synthesizer Transformer model for forecasting volatility. Our results show that the model outperforms existing state-of-the-art models when forecasting extreme volatility spikes for Bitcoin using CryptoQuant data as well as whale-alert tweets. We analysed our model with the Captum XAI library to investigate which features are most important. We also backtested our prediction results with different baseline trading strategies and the results show that we are able to minimize drawdown while keeping steady profits. Our findings underscore that the proposed method is a useful tool for forecasting extreme volatility movements in the Bitcoin market.


%Predicting market volatility is a vital mechanism in the financial markets as it measures not only the risk but the potential rewards as well. The cryptocurrency market is highly volatile compared to traditional financial instruments. Hence, forecasting its volatility is crucial for many applications, such as risk management or hedging. In this paper, we explore the on-chain analytics data, and whale-alert tweets and explore their relationship to Bitcoin's next day volatility. We focus on predicting extreme volatility spikes. The developed models are then tested in a trading strategy that aims to minimize large downswings while trying to maximize profits. We propose a deep learning Synthesizer Transformer model for forecasting volatility. In a thorough experiment, we compare our developed Transformer models with traditional models like Generalized AutoRegressive Conditional Heteroskedasticity (GARCH) as well as different variations of the Transformer architecture and other neural network architectures. The results show that the proposed Synthesizer Transformer model can outperform existing state-of-the-art models on the task of forecasting extreme volatility spikes for Bitcoin using on-chain data as well as whale-alert tweets, in addition to technical indicators calculated on this data. 
%We analyze our proposed Synthesizer Transformer model with Captum's Feature Ablation function, a PyTorch library for model interpreting and understanding, to investigate which input variables are most important and discovered that high\_low\_spread and volume are indeed important predictors of Bitcoin volatility. 
%Finally, we demonstrate the usefulness of our volatility prediction model by implementing a simple trading strategy with its signals.
%The backtesting results show that we are able to minimize drawdown while keeping steady profits. We also tried implementing volatility scaling in our strategies and the results show that we are able to increase our profits while taking a higher risk.
%Our findings underscore that our method is a useful and reliable alternative for forecasting extreme volatility predictions in Bitcoin market.


\end{abstract}

%%Graphical abstract
\begin{graphicalabstract}
\includegraphics[width=14cm]{cryptoquant.png}
\end{graphicalabstract}

%%Research highlights
\begin{highlights}
\item We propose a new Synthesizer Transformer model to predict next-day BTC volatility.
\item Our predictive model takes as input whale-alert data from Twitter.
\item Our model also uses CryptoQuant data, which includes on-chain and exchange data.
\item Explainable AI techniques (XAI) are used to uncover important features. 
\item Basic trading strategies show that the volatility predictions can reduce risk. 
% \item A thorough correlation analysis is made of all on-chain and whale-alert data with Bitcoin volatility. 
\end{highlights}

\begin{keyword}
Synthesizer Transformer\sep
Volatility Forecasting\sep
Cryptocurrency\sep
Bitcoin\sep
On-chain Analysis\sep
Twitter
%% keywords here, in the form: keyword \sep keyword
% \JEL G17 \sep C8 
%% PACS codes here, in the form: \PACS code \sep code

%% MSC codes here, in the form: \MSC code \sep code
%% or \MSC[2008] code \sep code (2000 is the default)

\end{keyword}

\end{frontmatter}

% \linenumbers

%added only for easy during editing: 
% \tableofcontents

\section{Introduction}

%% As said by \citet{hao2006combining}, we see that~\citep{hao2006combining}

%quick intro - including what we'll do

This paper studies the most popular cryptocurrency, Bitcoin, which is currently traded on more than 500 exchanges. Since Bitcoin is the first cryptocurrency, established in 2008~\citep{nakamoto2008Bitcoin}, it provides the longest historical data to study. Compared to traditional financial instruments like equities and commodities, cryptocurrencies like Bitcoin have large, so-called `whale' holders, which consist of about 1,000 people who own around 40\% of the market~\citep{kharif2017Bitcoin}. In this paper, we explore how large Bitcoin transactions from these whales affect the market volatility. We propose a state-of-the-art deep learning Synthesizer Transformer model \citep{tay2020Synthesizer} that predicts if Bitcoin's volatility will be extreme the next day, based on transaction data from these whales as well as a variety of features from CryptoQuant, including on-chain metrics, miner flows, and more. We compare this proposed model with existing baseline models and propose a simple trading strategy to demonstrate the practical usefulness of the predictions. In our experiments, we also analyse the importance of the different CryptoQuant and whale-alert features that most influence volatility. An overview of our paper is provided in Figure~\ref{fig:overview}. The code of our proposed (trained) models is made available online\footnote{\url{https://github.com/dorienh/bitcoin_synthesizer}}.
%

\begin{figure}[h!]
    \centering
    \includegraphics[width=\textwidth]{cryptoquant.png}
    \caption{Overview of the proposed study. }
    \label{fig:overview}
\end{figure}

% why bitcoin
% According to  \citet{hardle2020understanding}), cryptocurrencies may be grouped into seven broad classes. The first class considers it as a transaction mechanism, such as Bitcoin. The remaining classes consist of distributed computation tokens, utility tokens, security tokens, fungible tokens, non-fungible tokens, and stablecoins. 



% what is volatility
We focus on the volatility of Bitcoin as this digital asset dominates the cryptocurrency market with the largest market cap after USDT. In finance, volatility refers to the degree of variation of an asset's price over time~\citep{black2012dictionary}. Market volatility is generally considered a vital metric to evaluate the level of risk, and thus it plays a critical role in assessing the stock market risk and the pricing of derivative securities~\citep{yang2020html}. Compared to traditional financial instruments, the price of Bitcoin is highly volatile~\citep{blau2017price}. 
In general, the Bitcoin market is currently highly speculative, and thus more susceptible to speculative bubbles than other traditional currency markets \citep{grinberg2012Bitcoin, cheah2015speculative}. 
Bitcoin has recently also found its place in portfolios to hedge against the global geopolitical crisis  \citep{dyhrberg2016bitcoin} and reduce financial market uncertainty (\citep{platanakis2019portfolio, fang2019does, colon2021effect}, hence studying risk and assessing exposure is important to cryptocurrency investors, and it becomes important to model and forecast the volatility of Bitcoin. In this paper, we focus on predicting future spikes in Bitcoin's volatility. 


% how to trade on volatility
% Derivatives have allowed people to trade cryptocurrency volatility more efficiently, and therefore Bitcoin price patterns are structurally different before and after 2016, when many of these derivatives were born. For instance, the Deribit Bitcoin Futures and Options trading platform\footnote{\url{https://www.deribit.com/}} is an institutional-grade trading platform established in June 2016 to facilitate crypto-traders by offering plain ``Vanilla'' European Bitcoin options and Bitcoin futures contracts with margin~\citep{zulfiqar2021implied}. 
% The trading of Bitcoin futures contracts commenced at the Chicago Mercantile Exchange (CME) and the Chicago Board Options Exchange (CBOE) in December 2017~\citep{akyildirim2020development}. The available options to trade Bitcoin volatility more easily further expanded when the first US Bitcoin (BTC) exchange-traded fund (ETF), ``BITO'', started trading on 19 October 2021~\citep{todorov2021launch}. 


%overview of contributions
This study aims to gain further insights into the market conditions that may cause drastic increases in volatility in Bitcoin markets. Our contribution is threefold. We first thoroughly explore both CryptoQuant data and the influence of whale transactions on volatility. Second, we propose and evaluate a state-of-the-art Synthesizer Transformer model to predict volatility. Finally, we propose a basic trading strategy that leverages the volatility predictions to reduce downward risk. We briefly touch upon the importance of these contributions in what follows. 


%first contribution - I've rephrased this a bit to build it up to what we are doing. 
First, in this study, we gather a dataset from CryptoQuant\footnote{\url{http://cryptoquant.com}}, as well as whale transaction tweets from January 2018 to September 2021. The former includes information such as exchange and miners transactions as well as liquidations and open interest caused by trading with leverage (full feature set, see Table~\ref{tab:cryptoquant_table}). We thoroughly explore the relationship between this data and Bitcoin's next-day volatility, and focus on discovering large market movements induced by the ripple effects of large whale transactions and on-chain movements.  

%From this data, we use daily feature data as well as volatility so as to minimize noise.



%second contribution
Second, we propose a Synthesizer Transformer model to perform the volatility spike prediction. The Transformer architecture has proven to be extremely efficient for a range of tasks related to time series such as text translation \citep{vaswani2017attention}, music generation \citep{makris2021generating}, emotion prediction from movies \citep{thao2021attendaffectnet}, and speech synthesis \citep{li2019neural}. In finance, it has been shown to be efficient at stock price \citep{liu2019Transformer, zhang2022Transformer} and even stock \textit{volatility} prediction \citep{yang2020html}. In the cryptocurrency markets, we see that it has been used for Dogecoin \citep{sridhar2021multi} and Bitcoin \citep{jain2019cryptocurrency} price prediction. In this work, we expand the existing literature by including CryptoQuant and whale data (plus technical indicators calculated on this data). We then go beyond just building a black-box model, but also explore the influence of these features on volatility prediction through explainable artificial intelligence (XAI) techniques with the Captum library \citep{kokhlikyan2020captum}. Instead of using Vanilla (standard) Transformer architectures, we change the typical dot product self-attention mechanism to Synthesizer attention, which learns synthetic attention weights without token-to-token interactions. By doing so, we optimize the attention span of the model. Recent work has shown that Synthesizer Transformers outperform traditional Transformers. Even a simple Random Synthesizer has shown to be 60\% faster than a traditional Transformer \citep{tay2020Synthesizer}.
In an experiment, we compare our proposed architecture to other configurations and baseline traditional models like GARCH. We show that it is a useful and reliable method for forecasting volatility in cryptocurrencies. 





%third contribution
Finally, we explore the usefulness of our predictions by backtesting a number of trading strategies that use the predicted volatility. 
In practice, investors often use volatility to trade derivative instruments such as put and call options \citep{ni2008volatility}. Since it is hard to backtest such a strategy in a Bitcoin context, we propose examples of simple trading strategies which use trading signals based on our volatility prediction model. 
We explore four different strategies: buy \& hold, buy-low-sell-high, mean reversion and momentum-based. When we include position scaling based on volatility, we notice an increase in the cumulative returns as well as the Sharpe ratio. In future work, these strategies should further be improved, but for now, they serve as a simple example that our prediction model can be used to lower the downside risk of a portfolio. 
%This outcome supports our hypothesis that CryptoQuant data and whale-alert data are instrumental for our neural network to successfully predict volatility spikes, allowing us to profit from such volatility spikes as well as mitigate risk.


The rest of this paper is structured as follows. In Section~\ref{sec:2}, we review the existing literature, followed by a thorough description and visualisation of the dataset that was collected. Next, the proposed Synthesizer Transformer models are introduced in Section~\ref{sec:4}. Section~\ref{sec:5} provides a detailed account of the performance of the volatility prediction models compared to benchmarks, as well as insight into the important features through XAI. The setup and results of the backtesting experiment is described in Section~\ref{sec:6}. Finally, we provide conclusions and suggestions for further work in Section~\ref{sec:7}.



\section{Literature Review}
\label{sec:2}
We provide a brief overview of literature related to on-chain data, using Twitter data for volatility and price prediction, followed by deep models for cryptocurrency-related predictions. For a more complete overview, the reader is referred to \citep{zou2022multimodal, charandabi2021prediction, khedr2021cryptocurrency, charandabi2022survey}.



\subsection{Cryptocurrency-specific data}

%Talk about the unique nature of Bitcoin and how the entire order book is %transparent, but to use that would require so much computing power, we focus on %aggregated metrics. Still... anything done on this, we can mention in a short %paragraph. 
%\dorien{still thinking about these two first subsections.}

The cryptocurrency markets are fundamentally quite different from traditional stock markets. One of the key differences is the transparency provided by blockchain technologies \citep{biswas2019analysis}. Transparency is one of the key features of Bitcoin trading as the entire trading history is available and traders are provided with information on the complete state of the order book, but trading itself is pseudonymous. This transparency provides unique features that may be useful for price and volatility prediction. 

On-chain data includes information from the blockchain ledger, such as the details of each transaction (e.g. from which wallet, to which wallet, amount, fees paid to miners), and the difficulty of mining blocks as well as the block sizes \citep{jagannath2021chain, kim2022deep}. The availability of such data can gives us incredible insight in upcoming price movements \citep{zheng2021chain}. The transparency in the blockchain even allows us to access the entire transaction history ever recorded. There is no hidden volume (as in iceberg orders) nor dark pools~\citep{dimpfl2017bitcoin}. However, to use this data would require a huge amount of computing power, hence, we focus on aggregated on-chain data instead. CryptoQuant provides us with a wide selection of such features, and also includes exchange data such as the amount of liquidations, as well as data on Bitcoin miners. 

% On-chain data such as the number of Bitcoins sent, the exact time, wallet address, and fees that are being paid to miners as well as other activity details like exchange flow data provide insight into actual major players' money flow in the networks. 


Looking at existing literature, we see that utilizing this transparency allows one to establish a trader's edge. For instance, \citet{kim2022deep} show that on-chain data can be useful when predicting Bitcoin's price with a self-attention-based multiple long short-term memory model (SA-LSTM). While they provide a list of 42 variables used, there is no ablation study or XAI method used to identify which variables are most important. \citet{jagannath2021chain} equally show that the Ethereum price can be predicted using on-chain data and a self-adaptive LSTM model. A correlation analysis using their data reveals important correlated on-chain features to the price of Ethereum. These features include transaction rate, supply in smart contracts, block difficulty and hash rate. 
% \citet{an2021utilizing} ... in korean!


On-Chain data is not only useful for \textit{price} prediction, the correlation between on-chain transaction activities and \textit{volatility} has been shown by \citet{gkillas2021transaction}. \citet{raheman2021architecture}'s developed agent for crypto-portfolio management also uses on-chain data for price trend and volatility prediction. The literature available on the effects of various cryptocurrency-specific data such as on-chain data is still in its early shoes. In this work, we aim to not just build a predictive model for volatility, but also thoroughly analyse the patterns within the data and provide an XAI interpretation of the resulting model. 

In addition to CryptoQuant data, we also parsed a new dataset of whale transactions. An overview of the literature related to this is provided in the next subsection. 


%anything we can say about influence/correlation of on-chain data




\subsection{Importance of Twitter data for volatility}


The CryptoQuant data offers us nice insights into aggregated on-chain data, miner data and more. It does not, however, include transactions by so-called `crypto-whales', holders of very large wallets. It is well known that cryptocurrencies are very volatile in nature, thus creating both outstanding benefits as well as a huge risk to investors~\citep{bariviera2017some, klein2018Bitcoin}. Part of this volatility can be attributed to large (whale) transactions and their ripple effect on the market. 
In this work, we will be using very specific Twitter content, namely `whale-alert' tweets. The Twitter account\verb|@whale_alert|, is a third-party information provider that ``monitors millions of daily cryptocurrency transactions and publishes notable events on Twitter in near real-time'' \citep{saggu2022intraday}. \citet{scaillet2020high} found a correlation between their `whale index' and high-frequency price jumps of Bitcoin. 




%can consider not including the below paragraph: 
Social media sources such as Twitter have been shown to be helpful data sources for stock or cryptocurrency price predictions. To name a few examples, \citet{lamon2017cryptocurrency} study whether including sentiment analysis of news and social media can improve models when predicting the price of Bitcoin and Ethereum. \citet{aharon2022Twitter} explore the relationship between two novel Twitter-based measures of economic and market uncertainty and the performance of four major cryptocurrencies. % to determine whether investors can use these measures to predict cryptocurrency returns.
\citet{zou2022multimodal} shows that using BERT context embeddings of tweets with an LSTM model can improve Bitcoin price prediction. News and social media data have also been shown to be useful for \textit{volatility} prediction, as \citet{sapkota2022news} predicts Bitcoin volatility based on news sentiment, and \citet{akbiyik2021ask} use temporal convolutional neural networks for Bitcoin volatility prediction with Twitter sentiment. \citet{shen2019does} show that the number of tweets is a major determinant of the next dayâ€™s trading volume and realised volatility of Bitcoin. Finally, \citet{wu2021does} reported that there is a significant Granger-causality from Twitter-based uncertainty measures to Bitcoin, Ethereum, Litecoin, and Ripple prices in different time periods. In this work, we will focus on integrating tweets by \verb|@whale_alert| into our Transformer model.









\subsection{Deep neural networks for financial time series predictions}

Traditional models, like Generalised autoregressive conditional heteroscedasticity (GARCH)-based models) are widely used for volatility forecasting~\citep{engle1982autoregressive,bollerslev1986generalized}.  \citet{katsiampa2017volatility} and \citet{bergsli2022forecasting} study volatility forecasting for Bitcoin using GARCH and its variants. \citet{naimy2018modelling} concluded, however, that the predictive ability of GARCH is not good in the context of unusually high volatility, and performs better when volatility is relatively low. \citet{vilasuso2002forecasting} brings up one of GARCH's major limitations where ``its memory is sometimes not long enough to capture the persistence of some shocks that are observed to last for a very long time''. 
\citet{jiang2022forecasting} propose a time-varying mixture model, which includes an accelerating generalized autoregressive score (aGAS) technique into the Gaussian-Cauchy mixture (TVM)-aGAS model for forecasting Value-at-Risk for cryptocurrencies. 
Recently, however, many researchers have turned to ever more powerful deep learning models for financial time series prediction. 





Just like in the stock market \citep{ding2015deep, hu2021survey, jiang2021applications}, deep learning models have become popular tools for price prediction in cryptocurrency markets \citep{zou2022multimodal, yao2018predictive, patel2020deep, akyildirim2021prediction, alessandretti2018anticipating, khedr2021cryptocurrency}. Looking at time series in general, recurrent neural networks, such as long-short term memory models (LSTMs)~\citep{hochreiter1997long} and gated recurrent unit (GRUs)~\citep{chung2014empirical} have been widely used for forecasting. 
When it comes to volatility prediction, \citet{vidal2020gold} proposed an architecture based on convolutional neural networks (CNNs) and long-short term memory (LSTM) units to forecast gold volatility. LSTMs were also used by \citet{jung2021forecasting} to forecast currency exchange rate volatility. Finally, temporal convolutional neural networks have been used with Twitter sentiment data to predict Bitcoin volatility \citep{akbiyik2021ask}. 


In recent years, with the invention of the Transformer network \citep{vaswani2017attention}, deep models for time series prediction have become even more powerful. Transformers use a self-attention mechanism, to give relative focus on the context of an element of a time series, and are better able to capture long-term trends. In finance, we have seen the successful use of Transformer architectures for tasks such as stock price prediction \citep{ding2020hierarchical}, stock volatility prediction \citep{ramos2021multi}, and even cryptocurrency price prediction such as Dogecoin \citep{sridhar2021multi} and Bitcoin \citep{jain2019cryptocurrency}. The work on volatility prediction for Bitcoin with Transformers is relatively non-existent, except for the work by \citet{sapkota2022news} who built a model based on Twitter sentiment data. In this work, we explore how we can use the powerful Transformer architecture to perform Bitcoin volatility prediction, not only based on candlestick data, but also CryptoQuant data and whale-alert tweets. In addition, we implement the Synthesizer Transformer, to further optimize the attention mechanism. 




% \citep{alessandretti2018anticipating} (2018) test the performance of three forecasting models on daily cryptocurrency prices for 1,681 currencies, and built investment portfolios based on the predictions of different method.
% \citep{akyildirim2021prediction} study the predictability of cryptocurrency price changes using technical indicators and machine learning algorithms, and find that machine learning has broad prospects in predicting the trends of cryptocurrency market in short-term. 









\section{Dataset collection and analysis}
\label{sec:3}

The Bitcoin market provides interesting conditions from a volatility point of view. There is 24-hour continuous trading, 365 days a year, with a lack of central authorities (e.g., central banks), resulting in the absence of a volatility trading halt, and no pre-market/post-market  trading as compared to the equities market~\citep{brandvold2015price}. These market conditions, along with the complete transparency of the on-chain trading data, create an interesting opportunity for us to study the influence of different factors on volatility. To do so, we have gathered a dataset from January 2016 until September 2021, which consists of CryptoQuant (on-chain data and market data from cryptocurrency exchanges), and whale transaction tweets.  We will start below by discussing the features in this dataset and how we gathered them, and then move on to include technical indicators and data preprocessing. 


\subsection{Data sources}

In this section, we discuss how we gathered whale transaction data which includes many aspects such as whale accumulation, whale dumping, miners' inflow and outflow, as well as exchanges' inflow and outflow.


\subsubsection{Whale-alert data}

Crypto `whales' include some of the largest wallet holders, and hence have a significant influence on both price and volatility \citep{nguyen2018factors}. In any volatility model, it is thus essential to include data about whale transactions. In order to do so, we tracked the Twitter handle \verb|@whale_alert|, which provides continuous alerts as whale transactions happens. Some example tweets by this handle are shown below: 

\begin{itemize} \setlength\itemsep{0em} \setlength{\parskip}{0pt}%
    \item ``\verb|997 #BTC (6,269,280 USD) transferred from #Bitfinex| 
    
    \verb|to Unknown wallet|''
    \item ``\verb|11,000 #ETH (2,473,411 USD) transferred from Unknown|
    
    \verb|wallet to #Gemini|''
    
    \item ``\verb|6,000,000 #USDC (6,000,000 USD) burned at USDC Treasury|''
\end{itemize}



Once we collected all of the tweets from 12 September 2018 (earliest available) to 18 October 2021, we filtered transactions using the hashtag \#BTC and the keyword `transferred', resulting in a total of 52,787 tweets. We then wrote a parser that uses a set of rules to obtain useful data from these tweets such as total daily inflow and outflow of wallet to exchange, e.g. the word after `from' will be the source of transaction and the word after `to' will be the destination. For all of the tweets gathered in a day, we determine the overall net transaction outflow or inflow of wallets to exchanges in one day, resulting in the following daily features: 
%\dorien{Explain full procedure how you extracted the data from tweets, try to put your rules here.} 
\begin{description} \setlength\itemsep{0em} \setlength{\parskip}{0pt}%
    \item [BTCminus] The amount of Bitcoin flowing out of wallets into exchanges.
\item [BTCplus] The amount of Bitcoin flowing into wallets from exchanges.
\item [USDminus] The amount of USD flowing out of wallets into exchanges.
\item [USDplus] The amount of USD flowing into wallets from exchanges.
\end{description}


This data is relevant for our task: a transaction from wallet to exchange typically indicates a bearish sentiment given that the seller is closing their Bitcoin position and may want to exchange it into fiat currency. On the other hand, a transaction from exchange to wallet means that a buyer is planning to keep their Bitcoin position (or at least not exchange it into fiat) and is therefore bullish. For the purpose of this study, we only examine Bitcoin transactions that flow either from exchange to wallet or wallet to exchange. The total net flow of transactions from wallet to wallet and exchange to exchange is ignored. 




Figure~\ref{fig:whale_alert} plots the BTC price volatility against the number of BTC transactions measured in the daily amount of BTC that flowed to and from exchanges as per our whale-alert tweets. We see that there are patterns where volatility spikes during a spike in BTC transactions. There are 330 volatility spikes in total and we see that the net daily amount of BTC that flowed to or from exchanges (calculated as $abs(\text{BTCplus} - \text{BTCminus})$) has a Pearson correlation of 0.47 with daily BTC price volatility.

%\dorien{So after this you have 2 variables per day, can you show some statistics of them? Visualise them together with bitcoin price or volatility perhaps?}


\begin{figure}[h!]
  \centering
    \includegraphics[width=0.8\textwidth]{diagrams/twitter_analysis.png}
    \caption{The daily amount of net BTC that flowed to or from exchanges per day, calculated as $abs(\text{BTCplus} - \text{BTCminus})$ (top). The BTC price volatility (bottom). }
    \label{fig:whale_alert}
\end{figure}






\subsubsection{CryptoQuant on-chain and exchange data}

%\dorien{first explain what cryptoquant is, then that you used their API (with link in footnote), %and the list how many features you extracted, followed by some examples (most important ones), and %a reference to the full table of features. 


CryptoQuant data provides comprehensive on-chain and market data gathered from both the blockchain as well as major cryptocurrency exchanges. Every single transaction that occurs in these markets is tracked by CryptoQuant. CryptoQuant even keeps track of which addresses are exchanges or mining pools, and aggregates the amount of BTC flowing between different types of entities, such as miners, and exchanges. In this study, we use CryptoQuant's\footnote{\url{https://cryptoquant.com/docs}} API to gather BTC related data. %The CryptoQuant data that we collected is quite rich and includes data that covers general market data, indicators, and insights into BTC flowing between different types of entities, such as miners, and exchanges. 
While a full overview of all the features we use is provided in Table~\ref{tab:cryptoquant_table} based on CryptoQuant's documentation\footnote{\url{https://dataguide.cryptoquant.com/}}, we elaborate on a few specific examples below: 


\begin{description} 
    \item[miner\_inflow\_mean\_ma7] The 7-day moving average of miner inflow gives us insight into when whale accumulation occurs. Miners are often considered to be the original whales, as they typically hold large wallets. 

    \item[mtoe\_flow\_total] The miners-to-exchanges feature will keep track of how much BTC miners are transferring to exchanges. Typically, the main reason to send Bitcoin to an exchange would be to sell it, hence this can be a bearish indicator. 

    \item[miner\_outflow\_top10] The amount of Bitcoin that flows out of the 10 largest Bitcoin wallets held by miners. These whale wallets will be responsible for downward pressure and increased volatility if this variable increases. 

% \item[MPI] Or Miners' Position Index. 
%  Exchange inflow mean also tracks the average amount of Bitcoin deposited into all exchanges. If a sudden spike in exchange inflow mean occurs over a few minutes, then it might signal a Bitcoin whale dumping.


%     If miners withdraw an unusually large amount of Bitcoins compared to the past year, the MPI increases, and the Bitcoin price is likely to be bearish. On the other hand, if the MPI is low, miners hold Bitcoin, meaning it's likely to be bullish.


%     \item[]
    
    
    \item[long\_liquidation] The amount of leveraged positions in BTC that were forced to exit due to volatility. High values for this variable hence often go hand in hand with high volatility. 

\end{description}


%\dorien{It would be good to say more about the features you got from here. I think there are broad classes (the folders in the dataset?) Talk more about why you include this kind of data. How do you think it would be important. }











\subsection{Technical Indicators}

In order to improve the prediction of our volatility prediction model, we include some traditional technical indicators as input which have shown to be correlated to volatility~\citep{liashenko2020fractionally}. These include Exponential Moving Average (EMA), High-Low Spread, and Close-Open Spread. Exponential moving average indicators place a higher weighting on recent data compared to old data, hence, they are more reactive to the latest price changes compared to simple moving averages (SMAs). For this reason, we chose to include the 10th day EMA of the closing price instead of its SMA. This was calculated as per the below Equation~\ref{eq:ema} whereby $n$ is the number of days over which the EMA at time $t$ for a time series $X$ is calculated. The variable $S$ represents a smoothing factor, which we set to 2 for our study. 

\begin{equation}
  \text{EMA}_t = X_{t} \times (\frac{S}{1 + n}) + \text{EMA}_{t-1} \times (1 - \frac{S}{1 + n})
  \label{eq:ema}
\end{equation}


A second technical indicator is the High-Low Spread. This indicator gives insight into the intra-day total price movement. A higher value means that the price fluctuated in either direction in one day, thus indicating a higher volatility for that day, and vice versa.


\begin{equation}
  \text{High-Low Spread} = \frac{\text{High} - \text{Low}}{\text{Close}}  
\end{equation}


Finally, the Open-Close indicator provides a sense of the direction and size of the move. If the price goes up, this indicator will be negative, and vice versa. 

\begin{equation}
\text{Close-Open Spread} = \frac{\text{Close} - \text{Open}}{\text{Open}}
\end{equation}





\subsection{Data preprocessing}


\subsubsection{Missing values}


Some of the used technical indicators, such as exponential moving average, have a short warm-up period resulting in missing values. We can fill up the missing values by using the first available value since this only occurs at the very beginning of our (training) dataset. 


The whale exchange tweets and derivatives data were only available from 2018 onwards. Before that period, we consider them to be zero. For leverage and derivatives data, it is easy to assume that the missing values are 0 since these assets were not yet available or created. 



\subsubsection{Standardization}
Some of the distributions of the input features are skewed which would affect the Transformer's predictive abilities, hence we set out to standardize this. 
The descriptive statistics of features in Table~\ref{tab:before_normalization} are standardized in Table~\ref{tab:after_normalization}. Depending on how the data was skewed, we used five different techniques to standardize them as much as possible, as summarised in the latter table. 
We perform no change to features that are left skewed or that have a skewness less than 0.5 close to 0. As a default, for features with a higher right skewness (>0.5), we will perform a log() transform. In some cases, this can result in negative values, more specifically when the original values are <1, hence we cannot simply apply log(). We discuss the cases in which that happened and how we accounted for this: 

\begin{itemize} \setlength\itemsep{0em} \setlength{\parskip}{0pt}%
    \item %2. sqrt
MVRV, miner\_inflow\_mean\_ma7 and exchange\_mean\_ma7 have a skewness of 0.875, 1.55, and 1.16. Since all three of them have values in the range of 0.6 to 4, taking the logarithm would introduce negative values, therefore, we took the square root. 

\item %3. cube root
The features HL\_sprd, miner\_inflow\_mean, exchange\_inflow\_mean, exchange\_outflow\_mean 
and miner\_outflow\_mean, have a higher maximum value (>5) and skewness (>3). Hence, we perform a slightly stronger transformation and take the cube root, so as the make the maximum values closer to 1. 


\item 
%5. +1 log
Both the etom\_flow\_mean and mtoe\_flow\_mean features have a high skewness value of 17.19 and 20.48.
Since their minimum value is below 1 (0.0769 and 0.298), we first add a value of 1 to them and then take the logarithm. 

\item 
%4. 1/4
For the feature vol\_future, we took the power of $\frac{1}{4}$, to make the maximum value of 8.67 as close to 1 (threshold value) as possible. Given that this is our forecast variable, it was important to standardize this as good as possible. 

\end{itemize}




 





% \subsection{Stationary Checking}

% We use a significant level alpha of 0.05 and we check that the $p$-value for both our Returns and log-returns 
% are significantly smaller than alpha, which means that there is enough evidence to reject the null hypothesis.
% Hence, returns and log-returns both are not dependent on time or any trend.








\subsection{Volatility}

\subsubsection{Calculating volatility}
We calculated the daily volatility $V_{annualised}$ for our dataset using the formula below. 

\begin{equation}
    log-returns = x_i = ln(\frac{C_i}{C_{i-1}})
\end{equation}
\begin{equation}
    V_{annualised} =   \sqrt{ \frac{\sum\limits_{i=1}^N (x_i-\mu)^2 \times 365 }{N}}   
\end{equation}


\textbf{where:}\\ 
\[\textit {$\mu$} =\text{mean of log-returns}\]
\[\textit {$C_i$} = \text{closing price of day $i$}\]
\[\textit {$C_{i-1}$}  = \text{closing price of day $i-1$}\]
\[\textit {$N$} =\text{number of days}\]

As shown in Figure~\ref{fig:volatility_transformation}, the daily volatility in our dataset is in the range of 0.000234 to 8.67. This results in a long-tailed distribution (see Figure~\ref{fig:volatility_distribution}) with a skewness of 3.35. Since statistical learning models typically work better with normally distributed data, we apply a transformation to the volatility data by taking the power of $\frac{1}{4}$. This results in a distribution with a skewness of -0.001 and a volatility range of volatility of 0.124 to 1.72. 



\begin{figure}[htbp!]
  \centering
    \includegraphics[width=.8\textwidth]{diagrams/train_val_test.png}
    \caption{Daily Volatility before and after transformation.}
    \label{fig:volatility_transformation}
\end{figure}

\begin{figure}[htbp!]
  \centering
    \includegraphics[width=.8\textwidth]{diagrams/vol_dist_scaled.png}
    \caption{Volatility distribution before and after transformation.}
    \label{fig:volatility_distribution}
\end{figure}



% \begin{figure}[]
    
%     \centering
%     \includegraphics[width=15cm]{volatility_spikes.png}
%     \caption{Volatility spikes}
% \end{figure}





\subsubsection{Volatility spikes}
%\dorien{talk about what you consider a peak: cutoff is how much? And how did you do your training %/ test set split and how is the class distribution (volatility peak or not in this distribution? %Add a table or graph. Maybe overlay spies on a Bitcoin price chart? }

%\dorien{polish the below:}
As shown in Figure~\ref{fig:volatility_transformation}, we classify days with a volatility $\ge$ 1.0 and with positive log-returns of the closing price as a \textit{volatility spike}. We set this volatility threshold to 1, because after applying the preprocessing transformation to the volatility (taking the power of $\frac{1}{4}$), all of the high volatilities with a magnitude > 1 will still be greater than 1 even though their magnitude has shrunk, and all of the low volatilities with a magnitude < 1 will still remain < 1.  There are 232 volatility spikes in the training set and 38 volatility spikes in the validation set. In the test set, there are 60 volatility spikes.






\subsubsection{Feature correlation with volatility}


To explore which of the (input) features from our dataset may be most correlated with the next-day volatility, and thus most important for our predictive model, we calculated several correlation metrics. Table~\ref{tab:corr} shows the R$^2$, and the Pearson as well as the Spearman correlation coefficients. We can see from the table that some features, such as volume, exchange\_inflow\_total and High\_Low\_Spread show a high correlation with the volatility. This indicates that these features will likely be important to improve our model's predictive power. This will later be verified by doing a Captum analysis in Section~\ref{tab:captum_feature} to explore the importance of each feature in our predictive model. 




% \end{minipage} \hfill
% \begin{minipage}{0.5\textwidth}
% \begin{tabular}{lccc}
% \toprule
% \textbf{Feature } & \textbf{R2} & \textbf{Pearson}  & \textbf{Spearman}\\
% \midrule
% %\vdots & \vdots & \vdots & \vdots   \\


%  \bottomrule

% \end{tabular}

% \end{minipage}
% \caption{Correlation between volatility and different input features. Only the features with an absolute value greater than 0.1 are included in the table}
% \label{tab:corr}
% \end{table}
% \end{landscape}








\section{Proposed Synthesizer Transformer}
\label{sec:4}

In this paper, we leverage a new type of Transformer, the Synthesizer Transformer~\citep{tay2021Synthesizer}. To properly understand our architecture, we first provide an overview of the Vanilla Transformer architecture upon which our proposed model is based. 


\subsection{Transformer architecture}
\label{sec:model}


%\begin{figure}[]
%    \centering
%    \includegraphics[width=5cm]{Transformer_timeseries.drawio.png}
%    \caption{Transformer Architecture}
%\end{figure}







The architecture used in this paper draws inspiration from the Generative Pre-trained Transformer 2 (GPT-2)'s decoder-only Transformer \citep{radford2018improving}, as shown in the Figure~\ref{fig:Transformer_architecture}. In this architecture, the input to the Transformer is a multivariate time series. The decoder takes the masked target sequence so that at each time step the decoder can attend to the previous $i$ time steps. This is illustrated in Figure \ref{fig:Transformer_attention} where the first input $X_1$ will result in a prediction for the next time step: $X_2$'. In the next step, the decoder is given the ground truth $X_1$ and $X_2$ values to predict $X_3$' and so forth. Therefore, at every new step, the model receives all the true inputs prior to predicting its next output, whereby each output token contributes equally to the training loss.




\begin{figure}[htbp!]
    \centering 
      \begin{subfigure}[t]{.49\linewidth}
    \includegraphics[width=0.8\textwidth]{diagrams/kw_Transformer.drawio2.png}
    \caption{The proposed decoder-only Transformer architecture inspired by \citep{radford2018improving}.}
    \label{fig:Transformer_architecture}
      \end{subfigure}
      \begin{subfigure}[t]{.49\linewidth}
    \includegraphics[width=1\textwidth]{diagrams/self_attention_mechanism.drawio.png}
    \caption{Transformer self-attention flows. The arrows indicate which inputs are received for making each prediction based on a time series $X$. }
    \label{fig:Transformer_attention}
      \end{subfigure}
      \caption{Insights into the used Transformer architecture.}
\end{figure}




For every output token, the self-attention score measures the importance of looking at each of the tokens previously seen in the sequence, for predicting the current token. In this traditional attention model (left in Figure \ref{fig:self_attention_mechanism}), the formula to calculate the attention score is provided in Equation~\ref{eq:attention}, and involves computing the dot product between the query vector ($Q$) and the key vector ($K)$ of the current token. For details, the reader is referred to \citet{radford2018improving}. 




\begin{equation}
    \textit{Attention(Q, K, V)} = softmax(\frac{QK^T}{\sqrt{d_k}})V
    \label{eq:attention}
\end{equation}

whereby $\sqrt{d_k}$ represents both the dimension of the key vector $K$ as well as the query vector $Q$, and $V$ is the value vector.

%\[MultiHead(Q, K, V) = Concat(head_1, ..., head_h)W^O\]

%\[head_i = Attention(Q W^Q_i, K W^K_i, V W^V_i)\]




%\[FFN(x) = max(0,xW_1 + b_ 1)W_2 + b_2\]











%\textbf{DH: split this up in some subsubsections or paragraphs about clear themes. Add a model architecture figure too in which you show the full flow (input dimensions, output dim, layers, task, etc.)}







\subsection{Synthesizer Transformer}

 \citet{tay2021Synthesizer}'s Synthesizer Transformer  is able to learn attention weights synthetically, without token-token interaction. This increases the speed of the Transformer by up to 60\%. Synthesizer Transformers can do this by removing the notion of query-key-values in the self-attention calculation and instead directly synthesizing the attention matrix. This is done using input $X_{h,l} \in\mathbb{R}^{N \times d}$ , where $h$ is the number of heads, $l$ is the sequence length and $d$ is the dimensionality of the model. This eliminates the need to calculate the dot product attention as described in the previous subsection. In their original paper,  \citet{tay2021Synthesizer} propose several synthetic attention variants, in this work, we implemented some of the best performing variants: dense, random, both of their factorized version, as well as a combination of dense and random with the Vanilla Transformer attention. 


\begin{figure}[htbp!]
    \centering
    \includegraphics[width=1\textwidth]{diagrams/self_attention_mechanism2.drawio.png}
    \caption{Types of self-attention mechanisms. On the left side, the traditional self-attention mechanism is depicted. The Dense and Random Synthesizer attention mechanism are shown next to it. Figure inspired by \citet{tay2021Synthesizer}.}
    \label{fig:self_attention_mechanism}
\end{figure}



\noindent\textbf{Dense}

This type of dense synthetic attention uses a two-layer feed-forward network with ReLU activation to replace the traditional dot product attention. The attention matrix is simply learned by the dense neural network. 


\begin{equation}
     \textit{Attention(V)} = Softmax(W_{2}(\sigma_{R}(W_{1}(X_{h,l})))V
\end{equation}

whereby $W_{2}$ and $W_{1}$ are feed-forward layers and $\sigma_{R}$ is a ReLu function
\newline



\noindent\textbf{Random}\\

The random synthetic attention mechanism does not rely on pairwise token interactions or any information from individual tokens. This way, it aims to capture a global task-specific alignment that obtains good results across a large number of samples. The attention is calculated as follows: 
%\[Y_{h,l} = Softmax(R_{h,l})G_{h,l}(X_{h,l})\]


\begin{equation}
     \textit{Attention(V)} = Softmax(R)V
\end{equation}

whereby $R$ is a randomly initialised $N \times N$ matrix. The weights in this matrix are then optimized during training. 
\newline



\noindent\textbf{Factorized Dense and Random}

The number of parameters added to the network in the above variations is $d \times N$ and  $N \times N$ respectively. When the sequence length is large, these synthetic attention models can be slightly harder to train. Hence, we also included factorized variations, which allow the models to perform competitively in practice. In addition, this form of attention also seems to help prevent overfitting. For details on how to calculate attention the reader is referred to \citet{tay2021Synthesizer}.




% The dense factorised version of attention can be described as follows: 

% \begin{equation}
%      \textit{Attention(V)} = Softmax(H_{A}(X_{h,l})H_{B}(X_{h,l}))V
% \end{equation}

% whereby $A$ X $B$ = $N$. $H_{A}$ $\in\mathbb{R}^{A \times B}$ and $H_{B}$ $\in\mathbb{R}^{B \times A}$ are tiling function which duplicates the vector k times. k is a small value that is normally set less than 10.\\




% \begin{equation}
%      \textit{Attention(V)} = Softmax(R_{1}R^{T}_{2})V
% \end{equation}

% where $R_{1},R_{2} \in\mathbb{R}^{N \times K}$ are low rank random initialised matrices.\\





\noindent\textbf{Mixture dense and random}

All of the proposed synthetic attention variants can be mixed in an additive fashion. This results in mixture Synthesizer Transformers (mix). In this work, we experiment by mixing a dense Synthesizer Transformer and a Vanilla Transformer (mix dense) as well as a random Synthesizer and Vanilla Transformer (mix random). The resulting attention is calculated as the sum of the attention calculated by the Vanilla Transformer and the selected Synthesizer Transformer's attention. 


% \begin{equation}
%      \textit{Attention(Q, K, V)} = Softmax(W_{2}(\sigma_{R}(W_{1}(X_{h,l})) + \frac{QK^T}{\sqrt{d_k}})V
%     \label{eq:mix_dense}
% \end{equation}


% %Y_{h,l} = Softmax(\alpha_{1,h,l}S_{1,h,l}(X_{h,l})+...\alpha_{N,h,l}S_{N,h,ell}(X_{h,l}))G_{h,l}(X_{h,l})

% \noindent\textbf{Mix Random(Random Synthesizer with Vanilla Transformer)}


% %Y_{h,l} = Softmax(\alpha_{1,h,l}R_{1,h,l}R^{T}_{2,h,l}+\alpha_{2,h,l}F_{h,l}(X_{h,l}))G_{h,l}(X)

% \begin{equation}
%      \textit{Attention(Q, K, V)} = Softmax(R + \frac{QK^T}{\sqrt{d_k}})V
%    \label{eq:mix_random}
% \end{equation}






\section{Volatility prediction}
\label{sec:5}

\subsection{Experimental setup}

% \dorien{So what does the experiment test? }

% - hyperparameter tuning
% - for best settings, show all the metrics
% - interpretability
% anything else? 

%\dorien{talk about objectives of the experiment, high level description}

%What types of experiments will you run (volatility prediction - model tuning - ablation study - find most important features - backtesting == these will be the subsections of your Results section)
We conduct a thorough experiment to evaluate the performance of the volatility prediction Synthesizer Transformer models (with different attention mechanisms) and compare it to existing baseline models: Vanilla Transformer, LSTM, and GARCH. We first perform hyperparameter optimization using the validation set. The final results using the best parameters are reported on the test set. After finding the best model, we use Captum, a PyTorch library for model interpretability, to identify the input features that contribute most to the prediction result. 

We evaluate the models on two tasks: predicting calculated volatility (regression) and predicting volatility spikes (classification). The latter is accomplished by converting the predicted volatility values into two classes: `volatility spike' and `non-volatility spike'. A prediction is considered to be a volatility spike when the predicted volatility is greater than or equal to 1 and the log-returns were positive, otherwise we label it as `non-volatility spike'. 


% Y \(\geq\) 1 to positive class (high volatility) and Y < 1 to be negative class (low volatility). 



\subsubsection{Training-Test Split}

We train our models using the dataset described in Section~\ref{sec:3}, split into a training, validation, and test set as described below: 

\begin{itemize} \setlength\itemsep{0em} \setlength{\parskip}{0pt}%
\item Complete dataset: 02/01/2016 to 21/09/2021 (2,090 days).
\item Training set: 02/01/2016 to 02/01/2020 (1,462 days) (70\%).
\item Validation set: 03/01/2020 to 11/11/2020 (314 days) (15\%). 
\item Test set: 12/11/2020 to 21/09/2021 (314 days) (15\%).
\end{itemize}


There are a total of 232 volatility spikes in the training set and 38 volatility spikes in the validation set. In the test set, there are 46 volatility spikes. We should note that the non-stationarity of financial data is a known issue \citep{de2018advances}. Ideally, we would train and test with a rolling time frame over our entire dataset, however, due to the fact that the Transformer model needs as much data as possible, we use an out-of-time test set.


%The ground truth that we are comparing to is shown in Figure~\ref{fig:volatility_transformation}, where we labelled a day to be a volatility spike when both the volatility was greater than or equal to 1 \textit{and} the log-returns were positive. This means that we want our model to predict not only high volatility on the next day but also an increase in volatility on the next day.



%\textcolor{red}{maybe put table with hyperparameters here and discuss. }






\subsubsection{Baseline comparison models}

Since we are working with a new dataset, there are no existing benchmarks available to directly compare our results to. In order to overcome this, we trained a few baseline models: a Vanilla Transformer, long-short term memory model (LSTM), and GARCH. 


The Vanilla Transformer is the same architecture as our proposed Synthesizer Transformer, but uses the original attention mechanism as per Subsection~\ref{sec:model}. Secondly, long-short term memory models (LSTMs)~\citep{hochreiter1997long} are a type of recurrent neural network that are known for their ability to capture long-term dependencies in time series data as well as avoid the vanishing gradient issue~\citep{chuan2018modeling}. The full configuration of the networks used as baseline is described in Subsection~\ref{sec:tuning}. Finally, we also explore a statistical model often used in time series analysis: Generalized AutoRegressive Conditional Heteroskedasticity, or GARCH~\citep{li2002recent}. This model extends the Autoregressive Conditional Heteroskedastic Models (ARCH) model, by including a moving average component (ma) joint with the autoregressive component. This model is often used for volatility prediction, even for Bitcoin ~\citep{dyhrberg2016bitcoin}. As our baseline model, we use GARCH(1,1), which is the first order GARCH model using the ARCH library in Python \footnote{\url{https://github.com/bashtage/arch}}. 

% the conditional
% change in variance over time as well as changes in the time-dependent variance. 
% Specifically, the model includes lag variance terms together with lag residual errors from a mean process.


% p is the number of lag variances and q is the number of lag residual errors to include in the GARCH model. 
% We would be using GARCH(1, 1) which is the first order GARCH model.







\subsubsection{Evaluation Metrics}

We use several metrics to evaluate the volatility prediction models: root mean square error (RMSE), F1-score, precision, and recall. The first metric looks directly at the regression results, the others look at the resulting predicted volatility spikes (classification). 
% We use root mean square error (RMSE) as shown in Equation~\ref{eq:rmse} as the evaluation metric for our models on volatility prediction since we want to train the model to predict volatility spikes because RMSE is 
For the regression evaluation, we opted to use RMSE as it is more sensitive to prediction errors with a large difference from the ground truth. 


%This is because the error is squared before the average is reduced with the square root so the example with the largest error would skew the RMSE. 

% \begin{equation}
%     RMSE =\sqrt{\frac{1}{n}\Sigma_{i=1}^{n}{\Big(\frac{d_i -f_i}{\sigma_i}\Big)^2}}
%     \label{eq:rmse}
% \end{equation}

% However, in this case, investors focus on high volatility spikes rather low volatility periods as it means higher risk which would be a warning.All our models are regression based but since this journal is about 



When evaluating volatility \textit{spike} prediction, we need to take into account that our (test) dataset is not balanced as there are fewer volatility spikes (60) than non-volatility spikes (254). We use precision to see how many correctly predicted spikes (TP) the model predicted correctly out of all predicted spikes (TP+FP). 

\begin{equation}
    Precision = \frac{TP}{TP+FP}
    \label{eq:precision}
\end{equation}

Recall complements precision by measuring how many spikes the model predicted correctly out of the actual spikes. 

\begin{equation}
    Recall = \frac{TP}{TP+FN}
    \label{eq:recall}
\end{equation}

In addition, the F1-score provides an integrated metric as the harmonic mean between precision and recall. Overall, a balance of high recall and high precision is preferred because it assumes that the model is well fitted, although it is possible to rely solely on either recall or precision depending on the use case. 

\begin{equation}
    F1-score = \frac{2*Precision*Recall}{Precision+Recall} = \frac{2*TP}{2*TP+FP+FN}
    \label{eq:f1}
\end{equation}






%\begin{figure}[htbp!]
%    \centering
%    \includegraphics[width=15cm]{diagrams/Volatility_prediction.png}
%    \caption{Volatility Prediction}
%    \label{fig:figure9}
%\end{figure}









\subsection{Hyperparameter tuning and implementation details}
\label{sec:tuning}

We set the sequence length of all Transformer models to be 64 and the weight decay to be $1\text{e}^{-6}$. We train all the neural network models using Adam optimizer with an initial learning rate of $1\text{e}^{-5}$. All Transformer models use early stopping with the maximum number of epochs set to 10,000 and a patience of 200 to prevent overfitting. In addition, we use the validation set to finetune the models' hyperparameters as displayed in Table~\ref{tab:hyperparameters_tested}. The resulting best parameter settings with the lowest RMSE loss on the validation set are displayed in Table~\ref{tab:best_parameters}. 


\begin{table}[h!]
\small
%\caption{Features}
    \centering
    \begin{tabular}{lll}
    \toprule
        Feature & LSTM models & Transformer models\\
        \midrule
        Number of layers & 1, 2, 4, 8 & 1, 2, 4, 8\\
        Number of hidden layers & 16, 32, 64, 128 & NA\\
        Number of heads & NA & 2, 4, 8 \\
        Batch size & 4, 8, 16, 32, 64   & 4, 8, 16, 32, 64  \\
        Dropout & 0.1, 0.2  & 0.1, 0.2\\
        %Attention Type & normal, Synthesizer (Dense, Random, Factorized Dense, Factorized Random, Mix Dense, Mix Random)\\
           \bottomrule
    \end{tabular}
    \caption{An overview of the hyperparameters tested for different neural network architectures.}
    \label{tab:hyperparameters_tested}
    
\end{table}



\begin{table}[h!]
%\caption{Hyperparameters tested and best setting for the different models }
    \centering
\scriptsize
    \begin{tabular}{ll}
    \toprule
        Model & Best hyperparameter settings \\
        \midrule
        LSTM  & batch size=4, dropout=0.2, hidden layer=64, layers=8 \\
        Transformer (V) & batch size=4, dropout=0.2, heads=4, layers=2\\
        Synthesizer (R) &  batch size=4, dropout=0.2, heads=4, layers=4\\
        Synthesizer (FR) &  batch size=4, dropout=0.2, heads=4, layers=8\\
        Synthesizer (D) &  batch size=4, dropout=0.2, heads=8, layers=4\\
        Synthesizer (FD) & batch size=4, dropout=0.2, heads=4, layers=4\\
        Synthesizer (MD) &  batch size=4, dropout=0.1, heads=2, layers=4\\
        Synthesizer (MR) & batch size=4, dropout=0.1, heads=8, layers=2\\
           \bottomrule
    \end{tabular}
    \caption{The best hyperparameters based on the validation set, for the different Transformer models. We use R for random, F for factorised, M for mixed, D for dense, and V for Vanilla models.}
    \label{tab:best_parameters}
\end{table}








\subsection{Volatility prediction results}

The results for predicting next-day volatility are displayed in Table~\ref{tab:model_prediction_results}. The left column displays the RMSE for the regression problem (predicting next-day volatility). We then used a threshold $T$, to determine if a volatility spike was predicted. Our default value for $T$ is 1, and for this value we show the F1-score, precision, and recall in the table. We also included the number of True Positives and False Negatives for a few other thresholds to gain insight in how to improve the prediction certainty in Table~\ref{tab:threshold}. 

From the table, we can see that many of the proposed Synthesizer Transformer models perform well, both in terms of the F1-score (which is consistently above 0.377) as well as RMSE (which is close to 0.1). The baseline LSTM model as well as the Vanilla Transformer consistently perform worse with F1-scores of 0.1714 and 0.2857 respectively. We also ran a basic GARCH(1,1) model which does not perform very well. Since the predictions were too low, no spikes were detected, leaving the precision and recall as zero. We can speculate that GARCH is not the most appropriate model for our task definition. This is in line with the findings of \citet{naimy2018modelling}, who find that GARCH is not well suited in a high-volatility context. 

When comparing the different types of Synthesizer Transformers, the dense model has a slightly better performance, with the model with factorized dense attention obtaining 0.101 in RMSE and 0.4625 in F1-score. In general, the factorised models slightly outperform the non-factorized models in terms of the F1-score. 

\begin{table}[h!]
\hspace*{-0.8cm}
    \centering
    \scriptsize 
    \begin{tabular}{l|c|ccc|cccccc|cc}
    \toprule
            Model & RMSE & F1-score & Precision & Recall  &  TP & FN & TN & FP \\
             % &  &  &  &   &  \multicolumn{2}{c}{T $\ge$ 1.3}   & \multicolumn{2}{c}{T $\ge$ 1.2}  & \multicolumn{2}{c}{T $\ge$ 1.1} & \multicolumn{2}{c}{T $\ge$ 1.0}  \\
            
            \midrule
            GARCH(1,1) & 0.303  &0.000 &0.000 &0.000 & 0 & 60 &254 & 0\\
            
            LSTM   &0.095 &0.171 &0.600  &0.100 &6 &54 & 250 &4\\
            
            Transformer (V) & 0.095  &0.286 &0.500 &0.200    &12 &48 &242 &12\\
            
            Synthesizer (R) &0.114   &0.374 &0.303 &0.500    &30 &30& 185& 69\\
            
            Synthesizer (FR)  &0.123   & 0.414 &0.316 &0.600   &36 &24& 176 &78\\
            
            Synthesizer (D)  & 0.103  &0.448 &0.405 &0.500  &30 &30 & 210& 44\\
            
            Synthesizer (FD)  &0.101   &0.463 &0.370 & 0.617   &37 &23& 191 &63\\
            
            Synthesizer (MD)  & 0.100  &0.385 &0.429 &0.350  &21 &39& 226& 28\\
            
            Synthesizer (MR) & 0.101  &0.400 &0.400 &0.400  &24 &36& 218& 36\\
           \bottomrule
    \end{tabular}
    \caption{Model Prediction Results for predicting volatility  as regression (RMSE), and as a classification task (F1-score etc.). The True/False Positive/negative (TFPN) results for predicting extreme volatility spikes are also displayed in the last columns. We use R for random, F for factorised, M for mixed, D for dense, and V for Vanilla models.}
    \label{tab:model_prediction_results}
    \hspace*{2cm}
\end{table}


We included different values for our classification threshold $T$ and reported TP and FN values in Table~\ref{tab:threshold}. We see that if we want to have a higher certainty for true positives and a lower chance of false negatives, then setting a higher threshold can help us achieve this. Looking at the Synthesizer (FD), a threshold of 1.2 can help us obtain a recall of 0.85714 (6/(1+6) compared to the original 0.370. This means that we correctly predict 6 our of 7 (larger) volatility spikes. Even with a threshold of 1.1, the Synthesizer Transformer correctly predicts more than 50\% of the volatility spikes. 



% ORIGINAL TABLE
% \begin{table}[h!]
% \hspace*{-0.8cm}
%     \centering
%     \scriptsize 
%     \begin{tabular}{l|c|ccc|cccccc|cc}
%     \toprule
%             Model & RMSE & F1-score & Precision & Recall  &  TP  & FN   & TP   & FN  & TP   & FN &TP &FN  \\
%              &  &  &  &   &  \multicolumn{2}{c}{T $\ge$ 1.3}   & \multicolumn{2}{c}{T $\ge$ 1.2}  & \multicolumn{2}{c}{T $\ge$ 1.1} & \multicolumn{2}{c}{T $\ge$ 1.0}  \\
            
%             \midrule
%             GARCH & 0.303  &0 &0 &0 &0 &1 &0 &7 &0 &29 &0 &60\\
            
%             LSTM   &0.0946 &0.171 &0.600  &0.100 &1 &0 &2 &5 &4 &25 &6 &54\\
            
%             Transformer (V) & 0.0948  &0.286 &0.500 &0.200    &1 &0 &4 &3 &6 &23  &12 &48\\
            
%             Synthesizer (R) &0.114   &0.374 &0.303 &0.500   &1 &0 &5 &2 &18 &11  &30 &30\\
            
%             Synthesizer (FR)  &0.123   & 0.414 &0.316 &0.600   &1 &0 &6 &1 &21 &8  &36 &24\\
            
%             Synthesizer (D)  & 0.103  &0.448 &0.405 &0.500    &1 &0 &6 &1 &17 &12  &30 &30\\
            
%             Synthesizer (FD)  &0.101   &0.463 &0.370 & 0.617  &1 &0 &6 &1 &21 &8  &37 &23\\
            
%             Synthesizer (MD)  & 0.100  &0.385 &0.429 &0.350   &1 &0 &4 &3 &15 & 14 &21 &39\\
            
%             Synthesizer (MR) & 0.101  &0.400 &0.400 &0.400    &1 &0 &4 &3 &16 &13  &24 &36\\
%            \bottomrule
%     \end{tabular}
%     \caption{Model Prediction Results for predicting volatility  as regression (RMSE), and as a classification task (F1-score etc.). 
% The True/False Positive/negative (TFPN) results for predicting extreme volatility spikes with different thresholds are also displayed in the last columns. We use R for random, F for factorised, M for mixed, D for dense, and V for Vanilla models.}
%     \label{tab:model_prediction_results}
%     \hspace*{2cm}
% \end{table}




\begin{table}[h!]
\hspace*{-0.8cm}
    \centering
    \scriptsize 
    \begin{tabular}{l|cccccc}
    \toprule
            Model   TP  & FN   & TP   & FN  & TP   & FN & TP \\
             &  \multicolumn{2}{c}{T $\ge$ 1.3}   & \multicolumn{2}{c}{T $\ge$ 1.2}  & \multicolumn{2}{c}{T $\ge$ 1.1}  \\
            
            \midrule
            GARCH &0 &1 &0 &7 &0 &29 \\
            
            LSTM   &1 &0 &2 &5 &4 &25 \\
            
            Transformer (V)    &1 &0 &4 &3 &6 &23 \\
            
            Synthesizer (R)   &1 &0 &5 &2 &18 &11 \\
            
            Synthesizer (FR)    &1 &0 &6 &1 &21 &8 \\
            
            Synthesizer (D)   &1 &0 &6 &1 &17 &12  \\
            
            Synthesizer (FD)    &1 &0 &6 &1 &21 &8  \\
            
            Synthesizer (MD)    &1 &0 &4 &3 &15 & 14 \\
            
            Synthesizer (MR)     &1 &0 &4 &3 &16 &13 \\
           \bottomrule
    \end{tabular}
    \caption{True positive (TP) and false negative (FN) when predicting extreme volatility spikes with different thresholds. We use R for random, F for factorised, M for mixed, D for dense, and V for Vanilla models.}
    \label{tab:threshold}
    \hspace*{2cm}
\end{table}

% TN FP

%LSTM 250 4

%Transformer 242 12

%Dense 210 44

%Random 185 69

%Fac Dense 191 63

%Fac Random 176 78

%Mix Dense 226 28

%Mix Random 218 36



%  \begin{table}[htbp!]
%  %\footnotesize
% \centering

% \begin{tabular}{lcccccc}
%     \toprule
%     \newcommand{\tblmultiline}
% \multirow{Extreme Volatility Spikes} 
%         & \multicolumn{2}{c}{1.3} & \multicolumn{2}{c}{1.2} & \multicolumn{2}{c}{1.1}   \\
%     \cmidrule(lr){2-3} \cmidrule(lr){4-5} \cmidrule(lr){6-7}
%         & TP  & FN   & TP   & FN  & TP   & FN  \\
%     \midrule
%      GARCH    &0 &0 &0 &0 &0 &0  \\
%      LSTM   &0 &0 &0 &0 &0 &0 \\
%     Transformer  &1 &0 &3 &2 &5 &19  \\
%     Synthesizer (Random)  &1 &0 &4 &1 &20 &4  \\
%     Synthesizer (Factorized Random)  &1 &0 &4 &1 &18 &6  \\
%     Synthesizer (Dense) &1 &0 &4 &1 &17 &7  \\
%     Synthesizer (Factorized Dense)  &1 &0 &4 &1 &18 &6  \\
%     Synthesizer (Dense + Vanilla) &1 &0 &4 &1 &17 &7  \\
%     Synthesizer (Random + Vanilla) &1 &0 &3 &2 &12 &12  \\
%     \bottomrule
% \end{tabular}

%\caption{Extreme Volatility Spikes Results \\
%TP: True Positive FN: False Negative\\
%above 1.3 (total: 1)
%above 1.2 (total: 5)
%above 1.1 (total: 24)
%above 1.0 (total: 46) (recall)}
%\label{tab:volatility_spike_results}
 %   \end{table}











\subsection{Model explainability}
%\begin{landscape}


To gain insight into which features are important for predicting volatility, we used the Captum library for model interpretability. More specifically, we used the feature ablation function \citep{kokhlikyan2020captum} to understand important features that contribute to the prediction of each of the models. Table~\ref{tab:captum_feature} shows the top 3 features in terms of the absolute value of the weight attribute score based on the feature ablation attribution algorithm for each of the models. 

The absolute value of the score, informs us about the importance of this feature for predicting the next-day volatility. Some notable recurring features are important across different models based: taker\_buy\_volume, HL\_spread and volume. Looking back at the initial correlation analysis that we performed in Table~\ref{tab:corr}, we confirm the importance of HL\_spread and volume for volatility prediction since they have the highest correlation with vol\_future. 

The feature called taker\_buy\_volume refers to the volume of perpetual swap trades that market takers buy (and vice versa for taker\_sell\_volume). Being a `taker' indicates someone who buys or sells at the market price. When the takers' buy volume is much larger than the takers' sell volume, this indicates a bullish movement. Other important features include exchange\_outflow\_mean\_ma7 and exchange\_transactions\_count\_inflow. An increase in the latter indicates that more people are active in exchange flows which in turn indicates an increase in interest, leading to an increase in volatility.


% This indicates that more buyers are willing to purchase Bitcoin at a higher price based on the bid-ask spread, which means that there is a stronger buying pressure over selling pressure, which might result in an increase in price.

% When taker sell volume is much larger than taker buy volume, it signals a bearish sentiment. This indicates that more selling willing to sell Bitcoin at a lower price on the bid ask spread, meaning that there is a stronger selling pressure over buying pressure, resulting in a decrease in price.



Looking at the features that we extracted from Twitter, we find that our variables related to whale transactions also come out as being important with most of them listed as the 10th or 20th most important feature.  The most important is the USDminus, which is the 4th most important feature for the Synthesizer Transformer (FD) with an ablation score of -0.0398. This feature is also shown as the 12th most important feature for the Synthesizer Transformer (MD). 




%Any features from Twitter whales? 
%Transformer USDminus	-0.014670354 (21)
%dense USDplus	-0.02213909 (17)
%random BTCplus	-0.014366423 (22)
%fac dense USDminus	-0.03981128 (4)
%fac random USDminus	0.009277673 (32)
%mix dense USDminus	-0.026125401 (12)
%mix random USDplus	-0.015808647 (22)





%short\_liquidations\_usd     - Total Amount in short leveraged positions that are forced to exit caused by price volatility


%fund\_flow\_ratio   - Amount of Bitcoin that exchanges own among the amount of Bitcoin sent\\
%CO\_sprd   - Close-open spread



 \begin{table}[htbp!]

\hspace*{-1.6cm}
%\centering
% \footnotesize
\scriptsize
\begin{tabular}{llclclc} 
 
 \toprule
Model & Feature 1 & Score & Feature 2 &Score& Feature 3 & Score\\ 

 \midrule

V &HL\_sprd&0.09&volume&0.07 &funding\_rates &-0.06\\


R &taker\_sell\_volume&0.09&HL\_sprd&0.09 &taker\_buy\_volume&0.07 \\


D &exchange\_outflow\_mean\_ma7&-0.07&close&0.05&HL\_sprd&0.05\\


FR &HL\_sprd &0.08 &taker\_buy\_volume & 0.08&taker\_sell\_volume &0.08 \\

FD &close&0.07&volume &0.05 &exchange\_transactions\_count\_inflow&0.05\\


MD &taker\_buy\_volume&0.08&taker\_sell\_volume &0.06 &volume &0.06  \\

MR &volume &0.08&HL\_sprd&0.06 &taker\_buy\_volume&0.06 \\

\bottomrule
\end{tabular}
% \hspace*{2.5cm}
\caption{The top 3 most important features of the Transformer models according to Captum's feature ablation function with their attribute score. \\}
\label{tab:captum_feature}
\end{table}
%\end{landscape}


%We train the model with an early stopping criteria, hence we are unable to properly compare the training speed of the models. 







\section{Trading strategy experiment}
\label{sec:6}

In order to evaluate the usefulness of the volatility model, we implemented a few simple trading strategies that take signals from the volatility prediction model, and backtested them. It is worth noting that these strategies are very basic, and can undoubtedly be improved. They solely serve to show whether our predicted volatility metric can help increase our risk-adjusted profits. 



\subsection{Backtesting strategies}

We used the predicted volatility (for each model) and used it as a signal for our strategies. For all of the strategies, we start with an initial capital of \$10,000. Each buy signal will be 5\% of the remaining capital, with pyramiding. Trading costs were set to 0.1\% for this experiment which is relatively higher than many exchanges. The backtesting was performed using the Backtrader library in Python\footnote{\url{www.backtrader.com}}. 

% In our proposed strategies, we try to take advantage of high liquidation during period of high volatility.


We test each of the strategies with and without \textbf{volatility scaling} for setting the position size. As explained above, the strategies typically open a position by buying a fixed percentage of total capital (5\%). With volatility scaling, they open a position by buying 5\% of capital $times$ volatility. This means that when the volatility is higher, we are trying to gain an edge by using a higher percentage of capital to open a position. \citet{hoyle2018volatility} suggest that volatility scaling can potentially improve the Sharpe ratio of the returns. The four strategies that we tested are described below. 

\subsubsection{Buy-and-hold}

This baseline strategy buys Bitcoin at the start and holds it until the very last day. Due to its constant market exposure, we can expect a higher risk, with, during long enough certain periods, higher returns. 


\subsubsection{Buy-low-sell-high}

An often used strategy is to buy when prices are low, and sell when they are high. We modified this idea to buy when volatility is low ($V < 1$) and there is a decrease in log-returns, and sell when a volatility spike is detected ($V \ge 1$), regardless of the price. 


\subsubsection{Momentum}

The proposed Momentum strategy will buy when a volatility spike is predicted and there is an increase in log-returns over the past 2 days. The position will close the next day. 


\subsubsection{Mean Reversion}


The proposed Mean Reversion strategy will buy when a volatility spike is predicted and there is a decrease in the log-returns over the past 2 days. The position will close the next day. 





\subsection{Evaluation metrics}

We used the following metrics to evaluate our backtesting experiment:

\begin{description} \setlength\itemsep{0em} \setlength{\parskip}{0pt}%
    \item[Time in market] - The number of days for which a position was open. 


% \item[Sharpe Ratio] - By \citet{sharpe1998sharpe}, reflects the risk-adjusted return of an investment compared to a risk-free return at 0\%. as per Equation~\ref{eq:sharpe}


% \begin{equation}
%     \textit{Sharpe Ratio} = \frac{R_p - R_f}{\sigma_p}
%     \label{eq:sharpe}
% \end{equation}

% \text{where:}\\ 
% \[\textit {$R_p$} =\text{return of portfolio}\]
% \[\textit {$R_f$} = \text{risk-free rate}\]
% \[\textit {$\sigma_p$}  = \text{standard deviation of the portfolio's excess return}\]\\


\item[Max. Drawdown] - The maximum observed loss from the maximum portfolio value to a subsequent through value before a new maximum is attained (in percentage). 


\item[Kelly Criterion] - Determines the optimal theoretical positions size. 

% \begin{equation}
% \textit{K\%} = W - \frac{\left(1-W\right )}{R}
% \label{eq:kelly}
% \end{equation}
% \[\textit {K\%} = \text{The Kelly percentage}\]
% \[\textit{W} = \text{Winning probability}\]
% \[\textit{R} = \frac{Win}{loss} ratio  \] 

\item[Daily VaR(\%)] - Daily Value-at-Risk. The VaR reflects the potential loss within a day and a certain confidence level (95\%).


%\item[Profit factor] - The gross profit divided by the gross loss (including fees) for the entire trading period.


\item[PnL] The total profit and loss in percentage. 
    


\end{description}






\subsection{Backtesting results}

Table~\ref{tab:backtest_results} shows the result of our backtesting experiment. 
The Buy and hold strategy has a Profit and Loss (PnL) of 12.2\% for almost one year of holding. The disadvantage of such a strategy, is its constant market exposure, resulting in a high maximum drawdown of 13.66\%. Many investors may want to avoid such exposure and instead save fiat for bargain buying opportunities. The buy-low-sell-high strategy performs best in terms of PnL (24\%), especially with volatility scaling. This strategy, however, still has a very high time in the market, resulting in a max. drawdown ranging from  -15\% to -25\%. The Momentum strategy, on the other hand, shows a very low time in the market (less than  20\%), with a PnL between 2\% to 10\% for the different proposed Transformer models and a max. drawdown of less than 5\%. %and daily VAR of less than -1\% and both are the lowest among the 4 strategies.

% Mean reversion has an average time in market of about 16\% to 27\% and its PnL varies depending on the ML model, occasionally performing the best and its max drawdown and daily VAR is relatively lower than buy-low-sell-high.


In general, profit increases when volatility scaling is used. The Sharpe ratio, Kelly criterion and PnL generally all increase when using volatility scaling for position size, compared to unscaled position sizing. The risk, however, also increases in terms of Daily VaR and max. drawdown. Hence, investors and traders have to weigh the cost and benefits of volatility scaling and see whether are they comfortable adding more risk to their strategy so as to profit more. 






%\begin{landscape}

% \setlength\LTleft{-2cm}
{\scriptsize \begin{longtable}{lcccccc}
\\ \toprule 
  & Time In & Sharpe &Max &Kelly &Daily & \\
Model  & Market(\%)&  Ratio& Drawdown(\%)& Criterion(\%)& VaR(\%)&PnL(\%) \\
 \midrule
\endfirsthead

\multicolumn{7}{c}%
{{-- continued from previous page}} \\ %\bfseries \tablename\ \thetable{} 
\toprule 
  & Time In & Sharpe &Max &Kelly &Daily & \\
Model  & Market(\%)&  Ratio& Drawdown(\%)& Criterion(\%)& VaR(\%)&PnL(\%) \\
 \midrule
\endhead

\bottomrule \multicolumn{7}{r}{{Continued on next page}} \\
\caption{Backtesting Strategy Results. We use U for unscaled (no volatility scaling) position sizing, and S for volatility scaled position sizes. }
\label{tab:backtest_results}
\endfoot

% \hline \hline
\bottomrule \\
\caption{Backtesting Strategy Results. We use U for unscaled (no volatility scaling) position sizing, and S for volatility scaled position sizes. }
\label{tab:backtest_results}
\endlastfoot

% \begin{tabular}{lccccccc}
% \toprule
buy and hold & 100 & 0.8  & -13.66 & 6.84 & -1.27 & 12.2 \\
 \midrule
%Transformer & Time In Market(\%)& Sharpe Ratio&Max Drawdown(\%)&Kelly Criterion(\%)&Daily VAR(\%)&PnL(\%)\\
Transformer (V) & &  &&&&\\ \midrule

(U) buy-low-sell-high & 94.0 & 0.94  & -10.83 & 8.16 & -1.21 & 14.2\\
(S) buy-low-sell-high &94.0 &0.96  &-17.08 &8.4 &-2.0 &24.1 \\
(U) Momentum &2.0 &0.84  &-0.01 &43.76 &-0.01 &0.148 \\
(S) Momentum &2.0 &0.84  &-0.03 &43.7 &-0.03 &0.307 \\
(U) Mean Reversion   &9.0 &-0.14  &-2.56 &-2.4 &-0.31 &-0.571 \\
(S) Mean Reversion  &9.0 & 0.03 &-5.17 &0.55 &-0.71 &-0.006 \\
 \midrule


 %Random Synthesizer  Transformer & Time In Market(\%)& Sharpe Ratio&Max Drawdown(\%)&Kelly Criterion(\%)&Daily VAR(\%)&PnL(\%)\\
Synthesizer  Transformer (R)& &  &&&&\\
 \midrule
 
(U) buy-low-sell-high  &75.0 &1.06  &-7.76 &9.37 &-1.04 &14.1 \\
(S) buy-low-sell-high &75.0 & 1.0 &-12.83 &9.0 &-1.82 &22.9 \\
(U) Momentum &15.0 & 0.94 &-0.93 &10.65 &-0.21 &2.43 \\
(S) Momentum &15.0 &1.09  &-1.95 &12.13 & -0.48&6.58 \\
(U) Mean Reversion   &23.0 & -0.03 &-5.77 &-0.46 &-0.55 &-0.395 \\
(S) Mean Reversion  &23.0 &0.04  &-12.92 &0.55 &-1.31 &-0.361 \\
 \midrule


%Fac Random Synthesizer & Time In Market(\%)& Sharpe Ratio&Max Drawdown(\%)&Kelly Criterion(\%)&Daily VAR(\%)&PnL(\%)\\
Synthesizer  Transformer (FR) & &  &&&&\\
 \midrule
 
(U) buy-low-sell-high  & 70.0&0.59  &-13.37 &5.51 &-0.98 & 6.70\\
(S) buy-low-sell-high &70.0 &0.32  &-23.56 &3.12 &-1.77 &5.12 \\
(U) Momentum &17.0 &0.05  &-3.26 &0.72 &-0.28 &0.135 \\
(S) Momentum &17.0 &0.22  &-7.07 &2.8 &-0.64 &1.45 \\
(U) Mean Reversion   &27.0 &0.89  &-4.54 &9.79 &-0.7 &7.68 \\
(S) Mean Reversion  &27.0 &0.84  & -11.76&9.44 &-1.65 &16.8 \\
 \midrule

 
%Dense Synthesizer & Time In Market(\%)& Sharpe Ratio&Max Drawdown(\%)&Kelly Criterion(\%)&Daily VAR(\%)&PnL(\%)\\
 Synthesizer Transformer (D)& &  &&&&\\
 \midrule
 
(U) buy-low-sell-high  &78.0 &0.51  &-13.84 &4.65 &-0.98 &5.68 \\
(S) buy-low-sell-high &78.0 &0.41  &-24.03 &3.8 &-1.75 &7.25 \\
(U) Momentum &10.0 &0.81  & -1.5&12.13 &-0.19 &1.88 \\
(S) Momentum &10.0 &0.82  &-3.11 & 12.27&-0.39 &3.97 \\
(U) Mean Reversion   &23.0 &0.79  &-3.73 &9.61 &-0.71 &6.89 \\
(S) Mean Reversion  &23.0 &0.75  &-8.24 &9.22 &-1.48 & 13.2\\
 \midrule
 


%Fac Dense Synthesizer & Time In Market(\%)& Sharpe Ratio&Max Drawdown(\%)&Kelly Criterion(\%)&Daily VAR(\%)&PnL(\%)\\
Synthesizer Transformer (FD) r& &  &&&&\\
 \midrule
 
(U) buy-low-sell-high  &71.0 &0.19  &-14.59 & 1.89&-1.01 &1.71 \\
(S) buy-low-sell-high &71.0 &0.06  &-26.18 &0.59 &-1.88 &-0.753 \\
(U) Momentum &13.0 &1.54  &-1.36 &18.78 &-0.23 &4.55 \\
(S) Momentum &13.0 &1.56  &-2.64 &19.07 &-0.47 & 9.60\\
(U) Mean Reversion   &26.0 &1.03  &-3.72 &11.92 &-0.66 &8.67 \\
(S) Mean Reversion  &26.0 &1.03  &-8.39 &12.13 &-1.46 &19.2 \\
 \midrule


%Dense Mix Synthesizer & Time In Market(\%)& Sharpe Ratio&Max Drawdown(\%)&Kelly Criterion(\%)&Daily VAR(\%)&PnL(\%)\\
Synthesizer  Transformer (DM) & &  &&&&\\
 \midrule
 
(U) buy-low-sell-high  &85.0 &0.76  &-16.11&6.86 &-1.09 &9.96 \\
(S) buy-low-sell-high &85.0 & 0.77 &-25.36&6.95 &-1.78 & 16.21\\
(U) Momentum &4.0 &1.72  &-0.02& 48.36&-0.14 & 3.12\\
(S) Momentum &4.0 &1.72 &-0.05&48.37 &-0.3 &6.73 \\
(U) Mean Reversion  &16.0 &-0.04  &-6.99& -0.74&-0.58 &-0.483 \\
(S) Mean Reversion  &16.0 &0.01  &-15.37&0.2 & -1.32& -0.818 \\
 \midrule

%Random Mix Synthesizer & Time In Market(\%)& Sharpe Ratio&Max Drawdown(\%)&Kelly Criterion(\%)&Daily VAR(\%)&PnL(\%)\\
Synthesizer  Transformer (MR) & &  &&&&\\
\midrule

(U) buy-low-sell-high  & 83.0& 0.49 &-16.12 &4.4 &-1.13 &6.17 \\
(S) buy-low-sell-high &83.0 &0.5  &-26.36 &4.5& -1.93&10.0 \\
(U) Momentum &6.0 &1.97  &-0.24 &40.56 &-0.17 &4.32 \\
(S) Momentum &6.0 &1.98  &-0.5 &40.67 &-0.35 & 9.36\\
(U) Mean Reversion   &16.0 &0.07  &-6.97 &1.2 &-0.57 & 0.26\\
(S) Mean Reversion  &16.0 &0.16  &-15.53 &2.95 &-1.35 &1.62 \\

% \bottomrule
% \end{tabular}
 \end{longtable}}
%\end{landscape}





\begin{figure}[htbp!] 
    \centering
    \begin{subfigure}[t]{.76\linewidth}
    \includegraphics[width=1\textwidth]{diagrams/FD_scaled_buy_low_sell_high.png}
    \caption{The buy-low-sell-high strategy. }
    \end{subfigure}
      \begin{subfigure}[t]{.76\linewidth}
    \includegraphics[width=1\textwidth]{diagrams/FD_scaled_momentum.png}
    \caption{The Momentum strategy. }
    \end{subfigure}
  \begin{subfigure}[t]{.76\linewidth}
    \includegraphics[width=1\textwidth]{diagrams/FD_scaled_meanrvr.png}
    \caption{The Mean reversal strategy. }
    \end{subfigure}
    \caption{Backtesting graph of the Synthesizer (FD) model. The red down arrows indicate sell signals, and the green up arrows buy signals, and the red curve at the top shows the evolution of the portfolio value. }
    \label{fig:backtest_fac_dense}
\end{figure}



When looking at one of the better performing models in terms of extreme volatility prediction of the previous section, Synthesizer FD, we notice that the strategies based on this model consistently obtain one of the highest Sharpe ratios. Especially, the momentum and mean reversal strategies (with volatility position scaling), obtain a profit of 9.6\% and 19.2\%. In Figure~\ref{fig:backtest_fac_dense} details are shown of the actual trades for each of the three (scaled) strategies based on the Synthesizer (FD) model. We notice that the most steady increase in total portfolio value is obtained with both the momentum as well as the mean reversal strategy, which is consistent with the results in table. Overall, while these strategies are overly simple and have ample room for improvements, they show the potential of using volatility predictions for risk reduction and finding trading opportunities.  





%bookmarkdorien



\section{Conclusions}
\label{sec:7}

%abstract

In this work, we investigate the usefulness of CryptoQuant data (e.g. on-chain analytics, exchange data, miner data) as well as whale-alert tweets for predicting Bitcoin's next-day volatility. The dataset that was analysed in detail, and the correlation between features and next-day volatility was explored. This analysis uncovered the features important for volatility prediction. 

We then propose a deep learning Transformer model to predict extreme volatility spikes. In particular, we developed a Synthesizer Transformer, a state-of-the-art architecture that is known for its computational efficiency due to the elimination of the dot-product attention mechanism. After parameter tuning, we performed detailed experiments wherein we examined the influence of different synthetic attention mechanisms on the model's performance. We also compared the proposed models to baseline models such as LSTM, Vanilla Transformer, and GARCH. The different Synthesizer models outperform all of the baseline models, both in terms of volatility prediction (regression) as well as volatility spike prediction (classification). The proposed Synthesizer Transformer, especially the one with factorised dense attention, manages to obtain state-of-the-art performance when predicting volatility using CryptoQuant data and whale-alert tweets. 

To gain insight into the inner workings of our Transformer model, we used the Captum XAI library. This allowed us to uncover important input features such as `taker buy volume' and `exchange outflow (ma7)', and USDminus (USD flowing out of wallets into exchanges, from whale-alert tweets). We thus confirmed the importance of both on-chain and whale-alert Twitter features for volatility prediction. 

Finally, we integrated our prediction results with several simple baseline trading strategies. The results show that we are able to minimize drawdown while keeping steady profits. Notably, the Synthesizer Transformer with factorized dense attention performs very well and mitigates downside risk while maintaining a steady profit. We also notice that volatility predicted by our models is especially powerful when used to perform volatility scaling of position sizes, as it increases both the PnL as well as the Sharpe ratio. We should note that these strategies are very simple, each with their own strengths and downfalls, and that they should be improved for use in a real scenario, still, even in this simple form, they demonstrate the power and benefits of our volatility prediction model. 


In future research, it would be useful to expand the time frame of both the training and test data, to account for more types of markets. It may also be useful to explore this model for other asset types and on different time scales. Currently, our complete model source code (including trained models) is available online\footnote{\url{https://github.com/dorienh/bitcoin_synthesizer}}, so that it may be used by anyone interested in forecasting extreme volatility movements in the Bitcoin market. 



%end

% Our findings underscore that our method is a useful alternative for forecasting extreme volatility movements in the Bitcoin market.

% To our knowledge, our analysis is unique because not only did we transformed the data to predict volatility spikes but we also backtested the results from our investigation which contributes to the understanding of mitigating downside risk while trying to increase profits. 
  
%Result summary

% In table \ref{tab:model_prediction_results}, we can see that Synthesizers are able to predict more than half of the extreme volatility spikes of 1.1, 1.2 and 1.3. This is extremely useful more risk is associated the higher the volatility and investors are able to prepare at least 1 day ahead for the incoming volatility spike. 
% This shows that on chain data is a vital tool in helping investors make their decision especially high times of volatility. 

% Thirdly, we see that adding to position sizing increase maximum drawdown and daily VAR while increasing profits. Investors can do so if they are comfortable with the risk while trying to reap potential rewards. On the other hand, investors can reduce position sizing instead during such volatility spikes to reduce maximum drawdown and daily VAR. All in all, various strategies come with different strengths and downsides, it is up to investors for them to choose which strategies they are most comfortable with.

%Limitations and future work
% as well as real time volatility prediction model that makes more frequent predictions like hourly or per minute. 

% Models are susceptible to alpha decay and hence are not guaranteed to work in every scenario. Developing a holistic framework for Bitcoin volatility as well as a more comprehensive portfolio strategy should be created to model the price and volatility dynamics of Bitcoin.  

% This could be of great significant for fund managers and researchers as well as the general investors since the proposed model can improve risk management and forecasts in Bitcoin volatility. It could serve as a warning indicator for the market participants to allow them to quickly make appropriate decisions such as reducing leverage positions to avoid liquidation. 

%From an academic perspective, Synthesizers proofs to be a better alternative to the traditional Vanilla Transformer for time series forecasting. This time varying model can be more adaptable to the fluctuations in the cryptocurrency market which will help the researchers to further understand the complex nature of Bitcoin and allow new research ideas or methods in price and volatility predictions as well as risk management.

%if we were to increase the volatility threshold for spike identification to be more than 1.2 or 1.3, we can filter the results to predict extreme volatility spikes. And this could be use as an indicator to reduce leverage to avoid liquidation or this could as an opportunity to buy in.

% \textbf{Acknowledgements}\\

% The data used in this work were provided by Cryptoquant and whale alert tweets. They can be assessed at \url{https://cryptoquant.com/} and \url{https://twitter.com/whale_alert}. We are grateful for allowing the usage of the data for academic use.\newline


%%
%% The next two lines define the bibliography style to be used, and
%% the bibliography file.
%\bibliographystyle{ACM-Reference-Format}
%\bibliography{sample-base}




% \textbf{Appendix}


%\begin{figure}[htbp!]
%    \centering
%    \includegraphics[width=15cm]{diagrams/backtest(scaled_meanrvr).png}
%    \caption{backtesting graph}
%    \label{fig:backtest_result}
%\end{figure}

%\begin{figure}[htbp!]
%        \centering
%    \includegraphics[width=15cm]{diagrams/captum.pdf}
%    \caption{Captum result}
%    \label{fig:captum_result}
%\end{figure}



%% The Appendices part is started with the command \appendix;
%% appendix sections are then done as normal sections
% \newpage
\appendix
\include{tables}
%% \section{}
%% \label{}

%% If you have bibdatabase file and want bibtex to generate the
%% bibitems, please use
%%
 \bibliographystyle{elsarticle-harv} 
 \bibliography{paper}

%% else use the following coding to input the bibitems directly in the
%% TeX file.

% \begin{thebibliography}{00}

% %% \bibitem{label}
% %% Text of bibliographic item

% \bibitem{}

% \end{thebibliography}
\end{document}
\endinput
%%
%% End of file `elsarticle-template-num.tex'.
