\subsection{Methodological Discussion}
Our strawberry dataset while covering 200 meters of strawberries is still limited. Commercial sites in comparison have hectares of such crops, meaning our 200 meters is not as representative of larger sites with more intra-crop variability. However as previously mentioned data availability is scarce making it practically very difficult to collect hectares of data, not least due to actual or perceived data sensitivity by the respective growers. In spite of this, while there may need to be some adjustments to account for more intra-crop variability of these larger sites our neural networks perform well given the data availability. While the sites are smaller and easier to learn, they also have fewer data to do so, which we believe to be a fair trade-off with no loss in difficulty between their larger sites and our smaller site.

Our MTT used an interval of 4 hours despite our data being synchronised over 15 minutes intervals. This was a tradeoff between data density (thus model complexity), and data availability. Since we only had a finite number of concrete outcomes that we observed we had to limit the complexity and weights of the model so that it could train its fewer weights with what limited data we had for concrete observations. In contrast, if we had used a data density of 15 minutes intervals we would have had to have significantly larger weight matrices being backpropagated from the limited number of observed yield values. If however, we found ourselves with large hectare scale datasets with many more observed outcomes we could tune the model to be more complex to leverage this data, to allow the model to understand much more complex relationships like the aforementioned expected intra-crop variability.

It may also be noted that we use a simple missing data imputation algorithm strategy namely forward-fill which involves filling missing values with the last known value. This was chosen as we mostly only incurred individual or relatively sparsely missing data. In larger sites one might expect to find entire regions that have some data unavailability for some time, meaning more advanced data-filling strategies may be necessary under such conditions. However, in our site, since the missing values were relatively sparse, the forward fill strategy is sufficient to allow us to leverage data in spite of any missing observations or features. The only notable exception is that of yield values. Since yield values were recorded sparsely a single missing value represents a much larger significance. Thus any such missing values are excluded entirely. Thankfully we had very few such missing values.

% \todo{discussed fixed positional encoding rather than learned}
Due to the data scarcity we used fixed positional encoding as opposed to learned encoding. This means the gradients would not be shared with the learned positional encoding. This is sufficient since in the original transformer paper (\autocite{vaswani2017attention}) fixed positional encoding and learned positional encoding result in similar performance.

% \todo{discussed why chosen transformers and three timelines merged in this way}
Finally, we chose to use a tri-transformer architecture merged using a dense fully connected layer. We did this to allow the neural network to train separate contextualising units for each potential timeline. This way we can easily conceptualise the timelines as follows. The pasts purpose is to have a broad view of the relationship between the features and the expected outcomes. This is important as we want to ensure the network has context for how yields are expected to outcome given past scenarios. The present serves to contextualise how this current specific season or crop is performing such that it can later be related to what has happened in the past. The future timeline / transformer is to add mitigations and adverse effects, such that high expected fluctuations can be considered at the merging layer.

\subsection{Results Discussion}

As can be seen in Table \ref{tab:errors} our primary MTT that can forecast three weeks ahead within 8\% RMSE is a large improvement over current capabilities as forecasts by agronomists tend to not only vary wildly from agronomist to agronomists (14 to 30\%), rely on specialist human presence, and are less accurate than our current model. However, a large caveat is that our model was created with intensive/high-quality environmental and yield data, on a small site compared to the typical industrial settings.

The results shown in Table \ref{tab:errors} and Figure \ref{fig:models} are a significant step forward in the prediction of strawberry yields, however, there are some weaknesses to our approach and the yield outcomes. Firstly our ensemble is significantly under-performing especially since a single predictor trained on the whole dataset beats the ensemble significantly. This is likely due to data, with almost three times the parameters we suspect that we require more training data to learn adequately, yet they receive $1/3$ of the total training data each. However, as time progresses and more data becomes available to us over more seasons we believe this ensemble will outperform the single MTT while enabling ensemble-based certainty estimation. Secondly and most difficult is the data itself. While we are fortunate to have access to our Risehome campus and the strawberry tabletop site, there is still a lack of data available for use. This relatively small site means we likely have not learned some of the more complex variances present on larger sites where the sensors immediate environment might be significantly different to another area on the growing site some distance away meaning the data in such scenarios might be significantly less representative of the conditions experienced by the strawberries.

Another issue with our dataset is the similarity between rows. Largely while there is inter-row variance there is still a risk of overfitting since even if the neural network cannot see row 1 for instance, it may be able to relate the yields of row 1 from previously trained/ known yields of row 2. We would have ideally liked to have split by time, and claimed one whole season as a completely separate testing set with none of those rows being trained on. However, due to the reality of strawberry seasonality and that there are only so many seasons with which it was possible to collect data, we had to split in such a way as to give the neural networks context for at least two seasons from start to finish. This is only necessary since the current methods of strawberry prediction in industry are largely based on the occurrences of the last season. As such we attempted to base our methods on existing techniques, and intuitively the performance of the strawberries last year will be related to the performance of the current season unless some large shift in methods between the seasons occurs.

One clear area of future work is to improve on data availability, whether that be collecting more data on a larger site and making that more easily available, or ordaining easily implementable data collection methods that can scale well, and are private such that data can be shared with little concern of sensitivity.

Another area of future work is to implement certainty metrics that do not require the use of ensembles so that we can keep the parameters down, and reduce the necessary data to train more complex models.
\ifx\blind\undefined
We also seek to make transformers that are abelian compatible such that we can use some of our prior fully homomorphically encrypted (FHE \autocite{gentrysFirstFhe}) deep learning methods with these currently incompatible but performant transformers \autocite{onoufriou2021fully, FHEoNNG}.
\else
\fi

Lastly, we seek to find ways in which to make our data available for wider use, currently that is not possible due to contractual constraints which were necessary to enable us to collect this data with industrial varieties in the first instance. However, we seek to remedy this in future.