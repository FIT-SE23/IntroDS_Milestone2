\subsection{Experimental Results}
\label{sec:result}

\noindent \textbf{(1) Effectiveness evaluation (comparison with different classifiers).}
We compare the PR-AUC of \texttt{DeepLPP} and 10 deep learning models, and the results are listed in Table \ref{tab:deepmodels}. From a general view, the effect of our model \texttt{DeepLPP} is at least more than 30\% better than other traditional machine learning models. This is because we have introduced a Deep Network and a Recurrent Network, and comprehensively considered serialized features and continuous numerical features, which is not possible with traditional machine learning models. Followed by the three models RandomForest, XGBoost, and Gradient Boosting, they have PR-AUCs close to 0.6. As for other models, the average PR-AUCs are below 0.4, which indicates that they are not applicable to our scenario. This can be attributed to the fact that the features selected by the traditional machine learning models are all based on the GPS data generated by the userâ€™s mobile phone, which are tightly connected to the movement of users. But our data is obtained from vehicles, so only can our new model and more suitable features get a better result.

\noindent \textbf{(2) Efficiency evaluation (prediction time).}
The training time of all the models is also recorded in Table \ref{tab:deepmodels}. Although \texttt{DeepLPP} takes much more time than other models, the improvement in effect it brings is enough to make up for its time cost. As for the following models discussed above, RandomForest, XGBoost, and Gradient Boosting, only XGBoost requires relatively more time, while the other two can obtain the PR-AUC around 0.6 within 10 seconds. Even though the rest models costs little time, but their low effect is intolerable.

\noindent \textbf{(3) Ablation study.}
We conducted a comprehensive evaluation of each module or feature in our model, and the comparing results are listed in Table \ref{tab:ablation}. Our model \texttt{DeepLPP} outperforms all the other models, which are missing some features or modules, indicating that all the features contribute to the final result. 

In the Recurrent Network, namely, only LSTM module is retained, the result performs the worst with PR-AUC of 0.832. This is because serialized data contains so many features, which are not conducive to training and learning, and it is difficult to capture the common ground between users. Even though the result is acceptable, we have added some statistical characteristics of users to the model besides LSTM, namely, our Deep Network, which has brought more than 15\% improvement in the effect of our model. Then we evaluate all the four features (Mobility, Temporality, Activity and Economy). When the Economy feature is removed, the effect of our model drops most significantly, which may prove that the economic information plays a key role in predicting repayment scenarios, and to the best of our knowledge, this is the first time that such feature is adopted in LSTM and applied in such scenarios; by contrast, when the Time feature is removed, the PR-AUC drops most slightly. This may because even though time index is lost, our sequential input still contains certain time information (e.g., the order of stay points from a user), so the result is acceptable.

In the Deep Network, when all the sequential features are removed, our model can be reduced to a traditional neural network, and the PR-AUC is still nearly 90\%, indicating Deep Network performs better than Recurrent Network. As is discussed above, these features are more consistent in users, so they are more tolerant of data noise. We also evaluate the five features (Mobility home and company, Temporality, Activity and Economy. These features are static ones, different from the recurrent ones). When the Economy feature is removed, the PR-AUC is only 0.897. Intuitively, the Economy feature is very important because Economy is calculated as the PMT of car price, which is highly connected to the economic status of the user. Luckily, in our ablation experiments, this view is verified; the \texttt{DeepLPP} removed Mobility home and company resemble in PR-AUC, 0.932 and 0.924, respectively; but surprisingly, the absence of Temporality and Activity features did not significantly decrease the effect of the model. We cannot jump to the conclusion that the effect of these two features is not obvious, maybe when the duration of trajectories data is extended (e.g., in years), the features can be calculated more accurately, and matters more in the model.
%\noindent \textbf{(4) Parameter study (varying stay point distance $S_d$).}

\noindent \textbf{(4) Parameter study (varying stay point duration $S_t$).}
The parameter stay point duration $S_t$ represents the time threshold of our stay point detection algorithm. Namely, if certain points in a trajectory stay still for a period of time, we judge these points as one stay point if only the duration is longer than $S_t$. Apparently, if we have 2 time thresholds $S_{t_1}$ and $S_{t_2}$, with $S_{t_1}<S_{t_2}$, then all the stay points judged by $S_{t_2}$ will be included in those judged by $S_{t_1}$. 

Considering our GPS trajectories belong to vehicles, which would delay for other reasons (e.g., traffic congestion), we set the parameter $S_t$ to be more than 30min. We compare different values of $S_t$, ranging from 30min to 150min. As is discussed above, the shorter $S_t$, the more the number of stay points, and consequently more items in our training and testing sets. As a result, the number of items in training set and testing set span from 864 to 911, and 1,037 to 1,079 respectively. 

We found that our model performed the best when $S_t=120$ (with PR-AUC of 0.972, which is shown in Table \ref{tab:duration}). This may be because when $S_t$ is too small (30-60min), the identified stay points are noisy, with high proportion of ``temporary parking'' and other accidental events, which does not accurately reflect the user's behavioral characteristics; When $S_t$ is too large (over 150min), many stay points cannot be identified. Extremely, if $S_t$ is over 20 hours, only one stop point can be generated when the user did not drive for nearly a whole day, thus much of information is ignored. 

\noindent \textbf{(5) Parameter study (varying cell size).}
We separate the whole city map (Shenzhen and Chengdu) into $n*n$ grids, then the parameter cell size is set as the side length of each grid. Approximately, $n*cell size$ should be equal to the length of the city, and its square equal to the area of city. Then we index each grid starting from 0, and the index represents the location (including longitude and latitude) of the stay point.

The difference of the parameter cell size will cause two effects: 1) the number of location tokens: each grid can be regarded as a cluster of stay points, then the grid with larger cell size spans wider, resulting in more stay points indexed the same; 2) the judgement of POIs: we collect the POIs in its same grid and its 8 neighbour grids of the stay points to obtain its final POI, so a larger cell size takes more POIs into consideration. 

We found that the cell size 300m fits our model the best (with PR-AUC of 0.972, which is shown in Table \ref{tab:cellsize}). Too small cell size may result in the inability to obtain the common features between the stay points, and even cause over-fitting problems; while larger one neglects their differences.

\noindent \textbf{(6) Parameter study (varying diversity granularity).}
As for our features Temporality and Activity, they are continuous values. We discretize them through dividing them with a number (i.e., the parameter of diversity granularity), then getting their embedding vectors with pretraining model Skip-Gram, which is explained in previous sections. 

Table \ref{tab:embtokens} records the numbers of temporality tokens and activity tokens under different parameters of diversity granularity, where $diversity\ granularity=0.3$ performs the best. This may be explained by the distribution of the features Temporality and Activity: this two features are floats below 10, if they are divided with a span of 0.1, too many data items are independent of each other, which is not conducive to feature learning; if the span is too wide, features are hard to capture.

\noindent \textbf{(7) Parameter study (varying economic granularity).}
Similar to features Temporality and Activity, the feature Economy is also discretized and then embedded with Skip-Gram model. But differently, Economy is calculated as the PMT of car price, which is more than 700 below zero (the smallest one is -17,430), so we divide it with a larger integer (e.g., 50, 100 and 150).

The results can be seen in Table \ref{tab:economypar}, the highest PR-AUC is obtained when the parameter economic granularity is 100.

\noindent \textbf{(8) Case study.}

\begin{table}[]
\centering
\caption{Prediction results and running time.}
\begin{tabular}{l|cc|cc}
\hline
\multirow{2}{*}{Method} & \multicolumn{2}{c|}{Shenzhen}                              & \multicolumn{2}{c}{Chengdu}                                \\
                        & \multicolumn{1}{c}{PR-AUC} & \multicolumn{1}{c|}{Time (ms)} & \multicolumn{1}{c}{PR-AUC} & \multicolumn{1}{c}{Time (ms)} \\ \hline
RBFSVM                  & \multicolumn{1}{c}{0.163}       & \multicolumn{1}{c|}{6,370}          & \multicolumn{1}{c}{0.106}       & \multicolumn{1}{c}{59,182}          \\
LinearSVM               &          0.147                  &                     29,733           &           0.113                 &                     347,713          \\
Logistic Regression     &      0.145                      &                  6,362              &         0.121                   &               52,775                \\
KNN                     &         0.320                   &                  6,345              &         0.284                   &               52,853                \\
Decision Tree           &           0.197                 &              6,331                  &         0.133                   &                 52,766              \\
Random Forest           &             0.602               &              6,596                  &         0.618                   &                53,237               \\
Bayes                   &        0.161                    &             6,337                   &         0.177                   &                52,741               \\
AdaBoost                &           0.409                 &               6,483                 &         0.173                   &                53,369               \\
Gradient Boosting       &            0.523                &                 6,682               &         0.588                   &                54,582               \\
XGBoost                 &           0.599                 &                 24,003               &        0.599                    &                   81,275            \\
DeepLPP                 & \multicolumn{1}{c}{\textbf{0.972}}       & \multicolumn{1}{c|}{266,831}          & \multicolumn{1}{c}{\textbf{0.931}}       & \multicolumn{1}{c}{8,351,911}          \\ \hline
\end{tabular}
\label{tab:deepmodels}
\end{table}

\begin{table}[]
\centering
\caption{Ablation study for \texttt{DeepLPP}.}
\setlength{\tabcolsep}{5mm}{
\begin{tabular}{l|c}
\hline
Method             & PR-AUC               \\ \hline
DeepLPP            & \multicolumn{1}{c}{\textbf{0.972}} \\ \hline
w/o Deep Network  &       0.832               \\
w/o Home Location       & 0.932                     \\
w/o Company Location       &  0.924                    \\
w/o Temporality Diversity    &        0.936              \\
w/o Activity Diversity       &        0.945              \\
w/o Monthly Loan Payment (PMT)        &        0.897              \\ \hline
w/o Recurrent Network &        0.893              \\
w/o Mobility Embedding      &        0.927              \\
w/o Temporality Embedding    &        0.940              \\
w/o Activity Sequence Embedding       &        0.922              \\
w/o Economic-Aware Region Embedding        &        0.918              \\ \hline
\end{tabular}}
\label{tab:ablation}
\end{table}

% \begin{table}[]
% \centering
% \caption{Impacts of stay point radius (m) for \texttt{DeepLPP}.}
% \begin{tabular}{cccccc}
% \hline
% Parameter & 100 & 150 & 200 & 250 & 300 \\ \hline
% \# Tokens &     &     &     &     &     \\
% PR-AUC    &     &     &     &     &     \\ \hline
% \end{tabular}
% \end{table}

\begin{table}[h]
\centering
\caption{Impacts of stay point duration (mins) for \texttt{DeepLPP}.}
\begin{tabular}{lccccc}
\hline
Parameter & 30 & 60 & 90 & 120 & 150 \\ \hline
\# Training items &   911  &   892  &   880  &   871  &  864   \\
\# Testing items &   1,079  &   1,061  &  1,055   &   1,046  &  1,037   \\
%Training PR-AUC   &   1.0  &   0.9952  &  1.0   &  1.0   &  0.9970   \\
PR-AUC   &   0.949  &  0.950   &  0.961   &  \textbf{0.972}   &  0.957   \\ \hline
\label{tab:duration}
\end{tabular}
\end{table}


\begin{table}[h]
\centering
\caption{Impacts of cell size (m) for \texttt{DeepLPP}.}
\begin{tabular}{lccccc}
\hline
Parameter & 100 & 200 & 300 & 400 & 500 \\ \hline
\# Location tokens & 4,811    &  4,285   &  3,674   &  3,293   &  2,915   \\
%Training PR-AUC    &  1.0   &     &     &     &     \\
PR-AUC    &  0.964   &  0.966   &  \textbf{0.972}   &  0.966   & 0.966     \\ \hline
\end{tabular}
\label{tab:cellsize}
\end{table}

\begin{table}[h]
\centering
\caption{Impacts of diversity granularity for \texttt{DeepLPP}.}
\begin{tabular}{lccccc}
\hline
Parameter & 0.1 & 0.3 & 0.5 & 0.7 & 0.9 \\ \hline
\# Temporality tokens &  93   & 31    & 19    &  14   &  11   \\
\# Activity tokens &  83   &  28   &  17   &  12   &   10  \\
PR-AUC    &  0.960   &  \textbf{0.972}   &  0.969   &  0.960   & 0.948    \\ \hline
\label{tab:embtokens}
\end{tabular}
\end{table}

\begin{table}[h]
\centering
\caption{Impacts of economic granularity for \texttt{DeepLPP}.}
\begin{tabular}{lccccc}
\hline
Parameter & 50 & 100 & 150 & 200 & 250 \\ \hline
\# Region economic tokens &  3,865   &  1,932   &  1,288   &  966   &   733  \\
\# PMT tokens &  335   &  168   &   112  &   84  &   67  \\
PR-AUC  & 0.951  &  \textbf{0.972}   &  0.943   &  0.943   & 0.937         \\ \hline
\label{tab:economypar}
\end{tabular}
\end{table}

\begin{figure*}
%\hspace*{-.3cm} (114.057865, 22.543096)
\centering
\begin{tabular}{c}
	\begin{minipage}{\linewidth}
		\includegraphics[width=\linewidth, height=0.3\linewidth]{figures/case1.pdf}
	\end{minipage}
	\\
(a) User 1  \qquad \qquad \qquad \qquad (b) User 2  \qquad \qquad \qquad \qquad (c) User 3
  \\\\
  (d) Illustrating the extracted features for the User 1, 2 and 3.
  \\\\
  \begin{minipage}{\linewidth}
  \small{
  \centering
  \setlength{\tabcolsep}{1.5mm}{ %(114.057865, 22.543096)  
   \begin{tabular}{l||cccc||cccc||ccccl}
\hline
Case        & \multicolumn{4}{c||}{User 1}            & \multicolumn{4}{c||}{User 2}            & \multicolumn{5}{c}{User 3}                                 \\ \hline
Is overdue  & \multicolumn{4}{c||}{No}                & \multicolumn{4}{c||}{Yes}               & \multicolumn{5}{c}{Yes}                                    \\ \hline
DH          & \multicolumn{4}{c||}{(114.057865, 22.543096)}            & \multicolumn{4}{c||}{(114.057865, 22.543096)}            & \multicolumn{5}{c}{(114.057865, 22.543096)}                                 \\
DW          & \multicolumn{4}{c||}{(114.057865, 22.543096)}            & \multicolumn{4}{c||}{(114.057865, 22.543096)}            & \multicolumn{5}{c}{(114.057865, 22.543096)}                                 \\
DT          & \multicolumn{4}{c||}{0.555}             & \multicolumn{4}{c||}{0.555}             & \multicolumn{5}{c}{0.555}                                  \\
DA          & \multicolumn{4}{c||}{0.555}             & \multicolumn{4}{c||}{0.555}             & \multicolumn{5}{c}{0.555}                                  \\
DP          & \multicolumn{4}{c||}{2,345}             & \multicolumn{4}{c||}{2,345}             & \multicolumn{5}{c}{2,345}                                  \\ \hline
Stay points & RM     & RT       & RA        & RE     & RM     & RT       & RA        & RE     & RM     & RT       & RA        & \multicolumn{2}{c}{RE}     \\ \hline
$s_1$       & Cell-1 & 07:30 am & residence & 12,000 & Cell-1 & 07:30 am & residence & 12,000 & Cell-1 & 07:30 am & residence & \multicolumn{2}{c}{12,000} \\
$s_2$       & Cell-1 & 12:15 pm & F\&D      & 54,000 & Cell-1 & 12:15 pm & F\&D      & 54,000 & Cell-1 & 12:15 pm & F\&D      & \multicolumn{2}{c}{54,000} \\
$s_3$       & Cell-1 & 15:40 pm & working   & 53,000 & Cell-1 & 15:40 pm & working   & 53,000 & Cell-1 & 15:40 pm & working   & \multicolumn{2}{c}{53,000} \\
$s_4$       & Cell-1 & 09:00 pm & residence & 12,000 & Cell-1 & 09:00 pm & residence & 12,000 & Cell-1 & 09:00 pm & residence & \multicolumn{2}{c}{12,000} \\ \hline
\end{tabular}}}
\end{minipage}
\end{tabular}
%\vspace{-2mm}
%\setlength{\abovecaptionskip}{1pt}
\caption{Case study, where DH, DW, DT, DA and DP denote the features captured via the deep network for home and work locations, temporality diversity, activity diversity and PMT; RM, RT, RA and RE denote the features captured via the recurrent network for mobility, temporality, activity and economy.}
\label{fig:example}
%\vspace*{-2mm}
\end{figure*}