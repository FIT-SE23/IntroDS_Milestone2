\section{METHODOLOGY}
\label{sec:method}

\subsection{Overview}
To predict users’ socioeconomic statues with their mobility records, we propose a novel socioeconomic-aware deep model 
% for the Socioeconomic Inference, 
called \texttt{DeepSEI}. \texttt{DeepSEI} model consists of three components, including data preprocessing (Section~\ref{sec:preprocess}), deep network (Section~\ref{sec:static}) and recurrent network (Section~\ref{sec:dynamic}).
%
The data preprocessing component is to filter the noises in the mobility records data, extract the stay points from the mobility records, and further infer the activities behind the extracted stay points.
The deep network aims to capture some statistics based on users' mobility records (i.e., at a coarse level) and the recurrent network aims to capture the sequential patterns behind users' mobility records (i.e., at a detailed level). Therefore, the two networks collaboratively capture rich information from users' mobility records data.
%
\begin{figure}
  \centering
  \includegraphics[width=\linewidth]{figures/framework2.pdf}
  %\vspace{-3mm}
  \caption{The overall framework of \texttt{DeepSEI}, where $\bigoplus$ denotes the concatenation operation.}
  \label{fig:model}
  \vspace{-6mm}
\end{figure}

{\CommentZheng{
For the deep network, it is designed to quantify three important aspects of users' mobility characteristics, namely spatiality, temporality and activity. 
For spatiality, we consider users' radii of gyration extracted from the their trajectories, which describe the typical spatial range of users' activities.
%
For temporality, it is intuitive that people with different professions (e.g., government officials or IT engineers) would travel with different temporal patterns/regularities of activities. 
% differently, and the characteristics can be reflected on their temporality. 
We explore an entropy-based temporality indicator for capturing the regularity of users' activities. 
For activity, it is intuitive that an individual's daily activities could be used for inferring the user's socioeconomic status, e.g., rich people have diverse daily activities and travel more places in general, and thus we use an entropy-based activity indicator to capture that aspect. 

For the recurrent network, we focus on the sequential activity data extracted from users' mobility records, where 
% each entity in a sequence involves spatiality (i.e., the location), temporality (i.e., the time), and activity (e.g., the POI category).
each activity involves spatial, temporal, and semantic features. 
% , which provides necessary information to infer the condition of users’ socioeconomic. 
In particular, spatial and temporal features record where and when the user's activities are conducted, respectively. Semantic feature provides the context for understanding the intended activities that a user conducts, e.g., working or dining. Intuitively, the contextual semantics would be useful to profile the user's socioeconomic status, e.g., if a user visits the fast food restaurants frequently and rarely visits recreational venues such as fitness centers, he may have poor socioeconomic conditions. }}
%
We adopt a hierarchical LSTM for the recurrent network since the hierarchical structure can reduce the mobility sequence length, and extract periodic mobility behaviours of human.

We concatenate the outputs of the two networks into a long vector, which is further fed into a fully connected layer to produce the predicted users' statuses. 
We train the \texttt{DeepSEI} model in a supervised manner (Section~\ref{sec:training}). Figure~\ref{fig:model} illustrates the overall framework of \texttt{DeepSEI}. Next, we discuss the detailed designs of each component.
 
\subsection{Data Preprocessing}
\label{sec:preprocess}
% {\CommentZheng{
% Recall that we collect three types of data in our task, i.e., mobility data, POI data and house price data. 
The raw mobility records data corresponds to a sequence of sampled locations with time stamps. The data may involve noises (e.g., the GPS noises) and lack of semantics (e.g., activities). 
The data preprocessing component is to preprocess the data by (1) filtering the noises, (2) extracting the stay points (which indicate activity behaviors), and (3) inferring the semantics of the stay points (e.g., the categories of the POIs that are probably visited at the extracted stay points).
% Mobility data records the location of a user's movement at a timestamp and we use the data for constructing spatial and temporal features. We use POI data for constructing semantic features (i.e., working, food and drink), which describe the categories of the places visited by users. 
%
% We use the house price data as socioeconomic contexts for constructing the labels indicating users' socioeconomic classes.
% }}
Specifically, data preprocssing involves three steps, namely Noise Filtering, Stay Point Detection, and Activity Inference.

\smallskip
\noindent \textbf{Noise Filtering.} For mobility records data (or trajectory data), we first perform trajectory segmentation by dividing the data into one-week trajectory instances, which are used to train and test the model.
%
Within each segmented trajectory, we filter those noisy data points and then detect those stay points, where a stay point corresponds to one activity such as working, shopping, and staying at home. In particular, trajectory data usually contains noises due to the way how it is collected, e.g., a sampled location might be several hundred meters away from a true location. Such noisy points will affect the quality of stay point detection. We adopt a heuristic-based approach proposed in~\cite{zheng2015trajectory} for filtering noisy points in trajectories. It sequentially calculates the traveling speed for each point in a trajectory based on its precursor and itself. If the speed is larger than a threshold, the current examined point is removed from the point sequence.

\smallskip
\noindent \textbf{Stay Point Detection.} Based on the cleaned trajectories, we extract all stay points from them. Specifically, we adopt the stay point detection algorithm proposed in~\cite{li2008mining} for detecting stay points from trajectories. The algorithm first checks if the distance between an anchor point and its successors in a trajectory is larger than a given threshold $S_d$. It then calculates the duration between the anchor point and the last successor within $S_d$. If the duration is larger than a temporal threshold $S_t$, a stay point is detected, and the anchor point moves to the next point after the current stay point. Otherwise, the anchor point moves forward by one. This process is repeated until the anchor point moves to the end of the sequence so that all stay points in a trajectory are extracted.

\smallskip
\noindent \textbf{Activity Inference.} Based on the stay points extracted from users' mobility records, we infer the most relevant activity for each stay point. 
% To do this, we adopt a simple yet effective distance-based method to infer the activity. 
For each stay point, we check 8 neighbouring grid cells of the grid cell, in which the stay point is located. Then, we infer the activity associated with the stay point to be the most frequent POI category among those POIs in the 8 neighbouring grid cells.  In addition, if no POI is found within the grids, we infer the activity to be a special tag called ``other''. 
In Figure~\ref{fig:poi}, we illustrate the inferred activity distribution for Beijing in terms of training and testing (details will be presented in the sequel). We notice some other methods such as Markov based inference models~\cite{wu2016did,yan2013semantic,yan2011semitri} are also applicable for the task of activity inference. Since the POI categories are in the form of discrete tokens, we obtain the activity vectors {\CommentZheng{by embedding their tokens as one-hot vectors.}}

\begin{figure}
\centering
\begin{tabular}{c}
   \begin{minipage}{0.9\linewidth}
    \includegraphics[width=\linewidth]{figures/poi_bj.pdf}
    \end{minipage}
\end{tabular}
\vspace*{-2mm}
\caption{Activity distribution of Beijing.}
\label{fig:poi}
\vspace*{-4mm}
\end{figure}

\subsection{Deep Network}
\label{sec:static}
One previous study~\cite{xu2018human} reveals that users' socioeconomic statuses can be reflected by their mobility patterns/statistics. 
Inspired by this, we adopt some indicators that are computed from users' mobility records for our task. These indicators capture the spatiality, temporality and activity aspects of users' mobility patterns. 
% Those captured features are generally static with fixed values. 
We embed indicators via a deep network and concatenate the embeddings for our task. 

\smallskip
\noindent \textbf{Spatiality Diversity.}
To capture the mobility features from users' trajectory data, we consider radius of gyration, which is widely used as a spatial indicator to capture users' mobility characteristics~\cite{xu2018human,wu2019inferring}. Given a trajectory data $T = <(p_1,t_1),(p_2,t_2),...,(p_n,t_n)>$, where $p_i$ and $t_i$ ($1 \le i \le n$) denote the location $p_i$ of a moving object at time $t_i$. The radius of gyration $R_g$ is defined as follows.
\begin{equation}
    R_g = \sqrt{\frac{\sum_{i=1}^{n}(p_i-p_c)^2}{n}}, \quad p_c=\frac{\sum_{i=1}^{n}p_i}{n},
\end{equation}
where $p_c$ denotes the center of the sampled locations. The rationale of radius of gyration is to capture the spatial dispersion of a user' movement. Intuitively, a small radius indicates that the user's activities are mainly in a small area.

\smallskip
\noindent \textbf{Temporality Diversity.} We consider the feature in temporal aspect. It is intuitive that people with similar professions would travel similarly in terms of temporality. For example, office staff would commute between home and office regularly on weekdays while self-employers would mainly stay at home and only go out occasionally. In addition, some users (e.g., those work at a government department) would commute more regularly than others (those work at an IT company). These temporality patterns embed rich information that could be used for inferring socioeconomic statuses of users. Therefore, we explore a temporality indicator called temporality diversity. 

Given a user's stay points extracted from his/her mobility records ${(s_1, \Delta t_1), (s_2, \Delta t_2),...,(s_n,\Delta t_n)}$, where $s$ and $\Delta t$ denote the stay location and the duration of staying at that location, respectively. By following~\cite{scheiner2014gendered,xu2018human}, we calculate the temporality diversity via cross-entropy, which captures the duration distribution among those stay locations. Let $pt_i=\frac{\Delta t_i}{\sum_{j=1}^{n}\Delta t_j}$ denote the proportion of stay duration at location $s_i$, the temporality diversity (TD) is defined as follows:
\begin{equation}
    TD = -\sum_{i=1}^{n}pt_i\log(pt_i),
\end{equation}
where $\sum_{i=1}^{n}pt_i = 1$.
% which weights the stay locations with their duration. 
The rationale of the indicator is to consider the human daily regularity. For example, for some people (e.g., IT programmers), whose daily lives are mainly concentrated at home and work places, they would have a high regularity reflected as a low cross-entropy value.

\smallskip
\noindent \textbf{Activity Diversity.}
Previous studies~\cite{pappalardo2015using, xu2018human} exhibit that the diversity of individual daily activities has a strong correlation with their socioeconomic statuses. Intuitively, a well-developed city (e.g., Beijing in our study) provides many facilities for residents to conducct various activities, and richer people tend to have a higher activity diversity in daily lives, e.g., their jobs are generally with higher diversification.

Inspired by this, we explore the features for describing activity diversity in the model. 
Given a sequence of user's stay points $(s_1, \Delta t_1), (s_2, \Delta t_2),...,(s_n,\Delta t_n)$, where each stay location $s$ is potentially 
associated with an activity such as working. 
% Among these locations, w
We define two consecutive stay locations in a trip $e = (s_{i-1}, s_i) (0<i \leq n)$ as a source and destination pair. 
We let $E$ denote the set of all possible source and destination pairs extracted from the whole stay points, where the direction of movement could be ignored. For each pair $e \in E$, $p(e)$ denotes the proportion of observing the movement corresponding to the pair $e$ wrt the total number of movements (i.e., $n-1$) in the records. For example, given a user's stay points ${(a, \Delta t_1), (b, \Delta t_2), (c, \Delta t_3), (d, \Delta t_4), (c, \Delta t_5), (b, \Delta t_6), (a, \Delta t_7)}$, there are 6 movements in the records and 3 source and destination pairs in $E$ without considering direction, i.e., $(a,b)$, $(b,c)$ and $(c,d)$. For the pair $e=(a,b)$, $p(e)$ is calculated as $p(e)=\frac{2}{6}=0.33$. Note that $\sum_{e \in E}p(e)=1$, and we define the Activity Diversity (AD) via cross-entropy as follows:
\begin{equation}
    AD = -\sum_{e \in E}p(e)\log(p(e)).
\end{equation}
The rationale is that for a user, whose trips are concentrated on a few locations, he/she would have a high cross-entropy value.

\if 0
\noindent \textbf{Monthly Loan Payment (PMT).} 
The loan data records the associated vehicle attributes including car price, deposit, loan term. Here, we consider the monthly loan payment as a socioeconomic indicator. Intuitively, this indicator can directly reflect the user’s loan pressure, so as to infer whether there is a risk of overdue loan payment. The monthly loan payment, namely PMT, incorporates three information (i.e., car price, deposit and loan term), whose formula is defined as follows.
\begin{equation}
PMT = \frac{Pv*R}{1 - (1 + R)^{-n}},
\end{equation}
where $Pv$ denotes the amount of loan (i.e., car price - deposit), $R$ denotes the periodic interest rate, which is calculated as the ratio of annual percentage rate and the number of interest periods per year (i.e., a fixed number 0.154/12 in car loan), and $n$ denotes the loan term (i.e., total number of interest periods, 12 months or 24 months in the loan). 
\fi

\begin{figure}
  \centering
  \includegraphics[width=0.9\linewidth]{figures/deep.pdf}
  \vspace{-3mm}
  \caption{The architecture of deep network, where $\bigoplus$ denotes the concatenation.}
  \label{fig:model_static}
  \vspace{-5mm}
\end{figure}

\smallskip
\noindent \textbf{Network Architecture.}
Figure~\ref{fig:model_static} illustrates the architecture of the deep network. To feed the above features into the \texttt{DeepSEI} model, we first tokenize them. In particular, spatiality diversity, temporality diversity and activity diversity are naturally in the form of continuous float values, we tokenize them by partitioning the range with a predefined granularity, e.g., we partition the range 0.09-8,143.3
(resp. {\newCommentZheng{0-5.73 and 0.02-5.36}}) of spatiality diversity (resp. temporality diversity and activity diversity) with a 100 (resp. 0.5 and 0.5) granularity, and thus we obtain 82 (resp. 11 and 10) tokens.

After obtaining the tokens of above features, we then embed each feature via an embedding layer, and denote the embedded features (i.e., 32-dimensional vectors) to be fed to the \underline{d}eep network for \underline{s}patiality diversity, \underline{t}emporality diversity and \underline{a}ctivity diversity as $x^{ds}$,$x^{dt}$ and $x^{da}$, respectively. We concatenate the embedded feature vectors, and thus obtain a long vector with dimension $32*3$, which will be further used. To facilitate the training process, we pre-train the tokens of spatiality diversity, temporality diversity and activity diversity via Skip-Gram model~\cite{mikolov2013efficient}. After pre-training, the tokens with similar context (e.g., two tokens correspond to two similar values for a feature) will be embedded into similar vectors in a latent space. We then use the pre-trained vectors to initialize the embedding layer of the deep network, and those embeddings can further be optimized with the model training.

\subsection{Recurrent Network}
\label{sec:dynamic}
The recurrent network aims to capture sequential patterns of users' activities, each of which involves spatial, temporal and semantic features.
We first embed these features of each activity and then feed the embeddings to a hierarchical LSTM network.
% For the recurrent network, we adopt a hierarchical LSTM based network.
Compared with the implementation of vanilla RNN based models, hierarchical LSTM is more capable of capturing the sequential and periodic information from users' trajectory data. 
% Here, for each stay point in the trajectory, we capture the features in terms of spatiality, temporality and activity aspects. 

\smallskip
\noindent \textbf{Spatial Embedding.} 
% Recall that we partition a geographical space into grids, and map the GPS coordinates of a stay point to the grids. 
We partition the geographical space into grid cells with the grid size of 200m $\times$ 200m, and map the GPS coordinates of the stay point behind an activity to a grid cell. Naturally, the mapped grid cell, which encompasses the coordinates of the stay point, can represent its spatial information. We then obtain the spatial embedding of the activity as {\CommentZheng{a one-hot vector}}, which will be further fed into the hierarchical LSTM.

\smallskip
\noindent \textbf{Temporal Embedding.} For the extracted stay point behind each activity, it is associated with a timestamp to record the starting time of the stay. We take the timestamp as a temporal feature, and embed it into the model. 
% Intuitively, 
% although the users are with different kinds of lifestyles, 
% users have their temporalities are generally periodic and stable during their daily lives. 
By following the previous study~\cite{zhong2015you}, we split a one-week trajectory data into two parts, i.e., one for weekdays and one for weekends. For each part, we further split a day into 24 hourly time bins, and therefore we obtain $24*2=48$ time bins in total to tokenize the temporal feature, where we take the temporality for weekdays and weekends differently. This is because users generally have different lifestyles for these two parts. {\CommentZheng{Similarly, we obtain the temporality vectors by embedding their time bins as one-hot vectors}}, and fed them into the hierarchical LSTM.

\smallskip
\noindent \textbf{Semantic Embedding.}
The semantic feature of an activity corresponds to the POI we have inferred for a stay point (details in Section~\ref{sec:preprocess}). We obtain the embeddings of semantic features (i.e., POIs) as one-hot vectors.
% Recall that each detected stay point is potentially associated with an activity, we investigate whether users’ activity sequences are informative for inferring users’ socioeconomic status. 
% Note that these sequential patterns are not captured by the mobility indicators or the temporality indicators that are based on some statistics. 
% To this end, we use Recurrent Neural Networks (RNN)~\cite{hochreiter1997long} for embedding users’ activity sequences, where each activity is represented by the learned hidden vector of the stay point that the activity is associated with. 
% The hidden vector that is outputted by the network will be used for the task.

% In particular, we collect the POI data using {\newCommentZheng{Amap Map Service~\cite{api} with 11 major categories to capture human daily activities, including working, residence, food and drink, attractions, community, shopping, education, hospitals, lodging, traffic, and recreation.}} 
% Based on the stay points extracted from users' mobility records, an immediate task is to infer the most relevant activity for each stay point. To do this, we adopt a simple yet effective distance-based method to infer the activity. For each stay point, we check 8 neighbouring grids of the grid, where the stay point is located in. Then, the activity associated with the stay point is inferred as the most frequent POI category among those POIs in the 8 neighbouring grids. In addition, if no POI can be matched within the grids, we assign the activity with a special tag called ``other''. In Figure~\ref{fig:poi}, we illustrate the inferred activity distribution for Beijing in terms of training and testing (details will be presented in the sequel). We notice some other methods such as Markov based inference models~\cite{wu2016did,yan2013semantic,yan2011semitri} are also applicable for the task. Note that the POI categories are in the form of discrete tokens. Similarly, we can obtain the activity vectors {\CommentZheng{by embedding their tokens as one-hot vectors.}}

\if 0
\noindent \textbf{Socioeconomic-Aware Region Embedding.} 
Intuitively, users' socioeconomic status can sometimes be disclosed by what places they visit.
% , which could be revealed by their mobility records. 
We investigate the features of socioeconomic-aware regions in which users’ stay points are located. Here, we incorporate some socioeconomic contexts such as house prices collected from Lianjia as introduced in Section~\ref{sec:preprocess} and learn the region representations (i.e., embedding regions as vectors) of a targeted city for feature engineering. This component involves two steps, namely (1) map segmentation and (2) region representation learning. The map segmentation step is to partition the map of the target city (e.g., Shenzhen or Chengdu) into regions and the region representation learning step is to learn socioeconomic-aware embeddings of the regions formed in the first step. Figure~\ref{fig:region} visualizes the incorporated socioeconomic contexts (e.g., house prices) for Shenzhen or Chengdu.

For the map segmentation step, we use the land use data provided by OpenStreetMap~\cite{osmtool}, which involves a partition of the map and also the use type for each region.
We obtain 13,197 and 4,382 regions for Shenzhen and Chengdu, respectively.
Some other methods, e.g., partitioning the map using road networks is also applicable~\cite{yuan2012discovering}.

For the region representation learning step, we incorporate the socioeconomic contexts (e.g., house prices) for different regions. In particular, we map the GPS coordinates of the houses to the regions, and take the average house prices as the socioeconomic contexts for those mapped regions, and thus we obtain 955 and 1,413 regions with the socioeconomic contexts for Shenzhen and Chengdu, respectively. We call the regions socioeconomic-aware regions. Then, we partition the socioeconomic contexts with a granularity, e.g., the range of average house price is from 18,012 to 285,822 (resp. from 3,970 to 75,454) rmb/$m^2$ for Shenzhen (resp. Chengdu). With the setting of a 100 granularity, we obtain 2,678 and 714 tokens for Shenzhen and Chengdu, respectively. We pre-train the tokens via Skip-Gram model~\cite{mikolov2013efficient} and obtain the embedded socioeconomic contexts (e.g., vectors).
%
For each stay point of a user, we assign the stay point with an embedded socioeconomic context of the region where the stay point is located, and thus the stay point is associated with an socioeconomic-aware vector to capture the user’s socioeconomic status from his/her mobility. Note that for some stay points of a user that are not located in socioeconomic-aware regions, we assign the embedding of the user's house price to the stay points for padding his/her mobility sequences.
\fi

\begin{figure}
\vspace{-3mm}
  \centering
  \includegraphics[width=\linewidth]{figures/model.pdf}
  \vspace{-5mm}
  \caption{The architecture of recurrent network.}
  \label{fig:model_dynamic}
  \vspace{-5mm}
\end{figure}

\smallskip
\noindent \textbf{Network Architecture.}
Figure~\ref{fig:model_dynamic} illustrates the architecture of the recurrent network, which is implemented with a hierarchical LSTM, where the LSTM units~\cite{hochreiter1997long} are employed for both low-level and high-level structures. Take an one-week trajectory for example, the low-level LSTM is to capture the intra-transitions within a day and the high-level LSTM is to capture the inter-transitions across days.

For the low-level LSTM, we denote the embeddings used in the \underline{r}ecurrent network for \underline{s}patiality, \underline{t}emporality and \underline{a}ctivity as $x^{rs}$, $x^{rt}$ and $x^{ra}$. Then, we concatenate those embeddings at each time step $i$, denoted by $x^{rs}_i \oplus x^{rt}_i \oplus x^{ra}_i$, and feed the concatenation to low-level LSTM to obtain the hidden state $h^{L}_i$ at this time step.
\begin{equation}
h^{L}_i = \text{LSTM}^{L}(x^{rs}_i \oplus x^{rt}_i \oplus x^{ra}_i, h^{L}_{i-1}).
\end{equation}
We take the last hidden state within the day denoted by $h^{L}_n$, as a latent representation for capturing the intra-transitions within the day, and it is further fed into high-level LSTM. 
\begin{equation}
h^{H}_j = \text{LSTM}^{H}(h^{L}_n, h^{H}_{j-1}), 
\end{equation}
where $h^{H}_j$ denotes the high-level hidden state at the $j^{th}$ day, which is then fed to initialize the hidden state $h^{L}_0$ for the low-level LSTM. We feed the last hidden state at the high-level LSTM (i.e., suppose $h^{H}_7$ is the last day of a week) into a fully connected layer as the output of recurrent network.

With the hierarchical LSTM, it brings two advantages. First, compared with vanilla RNN, modeling the user’s mobility records in a hierarchical way is able to reduce the sequence length, where the low-level LSTM is for handling the records within a day and the high-level LSTM is for handling the records across days. The design helps alleviate the issue of degraded model performance for modeling long mobility sequences. Second, the hierarchical structure is able to capture both sequential and periodic information, where the low-level LSTM captures the sequential transitions from users’ mobility records, and high-level LSTM preserves the periodic information on a daily basis.

\smallskip
\subsection{Jointly Training deep and recurrent networks}
\label{sec:training}
We jointly train the two networks (i.e., deep and recurrent networks) for predicting users' socioeconomic statuses. 
Following the previous study~\cite{zhong2015you}, we partition the whole Geolife dataset into one-week trajectories. We randomly sample 70\% for training (i.e., {\newCommentZheng{1,360 one-week trajectories}}) and the remaining for testing (i.e., {\newCommentZheng{584 one-week trajectories}}). 
%
For each trajectory, it is associated a ground truth label indicating the user's socioeconomic status.
% , which is obtained using house price data described in Section~\ref{sec:setup}
%

{\newadd{To train the \texttt{DeepSEI}, we first pre-train the deep and recurrent networks separately and then jointly train them together. During the pre-training, it provides a warm-start for the two networks and boosts the convergence in the joint training. Specifically, we generate 50 episodes and use the deep or recurrent network separately for the classification task with cross entropy loss. During the joint training, we also generate 50 episodes and concatenate the outputs by the two networks as a long vector, which is further fed into a fully connected layer with the softmax function with the cross entropy loss.
% to produce the probability of indicating each class. 
We adopt the Adam stochastic gradient descent to optimize the network parameters. The training details are reported in Section~\ref{sec:result}.
}}