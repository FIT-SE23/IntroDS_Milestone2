%% Created for Prasanta Ghosh on -01-20

@article{hershey1999audio,
  title={Audio vision: Using audio-visual synchrony to locate sounds},
  author={Hershey, John and Movellan, Javier},
  journal={Advances in neural information processing systems},
  volume={12},
  year={1999}
}

@article{fisher2000learning,
  title={Learning joint statistical models for audio-visual fusion and segregation},
  author={Fisher III, John W and Darrell, Trevor and Freeman, William and Viola, Paul},
  journal={Advances in neural information processing systems},
  volume={13},
  year={2000}
}

@inproceedings{kidron2005pixels,
  title={Pixels that sound},
  author={Kidron, Einat and Schechner, Yoav Y and Elad, Michael},
  booktitle={2005 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR'05)},
  volume={1},
  pages={88--95},
  year={2005},
  organization={IEEE}
}
@article{Davis80-COP,
	Author = {Steven B. Davis and Paul Mermelstein},
	Journal = {IEEE Transactions on Acoustics, Speech and Signal Processing},
	Number = {4},
	Pages = {357--366},
	Title = {Comparison of Parametric Representation for Monosyllabic Word Recognition in Continuously Spoken Sentences},
	Volume = {28},
  Month = aug,
	Year = {1980}}
	
@inproceedings{selvaraju2017grad,
  title={Grad-cam: Visual explanations from deep networks via gradient-based localization},
  author={Selvaraju, Ramprasaath R and Cogswell, Michael and Das, Abhishek and Vedantam, Ramakrishna and Parikh, Devi and Batra, Dhruv},
  booktitle={Proceedings of the IEEE international conference on computer vision},
  pages={618--626},
  year={2017}
}
@inproceedings{farneback2003two,
  title={Two-frame motion estimation based on polynomial expansion},
  author={Farneb{\"a}ck, Gunnar},
  booktitle={Scandinavian conference on Image analysis},
  pages={363--370},
  year={2003},
  organization={Springer}
}

@article{Rabiner89-ATO,
	Author = {Lawrence R. Rabiner},
	Journal = {Proceedings of the IEEE},
	Number = {2},
	Pages = {257--286},
	Title = {A Tutorial on Hidden {Markov} Models and Selected Applications in Speech Recognition},
	Volume = {77},
  Month = feb,
	Year = {1989}}

@book{Hastie09-TEO,
	Address = {New York},
	Author = {Trevor Hastie and Robert Tibshirani and Jerome Friedman},
	Publisher = {Springer},
	Title = {The Elements of Statistical Learning -- Data Mining, Inference, and Prediction},
	Year = {2009}}

@proceedings{INTERSPEECH22,
	Booktitle = {Proceedings {INTERSPEECH} 2022 -- 23\textsuperscript{rd} Annual Conference of the International Speech Communication Association},
	Date-Modified = {2022-01-18},
	Key = {INTERSPEECH},
	Title = {Proceedings {INTERSPEECH} 2022 -- 23\textsuperscript{rd} Annual Conference of the International Speech Communication Association},
  Address = {{Incheon, Korea}},
  Month = {{Sep.}},
	Year = {2022}}

@inproceedings{harwath2018jointly,
  title={Jointly discovering visual objects and spoken words from raw sensory input},
  author={Harwath, David and Recasens, Adria and Sur{\'\i}s, D{\'\i}dac and Chuang, Galen and Torralba, Antonio and Glass, James},
  booktitle={Proceedings of the European conference on computer vision (ECCV)},
  pages={649--665},
  year={2018}
}

@article{lostanlen2018per,
  title={Per-channel energy normalization: Why and how},
  author={Lostanlen, Vincent and Salamon, Justin and Cartwright, Mark and McFee, Brian and Farnsworth, Andrew and Kelling, Steve and Bello, Juan Pablo},
  journal={IEEE Signal Processing Letters},
  volume={26},
  number={1},
  pages={39--43},
  year={2018},
  publisher={IEEE}
}

@inproceedings{qian2020multiple,
  title={Multiple sound sources localization from coarse to fine},
  author={Qian, Rui and Hu, Di and Dinkel, Heinrich and Wu, Mengyue and Xu, Ning and Lin, Weiyao},
  booktitle={European Conference on Computer Vision},
  pages={292--308},
  year={2020},
  organization={Springer}
}

@inproceedings{senocak2018learning,
  title={Learning to localize sound source in visual scenes},
  author={Senocak, Arda and Oh, Tae-Hyun and Kim, Junsik and Yang, Ming-Hsuan and Kweon, In So},
  booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  pages={4358--4366},
  year={2018}
}

@inproceedings{chen2020vggsound,
  title={Vggsound: A large-scale audio-visual dataset},
  author={Chen, Honglie and Xie, Weidi and Vedaldi, Andrea and Zisserman, Andrew},
  booktitle={ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  pages={721--725},
  year={2020},
  organization={IEEE}
}

@inproceedings{chen2021localizing,
  title={Localizing visual sounds the hard way},
  author={Chen, Honglie and Xie, Weidi and Afouras, Triantafyllos and Nagrani, Arsha and Vedaldi, Andrea and Zisserman, Andrew},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={16867--16876},
  year={2021}
}

@inproceedings{radford2021learning,
  title={Learning transferable visual models from natural language supervision},
  author={Radford, Alec and Kim, Jong Wook and Hallacy, Chris and Ramesh, Aditya and Goh, Gabriel and Agarwal, Sandhini and Sastry, Girish and Askell, Amanda and Mishkin, Pamela and Clark, Jack and others},
  booktitle={International Conference on Machine Learning},
  pages={8748--8763},
  year={2021},
  organization={PMLR}
}

@article{raghu2021vision,
  title={Do vision transformers see like convolutional neural networks?},
  author={Raghu, Maithra and Unterthiner, Thomas and Kornblith, Simon and Zhang, Chiyuan and Dosovitskiy, Alexey},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  year={2021}
}

@article{hu2020discriminative,
  title={Discriminative sounding objects localization via self-supervised audiovisual matching},
  author={Hu, Di and Qian, Rui and Jiang, Minyue and Tan, Xiao and Wen, Shilei and Ding, Errui and Lin, Weiyao and Dou, Dejing},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={10077--10087},
  year={2020}
}

@article{wu2021wav2clip,
  title={Wav2CLIP: Learning Robust Audio Representations From CLIP},
  author={Wu, Ho-Hsiang and Seetharaman, Prem and Kumar, Kundan and Bello, Juan Pablo},
  journal={ICASSP 2022},
  year={2022}
}

@article{fuentes2022urbansas,
  title={URBAN SOUND \& SIGHT: DATASET AND BENCHMARK FOR AUDIO-VISUAL URBAN SCENE UNDERSTANDING},
  author={Fuentes, Magdalena and Steers, Bea and Zinemanas, Pablo and Rocamora, Martin and Bondi, Luca and Wilkins, Julia and Shi, Qianyi and Hou, Yao and Das, Samarjit and Serra, Xavier and Bello, Juan},
  journal={ICASSP 2022},
  year={2022}
}

@article{oord2018representation,
  title={Representation learning with contrastive predictive coding},
  author={Oord, Aaron van den and Li, Yazhe and Vinyals, Oriol},
  journal={arXiv preprint arXiv:1807.03748},
  year={2018}
}

@article{alwassel2020self,
  title={Self-supervised learning by cross-modal audio-video clustering},
  author={Alwassel, Humam and Mahajan, Dhruv and Korbar, Bruno and Torresani, Lorenzo and Ghanem, Bernard and Tran, Du},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={9758--9770},
  year={2020}
}

@article{caron2020unsupervised,
  title={Unsupervised learning of visual features by contrasting cluster assignments},
  author={Caron, Mathilde and Misra, Ishan and Mairal, Julien and Goyal, Priya and Bojanowski, Piotr and Joulin, Armand},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={9912--9924},
  year={2020}
}


@inproceedings{chefer2021generic,
  title={Generic attention-model explainability for interpreting bi-modal and encoder-decoder transformers},
  author={Chefer, Hila and Gur, Shir and Wolf, Lior},
  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},
  pages={397--406},
  year={2021}
}

@article{aytar2016soundnet,
  title={Soundnet: Learning sound representations from unlabeled video},
  author={Aytar, Yusuf and Vondrick, Carl and Torralba, Antonio},
  journal={Advances in neural information processing systems},
  volume={29},
  year={2016}
}
@inproceedings{teed2020raft,
  title={Raft: Recurrent all-pairs field transforms for optical flow},
  author={Teed, Zachary and Deng, Jia},
  booktitle={European conference on computer vision},
  pages={402--419},
  year={2020},
  organization={Springer}
}
@article{simonyan2014two,
  title={Two-stream convolutional networks for action recognition in videos},
  author={Simonyan, Karen and Zisserman, Andrew},
  journal={Advances in neural information processing systems},
  volume={27},
  year={2014}
}
@inproceedings{senocak2022less,
  title={Less Can Be More: Sound Source Localization With a Classification Model},
  author={Senocak, Arda and Ryu, Hyeonggon and Kim, Junsik and Kweon, In So},
  booktitle={Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision},
  pages={3308--3317},
  year={2022}
}
@inproceedings{lin2014microsoft,
  title={Microsoft coco: Common objects in context},
  author={Lin, Tsung-Yi and Maire, Michael and Belongie, Serge and Hays, James and Perona, Pietro and Ramanan, Deva and Doll{\'a}r, Piotr and Zitnick, C Lawrence},
  booktitle={European conference on computer vision},
  pages={740--755},
  year={2014},
  organization={Springer}
}
@article{wang2021you,
  title={You only learn one representation: Unified network for multiple tasks},
  author={Wang, Chien-Yao and Yeh, I-Hau and Liao, Hong-Yuan Mark},
  journal={arXiv preprint arXiv:2105.04206},
  year={2021}
}

@article{wu2022listen,
  title={How to Listen? Rethinking Visual Sound Localization},
  author={Wu, Ho-Hsiang and Fuentes, Magdalena and Seetharaman, Prem and Bello, Juan Pablo},
  journal={arXiv preprint arXiv:2204.05156},
  year={2022}
}
@inproceedings{oya2020we,
  title={Do we need sound for sound source localization?},
  author={Oya, Takashi and Iwase, Shohei and Natsume, Ryota and Itazuri, Takahiro and Yamaguchi, Shugo and Morishima, Shigeo},
  booktitle={Proceedings of the Asian Conference on Computer Vision},
  year={2020}
}
@inproceedings{arandjelovic2017look,
  title={Look, listen and learn},
  author={Arandjelovic, Relja and Zisserman, Andrew},
  booktitle={Proceedings of the IEEE International Conference on Computer Vision},
  pages={609--617},
  year={2017}
}

@inproceedings{arandjelovic2018objects,
  title={Objects that sound},
  author={Arandjelovic, Relja and Zisserman, Andrew},
  booktitle={Proceedings of the European conference on computer vision (ECCV)},
  pages={435--451},
  year={2018}
}

@inproceedings{zhao2018sound,
  title={The sound of pixels},
  author={Zhao, Hang and Gan, Chuang and Rouditchenko, Andrew and Vondrick, Carl and McDermott, Josh and Torralba, Antonio},
  booktitle={Proceedings of the European conference on computer vision (ECCV)},
  pages={570--586},
  year={2018}
}

@inproceedings{rouditchenko2019self,
  title={Self-supervised audio-visual co-segmentation},
  author={Rouditchenko, Andrew and Zhao, Hang and Gan, Chuang and McDermott, Josh and Torralba, Antonio},
  booktitle={ICASSP 2019-2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  pages={2357--2361},
  year={2019},
  organization={IEEE}
}

@inproceedings{Wang2021_ICASSP,
    author = "Wang, Shanshan and Mesaros, Annamaria and Heittola, Toni and Virtanen, Tuomas",
    title = "A Curated Dataset of Urban Scenes for Audio-Visual Scene Analysis",
    booktitle = "2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)",
    year = "2021",
    note = "accepted",
    organization = "IEEE",
    keywords = "Audio-visual data, Scene analysis, Acous-tic scene, Pattern recognition, Transfer learning",
    abstract = "This paper introduces a curated dataset of urban scenes for audio-visual scene analysis which consists of carefully selected and recorded material. The data was recorded in multiple European cities, using the same equipment, in multiple locations for each scene, and is openly available. We also present a case study for audio-visual scene recognition and show that joint modeling of audio and visual modalities brings significant performance gain compared to state of the art uni-modal systems. Our approach obtained an 84.4\% accuracy compared to 76.8\% for the audio-only and 70.0\% for the video-only equivalent systems.",
    url = "https://arxiv.org/abs/2011.00030"
}

@inproceedings{gan2019self,
  title={Self-supervised moving vehicle tracking with stereo sound},
  author={Gan, Chuang and Zhao, Hang and Chen, Peihao and Cox, David and Torralba, Antonio},
  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},
  pages={7053--7062},
  year={2019}
}

@ARTICLE {9726801,
author = {D. Dai and A. Vasudevan and J. Matas and L. Van Gool},
journal = {IEEE Transactions on Pattern Analysis & Machine Intelligence},
title = {Binaural SoundNet: Predicting Semantics, Depth and Motion with Binaural Sounds},
year = {5555},
volume = {},
number = {01},
issn = {1939-3539},
pages = {1-1},
keywords = {task analysis;visualization;semantics;videos;spatial resolution;microphones;location awareness},
doi = {10.1109/TPAMI.2022.3155643},
publisher = {IEEE Computer Society},
address = {Los Alamitos, CA, USA},
month = {mar}
}

@article{blumstein2011acoustic,
  title={Acoustic monitoring in terrestrial environments using microphone arrays: applications, technological considerations and prospectus},
  author={Blumstein, Daniel T and Mennill, Daniel J and Clemins, Patrick and Girod, Lewis and Yao, Kung and Patricelli, Gail and Deppe, Jill L and Krakauer, Alan H and Clark, Christopher and Cortopassi, Kathryn A and others},
  journal={Journal of Applied Ecology},
  volume={48},
  number={3},
  pages={758--767},
  year={2011},
  publisher={Wiley Online Library}
}

@article{bardeli2010detecting,
  title={Detecting bird sounds in a complex acoustic environment and application to bioacoustic monitoring},
  author={Bardeli, Rolf and Wolff, Daniel and Kurth, Frank and Koch, Martina and Tauchert, K-H and Frommolt, K-H},
  journal={Pattern Recognition Letters},
  volume={31},
  number={12},
  pages={1524--1534},
  year={2010},
  publisher={Elsevier}
}

@inproceedings{kamath2021mdetr,
  title={MDETR-modulated detection for end-to-end multi-modal understanding},
  author={Kamath, Aishwarya and Singh, Mannat and LeCun, Yann and Synnaeve, Gabriel and Misra, Ishan and Carion, Nicolas},
  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},
  pages={1780--1790},
  year={2021}
}

@inproceedings{chen2020uniter,
  title={Uniter: Universal image-text representation learning},
  author={Chen, Yen-Chun and Li, Linjie and Yu, Licheng and El Kholy, Ahmed and Ahmed, Faisal and Gan, Zhe and Cheng, Yu and Liu, Jingjing},
  booktitle={European conference on computer vision},
  pages={104--120},
  year={2020},
  organization={Springer}
}

@inproceedings{tian2018audio,
  title={Audio-visual event localization in unconstrained videos},
  author={Tian, Yapeng and Shi, Jing and Li, Bochen and Duan, Zhiyao and Xu, Chenliang},
  booktitle={Proceedings of the European Conference on Computer Vision (ECCV)},
  pages={247--263},
  year={2018}
}
@inproceedings{zhao2019sound,
  title={The sound of motions},
  author={Zhao, Hang and Gan, Chuang and Ma, Wei-Chiu and Torralba, Antonio},
  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},
  pages={1735--1744},
  year={2019}
}
@inproceedings{afouras2020self,
  title={Self-supervised learning of audio-visual objects from video},
  author={Afouras, Triantafyllos and Owens, Andrew and Chung, Joon Son and Zisserman, Andrew},
  booktitle={European Conference on Computer Vision},
  pages={208--224},
  year={2020},
  organization={Springer}
}

@inproceedings{owens2018audio,
  title={Audio-visual scene analysis with self-supervised multisensory features},
  author={Owens, Andrew and Efros, Alexei A},
  booktitle={Proceedings of the European Conference on Computer Vision (ECCV)},
  pages={631--648},
  year={2018}
}

@article{dosovitskiy2020image,
  title={An image is worth 16x16 words: Transformers for image recognition at scale},
  author={Dosovitskiy, Alexey and Beyer, Lucas and Kolesnikov, Alexander and Weissenborn, Dirk and Zhai, Xiaohua and Unterthiner, Thomas and Dehghani, Mostafa and Minderer, Matthias and Heigold, Georg and Gelly, Sylvain and others},
  journal={ICLR},
  year={2021}
}

@article{tzinis2021into,
  title={Into the wild with audioscope: Unsupervised audio-visual separation of on-screen sounds},
  author={Tzinis, Efthymios and Wisdom, Scott and Jansen, Aren and Hershey, Shawn and Remez, Tal and Ellis, Daniel PW and Hershey, John R},
  journal={ICLR},
  year={2021}
}

@article{guidotti2018survey,
  title={A survey of methods for explaining black box models},
  author={Guidotti, Riccardo and Monreale, Anna and Ruggieri, Salvatore and Turini, Franco and Giannotti, Fosca and Pedreschi, Dino},
  journal={ACM computing surveys (CSUR)},
  volume={51},
  number={5},
  pages={1--42},
  year={2018},
  publisher={ACM New York, NY, USA}
}

@inproceedings{hu2019deep,
  title={Deep multimodal clustering for unsupervised audiovisual learning},
  author={Hu, Di and Nie, Feiping and Li, Xuelong},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={9248--9257},
  year={2019}
}

@article{lin2021unsupervised,
  title={Unsupervised sound localization via iterative contrastive learning},
  author={Lin, Yan-Bo and Tseng, Hung-Yu and Lee, Hsin-Ying and Lin, Yen-Yu and Yang, Ming-Hsuan},
  journal={CVPR Sight and Sound Workshop},
  year={2021}
}

@article{dabkowski2017real,
  title={Real time image saliency for black box classifiers},
  author={Dabkowski, Piotr and Gal, Yarin},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}

@article{zhang2018top,
  title={Top-down neural attention by excitation backprop},
  author={Zhang, Jianming and Bargal, Sarah Adel and Lin, Zhe and Brandt, Jonathan and Shen, Xiaohui and Sclaroff, Stan},
  journal={International Journal of Computer Vision},
  volume={126},
  number={10},
  pages={1084--1102},
  year={2018},
  publisher={Springer}
}

@article{erhan2009visualizing,
  title={Visualizing higher-layer features of a deep network},
  author={Erhan, Dumitru and Bengio, Yoshua and Courville, Aaron and Vincent, Pascal},
  journal={University of Montreal},
  volume={1341},
  number={3},
  pages={1},
  year={2009}
}

@article{bach2015pixel,
  title={On pixel-wise explanations for non-linear classifier decisions by layer-wise relevance propagation},
  author={Bach, Sebastian and Binder, Alexander and Montavon, Gr{\'e}goire and Klauschen, Frederick and M{\"u}ller, Klaus-Robert and Samek, Wojciech},
  journal={PloS one},
  volume={10},
  number={7},
  pages={e0130140},
  year={2015},
  publisher={Public Library of Science San Francisco, CA USA}
}

@article{fukui2016multimodal,
  title={Multimodal compact bilinear pooling for visual question answering and visual grounding},
  author={Fukui, Akira and Park, Dong Huk and Yang, Daylen and Rohrbach, Anna and Darrell, Trevor and Rohrbach, Marcus},
  journal={EMNLP},
  year={2016}
}

@inproceedings{ramaswamy2020see,
  title={See the sound, hear the pixels},
  author={Ramaswamy, Janani and Das, Sukhendu},
  booktitle={Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision},
  pages={2970--2979},
  year={2020}
}

@article{politis2020overview,
    author = "Politis, Archontis and Mesaros, Annamaria and Adavanne, Sharath and Heittola, Toni and Virtanen, Tuomas",
    title = "Overview and Evaluation of Sound Event Localization and Detection in DCASE 2019",
    journal = "IEEE/ACM Transactions on Audio, Speech, and Language Processing",
    volume = "29",
    pages = "684--698",
    year = "2020",
    publisher = "IEEE",
    abstract = "Sound event localization and detection is a novel area of research that emerged from the combined interest of analyzing the acoustic scene in terms of the spatial and temporal activity of sounds of interest. This paper presents an overview of the first international evaluation on sound event localization and detection, organized as a task of the DCASE 2019 Challenge. A large-scale realistic dataset of spatialized sound events was generated for the challenge, to be used for training of learning-based approaches, and for evaluation of the submissions in an unlabeled subset. The overview presents in detail how the systems were evaluated and ranked and the characteristics of the best-performing systems. Common strategies in terms of input features, model architectures, training approaches, exploitation of prior knowledge, and data augmentation are discussed. Since ranking in the challenge was based on individually evaluating localization and event classification performance, part of the overview focuses on presenting metrics for the joint measurement of the two, together with a reevaluation of submissions using these new metrics. The new analysis reveals submissions that performed better on the joint task of detecting the correct type of event close to its original location than some of the submissions that were ranked higher in the challenge. Consequently, ranking of submissions which performed strongly when evaluated separately on detection or localization, but not jointly on both, was affected negatively.",
    url = "https://https://arxiv.org/abs/2009.02792"
}

@article{Adavanne2018_JSTSP,
    author = "Adavanne, Sharath and Politis, Archontis and Nikunen, Joonas and Virtanen, Tuomas",
    journal = "IEEE Journal of Selected Topics in Signal Processing",
    title = "Sound Event Localization and Detection of Overlapping Sources Using Convolutional Recurrent Neural Networks",
    year = "2018",
    volume = "13",
    number = "1",
    pages = "34-48",
    keywords = "Direction-of-arrival estimation;Estimation;Task analysis;Azimuth;Microphone arrays;Recurrent neural networks;Sound event detection;direction of arrival estimation;convolutional recurrent neural network",
    abstract = "In this paper, we propose a convolutional recurrent neural network for joint sound event localization and detection (SELD) of multiple overlapping sound events in three-dimensional (3D) space. The proposed network takes a sequence of consecutive spectrogram time-frames as input and maps it to two outputs in parallel. As the first output, the sound event detection (SED) is performed as a multi-label classification task on each time-frame producing temporal activity for all the sound event classes. As the second output, localization is performed by estimating the 3D Cartesian coordinates of the direction-of-arrival (DOA) for each sound event class using multi-output regression. The proposed method is able to associate multiple DOAs with respective sound event labels and further track this association with respect to time. The proposed method uses separately the phase and magnitude component of the spectrogram calculated on each audio channel as the feature, thereby avoiding any method- and array-specific feature extraction. The method is evaluated on five Ambisonic and two circular array format datasets with different overlapping sound events in anechoic, reverberant and real-life scenarios. The proposed method is compared with two SED, three DOA estimation, and one SELD baselines. The results show that the proposed method is generic and applicable to any array structures, robust to unseen DOA values, reverberation, and low SNR scenarios. The proposed method achieved a consistently higher recall of the estimated number of DOAs across datasets in comparison to the best baseline. Additionally, this recall was observed to be significantly better than the best baseline method for a higher number of overlapping sound events.",
    doi = "10.1109/JSTSP.2018.2885636",
    issn = "1932-4553",
    month = "March",
    url = "https://ieeexplore.ieee.org/abstract/document/8567942"
}