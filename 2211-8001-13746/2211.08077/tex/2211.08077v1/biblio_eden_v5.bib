
@inproceedings{jia_spatio-temporal_2017,
	address = {Kansas City, MO},
	title = {Spatio-temporal autoencoder for feature learning in patient data with missing observations},
	isbn = {978-1-5090-3050-7},
	url = {http://ieeexplore.ieee.org/document/8217773/},
	doi = {10.1109/BIBM.2017.8217773},
	abstract = {Modern patient data tends to be large-scale and multi-dimensional, containing both spatial and temporal features. Learning good spatio-temporal features from large patient data is a challenging task, especially when there are missing observations. In this paper, we propose a spatio-temporal autoencoder (STAE), an unsupervised deep learning scheme, to learn features from large-scale and high-dimensional patient data with missing observations. Through both spatial and temporal encoding, STAE is able to automatically identify patterns and dependencies in the patient data, even with missing values, and learn a compact representation of each patient for better classiﬁcation. Publicly available electroencephalogram (EEG) data are extracted from the UCI Machine Learning Repository to test and support our ﬁndings. Through simulations, we compare STAE with several baseline feature selection methods and demonstrate its effectiveness in the presence of missing data.},
	language = {en},
	urldate = {2019-05-26},
	booktitle = {2017 {IEEE} {International} {Conference} on {Bioinformatics} and {Biomedicine} ({BIBM})},
	publisher = {IEEE},
	author = {Jia, Yao and Zhou, Chongyu and Motani, Mehul},
	month = nov,
	year = {2017},
	pages = {886--890},
	file = {Jia et al. - 2017 - Spatio-temporal autoencoder for feature learning i.pdf:/Users/elisedumas/Zotero/storage/W3V3SKTC/Jia et al. - 2017 - Spatio-temporal autoencoder for feature learning i.pdf:application/pdf},
}

@article{kingma_adam:_2014,
	title = {Adam: {A} {Method} for {Stochastic} {Optimization}},
	shorttitle = {Adam},
	url = {http://arxiv.org/abs/1412.6980},
	abstract = {We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments. The method is straightforward to implement, is computationally efficient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. Empirical results demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. Finally, we discuss AdaMax, a variant of Adam based on the infinity norm.},
	urldate = {2019-08-26},
	journal = {arXiv:1412.6980 [cs]},
	author = {Kingma, Diederik P. and Ba, Jimmy},
	month = dec,
	year = {2014},
	note = {arXiv: 1412.6980},
	keywords = {Computer Science - Machine Learning},
	file = {arXiv\:1412.6980 PDF:/Users/elisedumas/Zotero/storage/V8Y967XR/Kingma and Ba - 2014 - Adam A Method for Stochastic Optimization.pdf:application/pdf;arXiv.org Snapshot:/Users/elisedumas/Zotero/storage/NGIYYJK9/1412.html:text/html},
}

@inproceedings{lin_early_2018,
	title = {Early {Diagnosis} and {Prediction} of {Sepsis} {Shock} by {Combining} {Static} and {Dynamic} {Information} {Using} {Convolutional}-{LSTM}},
	doi = {10.1109/ICHI.2018.00032},
	abstract = {Deep neural network models, especially Long Short Term Memory (LSTM), have shown great success in analyzing Electronic Health Records (EHRs) due to their ability to capture temporal dependencies in time series data. In this paper, we proposed a general deep neural network framework which incorporates two additional components with the aim of improving LSTM. The first component, a Convolutional Neural Network (CNN), is added before LSTM to obtain local characteristics of EHRs. The second component, a fully connected neural network (FC), introduces static information (e.g., age) to LSTM, which is applied to handle dynamic information (e.g., lab result). The medical condition we aim to predict is septic shock - it is the most advanced complication of sepsis and is due to severe abnormalities in circulation and/or cellular metabolism. Our proposed framework was evaluated for two experimental tasks: visit level early diagnosis (left align) and event level early prediction (right align). Our results show that for visit level early diagnosis, by incorporating both CNN and static information, our framework consistently outperforms the original LSTM. For event level early prediction, the same outcome is observed when predicting {\textless}; 5 hours into the future, however, when predicting ≥ 5 hours into the future, the addition of the CNN component alone obtains the best results.},
	booktitle = {2018 {IEEE} {International} {Conference} on {Healthcare} {Informatics} ({ICHI})},
	author = {Lin, Chen and Zhang, Yuan and Ivy, Julie and Capan, Muge and Arnold, Ryan and Huddleston, Jeanne M. and Chi, Min},
	month = jun,
	year = {2018},
	note = {ISSN: 2575-2634},
	keywords = {Electronic Health Records, Computer architecture, Task analysis, LSTM, cellular metabolism, CNN component, convolution, Convolutional Neural Network, convolutional-LSTM, deep neural network framework, Disease Prediction, Diseases, dynamic information, Early Diagnosis, Electric shock, electronic health records, Feature extraction, feedforward neural nets, fully connected neural network, Logic gates, long short term memory, Long Short Term Memory, Neural networks, sepsis shock, septic shock, Septic Shock, static information, time series, time series data},
	pages = {219--228},
	file = {IEEE Xplore Abstract Record:/Users/elisedumas/Zotero/storage/AKZAINYM/8419365.html:text/html},
}

@article{bang_phased-lstm_nodate,
	title = {Phased-{LSTM} {Based} {Predictive} {Model} for longitudinal {EHR} {Data} with {Missing} {Values}},
	language = {en},
	author = {Bang, Seo-Jin and Wang, Yuchuan and Yang, Yang},
	pages = {9},
	file = {Bang et al. - Phased-LSTM Based Predictive Model for longitudina.pdf:/Users/elisedumas/Zotero/storage/XZLBWWER/Bang et al. - Phased-LSTM Based Predictive Model for longitudina.pdf:application/pdf},
}

@article{xu_development_2019,
	title = {Development and validation of case-finding algorithms for recurrence of breast cancer using routinely collected administrative data},
	volume = {19},
	issn = {1471-2407},
	url = {https://bmccancer.biomedcentral.com/articles/10.1186/s12885-019-5432-8},
	doi = {10.1186/s12885-019-5432-8},
	abstract = {Background: Recurrence is not explicitly documented in cancer registry data that are widely used for research. Patterns of events after initial treatment such as oncology visits, re-operation, and receipt of subsequent chemotherapy or radiation may indicate recurrence. This study aimed to develop and validate algorithms for identifying breast cancer recurrence using routinely collected administrative data.
Methods: The study cohort included all young (≤ 40 years) breast cancer patients (2007–2010), and all patients receiving neoadjuvant chemotherapy (2012–2014) in Alberta, Canada. Health events (including mastectomy, chemotherapy, radiation, biopsy and specialist visits) were obtained from provincial administrative data. The algorithms were developed using classification and regression tree (CART) models and validated against primary chart review.
Results: Among 598 patients, 121 (20.2\%) had recurrence after a median follow-up of 4 years. The high sensitivity algorithm achieved 94.2\% (95\% CI: 90.1–98.4\%) sensitivity, 93.7\% (91.5–95.9\%) specificity, 79.2\% (72.5–85.8\%) positive predictive value (PPV), and 98.5\% (97.3–99.6\%) negative predictive value (NPV). The high PPV algorithm had 75.2\% (67.5–82.9\%) sensitivity, 98.3\% (97.2–99.5\%) specificity, 91.9\% (86.6–97.3\%) PPV, and 94\% (91.9–96.1\%) NPV. Combining high PPV and high sensitivity algorithms with additional (7.5\%) chart review to resolve discordant cases resulted in 94.2\% (90.1–98.4\%) sensitivity, 98.3\% (97.2–99.5\%) specificity, 93.4\% (89.1–97.8\%) PPV, and 98.5\% (97.4–99.6\%) NPV.
Conclusion: The proposed algorithms based on routinely collected administrative data achieved favorably high validity for identifying breast cancer recurrences in a universal healthcare system in Canada.},
	language = {en},
	number = {1},
	urldate = {2020-12-02},
	journal = {BMC Cancer},
	author = {Xu, Yuan and Kong, Shiying and Cheung, Winson Y. and Bouchard-Fortier, Antoine and Dort, Joseph C. and Quan, Hude and Buie, Elizabeth M. and McKinnon, Geoff and Quan, May Lynn},
	month = dec,
	year = {2019},
	pages = {210},
	file = {Xu et al. - 2019 - Development and validation of case-finding algorit.pdf:/Users/elisedumas/Zotero/storage/ZQXLSQ79/Xu et al. - 2019 - Development and validation of case-finding algorit.pdf:application/pdf},
}

@article{chubak_administrative_2012,
	title = {Administrative {Data} {Algorithms} to {Identify} {Second} {Breast} {Cancer} {Events} {Following} {Early}-{Stage} {Invasive} {Breast} {Cancer}},
	volume = {104},
	issn = {1460-2105, 0027-8874},
	url = {https://academic.oup.com/jnci/article-lookup/doi/10.1093/jnci/djs233},
	doi = {10.1093/jnci/djs233},
	language = {en},
	number = {12},
	urldate = {2020-12-02},
	journal = {JNCI: Journal of the National Cancer Institute},
	author = {Chubak, Jessica and Yu, Onchee and Pocobelli, Gaia and Lamerato, Lois and Webster, Joe and Prout, Marianne N. and Ulcickas Yood, Marianne and Barlow, William E. and Buist, Diana S. M.},
	month = jun,
	year = {2012},
	pages = {931--940},
	file = {Chubak et al. - 2012 - Administrative Data Algorithms to Identify Second .pdf:/Users/elisedumas/Zotero/storage/AFV64L35/Chubak et al. - 2012 - Administrative Data Algorithms to Identify Second .pdf:application/pdf},
}

@article{ren_deep_2018,
	title = {Deep {Recurrent} {Survival} {Analysis}},
	url = {http://arxiv.org/abs/1809.02403},
	abstract = {Survival analysis is a hotspot in statistical research for modeling time-to-event information with data censorship handling, which has been widely used in many applications such as clinical research, information system and other fields with survivorship bias. Many works have been proposed for survival analysis ranging from traditional statistic methods to machine learning models. However, the existing methodologies either utilize counting-based statistics on the segmented data, or have a pre-assumption on the event probability distribution w.r.t. time. Moreover, few works consider sequential patterns within the feature space. In this paper, we propose a Deep Recurrent Survival Analysis model which combines deep learning for conditional probability prediction at fine-grained level of the data, and survival analysis for tackling the censorship. By capturing the time dependency through modeling the conditional probability of the event for each sample, our method predicts the likelihood of the true event occurrence and estimates the survival rate over time, i.e., the probability of the non-occurrence of the event, for the censored data. Meanwhile, without assuming any specific form of the event probability distribution, our model shows great advantages over the previous works on fitting various sophisticated data distributions. In the experiments on the three real-world tasks from different fields, our model significantly outperforms the state-of-the-art solutions under various metrics.},
	urldate = {2020-12-02},
	journal = {arXiv:1809.02403 [cs, stat]},
	author = {Ren, Kan and Qin, Jiarui and Zheng, Lei and Yang, Zhengyu and Zhang, Weinan and Qiu, Lin and Yu, Yong},
	month = nov,
	year = {2018},
	note = {arXiv: 1809.02403},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Survival\_detection},
	file = {arXiv Fulltext PDF:/Users/elisedumas/Zotero/storage/NQ3DCF7M/Ren et al. - 2018 - Deep Recurrent Survival Analysis.pdf:application/pdf;arXiv.org Snapshot:/Users/elisedumas/Zotero/storage/7VCRD8QL/1809.html:text/html},
}

@article{zhang_survival_2020,
	title = {Survival neural networks for time-to-event prediction in longitudinal study},
	volume = {62},
	issn = {0219-1377, 0219-3116},
	url = {http://link.springer.com/10.1007/s10115-020-01472-1},
	doi = {10.1007/s10115-020-01472-1},
	language = {en},
	number = {9},
	urldate = {2020-12-04},
	journal = {Knowledge and Information Systems},
	author = {Zhang, Jianfei and Chen, Lifei and Ye, Yanfang and Guo, Gongde and Chen, Rongbo and Vanasse, Alain and Wang, Shengrui},
	month = sep,
	year = {2020},
	keywords = {Survival\_detection},
	pages = {3727--3751},
	file = {Zhang et al. - 2020 - Survival neural networks for time-to-event predict.pdf:/Users/elisedumas/Zotero/storage/VTEZNZ2K/Zhang et al. - 2020 - Survival neural networks for time-to-event predict.pdf:application/pdf},
}

@article{katzman_deepsurv_2018,
	title = {{DeepSurv}: personalized treatment recommender system using a {Cox} proportional hazards deep neural network},
	volume = {18},
	issn = {1471-2288},
	shorttitle = {{DeepSurv}},
	url = {https://doi.org/10.1186/s12874-018-0482-1},
	doi = {10.1186/s12874-018-0482-1},
	abstract = {Medical practitioners use survival models to explore and understand the relationships between patients’ covariates (e.g. clinical and genetic features) and the effectiveness of various treatment options. Standard survival models like the linear Cox proportional hazards model require extensive feature engineering or prior medical knowledge to model treatment interaction at an individual level. While nonlinear survival methods, such as neural networks and survival forests, can inherently model these high-level interaction terms, they have yet to be shown as effective treatment recommender systems.},
	number = {1},
	urldate = {2020-12-04},
	journal = {BMC Medical Research Methodology},
	author = {Katzman, Jared L. and Shaham, Uri and Cloninger, Alexander and Bates, Jonathan and Jiang, Tingting and Kluger, Yuval},
	month = feb,
	year = {2018},
	keywords = {Deep learning, Survival analysis, Treatment recommendations, Survival\_detection},
	pages = {24},
	file = {Full Text:/Users/elisedumas/Zotero/storage/D3P2ELU5/Katzman et al. - 2018 - DeepSurv personalized treatment recommender syste.pdf:application/pdf;Snapshot:/Users/elisedumas/Zotero/storage/PB9TKML2/s12874-018-0482-1.html:text/html},
}

@article{kvamme_continuous_2019,
	title = {Continuous and {Discrete}-{Time} {Survival} {Prediction} with {Neural} {Networks}},
	url = {http://arxiv.org/abs/1910.06724},
	abstract = {Application of discrete-time survival methods for continuous-time survival prediction is considered. For this purpose, a scheme for discretization of continuous-time data is proposed by considering the quantiles of the estimated event-time distribution, and, for smaller data sets, it is found to be preferable over the commonly used equidistant scheme. Furthermore, two interpolation schemes for continuous-time survival estimates are explored, both of which are shown to yield improved performance compared to the discrete-time estimates. The survival methods considered are based on the likelihood for right-censored survival data, and parameterize either the probability mass function (PMF) or the discrete-time hazard rate, both with neural networks. Through simulations and study of real-world data, the hazard rate parametrization is found to perform slightly better than the parametrization of the PMF. Inspired by these investigations, a continuous-time method is proposed by assuming that the continuous-time hazard rate is piecewise constant. The method, named PC-Hazard, is found to be highly competitive with the aforementioned methods in addition to other methods for survival prediction found in the literature.},
	urldate = {2020-12-10},
	journal = {arXiv:1910.06724 [cs, stat]},
	author = {Kvamme, Håvard and Borgan, Ørnulf},
	month = oct,
	year = {2019},
	note = {arXiv: 1910.06724},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Statistics - Methodology, Survival\_detection},
	file = {arXiv Fulltext PDF:/Users/elisedumas/Zotero/storage/PAVCVDUW/Kvamme and Borgan - 2019 - Continuous and Discrete-Time Survival Prediction w.pdf:application/pdf;arXiv.org Snapshot:/Users/elisedumas/Zotero/storage/QPJPMKTZ/1910.html:text/html},
}

@article{fotso_deep_2018,
	title = {Deep {Neural} {Networks} for {Survival} {Analysis} {Based} on a {Multi}-{Task} {Framework}},
	url = {http://arxiv.org/abs/1801.05512},
	abstract = {Survival analysis/time-to-event models are extremely useful as they can help companies predict when a customer will buy a product, churn or default on a loan, and therefore help them improve their ROI. In this paper, we introduce a new method to calculate survival functions using the Multi-Task Logistic Regression (MTLR) model as its base and a deep learning architecture as its core. Based on the Concordance index (C-index) and Brier score, this method outperforms the MTLR in all the experiments disclosed in this paper as well as the Cox Proportional Hazard (CoxPH) model when nonlinear dependencies are found.},
	urldate = {2020-12-10},
	journal = {arXiv:1801.05512 [cs, stat]},
	author = {Fotso, Stephane},
	month = jan,
	year = {2018},
	note = {arXiv: 1801.05512},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, à lire ++++, Survival\_detection},
	file = {arXiv Fulltext PDF:/Users/elisedumas/Zotero/storage/TVSG9L3Z/Fotso - 2018 - Deep Neural Networks for Survival Analysis Based o.pdf:application/pdf;arXiv.org Snapshot:/Users/elisedumas/Zotero/storage/YC7Z96FP/1801.html:text/html},
}

@article{tuppin_value_2017,
	title = {Value of a national administrative database to guide public decisions: {From} the système national d'information interrégimes de l'{Assurance} {Maladie} ({SNIIRAM}) to the système national des données de santé ({SNDS}) in {France}},
	volume = {65 Suppl 4},
	issn = {0398-7620},
	shorttitle = {Value of a national administrative database to guide public decisions},
	doi = {10.1016/j.respe.2017.05.004},
	abstract = {In 1999, French legislators asked health insurance funds to develop a système national d'information interrégimes de l'Assurance Maladie (SNIIRAM) [national health insurance information system] in order to more precisely determine and evaluate health care utilization and health care expenditure of beneficiaries. These data, based on almost 66 million inhabitants in 2015, have already been the subject of numerous international publications on various topics: prevalence and incidence of diseases, patient care pathways, health status and health care utilization of specific populations, real-life use of drugs, assessment of adverse effects of drugs or other health care procedures, monitoring of national health insurance expenditure, etc. SNIIRAM comprises individual information on the sociodemographic and medical characteristics of beneficiaries and all hospital care and office medicine reimbursements, coded according to various systems. Access to data is controlled by permissions dependent on the type of data requested or used, their temporality and the researcher's status. In general, data can be analyzed by accredited agencies over a period covering the last three years plus the current year, and specific requests can be submitted to extract data over longer periods. A 1/97th random sample of SNIIRAM, the échantillon généraliste des bénéficiaires (EGB), representative of the national population of health insurance beneficiaries, was composed in 2005 to allow 20-year follow-up with facilitated access for medical research. The EGB is an open cohort, which includes new beneficiaries and newborn infants. SNIIRAM has continued to grow and extend to become, in 2016, the cornerstone of the future système national des données de santé (SNDS) [national health data system], which will gradually integrate new information (causes of death, social and medical data and complementary health insurance). In parallel, the modalities of data access and protection systems have also evolved. This article describes the SNIIRAM data warehouse and its transformation into SNDS, the data collected, the tools developed in order to facilitate data analysis, the limitations encountered, and changing access permissions.},
	language = {eng},
	journal = {Revue D'epidemiologie Et De Sante Publique},
	author = {Tuppin, P. and Rudant, J. and Constantinou, P. and Gastaldi-Ménager, C. and Rachas, A. and de Roquefeuil, L. and Maura, G. and Caillol, H. and Tajahmady, A. and Coste, J. and Gissot, C. and Weill, A. and Fagot-Campagna, A.},
	month = oct,
	year = {2017},
	pmid = {28756037},
	keywords = {Epidemiology, Humans, Databases, Factual, France, Medical Records Systems, Computerized, National Health Programs, Health insurance, Decision Making, Assurance Maladie, Consommation de soins, Épidémiologie, French national health data system, Health care use, Public Health Administration, Public Health Practice, Système national d’information en santé},
	pages = {S149--S167},
}

@article{gensheimer_scalable_2019,
	title = {A scalable discrete-time survival model for neural networks},
	volume = {7},
	issn = {2167-8359},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6348952/},
	doi = {10.7717/peerj.6257},
	abstract = {There is currently great interest in applying neural networks to prediction tasks in medicine. It is important for predictive models to be able to use survival data, where each patient has a known follow-up time and event/censoring indicator. This avoids information loss when training the model and enables generation of predicted survival curves. In this paper, we describe a discrete-time survival model that is designed to be used with neural networks, which we refer to as Nnet-survival. The model is trained with the maximum likelihood method using mini-batch stochastic gradient descent (SGD). The use of SGD enables rapid convergence and application to large datasets that do not fit in memory. The model is flexible, so that the baseline hazard rate and the effect of the input data on hazard probability can vary with follow-up time. It has been implemented in the Keras deep learning framework, and source code for the model and several examples is available online. We demonstrate the performance of the model on both simulated and real data and compare it to existing models Cox-nnet and Deepsurv.},
	urldate = {2021-07-19},
	journal = {PeerJ},
	author = {Gensheimer, Michael F. and Narasimhan, Balasubramanian},
	month = jan,
	year = {2019},
	pmid = {30701130},
	pmcid = {PMC6348952},
	keywords = {à lire ++++, Survival\_detection},
	pages = {e6257},
	file = {PubMed Central Full Text PDF:/Users/elisedumas/Zotero/storage/EGXMBE7R/Gensheimer and Narasimhan - 2019 - A scalable discrete-time survival model for neural.pdf:application/pdf},
}

@article{ching_cox-nnet_2018,
	title = {Cox-nnet: {An} artificial neural network method for prognosis prediction of high-throughput omics data},
	volume = {14},
	issn = {1553-734X},
	shorttitle = {Cox-nnet},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5909924/},
	doi = {10.1371/journal.pcbi.1006076},
	abstract = {Artificial neural networks (ANN) are computing architectures with many interconnections of simple neural-inspired computing elements, and have been applied to biomedical fields such as imaging analysis and diagnosis. We have developed a new ANN framework called Cox-nnet to predict patient prognosis from high throughput transcriptomics data. In 10 TCGA RNA-Seq data sets, Cox-nnet achieves the same or better predictive accuracy compared to other methods, including Cox-proportional hazards regression (with LASSO, ridge, and mimimax concave penalty), Random Forests Survival and CoxBoost. Cox-nnet also reveals richer biological information, at both the pathway and gene levels. The outputs from the hidden layer node provide an alternative approach for survival-sensitive dimension reduction. In summary, we have developed a new method for accurate and efficient prognosis prediction on high throughput data, with functional biological insights. The source code is freely available at https://github.com/lanagarmire/cox-nnet., The increasing application of high-througput transcriptomics data to predict patient prognosis demands modern computational methods. With the re-gaining popularity of artificial neural networks, we asked if a refined neural network model could be used to predict patient survival, as an alternative to the conventional methods, such as Cox proportional hazards (Cox-PH) methods with LASSO or ridge penalization. To this end, we have developed a neural network extension of the Cox regression model, called Cox-nnet. It is optimized for survival prediction from high throughput gene expression data, with comparable or better performance than other conventional methods. More importantly, Cox-nnet reveals much richer biological information, at both the pathway and gene levels, by analyzing features represented in the hidden layer nodes in Cox-nnet. Additionally, we propose to use hidden node features as a new approach for dimension reduction during survival data analysis.},
	number = {4},
	urldate = {2021-07-22},
	journal = {PLoS Computational Biology},
	author = {Ching, Travers and Zhu, Xun and Garmire, Lana X.},
	month = apr,
	year = {2018},
	pmid = {29634719},
	pmcid = {PMC5909924},
	keywords = {Survival\_detection},
	pages = {e1006076},
	file = {PubMed Central Full Text PDF:/Users/elisedumas/Zotero/storage/Y7HVI3S9/Ching et al. - 2018 - Cox-nnet An artificial neural network method for .pdf:application/pdf},
}

@article{lee_deephit_2018,
	title = {{DeepHit}: {A} {Deep} {Learning} {Approach} to {Survival} {Analysis} {With} {Competing} {Risks}},
	volume = {32},
	copyright = {Copyright (c)},
	issn = {2374-3468},
	shorttitle = {{DeepHit}},
	url = {https://ojs.aaai.org/index.php/AAAI/article/view/11842},
	doi = {10.1609/aaai.v32i1.11842},
	abstract = {Survival analysis (time-to-event analysis) is widely used in economics and finance, engineering, medicine and many other areas. A fundamental problem is to understand the relationship between the covariates and the (distribution of) survival times(times-to-event). Much of the previous work has approached the problem by viewing the survival time as the first hitting time of a stochastic process, assuming a specific form for the underlying stochastic process, using available data to learn the relationship between the covariates and the parameters of the model, and then deducing the relationship between covariates and the distribution of first hitting times (the risk). However, previous models rely on strong parametric assumptions that are often violated. This paper proposes a very different approach to survival analysis, DeepHit, that uses a deep neural network to learn the distribution of survival times directly.DeepHit makes no assumptions about the underlying stochastic process and allows for the possibility that the relationship between covariates and risk(s) changes over time. Most importantly, DeepHit smoothly handles competing risks; i.e. settings in which there is more than one possible event of interest.Comparisons with previous models on the basis of real and synthetic datasets demonstrate that DeepHit achieves large and statistically significant performance improvements over previous state-of-the-art methods.},
	language = {en},
	number = {1},
	urldate = {2022-07-19},
	journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
	author = {Lee, Changhee and Zame, William and Yoon, Jinsung and Schaar, Mihaela van der},
	month = apr,
	year = {2018},
	note = {Number: 1},
	keywords = {read, first-hitting-time analysis},
	file = {Lee et al. - 2018 - DeepHit A Deep Learning Approach to Survival Anal.pdf:/Users/elisedumas/Zotero/storage/NRPJEBW8/Lee et al. - 2018 - DeepHit A Deep Learning Approach to Survival Anal.pdf:application/pdf},
}

@article{kam_learning_2017,
	title = {Learning representations for the early detection of sepsis with deep neural networks},
	volume = {89},
	issn = {1879-0534},
	doi = {10.1016/j.compbiomed.2017.08.015},
	abstract = {BACKGROUND: Sepsis is one of the leading causes of death in intensive care unit patients. Early detection of sepsis is vital because mortality increases as the sepsis stage worsens.
OBJECTIVE: This study aimed to develop detection models for the early stage of sepsis using deep learning methodologies, and to compare the feasibility and performance of the new deep learning methodology with those of the regression method with conventional temporal feature extraction.
METHOD: Study group selection adhered to the InSight model. The results of the deep learning-based models and the InSight model were compared.
RESULTS: With deep feedforward networks, the area under the ROC curve (AUC) of the models were 0.887 and 0.915 for the InSight and the new feature sets, respectively. For the model with the combined feature set, the AUC was the same as that of the basic feature set (0.915). For the long short-term memory model, only the basic feature set was applied and the AUC improved to 0.929 compared with the existing 0.887 of the InSight model.
CONCLUSIONS: The contributions of this paper can be summarized in three ways: (i) improved performance without feature extraction using domain knowledge, (ii) verification of feature extraction capability of deep neural networks through comparison with reference features, and (iii) improved performance with feedforward neural networks using long short-term memory, a neural network architecture that can learn sequential patterns.},
	language = {eng},
	journal = {Computers in Biology and Medicine},
	author = {Kam, Hye Jin and Kim, Ha Young},
	month = oct,
	year = {2017},
	pmid = {28843829},
	keywords = {Adult, Female, Humans, Male, Machine Learning, Deep learning, LSTM, Feature extraction, Neural Networks, Computer, Models, Biological, Critical Care, read, Sepsis, Clinical decision support system, Early detection, Multivariate time-series},
	pages = {248--255},
	file = {Kam and Kim - 2017 - Learning representations for the early detection o.pdf:/Users/elisedumas/Zotero/storage/FS4PF3N4/Kam and Kim - 2017 - Learning representations for the early detection o.pdf:application/pdf},
}

@misc{choi_retain_2017,
	title = {{RETAIN}: {An} {Interpretable} {Predictive} {Model} for {Healthcare} using {Reverse} {Time} {Attention} {Mechanism}},
	shorttitle = {{RETAIN}},
	url = {http://arxiv.org/abs/1608.05745},
	abstract = {Accuracy and interpretability are two dominant features of successful predictive models. Typically, a choice must be made in favor of complex black box models such as recurrent neural networks (RNN) for accuracy versus less accurate but more interpretable traditional models such as logistic regression. This tradeoff poses challenges in medicine where both accuracy and interpretability are important. We addressed this challenge by developing the REverse Time AttentIoN model (RETAIN) for application to Electronic Health Records (EHR) data. RETAIN achieves high accuracy while remaining clinically interpretable and is based on a two-level neural attention model that detects influential past visits and significant clinical variables within those visits (e.g. key diagnoses). RETAIN mimics physician practice by attending the EHR data in a reverse time order so that recent clinical visits are likely to receive higher attention. RETAIN was tested on a large health system EHR dataset with 14 million visits completed by 263K patients over an 8 year period and demonstrated predictive accuracy and computational scalability comparable to state-of-the-art methods such as RNN, and ease of interpretability comparable to traditional models.},
	urldate = {2022-07-19},
	publisher = {arXiv},
	author = {Choi, Edward and Bahadori, Mohammad Taha and Kulas, Joshua A. and Schuetz, Andy and Stewart, Walter F. and Sun, Jimeng},
	month = feb,
	year = {2017},
	note = {arXiv:1608.05745 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Computer Science - Artificial Intelligence, read},
	file = {arXiv.org Snapshot:/Users/elisedumas/Zotero/storage/N6MMGY79/1608.html:text/html;Choi et al. - 2017 - RETAIN An Interpretable Predictive Model for Heal.pdf:/Users/elisedumas/Zotero/storage/4JEYFFFC/Choi et al. - 2017 - RETAIN An Interpretable Predictive Model for Heal.pdf:application/pdf},
}

@inproceedings{ma_dipole_2017,
	title = {Dipole: {Diagnosis} {Prediction} in {Healthcare} via {Attention}-based {Bidirectional} {Recurrent} {Neural} {Networks}},
	shorttitle = {Dipole},
	url = {http://arxiv.org/abs/1706.05764},
	doi = {10.1145/3097983.3098088},
	abstract = {Predicting the future health information of patients from the historical Electronic Health Records (EHR) is a core research task in the development of personalized healthcare. Patient EHR data consist of sequences of visits over time, where each visit contains multiple medical codes, including diagnosis, medication, and procedure codes. The most important challenges for this task are to model the temporality and high dimensionality of sequential EHR data and to interpret the prediction results. Existing work solves this problem by employing recurrent neural networks (RNNs) to model EHR data and utilizing simple attention mechanism to interpret the results. However, RNN-based approaches suffer from the problem that the performance of RNNs drops when the length of sequences is large, and the relationships between subsequent visits are ignored by current RNN-based approaches. To address these issues, we propose \{{\textbackslash}sf Dipole\}, an end-to-end, simple and robust model for predicting patients' future health information. Dipole employs bidirectional recurrent neural networks to remember all the information of both the past visits and the future visits, and it introduces three attention mechanisms to measure the relationships of different visits for the prediction. With the attention mechanisms, Dipole can interpret the prediction results effectively. Dipole also allows us to interpret the learned medical code representations which are confirmed positively by medical experts. Experimental results on two real world EHR datasets show that the proposed Dipole can significantly improve the prediction accuracy compared with the state-of-the-art diagnosis prediction approaches and provide clinically meaningful interpretation.},
	urldate = {2022-07-19},
	booktitle = {Proceedings of the 23rd {ACM} {SIGKDD} {International} {Conference} on {Knowledge} {Discovery} and {Data} {Mining}},
	author = {Ma, Fenglong and Chitta, Radha and Zhou, Jing and You, Quanzeng and Sun, Tong and Gao, Jing},
	month = aug,
	year = {2017},
	note = {arXiv:1706.05764 [cs]},
	keywords = {Computer Science - Machine Learning},
	pages = {1903--1911},
	file = {arXiv.org Snapshot:/Users/elisedumas/Zotero/storage/TEB3CM5R/1706.html:text/html;Ma et al. - 2017 - Dipole Diagnosis Prediction in Healthcare via Att.pdf:/Users/elisedumas/Zotero/storage/IFMVJT6H/Ma et al. - 2017 - Dipole Diagnosis Prediction in Healthcare via Att.pdf:application/pdf},
}

@unpublished{martinsson_model_2017,
	title = {A model for sequential prediction of time-to-event in the case of discrete or continuous censored data, recurrent events or time-varying covariates},
	abstract = {In this thesis we propose a new model for predicting time to events: the Weibull Time To Event RNN. This is a simple framework for time-series prediction of the time to the next event applicable when we have any or all of the problems of continuous or discrete time, right censoring, recurrent events, temporal patterns, time varying covariates or time series of varying lengths. All these problems are frequently encountered in customer churn, remaining useful life, failure, spike-train and event prediction.},
	language = {en},
	author = {Martinsson, Egil},
	year = {2017},
	file = {Martinsson - A model for sequential prediction of time-to-event.pdf:/Users/elisedumas/Zotero/storage/MAX67SD6/Martinsson - A model for sequential prediction of time-to-event.pdf:application/pdf},
}

@inproceedings{baytas_patient_2017,
	address = {New York, NY, USA},
	series = {{KDD} '17},
	title = {Patient {Subtyping} via {Time}-{Aware} {LSTM} {Networks}},
	isbn = {978-1-4503-4887-4},
	url = {https://doi.org/10.1145/3097983.3097997},
	doi = {10.1145/3097983.3097997},
	abstract = {In the study of various diseases, heterogeneity among patients usually leads to different progression patterns and may require different types of therapeutic intervention. Therefore, it is important to study patient subtyping, which is grouping of patients into disease characterizing subtypes. Subtyping from complex patient data is challenging because of the information heterogeneity and temporal dynamics. Long-Short Term Memory (LSTM) has been successfully used in many domains for processing sequential data, and recently applied for analyzing longitudinal patient records. The LSTM units are designed to handle data with constant elapsed times between consecutive elements of a sequence. Given that time lapse between successive elements in patient records can vary from days to months, the design of traditional LSTM may lead to suboptimal performance. In this paper, we propose a novel LSTM unit called Time-Aware LSTM (T-LSTM) to handle irregular time intervals in longitudinal patient records. We learn a subspace decomposition of the cell memory which enables time decay to discount the memory content according to the elapsed time. We propose a patient subtyping model that leverages the proposed T-LSTM in an auto-encoder to learn a powerful single representation for sequential records of patients, which are then used to cluster patients into clinical subtypes. Experiments on synthetic and real world datasets show that the proposed T-LSTM architecture captures the underlying structures in the sequences with time irregularities.},
	urldate = {2022-07-19},
	booktitle = {Proceedings of the 23rd {ACM} {SIGKDD} {International} {Conference} on {Knowledge} {Discovery} and {Data} {Mining}},
	publisher = {Association for Computing Machinery},
	author = {Baytas, Inci M. and Xiao, Cao and Zhang, Xi and Wang, Fei and Jain, Anil K. and Zhou, Jiayu},
	month = aug,
	year = {2017},
	keywords = {read, long-short term memory, patient subtyping, recurrent neural network},
	pages = {65--74},
	file = {Full Text PDF:/Users/elisedumas/Zotero/storage/FNVZ3MXZ/Baytas et al. - 2017 - Patient Subtyping via Time-Aware LSTM Networks.pdf:application/pdf},
}

@inproceedings{bai_interpretable_2018,
	address = {New York, NY, USA},
	series = {{KDD} '18},
	title = {Interpretable {Representation} {Learning} for {Healthcare} via {Capturing} {Disease} {Progression} through {Time}},
	isbn = {978-1-4503-5552-0},
	url = {https://doi.org/10.1145/3219819.3219904},
	doi = {10.1145/3219819.3219904},
	abstract = {Various deep learning models have recently been applied to predictive modeling of Electronic Health Records (EHR). In medical claims data, which is a particular type of EHR data, each patient is represented as a sequence of temporally ordered irregularly sampled visits to health providers, where each visit is recorded as an unordered set of medical codes specifying patient's diagnosis and treatment provided during the visit. Based on the observation that different patient conditions have different temporal progression patterns, in this paper we propose a novel interpretable deep learning model, called Timeline. The main novelty of Timeline is that it has a mechanism that learns time decay factors for every medical code. This allows the Timeline to learn that chronic conditions have a longer lasting impact on future visits than acute conditions. Timeline also has an attention mechanism that improves vector embeddings of visits. By analyzing the attention weights and disease progression functions of Timeline, it is possible to interpret the predictions and understand how risks of future visits change over time. We evaluated Timeline on two large-scale real world data sets. The specific task was to predict what is the primary diagnosis category for the next hospital visit given previous visits. Our results show that Timeline has higher accuracy than the state of the art deep learning models based on RNN. In addition, we demonstrate that time decay factors and attentions learned by Timeline are in accord with the medical knowledge and that Timeline can provide a useful insight into its predictions.},
	urldate = {2022-07-19},
	booktitle = {Proceedings of the 24th {ACM} {SIGKDD} {International} {Conference} on {Knowledge} {Discovery} \& {Data} {Mining}},
	publisher = {Association for Computing Machinery},
	author = {Bai, Tian and Zhang, Shanshan and Egleston, Brian L. and Vucetic, Slobodan},
	month = jul,
	year = {2018},
	keywords = {deep learning, electronic health records, healthcare, read, attention model},
	pages = {43--51},
	file = {Bai et al. - 2018 - Interpretable Representation Learning for Healthca.pdf:/Users/elisedumas/Zotero/storage/6CAGUIMY/Bai et al. - 2018 - Interpretable Representation Learning for Healthca.pdf:application/pdf},
}

@inproceedings{ye_lsan_2020,
	address = {New York, NY, USA},
	series = {{CIKM} '20},
	title = {{LSAN}: {Modeling} {Long}-term {Dependencies} and {Short}-term {Correlations} with {Hierarchical} {Attention} for {Risk} {Prediction}},
	isbn = {978-1-4503-6859-9},
	shorttitle = {{LSAN}},
	url = {https://doi.org/10.1145/3340531.3411864},
	doi = {10.1145/3340531.3411864},
	abstract = {Risk prediction using electronic health records (EHR) is a challenging data mining task due to the two-level hierarchical structure of EHR data. EHR data consist of a set of time-ordered visits, and within each visit, there is a set of unordered diagnosis codes. Existing approaches focus on modeling temporal visits with deep neural network (DNN) techniques. However, they ignore the importance of modeling diagnosis codes within visits, and a lot of task-unrelated information within visits usually leads to unsatisfactory performance of existing approaches. To minimize the effect caused by noise information of EHR data, in this paper, we propose a novel DNN for risk prediction termed as LSAN, which consists of a Hierarchical Attention Module (HAM) and a Temporal Aggregation Module (TAM). Particularly, LSAN applies HAM to model the hierarchical structure of EHR data. Using the attention mechanism in the hierarchy of diagnosis code, HAM is able to retain diagnosis details and assign flexible attention weights to different diagnosis codes by their relevance to corresponding diseases. Moreover, the attention mechanism in the hierarchy of visit learns a comprehensive feature throughout the visit history by paying greater attention to visits with higher relevance. Based on the foundation laying by HAM, TAM uses a two-pathway structure to learn a robust temporal aggregation mechanism among all visits for LSAN. It extracts long-term dependencies by a Transformer encoder and short-term correlations by a parallel convolutional layer among different visits. With the construction of HAM and TAM, LSAN achieves the state-of-the-art performance on three real-world datasets with larger AUCs, recalls and F1 scores. Furthermore, the model analysis results demonstrate the effectiveness of the network construction with good interpretability and robustness of decision making by LSAN.},
	urldate = {2022-07-19},
	booktitle = {Proceedings of the 29th {ACM} {International} {Conference} on {Information} \& {Knowledge} {Management}},
	publisher = {Association for Computing Machinery},
	author = {Ye, Muchao and Luo, Junyu and Xiao, Cao and Ma, Fenglong},
	month = oct,
	year = {2020},
	keywords = {data mining, electronic health records, read, attention mechanism, temporal modeling},
	pages = {1753--1762},
	file = {Ye et al. - 2020 - LSAN Modeling Long-term Dependencies and Short-te.pdf:/Users/elisedumas/Zotero/storage/UDNUVWG4/Ye et al. - 2020 - LSAN Modeling Long-term Dependencies and Short-te.pdf:application/pdf},
}

@misc{lipton_learning_2017,
	title = {Learning to {Diagnose} with {LSTM} {Recurrent} {Neural} {Networks}},
	url = {http://arxiv.org/abs/1511.03677},
	doi = {10.48550/arXiv.1511.03677},
	abstract = {Clinical medical data, especially in the intensive care unit (ICU), consist of multivariate time series of observations. For each patient visit (or episode), sensor data and lab test results are recorded in the patient's Electronic Health Record (EHR). While potentially containing a wealth of insights, the data is difficult to mine effectively, owing to varying length, irregular sampling and missing data. Recurrent Neural Networks (RNNs), particularly those using Long Short-Term Memory (LSTM) hidden units, are powerful and increasingly popular models for learning from sequence data. They effectively model varying length sequences and capture long range dependencies. We present the first study to empirically evaluate the ability of LSTMs to recognize patterns in multivariate time series of clinical measurements. Specifically, we consider multilabel classification of diagnoses, training a model to classify 128 diagnoses given 13 frequently but irregularly sampled clinical measurements. First, we establish the effectiveness of a simple LSTM network for modeling clinical data. Then we demonstrate a straightforward and effective training strategy in which we replicate targets at each sequence step. Trained only on raw time series, our models outperform several strong baselines, including a multilayer perceptron trained on hand-engineered features.},
	urldate = {2022-07-19},
	publisher = {arXiv},
	author = {Lipton, Zachary C. and Kale, David C. and Elkan, Charles and Wetzel, Randall},
	month = mar,
	year = {2017},
	note = {arXiv:1511.03677 [cs]},
	keywords = {Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/elisedumas/Zotero/storage/9S2L2IWC/Lipton et al. - 2017 - Learning to Diagnose with LSTM Recurrent Neural Ne.pdf:application/pdf;arXiv.org Snapshot:/Users/elisedumas/Zotero/storage/HWEQM7C9/1511.html:text/html},
}

@inproceedings{pham_deepcare_2016,
	address = {Cham},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {{DeepCare}: {A} {Deep} {Dynamic} {Memory} {Model} for {Predictive} {Medicine}},
	isbn = {978-3-319-31750-2},
	shorttitle = {{DeepCare}},
	doi = {10.1007/978-3-319-31750-2_3},
	abstract = {Personalized predictive medicine necessitates modeling of patient illness and care processes, which inherently have long-term temporal dependencies. Healthcare observations, recorded in electronic medical records, are episodic and irregular in time. We introduce DeepCare, a deep dynamic neural network that reads medical records and predicts future medical outcomes. At the data level, DeepCare models patient health state trajectories with explicit memory of illness. Built on Long Short-Term Memory (LSTM), DeepCare introduces time parameterizations to handle irregular timing by moderating the forgetting and consolidation of illness memory. DeepCare also incorporates medical interventions that change the course of illness and shape future medical risk. Moving up to the health state level, historical and present health states are then aggregated through multiscale temporal pooling, before passing through a neural network that estimates future outcomes. We demonstrate the efficacy of DeepCare for disease progression modeling and readmission prediction in diabetes, a chronic disease with large economic burden. The results show improved modeling and risk prediction accuracy.},
	language = {en},
	booktitle = {Advances in {Knowledge} {Discovery} and {Data} {Mining}},
	publisher = {Springer International Publishing},
	author = {Pham, Trang and Tran, Truyen and Phung, Dinh and Venkatesh, Svetha},
	editor = {Bailey, James and Khan, Latifur and Washio, Takashi and Dobbie, Gill and Huang, Joshua Zhexue and Wang, Ruili},
	year = {2016},
	keywords = {Dee PC, Disease Progression Model, Long Short-term Memory (LSTM), Memory Inference, Risk Prediction Accuracy},
	pages = {30--41},
	file = {Pham et al. - 2016 - DeepCare A Deep Dynamic Memory Model for Predicti.pdf:/Users/elisedumas/Zotero/storage/PNB5LUIQ/Pham et al. - 2016 - DeepCare A Deep Dynamic Memory Model for Predicti.pdf:application/pdf},
}

@article{alaa_deep_2017,
	title = {Deep {Multi}-task {Gaussian} {Processes} for {Survival} {Analysis} with {Competing} {Risks}},
	abstract = {Designing optimal treatment plans for patients with comorbidities requires accurate cause-speciﬁc mortality prognosis. Motivated by the recent availability of linked electronic health records, we develop a nonparametric Bayesian model for survival analysis with competing risks, which can be used for jointly assessing a patient’s risk of multiple (competing) adverse outcomes. The model views a patient’s survival times with respect to the competing risks as the outputs of a deep multi-task Gaussian process (DMGP), the inputs to which are the patients’ covariates. Unlike parametric survival analysis methods based on Cox and Weibull models, our model uses DMGPs to capture complex non-linear interactions between the patients’ covariates and cause-speciﬁc survival times, thereby learning ﬂexible patient-speciﬁc and cause-speciﬁc survival curves, all in a data-driven fashion without explicit parametric assumptions on the hazard rates. We propose a variational inference algorithm that is capable of learning the model parameters from time-to-event data while handling right censoring. Experiments on synthetic and real data show that our model outperforms the state-of-the-art survival models.},
	language = {en},
	author = {Alaa, Ahmed M},
	year = {2017},
	pages = {9},
	file = {Alaa - Deep Multi-task Gaussian Processes for Survival An.pdf:/Users/elisedumas/Zotero/storage/7I6S7Z34/Alaa - Deep Multi-task Gaussian Processes for Survival An.pdf:application/pdf},
}

@misc{kabeshova_zimm_2020,
	title = {{ZiMM}: a deep learning model for long term and blurry relapses with non-clinical claims data},
	shorttitle = {{ZiMM}},
	url = {http://arxiv.org/abs/1911.05346},
	doi = {10.48550/arXiv.1911.05346},
	abstract = {This paper considers the problems of modeling and predicting a long-term and ``blurry'' relapse that occurs after a medical act, such as a surgery. The relapse is observed only indirectly, in a ``blurry'' fashion, through longitudinal prescriptions of drugs over a long period of time after the medical act. We introduce a new model, called ZiMM (Zero-inflated Mixture of Multinomial distributions) in order to capture long-term and blurry relapses. On top of it, we build an end-to-end deep-learning architecture called ZiMM Encoder-Decoder (ZiMM ED) that can learn from the complex, irregular, highly heterogeneous and sparse patterns of health events that are observed through a claims-only database. ZiMM ED is applied on a ``non-clinical'' claims database, that contains only timestamped reimbursement codes for drug purchases, medical procedures and hospital diagnoses, the only available clinical feature being the age of the patient. This setting is more challenging than a setting where bedside clinical signals are available. Our motivation for using such a non-clinical claims database is its exhaustivity population-wise, compared to clinical electronic health records coming from a single or a small set of hospitals. Indeed, we consider a dataset containing the claims of almost {\textbackslash}emph\{all French citizens\} who had surgery for prostatic problems, with a history between 1.5 and 5 years. We consider a long-term (18 months) relapse (urination problems still occur despite surgery), which is blurry since it is observed only through the reimbursement of a specific set of drugs for urination problems. Our experiments show that ZiMM ED improves several baselines, including non-deep learning and deep-learning approaches, and that it allows working on such a dataset with minimal preprocessing work.},
	urldate = {2022-07-20},
	publisher = {arXiv},
	author = {Kabeshova, Anastasiia and Yu, Yiyang and Lukacs, Bertrand and Bacry, Emmanuel and Gaïffas, Stéphane},
	month = jul,
	year = {2020},
	note = {arXiv:1911.05346 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, read},
	file = {arXiv Fulltext PDF:/Users/elisedumas/Zotero/storage/GH8L8JBE/Kabeshova et al. - 2020 - ZiMM a deep learning model for long term and blur.pdf:application/pdf;arXiv.org Snapshot:/Users/elisedumas/Zotero/storage/GMVS2LWK/1911.html:text/html},
}

@misc{choi_doctor_2016,
	title = {Doctor {AI}: {Predicting} {Clinical} {Events} via {Recurrent} {Neural} {Networks}},
	shorttitle = {Doctor {AI}},
	url = {http://arxiv.org/abs/1511.05942},
	doi = {10.48550/arXiv.1511.05942},
	abstract = {Leveraging large historical data in electronic health record (EHR), we developed Doctor AI, a generic predictive model that covers observed medical conditions and medication uses. Doctor AI is a temporal model using recurrent neural networks (RNN) and was developed and applied to longitudinal time stamped EHR data from 260K patients over 8 years. Encounter records (e.g. diagnosis codes, medication codes or procedure codes) were input to RNN to predict (all) the diagnosis and medication categories for a subsequent visit. Doctor AI assesses the history of patients to make multilabel predictions (one label for each diagnosis or medication category). Based on separate blind test set evaluation, Doctor AI can perform differential diagnosis with up to 79\% recall@30, significantly higher than several baselines. Moreover, we demonstrate great generalizability of Doctor AI by adapting the resulting models from one institution to another without losing substantial accuracy.},
	urldate = {2022-07-21},
	publisher = {arXiv},
	author = {Choi, Edward and Bahadori, Mohammad Taha and Schuetz, Andy and Stewart, Walter F. and Sun, Jimeng},
	month = sep,
	year = {2016},
	note = {arXiv:1511.05942 [cs]},
	keywords = {Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/elisedumas/Zotero/storage/GWRZXKWM/Choi et al. - 2016 - Doctor AI Predicting Clinical Events via Recurren.pdf:application/pdf;arXiv.org Snapshot:/Users/elisedumas/Zotero/storage/QEFHHRQA/1511.html:text/html},
}

@article{xiao_opportunities_2018,
	title = {Opportunities and challenges in developing deep learning models using electronic health records data: a systematic review},
	volume = {25},
	issn = {1527-974X},
	shorttitle = {Opportunities and challenges in developing deep learning models using electronic health records data},
	doi = {10.1093/jamia/ocy068},
	abstract = {Objective: To conduct a systematic review of deep learning models for electronic health record (EHR) data, and illustrate various deep learning architectures for analyzing different data sources and their target applications. We also highlight ongoing research and identify open challenges in building deep learning models of EHRs.
Design/method: We searched PubMed and Google Scholar for papers on deep learning studies using EHR data published between January 1, 2010, and January 31, 2018. We summarize them according to these axes: types of analytics tasks, types of deep learning model architectures, special challenges arising from health data and tasks and their potential solutions, as well as evaluation strategies.
Results: We surveyed and analyzed multiple aspects of the 98 articles we found and identified the following analytics tasks: disease detection/classification, sequential prediction of clinical events, concept embedding, data augmentation, and EHR data privacy. We then studied how deep architectures were applied to these tasks. We also discussed some special challenges arising from modeling EHR data and reviewed a few popular approaches. Finally, we summarized how performance evaluations were conducted for each task.
Discussion: Despite the early success in using deep learning for health analytics applications, there still exist a number of issues to be addressed. We discuss them in detail including data and label availability, the interpretability and transparency of the model, and ease of deployment.},
	language = {eng},
	number = {10},
	journal = {Journal of the American Medical Informatics Association: JAMIA},
	author = {Xiao, Cao and Choi, Edward and Sun, Jimeng},
	month = oct,
	year = {2018},
	pmid = {29893864},
	pmcid = {PMC6188527},
	keywords = {Humans, Prognosis, Electronic Health Records, Neural Networks, Computer, Disease, read, Data Anonymization, Deep Learning},
	pages = {1419--1428},
	file = {Xiao et al. - 2018 - Opportunities and challenges in developing deep le.pdf:/Users/elisedumas/Zotero/storage/ZCZQPY44/Xiao et al. - 2018 - Opportunities and challenges in developing deep le.pdf:application/pdf},
}

@article{jagannatha_bidirectional_2016,
	title = {Bidirectional {RNN} for {Medical} {Event} {Detection} in {Electronic} {Health} {Records}},
	volume = {2016},
	doi = {10.18653/v1/n16-1056},
	abstract = {Sequence labeling for extraction of medical events and their attributes from unstructured text in Electronic Health Record (EHR) notes is a key step towards semantic understanding of EHRs. It has important applications in health informatics including pharmacovigilance and drug surveillance. The state of the art supervised machine learning models in this domain are based on Conditional Random Fields (CRFs) with features calculated from fixed context windows. In this application, we explored recurrent neural network frameworks and show that they significantly out-performed the CRF models.},
	language = {eng},
	journal = {Proceedings of the Conference. Association for Computational Linguistics. North American Chapter. Meeting},
	author = {Jagannatha, Abhyuday N. and Yu, Hong},
	month = jun,
	year = {2016},
	pmid = {27885364},
	pmcid = {PMC5119627},
	pages = {473--482},
	file = {Jagannatha and Yu - 2016 - Bidirectional RNN for Medical Event Detection in E.pdf:/Users/elisedumas/Zotero/storage/F5XBF9CN/Jagannatha and Yu - 2016 - Bidirectional RNN for Medical Event Detection in E.pdf:application/pdf},
}

@article{rasmussen_validated_2021,
	title = {A {Validated} {Register}-{Based} {Algorithm} to {Identify} {Patients} {Diagnosed} with {Recurrence} of {Malignant} {Melanoma} in {Denmark}},
	volume = {13},
	issn = {1179-1349},
	doi = {10.2147/CLEP.S295844},
	abstract = {PURPOSE: Information on cancer recurrence is rarely available outside clinical trials. Wide exclusion criteria used in clinical trials tend to limit the generalizability of findings to the entire population of people living beyond a cancer disease. Therefore, population-level evidence is needed. The aim of this study was to develop and validate a register-based algorithm to identify patients diagnosed with recurrence after curative treatment of malignant melanoma.
PATIENTS AND METHODS: Indicators of recurrence were diagnosis and procedure codes recorded in the Danish National Patient Register and pathology results recorded in the Danish National Pathology Register. Medical records on recurrence status and recurrence date in the Danish Melanoma Database served as the gold standard to assess the accuracy of the algorithm.
RESULTS: The study included 1747 patients diagnosed with malignant melanoma; 95 (5.4\%) were diagnosed with recurrence of malignant melanoma according to the gold standard. The algorithm reached a sensitivity of 93.7\% (95\% confidence interval (CI) 86.8-97.6), a specificity of 99.2\% (95\% CI: 98.6-99.5), a positive predictive value of 86.4\% (95\% CI: 78.2-92.4), and negative predictive value of 99.6\% (95\% CI: 99.2-99.9). Lin's concordance correlation coefficient was 0.992 (95\% CI: 0.989-0.996) for the agreement between the recurrence dates generated by the algorithm and by the gold standard.
CONCLUSION: The algorithm can be used to identify patients diagnosed with recurrence of malignant melanoma and to establish the timing of recurrence. This can generate population-level evidence on disease-free survival and diagnostic pathways for recurrence of malignant melanoma.},
	language = {eng},
	journal = {Clinical Epidemiology},
	author = {Rasmussen, Linda Aagaard and Jensen, Henry and Virgilsen, Line Flytkjaer and Hölmich, Lisbet Rosenkrantz and Vedsted, Peter},
	year = {2021},
	pmid = {33758549},
	pmcid = {PMC7979354},
	keywords = {Denmark, recurrence, algorithms, validation study, melanoma, registries},
	pages = {207--214},
	file = {Full Text:/Users/elisedumas/Zotero/storage/VUBXT3XH/Rasmussen et al. - 2021 - A Validated Register-Based Algorithm to Identify P.pdf:application/pdf},
}

@article{izci_systematic_2020,
	title = {A {Systematic} {Review} of {Estimating} {Breast} {Cancer} {Recurrence} at the {Population} {Level} {With} {Administrative} {Data}},
	volume = {112},
	issn = {1460-2105},
	doi = {10.1093/jnci/djaa050},
	abstract = {BACKGROUND: Exact numbers of breast cancer recurrences are currently unknown at the population level, because they are challenging to actively collect. Previously, real-world data such as administrative claims have been used within expert- or data-driven (machine learning) algorithms for estimating cancer recurrence. We present the first systematic review and meta-analysis, to our knowledge, of publications estimating breast cancer recurrence at the population level using algorithms based on administrative data.
METHODS: The systematic literature search followed Preferred Reporting Items for Systematic Reviews and Meta-Analysis guidelines. We evaluated and compared sensitivity, specificity, positive predictive value, negative predictive value, and overall accuracy of algorithms. A random-effects meta-analysis was performed using a generalized linear mixed model to obtain a pooled estimate of accuracy.
RESULTS: Seventeen articles met the inclusion criteria. Most articles used information from medical files as the gold standard, defined as any recurrence. Two studies included bone metastases only in the definition of recurrence. Fewer studies used a model-based approach (decision trees or logistic regression) (41.2\%) compared with studies using detection rules without specified model (58.8\%). The generalized linear mixed model for all recurrence types reported an accuracy of 92.2\% (95\% confidence interval = 88.4\% to 94.8\%).
CONCLUSIONS: Publications reporting algorithms for detecting breast cancer recurrence are limited in number and heterogeneous. A thorough analysis of the existing algorithms demonstrated the need for more standardization and validation. The meta-analysis reported a high accuracy overall, which indicates algorithms as promising tools to identify breast cancer recurrence at the population level. The rule-based approach combined with emerging machine learning algorithms could be interesting to explore in the future.},
	language = {eng},
	number = {10},
	journal = {Journal of the National Cancer Institute},
	author = {Izci, Hava and Tambuyzer, Tim and Tuand, Krizia and Depoorter, Victoria and Laenen, Annouschka and Wildiers, Hans and Vergote, Ignace and Van Eycken, Liesbet and De Schutter, Harlinde and Verdoodt, Freija and Neven, Patrick},
	month = oct,
	year = {2020},
	pmid = {32259259},
	pmcid = {PMC7566328},
	keywords = {Female, Humans, Algorithms, Breast Neoplasms, Neoplasm Recurrence, Local, Publications},
	pages = {979--988},
	file = {Full Text:/Users/elisedumas/Zotero/storage/5FS2G8TG/Izci et al. - 2020 - A Systematic Review of Estimating Breast Cancer Re.pdf:application/pdf},
}

@article{nordstrom_identification_2012,
	title = {Identification of metastatic cancer in claims data},
	volume = {21 Suppl 2},
	issn = {1099-1557},
	doi = {10.1002/pds.3247},
	abstract = {PURPOSE: To develop algorithms to identify metastatic cancer in claims data, using tumor stage from an oncology electronic medical record (EMR) data warehouse as the gold standard.
METHODS: Data from an outpatient oncology EMR database were linked to medical and pharmacy claims data. Patients diagnosed with breast, lung, colorectal, or prostate cancer with a stage recorded in the EMR between 2004 and 2010 and with medical claims available were eligible for the study. Separate algorithms were developed for each tumor type using variables from the claims, including diagnoses, procedures, drugs, and oncologist visits. Candidate variables were reviewed by two oncologists. For each tumor type, the selected variables were entered into a classification and regression tree model to determine the algorithm with the best combination of positive predictive value (PPV), sensitivity, and specificity.
RESULTS: A total of 1385 breast cancer, 1036 lung, 727 colorectal, and 267 prostate cancer patients qualified for the analysis. The algorithms varied by tumor type but typically included International Classification of Diseases-Ninth Revision codes for secondary neoplasms and use of chemotherapy and other agents typically given for metastatic disease. The final models had PPV ranging from 0.75 to 0.86, specificity 0.75-0.97, and sensitivity 0.60-0.81.
CONCLUSIONS: While most of these algorithms for metastatic cancer had good specificity and acceptable PPV, a tradeoff with sensitivity prevented any model from having good predictive ability on all measures. Results suggest that accurate ascertainment of metastatic status may require access to medical records or other confirmatory data sources.},
	language = {eng},
	journal = {Pharmacoepidemiology and Drug Safety},
	author = {Nordstrom, Beth L. and Whyte, Joanna L. and Stolar, Marilyn and Mercaldi, Catherine and Kallich, Joel D.},
	month = may,
	year = {2012},
	pmid = {22552976},
	keywords = {Adolescent, Adult, Female, Humans, Male, Databases, Factual, Algorithms, International Classification of Diseases, Aged, Antineoplastic Agents, Middle Aged, Neoplasm Staging, Young Adult, Electronic Health Records, Neoplasms, Second Primary, Insurance Claim Review, Sensitivity and Specificity},
	pages = {21--28},
	file = {Full Text:/Users/elisedumas/Zotero/storage/FSHFTDK3/Nordstrom et al. - 2012 - Identification of metastatic cancer in claims data.pdf:application/pdf},
}

@article{sathiakumar_accuracy_2017,
	title = {Accuracy of {Medicare} {Claim}-based {Algorithm} to {Detect} {Breast}, {Prostate}, or {Lung} {Cancer} {Bone} {Metastases}},
	volume = {55},
	issn = {1537-1948},
	doi = {10.1097/MLR.0000000000000539},
	abstract = {BACKGROUND: We had previously developed an algorithm for Medicare claims data to detect bone metastases associated with breast, prostate, or lung cancer. This study was conducted to examine whether this algorithm accurately documents bone metastases on the basis of diagnosis codes in Medicare claims data.
METHODS: We obtained data from Medicare claims and electronic medical records of patients 65 years or older with a breast, prostate, or lung cancer diagnosis at a teaching hospital and/or affiliated clinics during 2005 or 2006. We calculated the sensitivity and positive predictive value (PPV) of our algorithm using medical records as the "gold standard." The κ statistic was used to measure agreement between claims and medical record data.
RESULTS: The agreement between claims and medical record data for bone metastases among breast, prostate, and lung cancer patients was 0.93, 0.90, and 0.69, respectively. The sensitivities of our algorithm for bone metastasis in patients with breast, prostate, and lung were 96.8\% [95\% confidence interval (CI)=83.8\% to 99.4\%], 91.7\% (95\% CI=78.2\% to 97.1\%), and 74.1\% (95\% CI=55.3\% to 86.8\%), respectively; and the PPVs were 90.9\% (95\% CI=76.4\% to 96.9\%), 91.7\% (95\% CI=78.2\% to 97.1\%), and 71.4\% (95\% CI=52.9\% to 84.8\%), respectively.
CONCLUSIONS: The algorithm for detecting bone metastases in claims data had high sensitivity and PPV for breast and prostate cancer patients. Sensitivity and PPV were lower but still moderate for lung cancer patients.},
	language = {eng},
	number = {12},
	journal = {Medical Care},
	author = {Sathiakumar, Nalini and Delzell, Elizabeth and Yun, Huifeng and Jooste, Rene and Godby, Kelly and Falkson, Carla and Yong, Mellissa and Kilgore, Meredith L.},
	month = dec,
	year = {2017},
	pmid = {29135778},
	keywords = {Female, Humans, Male, United States, Algorithms, Breast Neoplasms, Lung Neoplasms, Aged, Aged, 80 and over, Prostatic Neoplasms, Medicare, Insurance Claim Review, Early Detection of Cancer},
	pages = {e144--e149},
	file = {Sathiakumar et al. - 2017 - Accuracy of Medicare Claim-based Algorithm to Dete.pdf:/Users/elisedumas/Zotero/storage/ME55FUZH/Sathiakumar et al. - 2017 - Accuracy of Medicare Claim-based Algorithm to Dete.pdf:application/pdf},
}

@article{whyte_evaluation_2015,
	title = {An {Evaluation} of {Algorithms} for {Identifying} {Metastatic} {Breast}, {Lung}, or {Colorectal} {Cancer} in {Administrative} {Claims} {Data}},
	volume = {53},
	issn = {1537-1948},
	doi = {10.1097/MLR.0b013e318289c3fb},
	abstract = {BACKGROUND: Administrative health care claims data are used for epidemiologic, health services, and outcomes cancer research and thus play a significant role in policy. Cancer stage, which is often a major driver of cost and clinical outcomes, is not typically included in claims data.
OBJECTIVES: Evaluate algorithms used in a dataset of cancer patients to identify patients with metastatic breast (BC), lung (LC), or colorectal (CRC) cancer using claims data.
METHODS: Clinical data on BC, LC, or CRC patients (between January 1, 2007 and March 31, 2010) were linked to a health care claims database. Inclusion required health plan enrollment ≥3 months before initial cancer diagnosis date. Algorithms were used in the claims database to identify patients' disease status, which was compared with physician-reported metastases. Generic and tumor-specific algorithms were evaluated using ICD-9 codes, varying diagnosis time frames, and including/excluding other tumors. Positive and negative predictive values, sensitivity, and specificity were assessed.
RESULTS: The linked databases included 14,480 patients; of whom, 32\%, 17\%, and 14.2\% had metastatic BC, LC, and CRC, respectively, at diagnosis and met inclusion criteria. Nontumor-specific algorithms had lower specificity than tumor-specific algorithms. Tumor-specific algorithms' sensitivity and specificity were 53\% and 99\% for BC, 55\% and 85\% for LC, and 59\% and 98\% for CRC, respectively.
CONCLUSIONS: Algorithms to distinguish metastatic BC, LC, and CRC from locally advanced disease should use tumor-specific primary cancer codes with 2 claims for the specific primary cancer {\textgreater}30-42 days apart to reduce misclassification. These performed best overall in specificity, positive predictive values, and overall accuracy to identify metastatic cancer in a health care claims database.},
	language = {eng},
	number = {7},
	journal = {Medical Care},
	author = {Whyte, Joanna L. and Engel-Nitz, Nicole M. and Teitelbaum, April and Gomez Rey, Gabriel and Kallich, Joel D.},
	month = jul,
	year = {2015},
	pmid = {23524464},
	keywords = {Female, Humans, Male, United States, Algorithms, Breast Neoplasms, International Classification of Diseases, Lung Neoplasms, Aged, Middle Aged, Neoplasm Metastasis, Retrospective Studies, Insurance Claim Review, Sensitivity and Specificity, Colorectal Neoplasms},
	pages = {e49--57},
	file = {Whyte et al. - 2015 - An Evaluation of Algorithms for Identifying Metast.pdf:/Users/elisedumas/Zotero/storage/FB339DYE/Whyte et al. - 2015 - An Evaluation of Algorithms for Identifying Metast.pdf:application/pdf},
}

@article{mcclish_using_2003,
	title = {Using {Medicare} claims to identify second primary cancers and recurrences in order to supplement a cancer registry},
	volume = {56},
	issn = {0895-4356},
	doi = {10.1016/s0895-4356(03)00091-x},
	abstract = {BACKGROUND AND OBJECTIVE: The purpose of this study was to use Medicare claims to develop models to assist cancer registries in identifying cancer patients with second primaries or recurrences (an "event").
METHODS: Medicare inpatient and Part B data were merged with a cancer registry for patients first diagnosed in 1993-1994. Logistic regression was used to model the occurrence of an event at least 1 year after initial diagnosis, and to identify factors that could discriminate between recurrences and second primaries.
RESULTS: Predictors of an event included an inpatient cancer diagnosis, cancer diagnosis different from the initial diagnosis from any source, and radiation or surgery claims in Part B. The ROC curve area was 0.90 with all Medicare data; 0.84 when restricted to inpatient data. A cancer diagnosis different from the initial diagnosis or having surgery predicted a second primary; regional or distant stage disease, diagnosis of secondary malignancy, or an inpatient diagnosis of primary cancer in a position other than principal predicted recurrence.
CONCLUSIONS: Although Medicare claims have not been evaluated as a stand-alone system to identify second primaries and recurrences, Medicare claims may be useful tools for Cancer Registries in case ascertainment and follow-up.},
	language = {eng},
	number = {8},
	journal = {Journal of Clinical Epidemiology},
	author = {McClish, Donna and Penberthy, Lynne and Pugh, Amy},
	month = aug,
	year = {2003},
	pmid = {12954468},
	keywords = {Female, Humans, Male, Aged, Aged, 80 and over, Logistic Models, Registries, Neoplasm Recurrence, Local, Time Factors, Medicare, Neoplasms, Second Primary, Insurance Claim Review, ROC Curve},
	pages = {760--767},
	file = {McClish et al. - 2003 - Using Medicare claims to identify second primary c.pdf:/Users/elisedumas/Zotero/storage/Y4Q4TCJP/McClish et al. - 2003 - Using Medicare claims to identify second primary c.pdf:application/pdf},
}

@article{chawla_limited_2014,
	title = {Limited validity of diagnosis codes in {Medicare} claims for identifying cancer metastases and inferring stage},
	volume = {24},
	issn = {1047-2797},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4135357/},
	doi = {10.1016/j.annepidem.2014.06.099},
	abstract = {Purpose
Researchers are using diagnosis codes from health claims to identify metastatic disease in cancer patients. The validity of this approach has not been established.

Methods
We used the linked 2005–2007 SEER-Medicare data to assess the validity of metastasis codes at diagnosis from claims compared with stage reported by SEER cancer registries. The cohort included 80,052 incident breast, lung, and colorectal cancer patients ages 65 and older. Using gold-standard SEER data, we evaluated sensitivity, specificity, positive predictive value (PPV) and negative predictive value (NPV) of claims-based stage, survival by stage-classification, and patient factors associated with stage misclassification using multivariable regression.

Results
For patients with a registry report of distant metastatic cancer, the sensitivity, specificity, and PPV of claims never simultaneously exceeded 80\% for any cancer: lung (42.7\%, 94.8\%, 88.1\%), breast (51.0\%, 98.3\%, 65.8\%), and colorectal (72.8\%, 93.8\%, 68.5\%). Misclassification of stage from Medicare claims was significantly associated with inaccurate estimates of stage-specific survival (p{\textless}0.001). In adjusted analysis, patients who were older, Black, or living in low-income areas were more likely to have their stage misclassified in claims.

Conclusion
Diagnosis codes in Medicare claims have limited validity for inferring cancer stage and metastatic disease.},
	number = {9},
	urldate = {2022-08-03},
	journal = {Annals of epidemiology},
	author = {Chawla, Neetu and Yabroff, K. Robin and Mariotto, Angela and McNeel, Timothy S. and Schrag, Deborah and Warren, Joan L.},
	month = sep,
	year = {2014},
	pmid = {25066409},
	pmcid = {PMC4135357},
	pages = {666--672.e2},
	file = {Accepted Version:/Users/elisedumas/Zotero/storage/9ZP8TAIR/Chawla et al. - 2014 - Limited validity of diagnosis codes in Medicare cl.pdf:application/pdf},
}

@article{hassett_validating_2014,
	title = {Validating billing/encounter codes as indicators of lung, colorectal, breast, and prostate cancer recurrence using 2 large contemporary cohorts},
	volume = {52},
	issn = {1537-1948},
	doi = {10.1097/MLR.0b013e318277eb6f},
	abstract = {BACKGROUND: A substantial proportion of cancer-related mortality is attributable to recurrent, not de novo metastatic disease, yet we know relatively little about these patients. To fill this gap, investigators often use administrative codes for secondary malignant neoplasm or chemotherapy to identify recurrent cases in population-based datasets. However, these algorithms have not been validated in large, contemporary, routine care cohorts.
OBJECTIVE: To evaluate the validity of secondary malignant neoplasm and chemotherapy codes as indicators of recurrence after definitive local therapy for stage I-III lung, colorectal, breast, and prostate cancer.
RESEARCH DESIGN, SUBJECTS, AND MEASURES: We assessed the sensitivity, specificity, and positive predictive value (PPV) of these codes 14 and 60 months after diagnosis using 2 administrative datasets linked with gold-standard recurrence status information: CanCORS/Medicare (diagnoses 2003-2005) and HMO/Cancer Research Network (diagnoses 2000-2005).
RESULTS: We identified 929 CanCORS/Medicare patients and 5298 HMO/CRN patients. Sensitivity, specificity, and PPV ranged widely depending on which codes were included and the type of cancer. For patients with lung, colorectal, and breast cancer, the combination of secondary malignant neoplasm and chemotherapy codes was the most sensitive (75\%-85\%); no code-set was highly sensitive and highly specific. For prostate cancer, no code-set offered even moderate sensitivity (≤ 19\%).
CONCLUSIONS: Secondary malignant neoplasm and chemotherapy codes could not identify recurrent cancer without some risk of misclassification. Findings based on existing algorithms should be interpreted with caution. More work is needed to develop a valid algorithm that can be used to characterize outcomes and define patient cohorts for comparative effectiveness research studies.},
	language = {eng},
	number = {10},
	journal = {Medical Care},
	author = {Hassett, Michael J. and Ritzwoller, Debra P. and Taback, Nathan and Carroll, Nikki and Cronin, Angel M. and Ting, Gladys V. and Schrag, Deb and Warren, Joan L. and Hornbrook, Mark C. and Weeks, Jane C.},
	month = oct,
	year = {2014},
	pmid = {23222531},
	pmcid = {PMC3600389},
	keywords = {Female, Humans, Male, United States, Algorithms, Breast Neoplasms, Lung Neoplasms, Aged, Aged, 80 and over, Middle Aged, Neoplasm Recurrence, Local, Neoplasm Staging, Cohort Studies, Prostatic Neoplasms, Medicare, Reproducibility of Results, Forecasting, Colorectal Neoplasms, Health Status Indicators, Clinical Coding, Health Maintenance Organizations},
	pages = {e65--73},
	file = {Accepted Version:/Users/elisedumas/Zotero/storage/IL3BLMGS/Hassett et al. - 2014 - Validating billingencounter codes as indicators o.pdf:application/pdf},
}

@article{in_cancer_2014,
	title = {Cancer {Recurrence}: {An} {Important} but {Missing} {Variable} in {National} {Cancer} {Registries}},
	volume = {21},
	issn = {1534-4681},
	shorttitle = {Cancer {Recurrence}},
	url = {https://doi.org/10.1245/s10434-014-3516-x},
	doi = {10.1245/s10434-014-3516-x},
	abstract = {Cancer recurrence is a critically important outcome to patients and providers. However, no publicly available cancer registry data contain recurrence information. The National Cancer Data Base (NCDB) collects recurrence data; however, this information is not provided to researchers because of completeness and accuracy concerns. Our objective was to examine completeness of cancer recurrence information in the NCDB.},
	language = {en},
	number = {5},
	urldate = {2022-08-05},
	journal = {Annals of Surgical Oncology},
	author = {In, Haejin and Bilimoria, Karl Y. and Stewart, Andrew K. and Wroblewski, Kristen E. and Posner, Mitchell C. and Talamonti, Mark S. and Winchester, David P.},
	month = may,
	year = {2014},
	keywords = {Cancer Registry, Cancer Site, Central Cancer Registry, Hospital Factor, National Cancer Data Base},
	pages = {1520--1529},
	file = {In et al. - 2014 - Cancer Recurrence An Important but Missing Variab.pdf:/Users/elisedumas/Zotero/storage/G2UZ7F95/In et al. - 2014 - Cancer Recurrence An Important but Missing Variab.pdf:application/pdf},
}

@article{warren_challenges_2015,
	title = {Challenges and opportunities in measuring cancer recurrence in the {United} {States}},
	volume = {107},
	issn = {1460-2105},
	doi = {10.1093/jnci/djv134},
	abstract = {Cancer recurrence and disease-free survival are key outcomes for measuring the burden of illness, assessing the quality of cancer care, and informing decisions about increasingly costly cancer therapies. Yet information about recurrence is not collected in cancer registries or other population-based data sources. To address the lack of population-based recurrence information, researchers are increasingly using algorithms applied to health claims to infer recurrence. However, the validity of these approaches has not been comprehensively evaluated. In this commentary, we review existing studies and discuss options for improving the availability of recurrence data. We found that the validity of claims-based approaches appears promising in small, single institution studies, but larger population-based studies have identified substantial limitations with using claims to identify recurrence. With the increasing availability of health data, there are potential options that can be implemented to enhance information about recurrence. These options include design of software for the electronic medical record that enables rapid and standardized reporting of recurrence, use of electronic pathology reports to facilitate streamlined collection of recurrence by cancer registries, and mandates by insurers to require reporting of recurrence on health claims submitted by physicians. All of these options will require that governmental agencies, health insurers, professional societies, and other groups recognize the importance of population-based recurrence data and determine that this information is a priority for assessing cancer outcomes and costs.},
	language = {eng},
	number = {8},
	journal = {Journal of the National Cancer Institute},
	author = {Warren, Joan L. and Yabroff, K. Robin},
	month = aug,
	year = {2015},
	pmid = {25971299},
	pmcid = {PMC4580558},
	keywords = {Humans, Neoplasms, Software, United States, Algorithms, Registries, Disease-Free Survival, Neoplasm Recurrence, Local, Electronic Health Records, Population Surveillance, Insurance, Health, Insurance Claim Review, Recurrence},
	pages = {djv134},
	file = {Warren and Yabroff - 2015 - Challenges and opportunities in measuring cancer r.pdf:/Users/elisedumas/Zotero/storage/YTRJ36ZB/Warren and Yabroff - 2015 - Challenges and opportunities in measuring cancer r.pdf:application/pdf},
}

@article{sung_global_2021,
	title = {Global {Cancer} {Statistics} 2020: {GLOBOCAN} {Estimates} of {Incidence} and {Mortality} {Worldwide} for 36 {Cancers} in 185 {Countries}},
	volume = {71},
	issn = {1542-4863},
	shorttitle = {Global {Cancer} {Statistics} 2020},
	doi = {10.3322/caac.21660},
	abstract = {This article provides an update on the global cancer burden using the GLOBOCAN 2020 estimates of cancer incidence and mortality produced by the International Agency for Research on Cancer. Worldwide, an estimated 19.3 million new cancer cases (18.1 million excluding nonmelanoma skin cancer) and almost 10.0 million cancer deaths (9.9 million excluding nonmelanoma skin cancer) occurred in 2020. Female breast cancer has surpassed lung cancer as the most commonly diagnosed cancer, with an estimated 2.3 million new cases (11.7\%), followed by lung (11.4\%), colorectal (10.0 \%), prostate (7.3\%), and stomach (5.6\%) cancers. Lung cancer remained the leading cause of cancer death, with an estimated 1.8 million deaths (18\%), followed by colorectal (9.4\%), liver (8.3\%), stomach (7.7\%), and female breast (6.9\%) cancers. Overall incidence was from 2-fold to 3-fold higher in transitioned versus transitioning countries for both sexes, whereas mortality varied {\textless}2-fold for men and little for women. Death rates for female breast and cervical cancers, however, were considerably higher in transitioning versus transitioned countries (15.0 vs 12.8 per 100,000 and 12.4 vs 5.2 per 100,000, respectively). The global cancer burden is expected to be 28.4 million cases in 2040, a 47\% rise from 2020, with a larger increase in transitioning (64\% to 95\%) versus transitioned (32\% to 56\%) countries due to demographic changes, although this may be further exacerbated by increasing risk factors associated with globalization and a growing economy. Efforts to build a sustainable infrastructure for the dissemination of cancer prevention measures and provision of cancer care in transitioning countries is critical for global cancer control.},
	language = {eng},
	number = {3},
	journal = {CA: a cancer journal for clinicians},
	author = {Sung, Hyuna and Ferlay, Jacques and Siegel, Rebecca L. and Laversanne, Mathieu and Soerjomataram, Isabelle and Jemal, Ahmedin and Bray, Freddie},
	month = may,
	year = {2021},
	pmid = {33538338},
	keywords = {Female, Humans, Male, Databases, Factual, Neoplasms, Incidence, Risk Factors, cancer, Global Health, mortality, epidemiology, incidence, Sex Distribution, Europe, Developing Countries, Population Dynamics, Africa, Americas, Asia, burden, Developed Countries, Internationality, Oceania},
	pages = {209--249},
}

@article{hudis_proposal_2007,
	title = {Proposal for {Standardized} {Definitions} for {Efficacy} {End} {Points} in {Adjuvant} {Breast} {Cancer} {Trials}: {The} {STEEP} {System}},
	volume = {25},
	issn = {0732-183X, 1527-7755},
	shorttitle = {Proposal for {Standardized} {Definitions} for {Efficacy} {End} {Points} in {Adjuvant} {Breast} {Cancer} {Trials}},
	url = {https://ascopubs.org/doi/10.1200/JCO.2006.10.3523},
	doi = {10.1200/JCO.2006.10.3523},
	abstract = {Purpose Standardized deﬁnitions of breast cancer clinical trial end points must be adopted to permit the consistent interpretation and analysis of breast cancer clinical trials and to facilitate cross-trial comparisons and meta-analyses. Standardizing terms will allow for uniformity in data collection across studies, which will optimize clinical trial utility and efﬁciency. A given end point term (eg, overall survival) used in a breast cancer trial should always encompass the same set of events (eg, death attributable to breast cancer, death attributable to cause other than breast cancer, death from unknown cause), and, in turn, each event within that end point should be commonly deﬁned across end points and studies.
Methods A panel of experts in breast cancer clinical trials representing medical oncology, biostatistics, and correlative science convened to formulate standard deﬁnitions and address the confusion that nonstandard deﬁnitions of widely used end point terms for a breast cancer clinical trial can generate. We propose standard deﬁnitions for efﬁcacy end points and events in early-stage adjuvant breast cancer clinical trials. In some cases, it is expected that the standard end points may not address a speciﬁc trial question, so that modiﬁed or customized end points would need to be prospectively deﬁned and consistently used.
Conclusion The use of the proposed common end point deﬁnitions will facilitate interpretation of trial outcomes. This approach may be adopted to develop standard outcome deﬁnitions for use in trials involving other cancer sites.},
	language = {en},
	number = {15},
	urldate = {2022-08-05},
	journal = {Journal of Clinical Oncology},
	author = {Hudis, Clifford A. and Barlow, William E. and Costantino, Joseph P. and Gray, Robert J. and Pritchard, Kathleen I. and Chapman, Judith-Anne W. and Sparano, Joseph A. and Hunsberger, Sally and Enos, Rebecca A. and Gelber, Richard D. and Zujewski, Jo Anne},
	month = may,
	year = {2007},
	pages = {2127--2132},
	file = {Hudis et al. - 2007 - Proposal for Standardized Definitions for Efficacy.pdf:/Users/elisedumas/Zotero/storage/B7AVNBRP/Hudis et al. - 2007 - Proposal for Standardized Definitions for Efficacy.pdf:application/pdf},
}

@article{luyendijk_assessment_2020,
	title = {Assessment of {Studies} {Evaluating} {Incremental} {Costs}, {Effectiveness}, or {Cost}-{Effectiveness} of {Systemic} {Therapies} in {Breast} {Cancer} {Based} on {Claims} {Data}: {A} {Systematic} {Review}},
	volume = {23},
	issn = {1524-4733},
	shorttitle = {Assessment of {Studies} {Evaluating} {Incremental} {Costs}, {Effectiveness}, or {Cost}-{Effectiveness} of {Systemic} {Therapies} in {Breast} {Cancer} {Based} on {Claims} {Data}},
	doi = {10.1016/j.jval.2020.05.008},
	abstract = {OBJECTIVES: Large secondary databases, such as those containing insurance claims data, are increasingly being used to compare the effects and costs of treatments in routine clinical practice. Despite their appeal, however, caution must be exercised when using these data. In this study, we aimed to identify and assess the methodological quality of studies that used claims data to compare the effectiveness, costs, or cost-effectiveness of systemic therapies for breast cancer.
METHODS: We searched Embase, the Cochrane Library, Medline, Web of Science, and Google Scholar for English-language publications and assessed the methodological quality using the Good Research for Comparative Effectiveness principles. This study was registered with the International Prospective Register of Systematic Reviews (PROSPERO) under number CRD42018103992.
RESULTS: We identified 1251 articles, of which 106 met the inclusion criteria. Most studies were conducted in the United States (74\%) and Taiwan (9\%) and were based on claims data sets (35\%) or claims data linked to cancer registries (58\%). Furthermore, most included large samples (mean 17 130 patients) and elderly patients, and they covered various outcomes (eg, survival, adverse events, resource use, and costs). Key methodological shortcomings were the lack of information on relevant confounders, the risk of immortal time bias, and the lack of information on the validity of outcomes. Only a few studies performed sensitivity analyses.
CONCLUSIONS: Many comparative studies of cost, effectiveness, and cost-effectiveness have been published in recent decades based on claims data, and the number of publications has increased over time. Despite the availability of guidelines to improve quality, methodological issues persist and are often inappropriately addressed or reported.},
	language = {eng},
	number = {11},
	journal = {Value in Health: The Journal of the International Society for Pharmacoeconomics and Outcomes Research},
	author = {Luyendijk, Marianne and Vernooij, Robin W. M. and Blommestein, Hedwig M. and Siesling, Sabine and Uyl-de Groot, Carin A.},
	month = nov,
	year = {2020},
	pmid = {33127021},
	keywords = {Female, Humans, United States, Breast Neoplasms, administrative data, Insurance Claim Review, Survival, claims data, Taiwan, Drug-Related Side Effects and Adverse Reactions, Cost-Benefit Analysis, secondary data},
	pages = {1497--1508},
}

@article{drohan_electronic_2009,
	title = {Electronic health records and the management of women at high risk of hereditary breast and ovarian cancer},
	volume = {15 Suppl 1},
	issn = {1524-4741},
	doi = {10.1111/j.1524-4741.2009.00796.x},
	abstract = {Currently, management strategies exist that can decrease the morbidity and mortality associated with having a BRCA1 or BRCA2 mutation. Unfortunately, the task of identifying these patients at high risk is a daunting challenge. This problem is intensified because Electronic Health Records (EHRs) today lack the functionality needed to identify these women and to manage those women once they have been identified. Numerous niche software programs have been developed to fill this gap. Unfortunately, these extremely valuable niche programs are prevented from being interoperable with the EHRs, on the premise that each EHR vendor will build their own programs. Effectively, in our efforts to adopt EHRs, we have lost sight of the fact that they can only have a major impact on quality of care if they contain structured data and if they interact with robust Clinical Decision Support (CDS) tools. We are at a cross roads in the development of the health care Information Technology infrastructure. We can choose a path where each EHR vendor develops each CDS module independently. Alternatively, we can choose a path where experts in each field develop external niche software modules that are interoperable with any EHR vendor. We believe that the modular approach to development of niche software programs that are interoperable with current EHRs will markedly increase the speed at which useful and functional EHRs that improve quality of care become a reality. Thus, in order to realize the benefits of CDS, we suggest vendors develop means to become interoperable with external modular niche programs.},
	language = {eng},
	journal = {The Breast Journal},
	author = {Drohan, Brian and Ozanne, Elissa M. and Hughes, Kevin S.},
	month = oct,
	year = {2009},
	pmid = {19775330},
	keywords = {Data Interpretation, Statistical, Female, Humans, Data Collection, Mutation, Breast Neoplasms, Electronic Health Records, Genes, BRCA1, Genes, BRCA2, Decision Support Systems, Clinical, Risk Management, Ovarian Neoplasms},
	pages = {S46--55},
}

@article{mayer_assuring_2015,
	title = {Assuring {Quality} {Cancer} {Survivorship} {Care}: {We}'ve {Only} {Just} {Begun}},
	issn = {1548-8756},
	shorttitle = {Assuring {Quality} {Cancer} {Survivorship} {Care}},
	doi = {10.14694/EdBook_AM.2015.35.e583},
	abstract = {Clinical practice guidelines, quality metrics, and performance improvement projects are the key tools of the national movement to improve and assure quality cancer care. Each of these evaluation instruments is intended to assess quality from a unique perspective, including that of the individual provider, the practice/hospital, and the health care system. A number of organizations have developed or endorsed quality measures specific to cancer, however, these have not formally included survivorship measures. Fortunately, the American Society of Clinical Oncology (ASCO), the National Comprehensive Cancer Network, the American Cancer Society, and the American College of Surgeons (ACoS) have taken a leadership role in developing survivorship guidelines and quality metrics. Both ASCO and ACoS have focused their efforts on the treatment summary and care plan, a document that was proposed in the 2006 Institute of Medicine report on cancer survivorship. ASCO has proposed a care plan template for implementation and incorporation into the electronic health records (EHR), which will lend itself to structure, process, and outcome measurement. ACoS, conversely, has included the care plan in its cancer program standards with annual evaluation metrics. In addition, ASCO has developed a number of key survivorship-relevant metrics as part of its Quality Oncology Practice Initiative (QOPI), a tool developed to measure quality cancer care and assess adherence to guidelines across academic and community practices. Together, these efforts will direct us to more effective ways to disseminate guideline recommendations and to better methods of assessing quality survivorship care nationally.},
	language = {eng},
	journal = {American Society of Clinical Oncology Educational Book. American Society of Clinical Oncology. Annual Meeting},
	author = {Mayer, Deborah K. and Shapiro, Charles L. and Jacobson, Paul and McCabe, Mary S.},
	year = {2015},
	pmid = {25993226},
	keywords = {Humans, Neoplasms, Quality of Health Care, United States, Delivery of Health Care, Electronic Health Records, Survivors},
	pages = {e583--591},
}

@article{abul-husn_personalized_2019,
	title = {Personalized medicine and the power of electronic health records},
	volume = {177},
	issn = {0092-8674},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6921466/},
	doi = {10.1016/j.cell.2019.02.039},
	abstract = {Personalized medicine has largely been enabled by the integration of genomic and other data with electronic health records (EHRs) in the U.S. and elsewhere. Increased EHR adoption across various clinical settings, and the establishment of EHR-linked population-based biobanks provide unprecedented opportunities for the types of translational and implementation research that drive personalized medicine. We review advances in the digitization of health information and the proliferation of genomic research in health systems, and provide insights into emerging paths for the widespread implementation of personalized medicine.},
	number = {1},
	urldate = {2022-08-05},
	journal = {Cell},
	author = {Abul-Husn, Noura S. and Kenny, Eimear E.},
	month = mar,
	year = {2019},
	pmid = {30901549},
	pmcid = {PMC6921466},
	pages = {58--69},
	file = {Full Text:/Users/elisedumas/Zotero/storage/44XJ88DI/Abul-Husn and Kenny - 2019 - Personalized medicine and the power of electronic .pdf:application/pdf},
}

@article{manca_electronic_2015,
	title = {Do electronic medical records improve quality of care?},
	volume = {61},
	issn = {0008-350X},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4607324/},
	number = {10},
	urldate = {2022-08-05},
	journal = {Canadian Family Physician},
	author = {Manca, Donna P.},
	month = oct,
	year = {2015},
	pmid = {26472786},
	pmcid = {PMC4607324},
	pages = {846--847},
}

@article{salmeron_assessing_2021,
	title = {Assessing health disparities in breast cancer incidence burden in {Tennessee}: geospatial analysis},
	volume = {21},
	issn = {1472-6874},
	shorttitle = {Assessing health disparities in breast cancer incidence burden in {Tennessee}},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8091807/},
	doi = {10.1186/s12905-021-01274-9},
	abstract = {Background
Tennessee women experience the 12th highest breast cancer mortality in the United States. We examined the geographic differences in breast cancer incidence in Tennessee between Appalachian and non-Appalachian counties from 2005 to 2015.

Methods
We used ArcGIS 10.7 geospatial analysis and logistic regression on the Tennessee Cancer Registry incidence data for adult women aged ≥ 18 years (N = 59,287) who were diagnosed with breast cancer from 2005 to 2015 to evaluate distribution patterns by Appalachian county designation. The Tennessee Cancer Registry is a population-based, central cancer registry serving the citizens of Tennessee and was established by Tennessee law to collect and monitor cancer incidence. The main outcome was breast cancer stage at diagnosis. Independent variables were age, race, marital status, type of health insurance, and county of residence.

Results
Majority of the sample were White (85.5\%), married (58.6\%), aged ≥ 70 (31.3\%) and diagnosed with an early stage breast cancer (69.6\%). More than half of the women had public health insurance (54.2\%), followed by private health insurance coverage (44.4\%). Over half of the women resided in non-Appalachian counties, whereas 47.6\% were in the Appalachian counties. We observed a significant association among breast cancer patients with respect to marital status and type of health insurance coverage (p =  {\textless} 0.0001). While the logistic regression did not show a significant result between county of residence and breast cancer incidence, the spatial analysis revealed geographic differences between Appalachian and non-Appalachian counties. The highest incidence rates of 997.49–1164.59/100,000 were reported in 6 Appalachian counties (Anderson, Blount, Knox, Rhea, Roane, and Van Buren) compared to 3 non-Appalachian counties (Fayette, Marshall, and Williamson).

Conclusions
There is a need to expand resources in Appalachian Tennessee to enhance breast cancer screening and early detection. Using geospatial techniques can further elucidate disparities that may be overlooked in conventional linear analyses to improve women’s cancer health and associated outcomes.},
	urldate = {2022-08-05},
	journal = {BMC Women's Health},
	author = {Salmeron, Bonita and Mamudu, Lohuwa and Liu, Xiaohui and Whiteside, Martin and Williams, Faustine},
	month = may,
	year = {2021},
	pmid = {33941168},
	pmcid = {PMC8091807},
	pages = {186},
	file = {Full Text:/Users/elisedumas/Zotero/storage/9WK8J9KC/Salmeron et al. - 2021 - Assessing health disparities in breast cancer inci.pdf:application/pdf},
}

@article{chubak_electronic_2017,
	title = {An {Electronic} {Health} {Record}-based {Algorithm} to {Ascertain} the {Date} of {Second} {Breast} {Cancer} {Events}},
	volume = {55},
	issn = {1537-1948},
	doi = {10.1097/MLR.0000000000000352},
	abstract = {OBJECTIVES: Studies of cancer recurrences and second primary tumors require information on outcome dates. Little is known about how well electronic health record-based algorithms can identify dates or how errors in dates can bias analyses.
RESEARCH DESIGN: We assessed rule-based and model-fitting approaches to assign event dates using a previously published electronic health record-based algorithm for second breast cancer events (SBCE). We conducted a simulation study to assess bias due to date assignment errors in time-to-event analyses.
SUBJECTS: From a cohort of 3152 early-stage breast cancer patients, 358 women accurately identified as having had an SBCE served as the basis for this analysis.
MEASURES: Percent of predicted SBCE dates identified within ±60 days of the true date was the primary measure of accuracy. In the simulation study, bias in hazard ratios (HRs) was estimated by averaging the difference between HRs based on algorithm-assigned dates and the true HR across 1000 simulations each with simulated N=4000.
RESULTS: The most accurate date algorithm had a median difference between the true and predicted dates of 0 days with 82\% of predicted dates falling within 60 days of the true date. Bias resulted when algorithm sensitivity and specificity varied by exposure status, but was minimal when date assignment errors were of the magnitude observed for our date assignment method.
CONCLUSIONS: SBCE date can be relatively accurately assigned based on a previous algorithm. While acceptable in many scenarios, algorithm-assigned dates are not appropriate to use when operating characteristics are likely to vary by the study exposure.},
	language = {eng},
	number = {12},
	journal = {Medical Care},
	author = {Chubak, Jessica and Onega, Tracy and Zhu, Weiwei and Buist, Diana S. M. and Hubbard, Rebecca A.},
	month = dec,
	year = {2017},
	pmid = {29135770},
	pmcid = {PMC4592686},
	keywords = {Adult, Female, Humans, United States, Algorithms, Breast Neoplasms, Aged, Middle Aged, Risk Factors, Neoplasm Recurrence, Local, Cohort Studies, Electronic Health Records, Sensitivity and Specificity},
	pages = {e81--e87},
	file = {Accepted Version:/Users/elisedumas/Zotero/storage/L36CSH3C/Chubak et al. - 2017 - An Electronic Health Record-based Algorithm to Asc.pdf:application/pdf},
}

@article{kroenke_enhancing_2016,
	title = {Enhancing {Breast} {Cancer} {Recurrence} {Algorithms} {Through} {Selective} {Use} of {Medical} {Record} {Data}},
	volume = {108},
	issn = {1460-2105},
	doi = {10.1093/jnci/djv336},
	abstract = {BACKGROUND: The utility of data-based algorithms in research has been questioned because of errors in identification of cancer recurrences. We adapted previously published breast cancer recurrence algorithms, selectively using medical record (MR) data to improve classification.
METHODS: We evaluated second breast cancer event (SBCE) and recurrence-specific algorithms previously published by Chubak and colleagues in 1535 women from the Life After Cancer Epidemiology (LACE) and 225 women from the Women's Health Initiative cohorts and compared classification statistics to published values. We also sought to improve classification with minimal MR examination. We selected pairs of algorithms-one with high sensitivity/high positive predictive value (PPV) and another with high specificity/high PPV-using MR information to resolve discrepancies between algorithms, properly classifying events based on review; we called this "triangulation." Finally, in LACE, we compared associations between breast cancer survival risk factors and recurrence using MR data, single Chubak algorithms, and triangulation.
RESULTS: The SBCE algorithms performed well in identifying SBCE and recurrences. Recurrence-specific algorithms performed more poorly than published except for the high-specificity/high-PPV algorithm, which performed well. The triangulation method (sensitivity = 81.3\%, specificity = 99.7\%, PPV = 98.1\%, NPV = 96.5\%) improved recurrence classification over two single algorithms (sensitivity = 57.1\%, specificity = 95.5\%, PPV = 71.3\%, NPV = 91.9\%; and sensitivity = 74.6\%, specificity = 97.3\%, PPV = 84.7\%, NPV = 95.1\%), with 10.6\% MR review. Triangulation performed well in survival risk factor analyses vs analyses using MR-identified recurrences.
CONCLUSIONS: Use of multiple recurrence algorithms in administrative data, in combination with selective examination of MR data, may improve recurrence data quality and reduce research costs.},
	language = {eng},
	number = {3},
	journal = {Journal of the National Cancer Institute},
	author = {Kroenke, Candyce H. and Chubak, Jessica and Johnson, Lisa and Castillo, Adrienne and Weltzien, Erin and Caan, Bette J.},
	month = mar,
	year = {2016},
	pmid = {26582243},
	pmcid = {PMC5943828},
	keywords = {Adult, Female, Humans, United States, Algorithms, Breast Neoplasms, Aged, Aged, 80 and over, Middle Aged, Prognosis, Risk Factors, Neoplasm Recurrence, Local, Odds Ratio, Cohort Studies, Sensitivity and Specificity, Medical Records},
	pages = {djv336},
	file = {Full Text:/Users/elisedumas/Zotero/storage/6FHVU5AK/Kroenke et al. - 2016 - Enhancing Breast Cancer Recurrence Algorithms Thro.pdf:application/pdf},
}

@misc{suresh_use_2017,
	title = {The {Use} of {Autoencoders} for {Discovering} {Patient} {Phenotypes}},
	url = {http://arxiv.org/abs/1703.07004},
	doi = {10.48550/arXiv.1703.07004},
	abstract = {We use autoencoders to create low-dimensional embeddings of underlying patient phenotypes that we hypothesize are a governing factor in determining how different patients will react to different interventions. We compare the performance of autoencoders that take fixed length sequences of concatenated timesteps as input with a recurrent sequence-to-sequence autoencoder. We evaluate our methods on around 35,500 patients from the latest MIMIC III dataset from Beth Israel Deaconess Hospital.},
	urldate = {2022-08-05},
	publisher = {arXiv},
	author = {Suresh, Harini and Szolovits, Peter and Ghassemi, Marzyeh},
	month = mar,
	year = {2017},
	note = {arXiv:1703.07004 [cs]},
	keywords = {Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/elisedumas/Zotero/storage/YRQ77HCE/Suresh et al. - 2017 - The Use of Autoencoders for Discovering Patient Ph.pdf:application/pdf;arXiv.org Snapshot:/Users/elisedumas/Zotero/storage/Y74ZKMUZ/1703.html:text/html},
}

@inproceedings{jia_spatio-temporal_2017-1,
	title = {Spatio-temporal autoencoder for feature learning in patient data with missing observations},
	doi = {10.1109/BIBM.2017.8217773},
	abstract = {Modern patient data tends to be large-scale and multi-dimensional, containing both spatial and temporal features. Learning good spatio-temporal features from large patient data is a challenging task, especially when there are missing observations. In this paper, we propose a spatio-temporal autoencoder (STAE), an unsupervised deep learning scheme, to learn features from large-scale and high-dimensional patient data with missing observations. Through both spatial and temporal encoding, STAE is able to automatically identify patterns and dependencies in the patient data, even with missing values, and learn a compact representation of each patient for better classification. Publicly available electroencephalogram (EEG) data are extracted from the UCI Machine Learning Repository to test and support our findings. Through simulations, we compare STAE with several baseline feature selection methods and demonstrate its effectiveness in the presence of missing data.},
	booktitle = {2017 {IEEE} {International} {Conference} on {Bioinformatics} and {Biomedicine} ({BIBM})},
	author = {Jia, Yao and Zhou, Chongyu and Motani, Mehul},
	month = nov,
	year = {2017},
	keywords = {Training, Feature extraction, Logic gates, Correlation, Decoding, Encoding},
	pages = {886--890},
	file = {IEEE Xplore Abstract Record:/Users/elisedumas/Zotero/storage/UA86IUKK/8217773.html:text/html},
}

@inproceedings{baumel_multi-label_2018,
	title = {Multi-{Label} {Classification} of {Patient} {Notes} a {Case} {Study} on {ICD} {Code} {Assignment}},
	abstract = {HA-GRU, a hierarchical approach to tag a document by identifying the sentences relevant for each label achieves state-of-the art results and highlights the model decision process, allows easier error analysis, and suggests future directions for improvement. In the context of the Electronic Health Record, automated diagnosis coding of patient notes is a useful task, but a challenging one due to the large number of codes and the length of patient notes. We investigate four models for assigning multiple ICD codes to discharge summaries taken from both MIMIC II and III. We present Hierarchical Attention-GRU (HA-GRU), a hierarchical approach to tag a document by identifying the sentences relevant for each label. HA-GRU achieves state-of-the art results. Furthermore, the learned sentence-level attention layer highlights the model decision process, allows easier error analysis, and suggests future directions for improvement.},
	booktitle = {{AAAI} {Workshops}},
	author = {Baumel, Tal and Nassour-Kassis, Jumana and Elhadad, Michael and Elhadad, Noémie},
	year = {2018},
	file = {Full Text PDF:/Users/elisedumas/Zotero/storage/ZZMFGSGC/Baumel et al. - 2018 - Multi-Label Classification of Patient Notes a Case.pdf:application/pdf},
}

@inproceedings{zhang_leap_2017,
	address = {New York, NY, USA},
	series = {{KDD} '17},
	title = {{LEAP}: {Learning} to {Prescribe} {Effective} and {Safe} {Treatment} {Combinations} for {Multimorbidity}},
	isbn = {978-1-4503-4887-4},
	shorttitle = {{LEAP}},
	url = {https://doi.org/10.1145/3097983.3098109},
	doi = {10.1145/3097983.3098109},
	abstract = {Managing patients with complex multimorbidity has long been recognized as a difficult problem due to complex disease and medication dependencies and the potential risk of adverse drug interactions. Existing work either uses complicated rule-based protocols which are hard to implement and maintain, or simple statistical models that treat each disease independently, which may lead to sub-optimal or even harmful drug combinations. In this work, we propose the LEAP (LEArn to Prescribe) algorithm to decompose the treatment recommendation into a sequential decision-making process while automatically determining the appropriate number of medications. A recurrent decoder is used to model label dependencies and content-based attention is used to capture label instance mapping. We further leverage reinforcement learning to fine tune the model parameters to ensure accuracy and completeness. We incorporate external clinical knowledge into the design of the reinforcement reward to effectively prevent generating unfavorable drug combinations. Both quantitative experiments and qualitative case studies are conducted on two real world electronic health record datasets to verify the effectiveness of our solution. On both datasets, LEAP significantly outperforms baselines by up to 10-30\% in terms of mean Jaccard coefficient and removes 99.8\% adverse drug interactions in the recommended treatment sets.},
	urldate = {2022-08-05},
	booktitle = {Proceedings of the 23rd {ACM} {SIGKDD} {International} {Conference} on {Knowledge} {Discovery} and {Data} {Mining}},
	publisher = {Association for Computing Machinery},
	author = {Zhang, Yutao and Chen, Robert and Tang, Jie and Stewart, Walter F. and Sun, Jimeng},
	month = aug,
	year = {2017},
	keywords = {multi-instance multilabel learning, multimorbidity, treatment recommendation},
	pages = {1315--1324},
}

@article{hong_predicting_2019,
	title = {Predicting {Alzheimer}’s {Disease} {Using} {LSTM}},
	volume = {7},
	issn = {2169-3536},
	doi = {10.1109/ACCESS.2019.2919385},
	abstract = {Alzheimer's Disease (AD) is a chronic neurodegenerative disease. Early diagnosis will considerably decrease the risk of further deterioration. Unfortunately, current studies mainly focus on classifying the states of disease in its current stage, instead of predicting the possible development of the disease. Long short-term memory (LSTM) is a special kind of recurrent neural network, which might be able to connect previous information to the present task. Noticing that the temporal data for a patient are potentially meaningful for predicting the development of the disease, we propose a predicting model based on LSTM. Therefore an LSTM network, with fully connected layer and activation layers, is built to encode the temporal relation between features and the next stage of Alzheimer's Disease. The Experiments show that our model outperforms most of the existing models.},
	journal = {IEEE Access},
	author = {Hong, Xin and Lin, Rongjie and Yang, Chenhui and Zeng, Nianyin and Cai, Chunting and Gou, Jin and Yang, Jane},
	year = {2019},
	note = {Conference Name: IEEE Access},
	keywords = {Biological neural networks, Predictive models, LSTM, Logic gates, Time series analysis, Magnetic Resonance Imaging, Alzheimer's disease, Alzheimer’s Disease, Magnetic resonance imaging, Prediction, Time Sequence},
	pages = {80893--80901},
	file = {IEEE Xplore Abstract Record:/Users/elisedumas/Zotero/storage/66JG9TUR/8723315.html:text/html;IEEE Xplore Full Text PDF:/Users/elisedumas/Zotero/storage/XNPZFEUI/Hong et al. - 2019 - Predicting Alzheimer’s Disease Using LSTM.pdf:application/pdf},
}

@article{li_diagnosis_2022,
	title = {Diagnosis of {Alzheimer}'s disease by feature weighted-{LSTM}: a preliminary study of temporal features in brain resting-state {fMRI}},
	volume = {21},
	issn = {0219-6352},
	shorttitle = {Diagnosis of {Alzheimer}'s disease by feature weighted-{LSTM}},
	doi = {10.31083/j.jin2102056},
	abstract = {The long short-term memory network (LSTM) is widely used in time series data processing as a temporal recursive network. The resting-state functional magnetic resonance data shows that not only are there temporal variations in the resting state, but there are also interactions between brain regions. To integrate the temporal and spatial characteristics of brain regions, this paper proposes a model called feature weighted-LSTM (FW-LSTM). The feature weight is defined by spatial characteristics calculating the frequency of connectivity of each brain region and further integrated into the LSTM. Thus, it can comprehensively model both temporal and spatial changes in rs-fMRI brain regions. The FW-LSTM model on the Alzheimer's disease neuroimaging initiative (ADNI) dataset is used to extract the time-varying characteristics of 90 brain regions for Alzheimer's disease (AD) classification. The model performances are 77.80\%, 76.41\%, and 78.81\% in accuracy, sensitivity, and specificity. It outperformed the one-dimensional convolutional neural networks (1D-CNN) model and LSTM model, which only used temporal features of brain regions.},
	language = {eng},
	number = {2},
	journal = {Journal of Integrative Neuroscience},
	author = {Li, Jiyun and Song, Binbin and Qian, Chen},
	month = mar,
	year = {2022},
	pmid = {35364644},
	keywords = {Humans, Neural Networks, Computer, Magnetic Resonance Imaging, Alzheimer Disease, Brain, FW-LSTM, Neuroimaging, Rs-fMRI data, Spatial characteristics, Temporal characteristics},
	pages = {56},
	file = {Full Text:/Users/elisedumas/Zotero/storage/UXJBZAPE/Li et al. - 2022 - Diagnosis of Alzheimer's disease by feature weight.pdf:application/pdf},
}

@article{rumelhart_learning_1986,
	title = {Learning representations by back-propagating errors},
	volume = {323},
	copyright = {1986 Nature Publishing Group},
	issn = {1476-4687},
	url = {https://www.nature.com/articles/323533a0},
	doi = {10.1038/323533a0},
	abstract = {We describe a new learning procedure, back-propagation, for networks of neurone-like units. The procedure repeatedly adjusts the weights of the connections in the network so as to minimize a measure of the difference between the actual output vector of the net and the desired output vector. As a result of the weight adjustments, internal ‘hidden’ units which are not part of the input or output come to represent important features of the task domain, and the regularities in the task are captured by the interactions of these units. The ability to create useful new features distinguishes back-propagation from earlier, simpler methods such as the perceptron-convergence procedure1.},
	language = {en},
	number = {6088},
	urldate = {2022-08-06},
	journal = {Nature},
	author = {Rumelhart, David E. and Hinton, Geoffrey E. and Williams, Ronald J.},
	month = oct,
	year = {1986},
	note = {Number: 6088
Publisher: Nature Publishing Group},
	keywords = {Humanities and Social Sciences, multidisciplinary, Science},
	pages = {533--536},
	file = {Snapshot:/Users/elisedumas/Zotero/storage/XE6KRB6H/323533a0.html:text/html},
}

@article{hochreiter_vanishing_1998,
	title = {The {Vanishing} {Gradient} {Problem} {During} {Learning} {Recurrent} {Neural} {Nets}  and {Problem} {Solutions}},
	volume = {06},
	issn = {0218-4885},
	url = {https://www.worldscientific.com/doi/abs/10.1142/s0218488598000094},
	doi = {10.1142/S0218488598000094},
	abstract = {Recurrent nets are in principle capable to store past inputs to produce the currently desired output. Because of this property recurrent nets are used in time series prediction and process control. Practical applications involve temporal dependencies spanning many time steps, e.g. between relevant inputs and desired outputs. In this case, however, gradient based learning methods take too much time. The extremely increased learning time arises because the error vanishes as it gets propagated back. In this article the de-caying error flow is theoretically analyzed. Then methods trying to overcome vanishing gradients are briefly discussed. Finally, experiments comparing conventional algorithms and alternative methods are presented. With advanced methods long time lag problems can be solved in reasonable time.},
	number = {02},
	urldate = {2022-08-06},
	journal = {International Journal of Uncertainty, Fuzziness and Knowledge-Based Systems},
	author = {Hochreiter, Sepp},
	month = apr,
	year = {1998},
	note = {Publisher: World Scientific Publishing Co.},
	keywords = {long short-term memory, long-term dependencies, Recurrent neural nets, vanishing gradient},
	pages = {107--116},
}

@article{noh_analysis_2021,
	title = {Analysis of {Gradient} {Vanishing} of {RNNs} and {Performance} {Comparison}},
	volume = {12},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	issn = {2078-2489},
	url = {https://www.mdpi.com/2078-2489/12/11/442},
	doi = {10.3390/info12110442},
	abstract = {A recurrent neural network (RNN) combines variable-length input data with a hidden state that depends on previous time steps to generate output data. RNNs have been widely used in time-series data analysis, and various RNN algorithms have been proposed, such as the standard RNN, long short-term memory (LSTM), and gated recurrent units (GRUs). In particular, it has been experimentally proven that LSTM and GRU have higher validation accuracy and prediction accuracy than the standard RNN. The learning ability is a measure of the effectiveness of gradient of error information that would be backpropagated. This study provided a theoretical and experimental basis for the result that LSTM and GRU have more efficient gradient descent than the standard RNN by analyzing and experimenting the gradient vanishing of the standard RNN, LSTM, and GRU. As a result, LSTM and GRU are robust to the degradation of gradient descent even when LSTM and GRU learn long-range input data, which means that the learning ability of LSTM and GRU is greater than standard RNN when learning long-range input data. Therefore, LSTM and GRU have higher validation accuracy and prediction accuracy than the standard RNN. In addition, it was verified whether the experimental results of river-level prediction models, solar power generation prediction models, and speech signal models using the standard RNN, LSTM, and GRUs are consistent with the analysis results of gradient vanishing.},
	language = {en},
	number = {11},
	urldate = {2022-08-06},
	journal = {Information},
	author = {Noh, Seol-Hyun},
	month = nov,
	year = {2021},
	note = {Number: 11
Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {LSTM, accuracy, RNN, gradient vanishing, GRU},
	pages = {442},
	file = {Full Text PDF:/Users/elisedumas/Zotero/storage/QVSEBNLJ/Noh - 2021 - Analysis of Gradient Vanishing of RNNs and Perform.pdf:application/pdf;Snapshot:/Users/elisedumas/Zotero/storage/YXKDXYR6/442.html:text/html},
}

@inproceedings{gers_learning_1999,
	title = {Learning to forget: continual prediction with {LSTM}},
	volume = {2},
	shorttitle = {Learning to forget},
	doi = {10.1049/cp:19991218},
	abstract = {Long short-term memory (LSTM) can solve many tasks not solvable by previous learning algorithms for recurrent neural networks (RNNs). We identify a weakness of LSTM networks processing continual input streams without explicitly marked sequence ends. Without resets, the internal state values may grow indefinitely and eventually cause the network to break down. Our remedy is an adaptive "forget gate" that enables an LSTM cell to learn to reset itself at appropriate times, thus releasing internal resources. We review an illustrative benchmark problem on which standard LSTM outperforms other RNN algorithms. All algorithms (including LSTM) fail to solve a continual version of that problem. LSTM with forget gates, however, easily solves it in an elegant way.},
	booktitle = {1999 {Ninth} {International} {Conference} on {Artificial} {Neural} {Networks} {ICANN} 99. ({Conf}. {Publ}. {No}. 470)},
	author = {Gers, F.A. and Schmidhuber, J. and Cummins, F.},
	month = sep,
	year = {1999},
	note = {ISSN: 0537-9989},
	pages = {850--855 vol.2},
	file = {IEEE Xplore Abstract Record:/Users/elisedumas/Zotero/storage/IYNPW77W/818041.html:text/html},
}

@article{bewick_statistics_2004,
	title = {Statistics review 12: {Survival} analysis},
	volume = {8},
	issn = {1364-8535},
	shorttitle = {Statistics review 12},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC1065034/},
	doi = {10.1186/cc2955},
	abstract = {This review introduces methods of analyzing data arising from studies where the response variable is the length of time taken to reach a certain end-point, often death. The Kaplan–Meier methods, log rank test and Cox's proportional hazards model are described.},
	number = {5},
	urldate = {2022-08-08},
	journal = {Critical Care},
	author = {Bewick, Viv and Cheek, Liz and Ball, Jonathan},
	year = {2004},
	pmid = {15469602},
	pmcid = {PMC1065034},
	pages = {389--394},
	file = {Full Text:/Users/elisedumas/Zotero/storage/XBEQKE5E/Bewick et al. - 2004 - Statistics review 12 Survival analysis.pdf:application/pdf},
}

@article{kaplan_nonparametric_1958,
	title = {Nonparametric {Estimation} from {Incomplete} {Observations}},
	volume = {53},
	issn = {0162-1459},
	url = {https://www.jstor.org/stable/2281868},
	doi = {10.2307/2281868},
	abstract = {In lifetesting, medical follow-up, and other fields the observation of the time of occurrence of the event of interest (called a death) may be prevented for some of the items of the sample by the previous occurrence of some other event (called a loss). Losses may be either accidental or controlled, the latter resulting from a decision to terminate certain observations. In either case it is usually assumed in this paper that the lifetime (age at death) is independent of the potential loss time; in practice this assumption deserves careful scrutiny. Despite the resulting incompleteness of the data, it is desired to estimate the proportion P(t) of items in the population whose lifetimes would exceed t (in the absence of such losses), without making any assumption about the form of the function P(t). The observation for each item of a suitable initial event, marking the beginning of its lifetime, is presupposed. For random samples of size N the product-limit (PL) estimate can be defined as follows: List and label the N observed lifetimes (whether to death or loss) in order of increasing magnitude, so that one has 0 ≤ t$_{\textrm{1}}$' ≤ t$_{\textrm{2}}$' ≤ ⋯ ≤ t$_{\textrm{N}}$'. Then {\textless}tex-math{\textgreater}\${\textbackslash}hat\{P\}(t) = {\textbackslash}prod\_r {\textbackslash}lbrack(N - r)/(N - r + 1){\textbackslash}rbrack\${\textless}/tex-math{\textgreater}, where r assumes those values for which t$_{\textrm{r}}$' ≤ t and for which t$_{\textrm{r}}$' measures the time to death. This estimate is the distribution, unrestricted as to form, which maximizes the likelihood of the observations. Other estimates that are discussed are the actuarial estimates (which are also products, but with the number of factors usually reduced by grouping); and reduced-sample (RS) estimates, which require that losses not be accidental, so that the limits of observation (potential loss times) are known even for those items whose deaths are observed. When no losses occur at ages less than t, the estimate of P(t) in all cases reduces to the usual binomial estimate, namely, the observed proportion of survivors.},
	number = {282},
	urldate = {2022-08-08},
	journal = {Journal of the American Statistical Association},
	author = {Kaplan, E. L. and Meier, Paul},
	year = {1958},
	note = {Publisher: [American Statistical Association, Taylor \& Francis, Ltd.]},
	pages = {457--481},
	file = {JSTOR Full Text PDF:/Users/elisedumas/Zotero/storage/PRHZZHHW/Kaplan and Meier - 1958 - Nonparametric Estimation from Incomplete Observati.pdf:application/pdf},
}

@article{cox_regression_1972,
	title = {Regression {Models} and {Life}-{Tables}},
	volume = {34},
	issn = {0035-9246},
	url = {https://www.jstor.org/stable/2985181},
	abstract = {The analysis of censored failure times is considered. It is assumed that on each individual are available values of one or more explanatory variables. The hazard function (age-specific failure rate) is taken to be a function of the explanatory variables and unknown regression coefficients multiplied by an arbitrary and unknown function of time. A conditional likelihood is obtained, leading to inferences about the unknown regression coefficients. Some generalizations are outlined.},
	number = {2},
	urldate = {2022-08-08},
	journal = {Journal of the Royal Statistical Society. Series B (Methodological)},
	author = {Cox, D. R.},
	year = {1972},
	note = {Publisher: [Royal Statistical Society, Wiley]},
	pages = {187--220},
	file = {JSTOR Full Text PDF:/Users/elisedumas/Zotero/storage/FDTVZBYJ/Cox - 1972 - Regression Models and Life-Tables.pdf:application/pdf},
}

@article{anderson_nonproportional_1991,
	title = {A {Nonproportional} {Hazards} {Weibull} {Accelerated} {Failure} {Time} {Regression} {Model}},
	volume = {47},
	issn = {0006-341X},
	url = {https://www.jstor.org/stable/2532512},
	doi = {10.2307/2532512},
	abstract = {We present a study of risk factors measured in men before age 50 and subsequent incidence of heart disease over 32 years of follow-up. The data are from the Framingham Heart Study. The standard accelerated failure time model assumes the logarithm of time until an event has a constant dispersion parameter and a location parameter that is a linear function of covariates. Parameters are estimated by maximum likelihood. We reject a standard Weibull model for these data in favor of a model with the dispersion parameter depending on the location parameter. This model suggests that the cumulative hazard ratio for two individuals shrinks toward unity over the follow-up period. Thus, not only the standard Weibull, but also the semiparametric proportional hazards (Cox) model is inadequate for this data. The model improvement appears particularly valuable when estimating the difference in predicted outcome probabilities for two individuals.},
	number = {1},
	urldate = {2022-08-08},
	journal = {Biometrics},
	author = {Anderson, Keaven M.},
	year = {1991},
	note = {Publisher: [Wiley, International Biometric Society]},
	pages = {281--288},
	file = {JSTOR Full Text PDF:/Users/elisedumas/Zotero/storage/S3GJE6FJ/Anderson - 1991 - A Nonproportional Hazards Weibull Accelerated Fail.pdf:application/pdf},
}

@article{wilson_analysis_1994,
	title = {The analysis of survival (mortality) data: fitting {Gompertz}, {Weibull}, and logistic functions},
	volume = {74},
	issn = {0047-6374},
	shorttitle = {The analysis of survival (mortality) data},
	doi = {10.1016/0047-6374(94)90095-7},
	abstract = {Survival functions are fitted to survival data from several large populations. The Gompertz survival function corresponds to exponential mortality rate increases with time. The Weibull survival function corresponds to mortality rates that increase as a power function of time. A two-parameter, logistic survival function is introduced, and corresponds to mortality rates that increase, and then decrease, with time. A three-parameter logistic-mortality function also is examined. It reflects mortality rates that rise, and then plateau, with age. Data are from published studies of medflies, Drosophila, house flies, flour beetles, and humans. Some survival data are better fit by a logistic survival function than by the more traditionally used Gompertz or Weibull functions. Gompertz, Weibull, or logistic survival functions often fit the survival of 95+\% of a population, and the 'tails' of the survival curves usually appear to fall between the values predicted by the three functions. For some populations, such 'tails' appear to be too complex to be fit well by any simple function. Survival data for males and females in some populations are best fit by different functions. Populations of 100 or more are needed to distinguish among the functions. When testing effects of environmental or genetic manipulations on survival, it has been common to determine the changes in parameter values for a given function, such as Gompertz. It may be equally important to determine whether the best-fit function has changed as well.},
	language = {eng},
	number = {1-2},
	journal = {Mechanisms of Ageing and Development},
	author = {Wilson, D. L.},
	month = may,
	year = {1994},
	pmid = {7934205},
	keywords = {Female, Humans, Male, Animals, Logistic Models, Survival Analysis, Mortality, Life Tables, Coleoptera, Diptera, Drosophila},
	pages = {15--33},
}

@article{schuster_bidirectional_1997,
	title = {Bidirectional recurrent neural networks},
	volume = {45},
	issn = {1941-0476},
	doi = {10.1109/78.650093},
	abstract = {In the first part of this paper, a regular recurrent neural network (RNN) is extended to a bidirectional recurrent neural network (BRNN). The BRNN can be trained without the limitation of using input information just up to a preset future frame. This is accomplished by training it simultaneously in positive and negative time direction. Structure and training procedure of the proposed network are explained. In regression and classification experiments on artificial data, the proposed structure gives better results than other approaches. For real data, classification experiments for phonemes from the TIMIT database show the same tendency. In the second part of this paper, it is shown how the proposed bidirectional structure can be easily modified to allow efficient estimation of the conditional posterior probability of complete symbol sequences without making any explicit assumption about the shape of the distribution. For this part, experiments on real data are reported.},
	number = {11},
	journal = {IEEE Transactions on Signal Processing},
	author = {Schuster, M. and Paliwal, K.K.},
	month = nov,
	year = {1997},
	note = {Conference Name: IEEE Transactions on Signal Processing},
	keywords = {Artificial neural networks, Control systems, Databases, Parameter estimation, Probability, Recurrent neural networks, Shape, Speech recognition, Telecommunication control, Training data},
	pages = {2673--2681},
	file = {IEEE Xplore Abstract Record:/Users/elisedumas/Zotero/storage/KRFNRYVT/650093.html:text/html;Submitted Version:/Users/elisedumas/Zotero/storage/G3J8R75V/Schuster and Paliwal - 1997 - Bidirectional recurrent neural networks.pdf:application/pdf},
}

@misc{dubois_effective_2018,
	title = {Effective {Representations} of {Clinical} {Notes}},
	url = {http://arxiv.org/abs/1705.07025},
	abstract = {Clinical notes are a rich source of information about patient state. However, using them to predict clinical events with machine learning models is challenging. They are very high dimensional, sparse and have complex structure. Furthermore, training data is often scarce because it is expensive to obtain reliable labels for many clinical events. These difficulties have traditionally been addressed by manual feature engineering encoding task specific domain knowledge. We explored the use of neural networks and transfer learning to learn representations of clinical notes that are useful for predicting future clinical events of interest, such as all causes mortality, inpatient admissions, and emergency room visits. Our data comprised 2.7 million notes and 115 thousand patients at Stanford Hospital. We used the learned representations, along with commonly used bag of words and topic model representations, as features for predictive models of clinical events. We evaluated the effectiveness of these representations with respect to the performance of the models trained on small datasets. Models using the neural network derived representations performed significantly better than models using the baseline representations with small (\$N {\textless} 1000\$) training datasets. The learned representations offer significant performance gains over commonly used baseline representations for a range of predictive modeling tasks and cohort sizes, offering an effective alternative to task specific feature engineering when plentiful labeled training data is not available.},
	urldate = {2022-08-08},
	publisher = {arXiv},
	author = {Dubois, Sebastien and Romano, Nathanael and Kale, David C. and Shah, Nigam and Jung, Kenneth},
	month = aug,
	year = {2018},
	note = {arXiv:1705.07025 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/elisedumas/Zotero/storage/FU9UKBX3/Dubois et al. - 2018 - Effective Representations of Clinical Notes.pdf:application/pdf;arXiv.org Snapshot:/Users/elisedumas/Zotero/storage/97NK2NGI/1705.html:text/html},
}

@article{brentnall_use_2018,
	title = {Use of the concordance index for predictors of censored survival data},
	volume = {27},
	issn = {0962-2802},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6041741/},
	doi = {10.1177/0962280216680245},
	abstract = {The concordance index is often used to measure how well a biomarker predicts the time to an event. Estimators of the concordance index for predictors of right-censored data are reviewed, including those based on censored pairs, inverse probability weighting and a proportional-hazards model. Predictive and prognostic biomarkers often lose strength with time, and in this case the aforementioned statistics depend on the length of follow up. A semi-parametric estimator of the concordance index is developed that accommodates converging hazards through a single parameter in a Pareto model. Concordance index estimators are assessed through simulations, which demonstrate substantial bias of classical censored-pairs and proportional-hazards model estimators. Prognostic biomarkers in a cohort of women diagnosed with breast cancer are evaluated using new and classical estimators of the concordance index.},
	number = {8},
	urldate = {2022-08-11},
	journal = {Statistical Methods in Medical Research},
	author = {Brentnall, Adam R and Cuzick, Jack},
	month = aug,
	year = {2018},
	pmid = {27920368},
	pmcid = {PMC6041741},
	pages = {2359--2373},
	file = {Full Text:/Users/elisedumas/Zotero/storage/XR4QH4QW/Brentnall and Cuzick - 2018 - Use of the concordance index for predictors of cen.pdf:application/pdf},
}

@article{harrell_multivariable_1996,
	title = {Multivariable prognostic models: issues in developing models, evaluating assumptions and adequacy, and measuring and reducing errors},
	volume = {15},
	issn = {0277-6715},
	shorttitle = {Multivariable prognostic models},
	doi = {10.1002/(SICI)1097-0258(19960229)15:4<361::AID-SIM168>3.0.CO;2-4},
	abstract = {Multivariable regression models are powerful tools that are used frequently in studies of clinical outcomes. These models can use a mixture of categorical and continuous variables and can handle partially observed (censored) responses. However, uncritical application of modelling techniques can result in models that poorly fit the dataset at hand, or, even more likely, inaccurately predict outcomes on new subjects. One must know how to measure qualities of a model's fit in order to avoid poorly fitted or overfitted models. Measurement of predictive accuracy can be difficult for survival time data in the presence of censoring. We discuss an easily interpretable index of predictive discrimination as well as methods for assessing calibration of predicted survival probabilities. Both types of predictive accuracy should be unbiasedly validated using bootstrapping or cross-validation, before using predictions in a new data series. We discuss some of the hazards of poorly fitted and overfitted regression models and present one modelling strategy that avoids many of the problems discussed. The methods described are applicable to all regression models, but are particularly needed for binary, ordinal, and time-to-event outcomes. Methods are illustrated with a survival analysis in prostate cancer using Cox regression.},
	language = {eng},
	number = {4},
	journal = {Statistics in Medicine},
	author = {Harrell, F. E. and Lee, K. L. and Mark, D. B.},
	month = feb,
	year = {1996},
	pmid = {8668867},
	keywords = {Data Interpretation, Statistical, Humans, Male, Models, Statistical, Regression Analysis, Software, Computer Simulation, Multivariate Analysis, Survival Analysis, Treatment Outcome, Linear Models, Prostatic Neoplasms, Clinical Trials as Topic, Computer Graphics, Discriminant Analysis, Mathematical Computing},
	pages = {361--387},
}

@article{early_breast_cancer_trialists_collaborative_group_ebctcg_relevance_2011,
	title = {Relevance of breast cancer hormone receptors and other factors to the efficacy of adjuvant tamoxifen: patient-level meta-analysis of randomised trials},
	volume = {378},
	issn = {1474-547X},
	shorttitle = {Relevance of breast cancer hormone receptors and other factors to the efficacy of adjuvant tamoxifen},
	doi = {10.1016/S0140-6736(11)60993-8},
	abstract = {BACKGROUND: As trials of 5 years of tamoxifen in early breast cancer mature, the relevance of hormone receptor measurements (and other patient characteristics) to long-term outcome can be assessed increasingly reliably. We report updated meta-analyses of the trials of 5 years of adjuvant tamoxifen.
METHODS: We undertook a collaborative meta-analysis of individual patient data from 20 trials (n=21,457) in early breast cancer of about 5 years of tamoxifen versus no adjuvant tamoxifen, with about 80\% compliance. Recurrence and death rate ratios (RRs) were from log-rank analyses by allocated treatment.
FINDINGS: In oestrogen receptor (ER)-positive disease (n=10,645), allocation to about 5 years of tamoxifen substantially reduced recurrence rates throughout the first 10 years (RR 0·53 [SE 0·03] during years 0-4 and RR 0·68 [0·06] during years 5-9 [both 2p{\textless}0·00001]; but RR 0·97 [0·10] during years 10-14, suggesting no further gain or loss after year 10). Even in marginally ER-positive disease (10-19 fmol/mg cytosol protein) the recurrence reduction was substantial (RR 0·67 [0·08]). In ER-positive disease, the RR was approximately independent of progesterone receptor status (or level), age, nodal status, or use of chemotherapy. Breast cancer mortality was reduced by about a third throughout the first 15 years (RR 0·71 [0·05] during years 0-4, 0·66 [0·05] during years 5-9, and 0·68 [0·08] during years 10-14; p{\textless}0·0001 for extra mortality reduction during each separate time period). Overall non-breast-cancer mortality was little affected, despite small absolute increases in thromboembolic and uterine cancer mortality (both only in women older than 55 years), so all-cause mortality was substantially reduced. In ER-negative disease, tamoxifen had little or no effect on breast cancer recurrence or mortality.
INTERPRETATION: 5 years of adjuvant tamoxifen safely reduces 15-year risks of breast cancer recurrence and death. ER status was the only recorded factor importantly predictive of the proportional reductions. Hence, the absolute risk reductions produced by tamoxifen depend on the absolute breast cancer risks (after any chemotherapy) without tamoxifen.
FUNDING: Cancer Research UK, British Heart Foundation, and Medical Research Council.},
	language = {eng},
	number = {9793},
	journal = {Lancet (London, England)},
	author = {{Early Breast Cancer Trialists' Collaborative Group (EBCTCG)} and Davies, C. and Godwin, J. and Gray, R. and Clarke, M. and Cutter, D. and Darby, S. and McGale, P. and Pan, H. C. and Taylor, C. and Wang, Y. C. and Dowsett, M. and Ingle, J. and Peto, R.},
	month = aug,
	year = {2011},
	pmid = {21802721},
	pmcid = {PMC3163848},
	keywords = {Female, Humans, Breast Neoplasms, Neoplasm Recurrence, Local, Chemotherapy, Adjuvant, Neoplasms, Second Primary, Receptors, Estrogen, Receptors, Progesterone, Randomized Controlled Trials as Topic, Antineoplastic Agents, Hormonal, Tamoxifen, Selective Estrogen Receptor Modulators},
	pages = {771--784},
	file = {Accepted Version:/Users/elisedumas/Zotero/storage/7NRN294B/Early Breast Cancer Trialists' Collaborative Group (EBCTCG) et al. - 2011 - Relevance of breast cancer hormone receptors and o.pdf:application/pdf},
}

@article{barretto_linking_2003,
	title = {Linking {Guidelines} to {Electronic} {Health} {Record} {Design} for {Improved} {Chronic} {Disease} {Management}},
	volume = {2003},
	issn = {1942-597X},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC1480104/},
	abstract = {The promise of electronic decision support to promote evidence based practice remains elusive in the context of chronic disease management. We examine the problem of achieving a close relationship of Electronic Health Record (EHR) content to other components of a clinical information system (guidelines, decision support and work-flow), particularly linking the decisions made by providers back to the guidelines. We use the openEHR architecture, which allows extension of a core Reference Model via Archetypes to refine the detailed information recording options for specific classes of encounter. We illustrate the use of openEHR for tracking the relationship of a series of clinical encounters to a guideline via a case study of guideline-compliant treatment of hypertension in diabetes. This case study shows the contribution guideline content can have on problem-specific EHR structure and demonstrates the potential for a constructive interaction of electronic decision support and the EHR.},
	urldate = {2022-08-30},
	journal = {AMIA Annual Symposium Proceedings},
	author = {Barretto, Sistine A. and Warren, Jim and Goodchild, Andrew and Bird, Linda and Heard, Sam and Stumptner, Markus},
	year = {2003},
	pmid = {14728135},
	pmcid = {PMC1480104},
	pages = {66--70},
	file = {PubMed Central Full Text PDF:/Users/elisedumas/Zotero/storage/IYK67XIH/Barretto et al. - 2003 - Linking Guidelines to Electronic Health Record Des.pdf:application/pdf},
}

@article{graf_assessment_1999,
	title = {Assessment and comparison of prognostic classification schemes for survival data},
	volume = {18},
	issn = {0277-6715},
	doi = {10.1002/(sici)1097-0258(19990915/30)18:17/18<2529::aid-sim274>3.0.co;2-5},
	abstract = {Prognostic classification schemes have often been used in medical applications, but rarely subjected to a rigorous examination of their adequacy. For survival data, the statistical methodology to assess such schemes consists mainly of a range of ad hoc approaches, and there is an alarming lack of commonly accepted standards in this field. We review these methods and develop measures of inaccuracy which may be calculated in a validation study in order to assess the usefulness of estimated patient-specific survival probabilities associated with a prognostic classification scheme. These measures are meaningful even when the estimated probabilities are misspecified, and asymptotically they are not affected by random censorship. In addition, they can be used to derive R(2)-type measures of explained residual variation. A breast cancer study will serve for illustration throughout the paper.},
	language = {eng},
	number = {17-18},
	journal = {Statistics in Medicine},
	author = {Graf, E. and Schmoor, C. and Sauerbrei, W. and Schumacher, M.},
	month = sep,
	year = {1999},
	pmid = {10474158},
	keywords = {Data Interpretation, Statistical, Female, Humans, Breast Neoplasms, Prognosis, Disease-Free Survival, Proportional Hazards Models, Decision Making},
	pages = {2529--2545},
}

@book{goodfellow_deep_2016,
	title = {Deep {Learning}},
	publisher = {MIT Press},
	author = {Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron},
	year = {2016},
}

@misc{jeanselme_deepjoint_2022,
	title = {{DeepJoint}: {Robust} {Survival} {Modelling} {Under} {Clinical} {Presence} {Shift}},
	shorttitle = {{DeepJoint}},
	url = {http://arxiv.org/abs/2205.13481},
	doi = {10.48550/arXiv.2205.13481},
	abstract = {Observational data in medicine arise as a result of the complex interaction between patients and the healthcare system. The sampling process is often highly irregular and itself constitutes an informative process. When using such data to develop prediction models, this phenomenon is often ignored, leading to sub-optimal performance and generalisability of models when practices evolve. We propose a multi-task recurrent neural network which models three clinical presence dimensions -- namely the longitudinal, the inter-observation and the missingness processes -- in parallel to the survival outcome. On a prediction task using MIMIC III laboratory tests, explicit modelling of these three processes showed improved performance in comparison to state-of-the-art predictive models (C-index at 1 day horizon: 0.878). More importantly, the proposed approach was more robust to change in the clinical presence setting, demonstrated by performance comparison between patients admitted on weekdays and weekends. This analysis demonstrates the importance of studying and leveraging clinical presence to improve performance and create more transportable clinical models.},
	urldate = {2022-10-02},
	publisher = {arXiv},
	author = {Jeanselme, Vincent and Martin, Glen and Peek, Niels and Sperrin, Matthew and Tom, Brian and Barrett, Jessica},
	month = may,
	year = {2022},
	note = {arXiv:2205.13481 [cs]},
	keywords = {Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/elisedumas/Zotero/storage/A83TG423/Jeanselme et al. - 2022 - DeepJoint Robust Survival Modelling Under Clinica.pdf:application/pdf;arXiv.org Snapshot:/Users/elisedumas/Zotero/storage/MWEHKL8X/2205.html:text/html},
}

@article{weerakody_review_2021,
	title = {A review of irregular time series data handling with gated recurrent neural networks},
	volume = {441},
	issn = {0925-2312},
	url = {https://www.sciencedirect.com/science/article/pii/S0925231221003003},
	doi = {10.1016/j.neucom.2021.02.046},
	abstract = {Irregular time series data is becoming increasingly prevalent with the growth of multi-sensor systems as well as the continued use of unstructured manual data recording mechanisms. Irregular data and the resulting missing values severely limit the data's ability to be analysed and modelled for classification and forecasting tasks. Often, conventional methods used for handling time series data introduce bias and make strong assumptions on the underlying data generation process, which can lead to poor model predictions. Traditional machine learning and deep learning methods, although at the forefront of data modelling, are at best compromised by irregular time series data sets and fail to model the temporal irregularity of incomplete time series. Gated recurrent neural networks (RNN), such as LSTM and GRU, have had outstanding success in sequential modelling, and have been applied in many application fields, including natural language processing. These models have become an obvious choice for time series modelling and a promising tool for handling irregular time series data. RNNs have a unique ability to be adapted to make effective use of missing value patterns, time intervals and complex temporal dependencies in irregular univariate and multivariate time series data. In this paper, we provide a systematic review of recent studies in which gated recurrent neural networks have been successfully applied to irregular time series data for prediction tasks within several fields, including medical, human activity recognition, traffic monitoring and environmental monitoring. The review highlights the two common approaches for handling irregular time series data: missing value imputation at the data pre-processing stage and modification of algorithms to directly handle missing values in the learning process. Reviewed models are confined to those that can address issues with irregular time series data and does not cover the broader range of models that deal more generally with sequences and regular time series. This paper aims to present the most effective techniques emerging within this branch of research as well as to identify remaining challenges, so that researchers may build upon this platform of work towards further novel techniques for handling irregular time series data.},
	language = {en},
	urldate = {2022-10-02},
	journal = {Neurocomputing},
	author = {Weerakody, Philip B. and Wong, Kok Wai and Wang, Guanjin and Ela, Wendell},
	month = jun,
	year = {2021},
	keywords = {Imputation methods, Irregular time series, Missing data, Recurrent neural networks},
	pages = {161--178},
	file = {ScienceDirect Snapshot:/Users/elisedumas/Zotero/storage/IYP8MDBU/S0925231221003003.html:text/html;Weerakody et al. - 2021 - A review of irregular time series data handling wi.pdf:/Users/elisedumas/Zotero/storage/2JKUMVF2/Weerakody et al. - 2021 - A review of irregular time series data handling wi.pdf:application/pdf},
}

@misc{mozer_discrete_2017,
	title = {Discrete {Event}, {Continuous} {Time} {RNNs}},
	url = {http://arxiv.org/abs/1710.04110},
	doi = {10.48550/arXiv.1710.04110},
	abstract = {We investigate recurrent neural network architectures for event-sequence processing. Event sequences, characterized by discrete observations stamped with continuous-valued times of occurrence, are challenging due to the potentially wide dynamic range of relevant time scales as well as interactions between time scales. We describe four forms of inductive bias that should benefit architectures for event sequences: temporal locality, position and scale homogeneity, and scale interdependence. We extend the popular gated recurrent unit (GRU) architecture to incorporate these biases via intrinsic temporal dynamics, obtaining a continuous-time GRU. The CT-GRU arises by interpreting the gates of a GRU as selecting a time scale of memory, and the CT-GRU generalizes the GRU by incorporating multiple time scales of memory and performing context-dependent selection of time scales for information storage and retrieval. Event time-stamps drive decay dynamics of the CT-GRU, whereas they serve as generic additional inputs to the GRU. Despite the very different manner in which the two models consider time, their performance on eleven data sets we examined is essentially identical. Our surprising results point both to the robustness of GRU and LSTM architectures for handling continuous time, and to the potency of incorporating continuous dynamics into neural architectures.},
	urldate = {2022-10-02},
	publisher = {arXiv},
	author = {Mozer, Michael C. and Kazakov, Denis and Lindsey, Robert V.},
	month = oct,
	year = {2017},
	note = {arXiv:1710.04110 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, I.2.6},
	file = {arXiv Fulltext PDF:/Users/elisedumas/Zotero/storage/T6VRLKNS/Mozer et al. - 2017 - Discrete Event, Continuous Time RNNs.pdf:application/pdf;arXiv.org Snapshot:/Users/elisedumas/Zotero/storage/TBMLZSLC/1710.html:text/html},
}

@article{guo_evaluation_2022,
	title = {Evaluation of domain generalization and adaptation on improving model robustness to temporal dataset shift in clinical medicine},
	volume = {12},
	copyright = {2022 The Author(s)},
	issn = {2045-2322},
	url = {https://www.nature.com/articles/s41598-022-06484-1},
	doi = {10.1038/s41598-022-06484-1},
	abstract = {Temporal dataset shift associated with changes in healthcare over time is a barrier to deploying machine learning-based clinical decision support systems. Algorithms that learn robust models by estimating invariant properties across time periods for domain generalization (DG) and unsupervised domain adaptation (UDA) might be suitable to proactively mitigate dataset shift. The objective was to characterize the impact of temporal dataset shift on clinical prediction models and benchmark DG and UDA algorithms on improving model robustness. In this cohort study, intensive care unit patients from the MIMIC-IV database were categorized by year groups (2008–2010, 2011–2013, 2014–2016 and 2017–2019).Tasks were predicting mortality, long length of stay, sepsis and invasive ventilation. Feedforward neural networks were used as prediction models. The baseline experiment trained models using empirical risk minimization (ERM) on 2008–2010 (ERM[08–10]) and evaluated them on subsequent year groups. DG experiment trained models using algorithms that estimated invariant properties using 2008–2016 and evaluated them on 2017–2019. UDA experiment leveraged unlabelled samples from 2017 to 2019 for unsupervised distribution matching. DG and UDA models were compared to ERM[08–16] models trained using 2008–2016. Main performance measures were area-under-the-receiver-operating-characteristic curve (AUROC), area-under-the-precision-recall curve and absolute calibration error. Threshold-based metrics including false-positives and false-negatives were used to assess the clinical impact of temporal dataset shift and its mitigation strategies. In the baseline experiments, dataset shift was most evident for sepsis prediction (maximum AUROC drop, 0.090; 95\% confidence interval (CI), 0.080–0.101). Considering a scenario of 100 consecutively admitted patients showed that ERM[08–10] applied to 2017–2019 was associated with one additional false-negative among 11 patients with sepsis, when compared to the model applied to 2008–2010. When compared with ERM[08–16], DG and UDA experiments failed to produce more robust models (range of AUROC difference, − 0.003 to 0.050).  In conclusion, DG and UDA failed to produce more robust models compared to ERM in the setting of temporal dataset shift. Alternate approaches are required to preserve model performance over time in clinical medicine.},
	language = {en},
	number = {1},
	urldate = {2022-10-02},
	journal = {Scientific Reports},
	author = {Guo, Lin Lawrence and Pfohl, Stephen R. and Fries, Jason and Johnson, Alistair E. W. and Posada, Jose and Aftandilian, Catherine and Shah, Nigam and Sung, Lillian},
	month = feb,
	year = {2022},
	note = {Number: 1
Publisher: Nature Publishing Group},
	keywords = {Computer science, Data processing, Health care, Machine learning, Prognosis},
	pages = {2726},
	file = {Full Text PDF:/Users/elisedumas/Zotero/storage/EXTMJ7H4/Guo et al. - 2022 - Evaluation of domain generalization and adaptation.pdf:application/pdf;Snapshot:/Users/elisedumas/Zotero/storage/34I6ANPY/s41598-022-06484-1.html:text/html},
}

@misc{nestor_feature_2019,
	title = {Feature {Robustness} in {Non}-stationary {Health} {Records}: {Caveats} to {Deployable} {Model} {Performance} in {Common} {Clinical} {Machine} {Learning} {Tasks}},
	shorttitle = {Feature {Robustness} in {Non}-stationary {Health} {Records}},
	url = {http://arxiv.org/abs/1908.00690},
	doi = {10.48550/arXiv.1908.00690},
	abstract = {When training clinical prediction models from electronic health records (EHRs), a key concern should be a model's ability to sustain performance over time when deployed, even as care practices, database systems, and population demographics evolve. Due to de-identification requirements, however, current experimental practices for public EHR benchmarks (such as the MIMIC-III critical care dataset) are time agnostic, assigning care records to train or test sets without regard for the actual dates of care. As a result, current benchmarks cannot assess how well models trained on one year generalise to another. In this work, we obtain a Limited Data Use Agreement to access year of care for each record in MIMIC and show that all tested state-of-the-art models decay in prediction quality when trained on historical data and tested on future data, particularly in response to a system-wide record-keeping change in 2008 (0.29 drop in AUROC for mortality prediction, 0.10 drop in AUROC for length-of-stay prediction with a random forest classifier). We further develop a simple yet effective mitigation strategy: by aggregating raw features into expert-defined clinical concepts, we see only a 0.06 drop in AUROC for mortality prediction and a 0.03 drop in AUROC for length-of-stay prediction. We demonstrate that this aggregation strategy outperforms other automatic feature preprocessing techniques aimed at increasing robustness to data drift. We release our aggregated representations and code to encourage more deployable clinical prediction models.},
	urldate = {2022-10-02},
	publisher = {arXiv},
	author = {Nestor, Bret and McDermott, Matthew B. A. and Boag, Willie and Berner, Gabriela and Naumann, Tristan and Hughes, Michael C. and Goldenberg, Anna and Ghassemi, Marzyeh},
	month = aug,
	year = {2019},
	note = {arXiv:1908.00690 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/elisedumas/Zotero/storage/FBFHNW6A/Nestor et al. - 2019 - Feature Robustness in Non-stationary Health Record.pdf:application/pdf;arXiv.org Snapshot:/Users/elisedumas/Zotero/storage/JYLLL7DT/1908.html:text/html},
}

@misc{xiao_modeling_2017,
	title = {Modeling {The} {Intensity} {Function} {Of} {Point} {Process} {Via} {Recurrent} {Neural} {Networks}},
	url = {http://arxiv.org/abs/1705.08982},
	doi = {10.48550/arXiv.1705.08982},
	abstract = {Event sequence, asynchronously generated with random timestamp, is ubiquitous among applications. The precise and arbitrary timestamp can carry important clues about the underlying dynamics, and has lent the event data fundamentally different from the time-series whereby series is indexed with fixed and equal time interval. One expressive mathematical tool for modeling event is point process. The intensity functions of many point processes involve two components: the background and the effect by the history. Due to its inherent spontaneousness, the background can be treated as a time series while the other need to handle the history events. In this paper, we model the background by a Recurrent Neural Network (RNN) with its units aligned with time series indexes while the history effect is modeled by another RNN whose units are aligned with asynchronous events to capture the long-range dynamics. The whole model with event type and timestamp prediction output layers can be trained end-to-end. Our approach takes an RNN perspective to point process, and models its background and history effect. For utility, our method allows a black-box treatment for modeling the intensity which is often a pre-defined parametric form in point processes. Meanwhile end-to-end training opens the venue for reusing existing rich techniques in deep network for point process modeling. We apply our model to the predictive maintenance problem using a log dataset by more than 1000 ATMs from a global bank headquartered in North America.},
	urldate = {2022-10-02},
	publisher = {arXiv},
	author = {Xiao, Shuai and Yan, Junchi and Chu, Stephen M. and Yang, Xiaokang and Zha, Hongyuan},
	month = may,
	year = {2017},
	note = {arXiv:1705.08982 [cs, stat]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/elisedumas/Zotero/storage/Q7SN294P/Xiao et al. - 2017 - Modeling The Intensity Function Of Point Process V.pdf:application/pdf;arXiv.org Snapshot:/Users/elisedumas/Zotero/storage/SLC74AGC/1705.html:text/html},
}

@misc{chen_neural_2019,
	title = {Neural {Ordinary} {Differential} {Equations}},
	url = {http://arxiv.org/abs/1806.07366},
	doi = {10.48550/arXiv.1806.07366},
	abstract = {We introduce a new family of deep neural network models. Instead of specifying a discrete sequence of hidden layers, we parameterize the derivative of the hidden state using a neural network. The output of the network is computed using a black-box differential equation solver. These continuous-depth models have constant memory cost, adapt their evaluation strategy to each input, and can explicitly trade numerical precision for speed. We demonstrate these properties in continuous-depth residual networks and continuous-time latent variable models. We also construct continuous normalizing flows, a generative model that can train by maximum likelihood, without partitioning or ordering the data dimensions. For training, we show how to scalably backpropagate through any ODE solver, without access to its internal operations. This allows end-to-end training of ODEs within larger models.},
	urldate = {2022-10-02},
	publisher = {arXiv},
	author = {Chen, Ricky T. Q. and Rubanova, Yulia and Bettencourt, Jesse and Duvenaud, David},
	month = dec,
	year = {2019},
	note = {arXiv:1806.07366 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Artificial Intelligence},
	file = {arXiv Fulltext PDF:/Users/elisedumas/Zotero/storage/VQVNVCKY/Chen et al. - 2019 - Neural Ordinary Differential Equations.pdf:application/pdf;arXiv.org Snapshot:/Users/elisedumas/Zotero/storage/P2GIEAFF/1806.html:text/html},
}

@article{jarrett_dynamic_2020,
	title = {Dynamic {Prediction} in {Clinical} {Survival} {Analysis} {Using} {Temporal} {Convolutional} {Networks}},
	volume = {24},
	issn = {2168-2208},
	doi = {10.1109/JBHI.2019.2929264},
	abstract = {Accurate prediction of disease trajectories is critical for early identification and timely treatment of patients at risk. Conventional methods in survival analysis are often constrained by strong parametric assumptions and limited in their ability to learn from high-dimensional data. This paper develops a novel convolutional approach that addresses the drawbacks of both traditional statistical approaches as well as recent neural network models for survival. We present Match-Net: a missingness-aware temporal convolutional hitting-time network, designed to capture temporal dependencies and heterogeneous interactions in covariate trajectories and patterns of missingness. To the best of our knowledge, this is the first investigation of temporal convolutions in the context of dynamic prediction for personalized risk prognosis. Using real-world data from the Alzheimer's disease neuroimaging initiative, we demonstrate state-of-the-art performance without making any assumptions regarding underlying longitudinal or time-to-event processes-attesting to the model's potential utility in clinical decision support.},
	language = {eng},
	number = {2},
	journal = {IEEE journal of biomedical and health informatics},
	author = {Jarrett, Daniel and Yoon, Jinsung and van der Schaar, Mihaela},
	month = feb,
	year = {2020},
	pmid = {31331898},
	keywords = {Algorithms, Alzheimer Disease, Humans, Neural Networks, Computer, Survival Analysis},
	pages = {424--436},
	file = {Jarrett et al. - 2020 - Dynamic Prediction in Clinical Survival Analysis U.pdf:/Users/elisedumas/Zotero/storage/M8UA53NB/Jarrett et al. - 2020 - Dynamic Prediction in Clinical Survival Analysis U.pdf:application/pdf},
}

@misc{rubanova_latent_2019,
	title = {Latent {ODEs} for {Irregularly}-{Sampled} {Time} {Series}},
	url = {http://arxiv.org/abs/1907.03907},
	doi = {10.48550/arXiv.1907.03907},
	abstract = {Time series with non-uniform intervals occur in many applications, and are difficult to model using standard recurrent neural networks (RNNs). We generalize RNNs to have continuous-time hidden dynamics defined by ordinary differential equations (ODEs), a model we call ODE-RNNs. Furthermore, we use ODE-RNNs to replace the recognition network of the recently-proposed Latent ODE model. Both ODE-RNNs and Latent ODEs can naturally handle arbitrary time gaps between observations, and can explicitly model the probability of observation times using Poisson processes. We show experimentally that these ODE-based models outperform their RNN-based counterparts on irregularly-sampled data.},
	urldate = {2022-10-02},
	publisher = {arXiv},
	author = {Rubanova, Yulia and Chen, Ricky T. Q. and Duvenaud, David},
	month = jul,
	year = {2019},
	note = {arXiv:1907.03907 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/elisedumas/Zotero/storage/N5J5A3ZV/Rubanova et al. - 2019 - Latent ODEs for Irregularly-Sampled Time Series.pdf:application/pdf;arXiv.org Snapshot:/Users/elisedumas/Zotero/storage/AC8ZUUCK/1907.html:text/html},
}

@article{van_houwelingen_dynamic_2007,
	title = {Dynamic {Prediction} by {Landmarking} in {Event} {History} {Analysis}},
	volume = {34},
	issn = {1467-9469},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1467-9469.2006.00529.x},
	doi = {10.1111/j.1467-9469.2006.00529.x},
	abstract = {Abstract. This article advocates the landmarking approach that dynamically adjusts predictive models for survival data during the follow up. This updating is achieved by directly fitting models for the individuals still at risk at the landmark point. Using this approach, simple proportional hazards models are able to catch the development over time for models with time-varying effects of covariates or data with time-dependent covariates (biomarkers). To smooth the effect of the landmarking, sequences of models are considered with parametric effects of the landmark time point and fitted by maximizing appropriate pseudo log-likelihoods that extend the partial log-likelihood to cover the landmarking approach.},
	language = {en},
	number = {1},
	urldate = {2022-10-02},
	journal = {Scandinavian Journal of Statistics},
	author = {Van Houwelingen, Hans C.},
	year = {2007},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/j.1467-9469.2006.00529.x},
	keywords = {landmark analysis, landmarking, pseudo-partial likelihood, survival analysis, time-dependent covariates, time-varying effects},
	pages = {70--85},
	file = {Snapshot:/Users/elisedumas/Zotero/storage/I9BTV2S4/j.1467-9469.2006.00529.html:text/html;Van Houwelingen - 2007 - Dynamic Prediction by Landmarking in Event History.pdf:/Users/elisedumas/Zotero/storage/63SLVKPV/Van Houwelingen - 2007 - Dynamic Prediction by Landmarking in Event History.pdf:application/pdf},
}

@misc{lipton_modeling_2016,
	title = {Modeling {Missing} {Data} in {Clinical} {Time} {Series} with {RNNs}},
	url = {http://arxiv.org/abs/1606.04130},
	doi = {10.48550/arXiv.1606.04130},
	abstract = {We demonstrate a simple strategy to cope with missing data in sequential inputs, addressing the task of multilabel classification of diagnoses given clinical time series. Collected from the pediatric intensive care unit (PICU) at Children's Hospital Los Angeles, our data consists of multivariate time series of observations. The measurements are irregularly spaced, leading to missingness patterns in temporally discretized sequences. While these artifacts are typically handled by imputation, we achieve superior predictive performance by treating the artifacts as features. Unlike linear models, recurrent neural networks can realize this improvement using only simple binary indicators of missingness. For linear models, we show an alternative strategy to capture this signal. Training models on missingness patterns only, we show that for some diseases, what tests are run can be as predictive as the results themselves.},
	urldate = {2022-10-02},
	publisher = {arXiv},
	author = {Lipton, Zachary C. and Kale, David C. and Wetzel, Randall},
	month = nov,
	year = {2016},
	note = {arXiv:1606.04130 [cs, stat]},
	keywords = {Computer Science - Information Retrieval, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/elisedumas/Zotero/storage/N2FRBRFD/Lipton et al. - 2016 - Modeling Missing Data in Clinical Time Series with.pdf:application/pdf;arXiv.org Snapshot:/Users/elisedumas/Zotero/storage/Y6VXCFRB/1606.html:text/html},
}

@article{lee_dynamic-deephit_2020,
	title = {Dynamic-{DeepHit}: {A} {Deep} {Learning} {Approach} for {Dynamic} {Survival} {Analysis} {With} {Competing} {Risks} {Based} on {Longitudinal} {Data}},
	volume = {67},
	issn = {1558-2531},
	shorttitle = {Dynamic-{DeepHit}},
	doi = {10.1109/TBME.2019.2909027},
	abstract = {Currently available risk prediction methods are limited in their ability to deal with complex, heterogeneous, and longitudinal data such as that available in primary care records, or in their ability to deal with multiple competing risks. This paper develops a novel deep learning approach that is able to successfully address current limitations of standard statistical approaches such as landmarking and joint modeling. Our approach, which we call Dynamic-DeepHit, flexibly incorporates the available longitudinal data comprising various repeated measurements (rather than only the last available measurements) in order to issue dynamically updated survival predictions for one or multiple competing risk(s). Dynamic-DeepHit learns the time-to-event distributions without the need to make any assumptions about the underlying stochastic models for the longitudinal and the time-to-event processes. Thus, unlike existing works in statistics, our method is able to learn data-driven associations between the longitudinal data and the various associated risks without underlying model specifications. We demonstrate the power of our approach by applying it to a real-world longitudinal dataset from the U.K. Cystic Fibrosis Registry, which includes a heterogeneous cohort of 5883 adult patients with annual follow-ups between 2009 to 2015. The results show that Dynamic-DeepHit provides a drastic improvement in discriminating individual risks of different forms of failures due to cystic fibrosis. Furthermore, our analysis utilizes post-processing statistics that provide clinical insight by measuring the influence of each covariate on risk predictions and the temporal importance of longitudinal measurements, thereby enabling us to identify covariates that are influential for different competing risks.},
	language = {eng},
	number = {1},
	journal = {IEEE transactions on bio-medical engineering},
	author = {Lee, Changhee and Yoon, Jinsung and Schaar, Mihaela van der},
	month = jan,
	year = {2020},
	pmid = {30951460},
	keywords = {Cystic Fibrosis, Databases, Factual, Deep Learning, Humans, Longitudinal Studies, Survival Analysis},
	pages = {122--133},
}

@article{devaux_individual_2022,
	title = {Individual dynamic prediction of clinical endpoint from large dimensional longitudinal biomarker history: a landmark approach},
	volume = {22},
	issn = {1471-2288},
	shorttitle = {Individual dynamic prediction of clinical endpoint from large dimensional longitudinal biomarker history},
	url = {https://doi.org/10.1186/s12874-022-01660-3},
	doi = {10.1186/s12874-022-01660-3},
	abstract = {The individual data collected throughout patient follow-up constitute crucial information for assessing the risk of a clinical event, and eventually for adapting a therapeutic strategy. Joint models and landmark models have been proposed to compute individual dynamic predictions from repeated measures to one or two markers. However, they hardly extend to the case where the patient history includes much more repeated markers. Our objective was thus to propose a solution for the dynamic prediction of a health event that may exploit repeated measures of a possibly large number of markers.},
	number = {1},
	urldate = {2022-10-02},
	journal = {BMC Medical Research Methodology},
	author = {Devaux, Anthony and Genuer, Robin and Peres, Karine and Proust-Lima, Cécile},
	month = jul,
	year = {2022},
	keywords = {Individual prediction, Landmark, Longitudinal data, Machine learning methods, Survival data},
	pages = {188},
	file = {Full Text PDF:/Users/elisedumas/Zotero/storage/DQEENHTT/Devaux et al. - 2022 - Individual dynamic prediction of clinical endpoint.pdf:application/pdf;Snapshot:/Users/elisedumas/Zotero/storage/DTTNLU8H/s12874-022-01660-3.html:text/html},
}

@article{wang_machine_2019,
	title = {Machine {Learning} for {Survival} {Analysis}: {A} {Survey}},
	volume = {51},
	issn = {0360-0300, 1557-7341},
	shorttitle = {Machine {Learning} for {Survival} {Analysis}},
	url = {https://dl.acm.org/doi/10.1145/3214306},
	doi = {10.1145/3214306},
	abstract = {Survival analysis is a subfield of statistics where the goal is to analyze and model data where the outcome is the time until an event of interest occurs. One of the main challenges in this context is the presence of instances whose event outcomes become unobservable after a certain time point or when some instances do not experience any event during the monitoring period. This so-called
              censoring
              can be handled most effectively using survival analysis techniques. Traditionally, statistical approaches have been widely developed in the literature to overcome the issue of censoring. In addition, many machine learning algorithms have been adapted to deal with such censored data and tackle other challenging problems that arise in real-world data. In this survey, we provide a comprehensive and structured review of the statistical methods typically used and the machine learning techniques developed for survival analysis, along with a detailed taxonomy of the existing methods. We also discuss several topics that are closely related to survival analysis and describe several successful applications in a variety of real-world application domains. We hope that this article will give readers a more comprehensive understanding of recent advances in survival analysis and offer some guidelines for applying these approaches to solve new problems arising in applications involving censored data.},
	language = {en},
	number = {6},
	urldate = {2022-10-02},
	journal = {ACM Computing Surveys},
	author = {Wang, Ping and Li, Yan and Reddy, Chandan K.},
	month = nov,
	year = {2019},
	pages = {1--36},
	file = {Wang et al. - 2019 - Machine Learning for Survival Analysis A Survey.pdf:/Users/elisedumas/Zotero/storage/ZF3XZURZ/Wang et al. - 2019 - Machine Learning for Survival Analysis A Survey.pdf:application/pdf},
}

@article{vecoven_bio-inspired_2021,
	title = {A bio-inspired bistable recurrent cell allows for long-lasting memory},
	volume = {16},
	issn = {1932-6203},
	url = {http://arxiv.org/abs/2006.05252},
	doi = {10.1371/journal.pone.0252676},
	abstract = {Recurrent neural networks (RNNs) provide state-of-the-art performances in a wide variety of tasks that require memory. These performances can often be achieved thanks to gated recurrent cells such as gated recurrent units (GRU) and long short-term memory (LSTM). Standard gated cells share a layer internal state to store information at the network level, and long term memory is shaped by network-wide recurrent connection weights. Biological neurons on the other hand are capable of holding information at the cellular level for an arbitrary long amount of time through a process called bistability. Through bistability, cells can stabilize to different stable states depending on their own past state and inputs, which permits the durable storing of past information in neuron state. In this work, we take inspiration from biological neuron bistability to embed RNNs with long-lasting memory at the cellular level. This leads to the introduction of a new bistable biologically-inspired recurrent cell that is shown to strongly improves RNN performance on time-series which require very long memory, despite using only cellular connections (all recurrent connections are from neurons to themselves, i.e. a neuron state is not influenced by the state of other neurons). Furthermore, equipping this cell with recurrent neuromodulation permits to link them to standard GRU cells, taking a step towards the biological plausibility of GRU.},
	number = {6},
	urldate = {2022-10-02},
	journal = {PLOS ONE},
	author = {Vecoven, Nicolas and Ernst, Damien and Drion, Guillaume},
	month = jun,
	year = {2021},
	note = {arXiv:2006.05252 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing},
	pages = {e0252676},
	file = {arXiv Fulltext PDF:/Users/elisedumas/Zotero/storage/MMLCUEB3/Vecoven et al. - 2021 - A bio-inspired bistable recurrent cell allows for .pdf:application/pdf;arXiv.org Snapshot:/Users/elisedumas/Zotero/storage/G9VRFHRT/2006.html:text/html},
}

@incollection{dagostino_evaluation_2003,
	series = {Advances in {Survival} {Analysis}},
	title = {Evaluation of the {Performance} of {Survival} {Analysis} {Models}: {Discrimination} and {Calibration} {Measures}},
	volume = {23},
	shorttitle = {Evaluation of the {Performance} of {Survival} {Analysis} {Models}},
	url = {https://www.sciencedirect.com/science/article/pii/S0169716103230017},
	abstract = {Prediction functions or Health Risk Appraisal Functions (HRAF) are mathematical models of the probability of an event. This chapter focuses on the evaluation of the performance of an HRAF with regard to its ability to predict the outcome variable. It considers a time to event survival model with censored observations, such as the Cox regression model. The performance of a model with regard to its discrimination and calibration is evaluated. Discrimination refers to a model's ability to correctly distinguish the two classes of outcomes. A model with good discrimination ability produces higher predicted probabilities to subjects who had events than subjects who did not have events. Perfect discrimination would result in two non-overlapping sets of predicted probabilities from the model: one set for the positive outcomes and the other for the negative outcomes. Calibration describes how closely the predicted probabilities agree numerically with the actual outcomes. A model is well calibrated when the predicted and observed values agree for any reasonable grouping of the observation, ordered by increasing predicted values. Calibration measures are often statistics that partition a data set into groups and assess how the average predicted probability compares with the outcome prevalence in each group.},
	language = {en},
	urldate = {2022-10-02},
	booktitle = {Handbook of {Statistics}},
	publisher = {Elsevier},
	author = {D'Agostino, R. B. and Nam, Byung-Ho},
	month = jan,
	year = {2003},
	doi = {10.1016/S0169-7161(03)23001-7},
	pages = {1--25},
	file = {D'Agostino and Nam - 2003 - Evaluation of the Performance of Survival Analysis.pdf:/Users/elisedumas/Zotero/storage/BBTSH8VA/D'Agostino and Nam - 2003 - Evaluation of the Performance of Survival Analysis.pdf:application/pdf;ScienceDirect Snapshot:/Users/elisedumas/Zotero/storage/Y4QUGQHG/S0169716103230017.html:text/html},
}

@article{gasparini_mixed-effects_2020,
	title = {Mixed-effects models for health care longitudinal data with an informative visiting process: {A} {Monte} {Carlo} simulation study},
	volume = {74},
	issn = {1467-9574},
	shorttitle = {Mixed-effects models for health care longitudinal data with an informative visiting process},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/stan.12188},
	doi = {10.1111/stan.12188},
	abstract = {Electronic health records are being increasingly used in medical research to answer more relevant and detailed clinical questions; however, they pose new and significant methodological challenges. For instance, observation times are likely correlated with the underlying disease severity: Patients with worse conditions utilise health care more and may have worse biomarker values recorded. Traditional methods for analysing longitudinal data assume independence between observation times and disease severity; yet, with health care data, such assumptions unlikely hold. Through Monte Carlo simulation, we compare different analytical approaches proposed to account for an informative visiting process to assess whether they lead to unbiased results. Furthermore, we formalise a joint model for the observation process and the longitudinal outcome within an extended joint modelling framework. We illustrate our results using data from a pragmatic trial on enhanced care for individuals with chronic kidney disease, and we introduce user-friendly software that can be used to fit the joint model for the observation process and a longitudinal outcome.},
	language = {en},
	number = {1},
	urldate = {2022-10-03},
	journal = {Statistica Neerlandica},
	author = {Gasparini, Alessandro and Abrams, Keith R. and Barrett, Jessica K. and Major, Rupert W. and Sweeting, Michael J. and Brunskill, Nigel J. and Crowther, Michael J.},
	year = {2020},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/stan.12188},
	keywords = {electronic health records, informative visiting process, inverse intensity of visiting weighting, longitudinal data, mixed-effects models, Monte Carlo simulation, recurrent-events models, selection bias},
	pages = {5--23},
	file = {Gasparini et al. - 2020 - Mixed-effects models for health care longitudinal .pdf:/Users/elisedumas/Zotero/storage/BXTPAUMI/Gasparini et al. - 2020 - Mixed-effects models for health care longitudinal .pdf:application/pdf;Snapshot:/Users/elisedumas/Zotero/storage/5434V8EH/stan.html:text/html},
}

@article{pullenayegum_longitudinal_2016,
	title = {Longitudinal data subject to irregular observation: {A} review of methods with a focus on visit processes, assumptions, and study design},
	volume = {25},
	issn = {1477-0334},
	shorttitle = {Longitudinal data subject to irregular observation},
	doi = {10.1177/0962280214536537},
	abstract = {When data are collected longitudinally, measurement times often vary among patients. This is of particular concern in clinic-based studies, for example retrospective chart reviews. Here, typically no two patients will share the same set of measurement times and moreover, it is likely that the timing of the measurements is associated with disease course; for example, patients may visit more often when unwell. While there are statistical methods that can help overcome the resulting bias, these make assumptions about the nature of the dependence between visit times and outcome processes, and the assumptions differ across methods. The purpose of this paper is to review the methods available with a particular focus on how the assumptions made line up with visit processes encountered in practice. Through this we show that no one method can handle all plausible visit scenarios and suggest that careful analysis of the visit process should inform the choice of analytic method for the outcomes. Moreover, there are some commonly encountered visit scenarios that are not handled well by any method, and we make recommendations with regard to study design that would minimize the chances of these problematic visit scenarios arising.},
	language = {eng},
	number = {6},
	journal = {Statistical Methods in Medical Research},
	author = {Pullenayegum, Eleanor M. and Lim, Lily Sh},
	month = dec,
	year = {2016},
	pmid = {24855119},
	keywords = {Bias, correlated, Dialysis, Female, Humans, informative observation, inverse-intensity weighting, longitudinal data, Longitudinal Studies, observational study, Pregnancy, random effects, Research Design, Retrospective Studies, Weight Gain},
	pages = {2992--3014},
}

@article{neuhaus_analysis_2018,
	title = {Analysis of longitudinal data from outcome-dependent visit processes: {Failure} of proposed methods in realistic settings and potential improvements},
	volume = {37},
	issn = {1097-0258},
	shorttitle = {Analysis of longitudinal data from outcome-dependent visit processes},
	doi = {10.1002/sim.7932},
	abstract = {The timing and frequency of the measurement of longitudinal outcomes in databases may be associated with the value of the outcome. Such visit processes are termed outcome dependent, and previous work showed that conducting standard analyses that ignore outcome-dependent visit times can produce highly biased estimates of the associations of covariates with outcomes. The literature contains several classes of approaches to analyze longitudinal data subject to outcome-dependent visit times, and all of these are based on simplifying assumptions about the visit process. Based on extensive discussions with subject matter investigators, we identified common characteristics of outcome-dependent visit processes that allowed us to evaluate the performance of existing methods in settings with more realistic visit processes than have been previously investigated. This paper uses the analysis of data from a study of kidney function, theory, and simulation studies to examine a range of settings that vary from those where all visits have a low degree of missingness and outcome dependence (which we call "regular" visits) to those where all visits have a high degree of missingness and outcome dependence (which we call "irregular" visits). Our results show that while all the approaches we studied can yield biased estimates of some covariate effects, other covariate effects can be estimated with little bias. In particular, mixed effects models fit by maximum likelihood yielded little bias in estimates of the effects of covariates not associated with the random effects and small bias in estimates of the effects of covariates associated with the random effects. Other approaches produced estimates with greater bias. Our results also show that the presence of some regular visits in the data set protects mixed model analyses from bias but not other methods.},
	language = {eng},
	number = {29},
	journal = {Statistics in Medicine},
	author = {Neuhaus, John M. and McCulloch, Charles E. and Boylan, Ross D.},
	month = dec,
	year = {2018},
	pmid = {30112825},
	keywords = {bias, Bias, Data Interpretation, Statistical, generalized linear mixed model, Glomerular Filtration Rate, Humans, informative visit process, Kidney Transplantation, Likelihood Functions, Longitudinal Studies, Models, Statistical, Renal Dialysis, Renal Insufficiency, Chronic, Time Factors, Treatment Outcome},
	pages = {4457--4471},
}

@article{oh_optimal_2021,
	title = {Optimal treatment recommendations for diabetes patients using the {Markov} decision process along with the {South} {Korean} electronic health records},
	volume = {11},
	copyright = {2021 The Author(s)},
	issn = {2045-2322},
	url = {https://www.nature.com/articles/s41598-021-86419-4},
	doi = {10.1038/s41598-021-86419-4},
	abstract = {The extensive utilization of electronic health records (EHRs) and the growth of enormous open biomedical datasets has readied the area for applications of computational and machine learning techniques to reveal fundamental patterns. This study’s goal is to develop a medical treatment recommendation system using Korean EHRs along with the Markov decision process (MDP). The sharing of EHRs by the National Health Insurance Sharing Service (NHISS) of Korea has made it possible to analyze Koreans’ medical data which include treatments, prescriptions, and medical check-up. After considering the merits and effectiveness of such data, we analyzed patients’ medical information and recommended optimal pharmaceutical prescriptions for diabetes, which is known to be the most burdensome disease for Koreans. We also proposed an MDP-based treatment recommendation system for diabetic patients to help doctors when prescribing diabetes medications. To build the model, we used the 11-year Korean NHISS database. To overcome the challenge of designing an MDP model, we carefully designed the states, actions, reward functions, and transition probability matrices, which were chosen to balance the tradeoffs between reality and the curse of dimensionality issues.},
	language = {en},
	number = {1},
	urldate = {2022-10-03},
	journal = {Scientific Reports},
	author = {Oh, Sang-Ho and Lee, Su Jin and Noh, Juhwan and Mo, Jeonghoon},
	month = mar,
	year = {2021},
	note = {Number: 1
Publisher: Nature Publishing Group},
	keywords = {Diabetes complications, Preventive medicine, Scientific data, Type 2 diabetes},
	pages = {6920},
	file = {Oh et al. - 2021 - Optimal treatment recommendations for diabetes pat.pdf:/Users/elisedumas/Zotero/storage/UGYWRS52/Oh et al. - 2021 - Optimal treatment recommendations for diabetes pat.pdf:application/pdf;Snapshot:/Users/elisedumas/Zotero/storage/R7ZRFBG9/s41598-021-86419-4.html:text/html},
}

@incollection{schaefer_modeling_2004,
	address = {Boston, MA},
	series = {International {Series} in {Operations} {Research} \& {Management} {Science}},
	title = {Modeling {Medical} {Treatment} {Using} {Markov} {Decision} {Processes}},
	isbn = {978-1-4020-8066-1},
	url = {https://doi.org/10.1007/1-4020-8066-2_23},
	abstract = {Medical treatment decisions are often sequential and uncertain. Markov decision processes (MDPs) are an appropriate technique for modeling and solving such stochastic and dynamic decisions. This chapter gives an overview of MDP models and solution techniques. We describe MDP modeling in the context of medical treatment and discuss when MDPs are an appropriate technique. We review selected successful applications of MDPs to treatment decisions in the literature. We conclude with a discussion of the challenges and opportunities for applying MDPs to medical treatment decisions.},
	language = {en},
	urldate = {2022-10-03},
	booktitle = {Operations {Research} and {Health} {Care}: {A} {Handbook} of {Methods} and {Applications}},
	publisher = {Springer US},
	author = {Schaefer, Andrew J. and Bailey, Matthew D. and Shechter, Steven M. and Roberts, Mark S.},
	editor = {Brandeau, Margaret L. and Sainfort, François and Pierskalla, William P.},
	year = {2004},
	doi = {10.1007/1-4020-8066-2_23},
	keywords = {Markov decision processes, Medical decision making, Optimal medical treatment, Stochastic dynamic programs, Stochastic optimal control},
	pages = {593--612},
}

@article{su_risk_2021,
	title = {Risk factor identification in cystic fibrosis by flexible hierarchical joint models},
	volume = {30},
	issn = {1477-0334},
	doi = {10.1177/0962280220950369},
	abstract = {Cystic fibrosis (CF) is a lethal autosomal disease hallmarked by respiratory failure. Maintaining lung function and minimizing frequency of acute respiratory events known as pulmonary exacerbations are essential to survival. Jointly modeling longitudinal lung function and exacerbation occurrences may provide better inference. We propose a shared-parameter joint hierarchical Gaussian process model with flexible link function to investigate the impacts of both demographic and time-varying clinical risk factors on lung function decline and to examine the associations between lung function and occurrence of pulmonary exacerbation. A two-level Gaussian process is used to capture the nonlinear longitudinal trajectory, and a flexible link function is introduced to the joint model in order to analyze binary process. Bayesian model assessment criteria are provided in examining the overall performance in joint models and marginal fitting in each submodel. We conduct simulation studies and apply the proposed model in a local CF center cohort. In the CF application, a nonlinear structure is supported in modeling both the longitudinal continuous and binary processes. A negative association is detected between lung function and pulmonary exacerbation by the joint model. The importance of risk factors, including gender, diagnostic status, insurance status, and BMI, is examined in joint models.},
	language = {eng},
	number = {1},
	journal = {Statistical Methods in Medical Research},
	author = {Su, Weiji and Wang, Xia and Szczesniak, Rhonda D.},
	month = jan,
	year = {2021},
	pmid = {32842919},
	pmcid = {PMC8865017},
	keywords = {Bayes Theorem, Bayesian joint model, Bayesian model assessment, binary process, Computer Simulation, cystic fibrosis, Cystic Fibrosis, flexible link function, Gaussian process, Humans, longitudinal data analysis, Lung, medical monitoring, Risk Factors},
	pages = {244--260},
	file = {Accepted Version:/Users/elisedumas/Zotero/storage/N9CRVZHZ/Su et al. - 2021 - Risk factor identification in cystic fibrosis by f.pdf:application/pdf},
}

@article{lin_deep_2022,
	title = {Deep learning for the dynamic prediction of multivariate longitudinal and survival data},
	volume = {41},
	issn = {1097-0258},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/sim.9392},
	doi = {10.1002/sim.9392},
	abstract = {The joint model for longitudinal and survival data improves time-to-event predictions by including longitudinal outcome variables in addition to baseline covariates. However, in practice, joint models may be limited by parametric assumptions in both the longitudinal and survival submodels. In addition, computational difficulties arise when considering multiple longitudinal outcomes due to the large number of random effects to be integrated out in the full likelihood. In this article, we discuss several recent machine learning methods for incorporating multivariate longitudinal data for time-to-event prediction. The presented methods use functional data analysis or convolutional neural networks to model the longitudinal data, both of which scale well to multiple longitudinal outcomes. In addition, we propose a novel architecture based on the transformer neural network, named TransformerJM, which jointly models longitudinal and time-to-event data. The prognostic abilities of each model are assessed and compared through both simulation and real data analysis on Alzheimer's disease datasets. Specifically, the models were evaluated based on their ability to dynamically update predictions as new longitudinal data becomes available. We showed that TransformerJM improves upon the predictive performance of existing methods across different scenarios.},
	language = {en},
	number = {15},
	urldate = {2022-10-03},
	journal = {Statistics in Medicine},
	author = {Lin, Jeffrey and Luo, Sheng},
	year = {2022},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/sim.9392},
	keywords = {Alzheimer's disease, functional data analysis, joint model, personalized medicine, temporal convolutions, transformer neural network},
	pages = {2894--2907},
	file = {Full Text PDF:/Users/elisedumas/Zotero/storage/TMRHL5D3/Lin and Luo - 2022 - Deep learning for the dynamic prediction of multiv.pdf:application/pdf;Snapshot:/Users/elisedumas/Zotero/storage/YKARQZG7/sim.html:text/html},
}

@article{hickey_joint_2016,
	title = {Joint modelling of time-to-event and multivariate longitudinal outcomes: recent developments and issues},
	volume = {16},
	issn = {1471-2288},
	shorttitle = {Joint modelling of time-to-event and multivariate longitudinal outcomes},
	url = {https://doi.org/10.1186/s12874-016-0212-5},
	doi = {10.1186/s12874-016-0212-5},
	abstract = {Available methods for the joint modelling of longitudinal and time-to-event outcomes have typically only allowed for a single longitudinal outcome and a solitary event time. In practice, clinical studies are likely to record multiple longitudinal outcomes. Incorporating all sources of data will improve the predictive capability of any model and lead to more informative inferences for the purpose of medical decision-making.},
	number = {1},
	urldate = {2022-10-04},
	journal = {BMC Medical Research Methodology},
	author = {Hickey, Graeme L. and Philipson, Pete and Jorgensen, Andrea and Kolamunnage-Dona, Ruwanthi},
	month = sep,
	year = {2016},
	keywords = {Joint models, Longitudinal data, Multivariate data, Software, Time-to-event data},
	pages = {117},
	file = {Full Text PDF:/Users/elisedumas/Zotero/storage/HHY3HE76/Hickey et al. - 2016 - Joint modelling of time-to-event and multivariate .pdf:application/pdf;Snapshot:/Users/elisedumas/Zotero/storage/L7UPR26H/s12874-016-0212-5.html:text/html},
}

@article{li_promising_2022,
	title = {A {Promising} {Approach} to {Optimizing} {Sequential} {Treatment} {Decisions} for {Depression}: {Markov} {Decision} {Process}},
	issn = {1179-2027},
	shorttitle = {A {Promising} {Approach} to {Optimizing} {Sequential} {Treatment} {Decisions} for {Depression}},
	url = {https://doi.org/10.1007/s40273-022-01185-z},
	doi = {10.1007/s40273-022-01185-z},
	abstract = {The most appropriate next step in depression treatment after the initial treatment fails is unclear. This study explores the suitability of the Markov decision process for optimizing sequential treatment decisions for depression. We conducted a formal comparison of a Markov decision process approach and mainstream state-transition models as used in health economic decision analysis to clarify differences in the model structure. We performed two reviews: the first to identify existing applications of the Markov decision process in the field of healthcare and the second to identify existing health economic models for depression. We then illustrated the application of a Markov decision process by reformulating an existing health economic model. This provided input for discussing the suitability of a Markov decision process for solving sequential treatment decisions in depression. The Markov decision process and state-transition models differed in terms of flexibility in modeling actions and rewards. In all, 23 applications of a Markov decision process within the context of somatic disease were included, 16 of which concerned sequential treatment decisions. Most existing health economic models relating to depression have a state-transition structure. The example application replicated the health economic model and enabled additional capacity to make dynamic comparisons of more interventions over time than was possible with traditional state-transition models. Markov decision processes have been successfully applied to address sequential treatment-decision problems, although the results have been published mostly in economics journals that are not related to healthcare. One advantage of a Markov decision process compared with state-transition models is that it allows extended action space: the possibility of making dynamic comparisons of different treatments over time. Within the context of depression, although existing state-transition models are too basic to evaluate sequential treatment decisions, the assumptions of a Markov decision process could be satisfied. The Markov decision process could therefore serve as a powerful model for optimizing sequential treatment in depression. This would require a sufficiently elaborate state-transition model at the cohort or patient level.},
	language = {en},
	urldate = {2022-10-04},
	journal = {PharmacoEconomics},
	author = {Li, Fang and Jörg, Frederike and Li, Xinyu and Feenstra, Talitha},
	month = sep,
	year = {2022},
	file = {Full Text PDF:/Users/elisedumas/Zotero/storage/RBHRE2RL/Li et al. - 2022 - A Promising Approach to Optimizing Sequential Trea.pdf:application/pdf},
}

@article{catling_temporal_2019,
	title = {Temporal convolutional networks allow early prediction of events in critical care},
	volume = {27},
	issn = {1067-5027},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7647248/},
	doi = {10.1093/jamia/ocz205},
	abstract = {Objective
Clinical interventions and death in the intensive care unit (ICU) depend on complex patterns in patients’ longitudinal data. We aim to anticipate these events earlier and more consistently so that staff can consider preemptive action.

Materials and Methods
We use a temporal convolutional network to encode longitudinal data and a feedforward neural network to encode demographic data from 4713 ICU admissions in 2014–2018. For each hour of each admission, we predict events in the subsequent 1–6 hours. We compare performance with other models including a recurrent neural network.

Results
Our model performed similarly to the recurrent neural network for some events and outperformed it for others. This performance increase was more evident in a sensitivity analysis where the prediction timeframe was varied. Average positive predictive value (95\% CI) was 0.786 (0.781–0.790) and 0.738 (0.732–0.743) for up- and down-titrating FiO2, 0.574 (0.519–0.625) for extubation, 0.139 (0.117–0.162) for intubation, 0.533 (0.492–0.572) for starting noradrenaline, 0.441 (0.433–0.448) for fluid challenge, and 0.315 (0.282–0.352) for death.

Discussion
Events were better predicted where their important determinants were captured in structured electronic health data, and where they occurred in homogeneous circumstances. We produce partial dependence plots that show our model learns clinically-plausible associations between its inputs and predictions.

Conclusion
Temporal convolutional networks improve prediction of clinical events when used to represent longitudinal ICU data.},
	number = {3},
	urldate = {2022-10-04},
	journal = {Journal of the American Medical Informatics Association : JAMIA},
	author = {Catling, Finneas J R and Wolff, Anthony H},
	month = dec,
	year = {2019},
	pmid = {31858114},
	pmcid = {PMC7647248},
	pages = {355--365},
	file = {PubMed Central Full Text PDF:/Users/elisedumas/Zotero/storage/MQ2K5KDZ/Catling and Wolff - 2019 - Temporal convolutional networks allow early predic.pdf:application/pdf},
}

@article{fouladvand_identifying_2022,
	title = {Identifying {Opioid} {Use} {Disorder} from {Longitudinal} {Healthcare} {Data} using a {Multi}-stream {Transformer}},
	volume = {2021},
	issn = {1942-597X},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8861731/},
	abstract = {Opioid Use Disorder (OUD) is a public health crisis costing the US billions of dollars annually in healthcare, lost workplace productivity, and crime. Analyzing longitudinal healthcare data is critical in addressing many real-world problems in healthcare. Leveraging the real-world longitudinal healthcare data, we propose a novel multi-stream transformer model called MUPOD for OUD identification. MUPOD is designed to simultaneously analyze multiple types of healthcare data streams, such as medications and diagnoses, by attending to segments within and across these data streams. Our model tested on the data from 392,492 patients with long-term back pain problems showed significantly better performance than the traditional models and recently developed deep learning models.},
	urldate = {2022-10-04},
	journal = {AMIA Annual Symposium Proceedings},
	author = {Fouladvand, Sajjad and Talbert, Jeffery and Dwoskin, Linda P and Bush, Heather and Meadows, Amy Lynn and Peterson, Lars E and Roggenkamp, Steve K and Kavuluru, Ramakanth and Chen, Jin},
	month = feb,
	year = {2022},
	pmid = {35308960},
	pmcid = {PMC8861731},
	pages = {476--485},
	file = {PubMed Central Full Text PDF:/Users/elisedumas/Zotero/storage/PB86BPHS/Fouladvand et al. - 2022 - Identifying Opioid Use Disorder from Longitudinal .pdf:application/pdf},
}

@article{zeng_pretrained_2022,
	title = {Pretrained transformer framework on pediatric claims data for population specific tasks},
	volume = {12},
	copyright = {2022 The Author(s)},
	issn = {2045-2322},
	url = {https://www.nature.com/articles/s41598-022-07545-1},
	doi = {10.1038/s41598-022-07545-1},
	abstract = {The adoption of electronic health records (EHR) has become universal during the past decade, which has afforded in-depth data-based research. By learning from the large amount of healthcare data, various data-driven models have been built to predict future events for different medical tasks, such as auto diagnosis and heart-attack prediction. Although EHR is abundant, the population that satisfies specific criteria for learning population-specific tasks is scarce, making it challenging to train data-hungry deep learning models. This study presents the Claim Pre-Training (Claim-PT) framework, a generic pre-training model that first trains on the entire pediatric claims dataset, followed by a discriminative fine-tuning on each population-specific task. The semantic meaning of medical events can be captured in the pre-training stage, and the effective knowledge transfer is completed through the task-aware fine-tuning stage. The fine-tuning process requires minimal parameter modification without changing the model architecture, which mitigates the data scarcity issue and helps train the deep learning model adequately on small patient cohorts. We conducted experiments on a real-world pediatric dataset with more than one million patient records. Experimental results on two downstream tasks demonstrated the effectiveness of our method: our general task-agnostic pre-training framework outperformed tailored task-specific models, achieving more than 10\% higher in model performance as compared to baselines. In addition, our framework showed a potential to transfer learned knowledge from one institution to another, which may pave the way for future healthcare model pre-training across institutions.},
	language = {en},
	number = {1},
	urldate = {2022-10-04},
	journal = {Scientific Reports},
	author = {Zeng, Xianlong and Linwood, Simon L. and Liu, Chang},
	month = mar,
	year = {2022},
	note = {Number: 1
Publisher: Nature Publishing Group},
	keywords = {Computational biology and bioinformatics, Health care},
	pages = {3651},
	file = {Full Text PDF:/Users/elisedumas/Zotero/storage/HF6Q4Q8K/Zeng et al. - 2022 - Pretrained transformer framework on pediatric clai.pdf:application/pdf;Snapshot:/Users/elisedumas/Zotero/storage/RJVCJ9GM/s41598-022-07545-1.html:text/html},
}

@article{zadeh_bias_2021,
	title = {Bias in {Cross}-{Entropy}-{Based} {Training} of {Deep} {Survival} {Networks}},
	volume = {43},
	issn = {1939-3539},
	doi = {10.1109/TPAMI.2020.2979450},
	abstract = {Over the last years, utilizing deep learning for the analysis of survival data has become attractive to many researchers. This has led to the advent of numerous network architectures for the prediction of possibly censored time-to-event variables. Unlike networks for cross-sectional data (used e.g., in classification), deep survival networks require the specification of a suitably defined loss function that incorporates typical characteristics of survival data such as censoring and time-dependent features. Here, we provide an in-depth analysis of the cross-entropy loss function, which is a popular loss function for training deep survival networks. For each time point t, the cross-entropy loss is defined in terms of a binary outcome with levels “event at or before t” and “event after t”. Using both theoretical and empirical approaches, we show that this definition may result in a high prediction error and a heavy bias in the predicted survival probabilities. To overcome this problem, we analyze an alternative loss function that is derived from the negative log-likelihood function of a discrete time-to-event model. We show that replacing the cross-entropy loss by the negative log-likelihood loss results in much better calibrated prediction rules and also in an improved discriminatory power, as measured by the concordance index.},
	number = {9},
	journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	author = {Zadeh, Shekoufeh Gorgi and Schmid, Matthias},
	month = sep,
	year = {2021},
	note = {Conference Name: IEEE Transactions on Pattern Analysis and Machine Intelligence},
	keywords = {Cross-entropy loss, deep recurrent survival analysis, deep survival network, Entropy, Hazards, Indexes, Mathematical model, model calibration, negative log-likelihood loss, Neural networks, Power measurement, Training},
	pages = {3126--3137},
	file = {IEEE Xplore Abstract Record:/Users/elisedumas/Zotero/storage/4G4XFM26/9028113.html:text/html},
}

@article{pedersen_incidence_2022,
	title = {The {Incidence} of {Breast} {Cancer} {Recurrence} 10-32 {Years} {After} {Primary} {Diagnosis}},
	volume = {114},
	issn = {0027-8874},
	url = {https://doi.org/10.1093/jnci/djab202},
	doi = {10.1093/jnci/djab202},
	abstract = {Extended, more effective breast cancer treatments have increased the prevalence of long-term survivors. We investigated the risk of late breast cancer recurrence (BCR), 10 years or more after primary diagnosis, and associations between patient and tumor characteristics at primary diagnosis and late BCR up to 32 years after primary breast cancer diagnosis.Using the Danish Breast Cancer Group clinical database, we identified all women with an incident early breast cancer diagnosed during 1987-2004. We restricted to women who survived 10 years without a recurrence or second cancer (10-year disease-free survivors) and followed them from 10 years after breast cancer diagnosis date until late recurrence, death, emigration, second cancer, or December 31, 2018. We calculated incidence rates per 1000 person-years and cumulative incidences for late BCR, stratifying by patient and tumor characteristics. Using Cox regression, we calculated adjusted hazard ratios for late BCR accounting for competing risks.Among 36 924 women with breast cancer, 20 315 became 10-year disease-free survivors. Of these, 2595 developed late BCR (incidence rate = 15.53 per 1000 person-years, 95\% confidence interval = 14.94 to 16.14; cumulative incidence = 16.6\%, 95\% confidence interval = 15.8\% to 17.5\%) from year 10 to 32 after primary diagnosis. Tumor size larger than 20 mm, lymph node–positive disease, and estrogen receptor–positive tumors were associated with increased cumulative incidences and hazards for late BCR.Recurrences continued to occur up to 32 years after primary diagnosis. Women with high lymph node burden, large tumor size, and estrogen receptor–positive tumors had increased risk of late recurrence. Such patients may warrant extended surveillance, more aggressive treatment, or new therapy approaches.},
	number = {3},
	urldate = {2022-10-04},
	journal = {JNCI: Journal of the National Cancer Institute},
	author = {Pedersen, Rikke Nørgaard and Esen, Buket Öztürk and Mellemkjær, Lene and Christiansen, Peer and Ejlertsen, Bent and Lash, Timothy Lee and Nørgaard, Mette and Cronin-Fenton, Deirdre},
	month = mar,
	year = {2022},
	pages = {391--399},
	file = {Full Text PDF:/Users/elisedumas/Zotero/storage/XXQN3D8S/Pedersen et al. - 2022 - The Incidence of Breast Cancer Recurrence 10-32 Ye.pdf:application/pdf;Snapshot:/Users/elisedumas/Zotero/storage/AD4ZVSZD/6423212.html:text/html},
}

@misc{bajor_embedding_2018,
	title = {Embedding {Complexity} {In} the {Data} {Representation} {Instead} of {In} the {Model}: {A} {Case} {Study} {Using} {Heterogeneous} {Medical} {Data}},
	shorttitle = {Embedding {Complexity} {In} the {Data} {Representation} {Instead} of {In} the {Model}},
	url = {http://arxiv.org/abs/1802.04233},
	abstract = {Electronic Health Records have become popular sources of data for secondary research, but their use is hampered by the amount of effort it takes to overcome the sparsity, irregularity, and noise that they contain. Modern learning architectures can remove the need for expert-driven feature engineering, but not the need for expert-driven preprocessing to abstract away the inherent messiness of clinical data. This preprocessing effort is often the dominant component of a typical clinical prediction project. In this work we propose using semantic embedding methods to directly couple the raw, messy clinical data to downstream learning architectures with truly minimal preprocessing. We examine this step from the perspective of capturing and encoding complex data dependencies in the data representation instead of in the model, which has the nice benefit of allowing downstream processing to be done with fast, lightweight, and simple models accessible to researchers without machine learning expertise. We demonstrate with three typical clinical prediction tasks that the highly compressed, embedded data representations capture a large amount of useful complexity, although in some cases the compression is not completely lossless.},
	urldate = {2022-10-04},
	publisher = {arXiv},
	author = {Bajor, Jacek M. and Mesa, Diego A. and Osterman, Travis J. and Lasko, Thomas A.},
	month = feb,
	year = {2018},
	note = {arXiv:1802.04233 [stat]},
	keywords = {Statistics - Applications},
	file = {arXiv Fulltext PDF:/Users/elisedumas/Zotero/storage/5CCVIWFW/Bajor et al. - 2018 - Embedding Complexity In the Data Representation In.pdf:application/pdf;arXiv.org Snapshot:/Users/elisedumas/Zotero/storage/4QQZEGH2/1802.html:text/html},
}

@inproceedings{zhu_predicting_2019,
	title = {Predicting {ICU} mortality by supervised bidirectional {LSTM} networks},
	url = {https://openreview.net/forum?id=SJZOcNzO-r},
	language = {en},
	urldate = {2022-10-04},
	author = {Zhu, Yao and Fan, Xiaoliang and Wu, Jinzhun and Liu, Xiao and Shi, Jia and Wang, Cheng},
	month = sep,
	year = {2019},
	file = {Full Text PDF:/Users/elisedumas/Zotero/storage/MVCDJPUI/Zhu et al. - 2019 - Predicting ICU mortality by supervised bidirection.pdf:application/pdf},
}

@misc{peng_bitenet_2020,
	title = {{BiteNet}: {Bidirectional} {Temporal} {Encoder} {Network} to {Predict} {Medical} {Outcomes}},
	shorttitle = {{BiteNet}},
	url = {http://arxiv.org/abs/2009.13252},
	abstract = {Electronic health records (EHRs) are longitudinal records of a patient's interactions with healthcare systems. A patient's EHR data is organized as a three-level hierarchy from top to bottom: patient journey - all the experiences of diagnoses and treatments over a period of time; individual visit - a set of medical codes in a particular visit; and medical code - a specific record in the form of medical codes. As EHRs begin to amass in millions, the potential benefits, which these data might hold for medical research and medical outcome prediction, are staggering - including, for example, predicting future admissions to hospitals, diagnosing illnesses or determining the efficacy of medical treatments. Each of these analytics tasks requires a domain knowledge extraction method to transform the hierarchical patient journey into a vector representation for further prediction procedure. The representations should embed a sequence of visits and a set of medical codes with a specific timestamp, which are crucial to any downstream prediction tasks. Hence, expressively powerful representations are appealing to boost learning performance. To this end, we propose a novel self-attention mechanism that captures the contextual dependency and temporal relationships within a patient's healthcare journey. An end-to-end bidirectional temporal encoder network (BiteNet) then learns representations of the patient's journeys, based solely on the proposed attention mechanism. We have evaluated the effectiveness of our methods on two supervised prediction and two unsupervised clustering tasks with a real-world EHR dataset. The empirical results demonstrate the proposed BiteNet model produces higher-quality representations than state-of-the-art baseline methods.},
	urldate = {2022-10-04},
	publisher = {arXiv},
	author = {Peng, Xueping and Long, Guodong and Shen, Tao and Wang, Sen and Jiang, Jing and Zhang, Chengqi},
	month = sep,
	year = {2020},
	note = {arXiv:2009.13252 [cs]
version: 1},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/elisedumas/Zotero/storage/35ZMDYN2/Peng et al. - 2020 - BiteNet Bidirectional Temporal Encoder Network to.pdf:application/pdf;arXiv.org Snapshot:/Users/elisedumas/Zotero/storage/W44LLYID/2009.html:text/html},
}

@article{graves_framewise_2005,
	series = {{IJCNN} 2005},
	title = {Framewise phoneme classification with bidirectional {LSTM} and other neural network architectures},
	volume = {18},
	issn = {0893-6080},
	url = {https://www.sciencedirect.com/science/article/pii/S0893608005001206},
	doi = {10.1016/j.neunet.2005.06.042},
	abstract = {In this paper, we present bidirectional Long Short Term Memory (LSTM) networks, and a modified, full gradient version of the LSTM learning algorithm. We evaluate Bidirectional LSTM (BLSTM) and several other network architectures on the benchmark task of framewise phoneme classification, using the TIMIT database. Our main findings are that bidirectional networks outperform unidirectional ones, and Long Short Term Memory (LSTM) is much faster and also more accurate than both standard Recurrent Neural Nets (RNNs) and time-windowed Multilayer Perceptrons (MLPs). Our results support the view that contextual information is crucial to speech processing, and suggest that BLSTM is an effective architecture with which to exploit it.11An abbreviated version of some portions of this article appeared in (Graves and Schmidhuber, 2005), as part of the IJCNN 2005 conference proceedings, published under the IEEE copyright.},
	language = {en},
	number = {5},
	urldate = {2022-10-04},
	journal = {Neural Networks},
	author = {Graves, Alex and Schmidhuber, Jürgen},
	month = jul,
	year = {2005},
	pages = {602--610},
	file = {ScienceDirect Full Text PDF:/Users/elisedumas/Zotero/storage/NBPCGDEC/Graves and Schmidhuber - 2005 - Framewise phoneme classification with bidirectiona.pdf:application/pdf;ScienceDirect Snapshot:/Users/elisedumas/Zotero/storage/5VXLC4B8/S0893608005001206.html:text/html},
}

@article{nagpal_deep_2021,
	title = {Deep {Survival} {Machines}: {Fully} {Parametric} {Survival} {Regression} and {Representation} {Learning} for {Censored} {Data} {With} {Competing} {Risks}},
	volume = {25},
	issn = {2168-2208},
	shorttitle = {Deep {Survival} {Machines}},
	doi = {10.1109/JBHI.2021.3052441},
	abstract = {We describe a new approach to estimating relative risks in time-to-event prediction problems with censored data in a fully parametric manner. Our approach does not require making strong assumptions of constant proportional hazards of the underlying survival distribution, as required by the Cox-proportional hazard model. By jointly learning deep nonlinear representations of the input covariates, we demonstrate the benefits of our approach when used to estimate survival risks through extensive experimentation on multiple real world datasets with different levels of censoring. We further demonstrate advantages of our model in the competing risks scenario. To the best of our knowledge, this is the first work involving fully parametric estimation of survival times with competing risks in the presence of censoring.},
	number = {8},
	journal = {IEEE Journal of Biomedical and Health Informatics},
	author = {Nagpal, Chirag and Li, Xinyu and Dubrawski, Artur},
	month = aug,
	year = {2021},
	note = {Conference Name: IEEE Journal of Biomedical and Health Informatics},
	keywords = {Adaptation models, Analytical models, Bioinformatics, censored regression, deep learning, graphical models, Hazards, mixture of experts, Optimization, Predictive models, Solid modeling, Survival analysis},
	pages = {3163--3175},
	file = {IEEE Xplore Abstract Record:/Users/elisedumas/Zotero/storage/DP9Q9UJX/9326348.html:text/html;Nagpal et al. - 2021 - Deep Survival Machines Fully Parametric Survival .pdf:/Users/elisedumas/Zotero/storage/GT9KHP9U/Nagpal et al. - 2021 - Deep Survival Machines Fully Parametric Survival .pdf:application/pdf},
}

@inproceedings{nagpal_deep_2021-1,
	title = {Deep {Parametric} {Time}-to-{Event} {Regression} with {Time}-{Varying} {Covariates}},
	url = {https://proceedings.mlr.press/v146/nagpal21a.html},
	abstract = {Time-to-event regression in healthcare and other domains, such as predictive maintenance, require working with time-series (or time-varying) data such as continuously monitored vital signs, electronic health records, or sensor readings. In such scenarios, the event-time distribution may have temporal dependencies at different time scales that are not easily captured by classical survival models that assume training data points to be independent. In this paper, we describe a fully parametric approach to model censored time-to-event outcomes with time varying covariates. It involves learning representations of the input temporal data using Recurrent Neural Networks such as LSTMs and GRUs, followed by describing the conditional event distribution as a fixed mixture of parametric distributions. The use of the recurrent neural networks allows the learned representations to model long-term dependencies in the input data while jointly estimating the Time-to-Event. We benchmark our approach on MIMIC III: a large, publicly available dataset collected from Intensive Care Unit (ICU) patients, focusing on predicting duration of their ICU stays and their short term life expectancy, and we demonstrate competitive performance of the proposed approach compared to established time-to-event regression models.},
	language = {en},
	urldate = {2022-10-05},
	booktitle = {Proceedings of {AAAI} {Spring} {Symposium} on {Survival} {Prediction} - {Algorithms}, {Challenges}, and {Applications} 2021},
	publisher = {PMLR},
	author = {Nagpal, Chirag and Jeanselme, Vincent and Dubrawski, Artur},
	month = may,
	year = {2021},
	note = {ISSN: 2640-3498},
	pages = {184--193},
	file = {Full Text PDF:/Users/elisedumas/Zotero/storage/AS5584J4/Nagpal et al. - 2021 - Deep Parametric Time-to-Event Regression with Time.pdf:application/pdf},
}

@inproceedings{nagpal_deep_2021-2,
	title = {Deep {Cox} {Mixtures} for {Survival} {Regression}},
	url = {https://proceedings.mlr.press/v149/nagpal21a.html},
	abstract = {Survival analysis is a challenging variation of regression modeling because of the presence of censoring, where the outcome measurement is only partially known, due to, for example, loss to follow up. Such problems come up frequently in medical applications, making survival analysis a key endeavor in biostatistics and machine learning for healthcare, with Cox regression models being amongst the most commonly employed models. We describe a new approach for survival analysis regression models, based on learning mixtures of Cox regressions to model individual survival distributions. We propose an approximation to the Expectation Maximization algorithm for this model that does hard assignments to mixture groups to make optimization efficient. In each group assignment, we fit the hazard ratios within each group using deep neural networks, and the baseline hazard for each mixture component non-parametrically. We perform experiments on multiple real world datasets, and look at the mortality rates of patients across ethnicity and gender. We emphasize the importance of calibration in healthcare settings and demonstrate that our approach outperforms classical and modern survival analysis baselines, both in terms of discriminative performance and calibration, with large gains in performance on the minority demographics.},
	language = {en},
	urldate = {2022-10-05},
	booktitle = {Proceedings of the 6th {Machine} {Learning} for {Healthcare} {Conference}},
	publisher = {PMLR},
	author = {Nagpal, Chirag and Yadlowsky, Steve and Rostamzadeh, Negar and Heller, Katherine},
	month = oct,
	year = {2021},
	note = {ISSN: 2640-3498},
	pages = {674--708},
	file = {Full Text PDF:/Users/elisedumas/Zotero/storage/XB8NIKD4/Nagpal et al. - 2021 - Deep Cox Mixtures for Survival Regression.pdf:application/pdf},
}

@inproceedings{bender_general_2021,
	address = {Cham},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {A {General} {Machine} {Learning} {Framework} for {Survival} {Analysis}},
	isbn = {978-3-030-67664-3},
	doi = {10.1007/978-3-030-67664-3_10},
	abstract = {The modeling of time-to-event data, also known as survival analysis, requires specialized methods that can deal with censoring and truncation, time-varying features and effects, and that extend to settings with multiple competing events. However, many machine learning methods for survival analysis only consider the standard setting with right-censored data and proportional hazards assumption. The methods that do provide extensions usually address at most a subset of these challenges and often require specialized software that can not be integrated into standard machine learning workflows directly. In this work, we present a very general machine learning framework for time-to-event analysis that uses a data augmentation strategy to reduce complex survival tasks to standard Poisson regression tasks. This reformulation is based on well developed statistical theory. With the proposed approach, any algorithm that can optimize a Poisson (log-)likelihood, such as gradient boosted trees, deep neural networks, model-based boosting and many more can be used in the context of time-to-event analysis. The proposed technique does not require any assumptions with respect to the distribution of event times or the functional shapes of feature and interaction effects. Based on the proposed framework we develop new methods that are competitive with specialized state of the art approaches in terms of accuracy, and versatility, but with comparatively small investments of programming effort or requirements for specialized methodological know-how.},
	language = {en},
	booktitle = {Machine {Learning} and {Knowledge} {Discovery} in {Databases}},
	publisher = {Springer International Publishing},
	author = {Bender, Andreas and Rügamer, David and Scheipl, Fabian and Bischl, Bernd},
	editor = {Hutter, Frank and Kersting, Kristian and Lijffijt, Jefrey and Valera, Isabel},
	year = {2021},
	keywords = {Competing risks, Gradient boosting, Multi-state models, Neural networks, Survival analysis},
	pages = {158--173},
	file = {Submitted Version:/Users/elisedumas/Zotero/storage/3HSNRS92/Bender et al. - 2021 - A General Machine Learning Framework for Survival .pdf:application/pdf},
}

@article{vale-silva_long-term_2021,
	title = {Long-term cancer survival prediction using multimodal deep learning},
	volume = {11},
	copyright = {2021 The Author(s)},
	issn = {2045-2322},
	url = {https://www.nature.com/articles/s41598-021-92799-4},
	doi = {10.1038/s41598-021-92799-4},
	abstract = {The age of precision medicine demands powerful computational techniques to handle high-dimensional patient data. We present MultiSurv, a multimodal deep learning method for long-term pan-cancer survival prediction. MultiSurv uses dedicated submodels to establish feature representations of clinical, imaging, and different high-dimensional omics data modalities. A data fusion layer aggregates the multimodal representations, and a prediction submodel generates conditional survival probabilities for follow-up time intervals spanning several decades. MultiSurv is the first non-linear and non-proportional survival prediction method that leverages multimodal data. In addition, MultiSurv can handle missing data, including single values and complete data modalities. MultiSurv was applied to data from 33 different cancer types and yields accurate pan-cancer patient survival curves. A quantitative comparison with previous methods showed that Multisurv achieves the best results according to different time-dependent metrics. We also generated visualizations of the learned multimodal representation of MultiSurv, which revealed insights on cancer characteristics and heterogeneity.},
	language = {en},
	number = {1},
	urldate = {2022-10-05},
	journal = {Scientific Reports},
	author = {Vale-Silva, Luís A. and Rohr, Karl},
	month = jun,
	year = {2021},
	note = {Number: 1
Publisher: Nature Publishing Group},
	keywords = {Cancer, Cancer genomics, Cancer imaging, Cancer models, Computational biology and bioinformatics, Computational models, Data integration},
	pages = {13505},
	file = {Full Text PDF:/Users/elisedumas/Zotero/storage/64RGDKGD/Vale-Silva and Rohr - 2021 - Long-term cancer survival prediction using multimo.pdf:application/pdf;Snapshot:/Users/elisedumas/Zotero/storage/GKKY83IJ/s41598-021-92799-4.html:text/html},
}

@inproceedings{wang_survtrace_2022,
	address = {New York, NY, USA},
	series = {{BCB} '22},
	title = {{SurvTRACE}: transformers for survival analysis with competing events},
	isbn = {978-1-4503-9386-7},
	shorttitle = {{SurvTRACE}},
	url = {https://doi.org/10.1145/3535508.3545521},
	doi = {10.1145/3535508.3545521},
	abstract = {In medicine, survival analysis studies the time duration to events of interest such as mortality. One major challenge is how to deal with multiple competing events (e.g., multiple disease diagnoses). In this work, we propose a transformer-based model that does not make the assumption for the underlying survival distribution and is capable of handling competing events, namely SurvTRACE. We account for the implicit confounders in the observational setting in multi-events scenarios, which causes selection bias as the predicted survival probability is influenced by irrelevant factors. To sufficiently utilize the survival data to train transformers from scratch, multiple auxiliary tasks are designed for multi-task learning. The model hence learns a strong shared representation from all these tasks and in turn serves for better survival analysis. We further demonstrate how to inspect the covariate relevance and importance through interpretable attention mechanisms of SurvTRACE, which suffices to great potential in enhancing clinical trial design and new treatment development. Experiments on METABRIC, SUPPORT, and SEER data with 470k patients validate the all-around superiority of our method. Software is available at https://github.com/RyanWangZf/SurvTRACE.},
	urldate = {2022-10-05},
	booktitle = {Proceedings of the 13th {ACM} {International} {Conference} on {Bioinformatics}, {Computational} {Biology} and {Health} {Informatics}},
	publisher = {Association for Computing Machinery},
	author = {Wang, Zifeng and Sun, Jimeng},
	month = aug,
	year = {2022},
	keywords = {competing events, survival analysis, transformers},
	pages = {1--9},
	file = {Full Text PDF:/Users/elisedumas/Zotero/storage/ITMF5QFI/Wang and Sun - 2022 - SurvTRACE transformers for survival analysis with.pdf:application/pdf},
}

@inproceedings{putzel_dynamic_2021,
	title = {Dynamic {Survival} {Analysis} with {Individualized} {Truncated} {Parametric} {Distributions}},
	url = {https://proceedings.mlr.press/v146/putzel21a.html},
	abstract = {Dynamic survival analysis is a variant of traditional survival analysis where time-to-event predictions are updated as new information arrives about an individual over time. In this paper we propose a new approach to dynamic survival analysis based on learning a global parametric distribution, followed by individualization via truncating and renormalizing that distribution at different locations over time. We combine this approach with a likelihood-based loss that includes predictions at every time step within an individual’s history, rather than just including one term per individual. The combination of this loss and model results in an interpretable approach to dynamic survival, requiring less fine tuning than existing methods, while still achieving good predictive performance. We evaluate the approach on the problem of predicting hospital mortality for a dataset with over 6900 COVID-19 patients.},
	language = {en},
	urldate = {2022-10-05},
	booktitle = {Proceedings of {AAAI} {Spring} {Symposium} on {Survival} {Prediction} - {Algorithms}, {Challenges}, and {Applications} 2021},
	publisher = {PMLR},
	author = {Putzel, Preston and Smyth, Padhraic and Yu, Jaehong and Zhong, Hua},
	month = may,
	year = {2021},
	note = {ISSN: 2640-3498},
	pages = {159--170},
	file = {Putzel et al. - 2021 - Dynamic Survival Analysis with Individualized Trun.pdf:/Users/elisedumas/Zotero/storage/IDN5UDCC/Putzel et al. - 2021 - Dynamic Survival Analysis with Individualized Trun.pdf:application/pdf},
}

@inproceedings{nowroozilarki_real-time_2021,
	title = {Real-time {Mortality} {Prediction} {Using} {MIMIC}-{IV} {ICU} {Data} {Via} {Boosted} {Nonparametric} {Hazards}},
	doi = {10.1109/BHI50953.2021.9508537},
	abstract = {Electronic Health Record (EHR) systems provide critical, rich and valuable information at high frequency. One of the most exciting applications of EHR data is in developing a real-time mortality warning system with tools from survival analysis. However, most of the survival analysis methods used recently are based on (semi)parametric models using static covariates. These models do not take advantage of the information conveyed by the time-varying EHR data. In this work we present an application of a highly scalable survival analysis method, BoXHED 2.0 [1], to develop a real-time in-ICU mortality warning indicator based on the MIMIC IV data set [2]. Importantly, BoXHED can incorporate time-dependent covariates in a fully nonparametric manner and is backed by theory [3]. Our in-ICU mortality model achieves an AUC-PRC of 0.41 and AUC-ROC of 0.83 out of sample, demonstrating the benefit of real-time monitoring.},
	booktitle = {2021 {IEEE} {EMBS} {International} {Conference} on {Biomedical} and {Health} {Informatics} ({BHI})},
	author = {Nowroozilarki, Zhale and Pakbin, Arash and Royalty, James and Lee, Donald K.K. and Mortazavi, Bobak J.},
	month = jul,
	year = {2021},
	note = {ISSN: 2641-3604},
	keywords = {Alarm systems, Conferences, Data models, Electronic Health Record, Hazard estimation, Hazards, MIMIC IV Dataset, MIMICs, Nonparametric, Real-time systems, Survival analysis, Time-dependent covariates, Tools},
	pages = {1--4},
	file = {IEEE Xplore Abstract Record:/Users/elisedumas/Zotero/storage/DQF7SUMH/9508537.html:text/html;Submitted Version:/Users/elisedumas/Zotero/storage/7DFQXQLC/Nowroozilarki et al. - 2021 - Real-time Mortality Prediction Using MIMIC-IV ICU .pdf:application/pdf},
}

@article{sun_attention-based_2021,
	title = {Attention-{Based} {Deep} {Recurrent} {Model} for {Survival} {Prediction}},
	volume = {2},
	issn = {2691-1957},
	url = {https://doi.org/10.1145/3466782},
	doi = {10.1145/3466782},
	abstract = {Survival analysis exhibits profound effects on health service management. Traditional approaches for survival analysis have a pre-assumption on the time-to-event probability distribution and seldom consider sequential visits of patients on medical facilities. Although recent studies leverage the merits of deep learning techniques to capture non-linear features and long-term dependencies within multiple visits for survival analysis, the lack of interpretability prevents deep learning models from being applied to clinical practice. To address this challenge, this article proposes a novel attention-based deep recurrent model, named AttenSurv, for clinical survival analysis. Specifically, a global attention mechanism is proposed to extract essential/critical risk factors for interpretability improvement. Thereafter, Bi-directional Long Short-Term Memory is employed to capture the long-term dependency on data from a series of visits of patients. To further improve both the prediction performance and the interpretability of the proposed model, we propose another model, named GNNAttenSurv, by incorporating a graph neural network into AttenSurv, to extract the latent correlations between risk factors. We validated our solution on three public follow-up datasets and two electronic health record datasets. The results demonstrated that our proposed models yielded consistent improvement compared to the state-of-the-art baselines on survival analysis.},
	number = {4},
	urldate = {2022-10-05},
	journal = {ACM Transactions on Computing for Healthcare},
	author = {Sun, Zhaohong and Dong, Wei and Shi, Jinlong and He, Kunlun and Huang, Zhengxing},
	month = sep,
	year = {2021},
	keywords = {deep learning, global attention mechanism, graph neural network, risk factor identification, Survival analysis},
	pages = {35:1--35:18},
	file = {Full Text PDF:/Users/elisedumas/Zotero/storage/BSB8ELI5/Sun et al. - 2021 - Attention-Based Deep Recurrent Model for Survival .pdf:application/pdf},
}

@misc{mozer_discrete_2017-1,
	title = {Discrete {Event}, {Continuous} {Time} {RNNs}},
	url = {http://arxiv.org/abs/1710.04110},
	doi = {10.48550/arXiv.1710.04110},
	abstract = {We investigate recurrent neural network architectures for event-sequence processing. Event sequences, characterized by discrete observations stamped with continuous-valued times of occurrence, are challenging due to the potentially wide dynamic range of relevant time scales as well as interactions between time scales. We describe four forms of inductive bias that should benefit architectures for event sequences: temporal locality, position and scale homogeneity, and scale interdependence. We extend the popular gated recurrent unit (GRU) architecture to incorporate these biases via intrinsic temporal dynamics, obtaining a continuous-time GRU. The CT-GRU arises by interpreting the gates of a GRU as selecting a time scale of memory, and the CT-GRU generalizes the GRU by incorporating multiple time scales of memory and performing context-dependent selection of time scales for information storage and retrieval. Event time-stamps drive decay dynamics of the CT-GRU, whereas they serve as generic additional inputs to the GRU. Despite the very different manner in which the two models consider time, their performance on eleven data sets we examined is essentially identical. Our surprising results point both to the robustness of GRU and LSTM architectures for handling continuous time, and to the potency of incorporating continuous dynamics into neural architectures.},
	urldate = {2022-10-05},
	publisher = {arXiv},
	author = {Mozer, Michael C. and Kazakov, Denis and Lindsey, Robert V.},
	month = oct,
	year = {2017},
	note = {arXiv:1710.04110 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, I.2.6},
	file = {arXiv Fulltext PDF:/Users/elisedumas/Zotero/storage/XFBPSS2B/Mozer et al. - 2017 - Discrete Event, Continuous Time RNNs.pdf:application/pdf;arXiv.org Snapshot:/Users/elisedumas/Zotero/storage/VCWIKZR8/1710.html:text/html},
}

@article{che_recurrent_2018,
	title = {Recurrent {Neural} {Networks} for {Multivariate} {Time} {Series} with {Missing} {Values}},
	volume = {8},
	copyright = {2018 The Author(s)},
	issn = {2045-2322},
	url = {https://www.nature.com/articles/s41598-018-24271-9},
	doi = {10.1038/s41598-018-24271-9},
	abstract = {Multivariate time series data in practical applications, such as health care, geoscience, and biology, are characterized by a variety of missing values. In time series prediction and other related tasks, it has been noted that missing values and their missing patterns are often correlated with the target labels, a.k.a., informative missingness. There is very limited work on exploiting the missing patterns for effective imputation and improving prediction performance. In this paper, we develop novel deep learning models, namely GRU-D, as one of the early attempts. GRU-D is based on Gated Recurrent Unit (GRU), a state-of-the-art recurrent neural network. It takes two representations of missing patterns, i.e., masking and time interval, and effectively incorporates them into a deep model architecture so that it not only captures the long-term temporal dependencies in time series, but also utilizes the missing patterns to achieve better prediction results. Experiments of time series classification tasks on real-world clinical datasets (MIMIC-III, PhysioNet) and synthetic datasets demonstrate that our models achieve state-of-the-art performance and provide useful insights for better understanding and utilization of missing values in time series analysis.},
	language = {en},
	number = {1},
	urldate = {2022-10-05},
	journal = {Scientific Reports},
	author = {Che, Zhengping and Purushotham, Sanjay and Cho, Kyunghyun and Sontag, David and Liu, Yan},
	month = apr,
	year = {2018},
	note = {Number: 1
Publisher: Nature Publishing Group},
	keywords = {Computational models, Computer science, Machine learning},
	pages = {6085},
	file = {Full Text PDF:/Users/elisedumas/Zotero/storage/MUQATIGI/Che et al. - 2018 - Recurrent Neural Networks for Multivariate Time Se.pdf:application/pdf;Snapshot:/Users/elisedumas/Zotero/storage/E39QX2VD/s41598-018-24271-9.html:text/html},
}

@article{zong_computational_2022,
	title = {Computational drug repurposing based on electronic health records: a scoping review},
	volume = {5},
	issn = {2398-6352},
	shorttitle = {Computational drug repurposing based on electronic health records},
	doi = {10.1038/s41746-022-00617-6},
	abstract = {Computational drug repurposing methods adapt Artificial intelligence (AI) algorithms for the discovery of new applications of approved or investigational drugs. Among the heterogeneous datasets, electronic health records (EHRs) datasets provide rich longitudinal and pathophysiological data that facilitate the generation and validation of drug repurposing. Here, we present an appraisal of recently published research on computational drug repurposing utilizing the EHR. Thirty-three research articles, retrieved from Embase, Medline, Scopus, and Web of Science between January 2000 and January 2022, were included in the final review. Four themes, (1) publication venue, (2) data types and sources, (3) method for data processing and prediction, and (4) targeted disease, validation, and released tools were presented. The review summarized the contribution of EHR used in drug repurposing as well as revealed that the utilization is hindered by the validation, accessibility, and understanding of EHRs. These findings can support researchers in the utilization of medical data resources and the development of computational methods for drug repurposing.},
	language = {eng},
	number = {1},
	journal = {NPJ digital medicine},
	author = {Zong, Nansu and Wen, Andrew and Moon, Sungrim and Fu, Sunyang and Wang, Liwei and Zhao, Yiqing and Yu, Yue and Huang, Ming and Wang, Yanshan and Zheng, Gang and Mielke, Michelle M. and Cerhan, James R. and Liu, Hongfang},
	month = jun,
	year = {2022},
	pmid = {35701544},
	pmcid = {PMC9198008},
	pages = {77},
	file = {Full Text:/Users/elisedumas/Zotero/storage/DWIC8WAJ/Zong et al. - 2022 - Computational drug repurposing based on electronic.pdf:application/pdf},
}

@article{javidi_identification_2022,
	title = {Identification of robust deep neural network models of longitudinal clinical measurements},
	volume = {5},
	copyright = {2022 The Author(s)},
	issn = {2398-6352},
	url = {https://www.nature.com/articles/s41746-022-00651-4},
	doi = {10.1038/s41746-022-00651-4},
	abstract = {Deep learning (DL) from electronic health records holds promise for disease prediction, but systematic methods for learning from simulated longitudinal clinical measurements have yet to be reported. We compared nine DL frameworks using simulated body mass index (BMI), glucose, and systolic blood pressure trajectories, independently isolated shape and magnitude changes, and evaluated model performance across various parameters (e.g., irregularity, missingness). Overall, discrimination based on variation in shape was more challenging than magnitude. Time-series forest-convolutional neural networks (TSF-CNN) and Gramian angular field(GAF)-CNN outperformed other approaches (P {\textless} 0.05) with overall area-under-the-curve (AUCs) of 0.93 for both models, and 0.92 and 0.89 for variation in magnitude and shape with up to 50\% missing data. Furthermore, in a real-world assessment, the TSF-CNN model predicted T2D with AUCs reaching 0.72 using only BMI trajectories. In conclusion, we performed an extensive evaluation of DL approaches and identified robust modeling frameworks for disease prediction based on longitudinal clinical measurements.},
	language = {en},
	number = {1},
	urldate = {2022-10-05},
	journal = {npj Digital Medicine},
	author = {Javidi, Hamed and Mariam, Arshiya and Khademi, Gholamreza and Zabor, Emily C. and Zhao, Ran and Radivoyevitch, Tomas and Rotroff, Daniel M.},
	month = jul,
	year = {2022},
	note = {Number: 1
Publisher: Nature Publishing Group},
	keywords = {Outcomes research, Type 2 diabetes},
	pages = {1--11},
	file = {Full Text PDF:/Users/elisedumas/Zotero/storage/7ESF4DQB/Javidi et al. - 2022 - Identification of robust deep neural network model.pdf:application/pdf;Snapshot:/Users/elisedumas/Zotero/storage/5KZCPTZU/s41746-022-00651-4.html:text/html},
}
