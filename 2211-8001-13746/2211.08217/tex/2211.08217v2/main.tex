\documentclass[10pt,twocolumn,letterpaper]{article}

% \usepackage{iccv}
\usepackage[pagenumbers]{cvpr} % To force page numbers, e.g. for an arXiv version
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}

\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{pifont}
\usepackage{color, colortbl}
\usepackage{xcolor}
\usepackage{tikz}

\newcommand{\cmark}{\ding{51}}
\newcommand{\xmark}{\ding{55}}
\newcommand\cmnt[1]{\textcolor{red}{#1}}

% definitions for medals
\definecolor{gold}{HTML}{BD820B}%{EEAD0E}
\definecolor{silver}{HTML}{909090}%{C0C0C0}
\definecolor{bronze}{HTML}{9A5F26}%{CD7F32}

\newcommand*\circledd[1]{\tikz[baseline=(char.base)]{
            \node[shape=circle,draw,inner sep=0.15pt] (char) {#1};}}      

\newcommand{\first}[1]{%
    {#1\raisebox{0.8pt}{\footnotesize \color{gold} \circledd{1}}}%
}
\newcommand{\second}[1]{%
    {#1\raisebox{0.8pt}{\footnotesize \color{silver} \circledd{2}}}%
}
\newcommand{\third}[1]{%
    {#1\raisebox{0.8pt}{\footnotesize \color{bronze} \circledd{3}}}%
}

% It is strongly recommended to use hyperref, especially for the review version.
% hyperref with option pagebackref eases the reviewers' job.
% Please disable hyperref *only* if you encounter grave issues, e.g. with the
% file validation for the camera-ready version.
%
% If you comment hyperref and then uncomment it, you should delete
% ReviewTempalte.aux before re-running LaTeX.
% (Or just hit 'q' on the first LaTeX run, let it finish, and you
%  should be clear).
\usepackage[pagebackref,breaklinks,colorlinks]{hyperref}
% \usepackage[accsupp]{axessibility}


% Support for easy cross-referencing
\usepackage[capitalize]{cleveref}
\crefname{section}{Sec.}{Secs.}
\Crefname{section}{Section}{Sections}
\Crefname{table}{Table}{Tables}
\crefname{table}{Tab.}{Tabs.}


% Include other packages here, before hyperref.

% If you comment hyperref and then uncomment it, you should delete
% egpaper.aux before re-running latex.  (Or just hit 'q' on the first latex
% run, let it finish, and you should be clear).
%\usepackage[pagebackref=true,breaklinks=true,letterpaper=true,colorlinks,bookmarks=false]{hyperref}

% \iccvfinalcopy % *** Uncomment this line for the final submission

\def\iccvPaperID{12275} % *** Enter the ICCV Paper ID here
\def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}

% Pages are numbered in submission mode, and unnumbered in camera-ready
% \ificcvfinal\pagestyle{empty}\fi

\begin{document}


%%%%%%%%% TITLE - PLEASE UPDATE
%\title{One Counter to Rule Them All -- Bridging Few-Shot and Zero-Shot Counting}
\title{A Low-Shot Object Counting Network With Iterative Prototype Adaptation}
%\title{Iterative Prototype Adaptation for Low-Shot Counting}

% \author{First Author\\
% Institution1\\
% Institution1 address\\
% {\tt\small firstauthor@i1.org}
% For a paper whose authors are all at the same institution,
% omit the following lines up until the closing ``}''.
% Additional authors and addresses can be added with ``\and'',
% just like the second author.
% To save space, use either the email address or home page, not both
% \and
% Second Author\\
% Institution2\\
% First line of institution2 address\\
% {\tt\small secondauthor@i2.org}
% }
\author{Nikola Đukić, Alan Lukežič, Vitjan Zavrtanik, Matej Kristan \\
{\small Faculty of Computer and Information Science, University of Ljubljana, Slovenia} \\
{\tt\small nikola.m.djukic@gmail.com, \{alan.lukezic, vitjan.zavrtanik, matej.kristan\}@fri.uni-lj.si}
\vspace{-0.5cm}
}
\maketitle

% \ificcvfinal\thispagestyle{empty}\fi

%%%%%%%%% ABSTRACT
\begin{abstract}
We consider low-shot counting of arbitrary semantic categories in the image using only few annotated exemplars (few-shot) or no exemplars (no-shot). The standard few-shot pipeline follows extraction of appearance queries from exemplars and matching them with image features to infer the object counts. 
Existing methods extract queries by feature pooling which neglects the shape information (e.g., size and aspect) and leads to a reduced object localization accuracy and count estimates.

We propose a Low-shot Object Counting network with iterative prototype Adaptation (LOCA).
Our main contribution is the new object prototype extraction module, which iteratively fuses the exemplar shape and appearance information with image features.
The module is easily adapted to zero-shot scenarios, enabling LOCA to cover the entire spectrum of low-shot counting problems.
LOCA outperforms all recent state-of-the-art methods on FSC147 benchmark by 20-30\% in RMSE on one-shot and few-shot and achieves state-of-the-art on zero-shot scenarios, while demonstrating better generalization capabilities.
%. LOCA also outperforms all state-of-the-art on detection-based and cross-dataset generalization scenarios. 
The code and models are available here: \url{https://github.com/djukicn/loca}.
\end{abstract}

%%%%%%%%% BODY TEXT
\section{Introduction}

Object counting considers estimation of the number of specific objects in the image. Solutions based on object detectors have been extensively explored for categories such as people~\cite{crowdcounting, crowdcounting2}, cars~\cite{cars, carpk} or animal species~\cite{animals, polyps}. However, 
%Object counting considers estimation of the number of potentially crowded objects in a given image. Counting specific object categories such as people~\cite{crowdcounting, crowdcounting2}, cars~\cite{cars, carpk} or animal species~\cite{animals, polyps} has been extensively explored, however, 
these methods require huge annotated training datasets and are not applicable to counting new, previously unobserved, classes with potentially only few annotations. The latter problem is explored by low-shot counting, which encompasses few-shot and zero-shot counting. Few-shot counters count all present objects of some class with only few of them annotated by bounding boxes (exemplars), while zero-shot counters consider counting the most frequent class without annotations.

\begin{figure}[h!]
    \centering
    \includegraphics[width=\linewidth]{figures/fig1v5c.pdf}
    \caption{ 
    % Most of the few-shot state-of-the-art cannot be applied to zero-shot scenarios, while the zero-shot methods cannot utilize the exemplars in few-shot scenarios, leading to high count RMSE. 
    % LOCA exploits shape and appearance and applies a prototype adaptation module, achieving excellent object localization and the lowest RMSE across the entire low-shot spectrum.
    LOCA injects shape and appearance information into object queries to precisely count objects of various sizes in densely and sparsely populated scenarios. It also extends to a zero-shot scenario and achieves excellent localization and count errors across the entire low-shot spectrum.
    }
    \label{fig:figure1}
\end{figure}

Few-shot counters have recently gained momentum with the emergence of a challenging dataset~\cite{famnet} and follow a common pipeline~\cite{gmn,famnet,laonet,bmnet,safecount}. Image and exemplar features are extracted into object prototypes, which are matched to the image by correlation. Finally, the obtained intermediate image representation is regressed into a 2D object density map, whose values sum to the object count estimate.
The methods primarily differ in the intermediate image representation construction method, which is based either on 
Siamese similarity~\cite{gmn,famnet},
cross-attention~\cite{countr,laonet} 
or feature and similarity fusion~\cite{bmnet,safecount}.
While receiving much less attention, zero-shot counters follow a similar principle, but either identify possible exemplars by majority vote from region proposals~\cite{repprncount} or implicitly by attention modules~\cite{hobley}.

All few-shot counters %(as well as those zero-shot based on exemplar identification) 
construct object prototypes by pooling image features extracted from the exemplars into fixed-sized correlation filters.
%exemplars by pooling image features of the annotated bounding boxes into fixed-size rectangular prototypes. 
% \textcolor{orange}{note the difference between an exemplar and a prototype: exemplar refers to the object in the image selected by a bounding box, while the prototype refers to the correlation filter estimated by the counting method}.
%In this work we refer to the selected object in the image as an exemplar, while we refer to the correlation filter estimated by the counting method as a prototype.
%Note the difference between an exemplar, which refers to the object in the image selected by a bounding box and a prototype -- a correlation filter estimated by the counting method.
The prototypes thus fail to encode the object shape information (i.e., width, height and aspect), resulting in a reduced accuracy of the density map. Recent works have shown that this information loss can be partially addressed by complex architectures for learning a nonlinear similarity function~\cite{bmnet}. Nevertheless, we argue that a much simpler counting architecture can be used instead, by explicitly addressing the exemplar shape and by applying an appropriate object prototype adaptation method. 

We propose a \underline{L}ow-shot \underline{O}bject \underline{C}ounting network with iterative prototype \underline{A}daptation (LOCA). 
Our main contribution is the new object prototype extraction module, which separately extracts the exemplar shape and appearance queries.
The shape queries are gradually adapted into object prototypes by considering the exemplar appearance as well as the appearance of non-annotated objects, obtaining excellent localization properties and leading to highly accurate counts (Figure~\ref{fig:figure1}).
To the best of our knowledge, LOCA is the first low-shot counting method that explicitly uses exemplars shape information for counting.
In contrast to most works~\cite{bmnet,famnet,cfocnet,safecount}, LOCA does not attempt to transfer exemplar appearance onto image features, but rather constructs strong prototypes that generalize across the image-level intra-class appearance. 
%By pretraining a general shape query, LOCA can be directly applied to zero-shot setup, which enables tackling the entire low-shot counting spectrum, including the extreme zero-shot setup.

LOCA outperforms all state-of-the-art (in many cases more complicated methods) on the recent FSC147 benchmark~\cite{famnet}. 
On the standard few-shot setup it achieves $\sim$30\% relative performance gains, on one-shot setup even outperforms methods specifically designed for this setup, achieves state-of-the-art on zero-shot counting.
In addition, LOCA demonstrates excellent cross-dataset generalization on the car counting dataset CARPK~\cite{carpk}.


\section{Related work}

Historically, object counting has been addressed by class-specific detectors for people~\cite{crowdcounting, crowdcounting2}, cars~\cite{cars, carpk} and animals~\cite{animals}, but these methods do not cope well with extremely crowded scenes. In a jellyfish polyp counting scenario,~\cite{polyps} thus proposed to segment the image and interpret the segmentation as a collection of circular objects.
Alternatively,~\cite{crowdcounting, regression1} framed counting as a regression of object density map, whose summation predicts the number of objects. 
A major drawback of these methods is that they require large annotated training datasets for each object class, which is often an unrealistic requirement.

In response, class-agnostic counters have been explored, that specialize to the object category at test-time using only a few user-provided object exemplars.
An early representative~\cite{gmn} proposed a two-stream Generic Matching Network, that extracts the image and exemplar object features, concatenates them and regresses the representation into the final density map. 
CFOCNet~\cite{cfocnet} noted that a mere concatenation leads to unreliable localization and proposed a Siamese correlation network inspired by the tracking literature~\cite{siamfc} to improve the localization and counts.
Ranjan et al.~\cite{famnet} proposed a further improvement of correlation robustness by test-time Siamese backbone adaptation.
Shi et al.~\cite{bmnet} proposed an alternative approach for jointly learning the representation as well as a nonlinear similarity metric for improved localization and applied self-attention to reduce the within-class appearance variability in the test image.
You et al.~\cite{safecount} combined the similarity map with the image features before applying location regression to improve count accuracy and proposed a learnable similarity metric to guide the fusion of exemplar and image features.
Liu et al.~\cite{countr} adopted a vision transformer~\cite{vit} for image feature extraction and a convolutional encoder to extract the exemplars. Cross-attention is  used to fuse image and exemplar features and a convolutional decoder regresses the density map.
Recently, few-shot counting has been extended to few-shot detection~\cite{countingdetr} by adopting the transformer-based object detector~\cite{anchor_detr} to predict also the object bounding box in addition to location.

While most works addressed situations with several (typically three) exemplars available, only few recent works considered reducing this number.
Lin et al.~\cite{laonet} proposed a counting method that requires only a single exemplar. 
Their method is based on a transformer architecture and formulates correlation between image and exemplar features by several self- and cross-attention blocks. 
An extreme case of zero-shot counting~\cite{repprncount, hobley} has been explored as well.
Ranjan and Hoai~\cite{repprncount} proposed RepRPN-Counter, which combines a region proposal network~\cite{fasterrcnn} that also predicts a repetition score of each proposal. 
Proposals with the highest repetition scores are used as exemplars and sent through FamNet~\cite{famnet} to predict multiple density maps.
On the other hand, Hobley and Prisacariu~\cite{hobley} developed a weakly-supervised method that implicitly identifies object category most likely to be counted and predicts a density map for that category. Vision transformer with a unsupervised training stage~\cite{countr} has also shown success in zero-shot counting.


\section{A low-shot prototype adaptation counter}\label{sec:method}

\begin{figure*}[t]
    \centering
    \includegraphics[width=1.0\linewidth]{figures/architecture-thin-2.pdf}
    \caption{The LOCA architecture.
    Input image is encoded into features $\mathbf{f}^{E}$, which are depth-wise correlated (*) by $n$ object queries predicted by the object prototype extraction module. 
    The response map $\tilde{\mathbf{R}}$ is obtained by computing per-element maximum of $n$ similarity maps $\tilde{\mathbf{R}}_i$ and then upsampled by decoder to the final density map.% $\mathbf{R}$
    }
    \label{fig:architecture}
\end{figure*}

Without loss of generality, we present our low-shot counting method LOCA in the context of few-shot counting.
% , and then explain the modification for zero-shot setup.
Given an input image $\mathbf{I} \in \mathbb{R}^{H_0 \times W_0 \times 3}$ and a set of $n$ bounding boxes denoting a few selected objects, LOCA predicts a density map $\boldsymbol{R} \in \mathbb{R}^{H_0 \times W_0}$ whose values sum into the number of all objects of the selected class present in $\mathbf{I}$.

The LOCA architecture (Figure~\ref{fig:architecture}) follows four steps: 
(i) image feature extraction (encoder), 
(ii) object prototype extraction, 
(iii) prototype matching and (iv) density regression (decoder).
The input image is resized to $H_{IN}\times W_{IN}$ pixels and encoded by a ResNet-50~\cite{resnet} backbone. 
Multi-scale features are extracted from the second, third and fourth block, resized to a common size of $h\times w$ and reduced by $1 \times 1$ convolutional layer into $d$ channels.
%, thus producing features $\mathbf{f}^{B} \in \mathbb{R}^{h\times w\times d}$. 
To further consolidate the encoded features and increase the similarity between same-category objects, a global (image-wide) self-attention block~\cite{transformer,detr} is applied, thus producing the encoded image features
%The resultin features are denoted as 
$\mathbf{f}^{E} \in \mathbb{R}^{h\times w\times d}$.

Next, $n$ object prototypes $\{ \mathbf{q}_{i}^{O} \in \mathbb{R}^{s\times s \times d} \}_{i=1:n}$ with spatial size $s \times s$, corresponding to the annotated bounding boxes are computed by the \textit{object prototype extraction module}, which considers the annotated objects shape and appearance properties (detailed in Section~\ref{sec:object_queries}). The image features $\mathbf{f}^{E}$ are depth-wise correlated with the prototypes. Each prototype thus generates a multi-channel similarity tensor $\tilde{\boldsymbol{R}}_i$, i.e.,
\begin{equation}  \label{eq:correlation}
    \tilde{\boldsymbol{R}}_i = \mathbf{f}^{E} * \mathbf{q}_{i}^{O},
\end{equation}
where $(*)$ is a depth-wise correlation. The individual $n$ prototype similarity tensors are fused by a per-channel, per-pixel max operation, yielding a joint response tensor
$\tilde{\boldsymbol{R}} \in \mathbb{R}^{h\times w\times d}$. 

Finally, a regression head %akin to~\cite{safecount} 
predicts the final 2D density map $\boldsymbol{R} \in \mathbb{R}^{H_{IN}\times W_{IN}}$. The regression head consists of three $3\times 3$ convolutional layers with 128, 64 and 32 feature channels, each followed by a Leaky ReLU, a $2\times$ bilinear upsampling layer, and a linear $1 \times 1$ convolution layer followed by a Leaky ReLU. The number of objects in the image $N$ is estimated by summing the density map values, i.e., $N = \mathrm{sum}(\boldsymbol{R})$.

\subsection{Object prototype extraction module}\label{sec:object_queries}

The object prototype extraction module (OPE) (Figure~\ref{fig:objectness}) constructs $n$ object prototypes 
$\{ \mathbf{q}_{i}^{O} \}_{i=1:n}$, with $\mathbf{q}_{i}^{O} \in \mathbb{R}^{s\times s \times d}$, using the image feature map $\mathbf{f}^{E} \in \mathbb{R}^{h\times w\times d}$ and the set of $n$ bounding boxes $\{ b_i \}_{i=1:n}$. 
Ideally, the prototypes should generalize over the appearance of the selected object category in the image and retain good localization properties. 
% This is achieved by initializing the prototypes with the annotated objects shape information, then iteratively transferring the appearance of the remaining non-annotated objects into the final prototypes, using the annotated objects appearance to supervise the adaptation. We describe this process in detail here.
Shape information is injected by initializing the prototypes with exemplar width and height features. The appearance of the remaining objects is then iteratively transferred into the final prototypes, with the exemplar appearance supervising the process. We details this process next.



\begin{figure}[!h]
    \centering
    \includegraphics[width=1.0\linewidth]{figures/objectness-module.pdf}
    \caption{Object prototype extraction module (OPE). Shape and appearance queries are extracted separately and iteratively adapted considering the image-wide information into $n$ object prototypes.
    }
    \label{fig:objectness}
\end{figure}

First, $n$ appearance queries $\mathbf{q}_{i}^{A} \in \mathbb{R}^{s\times s \times d}$ are extracted from the annotated objects by RoI pooling~\cite{maskrcnn} the image features $\mathbf{f}^{E}$ from individual bounding boxes $b_i$ into  $s \times s$ tensors.
The pooling operation makes the appearance queries shape-agnostic, since it maps features from different spatial shapes into rectangular queries of the same size.
We introduce shape queries $\mathbf{q}_{i}^{S}$ to recover the lost information as follows. %shape-specific objectness -> shape

The shape query corresponding to the $i$-th bounding box is computed by a nonlinear mapping $\mathbb{R}^2 \rightarrow \mathbb{R}^{s \times s \times d}$ of its width  and height $[b_i^w, b_i^h]$ into a high-dimensional tensor $\mathbf{q}_{i}^{S} = \phi([b_i^w, b_i^h])$. The mapping $\phi(\cdot)$ is implemented as a three-layer feed-forward network ($2 \rightarrow 64 \rightarrow d \rightarrow s^2 d$) with ReLU activations following each linear layer.

The shape and appearance queries are converted into object prototypes by an iterative adaptation module (Figure~\ref{fig:fusion}) using a recursive sequence of cross-attention blocks. 
%The information transfer from the image into object prototypes is carried out by an iterative adaptation module (Figure~\ref{fig:fusion}) as a recursive sequence of cross-attention blocks. 
Specifically, the shape queries $\mathbf{q}_{i}^{S}$ are reshaped into a matrix $\mathbf{Q}^{S} \in \mathbb{R}^{n s^2 \times d}$ and in the same way the appearance queries $\mathbf{q}_{i}^{A}$ and image features $\mathbf{f}^{E}$ are reshaped into $\mathbf{Q}^{A} \in \mathbb{R}^{n s^2 \times d}$ and $\mathbf{F}^{E} \in \mathbb{R}^{hw \times d}$, respectively. The adaptation iteration then follows the sequence
\begin{align}
    \mathbf{Q}_\ell' & = \text{MHA}(\text{LN}(\mathbf{Q}_{\ell-1}), \mathbf{Q}^{A},  \mathbf{Q}^{A}) + \mathbf{Q}_{\ell-1} \label{eq:fusion1}\\
    \mathbf{Q}_\ell'' & = \text{MHA}(\text{LN}(\mathbf{Q}_\ell'), \mathbf{F}^{E},  \mathbf{F}^{E}) + \mathbf{Q}_\ell' \label{eq:fusion2}\\
    \mathbf{Q}_\ell & = \text{FFN}(\text{LN}(\mathbf{Q}_\ell'')) + \mathbf{Q}_\ell''\label{eq:fusion3},
\end{align}
where the inputs at $\ell = 0$ are initialized by the shape queries (i.e., $\mathbf{Q}_0 = \mathbf{Q}^{S}$), MHA is the standard multi-head attention~\cite{transformer}, LN is layer normalization and FFN is a small feed-forward network. The process is performed for $L$ iterations, i.e., $\ell \in \{1,...,L\}$.
%Architecture of the fusion module is shown in Figure~\ref{fig:fusion}.
The output $\mathbf{Q}_L \in \mathbb{R}^{n s^2 \times d}$ is finally reshaped into a set of $n$ object prototypes $\mathbf{q}_{i}^{O} \in \mathbb{R}^{s\times s\times d}$.

\begin{figure}[!t]
    \centering
    \includegraphics[width=1.0\linewidth]{figures/fusion-module.pdf}
    \caption{The iterative adaptation module applies attention to gradually generalize prototypes to the object instances indicated by few input exemplars.}
    \label{fig:fusion}
\end{figure}

\subsubsection{Adaptation for zero-shot setup}\label{sec:zeroshot}

In the zero-shot scenario, the annotation-specific shape and appearance queries cannot be extracted due to absence of object annotations. Thus a minor modification of the OPE module is required to compute the object prototypes $\mathbf{q}_{i}^{O}$. In particular, the step (\ref{eq:fusion1}) is skipped, and $\mathbf{Q}'_\ell$ is initialized by trainable objectness queries $\mathbf{q}_{i}^{S'} \in \mathbb{R}^{s\times s \times d}$. The iterative adaptation module computational sequence then becomes (\ref{eq:fusion2}) and (\ref{eq:fusion3}).

\subsection{Training loss}  \label{sec:loss}

LOCA is trained using the $\ell_2$ loss between the predicted density map $\boldsymbol{R}$ and the ground-truth map $\boldsymbol{\hat{G}}$ normalized by the number of objects,
\begin{equation}
    \label{eq:ose}
    \mathcal{L}_{OSE} = \frac{1}{M}||\boldsymbol{\hat{G}} - \boldsymbol{R}||_2^2,
\end{equation}
where $M$ is the number of objects in the mini-batch.
The normalized loss emphasizes the errors in images with many objects, which usually contain the most challenging situations with high local object densities. 
%This loss penalizes examples with more objects in the image, which we found useful since these are usually the most challenging examples.

Auxiliary losses are added to better supervise the training of the iterative adaptation module (Figure~\ref{fig:fusion}). 
In particular, every intermediate output $\mathbf{Q}_\ell$ is reshaped into $n$ queries $\{ \mathbf{q}_i^{\ell} \in \mathbb{R}^{s\times s \times d} \}_{i=1:n}$ and applied to image features $\mathbf{f}^{E}$ as in~(\ref{eq:correlation}), generating an intermediate multi-channel response tensor $\tilde{\boldsymbol{R}}_i^{\ell}$. This is followed by the max operation and regression head to obtain an intermediate density map $\boldsymbol{R}^{\ell}$. The auxiliary loss is then computed as
%The per-channel, per-pixel maximum operation and an upsampling and a regression head are applied to each $\tilde{\boldsymbol{R}}_i^{\ell}$ to get final intermediate density map $\boldsymbol{R}^{\ell}$, similarly as $\boldsymbol{R}$ is computed in the main pipeline. 
\begin{equation}
    \label{eq:aux_loss}
    \mathcal{L}_{AUX} =  \frac{1}{M} \sum_{\ell=1}^{L-1} ||\boldsymbol{\hat{G}} - \boldsymbol{R}^{\ell}||_2^2.
\end{equation}
The final loss is thus $\mathcal{L} = \mathcal{L}_{OSE} + \lambda_{AUX} \mathcal{L}_{AUX}$, where $\lambda_{AUX}$ is the auxiliary loss weight. 


\section{Experiments}

\subsection{Implementation details}

% \textbf{Architecture details.} 
% LOCA resizes the input image to $512 \times 512$ and applies a SwAV~\cite{swav} pretrained ResNet50 for backbone with the features from the final three blocks upsampled to $64 \times 64$ pixels. 
% LOCA resizes the input image to $H_{IN}=W_{IN}=512$ pixels and applies a SwAV~\cite{swav} pretrained ResNet50 for backbone with the features from the final three blocks upsampled to $h=w=64$ pixels.
% In our experiments, the object prototypes spatial size is $s \times s$ with $s=3$, the number of adaptation layers is set to $L = 3$, and eight % heads are used in the MHA module with dropout~\cite{dropout} probability 0.1 after every MHA and the FFN module (Section~\ref{sec:object_queries}).

\textbf{Architecture details.} 
% LOCA resizes the input image to $512 \times 512$ and applies a SwAV~\cite{swav} pretrained ResNet50 for backbone with the features from the final three blocks upsampled to $64 \times 64$ pixels.
LOCA resizes the input image to $H_{IN}=W_{IN}=512$ pixels and applies the SwAV~\cite{swav} pretrained ResNet50 backbone with the features from the final three blocks upsampled to $h=w=64$ pixels. This results in an activation map with 3584 channels, which is further projected into $d=256$ channels by a $1 \times 1$ convolutional layer. The global self-attention block is a transformer encoder~\cite{detr, transformer} with 3 layers. MHA modules consist of 8 attention heads with the hidden dimension $d=256$, while the FFN has the hidden dimension of 1024. Dropout~\cite{dropout} is applied after every MHA and FFN module with probability 0.1. The iterative adaption module contains $L=3$ layers with the same MHA and FFN dimensions. The object prototype spatial size is $s \times s$ with $s=3$, while the dropout is not used. The ground truth density maps are generated by placing unit densities on object locations and smoothing with the Gaussian kernel, whose size is determined for each image separately. In particular, the kernel size is determined as $1/8$ of the average exemplar bounding box size.



\textbf{Training details.} Standard training image augmentation is applied, such as %continuous 
tiling, horizontal flipping and color jitter~\cite{countr}. The backbone network parameters are frozen, while all other LOCA parameters are trained for 200 epochs using the AdamW~\cite{adamw} optimizer with the fixed learning rate $10^{-4}$ and weight decay $10^{-4}$. The auxiliary loss weight in (\ref{eq:aux_loss}) is set to $\lambda_{AUX}=0.3$ and gradient clipping with maximum norm of 0.1 is used. LOCA is trained on two Tesla V100 GPUs with batch size 8 (4 images per GPU) for approximately 10 hours.


%We train for 200 epochs using the AdamW~\cite{adamw} optimizer with the fixed learning rate $10^{-4}$ and weight decay $10^{-4}$. 
%The auxiliary loss weight $\lambda_{AUX}$ is set to 0.3.
% Additionally, we apply auxiliary losses with weight 0.3 to every layer of the exemplar model predictor. 
%We apply gradient clipping with the maximum norm of 0.1. We train on two Tesla V100 GPUs with batch size 8 (4 images per GPU). The training takes approximately 10 hours.

% Ne vem če je ravno potrebno razlagati tako dolgo o arhitekturi
%The transformer encoder and exemplar model predictor consist of 3 layers ($L_e =3$, $L_d = 3$). All multi-head attention mechanisms use 8 heads ($h =8$). The hidden dimension is set to $d=256$ throughout the model with the exception of FFNs in transformer encoder and exemplar model predictor, where the dimension is $d=2048$. We apply dropout~\cite{dropout} with probability 0.1 after every multi-head attention module and the FFN module. We use Layer normalization~\cite{layernorm} with parameter $\varepsilon=10^{-5}$. The object prior predictor is a 3 layer FFN ($2 \rightarrow 64 \rightarrow d \rightarrow s^2d$) with linear layers followed by ReLU activation. We use the exemplar model size $s=3$. The regression head consists of 4 layers ($256 \rightarrow 128 \rightarrow 64 \rightarrow 32 \rightarrow 1$). First three layers consist of a $3 \times 3$ convolution, followed by Leaky ReLU activation and $2 \times$ bilinear upsampling, while the last layer consists of a $1 \times 1$ convolution and Leaky ReLU. Ground truth density maps are generated as described in Section~\ref{sec:density_maps}. For the few-shot scenario, we use size-based generation, while in the zero-shot scenario, we use the distance-based generation approach. We train for 200 epochs using the AdamW~\cite{adamw} optimizer with the fixed learning rate $10^{-4}$ and weight decay $10^{-4}$. Additionally, we apply auxiliary losses with weight 0.3 to every layer of the exemplar model predictor. We apply gradient clipping with the maximum norm of 0.1. We train on two Tesla V100 GPUs with batch size 8 (4 images per GPU). The training takes approximately 10 hours. Everything is implemented in PyTorch~\cite{pytorch}. 

\subsection{Comparison with the state of the art}
\label{sec:sota}

%{\bf The FSC147 dataset.}
%FSC147~\cite{famnet} is a recently proposed few-shot counting dataset and is the first of its kind. It contains 6135 images of 147 object categories split into training, validation and test sets consisting of 3659, 1286 and 1190 images, respectively. The sets of object categories present in each split are disjoint. Each image is accompanied by three bounding box annotations corresponding to exemplar objects and point annotations for all objects. All the annotated objects in a single image are of the same class.
LOCA is evaluated on the recent few-shot counting dataset FSC147~\cite{famnet}. The dataset contains 6135 images of 147 object categories split into training, validation and test sets consisting of 3659, 1286 and 1190 images, respectively. The sets of object categories present in each split are disjoint. Each image annotation consists of three bounding boxes of exemplar objects and point annotations for all objects of the same category as the exemplars.


%{\bf Evaluation on few-shot counting.}
%In the few-shot counting scenario, we compare our method to the baseline FamNet~\cite{famnet} and most recent state-of-the-art methods BMNet+~\cite{bmnet} and SAFECount~\cite{safecount}.  We follow the standard evaluation protocol~\cite{famnet, bmnet, safecount} and compute Mean Absolute Error (MAE) and Root of Mean Squared Error (RMSE) given the predicted and ground truth object counts (density map integrals). As shown in \cref{tab:results_fs}, LOCA outperforms all methods with relative improvement of 33.0~\%, 31.0~\%, 24.7~\% and 33.4~\% in MAE and RMSE on validation and test sets, respectively. The results for other methods are taken from their respective papers and we therefore do not have access to their uncertainty estimates. In this and all the subsequent experiments, our models' standard errors (estimated by bootstrap~\cite{bootstrap}) are below 1.0 for the validation set and below 1.5 for the test set. With this in mind, we omit the standard errors for brevity.

In the few-shot counting scenario, we compare LOCA with GMN~\cite{gmn}, MAML~\cite{maml}, FamNet~\cite{famnet} and the most recent state-of-the-art methods CFOCNet~\cite{cfocnet}, BMNet+~\cite{bmnet}, SAFECount~\cite{safecount} and CounTR~\cite{countr}.  We follow the standard evaluation protocol~\cite{famnet, bmnet, safecount} and compute Mean Absolute Error (MAE) and Root of Mean Squared Error (RMSE) given the predicted and ground truth object counts.  

Results are summarized in Table~\ref{tab:results_fs}. LOCA substantially outperforms all methods with a relative improvement of 22.0~\%, 9.7~\% in terms of MAE on validation and test sets, respectively, and 31.0~\% and 33.4~\% in terms of RMSE and sets a solid new state-of-the-art. Note that LOCA significantly outperforms even the most recent CounTR~\cite{countr}, which applies post-hoc error compensation routines (i.e., it estimates a correction factor for adjusting the estimated count).

\begin{table}[htbp]
    \centering
    \begin{tabular}{l l l l l}%{l c c c c}
        \toprule
        \multirow{2}{*}{Method}& \multicolumn{2}{c}{Validation set} & \multicolumn{2}{c}{Test set} \\
        & MAE & RMSE & MAE & RMSE \\ 
        \midrule
        GMN~\cite{gmnclass} & 29.66 & 89.81 & 26.52 & 124.57 \\
        MAML~\cite{maml} & 25.54 & 79.44 & 24.90 & 112.68 \\
        FamNet~\cite{famnet} & 23.75 & 69.07 & 22.08 & 99.54 \\
        CFOCNet~\cite{cfocnet} & 21.19 & 61.41 & 22.10 & 112.71 \\        
        BMNet+~\cite{bmnet} & 15.74 & 58.53 & 14.62 & 91.83 \\
        SAFECount~\cite{safecount} & \third{15.28} & \second{47.20} & \third{14.32} & \second{85.54} \\
        CounTR~\cite{countr} & \second{13.13} & \third{49.83} & \second{11.95} & \third{91.23} \\
        LOCA (ours) & \first{10.24} & \first{32.56} & \first{10.79} & \first{56.97} \\
        \bottomrule
    \end{tabular}
    \caption{Evaluation on a few-shot counting scenario.}
    \label{tab:results_fs}
\end{table}

For further insights we inspect the count errors with respect to the number of objects in the image (Figure \ref{fig:obj_count}).
LOCA outperforms the state-of-the-art across the different object numbers and most significantly outperforms the state-of-the-art on images with very high object counts. These typically contain extremely high object densities, presenting substantial challenge to all previous methods. But LOCA copes very well even with these cases, reducing the count errors by nearly $50\%$ compared to state-of-the-art.

%Figure \ref{fig:obj_count} shows the results on FSC-147 image subsets split by object count. LOCA achieves state-of-the-art performance on image subsets with a lower number of objects, but outperforms competing methods significantly on difficult examples with high object densities. In extreme scenarios with very high object counts LOCA improves the MAE by almost $50\%$ compared to other methods.

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{figures/object_count_performance_3.pdf}
    \caption{LOCA excells most in the highly challenging dense scenarios and outperforms state-of-the-art accross the density levels.
    % {\color{black} Performance of LOCA, BMNet+~\cite{bmnet} and CounTR~\cite{countr} on FSC-147 with respect to the number of objects.}
    % % image subsets of varying object counts. 
     }
    \label{fig:obj_count}
\end{figure}



%{\bf Evaluation on one-shot counting.}

\subsubsection{Evaluation on one-shot counting}
%For fair comparison, we re-train LOCA with only one exemplar per image.
We inspect performance under minimal user supervision with a single annotation -- a one-shot scenario. LOCA is compared with LaoNet~\cite{laonet}, which is designed specifically for one-shot scenarios,% and does not support multiple exemplar objects.
%We also compare LOCA 
as well as with the recent methods GMN~\cite{gmn}, CFOCNet~\cite{cfocnet}, FamNet~\cite{famnet}, BMNet+~\cite{bmnet} and CounTR~\cite{countr} which were specialized for the one-shot setting.
The results are shown in Table~\ref{tab:results_os}. LOCA outperforms the current state-of-the-art with a relative improvement of 13.6~\% MAE on the validation set, and $23.5\%$ and $16.3\%$ RMSE on validation and test set, respectively. This empirically confirms that LOCA generalizes well also to the minimal supervision counting case.

\begin{table}[htbp]
    \centering
    \begin{tabular}{l l l l l}%{l c c c c}
        \toprule
        \multirow{2}{*}{Method}& \multicolumn{2}{c}{Validation set} & \multicolumn{2}{c}{Test set} \\
        & MAE & RMSE & MAE & RMSE \\ 
        \midrule
        GMN~\cite{gmnclass} & 29.66 & 89.81 & 26.52 & 124.57 \\
        CFOCNet~\cite{cfocnet} & 27.82 & 71.99 & 28.60 & 123.96 \\ % to dobil iz laonet paperja       
        FamNet~\cite{famnet} & 26.55 & 77.01 & 26.76 & 110.95 \\
        BMNet+~\cite{bmnet} & 17.89 & 61.12 & 16.89 & \third{96.65} \\
        LaoNet~\cite{laonet}& \third{17.11} & \third{56.81} & \third{15.78} & 97.15 \\
        CounTR~\cite{countr}& \second{13.15} & \second{49.72} & \first{12.06} & \second{90.01} \\
        LOCA (ours) & \first{11.36} & \first{38.04} & \second{12.53} & \first{75.32} \\
        \bottomrule
    \end{tabular}
    \caption{Evaluation on a one-shot counting scenario.}
    \label{tab:results_os}
\end{table}

%{\bf Evaluation on zero-shot counting.}
\subsubsection{Evaluation on zero-shot counting}

%LOCA can be adapted to work in the zero-shot scenario as described in Section~\ref{sec:zero_shot}. We compare it to the state-of-the-art method in zero-shot counting~\cite{hobley}. LOCA achieves relative improvements of 18.0~\%, 9.0~\%, 20.5~\% and 0.8~\% in terms of MAE and RMSE on validation and test sets, respectively (\cref{tab:results_zs}). The results validate the claim that LOCA's architecture is easily modified into a zero-shot architecture and that the learned object priors are expressive enough to encode the general concept of objects that adapts to the objects in the query image, resulting in a strong counter that significantly outperforms the most recent state-of-the-art architecture specifically designed for zero-shot counting.

As noted in Section~\ref{sec:zeroshot}, LOCA can be easily applied to the unsupervised counting scenario with no user annotations, i.e., the zero-shot setup. 
We thus compare LOCA with zero-shot CounTR~\cite{countr} and state-of-the-art methods RepRPN-C~\cite{reprpn} and RCC~\cite{hobley} which are specialized for zero-shot counting. 
The results in Table~\ref{tab:results_zs} show that LOCA achieves relative improvements of 6.5~\%, and 0.5~\% in terms of RMSE on validation and test sets, respectively, compared to the state-of-the-art, and outperforms all zero-shot specialized architectures. This confirms that the proposed OPE module successfully adapts the trainable objectness queries into strong object prototypes capable of accurate count estimation even in the extreme case without manually annotated exemplars.
%, thus allowing LOCA to seamlessly address the extreme counting problem, achieving state-of-the-art results.

%Compared to the state-of-the-art LOCA achieves relative improvements of 6.5~\%, and 0.5~\% in terms of RMSE on validation and test sets, respectively (\cref{tab:results_zs}). LOCA's architecture is easily modified into a zero-shot architecture, where the learnable object priors encode the concept of objectness that generalizes well to the objects in the input image, resulting in a strong zero-shot counter that achieves state-of-the-art results.
%The results validate the claim that LOCA's architecture is easily modified into a zero-shot architecture and that the learned object priors are expressive enough to encode the general concept of objects that adapts to the objects in the query image, resulting in a strong counter that significantly outperforms the most recent state-of-the-art architecture specifically designed for zero-shot counting.

\begin{table}[htbp]
    \centering
    \begin{tabular}{l l l l l}%{l c c c c}
        \toprule
        \multirow{2}{*}{Method}& \multicolumn{2}{c}{Validation set} & \multicolumn{2}{c}{Test set} \\
        & MAE & RMSE & MAE & RMSE \\ 
        \midrule
        RepRPN-C \cite{reprpn} & 29.24 & 98.11 & 26.66 & 129.11 \\
        RCC \cite{hobley} & \third{17.49} & \second{58.81} & \third{17.12} & \second{104.53} \\
        CounTR \cite{countr} & \first{17.40} & \third{70.33} & \first{14.12} & \third{108.01} \\
        %LOCA (ours) & \textbf{16.72} & \textbf{58.78} & \textbf{17.01} & \textbf{102.68} \\
        % damo size-based gt za enotnost. 
        LOCA (ours) & \second{17.43} & \first{54.96} & \second{16.22} & \first{103.96} \\
        \bottomrule
    \end{tabular}
    \caption{Evaluation on a zero-shot counting scenario.}
    \label{tab:results_zs}
\end{table}

%{\bf Qualitative results.}
\subsubsection{Qualitative few-shot counting results}

\begin{figure*}
    \centering
    \includegraphics[width=\linewidth]{figures/sota_comparison_4.pdf}
    \caption{Qualitative results on the FSC147 dataset. Compared to related works, LOCA better discriminates between objects and background, predicts density maps with clear object locations and works well on smaller objects, while better capturing the intra-class variability and within-image size variability.}
    \label{fig:sota}
\end{figure*}

%We compare the predictions of our method with those of FamNet~\cite{famnet} and BMNet+~\cite{bmnet} (they are the only two methods with publicly available code) in \cref{fig:sota}. First, we can see that LOCA predicts density maps with clear object locations. Because of the exemplar model predictor, our method discriminates between the objects and the background much better (rows 1--3). In the second row, FamNet achieves almost perfect count but the density map shows that it puts density on parts of the image that do not contain objects. In contrast, our method puts density only on object locations. In rows 4--6, we see that our model works better on very small objects. This is most likely due to the explicit object prior modelling. At such small sizes, the features extracted from exemplar bounding boxes are not very informative. In contrast, our model combines them with prior knowledge about objects of that size. Our model is also more robust to large intra-category variability. The objects in row 6 are visually dissimilar due to different patterns being present on them. The model most likely utilizes the prior knowledge about possible intra-category variations to determine that all the stamps are from the same category. Finally, the object prior information also helps our model's robustness to object size variability within the image. In row 7, FamNet predicts density map with wide regions on small apples, while BMNet+ misses several larger apples. Because of the prior knowledge about possible size variations, our model successfully localizes both large and small apples. Similarly, BMNet+ fails to put enough density on larger marbles in row 8. FamNet on the other hand puts too much density on such marbles because it may mistake them for multiple small marbles.

%Similarly to using the transformer encoder in our method, BMNet+~\cite{bmnet} also tries to learn a general, globally-aware feature space by using self-attention on the joint features of the query image and the exemplars. We compare the feature space of BMNet+ with our model's feature space in \cref{fig:sim}. LOCA learns a more discriminative feature space (e.g., rows 1, 2, and 7). In the third row, the similarity of bricks and apples is very high for both BMNet+ and LOCA. Despite that, our method's exemplar model predictor combines prior knowledge and apple features to predict a new model that ensures the discrimination between apples and bricks (see third row in \cref{fig:sota}). In row 6, we see that LOCA's transformer encoder has learned to handle the within-image variability in object size due to being used in combination with the object prior predictor and the exemplar model predictor. 

Figure~\ref{fig:sota} visualizes the predicted object density maps from LOCA and BMNet+~\cite{bmnet}. Note that LOCA produces density maps with high fidelity object localization. The prototypes generated by the OPE module discriminate well between the objects and the background (columns 1--3). In the second column, LOCA generates clear density peaks on the object centers. In columns 4, 5, 6 and 7 we see that LOCA outperforms BMNet+ on small objects, with objects localized far better in the density map. This is likely due to explicitly accounting for object shape and size by shape-specific objectness queries, in contrast to other methods that consider only scale-agnostic object appearance extraction.
%utilizes an explicit object prior that addresses the shortcoming of direct exemplar feature extraction and comparison that loses much of the necessary scale information. 
The shape-specific information enables LOCA to more robustly address the object size variability within the image. In column 8, BMNet+ misses several larger apples while LOCA accurately localizes apples of all sizes. Similarly, BMNet+ underestimates the density of larger marbles in column 9, while LOCA produces a much more crisp and accurate density map. Columns 10, 11 and 12 show examples with few objects. LOCA also performs well in such scenarios.

%This is likely due to the explicit object prior modelling. At such small sizes, the features extracted from exemplar bounding boxes are not very informative. In contrast, our model combines them with prior knowledge about objects of that size. Our model is also more robust to large intra-category variability.

%Similarly to using the transformer encoder in LOCA, BMNet+~\cite{bmnet} also tries to learn a general, globally-aware feature space by using self-attention on the joint features of the query image and the exemplars. We compare the feature space of BMNet+ with our model's feature space in \cref{fig:sim}. LOCA learns a more discriminative feature space (e.g., rows 1, 2, and 7). In the third row, the similarity of bricks and apples is very high for both BMNet+ and LOCA. Despite that, our method's exemplar model predictor combines prior knowledge and apple features to predict a new model that ensures the discrimination between apples and bricks (see third row in \cref{fig:sota}). In row 6, we see that LOCA's transformer encoder has learned to handle the within-image variability in object size due to being used in combination with the object prior predictor and the exemplar model predictor. 


%\begin{figure}
%    \centering
%    \includegraphics[width=1.0\linewidth]{figures/sim.pdf}
%    \caption{Qualitative comparison of the feature space learned by BMNet+ and our method. Maps represent similarity between a random object pixel (blue dot in the first column) and all other pixels.}
%    \label{fig:sim}
%\end{figure}

\subsection{Comparison with object detectors} 
\label{sec:obj_det}

In limited cases where large training sets are available, objects can be counted using pretrained object detectors.
%Counting problems can be also addressed by object detectors, albeit only in limited cases containing objects for which large training datasets are available. 
It is thus instructive to evaluate the general few-shot object counters in these specialized cases in comparison with the classical detectors. The FSC147 dataset~\cite{famnet} in fact provides image subsets Val-COCO and Test-COCO containing only categories for which abundant annotated training images are available in COCO~\cite{cocodata}. 

This allows comparing counting capabilities of LOCA with those of classical detectors FasterRCNN~\cite{fasterrcnn}, MaskRCNN~\cite{maskrcnn}, and RetinaNet~\cite{retinanet} as well as the recent few-shot counting state-of-the-art FamNet~\cite{famnet}, BMNet+~\cite{bmnet} and CounTR~\cite{countr}. The results are reported in Table~\ref{tab:results_coco}. LOCA achieves state-of-the-art performance, most significantly on Val-COCO with a relative $31\%$ MAE and $36\%$ RMSE improvement over the best method.

%significantly outperforms the current state-of-the-art on the Val-COCO subset with a relative $31\%$ MAE and $36\%$ RMSE improvement. LOCA also achieves state-of-the-art performance on the Test-COCO subset.
%277 and 282 images respectivel



\begin{table}[htbp]
    \centering
    \begin{tabular}{l l l l l}%{l c c c c}
        \toprule
        \multirow{2}{*}{Method}& \multicolumn{2}{c}{Val-COCO} & \multicolumn{2}{c}{Test-COCO} \\
        & MAE & RMSE & MAE & RMSE \\ 
        \midrule
        Faster-RCNN \cite{fasterrcnn} & 52.79 & 172.46 & 36.20 & 79.59 \\
        RetinaNet \cite{retinanet} & 63.57 & 174.36 & 52.67 & 85.86 \\
        Mask-RCNN \cite{maskrcnn} & 52.51 & 172.21 & 35.56 & 80.00 \\
        Famnet \cite{famnet} & 39.82 & 108.13 & 22.76 & 45.92 \\
        BMNet+ \cite{bmnet} & \third{26.55} & \third{93.63} & \third{12.38} & \first{24.76} \\
        CounTR \cite{countr} & \second{24.66} & \second{83.84} & \second{10.89} & \second{31.11} \\
        LOCA (ours) & \first{16.86} & \first{53.22} & \first{10.73} & \third{31.31} \\
        \bottomrule
    \end{tabular}
    \caption{Evaluation on the COCO object-detection counting dataset.}
    \label{tab:results_coco}
\end{table}


\subsection{Cross-dataset generalization}
\label{sec:cross_dataset}

We evaluate the cross-dataset generalization capabilities of LOCA using the established evaluation protocol from~\cite{famnet}. In that protocol, a method is trained on the FSC147 dataset~\cite{famnet} and evaluated on the CARPK dataset~\cite{carpk}, which is a car-counting dataset containing aerial images of parking lots, which are considerably different from the FSC147 images. To ensure there is no object class overlap between the training and test dataset, the car images are omitted from the FSC-147 training set. For counting purposes, twelve exemplars are sampled from the training CARPK images and used in all test CARPK images. 

%Following the work of FamNet~\cite{famnet} and BMNet+~\cite{bmnet}, we evaluate LOCA on the cross-dataset generalization scenario, by training LOCA on the FSC147 dataset~\cite{famnet} and testing on the CARPK dataset~\cite{carpk}. CARPK is a car counting dataset that contains aerial images of parking lots, which are considerably different from FSC147. To ensure no object class overlap between the training and test datasets, the images of the car category are omitted from the FSC-147 training set.
%Twelve exemplars are sampled from the training images and %the chosen exemplars and their corresponding training images are used to extract \cmnt{[MK] Napačne variable!} $\boldsymbol{w}_e$ and $\boldsymbol{w}_p$ which 
%and are used at inference for all test images. %We adopt the same evaluation setup as FamNet and BMNet+. 

The results are reported in Table~\ref{tab:cars}. LOCA achieves better cross-dataset generalization with a relative  4.5~\% MAE and 9.2~\% RMSE improvement compared to the most recently published state-of-the-art method BMNet+, thus setting a new dataset generalization state-of-the-art among the few-shot counting methods.

\begin{table}[htbp]
    \centering
    \begin{tabular}{c c c}
        \toprule
        Method & MAE & RMSE \\ 
        \midrule
        FamNet~\cite{famnet} & \third{28.84} & \third{44.47} \\
        BMNet+~\cite{bmnet} & \second{10.44} & \second{13.77} \\
        LOCA (ours) & \first{9.97} & \first{12.51} \\
        \bottomrule
    \end{tabular}
    \caption{Cross-dataset generalization experiment on CARPK~\cite{carpk}. 
    %The models are trained on FSC147 dataset~\cite{famnet} with the car object category excluded.
    }
    \label{tab:cars}
\end{table}

\subsection{Ablation study}
\label{sec:ablation}

%This section analyzes the design choices of LOCA.

We finally analyze the architectural design choices and examine the influence of the object-normalized loss and the auxiliary losses. 
The experiments are performed on the FSC147 dataset in the few-shot setting. 
We report the performance by averaging a certain measure on validation and test sets. 
% The reported performance drops are calculated using the average performance on validation and test sets.
% and the size-based density map generation.

%We train several LOCA models with individual architectural components removed and evaluate with the evaluation procedure from~\cref{sec:sota}. The following changes are required in removal of individual components: (i) The appearance queries $\boldsymbol{q}_i^A$ are removed by omitting the first attention mechanism (\cref{eq:model_predictor1}) in the object prototype extraction module.(ii) %Removing shape-specific objectness queries $\boldsymbol{q}_i^S$ 
%\cmnt{[MK] ?? Nikola, lahko tole popraviš? Ne vem kaj pomeni.}
%scale-aware objectness prior $\boldsymbol{w}_p$ requires omitting the first attention mechanism and replacing $\boldsymbol{w}_p$ with $\boldsymbol{w}_e$ in the second attention mechanism (\cref{eq:model_predictor2}). 
% (iii) When removing the exemplar model predictor $D$, the final exemplar model is replaced with $\boldsymbol{w}_p + \boldsymbol{w}_e$ or only one of them if the other is not in use.

\noindent{\bf Architecture design.} 
Table~\ref{tab:ablation_components} reports the performance of re-trained LOCA variants with individual computational blocks removed. 
%Several LOCA variants were retrained to expose the contributions of individual computational blocks (Table~\ref{tab:ablation_components}).
To evaluate the importance of global attention in the image features encoder block, we removed this block ($LOCA_\mathrm{no\_att}$) and observe a  $12\%$ MAE performance drop. This indicates the importance of image feature consolidation by attention, which likely brings objects of the same category closer at feature level. 
The impact of the OPE module is evaluated by removing it and extracting the object prototypes directly from the encoder image features by pooling the features of the exemplar regions ($LOCA_\mathrm{no\_ope}$). This results in significant performance drop in order of $34\%$ MAE.
Removing both, global attention and OPE ($LOCA_\mathrm{no\_att\_ope}$) leads to further performance drops of $\sim 39\%$ MAE compared to the original LOCA.

Next, we explored the importance of the exemplar shape information in addition to their appearance. A variant $LOCA_\mathrm{no\_shape}$ was constructed, which ignores the shape queries $\mathbf{q}_i^S $ by omitting the first attention block in the OPE module (Figure~\ref{fig:objectness}) and replacing 
$Q^S$ with $Q^A$ in the second attention block. We observe a $25\%$ MAE reduction compared to original LOCA. This confirms the importance of accounting for the shape information in addition to the appearance in OPE.

% We also analyzed the importance of the mapping function that transforms the exemplar width and height into shape-specific objectness queries (Section~\ref{sec:object_queries}). 
Importance of the mapping function that transforms the exemplar width and height into shape-specific objectness queries (Section~\ref{sec:object_queries}) is analyzed in the following. 
Instead of predicting the shape queries from exemplars, we replace them with trainable queries ($LOCA_\mathrm{pre\_shape}$). 
Results show a significant drop in performance compared to LOCA with a $27\%$ change in MAE, indicating that useful shape-specific objectness information is indeed extracted from the exemplars size parameters and that it significantly contributes to object localization and accurate counts.

We also analyzed the role of the first cross-attention in the first OPE iteration (Equation~\ref{eq:fusion1}) by replacing it with a simple summation:
$\mathbf{Q}_1' = \mathbf{Q}^{A} + \mathbf{Q}^S$.
This results in a $5\%$ increase of MAE and a $22\%$ increase of RMSE, which indicates that the first MHA in OPE should not be considered as a simple matching operation, but rather as a modulation of the prototype construction process by the exemplar shape information.
This information is unique for every exemplar, thus optimally adjusting the resulting prototype to localize the objects of interest. 
% However, the first MHA in OPE should not be considered as a simple matching operation, but rather as a modulation of the prototype construction process by the exemplar shape information (which is different for every exemplar), thus optimally adjusting the resulting prototype (correlation filter) to localize the target objects. 
% By replacing the first MHA with summation, we observe a $5\%$ increase in MAE and a $22\%$ increase in RMSE.


Finally, we analyzed the impact of the number of adaptation iterations $L$ in the iterative adaptation module (Section~\ref{sec:object_queries}) on the joined FSC-147 evaluation sets. 
Results are shown in Table~\ref{tab:ablation_L}. 
The choice of $L=3$ provides the best performance while maintaining a low model complexity.

\noindent{\bf Complexity.} As shown in Table~\ref{tab:params}, the proposed architecture has almost $3 \times$ less parameters and almost $10 \times$ less trainable parameters than CounTR while being comparable to other state-of-the-art methods in both the number of parameters and computational complexity.
These results demonstrate that excellent low-shot object counting performance of LOCA comes from the methodological improvements instead of increased complexity.



%First the impact of the OPE block and the encoder global self-attention block are evaluated. In $LOCA_{no\_ope}$ the entire object prototype extraction module is removed to evaluate its impact. The appearance queries $\boldsymbol{q}_i^A$ are then correlated with the encoder features. Removing OPE leads to a significant drop in performance. In $LOCA_{no\_att}$ the global self-attention block is removed and the encoder features are taken directly from the backbone encoder, the performance drops compared to the full method shows the importance of global information aggregation in counting. Similarly, in $LOCA_{corr\_only}$ the global self-attention block is removed from the encoder in addition to the object prototype extraction module. This demonstrates the importance of both the global information aggregation of the self-attention block as well as the exemplar feature adaptation provided by the object prototype extraction module. \textbf{(ii)} The impact of shape information in LOCA are evaluated in $LOCA_{scale\_query}$ and $LOCA_{no\_scale}$. In $LOCA_{no\_scale}$ the shape queries $\boldsymbol{q}_i^S$ are removed which requires omitting the first attention mechanism in the object prototype extraction module and replacing $Q^S$ with $Q^A$ in the second attention mechanism (Figure~\ref{fig:fusion}). In $LOCA_{scale\_query}$ the scale queries $\boldsymbol{q}_i^S$ are not removed as in $LOCA_{no\_scale}$ but are instead replaced with trainable queries. The significant drops in performance in $LOCA_{scale\_query}$ and $LOCA_{no\_scale}$ compared to the full method demonstrate the benefit of being able to utilize explicit scale information provided by the exemplars. 
%Val MAE 13.00 RMSE 44.61
%Test MAE 15.80 RMSE 122.87
\begin{table}[htbp]
    \centering
    \begin{tabular}{l c c c c}
        \toprule
        \multirow{2}{*}{Method}& \multicolumn{2}{c}{Validation set} & \multicolumn{2}{c}{Test set} \\
        & MAE & RMSE & MAE & RMSE \\ 
        \midrule
        $LOCA_\mathrm{no\_att\_ope}$  & 17.62 & 55.78 & 16.92 & 101.96 \\
        $LOCA_{no\_ope}$  & 16.24 & 57.41 & 15.53 & 96.23 \\
        $LOCA_{no\_shape}$   & 13.77 & 49.60 & 14.29 & 112.48 \\
        $LOCA_{pre\_shape}$   & 13.00 & 44.61 & 15.80 & 122.87 \\
        $LOCA_{no\_att}$ & 11.99 & 36.67 & 11.96 & 78.72 \\
        LOCA               & 10.24 & 32.56 & 10.79 & 56.97 \\
        \bottomrule
    \end{tabular}
    \caption{Ablation study of individual architectural components. $LOCA_\mathrm{no\_att\_ope}$ removes the entire global self-attention block from the encoder and the OPE module,  $LOCA_{no\_ope}$ removes the OPE module, $LOCA_{no\_shape}$ removes the use of the shape queries and $LOCA_{no\_att}$ removes the global self-attention block from the encoder.}
    \label{tab:ablation_components}
\end{table}



%In Table \ref{tab:ablation_L} the impact of the number of iterations $L$ in the iterative adaptation module (Section~\ref{?}) is evaluated b  The evaluation is performed on the joined validation and test set of the FSC-147. LOCA is mostly robust to the choice of $L$, achieving good results at all tested values, however the performance does not improve with an $L$ value higher than $L=3$.

%\begin{figure}
%    \centering
%    \includegraphics[width=0.8\linewidth]{figures/L_ablation.pdf}
%    \caption{Ablation of the number of iterations $L$ in the iterative adaptation module.}
%    \label{fig:ablation_L}
%\end{figure}
%array([11.07139742, 10.81697092, 10.50433764, 10.8900727 , 11.59975767, 11.04247981])
\begin{table}[htbp]
    \centering
    \begin{tabular}{c c c c c c c}
        \toprule
        L & 1 & 2 & 3 & 4 & 5 & 6\\ 
        \midrule
        MAE & 11.07 & 10.81 & 10.50 & 10.89 & 11.60 & 11.04 \\
        \bottomrule
    \end{tabular}
    \caption{Ablation of the number of iterations $L$ in the iterative adaptation module. %MAE results are reported on the joined FSC-147 validation and test set.
    }
    \label{tab:ablation_L}
\end{table}

\begin{table}[htbp]
\centering
\begin{tabular}{c c c c}
 \toprule
 \multirow{2}{*}{Method} & \multirow{2}{*}{GFLOPS} & \multicolumn{2}{c}{Number of parameters} \\
 & & Total & Trainable \\
 \midrule
 FamNet & 55 & 26M & 760k \\
 BMNet+ & 27 & 13M & 12M \\
 SafeCount & 366 & 32M & 20M  \\
 CounTR & 91 & 100M & 99M \\
 LOCA (ours) & 80 & 37M & 11M \\  
 \bottomrule
\end{tabular}
\caption{Computational complexity and the number of parameters.}
\label{tab:params}
\end{table}

\noindent{\bf Backbone and resolution.} 
Importance of the backbone pre-training regime, input image resolution and the prototype spatial size are presented in Table~\ref{tab:other}.
Replacing the SwAV-pretrained backbone with the ImageNet-pretrained one ($LOCA_{ImNet}$) results in only a slight performance drop (8\% MAE and 4\% RMSE). 
Reducing the input image resolution from $512 \times 512$ to $384 \times 384$ pixels ($LOCA_{384}$) leads to the 9\% performance drop in both MAE and RMSE. 
Without any hyperparameter modifications, $LOCA_{384}$ remains the top-performing method in three out of four metrics.
% Finally, LOCA is robust to the prototype spatial size. 
Changing the prototype spatial size $s$ from 3 to 1 ($LOCA_{s=1}$) or 5 ($LOCA_{s=5}$) does not lead to significant performance drops. 
MAE is reduced by 3\% and 10\% while RMSE is reduced by 6\% and 11\% for $s=1$ and $s=5$, respectively, which confirms that LOCA is not sensitive to the prototype spatial size. 
All these results further verify that the design of the OPE module is the main driver of LOCA's superior performance.
% The choices of backbone, input image resolution and the prototype spatial size can, in many cases, have a substantial influence on the final model performance. As these are often different among state-of-the-art low-shot counting methods, we explore their influence on LOCA's performance. 
% The results are reported in Table~\ref{tab:other}. By replacing the SwAV-pretrained backbone with the ImageNet-pretrained backbone ($LOCA_{ImageNet}$), LOCA experiences only a minor drop in performance. 
% Without any hyperparameter modifications, LOCA remains state-of-the-art with the best performance in three out of four metrics when the input image resolution is changed from $512 \times 512$ to $384 \times 384$ ($LOCA_{384\times 384}$). 
% Finally, LOCA is robust to to the exact spatial size of prototypes. Changing $s$ to 1 ($LOCA_{s=1}$) or 5 ($LOCA_{s=5}$) does not lead to significant performance drops.
% This further verifies that the design of OPE module is the main driver of LOCA's superior performance.


\begin{table}[htbp]
    \centering
    \begin{tabular}{l c c c c}
        \toprule
        \multirow{2}{*}{Method}& \multicolumn{2}{c}{Validation set} & \multicolumn{2}{c}{Test set} \\
        & MAE & RMSE & MAE & RMSE \\ 
        \midrule
        $LOCA_{ImNet}$         & 11.40 & 37.10 & 11.56 & 55.89 \\
        $LOCA_{384}$           & 10.26 & 32.62 & 12.75 & 65.34 \\
        $LOCA_{s=1}$           & 10.90 & 38.66 & 10.79 & 56.97 \\
        $LOCA_{s=5}$           & 11.11 & 35.47 & 12.27 & 65.08 \\
        LOCA                   & 10.24 & 32.56 & 10.79 & 56.97 \\
        \bottomrule
    \end{tabular}
    \caption{Impact of the backbone pre-training regime, input image resolution and the prototype spatial size on LOCA's performance.}
    \label{tab:other}
\end{table}



% \begin{itemize}
%     \item Removing scale-agnostic visual features $\boldsymbol{w}_e$ requires omitting the first attention mechanism (\cref{eq:model_predictor1}) in the exemplar model predictor.
%     \item Removing scale-aware objectness prior $\boldsymbol{w}_p$ requires omitting the first attention mechanism and replacing $\boldsymbol{w}_p$ with $\boldsymbol{w}_e$ in the second attention mechanism (\cref{eq:model_predictor2}). This corresponds to the standard few-shot counting pipeline, where exemplar features are directly used for similarity computation.
%     \item Removing transformer encoder $E$ requires replacing its output with the backbone features $\boldsymbol{x_b}$.
%     \item When removing the exemplar model predictor $D$, the final exemplar model is replaced with $\boldsymbol{w}_p + \boldsymbol{w}_e$ or only one of them if the other is not in use.
% \end{itemize}





%Note that at least one of scale-aware objectness prior or scale-agnostic visual features is required as the input to the exemplar model predictor.

% (A1 vs. A2, B1 vs. B2, C1 vs. C2)
%The results are shown in \cref{tab:ablation_components}. 
%Omitting the transformer encoder $E$ (A) brings a substantial performance drop. $E$ learns a globally-aware feature space which improves the performance on instances of high intra-class or withing-image size variability. %Transformer encoder $E$ (B1--B3) brings a huge performance gain because it captures intra-class variability and within-image size variability, learning a globally-aware feature space in which similar objects lie closely together. % This is useful when correlation is computed because responses on similar objects are similar.
%Omitting the object prototype extraction module $D$ (B) causes a minor performance drop, although a simple summation of $\boldsymbol{w_e}$ and $\boldsymbol{w_p}$ already gives a well-performing model (B3). 
%Finally, omitting $\boldsymbol{w}_p$ increases the error more in comparison with omitting $\boldsymbol{w}_e$ (C). Scale-agnostic visual features are partially contained in the query image features, less information is therefore lost when removing $\boldsymbol{w}_e$ compared to $\boldsymbol{w}_p$.


%Using positional encodings in the transformer encoder to encode spatial information lost when flattening an image into a sequence is already a standard approach. Here, we test the usefulness of the positional encodings used in the exemplar model predictor (see \cref{sec:exemplar_model_prediction}). As shown in \cref{tab:positional_encodings}, additionally encoding spatial information into the exemplar model predictor is beneficial in terms of performance. Furthermore, \cref{fig:query_pos_emb} demonstrates that the spatial information is useful and allows different attention heads to put focus on different subparts of the scale-agnostic visual features. In contrast, all heads attend to all parts of the scale-agnostic visual features when positional encodings are not used. 

%\begin{table}[htbp]
%    \centering
%    \begin{tabular}{c c c c c}
%        \toprule
%        \multirow{2}{*}{Positional encodings} & \multicolumn{2}{c}{Validation set} & \multicolumn{2}{c}{Test set} \\
%        & MAE & RMSE & MAE & RMSE \\ 
%        \midrule
%        \xmark & 10.73 & 33.68 & 12.49 & 77.07 \\
%        \cmark & 10.24 & 32.56 & 10.79 & 56.97 \\
%        \bottomrule
%    \end{tabular}
%    \caption{Ablation study on the $s \times s$ positional encodings in the exemplar model predictor.}
%    \label{tab:positional_encodings}
%\end{table}

%\begin{figure}
%    \centering
%    \includegraphics[width=1.0\linewidth]{figures/query_pos_emb.pdf}
%    \caption{Attention of the upper left pixel of the scale-aware objectness prior of a single exemplar object to all pixels of the scale-agnostic visual features of a single exemplar object with (columns 1 and 2) and without the positional encodings (columns 3 and 4). Different attention maps correspond to different heads. Attention with positional encodings is more localized. }
%    \label{fig:query_pos_emb}
%\end{figure}

%As described in Section~\ref{sec:loss}, we use a size-based ground truth density map generation. The experimental comparison between the size-based and distance-based generation is shown in \cref{tab:density_maps}. By better encoding the object size and introducing the aspect ratio into density maps, size-based density map generation boosts counting performance.
%\begin{table}[htbp]
%    \centering
%    \begin{tabular}{l c c c c}
%        \toprule
%         & \multicolumn{2}{c}{Validation set} & \multicolumn{2}{c}{Test set} \\
%        & MAE & RMSE & MAE & RMSE \\ 
%        \midrule
%        Distance-based & 11.46 & 38.29 & 13.83 & 76.15 \\
%        Size-based & 10.24 & 32.56 & 10.79 & 56.97 \\
%        \bottomrule
%    \end{tabular}
%    \caption{Influence of using distance-based and size-based generated ground truth density map.}
%    \label{tab:density_maps}
%\end{table}

%\\
\noindent{\bf Model supervision.} We explored the impact of object count normalization in $\mathcal{L}_{OSE}$ (Equation~\ref{eq:ose}) and the importance of using the auxiliary losses on OPE blocks (Section~\ref{sec:loss}). % and ground truth generation. 
Results are shown in Table~\ref{tab:ablation_loss_function}. Avoiding the object count normalization leads to a $11\%$ performance drop in terms of MAE. This shows the benefits of the object count normalization which places a larger penalty on images with larger object counts providing an emphasis on difficult cases with high local object densities.
%show that loss normalization by the batch object count improves performance. The reason may lie that this loss places a larger penalty to images with larger object counts.
Additionally removing the auxiliary losses leads to a $17\%$ performance drop in terms of RMSE. This drop in performance indicates that supervision on individual iterations in OPE is beneficial as it encourages the OPE module to provide informative features throughout the iterative process.

\begin{table}[htbp]
    \centering
    \resizebox{1.0 \linewidth}{!}{\begin{tabular}{c c c c c c}
        \toprule
        \multirow{2}{*}{$\mathcal{L}_{OSE}$} & \multirow{2}{*}{Auxiliary loss} & \multicolumn{2}{c}{Validation set} & \multicolumn{2}{c}{Test set} \\
        & & MAE & RMSE & MAE & RMSE \\ 
        \midrule
        \xmark & \xmark & 10.87 & 35.68 & 11.93 & 72.83 \\
        \xmark & \cmark & 10.86 & 31.89 & 12.83 & 62.73 \\
        \cmark & \cmark & 10.24 & 32.56 & 10.79 & 56.97 \\
        \bottomrule
    \end{tabular}}
    \caption{Ablation study on the object-normalized $\ell_2$ loss ($\mathcal{L}_{OSE}$) and the auxiliary losses after every block in OPE. The mark \xmark with $\mathcal{L}_{OSE}$ indicates that the standard $\ell_2$~\cite{famnet} loss is used.}
    \label{tab:ablation_loss_function}
\end{table}



%{\bf Tiling augmentation.}
%We explore the impact of not using the tiling augmentation and using the discrete $2 \times 2$ tiling compared to our novel continuous augmentation. The results are shown in \cref{tab:ablation_tiling}. As expected, not using the tiling augmentation brings down the performance significantly, especially in terms of RMSE. This is because tiling helps the model work better on smaller objects, which often occur with high frequency, and on images with large object counts. Continuous tiling is better than the fixed $2 \times 2$ tiling because it explores a wider range of object sizes, aspect ratios and counts.  

%\begin{table}[htbp]
%    \centering
%    \begin{tabular}{l c c c c}
%        \toprule
%         & \multicolumn{2}{c}{Validation set} & \multicolumn{2}{c}{Test set} \\
%        & MAE & RMSE & MAE & RMSE \\ 
%        \midrule
%        No tiling & 12.66 & 52.23 & 13.82 & 104.37 \\
%        $2 \times 2$ tiling & 10.80 & 37.25 & 10.79 & 62.54 \\
%        Continuous tiling & 10.24 & 32.56 & 10.79 & 56.97 \\
%        \bottomrule
%    \end{tabular}
%    \caption{Ablation study on the tiling augmentation.}
%    \label{tab:ablation_tiling}
%\end{table}

%{\bf Influence of the number of exemplars.}
%As already shown in \cref{tab:results_fs}, \cref{tab:results_os} and \cref{tab:results_zs}, our method yields good performance in few-shot, one-shot and zero-shot settings. Looking at the relative improvement between one-shot and few-shot settings (\cref{tab:rel_impr}), we can observe that we achieve a larger improvement in RMSE compared to MAE. This differs from other methods, where improvement is larger in MAE. A possible explanation for this would be that our method benefits from additional exemplars when errors are large, e.g., when a query image contains many objects (and consequently smaller objects) or a great intra-category variability is present and the model makes a large mistake. This is a useful property -- with little additional effort we can bring down errors on the hardest images.

%\begin{table}[htbp]
%    \centering
%    \resizebox{1.0 \linewidth}{!}{\begin{tabular}{l c c c c}
%        \toprule
%        \multirow{2}{*}{Method} & \multicolumn{2}{c}{Validation set} & \multicolumn{2}{c}{Test set} \\
%        & MAE & RMSE & MAE & RMSE \\ 
%        \midrule
%        FamNet & 10.55 \% & 10.31 \% & 17.48 \% & 10.28 \% \\
%        BMNet+ & 12.0 \% & 4.24 \% & 13.43 \% & 4.90 \% \\
%        LOCA (ours) & 9.86 \% & 14.41 \% & 13.89 \% & 24.36 \% \\
%        \bottomrule
%    \end{tabular}}
%    \caption{Relative improvement when comparing few-shot to one-shot counting.}
%    \label{tab:rel_impr}
%\end{table}



%\subsection{Additional experiments}
%\label{sec:additional}

%In this Section, we experimentally verify the correctness of some design choices that are not part of the main contributions, however they report an account of development of the LOCA final architecture and may be valuable for researchers working in this area.

%{\bf Backbone network.}
%In all our experiments, we use the ResNet50~\cite{resnet} backbone pre-trained with SwAV~\cite{swav}. This regime of pre-training has shown performance gains in many tasks compared to the ResNet pre-trained in a supervised manner on ImageNet~\cite{imagenet}. Alternatively, we could opt for a more lightweight backbone, such as ResNet18, which has been used in SAFECount~\cite{safecount}. During training, we keep our backbone frozen instead of fine-tuning it. This way, we prevent it from overfitting to the object categories in the training set. As shown in \cref{tab:backbone}, our choice gives the best performance. ResNet50 pre-trained on ImageNet gives slightly worse performance. Fine-tuning the ResNet50 is obviously not a good choice. On the other hand, ResNet18 seems to benefit from fine-tuning with learning rate 10 times smaller than that of the rest of the model. The reason for that might be that it is not expressive enough and it needs fine-tuning to learn to extract features suitable for few-shot counting and does not yet start overfitting.

%\begin{table*}[htbp]
%    \centering
%    \begin{tabular}{c l l c c c c}
%        \toprule
%        \multirow{2}{*}{Backbone} & \multirow{2}{*}{Pre-training} & \multirow{2}{*}{Learning rate} & \multicolumn{2}{c}{Validation set} & \multicolumn{2}{c}{Test set} \\
%        & & & MAE & RMSE & MAE & RMSE \\ 
%        \midrule
%        \multirow{3}{*}{ResNet18} & ImageNet & same & 15.92 & 53.53 & 16.41 & 99.01 \\
%        &                           ImageNet & 10$\times$ & 11.07 & 36.05 & 11.37 & 65.15 \\
%        &                           ImageNet & frozen & 12.51 & 40.39 & 12.28 & 59.94 \\
%        \midrule
%        \multirow{4}{*}{ResNet50} & ImageNet & frozen & 11.40 & 37.10 & 11.56 & 55.89 \\
%        &                           SwAV & same & 11.89 & 41.99 & 11.48 & 65.12 \\
%        &                           SwAV & 10$\times$ & 11.49 & 39.00 & 13.64 & 78.77 \\
%        &                           SwAV & frozen & 10.24 & 32.56 & 10.79 & 56.97 \\
%    \bottomrule
%    \end{tabular}
%    \caption{Choice of a backbone network, its pre-training strategy and the fine-tuning. Learning rate "frozen" corresponds to not fine-tuning the backbone during training. "Same" and "$10\times$" correspond to fine-tuning with the same learning rate and 10 times smaller learning rate compared to the rest of the model.}
%    \label{tab:backbone}
%\end{table*}

%{\bf Pre-LN vs. Post-LN transformer.}
%Pre-LN and Post-LN are two formulations of transformers that differ on the location of the Layer normalization. The formulation from Section~\ref{sec:method} corresponds to the Pre-LN transformer. If using the Post-LN formulation, the location of Layer normalization changes. For example, \cref{eq:encoder1} would become 
%\begin{equation}
%    x_\ell' = \text{LN}(\text{MSA}(x_{\ell - 1}) + x_{\ell - 1}).
%\end{equation}
%It has been proven~\cite{preln} that the Post-LN transformer formulation has problems at initialization with large expected gradients for parameters near the output. To achieve stable training, it needs a learning rate warm-up stage. On the other hand, the theory shows that the gradients of Pre-LN transformer are well-behaved at initialization and that better results can be obtained with less training time without the need for the learning rate warm-up. We verify this experimentally by keeping all the settings the same and replacing Pre-LN transformer with the Post-LN one. The results are shown in \cref{tab:transformer}.

%\begin{table}[htbp]
%    \centering
%    \begin{tabular}{l c c c c}
%        \toprule
%         & \multicolumn{2}{c}{Validation set} & \multicolumn{2}{c}{Test set} \\
%        & MAE & RMSE & MAE & RMSE \\ 
%        \midrule
%        Post-LN & 11.17 & 36.30 & 12.06 & 84.24 \\
%        Pre-LN & 10.24 & 32.56 & 10.79 & 56.97 \\
%        \bottomrule
%    \end{tabular}
%    \caption{Pre-LN vs. Post-LN transformer.}
%    \label{tab:transformer}
%\end{table}

%{\bf Exemplar model size.}
%We test a smaller and a larger exemplar model compared to the one we use in other experiments. The results are shown in \cref{tab:model_dim}. Model of size 1 means that all the spatial information is lost. The performance improves for model size 3 but drops again for model size 5. Too large model size may result in less accurate localization and too wide density regions around objects. 

%\begin{table}[htbp]
%    \centering
%    \begin{tabular}{c c c c c}
%        \toprule
%        Exemplar model size $s$ & \multicolumn{2}{c}{Validation set} & \multicolumn{2}{c}{Test set} \\
%        & MAE & RMSE & MAE & RMSE \\ 
%        \midrule
%        1 & 10.90 & 38.66 & 11.06 & 66.10 \\
%        3 & 10.24 & 32.56 & 10.79 & 56.97 \\
%        5 & 11.11 & 35.47 & 12.27 & 65.08 \\
%        \bottomrule
%    \end{tabular}
%    \caption{Influence of the exemplar model size ($s$) on the few-shot counting performance.}
%    \label{tab:model_dim}
%\end{table}

% \subsection{Failure cases}

% Representative cases with localization and counting failures are shown in \cref{fig:failure_cases}.
% %In \cref{fig:failure_cases}, we show some representative examples where LOCA fails to accurately localize the objects. 
% In the first row of \cref{fig:failure_cases}, LOCA mistakenly predicts non-zero density on some regions, whose appearance is similar to the annotated objects (strawberries). 
% In the second row, LOCA's prediction error is very high, however, this is primarily due to exemplar and test-time annotation inconsistencies.
% %significantly differs from the ground truth due to a ground truth annotation error. 
% The bounding boxes are drawn around three individual spokes, including one on the green base plate, thus defining these as the object exemplars. However, at the test-time, the ground truth annotations are put solely on the yellow and dark green bricks as a whole, whereas LOCA correctly localises the green spokes. 
% In the third and fourth rows, LOCA fails to localize dots that are either significantly occluded or out-of-focus, thus no longer matching the appearance distribution specified by the annotated exemplars. 
% In the fifth row, the errors are primarily caused by poor localization of some of the bottle caps due to their significant occlusion by other caps.

\subsection{Qualitative analysis}

Figure~\ref{fig:loca_vs_countr} qualitatively compares LOCA with the recent 
state-of-the-art method CounTR~\cite{countr}. LOCA demonstrates superior performance in counting small objects (first and second row), large objects (third row) and objects of mixed sizes (fourth and fifth row), which supports the proposed design. Figure~\ref{fig:loca_vs_modified} qualitatively compares LOCA with a version that does not use shape information and a version without the OPE module. The shape information injection and the adaptation in OPE module both contribute to accurate localization and counts.

% {\color{black}
% The proposed LOCA is qualitatively compared to the recent state-of-the-art method for low-shot object counting, CounTR~\cite{countr}. Results are shown in Figure~\ref{fig:loca_vs_countr}. 
% LOCA demonstrates superior performance when counting small objects (first and second row), large objects (third row) and objects of various sizes in the same image (fourth and fifth row). 
% In the Figure~\ref{fig:loca_vs_modified} the qualitative comparison of LOCA and a version without shape information (first two rows) or without the whole OPE module (last two rows) is shown. 
% }

\begin{figure}
    \centering
    %\includegraphics[width=\linewidth]{figures/failure_cases2.pdf}
    \includegraphics[width=\linewidth]{figures/shape_importance.pdf}
    % \caption{LOCA outperforms CounTR~\cite{countr} across scales.}
    \caption{LOCA demonstrates superior performance on images with only small objects (first and second row), images with only large objects (third row), as well as images with objects of mixed sizes (fourth and fifth row).}
    \label{fig:loca_vs_countr}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{figures/loca_vs_modified.pdf}
    \caption{
    LOCA compared to a variant that does not use shape information (first two rows) and a variant without the OPE module (last two rows). Objects across scales are most accurately localized and counted when using the shape information and OPE. 
    }
    \label{fig:loca_vs_modified}
\end{figure}


\section{Conclusion}


We presented a new low-shot counting method LOCA, that addresses the limitations of the current state-of-the-art methods. LOCA considers the exemplar shape and appearance properties separately and iteratively adapts these into object prototypes by a new object prototype extraction (OPE) module considering the image-wide features. The prototypes thus generalize to the non-annotated objects in the image, leading to better localization properties and count estimates. %A minor adjustment allows application of LOCA to zero-shot setups making it suitable for the entire low-shot counting spectrum.

%In few-shot setups, LOCA separately extracts the exemplar shape and appearance information and iteratively adapts these into object prototypes by a new object prototype extraction (OPE) module considering the image-wide features. The prototypes thus generalize to the non-annotated objects in the image, leading to better localization properties and count estimates. A minor adjustment allows application of LOCA to zero-shot setups making it suitable for the entire low-shot counting spectrum.

% Experiments show that LOCA outperforms state-of-the-art on the FSC147 public benchmark in one-shot and few-shot settings with relative improvements in test set RMSE of $16.3\%$ and $33.4\%$, respectively. Additionally, LOCA achieves state-of-the-art performance in the zero-shot setting. On the COCO subsets of FSC147, LOCA outperforms recent state-of-the-art counting methods, as well as object detection methods, achieving a $31\%$ improvement in terms of MAE and a $36\%$ improvement in terms of RMSE on the COCO-val subset. LOCA also achieves excellent cross-dataset generalization and outperforms competing methods. On the CARPK dataset, LOCA achieves a relative improvement of $4.5\%$ and $9.2\%$ in terms of MAE and RMSE, respectively. The quantitative results convincingly support the benefits of the new OPE module, which is our main contribution.
Experiments show that LOCA outperforms state-of-the-art on the FSC147 public benchmark in few-shot, one-shot and zero-shot settings. 
We observed 
a relative RMSE improvement of $33.4\%$ in few-shot and $16.3\%$ in zero-shot scenarios.
On the COCO subsets of FSC147, LOCA outperforms recent state-of-the-art counting methods, as well as object detection methods, achieving a $36\%$ RMSE improvement. 
On the CARPK dataset, LOCA achieves a relative improvement of $9.2\%$ RMSE, which demonstrates excellent cross-dataset generalization. 
The quantitative results convincingly support the benefits of the new OPE module, which is our main contribution.

We envision several possible future research directions. 
Additional supervision levels such as introducing negative exemplar annotations could be introduced in LOCA for better specification of the selected object class. 
This could lead to interactive tools for accurate object counting. Furthermore, a gap between low-shot counters and object detectors could be further narrowed by enabling bounding box or segmentation mask prediction in LOCA to output additional statistics about the counted objects such as average size, etc., which is useful for many practical applications such as biomedical analysis.
 

%There are a variety of avenues for future LOCA extensions. Improving general objectness representations could enable the training of general object queries that could be used in robust zero-shot counting. Additionally, a gap could be closed between low-shot counting methods and object detectors, enabling LOCA to output additional information such as bounding boxes or segmentation masks for the localized objects. Additional supervision levels such as introducing negative example annotations could be introduced in LOCA for further practical use-cases. These will be some our future research directions.

%The conditioning of low-shot counting methods could also be expanded to not only include bounding box exemplar annotation but also additional information such as negative exemplars or other information about the background which could prove useful in practical scenarios.

%LOCA achieves excellent performance on dense scenes containing many small objects and on images with large intra-class variability and within-image size variability. This is achieved by the integration of the proposed object prototype extraction module that extracts exemplar appearance and shape queries and adapts them into corresponding object prototypes that enable excellent object localization properties and consequently a highly accurate object count.

%Instead of directly using the exemplar features as a kernel for similarity metric, we employ the exemplar model predictor that transforms prior information about objects of the given size into the exemplar model by fusing it with the information extracted from exemplar objects and the information from the whole query image. LOCA learns a general globally-aware feature space to better capture the intra-category variability and within-image size variability of object instances. The LOCA architecture affords a minimal adjustment to be readily applied to zero-shot counting scenarios. To our knowledge, LOCA is the first architecture that simultaneously covers a few-shot as well as zero-shot counting spectrum. 

%We show that it predicts density maps with clearer object locations and achieves better discrimination between the objects and the background. It also works better on smaller objects and on images with large intra-class variability and within-image size variability. 
%The latter is achieved by the combination of a globally-aware feature extractor, the explicit size-dependent object prior modeling and the transformer-based exemplar model predictor. LOCA also achieves better cross-dataset generalization compared to the competing methods, with relative improvement of 4.5~\% in terms of test set MAE compared to the most recent state-of-the-art. 

%Nonetheless, the discrimination capability of our model could be further improved. There are many other possible future research directions. For example, a surrogate task for unsupervised model pre-training could be devised. That way, the models could be pre-trained on a large general dataset and fine-tuned on a few-shot counting dataset. Current evaluation methods do not account for the quality of predicted density maps nor the models' localization capability. It would be beneficial to develop a different evaluation framework, in which the model comparison does not rely solely on the final object counts. Building on this, exploring direct point supervision, which has recently gained attention in class-specific counting~\cite{pointsupervision}, could be promising. Finally, bounding boxes are not the best possible representation of exemplar objects. Sometimes, a bounding box will contain a large portion of the background. If furthermore an object is small, the extracted exemplar features will be unreliable. Annotating exemplar objects with a segmentation mask and developing models for that setting is another interesting research venue.

% \section{Acknowledgements}

\hfill %\break

% \footnotesize
{\noindent \bf Acknowledgements}:
This work was supported by Slovenian research agency program 
P2-0214 % CV program
and projects 
J2-2506, % Davimar
Z2-4459, % Postdoc
23-20MR.R588 % MR
and 
J2-3169. % Danijel - anomaly


%%%%%%%%% REFERENCES
{\small
\bibliographystyle{ieee_fullname}
\bibliography{egbib}
}
\end{document}