%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%2345678901234567890123456789012345678901234567890123456789012345678901234567890
%        1         2         3         4         5         6         7         8

\documentclass[letterpaper, 10 pt, conference]{ieeeconf}  % Comment this line out if you need a4paper
%\documentclass[letterpaper,10 pt,journal,twoside]{ieeetran} 
\usepackage{graphicx}
\usepackage{tabularx}
\usepackage{multirow}
\usepackage{soul, color, xcolor}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{etoolbox}
\makeatletter
\patchcmd{\@makecaption}
  {\scshape}
  {}
  {}
  {}
\makeatletter
\patchcmd{\@makecaption}
  {\\}
  {:\ }
  {}
  {}
\makeatother
\def\tablename{Table}

%\documentclass[a4paper, 10pt, conference]{ieeeconf}      % Use this line for a4 paper
\soulregister{\ref}7
\soulregister{\cite}7
\IEEEoverridecommandlockouts                              % This command is only needed if 
                                                          % you want to use the \thanks command

\overrideIEEEmargins                                      % Needed to meet printer requirements.

%In case you encounter the following error:
%Error 1010 The PDF file may be corrupt (unable to open PDF file) OR
%Error 1000 An error occurred while parsing a contents stream. Unable to analyze the PDF file.
%This is a known problem with pdfLaTeX conversion filter. The file cannot be opened with acrobat reader
%Please use one of the alternatives below to circumvent this error by uncommenting one or the other
%\pdfobjcompresslevel=0
%\pdfminorversion=4

% See the \addtolength command later in the file to balance the column lengths
% on the last page of the document

% The following packages can be found on http:\\www.ctan.org
%\usepackage{graphics} % for pdf, bitmapped graphics files
%\usepackage{epsfig} % for postscript graphics files
%\usepackage{mathptmx} % assumes new font selection scheme installed
%\usepackage{times} % assumes new font selection scheme installed
%\usepackage{amsmath} % assumes amsmath package installed
%\usepackage{amssymb}  % assumes amsmath package installed

\title{\LARGE \bf
Simulated Mental Imagery for Robotic Task Planning
}



\author{Shijia Li,
        Tomas Kulvicius,
        Minija Tamosiunaite,
        and Florentin W\"org\"otter
\thanks{The research leading to these results has received funding from the German Science Foundation WO 388/16-1 and the European Commission, H2020-ICT-2018-20/H2020-ICT-2019-2, GA no.:871352, ReconCycle.}
\thanks{T. Kulvicius, M. Tamosiunaite and F. W\"org\"otter are with the Inst. of Physics 3, Dept. Computational Neuroscience, University of G\"ottingen, 37073 G\"ottingen, Germany, e-mail: shijia.li@phys.uni-goettingen.de.}% <-this % stops a space
\thanks{T. Kulvicius is also with University Medical Center G\"ottingen, Child and Adolescent Psychiatry and Psychotherapy, 37075 G\"ottingen, Germany.}
\thanks{M. Tamosiunaite is also with the Faculty of Computer Science, Vytautas Mangnus University, Kaunas, Lithuania.}
%\thanks{Manuscript received April 19, 2005; revised August 26, 2015.}
}



\begin{document}



\maketitle
\thispagestyle{empty}
\pagestyle{empty}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{abstract}

Traditional AI-planning methods for task planning in robotics require symbolically encoded domain description. While powerful in well-defined scenarios, setting this up requires substantial effort. Different from this, most everyday planning tasks are solved by humans intuitively, using mental imagery of the different planning steps. Here we suggest that the same approach can be used for robots, too, in cases which require only limited execution accuracy. In the current study, we propose a novel sub-symbolic method called Simulated Mental Imagery for Planning (SiMIP), which consists of several steps: perception, simulated action, success-checking and re-planning performed on 'imagined' images. We show that it is possible this way to implement mental imagery-based planning in an algorithmically sound way by combining regular convolutional neural networks and generative adversarial networks. With this method, the robot acquires the capability to use the initially existing scene to generate action plans without symbolic domain descriptions, hence, without the need to define an explicit representation of the environment. We create a dataset from real scenes for a packing problem of having to correctly place different objects into different target slots. This way efficiency and success rate of this algorithm could be quantified.
\end{abstract}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{INTRODUCTION}

Task planning is the process of generating an action sequence to achieve a certain goal. To do this with conventional AI-planning one needs to rigorously define symbolic structuring elements: the planning domain including planning operators, pre- and post-conditions, as well as search/planning algorithms \cite{c4,c5,c6}. This is powerful in different complex scenarios as shown by many studies and applications. Different from this, most every-day planning tasks are solved by humans without explicit structuring elements (e.g., even without pen\&paper) and many times our plans are rather ad hoc, comprising few steps only and involving frequent success-checking and re-planning (\cite{c2}). The sequence of planning, acting, checking, and re-planning can be described as a process, which includes mental imagery. Here we ask to what degree such a process can be transferred to robot planning problems.

%Often such plans of ours are based on multiple mental imagery steps. We imagine and mentally simulate a – usually short – sequence of actions with the seen objects and form mental images of the outcome of a planned action, which may be used as the start of the next imagined action. Often this does not even much enter our conscious though and planning depth is limited but we posit that a process that uses imagining for planning, acting, checking, and re-planning will many times also suffice for robots in everyday situations.

Development of deep learning over the years by now shows outstanding performance in the field of image processing. In particular, Generative Adversarial Networks (GAN) perform impressively for realistic image generation \cite{c33, karras2019style}. Here we pursue the hypothesis that it should be possible to implement mental imagery-based planning in an algorithmically sound way by combining regular convolutional neural netwoks and generative adversarial networks (GAN). GANs can be used to generate realistic images based on semantic input or on input images that are to be transformed in some way. However, using GANs to realistically envision potential outcomes of robot manipulation and perception tasks imposes many challenges, because larger GANs tend to suffer from instabilities leading to difficulty in modeling object interactions. In this paper, we do not use GANs to generate complete future scenes, but concentrate on GAN-based "imagination" of completion of individual objects in the scene. 

We present a novel sub-symbolic method called Simulated Mental Imagery for Planning (SiMIP) for task planning consisting of the following components: perception, imagination of the action effect, success checking and (re)planning. In our approach we use a simulated mental imagery process, which creates images of the outcome of an imagined action, then we use the imagined outcome as the input for the next imagined action, and so on. This way we can create a planning tree for which conventional search algorithms can be used to arrive at an action sequence that leads to the goal. The central aspect here is that such a sub-symbolic planning mechanisms does not any longer require to (manually) define explicit, symbolic representations.

The paper is structured as follows. In section II we discuss related work. Subsequently, an overview of our approach is presented in section III and implementation details are described in section IV. In Section V, we present experiments and results, and, finally, in section VI we provide a conclusion and outlook.

\begin{figure*}[htbp]
  \centering
  \includegraphics[width=0.85\textwidth]{imgs/overall.pdf}
  \caption{Our system mainly contains two parts: scene understanding and action planning. For scene understanding we use three deep models, a) Object detection, b) Affordance\&Semantic segmentation, and c) Object complementation. The details of the training and inference process can be seen in Figure \ref{structure}. Through the scene understanding part we can get the complete shape of the background and each individual object and its affordance class. Then, we can apply actions such as move and rotate to the object and use the information obtained from the affordance map to check whether the action is valid or not. If it is valid, we can perform the next action.}  \label{concept}
\end{figure*}


\begin{figure*}[htbp]
  \centering
  \includegraphics[width=0.75\textwidth]{imgs/structure.pdf}
  \caption{Training and inference of our deep model. In training: a) object detection, b) instance \& affordance assignment, c) scene de-occlusion. In the training phase, we train the three models individually and then combine the obtained results in the inference phase. Note that after finishing the object complementation (c, above), we need to do affordance segmentation (b, above) again, to get the complete object corresponding to the affordance classes (see red pointers).}  \label{structure}
\end{figure*}


\section{RELATED WORK}

We will first briefly discuss symbolic as well as simulation-based planning to emphasize the differences to our approach. Following this, we will focus on affordance and imagination.

\subsection{Symbolic Planning}

Artificial intelligence planning techniques are the usual choice for decision-making for robotic execution of any task. They use a symbolic, logic-based notation compatible with human language that permits an intuitive domain specification \cite{c3}. Early steps in AI planning go back to the STRIPS planning method \cite{c4}, which has inspired several more recent planning approaches \cite{c5,c6}. Other planning techniques go a step forward and handle real world uncertainties using a probabilistic approach. These techniques \cite{c7,c8,c9,c10,c11} are mostly based on decision-theoretic planning, which introduces uncertainties in the probabilities associated to symbolic state transitions of a Markov decision process representation of the environment. Despite the recent progress in AI planning applied to robotics, these techniques are still subject to the symbolization problem mentioned before: all the relevant aspects for the successful execution of the robotic actions should be considered in the planning problem definition using scenario-specific domain descriptions. This is a problem as it is onerous and can needlessly increase the dimensionality of the search space, which may reduce computational efficiency (mainly in probabilistic planning approaches). More importantly, task-specific domain definitions also restrict transferability to other scenarios and tasks, transforming the robotic application into a snapshot of a behavior only useful for that specific scenario. Our system tries to addresses these limitations by proposing a new decision-making approach that uses a sub-symbolic representation disengaged from the specific symbolism required by AI planning methods.

\subsection{Simulation}

Simulation of robotic actions, with the emphasis of multi-body dynamics, has by now become a prevalent technique in many domains. There are several software packages for that, like Gazebo \cite{c12}, V-Rep \cite{c13}, KUKA.Sim \cite{c14}, Verosim \cite{c15}, just to name a few. Simulation packages exist also for handling soft materials \cite{c16}. Fusion of simulation, including simulation of sensing, and robot control in virtual environments is an important recent development \cite{c17}. A fairly accurate prediction of the effects of a robot action can be achieved based on such simulations \cite{c19,c20}. In order to perform any simulation, however, one needs robot models, object models, as well as a full specification of the scenario. In industrial tasks, CAD models of parts and setups are usually available. However, this is often not the case in everyday environments. In addition, when object models are available, their placement and pose has to be accurately simulated to represent the desired scenario. Hence, modern simulation techniques need to deal with two complexities: complex action modeling and complex scenario-setup and -update modelling. The latter is partly attempted by using methods to automatically generate the scenario, which could be further processed by means of the action-simulation. In this work, we are not concerned with industrial, high-precision robotic action. Instead, we are targeting the everyday-domain like for service robots at home. In this domain, most actions need only to be “fairly” accurate for our planning requirements and one should not be forced to simulate actions and their outcomes with highest precision. This constraint underlies our approach similar to human actions, which are also not always super precise.

%\begin{figure}[htbp]
%  \centering
%  \includegraphics[width=0.45\textwidth]{imgs/examples_of_dataset.pdf}
%  \caption{Examples of the dataset}  \label{examples of dataset}
%\end{figure}



\subsection{Affordance recognition}

The term “affordance” originates from cognitive psychology \cite{c21}. The set of affordances can be briefly described as the set of momentarily possible interactions between an animal and its environment. In robotics this term very often takes the meaning of: “Which actions could a robot perform in a given situation (with some given objects)?”  The goal of affordance segmentation is to assign probabilities for a set of affordances to every location in an image. While this specific problem is quite new, there has been some history of affordance-related approaches (e.g. semantic segmentation approaches) in the fields of computer vision and robotics. A straightforward problem is trying to estimate affordances of whole objects. This has been addressed in the work of \cite{c22} and \cite{c23}. When neural networks are employed for affordance segmentation, obviously their architecture is crucial. One of the first deep learning-based approaches \cite{c24} had enhanced and retrained the VGG16 network \cite{c25} to densely predict class labels for semantic segmentation, with averaging up-sampled “skip” branches being one core idea here. The DeepLab model of \cite{c26} adds a conditional random field to improve the alignment between predictions of the CNN and the edges in the image. Eigen\&Fergus \cite{c27} jointly predict depth, normals and object class labels by alternating between convolutions and incorporating multi-scale skip connections. The SegNet architecture \cite{c28} stores the pooling indices of an encoder and passes them to up-sampling layers to preserve spatial accuracy. Pinheiro et al. \cite{c29} introduced refinement modules that integrate high- and low-level activations. The whole architecture is based on ResNet \cite{c30}. In \cite{lueddecke2019context}, the authors designed a PSPNet-based network and a U-Net-style network to densely predict affordances for single RGB images and used a selective binary cross-entropy loss function. All these Deep Learning based approaches yield good results on their respective tasks. Affordance recognition is also one crucial component in our system.


%\subsection{Image Inpainting}
%Image Inpainting is required to complete missing regions in an image based on information from the image itself or from an image library, making the repaired image look natural and difficult to distinguish from the undamaged image. This problem has been addressed over many years but especially recently it got into the focus of research. Classical approaches \cite{c31,c32} use patch-based methods for image inpainting, which fill the missing region patch-by-patch by searching for well matching patches from the undamaged regions of the image and paste them into the belonging holes. Using deep learning architectures, \cite{c33} propose a coarse-refine two stage framework with gated convolutional layers and SN-PatchGAN for free-form image inpainting. The study in \cite{c34} made additional use of the image edge map. It proposes a two-stage adversarial model, EdgeConnect, that comprises of an edge generator followed by an image completion network. The edge generator hallucinates edges of the missing regions and the image completion network fills them in, using these edges as boundaries. In \cite{c35}, the authors find that the mean and variance shifts, caused by full-spatial FN, limit the image inpainting network training and they propose a spatial region-wise normalization named Region Normalization (RN) to overcome this limitation. Because occlusion between objects frequently occurs, filling this in is a very important capability for our system, too. 

\subsection{Imagination and reinforcement learning}
In \cite{chen2018neural}, the authors propose training a recurrent long short-term memory (LSTM) network to recurrently predict action sequences by taking a scene image and the specified task as input. In \cite{kim2020learning}, the authors propose a GameGAN model, which aims to learn a simulator by watching an agent interact with an environment and then "imagines" the next screen using a generative adversarial network. Word Model \cite{ha2018world} is a model-free RL model that uses features extracted to train a simple policy to solve the planning task. For model-based methods, MuZero \cite{schrittwieser2020mastering} has achieved successes in domains that require precise and sophisticated look-ahead, such as chess and Go. In \cite{racaniere2017imagination}, the authors propose I2A model that combines model-free and model-based ideas, which learn to interpret future environment imagination to augment model-free decisions. In contrast to the mentioned works, we obtain imagination of future real world scenes \textit{without} training on sequences of actions.

\begin{table}[b]
\centering
\begin{tabularx}{0.45\textwidth} {|>{\hsize=.5\hsize\linewidth=\hsize}X|
>{\hsize=1.5\hsize\linewidth=\hsize}X|}
\hline
\textbf{Affordance} & \textbf{Description} \\
\hline
grasp & areas that can be grasped to apply another action. \\ 
place-on & a surface, where objects can be placed. \\ 
obstruct & areas, where objects are not allowed to be put. \\ 
hole & hollow space in a solid object. \\ 
\hline
\end{tabularx}
\caption{Description of the set of affordances used}
\label{table:1}
\end{table}

\section{METHOD OVERVIEW}
In Figure \ref{concept}, the general workflow in our system is visualized, which we will describe next (for more details, see below). 

We take as input an image of a scene with some objects in it. First, we perform object detection and pixel-level instance segmentation. In addition to this, we also create an affordance map for the initial scene. The goal of affordance segmentation is to assign probabilities for a set of affordances to every pixel in the image. Since examples of objects occluding each other are common in usual scenes, we then perform - as needed - de-occlusion using a Generative Adversarial Network (GAN). This way we split the whole scene into background and individual objects. This is followed by pose estimation to create information about "how" an action should be executed.

Following that, we imaginary-execute the action, where we can choose from pick\&place, rotate, or flip vertically. Finally we perform a validity checking process after which we obtain the imagined scene with the belonging action. 

Thus, in summary we have the workflow: a) Object detection, b) Instance\&Affordance assignment, c) Scene De-occlusion, d) Saving poses, e) Application of the action f) Evaluation of the result. Parts a, b, c require deep models, where the architectures for training and inference are given in Figure \ref{structure}. \\


\begin{figure*}[htbp]
  \centering
  \includegraphics[width=0.95\textwidth]{imgs/treelike.pdf}
  \caption{Demonstration of a planning tree. Each column represents an action step, the branches represent possible actions and each action is based on an imagined scenario, where the previous action had been completed. The red dashed boxes mark the scenes used for the valid planning sequences and the red circles indicate the objects on which the action is applied.} 
  \label{treelike}
\end{figure*}

\begin{figure}[htbp]
  \centering
  \includegraphics[width=0.45\textwidth]{imgs/poses_mapping.pdf}
  \caption{Example for pose mapping. We create a dictionary to store the horizontal and vertical pose of the blue cuboid. When we apply a flipping action on this object, we can lookup the dictionary and retrieve the corresponding pose}  \label{poses mapping}
\end{figure}

\underline{Quantification:} We use several initial scenarios and create decision graphs/trees based on imagined images and then check their validity. All valid image sequences then represent valid plans, where pre- and post-conditions are implicitly encoded by the images. In this way, we can quantify whether or not such a system shows graceful degradation along several planning steps, determining for different scenarios and manipulation sequences its actual usefulness for planning and execution.

\section{IMPLEMENTATION}

\subsection{Data set}

The data to train and evaluate our proposed method is created from a real environment. We used a top-view camera to collect all data with a resolution of $1024\times 768$. Note, that this is not a restriction of this method. At the end of this study, we show that top views can be generically generated by inverse perspective mapping. Hence, similar to human imagination processes, where we employ usually also some canonical "internal view" onto the imagined scene, also here the top view serves as the canonical perspective for our planning method.

All collected images need to be semantically- and affordance-segmented. For semantic as well as affordance segmentation, seven different semantic categories (can, cup, plate, bowl, apple, box, cuboid) and 4 different affordance categories (see Table \ref{table:1}) were considered and extracted for all visible regions. In total, our data set consists of 900 training samples and 296 testing samples, each with manually annotated semantic and affordance information.

%\subsection{Experimental setup}
%In our experiments we have used a top view and a relatively small area, in which the objects reside, because this leads to only small projection-based deformations, which can be neglected. Processing of all steps has been performed on a local computer with an IntelCore i7-3770 CPU and a single NVIDIA 1080 Ti1080Ti GPU.

\begin{table}[b]
\centering
\caption{The results for model components a,b,c (see Fig.\ref{structure} first blue box on top). mAP=mean average precision, mIoU=mean intersection over union, L1 is the L1 norm.}
\label{table:2}
\begin{tabularx}{0.45\textwidth} {|>{\hsize=0.6\hsize\linewidth=\hsize}X|
>{\hsize=0.4\hsize\linewidth=\hsize}X|}
\hline
\textbf{model}                    & \textbf{results}              \\ \hline
Object detection: EfficientDet-D0 & mAP@0.5:0.95: 69.21\%          \\ \hline
Semantic segmentation: Unet       & mIoU: 92.93\%              \\ \hline
Affordance segmentation: Unet     & mIoU: 91.24\%               \\ \hline
De-occlusion: PCNet-M\&PCNet-C    & L1: 0.0271, L2: 0.0028 \\ \hline
\end{tabularx}
\end{table}

\begin{figure*}[htbp]
  \centering
  \includegraphics[width=0.75\textwidth]{imgs/plans.pdf}
  \caption{Examples of three successful and three failed plans. The first column represents the initial scene, each following column represents then an individual action step. In the failure cases, the red dashed box means a wrong step. See explanation of the failures in the ``Results'' section below.}  \label{plans}
\end{figure*}

\subsection{Implementation details}

Many of the approaches combined here represent standard methods and will, thus, only be described briefly.

For object detection, considering the size of the dataset, we used EfficientDet-D0 \cite{tan2020efficientdet} as the backbone network. During training, we apply horizontal flipping, scale jittering and randomly masking to perform data augmentation, and then we resized the images to 512$\times$512 as input. We modified the output and added a pose classification head to predict whether the object is placed vertically or horizontally. The model is trained for 200 epochs with total batch size 4. We also used SGD optimizer with momentum 0.9 and reduced the initial learning rate 0.001 by factor 0.1 when the total loss has stopped improving after 3 epochs. The other parameters are same as in \cite{tan2020efficientdet}. 

For Affordance\&Semantic segmentation, we used a U-net like structure \cite{ronneberger2015u}. Let $\rm C_sk$ denote the Convolution-BatchNorm-ReLU,  $\rm DC_sk$ double convolution layers (Convolution-BatchNorm-ReLU-Convolution-BatchNorm-ReLU), $\rm PC_sk$  partial convolution layers \cite{liu2018image} (PartialConvolution-BatchNorm-ReLU), k number of filters, and subscript s the stride. Furthermore, $\rm UP$ denotes upsampling layers, $\rm DN$  downsampling layers. Then the encoder is defined the following way: $\rm DC_132-DN-DC_164-DN-DC_1128-DN-DC_1256-DN-DC_1256$ and the decoder by: $\rm UP-DC_1512-UP-DC_1256-UP-DC_1128-UP-DC_164$. After the last decoder layer, two classification heads are applied to obtain four-dimensional output for affordance segmentation and two-dimensional output for semantic segmentation (background and main body). To work with the complemented image for obtaining of the secondary segmentation mask, we cropped the image according to its axis-aligned bounding box and resized it to 256$\times$256. Combining all the outputs of the bounding box patches, we get the affordance\&semantic segmentation for the original image. The model is trained for 400 epochs with the Adam optimizer and a learning rate 0.0001 with batchsize 8. We apply the same loss fuction as in \cite{lueddecke2019context}.

For Image Complementation, we applied two Unet like architecture models PCNet-M and PCNet-C like in \cite{zhan2020self} as mask and image generator. For PCNet-M we used the same structure as used in segmantation and for PCNet-C we used 6 downsampling layers encoder (PC$_2$64-PC$_2$128-PC$_2$256-PC$_2$512-PC$_2$512-PC$_2$512) and 6 upsampling layers decoder (PC$_1$1024-UP-PC$_1$1024-UP-PC$_1$768-UP-PC$_1$384-UP-PC$_1$92-UP-PC$_1$67-UP). The last layer for the decoder has no BatchNorm and ReLU. For the discriminator, an SN-PatchGAN \cite{c33} is applied which uses 4 convolutional layers with spectral normalization (C$_2$64-C$_2$128-C$_2$256-C$_2$512) and one convolution map to  create a one-dimensional 70$\times$70 output. As in \cite{zhan2020self}, we also cropped each object according to its bounding box. During training, the PCNet-M and PCNet-C are trained for 200 epochs and 800 epochs respectively with the Adam optimizer with learning rate 0.0001 and batchsize 8. 

\subsection{Pose processing} 
The data collected in our experiments come from a top-down view of the RGB image, which is good for handling object movement and rotation in the horizontal direction. However, we also allow flipping of an object: for example, as shown in Figure~\ref{poses mapping}, a cuboid will appear different when placed horizontally or vertically. Hence we need to predict the pose of the object, horizontal or vertical, together with object detection. Since each unique object in the experiment belongs to one category, we create a dictionary to store the horizontal and vertical poses of each object. The category and pose of the object are jointly used as primary keys, and the corresponding object's RGB image, instance segmentation map, and affordance map are saved as values. The horizontal or vertical pose of each category of objects is saved only once. When we need to flip an object, we can use this dictionary to get the flipped pose of the corresponding object.

\subsection{Applying the action}
The actions, we can implement, are pick up, put down (i.e., pick\&place), rotate, and flip vertically. We regard each object as a separate layer and then we use traditional image processing methods, such as cut-and-paste and rotation, to perform the movement and horizontal rotation of the object. We take the center of the object's bounding box as the origin when applying the action. For flipping objects, we need to replace the corresponding object layer with the flipped pose according to the dictionary. The object layers  are afterwards overlaid on a background layer to get the resulting image showing the result of applying the action. 


\subsection{Result checking}
In the last step, having obtained images after action imagination, we check whether the action is valid or not. We require that the object is not placed in an area where the affordance is ‘obstruct’. Thus, the checking process is based on the affordance map. For this, we define conflict areas as the intersection of the 'obstruct' affordance with the manipulated object. We count the intersection pixels, where we set the threshold to 30 pixels. If the conflict area is less than 30 pixels, then we assume that the action is correct.

\section{EXPERIMENTS \& RESULTS}

\subsection{Experiments}
Our task is described as the need to organize our desktop by packing objects on the desktop into a box. The box has differently sized partitions and, similarly, objects have different sizes and shapes. This requires a robotic action plan such that all objects will fit and no object is left out.
%For the definition of the action plan: Our task is to pack the objects into a box. In the beginning we need to put the objects one by one into the appropriate compartment according to their size, until all the objects on the desktop are put into the appropriate compartment, or there is no spare and appropriate compartment, the plan ends.

\begin{table*}[!ht]
\vspace*{2mm}
\centering
\caption{Success rates for 296 planning cases with different plan lengths.}
\label{table:4}
    \resizebox{0.9\width}{!}{
    \begin{tabular}{|c|c|c|c|c|c|c|c|c|c|}
    \hline
        steps of plans  & 0 step & 1 step & 2 steps & 3 steps & 4 steps & 5 steps & 6 steps & 7 steps & total \\ \hline
        success cases & 29 & 68 & 37 & 54 & 43 & 23 & 7 & 7 & 268 \\ \hline
        failure cases & 4 & 3 & 3 & 4 & 8 & 2 & 2 & 2 & 28 \\ \hline
        total cases & 33 & 71 & 40 & 58 & 51 & 25 & 9 & 9 & 296 \\ \hline
        success rate & 87.88\% & 95.77\% & 92.50\% & 93.10\% & 84.31\% & 92.00\% & 77.78\% & 77.78\% & 90.54\% \\ \hline
    \end{tabular}
    }
\end{table*}

\begin{table*}[htbp]
\centering
\caption{Success rates for step by step analysis. A total of 771 planning steps were performed for the testing cases.}
\label{table:3}
    \resizebox{0.9\width}{!}{
    \begin{tabular}{|c|c|c|c|c|c|c|c|c|}
    \hline
        n-th step of plans  & step1 & step2 & step3 & step4 & step5 & step6 & step7 & total \\ \hline
        valid steps & 257 & 187 & 148 & 89 & 41 & 17 & 8 & 747 \\ \hline
        invalid  steps & 6 & 5 & 4 & 5 & 2 & 1 & 1 & 8 \\ \hline
        total  steps & 263 & 192 & 152 & 94 & 43 & 18 & 9 & 771 \\ \hline
        success rate & 97.81\% & 97.38\% & 97.39\% & 94.68\% & 95.35\% & 94.12\% & 88.89\% & 96.89\% \\ \hline
        \end{tabular}
}
\end{table*}

\subsection{Results}

We evaluated the deep learning models used in our process on the test data set, and the results are shown in Table \ref{table:2}. Note that for the object complementation task, we need to complement the occluded parts of objects, and we obtain an average L1 loss of 0.0271, and an average L2 loss of 0.0028. Since our dataset is relatively small and the difference between the training and test sets is not significant, these deep learning models perform well in our assigned task. Hence, these models build a solid foundation for our following task planning.


%Since the performance of  planning is strongly dependent on object detection and affordance segmentation results, our evaluation focuses on the success rate, as well as on the efficiency of the planning process.

We verified our method on the test dataset. These scenes differ in the number and location of objects. Our target is to place as many objects from outside the box as possible into the appropriate compartments in the box. To save computational resources, a depth-first search is used to find the complete plans, which are then checked whether they are suitable. 

Of the 296 test cases there exist plans with zero up to seven packing steps.  A ``zero step'' case corresponds to the situation where the box is fully packed and no planning steps are required. This is included to test the system's capability to recognize also such situations. Table~\ref{table:4} shows how many of these different cases had been successful. The grand average success rate across all cases was 90.54\%. The majority of failure cases are due to inaccuracies in object and affordance segmentation.

In Table \ref{table:3} we analyse all cases in a step-wise manner asking whether a plan step $n$ has been successful or not. A total of 771 steps were performed across all 296 cases in the plan-search process. As expected, the success rate deteriorates for longer planning sequences, but does not drop below 88\%. The overall average success rate is 96.89\%. This demonstrates that our method has a high success rate for every step to step transition which did not drop too much along longer plans. 

In Figure \ref{plans}, we show some successful and some failed plans. Three successful plans were able to complete our box packing task in a few steps. For the failed plans we observed here the following characteristic cases: In failure case 1, a part of the cup is incorrectly identified as another cup, which is caused by an inaccurate result of target detection. The same failure cause also happens in failure case 2, where a part of the plate is identified as a can, which in turn leads to a wrong action. In failure case 3, there were objects that could be packed, but no action was found in the search. This is, because none of the conflict areas calculated between the 'grasped' objects and all 'place-on' and 'hole' areas is smaller than the 30-pixel threshold value, which is caused by an inaccuracy of the result in affordance segmentation. 

%It should be noted that the timing numbers discussed next are given only to provide some general information about performance levels. It is expected that search speed can be improved significantly when using faster hardware. Planning requires that the processes shown in the top panels in Fig.~\ref{concept} are performed once (for the start scene) and the processes at the bottom are repeated a certain number of times ("steps") until a valid plan is found. Of all 126 randomly generated starting configurations, we get plans where the minimum number of steps was 0 and the maximum was 7 steps, with an average of 2.7 steps required. Zero steps means that the initial scene is already in a packed state.  For the hardware we have used here, we get an average time to create a plan by searching of 8.175s and the average search time per iteration step is 3.003s. 

\begin{figure*}[htbp]
  \centering
  \includegraphics[width=0.95\textwidth]{imgs/front2top.pdf}
  \caption{Example result of a simulated scene by applying inverse perspective mapping to create a top view from several side views.}  \label{front2top}
\end{figure*}

\section{CONCLUSION \& OUTLOOK}
As we mentioned above, the performance of  planning depends on whether we can get the correct results from object detection and affordance segmentation, as shown by some failed examples. Instance segmentation affects object detection and de-occlusion, and affordance segmentation affects the number and correctness of obtained action proposals.

The current algorithm uses top view cameras. This issue does not lead to any restriction, though, because one can address it using inverse perspective mapping methods. Figure \ref{front2top} shows how to generate top views from different camera perspectives. Here we created a simulated scene and placed four cameras at fixed positions around the scene for data collection. We first use inverse perspective mapping (IPM) to remap the images from four cameras into a preliminary orthographic projection based on the intrinsic and extrinsic camera parameters. Then we use a deep network (Unet) to further correct this distorted scene and finally get a near optimal birds-eye view image. We used 2000 images for training and 200 images for testing. As this is not in the center of this study, we directly used top view cameras, instead, to generate a canonical view for all our experiments avoiding shape deformation, which might interfere with the planning process. However, if required, IPM pre-processing can be included into our algorithms without problems.

In conclusion, the here-presented method may prove useful to avoid onerous design requirements in conventional robotic planning in all situations where a certain slack in accuracy is permissive (e.g. service robotics, kitchen robots, etc.). 




%We proposed an comprehensive system which contains some steps such as perception, simulated action, success-checking and re-planning. In this way, we hope that the robot will acquire the ability to use the vision information to conduct action plans, thus helping to reduce the effort of manually define an explicit representation of the environment. We define a simple scenario and collected some data to experimentally verify that such a system can achieve a relatively acceptable success rate, which demonstrates the potential of our proposed system for guiding robotic actions.

%As stated above, the method is applicable in some simplified scenarios and actions. For more complex task, it’s also important for robots to understand the interactions of objects with diverse geometries. Therefore, some research and study in this area can also be integrated into our system in the future. 


%\addtolength{\textheight}{-12cm}   % This command serves to balance the column lengths
                                  % on the last page of the document manually. It shortens
                                  % the textheight of the last page by a suitable amount.
                                  % This command does not take effect until the next page
                                  % so it should come on the page before the last. Make
                                  % sure that you do not shorten the textheight too much.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\iffalse 
%\section*{APPENDIX}


%\section*{ACKNOWLEDGMENT}
%
%\fi



\bibliographystyle{IEEEtran}
\bibliography{Shijia_et_al_2022} % your .bib file

\end{document}

