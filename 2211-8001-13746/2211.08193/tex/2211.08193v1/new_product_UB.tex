\documentclass{article}

\def\neurips{0}
\def\supplemental{0} % to be used with neurips=0
\def\tpdp{0} % to be used with neurips = 0 


\ifnum\neurips=1
    % ready for submission
    %\usepackage{neurips_2021}
    
    % to compile a preprint version, e.g., for submission to arXiv, add add the
    % [preprint] option:
         \usepackage[]{neurips_2021}
    
    % to compile a camera-ready version, add the [final] option, e.g.:
    %     \usepackage[final]{neurips_2021}
    
    % to avoid loading the natbib package, add option nonatbib:
    %    \usepackage[nonatbib]{neurips_2021}
\else 
\fi

\input{macros}

\ifnum\neurips=1
    \usepackage[utf8]{inputenc} % allow utf-8 input
    \usepackage[T1]{fontenc}    % use 8-bit T1 fonts
    \usepackage{hyperref}       % hyperlinks
    \usepackage{url}            % simple URL typesetting
    \usepackage{booktabs}       % professional-quality tables
    \usepackage{amsfonts}       % blackboard math symbols
    \usepackage{nicefrac}       % compact symbols for 1/2, etc.
    \usepackage{microtype}      % microtypography
    \usepackage{xcolor}         % colors
\else 
\fi
\begin{document}

\section{Upper Bound Analysis}

\subsection{Preliminary Bounds}
Let $Z_r$ be a $d$-dimensional random vector representing the noise added in Step 'insert step' of Algorithm~\ref{oldalg:prod}. For coordinates $j \in [d]$ to which no noise is added, set $Z_r[j] = 0$. The remaining coordinates of $Z_r$ correspond to independent zero-mean Gaussian random variables with standard deviation as per Step 'insert step' of Algorithm~\ref{oldalg:prod}. 
First, we prove that the empirical bias estimates are close to the true bias estimates with high probability.
\begin{lemma}\label{lem:empiricalest}
Fix a round $r \in [2R+1]$. Then, with probability at least $1-\frac{2\beta}{R}$, for all $j\in [d]$ with $0 \leq \biasesfixed_j \leq \frac{1}{2}$,
\begin{enumerate}
    \item if $0 \leq \biasesfixed_j < \frac{\alpha}{d}$, then $|\hat{\biasesfixed}_r[j] - \biasesfixed_j| \leq \frac{\alpha}{4d}$
    \item if $\frac{\alpha}{d} \leq \biasesfixed_j \leq \frac{1}{2}$, then $|\hat{\biasesfixed}_r[j] - \biasesfixed_j| \leq \frac{\biasesfixed_j}{16}.$
\end{enumerate}
\end{lemma}

\begin{proof}
Fix $r\in [2R+1]$ and fix $j \in S_r$. Note that $\E[\hat{\biasesfixed}_j^r] = \biasesfixed_j$ for all $r\in [2R+1]$. 

We begin with the first statement of the claim by doing a case analysis on $\frac{\alpha}{4d\biasesfixed_j}$. First, when  $\frac{\alpha}{4d\biasesfixed_j} < 1$, we use the multiplicative Chernoff bound in Claim~\ref{claim:cher_bounds} for $\delta \in (0,1)$

\begin{align*}
  \Pr\Big[\hat{\biasesfixed}^{r}_j > \biasesfixed_j\Big(1 + \frac{\alpha}{4d\biasesfixed_j}\Big)\Big] 
  \leq exp\left({-\frac{\alpha^2 \biasesfixed_j m}{48 d^2(\biasesfixed_j)^2}}\right) 
  = exp\left({-\frac{\alpha^2m}{48 d^2\biasesfixed_j}} \right)
  \leq exp\left({-\frac{\alpha}{12 d}\frac{1000 d \log(dR/\beta)}{\alpha}} \right)
  \leq \frac{\beta}{dR}
\end{align*}
where in the third inequality we used that $\biasesfixed_j \leq \frac{\alpha}{d}$ and substituted in a lower bound for the value of~$m$. 

Secondly, when $\frac{\alpha}{4d\biasesfixed_j} > 1$, we use the multiplicative Chernoff bound for all $\delta > 0$, given in Claim~\ref{oldclaim:mulcher2} to obtain that
\begin{align*}
  \Pr\Big[\hat{\biasesfixed}^r_j > \biasesfixed_j\Big(1 + \frac{\alpha}{4d\biasesfixed_j}\Big)\Big] 
  &\leq exp\left(- \frac{\alpha^2\biasesfixed_j m}{16d^2\biasesfixed_j^2(2 + \frac{\alpha}{4d\biasesfixed_j})}\right)
  \leq exp\left(- \frac{\alpha^2 m}{12d^2\biasesfixed_j(\frac{\alpha}{d})}\right)\\
  &= exp\left(- \frac{\alpha m}{12d\biasesfixed_j}\right)
  \leq  exp\left({-\frac{\alpha}{12 d} \frac{1000 d\log(dR/\beta)}{\alpha}} \right)
  \leq \frac{\beta}{dR}
\end{align*}
where in the first inequality we have used that since  $\frac{\alpha}{4d\biasesfixed_j} > 1$,  $\frac{\alpha}{4d\biasesfixed_j} + 2 \leq 3\frac{\alpha}{4d\biasesfixed_j}$, and in the third inequality we have substituted a lower bound for the value of $m$ and upper bounded $\biasesfixed_j$ by $\alpha$.  

Similar inequalities hold in the other direction. Taking a union bound over all $j \in S_{R+1}$ such that $\biasesfixed_j \leq \frac{\alpha}{d}$ completes the first claim in Lemma~\ref{lem:empiricalest}.

Next, assume that $\frac{\alpha}{d} \leq \biasesfixed_j \leq \frac{1}{2}$. Then, applying Chernoff bounds for $\delta \in (0,1)$, 
\begin{align*}
    \Pr\left[\hat{\biasesfixed}_r[j] - \biasesfixed_j \geq \frac{\biasesfixed_j}{16}\right] = \Pr\left[\hat{\biasesfixed}_r[j] 
    \geq \biasesfixed_j \left(1 + \frac{1}{16}\right)\right] 
    \leq exp\left(-\frac{\biasesfixed_j m}{3\cdot 16}\right)
    \leq \frac{\beta}{dR}
\end{align*}
where the final inequality holds since $\biasesfixed_j m \geq \frac{\alpha}{d}1000\frac{d}{\alpha}\log(\frac{dR}{\beta})$. A similar bound holds for the other direction.
\end{proof}
Next, we prove that the amount of noise added in any round is not too large relative to $u_r$ with high probability.
\begin{lemma}\label{lem:Gaussnoise}
Let $m \geq \frac{1200d \log^{5/4} \left( \frac{d R}{\alpha \beta (2\rho)^{1/2}} \right)}{\alpha (2 \rho)^{1/2}}$. Fix a round $r \in [2R+1]$. Then, with probability at least $1 - \frac{\beta}{R}$, for all $j \in S_r$,
\begin{equation*}
    |Z_r[j]| \leq \frac{\alpha u_r}{1000}.
\end{equation*}
\end{lemma}
\begin{proof}
For rounds $1,\dots,2R$, the standard deviation of univariate Gaussian noise $Z_r[j]$ added in round $r \in [2R]$ is $\sigma_r = \sqrt{\frac{3u_r |S_r| \log(\frac{mR}{\beta})}{\rho m^2}}$. Setting $t = \sqrt{2 \log(\frac{2dR}{\beta})}$, and using Lemma (insert lemma in appendix) on the concentration of a zero-mean Gaussian random variable along with a union bound,
$$ \Pr(\max_{j \in S_r} |Z_r[j]| \geq t \sigma_r) \leq \sum_{j \in S_r} \Pr(|Z_r[j]| \geq t \sigma_r) \leq  \sum_{j \in S_r} 2e^{-t^2/2} = \frac{\beta}{R}.$$
Lower bounding $m$ by $\frac{600 d \log(\frac{dR}{\beta})}{\sqrt{\rho} \alpha}$, upper bounding $|S_r|$ by $d$, and using the fact that $u_r \geq \frac{20}{d}$ for all $r \in [R]$, we get that for sufficiently large $d$, 
\begin{align*}
    t \sigma_r \leq  \alpha \sqrt{\frac{6 u_r}{3600 d \log^{1/4}(\frac{dR}{\beta})}} \leq \frac{\alpha u_r}{1000}  . 
\end{align*}
For $r = 2R+1$, $\sigma_r = \sqrt{\frac{100 \log(\frac{m}{\beta})}{\rho m^2}}$, so choosing the same value of $t$ gives the same result. 
\end{proof}
Finally, we argue that truncation happens with very low probability in any round. This claim is stated and proved in \cite{KLSU19} (the upper bound of the bottom most bucket is $20/d$ in our case instead of $1/d$ but the truncation ceiling $B_{2R+1}$ is also set larger than in their paper to balance this out.)
\begin{lemma}[\cite{KLSU19}, Claim 5.10, Claim 5.18]\label{lem:trunc}
Fix a round $r \in [2R+1]$. Assume that for all $j \in S_r$, $\biasesfixed_j \leq u_r$. Let $h$ be the probability that for every $i \in [m]$, $$\|\datafixed_i^r [S_r] \|_2 \leq  B_r,$$ that is, no rows are truncated in the calculation of $\text{tmean}_{B_r}(\Datafixed^r[S_r])$. Then, the following statements are true.
\begin{enumerate}
    \item If $r \in [2R]$, $h \geq 1 - \frac{\beta}{R}$.
     \item If $r = 2R+1$, then $h \geq 1 - \beta$.
\end{enumerate}
\end{lemma}

\subsection{Success of the Bucketing Phase}
In this section, we use the preliminary bounds to argue that the bucketing procedure succeeds with high probability.

\begin{lemma}\label{thm:bucketsuccess} 
If $\Datafixed^1, \dots, \Datafixed^R$ contain $m$ records where $m \geq \frac{1200d \log^{5/4} \left( \frac{d R}{\alpha \beta (2\rho)^{1/2}} \right)}{\alpha (2 \rho)^{1/2}}$, then with probability at least $1 - 3\beta$ over the randomness of the input data and the coins of $\sampler_{prod}$, we have that for all rounds $r \in [R+1]$ and for all coordinates $j \in [d]$
\begin{enumerate}
    \item If $r \in [R]$ is $j$'s last round, that is $\biasesfixed_j \in S_r$, and $\biasesfixed_j \not \in S_{r+1}$, then $u_r / 4 \leq \biasesfixed_j \leq u_{r}$.
    \item If $p_j \in S_{R+1}$, then
    $0 \leq \biasesfixed_j \leq u_{R+1}$.
\end{enumerate}
We call this event \bucket.
\end{lemma}
\begin{proof}
We prove that for all rounds $r \in [R+1]$, for $j \in S_r$, $\biasesfixed_j \leq u_r$, and if $j \not \in S_r$, $\biasesfixed_j \geq u_r / 2$; this is stronger than Item 1 of Theorem~\ref{thm:bucketsuccess}. We prove this by induction on $r$, arguing that each round `fails' with probability at most $\frac{3\beta}{R}$, and then taking a union bound over these failure events. For the first round (the base case of the induction), since $u_1 = 1/2$, and since by assumption $\biasesfixed_j \leq 1/2$ for all $j \in [d]$, we have that $\biasesfixed_j \leq u_1$. Additionally, since $S_1 = [d]$ it immediately follows that for $j \not \in S_{1}$, $p_j \geq u_{1}/2$. Next, fix any $r \in [R]$. The inductive hypothesis is that for round $r$, for all $j \in S_{r}$, $\biasesfixed_j \leq u_{r}$, and for $j \not \in S_{r}$, $\biasesfixed_j \geq u_r / 2 = u_{r+1}$. Then, we will prove that with high probability, the same statement holds for round $r+1$. 

We argue that the attribute bias is very close to the noisy empirical estimate, that truncation does not happen with high probability, and that the noise added is small. We will conclude by arguing that if the noisy empirical estimate is below the threshold $\tau_r$, then the attribute bias is smaller than the upper bound $u_{r+1}$ and if it is greater than $\tau_r$, then the attribute bias is larger than $u_{r+2}$.

By Item 2 of Lemma~\ref{lem:empiricalest}, with probability at least $1 - \frac{2 \beta}{R}$, for all $j \in S_r$ with $p_j > \frac{\alpha}{d}$,
$$|\hat{\biasesfixed}^r[j] - \biasesfixed_(j)| \leq \frac{\biasesfixed_j}{16} \leq \frac{u_r}{16}.$$
where the second inequality is by the induction hypothesis.

Similarly, by Item 1 of Lemma~\ref{lem:empiricalest}, with probability $\geq 1 - \frac{2 \beta}{R}$, for all $j \in S_r$ with $p_j \leq \frac{\alpha}{d}$,
$$|\hat{p}^r_j - p_(j)| \leq \frac{5}{4d} \leq \frac{u_r}{16},$$
where the second inequality is by the fact that $u_r \geq \frac{20}{d}$ for all $r \in [R]$ and $\alpha \leq 1$. 

By the inductive hypothesis, $\biasesfixed_j \leq u_r$ for all $j \in S_r$. Hence, we can apply Item 1 of Lemma~\ref{lem:trunc} to argue that no truncation happens in round $r$ with probability at least $1 - \frac{\beta}{R}$ . Additionally, by Lemma~\ref{lem:Gaussnoise}, with probability at least $1 - \frac{\beta}{R}$, $|Z_r[j]| \leq \frac{u_r}{1000}$ since $\alpha \leq 1$. Hence,  with probability at least $1 - \frac{2\beta}{R}$,
\begin{align*}
    |\hat{\biasesfixed}_r[j] - \Tilde{\biasesfixed}_r[j]| \leq \frac{u_r}{1000}.
\end{align*}

By the triangle inequality, we get that with probability at least $1 - \frac{3\beta}{R}$, for all $j \in S_r$,
$$|\Tilde{\biasesfixed}_r[j] - \biasesfixed_j| \leq  |\hat{\biasesfixed}_r[j] - \Tilde{\biasesfixed}_r[j]| + |\hat{\biasesfixed}_r[j] - \biasesfixed_j| \leq \frac{u_r}{16} + \frac{u_r}{1000} \leq \frac{u_r}{8}.$$ 

Define $e_r = \frac{u_r}{8}.$ With probability at least $1 - \frac{3\beta}{R}$, for all $j \in S_r$, $\biasesfixed_j \leq \Tilde{\biasesfixed}_r[j] + e_r$ and $\biasesfixed_j \geq \Tilde{\biasesfixed}_r[j] - e_r$. 

Next, we use this to prove a claim that is similar to Claim 5.14 in \cite{KLSU19}. \ssnote{Does this deserve to be its own claim?}
\begin{claim}
With probability at least $1-\frac{4\beta}{R}$, for every $j \in S_r$,
\begin{enumerate}
    \item If $\Tilde{\biasesfixed}_r[j] \leq \tau_r$, then  $\biasesfixed_j \leq u_{r+1} = \frac{u_r}{2}$
    \item Else, $\biasesfixed_j > \frac{u_{r+1}}{2} = u_{r+2}$.
\end{enumerate}
\end{claim}
\begin{proof}
Using the fact that $\Tilde{\biasesfixed}_r[j] \leq \tau_r$, we get that $\biasesfixed_j \leq \tau_r + e_r$. We have that $\tau_r = \frac{3u_{r+1}}{4}$ and $e_r =\frac{u_r}{8} = \frac{u_{r+1}}{4}$. Hence, $\biasesfixed_j \leq u_{r+1}$. By a similar argument, if $\Tilde{\biasesfixed}_r[j] > \tau_r$, then $\biasesfixed_j > u_{r+1}/2$.
\end{proof}

This completes the inductive step and proves that at the beginning of round $r+1$, with probability at least $1 - \frac{3\beta}{R}$, $p_j \leq u_{r+1}$ for all  $j \in S_{r+1}$, and $p_j \geq \frac{u_r}{2}$ for all $j \not \in S_{r+1}$. Taking a union bound over the failure probabilities proves Theorem~\ref{thm:bucketsuccess}.
\end{proof}

\subsection{Success of the Sampling Phase}
The main lemma we will prove in this section is the following.
\begin{lemma}[Success of sampling phase]\label{lem:mainacc}
For all $j \in [d]$, for $q[j]$ defined as in lines 14 and 21 of algorithm $\sampler_{prod}$, when $\sampler_{prod}$ is run with failure probability parameter $\beta \in(0, 1]$ and target accuracy $\alpha \in(0, 1]$ , 
\begin{enumerate}
    \item if  $0 \leq \biasesfixed_j \leq \frac{\alpha}{d}$, then
    $\displaystyle | \mathbb{E}[ q[j] - \biasesfixed_j ] | \leq \frac{\alpha}{d} + 7\beta;$
    \item if  $\frac{\alpha}{d} < \biasesfixed_j \leq \frac 1 2$, then
    $\displaystyle| \mathbb{E}[ q[j] - \biasesfixed_j ] | \leq 42\beta;$
\end{enumerate}
where the expectations are taken over the randomness of the data and the noise.
\end{lemma}

\begin{proof}
We start by proving the first statement of Lemma~\ref{lem:mainacc}. Fix any $j \in [d]$ with $0 \leq \biasesfixed_j \leq \frac{\alpha}{d}$.
By the law of total expectation and Lemma~\ref{thm:bucketsuccess},
\begin{align}
 |\E[ q[j] - \biasesfixed_j ]| 
  \leq |\E[ q[j] - \biasesfixed_j \mid \bucket]\Pr[\bucket]| + |\E[ q[j] - \biasesfixed_j \mid \overline{\bucket}]\Pr[\overline{\bucket}]| 
 \leq |\E[ q[j] - \biasesfixed_j \mid \bucket]| + 4\beta. \label{eq:interim3}
\end{align}
Now, we show that $|\E[ q[j] - \biasesfixed_j \mid \bucket]| \leq 3\beta + \frac{\alpha}{d}$. Observe that $|[\tilde{\biasesfixed}_r^j[j]]_0^1 - \biasesfixed_j | \leq |\tilde{\biasesfixed}_r^j[j] - \biasesfixed_j| = |Z_r[j]|$. We can apply successful bucketing \bucket and Claim~\ref{lem:Gaussnoise} to say that with probability at least $1-2\beta$, for all $j \in S_r$,
\begin{equation*}
    |Z_r[j]| \leq \frac{\alpha u_r}{1000} \leq \frac{ \biasesfixed_j}{250} \leq \frac{\alpha}{d},
\end{equation*}
where we use the fact that $\biasesfixed_j \leq \frac{\alpha}{d}$.
Thus, $|\E[ q[j] - \biasesfixed_j \mid \bucket]|\leq \frac{\alpha}{d} + 2\beta$. Combining this with Equation~\ref{eq:interim3} proves the first part of Lemma~\ref{lem:mainacc}. 

Next, we prove the second part of Lemma~\ref{lem:mainacc}. Fix any $j \in [d]$ with $\frac{\alpha}{d} \leq \biasesfixed_j \leq \frac{1}{2}$. First we bound the probability of clipping any of the estimates in their final round.


\begin{lemma}\label{claim:no_clip}
 Let $\clipped$ be the event that some $j \in [d]$ with $\frac{\alpha}{d} < \biasesfixed_j \leq \frac 1 2$ gets clipped in its last round $\rjfinal^j$. Then,
$$\Pr[\clipped] \leq 7\beta,$$
where the probability is over the noise and dataset.
\end{lemma}
\begin{proof}
Recall the definition of event \bucket, successful bucketing. By the law of total probability and  Lemma~\ref{thm:bucketsuccess},
\begin{align}
    \Pr\left(\clipped \right) = \Pr\left(\clipped \mid \bucket \right) \Pr(\bucket) + \Pr\left(\clipped \mid \overline{\bucket} \right) \Pr(\overline{\bucket}) \leq \Pr\left(\clipped \mid \bucket \right) + 4\beta. \label{eq:intermed}
\end{align}

Now, we reason about $\Pr(\clipped \mid \bucket)$. Note that $|\biasesfixed_j- \tilde{\biasesfixed}_{\rjfinal^j}[j]| \leq |\biasesfixed_j- \hat{\biasesfixed}_j| + |\hat{\biasesfixed}_j- \tilde{\biasesfixed}_{\rjfinal^j}[j]|$. By Lemma~\ref{lem:empiricalest}, the quantity $|\biasesfixed_j- \hat{\biasesfixed}_j|\leq \frac{\biasesfixed_j}{16}$ with probability at least $1-2\beta$. Furthermore, the quantity $|\hat{\biasesfixed}_j- \tilde{\biasesfixed}_{\rjfinal^j}[j]|$ is just $|Z_{\rjfinal}[j]|\leq \frac{\biasesfixed_j}{250}$ for all $j \in S_{\rjfinal}$ with probability at least $1-2\beta$, since we have successful bucketing.

Thus, with probability at least $1-3\beta$, for all $j\in [d]$, 
\begin{align*}
    0 < \frac{2\biasesfixed_j}{3} \leq  \tilde{\biasesfixed}_{\rjfinal^j}[j]
    \leq \frac{4\biasesfixed_j}{3} < 1,
\end{align*}
where the first and last inequalities hold since $0 < \biasesfixed_j \leq \frac{1}{2}$. Thus, for all $j\in [d]$, we have $\tilde{\biasesfixed}_{\rjfinal^j}[j]$ does not get clipped with probability at least $1-7\beta$.
\end{proof}

Let $\overline{\clipped\cup \trunc}$ be the event that no truncation and no clipping for any $j \in [d]$ occur. Now, fix any $j \in [d]$ with $ \frac{\alpha}{d} \leq \biasesfixed_j \leq \frac{1}{2}$. Then
\begin{align}
   |\mathbb{E}[q[j] -  \biasesfixed_j] |
   & =  |\mathbb{E}[q[j] -  \biasesfixed_j \mid \overline{\clipped \cup \trunc}] \cdot \Pr[\overline{\clipped \cup \trunc}]
    +  \mathbb{E}[ [q[j] -  \biasesfixed_j \mid \clipped \cup \trunc]\cdot \Pr[\clipped \cup \trunc]| \nonumber\\
   & \leq |\mathbb{E}[q[j]  \mid \overline{\clipped \cup \trunc}] - \biasesfixed_j|  + 14\beta  \leq \Bigg|\frac{\mathbb{E}[\hat{\biasesfixed}_{\rjfinal}^j[j] + Z] }{\Pr[\overline{\clipped \cup \trunc}]}- \biasesfixed_j \Bigg|  + 14\beta \nonumber\\
   & \leq \Bigg|\frac{\biasesfixed_j }{1-14\beta}- \biasesfixed_j \Bigg|  + 14\beta \leq \Bigg|\frac{1 }{1-14\beta}- 1 \Bigg|  + 14\beta \leq 42 \beta. \nonumber\\
\end{align}

The first equality holds by the law of total expectation, the first inequality holds since $\mathbb{E}[[q[j] -  \biasesfixed_j] \leq 1$ and by combining Lemmas~\ref{lem:trunc} and \ref{claim:no_clip} to get $\Pr[\clipped \cup \trunc] \leq 14\beta$. The second inequality uses the fact that when $\overline{\clipped \cup \trunc}$ occurs, there is no clipping, and $\E[A\mid B] \leq \E[A]/\Pr[B]$ for all events $A, B$. The last inequality holds because $\beta = \frac{\alpha}{d} \leq \frac {1}{28}$ for $d > 16$.  

\end{proof}

\begin{align}
   \mathbb{E}[ [q_{\rjfinal^j}^{\Datarvy}[j]]_0^1 -  \biasesfixed_j] 
   & =  \mathbb{E}[ [q_{\rjfinal^j}^{\Datarvy}[j]]_0^1 -  \biasesfixed_j \mid E_3]\Pr(E_3) 
    +  \mathbb{E}[ [q_{\rjfinal^j}^{\Datarvy}[j]]_0^1 -  \biasesfixed_j \mid \overline{E_3}]\Pr( \overline{E_3}) \nonumber\\
   & \leq  \mathbb{E}[ [q_{\rjfinal^j}^{\Datarvy}[j]]_0^1 -  \biasesfixed_j \mid E_3]  + 14\beta \nonumber\\
   & = \mathbb{E}[ q_{\rjfinal^j}^{\Datarvy}[j] \mid E_3] - \biasesfixed_j + 14\beta \nonumber\\
   & = \mathbb{E}[ \hat{\biasesfixed}^{\rjfinal^j}_j + Z \mid E_3] - \biasesfixed_j + 14\beta \nonumber\\
   & \leq \frac{\mathbb{E}[\hat{\biasesfixed}^{\rjfinal^j}_j + Z ]}{\Pr(E_3)} - \biasesfixed_j + 14\beta \nonumber\\
   & \leq \frac{\biasesfixed_j}{1 - 14\beta} - \biasesfixed_j + 14\beta \nonumber\\
   & \leq \biasesfixed_j + \Big[\frac{1}{1 - 14\beta} - 1 \Big]\biasesfixed_j -  \biasesfixed_j + 14\beta \nonumber\\
   & \leq  \Big[\frac{1}{1 - 14\beta} - 1 \Big] + 14\beta \leq 42\beta
\end{align}

\end{document}