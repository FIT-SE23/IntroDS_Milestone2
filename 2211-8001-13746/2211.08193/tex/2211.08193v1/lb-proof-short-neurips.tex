


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
% This is short version of the k-ary lower bound proof that appeared in the NeurIPS 2021 submission. Should be obsolete now. 
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\section{The lower bound for $k$-ary distributions}\label{sec:k-ary-lb-frequency-count-based}\label{sec:kary-short}

In this section, we prove the lower bound for sampling from discrete distributions with the universe of size at least 3.  As discussed in Section~\ref{intro:overview}, we prove that $n = \Omega(k/\alpha \eps)$. The crux of the proof is the case where the sampler is frequency-count-based, Poisson, and is $(\eps, \delta)$-DP for small $\eps$. The transformation from general samplers to this restricted type of sampler is presented in the supplementary materials. This transformation together with the following Lemma~\ref{lem:main-k-ary-lb} completes the proof of Theorem~\ref{thm:intro-k-ary-lb} for discrete distributions with the universe of size at least 3.

\begin{lemma}\label{lem:main-k-ary-lb} Fix $k, %\geq 1, % 1, not 3
n \in{\mathbb N}, \alpha \in (0,0.02], \eps \in(0,1/\ln (1/\alpha)],$ and $\delta \in [0, 0.1\alpha\eps/k]$. Let $\class_{2k+1}$ denote the class of discrete distributions over the universe $[2k+1]$. If sampler \sampler is $(\eps, \delta)$-DP, frequency-count-based, and $\alpha$-accurate on class $\class_{2k+1}$ with dataset size distributed as $\Po(n)$, then $n > \frac 1 {60}\cdot \frac{k}{\alpha \eps}$.
\end{lemma}

\begin{proof}
We consider the following distribution $\distr$ over the universe $\universe=[2k+1].$ Fix $\alpha^*=60 \alpha$ and a set $S^*\subset[2k]$ of size $k.$ Distribution $\distr$ has mass $\alpha^*/k$ on each element in $S^*$ and mass $1-\alpha^*$ on the {\em special} element $2k+1.$ 

Consider a sampler \sampler satisfying the conditions of Lemma~\ref{lem:main-k-ary-lb}. Let $\distroutput{\sampler, \distr}$ denote the output distribution of $\sampler$ when the dataset size $N \sim \Po(n)$ and the dataset $\Datarv \sim \distr^{\otimes N}$.
Observe that
\begin{eqnarray}\label{eq:main-dist-lb-delta}
d_{TV}(\distroutput{\sampler,\distr}, \distr)
  \geq \Pr_{\substack{N\sim\Po(n) \\ \Datarv\sim  \distr^{\otimes N}}}[\sampler(\Datarv) \notin Supp(\distr)].  
\end{eqnarray}
We show that if $n\leq \frac k{60\alpha \eps}$  and $\eps$ and $\delta$ are in the specified range, the right-hand side of (\ref{eq:main-dist-lb-delta}) is large. We start by deriving a lower bound on $\Pr[\sampler(\Datafixed)\notin Supp(\distr)]$ for a fixed dataset $\Datafixed$ of a fixed size $N$. Let $p_{j,F(\Datafixed)}$ be the probability that \sampler outputs a specific element in $[2k]$ that occurs $j$ times in $\Datafixed$, where $F(\Datafixed)$ is the frequency-count of \Datafixed; these probabilities are well-defined because \sampler is frequency-count-based. Let $F^*_0(\Datafixed)$ denote the number of elements in $[2k]$ that occur $0$ times in $\Datafixed$ (excluding the special element $2k+1$ from this count).
 By definition, $F^*_0(\Datafixed)\leq 2k.$ Consequently,
\begin{equation}\label{eq:notinsupport-delta}
\Pr[\sampler(\Datafixed) \notin Supp(\distr)] = k\cdot p_{0,F(\Datafixed)} \geq \frac{1}{2}\cdot  F^*_0(\Datafixed)\cdot p_{0,F(\Datafixed)}.
\end{equation}

The next claim uses the fact that %sampler 
\sampler is $(\eps,\delta)$-DP to show that the probability $p_{j,F(\Datafixed)}$ cannot be much larger than the probability that \sampler outputs a specific element in $\universe$ that does not appear in $\Datafixed$.

\begin{claim}\label{clm:epsdelfing}
For all $(\eps, \delta)$-DP samplers, frequency counts $f \in \mathbb{Z}^*$, and elements $j \in \universe$,
\begin{equation}\label{eq:grouppriv}
 p_{j,f} 
 \leq e^{\eps j} \Big( p_{0,f} + \frac{\delta}{\eps} \Big).
\end{equation}
\end{claim}

\begin{proof}
Consider a frequency count $f$ and a dataset $\Datafixed$ with $F(\Datafixed) = f$. Note that (\ref{eq:grouppriv}) is true trivially for all $j$ such that $F_j(\Datafixed)=0$ because, in that case, $p_{j,F(\Datafixed)}$ is set to $0$.

Fix any $j \in \universe$ with $F_j(\Datafixed) > 0$. Let $a$ be any element in $\universe$ that occurs $j$ times in the dataset $\Datafixed$. Let $b$ be any element in $\universe$ that is not in the support of the distribution $\distr$. Let $\Datafixed|_{a\rightarrow b}$ denote the dataset obtained by replacing every instance of $a$ in the dataset $\Datafixed$ with element $b$. By group privacy~\cite{DworkMNS06j},
\begin{equation}\label{eq:group_privacy-delta}
\Pr[\sampler(\Datafixed) = a] \leq e^{j \eps} \Pr[\sampler( \Datafixed|_{a \rightarrow b}) = a ] + \delta \cdot \frac{e^{\eps j} -1}{e^{\eps} - 1}.
\end{equation}
Note that the dataset $\Datafixed|_{a\rightarrow b} $ does not contain element $a$, since we've replaced every instance of it with $b$. Importantly, $F(\Datafixed|_{a \rightarrow b}) = F(\Datafixed)$ because $b$ is outside of the support of the distribution $\distr$ and hence does not occur in $\Datafixed$. Since $\sampler$ is frequency-count-based and $F(\Datafixed) = F(\Datafixed|_{a \rightarrow b})$, we get that $p_{0,F(\Datafixed)} = p_{0,F(\Datafixed|_{a \rightarrow b})}$. 
%
Substituting this into (\ref{eq:group_privacy-delta}) and using the fact that $e^{\eps}-1\geq \eps$ for all $\eps$, we get 
%when $F_j(\Datafixed) > 0$,
\begin{equation*}%\label{eq:relabel_assumption}
p_{j,F(\Datafixed)} \leq e^{j \eps}\cdot p_{0,F(\Datafixed)} + \delta \cdot \frac{e^{\eps j} - 1}{e^\eps -1}
\leq e^{\eps j} \Big(p_{0,f} + \frac{\delta}{\eps} \Big).
\end{equation*}
This completes the proof of Claim~\ref{clm:epsdelfing}.
\end{proof}

For a dataset $\Datafixed$ and $i\in[2k+1]$, let $N_i(\Datafixed)$ denote the number of occurrences of element $i$ in $\Datafixed$.
Next, we give a lower bound on $\Pr[\sampler(\Datafixed) \notin Supp(\distr)]$ in terms of the counts $N_i(\Datafixed)$.

\begin{claim}\label{clm:nonsupport-lb-fixed-delta}
Let $N\in\mathbb{N}$ and $\Datafixed \in [2k+1]^N$ be a fixed dataset. Set %$Y = \sum_{j=1}^N F_j(\Datafixed)e^{\eps j}$.
$Y=\sum_{i \in S^*}  \left[ e^{N_i(\Datafixed) \eps} \right]$.
Then
$$\Pr[\sampler(\Datafixed) \notin Supp(\distr)] \geq\frac{1}{2}
\cdot\frac{\Pr[\sampler(\Datafixed) \in [2k]]}{1+Y/k} -\frac{k\delta}\eps.$$
\end{claim}

\begin{proof}
In the following derivation, we use the fact that that an element $j\in[2k]$ that appears $j$ times in $\Datafixed$ is returned by \sampler with probability $p_{j,F(\Datafixed)}$, then split the elements into those that do not appear in $\Datafixed$ and those that do, next use the fact that all elements from $[2k]$ that appear in $\Datafixed$ must be in $S^*$, then apply Claim~\ref{clm:epsdelfing}, and finally substitute $Y$ for $\sum_{i \in S^*}  \left[ e^{N_i(\Datafixed) \eps} \right]$:
%$\sum_{j=1}^N F_j(\Datafixed)e^{\eps j}$:
\begin{align*}
\Pr[\sampler(\Datafixed) \in[2k]] 
&=\sum_{i\in[2k]}  p_{N_i(\Datafixed),F(\Datafixed)} 
=F^*_0(\Datafixed)\cdot p_{0,F(\Datafixed)}+\sum_{i\in[2k]\cap\Datafixed}  p_{N_i(\Datafixed),F(\Datafixed)} \\
& \leq F^*_0(\Datafixed)\cdot p_{0,F(\Datafixed)}+\sum_{i\in S^*}  p_{N_i(\Datafixed),F(\Datafixed)} \\
& \leq F^*_0(\Datafixed)\cdot p_{0,F(\Datafixed)}+\sum_{i\in S^*}  p_{0,F(\Datafixed)}\cdot \Big(e^{\eps N_i(\Datafixed)} + \frac{\delta}{\eps} \Big) \leq \Big(F^*_0(\Datafixed)+Y\Big)\Big(p_{0,F(\Datafixed)}+  \frac{\delta}{\eps}\Big).
\end{align*} 
We rearrange the terms to get \ 
$\displaystyle
p_{0,F(\Datafixed)}
\geq \frac{\Pr[\sampler(\Datafixed) \in[2k]]}{F^*_0(\Datafixed)+Y}-\frac \delta \eps.
$

Substituting this bound on $p_{0,F(\Datafixed)}$ into (\ref{eq:notinsupport-delta}), we obtain that $\Pr[\sampler(\Datafixed)\notin Support(P)]$ is at least
\begin{align*}
    \frac{1}{2} \cdot \frac {F^*_0(\Datafixed)\Pr[\sampler(\Datafixed) \in [2k]]}{F^*_0(\Datafixed) + Y} -\frac 12 \cdot\frac{F^*_0(\Datafixed)\cdot\delta}{\eps} 
  % &= \frac{1}{2} \cdot \frac {\Pr[\sampler(\Datafixed) \in [2k]]}{1 + Y/F^*_0(\Datafixed)} -\frac 12 \cdot\frac{F^*_0(\Datafixed)\cdot\delta}{\eps}\\
   \geq\frac{1}{2}
\cdot\frac{\Pr[\sampler(\Datafixed) \in [2k]]}{1+Y/k} -\frac{k\delta}\eps,
\end{align*}
where in the inequality, we used that $k\leq F^*_0(\Datafixed) \leq 2k$.
This holds since the support of $\distr$ excludes $k$ elements from $[2k]$ and since $F^*_0(\Datafixed)$ counts only elements from $[2k]$ that do not appear in~$\Datafixed.$
\end{proof}


Finally, we give a lower bound on the right-hand side of (\ref{eq:main-dist-lb-delta}). Assume for the sake of contradiction that $n\leq \frac{k}{\alpha^* \eps}$. 
%
By Claim~\ref{clm:nonsupport-lb-fixed-delta},
%
\begin{align}
 \Pr_{N, \Datarv}[\sampler(\Datarv) \notin Supp(\distr)]
 &\geq \E_{N, \Datarv}\Big[\frac{1}{2}
\cdot \frac{\Pr[\sampler(\Datarv) \in [2k]]}{1+Y/k} -\frac{k\delta}\eps\Big]\nonumber\\
& = \frac{1}{2} \cdot \E_{N, \Datarv}\Big[\frac{\Pr[\sampler(\Datarv) \in [2k]]}{1+Y/k}\Big] -  \frac{k\delta}{\eps}. \label{eq:mainlbcond}
\end{align}
Next, we analyze the expectation in (\ref{eq:mainlbcond}). Let $E$ be the event that $\frac{Y}{k} \leq e^3$. By the law of total expectation, 
\begin{align}\label{eq:mainlbcondfirst}
     \E_{N, \Datarv}\Big[\frac{\Pr[\sampler(\Datarv) \in [2k]]}{1+Y/k}\Big]
     & \geq \E_{N, \Datarv}\Big[\frac{\Pr[\sampler(\Datarv) \in [2k]]}{1+Y/k} \big | E\Big] \Pr(E).
\end{align}
In Claims~\ref{claim:eventE} and~\ref{claim:exp-of-regular-output}, we argue that both $\Pr(E)$ and $\E_{N, \Datarv}\left[\frac{\Pr[\sampler(\Datarv) \in [2k]]}{1+Y/k} \big | E\right]$ are large.
\begin{claim}\label{claim:eventE}
Suppose $n\leq \frac k{60\alpha \eps}$. Let $E$ be the event that $\frac{Y}{k} \leq e^3$. Then
    $\Pr(E) \geq 1 - \alpha$.
    \end{claim}
\begin{proof}
Recall that $Y$ was defined as $\sum_{i \in S^*}  \left[ e^{N_i(\Datafixed) \eps} \right]$ for a fixed dataset $\Datafixed.$ Now consider the case when dataset $\Datarv$ is a random variable. 
If $N\sim\Po(n)$ and $\Datarv \sim \distr^{\otimes N}$ then $N_i(\Datarv) \sim \Po(\frac{\alpha^* n}{k})$ for all $i \in S^*$ and, additionally, the random variables $N_i(\Datarv)$ are mutually independent. When $\Datarv$ is clear from the context, we write $N_i$ instead of $N_i(\Datarv)$. Now we calculate the moments of $\frac{Y}{k}$.  For all $\lambda > 0$, 
\begin{align}
\E_{\substack{N\sim\Po(n) \\ \Datarv \sim  \distr^{\otimes N}}} \Big[\Big(\frac{Y}{k}\Big)^{\lambda} \Big] 
 = \E_{\substack{N\sim\Po(n) \\ \Datarv \sim  \distr^{\otimes N}}} \Big[\Big(\frac{1}{k} \sum_{i \in S^*} e^{N_i(\Datarv) \eps} \Big)^{\lambda} \Big] 
 = \E_{N_1, \cdots, N_k \sim\Po(\frac{\alpha^* n}{k})} \Big[ \Big(\frac{1}{k} \sum_{i \in S^*} e^{N_i \eps} \Big)^{\lambda} \Big]. \label{eq:mompoiss}
\end{align}

%%%%%%%%%%%%%%%%%%%%%%%%
%% Moments of the average claim was moved to Appendix.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%Applying Claim~\ref{claim:momavg} to (\ref{eq:mompoiss}), we get that 
%\begin{align}\label{eq:mompoissfinal}
%\E_{\substack{N\sim\Po(n) \\ \Datafixed \sim  \distr^{\otimes N}}} \left[\left(\frac{Y}{k}\right)^{\lambda} \right] \nonumber  = %\E_{N_1, \cdots, N_k \sim\Po(\frac{\alpha^* n}{k})} \left[ \left(\frac{1}{k} \sum_{i \in S^*} e^{N_i \eps} \right)^{\lambda} \right] 
%    \leq \E_{N_1 \sim\Po(\frac{\alpha^* n}{k})} \left[ \left(e^{N_1 \eps} \right)^{\lambda} \right].
%\end{align}
Finally, we bound the probability of event $E$. Set $c=e^3$ and $\lambda = \ln \frac{1}{\alpha}$. By definition of $E$, 
\begin{align}
    \Pr(\overline{E}) 
    & = \Pr\Big(\frac{Y}{k} \geq c\Big)\nonumber 
     = \Pr\Big(\Big(\frac{Y}{k}\Big)^\lambda \geq c^{\lambda}\Big) 
    \leq \frac 1{c^{\lambda}}\cdot {\E_{\substack{N\sim\Po(n) \\ \Datarv \sim  \distr^{\otimes N}}}
    \Big[ \Big(\frac{Y}{k}\Big)^\lambda \Big]} \nonumber \\
    & \leq  \frac 1{c^{\lambda}}\cdot {\E_{N_1, \cdots, N_k \sim\Po(\frac{\alpha^* n}{k})} \Big[ \Big(\frac{1}{k} \sum_{i \in S^*} e^{N_i \eps} \Big)^{\lambda} \Big]} 
     \leq  \frac 1{c^{\lambda}}\cdot{\E_{N_1 \sim\Po(\frac{\alpha^* n}{k})} \Big[ \Big( e^{N_1 \eps} \Big) ^{\lambda} \Big]} \label{eq:moments2} \\
    & = c^{-\lambda}\cdot {e^{\frac{\alpha^* n}{k}(e^{\lambda \eps} - 1)}}
    \leq e^{-3\lambda}\cdot {e^{\frac{(e^{\lambda \eps} - 1 )}{\eps}}}
    \leq e^{-3\lambda}\cdot e^{2\lambda}= e^{-\lambda}
    =e^{-\ln (1/\alpha)}=\alpha, \label{eq:moments3}
\end{align}
where we use $\lambda > 0$ in the second equality, then apply Markov's inequality. To get the inequalities in (\ref{eq:moments2}), we apply (\ref{eq:mompoiss}) and then use the fact that the moments of the average of random variables is less than the moment of a single random variable (the proof of this fact is in the supplementary material). To get (\ref{eq:moments3}), we use the moment generating function of a Poisson random variable, and then we substitute $c=e^3$ and use the assumption that $n\leq \frac k{60\alpha \eps} =\frac k{\alpha^*\eps}$. The second inequality in (\ref{eq:moments3}) holds because $\lambda = \ln \frac{1}{\alpha}$ and $\eps \in (0,1/\ln \frac{1}{\alpha}]$, so $\lambda \eps \leq 1$ and hence $e^{\lambda \eps} \leq 1 + 2\lambda \eps$.
The final expression is obtained by substituting the value of $\lambda.$
We get that $\Pr(E) \geq 1-\alpha$, completing the proof of Claim~\ref{claim:eventE}.
\end{proof}
\begin{claim}\label{claim:exp-of-regular-output}
$\displaystyle\E_{\substack{N\sim\Po(n) \\ \Datarv\sim  \distr^{\otimes N}}}\Big[\frac{\Pr[\sampler(\Datarv) \in [2k]]}{1+Y/k} \big| E \Big] \geq 2.3\alpha.$
\end{claim}
\begin{proof}
When event $E$ occurs, $1+\frac{Y}{k} \leq 1+e^3<22$. Then  
\begin{align}\label{eq:conditioning}
    \E_{N, \Datarv}\Big[\frac{\Pr[\sampler(\Datarv) \in [2k]]}{1+Y/k} \big | E \Big] > \E_{N, \Datarv}\Big[\frac{\Pr[\sampler(\Datarv) \in [2k]]}{22} \big | E\Big]  = \frac 1{22}\cdot\E_{N, \Datarv}\Big[\Pr[\sampler(\Datarv) \in [2k]] \mid E \Big].
\end{align}
By the product rule,
$$\Pr[\sampler(\Datarv) \in [2k]] \mid E ]
=\frac{\Pr[\sampler(\Datarv) \in [2k]] \wedge E]}{\Pr[E]}
\geq \Pr[\sampler(\Datarv) \in [2k]] \wedge E]
\geq  {\Pr[\sampler(\Datarv) \in [2k]] - \Pr[\overline{E}]}.$$
Substituting this into (\ref{eq:conditioning}) and recalling that $\alpha^*=60\alpha$, we get
\begin{align*}
    \E_{N, \Datarv}\Big[\frac{\Pr[\sampler(\Datarv) \in [2k]]}{1+Y/k} \big | E \Big]
    &\geq  \frac 1{22}\cdot\E_{N, \Datarv}\Big[\Pr[\sampler(\Datarv) \in [2k]] - \Pr[\overline{E}] \Big]
%    = \Pr[\sampler(\Datarv) \in [2k]] \mid E ] \cdot \Big( \frac{1}{30} \Big) 
 %    = \frac{\Pr[\sampler(\Datarv) \in [2k]] \wedge E]}{\Pr[E]} \cdot \Big( \frac{1}{30} \Big) \\
 %    &\geq \frac{\Pr[\sampler(\Datarv) \in [2k]] - \Pr[\overline{E}]}{\Pr[E]} \cdot \Big( \frac{1}{30} \Big) 
     \geq \frac 1{22}\cdot \Big( \alpha^* - \alpha - \alpha \Big) 
     \geq 2.3\alpha,
\end{align*}
since sampler $\sampler$ is $\alpha$-accurate on $\distr$, and  $\distr$ has mass $\alpha^*$ on $[2k]$, and by Claim~\ref{claim:eventE}. 
\end{proof}
%Applying Claims~\ref{claim:eventE} and~\ref{claim:exp-of-regular-output} to (\ref{eq:mainlbcondfirst}), we get that
%\begin{align}
%     \frac{1}{2} \cdot \E_{\substack{N\sim\Po(n) \\ \Datarv\sim  \distr^{\otimes N}}}\Big[\frac{\Pr[\sampler(\Datarv) \in [2k]]}{1+Y/k}\Big] 
%      \geq  \frac{1}{2} \cdot \frac{\alpha^* - 2\alpha}{30}\cdot \Big(1 - \alpha\Big) 
%      \geq \frac{\alpha^* - \frac{5\alpha}{4}}{120}. \label{eq:alphainexp}
%\end{align}
%Substituting  (\ref{eq:alphainexp}) into (\ref{eq:mainlbcond}), we get that
%\begin{align}
% \Pr_{\substack{N\sim\Po(n) \\ \Datarv\sim  \distr^{\otimes N}}}[\sampler(\Datarv) \notin Supp(\distr)]
%& > \frac{\alpha^* - \frac{5\alpha}{4}}{120} - k\cdot \frac{\delta}{\eps}
%\end{align}

Combining (\ref{eq:main-dist-lb-delta}), (\ref{eq:mainlbcond}), and (\ref{eq:mainlbcondfirst}), applying Claims~\ref{claim:eventE} and~\ref{claim:exp-of-regular-output}, and recalling that $\delta\leq 0.1\cdot\alpha\eps/k$, we get
\begin{align*}
    d_{TV}&(\distr,\distroutput{\sampler, \distr})
  \geq \Pr_{N, \Datarv}[\sampler(\Datarv) \notin Supp(\distr)]
  \geq \frac{1}{2} \cdot \E_{N, \Datarv}\Big[\frac{\Pr[\sampler(\Datarv) \in [2k]]}{1+Y/k}\Big]  - \frac{k\delta}{\eps} \\
  &\geq \frac 12\cdot   \E_{N, \Datarv}\Big[\frac{\Pr[\sampler(\Datarv) \in [2k]]}{1+Y/k} \big | E\Big] \Pr(E) - 0.1\alpha 
  \geq  \frac{1}{2} \cdot 2.3\alpha\cdot \Big(1 - \alpha\Big) - 0.1\alpha 
  > \alpha,
%     \geq\frac{1}{1 + \frac 1k\E_{N\sim\Po(n),\Datarv\sim  \distr^{\otimes N}} \left[\sum_{i\in \Datarv} e^{N_i(\Datarv) \cdot \eps} \right]}  
 %   & > \frac{\alpha^* - \frac{5\alpha}{4}}{120}  - \frac{k\delta}{\eps} \\
%    & \geq \frac{\alpha^* - \frac{5\alpha}{4}}{120}  - \frac{\alpha}{120} \quad \text{(Since $\delta \leq \frac{\alpha\eps}{120k}$)}
%    \\
%    & > \alpha \quad \text{($\alpha^*$ set to $50 \alpha$)}
\end{align*}
where the last inequality holds since $\alpha\leq 0.02$. This contradicts $\alpha$-accuracy of $\sampler$ on datasets of size $\Po(n)$, where $n\leq \frac{k}{\alpha^* \eps}$, and completes the proof of Lemma~\ref{lem:main-k-ary-lb}.
\end{proof}
\color{black}