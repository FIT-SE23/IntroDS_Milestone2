% This is samplepaper.tex, a sample chapter demonstrating the
% LLNCS macro package for Springer Computer Science proceedings;
% Version 2.20 of 2017/10/04
%
\documentclass[runningheads]{llncs}
%
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs} % for professional tables
\usepackage{hyperref}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{algorithm}
\usepackage{algorithmic}
%\newcommand{\theHalgorithm}{\arabic{algorithm}}
\usepackage{bbm}
\usepackage{times}
\usepackage{soul}
\usepackage{multirow}
\usepackage{graphicx}
% Used for displaying a sample figure. If possible, figure files should
% be included in EPS format.
%
% If you use the hyperref package, please uncomment the following line
% to display URLs in blue roman font according to Springer's eBook style:
% \renewcommand\UrlFont{\color{blue}\rmfamily}

\begin{document}
%
\title{Universal Distributional Decision-based Black-box Adversarial Attack with Reinforcement Learning}%\thanks{Supported by organization x.}}
%
%\titlerunning{Abbreviated paper title}
% If the paper title is too long for the running head, you can set
% an abbreviated paper title here
%
\author{Yiran Huang\inst{1} \and
Yexu Zhou\inst{1} \and
Michael Hefenbrock\inst{1} \and Till Riedel\inst{1} \and Likun Fang\inst{1} \and Michael Beigl\inst{1}}%\orcidID{2222--3333-4444-5555}}
%
\authorrunning{F. Author et al.}
% First names are abbreviated in the running head.
% If there are more than two authors, 'et al.' is used.
%
\institute{Karlsruhe Institute of Technology, Karlsruhe, Germany \\ \email{\{yhuang, zhou, hefenbrock, riedel, fang, beigl\}@teco.edu}
}
%\institute{Princeton University, Princeton NJ 08544, USA \and
%Springer Heidelberg, Tiergartenstr. 17, 69121 Heidelberg, Germany
%\email{lncs@springer.com}\\
%\url{http://www.springer.com/gp/computer-science/lncs} \and
%ABC Institute, Rupert-Karls-University Heidelberg, Heidelberg, Germany\\
%\email{\{abc,lncs\}@uni-heidelberg.de}}
%
\maketitle              % typeset the header of the contribution
%
\begin{abstract}
The vulnerability of the high-performance machine learning models implies a security risk in applications with real-world consequences. Research on adversarial attacks is beneficial in guiding the development of machine learning models on the one hand and finding targeted defenses on the other. However, most of the adversarial attacks today leverage the gradient or logit information from the models to generate adversarial perturbation. Works in the more realistic domain: decision-based attacks, which generate adversarial perturbation solely based on observing the output label of the targeted model, are still relatively rare and mostly use gradient-estimation strategies. In this work, we propose a pixel-wise decision-based attack algorithm that finds a distribution of adversarial perturbation through a reinforcement learning algorithm. We call this method  Decision-based Black-box Attack with Reinforcement learning (DBAR). Experiments show that the proposed approach outperforms state-of-the-art decision-based attacks with a higher attack success rate and greater transferability.

\keywords{Adversarial attack \and Decision attack \and Reinforcement Learning.}
\end{abstract}
%
%
%
\section{Introduction}
Many high-performing machine learning algorithms used in computer vision, speech recognition and other areas are susceptible to minimal changes of their inputs~\cite{brendel2017decision}. Despite their good performance, the vulnerability of machine learning models has raised widespread concerns. Small perturbation on road signs can have a serious impact on automated driving. These actions, which modify the benign input by imperceptible perturbations and thus manipulate the machine learning model to suit the attacker's interests, are called adversarial attacks.% An example of the adversarial attack generated with the method proposed in this paper is shown in Figure~\ref{fig: dbar exp}.

Most adversarial attacks used to construct adversarial perturbation rely either on gradient information (white-box attack) or logit output (score-based attack) of the model. While these approaches help to study the limitations of current machine learning algorithms~\cite{papernot2017practical}, they do not reflect the level of information a real attacker would have access to in most scenarios. In contrast, decision-based attacks consider limited access to the targeted model, allowing only the label information output by the model to be used. Such limited access is far more common in the real-world scenarios making its study more practical.
%\todo[inline]{Add something like :Such limited access should be far more common in situations where model output is provided in terms of a service and the service provider does not want to reveal the internal workings ... or something.}
 %Different from the other adversarial attacks, which use information that is difficult to obtain in real-world scenarios, e.g., gradient of target model and Logit information of model output. The limited information of decision-based attacks is a reflection of real-world scenarios, which makes the study more practical. In addition, the study of decision-based attacks helps to identify the limitations of current machine learning algorithms and serves as a guideline for optimizing machine learning algorithms and developing defense methods. For example, Ensemble training~\cite{tramer2017ensemble} is a successful defends against transfer attack~\cite{papernot2017practical}.
% 这里可以分别给出例子，介绍具体点

% \begin{figure}[!t]
%     \centering
%     \includegraphics[width=3.3in]{plots/intro.png}
%     \caption{Examples of Decision-based adversarial attack using the DBAR algorithm: The first column contains two benign examples, and second column shows the average of the perturbation distributions, in form of Heatmap, in the RGB three dimensions generated for that examples with the proposed DBAR method; Two adversarial perturbations are sampled from each distribution separately, and they are shown in the third column in form of Heatmap; The fourth column shows the adversarial examples after adding the adversarial perturbation to the benign image. The result of the adversarial attack is presented in the last column.}
%     \label{fig: dbar exp}
% \end{figure}
% 强调distribution的动机还是强化学习？
% db攻击的描述有问题，从新
%\todo[inline]{reorgnize the text of presenting the idea}
%Since the definition of decision-based adversarial attacks was first proposed by~\cite{brendel2017decision}, there has been an increasing amount of related research in recent years (see Section~\ref{sec: relatied work}). 
Most of decision-based attacks start with an adversarial example with large perturbation. Then, adversarial examples with smaller perturbations are gradually found by sample-based gradient estimation. Different attacks exploit the samples in different ways, and therefore the efficiency of the algorithms varies. 
%\todo[inline]{We claim that some appraoches are bad because they require so many queries? Do we evaluate the number of queries of our appraoch vs other appraoches anywhere? - yes}
Such gradient-estimation-based approaches, however, require a large number of queries to the targeted model, which affects the efficiency of the algorithm and makes it impossible to perform real-time attack. In addition, the perturbations generated by gradient-based approach are too specific to the particular targeted model and benign example, and therefore lack transferability. To address these shortcomings, we propose a novel pixel-wise decision-based attack approach, called Decision-based Black-box Attack with Reinforcement learning (DBAR), which is guided by rewards instead of gradients. 
%In addition, rather than a single adversarial perturbation, we are looking for a distribution for a given benign example. From such a distribution we can sample adversarial perturbations that can successfully mislead the prediction of the targeted model. 
We therefore phrase the search for adversarial perturbations as an reinforcement learning task. Depending on whether the learned agent is targeting a single or multiple benign examples, two different attacks are designed, i.e., context-free attack and context-aware attack.

%into an optimization problem such that the center of the distribution found is as close as possible to the benign example while the adversarial perturbations sampled from the distribution maintain a high attack success rate. We then solve the optimization problem with reinforcement learning. We call the proposed method Decision-based Black-box Attack with Reinforcement learning (DBAR). 

%The reinforcement learning community now has well-established techniques to use sampled instances efficiently and to fit algorithms stably~\cite{hastings1970monte,mnih2016asynchronous,schulman2017proximal}. 
%\todo[inline]{Do your experiments support all of those hypothesis? e.g. speed of convergevs other methods? - have exp that limit the query number}
%We hypothesize that: (i) the speed of convergence can be improved using the reinforcement learning techniques; (ii) the reward-based nature of reinforcement learning allows for better transferability of the generated adversarial perturbations. (iii) since the reward definition makes no assumptions about the format of the benign example, such algorithm can be easily adjusted to attack super pixels of the image data or data other than images, e.g., time series data. 

Our contributions can be summarized as: (1) Context-free DBAR achieves state-of-the-art performance, and perturbations sampled from the discovered distribution are more transferable then those generated by the other decision based attacks. In addition, the context-free DBAR is an universal attack which can also attack time-series data and super pixel of image data. (2) The context-aware attack achieves an effective attack without any queries on the targeted model after training, which is not possible for most existing decision-based attacks. (3) The algorithm generates a distribution which can be used to sample multiple different attacks.
% 可以攻击各种数据包括图片和时间序列

%The rest of this work is organized as follows: Section~\ref{sec: relatied work} reviews previous work on adversarial attacks. Section~\ref{sec: methodology} presents a detailed description of the proposed DBAR algorithm. We demonstrate the performance of the proposed algorithm in Section~\ref{sec: 4} and conclude the full paper in Section~\ref{sec: 5}.

\section{Related work}
\label{sec: relatied work}
% 介绍点其他两个类型的方法

%Adversarial attacks can be broadly classified into three categories based on the information needed to generate the attack. White-box attacks assume complete access to the model, including structure and weights. 
%Most of these methods phrase the search for adversarial perturbations as a constrained optimization problem~\cite{kurakin2016adversarial,croce2021mind,zhu2021sparse}. On the other hand, score-based attacks consider only the logit information of the target model's prediction. Due to not having access to the model, gradient information is not available, and some works~\cite{liu2016delving,papernot2016transferability,papernot2017practical} simulate the target model by surrogate model. White-box techniques can then be used to generate an adversarial example for the surrogate model. 
%In addition, some methods~\cite{chen2017zoo,ilyas2018black,bhagoji2018practical} try to implement gradient-based attacks by estimating the gradient from the output information of the target model. Finally, decision-based attacks require only information about the top-ranked prediction of the model given an input~\cite{brendel2017decision}. Score-based attacks and decision-based attacks are collectively referred to as black-box attacks. 

%\todo[inline]{In general for any paper. Do not try to point at weaknesses in other methods that you do not address in your own appraoch.}

%\todo[inline]{Add some addtional sentence here that says that we are interested in the last kind of methods (and why).}
The definition of decision-based attack was first proposed in~\cite{brendel2017decision}. It starts with an example in the target category and optimizes the attack with random selection and validation. %extracts the perturbation from the proposed distribution, and keeps the perturbation if it is closer to the benign example and still belongs to the target category, otherwise discards it and resamples a new perturbation. 
This method is simple and effective; however, it is inefficient because the information from the sampled examples is not fully utilized e.g., information from the 'worse' samples. Several methods attempt to bridge this gap. For example, \cite{brunner2019guessing} biases the sampling process by combining low-frequency noise with gradients from surrogate models. However, its performance depends on the transferablity between the surrogate model and the target model.
%this approach has high requirements on the surrogate model. More precisely, the performance of the attack depends on the transferablity between the surrogate model and the target model. %DBAR does not rely the transferability assumption and can generate distribution that have good transferability.
Similarly, transfer-based attacks~\cite{papernot2017practical} also rely on carefully chosen surrogate models. However they obtain an attack on the original model by attacking the surrogate model. Opt attack~\cite{cheng2018query} transforms the adversarial attack problem into a continuous real-valued optimization problem, i.e., the direction and distance to the decision boundary. This optimization problem can be solved by any zeroth-order optimization algorithm. However, distance calculation and gradient estimation in large dimensions will consume a large number of queries, which reduces the efficiency of the algorithm. % similar 
Evo attack~\cite{dong2019efficient} applies evolutionary algorithms to generate adversarial perturbations and employs some techniques to reduce the dimensionality of the search space. 
It uses a custom variant in normal distribution and update the variant with (1+1)-CMA-ES. However, the variance is sign-independent and the sampling is therefore unstable.
%CAB~\cite{shi2020polishing} uses the noise to model the sensitivity of each pixel and polishes adversarial noise of each image with a customized sampling setting.
Rays~\cite{chen2020rays} uses the dichotomous method to find perturbations. Although it has achieved good results on many datasets, the effectiveness of the algorithm is difficult to prove as results depend strongly on the test set.
HSJ~\cite{chen2020hopskipjumpattack} estimate gradient in different way and achieve a decision-based attack. However, gradient estimation is time-consuming and, at the same time, reduces the transferability of the generated adversarial perturbations. In this paper, we try to solve the problem without estimating the gradient.
%\todo[inline]{This argument is too weak. Do we show that we require fewer evaluations? That we have faster convergence or require less queries?}
%estimate the gradient on the decision boundary with binary outputs directly.



% - Towards deep learning models resistant to adversarial attack

%\subsection{score based attacks}
%- zeroth-prder optimization


%\clearpage
%\todo[inline]{add 'Pixelwise' before the distirbution?}
\section{Methodology}
\label{sec: methodology}

%\todo[inline]{Aside form formula (1) and the reward definition, everything reads like extremly generic actor critic. I believe we can condense this part a lot and also remove a lot of formulas since they are generic and not problem specific anyway.  - yes }

We model the decision-based black-box adversarial attack problem as finding the adversarial distribution $p_\Theta$ with parameters $\Theta$ of an $m$-class deep classification model $\mathcal{M}: \mathbb{R}^d \to [m]$ that accepts an input $x \in [0,1]^d$ and outputs $y \in [m] = \left\{1, \cdots, m\right\}$. The objective function can be described as
%\todo[inline]{If you write a min add the optimisation variable. Also what is the first argument of $P(\cdot \mid \Theta)$ what does "$\cdot$" stand for it stand for? is it not actually $p(\eta \vert \Theta)$ ?}
%\todo[inline]{I think there are [ brackets missing}
\begin{equation}
    \min_\Theta\left( \lambda \underset{\eta \sim p_\Theta}{\mathbb{E}}\left\|\eta \right\|_\infty - \underset{\eta \sim p_\Theta}{\mathcal{P}}(\mathcal{M}(x + \eta) \neq \mathcal{M}(x))\right),
    \label{eq: opt_obj}
\end{equation}
where $\eta$ is the adversarial perturbation sampled from the distribution $p_\Theta$, $\left\|\cdot\right\|_\infty$ denotes the $l_\infty$ norm and $\mathcal{P}$ evaluates a probability. The objective function consists of two components, the expected $l_\infty$ norm of the perturbations sampled from the distribution and the attack success rate of the perturbations sampled from the distribution. $\lambda$ is a parameter that trade-off between the expectation and the success rate. The goal of the problem definition is to find a distribution such that the center of the distribution found is as close as possible to the benign example $x$ while the adversarial perturbations sampled from the distribution maintain a high attack success rate.
%with less perturbation added in compared to the benign example. 

%\todo[inline]{Would it be better to delete this paragraph?}
%According to three realities: 1) we have no information about the gradient and logit information of the model and gradient estimation is consuming; 2) we get the ability to disturb benign examples at any time; and 3) the output of the classifier is deterministic and depends only on the input example, which is in accordance with Markov's rule, we propose a reinforcement framework to solve the optimization problem. In the rest of this section, we first present the two different attack settings and then introduce the methodology to optimize the agent.
% \subsection{Preliminaries}
% In reinforcement learning, the environment plays a role in guiding the learning direction of the agent. It consists of four parts: a state space $\mathcal{S}$; an action space $\mathcal{A}$; a state transition model $T(\mathcal{S},\mathcal{A},\mathcal{S})\sim p(s'\vert s,a), [s,s'] \in \mathcal{S}, a \in \mathcal{A}$ which denotes the probability of taking action $a$ at the state $s$ transfers the current state to $s'$; and a reward function $r:\left\{\mathcal{S}, \mathcal{A}\right\}\to \mathbb{R}$ denotes the reward of taking action $a$ at the state $s$. The goal of reinforcement learning is:
% \begin{equation*}
% \begin{split}
%     \theta^* &= \underset{\theta}{\operatorname{argmax}}\quad J(\theta) \\ &= \underset{\theta}{\operatorname{argmax}}\quad \mathbb{E}_{\tau \sim p_\theta(\tau)}\left[\sum_{t=0}^T\gamma^t r(s_t,a_t)\right],\\
%     p_\theta(\tau)&=p(s_0)\prod_{t=0}^{T-1}\pi_\theta(a_t\vert s_t)p(s_{t+1}\vert s_t, a_t),
%     \label{eq: rl_obj}
% \end{split}
% \end{equation*}
% where $J(\theta)$ is the objective function, $\tau = \left(s_0, a_0, s_1, a_1, \cdots, s_T, a_T\right)$ is a trajectory, $T$ is the number of actions taken in a trajectory and $p_\theta(\tau)$ is trajectory distribution induced by the agent $\pi: \left\{\mathcal{S}, \mathcal{A}\right\}\to [0,1]$ that takes a state as input and predict the probability of each available action. $\gamma$ denotes the weight of the reward.

%\subsection{DBAR}

To solve \eqref{eq: opt_obj} through a reinforcement learning algorithm, depending on whether single or multiple benign examples are considered, we design two different environments: a context-free environment and a context-aware environment, which correspond to context-free attack and context-aware attack.
%\todo[inline]{If $p(\cdot\vert\Theta)$ is the perturbation distribution, why don't we write $p(\rho \vert \Theta)$? Also why d we use $\rho$ is looks very close to $p$ and $\mathcal{P}$. Maybe choose a symbol that is more different to your other symbols like $\eta$ or $\epsilon$.}
Both environments share the same setting except the transition model. The state and action space are both set to $\mathbb{R}^d$. The perturbation distribution $p_\Theta$ to optimized is regarded as the agent. Each adversarial perturbation $\eta$ sampled from the distribution is an action and each benign example $x$ is a state. Since the action is continuous, we model the agent with normal distribution $p_\Theta(\eta \mid x)=\mathcal{N}\big(\eta \mid \mu_\Theta(x), \text{diag}\left(\Sigma_\Theta(x)\right)\big)$. A trajectory $\tau$ consists of fixed number of decision step. In each decision step, a perturbation (action) is sample from the distribution (agent) and send to the environment to get the reward and next state.
To achieve the optimization goal, we define the reward function as
\begin{equation}
    r(x, \eta) = \frac{2\cdot \mathbbm{1}_{\left\{\mathcal{M}(x + \eta)\neq \mathcal{M}(x)\right\}}-1}{\left\|\eta\right\|_\infty},
    %r(x, \eta) = \mathbbm{1}_{\left\{\mathcal{M}(x + \eta)\neq \mathcal{M}(x)\right\}} - \alpha\left\|\eta\right\|_\infty
    \label{eq: reward}
\end{equation}
where $\mathbbm{1}$ is the indicator function to identify whether adversarial perturbations mislead the classifier $\mathcal{M}$. When the attack is successful, \mbox{$2\cdot \left(\mathbbm{1}_{\left\{\mathcal{M}(x + \eta)\neq \mathcal{M}(x)\right\}}-1\right) = 1$}, the algorithm can try to increases the reward by shrinking the perturbation. On the other hand, if the attack fails, the algorithm may try to find a successful attack by increasing the perturbation \footnote{Eq.~\ref{eq: reward} can be modified to a target attack by setting the condition of indicator function to $\mathcal{M}(x + \eta) = \text{target}$.}. %Another advantage of using division is that the smaller the perturbation, the greater the impact of its change on the reward. 
The expectation and probability in \eqref{eq: opt_obj} are approximated by Monte Carlo estimation using sampled trajectories.

%\todo[inline]{I am not sure if contexts should be considered states. In my understanding a context is something fixed, while the state is changing and influenced by the actions of the agent. - no idea how to change}
In both environment settings, the next state is selected independent of the current state and action. In the context-free environment, only one state, the benign example, exists, while in the context-aware environment, the next state is selected by random sampling.

The objective function can be written as:
\begin{equation}
\begin{split}
    J(\Theta) &= \int p_\Theta(\tau)\left(\sum_{t=0}^Tr(x_t, \eta_t)\right) d\tau,\\
    &p_\Theta(\tau) \approx \prod_{t=0}^{T-1}p_\Theta(\eta_t\vert x_t).
\end{split}
\label{eq: adv_obj}
\end{equation}
%\todo[inline]{the 2nd line does not contain any $\tau$ however you still integrate over $d\tau$
%Also, why is this $\approx$ and not $=$ ? - the constant removed}
%\todo[inline]{Why is the last line approximate? What is $x_t$ ? What does a state transition mean ? $x_t = \eta_t + x_{t-1}$? with $\eta \sim \mathcal{N}(..)$ ? ? approximate because of i remove the constant and there is no state transition here}


\iffalse
The corresponding transition models are different for different environments.
\textbf{The context-free environment} has only one state: the benign example $s_0$. Therefore, $p(s_0) = 1$ and transition function can be described as:
\begin{equation}
    p(s'\vert s,a) = \begin{cases}
    1\qquad s' = s_0,\\
    0\qquad s' \neq s_0.
    \end{cases}
\end{equation}

\textbf{The context-aware environment} optimizes the agent with multiple benign examples from the dataset that used to train the targeted model. Without loss of generality, we assume that there are a total of $n$ examples. we have $p(s_0) = \frac{1}{n}$. New state is generated with random selection from the example set. The transition function can therefore be described as:
\begin{equation}
    p(s'\vert s,a) = \frac{1}{n}.
\end{equation}
\fi
%缺少表示任意的符号
%\todo[inline]{So you need some gradients after all. Why are those cheaper than the ones the other people need ? - we do not estimate the gradient, this is the part that query-consuming}
%\todo[inline]{I feel oyu describe to many basic RL details. I feel all of that could be ommited since it is very generic. The only problem related part of these formulas is the occurance of $\eta$ - removing the boundary of trajectory is also the important part. that's why we can use mini batch to train the network. In fact, What important to us is only the function 4, that is the way we build the program. All the other is just showing the way to function 4. I have no idea if we can delete them and introduce function 4 directly?}
%\todo[inline]{Replace "the optima distribution" with $\pi$ or $p$ or whatever it is in the formulas.}
To learn $\Theta$, we need to calculate the gradient of the objective function \eqref{eq: adv_obj}. In the black-box adversarial attack, each reward in one trajectory is treated equally and does not depend on the actions in the other time step of the same trajectory. Therefore, when calculating the gradient, at time step $t$, terms that do not depend on the action $\eta_t$ can be omitted. Using the log derivation trick and Monte Carlo sampling~\cite{shapiro2003monte}, the gradient of the objective function can be expressed as

\begin{equation*}
\begin{split}
    \nabla J(\Theta) &\approx \sum_{i=0}^{I} \left[\left(\nabla_\Theta \operatorname{log}\left(\prod_{t=0}^{T-1}p_\Theta(\eta_{t,i}\vert x_{t,i})\right)\right) \left(\sum_{t=0}^T r(x_{t,i}, \eta_{t,i})\right)\right]\\
    %&=\mathbb{E}_{\tau\sim p_\Theta(\tau)}\left[\left(\sum_{t=0}^{T-1}\nabla_\Theta \text{log} \pi_\Theta(a_{i,t}\vert s_{i,t})\right)\\ &\left(\sum_{k=0}^{t-1}r(s_{i,k},a_{i,k})+r(s_{i,t},a_{i,t})+\sum_{k=t+1}^{T}r(s_{i,k},a_{i,k})\right)\right]\\
    &\approx \sum_{i=0}^{I} \left[ \sum_{t=0}^{T-1}r(x_{t,i},\eta_{t,i})\nabla_\Theta \operatorname{log} p_\Theta(\eta_{t,i}\vert x_{t,i})\right]\\
    &= \sum_{i=0}^{I \cdot T}r(x_{i},\eta_{i})\nabla_\Theta\operatorname{log} p_\Theta(\eta_{i}\vert x_{i})
\end{split}
\end{equation*}

%\todo[inline]{The reward does not have any parameters $\Theta$. The gradient should be in front of the log p. In the last line.}

%\todo[inline]{Why do we actually use $\Theta$ instead of $\theta$ ? $\theta$ is used in the preliminary, just to make it different}

%\todo[inline]{Please write your formulas with care. There are missing $d..$ after the integrals. Also please check again if the $\nabla$ is really at the correct place.}
%\todo[inline]{Is M no the number of MC samples or the t ? It cannot be both at the same time and would require a double index (timestep, sample) in my oppinion.  because it is double $\sum$, make $n$ the number of trajectories and $T$ the number of time step a trajectory. M = n * T}
%\todo[inline]{Why is the $\nabla$ in front of the reward? it does not depend on the decision variable. Also what is the gradient taken wrt in $\nabla J$. r here is a constant and can also be putted before the $\nabla$}

So far, this gradient is valid only for the samples generated by $p_\Theta$. We apply the importance sampling %~\cite{hastings1970monte} 
technique so that old trajectories can be reused. In addition, we limit the update step size as suggested in~\cite{schulman2017proximal}, since importance sampling only works when the update size is small. Together with the stable training trick mentioned in~\cite{mnih2016asynchronous}, the gradient can be expressed as
%\todo[inline]{Use $\cdot$ for scalar multiplication, not $*$}
\begin{equation*}
\begin{split}
    \nabla J(\Theta) 
    &= \sum_{i=0}^{M} \left(\nabla_\Theta\operatorname{min}(w_i(\Theta), \operatorname{clip}(w_i(\Theta), 1-\epsilon, 1+\epsilon))\right)\cdot 
    %&\qquad r(x_{i},\eta_{i}),\\%\operatorname{log}p_\Theta(\eta_i\vert x_i),\\
    \left(r(x_i,\eta_i) - V(x_i)\right) \\
    & \text{with} \quad w_i(\Theta) = \frac{p_\Theta(\eta_i\vert x_i)}{p_{\Theta_{\textrm{old}}}(\eta_i\vert x_i)},\\
\end{split}
\end{equation*}

% \begin{equation*}
% \begin{split}
%     &\nabla J(\Theta) \\
%     &= \sum_{i=0}^{M} \left(\nabla_\Theta\operatorname{min}(w_i(\Theta), \operatorname{clip}(w_i(\Theta), 1-\epsilon, 1+\epsilon))\right)\cdot\\
%     %&\qquad r(x_{i},\eta_{i}),\\%\operatorname{log}p_\Theta(\eta_i\vert x_i),\\
%     &\qquad\left(r(x_i,\eta_i) - V(x_i)\right)\\
%     &w_i(\Theta) = \frac{p_\Theta(\eta_i\vert x_i)}{p_{\Theta\_{\operatorname{old}}}(\eta_i\vert x_i)},\\
% \end{split}
% \end{equation*}
where $p_{\Theta_\textrm{old}}$ is the distribution that generates the training samples and $p_\Theta$ is the distribution, that is frequently updated. The parameter $\epsilon$ limits the update size and $V(x_i)$ is the expected reward given to a benign example $x_i$.
% So far, assuming that all rewards are positive, the gradients of good samples (high rewards) receive more reinforcement than the gradients of poor samples. The directions of all the gradients differ greatly, which will affect the efficiency of the update. Subtracting a state-dependent expected reward from the reward of samples can make the gradient vectors of the pool samples 'negated' and all gradient vectors show in similar direction~\cite{mnih2016asynchronous}. The gradient can be expressed as:
% \begin{equation}
% \begin{split}
%     &\nabla J(\Theta) \\
%     &= \sum_{i=0}^{M} \left(\nabla_\Theta\operatorname{min}(w_i(\Theta), \operatorname{clip}(w_i(\Theta), 1-\epsilon, 1+\epsilon))\right)\cdot\\
%     &\qquad\left(r(x_i,\eta_i) - V(x_i)\right),
% \end{split}
% \end{equation}
% where, $V(x_i)$ is the expected reward given benign example $x_i$.


% 介绍actor，critic
% \begin{figure}[!t]
%     \centering
%     \includegraphics[width=3.3in]{plots/dbar.pdf}
%     \caption{Illustration of an iteration of the proposed DBAR algorithm. }
%     \label{fig: dbar}
% \end{figure}
%\todo[inline]{maybe $\operatorname{Actor}:\mathbb{R}^d\to \mathbb{R}^d \times \mathbb{R}^d$ if it maps to two outputs}
Algorithm \ref{alg: dbar} summarizes the process of generating adversarial distribution, where Actor: $\mathbb{R}^d\to \mathbb{R}^d \times \mathbb{R}^d$ and
Critic: $\mathbb{R}^d\to \mathbb{R}$ are two neural networks with the same ResNet architecture except for the output layer. %They all consist of 19 convolutional layers and one fully connected layer. Actor takes a benign example as input and outputs the mean and standard deviation parameters of a normal distribution, while Critic predicts the expected reward for a given benign example. Figure~\ref{fig: dbar} shows an illustration of one iteration of the proposed DBAR algorithm. Each iteration consists of two parts: the sampling process and the updating process. In the sampling process, a benign example is fed into the Actor network to produce the mean and standard deviation of a normal distribution. With these parameters, a normal distribution is instantiated and an perturbation is sampled from the distribution. The environment takes the perturbation and gives feedback on the reward and next benign example. The triple (reward, log probability of the perturbation, benign example) is stored in the buffer. The next benign example generated by the environment is again fed back to the Actor network to generate next perturbation. This process is repeated $M$ times, after which an update process is executed. During the update process, all necessary information is saved in the buffer. Actor and Critic networks are updated as described in Algorithm~\ref{alg: dbar} with minibatch~\cite{li2014efficient}. 
%详细点介绍结构，这里的actor的输入输入是image的输入，和说法对不上，并不适用于所有的数据

%\todo[inline]{Why do we have an nn-critic? Why can this not simply be a table?}

\begin{algorithm}[tb]
\caption{Generating adversarial distribution through DBAR}
\label{alg: dbar}
\textbf{Input}: $x_0$ (benign example), $N$ (number of iterations), $M$ (number of samples in one iteration), $K$ (number of training), $L$ (size of minibatch), $init\_mean$, $init\_std$, $\epsilon$\\
%\textbf{Parameter}: Optional list of parameters\\
\textbf{Output}: Actor
\begin{algorithmic}[1] %[1] enables line numbers
\STATE Initialization: Initialize Actor($\cdot$) with $init\_mean$ and $init\_std$, Critic($\cdot$), $x \gets x_0$
\FOR{$i \gets 1$ to $N$}
\STATE $B \gets [ \;]$
\FOR{$j \gets 1$ to $M$}
\STATE $\mu$, $\mathbf{I}\cdot \sigma^2$$\gets$ Actor($x$)
\STATE $r, p, x'$ $\gets$ sample action from $\mathcal{N}(\mu,\mathbf{I}\cdot \sigma^2)$, calculate its log-probability and applied it to the environment to get reward and the next benign example
\STATE $B \gets B\cup \left\{\text{r}:r,\text{lp}:lp,\text{x}:x\right\}, x\gets x'$
\ENDFOR
\FOR{$k \gets 1$ to $K$}
\STATE $\left\{B_1,\cdots B_{\left \lfloor M/L \right \rfloor}\right\}$ $\gets$ generate mini-batch from $B$
\FOR{$b \gets \left\{B_1,\cdots B_{\left \lfloor M/L \right \rfloor}\right\}$}
\STATE $\mu', \mathbf{I}\cdot (\sigma')^{2} \gets$ Actor($b[\text{x}]$)
\STATE $lp' \gets$ compute log-probability of $b[\text{x}]$ in $\mathcal{N}(\mu', \mathbf{I}\cdot (\sigma')^2)$
\STATE $v \gets$ Critic($b[\text{x}]$)
\STATE $a \gets b[\text{r}] - v$
\STATE $w \gets \exp(lp'-b[\text{lp}])$
\STATE loss\_actor $\gets \text{min}(w, \text{clip}(w, 1-\epsilon, 1+\epsilon))\cdot a$
\STATE loss\_critic $\gets \text{MSE}(b[\text{r}], v)$
\STATE update Actor with the gradient of loss\_actor
\STATE update Critic with the gradient of loss\_critic
\ENDFOR
\ENDFOR
\ENDFOR
\STATE \textbf{return} Actor
\end{algorithmic}
\end{algorithm}

%\todo[inline]{For your algorithm and figures do not use $m$ and $std$ this looks bad. Also there is no "std" but a covariance matrix. Use $\mu$ and $\Sigma$ or, if your covariance matrix is diagonal $\mathbf{I}\cdot \sigma^2$.}

%\clearpage
\section{Experiments}
\label{sec: 4}
%In this section, we carry out experimental analysis of DBAR algorithm. We test the effectiveness of DBAR on different arts of datasets, including images and time series and compare the transferability of DBAR with several recently proposed decision-based attacks. In addition, we analyze the impact of different parameters in the DBAR algorithm. In summary, the following questions are investigated: 

In this section, we perform experiments to investigate the following questions:
(i) How does the context-free DBAR algorithm perform on image datasets compared to state-of-the-art decision-based attack methods?
(ii) Can context-free DBAR be applied to time-series datasets?
(iii) Are the perturbations discovered by the context-free DBAR algorithm more transferable than those discovered by state-of-the-art decision-based attack methods?
(iv) Can context-aware DBAR perform real-time attacks after training?
(v) How do the hyper-parameters affect the performance of context-free DBAR?
%(vi) Can the context-free DBAR algorithm attack super pixels in images?

%In the following, we will first introduce the experiment setting and then try to answer the respective questions experimentally.

\subsection{Experiment Setting}
\label{sec: 4.1}

\textbf{Baselines and hyper-parameters}: To evaluate DBAR, we compare it with the following decision-based attacks: (i) the state-of-the-art Decision-based black-box Boundary attack~\cite{brendel2017decision} and the HopSkipJump Attack~\cite{chen2020hopskipjumpattack} for image datasets. (ii) the Universal White-box attack FGSM~\cite{szegedy2013intriguing} and BIM~\cite{kurakin2016adversarial} for time series datasets. %~\footnote{We tried to apply the HopSkipJump Attack defined in Foolbox~\cite{rauber2017foolbox}to time series data, but it doesn't work.}.

All attacks are implemented by the python package Foolbox~\cite{rauber2017foolbox}. We use the default hyper-parameter settings for all attacks with a fixed random seed. %The maximal iteration number ($N$) is set to 100 with 200 samples ($M$) in each iteration. 
We limit the maximum number of queries for all the attacks to 20000. For the DBAR algorithm, the number of training epoch $K$ is set to $K=10$, with a mini-batch size of $L=10$ and $\epsilon=0.02$. Additionally $init\_mean=0$ and $init\_std=0.5$.
% add model for time series attack

\textbf{Datasets and models}: We carried out attacks over the following image datasets with varied dimensions and dataset sizes: CIFAR10~\cite{krizhevsky2009learning}, CIFAR100~\cite{krizhevsky2009learning}, STL10~\cite{coates2011analysis}, Caltech101~\cite{griffin2007caltech}. The pixel values of all images are normalized to $[0, 1]$. We also attack models with different structures such as ResNet20~\cite{he2016deep} with 272474 parameters and VGG11~\cite{simonyan2014very} with 9756426 parameters. Both models were obtained from Pytorch~\cite{paszke2019pytorch}.
In addition, we carried out attacks over the publicly available time series UCR archiv dataset~\cite{dau2019ucr} and attack the time series ResNet-ts model as defined by~\cite{fawaz2019adversarial}.  %\footnote{Due to the limitation of pages, we only conducted non-targets attack experiments. By modifying the reward function, the proposed method can be easily applied to target attack experiments.}. 


\subsection{Adversarial examples for image and time series data}
\label{sec: 4.2}

%To prove the effectiveness of the proposed algorithm, w
We perform non-target attacks on all the image datasets mentioned above and summarise the attack success rate (ASR) of each methods in Table~\ref{tab: exp1}. Concretely, all attacks are applied to $1000$ correctly classified test examples from each dataset. If an adversarial example can mislead the classification model and is in a 0.04 (10/255) $l_\infty$ neighborhood of the benign example, we denote this attack as a success. 
From the result, we see that, context-free DBAR achieves better results on most of the datasets, except for the STL10 dataset on the VGG11 model. Struggling with finding the first success attack is probably the main reason for the failure of the attack. This happens when the $init\_std$ is set too small for the given data set. The influence of the hyper-parameters on the attack performance an run-time is analyzed in experiment~\ref{sec: 4.6}. %In addition, the ASR score of DBAR against ResNet20 is generally higher than that of the attacks against VGG11. This is likely to be influenced by model complexity (number of parameters) rather than model accuracy. %This is because, although the model of VGG11 contains more parameters,%~\footnote{In this experiment, the number of parameters of model VGG11 is about 30 times higher than that of model ResNet20.}, 
%the classification accuracy of the ResNet20 model is higher than that of VGG11 in Table ~ref{tab: exp1} in most cases. This feature is also reflected in other attack methods. The data for Cifar10 and Cifar100 have the same complexity (dimensions), but Cifar100 is a 100-class classification task, which is more difficult than Cifar10, which is a 10-class classification task. 
%Comparing the performance of DBAR on dataset Cifar10 and Cifar100, we see that the difficulty of the task has a relatively small impact on the DBAR's performance. However, the complexity of the data impact the ASR score: the more complex the data, the smaller the success rate. 
%Many elements affect the time spent for attacking an example including the complexity of the targeted model, the size of the benign example, the parameter setting of the algorithm and configuration of the server. In this experiment, it takes about 5 minutes on average for context-free DBAR algorithm to attack an example from Cifar10 dataset against ResNet20 model, while HSJ takes on average 3.2 minutes and Boundary attack 1.5 minutes. How the parameters affect the attack performance and running time is analysed in the experiment~\ref{sec: 4.6}.




\begin{table}
\centering
\resizebox{\textwidth}{!}{\begin{tabular}{|c|c|c|c|c|c|c|c|c|c|c|c|}
\hline
\multicolumn{2}{|c|}{~}&Model Accuracy&BA&HSJ&DBAR&\multicolumn{2}{|c|}{~}&Model Accuracy&BA&HSJ&DBAR\\
\hline
\multirow{2}*{Cifar10}&ResNet20&0.91&\textbf{1.00}&\textbf{1.00}&\textbf{1.00}&\multirow{2}*{STL10}&ResNet20&0.83&\textbf{1.00}&\textbf{1.00}&\textbf{1.00}\\
\cline{2-6}\cline{8-12}
~&VGG11&0.90&0.71&0.78&\textbf{0.84}&~&VGG11&0.84&0.59&\textbf{0.90}&0.78\\
\hline

\multirow{2}*{Cifar100}&ResNet20&0.66&0.95&0.95&\textbf{0.96}&\multirow{2}*{CalTech101}&ResNet20&0.79&0.93&\textbf{1.00}&\textbf{1.00}\\
\cline{2-6}\cline{8-12}
~&VGG11&0.61&0.63&0.80&\textbf{0.84}&~&VGG11&0.68&0.51&0.80&\textbf{0.82}\\
\hline

%\multirow{2}*{STL10}&ResNet20&0.83&\textbf{1.00}&\textbf{1.00}&\textbf{1.00}\\
%\cline{2-6}
%~&VGG11&0.84&0.59&\textbf{0.90}&0.78\\
%\hline

%\multirow{2}*{CalTech101}&ResNet20&0.79&0.93&\textbf{1.00}&\textbf{1.00}\\
%\cline{2-6}
%~&VGG11&0.68&0.51&0.80&\textbf{0.82}\\
%\hline

\end{tabular}}
\caption{Attack success rate (ASR) of three different decision-based attack methods: Boudary Attack (BA), HopSkipJump Attack (HSJ) and the proposed context-free DBAR, against model ResNet20 and VGG11 on four different image datasets with varies sizes.}% Acc is the classification accuracy of the target black-box model.}
\label{tab: exp1}
\end{table}

% 解释为什么ba的成功率下降那么多，主要是维度大了，在有限的搜索次数下，他的成功率就会不足
% 

%\subsection{Adversarial examples for time series data}
%\label{sec: 4.3}

DBAR is universal in the sense that it is able to attack any example in form of $\mathcal{R}^d$. To prove this, we compare the performance of the proposed context-free DBAR against ResNet-ts model on the UCR open source time series datasets with the two popular  white-box attacks FGSM and BIM. When the size of the perturbation found by an attack is smaller than 0.1, as proposed in~\cite{fawaz2019adversarial}, and the adversarial example can mislead the classification, we regard the attack as success. The parameter settings of this experiment are different from other experiments because the time series data are not normalized to $[0, 1]$. Furthermore, note that DBAR, as opposed to FGSM and BIM is a black-box attack. We remove the step limit and set the initial standard deviation to $80$ to avoid struggling with finding the first successful attack. %~\footnote{80 is just a large number to create a large initial perturbation. So that the algorithm will not struggle with finding the first success attack}. 

%\todo[inline]{How did we find that 80 ? Is that the sneaky hyper-parameter that gives us better results compared to other methods? If you don't give any reason how to choose this or how you have found it people will get suspicious. If it is just some high number to have similar density for all of the space write that.}

\begin{table}
\centering
\resizebox{\textwidth}{!}{\begin{tabular}{|c|c|c|c|c|c|c|c|c|c|}
\hline
&Model Accuracy&FGSM&BIM&DBAR&&Model Accuracy&FGSM&BIM&DBAR\\
\hline
50words&0.73&0.77&0.88&\textbf{0.91}&DistalAge&0.80&0.78&\textbf{0.79}&0.64\\
\hline

Adiac&0.83&0.96&0.98&\textbf{0.99}&FaceAll&0.86&0.10&0.15&\textbf{0.20}\\
\hline

Beef&0.77&0.74&\textbf{0.87}&\textbf{0.87}&FaceUCR&0.95&0.17&\textbf{0.20}&0.19\\
\hline

Car&0.93&0.76&\textbf{0.92}&0.80&ElectricDevices&0.74&0.34&0.58&\textbf{1.00}\\
\hline

Diatom&0.30&0.00&0.00&\textbf{0.41}&ItalyPowerDemand&0.96&0.04&0.04&\textbf{0.20}\\
\hline

\end{tabular}}
\caption{Attack success rate (ASR) of three different adversarial attack methods: FGSM~\cite{szegedy2013intriguing}, BIM~\cite{kurakin2016adversarial} and the proposed context-free DBAR, against ResNet-ts on ten different time series datasets with varies size and dimensions.}% Acc is the classification accuracy of the target black-box model}
\label{tab: exp2}
\end{table}

The results can be seen in Table~\ref{tab: exp2}. Although compared to the white-box algorithms, the context-free DBAR achieves better results on six datasets, ties on one dataset, and worse results on three datasets. As for the image datasets, the ASR score on the time series data is independent of the accuracy of the target model. And probably because of the different principles of generating attacks, DBAR performs well on some datasets where BIM performs very poorly, e.g., Diatom, ItalyPowerDemand. At the same time, the opposite situation also exists, see DistalAge. It is important to note that although FGSM has the worst results, it is attacking in real time, while both BIM and the proposed context-free DBAR require multiple iterations. However, a similar real-time attack can be achieved by context-aware DBAR. This is demonstrated in Section~\ref{sec: 4.4}.

\subsection{Transferability of the perturbation distribution and real-time attack}
\label{sec: 4.4}
%There are a growing number of cloud-based machine learning services, e.g., the Cloud Vision from Google and the Rekognition from Amazon. 
The high transferability of the perturbation allows attacks generated against one platform to be applied to the other.
To demonstrate the transferability of the perturbation found by the proposed method, we run the experiment in Section~\ref{sec: 4.2} on the Cifar datasets again and apply the attacks generated against the ResNet20 model to the VGG11 model and vice versa. The ASR score is given in Table~\ref{tab: exp3}. 
%It is important to note that the performance of the two models (ResNet20 and VGG11) is different. Many examples exist where one model is able to predict correctly while another model is not. At this point any generated perturbation can launch a successful attack. This may be the reason that, in the Table~\ref{tab: exp3}, that Boundary Attack and HSJ have the same ASR score in most cases. 
The performance of the proposed method is significantly better than that of the other methods. In particular, the perturbations generated against the VGG11 model on Cifar100 dataset have a near-average ASR score against the ResNet20 model. Besides, we can see that perturbations generated against simple model (with fewer parameters, ResNet20) are difficult to perform success attack against the more complex VGG11 model. %Limiting the reduction speed of the standard deviation of the distribution during training may improve this success rate. 

% This 
%\multicolumn{2}{|c|}{~}
\begin{table}
\centering
\resizebox{\textwidth}{!}{\begin{tabular}{|c|c|c|c|c|c|c|c|c|c|}
\hline
&Targeted model&Boundary Attack&HSJ&DBAR&&Targeted model&Boundary Attack&HSJ&DBAR \\
\hline
\multirow{2}*{Cifar10}&ResNet20&0.04&0.04&\textbf{0.13}&\multirow{2}*{Cifar100}&ResNet20&0.14&0.14&\textbf{0.24}\\
\cline{2-5}\cline{7-10}
~&VGG11&0.10&0.10&\textbf{0.36}&~&VGG11&0.14&0.09&\textbf{0.45}\\
\hline

% \multirow{2}*{Cifar100}&ResNet20&0.14&0.14&\textbf{0.24}\\
% \cline{2-5}
% ~&VGG11&0.14&0.09&\textbf{0.45}\\
% \hline

\end{tabular}}
\caption{Attack success rate (ASR) of attacks generated for VGG11 and applied to ResNet20 and vice versa. Targeted model denotes the model used to generate the perturbation (attack).}
\label{tab: exp3}
\end{table}

%\subsection{Real-time Attack}
%\label{sec: 4.5}
 %There are also scenarios especially in cloud-based machine learning services, every query incurs with a huge cost~\cite{khalid2020fadec}, e.g., the cloud vision API from Google allow 1800 requests per minute in 2020.
%for example, online image classification platforms often set a limit on the allowed number of queries within a certain time period, e.g., the cloud vision API from Google currently allow 1800 requests per minute~\cite{chen2020hopskipjumpattack}.
%However, when the number of queries is limited, e.g., smaller then 1000, most black-box attack algorithms struggle to produce effective attacks. The average $l_\infty$ adversarial perturbations on Cifar dataset generated by Boundary attack is 0.13 and 0.10 for HopSkipJump Attacks. Context-aware DBAR algorithm, in contrast, can generate successful attack (in 16/255 $l_\infty$ neighbor of the benign example) without any queries after the training. 
To evaluate the capabilities for real time attacks, we apply context-aware DBAR against ResNet20 on the Cifar10 and Cifar100. The results can be seen in Fig.~\ref{fig: exp4}. We can find that after 60 iterations, the adversarial perturbation obtained by sampling are able to implement effective attacks that have a success rate of about 50\% on Cifar10 and 60\% on Cifar100. We observe higher $l_\infty$ norms for the perturbations at the beginning, which is most likely caused by the different update directions between the different benign examples. Ideally, the algorithm should reduce the size of the perturbation while increasing the success rate of the training. However, the attack success rate decreases as the training progresses, although the decrease is not significant. The algorithm improves the rewards obtained by reducing the perturbation size. This problem can be mitigated by increasing the contribution of the success attack in the reward function, see~\eqref{eq: reward}.

%The results of the first four experiments show that the DBAR algorithm outperforms the state-of-the-art decision-based attacks with higher attack success rate and greater transferability on image datasets. It also surpasses two popular white-box attacks on attacking time-series datasets. In addition, the context-aware DBAR algorithm can perform real-time attack on Cifar datasets after the training. 

% 
\begin{figure}[!t]
    \centering
    %\includegraphics[width=3.3in]{plots/exp4.png}
    \includegraphics[width=\textwidth]{plots/exp4.png}
    \caption{Performance of context-aware DBAR against ResNet20 on Ciar10 (left) and Cifar100 (right) datasets. The picture on the left shows the performance of Cifar10 and the one on the right shows the performance of Cifar100.}
    \label{fig: exp4}
\end{figure}
% cifar10再跑一遍，修改以下variant

\subsection{Impact of the parameter selection}
\label{sec: 4.6}

\begin{figure*}[!t]
    \centering
    \includegraphics[width=\textwidth]{plots/exp5.png}
    \caption{Performance comparison of context-free DBAR with different hyper-parameter settings. The $l_\infty$ norm is shown as a solid line in the plot, while the run time is shown as a dashed line.}% The three plots from left to right correspond to different settings of the $init\_std$, batch size $L$, and number of training $K$ parameters, respectively.}
    \label{fig: exp5} 
\end{figure*}
%The choice of hyper-parameters has a significant impact on the performance of the proposed algorithm. 
In this experiment, we analyse the effect of following hyper-parameters on the performance of context-free DBAR regarding $init\_std$, number of training epoch in each iteration $K$ and the size of mini batch $L$. The baseline hyper-parameters used in the experiments are set as follows as $init\_std$ = 0.5, $L$ = 64, $K$ = 30. In each trial we modify one of the above parameters and summarise the result in Fig.~\ref{fig: exp5}. Evidently, the parameters have a great impact on the convergence speed, stability and runtime of DBAR. The leftmost plot in Fig.~\ref{fig: exp5} shows the effect of the standard deviation of the initial distribution $init\_std$. The larger the parameter, the larger the perturbation sampled from the initial distribution. In general, the larger the initial perturbation, the higher the probability that it will successfully attack a benign example. When the $init\_std$ is too small and the initial perturbation fails to attack the benign example, the algorithm will struggle in looking for the first successful attack. The size of the training batches $L$ affects the stability, where lower $L$ results in less stability. 

Since the number of samples per iteration is constant, the smaller the batch size, the more times the agent is updated and the longer each iteration takes.

In addition, due to the importance sampling, the log-probability of actions needs to be recalculated before updating agent, which further increases the runtime. These are reflected in the plot in the center of Fig.~\ref{fig: exp5}. The rightmost plot shows that larger number of training $K$ in one iteration converges faster. However, since there are more updates per iteration, it also takes longer to run.

% 分析不全面，只是单独的分析每个参数

% \subsection{Attack on super pixel}
% \label{sec: 4.7}

% \begin{figure}[!t]
%     \centering
%     \includegraphics[width=3.3in]{plots/exp6.png}
%     \caption{Performance comparison of context-free DBAR algorithm for attacking super pixels of different sizes. The x-axis represents the number of iterations, the left y-axis is the $l_\infty$ norm of the perturbation sampled from the agent, and the right y-axis is the run time. The $l_\infty$ norm is shown as a solid line in the plot, while the run time is shown as a dashed line.}
%     \label{fig: exp6}
% \end{figure}
% The super pixel of the input example can be attacked by adjusting the dimensionality of the Actor output. That is, the same perturbation is applied to modify the value of several pixel instead of one. In this experiment, we investigate the feasibility of super pixel decision attacks with context-free DBAR on image data and the impact of different sizes of super pixels on the attack performance. We use $\left[h,w\right]$ to denote the size of the super pixels ($h, w$ represent the number of pixels in height and width). Three different sizes of super pixels are compared in this experiment, namely [1,1], [2,2] and [4,4]. In this experiment, we keep the same parameter settings as in the first experiment~\ref{sec: 4.2}. The results of the experiments are summarized in Figure~\ref{fig: exp6}. We can see that for three different super pixel sizes, the proposed algorithm can achieve an effective attack within a perturbation range of 16/255. And the larger the size of the super pixel, the shorter the running time of the algorithm. It is also worth noting that the larger the size of the pixel, the larger the minimum $l_\infty$ norm of the perturbation the algorithm can reach after convergence. Therefore, it is worthwhile to choose the size of the super pixel according to the specific task.

% % l_infity distance 这个名字有问题，应该重新选，比如l_infity norm
% % 实验2的重新跑一下

\section{Conclusion}
\label{sec: 5}
%\todo[inline]{Do not use section references in your conclusion. Just think if you have seen something like that in any other paper. If not, avoid it.}
In this paper, we formulate the decision-based black-box adversarial attack as a reinforcement learning task and search for the adversarial attack based on reward criterion.
We have experimentally demonstrated the feasibility of the proposed algorithm. In addition, have shown some advantages of the algorithm such as the ability to generate attacks for different kinds of data such as images and time series, the transferability of attacks between different models and the ability for real-time attack. %At the same time, the performance of the algorithm depends strongly on the hyper-parameters. %, which depends on the complexity of the targeted model and the dimensionality of the benign example. 
Besides, there is still room for further exploration in the proposed approach through using different reward functions or investigating the usage bounds by attacking very small perturbation or high resolution images.

\section{Acknowledgements}
This work was partially funded by the Ministry of The Ministry of Science, Research and the Arts Baden-Wuerttemberg as part of the SDSC-BW and by the German Ministry for Research as well as by Education as part of SDI-C (Grant 01IS19030A)
%
% ---- Bibliography ----
%
% BibTeX users should specify bibliography style 'splncs04'.
% References will then be sorted and formatted in the correct style.
%
\bibliographystyle{splncs04}
\bibliography{mybibliography}
%

\end{document}
