\newcommand{\tablesize}{\small}



\begin{table*}[t] \tablesize
\centering
\caption{Classification Results of different methods for the IN dataset.}
\vspace{-1mm}
%\vspace{4pt}
\setlength\tabcolsep{4pt}
\renewcommand\arraystretch{1.1}
\begin{tabular}{cccccccccc}
\toprule
Class & SVM & SAE & EMAP & CNN & SPC & MCNN & 3D-CNN & SSRN & PDML \\
\midrule 
OA(100\%) & 81.67$\pm$0.65 & 85.47$\pm$0.58 & 91.65$\pm$0.63 & 97.41$\pm$0.43 & 90.68$\pm$0.75 & 98.30$\pm$0.20 & 97.56$\pm$0.43 & 99.19$\pm$0.26 & \textbf{99.55$\pm$0.09}\\
AA(100\%) & 79.84$\pm$3.37 & 86.34$\pm$1.14 & 95.15$\pm$1.05 & 97.39$\pm$0.56 & 92.00$\pm$2.84 & 97.40$\pm$0.40 &  99.23$\pm$0.19 & 98.93$\pm$0.59 & \textbf{99.37$\pm$0.21}\\
$\kappa \times 100$ & 78.76$\pm$0.77 & 83.42$\pm$0.66 & 90.46$\pm$0.68 & 83.42$\pm$0.66 & 89.36$\pm$0.86 & 98.00$\pm$0.30 & 97.02$\pm$0.52 & 99.07$\pm$0.30 & \textbf{99.48$\pm$0.12}\\
\midrule 
1 & 96.78 & 81.82 & 94.87 & \textbf{100.00} & 83.15 & 98.30 & \textbf{100.00} & 97.82 & 99.46\\
2 & 78.74 & 82.16 & 84.10 & 97.27 & 86.81 & 96.20 & 96.34 & 99.17 & \textbf{99.66}\\
3 & 82.26 & 77.54 & 95.66 & 98.00 & 87.34 & 95.30 & 99.49 & 99.53 & \textbf{100.00}\\
4 & 99.03 & 68.11 & 98.37 & 92.81 & 91.32 & 94.20 & \textbf{100.00} & 97.79 & \textbf{100.00}\\
5 & 93.75 & 94.36 & 95.53 & 99.25 & 97.54 & 98.20 & \textbf{99.91} & 99.24 & 99.23\\ 
6 & 85.96 & 94.45 & 96.84 & 99.52 & 97.88 & 98.70 & \textbf{99.75} & 99.51 & 99.65\\ 
7 & 40.00 & 94.70 & \textbf{100.00} & 97.58 & 89.33 & 96.40 & \textbf{100.00} & 98.70 & \textbf{100.00}\\ 
8 & 91.80 & 94.36 & 99.32 & 99.00 & 90.85 & \textbf{100.00} & \textbf{100.00} & 99.85 & 98.93\\ 
9 & 0.00 & 82.56 & \textbf{100.00} & 96.95 & \textbf{100.00} & 92.20 & \textbf{100.00} & 98.50 & 98.22\\ 
10 & 86.00 & 81.28 & 90.63 & 95.38 & 81.92 & 97.80 & 98.72 & 98.74 & \textbf{99.15}\\ 
11 & 70.94 & 84.47 & 89.16 & 97.72 & 91.68 & 99.60 & 95.52 & 99.30 & \textbf{99.65}\\ 
12 & 74.73 & 83.77 & 86.88 & 97.13 & 85.14 & 97.20 & 99.47 & 98.43 & \textbf{99.56}\\ 
13 & 99.04 & 96.42 & 98.77 & 99.65 & 99.72 & \textbf{100.00} & \textbf{100.00} & \textbf{100.00} & \textbf{100.00}\\ 
14 & 94.29 & 92.27 & 94.13 & 97.95 & 97.44 & 99.70 & 99.55 & 99.31 & \textbf{99.79}\\ 
15 & 85.11 & 80.63 & 98.18 & 92.30 & 93.43 & 98.80 & \textbf{99.54} & 99.20 & 98.69\\ 
16 & 96.78 & 81.82 & \textbf{100.00} & \textbf{100.00} & 83.15 & 95.80 & 99.34 & 97.82 & 97.86\\
\bottomrule
\end{tabular}
\label{tab:IN_result}
\vspace{-2mm}
\end{table*}


\begin{table*}[tb] \tablesize
\centering
\caption{Classification Results of different methods for the UP dataset.}
\vspace{-1mm}
%\vspace{4pt}
\setlength\tabcolsep{4pt}
\renewcommand\arraystretch{1.1}
\begin{tabular}{cccccccccc}
\toprule
Class & SVM & SAE & EMAP & CNN & SPC & MCNN & 3D-CNN & SSRN & PDML \\
\midrule 
OA(100\%) & 90.58$\pm$0.47 & 94.25$\pm$0.18 & 93.52$\pm$0.34 & 98.85$\pm$0.15 & 98.88$\pm$0.22 & 99.50$\pm$0.20 & 99.54$\pm$0.11 & 99.79$\pm$0.09 & \textbf{99.91$\pm$0.03} \\
AA(100\%) & 92.99$\pm$0.36 & 93.34$\pm$0.39 & 94.82$\pm$0.40 & 98.40$\pm$0.30 & 98.40$\pm$0.27 & 99.30$\pm$0.10 & 99.66$\pm$0.11 & 99.66$\pm$0.17 & \textbf{99.89$\pm$0.07}\\
$\kappa \times 100$ & 87.21$\pm$0.70 & 92.35$\pm$0.25 & 91.65$\pm$0.49 & 98.47$\pm$0.20 & 98.52$\pm$0.30 & 99.30$\pm$0.20 & 99.41$\pm$0.15 & 99.72$\pm$0.12 & \textbf{99.82$\pm$0.09}\\
\midrule
1 & 87.24 & 94.59 & 91.30 & 98.98 & 99.01 & 99.60 & 99.36 & 99.92 & \textbf{99.98}\\
2 & 89.93 & 96.44 & 91.83 & 99.45 & 99.81 & 99.40 & 99.36 & \textbf{99.96} & 99.94\\
3 & 86.48 & 84.57 & 72.22 & 96.40 & 95.46 & 99.40 & \textbf{99.69} & 98.46 & 99.68\\
4 & 99.95 & 97.37 & 99.80 & 99.58 & 99.54 & 99.80 & 99.63 & 99.69 & \textbf{100.00}\\
5 & 95.78 & 99.60 & 99.93 & 99.39 & 99.84 & 99.90 & 99.95 & \textbf{99.99} & \textbf{99.99}\\ 
6 & 97.69 & 99.39 & 99.62 & 99.70 & 99.18 & 99.80 & 99.96 & 99.94 & \textbf{99.98}\\ 
7 & 95.44 & 88.57 & 99.70 & 97.18 & 98.15 & 98.80 & \textbf{100.00} & 99.82 & 99.89\\ 
8 & 84.40 & 85.66 & 99.27 & 95.73 & 94.65 & 98.90 & \textbf{99.65} & 99.22 & 99.55\\ 
9 & \textbf{100.00} & 99.88 & 99.79 & 99.56 & 99.99 & 98.40 & 99.38 & 99.95 & \textbf{100.00}\\ 
\bottomrule
\end{tabular}
\label{tab:UP_result}
\vspace{-4mm}
\end{table*}

\begin{table*}[tb] \tablesize
\centering
\caption{Classification Results of different methods for the KSC dataset.}
\vspace{-1mm}
%\vspace{4pt}
\setlength\tabcolsep{4pt}
s\begin{tabular}{cccccccccc}
\toprule
Class & SVM & SAE & EMAP & CNN & SPC & MCNN & 3D-CNN & SSRN & PDML \\
\midrule 
OA(100\%) & 80.20$\pm$0.58 & 92.99$\pm$0.59 & 91.23$\pm$0.59 & 97.08$\pm$0.47 & 97.90$\pm$0.49 & 98.49$\pm$0.65 & 96.30$\pm$1.25 & 99.61$\pm$0.22 & \textbf{99.71$\pm$0.18}\\
AA(100\%) & 65.64$\pm$0.86 & 89.76$\pm$1.25 & 89.17$\pm$1.64 & 95.09$\pm$0.70 & 96.56$\pm$0.69 & 97.12$\pm$0.62 & 94.68$\pm$1.97 & 99.33$\pm$0.57 & \textbf{99.44$\pm$0.31}\\
$\kappa \times 100$ & 77.97$\pm$0.65 & 92.18$\pm$0.91 & 90.25$\pm$0.54 & 96.74$\pm$0.53 & 97.66$\pm$0.55 & 98.32$\pm$0.74 & 95.90$\pm$1.39 & 99.56$\pm$0.25 & \textbf{99.67$\pm$0.18}\\
\midrule 
1 & 92.16 & 93.04 & 94.09 & 99.00 & 99.11 & 99.44 & 91.71 & 99.70 & \textbf{99.96}\\
2 & 86.16 & 92.04 & 86.15 & 98.48 & 99.19 & 98.82 & 89.73 & 99.88 & 99.81\\
3 & 42.55 & 85.59 & 98.05 & 92.16 & 92.60 & \textbf{100.00} & 92.16 & 99.00 & 99.08\\
4 & 67.69 & 72.12 & 67.33 & 81.84 & 85.49 & 82.95 & 86.94 & 98.26 & \textbf{98.58}\\
5 & 0.00 & 82.20 & 71.32 & 85.38 & 89.63 & 84.07 & 94.79 & 99.03 & 96.67\\ 
6 & 54.71 & 83.15 & 89.13 & 90.96 & 95.94 & \textbf{100.00} & 90.92 & 99.43 & 99.68\\ 
7 & 0.00 & 76.46 & 95.24 & 93.21 & 96.38 & 97.30 & 91.57 & 97.03 & \textbf{99.34}\\ 
8 & 65.12 & 94.10 & 88.70 & 98.21 & 98.09 & \textbf{100.00} & 96.22 & 99.54 & 99.61\\ 
9 & 67.82 & 94.57 & 95.43 & 99.04 & 99.53 & \textbf{100.00} & 99.53 & 99.70 & 99.97\\ 
10 & 93.40 & 98.91 & \textbf{100.00} & 99.85 & 99.96 & \textbf{100.00} & 99.81 & 99.96 & \textbf{100.00}\\ 
11 & \textbf{100.00} & 98.39 & 93.45 & 98.89 & 99.86 & \textbf{100.00} & 97.69 & 99.80 & \textbf{100.00}\\ 
12 & 83.75 & 96.42 & 84.12 & 99.43 & 99.51 & \textbf{100.00} & 97.69 & \textbf{100.00} & 99.97\\ 
13 & \textbf{100.00} & 99.83 & 96.23 & 99.79 & 99.97 & \textbf{100.00} & \textbf{100.00} & \textbf{100.00} & \textbf{100.00}\\ 
\bottomrule
\end{tabular}
\label{tab:KSC_result}
\vspace{-2mm}
\end{table*}


\begin{table*}[tb] \tablesize
\centering
\caption{Classification Results of different methods for the Houston 2013 dataset.}
\vspace{-1mm}
%\vspace{4pt}
\setlength\tabcolsep{8.7pt}
\renewcommand\arraystretch{1.1}
\begin{tabular}{ccccccccccc}
\toprule
Class & CNN & ECNN & GCNN & 3D-CNN & MSDNSA & SSRN & CASSN & RIAN & SSAtt  & PDML\\
\midrule 
OA(100\%) & 61.85 & 84.04 & 84.12 & 78.19 & 87.78 & 89.46 & 85.32 & 86.37 & 90.38 &  \textbf{92.62$\pm$0.47}\\
AA(100\%) & 62.98 & 83.33 & 82.94 & 75.79 & 89.28 & 89.45 & 87.28 & 88.68 & 89.76 & \textbf{93.89$\pm$0.81}\\
$\kappa \times 100$ & 58.64 & 82.54 & 82.51 & 76.27 & 86.73 & 88.58 & 84.07 & 85.14 & 89.55 & \textbf{91.99$\pm$0.56}\\
\midrule 
1 & 56.73 & 87.49 & 87.47 & 78.63 & 82.72 & 81.48 & 83.10 & 83.00 & 82.54 & \textbf{96.30}\\
2 & 64.68 & 80.99 & 86.01 & 93.23 & 99.81 & 92.48 & 85.15 & 83.74 & \textbf{99.92} & 99.62\\
3 & 44.67 & 87.72 & 78.22 & 40.99 & 89.70 & 98.02 & \textbf{100.00} & 89.70 & 86.48 & \textbf{100.00}\\
4 & 59.05 & 90.43 & 85.02 & 97.44 & 95.08 & 98.11 & 93.28 & 92.52 & \textbf{99.57} & 97.54\\
5 & 68.31 & \textbf{100.00} & 99.89 & 87.31 & 94.89 & 99.91 & 99.24 & 99.81 & 99.61 & 99.72\\ 
6 & 70.21 & 97.90 & 89.44 & 79.02 & 95.80 & 95.80 & 95.80 & \textbf{100.00} & 83.71 & 99.30\\ 
7 & 82.57 & 90.48 & 90.19 & \textbf{90.49} & 85.63 & 89.46 & 82.65 & 87.59 & 89.92 & \textbf{90.49}\\ 
8 & 52.12 & 58.51 & 74.44 & 59.83 & 85.57 & 69.90 & 66.48 & 73.22 & 81.94 & \textbf{90.12}\\ 
9 & 70.35 & 79.77 & 84.42 & 81.11 & \textbf{86.02} & 84.04 & 79.60 & 81.21 & 85.99 & 85.36\\ 
10 & 60.37 & 64.28 & 63.61 & 69.59 & 60.33 & 82.34 & 66.80 & 68.15 & \textbf{88.81} & 87.64\\ 
11 & 72.16 & 78.37 & 80.06 & 75.14 & 87.67 & \textbf{93.17} & 90.42 & 89.85 & 90.53 & 81.21\\ 
12 & 44.63 & 78.29 & 87.30 & 82.23 & 90.78 & \textbf{90.80} & 89.15 & 89.15 & 89.48 & 88.76\\ 
13 & 87.02 & 76.84 & 85.06 & 82.11 & 90.88 & 72.98 & 77.54 & 92.28 & 86.81 & \textbf{92.98}\\ 
14 & 96.92 & 99.19 & \textbf{100.00} & 80.57 & 99.60 & 99.19 & \textbf{100.00} & \textbf{100.00} & 95.34 & \textbf{100.00}\\ 
15 & 14.93 & 77.04 & 56.95 & 39.11 & 94.71 & 94.08 & \textbf{100.00} & \textbf{100.00} & 85.77 & 99.37\\ 
\bottomrule
\end{tabular}
\label{tab:Hou_result}
\vspace{-4mm}
\end{table*}



\section{Experiments}
In this section, we conducted various experiments to evaluate the classification performance of the proposed PDML framework on four widely-used benchmark datasets: Indian Pines (IN) dataset, University of Pavia (UP) dataset, Kennedy Space Center (KSC) dataset, and Houston 2013 dataset.

\subsection{Datasets}
For fair comparisons with existing methods, we randomly partitioned the IN, UP, and KSC datasets into the training, validation, and testing subsets following~\cite{article}, while we constructed the training and testing subsets for the Houston 2013 dataset following~\cite{debes2014hyperspectral}. 
We provide the detailed information of each dataset as follows:


\begin{itemize}

\item The \textbf{IN} dataset was collected by the Airborne Visible Infrared Imaging Spectrometer (AVIRIS) in Northwest Indiana in 1996. It contains 16 different land-cover classes of $145\times 145$ pixels. Originally, the Indian Pines dataset has 220 spectral channels with a resolution of 20 m by pixel. Since 20 bands are corrupted by water absorption effects, we discard the above 20 bands and utilize the remaining 200 bands for research, which cover the range from 0.4 to 2.5$\mu m$. Specifically, we randomly choose 20$\%$, 10$\%$, and 70$\%$ of the labeled data for training, validation, and testing respectively. 
 
\item The \textbf{UP} dataset was gathered by the Reflective Optics System Imaging Spectrometer in Italy in 2001, which contains 9 land-cover classes of 610 $\times$ 340 pixels from the city of Pavia. After the noisy bands are removed from the hyperspectral images, the remaining images include 103 spectral bands with a resolution of 1.3 m by pixel and range from 0.43 to 0.86$\mu m$. Different from the IN dataset, we randomly divide the UP dataset into training, validation, and testing subsets while the ratio of the division is 10$\%$, 10$\%$, and 80$\%$, respectively.

\item The \textbf{KSC} dataset was also gathered by AVIRIS from the Kennedy Space Center located in Florida in 1996, which contains 13 types of swamps of 512 $\times$ 614 pixels with a spatial resolution of 18 m by pixel. In addition, we obtain relative data with 176 spectral bands without the water absorption bands. Similar to the IN dataset, we randomly divide the KSC dataset into training, validation, and testing subsets and the ratio of the division is the same as that of the IN dataset.

\item The \textbf{Houston 2013} dataset was collected from the University of Houston campus in 2012~\cite{debes2014hyperspectral}. The dataset contains 15 different land covers with 144 spectral bands and a spatial size of 349 $\times$ 1905. 
We partitioned the dataset into training and testing subsets following~\cite{debes2014hyperspectral}.



\end{itemize}



\subsection{Implementation Details}
We employed the Tensorflow framework to conduct all the experiments. 
We adopted part of the architecture in SSRN~\cite{article} as the trunk CNN model and added 2 randomly initialized convolutional layers as the embedding structure to obtain the corresponding mean and variance embedding.
We standardized all the hyperspectral images of the four datasets to zero mean value and unit variance value.
We fixed the batch size to 16 for the IN, UP, and KSC datasets and 128 for the Houston 2013 dataset.
We used the RMSProp optimizer~\cite{tieleman2012lecture} with learning rates of $10^{-4}$ for all the datasets for fair comparisons.
Besides, we set the input patch size to $5\times5$, the sensitive parameter $\alpha$ to $0.2$, and the number $K$ of our Monte-Carlo sampling to $3$ in our experiments.
The parameter $\lambda_1$, $\lambda_2$, and $\lambda_3$ were set to 1.
We ran $300$ epochs during the training procedure and utilized the model which performed the best in the validation samples to test the classification results for the IN, UP, and KSC datasets. For the Houston 2013 dataset, we provided the best testing performance of the whole training procedure.
We repeated all the experiments ten times and reported the average classification results including the overall accuracy (OA), the average accuracy (AA), the Kappa coefficient, and the per-class accuracy.

\begin{table}[t]\scriptsize
\caption{OA (\%) and $\kappa$ of PDML on various architectures for the UP dataset.}
\centering
\vspace{-1mm}
%\vspace{5pt}
\setlength\tabcolsep{1pt}
\renewcommand\arraystretch{1.1}
\begin{tabular}{c|cccccc}
\hline
Model & HNet & P-HNet & LNet & P-LNet & DFFN & P-DFFN\\
\hline 
OA(100\%) & 93.10$\pm$0.96 & \textbf{94.89$\pm$0.89} & 96.61$\pm$0.91 & \textbf{97.80$\pm$0.67} & 97.99$\pm$0.37 & \textbf{98.81$\pm$0.27}\\
$\kappa \times 100$ & 90.92$\pm$1.26 & \textbf{93.34$\pm$1.05} & 95.63$\pm$0.99 & \textbf{97.13$\pm$0.63} & 97.05$\pm0.42$ & \textbf{98.56$\pm$0.32}\\
\hline
\end{tabular}
\label{tab:implementation}
\vspace{-2mm}
\end{table}

\begin{table}[t] \tablesize
\centering
\caption{OA (\%) of PDML with different loss functions in the embedding space.}
%\vspace{5pt}
\vspace{-1mm}
\setlength\tabcolsep{2pt}
\renewcommand\arraystretch{1.1}
\begin{tabular}{c|cccc}
\hline
Loss & IN & UP & KSC & Houston 2013\\
\hline 
P. Contrastive & \textbf{99.55$\pm$0.09} & \textbf{99.91$\pm$0.03} & \textbf{99.71$\pm$0.18} & \textbf{92.62$\pm$0.47}\\
Contrastive & 99.21$\pm$0.17 & 99.82$\pm$0.08 & 99.58$\pm$0.18 & 91.88$\pm$0.62\\
Triplet-R  & 98.45$\pm$0.41 & 99.19$\pm$0.13 & 99.04$\pm$0.46 & 91.34$\pm$0.65\\
Triplet-SH & 98.52$\pm$0.33 & 99.32$\pm$0.08 & 98.87$\pm$0.42 & 92.29$\pm$0.51\\
Npair & 98.63$\pm$0.71 & 99.11$\pm$0.25 & 98.94$\pm$0.51 & 90.75$\pm$0.73\\
Softmax & 98.55$\pm$0.68 & 99.17$\pm$0.33 & 98.67$\pm$0.49 & 90.32$\pm$0.82\\
\hline
\end{tabular}
\label{tab:loss}
\vspace{-2mm}
\end{table}

\subsection{Results and Analysis}
\textbf{Comparisions with state-of-the-art methods:}
We compared the proposed PDML framework with state-of-the-art hyperspectral image classification methods including
SVM~\cite{waske2010sensitivity}, SAE~\cite{chen2014deep}, EMAP~\cite{dalla2010morphological}, CNN~\cite{makantasis2015deep}, SPC~\cite{article}, MCNN~\cite{li2019convolutional}, 3D-CNN~\cite{chen2016deep}, ECNN~\cite{aptoula2016deep}, GCNN~\cite{chen2017hyperspectral}, MSDNSA~\cite{fang2019hyperspectral}, SSAtt~\cite{hang2020hyperspectral}, CASSN~\cite{9641863},
RIAN~\cite{9785505}, and SSRN~\cite{article}.
Tables~\ref{tab:IN_result}, ~\ref{tab:UP_result}, ~\ref{tab:KSC_result}, and ~\ref{tab:Hou_result} show the quantitative experimental results on the widely used IN, UP, KSC, and Houston 2013 datasets, respectively.
We observe that our PDML obtained the best classification performance in all four datasets. For instance, the mean value of the overall accuracy (OA) of our method for the UP dataset is the highest while the standard deviation value is the lowest among all of the compared approaches. Additionally, for the challenging Houston 2013 dataset, our PDML framework achieves significantly better results than existing methods. This is because our method exploits more information in the input patches and directly models the label uncertainty of the neighboring pixels in a probabilistic form.




\newcommand\figwidth{0.228}
\begin{figure}[t]
\centering
\subcaptionbox{ 1\% training samples\label{subfig:1}}{
\includegraphics[width=\figwidth\textwidth]{figures/1.pdf}
}\hfill
\subcaptionbox{ 5\% training samples\label{subfig:5}}{
\includegraphics[width=\figwidth\textwidth]{figures/5.pdf}
}
\caption{The experimental results with fewer training samples on four datasets using SSRN and PDML.}
\label{fig:sample}
 \vspace{-4mm}
\end{figure}



\newcommand\figwidthh{0.228}
\begin{figure}[t]
\centering
\subcaptionbox{ Training sample proportions.\label{fig:training_proportions}}{
\includegraphics[width=\figwidthh\textwidth]{figures/training_sample.pdf}
}\hfill
\subcaptionbox{ Patch size.\label{fig:patch_size}}{
\includegraphics[width=\figwidthh\textwidth]{figures/patch_size.pdf}
}
\caption{OA (\%) of PDML with different factors in the IN, UP, KSC, and Houston 2013 datasets.}
\label{fig:tsne}
\vspace{-2mm}
\end{figure}




\begin{table}[t]\caption{OA (\%) of PDML with different numbers of Gaussian distributions for IN, UP, KSC, and Houston 2013.}
\centering
\vspace{-1mm}
\setlength\tabcolsep{6pt}
\renewcommand\arraystretch{1.1}
\begin{tabular}{c|cccc}
\hline
Number & IN & UP & KSC & Houston 2013\\
\hline 
1 & \textbf{99.55$\pm$0.09} & \textbf{99.91$\pm$0.03} & 99.71$\pm$0.18 & 92.62$\pm$0.47\\
2 & 99.36$\pm$0.09 & 99.88$\pm$0.05 & 99.68$\pm$0.15 & \textbf{92.91$\pm$0.53}\\
3 & 99.01$\pm$0.13 & 99.65$\pm$0.06 & \textbf{99.74$\pm$0.16} & 91.69$\pm$0.66\\
\hline
\end{tabular}
\label{tab:Gaussian}
\vspace{-2mm}
\end{table}


\begin{table}[t]\caption{OA (\%) of PDML with different $\lambda_1$, $\lambda_2$, and $\lambda_3$ on four datasets.}
\centering
\vspace{-1mm}
\setlength\tabcolsep{4pt}
\renewcommand\arraystretch{1.1}
\begin{tabular}{ccc|cccc}
\hline
$\lambda_1$ & $\lambda_2$ & $\lambda_3$ & IN & UP & KSC & Houston 2013\\
\hline 
1.0 & 1.0 & 1.0 & 99.55$\pm$0.09 & 99.91$\pm$0.03 & 99.71$\pm$0.18 & 92.62$\pm$0.47\\
1.0 & 1.0 & 0.5 & 99.48$\pm$0.09 & 99.89$\pm$0.04 & 99.65$\pm$0.22 & 92.48$\pm$0.55\\
1.0 & 1.0 & 0.0 & 72.55$\pm$1.88 & 78.49$\pm$1.27 & 70.91$\pm$2.43 & 46.73$\pm$2.24\\
1.0 & 0.5 & 1.0 & 99.45$\pm$0.15 & 99.89$\pm$0.05 & 99.63$\pm$0.21 & 91.89$\pm$0.71\\
1.0 & 0.0 & 1.0 & 99.23$\pm$0.16 & 99.81$\pm$0.05 & 99.61$\pm$0.25 & 89.85$\pm$0.65\\
0.5 & 1.0 & 1.0 & 99.51$\pm$0.11 & 99.91$\pm$0.03 & 99.66$\pm$0.16 & 92.59$\pm$0.42\\
0.0 & 1.0 & 1.0 & 99.49$\pm$0.09 & 99.89$\pm$0.04 & 99.65$\pm$0.18 & 92.23$\pm$0.46\\
\hline
\end{tabular}
\label{tab:parameter}
\vspace{-2mm}
\end{table}


\begin{table}[t]\caption{The computation time (seconds) of one epoch for PDML compared with the SSRN baseline for the IN, UP, KSC and Houston 2013 datasets. 
}
\centering
\vspace{-1mm}
\setlength\tabcolsep{7pt}
\renewcommand\arraystretch{1.1}
\begin{tabular}{c|cccc}
\hline
Method & IN & UP & KSC & Houston 2013\\
\hline 
SSRN (Training) & 262.69 & 1006.74 & 136.41 & 151.68\\
PDML (Training) & 986.61 & 3621.60 & 532.67 & 693.71\\
\hline
SSRN (Testing) & 32.18 & 156.29 & 17.03 & 59.58\\
PDML (Testing) & 31.95 & 161.21 & 17.42 & 57.49\\
\hline
\end{tabular}
\label{tab:time}
\vspace{-2mm}
\end{table}




\textbf{Application of PDML to existing methods:}
We applied our PDML framework to other deep models to test the effectiveness of the method. Specifically, the models include HamidaNet~\cite{hamida20183} (HNet), LiNet~\cite{li2017spectral} (LNet), and DFFN~\cite{song2018hyperspectral}. 
As for the structure of these models, HNet and LNet are both based on the 3-D CNN blocks while the number of layers and the size of the CNN layer vary with each other. Differently, DFFN is based on 2-D CNN block.
We constructed the corresponding architectures of the models to test the original classification performance and then replaced the last fully connected layer with two convolutional layers to generate the embedding mean matrix and the variance matrix as we have described before, followed by the same probabilistic procedure.
Table~\ref{tab:implementation} shows the original classification results of the three models that we have obtained by our implementation together with the results after we added the PDML framework to the original architectures. We observe a constant performance boost to the models on both the overall accuracy and the kappa coefficient. For example, our PDML framework delivered a $1.79\%$ increase in the overall accuracy and a $2.42\%$ increase in the kappa coefficient for LNet, which dramatically proves that our proposed framework can be properly implemented on various deep models and comprehensively enhances the classification results of the original architectures.





\begin{figure*}[t]
\centering
\includegraphics[width=0.9\textwidth]{figures/IN_classification_map.pdf}
\vspace{-2mm}
\caption{Classification maps for the Indian Pines dataset. (a) False-color image. (b) Ground-truth map. (c)-(k) Classification maps of SVM, SAE, EMAP, CNN, SPC, MCNN, 3D-CNN, SSRN, and PDML.
(Best viewed in color.)
}
\label{fig:IN_classification_map}
\vspace{-5mm}
\end{figure*}

\begin{figure*}[t]
\centering
\includegraphics[width=0.9\textwidth]{figures/UP_classification_map.pdf}
\vspace{-3mm}
\caption{Classification maps for the University of Pavia dataset. (a) False-color image. (b) Ground-truth map. (c)-(k) Classification maps of SVM, SAE, EMAP, CNN, SPC, MCNN, 3D-CNN, SSRN, and PDML.
(Best viewed in color.)
}
\label{fig:UP_classification_map}
\vspace{-5mm}
\end{figure*}

\begin{figure*}[t]
\centering
\includegraphics[width=0.9\textwidth]{figures/KSC_classification_map.pdf}
\caption{Classification maps for the Kennedy Space Center dataset. (a) False-color image. (b) Ground-truth map. (c)-(k) Classification maps of SVM, SAE, EMAP, CNN, SPC, MCNN, 3D-CNN, SSRN, and PDML.
(Best viewed in color.)
}
\label{fig:KSC_classification_map}
\vspace{-5mm}
\end{figure*}

\begin{figure*}[t]
\centering
\includegraphics[width=0.9\textwidth]{figures/Hou_classification_map.pdf}
\caption{Classification maps for the Houston 2013 dataset. (a) False-color image. (b) Ground-truth map. (c)-(j) Classification maps of CNN, ECNN, GCNN, 3D-CNN, MSDNSA, SSRN, SSAtt, and PDML.
(Best viewed in color.)
}
\label{fig:Hou_classification_map}
\vspace{-5mm}
\end{figure*}



\begin{figure}[t]
\centering
\subcaptionbox{Before training.\label{subfig:before}}{
\includegraphics[width=0.21\textwidth]{figures/tsne_before.pdf}
}\hfill
\subcaptionbox{After training.\label{subfig:after}}{
\includegraphics[width=0.21\textwidth]{figures/tsne_after.pdf}
}
\caption{The t-SNE plots of the embeddings for the Houston 2013 dataset before and after training. (Best viewed in color.)}
\label{fig:tsne}
\vspace{-2mm}
\end{figure}


\textbf{Effect of the probabilistic contrastive loss:}
We conducted a series of ablation experiments on four datasets to analyze the effect of the probabilistic contrastive loss in the proposed PDML framework.
To be specific, we respectively substitute the probabilistic contrastive loss with the conventional contrastive loss, the triplet loss with random sampling, the triplet loss with semi-hard sampling, the N-pair loss, and the softmax loss while keeping other structures unchanged during training. 

The results are shown in Table~\ref{tab:loss}. We can see that the probabilistic contrastive loss outperforms other loss structures in all four datasets. That is because the probabilistic contrastive loss introduces the match probability of two samples which offers a better judgment of samples as well as involves more learnable parameters which are more reasonable and flexible for deep models.
By comparison, the contrastive loss and the triplet-based losses behave better than the N-pair loss and the softmax loss.
This is because the contrastive loss and the triplet loss are more effective for the clustering in the embedding space, which might be more suitable in our settings.

\textbf{Effect of the training sample proportions:}
We compared the performances of SSRN and PDML with fewer training samples on four datasets, as shown in Figure~\ref{fig:sample}. We observe that PDML achieves consistent performance boost compared with SSRN with 1\% and 5\% training samples.
In addition, we respectively used 1\%, 5\%, 10\%, and 20\% of labeled pixels in the datasets to train our PDML framework. The experimental results are shown in Figure~\ref{fig:training_proportions}. We see that the overall accuracy increases when taking more training samples for all the datasets. Additionally, the classification results are competitive as the proportions reach 10\%, and using more training samples does not significantly boost the performance.

\textbf{Effect of the patch sizes:}
The patch size influences the spatial information involved for each pixel. Therefore, we considered using the patch size of $3\times3$, $5\times5$, $7\times7$, and $9\times9$ to conduct the experiment. As illustrated in Figure~\ref{fig:patch_size}, the IN, UP, and KSC datasets are less sensitive to the patch size and we observe the best performance when the patch size is set to $5\times5$. As for the Houston 2013 dataset, the overall accuracy markedly increases as the patch size increases from $3\times3$ to $5\times5$. Nevertheless, the classification results indicate little enhancement for larger patch size but involve extra computational cost on the contrary. Under such circumstances, we fix the patch size to $5\times5$ for subsequent experiments.

\textbf{Effect of the forms of Gaussian distributions:}
In the proposed PDML framework, we model each pixel with a single Gaussian distribution to consider the data uncertainty. We further verified the performances of PDML when applying multiple Gaussian distributions. We present the classification results in Table~\ref{tab:Gaussian}. We see that modeling each pixel with multiple Gaussian distributions is not beneficial to the performance. Furthermore, the overall accuracy even decreases when using 3 Gaussian distributions for the Houston 2013 dataset, which demonstrates that a single Gaussian distribution is capable of modeling each pixel, and utilizing multiple Gaussian distributions might involve negative influences.

\textbf{Effect of $\lambda_1$, $\lambda_2$, and $\lambda_3$:}
The parameter $\lambda_1$, $\lambda_2$, and $\lambda_3$ were set to 1 during training, which demonstrates an equal weight for three loss functions. We respectively fixed two parameters and modified the value of the other parameter to analyze the contribution of each parameter. Table~\ref{tab:parameter} presents the experimental results. Firstly, the overall accuracy significantly decreases when $\lambda_3$ is set to 0 because the final classification layer can not be properly trained but provides random outputs. Additionally, we see a worse performance when we reduce the weight of $\lambda_2$, which certifies that our probabilistic deep metric learning term indeed boosts the overall classification results. Particularly, when $\lambda_2=0$, our framework is similar to the conventional hyperspectral image classification framework, which demonstrates the universality of our method. Moreover, we observe that $\lambda_1$ influences the overall accuracy to some extent, demonstrating the effectiveness of enforcing a large variance for outer pixels in a patch.




\textbf{Computation time:}
We provide the computation time of one epoch for PDML and SSRN~\cite{article} in Table~\ref{tab:time}. 
Note that we use a larger batch size for the Houston 2013 dataset, resulting in higher efficiency. 
We see that PDML requires more computation time during training on four datasets compared with SSRN due to the Monte-Carlo sampling.
However, our method introduces no additional computation workload during inference.
Therefore, it is fair to directly compare our PDML with the baseline methods.
We think the increase in the training time is acceptable given the superior performance and similar inference time of our framework.

\textbf{Visualization of classification results:}
We qualitatively visualize the classification maps of the best trained models with their corresponding false-color images and ground truth maps for four datasets in Figure~\ref{fig:IN_classification_map}, ~\ref{fig:UP_classification_map}, ~\ref{fig:KSC_classification_map}, ~\ref{fig:Hou_classification_map}, respectively. Specifically, we can see that the classification maps of the previous methods except SSRN have more or less noisy areas whether they belong to the conventional methods or the deep learning-based models. On the contrary, the classification maps of SSRN are smoother and the corresponding classification results of each class are more accurate apparently. However, our PDML structure generates a better quality of classification maps which not only get rid of irregular noise information but also precisely present the same color as the ground truth map for the testing samples, which further demonstrates the superior performance of our proposed framework.

\textbf{Visualization of embedding space:}
Furthermore, we visualize the embeddings before and after training with the t-SNE plots in Figure~\ref{fig:tsne} with different colors standing for different classes of embeddings. We observe that the embeddings are presented desultorily before the training procedure while the embeddings of the same class tend to gather together after training, which verifies that our PDML framework effectively reduces the intraclass distances and enlarges the interclass distances in the embedding space.

