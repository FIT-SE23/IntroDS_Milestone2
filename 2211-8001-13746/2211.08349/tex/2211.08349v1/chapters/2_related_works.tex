\section{Related Works}
In this section, we review recent advancements in three related areas: deep metric learning, probabilistic embedding learning, and hyperspectral image classification.
\subsection{Deep Metric Learning}
Metric learning aims to construct a discriminative distance function to effectively represent the similarities between samples. 
Deep metric learning employs deep networks to transform images to an embedding space with the objective to decrease intraclass distances and increase interclass distances~\cite{wang2022introspective,zheng2022dynamic,zheng2021deep1,zheng2021deep2,zhang2022attributable,zheng2020structural}. 
Numerous deep metric learning methods focus on designing discriminative losses to instantiate this objective~\cite{cakir2019deep,sohn2016improved,song2016deep,wang2019multi,yu2019deep}. 
For example, the contrastive loss~\cite{hadsell2006dimensionality} pulls samples from the same class as close as possible and pushes away samples of different categories by at least a fixed margin. 
The triplet loss is more flexible which only enforces a distance ranking within triplets~\cite{schroff2015facenet}. 
Furthermore, Wang~\emph{et al.}~\cite{wang2019multi} proposed a multi-similarity loss to consider three types of similarities between samples.


In addition to the loss functions, how to effectively construct training tuples significantly affects the performance of deep metric learning~\cite{wu2017sampling,duan2018deep,zheng2019hardness,harwood2017smart,xuan2020improved,zheng2020deep}. 
For example, Harwood~\emph{et al.}~\cite{harwood2017smart} improved the widely used hard negative mining strategy~\cite{schroff2015facenet} and proposed a smart mining strategy to choose challenging negative samples adaptively considering the current training stage.
 In addition, Wu~\emph{et al.}~\cite{wu2017sampling} uniformly selected samples based on the distance distributions and Xuan~\emph{et al.}~\cite{xuan2020improved} claimed that sampling easy positive samples is helpful to the generalization performance.


\subsection{Probabilistic Embedding Learning}
Probabilistic embedding learning was initially proposed and developed for word embedding since it can naturally model the inherent hierarchies and ambiguity in language.
It is effectively applied to various natural language processing tasks~\cite{vilnis2014word,li2018smoothing,li2020adaptive,nguyen2017mixture}. 
For example, Vilnis~\emph{et al.}~\cite{vilnis2014word} introduced the Gaussian embedding for word representation. 
Li~\emph{et al.}~\cite{li2020adaptive} proposed a probabilistic embedding learning approach that adaptively adjusts and updates word embeddings. 
Probabilistic embedding learning is also proven to be beneficial to computer vision tasks, such as face recognition~\cite{shi2019probabilistic,chang2020data}, pose estimation~\cite{sun2020view}, cross-modal retrieval~\cite{chun2021probabilistic}, and regression problems~\cite{li2021learning}. 
Recently, probabilistic embedding has been introduced to metric learning by Oh~\emph{et al.}~\cite{oh2018modeling} who proposed a hedged instance embedding learning method to address the semantic ambiguity of natural images.

Our work further extends probabilistic embedding learning to hyperspectral image classification.
We model the spectral and label uncertainty in image patches using Gaussian distributions and propose a probabilistic metric learning framework to constrain the distributions. 
Different from existing works, we impose an uncertainty prior to the variance of the distributions so that the variance of outer pixels in a patch is larger than that of inner pixels. 
To the best of our knowledge, PDML is the first work that employs probabilistic embedding to model the spectral and label uncertainty in hyperspectral remote sensing.

\subsection{Hyperspectral Image Classification}
Hyperspectral Image Classification (HSIC) aims to assign the correct semantic label to each pixel in the hyperspectral images. 
Benifiting from the great power of deep neural networks, deep-learning-based methods~\cite{9785505,9782104,9585383,9693311,8463629,zhan2017semisupervised,zhu2020residual,sun2019spectral,yang2018hyperspectral,yu2021feedback,he2018feature} significantly outperform conventional methods~\cite{blanzieri2008nearest,sun2014active,mianji2011robust,li2011spectral,li2013generalized} and have dominated the HSIC fields. 
For example, pioneer methods employed 1-D CNN~\cite{hu2015deep}, recurrent neural networks (RNN)~\cite{mou2017deep}, and generative adversarial networks~\cite{zhan2017semisupervised} to capture abundant information from spectral bands. 
Subsequent methods began to exploit the spatial information to address the spectral variability of hyperspectral images. 
For example, Yang~\emph{et al.}~\cite{yang2018hyperspectral} proposed a 3D structure to simultaneously consider the spectral-spatial information using image patches as inputs.
He~\emph{et al.}~\cite{he2018feature} proposed to use the multiscale covariance maps for better extraction of spatial information. 
Recently, a number of methods employed the attention-based networks for the HSIC task and achieved excellent performance~\cite{9693311,yu2021feedback,sun2019spectral}.

In addition, metric learning has been applied to hyperspectral classification~\cite{deng2019deep,cao2019hyperspectral,dong2017dimensionality} for better discriminative representations of pixels.
Compared with conventional deep-learning-based methods, they further constrained the relations between patches and used a metric learning loss as an auxiliary term to improve the performance.
Differently, our PDML framework models pixels as probabilistic distributions and then computes the distance between distributions to address the spectral uncertainty and label uncertainty.