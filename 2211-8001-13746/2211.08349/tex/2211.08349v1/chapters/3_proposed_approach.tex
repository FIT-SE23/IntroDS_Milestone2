\section{Proposed Approach}
In this section, we first formulate the problem of hyperspectral image classification and provide a unified view of existing spectral-spatial-based hyperspectral image classification methods. Then, we present the uncertainty modeling method of spectral distribution. After that, we introduce how to measure the distances between distributions and how to conduct the discriminative loss to instruct the training procedure. Lastly, we elaborate on the proposed probabilistic deep metric learning framework for HSIC.

\subsection{Revisit Spectralâ€“Spatial-Based HSIC }
Consider a hyperspectral image set $\mathbf{X}$ composed of $N$ training samples $\{\mathbf{x}_1,\mathbf{x}_2,\cdots,\mathbf{x}_N\}$, in which each sample $\mathbf{x}_i$ contains $M$ pixels $\{\mathbf{p}_i^1,\mathbf{p}_i^2,\cdots,\mathbf{p}_i^M\}$ with their corresponding high dimensional spectral vectors $\mathbf{S}_i = \{s_i^1,s_i^2,\cdots,s_i^M\}$ and ground truth labels $\mathbf{L}_i = \{l_i^1, l_i^2, \cdots , l_i^M \}$, where $l_i^m \in \{1,2,\cdots,K\}$ indicates that $\mathbf{p}_i^m$ belongs to the $l_i^m$th class. Let $c_i^m$ be the real appearance of the objects in the pixel $\mathbf{p}_i^m$. General HSIC tasks aim at learning an accurate mapping from the real appearance of the objects in the pixel $\mathbf{p}_i^m$ to the predicted label of the pixel $y_i^m$, which can be regarded as predicting a probability $p(y_i^m|c_i^m)$. As for spectral-feature networks for HSIC, they only consider the spectral information of a given pixel and predict a relative probability $p(y_i^m|s_i^m)$. In most previous studies, the probability was computed by a softmax function, which is formulated as follows:
\begin{eqnarray}\label{equ:softmax}
p(y_i^m=k|s_i^m) = \frac{\exp({V_k})}{\sum_{j=1}^{K}\exp({V_j})},
\end{eqnarray}
where $V_k$ denotes the $k$th number in the output vector of the deep network. Obviously, the softmax function normalizes the numbers in the output vector and maps them to a range from $0$ to $1$. Besides, the softmax function is always followed by a cross entropy loss which optimizes the whole network for better classification performance:
\begin{eqnarray}\label{equ:cross entropy}
L = -\sum_{k=1}^{K}{\mathbb{I}(l_i^m=k)log(p(y_i^m=k|s_i^m))},
\end{eqnarray}
where $\mathbb{I}(\cdot)$ denotes the indicator function which takes $1$ as the output when the expression evaluates to true while outputs $0$ otherwise.

However, the aforementioned spectral variability demonstrates that even the same captured object may present different spectral features, which means the probability $p(s_i^m|c_i^m)$
exists. Under such circumstances, the procedure of the spectral-based deep learning methods could be formulated as follows:
\begin{eqnarray}\label{equ:spectral}
p(y_i^m|s_i^m) = H(p(y_i^m|c_i^m),p(s_i^m|c_i^m)),
\end{eqnarray}
where $H(\cdot)$ represents a non-linear mapping. Therefore, only using the spectral information of a hyperspectral image to perform hyperspectral classification tasks might be influenced by the spectral variability and result in unfaithful performance. 

Given the above consideration, spatial information has been involved in HSIC in order to weaken the spectral uncertainty and improve the classification performance. To be specific, spatial-spectral-based methods tend to utilize the neighbor information of a pixel and take a patch as input. That is to say, a $5\times5$ patch includes the central pixel and the other $24$ pixels around it. In addition, the label of the patch is absolutely the same as that of the central pixel and the subsequent processing is also conducted by the softmax function and the cross entropy loss. For a better understanding of spatial-spectral-based methods, we provide a simple probabilistic explanation of why these methods truly work. 

Low as the resolution of hyperspectral images is, the labels of the neighbor pixels are usually the same as the center pixel, which signifies that the pixels in a patch tend to encode the same material. Thus, we suppose that obtaining the wrong prediction of a certain category of pixels is of the same probability $p_0$. On such assumption, the accuracy of classifying a single hyperspectral pixel using the spectral-based methods is $1-p_0$. However, as for the spatial-spectral-based approaches, the corresponding probability becomes closer to $1-p_0^T$, where $T$ denotes the number of pixels in a patch ($T=25$ when the patch size is $5\times5$, for instance). In view of $p_0<1$ in most cases, we can easily conclude that $1-p_0^T > 1-p_0$, which indicates that the classification accuracy of spatial-spectral-based methods is bound to surpass the accuracy of spectral-based approaches. Moreover, multiscale hyperspectral methods simultaneously consider multiple patches of diverse sizes, which utilizes more spatial information and further enhances the classification performance.

Nevertheless, the existence of low spatial resolution of hyperspectral images results in the problem that different pixels and even the same pixel in a patch are likely to encode diverse materials (a pixel contains both water and land, for example). Therefore, the aforementioned probability $p_0$ should not be the same for each pixel in a patch all the time and simply attaching the label of the central pixel to the neighbor pixels would sometimes ignore abundant spatial information and cause serious problems. For instance, provided a pixel is at the edge of a ground object, the patch center on this pixel will beyond all doubt contain a certain number of pixels that are definitely of different categories compared with the central one. On such occasions, forcing the deep model to classify the above patch according to the label of the central pixel sounds unreasonable. In other words, the labels of the patches are to some extent of uncertainty and previous spatial-spectral-based hyperspectral classification approaches have not taken the label uncertainty into consideration.

\begin{figure*}[t]
\centering
\includegraphics[width=0.96\textwidth]{figures/overall_framework.pdf}
\caption{An illustration of the proposed PDML framework. 
We first employ a backbone CNN to extract meaningful features from the input hyperspectral patch.
We then obtain the mean and variance embeddings of each pixel in the patch and model each pixel as a high-dimensional Gaussian distribution.
We only demonstrate one pixel in the figure for simplicity. 
We employ Monte-Carlo sampling to generate a few samples from the Gaussian distribution and use them to compute the match probability of a pair of pixels.
We then impose the probabilistic contrastive loss to enlarge the interclass match probabilities and decrease the intraclass match probabilities.
We further use a variance loss to constrain the variance of an outer pixel to be larger than that of an inner pixel. 
We further employ a softmax layer and the cross-entropy loss to train a classifier to predict the category of the center pixel.
The overall framework can be trained simultaneously in an end-to-end manner.
(Best viewed in color.)
}
\label{fig:framework}
\vspace{-5mm}
\end{figure*}

\subsection{Uncertainty Modeling of Spectral Distribution}
In order to effectively extract more information from the hyperspectral patches and achieve better classification performance, we attempt to directly model the label uncertainty by introducing the spectral distribution of each pixel in a patch. The modeling operation mainly consists of two separate parts: the basic convolutional neural network structure and the uncertainty modeling part.

Specifically, the convolutional neural network aims at extracting abundant features from the input hyperspectral patches, which takes the advantage of the local perception of each input figure and is widely used in computer vision tasks from diverse research fields. In our research, let $\mathbf{D}$ be the patch set and $\{\mathbf{d}_1,\mathbf{d}_2,\cdots,\mathbf{d}_N\}$ be the corresponding patch samples with the size of $s\times s\times d$, where $s\times s$ denotes the patch size while $d$ denotes the spectral dimension of the original images. As mentioned above, the ground truth label of each patch is the same as the label of the central pixel in the patch. Therefore, the processing of the CNN architecture can be formulated as $\mathbf{f}_i = \mathbf{g}(\mathbf{d}_i)$, where $\mathbf{f}_i$ denotes the output feature of the convolutional network of the $i$th input. 
We further control the patch size of the output features to be consistent with the input patches, which is of benefit to the subsequent study. 
On the contrary, the size of the spectral dimension decreases through each layer of the CNN to avoid the redundancy of the original high-dimensional spectral information.

With the corresponding extracted features of the same patch size as the input patches, we utilize the Gaussian distribution to model the categorical uncertainty of the spectral distribution. Concretely, we consider each pixel in a patch and model each pixel as a high-dimensional Gaussian distribution but not a point embedding in previous research. To obtain the corresponding mean and variance embeddings of each pixel, we employ two convolutional layers which map the extracted features into a high-dimensional embedding space in which the mean matrix and the variance matrix are of the same size $s\times s\times r$, where $r$ denotes the dimension of the Gaussian distribution of each pixel. The above mapping can be formulated as follows ($T=s\times s$ denoting the number of pixels in a certain patch):
\begin{eqnarray}
&&\mathbf{M}_i = \mathbf{g}_1(\mathbf{f}_i) = \{m_i^1,m_i^2,...,m_i^T\}, \label{equ:Mean} \\
%\end{eqnarray}
%\begin{eqnarray}
&&\mathbf{V}_i = \mathbf{g}_2(\mathbf{f}_i) = \{v_i^1,v_i^2,...,v_i^T\}, \label{equ:variance}
\end{eqnarray}
where the mean matrix $\mathbf{M}_i$ and the variance matrix $\mathbf{V}_i$ can be regarded as the combination of $T$ vectors with the size of $r$. Besides, the $k$th vector in $\mathbf{M}_i$ ($m_i^k$) and the corresponding $k$th vector in $\mathbf{V}_i$ ($v_i^k$) respectively represent the mean and variance parameters of the Gaussian distribution of the $k$th pixel in the $i$th patch.
After the above treatment, each pixel in the patch has been mapped to a corresponding stochastic embedding. In addition, we assign all of the embeddings in a patch with the label of the center pixel, which makes it possible for the subsequent training procedure.

\subsection{Deep Probabilistic Metric Learning}
Having obtained the Gaussian distribution as well as the label information of each pixel embedding, we attempt to optimize the parameters of these distributions and effectively utilize the distributions to enhance the overall classification performance of our architecture.

Firstly, in view of the fact that outer pixels in a patch tend to encode more different materials with the central pixel compared with inner pixels, indicating that outer pixels possess more label uncertainty, we enforce a larger Gaussian variance for outer pixels, which can be achieved by controlling the average variance of outer pixels to be larger than that of inner pixels by a predefined margin. Specifically, a $s\times s$ patch contains $\frac{s+1}{2}$ lap pixels and the $j$th lap contains $n_j$ pixels, which can be formulated as follows:
\begin{eqnarray}
n_j=
\begin{cases}
1 &  j=1, \\
(2j-1)^2-(2j-3)^2 & j\geq 2.
\end{cases}
\end{eqnarray}
Therefore, the constraint on the variance of the $j$th lap is:
\begin{eqnarray}\label{equ:variance control}
\mathbf{J}_{var(j)}^{(i)} = -(\frac{1}{n_j}\sum_{k}v_i^k - (1+\alpha)\frac{1}{n_{j-1}}\sum_{q}v_i^q),
\end{eqnarray}
where $j=\{2,3,...,\frac{s+1}{2}\}$, $\alpha$ is a predefined sensitive parameter to control the variance differences between neighboring laps of pixels, $\sum_{k}v_i^k$ denotes adding the variance of the pixel embeddings from the $j$th lap and $\sum_{q}v_i^q$ denotes the same procedure from the $(j-1)$th lap.

Furthermore, we directly obtain multiple samples from the aforementioned Gaussian distributions via the Monte-Carlo sampling method and we utilize a probabilistic metric learning loss function to model the corresponding distance between them. For example, suppose two pixels are mapped to the embedding space and the Gaussian distributions of them are $p(\mathbf{z}_1|\mathbf{p}_1) \sim N(m_1,v_1)$ and $p(\mathbf{z}_2|\mathbf{p}_2) \sim N(m_2,v_2)$ respectively. Therefore, the Monte-Carlo sampling strategy can be recognized as follows:
\begin{eqnarray}
z_1^{(k_1)} = m_1 + v_1\cdot \epsilon^{(k_1)}, \label{equ:Monte-Carlo} \\
%\end{eqnarray}
%\begin{eqnarray}
z_2^{(k_2)} = m_2 + v_2\cdot \epsilon^{(k_2)}, \label{equ:Monte-Carlo} 
\end{eqnarray}
where $k_1=\{1,2,...,K\}$ and $k_2=\{1,2,...,K\}$ indicate extracting $K$ individual samples from each Gaussian distribution, $\epsilon^{(k_1)} ,\epsilon^{(k_2)} \sim N(0,\mathbf{I})$. With the above samples and the corresponding ground truth label information, we can utilize various deep metric learning losses to conduct the optimization, such as contrastive loss and triplet loss. Differently, in our research, we employ a probabilistic contrastive loss to enlarge the interclass distances as well as reduce the intraclass distances of the samples. 
Specifically, we introduce the conventional contrastive loss function and the probabilistic contrastive loss formation, and we also detail how the probabilistic contrastive loss can be used in our Gaussian samples.

The original contrastive loss is trained on a pair of training data $(z_i,z_j)$ together with a sign parameter $y_{ij}$ which is $1$ when the two samples are from the same class and is $0$ otherwise. The contrastive loss directly minimizes the distances between the positive pairs and punishes the distances between the negative pairs for being larger than a predefined margin $\beta$, which is formulated as follows:
\begin{eqnarray}\label{equ:contrastive loss}
J_{con} = \frac{1}{B}\sum_{(i,j)}^{B/2}y_{ij}D_{ij}^2+(1-y_{ij})[\beta-D_{ij}]_+^2,
\end{eqnarray}
where $B$ denotes the batch size of the training samples,  $[\cdot]_+=max(\cdot,0)$, and $D_{ij}=D(z_i,z_j)=||z_i-z_j||_2$. 

However, the conventional contrastive loss is absolutely a certain formation of metric learning losses because the relations between two samples merely include match and non-match. Thus, it is necessary to provide a probabilistic contrastive loss formation that can represent the probability that two samples are matching and is more appropriate for our probabilistic embedding strategy. The match probability of two samples is defined as follows:
\begin{eqnarray}\label{equ:match probability}
p(z_i,z_j)=\sigma(-aD_{ij}+b),
\end{eqnarray}
where $\sigma(\cdot)=\frac{1}{1+e^{-(\cdot)}}$ is the sigmoid function varying from $0$ to $1$, while $a>0$ and $b\in R$ are two learnable parameters standing for the soft threshold of the pairwise distances. Under such circumstances, the match probability ranges from $0$ to $1$ and increases when the distance $D_{ij}$ decreases, which is consistent with our objective.

With the match probability of two samples $p(z_i,z_j)$, the probabilistic contrastive loss is defined as:
\begin{eqnarray}\label{equ:probabilistic contrastive loss}
\mathbf{J}_{pcon}(z_i,z_j)=
\begin{cases}
-\log p(z_i,z_j) & \mathbf{if\quad match} ,\\
-\log (1-p(z_i,z_j)) & \mathbf{otherwise},
\end{cases}
\end{eqnarray}
where the probabilistic contrastive loss enlarges the match probability when the two samples are from the same class while reducing the probability for negative pairs.

As for the samples extracted from the Monte-Carlo sampling, we directly add the pairwise match probabilities of the various samples from two Gaussian distributions and consider the result as the match probability of the two distributions, which can be formulated as follows:
\begin{eqnarray}
p(\mathbf{p}_1,\mathbf{p}_2) \approx \frac{1}{K^2} \sum_{k_1=1}^K \sum_{k_2=1}^K p(z_1^{(k_1)},z_2^{(k_2)}),
\end{eqnarray}
where we use the pixel pair $(\mathbf{p}_1,\mathbf{p}_2)$ to represent the Gaussian distributions of pixels in the embedding space for simplicity.

Therefore, we apply the probabilistic contrastive loss to the match probability of distributions:
\begin{eqnarray}\
\mathbf{J}_{pcon}(\mathbf{p}_1,\mathbf{p}_2)=
\begin{cases}
-\log p(\mathbf{p}_1,\mathbf{p}_2) & \mathbf{if\quad match} ,\\
-\log (1-p(\mathbf{p}_1,\mathbf{p}_2)) & \mathbf{otherwise}.
\end{cases}
\end{eqnarray}

Finally, we combine the constraint on the variance of the Gaussian distributions and the probabilistic contrastive loss for a batch of input patches, which can be formulated as follows:
\begin{eqnarray}
\mathbf{J}_{dist} = \lambda_1\frac{1}{B}(\sum_{i=1}^B \sum_{j=2}^{(s+1)/2}\mathbf{J}_{var(j)}^{(i)}) \nonumber\\
+ \lambda_2\frac{1}{B\times T}(\sum_{i,j}\mathbf{J}_{pcon}(\mathbf{p}_i,\mathbf{p}_j)),
\end{eqnarray}
where $\lambda_1$ and $\lambda_2$ are pre-defined parameters to balance the contributions of different losses while $T=s\times s$ denotes the number of pixels in a patch, $\sum_{i,j}\mathbf{J}_{pcon}(\mathbf{p}_i,\mathbf{p}_j)$ indicates adding all of the probabilistic contrastive loss terms of the pairwise distributions from the embedding space in the batch.

Our probabilistic deep metric learning framework not only models the label uncertainty of the input hyperspectral patches but also takes advantage of more samples in the embedding space for training, which can exploit full information from each input patch.



\renewcommand{\algorithmicrequire}{\textbf{Input:}}
\renewcommand{\algorithmicensure}{\textbf{Output:}}
\setlength{\textfloatsep}{7pt}
\begin{algorithm}[tb]
\caption{\!\!\! \textbf{:} PDML} \label{alg: PDML}
\begin{algorithmic}[1]
\Require
Training image set, labels, learning rates, iteration number $T$, network weights $\theta$, variance controlling parameter $\alpha$, and sampling number of the Monte-Carlos sampling $K$.
\Ensure
Parameters $\theta$, and classification results.
\For{$iter=1,2,\cdots,T$}
\State Utilize the CNN modules to extract features $\textbf{f}$ from the input images.
\State Obtain the mean matrix and the variance matrix of each pixel through two convolution layers respectively.
\State Generate K samples from the Gaussian distribution using the Monte-Carlo sampling strategy.
\State Employ a probabilistic metric learning loss to restrain the distances of the samples.
\State Control the Gaussian variance of outer pixels to be larger than that of inner pixels and compute the corresponding loss.
\State Map the mean matrix to get the classification results with a softmax structure and apply the cross entropy loss to compute the classification errors.
\State Update the network parameters $\theta$ with the above losses.
\EndFor\\
\Return
$\theta$.
\end{algorithmic}
\end{algorithm}


\subsection{Hyperspectral Image Classification with PDML}
To conduct hyperspectral image classification with our probabilistic metric learning framework, we involve the aforementioned probabilistic constraint in the conventional hyperspectral classification methods. Specifically, we employ a convolutional layer on the mean matrix $\mathbf{M}_i$ followed by a softmax structure to obtain the classification results, which can be formulated as follows:
\begin{eqnarray}
p(y_i=k)=\frac{\exp(f_l(\mathbf{M}_i)_k)}{\sum_{j=1}^K\exp(f_l(\mathbf{M}_i)_j)},
\end{eqnarray}
where $y_i$ denotes the predicted label of the $i$th patch, $f_l(\cdot)$ denotes the non-linear mapping, and $f_l(\mathbf{M}_i)_k$ denotes the $k$th term of $f_l(\mathbf{M}_i)$. Therefore, the cross entropy loss term is:
\begin{eqnarray}
\mathbf{J}_{ce}=-\sum_{i}\sum_{k=1}^{K}{\mathbb{I}(l_i=k)log(p(y_i=k))},
\end{eqnarray}
where $l_i$ denotes the ground truth label of the $i$th hyperspectral patch and $\mathbf{J}_{ce}$ indicates adding all the cross entropy terms of the patches in an input batch. In summary, the objective function of the whole PDML framework to conduct HSIC is:
\begin{eqnarray}
\mathbf{J}_{PDML}=\mathbf{J}_{dist} + \lambda_3 \mathbf{J}_{ce},
\end{eqnarray}
where $\lambda_3$ is a balance factor. 

During training, the parameters of the layers which map the mean matrix to the classification results will only be updated by the cross entropy term while the convolutional layer mapping the features to the variance matrix will merely be optimized by the Gaussian probabilistic loss. Nevertheless, other parts of our architecture will be optimized by both loss function terms. The overall framework of our proposed PDML is illustrated in Figure~\ref{fig:framework} and the full training procedure of our method can be found in Algorithm~\ref{alg: PDML}.

During testing, we only pass the hyperspectral patches through the mean matrix network and then use the softmax function to obtain the probability that the input patches belong to each category, which introduces no additional workload compared with conventional methods.

It is worth mentioning that we can apply our framework to various existing hyperspectral image classification approaches. First of all, the feature extracted architecture can be replaced by numerous structures, such as diverse 3D-CNN, 2D-CNN, and the recurrent neural network, which might respectively acquire different features of the input hyperspectral image. In addition, with the generated samples in the embedding space, the probabilistic contrastive loss function can be substituted with popular metric learning functions like margin loss with distance weighted sampling strategy, which could further effectively select valuable samples to optimize the network for better performance. Besides, the widely used attention modules in computer vision tasks could also be added to our architecture, which focuses on more discriminative positions and channels of the hyperspectral images and is beneficial to the classification task.
