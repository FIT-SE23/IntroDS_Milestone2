% CVPR 2023 Paper Template
% based on the CVPR template provided by Ming-Ming Cheng (https://github.com/MCG-NKU/CVPR_Template)
% modified and extended by Stefan Roth (stefan.roth@NOSPAMtu-darmstadt.de)

\documentclass[10pt,twocolumn,letterpaper]{article}

%%%%%%%%% PAPER TYPE  - PLEASE UPDATE FOR FINAL VERSION
\usepackage[pagenumbers]{cvpr} % To force page numbers, e.g. for an arXiv version

% Include other packages here, before hyperref.
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{booktabs}

\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{xcolor}         % colors
\usepackage{multirow}
% \usepackage{subfigure}
\usepackage{bm}

\newcommand{\BB}[1]{\textcolor{cyan}{[BB:#1]}}
\newcommand{\XY}[1]{\textcolor{red}{[XY:#1]}}
\newcommand{\LC}[1]{\textcolor{purple}{[LC:#1]}}
\newcommand{\LLC}[1]{\textcolor{blue}{[LLC:#1]}}
\newcommand{\robby}[1]{\textcolor{red}{[Robby:#1]}}

% It is strongly recommended to use hyperref, especially for the review version.
% hyperref with option pagebackref eases the reviewers' job.
% Please disable hyperref *only* if you encounter grave issues, e.g. with the
% file validation for the camera-ready version.
%
% If you comment hyperref and then uncomment it, you should delete
% ReviewTempalte.aux before re-running LaTeX.
% (Or just hit 'q' on the first LaTeX run, let it finish, and you
%  should be clear).
\usepackage[pagebackref,breaklinks,colorlinks]{hyperref}


% Support for easy cross-referencing
\usepackage[capitalize]{cleveref}
\crefname{section}{Sec.}{Secs.}
\Crefname{section}{Section}{Sections}
\Crefname{table}{Table}{Tables}
\crefname{table}{Tab.}{Tabs.}




\begin{document}

%%%%%%%%% TITLE - PLEASE UPDATE

\title{Uncertainty-aware Gait Recognition \\ via Learning from Dirichlet Distribution-based Evidence}
% \author{First Author\\
% Institution1\\
% Institution1 address\\
% {\tt\small firstauthor@i1.org}
% % For a paper whose authors are all at the same institution,
% % omit the following lines up until the closing ``}''.
% % Additional authors and addresses can be added with ``\and'',
% % just like the second author.
% % To save space, use either the email address or home page, not both
% \and
% Second Author\\
% Institution2\\
% First line of institution2 address\\
% {\tt\small secondauthor@i2.org}
% }
\author{Beibei Lin\textsuperscript{1}, Chen Liu\textsuperscript{2}, Lincheng Li\textsuperscript{3}, Robby T. Tan\textsuperscript{1,4}~and Xin Yu\textsuperscript{2} \\
\textsuperscript{1} National University of Singapore,
\textsuperscript{2} University of Technology Sydney\\
\textsuperscript{3} Netease Fuxi AI Lab,
\textsuperscript{4} Yale-NUS College\\ 
{\tt\small beibei.lin@u.nus.edu, Chen.Liu-4@student.uts.edu.au, lilincheng@corp.netease.com,}\\ {\tt\small robby.tan@nus.edu.sg, xin.yu@uts.edu.au}
}
\maketitle

%%%%%%%%% ABSTRACT
\begin{abstract}
Existing gait recognition frameworks retrieve an identity in the gallery based on the distance between a probe sample and the identities in the gallery. However, existing methods often neglect that the gallery may not contain identities corresponding to the probes, leading to recognition errors rather than raising an alarm. In this paper, we introduce a novel uncertainty-aware gait recognition method that models the uncertainty of identification based on learned evidence. Specifically, we treat our recognition model as an evidence collector to gather evidence from input samples and parameterize a Dirichlet distribution over the evidence. The Dirichlet distribution essentially represents the density of the probability assigned to the input samples. We utilize the distribution to evaluate the resultant uncertainty of each probe sample and then determine whether a probe has a counterpart in the gallery or not. To the best of our knowledge, our method is the first attempt to tackle gait recognition with uncertainty modelling. Moreover, our uncertain modeling significantly improves the robustness against out-of-distribution (OOD) queries. Extensive experiments demonstrate that our method achieves state-of-the-art performance on datasets with OOD queries, and can also generalize well to other identity-retrieval tasks. Importantly, our method outperforms the state-of-the-art by a large margin of 44.19\% when the OOD query rate is around 50\% on OUMVLP. 
\end{abstract}



%%%%%%%%% BODY TEXT

\begin{figure}[t]
% \subfigure[Original Gait Sequence]
\centering
{\includegraphics[width=0.33\textwidth]{images/schematic1.pdf}}\\
\vspace{-0.5em}
(a) Traditional gait recognition\\
% \vspace{0.5em}
{\includegraphics[width=0.48\textwidth]{images/schematic2.pdf}}\\
\vspace{-0.5em}
(b) Proposed uncertainty-aware gait recognition

\vspace{-0.5em}
\caption{
% Comparison of the original and dynamic gait sequences. The red rectangle denotes the region with a bag.
% The in-distribution and out-of-distribution cases for traditional gait recognition and our uncertainty-aware gait recognition frameworks.
The in-distribution and out-of-distribution cases for traditional gait recognition framework, and our uncertainty-aware gait recognition framework, which handles OOD queries.
}
\vspace*{-2em}
\label{fig_schematic}
\end{figure}

\section{Introduction}
\label{sec:intro}

Gait recognition, as an important biometric retrieval task, aims to identify people by their way of walking \cite{wan2018survey}, and it is widely used in security monitoring and forensics.
Gait recognition has been formulated as a retrieval task, where a query gait sequence, namely a probe, is compared to gait sequences from a gallery. The sequence from the gallery that has the minimal distance to the probe will be regarded as a match and finally its identity will be assigned to the probe. 

Existing gait recognition methods assume there are always corresponding identities in the gallery set. However, in practice, this assumption often does not hold. Some probes possibly do not have their corresponding identities in the gallery, which we call out-of-distribution (OOD) queries.
As shown in Fig.\ref{fig_schematic}(a), current methods still find an identity from the gallery even though actually there is no identity of the probe in the gallery, leading to erroneous results. Therefore, it is highly desirable to develop a gait recognition method that is able to not only find correct identities but also to identify OOD queries. Unfortunately, existing methods that often formulate gait recognition as a feature matching problem do not have a mechanism to address the OOD cases.


% Employing a classifier is a straightforward way to determine whether two samples are from the same class. However, a classifier may fail to predict accurate results, when testing query features are out-of-distribution. Therefore, a simple classifier cannot address this issue. 
% \BB{Another approach is to employ a distance threshold for feature matching. If the distance of two features is higher than a given threshold, they are regarded as different classes. 
% However, the threshold-based strategy may not effectively distinguish between OOD cases and hard in-distribution cases (e.g., pairs of features from different classes but with a small distance).}

In contrast to existing gait recognition approaches, in this work, we formulate gait recognition as an uncertainty-aware feature matching problem. We propose an uncertainty-aware gait recognition framework that can effectively address both in-distribution and out-of-distribution queries.
Our uncertainty-aware framework is agnostic against gait recognition networks, and thus it can adopt any existing gait recognition network as a feature extractor from a probe and gallery sequences. Once we obtain the features of gait sequences, we will retrieve the gallery feature that has the minimal distance to that of the probe. 
Unlike existing methods that directly assign the identity to the probe, we further determine whether the probe and gallery features are from the same identity or not, as shown in Fig.\ref{fig_schematic}(b). 

To examine the identity matches between the probe and gallery, we introduce the theory of evidential deep learning into the process of feature matching. Evidential deep learning \cite{sensoy2018evidential,dempster1968generalization, jsang2016subjective} regards deep networks as an evidence collector and models class probabilities via a Dirichlet distribution. 
Motivated by this, we propose a gait classification model that is employed as an evidence collector to gather opinions or evidence from the paired features of the probe and gallery. The evidence is then utilized to parameterize a Dirichlet distribution. Moreover, the theory of Subjective Logic \cite{sensoy2018evidential} indicates that the Dirichlet distribution represents the probability density of the input features, including in-distribution and out-distribution query scenarios. In other words, the Dirichlet distribution reflects the mass assignment of the input features and their uncertainty in the feature space, thus providing a mechanism to detect OOD probes. Once we obtain the Dirichlet distribution parameterized over the input features, we can apply it to examine whether a pair of retrieved gaits is from the same person and how confident our prediction is.
As a result, our model predicts three scores: a matching score, an unmatching score and an uncertainty score. Based on the three scores, we are able to determine in-distribution and out-of-distribution queries.


% \BB{Different from existing works, we further determine whether the probe and gallery features are from the same identity or not, as shown in Fig.\ref{fig_schematic}(b). Toward this goal, we utilize the theory of evidential deep learning to model the uncertainty in the feature matching. Evidential deep learning \cite{sensoy2018evidential,dempster1968generalization, jsang2016subjective} regards deep networks as an evidence collector to model a Dirichlet distribution of class probabilities. Specifically, we propose a gait classification model that is employed as an evidence collector to gather opinions or evidence from input samples. In order to compare whether two gait features are from the same identity or not, we first compute their discrepancy as a residual feature.}
% Then, our gait classification model, as an evidence collector, will learn a distribution of our constructed residual features (\ie, evidence or opinions) rather than a classification boundary like a simple classifier. 
% Based on the theory of Subjective Logic \cite{sensoy2018evidential}, we leverage the Dirichlet distribution to represent the probability density of the constructed residual features, including in-distribution and out-distribution query scenarios. This also enables us to model the uncertainty of a residual feature. 
% In other words, the Dirichlet distribution reflects the mass assignment of the residual features and their uncertainty in the feature space, thus providing a mechanism to detect OOD probes.

% Once we obtain the Dirichlet distribution parameterized over the residual features, we can apply it to examine whether a pair of retrieved gaits are from the same person and how confident our prediction will be. \BB{To be specific, our model predicts three scores: the matching score (the same identity), the unmatching score and the uncertainty score. In this paper, we model the relation of the three scores and then formulate it to distinguish in-distribution and out-of-distribution scenarios.}

Extensive experiments demonstrate that our method achieves state-of-the-art performance on gait recognition with OOD queries. Importantly, our method outperforms the state-of-the-art method, \ie, GaitGL \cite{lin2021gait}, by a large margin of 44.19\% when the rate of OOD queries is 50\% on one of the largest gait recognition datasets OUMVLP \cite{takemura2018multi}. 
Our main contributions are summarized as follows:

{{
1) We propose a novel uncertainty-aware gait recognition model that can tackle both in-distribution and out-of-distribution queries in a unified framework. To the best of our knowledge, we are the first to address OOD cases for gait recognition.
}}

{{
2) We introduce the theory of evidential deep learning to represent the probability density of a pair of retrieved features and thus model the uncertainty of our prediction.
}}

{{
3) Our framework is agnostic against gait recognition backbones. It can be adopted by existing methods with minimal effort to address OOD queries, thus significantly improving their robustness.
}}



\section{Related Work}
\paragraph{Gait Recognition}
Gait recognition aims to learn discriminative feature representations from people's skeletons or silhouettes for identification purposes \cite{sepas2022deep, shen2022comprehensive, yu2021hid, zhu2021gait, zheng2022gait, zhang2021cross, li2022multi}. 
Its objective is to obtain the identity information for a probe sample from the gallery.
To achieve this goal, many studies focus on improving the feature extraction capability of the model.
These methods can be categorized into three classes, \ie, model-based methods, pose-based methods, and silhouette-based methods.
Model-based methods are designed to extract features from RGB sequences, such as shapes, view angles, and postures ~\cite{wagg2004automated, li2020end, li2021end}. 
Pose-based methods extract 2D poses or 3D poses of human bodies to obtain discriminative feature representations~\cite{teepe2021gaitgraph, liao2020model,hsu2022gaittake,teepe2022towards}. 
Silhouette-based methods generate feature representations by aggregating all temporal information of a gait sequence \cite{shiraga2016geinet,li2020gait,wang2020human, shen2022gait, lin2021gaitmask, lin2020gait, chai2022lagrange, huang2021context, lin2022gaitgl, wang2022gaitstrip, lin2021multi, yu2022cntn, yu2022generalized, liang2022gaitedge, dou2022gaitmpl, dou2022metagait, huang2022enhanced, hou2021set, hou2020gait, huang20213d}.
Although the above methods obtain high recognition accuracy, they often neglect the fact that a probe might be an OOD sample, which does not have counterparts in the gallery. When such probes are exhibited, current methods would fail and cannot tell the gallery does not contain corresponding identities as they do not have a mechanism to address OOD queries. 
To the best of our knowledge, our work is the first attempt to address the OOD query scenario in gait recognition. 

Moreover, many other recognition tasks, such as vehicle and human re-identification ~\cite{zhu2020voc,tang2019cityflow,devyatkov2018multicamera} and face recognition ~\cite{guo2020learning,kalayeh2018human}, also follow a very similar recognition paradigm as gait recognition, and thus face the same situation of OOD queries.
Therefore, our method can be readily applied to those tasks to improve their recognition robustness against OOD queries, thus avoiding incorrect identity assignments. 
In addition, our method does not need to re-train the backbone networks, significantly facilitating existing networks to adapt to OOD queries. 



% \vspace{-0.5em}
% \paragraph{Evidential Deep Learning and Uncertainty Estimation.}
% Our work is built upon uncertainty estimation \cite{kendall2017uncertainties, papadopoulos2011reliable} and models the probability distribution of residual features using a deep neural network.
% %1 Deep Evidential Regression
% Predication variance or uncertainty can also be estimated through dropout \cite{gal2016dropout, molchanov2017variational,gal2017concrete,amini2018spatial}, ensembling \cite{pearce2018uncertainty, lakshminarayanan2017simple} or other sampling approaches \cite{blundell2015weight, hernandez2015probabilistic}. However, this stream of works often relies on expensive sampling operations and it is hard to apply them in feature matching tasks since the probe and gallery features are often fixed.
% Within Bayesian inference, placing prior distributions over deep models to estimate uncertainty has been widely explored \cite{gelman2006prior, gelman2008weakly}. 
% For instance, Evidential Deep Learning \cite{sensoy2018evidential} and Prior Networks \cite{malinin2018predictive, malinin2019reverse} place Dirichlet priors over discrete classification predictions.
% Several works \cite{sensoy2018evidential, tsiligkaridis2021information} estimate uncertainty between a well-defined prior and the data distribution by computing their divergence, while other methods require OOD training data \cite{malinin2018predictive, malinin2019uncertainty, chen2018variational, hafner2020noise} to model uncertainty.
% \BB{In this work, we build our uncertainty-aware gait recognition model by employing the theoretical framework of evidential deep learning \cite{sensoy2018evidential}. To be specific, we place the Dirichlet distribution over our network to model uncertainty. In this way, we can not only correctly classify the labels of inputs but also detect OOD queries.}

\vspace{-1em}
\paragraph{OOD Recognition} In our task, the test set involves identities that have been seen and unseen in the training set. If a testing identity is not registered to the gallery set but exists in the probe set, it will be regarded as an OOD identity. Open-set recognition methods \cite{scheirer2012toward, bao2021evidential} have been proposed to address the OOD problem in recognition. These methods aim to classify the known classes while labeling all OOD samples to one unknown class. However, they cannot be used for our task. 
This is because query identities that have been unseen in the training set will be regarded as unknown classes by open-set recognition methods, while our method still needs to find their counterparts from the gallery set, and the query identities are usually not provided in training.

% {\textcolor{cyan}{
% For the \emph{recognition} task, the classes in the training set are defined as known classes while the testing classes that have not been seen during training are all considered as an unknown (OOD) class. 
% }}
% {\textcolor{cyan}{
% For the \emph{identification} task, the test set involves identities that have been seen and unseen in the training set. If a testing identity is not registered to the gallery set but exists in the probe set, it will be regarded as an OOD identity.
% }}

% {\textcolor{cyan}{
% Recently, some open set recognition methods \cite{scheirer2012toward, bao2021evidential} have been proposed to address the OOD problem in recognition. However, they cannot be used for our task. This is because, for the identification task, many identities that have been unseen in the training set and we still need to find their counterparts from the gallery set for those samples in the probe set.
% }}

% \vspace{-0.5em}
% \paragraph{OOD for identification and recognition.}
% {\textcolor{cyan}{
% For the \emph{recognition} task, the classes in the training set are defined as known classes while the testing classes that have not been seen during training are all considered as an unknown (OOD) class. 
% }}
% {\textcolor{cyan}{
% For the \emph{identification} task, the test set involves identities that have been seen and unseen in the training set. If a testing identity is not registered to the gallery set but exists in the probe set, it will be regarded as an OOD identity.
% }}

% {\textcolor{cyan}{
% Recently, some open set recognition methods \cite{scheirer2012toward, bao2021evidential} have been proposed to address the OOD problem in recognition. However, they cannot be used for our task. This is because, for the identification task, many identities that have been unseen in the training set and we still need to find their counterparts from the gallery set for those samples in the probe set.
% }}


% \vspace{-0.5em}
% \paragraph{OOD for identification, recognition and verification.}
% {\textcolor{cyan}{
% For the \emph{recognition} task, the classes in the training set are defined as known classes while the testing classes that have not been seen during training are all considered as an unknown (OOD) class. 
% % In other words, OOD detection recognition methods \cite{scheirer2012toward, bao2021evidential} aim to classify the known classes while labeling all OOD samples to one unknown class. However, they cannot be used for our task. This is because, for the identification task, the in-distribution probe can be both seen and unseen classes. We still need to find their counterparts from the gallery set for those unseen samples in the probe set.
% }}

% {\textcolor{cyan}{
% For the \emph{identification} task, the test set involves identities that have been seen and unseen in the training set. The test set is divided into the probe set and the gallery set for evaluation. The probe identities (can be seen or unseen classes of the training set) that do not exist their counterparts from the gallery set are defined as OOD identities.
% For the \emph{identification} task, the test set involves identities that have been seen and unseen in the training set. If a testing identity is not registered to the gallery set but exists in the probe set, it will be regarded as an OOD identity. Different from the recognition task, 



% the test set involves identities that have been seen and unseen in the training set. The test set is divided into the probe set and the gallery set for evaluation. The probe identities (can be seen or unseen classes of the training set) that do not exist their counterparts from the gallery set are defined as OOD identities.
% we still need to find their counterparts from the gallery set for those samples in the probe set
% }}

% {\textcolor{cyan}{
% \emph{Verification} is a specific identification method. It usually employs a classification model to directly determine whether two samples are from the same identity. Thus, the verification-based frameworks \cite{wu2016comprehensive} do not have OOD cases. However, it is hard to optimize inter-class and intra-class distance in these frameworks. In this paper, we combine both identification and verification as a baseline method for a fair comparison.
% }}
% the test probe set involves identities that have been seen and unseen in the training set and we still need to find their counterparts from the gallery set for those samples in the probe set.

% For the \emph{identification} task, the test probe set involves identities that have been seen and unseen in the training set 

% and we still need to find their counterparts from the gallery set for those samples in the probe set.
% For the \emph{recognition} task, the classes in the training set are defined as known classes while the testing classes that have not been seen during training are all considered as an unknown (OOD) class. In other words, 

% Thus, the open set recognition aims to classify the known classes while labeling all OOD samples to one unknown class. 

% Gait recognition is a identification task that are essentially different from recognition and verification tasks.

% \noindent \textbf{Identification \emph{vs} Recognition.} For the recognition task, the classes in the training set are defined as known classes while the testing classes that have not been seen during training are all considered as an unknown (OOD) class. Thus, the open set recognition aims to classify the known classes while labeling all OOD samples to one unknown class.


\section{Uncertainty-aware Gait Recognition Framework}

% \begin{figure*}[t] 
% \begin{center}  
% 	\includegraphics[width=0.95\linewidth]{images/Fig1_The overall framework.pdf} 
% 	\vspace{-0.5em}
% 	\caption{Overview of the uncertainty-aware gait recognition framework.}
% \label{Figure_1}
% \end{center}
% \vspace{-2em}
% \end{figure*}
\begin{figure*}[t] 
\begin{center}  
	\includegraphics[width=1.0\linewidth]{images/Fig_training.pdf} 
	\vspace{-1.5 em}
	\caption{Overview of the uncertainty-aware gait recognition framework.
	We first utilize existing backbones to extract features and then construct hard positive and negative feature pairs to train our evidence collector. By Dirichlet-based evidence learning, our method is able to predict matching, unmatching and uncertainty scores of a pair of retrieved features. As a result, our method is able to determine whether they are from the same identity.
	Note that our method does not need to re-train the backbone networks and thus can be easily applied to different backbones and other similar tasks.
	}
\label{Figure_1}
\end{center}
\vspace{-2.5em}
\end{figure*}

Existing gait recognition methods follow a standard paradigm, which can be divided into three steps:
% ------------------first---------------------
(i) Designing and training a gait recognition network. The gait recognition network can be trained with various losses to achieve strong feature extraction capability.
% ------------------Second---------------------
% (ii) Once the gait recognition network has been trained, we use it to extract features from a probe sequence sampled from a probe set and a gallery set. 
% Here, we denote $\mathbb{P} = \{p_{1},p_{2},...,p_{K}\}$ as all the probe features from the probe set and $\mathbb{G} = \{g_{1},g_{2},...,g_{Q}\}$ as all the gallery features from the gallery set, where $p_{i}$ is $i$-th probe feature, $g_{j}$ is the $j$-th gallery feature, and $K$ and $Q$ indicate the number of the probe samples and gallery samples, respectively;
% (iii) One needs to compute the distance between a probe feature $p_i$ and all the gallery features $g_j, j=0,\cdots, Q$.
% For a probe feature $p_{i}$, its distances with respect to all the gallery features can be represented as 
% $\mathbb{D}_{i} = \{D(p_{i}, g_{1}),D(p_{i}, g_{2}),...,D(p_{i}, g_{Q})\}$, where $D$ indicates the Euclidean distance in our experiments.
% Next, we will find the minimum distance in $\mathbb{D}_{i}$. Suppose $D(p_{i}, g_{j})$ is the minimum value in $\mathbb{D}_{i}$, and then the identity of $g_j$ will be assigned to the probe $i$.
(ii) Extracting features. Once the gait recognition network has been trained, we use it to extract features from a sequence sampled from the probe and gallery sets. 
Here, we denote $\mathbb{Q} = \{q_{1},q_{2},...,q_{M}\}$ as all the probe features from the probe set and $\mathbb{G} = \{g_{1},g_{2},...,g_{N}\}$ as all the gallery features from the gallery set, where $q_{i}$ is $i$-th probe feature, $g_{j}$ is the $j$-th gallery feature, and $M$ and $N$ indicate the number of the probe samples and gallery samples, respectively;
(iii) Computing the distance to the gallery features. We compute the distance between a probe feature $q_i$ and all the gallery features.
For a probe feature $q_{i}$, its distances with respect to all the gallery features can be represented as 
$\mathbb{D}_{i} = \{D(q_{i}, g_{1}),D(q_{i}, g_{2}),...,D(q_{i}, g_{Q})\}$, where $D$ indicates the Euclidean distance in our experiments.
Subsequently, we find the minimum distance in $\mathbb{D}_{i}$. Suppose $D(q_{i}, g_{j})$ is the minimum value in $\mathbb{D}_{i}$, and then the identity of $g_j$ will be assigned to the probe $i$.

However, in step (iii), conventional gait recognition methods neglect the fact there might be no matching identity in the gallery. Although $D(q_{i}, g_{j})$ is minimum, the identity $j$ should not be assigned to the probe $i$. When the probe does not have a counterpart identity in the gallery, we refer to it as an OOD query. To address both in-distribution and out-of-distribution queries in a unified framework, we present an uncertainty-aware gait recognition method, as illustrated in Figure \ref{Figure_1}.


% \subsection{Preliminary}
\subsection{Dirichlet-based Evidence Learning}

To address both in-distribution and out-of-distribution queries in a unified framework, our work aims to model the probability distribution of the distance of gait features and estimate the uncertainty of our classification model.
%
Previous methods estimate prediction variance or uncertainty through dropout \cite{gal2016dropout, molchanov2017variational,gal2017concrete,amini2018spatial}, ensembling \cite{pearce2018uncertainty, lakshminarayanan2017simple} or other sampling approaches \cite{blundell2015weight, hernandez2015probabilistic}. However, this stream of works often relies on expensive sampling operations and it is hard to apply them to feature matching tasks, since the probe and gallery features are often fixed.

In Bayesian inference, placing prior distributions over deep models to estimate uncertainty has been widely explored \cite{gelman2006prior, gelman2008weakly}, e.g,  Evidential Deep Learning (EDL) \cite{sensoy2018evidential} and Prior Networks \cite{malinin2018predictive, malinin2019reverse} place Dirichlet priors over discrete classification predictions. Motivated by this, we employ the theoretical framework of EDL to model the probability distribution.


{{
Given an input feature $\bm{x}$, EDL first uses an evidence collector to generate the evidence $\bm{e} = \{e_{k}|k=1,\ldots,K\}$, where $K$ is the number of classes and $e_{k}$ denotes the evidence of the $k$-th class. 
Based on the Subjective Logic theory \cite{sensoy2018evidential}, the evidence $\bm{e}$ corresponds to a Dirichlet distribution parameterized by $\bm{\alpha} = \{\alpha_{k}|k=1,\ldots,K\}$, where $\alpha_{k} = e_{k} + 1$. 
A Dirichlet distribution is a probability density function for all the possible values of a probability mass function $\bm{p}$, and can be represented as:
\begin{equation}
\begin{array}{l}
\mathbf{D}(\bm{p} \mid \bm{\alpha})=\left\{\begin{array}{ll}
\frac{1}{B(\bm{\alpha})} \prod_{i=1}^{K} p_{i}^{\alpha_{i}-1} & \text { for } \bm{p} \in \mathcal{S}_{K}, \\
0 & \text { otherwise },
\end{array}\right. 
\end{array}
\end{equation}
where $\mathcal{S}_{K}=\left\{\bm{p} \mid \sum_{i=1}^{K} p_{i}=1 \text { and } 0 \leq p_{1}, \ldots, p_{K} \leq 1\right\}$ is the $K$-dimensional unit simplex,
and $B(\alpha)$ is the $K$-dimensional multinomial beta function \cite{kotz2004continuous}. Here, $p_{k}$ is the expected probability for the $k$-th class and is calculated as $p_k = \frac{\alpha_k}{S}$, where $S= \sum_{i=1}^{K} \alpha_{i} $.
}}

{{
We denote the ground-truth label of $\bm{x}$ is $\bm{y}$ that is a $K$-dimensional one-hot vector with $y_{i}$ = 1 and $y_{k}$ = 0 for all $k \neq i$. 
The Mean Square Error (MSE) is employed to measure the differences between the estimated class probabilities and the ground truth and can be expressed as:
\begin{align}
\mathcal{L}_{m}(\Theta) &=\int\left\|\bm{y}-\bm{p}\right\|_{2}^{2} \mathbf{D}(\bm{p} \mid \bm{\alpha}) \notag
\\
% &=\sum_{j=1}^{K} \mathbb{E}\left[y_{i j}^{2}-2 y_{i j} p_{i j}+p_{i j}^{2}\right] \notag
% \\
&=\sum_{i=1}^{K}\left(y_{i}^{2}-2 y_{i} \mathbb{E}\left[p_{i}\right]+\mathbb{E}\left[p_{i}^{2}\right]\right),
\end{align}
where $\mathbb{E}$ indicates the expectation operation.
}}

{{
Furthermore, if an input feature cannot be correctly classified, its evidence ideally should shrink to zero. In other words, the Dirichlet distribution will be a uniform Dirichlet distribution. To this end, Sensoy et al. \cite{sensoy2018evidential} introduce a Kullback-Leibler (KL) divergence term. As a result, the total loss function is represented as:
\begin{align}
\mathcal{L}(\Theta) 
=  \mathcal{L}_{m}(\Theta) +\lambda_{t} K L\left[\mathbf{D}\left(\bm{p} \mid \tilde{\bm{\alpha}}\right) \| \mathbf{D}\left(\bm{p} \mid\langle 1, \ldots, 1\rangle\right)\right],
\end{align}
where $\lambda_t = \min(1.0, t/10)$ $\in$ [0, 1] is the annealing coefficient. $\mathbf{D}\left(\bm{p} \mid\langle 1, \ldots, 1\rangle\right)$ indicates a uniform Dirichlet distribution, $t$ is the index of the current training iteration, and 
$\tilde{\bm{\alpha}} = \bm{y} + (1 - \bm{y}) \bigodot \bm{\alpha}$ is the Dirichlet parameters after the removal of the non-misleading evidence from predicted parameters $\bm{\alpha}$ for the input feature $\bm{x}$. 
}}

% \begin{align}
% \mathcal{L}(\Theta) 
% &= \sum_{i = 1}^{N} \mathcal{L}_{i}(\Theta) \notag
% \\
% &+\lambda_{t} \sum_{i = 1}^{N} K L\left[\mathbf{D}\left(\mathbf{p}_{\mathbf{i}} \mid \tilde{\boldsymbol{\alpha}}_{i}\right) \| \mathbf{D}\left(\mathbf{p}_{i} \mid\langle 1, \ldots, 1\rangle\right)\right],
% \end{align}
% where $\lambda_t = min(1.0, t/10)$ $\in$ [0, 1] is the annealing coefficient, $\mathbf{D}\left(\mathbf{p}_{i} \mid\langle 1, \ldots, 1\rangle\right)$ indicates a uniform Dirichlet distribution, $t$ is the index of the current training iteration, and 
% $\tilde{\boldsymbol{\alpha}}_{i} = \mathbf{y}_{i} + (1 - \mathbf{y}_{i}) \bigodot \boldsymbol{\alpha}_{i}$ is the Dirichlet parameters after removal of the non-misleading evidence from predicted parameters $\boldsymbol{\alpha}_{i}$ for the residual feature $i$. 

% {\textcolor{cyan}{
% Furthermore, if a pair of retrieved features cannot be correctly, its evidence ideally should shrink to zero. In other words, the Dirichlet distribution will be a uniform Dirichlet distribution. To solve this problem, sensoy et al. \cite{sensoy2018evidential} introduce a Kullback-Leibler (KL) divergence term. As a result, the total loss function is represented as:
% \begin{align}
% \mathcal{L}(\Theta) 
% &= \sum_{i = 1}^{N} \mathcal{L}_{i}(\Theta) \notag
% \\
% &+\lambda_{t} \sum_{i = 1}^{N} K L\left[\mathbf{D}\left(\mathbf{p}_{\mathbf{i}} \mid \tilde{\boldsymbol{\alpha}}_{i}\right) \| \mathbf{D}\left(\mathbf{p}_{i} \mid\langle 1, \ldots, 1\rangle\right)\right],
% \end{align}
% where $\lambda_t = min(1.0, t/10)$ $\in$ [0, 1] is the annealing coefficient, $\mathbf{D}\left(\mathbf{p}_{i} \mid\langle 1, \ldots, 1\rangle\right)$ indicates a uniform Dirichlet distribution, $t$ is the index of the current training iteration, and 
% $\tilde{\boldsymbol{\alpha}}_{i} = \mathbf{y}_{i} + (1 - \mathbf{y}_{i}) \bigodot \boldsymbol{\alpha}_{i}$ is the Dirichlet parameters after removal of the non-misleading evidence from predicted parameters $\boldsymbol{\alpha}_{i}$ for the residual feature $i$. 
% }}


% {\textcolor{cyan}{
% Unlike traditional binary classifiers, we need to estimate evidence $e_1$, and $e_2$, based on the theory of evidence, and thus we denote the ground-truth label $\mathbf{y}$ as a $K$-dimensional one-hot vector.
% Then, we employ the Mean Square Error (MSE) to measure the differences between the estimated and ground-truth evidence. Here, the ground-truth evidence is written as $\mathbf{p} = \frac{\mathbf{\alpha}}{S}$. Recall $S$ indicates the strength of a Dirichlet distribution.
% Our loss function $\mathcal{L}_{i}(\Theta)$ for a residual feature $i$ is expressed as:
% \begin{align}
% \mathcal{L}_{i}(\Theta) &=\int\left\|\mathbf{y}_{i}-\mathbf{p}_{i}\right\|_{2}^{2} \mathbf{D}(\mathbf{p}_i \mid \boldsymbol{\alpha}_i) \notag
% \\
% % &=\sum_{j=1}^{K} \mathbb{E}\left[y_{i j}^{2}-2 y_{i j} p_{i j}+p_{i j}^{2}\right] \notag
% % \\
% &=\sum_{j=1}^{K}\left(y_{i j}^{2}-2 y_{i j} \mathbb{E}\left[p_{i j}\right]+\mathbb{E}\left[p_{i j}^{2}\right]\right),
% \end{align}
% where $\mathbb{E}$ indicates the Expectation operation. $\mathbf{y}_{i}$ is a one-hot vector of a residual feature $i$ with $y_{ij}$ = 1 and $y_{ik}$ = 0 for all $k \neq j$. $\boldsymbol{\alpha}_i$ denotes the parameters of the Dirichlet density on the prediction of the residual feature $i$.}}

% {\textcolor{cyan}{
% Furthermore, if a pair of retrieved features cannot be classified as a matching or unmatching pair, its evidence ideally should shrink to zero. In other words, the Dirichlet distribution will be a uniform Dirichlet distribution. To this end, a Kullback-Leibler (KL) divergence term is introduced into our loss function. The total loss function of our model is represented as:
% \begin{align}
% \mathcal{L}(\Theta) 
% &= \sum_{i = 1}^{N} \mathcal{L}_{i}(\Theta) \notag
% \\
% &+\lambda_{t} \sum_{i = 1}^{N} K L\left[\mathbf{D}\left(\mathbf{p}_{\mathbf{i}} \mid \tilde{\boldsymbol{\alpha}}_{i}\right) \| \mathbf{D}\left(\mathbf{p}_{i} \mid\langle 1, \ldots, 1\rangle\right)\right],
% \end{align}
% where $\lambda_t = min(1.0, t/10)$ $\in$ [0, 1] is the annealing coefficient, $\mathbf{D}\left(\mathbf{p}_{i} \mid\langle 1, \ldots, 1\rangle\right)$ indicates a uniform Dirichlet distribution, $t$ is the index of the current training iteration, and 
% $\tilde{\boldsymbol{\alpha}}_{i} = \mathbf{y}_{i} + (1 - \mathbf{y}_{i}) \bigodot \boldsymbol{\alpha}_{i}$ is the Dirichlet parameters after removal of the non-misleading evidence from predicted parameters $\boldsymbol{\alpha}_{i}$ for the residual feature $i$. 
% }}





\subsection{Uncertainty Modelling for Gait Recognition}

As aforementioned, the premise that the gallery must contain an identity corresponding to the probe sample is considerably difficult to satisfy in practice, and OOD query cases may more likely happen if the gallery size is limited.
Therefore, a mechanism to tackle OOD gait recognition is highly desirable.


A simple solution to this problem is to develop a binary classifier that classifies whether a probe sample and a gallery sample that has the minimum distance to the probe feature belong to the same identity. 
Here, the errors from the gait recognition networks should not affect the classifier performance as the adopted networks already achieve appealing performance on the gait feature representation. 
Using the simple classifier, the result of each retrieve can be formulated as:
\begin{equation}
Y = 
\left\{\begin{matrix}
\textit{OOD} \quad  F_{c}(q_{i}, g_{j}) \leq 0.5,\\ \hfill 
\textit{ID}  \qquad   F_{c}(q_{i}, g_{j}) > 0.5, \hfill 
\end{matrix}\right.
\end{equation}
where $Y$ is the framework output. $\textit{OOD}$ indicates the input probe feature $q_{i}$ and gallery feature $g_{j}$ are from different identities. $\textit{ID}$ represents the input probe feature $q_{i}$ and gallery feature $g_{j}$ are from the same identity. $F_{c}(\cdot)$ denotes a binary classifier that takes a probe feature and a gallery feature as input.
%
Unfortunately, a simple binary classifier tends to output an over-confident erroneous prediction especially when an OOD query is fed into it. Hence, a classifier would fail to detect the OOD query and thus the gait recognition methods would assign incorrect identities to probes. 

To address this problem, we introduce the theory of Evidential Deep Learning (EDL) \cite{sensoy2018evidential} into current gait recognition tasks and then model the uncertainty of whether the query and retrieved features are from the same identity.
The EDL theory is built on the Dempster-Shafer Theory of Evidence (DST),  a generalization of the Bayesian theory to subjective probabilities \cite{DST}.
To be specific, EDL assigns belief masses (\eg, probabilities) to a set of mutually exclusive labels, \eg, class labels for a sample. 
A belief mass can be assigned to any subset of a frame \cite{DST}, including the whole frame itself. Here, the frame refers to the possible labels for a sample.
Different from traditional binary classification, we can introduce a label ``I do not know'' into the frame, representing the uncertainty to a given sample based on the theoretical framework of EDL.
To formulate the belief mass assignments, Subjective Logic (SL) is an effective tool. 
Specifically, SL can formalize the belief assignments over a frame of discernment via a Dirichlet distribution. Thus, SL allows us to employ the principles of evidential theory to quantify the belief masses and uncertainty.

Driven by SL, we propose an uncertainty-aware gait recognition model that enables us to recognize ID and OOD queries in a unified framework (seeing Figure \ref{Figure_1}).
In order to apply the uncertainty modeling of SL to our method, we first employ a gait recognition network to retrieve a gallery feature that has the minimum distance to the probe feature, and then compute the feature discrepancy between the probe and gallery features, called \emph{residual feature}.
%
Instead of directly measuring whether a query is OOD, we opt to examine the distribution of residual features. 
% In this way, we formulate the retrieval task into a classification problem.
This is because modeling the gallery feature distribution explicitly is intractable. 

Modeling the gallery distribution implicitly via a discriminator will degenerate into the case of learning a simple classifier, which has been demonstrated ineffective for the gait recognition problem.
% Second, we propose a novel uncertainty-aware classification model to collect the evidence of a pair of the retrieved sample and then parameterize a Dirichlet distribution over the evidence. 
In contrast, the label space for a residual feature only contains $\textit{OOD}$ and $\textit{ID}$, which is also much smaller than the identity number of the gallery set.
In accordance with SL, we provide a belief mass $b_k$ to the $K=2$ labels (\eg, $b_1$ indicates the probability of a matching pair, and $b_2$ indicates the probability of an unmatching pair) and an overall uncertainty mass $\mu$. These 3 mass values are all non-negative and sum up to $1$:
\vspace{-0.5em}
\begin{equation}
\label{sum_all_scores}
\mu +\sum_{k=1}^{K} b_{k}=1.
\end{equation}
Therefore, the retrieval result is determined by:
\begin{equation}
\label{eqn_determine}
Y = 
\left\{\begin{matrix}
\textit{OOD}   \quad  b_2 + \mu \geq b_1, \\ \hfill 
\textit{ID}  \qquad   b_2 + \mu < b_1.  \hfill 
\end{matrix}\right.
\end{equation}
When the prediction score of the matching class $b_1$ is higher than the sum of the score of the unmatching class $b_2$ and the uncertainty score, we believe the retrieved identity should be assigned to the probe. Otherwise, we will discard the retrieved identity as the probe sample could be an OOD query. Thanks to the uncertainty modeling of SL, we can effectively refuse OOD queries without harming the overall recognition performance.



\subsection{Evidence Collector Design}
In Eqn.~\eqref{sum_all_scores}, the mass values of $b_k$ and $\mu$ are derived from the evidence ($e_k \ge 0$) and the evidence will be used to determine the Dirichlet distribution and uncertainty. 
Based on EDL, the evidence is a measure of support collected from data in favor of a sample to be classified into a certain class. Therefore, we develop a simple Multi-Layer Perceptron (MLP), including two fully-connected (FC) layers, to collect the evidence. That means the output of MLP is the evidence $\bm{e} = \{e_k | k = 1,\ldots, K\}$, where $e_k$ denotes the evidence of the $k$-th class. To ensure the value of the evidence is non-negative, we adopt the ReLU function to filter the negative value.
Once we obtain evidence $e_k$, the belief mass $b_k$ and uncertainty mass $\mu$ are expressed as:
\begin{equation}
\label{eqn_bk}
b_{k}=\frac{e_{k}}{S} \quad \text{and} \quad \mu = \frac{K}{S} ,
\end{equation}
where $ S = \sum_{i=1}^{K} (e_{i} + 1)$. According to SL, a belief mass assignment corresponds to a Dirichlet distribution with parameters $\alpha_k=e_k + 1$ and $S = \sum_{i=1}^{K} a_{i}$ is the Dirichlet strength. Eqn. \eqref{eqn_bk} can be written as:
\begin{equation}
\label{eq5}
b_{k}=\frac{a_{k}-1}{\sum_{i=1}^{K} a_{i}} \quad \text{and} \quad \mu = \frac{K}{\sum_{i=1}^{K} a_{i}}.
\end{equation}
Eqn. \eqref{eq5} implies if the strength of a Dirichlet distribution or evidence is larger, the Dirichlet distribution is more concentrated and the uncertainty of the prediction will become smaller. At that point, the prediction of the network will be more reliable.
If evidence is lower (\ie, the probabilities assigned to the two classes are small), the Dirichlet distribution will be flatter 
and the uncertainty of the prediction will be higher. Thus, the input might come from an OOD query.



\subsection{Query Pair Construction}
\label{sec:query_pair_constr}
Our uncertainty-aware classification model aims to determine whether a pair of retrieved features are from the same identity or not. To train our classification model, we need to simulate the testing input. 
For this purpose, we construct pairs of samples, including positive and negative pairs. 
To be specific, a positive pair is obtained by sampling different sequences from the same identity, while we sample two sequences from different identities to construct a negative pair.


A simple strategy is to construct positive and negative pairs by randomly selecting two samples from the training set. 
Here, negative pairs are randomly chosen from two identities and positive pairs are randomly selected within the same identity.
Even though we balance the number of positive and negative pairs, we find the network cannot effectively classify hard negative pairs.
This is because the constructed pairs contain numerous simple cases that can be distinguished easily. In other words, our model does not have many chances to learn uncertain cases during training.

Inspired by the hard mining strategies, we construct the hardest positive and the hardest negative pairs within each training batch and thus enforce our model to classify highly uncertain samples.
To be specific, we first randomly select $P$ identities from the training set and then randomly choose $V$ samples from each identity. Therefore, a total of $P\times V$ samples can be selected. Denote $I = \{f_{(1,1)}, f_{(1,2)}, \ldots, f_{(i,j)}, \ldots, f_{(P,V)}\}$ be the features of all selected samples. $(i,j)$ indicates the $j$-th samples of the $i$-th identity. 
For the feature $f_{(i,j)}$, we can calculate the Euclidean distance between $f_{(i,j)}$ and each feature in $I$. 
Finally, the hardest positive pair can be constructed as $< f_{(i,j)} , f_{(i,q)} >$, where $f_{(i,q)}$ has the maximum distance with the feature $f_{(i,j)}$ for all samples in the $i$-th identity. The hardest negative pair can be constructed as $< f_{(i,j)} , f_{(v,w)} >$, where $f_{(v,w)}$ has the minimum distance with the feature $f_{(i,j)}$ and $v \neq i$. 
For all the features of $I$ in the batch, we can totally generate $P \times V$ hardest positive pairs and $P \times V$ hardest negative pairs.


\subsection{Inference}
\begin{figure}[t]
% \subfigure[Original Gait Sequence]
\centering
{\includegraphics[width=0.47\textwidth]{images/Fig_inference.pdf}}\\
\vspace{-0.5em}

\caption{
Inference for the uncertainty-aware gait recognition framework. Given a probe feature $q_i$, we first search a gallery feature $g_j$ that has the minimum distance to it, and then feed the two features to our evidence collector to determine in-distribution and out-of-distribution queries.
}
\vspace*{-1em}
\label{fig_inference}
\end{figure}


The inference stage of the uncertainty-aware gait recognition framework is shown in Figure \ref{fig_inference}.
%
We first use a gait backbone to extract all the probe and gallery features. Recall $\mathbb{Q} = \{q_{1},q_{2},...,q_{M}\}$ denotes all the probe features from the probe set and $\mathbb{G} = \{g_{1},g_{2},...,g_{N}\}$ represents all the gallery features from the gallery set. 
For a probe feature $q_{i}$, we compute the distances between $q_{i}$ and all the gallery features and then find a gallery feature $g_j$ that has the minimum distance to the probe. 

Different from existing methods that assign the identity of $g_j$ to the probe sample $i$, we utilize our uncertainty-aware gait recognition model to further determine whether the two features are from the same identity. To be specific, we compute the feature discrepancy between the two features, expressed by $\bm{x}$. The feature discrepancy $\bm{x}$ is then fed into our uncertainty-aware gait recognition model to generate evidence $\bm{e} =  \{ e_1, e_2 \}$. From Eqn.~\eqref{eqn_bk}, we can obtain the matching probability $b_1$, the unmatching probability $b_2$ and the uncertainty probability $\mu$. 
Based on the three probabilities and Eqn.~\eqref{eqn_determine}, we can determine the identity of the probe sample $i$.


% Here, we denote $\mathbb{Q} = \{q_{1},q_{2},...,q_{M}\}$ as all the probe features from the probe set and $\mathbb{G} = \{g_{1},g_{2},...,g_{N}\}$ as all the gallery features from the gallery set, where $q_{i}$ is $i$-th probe feature, $g_{j}$ is the $j$-th gallery feature, and $M$ and $N$ indicate the number of the probe samples and gallery samples, respectively;


% \subsection{Preliminary: EDL}
% {\textcolor{cyan}{
% Given a residual feature $i$, we first use the proposed evidence collector to generate the evidence $f (x_i|\Theta)$, where $\Theta$ is the parameters of our evidence collector. 
% Based on SL theory, the evidence corresponds to a Dirichlet distribution parameterized by $\alpha_k = f (x_i|\Theta) + 1$.
% A Dirichlet distribution is a probability density function for all the possible values of a probability mass function $\mathbf{p}$. It is depicted by parameters $\mathbf{\alpha} = [\alpha_1,\ldots,\alpha_K]$, expressed by:
% \begin{equation}
% \begin{array}{l}
% \mathbf{D}(\mathbf{p} \mid \boldsymbol{\alpha})=\left\{\begin{array}{ll}
% \frac{1}{B(\boldsymbol{\alpha})} \prod_{i=1}^{K} p_{i}^{\alpha_{i}-1} & \text { for } \mathbf{p} \in \mathcal{S}_{K}, \\
% 0 & \text { otherwise },
% \end{array}\right. 
% \end{array}
% \end{equation}
% where $\mathcal{S}_{K}=\left\{\mathbf{p} \mid \sum_{i=1}^{K} p_{i}=1 \text { and } 0 \leq p_{1}, \ldots, p_{K} \leq 1\right\}$ is the $K$-dimensional unit simplex,
% and $B(\alpha)$ is the $K$-dimensional multi-nomial beta function \cite{kotz2004continuous}. 
% }}

% {\textcolor{cyan}{
% Unlike traditional binary classifiers, we need to estimate evidence $e_1$, and $e_2$, based on the theory of evidence, and thus we denote the ground-truth label $\mathbf{y}$ as a $K$-dimensional one-hot vector.
% Then, we employ the Mean Square Error (MSE) to measure the differences between the estimated and ground-truth evidence. Here, the ground-truth evidence is written as $\mathbf{p} = \frac{\mathbf{\alpha}}{S}$. Recall $S$ indicates the strength of a Dirichlet distribution.
% Our loss function $\mathcal{L}_{i}(\Theta)$ for a residual feature $i$ is expressed as:
% \begin{align}
% \mathcal{L}_{i}(\Theta) &=\int\left\|\mathbf{y}_{i}-\mathbf{p}_{i}\right\|_{2}^{2} \mathbf{D}(\mathbf{p}_i \mid \boldsymbol{\alpha}_i) \notag
% \\
% % &=\sum_{j=1}^{K} \mathbb{E}\left[y_{i j}^{2}-2 y_{i j} p_{i j}+p_{i j}^{2}\right] \notag
% % \\
% &=\sum_{j=1}^{K}\left(y_{i j}^{2}-2 y_{i j} \mathbb{E}\left[p_{i j}\right]+\mathbb{E}\left[p_{i j}^{2}\right]\right),
% \end{align}

% where $\mathbb{E}$ indicates the Expectation operation. $\mathbf{y}_{i}$ is a one-hot vector of a residual feature $i$ with $y_{ij}$ = 1 and $y_{ik}$ = 0 for all $k \neq j$. $\boldsymbol{\alpha}_i$ denotes the parameters of the Dirichlet density on the prediction of the residual feature $i$.
% }}

% {\textcolor{cyan}{
% Furthermore, if a pair of retrieved features cannot be classified as a matching or unmatching pair, its evidence ideally should shrink to zero. In other words, the Dirichlet distribution will be a uniform Dirichlet distribution. To this end, a Kullback-Leibler (KL) divergence term is introduced into our loss function. The total loss function of our model is represented as:
% \begin{align}
% \mathcal{L}(\Theta) 
% &= \sum_{i = 1}^{N} \mathcal{L}_{i}(\Theta) \notag
% \\
% &+\lambda_{t} \sum_{i = 1}^{N} K L\left[\mathbf{D}\left(\mathbf{p}_{\mathbf{i}} \mid \tilde{\boldsymbol{\alpha}}_{i}\right) \| \mathbf{D}\left(\mathbf{p}_{i} \mid\langle 1, \ldots, 1\rangle\right)\right],
% \end{align}
% where $\lambda_t = min(1.0, t/10)$ $\in$ [0, 1] is the annealing coefficient, $\mathbf{D}\left(\mathbf{p}_{i} \mid\langle 1, \ldots, 1\rangle\right)$ indicates a uniform Dirichlet distribution, $t$ is the index of the current training iteration, and 
% $\tilde{\boldsymbol{\alpha}}_{i} = \mathbf{y}_{i} + (1 - \mathbf{y}_{i}) \bigodot \boldsymbol{\alpha}_{i}$ is the Dirichlet parameters after removal of the non-misleading evidence from predicted parameters $\boldsymbol{\alpha}_{i}$ for the residual feature $i$. 
% }}



\section{Experiments}

% % Table generated by Excel2LaTeX from sheet 'Sheet1'
% \begin{table*}[t]
% \footnotesize
% % \sisetup{table-number-alignment=center}
%   \centering
%   \caption{Rank-1 accuracy (\%) on the OUMVLP dataset under different OOD percentages. The standard deviation is shown in parentheses.}
%   \vspace{-1.0em}
%   \resizebox{0.85\textwidth}{!}{
%     \begin{tabular}{c|c|c|c|c|c}
%     \toprule
%     \multicolumn{1}{c|}{\multirow{2}[2]{*}{Methods}} & \multicolumn{5}{c}{Different OOD percentages} \\
%     \cmidrule{2-6}  & \multicolumn{1}{c|}{15\%} & \multicolumn{1}{c|}{25\%} & \multicolumn{1}{c|}{35\%} & \multicolumn{1}{c|}{45\%} & \multicolumn{1}{c}{55\%} \\
%     \midrule
%     GaitSet  & 79.71  & 70.35  & 61.80  & 52.29  & 42.73  \\
%     % \midrule
%     %  GaitSet + Verification & 85.46 ($\pm$ 0.00) & 80.64 ($\pm$ 0.00)  & 74.79 ($\pm$ 0.05)  & 69.64 ($\pm$ 0.05) &  64.12 ($\pm$ 0.04) &  58.90 ($\pm$ 0.10) \\
%     \midrule
%      GaitSet + Navie CLS & 83.09 ($\pm$ 0.00)  & 76.94 ($\pm$ 0.00)  & 71.86 ($\pm$ 0.02) &  67.00 ($\pm$ 0.07) &  63.28 ($\pm$ 0.06) \\
%     \midrule
%     GaitSet + Verification  & 80.53 ($\pm$ 0.01)  & 72.26 ($\pm$ 0.02)  & 64.85 ($\pm$ 0.02) &  56.83 ($\pm$ 0.02) &  49.22 ($\pm$ 0.03) \\
%     \midrule
%     GaitSet + Ours   & \textbf{86.64 ($\pm$ 0.03)}  & \textbf{85.16 ($\pm$ 0.02)} & \textbf{84.26 ($\pm$ 0.03)} & \textbf{83.84 ($\pm$ 0.06)} & \textbf{84.09 ($\pm$ 0.03)}
%     \\
%     \midrule
%     \midrule
%     GaitPart & 80.97  & 71.39  & 62.65  & 52.96  & 43.23  \\
%     \midrule
%     %  GaitPart + Verification & 87.08 ($\pm$ 0.00) & 81.94 ($\pm$ 0.01)  & 75.78 ($\pm$ 0.03)  & 70.23 ($\pm$ 0.14) &  64.46 ($\pm$ 0.05) &  59.00 ($\pm$ 0.08) \\
%     % \midrule
%      GaitPart + Navie CLS  & 85.78 ($\pm$ 0.02)  & 80.64 ($\pm$ 0.02)  & 76.49 ($\pm$ 0.05) &  72.71 ($\pm$ 0.06) &  70.05 ($\pm$ 0.08) \\
%     \midrule
%     GaitPart + Verification  & 81.88 ($\pm$ 0.01)  & 73.65 ($\pm$ 0.02)  & 66.37 ($\pm$ 0.01) &  58.54 ($\pm$ 0.03) &  51.16 ($\pm$ 0.02) \\
%     \midrule
%     GaitPart + Ours   & \textbf{88.41 ($\pm$ 0.01)}  & \textbf{86.96 ($\pm$ 0.03)} & \textbf{86.03 ($\pm$ 0.06)} & \textbf{85.53 ($\pm$ 0.01)} & \textbf{85.68 ($\pm$ 0.01)} \\
%     \midrule
%     \midrule
%     GaitGL & 82.15  & 72.35  & 63.45  & 53.58  & 43.68  \\
%     \midrule
%     % GaitGL + Verification & 90.07 ($\pm$ 0.00) & 83.46 ($\pm$ 0.02)  & 75.50 ($\pm$ 0.04)  & 68.42 ($\pm$ 0.04) &  60.73 ($\pm$ 0.04) &  53.34 ($\pm$ 0.02)  \\
%     % \midrule
%     GaitGL + Navie CLS & 88.81 ($\pm$ 0.01)  & 84.99 ($\pm$ 0.03)  & 82.02 ($\pm$ 0.05) &  79.51 ($\pm$ 0.05) &  77.87 ($\pm$ 0.08)  \\
%     \midrule
%     GaitGL + Verification & 83.06 ($\pm$ 0.00)  & 74.14 ($\pm$ 0.02)  & 66.19 ($\pm$ 0.02) & 57.67 ($\pm$ 0.06) &  49.52 ($\pm$ 0.07) \\
%     \midrule
%     GaitGL + Ours  & \textbf{90.27 ($\pm$ 0.01)}  & \textbf{89.03 ($\pm$ 0.03)} & \textbf{88.21 ($\pm$ 0.04)} & \textbf{87.82 ($\pm$ 0.02)} & \textbf{87.87 ($\pm$ 0.05)} \\
%     \bottomrule
%     \end{tabular}%
%     }
%     \vspace{-1.0em}
%   \label{tab:tab1}%
% \end{table*}%

% Table generated by Excel2LaTeX from sheet 'Sheet1'
\begin{table*}[t]
\footnotesize
% \sisetup{table-number-alignment=center}
  \centering
  \caption{Rank-1 accuracy (\%) on the OUMVLP dataset under different OOD percentages. The standard deviation is shown in parentheses. 
  For a fair comparison, we join the backbones with different OOD detection strategies, e.g., Navie CLS, Verification and Anomaly Detection.
% For a fair comparison, we combined the backbones with three OOD detection strategies, respectively.
  }
  \vspace{-0.8em}
%   \resizebox{0.80\textwidth}{!}{
\renewcommand{\arraystretch}{0.5}
  \setlength{\tabcolsep}{2.7mm}{
% \resizebox{10mm}{12mm}
    \begin{tabular}{c|c|c|c|c|c|c}
    \toprule
    \multicolumn{1}{c|}{\multirow{2}[2]{*}{Methods}} & \multicolumn{6}{c}{Different OOD percentages} \\
    \cmidrule{2-7} & \multicolumn{1}{c|}{7\%} & \multicolumn{1}{c|}{15\%} & \multicolumn{1}{c|}{25\%} & \multicolumn{1}{c|}{35\%} & \multicolumn{1}{c|}{45\%} & \multicolumn{1}{c}{55\%} \\
    \midrule
    GaitSet & 87.34  & 79.71  & 70.35  & 61.80  & 52.29  & 42.73  \\
    % \midrule
    %  GaitSet + Verification & 85.46 ($\pm$ 0.00) & 80.64 ($\pm$ 0.00)  & 74.79 ($\pm$ 0.05)  & 69.64 ($\pm$ 0.05) &  64.12 ($\pm$ 0.04) &  58.90 ($\pm$ 0.10) \\
    \midrule
     GaitSet + Navie CLS & \textbf{88.43 ($\pm$ 0.00)} & 83.09 ($\pm$ 0.00)  & 76.94 ($\pm$ 0.00)  & 71.86 ($\pm$ 0.02) &  67.00 ($\pm$ 0.07) &  63.28 ($\pm$ 0.06) \\
    \midrule
    GaitSet + Verification & 87.37 ($\pm$ 0.00) & 80.53 ($\pm$ 0.01)  & 72.26 ($\pm$ 0.02)  & 64.85 ($\pm$ 0.02) &  56.83 ($\pm$ 0.02) &  49.22 ($\pm$ 0.03) \\
    \midrule
    GaitSet + Anomaly Detection & 74.03 ($\pm$ 0.00) & 75.28 ($\pm$ 0.03)  & 76.82 ($\pm$ 0.04)  & 78.26 ($\pm$ 0.02) &  79.94 ($\pm$ 0.04) &  81.69 ($\pm$ 0.05) \\
    \midrule    
    GaitSet + Ours & 88.14 ($\pm$ 0.00)  & \textbf{86.64 ($\pm$ 0.03)}  & \textbf{85.16 ($\pm$ 0.02)} & \textbf{84.26 ($\pm$ 0.03)} & \textbf{83.84 ($\pm$ 0.06)} & \textbf{84.09 ($\pm$ 0.03)}
    \\
    \midrule
    \midrule
    GaitPart & 88.80  & 80.97  & 71.39  & 62.65  & 52.96  & 43.23  \\
    \midrule
    %  GaitPart + Verification & 87.08 ($\pm$ 0.00) & 81.94 ($\pm$ 0.01)  & 75.78 ($\pm$ 0.03)  & 70.23 ($\pm$ 0.14) &  64.46 ($\pm$ 0.05) &  59.00 ($\pm$ 0.08) \\
    % \midrule
     GaitPart + Navie CLS & \textbf{90.38 ($\pm$ 0.00)} & 85.78 ($\pm$ 0.02)  & 80.64 ($\pm$ 0.02)  & 76.49 ($\pm$ 0.05) &  72.71 ($\pm$ 0.06) &  70.05 ($\pm$ 0.08) \\
    \midrule
    GaitPart + Verification & 88.72 ($\pm$ 0.00) & 81.88 ($\pm$ 0.01)  & 73.65 ($\pm$ 0.02)  & 66.37 ($\pm$ 0.01) &  58.54 ($\pm$ 0.03) &  51.16 ($\pm$ 0.02) \\
    \midrule
    GaitPart + Anomaly Detection & 75.92 ($\pm$ 0.00) & 76.82 ($\pm$ 0.02)  & 77.93 ($\pm$ 0.03)  & 78.97 ($\pm$ 0.02) &  80.23 ($\pm$ 0.01) &  81.56 ($\pm$ 0.02) \\
    \midrule
    GaitPart + Ours & 89.88 ($\pm$ 0.00)  & \textbf{88.41 ($\pm$ 0.01)}  & \textbf{86.96 ($\pm$ 0.03)} & \textbf{86.03 ($\pm$ 0.06)} & \textbf{85.53 ($\pm$ 0.01)} & \textbf{85.68 ($\pm$ 0.01)} \\
    \midrule
    \midrule
    GaitGL & 90.17  & 82.15  & 72.35  & 63.45  & 53.58  & 43.68  \\
    \midrule
    % GaitGL + Verification & 90.07 ($\pm$ 0.00) & 83.46 ($\pm$ 0.02)  & 75.50 ($\pm$ 0.04)  & 68.42 ($\pm$ 0.04) &  60.73 ($\pm$ 0.04) &  53.34 ($\pm$ 0.02)  \\
    % \midrule
    GaitGL + Navie CLS & \textbf{92.27 ($\pm$ 0.00)} & 88.81 ($\pm$ 0.01)  & 84.99 ($\pm$ 0.03)  & 82.02 ($\pm$ 0.05) &  79.51 ($\pm$ 0.05) &  77.87 ($\pm$ 0.08)  \\
    \midrule
    GaitGL + Verification & 90.47 ($\pm$ 0.00) & 83.06 ($\pm$ 0.00)  & 74.14 ($\pm$ 0.02)  & 66.19 ($\pm$ 0.02) & 57.67 ($\pm$ 0.06) &  49.52 ($\pm$ 0.07) \\
    \midrule
    GaitGL + Anomaly Detection & 74.07 ($\pm$ 0.00) & 75.63 ($\pm$ 0.02)  & 77.58 ($\pm$ 0.04)  & 79.42 ($\pm$ 0.03) & 81.57 ($\pm$ 0.02) &  83.79 ($\pm$ 0.02) \\
    \midrule
    GaitGL + Ours & 91.52 ($\pm$ 0.00)  & \textbf{90.27 ($\pm$ 0.01)}  & \textbf{89.03 ($\pm$ 0.03)} & \textbf{88.21 ($\pm$ 0.04)} & \textbf{87.82 ($\pm$ 0.02)} & \textbf{87.87 ($\pm$ 0.05)} \\
    \bottomrule
    \end{tabular}%
    }
    \vspace{-1.2em}
  \label{tab:tab1}%
\end{table*}%

\subsection{Dataset}
% We evaluate the effectiveness of our method on one of the largest gait recognition benchmarks OUMVLP \cite{takemura2018multi}.
% We evaluate the effectiveness of our method on two gait recognition benchmarks OUMVLP \cite{takemura2018multi} and Gait3D \cite{zheng2022gait}. The details of these datasets are depicted as follows.
We evaluate the effectiveness of our method on two gait recognition benchmarks OUMVLP \cite{takemura2018multi} and CASIA-B \cite{yu2006framework}. The details of these datasets are depicted as follows.

\noindent \textbf{OUMVLP} \cite{takemura2018multi} is a large-scale gait dataset. 
It includes 10,307 subjects, each of which contains a gallery set and a probe set. Each subject in a set is collected from 14 views ($0^\circ$-$90^\circ$ and $180^\circ$-$270^\circ$ with a sampling interval of  $15^\circ$). In this work, we use the evaluation protocol \cite{chao2019gaitset,fan2020gaitpart,lin2021gait} to verify the effectiveness of the proposed uncertainty-aware gait recognition method. 
Under the default evaluation protocol,  7\% of the probes are OOD queries, which brings a challenge to the state-of-the-art methods. In addition, we manually remove some gallery sets of subjects to construct more severe OOD query scenarios.


\noindent \textbf{CASIA-B} \cite{yu2006framework} is one of the most popular gait datasets. It includes 124 subjects and about 1.3K sequences. Each subject is collected in sequences from 11 views ($0^{\circ}$, $18^{\circ}$, \ldots, $180^{\circ}$). For each view, 10 groups of sequences are collected.  In our experiments, the CASIA-B dataset is divided into three subsets: a training set, a query set, and a gallery set. To be specific, 74 IDs\footnote{Here, ID refers to identity. With a slight abuse of notation, we refer ID to as in-distribution for simplicity.} are used to construct the training set and the remaining 50 IDs are used as the query and gallery sets. 
We select one image from each ID as the gallery set and the rest images as the query set. By removing IDs from the gallery set, we construct various OOD query cases.

% It contains 124 subjects with three different variations, including view angles, clothing and carrying conditions. For each subject, a total of 11 view angles ($0^{\circ}$-$180^{\circ}$ with interval $18^{\circ}$). 
% For each view, 10 groups of videos with three different settings (normal walking (NM), walking with a bag (BG) and walking with a coat (CL)) are collected.
% Therefore, there are 124(subjects) $\times$ 11(view angles) $\times$ 10 (6 NM + 2 BG + 2 CL)  = 13,640 gait sequences in the CASIA-B benchmark dataset.
% \textbf{GREW} \cite{zhu2021gait} is a large-scale and in-the-wild dataset. It includes 26,345 subjects and 128,671 sequences. It also contains four data types: Silhouettes, Optical Flow, 2D Pose and 3D pose. The dataset has been divided into three parts, the training set, the validation set and the test set. The training set consists of 20,000 identities and 102,887 sequences, while the validation set contains 345 identities and 1,784 sequences. The test set includes 6,000 identities and each identity contains four sequences. In particular, GREW also contains a distractor set consisting of 233,857 sequences (unlabeled data). In the test stage, we take two sequences as the gallery set and the rest sequences are used as the probe set.

% \noindent \textbf{Gait3D} \cite{zheng2022gait} is a large-scale and in-the-wild dataset. It includes 4K subjects and 25K sequences captured by 39 surveillance cameras in the wild. In our experiments, the Gait3D dataset is divided into three subsets: a training set, a query set, and a gallery set. To be specific, 3,000 IDs\footnote{Here, ID refers to identity. With a slight abuse of notation, we refer ID to as in-distribution for simplicity.} are used to construct the training set and the remaining 1,000 IDs are used as the query and gallery sets. We select one image from each ID as the gallery set and the rest images as the query set. By removing IDs from the gallery set, we construct various OOD query cases.

Since we construct OOD queries on both OUMVLP and CASIA-B datasets, we will release all OOD configurations for reproducibility.
% Moreover, to verify the applicability of our method to other tasks, we also conduct experiments on vehicle re-identification benchmark VERI-Wild \cite{lou2019large, lou2021large} and synthetic person-identification benchmark VC-Clothes \cite{wan2020person}. More details can be found in the supplementary files.
% \noindent \textbf{VERI-Wild} \cite{lou2019large, lou2021large} is a large-scale dataset used for vehicle re-identification tasks.
% It includes 416,314 images captured by 174 surveillance cameras in the wild. In general, the VERI-Wild dataset is divided into three subsets: a training set, a query set, and a gallery set. 
% We use the evaluation protocol as in \cite{he2020fastreid}, which includes three different settings, \emph{i.e.}, Small-size Setting (SS), Medium-size Setting (MS) and Large-size Setting (LS). 
% In the three settings, 3,000 IDs,\footnote{Here, ID refers to identity. With a slight abuse of notation, we refer ID to as in-distribution for simplicity.} 5,000 IDs and 10,000 IDs are used to construct the query and gallery sets, respectively. In each setting, we select one image from each ID as the gallery set and the rest images as the probe set. 
% By removing IDs from the gallery set or probe set, we construct various OOD query cases.


% \noindent \textbf{VC-Clothes}\cite{wan2020person} is a synthetic person re-identification dataset. It includes 512 subjects and each subject has 36 corresponding images. 
% The dataset has been divided into two subsets: a training set and a testing set. Each set contains 256 IDs. 
% In the inference stage, we randomly choose four images from each subject as the gallery set and the rest images are used as the probe set. Hence, the gallery and probe sets contain 1020 images and 8591 images, respectively.



\subsection{Implementation Details}
Our uncertainty-aware classification model is built on multilayer perceptron (MLP) including two FC layers. 
For the OUMVLP dataset, we take three common gait recognition frameworks, GaitGL \cite{lin2021gait}, GaitPart \cite{fan2020gaitpart} and GaitSet \cite{chao2019gaitset}, as the backbones to extract gait features and evaluate the effectiveness of our proposed uncertainty-aware classification model. Since the feature sizes of the three backbones are different, we set the hidden layer size of our MLP to 544, 544 and 768, respectively. 
For the CASIA-B dataset, the MLP hidden layer size of three frameworks is set to 8192.
% For the VERI-Wild dataset, BoT \cite{luo2019bag} is used as the backbone and thus the hidden layer size of MLP is set to 1024.
% For the VC-clothes dataset, we take CBN \cite{zhuang2020rethinking} as the backbone and the hidden layer size of MLP is set to 4096.

In Sec.~\ref{sec:query_pair_constr}, we propose a sampling strategy that constructs positive and negative pairs of samples to train our model. The proposed sampling strategy includes two hyper-parameters $P$ and $V$. $P$ and $V$ are set to 32 and 8 respectively for all the experiments. The learning rate is set to 1e-3 and then decays by a factor of 0.1 every 10 iterations.
The total training iteration number for all the experiments is set to 50. 
In our experiments, we also run each OOD setting by three times in order to demonstrate the robustness of our method against OOD queries. Specifically, in each OOD setting, we randomly remove a certain percentage of the identities of the original gallery set three times to construct a different gallery set. We will release our training details and codes.

% \subsection{Baseline Algorithms}
% {\textcolor{cyan}{Since existing backbones cannot handle OOD cases, we introduce different strategies and join them with the backbone to construct baseline algorithms. In other words, the upper bound of baseline algorithms under different OOD percentages is 100\%. The details of baseline algorithms are as follows:}}

% \noindent {\textcolor{cyan}{ (1) \textbf{Backbone + Verification}: A classifier is designed to classify whether two features are from the same identity or not.}}

% \noindent {\textcolor{cyan}{ (2) \textbf{Backbone + Threshold}: Setting a distance threshold is also a straightforward way to address OOD cases. When the distance between the probe and the closest sample from the gallery is larger than the threshold, the probe will be regarded as an OOD query. In this paper, the threshold is calculated by Equal Error Rate (EER) in the verification setting. To be specific, we construct 20k gait pairs, including 10k paired samples and 10k unpaired samples. Then, we search for the best threshold leading to the lowest EER. Finally, the searched threshold is used to further determine whether two samples are from the same identity.}}

% \begin{table*}[t]
% \footnotesize
%   \centering
%   \caption{Rank-1 accuracy (\%) on the VERI-Wild dataset under different OOD percentages. The standard deviation is reported in parentheses.}
% %   \vspace{-0.5em}
%   \resizebox{0.80\textwidth}{!}{
%     \begin{tabular}{c|c|c|c|c|c|c}
%     \toprule
%     \multirow{2}[2]{*}{Setting} & \multirow{2}[2]{*}{Methods} & \multicolumn{5}{c}{Different OOD percentages} \\
% \cmidrule{3-7}          &       & \multicolumn{1}{c|}{15\%} & \multicolumn{1}{c|}{25\%} & \multicolumn{1}{c|}{35\%} & \multicolumn{1}{c|}{45\%} & \multicolumn{1}{c}{55\%} \\
%     \midrule
% \multirow{5}[2]{*}{SS (\#ID=3k)} & BoT   & 74.07  & 66.02  & 57.86  & 49.40  & 40.90  \\
% \cmidrule{2-7}          & BoT + NN (verification protocol) & 69.26 ($\pm$ 0.33)  & 66.76 ($\pm$ 0.26) & 64.81 ($\pm$ 0.14) &  63.25 ($\pm$ 0.15) &  62.82 ($\pm$ 0.30)  \\
% \cmidrule{2-7}          & BoT + Threshold (EER)  & 74.10 ($\pm$ 0.20)  & 66.36 ($\pm$ 0.28) & 58.99 ($\pm$ 0.11) &  52.01 ($\pm$ 0.01) &  46.24 ($\pm$ 0.35) \\
% \cmidrule{2-7}          & BoT + Ours & \textbf{78.19 ($\pm$ 0.21)} & \textbf{76.34 ($\pm$ 0.20)} & \textbf{75.09 ($\pm$ 0.05)} & \textbf{74.71 ($\pm$ 0.21)} & \textbf{75.34 ($\pm$ 0.25)} \\
%     \midrule
%     \midrule

%     \multirow{5}[2]{*}{MS (\#ID=5k)} & BoT   & 71.00  & 63.64  & 55.83  & 47.73  & 39.87  \\
% \cmidrule{2-7}          & BoT + NN (verification protocol) & 64.81 ($\pm$ 0.15)  & 61.65 ($\pm$ 0.34) & 59.33 ($\pm$ 0.17) &  57.17 ($\pm$ 0.40) &  55.84 ($\pm$ 0.42) \\
% \cmidrule{2-7}          & BoT + Threshold (EER) & 70.85 ($\pm$ 0.16)  & 63.34 ($\pm$ 0.30) & 55.86 ($\pm$ 0.27) &  48.17 ($\pm$ 0.17) &  41.04 ($\pm$ 0.26)\\
% \cmidrule{2-7}          & BoT + Ours & \textbf{73.70 ($\pm$ 0.22)} & \textbf{70.85 ($\pm$ 0.43)} & \textbf{68.97 ($\pm$ 0.20)} & \textbf{67.65 ($\pm$ 0.21)} & \textbf{67.36 ($\pm$ 0.23)} \\

% \midrule
% \midrule

%     \multirow{5}[2]{*}{LS (\#ID=10k)} & BoT   & 65.58  & 58.65  & 51.54  & 44.15  & 36.82  \\
% \cmidrule{2-7}          & BoT + NN (verification protocol) & 59.25 ($\pm$ 0.06)  & 56.28 ($\pm$ 0.20) & 53.39 ($\pm$ 0.04) &  50.99 ($\pm$ 0.36) &  49.15 ($\pm$ 0.39) \\
% \cmidrule{2-7}          & BoT + Threshold (EER)  & 65.43 ($\pm$ 0.11)  & 58.63 ($\pm$ 0.06) & 51.57 ($\pm$ 0.02) &  44.40 ($\pm$ 0.16) &  37.14 ($\pm$ 0.15) \\
% \cmidrule{2-7}          & BoT + Ours & \textbf{67.45 ($\pm$ 0.06)} & \textbf{64.14 ($\pm$ 0.22)} & \textbf{61.25 ($\pm$ 0.16)} & \textbf{59.07 ($\pm$ 0.48)} & \textbf{57.92 ($\pm$ 0.54)} \\
% \bottomrule
%     \end{tabular}%
%     }
%   \label{tab_vehicle}%
% \vspace{-1.0em}
% \end{table*}%

% Table generated by Excel2LaTeX from sheet 'Sheet1'
\begin{table*}[t]
\footnotesize
% \sisetup{table-number-alignment=center}
  \centering
  \caption{Rank-1 accuracy (\%) on the CASIA-B dataset under different OOD percentages. The standard deviation is shown in parentheses.}
  \vspace{-0.8em}
  \renewcommand{\arraystretch}{0.5}
\setlength{\tabcolsep}{3.8mm}{
% \resizebox{0.77\textwidth}{!}{
    \begin{tabular}{c|c|c|c|c|c|c}
    \toprule
    \multicolumn{1}{c|}{\multirow{2}[2]{*}{Methods}} & \multicolumn{6}{c}{Different OOD percentages} \\
    \cmidrule{2-7} & \multicolumn{1}{c|}{0\%} & \multicolumn{1}{c|}{15\%} & \multicolumn{1}{c|}{25\%} & \multicolumn{1}{c|}{35\%} & \multicolumn{1}{c|}{45\%} & \multicolumn{1}{c}{55\%} \\
    \midrule
    GaitSet & \textbf{91.81}  & 79.45  & 71.05  & 61.66  & 53.13  & 44.11  \\
    \midrule
    GaitSet + Ours & 91.38 ($\pm$ 0.00)  & \textbf{82.00 ($\pm$ 0.74)}  & \textbf{76.28 ($\pm$ 0.66)} & \textbf{72.21 ($\pm$ 1.35)} & \textbf{68.51 ($\pm$ 2.25)} & \textbf{63.90 ($\pm$ 1.47)}
    \\
    \midrule
    \midrule
    GaitPart & \textbf{92.61}  & 80.07  & 70.96  & 61.16  & 52.85  & 44.39  \\
    \midrule
    GaitPart + Ours & 90.95 ($\pm$ 0.00)  & \textbf{81.89 ($\pm$ 0.45)}  & \textbf{74.70 ($\pm$ 0.48)} & \textbf{69.76 ($\pm$ 1.04)} & \textbf{65.24 ($\pm$ 1.97)} & \textbf{61.09 ($\pm$ 1.33)} \\
    \midrule
    \midrule
    GaitGL & \textbf{94.78}  & 82.45  & 73.37  & 63.85  & 54.34  & 44.97  \\
    \midrule
    GaitGL + Ours & 93.79 ($\pm$ 0.00)  & \textbf{84.66 ($\pm$ 1.30)}  & \textbf{78.33 ($\pm$ 0.84)} & \textbf{73.22 ($\pm$ 1.62)} & \textbf{69.71 ($\pm$ 1.50)} & \textbf{65.83 ($\pm$ 0.27)} \\
    \bottomrule
    \end{tabular}%
    }
    \vspace{-1.2em}
  \label{tab_casia}%
\end{table*}%

% Table generated by Excel2LaTeX from sheet 'Sheet1'
% \begin{table*}[t]
% \footnotesize
% % \sisetup{table-number-alignment=center}
%   \centering
% %   \caption{Rank-1 accuracy (\%) on the Gait3D dataset under different OOD percentages. The standard deviation is shown in parentheses.}
%   \caption{Rank-1 accuracy (\%) on the CASIA-B dataset under different OOD percentages. The standard deviation is shown in parentheses.}
%   \vspace{-1.0em}
%   \resizebox{0.85\textwidth}{!}{
%     \begin{tabular}{c|c|c|c|c|c}
%     \toprule
%     \multicolumn{1}{c|}{\multirow{2}[2]{*}{Methods}} & \multicolumn{5}{c}{Different OOD percentages} \\
%     \cmidrule{2-6} & \multicolumn{1}{c|}{15\%} & \multicolumn{1}{c|}{25\%} & \multicolumn{1}{c|}{35\%} & \multicolumn{1}{c|}{45\%} & \multicolumn{1}{c}{55\%} \\
%     \midrule
%     GaitSet  &  22.61  & 21.04  & 18.64   & 16.63  & 14.36    \\
%     % \midrule
%     % GaitSet + Verification &    &    &   &    &    \\
%     % \midrule
%     % GaitSet + Threshold &    &    &   &    &    \\
%     \midrule
%     GaitSet + Ours    & \textbf{24.63 ($\pm$ 0.97)}  & \textbf{31.50 ($\pm$ 1.66)} & \textbf{39.74 ($\pm$ 1.58)} & \textbf{48.42 ($\pm$ 1.06)} & \textbf{57.03 ($\pm$ 1.24)}
%     \\
%     \midrule
%     \midrule
%     GaitPart   & 15.58   & 14.64   & 13.07    & 11.77    & 10.48   \\
%     % \midrule
%     % GaitPart + Verification &    &    &   &    &    \\
%     % \midrule
%     % GaitPart + Threshold  &    &    &   &    &    \\
%     \midrule
%     GaitPart + Ours    & \textbf{17.18 ($\pm$ 0.80)}  & \textbf{25.56 ($\pm$ 1.68)} & \textbf{35.31 ($\pm$ 1.43)} & \textbf{45.43 ($\pm$ 0.78)} & \textbf{55.12 ($\pm$ 1.36)}\\
%     \midrule
%     \midrule
%     GaitGL   &  37.64  & 34.73   & 31.38 & 27.24   &  22.74  \\
%     % \midrule
%     % GaitGL + Verification &    &    &   &    &    \\
%     % \midrule
%     % GaitGL + Threshold & 36.80 ($\pm$ 1.02)  & 34.68 ($\pm$ 0.85) & 32.61 ($\pm$ 1.53) & 30.36 ($\pm$ 1.35) & 28.42 ($\pm$ 0.78)   \\
%     \midrule
%     GaitGL + Ours    & \textbf{39.92 ($\pm$ 1.21)}  & \textbf{43.74 ($\pm$ 1.64)} & \textbf{48.58 ($\pm$ 1.33)} & \textbf{53.71 ($\pm$ 0.54)} & \textbf{59.75 ($\pm$ 0.38)}\\
%     \bottomrule
%     \end{tabular}%
%         }
%     \vspace{-1.0em}
%   \label{tab_casia}%
% \end{table*}%

% % Table generated by Excel2LaTeX from sheet 'Sheet1'
% \begin{table*}[t]
% \footnotesize
%   \centering
%   \caption{Rank-1 accuracy (\%) on the VC-Clothes dataset under different OOD percentages. The standard deviation is reported in parentheses.}
% %   \vspace{-0.5em}
%     \begin{tabular}{c|c|c|c|c|c}
%     \toprule
%     \multirow{2}[2]{*}{Methods} & \multicolumn{5}{c}{Different OOD percentages}  \\
% \cmidrule{2-6}          & \multicolumn{1}{c|}{15\%} & \multicolumn{1}{c|}{25\%} & \multicolumn{1}{c|}{35\%} & \multicolumn{1}{c|}{45\%} & \multicolumn{1}{c}{55\%}  \\
%     \midrule
%     \multirow{1}[1]{*}{CBN}  & 83.91  & 73.67  & 64.10  & 54.34  & 44.98   \\
%     \midrule
%     \multirow{1}[1]{*}{CBN + Verification}  & 89.68 ($\pm$ 0.10) & 90.71 ($\pm$ 0.17) & 91.72 ($\pm$ 0.40) & 92.90 ($\pm$ 0.43) & 93.95 ($\pm$ 0.14)   \\
%     \midrule
%     \multirow{1}[1]{*}{CBN + Threshold}  & 87.96 ($\pm$ 0.09) & 80.93 ($\pm$ 0.22) & 75.74 ($\pm$ 0.29) & 72.00 ($\pm$ 0.56) & 69.17 ($\pm$ 0.82)    \\
%     \midrule
%     \multirow{1}[1]{*}{CBN + Ours} & \textbf{97.11 ($\pm$ 0.23)} & \textbf{97.15 ($\pm$ 0.37)} & \textbf{97.29 ($\pm$ 0.33)} & \textbf{97.58 ($\pm$ 0.38)} & \textbf{97.70 ($\pm$ 0.15)}  \\
%     \bottomrule
%     \end{tabular}%
  
%   \label{tab_person}%
%   \vspace{-1.5em}
% \end{table*}%



\subsection{Comparisons with the State-of-the-art}

% \paragraph{Evaluation on OUMVLP under different OOD percentages.} 
\noindent \textbf{Evaluation on OUMVLP under different OOD percentages}
We introduce our uncertainty-aware classification model to several state-of-the-art gait recognition models, including GaitSet \cite{chao2019gaitset}, GaitPart \cite{fan2020gaitpart}, and GaitGL \cite{lin2021gait} on OUMVLP, and compare them with/without employing our method.
In the experiments, we run three gait recognition models on five test sets with different percentages of OOD samples. 
As indicated in Table \ref{tab:tab1}, after introducing our method, all of the gait recognition models obtain improvements on the test sets with OOD samples.
As the percentage of OOD samples increases in the test sets, we observe that existing methods suffer severe performance degradation. As expected, when the OOD query rate reaches 50\%, the accuracy of current methods almost decreases to half of their original performance (from 90.17\% to 43.68\% for GaitGL).
This implies that current gait recognition methods do not have the ability to detect OOD probes. 
On the contrary, our uncertainty-aware classification model is able to identify the OOD queries and thus avoids assigning gallery IDs to the probes.
As a result, our method maintains high recognition accuracy while successfully recognizing OOD probes. 
This indicates the effectiveness and superiority of our proposed method.
Moreover, the small standard deviation of our results demonstrates the stability of our model in recognizing OOD samples.

% \paragraph{Evaluation on CASIA-B under different OOD percentages.}
\noindent \textbf{Evaluation on CASIA-B under different OOD percentages}
For the CASIA-B dataset, we also choose GaitSet \cite{chao2019gaitset}, GaitPart \cite{fan2020gaitpart}, and GaitGL \cite{lin2021gait} as our baseline algorithms. Table. \ref{tab_casia} shows the experimental results.
It can be observed that our method decreases slightly performance without OOD (from 94.78\% to 93.79\% for GaitGL).
This is because our method is prone to ensure the found matching pairs to be correct with minimal risk. In contrast, our method achieves promising performance in all OOD cases. To be specific, the accuracy of GaitGL without our uncertainty model decreases significantly as the percentage of OOD samples increases (from 94.78\% to 44.97\% for GaitGL), while our method outperforms the baseline method for the five OOD percentages.


% \paragraph{Evaluation on Gait3D under different OOD percentages.}
% For the Gait3D dataset, we also choose GaitSet \cite{chao2019gaitset}, GaitPart \cite{fan2020gaitpart}, and GaitGL \cite{lin2021gait} as our baseline algorithms. Table. \ref{tab_gait3d} shows the experimental results. It can be observed that our method achieves promising performance in all cases. To be specific, our method achieves stable recognition accuracy for the five OOD percentages while outperforming the baseline method. In contrast, the accuracy of GaitGL without our uncertainty model decreases significantly as the percentage of OOD samples increases.




% \begin{figure*}[t] 
% \centering
% \begin{minipage}{0.485\textwidth}
% 	\scalebox{0.9}[1]{\includegraphics[width=1\linewidth]{images/Fig_3_1_lineChart.pdf}} 
% % 	\vspace{-0.5em}
% 	\caption{Comparisons of GaitGL, GaitGL with an NN classifier, and GaitGL with our model wrt. different percentages of OOD queries. \label{Fig_ablation_OOD}}
% \end{minipage}
% \hfill
% \begin{minipage}{0.485\textwidth}
% 	\scalebox{0.9}[1]{\includegraphics[width=1\linewidth]{images/Fig_3_33_lineChart.pdf}} 
% % 	\vspace{-0.5em}
% 	\caption{Impacts of different query pair construction on training our model. We test the models with different percentages of OOD queries. \label{Fig_ablation_QC}}
% \end{minipage}
% \vspace{-1em}
% \end{figure*}

% \BB{By employing the verification and threshold strategies, the existing framework could address a part of OOD cases. For example, ``GaitGL + Verification'' and ``GaitGL + Threshold'' achieves 53.34\% and 49.52\% under the 55\% OOD setting, respectively, while the GaitGL backbone only achieves a rank-1 accuracy of 43.68\%. This is because the verification-based strategy utilizes a classifier to determine whether two retrieved samples are from the same class. The }

\subsection{Ablation Study} \label{Ablation_Study} 
To verify the effectiveness of our proposed components, \ie, the uncertainty modeling and query construction, we conduct ablation studies on gait recognition. 
% To be specific, we employ one state-of-the-art method GaitGL as our baseline and conduct experiments on the OUMVLP dataset. For fair comparisons, GaitGL is pre-trained on the OUMVLP dataset without fine-tuning.

% {\textcolor{cyan}{Since existing backbones cannot handle OOD cases, we introduce different strategies and join them with the backbone to construct baseline algorithms. In other words, the upper bound of baseline algorithms under different OOD percentages is 100\%. The details of baseline algorithms are as follows:}}


\noindent \textbf{Effectiveness of the uncertainty modeling}
Since existing backbones cannot handle OOD cases, they cannot reach 100\% accuracy in all OOD settings.
For a fair comparison, we introduce different OOD detection strategies and join them with the gait backbone to construct baseline algorithms. As a result, the upper bound accuracy of the constructed baseline algorithms is 100\%. The details of baseline algorithms are as follows: 

\noindent \textbf{Backbone + Naive CLS} An NN classifier is designed to classify whether two features are from the same identity or not. The classifier is trained by a binary cross-entropy loss and our constructed pairs. The difference between this classifier and our model is that our model has uncertainty modeling. Thus, our model not only classifies whether two features are from the same identity but also provides the uncertainty estimation of the prediction by a Dirichlet distribution parameterized over all the constructed training pairs, while a simple classifier only makes decisions based on its predicted labels since it only learns a classification boundary.
The experimental results are shown in Table \ref{tab:tab1}. As expected, without any mechanism to distinguish OOD queries, the performance degrades as the number of OOD samples increases. 
Although a simple classifier outperforms slightly our method in the 7\% OOD percentage, we can see that a simple classifier still struggles to recognize OOD queries. This is because a simple classifier tends to overfit the in-distribution pairs. Thus, it may produce inaccurate predictions, when the input pairs of features are out-of-distribution.


\noindent \textbf{Backbone + Verification} Setting a similarity threshold is another straightforward way to address OOD cases. When the similarity between the probe and the closet sample from the gallery is smaller than the threshold, the probe will be regarded as an OOD query. In our cases, the threshold is calculated by Equal Error Rate (EER) from the verification setting. To be specific, we construct 20k gait pairs, including 10k paired samples and 10k unpaired samples. Then, we search for the best threshold leading to the lowest EER. Finally, the searched threshold is used to further determine whether two samples are from the same identity.
We will release the constructed gait pairs for reproducibility. 
Table \ref{tab:tab1} shows that the verification-based methods address a few OOD cases, leading to a slight performance improvement. For example, in the 15\% OOD setting, ``GaitGL + Verification'' outperforms GaitGL by 0.91\% on OUMVLP. However, the verification-based methods cannot handle most OOD cases. This is because most OOD queries are hard samples. Here, ``hard'' means the OOD query is very similar to the closet sample from the gallery set. In other words, its similarity is higher than the given threshold, leading to erroneous results.
Moreover, we manually search all thresholds for feature matching. The experimental results can be found in the supplementary material.

\noindent \textbf{Backbone + Anomaly Detection} One-Class Novelty Detection \cite{yang2021generalized} is proposed to address OOD cases in recognition. It aims to classify all OOD cases into one class. Although it cannot be used directly in our task, we combine it with an NN classifier to detect our OOD cases. To be specific, we randomly construct in-distribution and out-of-distribution feature pairs to train an NN classifier. The trained classifier is then employed in feature matching to determine whether the retrieved gait pairs are the same identity or not. The experimental results are shown in Table \ref{tab:tab1}. It can be observed that one-class novelty detection effectively addresses most OOD cases. For example, ``GaitGL +  Anomaly Detection'' outperforms GaitGL by 40.19\% when the OOD query rate is 55\% on OUMVLP. However, in the 7\% and 15\% OOD settings, its performance decreases significantly. This is because one-class novelty detection tends to classify an in-distribution pair that has a large distance to OOD cases. As a result, the accuracy in low OOD percentage shows a significant decrease.

On the contrary, our method maintains the recognition performance even though the rate of OOD samples is more than 50\%. This indicates that our method successfully identifies OOD queries and assigns correct identities to in-distribution queries. We also conduct our experiments three times on different gallery sets. The small standard deviation values also prove the stability of our method.
% In Figure \ref{Fig_ablation_OOD}, we show three cases: direct employment of the baseline, baseline with a simple classifier, and our method. As expected, without any mechanism to distinguish OOD queries, the performance degrades as the number of OOD samples increases. 
% Although a simple classifier can improve performance slightly, we can see that a simple classifier still struggles to recognize OOD queries. On the contrary, our method maintains the recognition performance even though the rate of OOD samples is more than 50\%. This indicates that our method successfully identifies OOD queries and assigns correct identities to in-distribution queries. We also conduct our experiments three times on different gallery sets. Due to the small standard deviation values, we do not draw standard deviation in Figure \ref{Fig_ablation_OOD}.

\begin{figure}[t] 
\centering

\scalebox{0.95}[1]{\includegraphics[width=1\linewidth]{images/Fig_3_33_lineChart.pdf}} 
	\vspace{-0.6em}
\caption{Impacts of different query pair construction on training our model. We test the models with different percentages of OOD queries. \label{Fig_ablation_QC}}
\vspace{-1.2em}
\end{figure}

\noindent \textbf{Analysis of different query pair construction}
During training, we propose an OOD query pair construction that allows our network to grasp the idea of in-distribution and OOD residual features. 
We also compare three different strategies: randomly selecting feature pairs, dataset-based hard negative and positive feature pairs, and batch-based hard negative and positive feature pairs. All experiments are conducted on OUMVLP and we take GaitGL as our backbone.
As indicated in Figure \ref{Fig_ablation_QC}, both random selection of feature pairs and batch-based hard negative and positive feature pair mining strategies improve the recognition performance on OOD cases in testing.
Note that, as the percentage of the OOD queries increases, the performance of the model trained with random selection of feature pairs decreases. This is mainly because this pair construction may not effectively capture the most uncertain cases for model training.
The dataset-based mining strategy does not facilitate the representation of the distribution of the residual features. 
In most cases, the negative feature pairs from the dataset-based mining strategy are hard to be classified and thus our classifier will exert more efforts to classify the negative pairs after network training while sacrificing the classification accuracy on positive pairs. This is the reason its performance increases when the percentage of OOD samples increases.
Mining in a batch-based pair will significantly improve the variety of training samples, thus leading to better recognition performance.
Thanks to our feature pair construction, we can effectively capture the latent distribution of residual features, thus achieving accurate detection of OOD samples.

\section{Discussion and Limitation} \label{Discussion} 
We also conduct experiments on Gait3D \cite{zheng2022gait} which is a large-scale and in-the-wild dataset. The experimental results can be found in the supplementary material. Although our method still achieves a significant performance improvement in all OOD percentages, it suffers performance degradation without OOD. The main reason is that existing SOTA methods cannot effectively extract discriminative features on Gait3D (the accuracy of SOTA methods is lower than 50\%). When the extracted features are not sufficiently discriminative, in handling in-distribution queries, our method tends to be inaccurate. This is because, in such cases, the distance between a matching pair is large, leading to a higher risk. Meanwhile our network prefers to accept matching pairs with minimal risk.
To verify the applicability of our method to other tasks, we also conduct experiments on vehicle re-identification benchmark VERI-Wild \cite{lou2019large, lou2021large} and synthetic person-identification benchmark VC-Clothes \cite{wan2020person}. The experimental details and results can be found in the supplementary material. These experiments demonstrate our uncertainty-aware model is able to effectively recognize matching feature pairs and OOD queries through its prediction and uncertainty modeling, thus demonstrating the superiority of our method to the SOTA methods. 

% This is because our network is agnostic against backbones and trained by features extracted from existing backbones. If the discriminativeness of the extracted features is not good, our network is difficult to fit in-distribution cases. As a result, most in-distribution queries cannot correctly classify. 

% More details are referred to the supplementary files.

% Although our model is designed to tackle OOD queries in gait recognition, it can be easily extended to other ReID tasks, such as vehicle and person re-identification. The experimental results from the supplementary files demonstrate the superiority of our method to the competing method.

% \subsection{Extension to Other ReID Tasks} 
% Although our model is designed to tackle OOD queries in gait recognition, it can be easily extended to other tasks, such as vehicle and person re-identification. To further verify the effectiveness of our method on other tasks, we conduct experiments on other two datasets, i.e., VERI-Wild and VC-Clothes. BoT \cite{luo2019bag} for VERI-Wild and CBN \cite{zhuang2020rethinking} for VC-Clothes are chosen as our baseline algorithms, respectively.
% Table \ref{tab_person} and Table \ref{tab_vehicle} indicate that our uncertainty-aware model is able to effectively recognize matching feature pairs and OOD queries through its prediction and uncertainty modeling, thus demonstrating the superiority of our method to the competing method.

% \begin{figure}[t] 
% \begin{center}  
% 	\includegraphics[width=1.\linewidth]{images/Fig_3_1_lineChart.pdf} 
% 	\vspace{-1.5em}
% 	\caption{Comparisons of GaitGL, GaitGL with an NN classifier, and GaitGL with our model wrt. different percentages of OOD queries.}
% \label{Fig_ablation_OOD}
% \end{center}
% \vspace{-2em}
% \end{figure}

% \begin{figure}[t] 
% \begin{center}  
% 	\includegraphics[width=1.\linewidth]{images/Fig_3_33_lineChart.pdf} 
% 	\vspace{-1.5em}
% 	\caption{Impacts of different query pair construction on training our model. We test the models with different percentages of OOD queries.}
% \label{Fig_ablation_QC}
% \end{center}
% \vspace{-2em}
% \end{figure}

% \begin{table}[t]
% \footnotesize
%   \centering
%   \caption{Rank-1 accuracy (\%) on the VERI-Wild dataset under different OOD percentages.}
% %   \vspace{-0.5em}
%   \resizebox{0.99\linewidth}{!}{
%     \begin{tabular}{c|c|c|c|c|c}
%     \toprule
%      \multirow{2}[2]{*}{Methods} & \multicolumn{5}{c}{Different OOD percentages} \\
% \cmidrule{3-7}          &       & \multicolumn{1}{c|}{15\%} & \multicolumn{1}{c|}{25\%} & \multicolumn{1}{c|}{35\%} & \multicolumn{1}{c|}{45\%} & \multicolumn{1}{c}{55\%} \\
%     \midrule
% BoT   & 74.07  & 66.02  & 57.86  & 49.40  & 40.90  \\
% % \cmidrule{2-7}          & BoT + NN (verification protocol) & 69.26 ($\pm$ 0.33)  & 66.76 ($\pm$ 0.26) & 64.81 ($\pm$ 0.14) &  63.25 ($\pm$ 0.15) &  62.82 ($\pm$ 0.30)  \\
% % \cmidrule{2-7}          & BoT + Threshold (EER)  & 74.10 ($\pm$ 0.20)  & 66.36 ($\pm$ 0.28) & 58.99 ($\pm$ 0.11) &  52.01 ($\pm$ 0.01) &  46.24 ($\pm$ 0.35) \\
% BoT + Ours & \textbf{78.19} & \textbf{76.34} & \textbf{75.09} & \textbf{74.71} & \textbf{75.34} \\
%     \midrule
%     \midrule

%     \multirow{2}[2]{*}{MS} & BoT   & 71.00  & 63.64  & 55.83  & 47.73  & 39.87  \\
% % \cmidrule{2-7}          & BoT + NN (verification protocol) & 64.81 ($\pm$ 0.15)  & 61.65 ($\pm$ 0.34) & 59.33 ($\pm$ 0.17) &  57.17 ($\pm$ 0.40) &  55.84 ($\pm$ 0.42) \\
% % \cmidrule{2-7}          & BoT + Threshold (EER) & 70.85 ($\pm$ 0.16)  & 63.34 ($\pm$ 0.30) & 55.86 ($\pm$ 0.27) &  48.17 ($\pm$ 0.17) &  41.04 ($\pm$ 0.26)\\
% \cmidrule{2-7}          & BoT + Ours & \textbf{73.70 } & \textbf{70.85 } & \textbf{68.97} & \textbf{67.65} & \textbf{67.36} \\

% \midrule
% \midrule

%     \multirow{2}[2]{*}{LS} & BoT   & 65.58  & 58.65  & 51.54  & 44.15  & 36.82  \\
% % \cmidrule{2-7}          & BoT + NN (verification protocol) & 59.25 ($\pm$ 0.06)  & 56.28 ($\pm$ 0.20) & 53.39 ($\pm$ 0.04) &  50.99 ($\pm$ 0.36) &  49.15 ($\pm$ 0.39) \\
% % \cmidrule{2-7}          & BoT + Threshold (EER)  & 65.43 ($\pm$ 0.11)  & 58.63 ($\pm$ 0.06) & 51.57 ($\pm$ 0.02) &  44.40 ($\pm$ 0.16) &  37.14 ($\pm$ 0.15) \\
% \cmidrule{2-7}          & BoT + Ours & \textbf{67.45} & \textbf{64.14} & \textbf{61.25} & \textbf{59.07} & \textbf{57.92} \\
% \bottomrule
%     \end{tabular}%
%     }
%   \label{tab_vehicle}%
% \vspace{-1.0em}
% \end{table}%

% Table generated by Excel2LaTeX from sheet 'Sheet1'
% \begin{table}[t]
% \footnotesize
%   \centering
%   \caption{Rank-1 accuracy (\%) on the VERI-Wild dataset under different OOD percentages.}
%   \vspace{-0.5em}
%     \begin{tabular}{c|c|c|c|c|c}
%     \toprule
%     \multirow{2}[2]{*}{Methods} & \multicolumn{5}{c}{Different OOD percentages}  \\
% \cmidrule{2-6}          & \multicolumn{1}{c|}{15\%} & \multicolumn{1}{c|}{25\%} & \multicolumn{1}{c|}{35\%} & \multicolumn{1}{c|}{45\%} & \multicolumn{1}{c}{55\%}  \\
%     \midrule
%     \multirow{1}[1]{*}{BoT}  & 74.07  & 66.02  & 57.86  & 49.40  & 40.90  \\
%     \midrule
%     % \multirow{1}[1]{*}{CBN + Ours} & \textbf{97.11 ($\pm$ 0.23)} & \textbf{97.15 ($\pm$ 0.37)} & \textbf{97.29 ($\pm$ 0.33)} & \textbf{97.58 ($\pm$ 0.38)} & \textbf{97.70 ($\pm$ 0.15)}  \\
%     \multirow{1}[1]{*}{BoT + Ours} & \textbf{78.19} & \textbf{76.34} & \textbf{75.09} & \textbf{74.71} & \textbf{75.34}  \\
%     \bottomrule
%     \end{tabular}%

%   \label{tab_vehicle}%
%   \vspace{-1.0em}
% \end{table}%

% % Table generated by Excel2LaTeX from sheet 'Sheet1'
% \begin{table}[t]
% \footnotesize
%   \centering
%   \caption{Rank-1 accuracy (\%) on the VC-Clothes dataset under different OOD percentages.}
%   \vspace{-1.0em}
%     \begin{tabular}{c|c|c|c|c|c}
%     \toprule
%     \multirow{2}[2]{*}{Methods} & \multicolumn{5}{c}{Different OOD percentages}  \\
% \cmidrule{2-6}          & \multicolumn{1}{c|}{15\%} & \multicolumn{1}{c|}{25\%} & \multicolumn{1}{c|}{35\%} & \multicolumn{1}{c|}{45\%} & \multicolumn{1}{c}{55\%}  \\
%     \midrule
%     \multirow{1}[1]{*}{CBN}  & 83.91  & 73.67  & 64.10  & 54.34  & 44.98   \\
%     \midrule
%     % \multirow{1}[1]{*}{CBN + Ours} & \textbf{97.11 ($\pm$ 0.23)} & \textbf{97.15 ($\pm$ 0.37)} & \textbf{97.29 ($\pm$ 0.33)} & \textbf{97.58 ($\pm$ 0.38)} & \textbf{97.70 ($\pm$ 0.15)}  \\
%     \multirow{1}[1]{*}{CBN + Ours} & \textbf{97.11} & \textbf{97.15} & \textbf{97.29} & \textbf{97.58} & \textbf{97.70}  \\
%     \bottomrule
%     \end{tabular}%
%   \label{tab_person}%
%   \vspace{-2.0em}
% \end{table}%




\section{Conclusion}
% In this paper, we propose a novel uncertainty-aware gait recognition model that can effectively identify whether a probe sample is out of the distribution of the gallery samples. 
% To the best of our knowledge, our work is the first attempt to endow gait recognition with the capability to address OOD queries via uncertainty modeling.
% Furthermore, benefiting from the theory of evidential deep learning, our proposed method is a unified framework that can not only address OOD query samples but also successfully recognize the in-distribution samples.
% Since our method takes extracted gait features as input, it can be applied to various gait recognition networks, thus being agnostic against backbone networks.
% More importantly, our model can be generalized to other recognition tasks, such as vehicle and person re-identification, and consistently improves algorithmic robustness against OOD queries, which are frequently met in practice. 
% Extensive experiments demonstrate that our method significantly improves state-of-the-art performance on various tasks with OOD scenarios.

In this paper, we propose a novel uncertainty-aware gait recognition model that can effectively identify whether a probe sample is out of the distribution of the gallery samples. 
To the best of our knowledge, our work is the first attempt to endow gait recognition with the capability to address OOD queries via uncertainty modeling.
Furthermore, our proposed method is a unified framework that is not only able to address OOD query samples but also to successfully recognize the in-distribution samples.
Since our method takes extracted gait features as input, it can be applied to various gait recognition networks, and thus agnostic against backbone networks.
More importantly, our model can be generalized to other recognition tasks, such as vehicle and person re-identification, and improves the robustness to OOD queries, which frequently occur in practice.

\section*{Acknowledgment}
\noindent This work was supported by the Australian Research Council (DP220100800, DE230100477).



%%%%%%%%% REFERENCES
{\small
\bibliographystyle{ieee_fullname}
\bibliography{egbib}
}



\newpage
\newpage





\begin{table*}[t]
\footnotesize
% \sisetup{table-number-alignment=center}
  \centering
  \caption{Rank-1 accuracy (\%) on the Gait3D dataset under different OOD percentages. The standard deviation is shown in parentheses.}

%   \vspace{-1.0em}
% \renewcommand{\arraystretch}{0.5}
\setlength{\tabcolsep}{3.7mm}{
    \begin{tabular}{c|c|c|c|c|c|c}
    \toprule
    \multicolumn{1}{c|}{\multirow{2}[2]{*}{Methods}} & \multicolumn{5}{c}{Different OOD percentages} \\
    \cmidrule{2-7}& \multicolumn{1}{c|}{0\%} & \multicolumn{1}{c|}{15\%} & \multicolumn{1}{c|}{25\%} & \multicolumn{1}{c|}{35\%} & \multicolumn{1}{c|}{45\%} & \multicolumn{1}{c}{55\%} \\
    \midrule
    GaitGL   &  \textbf{42.52} &  37.64  & 34.73   & 31.38 & 27.24   &  22.74  \\
    % \midrule
    % GaitGL + Verification &    &    &   &    &    \\
    % \midrule
    % GaitGL + Threshold & 36.80 ($\pm$ 1.02)  & 34.68 ($\pm$ 0.85) & 32.61 ($\pm$ 1.53) & 30.36 ($\pm$ 1.35) & 28.42 ($\pm$ 0.78)   \\
    \midrule
    GaitGL + Ours  & 32.40 ($\pm$ 0.00)   & \textbf{39.92 ($\pm$ 1.21)}  & \textbf{43.74 ($\pm$ 1.64)} & \textbf{48.58 ($\pm$ 1.33)} & \textbf{53.71 ($\pm$ 0.54)} & \textbf{59.75 ($\pm$ 0.38)}\\
    \bottomrule
    \end{tabular}%
        }
    % \vspace{-1.0em}
  \label{tab_gait3d}%
\end{table*}%


% Table generated by Excel2LaTeX from sheet 'Sheet1'
\begin{table*}[t]
\footnotesize
% \sisetup{table-number-alignment=center}
  \centering
  \caption{Rank-1 accuracy (\%) on the OUMVLP dataset under different OOD percentages. The standard deviation is shown in parentheses. 
  For a fair comparison, we join the backbones with different OOD detection strategies, e.g., Navie CLS, Verification and Anomaly Detection.
% For a fair comparison, we combined the backbones with three OOD detection strategies, respectively.
  }
  \vspace{-0.8em}
%   \resizebox{0.80\textwidth}{!}{
% \renewcommand{\arraystretch}{0.5}
  \setlength{\tabcolsep}{2.6mm}{
% \resizebox{10mm}{12mm}
    \begin{tabular}{c|c|c|c|c|c|c}
    \toprule
    \multicolumn{1}{c|}{\multirow{2}[2]{*}{Methods}} & \multicolumn{6}{c}{Different OOD percentages} \\
    \cmidrule{2-7} & \multicolumn{1}{c|}{7\%} & \multicolumn{1}{c|}{15\%} & \multicolumn{1}{c|}{25\%} & \multicolumn{1}{c|}{35\%} & \multicolumn{1}{c|}{45\%} & \multicolumn{1}{c}{55\%} \\

    \midrule
    GaitGL & 90.17  & 82.15  & 72.35  & 63.45  & 53.58  & 43.68  \\
    \midrule
    GaitGL + Threshold=0.4 & 90.17 ($\pm$ 0.00) & 82.15 ($\pm$ 0.00)  & 72.35 ($\pm$ 0.00)  & 63.45 ($\pm$ 0.00) & 53.58 ($\pm$ 0.00) &  43.68 ($\pm$ 0.00) \\
    \midrule
    GaitGL + Threshold=0.5 & 90.20 ($\pm$ 0.00) & 82.22 ($\pm$ 0.02)  & 72.50 ($\pm$ 0.01)  & 63.66 ($\pm$ 0.02) & 53.90 ($\pm$ 0.01) &  44.21 ($\pm$ 0.02) \\
    \midrule   
    GaitGL + Threshold=0.6 & 90.41 ($\pm$ 0.00) & 84.74 ($\pm$ 0.01)  & 78.05 ($\pm$ 0.03)  & 72.30 ($\pm$ 0.03) & 66.46 ($\pm$ 0.12) &  61.23 ($\pm$ 0.12) \\
    \midrule   
    GaitGL + Threshold=0.7 & 73.08 ($\pm$ 0.00) & 74.21 ($\pm$ 0.02)  & 75.72 ($\pm$ 0.03)  & 77.24 ($\pm$ 0.04) & 79.21 ($\pm$ 0.07) &  81.35 ($\pm$ 0.06) \\
    \midrule   
    GaitGL + Threshold=0.8 & 27.34 ($\pm$ 0.00) & 33.87 ($\pm$ 0.02)  & 41.83 ($\pm$ 0.02)  & 49.07 ($\pm$ 0.00) & 57.09 ($\pm$ 0.03) &  65.07 ($\pm$ 0.05) \\
    \midrule   
    GaitGL + Threshold=0.9 & 9.42 ($\pm$ 0.00) & 17.56 ($\pm$ 0.01)  & 27.52 ($\pm$ 0.01)  & 36.57 ($\pm$ 0.02) & 46.56 ($\pm$ 0.01) &  56.51 ($\pm$ 0.01) \\
    \midrule       
    GaitGL + Navie CLS & \textbf{92.27 ($\pm$ 0.00)} & 88.81 ($\pm$ 0.01)  & 84.99 ($\pm$ 0.03)  & 82.02 ($\pm$ 0.05) &  79.51 ($\pm$ 0.05) &  77.87 ($\pm$ 0.08)  \\
    \midrule
    GaitGL + Verification & 90.47 ($\pm$ 0.00) & 83.06 ($\pm$ 0.00)  & 74.14 ($\pm$ 0.02)  & 66.19 ($\pm$ 0.02) & 57.67 ($\pm$ 0.06) &  49.52 ($\pm$ 0.07) \\
    \midrule
    GaitGL + Anomaly Detection & 74.07 ($\pm$ 0.00) & 75.63 ($\pm$ 0.02)  & 77.58 ($\pm$ 0.04)  & 79.42 ($\pm$ 0.03) & 81.57 ($\pm$ 0.02) &  83.79 ($\pm$ 0.02) \\
    \midrule
    GaitGL + Ours & 91.52 ($\pm$ 0.00)  & \textbf{90.27 ($\pm$ 0.01)}  & \textbf{89.03 ($\pm$ 0.03)} & \textbf{88.21 ($\pm$ 0.04)} & \textbf{87.82 ($\pm$ 0.02)} & \textbf{87.87 ($\pm$ 0.05)} \\
    \bottomrule
    \end{tabular}%
    }
    \vspace{-1.2em}
  \label{tab_threshold}%
\end{table*}%



\begin{table*}[t]
\vspace{-13cm}
\footnotesize
  \centering
  \caption{Rank-1 accuracy (\%) on the VERI-Wild dataset under different OOD percentages. The standard deviation is reported in parentheses.}
%   \vspace{-0.5em}
% \renewcommand{\arraystretch}{0.5}
\setlength{\tabcolsep}{3.8mm}{
    \begin{tabular}{c|c|c|c|c|c|c}
    \toprule
    \multirow{2}[2]{*}{Setting} & \multirow{2}[2]{*}{Methods} & \multicolumn{5}{c}{Different OOD percentages} \\
\cmidrule{3-7}          &       & \multicolumn{1}{c|}{15\%} & \multicolumn{1}{c|}{25\%} & \multicolumn{1}{c|}{35\%} & \multicolumn{1}{c|}{45\%} & \multicolumn{1}{c}{55\%} \\
    \midrule
\multirow{2}[2]{*}{SS (\#ID=3k)} & BoT   & 74.07  & 66.02  & 57.86  & 49.40  & 40.90  \\
% \cmidrule{2-7}          & BoT + NN (verification protocol) & 69.26 ($\pm$ 0.33)  & 66.76 ($\pm$ 0.26) & 64.81 ($\pm$ 0.14) &  63.25 ($\pm$ 0.15) &  62.82 ($\pm$ 0.30)  \\
% \cmidrule{2-7}          & BoT + Threshold (EER)  & 74.10 ($\pm$ 0.20)  & 66.36 ($\pm$ 0.28) & 58.99 ($\pm$ 0.11) &  52.01 ($\pm$ 0.01) &  46.24 ($\pm$ 0.35) \\
\cmidrule{2-7}          & BoT + Ours & \textbf{78.19 ($\pm$ 0.21)} & \textbf{76.34 ($\pm$ 0.20)} & \textbf{75.09 ($\pm$ 0.05)} & \textbf{74.71 ($\pm$ 0.21)} & \textbf{75.34 ($\pm$ 0.25)} \\
    \midrule
    \midrule

    \multirow{2}[2]{*}{MS (\#ID=5k)} & BoT   & 71.00  & 63.64  & 55.83  & 47.73  & 39.87  \\
% \cmidrule{2-7}          & BoT + NN (verification protocol) & 64.81 ($\pm$ 0.15)  & 61.65 ($\pm$ 0.34) & 59.33 ($\pm$ 0.17) &  57.17 ($\pm$ 0.40) &  55.84 ($\pm$ 0.42) \\
% \cmidrule{2-7}          & BoT + Threshold (EER) & 70.85 ($\pm$ 0.16)  & 63.34 ($\pm$ 0.30) & 55.86 ($\pm$ 0.27) &  48.17 ($\pm$ 0.17) &  41.04 ($\pm$ 0.26)\\
\cmidrule{2-7}          & BoT + Ours & \textbf{73.70 ($\pm$ 0.22)} & \textbf{70.85 ($\pm$ 0.43)} & \textbf{68.97 ($\pm$ 0.20)} & \textbf{67.65 ($\pm$ 0.21)} & \textbf{67.36 ($\pm$ 0.23)} \\

\midrule
\midrule

    \multirow{2}[2]{*}{LS (\#ID=10k)} & BoT   & 65.58  & 58.65  & 51.54  & 44.15  & 36.82  \\
% \cmidrule{2-7}          & BoT + NN (verification protocol) & 59.25 ($\pm$ 0.06)  & 56.28 ($\pm$ 0.20) & 53.39 ($\pm$ 0.04) &  50.99 ($\pm$ 0.36) &  49.15 ($\pm$ 0.39) \\
% \cmidrule{2-7}          & BoT + Threshold (EER)  & 65.43 ($\pm$ 0.11)  & 58.63 ($\pm$ 0.06) & 51.57 ($\pm$ 0.02) &  44.40 ($\pm$ 0.16) &  37.14 ($\pm$ 0.15) \\
\cmidrule{2-7}          & BoT + Ours & \textbf{67.45 ($\pm$ 0.06)} & \textbf{64.14 ($\pm$ 0.22)} & \textbf{61.25 ($\pm$ 0.16)} & \textbf{59.07 ($\pm$ 0.48)} & \textbf{57.92 ($\pm$ 0.54)} \\
\bottomrule
    \end{tabular}%
    }
  \label{tab_vehicle}%
\vspace{-1.0em}
\end{table*}%


% Table generated by Excel2LaTeX from sheet 'Sheet1'
\begin{table*}[t]
\vspace{-27cm}
\footnotesize
  \centering
  \caption{Rank-1 accuracy (\%) on the VC-Clothes dataset under different OOD percentages. The standard deviation is reported in parentheses.}
%   \vspace{-0.5em}
% \renewcommand{\arraystretch}{0.5}
\setlength{\tabcolsep}{5.7mm}{
    \begin{tabular}{c|c|c|c|c|c}
    \toprule
    \multirow{2}[2]{*}{Methods} & \multicolumn{5}{c}{Different OOD percentages}  \\
\cmidrule{2-6}          & \multicolumn{1}{c|}{15\%} & \multicolumn{1}{c|}{25\%} & \multicolumn{1}{c|}{35\%} & \multicolumn{1}{c|}{45\%} & \multicolumn{1}{c}{55\%}  \\
    \midrule
    \multirow{1}[1]{*}{CBN}  & 83.91  & 73.67  & 64.10  & 54.34  & 44.98   \\
    \midrule
    % \multirow{1}[1]{*}{CBN + Verification}  & 89.68 ($\pm$ 0.10) & 90.71 ($\pm$ 0.17) & 91.72 ($\pm$ 0.40) & 92.90 ($\pm$ 0.43) & 93.95 ($\pm$ 0.14)   \\
    % \midrule
    % \multirow{1}[1]{*}{CBN + Threshold}  & 87.96 ($\pm$ 0.09) & 80.93 ($\pm$ 0.22) & 75.74 ($\pm$ 0.29) & 72.00 ($\pm$ 0.56) & 69.17 ($\pm$ 0.82)    \\
    % \midrule
    \multirow{1}[1]{*}{CBN + Ours} & \textbf{97.11 ($\pm$ 0.23)} & \textbf{97.15 ($\pm$ 0.37)} & \textbf{97.29 ($\pm$ 0.33)} & \textbf{97.58 ($\pm$ 0.38)} & \textbf{97.70 ($\pm$ 0.15)}  \\
    \bottomrule
    \end{tabular}%
  }
  \label{tab_person}%
%   \vspace{-1.5em}
\end{table*}%



\section{Appendix}

In this section, we evaluate the effectiveness of our method on the real-world gait dataset {Gait3D} \cite{zheng2022gait}. Moreover, to verify the applicability of our method to other tasks, we also conduct experiments on other similar tasks, such as vehicle re-identification \cite{lou2019large, lou2021large} and synthetic person-identification benchmark VC-Clothes \cite{wan2020person}. The details of these datasets are depicted as follows.

\subsection{Dataset}

\noindent \textbf{Gait3D} \cite{zheng2022gait} is a large-scale and in-the-wild dataset. It includes 4K subjects and 25K sequences captured by 39 surveillance cameras in the wild. In our experiments, the Gait3D dataset is divided into three subsets: a training set, a query set, and a gallery set. To be specific, 3,000 IDs\footnote{Here, ID refers to identity. With a slight abuse of notation, we refer ID to as in-distribution for simplicity.} are used to construct the training set and the remaining 1,000 IDs are used as the query and gallery sets. We select one image from each ID as the gallery set and the rest images as the query set. By removing IDs from the gallery set, we construct various OOD query cases.

\noindent \textbf{VERI-Wild} \cite{lou2019large, lou2021large} is a large-scale dataset used for vehicle re-identification tasks.
It includes 416,314 images captured by 174 surveillance cameras in the wild. In general, the VERI-Wild dataset is divided into three subsets: a training set, a query set, and a gallery set. 
We use the evaluation protocol as in \cite{he2020fastreid}, which includes three different settings, \emph{i.e.}, Small-size Setting (SS), Medium-size Setting (MS) and Large-size Setting (LS). 
In the three settings, 3,000 IDs, 5,000 IDs and 10,000 IDs are used to construct the query and gallery sets, respectively. In each setting, we select one image from each ID as the gallery set and the rest images as the probe set. 
By removing IDs from the gallery set or probe set, we construct various OOD query cases.


\noindent \textbf{VC-Clothes}\cite{wan2020person} is a synthetic person re-identification dataset. It includes 512 subjects and each subject has 36 corresponding images. 
The dataset has been divided into two subsets: a training set and a testing set. Each set contains 256 IDs. 
In the inference stage, we randomly choose four images from each subject as the gallery set and the rest images are used as the probe set. Hence, the gallery and probe sets contain 1020 images and 8591 images, respectively.

\subsection{Implementation Details}
For the Gait3D dataset, we take GaitGL \cite{lin2021gait} as the backbones to extract gait features and evaluate the effectiveness of our proposed uncertainty-aware classification model. We set the hidden layer size of our MLP to 5000. 
For the VERI-Wild dataset, BoT \cite{luo2019bag} is used as the backbone and thus the hidden layer size of MLP of the uncertainty-aware classification model is set to 1024. For the VC-clothes dataset, we take CBN \cite{zhuang2020rethinking} as the backbone and the hidden layer size of MLP is set to 4096.
The hyper-parameters $P$ and $K$ of the proposed sampling strategy are set to 32 and 8 respectively. The learning rate is set to 1e-3 and then decays by a factor of 0.1 every 10 iterations. The total training iteration number for all the experiments is set to 50. 
In our experiments, we also run each OOD setting by three times in order to demonstrate the robustness of our method against OOD queries.

\subsection{Comparisons with the State-of-the-art}

\noindent \textbf{Evaluation on Gait3D under different OOD percentages}
For the Gait3D dataset, we choose GaitGL \cite{lin2021gait} as our baseline backbone. Table. \ref{tab_gait3d} shows the experimental results. It can be observed that our method achieves a significant performance improvement in all cases. For instance, in the 55\% OOD percentage, our method outperforms GaitGL by 37.01\%. Although our method still achieves a significant performance improvement in all OOD percentages, it suffers performance degradation without OOD. The main reason is that existing SOTA methods cannot effectively extract discriminative features on Gait3D (the accuracy of SOTA methods is lower than 50\%). When the extracted features are not sufficiently discriminative, in handling in-distribution queries, our method tends to be inaccurate. This is because, in such cases, the distance between a matching pair is large, leading to a higher risk. Meanwhile our network prefers to accept matching pairs with minimal risk.

\vspace{1mm}
\noindent \textbf{Evaluation on VERI-Wild and VC-Clothes under different OOD percentages}

Although our model is designed to tackle OOD queries in gait recognition, it can be easily extended to other tasks, such as vehicle re-identification and person re-identification. 
To further verify the effectiveness of our method on other tasks, we conduct experiments on VERI-Wild and VC-Clothes.

For the VERI-Wild dataset, we choose the BoT framework \cite{luo2019bag} as our baseline algorithm.
The experimental results are shown in Table. \ref{tab_vehicle}. It can be observed that our method (BoT + Ours) achieves appealing performance in all cases. To be specific, our method achieves stable recognition accuracy for the five OOD percentages while outperforming the baseline method. In contrast, the accuracy of BoT without our uncertainty model decreases significantly as the percentage of OOD samples increases.
For the VC-Clothes dataset, we employ the CBN framework \cite{zhuang2020rethinking} as our baseline algorithm. As indicated in Table. \ref{tab_person}, our methods achieve approximately more than 97\% recognition accuracy in all the cases, \ie, OOD percentage ranges from 15\% to 55\%. On the contrary, the recognition accuracy of CBN without our uncertainty model is 44.98\% in the setting of 55\% OOD samples. 
This is because the current methods cannot address OOD queries and thus their methods only output erroneous recognition results. Note that, when we apply our method to the baseline methods, we do not need to re-train the baselines but treat them as off-the-shelf backbone networks. This also indicates that our uncertainty-aware model is able to effectively recognize matching feature pairs and OOD queries through its prediction and uncertainty modeling, thus demonstrating the superiority of our method to competing methods.

\subsection{Ablation Study} \label{Ablation_Study} 

For a fair comparison, we introduce different OOD detection strategies. One of our used strategies is to set a similarity threshold. If the similarity between the probe and the closet sample from the gallery is smaller than the threshold, the probe will be regarded as an OOD query. In this paper, we introduce a verification setting to automatically calculate the threshold. Obviously, the threshold can be set manually. Thus, we conduct experiments on OUMVLP by manually searching all thresholds. The experimental results are shown in Table \ref{tab_threshold}. When the given threshold is lower than 0.5, the threshold-based method cannot detect any OOD cases. Thus, the accuracy in these threshold settings is the same as the accuracy using only the backbone. We also observe that a small threshold achieves appealing performance when the OOD percentage is less than 25\%, but it cannot address the condition with higher OOD percentages. In contrast, a high threshold achieves promising performance in high OOD percentages ($\geq$ 35\%). These results indicate that it is difficult to choose a suitable threshold for different OOD percentages. Thus, the threshold-based methods are impractical. Furthermore, it can be found that our method outperforms the threshold-based methods in all OOD percentages and the recognition accuracy in all OOD percentages is stable. We believe that the proposed method is a more practical approach to addressing OOD queries in a real scene.


\end{document}


