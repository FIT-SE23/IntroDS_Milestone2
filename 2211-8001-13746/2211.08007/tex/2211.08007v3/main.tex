
%% bare_jrnl_compsoc.tex
%% V1.4b
%% 2015/08/26
%% by Michael Shell
%% See:
%% http://www.michaelshell.org/
%% for current contact information.
%%
%% This is a skeleton file demonstrating the use of IEEEtran.cls
%% (requires IEEEtran.cls version 1.8b or later) with an IEEE
%% Computer Society journal paper.
%%
%% Support sites:
%% http://www.michaelshell.org/tex/ieeetran/
%% http://www.ctan.org/pkg/ieeetran
%% and
%% http://www.ieee.org/
 
%%*************************************************************************
%% Legal Notice:
%% This code is offered as-is without any warranty either expressed or
%% implied; without even the implied warranty of MERCHANTABILITY or
%% FITNESS FOR A PARTICULAR PURPOSE! 
%% User assumes all risk.
%% In no event shall the IEEE or any contributor to this code be liable for
%% any damages or losses, including, but not limited to, incidental,
%% consequential, or any other damages, resulting from the use or misuse
%% of any information contained here.
%%
%% All comments are the opinions of their respective authors and are not
%% necessarily endorsed by the IEEE.
%%
%% This work is distributed under the LaTeX Project Public License (LPPL)
%% ( http://www.latex-project.org/ ) version 1.3, and may be freely used,
%% distributed and modified. A copy of the LPPL, version 1.3, is included
%% in the base LaTeX documentation of all distributions of LaTeX released
%% 2003/12/01 or later.
%% Retain all contribution notices and credits.
%% ** Modified files should be clearly indicated as such, including  **
%% ** renaming them and changing author support contact information. **
%%*************************************************************************


% *** Authors should verify (and, if needed, correct) their LaTeX system  ***
% *** with the testflow diagnostic prior to trusting their LaTeX platform ***
% *** with production work. The IEEE's font choices and paper sizes can   ***
% *** trigger bugs that do not appear when using other class files.       ***                          ***
% The testflow support page is at:
% http://www.michaelshell.org/tex/testflow/


\documentclass[10pt,journal,compsoc]{IEEEtran}
%
% If IEEEtran.cls has not been installed into the LaTeX system files,
% manually specify the path to it like:
% \documentclass[10pt,journal,compsoc]{../sty/IEEEtran}





% Some very useful LaTeX packages include:
% (uncomment the ones you want to load)


% *** MISC UTILITY PACKAGES ***
%
%\usepackage{ifpdf}
% Heiko Oberdiek's ifpdf.sty is very useful if you need conditional
% compilation based on whether the output is pdf or dvi.
% usage:
% \ifpdf
%   % pdf code
% \else
%   % dvi code
% \fi
% The latest version of ifpdf.sty can be obtained from:
% http://www.ctan.org/pkg/ifpdf
% Also, note that IEEEtran.cls V1.7 and later provides a builtin
% \ifCLASSINFOpdf conditional that works the same way.
% When switching from latex to pdflatex and vice-versa, the compiler may
% have to be run twice to clear warning/error messages.






% *** CITATION PACKAGES ***
%
\ifCLASSOPTIONcompsoc
  % IEEE Computer Society needs nocompress option
  % requires cite.sty v4.0 or later (November 2003)
  \usepackage[nocompress]{cite}
\else
  % normal IEEE
  \usepackage{cite}
\fi
% cite.sty was written by Donald Arseneau
% V1.6 and later of IEEEtran pre-defines the format of the cite.sty package
%~\cite{} output to follow that of the IEEE. Loading the cite package will
% result in citation numbers being automatically sorted and properly
% "compressed/ranged". e.g., [1], [9], [2], [7], [5], [6] without using
% cite.sty will become [1], [2], [5]--[7], [9] using cite.sty. cite.sty's
%~\cite will automatically add leading space, if needed. Use cite.sty's
% noadjust option (cite.sty V3.8 and later) if you want to turn this off
% such as if a citation ever needs to be enclosed in parenthesis.
% cite.sty is already installed on most LaTeX systems. Be sure and use
% version 5.0 (2009-03-20) and later if using hyperref.sty.
% The latest version can be obtained at:
% http://www.ctan.org/pkg/cite
% The documentation is contained in the cite.sty file itself.
%
% Note that some packages require special options to format as the Computer
% Society requires. In particular, Computer Society  papers do not use
% compressed citation ranges as is done in typical IEEE papers
% (e.g., [1]-[4]). Instead, they list every citation separately in order
% (e.g., [1], [2], [3], [4]). To get the latter we need to load the cite
% package with the nocompress option which is supported by cite.sty v4.0
% and later. Note also the use of a CLASSOPTION conditional provided by
% IEEEtran.cls V1.7 and later.





% *** GRAPHICS RELATED PACKAGES ***
%
\ifCLASSINFOpdf
  % \usepackage[pdftex]{graphicx}
  % declare the path(s) where your graphic files are
  % \graphicspath{{../pdf/}{../jpeg/}}
  % and their extensions so you won't have to specify these with
  % every instance of \includegraphics
  % \DeclareGraphicsExtensions{.pdf,.jpeg,.png}
\else
  % or other class option (dvipsone, dvipdf, if not using dvips). graphicx
  % will default to the driver specified in the system graphics.cfg if no
  % driver is specified.
  % \usepackage[dvips]{graphicx}
  % declare the path(s) where your graphic files are
  % \graphicspath{{../eps/}}
  % and their extensions so you won't have to specify these with
  % every instance of \includegraphics
  % \DeclareGraphicsExtensions{.eps}
\fi
% graphicx was written by David Carlisle and Sebastian Rahtz. It is
% required if you want graphics, photos, etc. graphicx.sty is already
% installed on most LaTeX systems. The latest version and documentation
% can be obtained at: 
% http://www.ctan.org/pkg/graphicx
% Another good source of documentation is "Using Imported Graphics in
% LaTeX2e" by Keith Reckdahl which can be found at:
% http://www.ctan.org/pkg/epslatex
%
% latex, and pdflatex in dvi mode, support graphics in encapsulated
% postscript (.eps) format. pdflatex in pdf mode supports graphics
% in .pdf, .jpeg, .png and .mps (metapost) formats. Users should ensure
% that all non-photo figures use a vector format (.eps, .pdf, .mps) and
% not a bitmapped formats (.jpeg, .png). The IEEE frowns on bitmapped formats
% which can result in "jaggedy"/blurry rendering of lines and letters as
% well as large increases in file sizes.
%
% You can find documentation about the pdfTeX application at:
% http://www.tug.org/applications/pdftex






% *** MATH PACKAGES ***
%
%\usepackage{amsmath}
% A popular package from the American Mathematical Society that provides
% many useful and powerful commands for dealing with mathematics.
%
% Note that the amsmath package sets \interdisplaylinepenalty to 10000
% thus preventing page breaks from occurring within multiline equations. Use:
%\interdisplaylinepenalty=2500
% after loading amsmath to restore such page breaks as IEEEtran.cls normally
% does. amsmath.sty is already installed on most LaTeX systems. The latest
% version and documentation can be obtained at:
% http://www.ctan.org/pkg/amsmath





% *** SPECIALIZED LIST PACKAGES ***
%
%\usepackage{algorithmic}
% algorithmic.sty was written by Peter Williams and Rogerio Brito.
% This package provides an algorithmic environment fo describing algorithms.
% You can use the algorithmic environment in-text or within a figure
% environment to provide for a floating algorithm. Do NOT use the algorithm
% floating environment provided by algorithm.sty (by the same authors) or
% algorithm2e.sty (by Christophe Fiorio) as the IEEE does not use dedicated
% algorithm float types and packages that provide these will not provide
% correct IEEE style captions. The latest version and documentation of
% algorithmic.sty can be obtained at:
% http://www.ctan.org/pkg/algorithms
% Also of interest may be the (relatively newer and more customizable)
% algorithmicx.sty package by Szasz Janos:
% http://www.ctan.org/pkg/algorithmicx




% *** ALIGNMENT PACKAGES ***
%
%\usepackage{array}
% Frank Mittelbach's and David Carlisle's array.sty patches and improves
% the standard LaTeX2e array and tabular environments to provide better
% appearance and additional user controls. As the default LaTeX2e table
% generation code is lacking to the point of almost being broken with
% respect to the quality of the end results, all users are strongly
% advised to use an enhanced (at the very least that provided by array.sty)
% set of table tools. array.sty is already installed on most systems. The
% latest version and documentation can be obtained at:
% http://www.ctan.org/pkg/array


% IEEEtran contains the IEEEeqnarray family of commands that can be used to
% generate multiline equations as well as matrices, tables, etc., of high
% quality.




% *** SUBFIGURE PACKAGES ***
%\ifCLASSOPTIONcompsoc
%  \usepackage[caption=false,font=footnotesize,labelfont=sf,textfont=sf]{subfig}
%\else
%  \usepackage[caption=false,font=footnotesize]{subfig}
%\fi
% subfig.sty, written by Steven Douglas Cochran, is the modern replacement
% for subfigure.sty, the latter of which is no longer maintained and is
% incompatible with some LaTeX packages including fixltx2e. However,
% subfig.sty requires and automatically loads Axel Sommerfeldt's caption.sty
% which will override IEEEtran.cls' handling of captions and this will result
% in non-IEEE style figure/table captions. To prevent this problem, be sure
% and invoke subfig.sty's "caption=false" package option (available since
% subfig.sty version 1.3, 2005/06/28) as this is will preserve IEEEtran.cls
% handling of captions.
% Note that the Computer Society format requires a sans serif font rather
% than the serif font used in traditional IEEE formatting and thus the need
% to invoke different subfig.sty package options depending on whether
% compsoc mode has been enabled.
%
% The latest version and documentation of subfig.sty can be obtained at:
% http://www.ctan.org/pkg/subfig




% *** FLOAT PACKAGES ***
%
%\usepackage{fixltx2e}
% fixltx2e, the successor to the earlier fix2col.sty, was written by
% Frank Mittelbach and David Carlisle. This package corrects a few problems
% in the LaTeX2e kernel, the most notable of which is that in current
% LaTeX2e releases, the ordering of single and double column floats is not
% guaranteed to be preserved. Thus, an unpatched LaTeX2e can allow a
% single column figure to be placed prior to an earlier double column
% figure.
% Be aware that LaTeX2e kernels dated 2015 and later have fixltx2e.sty's
% corrections already built into the system in which case a warning will
% be issued if an attempt is made to load fixltx2e.sty as it is no longer
% needed.
% The latest version and documentation can be found at:
% http://www.ctan.org/pkg/fixltx2e


%\usepackage{stfloats}
% stfloats.sty was written by Sigitas Tolusis. This package gives LaTeX2e
% the ability to do double column floats at the bottom of the page as well
% as the top. (e.g., "\begin{figure*}[!b]" is not normally possible in
% LaTeX2e). It also provides a command:
%\fnbelowfloat
% to enable the placement of footnotes below bottom floats (the standard
% LaTeX2e kernel puts them above bottom floats). This is an invasive package
% which rewrites many portions of the LaTeX2e float routines. It may not work
% with other packages that modify the LaTeX2e float routines. The latest
% version and documentation can be obtained at:
% http://www.ctan.org/pkg/stfloats
% Do not use the stfloats baselinefloat ability as the IEEE does not allow
% \baselineskip to stretch. Authors submitting work to the IEEE should note
% that the IEEE rarely uses double column equations and that authors should try
% to avoid such use. Do not be tempted to use the cuted.sty or midfloat.sty
% packages (also by Sigitas Tolusis) as the IEEE does not format its papers in
% such ways.
% Do not attempt to use stfloats with fixltx2e as they are incompatible.
% Instead, use Morten Hogholm'a dblfloatfix which combines the features
% of both fixltx2e and stfloats:
%
% \usepackage{dblfloatfix}
% The latest version can be found at:
% http://www.ctan.org/pkg/dblfloatfix




%\ifCLASSOPTIONcaptionsoff
%  \usepackage[nomarkers]{endfloat}
% \let\MYoriglatexcaption\caption
% \renewcommand{\caption}[2][\relax]{\MYoriglatexcaption[#2]{#2}}
%\fi
% endfloat.sty was written by James Darrell McCauley, Jeff Goldberg and 
% Axel Sommerfeldt. This package may be useful when used in conjunction with 
% IEEEtran.cls'  captionsoff option. Some IEEE journals/societies require that
% submissions have lists of figures/tables at the end of the paper and that
% figures/tables without any captions are placed on a page by themselves at
% the end of the document. If needed, the draftcls IEEEtran class option or
% \CLASSINPUTbaselinestretch interface can be used to increase the line
% spacing as well. Be sure and use the nomarkers option of endfloat to
% prevent endfloat from "marking" where the figures would have been placed
% in the text. The two hack lines of code above are a slight modification of
% that suggested by in the endfloat docs (section 8.4.1) to ensure that
% the full captions always appear in the list of figures/tables - even if
% the user used the short optional argument of \caption[]{}.
% IEEE papers do not typically make use of \caption[]'s optional argument,
% so this should not be an issue. A similar trick can be used to disable
% captions of packages such as subfig.sty that lack options to turn off
% the subcaptions:
% For subfig.sty:
% \let\MYorigsubfloat\subfloat
% \renewcommand{\subfloat}[2][\relax]{\MYorigsubfloat[]{#2}}
% However, the above trick will not work if both optional arguments of
% the \subfloat command are used. Furthermore, there needs to be a
% description of each subfigure *somewhere* and endfloat does not add
% subfigure captions to its list of figures. Thus, the best approach is to
% avoid the use of subfigure captions (many IEEE journals avoid them anyway)
% and instead reference/explain all the subfigures within the main caption.
% The latest version of endfloat.sty and its documentation can obtained at:
% http://www.ctan.org/pkg/endfloat
%
% The IEEEtran \ifCLASSOPTIONcaptionsoff conditional can also be used
% later in the document, say, to conditionally put the References on a 
% page by themselves.




% *** PDF, URL AND HYPERLINK PACKAGES ***
%
%\usepackage{url}
% url.sty was written by Donald Arseneau. It provides better support for
% handling and breaking URLs. url.sty is already installed on most LaTeX
% systems. The latest version and documentation can be obtained at:
% http://www.ctan.org/pkg/url
% Basically, \url{my_url_here}.





% *** Do not adjust lengths that control margins, column widths, etc. ***
% *** Do not use packages that alter fonts (such as pslatex).         ***
% There should be no need to do such things with IEEEtran.cls V1.6 and later.
% (Unless specifically asked to do so by the journal or conference you plan
% to submit to, of course. )

% 

% correct bad hyphenation here
\hyphenation{op-tical net-works semi-conduc-tor}

\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{xcolor}
\usepackage{multirow}
\usepackage{subfigure}
\usepackage{booktabs}
\usepackage{cite}
\usepackage{bm}
% \usepackage{color}
\usepackage{color, colortbl}
% \def\ie{\emph{i.e}\onedot} 
% \def\Ie{\emph{I.e}\onedot}
% \def\eg{\emph{e.g}\onedot} 
% \def\Ie{\emph{E.g}\onedot}
% \def\ie{\emph{i.e.}} 
% \def\Ie{\emph{I.e.}}
% \def\eg{\emph{e.g.}} 
% \def\Ie{\emph{E.g.}}
\usepackage{xspace}

% Add a period to the end of an abbreviation unless there's one
% already, then \xspace.
\makeatletter
\DeclareRobustCommand\onedot{\futurelet\@let@token\@onedot}
\def\@onedot{\ifx\@let@token.\else.\null\fi\xspace}

\def\eg{\emph{e.g}\onedot} \def\Eg{\emph{E.g}\onedot}
\def\ie{\emph{i.e}\onedot} \def\Ie{\emph{I.e}\onedot}
\def\cf{\emph{c.f}\onedot} \def\Cf{\emph{C.f}\onedot}
\def\etc{\emph{etc}\onedot} \def\vs{\emph{vs}\onedot}
\def\wrt{w.r.t\onedot} \def\dof{d.o.f\onedot}
\def\etal{\emph{et al}\onedot}
\makeatother

\newcommand{\XY}[1]{\textcolor{red}{[XY:#1]}}
\newcommand{\BB}[1]{\textcolor{red}{[BB:#1]}}
% \\def\BB#1{{\color{blue}{\bf [BB:} {\it{#1}}{\bf ]}}}
% \def\WM#1{{\color{mygreen}{\bf [WM:} {\it{#1}}{\bf ]}}}

\definecolor{myred}{RGB}{234,67,53}
\definecolor{myblue}{RGB}{66,133,244}
\usepackage{hyperref}

\begin{document}
%
% paper title
% Titles are generally capitalized except for words such as a, an, and, as,
% at, but, by, for, in, nor, of, on, or, the, to and up, which are usually
% not capitalized unless they are the first or last word of the title.
% Linebreaks \\ can be used within to get better formatting as desired.
% Do not put math or special symbols in the title.
\title{
% Uncertainty-aware Gait Recognition via Learning from Dirichlet Distribution-based Evidence
Evidence-based Match-status-Aware Gait Recognition for Out-of-Gallery Gait Identification
}

% evidence-based uncertainty Out-of-Target, Non-matching
%
%
% author names and IEEE memberships
% note positions of commas and nonbreaking spaces ( ~ ) LaTeX will not break
% a structure at a ~ so this keeps an author's name from being broken across
% two lines.
% use \thanks{} to gain access to the first footnote area
% a separate \thanks must be used for each paragraph as LaTeX2e's \thanks
% was not built to handle multiple paragraphs
%
%
%\IEEEcompsocitemizethanks is a special \thanks that produces the bulleted
% lists the Computer Society journals use for "first footnote" author
% affiliations. Use \IEEEcompsocthanksitem which works much like \item
% for each affiliation group. When not in compsoc mode,
% \IEEEcompsocitemizethanks becomes like \thanks and
% \IEEEcompsocthanksitem becomes a line break with idention. This
% facilitates dual compilation, although admittedly the differences in the
% desired content of \author between the different types of papers makes a
% one-size-fits-all approach a daunting prospect. For instance, compsoc 
% journal papers have the author affiliations above the "Manuscript
% received ..."  text while in non-compsoc journals this is reversed. Sigh.
\author{Heming Du\textsuperscript{$\star$},
        Chen Liu\textsuperscript{$\star$},
        Ming Wang,
        Lincheng Li,
        Shunli Zhang\textsuperscript{$\dagger$},
        and~Xin~Yu\textsuperscript{$\dagger$}% <-this % stops a space
% \author{Michael~Shell,~\IEEEmembership{Member,~IEEE,}
%         John~Doe,~\IEEEmembership{Fellow,~OSA,}
%         and~Jane~Doe,~\IEEEmembership{Life~Fellow,~IEEE}% <-this % stops a space


% \author{Beibei Lin\textsuperscript{1}, Chen Liu\textsuperscript{2}, Lincheng Li\textsuperscript{3}, Robby T. Tan\textsuperscript{1,4}~and Xin Yu\textsuperscript{2} \\
% \textsuperscript{1} National University of Singapore,
% \textsuperscript{2} University of Technology Sydney\\
% \textsuperscript{3} Netease Fuxi AI Lab,
% \textsuperscript{4} Yale-NUS College\\ 
% {\tt\small beibei.lin@u.nus.edu, Chen.Liu-4@student.uts.edu.au, lilincheng@corp.netease.com,}\\ {\tt\small robby.tan@nus.edu.sg, xin.yu@uts.edu.au}
% }

\IEEEcompsocitemizethanks{

\IEEEcompsocthanksitem Heming Du, Chen Liu, and Xin Yu were with the University of Queensland, Australia.
\IEEEcompsocthanksitem Lincheng Li was with the Fuxi Lab, NetEase, Hangzhou, China.
\IEEEcompsocthanksitem Ming Wang and Shunli Zhang were with the School of Software Engineering, Beijing Jiaotong University, Beijing, China.
\IEEEcompsocthanksitem \textsuperscript{$\star$} Joint first authors
\IEEEcompsocthanksitem \textsuperscript{$\dagger$} Xin~Yu is the corresponding author. E-mail: xin.yu@uq.edu.au
% \IEEEcompsocthanksitem Shunli Zhang (slzhang@bjtu.edu.cn) is the corresponding author.
}
}
% \IEEEcompsocitemizethanks{\IEEEcompsocthanksitem M. Shell was with the Department
% of Electrical and Computer Engineering, Georgia Institute of Technology, Atlanta,
% GA, 30332.\protect\\
% % note need leading \protect in front of \\ to get a newline within \thanks as
% % \\ is fragile and will error, could use \hfil\break instead.
% E-mail: see http://www.michaelshell.org/contact.html
% \IEEEcompsocthanksitem J. Doe and J. Doe are with Anonymous University.}% <-this % stops an unwanted space
% % \thanks{Manuscript received April 19, 2005; revised August 26, 2015.}
%} 

% note the % following the last \IEEEmembership and also \thanks - 
% these prevent an unwanted space from occurring between the last author name
% and the end of the author line. i.e., if you had this:
% 
% \author{....lastname \thanks{...} \thanks{...}} 
%                     ^------------^------------^----Do not want these spaces!
%
% a space would be appended to the last name and could cause every name on that
% line to be shifted left slightly. This is one of those "LaTeX things". For
% instance, "\textbf{A} \textbf{B}" will typeset as "A B" not "AB". To get
% "AB" then you have to do: "\textbf{A}\textbf{B}"
% \thanks is no different in this regard, so shield the last}  of each \thanks
% that ends a line with a % and do not let a space in before the next \thanks.
% Spaces after \IEEEmembership other than the last one are OK (and needed) as
% you are supposed to have spaces between the names. For what it is worth,
% this is a minor point as most people would not even notice if the said evil
% space somehow managed to creep in.



% The paper headers
\markboth{IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE,~VOL.~XX,~NO.~XX,~XX~2022}
% \markboth{Journal of \LaTeX\ Class Files,~Vol.~14, No.~8, August~2015}%
{Shell \MakeLowercase{\textit{et al.}}: Bare Demo of IEEEtran.cls for Computer Society Journals}
% The only time the second header will appear is for the odd numbered pages
% after the title page when using the twoside option.
% 
% *** Note that you probably will NOT want to include the author's ***
% *** name in the headers of peer review papers.                   ***
% You can use \ifCLASSOPTIONpeerreview for conditional compilation here if
% you desire.



% The publisher's ID mark at the bottom of the page is less important with
% Computer Society journal papers as those publications place the marks
% outside of the main text columns and, therefore, unlike regular IEEE
% journals, the available text space is not reduced by their presence.
% If you want to put a publisher's ID mark on the page you can do it like
% this:
%\IEEEpubid{0000--0000/00\$00.00~\copyright~2015 IEEE}
% or like this to get the Computer Society new two part style.
%\IEEEpubid{\makebox[\columnwidth]{\hfill 0000--0000/00/\$00.00~\copyright~2015 IEEE}%
%\hspace{\columnsep}\makebox[\columnwidth]{Published by the IEEE Computer Society\hfill}}
% Remember, if you use this you must call \IEEEpubidadjcol in the second
% column for its text to clear the IEEEpubid mark (Computer Society jorunal
% papers don't need this extra clearance.)



% use for special paper notices
%\IEEEspecialpapernotice{(Invited Paper)}



% for Computer Society papers, we must declare the abstract and index terms
% PRIOR to the title within the \IEEEtitleabstractindextext IEEEtran
% command as these need to go into the title area created by \maketitle.
% As a general rule, do not put math, special symbols or citations
% in the abstract or keywords.
\IEEEtitleabstractindextext{%
\begin{abstract}

% Existing gait recognition frameworks retrieve an identity in the gallery based on the distance between a probe sample and the identities in the gallery. However, existing methods often neglect that the gallery may not contain identities corresponding to the probes, leading to recognition errors rather than raising an alarm. In this paper, we introduce a novel uncertainty-aware gait recognition method that models the uncertainty of identification based on learned evidence. Specifically, we treat our recognition model as an evidence collector to gather evidence from input samples and parameterize a Dirichlet distribution over the evidence. The Dirichlet distribution essentially represents the density of the probability assigned to the input samples. We utilize the distribution to evaluate the resultant uncertainty of each probe sample and then determine whether a probe has a counterpart in the gallery or not. To the best of our knowledge, our method is the first attempt to tackle gait recognition with uncertainty modelling. Moreover, our uncertain modeling significantly improves the robustness against out-of-target-scope (OOG) queries. Extensive experiments demonstrate that our method achieves state-of-the-art performance on datasets with OOG queries, and can also generalize well to other identity-retrieval tasks. Importantly, our method outperforms the state-of-the-art by a large margin of 51.26\% when the OOG query rate is around 50\% on OUMVLP. 
Existing gait recognition methods typically identify individuals based on the similarity between probe and gallery samples. 
However, these methods often neglect the fact that the gallery may not contain identities corresponding to the probes, leading to incorrect recognition. % and thus fail to recognize Out-of-Gallery (OOG) queries without corresponding identities in the gallery. 
To identify Out-of-Gallery (OOG) gait queries, we propose an Evidence-based Match-status-Aware Gait Recognition (EMA-GR) framework. 
Inspired by Evidential Deep Learning (EDL), EMA-GR is designed to quantify the uncertainty associated with the match status of recognition. Thus, EMA-GR identifies whether the probe has a counterpart in the gallery. 
% EMA-GR is designed to model the matching uncertainty of the recognition with an evidence-parameterized Dirichlet distribution. 
Specifically, we adopt an evidence collector to gather match status evidence from a recognition result pair and parameterize a Dirichlet distribution over the gathered evidence, following the Dempster-Shafer Theory of Evidence (DST).
We measure the uncertainty and predict the match status of the recognition results, and thus determine whether the probe is an OOG query.
% Inspired by the Dempster-Shafer Theory of Evidence (DST), we quantify the uncertainty of the recognition and identify whether the probe has a counterpart in the gallery. 
To the best of our knowledge, our method is the first attempt to tackle OOG queries in gait recognition. 
Moreover, EMA-GR is agnostic against gait recognition methods and improves the robustness against OOG queries. 
% Moreover, the proposed framework can not only discern whether a probe matches any gallery identity but also enhance robustness against OOG queries. 
Extensive experiments demonstrate that our method achieves state-of-the-art performance on datasets with OOG queries, and can also generalize well to other identity-retrieval tasks. Importantly, our method surpasses existing state-of-the-art methods by a substantial margin, achieving a 51.26\% improvement when the OOG query rate is around 50\% on OUMVLP.

\end{abstract}

% Note that keywords are not normally used for peerreview papers.
\begin{IEEEkeywords}
Gait Recognition, Out-of-Gallery
\end{IEEEkeywords}}


% make the title area
\maketitle
% To allow for easy dual compilation without having to reenter the
% abstract/keywords data, the \IEEEtitleabstractindextext text will
% not be used in maketitle, but will appear (i.e., to be "transported")
% here as \IEEEdisplaynontitleabstractindextext when the compsoc 
% or transmag modes are not selected <OR> if conference mode is selected 
% - because all conference papers position the abstract like regular
% papers do.
% \IEEEdisplaynontitleabstractindextext has no effect when using
% compsoc or transmag under a non-conference mode.


\IEEEdisplaynontitleabstractindextext

% For peer review papers, you can put extra information on the cover
% page as needed:
% \ifCLASSOPTIONpeerreview
% \begin{center} \bfseries EDICS Category: 3-BBND \end{center}
% \fi
%
% For peerreview papers, this IEEEtran command inserts a page break and
% creates the second title. It will be ignored for other modes.
\IEEEpeerreviewmaketitle



\IEEEraisesectionheading{\section{Introduction}\label{sec:introduction}}



\IEEEPARstart{G}{ait} recognition, as an important biometric retrieval task, aims to identify people by their way of walking~\cite{wan2018survey} and is widely used in security monitoring and forensics.
Gait recognition is formulated as a retrieval task, where a query gait sequence, namely a probe, is compared to gait sequences from a gallery. The sequence from the gallery with the minimal distance to the probe will be regarded as a match, and finally, its identity will be assigned to the probe. 

Existing gait recognition methods assume that there are always corresponding identities in the gallery set. However, in practice, this assumption often does not hold. Some probe queries possibly do not have their corresponding identities in the gallery, referred to as Out-of-Gallery (OOG) queries.
Unfortunately, existing methods do not have a mechanism to address OOG cases. As shown in Figure~\ref{fig_schematic}(a), current methods still find an identity in the gallery even though there is actually no identity of the probe in the gallery, leading to erroneous results. 
Therefore, it is highly desirable to develop a gait recognition method that is able not only to find correct identities of In-Gallery (IG) queries but also to identify OOG queries.

\begin{figure}[t]
\centering
    {
        \includegraphics[width=0.33\textwidth]{images/schematic1.pdf}
    }\\
    (a) Existing gait recognition methods\\
    \vspace{1em}
    {
        \includegraphics[width=0.48\textwidth]{images/schematic2.pdf}
    }\\
    (b) The proposed Evidence-based Match-status-Aware Gait Recognition (EMA-GR) framework
    \caption{
        Comparison between traditional gait recognition framework and the proposed uncertainty-aware gait recognition framework when addressing In-Gallery (IG) and Out-of-Gallery (OOG) scenarios. Our framework can accurately recognize the OOG case. 
    }
    \label{fig_schematic}
\end{figure}


In contrast to existing gait recognition approaches, in this work, we formulate gait recognition as an uncertainty-aware feature matching problem. We propose an Evidence-based Match-status-Aware Gait Recognition (EMA-GR) framework that can effectively address both IG and OOG queries.
EMA-GR is agnostic against gait recognition methods and can adopt any existing gait recognition network as a feature extractor for a probe query and gallery sequences. Once we obtain the features of gait sequences, we will retrieve the gallery sequence feature that has a minimal distance from that of the probe. 
Unlike existing methods that directly assign the identity of the retrieved gallery sequence to the probe, we further determine whether the features of the probe query and the retrieved gallery sequence are from the same identity, as shown in Figure~\ref{fig_schematic}(b). 

Specifically, we adopt a deep network as an evidence collector to measure the amount of support gathered from the probe and the gallery features in favor of it being classified into a certain matching status, \ie, matching or non-matching.
Inspired by hard mining strategies, we design a query pair construction strategy to train the evidence collector. We combine the features with matched or unmatched identities into paired features for the evidence collector training. 
However, since the distribution of the features of the probe query and the gallery sequence retrieved in practice may fall outside the intervals trained by the evidence collector, it can lead to inaccuracies in the predictions. Therefore, it becomes essential to estimate uncertainty concurrently with predicting match status. 
% We first adopt a deep network as an evidence collector to collect evidence\footnote{We term evidence as a measure of the amount of support collected from the paired feature in favor of it being classified into a certain class. } on the matching status, \ie, matching and non-matching, of the probe and the gallery features.
% To build training data for the evidence collector, we design an OOG query pair construction strategy. 
% However, we notice that it is difficult to discern the match status of the paired features in practical use because of the variety of retrieved gallery sequences. 



Inspired by Evidential Deep Learning (EDL), we adopt the Dempster-Shafer Theory of Evidence (DST)~\cite{DBLP:series/sfsc/Dempster08b} to assign belief masses to a possible status based on collected evidence. 
To be specific, we utilize the evidence-parameterized Dirichlet distribution to assign belief masses, following the theory of Subjective Logic~\cite{sensoy2018evidential}.
Unlike the evidence for one possible state, a belief mass can be assigned to any possible state of the frame, including the whole frame itself. By assigning the belief mass to the entire frame, every class can be assigned with the belief mass equally. 
In other words, unlike evidence that supports only one opinion about a match status, converting the collected evidence to belief masses allows our network to estimate the uncertainty of the prediction and thus express another opinion: ``\textit{I do not know}''.
Therefore, EMA-GR reflects the mass assignment of the input features and their uncertainty in the feature space under different conditions, including the IG and OOG query scenarios.
As a result, EMA-GR provides a mechanism for detecting OOG probes. Once we obtain the Dirichlet distribution parameterized over the input features, we can apply it to examine whether a pair of retrieved gaits is from the same person and how confident our prediction is.
As a result, our model predicts three scores: a matching score, a non-matching score and an uncertainty score. Based on the three scores, we are able to determine IG and OOG queries.
% In other words, it represents the opinion of ``I do not know'' about all possible states. 


% Inspired by Evidential Deep Learning (EDL), we utilize the Dempster-Shafer Theory of Evidence (DST)~\cite{DBLP:series/sfsc/Dempster08b} to collect evidence on the matching status, \ie, matching and non-matching, of the probe and the gallery features.
% We term evidence as a measure of the amount of support collected from the paired feature in favor of it being classified into a certain class. 
% % examine the identity matches between the probe and the gallery.
% To collect evidence of the possible matching status, we first adopt a deep network as an evidence collector. 



% In order to quantify the uncertainty, we utilize the Dirichlet distribution to transfer the collected evidence to belief masses in a frame of discernment. 
% Unlike the evidence for one possible state, a belief mass can be assigned to any possible state of the frame, including the whole frame itself. By assigning the belief mass to the entire frame, every class can be assigned with the belief mass equally. In other words, it represents the opinion of ``I do not know'' about all possible states.
% To examine the identity matches between the probe and gallery, we introduce the theory of evidential deep learning into the process of feature matching.
% Evidential deep learning~\cite{sensoy2018evidential,DBLP:series/sfsc/Dempster08b,jsang2016subjective} reinterprets neural network output using Dirichlet distributions to assess prediction uncertainty.
% This approach regards deep networks as an evidence collector to gather opinions or evidence of the uncertainty of the predictions. % and thus measures the prediction uncertainty. 
% The gathered evidence measures the amount of support collected from the data in favor of a sample to be classified into a certain class.

% Motivated by this, we propose a gait classification model employed as an evidence collector to gather opinions or evidence from the paired features of the probe and gallery. The gathered evidence is then utilized to parameterize a Dirichlet distribution. 



% Moreover, the theory of Subjective Logic~\cite{sensoy2018evidential} indicates that the evidence-parameterized Dirichlet distribution represents the probability density of the input features.
% In other words, it reflects the mass assignment of the input features and their uncertainty in the feature space under different conditions, including IG and OOG query scenarios.
% Thus, it provides a mechanism to detect OOG probes. Once we obtain the Dirichlet distribution parameterized over the input features, we can apply it to examine whether a pair of retrieved gaits is from the same person and how confident our prediction is.
% As a result, our model predicts three scores: a matching score, a non-matching score and an uncertainty score. Based on the three scores, we are able to determine IG and OOG queries.



Extensive experiments demonstrate that our method achieves state-of-the-art performance in gait recognition with OOG queries. Importantly, our method outperforms the state-of-the-art method, \ie, DyGait~\cite{wang2023dygait}, by a large margin of 51.26\% when the rate of OOG queries is 50\% on one of the largest gait recognition datasets OUMVLP~\cite{takemura2018multi}. 
Our main contributions are summarized as follows:
\begin{itemize}
    \item We propose a novel uncertainty-aware gait recognition model that can tackle both In-Gallery (IG) and Out-of-Gallery (OOG) queries in a unified framework. To the best of our knowledge, we are the first to address OOG cases for gait recognition.

    \item We model the uncertainty of the retrieved identity by applying the theory of evidential deep learning to represent the probability density. 
    % We introduce the theory of evidential deep learning to represent the probability density of a pair of retrieved features and thus model the uncertainty of our prediction.

    \item Our framework is agnostic against gait recognition backbones. It can be adopted by existing methods with minimal effort to address OOG queries, thus significantly improving the robustness of existing methods.
\end{itemize}









\section{Related Work}
Gait recognition aims to learn discriminative feature representations from people's skeletons or silhouettes for identification purposes~\cite{sepas2022deep, shen2022comprehensive, yu2021hid, zhu2021gait, zheng2022gait, zhang2021cross, li2022multi}. 
Its objective is to obtain the identity information for a probe sample from the gallery.
To achieve this goal, many studies focus on improving the feature extraction capability of the model.
These methods can be categorized into three classes, \ie, model-based methods, pose-based methods, and silhouette-based methods.
Model-based methods are designed to extract features from RGB sequences, such as shapes, view angles, and postures ~\cite{wagg2004automated, li2020end, li2021end}. 
Pose-based methods extract 2D poses or 3D poses of human bodies to obtain discriminative feature representations~\cite{teepe2021gaitgraph, liao2020model,hsu2022gaittake,teepe2022towards}. 
Silhouette-based methods generate feature representations by aggregating all temporal information of a gait sequence~\cite{shiraga2016geinet,li2020gait,wang2020human, shen2022gait, lin2021gaitmask, lin2020gait, chai2022lagrange, huang2021context, lin2022gaitgl, wang2022gaitstrip, lin2021multi, yu2022cntn, yu2022generalized, liang2022gaitedge, dou2022gaitmpl, dou2022metagait, huang2022enhanced, hou2021set, hou2020gait, huang20213d}.
Although the above methods obtain high recognition accuracy, they often neglect the fact that a probe might be an Out-of-Gallery (OOG) sample, which does not have counterparts within the gallery. When such probes are exhibited, current methods would fail and cannot discern the absence of matching identities in the gallery. This is because they do not have a mechanism to handle OOG queries. 
To the best of our knowledge, our work is the first attempt to address the OOG query scenario in gait recognition. 

Furthermore, many other recognition tasks, such as vehicle and human re-identification~\cite{zhu2020voc,tang2019cityflow,devyatkov2018multicamera}, as well as face recognition~\cite{guo2020learning,kalayeh2018human}, also follow a very similar recognition paradigm as gait recognition. Consequently, they encounter the same challenges posed by OOG queries.
Our proposed method can be readily integrated into these tasks to enhance their recognition robustness against OOG queries, thus avoiding incorrect identity assignments. 
In addition, our method does not need to re-train the backbone networks, significantly facilitating the adaptation of existing networks to handle OOG queries. 

Additionally, open-set recognition methods~\cite{scheirer2012toward, bao2021evidential} have been proposed to address the Out-of-Distribution (OOD) problem in recognition. These methods aim to classify the known classes while labeling all OOD samples to one unknown class. 
The distinction between our task and open-set recognition lies in the fact that all our probe samples are unseen during training. The most significant difference, however, is that our objective is not to identify OOD samples that do not belong to the training data but rather to ascertain whether a given probe is absent from the gallery. Therefore, the open-set recognition methods cannot be used for our task. 



\section{Uncertainty-aware Gait Recognition Framework}

\begin{figure*}[t] 
\centering
	\includegraphics[width=\linewidth]{images/architecture_training.pdf} 
	\caption{
        Overview of the uncertainty-aware gait recognition framework.
    	We first utilize existing backbones to extract features and then construct hard positive and negative feature pairs to train our evidence collector. By Dirichlet-based evidence learning, our method is able to predict matching, non-matching and uncertainty scores of a pair of the probe feature and the retrieved feature. As a result, our method is able to determine whether they are from the same identity.
    	Note that our method does not need to re-train the backbone networks and thus can be easily applied to different backbones and other similar tasks.
	}
    \label{fig:architecture_training}
\end{figure*}


\subsection{Gait Recognition Revisit}
Existing gait recognition methods follow a standard paradigm, which can be divided into three steps:
(i) Designing and training a gait recognition network. The gait recognition network can be trained with various losses to achieve strong feature extraction capability.
(ii) Extracting features. Once the gait recognition network has been trained, it extracts features from a sequence sampled from the probe and gallery sets. Here, we denote $\mathbb{Q} = \{q_{1},q_{2},...,q_{M}\}$ as all the probe features from the probe set and $\mathbb{G} = \{g_{1},g_{2},...,g_{N}\}$ as all the gallery features from the gallery set, where $q_{i}$ is $i$-th probe feature, $g_{j}$ is the $j$-th gallery feature, and $M$ and $N$ indicate the number of the probe samples and gallery samples, respectively.
(iii) Identifying probes based on the distance to the gallery features. We compute the distance between a probe feature $q_i$ and all the gallery features. For a probe feature $q_{i}$, its distances with respect to all the gallery features can be represented as $\mathbb{D}_{i} = \{D(q_{i}, g_{1}), D(q_{i}, g_{2}),..., D(q_{i}, g_{Q})\}$, where $D$ indicates the Euclidean distance in our experiments. Subsequently, we find the minimum distance in $\mathbb{D}_{i}$. Suppose $D(q_{i}, g_{j})$ is the minimum value in $\mathbb{D}_{i}$, and then the identity of $g_j$ will be assigned to the probe $i$.

However, in step (iii), conventional gait recognition methods neglect the fact there might be no matching identity in the gallery. Although $D(q_{i}, g_{j})$ is minimum, the identity $j$ should not be assigned to the probe $i$. 
When a probe does not have a counterpart identity in the gallery, we refer to it as an Out-of-Gallery (OOG) query, and a probe with matching identities is referred to as an In-Gallery (IG) query. To address both IG and OOG queries in a unified framework, we present an uncertainty-aware gait recognition method, as illustrated in Figure~\ref{fig:architecture_training}.


% \subsection{Preliminary}
\subsection{Dirichlet-based Evidence Learning}

To distinguish IG and OOG queries in a unified framework, we model the probability distribution of the matching uncertainty based on the gathered evidence.
Previous methods estimate prediction variance or uncertainty through dropout~\cite{gal2016dropout,molchanov2017variational,gal2017concrete,amini2018spatial}, ensembling~\cite{pearce2018uncertainty, lakshminarayanan2017simple} or other sampling approaches~\cite{blundell2015weight, hernandez2015probabilistic}. However, this stream of work often relies on expensive sampling operations, and it is hard to apply them to feature-matching tasks since the probe and gallery features are often fixed.

% In Bayesian inference, placing prior distributions over deep models to estimate uncertainty has been widely explored~\cite{gelman2006prior,gelman2008weakly}, \eg, Evidential Deep Learning (EDL)~\cite{sensoy2018evidential} and Prior Networks~\cite{malinin2018predictive,malinin2019reverse} place Dirichlet priors over discrete classification predictions. 
In contrast, Bayesian inference offers a distinct advantage by enabling the estimation of uncertainty without the need for sampling. Bayesian inference methods~\cite{gelman2006prior,gelman2008weakly} place prior distributions over deep models. For example, Evidential Deep Learning (EDL)~\cite{sensoy2018evidential} and Prior Networks\cite{malinin2018predictive,malinin2019reverse} effectively apply Dirichlet priors over discrete classification predictions to provide robust uncertainty estimation.
% Motivated by this, we employ the theoretical framework of EDL to model the probability distribution of the uncertainty.



Given an input feature $\bm{x}$, EDL first uses an evidence collector to generate the evidence $\bm{e} = \{e_{k}|k=1,\ldots,K\}$, where $K$ is the number of classes and $e_{k}$ denotes the evidence of the $k$-th class. 
Based on the Subjective Logic theory~\cite{sensoy2018evidential}, the evidence $\bm{e}$ corresponds to a Dirichlet distribution parameterized by $\bm{\alpha} = \{\alpha_{k}|k=1,\ldots,K\}$, where $\alpha_{k} = e_{k} + 1$. 
A Dirichlet distribution is a probability density function for all the possible values of a probability mass function $\bm{p}$, and can be represented as:
\begin{equation}
\begin{array}{l}
\mathbf{D}(\bm{p} \mid \bm{\alpha})=\left\{\begin{array}{ll}
\frac{1}{B(\bm{\alpha})} \prod_{i=1}^{K} p_{i}^{\alpha_{i}-1} & \text { for } \bm{p} \in \mathcal{S}_{K}, \\
0 & \text { otherwise },
\end{array}\right. 
\end{array}
\end{equation}
where $\mathcal{S}_{K}=\left\{\bm{p} \mid \sum_{i=1}^{K} p_{i}=1 \text { and } 0 \leq p_{1}, \ldots, p_{K} \leq 1\right\}$ is the $K$-dimensional unit simplex,
and $B(\alpha)$ is the $K$-dimensional multinomial beta function~\cite{kotz2004continuous}. Here, $p_{k}$ is the expected probability for the $k$-th class and is calculated as $p_k = \frac{\alpha_k}{S}$, where $S= \sum_{i=1}^{K} \alpha_{i} $.



We denote the ground-truth label of $\bm{x}$ is $\bm{y}$ that is a $K$-dimensional one-hot vector with $y_{i}$ = 1 and $y_{k}$ = 0 for all $k \neq i$. 
Mean Square Error (MSE) is employed to measure the differences between the estimated class probabilities and the ground truth. It can be expressed as follows: % and can be expressed as:
\begin{align}
\mathcal{L}_{m}(\Theta) &=\int\left\|\bm{y}-\bm{p}\right\|_{2}^{2} \mathbf{D}(\bm{p} \mid \bm{\alpha}) \notag
\\
% &=\sum_{j=1}^{K} \mathbb{E}\left[y_{i j}^{2}-2 y_{i j} p_{i j}+p_{i j}^{2}\right] \notag
% \\
&=\sum_{i=1}^{K}\left(y_{i}^{2}-2 y_{i} \mathbb{E}\left[p_{i}\right]+\mathbb{E}\left[p_{i}^{2}\right]\right),
\end{align}
where $\mathbb{E}$ indicates the expectation operation.



Furthermore, if an input feature cannot be correctly classified, its evidence ideally should shrink to zero. In other words, the Dirichlet distribution will be a uniform Dirichlet distribution. To this end, Sensoy~\etal~\cite{sensoy2018evidential} introduce a Kullback-Leibler (KL) divergence term. As a result, the total loss function is represented as:
\begin{align}
    \mathcal{L}(\Theta) = \mathcal{L}_{m}(\Theta) +\lambda_{t} K L\left[\mathbf{D}\left(\bm{p} \mid \tilde{\bm{\alpha}}\right) \| \mathbf{D}\left(\bm{p} \mid\langle 1, \ldots, 1\rangle\right)\right],
\end{align}
where $\lambda_t = \min(1.0, t/10) \in [0, 1]$ is the annealing coefficient. $\mathbf{D}\left(\bm{p} \mid\langle 1, \ldots, 1\rangle\right)$ indicates a uniform Dirichlet distribution, $t$ is the index of the current training iteration, and 
$\tilde{\bm{\alpha}} = \bm{y} + (1 - \bm{y}) \odot \bm{\alpha}$ is the Dirichlet parameters after the removal of the non-misleading evidence from predicted parameters $\bm{\alpha}$ for the input feature $\bm{x}$. 


\begin{figure*}[t]
    \centering
    \includegraphics[width=\textwidth]{images/architecture_inference.pdf}
    \caption{
        Inference for the uncertainty-aware gait recognition framework. Given a probe feature $q_i$, we first search for a gallery feature $g_j$ that has the minimum distance of $q_i$. Then, the combined feature is processed through the evidence collector to determine whether the probe is an IG and OOG query.
    }
    \label{fig:architecture_inference}
\end{figure*}


\subsection{Uncertainty Modeling for Gait Recognition}

As aforementioned, the premise that the gallery must contain an identity corresponding to the probe sample is considerably difficult to satisfy in practice, and OOG query cases may more likely happen if the gallery size is limited.
Therefore, a mechanism to tackle OOG gait recognition is highly desirable.


A straightforward approach to address this problem is to develop a binary classifier. This classifier determines whether a probe sample and a gallery sample, which exhibits the minimum distance to the probe feature, share the same identity.
Here, the errors from the gait recognition networks should not affect the classifier performance as the adopted networks already achieve appealing performance on the gait feature representation. 
Using the simple classifier, the result of each retrieval can be formulated as follows:
\begin{equation}
    Y = \left\{
        \begin{matrix}
            \textit{OOG} \quad & F_{c}(q_{i}, g_{j}) \leq 0.5,\\ \hfill 
            \textit{IG} \quad & F_{c}(q_{i}, g_{j}) > 0.5, \hfill 
        \end{matrix}
    \right.
\end{equation}
where $Y$ is the framework output. $\textit{OOG}$ indicates the input probe feature $q_{i}$ and gallery feature $g_{j}$ are from different identities. $\textit{IG}$ represents the input probe feature $q_{i}$ and gallery feature $g_{j}$ are from the same identity. $F_{c}(\cdot)$ denotes the binary classifier that takes a probe feature and a gallery feature as input.
Unfortunately, a simple binary classifier tends to output an over-confident erroneous prediction especially when an OOG query is fed into it. Hence, a classifier would fail to detect the OOG query, and thus the gait recognition methods would assign incorrect identities to probes. 

To address this problem, we introduce the theory of EDL~\cite{sensoy2018evidential} into current gait recognition methods and then model the uncertainty of whether the query and retrieved features are from the same identity.
The EDL theory is built on the Dempster-Shafer Theory of Evidence (DST), a generalization of the Bayesian theory to subjective probabilities~\cite{DST}.
To be specific, EDL assigns belief masses, \eg, probabilities, to a set of mutually exclusive labels, \eg, class labels for a sample. 
A belief mass can be assigned to any subset of a frame~\cite{DST}, including the whole frame itself. Here, the frame refers to the possible labels for a sample.
Different from traditional binary classification, we can introduce a label ``I do not know'' into the frame, representing the uncertainty of a given sample based on the theoretical framework of EDL.
To formulate the belief mass assignments, Subjective Logic theory (SL)~\cite{sensoy2018evidential} is an effective tool. 
Specifically, SL can formalize the belief assignments over a frame of discernment via a Dirichlet distribution. Thus, SL allows us to employ the principles of evidential theory to quantify the belief masses and uncertainty.

Driven by SL, we propose an uncertainty-aware gait recognition framework that enables us to recognize IG and OOG queries in a unified framework, as shown in Figure~\ref{fig:architecture_training}.
In order to apply the uncertainty modeling of SL to our method, we first employ a gait recognition network to retrieve a gallery feature that has the minimum distance to a probe feature. Then, we compute the feature discrepancy between the probe and gallery features, called \emph{residual feature}.
Given the residual features, we aim to identify whether a query corresponds to an IG or OOG sample.
Instead of directly assessing the similarity between the probe and every gallery sample, we opt to examine the distribution of residual features. 
This approach allows us to estimate the uncertainty of the probe without the computational burden of detailed gallery feature distribution modeling.


Modeling the gallery distribution implicitly via a discriminator will degenerate into the case of learning a simple classifier, which has been demonstrated ineffective for the gait recognition problem.
In contrast, we consider $K=2$ mutually exclusive singletons, \ie, $\textit{matching}$ and $\textit{non-matching}$, by providing a belief mass $b_k$ for each singleton based on SL. 
Therefore, the singleton space is significantly smaller than the number of identities in the gallery set and simplifies the classification process.
We assign belief masses $b_1$ and $b_2$, respectively, which represent the subjective probabilities of matching and non-matching pairs, alongside an overall uncertainty mass $\mu$. The mass values are non-negative and collectively sum to unity, and can be expressed as follows:
\begin{equation}
    \label{sum_all_scores}
    \mu +\sum_{k=1}^{K} b_{k}=1.
\end{equation}
The retrieval result is determined by:
\begin{equation}
    \label{eqn_determine}
    Y = \left\{
        \begin{matrix}
            \textit{OOG} \quad & b_2 + \mu \geq b_1, \\ \hfill 
            \textit{IG} \quad & b_2 + \mu < b_1.  \hfill 
        \end{matrix}
    \right.
\end{equation}
When the prediction score of the matching class $b_1$ is higher than the sum of the score of the non-matching class $b_2$ and the uncertainty score, we believe the retrieved identity should be assigned to the probe. Otherwise, we will discard the retrieved identity as the probe sample could be an OOG query. Thanks to the uncertainty modeling of SL, we can effectively refuse OOG queries without harming the overall recognition performance.



\subsection{Evidence Collector Design}

In Equation~\eqref{sum_all_scores}, the mass values of $b_k$ and $\mu$ are derived from the evidence ($e_k \ge 0$) and the evidence will be used to determine the Dirichlet distribution and uncertainty. 
Based on EDL, the evidence is a measure of support collected from data in favor of a sample to be classified into a certain class. Therefore, we develop a simple Multi-Layer Perceptron (MLP), including two fully-connected (FC) layers, to collect the evidence. That means the output of MLP is the evidence $\bm{e} = \{e_k | k = 1,\ldots, K\}$. To ensure the value of the evidence is non-negative, we adopt the ReLU function to filter the negative value.
Once we obtain evidence $e_k$, the belief mass $b_k$ and uncertainty mass $\mu$ are expressed as:
\begin{equation}
    \label{eqn_bk}
    b_{k}=\frac{e_{k}}{S} \quad \text{and} \quad \mu = \frac{K}{S},
\end{equation}
where $ S = \sum_{i=1}^{K} (e_{i} + 1)$. According to SL, a belief mass assignment corresponds to a Dirichlet distribution with parameters $\alpha_k=e_k + 1$ and $S = \sum_{i=1}^{K} a_{i}$ is the Dirichlet strength. Equation~\eqref{eqn_bk} can be written as:
\begin{equation}
    \label{eq5}
    b_{k}=\frac{a_{k}-1}{\sum_{i=1}^{K} a_{i}} \quad \text{and} \quad \mu = \frac{K}{\sum_{i=1}^{K} a_{i}}.
\end{equation}
Equation~\eqref{eq5} implies if the strength of a Dirichlet distribution or evidence is larger, the Dirichlet distribution is more concentrated and the uncertainty of the prediction will become smaller. At that point, the prediction of the network will be more reliable. If the network predicts that the probe and the retrieved gallery sample share the same identity, as illustrated in Equation~\eqref{eqn_determine}, the probe is likely to be an IG query. Otherwise, the probe might be considered as an OOG query.
If evidence is lower (\ie, the probabilities assigned to the two classes are small), the Dirichlet distribution will be flatter 
and the uncertainty of the prediction will be higher. Thus, the input might come from an OOG query.



\subsection{Query Pair Construction}
\label{sec:query_pair_constr}
Our uncertainty-aware classification model aims to determine whether a pair of retrieved features are from the same identity or not. To train our classification model, we need to simulate the testing input. 
For this purpose, we construct pairs of samples, including positive and negative pairs. 
To be specific, a positive pair is obtained by sampling different sequences from the same identity, while we sample two sequences from different identities to construct a negative pair.


A simple strategy is to construct positive and negative pairs by randomly selecting two samples from the training set. 
Here, negative pairs are randomly chosen from two identities and positive pairs are randomly selected within the same identity.
Even though we balance the number of positive and negative pairs, we find the network cannot effectively classify hard negative pairs.
This is because the constructed pairs contain numerous simple cases that can be distinguished easily. In other words, our model does not have many chances to learn uncertain cases during training.

Inspired by the hard mining strategies, we construct the hardest positive and the hardest negative pairs within each training batch and thus enforce our model to classify highly uncertain samples.
To be specific, we first randomly select $P$ identities from the training set and then randomly choose $V$ samples from each identity. Therefore, a total of $P\times V$ samples can be selected. Denote $\mathbb{I} = \{f_{(1,1)}, f_{(1,2)}, \ldots, f_{(i,j)}, \ldots, f_{(P,V)}\}$ be the features of all selected samples. $(i,j)$ indicates the $j$-th samples of the $i$-th identity. 
For the feature $f_{(i,j)}$, we can calculate the Euclidean distance between $f_{(i,j)}$ and each feature in $\mathbb{I}$. 
Finally, the hardest positive pair can be constructed as $<f_{(i,j)} , f_{(i,q)}>$, where $f_{(i,q)}$ has the maximum distance with the feature $f_{(i,j)}$ for all samples in the $i$-th identity. The hardest negative pair can be constructed as $< f_{(i,j)} , f_{(v,w)} >$, where $f_{(v,w)}$ has the minimum distance with the feature $f_{(i,j)}$ and $v \neq i$. 
For all the features of $I$ in the batch, we can totally generate $P \times V$ hardest positive pairs and $P \times V$ hardest negative pairs.


\subsection{Inference}

The inference stage of the uncertainty-aware gait recognition framework is shown in Figure~\ref{fig:architecture_inference}.
We first use a gait backbone to extract all the probe and gallery features. Recall that $\mathbb{Q} = \{q_{1},q_{2},...,q_{M}\}$ denotes the probe features from the probe set and $\mathbb{G} = \{g_{1},g_{2},...,g_{N}\}$ represents the gallery features from the gallery set. 
For each probe feature $q_{i}$, we compute the distances between $q_{i}$ and all the gallery features and then find the gallery feature $g_j$ that has the minimum distance to the probe. 


Different from existing methods that assign the identity of $g_j$ to the probe sample $i$, we utilize our uncertainty-aware gait recognition model to further determine whether the two features are from the same identity.
To be specific, we compute the feature discrepancy between the two features. The feature discrepancy is then fed into our uncertainty-aware gait recognition model to generate evidence $\bm{e} = \{ e_1, e_2 \}$. From Equation~\eqref{eqn_bk}, we can obtain the matching probability $b_1$, the non-matching probability $b_2$ and the uncertainty probability $\mu$. 
Given the three probabilities, we can determine whether the probe sample $i$ is an IG or OOG query based on Equation~\eqref{eqn_determine}.



\section{Experiments}

\begin{table*}[t]
\footnotesize
    \centering
    \caption{
        Rank-1 accuracy (\%) on the OUMVLP dataset under different OOG percentages. The standard deviation is shown in parentheses. 
        For a fair comparison, we join the backbones with different OOG detection strategies, \eg, Naive CLS, Verification and Anomaly Detection. Here, in the default evaluation protocol, 7\% of the probes are OOG queries.
    }
    \renewcommand{\arraystretch}{0.5}
    \setlength{\tabcolsep}{2.7mm}{
    \begin{tabular}{l|c|c|c|c|c|c}
        \toprule
        \multicolumn{1}{c|}{\multirow{2}[2]{*}{Methods}} & \multicolumn{6}{c}{OOG percentages} \\
        \cmidrule{2-7} & \multicolumn{1}{c|}{7\%} & \multicolumn{1}{c|}{15\%} & \multicolumn{1}{c|}{25\%} & \multicolumn{1}{c|}{35\%} & \multicolumn{1}{c|}{45\%} & \multicolumn{1}{c}{55\%} \\
        \midrule
        GaitSet & 87.34  & 79.71  & 70.35  & 61.80  & 52.29  & 42.73  \\
        \midrule
         GaitSet + Naive CLS & 88.43 ($\pm$ 0.00) & 83.09 ($\pm$ 0.00)  & 76.94 ($\pm$ 0.00)  & 71.86 ($\pm$ 0.02) &  67.00 ($\pm$ 0.07) &  63.28 ($\pm$ 0.06) \\
        \midrule
        GaitSet + Verification & 87.37 ($\pm$ 0.00) & 80.53 ($\pm$ 0.01)  & 72.26 ($\pm$ 0.02)  & 64.85 ($\pm$ 0.02) &  56.83 ($\pm$ 0.02) &  49.22 ($\pm$ 0.03) \\
        \midrule
        GaitSet + Anomaly Detection & 87.85 ($\pm$ 0.00) & 81.24 ($\pm$ 0.03)  & 73.39 ($\pm$ 0.03)  & 66.58 ($\pm$ 0.02) &  59.54 ($\pm$ 0.06) &  53.40 ($\pm$ 0.06) \\
        \midrule    
        GaitSet + Ours & \textbf{88.14 ($\pm$ 0.00)}  & \textbf{86.64 ($\pm$ 0.03)}  & \textbf{85.16 ($\pm$ 0.02)} & \textbf{84.26 ($\pm$ 0.03)} & \textbf{83.84 ($\pm$ 0.06)} & \textbf{84.09 ($\pm$ 0.03)}   \\    \midrule \midrule
        GaitPart & 88.80  & 80.97  & 71.39  & 62.65  & 52.96  & 43.23  \\
        \midrule
         GaitPart + Naive CLS & 90.38 ($\pm$ 0.00) & 85.78 ($\pm$ 0.02)  & 80.64 ($\pm$ 0.02)  & 76.49 ($\pm$ 0.05) &  72.71 ($\pm$ 0.06) &  70.05 ($\pm$ 0.08) \\
        \midrule
        GaitPart + Verification & 88.72 ($\pm$ 0.00) & 81.88 ($\pm$ 0.01)  & 73.65 ($\pm$ 0.02)  & 66.37 ($\pm$ 0.01) &  58.54 ($\pm$ 0.03) &  51.16 ($\pm$ 0.02) \\
        \midrule
        GaitPart + Anomaly Detection & 89.66 ($\pm$ 0.00) & 83.50 ($\pm$ 0.01)  & 76.33 ($\pm$ 0.01)  & 70.22 ($\pm$ 0.02) &  64.15 ($\pm$ 0.11) &  59.11  ($\pm$ 0.10) \\
        \midrule
        GaitPart + Ours & \textbf{89.88 ($\pm$ 0.00)}  & \textbf{88.41 ($\pm$ 0.01)}  & \textbf{86.96 ($\pm$ 0.03)} & \textbf{86.03 ($\pm$ 0.06)} & \textbf{85.53 ($\pm$ 0.01)} & \textbf{85.68 ($\pm$ 0.01)} \\    \midrule \midrule
        GaitGL & 90.17  & 82.15  & 72.35  & 63.45  & 53.58  & 43.68  \\
        \midrule
        GaitGL + Naive CLS & 92.27 ($\pm$ 0.00) & 88.81 ($\pm$ 0.01)  & 84.99 ($\pm$ 0.03)  & 82.02 ($\pm$ 0.05) &  79.51 ($\pm$ 0.05) &  77.87 ($\pm$ 0.08)  \\
        \midrule
        GaitGL + Verification & 90.47 ($\pm$ 0.00) & 83.06 ($\pm$ 0.00)  & 74.14 ($\pm$ 0.02)  & 66.19 ($\pm$ 0.02) & 57.67 ($\pm$ 0.06) &  49.52 ($\pm$ 0.07) \\
        \midrule
        GaitGL + Anomaly Detection & 91.53 ($\pm$ 0.00) & 86.21 ($\pm$ 0.02)  & 80.15 ($\pm$ 0.05)  & 75.15 ($\pm$ 0.06) & 70.43 ($\pm$ 0.05) &  66.77  ($\pm$ 0.09) \\
        \midrule
        GaitGL + Ours & \textbf{91.52 ($\pm$ 0.00)}  & \textbf{90.27 ($\pm$ 0.01)}  & \textbf{89.03 ($\pm$ 0.03)} & \textbf{88.21 ($\pm$ 0.04)} & \textbf{87.82 ($\pm$ 0.02)} & \textbf{87.87 ($\pm$ 0.05)} \\    \midrule \midrule
        GaitBase &   92.88  &	84.91 &	74.65 &	64.94 &	54.96 &	44.81  \\
        \midrule
        GaitBase + Naive CLS &  96.35 ($\pm$ 0.00) &	92.47 ($\pm$ 0.02) &	88.21 ($\pm$ 0.01) &	84.90 ($\pm$ 0.02)	& 82.01 ($\pm$ 0.05) &	80.26 ($\pm$ 0.05)\\
        \midrule
        GaitBase + Verification &  97.24 ($\pm$ 0.00) &	94.74 ($\pm$ 0.03) &	92.02 ($\pm$ 0.03) 	& 89.80 ($\pm$ 0.02) &	87.76 ($\pm$ 0.03)	& 86.32 ($\pm$ 0.02) \\
        \midrule
        GaitBase + Anomaly Detection & 94.83 ($\pm$ 0.00) &	88.85 ($\pm$ 0.01) &	82.06 ($\pm$ 0.04) &	76.43 ($\pm$ 0.02) &	71.06 ($\pm$ 0.05) &	67.04  ($\pm$ 0.07)\\
        \midrule
        GaitBase + Ours &  \textbf{98.63 ($\pm$ 0.00)} &	\textbf{97.48 ($\pm$ 0.01)} &	\textbf{96.33 ($\pm$ 0.00)} &	\textbf{95.50 ($\pm$ 0.02)} &	\textbf{94.93 ($\pm$ 0.03)} &	\textbf{94.73 ($\pm$ 0.00)} \\    \midrule \midrule
        DyGait &   92.94  &	84.96 &	74.68 &	64.97 	& 54.98 &	44.82   \\
        \midrule
        DyGait + Naive CLS & 97.30 ($\pm$ 0.00) &	94.48 ($\pm$ 0.05) & 91.51 ($\pm$ 0.01) &	89.29 ($\pm$ 0.03) &	87.48 ($\pm$ 0.07) &	86.61 ($\pm$ 0.04)  \\
        \midrule
        DyGait + Verification & 97.28 ($\pm$ 0.00) &	94.19 ($\pm$ 0.06) &	90.74 ($\pm$ 0.05) & 87.89 ($\pm$ 0.05) &	85.28 ($\pm$ 0.06) &	83.41 ($\pm$ 0.08) \\
        \midrule
        DyGait + Anomaly Detection & 95.60 ($\pm$ 0.00) &	90.61 ($\pm$ 0.05) &	85.11 ($\pm$ 0.04) &	80.72 ($\pm$ 0.07) &	76.83 ($\pm$ 0.02) &	74.29 ($\pm$ 0.04) \\
        \midrule
        DyGait + Ours & \textbf{99.00 ($\pm$ 0.00)} &	\textbf{98.13 ($\pm$ 0.04)} &	\textbf{97.26 ($\pm$ 0.01)} &	\textbf{96.65 ($\pm$ 0.02)} &	\textbf{96.20 ($\pm$ 0.03)}	& \textbf{96.08  ($\pm$ 0.02)} \\
        \bottomrule
    \end{tabular}
    }
    \label{tab:tab1}
\end{table*}

\subsection{Dataset}
% We evaluate the effectiveness of our method on one of the largest gait recognition benchmarks OUMVLP~\cite{takemura2018multi}.
% We evaluate the effectiveness of our method on two gait recognition benchmarks OUMVLP~\cite{takemura2018multi} and Gait3D~\cite{zheng2022gait}. The details of these datasets are depicted as follows.
We evaluate the effectiveness of our method on three gait recognition benchmarks OUMVLP~\cite{takemura2018multi}, CASIA-B~\cite{yu2006framework}, and Gait3D~\cite{zheng2022gait}. 
Moreover, to verify the applicability of our method to other tasks, we also conduct experiments on other similar benchmarks, such as vehicle re-identification benchmark VERI-Wild~\cite{lou2019large,lou2021large} and synthetic person-identification benchmark VC-Clothes~\cite{wan2020person}. The details of these datasets are depicted as follows.


\noindent \textbf{OUMVLP}~\cite{takemura2018multi} is a large-scale gait dataset. 
It includes 10,307 subjects, each of which contains a gallery set and a probe set. Each subject in a set is collected from 14 views ($0^\circ \text{--} 90^\circ$ and $180^\circ \text{--} 270^\circ$ with a sampling interval of $15^\circ$). In this work, we use the evaluation protocol presented in~\cite{chao2019gaitset,fan2020gaitpart,lin2021gait} to verify the effectiveness of the proposed uncertainty-aware gait recognition framework. 
Under the default evaluation protocol, 7\% of the probes are OOG queries, which brings a challenge to the state-of-the-art methods. In addition, we randomly remove some gallery sets of subjects to construct more severe OOG query scenarios.


\noindent \textbf{CASIA-B}~\cite{yu2006framework} is one of the most popular gait datasets. It includes 124 subjects and about 1.3K sequences. Each subject is collected in sequences from 11 views ($0^{\circ}, 18^{\circ}, \ldots, 180^{\circ}$). For each view, 10 groups of sequences are collected. 
In our experiments, the CASIA-B dataset is divided into three subsets: a training set, a query set, and a gallery set. To be specific, 74 IDs\footnote{Here, ID refers to identity. With a slight abuse of notation, we refer to ID as IG for simplicity.\label{ref:footnote}} are used to construct the training set, and the remaining 50 IDs are used as the query and gallery sets. 
We select one image from each ID as the gallery set and the rest of the images as the query set. By randomly removing IDs from the gallery set, we construct various OOG query cases.


\noindent \textbf{Gait3D}~\cite{zheng2022gait} is a large-scale and in-the-wild dataset. It includes 4K subjects and 25K sequences captured by 39 surveillance cameras in the wild. In our experiments, the Gait3D dataset is divided into three subsets: a training set, a query set, and a gallery set. To be specific, 3K IDs\footref{ref:footnote} are used to construct the training set, and the remaining 1K IDs are used as the query and gallery sets. We select one image from each ID as the query set and the rest of the images as the gallery set. By removing IDs from the gallery set, we construct various OOG query cases.
Note that, in the default evaluation protocol, 5\% of the probes are OOG queries. Moreover, we randomly remove some subjects in the gallery set to construct more severe OOG query scenarios.


\noindent \textbf{VERI-Wild}~\cite{lou2019large,lou2021large} is a large-scale dataset used for vehicle re-identification tasks.
It includes 416,314 images captured by 174 surveillance cameras in the wild. In general, the VERI-Wild dataset is divided into three subsets: a training set, a query set, and a gallery set. 
We use the evaluation protocol as in~\cite{he2020fastreid}, which includes three different settings, \ie, Small-size Setting (SS), Medium-size Setting (MS) and Large-size Setting (LS). 
In the three settings, 3K IDs, 5K IDs and 10K IDs are used to construct the query and gallery sets, respectively. In each setting, we select one image from each ID as the gallery set and the rest of the images as the probe set. 
By randomly removing IDs from the gallery set or probe set, we construct various OOG query cases.


\noindent \textbf{VC-Clothes}~\cite{wan2020person} is a synthetic person re-identification dataset. It includes 512 subjects, and each subject has 36 corresponding images. 
The dataset has been divided into two subsets: a training set and a testing set. Each set contains 256 IDs. 
In the inference stage, we randomly choose four images from each subject for the gallery set and the rest of the images are used in the probe set. Hence, the gallery and probe sets contain 1020 images and 8591 images, respectively.


% Since we construct OOG queries on both OUMVLP, CASIA-B, and Gait3D datasets, we will release all OOG configurations for reproducibility.




\subsection{Implementation Details}
The proposed evidence collector is built on a multilayer perceptron (MLP), including two FC layers. 
For the OUMVLP dataset, we adopt five common gait recognition methods, GaitGL~\cite{lin2021gait}, GaitPart~\cite{fan2020gaitpart}, GaitSet~\cite{chao2019gaitset}, OpenGait~\cite{fan2023opengait}, and DyGait~\cite{wang2023dygait}, as the backbones to extract gait features and evaluate the effectiveness of the proposed evidence collector. Since the feature sizes of the five backbones are different, we set the hidden layer size of our MLP to 544, 544, 768, 768, and 769, respectively. 
For the CASIA-B dataset~\cite{yu2006framework}, the MLP hidden layer size of five frameworks is set to 8192.
For the Gait3D dataset~\cite{zheng2022gait}, we also take the five frameworks as the backbones to extract gait features and evaluate the effectiveness of the proposed evidence collector. We set the hidden layer size of our MLP to 8192. 
For the VERI-Wild dataset~\cite{lou2019large,lou2021large}, BoT~\cite{luo2019bag} is used as the backbone, and thus the hidden layer size of MLP of the uncertainty-aware classification model is set to 1024. For the VC-clothes dataset, we take CBN~\cite{zhuang2020rethinking} as the backbone and the hidden layer size of MLP is set to 4096.


In Section~\ref{sec:query_pair_constr}, we design a query pair construction strategy to generate positive and negative sample pairs for the model training. The designed strategy is controlled by two hyper-parameters, $P$ and $V$, which are fixed at 32 and 8, respectively, across all experiments. Meanwhile, the learning rate is initialized at $1 \times 10^{-3}$ and undergoes a tenfold reduction every 10 iterations. Additionally, the total number of training iterations is set to 50 in all experimental settings for fair comparison.


Furthermore, we run each OOG percentage setting three times in order to demonstrate the robustness of our method against OOG queries. Specifically, in each OOG percentage setting, we randomly remove a certain percentage of the identities from the original gallery set to construct a different gallery set.
Since we construct OOG queries on both OUMVLP~\cite{takemura2018multi}, CASIA-B~\cite{yu2006framework}, and Gait3D~\cite{zheng2022gait} datasets, we will release all OOG configurations, along with the training and evaluation code, including checkpoints, to ensure reproducibility.


\begin{table*}[t]
    \footnotesize
    \centering
    \caption{
        Rank-1 accuracy (\%) on the CASIA-B dataset under different OOG percentages. The standard deviation is shown in parentheses. For a fair comparison, we join the backbones with different OOG detection strategies, \eg, Naive CLS, Verification and Anomaly Detection.
    }
    \renewcommand{\arraystretch}{0.5}
    \setlength{\tabcolsep}{2.8mm}{
        \begin{tabular}{l|c|c|c|c|c|c}
            \toprule
            \multicolumn{1}{c|}{\multirow{2}[2]{*}{Methods}} & \multicolumn{6}{c}{OOG percentages} \\
            \cmidrule{2-7} & \multicolumn{1}{c|}{0\%} & \multicolumn{1}{c|}{15\%} & \multicolumn{1}{c|}{25\%} & \multicolumn{1}{c|}{35\%} & \multicolumn{1}{c|}{45\%} & \multicolumn{1}{c}{55\%} \\
            \midrule
            GaitSet & 91.81  & 79.39  & 70.92  & 63.23  & 53.22  & 42.53   \\
            \midrule
            GaitSet +  Naive CLS &  91.78 ($\pm$ 0.00) &	81.77 ($\pm$ 0.87) &	74.10 ($\pm$ 1.06) &	66.89 ($\pm$ 1.46) &	61.05 ($\pm$ 1.98) &	55.18 ($\pm$ 0.62) \\
            \midrule
            GaitSet +  Verification &  83.31 ($\pm$ 0.00) &	78.22 ($\pm$ 0.55) &	71.89 ($\pm$ 0.48) &	70.39 	($\pm$ 1.12) & 68.45 	($\pm$ 1.76) & 66.85 ($\pm$ 1.59) \\
            \midrule
            GaitSet +  Anomaly Detection & 91.72 ($\pm$ 0.00) &	 	81.69 ($\pm$ 0.79) &		74.78 ($\pm$ 0.89) &		68.17 ($\pm$ 1.91) &		62.30 	($\pm$ 1.43) &	55.34 ($\pm$ 1.00)    \\
            \midrule
            GaitSet + Ours & \textbf{91.38 ($\pm$ 0.00)}  & \textbf{83.25 ($\pm$ 1.13)}  & \textbf{76.55 ($\pm$ 0.77)} & \textbf{71.80 ($\pm$ 1.80)} & \textbf{68.94 ($\pm$ 3.62)} & \textbf{63.12 ($\pm$ 2.25)}    \\
            \midrule
            \midrule
            GaitPart & 92.61  & 80.16 & 	72.19 	& 63.63 	& 53.81 &	44.20  \\
            \midrule
            GaitPart + Naive CLS  & 91.38 ($\pm$ 0.00)  &	81.10 ($\pm$ 0.75)  &	73.44 ($\pm$ 1.16)  &	66.95 ($\pm$ 0.22)  &	61.47 ($\pm$ 1.23)  &	56.70 ($\pm$ 1.13)    \\
            \midrule
            GaitPart + Verification &  81.00 ($\pm$ 0.00)  &	74.96 ($\pm$ 0.79)  &	68.28 ($\pm$ 0.80)  &	66.84 ($\pm$ 1.25)  &	65.36 ($\pm$ 1.42)  &	65.46 ($\pm$ 0.92)  \\
            \midrule
            GaitPart + Anomaly Detection & 91.59  ($\pm$ 0.00)  &	79.49  ($\pm$ 0.55)  & 	70.95  ($\pm$ 0.97)  &	63.04  ($\pm$ 0.82)  &	55.59  ($\pm$ 1.18)  &	49.45  ($\pm$ 1.33)  \\
            \midrule
            GaitPart + Ours & \textbf{90.94 ($\pm$ 0.00)}  & \textbf{81.80 ($\pm$ 0.37)}  & \textbf{74.93 ($\pm$ 0.64)} & \textbf{70.00($\pm$ 1.00)} & \textbf{66.22 ($\pm$ 1.63)} & \textbf{61.88  ($\pm$ 1.94)} \\
            \midrule
            \midrule
            GaitGL & 94.77  & 81.89  & 73.27  & 64.50  & 54.74  & 44.01  \\
            \midrule
            GaitGL + Naive CLS  & 94.47 ($\pm$ 0.00)  &	85.10 ($\pm$ 1.17 )  &	77.27 ($\pm$ 1.02)  &	71.71 	($\pm$ 1.28)  & 66.38 	($\pm$ 0.58)  & 59.90 ($\pm$ 0.59)   \\
            \midrule
            GaitGL + Verification & 32.00  ($\pm$ 0.00)  &	41.62  ($\pm$ 0.14)  &	47.98  ($\pm$ 0.78)  &	54.79  ($\pm$ 0.63)  &	61.42 ($\pm$ 0.047  & 	68.33 ($\pm$ 0.60)   \\
            \midrule
            GaitGL + Anomaly Detection & 94.77 ($\pm$ 0.00)  &	83.55 ($\pm$ 0.56)  &	74.91 ($\pm$ 0.70)  &	66.62 ($\pm$ 1.46)  &	58.58 ($\pm$ 1.26)  &	48.93 ($\pm$ 0.75)    \\
            \midrule
            GaitGL + Ours & \textbf{93.79 ($\pm$ 0.00)}  & \textbf{85.61 ($\pm$ 1.08)}  & \textbf{79.06 ($\pm$ 0.92)} & \textbf{74.83  ($\pm$ 1.52)} & \textbf{70.96 ($\pm$ 0.61)} & \textbf{65.83 ($\pm$ 0.27)} \\
            \midrule
            \midrule
            GaitBase & 96.01 &	82.97 &	74.11 	& 64.44 &	54.64 	& 44.82  \\
            \midrule
            GaitBase + Naive CLS  & 95.89 ($\pm$ 0.00)  & 	84.69 ($\pm$ 0.29)  & 	77.65 ($\pm$ 1.29)  & 	69.71 ($\pm$ 0.54)  & 	63.37 ($\pm$ 1.80)  & 	57.36  ($\pm$ 0.65)   \\
            \midrule
            GaitBase + Verification & 96.01 ($\pm$ 0.00)  & 	82.89 ($\pm$ 0.57)  & 	73.78 ($\pm$ 0.66)  &  	64.42 ($\pm$ 0.65)  & 	55.32 ($\pm$ 0.15)  & 	47.02 ($\pm$ 1.06)  \\
            \midrule
            GaitBase + Anomaly Detection & 95.89 ($\pm$ 0.00)  &	84.36 ($\pm$ 0.31)  & 	77.05 ($\pm$ 1.07)  & 	68.75 ($\pm$ 0.36)  &	62.10 ($\pm$ 1.70)  &	55.44 ($\pm$ 0.51)  \\
            \midrule
            GaitBase + Ours & \textbf{95.45 ($\pm$ 0.00)}  &	\textbf{86.75 ($\pm$ 0.75)}  &	\textbf{81.53 ($\pm$ 1.36)}  &	\textbf{76.20 ($\pm$ 0.69)}  &	\textbf{72.43 ($\pm$ 2.45)}  &	\textbf{68.62 ($\pm$ 2.78)}   \\
            \midrule
            \midrule
            DyGait & 97.28 &	84.05 	& 74.94 &	64.41 	& 54.47 &	44.56   \\
            \midrule
            DyGait + Naive CLS  & 97.28 ($\pm$ 0.00)  &	86.22 ($\pm$ 0.32)  &	77.66 ($\pm$ 0.31)  &	70.75 ($\pm$ 0.88)  &	63.67 ($\pm$ 0.75)  &	57.26 ($\pm$ 1.02)  \\
            \midrule
            DyGait + Verification &  42.53 ($\pm$ 0.00)  &	50.95 ($\pm$ 0.29)  &	56.66 ($\pm$ 0.88)  &	62.23 ($\pm$ 1.50)  &	67.79 	($\pm$ 1.10)  & 73.88 ($\pm$ 1.25)   \\
            \midrule
            DyGait + Anomaly Detection & 97.28 ($\pm$ 0.00)  &	86.23 ($\pm$ 0.37)  &	77.68 ($\pm$ 0.33)  &	70.46 ($\pm$ 0.96)  &	63.16 ($\pm$ 0.51)  &	56.29 ($\pm$ 0.76)   \\
            \midrule
            DyGait + Ours & \textbf{95.76 ($\pm$ 0.00)}  &	\textbf{89.38 ($\pm$ 0.35)}  &	\textbf{85.06 ($\pm$ 0.89)}  &	\textbf{83.36 ($\pm$ 1.76)}  &	\textbf{81.66 	($\pm$ 1.42)}  & \textbf{77.98  ($\pm$ 1.48)}  \\
            \bottomrule
        \end{tabular}
    }
    \label{tab_casia}
\end{table*}


\begin{table*}[t]
\footnotesize
    \centering
    \caption{
        Rank-1 accuracy (\%) on the Gait3D dataset under different OOG percentages. The standard deviation is shown in parentheses. For a fair comparison, we join the backbones with different OOG detection strategies, \eg, Naive CLS, Verification and Anomaly Detection. Note that, the default evaluation protocol contains 5\% of the probes which are OOG queries.
    }
    \setlength{\tabcolsep}{3.0mm}{
        \begin{tabular}{l|c|c|c|c|c|c}
            \toprule
            \multicolumn{1}{c|}{\multirow{2}[2]{*}{Methods}} & \multicolumn{5}{c}{OOG percentages} \\
            \cmidrule{2-7}& \multicolumn{1}{c|}{5\%} & \multicolumn{1}{c|}{15\%} & \multicolumn{1}{c|}{25\%} & \multicolumn{1}{c|}{35\%} & \multicolumn{1}{c|}{45\%} & \multicolumn{1}{c}{55\%} \\
            \midrule
            \midrule
            GaitSet   & 55.33 &	40.24 &	32.46 &	27.66 &	21.51 &	17.00  \\
            \midrule
            GaitSet + Naive CLS  & 54.27 ($\pm$ 0.00)  &	43.26 ($\pm$ 1.42)  &	40.12 ($\pm$ 1.40)  &	41.59 	($\pm$ 0.88)  &43.67 ($\pm$ 0.30)  &	45.72 ($\pm$ 1.18)   \\
            \midrule
            GaitSet + Verification & 55.33 	($\pm$ 0.00)  & 42.36 	($\pm$ 1.45)  & 34.38 	($\pm$ 1.02)  & 29.79 	($\pm$ 0.52)  & 26.41 ($\pm$ 1.46)  &	24.46 ($\pm$ 1.76)   \\
            \midrule
            GaitSet + Anomaly Detection & 55.61 ($\pm$ 0.00)  &	43.16 ($\pm$ 0.94)  &	37.78 ($\pm$ 0.73)  &	36.02 ($\pm$ 1.02)  &	35.63 ($\pm$ 0.48)  &	36.53 ($\pm$ 0.97)  \\
            \midrule
            GaitSet + Ours  & \textbf{51.20 ($\pm$ 0.00)}  &	\textbf{39.92 ($\pm$ 0.84)}  & \textbf{38.99 ($\pm$ 0.56)}  &	\textbf{41.82 ($\pm$ 0.93)}  &	\textbf{46.36 ($\pm$ 0.95)}  &	\textbf{51.33 ($\pm$ 0.95)}    \\
            \midrule
            \midrule
            GaitPart   &  43.32 	& 29.49 &	23.72 &	20.17 &	15.75 &	12.48 \\
            \midrule
            GaitPart + Naive CLS  & 39.09 ($\pm$ 0.00)  &	30.86 ($\pm$ 0.51)  &	31.54 ($\pm$ 0.23)  &	33.33 	($\pm$ 1.09)  &37.01 ($\pm$ 0.88)  &	38.97 ($\pm$ 0.66)   \\
            \midrule
            GaitPart + Verification & 43.41 ($\pm$ 0.00)  &	30.93 ($\pm$ 0.90)  &	24.94 ($\pm$ 0.67)  &	21.61 	($\pm$ 0.67)  &19.24($\pm$ 1.10)  & 	17.29 ($\pm$ 0.92)  \\
            \midrule
            GaitPart + Anomaly Detection & 40.63 ($\pm$ 0.00)  &		30.58 ($\pm$ 1.20)  &		29.10 ($\pm$ 0.89)  &		29.26 ($\pm$ 0.71)  &		30.89 ($\pm$ 0.73)  &		30.93 ($\pm$ 0.69)  \\
            \midrule
            GaitPart + Ours  & \textbf{39.86 ($\pm$ 0.00)}  &	\textbf{30.70 ($\pm$ 0.84)}  &	\textbf{31.86 ($\pm$ 0.32)}  &	\textbf{35.83 ($\pm$ 0.55)}  &	\textbf{41.08 ($\pm$ 0.71)}  &	\textbf{45.18 ($\pm$ 1.53)}   \\
            \midrule
            \midrule
            GaitGL   &  73.77  &	56.48  &	48.41  &	40.44 	 & 31.69  &	24.78   \\
            \midrule
            GaitGL + Naive CLS  & 73.87 ($\pm$ 0.00)  &	58.59 ($\pm$ 0.39)  &	53.05 ($\pm$ 0.57)  &	48.22 ($\pm$ 0.36)  &	46.46 ($\pm$ 0.91)  &	45.18 ($\pm$ 1.41)   \\
            \midrule
            GaitGL + Verification & 72.04 ($\pm$ 0.00)  &		56.80 ($\pm$ 0.12)  &		50.24 ($\pm$ 0.68)  &		45.02 ($\pm$ 0.70)  &		42.55 ($\pm$ 1.01)  &		40.28 ($\pm$ 1.01)  \\
            \midrule
            GaitGL + Anomaly Detection & 73.77 ($\pm$ 0.00)  &		57.79 ($\pm$ 0.53)  &		50.75($\pm$ 0.87)  &	 	44.54 ($\pm$ 0.36)  &		40.92 ($\pm$ 1.60)  &		38.48 ($\pm$ 1.00)   \\
            \midrule
            GaitGL + Ours  & \textbf{74.15 ($\pm$ 0.00)}  & 	\textbf{59.49 ($\pm$ 1.58)}  &	\textbf{56.06 ($\pm$ 0.89)}  &	\textbf{53.40 ($\pm$ 1.49)}  &	\textbf{54.05 ($\pm$ 0.82)}  &	\textbf{55.00($\pm$ 0.80)}   \\
            \midrule
            \midrule
            GaitBase   & 75.02 	& 57.15  &	47.74 &	40.82 &	34.48 &	27.56  \\
            \midrule
            GaitBase + Naive CLS  &  73.29 ($\pm$ 0.00)  &	60.13 ($\pm$ 1.44)  &	55.20 	($\pm$ 1.30)  & 54.24($\pm$ 1.18)  & 	55.30 ($\pm$ 0.79)  &	57.05 ($\pm$ 0.77)   \\
            \midrule
            GaitBase + Verification  & 74.92  ($\pm$ 0.00)  &	59.39  ($\pm$ 1.65)  &	49.88  ($\pm$ 1.33)  &	42.26  ($\pm$ 0.55)  &	36.40  ($\pm$ 1.03)  &	32.46  ($\pm$ 1.31)   \\
            \midrule
            GaitBase + Anomaly Detection & 74.92 ($\pm$ 0.00)  &	59.65 ($\pm$ 1.69)  &	51.00 ($\pm$ 1.23)  &	44.38 	($\pm$ 0.47)  &39.73 ($\pm$ 0.53)  &	37.27($\pm$ 1.41)    \\
            \midrule
            GaitBase + Ours  & \textbf{75.31  ($\pm$ 0.00)}  &	\textbf{60.35  ($\pm$ 1.11)}  &	\textbf{56.09  ($\pm$ 1.26)}  &	\textbf{55.52  ($\pm$ 1.54)}  &	\textbf{57.06  ($\pm$ 0.75)}  &	\textbf{59.07  ($\pm$ 0.14)}  \\
            \midrule
            \midrule
            DyGait   &  75.98 &	62.05 &	52.25 &	45.43 &	37.46 &	30.45 \\
            \midrule
            DyGait + Naive CLS  & 76.46 ($\pm$ 0.00)  &		65.96 ($\pm$ 0.92)  &		60.39 ($\pm$ 1.45)  &	 	58.69 ($\pm$ 0.96)  &		59.42 	($\pm$ 0.83)  &	61.54  ($\pm$ 0.16)   \\
            \midrule
            DyGait + Verification  & 72.62 ($\pm$ 0.00)  &		62.18 ($\pm$ 0.73)  &	 	56.51 ($\pm$ 1.20)  &		54.43 ($\pm$ 0.25)  &	 	53.66 ($\pm$ 1.14)  &		53.92  ($\pm$ 0.71)   \\
            \midrule
            DyGait + Anomaly Detection & 76.17 ($\pm$ 0.00)  &		64.90 ($\pm$ 0.74)  &		58.05 ($\pm$ 1.22)  &		54.14 ($\pm$ 1.39)  &		52.42 ($\pm$ 1.89)  &		53.44 ($\pm$ 1.16)    \\
            \midrule
            DyGait + Ours  &  \textbf{76.27 ($\pm$ 0.00)}  &		\textbf{66.92 ($\pm$ 1.63)}  &		\textbf{62.91 ($\pm$ 1.71)}  &		\textbf{62.76 ($\pm$ 0.83)}  &		\textbf{64.48 ($\pm$ 0.71)}  &		\textbf{66.76 ($\pm$ 0.21)}   \\
            \bottomrule
        \end{tabular}
    }
    \label{tab_gait3d}
\end{table*}

\subsection{Comparisons with the State-of-the-Art}

\noindent \textbf{Evaluation on OUMVLP under different OOG percentages:}
We adopt our uncertainty-aware classification model with several state-of-the-art gait recognition models, including GaitGL~\cite{lin2021gait}, GaitPart~\cite{fan2020gaitpart}, GaitSet~\cite{chao2019gaitset}, OpenGait~\cite{fan2023opengait}, and DyGait~\cite{wang2023dygait} on the OUMVLP dataset, and compare their performance with and without employing our method.
In the experiments, we run five gait recognition models on six test sets with different percentages of OOG samples. 
As indicated in Table~\ref{tab:tab1}, after introducing our method, all gait recognition models obtain improvements on the test sets with OOG samples.
As the percentage of OOG samples increases in the test sets, we observe that existing methods suffer severe performance degradation. As expected, when the OOG query rate reaches 50\%, the accuracy of these methods is reduced to nearly half of their original performance, with the accuracy of DyGait dropping from 92.94\% to 44.82\%. This implies that current gait recognition methods do not have the ability to detect OOG probes. In contrast, our uncertainty-aware classification model is able to identify OOG queries and thus avoids assigning gallery IDs to the probes.
As a result, our method maintains high recognition accuracy while successfully recognizing OOG probes, indicating the effectiveness and superiority of the proposed method. Moreover, the small standard deviation of our results demonstrates the stability of our model in recognizing OOG samples.


\noindent \textbf{Evaluation on CASIA-B under different OOG percentages:}
For the CASIA-B dataset, we also choose GaitGL~\cite{lin2021gait}, GaitPart~\cite{fan2020gaitpart}, GaitSet~\cite{chao2019gaitset}, OpenGait~\cite{fan2023opengait}, and DyGait~\cite{wang2023dygait} as our baseline algorithms. Table~\ref{tab_casia} presents the experimental results. It can be observed that our method leads to a slight decrease in performance when OOG samples are not present, with the accuracy of DyGait decreasing from 97.28\% to 95.76\%.
This is because our method is designed to prioritize accuracy in identifying matching pairs, minimizing the risk of incorrect pairings. Consequently, our method achieves promising performance in all OOG scenarios. Specifically, the accuracy of DyGait without our uncertainty model decreases significantly as the percentage of OOG samples increases, with the accuracy of GaitGL dropping from 97.28\% to 44.56\%. Meanwhile, our method consistently outperforms the baseline in all six OOG percentages.


\noindent \textbf{Evaluation on Gait3D under different OOG percentages:}
For the Gait3D dataset, we choose GaitGL~\cite{lin2021gait}, GaitPart~\cite{fan2020gaitpart}, GaitSet~\cite{chao2019gaitset}, OpenGait~\cite{fan2023opengait}, and DyGait~\cite{wang2023dygait} as our baseline backbones. Table~\ref{tab_gait3d} shows the experimental results. It can be observed that our method achieves a significant performance improvement in all cases. For instance, in the 55\% OOG percentage, our method outperforms DyGait by 36.31\%. Although our method still achieves a significant performance improvement in all OOG percentages, it suffers performance degradation in the 5\% OOG percentage for GaitSet and GaitPart backbones. 
Since previous state-of-the-art methods cannot effectively extract discriminative features on the Gait3D dataset, their accuracy remains below 55\%. When the extracted features are not sufficiently discriminative, in handling IG queries, our method tends to be inaccurate. This is because, in such cases, the distance between a matching pair is large, leading to a higher risk. Meanwhile, our network prefers to accept matching pairs with minimal risk.

\noindent \textbf{Evaluation on VERI-Wild and VC-Clothes under different OOG percentages:}
Although our model is designed to tackle OOG queries in gait recognition, it can be easily extended to other tasks, such as vehicle re-identification and person re-identification. 
To further verify the effectiveness of our method on other tasks, we conduct experiments on VERI-Wild~\cite{lou2019large,lou2021large} and VC-Clothes~\cite{wan2020person}.

For the VERI-Wild dataset, we choose the BoT framework~\cite{luo2019bag} as our baseline algorithm.
The experimental results are shown in Table~\ref{tab_vehicle}. It can be observed that our method (BoT + Ours) achieves appealing performance in all cases. To be specific, our method achieves stable recognition accuracy for the five OOG percentages while outperforming the baseline method. In contrast, the accuracy of BoT without our uncertainty model decreases significantly as the percentage of OOG samples increases.
For the VC-Clothes dataset, we employ the CBN framework~\cite{zhuang2020rethinking} as our baseline algorithm. As indicated in Table~\ref{tab_person}, our methods achieve approximately more than 97\% recognition accuracy in all the cases, \ie, OOG percentage ranges from 15\% to 55\%. On the contrary, the recognition accuracy of CBN without our uncertainty model is 44.98\% in the setting of 55\% OOG samples. 
This is because the current methods cannot address OOG queries, and thus their methods only output erroneous recognition results. Note that, when we apply our method to the baseline methods, we do not need to re-train the baselines but treat them as off-the-shelf backbone networks. This also indicates that our method is able to effectively recognize matching feature pairs and OOG queries through its prediction and uncertainty modeling, thus demonstrating the superiority of our method to competing methods.



\begin{table*}[t]
    \footnotesize
    \centering
    \caption{
        Rank-1 accuracy (\%) on the OUMVLP dataset under different OOG percentages. The standard deviation is shown in parentheses. 
        For a fair comparison, we join the backbones with different OOG detection strategies, \eg, Naive CLS, Verification and Anomaly Detection.
    }
    \setlength{\tabcolsep}{2.6mm}{
        \begin{tabular}{c|c|c|c|c|c|c}
            \toprule
            \multicolumn{1}{c|}{\multirow{2}[2]{*}{Methods}} & \multicolumn{6}{c}{OOG percentages} \\
            \cmidrule{2-7} & \multicolumn{1}{c|}{7\%} & \multicolumn{1}{c|}{15\%} & \multicolumn{1}{c|}{25\%} & \multicolumn{1}{c|}{35\%} & \multicolumn{1}{c|}{45\%} & \multicolumn{1}{c}{55\%} \\
            \midrule
            GaitGL & 90.17  & 82.15  & 72.35  & 63.45  & 53.58  & 43.68  \\
            \midrule
            GaitGL + Threshold=0.6 & 90.41 ($\pm$ 0.00) & 84.74 ($\pm$ 0.01)  & 78.05 ($\pm$ 0.03)  & 72.30 ($\pm$ 0.03) & 66.46 ($\pm$ 0.12) &  61.23 ($\pm$ 0.12) \\
            \midrule   
            GaitGL + Threshold=0.7 & 73.08 ($\pm$ 0.00) & 74.21 ($\pm$ 0.02)  & 75.72 ($\pm$ 0.03)  & 77.24 ($\pm$ 0.04) & 79.21 ($\pm$ 0.07) &  81.35 ($\pm$ 0.06) \\
            \midrule   
            GaitGL + Threshold=0.8 & 27.34 ($\pm$ 0.00) & 33.87 ($\pm$ 0.02)  & 41.83 ($\pm$ 0.02)  & 49.07 ($\pm$ 0.00) & 57.09 ($\pm$ 0.03) &  65.07 ($\pm$ 0.05) \\
            \midrule   
            GaitGL + Threshold=0.9 & 9.42 ($\pm$ 0.00) & 17.56 ($\pm$ 0.01)  & 27.52 ($\pm$ 0.01)  & 36.57 ($\pm$ 0.02) & 46.56 ($\pm$ 0.01) &  56.51 ($\pm$ 0.01) \\
            \midrule       
            GaitGL + Naive CLS & 92.27 ($\pm$ 0.00) & 88.81 ($\pm$ 0.01)  & 84.99 ($\pm$ 0.03)  & 82.02 ($\pm$ 0.05) &  79.51 ($\pm$ 0.05) &  77.87 ($\pm$ 0.08)  \\
            \midrule
            GaitGL + Verification & 90.47 ($\pm$ 0.00) & 83.06 ($\pm$ 0.00)  & 74.14 ($\pm$ 0.02)  & 66.19 ($\pm$ 0.02) & 57.67 ($\pm$ 0.06) &  49.52 ($\pm$ 0.07) \\
            \midrule
            GaitGL + Anomaly Detection & 74.07 ($\pm$ 0.00) & 75.63 ($\pm$ 0.02)  & 77.58 ($\pm$ 0.04)  & 79.42 ($\pm$ 0.03) & 81.57 ($\pm$ 0.02) &  83.79 ($\pm$ 0.02) \\
            \midrule
            GaitGL + Ours & \textbf{91.52 ($\pm$ 0.00)}  & \textbf{90.27 ($\pm$ 0.01)}  & \textbf{89.03 ($\pm$ 0.03)} & \textbf{88.21 ($\pm$ 0.04)} & \textbf{87.82 ($\pm$ 0.02)} & \textbf{87.87 ($\pm$ 0.05)} \\
            \midrule
            \midrule
            DyGait &   92.94  &	84.96 &	74.68 &	64.97 	& 54.98 &	44.82   \\
            \midrule   
            DyGait + Threshold=0.6 & 92.94  ($\pm$ 0.00) &	84.96 ($\pm$ 0.00) &	74.68 ($\pm$ 0.00) &	64.97 	($\pm$ 0.00) & 54.98 ($\pm$ 0.00) &	44.82 ($\pm$ 0.00)   \\
            \midrule   
            DyGait + Threshold=0.7 &  93.36 ($\pm$ 0.00) &	85.00 ($\pm$ 0.01) &	74.80 ($\pm$ 0.02) &	65.54 ($\pm$ 0.03) &	55.39 ($\pm$ 0.01) &	45.33  ($\pm$ 0.02) \\
            \midrule   
            DyGait + Threshold=0.8 & 97.91 ($\pm$ 0.00) &	95.72 ($\pm$ 0.06) &	93.32 ($\pm$ 0.04) &	91.39 ($\pm$ 0.02) &	89.69 ($\pm$ 0.05) &	88.53 ($\pm$ 0.04) \\
            \midrule   
            DyGait + Threshold=0.9 & 31.50 ($\pm$ 0.00) &	37.64 ($\pm$ 0.03) &	45.19 ($\pm$ 0.02) &	52.03 ($\pm$ 0.05) &	59.57 ($\pm$ 0.03) &	67.09 ($\pm$ 0.05)  \\
            \midrule       
            DyGait + Naive CLS & 97.30 ($\pm$ 0.00) &	94.48 ($\pm$ 0.05) & 91.51 ($\pm$ 0.01) &	89.29 ($\pm$ 0.03) &	87.48 ($\pm$ 0.07) &	86.61 ($\pm$ 0.04)  \\
            \midrule
            DyGait + Verification & 97.28 ($\pm$ 0.00) &	94.19 ($\pm$ 0.06) &	90.74 ($\pm$ 0.05) & 87.89 ($\pm$ 0.05) &	85.28 ($\pm$ 0.06) &	83.41 ($\pm$ 0.08) \\
            \midrule
            DyGait + Anomaly Detection & 95.60 ($\pm$ 0.00) &	90.61 ($\pm$ 0.05) &	85.11 ($\pm$ 0.04) &	80.72 ($\pm$ 0.07) &	76.83 ($\pm$ 0.02) &	74.29 ($\pm$ 0.04) \\
            \midrule
            DyGait + Ours & \textbf{99.00 ($\pm$ 0.00)} &	\textbf{98.13 ($\pm$ 0.04)} &	\textbf{97.26 ($\pm$ 0.01)} &	\textbf{96.65 ($\pm$ 0.02)} &	\textbf{96.20 ($\pm$ 0.03)}	& \textbf{96.08  ($\pm$ 0.02)} \\
            \bottomrule
        \end{tabular}
    }
    \label{tab_threshold}
\end{table*}



\begin{table}[t]
    \footnotesize
    \centering
    \caption{
        Rank-1 accuracy (\%) on the VERI-Wild dataset under different OOG percentages. The standard deviation is reported in parentheses. SS, MS and LS indicate 3K, 5K and 10K training IDs, respectively.
    }
    \resizebox{0.48\textwidth}{!}{
    \begin{tabular}{c|c|c|c|c|c|c}
        \toprule
        \multirow{2}[2]{*}{Setting} & \multirow{2}[2]{*}{Methods} & \multicolumn{5}{c}{OOG percentages} \\
        \cmidrule{3-7}          &       & \multicolumn{1}{c|}{15\%} & \multicolumn{1}{c|}{25\%} & \multicolumn{1}{c|}{35\%} & \multicolumn{1}{c|}{45\%} & \multicolumn{1}{c}{55\%} \\
        \midrule
        \multirow{2}[2]{*}{SS} & BoT   & 74.07  & 66.02  & 57.86  & 49.40  & 40.90  \\
        \cmidrule{2-7}          & BoT + Naive CLS & 69.26  & 66.76  & 64.81  &  63.25 &  62.82  \\
        \cmidrule{2-7}          & BoT + Verification  & 74.10   & 66.36  & 58.99  &  52.01 &  46.24 \\
        \cmidrule{2-7}          & BoT + Ours & \textbf{78.19} & \textbf{76.34} & \textbf{75.09} & \textbf{74.71} & \textbf{75.34} \\
        \midrule
        \midrule
        \multirow{2}[2]{*}{MS} & BoT   & 71.00  & 63.64  & 55.83  & 47.73  & 39.87  \\
        \cmidrule{2-7}          & BoT + Naive CLS  & 64.81   & 61.65 & 59.33  &  57.17  &  55.84 \\
        \cmidrule{2-7}          & BoT + Verification & 70.85   & 63.34  & 55.86 &  48.17  &  41.04 \\
        \cmidrule{2-7}          & BoT + Ours & \textbf{73.70} & \textbf{70.85} & \textbf{68.97} & \textbf{67.65} & \textbf{67.36} \\
        \midrule
        \midrule
        \multirow{2}[2]{*}{LS} & BoT   & 65.58  & 58.65  & 51.54  & 44.15  & 36.82  \\
        \cmidrule{2-7}          & BoT + Naive CLS & 59.25  & 56.28 & 53.39  &  50.99 &  49.15  \\
        \cmidrule{2-7}          & BoT + Verification  & 65.43   & 58.63  & 51.57  &  44.40  &  37.14 \\
        \cmidrule{2-7}          & BoT + Ours & \textbf{67.45} & \textbf{64.14} & \textbf{61.25} & \textbf{59.07} & \textbf{57.92} \\
        \bottomrule
        \end{tabular}
    }
    \label{tab_vehicle}
\end{table}

\begin{table}[t]
    \footnotesize
    \centering
    \caption{
        Rank-1 accuracy (\%) on the VC-Clothes dataset under different OOG percentages. The standard deviation is reported in parentheses.
    }
    \resizebox{0.48\textwidth}{!}{
    \begin{tabular}{c|c|c|c|c|c}
        \toprule
        \multirow{2}[2]{*}{Methods} & \multicolumn{5}{c}{OOG percentages}  \\
        \cmidrule{2-6}          & \multicolumn{1}{c|}{15\%} & \multicolumn{1}{c|}{25\%} & \multicolumn{1}{c|}{35\%} & \multicolumn{1}{c|}{45\%} & \multicolumn{1}{c}{55\%}  \\
        \midrule
        \multirow{1}[1]{*}{CBN}  & 83.91  & 73.67  & 64.10  & 54.34  & 44.98   \\
        \midrule
        \multirow{1}[1]{*}{CBN + Verification}  & 89.68 & 90.71  & 91.72 & 92.90  & 93.95    \\
        \midrule
        \multirow{1}[1]{*}{CBN + Threshold}  & 87.96  & 80.93 & 75.74 & 72.00  & 69.17 \\
        \midrule
        \multirow{1}[1]{*}{CBN + Ours} & \textbf{97.11} & \textbf{97.15} & \textbf{97.29} & \textbf{97.58} & \textbf{97.70}  \\
        \bottomrule
        \end{tabular}
    }
    \label{tab_person}
\end{table}


\subsection{Ablation Study} \label{Ablation_Study} 
To verify the effectiveness of the proposed components, \ie, the uncertainty modeling and query construction, we conduct ablation studies on the OUMVLP dataset~\cite{takemura2018multi}. 

\subsubsection{Effectiveness of the Uncertainty Modeling}
% \noindent \textbf{Effectiveness of the uncertainty modeling:}
Since existing backbones cannot handle OOG cases, they cannot reach 100\% accuracy in all OOG settings.
For a fair comparison, we introduce different OOG detection strategies and join them with the gait backbone to construct baseline algorithms. As a result, the upper bound accuracy of the constructed baseline algorithms is 100\%. The details of baseline algorithms are as follows: 

\noindent \textbf{Backbone + Naive CLS:} An NN classifier is designed to classify whether two features are from the same identity or not. The classifier is trained on the constructed pairs using binary cross-entropy loss as the supervision. The difference between this classifier and our model is that our model has uncertainty modeling. Thus, our model not only classifies whether two features are from the same identity but also provides the uncertainty estimation of the prediction by a Dirichlet distribution parameterized over all the constructed training pairs, while a simple classifier only makes decisions based on its predicted labels since it only learns a classification boundary.
As shown in Table~\ref{tab:tab1}, without any mechanism to distinguish OOG queries, the performance degrades as the number of OOG samples increases. 
Although a simple classifier slightly outperforms our method in the 7\% OOG percentage, we can see that as the percentage of OOG queries increases, the performance of the simple classifier suffers significant degradation. This is because a simple classifier tends to overfit the IG pairs. Thus, it may produce inaccurate predictions, when the input probe are OOG quires.


\noindent \textbf{Backbone + Verification:} Setting a similarity threshold is another straightforward way to address OOG cases. When the similarity between the probe and the closet sample from the gallery is smaller than the threshold, the probe will be regarded as an OOG query. In our cases, the threshold is calculated by Equal Error Rate (EER) from the verification setting. To be specific, we construct 20k gait pairs, including 10k paired samples and 10k unpaired samples. Then, we search for the best threshold leading to the lowest EER. Finally, the searched threshold is used to further determine whether two samples are from the same identity.
We will release the constructed gait pairs for reproducibility. 
Table~\ref{tab:tab1} shows that the verification-based methods address a few OOG cases, leading to a slight performance improvement. For example, in the 15\% OOG setting, ``DyGait + Verification'' outperforms DyGait by 9.23\% on OUMVLP. However, the verification-based methods cannot handle most OOG cases. This is because most OOG queries are hard samples. Here, ``hard'' means the OOG query is very similar to the closet sample from the gallery set. In other words, its similarity is higher than the given threshold, leading to erroneous results.
% Moreover, we manually search all thresholds for feature matching. The experimental results can be found in the supplementary material.

\noindent \textbf{Backbone + Anomaly Detection:} One-Class Novelty Detection~\cite{yang2021generalized} is proposed to address OOG cases in recognition. It aims to classify all OOG cases into a single class. Although it cannot be used directly in our task, we combine it with an NN classifier to detect our OOG cases. Specifically, we randomly construct IG and OOG feature pairs to train an NN classifier. The trained classifier is then employed in feature matching to determine whether the retrieved gait pairs have the same identity or not. As shown in Table~\ref{tab:tab1}, one-class novelty detection effectively addresses a few OOG cases. For example, ``DyGait + Anomaly Detection'' outperforms DyGait by 2.66\% when the OOG query rate is 5\% on OUMVLP. However, as the OOG rate increases to 45\% and 55\%, its performance declines significantly. This is because one-class novelty detection tends to classify an IG pair that has a large distance to OOG cases. As a result, the accuracy in low OOG percentage shows a significant decrease.
In contrast, our method maintains recognition performance even though the OOG sample rate is more than 50\%. Our method successfully identifies OOG queries and assigns correct identities to IG queries. We conduct experiments three times on different gallery sets. The small standard deviation values prove the stability of our method.

\noindent \textbf{Impacts of different thresholds: }
For a fair comparison, we introduce different OOG detection strategies. One of our used strategies is to set a similarity threshold. If the similarity between the probe and the closet sample from the gallery is smaller than the threshold, the probe will be regarded as an OOG query. In this paper, we introduce a verification setting to automatically calculate the threshold. Obviously, the threshold can be set manually. Thus, we conduct experiments on OUMVLP by manually searching all thresholds. The experimental results are shown in Table~\ref{tab_threshold}. When the given threshold is lower than 0.6, the threshold-based method cannot detect any OOG cases. Thus, the accuracy in these threshold settings is the same as the accuracy using only the backbone. We also observe that a low threshold for GaitGL achieves appealing performance when the OOG percentage is less than 25\%, but it cannot effectively address the condition with higher OOG percentages. In contrast, a high threshold achieves promising performance in high OOG percentages ($\geq$ 35\%). For the DyGait backbone, we find that only ``Threshold = 0.8'' achieves promising performance, while other threshold settings lead to significant performance degradation.. These results indicate that it is difficult to choose a suitable threshold for different OOG percentages. Thus, the threshold-based methods are impractical. Furthermore, it can be found that our method outperforms the threshold-based methods in all OOG percentages and the recognition accuracy in all OOG percentages is stable. We believe that the proposed method is a more practical approach to addressing OOG queries in a real scene.


\begin{figure}[t] 
    \centering
    \includegraphics[width=0.95\linewidth]{images/querypair_construction_ablationstudy.pdf}
    \caption{Impacts of different query pair construction on training our model. We test the models with different percentages of OOG queries.}
    \label{fig:ablation_qc}
\end{figure}




\subsubsection{Effectiveness of the Query Pair Construction}
% \noindent \textbf{Impacts of different query pair construction:}
We propose an OOG query pair construction that allows our network to grasp the idea of IG and OOG residual features. 
We also compare three different strategies: randomly selecting feature pairs, dataset-based hard negative and positive feature pairs, and batch-based hard negative and positive feature pairs. All experiments are conducted on OUMVLP and we take GaitGL as our backbone.
As indicated in Figure~\ref{fig:ablation_qc}, both random selection and batch-based hard negative and positive feature pair mining strategies improve the recognition performance on OOG cases in testing.
Note that, as the percentage of the OOG queries increases, the performance of the model trained with randomly selected feature pairs decreases. This is mainly because this pair construction may not effectively capture the most uncertain cases for model training.
The dataset-based mining strategy does not facilitate the representation of the distribution of the residual features. 
In most cases, the negative feature pairs from the dataset-based mining strategy are hard to classify, and thus our classifier will exert more effort to classify the negative pairs after training while sacrificing the classification accuracy of positive pairs. 
% This is the reason its performance increases when the percentage of OOG samples increases.
Mining in a batch-based pair will significantly improve the variety of training samples, thus leading to better recognition performance.
Thanks to our feature pair construction, we can effectively capture the latent distribution of residual features, thus achieving accurate detection of OOG samples.

\section{Discussion, Limitation and Extension} \label{Discussion} 
{In this section, we further analyze the advantages and limitations of EMA-GR. Additionally, we extend our method to other tasks, \ie, vehicle re-identification and person re-identification.}

\noindent \textbf{Performance of the default evaluation protocol:} In this paper, we combine the backbone networks with different OOG detection strategies, including Naive CLS, Verification, and Anomaly Detection. Experimental results are reported in Table~\ref{tab:tab1}, Table~\ref{tab_casia} and Table~\ref{tab_gait3d}, respectively. Our method achieves a significant performance improvement in all OOG percentages, but it also suffers slight performance degradation in the IG scenario. 
For instance, under the default evaluation protocol on the OUMVLP dataset, which includes 7\% OOG queries, the accuracy of GaitGL combined with our method is 91.52\%, slightly lower than the 92.27\% accuracy achieved by GaitGL with the Naive CLS. We hypothesize that the primary reason for this discrepancy lies in the insufficiently discriminative feature space of the three backbones (\ie, GaitSet, GaitPart, and GaitGL) across different subjects. Specifically, the lack of sufficient margins or distance between features for some challenging cases results in these samples being rejected by our method.
% For example, in the default evaluation protocol on the OUMVLP dataset (with 7\% OOG queries), the accuracy of GaitGL with our methods (91.52\%) is lower than the accuracy of GaitGL with the Naive CLS (92.27\%). 
% We speculate the main reason is that the feature space of the three backbones (GaitSet, GaitPart and GaitGL) among different subjects is not discriminative enough (without sufficient margins/distance) for some hard cases and thus those samples will be rejected by our method. 
Since our method prefers to accept matching pairs with minimal risk, these high-risk predictions are rejected and the overall accuracy drops in the IG case.



\noindent \textbf{Performance across different datasets:} We conduct experiments on three popular used datasets, \ie, OUMVLP~\cite{takemura2018multi}, CASIA-B~\cite{yu2006framework}, and Gait3D~\cite{zheng2022gait}, to verify the effectiveness of our uncertainty gait recognition framework. Table~\ref{tab:tab1}, Table~\ref{tab_casia} and Table~\ref{tab_gait3d} demonstrate state-of-the-art backbones with our methods achieving superior performance in all OOG percentages.
Additionally, for the default setting on the OUMVLP dataset, DyGait, with our method, achieves an accuracy of 99.00\%. When the OOG percentage increases to 55\%, our method still obtains 96.08\% Rank-1 accuracy. The performance degradation on the OUMVLP dataset is only 3\%. However, there is an obvious performance degradation on the CASIA-B dataset when the OOG percentage increases from 0\% to 55\%. We speculate that our method trained on a small dataset, like CASIA-B, which only contains 74 subjects for training, may not learn sufficiently discriminative features and thus suffer a performance drop in high OOG percentage. 


For the Gait3D dataset, it is observed that the performance degrades about 10\% as OOG percentage increases from 5\% to 55\%. The main reason is that existing methods cannot effectively extract discriminative features on Gait3D (the accuracy of these methods is lower than 50\%). When the extracted features are not discriminative in handling IG queries, we do not expect these features to be robust enough to address OOG queries.


\noindent \textbf{Extension to other tasks:} To verify the applicability of our method to other tasks, we also conduct experiments on vehicle re-identification benchmark VERI-Wild~\cite{lou2019large,lou2021large} and synthetic person-identification benchmark VC-Clothes~\cite{wan2020person}. The experimental details and results can be found in Table~\ref{tab_vehicle} and Table~\ref{tab_person}. These experiments demonstrate our uncertainty-aware model is able to effectively recognize matching feature pairs and OOG queries through its prediction and uncertainty modeling, thus demonstrating the superiority of our method to the state-of-the-art methods. 







\section{Conclusion}

In this paper, we propose an Evidence-based Match-status-Aware Gait Recognition (EMA-GR) framework that can effectively identify whether a probe sample is out of the gallery samples. 
To the best of our knowledge, EMA-GR is the first attempt to endow gait recognition with the capability to address Out-of-Gallery (OOG) queries via uncertainty modeling.
Furthermore, the proposed method is a unified framework that is not only able to address OOG query samples but also to successfully recognize identities of the In-Gallery (IG) samples.
Since EMA-GR takes extracted gait features as input, it can be applied to various gait recognition networks, and thus agnostic against backbone networks.
More importantly, EMA-GR can be generalized to other recognition tasks, such as vehicle and person re-identification, and improves the robustness of OOG queries, which frequently occur in practice.





% if have a single appendix:
%\appendix[Proof of the Zonklar Equations]
% or
%\appendix  % for no appendix heading
% do not use \section anymore after \appendix, only \section*
% is possibly needed

% use appendices with more than one appendix
% then use \section to start each appendix
% you must declare a \section before using any
% \subsection or using \label (\appendices by itself
% starts a section numbered zero.)
%


% \appendices
% \section{Proof of the First Zonklar Equation}
% Appendix one text goes here.

% % you can choose not to have a title for an appendix
% % if you want by leaving the argument blank
% \section{}
% Appendix two text goes here.


% use section* for acknowledgment
\ifCLASSOPTIONcompsoc
  % The Computer Society usually uses the plural form
  \section*{Acknowledgments}
\else
  % regular IEEE prefers the singular form
  \section*{Acknowledgment}
\fi
% The authors would like to thank the anonymous reviewers and the associate editor for their helpful suggestions and valuable comments.
% This work was supported by the National Natural Science Foundation of China (61976017 and 61601021), the Beijing Natural Science Foundation (4202056), the Fundamental Research Funds for the Central Universities (2022JBMC013) and the Australian Research Council (DP220100800, DE230100477). The support and resources from the Center for High Performance Computing at Beijing Jiaotong University (http://hpc.bjtu.edu.cn) are gratefully acknowledged. 

This work was supported by the ARC-Discovery grant (DP220100800) and ARC-DECRA grant (DE230100477), the National Natural Science Foundation of China (61976017 and 61601021), the Beijing Natural Science Foundation (4202056), the Fundamental Research Funds for the Central Universities (2022JBMC013). The support and resources from the Center for High Performance Computing at Beijing Jiaotong University (http://hpc.bjtu.edu.cn) are gratefully acknowledged.



% Can use something like this to put references on a page
% by themselves when using endfloat and the captionsoff option.
\ifCLASSOPTIONcaptionsoff
  \newpage
\fi



% trigger a \newpage just before the given reference
% number - used to balance the columns on the last page
% adjust value as needed - may need to be readjusted if
% the document is modified later
%\IEEEtriggeratref{8}
% The "triggered" command can be changed if desired:
%\IEEEtriggercmd{\enlargethispage{-5in}}

% references section

% can use a bibliography generated by BibTeX as a .bbl file
% BibTeX documentation can be easily obtained at:
% http://mirror.ctan.org/biblio/bibtex/contrib/doc/
% The IEEEtran BibTeX style support page is at:
% http://www.michaelshell.org/tex/ieeetran/bibtex/
% \bibliographystyle{IEEEtran}
% argument is your BibTeX string definitions and bibliography database(s)
\bibliographystyle{IEEEtran}
% \bibliography{IEEEabrv,../bib/paper}
\bibliography{egbib}

% \vspace{-1.0cm}

% \begin{IEEEbiography}[{\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{Fig/Lin.jpg}}]{Beibei Lin} received the B.E. and M.S. degrees from Beijing Union University in 2018 and Beijing Jiaotong University in 2021, respectively. His research interests include computer vision, pattern recognition, and image and video processing. 
% \end{IEEEbiography}

% \vspace{-1.0cm}
% \begin{IEEEbiography}[{\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{Fig/zhang.jpg}}]{Shunli Zhang} received the B.S. and M.S. degrees in electronics and information engineering from Shandong University, Jinan, China, in 2008 and 2011, respectively, and the Ph.D. degree in signal and information processing from Tsinghua University in 2016. He is currently an Associate Professor with the School of Software Engineering, Beijing Jiaotong University. His research interests include pattern recognition, computer vision, and image processing. 
% \end{IEEEbiography}

% \vspace{-1.0cm}
% \begin{IEEEbiography}[{\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{Fig/Wang.jpg}}]{Ming Wang} received the B.E. degree from Qingdao University in 2021. She is currently a graduate student in Beijing Jiaotong University. Her research interests include computer vision and pattern recognition.
% \end{IEEEbiography}

% \vspace{-1.0cm}
% \begin{IEEEbiography}[{\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{Fig/Li.jpg}}]{Lincheng Li} received the B.S. and Ph.D. degree in electronic engineering from Tsinghua University, Beijing, China, in 2011 and 2017 respectively. He is currently a researcher in Netease Fuxi AI Lab, Hangzhou, China. His research interests include computer vision, pattern recognition, and image and video processing.
% \end{IEEEbiography}

% % \vspace{-3.0cm}


% \vspace{-1.0cm}
% \begin{IEEEbiography}[{\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{Fig/Yu.jpg}}]{Xin Yu} received the B.S. degree in electronic engineering from the University of Electronic Science and Technology of China, Chengdu, China, in 2009, the Ph.D. degree from the Department of Electronic Engineering, Tsinghua University, Beijing, China, in 2015, and the Ph.D. degree from the College of Engineering and Computer Science, Australian National University, Canberra, Australia, in 2019. He is currently a Senior Lecturer at the University of Queensland. His research interests include computer vision and image processing. 
% \end{IEEEbiography}



%
% <OR> manually copy in the resultant .bbl file
% set second argument of \begin to the number of references
% (used to reserve space for the reference number labels box)
% \begin{thebibliography}{1}

% \bibitem{IEEEhowto:kopka}
% H.~Kopka and P.~W. Daly, \emph{A Guide to \LaTeX}, 3rd~ed.\hskip 1em plus
%   0.5em minus 0.4em\relax Harlow, England: Addison-Wesley, 1999.

% \end{thebibliography}

% biography section
% 
% If you have an EPS/PDF photo (graphicx package needed) extra braces are
% needed around the contents of the optional argument to biography to prevent
% the LaTeX parser from getting confused when it sees the complicated
% \includegraphics command within an optional argument. (You could create
% your own custom macro containing the \includegraphics command to make things
% simpler here.)
% \begin{IEEEbiography}[{\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{mshell}}]{Michael Shell}
% or if you just want to reserve a space for a photo:

% \begin{IEEEbiography}{Michael Shell}
% Biography text here.
% \end{IEEEbiography}

% % if you will not have a photo at all:
% \begin{IEEEbiographynophoto}{John Doe}
% Biography text here.
% \end{IEEEbiographynophoto}

% % insert where needed to balance the two columns on the last page with
% % biographies
% %\newpage

% \begin{IEEEbiographynophoto}{Jane Doe}
% Biography text here.
% \end{IEEEbiographynophoto}

% You can push biographies down or up by placing
% a \vfill before or after them. The appropriate
% use of \vfill depends on what kind of text is
% on the last page and whether or not the columns
% are being equalized.

%\vfill

% Can be used to pull up biographies so that the bottom of the last one
% is flush with the other column.
%\enlargethispage{-5in}



% that's all folks
\end{document}


