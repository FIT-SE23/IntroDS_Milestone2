
%% bare_jrnl_compsoc.tex
%% V1.4b
%% 2015/08/26
%% by Michael Shell
%% See:
%% http://www.michaelshell.org/
%% for current contact information.
%%
%% This is a skeleton file demonstrating the use of IEEEtran.cls
%% (requires IEEEtran.cls version 1.8b or later) with an IEEE
%% Computer Society journal paper.
%%
%% Support sites:
%% http://www.michaelshell.org/tex/ieeetran/
%% http://www.ctan.org/pkg/ieeetran
%% and
%% http://www.ieee.org/
 
%%*************************************************************************
%% Legal Notice:
%% This code is offered as-is without any warranty either expressed or
%% implied; without even the implied warranty of MERCHANTABILITY or
%% FITNESS FOR A PARTICULAR PURPOSE! 
%% User assumes all risk.
%% In no event shall the IEEE or any contributor to this code be liable for
%% any damages or losses, including, but not limited to, incidental,
%% consequential, or any other damages, resulting from the use or misuse
%% of any information contained here.
%%
%% All comments are the opinions of their respective authors and are not
%% necessarily endorsed by the IEEE.
%%
%% This work is distributed under the LaTeX Project Public License (LPPL)
%% ( http://www.latex-project.org/ ) version 1.3, and may be freely used,
%% distributed and modified. A copy of the LPPL, version 1.3, is included
%% in the base LaTeX documentation of all distributions of LaTeX released
%% 2003/12/01 or later.
%% Retain all contribution notices and credits.
%% ** Modified files should be clearly indicated as such, including  **
%% ** renaming them and changing author support contact information. **
%%*************************************************************************


% *** Authors should verify (and, if needed, correct) their LaTeX system  ***
% *** with the testflow diagnostic prior to trusting their LaTeX platform ***
% *** with production work. The IEEE's font choices and paper sizes can   ***
% *** trigger bugs that do not appear when using other class files.       ***                          ***
% The testflow support page is at:
% http://www.michaelshell.org/tex/testflow/


\documentclass[10pt,journal,compsoc]{IEEEtran}
%
% If IEEEtran.cls has not been installed into the LaTeX system files,
% manually specify the path to it like:
% \documentclass[10pt,journal,compsoc]{../sty/IEEEtran}





% Some very useful LaTeX packages include:
% (uncomment the ones you want to load)


% *** MISC UTILITY PACKAGES ***
%
%\usepackage{ifpdf}
% Heiko Oberdiek's ifpdf.sty is very useful if you need conditional
% compilation based on whether the output is pdf or dvi.
% usage:
% \ifpdf
%   % pdf code
% \else
%   % dvi code
% \fi
% The latest version of ifpdf.sty can be obtained from:
% http://www.ctan.org/pkg/ifpdf
% Also, note that IEEEtran.cls V1.7 and later provides a builtin
% \ifCLASSINFOpdf conditional that works the same way.
% When switching from latex to pdflatex and vice-versa, the compiler may
% have to be run twice to clear warning/error messages.






% *** CITATION PACKAGES ***
%
\ifCLASSOPTIONcompsoc
  % IEEE Computer Society needs nocompress option
  % requires cite.sty v4.0 or later (November 2003)
  \usepackage[nocompress]{cite}
\else
  % normal IEEE
  \usepackage{cite}
\fi
% cite.sty was written by Donald Arseneau
% V1.6 and later of IEEEtran pre-defines the format of the cite.sty package
% \cite{} output to follow that of the IEEE. Loading the cite package will
% result in citation numbers being automatically sorted and properly
% "compressed/ranged". e.g., [1], [9], [2], [7], [5], [6] without using
% cite.sty will become [1], [2], [5]--[7], [9] using cite.sty. cite.sty's
% \cite will automatically add leading space, if needed. Use cite.sty's
% noadjust option (cite.sty V3.8 and later) if you want to turn this off
% such as if a citation ever needs to be enclosed in parenthesis.
% cite.sty is already installed on most LaTeX systems. Be sure and use
% version 5.0 (2009-03-20) and later if using hyperref.sty.
% The latest version can be obtained at:
% http://www.ctan.org/pkg/cite
% The documentation is contained in the cite.sty file itself.
%
% Note that some packages require special options to format as the Computer
% Society requires. In particular, Computer Society  papers do not use
% compressed citation ranges as is done in typical IEEE papers
% (e.g., [1]-[4]). Instead, they list every citation separately in order
% (e.g., [1], [2], [3], [4]). To get the latter we need to load the cite
% package with the nocompress option which is supported by cite.sty v4.0
% and later. Note also the use of a CLASSOPTION conditional provided by
% IEEEtran.cls V1.7 and later.





% *** GRAPHICS RELATED PACKAGES ***
%
\ifCLASSINFOpdf
  % \usepackage[pdftex]{graphicx}
  % declare the path(s) where your graphic files are
  % \graphicspath{{../pdf/}{../jpeg/}}
  % and their extensions so you won't have to specify these with
  % every instance of \includegraphics
  % \DeclareGraphicsExtensions{.pdf,.jpeg,.png}
\else
  % or other class option (dvipsone, dvipdf, if not using dvips). graphicx
  % will default to the driver specified in the system graphics.cfg if no
  % driver is specified.
  % \usepackage[dvips]{graphicx}
  % declare the path(s) where your graphic files are
  % \graphicspath{{../eps/}}
  % and their extensions so you won't have to specify these with
  % every instance of \includegraphics
  % \DeclareGraphicsExtensions{.eps}
\fi
% graphicx was written by David Carlisle and Sebastian Rahtz. It is
% required if you want graphics, photos, etc. graphicx.sty is already
% installed on most LaTeX systems. The latest version and documentation
% can be obtained at: 
% http://www.ctan.org/pkg/graphicx
% Another good source of documentation is "Using Imported Graphics in
% LaTeX2e" by Keith Reckdahl which can be found at:
% http://www.ctan.org/pkg/epslatex
%
% latex, and pdflatex in dvi mode, support graphics in encapsulated
% postscript (.eps) format. pdflatex in pdf mode supports graphics
% in .pdf, .jpeg, .png and .mps (metapost) formats. Users should ensure
% that all non-photo figures use a vector format (.eps, .pdf, .mps) and
% not a bitmapped formats (.jpeg, .png). The IEEE frowns on bitmapped formats
% which can result in "jaggedy"/blurry rendering of lines and letters as
% well as large increases in file sizes.
%
% You can find documentation about the pdfTeX application at:
% http://www.tug.org/applications/pdftex






% *** MATH PACKAGES ***
%
%\usepackage{amsmath}
% A popular package from the American Mathematical Society that provides
% many useful and powerful commands for dealing with mathematics.
%
% Note that the amsmath package sets \interdisplaylinepenalty to 10000
% thus preventing page breaks from occurring within multiline equations. Use:
%\interdisplaylinepenalty=2500
% after loading amsmath to restore such page breaks as IEEEtran.cls normally
% does. amsmath.sty is already installed on most LaTeX systems. The latest
% version and documentation can be obtained at:
% http://www.ctan.org/pkg/amsmath





% *** SPECIALIZED LIST PACKAGES ***
%
%\usepackage{algorithmic}
% algorithmic.sty was written by Peter Williams and Rogerio Brito.
% This package provides an algorithmic environment fo describing algorithms.
% You can use the algorithmic environment in-text or within a figure
% environment to provide for a floating algorithm. Do NOT use the algorithm
% floating environment provided by algorithm.sty (by the same authors) or
% algorithm2e.sty (by Christophe Fiorio) as the IEEE does not use dedicated
% algorithm float types and packages that provide these will not provide
% correct IEEE style captions. The latest version and documentation of
% algorithmic.sty can be obtained at:
% http://www.ctan.org/pkg/algorithms
% Also of interest may be the (relatively newer and more customizable)
% algorithmicx.sty package by Szasz Janos:
% http://www.ctan.org/pkg/algorithmicx




% *** ALIGNMENT PACKAGES ***
%
%\usepackage{array}
% Frank Mittelbach's and David Carlisle's array.sty patches and improves
% the standard LaTeX2e array and tabular environments to provide better
% appearance and additional user controls. As the default LaTeX2e table
% generation code is lacking to the point of almost being broken with
% respect to the quality of the end results, all users are strongly
% advised to use an enhanced (at the very least that provided by array.sty)
% set of table tools. array.sty is already installed on most systems. The
% latest version and documentation can be obtained at:
% http://www.ctan.org/pkg/array


% IEEEtran contains the IEEEeqnarray family of commands that can be used to
% generate multiline equations as well as matrices, tables, etc., of high
% quality.




% *** SUBFIGURE PACKAGES ***
%\ifCLASSOPTIONcompsoc
%  \usepackage[caption=false,font=footnotesize,labelfont=sf,textfont=sf]{subfig}
%\else
%  \usepackage[caption=false,font=footnotesize]{subfig}
%\fi
% subfig.sty, written by Steven Douglas Cochran, is the modern replacement
% for subfigure.sty, the latter of which is no longer maintained and is
% incompatible with some LaTeX packages including fixltx2e. However,
% subfig.sty requires and automatically loads Axel Sommerfeldt's caption.sty
% which will override IEEEtran.cls' handling of captions and this will result
% in non-IEEE style figure/table captions. To prevent this problem, be sure
% and invoke subfig.sty's "caption=false" package option (available since
% subfig.sty version 1.3, 2005/06/28) as this is will preserve IEEEtran.cls
% handling of captions.
% Note that the Computer Society format requires a sans serif font rather
% than the serif font used in traditional IEEE formatting and thus the need
% to invoke different subfig.sty package options depending on whether
% compsoc mode has been enabled.
%
% The latest version and documentation of subfig.sty can be obtained at:
% http://www.ctan.org/pkg/subfig




% *** FLOAT PACKAGES ***
%
%\usepackage{fixltx2e}
% fixltx2e, the successor to the earlier fix2col.sty, was written by
% Frank Mittelbach and David Carlisle. This package corrects a few problems
% in the LaTeX2e kernel, the most notable of which is that in current
% LaTeX2e releases, the ordering of single and double column floats is not
% guaranteed to be preserved. Thus, an unpatched LaTeX2e can allow a
% single column figure to be placed prior to an earlier double column
% figure.
% Be aware that LaTeX2e kernels dated 2015 and later have fixltx2e.sty's
% corrections already built into the system in which case a warning will
% be issued if an attempt is made to load fixltx2e.sty as it is no longer
% needed.
% The latest version and documentation can be found at:
% http://www.ctan.org/pkg/fixltx2e


%\usepackage{stfloats}
% stfloats.sty was written by Sigitas Tolusis. This package gives LaTeX2e
% the ability to do double column floats at the bottom of the page as well
% as the top. (e.g., "\begin{figure*}[!b]" is not normally possible in
% LaTeX2e). It also provides a command:
%\fnbelowfloat
% to enable the placement of footnotes below bottom floats (the standard
% LaTeX2e kernel puts them above bottom floats). This is an invasive package
% which rewrites many portions of the LaTeX2e float routines. It may not work
% with other packages that modify the LaTeX2e float routines. The latest
% version and documentation can be obtained at:
% http://www.ctan.org/pkg/stfloats
% Do not use the stfloats baselinefloat ability as the IEEE does not allow
% \baselineskip to stretch. Authors submitting work to the IEEE should note
% that the IEEE rarely uses double column equations and that authors should try
% to avoid such use. Do not be tempted to use the cuted.sty or midfloat.sty
% packages (also by Sigitas Tolusis) as the IEEE does not format its papers in
% such ways.
% Do not attempt to use stfloats with fixltx2e as they are incompatible.
% Instead, use Morten Hogholm'a dblfloatfix which combines the features
% of both fixltx2e and stfloats:
%
% \usepackage{dblfloatfix}
% The latest version can be found at:
% http://www.ctan.org/pkg/dblfloatfix




%\ifCLASSOPTIONcaptionsoff
%  \usepackage[nomarkers]{endfloat}
% \let\MYoriglatexcaption\caption
% \renewcommand{\caption}[2][\relax]{\MYoriglatexcaption[#2]{#2}}
%\fi
% endfloat.sty was written by James Darrell McCauley, Jeff Goldberg and 
% Axel Sommerfeldt. This package may be useful when used in conjunction with 
% IEEEtran.cls'  captionsoff option. Some IEEE journals/societies require that
% submissions have lists of figures/tables at the end of the paper and that
% figures/tables without any captions are placed on a page by themselves at
% the end of the document. If needed, the draftcls IEEEtran class option or
% \CLASSINPUTbaselinestretch interface can be used to increase the line
% spacing as well. Be sure and use the nomarkers option of endfloat to
% prevent endfloat from "marking" where the figures would have been placed
% in the text. The two hack lines of code above are a slight modification of
% that suggested by in the endfloat docs (section 8.4.1) to ensure that
% the full captions always appear in the list of figures/tables - even if
% the user used the short optional argument of \caption[]{}.
% IEEE papers do not typically make use of \caption[]'s optional argument,
% so this should not be an issue. A similar trick can be used to disable
% captions of packages such as subfig.sty that lack options to turn off
% the subcaptions:
% For subfig.sty:
% \let\MYorigsubfloat\subfloat
% \renewcommand{\subfloat}[2][\relax]{\MYorigsubfloat[]{#2}}
% However, the above trick will not work if both optional arguments of
% the \subfloat command are used. Furthermore, there needs to be a
% description of each subfigure *somewhere* and endfloat does not add
% subfigure captions to its list of figures. Thus, the best approach is to
% avoid the use of subfigure captions (many IEEE journals avoid them anyway)
% and instead reference/explain all the subfigures within the main caption.
% The latest version of endfloat.sty and its documentation can obtained at:
% http://www.ctan.org/pkg/endfloat
%
% The IEEEtran \ifCLASSOPTIONcaptionsoff conditional can also be used
% later in the document, say, to conditionally put the References on a 
% page by themselves.




% *** PDF, URL AND HYPERLINK PACKAGES ***
%
%\usepackage{url}
% url.sty was written by Donald Arseneau. It provides better support for
% handling and breaking URLs. url.sty is already installed on most LaTeX
% systems. The latest version and documentation can be obtained at:
% http://www.ctan.org/pkg/url
% Basically, \url{my_url_here}.





% *** Do not adjust lengths that control margins, column widths, etc. ***
% *** Do not use packages that alter fonts (such as pslatex).         ***
% There should be no need to do such things with IEEEtran.cls V1.6 and later.
% (Unless specifically asked to do so by the journal or conference you plan
% to submit to, of course. )

% 

% correct bad hyphenation here
\hyphenation{op-tical net-works semi-conduc-tor}

\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{xcolor}
\usepackage{multirow}
\usepackage{subfigure}
\usepackage{booktabs}
\usepackage{cite}
\usepackage{bm}
% \usepackage{color}
\usepackage{color, colortbl}
% \def\ie{\emph{i.e}\onedot} 
% \def\Ie{\emph{I.e}\onedot}
% \def\eg{\emph{e.g}\onedot} 
% \def\Ie{\emph{E.g}\onedot}
\def\ie{\emph{i.e.}} 
\def\Ie{\emph{I.e.}}
\def\eg{\emph{e.g.}} 
\def\Ie{\emph{E.g.}}
\newcommand{\XY}[1]{\textcolor{red}{[XY:#1]}}
\newcommand{\BB}[1]{\textcolor{red}{[BB:#1]}}
% \\def\BB#1{{\color{blue}{\bf [BB:} {\it{#1}}{\bf ]}}}
% \def\WM#1{{\color{mygreen}{\bf [WM:} {\it{#1}}{\bf ]}}}

\definecolor{myred}{RGB}{234,67,53}
\definecolor{myblue}{RGB}{66,133,244}
\usepackage{hyperref}

\begin{document}
%
% paper title
% Titles are generally capitalized except for words such as a, an, and, as,
% at, but, by, for, in, nor, of, on, or, the, to and up, which are usually
% not capitalized unless they are the first or last word of the title.
% Linebreaks \\ can be used within to get better formatting as desired.
% Do not put math or special symbols in the title.
\title{
Uncertainty-aware Gait Recognition via Learning from Dirichlet Distribution-based Evidence
}
%
%
% author names and IEEE memberships
% note positions of commas and nonbreaking spaces ( ~ ) LaTeX will not break
% a structure at a ~ so this keeps an author's name from being broken across
% two lines.
% use \thanks{} to gain access to the first footnote area
% a separate \thanks must be used for each paragraph as LaTeX2e's \thanks
% was not built to handle multiple paragraphs
%
%
%\IEEEcompsocitemizethanks is a special \thanks that produces the bulleted
% lists the Computer Society journals use for "first footnote" author
% affiliations. Use \IEEEcompsocthanksitem which works much like \item
% for each affiliation group. When not in compsoc mode,
% \IEEEcompsocitemizethanks becomes like \thanks and
% \IEEEcompsocthanksitem becomes a line break with idention. This
% facilitates dual compilation, although admittedly the differences in the
% desired content of \author between the different types of papers makes a
% one-size-fits-all approach a daunting prospect. For instance, compsoc 
% journal papers have the author affiliations above the "Manuscript
% received ..."  text while in non-compsoc journals this is reversed. Sigh.
\author{Beibei Lin\textsuperscript{$\star$},
        Chen Liu\textsuperscript{$\star$},
        Ming Wang,
        Lincheng Li,
        Shunli Zhang,
        Robby T. Tan,
        and~Xin~Yu\textsuperscript{$\dagger$}% <-this % stops a space
% \author{Michael~Shell,~\IEEEmembership{Member,~IEEE,}
%         John~Doe,~\IEEEmembership{Fellow,~OSA,}
%         and~Jane~Doe,~\IEEEmembership{Life~Fellow,~IEEE}% <-this % stops a space


% \author{Beibei Lin\textsuperscript{1}, Chen Liu\textsuperscript{2}, Lincheng Li\textsuperscript{3}, Robby T. Tan\textsuperscript{1,4}~and Xin Yu\textsuperscript{2} \\
% \textsuperscript{1} National University of Singapore,
% \textsuperscript{2} University of Technology Sydney\\
% \textsuperscript{3} Netease Fuxi AI Lab,
% \textsuperscript{4} Yale-NUS College\\ 
% {\tt\small beibei.lin@u.nus.edu, Chen.Liu-4@student.uts.edu.au, lilincheng@corp.netease.com,}\\ {\tt\small robby.tan@nus.edu.sg, xin.yu@uts.edu.au}
% }

\IEEEcompsocitemizethanks{
\IEEEcompsocthanksitem Beibei Lin and Robby T. Tan were with the National University of Singapore,
\IEEEcompsocthanksitem Chen Liu and Xin Yu were with the University of Queensland, Australia.
\IEEEcompsocthanksitem Lincheng Li was with the Fuxi Lab, NetEase, Hangzhou, China.
\IEEEcompsocthanksitem Ming Wang and Shunli Zhang were with the School of Software Engineering, Beijing Jiaotong University, Beijing, China.
\IEEEcompsocthanksitem \textsuperscript{$\star$} Joint first authors
\IEEEcompsocthanksitem \textsuperscript{$\dagger$} Xin~Yu is the corresponding author. E-mail: xin.yu@uq.edu.au
% \IEEEcompsocthanksitem Shunli Zhang (slzhang@bjtu.edu.cn) is the corresponding author.
}
}
% \IEEEcompsocitemizethanks{\IEEEcompsocthanksitem M. Shell was with the Department
% of Electrical and Computer Engineering, Georgia Institute of Technology, Atlanta,
% GA, 30332.\protect\\
% % note need leading \protect in front of \\ to get a newline within \thanks as
% % \\ is fragile and will error, could use \hfil\break instead.
% E-mail: see http://www.michaelshell.org/contact.html
% \IEEEcompsocthanksitem J. Doe and J. Doe are with Anonymous University.}% <-this % stops an unwanted space
% % \thanks{Manuscript received April 19, 2005; revised August 26, 2015.}
%} 

% note the % following the last \IEEEmembership and also \thanks - 
% these prevent an unwanted space from occurring between the last author name
% and the end of the author line. i.e., if you had this:
% 
% \author{....lastname \thanks{...} \thanks{...}} 
%                     ^------------^------------^----Do not want these spaces!
%
% a space would be appended to the last name and could cause every name on that
% line to be shifted left slightly. This is one of those "LaTeX things". For
% instance, "\textbf{A} \textbf{B}" will typeset as "A B" not "AB". To get
% "AB" then you have to do: "\textbf{A}\textbf{B}"
% \thanks is no different in this regard, so shield the last}  of each \thanks
% that ends a line with a % and do not let a space in before the next \thanks.
% Spaces after \IEEEmembership other than the last one are OK (and needed) as
% you are supposed to have spaces between the names. For what it is worth,
% this is a minor point as most people would not even notice if the said evil
% space somehow managed to creep in.



% The paper headers
\markboth{IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE,~VOL.~XX,~NO.~XX,~XX~2022}
% \markboth{Journal of \LaTeX\ Class Files,~Vol.~14, No.~8, August~2015}%
{Shell \MakeLowercase{\textit{et al.}}: Bare Demo of IEEEtran.cls for Computer Society Journals}
% The only time the second header will appear is for the odd numbered pages
% after the title page when using the twoside option.
% 
% *** Note that you probably will NOT want to include the author's ***
% *** name in the headers of peer review papers.                   ***
% You can use \ifCLASSOPTIONpeerreview for conditional compilation here if
% you desire.



% The publisher's ID mark at the bottom of the page is less important with
% Computer Society journal papers as those publications place the marks
% outside of the main text columns and, therefore, unlike regular IEEE
% journals, the available text space is not reduced by their presence.
% If you want to put a publisher's ID mark on the page you can do it like
% this:
%\IEEEpubid{0000--0000/00\$00.00~\copyright~2015 IEEE}
% or like this to get the Computer Society new two part style.
%\IEEEpubid{\makebox[\columnwidth]{\hfill 0000--0000/00/\$00.00~\copyright~2015 IEEE}%
%\hspace{\columnsep}\makebox[\columnwidth]{Published by the IEEE Computer Society\hfill}}
% Remember, if you use this you must call \IEEEpubidadjcol in the second
% column for its text to clear the IEEEpubid mark (Computer Society jorunal
% papers don't need this extra clearance.)



% use for special paper notices
%\IEEEspecialpapernotice{(Invited Paper)}



% for Computer Society papers, we must declare the abstract and index terms
% PRIOR to the title within the \IEEEtitleabstractindextext IEEEtran
% command as these need to go into the title area created by \maketitle.
% As a general rule, do not put math, special symbols or citations
% in the abstract or keywords.
\IEEEtitleabstractindextext{%
\begin{abstract}

Existing gait recognition frameworks retrieve an identity in the gallery based on the distance between a probe sample and the identities in the gallery. However, existing methods often neglect that the gallery may not contain identities corresponding to the probes, leading to recognition errors rather than raising an alarm. In this paper, we introduce a novel uncertainty-aware gait recognition method that models the uncertainty of identification based on learned evidence. Specifically, we treat our recognition model as an evidence collector to gather evidence from input samples and parameterize a Dirichlet distribution over the evidence. The Dirichlet distribution essentially represents the density of the probability assigned to the input samples. We utilize the distribution to evaluate the resultant uncertainty of each probe sample and then determine whether a probe has a counterpart in the gallery or not. To the best of our knowledge, our method is the first attempt to tackle gait recognition with uncertainty modelling. Moreover, our uncertain modeling significantly improves the robustness against out-of-distribution (OOD) queries. Extensive experiments demonstrate that our method achieves state-of-the-art performance on datasets with OOD queries, and can also generalize well to other identity-retrieval tasks. Importantly, our method outperforms the state-of-the-art by a large margin of 51.26\% when the OOD query rate is around 50\% on OUMVLP. 


\end{abstract}

% Note that keywords are not normally used for peerreview papers.
\begin{IEEEkeywords}
Gait Recognition, Out-of-distribution
\end{IEEEkeywords}}


% make the title area
\maketitle


% To allow for easy dual compilation without having to reenter the
% abstract/keywords data, the \IEEEtitleabstractindextext text will
% not be used in maketitle, but will appear (i.e., to be "transported")
% here as \IEEEdisplaynontitleabstractindextext when the compsoc 
% or transmag modes are not selected <OR> if conference mode is selected 
% - because all conference papers position the abstract like regular
% papers do.
\IEEEdisplaynontitleabstractindextext
% \IEEEdisplaynontitleabstractindextext has no effect when using
% compsoc or transmag under a non-conference mode.



% For peer review papers, you can put extra information on the cover
% page as needed:
% \ifCLASSOPTIONpeerreview
% \begin{center} \bfseries EDICS Category: 3-BBND \end{center}
% \fi
%
% For peerreview papers, this IEEEtran command inserts a page break and
% creates the second title. It will be ignored for other modes.
\IEEEpeerreviewmaketitle



\IEEEraisesectionheading{\section{Introduction}\label{sec:introduction}}



\IEEEPARstart{G}{ait} recognition, as an important biometric retrieval task, aims to identify people by their way of walking \cite{wan2018survey}, and it is widely used in security monitoring and forensics.
Gait recognition has been formulated as a retrieval task, where a query gait sequence, namely a probe, is compared to gait sequences from a gallery. The sequence from the gallery that has the minimal distance to the probe will be regarded as a match and finally its identity will be assigned to the probe. 

Existing gait recognition methods assume there are always corresponding identities in the gallery set. However, in practice, this assumption often does not hold. Some probes possibly do not have their corresponding identities in the gallery, which we call out-of-distribution (OOD) queries.
As shown in Fig.\ref{fig_schematic}(a), current methods still find an identity from the gallery even though actually there is no identity of the probe in the gallery, leading to erroneous results. Therefore, it is highly desirable to develop a gait recognition method that is able to not only find correct identities but also to identify OOD queries. Unfortunately, existing methods that often formulate gait recognition as a feature matching problem do not have a mechanism to address the OOD cases.

\begin{figure}[t]
% \subfigure[Original Gait Sequence]
\centering
{\includegraphics[width=0.33\textwidth]{images/schematic1.pdf}}\\
% \vspace{-0.5em}
(a) Traditional gait recognition\\
% \vspace{0.5em}
{\includegraphics[width=0.48\textwidth]{images/schematic2.pdf}}\\
% \vspace{-0.5em}
(b) Proposed uncertainty-aware gait recognition

% \vspace{-0.5em}
\caption{
% Comparison of the original and dynamic gait sequences. The red rectangle denotes the region with a bag.
% The in-distribution and out-of-distribution cases for traditional gait recognition and our uncertainty-aware gait recognition frameworks.
The in-distribution and out-of-distribution cases for traditional gait recognition framework, and our uncertainty-aware gait recognition framework, which handles OOD queries.
}
% \vspace*{-2em}
\label{fig_schematic}
\end{figure}

In contrast to existing gait recognition approaches, in this work, we formulate gait recognition as an uncertainty-aware feature matching problem. We propose an uncertainty-aware gait recognition framework that can effectively address both in-distribution and out-of-distribution queries.
Our uncertainty-aware framework is agnostic against gait recognition networks, and thus it can adopt any existing gait recognition network as a feature extractor from a probe and gallery sequences. Once we obtain the features of gait sequences, we will retrieve the gallery feature that has the minimal distance to that of the probe. 
Unlike existing methods that directly assign the identity to the probe, we further determine whether the probe and gallery features are from the same identity or not, as shown in Fig.\ref{fig_schematic}(b). 

To examine the identity matches between the probe and gallery, we introduce the theory of evidential deep learning into the process of feature matching. Evidential deep learning \cite{sensoy2018evidential,dempster1968generalization, jsang2016subjective} regards deep networks as an evidence collector and models class probabilities via a Dirichlet distribution. 
Motivated by this, we propose a gait classification model that is employed as an evidence collector to gather opinions or evidence from the paired features of the probe and gallery. The evidence is then utilized to parameterize a Dirichlet distribution. Moreover, the theory of Subjective Logic \cite{sensoy2018evidential} indicates that the Dirichlet distribution represents the probability density of the input features, including in-distribution and out-distribution query scenarios. In other words, the Dirichlet distribution reflects the mass assignment of the input features and their uncertainty in the feature space, thus providing a mechanism to detect OOD probes. Once we obtain the Dirichlet distribution parameterized over the input features, we can apply it to examine whether a pair of retrieved gaits is from the same person and how confident our prediction is.
As a result, our model predicts three scores: a matching score, an unmatching score and an uncertainty score. Based on the three scores, we are able to determine in-distribution and out-of-distribution queries.



Extensive experiments demonstrate that our method achieves state-of-the-art performance on gait recognition with OOD queries. Importantly, our method outperforms the state-of-the-art method, \ie, DyGait \cite{wang2023dygait}, by a large margin of 51.26\% when the rate of OOD queries is 50\% on one of the largest gait recognition datasets OUMVLP \cite{takemura2018multi}. 
Our main contributions are summarized as follows:

{{
1) We propose a novel uncertainty-aware gait recognition model that can tackle both in-distribution and out-of-distribution queries in a unified framework. To the best of our knowledge, we are the first to address OOD cases for gait recognition.
}}

{{
2) We introduce the theory of evidential deep learning to represent the probability density of a pair of retrieved features and thus model the uncertainty of our prediction.
}}

{{
3) Our framework is agnostic against gait recognition backbones. It can be adopted by existing methods with minimal effort to address OOD queries, thus significantly improving their robustness.
}}








\section{Related Work}
\noindent \textbf{Gait Recognition}
Gait recognition aims to learn discriminative feature representations from people's skeletons or silhouettes for identification purposes \cite{sepas2022deep, shen2022comprehensive, yu2021hid, zhu2021gait, zheng2022gait, zhang2021cross, li2022multi}. 
Its objective is to obtain the identity information for a probe sample from the gallery.
To achieve this goal, many studies focus on improving the feature extraction capability of the model.
These methods can be categorized into three classes, \ie, model-based methods, pose-based methods, and silhouette-based methods.
Model-based methods are designed to extract features from RGB sequences, such as shapes, view angles, and postures ~\cite{wagg2004automated, li2020end, li2021end}. 
Pose-based methods extract 2D poses or 3D poses of human bodies to obtain discriminative feature representations~\cite{teepe2021gaitgraph, liao2020model,hsu2022gaittake,teepe2022towards}. 
Silhouette-based methods generate feature representations by aggregating all temporal information of a gait sequence \cite{shiraga2016geinet,li2020gait,wang2020human, shen2022gait, lin2021gaitmask, lin2020gait, chai2022lagrange, huang2021context, lin2022gaitgl, wang2022gaitstrip, lin2021multi, yu2022cntn, yu2022generalized, liang2022gaitedge, dou2022gaitmpl, dou2022metagait, huang2022enhanced, hou2021set, hou2020gait, huang20213d}.
Although the above methods obtain high recognition accuracy, they often neglect the fact that a probe might be an OOD sample, which does not have counterparts in the gallery. When such probes are exhibited, current methods would fail and cannot tell the gallery does not contain corresponding identities as they do not have a mechanism to address OOD queries. 
To the best of our knowledge, our work is the first attempt to address the OOD query scenario in gait recognition. 

Moreover, many other recognition tasks, such as vehicle and human re-identification ~\cite{zhu2020voc,tang2019cityflow,devyatkov2018multicamera} and face recognition ~\cite{guo2020learning,kalayeh2018human}, also follow a very similar recognition paradigm as gait recognition, and thus face the same situation of OOD queries.
Therefore, our method can be readily applied to those tasks to improve their recognition robustness against OOD queries, thus avoiding incorrect identity assignments. 
In addition, our method does not need to re-train the backbone networks, significantly facilitating existing networks to adapt to OOD queries. 

% \vspace{-1em}
\noindent \textbf{OOD Recognition} In our task, the test set involves identities that have been seen and unseen in the training set. If a testing identity is not registered to the gallery set but exists in the probe set, it will be regarded as an OOD identity. Open-set recognition methods \cite{scheirer2012toward, bao2021evidential} have been proposed to address the OOD problem in recognition. These methods aim to classify the known classes while labeling all OOD samples to one unknown class. However, they cannot be used for our task. 
This is because query identities that have been unseen in the training set will be regarded as unknown classes by open-set recognition methods, while our method still needs to find their counterparts from the gallery set, and the query identities are usually not provided in training.



\section{Uncertainty-aware Gait Recognition Framework}

% \begin{figure*}[t] 
% \begin{center}  
% 	\includegraphics[width=0.95\linewidth]{images/Fig1_The overall framework.pdf} 
% 	\vspace{-0.5em}
% 	\caption{Overview of the uncertainty-aware gait recognition framework.}
% \label{Figure_1}
% \end{center}
% \vspace{-2em}
% \end{figure*}
\begin{figure*}[t] 
\begin{center}  
	\includegraphics[width=1.0\linewidth]{images/Fig_training.pdf} 
	% \vspace{-1.5 em}
	\caption{Overview of the uncertainty-aware gait recognition framework.
	We first utilize existing backbones to extract features and then construct hard positive and negative feature pairs to train our evidence collector. By Dirichlet-based evidence learning, our method is able to predict matching, unmatching and uncertainty scores of a pair of retrieved features. As a result, our method is able to determine whether they are from the same identity.
	Note that our method does not need to re-train the backbone networks and thus can be easily applied to different backbones and other similar tasks.
	}
\label{Figure_1}
\end{center}
% \vspace{-2.5em}
\end{figure*}

Existing gait recognition methods follow a standard paradigm, which can be divided into three steps:
% ------------------first---------------------
(i) Designing and training a gait recognition network. The gait recognition network can be trained with various losses to achieve strong feature extraction capability.
% ------------------Second---------------------
% (ii) Once the gait recognition network has been trained, we use it to extract features from a probe sequence sampled from a probe set and a gallery set. 
% Here, we denote $\mathbb{P} = \{p_{1},p_{2},...,p_{K}\}$ as all the probe features from the probe set and $\mathbb{G} = \{g_{1},g_{2},...,g_{Q}\}$ as all the gallery features from the gallery set, where $p_{i}$ is $i$-th probe feature, $g_{j}$ is the $j$-th gallery feature, and $K$ and $Q$ indicate the number of the probe samples and gallery samples, respectively;
% (iii) One needs to compute the distance between a probe feature $p_i$ and all the gallery features $g_j, j=0,\cdots, Q$.
% For a probe feature $p_{i}$, its distances with respect to all the gallery features can be represented as 
% $\mathbb{D}_{i} = \{D(p_{i}, g_{1}),D(p_{i}, g_{2}),...,D(p_{i}, g_{Q})\}$, where $D$ indicates the Euclidean distance in our experiments.
% Next, we will find the minimum distance in $\mathbb{D}_{i}$. Suppose $D(p_{i}, g_{j})$ is the minimum value in $\mathbb{D}_{i}$, and then the identity of $g_j$ will be assigned to the probe $i$.
(ii) Extracting features. Once the gait recognition network has been trained, we use it to extract features from a sequence sampled from the probe and gallery sets. 
Here, we denote $\mathbb{Q} = \{q_{1},q_{2},...,q_{M}\}$ as all the probe features from the probe set and $\mathbb{G} = \{g_{1},g_{2},...,g_{N}\}$ as all the gallery features from the gallery set, where $q_{i}$ is $i$-th probe feature, $g_{j}$ is the $j$-th gallery feature, and $M$ and $N$ indicate the number of the probe samples and gallery samples, respectively;
(iii) Computing the distance to the gallery features. We compute the distance between a probe feature $q_i$ and all the gallery features.
For a probe feature $q_{i}$, its distances with respect to all the gallery features can be represented as 
$\mathbb{D}_{i} = \{D(q_{i}, g_{1}),D(q_{i}, g_{2}),...,D(q_{i}, g_{Q})\}$, where $D$ indicates the Euclidean distance in our experiments.
Subsequently, we find the minimum distance in $\mathbb{D}_{i}$. Suppose $D(q_{i}, g_{j})$ is the minimum value in $\mathbb{D}_{i}$, and then the identity of $g_j$ will be assigned to the probe $i$.

However, in step (iii), conventional gait recognition methods neglect the fact there might be no matching identity in the gallery. Although $D(q_{i}, g_{j})$ is minimum, the identity $j$ should not be assigned to the probe $i$. When the probe does not have a counterpart identity in the gallery, we refer to it as an OOD query. To address both in-distribution and out-of-distribution queries in a unified framework, we present an uncertainty-aware gait recognition method, as illustrated in Figure \ref{Figure_1}.


% \subsection{Preliminary}
\subsection{Dirichlet-based Evidence Learning}

To address both in-distribution and out-of-distribution queries in a unified framework, our work aims to model the probability distribution of the distance of gait features and estimate the uncertainty of our classification model.
%
Previous methods estimate prediction variance or uncertainty through dropout \cite{gal2016dropout, molchanov2017variational,gal2017concrete,amini2018spatial}, ensembling \cite{pearce2018uncertainty, lakshminarayanan2017simple} or other sampling approaches \cite{blundell2015weight, hernandez2015probabilistic}. However, this stream of works often relies on expensive sampling operations and it is hard to apply them to feature matching tasks, since the probe and gallery features are often fixed.

In Bayesian inference, placing prior distributions over deep models to estimate uncertainty has been widely explored \cite{gelman2006prior, gelman2008weakly}, e.g,  Evidential Deep Learning (EDL) \cite{sensoy2018evidential} and Prior Networks \cite{malinin2018predictive, malinin2019reverse} place Dirichlet priors over discrete classification predictions. Motivated by this, we employ the theoretical framework of EDL to model the probability distribution.


{{
Given an input feature $\bm{x}$, EDL first uses an evidence collector to generate the evidence $\bm{e} = \{e_{k}|k=1,\ldots,K\}$, where $K$ is the number of classes and $e_{k}$ denotes the evidence of the $k$-th class. 
Based on the Subjective Logic theory \cite{sensoy2018evidential}, the evidence $\bm{e}$ corresponds to a Dirichlet distribution parameterized by $\bm{\alpha} = \{\alpha_{k}|k=1,\ldots,K\}$, where $\alpha_{k} = e_{k} + 1$. 
A Dirichlet distribution is a probability density function for all the possible values of a probability mass function $\bm{p}$, and can be represented as:
\begin{equation}
\begin{array}{l}
\mathbf{D}(\bm{p} \mid \bm{\alpha})=\left\{\begin{array}{ll}
\frac{1}{B(\bm{\alpha})} \prod_{i=1}^{K} p_{i}^{\alpha_{i}-1} & \text { for } \bm{p} \in \mathcal{S}_{K}, \\
0 & \text { otherwise },
\end{array}\right. 
\end{array}
\end{equation}
where $\mathcal{S}_{K}=\left\{\bm{p} \mid \sum_{i=1}^{K} p_{i}=1 \text { and } 0 \leq p_{1}, \ldots, p_{K} \leq 1\right\}$ is the $K$-dimensional unit simplex,
and $B(\alpha)$ is the $K$-dimensional multinomial beta function \cite{kotz2004continuous}. Here, $p_{k}$ is the expected probability for the $k$-th class and is calculated as $p_k = \frac{\alpha_k}{S}$, where $S= \sum_{i=1}^{K} \alpha_{i} $.
}}

{{
We denote the ground-truth label of $\bm{x}$ is $\bm{y}$ that is a $K$-dimensional one-hot vector with $y_{i}$ = 1 and $y_{k}$ = 0 for all $k \neq i$. 
The Mean Square Error (MSE) is employed to measure the differences between the estimated class probabilities and the ground truth and can be expressed as:
\begin{align}
\mathcal{L}_{m}(\Theta) &=\int\left\|\bm{y}-\bm{p}\right\|_{2}^{2} \mathbf{D}(\bm{p} \mid \bm{\alpha}) \notag
\\
% &=\sum_{j=1}^{K} \mathbb{E}\left[y_{i j}^{2}-2 y_{i j} p_{i j}+p_{i j}^{2}\right] \notag
% \\
&=\sum_{i=1}^{K}\left(y_{i}^{2}-2 y_{i} \mathbb{E}\left[p_{i}\right]+\mathbb{E}\left[p_{i}^{2}\right]\right),
\end{align}
where $\mathbb{E}$ indicates the expectation operation.
}}

{{
Furthermore, if an input feature cannot be correctly classified, its evidence ideally should shrink to zero. In other words, the Dirichlet distribution will be a uniform Dirichlet distribution. To this end, Sensoy et al. \cite{sensoy2018evidential} introduce a Kullback-Leibler (KL) divergence term. As a result, the total loss function is represented as:
\begin{align}
\mathcal{L}(\Theta) 
=  \mathcal{L}_{m}(\Theta) +\lambda_{t} K L\left[\mathbf{D}\left(\bm{p} \mid \tilde{\bm{\alpha}}\right) \| \mathbf{D}\left(\bm{p} \mid\langle 1, \ldots, 1\rangle\right)\right],
\end{align}
where $\lambda_t = \min(1.0, t/10)$ $\in$ [0, 1] is the annealing coefficient. $\mathbf{D}\left(\bm{p} \mid\langle 1, \ldots, 1\rangle\right)$ indicates a uniform Dirichlet distribution, $t$ is the index of the current training iteration, and 
$\tilde{\bm{\alpha}} = \bm{y} + (1 - \bm{y}) \bigodot \bm{\alpha}$ is the Dirichlet parameters after the removal of the non-misleading evidence from predicted parameters $\bm{\alpha}$ for the input feature $\bm{x}$. 
}}

% \begin{align}
% \mathcal{L}(\Theta) 
% &= \sum_{i = 1}^{N} \mathcal{L}_{i}(\Theta) \notag
% \\
% &+\lambda_{t} \sum_{i = 1}^{N} K L\left[\mathbf{D}\left(\mathbf{p}_{\mathbf{i}} \mid \tilde{\boldsymbol{\alpha}}_{i}\right) \| \mathbf{D}\left(\mathbf{p}_{i} \mid\langle 1, \ldots, 1\rangle\right)\right],
% \end{align}
% where $\lambda_t = min(1.0, t/10)$ $\in$ [0, 1] is the annealing coefficient, $\mathbf{D}\left(\mathbf{p}_{i} \mid\langle 1, \ldots, 1\rangle\right)$ indicates a uniform Dirichlet distribution, $t$ is the index of the current training iteration, and 
% $\tilde{\boldsymbol{\alpha}}_{i} = \mathbf{y}_{i} + (1 - \mathbf{y}_{i}) \bigodot \boldsymbol{\alpha}_{i}$ is the Dirichlet parameters after removal of the non-misleading evidence from predicted parameters $\boldsymbol{\alpha}_{i}$ for the residual feature $i$. 

% {\textcolor{cyan}{
% Furthermore, if a pair of retrieved features cannot be correctly, its evidence ideally should shrink to zero. In other words, the Dirichlet distribution will be a uniform Dirichlet distribution. To solve this problem, sensoy et al. \cite{sensoy2018evidential} introduce a Kullback-Leibler (KL) divergence term. As a result, the total loss function is represented as:
% \begin{align}
% \mathcal{L}(\Theta) 
% &= \sum_{i = 1}^{N} \mathcal{L}_{i}(\Theta) \notag
% \\
% &+\lambda_{t} \sum_{i = 1}^{N} K L\left[\mathbf{D}\left(\mathbf{p}_{\mathbf{i}} \mid \tilde{\boldsymbol{\alpha}}_{i}\right) \| \mathbf{D}\left(\mathbf{p}_{i} \mid\langle 1, \ldots, 1\rangle\right)\right],
% \end{align}
% where $\lambda_t = min(1.0, t/10)$ $\in$ [0, 1] is the annealing coefficient, $\mathbf{D}\left(\mathbf{p}_{i} \mid\langle 1, \ldots, 1\rangle\right)$ indicates a uniform Dirichlet distribution, $t$ is the index of the current training iteration, and 
% $\tilde{\boldsymbol{\alpha}}_{i} = \mathbf{y}_{i} + (1 - \mathbf{y}_{i}) \bigodot \boldsymbol{\alpha}_{i}$ is the Dirichlet parameters after removal of the non-misleading evidence from predicted parameters $\boldsymbol{\alpha}_{i}$ for the residual feature $i$. 
% }}


% {\textcolor{cyan}{
% Unlike traditional binary classifiers, we need to estimate evidence $e_1$, and $e_2$, based on the theory of evidence, and thus we denote the ground-truth label $\mathbf{y}$ as a $K$-dimensional one-hot vector.
% Then, we employ the Mean Square Error (MSE) to measure the differences between the estimated and ground-truth evidence. Here, the ground-truth evidence is written as $\mathbf{p} = \frac{\mathbf{\alpha}}{S}$. Recall $S$ indicates the strength of a Dirichlet distribution.
% Our loss function $\mathcal{L}_{i}(\Theta)$ for a residual feature $i$ is expressed as:
% \begin{align}
% \mathcal{L}_{i}(\Theta) &=\int\left\|\mathbf{y}_{i}-\mathbf{p}_{i}\right\|_{2}^{2} \mathbf{D}(\mathbf{p}_i \mid \boldsymbol{\alpha}_i) \notag
% \\
% % &=\sum_{j=1}^{K} \mathbb{E}\left[y_{i j}^{2}-2 y_{i j} p_{i j}+p_{i j}^{2}\right] \notag
% % \\
% &=\sum_{j=1}^{K}\left(y_{i j}^{2}-2 y_{i j} \mathbb{E}\left[p_{i j}\right]+\mathbb{E}\left[p_{i j}^{2}\right]\right),
% \end{align}
% where $\mathbb{E}$ indicates the Expectation operation. $\mathbf{y}_{i}$ is a one-hot vector of a residual feature $i$ with $y_{ij}$ = 1 and $y_{ik}$ = 0 for all $k \neq j$. $\boldsymbol{\alpha}_i$ denotes the parameters of the Dirichlet density on the prediction of the residual feature $i$.}}

% {\textcolor{cyan}{
% Furthermore, if a pair of retrieved features cannot be classified as a matching or unmatching pair, its evidence ideally should shrink to zero. In other words, the Dirichlet distribution will be a uniform Dirichlet distribution. To this end, a Kullback-Leibler (KL) divergence term is introduced into our loss function. The total loss function of our model is represented as:
% \begin{align}
% \mathcal{L}(\Theta) 
% &= \sum_{i = 1}^{N} \mathcal{L}_{i}(\Theta) \notag
% \\
% &+\lambda_{t} \sum_{i = 1}^{N} K L\left[\mathbf{D}\left(\mathbf{p}_{\mathbf{i}} \mid \tilde{\boldsymbol{\alpha}}_{i}\right) \| \mathbf{D}\left(\mathbf{p}_{i} \mid\langle 1, \ldots, 1\rangle\right)\right],
% \end{align}
% where $\lambda_t = min(1.0, t/10)$ $\in$ [0, 1] is the annealing coefficient, $\mathbf{D}\left(\mathbf{p}_{i} \mid\langle 1, \ldots, 1\rangle\right)$ indicates a uniform Dirichlet distribution, $t$ is the index of the current training iteration, and 
% $\tilde{\boldsymbol{\alpha}}_{i} = \mathbf{y}_{i} + (1 - \mathbf{y}_{i}) \bigodot \boldsymbol{\alpha}_{i}$ is the Dirichlet parameters after removal of the non-misleading evidence from predicted parameters $\boldsymbol{\alpha}_{i}$ for the residual feature $i$. 
% }}





\subsection{Uncertainty Modelling for Gait Recognition}

As aforementioned, the premise that the gallery must contain an identity corresponding to the probe sample is considerably difficult to satisfy in practice, and OOD query cases may more likely happen if the gallery size is limited.
Therefore, a mechanism to tackle OOD gait recognition is highly desirable.


A simple solution to this problem is to develop a binary classifier that classifies whether a probe sample and a gallery sample that has the minimum distance to the probe feature belong to the same identity. 
Here, the errors from the gait recognition networks should not affect the classifier performance as the adopted networks already achieve appealing performance on the gait feature representation. 
Using the simple classifier, the result of each retrieve can be formulated as:
\begin{equation}
Y = 
\left\{\begin{matrix}
\textit{OOD} \quad  F_{c}(q_{i}, g_{j}) \leq 0.5,\\ \hfill 
\textit{ID}  \qquad   F_{c}(q_{i}, g_{j}) > 0.5, \hfill 
\end{matrix}\right.
\end{equation}
where $Y$ is the framework output. $\textit{OOD}$ indicates the input probe feature $q_{i}$ and gallery feature $g_{j}$ are from different identities. $\textit{ID}$ represents the input probe feature $q_{i}$ and gallery feature $g_{j}$ are from the same identity. $F_{c}(\cdot)$ denotes a binary classifier that takes a probe feature and a gallery feature as input.
%
Unfortunately, a simple binary classifier tends to output an over-confident erroneous prediction especially when an OOD query is fed into it. Hence, a classifier would fail to detect the OOD query and thus the gait recognition methods would assign incorrect identities to probes. 

To address this problem, we introduce the theory of Evidential Deep Learning (EDL) \cite{sensoy2018evidential} into current gait recognition tasks and then model the uncertainty of whether the query and retrieved features are from the same identity.
The EDL theory is built on the Dempster-Shafer Theory of Evidence (DST),  a generalization of the Bayesian theory to subjective probabilities \cite{DST}.
To be specific, EDL assigns belief masses (\eg, probabilities) to a set of mutually exclusive labels, \eg, class labels for a sample. 
A belief mass can be assigned to any subset of a frame \cite{DST}, including the whole frame itself. Here, the frame refers to the possible labels for a sample.
Different from traditional binary classification, we can introduce a label ``I do not know'' into the frame, representing the uncertainty to a given sample based on the theoretical framework of EDL.
To formulate the belief mass assignments, Subjective Logic (SL) is an effective tool. 
Specifically, SL can formalize the belief assignments over a frame of discernment via a Dirichlet distribution. Thus, SL allows us to employ the principles of evidential theory to quantify the belief masses and uncertainty.

Driven by SL, we propose an uncertainty-aware gait recognition model that enables us to recognize ID and OOD queries in a unified framework (seeing Figure \ref{Figure_1}).
In order to apply the uncertainty modeling of SL to our method, we first employ a gait recognition network to retrieve a gallery feature that has the minimum distance to the probe feature, and then compute the feature discrepancy between the probe and gallery features, called \emph{residual feature}.
%
Instead of directly measuring whether a query is OOD, we opt to examine the distribution of residual features. 
% In this way, we formulate the retrieval task into a classification problem.
This is because modeling the gallery feature distribution explicitly is intractable. 

Modeling the gallery distribution implicitly via a discriminator will degenerate into the case of learning a simple classifier, which has been demonstrated ineffective for the gait recognition problem.
% Second, we propose a novel uncertainty-aware classification model to collect the evidence of a pair of the retrieved sample and then parameterize a Dirichlet distribution over the evidence. 
In contrast, the label space for a residual feature only contains $\textit{OOD}$ and $\textit{ID}$, which is also much smaller than the identity number of the gallery set.
In accordance with SL, we provide a belief mass $b_k$ to the $K=2$ labels (\eg, $b_1$ indicates the probability of a matching pair, and $b_2$ indicates the probability of an unmatching pair) and an overall uncertainty mass $\mu$. These 3 mass values are all non-negative and sum up to $1$:
\vspace{-0.5em}
\begin{equation}
\label{sum_all_scores}
\mu +\sum_{k=1}^{K} b_{k}=1.
\end{equation}
Therefore, the retrieval result is determined by:
\begin{equation}
\label{eqn_determine}
Y = 
\left\{\begin{matrix}
\textit{OOD}   \quad  b_2 + \mu \geq b_1, \\ \hfill 
\textit{ID}  \qquad   b_2 + \mu < b_1.  \hfill 
\end{matrix}\right.
\end{equation}
When the prediction score of the matching class $b_1$ is higher than the sum of the score of the unmatching class $b_2$ and the uncertainty score, we believe the retrieved identity should be assigned to the probe. Otherwise, we will discard the retrieved identity as the probe sample could be an OOD query. Thanks to the uncertainty modeling of SL, we can effectively refuse OOD queries without harming the overall recognition performance.



\subsection{Evidence Collector Design}
In Eqn.~\eqref{sum_all_scores}, the mass values of $b_k$ and $\mu$ are derived from the evidence ($e_k \ge 0$) and the evidence will be used to determine the Dirichlet distribution and uncertainty. 
Based on EDL, the evidence is a measure of support collected from data in favor of a sample to be classified into a certain class. Therefore, we develop a simple Multi-Layer Perceptron (MLP), including two fully-connected (FC) layers, to collect the evidence. That means the output of MLP is the evidence $\bm{e} = \{e_k | k = 1,\ldots, K\}$, where $e_k$ denotes the evidence of the $k$-th class. To ensure the value of the evidence is non-negative, we adopt the ReLU function to filter the negative value.
Once we obtain evidence $e_k$, the belief mass $b_k$ and uncertainty mass $\mu$ are expressed as:
\begin{equation}
\label{eqn_bk}
b_{k}=\frac{e_{k}}{S} \quad \text{and} \quad \mu = \frac{K}{S} ,
\end{equation}
where $ S = \sum_{i=1}^{K} (e_{i} + 1)$. According to SL, a belief mass assignment corresponds to a Dirichlet distribution with parameters $\alpha_k=e_k + 1$ and $S = \sum_{i=1}^{K} a_{i}$ is the Dirichlet strength. Eqn. \eqref{eqn_bk} can be written as:
\begin{equation}
\label{eq5}
b_{k}=\frac{a_{k}-1}{\sum_{i=1}^{K} a_{i}} \quad \text{and} \quad \mu = \frac{K}{\sum_{i=1}^{K} a_{i}}.
\end{equation}
Eqn. \eqref{eq5} implies if the strength of a Dirichlet distribution or evidence is larger, the Dirichlet distribution is more concentrated and the uncertainty of the prediction will become smaller. At that point, the prediction of the network will be more reliable.
If evidence is lower (\ie, the probabilities assigned to the two classes are small), the Dirichlet distribution will be flatter 
and the uncertainty of the prediction will be higher. Thus, the input might come from an OOD query.



\subsection{Query Pair Construction}
\label{sec:query_pair_constr}
Our uncertainty-aware classification model aims to determine whether a pair of retrieved features are from the same identity or not. To train our classification model, we need to simulate the testing input. 
For this purpose, we construct pairs of samples, including positive and negative pairs. 
To be specific, a positive pair is obtained by sampling different sequences from the same identity, while we sample two sequences from different identities to construct a negative pair.


A simple strategy is to construct positive and negative pairs by randomly selecting two samples from the training set. 
Here, negative pairs are randomly chosen from two identities and positive pairs are randomly selected within the same identity.
Even though we balance the number of positive and negative pairs, we find the network cannot effectively classify hard negative pairs.
This is because the constructed pairs contain numerous simple cases that can be distinguished easily. In other words, our model does not have many chances to learn uncertain cases during training.

Inspired by the hard mining strategies, we construct the hardest positive and the hardest negative pairs within each training batch and thus enforce our model to classify highly uncertain samples.
To be specific, we first randomly select $P$ identities from the training set and then randomly choose $V$ samples from each identity. Therefore, a total of $P\times V$ samples can be selected. Denote $I = \{f_{(1,1)}, f_{(1,2)}, \ldots, f_{(i,j)}, \ldots, f_{(P,V)}\}$ be the features of all selected samples. $(i,j)$ indicates the $j$-th samples of the $i$-th identity. 
For the feature $f_{(i,j)}$, we can calculate the Euclidean distance between $f_{(i,j)}$ and each feature in $I$. 
Finally, the hardest positive pair can be constructed as $< f_{(i,j)} , f_{(i,q)} >$, where $f_{(i,q)}$ has the maximum distance with the feature $f_{(i,j)}$ for all samples in the $i$-th identity. The hardest negative pair can be constructed as $< f_{(i,j)} , f_{(v,w)} >$, where $f_{(v,w)}$ has the minimum distance with the feature $f_{(i,j)}$ and $v \neq i$. 
For all the features of $I$ in the batch, we can totally generate $P \times V$ hardest positive pairs and $P \times V$ hardest negative pairs.


\subsection{Inference}
\begin{figure*}[t]
% \subfigure[Original Gait Sequence]
\centering
{\includegraphics[width=0.95\textwidth]{images/Fig_inference.pdf}}\\
% \vspace{-0.5em}

\caption{
Inference for the uncertainty-aware gait recognition framework. Given a probe feature $q_i$, we first search a gallery feature $g_j$ that has the minimum distance to it, and then feed the two features to our evidence collector to determine in-distribution and out-of-distribution queries.
}
% \vspace*{-1em}
\label{fig_inference}
\end{figure*}


The inference stage of the uncertainty-aware gait recognition framework is shown in Figure \ref{fig_inference}.
%
We first use a gait backbone to extract all the probe and gallery features. Recall $\mathbb{Q} = \{q_{1},q_{2},...,q_{M}\}$ denotes all the probe features from the probe set and $\mathbb{G} = \{g_{1},g_{2},...,g_{N}\}$ represents all the gallery features from the gallery set. 
For a probe feature $q_{i}$, we compute the distances between $q_{i}$ and all the gallery features and then find a gallery feature $g_j$ that has the minimum distance to the probe. 

Different from existing methods that assign the identity of $g_j$ to the probe sample $i$, we utilize our uncertainty-aware gait recognition model to further determine whether the two features are from the same identity. To be specific, we compute the feature discrepancy between the two features, expressed by $\bm{x}$. The feature discrepancy $\bm{x}$ is then fed into our uncertainty-aware gait recognition model to generate evidence $\bm{e} =  \{ e_1, e_2 \}$. From Eqn.~\eqref{eqn_bk}, we can obtain the matching probability $b_1$, the unmatching probability $b_2$ and the uncertainty probability $\mu$. 
Based on the three probabilities and Eqn.~\eqref{eqn_determine}, we can determine the identity of the probe sample $i$.


% Here, we denote $\mathbb{Q} = \{q_{1},q_{2},...,q_{M}\}$ as all the probe features from the probe set and $\mathbb{G} = \{g_{1},g_{2},...,g_{N}\}$ as all the gallery features from the gallery set, where $q_{i}$ is $i$-th probe feature, $g_{j}$ is the $j$-th gallery feature, and $M$ and $N$ indicate the number of the probe samples and gallery samples, respectively;


% \subsection{Preliminary: EDL}
% {\textcolor{cyan}{
% Given a residual feature $i$, we first use the proposed evidence collector to generate the evidence $f (x_i|\Theta)$, where $\Theta$ is the parameters of our evidence collector. 
% Based on SL theory, the evidence corresponds to a Dirichlet distribution parameterized by $\alpha_k = f (x_i|\Theta) + 1$.
% A Dirichlet distribution is a probability density function for all the possible values of a probability mass function $\mathbf{p}$. It is depicted by parameters $\mathbf{\alpha} = [\alpha_1,\ldots,\alpha_K]$, expressed by:
% \begin{equation}
% \begin{array}{l}
% \mathbf{D}(\mathbf{p} \mid \boldsymbol{\alpha})=\left\{\begin{array}{ll}
% \frac{1}{B(\boldsymbol{\alpha})} \prod_{i=1}^{K} p_{i}^{\alpha_{i}-1} & \text { for } \mathbf{p} \in \mathcal{S}_{K}, \\
% 0 & \text { otherwise },
% \end{array}\right. 
% \end{array}
% \end{equation}
% where $\mathcal{S}_{K}=\left\{\mathbf{p} \mid \sum_{i=1}^{K} p_{i}=1 \text { and } 0 \leq p_{1}, \ldots, p_{K} \leq 1\right\}$ is the $K$-dimensional unit simplex,
% and $B(\alpha)$ is the $K$-dimensional multi-nomial beta function \cite{kotz2004continuous}. 
% }}

% {\textcolor{cyan}{
% Unlike traditional binary classifiers, we need to estimate evidence $e_1$, and $e_2$, based on the theory of evidence, and thus we denote the ground-truth label $\mathbf{y}$ as a $K$-dimensional one-hot vector.
% Then, we employ the Mean Square Error (MSE) to measure the differences between the estimated and ground-truth evidence. Here, the ground-truth evidence is written as $\mathbf{p} = \frac{\mathbf{\alpha}}{S}$. Recall $S$ indicates the strength of a Dirichlet distribution.
% Our loss function $\mathcal{L}_{i}(\Theta)$ for a residual feature $i$ is expressed as:
% \begin{align}
% \mathcal{L}_{i}(\Theta) &=\int\left\|\mathbf{y}_{i}-\mathbf{p}_{i}\right\|_{2}^{2} \mathbf{D}(\mathbf{p}_i \mid \boldsymbol{\alpha}_i) \notag
% \\
% % &=\sum_{j=1}^{K} \mathbb{E}\left[y_{i j}^{2}-2 y_{i j} p_{i j}+p_{i j}^{2}\right] \notag
% % \\
% &=\sum_{j=1}^{K}\left(y_{i j}^{2}-2 y_{i j} \mathbb{E}\left[p_{i j}\right]+\mathbb{E}\left[p_{i j}^{2}\right]\right),
% \end{align}

% where $\mathbb{E}$ indicates the Expectation operation. $\mathbf{y}_{i}$ is a one-hot vector of a residual feature $i$ with $y_{ij}$ = 1 and $y_{ik}$ = 0 for all $k \neq j$. $\boldsymbol{\alpha}_i$ denotes the parameters of the Dirichlet density on the prediction of the residual feature $i$.
% }}

% {\textcolor{cyan}{
% Furthermore, if a pair of retrieved features cannot be classified as a matching or unmatching pair, its evidence ideally should shrink to zero. In other words, the Dirichlet distribution will be a uniform Dirichlet distribution. To this end, a Kullback-Leibler (KL) divergence term is introduced into our loss function. The total loss function of our model is represented as:
% \begin{align}
% \mathcal{L}(\Theta) 
% &= \sum_{i = 1}^{N} \mathcal{L}_{i}(\Theta) \notag
% \\
% &+\lambda_{t} \sum_{i = 1}^{N} K L\left[\mathbf{D}\left(\mathbf{p}_{\mathbf{i}} \mid \tilde{\boldsymbol{\alpha}}_{i}\right) \| \mathbf{D}\left(\mathbf{p}_{i} \mid\langle 1, \ldots, 1\rangle\right)\right],
% \end{align}
% where $\lambda_t = min(1.0, t/10)$ $\in$ [0, 1] is the annealing coefficient, $\mathbf{D}\left(\mathbf{p}_{i} \mid\langle 1, \ldots, 1\rangle\right)$ indicates a uniform Dirichlet distribution, $t$ is the index of the current training iteration, and 
% $\tilde{\boldsymbol{\alpha}}_{i} = \mathbf{y}_{i} + (1 - \mathbf{y}_{i}) \bigodot \boldsymbol{\alpha}_{i}$ is the Dirichlet parameters after removal of the non-misleading evidence from predicted parameters $\boldsymbol{\alpha}_{i}$ for the residual feature $i$. 
% }}



\section{Experiments}

% % Table generated by Excel2LaTeX from sheet 'Sheet1'
% \begin{table*}[t]
% \footnotesize
% % \sisetup{table-number-alignment=center}
%   \centering
%   \caption{Rank-1 accuracy (\%) on the OUMVLP dataset under different OOD percentages. The standard deviation is shown in parentheses.}
%   \vspace{-1.0em}
%   \resizebox{0.85\textwidth}{!}{
%     \begin{tabular}{c|c|c|c|c|c}
%     \toprule
%     \multicolumn{1}{c|}{\multirow{2}[2]{*}{Methods}} & \multicolumn{5}{c}{Different OOD percentages} \\
%     \cmidrule{2-6}  & \multicolumn{1}{c|}{15\%} & \multicolumn{1}{c|}{25\%} & \multicolumn{1}{c|}{35\%} & \multicolumn{1}{c|}{45\%} & \multicolumn{1}{c}{55\%} \\
%     \midrule
%     GaitSet  & 79.71  & 70.35  & 61.80  & 52.29  & 42.73  \\
%     % \midrule
%     %  GaitSet + Verification & 85.46 ($\pm$ 0.00) & 80.64 ($\pm$ 0.00)  & 74.79 ($\pm$ 0.05)  & 69.64 ($\pm$ 0.05) &  64.12 ($\pm$ 0.04) &  58.90 ($\pm$ 0.10) \\
%     \midrule
%      GaitSet + Naive CLS & 83.09 ($\pm$ 0.00)  & 76.94 ($\pm$ 0.00)  & 71.86 ($\pm$ 0.02) &  67.00 ($\pm$ 0.07) &  63.28 ($\pm$ 0.06) \\
%     \midrule
%     GaitSet + Verification  & 80.53 ($\pm$ 0.01)  & 72.26 ($\pm$ 0.02)  & 64.85 ($\pm$ 0.02) &  56.83 ($\pm$ 0.02) &  49.22 ($\pm$ 0.03) \\
%     \midrule
%     GaitSet + Ours   & \textbf{86.64 ($\pm$ 0.03)}  & \textbf{85.16 ($\pm$ 0.02)} & \textbf{84.26 ($\pm$ 0.03)} & \textbf{83.84 ($\pm$ 0.06)} & \textbf{84.09 ($\pm$ 0.03)}
%     \\
%     \midrule
%     \midrule
%     GaitPart & 80.97  & 71.39  & 62.65  & 52.96  & 43.23  \\
%     \midrule
%     %  GaitPart + Verification & 87.08 ($\pm$ 0.00) & 81.94 ($\pm$ 0.01)  & 75.78 ($\pm$ 0.03)  & 70.23 ($\pm$ 0.14) &  64.46 ($\pm$ 0.05) &  59.00 ($\pm$ 0.08) \\
%     % \midrule
%      GaitPart + Naive CLS  & 85.78 ($\pm$ 0.02)  & 80.64 ($\pm$ 0.02)  & 76.49 ($\pm$ 0.05) &  72.71 ($\pm$ 0.06) &  70.05 ($\pm$ 0.08) \\
%     \midrule
%     GaitPart + Verification  & 81.88 ($\pm$ 0.01)  & 73.65 ($\pm$ 0.02)  & 66.37 ($\pm$ 0.01) &  58.54 ($\pm$ 0.03) &  51.16 ($\pm$ 0.02) \\
%     \midrule
%     GaitPart + Ours   & \textbf{88.41 ($\pm$ 0.01)}  & \textbf{86.96 ($\pm$ 0.03)} & \textbf{86.03 ($\pm$ 0.06)} & \textbf{85.53 ($\pm$ 0.01)} & \textbf{85.68 ($\pm$ 0.01)} \\
%     \midrule
%     \midrule
%     GaitGL & 82.15  & 72.35  & 63.45  & 53.58  & 43.68  \\
%     \midrule
%     % GaitGL + Verification & 90.07 ($\pm$ 0.00) & 83.46 ($\pm$ 0.02)  & 75.50 ($\pm$ 0.04)  & 68.42 ($\pm$ 0.04) &  60.73 ($\pm$ 0.04) &  53.34 ($\pm$ 0.02)  \\
%     % \midrule
%     GaitGL + Naive CLS & 88.81 ($\pm$ 0.01)  & 84.99 ($\pm$ 0.03)  & 82.02 ($\pm$ 0.05) &  79.51 ($\pm$ 0.05) &  77.87 ($\pm$ 0.08)  \\
%     \midrule
%     GaitGL + Verification & 83.06 ($\pm$ 0.00)  & 74.14 ($\pm$ 0.02)  & 66.19 ($\pm$ 0.02) & 57.67 ($\pm$ 0.06) &  49.52 ($\pm$ 0.07) \\
%     \midrule
%     GaitGL + Ours  & \textbf{90.27 ($\pm$ 0.01)}  & \textbf{89.03 ($\pm$ 0.03)} & \textbf{88.21 ($\pm$ 0.04)} & \textbf{87.82 ($\pm$ 0.02)} & \textbf{87.87 ($\pm$ 0.05)} \\
%     \bottomrule
%     \end{tabular}%
%     }
%     \vspace{-1.0em}
%   \label{tab:tab1}%
% \end{table*}%

% Table generated by Excel2LaTeX from sheet 'Sheet1'
\begin{table*}[t]
\footnotesize
% \sisetup{table-number-alignment=center}
  \centering
  \caption{Rank-1 accuracy (\%) on the OUMVLP dataset under different OOD percentages. The standard deviation is shown in parentheses. 
  For a fair comparison, we join the backbones with different OOD detection strategies, e.g., Naive CLS, Verification and Anomaly Detection. {Here, in the default evaluation protocol, 7\% of the probes are OOD queries.}
% For a fair comparison, we combined the backbones with three OOD detection strategies, respectively.
  }
  % \vspace{-0.8em}
%   \resizebox{0.80\textwidth}{!}{
\renewcommand{\arraystretch}{0.5}
  \setlength{\tabcolsep}{2.7mm}{
% \resizebox{10mm}{12mm}
    \begin{tabular}{l|c|c|c|c|c|c}
    \toprule
    \multicolumn{1}{c|}{\multirow{2}[2]{*}{Methods}} & \multicolumn{6}{c}{Different OOD percentages} \\
    \cmidrule{2-7} & \multicolumn{1}{c|}{7\%} & \multicolumn{1}{c|}{15\%} & \multicolumn{1}{c|}{25\%} & \multicolumn{1}{c|}{35\%} & \multicolumn{1}{c|}{45\%} & \multicolumn{1}{c}{55\%} \\
    \midrule
    GaitSet & 87.34  & 79.71  & 70.35  & 61.80  & 52.29  & 42.73  \\
    % \midrule
    %  GaitSet + Verification & 85.46 ($\pm$ 0.00) & 80.64 ($\pm$ 0.00)  & 74.79 ($\pm$ 0.05)  & 69.64 ($\pm$ 0.05) &  64.12 ($\pm$ 0.04) &  58.90 ($\pm$ 0.10) \\
    \midrule
     GaitSet + Naive CLS & 88.43 ($\pm$ 0.00) & 83.09 ($\pm$ 0.00)  & 76.94 ($\pm$ 0.00)  & 71.86 ($\pm$ 0.02) &  67.00 ($\pm$ 0.07) &  63.28 ($\pm$ 0.06) \\
    \midrule
    GaitSet + Verification & 87.37 ($\pm$ 0.00) & 80.53 ($\pm$ 0.01)  & 72.26 ($\pm$ 0.02)  & 64.85 ($\pm$ 0.02) &  56.83 ($\pm$ 0.02) &  49.22 ($\pm$ 0.03) \\
    \midrule
    GaitSet + Anomaly Detection & 87.85 ($\pm$ 0.00) & 81.24 ($\pm$ 0.03)  & 73.39 ($\pm$ 0.03)  & 66.58 ($\pm$ 0.02) &  59.54 ($\pm$ 0.06) &  53.40 ($\pm$ 0.06) \\
    \midrule    
    GaitSet + Ours & \textbf{88.14 ($\pm$ 0.00)}  & \textbf{86.64 ($\pm$ 0.03)}  & \textbf{85.16 ($\pm$ 0.02)} & \textbf{84.26 ($\pm$ 0.03)} & \textbf{83.84 ($\pm$ 0.06)} & \textbf{84.09 ($\pm$ 0.03)}
    \\
    \midrule
    \midrule
    GaitPart & 88.80  & 80.97  & 71.39  & 62.65  & 52.96  & 43.23  \\
    \midrule
    %  GaitPart + Verification & 87.08 ($\pm$ 0.00) & 81.94 ($\pm$ 0.01)  & 75.78 ($\pm$ 0.03)  & 70.23 ($\pm$ 0.14) &  64.46 ($\pm$ 0.05) &  59.00 ($\pm$ 0.08) \\
    % \midrule
     GaitPart + Naive CLS & 90.38 ($\pm$ 0.00) & 85.78 ($\pm$ 0.02)  & 80.64 ($\pm$ 0.02)  & 76.49 ($\pm$ 0.05) &  72.71 ($\pm$ 0.06) &  70.05 ($\pm$ 0.08) \\
    \midrule
    GaitPart + Verification & 88.72 ($\pm$ 0.00) & 81.88 ($\pm$ 0.01)  & 73.65 ($\pm$ 0.02)  & 66.37 ($\pm$ 0.01) &  58.54 ($\pm$ 0.03) &  51.16 ($\pm$ 0.02) \\
    \midrule
    GaitPart + Anomaly Detection & 89.66 ($\pm$ 0.00) & 83.50 ($\pm$ 0.01)  & 76.33 ($\pm$ 0.01)  & 70.22 ($\pm$ 0.02) &  64.15 ($\pm$ 0.11) &  59.11  ($\pm$ 0.10) \\
    \midrule
    GaitPart + Ours & \textbf{89.88 ($\pm$ 0.00)}  & \textbf{88.41 ($\pm$ 0.01)}  & \textbf{86.96 ($\pm$ 0.03)} & \textbf{86.03 ($\pm$ 0.06)} & \textbf{85.53 ($\pm$ 0.01)} & \textbf{85.68 ($\pm$ 0.01)} \\
    \midrule
    \midrule
    GaitGL & 90.17  & 82.15  & 72.35  & 63.45  & 53.58  & 43.68  \\
    \midrule
    % GaitGL + Verification & 90.07 ($\pm$ 0.00) & 83.46 ($\pm$ 0.02)  & 75.50 ($\pm$ 0.04)  & 68.42 ($\pm$ 0.04) &  60.73 ($\pm$ 0.04) &  53.34 ($\pm$ 0.02)  \\
    % \midrule
    GaitGL + Naive CLS & 92.27 ($\pm$ 0.00) & 88.81 ($\pm$ 0.01)  & 84.99 ($\pm$ 0.03)  & 82.02 ($\pm$ 0.05) &  79.51 ($\pm$ 0.05) &  77.87 ($\pm$ 0.08)  \\
    \midrule
    GaitGL + Verification & 90.47 ($\pm$ 0.00) & 83.06 ($\pm$ 0.00)  & 74.14 ($\pm$ 0.02)  & 66.19 ($\pm$ 0.02) & 57.67 ($\pm$ 0.06) &  49.52 ($\pm$ 0.07) \\
    \midrule
    GaitGL + Anomaly Detection & 91.53 ($\pm$ 0.00) & 86.21 ($\pm$ 0.02)  & 80.15 ($\pm$ 0.05)  & 75.15 ($\pm$ 0.06) & 70.43 ($\pm$ 0.05) &  66.77  ($\pm$ 0.09) \\
    \midrule
    GaitGL + Ours & \textbf{91.52 ($\pm$ 0.00)}  & \textbf{90.27 ($\pm$ 0.01)}  & \textbf{89.03 ($\pm$ 0.03)} & \textbf{88.21 ($\pm$ 0.04)} & \textbf{87.82 ($\pm$ 0.02)} & \textbf{87.87 ($\pm$ 0.05)} \\

    \midrule
    \midrule
    GaitBase &   92.88  &	84.91 &	74.65 &	64.94 &	54.96 &	44.81  \\
    \midrule
    GaitBase + Naive CLS &  96.35 ($\pm$ 0.00) &	92.47 ($\pm$ 0.02) &	88.21 ($\pm$ 0.01) &	84.90 ($\pm$ 0.02)	& 82.01 ($\pm$ 0.05) &	80.26 ($\pm$ 0.05)\\
    \midrule
    GaitBase + Verification &  97.24 ($\pm$ 0.00) &	94.74 ($\pm$ 0.03) &	92.02 ($\pm$ 0.03) 	& 89.80 ($\pm$ 0.02) &	87.76 ($\pm$ 0.03)	& 86.32 ($\pm$ 0.02) \\
    \midrule
    GaitBase + Anomaly Detection & 94.83 ($\pm$ 0.00) &	88.85 ($\pm$ 0.01) &	82.06 ($\pm$ 0.04) &	76.43 ($\pm$ 0.02) &	71.06 ($\pm$ 0.05) &	67.04  ($\pm$ 0.07)\\
    \midrule
    GaitBase + Ours &  \textbf{98.63 ($\pm$ 0.00)} &	\textbf{97.48 ($\pm$ 0.01)} &	\textbf{96.33 ($\pm$ 0.00)} &	\textbf{95.50 ($\pm$ 0.02)} &	\textbf{94.93 ($\pm$ 0.03)} &	\textbf{94.73 ($\pm$ 0.00)} \\

\midrule
    \midrule
    DyGait &   92.94  &	84.96 &	74.68 &	64.97 	& 54.98 &	44.82   \\
    \midrule
    DyGait + Naive CLS & 97.30 ($\pm$ 0.00) &	94.48 ($\pm$ 0.05) & 91.51 ($\pm$ 0.01) &	89.29 ($\pm$ 0.03) &	87.48 ($\pm$ 0.07) &	86.61 ($\pm$ 0.04)  \\
    \midrule
    DyGait + Verification & 97.28 ($\pm$ 0.00) &	94.19 ($\pm$ 0.06) &	90.74 ($\pm$ 0.05) & 87.89 ($\pm$ 0.05) &	85.28 ($\pm$ 0.06) &	83.41 ($\pm$ 0.08) \\
    \midrule
    DyGait + Anomaly Detection & 95.60 ($\pm$ 0.00) &	90.61 ($\pm$ 0.05) &	85.11 ($\pm$ 0.04) &	80.72 ($\pm$ 0.07) &	76.83 ($\pm$ 0.02) &	74.29 ($\pm$ 0.04) \\
    \midrule
    DyGait + Ours & \textbf{99.00 ($\pm$ 0.00)} &	\textbf{98.13 ($\pm$ 0.04)} &	\textbf{97.26 ($\pm$ 0.01)} &	\textbf{96.65 ($\pm$ 0.02)} &	\textbf{96.20 ($\pm$ 0.03)}	& \textbf{96.08  ($\pm$ 0.02)} \\

    
    
    \bottomrule
    \end{tabular}%
    }
    % \vspace{-1.2em}
  \label{tab:tab1}%
\end{table*}%

\subsection{Dataset}
% We evaluate the effectiveness of our method on one of the largest gait recognition benchmarks OUMVLP \cite{takemura2018multi}.
% We evaluate the effectiveness of our method on two gait recognition benchmarks OUMVLP \cite{takemura2018multi} and Gait3D \cite{zheng2022gait}. The details of these datasets are depicted as follows.
We evaluate the effectiveness of our method on three gait recognition benchmarks OUMVLP \cite{takemura2018multi}, CASIA-B \cite{yu2006framework} and Gait3D \cite{zheng2022gait}. 
Moreover, to verify the applicability of our method to other tasks, we also conduct experiments on other similar tasks, such as vehicle re-identification \cite{lou2019large, lou2021large} and synthetic person-identification benchmark VC-Clothes \cite{wan2020person}. The details of these datasets are depicted as follows.

\noindent \textbf{OUMVLP} \cite{takemura2018multi} is a large-scale gait dataset. 
It includes 10,307 subjects, each of which contains a gallery set and a probe set. Each subject in a set is collected from 14 views ($0^\circ$-$90^\circ$ and $180^\circ$-$270^\circ$ with a sampling interval of  $15^\circ$). In this work, we use the evaluation protocol \cite{chao2019gaitset,fan2020gaitpart,lin2021gait} to verify the effectiveness of the proposed uncertainty-aware gait recognition method. 
Under the default evaluation protocol, 7\% of the probes are OOD queries, which brings a challenge to the state-of-the-art methods. In addition, we manually remove some gallery sets of subjects to construct more severe OOD query scenarios.


\noindent \textbf{CASIA-B} \cite{yu2006framework} is one of the most popular gait datasets. It includes 124 subjects and about 1.3K sequences. Each subject is collected in sequences from 11 views ($0^{\circ}$, $18^{\circ}$, \ldots, $180^{\circ}$). For each view, 10 groups of sequences are collected.  In our experiments, the CASIA-B dataset is divided into three subsets: a training set, a query set, and a gallery set. To be specific, 74 IDs\footnote{Here, ID refers to identity. With a slight abuse of notation, we refer ID to as in-distribution for simplicity.} are used to construct the training set and the remaining 50 IDs are used as the query and gallery sets. 
We select one image from each ID as the gallery set and the rest images as the query set. By removing IDs from the gallery set, we construct various OOD query cases.

\noindent \textbf{Gait3D} \cite{zheng2022gait} is a large-scale and in-the-wild dataset. It includes 4K subjects and 25K sequences captured by 39 surveillance cameras in the wild. In our experiments, the Gait3D dataset is divided into three subsets: a training set, a query set, and a gallery set. To be specific, 3,000 IDs\footnote{Here, ID refers to identity. With a slight abuse of notation, we refer ID to as in-distribution for simplicity.} are used to construct the training set and the remaining 1,000 IDs are used as the query and gallery sets. We select one image from each ID as the query set and the rest images as the gallery set. By removing IDs from the gallery set, we construct various OOD query cases. {
Note that, in the default evaluation protocol, 5\% of the probes are OOD queries. Moreover, we manually remove some subjects in the gallery set to construct more severe OOD query scenarios.}

\noindent \textbf{VERI-Wild} \cite{lou2019large, lou2021large} is a large-scale dataset used for vehicle re-identification tasks.
It includes 416,314 images captured by 174 surveillance cameras in the wild. In general, the VERI-Wild dataset is divided into three subsets: a training set, a query set, and a gallery set. 
We use the evaluation protocol as in \cite{he2020fastreid}, which includes three different settings, \emph{i.e.}, Small-size Setting (SS), Medium-size Setting (MS) and Large-size Setting (LS). 
In the three settings, 3,000 IDs, 5,000 IDs and 10,000 IDs are used to construct the query and gallery sets, respectively. In each setting, we select one image from each ID as the gallery set and the rest images as the probe set. 
By removing IDs from the gallery set or probe set, we construct various OOD query cases.


\noindent \textbf{VC-Clothes}\cite{wan2020person} is a synthetic person re-identification dataset. It includes 512 subjects and each subject has 36 corresponding images. 
The dataset has been divided into two subsets: a training set and a testing set. Each set contains 256 IDs. 
In the inference stage, we randomly choose four images from each subject for the gallery set and the rest images are used in the probe set. Hence, the gallery and probe sets contain 1020 images and 8591 images, respectively.
Since we construct OOD queries on both OUMVLP, CASIA-B and Gait3D datasets, we will release all OOD configurations for reproducibility.




\subsection{Implementation Details}
Our uncertainty-aware classification model is built on multilayer perceptron (MLP) including two FC layers. 
For the OUMVLP dataset, we take three common gait recognition frameworks, GaitGL \cite{lin2021gait}, GaitPart \cite{fan2020gaitpart}, GaitSet \cite{chao2019gaitset}, OpenGait \cite{fan2023opengait} and DyGait \cite{wang2023dygait}, as the backbones to extract gait features and evaluate the effectiveness of our proposed uncertainty-aware classification model. Since the feature sizes of the five backbones are different, we set the hidden layer size of our MLP to 544, 544, 768, 768 and 769, respectively. 
For the CASIA-B dataset, the MLP hidden layer size of five frameworks is set to 8192.
% For the VERI-Wild dataset, BoT \cite{luo2019bag} is used as the backbone and thus the hidden layer size of MLP is set to 1024.
% For the VC-clothes dataset, we take CBN \cite{zhuang2020rethinking} as the backbone and the hidden layer size of MLP is set to 4096.
For the Gait3D dataset, we also take the five frameworks as the backbones to extract gait features and evaluate the effectiveness of our proposed uncertainty-aware classification model. We set the hidden layer size of our MLP to 8192. 
For the VERI-Wild dataset, BoT \cite{luo2019bag} is used as the backbone and thus the hidden layer size of MLP of the uncertainty-aware classification model is set to 1024. For the VC-clothes dataset, we take CBN \cite{zhuang2020rethinking} as the backbone and the hidden layer size of MLP is set to 4096.
% The hyper-parameters $P$ and $K$ of the proposed sampling strategy are set to 32 and 8 respectively. The learning rate is set to 1e-3 and then decays by a factor of 0.1 every 10 iterations. The total training iteration number for all the experiments is set to 50. 
% In our experiments, we also run each OOD setting by three times in order to demonstrate the robustness of our method against OOD queries.

In Sec.~\ref{sec:query_pair_constr}, we propose a sampling strategy that constructs positive and negative pairs of samples to train our model. The proposed sampling strategy includes two hyper-parameters $P$ and $V$. $P$ and $V$ are set to 32 and 8 respectively for all the experiments. The learning rate is set to 1e-3 and then decays by a factor of 0.1 every 10 iterations.
The total training iteration number for all the experiments is set to 50. 
In our experiments, we also run each OOD setting by three times in order to demonstrate the robustness of our method against OOD queries. Specifically, in each OOD setting, we randomly remove a certain percentage of the identities of the original gallery set three times to construct a different gallery set. We will release our training details and codes.

% \subsection{Baseline Algorithms}
% {\textcolor{cyan}{Since existing backbones cannot handle OOD cases, we introduce different strategies and join them with the backbone to construct baseline algorithms. In other words, the upper bound of baseline algorithms under different OOD percentages is 100\%. The details of baseline algorithms are as follows:}}

% \noindent {\textcolor{cyan}{ (1) \textbf{Backbone + Verification}: A classifier is designed to classify whether two features are from the same identity or not.}}

% \noindent {\textcolor{cyan}{ (2) \textbf{Backbone + Threshold}: Setting a distance threshold is also a straightforward way to address OOD cases. When the distance between the probe and the closest sample from the gallery is larger than the threshold, the probe will be regarded as an OOD query. In this paper, the threshold is calculated by Equal Error Rate (EER) in the verification setting. To be specific, we construct 20k gait pairs, including 10k paired samples and 10k unpaired samples. Then, we search for the best threshold leading to the lowest EER. Finally, the searched threshold is used to further determine whether two samples are from the same identity.}}

% \begin{table*}[t]
% \footnotesize
%   \centering
%   \caption{Rank-1 accuracy (\%) on the VERI-Wild dataset under different OOD percentages. The standard deviation is reported in parentheses.}
% %   \vspace{-0.5em}
%   \resizebox{0.80\textwidth}{!}{
%     \begin{tabular}{c|c|c|c|c|c|c}
%     \toprule
%     \multirow{2}[2]{*}{Setting} & \multirow{2}[2]{*}{Methods} & \multicolumn{5}{c}{Different OOD percentages} \\
% \cmidrule{3-7}          &       & \multicolumn{1}{c|}{15\%} & \multicolumn{1}{c|}{25\%} & \multicolumn{1}{c|}{35\%} & \multicolumn{1}{c|}{45\%} & \multicolumn{1}{c}{55\%} \\
%     \midrule
% \multirow{5}[2]{*}{SS (\#ID=3k)} & BoT   & 74.07  & 66.02  & 57.86  & 49.40  & 40.90  \\
% \cmidrule{2-7}          & BoT + NN (verification protocol) & 69.26 ($\pm$ 0.33)  & 66.76 ($\pm$ 0.26) & 64.81 ($\pm$ 0.14) &  63.25 ($\pm$ 0.15) &  62.82 ($\pm$ 0.30)  \\
% \cmidrule{2-7}          & BoT + Threshold (EER)  & 74.10 ($\pm$ 0.20)  & 66.36 ($\pm$ 0.28) & 58.99 ($\pm$ 0.11) &  52.01 ($\pm$ 0.01) &  46.24 ($\pm$ 0.35) \\
% \cmidrule{2-7}          & BoT + Ours & \textbf{78.19 ($\pm$ 0.21)} & \textbf{76.34 ($\pm$ 0.20)} & \textbf{75.09 ($\pm$ 0.05)} & \textbf{74.71 ($\pm$ 0.21)} & \textbf{75.34 ($\pm$ 0.25)} \\
%     \midrule
%     \midrule

%     \multirow{5}[2]{*}{MS (\#ID=5k)} & BoT   & 71.00  & 63.64  & 55.83  & 47.73  & 39.87  \\
% \cmidrule{2-7}          & BoT + NN (verification protocol) & 64.81 ($\pm$ 0.15)  & 61.65 ($\pm$ 0.34) & 59.33 ($\pm$ 0.17) &  57.17 ($\pm$ 0.40) &  55.84 ($\pm$ 0.42) \\
% \cmidrule{2-7}          & BoT + Threshold (EER) & 70.85 ($\pm$ 0.16)  & 63.34 ($\pm$ 0.30) & 55.86 ($\pm$ 0.27) &  48.17 ($\pm$ 0.17) &  41.04 ($\pm$ 0.26)\\
% \cmidrule{2-7}          & BoT + Ours & \textbf{73.70 ($\pm$ 0.22)} & \textbf{70.85 ($\pm$ 0.43)} & \textbf{68.97 ($\pm$ 0.20)} & \textbf{67.65 ($\pm$ 0.21)} & \textbf{67.36 ($\pm$ 0.23)} \\

% \midrule
% \midrule

%     \multirow{5}[2]{*}{LS (\#ID=10k)} & BoT   & 65.58  & 58.65  & 51.54  & 44.15  & 36.82  \\
% \cmidrule{2-7}          & BoT + NN (verification protocol) & 59.25 ($\pm$ 0.06)  & 56.28 ($\pm$ 0.20) & 53.39 ($\pm$ 0.04) &  50.99 ($\pm$ 0.36) &  49.15 ($\pm$ 0.39) \\
% \cmidrule{2-7}          & BoT + Threshold (EER)  & 65.43 ($\pm$ 0.11)  & 58.63 ($\pm$ 0.06) & 51.57 ($\pm$ 0.02) &  44.40 ($\pm$ 0.16) &  37.14 ($\pm$ 0.15) \\
% \cmidrule{2-7}          & BoT + Ours & \textbf{67.45 ($\pm$ 0.06)} & \textbf{64.14 ($\pm$ 0.22)} & \textbf{61.25 ($\pm$ 0.16)} & \textbf{59.07 ($\pm$ 0.48)} & \textbf{57.92 ($\pm$ 0.54)} \\
% \bottomrule
%     \end{tabular}%
%     }
%   \label{tab_vehicle}%
% \vspace{-1.0em}
% \end{table*}%

% Table generated by Excel2LaTeX from sheet 'Sheet1'
\begin{table*}[t]
\footnotesize
% \sisetup{table-number-alignment=center}
  \centering
  \caption{Rank-1 accuracy (\%) on the CASIA-B dataset under different OOD percentages. The standard deviation is shown in parentheses. For a fair comparison, we join the backbones with different OOD detection strategies, e.g., Naive CLS, Verification and Anomaly Detection.}
  % \vspace{-0.8em}
  \renewcommand{\arraystretch}{0.5}
\setlength{\tabcolsep}{2.8mm}{
% \resizebox{0.77\textwidth}{!}{
    \begin{tabular}{l|c|c|c|c|c|c}
    \toprule
    \multicolumn{1}{c|}{\multirow{2}[2]{*}{Methods}} & \multicolumn{6}{c}{Different OOD percentages} \\
    \cmidrule{2-7} & \multicolumn{1}{c|}{0\%} & \multicolumn{1}{c|}{15\%} & \multicolumn{1}{c|}{25\%} & \multicolumn{1}{c|}{35\%} & \multicolumn{1}{c|}{45\%} & \multicolumn{1}{c}{55\%} \\
    \midrule
    GaitSet & 91.81  & 79.39  & 70.92  & 63.23  & 53.22  & 42.53   \\

    \midrule
    GaitSet +  Naive CLS &  91.78 ($\pm$ 0.00) &	81.77 ($\pm$ 0.87) &	74.10 ($\pm$ 1.06) &	66.89 ($\pm$ 1.46) &	61.05 ($\pm$ 1.98) &	55.18 ($\pm$ 0.62) \\

    \midrule
    GaitSet +  Verification &  83.31 ($\pm$ 0.00) &	78.22 ($\pm$ 0.55) &	71.89 ($\pm$ 0.48) &	70.39 	($\pm$ 1.12) & 68.45 	($\pm$ 1.76) & 66.85 ($\pm$ 1.59) \\

    \midrule
    GaitSet +  Anomaly Detection & 91.72 ($\pm$ 0.00) &	 	81.69 ($\pm$ 0.79) &		74.78 ($\pm$ 0.89) &		68.17 ($\pm$ 1.91) &		62.30 	($\pm$ 1.43) &	55.34 ($\pm$ 1.00)   \\

    \midrule
    
    GaitSet + Ours & \textbf{91.38 ($\pm$ 0.00)}  & \textbf{83.25 ($\pm$ 1.13)}  & \textbf{76.55 ($\pm$ 0.77)} & \textbf{71.80 ($\pm$ 1.80)} & \textbf{68.94 ($\pm$ 3.62)} & \textbf{63.12 ($\pm$ 2.25)}
    
    
    \\
    \midrule
    \midrule
    GaitPart & 92.61  & 80.16 & 	72.19 	& 63.63 	& 53.81 &	44.20  \\

    \midrule
    GaitPart + Naive CLS  & 91.38 ($\pm$ 0.00)  &	81.10 ($\pm$ 0.75)  &	73.44 ($\pm$ 1.16)  &	66.95 ($\pm$ 0.22)  &	61.47 ($\pm$ 1.23)  &	56.70 ($\pm$ 1.13)    \\

    \midrule
    GaitPart + Verification &  81.00 ($\pm$ 0.00)  &	74.96 ($\pm$ 0.79)  &	68.28 ($\pm$ 0.80)  &	66.84 ($\pm$ 1.25)  &	65.36 ($\pm$ 1.42)  &	65.46 ($\pm$ 0.92)  \\

    \midrule
    GaitPart + Anomaly Detection & 91.59  ($\pm$ 0.00)  &	79.49  ($\pm$ 0.55)  & 	70.95  ($\pm$ 0.97)  &	63.04  ($\pm$ 0.82)  &	55.59  ($\pm$ 1.18)  &	49.45  ($\pm$ 1.33)  \\

    \midrule
    GaitPart + Ours & \textbf{90.94 ($\pm$ 0.00)}  & \textbf{81.80 ($\pm$ 0.37)}  & \textbf{74.93 ($\pm$ 0.64)} & \textbf{70.00($\pm$ 1.00)} & \textbf{66.22 ($\pm$ 1.63)} & \textbf{61.88  ($\pm$ 1.94)} \\
     \midrule
    \midrule
    GaitGL & 94.77  & 81.89  & 73.27  & 64.50  & 54.74  & 44.01  \\
    \midrule
    GaitGL + Naive CLS  & 94.47 ($\pm$ 0.00)  &	85.10 ($\pm$ 1.17 )  &	77.27 ($\pm$ 1.02)  &	71.71 	($\pm$ 1.28)  & 66.38 	($\pm$ 0.58)  & 59.90 ($\pm$ 0.59)   \\

    \midrule
    GaitGL + Verification & 32.00  ($\pm$ 0.00)  &	41.62  ($\pm$ 0.14)  &	47.98  ($\pm$ 0.78)  &	54.79  ($\pm$ 0.63)  &	61.42 ($\pm$ 0.047  & 	68.33 ($\pm$ 0.60)   \\

    \midrule
    GaitGL + Anomaly Detection & 94.77 ($\pm$ 0.00)  &	83.55 ($\pm$ 0.56)  &	74.91 ($\pm$ 0.70)  &	66.62 ($\pm$ 1.46)  &	58.58 ($\pm$ 1.26)  &	48.93 ($\pm$ 0.75)    \\

    \midrule
    GaitGL + Ours & \textbf{93.79 ($\pm$ 0.00)}  & \textbf{85.61 ($\pm$ 1.08)}  & \textbf{79.06 ($\pm$ 0.92)} & \textbf{74.83  ($\pm$ 1.52)} & \textbf{70.96 ($\pm$ 0.61)} & \textbf{65.83 ($\pm$ 0.27)} \\

    \midrule
    \midrule
    GaitBase & 96.01 &	82.97 &	74.11 	& 64.44 &	54.64 	& 44.82  \\

    \midrule
    GaitBase + Naive CLS  & 95.89 ($\pm$ 0.00)  & 	84.69 ($\pm$ 0.29)  & 	77.65 ($\pm$ 1.29)  & 	69.71 ($\pm$ 0.54)  & 	63.37 ($\pm$ 1.80)  & 	57.36  ($\pm$ 0.65)   \\

    \midrule
    GaitBase + Verification & 96.01 ($\pm$ 0.00)  & 	82.89 ($\pm$ 0.57)  & 	73.78 ($\pm$ 0.66)  &  	64.42 ($\pm$ 0.65)  & 	55.32 ($\pm$ 0.15)  & 	47.02 ($\pm$ 1.06)  \\

    \midrule
    GaitBase + Anomaly Detection & 95.89 ($\pm$ 0.00)  &	84.36 ($\pm$ 0.31)  & 	77.05 ($\pm$ 1.07)  & 	68.75 ($\pm$ 0.36)  &	62.10 ($\pm$ 1.70)  &	55.44 ($\pm$ 0.51)  \\

    \midrule
    GaitBase + Ours & \textbf{95.45 ($\pm$ 0.00)}  &	\textbf{86.75 ($\pm$ 0.75)}  &	\textbf{81.53 ($\pm$ 1.36)}  &	\textbf{76.20 ($\pm$ 0.69)}  &	\textbf{72.43 ($\pm$ 2.45)}  &	\textbf{68.62 ($\pm$ 2.78)}   \\


    \midrule
    \midrule
    DyGait & 97.28 &	84.05 	& 74.94 &	64.41 	& 54.47 &	44.56   \\

    \midrule
    DyGait + Naive CLS  & 97.28 ($\pm$ 0.00)  &	86.22 ($\pm$ 0.32)  &	77.66 ($\pm$ 0.31)  &	70.75 ($\pm$ 0.88)  &	63.67 ($\pm$ 0.75)  &	57.26 ($\pm$ 1.02)  \\

    \midrule
    DyGait + Verification &  42.53 ($\pm$ 0.00)  &	50.95 ($\pm$ 0.29)  &	56.66 ($\pm$ 0.88)  &	62.23 ($\pm$ 1.50)  &	67.79 	($\pm$ 1.10)  & 73.88 ($\pm$ 1.25)   \\

    \midrule
    DyGait + Anomaly Detection & 97.28 ($\pm$ 0.00)  &	86.23 ($\pm$ 0.37)  &	77.68 ($\pm$ 0.33)  &	70.46 ($\pm$ 0.96)  &	63.16 ($\pm$ 0.51)  &	56.29 ($\pm$ 0.76)   \\

    
    \midrule
    DyGait + Ours & \textbf{95.76 ($\pm$ 0.00)}  &	\textbf{89.38 ($\pm$ 0.35)}  &	\textbf{85.06 ($\pm$ 0.89)}  &	\textbf{83.36 ($\pm$ 1.76)}  &	\textbf{81.66 	($\pm$ 1.42)}  & \textbf{77.98  ($\pm$ 1.48)}  \\



    \bottomrule
    \end{tabular}%
    }
    % \vspace{-1.2em}
  \label{tab_casia}%
\end{table*}%


\begin{table*}[t]
\footnotesize
% \sisetup{table-number-alignment=center}
  \centering
  \caption{Rank-1 accuracy (\%) on the Gait3D dataset under different OOD percentages. The standard deviation is shown in parentheses. For a fair comparison, we join the backbones with different OOD detection strategies, e.g., Naive CLS, Verification and Anomaly Detection. {Note that, the default evaluation protocol contains 5\% of the probes which are OOD queries.}}

%   \vspace{-1.0em}
% \renewcommand{\arraystretch}{0.5}
\setlength{\tabcolsep}{3.0mm}{
    \begin{tabular}{l|c|c|c|c|c|c}
    \toprule
    \multicolumn{1}{c|}{\multirow{2}[2]{*}{Methods}} & \multicolumn{5}{c}{Different OOD percentages} \\
    \cmidrule{2-7}& \multicolumn{1}{c|}{5\%} & \multicolumn{1}{c|}{15\%} & \multicolumn{1}{c|}{25\%} & \multicolumn{1}{c|}{35\%} & \multicolumn{1}{c|}{45\%} & \multicolumn{1}{c}{55\%} \\


    \midrule
    \midrule
    GaitSet   & 55.33 &	40.24 &	32.46 &	27.66 &	21.51 &	17.00  \\
    \midrule
    GaitSet + Naive CLS  & 54.27 ($\pm$ 0.00)  &	43.26 ($\pm$ 1.42)  &	40.12 ($\pm$ 1.40)  &	41.59 	($\pm$ 0.88)  &43.67 ($\pm$ 0.30)  &	45.72 ($\pm$ 1.18)   \\

    \midrule
    GaitSet + Verification & 55.33 	($\pm$ 0.00)  & 42.36 	($\pm$ 1.45)  & 34.38 	($\pm$ 1.02)  & 29.79 	($\pm$ 0.52)  & 26.41 ($\pm$ 1.46)  &	24.46 ($\pm$ 1.76)   \\

    \midrule
    GaitSet + Anomaly Detection & 55.61 ($\pm$ 0.00)  &	43.16 ($\pm$ 0.94)  &	37.78 ($\pm$ 0.73)  &	36.02 ($\pm$ 1.02)  &	35.63 ($\pm$ 0.48)  &	36.53 ($\pm$ 0.97)  \\
    \midrule
    GaitSet + Ours  & \textbf{51.20 ($\pm$ 0.00)}  &	\textbf{39.92 ($\pm$ 0.84)}  & \textbf{38.99 ($\pm$ 0.56)}  &	\textbf{41.82 ($\pm$ 0.93)}  &	\textbf{46.36 ($\pm$ 0.95)}  &	\textbf{51.33 ($\pm$ 0.95)}    \\
    


    \midrule
    \midrule
    GaitPart   &  43.32 	& 29.49 &	23.72 &	20.17 &	15.75 &	12.48 \\
    \midrule
    GaitPart + Naive CLS  & 39.09 ($\pm$ 0.00)  &	30.86 ($\pm$ 0.51)  &	31.54 ($\pm$ 0.23)  &	33.33 	($\pm$ 1.09)  &37.01 ($\pm$ 0.88)  &	38.97 ($\pm$ 0.66)   \\

    \midrule
    GaitPart + Verification & 43.41 ($\pm$ 0.00)  &	30.93 ($\pm$ 0.90)  &	24.94 ($\pm$ 0.67)  &	21.61 	($\pm$ 0.67)  &19.24($\pm$ 1.10)  & 	17.29 ($\pm$ 0.92)  \\

    \midrule
    GaitPart + Anomaly Detection & 40.63 ($\pm$ 0.00)  &		30.58 ($\pm$ 1.20)  &		29.10 ($\pm$ 0.89)  &		29.26 ($\pm$ 0.71)  &		30.89 ($\pm$ 0.73)  &		30.93 ($\pm$ 0.69)  \\
    \midrule
    GaitPart + Ours  & \textbf{39.86 ($\pm$ 0.00)}  &	\textbf{30.70 ($\pm$ 0.84)}  &	\textbf{31.86 ($\pm$ 0.32)}  &	\textbf{35.83 ($\pm$ 0.55)}  &	\textbf{41.08 ($\pm$ 0.71)}  &	\textbf{45.18 ($\pm$ 1.53)}   \\

    \midrule
    \midrule
    GaitGL   &  73.77  &	56.48  &	48.41  &	40.44 	 & 31.69  &	24.78   \\
    \midrule
    GaitGL + Naive CLS  & 73.87 ($\pm$ 0.00)  &	58.59 ($\pm$ 0.39)  &	53.05 ($\pm$ 0.57)  &	48.22 ($\pm$ 0.36)  &	46.46 ($\pm$ 0.91)  &	45.18 ($\pm$ 1.41)   \\

    \midrule
    GaitGL + Verification & 72.04 ($\pm$ 0.00)  &		56.80 ($\pm$ 0.12)  &		50.24 ($\pm$ 0.68)  &		45.02 ($\pm$ 0.70)  &		42.55 ($\pm$ 1.01)  &		40.28 ($\pm$ 1.01)  \\
    
    \midrule
    GaitGL + Anomaly Detection & 73.77 ($\pm$ 0.00)  &		57.79 ($\pm$ 0.53)  &		50.75($\pm$ 0.87)  &	 	44.54 ($\pm$ 0.36)  &		40.92 ($\pm$ 1.60)  &		38.48 ($\pm$ 1.00)   \\
    
    \midrule
    GaitGL + Ours  & \textbf{74.15 ($\pm$ 0.00)}  & 	\textbf{59.49 ($\pm$ 1.58)}  &	\textbf{56.06 ($\pm$ 0.89)}  &	\textbf{53.40 ($\pm$ 1.49)}  &	\textbf{54.05 ($\pm$ 0.82)}  &	\textbf{55.00($\pm$ 0.80)}   \\
    
    \midrule
    \midrule
    GaitBase   & 75.02 	& 57.15  &	47.74 &	40.82 &	34.48 &	27.56  \\
    \midrule
    GaitBase + Naive CLS  &  73.29 ($\pm$ 0.00)  &	60.13 ($\pm$ 1.44)  &	55.20 	($\pm$ 1.30)  & 54.24($\pm$ 1.18)  & 	55.30 ($\pm$ 0.79)  &	57.05 ($\pm$ 0.77)   \\

    \midrule
    GaitBase + Verification  & 74.92  ($\pm$ 0.00)  &	59.39  ($\pm$ 1.65)  &	49.88  ($\pm$ 1.33)  &	42.26  ($\pm$ 0.55)  &	36.40  ($\pm$ 1.03)  &	32.46  ($\pm$ 1.31)   \\

    \midrule
    GaitBase + Anomaly Detection & 74.92 ($\pm$ 0.00)  &	59.65 ($\pm$ 1.69)  &	51.00 ($\pm$ 1.23)  &	44.38 	($\pm$ 0.47)  &39.73 ($\pm$ 0.53)  &	37.27($\pm$ 1.41)    \\
    \midrule
    GaitBase + Ours  & \textbf{75.31  ($\pm$ 0.00)}  &	\textbf{60.35  ($\pm$ 1.11)}  &	\textbf{56.09  ($\pm$ 1.26)}  &	\textbf{55.52  ($\pm$ 1.54)}  &	\textbf{57.06  ($\pm$ 0.75)}  &	\textbf{59.07  ($\pm$ 0.14)}  \\

    \midrule
    \midrule
    DyGait   &  75.98 &	62.05 &	52.25 &	45.43 &	37.46 &	30.45 \\
    \midrule
    DyGait + Naive CLS  & 76.46 ($\pm$ 0.00)  &		65.96 ($\pm$ 0.92)  &		60.39 ($\pm$ 1.45)  &	 	58.69 ($\pm$ 0.96)  &		59.42 	($\pm$ 0.83)  &	61.54  ($\pm$ 0.16)   \\

    \midrule
    DyGait + Verification  & 72.62 ($\pm$ 0.00)  &		62.18 ($\pm$ 0.73)  &	 	56.51 ($\pm$ 1.20)  &		54.43 ($\pm$ 0.25)  &	 	53.66 ($\pm$ 1.14)  &		53.92  ($\pm$ 0.71)   \\

    \midrule
    DyGait + Anomaly Detection & 76.17 ($\pm$ 0.00)  &		64.90 ($\pm$ 0.74)  &		58.05 ($\pm$ 1.22)  &		54.14 ($\pm$ 1.39)  &		52.42 ($\pm$ 1.89)  &		53.44 ($\pm$ 1.16)    \\
    \midrule
    DyGait + Ours  &  \textbf{76.27 ($\pm$ 0.00)}  &		\textbf{66.92 ($\pm$ 1.63)}  &		\textbf{62.91 ($\pm$ 1.71)}  &		\textbf{62.76 ($\pm$ 0.83)}  &		\textbf{64.48 ($\pm$ 0.71)}  &		\textbf{66.76 ($\pm$ 0.21)}   \\
    \bottomrule
    \end{tabular}%
        }
    % \vspace{-1.0em}
  \label{tab_gait3d}%
\end{table*}%

\subsection{Comparisons with the State-of-the-art}

% \paragraph{Evaluation on OUMVLP under different OOD percentages.} 
\noindent \textbf{Evaluation on OUMVLP under different OOD percentages:}
We introduce our uncertainty-aware classification model to several state-of-the-art gait recognition models, including GaitGL \cite{lin2021gait}, GaitPart \cite{fan2020gaitpart}, GaitSet \cite{chao2019gaitset}, OpenGait \cite{fan2023opengait} and DyGait \cite{wang2023dygait} on OUMVLP, and compare them with/without employing our method.
In the experiments, we run five gait recognition models on six test sets with different percentages of OOD samples. 
As indicated in Table \ref{tab:tab1}, after introducing our method, all of the gait recognition models obtain improvements on the test sets with OOD samples.
As the percentage of OOD samples increases in the test sets, we observe that existing methods suffer severe performance degradation. As expected, when the OOD query rate reaches 50\%, the accuracy of current methods almost decreases to half of their original performance (from 92.94\% to 44.82\% for DyGait).
This implies that current gait recognition methods do not have the ability to detect OOD probes. 
On the contrary, our uncertainty-aware classification model is able to identify the OOD queries and thus avoids assigning gallery IDs to the probes.
As a result, our method maintains high recognition accuracy while successfully recognizing OOD probes. 
This indicates the effectiveness and superiority of our proposed method.
Moreover, the small standard deviation of our results demonstrates the stability of our model in recognizing OOD samples.

% \paragraph{Evaluation on CASIA-B under different OOD percentages.}
\noindent \textbf{Evaluation on CASIA-B under different OOD percentages:}
For the CASIA-B dataset, we also choose GaitGL \cite{lin2021gait}, GaitPart \cite{fan2020gaitpart}, GaitSet \cite{chao2019gaitset}, OpenGait \cite{fan2023opengait} and DyGait \cite{wang2023dygait} as our baseline algorithms. Table. \ref{tab_casia} shows the experimental results.
It can be observed that our method decreases slightly performance without OOD (from 97.28\% to 95.76\% for DyGait).
This is because our method is prone to ensure the found matching pairs to be correct with minimal risk. In contrast, our method achieves promising performance in all OOD cases. To be specific, the accuracy of DyGait without our uncertainty model decreases significantly as the percentage of OOD samples increases (from 97.28\% to 44.56\% for GaitGL), while our method outperforms the baseline method for the six OOD percentages.


\noindent \textbf{Evaluation on Gait3D under different OOD percentages:}
For the Gait3D dataset, we choose GaitGL \cite{lin2021gait}, GaitPart \cite{fan2020gaitpart}, GaitSet \cite{chao2019gaitset}, OpenGait \cite{fan2023opengait} and DyGait \cite{wang2023dygait} as our baseline backbones. Table \ref{tab_gait3d} shows the experimental results. It can be observed that our method achieves a significant performance improvement in all cases. For instance, in the 55\% OOD percentage, our method outperforms DyGait by 36.31\%. Although our method still achieves a significant performance improvement in all OOD percentages, it suffers performance degradation in the 5\% OOD percentage for GaitSet and GaitPart backbones. The main reason is that previous SOTA methods cannot effectively extract discriminative features on Gait3D (the accuracy of SOTA methods is lower than 55\%). When the extracted features are not sufficiently discriminative, in handling in-distribution queries, our method tends to be inaccurate. This is because, in such cases, the distance between a matching pair is large, leading to a higher risk. Meanwhile our network prefers to accept matching pairs with minimal risk.

% \vspace{1mm}
\noindent \textbf{Evaluation on VERI-Wild and VC-Clothes under different OOD percentages:}
Although our model is designed to tackle OOD queries in gait recognition, it can be easily extended to other tasks, such as vehicle re-identification and person re-identification. 
To further verify the effectiveness of our method on other tasks, we conduct experiments on VERI-Wild and VC-Clothes.

For the VERI-Wild dataset, we choose the BoT framework \cite{luo2019bag} as our baseline algorithm.
The experimental results are shown in Table \ref{tab_vehicle}. It can be observed that our method (BoT + Ours) achieves appealing performance in all cases. To be specific, our method achieves stable recognition accuracy for the five OOD percentages while outperforming the baseline method. In contrast, the accuracy of BoT without our uncertainty model decreases significantly as the percentage of OOD samples increases.
For the VC-Clothes dataset, we employ the CBN framework \cite{zhuang2020rethinking} as our baseline algorithm. As indicated in Table \ref{tab_person}, our methods achieve approximately more than 97\% recognition accuracy in all the cases, \ie, OOD percentage ranges from 15\% to 55\%. On the contrary, the recognition accuracy of CBN without our uncertainty model is 44.98\% in the setting of 55\% OOD samples. 
This is because the current methods cannot address OOD queries and thus their methods only output erroneous recognition results. Note that, when we apply our method to the baseline methods, we do not need to re-train the baselines but treat them as off-the-shelf backbone networks. This also indicates that our method is able to effectively recognize matching feature pairs and OOD queries through its prediction and uncertainty modeling, thus demonstrating the superiority of our method to competing methods.



% Table generated by Excel2LaTeX from sheet 'Sheet1'
\begin{table*}[t]
\footnotesize
% \sisetup{table-number-alignment=center}
  \centering
  \caption{Rank-1 accuracy (\%) on the OUMVLP dataset under different OOD percentages. The standard deviation is shown in parentheses. 
  For a fair comparison, we join the backbones with different OOD detection strategies, e.g., Naive CLS, Verification and Anomaly Detection.
% For a fair comparison, we combined the backbones with three OOD detection strategies, respectively.
  }
  % \vspace{-0.8em}
%   \resizebox{0.80\textwidth}{!}{
% \renewcommand{\arraystretch}{0.5}
  \setlength{\tabcolsep}{2.6mm}{
% \resizebox{10mm}{12mm}
    \begin{tabular}{c|c|c|c|c|c|c}
    \toprule
    \multicolumn{1}{c|}{\multirow{2}[2]{*}{Methods}} & \multicolumn{6}{c}{Different OOD percentages} \\
    \cmidrule{2-7} & \multicolumn{1}{c|}{7\%} & \multicolumn{1}{c|}{15\%} & \multicolumn{1}{c|}{25\%} & \multicolumn{1}{c|}{35\%} & \multicolumn{1}{c|}{45\%} & \multicolumn{1}{c}{55\%} \\

    \midrule
    GaitGL & 90.17  & 82.15  & 72.35  & 63.45  & 53.58  & 43.68  \\
    \midrule
    % GaitGL + Threshold=0.4 & 90.17 ($\pm$ 0.00) & 82.15 ($\pm$ 0.00)  & 72.35 ($\pm$ 0.00)  & 63.45 ($\pm$ 0.00) & 53.58 ($\pm$ 0.00) &  43.68 ($\pm$ 0.00) \\
    % \midrule
    % GaitGL + Threshold=0.5 & 90.20 ($\pm$ 0.00) & 82.22 ($\pm$ 0.02)  & 72.50 ($\pm$ 0.01)  & 63.66 ($\pm$ 0.02) & 53.90 ($\pm$ 0.01) &  44.21 ($\pm$ 0.02) \\
    % \midrule   
    GaitGL + Threshold=0.6 & 90.41 ($\pm$ 0.00) & 84.74 ($\pm$ 0.01)  & 78.05 ($\pm$ 0.03)  & 72.30 ($\pm$ 0.03) & 66.46 ($\pm$ 0.12) &  61.23 ($\pm$ 0.12) \\
    \midrule   
    GaitGL + Threshold=0.7 & 73.08 ($\pm$ 0.00) & 74.21 ($\pm$ 0.02)  & 75.72 ($\pm$ 0.03)  & 77.24 ($\pm$ 0.04) & 79.21 ($\pm$ 0.07) &  81.35 ($\pm$ 0.06) \\
    \midrule   
    GaitGL + Threshold=0.8 & 27.34 ($\pm$ 0.00) & 33.87 ($\pm$ 0.02)  & 41.83 ($\pm$ 0.02)  & 49.07 ($\pm$ 0.00) & 57.09 ($\pm$ 0.03) &  65.07 ($\pm$ 0.05) \\
    \midrule   
    GaitGL + Threshold=0.9 & 9.42 ($\pm$ 0.00) & 17.56 ($\pm$ 0.01)  & 27.52 ($\pm$ 0.01)  & 36.57 ($\pm$ 0.02) & 46.56 ($\pm$ 0.01) &  56.51 ($\pm$ 0.01) \\
    \midrule       
    GaitGL + Naive CLS & 92.27 ($\pm$ 0.00) & 88.81 ($\pm$ 0.01)  & 84.99 ($\pm$ 0.03)  & 82.02 ($\pm$ 0.05) &  79.51 ($\pm$ 0.05) &  77.87 ($\pm$ 0.08)  \\
    \midrule
    GaitGL + Verification & 90.47 ($\pm$ 0.00) & 83.06 ($\pm$ 0.00)  & 74.14 ($\pm$ 0.02)  & 66.19 ($\pm$ 0.02) & 57.67 ($\pm$ 0.06) &  49.52 ($\pm$ 0.07) \\
    \midrule
    GaitGL + Anomaly Detection & 74.07 ($\pm$ 0.00) & 75.63 ($\pm$ 0.02)  & 77.58 ($\pm$ 0.04)  & 79.42 ($\pm$ 0.03) & 81.57 ($\pm$ 0.02) &  83.79 ($\pm$ 0.02) \\
    \midrule
    GaitGL + Ours & \textbf{91.52 ($\pm$ 0.00)}  & \textbf{90.27 ($\pm$ 0.01)}  & \textbf{89.03 ($\pm$ 0.03)} & \textbf{88.21 ($\pm$ 0.04)} & \textbf{87.82 ($\pm$ 0.02)} & \textbf{87.87 ($\pm$ 0.05)} \\
    \midrule
    \midrule
    DyGait &   92.94  &	84.96 &	74.68 &	64.97 	& 54.98 &	44.82   \\
    \midrule   
    DyGait + Threshold=0.6 & 92.94  ($\pm$ 0.00) &	84.96 ($\pm$ 0.00) &	74.68 ($\pm$ 0.00) &	64.97 	($\pm$ 0.00) & 54.98 ($\pm$ 0.00) &	44.82 ($\pm$ 0.00)   \\
    \midrule   
    DyGait + Threshold=0.7 &  93.36 ($\pm$ 0.00) &	85.00 ($\pm$ 0.01) &	74.80 ($\pm$ 0.02) &	65.54 ($\pm$ 0.03) &	55.39 ($\pm$ 0.01) &	45.33  ($\pm$ 0.02) \\
    \midrule   
    DyGait + Threshold=0.8 & 97.91 ($\pm$ 0.00) &	95.72 ($\pm$ 0.06) &	93.32 ($\pm$ 0.04) &	91.39 ($\pm$ 0.02) &	89.69 ($\pm$ 0.05) &	88.53 ($\pm$ 0.04) \\
    \midrule   
    DyGait + Threshold=0.9 & 31.50 ($\pm$ 0.00) &	37.64 ($\pm$ 0.03) &	45.19 ($\pm$ 0.02) &	52.03 ($\pm$ 0.05) &	59.57 ($\pm$ 0.03) &	67.09 ($\pm$ 0.05)  \\
    \midrule       
    DyGait + Naive CLS & 97.30 ($\pm$ 0.00) &	94.48 ($\pm$ 0.05) & 91.51 ($\pm$ 0.01) &	89.29 ($\pm$ 0.03) &	87.48 ($\pm$ 0.07) &	86.61 ($\pm$ 0.04)  \\
    \midrule
    DyGait + Verification & 97.28 ($\pm$ 0.00) &	94.19 ($\pm$ 0.06) &	90.74 ($\pm$ 0.05) & 87.89 ($\pm$ 0.05) &	85.28 ($\pm$ 0.06) &	83.41 ($\pm$ 0.08) \\
    \midrule
    DyGait + Anomaly Detection & 95.60 ($\pm$ 0.00) &	90.61 ($\pm$ 0.05) &	85.11 ($\pm$ 0.04) &	80.72 ($\pm$ 0.07) &	76.83 ($\pm$ 0.02) &	74.29 ($\pm$ 0.04) \\
    \midrule
    DyGait + Ours & \textbf{99.00 ($\pm$ 0.00)} &	\textbf{98.13 ($\pm$ 0.04)} &	\textbf{97.26 ($\pm$ 0.01)} &	\textbf{96.65 ($\pm$ 0.02)} &	\textbf{96.20 ($\pm$ 0.03)}	& \textbf{96.08  ($\pm$ 0.02)} \\


    
    \bottomrule
    \end{tabular}%
    }
    \vspace{-1.2em}
  \label{tab_threshold}%
\end{table*}%



\begin{table}[t]
% \vspace{-13cm}
\footnotesize
  \centering
  \caption{Rank-1 accuracy (\%) on the VERI-Wild dataset under different OOD percentages. The standard deviation is reported in parentheses. {SS, MS and LS indicate 3K, 5K and 10K training IDs, respectively.} }
%   \vspace{-0.5em}
% \renewcommand{\arraystretch}{0.5}
% \setlength{\tabcolsep}{3.0mm}{
\resizebox{0.48\textwidth}{!}{
    \begin{tabular}{c|c|c|c|c|c|c}
    \toprule
    \multirow{2}[2]{*}{Setting} & \multirow{2}[2]{*}{Methods} & \multicolumn{5}{c}{Different OOD percentages} \\
\cmidrule{3-7}          &       & \multicolumn{1}{c|}{15\%} & \multicolumn{1}{c|}{25\%} & \multicolumn{1}{c|}{35\%} & \multicolumn{1}{c|}{45\%} & \multicolumn{1}{c}{55\%} \\
    \midrule
\multirow{2}[2]{*}{SS} & BoT   & 74.07  & 66.02  & 57.86  & 49.40  & 40.90  \\
\cmidrule{2-7}          & BoT + Naive CLS & 69.26  & 66.76  & 64.81  &  63.25 &  62.82  \\
\cmidrule{2-7}          & BoT + Verification  & 74.10   & 66.36  & 58.99  &  52.01 &  46.24 \\
\cmidrule{2-7}          & BoT + Ours & \textbf{78.19} & \textbf{76.34} & \textbf{75.09} & \textbf{74.71} & \textbf{75.34} \\
    \midrule
    \midrule

    \multirow{2}[2]{*}{MS} & BoT   & 71.00  & 63.64  & 55.83  & 47.73  & 39.87  \\
\cmidrule{2-7}          & BoT + Naive CLS  & 64.81   & 61.65 & 59.33  &  57.17  &  55.84 \\
\cmidrule{2-7}          & BoT + Verification & 70.85   & 63.34  & 55.86 &  48.17  &  41.04 \\
\cmidrule{2-7}          & BoT + Ours & \textbf{73.70} & \textbf{70.85} & \textbf{68.97} & \textbf{67.65} & \textbf{67.36} \\

\midrule
\midrule

    \multirow{2}[2]{*}{LS} & BoT   & 65.58  & 58.65  & 51.54  & 44.15  & 36.82  \\
\cmidrule{2-7}          & BoT + Naive CLS & 59.25  & 56.28 & 53.39  &  50.99 &  49.15  \\
\cmidrule{2-7}          & BoT + Verification  & 65.43   & 58.63  & 51.57  &  44.40  &  37.14 \\
\cmidrule{2-7}          & BoT + Ours & \textbf{67.45} & \textbf{64.14} & \textbf{61.25} & \textbf{59.07} & \textbf{57.92} \\
\bottomrule
    \end{tabular}%
    }
  \label{tab_vehicle}%
\vspace{-1.0em}
\end{table}%

% \begin{table*}[t]
% % \vspace{-13cm}
% \footnotesize
%   \centering
%   \caption{Rank-1 accuracy (\%) on the VERI-Wild dataset under different OOD percentages. The standard deviation is reported in parentheses.}
% %   \vspace{-0.5em}
% % \renewcommand{\arraystretch}{0.5}
% \setlength{\tabcolsep}{3.8mm}{
%     \begin{tabular}{c|c|c|c|c|c|c}
%     \toprule
%     \multirow{2}[2]{*}{Setting} & \multirow{2}[2]{*}{Methods} & \multicolumn{5}{c}{Different OOD percentages} \\
% \cmidrule{3-7}          &       & \multicolumn{1}{c|}{15\%} & \multicolumn{1}{c|}{25\%} & \multicolumn{1}{c|}{35\%} & \multicolumn{1}{c|}{45\%} & \multicolumn{1}{c}{55\%} \\
%     \midrule
% \multirow{2}[2]{*}{SS (\#ID=3k)} & BoT   & 74.07  & 66.02  & 57.86  & 49.40  & 40.90  \\
% \cmidrule{2-7}          & BoT + NN (verification protocol) & 69.26 ($\pm$ 0.33)  & 66.76 ($\pm$ 0.26) & 64.81 ($\pm$ 0.14) &  63.25 ($\pm$ 0.15) &  62.82 ($\pm$ 0.30)  \\
% \cmidrule{2-7}          & BoT + Threshold (EER)  & 74.10 ($\pm$ 0.20)  & 66.36 ($\pm$ 0.28) & 58.99 ($\pm$ 0.11) &  52.01 ($\pm$ 0.01) &  46.24 ($\pm$ 0.35) \\
% \cmidrule{2-7}          & BoT + Ours & \textbf{78.19 ($\pm$ 0.21)} & \textbf{76.34 ($\pm$ 0.20)} & \textbf{75.09 ($\pm$ 0.05)} & \textbf{74.71 ($\pm$ 0.21)} & \textbf{75.34 ($\pm$ 0.25)} \\
%     \midrule
%     \midrule

%     \multirow{2}[2]{*}{MS (\#ID=5k)} & BoT   & 71.00  & 63.64  & 55.83  & 47.73  & 39.87  \\
% \cmidrule{2-7}          & BoT + NN (verification protocol) & 64.81 ($\pm$ 0.15)  & 61.65 ($\pm$ 0.34) & 59.33 ($\pm$ 0.17) &  57.17 ($\pm$ 0.40) &  55.84 ($\pm$ 0.42) \\
% \cmidrule{2-7}          & BoT + Threshold (EER) & 70.85 ($\pm$ 0.16)  & 63.34 ($\pm$ 0.30) & 55.86 ($\pm$ 0.27) &  48.17 ($\pm$ 0.17) &  41.04 ($\pm$ 0.26)\\
% \cmidrule{2-7}          & BoT + Ours & \textbf{73.70 ($\pm$ 0.22)} & \textbf{70.85 ($\pm$ 0.43)} & \textbf{68.97 ($\pm$ 0.20)} & \textbf{67.65 ($\pm$ 0.21)} & \textbf{67.36 ($\pm$ 0.23)} \\

% \midrule
% \midrule

%     \multirow{2}[2]{*}{LS (\#ID=10k)} & BoT   & 65.58  & 58.65  & 51.54  & 44.15  & 36.82  \\
% \cmidrule{2-7}          & BoT + NN (verification protocol) & 59.25 ($\pm$ 0.06)  & 56.28 ($\pm$ 0.20) & 53.39 ($\pm$ 0.04) &  50.99 ($\pm$ 0.36) &  49.15 ($\pm$ 0.39) \\
% \cmidrule{2-7}          & BoT + Threshold (EER)  & 65.43 ($\pm$ 0.11)  & 58.63 ($\pm$ 0.06) & 51.57 ($\pm$ 0.02) &  44.40 ($\pm$ 0.16) &  37.14 ($\pm$ 0.15) \\
% \cmidrule{2-7}          & BoT + Ours & \textbf{67.45 ($\pm$ 0.06)} & \textbf{64.14 ($\pm$ 0.22)} & \textbf{61.25 ($\pm$ 0.16)} & \textbf{59.07 ($\pm$ 0.48)} & \textbf{57.92 ($\pm$ 0.54)} \\
% \bottomrule
%     \end{tabular}%
%     }
%   \label{tab_vehicle}%
% \vspace{-1.0em}
% \end{table*}%

% Table generated by Excel2LaTeX from sheet 'Sheet1'
\begin{table}[t]
% \vspace{-27cm}
\footnotesize
  \centering
  \caption{Rank-1 accuracy (\%) on the VC-Clothes dataset under different OOD percentages. The standard deviation is reported in parentheses.}
%   \vspace{-0.5em}
% \renewcommand{\arraystretch}{0.5}
% \setlength{\tabcolsep}{5.7mm}{
\resizebox{0.48\textwidth}{!}{
    \begin{tabular}{c|c|c|c|c|c}
    \toprule
    \multirow{1}[2]{*}{Methods} & \multicolumn{5}{c}{Different OOD percentages}  \\
\cmidrule{2-6}          & \multicolumn{1}{c|}{15\%} & \multicolumn{1}{c|}{25\%} & \multicolumn{1}{c|}{35\%} & \multicolumn{1}{c|}{45\%} & \multicolumn{1}{c}{55\%}  \\
    \midrule
    \multirow{1}[1]{*}{CBN}  & 83.91  & 73.67  & 64.10  & 54.34  & 44.98   \\
    \midrule
    \multirow{1}[1]{*}{CBN + Verification}  & 89.68 & 90.71  & 91.72 & 92.90  & 93.95    \\
    \midrule
    \multirow{1}[1]{*}{CBN + Threshold}  & 87.96  & 80.93 & 75.74 & 72.00  & 69.17 \\
    \midrule
    \multirow{1}[1]{*}{CBN + Ours} & \textbf{97.11} & \textbf{97.15} & \textbf{97.29} & \textbf{97.58} & \textbf{97.70}  \\
    \bottomrule
    \end{tabular}%
  }
  \label{tab_person}%
%   \vspace{-1.5em}
\end{table}%

% % Table generated by Excel2LaTeX from sheet 'Sheet1'
% \begin{table*}[t]
% % \vspace{-27cm}
% \footnotesize
%   \centering
%   \caption{Rank-1 accuracy (\%) on the VC-Clothes dataset under different OOD percentages. The standard deviation is reported in parentheses.}
% %   \vspace{-0.5em}
% % \renewcommand{\arraystretch}{0.5}
% \setlength{\tabcolsep}{5.7mm}{
%     \begin{tabular}{c|c|c|c|c|c}
%     \toprule
%     \multirow{1}[2]{*}{Methods} & \multicolumn{5}{c}{Different OOD percentages}  \\
% \cmidrule{2-6}          & \multicolumn{1}{c|}{15\%} & \multicolumn{1}{c|}{25\%} & \multicolumn{1}{c|}{35\%} & \multicolumn{1}{c|}{45\%} & \multicolumn{1}{c}{55\%}  \\
%     \midrule
%     \multirow{1}[1]{*}{CBN}  & 83.91  & 73.67  & 64.10  & 54.34  & 44.98   \\
%     \midrule
%     \multirow{1}[1]{*}{CBN + Verification}  & 89.68 ($\pm$ 0.10) & 90.71 ($\pm$ 0.17) & 91.72 ($\pm$ 0.40) & 92.90 ($\pm$ 0.43) & 93.95 ($\pm$ 0.14)   \\
%     \midrule
%     \multirow{1}[1]{*}{CBN + Threshold}  & 87.96 ($\pm$ 0.09) & 80.93 ($\pm$ 0.22) & 75.74 ($\pm$ 0.29) & 72.00 ($\pm$ 0.56) & 69.17 ($\pm$ 0.82)    \\
%     \midrule
%     \multirow{1}[1]{*}{CBN + Ours} & \textbf{97.11 ($\pm$ 0.23)} & \textbf{97.15 ($\pm$ 0.37)} & \textbf{97.29 ($\pm$ 0.33)} & \textbf{97.58 ($\pm$ 0.38)} & \textbf{97.70 ($\pm$ 0.15)}  \\
%     \bottomrule
%     \end{tabular}%
%   }
%   \label{tab_person}%
% %   \vspace{-1.5em}
% \end{table*}%

\subsection{Ablation Study} \label{Ablation_Study} 
To verify the effectiveness of our proposed components, \ie, the uncertainty modeling and query construction, we conduct ablation studies on gait recognition. 

\noindent \textbf{Effectiveness of the uncertainty modeling:}
Since existing backbones cannot handle OOD cases, they cannot reach 100\% accuracy in all OOD settings.
For a fair comparison, we introduce different OOD detection strategies and join them with the gait backbone to construct baseline algorithms. As a result, the upper bound accuracy of the constructed baseline algorithms is 100\%. The details of baseline algorithms are as follows: 

\noindent \textbf{Backbone + Naive CLS:} An NN classifier is designed to classify whether two features are from the same identity or not. The classifier is trained by a binary cross-entropy loss and our constructed pairs. The difference between this classifier and our model is that our model has uncertainty modeling. Thus, our model not only classifies whether two features are from the same identity but also provides the uncertainty estimation of the prediction by a Dirichlet distribution parameterized over all the constructed training pairs, while a simple classifier only makes decisions based on its predicted labels since it only learns a classification boundary.
The experimental results are shown in Table \ref{tab:tab1}. As expected, without any mechanism to distinguish OOD queries, the performance degrades as the number of OOD samples increases. 
Although a simple classifier outperforms slightly our method in the 7\% OOD percentage, we can see that a simple classifier still struggles to recognize OOD queries. This is because a simple classifier tends to overfit the in-distribution pairs. Thus, it may produce inaccurate predictions, when the input pairs of features are out-of-distribution.


\noindent \textbf{Backbone + Verification:} Setting a similarity threshold is another straightforward way to address OOD cases. When the similarity between the probe and the closet sample from the gallery is smaller than the threshold, the probe will be regarded as an OOD query. In our cases, the threshold is calculated by Equal Error Rate (EER) from the verification setting. To be specific, we construct 20k gait pairs, including 10k paired samples and 10k unpaired samples. Then, we search for the best threshold leading to the lowest EER. Finally, the searched threshold is used to further determine whether two samples are from the same identity.
We will release the constructed gait pairs for reproducibility. 
Table \ref{tab:tab1} shows that the verification-based methods address a few OOD cases, leading to a slight performance improvement. For example, in the 15\% OOD setting, ``DyGait + Verification'' outperforms DyGait by 9.23\% on OUMVLP. However, the verification-based methods cannot handle most OOD cases. This is because most OOD queries are hard samples. Here, ``hard'' means the OOD query is very similar to the closet sample from the gallery set. In other words, its similarity is higher than the given threshold, leading to erroneous results.
% Moreover, we manually search all thresholds for feature matching. The experimental results can be found in the supplementary material.

\noindent \textbf{Backbone + Anomaly Detection:} One-Class Novelty Detection \cite{yang2021generalized} is proposed to address OOD cases in recognition. It aims to classify all OOD cases into one class. Although it cannot be used directly in our task, we combine it with an NN classifier to detect our OOD cases. To be specific, we randomly construct in-distribution and out-of-distribution feature pairs to train an NN classifier. The trained classifier is then employed in feature matching to determine whether the retrieved gait pairs are the same identity or not. The experimental results are shown in Table \ref{tab:tab1}. It can be observed that one-class novelty detection effectively addresses a few OOD cases. For example, ``DyGait +  Anomaly Detection'' outperforms DyGait by 2.66\% when the OOD query rate is 5\% on OUMVLP. However, in the 45\% and 55\% OOD settings, its performance decreases significantly. This is because one-class novelty detection tends to classify an in-distribution pair that has a large distance to OOD cases. As a result, the accuracy in low OOD percentage shows a significant decrease.
On the contrary, our method maintains the recognition performance even though the rate of OOD samples is more than 50\%. Our method successfully identifies OOD queries and assigns correct identities to in-distribution queries. We also conduct our experiments three times on different gallery sets. The small standard deviation values also prove the stability of our method.

\noindent \textbf{Impacts of different thresholds: }
For a fair comparison, we introduce different OOD detection strategies. One of our used strategies is to set a similarity threshold. If the similarity between the probe and the closet sample from the gallery is smaller than the threshold, the probe will be regarded as an OOD query. In this paper, we introduce a verification setting to automatically calculate the threshold. Obviously, the threshold can be set manually. Thus, we conduct experiments on OUMVLP by manually searching all thresholds. The experimental results are shown in Table \ref{tab_threshold}. When the given threshold is lower than 0.6, the threshold-based method cannot detect any OOD cases. Thus, the accuracy in these threshold settings is the same as the accuracy using only the backbone. We also observe that a low threshold for GaitGL achieves appealing performance when the OOD percentage is less than 25\%, but it cannot effectively address the condition with higher OOD percentages. In contrast, a high threshold achieves promising performance in high OOD percentages ($\geq$ 35\%). For the DyGait backbone, we find that only ``Threshold = 0.8'' achieves promising performance, while other threshold settings lead to significant performance degradation.. These results indicate that it is difficult to choose a suitable threshold for different OOD percentages. Thus, the threshold-based methods are impractical. Furthermore, it can be found that our method outperforms the threshold-based methods in all OOD percentages and the recognition accuracy in all OOD percentages is stable. We believe that the proposed method is a more practical approach to addressing OOD queries in a real scene.




\noindent \textbf{Impacts of different query pair construction:}
We propose an OOD query pair construction that allows our network to grasp the idea of in-distribution and OOD residual features. 
We also compare three different strategies: randomly selecting feature pairs, dataset-based hard negative and positive feature pairs, and batch-based hard negative and positive feature pairs. All experiments are conducted on OUMVLP and we take GaitGL as our backbone.
As indicated in Figure \ref{Fig_ablation_QC}, both random selection of feature pairs and batch-based hard negative and positive feature pair mining strategies improve the recognition performance on OOD cases in testing.
Note that, as the percentage of the OOD queries increases, the performance of the model trained with random selection of feature pairs decreases. This is mainly because this pair construction may not effectively capture the most uncertain cases for model training.
The dataset-based mining strategy does not facilitate the representation of the distribution of the residual features. 
In most cases, the negative feature pairs from the dataset-based mining strategy are hard to be classified and thus our classifier will exert more efforts to classify the negative pairs after training while sacrificing the classification accuracy on positive pairs. 
% This is the reason its performance increases when the percentage of OOD samples increases.
Mining in a batch-based pair will significantly improve the variety of training samples, thus leading to better recognition performance.
Thanks to our feature pair construction, we can effectively capture the latent distribution of residual features, thus achieving accurate detection of OOD samples.

\section{Discussion, Limitation and Extension} \label{Discussion} 
{In this section, we further analyze the advantages and limitations of our proposed uncertainty gait recognition framework. Additionally, we extend our method to other tasks, \emph{i.e.}, vehicle re-identification and person re-identification.}

\noindent \textbf{Performance of the default evaluation protocol:} {In this paper, we combine the backbone networks with different OOD detection strategies, including Naive CLS, Verification, and Anomaly Detection. Experimental results are reported in Table. \ref{tab:tab1}, Table. \ref{tab_casia} and Table. \ref{tab_gait3d}, respectively. Our method achieves a significant performance improvement in all OOD percentages, but it also suffers slight performance degradation in the in-distribution scenario. 
For example, in the default evaluation protocol on the OUMVLP dataset (with 7\% OOD queries), the accuracy of GaitGL with our methods (91.52\%) is lower than the accuracy of GaitGL with the Naive CLS (92.27\%). We speculate the main reason is that the feature space of the three backbones (GaitSet, GaitPart and GaitGL) among different subjects is not discriminative enough (without sufficient margins/distance) for some hard cases and thus those samples will be rejected by our method. Since our method prefers to accept matching pairs with minimal risk, these high-risk predictions are rejected and the overall accuracy drops in the in-distribution case.
}


\begin{figure}[t] 
\centering

\scalebox{0.95}[1]{\includegraphics[width=1\linewidth]{images/Fig_3_33_lineChart.pdf}} 
	% \vspace{-0.6em}
\caption{Impacts of different query pair construction on training our model. We test the models with different percentages of OOD queries. \label{Fig_ablation_QC}}
% \vspace{-1.2em}
\end{figure}




\noindent \textbf{Performance across different datasets:} {We conduct experiments on three popular used datasets, CASIA-B, OUMVLP and Gait3D, to verify the effectiveness of our uncertainty gait recognition framework. Table. \ref{tab:tab1}, Table. \ref{tab_casia} and Table. \ref{tab_gait3d} demonstrate state-of-the-art backbones with our methods achieve superior performance in all OOD percentages. }
{Additionally, for the default setting on the OUMVLP dataset, DyGait with our method achieves an accuracy of 99.00\%. When the OOD percentage increases to 55\%, our method still obtains 96.08\% Rank-1 accuracy. The performance degradation on the OUMVLP dataset is only 3\%. However, there is an obvious performance degradation on the CASIA-B dataset when the OOD percentage increases from 0\% to 55\%. We speculate that our method trained on a small dataset, like CASIA-B which only contains 74 subjects for training, may not learn sufficiently discriminative features and thus suffers a performance drop in high OOD percentage. 
}

{For the Gait3D dataset, it is observed that the performance degrades about 10\% as OOD percentage increases from 5\% to 55\%. The main reason is that existing methods cannot effectively extract discriminative features on Gait3D (the accuracy of these methods is lower than 50\%). When the extracted features are not discriminative in handling in-distribution queries, we do not expect these features to be robust enough to address OOD queries. }

% We also conduct experiments on Gait3D \cite{zheng2022gait} which is a large-scale and in-the-wild dataset. The experimental results can be found in the supplementary material. Although our method still achieves a significant performance improvement in all OOD percentages, it suffers performance degradation without OOD. The main reason is that existing SOTA methods cannot effectively extract discriminative features on Gait3D (the accuracy of SOTA methods is lower than 50\%). When the extracted features are not sufficiently discriminative, in handling in-distribution queries, our method tends to be inaccurate. This is because, in such cases, the distance between a matching pair is large, leading to a higher risk. Meanwhile our network prefers to accept matching pairs with minimal risk.
\noindent \textbf{Extension to other tasks:} To verify the applicability of our method to other tasks, we also conduct experiments on vehicle re-identification benchmark VERI-Wild \cite{lou2019large, lou2021large} and synthetic person-identification benchmark VC-Clothes \cite{wan2020person}. The experimental details and results can be found in Table. \ref{tab_vehicle} and Table. \ref{tab_person}. These experiments demonstrate our uncertainty-aware model is able to effectively recognize matching feature pairs and OOD queries through its prediction and uncertainty modeling, thus demonstrating the superiority of our method to the SOTA methods. 







\section{Conclusion}

In this paper, we propose a novel uncertainty-aware gait recognition model that can effectively identify whether a probe sample is out of the distribution of the gallery samples. 
To the best of our knowledge, our work is the first attempt to endow gait recognition with the capability to address OOD queries via uncertainty modeling.
Furthermore, our proposed method is a unified framework that is not only able to address OOD query samples but also to successfully recognize the in-distribution samples.
Since our method takes extracted gait features as input, it can be applied to various gait recognition networks, and thus agnostic against backbone networks.
More importantly, our model can be generalized to other recognition tasks, such as vehicle and person re-identification, and improves the robustness to OOD queries, which frequently occur in practice.







% \section{Appendix}

% In this section, we evaluate the effectiveness of our method on the real-world gait dataset {Gait3D} \cite{zheng2022gait}. Moreover, to verify the applicability of our method to other tasks, we also conduct experiments on other similar tasks, such as vehicle re-identification \cite{lou2019large, lou2021large} and synthetic person-identification benchmark VC-Clothes \cite{wan2020person}. The details of these datasets are depicted as follows.

% \subsection{Dataset}

% \noindent \textbf{Gait3D} \cite{zheng2022gait} is a large-scale and in-the-wild dataset. It includes 4K subjects and 25K sequences captured by 39 surveillance cameras in the wild. In our experiments, the Gait3D dataset is divided into three subsets: a training set, a query set, and a gallery set. To be specific, 3,000 IDs\footnote{Here, ID refers to identity. With a slight abuse of notation, we refer ID to as in-distribution for simplicity.} are used to construct the training set and the remaining 1,000 IDs are used as the query and gallery sets. We select one image from each ID as the gallery set and the rest images as the query set. By removing IDs from the gallery set, we construct various OOD query cases.

% \noindent \textbf{VERI-Wild} \cite{lou2019large, lou2021large} is a large-scale dataset used for vehicle re-identification tasks.
% It includes 416,314 images captured by 174 surveillance cameras in the wild. In general, the VERI-Wild dataset is divided into three subsets: a training set, a query set, and a gallery set. 
% We use the evaluation protocol as in \cite{he2020fastreid}, which includes three different settings, \emph{i.e.}, Small-size Setting (SS), Medium-size Setting (MS) and Large-size Setting (LS). 
% In the three settings, 3,000 IDs, 5,000 IDs and 10,000 IDs are used to construct the query and gallery sets, respectively. In each setting, we select one image from each ID as the gallery set and the rest images as the probe set. 
% By removing IDs from the gallery set or probe set, we construct various OOD query cases.


% \noindent \textbf{VC-Clothes}\cite{wan2020person} is a synthetic person re-identification dataset. It includes 512 subjects and each subject has 36 corresponding images. 
% The dataset has been divided into two subsets: a training set and a testing set. Each set contains 256 IDs. 
% In the inference stage, we randomly choose four images from each subject as the gallery set and the rest images are used as the probe set. Hence, the gallery and probe sets contain 1020 images and 8591 images, respectively.

% \subsection{Implementation Details}
% For the Gait3D dataset, we take GaitGL \cite{lin2021gait} as the backbones to extract gait features and evaluate the effectiveness of our proposed uncertainty-aware classification model. We set the hidden layer size of our MLP to 5000. 
% For the VERI-Wild dataset, BoT \cite{luo2019bag} is used as the backbone and thus the hidden layer size of MLP of the uncertainty-aware classification model is set to 1024. For the VC-clothes dataset, we take CBN \cite{zhuang2020rethinking} as the backbone and the hidden layer size of MLP is set to 4096.
% The hyper-parameters $P$ and $K$ of the proposed sampling strategy are set to 32 and 8 respectively. The learning rate is set to 1e-3 and then decays by a factor of 0.1 every 10 iterations. The total training iteration number for all the experiments is set to 50. 
% In our experiments, we also run each OOD setting by three times in order to demonstrate the robustness of our method against OOD queries.

% \subsection{Comparisons with the State-of-the-art}

% \noindent \textbf{Evaluation on Gait3D under different OOD percentages}
% For the Gait3D dataset, we choose GaitGL \cite{lin2021gait} as our baseline backbone. Table \ref{tab_gait3d} shows the experimental results. It can be observed that our method achieves a significant performance improvement in all cases. For instance, in the 55\% OOD percentage, our method outperforms GaitGL by 37.01\%. Although our method still achieves a significant performance improvement in all OOD percentages, it suffers performance degradation without OOD. The main reason is that existing SOTA methods cannot effectively extract discriminative features on Gait3D (the accuracy of SOTA methods is lower than 50\%). When the extracted features are not sufficiently discriminative, in handling in-distribution queries, our method tends to be inaccurate. This is because, in such cases, the distance between a matching pair is large, leading to a higher risk. Meanwhile our network prefers to accept matching pairs with minimal risk.

% % \vspace{1mm}
% \noindent \textbf{Evaluation on VERI-Wild and VC-Clothes under different OOD percentages}
% Although our model is designed to tackle OOD queries in gait recognition, it can be easily extended to other tasks, such as vehicle re-identification and person re-identification. 
% To further verify the effectiveness of our method on other tasks, we conduct experiments on VERI-Wild and VC-Clothes.

% For the VERI-Wild dataset, we choose the BoT framework \cite{luo2019bag} as our baseline algorithm.
% The experimental results are shown in Table \ref{tab_vehicle}. It can be observed that our method (BoT + Ours) achieves appealing performance in all cases. To be specific, our method achieves stable recognition accuracy for the five OOD percentages while outperforming the baseline method. In contrast, the accuracy of BoT without our uncertainty model decreases significantly as the percentage of OOD samples increases.
% For the VC-Clothes dataset, we employ the CBN framework \cite{zhuang2020rethinking} as our baseline algorithm. As indicated in Table \ref{tab_person}, our methods achieve approximately more than 97\% recognition accuracy in all the cases, \ie, OOD percentage ranges from 15\% to 55\%. On the contrary, the recognition accuracy of CBN without our uncertainty model is 44.98\% in the setting of 55\% OOD samples. 
% This is because the current methods cannot address OOD queries and thus their methods only output erroneous recognition results. Note that, when we apply our method to the baseline methods, we do not need to re-train the baselines but treat them as off-the-shelf backbone networks. This also indicates that our uncertainty-aware model is able to effectively recognize matching feature pairs and OOD queries through its prediction and uncertainty modeling, thus demonstrating the superiority of our method to competing methods.

% \subsection{Ablation Study} \label{Ablation_Study} 

% For a fair comparison, we introduce different OOD detection strategies. One of our used strategies is to set a similarity threshold. If the similarity between the probe and the closet sample from the gallery is smaller than the threshold, the probe will be regarded as an OOD query. In this paper, we introduce a verification setting to automatically calculate the threshold. Obviously, the threshold can be set manually. Thus, we conduct experiments on OUMVLP by manually searching all thresholds. The experimental results are shown in Table \ref{tab_threshold}. When the given threshold is lower than 0.5, the threshold-based method cannot detect any OOD cases. Thus, the accuracy in these threshold settings is the same as the accuracy using only the backbone. We also observe that a small threshold achieves appealing performance when the OOD percentage is less than 25\%, but it cannot address the condition with higher OOD percentages. In contrast, a high threshold achieves promising performance in high OOD percentages ($\geq$ 35\%). These results indicate that it is difficult to choose a suitable threshold for different OOD percentages. Thus, the threshold-based methods are impractical. Furthermore, it can be found that our method outperforms the threshold-based methods in all OOD percentages and the recognition accuracy in all OOD percentages is stable. We believe that the proposed method is a more practical approach to addressing OOD queries in a real scene.

% if have a single appendix:
%\appendix[Proof of the Zonklar Equations]
% or
%\appendix  % for no appendix heading
% do not use \section anymore after \appendix, only \section*
% is possibly needed

% use appendices with more than one appendix
% then use \section to start each appendix
% you must declare a \section before using any
% \subsection or using \label (\appendices by itself
% starts a section numbered zero.)
%


% \appendices
% \section{Proof of the First Zonklar Equation}
% Appendix one text goes here.

% % you can choose not to have a title for an appendix
% % if you want by leaving the argument blank
% \section{}
% Appendix two text goes here.


% use section* for acknowledgment
\ifCLASSOPTIONcompsoc
  % The Computer Society usually uses the plural form
  \section*{Acknowledgments}
\else
  % regular IEEE prefers the singular form
  \section*{Acknowledgment}
\fi
% The authors would like to thank the anonymous reviewers and the associate editor for their helpful suggestions and valuable comments.
% This work was supported by the National Natural Science Foundation of China (61976017 and 61601021), the Beijing Natural Science Foundation (4202056), the Fundamental Research Funds for the Central Universities (2022JBMC013) and the Australian Research Council (DP220100800, DE230100477). The support and resources from the Center for High Performance Computing at Beijing Jiaotong University (http://hpc.bjtu.edu.cn) are gratefully acknowledged. 

This work was supported by the ARC-Discovery grant (DP220100800) and ARC-DECRA grant (DE230100477), the National Natural Science Foundation of China (61976017 and 61601021), the Beijing Natural Science Foundation (4202056), the Fundamental Research Funds for the Central Universities (2022JBMC013). The support and resources from the Center for High Performance Computing at Beijing Jiaotong University (http://hpc.bjtu.edu.cn) are gratefully acknowledged.



% Can use something like this to put references on a page
% by themselves when using endfloat and the captionsoff option.
\ifCLASSOPTIONcaptionsoff
  \newpage
\fi



% trigger a \newpage just before the given reference
% number - used to balance the columns on the last page
% adjust value as needed - may need to be readjusted if
% the document is modified later
%\IEEEtriggeratref{8}
% The "triggered" command can be changed if desired:
%\IEEEtriggercmd{\enlargethispage{-5in}}

% references section

% can use a bibliography generated by BibTeX as a .bbl file
% BibTeX documentation can be easily obtained at:
% http://mirror.ctan.org/biblio/bibtex/contrib/doc/
% The IEEEtran BibTeX style support page is at:
% http://www.michaelshell.org/tex/ieeetran/bibtex/
% \bibliographystyle{IEEEtran}
% argument is your BibTeX string definitions and bibliography database(s)
\bibliographystyle{IEEEtran}
% \bibliography{IEEEabrv,../bib/paper}
\bibliography{egbib}

% \vspace{-1.0cm}

% \begin{IEEEbiography}[{\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{Fig/Lin.jpg}}]{Beibei Lin} received the B.E. and M.S. degrees from Beijing Union University in 2018 and Beijing Jiaotong University in 2021, respectively. His research interests include computer vision, pattern recognition, and image and video processing. 
% \end{IEEEbiography}

% \vspace{-1.0cm}
% \begin{IEEEbiography}[{\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{Fig/zhang.jpg}}]{Shunli Zhang} received the B.S. and M.S. degrees in electronics and information engineering from Shandong University, Jinan, China, in 2008 and 2011, respectively, and the Ph.D. degree in signal and information processing from Tsinghua University in 2016. He is currently an Associate Professor with the School of Software Engineering, Beijing Jiaotong University. His research interests include pattern recognition, computer vision, and image processing. 
% \end{IEEEbiography}

% \vspace{-1.0cm}
% \begin{IEEEbiography}[{\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{Fig/Wang.jpg}}]{Ming Wang} received the B.E. degree from Qingdao University in 2021. She is currently a graduate student in Beijing Jiaotong University. Her research interests include computer vision and pattern recognition.
% \end{IEEEbiography}

% \vspace{-1.0cm}
% \begin{IEEEbiography}[{\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{Fig/Li.jpg}}]{Lincheng Li} received the B.S. and Ph.D. degree in electronic engineering from Tsinghua University, Beijing, China, in 2011 and 2017 respectively. He is currently a researcher in Netease Fuxi AI Lab, Hangzhou, China. His research interests include computer vision, pattern recognition, and image and video processing.
% \end{IEEEbiography}

% % \vspace{-3.0cm}


% \vspace{-1.0cm}
% \begin{IEEEbiography}[{\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{Fig/Yu.jpg}}]{Xin Yu} received the B.S. degree in electronic engineering from the University of Electronic Science and Technology of China, Chengdu, China, in 2009, the Ph.D. degree from the Department of Electronic Engineering, Tsinghua University, Beijing, China, in 2015, and the Ph.D. degree from the College of Engineering and Computer Science, Australian National University, Canberra, Australia, in 2019. He is currently a Senior Lecturer at the University of Queensland. His research interests include computer vision and image processing. 
% \end{IEEEbiography}



%
% <OR> manually copy in the resultant .bbl file
% set second argument of \begin to the number of references
% (used to reserve space for the reference number labels box)
% \begin{thebibliography}{1}

% \bibitem{IEEEhowto:kopka}
% H.~Kopka and P.~W. Daly, \emph{A Guide to \LaTeX}, 3rd~ed.\hskip 1em plus
%   0.5em minus 0.4em\relax Harlow, England: Addison-Wesley, 1999.

% \end{thebibliography}

% biography section
% 
% If you have an EPS/PDF photo (graphicx package needed) extra braces are
% needed around the contents of the optional argument to biography to prevent
% the LaTeX parser from getting confused when it sees the complicated
% \includegraphics command within an optional argument. (You could create
% your own custom macro containing the \includegraphics command to make things
% simpler here.)
% \begin{IEEEbiography}[{\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{mshell}}]{Michael Shell}
% or if you just want to reserve a space for a photo:

% \begin{IEEEbiography}{Michael Shell}
% Biography text here.
% \end{IEEEbiography}

% % if you will not have a photo at all:
% \begin{IEEEbiographynophoto}{John Doe}
% Biography text here.
% \end{IEEEbiographynophoto}

% % insert where needed to balance the two columns on the last page with
% % biographies
% %\newpage

% \begin{IEEEbiographynophoto}{Jane Doe}
% Biography text here.
% \end{IEEEbiographynophoto}

% You can push biographies down or up by placing
% a \vfill before or after them. The appropriate
% use of \vfill depends on what kind of text is
% on the last page and whether or not the columns
% are being equalized.

%\vfill

% Can be used to pull up biographies so that the bottom of the last one
% is flush with the other column.
%\enlargethispage{-5in}



% that's all folks
\end{document}


