\section{Method}
\label{sec:method}


Supervised contrastive learning (i.e., SupCon~\cite{khosla2020supervised}) leverages discrete labels (i.e., categories) to define positive and negative samples. Starting from a sample $x_i$, called the \emph{anchor}, and its latent representation $z_i = f(x_i)$, SupCon aligns the representations of all positive samples (i.e. sharing the same class as $x_i$) to $z_i$, while repelling the representations of the negative ones (i.e., different class). The SupCon loss is thus not adapted for regression (i.e., continuous labels), as it is not possible to determine a hard boundary between positive and negative samples. 
All samples are somehow positive and negative at the same time. Given the continuous label $y_i$ for the anchor and $y_k$ for a sample $k$, one could threshold the distance $d$ between $y_i$ and $y_k$ at a certain value $\tau$ in order to create positive and negative samples (i.e., k is positive if $d(y_i, y_k)<\tau$). The problem would then be how to choose $\tau$. Differently, we propose to define a degree of ``positiveness'' between samples using a kernel function $w_k = K(y_i - y_k)$, where $0 \leq w_k \leq 1$. 
Our goal is thus to learn a parametric function $f: \mathcal{X} \rightarrow \mathbb{S}^d$ that maps samples with a high degree of positiveness ($w_k \sim 1$) close in the latent space and samples with a low degree ($w_k \sim 0$) far away from each other. 

\subsection{Contrastive Learning Regression Loss}
\label{sec:method-weakly-contrastive}
In contrastive learning, we look for a parametric mapping function $f$ such that the following condition is always satisfied: $s^-_t - s^+_k \leq 0\quad \forall t,k $, where we denote as $s_k =$ sim($ f(x_i), f(x_k))$ the similarity between the representations of the $k$-th sample and the anchor $i$ (e.g., cosine similarity) and $x^-_t$ and $x^+_k$ are the negative and positive samples respectively. The notion of ``negative'' (dissimilar from the anchor) and ``positive'' (similar to the anchor) samples is thus rooted in the contrastive learning framework. To adapt such a framework to continuous labels, we propose to use a kernel function $w_k$, and we develop multiple formulations, illustrated in Fig.~\ref{fig:viz-losses}. To derive our proposed loss, we employ a metric learning approach, as in ~\cite{barbano2023unbiased}, which allows us to easily add conditioning and regularisation. \\
A first approach would be to consider as ``positive'' only the samples that have a degree of positiveness greater than 0, and align them with a strength proportional to the degree, namely:
\begin{equation}
        \frac{w_k}{\sum_j w_j}(s_t - s_k) \leq 0 \quad  %
        \forall j,k,t \neq k \in A(i) 
\label{eq:yaware-condition}
\end{equation}
where we have normalized the kernel so that the sum over all samples is equal to 1 and we denote with $A(i)$ the indices of samples in the minibatch distinct from $x_i$. Eq.\ref{eq:yaware-condition} can be transformed in an optimization problem using, as it is common in contrastive learning, the $\max$ operator and its smooth approximation \textit{LogSumExp}:
\begin{equation}
    \begin{gathered}
        \argmin_f \sum_k \max(0, \frac{w_k}{\sum_t w_t}\{ s_t - s_k \}_{\substack{t=1,...,N \\ t \neq k}}) =\\ 
        \argmin_f \sum_k \frac{w_k}{\sum_t w_t} \max(0, \{ s_t - s_k \}_{\substack{t=1,...,N \\ t \neq k}}) \\ 
        \approx  \mathcal{L}^{y-aware}= - \sum_k \frac{w_k}{\sum_t w_t} \log \left( \frac{\exp(s_k)}{\sum_{t=1}^N \exp(s_t)}  \right)
    \end{gathered}
\end{equation}
Interestingly, this is exactly the \emph{y-aware} loss proposed in \cite{dufumier2021contrastive} for classification with weak continuous attributes. 
Due to the non-hard boundary between positive and negative samples, both $s_t$ and $s_k$ are defined over the entire minibatch. The kernel $w_k$ is used to avoid aligning samples not similar to the anchor (i.e. $w_k\approx 0$). 
It can be noted that, while the numerator aligns $x_k$, in the denominator, the uniformity term (as defined in \cite{wang_understanding_2020}) focuses more on the closest samples in the representation space: this could be undesirable, as these samples might have a greater degree of positiveness  than the considered $x_k$ (Fig.~\ref{fig:viz-yaware}). %

\noindent To avoid that, we formulate a first extension ($\mathcal{L}^{thr}$) of~\eqref{eq:yaware-condition}, which limits the uniformity term (i.e., denominator) to the samples that are at least more distant from the anchor than the considered $x_k$ in the kernel space (omitting the normalization in the starting condition):
\begin{equation}
\begin{gathered}
    w_k(s_t - s_k) \leq 0 \quad \text{if } w_t - w_k \leq 0 \quad \forall k,t \neq k \in A(i) \\
\mathcal{L}^{thr} = -\sum_k \frac{w_k}{\sum_t \delta_{w_t < w_k} w_t} \log \left( \frac{\exp(s_k)}{\sum_{t\neq k} \delta_{w_t < w_k} \exp(s_t)}\right)
\end{gathered}
\end{equation}
Ideally, $\mathcal{L}^{thr}$ avoids repelling samples more similar than $x_k$. However, it still focuses more on the closest sample ``less positive'' than $x_k$, i.e. $x_t$ s.t $w_t > w_x$ and $w_t \leq w_j\,\, \forall j\neq k$ (Fig.~\ref{fig:viz-threshold}). As noted in~\cite{barbano2023unbiased,khosla2020supervised}, increasing the margin with respect to the closest ``negative'' sample works well for classification, however, it might not be best suited for regression. \\
For this reason, we propose a second formulation ($\mathcal{L}^{exp}$) that takes an opposite approach. 
Instead of focusing on repelling the closest ``less positive'' sample, we increase the repulsion strength for samples proportionally to their distance from the anchor in the kernel space:
\begin{equation}
\begin{gathered}
    w_k[s_t(1-w_t) - s_k] \leq 0 \quad \forall k,t \neq k\in A(i) \\
    \mathcal{L}^{exp} = -\frac{1}{\sum_t w_t}\sum_{k}w_k \log \frac{\exp(s_k)}{\sum_{t \neq k} \exp(s_t(1-w_t))}
\end{gathered}
\end{equation}
In the resulting $\L^{exp}$ formulation, the weighting factor $1 - w_t$ acts like a temperature value, by giving more weight to the samples which are farther away from the anchor in the kernel space (Fig.~\ref{fig:viz-expw}). Also, for a proper kernel choice, samples closer than $x_k$ will be repelled with very low strength ($\sim$0). We argue that this approach is more suited for continuous attributes (i.e., regression task), as it enforces that samples close in the kernel space will be close in the representation space.


