\section{Related works}
\label{sec:related}

%
Our contribution is based on the related literature in contrastive learning, metric learning, fairness, and debiasing. 
Contrastive methods learn a representation space of input data by attracting semantically similar \textit{positive} samples (e.g. with the same labels~\cite{khosla2020supervised}) and by repelling dissimilar \textit{negative} samples (e.g. with distinct labels). When label information is not available, instance-based discrimination approaches~\cite{chen2020simCLR, chen2020mocov2, zbontar_barlow_2021, dwibedi2021little, caron2020unsupervised, grill2020bootstrap} have gained considerable success, almost closing the gap with its supervised counterpart. Most recent research in CL has focused on designing the right data augmentations~\cite{chen2020simCLR,zhang2022rethinking}, contrastive losses~\cite{chen2020simCLR, grill2020bootstrap, zbontar_barlow_2021}, memory banks~\cite{chen2020mocov2, grill2020bootstrap, dwibedi2021little}, prototypes (or clusters)~\cite{caron2020unsupervised, li2020prototypical} typically for downstream  classification, detection or segmentation tasks-- leaving open the question for regression tasks. When weak labels are available (either discrete or continuous), kernel approaches have been explored for CL~\cite{dufumier2021contrastive, dufumier2022rethinking}, using a re-weighting scheme to adapt alignment term between samples with close weak labels. 
