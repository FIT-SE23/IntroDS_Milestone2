\section{Introduction and Motivation}

With the rise of digital learning or education platforms, the frequency of teachers posing tasks and questions digitally has increased substantially.
However, due to temporal constraints, it would be infeasible for teachers to constantly create novel and individual questions tailored to each different student.
With the aid of Artificial Intelligence (AI), they can submit AI-generated learning tests more frequently, which can lead to student performance improvement.
Moreover, many teachers develop exam questions without exchanging ideas or material with their peers. In many cases, this may unnecessarily cost them a lot of time and effort. Instead, they should be able to focus on explaining the concepts to their students.
To address these shortcomings, we propose using Wikidata as a multilingual framework that allows for collaborative worldwide teacher knowledge engineering and subsequent AI-aided question generation, test, and correction.
Using Wikidata in education leads to the research problem need to compare and identify the best-performing methods to generate questions from Wikidata knowledge.

As a proof of concept
%(POC)
for the physics domain, we develop and evaluate a >>PhysWikiQuiz<< question generator and solution test engine (example in Figure \ref{fig:ExampleSpeed}), hosted by Wikimedia at \url{https://physwikiquiz.wmflabs.org} with a demovideo available at \url{https://purl.org/physwikiquiz}.
The system addresses the teacher's demand by automatically generating an unlimited number of different questions and values for each student separately. It employs the open access semantic knowledge-base Wikidata\footnote{\url{https://www.wikidata.org}}
%(see Section \ref{subsec:Wikidata})
to retrieve Wikimedia community-curated physics formulae with identifier (variables with no fixed value\footnote{\url{https://www.w3.org/TR/MathML3/chapter4.html\#contm.ci}}) properties and units using their concept name as input. A given formula is then rearranged, i.e., solved for each occurring identifier by a Computer Algebra System
%(CAS)
to create more question sets. For each rearrangement, random identifier values are generated.
%(in a specified range)
Finally, the system compares the student's answer input to a CAS computed solution for both value and unit separately.
%
\begin{figure}[t]
    \centering
    \includegraphics[width=0.98\textwidth]{image.png}
    \caption{Example question generation (including variable names, symbols, and units) and answer correction (of both solution value and unit) for the formula concept name `speed'. The PhysWikiQuiz system also generates an explanation text with information reference and calculation path.}
    \label{fig:ExampleSpeed}
\end{figure}
%
%\begin{figure}[]
%    \centering
%    \includegraphics[width=\textwidth]{images/PhysWikiQuiz-Screenshot_example.png}
%    \caption{Example question generation (including name, symbol and unit), answer correction (of both value and unit) and solution explanation (source and calculation) for the formula concept name `speed'.}
%    \label{fig:ExampleSpeed}
%\end{figure}
%
PhysWikiQuiz also provides an API for integration in external education systems or platforms.

To evaluate the system, we pose the following research questions for the assessment of test question generation from Wikidata knowledge (RQs):

\begin{enumerate}
%    \item How can we build a scalable framework for collaborative AI-aided test question generation?
    \item What are the state-of-the-art systems? How to address their shortcomings?
    \item Which information retrieval methods and databases can we employ?
    \item What performance can we achieve?
    \item What are the contributions of the system's modules to this performance?
    \item What challenges occur during implementation and operation?
    \item How can we address these challenges?
\end{enumerate}

\textbf{Structure.} The remainder of this paper is structured as follows. We discuss RQ 1 in Section \ref{sec:Background}, RQs 2-4 in Section \ref{sec:Methods} and \ref{sec:Evaluation}, and RQs 5-6 in Section \ref{sec:Discussion}.

\section{Background and Related Work}\label{sec:Background}

%Including reviewer's suggestions

In this section, we review the prerequisite background knowledge
%from several research topics
the project builds upon, including the research gap and the employed methods.
%PhysWikiQuiz 1) generates questions as part of an education platform, 2) employs Wikidata as knowledge-base to, 3) use mathematical entity linking for formula concepts.

%The prerequisites for the PhysWikiQuiz system are that it:
%PhysWikiQuiz 1) is intended to generate questions as part of an education platform, 2) employs Wikidata as knowledge-base, 3) works on formula concepts, 4) requires formula and identifier unit retrieval, and 5) utilizes a Computer Algebra System to correct the student's answer.

%In the following, we shortly discuss the state-of-the-art in each of the prerequisite topics to position our contribution.

%\subsection{Education Platforms and Test Question Generation}

%\paragraph{Education Platforms}

%TODO (maybe): shorten here

%\textbf{E-learning Platforms} can help to make the learning process more individual, transparent, interactive, and efficient. In a Learning Content Management System (LCMS), a teacher can curate learning material to selectively release and distribute to students in a Virtual Learning Environment (VLE) digital interface~\cite{martin2009learning}.
%Wikis are often employed as a collaborative online didactic tool that enables students to participate in the knowledge management process~\cite{DBLP:journals/ce/BiasuttiE12}. Moreover, students use Wikipedia to get a quick overview of a given topic and search for further resources.
%In physics, the
%Learning Management System (LMS)
%LCMS `Moodle' is often employed as a teaching tool to organize, manage and deliver course materials. The platform provides several multimedia tools to set up engaging activities to make the learning process more fun for the students~\cite{DBLP:journals/ce/Martin-BlasS09}.
%Furthermore, ontologies, such as `OntoMathEdu' are developed as Linked Open Data hub to hierarchically organize mathematical and linguistic knowledge concept entities and their relationships~\cite{DBLP:conf/mkm/KirillovichNFLS20}.

%\paragraph{Question Generation}

\textbf{Question Generation (QG)} is a natural language processing task to generate question-answer (QA) pairs from data (text, knowledge triples, tables, images, and more)\footnote{https://www.microsoft.com/en-us/research/project/question-generation-qg}. The generated QA pairs can then be employed in dialogue systems, such as Question Answering, chatbots, or quizzes.
State of the art is to typically use neural networks to generate structured pairs out of unstructured content extracted from crawled web pages~\cite{DBLP:conf/emnlp/DuanTCZ17}.
There is a number of datasets
%(SQuAD, WikiQA, TriviaQA, and more)
and models
%(e.g., Info-HCVAE and MDN)
openly available with a competitive comparison at \url{https://paperswithcode.com/task/question-generation}.
%%TODO (maybe): cite individual datasets (SQuAD, WikiQA, TriviaQA, and more) and models (Info-HCVAE and MDN)
In the last decade, QA has been increasingly employed and researched for educational applications~\cite{DBLP:conf/icwsm/ChenYHH18,DBLP:conf/acl/SrivastavaG20}. Despite the large variety of techniques, in 2014 only a few had been successfully deployed in real classroom settings~\cite{DBLP:conf/iccsama/LeKP14}.

%\paragraph{Test Generation}

\textbf{Automated Test Generation (ATG)} for intelligent tutoring systems has so far been tackled using linked open data ontologies to create natural language multiple-choice questions~\cite{DBLP:journals/semweb/VK17}.
%Automated generation of assessment tests from domain ontologies
%http://www.semantic-web-journal.net/system/files/swj1312.pdf
%
The evaluation is typically domain-dependent. For example, Jouault et al. conduct a human expert evaluation in the history domain, comparing automatically with manually generated questions to find about 80\% coverage~\cite{jouault2016content}.
%Content-dependent question generation using LOD from history learning in open learning space
%https://www.jstage.jst.go.jp/article/tjsai/advpub/0/advpub_LOD-F/_pdf
%
Some approaches use Wikipedia-based datasets consisting of URLs of Wikipedia articles to generate solution distractors via text similarity~\cite{shah2017automatic}.
%Automatic question generation for intelligent tutoring systems
%https://ieeexplore.ieee.org/document/8066538
%though we have developed our system for the field of physics, it can be extended to any field
%
Since Wikipedia is only semi-structured, it may be more efficient to instead employ highly structured databases. This was attempted by `Clover Quiz', a trivia game powered by DBpedia for multiple-choice questions in English and Spanish.
However, the creators observed the system to have high latency, which is intolerable for a live game. The limitations are addressed by creating questions offline through a data extraction pipeline~\cite{DBLP:journals/semweb/Vega-Gorgojo19}.
%Clover Quiz: a trivia game powered by DBpedia
%http://semantic-web-journal.net/system/files/swj1826.pdf
%
For the mathematics domain, Wolfram Research released the Wolfram Problem Generator\footnote{\url{https://www.wolframalpha.com/problem-generator}} for AI-generated practice problems and answers. The system covers arithmetics, number theory, algebra, calculus, linear algebra, and statistics, yet is restricted to core mathematics while physics is currently not supported.
%
Current systems for the physics domain, e.g., `Mr Watts Physics'~\footnote{\url{http://wattsphysics.com/questionGen.html}} and `physQuiz'~\footnote{\url{https://physics.mrkhairi.com}}, are curated only by single maintainers, which leads to a very limited availability of concepts and questions (see Table \ref{tab:ComparisonCompetitors}).

%TODO: wo sind wir anders / besser ?
%http://wattsphysics.com/questions/GCSE/speed.php?numQuestions=20 is limited to 20 questions and 36 concepts
%https://physics.mrkhairi.com is also limited to 20 questions and only 8 concepts
%PhysWikiQuiz: unlimited questions for 469 concepts
%-> außerdem nutzen sie kein (Math) Entity Linking! -> described in the following

% only 8 concepts available
% curated only by one person (physics teacher\footnote{\url{https://twitter.com/MrKhairi_}}
%-> physwikiquiz 469 (24/01/22) %https://w.wiki/3zt9
% collaboration world wiki wide (teachers and non-teaching physicists)

\begin{table}[b]
\resizebox{\columnwidth}{!}{
\begin{tabular}{llll}
\hline
\textbf{System}       & \textbf{Mr Watts Physics} & \textbf{physQuiz Equations} & \textbf{PhysWikiQuiz} \\ \hline
Concepts              & 36                        & 8                           & 469 (Wikidata)        \\ %\hline
Questions per concept & 20                        & 20                          & unlimited             \\ \hline
\end{tabular}}
\caption{Comparison of PhysWikiQuiz scope to competitors.}\label{tab:ComparisonCompetitors}
\end{table}

\textbf{We address the reported shortcomings} by presenting a system for the physics domain that allows for unlimited live question generation from community-curated (Wikidata) knowledge.
%
Since Wikidata
%(discussed in the next Subsection \ref{subsec:Wikidata})
is constantly growing, our approach scales better than the aforementioned %currently available
static resources curated by single teachers.
%Out of a total of 475 unique concepts, 8/475 = 2\% were available on `physQuiz Equations', 36/475 = 8\% on `Mr Watts Physics', and 469/475 = 99\% on our `PhysWikiQuiz'.
Only 2\% of unique concepts were available on `physQuiz Equations' (8 out of 475) and 8\% on `Mr Watts Physics' (36 out of 475), yet 99\% on our `PhysWikiQuiz' (469 out of 475).
%
In the case of mathematical knowledge, Wikidata currently contains around 5,000 statements that link an item concept name to a formula~\cite{DBLP:conf/semweb/ScharpfSG21}. As stated above, almost 500 of them are from the physics domain.
%\footnote{\url{https://w.wiki/z8p}}.
%
PhysWikiQuiz exploits this information to create, pose, and correct physics questions using mathematical entity linking~\cite{DBLP:conf/www/ScharpfSG21} (in contrast to the competitors), which we review in the following.

%\subsection{Wikidata}
%\label{subsec:Wikidata}

%In Wikipedia, each language has its own article for a specific topic. To connect unique concepts in language-independent items, Wikimedia launched Wikidata\footnote{\url{https://www.wikidata.org}} in 2012~\cite{DBLP:journals/cacm/VrandecicK14}. Wikidata is a free, open access, collaborative semantic knowledge-base. Each Wikidata item (with query ID, short QID) has a concept label,
%for example `mass-energy equivalence'
%%, for example, `Eiffel tower' (Q243)\footnote{\url{https://www.wikidata.org/wiki/Q243}}. 
%and usually contains a description
%(in this case `physical law relating mass to energy')
%%(in this case, `tower located on the Champ de Mars in Paris, France')
%and aliases in multiple languages
%(e.g., `Équivalence masse-énergie' in French).
%%(e.g., `tour Eiffel' in French).
%More importantly, it is made of statements with factual information that is framed as claims with corresponding references~\cite{DBLP:conf/www/ScharpfSG21}. The claims link items using properties
%(e.g., `defining formula' (P2534): \verb|E = m c^2|)
%%(e.g., `named after' (P138) `Gustave Eiffel' (Q20882)) 
%or assign property numbers as attribute values.
%%(e.g., height (P2048) `300 meter').
%Humans or bots can create, read, accept, decline or edit item content~\cite{DBLP:journals/corr/abs-1907-01642}.
%%TODO (essential): the paragraphs below are redundant if the paper gets extended
%In the case of mathematical knowledge, Wikidata currently contains around 5,000 statements that link an item concept name to a mathematical formula\footnote{\url{https://w.wiki/z8p}}.
%TODO  (essential): update number
%Our PhysWikiQuiz system exploits this information to create, ask, and correct physics questions using mathematical entity linking.

%Architecture for a multilingual Wikipedia
%https://arxiv.org/abs/2004.04733

%\subsection{Formula Concepts}
%\label{subsec:FormulaConcepts}

\textbf{Mathematical Entity Linking (MathEL)} is the task of linking mathematical formulae or identifiers to unique web resources (URLs), e.g., Wikipedia articles. This requires formula concepts to be identified (first defined and later recognized). For this goal, a `Formula Concept' was defined~\cite{DBLP:conf/sigir/ScharpfSG18,DBLP:conf/sigir/ScharpfSCG19} as a `labeled collection of mathematical formulae that are equivalent but have different representations through notation, e.g., the use of different identifier symbols or commutations'~\cite{DBLP:conf/www/ScharpfSG21}. Formulae appearing in different representations make it difficult for humans and machines to recognize them as instances of the same semantic concept. For example, the formula concept `mass-energy equivalence' can either be written as $E=mc^2$ or $\mu=\epsilon/c^2$ or using a variety of other symbols. To facilitate and accelerate the creation of a large dataset~\cite{DBLP:conf/jcdl/SchubotzGSMCG18} for the training of Formula Concept Retrieval (FCR) methods, a formula and identifier annotation recommender system for Wikipedia articles was developed~\cite{DBLP:conf/recsys/ScharpfMSBBG19}. The FCR approaches are intended to improve the performance of Mathematical Information Retrieval (MathIR) methods, such as Mathematical Question Answering (MathQA)~\cite{mathqajournal,DBLP:conf/clef/ScharpfSGOTG20}, Plagiarism Detection (PD), STEM literature recommendation or classification~\cite{DBLP:conf/jcdl/ScharpfSYHMG20,DBLP:conf/mkm/SchubotzSTKBG20}.

%\subsection{Math Problem Solving}

%Microsoft Math Solver - Math Problem Solver & Calculator
%\url{https://math.microsoft.com}

%Are NLP Models really able to Solve Simple Math Word Problems?
%DBLP:conf/naacl/PatelBG21
%https://arxiv.org/pdf/2103.07191.pdf
%The problem of designing NLP solvers for math word problems (MWP) has seen sustained research activity and steady gains in test accuracy. Since existing solvers achieve high performance on the benchmark datasets for elementary level MWPs containing one-unknown arithmetic word problems, such problems are often considered “solved” with the bulk of research attention moving to more complex MWPs. In this paper, we restrict our attention to English MWPs taught in grades four and lower. We provide strong evidence that the existing MWP solvers rely on shallow heuristics to achieve high performance on the benchmark datasets. To this end, we show that MWP solvers that do not have access to the question asked in the MWP can still solve a large fraction of MWPs. Similarly, models that treat MWPs as bag-of-words can also achieve surprisingly high accuracy. Further, we introduce a challenge dataset, SVAMP, created by applying carefully chosen variations over examples sampled from existing datasets. The best accuracy achieved by state-of-the-art models is substantially lower on SVAMP, thus showing that much remains to be done even for the simplest of the MWPs.

%The benefits of computer-generated feedback for mathematics problem solving
%fyfe2016benefits
%The goal of the current research was to better understand when and why feedback has positive effects on learning and to identify features of feedback that may improve its efficacy. In a randomized experiment, second-grade children received instruction on a correct problem-solving strategy and then solved a set of relevant problems. Children were assigned to receive no feedback, immediate feedback, or summative feedback from the computer. On a posttest the following day, feedback resulted in higher scores relative to no feedback for children who started with low prior knowledge. Immediate feedback was particularly effective, facilitating mastery of the material for children with both low and high prior knowledge. Results suggest that minimal computer-generated feedback can be a powerful form of guidance during problem solving.

%First Evaluation of the Physics Instantiation of a Problem-Solving-Based Online Learning Platform
%DBLP:conf/aied/KumarCMR15
%Problem solving is a commonly used learning activity around which a large number of state-of-the-art Intelligent Tutoring Systems are developed and evaluated. In this paper, we present our problem-solving-based online learning platform and discuss a preliminary laboratory trial of this platform. While the platform itself is domain independent, for this evaluation, it was instantiated with a collection of problems from the unit of Electricity and Magnetism taught in high-school-level physics. Results indicate pedagogical effectiveness of problem solving in the Physics instance of the platform, with 41% of participants exceeding the stringent reliable change index.

%\subsection{Physical Quantity Unit Retrieval}
%
%%TODO (maybe): shorten here
%
%Physical quantities have units, such as `kilogram (kg)' for `mass' or `seconds (s)' for `time.' For a physics calculation result to be complete, it is thus necessary that units are included. In computer calculations, this poses an additional challenge. Several methods have been developed to address this.
%Antoniu et al.~\cite{DBLP:conf/icse/AntoniuSKNF04} present `XeLda', a tool for unit checking (correctness validation) in excel spreadsheets. The tool identifies clashes with unit annotations and highlights cells with incorrect units. It also tracks the formula sources by drawing arrows. Using XeLda, the authors demonstrate some errors in published scientific documents.
%Liew et al.~\cite{DBLP:conf/flairs/LiewS02} describe a technique for checking dimensional correctness in physics equations. The motivation is to support Intelligent Tutoring System (ITS) in their ability to reason about a student input and pinpoint mistakes. The system addresses physics problems that are answered by a set of algebraic equations. Their solution demands both an understanding of the physics concepts and algebraic skills. The authors describe their success in determining the dimensional consistency of the physics equations using `constraint propagation' with a knowledge base of well-known physics variables. The approach is evaluated on college-level introductory physics student answers.
%Schubotz et al.~\cite{DBLP:conf/cikm/SchubotzVC16} present an investigation to automate the process of `getting the units right'. They tackle the problem that in many cases, identifier units are not given explicitly in formulae requiring them to be inferred contextually or algebraically. The method is applied to physics formulae from Wikipedia articles, which are augmented with information from Wikidata.
%%In the future, the authors aim to employ logical constraints as a feedback mechanism for statistical methods in natural language processing.
%
%\subsection{Computer Algebra Systems in Physics}
%
%To solve equations or equation systems for specific variables, Computer Algebra Systems (CAS) are employed as a powerful analytic, numerical, and computational tool.
%Savelsbergh et al.~\cite{DBLP:journals/jcal/SavelsberghJF00} discuss learning physics with a computer algebra system and present methods to promote the ability of students to connect problem features to solution methods. Their approach is based on the commercial software `Mathematica'\footnote{\url{https://www.wolfram.com/mathematica}} and tested for an electrostatics course.
%Wooff et al.~\cite{wooff1988computer} describe and discuss the merits of using computer algebra in physics teaching. They present approaches to perform calculations including units from the fields of circuit analysis, special relativity, and classical mechanics. Belu~\cite{belu2009teaching} claims that the conceptual and reasoning difficulties that many students have in introductory and advanced physics courses are often caused by difficulties to connect physical concepts and situations with relevant mathematical formalisms. The authors illustrate ways for teachers to use CAS like `Maple', `Mathcad', `Macsyma' or `Mathematica' to visualize characteristics and mechanisms of equations and help students to understand the physics concepts.

\section{Methods and Implementation}\label{sec:Methods}

In this section, we describe the development of our PhysWikiQuiz physics question generation and test engine, along with the system workflow and module details. PhysWikiQuiz employs the method of Mathematical Entity Linking (see Section \ref{sec:Background}).

The prerequisites for the PhysWikiQuiz system are that it 1) is intended to generate questions as part of an education platform, 2) employs Wikidata as knowledge-base, 3) works on formula concepts, 4) requires formula and identifier unit retrieval, and 5) utilizes a Computer Algebra System to correct the student's answer.

\subsection{System Workflow}

% not needed anymore, since introduced before:

%In the following, we use the term `identifier' for the symbols occurring in the defining formula of the concept question. For example, the concept `mass-energy equivalence' (see Section \ref{subsec:FormulaConcepts}) has the `defining formula' $E= m c^2$ with identifiers E, m, and c.

Figure \ref{fig:ExampleSpeed} shows the PhysWikiQuiz User Interface (UI) for an example formula concept name input `speed' with a defining formula of $v=s/t$. The formula can be rearranged as $s=v*t$ or $t=s/v$ (two question sets). For the identifier symbols $v$, $s$, and $t$, their names `velocity', `distance', and `duration' and units `\verb|m s^-1|', `\verb|m|', and `\verb|s|' are retrieved from the corresponding Wikidata item\footnote{\url{https://www.wikidata.org/wiki/Q3711325}}. In the example, the answer is considered as correct in both value `60' and unit `\verb|m|'. If the user clicks again on the `Generate' button, a new question with different formula rearrangement and identifier values is generated. For a system feedback of `Value incorrect!' and/or `Unit incorrect!', the student has the possibility to try other inputs by changing the input field content and clicking again on the `Answer' button.
%
%\paragraph{Modules}
%
%Figure \ref{fig:ModulesWorkflow} is an overview of the system workflow with its six modules.

The PhysWikiQuiz workflow is divided into six modules (abbreviated by Mx in the following).
In M1, formula and identifier data is retrieved from Wikidata. In M2, the formula is rearranged using the python CAS Sympy\footnote{\label{foot:SymPy}\url{https://www.sympy.org}}. In M3, random values are generated for the formula identifiers. In M4, the question text is generated from the available information. In M5, the student's answer is compared to the system's solution. Finally, M6 generates an explanation text for the student. In case some step or module cannot be successfully executed, the user is notified, e.g., `No Wikidata item with formula found.'
%
%\begin{figure}[]
%    \centering
%    \begin{adjustbox}{angle=90}
%    \includegraphics[]{images/PhysWikiQuiz-Workflow(compact%).png}
%    \end{adjustbox}
%    \caption{Modules workflow (M1-M6) of >>PhysWikiQuiz<<.}
%    \label{fig:ModulesWorkflow}
%\end{figure}
%
%\paragraph{Example}
%
%Figure \ref{fig:ExampleAcceleration} shows an example workflow for the concept `acceleration'. The formula expression tree~\cite{DBLP:conf/mkm/SchubotzMHCG17} is visualized. Below each of the identifier symbols a, v, and t, information about its properties (Wikidata item name and QID, unit dimension) is displayed.
%
%\begin{figure}[]
%    \centering
%    \includegraphics[width=\textwidth]{images/workflow_acceleration.png}
%    \caption{Example identifier property tree for `acceleration'. For the defining formula $a = \frac{dv}{dt}$, the identifier information needed for the question generation and answer correction is retrieved from the Wikidata item Q11376.}
%    \label{fig:ExampleAcceleration}
%\end{figure}

\subsection{Modules}
\label{subsec:Modules}

%\paragraph{Module 1: Formula Concept and Identifier Property Retrieval}

After the user inputs the formula concept name or Wikidata QID (see Figure~\ref{fig:ExampleSpeed}), M1 retrieves the `defining formula' and identifier properties.
%Figure \ref{figWikidataItemSpeed} shows the relevant properties in the UI of the Wikidata item.
%
%\begin{figure}[]
%    \centering
%    \includegraphics[width=\textwidth]{images/speed_item(condensed).png}
%    \caption{Example Wikidata Item for formula concept `speed' (\url{https://www.wikidata.org/wiki/Q3711325}). The system retrieves the concept name, `defining formula' (property: P2534), identifier properties `in defining formula' (P7235), and formula unit `ISQ dimension' (P4020).}
%    \label{figWikidataItemSpeed}
%\end{figure}
%
%Figure \ref{fig:ExampleAcceleration} displays an example identifier property tree for the `acceleration' formula. The information is retrieved from the Wikidata item Q11376.
%
%Unfortunately, there is no community consensus on storing the identifier information. There are several competing properties, such as `has part' (P527), `calculated from' (P4934), and `in defining formula' (P7235). For a discussion of the different usages, refer to~\cite{DBLP:conf/www/ScharpfSG21,Scharpf2021b}
%TODO (maybe): update Wikidata workshop citation to dblp version
%and the Wikidata property proposal discussion page\footnote{\label{foot:CommunityDiscussion}https://www.wikidata.org/wiki/Wikidata:Property\_proposal/symbol\_represents}.
PhysWikiQuiz supports all current identifier information formats and strives to stay up to date. The identifier units need to be retrieved from the linked items (in some formats, also the names). Currently, units are stored using the `ISQ dimension' property (P4020 in Wikidata). To make the format more readable for students, the unit strings (e.g., \verb|'L T^-1'|) are translated into SI unit symbols\footnote{\url{https://en.wikipedia.org/wiki/International_System_of_Quantities}} (e.g., \verb|'m s^-1'|).
%using a the following mapping table: \verb|{'L': 'm', 'M': 'kg', 'T': 's', 'I': 'A'|,\\
%\verb|'\Theta': 'K', 'N': 'mol', 'J': 'cd'}|.

%\paragraph{Module 2: Formula Rearrangement}

Having retrieved the required formula and identifier information, M2 is called to generate possible rearrangements using the CAS of SymPy\footref{foot:SymPy}, a python library for symbolic mathematics~\cite{DBLP:journals/cca/JoynerCMG11}.
%SymPy includes features ranging from basic symbolic arithmetic to calculus, algebra, discrete mathematics, and physics.
%
Since the `defining formula' property of the Wikidata item stores the formula in \LaTeX\ format, which is different from the calculable Sympy CAS representation, a translation is necessary. There are several possibilities available for this task.
%
%\paragraph{LaTeX2Sympy}
%
%\url{https://github.com/augustt198/latex2sympy} -> original
%\url{https://github.com/OrangeX4/latex2sympy} -> used
The python package LaTeX2Sympy\footnote{\url{https://github.com/OrangeX4/latex2sympy}} is designed to parse \LaTeX\ math expressions and convert it into the equivalent SymPy form.
%
%\paragraph{LaCASt}
The Java converter LaCASt~\cite{DBLP:journals/aslib/Greiner-PetterS19,greiner2022math}, provided by the VMEXT~\cite{DBLP:conf/mkm/SchubotzMHCG17} API\footnote{\url{https://vmext-demo.formulasearchengine.com/swagger-ui.html}}
%https://vmext-demo.formulasearchengine.com/v2/api-docs
translates a semantic \LaTeX\ string to a specified CAS.
%The available systems are Maple, Mathematica, and SymPy.
In our system evaluation (Section~\ref{sec:Evaluation}), we compare the performance of both translators. For them to work correctly, PhysWikiQuiz performs a number of \LaTeX\ cleanings beforehand, such as replacements and removals that improve the translation performance. %which were identified heuristically.
%: 1) Simple cleanings, such as replacing `E' by `e', such that Sympy does not confuse it with the power of ten operator or substituting \verb|\cdot| and \verb|\times| by \verb|*|; 2) Argument cleanings, such as removing \verb|\vec{}|, \verb|\mathrm{}|, or \verb|\boldsymbol{}| identifier environments.

%Using Sympy, PhysWikiQuiz can generate as many representations as there are different identifiers in the formula.

%\paragraph{Module 3: Identifier Value Generation}

With the Sympy calculable formula representation available, M3 is ready to replace the right-hand side identifiers with randomly generated integer values. A lower and upper value can be chosen freely. We use the default range from 1 to 10 in our evaluation.
%As a prerequisite for the substitution, the individual identifiers need to be converted into a Sympy form.
Finally, having successfully replaced the right-hand side identifiers by their respective generated random values, the left-hand side identifier value is calculated. The value is later compared to the student input by M5 (answer correction) to check the validity of the question-answer value.
%
%\paragraph{Module 4: Question Text Generation}
%
At this stage, all information needed to generate a question is available: (1) the formula, (2) the identifier symbols, (3) the identifier (random) values, and (4) the identifier units. M4 generates the question text by inserting the respective information into gaps of a predefined template with placeholders for formula identifier names, symbols, and units.
%: \verb|What is the a b, given c = d e, ..., f = g h ?|, with
%variables a-h storing left- and right-hand side identifier names, symbols, values, and units.
%\begin{itemize}
%    \item a: left-hand side identifier name,
%    \item b: left-hand side identifier symbol,
%    \item c: first right-hand side identifier symbol,
%    \item d: first right-hand side (random) identifier value,
%    \item e: first right-hand side identifier unit,
%    \item f: last right-hand side identifier symbol,
%    \item g: last right-hand side (random) identifier value,
%    \item h: last right-hand side identifier unit.
%\end{itemize}
%Note that between the first and the last right-hand side identifier information c-e and f-h, there is a space \verb|...| of arbitrary length for additional identifier properties.
For a question text example, refer to the screenshot in Figure~\ref{fig:ExampleSpeed}.

%\paragraph{Module 5: Answer Correction}

After the question text is displayed by the UI, the student can enter an answer consisting of value and unit %(space-separated, if not the system reminds)
for the left-hand side identifier solution. The information is then parsed by M5. It is subsequently compared to the value output of M1 (solution unit) and M3 (solution value). The student gets feedback on the correctness of value and unit separately.
%In most cases, since the random values are integers, fractions as input are sufficient, and there is no need to use a calculator to get decimal results.
The system accepts fractions or decimal numbers as input (e.g., $5/2 = 2.5$), which is then compared to the solution with a tolerance that can be specified (default value is $\pm1\%$).
%
%\paragraph{Module 6: Explanation Generation}
%
Finally, after the question is generated and the correctness of the solution is assessed by the system, M6 generates an explanation such that the student can understand how a given solution is obtained. The system returns and displays an explanation text
%of the form\\
%\verb|Solution from www.wikidata.org/wiki/Qxxx|\\ \verb|formula a = b with c d = e f ... g h .|, with a-f 
storing left- and right-hand side identifier names, symbols, values, and units (see M4).
%\begin{itemize}
%    \item Qxxx: the formula concept Wikidata item QID,
%    \item a: the formula left-hand side
%    \item b: the formula right-hand side
%    \item c: the left-hand side identifier value
%    \item d: the left-hand side identifier unit
%    \item e: the first right-hand side identifier value
%    \item f: the first right-hand side identifier unit
%    \item g: the last right-hand side identifier value
%    \item h: the last right-hand side identifier unit
%\end{itemize}
%Again, between the first and the last right-hand side identifier information e f and g h, there is a space \verb|...| of arbitrary length for additional identifier properties.
For an explanation text example, refer to the screenshot in Figure \ref{fig:ExampleSpeed}.

\section{Evaluation}\label{sec:Evaluation}

In this section, we present and discuss the results of a detailed PhysWikiQuiz system evaluation at each individual stage of its workflow. We carry out module tests for the individual modules and an integration test to assess the overall performance on a formula concept benchmark dataset (see Section \ref{sec:BenchmarkDataset}). All detailed tables can be found in the \verb|evaluation| folder of the repository\footnote{\label{foot:evalrepo}\url{https://github.com/ag-gipp/PhysWikiQuiz/blob/main/evaluation}}.

\subsection{Benchmark Dataset}
\label{sec:BenchmarkDataset}

The open-access platform `MathMLben'\footnote{\url{https://mathmlben.wmflabs.org/}} stores and displays a benchmark of semantically annotated mathematical formulae~\cite{DBLP:conf/jcdl/SchubotzGSMCG18}. They were extracted from Wikipedia, the arXiv and the Digital Library of Mathematical Functions (DLMF)\footnote{\url{https://dlmf.nist.gov}} and augmented by Wikidata markup~\cite{DBLP:conf/sigir/ScharpfSG18}. The benchmark can be used to evaluate a variety of MathIR tasks, such as the automatic conversion between different CAS~\cite{DBLP:conf/jcdl/SchubotzGSMCG18} or MathQA~\cite{mathqajournal}. The system visualizes the formula expression tree using VMEXT~\cite{DBLP:conf/mkm/SchubotzMHCG17} to reveal how a given formula is processed.
In our PhysWikiQuiz evaluation, we employ a selection of formulae
%(GoldID 310-375)
from the MathMLben benchmark.
%\footnote{\url{https://github.com/ag-gipp/PhysWikiQuiz/blob/main/evaluation/benchmark_dump/sample_items.json}}
%%TODO (maybe): comment out
The formula concepts were extracted from Wikipedia articles using the formula and identifier annotation recommendation system~\cite{DBLP:conf/recsys/ScharpfMSBBG19,DBLP:conf/www/ScharpfSG21} >>AnnoMathTeX<<\footnote{\label{foot:AMT}\url{https://annomathtex.wmflabs.org}}.

\subsection{Overall System Performance}\label{subsec:SystemPerformance}

Table \ref{tab:IntegrationTest} shows example evaluation results (selection of instances and features) on the MathMLben formula concept benchmark.
%We evaluate the PhysWikiQuiz system performance on the selected MathMLben formula concept benchmark.
%(MathMLben GoldID 310-375)
For each example concept in the benchmark selection, e.g., `acceleration' (GoldID 310 or Wikidata Q11376), the individual modules are tested individually.
%At the bottom, the total percentage of the available functionality is provided.
%
%\begin{table*}[t]
%    \caption{Evaluation results for a formula concept selection
%    %(GoldID 310-375)
%    from the benchmark MathMLben (\url{mathmlben.wmflabs.org}). Each individual crucial step of the PhysWikiQuiz workflow is evaluated.}
%    \includegraphics[width=\textwidth]{images/unit_test_module_workflow_latex2sympy_evaluated(short).png}
%    \label{tab:IntegrationTest}
%\end{table*}

%\begin{table}[h]
%\centering
%\resizebox{\columnwidth}{!}{
%\begin{tabular}{|l|l|l|l|l|l|l|l|l|}
%\hline
%\textbf{GoldID} & \textbf{QID} & \textbf{Name}        & \textbf{Formula unit} & \textbf{Identifier units} & \textbf{Formula sympy} & \textbf{Identifiers sympy} & \textbf{Sympy solve} & \textbf{Sympy rhs} \\ \hline
%310             & Q11376       & acceleration         & yes                   & yes                       & no                     & yes                        & no                   & no                 \\ \hline
%311             & Q186300      & angular acceleration & yes                   & yes                       & no                     & yes                        & no                   & no                 \\ \hline
%312             & Q834020      & angular frequency    & yes                   & yes                       & yes                    & yes                        & yes                  & yes                \\ \hline
%...             & Qxyz         & concept name         & yes / no              & yes / no                  & yes / no               & yes / no                   & yes / no             & yes / no           \\ \hline
%\end{tabular}}
%\caption{Example evaluation results for a formula concept selection
%%    %(GoldID 310-375)
%from the benchmark MathMLben (\url{mathmlben.wmflabs.org}). Each individual crucial step (module) of the PhysWikiQuiz workflow is evaluated: identifier semantics and unit retrieval, formula translation, and explanation text generation.}
%\label{tab:IntegrationTest}
%\end{table}
%
Using a workflow evaluation automation script, we create two separate evaluation tables for the two \LaTeX\ to SymPy translators that we employ (LaTeX2Sympy and LaCASt, see the description of M2 in Section \ref{subsec:Modules}).
%
%LaTeX2Sympy:
%Working: 13 yes, 34 no, 19 part
The overall system performance using the LaTeX2Sympy converter is the following. For 20\% of the concepts, all modules are working properly, and PhysWikiQuiz can provide both a question text, an answer verification with correct internal calculation, and an explanation text. For 29\%, only the question can be displayed, but the system's calculation is wrong, such that the answer correction and explanation text generation do not work correctly. For 52\%, PhysWikiQuiz cannot provide a question.
In summary, the system is able to yield 48\% `question or correction'\footnote{Although the case of `no question but correction' is not very intuitive, it did occur.}, 20\% `question and correction', 29\% question, and 52\% none.
%
%LaCASt:
%Working: 17 yes, 37 no, 12 part
The overall system performance using the LaCASt converter is the following. For 26\% of the concepts, all modules are working properly. For 18\%, only the question can be displayed. For 56\%, PhysWikiQuiz can not provide a question.
In summary, the system is able to yield 44\% `question or correction', 26\% `question and correction', 18\% question, 56\% none.

Table \ref{tab:LaTeX2SympyvsLaCASt} summarizes the performance comparison of the two translators.
%Since some of these numbers are low, we dedicate an entire section (Section \ref{sec:Discussion}) to the discussion of the challenges.
We include a detailed discussion of the issues in external dependencies that cause this relatively low performance in Section \ref{sec:Discussion}.
%
Overall, LaCASt performs better in generating both question and correction but cannot provide either question or correction on slightly more instances. We deploy LaCASt in production.

%Our evaluation automation script generates detailed logs with all properties of interest.
%listed in Table \ref{tab:log_props}.
%The log files for each converter can be found in the \verb|evaluation| folder in the repository\footref{foot:evalrepo}.
%
%\begin{table}
%\caption{Properties included in the evaluation logs to trace back each step in the workflow for each test example.}
%\resizebox{\columnwidth}{!}{
%\begin{tabular}{|l|l|}
%\hline
%\textbf{Property}                     & \textbf{Example}                                                          \\ \hline
%Concept Name                          & frequency                                                                 \\ \hline
%Concept QID                           & Q11652                                                                    \\ \hline
%Defining formula                      & f = \textbackslash{}frac\{1\}\{T\}                                        \\ \hline
%Formula unit                          & \textbackslash{}mathsf\{T\}\textasciicircum{}\{-1\}                       \\ \hline
%Identifier properties & {[}('frequency', 'f',   's\textasciicircum{}-1'), ('period', 'T', 's'){]} \\ \hline
%Properties in                         & P7235                                                                     \\ \hline
%Identifiers sympy                     & (f, T)                                                                    \\ \hline
%Formula sympy                         & 1/T                                                                       \\ \hline
%Sympy rhs                             & 1                                                                         \\ \hline
%Question text                         & What is the frequency   f, given period T = 1 s ?                         \\ \hline
%\end{tabular}}\label{tab:log_props}
%\end{table}
%

\begin{table}[t]
\centering
\resizebox{\columnwidth}{!}{
\begin{tabular}{lllll}
\hline
\textbf{Translator}  & \textbf{quest. OR corr.} & \textbf{quest. AND corr.} & \textbf{only quest.} & \textbf{none} \\ \hline
\textit{LaTeX2Sympy} & 48\%                     & 20\%                      & 29\%                 & 52\%          \\ %\hline
\textit{LaCASt}      & 44\%                     & 26\%                      & 18\%                 & 56\%          \\ \hline
\end{tabular}}
\caption{Comparison of \textit{LaTeX2Sympy} and \textit{LaCASt} translator in overall system performance for question (quest.) generation and correction (corr.) ability.}
\label{tab:LaTeX2SympyvsLaCASt}
\end{table}

\begin{table}[]
\centering
\resizebox{\columnwidth}{!}{
\begin{tabular}{llllll}
\hline
\textbf{GoldID} & \textbf{QID} & \textbf{Name}        & \textbf{Identifier semantics} & \textbf{Formula translation} & \textbf{Explanation text} \\ \hline
310             & Q11376       & acceleration         & yes                           & no                           & yes                       \\ %\hline
311             & Q186300      & angular acceleration & yes                           & no                           & yes                       \\ %\hline
312             & Q834020      & angular frequency    & yes                           & yes                          & yes                       \\ \hline
%...             & Qxyz         & concept name         & yes / no                      & yes / no                     & yes / no                  \\
%\hline
\textbf{Total}             &          & \textbf{Performance}         & \textbf{97\% yes}                      & \textbf{60\% yes}                     & \textbf{27\% yes}                  \\
\hline
\end{tabular}}
\caption{Three example evaluation results out of a formula concept selection
%    %(GoldID 310-375)
from the benchmark MathMLben (\url{https://mathmlben.wmflabs.org/}). Each individual module of the PhysWikiQuiz workflow is evaluated. Here, we only show a summary of the main steps (last three columns condensed from eight, see the
repository).}
\label{tab:IntegrationTest}
\end{table}
%TODO: describe how 8 features -> 3

\subsection{Module Evaluation}

In the following, we present a detailed evaluation of the individual modules or stages in the workflow.

\paragraph{Retrieval Formula Identifier Semantics and Units}

The first stage of module tests is the assessment of the correct retrieval of the identifier semantics.
%(columns `Properties', and `Identifier units' in Table \ref{tab:IntegrationTest}).
%The log file traces and records the set of identifier properties as (name, symbol, unit) triples.
%(see Table \ref{tab:log_props}).
Since names and symbols are fetched from Wikidata items that are linked to the main concept item, the retrieval process is prone to errors.
%TODO (maybe): comment out
%First, we evaluate the available property type, `has part': `hp', `calculated from': `cf', and `in defining formula': `idf' (see the description of module 1 in Section \ref{subsec:Modules}). We find 33 hp, 21 cf, 29 idf, and 2 none.
%This means that
However, we find that for 97\% of the concepts, identifier properties are available in some of the supported formats. %For `conservation of momentum' (Q2305665), a new property `depicts' (P180) was introduced, which is not the community standard\footref{foot:CommunityDiscussion}. For `Hamiltonian operator' (Q660488), no identifier properties were available. We hope that in the future, format inconsistencies will be more rigorously addressed by the community.
%
%\subsection{Retrieval of Formula and Identifier Units}

\paragraph{Retrieval of Formula and Identifier Units}

The next workflow stage we evaluate is the formula and identifier unit retrieval.
%Columns `Formula unit' and `Identifier units' of Table \ref{tab:IntegrationTest} show the assessment results.
For 53\% of the test examples, a formula unit is available on the corresponding main concept Wikidata item. For the remaining 47\%, identifier units are available on the respective linked Wikidata items.

\paragraph{LaTeX to SymPy Translation}

The subsequent module tests are concerned with the LaTeX to SymPy translations, which is mandatory for having Sympy rearrange the formula and yield a right-hand side value given random identifier value substitutions (modules 2 and 3).
%Columns `Formula sympy' and `Identifiers sympy' in Table \ref{tab:IntegrationTest} describe the success of the translation.
We evaluate the two converters LaTeX2Sympy and LaCASt in comparison, which were introduced in Section \ref{subsec:Modules}. LaTeX2Sympy is able to yield a correct and calculable SymPy formula in 50\% of the cases. Moreover, it can provide usable Sympy identifiers for the substitutions in 47\% of the cases. For LaCASt, the SymPy formula is correct in 60\% and the SymPy identifiers in 47\%. This means that LaCASt has a better translation performance (10\% more), while the identifier conversion remains the same.

\paragraph{Formula Rearrangement Generation}

Formula rearrangements enhance the availability of additional question variations. In the case of our example `speed', when using Sympy rearrangements, the other variables `distance' and `durations' can also be queried, providing additional concept questions.
%
%The number of questions generated by PhysWikiQuiz can be calculated as
%\begin{align*}
%    N_\text{generated} = N_\text{identifiers} \cdot R_\text{values}^{(N_\text{identifiers} - 1)} 
%\end{align*}
%with
%$N_\text{identifiers}$: Number of identifiers in formula concept,\\
%$R_\text{values}$ = 10: Range for random identifier value generation (here from 1-10).
%
%This leads to the following table:\\
%
%\begin{tabular}{|l|l|}
%\hline
%N\_identifiers & N\_generated \\ \hline
%1              & 1            \\ \hline
%2              & 20           \\ \hline
%3              & 300          \\ \hline
%4              & 4000         \\ \hline
%5              & 50000        \\ \hline
%6              & 600000       \\ \hline
%7              & 7000000      \\ \hline
%8              & 80000000     \\ \hline
%9              & 900000000    \\ \hline
%10             & 10000000000  \\ \hline
%\end{tabular}\\
%
For lengthy formulae, PhysWikiQuiz can generate a very large amount of question variations. But even for a small formula with 2 identifiers, there are already many possibilities by substituting different numbers as identifier values. On average, the formulae in the test set contain 3 identifiers. Substituting combinations of numbers from 1 to 10, this leads to several hundred potential questions per formula concept.
%
%As column `Sympy solve' in Table \ref{tab:IntegrationTest} shows,
We find that in 27\% of the cases, Sympy can rearrange the `defining formula.' The result is the same for both LaTex2Sympy and LaCASt translation. In comparison to a workflow without M2, %$10*(2*3+12*2+2*1) = 
more than 300 additional questions can be generated.
%\footnote{Taking $R_\text{values} = 10$ multiplied by the sum of additional identifiers.}.
%This corresponds to an average of 20 per concept (two additional identifier concepts). In two cases, the rearrangement is trivial and does not lead to additional questions since the formula contains only one identifier.

\paragraph{Right-Hand Side Substitutions and Explanation Text Generation}

The last two module test evaluations assess the success of right-hand side substitutions and explanation text generation.
%(column `Sympy rhs')
For LaTeX2Sympy, 45\% of substitutions are made correctly, whereas LaCASt achieves 53\%. Both translators generate correct identifier symbol-value-unit substitutions for the explanation text in 39\% of the test cases.

\section{Discussion}\label{sec:Discussion}

In this section, we discuss our results, contribution, and retrieval challenges of the individual workflow stages and modules. The full list of challenges can be found in the
%appendix in the
repository\footref{foot:evalrepo}.
%\footnote{\label{foot:appendix}\url{https://github.com/ag-gipp/PhysWikiQuiz/blob/main/evaluation/PhysWikiQuiz_Challenges.pdf}}.

\paragraph{Results and Contribution}

Wikidata currently contains around 5,000 concept items with mathematical formula. Out of these, about 500 are from the physics domain.
Using a Computer Algebra System, PhyWikiQuiz can generate concept questions with value and unit and corrections in around 50\% of the cases.
For a detailed analysis of the errors in the remaining 50\% and a discussion of the challenges to tackle, see the next subsection.

Our contribution is a proof of concept for the physics domain to use Wikidata in education.
We develop a >>PhysWikiQuiz<< question generator and solution test engine and evaluate it on an open formula concept benchmark dataset.
Our work addresses the research gap in comparing methods to generate physics questions from Wikidata knowledge.
We find that using Wikidata and a Computer Algebra System, it is possible to generate an unlimited amount of physics questions for a given formula concept name. Although they all follow a very similar template with very little variation, they contain different variable values, which makes them suitable to provide individual questions for various students.

\subsection{Challenges and Limitations}

\paragraph{Formula Semantics and Translation}

%Retrieval of Formula Identifier Semantics and Units

%
%\paragraph{Challenges}
%
We manually examine the concepts for which the Wikidata items do not provide units. For some of them, we identify semantic challenges. In our estimation, the concepts either (1) should not have a unit (`ISQ dimension' property) or (2) it is debatable whether they should have one. Example QIDs for the respective cases can be found in the repository\footref{foot:evalrepo}.
In the first case, the respective formulae do not describe physical quantities but formalisms, transformations, systems, or objects. In particular, the formula right-hand side identifier that is calculated does not correspond to the concept item name. In the second case, the corresponding formula provides the calculation of a physical quantity that is not reflected in the concept name. Finally, there is a third case in which the concept item should have a unit property since the formula describes a physical quantity on the right-hand side that is defined by the concept name.
%
%Examples for case 1) are `center of mass
%' (Q2945123), `Euler-Lagrange equation' (Q875744), `Galilean transformation' (Q219207), `Hamilton-Jacobi equation' (Q1060137), `Lorentz transformation' (Q217255), `oscillation' (Q170475), `pendulum' (Q20702), `sphere' (Q12507), and `Spherical pendulum' (Q3299367). For these concepts, ...
%
%Examples for case 2) are `Newton's second law of motion for constant mass' (Q2397319)`conservation of energy' (Q11382), `conservation of momentum' (Q2305665), `damping', `Dirac equation', `Dirac equation in curved spacetime', `energy-momentum relation' (Q103439852), `free fall' (Q140028), `Hamiltonian operator' (Q660488), `Lagrangian operator' (Q103687426), `mass-energy equivalence' (Q35875), `Newton's second law of motion for constant mass' (Q2397319), `Newton's third law of motion' (Q3235565), `Stokes' law' (Q824561), and `uniform motion' (Q376742). As the community pointed out in revision comments, `a formula or law does not have a dimension'. Yet, in some cases, the corresponding formula provides the calculation of a physical quantity, which is not reflected in the concept name. For instance, `Stokes law' is a formula to calculate a `force', which has a unit. Thus, the items of case 2) are missing in the PhysWikiQuiz repertoire due to semantic concerns of the community and the subsequent absence of an `ISQ dimension' property. PhysWikiQuiz cannot provide questions for them, which actually would have made didactic sense.
%
%Finally, there is also case 3), in which the concept item should have a unit property since the formula describes a physical quantity on the right-hand side that is defined by the concept name. Examples are: `electromagnetic force' (Q849919), `electrostatic force (Q103438301), `four-momentum' (Q1068463), `four-velocity' (Q1322540), `generalized momentum' (Q6806305), `Lorentz factor' (Q599404), and `tangential velocity' (Q103715245).
%
%LaTeX to SymPy Translation
%
%
%\paragraph{Challenges}
%
Examining the examples for which the converters cannot provide a properly working translation, we find some challenges that require the development of more advanced \LaTeX\ cleaning methods. Derivative fractions can contain identifier differentiation with or without separating spaces. For example, `acceleration' can be calculated either as \verb|\frac{d v}{d t}| or \verb|\frac{dv}{dt}|. The first formula is correctly translated to the calculable SymPy form \verb|Derivative(v, t)|, whereas the second does not work. Unfortunately, the spaces cannot be introduced automatically in the arguments without losing generality (e.g., \verb|dv| could also mean a multiplication of some identifiers d and v as \verb|d * v|). Implicit multiplication is a general problem. However, it is very likely for a \verb|\frac{}{}| expression with leading \verb|d| symbols in its arguments to contain a derivative, and the risk of losing generality should maybe be taken. In the case of partial derivatives, such as \verb|\frac{\partial v}{\partial t}| the problem does not arise since \verb|\partial| needs a following space to be a proper \LaTeX\ expression.
%
Some formulae are not appropriate for PhysWikiQuiz question generation and test. The expression \verb|\sum_{i=1}^n m_i(r_i - R) = 0| in `center of mass
' (Q2945123) does not have a single left-hand-side identifier to calculate. The right-hand side is always zero. The equation (correctly) also does not have a formula unit. Finally, expressions like \verb|p_{tot,1} = p_{tot,2}| in `conservation of momentum' (Q2305665) are no functional linkage of identifier variables and thus do not serve as basis for calculation questions.

\paragraph{Identifier Substitutions and Explanation Text Generation}

%Right-Hand Side Substitutions
%
%\paragraph{Challenges}
%
For about half of the test examples, the substitution is unsuccessful due to some peculiarities in the defining formula. The full list can be found in the repository\footref{foot:evalrepo}. We encounter the problems that (1) substitutions cannot be made if identifier properties are not available, (2) for some equations, the left-hand side is not a single identifier, but a complex expression or the right-hand side is zero, (3) two equation signs occur in some instances, and (4) identifier properties and formula are not matching in their Wikidata items for some items.
%
%We now present and summarize some of the most interesting examples:
%
%\begin{itemize}
%\item If identifier properties were not available, substitutions could not be made, e.g., for `conservation of momentum' (Q2305665).
%\item Indices like \verb|a_C=| in `Centripetal acceleration' (Q2248131) are not supported.
%\item For some equations, the right-hand side is set zero, e.g., `center of mass' (Q2945123).
%\item For some equations, the left-hand side is not a single identifier but a complex expression, e.g., `Euler–Lagrange equation' (Q875744).
%\item Sometimes two equation signs \verb|=| occur, e.g., `escape velocity' (Q166530).
%\item For some items, identifier properties and formula are not matching, e.g., `force' (Q11402). The provided identifiers do not belong to the provided formula.
%\item Equations like the `Four-momentum' (Q1068463) represent vectors that have multiple indices. Vector and matrix calculations are beyond the scope of PhysWikiQuiz since they are too complex for the target group of students.
%\item Some operators like \verb|\mapsto| or \verb|\dot| are not supported, e.g., `Galilean transformation' (Q219207). They could, however, be translated into calculable forms.
%\item Arguments with variable dependencies like in `Hamiltonian operator' (Q660488) are not supported. They could be eliminated using some cleaning transformations.
%\end{itemize}
%
%\subsection{Explanation Text Generation}
%
The last stage in our workflow evaluation is the assessment of the explanation text correctness. All in all, for 27\% of the concepts, explanation texts can be generated, out of which 39\% contain correct identifier symbol-value-unit substitutions. We conclude that the calculation path display is error-prone and outline some challenges in the following.
%
%\paragraph{Challenges}
%
For the explanation texts that are incorrect, we identify some of the potential reasons. We find that (1) in some cases, operators like multiplications are missing, (2) some equations contain dimensionless identifiers, for which the unit is written as the number 1, and (3) in case integrals appear in the formulae, sometimes a mixture of non-evaluated expressions and quantities is displayed.
%Again, here we only discuss some examples we find most interesting. For the complete list, refer to the repository\footref{foot:evalrepo}.

%\begin{itemize}
%\item In some cases, e.g., `energy-momentum relation' (Q103439852) (implicit) operators like multiplication or square power (brackets) are missing.
%\item Some equations like `Hooke's law' (Q170282) contain dimensionless identifiers, for which the unit is written as the number 1. This leads to strange expressions, such as \verb|10 1| instead of \verb|10 m|.
%\item For equations like `mass-energy equivalence' (Q35875) the right-hand side value is missing because it could not be calculated by SymPy.
%\item In case integrals appear in the formulae, sometimes a mixture of non-evaluated expressions and quantities is displayed. For example \verb|J_{Q} = \int r_{Q}^{2} d  7 kg| in the explanation of `moment of inertia' (Q165618).
%\end{itemize}

%---

\subsection{Takeaways}

\paragraph{Answering the research questions.}

% AS:
%At the end of the paper, the reader should take home three messages:
%1. This is a good idea and it is solidly implemented
%2. Performance problems are due to input data / libraries (that can be 
%fixed)
%3. Steps necessary to fixing the challenges are clear and obvious so 
%future work by the community can address them
%

Having implemented and evaluated the system, we can answer our research question as follows:

%\item Can we build a scalable framework for collaborative AI-aided test question generation?

\begin{enumerate}
%    \item Using a semantic-knowledge base (Wikidata) and a Computer Algebra System (CAS), we are able to successfully build a scalable framework for collaborative AI-aided test question generation in the physics domain.
\item PhysWikiQuiz outperforms its competitors by providing a constantly growing number of more than 10 times additional community-curated questions.
    \item We employ and adapt the method of Mathematical Entity Linking
%~\cite{DBLP:conf/www/ScharpfSG21}
of formula concepts
%~\cite{DBLP:conf/sigir/ScharpfSCG19}
for question generation using Wikidata.%adapt?
    \item About 50\% of the benchmark formula Wikidata items can be successfully transformed into questions with correction and explanation. For the remaining cases, we provide an extensive error analysis.
    \item The performance directly depends on formula and identifier name, symbol and unit retrieval, as well as translation to and solving by a CAS.
    \item The bottleneck is caused by the dependencies, such as the CAS Sympy and translator LaCASt. A clearer community agreement on data quality guidelines in Wikidata would also improve the results.
    \item We can improve the quality of the formula cleanliness with user feedback by addressing the issues in the dependencies.
\end{enumerate}

\paragraph{Addressing the challenges.}

% AS:
%For all challenges, it would then also be good to discuss possible solutions and ways of addressing them - i.e., we need to make sure the reader gains knowledge on how to proceed

To tackle the current limitations, we propose the following solutions:

\begin{itemize}
    \item Formula semantics: Limit use to concepts that can be indisputably associated with formulae and units to avoid unreliability due to community objection.
    \item Formula translation: Increasingly improve the converter performance by receiving and implementing community feedback to enhance concept coverage.
    %Developing methods to include derivatives and integrals.
    \item Identifier substitutions: Motivate the Wikidata community to seed the missing identifier properties. This will increase coverage by enabling lacking identifier value substitutions.
    %Improve the quality of the formula cleanliness with user feedback
    \item Explanation text generation: The problems are expected to be settled with increased data quality of the formula items in Wikidata.
\end{itemize}

Despite the challenges, we have already built an in-production system (with 13 times more coverage than its best-performing competitor) that can and will be used by teachers in practice.

\section{Conclusion and Future Work}

%Conclusion

In this paper, we present >>PhysWikiQuiz<<, a physics question generation and test engine. Our system can provide a variety of different questions for a given physics concept, retrieving formula information from Wikidata, correcting the student's answer, and explaining the solution with a calculation path. We separately evaluate each of the six modules of our system to identify and discuss systematic challenges in the individual stages of the workflow. We find that about half of the questions cannot be generated or corrected due to issues that can be addressed by improving the quality of the external dependencies (Wikidata, LaTeX2Sympy, LaCASt, and Sympy) of our system. Our
%PhysWikiQuiz MathIR
application demonstrates the potential of mathematical entity linking for education question generation and correction.
%Our work shows that MathEL is not limited to use only in Recommender System for STEM documents and Mathematical Question Answering~\cite{mathqajournal}.
%
%Future Work
%

PhysWikiQuiz is listed on the `Wikidata tool pages' for querying data\footnote{\url{https://www.wikidata.org/wiki/Wikidata:Tools/Query_data}}. We welcome the reader to test our system and provide feedback for improvements. If the population of mathematical Wikidata items continues (e.g., by using tools such as >>AnnoMathTeX<<\footref{foot:AMT}), our system will be able to increasingly support additional questions.
We will continue to assess the overall effectiveness of the knowledge transfer from Wikipedia articles to Wikidata items to PhysWikiQuiz questions.
%
Moreover, we are developing an automation for the Wikidata physics concept item bulk to detect if the question generation or correction is correct, or if the respective items need human edits to make PhysWikiQuiz work.
Detecting these cases will extend the system's operating range and ensures that it works despite the limitations. We also plan to test the system with a larger group of end users.

As a long-term goal, we envision integrating our system into larger education platforms, allowing teachers to simply enter a physics concept about which they want to quiz the students. Students would then receive individually generated questions (via app push notification) on their mobile phones. Having collected all the answers, teachers could then obtain a detailed analysis of the student's strengths and weaknesses and use them to address common mistakes in their lectures. We will evaluate the integrated system with teachers.
%Since there is the potential for numerous interdisciplinary research questions, e.g., from computer science or educational sciences, we welcome other researchers to collaborate with us.
%
Finally, we plan to extend our framework with additional question domains, possibly integrating state-of-the-art external dependencies, Wikifunctions\footnote{\url{https://wikifunctions.beta.wmflabs.org}}, and language models as they are developed to increase the coverage further.
With PhyWikiQuiz and its extensions to other educational domains, we hope to make an important contribution to the `Wikidata for Education' project\footnote{\url{https://www.wikidata.org/wiki/Wikidata:Wikidata_for_Education}}.

%Finally, we will tackle the very challenging problem of generating real-world application questions for the given concepts, such as `A car drives with a speed of 3 meters per second. What distance can it travel in 1 second?'. In a first step, we collected examples of those applied questions 
%TODO (maybe): make sure that the footnote is not on the reference page, otherwise maybe not as footnote
%\footnote{\url{https://github.com/ag-gipp/PhysWikiQuiz/blob/main/evaluation/Applied_questions.csv}} to identify the requirements for the information system and natural language processing models.

\section*{Acknowledgment}

This work was supported by the German Research Foundation (DFG grant GI-1259-1).

\printbibliography[keyword=primary]