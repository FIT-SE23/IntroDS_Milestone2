
@book{AcademicallyAdrift,
  title = {Academically {{Adrift}}},
  abstract = {In spite of soaring tuition costs, more and more students go to college every year. A bachelor's degree is now required for entry into a growing number of professions. And some parents begin planning for the expense of sending their kids to college when they're born. Almost everyone strives to go, but almost no one asks the fundamental question posed by Academically Adrift: are undergraduates really learning anything once they get there?For a large proportion of students, Richard Arum and Josipa Roksa's answer to that question is a definitive no. Their extensive research draws on survey responses, transcript data, and, for the first time, the state-of-the-art Collegiate Learning Assessment, a standardized test administered to students in their first semester and then again at the end of their second year. According to their analysis of more than 2,300 undergraduates at twenty-four institutions, 45 percent of these students demonstrate no significant improvement in a range of skills\textemdash including critical thinking, complex reasoning, and writing\textemdash during their first two years of college. As troubling as their findings are, Arum and Roksa argue that for many faculty and administrators they will come as no surprise\textemdash instead, they are the expected result of a student body distracted by socializing or working and an institutional culture that puts undergraduate learning close to the bottom of the priority list.Academically Adrift holds sobering lessons for students, faculty, administrators, policy makers, and parents\textemdash all of whom are implicated in promoting or at least ignoring contemporary campus culture. Higher education faces crises on a number of fronts, but Arum and Roksa's report that colleges are failing at their most basic mission will demand the attention of us all.},
  file = {/Users/nvk/Zotero/storage/EBGQBDRX/bo10327226.html}
}

@misc{AdvancedCyberinfrastructureCoordination,
  title = {Advanced {{Cyberinfrastructure Coordination Ecosystem}}: {{Services}} \& {{Support}} - {{Coordination Office}} ({{ACCESS-ACO}}) (Nsf21556) | {{NSF}} - {{National Science Foundation}}},
  howpublished = {https://www.nsf.gov/pubs/2021/nsf21556/nsf21556.htm},
  file = {/Users/nvk/Zotero/storage/DX4J43MD/nsf21556.html}
}

@inproceedings{ahmedEquationAttentionRelationship2021,
  title = {Equation {{Attention Relationship Network}} ({{EARN}}) : {{A Geometric Deep Metric Framework}} for {{Learning Similar Math Expression Embedding}}},
  shorttitle = {Equation {{Attention Relationship Network}} ({{EARN}})},
  booktitle = {2020 25th {{International Conference}} on {{Pattern Recognition}} ({{ICPR}})},
  author = {Ahmed, Saleem and Davila, Kenny and Setlur, Srirangaraj and Govindaraju, Venu},
  year = {2021},
  month = jan,
  pages = {6282--6289},
  issn = {1051-4651},
  doi = {10.1109/ICPR48806.2021.9412619},
  abstract = {Representational Learning in the form of high dimensional embeddings have been used for multiple pattern recognition applications. There has been a significant interest in building embedding based systems for learning representations in the mathematical domain. At the same time, retrieval of structured information such as mathematical expressions is an important need for modern IR systems. In this work, our motivation is to introduce a robust framework for learning representations for similarity based retrieval of mathematical expressions. Given a query by example, the embedding can find the closest matching expression as a function of euclidean distance between them. We leverage recent advancements in image-based and graph-based deep learning algorithms to learn our similarity embeddings. We do this first, by using unimodal encoders in graph space and image space and then, a multi-modal combination of the same. To overcome the lack of training data, we force the networks to learn a deep metric using triplets generated with a heuristic scoring function. We also adopt a custom strategy for mining hard samples to train our neural networks. Our system produces rankings similar to those generated by the original scoring function, but using only a fraction of the time. Our results establish the viability of using such a multi-modal embedding for this task.},
  keywords = {Deep learning,Extraterrestrial measurements,Feature extraction,Force,Graph matching,Mathematical Information retrieval,Message passing,Neural networks,Semi-supervised learning,Training data},
  file = {/Users/nvk/Zotero/storage/5UIYRR6H/9412619.html}
}

@inproceedings{aizawaNTCIR10MathPilot2013,
  title = {{{NTCIR-10 Math Pilot Task Overview}}.},
  booktitle = {{{NTCIR}}},
  author = {Aizawa, Akiko and Kohlhase, Michael and Ounis, Iadh},
  year = {2013}
}

@article{aizawaNTCIR10MathPilot2013a,
  title = {{{NTCIR-10 Math Pilot Task Overview}}},
  author = {Aizawa, Akiko and Kohlhase, Michael and Ounis, Iadh},
  year = {2013},
  pages = {8},
  abstract = {This paper presents an overview of a new pilot task, the NTCIR Math Task, which is specifically dedicated to information access to mathematical content. In particular, the paper summarizes the subtasks addressed at the NTCIR Math Task as well as the main approaches deployed by the participating groups.},
  langid = {english},
  file = {/Users/nvk/Zotero/storage/YNGSN4G4/Aizawa et al. - 2013 - NTCIR-10 Math Pilot Task Overview.pdf}
}

@inproceedings{aizawaNTCIR11Math2Task2014,
  title = {{{NTCIR-11 Math-2 Task Overview}}.},
  booktitle = {{{NTCIR}}},
  author = {Aizawa, Akiko and Kohlhase, Michael and Ounis, Iadh and Schubotz, Moritz},
  year = {2014},
  volume = {11},
  pages = {88--98},
  file = {/Users/nvk/Zotero/storage/BWJGDIKV/Aizawa et al. - 2014 - NTCIR-11 Math-2 Task Overview..pdf}
}

@inproceedings{allamanisLearningContinuousSemantic2017,
  title = {Learning {{Continuous Semantic Representations}} of {{Symbolic Expressions}}},
  booktitle = {Proceedings of the 34th {{International Conference}} on {{Machine Learning}}},
  author = {Allamanis, Miltiadis and Chanthirasegaran, Pankajan and Kohli, Pushmeet and Sutton, Charles},
  year = {2017},
  month = jul,
  pages = {80--88},
  publisher = {{PMLR}},
  issn = {2640-3498},
  abstract = {Combining abstract, symbolic reasoning with continuous neural reasoning is a grand challenge of representation learning. As a step in this direction, we propose a new architecture, called neural equivalence network, for the problem of learning continuous semantic representations of algebraic and logical expressions. These networks are trained to represent semantic equivalence, even of expressions that are syntactically very different. The challenge is that semantic representations must be computed in a syntax-directed manner, because semantics is compositional, but at the same time, small changes in syntax can lead to very large changes in semantics, which can be difficult for continuous neural architectures. We perform an exhaustive evaluation on the task of checking equivalence on a highly diverse class of symbolic algebraic and boolean expression types, showing that our model significantly outperforms existing architectures.},
  langid = {english},
  file = {/Users/nvk/Zotero/storage/3PZ5I8W7/Allamanis et al. - 2017 - Learning Continuous Semantic Representations of Sy.pdf;/Users/nvk/Zotero/storage/8IWF6G82/Allamanis et al. - 2017 - Learning Continuous Semantic Representations of Sy.pdf}
}

@article{allamQuestionAnsweringSystems2012,
  title = {The {{Question Answering Systems}}: {{A Survey}}.},
  author = {Allam, Ali Mohamed Nabil and Haggag, Mohamed Hassan},
  year = {2012},
  volume = {2},
  number = {3},
  pages = {12},
  abstract = {Question Answering (QA) is a specialized area in the field of Information Retrieval (IR). The QA systems are concerned with providing relevant answers in response to questions proposed in natural language. QA is therefore composed of three distinct modules, each of which has a core component beside other supplementary components. These three core components are: question classification, information retrieval, and answer extraction. Question classification plays an essential role in QA systems by classifying the submitted question according to its type. Information retrieval is very important for question answering, because if no correct answers are present in a document, no further processing could be carried out to find an answer. Finally, answer extraction aims to retrieve the answer for a question asked by the user. This survey paper provides an overview of Question-Answering and its system architecture, as well as the previous related work comparing each research against the others with respect to the components that were covered and the approaches that were followed. At the end, the survey provides an analytical discussion of the proposed QA models, along with their main contributions, experimental results, and limitations.},
  langid = {english},
  file = {/Users/nvk/Zotero/storage/WWH3X3LU/Allam and Haggag - 2012 - The Question Answering Systems A Survey..pdf}
}

@article{alomHistoryBeganAlexNet2018,
  title = {The {{History Began}} from {{AlexNet}}: {{A Comprehensive Survey}} on {{Deep Learning Approaches}}},
  shorttitle = {The {{History Began}} from {{AlexNet}}},
  author = {Alom, Md Zahangir and Taha, Tarek M. and Yakopcic, Christopher and Westberg, Stefan and Sidike, Paheding and Nasrin, Mst Shamima and Van Esesn, Brian C. and Awwal, Abdul A. S. and Asari, Vijayan K.},
  year = {2018},
  month = mar,
  abstract = {Deep learning has demonstrated tremendous success in variety of application domains in the past few years. This new field of machine learning has been growing rapidly and applied in most of the application domains with some new modalities of applications, which helps to open new opportunity. There are different methods have been proposed on different category of learning approaches, which includes supervised, semi-supervised and un-supervised learning. The experimental results show state-of-the-art performance of deep learning over traditional machine learning approaches in the field of Image Processing, Computer Vision, Speech Recognition, Machine Translation, Art, Medical imaging, Medical information processing, Robotics and control, Bio-informatics, Natural Language Processing (NLP), Cyber security, and many more. This report presents a brief survey on development of DL approaches, including Deep Neural Network (DNN), Convolutional Neural Network (CNN), Recurrent Neural Network (RNN) including Long Short Term Memory (LSTM) and Gated Recurrent Units (GRU), Auto-Encoder (AE), Deep Belief Network (DBN), Generative Adversarial Network (GAN), and Deep Reinforcement Learning (DRL). In addition, we have included recent development of proposed advanced variant DL techniques based on the mentioned DL approaches. Furthermore, DL approaches have explored and evaluated in different application domains are also included in this survey. We have also comprised recently developed frameworks, SDKs, and benchmark datasets that are used for implementing and evaluating deep learning approaches. There are some surveys have published on Deep Learning in Neural Networks [1, 38] and a survey on RL [234]. However, those papers have not discussed the individual advanced techniques for training large scale deep learning models and the recently developed method of generative models [1].},
  langid = {english},
  file = {/Users/nvk/Zotero/storage/T2MZVDM8/Alom et al. - 2018 - The History Began from AlexNet A Comprehensive Su.pdf;/Users/nvk/Zotero/storage/3YA5IHKS/1803.html}
}

@inproceedings{andersonSyntaxdirectedRecognitionHandprinted1967,
  title = {Syntax-Directed Recognition of Hand-Printed Two-Dimensional Mathematics},
  booktitle = {Symposium on {{Interactive Systems}} for {{Experimental Applied Mathematics}}: {{Proceedings}} of the {{Association}} for {{Computing Machinery Inc}}. {{Symposium}}},
  author = {Anderson, Robert H.},
  year = {1967},
  month = aug,
  pages = {436--459},
  publisher = {{Association for Computing Machinery}},
  address = {{Washington, D.C.}},
  doi = {10.1145/2402536.2402585},
  abstract = {Research in the real-time recognition of hand-printed characters [1--5] offers the possibility of drawing mathematical expressions on a RAND Tablet [6] or similar input device, and obtaining a list of the characters and their positions in an x-, y-coordinate system. This paper discusses the use of a set of replacement rules to recognize, or "parse," such two-dimensional configurations of characters. The replacement rules might be considered to be a generalization of the context-free Backus Normal Form rules used to describe a class of syntaxes for character strings.},
  isbn = {978-1-4503-7309-8},
  file = {/Users/nvk/Zotero/storage/MPWRXX26/Anderson - 1967 - Syntax-directed recognition of hand-printed two-di.pdf}
}

@incollection{andersonTwoDimensionalMathematicalNotation1977,
  title = {Two-{{Dimensional Mathematical Notation}}},
  booktitle = {Syntactic {{Pattern Recognition}}, {{Applications}}},
  author = {Anderson, R. H.},
  editor = {Fu, King Sun},
  year = {1977},
  series = {Communication and {{Cybernetics}}},
  pages = {147--177},
  publisher = {{Springer}},
  address = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-642-66438-0_7},
  abstract = {This chapter discusses the use of coordinate grammars for the recognition of two-dimensional (2 D) mathematics notation. The discussion is based on material in [7.1\textendash 2], and includes complete grammars for the recognition of commonly used arithmetic and matrix math notation. The format of the coordinate grammar rules has been completely revised for this chapter from earlier versions in an effort to increase the readability of the grammar. A top-down parsing scheme is used to partition a two-dimensional character configuration into subproblems. Syntax rules contain all necessary partitioning instructions. The syntax-directed recognition system described here has been successfully demonstrated in several prototype systems, and could be used to provide real-time interpretation of 2 D math expressions handprinted on a data tablet as input to an interactive computing system.},
  isbn = {978-3-642-66438-0},
  langid = {english},
  keywords = {Recognition Algorithm,Replacement Rule,Syntactic Analysis,Syntactic Category,Terminal Symbol},
  file = {/Users/nvk/Zotero/storage/ALG7672C/Anderson - 1977 - Two-Dimensional Mathematical Notation.pdf}
}

@inproceedings{ankitPUMAProgrammableUltraefficient2019,
  title = {{{PUMA}}: {{A Programmable Ultra-efficient Memristor-based Accelerator}} for {{Machine Learning Inference}}},
  shorttitle = {{{PUMA}}},
  booktitle = {Proceedings of the {{Twenty-Fourth International Conference}} on {{Architectural Support}} for {{Programming Languages}} and {{Operating Systems}}},
  author = {Ankit, Aayush and Hajj, Izzat El and Chalamalasetti, Sai Rahul and Ndu, Geoffrey and Foltin, Martin and Williams, R. Stanley and Faraboschi, Paolo and Hwu, Wen-mei W and Strachan, John Paul and Roy, Kaushik and Milojicic, Dejan S.},
  year = {2019},
  month = apr,
  pages = {715--731},
  publisher = {{ACM}},
  address = {{Providence RI USA}},
  doi = {10.1145/3297858.3304049},
  abstract = {Memristor crossbars are circuits capable of performing analog matrix-vector multiplications, overcoming the fundamental energy efficiency limitations of digital logic. They have been shown to be effective in special-purpose accelerators for a limited set of neural network applications.},
  isbn = {978-1-4503-6240-5},
  langid = {english},
  file = {/Users/nvk/Zotero/storage/UEX4GWZ6/Ankit et al. - 2019 - PUMA A Programmable Ultra-efficient Memristor-bas.pdf}
}

@inproceedings{ankitRESPARCReconfigurableEnergyEfficient2017,
  title = {{{RESPARC}}: {{A Reconfigurable}} and {{Energy-Efficient Architecture}} with {{Memristive Crossbars}} for {{Deep Spiking Neural Networks}}},
  shorttitle = {{{RESPARC}}},
  booktitle = {Proceedings of the 54th {{Annual Design Automation Conference}} 2017},
  author = {Ankit, Aayush and Sengupta, Abhronil and Panda, Priyadarshini and Roy, Kaushik},
  year = {2017},
  month = jun,
  series = {{{DAC}} '17},
  pages = {1--6},
  publisher = {{Association for Computing Machinery}},
  address = {{Austin, TX, USA}},
  doi = {10.1145/3061639.3062311},
  abstract = {Neuromorphic computing using post-CMOS technologies is gaining immense popularity due to its promising abilities to address the memory and power bottlenecks in von-Neumann computing systems. In this paper, we propose RESPARC - a reconfigurable and energy efficient architecture built-on Memristive Crossbar Arrays (MCA) for deep Spiking Neural Networks (SNNs). Prior works were primarily focused on device and circuit implementations of SNNs on crossbars. RESPARC advances this by proposing a complete system for SNN acceleration and its subsequent analysis. RESPARC utilizes the energy-efficiency of MCAs for inner-product computation and realizes a hierarchical reconfigurable design to incorporate the data-flow patterns in an SNN in a scalable fashion. We evaluate the proposed architecture on different SNNs ranging in complexity from 2k-230k neurons and 1.2M-5.5M synapses. Simulation results on these networks show that compared to the baseline digital CMOS architecture, RESPARC achieves 500x (15x) efficiency in energy benefits at 300x (60x) higher throughput for multi-layer perceptrons (deep convolutional networks). Furthermore, RESPARC is a technology-aware architecture that maps a given SNN topology to the most optimized MCA size for the given crossbar technology.},
  isbn = {978-1-4503-4927-7},
  keywords = {Energy-Efficiency,Memristive Crossbars,Reconfigurablity,Spiking Neural Network},
  file = {/Users/nvk/Zotero/storage/L3AXSZRM/Ankit et al. - 2017 - RESPARC A Reconfigurable and Energy-Efficient Arc.pdf}
}

@article{aulamoOpusFilterConfigurableParallel2020,
  title = {{{OpusFilter}} : {{A Configurable Parallel Corpus Filtering Toolbox}}},
  shorttitle = {{{OpusFilter}}},
  author = {Aulamo, Mikko and Virpioja, Sami and Tiedemann, J{\"o}rg},
  year = {2020},
  month = jun,
  publisher = {{The Association for Computational Linguistics}},
  abstract = {This paper introduces OpusFilter, a flexible and modular toolbox for filtering parallel corpora. It implements a number of components based on heuristic filters, language identification libraries, character-based language models, and word alignment tools, and it can easily be extended with custom filters. Bitext segments can be ranked according to their quality or domain match using single features or a logistic regression model that can be trained without manually labeled training data. We demonstrate the effectiveness of OpusFilter on the example of a Finnish-English news translation task based on noisy web-crawled training data. Applying our tool leads to improved translation quality while significantly reducing the size of the training data, also clearly outperforming an alternative ranking given in the crawled data set. Furthermore, we show the ability of OpusFilter to perform data selection for domain adaptation.},
  copyright = {cc\_by},
  langid = {english},
  annotation = {Accepted: 2020-09-22T14:51:01Z},
  file = {/Users/nvk/Zotero/storage/NI769U75/Aulamo et al. - 2020 - OpusFilter  A Configurable Parallel Corpus Filter.pdf;/Users/nvk/Zotero/storage/BF7TH4LD/319556.html}
}

@misc{AutomatingScientificKnowledge,
  title = {Automating {{Scientific Knowledge Extraction}}},
  howpublished = {https://www.darpa.mil/program/automating-scientific-knowledge-extraction},
  file = {/Users/nvk/Zotero/storage/X5Z85DBK/automating-scientific-knowledge-extraction.html}
}

@misc{bahdanauNeuralMachineTranslation2016,
  title = {Neural {{Machine Translation}} by {{Jointly Learning}} to {{Align}} and {{Translate}}},
  author = {Bahdanau, Dzmitry and Cho, Kyunghyun and Bengio, Yoshua},
  year = {2016},
  month = may,
  number = {arXiv:1409.0473},
  eprint = {1409.0473},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  institution = {{arXiv}},
  doi = {10.48550/arXiv.1409.0473},
  abstract = {Neural machine translation is a recently proposed approach to machine translation. Unlike the traditional statistical machine translation, the neural machine translation aims at building a single neural network that can be jointly tuned to maximize the translation performance. The models proposed recently for neural machine translation often belong to a family of encoder-decoders and consists of an encoder that encodes a source sentence into a fixed-length vector from which a decoder generates a translation. In this paper, we conjecture that the use of a fixed-length vector is a bottleneck in improving the performance of this basic encoder-decoder architecture, and propose to extend this by allowing a model to automatically (soft-)search for parts of a source sentence that are relevant to predicting a target word, without having to form these parts as a hard segment explicitly. With this new approach, we achieve a translation performance comparable to the existing state-of-the-art phrase-based system on the task of English-to-French translation. Furthermore, qualitative analysis reveals that the (soft-)alignments found by the model agree well with our intuition.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Statistics - Machine Learning},
  file = {/Users/nvk/Zotero/storage/KXCU9SBG/Bahdanau et al. - 2016 - Neural Machine Translation by Jointly Learning to .pdf;/Users/nvk/Zotero/storage/JQYDGIQL/1409.html}
}

@article{balaidKnowledgeMapsSystematic2016,
  title = {Knowledge Maps: {{A}} Systematic Literature Review and Directions for Future Research},
  shorttitle = {Knowledge Maps},
  author = {Balaid, Ali and Abd Rozan, Mohd Zaidi and Hikmi, Syed Norris and Memon, Jamshed},
  year = {2016},
  month = jun,
  journal = {International Journal of Information Management},
  volume = {36},
  number = {3},
  pages = {451--475},
  issn = {0268-4012},
  doi = {10.1016/j.ijinfomgt.2016.02.005},
  abstract = {Context Nowadays the concept of knowledge mapping has attracted increased attention from scientists in a variety of academic disciplines and professional practice areas. Among the most important attributes of a knowledge map is its ability to increase communication and share common practices across an entire organisation. However, despite being a promising area for research, the knowledge maps community lacks a widespread understanding of the current state of the art. Objective The objective of this article is to explore the world of knowledge mapping by reviewing and analysing the current state of research and providing an overview of knowledge mapping's concepts, benefits, techniques, classifications and methodologies, which are precisely reviewed, and their features are highlighted. In addition, we offer directions for future research. Method Based on the systematic literature review method this study collects, synthesises, and analyses numerous articles on a variety of topics closely related to a knowledge map published from January 2000 to December 2013 on six electronic databases by following a pre-defined review protocol. The articles have been retrieved through a combination of automatic and manual search, hence extensive quantitative and qualitative results of the research are provided. Results From the review study, we identified 132 articles addressing knowledge maps that have been reviewed in order to extract relevant information on a set of research questions. We found a generally increasing level of activity during this 5-year period. We noted that while existing research covers a large number of studies on some disciplines, such as systems and tools development, it contains very few studies on other disciplines, such as knowledge maps adoption. To aid this situation, we offer directions for future research. Conclusions The results demonstrated that a knowledge map is an imperative strategy for increasing organisations' effectiveness. In addition, there is a need for more knowledge maps research.},
  langid = {english},
  keywords = {Knowledge management,Knowledge maps,Systematic literature review},
  file = {/Users/nvk/Zotero/storage/VN9SK9J5/S0268401216000098.html}
}

@inproceedings{baldwinLanguageIdentificationLong2010,
  title = {Language {{Identification}}: {{The Long}} and the {{Short}} of the {{Matter}}},
  shorttitle = {Language {{Identification}}},
  booktitle = {Human {{Language Technologies}}: {{The}} 2010 {{Annual Conference}} of the {{North American Chapter}} of the {{Association}} for {{Computational Linguistics}}},
  author = {Baldwin, Timothy and Lui, Marco},
  year = {2010},
  month = jun,
  pages = {229--237},
  publisher = {{Association for Computational Linguistics}},
  address = {{Los Angeles, California}},
  file = {/Users/nvk/Zotero/storage/E9TJ3EZT/Baldwin and Lui - 2010 - Language Identification The Long and the Short of.pdf}
}

@book{baldwinProceedingsEACL20092009,
  title = {Proceedings of the {{EACL}} 2009 {{Workshop}} on the {{Interaction}} between {{Linguistics}} and {{Computational Linguistics}}: {{Virtuous}}, {{Vicious}} or {{Vacuous}}?},
  shorttitle = {Proceedings of the {{EACL}} 2009 {{Workshop}} on the {{Interaction}} between {{Linguistics}} and {{Computational Linguistics}}},
  editor = {Baldwin, Timothy and Kordoni, Valia},
  year = {2009},
  month = mar,
  publisher = {{Association for Computational Linguistics}},
  address = {{Athens, Greece}},
  file = {/Users/nvk/Zotero/storage/FWF7IXYE/Baldwin and Kordoni - 2009 - Proceedings of the EACL 2009 Workshop on the Inter.pdf}
}

@book{bejanAdvancedEngineeringThermodynamics2016,
  title = {Advanced {{Engineering Thermodynamics}}},
  author = {Bejan, Adrian},
  year = {2016},
  month = sep,
  publisher = {{John Wiley \& Sons}},
  abstract = {An advanced, practical approach to the first and second laws of thermodynamics Advanced Engineering Thermodynamics bridges the gap between engineering applications and the first and second laws of thermodynamics. Going beyond the basic coverage offered by most textbooks, this authoritative treatment delves into the advanced topics of energy and work as they relate to various engineering fields. This practical approach describes real-world applications of thermodynamics concepts, including solar energy, refrigeration, air conditioning, thermofluid design, chemical design, constructal design, and more. This new fourth edition has been updated and expanded to include current developments in energy storage, distributed energy systems, entropy minimization, and industrial applications, linking new technologies in sustainability to fundamental thermodynamics concepts. Worked problems have been added to help students follow the thought processes behind various applications, and additional homework problems give them the opportunity to gauge their knowledge. The growing demand for sustainability and energy efficiency has shined a spotlight on the real-world applications of thermodynamics. This book helps future engineers make the fundamental connections, and develop a clear understanding of this complex subject.  Delve deeper into the engineering applications of thermodynamics Work problems directly applicable to engineering fields Integrate thermodynamics concepts into sustainability design and policy Understand the thermodynamics of emerging energy technologies  Condensed introductory chapters allow students to quickly review the fundamentals before diving right into practical applications. Designed expressly for engineering students, this book offers a clear, targeted treatment of thermodynamics topics with detailed discussion and authoritative guidance toward even the most complex concepts. Advanced Engineering Thermodynamics is the definitive modern treatment of energy and work for today's newest engineers.},
  googlebooks = {j0zSDAAAQBAJ},
  isbn = {978-1-119-05209-8},
  langid = {english},
  keywords = {Technology \& Engineering / Manufacturing,Technology \& Engineering / Mechanical}
}

@article{benderAchievingEvaluatingLanguageIndependence2011,
  title = {On {{Achieving}} and {{Evaluating Language-Independence}} in {{NLP}}},
  author = {Bender, Emily M.},
  year = {2011},
  journal = {Linguistic Issues in Language Technology},
  volume = {6},
  number = {0},
  issn = {1945-3604},
  abstract = {Language independence is commonly presented as one of the advantages  of modern, machine-learning approaches to NLP, and it is an important  type of scalability.  			In this position paper, I critically  review the widespread approaches to achieving and evaluating language  independence in the field of com- putational linguistics and argue that,  on the one hand, we are not truly evaluating language independence with  any systematicity and on the other hand, that truly  language-independent technology requires more linguistic sophistication  than is the norm.},
  langid = {english},
  keywords = {language independence,syntax},
  file = {/Users/nvk/Zotero/storage/8LEFG2KE/Bender - 2011 - On Achieving and Evaluating Language-Independence .pdf}
}

@misc{BenderRuleNamingLanguages2019,
  title = {The \#{{BenderRule}}: {{On Naming}} the {{Languages We Study}} and {{Why It Matters}}},
  shorttitle = {The \#{{BenderRule}}},
  year = {2019},
  month = sep,
  journal = {The Gradient},
  abstract = {Progress in the field of Natural Language Processing (NLP) depends on the existence of language resources:},
  howpublished = {https://thegradient.pub/the-benderrule-on-naming-the-languages-we-study-and-why-it-matters/},
  langid = {english},
  file = {/Users/nvk/Zotero/storage/3UD8DJYY/the-benderrule-on-naming-the-languages-we-study-and-why-it-matters.html}
}

@book{bengioNeuralProbabilisticLanguage,
  title = {Neural {{Probabilistic Language Models}}},
  author = {Bengio, Yoshua and Schwenk, Holger and Sen{\'e}cal, Jean-S{\'e}bastien and Morin, Fr{\'e}deric and Gauvain, Jean-Luc},
  journal = {Innovations in Machine Learning},
  issn = {1434-9922, 1860-0808},
  doi = {10.1007/3-540-33486-6_6},
  abstract = {{$<$}p{$>$}A central goal of statistical language modeling is to learn the joint probability function of sequences of words in a language. This is intrinsically difficult because of the \textbf{curse of dimensionality}: a word sequence on which the model will be tested is likely to be different from all the word sequences seen during training. Traditional but very successful approaches based on n-grams obtain generalization by concatenating very short overlapping sequences seen in the training set. We propose to fight the curse of dimensionality by \textbf{learning a distributed representation for words} which allows each training sentence to inform the model about an exponential number of semantically neighboring sentences. Generalization is obtained because a sequence of words that has never been seen before gets high probability if it is made of words that are similar (in the sense of having a nearby representation) to words forming an already seen sentence. Training such large models (with millions of parameters) within a reasonable time is itself a significant challenge. We report on several methods to speed-up both training and probability computation, as well as comparative experiments to evaluate the improvements brought by these techniques. We finally describe the incorporation of this new language model into a state-of-the-art speech recognizer of conversational speech.{$<$}/p{$>$}},
  isbn = {978-3-540-30609-2 978-3-540-33486-6},
  langid = {english},
  file = {/Users/nvk/Zotero/storage/R6A57NA4/bwmeta1.element.html}
}

@article{bengioNeuralProbabilisticLanguage2003,
  title = {A {{Neural Probabilistic Language Model}}},
  author = {Bengio, Yoshua and Ducharme, R{\'e}jean and Vincent, Pascal and Jauvin, Christian},
  year = {2003},
  month = feb,
  pages = {19},
  abstract = {A goal of statistical language modeling is to learn the joint probability function of sequences of words in a language. This is intrinsically difficult because of the curse of dimensionality: a word sequence on which the model will be tested is likely to be different from all the word sequences seen during training. Traditional but very successful approaches based on n-grams obtain generalization by concatenating very short overlapping sequences seen in the training set. We propose to fight the curse of dimensionality by learning a distributed representation for words which allows each training sentence to inform the model about an exponential number of semantically neighboring sentences. The model learns simultaneously (1) a distributed representation for each word along with (2) the probability function for word sequences, expressed in terms of these representations. Generalization is obtained because a sequence of words that has never been seen before gets high probability if it is made of words that are similar (in the sense of having a nearby representation) to words forming an already seen sentence. Training such large models (with millions of parameters) within a reasonable time is itself a significant challenge. We report on experiments using neural networks for the probability function, showing on two text corpora that the proposed approach significantly improves on state-of-the-art n-gram models, and that the proposed approach allows to take advantage of longer contexts.},
  langid = {english},
  file = {/Users/nvk/Zotero/storage/KDRZVTTK/Bengio et al. - A Neural Probabilistic Language Model.pdf}
}

@inproceedings{bermanOpticalCharacterRecognition1994,
  title = {Optical Character Recognition for Typeset Mathematics},
  booktitle = {Proceedings of the International Symposium on {{Symbolic}} and Algebraic Computation},
  author = {Berman, Benjamin P. and Fateman, Richard J.},
  year = {1994},
  month = aug,
  series = {{{ISSAC}} '94},
  pages = {348--353},
  publisher = {{Association for Computing Machinery}},
  address = {{Oxford, United Kingdom}},
  doi = {10.1145/190347.190438},
  abstract = {There is a wealth of mathematical knowledge that could be potentially very useful in many computational applications, but is not available in electronic form. This knowledge comes in the form of mechanically typeset books and journals going back more than a hundred years. Besides these older sources, there are a great many current publications, filled with useful mathematical information, which are difficult if not impossible to obtain in electronic form. What we would like to do is extract character information from these documents, which could then be passed to higher-level parsing routines for further extraction of mathematical content (or any other useful 2-dimensional semantic content). Unfortunately, current commercial OCR (optical character recognition) software packages are quite unable to handle mathematical formulas, since their algorithms at all levels use heuristics developed for other document styles. We are concerned with the development of OCR methods that are able to handle this specialized task of mathematical expression recognition.},
  isbn = {978-0-89791-638-7},
  file = {/Users/nvk/Zotero/storage/M88TSXEF/Berman and Fateman - 1994 - Optical character recognition for typeset mathemat.pdf}
}

@article{bernerModernMathematicsDeep2021,
  title = {The {{Modern Mathematics}} of {{Deep Learning}}},
  author = {Berner, Julius and Grohs, Philipp and Kutyniok, Gitta and Petersen, Philipp},
  year = {2021},
  month = may,
  journal = {arXiv:2105.04026 [cs, stat]},
  eprint = {2105.04026},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {We describe the new field of mathematical analysis of deep learning. This field emerged around a list of research questions that were not answered within the classical framework of learning theory. These questions concern: the outstanding generalization power of overparametrized neural networks, the role of depth in deep architectures, the apparent absence of the curse of dimensionality, the surprisingly successful optimization performance despite the non-convexity of the problem, understanding what features are learned, why deep architectures perform exceptionally well in physical problems, and which fine aspects of an architecture affect the behavior of a learning task in which way. We present an overview of modern approaches that yield partial answers to these questions. For selected approaches, we describe the main ideas in more detail.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/nvk/Zotero/storage/VBK5TGGG/Berner et al. - 2021 - The Modern Mathematics of Deep Learning.pdf;/Users/nvk/Zotero/storage/JZLTQYFY/2105.html}
}

@article{berryMatricesVectorSpaces1999,
  title = {Matrices, {{Vector Spaces}}, and {{Information Retrieval}}},
  author = {Berry, Michael W. and Drmac, Zlatko and Jessup, Elizabeth R.},
  year = {1999},
  month = jan,
  journal = {SIAM Review},
  volume = {41},
  number = {2},
  pages = {335--362},
  publisher = {{Society for Industrial and Applied Mathematics}},
  issn = {0036-1445},
  doi = {10.1137/S0036144598347035},
  abstract = {The evolution of digital libraries and the Internet has dramatically transformed the processing, storage, and retrieval of information. Efforts to digitize text, images, video, and audio now consume a substantial portion of both academic and industrial activity. Even when there is no shortage of textual materials on a particular topic, procedures for indexing or extracting the knowledge or conceptual information contained in them can be lacking. Recently developed information retrieval technologies are based on the concept of a vector space. Data are modeled as a matrix, and a user's query of the database is represented as a vector. Relevant documents in the database are then identified via simple vector operations. Orthogonal factorizations of the matrix provide mechanisms for handling uncertainty in the database itself. The purpose of this paper is to show how such fundamental mathematical concepts from linear algebra can be used to manage and index large text collections.},
  keywords = {15-01,15A03,15A18,65F50,68P20,information retrieval,linear algebra,QR factorization,singular value decomposition,vector spaces},
  file = {/Users/nvk/Zotero/storage/IBZY7Z2X/Berry et al. - 1999 - Matrices, Vector Spaces, and Information Retrieval.pdf}
}

@article{bersukerReliableRRAMPerformance2017,
  title = {Toward Reliable {{RRAM}} Performance: Macro- and Micro-Analysis of Operation Processes},
  shorttitle = {Toward Reliable {{RRAM}} Performance},
  author = {Bersuker, Gennadi and Veksler, Dmitry and Nminibapiel, David M. and Shrestha, Pragya R. and Campbell, Jason P. and Ryan, Jason T. and Baumgart, Helmut and Mason, Maribeth S. and Cheung, Kin P.},
  year = {2017},
  month = dec,
  journal = {Journal of Computational Electronics},
  volume = {16},
  number = {4},
  pages = {1085--1094},
  issn = {1572-8137},
  doi = {10.1007/s10825-017-1105-5},
  abstract = {Resistive random access memory (RRAM) technology promises superior performance and scalability while employing well-developed fabrication processes. Conductance in insulating oxides employed in RRAM devices can be strongly affected by atomic-level changes that makes cell switching properties extremely sensitive to operation conditions inducing local structural modifications. This opens an opportunity to condition the memory cell stack by forming a conductive filament capable of high frequency, low energy switching. Certain materials with pre-existing conductive paths, in particular some polycrystalline oxides, like hafnia, are shown to respond well to this approach. For this class of materials, the concept of ultra-fast pulse technique as an ultimate method for assessing RRAM switching capabilities in circuitry operations is discussed. Hafnia-based cells demonstrate compliance-free (1R) forming with no current overshoot, low operation currents, and reduced variability.},
  langid = {english},
  file = {/Users/nvk/Zotero/storage/JMRM7XKW/Bersuker et al. - 2017 - Toward reliable RRAM performance macro- and micro.pdf}
}

@inproceedings{bialeckiApacheLucene2012,
  title = {Apache Lucene 4},
  booktitle = {{{SIGIR}} 2012 Workshop on Open Source Information Retrieval},
  author = {Bia{\l}ecki, Andrzej and Muir, Robert and Ingersoll, Grant and Imagination, Lucid},
  year = {2012},
  pages = {17}
}

@book{bialeckiApacheLucene2012a,
  title = {Apache {{Lucene}} 4},
  author = {Bia{\l}ecki, Andrzej and Muir, Robert and Ingersoll, Grant},
  year = {2012},
  month = aug,
  pages = {24},
  abstract = {Apache Lucene is a modern, open source search library designed to provide both relevant results as well as high performance. Furthermore, Lucene has undergone significant change over the years, starting as a one-person project to one of the leading search solutions available. Lucene is used in a vast range of applications from mobile devices and desktops through Internet scale solutions. The evolution of Lucene has been quite dramatic at times, none more so than in the current release of Lucene 4.0. This paper presents both an overview of Lucene's features as well as details on its community development model, architecture and implementation, including coverage of its indexing and scoring capabilities.},
  isbn = {978-0-473-22026-6}
}

@misc{bjorckUnderstandingDecoupledEarly2020,
  title = {Understanding {{Decoupled}} and {{Early Weight Decay}}},
  author = {Bjorck, Johan and Weinberger, Kilian and Gomes, Carla},
  year = {2020},
  month = dec,
  number = {arXiv:2012.13841},
  eprint = {2012.13841},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  institution = {{arXiv}},
  abstract = {Weight decay (WD) is a traditional regularization technique in deep learning, but despite its ubiquity, its behavior is still an area of active research. Golatkar et al. have recently shown that WD only matters at the start of the training in computer vision, upending traditional wisdom. Loshchilov et al. show that for adaptive optimizers, manually decaying weights can outperform adding an \$l\_2\$ penalty to the loss. This technique has become increasingly popular and is referred to as decoupled WD. The goal of this paper is to investigate these two recent empirical observations. We demonstrate that by applying WD only at the start, the network norm stays small throughout training. This has a regularizing effect as the effective gradient updates become larger. However, traditional generalizations metrics fail to capture this effect of WD, and we show how a simple scale-invariant metric can. We also show how the growth of network weights is heavily influenced by the dataset and its generalization properties. For decoupled WD, we perform experiments in NLP and RL where adaptive optimizers are the norm. We demonstrate that the primary issue that decoupled WD alleviates is the mixing of gradients from the objective function and the \$l\_2\$ penalty in the buffers of Adam (which stores the estimates of the first-order moment). Adaptivity itself is not problematic and decoupled WD ensures that the gradients from the \$l\_2\$ term cannot "drown out" the true objective, facilitating easier hyperparameter tuning.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/nvk/Zotero/storage/BQMNEK8N/Bjorck et al. - 2020 - Understanding Decoupled and Early Weight Decay.pdf;/Users/nvk/Zotero/storage/F9YC4B4P/2012.html}
}

@article{blodgettDemographicDialectalVariation2016,
  title = {Demographic {{Dialectal Variation}} in {{Social Media}}: {{A Case Study}} of {{African-American English}}},
  shorttitle = {Demographic {{Dialectal Variation}} in {{Social Media}}},
  author = {Blodgett, Su Lin and Green, Lisa and O'Connor, Brendan},
  year = {2016},
  month = aug,
  journal = {arXiv:1608.08868 [cs]},
  eprint = {1608.08868},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {Though dialectal language is increasingly abundant on social media, few resources exist for developing NLP tools to handle such language. We conduct a case study of dialectal language in online conversational text by investigating African-American English (AAE) on Twitter. We propose a distantly supervised model to identify AAE-like language from demographics associated with geo-located messages, and we verify that this language follows well-known AAE linguistic phenomena. In addition, we analyze the quality of existing language identification and dependency parsing tools on AAE-like text, demonstrating that they perform poorly on such text compared to text associated with white speakers. We also provide an ensemble classifier for language identification which eliminates this disparity and release a new corpus of tweets containing AAE-like language.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/nvk/Zotero/storage/PFI85GXY/Blodgett et al. - 2016 - Demographic Dialectal Variation in Social Media A.pdf;/Users/nvk/Zotero/storage/UEN8RM4U/1608.html}
}

@article{blodgettDemographicDialectalVariation2016a,
  title = {Demographic {{Dialectal Variation}} in {{Social Media}}: {{A Case Study}} of {{African-American English}}},
  shorttitle = {Demographic {{Dialectal Variation}} in {{Social Media}}},
  author = {Blodgett, Su Lin and Green, Lisa and O'Connor, Brendan},
  year = {2016},
  month = aug,
  journal = {arXiv:1608.08868 [cs]},
  eprint = {1608.08868},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {Though dialectal language is increasingly abundant on social media, few resources exist for developing NLP tools to handle such language. We conduct a case study of dialectal language in online conversational text by investigating African-American English (AAE) on Twitter. We propose a distantly supervised model to identify AAE-like language from demographics associated with geo-located messages, and we verify that this language follows well-known AAE linguistic phenomena. In addition, we analyze the quality of existing language identification and dependency parsing tools on AAE-like text, demonstrating that they perform poorly on such text compared to text associated with white speakers. We also provide an ensemble classifier for language identification which eliminates this disparity and release a new corpus of tweets containing AAE-like language.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/nvk/Zotero/storage/5QNL2EQG/Blodgett et al. - 2016 - Demographic Dialectal Variation in Social Media A.pdf;/Users/nvk/Zotero/storage/IZ9ASS6X/1608.html}
}

@article{blodgettRacialDisparityNatural2017,
  title = {Racial {{Disparity}} in {{Natural Language Processing}}: {{A Case Study}} of {{Social Media African-American English}}},
  shorttitle = {Racial {{Disparity}} in {{Natural Language Processing}}},
  author = {Blodgett, Su Lin and O'Connor, Brendan},
  year = {2017},
  month = jun,
  journal = {arXiv:1707.00061 [cs]},
  eprint = {1707.00061},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {We highlight an important frontier in algorithmic fairness: disparity in the quality of natural language processing algorithms when applied to language from authors of different social groups. For example, current systems sometimes analyze the language of females and minorities more poorly than they do of whites and males. We conduct an empirical analysis of racial disparity in language identification for tweets written in African-American English, and discuss implications of disparity in NLP.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Computers and Society},
  file = {/Users/nvk/Zotero/storage/WMT334I8/Blodgett and O'Connor - 2017 - Racial Disparity in Natural Language Processing A.pdf;/Users/nvk/Zotero/storage/D9QHV8GW/1707.html}
}

@article{bohnerAnalyticalSolutionsBlack2009,
  title = {On Analytical Solutions of the {{Black}}\textendash{{Scholes}} Equation},
  author = {Bohner, Martin and Zheng, Yao},
  year = {2009},
  month = mar,
  journal = {Applied Mathematics Letters},
  volume = {22},
  number = {3},
  pages = {309--313},
  issn = {0893-9659},
  doi = {10.1016/j.aml.2008.04.002},
  abstract = {This work presents a theoretical analysis for the Black\textendash Scholes equation. Given a terminal condition, the analytical solution of the Black\textendash Scholes equation is obtained by using the Adomian approximate decomposition technique. The mathematical technique employed in this work also has significance in studying some other problems in finance theory.},
  langid = {english},
  keywords = {Adomian decomposition,Analytical solution,Blackâ€“Scholes equation},
  file = {/Users/nvk/Zotero/storage/82EFT7QZ/S0893965908001493.html}
}

@article{bojanowskiEnrichingWordVectors2017,
  title = {Enriching {{Word Vectors}} with {{Subword Information}}},
  author = {Bojanowski, Piotr and Grave, Edouard and Joulin, Armand and Mikolov, Tomas},
  year = {2017},
  journal = {Transactions of the Association for Computational Linguistics},
  volume = {5},
  pages = {135--146},
  doi = {10.1162/tacl_a_00051},
  abstract = {Continuous word representations, trained on large unlabeled corpora are useful for many natural language processing tasks. Popular models that learn such representations ignore the morphology of words, by assigning a distinct vector to each word. This is a limitation, especially for languages with large vocabularies and many rare words. In this paper, we propose a new approach based on the skipgram model, where each word is represented as a bag of character n-grams. A vector representation is associated to each character n-gram; words being represented as the sum of these representations. Our method is fast, allowing to train models on large corpora quickly and allows us to compute word representations for words that did not appear in the training data. We evaluate our word representations on nine different languages, both on word similarity and analogy tasks. By comparing to recently proposed morphological word representations, we show that our vectors achieve state-of-the-art performance on these tasks.},
  file = {/Users/nvk/Zotero/storage/VB8N6SYL/Bojanowski et al. - 2017 - Enriching Word Vectors with Subword Information.pdf}
}

@misc{booksTermRewritingProgrammingLanguages,
  title = {Term-{{Rewriting Programming Languages}}: {{Mathematica}}, {{Clean}}, {{Maude System}}, {{Refal}}, {{Pure}}, {{Q}}, {{Stratego}}]Xt, {{Asf}}+sdf {{Meta Environment}} by {{LLC Books}} | 9781155838960 | {{Reviews}}, {{Description}} and {{More}} @ {{BetterWorldBooks}}.Com},
  shorttitle = {Term-{{Rewriting Programming Languages}}},
  author = {Books, Better World},
  journal = {Better World Books},
  abstract = {Shop our inventory for Term-Rewriting Programming Languages: Mathematica, Clean, Maude System, Refal, Pure, Q, Stratego]xt, Asf+sdf Meta Environment by LLC Books with fast free shipping on every used book we have in stock!},
  howpublished = {https://www.betterworldbooks.com/product/detail/term-rewriting-programming-languages-mathematica-clean-maude-system-refal-pure-q-stratego-xt-asf-1155838963},
  file = {/Users/nvk/Zotero/storage/5Z6JYBWT/term-rewriting-programming-languages-mathematica-clean-maude-system-refal-pure-q-stratego-xt-as.html}
}

@inproceedings{borbinhaProjectEuDMLFirst2011,
  title = {Project {{EuDML}} \textendash{} {{A First Year Demonstration}}},
  booktitle = {Intelligent {{Computer Mathematics}}},
  author = {Borbinha, Jos{\'e} and Bouche, Thierry and Nowi{\'n}ski, Aleksander and Sojka, Petr},
  editor = {Davenport, James H. and Farmer, William M. and Urban, Josef and Rabe, Florian},
  year = {2011},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {281--284},
  publisher = {{Springer}},
  address = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-642-22673-1_21},
  abstract = {This demonstration describes the results of the first year of the EuDML project, an initiative building a new multilingual service for searching and browsing the content of existing European portals of mathematical content. We demonstrate the first versions and proofs of concept of the EuDML portal, its contents' aggregator, and a toolset for added value.},
  isbn = {978-3-642-22673-1},
  langid = {english},
  file = {/Users/nvk/Zotero/storage/9XICPPGI/Borbinha et al. - 2011 - Project EuDML â€“ A First Year Demonstration.pdf}
}

@article{borodinovDeepNeuralNetworks2019,
  title = {Deep Neural Networks for Understanding Noisy Data Applied to Physical Property Extraction in Scanning Probe Microscopy},
  author = {Borodinov, Nikolay and Neumayer, Sabine and Kalinin, Sergei V. and Ovchinnikova, Olga S. and Vasudevan, Rama K. and Jesse, Stephen},
  year = {2019},
  month = feb,
  journal = {npj Computational Materials},
  volume = {5},
  number = {1},
  pages = {1--8},
  publisher = {{Nature Publishing Group}},
  issn = {2057-3960},
  doi = {10.1038/s41524-019-0148-5},
  abstract = {The rapid development of spectral-imaging methods in scanning probe, electron, and optical microscopy in the last decade have given rise for large multidimensional datasets. In many cases, the reduction of hyperspectral data to the lower-dimension materials-specific parameters is based on functional fitting, where an approximate form of the fitting function is known, but the parameters of the function need to be determined. However, functional fits of noisy data realized via iterative methods, such as least-square gradient descent, often yield spurious results and are very sensitive to initial guesses. Here, we demonstrate an approach for the reduction of the hyperspectral data using a deep neural network approach. A combined deep neural network/least-square approach is shown to improve the effective signal-to-noise ratio of band-excitation piezoresponse force microscopy by more than an order of magnitude, allowing characterization when very small driving signals are used or when a material's response is weak.},
  copyright = {2019 This is a U.S. government work and not under copyright protection in the U.S.; foreign copyright protection may apply},
  langid = {english},
  file = {/Users/nvk/Zotero/storage/3PXGTXXA/Borodinov et al. - 2019 - Deep neural networks for understanding noisy data .pdf;/Users/nvk/Zotero/storage/GGY5KUUP/s41524-019-0148-5.html}
}

@article{borodinovMachineLearningbasedMultidomain2020,
  title = {Machine Learning-Based Multidomain Processing for Texture-Based Image Segmentation and Analysis},
  author = {Borodinov, Nikolay and Tsai, Wan-Yu and Korolkov, Vladimir V. and Balke, Nina and Kalinin, Sergei V. and Ovchinnikova, Olga S.},
  year = {2020},
  month = jan,
  journal = {Applied Physics Letters},
  volume = {116},
  number = {4},
  pages = {044103},
  publisher = {{American Institute of Physics}},
  issn = {0003-6951},
  doi = {10.1063/1.5135328},
  abstract = {Atomic and molecular resolved atomic force microscopy (AFM) images offer unique insights into materials' properties such as local ordering, molecular orientation, and topological defects, which can be used to pinpoint physical and chemical interactions occurring at the surface. Utilizing machine learning for extracting underlying physical parameters increases the throughput of AFM data processing and eliminates inconsistencies intrinsic to manual image analysis, thus enabling the creation of reliable frameworks for qualitative and quantitative evaluation of experimental data. Here, we present a robust and scalable approach to the segmentation of AFM images based on flexible pre-selected classification criteria. The usage of supervised learning and feature extraction allows us to retain the consideration of specific problem-dependent features (such as types of periodical structures observed in the images and the associated numerical parameters: spacing, orientation, etc.). We highlight the applicability of this approach for the segmentation of molecular resolved AFM images based on the crystal orientation of the observed domains, automated selection of boundaries, and collection of relevant statistics. Overall, we outline a general strategy for machine learning-enabled analysis of nanoscale systems exhibiting periodic order that could be applied to any analytical imaging technique.},
  file = {/Users/nvk/Zotero/storage/3N5ZSW39/Borodinov et al. - 2020 - Machine learning-based multidomain processing for .pdf}
}

@inproceedings{brantZUMAOpenFPGA2012,
  title = {{{ZUMA}}: {{An Open FPGA Overlay Architecture}}},
  shorttitle = {{{ZUMA}}},
  booktitle = {2012 {{IEEE}} 20th {{International Symposium}} on {{Field-Programmable Custom Computing Machines}}},
  author = {Brant, Alexander and Lemieux, Guy G.F.},
  year = {2012},
  month = apr,
  pages = {93--96},
  doi = {10.1109/FCCM.2012.25},
  abstract = {This paper presents the ZUMA open FPGA overlay architecture. It is an open-source, cross-compatible embedded FPGA architecture that is intended to overlay on top of an existing FPGA, in essence an ''FPGA-on-an-FPGA.'' This approach has a number of benefits, including bitstream compatibility between different vendors and parts, compatibility with open FPGA tool Hows, and the ability to embed some programmable logic into systems on FPGAs without the need for releasing or recompiling the master netlist. These options can enhance design possibilities and improve designer productivity. Previous attempts to map an FPGA architecture into a commercial FPGA have had an area penalty of 100x at best [4]. Through careful architectural and implementation choices to exploit low-level elements of the host architecture, ZUMA reduces this penalty to as low as 40x. Using the VTR (VPR6) academic tool How, we have been able to compile the entire MCNC benchmark suite to ZUMA. We invite authors of other tool Hows to target ZUMA.},
  keywords = {bitstream compatibility,commercial FPGA,Design automation,designer productivity,embedded systems,field programmable gate arrays,Field programmable gate arrays,FPGA-on-an-FPGA,Hardware design languages,low-level elements,master netlist,MCNC benchmark suite,open FPGA tool,open-source cross-compatible embedded FPGA architecture,productivity,programmable logic,programmable logic devices,public domain software,reconfigurable architectures,Reconfigurable architectures,Routing,Switches,Table lookup,VPR6 academic tool,VTR academic tool,ZUMA open FPGA overlay architecture},
  file = {/Users/nvk/Zotero/storage/9WA9GI34/Brant and Lemieux - 2012 - ZUMA An Open FPGA Overlay Architecture.pdf;/Users/nvk/Zotero/storage/P6ZND3A6/6239797.html}
}

@article{bretzelBarnettEffectThin2009,
  title = {Barnett Effect in Thin Magnetic Films and Nanostructures},
  author = {Bretzel, Stefan and Bauer, Gerrit E. W. and Tserkovnyak, Yaroslav and Brataas, Arne},
  year = {2009},
  month = sep,
  journal = {Applied Physics Letters},
  volume = {95},
  number = {12},
  pages = {122504},
  publisher = {{American Institute of Physics}},
  issn = {0003-6951},
  doi = {10.1063/1.3232221},
  abstract = {The Barnett effect refers to the magnetization induced by rotation of a demagnetized ferromagnet. We describe the location and stability of stationary states in rotating nanostructures using the Landau\textendash Lifshitz\textendash Gilbert equation. The conditions for an experimental observation of the Barnett effect in different materials and sample geometries are discussed.},
  file = {/Users/nvk/Zotero/storage/EWMVPT7Y/Bretzel et al. - 2009 - Barnett effect in thin magnetic films and nanostru.pdf;/Users/nvk/Zotero/storage/GL6RG2XX/1.html}
}

@inproceedings{buckleyRetrievalEvaluationIncomplete2004,
  title = {Retrieval Evaluation with Incomplete Information},
  booktitle = {Proceedings of the 27th Annual International {{ACM SIGIR}} Conference on {{Research}} and Development in Information Retrieval},
  author = {Buckley, Chris and Voorhees, Ellen M.},
  year = {2004},
  month = jul,
  series = {{{SIGIR}} '04},
  pages = {25--32},
  publisher = {{Association for Computing Machinery}},
  address = {{New York, NY, USA}},
  doi = {10.1145/1008992.1009000},
  abstract = {This paper examines whether the Cranfield evaluation methodology is robust to gross violations of the completeness assumption (i.e., the assumption that all relevant documents within a test collection have been identified and are present in the collection). We show that current evaluation measures are not robust to substantially incomplete relevance judgments. A new measure is introduced that is both highly correlated with existing measures when complete judgments are available and more robust to incomplete judgment sets. This finding suggests that substantially larger or dynamic test collections built using current pooling practices should be viable laboratory tools, despite the fact that the relevance information will be incomplete and imperfect.},
  isbn = {978-1-58113-881-8},
  keywords = {cranfield,incomplete judgments},
  file = {/Users/nvk/Zotero/storage/XNJXI6PX/Buckley and Voorhees - 2004 - Retrieval evaluation with incomplete information.pdf}
}

@article{carbonellLectureNotesArtificial,
  title = {Lecture {{Notes}} in {{Artificial Intelligence}}},
  author = {Carbonell, Edited J G and Siekmann, J},
  pages = {418},
  langid = {english},
  file = {/Users/nvk/Zotero/storage/RTBJASXQ/Carbonell and Siekmann - Lecture Notes in Artificial Intelligence.pdf}
}

@inproceedings{caretteReviewMathematicalKnowledge2009,
  title = {A {{Review}} of {{Mathematical Knowledge Management}}},
  booktitle = {Intelligent {{Computer Mathematics}}},
  author = {Carette, Jacques and Farmer, William M.},
  editor = {Carette, Jacques and Dixon, Lucas and Coen, Claudio Sacerdoti and Watt, Stephen M.},
  year = {2009},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {233--246},
  publisher = {{Springer}},
  address = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-642-02614-0_21},
  abstract = {Mathematical Knowledge Management (MKM), as a field, has seen tremendous growth in the last few years. This period was one where many research threads were started and the field was defining itself. We believe that we are now in a position to use the MKM body of knowledge as a means to define what MKM is, what it worries about, etc. In this paper, we review the literature of MKM and gather various metadata from these papers. After offering some definitions surrounding MKM, we analyze the metadata we have gathered from these papers, in an effort to cast more light on the field of MKM and its evolution.},
  isbn = {978-3-642-02614-0},
  langid = {english},
  keywords = {Interactive View,Logical Theory,Mathematical Document,Mathematical Knowledge,Relevance Ranking},
  file = {/Users/nvk/Zotero/storage/69Z57CZW/Carette and Farmer - 2009 - A Review of Mathematical Knowledge Management.pdf}
}

@article{castelluccioMusicGenomeProject2006,
  title = {The Music Genome Project},
  author = {Castelluccio, Michael},
  year = {2006},
  month = dec,
  journal = {Strategic Finance},
  pages = {57--59},
  publisher = {{Institute of Management Accountants}},
  issn = {1524833X},
  langid = {english}
}

@article{castelluccioMusicGenomeProject2006a,
  title = {The Music Genome Project},
  author = {Castelluccio, Michael},
  year = {2006},
  month = dec,
  journal = {Strategic Finance},
  pages = {57--59},
  publisher = {{Institute of Management Accountants}},
  issn = {1524833X},
  langid = {english},
  file = {/Users/nvk/Zotero/storage/A27RYS9B/i.html}
}

@book{catheySchaumOutlineElectronic2002,
  title = {Schaum's {{Outline}} of {{Electronic Devices}} and {{Circuits}}, {{Second Edition}}},
  author = {Cathey, Jimmie J.},
  year = {2002},
  month = feb,
  publisher = {{McGraw Hill Professional}},
  abstract = {This updated version of its internationally popular predecessor provides and introductory problem-solved text for understanding fundamental concepts of electronic devices, their design, and their circuitry. Providing an interface with Pspice, the most widely used program in electronics, new key features include a new chapter presenting the basics of switched mode power supplies, thirty-one new examples, and twenty-three PS solved problems.},
  googlebooks = {Qi\_ThWLYjaMC},
  isbn = {978-0-07-139830-5},
  langid = {english},
  keywords = {Study Aids / Tests (Other)}
}

@inproceedings{cavnarNGramBasedTextCategorization1994,
  title = {N-{{Gram-Based Text Categorization}}},
  booktitle = {In {{Proceedings}} of {{SDAIR-94}}, 3rd {{Annual Symposium}} on {{Document Analysis}} and {{Information Retrieval}}},
  author = {Cavnar, William and Trenkle, John M.},
  year = {1994},
  pages = {161--175},
  abstract = {Text categorization is a fundamental task in document processing, allowing the automated handling of enormous streams of documents in electronic form. One difficulty in handling some classes of documents is the presence of different kinds of textual errors, such as spelling and grammatical errors in email, and character recognition errors in documents that come through OCR. Text categorization must work reliably on all input, and thus must tolerate some level of these kinds of problems. We describe here an N-gram-based approach to text categorization that is tolerant of textual errors. The system is small, fast and robust. This system worked very well for language classification, achieving in one test a 99.8\% correct classification rate on Usenet newsgroup articles written in different languages. The system also worked reasonably well for classifying articles from a number of different computer-oriented newsgroups according to subject, achieving as high as an 80\% correct classification...},
  file = {/Users/nvk/Zotero/storage/7MD2NJKN/Cavnar and Trenkle - 1994 - N-Gram-Based Text Categorization.pdf;/Users/nvk/Zotero/storage/56642PRM/summary.html}
}

@article{chanErrorDetectionError2001,
  title = {Error Detection, Error Correction and Performance Evaluation in on-Line Mathematical Expression Recognition},
  author = {Chan, Kam-Fai and Yeung, Dit-Yan},
  year = {2001},
  month = aug,
  journal = {Pattern Recognition},
  volume = {34},
  number = {8},
  pages = {1671--1684},
  issn = {0031-3203},
  doi = {10.1016/S0031-3203(00)00102-3},
  abstract = {Automatic recognition of on-line mathematical expressions is difficult especially when there exist errors. In this paper, we incorporate an error detection and correction mechanism into a parser developed previously by us based on definite clause grammar (DCG). The resulting system can handle lexical, syntactic and some semantic errors. The recognition speed for 600 commonly seen expressions is quite acceptable, ranging from 0.73 to 6s per expression on a modest workstation. In addition, we propose a performance evaluation scheme which can be used to demonstrate the effectiveness of both the symbol recognition and structural analysis stages by a single measure.},
  langid = {english},
  keywords = {Definite clause grammar,Error detection and correction,Error-correcting parsing,Hierarchical decomposition parsing,On-line mathematical expression recognition,Performance evaluation,Structural analysis},
  file = {/Users/nvk/Zotero/storage/5K42TG7W/Chan and Yeung - 2001 - Error detection, error correction and performance .pdf;/Users/nvk/Zotero/storage/JHCEFTXD/S0031320300001023.html}
}

@article{changMethodStructuralAnalysis1970,
  title = {A Method for the Structural Analysis of Two-Dimensional Mathematical Expressions},
  author = {Chang, Shi-Kuo},
  year = {1970},
  month = jul,
  journal = {Information Sciences},
  volume = {2},
  number = {3},
  pages = {253--272},
  issn = {0020-0255},
  doi = {10.1016/S0020-0255(70)80052-4},
  abstract = {A structure specification scheme is described which can be used to specify the structures of certain two-dimensional patterns. Algorithms are developed to test whether a pattern has a (strongly) well-formed structure with respect to a given structure specification scheme. This method is applicable to the analysis of two-dimensional mathematical expressions and the format of printed material. The usefulness of this method is limited to the analysis of patterns whose structures are based upon a number of operators. When it is applicable, a strongly well-formed structure can be constructed in time n2, where n is the number of primitive components of the pattern.},
  langid = {english},
  file = {/Users/nvk/Zotero/storage/MGELA2C6/Chang - 1970 - A method for the structural analysis of two-dimens.pdf;/Users/nvk/Zotero/storage/J7Z4Z9TY/S0020025570800524.html}
}

@misc{ChapterQuickIntroduction,
  title = {Chapter~1.~{{Quick Introduction}} to {{Term Rewriting}}},
  howpublished = {https://homepages.cwi.nl/\textasciitilde daybuild/daily-books/extraction-transformation/term-rewriting/term-rewriting.html},
  file = {/Users/nvk/Zotero/storage/YBT8XEAN/term-rewriting.html}
}

@misc{chartonLearningAdvancedMathematical2021,
  title = {Learning Advanced Mathematical Computations from Examples},
  author = {Charton, Fran{\c c}ois and Hayat, Amaury and Lample, Guillaume},
  year = {2021},
  month = mar,
  number = {arXiv:2006.06462},
  eprint = {2006.06462},
  eprinttype = {arxiv},
  primaryclass = {cs},
  institution = {{arXiv}},
  doi = {10.48550/arXiv.2006.06462},
  abstract = {Using transformers over large generated datasets, we train models to learn mathematical properties of differential systems, such as local stability, behavior at infinity and controllability. We achieve near perfect prediction of qualitative characteristics, and good approximations of numerical features of the system. This demonstrates that neural networks can learn to perform complex computations, grounded in advanced theory, from examples, without built-in mathematical knowledge.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/Users/nvk/Zotero/storage/5H4W59M7/Charton et al. - 2021 - Learning advanced mathematical computations from e.pdf;/Users/nvk/Zotero/storage/KCWMXFP7/2006.html}
}

@article{chemboTheoryApplicationsLugiatoLefever2017,
  title = {Theory and Applications of the {{Lugiato-Lefever Equation}}},
  author = {Chembo, Yanne K. and Gomila, Dami{\`a} and Tlidi, Mustapha and Menyuk, Curtis R.},
  year = {2017},
  month = nov,
  journal = {The European Physical Journal D},
  volume = {71},
  number = {11},
  pages = {299, epjd/e2017-80572-0},
  issn = {1434-6060, 1434-6079},
  doi = {10.1140/epjd/e2017-80572-0},
  langid = {english},
  file = {/Users/nvk/Zotero/storage/MUEC9CRG/Chembo et al. - 2017 - Theory and applications of the Lugiato-Lefever Equ.pdf}
}

@inproceedings{chenEfficientEffectivePacking2014,
  title = {Efficient and Effective Packing and Analytical Placement for Large-Scale Heterogeneous {{FPGAs}}},
  booktitle = {2014 {{IEEE}}/{{ACM International Conference}} on {{Computer-Aided Design}} ({{ICCAD}})},
  author = {Chen, Yu-Chen and Chen, Sheng-Yen and Chang, Yao-Wen},
  year = {2014},
  month = nov,
  pages = {647--654},
  issn = {1558-2434},
  doi = {10.1109/ICCAD.2014.7001421},
  abstract = {As FPGA architecture evolves, complex heterogenous blocks, such as RAMs and DSPs, are widely used to effectively implement various circuit applications. These complex blocks often consist of datapath-intensive circuits, which are not adequately addressed in existing packing and placement algorithms. Besides, scalability has become a first-order metric for modern FPGA design, mainly due to the dramatically increasing design complexity. This paper presents efficient and effective packing and analytical placement algorithms for large-scale heterogeneous FPGAs to deal with issues on heterogeneity, datapath regularity, and scalability. Compared to the well-known academic tool VPR, experimental results show that our packing and placement algorithms achieve respective 199.80X and 3.07X speedups with better wirelength, and our overall flow achieves 50\% shorter wirelength, with an 18.30X overall speedup.},
  keywords = {Algorithm design and analysis,analytical placement algorithm,circuit reliability,Clustering algorithms,complex heterogenous blocks,datapath regularity,datapath-intensive circuits,design complexity,Digital signal processing,effective packing,field programmable gate arrays,Field programmable gate arrays,FPGA design,large-scale heterogeneous FPGA architecture,logic design,Optimization,Random access memory,reconfigurable architectures,scalability,Scalability,VPR academic tool},
  file = {/Users/nvk/Zotero/storage/XGNLUMRP/Chen et al. - 2014 - Efficient and effective packing and analytical pla.pdf;/Users/nvk/Zotero/storage/T3UGHWE5/7001421.html}
}

@article{chenEvolutionConditionalDispersal2008,
  title = {Evolution of Conditional Dispersal: A Reaction\textendash Diffusion\textendash Advection Model},
  shorttitle = {Evolution of Conditional Dispersal},
  author = {Chen, Xinfu and Hambrock, Richard and Lou, Yuan},
  year = {2008},
  month = sep,
  journal = {Journal of Mathematical Biology},
  volume = {57},
  number = {3},
  pages = {361--386},
  issn = {1432-1416},
  doi = {10.1007/s00285-008-0166-2},
  abstract = {To study evolution of conditional dispersal, a Lotka\textendash Volterra reaction\textendash diffusion\textendash advection model for two competing species in a heterogeneous environment is proposed and investigated. The two species are assumed to be identical except their dispersal strategies: both species disperse by random diffusion and advection along environmental gradients, but one species has stronger biased movement (i.e., advection along the environmental gradients) than the other one. It is shown that at least two scenarios can occur: if only one species has a strong tendency to move upward the environmental gradients, the two species can coexist since one species mainly pursues resources at places of locally most favorable environments while the other relies on resources from other parts of the habitat; if both species have such strong biased movements, it can lead to overcrowding of the whole population at places of locally most favorable environments, which causes the extinction of the species with stronger biased movement. These results provide a new mechanism for the coexistence of competing species, and they also imply that selection is against excessive advection along environmental gradients, and an intermediate biased movement rate may evolve.},
  langid = {english},
  file = {/Users/nvk/Zotero/storage/G7VCZZCQ/Chen et al. - 2008 - Evolution of conditional dispersal a reactionâ€“dif.pdf}
}

@article{chenNeuroSimCircuitLevelMacro2018,
  title = {{{NeuroSim}}: {{A Circuit-Level Macro Model}} for {{Benchmarking Neuro-Inspired Architectures}} in {{Online Learning}}},
  shorttitle = {{{NeuroSim}}},
  author = {Chen, Pai-Yu and Peng, Xiaochen and Yu, Shimeng},
  year = {2018},
  month = dec,
  journal = {IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems},
  volume = {37},
  number = {12},
  pages = {3067--3080},
  issn = {1937-4151},
  doi = {10.1109/TCAD.2018.2789723},
  abstract = {Neuro-inspired architectures based on synaptic memory arrays have been proposed for on-chip acceleration of weighted sum and weight update in machine/deep learning algorithms. In this paper, we developed NeuroSim, a circuit-level macro model that estimates the area, latency, dynamic energy, and leakage power to facilitate the design space exploration of neuro-inspired architectures with mainstream and emerging device technologies. NeuroSim provides flexible interface and a wide variety of design options at the circuit and device level. Therefore, NeuroSim can be used by neural networks (NNs) as a supporting tool to provide circuit-level performance evaluation. With NeuroSim, an integrated framework can be built with hierarchical organization from the device level (synaptic device properties) to the circuit level (array architectures) and then to the algorithm level (NN topology), enabling instruction-accurate evaluation on the learning accuracy as well as the circuit-level performance metrics at the run-time of online learning. Using multilayer perceptron as a case-study algorithm, we investigated the impact of the ``analog'' emerging nonvolatile memory (eNVM)'s ``nonideal'' device properties and benchmarked the tradeoffs between SRAM, digital, and analog eNVM-based architectures for online learning and offline classification.},
  keywords = {Algorithm design and analysis,array architectures,Artificial neural networks,benchmarking neuro-inspired architectures,circuit level,circuit-level macro model,circuit-level performance evaluation,circuit-level performance metrics,Computer architecture,design space exploration,device level performance evaluation,emerging device technologies,Emerging nonvolatile memory (eNVM),eNVM,Integrated circuit modeling,learning (artificial intelligence),learning accuracy,machine learning,machine/deep learning algorithms,Microprocessors,multilayer perceptron,multilayer perceptrons,neural network (NN),neuromorphic computing,Neuromorphics,NeuroSim model,nonvolatile memory,offline classification,online learning,random-access storage,SRAM chips,synaptic device properties,synaptic devices,synaptic memory arrays},
  file = {/Users/nvk/Zotero/storage/NDSU8RSP/Chen et al. - 2018 - NeuroSim A Circuit-Level Macro Model for Benchmar.pdf;/Users/nvk/Zotero/storage/679995SP/8246561.html}
}

@article{choudharyPhysicsenhancedNeuralNetworks2020,
  title = {Physics-Enhanced Neural Networks Learn Order and Chaos},
  author = {Choudhary, Anshul and Lindner, John F. and Holliday, Elliott G. and Miller, Scott T. and Sinha, Sudeshna and Ditto, William L.},
  year = {2020},
  month = jun,
  journal = {Physical Review E},
  volume = {101},
  number = {6},
  pages = {062207},
  publisher = {{American Physical Society}},
  doi = {10.1103/PhysRevE.101.062207},
  abstract = {Artificial neural networks are universal function approximators. They can forecast dynamics, but they may need impractically many neurons to do so, especially if the dynamics is chaotic. We use neural networks that incorporate Hamiltonian dynamics to efficiently learn phase space orbits even as nonlinear systems transition from order to chaos. We demonstrate Hamiltonian neural networks on a widely used dynamics benchmark, the H\'enon-Heiles potential, and on nonperturbative dynamical billiards. We introspect to elucidate the Hamiltonian neural network forecasting.},
  file = {/Users/nvk/Zotero/storage/XW25IDPZ/Choudhary et al. - 2020 - Physics-enhanced neural networks learn order and c.pdf;/Users/nvk/Zotero/storage/742H4H6T/PhysRevE.101.html}
}

@article{churchWord2Vec2017,
  title = {{{Word2Vec}}},
  author = {Church, Kenneth Ward},
  year = {2017},
  month = jan,
  journal = {Natural Language Engineering},
  volume = {23},
  number = {1},
  pages = {155--162},
  publisher = {{Cambridge University Press}},
  issn = {1351-3249, 1469-8110},
  doi = {10.1017/S1351324916000334},
  abstract = {My last column ended with some comments about Kuhn and word2vec. Word2vec has racked up plenty of citations because it satisifies both of Kuhn's conditions for emerging trends: (1) a few initial (promising, if not convincing) successes that motivate early adopters (students) to do more, as well as (2) leaving plenty of room for early adopters to contribute and benefit by doing so. The fact that Google has so much to say on `How does word2vec work' makes it clear that the definitive answer to that question has yet to be written. It also helps citation counts to distribute code and data to make it that much easier for the next generation to take advantage of the opportunities (and cite your work in the process).},
  langid = {english},
  file = {/Users/nvk/Zotero/storage/PC2E57SF/Church - 2017 - Word2Vec.pdf;/Users/nvk/Zotero/storage/CNE4F3A2/B84AE4446BD47F48847B4904F0B36E0B.html}
}

@article{cohlDiscoveringMathematicalObjects2020,
  title = {Discovering {{Mathematical Objects}} of {{Interest}} - {{A Study}} of {{Mathematical Notations}}},
  author = {Cohl, Howard S. and Petter, Andre Greiner and Schubotz, Moritz and Breitinger, Corinna and Muller, Fabian and Aizawa, Akiko and Gipp, Bela},
  year = {2020},
  month = mar,
  abstract = {Mathematical notation, i.e., the writing system used to communicate concepts in mathematics, encodes valuable information for a variety of information search an},
  langid = {english},
  annotation = {Last Modified: 2021-05-04T09:33-04:00},
  file = {/Users/nvk/Zotero/storage/35QMLQI3/discovering-mathematical-objects-interest-study-mathematical-notations.html}
}

@inproceedings{collobertUnifiedArchitectureNatural2008,
  title = {A Unified Architecture for Natural Language Processing: Deep Neural Networks with Multitask Learning},
  shorttitle = {A Unified Architecture for Natural Language Processing},
  booktitle = {Proceedings of the 25th International Conference on {{Machine}} Learning},
  author = {Collobert, Ronan and Weston, Jason},
  year = {2008},
  month = jul,
  series = {{{ICML}} '08},
  pages = {160--167},
  publisher = {{Association for Computing Machinery}},
  address = {{New York, NY, USA}},
  doi = {10.1145/1390156.1390177},
  abstract = {We describe a single convolutional neural network architecture that, given a sentence, outputs a host of language processing predictions: part-of-speech tags, chunks, named entity tags, semantic roles, semantically similar words and the likelihood that the sentence makes sense (grammatically and semantically) using a language model. The entire network is trained jointly on all these tasks using weight-sharing, an instance of multitask learning. All the tasks use labeled data except the language model which is learnt from unlabeled text and represents a novel form of semi-supervised learning for the shared tasks. We show how both multitask learning and semi-supervised learning improve the generalization of the shared tasks, resulting in state-of-the-art-performance.},
  isbn = {978-1-60558-205-4},
  file = {/Users/nvk/Zotero/storage/9FDCBV9V/Collobert and Weston - 2008 - A unified architecture for natural language proces.pdf}
}

@misc{communicationsGrowBasicTranslational,
  title = {Grow {{Basic}} and {{Translational Research Impact}}},
  author = {and Communications, Grainger Engineering Office of Marketing},
  abstract = {Grow Basic and Translational Research Impact},
  howpublished = {https://grainger.illinois.edu/about/strategic-plan/grow-research-impact},
  langid = {english},
  file = {/Users/nvk/Zotero/storage/6ZBRR7IN/grow-research-impact.html}
}

@misc{ComputerInformationScience,
  title = {Computer and {{Information Science}} and {{Engineering}} : {{Core Programs}}},
  shorttitle = {Computer and {{Information Science}} and {{Engineering}}},
  journal = {Beta site for NSF - National Science Foundation},
  howpublished = {https://beta.nsf.gov/funding/opportunities/computer-and-information-science-and-engineering-core-programs},
  langid = {english}
}

@misc{CostEstimator,
  title = {Cost {{Estimator}}},
  howpublished = {https://patientportal.pmmconline.com/carle/}
}

@misc{CostEstimatora,
  title = {Cost {{Estimator}}},
  howpublished = {https://patientportal.pmmconline.com/carle/},
  file = {/Users/nvk/Zotero/storage/5SRY47YQ/carle.html}
}

@inproceedings{cruzSupervisedTextRank2006,
  title = {Supervised {{TextRank}}},
  booktitle = {Advances in {{Natural Language Processing}}},
  author = {Cruz, Ferm{\'i}n and Troyano, Jos{\'e} A. and Enr{\'i}quez, Fernando},
  editor = {Salakoski, Tapio and Ginter, Filip and Pyysalo, Sampo and Pahikkala, Tapio},
  year = {2006},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {632--639},
  publisher = {{Springer}},
  address = {{Berlin, Heidelberg}},
  doi = {10.1007/11816508_63},
  abstract = {In this paper we investigate how to adapt the TextRank method to make it work in a supervised way. TextRank is a graph based method that applies the ideas of the ranking algorithm used in Google (PageRank) to Natural Language Processing (NLP) tasks. This approach has given very good results in many NLP tasks like text summarization, keyword extraction or word sense disambiguation. In all these tasks TextRank operates in an unsupervised way, without using any training corpus. Our main contribution is the definition of a method that allows to apply TextRank to a graph that includes information generated from a training tagged corpus. We have tested our method with the Part of Speech (POS) tagging task, comparing the results with those obtained with tools specialized in this task. The performance of our system is quite near to these tools, improving the results of two of them when the corpus tagset is big and therefore the tagging task more complicated.},
  isbn = {978-3-540-37336-0},
  langid = {english},
  keywords = {Natural Language Processing,Ranking Algorithm,Training Corpus,Unknown Word,Word Sense Disambiguation},
  file = {/Users/nvk/Zotero/storage/9WL4D2I7/Cruz et al. - 2006 - Supervised TextRank.pdf}
}

@article{culpepperResearchFrontiersInformation2018,
  title = {Research {{Frontiers}} in {{Information Retrieval}}: {{Report}} from the {{Third Strategic Workshop}} on {{Information Retrieval}} in {{Lorne}} ({{SWIRL}} 2018)},
  shorttitle = {Research {{Frontiers}} in {{Information Retrieval}}},
  author = {Culpepper, J. Shane and Diaz, Fernando and Smucker, Mark D.},
  year = {2018},
  month = aug,
  journal = {ACM SIGIR Forum},
  volume = {52},
  number = {1},
  pages = {34--90},
  issn = {0163-5840},
  doi = {10.1145/3274784.3274788},
  abstract = {The purpose of the Strategic Workshop in Information Retrieval in Lorne is to explore the long-range issues of the Information Retrieval field, to recognize challenges that are on \textendash{} or even over \textendash{} the horizon, to build consensus on some of the key challenges, and to disseminate the resulting information to the research community. The intent is that this description of open problems will help to inspire researchers and graduate students to address the questions, and will provide funding agencies data to focus and coordinate support for information retrieval research.},
  langid = {english},
  file = {/Users/nvk/Zotero/storage/5KBV7N2E/Culpepper et al. - 2018 - Research Frontiers in Information Retrieval Repor.pdf}
}

@misc{dascoliDeepSymbolicRegression2022,
  title = {Deep {{Symbolic Regression}} for {{Recurrent Sequences}}},
  author = {{d'Ascoli}, St{\'e}phane and Kamienny, Pierre-Alexandre and Lample, Guillaume and Charton, Fran{\c c}ois},
  year = {2022},
  month = jan,
  number = {arXiv:2201.04600},
  eprint = {2201.04600},
  eprinttype = {arxiv},
  primaryclass = {cs},
  institution = {{arXiv}},
  doi = {10.48550/arXiv.2201.04600},
  abstract = {Symbolic regression, i.e. predicting a function from the observation of its values, is well-known to be a challenging task. In this paper, we train Transformers to infer the function or recurrence relation underlying sequences of integers or floats, a typical task in human IQ tests which has hardly been tackled in the machine learning literature. We evaluate our integer model on a subset of OEIS sequences, and show that it outperforms built-in Mathematica functions for recurrence prediction. We also demonstrate that our float model is able to yield informative approximations of out-of-vocabulary functions and constants, e.g. \$\textbackslash operatorname\{bessel0\}(x)\textbackslash approx \textbackslash frac\{\textbackslash sin(x)+\textbackslash cos(x)\}\{\textbackslash sqrt\{\textbackslash pi x\}\}\$ and \$1.644934\textbackslash approx \textbackslash pi\^2/6\$. An interactive demonstration of our models is provided at https://bit.ly/3niE5FS.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning},
  file = {/Users/nvk/Zotero/storage/NAMFESKP/d'Ascoli et al. - 2022 - Deep Symbolic Regression for Recurrent Sequences.pdf;/Users/nvk/Zotero/storage/42MAHED2/2201.html}
}

@misc{DatasetanalyticsJupyterNotebook,
  title = {Dataset-Analytics - {{Jupyter Notebook}}},
  howpublished = {http://localhost:8888/notebooks/equation-embeddings/dataset-analytics.ipynb},
  file = {/Users/nvk/Zotero/storage/9FHE293N/dataset-analytics.html}
}

@article{davilaChartMiningSurvey2020,
  title = {Chart {{Mining}}: {{A Survey}} of {{Methods}} for {{Automated Chart Analysis}}},
  shorttitle = {Chart {{Mining}}},
  author = {Davila, Kenny and Setlur, Srirangaraj and Doermann, David and Bhargava, Urala Kota and Govindaraju, Venu},
  year = {2020},
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  pages = {1--1},
  issn = {0162-8828, 2160-9292, 1939-3539},
  doi = {10.1109/TPAMI.2020.2992028}
}

@inproceedings{davilaLayoutSemanticsCombining2017,
  title = {Layout and {{Semantics}}: {{Combining Representations}} for {{Mathematical Formula Search}}},
  shorttitle = {Layout and {{Semantics}}},
  booktitle = {Proceedings of the 40th {{International ACM SIGIR Conference}} on {{Research}} and {{Development}} in {{Information Retrieval}}},
  author = {Davila, Kenny and Zanibbi, Richard},
  year = {2017},
  month = aug,
  series = {{{SIGIR}} '17},
  pages = {1165--1168},
  publisher = {{Association for Computing Machinery}},
  address = {{New York, NY, USA}},
  doi = {10.1145/3077136.3080748},
  abstract = {Math-aware search engines need to support formulae in queries. Mathematical expressions are typically represented as trees defining their operational semantics or visual layout. We propose searching both formula representations using a three-layer model. The first layer selects candidates using spectral matching over tree node pairs. The second layer aligns a query with candidates and computes similarity scores based on structural matching. In the third layer, similarity scores are combined using linear regression. The two representations are combined using retrieval in parallel indices and regression over similarity scores. For NTCIR-12 Wikipedia Formula Browsing task relevance rankings, we see each layer increasing ranking quality and improved results when combining representations as measured by Bpref and nDCG scores.},
  isbn = {978-1-4503-5022-8},
  keywords = {formula retrieval,operator tree,symbol layout tree},
  file = {/Users/nvk/Zotero/storage/SGH3TT8K/Davila and Zanibbi - 2017 - Layout and Semantics Combining Representations fo.pdf}
}

@inproceedings{davilaLayoutSemanticsCombining2017a,
  title = {Layout and {{Semantics}}: {{Combining Representations}} for {{Mathematical Formula Search}}},
  shorttitle = {Layout and {{Semantics}}},
  booktitle = {Proceedings of the 40th {{International ACM SIGIR Conference}} on {{Research}} and {{Development}} in {{Information Retrieval}}},
  author = {Davila, Kenny and Zanibbi, Richard},
  year = {2017},
  month = aug,
  series = {{{SIGIR}} '17},
  pages = {1165--1168},
  publisher = {{Association for Computing Machinery}},
  address = {{New York, NY, USA}},
  doi = {10.1145/3077136.3080748},
  abstract = {Math-aware search engines need to support formulae in queries. Mathematical expressions are typically represented as trees defining their operational semantics or visual layout. We propose searching both formula representations using a three-layer model. The first layer selects candidates using spectral matching over tree node pairs. The second layer aligns a query with candidates and computes similarity scores based on structural matching. In the third layer, similarity scores are combined using linear regression. The two representations are combined using retrieval in parallel indices and regression over similarity scores. For NTCIR-12 Wikipedia Formula Browsing task relevance rankings, we see each layer increasing ranking quality and improved results when combining representations as measured by Bpref and nDCG scores.},
  isbn = {978-1-4503-5022-8},
  keywords = {formula retrieval,operator tree,symbol layout tree},
  file = {/Users/nvk/Zotero/storage/QB62TK4J/Davila and Zanibbi - 2017 - Layout and Semantics Combining Representations fo.pdf}
}

@article{davilaTangent3NTCIR12MathIR,
  title = {Tangent-3 at theâ€¨ {{NTCIR-12 MathIR Task}}},
  author = {Davila, Kenny and Zanibbi, Richard and Kane, Andrew and Tompa, Frank Wm},
  pages = {23},
  langid = {english},
  file = {/Users/nvk/Zotero/storage/9ZJLLMNS/storagestoragestoragestoragestoragestoragestoragestoragestoragestoragestoragestoragestoragestoragestoragestoragestoragestoragestoragestoragestoragestoragestoragestoragestoragestoragestoragestoragestoragestorag.pdf}
}

@inproceedings{davilaTangentVMathFormula2019,
  title = {Tangent-{{V}}: {{Math Formula Image Search Using Line-of-Sight Graphs}}},
  shorttitle = {Tangent-{{V}}},
  booktitle = {Advances in {{Information Retrieval}}},
  author = {Davila, Kenny and Joshi, Ritvik and Setlur, Srirangaraj and Govindaraju, Venu and Zanibbi, Richard},
  year = {2019},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {681--695},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-030-15712-8_44},
  abstract = {We present a visual search engine for graphics such as math, chemical diagrams, and figures. Graphics are represented using Line-of-Sight (LOS) graphs, with symbols connected only when they can `see' each other along an unobstructed line. Symbol identities may be provided (e.g., in PDF) or taken from Optical Character Recognition applied to images. Graphics are indexed by pairs of symbols that `see' each other using their labels, spatial displacement, and size ratio. Retrieval has two layers: the first matches query symbol pairs in an inverted index, while the second aligns candidates with the query and scores the resulting matches using the identity and relative position of symbols. For PDFs, we also introduce a new tool that quickly extracts characters and their locations. We have applied our model to the NTCIR-12 Wikipedia Formula Browsing Task, and found that the method can locate relevant matches without unification of symbols or using a math expression grammar. In the future, one might index LOS graphs for entire pages and search for text and graphics. Our source code has been made publicly available.},
  isbn = {978-3-030-15712-8},
  langid = {english},
  keywords = {Graphics search,Image search,Mathematical Information Retrieval (MIR),PDF symbol extraction},
  file = {/Users/nvk/Zotero/storage/WNQYLZ3F/Davila et al. - 2019 - Tangent-V Math Formula Image Search Using Line-of.pdf}
}

@article{davisUseDeepLearning2019,
  title = {The {{Use}} of {{Deep Learning}} for {{Symbolic Integration}}: {{A Review}} of ({{Lample}} and {{Charton}}, 2019)},
  shorttitle = {The {{Use}} of {{Deep Learning}} for {{Symbolic Integration}}},
  author = {Davis, Ernest},
  year = {2019},
  month = dec,
  journal = {arXiv:1912.05752 [cs]},
  eprint = {1912.05752},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {Lample and Charton (2019) describe a system that uses deep learning technology to compute symbolic, indefinite integrals, and to find symbolic solutions to first- and second-order ordinary differential equations, when the solutions are elementary functions. They found that, over a particular test set, the system could find solutions more successfully than sophisticated packages for symbolic mathematics such as Mathematica run with a long time-out. This is an impressive accomplishment, as far as it goes. However, the system can handle only a quite limited subset of the problems that Mathematica deals with, and the test set has significant built-in biases. Therefore the claim that this outperforms Mathematica on symbolic integration needs to be very much qualified.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning},
  file = {/Users/nvk/Zotero/storage/W6K6Q8BG/Davis - 2019 - The Use of Deep Learning for Symbolic Integration.pdf;/Users/nvk/Zotero/storage/7NJUBW8C/1912.html}
}

@article{dawPhysicsguidedNeuralNetworks2021,
  title = {Physics-Guided {{Neural Networks}} ({{PGNN}}): {{An Application}} in {{Lake Temperature Modeling}}},
  shorttitle = {Physics-Guided {{Neural Networks}} ({{PGNN}})},
  author = {Daw, Arka and Karpatne, Anuj and Watkins, William and Read, Jordan and Kumar, Vipin},
  year = {2021},
  month = sep,
  journal = {arXiv:1710.11431 [physics, stat]},
  eprint = {1710.11431},
  eprinttype = {arxiv},
  primaryclass = {physics, stat},
  abstract = {This paper introduces a framework for combining scientific knowledge of physics-based models with neural networks to advance scientific discovery. This framework, termed physics-guided neural networks (PGNN), leverages the output of physics-based model simulations along with observational features in a hybrid modeling setup to generate predictions using a neural network architecture. Further, this framework uses physics-based loss functions in the learning objective of neural networks to ensure that the model predictions not only show lower errors on the training set but are also scientifically consistent with the known physics on the unlabeled set. We illustrate the effectiveness of PGNN for the problem of lake temperature modeling, where physical relationships between the temperature, density, and depth of water are used to design a physics-based loss function. By using scientific knowledge to guide the construction and learning of neural networks, we are able to show that the proposed framework ensures better generalizability as well as scientific consistency of results. All the code and datasets used in this study have been made available on this link \textbackslash url\{https://github.com/arkadaw9/PGNN\}.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Physics - Data Analysis; Statistics and Probability,Statistics - Machine Learning},
  file = {/Users/nvk/Zotero/storage/3MNEX9T2/Daw et al. - 2021 - Physics-guided Neural Networks (PGNN) An Applicat.pdf;/Users/nvk/Zotero/storage/DRIQ23PN/1710.html}
}

@misc{DearColleagueLetter,
  title = {Dear {{Colleague Letter}}: {{Encouraging Research}} on {{Open Knowledge Networks}} (Nsf22017) | {{NSF}} - {{National Science Foundation}}},
  howpublished = {https://www.nsf.gov/pubs/2022/nsf22017/nsf22017.jsp?WT.mc\_ev=click\&WT.mc\_id=USNSF\_27\&utm\_medium=email\&utm\_source=govdelivery},
  file = {/Users/nvk/Zotero/storage/BRA4TFC4/nsf22017.html}
}

@misc{DearColleagueLettera,
  title = {Dear {{Colleague Letter}}: {{Research Collaboration Opportunity}} in {{Europe}} for {{NSF Awardees}} (Nsf22056) | {{NSF}} - {{National Science Foundation}}},
  howpublished = {https://www.nsf.gov/pubs/2022/nsf22056/nsf22056.jsp?WT.mc\_ev=click\&WT.mc\_id=USNSF\_29\&utm\_medium=email\&utm\_source=govdelivery},
  file = {/Users/nvk/Zotero/storage/HXPBDETC/nsf22056.html}
}

@misc{DearColleagueLetterb,
  title = {Dear {{Colleague Letter}}: {{Computer}} and {{Information Science}} and {{Engineering Graduate Fellowships}} ({{CSGrad4US}}) (Nsf22061) | {{NSF}} - {{National Science Foundation}}},
  howpublished = {https://www.nsf.gov/pubs/2022/nsf22061/nsf22061.jsp?WT.mc\_ev=click\&WT.mc\_id=USNSF\_27\&utm\_medium=email\&utm\_source=govdelivery},
  file = {/Users/nvk/Zotero/storage/B32FN7QE/nsf22061.html}
}

@misc{DeepDifferentialSystem,
  title = {â€ª{{Deep}} Differential System Stability\textendash Learning Advanced Computations from Examplesâ€¬},
  abstract = {â€ªF Charton, A Hayat, G Lampleâ€¬, â€ªPreprint at https://arxiv. org/pdf/2006.06462. pdf, 2020â€¬ - â€ªCited by 4â€¬},
  howpublished = {https://scholar.google.com/citations?view\_op=view\_citation\&hl=en\&user=H7sVDmIAAAAJ\&sortby=pubdate\&citation\_for\_view=H7sVDmIAAAAJ:M3NEmzRMIkIC}
}

@misc{DeepWalkProceedings20th,
  title = {{{DeepWalk}} | {{Proceedings}} of the 20th {{ACM SIGKDD}} International Conference on {{Knowledge}} Discovery and Data Mining},
  howpublished = {https://dl.acm.org/doi/abs/10.1145/2623330.2623732?casa\_token=xc4fD9BDfaUAAAAA:5\_NcI0ysoosSTBxhezV35kpVZ7Ys42DvIfMOlHw\_MLq12-AlAdiX4D0xGAsfLBxynUhbX0vJyjNgVw},
  file = {/Users/nvk/Zotero/storage/7G87FLJS/2623330.html}
}

@article{dehakCosineSimilarityScoring,
  title = {Cosine {{Similarity Scoring}} without {{Score Normalization Techniques}}},
  author = {Dehak, Najim and Dehak, Reda and Glass, James and Reynolds, Douglas and Kenny, Patrick},
  pages = {5},
  abstract = {In recent work [1], a simplified and highly effective approach to speaker recognition based on the cosine similarity between lowdimensional vectors, termed ivectors, defined in a total variability space was introduced. The total variability space representation is motivated by the popular Joint Factor Analysis (JFA) approach, but does not require the complication of estimating separate speaker and channel spaces and has been shown to be less dependent on score normalization procedures, such as znorm and t-norm. In this paper, we introduce a modification to the cosine similarity that does not require explicit score normalization, relying instead on simple mean and covariance statistics from a collection of impostor speaker ivectors. By avoiding the complication of z- and t-norm, the new approach further allows for application of a new unsupervised speaker adaptation technique to models defined in the ivector space. Experiments are conducted on the core condition of the NIST 2008 corpora, where, with adaptation, the new approach produces an equal error rate (EER) of 4.8\% and min decision cost function (MinDCF) of 2.3\% on all female speaker trials.},
  langid = {english},
  file = {/Users/nvk/Zotero/storage/A25U95W9/Dehak et al. - Cosine Similarity Scoring without Score Normalizat.pdf}
}

@article{dengImagetoMarkupGenerationCoarsetoFine2017,
  title = {Image-to-{{Markup Generation}} with {{Coarse-to-Fine Attention}}},
  author = {Deng, Yuntian and Kanervisto, Anssi and Ling, Jeffrey and Rush, Alexander M.},
  year = {2017},
  month = jun,
  journal = {arXiv:1609.04938 [cs]},
  eprint = {1609.04938},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {We present a neural encoder-decoder model to convert images into presentational markup based on a scalable coarse-to-fine attention mechanism. Our method is evaluated in the context of imageto-LaTeX generation, and we introduce a new dataset of real-world rendered mathematical expressions paired with LaTeX markup. We show that unlike neural OCR techniques using CTCbased models, attention-based approaches can tackle this non-standard OCR task. Our approach outperforms classical mathematical OCR systems by a large margin on in-domain rendered data, and, with pretraining, also performs well on out-of-domain handwritten data. To reduce the inference complexity associated with the attention-based approaches, we introduce a new coarse-to-fine attention layer that selects a support region before applying attention.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computation and Language,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing},
  file = {/Users/nvk/Zotero/storage/JR995YQM/Deng et al. - 2017 - Image-to-Markup Generation with Coarse-to-Fine Att.pdf}
}

@incollection{dennisApplicationsEcology1988,
  title = {Applications in {{Ecology}}},
  booktitle = {Lognormal {{Distributions}}},
  author = {Dennis, Brian and Patil, G. P.},
  year = {1988},
  publisher = {{Routledge}},
  abstract = {This chapter discusses some of the theoretical and descriptive modeling studies in ecology that have featured the lognormal. It focuses on the lognormal as a model of the abundances of species and not as a model of the size growth of individual organisms. The chapter examines the lognormal as a theoretical model of population abundance. It describes a different stochastic growth model leading to the lognormal. The model is a stochastic differential equation based on the Gompertz growth equation. The chapter explores three typical modifications of the lognormal. First, ecological data sets often consist of count data. Second, ecological abundance surveys often contain an overly large number of samples with abundances of zero. Third, ecological abundances observed in samples sometimes grew from random numbers of initial propagules in each sample. The lognormal is commonly used in ecology in a purely descriptive role as a model of abundance of a single species present in different samples.},
  isbn = {978-0-203-74866-4}
}

@inproceedings{denoyerWikipediaXmlCorpus2006,
  title = {The Wikipedia Xml Corpus},
  booktitle = {International {{Workshop}} of the {{Initiative}} for the {{Evaluation}} of {{XML Retrieval}}},
  author = {Denoyer, Ludovic and Gallinari, Patrick},
  year = {2006},
  pages = {12--19},
  publisher = {{Springer}}
}

@book{Developing21stCentury2014,
  title = {Developing a 21st {{Century Global Library}} for {{Mathematics Research}}},
  year = {2014},
  month = mar,
  pages = {18619},
  publisher = {{National Academies Press}},
  address = {{Washington, D.C.}},
  doi = {10.17226/18619},
  isbn = {978-0-309-29848-3},
  langid = {english},
  file = {/Users/nvk/Zotero/storage/JHVIXR4C/2014 - Developing a 21st Century Global Library for Mathe.pdf}
}

@book{Developing21stCentury2014a,
  title = {Developing a 21st {{Century Global Library}} for {{Mathematics Research}}},
  year = {2014},
  month = mar,
  pages = {18619},
  publisher = {{National Academies Press}},
  address = {{Washington, D.C.}},
  doi = {10.17226/18619},
  isbn = {978-0-309-29848-3},
  langid = {english},
  file = {/Users/nvk/Zotero/storage/K7NEAHY9/2014 - Developing a 21st Century Global Library for Mathe.pdf}
}

@article{devlinBERTPretrainingDeep2019,
  title = {{{BERT}}: {{Pre-training}} of {{Deep Bidirectional Transformers}} for {{Language Understanding}}},
  shorttitle = {{{BERT}}},
  author = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  year = {2019},
  month = may,
  journal = {arXiv:1810.04805 [cs]},
  eprint = {1810.04805},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models, BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5\% (7.7\% point absolute improvement), MultiNLI accuracy to 86.7\% (4.6\% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/nvk/Zotero/storage/WAQFASIF/Devlin et al. - 2019 - BERT Pre-training of Deep Bidirectional Transform.pdf;/Users/nvk/Zotero/storage/M8M2SFHE/1810.html}
}

@misc{DirectManipulationProceedings,
  title = {Direct Manipulation | {{Proceedings}} of the {{Joint Conference}} on {{Easier}} and {{More Productive Use}} of {{Computer Systems}}. ({{Part}} - {{II}}): {{Human Interface}} and the {{User Interface}} - {{Volume}} 1981},
  howpublished = {https://dl.acm.org/doi/abs/10.1145/800276.810991}
}

@inproceedings{dml:702604,
  title = {Web Interface and Collection for Mathematical Retrieval: {{WebMIaS}} and {{MREC}}},
  booktitle = {Towards a Digital Mathematics Library.},
  author = {L{\'i}{\v s}ka, Martin and Sojka, Petr and R{\r{u}}{\v z}i{\v c}ka, Michal and Mravec, Petr},
  editor = {Sojka, Petr and Bouche, Thierry},
  year = {2011},
  month = jul,
  pages = {77--84},
  publisher = {{Masaryk University}},
  address = {{Bertinoro, Italy}},
  isbn = {978-80-210-5542-1}
}

@inproceedings{duranRewriteEnginesCompetitions2019,
  title = {The {{Rewrite Engines Competitions}}: {{A RECtrospective}}},
  shorttitle = {The {{Rewrite Engines Competitions}}},
  booktitle = {Tools and {{Algorithms}} for the {{Construction}} and {{Analysis}} of {{Systems}}},
  author = {Dur{\'a}n, Francisco and Garavel, Hubert},
  editor = {Beyer, Dirk and Huisman, Marieke and Kordon, Fabrice and Steffen, Bernhard},
  year = {2019},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {93--100},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-030-17502-3_6},
  abstract = {Term rewriting is a simple, yet expressive model of computation, which finds direct applications in specification and programming languages (many of which embody rewrite rules, pattern matching, and abstract data types), but also indirect applications, e.g., to express the semantics of data types or concurrent processes, to specify program transformations, to perform computer-aided verification, etc. The Rewrite Engines Competition (REC) was created under the aegis of the Workshop on Rewriting Logic and its Applications (WRLA) to serve three main goals: (i) being a forum in which tool developers and potential users of term rewrite engines can share experience; (ii) bringing together the various language features and implementation techniques used for term rewriting; and (iii) comparing the available term rewriting languages and tools in their common features. The present article provides a retrospective overview of the four editions of the Rewrite Engines Competition (2006, 2008, 2010, and 2018) and traces their evolution over time.},
  isbn = {978-3-030-17502-3},
  langid = {english},
  file = {/Users/nvk/Zotero/storage/CY5LAVSY/DurÃ¡n and Garavel - 2019 - The Rewrite Engines Competitions A RECtrospective.pdf}
}

@inproceedings{einwohnerSearchingTechniquesIntegral1995,
  title = {Searching Techniques for Integral Tables},
  booktitle = {Proceedings of the 1995 International Symposium on {{Symbolic}} and Algebraic Computation  - {{ISSAC}} '95},
  author = {Einwohner, T. H. and Fateman, Richard J.},
  year = {1995},
  pages = {133--139},
  publisher = {{ACM Press}},
  address = {{Montreal, Quebec, Canada}},
  doi = {10.1145/220346.220364},
  abstract = {We describe the design of data structures and a computer program for storing a table of symbolic indefinite or definite integrals and retrieving user-requested integrals on demand. Typical times are so short that a preliminary look-up attempt prior to any algorithmic integration approach seems justified. In one such test for a table with around 700 entries, mat ches were found requiring an average of 2.8 milliseconds per request, on a Hewlett Packard 9000/712 workstation.},
  isbn = {978-0-89791-699-8},
  langid = {english},
  file = {/Users/nvk/Zotero/storage/JH2Q3724/Einwohner and Fateman - 1995 - Searching techniques for integral tables.pdf}
}

@inproceedings{el-sawyCNNHandwrittenArabic2017,
  title = {{{CNN}} for {{Handwritten Arabic Digits Recognition Based}} on {{LeNet-5}}},
  booktitle = {Proceedings of the {{International Conference}} on {{Advanced Intelligent Systems}} and {{Informatics}} 2016},
  author = {{El-Sawy}, Ahmed and {EL-Bakry}, Hazem and Loey, Mohamed},
  editor = {Hassanien, Aboul Ella and Shaalan, Khaled and Gaber, Tarek and Azar, Ahmad Taher and Tolba, M. F.},
  year = {2017},
  series = {Advances in {{Intelligent Systems}} and {{Computing}}},
  pages = {566--575},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-319-48308-5_54},
  abstract = {In recent years, handwritten digits recognition has been an important area due to its applications in several fields. This work is focusing on the recognition part of handwritten Arabic digits recognition that face several challenges, including the unlimited variation in human handwriting and the large public databases. The paper provided a deep learning technique that can be effectively apply to recognizing Arabic handwritten digits. LeNet-5, a Convolutional Neural Network (CNN) trained and tested MADBase database (Arabic handwritten digits images) that contain 60000 training and 10000 testing images. A comparison is held amongst the results, and it is shown by the end that the use of CNN was leaded to significant improvements across different machine-learning classification algorithms.},
  isbn = {978-3-319-48308-5},
  langid = {english},
  keywords = {Convolutional Neural Network,Deep Learning,Digit Recognition,Handwritten Digit,Recognition Rate},
  file = {/Users/nvk/Zotero/storage/3XZFDERL/El-Sawy et al. - 2017 - CNN for Handwritten Arabic Digits Recognition Base.pdf}
}

@inproceedings{etoMathematicalFormulaRecognition2001,
  title = {Mathematical Formula Recognition Using Virtual Link Network},
  booktitle = {Proceedings of {{Sixth International Conference}} on {{Document Analysis}} and {{Recognition}}},
  author = {Eto, Y. and Suzuki, M.},
  year = {2001},
  month = sep,
  pages = {762--767},
  doi = {10.1109/ICDAR.2001.953891},
  abstract = {We propose a new method of recognizing mathematical formulae. The method is robust against the recognition errors of characters and the variation of the printing styles of the documents. The outline is as follows: we first construct a network with vertices representing the characters (symbols), linked with each other by several edges with labels and costs representing the possible relations of the pair of characters. The network has multiple edges with different labels and costs representing the ambiguity of the decision of the relation of character pairs. Then, we output the spanning tree of the network with minimum cost which corresponds to the recognition result of the structure of the mathematical formula, using not only the local costs initially attached to the network but the costs reflecting global structure of the formula. The advantage of this method is that local errors of the recognition are recovered automatically by the total cost of the recognition tree.},
  keywords = {character pairs,character recognition,Character recognition,Costs,Dynamic programming,mathematical formula recognition,Mathematics,Optical character recognition software,Printing,printing styles,recognition errors,recognition tree,Robustness,Search methods,spanning tree,trees (mathematics),vertices,virtual link network},
  file = {/Users/nvk/Zotero/storage/I2YA8YB4/Eto and Suzuki - 2001 - Mathematical formula recognition using virtual lin.pdf;/Users/nvk/Zotero/storage/IDVCWWBB/953891.html}
}

@misc{EvaluationSimpleEffective,
  title = {Evaluation of a Simple and Effective Music Information Retrieval Method | {{Proceedings}} of the 23rd Annual International {{ACM SIGIR}} Conference on {{Research}} and Development in Information Retrieval},
  howpublished = {https://dl.acm.org/doi/abs/10.1145/345508.345551},
  file = {/Users/nvk/Zotero/storage/FDKDTFXH/345508.html}
}

@article{fanFerroelectricHfO2basedMaterials2016,
  title = {Ferroelectric {{HfO2-based}} Materials for next-Generation Ferroelectric Memories},
  author = {Fan, Zhen and Chen, Jingsheng and Wang, John},
  year = {2016},
  month = may,
  journal = {Journal of Advanced Dielectrics},
  volume = {06},
  number = {02},
  pages = {1630003},
  publisher = {{World Scientific Publishing Co.}},
  issn = {2010-135X},
  doi = {10.1142/S2010135X16300036},
  abstract = {Ferroelectric random access memory (FeRAM) based on conventional ferroelectric perovskites, such as Pb(Zr,Ti)O3 and SrBi2Ta2O9, has encountered bottlenecks on memory density and cost, because those conventional perovskites suffer from various issues mainly including poor complementary metal-oxide-semiconductor (CMOS)-compatibility and limited scalability. Next-generation cost-efficient, high-density FeRAM shall therefore rely on a material revolution. Since the discovery of ferroelectricity in Si:HfO2 thin films in 2011, HfO2-based materials have aroused widespread interest in the field of FeRAM, because they are CMOS-compatible and can exhibit robust ferroelectricity even when the film thickness is scaled down to below 10 nm. A review on this new class of ferroelectric materials is therefore of great interest. In this paper, the most appealing topics about ferroelectric HfO2-based materials including origins of ferroelectricity, advantageous material properties, and current and potential applications in FeRAM, are briefly reviewed.},
  file = {/Users/nvk/Zotero/storage/XS9X82R5/Fan et al. - 2016 - Ferroelectric HfO2-based materials for next-genera.pdf;/Users/nvk/Zotero/storage/NBS8BL3B/S2010135X16300036.html}
}

@article{fatemanParsingTEXMathematics1999,
  title = {Parsing {{TEX}} into Mathematics},
  author = {Fateman, Richard J. and Caspi, Eylon},
  year = {1999},
  month = sep,
  journal = {ACM SIGSAM Bulletin},
  volume = {33},
  number = {3},
  pages = {26},
  issn = {0163-5824},
  doi = {10.1145/347127.347441},
  abstract = {Communication, storage, transmission, and searching of complex material has become increasingly important. Mathematical computing in a distributed environment is also becoming more plausible as libraries and computing facilities are connected with each other and with user facilities. TEX is a wellknown mathematical typesetting language, and from the display perspective it might seem that it could be used for communication between computer systems as well as an intermediate form for the results of OCR (optical character recognition) of mathematical expressions. There are flaws in this reasoning, since exchanging mathematical information requires a system to parse and semantically ``understand'' the TEX, even if it is ``ambiguous'' notationally. A program we developed can handle 43\% of 10,740 TEX formulas in a well-known table of integrals. We expect that a higher success rate can be achieved easily.},
  langid = {english},
  file = {/Users/nvk/Zotero/storage/SXSGY54I/Fateman and Caspi - 1999 - Parsing T E X into mathematics.pdf}
}

@inproceedings{fitzgeraldMathpadFuzzyLogicbased2007,
  title = {Mathpad: {{A}} Fuzzy Logic-Based Recognition System for Handwritten Mathematics},
  booktitle = {Ninth {{International Conference}} on {{Document Analysis}} and {{Recognition}} ({{ICDAR}} 2007)},
  author = {Fitzgerald, J. and Geiselbrechtinger, Franz and Kechadi, Tahar},
  year = {2007},
  volume = {2},
  pages = {694--698},
  publisher = {{IEEE}},
  isbn = {0-7695-2822-8}
}

@incollection{fraserChoosingMathFeatures2018,
  title = {Choosing {{Math Features}} for {{BM25 Ranking}} with {{Tangent-L}}},
  booktitle = {Proceedings of the {{ACM Symposium}} on {{Document Engineering}} 2018},
  author = {Fraser, Dallas and Kane, Andrew and Tompa, Frank Wm.},
  year = {2018},
  month = aug,
  number = {17},
  pages = {1--10},
  publisher = {{Association for Computing Machinery}},
  address = {{New York, NY, USA}},
  abstract = {Combining text and mathematics when searching in a corpus with extensive mathematical notation remains an open problem. Recent results for Tangent-3 on the math and text retrieval task at NTCIR-12, for example, have room for improvement, even though formula retrieval appeared to be fairly successful. This paper explores how to adapt the state-of-the-art BM25 text ranking method to work well when searching for math together with text. Following the approach proposed for the Tangent math search system, we use symbol layout trees to represent math formulae. We extract features from the symbol layout trees to serve as search terms to be ranked using BM25 and then explore the effects on retrieval performance of various classes of features. Based on the results, we recommend which features can be used effectively in a conventional text-based retrieval engine. We validate our overall approach using a NTCIR-12 math and text benchmark.},
  isbn = {978-1-4503-5769-2},
  keywords = {Lucene,Mathematical content representation,Mathematics information retrieval (MIR),MathML,Okapi BM25},
  file = {/Users/nvk/Zotero/storage/DDHCPRN6/Fraser et al. - 2018 - Choosing Math Features for BM25 Ranking with Tange.pdf}
}

@inproceedings{fuKnowledgeMaintenanceScientific2020,
  title = {Towards {{Knowledge Maintenance}} in {{Scientific Digital Libraries}} with the {{Keystone Framework}}},
  booktitle = {Proceedings of the {{ACM}}/{{IEEE Joint Conference}} on {{Digital Libraries}} in 2020},
  author = {Fu, Yuanxi and Schneider, Jodi},
  year = {2020},
  month = aug,
  pages = {217--226},
  publisher = {{ACM}},
  address = {{Virtual Event China}},
  doi = {10.1145/3383583.3398514},
  abstract = {Scientific digital libraries speed dissemination of scientific publications, but also the propagation of invalid or unreliable knowledge. Although many papers with known validity problems are highly cited, no auditing process is currently available to determine whether a citing paper's findings fundamentally depend on invalid or unreliable knowledge. To address this, we introduce a new framework, the keystone framework, designed to identify when and how citing unreliable findings impacts a paper, using argumentation theory and citation context analysis. Through two pilot case studies, we demonstrate how the keystone framework can be applied to knowledge maintenance tasks for digital libraries, including addressing citations of a non-reproducible paper and identifying statements most needing validation in a high-impact paper. We identify roles for librarians, database maintainers, knowledgebase curators, and research software engineers in applying the framework to scientific digitallibraries.},
  isbn = {978-1-4503-7585-6},
  langid = {english},
  file = {/Users/nvk/Zotero/storage/ZVCIL8E3/Fu and Schneider - 2020 - Towards Knowledge Maintenance in Scientific Digita.pdf}
}

@article{fullerLiIonSynapticTransistor2017,
  title = {Li-{{Ion Synaptic Transistor}} for {{Low Power Analog Computing}}},
  author = {Fuller, Elliot J. and Gabaly, Farid El and L{\'e}onard, Fran{\c c}ois and Agarwal, Sapan and Plimpton, Steven J. and Jacobs-Gedrim, Robin B. and James, Conrad D. and Marinella, Matthew J. and Talin, A. Alec},
  year = {2017},
  journal = {Advanced Materials},
  volume = {29},
  number = {4},
  pages = {1604310},
  issn = {1521-4095},
  doi = {10.1002/adma.201604310},
  abstract = {Nonvolatile redox transistors (NVRTs) based upon Li-ion battery materials are demonstrated as memory elements for neuromorphic computer architectures with multi-level analog states, ``write'' linearity, low-voltage switching, and low power dissipation. Simulations of backpropagation using the device properties reach ideal classification accuracy. Physics-based simulations predict energy costs per ``write'' operation of {$<$}10 aJ when scaled to 200 nm \texttimes{} 200 nm.},
  copyright = {\textcopyright{} 2016 WILEY-VCH Verlag GmbH \& Co. KGaA, Weinheim},
  keywords = {data storage,nanodevices,transistors},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/adma.201604310},
  file = {/Users/nvk/Zotero/storage/7XWDRFMV/Fuller et al. - 2017 - Li-Ion Synaptic Transistor for Low Power Analog Co.pdf;/Users/nvk/Zotero/storage/KB82R4KE/adma.html}
}

@article{gaizauskasCombinedIRNLP,
  title = {A {{Combined IR}}/{{NLP Approach}} to {{Question Answering Against Large Text Collections}}},
  author = {Gaizauskas, Robert and Humphreys, Kevin},
  pages = {17},
  langid = {english},
  file = {/Users/nvk/Zotero/storage/JMYTJV7Q/Gaizauskas and Humphreys - A Combined IRNLP Approach to Question Answering A.pdf}
}

@incollection{ganesalingamAmbiguity2013,
  title = {Ambiguity},
  booktitle = {The {{Language}} of {{Mathematics}}: {{A Linguistic}} and {{Philosophical Investigation}}},
  author = {Ganesalingam, Mohan},
  editor = {Ganesalingam, Mohan},
  year = {2013},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {87--112},
  publisher = {{Springer}},
  address = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-642-37012-0_4},
  abstract = {Ambiguity and the resolution of ambiguity will occupy us for the next three chapters. Before we delve into specifics, it is worth setting out the particular points that make the disambiguation of mathematics difficult. Some of these points can be compactly conveyed; others will be sketched here and substantiated in the remainder of this chapter, which surveys ambiguity in mathematics. All of the points will be addressed in the course of Chapter 5 and Chapter 6.},
  isbn = {978-3-642-37012-0},
  langid = {english},
  keywords = {Formal Language,Noun Phrase,Parse Tree,Textual Material,Textual Structure}
}

@incollection{ganesalingamAmbiguity2013a,
  title = {Ambiguity},
  booktitle = {The {{Language}} of {{Mathematics}}: {{A Linguistic}} and {{Philosophical Investigation}}},
  author = {Ganesalingam, Mohan},
  editor = {Ganesalingam, Mohan},
  year = {2013},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {87--112},
  publisher = {{Springer}},
  address = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-642-37012-0_4},
  abstract = {Ambiguity and the resolution of ambiguity will occupy us for the next three chapters. Before we delve into specifics, it is worth setting out the particular points that make the disambiguation of mathematics difficult. Some of these points can be compactly conveyed; others will be sketched here and substantiated in the remainder of this chapter, which surveys ambiguity in mathematics. All of the points will be addressed in the course of Chapter 5 and Chapter 6.},
  isbn = {978-3-642-37012-0},
  langid = {english},
  keywords = {Formal Language,Noun Phrase,Parse Tree,Textual Material,Textual Structure},
  file = {/Users/nvk/Zotero/storage/YL4KZ35H/Ganesalingam - 2013 - Ambiguity.pdf}
}

@incollection{ganesalingamConclusion2013,
  title = {Conclusion},
  booktitle = {The {{Language}} of {{Mathematics}}: {{A Linguistic}} and {{Philosophical Investigation}}},
  author = {Ganesalingam, Mohan},
  editor = {Ganesalingam, Mohan},
  year = {2013},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {249--252},
  publisher = {{Springer}},
  address = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-642-37012-0_9},
  abstract = {First of all, we required that the theory be able to describe all of pure mathematics. This requirement pervaded the work described in this book, although its impact was rarely explicit. Had we been giving a theory suited to one or two domains, large portions of the book would have been unnecessary. For example, if we had only been concerned with real analysis, then all of our discussions of structures and relational types would have been unnecessary. If we had only been concerned with combinatorics, our extensive discussion of the number system could have been dispensed with. (And so on.) More importantly, if we had only been discussing individual domains, we would 250 9 Conclusion not have found many of the generalisations which we did; for example, it is unlikely that we would have discovered a general mechanism that could track both the dimensions of a matrix and the group in which a particular instance of group multiplication occurred (\textsection 8.3). Thus our requirement of generality exerted a quiet but continual pressure throughout the book, forcing us to discover deeper patterns rather than giving superficial analysis of the phenomena at hand.},
  isbn = {978-3-642-37012-0},
  langid = {english},
  keywords = {Intuitive Belief,Mathematical Object,Pure Mathematic,Symbolic Material,Syntactic Category}
}

@incollection{ganesalingamConclusion2013a,
  title = {Conclusion},
  booktitle = {The {{Language}} of {{Mathematics}}: {{A Linguistic}} and {{Philosophical Investigation}}},
  author = {Ganesalingam, Mohan},
  editor = {Ganesalingam, Mohan},
  year = {2013},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {249--252},
  publisher = {{Springer}},
  address = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-642-37012-0_9},
  abstract = {First of all, we required that the theory be able to describe all of pure mathematics. This requirement pervaded the work described in this book, although its impact was rarely explicit. Had we been giving a theory suited to one or two domains, large portions of the book would have been unnecessary. For example, if we had only been concerned with real analysis, then all of our discussions of structures and relational types would have been unnecessary. If we had only been concerned with combinatorics, our extensive discussion of the number system could have been dispensed with. (And so on.) More importantly, if we had only been discussing individual domains, we would 250 9 Conclusion not have found many of the generalisations which we did; for example, it is unlikely that we would have discovered a general mechanism that could track both the dimensions of a matrix and the group in which a particular instance of group multiplication occurred (\textsection 8.3). Thus our requirement of generality exerted a quiet but continual pressure throughout the book, forcing us to discover deeper patterns rather than giving superficial analysis of the phenomena at hand.},
  isbn = {978-3-642-37012-0},
  langid = {english},
  keywords = {Intuitive Belief,Mathematical Object,Pure Mathematic,Symbolic Material,Syntactic Category},
  file = {/Users/nvk/Zotero/storage/SANT83RJ/Ganesalingam - 2013 - Conclusion.pdf}
}

@incollection{ganesalingamExtensions2013,
  title = {Extensions},
  booktitle = {The {{Language}} of {{Mathematics}}: {{A Linguistic}} and {{Philosophical Investigation}}},
  author = {Ganesalingam, Mohan},
  editor = {Ganesalingam, Mohan},
  year = {2013},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {237--247},
  publisher = {{Springer}},
  address = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-642-37012-0_8},
  abstract = {There are a large number of minor topics in the language of mathematics which do have not have space to describe in depth. \textsection 8.1 and \textsection 8.2 give brief outlines of our approach to these miscellaneous topics in textual and symbolic mathematics respectively. \textsection 8.3 discusses a final major topic.},
  isbn = {978-3-642-37012-0},
  langid = {english}
}

@incollection{ganesalingamExtensions2013a,
  title = {Extensions},
  booktitle = {The {{Language}} of {{Mathematics}}: {{A Linguistic}} and {{Philosophical Investigation}}},
  author = {Ganesalingam, Mohan},
  editor = {Ganesalingam, Mohan},
  year = {2013},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {237--247},
  publisher = {{Springer}},
  address = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-642-37012-0_8},
  abstract = {There are a large number of minor topics in the language of mathematics which do have not have space to describe in depth. \textsection 8.1 and \textsection 8.2 give brief outlines of our approach to these miscellaneous topics in textual and symbolic mathematics respectively. \textsection 8.3 discusses a final major topic.},
  isbn = {978-3-642-37012-0},
  langid = {english},
  file = {/Users/nvk/Zotero/storage/RYQAPD6D/Ganesalingam - 2013 - Extensions.pdf}
}

@incollection{ganesalingamFoundations2013,
  title = {Foundations},
  booktitle = {The {{Language}} of {{Mathematics}}: {{A Linguistic}} and {{Philosophical Investigation}}},
  author = {Ganesalingam, Mohan},
  editor = {Ganesalingam, Mohan},
  year = {2013},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {175--235},
  publisher = {{Springer}},
  address = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-642-37012-0_7},
  abstract = {In \textsection 5.3, we noted that type distinctions among various elementary objects were needed in order to avoid two linguistic problems. The first of these was notational collision; we saw that if we took, say, natural numbers and sets to be of the same type, then our theory incorrectly predicted that expressions like `1 + 1' had multiple readings. Constructing this type distinction was hard because the standard foundational account defines natural numbers as sets. The second problem was that of unwarranted distinctions; we saw, for example, that the actual use of mathematical language required real numbers and complex numbers to be of the same type in order to assign a reading to an expression such as `zz {${_\ast}$} . {$<$} 1'. Preventing a type distinction here was difficult because the standard foundational account defines complex numbers as particular structures containing real numbers.},
  isbn = {978-3-642-37012-0},
  langid = {english},
  keywords = {Global Condition,Mathematical Object,Natural Number,Nonnegative Integer,Rational Number}
}

@incollection{ganesalingamFoundations2013a,
  title = {Foundations},
  booktitle = {The {{Language}} of {{Mathematics}}: {{A Linguistic}} and {{Philosophical Investigation}}},
  author = {Ganesalingam, Mohan},
  editor = {Ganesalingam, Mohan},
  year = {2013},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {175--235},
  publisher = {{Springer}},
  address = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-642-37012-0_7},
  abstract = {In \textsection 5.3, we noted that type distinctions among various elementary objects were needed in order to avoid two linguistic problems. The first of these was notational collision; we saw that if we took, say, natural numbers and sets to be of the same type, then our theory incorrectly predicted that expressions like `1 + 1' had multiple readings. Constructing this type distinction was hard because the standard foundational account defines natural numbers as sets. The second problem was that of unwarranted distinctions; we saw, for example, that the actual use of mathematical language required real numbers and complex numbers to be of the same type in order to assign a reading to an expression such as `zz {${_\ast}$} . {$<$} 1'. Preventing a type distinction here was difficult because the standard foundational account defines complex numbers as particular structures containing real numbers.},
  isbn = {978-3-642-37012-0},
  langid = {english},
  keywords = {Global Condition,Mathematical Object,Natural Number,Nonnegative Integer,Rational Number},
  file = {/Users/nvk/Zotero/storage/ZHEMLMR7/Ganesalingam - 2013 - Foundations.pdf}
}

@incollection{ganesalingamIntroduction2013,
  title = {Introduction},
  booktitle = {The {{Language}} of {{Mathematics}}: {{A Linguistic}} and {{Philosophical Investigation}}},
  author = {Ganesalingam, Mohan},
  editor = {Ganesalingam, Mohan},
  year = {2013},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {1--15},
  publisher = {{Springer}},
  address = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-642-37012-0_1},
  abstract = {The aim of this book is to give a formal, objective and above all precise analysis of the language used by mathematicians in textbooks and papers. Our analysis will closely parallel the analyses of human languages by syntacticians and semanticians in the generative tradition. In particular, it will let us take mathematical sentences, determine their syntactic structure, and extract their underlying meaning in an appropriate logic.},
  isbn = {978-3-642-37012-0},
  langid = {english}
}

@incollection{ganesalingamIntroduction2013a,
  title = {Introduction},
  booktitle = {The {{Language}} of {{Mathematics}}: {{A Linguistic}} and {{Philosophical Investigation}}},
  author = {Ganesalingam, Mohan},
  editor = {Ganesalingam, Mohan},
  year = {2013},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {1--15},
  publisher = {{Springer}},
  address = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-642-37012-0_1},
  abstract = {The aim of this book is to give a formal, objective and above all precise analysis of the language used by mathematicians in textbooks and papers. Our analysis will closely parallel the analyses of human languages by syntacticians and semanticians in the generative tradition. In particular, it will let us take mathematical sentences, determine their syntactic structure, and extract their underlying meaning in an appropriate logic.},
  isbn = {978-3-642-37012-0},
  langid = {english},
  file = {/Users/nvk/Zotero/storage/6LAUSENL/Ganesalingam - 2013 - Introduction.pdf}
}

@incollection{ganesalingamLanguageMathematics2013,
  title = {The {{Language}} of {{Mathematics}}},
  booktitle = {The {{Language}} of {{Mathematics}}: {{A Linguistic}} and {{Philosophical Investigation}}},
  author = {Ganesalingam, Mohan},
  editor = {Ganesalingam, Mohan},
  year = {2013},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {17--38},
  publisher = {{Springer}},
  address = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-642-37012-0_2},
  abstract = {We will now give an informal description of the language of mathematics, and highlight some of the major issues that arise to confront a theory of mathematical language. No systematic survey of this kind exists in the literature, and we will therefore for the most part construct our description ab initio. Exceptions will be drawn in certain areas where Ranta has discussed similar phenomena, especially in \textsection 2.4.We will start by introducing a basic division of the language of mathematics into `textual' and `symbolic' halves (\textsection 2.1) and introducing the most important way in which the language of mathematics differs from natural languages (\textsection 2.2). We will then examine each of textual and symbolic mathematics in greater detail (\textsection 2.3 and \textsection 2.4), and finally turn to the macroscopic discourse structure of mathematical language (\textsection 2.5).},
  isbn = {978-3-642-37012-0},
  langid = {english},
  keywords = {Mathematical Language,Mathematical Text,Natural Language,Noun Phrase,Symbolic Mathematic}
}

@incollection{ganesalingamLanguageMathematics2013a,
  title = {The {{Language}} of {{Mathematics}}},
  booktitle = {The {{Language}} of {{Mathematics}}: {{A Linguistic}} and {{Philosophical Investigation}}},
  author = {Ganesalingam, Mohan},
  editor = {Ganesalingam, Mohan},
  year = {2013},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {17--38},
  publisher = {{Springer}},
  address = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-642-37012-0_2},
  abstract = {We will now give an informal description of the language of mathematics, and highlight some of the major issues that arise to confront a theory of mathematical language. No systematic survey of this kind exists in the literature, and we will therefore for the most part construct our description ab initio. Exceptions will be drawn in certain areas where Ranta has discussed similar phenomena, especially in \textsection 2.4.We will start by introducing a basic division of the language of mathematics into `textual' and `symbolic' halves (\textsection 2.1) and introducing the most important way in which the language of mathematics differs from natural languages (\textsection 2.2). We will then examine each of textual and symbolic mathematics in greater detail (\textsection 2.3 and \textsection 2.4), and finally turn to the macroscopic discourse structure of mathematical language (\textsection 2.5).},
  isbn = {978-3-642-37012-0},
  langid = {english},
  keywords = {Mathematical Language,Mathematical Text,Natural Language,Noun Phrase,Symbolic Mathematic},
  file = {/Users/nvk/Zotero/storage/MBZH9VX9/Ganesalingam - 2013 - The Language of Mathematics.pdf}
}

@incollection{ganesalingamTheoreticalFramework2013,
  title = {Theoretical {{Framework}}},
  booktitle = {The {{Language}} of {{Mathematics}}: {{A Linguistic}} and {{Philosophical Investigation}}},
  author = {Ganesalingam, Mohan},
  editor = {Ganesalingam, Mohan},
  year = {2013},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {39--85},
  publisher = {{Springer}},
  address = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-642-37012-0_3},
  abstract = {Our syntactic framework for mathematics will be shaped by two major concerns. First, the framework will need to allow for the introduction of new terms and notation via definitions (\textsection 2.2); and we would like to be able to describe the effect of those definitions in an simple, consistent way. It would also be advantageous if the mechanism describing definitions could treat textual and symbolic material in essentially the same fashion. Thus adaptivity leads us to want a syntactic framework that is as simple as possible, and that describes text and symbol in a unified way.},
  isbn = {978-3-642-37012-0},
  langid = {english},
  keywords = {Noun Phrase,Predicate Calculus,Semantic Function,Semantic Representation,Syntactic Rule}
}

@incollection{ganesalingamTheoreticalFramework2013a,
  title = {Theoretical {{Framework}}},
  booktitle = {The {{Language}} of {{Mathematics}}: {{A Linguistic}} and {{Philosophical Investigation}}},
  author = {Ganesalingam, Mohan},
  editor = {Ganesalingam, Mohan},
  year = {2013},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {39--85},
  publisher = {{Springer}},
  address = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-642-37012-0_3},
  abstract = {Our syntactic framework for mathematics will be shaped by two major concerns. First, the framework will need to allow for the introduction of new terms and notation via definitions (\textsection 2.2); and we would like to be able to describe the effect of those definitions in an simple, consistent way. It would also be advantageous if the mechanism describing definitions could treat textual and symbolic material in essentially the same fashion. Thus adaptivity leads us to want a syntactic framework that is as simple as possible, and that describes text and symbol in a unified way.},
  isbn = {978-3-642-37012-0},
  langid = {english},
  keywords = {Noun Phrase,Predicate Calculus,Semantic Function,Semantic Representation,Syntactic Rule},
  file = {/Users/nvk/Zotero/storage/T7E64ET3/Ganesalingam - 2013 - Theoretical Framework.pdf}
}

@incollection{ganesalingamType2013,
  title = {Type},
  booktitle = {The {{Language}} of {{Mathematics}}: {{A Linguistic}} and {{Philosophical Investigation}}},
  author = {Ganesalingam, Mohan},
  editor = {Ganesalingam, Mohan},
  year = {2013},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {113--156},
  publisher = {{Springer}},
  address = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-642-37012-0_5},
  abstract = {In this chapter, following on from our discussion of ambiguity (Chapter 4), we will introduce the types which are necessary to disambiguate mathematical language. As noted in \textsection 4.4, the actual inference of types will need to be fused into the parsing process; this will be the subject of the next chapter (Chapter 6). Additionally, in the course of this chapter, we will encounter a number of issues relating to the foundations of mathematics; these issues will be discussed extensively in Chapter 7.There are two major difficulties that arise when one is trying to find the types necessary to describe mathematics. First, a great many type systems seem reasonable until a key counterexample is discovered; and, more specifically, these counterexamples are difficult to find because they are mathematically counterintuitive. There is a real disparity between what the rules of mathematics itself allow, and the way in which we think about mathematical objects. Each of the counterexamples we will present below reflects this disparity in a different way.},
  isbn = {978-3-642-37012-0},
  langid = {english},
  keywords = {Inference Rule,Mathematical Object,Semantic Representation,Topological Space,Type System}
}

@incollection{ganesalingamType2013a,
  title = {Type},
  booktitle = {The {{Language}} of {{Mathematics}}: {{A Linguistic}} and {{Philosophical Investigation}}},
  author = {Ganesalingam, Mohan},
  editor = {Ganesalingam, Mohan},
  year = {2013},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {113--156},
  publisher = {{Springer}},
  address = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-642-37012-0_5},
  abstract = {In this chapter, following on from our discussion of ambiguity (Chapter 4), we will introduce the types which are necessary to disambiguate mathematical language. As noted in \textsection 4.4, the actual inference of types will need to be fused into the parsing process; this will be the subject of the next chapter (Chapter 6). Additionally, in the course of this chapter, we will encounter a number of issues relating to the foundations of mathematics; these issues will be discussed extensively in Chapter 7.There are two major difficulties that arise when one is trying to find the types necessary to describe mathematics. First, a great many type systems seem reasonable until a key counterexample is discovered; and, more specifically, these counterexamples are difficult to find because they are mathematically counterintuitive. There is a real disparity between what the rules of mathematics itself allow, and the way in which we think about mathematical objects. Each of the counterexamples we will present below reflects this disparity in a different way.},
  isbn = {978-3-642-37012-0},
  langid = {english},
  keywords = {Inference Rule,Mathematical Object,Semantic Representation,Topological Space,Type System},
  file = {/Users/nvk/Zotero/storage/DJXW39YX/Ganesalingam - 2013 - Type.pdf}
}

@incollection{ganesalingamTypedParsing2013,
  title = {Typed {{Parsing}}},
  booktitle = {The {{Language}} of {{Mathematics}}: {{A Linguistic}} and {{Philosophical Investigation}}},
  author = {Ganesalingam, Mohan},
  editor = {Ganesalingam, Mohan},
  year = {2013},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {157--173},
  publisher = {{Springer}},
  address = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-642-37012-0_6},
  abstract = {This chapter builds on Chapter 4 and Chapter 5 to describe the combined type inference and parsing procedure needed to interpret and disambiguate mathematics. Although some type-related concepts are used in this chapter, few of the actual technical details from Chapter 5 are needed. In particular, readers need not assimilate the details of the technical presentation of types from \textsection 5.4.},
  isbn = {978-3-642-37012-0},
  langid = {english}
}

@incollection{ganesalingamTypedParsing2013a,
  title = {Typed {{Parsing}}},
  booktitle = {The {{Language}} of {{Mathematics}}: {{A Linguistic}} and {{Philosophical Investigation}}},
  author = {Ganesalingam, Mohan},
  editor = {Ganesalingam, Mohan},
  year = {2013},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {157--173},
  publisher = {{Springer}},
  address = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-642-37012-0_6},
  abstract = {This chapter builds on Chapter 4 and Chapter 5 to describe the combined type inference and parsing procedure needed to interpret and disambiguate mathematics. Although some type-related concepts are used in this chapter, few of the actual technical details from Chapter 5 are needed. In particular, readers need not assimilate the details of the technical presentation of types from \textsection 5.4.},
  isbn = {978-3-642-37012-0},
  langid = {english},
  file = {/Users/nvk/Zotero/storage/WWKGWKZL/Ganesalingam - 2013 - Typed Parsing.pdf}
}

@article{gaoMathRetrievalSystem2016,
  title = {The {{Math Retrieval System}} of {{ICST}} for {{NTCIR-12 MathIR Task}}},
  author = {Gao, Liangcai and Yuan, Ke and Wang, Yuehan and Jiang, Zhuoren and Tang, Zhi},
  year = {2016},
  pages = {5},
  abstract = {This paper is the summarized experiences of ICST team in the NTCIR-12 MathIR main tasks (ArXiv and Wikipedia main task). Our approach is based on keyword, structure and importance of formulae in a document. A novel hybrid indexing and matching model is proposed to support exact and fuzzing matching. In this hybrid model, both keyword and structure information of formulae are taken into consideration. In addition, the concept of formula importance within a document is introduced into the model. In order to make the ranking results more reasonable, our system (WikiMir) applies the learning to rank algorithm (RankBoost) to rank the retrieved formulae , and then re-ranks the top-k formulae by the regular expressions matching of the query formula. The experimental results show that the method of our system is effective for all metrics and promising in practical application.},
  langid = {english},
  file = {/Users/nvk/Zotero/storage/37C45CWI/Gao et al. - 2016 - The Math Retrieval System of ICST for NTCIR-12 Mat.pdf}
}

@article{gaoPreliminaryExplorationFormula2017,
  title = {Preliminary {{Exploration}} of {{Formula Embedding}} for {{Mathematical Information Retrieval}}: Can Mathematical Formulae Be Embedded like a Natural Language?},
  shorttitle = {Preliminary {{Exploration}} of {{Formula Embedding}} for {{Mathematical Information Retrieval}}},
  author = {Gao, Liangcai and Jiang, Zhuoren and Yin, Yue and Yuan, Ke and Yan, Zuoyu and Tang, Zhi},
  year = {2017},
  month = aug,
  journal = {arXiv:1707.05154 [cs]},
  eprint = {1707.05154},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {While neural network approaches are achieving breakthrough performance in the natural language related fields, there have been few similar attempts at mathematical language related tasks. In this study, we explore the potential of applying neural representation techniques to Mathematical Information Retrieval (MIR) tasks. In more detail, we first briefly analyze the characteristic differences between natural language and mathematical language. Then we design a "symbol2vec" method to learn the vector representations of formula symbols (numbers, variables, operators, functions, etc.) Finally, we propose a "formula2vec" based MIR approach and evaluate its performance. Preliminary experiment results show that there is a promising potential for applying formula embedding models to mathematical language representation and MIR tasks.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Information Retrieval},
  file = {/Users/nvk/Zotero/storage/6UKJUWVQ/Gao et al. - 2017 - Preliminary Exploration of Formula Embedding for M.pdf;/Users/nvk/Zotero/storage/WYE973F8/1707.html}
}

@article{gaoPreliminaryExplorationFormula2017a,
  title = {Preliminary {{Exploration}} of {{Formula Embedding}} for {{Mathematical Information Retrieval}}: Can Mathematical Formulae Be Embedded like a Natural Language?},
  shorttitle = {Preliminary {{Exploration}} of {{Formula Embedding}} for {{Mathematical Information Retrieval}}},
  author = {Gao, Liangcai and Jiang, Zhuoren and Yin, Yue and Yuan, Ke and Yan, Zuoyu and Tang, Zhi},
  year = {2017},
  month = aug,
  journal = {arXiv:1707.05154 [cs]},
  eprint = {1707.05154},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {While neural network approaches are achieving breakthrough performance in the natural language related fields, there have been few similar attempts at mathematical language related tasks. In this study, we explore the potential of applying neural representation techniques to Mathematical Information Retrieval (MIR) tasks. In more detail, we first briefly analyze the characteristic differences between natural language and mathematical language. Then we design a "symbol2vec" method to learn the vector representations of formula symbols (numbers, variables, operators, functions, etc.) Finally, we propose a "formula2vec" based MIR approach and evaluate its performance. Preliminary experiment results show that there is a promising potential for applying formula embedding models to mathematical language representation and MIR tasks.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Information Retrieval},
  file = {/Users/nvk/Zotero/storage/WIB55UQJ/Gao et al. - 2017 - Preliminary Exploration of Formula Embedding for M.pdf;/Users/nvk/Zotero/storage/CT8QJ2BD/1707.html}
}

@article{georgeProgrammableConfigurableMixedMode2016,
  title = {A {{Programmable}} and {{Configurable Mixed-Mode FPAA SoC}}},
  author = {George, Suma and Kim, Sihwan and Shah, Sahil and Hasler, Jennifer and Collins, Michelle and Adil, Farhan and Wunderlich, Richard and Nease, Stephen and Ramakrishnan, Shubha},
  year = {2016},
  month = jun,
  journal = {IEEE Transactions on Very Large Scale Integration (VLSI) Systems},
  volume = {24},
  number = {6},
  pages = {2253--2261},
  issn = {1557-9999},
  doi = {10.1109/TVLSI.2015.2504119},
  abstract = {This paper presents a floating-gate (FG)-based, field-programmable analog array (FPAA) system-on-chip (SoC) that integrates analog and digital programmable and configurable blocks with a 16-bit open-source MSP430 microprocessor ({$\mu$}P) and resulting interface circuitry. We show the FPAA SoC architecture, experimental results from a range of circuits compiled into this architecture, and system measurements. A compiled analog acoustic command-word classifier on the FPAA SoC requires 23 {$\mu$}W to experimentally recognize the word dark in a TIMIT database phrase. This paper jointly optimizes high parameter density (number of programmable elements/area/process normalized), as well as high accessibility of the computations due to its data flow handling; the SoC FPAA is 600 000 \texttimes{} higher density than other non-FG approaches.},
  keywords = {analog acoustic command-word classifier,Computer architecture,Fabrics,Field programmable analog arrays,field programmable analogue arrays,floating-gate based field-programmable analog array,floating-gate circuits,floating-gate circuits.,FPAA,MSP430 microprocessor,power 23 muW,Program processors,Programming,Routing,SoC,system-on-chip,TIMIT database phrase,word length 16 bit},
  file = {/Users/nvk/Zotero/storage/2Y4NJMK6/George et al. - 2016 - A Programmable and Configurable Mixed-Mode FPAA So.pdf;/Users/nvk/Zotero/storage/IR563A7U/7374749.html}
}

@article{ginevArXMLiv0820182018,
  title = {{{arXMLiv}}: 08.2018 Dataset, an {{HTML5}} Conversion of {{arXiv}}. Org},
  author = {Ginev, Deyan},
  year = {2018},
  journal = {SIGMathLing\textendash Special Interest Group on Math Linguistics}
}

@misc{ginevArXMLiv2020HTML52020,
  title = {{{arXMLiv}} 2020 - {{An HTML5}} Dataset for {{arXiv}}.Org {$\cdot$} {{SIGMathLing}}},
  author = {Ginev, Deyan},
  year = {2020},
  file = {/Users/nvk/Zotero/storage/ZE7LIIJ2/arxmliv-dataset-2020.html}
}

@article{ginevConstructionSemanticFormula2010,
  title = {Construction of {{Semantic Formula Trees}} for {{arXMLiv Mathematical Expressions Project}} Report},
  author = {Ginev, Deyan},
  year = {2010}
}

@inproceedings{ginevLaTeXML2012Year2013,
  title = {{{LaTeXML}} 2012 - {{A Year}} of {{LaTeXML}}},
  booktitle = {Intelligent {{Computer Mathematics}}},
  author = {Ginev, Deyan and Miller, Bruce R.},
  editor = {Carette, Jacques and Aspinall, David and Lange, Christoph and Sojka, Petr and Windsteiger, Wolfgang},
  year = {2013},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {335--338},
  publisher = {{Springer}},
  address = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-642-39320-4_24},
  abstract = {LaTeXML, a TeX to XML converter, is being used in a wide range of MKM applications. In this paper, we present a progress report for the 2012 calendar year. Noteworthy enhancements include: increased coverage such as Wikipedia syntax; enhanced capabilities such as embeddable JavaScript and CSS resources and RDFa support; a web service for remote processing via web-sockets; along with general accuracy and reliability improvements. The outlook for an 0.8.0 release in mid-2013 is also discussed.},
  isbn = {978-3-642-39320-4},
  langid = {english},
  file = {/Users/nvk/Zotero/storage/PEW8L7V6/Ginev and Miller - 2013 - LaTeXML 2012 - A Year of LaTeXML.pdf}
}

@article{ginspargArXiv202011,
  title = {{{ArXiv}} at 20},
  author = {Ginsparg, Paul},
  year = {2011},
  journal = {Nature},
  volume = {476},
  number = {7359},
  pages = {145--147},
  publisher = {{Nature Publishing Group}},
  isbn = {1476-4687}
}

@article{glaunerUseSolrXapian2013,
  title = {Use of {{Solr}} and {{Xapian}} in the {{Invenio}} Document Repository Software},
  author = {Glauner, Patrick O. and Iwaszkiewicz, Jan and Meur, Jean-Yves Le and Simko, Tibor},
  year = {2013},
  journal = {arXiv preprint arXiv:1310.0250},
  eprint = {1310.0250},
  eprinttype = {arxiv},
  archiveprefix = {arXiv}
}

@article{gokmenAccelerationDeepNeural2016,
  title = {Acceleration of {{Deep Neural Network Training}} with {{Resistive Cross-Point Devices}}: {{Design Considerations}}},
  shorttitle = {Acceleration of {{Deep Neural Network Training}} with {{Resistive Cross-Point Devices}}},
  author = {Gokmen, Tayfun and Vlasov, Yurii},
  year = {2016},
  journal = {Frontiers in Neuroscience},
  volume = {10},
  publisher = {{Frontiers}},
  issn = {1662-453X},
  doi = {10.3389/fnins.2016.00333},
  abstract = {In recent years, deep neural networks (DNN) have demonstrated significant business impact in large scale analysis and classification tasks such as speech recognition, visual object detection, pattern extraction, etc. Training of large DNNs, however, is universally considered as time consuming and computationally intensive task that demands datacenter-scale computational resources recruited for many days. Here we propose a concept of resistive processing unit (RPU) devices that can potentially accelerate DNN training by orders of magnitude while using much less power. The proposed RPU device can store and update the weight values locally thus minimizing data movement during training and allowing to fully exploit the locality and the parallelism of the training algorithm. We evaluate the effect of various RPU device features/non-idealities and system parameters on performance in order to derive the device and system level specifications for implementation of an accelerator chip for DNN training in a realistic CMOS-compatible technology. For large DNNs with about 1 billion weights this massively parallel RPU architecture can achieve acceleration factors of 30,000X compared to state-of-the-art microprocessors while providing power efficiency of 84,000 GigaOps/s/W. Problems that currently require days of training on a datacenter-size cluster with thousands of machines can be addressed within hours on a single RPU accelerator. A system consisting of a cluster of RPU accelerators will be able to tackle Big Data problems with trillions of parameters that is impossible to address today like, for example, natural speech recognition and translation between all world languages, real-time analytics on large streams of business and scientific data, integration and analysis of multimodal sensory data flows from a massive number of IoT (Internet of Things) sensors.},
  langid = {english},
  keywords = {artificial intelligence,artificial neural networks,Deep neural network,Electronic devices,machine learning,Materials Engineering,Memristive Devices,Nanotechnology,synaptic device},
  file = {/Users/nvk/Zotero/storage/X4URDIHZ/Gokmen and Vlasov - 2016 - Acceleration of Deep Neural Network Training with .pdf}
}

@article{gokmenAlgorithmTrainingNeural2020,
  title = {Algorithm for {{Training Neural Networks}} on {{Resistive Device Arrays}}},
  author = {Gokmen, Tayfun and Haensch, Wilfried},
  year = {2020},
  month = feb,
  journal = {Frontiers in Neuroscience},
  volume = {14},
  issn = {1662-4548},
  doi = {10.3389/fnins.2020.00103},
  abstract = {Hardware architectures composed of resistive cross-point device arrays can provide significant power and speed benefits for deep neural network training workloads using stochastic gradient descent (SGD) and backpropagation (BP) algorithm. The training accuracy on this imminent analog hardware, however, strongly depends on the switching characteristics of the cross-point elements. One of the key requirements is that these resistive devices must change conductance in a symmetrical fashion when subjected to positive or negative pulse stimuli. Here, we present a new training algorithm, so-called the ``Tiki-Taka'' algorithm, that eliminates this stringent symmetry requirement. We show that device asymmetry introduces an unintentional implicit cost term into the SGD algorithm, whereas in the ``Tiki-Taka'' algorithm a coupled dynamical system simultaneously minimizes the original objective function of the neural network and the unintentional cost term due to device asymmetry in a self-consistent fashion. We tested the validity of this new algorithm on a range of network architectures such as fully connected, convolutional and LSTM networks. Simulation results on these various networks show that the accuracy achieved using the conventional SGD algorithm with symmetric (ideal) device switching characteristics is matched in accuracy achieved using the ``Tiki-Taka'' algorithm with non-symmetric (non-ideal) device switching characteristics. Moreover, all the operations performed on the arrays are still parallel and therefore the implementation cost of this new algorithm on array architectures is minimal; and it maintains the aforementioned power and speed benefits. These algorithmic improvements are crucial to relax the material specification and to realize technologically viable resistive crossbar arrays that outperform digital accelerators for similar training tasks.},
  pmcid = {PMC7054461},
  pmid = {32174807},
  file = {/Users/nvk/Zotero/storage/DWVVZ7NS/Gokmen and Haensch - 2020 - Algorithm for Training Neural Networks on Resistiv.pdf}
}

@inproceedings{gokmenMarriageTrainingInference2019,
  title = {The Marriage of Training and Inference for Scaled Deep Learning Analog Hardware},
  booktitle = {2019 {{IEEE International Electron Devices Meeting}} ({{IEDM}})},
  author = {Gokmen, Tayfun and Rasch, Malte J. and Haensch, Wilfried},
  year = {2019},
  month = dec,
  pages = {22.3.1-22.3.4},
  issn = {2156-017X},
  doi = {10.1109/IEDM19573.2019.8993573},
  abstract = {Resistive crossbar arrays are promising candidates for efficient execution of deep neural network (DNN) inference workloads. The weight matrices of a neural network are mapped to the conductance values on crossbar arrays and then used as vector-matrix multiply engines. Although this mapping seems straightforward, we show that for large scale DNNs the weights must come from a training procedure that accounts for hardware induced constraints, such as ADC, DAC, noise and device fails, for the inference task to run successfully on analog hardware composed of crossbar arrays.},
  keywords = {conductance values,convolutional neural nets,deep learning analog hardware,deep neural network inference workloads,hardware induced constraints,inference task,learning (artificial intelligence),matrix algebra,resistive crossbar arrays,training procedure,vector-matrix multiply engines,weight matrices},
  file = {/Users/nvk/Zotero/storage/W27Q68DF/Gokmen et al. - 2019 - The marriage of training and inference for scaled .pdf;/Users/nvk/Zotero/storage/E7HT5ENB/8993573.html}
}

@article{gokmenTrainingDeepConvolutional2017,
  title = {Training {{Deep Convolutional Neural Networks}} with {{Resistive Cross-Point Devices}}},
  author = {Gokmen, Tayfun and Onen, Murat and Haensch, Wilfried},
  year = {2017},
  journal = {Frontiers in Neuroscience},
  volume = {11},
  publisher = {{Frontiers}},
  issn = {1662-453X},
  doi = {10.3389/fnins.2017.00538},
  abstract = {In a previous work we have detailed the requirements for obtaining maximal deep learning performance benefit by implementing fully connected deep neural networks (DNN) in the form of arrays of resistive devices. Here we extend the concept of Resistive Processing Unit (RPU) devices to convolutional neural networks (CNNs). We show how to map the convolutional layers to fully connected RPU arrays such that the parallelism of the hardware can be fully utilized in all three cycles of the backpropagation algorithm. We find that the noise and bound limitations imposed by the analog nature of the computations performed on the arrays significantly affect the training accuracy of the CNNs. Noise and bound management techniques are presented that mitigate these problems without introducing any additional complexity in the analog circuits and that can be addressed by the digital circuits. In addition, we discuss digitally programmable update management and device variability reduction techniques that can be used selectively for some of the layers in a CNN. We show that a combination of all those techniques enables a successful application of the RPU concept for training CNNs. The techniques discussed here are more general and can be applied beyond CNN architectures and therefore enables applicability of the RPU approach to a large class of neural network architectures.},
  langid = {english},
  keywords = {Convolutional Neural Networks (CNN),deep learning,Deep neural network,Resistive memories,Resistive Processing Unit (RPU),resistive random access memory (RRAM),resistive switching},
  file = {/Users/nvk/Zotero/storage/6RKEGJUP/Gokmen et al. - 2017 - Training Deep Convolutional Neural Networks with R.pdf}
}

@article{gokmenTrainingLSTMNetworks2018,
  title = {Training {{LSTM Networks With Resistive Cross-Point Devices}}},
  author = {Gokmen, Tayfun and Rasch, Malte J. and Haensch, Wilfried},
  year = {2018},
  journal = {Frontiers in Neuroscience},
  volume = {12},
  publisher = {{Frontiers}},
  issn = {1662-453X},
  doi = {10.3389/fnins.2018.00745},
  abstract = {In our previous work we have shown that resistive cross point devices, so called Resistive Processing Unit (RPU) devices, can provide significant power and speed benefits when training deep fully connected networks as well as convolutional neural networks. In this work, we further extend the RPU concept for training recurrent neural networks (RNNs) namely LSTMs. We show that the mapping of recurrent layers is very similar to the mapping of fully connected layers and therefore the RPU concept can potentially provide large acceleration factors for RNNs as well. In addition, we study the effect of various device imperfections and system parameters on training performance. Symmetry of updates becomes even more crucial for RNNs; already a few percent asymmetry results in an increase in the test error compared to the ideal case trained with floating point numbers. Furthermore, the input signal resolution to the device arrays needs to be at least 7 bits for successful training. However, we show that a stochastic rounding scheme can reduce the input signal resolution back to 5 bits. Further, we find that RPU device variations and hardware noise are enough to mitigate overfitting, so that there is less need for using dropout. Here we attempt to study the validity of the RPU approach by simulating large scale networks. For instance, the models studied here are roughly 1500 times larger than the more often studied multilayer perceptron models trained on the MNIST dataset in terms of the total number of multiplication and summation operations performed per epoch.},
  langid = {english},
  keywords = {deep learning,deep neural network training,deep neural networks,Electronic devices,Long Short-Term Memory,machine learning,material engineering,Memristive Devices,Resistive Processing Unit (RPU),resistive switching,synaptic device},
  file = {/Users/nvk/Zotero/storage/I8GQQI4U/Gokmen et al. - 2018 - Training LSTM Networks With Resistive Cross-Point .pdf}
}

@article{gongSignalNoiseExtraction2018,
  title = {Signal and Noise Extraction from Analog Memory Elements for Neuromorphic Computing},
  author = {Gong, N. and Id{\'e}, T. and Kim, S. and Boybat, I. and Sebastian, A. and Narayanan, V. and Ando, T.},
  year = {2018},
  month = may,
  journal = {Nature Communications},
  volume = {9},
  number = {1},
  pages = {2102},
  publisher = {{Nature Publishing Group}},
  issn = {2041-1723},
  doi = {10.1038/s41467-018-04485-1},
  abstract = {Dense crossbar arrays of non-volatile memory (NVM) can potentially enable massively parallel and highly energy-efficient neuromorphic computing systems. The key requirements for the NVM elements are continuous (analog-like) conductance tuning capability and switching symmetry with acceptable noise levels. However, most NVM devices show non-linear and asymmetric switching behaviors. Such non-linear behaviors render separation of signal and noise extremely difficult with conventional characterization techniques. In this study, we establish a practical methodology based on Gaussian process regression to address this issue. The methodology is agnostic to switching mechanisms and applicable to various NVM devices. We show tradeoff between switching symmetry and signal-to-noise ratio for HfO2-based resistive random access memory. Then, we characterize 1000 phase-change memory devices based on Ge2Sb2Te5 and separate total variability into device-to-device variability and inherent randomness from individual devices. These results highlight the usefulness of our methodology to realize ideal NVM devices for neuromorphic computing.},
  copyright = {2018 The Author(s)},
  langid = {english},
  file = {/Users/nvk/Zotero/storage/2S5ZW6XV/Gong et al. - 2018 - Signal and noise extraction from analog memory ele.pdf;/Users/nvk/Zotero/storage/DPMUXCBV/s41467-018-04485-1.html}
}

@book{gottlobfregeArithmetischenNachgebildeteFormelsprache1879,
  title = {Eine Der Arithmetischen Nachgebildete {{Formelsprache}} Des Reinen {{Denkens}}},
  author = {GOTTLOB FREGE, Begriffsschrift},
  year = {1879},
  series = {Nebert, {{Halle}}}
}

@article{goTwitterSentimentClassification,
  title = {Twitter {{Sentiment Classification}} Using {{Distant Supervision}}},
  author = {Go, Alec and Bhayani, Richa and Huang, Lei},
  pages = {6},
  abstract = {We introduce a novel approach for automatically classifying the sentiment of Twitter messages. These messages are classified as either positive or negative with respect to a query term. This is useful for consumers who want to research the sentiment of products before purchase, or companies that want to monitor the public sentiment of their brands. There is no previous research on classifying sentiment of messages on microblogging services like Twitter. We present the results of machine learning algorithms for classifying the sentiment of Twitter messages using distant supervision. Our training data consists of Twitter messages with emoticons, which are used as noisy labels. This type of training data is abundantly available and can be obtained through automated means. We show that machine learning algorithms (Naive Bayes, Maximum Entropy, and SVM) have accuracy above 80\% when trained with emoticon data. This paper also describes the preprocessing steps needed in order to achieve high accuracy. The main contribution of this paper is the idea of using tweets with emoticons for distant supervised learning.},
  langid = {english},
  file = {/Users/nvk/Zotero/storage/WQC4TXD7/Go et al. - Twitter Sentiment Classiï¬cation using Distant Supe.pdf}
}

@inproceedings{grafSubstitutionTreeIndexing1995,
  title = {Substitution Tree Indexing},
  booktitle = {Rewriting {{Techniques}} and {{Applications}}},
  author = {Graf, Peter},
  year = {1995},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {117--131},
  publisher = {{Springer}},
  address = {{Berlin, Heidelberg}},
  doi = {10.1007/3-540-59200-8_52},
  abstract = {Sophisticated maintenance and retrieval of first-order predicate calculus terms is a major key to efficient automated reasoning. We present a new indexing technique, which accelerates the speed of the basic retrieval operations such as finding complementary literals in resolution theorem proving or finding critical pairs during completion. Subsumption and reduction are also supported. Moreover, the new technique not only provides maintenance and efficient retrieval of terms but also of idem-potent substitutions. Substitution trees achieve maximal search speed paired with minimal memory requirements in various experiments and outperform traditional techniques such as path indexing, discrimination tree indexing, and abstraction trees by combining their advantages and adding some new features.},
  isbn = {978-3-540-49223-8},
  langid = {english},
  keywords = {Indexing Technique,Leaf Node,Retrieval Algorithm,Tree Indexing,Variable Binding},
  file = {/Users/nvk/Zotero/storage/JIYT2MAL/Graf - 1995 - Substitution tree indexing.pdf}
}

@article{greffLSTMSearchSpace2017,
  title = {{{LSTM}}: {{A Search Space Odyssey}}},
  shorttitle = {{{LSTM}}},
  author = {Greff, Klaus and Srivastava, Rupesh Kumar and Koutn{\'i}k, Jan and Steunebrink, Bas R. and Schmidhuber, J{\"u}rgen},
  year = {2017},
  month = oct,
  journal = {IEEE Transactions on Neural Networks and Learning Systems},
  volume = {28},
  number = {10},
  eprint = {1503.04069},
  eprinttype = {arxiv},
  pages = {2222--2232},
  issn = {2162-237X, 2162-2388},
  doi = {10.1109/TNNLS.2016.2582924},
  abstract = {Several variants of the Long Short-Term Memory (LSTM) architecture for recurrent neural networks have been proposed since its inception in 1995. In recent years, these networks have become the state-of-the-art models for a variety of machine learning problems. This has led to a renewed interest in understanding the role and utility of various computational components of typical LSTM variants. In this paper, we present the first large-scale analysis of eight LSTM variants on three representative tasks: speech recognition, handwriting recognition, and polyphonic music modeling. The hyperparameters of all LSTM variants for each task were optimized separately using random search, and their importance was assessed using the powerful fANOVA framework. In total, we summarize the results of 5400 experimental runs (\$\textbackslash approx 15\$ years of CPU time), which makes our study the largest of its kind on LSTM networks. Our results show that none of the variants can improve upon the standard LSTM architecture significantly, and demonstrate the forget gate and the output activation function to be its most critical components. We further observe that the studied hyperparameters are virtually independent and derive guidelines for their efficient adjustment.},
  archiveprefix = {arXiv},
  keywords = {68T10,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,H.5.5,I.2.6,I.2.7,I.5.1},
  file = {/Users/nvk/Zotero/storage/352AH5GG/Greff et al. - 2017 - LSTM A Search Space Odyssey.pdf;/Users/nvk/Zotero/storage/YMS3UC4X/1503.html}
}

@inproceedings{greggWhereDataWhy2011,
  title = {Where Is the Data? {{Why}} You Cannot Debate {{CPU}} vs. {{GPU}} Performance without the Answer},
  shorttitle = {Where Is the Data?},
  booktitle = {({{IEEE ISPASS}}) {{IEEE International Symposium}} on {{Performance Analysis}} of {{Systems}} and {{Software}}},
  author = {Gregg, Chris and Hazelwood, Kim},
  year = {2011},
  month = apr,
  pages = {134--144},
  doi = {10.1109/ISPASS.2011.5762730},
  abstract = {General purpose GPU Computing (GPGPU) has taken off in the past few years, with great promises for increased desktop processing power due to the large number of fast computing cores on high-end graphics cards. Many publications have demonstrated phenomenal performance and have reported speedups as much as 1000\texttimes{} over code running on multi-core CPUs. Other studies have claimed that well-tuned CPU code reduces the performance gap significantly. We demonstrate that this important discussion is missing a key aspect, specifically the question of where in the system data resides, and the overhead to move the data to where it will be used, and back again if necessary. We have benchmarked a broad set of GPU kernels on a number of platforms with different GPUs and our results show that when memory transfer times are included, it can easily take between 2 to 50\texttimes{} longer to run a kernel than the GPU processing time alone. Therefore, it is necessary to either include memory transfer overhead when reporting GPU performance, or to explain why this is not relevant for the application in question. We suggest a taxonomy for future CPU/GPU comparisons, and we argue that this is not only germane for reporting performance, but is important to heterogeneous scheduling research in general.},
  keywords = {Bandwidth,Benchmark testing,computer graphic equipment,coprocessors,Databases,desktop processing power,fast computing cores,general purpose computers,general purpose GPU computing,GPU kernels,Graphics processing unit,high-end graphics cards,Histograms,Kernel,memory transfer times,microprocessor chips,multicore CPUs,multiprocessing systems,Performance evaluation,system data},
  file = {/Users/nvk/Zotero/storage/YJR2YWW9/Gregg and Hazelwood - 2011 - Where is the data Why you cannot debate CPU vs. G.pdf;/Users/nvk/Zotero/storage/J5R9XMH9/5762730.html}
}

@inproceedings{greiner-petterDiscoveringMathematicalObjects2020,
  title = {Discovering {{Mathematical Objects}} of {{Interest}}\&\#x2014;{{A Study}} of {{Mathematical Notations}}},
  booktitle = {Proceedings of {{The Web Conference}} 2020},
  author = {{Greiner-Petter}, Andr{\'e} and Schubotz, Moritz and M{\"u}ller, Fabian and Breitinger, Corinna and Cohl, Howard and Aizawa, Akiko and Gipp, Bela},
  year = {2020},
  month = apr,
  series = {{{WWW}} '20},
  pages = {1445--1456},
  publisher = {{Association for Computing Machinery}},
  address = {{Taipei, Taiwan}},
  doi = {10.1145/3366423.3380218},
  abstract = {Mathematical notation, i.e., the writing system used to communicate concepts in mathematics, encodes valuable information for a variety of information search and retrieval systems. Yet, mathematical notations remain mostly unutilized by today's systems. In this paper, we present the first in-depth study on the distributions of mathematical notation in two large scientific corpora: the open access arXiv (2.5B mathematical objects) and the mathematical reviewing service for pure and applied mathematics zbMATH (61M mathematical objects). Our study lays a foundation for future research projects on mathematical information retrieval for large scientific corpora. Further, we demonstrate the relevance of our results to a variety of use-cases. For example, to assist semantic extraction systems, to improve scientific search engines, and to facilitate specialized math recommendation systems. The contributions of our presented research are as follows: (1) we present the first distributional analysis of mathematical formulae on arXiv and zbMATH; (2) we retrieve relevant mathematical objects for given textual search queries (e.g., linking with `Jacobi polynomial'); (3) we extend zbMATH's search engine by providing relevant mathematical formulae; and (4) we exemplify the applicability of the results by presenting auto-completion for math inputs as the first contribution to math recommendation systems. To expedite future research projects, we have made available our source code and data.},
  isbn = {978-1-4503-7023-3},
  keywords = {Distributions of Mathematical Objects,Mathematical Information Retrieval,Mathematical Objects of Interest,Mathematical Search Engine,Term Frequency-Inverse Document Frequency},
  file = {/Users/nvk/Zotero/storage/TY6TMX88/Greiner-Petter et al. - 2020 - Discovering Mathematical Objects of Interest&#x201.pdf}
}

@article{greiner-petterMathwordEmbeddingMath2020,
  title = {Math-Word Embedding in Math Search and Semantic Extraction},
  author = {{Greiner-Petter}, Andr{\'e} and Youssef, Abdou and Ruas, Terry and Miller, Bruce R. and Schubotz, Moritz and Aizawa, Akiko and Gipp, Bela},
  year = {2020},
  month = dec,
  journal = {Scientometrics},
  volume = {125},
  number = {3},
  pages = {3017--3046},
  issn = {1588-2861},
  doi = {10.1007/s11192-020-03502-9},
  abstract = {Word embedding, which represents individual words with semantically fixed-length vectors, has made it possible to successfully apply deep learning to natural language processing tasks such as semantic role-modeling, question answering, and machine translation. As math text consists of natural text, as well as math expressions that similarly exhibit linear correlation and contextual characteristics, word embedding techniques can also be applied to math documents. However, while mathematics is a precise and accurate science, it is usually expressed through imprecise and less accurate descriptions, contributing to the relative dearth of machine learning applications for information retrieval in this domain. Generally, mathematical documents communicate their knowledge with an ambiguous, context-dependent, and non-formal language. Given recent advances in word embedding, it is worthwhile to explore their use and effectiveness in math information retrieval tasks, such as math language processing and semantic knowledge extraction. In this paper, we explore math embedding by testing it on several different scenarios, namely, (1) math-term similarity, (2) analogy, (3) numerical concept-modeling based on the centroid of the keywords that characterize a concept, (4) math search using query expansions, and (5) semantic extraction, i.e., extracting descriptive phrases for math expressions. Due to the lack of benchmarks, our investigations were performed using the arXiv collection of STEM documents and carefully selected illustrations on the Digital Library of Mathematical Functions (DLMF: NIST digital library of mathematical functions. Release 1.0.20 of 2018-09-1, 2018). Our results show that math embedding holds much promise for similarity, analogy, and search tasks. However, we also observed the need for more robust math embedding approaches. Moreover, we explore and discuss fundamental issues that we believe thwart the progress in mathematical information retrieval in the direction of machine learning.},
  langid = {english},
  file = {/Users/nvk/Zotero/storage/F9ZCKPDG/Greiner-Petter et al. - 2020 - Math-word embedding in math search and semantic ex.pdf}
}

@article{greiner-petterWhyMachinesCannot2019,
  title = {Why {{Machines Cannot Learn Mathematics}}, {{Yet}}},
  author = {{Greiner-Petter}, Andr{\'e} and Ruas, Terry and Schubotz, Moritz and Aizawa, Akiko and Grosky, William and Gipp, Bela},
  year = {2019},
  month = may,
  journal = {arXiv:1905.08359 [cs]},
  eprint = {1905.08359},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {Nowadays, Machine Learning (ML) is seen as the universal solution to improve the effectiveness of information retrieval (IR) methods. However, while mathematics is a precise and accurate science, it is usually expressed by less accurate and imprecise descriptions, contributing to the relative dearth of machine learning applications for IR in this domain. Generally, mathematical documents communicate their knowledge with an ambiguous, context-dependent, and non-formal language. Given recent advances in ML, it seems canonical to apply ML techniques to represent and retrieve mathematics semantically. In this work, we apply popular text embedding techniques to the arXiv collection of STEM documents and explore how these are unable to properly understand mathematics from that corpus. In addition, we also investigate the missing aspects that would allow mathematics to be learned by computers.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Digital Libraries,Computer Science - Information Retrieval},
  file = {/Users/nvk/Zotero/storage/X8M34W3R/Greiner-Petter et al. - 2019 - Why Machines Cannot Learn Mathematics, Yet.pdf;/Users/nvk/Zotero/storage/79ZPYANX/1905.html}
}

@article{guidiSurveyRetrievalMathematical2016,
  title = {A {{Survey}} on {{Retrieval}} of {{Mathematical Knowledge}}},
  author = {Guidi, Ferruccio and Sacerdoti Coen, Claudio},
  year = {2016},
  month = dec,
  journal = {Mathematics in Computer Science},
  volume = {10},
  number = {4},
  pages = {409--427},
  issn = {1661-8289},
  doi = {10.1007/s11786-016-0274-0},
  abstract = {We present a survey of the literature on indexing and retrieval of mathematical knowledge, with pointers to 77 papers and tentative taxonomies of both retrieval problems and recurring techniques.},
  langid = {english},
  file = {/Users/nvk/Zotero/storage/ALI7I6UF/Guidi and Sacerdoti Coen - 2016 - A Survey on Retrieval of Mathematical Knowledge.pdf}
}

@article{haenschNextGenerationDeep2019,
  title = {The {{Next Generation}} of {{Deep Learning Hardware}}: {{Analog Computing}}},
  shorttitle = {The {{Next Generation}} of {{Deep Learning Hardware}}},
  author = {Haensch, Wilfried and Gokmen, Tayfun and Puri, Ruchir},
  year = {2019},
  month = jan,
  journal = {Proceedings of the IEEE},
  volume = {107},
  number = {1},
  pages = {108--122},
  issn = {1558-2256},
  doi = {10.1109/JPROC.2018.2871057},
  abstract = {Initially developed for gaming and 3-D rendering, graphics processing units (GPUs) were recognized to be a good fit to accelerate deep learning training. Its simple mathematical structure can easily be parallelized and can therefore take advantage of GPUs in a natural way. Further progress in compute efficiency for deep learning training can be made by exploiting the more random and approximate nature of deep learning work flows. In the digital space that means to trade off numerical precision for accuracy at the benefit of compute efficiency. It also opens the possibility to revisit analog computing, which is intrinsically noisy, to execute the matrix operations for deep learning in constant time on arrays of nonvolatile memories. To take full advantage of this in-memory compute paradigm, current nonvolatile memory materials are of limited use. A detailed analysis and design guidelines how these materials need to be reengineered for optimal performance in the deep learning space shows a strong deviation from the materials used in memory applications.},
  keywords = {analog computing,Analog computing,analogue computer circuits,analogue simulation,Convolution,current nonvolatile memory materials,deep learning,Deep learning,deep learning hardware,deep learning space,deep learning training,deep learning work,graphics processing units,Graphics processing units,Hardware,in-memory compute paradigm,learning (artificial intelligence),Machine learning,neural network,Neural networks,neuromorphic computing,nonvolatile memory,Nonvolatile memory,synapse},
  file = {/Users/nvk/Zotero/storage/MMEXRX44/Haensch et al. - 2019 - The Next Generation of Deep Learning Hardware Ana.pdf;/Users/nvk/Zotero/storage/2YWXB8LC/8490883.html}
}

@article{haiSpatialDataFulltext2017,
  title = {Spatial {{Data Full-text Retrieval Method Based}} on {{Whoosh}}},
  author = {Hai, ZHOU},
  year = {2017},
  journal = {Geospatial Information},
  number = {11},
  pages = {6}
}

@article{hambasanMathWebSearchNTCIR12014,
  title = {{{MathWebSearch}} at {{NTCIR-1}}},
  author = {Hambasan, Radu and Kohlhase, Michael and Prodescu, Corneliu},
  year = {2014},
  pages = {6},
  abstract = {We present and analyze the results of the MATHWEBSEARCH (MWS) system in the Math-2 task in the NTCIR-11 Information Retrieval challenge. MWS is a content-based full-text search engine that focuses on low-latency query answering for interactive applications. It combines a powerful exact formula unification/matching with the fulltext search capabilities of ElasticSearch to achieve simultaneous full-text search for mathematical/technical documents.},
  langid = {english},
  file = {/Users/nvk/Zotero/storage/HPEJLVD2/Hambasan et al. - 2014 - MathWebSearch at NTCIR-1.pdf}
}

@article{hanFastNumericalMethod2003,
  title = {A {{Fast Numerical Method}} for the {{Black--Scholes Equation}} of {{American Options}}},
  author = {Han, Houde and Wu, Xiaonan},
  year = {2003},
  month = jan,
  journal = {SIAM Journal on Numerical Analysis},
  volume = {41},
  number = {6},
  pages = {2081--2095},
  publisher = {{Society for Industrial and Applied Mathematics}},
  issn = {0036-1429},
  doi = {10.1137/S0036142901390238},
  abstract = {This paper introduces a fast numerical method for computing American option pricing problems governed by the Black--Scholes equation. The treatment of the free boundary is based on some properties of the solution of the Black--Scholes equation. An artificial boundary condition is also used at the other end of the domain. The finite difference method is used to solve the resulting problem. Computational results are given for some American call option problems. The results show that the new treatment is very efficient and gives better accuracy than the normal finite difference method.},
  keywords = {35A35,35A40,65N99,American option,artificial boundary condition,finite difference method,free boundary},
  file = {/Users/nvk/Zotero/storage/A42RU2UP/Han and Wu - 2003 - A Fast Numerical Method for the Black--Scholes Equ.pdf}
}

@article{hargittaiStephenWolframIdea2016,
  title = {Stephen {{Wolfram}}, {{Idea Makers}}: {{Personal Perspectives}} on the {{Lives}} \& {{Ideas}} of {{Some Notable People}}},
  shorttitle = {Stephen {{Wolfram}}, {{Idea Makers}}},
  author = {Hargittai, Istvan},
  year = {2016},
  month = dec,
  journal = {Structural Chemistry},
  volume = {27},
  number = {6},
  pages = {1865--1867},
  issn = {1572-9001},
  doi = {10.1007/s11224-016-0826-6},
  langid = {english},
  file = {/Users/nvk/Zotero/storage/9SXKGITQ/Hargittai - 2016 - Stephen Wolfram, Idea Makers Personal Perspective.pdf}
}

@article{harrisDistributionalStructure1954,
  title = {Distributional {{Structure}}},
  author = {Harris, Zellig S.},
  year = {1954},
  month = aug,
  journal = {\emph{WORD}},
  volume = {10},
  number = {2-3},
  pages = {146--162},
  issn = {0043-7956, 2373-5112},
  doi = {10.1080/00437956.1954.11659520},
  langid = {english},
  file = {/Users/nvk/Zotero/storage/XWY7JU5N/Harris - 1954 - Distributional Structure.pdf}
}

@article{hirschInteractiveVisualizationTools,
  title = {Interactive {{Visualization Tools}} for {{Exploring}} the {{Semantic Graph}} of {{Large Knowledge Spaces}}},
  author = {Hirsch, Christian and Hosking, John and Grundy, John},
  pages = {6},
  abstract = {While the amount of available information on the Web is increasing rapidly, the problem of managing it becomes more difficult. We present two applications, Thinkbase and Thinkpedia, which aim to make Web content more accessible and usable by utilizing visualizations of the semantic graph as a means to navigate and explore large knowledge repositories. Both of our applications implement a similar concept: They extract semantically enriched contents from a large knowledge spaces (Freebase and Wikipedia respectively), create an interactive graph-based representation out of it, and combine them into one interface together with the original text based content. We describe the design and implementation of our applications, and provide a discussion based on an informal evaluation.},
  langid = {english},
  file = {/Users/nvk/Zotero/storage/DQSAEVKK/Hirsch et al. - Interactive Visualization Tools for Exploring the .pdf}
}

@article{hirschmanNaturalLanguageQuestion2001,
  title = {Natural Language Question Answering: The View from Here},
  shorttitle = {Natural Language Question Answering},
  author = {Hirschman, L. and Gaizauskas, R.},
  year = {2001},
  month = dec,
  journal = {Natural Language Engineering},
  volume = {7},
  number = {4},
  pages = {275--300},
  publisher = {{Cambridge University Press}},
  issn = {1469-8110, 1351-3249},
  doi = {10.1017/S1351324901002807},
  abstract = {As users struggle to navigate the wealth of on-line information now available, the  need for automated question answering systems becomes more urgent. We need  systems that allow a user to ask a question in everyday language and receive an  answer quickly and succinctly, with sufficient context to validate the answer. Current  search engines can return ranked lists of documents, but they do not deliver answers  to the user.Question answering systems address this problem. Recent successes have been  reported in a series of question-answering evaluations that started in 1999 as part  of the Text Retrieval Conference (TREC). The best systems are now able to answer  more than two thirds of factual questions in this evaluation.},
  langid = {english},
  file = {/Users/nvk/Zotero/storage/YZT8C9RV/Hirschman and Gaizauskas - 2001 - Natural language question answering the view from.pdf;/Users/nvk/Zotero/storage/H5SYWHKY/95EA883AFC7EB2B8EC050D3920F39DE2.html}
}

@article{hirschThinkbaseVisualSemantic,
  title = {Thinkbase: {{A Visual Semantic Wiki}}},
  author = {Hirsch, Christian and Grundy, John and Hosking, John},
  pages = {2},
  abstract = {Thinkbase is a visual navigation and exploration tool for Freebase, an open, shared database of the world's knowledge. Thinkbase extracts the contents, including semantic relationships, from Freebase and visualizes them using an interactive visual representation. Providing a focus plus context view the visualization is displayed along with the Freebase article. Thinkbase provides a proof of concept of how visualizations can improve and support Semantic Web applications. The application is available via http://thinkbase.cs.auckland.ac.nz.},
  langid = {english},
  file = {/Users/nvk/Zotero/storage/XYHAK6IN/Hirsch et al. - Thinkbase A Visual Semantic Wiki.pdf}
}

@inproceedings{hovySocialImpactNatural2016,
  title = {The {{Social Impact}} of {{Natural Language Processing}}},
  booktitle = {Proceedings of the 54th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}} ({{Volume}} 2: {{Short Papers}})},
  author = {Hovy, Dirk and Spruit, Shannon L.},
  year = {2016},
  month = aug,
  pages = {591--598},
  publisher = {{Association for Computational Linguistics}},
  address = {{Berlin, Germany}},
  doi = {10.18653/v1/P16-2096},
  file = {/Users/nvk/Zotero/storage/T52YMLPR/Hovy and Spruit - 2016 - The Social Impact of Natural Language Processing.pdf}
}

@article{huangHighlyParallelEnergy2018,
  title = {A {{Highly Parallel}} and {{Energy Efficient Three-Dimensional Multilayer CMOS-RRAM Accelerator}} for {{Tensorized Neural Network}}},
  author = {Huang, Hantao and Ni, Leibin and Wang, Kanwen and Wang, Yuangang and Yu, Hao},
  year = {2018},
  month = jul,
  journal = {IEEE Transactions on Nanotechnology},
  volume = {17},
  number = {4},
  pages = {645--656},
  issn = {1941-0085},
  doi = {10.1109/TNANO.2017.2732698},
  abstract = {It is a grand challenge to develop highly parallel yet energy-efficient machine learning hardware accelerator. This paper introduces a three-dimensional (3-D) multilayer CMOSRRAM accelerator for atensorized neural network. Highly parallel matrix-vector multiplication can be performed with low power in the proposed 3-D multilayer CMOS-RRAM accelerator. The adoption of tensorization can significantly compress the weight matrix of a neural network using much fewer parameters. Simulation results using the benchmark MNIST show that the proposed accelerator has 1.283\texttimes{} speed-up, 4.276\texttimes{} energy-saving, and 9.339\texttimes{} area-saving compared to the 3-D CMOS-ASIC implementation; and 6.37\texttimes{} speed-up and 2612\texttimes{} energy-saving compared to 2-D CPU implementation. In addition, 14.85\texttimes{} model compression can be achieved by tensorization with acceptable accuracy loss.},
  keywords = {3-D multilayer CMOS-RRAM accelerator,application specific integrated circuits,energy conservation,energy efficient three-dimensional multilayer CMOS-RRAM accelerator,energy-efficient machine learning hardware accelerator,Hardware,highly parallel multilayer CMOS-RRAM accelerator,learning (artificial intelligence),low-power electronics,Matrix decomposition,matrix multiplication,neural nets,Neural networks,parallel matrix-vector multiplication,parallel processing,power aware computing,resistive RAM,resistive random access memory,RRAM computing,Tensile stress,tensorization,tensorized neural network,tensorized neural network (TNN),tensors,three-dimensional (3-D) accelerator,Three-dimensional displays,Training,Two dimensional displays,vectors,weight matrix},
  file = {/Users/nvk/Zotero/storage/VAWDPAPM/Huang et al. - 2018 - A Highly Parallel and Energy Efficient Three-Dimen.pdf;/Users/nvk/Zotero/storage/ZLDQS2SJ/7994714.html}
}

@article{hughesReconsideringLanguageIdentification2006,
  title = {Reconsidering Language Identification for Written Language Resources},
  author = {Hughes, Baden and Baldwin, Timothy and Bird, Steven and Nicholson, Jeremy and Mackinlay, Andrew},
  year = {2006},
  publisher = {{European Language Resources Association}},
  abstract = {The task of identifying the language in which a given document (ranging from a sentence to thousands of pages) is written has been relatively well studied over several decades. Automated approaches to written language identification are used widely throughout research and industrial contexts, over both oral and written source materials. Despite this widespread acceptance, a review of previous research in written language identification reveals a number of questions which remain open and ripe for further investigation.},
  langid = {english},
  file = {/Users/nvk/Zotero/storage/PJ3TEQZS/Hughes et al. - 2006 - Reconsidering language identification for written .pdf;/Users/nvk/Zotero/storage/USXXBDSV/48dfafda-c933-58ec-b13b-90d38df36a82.html}
}

@article{hussainRetrievalMathematicalInformation,
  title = {Retrieval of {{Mathematical Information}} with {{Syntactic}} and {{Semantic Structure}} over {{Web}}},
  author = {Hussain, Sharaf and Khoja, Shakeel},
  pages = {23},
  abstract = {Efficient retrieval of mathematical expressions over web is a complex process as compared to simple text search. This is only possible when the syntactic (e.g. Textual) and semantic (e.g. Structural) information of a mathematical expression is retrieved properly and analyzed methodically. In this paper, we are proposing a technique that indexes expressions along with their syntactic and semantic information. These expressions are represented in ContentMathML(CMML). To improve the memory efficiency in index, an encoding technique is introduced which encode CMML mathematical expressions in Braille Unicode characters. In order to improve ranking of retrieved documents, a weighting function is introduced which assign a weight to each indexing term. The weighting score of each term contributes in ranking function that improves the rank of a document which contains query terms. The proposed technique is evaluated on NTCIR-12 Wikipedia and Arxiv corpora. Performance is also measured using NTCIR-MathIR evaluation criteria. The precision for Wikipedia-formula-queries is achieved 47\% and for Arxiv is achieved 44\% at top 5 documents.},
  langid = {english},
  file = {/Users/nvk/Zotero/storage/6IGSQT3N/Hussain and Khoja - Retrieval of Mathematical Information with Syntact.pdf}
}

@article{hutchinsDirectManipulationInterfaces1985,
  title = {Direct {{Manipulation Interfaces}}},
  author = {Hutchins, Edwin L. and Hollan, James D. and Norman, Donald A.},
  year = {1985},
  month = dec,
  journal = {Human\textendash Computer Interaction},
  volume = {1},
  number = {4},
  pages = {311--338},
  publisher = {{Taylor \& Francis}},
  issn = {0737-0024},
  doi = {10.1207/s15327051hci0104_2},
  abstract = {Direct manipulation has been lauded as a good form of interface design, and some interfaces that have this property have been well received by users. In this article we seek a cognitive account of both the advantages and disadvantages of direct manipulation interfaces. We identify two underlying phenomena that give rise to the feeling of directness. One deals with the information processing distance between the user's intentions and the facilities provided by the machine. Reduction of this distance makes the interface feel direct by reducing the effort required of the user to accomplish goals. The second phenomenon concerns the relation between the input and output vocabularies of the interface language. In particular, direct manipulation requires that the system provide representations of objects that behave as if they are the objects themselves. This provides the feeling of directness of manipulation.},
  annotation = {\_eprint: https://doi.org/10.1207/s15327051hci0104\_2},
  file = {/Users/nvk/Zotero/storage/JARMVE8W/Hutchins et al. - 1985 - Direct Manipulation Interfaces.pdf;/Users/nvk/Zotero/storage/MFZFABEJ/S15327051HCI0104_2.html}
}

@inproceedings{huWikiMirsMathematicalInformation2013,
  title = {{{WikiMirs}}: A Mathematical Information Retrieval System for Wikipedia},
  shorttitle = {{{WikiMirs}}},
  booktitle = {Proceedings of the 13th {{ACM}}/{{IEEE-CS}} Joint Conference on {{Digital}} Libraries},
  author = {Hu, Xuan and Gao, Liangcai and Lin, Xiaoyan and Tang, Zhi and Lin, Xiaofan and Baker, Josef B.},
  year = {2013},
  month = jul,
  series = {{{JCDL}} '13},
  pages = {11--20},
  publisher = {{Association for Computing Machinery}},
  address = {{New York, NY, USA}},
  doi = {10.1145/2467696.2467699},
  abstract = {Mathematical formulae in structural formats such as MathML and LaTeX are becoming increasingly available. Moreover, repositories and websites, including ArXiv and Wikipedia, and growing numbers of digital libraries use these structural formats to present mathematical formulae. This presents an important new and challenging area of research, namely Mathematical Information Retrieval (MIR). In this paper, we propose WikiMirs, a tool to facilitate mathematical formula retrieval in Wikipedia. WikiMirs is aimed at searching for similar mathematical formulae based upon both textual and spatial similarities, using a new indexing and matching model developed for layout structures. A hierarchical generalization technique is proposed to generate sub-trees from presentation trees of mathematical formulae, and similarity is calculated based upon matching at different levels of these trees. Experimental results show that WikiMirs can efficiently support sub-structure matching and similarity matching of mathematical formulae. Moreover, WikiMirs obtains both higher accuracy and better ranked results over Wikipedia in comparison to Wikipedia Search and Egomath. We conclude that WikiMirs provides a new, alternative, and hopefully better service for users to search mathematical expressions within Wikipedia.},
  isbn = {978-1-4503-2077-1},
  keywords = {mathematical information retrieval,mathematical resource management,structure matching},
  file = {/Users/nvk/Zotero/storage/NSK4GDEL/Hu et al. - 2013 - WikiMirs a mathematical information retrieval sys.pdf}
}

@misc{ImprovingRepresentationConversion,
  title = {Improving the {{Representation}} and {{Conversion}} of {{Mathematical Formulae}} by {{Considering}} Their {{Textual Context}} | {{Proceedings}} of the 18th {{ACM}}/{{IEEE}} on {{Joint Conference}} on {{Digital Libraries}}},
  howpublished = {https://dl.acm.org/doi/abs/10.1145/3197026.3197058?casa\_token=asImfxEaYbEAAAAA:roeseqqXfoXMU8buIC-aE9X7B9gWFS5aJmdiCcc1Lj0dFzOkRqzKewhpGFt4TG1-TDyUgYtYx-Zf},
  file = {/Users/nvk/Zotero/storage/N83367QP/3197026.html}
}

@misc{InformationIntelligentSystems,
  title = {Information and {{Intelligent Systems}} ({{IIS}}): {{Core Programs}} (Nsf18570) | {{NSF}} \textendash{} {{National Science Foundation}}},
  howpublished = {https://www.nsf.gov/pubs/2018/nsf18570/nsf18570.htm},
  file = {/Users/nvk/Zotero/storage/SLJXJYSZ/nsf18570.html}
}

@article{ingberStatisticalMechanicsNonlinear1984,
  title = {Statistical Mechanics of Nonlinear Nonequilibrium Financial Markets},
  author = {Ingber, Lester},
  year = {1984},
  month = jan,
  journal = {Mathematical Modelling},
  volume = {5},
  number = {6},
  pages = {343--361},
  issn = {0270-0255},
  doi = {10.1016/0270-0255(84)90022-8},
  abstract = {An approach to understanding the nature of markets is modelled using methods of modern nonlinear nonequilibrium statistical mechanics. This permits examination of the premise that markets can be described by nonlinear nonequilibrium Markovian distributions. Corrections to previous nonlinear continuous time models are explicitly presented. A quite general microscopic model is presented of individual agents operating on a market, and explicit relationships are derived between variables describing these agents and the macroscopic market.},
  langid = {english},
  file = {/Users/nvk/Zotero/storage/XZ3CFCZC/Ingber - 1984 - Statistical mechanics of nonlinear nonequilibrium .pdf}
}

@misc{InnovationsGraduateEducation,
  title = {Innovations in {{Graduate Education Program}}},
  journal = {Beta site for NSF - National Science Foundation},
  howpublished = {https://beta.nsf.gov/funding/opportunities/innovations-graduate-education-program},
  langid = {english}
}

@misc{InputStackBoxes,
  title = {Input\_stack\_boxes - {{Google Drawings}}},
  journal = {Google Docs},
  howpublished = {https://docs.google.com/drawings/u/0/d/1gMU8t0NUs4u\_mgvwJ81bTZQ3SXwxFPx0xDIgOSGBKYk/edit?usp=embed\_facebook},
  langid = {english},
  file = {/Users/nvk/Zotero/storage/8YJSIHTC/edit.html}
}

@misc{InternationalMathematicalKnowledge,
  title = {International {{Mathematical Knowledge Trust}}},
  journal = {IMKT},
  howpublished = {https://imkt.org/},
  langid = {american},
  file = {/Users/nvk/Zotero/storage/ZXPGHTA4/imkt.org.html}
}

@misc{IntroductionInformationRetrieval,
  title = {Introduction to {{Information Retrieval}}},
  howpublished = {https://nlp.stanford.edu/IR-book/information-retrieval-book.html}
}

@misc{IntroductionInformationRetrievala,
  title = {Introduction to {{Information Retrieval}}},
  howpublished = {https://nlp.stanford.edu/IR-book/information-retrieval-book.html},
  file = {/Users/nvk/Zotero/storage/878WNND9/information-retrieval-book.html}
}

@book{ionMathematicalMarkupLanguage1998,
  title = {Mathematical {{Markup Language}} ({{MathML}}) 1.0 {{Specification}}},
  author = {Ion, Patrick and Miner, Robert and Buswell, Stephen and Devitt, A.},
  year = {1998},
  publisher = {{World Wide Web Consortium (W3C)}}
}

@article{jainNeuralNetworkAccelerator2019,
  title = {Neural Network Accelerator Design with Resistive Crossbars: {{Opportunities}} and Challenges},
  shorttitle = {Neural Network Accelerator Design with Resistive Crossbars},
  author = {Jain, S. and Ankit, A. and Chakraborty, I. and Gokmen, T. and Rasch, M. and Haensch, W. and Roy, K. and Raghunathan, A.},
  year = {2019},
  month = nov,
  journal = {IBM Journal of Research and Development},
  volume = {63},
  number = {6},
  pages = {10:1-10:13},
  issn = {0018-8646},
  doi = {10.1147/JRD.2019.2947011},
  abstract = {Deep neural networks (DNNs) achieve best-known accuracies in many machine learning tasks involved in image, voice, and natural language processing and are being used in an ever-increasing range of applications. However, their algorithmic benefits are accompanied by extremely high computation and storage costs, sparking intense efforts in optimizing the design of computing platforms for DNNs. Today, graphics processing units (GPUs) and specialized digital CMOS accelerators represent the state-of-the-art in DNN hardware, with near-term efforts focusing on approximate computing through reduced precision. However, the ever-increasing complexities of DNNs and the data they process have fueled an active interest in alternative hardware fabrics that can deliver the next leap in efficiency. Resistive crossbars designed using emerging nonvolatile memory technologies have emerged as a promising candidate building block for future DNN hardware fabrics since they can natively execute massively parallel vector-matrix multiplications (the dominant compute kernel in DNNs) in the analog domain within the memory arrays. Leveraging in-memory computing and dense storage, resistive-crossbar-based systems cater to both the high computation and storage demands of complex DNNs and promise energy efficiency beyond current DNN accelerators by mitigating data transfer and memory bottlenecks. However, several design challenges need to be addressed to enable their adoption. For example, the overheads of peripheral circuits (analog-to-digital converters and digital-to-analog converters) and other components (scratchpad memories and on-chip interconnect) may significantly diminish the efficiency benefits at the system level. Additionally, the analog crossbar computations are intrinsically subject to noise due to a range of device- and circuit-level nonidealities, potentially leading to lower accuracy at the application level. In this article, we highlight the prospects for designing hardware accelerators for neural networks using resistive crossbars. We also underscore the key open challenges and some possible approaches to address them.},
  keywords = {Encoding,Hardware,Performance evaluation,Programming,Task analysis,Training,Virtual machine monitors},
  file = {/Users/nvk/Zotero/storage/CRHL2HWP/Jain et al. - 2019 - Neural network accelerator design with resistive c.pdf;/Users/nvk/Zotero/storage/KVZKLBTT/8865106.html}
}

@article{jainRxNNFrameworkEvaluating2020,
  title = {{{RxNN}}: {{A Framework}} for {{Evaluating Deep Neural Networks}} on {{Resistive Crossbars}}},
  shorttitle = {{{RxNN}}},
  author = {Jain, Shubham and Sengupta, Abhronil and Roy, Kaushik and Raghunathan, Anand},
  year = {2020},
  month = jun,
  journal = {arXiv:1809.00072 [cs]},
  eprint = {1809.00072},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {Resistive crossbars designed with non-volatile memory devices have emerged as promising building blocks for Deep Neural Network (DNN) hardware, due to their ability to compactly and efficiently realize vector-matrix multiplication (VMM), the dominant computational kernel in DNNs. However, a key challenge with resistive crossbars is that they suffer from a range of device and circuit level non-idealities such as interconnect parasitics, peripheral circuits, sneak paths, and process variations. These non-idealities can lead to errors in VMMs, eventually degrading the DNN's accuracy. It is therefore critical to study the impact of crossbar non-idealities on the accuracy of large-scale DNNs. However, this is challenging because existing device and circuit models are too slow to use in application-level evaluations. We present RxNN, a fast and accurate simulation framework to evaluate large-scale DNNs on resistive crossbar systems. RxNN splits and maps the computations involved in each DNN layer into crossbar operations, and evaluates them using a Fast Crossbar Model (FCM) that accurately captures the errors arising due to crossbar non-idealities while being four-to-five orders of magnitude faster than circuit simulation. FCM models a crossbar-based VMM operation using three stages - non-linear models for the input and output peripheral circuits (DACs and ADCs), and an equivalent non-ideal conductance matrix for the core crossbar array. We implement RxNN by extending the Caffe machine learning framework and use it to evaluate a suite of six large-scale DNNs developed for the ImageNet Challenge. Our experiments reveal that resistive crossbar non-idealities can lead to significant accuracy degradations (9.6\%-32\%) for these large-scale DNNs. To the best of our knowledge, this work is the first quantitative evaluation of the accuracy of large-scale DNNs on resistive crossbar based hardware.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Emerging Technologies,Computer Science - Machine Learning},
  file = {/Users/nvk/Zotero/storage/VIHW4NFN/Jain et al. - 2020 - RxNN A Framework for Evaluating Deep Neural Netwo.pdf;/Users/nvk/Zotero/storage/9TXM2PNI/1809.html}
}

@article{janaConductivebridgingRandomAccess2015,
  title = {Conductive-Bridging Random Access Memory: Challenges and Opportunity for {{3D}} Architecture},
  shorttitle = {Conductive-Bridging Random Access Memory},
  author = {Jana, Debanjan and Roy, Sourav and Panja, Rajeswar and Dutta, Mrinmoy and Rahaman, Sheikh Ziaur and Mahapatra, Rajat and Maikap, Siddheswar},
  year = {2015},
  month = apr,
  journal = {Nanoscale Research Letters},
  volume = {10},
  number = {1},
  pages = {188},
  issn = {1556-276X},
  doi = {10.1186/s11671-015-0880-9},
  abstract = {The performances of conductive-bridging random access memory (CBRAM) have been reviewed for different switching materials such as chalcogenides, oxides, and bilayers in different structures. The structure consists of an inert electrode and one oxidized electrode of copper (Cu) or silver (Ag). The switching mechanism is the formation/dissolution of a metallic filament in the switching materials under external bias. However, the growth dynamics of the metallic filament in different switching materials are still debated. All CBRAM devices are switching under an operation current of 0.1~{$\mu$}A to 1~mA, and an operation voltage of {$\pm$}2~V is also needed. The device can reach a low current of 5 pA; however, current compliance-dependent reliability is a challenging issue. Although a chalcogenide-based material has opportunity to have better endurance as compared to an oxide-based material, data retention and integration with the complementary metal-oxide-semiconductor (CMOS) process are also issues. Devices with bilayer switching materials show better resistive switching characteristics as compared to those with a single switching layer, especially a program/erase endurance of {$>$}105~cycles with a high speed of few nanoseconds. Multi-level cell operation is possible, but the stability of the high resistance state is also an important reliability concern. These devices show a good data retention of {$>$}105~s at {$>$}85\textdegree C. However, more study is needed to achieve a 10-year guarantee of data retention for non-volatile memory application. The crossbar memory is benefited for high density with low power operation. Some CBRAM devices as a chip have been reported for proto-typical production. This review shows that operation current should be optimized for few microamperes with a maintaining speed of few nanoseconds, which will have challenges and also opportunities for three-dimensional (3D) architecture.},
  langid = {english},
  file = {/Users/nvk/Zotero/storage/VWPV24YN/Jana et al. - 2015 - Conductive-bridging random access memory challeng.pdf}
}

@inproceedings{jeschkeInformationExtractionMathematical2007,
  title = {Information Extraction from Mathematical Texts by Means of Natural Language Processing Techniques},
  booktitle = {Proceedings of the International Workshop on {{Educational}} Multimedia and Multimedia Education  - {{Emme}} '07},
  author = {Jeschke, Sabina and Wilke, Marc and Blanke, Marie and Natho, Nicole M. and Pfeiffer, Olivier F.},
  year = {2007},
  pages = {109},
  publisher = {{ACM Press}},
  address = {{Augsburg, Bavaria, Germany}},
  doi = {10.1145/1290144.1290162},
  abstract = {Particularly with regard to the widespread use of the internet, the increasing amount of scientific publications creates new requirements for sophisticated information retrieval systems. The discovery of semantic annotation for describing mathematical texts themselves and the structure of the observed mathematical field is an important issue supporting such information retrieval systems. A lot of good statistical approaches for finding correlations in texts exist \textendash{} e.g. as used by Google. mArachna follows a different approach and uses natural language processing techniques to recover all the fine-grained information snippets within mathematical texts. The extracted information is stored in knowledge bases, creating a low-level ontology of mathematics. In this article we represent our further developments in this field and the technical implementation of the mArachna prototype.},
  isbn = {978-1-59593-783-4},
  langid = {english},
  file = {/Users/nvk/Zotero/storage/ZCY3SIDC/Jeschke et al. - 2007 - Information extraction from mathematical texts by .pdf}
}

@article{jiaDissectingGraphcoreIPU2019,
  title = {Dissecting the {{Graphcore IPU Architecture}} via {{Microbenchmarking}}},
  author = {Jia, Zhe and Tillman, Blake and Maggioni, Marco and Scarpazza, Daniele Paolo},
  year = {2019},
  month = dec,
  journal = {arXiv:1912.03413 [cs]},
  eprint = {1912.03413},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {This report focuses on the architecture and performance of the Intelligence Processing Unit (IPU), a novel, massively parallel platform recently introduced by Graphcore and aimed at Artificial Intelligence/Machine Learning (AI/ML) workloads. We dissect the IPU's performance behavior using microbenchmarks that we crafted for the purpose. We study the IPU's memory organization and performance. We study the latency and bandwidth that the on-chip and off-chip interconnects offer, both in point-to-point transfers and in a spectrum of collective operations, under diverse loads. We evaluate the IPU's compute power over matrix multiplication, convolution, and AI/ML primitives. We discuss actual performance in comparison with its theoretical limits. Our findings reveal how the IPU's architectural design affects its performance. Moreover, they offer simple mental models to predict an application's performance on the IPU, on the basis of the computation and communication steps it involves. This report is the natural extension to a novel architecture of a continuing effort of ours that focuses on the microbenchmark-based discovery of massively parallel architectures.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Distributed; Parallel; and Cluster Computing,Computer Science - Hardware Architecture,Computer Science - Performance},
  file = {/Users/nvk/Zotero/storage/UU9L48MR/Jia et al. - 2019 - Dissecting the Graphcore IPU Architecture via Micr.pdf;/Users/nvk/Zotero/storage/3X2DWKXM/1912.html}
}

@article{jiangEvaluatingBERTbasedScientific2021,
  title = {Evaluating {{BERT-based}} Scientific Relation Classifiers for Scholarly Knowledge Graph Construction on Digital Library Collections},
  author = {Jiang, Ming and D'Souza, Jennifer and Auer, S{\"o}ren and Downie, J. Stephen},
  year = {2021},
  month = nov,
  journal = {International Journal on Digital Libraries},
  issn = {1432-1300},
  doi = {10.1007/s00799-021-00313-y},
  abstract = {The rapid growth of research publications has placed great demands on digital libraries (DL) for advanced information management technologies. To cater to these demands, techniques relying on knowledge-graph structures are being advocated. In such graph-based pipelines, inferring semantic relations between related scientific concepts is a crucial step. Recently, BERT-based pre-trained models have been popularly explored for automatic relation classification. Despite significant progress, most of them were evaluated in different scenarios, which limits their comparability. Furthermore, existing methods are primarily evaluated on clean texts, which ignores the digitization context of early scholarly publications in terms of machine scanning and optical character recognition (OCR). In such cases, the texts may contain OCR noise, in turn creating uncertainty about existing classifiers' performances. To address these limitations, we started by creating OCR-noisy texts based on three clean corpora. Given these parallel corpora, we conducted a thorough empirical evaluation of eight Bert-based classification models by focusing on three factors: (1) Bert variants; (2) classification strategies; and, (3) OCR noise impacts. Experiments on clean data show that the domain-specific pre-trained Bert is the best variant to identify scientific relations. The strategy of predicting a single relation each time outperforms the one simultaneously identifying multiple relations in general. The optimal classifier's performance can decline by around 10\% to 20\% in F-score on the noisy corpora. Insights discussed in this study can help DL stakeholders select techniques for building optimal knowledge-graph-based systems.},
  langid = {english},
  file = {/Users/nvk/Zotero/storage/SLKN2JLR/Jiang et al. - 2021 - Evaluating BERT-based scientific relation classifi.pdf}
}

@article{jiangTuringInstabilityPattern2022,
  title = {Turing Instability and Pattern Formations for Reaction-Diffusion Systems on {{2D}} Bounded Domain},
  author = {Jiang, Weihua and Cao, Xun and Wang, Chuncheng},
  year = {2022},
  journal = {Discrete \& Continuous Dynamical Systems - B},
  volume = {27},
  number = {2},
  pages = {1163},
  publisher = {{American Institute of Mathematical Sciences}},
  doi = {10.3934/dcdsb.2021085},
  abstract = {{$<$}p style='text-indent:20px;'{$>$}In this article, Turing instability and the formations of spatial patterns for a general two-component reaction-diffusion system defined on 2D bounded domain, are investigated. By analyzing characteristic equation at positive constant steady states and further selecting diffusion rate {$<$}inline-formula{$><$}tex-math id="M1"{$>\backslash$}begin\{document\}\$ d \$\textbackslash end\{document\}{$<$}/tex-math{$><$}/inline-formula{$>$} and diffusion ratio {$<$}inline-formula{$><$}tex-math id="M2"{$>\backslash$}begin\{document\}\$ \textbackslash varepsilon \$\textbackslash end\{document\}{$<$}/tex-math{$><$}/inline-formula{$>$} as bifurcation parameters, sufficient and necessary conditions for the occurrence of Turing instability are established, which is called the first Turing bifurcation curve. Furthermore, parameter regions in which single-mode Turing patterns arise and multiple-mode (or superposition) Turing patterns coexist when bifurcations parameters are chosen, are described. Especially, the boundary of parameter region for the emergence of single-mode Turing patterns, consists of the first and the second Turing bifurcation curves which are given in explicit formulas. Finally, by taking diffusive Schnakenberg system as an example, parameter regions for the emergence of various kinds of spatially inhomogeneous patterns with different spatial frequencies and superposition Turing patterns, are estimated theoretically and shown numerically.{$<$}/p{$>$}},
  copyright = {http://creativecommons.org/licenses/by/3.0/},
  langid = {english},
  file = {/Users/nvk/Zotero/storage/FF9JPVMB/Jiang et al. - 2022 - Turing instability and pattern formations for reac.pdf;/Users/nvk/Zotero/storage/IILWFHQP/dcdsb.html}
}

@article{jiaSPINBISSpintronicsBased2020,
  title = {{{SPINBIS}}: {{Spintronics}} Based {{Bayesian Inference System}} with {{Stochastic Computing}}},
  shorttitle = {{{SPINBIS}}},
  author = {Jia, Xiaotao and Yang, Jianlei and Dai, Pengcheng and Liu, Runze and Chen, Yiran and Zhao, Weisheng},
  year = {2020},
  month = apr,
  journal = {IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems},
  volume = {39},
  number = {4},
  eprint = {1902.06886},
  eprinttype = {arxiv},
  pages = {789--802},
  issn = {0278-0070, 1937-4151},
  doi = {10.1109/TCAD.2019.2897631},
  abstract = {Bayesian inference is an effective approach for solving statistical learning problems, especially with uncertainty and incompleteness. However, Bayesian inference is a computing-intensive task whose efficiency is physically limited by the bottlenecks of conventional computing platforms. In this work, a spintronics based stochastic computing approach is proposed for efficient Bayesian inference. The inherent stochastic switching behaviors of spintronic devices are exploited to build stochastic bitstream generator (SBG) for stochastic computing with hybrid CMOS/MTJ circuits design. Aiming to improve the inference efficiency, an SBG sharing strategy is leveraged to reduce the required SBG array scale by integrating a switch network between SBG array and stochastic computing logic. A device-to-architecture level framework is proposed to evaluate the performance of spintronics based Bayesian inference system (SPINBIS). Experimental results on data fusion applications have shown that SPINBIS could improve the energy efficiency about 12X than MTJ-based approach with 45\% design area overhead and about 26X than FPGA-based approach.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Emerging Technologies,Computer Science - Hardware Architecture},
  file = {/Users/nvk/Zotero/storage/AWZ7HX7W/Jia et al. - 2020 - SPINBIS Spintronics based Bayesian Inference Syst.pdf;/Users/nvk/Zotero/storage/Z6SCI99H/1902.html}
}

@book{joseAdvancesInformationRetrieval2020,
  title = {Advances in {{Information Retrieval}}: 42nd {{European Conference}} on {{IR Research}}, {{ECIR}} 2020, {{Lisbon}}, {{Portugal}}, {{April}} 14\textendash 17, 2020, {{Proceedings}}, {{Part I}}},
  shorttitle = {Advances in {{Information Retrieval}}},
  editor = {Jose, Joemon M. and Yilmaz, Emine and Magalh{\~a}es, Jo{\~a}o and Castells, Pablo and Ferro, Nicola and Silva, M{\'a}rio J. and Martins, Fl{\'a}vio},
  year = {2020},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  volume = {12035},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-030-45439-5},
  isbn = {978-3-030-45438-8 978-3-030-45439-5},
  langid = {english},
  file = {/Users/nvk/Zotero/storage/TXR55SPW/Jose et al. - 2020 - Advances in Information Retrieval 42nd European C.pdf}
}

@article{joshiStateFateLinguistic2020,
  title = {The {{State}} and {{Fate}} of {{Linguistic Diversity}} and {{Inclusion}} in the {{NLP World}}},
  author = {Joshi, Pratik and Santy, Sebastin and Budhiraja, Amar and Bali, Kalika and Choudhury, Monojit},
  year = {2020},
  month = jun,
  journal = {arXiv:2004.09095 [cs]},
  eprint = {2004.09095},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {Language technologies contribute to promoting multilingualism and linguistic diversity around the world. However, only a very small number of the over 7000 languages of the world are represented in the rapidly evolving language technologies and applications. In this paper we look at the relation between the types of languages, resources, and their representation in NLP conferences to understand the trajectory that different languages have followed over time. Our quantitative investigation underlines the disparity between languages, especially in terms of their resources, and calls into question the "language agnostic" status of current models and systems. Through this paper, we attempt to convince the ACL community to prioritise the resolution of the predicaments highlighted here, so that no language is left behind.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/nvk/Zotero/storage/5XAYHN73/Joshi et al. - 2020 - The State and Fate of Linguistic Diversity and Inc.pdf;/Users/nvk/Zotero/storage/KDHPVMZT/2004.html}
}

@inproceedings{joshiUseNeuralNetworks1994,
  title = {The Use of Neural Networks to Support "Intelligent" Scientific Computing},
  booktitle = {Proceedings of 1994 {{IEEE International Conference}} on {{Neural Networks}} ({{ICNN}}'94)},
  author = {Joshi, A. and Weerawarana, S. and Houstis, E.N.},
  year = {1994},
  month = jun,
  volume = {4},
  pages = {2197-2202 vol.4},
  doi = {10.1109/ICNN.1994.374557},
  abstract = {We report on the use of backpropagation based neural networks to implement a phase of the computational intelligence process of the PYTHIA expert system for supporting the numerical simulation of applications modelled by partial differential equations (PDEs). PYTHIA is an exemplar based reasoning system that provides advice on what method and parameters to use for the simulation of a specified PDE based application. When advice is requested, the characteristics of the given model are matched with the characteristics of previously seen classes of models. The performance of various solution methods on previously seen similar classes of models is then used as a basis for predicting what method to use. Thus, a major step of the reasoning process in PYTHIA involves the analysis and categorization of models into classes of models based on their characteristics. In this study we demonstrate the use of neural networks to identify the class of predefined models whose characteristics match the ones of the specified PDE based application.{$<>$}},
  keywords = {backpropagation,Backpropagation,case-based reasoning,Competitive intelligence,Computational intelligence,computational intelligence process,Computational modeling,exemplar based reasoning system,expert systems,Expert systems,Intelligent networks,model analysis,neural nets,neural networks,Neural networks,numerical analysis,numerical simulation,Numerical simulation,partial differential equations,Partial differential equations,PYTHIA expert system,Scientific computing,simulation},
  file = {/Users/nvk/Zotero/storage/N4XVXYUH/Joshi et al. - 1994 - The use of neural networks to support intelligent.pdf;/Users/nvk/Zotero/storage/CUEDLPQY/374557.html}
}

@article{jozefowiczEmpiricalExplorationRecurrent,
  title = {An {{Empirical Exploration}} of {{Recurrent Network Architectures}}},
  author = {Jozefowicz, Rafal and Zaremba, Wojciech and Sutskever, Ilya},
  pages = {9},
  abstract = {The Recurrent Neural Network (RNN) is an extremely powerful sequence model that is often difficult to train. The Long Short-Term Memory (LSTM) is a specific RNN architecture whose design makes it much easier to train. While wildly successful in practice, the LSTM's architecture appears to be ad-hoc so it is not clear if it is optimal, and the significance of its individual components is unclear.},
  langid = {english},
  file = {/Users/nvk/Zotero/storage/322YL7RL/Jozefowicz et al. - An Empirical Exploration of Recurrent Network Arch.pdf}
}

@inproceedings{kamaliNewMathematicsRetrieval2010,
  title = {A New Mathematics Retrieval System},
  booktitle = {Proceedings of the 19th {{ACM}} International Conference on {{Information}} and Knowledge Management},
  author = {Kamali, Shahab and Tompa, Frank Wm.},
  year = {2010},
  month = oct,
  series = {{{CIKM}} '10},
  pages = {1413--1416},
  publisher = {{Association for Computing Machinery}},
  address = {{New York, NY, USA}},
  doi = {10.1145/1871437.1871635},
  abstract = {The Web contains a large collection of documents, some with mathematical expressions. Because mathematical expressions are objects with complex structures and rather few distinct symbols, conventional text retrieval systems are not very successful in mathematics retrieval. The lack of a definition for similarity between mathematical expressions, and the inadequacy of searching for exact matches only, makes the problem of mathematics retrieval even harder. As a result, the few existing mathematics retrieval systems are not very helpful in addressing users' needs. We propose a powerful query language for mathematical expressions that augments exact matching with approximate matching, but in a way that is controlled by the user. We also introduce a novel indexing scheme that scales well for large collections of expressions. Based on this indexing scheme, an efficient lookup algorithm is proposed.},
  isbn = {978-1-4503-0099-5},
  keywords = {indexing,mathematics retrieval,query language,search},
  file = {/Users/nvk/Zotero/storage/RQUZ7JPB/Kamali and Tompa - 2010 - A new mathematics retrieval system.pdf}
}

@inproceedings{kamaliStructuralSimilaritySearch2013,
  title = {Structural {{Similarity Search}} for {{Mathematics Retrieval}}},
  booktitle = {Intelligent {{Computer Mathematics}}},
  author = {Kamali, Shahab and Tompa, Frank Wm.},
  year = {2013},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {246--262},
  publisher = {{Springer}},
  address = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-642-39320-4_16},
  abstract = {Retrieving documents by querying their mathematical content directly can be useful in various domains, including education, engineering, patent research, physics, and medical sciences. As distinct from text retrieval, however, mathematical symbols in isolation do not contain much semantic information, and the structure of an expression must be considered as well. Unfortunately, considering the structure to calculate the relevance scores of documents results in ranking algorithms that are computationally more expensive than the typical ranking algorithms employed for text documents. As a result, current math retrieval systems either limit themselves to exact matches, or they ignore the structure completely; they sacrifice either recall or precision for efficiency. We propose instead an efficient end-to-end math retrieval system based on a structural similarity ranking algorithm. We describe novel optimizations techniques to reduce the index size and the query processing time, and we experimentally validate our system in terms of correctness and efficiency. Thus, with the proposed optimizations, mathematical contents can be fully exploited to rank documents in response to mathematical queries.},
  isbn = {978-3-642-39320-4},
  langid = {english},
  keywords = {Edit Operation,Index Size,Mathematical Content,Mathematical Expression,Query Processing},
  file = {/Users/nvk/Zotero/storage/SBFTK6SJ/Kamali and Tompa - 2013 - Structural Similarity Search for Mathematics Retri.pdf}
}

@misc{kamiennyEndtoendSymbolicRegression2022,
  title = {End-to-End Symbolic Regression with Transformers},
  author = {Kamienny, Pierre-Alexandre and {d'Ascoli}, St{\'e}phane and Lample, Guillaume and Charton, Fran{\c c}ois},
  year = {2022},
  month = apr,
  number = {arXiv:2204.10532},
  eprint = {2204.10532},
  eprinttype = {arxiv},
  primaryclass = {cs},
  institution = {{arXiv}},
  doi = {10.48550/arXiv.2204.10532},
  abstract = {Symbolic regression, the task of predicting the mathematical expression of a function from the observation of its values, is a difficult task which usually involves a two-step procedure: predicting the "skeleton" of the expression up to the choice of numerical constants, then fitting the constants by optimizing a non-convex loss function. The dominant approach is genetic programming, which evolves candidates by iterating this subroutine a large number of times. Neural networks have recently been tasked to predict the correct skeleton in a single try, but remain much less powerful. In this paper, we challenge this two-step procedure, and task a Transformer to directly predict the full mathematical expression, constants included. One can subsequently refine the predicted constants by feeding them to the non-convex optimizer as an informed initialization. We present ablations to show that this end-to-end approach yields better results, sometimes even without the refinement step. We evaluate our model on problems from the SRBench benchmark and show that our model approaches the performance of state-of-the-art genetic programming with several orders of magnitude faster inference.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning},
  file = {/Users/nvk/Zotero/storage/DQMYHNBI/Kamienny et al. - 2022 - End-to-end symbolic regression with transformers.pdf;/Users/nvk/Zotero/storage/XM52RM6P/2204.html}
}

@article{kaniNonmonotonicProbabilityThermal2017,
  title = {Non-Monotonic Probability of Thermal Reversal in Thin-Film Biaxial Nanomagnets with Small Energy Barriers},
  author = {Kani, N. and Naeemi, A. and Rakheja, S.},
  year = {2017},
  month = may,
  journal = {AIP Advances},
  volume = {7},
  number = {5},
  pages = {056006},
  publisher = {{American Institute of Physics}},
  doi = {10.1063/1.4974017},
  abstract = {The goal of this paper is to investigate the short time-scale, thermally-induced probability of magnetization reversal for an biaxial nanomagnet that is characterized with a biaxial magnetic anisotropy. For the first time, we clearly show that for a given energy barrier of the nanomagnet, the magnetization reversal probability of an biaxial nanomagnet exhibits a non-monotonic dependence on its saturation magnetization. Specifically, there are two reasons for this non-monotonic behavior in rectangular thin-film nanomagnets that have a large perpendicular magnetic anisotropy. First, a large perpendicular anisotropy lowers the precessional period of the magnetization making it more likely to precess across the  {$\mathsl{x}$}{\^ }~ =0  plane if the magnetization energy exceeds the energy barrier. Second, the thermal-field torque at a particular energy increases as the magnitude of the perpendicular anisotropy increases during the magnetization precession. This non-monotonic behavior is most noticeable when analyzing the magnetization reversals on time-scales up to several tens of ns. In light of the several proposals of spintronic devices that require data retention on time-scales up to 10's of ns, understanding the probability of magnetization reversal on the short time-scales is important. As such, the results presented in this paper will be helpful in quantifying the reliability and noise sensitivity of spintronic devices in which thermal noise is inevitably present.},
  file = {/Users/nvk/Zotero/storage/NK9WEGV9/Kani et al. - 2017 - Non-monotonic probability of thermal reversal in t.pdf}
}

@article{karakayaTrueRandomBit2019,
  title = {A True Random Bit Generator Based on a Memristive Chaotic Circuit: {{Analysis}}, Design and {{FPGA}} Implementation},
  shorttitle = {A True Random Bit Generator Based on a Memristive Chaotic Circuit},
  author = {Karakaya, Bar{\i}{\c s} and G{\"u}lten, Arif and Frasca, Mattia},
  year = {2019},
  month = feb,
  journal = {Chaos, Solitons \& Fractals},
  volume = {119},
  pages = {143--149},
  issn = {0960-0779},
  doi = {10.1016/j.chaos.2018.12.021},
  abstract = {The aim of this paper is to present a true random bit generator (TRBG) based on a memristive chaotic circuit and its implementation on Field Programmable Gate Array (FPGA) board. The proposed TRBG architecture makes use of a memristive canonical Chua's oscillator and a logistic map as entropy sources, while the XOR function is used for post-processing. The optimal parameter set for the chaotic systems has been chosen by carrying out numerical simulations of the system and adopting the scale index parameter to determine the degree of non-periodicity of the obtained bit streams. The proposed TRBG system has been then modeled and co-simulated on the Xilinx System Generator (XSG) platform and implemented on the Xilinx Kintex-7 KC705 FPGA Evaluation Board, obtaining experimental results in agreement with the expectations. Finally, the system has been validated with statistical analysis by using the NIST 800.22 statistical test suite.},
  langid = {english},
  keywords = {Chaos,Chua's oscillator,FPGA,Logistic map,Memristor,True random bit generator},
  file = {/Users/nvk/Zotero/storage/4RPVIVWF/Karakaya et al. - 2019 - A true random bit generator based on a memristive .pdf;/Users/nvk/Zotero/storage/MN3FYMVE/S0960077918310518.html}
}

@article{kendallTrainingEndtoEndAnalog2020,
  title = {Training {{End-to-End Analog Neural Networks}} with {{Equilibrium Propagation}}},
  author = {Kendall, Jack and Pantone, Ross and Manickavasagam, Kalpana and Bengio, Yoshua and Scellier, Benjamin},
  year = {2020},
  month = jun,
  journal = {arXiv:2006.01981 [cs]},
  eprint = {2006.01981},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {We introduce a principled method to train end-to-end analog neural networks by stochastic gradient descent. In these analog neural networks, the weights to be adjusted are implemented by the conductances of programmable resistive devices such as memristors [Chua, 1971], and the nonlinear transfer functions (or `activation functions') are implemented by nonlinear components such as diodes. We show mathematically that a class of analog neural networks (called nonlinear resistive networks) are energy-based models: they possess an energy function as a consequence of Kirchhoff's laws governing electrical circuits. This property enables us to train them using the Equilibrium Propagation framework [Scellier and Bengio, 2017]. Our update rule for each conductance, which is local and relies solely on the voltage drop across the corresponding resistor, is shown to compute the gradient of the loss function. Our numerical simulations, which use the SPICE-based Spectre simulation framework to simulate the dynamics of electrical circuits, demonstrate training on the MNIST classification task, performing comparably or better than equivalent-size software-based neural networks. Our work can guide the development of a new generation of ultra-fast, compact and low-power neural networks supporting on-chip learning.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing},
  file = {/Users/nvk/Zotero/storage/EX7RCL65/Kendall et al. - 2020 - Training End-to-End Analog Neural Networks with Eq.pdf;/Users/nvk/Zotero/storage/NKYMMVQG/2006.html}
}

@article{khatriAbstractiveExtractiveText2018,
  title = {Abstractive and {{Extractive Text Summarization}} Using {{Document Context Vector}} and {{Recurrent Neural Networks}}},
  author = {Khatri, Chandra and Singh, Gyanit and Parikh, Nish},
  year = {2018},
  month = jul,
  journal = {arXiv:1807.08000 [cs]},
  eprint = {1807.08000},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {Sequence to sequence (Seq2Seq) learning has recently been used for abstractive and extractive summarization. In current study, Seq2Seq models have been used for eBay product description summarization. We propose a novel Document-Context based Seq2Seq models using RNNs for abstractive and extractive summarizations. Intuitively, this is similar to humans reading the title, abstract or any other contextual information before reading the document. This gives humans a high-level idea of what the document is about. We use this idea and propose that Seq2Seq models should be started with contextual information at the first time-step of the input to obtain better summaries. In this manner, the output summaries are more document centric, than being generic, overcoming one of the major hurdles of using generative models. We generate document-context from user-behavior and seller provided information. We train and evaluate our models on human-extracted-golden-summaries. The document-contextual Seq2Seq models outperform standard Seq2Seq models. Moreover, generating human extracted summaries is prohibitively expensive to scale, we therefore propose a semi-supervised technique for extracting approximate summaries and using it for training Seq2Seq models at scale. Semi-supervised models are evaluated against human extracted summaries and are found to be of similar efficacy. We provide side by side comparison for abstractive and extractive summarizers (contextual and non-contextual) on same evaluation dataset. Overall, we provide methodologies to use and evaluate the proposed techniques for large document summarization. Furthermore, we found these techniques to be highly effective, which is not the case with existing techniques.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/nvk/Zotero/storage/SUTGFY8T/Khatri et al. - 2018 - Abstractive and Extractive Text Summarization usin.pdf;/Users/nvk/Zotero/storage/E7WEPPRK/1807.html}
}

@inproceedings{kimEnergyefficientRandomNumber2016,
  title = {An Energy-Efficient Random Number Generator for Stochastic Circuits},
  booktitle = {2016 21st {{Asia}} and {{South Pacific Design Automation Conference}} ({{ASP-DAC}})},
  author = {Kim, Kyounghoon and Lee, Jongeun and Choi, Kiyoung},
  year = {2016},
  month = jan,
  pages = {256--261},
  issn = {2153-697X},
  doi = {10.1109/ASPDAC.2016.7428020},
  abstract = {Stochastic circuits provide very high efficiency in terms of gate area and power consumption compared with conventional binary logic. However, they require random bit streams generated by stochastic number generators (SNGs), which account for a significant portion of area and energy offsetting their merits. In this paper, we propose a new SNG that significantly reduces area and energy while improving accuracy in progressive precision. Experimental results show that the proposed SNG reduces energy by more than 72\% compared to the state-of-the-art designs.},
  keywords = {binary logic,Computers,Encoding,energy-efficient random number generator,gate area,Generators,Hardware,Indexes,logic circuits,Logic gates,power consumption,Power demand,random bit streams,random number generation,SNGs,stochastic circuits,stochastic number generators},
  file = {/Users/nvk/Zotero/storage/LUZFDARZ/Kim et al. - 2016 - An energy-efficient random number generator for st.pdf;/Users/nvk/Zotero/storage/6YFX2DFP/7428020.html}
}

@phdthesis{kipfDeepLearningGraphstructured2020,
  title = {Deep Learning with Graph-Structured Representations},
  author = {Kipf, Thomas},
  year = {2020},
  isbn = {9789463758512},
  langid = {english},
  annotation = {OCLC: 1151818405},
  file = {/Users/nvk/Zotero/storage/I7J2I3YL/Kipf - 2020 - Deep learning with graph-structured representation.pdf}
}

@article{kirosSkipThoughtVectors,
  title = {Skip-{{Thought Vectors}}},
  author = {Kiros, Ryan and Zhu, Yukun and Salakhutdinov, Russ R and Zemel, Richard and Urtasun, Raquel and Torralba, Antonio and Fidler, Sanja},
  pages = {9},
  abstract = {We describe an approach for unsupervised learning of a generic, distributed sentence encoder. Using the continuity of text from books, we train an encoderdecoder model that tries to reconstruct the surrounding sentences of an encoded passage. Sentences that share semantic and syntactic properties are thus mapped to similar vector representations. We next introduce a simple vocabulary expansion method to encode words that were not seen as part of training, allowing us to expand our vocabulary to a million words. After training our model, we extract and evaluate our vectors with linear models on 8 tasks: semantic relatedness, paraphrase detection, image-sentence ranking, question-type classification and 4 benchmark sentiment and subjectivity datasets. The end result is an off-the-shelf encoder that can produce highly generic sentence representations that are robust and perform well in practice.},
  langid = {english},
  file = {/Users/nvk/Zotero/storage/MSR6SQJ9/Kiros et al. - Skip-Thought Vectors.pdf}
}

@inproceedings{kirosSkipThoughtVectors2015,
  title = {Skip-{{Thought Vectors}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Kiros, Ryan and Zhu, Yukun and Salakhutdinov, Russ R and Zemel, Richard and Urtasun, Raquel and Torralba, Antonio and Fidler, Sanja},
  year = {2015},
  volume = {28},
  publisher = {{Curran Associates, Inc.}},
  file = {/Users/nvk/Zotero/storage/66NTQXJN/Kiros et al. - 2015 - Skip-Thought Vectors.pdf}
}

@article{kishEndMooreLaw2002,
  title = {End of {{Moore}}'s Law: Thermal (Noise) Death of Integration in Micro and Nano Electronics},
  shorttitle = {End of {{Moore}}'s Law},
  author = {Kish, Laszlo B},
  year = {2002},
  month = dec,
  journal = {Physics Letters A},
  volume = {305},
  number = {3},
  pages = {144--149},
  issn = {0375-9601},
  doi = {10.1016/S0375-9601(02)01365-8},
  abstract = {The exponential growth of memory size and clock frequency in computers has a great impact on everyday life. The growth is empirically described by Moore's law of miniaturization. Physical limitations of this growth would have a serious impact on technology and economy. A~thermodynamical effect, the increasing thermal noise voltage (Johnson\textendash Nyquist noise) on decreasing characteristic capacitances, together with the constrain of using lower supply voltages to keep power dissipation manageable on the contrary of increasing clock frequency, has the potential to break abruptly Moore's law within 6\textendash 8 years, or earlier.},
  langid = {english},
  file = {/Users/nvk/Zotero/storage/6SAMDHXY/Kish - 2002 - End of Moore's law thermal (noise) death of integ.pdf;/Users/nvk/Zotero/storage/VWRJ9KP5/S0375960102013658.html}
}

@inproceedings{kleinOpenNMTNeuralMachine2020,
  title = {The {{OpenNMT Neural Machine Translation Toolkit}}: 2020 {{Edition}}},
  shorttitle = {The {{OpenNMT Neural Machine Translation Toolkit}}},
  booktitle = {Proceedings of the 14th {{Conference}} of the {{Association}} for {{Machine Translation}} in the {{Americas}} ({{Volume}} 1: {{Research Track}})},
  author = {Klein, Guillaume and Hernandez, Fran{\c c}ois and Nguyen, Vincent and Senellart, Jean},
  year = {2020},
  month = oct,
  pages = {102--109},
  publisher = {{Association for Machine Translation in the Americas}},
  address = {{Virtual}},
  file = {/Users/nvk/Zotero/storage/U6VTFLE3/Klein et al. - 2020 - The OpenNMT Neural Machine Translation Toolkit 20.pdf}
}

@book{knuthTEXMETAFONTNew1979,
  title = {{{TEX}} and {{METAFONT}}: {{New}} Directions in Typesetting},
  author = {Knuth, Donald Ervin},
  year = {1979},
  publisher = {{American Mathematical Society}},
  isbn = {0-932376-02-9}
}

@inproceedings{knuthTEXProgram1984,
  title = {{{TEX}}: The {{Program}}},
  booktitle = {Tepr},
  author = {Knuth, Donald E.},
  year = {1984}
}

@book{kohlhaseMathematicalKnowledgeManagement2006,
  title = {Mathematical {{Knowledge Management}}: 4th {{International Conference}}, {{MKM}} 2005, {{Bremen}}, {{Germany}}, {{July}} 15-17, 2005, {{Revised Selected Papers}}},
  shorttitle = {Mathematical {{Knowledge Management}}},
  author = {Kohlhase, Michael},
  year = {2006},
  month = jan,
  publisher = {{Springer}},
  abstract = {This volume contains the proceedings of the Fourth International Conference on Mathematical Knowledge Management MKM 2005 held July 15\textendash 17, 2005 at - ternational University Bremen, Germany. Previous conferences have been at the Research Institute for Symbolic Computation (RISC) Linz, Austria (September 2001), at Bertinoro, Italy (March 2003), and Bialowiecze, Poland (September 2004). Mathematical knowledge management (MKM) is a ?eld in the intersection of mathematics and computer science, providing new techniques for managing the enormous volume of mathematical knowledge available in current mathematical sources and making it available through the new developments in information technology. The annual MKM Conference brings together mathematicians, software - velopers, publishing companies, math organizations, math users, and educators to exchange their views and approaches, current activities and new initiatives. For the ?rst time, MKM 2005 chose to have post-conference proceedings, as otherwise the submission deadline would have collided with other conferences and crimped time since MKM 2004 in September 2004. The decision also faci- tatedkeepingtheconferenceopentonewideasaswellaskeepingupthematurity of the papers necessary for inclusion into archival proceedings. With a May 15 deadline, MKM 2005 received 38 submissions. Each submission was reviewed by at least three programme committee members. The committee decided to - cept 27 papers for presentation at the conference. Out of these, 26 papers were accepted for publication in the conference proceedings after re-evaluation by the Programme Committee since they included signi?cant improvements triggered by the referee reports and the discussions at the conference.},
  googlebooks = {EDb3BwAAQBAJ},
  isbn = {978-3-540-31431-8},
  langid = {english},
  keywords = {Computers / Artificial Intelligence / General,Computers / Database Administration \& Management,Computers / Information Technology,Computers / Networking / Hardware,Computers / System Administration / Storage \& Retrieval,Mathematics / General}
}

@article{kohlhaseMathObjectIdentifiers,
  title = {Math {{Object Identifiers}} \textendash{} {{Towards Research Data}} in {{Mathematics}}},
  author = {Kohlhase, Michael},
  pages = {12},
  abstract = {We propose to develop a system of ``Math Object Identifiers'' (MOIs: digital object identifiers for mathematical concepts, objects, and models) and a process of registering them. These envisioned MOIs constitute a very lightweight form of semantic annotation that can support many knowledge-based workflows in mathematics, e.g. classification of articles via the objects mentioned or object-based search. In essence MOIs are an enabling technology for Linked Open Data for mathematics and thus makes (parts of) the mathematical literature into mathematical research data.},
  langid = {english},
  file = {/Users/nvk/Zotero/storage/6GUPWJD8/Kohlhase - Math Object Identiï¬ers â€“ Towards Research Data in .pdf}
}

@incollection{kohlhaseMathWebSearchScalingOpen2012,
  title = {{{MathWebSearch}} 0.5: {{Scaling}} an {{Open Formula Search Engine}}},
  shorttitle = {{{MathWebSearch}} 0.5},
  booktitle = {Intelligent {{Computer Mathematics}}},
  author = {Kohlhase, Michael and Matican, Bogdan A. and Prodescu, Corneliu-Claudiu},
  editor = {Jeuring, Johan and Campbell, John A. and Carette, Jacques and Dos Reis, Gabriel and Sojka, Petr and Wenzel, Makarius and Sorge, Volker},
  year = {2012},
  volume = {7362},
  pages = {342--357},
  publisher = {{Springer Berlin Heidelberg}},
  address = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-642-31374-5_23},
  abstract = {MathWebSearch is an open-source, open-format, contentoriented search engine for mathematical formulae. It is a complete system capable of crawling, indexing, and querying expressions based on their functional structure (operator tree) rather than their presentation.},
  isbn = {978-3-642-31373-8 978-3-642-31374-5},
  langid = {english},
  file = {/Users/nvk/Zotero/storage/E2ENAUK7/Kohlhase et al. - 2012 - MathWebSearch 0.5 Scaling an Open Formula Search .pdf}
}

@inproceedings{kohlhaseSearchEngineMathematical2006,
  title = {A {{Search Engine}} for {{Mathematical Formulae}}},
  booktitle = {Artificial {{Intelligence}} and {{Symbolic Computation}}},
  author = {Kohlhase, Michael and Sucan, Ioan},
  year = {2006},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {241--253},
  publisher = {{Springer}},
  address = {{Berlin, Heidelberg}},
  doi = {10.1007/11856290_21},
  abstract = {We present a search engine for mathematical formulae. The MathWebSearch system harvests the web for content representations (currently MathML and OpenMath) of formulae and indexes them with substitution tree indexing, a technique originally developed for accessing intermediate results in automated theorem provers. For querying, we present a generic language extension approach that allows constructing queries by minimally annotating existing representations. First experiments show that this architecture results in a scalable application.},
  isbn = {978-3-540-39730-4},
  langid = {english},
  keywords = {Content Representation,Mathematical Formula,Query Term,Search Engine,Term Indexing},
  file = {/Users/nvk/Zotero/storage/Y8VEKXQN/Kohlhase and Sucan - 2006 - A Search Engine for Mathematical Formulae.pdf}
}

@book{kotElementsMathematicalEcology2001,
  title = {Elements of {{Mathematical Ecology}}},
  author = {Kot, Mark},
  year = {2001},
  month = jul,
  publisher = {{Cambridge University Press}},
  abstract = {Elements of Mathematical Ecology provides an introduction to classical and modern mathematical models, methods, and issues in population ecology. The first part of the book is devoted to simple, unstructured population models that ignore much of the variability found in natural populations for the sake of tractability. Topics covered include density dependence, bifurcations, demographic stochasticity, time delays, population interactions (predation, competition, and mutualism), and the application of optimal control theory to the management of renewable resources. The second part of this book is devoted to structured population models, covering spatially-structured population models (with a focus on reaction-diffusion models), age-structured models, and two-sex models. Suitable for upper level students and beginning researchers in ecology, mathematical biology, and applied mathematics, the volume includes numerous clear line diagrams that clarify the mathematics, relevant problems throughout the text that aid understanding, and supplementary mathematical and historical material that enrich the main text.},
  googlebooks = {7\_IRlnNON7oC},
  isbn = {978-0-521-00150-2},
  langid = {english},
  keywords = {Mathematics / Applied,Nature / Ecology,Science / Life Sciences / Ecology}
}

@inproceedings{kristiantoExploitingTextualDescriptions2014,
  title = {Exploiting Textual Descriptions and Dependency Graph for Searching Mathematical Expressions in Scientific Papers},
  booktitle = {Ninth {{International Conference}} on {{Digital Information Management}} ({{ICDIM}} 2014)},
  author = {Kristianto, Giovanni Yoko and Topi{\'c}, Goran and Aizawa, Akiko},
  year = {2014},
  month = sep,
  pages = {110--117},
  doi = {10.1109/ICDIM.2014.6991403},
  abstract = {Mathematical expressions are important for communication of scientific information, for instance, to explain or define concepts written in natural language. Despite their importance, current conventional search systems can not establish access to the mathematical expressions contained in a scientific paper. The major focus of current development of mathematical search systems is mathematical tree structure indexing, but utilizing textual information surrounding the expressions in these systems is also important. We examine how textual information contributes to a mathematical search system, primarily in the ranking process. We investigate the impact of two types of textual information in the ranking performances of a mathematical search system: words in context windows (baseline), which is easily extracted from sentence tokenization result, and descriptions, which are extracted using a machine learning method. We also examine the improvement in ranking obtained by utilizing the dependency graph of mathematical expressions. The experiment results show that the use of description and dependency graph together deliver better ranking performance than the use of context or when no textual information is used. The results also show that the dependency graph is crucial for increasing the number of mathematical expressions being assigned descriptions, and thus its use with descriptions together presented higher ranking performance than the use of descriptions only. This study suggests that descriptions represent mathematical expressions better (more precisely) than context windows, and even descriptions from child (indirect) expressions still represent the target expression better than the context from the target expression itself.},
  keywords = {Context,Data mining,Indexing,Measurement,Natural languages,Search engines,Semantics},
  file = {/Users/nvk/Zotero/storage/ZCU8K22M/Kristianto et al. - 2014 - Exploiting textual descriptions and dependency gra.pdf;/Users/nvk/Zotero/storage/NYNICSC7/6991403.html}
}

@inproceedings{kristiantoExploitingTextualDescriptions2014a,
  title = {Exploiting Textual Descriptions and Dependency Graph for Searching Mathematical Expressions in Scientific Papers},
  booktitle = {Ninth {{International Conference}} on {{Digital Information Management}} ({{ICDIM}} 2014)},
  author = {Kristianto, Giovanni Yoko and Topi{\'c}, Goran and Aizawa, Akiko},
  year = {2014},
  month = sep,
  pages = {110--117},
  doi = {10.1109/ICDIM.2014.6991403},
  abstract = {Mathematical expressions are important for communication of scientific information, for instance, to explain or define concepts written in natural language. Despite their importance, current conventional search systems can not establish access to the mathematical expressions contained in a scientific paper. The major focus of current development of mathematical search systems is mathematical tree structure indexing, but utilizing textual information surrounding the expressions in these systems is also important. We examine how textual information contributes to a mathematical search system, primarily in the ranking process. We investigate the impact of two types of textual information in the ranking performances of a mathematical search system: words in context windows (baseline), which is easily extracted from sentence tokenization result, and descriptions, which are extracted using a machine learning method. We also examine the improvement in ranking obtained by utilizing the dependency graph of mathematical expressions. The experiment results show that the use of description and dependency graph together deliver better ranking performance than the use of context or when no textual information is used. The results also show that the dependency graph is crucial for increasing the number of mathematical expressions being assigned descriptions, and thus its use with descriptions together presented higher ranking performance than the use of descriptions only. This study suggests that descriptions represent mathematical expressions better (more precisely) than context windows, and even descriptions from child (indirect) expressions still represent the target expression better than the context from the target expression itself.},
  keywords = {Context,Data mining,Indexing,Measurement,Natural languages,Search engines,Semantics},
  file = {/Users/nvk/Zotero/storage/Y94Z4JMA/6991403.html}
}

@article{kristiantoExtractingTextualDescriptions2014,
  title = {Extracting {{Textual Descriptions}} of {{Mathematical Expressions}} in {{Scientific Papers}}},
  author = {Kristianto, Giovanni Yoko and Topi{\'c}, Goran and Aizawa, Akiko},
  year = {2014},
  month = nov,
  journal = {D-Lib Magazine},
  volume = {20},
  number = {11/12},
  issn = {1082-9873},
  doi = {10.1045/november14-kristianto},
  langid = {english},
  file = {/Users/nvk/Zotero/storage/D3S6JTUT/Kristianto et al. - 2014 - Extracting Textual Descriptions of Mathematical Ex.pdf}
}

@article{kristiantoExtractingTextualDescriptions2014a,
  title = {Extracting {{Textual Descriptions}} of {{Mathematical Expressions}} in {{Scientific Papers}}},
  author = {Kristianto, Giovanni Yoko and Topi{\'c}, Goran and Aizawa, Akiko},
  year = {2014},
  month = nov,
  journal = {D-Lib Magazine},
  volume = {20},
  number = {11/12},
  issn = {1082-9873},
  doi = {10.1045/november14-kristianto},
  langid = {english}
}

@article{kristiantoMCATMathRetrieval2014,
  title = {The {{MCAT Math Retrieval System}} for {{NTCIR-11 Math Track}}},
  author = {Kristianto, Giovanni Yoko and Topic, Goran and Ho, Florence},
  year = {2014},
  pages = {7},
  abstract = {This paper describes the participation of our MCAT search system in the NTCIR-11 Math-2 Task. The purpose of this task is to search mathematical expressions using hybrid queries containing both formulae and keywords. We introduce an encoding technique to capture the structure and content of the mathematical expressions. Each expression is accompanied by two types of automatically extracted textual information, namely words in context window and descriptions. In addition, we examine the improvement in ranking obtained by utilizing dependency graph of mathematical expressions and post-retrieval reranking method. The results show that the use of description and dependency graph together delivers better ranking performances than the use of context window. Furthermore, using both the description and context window together delivers even better results. The evaluation results also indicate that our reranking method is effective for improving the ranking performances.},
  langid = {english},
  file = {/Users/nvk/Zotero/storage/59ACJWS5/Kristianto et al. - 2014 - The MCAT Math Retrieval System for NTCIR-11 Math T.pdf}
}

@article{kristiantoMCATMathRetrieval2016,
  title = {{{MCAT Math Retrieval System}} for {{NTCIR-12 MathIR Task}}},
  author = {Kristianto, Giovanni Yoko and Topic, Goran and Aizawa, Akiko},
  year = {2016},
  pages = {9},
  abstract = {This paper describes the participation of our MCAT search system in the NTCIR-12 MathIR Task. We introduce three granularity levels of textual information, new approach for generating dependency graph of math expressions, score normalization, cold-start weights, and unification. We find that these modules, except the cold-start weights, have a very good impact on the search performance of our system. The use of dependency graph significantly improves precision of our system, i.e., up to 24.52\% and 104.20\% relative improvements in the Main and Simto subtasks of the arXiv task, respectively. In addition, the implementation of unification delivers up to 2.90\% and 57.14\% precision improvements in the Main and Simto subtasks, respectively. Overall, our best submission achieves P@5 of 0.5448 in the Main subtask and 0.5500 in the Simto subtask. In the Wikipedia task, our system also performs well at the MathWikiFormula subtask. At the MathWiki subtask, however, due to a problem with handling queries formed as questions that contain many stop words, our system finishes second.},
  langid = {english},
  file = {/Users/nvk/Zotero/storage/IKIHRRZF/Kristianto et al. - 2016 - MCAT Math Retrieval System for NTCIR-12 MathIR Tas.pdf}
}

@article{kristiantoUtilizingDependencyRelationships2017,
  title = {Utilizing Dependency Relationships between Math Expressions in Math {{IR}}},
  author = {Kristianto, Giovanni Yoko and Topi{\'c}, Goran and Aizawa, Akiko},
  year = {2017},
  month = apr,
  journal = {Information Retrieval Journal},
  volume = {20},
  number = {2},
  pages = {132--167},
  issn = {1386-4564, 1573-7659},
  doi = {10.1007/s10791-017-9296-8},
  abstract = {Current mathematical search systems allow math expressions within a document to be queried using math expressions and keywords. To accept such queries, math search systems must index both math expressions and textual information in documents. Each indexed math expression is usually associated with all the words in its surrounding context within a given window size. However, we found that this context is often ineffective for explaining math expressions in scientific papers. The meaning of an expression is usually defined in the early part of a document, and the meaning of each symbol contained in the expression can be useful for explaining the entire expression. This explanation may not be captured within the context of a math expression, unless we set the context to have a very wide window size. However, widening the window size also increases the proportion of words that are unrelated to the expression. This paper proposes the use of dependency relationships between math expressions to enrich the textual information of each expression. We examine the influence of this enrichment in a math search system. The experimental results show that significantly better precision can be obtained using the enriched textual information rather than the math expressions' own textual information. This indicates that the enrichment of textual information for each math expression using dependency relationships enhances the math search system.},
  langid = {english},
  file = {/Users/nvk/Zotero/storage/8VH2VNK5/Kristianto et al. - 2017 - Utilizing dependency relationships between math ex.pdf}
}

@article{kristiantoUtilizingDependencyRelationships2017a,
  title = {Utilizing Dependency Relationships between Math Expressions in Math {{IR}}},
  author = {Kristianto, Giovanni Yoko and Topi{\'c}, Goran and Aizawa, Akiko},
  year = {2017},
  month = apr,
  journal = {Information Retrieval Journal},
  volume = {20},
  number = {2},
  pages = {132--167},
  issn = {1573-7659},
  doi = {10.1007/s10791-017-9296-8},
  abstract = {Current mathematical search systems allow math expressions within a document to be queried using math expressions and keywords. To accept such queries, math search systems must index both math expressions and textual information in documents. Each indexed math expression is usually associated with all the words in its surrounding context within a given window size. However, we found that this context is often ineffective for explaining math expressions in scientific papers. The meaning of an expression is usually defined in the early part of a document, and the meaning of each symbol contained in the expression can be useful for explaining the entire expression. This explanation may not be captured within the context of a math expression, unless we set the context to have a very wide window size. However, widening the window size also increases the proportion of words that are unrelated to the expression. This paper proposes the use of dependency relationships between math expressions to enrich the textual information of each expression. We examine the influence of this enrichment in a math search system. The experimental results show that significantly better precision can be obtained using the enriched textual information rather than the math expressions' own textual information. This indicates that the enrichment of textual information for each math expression using dependency relationships enhances the math search system.},
  langid = {english},
  keywords = {Contextual information,Dependency graph,Mathematical expression encoding,Mathematical information retrieval},
  file = {/Users/nvk/Zotero/storage/872C7Z4X/Kristianto et al. - 2017 - Utilizing dependency relationships between math ex.pdf}
}

@article{krstovskiEquationEmbeddings2018,
  title = {Equation {{Embeddings}}},
  author = {Krstovski, Kriste and Blei, David M.},
  year = {2018},
  month = mar,
  journal = {arXiv:1803.09123 [cs, stat]},
  eprint = {1803.09123},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {We present an unsupervised approach for discovering semantic representations of mathematical equations. Equations are challenging to analyze because each is unique, or nearly unique. Our method, which we call equation embeddings, finds good representations of equations by using the representations of their surrounding words. We used equation embeddings to analyze four collections of scientific articles from the arXiv, covering four computer science domains (NLP, IR, AI, and ML) and \$\textbackslash sim\$98.5k equations. Quantitatively, we found that equation embeddings provide better models when compared to existing word embedding approaches. Qualitatively, we found that equation embeddings provide coherent semantic representations of equations and can capture semantic similarity to other equations and to words.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/nvk/Zotero/storage/S7I8CUQ9/Krstovski and Blei - 2018 - Equation Embeddings.pdf;/Users/nvk/Zotero/storage/7GGG8GC5/1803.html}
}

@article{krstovskiEquationEmbeddings2018a,
  title = {Equation {{Embeddings}}},
  author = {Krstovski, Kriste and Blei, David M.},
  year = {2018},
  month = mar,
  journal = {arXiv:1803.09123 [cs, stat]},
  eprint = {1803.09123},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {We present an unsupervised approach for discovering semantic representations of mathematical equations. Equations are challenging to analyze because each is unique, or nearly unique. Our method, which we call equation embeddings, finds good representations of equations by using the representations of their surrounding words. We used equation embeddings to analyze four collections of scientific articles from the arXiv, covering four computer science domains (NLP, IR, AI, and ML) and \$\textbackslash sim\$98.5k equations. Quantitatively, we found that equation embeddings provide better models when compared to existing word embedding approaches. Qualitatively, we found that equation embeddings provide coherent semantic representations of equations and can capture semantic similarity to other equations and to words.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/nvk/Zotero/storage/XN6JKAP3/Krstovski and Blei - 2018 - Equation Embeddings.pdf;/Users/nvk/Zotero/storage/G2RUVFSN/1803.html}
}

@article{krusePricingForwardStarting2005,
  title = {On the Pricing of Forward Starting Options in {{Heston}}'s Model on Stochastic Volatility},
  author = {Kruse, Susanne and N{\"o}gel, Ulrich},
  year = {2005},
  month = apr,
  journal = {Finance and Stochastics},
  volume = {9},
  number = {2},
  pages = {233--250},
  issn = {1432-1122},
  doi = {10.1007/s00780-004-0146-3},
  abstract = {We consider the problem of pricing European forward starting options in the presence of stochastic volatility. By performing a change of measure using the asset price at the time of strike determination as a numeraire, we derive a closed-form solution within Heston's stochastic volatility framework applying distribution properties of the volatility process. In this paper we develop a new and more suitable formula for pricing forward starting options. This formula allows to cover the smile effects observed in a Black-Scholes environment, in which the extreme exposure of forward starting options to volatility changes is ignored.},
  langid = {english},
  file = {/Users/nvk/Zotero/storage/FCRZ2XVQ/Kruse and NÃ¶gel - 2005 - On the pricing of forward starting options in Hest.pdf}
}

@article{lamportLaTEXUserGuide1986,
  title = {{{LaTEX}}: {{User}}'s {{Guide}} \& {{Reference Manual}}},
  author = {Lamport, Leslie},
  year = {1986},
  publisher = {{Addison-Wesley Reading}}
}

@inproceedings{lanMathematicalLanguageProcessing2015,
  title = {Mathematical {{Language Processing}}: {{Automatic Grading}} and {{Feedback}} for {{Open Response Mathematical Questions}}},
  shorttitle = {Mathematical {{Language Processing}}},
  booktitle = {Proceedings of the {{Second}} (2015) {{ACM Conference}} on {{Learning}} @ {{Scale}}},
  author = {Lan, Andrew S. and Vats, Divyanshu and Waters, Andrew E. and Baraniuk, Richard G.},
  year = {2015},
  month = mar,
  series = {L@{{S}} '15},
  pages = {167--176},
  publisher = {{Association for Computing Machinery}},
  address = {{New York, NY, USA}},
  doi = {10.1145/2724660.2724664},
  abstract = {While computer and communication technologies have provided effective means to scale up many aspects of education, the submission and grading of assessments such as homework assignments and tests remains a weak link. In this paper, we study the problem of automatically grading the kinds of open response mathematical questions that figure prominently in STEM (science, technology, engineering, and mathematics) courses. Our data-driven framework for mathematical language processing (MLP) leverages solution data from a large number of learners to evaluate the correctness of their solutions, assign partial-credit scores, and provide feedback to each learner on the likely locations of any errors. MLP takes inspiration from the success of natural language processing for text data and comprises three main steps. First, we convert each solution to an open response mathematical question into a series of numerical features. Second, we cluster the features from several solutions to uncover the structures of correct, partially correct, and incorrect solutions. We develop two different clustering approaches, one that leverages generic clustering algorithms and one based on Bayesian nonparametrics. Third, we automatically grade the remaining (potentially large number of) solutions based on their assigned cluster and one instructor-provided grade per cluster. As a bonus, we can track the cluster assignment of each step of a multistep solution and determine when it departs from a cluster of correct solutions, which enables us to indicate the likely locations of errors to learners. We test and validate MLP on real-world MOOC data to demonstrate how it can substantially reduce the human effort required in large-scale educational platforms.},
  isbn = {978-1-4503-3411-2},
  keywords = {assessment,automatic grading,bayesian nonparametrics,clustering,feedback,machine learning,mathematical language processing},
  file = {/Users/nvk/Zotero/storage/F6EAW862/Lan et al. - 2015 - Mathematical Language Processing Automatic Gradin.pdf}
}

@inproceedings{larsonAbjectFailureKeyword,
  title = {The {{Abject Failure}} of {{Keyword IR}} for {{Mathematics Search}}: {{Berkeley}} at {{NTCIR-10 Math}}.},
  author = {Larson, Ray R. and Reynolds, Chloe and Gey, Fredric C.}
}

@inproceedings{leDistributedRepresentationsSentences2014,
  title = {Distributed {{Representations}} of {{Sentences}} and {{Documents}}},
  booktitle = {International {{Conference}} on {{Machine Learning}}},
  author = {Le, Quoc and Mikolov, Tomas},
  year = {2014},
  month = jun,
  pages = {1188--1196},
  publisher = {{PMLR}},
  issn = {1938-7228},
  langid = {english},
  file = {/Users/nvk/Zotero/storage/Y57Q87N7/Le and Mikolov - 2014 - Distributed Representations of Sentences and Docum.pdf}
}

@inproceedings{leibinniEnergyefficientMatrixMultiplication2016,
  title = {An Energy-Efficient Matrix Multiplication Accelerator by Distributed in-Memory Computing on Binary {{RRAM}} Crossbar},
  booktitle = {2016 21st {{Asia}} and {{South Pacific Design Automation Conference}} ({{ASP-DAC}})},
  author = {Leibin Ni and Yuhao Wang and Yu, Hao and Wei Yang and Chuliang Weng and Junfeng Zhao},
  year = {2016},
  month = jan,
  pages = {280--285},
  issn = {2153-697X},
  doi = {10.1109/ASPDAC.2016.7428024},
  abstract = {Emerging resistive random-access memory (RRAM) can provide non-volatile memory storage but also intrinsic logic for matrix-vector multiplication, which is ideal for low-power and high-throughput data analytics accelerator performed in memory. However, the existing RRAM-based computing device is mainly assumed on a multi-level analog computing, whose result is sensitive to process non-uniformity as well as additional AD- conversion and I/O overhead. This paper explores the data analytics accelerator on binary RRAM-crossbar. Accordingly, one distributed in-memory computing architecture is proposed with design of according component and control protocol. Both memory array and logic accelerator can be implemented by RRAM-crossbar purely in binary, where logic-memory pairs can be distributed with protocol of control bus. Based on numerical results for fingerprint matching that is mapped on the proposed RRAM-crossbar, the proposed architecture has shown 2.86x faster speed, 154x better energy efficiency, and 100x smaller area when compared to the same design by CMOS-based ASIC.},
  keywords = {Arrays,binary RRAM crossbar,binary RRAM-crossbar,control protocol,data analysis,data analytics accelerator,Decoding,digital arithmetic,distributed in-memory computing architecture,distributed processing,energy-efficient matrix multiplication accelerator,fingerprint matching,intrinsic logic,logic accelerator,Logic arrays,logic circuits,logic-memory pairs,mathematics computing,matrix multiplication,matrix-vector multiplication,memory array,multilevel analog computing,nonvolatile memory storage,Process control,Protocols,Resistance,resistive RAM,resistive random-access memory,RRAM-based computing device,vectors},
  file = {/Users/nvk/Zotero/storage/TZG2RT6G/Leibin Ni et al. - 2016 - An energy-efficient matrix multiplication accelera.pdf;/Users/nvk/Zotero/storage/AKUFSJ4T/7428024.html}
}

@inproceedings{leibinniEnergyefficientMatrixMultiplication2016a,
  title = {An Energy-Efficient Matrix Multiplication Accelerator by Distributed in-Memory Computing on Binary {{RRAM}} Crossbar},
  booktitle = {2016 21st {{Asia}} and {{South Pacific Design Automation Conference}} ({{ASP-DAC}})},
  author = {Leibin Ni and Yuhao Wang and Yu, Hao and Wei Yang and Chuliang Weng and Junfeng Zhao},
  year = {2016},
  month = jan,
  pages = {280--285},
  issn = {2153-697X},
  doi = {10.1109/ASPDAC.2016.7428024},
  abstract = {Emerging resistive random-access memory (RRAM) can provide non-volatile memory storage but also intrinsic logic for matrix-vector multiplication, which is ideal for low-power and high-throughput data analytics accelerator performed in memory. However, the existing RRAM-based computing device is mainly assumed on a multi-level analog computing, whose result is sensitive to process non-uniformity as well as additional AD- conversion and I/O overhead. This paper explores the data analytics accelerator on binary RRAM-crossbar. Accordingly, one distributed in-memory computing architecture is proposed with design of according component and control protocol. Both memory array and logic accelerator can be implemented by RRAM-crossbar purely in binary, where logic-memory pairs can be distributed with protocol of control bus. Based on numerical results for fingerprint matching that is mapped on the proposed RRAM-crossbar, the proposed architecture has shown 2.86x faster speed, 154x better energy efficiency, and 100x smaller area when compared to the same design by CMOS-based ASIC.},
  keywords = {Arrays,binary RRAM crossbar,binary RRAM-crossbar,control protocol,data analysis,data analytics accelerator,Decoding,digital arithmetic,distributed in-memory computing architecture,distributed processing,energy-efficient matrix multiplication accelerator,fingerprint matching,intrinsic logic,logic accelerator,Logic arrays,logic circuits,logic-memory pairs,mathematics computing,matrix multiplication,matrix-vector multiplication,memory array,multilevel analog computing,nonvolatile memory storage,Process control,Protocols,Resistance,resistive RAM,resistive random-access memory,RRAM-based computing device,vectors},
  file = {/Users/nvk/Zotero/storage/T3YGKCX8/Leibin Ni et al. - 2016 - An energy-efficient matrix multiplication accelera.pdf;/Users/nvk/Zotero/storage/WVPB989W/7428024.html}
}

@inproceedings{libbrechtCrossCurriculumSearchIntergeo2008,
  title = {Cross-{{Curriculum Search}} for {{Intergeo}}},
  booktitle = {Intelligent {{Computer Mathematics}}},
  author = {Libbrecht, Paul and Desmoulins, Cyrille and Mercat, Christian and Laborde, Colette and Dietrich, Michael and Hendriks, Maxim},
  editor = {Autexier, Serge and Campbell, John and Rubio, Julio and Sorge, Volker and Suzuki, Masakazu and Wiedijk, Freek},
  year = {2008},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {520--535},
  publisher = {{Springer}},
  address = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-540-85110-3_42},
  abstract = {Intergeo is a European project dedicated to the sharing of interactive geometry constructions. This project is setting up an annotation and search web platform which will offer and provide access to thousands of interactive geometry constructions and resources using them. The search platform should cross the boundaries of the curriculum standards of Europe. A topics and competency based approach to retrieval for interactive geometry with designation of the semantic entities has been adopted: it requests the contributor of an interactive geometry resource to input the competencies and topics involved in a construction, and allows the searcher to find it by the input of competencies and topics close to them; both rely on plain-text-input.This paper describes the current prototypes, the input-methods, the workflows used, and the integration into the Intergeo platform.},
  isbn = {978-3-540-85110-3},
  langid = {english},
  keywords = {Competency Base Approach,Curriculum Standard,False Friend,Interactive Geometry,Semantic Entity},
  file = {/Users/nvk/Zotero/storage/J9DQVI3N/Libbrecht et al. - 2008 - Cross-Curriculum Search for Intergeo.pdf}
}

@inproceedings{liMergingInterfacePower2015,
  title = {Merging the Interface: Power, Area and Accuracy Co-Optimization for {{RRAM}} Crossbar-Based Mixed-Signal Computing System},
  shorttitle = {Merging the Interface},
  booktitle = {Proceedings of the 52nd {{Annual Design Automation Conference}}},
  author = {Li, Boxun and Xia, Lixue and Gu, Peng and Wang, Yu and Yang, Huazhong},
  year = {2015},
  month = jun,
  series = {{{DAC}} '15},
  pages = {1--6},
  publisher = {{Association for Computing Machinery}},
  address = {{San Francisco, California}},
  doi = {10.1145/2744769.2744870},
  abstract = {The invention of resistive-switching random access memory (RRAM) devices and RRAM crossbar-based computing system (RCS) demonstrate a promising solution for better performance and power efficiency. The interfaces between analog and digital units, especially AD/DAs, take up most of the area and power consumption of RCS and are always the bottleneck of mixed-signal computing systems. In this work, we propose a novel architecture, MEI, to minimize the overhead of AD/DA by MErging the Interface into the RRAM crossbar. An optional ensemble method, the Serial Array Adaptive Boosting (SAAB), is also introduced to take advantage of the area and power saved by MEI and boost the accuracy and robustness of RCS. On top of these two methods, a design space exploration is proposed to achieve trade-offs among accuracy, area, and power consumption. Experimental results on 6 diverse benchmarks demonstrate that, compared with the traditional architecture with AD/DAs, MEI is able to save 54.63\%\textasciitilde 86.14\% area and reduce 61.82\%\textasciitilde 86.80\% power consumption under quality guarantees; and SAAB can further improve the accuracy by 5.76\% on average and ensure the system performance under noisy conditions.},
  isbn = {978-1-4503-3520-1},
  file = {/Users/nvk/Zotero/storage/G6DFX29C/Li et al. - 2015 - Merging the interface power, area and accuracy co.pdf}
}

@incollection{liMultilevelInteractiveLifelog2020,
  title = {A {{Multi-level Interactive Lifelog Search Engine}} with {{User Feedback}}},
  booktitle = {Proceedings of the {{Third Annual Workshop}} on {{Lifelog Search Challenge}}},
  author = {Li, Jiayu and Zhang, Min and Ma, Weizhi and Liu, Yiqun and Ma, Shaoping},
  year = {2020},
  month = jun,
  pages = {29--35},
  publisher = {{Association for Computing Machinery}},
  address = {{New York, NY, USA}},
  abstract = {With the rise of portable wearable devices, it is easier for users to save their lifelog data. As lifelog is usually disorganized with multi-modal information (even noisy sometimes), an interactive search engine is crucial for users to review and explore their lifelog. Unlike traditional search engines, lifelog search includes multi-modality information of images, text and other data from sensors, which brings challenges to data arrangement and search. Accordingly, users' information need is also multi-level. Hence, a single interaction mechanism may not be able to satisfy users' requirements. As the data set is highly personalized, interaction and feedback from users should also be considered in the search engine. Therefore, in this paper we present an interactive multi-modality lifelog search engine to help users manage and find lifelog data. To this end, lifelog data is clustered and processed in multi-level processing. Then, we build an interactive search engine, includingtext as query, image as query, andtimeline view modules. Besides, the system is able to adopt user feedback mechanisms in multi-round queries. Our system shows promising experimental results on LSC'20 dataset and development topics. The text-based search module gives correct results on more than 60\% of the development topics at LSC'20.},
  isbn = {978-1-4503-7136-0},
  keywords = {information retrieval,interactive search engine,lifelogging},
  file = {/Users/nvk/Zotero/storage/Q6NXRJ34/Li et al. - 2020 - A Multi-level Interactive Lifelog Search Engine wi.pdf}
}

@inproceedings{linMathematicsRetrievalSystem2014,
  title = {A Mathematics Retrieval System for Formulae in Layout Presentations},
  booktitle = {Proceedings of the 37th International {{ACM SIGIR}} Conference on {{Research}} \& Development in Information Retrieval},
  author = {Lin, Xiaoyan and Gao, Liangcai and Hu, Xuan and Tang, Zhi and Xiao, Yingnan and Liu, Xiaozhong},
  year = {2014},
  month = jul,
  pages = {697--706},
  publisher = {{ACM}},
  address = {{Gold Coast Queensland Australia}},
  doi = {10.1145/2600428.2609611},
  abstract = {The semantics of mathematical formulae depend on their spatial structure, and they usually exist in layout presentations such as PDF, LATEX, and Presentation MathML, which challenges previous text index and retrieval methods. This paper proposes an innovative mathematics retrieval system along with the novel algorithms, which enables efficient formula index and retrieval from both webpages and PDF documents. Unlike prior studies, which require users to manually input formula markup language as query, the new system enables users to ``copy'' formula queries directly from PDF documents. Furthermore, by using a novel indexing and matching model, the system is aimed at searching for similar mathematical formulae based on both textual and spatial similarities. A hierarchical generalization technique is proposed to generate sub-trees from the semi-operator tree of formulae and support substructure match and fuzzy match. Experiments based on massive Wikipedia and CiteSeer repositories show that the new system along with novel algorithms, comparing with two representative mathematics retrieval systems, provides more efficient mathematical formula index and retrieval, while simplifying user query input for PDF documents.},
  isbn = {978-1-4503-2257-7},
  langid = {english},
  file = {/Users/nvk/Zotero/storage/7XHU4B9H/Lin et al. - 2014 - A mathematics retrieval system for formulae in lay.pdf}
}

@article{linSmartContentbasedImage2009,
  title = {A Smart Content-Based Image Retrieval System Based on Color and Texture Feature},
  author = {Lin, Chuen-Horng and Chen, Rong-Tai and Chan, Yung-Kuan},
  year = {2009},
  month = may,
  journal = {Image and Vision Computing},
  volume = {27},
  number = {6},
  pages = {658--665},
  issn = {0262-8856},
  doi = {10.1016/j.imavis.2008.07.004},
  abstract = {In this paper, three image features are proposed for image retrieval. In addition, a feature selection technique is also brought forward to select optimal features to not only maximize the detection rate but also simplify the computation of image retrieval. The first and second image features are based on color and texture features, respectively called color co-occurrence matrix (CCM) and difference between pixels of scan pattern (DBPSP) in this paper. The third image feature is based on color distribution, called color histogram for K-mean (CHKM). CCM is the conventional pattern co-occurrence matrix that calculates the probability of the occurrence of same pixel color between each pixel and its adjacent ones in each image, and this probability is considered as the attribute of the image. According to the sequence of motifs of scan patterns, DBPSP calculates the difference between pixels and converts it into the probability of occurrence on the entire image. Each pixel color in an image is then replaced by one color in the common color palette that is most similar to color so as to classify all pixels in image into k-cluster, called the CHKM feature. Difference in image properties and contents indicates that different features are contained. Some images have stronger color and texture features, while others are more sensitive to color and spatial features. Thus, this study integrates CCM, DBPSP, and CHKM to facilitate image retrieval. To enhance image detection rate and simplify computation of image retrieval, sequential forward selection is adopted for feature selection. Besides, based on the image retrieval system (CTCHIRS), a series of analyses and comparisons are performed in our experiment. Three image databases with different properties are used to carry out feature selection. Optimal features are selected from original features to enhance the detection rate.},
  langid = {english},
  keywords = {Co-occurrence,Color,Feature selection,Image retrieval,Motif,Texture}
}

@inproceedings{liOnlineRecognitionHandwritten2008,
  title = {Online Recognition of Handwritten Mathematical Expressions with Support for Matrices},
  booktitle = {2008 19th {{International Conference}} on {{Pattern Recognition}}},
  author = {Li, Chuanjun and Zeleznik, Robert and Miller, Timothy and LaViola, Joseph J.},
  year = {2008},
  month = dec,
  pages = {1--4},
  issn = {1051-4651},
  doi = {10.1109/ICPR.2008.4761825},
  abstract = {We present an online system for recognizing handwritten mathematical matrices in the context of an interactive computational tool called MathPaper. Automatic segmentation and recognition of multiple expressions are supported based on a spacing algorithm that leverages recognized symbol identities, sizes, and relative locations of individual symbols. Matrices with ellipses can be recognized and instantiated with non-ellipsis elements. Both well- and non-well-formed matrices can also be recognized. Matrix elements can be any general mathematical expressions including imbedded matrices. Our recognizer also addresses the poor column alignment problem of handwritten matrices, and allows for slight horizontal overlaps between elements in neighboring columns and different rows.},
  keywords = {Engines,Genetic expression,Handwriting recognition,handwritten character recognition,handwritten mathematical expressions,handwritten matrices,imbedded matrices,Ink,mathematics computing,MathPaper,matrix algebra,Matrix converters,online recognition,Personal communication networks,Testing,Tree data structures,Typesetting,User interfaces},
  file = {/Users/nvk/Zotero/storage/NC7XX9QT/Li et al. - 2008 - Online recognition of handwritten mathematical exp.pdf;/Users/nvk/Zotero/storage/TS7F78PT/4761825.html}
}

@inproceedings{liskaCombiningTextFormula2015,
  title = {Combining {{Text}} and {{Formula Queries}} in {{Math Information Retrieval}}: {{Evaluation}} of {{Query Results Merging Strategies}}},
  shorttitle = {Combining {{Text}} and {{Formula Queries}} in {{Math Information Retrieval}}},
  booktitle = {Proceedings of the {{First International Workshop}} on {{Novel Web Search Interfaces}} and {{Systems}}},
  author = {L{\'i}{\v s}ka, Martin and Sojka, Petr and R{\r{u}}{\v z}i{\v c}ka, Michal},
  year = {2015},
  month = oct,
  series = {{{NWSearch}} '15},
  pages = {7--9},
  publisher = {{Association for Computing Machinery}},
  address = {{New York, NY, USA}},
  doi = {10.1145/2810355.2810359},
  abstract = {Specific to Math Information Retrieval is combining text with mathematical formulae both in documents and in queries. Rigorous evaluation of query expansion and merging strategies combining math and standard textual keyword terms in a query are given. It is shown that techniques similar to those known from textual query processing may be applied in math information retrieval as well, and lead to a cutting edge performance. Striping and merging partial results from subqueries is one technique that improves results measured by information retrieval evaluation metrics like Bpref.},
  isbn = {978-1-4503-3789-2},
  keywords = {algorithms,digital mathematical libraries,math indexing and retrieval,performance,query expansion,query reformulation,ranking},
  file = {/Users/nvk/Zotero/storage/J6637Q7I/LÃ­Å¡ka et al. - 2015 - Combining Text and Formula Queries in Math Informa.pdf}
}

@inproceedings{liTrainingItselfMixedsignal2014,
  title = {Training Itself: {{Mixed-signal}} Training Acceleration for Memristor-Based Neural Network},
  shorttitle = {Training Itself},
  booktitle = {2014 19th {{Asia}} and {{South Pacific Design Automation Conference}} ({{ASP-DAC}})},
  author = {Li, Boxun and Wang, Yuzhi and Wang, Yu and Chen, Yiran and Yang, Huazhong},
  year = {2014},
  month = jan,
  pages = {361--366},
  issn = {2153-697X},
  doi = {10.1109/ASPDAC.2014.6742916},
  abstract = {The artificial neural network (ANN) is among the most widely used methods in data processing applications. The memristor-based neural network further demonstrates a power efficient hardware realization of ANN. Training phase is the critical operation of memristor-based neural network. However, the traditional training method for memristor-based neural network is time consuming and energy inefficient. Users have to first work out the parameters of memristors through digital computing systems and then tune the memristor to the corresponding state. In this work, we introduce a mixed-signal training acceleration framework, which realizes the self-training of memristor-based neural network. We first modify the original stochastic gradient descent algorithm by approximating calculations and designing an alternative computing method. We then propose a mixed-signal acceleration architecture for the modified training algorithm by equipping the original memristor-based neural network architecture with the copy crossbar technique, weight update units, sign calculation units and other assistant units. The experiment on the MNIST database demonstrates that the proposed mixed-signal acceleration is 3 orders of magnitude faster and 4 orders of magnitude more energy efficient than the CPU implementation counterpart at the cost of a slight decrease of the recognition accuracy ({$<$}; 5\%).},
  keywords = {Acceleration,Arrays,artificial neural network,Convergence,digital computing systems,gradient methods,learning (artificial intelligence),Memristor,memristor-based neural network,memristors,Memristors,Mirrors,mixed-signal training acceleration,neural nets,Neural Network,Neural networks,power efficient hardware realization,stochastic gradient descent algorithm,stochastic processes,Training,training phase},
  file = {/Users/nvk/Zotero/storage/JJVS2V3M/Li et al. - 2014 - Training itself Mixed-signal training acceleratio.pdf;/Users/nvk/Zotero/storage/I8RR9IPS/6742916.html}
}

@inproceedings{liuInternetNewsHeadlines2012,
  title = {Internet News Headlines Classification Method Based on the {{N-Gram}} Language Model},
  booktitle = {2012 {{International Conference}} on {{Computer Science}} and {{Information Processing}} ({{CSIP}})},
  author = {Liu, Xin and Rujia, Gao and Liufu, Song},
  year = {2012},
  month = aug,
  pages = {826--828},
  doi = {10.1109/CSIP.2012.6308980},
  abstract = {This paper aiming at the Internet news headlines short text classification. After analysis of the traditional classification model and the characteristics of Internet news headlines, this paper presents a classification model of the N-Gram language model as the Internet news headlines. Internet news headlines classification process is divided into three modules, the preprocessing module, the training module and the prediction module. Designing a classification algorithm based on N-Gram language model Internet news headlines. The algorithm classify the Internet news headlines by calculating the probability value of the words string of unclassified and category C, while calculating the probability value it can also take into account the relevance of the previous term. So it has better classification performance.},
  keywords = {Editorials,Frequency statistics,Internet news headlines,N-Gram language model,short text classification},
  file = {/Users/nvk/Zotero/storage/HH34WE55/Liu et al. - 2012 - Internet news headlines classification method base.pdf;/Users/nvk/Zotero/storage/QQDZAYV9/6308980.html}
}

@inproceedings{lohmannRelFinderUserInterface2010,
  title = {The {{RelFinder}} User Interface: Interactive Exploration of Relationships between Objects of Interest},
  shorttitle = {The {{RelFinder}} User Interface},
  booktitle = {Proceedings of the 15th International Conference on {{Intelligent}} User Interfaces},
  author = {Lohmann, Steffen and Heim, Philipp and Stegemann, Timo and Ziegler, J{\"u}rgen},
  year = {2010},
  month = feb,
  series = {{{IUI}} '10},
  pages = {421--422},
  publisher = {{Association for Computing Machinery}},
  address = {{New York, NY, USA}},
  doi = {10.1145/1719970.1720052},
  abstract = {Being aware of the relationships that exist between objects of interest is crucial in many situations. The RelFinder user interface helps to get an overview: Even large amounts of relationships can be visualized, filtered, and analyzed by the user. Common concepts of knowledge representation are exploited in order to support interactive exploration both on the level of global filters and single relationships. The RelFinder is easy-to-use and works on every RDF knowledge base that provides standardized SPARQL access},
  isbn = {978-1-60558-515-4},
  keywords = {dbpedia,decision support,graph visualization,linked data,relationship discovery,relationship web,semantic user interfaces,semantic web,sparql,visual exploration}
}

@article{lohSplitSelectionMethods1997,
  title = {Split {{Selection Methods}} for {{Classification Trees}}},
  author = {Loh, Wei-Yin and Shih, Yu-Shan},
  year = {1997},
  journal = {Statistica Sinica},
  volume = {7},
  number = {4},
  pages = {815--840},
  publisher = {{Institute of Statistical Science, Academia Sinica}},
  issn = {1017-0405},
  abstract = {Classification trees based on exhaustive search algorithms tend to be biased towards selecting variables that afford more splits. As a result, such trees should be interpreted with caution. This article presents an algorithm called QUEST that has negligible bias. Its split selection strategy shares similarities with the FACT method, but it yields binary splits and the final tree can be selected by a direct stopping rule or by pruning. Real and simulated data are used to compare QUEST with the exhaustive search approach. QUEST is shown to be substantially faster and the size and classification accuracy of its trees are typically comparable to those of exhaustive search.}
}

@article{lozierNISTDigitalLibrary2003,
  title = {{{NIST Digital Library}} of {{Mathematical Functions}}},
  author = {Lozier, Daniel W.},
  year = {2003},
  month = may,
  journal = {Annals of Mathematics and Artificial Intelligence},
  volume = {38},
  number = {1},
  pages = {105--119},
  issn = {1573-7470},
  doi = {10.1023/A:1022915830921},
  abstract = {The National Institute of Standards and Technology is preparing a Digital Library of Mathematical Functions (DLMF) to provide useful data about special functions for a wide audience. The initial products will be a published handbook and companion Web site, both scheduled for completion in 2003. More than 50 mathematicians, physicists and computer scientists from around the world are participating in the work. The data to be covered include mathematical formulas, graphs, references, methods of computation, and links to software. Special features of the Web site include 3D interactive graphics and an equation search capability. The information technology tools that are being used are, of necessity, ones that are widely available now, even though better tools are in active development. For example, LaTeX files are being used as the common source for both the handbook and the Web site. This is the technology of choice for presentation of mathematics in print but it is not well suited to equation search, for example, or for input to computer algebra systems. These and other problems, and some partially successful work-arounds, are discussed in this paper and in the companion paper by Miller and Youssef.},
  langid = {english},
  file = {/Users/nvk/Zotero/storage/P5ZERPL3/Lozier - 2003 - NIST Digital Library of Mathematical Functions.pdf}
}

@incollection{luGenderBiasNeural2020,
  title = {Gender {{Bias}} in {{Neural Natural Language Processing}}},
  booktitle = {Logic, {{Language}}, and {{Security}}: {{Essays Dedicated}} to {{Andre Scedrov}} on the {{Occasion}} of {{His}} 65th {{Birthday}}},
  author = {Lu, Kaiji and Mardziel, Piotr and Wu, Fangjing and Amancharla, Preetam and Datta, Anupam},
  editor = {Nigam, Vivek and Ban Kirigin, Tajana and Talcott, Carolyn and Guttman, Joshua and Kuznetsov, Stepan and Thau Loo, Boon and Okada, Mitsuhiro},
  year = {2020},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {189--202},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-030-62077-6_14},
  abstract = {We examine whether neural natural language processing (NLP) systems reflect historical biases in training data. We define a general benchmark to quantify gender bias in a variety of neural NLP tasks. Our empirical evaluation with state-of-the-art neural coreference resolution and textbook RNN-based language models trained on benchmark data sets finds significant gender bias in how models view occupations. We then mitigate bias with counterfactual data augmentation (CDA): a generic methodology for corpus augmentation via causal interventions that breaks associations between gendered and gender-neutral words. We empirically show that CDA effectively decreases gender bias while preserving accuracy. We also explore the space of mitigation strategies with CDA, a prior approach to word embedding debiasing (WED), and their compositions. We show that CDA outperforms WED, drastically so when word embeddings are trained. For pre-trained embeddings, the two methods can be effectively composed. We also find that as training proceeds on the original data set with gradient descent the gender bias grows as the loss reduces, indicating that the optimization encourages bias; CDA mitigates this behavior.},
  isbn = {978-3-030-62077-6},
  langid = {english},
  keywords = {Deep learning,Fairness,Machine learning,Natural language processing},
  file = {/Users/nvk/Zotero/storage/UXIU9JWT/Lu et al. - 2020 - Gender Bias in Neural Natural Language Processing.pdf}
}

@article{luhnStatisticalApproachMechanized1957,
  title = {A {{Statistical Approach}} to {{Mechanized Encoding}} and {{Searching}} of {{Literary Information}}},
  author = {Luhn, H. P.},
  year = {1957},
  month = oct,
  journal = {IBM Journal of Research and Development},
  volume = {1},
  number = {4},
  pages = {309--317},
  issn = {0018-8646},
  doi = {10.1147/rd.14.0309},
  abstract = {Written communication of ideas is carried out on the basis of statistical probability in that a writer chooses that level of subject specificity and that combination of words which he feels will convey the most meaning. Since this process varies among individuals and since similar ideas are therefore relayed at different levels of specificity and by means of different words, the problem of literature searching by machines still presents major difficulties. A statistical approach to this problem will be outlined and the various steps of a system based on this approach will be described. Steps include the statistical analysis of a collection of documents in a field of interest, the establishment of a set of ``notions'' and the vocabulary by which they are expressed, the compilation of a thesaurus-type dictionary and index, the automatic encoding of documents by machine with the aid of such a dictionary, the encoding of topological notations (such as branched structures), the recording of the coded information, the establishment of a searching pattern for finding pertinent information, and the programming of appropriate machines to carry out a search.},
  file = {/Users/nvk/Zotero/storage/ASY3EIMY/5392697.html}
}

@article{luiCrossdomainFeatureSelection,
  title = {Cross-Domain {{Feature Selection}} for {{Language Identification}}},
  author = {Lui, Marco and Baldwin, Timothy},
  pages = {9},
  abstract = {We show that transductive (cross-domain) learning is an important consideration in building a general-purpose language identification system, and develop a feature selection method that generalizes across domains. Our results demonstrate that our method provides improvements in transductive transfer learning for language identification. We provide an implementation of the method and show that our system is faster than popular standalone language identification systems, while maintaining competitive accuracy.},
  langid = {english},
  file = {/Users/nvk/Zotero/storage/XCSDYRMK/Lui and Baldwin - Cross-domain Feature Selection for Language Identi.pdf}
}

@article{luiCrossdomainFeatureSelectiona,
  title = {Cross-Domain {{Feature Selection}} for {{Language Identification}}},
  author = {Lui, Marco and Baldwin, Timothy},
  pages = {9},
  abstract = {We show that transductive (cross-domain) learning is an important consideration in building a general-purpose language identification system, and develop a feature selection method that generalizes across domains. Our results demonstrate that our method provides improvements in transductive transfer learning for language identification. We provide an implementation of the method and show that our system is faster than popular standalone language identification systems, while maintaining competitive accuracy.},
  langid = {english},
  file = {/Users/nvk/Zotero/storage/PJGASEQ4/Lui and Baldwin - Cross-domain Feature Selection for Language Identi.pdf}
}

@article{luiLangidPyOfftheshelf,
  title = {Langid.Py: {{An Off-the-shelf Language Identification Tool}}},
  author = {Lui, Marco and Baldwin, Timothy},
  pages = {6},
  abstract = {We present langid.py, an off-the-shelf language identification tool. We discuss the design and implementation of langid.py, and provide an empirical comparison on 5 longdocument datasets, and 2 datasets from the microblog domain. We find that langid.py maintains consistently high accuracy across all domains, making it ideal for end-users that require language identification without wanting to invest in preparation of in-domain training data.},
  langid = {english},
  file = {/Users/nvk/Zotero/storage/JILBDUI5/Lui and Baldwin - langid.py An Off-the-shelf Language Identificatio.pdf}
}

@article{luiLangidPyOfftheshelfa,
  title = {Langid.Py: {{An Off-the-shelf Language Identification Tool}}},
  author = {Lui, Marco and Baldwin, Timothy},
  pages = {6},
  abstract = {We present langid.py, an off-the-shelf language identification tool. We discuss the design and implementation of langid.py, and provide an empirical comparison on 5 longdocument datasets, and 2 datasets from the microblog domain. We find that langid.py maintains consistently high accuracy across all domains, making it ideal for end-users that require language identification without wanting to invest in preparation of in-domain training data.},
  langid = {english},
  file = {/Users/nvk/Zotero/storage/69BWKQVV/Lui and Baldwin - langid.py An Off-the-shelf Language Identificatio.pdf}
}

@misc{lukasiewiczPhilosophischeBemerkungenMehrwertigen1930,
  title = {Philosophische {{Bemerkungen}} Zu Mehrwertigen {{Systemen}} Des {{Aussagenkalk\"uls}}},
  author = {Lukasiewicz, Jan},
  year = {1930},
  publisher = {{Comtes Rendus des S eances de la Soci et e des Sciences et des Lettres de Varsovie, cl}}
}

@article{luMultiaspectSentimentAnalysis,
  title = {Multi-Aspect {{Sentiment Analysis}} with {{Topic Models}}},
  author = {Lu, Bin and Ott, Myle and Cardie, Claire and Tsou, Benjamin},
  pages = {8},
  abstract = {We investigate the efficacy of topic model based approaches to two multi-aspect sentiment analysis tasks: multiaspect sentence labeling and multi-aspect rating prediction. For sentence labeling, we propose a weakly-supervised approach that utilizes only minimal prior knowledge\textemdash in the form of seed words\textemdash to enforce a direct correspondence between topics and aspects. This correspondence is used to label sentences with performance that approaches a fully supervised baseline. For multi-aspect rating prediction, we find that overall ratings can be used in conjunction with our sentence labelings to achieve reasonable performance compared to a fully supervised baseline. When gold-standard aspect-ratings are available, we find that topic model based features can be used to improve unsophisticated supervised baseline performance, in agreement with previous multi-aspect rating prediction work. This improvement is diminished, however, when topic model features are paired with a more competitive supervised baseline\textemdash a finding not acknowledged in previous work.},
  langid = {english},
  file = {/Users/nvk/Zotero/storage/4MTXAA7I/Lu et al. - Multi-aspect Sentiment Analysis with Topic Models.pdf}
}

@article{luuVTRNextGeneration2014,
  title = {{{VTR}} 7.0: {{Next Generation Architecture}} and {{CAD System}} for {{FPGAs}}},
  shorttitle = {{{VTR}} 7.0},
  author = {Luu, Jason and Goeders, Jeffrey and Wainberg, Michael and Somerville, Andrew and Yu, Thien and Nasartschuk, Konstantin and Nasr, Miad and Wang, Sen and Liu, Tim and Ahmed, Nooruddin and Kent, Kenneth B. and Anderson, Jason and Rose, Jonathan and Betz, Vaughn},
  year = {2014},
  month = jul,
  journal = {ACM Transactions on Reconfigurable Technology and Systems},
  volume = {7},
  number = {2},
  pages = {6:1--6:30},
  issn = {1936-7406},
  doi = {10.1145/2617593},
  abstract = {Exploring architectures for large, modern FPGAs requires sophisticated software that can model and target hypothetical devices. Furthermore, research into new CAD algorithms often requires a complete and open source baseline CAD flow. This article describes recent advances in the open source Verilog-to-Routing (VTR) CAD flow that enable further research in these areas. VTR now supports designs with multiple clocks in both timing analysis and optimization. Hard adder/carry logic can be included in an architecture in various ways and significantly improves the performance of arithmetic circuits. The flow now models energy consumption, an increasingly important concern. The speed and quality of the packing algorithms have been significantly improved. VTR can now generate a netlist of the final post-routed circuit which enables detailed simulation of a design for a variety of purposes. We also release new FPGA architecture files and models that are much closer to modern commercial architectures, enabling more realistic experiments. Finally, we show that while this version of VTR supports new and complex features, it has a 1.5\texttimes{} compile time speed-up for simple architectures and a 6\texttimes{} speed-up for complex architectures compared to the previous release, with no degradation to timing or wire-length quality.},
  keywords = {architecture modeling,CAD,FPGA},
  file = {/Users/nvk/Zotero/storage/WAXRATCC/Luu et al. - 2014 - VTR 7.0 Next Generation Architecture and CAD Syst.pdf}
}

@inproceedings{magerFindingsAmericasNLP20212021,
  title = {Findings of the {{AmericasNLP}} 2021 {{Shared Task}} on {{Open Machine Translation}} for {{Indigenous Languages}} of the {{Americas}}},
  booktitle = {Mager, {{Manuel}}; {{Oncevay}}, {{Arturo}}; {{Ebrahimi}}, {{Abteen}}; {{Ortega}}, {{John}}; {{Rios}}, {{Annette}}; {{Fan}}, {{Angela}}; {{Gutierrez-Vasques}}, {{Ximena}}; {{Chiruzzo}}, {{Luis}}; {{Gim\'enez-Lugo}}, {{Gustavo}}; {{Ramos}}, {{Ricardo}}; {{Meza Ruiz}}, {{Ivan Vladimir}}; {{Coto-Solano}}, {{Rolando}}; {{Palmer}}, {{Alexis}}; {{Mager-Hois}}, {{Elisabeth}}; {{Chaudhary}}, {{Vishrav}}; {{Neubig}}, {{Graham}}; {{Vu}}, {{Ngoc Thang}}; {{Kann}}, {{Katharina}}  (2021). {{Findings}} of the {{AmericasNLP}} 2021 {{Shared Task}} on {{Open Machine Translation}} for {{Indigenous Languages}} of the {{Americas}}.  {{In}}: {{Proceedings}} of the {{First Workshop}} on {{Natural Language Processing}} for {{Indigenous Languages}} of the {{Americas}}, {{Online}}, 11 {{June}} 2021. {{Association}} for {{Computational Linguistics}}, 202-217.},
  author = {Mager, Manuel and Oncevay, Arturo and Ebrahimi, Abteen and Ortega, John and Rios, Annette and Fan, Angela and {Gutierrez-Vasques}, Ximena and Chiruzzo, Luis and {Gim{\'e}nez-Lugo}, Gustavo and Ramos, Ricardo and Meza Ruiz, Ivan Vladimir and {Coto-Solano}, Rolando and Palmer, Alexis and {Mager-Hois}, Elisabeth and Chaudhary, Vishrav and Neubig, Graham and Vu, Ngoc Thang and Kann, Katharina},
  year = {2021},
  month = jun,
  pages = {202--217},
  publisher = {{Association for Computational Linguistics}},
  address = {{Online}},
  doi = {10.5167/uzh-203438},
  abstract = {This paper presents the results of the 2021 Shared Task on Open Machine Translation for Indigenous Languages of the Americas. The shared task featured two independent tracks, and participants submitted machine translation systems for up to 10 indigenous languages. Overall, 8 teams participated with a total of 214 submissions. We provided training sets consisting of data collected from various sources, as well as manually translated sentences for the development and test sets. An official baseline trained on this data was also provided. Team submissions featured a variety of architectures, including both statistical and neural models, and for the majority of languages, many teams were able to considerably improve over the baseline. The best performing systems achieved 12.97 ChrF higher than baseline, when averaged across languages.},
  copyright = {info:eu-repo/semantics/openAccess},
  langid = {english},
  file = {/Users/nvk/Zotero/storage/IBVL57VH/Mager et al. - 2021 - Findings of the AmericasNLP 2021 Shared Task on Op.pdf;/Users/nvk/Zotero/storage/9XJCQX6Y/203438.html}
}

@phdthesis{mahesarComputingRelativelyLarge2014,
  type = {D\_ph},
  title = {Computing Relatively Large Algebraic Structures by Automated Theory Exploration},
  author = {Mahesar, Quratul-ain},
  year = {2014},
  month = jul,
  abstract = {Automated reasoning technology provides means for inference in a formal context via a multitude of disparate reasoning techniques. Combining different techniques not only increases the effectiveness of single systems but also provides a more powerful approach to solving hard problems. Consequently combined reasoning systems have been successfully employed to solve non-trivial mathematical problems in combinatorially rich domains that are intractable by traditional mathematical means.  Nevertheless, the lack of domain specific knowledge often limits the effectiveness of these systems.  In this thesis we investigate how the combination of diverse reasoning techniques can be employed to pre-compute additional knowledge to enable mathematical discovery in finite and potentially infinite domains that is otherwise not feasible. In particular, we demonstrate how we can exploit bespoke symbolic computations and automated theorem proving to automatically compute and evolve the structural knowledge of small size finite structures in the algebraic theory of quasigroups. This allows us to increase the solvability horizon of model generation systems to find solution models for large size finite algebraic structures previously unattainable. We also present an approach to exploring infinite models using a mixture of automated tools and user interaction to iteratively inspect the structure of solutions and refine search. A practical implementation combines a specialist term rewriting system with bespoke graph algorithms and visualization tools and has been applied to solve the generalized version of Kuratowski's classical closure-complement problem from point-set topology that had remained open for several years.},
  langid = {english},
  school = {University of Birmingham},
  file = {/Users/nvk/Zotero/storage/4S2XLIVM/Mahesar - 2014 - Computing relatively large algebraic structures by.pdf;/Users/nvk/Zotero/storage/HG2NA46L/5023.html}
}

@misc{MakingMusicInformation,
  title = {â€ª{{Making Music Information Retrieval Evaluation Scenarios}} a {{Reality}}â€¬},
  abstract = {â€ªJS Downieâ€¬, â€ªISMIR 2003: Proceedings of the Fourth International Conference on Music~\ldots, 2003â€¬},
  howpublished = {https://scholar.google.com/citations?view\_op=view\_citation\&hl=en\&user=fJLNqvIAAAAJ\&cstart=200\&pagesize=100\&sortby=pubdate\&citation\_for\_view=fJLNqvIAAAAJ:sSrBHYA8nusC},
  file = {/Users/nvk/Zotero/storage/HSS22UGS/citations.html}
}

@article{maliScanSSDScanningSingle2020,
  title = {{{ScanSSD}}: {{Scanning Single Shot Detector}} for {{Mathematical Formulas}} in {{PDF Document Images}}},
  shorttitle = {{{ScanSSD}}},
  author = {Mali, Parag and Kukkadapu, Puneeth and Mahdavi, Mahshad and Zanibbi, Richard},
  year = {2020},
  month = mar,
  journal = {arXiv:2003.08005 [cs]},
  eprint = {2003.08005},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {We introduce the Scanning Single Shot Detector (ScanSSD) for locating math formulas offset from text and embedded in textlines. ScanSSD uses only visual features for detection: no formatting or typesetting information such as layout, font, or character labels are employed. Given a 600 dpi document page image, a Single Shot Detector (SSD) locates formulas at multiple scales using sliding windows, after which candidate detections are pooled to obtain page-level results. For our experiments we use the TFD-ICDAR2019v2 dataset, a modification of the GTDB scanned math article collection. ScanSSD detects characters in formulas with high accuracy, obtaining a 0.926 f-score, and detects formulas with high recall overall. Detection errors are largely minor, such as splitting formulas at large whitespace gaps (e.g., for variable constraints) and merging formulas on adjacent textlines. Formula detection f-scores of 0.796 (IOU \$\textbackslash geq0.5\$) and 0.733 (IOU \$\textbackslash ge 0.75\$) are obtained. Our data, evaluation tools, and code are publicly available.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/nvk/Zotero/storage/NV3S2CLP/Mali et al. - 2020 - ScanSSD Scanning Single Shot Detector for Mathema.pdf;/Users/nvk/Zotero/storage/J7EPBHVA/2003.html}
}

@article{malonMathematicalSymbolRecognition2008,
  title = {Mathematical Symbol Recognition with Support Vector Machines},
  author = {Malon, Christopher and Uchida, Seiichi and Suzuki, Masakazu},
  year = {2008},
  journal = {Pattern Recognition Letters},
  volume = {29},
  number = {9},
  pages = {1326--1332},
  publisher = {{Elsevier}},
  isbn = {0167-8655}
}

@book{manningIntroductionInformationRetrieval2008,
  title = {Introduction to {{Information Retrieval}}},
  author = {Manning, Christopher D.},
  year = {2008},
  month = jul,
  edition = {1st edition},
  publisher = {{Cambridge University Press}},
  address = {{New York}},
  isbn = {978-0-521-86571-5},
  langid = {english}
}

@inproceedings{mansouriCharacterizingSearchesMathematical2019,
  title = {Characterizing {{Searches}} for {{Mathematical Concepts}}},
  booktitle = {2019 {{ACM}}/{{IEEE Joint Conference}} on {{Digital Libraries}} ({{JCDL}})},
  author = {Mansouri, Behrooz and Zanibbi, Richard and Oard, Douglas W.},
  year = {2019},
  month = jun,
  pages = {57--66},
  doi = {10.1109/JCDL.2019.00019},
  abstract = {Although there has been considerable interest in recent years in the development of specialized mathematical digital libraries that can index and search for mathematical documents, little is yet known about how people will search for mathematical concepts. To begin to gain some insight into that question, this paper examines the nature of queries for mathematical concepts that were created by users of a search engine. A total of 392,586 queries that contained at least one distinctive mathematical term (e.g., "Taylor Series") were identified in a two-year query log from the Parsijoo search engine; these queries were each issued in one of 69,014 search sessions. Descriptive and comparative analysis indicates that math search sessions are typically longer and less successful than general search sessions. Queries for mathematical concepts exhibit greater diversity than do queries in general - essentially the query distribution is nearly all "tail." The use of well-formed questions as queries (e.g., "How can I prove that ...") is also surprisingly common, as is the creation of queries by copying text from a document. Among the implications of this study for the design of search engines with specialized functions for handling mathematical notation are that robust support for query refinement and reformulation could prove beneficial.},
  keywords = {Entropy,Libraries,Math search; User behavior; Query log analysis,Mathematics,Search engines,Task analysis,Web pages},
  file = {/Users/nvk/Zotero/storage/4DEXZ5CC/8791164.html}
}

@article{mansouriDPRLSystemsCLEF,
  title = {{{DPRL Systems}} in the {{CLEF}} 2020 {{ARQMath Lab}}},
  author = {Mansouri, Behrooz and Oard, Douglas W and Zanibbi, Richard},
  pages = {12},
  abstract = {This paper describes the participation of the Document and Pattern Recognition Lab from the Rochester Institute of Technology in the CLEF 2020 ARQMath lab. There are two tasks defined for ARQMath: (1) Question Answering, and (2) Formula Retrieval. Four runs were submitted for Task 1 using systems that take advantage of text and formula embeddings. For Task 2, three runs were submitted: one uses only formula embedding, another uses formula and text embeddings, and the final one uses formula embedding followed by re-ranking results by tree-edit distance. The Task 2 runs yielded strong results, the Task 1 results were less competitive.},
  langid = {english},
  file = {/Users/nvk/Zotero/storage/T7Q5MQIV/Mansouri et al. - DPRL Systems in the CLEF 2020 ARQMath Lab.pdf}
}

@inproceedings{mansouriFindingOldAnswers2020,
  title = {Finding {{Old Answers}} to {{New Math Questions}}: {{The ARQMath Lab}} at {{CLEF}} 2020},
  shorttitle = {Finding {{Old Answers}} to {{New Math Questions}}},
  booktitle = {Advances in {{Information Retrieval}}},
  author = {Mansouri, Behrooz and Agarwal, Anurag and Oard, Douglas and Zanibbi, Richard},
  editor = {Jose, Joemon M. and Yilmaz, Emine and Magalh{\~a}es, Jo{\~a}o and Castells, Pablo and Ferro, Nicola and Silva, M{\'a}rio J. and Martins, Fl{\'a}vio},
  year = {2020},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {564--571},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-030-45442-5_73},
  abstract = {The ARQMath Lab at CLEF 2020 considers the problem of finding answers to new mathematical questions among posted answers on a community question answering site (Math Stack Exchange). Queries are question postings held out from the test collection, each containing both text and at least one formula. We expect this to be a challenging task, as both math and text may be needed to find relevant answer posts. While several models have been proposed for text question answering, math question answering is in an earlier stage of development. To advance math-aware search and mathematical question answering systems, we will create a standard test collection for researchers to use for benchmarking. ARQMath will also include a formula retrieval sub-task: individual formulas from question posts are used to locate formulas in earlier answer posts, with relevance determined by narrative fields created based on the original question. We will use these narrative fields to explore diverse information needs for formula search (e.g., alternative notation, applications in specific fields or definition).},
  isbn = {978-3-030-45442-5},
  langid = {english},
  keywords = {Community question answering,Formula retrieval,Math-aware search,Mathematical Information Retrieval},
  file = {/Users/nvk/Zotero/storage/XSQ3RQLK/Mansouri et al. - 2020 - Finding Old Answers to New Math Questions The ARQ.pdf}
}

@inproceedings{mansouriTangentCFTEmbeddingModel2019,
  title = {Tangent-{{CFT}}: {{An Embedding Model}} for {{Mathematical Formulas}}},
  shorttitle = {Tangent-{{CFT}}},
  booktitle = {Proceedings of the 2019 {{ACM SIGIR International Conference}} on {{Theory}} of {{Information Retrieval}}},
  author = {Mansouri, Behrooz and Rohatgi, Shaurya and Oard, Douglas W. and Wu, Jian and Giles, C. Lee and Zanibbi, Richard},
  year = {2019},
  month = sep,
  series = {{{ICTIR}} '19},
  pages = {11--18},
  publisher = {{Association for Computing Machinery}},
  address = {{Santa Clara, CA, USA}},
  doi = {10.1145/3341981.3344235},
  abstract = {When searching for mathematical content, accurate measures of formula similarity can help with tasks such as document ranking, query recommendation, and result set clustering. While there have been many attempts at embedding words and graphs, formula embedding is in its early stages. We introduce a new formula embedding model that we use with two hierarchical representations, (1) Symbol Layout Trees (SLTs) for appearance, and (2) Operator Trees (OPTs) for mathematical content. Following the approach of graph embeddings such as DeepWalk, we generate tuples representing paths between pairs of symbols depth-first, embed tuples using the fastText n-gram embedding model, and then represent an SLT or OPT by its average tuple embedding vector. We then combine SLT and OPT embeddings, leading to state-of-the-art results for the NTCIR-12 formula retrieval task. Our fine-grained holistic vector representations allow us to retrieve many more partially similar formulas than methods using structural matching in trees. Combining our embedding model with structural matching in the Approach0 formula search engine produces state-of-the-art results for both fully and partially relevant results on the NTCIR-12 benchmark. Source code for our system is publicly available.},
  isbn = {978-1-4503-6881-0},
  keywords = {formula embeddings,math formula retrieval,tree embeddings},
  file = {/Users/nvk/Zotero/storage/HF3NC3MK/Mansouri et al. - 2019 - Tangent-CFT An Embedding Model for Mathematical F.pdf}
}

@inproceedings{markidisNVIDIATensorCore2018,
  title = {{{NVIDIA Tensor Core Programmability}}, {{Performance Precision}}},
  booktitle = {2018 {{IEEE International Parallel}} and {{Distributed Processing Symposium Workshops}} ({{IPDPSW}})},
  author = {Markidis, Stefano and Chien, Steven Wei Der and Laure, Erwin and Peng, Ivy Bo and Vetter, Jeffrey S.},
  year = {2018},
  month = may,
  pages = {522--531},
  doi = {10.1109/IPDPSW.2018.00091},
  abstract = {The NVIDIA Volta GPU microarchitecture introduces a specialized unit, called Tensor Core that performs one matrix-multiply-and-accumulate on 4x4 matrices per clock cycle. The NVIDIA Tesla V100 accelerator, featuring the Volta microarchitecture, provides 640 Tensor Cores with a theoretical peak performance of 125 Tflops/s in mixed precision. In this paper, we investigate current approaches to program NVIDIA Tensor Cores, their performances and the precision loss due to computation in mixed precision. Currently, NVIDIA provides three different ways of programming matrix-multiply-and-accumulate on Tensor Cores: the CUDA Warp Matrix Multiply Accumulate (WMMA) API, CUTLASS, a templated library based on WMMA, and cuBLAS GEMM. After experimenting with different approaches, we found that NVIDIA Tensor Cores can deliver up to 83 Tflops/s in mixed precision on a Tesla V100 GPU, seven and three times the performance in single and half precision respectively. A WMMA implementation of batched GEMM reaches a performance of 4 Tflops/s. While precision loss due to matrix multiplication with half precision input might be critical in many HPC applications, it can be considerably reduced at the cost of increased computation. Our results indicate that HPC applications using matrix multiplications can strongly benefit from using of NVIDIA Tensor Cores.},
  keywords = {application program interfaces,Computer architecture,cuBLAS GEMM,CUDA Warp Matrix Multiply Accumulate API,CUDA WMMA API,CUTLASS,GEMM,GPU Programming,graphics processing units,Graphics processing units,Hardware,HPC applications,Instruction sets,matrix multiplication,Mixed Precision,Neural networks,NVIDIA Tensor Core programmability,NVIDIA Tensor Cores,NVIDIA Tesla V100 accelerator,NVIDIA Volta GPU microarchitecture,parallel architectures,parallel programming,Programming,programming matrix-multiply- accumulate,Tensile stress,tensors,Tesla V100 GPU,Volta microarchitecture},
  file = {/Users/nvk/Zotero/storage/SDFLP5HF/Markidis et al. - 2018 - NVIDIA Tensor Core Programmability, Performance Pr.pdf;/Users/nvk/Zotero/storage/6MRMTVQN/8425458.html}
}

@article{markovFundamentalLimitScaling2019,
  title = {Fundamental {{Limit}} to {{Scaling Si Field-Effect Transistors Due}} to {{Source-to-Drain Direct Tunneling}}},
  author = {Markov, Stanislav and Kwok, Yanho and Li, Jun and Zhou, Weijun and Zhou, Yi and Chen, Guanhua},
  year = {2019},
  month = mar,
  journal = {IEEE Transactions on Electron Devices},
  volume = {66},
  number = {3},
  pages = {1167--1173},
  issn = {1557-9646},
  doi = {10.1109/TED.2019.2894967},
  abstract = {How far can the miniaturization of metal-oxide-semiconductor field-effect transistors (MOSFETs) continue is a recurring question, essential to all aspects of digital technology. Recent claims of well-performing MOSFETs with gate lengths below 4 nm apparently defy the fundamental limit of source-to-drain direct tunneling (SDDT). Here, we investigate that limit by simulating gate-all-around Si nanowire FETs with gate lengths between 8 and 3 nm using the state-of-the-art atomistic quantum transport modeling. We find that at 3-nm gate length, the current is dominated by SDDT, resulting in large source-drain leakage and poor switching performance even if the gate modulates the potential barrier between the source and drain sufficiently well. However, at 6-nm gate-length SDDT barely starts to degrade the subthreshold characteristics at large drain bias, and the ballistic ON-/OFF-current ratio is 106 with a subthreshold swing of 70 mV/decade, on par with contemporary Si technology. This means that in the best case, the technology roadmap could potentially be extended for several generations beyond the currently projected nodes. In addition, the results substantiate that the experimental devices with the claimed gate lengths below 6 nm in fact operate with a longer effective gate lengths.},
  keywords = {atomistic quantum transport modeling,Atomistic simulation,ballistic ON-OFF-current ratio,contemporary silicon technology,density functional tight binding,digital technology,drain bias,effective gate lengths,Electric potential,elemental semiconductors,gate-all-around silicon nanowire FETs,gate-length SDDT,Logic gates,Markov processes,metal-oxide-semiconductor field-effect transistors,metalâ€“oxideâ€“semiconductor field-effect transistor (MOSFET),MOSFET,MOSFETs,nanoelectronics,nanowires,quantum transport,Semiconductor device modeling,semiconductor device models,Si,silicon,Silicon,silicon field-effect transistors,size 8 nm to 3 nm,source-drain leakage,source-to-drain direct tunneling,subthreshold characteristics,switching performance,tunnelling},
  file = {/Users/nvk/Zotero/storage/9FFHB6J7/Markov et al. - 2019 - Fundamental Limit to Scaling Si Field-Effect Trans.pdf;/Users/nvk/Zotero/storage/MVY4TWCG/8634937.html}
}

@inproceedings{martinComputerInputOutput1971,
  title = {Computer Input/Output of Mathematical Expressions},
  booktitle = {Proceedings of the Second {{ACM}} Symposium on {{Symbolic}} and Algebraic Manipulation},
  author = {Martin, William A.},
  year = {1971},
  month = mar,
  series = {{{SYMSAC}} '71},
  pages = {78--89},
  publisher = {{Association for Computing Machinery}},
  address = {{Los Angeles, California, USA}},
  doi = {10.1145/800204.806270},
  abstract = {Studying mathematics is, in part, a language problem. Naturally, mathematicians are more likely to resist using a computer as a tool in their work if the tedious task of learning a new language for mathematics is part of the bargain. Furthermore, until a new language is thoroughly learned, difficulty in communication will make it less likely that their new experience will be a successful one. Beyond that, if a new computer language does not provide the same visual clues as standard mathematical notation, it may never adequately serve the mathematician. It therefore seems fair to assume that the computer input/output of mathematical expressions in a form resembling standard notation is an important goal. This form of expression is considerably more complex and expensive to handle than those used in, for example, the programming language FORTRAN. But the alternative (e.g., correctly manipulating a one page FORTRAN expression) is often quite painful.},
  isbn = {978-1-4503-7786-7},
  file = {/Users/nvk/Zotero/storage/I6UFGENG/Martin - 1971 - Computer inputoutput of mathematical expressions.pdf}
}

@article{MathematicalMarkupLanguage,
  title = {Mathematical {{Markup Language}} ({{MathML}}) 1.01 {{Specification}}},
  pages = {217},
  abstract = {This specification defines the Mathematical Markup Language, or MathML. MathML is an XML application for describing mathematical notation and capturing both its structure and content. The goal of MathML is to enable mathematics to be served, received, and processed on the Web, just as HTML has enabled this functionality for text.},
  langid = {english},
  file = {/Users/nvk/Zotero/storage/PKAJATJB/Mathematical Markup Language (MathML) 1.01 Specifi.pdf}
}

@misc{MathematicaSystemDoing,
  title = {Mathematica: A System for Doing Mathematics by Computer (2nd Ed.) | {{Guide}} Books},
  shorttitle = {Mathematica},
  howpublished = {https://dl.acm.org/doi/abs/10.5555/129711},
  langid = {english},
  file = {/Users/nvk/Zotero/storage/8LY4SL3P/129711.html}
}

@book{mayergoyzNonlinearMagnetizationDynamics2009,
  title = {Nonlinear {{Magnetization Dynamics}} in {{Nanosystems}}},
  author = {Mayergoyz, Isaak D. and Bertotti, Giorgio and Serpico, Claudio},
  year = {2009},
  month = apr,
  publisher = {{Elsevier}},
  abstract = {As data transfer rates increase within the magnetic recording industry, improvements in device performance and reliability crucially depend on the thorough understanding of nonlinear magnetization dynamics at a sub-nanoscale level. This book offers a modern, stimulating approach to the subject of nonlinear magnetization dynamics by discussing important aspects such as the Landau-Lifshitz-Gilbert (LLG) equation, analytical solutions, and the connection between the general topological and structural aspects of dynamics. An advanced reference for the study and understanding of nonlinear magnetization dynamics, it addresses situations such as the understanding of spin dynamics in short time scales and device performance and reliability in magnetic recording. Topics covered include nonlinear magnetization dynamics and the Landau-Lifshitz-Gilbert equation, nonlinear dynamical systems, spin waves, ferromagnetic resonance and pulsed magnetization switching. The book explains how to derive exact analytical solutions for the complete nonlinear problem and emphasises the connection between the general topological and structural aspects of nonlinear magnetization dynamics and the discretization schemes better suited to its numerical study. It is an exceptional research tool providing an advanced understanding of the study of magnetization dynamics in situations of fundamental and technological interest.},
  googlebooks = {QH4ShV3mKmkC},
  isbn = {978-0-08-091379-7},
  langid = {english},
  keywords = {Science / Physics / Electricity,Technology \& Engineering / Electronics / General,Technology \& Engineering / Nanotechnology \& MEMS}
}

@inproceedings{mccarthyJohnsHopkinsUniversity2020,
  title = {The {{Johns Hopkins University Bible Corpus}}: 1600+ {{Tongues}} for {{Typological Exploration}}},
  shorttitle = {The {{Johns Hopkins University Bible Corpus}}},
  booktitle = {Proceedings of the 12th {{Language Resources}} and {{Evaluation Conference}}},
  author = {McCarthy, Arya D. and Wicks, Rachel and Lewis, Dylan and Mueller, Aaron and Wu, Winston and Adams, Oliver and Nicolai, Garrett and Post, Matt and Yarowsky, David},
  year = {2020},
  month = may,
  pages = {2884--2892},
  publisher = {{European Language Resources Association}},
  address = {{Marseille, France}},
  abstract = {We present findings from the creation of a massively parallel corpus in over 1600 languages, the Johns Hopkins University Bible Corpus (JHUBC). The corpus consists of over 4000 unique translations of the Christian Bible and counting. Our data is derived from scraping several online resources and merging them with existing corpora, combining them under a common scheme that is verse-parallel across all translations. We detail our effort to scrape, clean, align, and utilize this ripe multilingual dataset. The corpus captures the great typological variety of the world's languages. We catalog this by showing highly similar proportions of representation of Ethnologue's typological features in our corpus. We also give an example application: projecting pronoun features like clusivity across alignments to richly annotate languages which do not mark the distinction.},
  isbn = {979-10-95546-34-4},
  langid = {english},
  file = {/Users/nvk/Zotero/storage/MQ2ZMUKM/McCarthy et al. - 2020 - The Johns Hopkins University Bible Corpus 1600+ T.pdf}
}

@misc{meurerSymPySymbolicComputing2017,
  title = {{{SymPy}}: Symbolic Computing in {{Python}}},
  author = {Meurer, Aaron and Smith, Christopher P. and Paaprocki, Masteusz and Certik, Ondrej and Kirpichev and Rocklin, Matthew and Kumar, AMit and Ivanov, Sergiu and Moore, Jason and Singh, Sartaj and Rathnayake, Thilina and Vig, Sean and Granger, Brian E. and Muller, Richard P. and Bonazzi, Francesco and Gupta, Harsh and Vats, Shivam and Johansson, Fredrik and Pedregosa, Fabian and Curry, Matthew J. and Terrel, Andy R. and Roucka, Stepan and Saboo, Ashutosh and Fernando, Isuru and Kulal, Sumith and Cimrman, Robert and Scopatz, Anthony},
  year = {2017},
  month = jan,
  publisher = {{PeerJ Computer Science =}},
  abstract = {SymPy is an open source computer algebra system written in pure Python. It is built with a focus on extensibility and ease of use, through both interactive and programmatic applications. These characteristics have led SymPy to become a popular symbolic library for the scientific Python ecosystem. This paper presents the architecture of SymPy, a description of its features, and a discussion of select submodules. The supplementary material provide additional examples and further outline details of the architecture and features of SymPy.},
  langid = {english}
}

@inproceedings{meuschkeAnalyzingMathematicalContent2017,
  title = {Analyzing {{Mathematical Content}} to {{Detect Academic Plagiarism}}},
  booktitle = {Proceedings of the 2017 {{ACM}} on {{Conference}} on {{Information}} and {{Knowledge Management}}},
  author = {Meuschke, Norman and Schubotz, Moritz and Hamborg, Felix and Skopal, Tomas and Gipp, Bela},
  year = {2017},
  month = nov,
  series = {{{CIKM}} '17},
  pages = {2211--2214},
  publisher = {{Association for Computing Machinery}},
  address = {{Singapore, Singapore}},
  doi = {10.1145/3132847.3133144},
  abstract = {This paper presents, to our knowledge, the first study on analyzing mathematical expressions to detect academic plagiarism. We make the following contributions. First, we investigate confirmed cases of plagiarism to categorize the similarities of mathematical content commonly found in plagiarized publications. From this investigation, we derive possible feature selection and feature comparison strategies for developing math-based detection approaches and a ground truth for our experiments. Second, we create a test collection by embedding confirmed cases of plagiarism into the NTCIR-11 MathIR Task dataset, which contains approx. 60 million mathematical expressions in 105,120 documents from arXiv.org. Third, we develop a first math-based detection approach by implementing and evaluating different feature comparison approaches using an open source parallel data processing pipeline built using the Apache Flink framework. The best performing approach identifies all but two of our real-world test cases at the top rank and achieves a mean reciprocal rank of 0.86. The results show that mathematical expressions are promising text-independent features to identify academic plagiarism in large collections. To facilitate future research on math-based plagiarism detection, we make our source code and data available.},
  isbn = {978-1-4503-4918-5},
  keywords = {mathematical information retrieval,plagiarism detection},
  file = {/Users/nvk/Zotero/storage/EGKYR7AH/Meuschke et al. - 2017 - Analyzing Mathematical Content to Detect Academic .pdf}
}

@inproceedings{meyersLinearRepresentationTree1971,
  title = {Linear Representation of Tree Structure - a Mathematical Theory of Parenthesis-Free Notations},
  booktitle = {Proceedings of the Third Annual {{ACM}} Symposium on {{Theory}} of Computing},
  author = {Meyers, W. J.},
  year = {1971},
  month = may,
  series = {{{STOC}} '71},
  pages = {50--62},
  publisher = {{Association for Computing Machinery}},
  address = {{New York, NY, USA}},
  doi = {10.1145/800157.805038},
  abstract = {In this paper we present a substantially general theory of parenthesis-free notations for finite plane trees. We obtain stronger one-to-oneness results, including a characterization of one-to-oneness for a large class of notations, and a quite general sufficient condition for one-to-oneness that involves the recursive structure of plane trees in what appears to be a minimal way. We then study various properties of notations that promise to be of practical interest. The two most important of these\textemdash single scan bottom-up readability, and single scan top-down readability\textemdash both turn out to be particular cases of our general sufficient condition for one-to-oneness. We formulate simple descriptions of all notations that have one or the other of these properties, and find a simple transformation of notations that establishes a one-to-one correspondence between them. The unique fixed point of this transformation turns out to be the only notation single scan readable in both directions.},
  isbn = {978-1-4503-7464-4},
  file = {/Users/nvk/Zotero/storage/IFICW7JR/Meyers - 1971 - Linear representation of tree structure - a mathem.pdf}
}

@misc{millerLaTeXMLLatexXml2010,
  title = {{{LaTeXML}}: {{A Latex}} to Xml Converter},
  author = {Miller, Bruce},
  year = {2010},
  howpublished = {NIST}
}

@article{millerSPHINXFrameworkCreating1998,
  title = {{{SPHINX}}: A Framework for Creating Personal, Site-Specific {{Web}} Crawlers},
  author = {Miller, Robert C. and Bharat, Krishna},
  year = {1998},
  journal = {Computer Networks and ISDN systems},
  volume = {30},
  number = {1-7},
  pages = {119--130},
  publisher = {{Elsevier}},
  isbn = {0169-7552}
}

@article{millerTechnicalAspectsDigital2003,
  title = {Technical {{Aspects}} of the {{Digital Library}} of {{Mathematical Functions}}},
  author = {Miller, Bruce R. and Youssef, Abdou},
  year = {2003},
  month = may,
  journal = {Annals of Mathematics and Artificial Intelligence},
  volume = {38},
  number = {1},
  pages = {121--136},
  issn = {1573-7470},
  doi = {10.1023/A:1022967814992},
  abstract = {The NIST Digital Library of Mathematical Functions (DLMF) Project, begun in 1997, is preparing a handbook and Web site intended for wide communities of users. The contents are primarily mathematical formulas, graphs, methods of computation, references, and links to software. The task of developing a Web handbook of this nature presents several technical challenges. We describe the goals of the Digital Library of Mathematical Functions Project and the realities that constrain those goals. We propose practical initial solutions, in order to ease the authoring of adaptable content: a LaTeX class which encourages a modestly semantic markup style; and a mathematical search engine that adapts a text search engine to the task.},
  langid = {english},
  file = {/Users/nvk/Zotero/storage/P37KGMYJ/Miller and Youssef - 2003 - Technical Aspects of the Digital Library of Mathem.pdf}
}

@article{minerImportanceMathMLMathematics2005,
  title = {The {{Importance}} of {{MathML}} to {{Mathematics Communication}}},
  author = {Miner, Robert},
  year = {2005},
  volume = {52},
  number = {5},
  pages = {7},
  langid = {english},
  file = {/Users/nvk/Zotero/storage/R7MNL7AJ/Miner - 2005 - The Importance of MathML to Mathematics Communicat.pdf}
}

@inproceedings{mingchengTIMETraininginmemoryArchitecture2017,
  title = {{{TIME}}: {{A}} Training-in-Memory Architecture for Memristor-Based Deep Neural Networks},
  shorttitle = {{{TIME}}},
  booktitle = {2017 54th {{ACM}}/{{EDAC}}/{{IEEE Design Automation Conference}} ({{DAC}})},
  author = {Ming Cheng and Lixue Xia and Zhenhua Zhu and Yi Cai and Yuan Xie and Yu Wang and Huazhong Yang},
  year = {2017},
  month = jun,
  pages = {1--6},
  doi = {10.1145/3061639.3062326},
  abstract = {The training of neural network (NN) is usually time-consuming and resource intensive. Memristor has shown its potential in computation of NN. Especially for the metal-oxide resistive random access memory (RRAM), its crossbar structure and multi-bit characteristic can perform the matrix-vector product in high precision, which is the most common operation of NN. However, there exist two challenges on realizing the training of NN. Firstly, the current architecture can only support the inference phase of training and cannot perform the backpropagation (BP), the weights update of NN. Secondly, the training of NN requires enormous iterations and constantly updates the weights to reach the convergence, which leads to large energy consumption because of lots of write and read operations. In this work, we propose a novel architecture, TIME, and peripheral circuit designs to enable the training of NN in RRAM. TIME supports the BP and the weights update while maximizing the reuse of peripheral circuits for the inference operation on RRAM. Meanwhile, a variability-free tuning scheme and gradually-write circuits are designed to reduce the cost of tuning RRAM. We explore the performance of both SL (supervised learning) and DRL (deep reinforcement learning) in TIME, and a specific mapping method of DRL is also introduced to further improve the energy efficiency. Experimental results show that, in SL, TIME can achieve 5.3\texttimes{} higher energy efficiency on average compared with the most powerful application-specific integrated circuits (ASIC) in the literature. In DRL, TIME can perform averagely 126\texttimes{} higher than GPU in energy efficiency. If the cost of tuning RRAM can be further reduced, TIME have the potential of boosting the energy efficiency by 2 orders of magnitude compared with ASIC.},
  keywords = {application specific integrated circuits,Artificial neural networks,backpropagation,Computer architecture,deep neural networks,energy efficiency,learning (artificial intelligence),logic design,memory architecture,memristor,memristors,metal-oxide resistive random access memory,neural nets,Random access memory,Resistance,resistive RAM,RRAM,Training,training-in-memory architecture,Tuning},
  file = {/Users/nvk/Zotero/storage/WQIBPNVI/Ming Cheng et al. - 2017 - TIME A training-in-memory architecture for memris.pdf;/Users/nvk/Zotero/storage/GWVBHEJC/8060399.html}
}

@article{mischoInnovationsDiscoverySystems2018,
  title = {Innovations in {{Discovery Systems}}: {{Charleston Library Conference}}},
  shorttitle = {Innovations in {{Discovery Systems}}},
  author = {Mischo, William and Norman, Michael and Schlembach, Mary},
  editor = {Bernhardt, Beth R. and Hinds, Leah and Meyer, Lars},
  year = {2018},
  journal = {What's Past is Prologue},
  pages = {299--304},
  publisher = {{Purdue University Press}},
  doi = {10.5703/1288284316700},
  abstract = {Over the past 30 years, library discovery services have evolved through expanded OPACs, federated search systems employing broadcast searching; Web-scale discovery systems (WSDS) that aggregate metadata and full-text content into a single integrated index; and, currently, hybrid bento-style systems that use federated techniques over WSDS, OPACs, and local information content. The bento systems partition search results into separate zoned screen displays grouped by content format type and/or local service results. Recent studies on Web-scale discovery systems have identified a number of user access issues centering on problems with blended result displays, problematical relevancy rankings of search results, full-text search problems, and the inability of WSDS to adequately provide access to local library services and resources. The concept of ``full library discovery,'' a phrase first coined by Lorcan Dempsey, has been introduced to refer to discovery approaches that move beyond the retrieval of collection materials to also include local information services and local content and links. The bento-based systems are an attempt to address the identified problems with WSDS and also provide discovery services that address user needs, in particular known item search and streamlined full-text access. This presentation will provide an analysis of the 38 libraries presently employing the bento approach and will look at identified user needs and search behaviors, as revealed in detailed search and clickthrough transaction log analyses. There is a clear need for an evidence-based analysis of user search behaviors in retrieval environments characterized by access to distributed information resources.},
  file = {/Users/nvk/Zotero/storage/TG9SM47K/Mischo et al. - 2018 - Innovations in Discovery Systems Charleston Libra.pdf}
}

@article{mittalSurveyReRAMBasedArchitectures2019,
  title = {A {{Survey}} of {{ReRAM-Based Architectures}} for {{Processing-In-Memory}} and {{Neural Networks}}},
  author = {Mittal, Sparsh},
  year = {2019},
  month = mar,
  journal = {Machine Learning and Knowledge Extraction},
  volume = {1},
  number = {1},
  pages = {75--114},
  publisher = {{Multidisciplinary Digital Publishing Institute}},
  doi = {10.3390/make1010005},
  abstract = {As data movement operations and power-budget become key bottlenecks in the design of computing systems, the interest in unconventional approaches such as processing-in-memory (PIM), machine learning (ML), and especially neural network (NN)-based accelerators has grown significantly. Resistive random access memory (ReRAM) is a promising technology for efficiently architecting PIM- and NN-based accelerators due to its capabilities to work as both: High-density/low-energy storage and in-memory computation/search engine. In this paper, we present a survey of techniques for designing ReRAM-based PIM and NN architectures. By classifying the techniques based on key parameters, we underscore their similarities and differences. This paper will be valuable for computer architects, chip designers and researchers in the area of machine learning.},
  copyright = {http://creativecommons.org/licenses/by/3.0/},
  langid = {english},
  keywords = {artificial intelligence,deep learning,emerging memory technology,hardware architecture,machine learning,memristor,non-volatile memory,processing-in-memory,resistive memory,review},
  file = {/Users/nvk/Zotero/storage/UJIB4T3H/Mittal - 2019 - A Survey of ReRAM-Based Architectures for Processi.pdf;/Users/nvk/Zotero/storage/HICEVBXK/5.html}
}

@inproceedings{mojzisovaAutomaticTestGenerator2016,
  title = {Automatic Test Generator},
  booktitle = {2016 17th {{International Carpathian Control Conference}} ({{ICCC}})},
  author = {Moj{\v z}i{\v s}ova, Andrea and P{\'o}csov{\'a}, Jana and Skovranek, Tomas},
  year = {2016},
  month = may,
  pages = {511--516},
  doi = {10.1109/CarpathianCC.2016.7501151},
  abstract = {Modern education must implement new educational tools. One of the most popular of these tools are worksheet generators. Worksheet generators are used to create students' worksheets with practical assignments. Typically, worksheets are generated from a database of assignments. In this paper, we propose an automatic test generator based on worksheet generator. Our solution creates examples automatically and does not need a predefined set of examples. We focus on the analysis and design of examples and on the selection of an appropriate method for creation of all types of examples and generation of tests.},
  keywords = {analysis,design,Education,Educational tools,example,Generators,Linear algebra,Mathematical model,MATLAB,Tungsten,worksheet generator},
  file = {/Users/nvk/Zotero/storage/ADUNRSQA/7501151.html}
}

@article{morinHierarchicalProbabilisticNeural2005,
  title = {Hierarchical {{Probabilistic Neural Network Language Model}}},
  author = {Morin, Frederic and Bengio, Yoshua},
  year = {2005},
  journal = {International workshop on artificial intelligence and statistics},
  pages = {246--252},
  abstract = {In recent years, variants of a neural network architecture for statistical language modeling have been proposed and successfully applied, e.g. in the language modeling component of speech recognizers. The main advantage of these architectures is that they learn an embedding for words (or other symbols) in a continuous space that helps to smooth the language model and provide good generalization even when the number of training examples is insufficient. However, these models are extremely slow in comparison to the more commonly used n-gram models, both for training and recognition. As an alternative to an importance sampling method proposed to speed-up training, we introduce a hierarchical decomposition of the conditional probabilities that yields a speed-up of about 200 both during training and recognition. The hierarchical decomposition is a binary hierarchical clustering constrained by the prior knowledge extracted from the WordNet semantic hierarchy.},
  langid = {english},
  file = {/Users/nvk/Zotero/storage/C2FYEVUY/Morin and Bengio - Hierarchical Probabilistic Neural Network Language.pdf}
}

@article{munotComparativeStudyText2014,
  title = {Comparative {{Study}} of {{Text Summarization Methods}}},
  author = {Munot, Nikita and Govilkar, Sharvari},
  year = {2014},
  month = sep,
  journal = {International Journal of Computer Applications},
  volume = {102},
  pages = {33--37},
  doi = {10.5120/17870-8810},
  file = {/Users/nvk/Zotero/storage/6GI22YQD/Munot and Govilkar - 2014 - Comparative Study of Text Summarization Methods.pdf}
}

@incollection{musielaStochasticPartialDifferential2010,
  title = {Stochastic {{Partial Differential Equations}} and {{Portfolio Choice}}},
  booktitle = {Contemporary {{Quantitative Finance}}: {{Essays}} in {{Honour}} of {{Eckhard Platen}}},
  author = {Musiela, Marek and Zariphopoulou, Thaleia},
  editor = {Chiarella, Carl and Novikov, Alexander},
  year = {2010},
  pages = {195--216},
  publisher = {{Springer}},
  address = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-642-03479-4_11},
  abstract = {We introduce a stochastic partial differential equation which describes the evolution of the investment performance process in portfolio choice models. The equation is derived for two formulations of the investment problem, namely, the traditional one (based on maximal expected utility of terminal wealth) and the recently developed forward formulation. The novel element in the forward case is the volatility process which is up to the investor to choose. We provide various examples for both cases and discuss the differences and similarities between the different forms of the equation as well as the associated solutions and optimal processes.},
  isbn = {978-3-642-03479-4},
  langid = {english},
  keywords = {Incomplete Market,Optimal Portfolio,Portfolio Choice,Stochastic Partial Differential Equation,Stochastic Volatility},
  file = {/Users/nvk/Zotero/storage/CLCNXFQV/Musiela and Zariphopoulou - 2010 - Stochastic Partial Differential Equations and Port.pdf}
}

@article{nafBuildingFutureInvesting,
  title = {Building the {{Future}} - {{Investing}} in Discovery and Innovation},
  author = {NAF},
  file = {/Users/nvk/Zotero/storage/IVZS835A/nsf18045.pdf}
}

@misc{NationalArtificialIntelligence,
  title = {National {{Artificial Intelligence Research Institutes}}},
  journal = {Beta site for NSF - National Science Foundation},
  howpublished = {https://beta.nsf.gov/funding/opportunities/national-artificial-intelligence-research-institutes},
  langid = {english}
}

@misc{NeedMusicInformation,
  title = {The Need for Music Information Retrieval with User-Centered and Multimodal Strategies | {{Proceedings}} of the 1st International {{ACM}} Workshop on {{Music}} Information Retrieval with User-Centered and Multimodal Strategies},
  howpublished = {https://dl.acm.org/doi/abs/10.1145/2072529.2072531?casa\_token=rPiYQDPuPYAAAAAA:jSVs5wPLLz0zb\_tyGJ2ISAkJdo\_AIPNlcUuRzdeZsMIn0IONGkEYcSKDMQ3gl53e6f2HYlDUh1Fs}
}

@article{ngDowsingMathAnswers,
  title = {Dowsing for {{Math Answers}} with {{Tangent-L}}},
  author = {Ng, Yin Ki and Fraser, Dallas J and Kassaie, Besat and Labahn, George and Marzouk, Mirette S and Tompa, Frank Wm and Wang, Kevin},
  pages = {40},
  abstract = {We present our application of the math-aware search engine Tangent-L to the ARQMath Community Question Answering (CQA) task. Our approach performs well, placing in the top three positions out of all 23 submissions, including the baseline runs.},
  langid = {english},
  file = {/Users/nvk/Zotero/storage/YISMP6GU/Ng et al. - Dowsing for Math Answers with Tangent-L.pdf}
}

@article{nikonovOverviewBeyondCMOSDevices2013,
  title = {Overview of {{Beyond-CMOS Devices}} and a {{Uniform Methodology}} for {{Their Benchmarking}}},
  author = {Nikonov, Dmitri E. and Young, Ian A.},
  year = {2013},
  month = dec,
  journal = {Proceedings of the IEEE},
  volume = {101},
  number = {12},
  pages = {2498--2533},
  issn = {1558-2256},
  doi = {10.1109/JPROC.2013.2252317},
  abstract = {Multiple logic devices are presently under study within the Nanoelectronic Research Initiative (NRI) to carry the development of integrated circuits beyond the complementary metal-oxide-semiconductor (CMOS) roadmap. Structure and operational principles of these devices are described. Theories used for benchmarking these devices are overviewed, and a general methodology is described for consistent estimates of the circuit area, switching time, and energy. The results of the comparison of the NRI logic devices using these benchmarks are presented.},
  keywords = {Adder,Adders,benchmark testing,Benchmark testing,benchmarking,beyond CMOS devices,beyond complementary metalâ€“oxideâ€“semiconductor (beyond-CMOS),CMOS logic circuits,CMOS technology,complementary metal oxide semiconductor,computational throughput,electronics,integrated circuit testing,integrated circuits,Integrated circuits,logic,logic devices,Nanoelectronic Research Initiative,nanoelectronics,Nanoelectronics,NRI logic devices,power dissipation,Power dissipation,Semiconductor devices,spintronics,Throughput},
  file = {/Users/nvk/Zotero/storage/3SSU8JZI/Nikonov and Young - 2013 - Overview of Beyond-CMOS Devices and a Uniform Meth.pdf;/Users/nvk/Zotero/storage/PIBFIBQG/6527325.html}
}

@inproceedings{nishizawaMathSeerMathAwareSearch2020,
  title = {{{MathSeer}}: {{A Math-Aware Search Interface}} with {{Intuitive Formula Editing}}, {{Reuse}}, and {{Lookup}}},
  booktitle = {European {{Conference}} on {{Information Retrieval}}},
  author = {Nishizawa, Gavin and Liu, Jennifer and Diaz, Yancarlos and Dmello, Abishai and Zhong, Wei and Zanibbi, Richard},
  year = {2020},
  pages = {470--475},
  publisher = {{Springer}}
}

@misc{NSFInnovationCorps,
  title = {{{NSF Innovation Corps Hubs Program}} ({{I-Corps}}Â™ {{Hubs}})},
  journal = {Beta site for NSF - National Science Foundation},
  howpublished = {https://beta.nsf.gov/funding/opportunities/nsf-innovation-corps-hubs-program-i-corps-hubs},
  langid = {english}
}

@incollection{oardFutureInformationRetrieval2021,
  title = {The {{Future}} of {{Information Retrieval Evaluation}}},
  booktitle = {Evaluating {{Information Retrieval}} and {{Access Tasks}}: {{NTCIR}}'s {{Legacy}} of {{Research Impact}}},
  author = {Oard, Douglas W.},
  editor = {Sakai, Tetsuya and Oard, Douglas W. and Kando, Noriko},
  year = {2021},
  series = {The {{Information Retrieval Series}}},
  pages = {205--216},
  publisher = {{Springer}},
  address = {{Singapore}},
  doi = {10.1007/978-981-15-5554-1_14},
  abstract = {Looking back over the storied history of NTCIR that is recounted in this volume, we can see many impactful contributions. As we look at the future, we might then ask what points of continuity and change we might reasonably anticipate. Beginning that discussion is the focus of this chapter.},
  isbn = {9789811555541},
  langid = {english},
  file = {/Users/nvk/Zotero/storage/DEY7WP8Z/Oard - 2021 - The Future of Information Retrieval Evaluation.pdf}
}

@misc{OfficeFoundationRelations,
  title = {Office of {{Foundation Relations}}},
  abstract = {Phone: 217-265-5322 E-mail:~foundationrelations@illinois.edu WELCOME TO THE OFFICE OF FOUNDATION RELATIONS! The Office of Foundation Relations at the University of Illinois Urbana-Champaign is a campus-wide resource for faculty and administrators seeking to advance relationships with national and selected regional private foundations. As each foundation is different, we are here to guide you through their processes and [\ldots ]},
  howpublished = {https://with.illinois.edu/foundationrelations/},
  langid = {american},
  file = {/Users/nvk/Zotero/storage/CSJ6J9FB/foundationrelations.html}
}

@article{ohashiEfficientAlgorithmMath2016,
  title = {Efficient {{Algorithm}} for {{Math Formula Semantic Search}}},
  author = {Ohashi, Shunsuke and Kristianto, Giovanni Yoko and Topic, Goran and Aizawa, Akiko},
  year = {2016},
  journal = {IEICE Transactions on Information and Systems},
  volume = {E99.D},
  number = {4},
  pages = {979--988},
  issn = {0916-8532, 1745-1361},
  doi = {10.1587/transinf.2015DAP0023},
  abstract = {Mathematical formulae play an important role in many scientific domains. Regardless of the importance of mathematical formula search, conventional keyword-based retrieval methods are not sufficient for searching mathematical formulae, which are structured as trees. The increasing number as well as the structural complexity of mathematical formulae in scientific articles lead to the necessity for large-scale structureaware formula search techniques. In this paper, we formulate three types of measures that represent distinctive features of semantic similarity of math formulae, and develop efficient hash-based algorithms for the approximate calculation. Our experiments using NTCIR-11 Math-2 Task dataset, a large-scale test collection for math information retrieval with about 60million formulae, show that the proposed method improves the search precision while also keeps the scalability and runtime efficiency high.},
  langid = {english},
  file = {/Users/nvk/Zotero/storage/KX7BMIGL/Ohashi et al. - 2016 - Efficient Algorithm for Math Formula Semantic Sear.pdf}
}

@misc{OpenResearchFunders,
  title = {Open {{Research Funders Group}}},
  journal = {Open Research Funders Group},
  howpublished = {https://www.orfg.org/},
  langid = {american},
  file = {/Users/nvk/Zotero/storage/U36YVYTQ/members.html}
}

@article{oxleyDeepLearningInterface2020,
  title = {Deep Learning of Interface Structures from Simulated {{4D STEM}} Data: Cation Intermixing vs. Roughening},
  shorttitle = {Deep Learning of Interface Structures from Simulated {{4D STEM}} Data},
  author = {Oxley, M. P. and Yin, J. and Borodinov, N. and Somnath, S. and Ziatdinov, M. and Lupini, A. R. and Jesse, S. and Vasudevan, R. K. and Kalinin, S. V.},
  year = {2020},
  month = nov,
  journal = {Machine Learning: Science and Technology},
  volume = {1},
  number = {4},
  pages = {04LT01},
  publisher = {{IOP Publishing}},
  issn = {2632-2153},
  doi = {10.1088/2632-2153/aba32d},
  abstract = {Interface structures in complex oxides remain an active area of condensed matter physics research, largely enabled by recent advances in scanning transmission electron microscopy (STEM). Yet the nature of the STEM contrast in which the structure is projected along the given direction precludes separation of possible structural models. Here, we utilize deep convolutional neural networks (DCNN) trained on simulated 4D STEM datasets to predict structural descriptors of interfaces. We focus on the widely studied interface between LaAlO3 and SrTiO3, using dynamical diffraction theory and leveraging high performance computing to simulate thousands of possible 4D STEM datasets to train the DCNN to learn properties of the underlying structures on which the simulations are based. We test the DCNN on simulated data and show that it is possible (with {$>$}95\% accuracy) to identify a physically rough from a chemically diffuse interface and create a DCNN regression model to predict step positions. We quantify the applicability of the model to different thicknesses and the transferability of the approach. The method shown here is general and can be applied for any inverse imaging problem where forward models are present.},
  langid = {english},
  file = {/Users/nvk/Zotero/storage/BRMBBW8G/Oxley et al. - 2020 - Deep learning of interface structures from simulat.pdf}
}

@article{pagaelMathematicalLanguageProcessing2014,
  title = {Mathematical {{Language Processing Project}}},
  author = {Pagael, Robert and Schubotz, Moritz},
  year = {2014},
  month = jul,
  journal = {arXiv:1407.0167 [cs]},
  eprint = {1407.0167},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {In natural language, words and phrases themselves imply the semantics. In contrast, the meaning of identifiers in mathematical formulae is undefined. Thus scientists must study the context to decode the meaning. The Mathematical Language Processing (MLP) project aims to support that process. In this paper, we compare two approaches to discover identifier-definition tuples. At first we use a simple pattern matching approach. Second, we present the MLP approach that uses part-of-speech tag based distances as well as sentence positions to calculate identifier-definition probabilities. The evaluation of our prototypical system, applied on the Wikipedia text corpus, shows that our approach augments the user experience substantially. While hovering the identifiers in the formula, tool-tips with the most probable definitions occur. Tests with random samples show that the displayed definitions provide a good match with the actual meaning of the identifiers.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Digital Libraries,Computer Science - Information Retrieval},
  file = {/Users/nvk/Zotero/storage/ZDKDHBJ9/Pagael and Schubotz - 2014 - Mathematical Language Processing Project.pdf;/Users/nvk/Zotero/storage/AZQKH3LC/1407.html}
}

@article{palangiDeepSentenceEmbedding2016,
  title = {Deep {{Sentence Embedding Using Long Short-Term Memory Networks}}: {{Analysis}} and {{Application}} to {{Information Retrieval}}},
  shorttitle = {Deep {{Sentence Embedding Using Long Short-Term Memory Networks}}},
  author = {Palangi, Hamid and Deng, Li and Shen, Yelong and Gao, Jianfeng and He, Xiaodong and Chen, Jianshu and Song, Xinying and Ward, Rabab},
  year = {2016},
  month = apr,
  journal = {IEEE/ACM Transactions on Audio, Speech, and Language Processing},
  volume = {24},
  number = {4},
  eprint = {1502.06922},
  eprinttype = {arxiv},
  pages = {694--707},
  issn = {2329-9290, 2329-9304},
  doi = {10.1109/TASLP.2016.2520371},
  abstract = {This paper develops a model that addresses sentence embedding, a hot topic in current natural language processing research, using recurrent neural networks with Long Short-Term Memory (LSTM) cells. Due to its ability to capture long term memory, the LSTM-RNN accumulates increasingly richer information as it goes through the sentence, and when it reaches the last word, the hidden layer of the network provides a semantic representation of the whole sentence. In this paper, the LSTM-RNN is trained in a weakly supervised manner on user click-through data logged by a commercial web search engine. Visualization and analysis are performed to understand how the embedding process works. The model is found to automatically attenuate the unimportant words and detects the salient keywords in the sentence. Furthermore, these detected keywords are found to automatically activate different cells of the LSTM-RNN, where words belonging to a similar topic activate the same cell. As a semantic representation of the sentence, the embedding vector can be used in many different applications. These automatic keyword detection and topic allocation abilities enabled by the LSTM-RNN allow the network to perform document retrieval, a difficult language processing task, where the similarity between the query and documents can be measured by the distance between their corresponding sentence embedding vectors computed by the LSTM-RNN. On a web search task, the LSTM-RNN embedding is shown to significantly outperform several existing state of the art methods. We emphasize that the proposed model generates sentence embedding vectors that are specially useful for web document retrieval tasks. A comparison with a well known general sentence embedding method, the Paragraph Vector, is performed. The results show that the proposed method in this paper significantly outperforms it for web document retrieval task.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Information Retrieval,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing},
  file = {/Users/nvk/Zotero/storage/WMSKQG7Q/Palangi et al. - 2016 - Deep Sentence Embedding Using Long Short-Term Memo.pdf;/Users/nvk/Zotero/storage/HTE7VJMA/1502.html}
}

@article{papagiannopoulouReviewKeyphraseExtraction2019,
  title = {A {{Review}} of {{Keyphrase Extraction}}},
  author = {Papagiannopoulou, Eirini and Tsoumakas, Grigorios},
  year = {2019},
  month = jul,
  journal = {arXiv:1905.05044 [cs]},
  eprint = {1905.05044},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {Keyphrase extraction is a textual information processing task concerned with the automatic extraction of representative and characteristic phrases from a document that express all the key aspects of its content. Keyphrases constitute a succinct conceptual summary of a document, which is very useful in digital information management systems for semantic indexing, faceted search, document clustering and classification. This article introduces keyphrase extraction, provides a well-structured review of the existing work, offers interesting insights on the different evaluation approaches, highlights open issues and presents a comparative experimental study of popular unsupervised techniques on five datasets.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Information Retrieval},
  file = {/Users/nvk/Zotero/storage/WWGQXFIM/Papagiannopoulou and Tsoumakas - 2019 - A Review of Keyphrase Extraction.pdf;/Users/nvk/Zotero/storage/FG8MGT49/1905.html}
}

@misc{PartnershipsInnovationPFI,
  title = {Partnerships for {{Innovation}} ({{PFI}})},
  journal = {Beta site for NSF - National Science Foundation},
  howpublished = {https://beta.nsf.gov/funding/opportunities/partnerships-innovation-pfi-0},
  langid = {english},
  file = {/Users/nvk/Zotero/storage/TX4WWN5B/partnerships-innovation-pfi-0.html}
}

@misc{PartnershipsInnovationPFIa,
  title = {Partnerships for {{Innovation}} ({{PFI}})},
  journal = {Beta site for NSF - National Science Foundation},
  howpublished = {https://beta.nsf.gov/funding/opportunities/partnerships-innovation-pfi-0},
  langid = {english}
}

@article{pathakFormulaEmbeddingApproach2018,
  title = {A {{Formula Embedding Approach}} to {{Math Information Retrieval}}},
  author = {Pathak, Amarnath and Pakray, Partha and Gelbukh, Alexander and Pathak, Amarnath and Pakray, Partha and Gelbukh, Alexander},
  year = {2018},
  month = sep,
  journal = {Computaci\'on y Sistemas},
  volume = {22},
  number = {3},
  pages = {819--833},
  publisher = {{Instituto Polit\'ecnico Nacional, Centro de Investigaci\'on en Computaci\'on}},
  issn = {1405-5546},
  doi = {10.13053/cys-22-3-3015},
  langid = {english},
  file = {/Users/nvk/Zotero/storage/Y6A9H33E/Pathak et al. - 2018 - A Formula Embedding Approach to Math Information R.pdf}
}

@article{pattaniyilCombiningTFIDFText2014,
  title = {Combining {{TF-IDF Text Retrieval}} with an {{Inverted Index}} over {{Symbol Pairs}} in {{Math Expressions}}: {{The Tangent Math Search Engine}} at {{NTCIR}} 2014},
  author = {Pattaniyil, Nidhin and Zanibbi, Richard},
  year = {2014},
  pages = {8},
  abstract = {We report on the system design and NTCIR-Math-2 task results for the Tangent math-aware search engine. Tangent uses a federated search over two indices: 1) a TF-IDF textual search engine (Lucene), and 2) a query-by-expression engine. Query-by-expression is performed using a bag-ofwords approach where expressions are represented by pairs of symbols computed from symbol layout trees (e.g. as expressed in LATEX or Presentation MathML). Extensions to support matrices and prefix subscripts and superscripts are described. Our system produced the highest highly + partially relevant Precision@5 result for the main text/math query task (92\%), and the highest Top-1 specific-item recall for the Wikipedia query-by-expression subtask (68\%). The current implementation is slow and produces large indices for large corpora, but we believe this can be ameliorated. Source code for our system is publicly available.},
  langid = {english},
  file = {/Users/nvk/Zotero/storage/WURNFQFX/Pattaniyil and Zanibbi - 2014 - Combining TF-IDF Text Retrieval with an Inverted I.pdf}
}

@inproceedings{pavankumarStructureBasedApproach2012,
  title = {A {{Structure Based Approach}} for {{Mathematical Expression Retrieval}}},
  booktitle = {Multi-Disciplinary {{Trends}} in {{Artificial Intelligence}}},
  author = {Pavan Kumar, P. and Agarwal, Arun and Bhagvati, Chakravarthy},
  year = {2012},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {23--34},
  publisher = {{Springer}},
  address = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-642-35455-7_3},
  abstract = {Mathematical expression (ME) retrieval problem has currently received much attention due to wide-spread availability of MEs on the World Wide Web. As MEs are two-dimensional in nature, traditional text retrieval techniques used in natural language processing are not sufficient for their retrieval. In this paper, we have proposed a novel structure based approach to ME retrieval problem. In our approach, query given in \textbackslash LaTeX\textbackslash LaTeX\textbackslash mbox\{\textbackslash LaTeX\} format is preprocessed to eliminate extraneous keywords (like \textbackslash displaystyle, \textbackslash begin\{array\} etc.) while retaining the structure information like superscript and subscript relationships. MEs in the database are also preprocessed and stored in the same manner. We have created a database of 829 MEs in \textbackslash LaTeX\textbackslash LaTeX\textbackslash mbox\{\textbackslash LaTeX\} form, that covers various branches of mathematics like Algebra, Trigonometry, Calculus etc. Preprocessed query is matched against the database of preprocessed MEs using Longest Common Subsequence (LCS) algorithm. LCS algorithm is used as it preserves the order of keywords in the preprocessed MEs unlike bag of words approach in the traditional text retrieval techniques. We have incorporated structure information into LCS algorithm and proposed a measure based on the modified algorithm, for ranking MEs in the database. As proposed approach exploits structure information, it is closer to human intuition. Retrieval performance has been evaluated using standard precision measure.},
  isbn = {978-3-642-35455-7},
  langid = {english},
  keywords = {longest common subsequence,Mathematical expressions,retrieval,structure information},
  file = {/Users/nvk/Zotero/storage/VGHCV4TZ/Pavan Kumar et al. - 2012 - A Structure Based Approach for Mathematical Expres.pdf}
}

@inproceedings{pengCombiningNaiveBayes2003,
  title = {Combining {{Naive Bayes}} and N-{{Gram Language Models}} for {{Text Classification}}},
  booktitle = {Advances in {{Information Retrieval}}},
  author = {Peng, Fuchun and Schuurmans, Dale},
  editor = {Sebastiani, Fabrizio},
  year = {2003},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {335--350},
  publisher = {{Springer}},
  address = {{Berlin, Heidelberg}},
  doi = {10.1007/3-540-36618-0_24},
  abstract = {We augment the naive Bayes model with an n-gram language model to address two shortcomings of naive Bayes text classifiers. The chain augmented naive Bayes classifiers we propose have two advantages over standard naive Bayes classifiers. First, a chain augmented naive Bayes model relaxes some of the independence assumptions of naive Bayes\textemdash{} allowing a local Markov chain dependence in the observed variables\textemdash while still permitting efficient inference and learning. Second, smoothing techniques from statistical language modeling can be used to recover better estimates than the Laplace smoothing techniques usually used in naive Bayes classification. Our experimental results on three real world data sets show that we achieve substantial improvements over standard naive Bayes classification, while also achieving state of the art performance that competes with the best known methods in these cases.},
  isbn = {978-3-540-36618-8},
  langid = {english},
  keywords = {Discount Probability,Laplace Smoothing,Smoothing Technique,Topic Detection,Word Segmentation},
  file = {/Users/nvk/Zotero/storage/BFQFRCGQ/Peng and Schuurmans - 2003 - Combining Naive Bayes and n-Gram Language Models f.pdf}
}

@inproceedings{penningtonGloVeGlobalVectors2014,
  title = {{{GloVe}}: {{Global Vectors}} for {{Word Representation}}},
  shorttitle = {{{GloVe}}},
  booktitle = {Proceedings of the 2014 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}} ({{EMNLP}})},
  author = {Pennington, Jeffrey and Socher, Richard and Manning, Christopher},
  year = {2014},
  month = oct,
  pages = {1532--1543},
  publisher = {{Association for Computational Linguistics}},
  address = {{Doha, Qatar}},
  doi = {10.3115/v1/D14-1162},
  file = {/Users/nvk/Zotero/storage/4MV9WXKV/Pennington et al. - 2014 - GloVe Global Vectors for Word Representation.pdf}
}

@article{peronaScalespaceEdgeDetection1990,
  title = {Scale-Space and Edge Detection Using Anisotropic Diffusion},
  author = {Perona, P. and Malik, J.},
  year = {1990},
  month = jul,
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  volume = {12},
  number = {7},
  pages = {629--639},
  issn = {01628828},
  doi = {10.1109/34.56205},
  langid = {english},
  file = {/Users/nvk/Zotero/storage/GHGPYWKG/Perona and Malik - 1990 - Scale-space and edge detection using anisotropic d.pdf}
}

@article{perriconeDesignStochasticComputing2016,
  title = {Design of {{Stochastic Computing Circuits Using Nanomagnetic Logic}}},
  author = {Perricone, Robert and Liu, Yang and Dingler, Aaron and Hu, X. Sharon and Niemier, Michael},
  year = {2016},
  month = mar,
  journal = {IEEE Transactions on Nanotechnology},
  volume = {15},
  number = {2},
  pages = {179--187},
  issn = {1941-0085},
  doi = {10.1109/TNANO.2015.2511072},
  abstract = {We consider the design of stochastic computing (SC) hardware based on spintronic devices. SC offers low-cost implementations of arithmetic operations and high degrees of error tolerance. When compared to charge-based devices, spin-based devices could be lower energy, nonvolatile, etc. However, spin-based devices can be fundamentally more error prone than charge-based devices. The marriage of SC architectures and spin-based devices has the potential to produce information processing systems that are robust, low energy, and nonvolatile. In this paper, we investigate SC hardware comprised of nanomagnetic logic (NML) devices. NML is a ``beyond-CMOS'' technology that uses bistable magnets to store, process, and move binary information. We introduce new NML circuit structures that exploit unique features of the technology to efficiently realize hardware components required for an SC system (e.g., random number generation). We also benchmark NML-based SC circuits against other implementations, and illustrate how features that are unique to NML (e.g., inherent pipelining) can lead to improved performance. Our results indicate NML SC implementations achieve smaller area footprints with reduced energy and delay when compared to CMOS equivalents.},
  keywords = {arithmetic operations,beyond-CMOS technology,bistable magnets,CMOS integrated circuits,error tolerance,Fault tolerance,Hardware,hardware components,information processing systems,logic devices,Logic gates,Magnetic circuits,Magnetic domain walls,Magnetic domains,magnetoelectronics,magnets,nanomagnetic logic devices,nanomagnetics,random number generation,Random number generation,spin-based devices,spintronic devices,stochastic computing circuits,stochastic computing hardware,Stochastic logic circuits,Switches,Threshold logic circuits},
  file = {/Users/nvk/Zotero/storage/6R2GRM24/7362190.html}
}

@article{pershinPracticalApproachProgrammable2010,
  title = {Practical {{Approach}} to {{Programmable Analog Circuits With Memristors}}},
  author = {Pershin, Yuriy V. and Di Ventra, Massimiliano},
  year = {2010},
  month = aug,
  journal = {IEEE Transactions on Circuits and Systems I: Regular Papers},
  volume = {57},
  number = {8},
  pages = {1857--1864},
  issn = {1558-0806},
  doi = {10.1109/TCSI.2009.2038539},
  abstract = {We suggest an approach to use memristors (resistors with memory) in programmable analog circuits. Our idea consists in a circuit design in which low voltages are applied to memristors during their operation as analog circuit elements and high voltages are used to program the memristor's states. This way, as it was demonstrated in recent experiments, the state of memristors does not essentially change during analog mode operation. As an example of our approach, we have built several programmable analog circuits demonstrating memristor-based programming of threshold, gain and frequency. In these circuits the role of memristor is played by a memristor emulator developed by us.},
  keywords = {Analog circuits,analog memories,Analog memory,analogue circuits,Biological system modeling,circuit design,Low voltage,memory,memristors,Memristors,Physics,Predictive models,programmable analog circuits,programmable circuits,resistance,Resistors,Terminology,Thin film circuits},
  file = {/Users/nvk/Zotero/storage/68S95YZB/Pershin and Di Ventra - 2010 - Practical Approach to Programmable Analog Circuits.pdf;/Users/nvk/Zotero/storage/ZS37P98E/5405039.html}
}

@article{petersDeepContextualizedWord2018,
  title = {Deep Contextualized Word Representations},
  author = {Peters, Matthew E. and Neumann, Mark and Iyyer, Mohit and Gardner, Matt and Clark, Christopher and Lee, Kenton and Zettlemoyer, Luke},
  year = {2018},
  month = mar,
  journal = {arXiv:1802.05365 [cs]},
  eprint = {1802.05365},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {We introduce a new type of deep contextualized word representation that models both (1) complex characteristics of word use (e.g., syntax and semantics), and (2) how these uses vary across linguistic contexts (i.e., to model polysemy). Our word vectors are learned functions of the internal states of a deep bidirectional language model (biLM), which is pre-trained on a large text corpus. We show that these representations can be easily added to existing models and significantly improve the state of the art across six challenging NLP problems, including question answering, textual entailment and sentiment analysis. We also present an analysis showing that exposing the deep internals of the pre-trained network is crucial, allowing downstream models to mix different types of semi-supervision signals.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/nvk/Zotero/storage/A7SRTS42/Peters et al. - 2018 - Deep contextualized word representations.pdf;/Users/nvk/Zotero/storage/6TCKE5BS/1802.html}
}

@article{piatekIncentivesBuildRobustness,
  title = {Do Incentives Build Robustness in {{BitTorrent}}?},
  author = {Piatek, Michael and Isdal, Tomas and Anderson, Thomas and Krishnamurthy, Arvind and Venkataramani, Arun},
  pages = {14},
  abstract = {A fundamental problem with many peer-to-peer systems is the tendency for users to ``free ride''\textemdash to consume resources without contributing to the system. The popular file distribution tool BitTorrent was explicitly designed to address this problem, using a tit-for-tat reciprocity strategy to provide positive incentives for nodes to contribute resources to the swarm. While BitTorrent has been extremely successful, we show that its incentive mechanism is not robust to strategic clients. Through performance modeling parameterized by real world traces, we demonstrate that all peers contribute resources that do not directly improve their performance. We use these results to drive the design and implementation of BitTyrant, a strategic BitTorrent client that provides a median 70\% performance gain for a 1 Mbit client on live Internet swarms. We further show that when applied universally, strategic clients can hurt average per-swarm performance compared to today's BitTorrent client implementations.},
  langid = {english},
  file = {/Users/nvk/Zotero/storage/C3H5U93S/Piatek et al. - Do incentives build robustness in BitTorrent.pdf}
}

@inproceedings{possasSetbasedModelNew2002,
  title = {Set-Based Model: A New Approach for Information Retrieval},
  shorttitle = {Set-Based Model},
  booktitle = {Proceedings of the 25th Annual International {{ACM SIGIR}} Conference on {{Research}} and Development in Information Retrieval},
  author = {P{\^o}ssas, Bruno and Ziviani, Nivio and Meira, Wagner and {Ribeiro-Neto}, Berthier},
  year = {2002},
  month = aug,
  series = {{{SIGIR}} '02},
  pages = {230--237},
  publisher = {{Association for Computing Machinery}},
  address = {{New York, NY, USA}},
  doi = {10.1145/564376.564417},
  abstract = {The objective of this paper is to present a new technique for computing term weights for index terms, which leads to a new ranking mechanism, referred to as set-based model. The components in our model are no longer terms, but termsets. The novelty is that we compute term weights using a data mining technique called association rules, which is time efficient and yet yields nice improvements in retrieval effectiveness. The set-based model function for computing the similarity between a document and a query considers the termset frequency in the document and its scarcity in the document collection. Experimental results show that our model improves the average precision of the answer set for all three collections evaluated. For the TReC-3 collection, our set-based model led to a gain, relative to the standard vector space model, of 37\% in average precision curves and of 57\% in average precision for the top 10 documents. Like the vector space model, the set-based model has time complexity that is linear in the number of documents in the collection.},
  isbn = {978-1-58113-561-9},
  keywords = {closed association rule mining,data mining,information retrieval models,set theory,weighting index term co-occurrences},
  file = {/Users/nvk/Zotero/storage/VYUP3EFU/PÃ´ssas et al. - 2002 - Set-based model a new approach for information re.pdf}
}

@inproceedings{prodromakisReviewMemristiveDevices2010,
  title = {A Review on Memristive Devices and Applications},
  booktitle = {2010 17th {{IEEE International Conference}} on {{Electronics}}, {{Circuits}} and {{Systems}}},
  author = {Prodromakis, T. and Toumazou, C.},
  year = {2010},
  month = dec,
  pages = {934--937},
  doi = {10.1109/ICECS.2010.5724666},
  abstract = {Recent discovery of the memristor has sparked a new wave of enthusiasm and optimism in revolutionising circuit design, marking a new era for the advancement of neuromorphic and analogue applications. In this work, we consider practical applications in which the highly non-linear dynamic response of the memristor can be employed. It is shown that the device can be utilised as a non-volatile memory element and/or a programmable dynamic load, with particular emphasis given into bio-inspired analog implementations that typically exploit the ability of the memristor to support both logic and memory simultaneously. Finally, a novel concept is presented demonstrating the capacity of memristive networks in realising demanding image processing algorithms and more specifically edge detection.},
  keywords = {analogue application,bio-inspired materials,bioinspired analog implementations,circuit design,dynamic load,edge detection,image processing algorithms,logic circuit,memory circuit,memristive devices,memristive dynamics,Memristor,memristors,neuromorphic,neuromorphic application,non-volatile memory,nonlinear dynamic response,nonvolatile memory element,programmable dynamic load,random-access storage},
  file = {/Users/nvk/Zotero/storage/KPWP8I6N/Prodromakis and Toumazou - 2010 - A review on memristive devices and applications.pdf;/Users/nvk/Zotero/storage/VM5N3KJA/5724666.html}
}

@inproceedings{qiuQueryOrientedKeyphraseExtraction2012,
  title = {Query-{{Oriented Keyphrase Extraction}}},
  booktitle = {Information {{Retrieval Technology}}},
  author = {Qiu, Minghui and Li, Yaliang and Jiang, Jing},
  editor = {Hou, Yuexian and Nie, Jian-Yun and Sun, Le and Wang, Bo and Zhang, Peng},
  year = {2012},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {64--75},
  publisher = {{Springer}},
  address = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-642-35341-3_6},
  abstract = {People often issue informational queries to search engines to find out more about some entities or events. While a Wikipedia-like summary would be an ideal answer to such queries, not all queries have a corresponding Wikipedia entry. In this work we propose to study query-oriented keyphrase extraction, which can be used to assist search results summarization. We propose a general method for keyphrase extraction for our task, where we consider both phraseness and informativeness. We discuss three criteria for phraseness and four ways to compute informativeness scores. Using a large Wikipedia corpus and 40 queries, our empirical evaluation shows that using a named entity-based phraseness criterion and a language model-based informativeness score gives the best performance on our task. This method also outperforms two state-of-the-art baseline methods.},
  isbn = {978-3-642-35341-3},
  langid = {english},
  keywords = {informativeness,Keyphrase extraction,language model,phraseness},
  file = {/Users/nvk/Zotero/storage/985SNTQP/Qiu et al. - 2012 - Query-Oriented Keyphrase Extraction.pdf}
}

@inproceedings{quBERTHistoryAnswer2019,
  title = {{{BERT}} with {{History Answer Embedding}} for {{Conversational Question Answering}}},
  booktitle = {Proceedings of the 42nd {{International ACM SIGIR Conference}} on {{Research}} and {{Development}} in {{Information Retrieval}}},
  author = {Qu, Chen and Yang, Liu and Qiu, Minghui and Croft, W. Bruce and Zhang, Yongfeng and Iyyer, Mohit},
  year = {2019},
  month = jul,
  series = {{{SIGIR}}'19},
  pages = {1133--1136},
  publisher = {{Association for Computing Machinery}},
  address = {{New York, NY, USA}},
  doi = {10.1145/3331184.3331341},
  abstract = {Conversational search is an emerging topic in the information retrieval community. One of the major challenges to multi-turn conversational search is to model the conversation history to answer the current question. Existing methods either prepend history turns to the current question or use complicated attention mechanisms to model the history. We propose a conceptually simple yet highly effective approach referred to as history answer embedding. It enables seamless integration of conversation history into a conversational question answering (ConvQA) model built on BERT (Bidirectional Encoder Representations from Transformers). We first explain our view that ConvQA is a simplified but concrete setting of conversational search, and then we provide a general framework to solve ConvQA. We further demonstrate the effectiveness of our approach under this framework. Finally, we analyze the impact of different numbers of history turns under different settings to provide new insights into conversation history modeling in ConvQA.},
  isbn = {978-1-4503-6172-9},
  keywords = {BERT,conversational question answering,conversational search,history answer embedding},
  file = {/Users/nvk/Zotero/storage/PRXII2PM/Qu et al. - 2019 - BERT with History Answer Embedding for Conversatio.pdf}
}

@article{raghavanCriticalAnalysisVector1986,
  title = {A Critical Analysis of Vector Space Model for Information Retrieval},
  author = {Raghavan, Vijay V. and Wong, S. K. M.},
  year = {1986},
  journal = {Journal of the American Society for Information Science},
  volume = {37},
  number = {5},
  pages = {279--287},
  issn = {1097-4571},
  doi = {10.1002/(SICI)1097-4571(198609)37:5<279::AID-ASI1>3.0.CO;2-Q},
  abstract = {Notations and definitions necessary to identify the concepts and relationships that are important in modelling information retrieval objects and processes in the context of vector spaces are presented. Earlier work on the use of vector model is evaluated in terms of the concepts introduced and certain problems and inconsistencies are identified. More importantly, this investigation should lead to a clear understanding of the issues and problems in using the vector space model in information retrieval. \textcopyright{} 1986 John Wiley \& Sons, Inc.},
  langid = {english},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/\%28SICI\%291097-4571\%28198609\%2937\%3A5\%3C279\%3A\%3AAID-ASI1\%3E3.0.CO\%3B2-Q},
  file = {/Users/nvk/Zotero/storage/JBURWE9R/Raghavan and Wong - 1986 - A critical analysis of vector space model for info.pdf;/Users/nvk/Zotero/storage/KZ4ED5NS/(SICI)1097-4571(198609)375279AID-ASI13.0.html}
}

@article{ramosUsingTFIDFDetermine,
  title = {Using {{TF-IDF}} to {{Determine Word Relevance}} in {{Document Queries}}},
  author = {Ramos, Juan},
  pages = {4},
  abstract = {In this paper, we examine the results of applying Term Frequency Inverse Document Frequency (TF-IDF) to determine what words in a corpus of documents might be more favorable to use in a query. As the term implies, TF-IDF calculates values for each word in a document through an inverse proportion of the frequency of the word in a particular document to the percentage of documents the word appears in. Words with high TF-IDF numbers imply a strong relationship with the document they appear in, suggesting that if that word were to appear in a query, the document could be of interest to the user. We provide evidence that this simple algorithm efficiently categorizes relevant words that can enhance query retrieval.},
  langid = {english},
  file = {/Users/nvk/Zotero/storage/MWAWAK95/Ramos - Using TF-IDF to Determine Word Relevance in Docume.pdf}
}

@inproceedings{ravuriComparativeStudyNeural2015,
  title = {A Comparative Study of Neural Network Models for Lexical Intent Classification},
  booktitle = {2015 {{IEEE Workshop}} on {{Automatic Speech Recognition}} and {{Understanding}} ({{ASRU}})},
  author = {Ravuri, Suman and Stoicke, Andreas},
  year = {2015},
  month = dec,
  pages = {368--374},
  doi = {10.1109/ASRU.2015.7404818},
  abstract = {Domain and intent classification are critical pre-processing steps for many speech understanding and dialog systems, as it allows for certain types of utterances to be routed to particular subsystems. In previous work, we explored many types of neural network (NN) architectures \textemdash{} some feedforward and some recurrent \textemdash{} for lexical intent classification and found that they improved upon more traditional statistical baselines. In this paper we carry out a more comprehensive comparison of NN models including the recently proposed gated recurrent unit network, for two domain/intent classification tasks. Furthermore, whereas the previous work was confined to relatively small and controlled datasets, we now include experiments based on a large set obtained from the Cortana personal assistant application. We compare feedforward, recurrent, and gated \textemdash{} such as LSTM and GRU \textemdash{} networks against each other. On both the ATIS intent task and the much larger Cortana domain classification tasks, gated networks outperform recurrent models, which in turn outperform feedforward networks. Also, we compared standard word vector models against a representation which encodes words as sets of character n-grams to mitigate the out-of-vocabulary problem. We find that in nearly all cases, the standard word vectors outperform character-based word representations. Best results are obtained by linearly combining scores from NN models with log likelihood ratios obtained from N-gram language models.},
  keywords = {Artificial neural networks,Feedforward neural networks,Logic gates,Speech,Standards,Training},
  file = {/Users/nvk/Zotero/storage/KGFJ567G/Ravuri and Stoicke - 2015 - A comparative study of neural network models for l.pdf;/Users/nvk/Zotero/storage/7MIZGBRT/7404818.html}
}

@article{reimersSentenceBERTSentenceEmbeddings2019,
  title = {Sentence-{{BERT}}: {{Sentence Embeddings}} Using {{Siamese BERT-Networks}}},
  shorttitle = {Sentence-{{BERT}}},
  author = {Reimers, Nils and Gurevych, Iryna},
  year = {2019},
  month = aug,
  journal = {arXiv:1908.10084 [cs]},
  eprint = {1908.10084},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {BERT (Devlin et al., 2018) and RoBERTa (Liu et al., 2019) has set a new state-of-the-art performance on sentence-pair regression tasks like semantic textual similarity (STS). However, it requires that both sentences are fed into the network, which causes a massive computational overhead: Finding the most similar pair in a collection of 10,000 sentences requires about 50 million inference computations (\textasciitilde 65 hours) with BERT. The construction of BERT makes it unsuitable for semantic similarity search as well as for unsupervised tasks like clustering. In this publication, we present Sentence-BERT (SBERT), a modification of the pretrained BERT network that use siamese and triplet network structures to derive semantically meaningful sentence embeddings that can be compared using cosine-similarity. This reduces the effort for finding the most similar pair from 65 hours with BERT / RoBERTa to about 5 seconds with SBERT, while maintaining the accuracy from BERT. We evaluate SBERT and SRoBERTa on common STS tasks and transfer learning tasks, where it outperforms other state-of-the-art sentence embeddings methods.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/nvk/Zotero/storage/KAWF9S8U/Reimers and Gurevych - 2019 - Sentence-BERT Sentence Embeddings using Siamese B.pdf;/Users/nvk/Zotero/storage/VLYDEP92/1908.html}
}

@article{reimersSentenceBERTSentenceEmbeddings2019a,
  title = {Sentence-{{BERT}}: {{Sentence Embeddings}} Using {{Siamese BERT-Networks}}},
  shorttitle = {Sentence-{{BERT}}},
  author = {Reimers, Nils and Gurevych, Iryna},
  year = {2019},
  month = aug,
  journal = {arXiv:1908.10084 [cs]},
  eprint = {1908.10084},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {BERT (Devlin et al., 2018) and RoBERTa (Liu et al., 2019) has set a new state-of-the-art performance on sentence-pair regression tasks like semantic textual similarity (STS). However, it requires that both sentences are fed into the network, which causes a massive computational overhead: Finding the most similar pair in a collection of 10,000 sentences requires about 50 million inference computations (\textasciitilde 65 hours) with BERT. The construction of BERT makes it unsuitable for semantic similarity search as well as for unsupervised tasks like clustering. In this publication, we present Sentence-BERT (SBERT), a modification of the pretrained BERT network that use siamese and triplet network structures to derive semantically meaningful sentence embeddings that can be compared using cosine-similarity. This reduces the effort for finding the most similar pair from 65 hours with BERT / RoBERTa to about 5 seconds with SBERT, while maintaining the accuracy from BERT. We evaluate SBERT and SRoBERTa on common STS tasks and transfer learning tasks, where it outperforms other state-of-the-art sentence embeddings methods.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/nvk/Zotero/storage/UAD6NJAM/Reimers and Gurevych - 2019 - Sentence-BERT Sentence Embeddings using Siamese B.pdf;/Users/nvk/Zotero/storage/V2ZNDH4M/1908.html}
}

@misc{ResearchEmergingTechnologies,
  title = {Research on {{Emerging Technologies}} for {{Teaching}} and {{Learning}}},
  journal = {Beta site for NSF - National Science Foundation},
  howpublished = {https://beta.nsf.gov/funding/opportunities/research-emerging-technologies-teaching-and-learning},
  langid = {english}
}

@misc{ResearchExperiencesUndergraduates,
  title = {Research {{Experiences}} for {{Undergraduates}} ({{REU}})},
  journal = {Beta site for NSF - National Science Foundation},
  howpublished = {https://beta.nsf.gov/funding/opportunities/research-experiences-undergraduates-reu},
  langid = {english},
  file = {/Users/nvk/Zotero/storage/FBZWEU87/research-experiences-undergraduates-reu.html}
}

@article{robertsonRelevanceWeightingSearch1976,
  title = {Relevance Weighting of Search Terms},
  author = {Robertson, S. E. and Jones, K. Sparck},
  year = {1976},
  journal = {Journal of the American Society for Information Science},
  volume = {27},
  number = {3},
  pages = {129--146},
  issn = {1097-4571},
  doi = {10.1002/asi.4630270302},
  abstract = {This paper examines statistical techniques for exploiting relevance information to weight search terms. These techniques are presented as a natural extension of weighting methods using information about the distribution of index terms in documents in general. A series of relevance weighting functions is derived and is justified by theoretical considerations. In particular, it is shown that specific weighted search methods are implied by a general probabilistic theory of retrieval. Different applications of relevance weighting are illustrated by experimental results for test collections.},
  copyright = {Copyright \textcopyright{} 1976 Wiley Periodicals, Inc., A Wiley Company},
  langid = {english},
  keywords = {basic_probabilistic_model},
  annotation = {\_eprint: https://asistdl.onlinelibrary.wiley.com/doi/pdf/10.1002/asi.4630270302},
  file = {/Users/nvk/Zotero/storage/7HMXXV3D/Robertson and Jones - 1976 - Relevance weighting of search terms.pdf;/Users/nvk/Zotero/storage/NTY7WVVM/asi.html}
}

@inproceedings{robertsonSimpleEffectiveApproximations1994,
  title = {Some {{Simple Effective Approximations}} to the 2-{{Poisson Model}} for {{Probabilistic Weighted Retrieval}}},
  booktitle = {{{SIGIR}} '94},
  author = {Robertson, S. E. and Walker, S.},
  editor = {Croft, Bruce W. and {van Rijsbergen}, C. J.},
  year = {1994},
  pages = {232--241},
  publisher = {{Springer}},
  address = {{London}},
  doi = {10.1007/978-1-4471-2099-5_24},
  abstract = {The 2-Poisson model for term frequencies is used to suggest ways of incorporating certain variables in probabilistic models for information retrieval. The variables concerned are within-document term frequency, document length, and within-query term frequency. Simple weighting functions are developed, and tested on the TREC test collection. Considerable performance improvements (over simple inverse collection frequency weighting) are demonstrated.},
  isbn = {978-1-4471-2099-5},
  langid = {english},
  keywords = {Document Length,Query Term,Relevance Feedback,Short Query,Term Frequency},
  file = {/Users/nvk/Zotero/storage/7CK3CWLD/Robertson and Walker - 1994 - Some Simple Effective Approximations to the 2-Pois.pdf}
}

@article{rohatgiPSUCLEF2020ARQMath,
  title = {{{PSU}} at {{CLEF-2020 ARQMath Track}}: {{Unsupervised Re-ranking}} Using {{Pretraining}}},
  author = {Rohatgi, Shaurya and Wu, Jian and Giles, C Lee},
  pages = {8},
  abstract = {This paper elaborates on our submission to the ARQMath track at CLEF 2020. Our primary run for the main Task-1: Question Answering uses a two-stage retrieval technique in which the first stage is a fusion of traditional BM25 scoring and tf-idf with cosine similarity-based retrieval while the second stage is a finer re-ranking technique using contextualized embeddings. For the re-ranking we use a pre-trained robertabase model (110 million parameters) to make the language model more math-aware. Our approach achieves a higher NDCG score than the baseline, while our MAP and P@10 scores are competitive, performing better than the best submission (MathDowsers) for text and text+formula dependent topics.},
  langid = {english},
  file = {/Users/nvk/Zotero/storage/7W8SEA85/Rohatgi et al. - PSU at CLEF-2020 ARQMath Track Unsupervised Re-ra.pdf}
}

@article{romijnAutomaticAnalysisTerm1995,
  title = {Automatic Analysis of Term Rewriting Systems: Proving Properties of Term Rewriting Systems Derived from {{ASF}}+{{SDF}} Specifications},
  shorttitle = {Automatic Analysis of Term Rewriting Systems},
  author = {Romijn, J.},
  year = {1995},
  journal = {undefined},
  abstract = {A supporting tool is presented that derives a conditional term rewriting system from an Asf+Sdf speci cation, and tests this system for certain properties, including non-erasing, orthogonal, recursive program scheme and strongly normalizing. Term rewriting systems are used frequently as an implementation technique for algebraic speci cations. We present a supporting tool for the Asf+SdfMeta-environment that derives a conditional term rewriting system from an Asf+Sdf speci cation, and tests this system for certain properties. The properties included are: non-erasing, orthogonal, recursive program scheme and strongly normalizing. Since the speci cation from which we derive the conditional term rewriting system has some speci c Asf+Sdf features, the theory that supports this derivation is somewhat di erent from well-known term rewrite theories. To keep this supporting theory sound, not every Asf+Sdf feature is allowed. The property strongly normalizing is in general not decidable. We chose the method of recursive path orderings to be able to prove the property for a subset of the possible term rewriting systems. The future Asf+Sdf Meta-environment will be based on Asf+Sdf speci cations, so-called meta-level speci cations. Although this new Meta-environment is not yet ready for use, the implementation of the tool presented here is based on the future situation.},
  langid = {english},
  file = {/Users/nvk/Zotero/storage/5SFZ5MXU/ef70d0bdebb612cb951acf69ea5bb0af825ec76f.html}
}

@inproceedings{roseVTRProjectArchitecture2012,
  title = {The {{VTR}} Project: Architecture and {{CAD}} for {{FPGAs}} from Verilog to Routing},
  shorttitle = {The {{VTR}} Project},
  booktitle = {Proceedings of the {{ACM}}/{{SIGDA}} International Symposium on {{Field Programmable Gate Arrays}}},
  author = {Rose, Jonathan and Luu, Jason and Yu, Chi Wai and Densmore, Opal and Goeders, Jeffrey and Somerville, Andrew and Kent, Kenneth B. and Jamieson, Peter and Anderson, Jason},
  year = {2012},
  month = feb,
  series = {{{FPGA}} '12},
  pages = {77--86},
  publisher = {{Association for Computing Machinery}},
  address = {{Monterey, California, USA}},
  doi = {10.1145/2145694.2145708},
  abstract = {To facilitate the development of future FPGA architectures and CAD tools -- both embedded programmable fabrics and pure-play FPGAs -- there is a need for a large scale, publicly available software suite that can synthesize circuits into easily-described hypothetical FPGA architectures. These circuits should be captured at the HDL level, or higher, and pass through logical and physical synthesis. Such a tool must provide detailed modelling of area, performance and energy to enable architecture exploration. As software flows themselves evolve to permit design capture at ever higher levels of abstraction, this downstream full-implementation flow will always be required. This paper describes the current status and new release of an ongoing effort to create such a flow - the 'Verilog to Routing' (VTR) project, which is a broad collaboration of researchers. There are three core tools: ODIN II for Verilog Elaboration and front-end hard-block synthesis, ABC for logic synthesis, and VPR for physical synthesis and analysis. ODIN II now has a simulation capability to help verify that its output is correct, as well as specialized synthesis at the elaboration step for multipliers and memories. ABC is used to optimize the 'soft' logic of the FPGA. The VPR-based packing, placement and routing is now fully timing-driven (the previous release was not) and includes new capability to target complex logic blocks. In addition we have added a set of four large benchmark circuits to a suite of previously-released Verilog HDL circuits. Finally, we illustrate the use of the new flow by using it to help architect a floating-point unit in an FPGA, and contrast it with a prior, much longer effort that was required to do the same thing.},
  isbn = {978-1-4503-1155-7},
  keywords = {architecture,CAD,FPGA},
  file = {/Users/nvk/Zotero/storage/DQ7A5VNW/Rose et al. - 2012 - The VTR project architecture and CAD for FPGAs fr.pdf}
}

@inproceedings{ruiOptimizingLearningImage2000,
  title = {Optimizing Learning in Image Retrieval},
  booktitle = {Proceedings {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}}. {{CVPR}} 2000 ({{Cat}}. {{No}}.{{PR00662}})},
  author = {Rui, Y. and Huang, T.},
  year = {2000},
  month = jun,
  volume = {1},
  pages = {236-243 vol.1},
  issn = {1063-6919},
  doi = {10.1109/CVPR.2000.855825},
  abstract = {Combining learning with vision techniques in interactive image retrieval has been an active research topic during the past few years. However, existing learning techniques either are based on heuristics or fail to analyze the working conditions. Furthermore, there is almost no in depth study on how to effectively learn from the users when there are multiple visual features in the retrieval system. To address these limitations, in this paper we present a vigorous optimization formulation of the learning process and solve the problem in a principled way. By using Lagrange multipliers, we have derived explicit solutions, which are both optimal and fast to compute. Extensive comparisons against state-of-the-art techniques have been performed. Experiments were carried out on a large-size heterogeneous image collection consisting of 17,000 images. Retrieval performance was tested under a wide range of conditions. Various evaluation criteria, including precision-recall curve and rank measure, have demonstrated the effectiveness and robustness of the proposed technique.},
  keywords = {Cost accounting,Electronic switching systems,Failure analysis,Feedback,Humans,Image retrieval,Lagrangian functions,Machine vision,Statistics,Testing},
  file = {/Users/nvk/Zotero/storage/UTABZ2K6/Rui and Huang - 2000 - Optimizing learning in image retrieval.pdf;/Users/nvk/Zotero/storage/3TB55JVU/855825.html}
}

@article{ruMathIndexerSearcher2014,
  title = {Math {{Indexer}} and {{Searcher}} under the {{Hood}}: {{History}} and {{Development}} of a {{Winning Strategy}}},
  author = {Ru, Michal and Sojka, Petr and L{\'i}{\v s}ka, Martin},
  year = {2014},
  pages = {8},
  abstract = {This paper describes and summarizes experience of Masaryk University Math Information Retrieval team (MIRMU) with the mathematical search developed and performed for the NTCIR-11 Math-2 Task. Our approach is the similarity search based on canonicalized MathML and second generation of scalable full text search engine Math Indexer and Searcher (MIaS) with attested state-of-the-art information retrieval techniques like query expansion. The capability of MIaS system in terms of math query notation, normalization and combining math with textual query tokens was deployed by submitting multiple runs with four query notations provided, and with results merged from multiple queries. The analysis of the evaluation results shows that the system performs best using TEX queries that are translated and canonicalized to Content MathML, where MIaS ranked as \#1 for all metrics returning very relevant results.},
  langid = {english},
  file = {/Users/nvk/Zotero/storage/TDFF4QNF/Ru et al. - 2014 - Math Indexer and Searcher under the Hood History .pdf}
}

@article{ruMathIndexerSearcher2016,
  title = {Math {{Indexer}} and {{Searcher}} under the {{Hood}}: {{Fine-tuning Query Expansion}} and {{Unification Strategies}}},
  author = {Ru, Michal and Sojka, Petr and L{\'i}{\v s}ka, Martin},
  year = {2016},
  pages = {7},
  abstract = {This paper summarizes the experience of Math Information Retrieval team of Masaryk University (MIRMU) with the NTCIR-12 MathIR arXiv Main Task and its subtasks.},
  langid = {english},
  file = {/Users/nvk/Zotero/storage/IGZYEY7B/Ru et al. - 2016 - Math Indexer and Searcher under the Hood Fine-tun.pdf}
}

@article{rumelhartLearningRepresentationsBackpropagating1986,
  title = {Learning Representations by Back-Propagating Errors},
  author = {Rumelhart, David E. and Hinton, Geoffrey E. and Williams, Ronald J.},
  year = {1986},
  month = oct,
  journal = {Nature},
  volume = {323},
  number = {6088},
  pages = {533--536},
  publisher = {{Nature Publishing Group}},
  issn = {1476-4687},
  doi = {10.1038/323533a0},
  abstract = {We describe a new learning procedure, back-propagation, for networks of neurone-like units. The procedure repeatedly adjusts the weights of the connections in the network so as to minimize a measure of the difference between the actual output vector of the net and the desired output vector. As a result of the weight adjustments, internal `hidden' units which are not part of the input or output come to represent important features of the task domain, and the regularities in the task are captured by the interactions of these units. The ability to create useful new features distinguishes back-propagation from earlier, simpler methods such as the perceptron-convergence procedure1.},
  copyright = {1986 Nature Publishing Group},
  langid = {english},
  annotation = {Bandiera\_abtest: a Cg\_type: Nature Research Journals Primary\_atype: Research},
  file = {/Users/nvk/Zotero/storage/7Y7R8J8Z/Rumelhart et al. - 1986 - Learning representations by back-propagating error.pdf;/Users/nvk/Zotero/storage/9ASEHPJS/323533a0.html}
}

@article{sabourDynamicRoutingCapsules2017,
  title = {Dynamic {{Routing Between Capsules}}},
  author = {Sabour, Sara and Frosst, Nicholas and Hinton, Geoffrey E.},
  year = {2017},
  month = nov,
  journal = {arXiv:1710.09829 [cs]},
  eprint = {1710.09829},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {A capsule is a group of neurons whose activity vector represents the instantiation parameters of a specific type of entity such as an object or an object part. We use the length of the activity vector to represent the probability that the entity exists and its orientation to represent the instantiation parameters. Active capsules at one level make predictions, via transformation matrices, for the instantiation parameters of higher-level capsules. When multiple predictions agree, a higher level capsule becomes active. We show that a discrimininatively trained, multi-layer capsule system achieves state-of-the-art performance on MNIST and is considerably better than a convolutional net at recognizing highly overlapping digits. To achieve these results we use an iterative routing-by-agreement mechanism: A lower-level capsule prefers to send its output to higher level capsules whose activity vectors have a big scalar product with the prediction coming from the lower-level capsule.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/nvk/Zotero/storage/KDCSRGLN/Sabour et al. - 2017 - Dynamic Routing Between Capsules.pdf;/Users/nvk/Zotero/storage/TXX4RHWG/1710.html}
}

@book{sakaiEvaluatingInformationRetrieval2021,
  title = {Evaluating {{Information Retrieval}} and {{Access Tasks}}: {{NTCIR}}'s {{Legacy}} of {{Research Impact}}},
  shorttitle = {Evaluating {{Information Retrieval}} and {{Access Tasks}}},
  editor = {Sakai, Tetsuya and Oard, Douglas W. and Kando, Noriko},
  year = {2021},
  series = {The {{Information Retrieval Series}}},
  volume = {43},
  publisher = {{Springer Singapore}},
  address = {{Singapore}},
  doi = {10.1007/978-981-15-5554-1},
  isbn = {9789811555534 9789811555541},
  langid = {english},
  file = {/Users/nvk/Zotero/storage/Y2NVGS58/Sakai et al. - 2021 - Evaluating Information Retrieval and Access Tasks.pdf}
}

@book{sakaiEvaluatingInformationRetrieval2021a,
  title = {Evaluating {{Information Retrieval}} and {{Access Tasks}}: {{NTCIR}}'s {{Legacy}} of {{Research Impact}}},
  shorttitle = {Evaluating {{Information Retrieval}} and {{Access Tasks}}},
  editor = {Sakai, Tetsuya and Oard, Douglas W. and Kando, Noriko},
  year = {2021},
  series = {The {{Information Retrieval Series}}},
  volume = {43},
  publisher = {{Springer Singapore}},
  address = {{Singapore}},
  doi = {10.1007/978-981-15-5554-1},
  isbn = {9789811555534 9789811555541},
  langid = {english},
  file = {/Users/nvk/Zotero/storage/U5IP4UKK/Sakai et al. - 2021 - Evaluating Information Retrieval and Access Tasks.pdf}
}

@article{saltonSPECIFICATIONTERMVALUES1973,
  title = {{{ON THE SPECIFICATION OF TERM VALUES IN AUTOMATIC INDEXING}}},
  author = {SALTON, G. and YANG, C.S.},
  year = {1973},
  month = jan,
  journal = {Journal of Documentation},
  volume = {29},
  number = {4},
  pages = {351--372},
  publisher = {{MCB UP Ltd}},
  issn = {0022-0418},
  doi = {10.1108/eb026562},
  abstract = {The existing practice in automatic indexing is reviewed, and it is shown that the standard theories for the specification of term values (or weights) are not adequate. New techniques are introduced for the assignment of weights to index terms, based on the characteristics of individual document collections. The effectiveness of some of the proposed methods is evaluated.},
  keywords = {basic_vector_model},
  file = {/Users/nvk/Zotero/storage/99EMLGRD/SALTON and YANG - 1973 - ON THE SPECIFICATION OF TERM VALUES IN AUTOMATIC I.pdf;/Users/nvk/Zotero/storage/EDCEXDLM/html.html}
}

@article{saltonVectorSpaceModel1975,
  title = {A Vector Space Model for Automatic Indexing},
  author = {Salton, G. and Wong, A. and Yang, C. S.},
  year = {1975},
  month = nov,
  journal = {Communications of the ACM},
  volume = {18},
  number = {11},
  pages = {613--620},
  issn = {0001-0782},
  doi = {10.1145/361219.361220},
  abstract = {In a document retrieval, or other pattern matching environment where stored entities (documents) are compared with each other or with incoming patterns (search requests), it appears that the best indexing (property) space is one where each entity lies as far away from the others as possible; in these circumstances the value of an indexing system may be expressible as a function of the density of the object space; in particular, retrieval performance may correlate inversely with space density. An approach based on space density computations is used to choose an optimum indexing vocabulary for a collection of documents. Typical evaluation results are shown, demonstating the usefulness of the model.},
  keywords = {automatic indexing,automatic information retrieval,basic_vector_model,content analysis,document space},
  file = {/Users/nvk/Zotero/storage/55T8SXQ8/Salton et al. - 1975 - A vector space model for automatic indexing.pdf}
}

@article{sangatiIncrementalTreeSubstitution2013,
  title = {Incremental {{Tree Substitution Grammar}} for {{Parsing}} and {{Sentence Prediction}}},
  author = {Sangati, Federico and Keller, Frank},
  year = {2013},
  month = dec,
  journal = {Transactions of the Association for Computational Linguistics},
  volume = {1},
  pages = {111--124},
  issn = {2307-387X},
  doi = {10.1162/tacl_a_00214},
  abstract = {In this paper, we present the first incremental parser for Tree Substitution Grammar (TSG). A TSG allows arbitrarily large syntactic fragments to be combined into complete trees; we show how constraints (including lexicalization) can be imposed on the shape of the TSG fragments to enable incremental processing. We propose an efficient Earley-based algorithm for incremental TSG parsing and report an F-score competitive with other incremental parsers. In addition to whole-sentence F-score, we also evaluate the partial trees that the parser constructs for sentence prefixes; partial trees play an important role in incremental interpretation, language modeling, and psycholinguistics. Unlike existing parsers, our incremental TSG parser can generate partial trees that include predictions about the upcoming words in a sentence. We show that it outperforms an n-gram model in predicting more than one upcoming word.},
  langid = {english},
  file = {/Users/nvk/Zotero/storage/CRC86CAX/Sangati and Keller - 2013 - Incremental Tree Substitution Grammar for Parsing .pdf}
}

@article{scharpfARQMathLabIncubator,
  title = {{{ARQMath Lab}}: {{An Incubator}} for {{Semantic Formula Search}} in {{zbMATH Open}}?},
  author = {Scharpf, Philipp and Schubotz, Moritz and {Greiner-Petter}, Andr{\'e} and Ostendorff, Malte and Teschke, Olaf and Gipp, Bela},
  pages = {26},
  abstract = {The zbMATH database contains more than 4 million bibliographic entries. We aim to provide easy access to these entries. Therefore, we maintain different index structures, including a formula index. To optimize the findability of the entries in our database, we continuously investigate new approaches to satisfy the information needs of our users. We believe that the findings from the ARQMath evaluation will generate new insights into which index structures are most suitable to satisfy mathematical information needs. Search engines, recommender systems, plagiarism checking software, and many other added-value services acting on databases such as the arXiv and zbMATH need to combine natural and formula language. One initial approach to address this challenge is to enrich the mostly unstructured document data via Entity Linking. The ARQMath Task at CLEF 2020 aims to tackle the problem of linking newly posted questions from Math Stack Exchange (MSE) to existing ones that were already answered by the community. To deeply understand MSE information needs, answer-, and formula types, we performed manual runs for tasks 1 and 2. Furthermore, we explored several formula retrieval methods: For task 2, such as fuzzy string search, k-nearest neighbors, and our recently introduced approach to retrieve Mathematical Objects of Interest (MOI) with textual search queries. The task results show that neither our automated methods nor our manual runs archived good scores in the competition. However, the perceived quality of the hits returned by the MOI search particularly motivates us to conduct further research about MOI.},
  langid = {english},
  file = {/Users/nvk/Zotero/storage/YIGGTIT5/Scharpf et al. - ARQMath Lab An Incubator for Semantic Formula Sea.pdf}
}

@inproceedings{schellenbergLayoutbasedSubstitutionTree2012,
  title = {Layout-Based Substitution Tree Indexing and Retrieval for Mathematical Expressions},
  booktitle = {{{IS}}\&{{T}}/{{SPIE Electronic Imaging}}},
  author = {Schellenberg, Thomas and Yuan, Bo and Zanibbi, Richard},
  editor = {{Viard-Gaudin}, Christian and Zanibbi, Richard},
  year = {2012},
  month = jan,
  pages = {82970I},
  address = {{Burlingame, California, United States}},
  doi = {10.1117/12.912502},
  abstract = {We introduce a new system for layout-based (LATEX) indexing and retrieving mathematical expressions using substitution trees. Substitution trees can efficiently store and find expressions based on the similarity of their symbols, symbol layout, sub-expressions and size. We describe our novel design and some of our contributions to the substitution tree indexing and retrieval algorithms. We provide an experiment testing our system against the TF-IDF keyword-based system of Zanibbi and Yuan and demonstrate that, in many cases, the quality of search results returned by both systems are comparable (overall means, substitution tree vs. keyword-based: 100\% vs. 89\% for top 1; 48\% vs. 51\% for top 5; 22\% vs. 28\% for top 20). Overall, we present a promising first attempt at layout-based substitution tree indexing and retrieval for mathematical expressions and believe that this method will prove beneficial to the field of mathematical information retrieval as a whole.},
  langid = {english},
  file = {/Users/nvk/Zotero/storage/FXR9P9QA/Schellenberg et al. - 2012 - Layout-based substitution tree indexing and retrie.pdf}
}

@misc{ScholarPhi,
  title = {{{ScholarPhi}}},
  howpublished = {https://scholarphi.org/},
  file = {/Users/nvk/Zotero/storage/76DS32RX/scholarphi.org.html}
}

@inproceedings{schubotzEvaluatingImprovingExtraction2017,
  title = {Evaluating and {{Improving}} the {{Extraction}} of {{Mathematical Identifier Definitions}}},
  booktitle = {Experimental {{IR Meets Multilinguality}}, {{Multimodality}}, and {{Interaction}}},
  author = {Schubotz, Moritz and Kr{\"a}mer, Leonard and Meuschke, Norman and Hamborg, Felix and Gipp, Bela},
  editor = {Jones, Gareth J.F. and Lawless, S{\'e}amus and Gonzalo, Julio and Kelly, Liadh and Goeuriot, Lorraine and Mandl, Thomas and Cappellato, Linda and Ferro, Nicola},
  year = {2017},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {82--94},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-319-65813-1_7},
  abstract = {Mathematical formulae in academic texts significantly contribute to the overall semantic content of such texts, especially in the fields of Science, Technology, Engineering and Mathematics. Knowing the definitions of the identifiers in mathematical formulae is essential to understand the semantics of the formulae. Similar to the sense-making process of human readers, mathematical information retrieval systems can analyze the text that surrounds formulae to extract the definitions of identifiers occurring in the formulae. Several approaches for extracting the definitions of mathematical identifiers from documents have been proposed in recent years. So far, these approaches have been evaluated using different collections and gold standard datasets, which prevented comparative performance assessments. To facilitate future research on the task of identifier definition extraction, we make three contributions. First, we provide an automated evaluation framework, which uses the dataset and gold standard of the NTCIR-11 Math Retrieval Wikipedia task. Second, we compare existing identifier extraction approaches using the developed evaluation framework. Third, we present a new identifier extraction approach that uses machine learning to combine the well-performing features of previous approaches. The new approach increases the precision of extracting identifier definitions from 17.85\% to 48.60\%, and increases the recall from 22.58\% to 28.06\%. The evaluation framework, the dataset and our source code are openly available at: https://ident.formulasearchengine.com.},
  isbn = {978-3-319-65813-1},
  langid = {english},
  file = {/Users/nvk/Zotero/storage/L275Q8I3/Schubotz et al. - 2017 - Evaluating and Improving the Extraction of Mathema.pdf}
}

@article{schubotzExploringOnebrainBarrier2016,
  title = {Exploring the {{One-brain Barrier}} : A {{Manual Contribution}} to the {{NTCIR-12 MathIR Task}}},
  author = {Schubotz, Moritz},
  year = {2016},
  pages = {9},
  abstract = {This paper compares the search capabilities of a single human brain supported by the text search built into Wikipedia with state-of-the-art math search systems. To achieve this, we compare results of manual Wikipedia searches with the aggregated and assessed results of all systems participating in the NTCIR-12 MathIR Wikipedia Task. For 26 of the 30 topics, the average relevance score of our manually retrieved results exceeded the average relevance score of other participants by more than one standard deviation. However, math search engines at large achieved better recall and retrieved highly relevant results that our `single-brain system' missed for 12 topics. By categorizing the topics of NTCIR-12 into six types of queries, we observe a particular strength of math search engines to answer queries of the types `definition lookup' and `application look-up'. However, we see the low precision of current math search engines as the main challenge that prevents their wide-spread adoption in STEM research. By combining our results with highly relevant results of all other participants, we compile a new gold standard dataset and a dataset of duplicate content items. We discuss how the two datasets can be used to improve the query formulation and content augmentation capabilities of match search engines in the future.},
  langid = {english},
  file = {/Users/nvk/Zotero/storage/XBBV9XE7/Schubotz - 2016 - Exploring the One-brain Barrier  a Manual Contrib.pdf}
}

@inproceedings{schubotzImprovingRepresentationConversion2018,
  title = {Improving the {{Representation}} and {{Conversion}} of {{Mathematical Formulae}} by {{Considering}} Their {{Textual Context}}},
  booktitle = {Proceedings of the 18th {{ACM}}/{{IEEE}} on {{Joint Conference}} on {{Digital Libraries}}},
  author = {Schubotz, Moritz and {Greiner-Petter}, Andr{\'e} and Scharpf, Philipp and Meuschke, Norman and Cohl, Howard S. and Gipp, Bela},
  year = {2018},
  month = may,
  pages = {233--242},
  publisher = {{ACM}},
  address = {{Fort Worth Texas USA}},
  doi = {10.1145/3197026.3197058},
  abstract = {Mathematical formulae represent complex semantic information in a concise form. Especially in Science, Technology, Engineering, and Mathematics, mathematical formulae are crucial to communicate information, e.g., in scientific papers, and to perform computations using computer algebra systems. Enabling computers to access the information encoded in mathematical formulae requires machine-readable formats that can represent both the presentation and content, i.e., the semantics, of formulae. Exchanging such information between systems additionally requires conversion methods for mathematical representation formats. We analyze how the semantic enrichment of formulae improves the format conversion process and show that considering the textual context of formulae reduces the error rate of such conversions. Our main contributions are: (1) providing an openly available benchmark dataset for the mathematical format conversion task consisting of a newly created test collection, an extensive, manually curated gold standard and task-specific evaluation metrics; (2) performing a quantitative evaluation of state-of-the-art tools for mathematical format conversions; (3) presenting a new approach that considers the textual context of formulae to reduce the error rate for mathematical format conversions. Our benchmark dataset facilitates future research on mathematical format conversions as well as research on many problems in mathematical information retrieval. Because we annotated and linked all components of formulae, e.g., identifiers, operators and other entities, to Wikidata entries, the gold standard can, for instance, be used to train methods for formula concept discovery and recognition. Such methods can then be applied to improve mathematical information retrieval systems, e.g., for semantic formula search, recommendation of mathematical content, or detection of mathematical plagiarism.},
  isbn = {978-1-4503-5178-2},
  langid = {english},
  file = {/Users/nvk/Zotero/storage/8N4VFKNL/Schubotz et al. - 2018 - Improving the Representation and Conversion of Mat.pdf}
}

@inproceedings{schubotzSemantificationIdentifiersMathematics2016,
  title = {Semantification of {{Identifiers}} in {{Mathematics}} for {{Better Math Information Retrieval}}},
  booktitle = {Proceedings of the 39th {{International ACM SIGIR}} Conference on {{Research}} and {{Development}} in {{Information Retrieval}}},
  author = {Schubotz, Moritz and Grigorev, Alexey and Leich, Marcus and Cohl, Howard S. and Meuschke, Norman and Gipp, Bela and Youssef, Abdou S. and Markl, Volker},
  year = {2016},
  month = jul,
  series = {{{SIGIR}} '16},
  pages = {135--144},
  publisher = {{Association for Computing Machinery}},
  address = {{New York, NY, USA}},
  doi = {10.1145/2911451.2911503},
  abstract = {Mathematical formulae are essential in science, but face challenges of ambiguity, due to the use of a small number of identifiers to represent an immense number of concepts. Corresponding to word sense disambiguation in Natural Language Processing, we disambiguate mathematical identifiers. By regarding formulae and natural text as one monolithic information source, we are able to extract the semantics of identifiers in a process we term Mathematical Language Processing (MLP). As scientific communities tend to establish standard (identifier) notations, we use the document domain to infer the actual meaning of an identifier. Therefore, we adapt the software development concept of namespaces to mathematical notation. Thus, we learn namespace definitions by clustering the MLP results and mapping those clusters to subject classification schemata. In addition, this gives fundamental insights into the usage of mathematical notations in science, technology, engineering and mathematics. Our gold standard based evaluation shows that MLP extracts relevant identifier-definitions. Moreover, we discover that identifier namespaces improve the performance of automated identifier-definition extraction, and elevate it to a level that cannot be achieved within the document context alone.},
  isbn = {978-1-4503-4069-4},
  keywords = {definitions,identifiers,mathematical information retrieval,mathematical knowledge management,mathematical language processing,mathematics,mathoid,mathosphere,MIR,MLP,namespace discovery,wikipedia},
  file = {/Users/nvk/Zotero/storage/DLNMRJIK/Schubotz et al. - 2016 - Semantification of Identifiers in Mathematics for .pdf}
}

@misc{ScienceScienceDiscovery,
  title = {Science of {{Science}}: {{Discovery}}, {{Communication}}, and {{Impact}} ({{SoS}}:{{DCI}})},
  shorttitle = {Science of {{Science}}},
  journal = {Beta site for NSF - National Science Foundation},
  howpublished = {https://beta.nsf.gov/funding/opportunities/science-science-discovery-communication-and-impact-sosdci},
  langid = {english},
  file = {/Users/nvk/Zotero/storage/758NFHLI/science-science-discovery-communication-and-impact-sosdci.html}
}

@misc{ScienceScienceDiscoverya,
  title = {Science of {{Science}}: {{Discovery}}, {{Communication}}, and {{Impact}} ({{SoS}}:{{DCI}})},
  shorttitle = {Science of {{Science}}},
  journal = {Beta site for NSF - National Science Foundation},
  howpublished = {https://beta.nsf.gov/funding/opportunities/science-science-discovery-communication-and-impact-sosdci},
  langid = {english},
  file = {/Users/nvk/Zotero/storage/PTG2B4ZV/science-science-discovery-communication-and-impact-sosdci.html}
}

@article{seeGetPointSummarization2017,
  title = {Get {{To The Point}}: {{Summarization}} with {{Pointer-Generator Networks}}},
  shorttitle = {Get {{To The Point}}},
  author = {See, Abigail and Liu, Peter J. and Manning, Christopher D.},
  year = {2017},
  month = apr,
  journal = {arXiv:1704.04368 [cs]},
  eprint = {1704.04368},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {Neural sequence-to-sequence models have provided a viable new approach for abstractive text summarization (meaning they are not restricted to simply selecting and rearranging passages from the original text). However, these models have two shortcomings: they are liable to reproduce factual details inaccurately, and they tend to repeat themselves. In this work we propose a novel architecture that augments the standard sequence-to-sequence attentional model in two orthogonal ways. First, we use a hybrid pointer-generator network that can copy words from the source text via pointing, which aids accurate reproduction of information, while retaining the ability to produce novel words through the generator. Second, we use coverage to keep track of what has been summarized, which discourages repetition. We apply our model to the CNN / Daily Mail summarization task, outperforming the current abstractive state-of-the-art by at least 2 ROUGE points.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/nvk/Zotero/storage/NXRZRFK7/See et al. - 2017 - Get To The Point Summarization with Pointer-Gener.pdf;/Users/nvk/Zotero/storage/IDBV4PJF/1704.html}
}

@article{senWhatModelsLearn2020,
  title = {What Do {{Models Learn}} from {{Question Answering Datasets}}?},
  author = {Sen, Priyanka and Saffari, Amir},
  year = {2020},
  journal = {Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)},
  eprint = {2004.03490},
  eprinttype = {arxiv},
  pages = {2429--2438},
  doi = {10.18653/v1/2020.emnlp-main.190},
  abstract = {While models have reached superhuman performance on popular question answering (QA) datasets such as SQuAD, they have yet to outperform humans on the task of question answering itself. In this paper, we investigate if models are learning reading comprehension from QA datasets by evaluating BERT-based models across five datasets. We evaluate models on their generalizability to out-of-domain examples, responses to missing or incorrect data, and ability to handle question variations. We find that no single dataset is robust to all of our experiments and identify shortcomings in both datasets and evaluation methods. Following our analysis, we make recommendations for building future QA datasets that better evaluate the task of question answering through reading comprehension. We also release code to convert QA datasets to a shared format for easier experimentation at https://github.com/amazon-research/qa-dataset-converter.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/nvk/Zotero/storage/34PA84ZD/Sen and Saffari - 2020 - What do Models Learn from Question Answering Datas.pdf;/Users/nvk/Zotero/storage/KRX5UPWN/2004.html}
}

@article{serrano-guerreroSentimentAnalysisReview2015,
  title = {Sentiment Analysis: {{A}} Review and Comparative Analysis of Web Services},
  shorttitle = {Sentiment Analysis},
  author = {{Serrano-Guerrero}, Jesus and Olivas, Jose A. and Romero, Francisco P. and {Herrera-Viedma}, Enrique},
  year = {2015},
  month = aug,
  journal = {Information Sciences},
  volume = {311},
  pages = {18--38},
  issn = {0020-0255},
  doi = {10.1016/j.ins.2015.03.040},
  abstract = {Sentiment Analysis (SA), also called Opinion Mining, is currently one of the most studied research fields. It aims to analyze people's sentiments, opinions, attitudes, emotions, etc., towards elements such as topics, products, individuals, organizations, and services. Different techniques and software tools are being developed to carry out Sentiment Analysis. The goal of this work is to review and compare some free access web services, analyzing their capabilities to classify and score different pieces of text with respect to the sentiments contained therein. For that purpose, three well-known collections have been used to perform several experiments whose results are shown and commented upon, leading to some interesting conclusions about the capabilities of each analyzed tool.},
  langid = {english},
  keywords = {Rating prediction,Sentiment analysis,Sentiment classification,Web services},
  file = {/Users/nvk/Zotero/storage/MUVRV9P9/Serrano-Guerrero et al. - 2015 - Sentiment analysis A review and comparative analy.pdf;/Users/nvk/Zotero/storage/74TXD5KU/S0020025515002054.html}
}

@misc{SetbasedVectorModel,
  title = {Set-Based Vector Model: {{An}} Efficient Approach for Correlation-Based Ranking: {{ACM Transactions}} on {{Information Systems}}: {{Vol}} 23, {{No}} 4},
  howpublished = {https://dl.acm.org/doi/abs/10.1145/1095872.1095874},
  file = {/Users/nvk/Zotero/storage/GPYRK6IN/1095872.html}
}

@article{sevaEnergyEfficientFPGABasedParallel2017,
  title = {Energy-{{Efficient FPGA-Based Parallel Quasi-Stochastic Computing}}},
  author = {Seva, Ramu and Metku, Prashanthi and Choi, Minsu},
  year = {2017},
  month = dec,
  journal = {Journal of Low Power Electronics and Applications},
  volume = {7},
  number = {4},
  pages = {29},
  publisher = {{Multidisciplinary Digital Publishing Institute}},
  doi = {10.3390/jlpea7040029},
  abstract = {The high performance of FPGA (Field Programmable Gate Array) in image processing applications is justified by its flexible reconfigurability, its inherent parallel nature and the availability of a large amount of internal memories. Lately, the Stochastic Computing (SC) paradigm has been found to be significantly advantageous in certain application domains including image processing because of its lower hardware complexity and power consumption. However, its viability is deemed to be limited due to its serial bitstream processing and excessive run-time requirement for convergence. To address these issues, a novel approach is proposed in this work where an energy-efficient implementation of SC is accomplished by introducing fast-converging Quasi-Stochastic Number Generators (QSNGs) and parallel stochastic bitstream processing, which are well suited to leverage FPGA's reconfigurability and abundant internal memory resources. The proposed approach has been tested on the Virtex-4 FPGA, and results have been compared with the serial and parallel implementations of conventional stochastic computation using the well-known SC edge detection and multiplication circuits. Results prove that by using this approach, execution time, as well as the power consumption are decreased by a factor of     3.5     and     4.5     for the edge detection circuit and multiplication circuit, respectively.},
  copyright = {http://creativecommons.org/licenses/by/3.0/},
  langid = {english},
  keywords = {edge detection,FPGA,quasi-stochastic number generator,reconfigurability,stochastic computing},
  file = {/Users/nvk/Zotero/storage/XUTRR2WN/Seva et al. - 2017 - Energy-Efficient FPGA-Based Parallel Quasi-Stochas.pdf;/Users/nvk/Zotero/storage/3Z9M266I/29.html}
}

@article{shahPredictiveBiasesNatural2020,
  title = {Predictive {{Biases}} in {{Natural Language Processing Models}}: {{A Conceptual Framework}} and {{Overview}}},
  shorttitle = {Predictive {{Biases}} in {{Natural Language Processing Models}}},
  author = {Shah, Deven and Schwartz, H. Andrew and Hovy, Dirk},
  year = {2020},
  journal = {Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics},
  eprint = {1912.11078},
  eprinttype = {arxiv},
  pages = {5248--5264},
  doi = {10.18653/v1/2020.acl-main.468},
  abstract = {An increasing number of works in natural language processing have addressed the effect of bias on the predicted outcomes, introducing mitigation techniques that act on different parts of the standard NLP pipeline (data and models). However, these works have been conducted in isolation, without a unifying framework to organize efforts within the field. This leads to repetitive approaches, and puts an undue focus on the effects of bias, rather than on their origins. Research focused on bias symptoms rather than the underlying origins could limit the development of effective countermeasures. In this paper, we propose a unifying conceptualization: the predictive bias framework for NLP. We summarize the NLP literature and propose a general mathematical definition of predictive bias in NLP along with a conceptual framework, differentiating four main origins of biases: label bias, selection bias, model overamplification, and semantic bias. We discuss how past work has countered each bias origin. Our framework serves to guide an introductory overview of predictive bias in NLP, integrating existing work into a single structure and opening avenues for future research.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/Users/nvk/Zotero/storage/TKQ8NFV5/Shah et al. - 2020 - Predictive Biases in Natural Language Processing M.pdf;/Users/nvk/Zotero/storage/QUJ9I873/1912.html}
}

@article{shefferReviewPrincipiaMathematica1926,
  title = {Review of {{Principia Mathematica}}, {{Whitehead}}, {{Alfred North}}, {{Russell}}, {{Bertrand}}},
  author = {Sheffer, Henry M. and North, Whitehead},
  year = {1926},
  journal = {Isis},
  volume = {8},
  number = {1},
  pages = {226--231},
  publisher = {{[The University of Chicago Press, The History of Science Society]}},
  issn = {0021-1753},
  collaborator = {Bertrand, Russell}
}

@inproceedings{shenBackdoorPretrainedModels2021,
  title = {Backdoor {{Pre-trained Models Can Transfer}} to {{All}}},
  booktitle = {Proceedings of the 2021 {{ACM SIGSAC Conference}} on {{Computer}} and {{Communications Security}}},
  author = {Shen, Lujia and Ji, Shouling and Zhang, Xuhong and Li, Jinfeng and Chen, Jing and Shi, Jie and Fang, Chengfang and Yin, Jianwei and Wang, Ting},
  year = {2021},
  month = nov,
  eprint = {2111.00197},
  eprinttype = {arxiv},
  primaryclass = {cs},
  pages = {3141--3158},
  doi = {10.1145/3460120.3485370},
  abstract = {Pre-trained general-purpose language models have been a dominating component in enabling real-world natural language processing (NLP) applications. However, a pre-trained model with backdoor can be a severe threat to the applications. Most existing backdoor attacks in NLP are conducted in the fine-tuning phase by introducing malicious triggers in the targeted class, thus relying greatly on the prior knowledge of the fine-tuning task. In this paper, we propose a new approach to map the inputs containing triggers directly to a predefined output representation of the pre-trained NLP models, e.g., a predefined output representation for the classification token in BERT, instead of a target label. It can thus introduce backdoor to a wide range of downstream tasks without any prior knowledge. Additionally, in light of the unique properties of triggers in NLP, we propose two new metrics to measure the performance of backdoor attacks in terms of both effectiveness and stealthiness. Our experiments with various types of triggers show that our method is widely applicable to different fine-tuning tasks (classification and named entity recognition) and to different models (such as BERT, XLNet, BART), which poses a severe threat. Furthermore, by collaborating with the popular online model repository Hugging Face, the threat brought by our method has been confirmed. Finally, we analyze the factors that may affect the attack performance and share insights on the causes of the success of our backdoor attack.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Cryptography and Security,Computer Science - Machine Learning},
  file = {/Users/nvk/Zotero/storage/SSFGFDPW/Shen et al. - 2021 - Backdoor Pre-trained Models Can Transfer to All.pdf;/Users/nvk/Zotero/storage/6UAHCJHQ/2111.html}
}

@article{shenDiSANDirectionalSelfAttention2017,
  title = {{{DiSAN}}: {{Directional Self-Attention Network}} for {{RNN}}/{{CNN-Free Language Understanding}}},
  shorttitle = {{{DiSAN}}},
  author = {Shen, Tao and Zhou, Tianyi and Long, Guodong and Jiang, Jing and Pan, Shirui and Zhang, Chengqi},
  year = {2017},
  month = nov,
  journal = {arXiv:1709.04696 [cs]},
  eprint = {1709.04696},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {Recurrent neural nets (RNN) and convolutional neural nets (CNN) are widely used on NLP tasks to capture the long-term and local dependencies, respectively. Attention mechanisms have recently attracted enormous interest due to their highly parallelizable computation, significantly less training time, and flexibility in modeling dependencies. We propose a novel attention mechanism in which the attention between elements from input sequence(s) is directional and multi-dimensional (i.e., feature-wise). A light-weight neural net, "Directional Self-Attention Network (DiSAN)", is then proposed to learn sentence embedding, based solely on the proposed attention without any RNN/CNN structure. DiSAN is only composed of a directional self-attention with temporal order encoded, followed by a multi-dimensional attention that compresses the sequence into a vector representation. Despite its simple form, DiSAN outperforms complicated RNN models on both prediction quality and time efficiency. It achieves the best test accuracy among all sentence encoding methods and improves the most recent best result by 1.02\% on the Stanford Natural Language Inference (SNLI) dataset, and shows state-of-the-art test accuracy on the Stanford Sentiment Treebank (SST), Multi-Genre natural language inference (MultiNLI), Sentences Involving Compositional Knowledge (SICK), Customer Review, MPQA, TREC question-type classification and Subjectivity (SUBJ) datasets.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {/Users/nvk/Zotero/storage/4PNQXKLY/Shen et al. - 2017 - DiSAN Directional Self-Attention Network for RNN.pdf;/Users/nvk/Zotero/storage/BNUCU3LU/1709.html}
}

@article{shenMathBERTPretrainedLanguage2021,
  title = {{{MathBERT}}: {{A Pre-trained Language Model}} for {{General NLP Tasks}} in {{Mathematics Education}}},
  shorttitle = {{{MathBERT}}},
  author = {Shen, Jia Tracy and Yamashita, Michiharu and Prihar, Ethan and Heffernan, Neil and Wu, Xintao and Graff, Ben and Lee, Dongwon},
  year = {2021},
  month = sep,
  journal = {arXiv:2106.07340 [cs]},
  eprint = {2106.07340},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {Since the introduction of the original BERT (i.e., BASE BERT), researchers have developed various customized BERT models with improved performance for specific domains and tasks by exploiting the benefits of transfer learning. Due to the nature of mathematical texts, which often use domain specific vocabulary along with equations and math symbols, we posit that the development of a new BERT model for mathematics would be useful for many mathematical downstream tasks. In this resource paper, we introduce our multi-institutional effort (i.e., two learning platforms and three academic institutions in the US) toward this need: MathBERT, a model created by pre-training the BASE BERT model on a large mathematical corpus ranging from pre-kindergarten (pre-k), to high-school, to college graduate level mathematical content. In addition, we select three general NLP tasks that are often used in mathematics education: prediction of knowledge component, auto-grading open-ended Q\&A, and knowledge tracing, to demonstrate the superiority of MathBERT over BASE BERT. Our experiments show that MathBERT outperforms prior best methods by 1.2-22\% and BASE BERT by 2-8\% on these tasks. In addition, we build a mathematics specific vocabulary `mathVocab' to train with MathBERT. We discover that MathBERT pre-trained with `mathVocab' outperforms MathBERT trained with the BASE BERT vocabulary (i.e., `origVocab'). MathBERT is currently being adopted at the participated leaning platforms: Stride, Inc, a commercial educational resource provider, and ASSISTments.org, a free online educational platform. We release MathBERT for public usage at: https://github.com/tbs17/MathBERT.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {/Users/nvk/Zotero/storage/REJ2FSAE/Shen et al. - 2021 - MathBERT A Pre-trained Language Model for General.pdf}
}

@inproceedings{shivaprasadSentimentAnalysisProduct2017,
  title = {Sentiment Analysis of Product Reviews: {{A}} Review},
  shorttitle = {Sentiment Analysis of Product Reviews},
  booktitle = {2017 {{International Conference}} on {{Inventive Communication}} and {{Computational Technologies}} ({{ICICCT}})},
  author = {Shivaprasad, T. K. and Shetty, Jyothi},
  year = {2017},
  month = mar,
  pages = {298--301},
  doi = {10.1109/ICICCT.2017.7975207},
  abstract = {Now a day's internet is the most valuable source of learning, getting ideas, reviews for a product or a service. Everyday millions of reviews are generated in the internet about a product, person or a place. Because of their huge number and size it is very difficult to handle and understand such reviews. Sentiment analysis is such a research area which understands and extracts the opinion from the given review and the analysis process includes natural language processing (NLP), computational linguistics, text analytics and classifying the polarity of the opinion. In the field of sentiment analysis there are many algorithms exist to tackle NLP problems. Each algorithm is used by several applications. In this paper we have shown the taxonomy of various sentiment analysis methods. This paper also shows that Support vector machine (SVM) gives high accuracy compared to Na\"ive bayes and maximum entropy methods.},
  keywords = {Classification algorithms,computational linguistics,Dictionaries,Feature extraction,Internet,learning (artificial intelligence),maximum entropy methods,Naive Bayes,natural language processing,Natural language processing,NLP problems,Opinion mining,pattern classification,product review sentiment analysis,Product reviews,Semantics,sentiment analysis,Sentiment analysis,Supervised learning,Support vector machine,support vector machines,SVM,text analytics},
  file = {/Users/nvk/Zotero/storage/NZJR3F6X/Shivaprasad and Shetty - 2017 - Sentiment analysis of product reviews A review.pdf;/Users/nvk/Zotero/storage/IJ9UVP6X/7975207.html}
}

@misc{SimpleBM25Extension,
  title = {Simple {{BM25}} Extension to Multiple Weighted Fields | {{Proceedings}} of the Thirteenth {{ACM}} International Conference on {{Information}} and Knowledge Management},
  howpublished = {https://dl.acm.org/doi/abs/10.1145/1031171.1031181?casa\_token=6qP2UEe9pHoAAAAA:haB5pSRJqKGZDalhHgvI4fA2IZEw9Z88nfXxBgUQJRU9Afzi9r1gdFJawnry4t7a3tvIzoL6yl-HKg},
  file = {/Users/nvk/Zotero/storage/5PHG7ECA/1031171.html}
}

@inproceedings{singhReviewNearMemoryComputing2018,
  title = {A {{Review}} of {{Near-Memory Computing Architectures}}: {{Opportunities}} and {{Challenges}}},
  shorttitle = {A {{Review}} of {{Near-Memory Computing Architectures}}},
  booktitle = {2018 21st {{Euromicro Conference}} on {{Digital System Design}} ({{DSD}})},
  author = {Singh, Gagandeep and Chelini, Lorenzo and Corda, Stefano and Javed Awan, Ahsan and Stuijk, Sander and Jordans, Roel and Corporaal, Henk and Boonstra, Albert-Jan},
  year = {2018},
  month = aug,
  pages = {608--617},
  doi = {10.1109/DSD.2018.00106},
  abstract = {The conventional approach of moving stored data to the CPU for computation has become a major performance bottleneck for emerging scale-out data-intensive applications due to their limited data reuse. At the same time, the advancement in integration technologies have made the decade-old concept of coupling compute units close to the memory (called Near-Memory Computing) more viable. Processing right at the "home" of data can completely diminish the data movement problem of data-intensive applications. This paper focuses on analyzing and organizing the extensive body of literature on near-memory computing across various dimensions: starting from the memory level where this paradigm is applied, to the granularity of the application that could be executed on the near-memory units. We highlight the challenges as well as the critical need of evaluation methodologies that can be employed in designing these special architectures. Using a case study, we present our methodology and also identify topics for future research to unlock the full potential of near-memory computing.},
  keywords = {application characterization,Central Processing Unit,Coherence,computer architecture,CPU,data centric computing,data movement problem,data-intensive applications,digital storage,evaluation methodologies,Graphics processing units,memory architecture,memory level,Memory management,modeling,near data processing,near-memory computing,near-memory computing architectures,near-memory units,processing in memory,Programming,stored data,survey,Tools},
  file = {/Users/nvk/Zotero/storage/CFV5TPII/Singh et al. - 2018 - A Review of Near-Memory Computing Architectures O.pdf;/Users/nvk/Zotero/storage/CIQC5P5M/8491877.html}
}

@article{smithiesEquationEntryEditing2001,
  title = {Equation Entry and Editing via Handwriting and Gesture Recognition},
  author = {Smithies, Steve and Novins, Kevin and Arvo, James},
  year = {2001},
  month = jan,
  journal = {Behaviour \& Information Technology},
  volume = {20},
  number = {1},
  pages = {53--67},
  issn = {0144-929X, 1362-3001},
  doi = {10.1080/01449290010020657},
  abstract = {We describe a system for freehand entry and editing of mathematical expressions using a pen and tablet. The expressions are entered in the same way that they would be written on paper. The system interprets the results and generates output in a form suitable for use in other applications, such as word processors or symbolic manipulators. Interpretation includes character segmentation, character recognition, and formula parsing. Our interface incorporates easy to use tools for correcting interpretation errors at any stage. The user can also edit the handwritten representation and ask the system to reinterpret the results. By recovering the formula's structure directly from its handwritten form, the user is free to use common conventions of mathematical notation without regard to internal representation. We report the results of a small user study, which indicate that the new style of interaction is e\OE ective.},
  langid = {english},
  file = {/Users/nvk/Zotero/storage/8JWI4ND4/Smithies et al. - 2001 - Equation entry and editing via handwriting and ges.pdf}
}

@inproceedings{sojkaArtMathematicsRetrieval2011,
  title = {The Art of Mathematics Retrieval},
  booktitle = {Proceedings of the 11th {{ACM}} Symposium on {{Document}} Engineering - {{DocEng}} '11},
  author = {Sojka, Petr and L{\'i}aka, Martin},
  year = {2011},
  pages = {57},
  publisher = {{ACM Press}},
  address = {{Mountain View, California, USA}},
  doi = {10.1145/2034691.2034703},
  abstract = {The design and architecture of MIaS (Math Indexer and Searcher), a system for mathematics retrieval is presented, and design decisions are discussed. We argue for an approach based on Presentation MathML using a similarity of math subformulae. The system was implemented as a math-aware search engine based on the state-ofthe-art system Apache Lucene. Scalability issues were checked against more than 400,000 arXiv documents with 158 million mathematical formulae. Almost three billion MathML subformulae were indexed using a Solr-compatible Lucene.},
  isbn = {978-1-4503-0863-2},
  langid = {english},
  file = {/Users/nvk/Zotero/storage/WVNVV5NC/Sojka and LÃ­aka - 2011 - The art of mathematics retrieval.pdf}
}

@inproceedings{sojkaIndexingSearchingMathematics2011,
  title = {Indexing and {{Searching Mathematics}} in~{{Digital~Libraries}}},
  booktitle = {Intelligent {{Computer Mathematics}}},
  author = {Sojka, Petr and L{\'i}{\v s}ka, Martin},
  year = {2011},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {228--243},
  publisher = {{Springer}},
  address = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-642-22673-1_16},
  abstract = {This paper surveys approaches and systems for searching mathematical formulae in mathematical corpora and on the web. The design and architecture of our MIaS (Math Indexer and Searcher) system is presented, and our design decisions are discussed in detail. An approach based on Presentation MathML using a similarity of math subformulae is suggested and verified by implementing it as a math-aware search engine based on the state-of-the-art system, Apache Lucene.Scalability issues were checked based on 324,000 real scientific documents from arXiv archive with 112 million mathematical formulae. More than two billions MathML subformulae were indexed using our Solr-compatible Lucene extension.},
  isbn = {978-3-642-22673-1},
  langid = {english},
  keywords = {document ranking of mathematical papers,information retrieval,information systems,math indexing and retrieval,math text mining,mathematical content search,mathematical digital libraries,MIaS,WebMIaS},
  file = {/Users/nvk/Zotero/storage/QFXQL34X/Sojka and LÃ­Å¡ka - 2011 - Indexing and Searching Mathematics inÂ DigitalÂ Libr.pdf}
}

@inproceedings{sojkaMIaSMathAwareRetrieval2018,
  title = {{{MIaS}}: {{Math-Aware Retrieval}} in {{Digital Mathematical Libraries}}},
  shorttitle = {{{MIaS}}},
  booktitle = {Proceedings of the 27th {{ACM International Conference}} on {{Information}} and {{Knowledge Management}}},
  author = {Sojka, Petr and R{\r{u}}{\v z}i{\v c}ka, Michal and Novotn{\'y}, V{\'i}t},
  year = {2018},
  month = oct,
  pages = {1923--1926},
  publisher = {{ACM}},
  address = {{Torino Italy}},
  doi = {10.1145/3269206.3269233},
  abstract = {Digital mathematical libraries (DMLs) such as arXiv, Numdam, and EuDML contain mainly documents from STEM fields, where mathematical formulae are often more important than text for understanding. Conventional information retrieval (IR) systems are unable to represent formulae and they are therefore ill-suited for math information retrieval (MIR). To fill the gap, we have developed, and open-sourced the MIaS MIR system. MIaS is based on the fulltext search engine Apache Lucene. On top of text retrieval, MIaS also incorporates a set of tools for preprocessing mathematical formulae. We describe the design of the system and present speed, and quality evaluation results. We show that MIaS is both efficient, and effective, as evidenced by our victory in the NTCIR-11 Math-2 task.},
  isbn = {978-1-4503-6014-2},
  langid = {english},
  file = {/Users/nvk/Zotero/storage/7WBA3DLX/Sojka et al. - 2018 - MIaS Math-Aware Retrieval in Digital Mathematical.pdf}
}

@incollection{songAutoencoderBasedData2013,
  title = {Auto-Encoder {{Based Data Clustering}}},
  booktitle = {Progress in {{Pattern Recognition}}, {{Image Analysis}}, {{Computer Vision}}, and {{Applications}}},
  author = {Song, Chunfeng and Liu, Feng and Huang, Yongzhen and Wang, Liang and Tan, Tieniu},
  editor = {Hutchison, David and Kanade, Takeo and Kittler, Josef and Kleinberg, Jon M. and Mattern, Friedemann and Mitchell, John C. and Naor, Moni and Nierstrasz, Oscar and Pandu Rangan, C. and Steffen, Bernhard and Sudan, Madhu and Terzopoulos, Demetri and Tygar, Doug and Vardi, Moshe Y. and Weikum, Gerhard and {Ruiz-Shulcloper}, Jos{\'e} and {Sanniti di Baja}, Gabriella},
  year = {2013},
  volume = {8258},
  pages = {117--124},
  publisher = {{Springer Berlin Heidelberg}},
  address = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-642-41822-8_15},
  abstract = {Linear or non-linear data transformations are widely used processing techniques in clustering. Usually, they are beneficial to enhancing data representation. However, if data have a complex structure, these techniques would be unsatisfying for clustering. In this paper, based on the auto-encoder network, which can learn a highly non-linear mapping function, we propose a new clustering method. Via simultaneously considering data reconstruction and compactness, our method can obtain stable and effective clustering. Experiments on three databases show that the proposed clustering model achieves excellent performance in terms of both accuracy and normalized mutual information.},
  isbn = {978-3-642-41821-1 978-3-642-41822-8},
  langid = {english},
  file = {/Users/nvk/Zotero/storage/QFV8K8PF/Song et al. - 2013 - Auto-encoder Based Data Clustering.pdf}
}

@inproceedings{songPipeLayerPipelinedReRAMBased2017,
  title = {{{PipeLayer}}: {{A Pipelined ReRAM-Based Accelerator}} for {{Deep Learning}}},
  shorttitle = {{{PipeLayer}}},
  booktitle = {2017 {{IEEE International Symposium}} on {{High Performance Computer Architecture}} ({{HPCA}})},
  author = {Song, Linghao and Qian, Xuehai and Li, Hai and Chen, Yiran},
  year = {2017},
  month = feb,
  pages = {541--552},
  issn = {2378-203X},
  doi = {10.1109/HPCA.2017.55},
  abstract = {Convolutional neural networks (CNNs) are the heart of deep learning applications. Recent works PRIME [1] and ISAAC [2] demonstrated the promise of using resistive random access memory (ReRAM) to perform neural computations in memory. We found that training cannot be efficiently supported with the current schemes. First, they do not consider weight update and complex data dependency in training procedure. Second, ISAAC attempts to increase system throughput with a very deep pipeline. It is only beneficial when a large number of consecutive images can be fed into the architecture. In training, the notion of batch (e.g. 64) limits the number of images can be processed consecutively, because the images in the next batch need to be processed based on the updated weights. Third, the deep pipeline in ISAAC is vulnerable to pipeline bubbles and execution stall. In this paper, we present PipeLayer, a ReRAM-based PIM accelerator for CNNs that support both training and testing. We analyze data dependency and weight update in training algorithms and propose efficient pipeline to exploit inter-layer parallelism. To exploit intra-layer parallelism, we propose highly parallel design based on the notion of parallelism granularity and weight replication. With these design choices, PipeLayer enables the highly pipelined execution of both training and testing, without introducing the potential stalls in previous work. The experiment results show that, PipeLayer achieves the speedup of 42.45x compared with GPU platform on average. The average energy saving of PipeLayer compared with GPU implementation is 7.17x.},
  keywords = {CNNs,complex data dependency,Computer architecture,convolutional neural networks,data analysis,deep learning applications,GPU implementation,GPU platform,graphics processing units,inter-layer parallelism,ISAAC,Kernel,learning (artificial intelligence),Machine learning,neural memory computations,neural nets,Neural networks,PipeLayer,pipeline bubbles,pipeline processing,pipelined ReRAM-based accelerator,Pipelines,PRIME,ReRAM-based PIM accelerator,resistive RAM,resistive random access memory,Testing,Training,training algorithms and,weight update},
  file = {/Users/nvk/Zotero/storage/7SE5N2BW/Song et al. - 2017 - PipeLayer A Pipelined ReRAM-Based Accelerator for.pdf;/Users/nvk/Zotero/storage/G8WA2U3H/7920854.html}
}

@article{sparckjonesSTATISTICALINTERPRETATIONTERM1972,
  title = {A {{STATISTICAL INTERPRETATION OF TERM SPECIFICITY AND ITS APPLICATION IN RETRIEVAL}}},
  author = {SPARCK JONES, KAREN},
  year = {1972},
  month = jan,
  journal = {Journal of Documentation},
  volume = {28},
  number = {1},
  pages = {11--21},
  publisher = {{MCB UP Ltd}},
  issn = {0022-0418},
  doi = {10.1108/eb026526},
  abstract = {The exhaustivity of document descriptions and the specificity of index terms are usually regarded as independent. It is suggested that specificity should be interpreted statistically, as a function of term use rather than of term meaning. The effects on retrieval of variations in term specificity are examined, experiments with three test collections showing in particular that frequently-occurring terms are required for good overall performance. It is argued that terms should be weighted according to collection frequency, so that matches on less frequent, more specific, terms are of greater value than matches on frequent terms. Results for the test collections show that considerable improvements in performance are obtained with this very simple procedure.},
  keywords = {basic_vector_model},
  file = {/Users/nvk/Zotero/storage/H7XXCKDN/html.html}
}

@misc{SpecialProgramsUniversity,
  title = {Special {{Programs}} | {{University}} of {{Illinois}}},
  journal = {Small equipment competition},
  howpublished = {https://specialprograms.research.illinois.edu/Program.aspx?ProgramGuid=1D0DAEB7-9C77-42B7-9115-4135F619B791\#piReq},
  file = {/Users/nvk/Zotero/storage/ASK9IX4H/Program.html}
}

@misc{SpeechLanguageProcessing,
  title = {Speech and {{Language Processing}}},
  howpublished = {https://web.stanford.edu/\textasciitilde jurafsky/slp3/},
  file = {/Users/nvk/Zotero/storage/A8QA24TZ/slp3.html}
}

@inproceedings{srihariModelMultimodalInformation2000,
  title = {A Model for Multimodal Information Retrieval},
  booktitle = {2000 {{IEEE International Conference}} on {{Multimedia}} and {{Expo}}. {{ICME2000}}. {{Proceedings}}. {{Latest Advances}} in the {{Fast Changing World}} of {{Multimedia}} ({{Cat}}. {{No}}.{{00TH8532}})},
  author = {Srihari, R.K. and Rao, A. and Han, B. and Munirathnam, S. and Wu, Xiaoyun},
  year = {2000},
  month = jul,
  volume = {2},
  pages = {701-704 vol.2},
  doi = {10.1109/ICME.2000.871458},
  abstract = {Finding useful information from large multimodal document collections such as the WWW without encountering numerous false positives poses a challenge to multimodal information retrieval systems (MMIR). A general model for multimodal information retrieval is proposed by which a user's information need is expressed through composite, multimodal queries, and the most appropriate weighted combination of indexing techniques is determined by a machine learning approach in order to best satisfy the information need. The focus is on improving precision and recall in a MMIR system by optimally combining text and image similarity. Experiments are presented which demonstrate the utility of individual indexing systems in improving overall average precision.},
  keywords = {Content based retrieval,Database languages,Feedback,Image retrieval,Indexing,Information retrieval,Logic,Machine learning,Utility theory,World Wide Web}
}

@inproceedings{stalnakerMathExpressionRetrieval2015,
  title = {Math Expression Retrieval Using an Inverted Index over Symbol Pairs},
  booktitle = {{{IS}}\&{{T}}/{{SPIE Electronic Imaging}}},
  author = {Stalnaker, David and Zanibbi, Richard},
  year = {2015},
  month = feb,
  pages = {940207},
  address = {{San Francisco, California, USA}},
  doi = {10.1117/12.2074084},
  abstract = {We introduce a new method for indexing and retrieving mathematical expressions, and a new protocol for evaluating math formula retrieval systems. The Tangent search engine uses an inverted index over pairs of symbols in math expressions. Each key in the index is a pair of symbols along with their relative distance and vertical displacement within an expression. Matched expressions are ranked by the harmonic mean of the percentage of symbol pairs matched in the query, and the percentage of symbol pairs matched in the candidate expression. We have found that our method is fast enough for use in real time and finds partial matches well, such as when subexpressions are re-arranged (e.g. expressions moved from the left to the right of an equals sign) or when individual symbols (e.g. variables) differ from a query expression. In an experiment using expressions from English Wikipedia, student and faculty participants (N=20) found expressions returned by Tangent significantly more similar than those from a text-based retrieval system (Lucene) adapted for mathematical expressions. Participants provided similarity ratings using a 5-point Likert scale, evaluating expressions from both algorithms one-at-a-time in a randomized order to avoid bias from the position of hits in search result lists. For the Lucenebased system, precision for the top 1 and 10 hits averaged 60\% and 39\% across queries respectively, while for Tangent mean precision at 1 and 10 were 99\% and 60\%. A demonstration and source code are publicly available.},
  langid = {english},
  file = {/Users/nvk/Zotero/storage/SYSQJWXM/Stalnaker and Zanibbi - 2015 - Math expression retrieval using an inverted index .pdf}
}

@article{steffenNGramLanguageModeling,
  title = {N-{{Gram Language Modeling}} for {{Robust Multi-Lingual Document Classification}}},
  author = {Steffen, Jorg},
  pages = {4},
  abstract = {Statistical n-gram language modeling is used in many domains like speech recognition, language identification, machine translation, character recognition and topic classification. Most language modeling approaches work on n-grams of terms. This paper reports about ongoing research in the MEMPHIS project which employs models based on character-level n-grams instead of term n-grams. The models are used for the multi-lingual classification of documents according to the topics of the MEMPHIS domains. We present methods capable of dealing robustly with large vocabularies and informal, erroneous texts in different languages. We also report on our results of using multi-lingual language models and experimenting with different classification parameters like smoothing techniques and n-grams lengths.},
  langid = {english},
  file = {/Users/nvk/Zotero/storage/E6K452NV/Steffen - N-Gram Language Modeling for Robust Multi-Lingual .pdf}
}

@article{steinbuchLearningMatricesTheir1963,
  title = {Learning Matrices and Their Applications},
  author = {Steinbuch, K. and Piske, U. A. W.},
  year = {1963},
  month = dec,
  journal = {IEEE Transactions on Electronic Computers},
  volume = {EC-12},
  number = {6},
  pages = {846--862},
  issn = {0367-7508},
  doi = {10.1109/PGEC.1963.263588},
  abstract = {The paper gives a survey of the learning circuits which became known as learning matrices and some of their possible technological applications. The first section describes the principle of learning matrices. So-called conditioned connections between the characteristics of an object and the meaning of an object are formed in the learning phase. During the operation of connecting the characteristics of an object with its meaning (EB operation of the knowing phase) upon presenting the object characteristics, the associated most similar meaning is realized in the form of a signal by maximum likelihood decoding. Conversely, in operation from the meaning of an object to its characteristics (BE operation) the associated object characteristics are obtained as signals by parallel reading upon application of an object meaning. According to the characteristic signals processed (binary or analog signals) discrimination must be made between binary and nonbinary learning matrices. In the case of the binary learning matrix the conditioned connections are a statistical measure for the frequency of the coordination of object characteristics and object meaning, in the case of the nonbinary learning matrix they are a measure for an analog value proportional to a characteristic. Both types of matrices allow for the characteristic sets applied during EB operation to be unsystematically disturbed within limits. Moreover, the nonbinary learning matrix is invariant to systematic deviations between presented and learned characteristic sets (invariance to affine transformation, translation and rotated skewness).},
  keywords = {Automatic control,Circuits,Filters,Frequency measurement,Machine learning,National electric code,Signal processing,Speech analysis,Speech processing,Speech recognition},
  file = {/Users/nvk/Zotero/storage/RGNSZ8SR/Steinbuch and Piske - 1963 - Learning matrices and their applications.pdf;/Users/nvk/Zotero/storage/YM8HHXGF/4038032.html}
}

@article{strickerJavaProgrammingDevelopers1994,
  title = {Java Programming for {{C}}/{{C}}++ Developers},
  author = {Stricker, Scott},
  year = {1994},
  pages = {47},
  langid = {english},
  file = {/Users/nvk/Zotero/storage/NKYRTICS/Stricker - 1994 - Java programming for CC++ developers.pdf}
}

@inproceedings{sutskeverSequenceSequenceLearning2014,
  title = {Sequence to {{Sequence Learning}} with {{Neural Networks}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Sutskever, Ilya and Vinyals, Oriol and Le, Quoc V},
  year = {2014},
  volume = {27},
  publisher = {{Curran Associates, Inc.}},
  abstract = {Deep Neural Networks (DNNs) are powerful models that have achieved excellent performance on difficult learning tasks. Although DNNs work well whenever large labeled training sets are available, they cannot be used to map sequences to sequences. In this paper, we present a general end-to-end approach to sequence learning that makes minimal assumptions on the sequence structure. Our method uses a multilayered Long Short-Term Memory (LSTM) to map the input sequence to a vector of a fixed dimensionality, and then another deep LSTM to decode the target sequence from the vector. Our main result is that on an English to French translation task from the WMT-14 dataset, the translations produced by the LSTM achieve a BLEU score of 34.8 on the entire test set, where the LSTM's BLEU score was penalized on out-of-vocabulary words. Additionally, the LSTM did not have difficulty on long sentences. For comparison, a phrase-based SMT system achieves a BLEU score of 33.3 on the same dataset. When we used the LSTM to rerank the 1000 hypotheses produced by the aforementioned SMT system, its BLEU score increases to 36.5, which is close to the previous state of the art. The LSTM also learned sensible phrase and sentence representations that are sensitive to word order and are relatively invariant to the active and the passive voice. Finally, we found that reversing the order of the words in all source sentences (but not target sentences) improved the LSTM's performance markedly, because doing so introduced many short term dependencies between the source and the target sentence which made the optimization problem easier.},
  file = {/Users/nvk/Zotero/storage/JM8DEZP8/Sutskever et al. - 2014 - Sequence to Sequence Learning with Neural Networks.pdf}
}

@misc{sutskeverSequenceSequenceLearning2014a,
  title = {Sequence to {{Sequence Learning}} with {{Neural Networks}}},
  author = {Sutskever, Ilya and Vinyals, Oriol and Le, Quoc V.},
  year = {2014},
  month = dec,
  number = {arXiv:1409.3215},
  eprint = {1409.3215},
  eprinttype = {arxiv},
  primaryclass = {cs},
  institution = {{arXiv}},
  abstract = {Deep Neural Networks (DNNs) are powerful models that have achieved excellent performance on difficult learning tasks. Although DNNs work well whenever large labeled training sets are available, they cannot be used to map sequences to sequences. In this paper, we present a general end-to-end approach to sequence learning that makes minimal assumptions on the sequence structure. Our method uses a multilayered Long Short-Term Memory (LSTM) to map the input sequence to a vector of a fixed dimensionality, and then another deep LSTM to decode the target sequence from the vector. Our main result is that on an English to French translation task from the WMT'14 dataset, the translations produced by the LSTM achieve a BLEU score of 34.8 on the entire test set, where the LSTM's BLEU score was penalized on out-of-vocabulary words. Additionally, the LSTM did not have difficulty on long sentences. For comparison, a phrase-based SMT system achieves a BLEU score of 33.3 on the same dataset. When we used the LSTM to rerank the 1000 hypotheses produced by the aforementioned SMT system, its BLEU score increases to 36.5, which is close to the previous best result on this task. The LSTM also learned sensible phrase and sentence representations that are sensitive to word order and are relatively invariant to the active and the passive voice. Finally, we found that reversing the order of the words in all source sentences (but not target sentences) improved the LSTM's performance markedly, because doing so introduced many short term dependencies between the source and the target sentence which made the optimization problem easier.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/Users/nvk/Zotero/storage/W68MLIKJ/Sutskever et al. - 2014 - Sequence to Sequence Learning with Neural Networks.pdf;/Users/nvk/Zotero/storage/IJJ65JQM/1409.html}
}

@inproceedings{suzukiINFTYIntegratedOCR2003,
  title = {{{INFTY}}: An Integrated {{OCR}} System for Mathematical Documents},
  booktitle = {Proceedings of the 2003 {{ACM}} Symposium on {{Document}} Engineering},
  author = {Suzuki, Masakazu and Tamari, Fumikazu and Fukuda, Ryoji and Uchida, Seiichi and Kanahori, Toshihiro},
  year = {2003},
  pages = {95--104}
}

@book{suzukiInftyReaderOCR2013,
  title = {Infty Reader: {{An OCR}} System for {{Math}} Documents},
  author = {Suzuki, M. and Yamaguchi, K. and Gardner, J.},
  year = {2013}
}

@inproceedings{suzukiIntegratedOCRSoftware2004,
  title = {An {{Integrated OCR Software}} for {{Mathematical Documents}} and {{Its Output}} with {{Accessibility}}},
  booktitle = {Computers {{Helping People}} with {{Special Needs}}},
  author = {Suzuki, Masakazu and Kanahori, Toshihiro and Ohtake, Nobuyuki and Yamaguchi, Katsuhito},
  editor = {Miesenberger, Klaus and Klaus, Joachim and Zagler, Wolfgang L. and Burger, Dominique},
  year = {2004},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {648--655},
  publisher = {{Springer}},
  address = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-540-27817-7_97},
  abstract = {This paper describes shortly a practical integrated system for scientific documents including mathematical formulae, named `Infty'. The system consists of three components of applications: an OCR system named `InftyReader', an editor named `InftyEditor' and converting tools into various formats. Those applications are linked each other via XML files.InftyReader recognizes scanned images of clearly printed mathematical documents and outputs the recognition results in a XML format. It recognizes complex mathematical formulae used in various research papers of mathematics including matrices. InftyEditor provides a very efficient interface to correct the recognition results using keyboard. Another feature of InftyEditor is its handwriting interface to input mathematical formulae for users with vision and speech interface for visually impaired uses.The XML files output by InftyReader/Editor can be converted into various formats: LATEX, MathML, HTML and Braille Codes; in UBC (Unified Braille Codes) for English texts and in Japanese Braille Codes for Japanese texts.},
  isbn = {978-3-540-27817-7},
  langid = {english},
  keywords = {Mathematical Formula,Recognition Result,Screen Reader,Text Area,Virtual Link},
  file = {/Users/nvk/Zotero/storage/SZZ62UBS/Suzuki et al. - 2004 - An Integrated OCR Software for Mathematical Docume.pdf}
}

@article{taiImprovedSemanticRepresentations2015,
  title = {Improved {{Semantic Representations From Tree-Structured Long Short-Term Memory Networks}}},
  author = {Tai, Kai Sheng and Socher, Richard and Manning, Christopher D.},
  year = {2015},
  month = may,
  journal = {arXiv:1503.00075 [cs]},
  eprint = {1503.00075},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {Because of their superior ability to preserve sequence information over time, Long Short-Term Memory (LSTM) networks, a type of recurrent neural network with a more complex computational unit, have obtained strong results on a variety of sequence modeling tasks. The only underlying LSTM structure that has been explored so far is a linear chain. However, natural language exhibits syntactic properties that would naturally combine words to phrases. We introduce the Tree-LSTM, a generalization of LSTMs to tree-structured network topologies. TreeLSTMs outperform all existing systems and strong LSTM baselines on two tasks: predicting the semantic relatedness of two sentences (SemEval 2014, Task 1) and sentiment classification (Stanford Sentiment Treebank).},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/Users/nvk/Zotero/storage/NZVX4LDS/Tai et al. - 2015 - Improved Semantic Representations From Tree-Struct.pdf}
}

@inproceedings{tapiaRecognitionOnlineHandwritten2003,
  title = {Recognition of On-Line Handwritten Mathematical Formulas in the {{E-chalk}} System},
  booktitle = {Seventh {{International Conference}} on {{Document Analysis}} and {{Recognition}}, 2003. {{Proceedings}}.},
  author = {Tapia, E. and Rojas, R.},
  year = {2003},
  volume = {1},
  pages = {980--984},
  publisher = {{IEEE Comput. Soc}},
  address = {{Edinburgh, UK}},
  doi = {10.1109/ICDAR.2003.1227805},
  abstract = {In this article, we present a system for the recognition of on-line handwritten mathematical formulas which is used in the electronic chalkboard (E-chalk), a multimedia system for distance-teaching. We discuss the classification of symbols and the construction of the tree of spatial relationships among them. The classification is based on support vector machines and the construction of formulas is based on baseline structure analysis.},
  isbn = {978-0-7695-1960-9},
  langid = {english},
  file = {/Users/nvk/Zotero/storage/B5B6RAGV/Tapia and Rojas - 2003 - Recognition of on-line handwritten mathematical fo.pdf}
}

@inproceedings{tatmanGenderDialectBias2017,
  title = {Gender and {{Dialect Bias}} in {{YouTube}}'s {{Automatic Captions}}},
  booktitle = {Proceedings of the {{First ACL Workshop}} on {{Ethics}} in {{Natural Language Processing}}},
  author = {Tatman, Rachael},
  year = {2017},
  month = apr,
  pages = {53--59},
  publisher = {{Association for Computational Linguistics}},
  address = {{Valencia, Spain}},
  doi = {10.18653/v1/W17-1606},
  abstract = {This project evaluates the accuracy of YouTube's automatically-generated captions across two genders and five dialect groups. Speakers' dialect and gender was controlled for by using videos uploaded as part of the ``accent tag challenge'', where speakers explicitly identify their language background. The results show robust differences in accuracy across both gender and dialect, with lower accuracy for 1) women and 2) speakers from Scotland. This finding builds on earlier research finding that speaker's sociolinguistic identity may negatively impact their ability to use automatic speech recognition, and demonstrates the need for sociolinguistically-stratified validation of systems.},
  file = {/Users/nvk/Zotero/storage/57SBAL38/Tatman - 2017 - Gender and Dialect Bias in YouTube's Automatic Cap.pdf}
}

@article{theisEndMooreLaw2017,
  title = {The {{End}} of {{Moore}}'s {{Law}}: {{A New Beginning}} for {{Information Technology}}},
  shorttitle = {The {{End}} of {{Moore}}'s {{Law}}},
  author = {Theis, Thomas N. and Wong, H.-S. Philip},
  year = {2017},
  month = mar,
  journal = {Computing in Science Engineering},
  volume = {19},
  number = {2},
  pages = {41--50},
  issn = {1558-366X},
  doi = {10.1109/MCSE.2017.29},
  abstract = {The insights contained in Gordon Moore's now famous 1965 and 1975 papers have broadly guided the development of semiconductor electronics for over 50 years. However, the field-effect transistor is approaching some physical limits to further miniaturization, and the associated rising costs and reduced return on investment appear to be slowing the pace of development. Far from signaling an end to progress, this gradual "end of Moore's law" will open a new era in information technology as the focus of research and development shifts from miniaturization of long-established technologies to the coordinated introduction of new devices, new integration technologies, and new architectures for computing.},
  keywords = {Algorithm design and analysis,algorithms implemented in hardware,Computer architecture,emerging technologies,field effect transistor,Field effect transistors,Gordon Moore,information technology,introductory and survey,Memory management,memory technologies,Moore's law,Moore's Law,neural nets,Random access memory,research and development,scientific computing,Scientific computing,semiconductor electronics,Switching circuits},
  file = {/Users/nvk/Zotero/storage/A4XA4KFJ/Theis and Wong - 2017 - The End of Moore's Law A New Beginning for Inform.pdf;/Users/nvk/Zotero/storage/824UPSG2/7878935.html}
}

@article{thurstonProofProgressMathematics1994,
  title = {On Proof and Progress in Mathematics},
  author = {Thurston, William P.},
  year = {1994},
  month = mar,
  journal = {arXiv:math/9404236},
  eprint = {math/9404236},
  eprinttype = {arxiv},
  abstract = {In response to Jaffe and Quinn [math.HO/9307227], the author discusses forms of progress in mathematics that are not captured by formal proofs of theorems, especially in his own work in the theory of foliations and geometrization of 3-manifolds and dynamical systems.},
  archiveprefix = {arXiv},
  keywords = {Mathematics - History and Overview},
  file = {/Users/nvk/Zotero/storage/836PTPCA/Thurston - 1994 - On proof and progress in mathematics.pdf;/Users/nvk/Zotero/storage/49AF6A32/9404236.html}
}

@inproceedings{tiedemannOPUSCorpusParallel2004,
  title = {The {{OPUS}} Corpus - Parallel and Free},
  booktitle = {In {{Proceedings}} of the {{Fourth International Conference}} on {{Language Resources}} and {{Evaluation}} ({{LREC04}}},
  author = {Tiedemann, J{\"o}rg and Nygaard, Lars and Hf, Tekstlaboratoriet},
  year = {2004},
  pages = {26--28},
  abstract = {The OPUS corpus is a growing collection of translated documents collected from the internet. The current version contains about 30 million words in 60 languages. The entire corpus is sentence aligned and it also contains linguistic markup for certain languages. 1.},
  file = {/Users/nvk/Zotero/storage/J4G4CDX9/Tiedemann et al. - 2004 - The OPUS corpus - parallel and free.pdf;/Users/nvk/Zotero/storage/NIREGTJ3/download.html}
}

@misc{trottECFProjectFinalReport2013,
  title = {The {{eCF-Project}}\textemdash{{Final Report}}},
  author = {Trott, Michael and Weisstein, Eric and Marichev, Oleg and Rowland, Todd},
  year = {2013},
  month = oct,
  publisher = {{Wolfram Foundation}},
  file = {/Users/nvk/Zotero/storage/H6TNBEWE/FinalReport.pdf}
}

@article{trottECFProjectFinalReport2013a,
  title = {The {{eCF-Project}}\textemdash{{Final Report}}},
  author = {Trott, Micheal and Weisstein, Eric and Marichev, Oleg and Rowland, Todd},
  year = {2013},
  month = oct
}

@article{typkeSURVEYMUSICINFORMATION,
  title = {A {{SURVEY OF MUSIC INFORMATION RETRIEVAL SYSTEMS}}},
  author = {Typke, Rainer and Wiering, Frans and Veltkamp, Remco C},
  pages = {8},
  abstract = {This survey paper provides an overview of content-based music information retrieval systems, both for audio and for symbolic music notation. Matching algorithms and indexing methods are briefly presented. The need for a TREC-like comparison of matching algorithms such as MIREX at ISMIR becomes clear from the high number of quite different methods which so far only have been used on different data collections. We placed the systems on a map showing the tasks and users for which they are suitable, and we find that existing content-based retrieval systems fail to cover a gap between the very general and the very specific retrieval tasks.},
  langid = {english},
  file = {/Users/nvk/Zotero/storage/V7VEU3PU/Typke et al. - A SURVEY OF MUSIC INFORMATION RETRIEVAL SYSTEMS.pdf}
}

@inproceedings{ullmanWikiNavMapVisualisationSupplement2007,
  title = {{{WikiNavMap}}: A Visualisation to Supplement Team-Based Wikis},
  shorttitle = {{{WikiNavMap}}},
  booktitle = {{{CHI}} '07 {{Extended Abstracts}} on {{Human Factors}} in {{Computing Systems}}},
  author = {Ullman, Adam John and Kay, Judy},
  year = {2007},
  month = apr,
  series = {{{CHI EA}} '07},
  pages = {2711--2716},
  publisher = {{Association for Computing Machinery}},
  address = {{New York, NY, USA}},
  doi = {10.1145/1240866.1241067},
  abstract = {Wikis are an invaluable tool for quickly and easily creating and editing a collection of web pages. Their use is particularly interesting in small teams to serve as a support for group communication, for co-ordination, as well as for creating collaborative document products. In spite of the very real appeal of the wiki for these purposes, there is a serious challenge due to their complexity. Team members can have difficulty identifying the structure and salient elements of the wiki. This paper describes the design of WikiNavMap, an alternative visual representation for wikis, which provides an overview of the wiki structure. Based on analysis of student wikis, we identified factors that help team members identify which wiki pages are currently relevant to them. We hypothesised that a structural overview coupled with the visual representations of these factors could assist users with wiki navigation decisions. We report a preliminary evaluation with a large group wiki, created over a full university semester by a group of ten users. The results are promising for a small wiki but point to challenges in coping with the complexity of a larger one.},
  isbn = {978-1-59593-642-4},
  keywords = {visualization,world wide web and hypermedia}
}

@inproceedings{umezawaEffectiveFlippedClassroom2015,
  title = {An {{Effective Flipped Classroom Based}} on {{Log Information}} of {{Self-Study}}},
  booktitle = {2015 3rd {{International Conference}} on {{Applied Computing}} and {{Information Technology}}/2nd {{International Conference}} on {{Computational Science}} and {{Intelligence}}},
  author = {Umezawa, Katsuyuki and Aramoto, Michitaka and Kobayashi, Manabu and Ishida, Takashi and Nakazawa, Makoto and Hirasawa, Shigeichi},
  year = {2015},
  month = jul,
  pages = {248--253},
  doi = {10.1109/ACIT-CSI.2015.52},
  abstract = {Flipped classrooms have recently begun to attract attention. In a flipped classroom, the roles of a classroom and homework are reversed. Specifically, the students study on their own by using digital teaching materials or e-learning prior to a school hours and apply their learning mainly in the classroom discussions. In this paper, we propose a method for an effective flipped classroom based on the log information of the self-study. Specifically, when students study by e-learning at home, we collect learning logs. We classify students into groups on the basis of their study time and degree of understanding by analyzing the learning logs. We can improve the degree of understanding of the students by conducting a discussion and/or giving a presentation to each group in the classroom.},
  keywords = {Authentication,Ciphers,Electronic learning,Electronic mail,Learning systems,Mathematics},
  file = {/Users/nvk/Zotero/storage/DFPHLV88/Umezawa et al. - 2015 - An Effective Flipped Classroom Based on Log Inform.pdf;/Users/nvk/Zotero/storage/X9CDJ33E/7336069.html}
}

@article{UniversityCaliforniaQualified,
  title = {The {{University}} of {{California}}  {{Qualified Students Face}} an {{Inconsistent}} and {{Unfair Admissions System That Has Been Improperly Influenced}} by {{Relationships}} and {{Monetary Donations}}},
  pages = {82},
  langid = {english},
  file = {/Users/nvk/Zotero/storage/ZQQTE4KR/The University of California  Qualified Students F.pdf}
}

@article{vargheseProgrammingAdaptevaEpiphany2017,
  title = {Programming the {{Adapteva Epiphany}} 64-Core Network-on-Chip Coprocessor},
  author = {Varghese, Anish and Edwards, Bob and Mitra, Gaurav and Rendell, Alistair P},
  year = {2017},
  month = jul,
  journal = {The International Journal of High Performance Computing Applications},
  volume = {31},
  number = {4},
  pages = {285--302},
  publisher = {{SAGE Publications Ltd STM}},
  issn = {1094-3420},
  doi = {10.1177/1094342015599238},
  abstract = {Energy efficiency is the primary impediment in the path to exascale computing. Consequently, the high-performance computing community is increasingly interested in low-power high-performance embedded systems as building blocks for large-scale high-performance systems. The Adapteva Epiphany architecture integrates low-power RISC cores on a 2D mesh network and promises up to 70 GFLOPS/Watt of theoretical performance. However, with just 32\,KB of memory per eCore for storing both data and code, programming the Epiphany system presents significant challenges. In this paper we evaluate the performance of a 64-core Epiphany system with a variety of basic compute and communication micro-benchmarks. Further, we implemented two well known application kernels, 5-point star-shaped heat stencil with a peak performance of 65.2 GFLOPS and matrix multiplication with 65.3 GFLOPS in single precision across 64 Epiphany cores. We discuss strategies for implementing high-performance computing application kernels on such memory constrained low-power devices and compare the Epiphany with competing low-power systems. With future Epiphany revisions expected to house thousands of cores on a single chip, understanding the merits of such an architecture is of prime importance to the exascale initiative.},
  langid = {english},
  file = {/Users/nvk/Zotero/storage/G8LT3FXW/Varghese et al. - 2017 - Programming the Adapteva Epiphany 64-core network-.pdf}
}

@inproceedings{vaswaniAttentionAllYou2017,
  title = {Attention Is {{All}} You {{Need}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  year = {2017},
  volume = {30},
  publisher = {{Curran Associates, Inc.}},
  file = {/Users/nvk/Zotero/storage/TWC7LLWN/Vaswani et al. - 2017 - Attention is All you Need.pdf}
}

@inproceedings{vaswaniAttentionAllYou2017a,
  title = {Attention Is {{All}} You {{Need}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  year = {2017},
  volume = {30},
  publisher = {{Curran Associates, Inc.}},
  abstract = {The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.},
  file = {/Users/nvk/Zotero/storage/EY75Z7T5/Vaswani et al. - 2017 - Attention is All you Need.pdf}
}

@article{vazquezHelsinkiSubmissionAmericasNLP2021,
  title = {The {{Helsinki}} Submission to the {{AmericasNLP}} Shared Task},
  author = {V{\'a}zquez, Ra{\'u}l and Scherrer, Yves and Virpioja, Sami and Tiedemann, J{\"o}rg},
  year = {2021},
  month = jun,
  publisher = {{The Association for Computational Linguistics}},
  abstract = {The University of Helsinki participated in the AmericasNLP shared task for all ten language pairs. Our multilingual NMT models reached the first rank on all language pairs in track 1, and first rank on nine out of ten language pairs in track 2. We focused our efforts on three aspects: (1) the collection of additional data from various sources such as Bibles and political constitutions, (2) the cleaning and filtering of training data with the OpusFilter toolkit, and (3) different multilingual training techniques enabled by the latest version of the OpenNMT-py toolkit to make the most efficient use of the scarce data. This paper describes our efforts in detail.},
  copyright = {cc\_by},
  langid = {english},
  annotation = {Accepted: 2021-09-10T07:49:02Z},
  file = {/Users/nvk/Zotero/storage/ZXMIUCII/VÃ¡zquez et al. - 2021 - The Helsinki submission to the AmericasNLP shared .pdf;/Users/nvk/Zotero/storage/V83TWPHS/334239.html}
}

@inproceedings{venkatesanSPINTASTICSpinBasedStochastic2015,
  title = {{{SPINTASTIC}}: {{Spin-Based Stochastic Logic}} for {{Energy-Efficient Computing}}},
  shorttitle = {{{SPINTASTIC}}},
  booktitle = {Design, {{Automation}} \& {{Test}} in {{Europe Conference}} \& {{Exhibition}} ({{DATE}}), 2015},
  author = {Venkatesan, Rangharajan and Venkataramani, Swagath and Fong, Xuanyao and Roy, Kaushik and Raghunathan, Anand},
  year = {2015},
  pages = {1575--1578},
  publisher = {{IEEE Conference Publications}},
  address = {{Grenoble, France}},
  doi = {10.7873/DATE.2015.0460},
  abstract = {Spintronics is one of the leading technologies under consideration for the post-CMOS era. While spintronic memories have demonstrated great promise due to their density, nonvolatility and low leakage, efforts to realize spintronic logic have been much less fruitful. Recent studies project the performance and energy efficiency of spintronic logic to be considerably inferior to CMOS. In this work, we explore Stochastic Computing (SC) as a new direction for the realization of energy-efficient logic using spintronic devices. We establish the synergy between stochastic computing and spintronics by demonstrating that (i) the peripheral circuits required for SC to convert to/from stochastic domains, which incur significant energy overheads in CMOS, can be efficiently realized by exploiting the characteristics of spintronic devices, and (ii) the low logic complexity and finegrained parallelism in SC circuits can be leveraged to alleviate the shortcomings of spintronic logic. We propose SPINTASTIC, a new design approach in which all the components of stochastic circuits \textemdash{} stochastic number generators, stochastic arithmetic units, and stochastic-to-binary converters \textemdash{} are realized using spintronic devices. Our experiments on a range of benchmarks from different application domains demonstrate that SPINTASTIC achieves 2.8X improvement in energy over CMOS stochastic implementations and 1.9X over a CMOS binary baseline.},
  isbn = {978-3-9815370-4-8},
  langid = {english},
  file = {/Users/nvk/Zotero/storage/DITKMRSC/Venkatesan et al. - 2015 - SPINTASTIC Spin-Based Stochastic Logic for Energy.pdf}
}

@article{vinyalsPointerNetworks2017,
  title = {Pointer {{Networks}}},
  author = {Vinyals, Oriol and Fortunato, Meire and Jaitly, Navdeep},
  year = {2017},
  month = jan,
  journal = {arXiv:1506.03134 [cs, stat]},
  eprint = {1506.03134},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {We introduce a new neural architecture to learn the conditional probability of an output sequence with elements that are discrete tokens corresponding to positions in an input sequence. Such problems cannot be trivially addressed by existent approaches such as sequence-to-sequence and Neural Turing Machines, because the number of target classes in each step of the output depends on the length of the input, which is variable. Problems such as sorting variable sized sequences, and various combinatorial optimization problems belong to this class. Our model solves the problem of variable size output dictionaries using a recently proposed mechanism of neural attention. It differs from the previous attention attempts in that, instead of using attention to blend hidden units of an encoder to a context vector at each decoder step, it uses attention as a pointer to select a member of the input sequence as the output. We call this architecture a Pointer Net (Ptr-Net). We show Ptr-Nets can be used to learn approximate solutions to three challenging geometric problems -- finding planar convex hulls, computing Delaunay triangulations, and the planar Travelling Salesman Problem -- using training examples alone. Ptr-Nets not only improve over sequence-to-sequence with input attention, but also allow us to generalize to variable size output dictionaries. We show that the learnt models generalize beyond the maximum lengths they were trained on. We hope our results on these tasks will encourage a broader exploration of neural learning for discrete problems.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computational Geometry,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Statistics - Machine Learning},
  file = {/Users/nvk/Zotero/storage/WXMR8F4C/Vinyals et al. - 2017 - Pointer Networks.pdf;/Users/nvk/Zotero/storage/RL3LEKDJ/1506.html}
}

@inproceedings{virginiacommonwealthuniversityusaDialectSpecificModelsAutomatic2019,
  title = {Dialect-{{Specific Models}} for {{Automatic Speech Recognition}} of {{African American Vernacular English}}},
  booktitle = {Proceedings of the {{Student Research Workshop Associated}} with {{RANLP}} 2019},
  author = {{Virginia Commonwealth University, USA} and Dorn, Rachel},
  year = {2019},
  month = sep,
  pages = {16--20},
  publisher = {{Incoma Ltd.}},
  doi = {10.26615/issn.2603-2821.2019_003},
  abstract = {African American Vernacular English (AAVE) is a widely-spoken dialect of English, yet it is under-represented in major speech corpora. As a result, speakers of this dialect are often misunderstood by NLP applications. This study explores the effect on transcription accuracy of an automatic voice recognition system when AAVE data is used. Models trained on AAVE data and on Standard American English data were compared to a baseline model trained on a combination of the two dialects. The accuracy for both dialectspecific models was significantly higher than the baseline model, with the AAVE model showing over 18\% improvement. By isolating the effect of having AAVE speakers in the training data, this study highlights the importance of increasing diversity in the field of natural language processing.},
  langid = {english},
  file = {/Users/nvk/Zotero/storage/P64FR5Q3/Virginia Commonwealth University, USA and Dorn - 2019 - Dialect-Specific Models for Automatic Speech Recog.pdf}
}

@article{volpertReactionDiffusionWaves2009,
  title = {Reaction\textendash Diffusion Waves in Biology},
  author = {Volpert, V. and Petrovskii, S.},
  year = {2009},
  month = dec,
  journal = {Physics of Life Reviews},
  volume = {6},
  number = {4},
  pages = {267--310},
  issn = {1571-0645},
  doi = {10.1016/j.plrev.2009.10.002},
  abstract = {The theory of reaction\textendash diffusion waves begins in the 1930s with the works in population dynamics, combustion theory and chemical kinetics. At the present time, it is a well developed area of research which includes qualitative properties of travelling waves for the scalar reaction\textendash diffusion equation and for system of equations, complex nonlinear dynamics, numerous applications in physics, chemistry, biology, medicine. This paper reviews biological applications of reaction\textendash diffusion waves.},
  langid = {english},
  keywords = {Atherosclerosis,Cell dynamics,Evolutionary branching,Leukemia,Population dynamics,Reactionâ€“diffusion systems,Travelling waves},
  file = {/Users/nvk/Zotero/storage/T63PQFIT/S1571064509000347.html}
}

@inproceedings{wangAutomaticKeywordExtraction2012,
  title = {Automatic {{Keyword Extraction}} from {{Single-Sentence Natural Language Queries}}},
  booktitle = {{{PRICAI}} 2012: {{Trends}} in {{Artificial Intelligence}}},
  author = {Wang, David X. and Gao, Xiaoying and Andreae, Peter},
  editor = {Anthony, Patricia and Ishizuka, Mitsuru and Lukose, Dickson},
  year = {2012},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {637--648},
  publisher = {{Springer}},
  address = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-642-32695-0_56},
  abstract = {This paper presents a novel algorithm of extracting keywords from single-sentence natural language queries in English. The process involves applying a series of rules to a parsed query in order to pick out potential keywords based on part-of-speech and the surrounding phrase structure. A supervised machine learning method is also explored in order to find suitable rules, which has shown promising results when cross-validated with various training sets.},
  isbn = {978-3-642-32695-0},
  langid = {english},
  keywords = {Computational Linguistics,Noun Phrase,Parse Tree,Prepositional Phrase,Selection Rule},
  file = {/Users/nvk/Zotero/storage/5JU97CPM/Wang et al. - 2012 - Automatic Keyword Extraction from Single-Sentence .pdf}
}

@inproceedings{wangCascadeRankingModel2011,
  title = {A Cascade Ranking Model for Efficient Ranked Retrieval},
  booktitle = {Proceedings of the 34th International {{ACM SIGIR}} Conference on {{Research}} and Development in {{Information Retrieval}}},
  author = {Wang, Lidan and Lin, Jimmy and Metzler, Donald},
  year = {2011},
  month = jul,
  series = {{{SIGIR}} '11},
  pages = {105--114},
  publisher = {{Association for Computing Machinery}},
  address = {{New York, NY, USA}},
  doi = {10.1145/2009916.2009934},
  abstract = {There is a fundamental tradeoff between effectiveness and efficiency when designing retrieval models for large-scale document collections. Effectiveness tends to derive from sophisticated ranking functions, such as those constructed using learning to rank, while efficiency gains tend to arise from improvements in query evaluation and caching strategies. Given their inherently disjoint nature, it is difficult to jointly optimize effectiveness and efficiency in end-to-end systems. To address this problem, we formulate and develop a novel cascade ranking model, which unlike previous approaches, can simultaneously improve both top k ranked effectiveness and retrieval efficiency. The model constructs a cascade of increasingly complex ranking functions that progressively prunes and refines the set of candidate documents to minimize retrieval latency and maximize result set quality. We present a novel boosting algorithm for learning such cascades to directly optimize the tradeoff between effectiveness and efficiency. Experimental results show that our cascades are faster and return higher quality results than comparable ranking models.},
  isbn = {978-1-4503-0757-4},
  keywords = {effectiveness,efficiency,learning to rank},
  file = {/Users/nvk/Zotero/storage/CDY8UYZM/Wang et al. - 2011 - A cascade ranking model for efficient ranked retri.pdf}
}

@misc{wangMachineComprehensionUsing2016,
  title = {Machine {{Comprehension Using Match-LSTM}} and {{Answer Pointer}}},
  author = {Wang, Shuohang and Jiang, Jing},
  year = {2016},
  month = nov,
  number = {arXiv:1608.07905},
  eprint = {1608.07905},
  eprinttype = {arxiv},
  primaryclass = {cs},
  institution = {{arXiv}},
  abstract = {Machine comprehension of text is an important problem in natural language processing. A recently released dataset, the Stanford Question Answering Dataset (SQuAD), offers a large number of real questions and their answers created by humans through crowdsourcing. SQuAD provides a challenging testbed for evaluating machine comprehension algorithms, partly because compared with previous datasets, in SQuAD the answers do not come from a small set of candidate answers and they have variable lengths. We propose an end-to-end neural architecture for the task. The architecture is based on match-LSTM, a model we proposed previously for textual entailment, and Pointer Net, a sequence-to-sequence model proposed by Vinyals et al.(2015) to constrain the output tokens to be from the input sequences. We propose two ways of using Pointer Net for our task. Our experiments show that both of our two models substantially outperform the best results obtained by Rajpurkar et al.(2016) using logistic regression and manually crafted features.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {/Users/nvk/Zotero/storage/PGBKTKIJ/Wang and Jiang - 2016 - Machine Comprehension Using Match-LSTM and Answer .pdf;/Users/nvk/Zotero/storage/86XE5FNB/1608.html}
}

@article{wangMultipassageBERTGlobally2019,
  title = {Multi-Passage {{BERT}}: {{A Globally Normalized BERT Model}} for {{Open-domain Question Answering}}},
  shorttitle = {Multi-Passage {{BERT}}},
  author = {Wang, Zhiguo and Ng, Patrick and Ma, Xiaofei and Nallapati, Ramesh and Xiang, Bing},
  year = {2019},
  month = oct,
  journal = {arXiv:1908.08167 [cs]},
  eprint = {1908.08167},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {BERT model has been successfully applied to open-domain QA tasks. However, previous work trains BERT by viewing passages corresponding to the same question as independent training instances, which may cause incomparable scores for answers from different passages. To tackle this issue, we propose a multi-passage BERT model to globally normalize answer scores across all passages of the same question, and this change enables our QA model find better answers by utilizing more passages. In addition, we find that splitting articles into passages with the length of 100 words by sliding window improves performance by 4\%. By leveraging a passage ranker to select high-quality passages, multi-passage BERT gains additional 2\%. Experiments on four standard benchmarks showed that our multi-passage BERT outperforms all state-of-the-art models on all benchmarks. In particular, on the OpenSQuAD dataset, our model gains 21.4\% EM and 21.5\% \$F\_1\$ over all non-BERT models, and 5.8\% EM and 6.5\% \$F\_1\$ over BERT-based models.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {/Users/nvk/Zotero/storage/X6IBN7UJ/Wang et al. - 2019 - Multi-passage BERT A Globally Normalized BERT Mod.pdf;/Users/nvk/Zotero/storage/USRS3TKU/1908.html}
}

@inproceedings{wangParallelStatefulLogic2019,
  title = {Parallel {{Stateful Logic}} in {{RRAM}}: {{Theoretical Analysis}} and {{Arithmetic Design}}},
  shorttitle = {Parallel {{Stateful Logic}} in {{RRAM}}},
  booktitle = {2019 {{IEEE}} 30th {{International Conference}} on {{Application-specific Systems}}, {{Architectures}} and {{Processors}} ({{ASAP}})},
  author = {Wang, Feng and Luo, Guojie and Sun, Guangyu and Zhang, Jiaxi and Huang, Peng and Kang, Jinfeng},
  year = {2019},
  month = jul,
  volume = {2160-052X},
  pages = {157--164},
  issn = {2160-052X},
  doi = {10.1109/ASAP.2019.000-8},
  abstract = {Processing-in-memory (PIM) provides massive parallelism with high energy efficiency and becomes a promising solution to the memory wall problem. Recently, the emerging metal-oxide resistive random access memory (RRAM) has shown its potential to design a PIM architecture. Several stateful logic operations, e.g., NOR and NAND, can be executed in parallel in an RRAM crossbar. Although previous works have designed some algorithms using the stateful logic, it is still under exploration how to fully exploit its potential high parallelism and design an asymptotically fast algorithm for a given function. In this work, we theoretically analyze the parallelism in an RRAM crossbar and design several asymptotically optimal arithmetic algorithms. In detail, we first propose the Single Instruction Multiple Lines (SIML) model to unify the stateful logic families and prove three lower bounds on the time complexity of a parallel RRAM algorithm. Then, we design three algorithms for integer addition functions with the stateful logic, guided by the lower bound analysis. All of them reach the time complexity lower bound. Finally, We make two extensions of the integer addition algorithms, supporting multiplication functions by decomposing them to additions and supporting the flex-point data type by proposing an exponent and mantissa update flow. Experimental evaluation shows that our integer algorithms achieves a speedup up to 13.79x over the previous RRAM algorithms. Our flex-point implementation achieves a 26.60x speedup and saves 73.68\% energy compared to an ARM.},
  keywords = {1.566E+12,arithmetic design,asymptotically fast algorithm,Computational modeling,Computer architecture,integer addition algorithms,integer addition functions,logic circuits,logic design,lower bound analysis,massive parallelism,memory wall problem,metal-oxide resistive random access memory,multiplication functions,Parallel processing,parallel RRAM algorithm,parallel stateful logic,PIM architecture,processing-in-memory,Random access memory,Resistance,resistive RAM,RRAM crossbar,single instruction multiple lines model,stateful logic families,stateful logic operations,Switches,Time complexity},
  file = {/Users/nvk/Zotero/storage/9INDWGFJ/Wang et al. - 2019 - Parallel Stateful Logic in RRAM Theoretical Analy.pdf;/Users/nvk/Zotero/storage/WIRFUPJX/8825150.html}
}

@article{wangTextAbstractionSummary2019,
  title = {A {{Text Abstraction Summary Model Based}} on {{BERT Word Embedding}} and {{Reinforcement Learning}}},
  author = {Wang, Qicai and Liu, Peiyu and Zhu, Zhenfang and Yin, Hongxia and Zhang, Qiuyue and Zhang, Lindong},
  year = {2019},
  month = jan,
  journal = {Applied Sciences},
  volume = {9},
  number = {21},
  pages = {4701},
  publisher = {{Multidisciplinary Digital Publishing Institute}},
  doi = {10.3390/app9214701},
  abstract = {As a core task of natural language processing and information retrieval, automatic text summarization is widely applied in many fields. There are two existing methods for text summarization task at present: abstractive and extractive. On this basis we propose a novel hybrid model of extractive-abstractive to combine BERT (Bidirectional Encoder Representations from Transformers) word embedding with reinforcement learning. Firstly, we convert the human-written abstractive summaries to the ground truth labels. Secondly, we use BERT word embedding as text representation and pre-train two sub-models respectively. Finally, the extraction network and the abstraction network are bridged by reinforcement learning. To verify the performance of the model, we compare it with the current popular automatic text summary model on the CNN/Daily Mail dataset, and use the ROUGE (Recall-Oriented Understudy for Gisting Evaluation) metrics as the evaluation method. Extensive experimental results show that the accuracy of the model is improved obviously.},
  copyright = {http://creativecommons.org/licenses/by/3.0/},
  langid = {english},
  keywords = {BERT word embedding,reinforce learning,text summary},
  file = {/Users/nvk/Zotero/storage/IZJC7GI9/Wang et al. - 2019 - A Text Abstraction Summary Model Based on BERT Wor.pdf}
}

@inproceedings{wangWikiMirsHybridMIR2015,
  title = {{{WikiMirs}} 3.0: {{A Hybrid MIR System Based}} on the {{Context}}, {{Structure}} and {{Importance}} of {{Formulae}} in a {{Document}}},
  shorttitle = {{{WikiMirs}} 3.0},
  booktitle = {Proceedings of the 15th {{ACM}}/{{IEEE-CS Joint Conference}} on {{Digital Libraries}}},
  author = {Wang, Yuehan and Gao, Liangcai and Wang, Simeng and Tang, Zhi and Liu, Xiaozhong and Yuan, Ke},
  year = {2015},
  month = jun,
  series = {{{JCDL}} '15},
  pages = {173--182},
  publisher = {{Association for Computing Machinery}},
  address = {{New York, NY, USA}},
  doi = {10.1145/2756406.2756918},
  abstract = {Nowadays, mathematical information is increasingly available in websites and repositories, such like ArXiv, Wikipedia and growing numbers of digital libraries. Mathematical formulae are highly structured and usually presented in layout presentations, such as PDF, LATEX and Presentation MathML. The differences of presentation between text and formulae challenge traditional text-based index and retrieval methods. To address the challenge, this paper proposes an upgraded Mathematical Information Retrieval (MIR) system, namely WikiMirs 3.0, based on the context, structure and importance of formulae in a document. In WikiMirs 3.0, users can easily "cut" formulae and contexts from PDF documents as well as type in queries. Furthermore, a novel hybrid indexing and matching model is proposed to support both exact and fuzzy matching. In the hybrid model, both context and structure information of formulae are taken into consideration. In addition, the concept of formula importance within a document is introduced into the model for more reasonable ranking. Experimental results, compared with two classical MIR systems, demonstrate that the proposed system along with the novel model provides higher accuracy and better ranking results over Wikipedia.},
  isbn = {978-1-4503-3594-2},
  keywords = {context information,importance of formulae,mathematical information retrieval,structure matching},
  file = {/Users/nvk/Zotero/storage/CTMX4K7V/Wang et al. - 2015 - WikiMirs 3.0 A Hybrid MIR System Based on the Con.pdf}
}

@article{waserRedoxBasedResistiveSwitching2009,
  title = {Redox-{{Based Resistive Switching Memories}} \textendash{} {{Nanoionic Mechanisms}}, {{Prospects}}, and {{Challenges}}},
  author = {Waser, Rainer and Dittmann, Regina and Staikov, Georgi and Szot, Kristof},
  year = {2009},
  journal = {Advanced Materials},
  volume = {21},
  number = {25-26},
  pages = {2632--2663},
  issn = {1521-4095},
  doi = {10.1002/adma.200900375},
  abstract = {This review article introduces resistive switching processes that are being considered for nanoelectronic nonvolatile memories. The three main classes are based on an electrochemical metallization mechanism, a valence change mechanism, and a thermochemical mechanism, respectively. The current understanding of the microscopic mechanisms is discussed and the scaling potential is outlined..},
  copyright = {Copyright \textcopyright{} 2009 WILEY-VCH Verlag GmbH \& Co. KGaA, Weinheim},
  keywords = {data storage,defects,electrochemical metallization cells,memory devices,memristors,resistive switching oxides,valence change},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/adma.200900375},
  file = {/Users/nvk/Zotero/storage/SLEVMEJA/Waser et al. - 2009 - Redox-Based Resistive Switching Memories â€“ Nanoion.pdf;/Users/nvk/Zotero/storage/XLRA8Y4C/adma.html}
}

@inproceedings{weissteinComputableDataMathematics2014,
  title = {Computable~{{Data}},~{{Mathematics}},~and~{{Digital Libraries}}~in~{{Mathematica}}~and~{{Wolfram}}|{{Alpha}}},
  booktitle = {Intelligent {{Computer Mathematics}}},
  author = {Weisstein, Eric},
  editor = {Watt, Stephen M. and Davenport, James H. and Sexton, Alan P. and Sojka, Petr and Urban, Josef},
  year = {2014},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {26--29},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-319-08434-3_3},
  abstract = {This talk will focus on the infrastructure developed for representing and accessing data (especially mathematical data) in Wolfram|Alpha, as well as on the technologies and language extensions developed in the most recent version of Mathematica for making this data even more computationally accessible. Based on experiences using these technologies to create a prototype semantic digital library for a subset of mathematics, we believe the ambitious dream of creating of a semantic digital library for all of mathematics is now within reach.},
  isbn = {978-3-319-08434-3},
  langid = {english},
  keywords = {Computable Data,Digital Library,Entity Class,Language Extension,Sloan Foundation}
}

@inproceedings{weissteinComputableDataMathematics2014a,
  title = {Computable~{{Data}},~{{Mathematics}},~and~{{Digital Libraries}}~in~{{Mathematica}}~and~{{Wolfram}}|{{Alpha}}},
  booktitle = {Intelligent {{Computer Mathematics}}},
  author = {Weisstein, Eric},
  editor = {Watt, Stephen M. and Davenport, James H. and Sexton, Alan P. and Sojka, Petr and Urban, Josef},
  year = {2014},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {26--29},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-319-08434-3_3},
  abstract = {This talk will focus on the infrastructure developed for representing and accessing data (especially mathematical data) in Wolfram|Alpha, as well as on the technologies and language extensions developed in the most recent version of Mathematica for making this data even more computationally accessible. Based on experiences using these technologies to create a prototype semantic digital library for a subset of mathematics, we believe the ambitious dream of creating of a semantic digital library for all of mathematics is now within reach.},
  isbn = {978-3-319-08434-3},
  langid = {english},
  keywords = {Computable Data,Digital Library,Entity Class,Language Extension,Sloan Foundation},
  file = {/Users/nvk/Zotero/storage/9LXRECTZ/Weisstein - 2014 - ComputableÂ Data,Â Mathematics,Â andÂ Digital Librarie.pdf}
}

@misc{WhatCriticismsGrade,
  title = {What {{Criticisms}} of {{Grade Inflation Really Tell Us}} | {{Opinion}} | {{The Harvard Crimson}}},
  abstract = {Criticisms of grade inflation mistakenly take a handful of letters to represent a potent educational spirit that goes much deeper. But in perhaps a greater way, they grasp a truth of Harvard we would all do well to admit.},
  howpublished = {https://www.thecrimson.com/article/2021/11/23/barone-grade-inflation/},
  file = {/Users/nvk/Zotero/storage/L9M7WGHM/barone-grade-inflation.html}
}

@article{whiteheadPrincipiaMathematica1910,
  title = {Principia {{Mathematica}}},
  author = {Whitehead, Alfred North and Russell, Bertrand},
  year = {1910},
  volume = {1--3},
  file = {/Users/nvk/Zotero/storage/X5MF5INM/principia-mathematica.html}
}

@article{WhyGradeInflation,
  title = {Why Grade Inflation (Even at {{Harvard}}) Is a Big Problem},
  journal = {Washington Post},
  issn = {0190-8286},
  abstract = {Three out of five undergraduates now believe their inflated grades understate their true academic ability},
  langid = {american},
  file = {/Users/nvk/Zotero/storage/3NGQ7ZVJ/why-grade-inflation-even-at-harvard-is-a-big-problem.html}
}

@misc{WikipediaParticipationChallenge,
  title = {Wikipedia's {{Participation Challenge}}},
  abstract = {This competition challenges data-mining experts to build a predictive model that predicts the number of edits an editor will make five months from the end date of the training dataset.},
  howpublished = {https://kaggle.com/c/wikichallenge},
  langid = {english},
  file = {/Users/nvk/Zotero/storage/I2PYGTK7/data.html}
}

@article{willseyEggFastExtensible2021,
  title = {Egg: {{Fast}} and {{Extensible Equality Saturation}}},
  shorttitle = {Egg},
  author = {Willsey, Max and Nandi, Chandrakana and Wang, Yisu Remy and Flatt, Oliver and Tatlock, Zachary and Panchekha, Pavel},
  year = {2021},
  month = jan,
  journal = {Proceedings of the ACM on Programming Languages},
  volume = {5},
  number = {POPL},
  eprint = {2004.03082},
  eprinttype = {arxiv},
  pages = {1--29},
  issn = {2475-1421},
  doi = {10.1145/3434304},
  abstract = {An e-graph efficiently represents a congruence relation over many expressions. Although they were originally developed in the late 1970s for use in automated theorem provers, a more recent technique known as equality saturation repurposes e-graphs to implement state-of-the-art, rewrite-driven compiler optimizations and program synthesizers. However, e-graphs remain unspecialized for this newer use case. Equality saturation workloads exhibit distinct characteristics and often require ad-hoc e-graph extensions to incorporate transformations beyond purely syntactic rewrites. This work contributes two techniques that make e-graphs fast and extensible, specializing them to equality saturation. A new amortized invariant restoration technique called rebuilding takes advantage of equality saturation's distinct workload, providing asymptotic speedups over current techniques in practice. A general mechanism called e-class analyses integrates domain-specific analyses into the e-graph, reducing the need for ad hoc manipulation. We implemented these techniques in a new open-source library called egg. Our case studies on three previously published applications of equality saturation highlight how egg's performance and flexibility enable state-of-the-art results across diverse domains.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Programming Languages},
  file = {/Users/nvk/Zotero/storage/8XNKESDZ/Willsey et al. - 2021 - egg Fast and Extensible Equality Saturation.pdf;/Users/nvk/Zotero/storage/6VWZXQ65/2004.html}
}

@article{winklerToolsTermRewriting2020,
  title = {Tools in {{Term Rewriting}} for {{Education}}},
  author = {Winkler, Sarah and Middeldorp, Aart},
  year = {2020},
  month = feb,
  journal = {Electronic Proceedings in Theoretical Computer Science},
  volume = {313},
  eprint = {2002.12554},
  eprinttype = {arxiv},
  pages = {54--72},
  issn = {2075-2180},
  doi = {10.4204/EPTCS.313.4},
  abstract = {Term rewriting is a Turing complete model of computation. When taught to students of computer science, key properties of computation as well as techniques to analyze programs on an abstract level are conveyed. This paper gives a swift introduction to term rewriting and presents several automatic tools to analyze term rewrite systems which were developed by the Computational Logic Group at the University of Innsbruck. These include the termination tool TTT2, the confluence prover CSI, the completion tools mkbTT and KBCV, the complexity tool TcT, the strategy tool AutoStrat, as well as FORT, an implementation of the decision procedure for the first-order theory for a decidable class of rewrite systems. Besides its applications in research, this software pool has also proved invaluable for teaching, e.g., in multiple editions of the International Summer School on Rewriting.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Logic in Computer Science,F.4.1,K.3.2},
  file = {/Users/nvk/Zotero/storage/5DECPFS2/Winkler and Middeldorp - 2020 - Tools in Term Rewriting for Education.pdf;/Users/nvk/Zotero/storage/XZY3CP5U/2002.html}
}

@article{wolframAppraisalINTERNISTI1995,
  title = {An Appraisal of {{INTERNIST-I}}},
  author = {Wolfram, D. A.},
  year = {1995},
  month = apr,
  journal = {Artificial Intelligence in Medicine},
  volume = {7},
  number = {2},
  pages = {93--116},
  issn = {0933-3657},
  doi = {10.1016/0933-3657(94)00028-Q},
  abstract = {INTERNIST-I was an expert system designed in the early 1970's to diagnose multiple diseases in internal medicine by modelling the behaviour of clinicians. Its form and operation are described, and evaluations of the system are surveyed. The major result of the project was its knowledge base which has been used in successor systems for medical education and clinical use. We also survey the effects of the project through these systems, and conclude that the most successful of them in the near future is likely to be Quick Medical Reference (QMR) when used as an ``electronic textbook'' of medicine.},
  langid = {english},
  keywords = {CPCS,Diagnostic assistant,Diagnostic expert system,Electronic textbook,Internal medicine,INTERNIST-I,QMR},
  file = {/Users/nvk/Zotero/storage/KFWIB65Z/093336579400028Q.html}
}

@article{wolskaSymbolDeclarationsMathematical2010,
  title = {Symbol {{Declarations}} in {{Mathematical Writing}}},
  author = {Wolska, Magdalena and Grigore, Mihai},
  year = {2010},
  pages = {119--127},
  publisher = {{Masaryk University Press(Brno, Czech Republic)}},
  langid = {english},
  file = {/Users/nvk/Zotero/storage/NX4X5VSI/221086.html}
}

@misc{WordEmbeddingsDocument,
  title = {From Word Embeddings to Document Similarities for Improved Information Retrieval in Software Engineering | {{Proceedings}} of the 38th {{International Conference}} on {{Software Engineering}}},
  howpublished = {https://dl.acm.org/doi/abs/10.1145/2884781.2884862?casa\_token=EOZ-5jopi6QAAAAA\%3Az3mDj-vC6bPGoH2aTfGr2UF6mln84iFyjOxF6OZ2c2SmIgEnNEx0rHi2olTJ4i51c6S6Y\_fQI02kIg},
  file = {/Users/nvk/Zotero/storage/RL6IMJDU/2884781.html}
}

@misc{WordEmbeddingsPart2016,
  title = {On Word Embeddings - {{Part}} 1},
  year = {2016},
  month = apr,
  journal = {Sebastian Ruder},
  abstract = {Word embeddings popularized by word2vec are pervasive in current NLP applications. The history of word embeddings, however, goes back a lot further. This post explores the history of word embeddings in the context of language modelling.},
  howpublished = {https://ruder.io/word-embeddings-1/},
  langid = {english},
  file = {/Users/nvk/Zotero/storage/CYH8Q9UC/index.html}
}

@article{wuArtificialNeuralNetworks1997,
  title = {Artificial Neural Networks for Molecular Sequence Analysis},
  author = {Wu, Cathy H.},
  year = {1997},
  month = jan,
  journal = {Computers \& Chemistry},
  series = {Open {{Problems}} of {{Computational Molecular Biology}}},
  volume = {21},
  number = {4},
  pages = {237--256},
  issn = {0097-8485},
  doi = {10.1016/S0097-8485(96)00038-1},
  abstract = {Artificial neural networks provide a unique computing architecture whose potential has attracted interest from researchers across different disciplines. As a technique for computational analysis, neural network technology is very well suited for the analysis of molecular sequence data. It has been applied successfully to a variety of problems, ranging from gene identification, to protein structure prediction and sequence classification. This article provides an overview of major neural network paradigms, discusses design issues, and reviews current applications in DNA/RNA and protein sequence analysis.},
  langid = {english},
  keywords = {artificial neural networks,back-propagation,gene identification,protein structure prediction,sequence analysis,sequence classification},
  file = {/Users/nvk/Zotero/storage/PNQDZSYY/Wu - 1997 - Artificial neural networks for molecular sequence .pdf;/Users/nvk/Zotero/storage/Z29CAYMA/S0097848596000381.html}
}

@article{xiaDemotingRacialBias2020,
  title = {Demoting {{Racial Bias}} in {{Hate Speech Detection}}},
  author = {Xia, Mengzhou and Field, Anjalie and Tsvetkov, Yulia},
  year = {2020},
  month = may,
  journal = {arXiv:2005.12246 [cs]},
  eprint = {2005.12246},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {In current hate speech datasets, there exists a high correlation between annotators' perceptions of toxicity and signals of African American English (AAE). This bias in annotated training data and the tendency of machine learning models to amplify it cause AAE text to often be mislabeled as abusive/offensive/hate speech with a high false positive rate by current hate speech classifiers. In this paper, we use adversarial training to mitigate this bias, introducing a hate speech classifier that learns to detect toxic sentences while demoting confounds corresponding to AAE texts. Experimental results on a hate speech dataset and an AAE dataset suggest that our method is able to substantially reduce the false positive rate for AAE text while only minimally affecting the performance of hate speech classification.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/nvk/Zotero/storage/VX7S6HFN/Xia et al. - 2020 - Demoting Racial Bias in Hate Speech Detection.pdf;/Users/nvk/Zotero/storage/CRWQBD2E/2005.html}
}

@article{xiaMNSIMSimulationPlatform2018,
  title = {{{MNSIM}}: {{Simulation Platform}} for {{Memristor-Based Neuromorphic Computing System}}},
  shorttitle = {{{MNSIM}}},
  author = {Xia, Lixue and Li, Boxun and Tang, Tianqi and Gu, Peng and Chen, Pai-Yu and Yu, Shimeng and Cao, Yu and Wang, Yu and Xie, Yuan and Yang, Huazhong},
  year = {2018},
  month = may,
  journal = {IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems},
  volume = {37},
  number = {5},
  pages = {1009--1022},
  issn = {1937-4151},
  doi = {10.1109/TCAD.2017.2729466},
  abstract = {Memristor-based computation provides a promising solution to boost the power efficiency of the neuromorphic computing system. However, a behavior-level memristor-based neuromorphic computing simulator, which can model the performance and realize an early stage design space exploration, is still missing. In this paper, we propose a simulation platform for the memristor-based neuromorphic system, called MNSIM. A hierarchical structure for memristor-based neuromorphic computing accelerator is proposed to provides flexible interfaces for customization. A detailed reference design is provided for large-scale applications. A behavior-level computing accuracy model is incorporated to evaluate the computing error rate affected by interconnect lines and nonideal device factors. Experimental results show that MNSIM achieves over 7000 times speed-up than SPICE simulation. MNSIM can optimize the design and estimate the tradeoff relationships among different performance metrics for users.},
  keywords = {behavior-level computing accuracy model,behavior-level memristor-based neuromorphic computing system,circuit simulation,Computational modeling,Computer architecture,Design optimization,early stage design space exploration,energy efficiency,error rate computation,flexible interfaces,hierarchical structure,Integrated circuit modeling,interconnect lines,memristor circuits,memristors,Memristors,Microprocessors,MNSIM,neural chips,neural network,neuromorphic computing accelerator,neuromorphic computing simulator,Neuromorphics,Neurons,nonideal device factors,numerical simulation,performance metrics,SPICE simulation,SPICE simulation platform},
  file = {/Users/nvk/Zotero/storage/4MFBL2YV/Xia et al. - 2018 - MNSIM Simulation Platform for Memristor-Based Neu.pdf;/Users/nvk/Zotero/storage/MEMM2QRG/7984877.html}
}

@article{xiaoImprovingBugLocalization2019,
  title = {Improving Bug Localization with Word Embedding and Enhanced Convolutional Neural Networks},
  author = {Xiao, Yan and Keung, Jacky and Bennin, Kwabena E. and Mi, Qing},
  year = {2019},
  month = jan,
  journal = {Information and Software Technology},
  volume = {105},
  pages = {17--29},
  issn = {0950-5849},
  doi = {10.1016/j.infsof.2018.08.002},
  abstract = {Context: Automatic localization of buggy files can speed up the process of bug fixing to improve the efficiency and productivity of software quality assurance teams. Useful semantic information is available in bug reports and source code, but it is usually underutilized by existing bug localization approaches. Objective: To improve the performance of bug localization, we propose DeepLoc, a novel deep learning-based model that makes full use of semantic information. Method: DeepLoc is composed of an enhanced convolutional neural network (CNN) that considers bug-fixing recency and frequency, together with word-embedding and feature-detecting techniques. DeepLoc uses word embeddings to represent the words in bug reports and source files that retain their semantic information, and different CNNs to detect features from them. DeepLoc is evaluated on over 18,500 bug reports extracted from AspectJ, Eclipse, JDT, SWT, and Tomcat projects. Results: The experimental results show that DeepLoc achieves 10.87\%\textendash 13.4\% higher MAP (mean average precision) than conventional CNN. DeepLoc outperforms four current state-of-the-art approaches (DeepLocator, HyLoc, LR+WE, and BugLocator) in terms of Accuracy@k (the percentage of bug reports for which at least one real buggy file is located within the top k rank), MAP, and MRR (mean reciprocal rank) using less computation time. Conclusion: DeepLoc is capable of automatically connecting bug reports to the corresponding buggy files and achieves better performance than four state-of-the-art approaches based on a deep understanding of semantics in bug reports and source code.},
  langid = {english},
  keywords = {Bug localization,Convolutional neural network,Deep learning,Semantic information,TF-IDF,Word embedding},
  file = {/Users/nvk/Zotero/storage/K8D77CE6/S0950584918301654.html}
}

@inproceedings{xiaSwitchedInputPower2016,
  title = {Switched by Input: Power Efficient Structure for {{RRAM-based}} Convolutional Neural Network},
  shorttitle = {Switched by Input},
  booktitle = {Proceedings of the 53rd {{Annual Design Automation Conference}}},
  author = {Xia, Lixue and Tang, Tianqi and Huangfu, Wenqin and Cheng, Ming and Yin, Xiling and Li, Boxun and Wang, Yu and Yang, Huazhong},
  year = {2016},
  month = jun,
  series = {{{DAC}} '16},
  pages = {1--6},
  publisher = {{Association for Computing Machinery}},
  address = {{Austin, Texas}},
  doi = {10.1145/2897937.2898101},
  abstract = {Convolutional Neural Network (CNN) is a powerful technique widely used in computer vision area, which also demands much more computations and memory resources than traditional solutions. The emerging metal-oxide resistive random-access memory (RRAM) and RRAM crossbar have shown great potential on neuromorphic applications with high energy efficiency. However, the interfaces between analog RRAM crossbars and digital peripheral functions, namely Analog-to-Digital Converters (ADCs) and Digital-to-Analog Converters (DACs), consume most of the area and energy of RRAM-based CNN design due to the large amount of intermediate data in CNN. In this paper, we propose an energy efficient structure for RRAM-based CNN. Based on the analysis of data distribution, a quantization method is proposed to transfer the intermediate data into 1 bit and eliminate DACs. An energy efficient structure using input data as selection signals is proposed to reduce the ADC cost for merging results of multiple crossbars. The experimental results show that the proposed method and structure can save 80\% area and more than 95\% energy while maintaining the same or comparable classification accuracy of CNN on MNIST.},
  isbn = {978-1-4503-4236-0},
  file = {/Users/nvk/Zotero/storage/TLUK2SL7/Xia et al. - 2016 - Switched by input power efficient structure for R.pdf}
}

@article{xuQuestionAnsweringFreebase2016,
  title = {Question {{Answering}} on {{Freebase}} via {{Relation Extraction}} and {{Textual Evidence}}},
  author = {Xu, Kun and Reddy, Siva and Feng, Yansong and Huang, Songfang and Zhao, Dongyan},
  year = {2016},
  month = jun,
  journal = {arXiv:1603.00957 [cs]},
  eprint = {1603.00957},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {Existing knowledge-based question answering systems often rely on small annotated training data. While shallow methods like relation extraction are robust to data scarcity, they are less expressive than the deep meaning representation methods like semantic parsing, thereby failing at answering questions involving multiple constraints. Here we alleviate this problem by empowering a relation extraction method with additional evidence from Wikipedia. We first present a neural network based relation extractor to retrieve the candidate answers from Freebase, and then infer over Wikipedia to validate these answers. Experiments on the WebQuestions question answering dataset show that our method achieves an F\_1 of 53.3\%, a substantial improvement over the state-of-the-art.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/nvk/Zotero/storage/VMJFSBWR/Xu et al. - 2016 - Question Answering on Freebase via Relation Extrac.pdf;/Users/nvk/Zotero/storage/LW9AW24A/1603.html}
}

@article{xuQuestionAnsweringFreebase2016a,
  title = {Question {{Answering}} on {{Freebase}} via {{Relation Extraction}} and {{Textual Evidence}}},
  author = {Xu, Kun and Reddy, Siva and Feng, Yansong and Huang, Songfang and Zhao, Dongyan},
  year = {2016},
  month = jun,
  journal = {arXiv:1603.00957 [cs]},
  eprint = {1603.00957},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {Existing knowledge-based question answering systems often rely on small annotated training data. While shallow methods like relation extraction are robust to data scarcity, they are less expressive than the deep meaning representation methods like semantic parsing, thereby failing at answering questions involving multiple constraints. Here we alleviate this problem by empowering a relation extraction method with additional evidence from Wikipedia. We first present a neural network based relation extractor to retrieve the candidate answers from Freebase, and then infer over Wikipedia to validate these answers. Experiments on the WebQuestions question answering dataset show that our method achieves an F\_1 of 53.3\%, a substantial improvement over the state-of-the-art.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/nvk/Zotero/storage/CLU34KE9/Xu et al. - 2016 - Question Answering on Freebase via Relation Extrac.pdf;/Users/nvk/Zotero/storage/A5T6H7QB/1603.html}
}

@inproceedings{yangDesignAccurateStochastic2017,
  title = {Design of Accurate Stochastic Number Generators with Noisy Emerging Devices for Stochastic Computing},
  booktitle = {2017 {{IEEE}}/{{ACM International Conference}} on {{Computer-Aided Design}} ({{ICCAD}})},
  author = {Yang, Meng and Hayes, John P. and Fan, Deliang and Qian, Weikang},
  year = {2017},
  month = nov,
  pages = {638--644},
  issn = {1558-2434},
  doi = {10.1109/ICCAD.2017.8203837},
  abstract = {Stochastic computing (SC) is an unconventional computing paradigm that operates on stochastic bit streams. It has gained attention recently because of the very low area and power needs of its computing core. SC relies on stochastic number generators (SNGs) to map input binary numbers to stochastic bit streams. A conventional SNG comprises a random number source (RNS), typically an LFSR, and a comparator. It needs far more area and power than the SC core, offsetting the latter's main advantages. To mitigate this problem, SNGs employing emerging nanoscale devices such as memristors and spintronic devices have been proposed. However, these devices tend to have large errors in their output probabilities due to unpredictable variations in their fabrication processes and noise in their control signals. We present a novel method of exploiting such devices to design a highly accurate SNG. It is built around an RNS that generates uniformly distributed random numbers under ideal (nominal) conditions. It also has a novel error-cancelling probability conversion circuit (ECPCC) that guarantees very high accuracy in the output probability under realistic conditions when the RNS is subject to errors. An ECPCC can also be used to generate maximally correlated stochastic streams, a useful property for some applications.},
  keywords = {accurate stochastic number generators,circuit CAD,computing core,ECPCC,error-cancelling probability conversion circuit,Generators,highly accurate SNG,input binary numbers,Logic gates,maximally correlated stochastic streams,memristors,Memristors,nanoscale devices,Noise measurement,noisy emerging devices,output probability,probability,Programming,random number generation,random number source,random numbers,RNS,SNGs,spintronic devices,stochastic bit streams,stochastic computing,stochastic processes,Stochastic processes,Switches,unconventional computing paradigm},
  file = {/Users/nvk/Zotero/storage/M4PEQ43S/Yang et al. - 2017 - Design of accurate stochastic number generators wi.pdf;/Users/nvk/Zotero/storage/D95K4NP8/8203837.html}
}

@inproceedings{yangWikiQAChallengeDataset2015,
  title = {{{WikiQA}}: {{A Challenge Dataset}} for {{Open-Domain Question Answering}}},
  shorttitle = {{{WikiQA}}},
  booktitle = {Proceedings of the 2015 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}}},
  author = {Yang, Yi and Yih, Wen-tau and Meek, Christopher},
  year = {2015},
  month = sep,
  pages = {2013--2018},
  publisher = {{Association for Computational Linguistics}},
  address = {{Lisbon, Portugal}},
  doi = {10.18653/v1/D15-1237},
  file = {/Users/nvk/Zotero/storage/T99WSE68/Yang et al. - 2015 - WikiQA A Challenge Dataset for Open-Domain Questi.pdf}
}

@inproceedings{yangWikiQAChallengeDataset2015a,
  title = {{{WikiQA}}: {{A Challenge Dataset}} for {{Open-Domain Question Answering}}},
  shorttitle = {{{WikiQA}}},
  booktitle = {Proceedings of the 2015 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}}},
  author = {Yang, Yi and Yih, Wen-tau and Meek, Christopher},
  year = {2015},
  month = sep,
  pages = {2013--2018},
  publisher = {{Association for Computational Linguistics}},
  address = {{Lisbon, Portugal}},
  doi = {10.18653/v1/D15-1237},
  file = {/Users/nvk/Zotero/storage/3HIKNWEF/Yang et al. - 2015 - WikiQA A Challenge Dataset for Open-Domain Questi.pdf}
}

@article{yasunagaTopicEqJointTopic2019,
  title = {{{TopicEq}}: {{A Joint Topic}} and {{Mathematical Equation Model}} for {{Scientific Texts}}},
  shorttitle = {{{TopicEq}}},
  author = {Yasunaga, Michihiro and Lafferty, John},
  year = {2019},
  month = apr,
  journal = {arXiv:1902.06034 [cs, stat]},
  eprint = {1902.06034},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {Scientific documents rely on both mathematics and text to communicate ideas. Inspired by the topical correspondence between mathematical equations and word contexts observed in scientific texts, we propose a novel topic model that jointly generates mathematical equations and their surrounding text (TopicEq). Using an extension of the correlated topic model, the context is generated from a mixture of latent topics, and the equation is generated by an RNN that depends on the latent topic activations. To experiment with this model, we create a corpus of 400K equation-context pairs extracted from a range of scientific articles from arXiv, and fit the model using a variational autoencoder approach. Experimental results show that this joint model significantly outperforms existing topic models and equation models for scientific texts. Moreover, we qualitatively show that the model effectively captures the relationship between topics and mathematics, enabling novel applications such as topic-aware equation generation, equation topic inference, and topic-aware alignment of mathematical symbols and words.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Information Retrieval,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/nvk/Zotero/storage/QURXPKG8/Yasunaga and Lafferty - 2019 - TopicEq A Joint Topic and Mathematical Equation M.pdf;/Users/nvk/Zotero/storage/PFRKABT9/1902.html}
}

@article{yasunagaTopicEqJointTopic2019a,
  title = {{{TopicEq}}: {{A Joint Topic}} and {{Mathematical Equation Model}} for {{Scientific Texts}}},
  shorttitle = {{{TopicEq}}},
  author = {Yasunaga, Michihiro and Lafferty, John D.},
  year = {2019},
  month = jul,
  journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
  volume = {33},
  number = {01},
  pages = {7394--7401},
  issn = {2374-3468},
  doi = {10.1609/aaai.v33i01.33017394},
  copyright = {Copyright (c) 2019 Association for the Advancement of Artificial Intelligence},
  langid = {english},
  file = {/Users/nvk/Zotero/storage/68M25BZ2/Yasunaga and Lafferty - 2019 - TopicEq A Joint Topic and Mathematical Equation M.pdf}
}

@article{yavitsInDataVsNearData,
  title = {In-{{Data}} vs. {{Near-Data Processing}}: {{The Case}} for {{Processing}} in {{Resistive CAM}}},
  author = {Yavits, Leonid and Kaplan, Roman and Ginosar, Ran},
  pages = {13},
  abstract = {Near-data in-storage processing research has been gaining momentum in recent years. Typical processing-in-storage architecture places a single or several processing cores inside the storage and allows data processing without transferring it to the host CPU. Since this approach replicates von Neumann architecture inside storage, it is exposed to the problems faced by von Neumann architecture, especially the bandwidth wall. We present a novel processing-in-storage system based on Resistive Content Addressable Memory (RCAM). RCAM functions simultaneously as a storage and a massively parallel associative processor. RCAM processing-in-storage resolves the bandwidth wall faced by conventional processing-in-storage architectures by keeping the computing inside the storage arrays, thus implementing indata, rather than near-data, processing. We show that RCAM based processing-in-storage architecture may outperform existing in-storage designs and accelerator based designs. RCAM processing-in-storage implementation of K-means achieves speedup of 4.6\textemdash 68 relative to CPU, GPU and FPGA based solutions. For K-Nearest Neighbors, RCAM processing-in-storage achieves speedup of 17.9\textemdash 17,470 and for Smith-Waterman sequence alignment it reaches speedup of almost 5 over a GPU cluster based solution.},
  langid = {english},
  file = {/Users/nvk/Zotero/storage/325YEL3W/Yavits et al. - In-Data vs. Near-Data Processing The Case for Pro.pdf}
}

@article{yePenroseMathematicalNotation,
  title = {Penrose: {{From Mathematical Notation}} to {{Beautiful Diagrams}}},
  author = {Ye, Katherine and Ni, Wode and Krieger, Max and Ma'ayan, Dor and Wise, Jenna and Aldrich, Jonathan and Sunshine, Joshua and Crane, Keenan},
  volume = {39},
  number = {4},
  pages = {16},
  langid = {english},
  file = {/Users/nvk/Zotero/storage/LXAQZP5K/Ye et al. - Penrose From Mathematical Notation to Beautiful D.pdf}
}

@inproceedings{yihValueSemanticParse2016,
  title = {The {{Value}} of {{Semantic Parse Labeling}} for {{Knowledge Base Question Answering}}},
  booktitle = {Proceedings of the 54th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}} ({{Volume}} 2: {{Short Papers}})},
  author = {Yih, Wen-tau and Richardson, Matthew and Meek, Chris and Chang, Ming-Wei and Suh, Jina},
  year = {2016},
  month = aug,
  pages = {201--206},
  publisher = {{Association for Computational Linguistics}},
  address = {{Berlin, Germany}},
  doi = {10.18653/v1/P16-2033},
  file = {/Users/nvk/Zotero/storage/5RK4SHSY/Yih et al. - 2016 - The Value of Semantic Parse Labeling for Knowledge.pdf}
}

@inproceedings{yihValueSemanticParse2016a,
  title = {The {{Value}} of {{Semantic Parse Labeling}} for {{Knowledge Base Question Answering}}},
  booktitle = {Proceedings of the 54th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}} ({{Volume}} 2: {{Short Papers}})},
  author = {Yih, Wen-tau and Richardson, Matthew and Meek, Chris and Chang, Ming-Wei and Suh, Jina},
  year = {2016},
  month = aug,
  pages = {201--206},
  publisher = {{Association for Computational Linguistics}},
  address = {{Berlin, Germany}},
  doi = {10.18653/v1/P16-2033},
  file = {/Users/nvk/Zotero/storage/4EVFRCBI/Yih et al. - 2016 - The Value of Semantic Parse Labeling for Knowledge.pdf}
}

@article{yinComparativeStudyCNN2017,
  title = {Comparative {{Study}} of {{CNN}} and {{RNN}} for {{Natural Language Processing}}},
  author = {Yin, Wenpeng and Kann, Katharina and Yu, Mo and Sch{\"u}tze, Hinrich},
  year = {2017},
  month = feb,
  journal = {arXiv:1702.01923 [cs]},
  eprint = {1702.01923},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {Deep neural networks (DNN) have revolutionized the field of natural language processing (NLP). Convolutional neural network (CNN) and recurrent neural network (RNN), the two main types of DNN architectures, are widely explored to handle various NLP tasks. CNN is supposed to be good at extracting position-invariant features and RNN at modeling units in sequence. The state of the art on many NLP tasks often switches due to the battle between CNNs and RNNs. This work is the first systematic comparison of CNN and RNN on a wide range of representative NLP tasks, aiming to give basic guidance for DNN selection.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/nvk/Zotero/storage/E3MPTSP7/Yin et al. - 2017 - Comparative Study of CNN and RNN for Natural Langu.pdf;/Users/nvk/Zotero/storage/GIQHZLBJ/1702.html}
}

@article{yinComparativeStudyCNN2017a,
  title = {Comparative {{Study}} of {{CNN}} and {{RNN}} for {{Natural Language Processing}}},
  author = {Yin, Wenpeng and Kann, Katharina and Yu, Mo and Sch{\"u}tze, Hinrich},
  year = {2017},
  month = feb,
  journal = {arXiv:1702.01923 [cs]},
  eprint = {1702.01923},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {Deep neural networks (DNN) have revolutionized the field of natural language processing (NLP). Convolutional neural network (CNN) and recurrent neural network (RNN), the two main types of DNN architectures, are widely explored to handle various NLP tasks. CNN is supposed to be good at extracting position-invariant features and RNN at modeling units in sequence. The state of the art on many NLP tasks often switches due to the battle between CNNs and RNNs. This work is the first systematic comparison of CNN and RNN on a wide range of representative NLP tasks, aiming to give basic guidance for DNN selection.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/nvk/Zotero/storage/TC4D5RPE/Yin et al. - 2017 - Comparative Study of CNN and RNN for Natural Langu.pdf;/Users/nvk/Zotero/storage/K5IRDF4N/1702.html}
}

@inproceedings{youssefExplorationsUseWord2019,
  title = {Explorations into the {{Use}} of {{Word Embedding}} in {{Math Search}} and {{Math Semantics}}},
  booktitle = {Intelligent {{Computer Mathematics}}},
  author = {Youssef, Abdou and Miller, Bruce R.},
  editor = {Kaliszyk, Cezary and Brady, Edwin and Kohlhase, Andrea and Sacerdoti Coen, Claudio},
  year = {2019},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {291--305},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-030-23250-4_20},
  abstract = {Word embedding, which represents individual words with semantically rich numerical vectors, has made it possible to successfully apply deep learning to NLP tasks such as semantic role modeling, question answering, and machine translation. As math text consists of natural text as well as math expressions that similarly exhibit linear correlation and contextual characteristics, word embedding can be applied to math documents as well. On the other hand, math terms also show characteristics (e.g., abstractions) that are different from textual words. Accordingly, it is worthwhile to explore the use and effectiveness of word embedding in math language processing and MKM.In this paper, we present exploratory investigations of math embedding by testing it on some basic tasks such as (1) math-term similarity, (2) analogy, (3) basic numerical concept-modeling using a novel approach based on computing the (weighted) centroid of the keywords that characterize a concept, and (4) math search, especially query expansion using the weighted centroid of the query keywords and then expanding the query with new keywords that are most similar to the centroid. Due to lack of benchmarks, our investigations were done using carefully selected illustrations on the DLMF. We draw from our investigations some general observations and lessons that form a trajectory for future statistically significant testing on large benchmarks. Our preliminary results and observations show that math embedding holds much promise but also point to the need for more robust embedding.},
  isbn = {978-3-030-23250-4},
  langid = {english},
  file = {/Users/nvk/Zotero/storage/I6KT92XD/Youssef and Miller - 2019 - Explorations into the Use of Word Embedding in Mat.pdf}
}

@inproceedings{youssefExplorationsUseWord2019a,
  title = {Explorations into the {{Use}} of {{Word Embedding}} in {{Math Search}} and {{Math Semantics}}},
  booktitle = {Intelligent {{Computer Mathematics}}},
  author = {Youssef, Abdou and Miller, Bruce R.},
  editor = {Kaliszyk, Cezary and Brady, Edwin and Kohlhase, Andrea and Sacerdoti Coen, Claudio},
  year = {2019},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {291--305},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-030-23250-4_20},
  abstract = {Word embedding, which represents individual words with semantically rich numerical vectors, has made it possible to successfully apply deep learning to NLP tasks such as semantic role modeling, question answering, and machine translation. As math text consists of natural text as well as math expressions that similarly exhibit linear correlation and contextual characteristics, word embedding can be applied to math documents as well. On the other hand, math terms also show characteristics (e.g., abstractions) that are different from textual words. Accordingly, it is worthwhile to explore the use and effectiveness of word embedding in math language processing and MKM.In this paper, we present exploratory investigations of math embedding by testing it on some basic tasks such as (1) math-term similarity, (2) analogy, (3) basic numerical concept-modeling using a novel approach based on computing the (weighted) centroid of the keywords that characterize a concept, and (4) math search, especially query expansion using the weighted centroid of the query keywords and then expanding the query with new keywords that are most similar to the centroid. Due to lack of benchmarks, our investigations were done using carefully selected illustrations on the DLMF. We draw from our investigations some general observations and lessons that form a trajectory for future statistically significant testing on large benchmarks. Our preliminary results and observations show that math embedding holds much promise but also point to the need for more robust embedding.},
  isbn = {978-3-030-23250-4},
  langid = {english},
  file = {/Users/nvk/Zotero/storage/ITMMAGHL/Youssef and Miller - 2019 - Explorations into the Use of Word Embedding in Mat.pdf}
}

@inproceedings{youssefPartofMathTaggingApplications2017,
  title = {Part-of-{{Math Tagging}} and {{Applications}}},
  booktitle = {Intelligent {{Computer Mathematics}}},
  author = {Youssef, Abdou},
  editor = {Geuvers, Herman and England, Matthew and Hasan, Osman and Rabe, Florian and Teschke, Olaf},
  year = {2017},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {356--374},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-319-62075-6_25},
  abstract = {Nearly all of the recent mathematical literature, and much of the old literature, are online and mostly in natural-language form. Therefore, math content processing presents some of the same challenges faced in natural language processing (NLP), such as math disambiguation and math semantics determination. These challenges must be surmounted to enable more effective math knowledge management, math knowledge discovery, automated presentation-to-computation (P2C) conversion, and automated math reasoning. To meet this goal, considerable math language processing (MLP) technology is needed.This project aims to advance MLP by developing (1) a sophisticated part-of-math (POM) tagger, (2) math-sense disambiguation techniques along with supporting Machine-Learning (ML) based MLP algorithms, and (3) semantics extraction from, and enrichment of, math expressions. Specifically, the project first created an evolving tagset for math terms and expressions, and is developing a general-purpose POM tagger. The tagger works in several scans and interacts with other MLP algorithms that will be developed in this project. In the first scan of an input math document, each math term and some sub-expressions are tagged with two kinds of tags. The 1st1st1\^\textbackslash mathrm\{st\} kind consists of definite tags (such as operation, relation, numerator, etc.) that the tagger is certain of. The 2nd2nd2\^\textbackslash mathrm\{nd\} kind consists of alternative, tentative features (including alternative roles and meanings) drawn from a knowledge base that has been developed for this project. The 2nd2nd2\^\textbackslash mathrm\{nd\} and 3rd3rd3\^\textbackslash mathrm\{rd\} scan will, in conjunction with some NLP/ML-based algorithms, select the right features from among those alternative features, disambiguate the terms, group subsequences of terms into unambiguous sub-expressions and tag them, and thus derive definite unambiguous semantics of math terms and expressions. The NLP/ML-based algorithms needed for this work will be another part of this project. These include math topic modeling, math context modeling, math document classification (into various standard areas of math), and definition-harvesting algorithms.The project will create significant new concepts and techniques that will advance knowledge in two respects. First, the tagger, math disambiguation techniques, and NLP/ML-based algorithms, though they correspond to NLP and ML counterparts, will be quite novel because math expressions are radically different from natural language. Second, the project outcomes will enable the development of new advanced applications such as: (1) techniques for computer-aided semantic enrichment of digital math libraries; (2) automated P2C conversion of math expressions from natural form to (i) a machine-computable form and (ii) a formal form suitable for automated reasoning; (3) math question-answering capabilities at the manuscript level and collection level; (4) richer math UIs; and (5) more accurate math optical character recognition.},
  isbn = {978-3-319-62075-6},
  langid = {english},
  keywords = {Math Expressions,Math Language,Math Terms,Tagset,Topic Model}
}

@inproceedings{youssefRolesMathSearch2006,
  title = {Roles of {{Math Search}} in {{Mathematics}}},
  booktitle = {Mathematical {{Knowledge Management}}},
  author = {Youssef, Abdou},
  editor = {Borwein, Jonathan M. and Farmer, William M.},
  year = {2006},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {2--16},
  publisher = {{Springer}},
  address = {{Berlin, Heidelberg}},
  doi = {10.1007/11812289_2},
  abstract = {Math-aware fine-grain search is expected to be widely available. A key question is what roles it can play in mathematics. It will be argued that, besides finding information, math search can help advance and manage mathematical knowledge. This paper will present the short-term goals and state of the art of math-aware fine-grain search. Afterwards, it will focus on how math search can help advance and manage mathematical knowledge, and discuss what needs to be done to fulfill those roles, emphasizing two key components. The first is similarity search, and how it applies to (1) discovering and drawing upon connections between different fields, and (2) proof development. The second is math metadata, which math search will surely encourage and benefit from, and which will be pivotal to mathematical knowledge management.},
  isbn = {978-3-540-37106-9},
  langid = {english},
  keywords = {Digital Library,Math Expression,Mathematical Knowledge,Parse Tree,Search System},
  file = {/Users/nvk/Zotero/storage/9FUWQPWG/Youssef - 2006 - Roles of Math Search in Mathematics.pdf}
}

@inproceedings{zanibbiMultiStageMathFormula2016,
  title = {Multi-{{Stage Math Formula Search}}: {{Using Appearance-Based Similarity Metrics}} at {{Scale}}},
  shorttitle = {Multi-{{Stage Math Formula Search}}},
  booktitle = {Proceedings of the 39th {{International ACM SIGIR}} Conference on {{Research}} and {{Development}} in {{Information Retrieval}}},
  author = {Zanibbi, Richard and Davila, Kenny and Kane, Andrew and Tompa, Frank Wm.},
  year = {2016},
  month = jul,
  series = {{{SIGIR}} '16},
  pages = {145--154},
  publisher = {{Association for Computing Machinery}},
  address = {{New York, NY, USA}},
  doi = {10.1145/2911451.2911512},
  abstract = {When using a mathematical formula for search (query-by-expression), the suitability of retrieved formulae often depends more upon symbol identities and layout than deep mathematical semantics. Using a Symbol Layout Tree representation for formula appearance, we propose the Maximum Subtree Similarity (MSS) for ranking formulae based upon the subexpression whose symbols and layout best match a query formula. Because MSS is too expensive to apply against a complete collection, the Tangent-3 system first retrieves expressions using an inverted index over symbol pair relationships, ranking hits using the Dice coefficient; the top-k formulae are then re-ranked by MSS. Tangent-3 obtains state-of-the-art performance on the NTCIR-11 Wikipedia formula retrieval benchmark, and is efficient in terms of both space and time. Retrieval systems for other graphical forms, including chemical diagrams, flowcharts, figures, and tables, may benefit from adopting this approach.},
  isbn = {978-1-4503-4069-4},
  keywords = {inverted index,mathematical information retrieval (MIR),query-by-expression,subtree similarity},
  file = {/Users/nvk/Zotero/storage/HTX3ED2J/Zanibbi et al. - 2016 - Multi-Stage Math Formula Search Using Appearance-.pdf}
}

@article{zanibbiNTCIR12MathIRTask2016,
  title = {{{NTCIR-12 MathIR Task Overview}}},
  author = {Zanibbi, Richard and Aizawa, Akiko and Kohlhase, Michael},
  year = {2016},
  pages = {10},
  abstract = {We present an overview of the NTCIR-12 MathIR Task, dedicated to information access for mathematical content. The MathIR task makes use of two corpora. The first corpus contains excerpts from technical articles in the arXiv, while the second corpus contains English Wikipedia articles. For each corpus, there were two subtasks. Three subtasks contain queries with keywords and formulae (arXiv-main, Wikimain, and arXiv-simto), while the fourth considers isolated formula queries (Wiki-formula). In this overview paper, we summarize the task design, corpora, submitted runs, results, and the approaches used by participating groups.},
  langid = {english},
  file = {/Users/nvk/Zotero/storage/7A7LS2G9/Zanibbi et al. - 2016 - NTCIR-12 MathIR Task Overview.pdf}
}

@inproceedings{zanibbiOverviewARQMath20202020,
  title = {Overview of {{ARQMath}} 2020: {{CLEF Lab}} on {{Answer Retrieval}} for {{Questions}} on {{Math}}},
  shorttitle = {Overview of {{ARQMath}} 2020},
  booktitle = {Experimental {{IR Meets Multilinguality}}, {{Multimodality}}, and {{Interaction}}},
  author = {Zanibbi, Richard and Oard, Douglas W. and Agarwal, Anurag and Mansouri, Behrooz},
  editor = {Arampatzis, Avi and Kanoulas, Evangelos and Tsikrika, Theodora and Vrochidis, Stefanos and Joho, Hideo and Lioma, Christina and Eickhoff, Carsten and N{\'e}v{\'e}ol, Aur{\'e}lie and Cappellato, Linda and Ferro, Nicola},
  year = {2020},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {169--193},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-030-58219-7_15},
  abstract = {The ARQMath Lab at CLEF considers finding answers to new mathematical questions among posted answers on a community question answering site (Math Stack Exchange). Queries are question posts held out from the searched collection, each containing both text and at least one formula. This is a challenging task, as both math and text may be needed to find relevant answer posts. ARQMath also includes a formula retrieval sub-task: individual formulas from question posts are used to locate formulae in earlier question and answer posts, with relevance determined considering the context of the post from which a query formula is taken, and the posts in which retrieved formulae appear.},
  isbn = {978-3-030-58219-7},
  langid = {english},
  keywords = {Community Question Answering (CQA),Math formula search,Math-aware search,Mathematical Information Retrieval},
  file = {/Users/nvk/Zotero/storage/QLQVKT9P/Zanibbi et al. - 2020 - Overview of ARQMath 2020 CLEF Lab on Answer Retri.pdf}
}

@article{zanibbiRecognitionRetrievalMathematical2012,
  title = {Recognition and Retrieval of Mathematical Expressions},
  author = {Zanibbi, Richard and Blostein, Dorothea},
  year = {2012},
  month = dec,
  journal = {International Journal on Document Analysis and Recognition (IJDAR)},
  volume = {15},
  number = {4},
  pages = {331--357},
  issn = {1433-2833, 1433-2825},
  doi = {10.1007/s10032-011-0174-4},
  langid = {english},
  file = {/Users/nvk/Zotero/storage/KYSY5V8V/Zanibbi and Blostein - 2012 - Recognition and retrieval of mathematical expressi.pdf}
}

@article{zanibbiTangentSearchEngine2015,
  title = {The {{Tangent Search Engine}}: {{Improved Similarity Metrics}} and {{Scalability}} for {{Math Formula Search}}},
  shorttitle = {The {{Tangent Search Engine}}},
  author = {Zanibbi, Richard and Davila, Kenny and Kane, Andrew and Tompa, Frank},
  year = {2015},
  month = jul,
  journal = {arXiv:1507.06235 [cs]},
  eprint = {1507.06235},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {With the ever-increasing quantity and variety of data worldwide, the Web has become a rich repository of mathematical formulae. This necessitates the creation of robust and scalable systems for Mathematical Information Retrieval, where users search for mathematical information using individual formulae (query-by-expression) or a combination of keywords and formulae. Often, the pages that best satisfy users' information needs contain expressions that only approximately match the query formulae. For users trying to locate or re-find a specific expression, browse for similar formulae, or who are mathematical non-experts, the similarity of formulae depends more on the relative positions of symbols than on deep mathematical semantics.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Information Retrieval,H.2.4,H.3.3,H.3.4},
  file = {/Users/nvk/Zotero/storage/45ZQ2CTV/Zanibbi et al. - 2015 - The Tangent Search Engine Improved Similarity Met.pdf}
}

@article{zhaIMECFullyMorphable2017,
  title = {{{IMEC}}: {{A Fully Morphable In-Memory Computing Fabric Enabled}} by {{Resistive Crossbar}}},
  shorttitle = {{{IMEC}}},
  author = {Zha, Yue and Li, Jing},
  year = {2017},
  month = jul,
  journal = {IEEE Computer Architecture Letters},
  volume = {16},
  number = {2},
  pages = {123--126},
  issn = {1556-6064},
  doi = {10.1109/LCA.2017.2672558},
  abstract = {In this paper, we propose a fully morphable In-MEmory Computing (IMEC) fabric to better implement the concept of processing inside memory (PIM). Enabled by emerging nonvolatile memory, i.e., RRAM and its monolithic 3D integration, IMEC can be configured into one or a combination of four distinct functions, 1) logic, 2) ternary content addressable memory, 3) memory, and 4) interconnect. Thus, IMEC exploits a continuum of PIM capabilities across the whole spectrum, ranging from 0 percent (pure data storage) to 100 percent (pure compute engine), or intermediate states in between. IMEC can be modularly integrated into the DDRx memory subsystem, communicating with processors by the ordinary DRAM commands. Additionally, to reduce the programming burden, we provide a complete framework to compile applications written in high-level programming language (e.g., OpenCL) onto IMEC. This framework also enables code portability across different platforms for heterogeneous computing. By using this framework, several benchmarks are mapped onto IMEC for evaluating its performance, energy and resource utilization. The simulation results show that, IMEC reduces the energy consumption by 99.6 percent, and achieves \$644\textbackslash times\$ speedup, compared to a baseline CPU system. We further compare IMEC with FPGA architecture, and demonstrate that the performance improvement is not simply obtained by replacing SRAM cells with denser RRAM cells.},
  keywords = {Decoding,Energy efficiency,energy-efficiency computing,Field programmable gate arrays,Non-volatile memory,Nonvolatile memory,processing-in-memory,Program processors,TCAM},
  file = {/Users/nvk/Zotero/storage/UTGJMWRE/Zha and Li - 2017 - IMEC A Fully Morphable In-Memory Computing Fabric.pdf;/Users/nvk/Zotero/storage/RJ2BVHUS/7862129.html}
}

@book{zhaiTextDataManagement2016,
  title = {Text {{Data Management}} and {{Analysis}}: {{A Practical Introduction}} to {{Information Retrieval}} and {{Text Mining}}},
  shorttitle = {Text {{Data Management}} and {{Analysis}}},
  author = {Zhai, ChengXiang and Massung, Sean},
  year = {2016},
  month = jun,
  publisher = {{Morgan \& Claypool}},
  abstract = {Recent years have seen a dramatic growth of natural language text data, including web pages, news articles, scientific literature, emails, enterprise documents, and social media such as blog articles, forum posts, product reviews, and tweets. This has led to an increasing demand for powerful software tools to help people analyze and manage vast amounts of text data effectively and efficiently. Unlike data generated by a computer system or sensors, text data are usually generated directly by humans, and are accompanied by semantically rich content. As such, text data are especially valuable for discovering knowledge about human opinions and preferences, in addition to many other kinds of knowledge that we encode in text. In contrast to structured data, which conform to well-defined schemas (thus are relatively easy for computers to handle), text has less explicit structure, requiring computer processing toward understanding of the content encoded in text. The current technology of natural language processing has not yet reached a point to enable a computer to precisely understand natural language text, but a wide range of statistical and heuristic approaches to analysis and management of text data have been developed over the past few decades. They are usually very robust and can be applied to analyze and manage text data in any natural language, and about any topic.  This book provides a systematic introduction to all these approaches, with an emphasis on covering the most useful knowledge and skills required to build a variety of practically useful text information systems. The focus is on text mining applications that can help users analyze patterns in text data to extract and reveal useful knowledge. Information retrieval systems, including search engines and recommender systems, are also covered as supporting technology for text mining applications. The book covers the major concepts, techniques, and ideas in text data mining and information retrieval from a practical viewpoint, and includes many hands-on exercises designed with a companion software toolkit (i.e., MeTA) to help readers learn how to apply techniques of text mining and information retrieval to real-world text data and how to experiment with and improve some of the algorithms for interesting application tasks. The book can be used as a textbook for a computer science undergraduate course or a reference book for practitioners working on relevant problems in analyzing and managing text data.},
  googlebooks = {0zq0DAAAQBAJ},
  isbn = {978-1-970001-18-1},
  langid = {english},
  keywords = {Computers / Database Administration \& Management,Computers / System Administration / Storage \& Retrieval}
}

@article{zhangEnablingHighlyEfficient2019,
  title = {Enabling {{Highly Efficient Capsule Networks Processing Through A PIM-Based Architecture Design}}},
  author = {Zhang, Xingyao and Song, Shuaiwen Leon and Xie, Chenhao and Wang, Jing and Zhang, Weigong and Fu, Xin},
  year = {2019},
  month = nov,
  journal = {arXiv:1911.03451 [cs]},
  eprint = {1911.03451},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {In recent years, the CNNs have achieved great successes in the image processing tasks, e.g., image recognition and object detection. Unfortunately, traditional CNN's classification is found to be easily misled by increasingly complex image features due to the usage of pooling operations, hence unable to preserve accurate position and pose information of the objects. To address this challenge, a novel neural network structure called Capsule Network has been proposed, which introduces equivariance through capsules to significantly enhance the learning ability for image segmentation and object detection. Due to its requirement of performing a high volume of matrix operations, CapsNets have been generally accelerated on modern GPU platforms that provide highly optimized software library for common deep learning tasks. However, based on our performance characterization on modern GPUs, CapsNets exhibit low efficiency due to the special program and execution features of their routing procedure, including massive unshareable intermediate variables and intensive synchronizations, which are very difficult to optimize at software level. To address these challenges, we propose a hybrid computing architecture design named \textbackslash textit\{PIM-CapsNet\}. It preserves GPU's on-chip computing capability for accelerating CNN types of layers in CapsNet, while pipelining with an off-chip in-memory acceleration solution that effectively tackles routing procedure's inefficiency by leveraging the processing-in-memory capability of today's 3D stacked memory. Using routing procedure's inherent parallellization feature, our design enables hierarchical improvements on CapsNet inference efficiency through minimizing data movement and maximizing parallel processing in memory.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Distributed; Parallel; and Cluster Computing,Computer Science - Hardware Architecture,Computer Science - Machine Learning},
  file = {/Users/nvk/Zotero/storage/YS7EZZKT/Zhang et al. - 2019 - Enabling Highly Efficient Capsule Networks Process.pdf;/Users/nvk/Zotero/storage/RYZMIBYF/1911.html}
}

@article{zhaoLongShortTermMemory2019,
  title = {Long {{Short-Term Memory Network Design}} for {{Analog Computing}}},
  author = {Zhao, Zhou and Srivastava, Ashok and Peng, Lu and Chen, Qing},
  year = {2019},
  month = feb,
  journal = {ACM Journal on Emerging Technologies in Computing Systems},
  volume = {15},
  number = {1},
  pages = {1--27},
  issn = {1550-4832, 1550-4840},
  doi = {10.1145/3289393},
  langid = {english},
  file = {/Users/nvk/Zotero/storage/UVMGVAXZ/Zhao et al. - 2019 - Long Short-Term Memory Network Design for Analog C.pdf}
}

@inproceedings{zhaReconfigurableInmemoryComputing2016,
  title = {Reconfigurable In-Memory Computing with Resistive Memory Crossbar},
  booktitle = {Proceedings of the 35th {{International Conference}} on {{Computer-Aided Design}}},
  author = {Zha, Yue and Li, Jing},
  year = {2016},
  month = nov,
  series = {{{ICCAD}} '16},
  pages = {1--8},
  publisher = {{Association for Computing Machinery}},
  address = {{Austin, Texas}},
  doi = {10.1145/2966986.2967069},
  abstract = {Driven by recent advances in resistive random-access memory (RRAM), there have been growing interests in exploring alternative computing concept, i.e., in-memory processing, to address the classical von Neumann bottlenecks. Despite of their great promise in improving performance and energy efficiency, most existing works are built on the inherent matrix-vector multiplication capability of RRAM crossbar structure, and thus lack the flexibility to adapt to future market/technology induced changes in data-intensive applications. To address these challenges, we propose an in-memory reconfigurable architecture based on RRAM crossbar structure. For the first time, it achieves a full programmability across computation and storage, and thereby provides more flexibilities of partitioning the hardware resources based on applications' needs. We further develop two complete CAD design flows to facilitate development of applications written in hardware description languages (HDLs) for our architecture, based on: 1) adaption from existing tool set developed for FPGA, 2) a custom tool design optimized towards the new architecture. Our experiments show that, both design flows are effective in exploiting flexible resources offered by our architecture and thus achieves better efficiency than state-of-art FPGAs (30\% improvement in performance with 66\% reduction in area). In addition, compared to adapted design flow, our custom design flow achieves speedup by 3.3\texttimes, and further improves mapping quality.},
  isbn = {978-1-4503-4466-1},
  keywords = {in-memory computing,reconfigurable,RRAM},
  file = {/Users/nvk/Zotero/storage/SEXGP333/Zha and Li - 2016 - Reconfigurable in-memory computing with resistive .pdf}
}

@inproceedings{zhongAcceleratingSubstructureSimilarity2020,
  title = {Accelerating {{Substructure Similarity Search}} for {{Formula Retrieval}}},
  booktitle = {Advances in {{Information Retrieval}}},
  author = {Zhong, Wei and Rohatgi, Shaurya and Wu, Jian and Giles, C. Lee and Zanibbi, Richard},
  editor = {Jose, Joemon M. and Yilmaz, Emine and Magalh{\~a}es, Jo{\~a}o and Castells, Pablo and Ferro, Nicola and Silva, M{\'a}rio J. and Martins, Fl{\'a}vio},
  year = {2020},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {714--727},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-030-45439-5_47},
  abstract = {Formula retrieval systems using substructure matching are effective, but suffer from slow retrieval times caused by the complexity of structure matching. We present a specialized inverted index and rank-safe dynamic pruning algorithm for faster substructure retrieval. Formulas are indexed from their Operator Tree (OPT) representations. Our model is evaluated using the NTCIR-12 Wikipedia Formula Browsing Task and a new formula corpus produced from Math StackExchange posts. Our approach preserves the effectiveness of structure matching while allowing queries to be executed in real-time.},
  isbn = {978-3-030-45439-5},
  langid = {english},
  keywords = {Dynamic pruning,Math information retrieval,Query processing optimization},
  file = {/Users/nvk/Zotero/storage/8GTBMRB6/Zhong et al. - 2020 - Accelerating Substructure Similarity Search for Fo.pdf}
}

@inproceedings{zhongAcceleratingSubstructureSimilarity2020a,
  title = {Accelerating {{Substructure Similarity Search}} for {{Formula Retrieval}}},
  booktitle = {Advances in {{Information Retrieval}}},
  author = {Zhong, Wei and Rohatgi, Shaurya and Wu, Jian and Giles, C. Lee and Zanibbi, Richard},
  year = {2020},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {714--727},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-030-45439-5_47},
  abstract = {Formula retrieval systems using substructure matching are effective, but suffer from slow retrieval times caused by the complexity of structure matching. We present a specialized inverted index and rank-safe dynamic pruning algorithm for faster substructure retrieval. Formulas are indexed from their Operator Tree (OPT) representations. Our model is evaluated using the NTCIR-12 Wikipedia Formula Browsing Task and a new formula corpus produced from Math StackExchange posts. Our approach preserves the effectiveness of structure matching while allowing queries to be executed in real-time.},
  isbn = {978-3-030-45439-5},
  langid = {english},
  keywords = {Dynamic pruning,Math information retrieval,Query processing optimization},
  file = {/Users/nvk/Zotero/storage/KGK8HRCW/Zhong et al. - 2020 - Accelerating Substructure Similarity Search for Fo.pdf}
}

@inproceedings{zhongStructuralSimilaritySearch2019,
  title = {Structural {{Similarity Search}} for {{Formulas Using Leaf-Root Paths}} in {{Operator Subtrees}}},
  booktitle = {Advances in {{Information Retrieval}}},
  author = {Zhong, Wei and Zanibbi, Richard},
  editor = {Azzopardi, Leif and Stein, Benno and Fuhr, Norbert and Mayr, Philipp and Hauff, Claudia and Hiemstra, Djoerd},
  year = {2019},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {116--129},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-030-15712-8_8},
  abstract = {We present a new search method for mathematical formulas based on Operator Trees (OPTs) representing the application of operators to operands. Our method provides (1) a simple indexing scheme using OPT leaf-root paths, (2) practical matching of the K largest common subexpressions, and (3) scoring matched OPT subtrees by counting nodes corresponding to visible symbols, weighting operators lower than operands. Using the largest common subexpression (K = 1), we outperform existing formula search engines for non-wildcard queries on the NTCIR-12 Wikipedia Formula Browsing Task. Stronger results are obtained when using additional subexpressions for scoring. Without parallelization or pruning, our system has practical execution times with low variance when compared to other state-of-the-art formula search engines.},
  isbn = {978-3-030-15712-8},
  langid = {english},
  keywords = {Formula search,Mathematical Information Retrieval,Similarity search,Subexpression matching},
  file = {/Users/nvk/Zotero/storage/5I82MKRB/Zhong and Zanibbi - 2019 - Structural Similarity Search for Formulas Using Le.pdf}
}

@article{zhouCLSTMNeuralNetwork2015,
  title = {A {{C-LSTM Neural Network}} for {{Text Classification}}},
  author = {Zhou, Chunting and Sun, Chonglin and Liu, Zhiyuan and Lau, Francis C. M.},
  year = {2015},
  month = nov,
  journal = {arXiv:1511.08630 [cs]},
  eprint = {1511.08630},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {Neural network models have been demonstrated to be capable of achieving remarkable performance in sentence and document modeling. Convolutional neural network (CNN) and recurrent neural network (RNN) are two mainstream architectures for such modeling tasks, which adopt totally different ways of understanding natural languages. In this work, we combine the strengths of both architectures and propose a novel and unified model called C-LSTM for sentence representation and text classification. C-LSTM utilizes CNN to extract a sequence of higher-level phrase representations, and are fed into a long short-term memory recurrent neural network (LSTM) to obtain the sentence representation. C-LSTM is able to capture both local features of phrases as well as global and temporal sentence semantics. We evaluate the proposed architecture on sentiment classification and question classification tasks. The experimental results show that the C-LSTM outperforms both CNN and LSTM and can achieve excellent performance on these tasks.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/nvk/Zotero/storage/YE4GAPCH/Zhou et al. - 2015 - A C-LSTM Neural Network for Text Classification.pdf;/Users/nvk/Zotero/storage/7SZHT5QC/1511.html}
}

@inproceedings{zuninoAnalogImplementationSoftMax2002,
  title = {Analog Implementation of the {{SoftMax}} Function},
  booktitle = {2002 {{IEEE International Symposium}} on {{Circuits}} and {{Systems}}. {{Proceedings}} ({{Cat}}. {{No}}.{{02CH37353}})},
  author = {Zunino, R. and Gastaldo, P.},
  year = {2002},
  month = may,
  volume = {2},
  pages = {II-II},
  doi = {10.1109/ISCAS.2002.1010938},
  abstract = {The paper describes the analog implementation of the SoftMax function by using CMOS circuitry. First, an optimization-based strategy is presented, which fits technology requirements to the desired accuracy in the SoftMax-mapping. Then circuit solutions are described to both the approximation of the exp(x) function and the normalizing ratio. These optimized design aspects improve overall effectiveness, reduce VLSI complexity by exploiting inherent parallelism, and ultimately limit overall power consumption.},
  keywords = {analogue processing circuits,circuit optimisation,circuit solutions,Circuits,CMOS analog implementation,CMOS analogue integrated circuits,Complexity theory,Costs,current-mode circuits,current-mode CMOS circuit,Design optimization,Energy consumption,exp(x) function approximation,function approximation,Hardware,inherent parallelism,integrated circuit design,nonlinear signal processing,normalizing ratio,optimization-based strategy,optimized design aspects,Polynomials,power consumption,Samarium,SoftMax function,technology requirements,Transfer functions,Very large scale integration,VLSI complexity},
  file = {/Users/nvk/Zotero/storage/IHJB22S9/1010938.html}
}


@article{meurer2017sympy,
 title = {SymPy: symbolic computing in Python},
 author = {Meurer, Aaron and Smith, Christopher P. and Paprocki, Mateusz and \v{C}ert\'{i}k, Ond\v{r}ej and Kirpichev, Sergey B. and Rocklin, Matthew and Kumar, Amit and Ivanov, Sergiu and Moore, Jason K. and Singh, Sartaj and Rathnayake, Thilina and Vig, Sean and Granger, Brian E. and Muller, Richard P. and Bonazzi, Francesco and Gupta, Harsh and Vats, Shivam and Johansson, Fredrik and Pedregosa, Fabian and Curry, Matthew J. and Terrel, Andy R. and Rou\v{c}ka, \v{S}t\v{e}p\'{a}n and Saboo, Ashutosh and Fernando, Isuru and Kulal, Sumith and Cimrman, Robert and Scopatz, Anthony},
 year = 2017,
 month = Jan,
 keywords = {Python, Computer algebra system, Symbolics},
 abstract = {
            SymPy is an open-source computer algebra system written in pure Python. It is built with a focus on extensibility and ease of use, through both interactive and programmatic applications. These characteristics have led SymPy to become a popular symbolic library for the scientific Python ecosystem. This paper presents the architecture of SymPy, a description of its features, and a discussion of select submodules. The supplementary material provides additional examples and further outlines details of the architecture and features of SymPy.
         },
 volume = 3,
 pages = {e103},
 journal = {PeerJ Computer Science},
 issn = {2376-5992},
 url = {https://doi.org/10.7717/peerj-cs.103},
 doi = {10.7717/peerj-cs.103}
}

@inproceedings{mikolovEfficientEstimationWord2013,
  title={Efficient Estimation of Word Representations in Vector Space},
  author={Tomas Mikolov and Kai Chen and Gregory S. Corrado and Jeffrey Dean},
  booktitle={ICLR},
  year={2013}
}

@article{krstovski2018equation,
  title={Equation embeddings},
  author={Krstovski, Kriste and Blei, David M},
  journal={arXiv preprint arXiv:1803.09123},
  year={2018}
}

@article{sutskever2014sequence,
  title={Sequence to sequence learning with neural networks},
  author={Sutskever, Ilya and Vinyals, Oriol and Le, Quoc V},
  journal={Advances in neural information processing systems},
  volume={27},
  year={2014}
}

@article{vaswani2017attention,
  title={Attention is all you need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}

@article{kiros2015skip,
  title={Skip-thought vectors},
  author={Kiros, Ryan and Zhu, Yukun and Salakhutdinov, Russ R and Zemel, Richard and Urtasun, Raquel and Torralba, Antonio and Fidler, Sanja},
  journal={Advances in neural information processing systems},
  volume={28},
  year={2015}
}

@inproceedings{allamanis2017learning,
	title={Learning continuous semantic representations of symbolic expressions},
	author={Allamanis, Miltiadis and Chanthirasegaran, Pankajan and Kohli, Pushmeet and Sutton, Charles},
	booktitle={International Conference on Machine Learning},
	pages={80--88},
	year={2017},
	organization={PMLR}
}

@article{Kingma2015AdamAM,
  title={Adam: A Method for Stochastic Optimization},
  author={Diederik P. Kingma and Jimmy Ba},
  journal={CoRR},
  year={2015},
  volume={abs/1412.6980}
}

@inproceedings{liu2022eqnet,
  title={EqNet-L: A new representation learning method for mathematical expression},
  author={Liu, Jiaxin},
  booktitle={2022 International Conference on Big Data, Information and Computer Network (BDICN)},
  pages={547--551},
  year={2022},
  organization={IEEE}
}

@inproceedings{larson2013abject,
  title={The Abject Failure of Keyword IR for Mathematics Search: Berkeley at NTCIR-10 Math.},
  author={Larson, Ray R and Reynolds, Chloe and Gey, Fredric C},
  booktitle={NTCIR},
  year={2013}
}

@article{williams1989learning,
  title={A Learning Algorithm for Continually Running Fully Recurrent Neural Networks},
  author={Williams, Ronald J and Zipser, David},
  journal={Neural Computation},
  volume={1},
  number={2},
  pages={270--280},
  year={1989},
  publisher={MIT Press}
}

@inproceedings{koehn2004pharaoh,
  title={Pharaoh: a beam search decoder for phrase-based statistical machine translation models},
  author={Koehn, Philipp},
  booktitle={Conference of the Association for Machine Translation in the Americas},
  pages={115--124},
  year={2004},
  organization={Springer}
}

@article{ion1998mathematical,
  title={Mathematical Markup Language (MathML) Version 3.0},
  author={Ion, Patrick and Miner, Robert and Ausbrooks, Ron and others},
  year={1998}
}

@inproceedings{bahdanau2015neural,
  title={Neural machine translation by jointly learning to align and translate},
  author={Bahdanau, Dzmitry and Cho, Kyung Hyun and Bengio, Yoshua},
  booktitle={3rd International Conference on Learning Representations, ICLR 2015},
  year={2015}
}

@article{kristianto2014extracting,
  title={Extracting textual descriptions of mathematical expressions in scientific papers},
  author={Kristianto, Giovanni Yoko and Aizawa, Akiko and others},
  journal={D-Lib Magazine},
  volume={20},
  number={11},
  pages={9},
  year={2014},
  publisher={Corporation for National Research Initiatives}
}

@inproceedings{lampleDeepLearningSymbolic2019,
  title={Deep Learning For Symbolic Mathematics},
  author={Lample, Guillaume and Charton, Fran{\c{c}}ois},
  booktitle={International Conference on Learning Representations},
  year={2019}
}

@inproceedings{loshchilovDecoupledWeightDecay2019,
  title={Decoupled Weight Decay Regularization},
  author={Loshchilov, Ilya and Hutter, Frank},
  booktitle={International Conference on Learning Representations},
  year={2018}
}

@inproceedings{wangGLUEMultiTaskBenchmark2019,
    title = "{GLUE}: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding",
    author = "Wang, Alex  and
      Singh, Amanpreet  and
      Michael, Julian  and
      Hill, Felix  and
      Levy, Omer  and
      Bowman, Samuel",
    booktitle = "Proceedings of the 2018 {EMNLP} Workshop {B}lackbox{NLP}: Analyzing and Interpreting Neural Networks for {NLP}",
    month = nov,
    year = "2018",
    address = "Brussels, Belgium",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/W18-5446",
    doi = "10.18653/v1/W18-5446",
    pages = "353--355",
    abstract = "Human ability to understand language is \textit{general, flexible, and robust}. In contrast, most NLU models above the word level are designed for a specific task and struggle with out-of-domain data. If we aspire to develop models with understanding beyond the detection of superficial correspondences between inputs and outputs, then it is critical to develop a unified model that can execute a range of linguistic tasks across different domains. To facilitate research in this direction, we present the General Language Understanding Evaluation (GLUE, gluebenchmark.com): a benchmark of nine diverse NLU tasks, an auxiliary dataset for probing models for understanding of specific linguistic phenomena, and an online platform for evaluating and comparing models. For some benchmark tasks, training data is plentiful, but for others it is limited or does not match the genre of the test set. GLUE thus favors models that can represent linguistic knowledge in a way that facilitates sample-efficient learning and effective knowledge-transfer across tasks. While none of the datasets in GLUE were created from scratch for the benchmark, four of them feature privately-held test data, which is used to ensure that the benchmark is used fairly. We evaluate baselines that use ELMo (Peters et al., 2018), a powerful transfer learning technique, as well as state-of-the-art sentence representation models. The best models still achieve fairly low absolute scores. Analysis with our diagnostic dataset yields similarly weak performance over all phenomena tested, with some exceptions.",
}