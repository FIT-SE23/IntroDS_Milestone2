\appendix

\newpage
\section{Model Details}
\label{appendix:model}

\paragraph{Model.}
Let $\mathbf{x} = (x_1, x_2 \dots x_N)$ and $\mathbf{y} = (y_1, y_2 \dots y_M)$ represent an input and output expression pair. Each $x_i$  is first mapped to a static vector $\mathbf{e}_i$. This sequence of vectors is passed to a GRU which produces a hidden state $\mathbf{h}_i$ for each time step $1 \le i \le N$ according to the following equations:
\begin{equation}
    \label{eq:gru}
    \begin{split}
        \mathbf{r}_i &= \sigma (\mathbf{W}_r \mathbf{e}_i + \mathbf{U}_r \mathbf{h}_{i-1})
        \\
        \mathbf{z}_i &= \sigma (\mathbf{W}_z \mathbf{e}_i + \mathbf{U}_z \mathbf{h}_{i-1})
        \\
        \tilde{\mathbf{h}}_i &= \tanh (\mathbf{W} \mathbf{e}_i + \mathbf{U}(\mathbf{r_t \odot \mathbf{h}_{i-1}}))
        \\
        \mathbf{h}_i &= (1 - \mathbf{z}_i) \odot \mathbf{h}_{i - 1} + \mathbf{z}_i \odot \tilde{\mathbf{h}_i}
    \end{split}
\end{equation}
where $\mathbf{r}_i$, $\mathbf{z}_i$, and $\tilde{\mathbf{h}}_i$ are the reset gate, update gate, and proposed state at time step $i$, respectively. $\sigma$ and $\odot$ denote the sigmoid function and element-wise product, respectively. The final hidden state, $\mathbf{h}_N$, depends on the entire input sequence and interpreted as the \textit{continuous vector representation} of the input expression. The decoder generates an output expression conditioned on the encoder hidden states $\mathbf{h}_i$ for $1 \le i \le N$. It first computes a context vector $\mathbf{c}_t$ at time step $t$ according to the following equations:
\begin{equation}
    \begin{split}
        a_{i, t} &= \mathbf{v}_a^T \tanh(\mathbf{W}_a \mathbf{s}_{t - 1} + \mathbf{U}_a \mathbf{h}_i) \\
        % \alpha_{i, t} &= \frac{\exp(a_{i, t})}{\sum_{i=1}^{N} \exp(a_{i, t})} \\
        \mathbf{c}_t &= \sum_{i = 1}^{N} \frac{\exp(a_{i, t})}{\sum_{j=1}^{N} \exp(a_{j, t})} \mathbf{h}_i
    \end{split}
\end{equation}
where $\mathbf{s}_{t}$ represents the decoder hidden state at time step $t$. The context vector $\mathbf{c}_t$ is combined with the static vector representation of the output token predicted at the previous time step as:
\begin{equation}
    \mathbf{d}_t = \mathbf{W}_c [\mathbf{c}_t, \mathbf{o}_{t-1}]
\end{equation}
where $\mathbf{o}_{t}$ represents the static vector representation of $y_t$, the output token at time step $t$. $\mathbf{d}_t$ follows %equation \ref{eq:gru} 
(\ref{eq:gru}) to generate the decoder hidden state $\mathbf{s}_t$. The probability of the output token $y_t$ is defined as:
\begin{equation}
\begin{aligned}
    P(y_t \mid y_1 \dots y_{t-1}, \mathbf{x}) = \textrm{softmax} (\mathbf{W}_o \mathbf{s}_t)
\end{aligned}
\end{equation}

\paragraph{Loss.}
For each time step, the decoder produces a probability distribution of $y_t$. The probability of the output sequence $\mathbf{y}$ is defined as:
\begin{equation}
\begin{aligned}
    P(\mathbf{y} \mid \mathbf{x}) = \prod_{t = 1}^{M} P(y_t \mid y_1 \dots y_{t-1}, \mathbf{x}) 
\end{aligned}
\end{equation}

We maximize the log-likelihood of the output sequence. For an example pair $(\mathbf{x}, \mathbf{y})$, the loss is defined as:
\begin{equation}
    l(\mathbf{x}, \mathbf{y}) = - \sum_{t = 1}^{M} \log P(y_t \mid y_1 \dots y_{t - 1}, \mathbf{x}) 
\end{equation}

\paragraph{Architecture.}
Table \ref{tab:model_dimensions} shows the dimensions of different layers in our architecture shown in Figure~\ref{fig:model_architecture}.

\section{Equivalent Expression Generation}
\label{appendix:eq_exp_generation}
We use SymPy to generate mathematically equivalent expressions for a given expression. We apply the operations shown in Table \ref{tab:equiv_gen_sympy_ops} to a given expression to get mathematically equivalent expressions. The examples shown in the table are intended to give an idea about each operation. The actual behavior of these operations is much more complex. Refer to the SymPy documentation for more details.\footnote{\href{https://docs.sympy.org/latest/tutorial/simplification.html}{https://docs.sympy.org/latest/tutorial/simplification.html}} We use the \texttt{rewrite} function for expressions containing trigonometric or hyperbolic operations. Specifically, we use this function to rewrite an expression containing trigonometric (or hyperbolic) operators in terms of other trigonometric (or hyperbolic) operators.

% \newpage
\section{\semvec{} Datasets}
\label{appendix:semvec_datasets}

\begin{table}[t]
    \centering
    \begin{tabular}{lrr}
        \toprule
        Dataset & \expemba{} & \expembe{} \\
        \midrule
        \textsc{SimpBool8} & 21,604 & 4,440,450 \\
        \textsc{SimpBool10} & 13,081 & 1,448,804 \\
        \textsc{Bool5} & 712 & 17,934 \\
        \textsc{Bool8} & 146,488 & 16,143,072 \\
        \textsc{Bool10} & 25,560 & 3,041,640 \\
        \textsc{SimpBoolL5} & 6,009 & 66,876 \\
        \textsc{BoolL5} & 23,219 & 552,642 \\
        \textsc{SimpPoly5} & 156 & 882 \\
        \textsc{SimpPoly8} & 1,934 & 113,660 \\
        \textsc{SimpPoly10} & 31,143 & 6,731,858 \\
        \textsc{oneV-Poly10} & 767 & 25,590 \\
        \textsc{oneV-Poly13} & 60,128 & 9,958,406 \\
        \textsc{Poly5} & 352 & 1,350 \\
        \textsc{Poly8} & 6,785 & 257,190 \\
        \bottomrule
    \end{tabular}
    \caption{Number of training examples in the transformed training sets of the \semvec{} datasets.}
    \label{tab:semvec_training_size}
\end{table}

\begin{table*}[t]
    \centering
    \begin{tabular}{lll}
        \toprule
        Function & Input(s) & Output \\
        \midrule
        \texttt{simplify} & $\sin(x)^2 + \cos(x)^2$ & 1 \\
        \texttt{expand} & $(x + 1)^2$ & $x^2 + 2x + 1$ \\
        \texttt{factor} & $x^2 + 5x + 6$ & $(x + 2)(x + 3)$ \\
        \texttt{cancel} & $(x^3 + 2x)/x$ & $x^2 + 2$ \\
        \texttt{trigsimp} & $\sin(x) \cot(x)$ & $\cos(x)$ \\
        \texttt{expand\_log} & $\ln(x^2)$ & $2 \ln(x)$ \\
        \texttt{logcombine} & $\ln(x) + \ln(2)$ & $\ln(2x)$ \\
        \texttt{rewrite} & $\sin(x)$, $\cos$ & $\cos(x - \pi/2)$ \\
        {} & $\sinh(x)$, $\tanh$ & $2 \tanh{\left(\frac{x}{2} \right)}(1 - \tanh^{2}{\left(\frac{x}{2} \right)})^{-1}$ \\
        \bottomrule
    \end{tabular}
    \caption{List of SymPy functions with examples that are used to generate equivalent expressions.}
    \label{tab:equiv_gen_sympy_ops}
\end{table*}

\begin{table*}[htb]
    \centering
    \begin{tabular}{llll}
        \toprule
         & Layer & Input & Output \\
        \midrule
        Encoder & Embedding & $B \times S$ & $B \times S \times H$  \\
        & GRU & $B \times S \times H$, $B \times H$ & $B \times S \times H$ \\
        \midrule
        Decoder & Embedding & $B \times T$ & $B \times T \times H$ \\
        & Attention & $B \times S \times H$, $B \times T \times H$ & $B \times T \times H$ \\
        & Linear-1 & $B \times T \times 2H$ & $B \times T \times H$ \\
        & GRU & $B \times T \times H$, $B \times H$ & $B \times T \times H$ \\
        & Linear-2 & $B \times T \times H$ & $B \times T \times V$ \\
        & Softmax & $B \times T \times V$ & $B \times T \times V$ \\
        \bottomrule
    \end{tabular}
    \caption{Input and output dimensions of the layers in our model architecture ($B$: batch size, $S$: input sequence length, $T$: output sequence length, $V$: vocabulary size, $H$: model dimension).}
    \label{tab:model_dimensions}
\end{table*}

Table \ref{tab:semvec_training_size} shows the number of examples in our training set of the \semvec{} datasets. We use the validation and test sets in their original form. To generate $score_k(q)$ for our model, we use the source code provided by \citet{allamanis2017learning}.


Table \ref{tab:semvec_ours_detailed} shows the results of our approach on the \semvec{} datasets. It can be seen that \expemba{} performs much worse than \expembe{} for all values of $H$. This is not surprising as the autoencoder training looks at each expression in isolation and does not utilize the fact that each expression belongs to a particular class. Hence, the representations learned by this model do not capture the semantics of mathematical expressions and only capture the structure.

\begin{table*}[t]
    \centering
    \begin{tabular}{lrrrrrrrrrr}
        \toprule
        Dataset & \multicolumn{5}{c}{\textsc{ExpEmb-A}} & \multicolumn{5}{c}{\textsc{ExpEmb-E}} \\ 
          & { \small $H = 64$ } & { \small $128$ } & { \small $256$ } & { \small $512$ } & { \small $1024$ } & { \small $64$ } & { \small $128$ } & { \small $256$ } & { \small $512$ } & { \small $1024$ } \\
        \midrule
        \textsc{SimpBool8} & 22.7 & 23.4 & 20.7 & 26.6 & 27.1 & 93.2 & 95.5 & 97.2 & 98.8 & 99.5 \\
        \textsc{SimpBool10} & 10.0 & 16.3 & 10.0 & 9.3 & 11.7 & 59.4 & 62.1 & 80.9 & 75.7 & 78.2 \\
        \textsc{Bool5} & 16.3 & 9.1 & 25.1 & 21.3 & 21.3 & 57.1 & 49.5 & 51.9 & 52.3 & 56.9 \\
        \textsc{Bool8} & 11.8 & 10.6 & 15.0 & 22.5 & 19.6 & 98.4 & 99.9 & 100.0 & 100.0 & 100.0 \\
        \textsc{Bool10} & 3.0 & 4.0 & 3.5 & 4.3 & 4.6 & 29.5 & 39.1 & 47.8 & 71.5 & 77.5 \\
        \textsc{SimpBoolL5} & 28.5 & 16.8 & 16.9 & 23.3 & 22.4 & 46.1 & 79.7 & 63.2 & 76.3 & 68.1 \\
        \textsc{BoolL5} & 10.9 & 15.2 & 3.8 & 15.1 & 15.5 & 46.4 & 46.8 & 70.4 & 52.0 & 50.7 \\
        \textsc{SimpPoly5} & 8.3 & 4.2 & 12.5 & 4.2 & 4.2 & 14.6 & 15.6 & 27.1 & 25.0 & 31.2 \\
        \textsc{SimpPoly8} & 31.3 & 28.3 & 26.7 & 28.6 & 29.8 & 82.7 & 95.4 & 97.2 & 96.7 & 91.8 \\
        \textsc{SimpPoly10} & 24.6 & 25.5 & 25.1 & 26.0 & 28.7 & 99.8 & 99.9 & 100.0 & 99.9 & 100.0 \\
        \textsc{oneV-Poly10} & 43.8 & 36.6 & 44.3 & 44.8 & 55.8 & 48.5 & 58.2 & 70.9 & 74.6 & 75.5 \\
        \textsc{oneV-Poly13} & 20.9 & 28.2 & 25.7 & 26.2 & 26.6 & 94.0 & 99.1 & 99.5 & 99.7 & 99.7 \\
        \textsc{Poly5} & 7.8 & 18.8 & 17.9 & 20.5 & 22.0 & 28.9 & 48.1 & 41.9 & 36.8 & 30.6 \\
        \textsc{Poly8} & 8.7 & 18.3 & 18.7 & 19.8 & 18.8 & 41.6 & 64.5 & 69.3 & 76.6 & 76.3 \\
        \bottomrule
    \end{tabular}
    \caption{$score_5(\%)$ achieved by our model on \textsc{UnseenEqClass} of the \semvec{} datasets.}
    \label{tab:semvec_ours_detailed}
\end{table*}

\section{Distance Analysis}
\label{appendix:distance_analysis}
Table \ref{appendix_tab:distance_analysis} shows more examples of the distance analysis as discussed in Section \ref{sec:qual_results}.

\begin{table*}[]
    \centering
    \begin{tabular}{lll}
        \toprule
        Query & \expemba{} & \expembe{} \\ 
        \midrule
        $x^{2} - 8 \sin{\left(x \right)}$ & 1.~$e \left(- x + \sin{\left(x \right)}\right)$ & 1.~$20 x^{2} + 20 \sin{\left(x \right)}$ \\ 
        \addlinespace[0.3em] & 2.~$\left(4 x + 36\right) \cos{\left(x \right)}$ & 2.~$- 2 x^{2} + 3 \sin{\left(x \right)}$ \\ 
        \addlinespace[0.3em] & 3.~$x + \sqrt{7} \cos{\left(x \right)}$ & 3.~$9 x + 12 \sin{\left(x \right)}$ \\ 
        \addlinespace[0.3em] & 4.~$- 6 x + 6 \sin{\left(x \right)}$ & 4.~$x^{2} + \frac{\sin{\left(x \right)}}{20}$ \\ 
        \addlinespace[0.3em] & 5.~$- 8 x + 9 \sin{\left(x \right)}$ & 5.~$80 x^{2} + 160 \sin{\left(x \right)}$ \\ 
        
        \midrule 
        $\cos{\left(x + 1 \right)} + \frac{1}{2}$ & 1.~$32 \tan{\left(x + 1 \right)}$ & 1.~$\frac{\sin{\left(x + 1 \right)}}{2} + \frac{3}{2}$ \\ 
        \addlinespace[0.3em] & 2.~$6 \cos{\left(x + 1 \right)}$ & 2.~$4 x + \cos{\left(x + 1 \right)} + 3$ \\ 
        \addlinespace[0.3em] & 3.~$\tan{\left(x + 1 \right)} + 125$ & 3.~$6 \cos{\left(x + 1 \right)}$ \\ 
        \addlinespace[0.3em] & 4.~$\sinh{\left(x + 1 \right)} + 6$ & 4.~$\frac{4 x \cos{\left(x + 1 \right)}}{5}$ \\ 
        \addlinespace[0.3em] & 5.~$\sqrt{x + 1} - 3$ & 5.~$\cos{\left(x + 4 \right)} + \operatorname{acos}{\left(5 \right)}$ \\

        \midrule
        
        $x \left(x \cos{\left(5 \right)} + \operatorname{asinh}{\left(x \right)}\right)$ & 1.~$x + \left(x + \operatorname{asinh}{\left(5 \right)}\right) \operatorname{asinh}{\left(x \right)}$ & 1.~$x \left(- x \cos{\left(x \right)} + x + \operatorname{acos}{\left(x \right)}\right)$ \\ 
        \addlinespace[0.3em] & 2.~$x \left(x + e^{\operatorname{asinh}{\left(x \right)} + 5}\right)$ & 2.~$x \left(x + \cos{\left(2 \right)}\right) + \operatorname{asin}{\left(x \right)}$ \\ 
        \addlinespace[0.3em] & 3.~$\sqrt{x + \tan{\left(5 \right)}} + \operatorname{asinh}{\left(x \right)}$ & 3.~$x^{2} \cos{\left(x \right)} + x + \operatorname{asin}{\left(x \right)} + 2$ \\ 
        \addlinespace[0.3em] & 4.~$x \sin{\left(5 \right)} + x + \log{\left(x \right)}$ & 4.~$x \sin{\left(1 \right)} + 3 x + \operatorname{acosh}{\left(x \right)}$ \\ 
        \addlinespace[0.3em] & 5.~$\left(\sqrt{2} x + 3\right) \sin{\left(x \right)}$ & 5.~$x \left(x \operatorname{asinh}{\left(3 \right)} + \cos{\left(x \right)}\right)$ \\ 
        
        \midrule
        
        $2 x^{2} \left(x + \operatorname{acos}{\left(2 x \right)} + 5\right)$ & 1.~$4 x^{2} \left(\sqrt{x} + 2 x + 2\right)$ & 1.~$2 x \left(6 x + \operatorname{atanh}{\left(2 x \right)} + 3\right)$ \\ 
        \addlinespace[0.3em] & 2.~$2 x - \sin{\left(\operatorname{asinh}{\left(x \right)} \right)} + 5$ & 2.~$\left(x + 3\right) \left(x + \operatorname{asin}{\left(2 x \right)} + 1\right)$ \\ 
        \addlinespace[0.3em] & 3.~$2 x + \cos{\left(x \right)} + \tan{\left(2 x \right)} + 3$ & 3.~$x^{2} \left(\operatorname{asinh}{\left(2 x \right)} + 2\right)$ \\ 
        \addlinespace[0.3em] & 4.~$x \operatorname{asin}{\left(2 x \right)} + 2 x + 3$ & 4.~$x \left(2 x + \operatorname{atanh}{\left(2 x \right)} - 1\right)$ \\ 
        \addlinespace[0.3em] & 5.~$\frac{\sqrt{x} + 2 x - 5}{x^{4}}$ & 5.~$x \left(x + \sinh{\left(2 x \right)} + 4\right) + x$ \\ 

        \bottomrule
    \end{tabular}
    \caption{Expressions closest to a given query computed using representations generated by \expemba{} and \expembe{}.}
    \label{appendix_tab:distance_analysis}
\end{table*}

\section{Training Details}
\label{appendix:training_details}
For training our models, we consider expressions with a maximum of 512 tokens. We use the AdamW optimizer \citep{loshchilovDecoupledWeightDecay2019} for training our models. 
For the Equivalent Expressions Dataset, a learning rate of $10^{-4}$ and a batch size of 64 are used. The same learning rate and batch size are used for the majority of the \semvec{} datasets, barring the ones mentioned in Table \ref{tab:training_details}. For the dataset with longer equivalent expressions (discussed in Section \ref{sec:limitations}), we use a batch size of 32 with a learning rate of $10^{-4}$.
%We use a learning rate of $10^{-4}$ and a batch size of 64 for the Equivalent Expressions Dataset. For the \semvec{} datasets, we use a learning rate of $10^{-4}$ and a batch size of 64 for all except the ones mentioned in table \ref{tab:training_details}.

We use a single Quadro RTX 6000 GPU for training \expembe{} on the dataset with longer expressions and a single V100 GPU for all other models. For the Equivalent Expressions Dataset, \expemba{} ($H = 128$) takes $\sim$1 hour and \expembe{} ($H = 1024$) takes $\sim$2 hours to train for an epoch. We train both of these models for 20 epochs. For the dataset with longer expressions, the training times for an epoch are $\sim$4 hours and $\sim$20 hours for \expemba{} ($H = 128$) and \expembe{} ($H = 2048$) respectively. %, and we train \expemba{} for 15 epochs and \expembe{} for 10 epochs. The validation and testing combined takes between $\sim$6-10 hours each. Most time during validation and testing is taken by SymPy which uses CPU.
Table \ref{tab:semvec_training_time} shows the approximate training time per epoch (in seconds) for the \semvec{} datasets. %The validation time for all datasets is $< 1.5$ hours except for \textsc{Bool8} and \textsc{oneV-Poly13}. For these datasets, the validation takes $\sim$10 hours each. The test time for all \semvec{} datasets is $< 1$ hour each.

\begin{table*}[]
    \centering
    \begin{tabular}{lrrrr}
        \toprule
        Dataset & Decoder Setting & $H$ & Learning Rate & Batch Size \\
        \midrule
        \textsc{Bool5} & \expemba{} & All & $10^{-4}$ & 8 \\
        \textsc{Bool5} & \expembe{} & 64 & $2 \times 10^{-4}$ & 64 \\
        \textsc{SimpPoly5} & \expemba{} & All & $5 \times 10^{-5}$ & 8 \\
        \textsc{SimpPoly5} & \expembe{} & All & $5 \times 10^{-5}$ & 16 \\
        \textsc{SimpPoly8} & \expemba{} & All & $10^{-4}$ & 16 \\
        \textsc{oneV-Poly10} & \expembe{} & 64 & $2 \times 10^{-4}$ & 64 \\
        \textsc{oneV-Poly10} & \expemba{} & All except 64 & $5 \times 10^{-5}$ & 8 \\
        \textsc{oneV-Poly10} & \expemba{} & 64 & $10^{-4}$ & 8 \\
        \textsc{oneV-Poly13} & \expemba{} & All & $5 \times 10^{-5}$ & 32 \\
        \textsc{Poly5} & \expembe{} & All & $10^{-4}$ & 32 \\
        \textsc{Poly5} & \expemba{} & All & $5 \times 10^{-5}$ & 8 \\
        \bottomrule
    \end{tabular}
    \caption{Datasets with hyperparameters for which the learning rate and batch sizes are different than $10^{-4}$ and 64 respectively.}
    \label{tab:training_details}
\end{table*}

\begin{table*}[]
    \centering
    \begin{tabular}{lrrrrrrrrrr}
        \toprule
        Dataset & \multicolumn{5}{c}{\textsc{ExpEmb-A}} & \multicolumn{5}{c}{\textsc{ExpEmb-E}} \\ 
          & { \small $H = 64$ } & { \small $128$ } & { \small $256$ } & { \small $512$ } & { \small $1024$ } & { \small $64$ } & { \small $128$ } & { \small $256$ } & { \small $512$ } & { \small $1024$ } \\
        \midrule
        \textsc{SimpBool8} & 17 & 19 & 18 & 19 & 21 & 3405 & 1921 & 2164 & 3313 & 2245 \\
        \textsc{SimpBool10} & 12 & 12 & 14 & 12 & 13 & 1141 & 714 & 823 & 1391 & 850 \\
        \textsc{Bool5} & 3 & 3 & 3 & 3 & 3 & 11 & 6 & 7 & 12 & 7 \\
        \textsc{Bool8} & 111 & 127 & 129 & 135 & 119 & 11088 & 9706 & 7087 & 9737 & 12618 \\
        \textsc{Bool10} & 22 & 26 & 25 & 24 & 26 & 2574 & 1504 & 1706 & 2719 & 1782 \\
        \textsc{SimpBoolL5} & 4 & 4 & 4 & 4 & 4 & 37 & 23 & 23 & 38 & 27 \\
        \textsc{BoolL5} & 15 & 15 & 15 & 15 & 15 & 298 & 186 & 187 & 354 & 214 \\
        \textsc{SimpPoly5} & 1 & 1 & 1 & 1 & 1 & 2 & 2 & 2 & 2 & 2 \\
        \textsc{SimpPoly8} & 5 & 5 & 6 & 5 & 6 & 79 & 45 & 47 & 81 & 53 \\
        \textsc{SimpPoly10} & 27 & 26 & 30 & 27 & 27 & 7932 & 3089 & 3140 & 3153 & 3634 \\
        \textsc{oneV-Poly10} & 4 & 4 & 4 & 5 & 5 & 22 & 18 & 13 & 23 & 14 \\
        \textsc{oneV-Poly13} & 112 & 112 & 118 & 117 & 118 & 5958 & 5869 & 6041 & 6177 & 6994 \\
        \textsc{Poly5} & 2 & 2 & 2 & 2 & 2 & 2 & 2 & 2 & 2 & 2 \\
        \textsc{Poly8} & 5 & 5 & 6 & 6 & 6 & 183 & 102 & 103 & 187 & 120 \\
        \bottomrule
    \end{tabular}
    \caption{Approximate training time in seconds per epoch for the \semvec{} datasets.}
    \label{tab:semvec_training_time}
\end{table*}

\section{Performance of Validation Datasets}
For the Equivalent Expressions Dataset, the validation is performed using the model accuracy (as described in Section \ref{sec:equiv_exp_gen_results}) with a beam size of 1. Table \ref{tab:expemb_val_accuracy} shows the accuracy of \expemba{} and \expembe{} on the Equivalent Expressions Dataset and the dataset with longer expressions. Table \ref{tab:semvec_val_results} shows the scores, $score_k(q)$, achieved by our model on the validation sets of the \semvec{} datasets.

\begin{table*}[]
    \centering
    \begin{tabular}{lrrrr}
        \toprule
        Beam Size & \multicolumn{2}{c}{Equivalent Expressions Dataset} & \multicolumn{2}{c}{Dataset with Longer Expressions} \\
        {} & \expemba{} & \expembe{} & \expemba{} & \expembe{} \\
        \midrule
        1 & 1.0000 & 0.6015 & 0.9994 & 0.4341 \\
        \bottomrule
    \end{tabular}
    \caption{Accuracy of \expemba{} and \expembe{} on the validation datasets of the Equivalent Expressions Dataset and the dataset with longer expressions.}
    \label{tab:expemb_val_accuracy}
\end{table*}

\begin{table*}[]
    \centering
    \begin{tabular}{lrrrrrrrrrr}
        \toprule
        Dataset & \multicolumn{5}{c}{\textsc{ExpEmb-A}} & \multicolumn{5}{c}{\textsc{ExpEmb-E}} \\ 
          & { \small $H = 64$ } & { \small $128$ } & { \small $256$ } & { \small $512$ } & { \small $1024$ } & { \small $64$ } & { \small $128$ } & { \small $256$ } & { \small $512$ } & { \small $1024$ } \\
        \midrule
        \textsc{SimpBool8} & 23.8 & 22.5 & 21.4 & 26.0 & 24.1 & 95.4 & 96.9 & 98.0 & 99.4 & 99.6 \\
        \textsc{SimpBool10} & 6.4 & 8.8 & 5.5 & 6.6 & 6.8 & 54.0 & 56.2 & 80.4 & 67.1 & 71.5 \\
        \textsc{Bool5} & 17.9 & 15.6 & 19.2 & 20.8 & 21.5 & 57.4 & 45.7 & 52.8 & 52.4 & 53.2 \\
        \textsc{Bool8} & 11.3 & 11.8 & 13.2 & 16.3 & 15.0 & 98.1 & 99.9 & 100.0 & 100.0 & 100.0 \\
        \textsc{Bool10} & 1.8 & 2.3 & 2.0 & 1.8 & 2.1 & 26.0 & 34.4 & 37.9 & 67.9 & 74.6 \\
        \textsc{SimpBoolL5} & 23.4 & 22.5 & 24.6 & 25.2 & 26.0 & 51.7 & 75.3 & 73.1 & 74.3 & 75.3 \\
        \textsc{BoolL5} & 11.7 & 15.2 & 11.0 & 14.3 & 14.6 & 66.0 & 69.7 & 73.7 & 67.4 & 64.3 \\
        \textsc{SimpPoly5} & 25.9 & 25.9 & 29.6 & 18.5 & 22.2 & 33.3 & 40.7 & 37.0 & 40.7 & 33.3 \\
        \textsc{SimpPoly8} & 15.4 & 15.2 & 14.3 & 15.2 & 12.6 & 50.0 & 74.5 & 91.6 & 80.6 & 78.0 \\
        \textsc{SimpPoly10} & 11.7 & 12.7 & 11.0 & 10.5 & 11.0 & 99.3 & 99.3 & 99.4 & 99.4 & 99.4 \\
        \textsc{oneV-Poly10} & 24.5 & 29.7 & 28.9 & 24.6 & 26.8 & 27.3 & 32.6 & 53.7 & 58.9 & 61.8 \\
        \textsc{oneV-Poly13} & 19.8 & 22.1 & 20.2 & 20.5 & 20.0 & 91.8 & 98.2 & 98.7 & 98.7 & 98.7 \\
        \textsc{Poly5} & 27.8 & 22.2 & 16.7 & 16.7 & 16.7 & 44.4 & 55.6 & 50.0 & 41.7 & 33.3 \\
        \textsc{Poly8} & 10.3 & 14.6 & 15.0 & 15.5 & 12.1 & 61.6 & 82.4 & 81.5 & 85.1 & 84.3 \\
        \bottomrule
    \end{tabular}
    \caption{$score_5(\%)$ achieved by our model on the validation sets of the \semvec{} datasets.}
    \label{tab:semvec_val_results}
\end{table*}
