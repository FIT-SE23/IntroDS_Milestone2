\section{Proposed Approach}
\label{sec:proposed_approach}
%In this work, we are interested in learning representations that can capture semantics of mathematical expressions and not just the structure. 
We frame the problem of embedding mathematical expressions as a sequence-to-sequence learning problem and utilize the encoder-decoder framework. While word2vec-based approaches assume that proximity to a word suggests similarity, we contend that \textbf{for mathematical expressions, equivalence implies similarity}. Furthermore, if we train a sequence-to-sequence model to generate expressions that are mathematically equivalent to the input, the encoder should learn to produce embeddings that map semantically equivalent expressions closer together. Figure \ref{fig:equiv_exp_example} shows an example of this approach. To accomplish this, we need a machine learning model capable of learning equivalent expressions and a dataset of equivalent expressions to train the model (Section \ref{sec:datagen}).

\paragraph{Model.}
In encoder-decoder architectures, the encoder maps an input sequence to a vector. The decoder is conditioned on this vector to generate an output sequence. In our approach, the encoder maps an expression to a vector that is referred to as the \textit{continuous vector representation} of this expression. The decoder uses this representation to generate an output expression. We train our model in two decoder settings: (1) \textit{equivalent expression setting} (\expembe{}), in which the decoder generates an expression that is mathematically equivalent to the input expression, and (2) \textit{autoencoder setting} (\expemba{}), in which the decoder generates the input expression exactly.

%Two expressions $\mathbf{x}_1$ and $\mathbf{x}_2$ are mathematically equivalent if $\mathbf{x}_1 - \mathbf{x}_2 = 0$ and $\mathbf{x}_1 \neq \mathbf{x}_2$. We refer to the autoencoder setting as \expemba{} and the equivalent expression setting as \expembe{} hereafter.

\begin{figure}[h]
    \centering
    \fontsize{7}{10}\selectfont
    \includesvg[scale=0.6]{./img/architecture.drawio.svg}
    \caption{Our model architecture. An input expression (represented in the Polish notation) is fed into the encoder at the bottom as a sequence of tokens. The encoder processes the input and generates the embedding. The hidden states of the encoder are passed to the decoder to generate an output expression.}
    \label{fig:model_architecture}
\end{figure}

There are several choices for modeling encoders and decoders, for example, Recurrent Neural Networks (RNN), Gated Recurrent Unit (GRU), Long Short-Term Memory (LSTM), Convolutional Networks (ConvNet), Transformer, etc. In this work, we use an architecture similar to the one proposed by \citet{kiros2015skip} for learning skip-thought vectors. \textbf{We use GRU to model our encoder and decoder and use the additive attention mechanism \citep{bahdanau2015neural} in the decoder.} The final hidden state of the encoder depends on the entire input sequence and interpreted as the \textit{continuous vector representation} or \textit{embedding} of the input expression. An overview of our model is shown in Figure \ref{fig:model_architecture}. Refer to Appendix \ref{appendix:model} for a mathematical description of the model and dimensions of different layers in our architecture.

% We use with $H$ = 128 (\expemba{}) and $H$ = 1024 (\expembe{}) for the Equivalent Expressions Dataset and $H$ = 64, 128, 256, 512, 1024 (\expemba{} and \expembe{}) for the \semvec{} datasets. We use the AdamW optimizer \citep{loshchilovDecoupledWeightDecay2019} for our experiments. See appendix \ref{appendix:training_details} for details.

\paragraph{Data Formatting.}
Mathematical expressions are typically modeled as trees with nodes describing a variable or an operator \citep{ion1998mathematical}. Since we are using a sequence-to-sequence model, we use the Polish (prefix) notation to convert a tree into a sequence of tokens \citep{lampleDeepLearningSymbolic2019}. As shown in Figure \ref{fig:equiv_exp_example}, this transforms the expression $\frac{\sin(x)}{\cos(x)}$ into the sequence $[\textrm{div}, \sin, x, \cos, x]$. Tokens are encoded as one-hot vectors and passed through an embedding layer before being fed to the encoder or decoder.

\paragraph{Other Details.}
Crucial to the success of our model is the adoption of the teacher forcing algorithm \citep{williams1989learning} during training and the beam search algorithm \citep{koehn2004pharaoh} during validation and testing. Because not all sequences produce valid expression trees, the beam search algorithm is essential in finding the output with minimum loss. As the training of our network progresses, the incidences of output expressions that produce invalid trees become increasingly rare, suggesting that the network is capable of learning mathematical tree structure.
