\section{Related Work}
\label{sec:related}
%answerTODO{}

While information processing for mathematical expressions is still a relatively new field, other groups have attempted to create embedding methods for mathematical content.

Starting with embedding schemes for mathematical \textit{tokens}, \citet{gaoPreliminaryExplorationFormula2017} embedded mathematical symbols using word2vec \citep{mikolovEfficientEstimationWord2013} on the Wikipedia corpus. They created a mathematical token embedding (symbol2vec) in which tokens used in similar contexts ($\sin$ \& $\cos$, $=$ \& $\approx$) were grouped together. Using these symbol embeddings, they extended their approach to embed entire formulae using the paragraph-to-vector approach (formula2vec).
%
More commonly, other approaches have attempted to extract textual descriptors of equations using surrounding text and use pre-trained embeddings of those textual keywords to create an embedding of the equation in question \citep{kristianto2014extracting, schubotzEvaluatingImprovingExtraction2017}.

Expanding token descriptors to represent equations is more complex. \citet{schubotzSemantificationIdentifiersMathematics2016} created equation descriptors by combining the textual keyword descriptors of the mathematical tokens included within an expression. By organizing the equation descriptors into keyword pairs, clustering algorithms grouped equations according to Wikipedia categories. In a similar approach, \citet{kristiantoUtilizingDependencyRelationships2017} viewed equations as dependency relationships between mathematical tokens. Nominal NLP methods were used to extract textual descriptors for these tokens, and the equation was transformed into a dependency graph. A combination of interdependent textual descriptors allowed the authors to derive better formula descriptors that were used by indexers for retrieval tasks.

Alternatively, multiple groups have attempted to represent equations as feature sets, sequences of symbols that partially describe an equationâ€™s visual layout \citep{zanibbiTangentSearchEngine2015, fraserChoosingMathFeatures2018}. \citet{krstovskiEquationEmbeddings2018} used word2vec in two different manners to find equation embeddings. In one approach, they treated an equation as a single token. In their second approach, they treated variables, symbols, and operators as tokens. The equation embedding was computed as the average of the embeddings of these individual tokens. \citet{mansouriTangentCFTEmbeddingModel2019} used a modified version of the latter method that extracted features from both the symbol layout tree and operator tree representations of the expression. \citet{ahmedEquationAttentionRelationship2021} used a complex schema where an equation was embedded using a combination of a message-passing network (to process its graph representation) and residual neural network (to process its visual representation).

Most of these approaches depend on embedding mathematical tokens and expressions using surrounding textual information, effectively establishing a homomorphism between mathematical and textual information. While these approaches have produced crucial initial results, there are still two important limitations that need to be addressed. Firstly, an ideal embedding scheme should be able to process equations without surrounding text, such as in the case of pure math texts like the Digital Library of Mathematical Functions (DLMF) \citep{lozierNISTDigitalLibrary2003}.

Secondly, expressions that have equivalent semantic meaning may be written a multitude of ways (consider $x^{-1} = \frac{1}{x}$ or $\sin(x) = \cos(x-\frac{\pi}{2})$). The embedding method should understand that the expressions are equivalent and produce similar embeddings. \citet{allamanis2017learning} and \citet{liu2022eqnet} have previously proposed \eqnet{} and \eqnetl{}, respectively, for finding semantic representations of simple symbolic expressions. However, these approaches only focus on ensuring that the embeddings of \textit{equivalent} expressions are grouped together. They do not consider or explore semantically similar but non-equivalent expressions. %Embedding semantically similar but non-equivalent expressions remains an open problem.

To this end, we look at mathematical expressions in isolation without any context and propose an approach to semantically represent these expressions in a continuous vector space. Our approach considers semantic similarity in addition to mathematical equivalence.

  
% Expanding these techniques to equation embedding 
% The desire to retrieve similar \cite{krstovskiEquationEmbeddings2018}. 
% % Using the ``distributional hypothesis'' ( 