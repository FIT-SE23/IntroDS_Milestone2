\section{Introduction}
%\answerTODO{}
%While human beings mainly use natural languages, we also convey information using images or sound. Just like modern search engines (Google, Bing, etc.) have achieved high precision retrieval of natural language documents, so too is there a mature industry built around the retrieval of images (Tineye, Google image search, etc.) and sound (Pandora, Shazam, Musicopedia, etc.). 
Despite there being well-established search technologies for most other modes of data, there exist no \textit{major} algorithms for processing mathematical content \citep{larson2013abject}. The first step toward processing new modes of data is to create embedding methods that can transform that information block into a machine-readable format. Most mathematical equation modeling attempts have focused on establishing a homomorphism between an equation and surrounding mathematical text \citep{zanibbiNTCIR12MathIRTask2016, krstovskiEquationEmbeddings2018}. While this approach can help find equations used in similar contexts, it overlooks the following key points: (1) there is a large chunk of data in which equations occur without any context (consider textbooks that contain equations with minimal explanation), and (2) in scientific literature, the same equations may be used in a variety of disciplines with different contexts; encoding equations according to textual context hampers cross-disciplinary retrieval.

We argue that context alone is not sufficient for finding representations of mathematical content, and the embedding models must consider the equation itself. To this end, we present a novel embedding technique that learns to generate representations of mathematical expressions based on semantics. In our proposed approach, \textit{we train a sequence-to-sequence model on equivalent expression pairs} and use the trained encoder to generate vector representations. Figure 1 shows an example of our approach that embeds expressions according to their mathematical semantics producing better clustering, retrieval, and analogy results. The efficacy of our approach is highlighted when compared to an autoencoder. We also compare our embedding model with two existing methods: \eqnet{} \citep{allamanis2017learning} and \eqnetl{} \citep{liu2022eqnet}, further proving our modelâ€™s ability to capture the semantic meaning of an expression.

The contributions of our work are threefold:
\begin{enumerate}
    \itemsep0em
    \item We show that a sequence-to-sequence model can learn to generate expressions that are mathematically equivalent to a given input.
    \item We use the encoder of such a model to generate continuous vector representations of mathematical expressions. These representations capture semantics and not just the structure. These embeddings produce better clustering and retrieval of similar mathematical expressions.
    \item We publish a corpus of equivalent transcendental and algebraic expressions pairs which can be used to develop more complex mathematical embedding approaches.
\end{enumerate}

We end this manuscript with a comprehensive study of the embedding schema detailing its potential for general information processing and retrieval tasks and noting the limitations of our approach. All datasets and source code are available on our project page. \footnote{\href{https://github.com/mlpgroup/expemb}{https://github.com/mlpgroup/expemb}}

\begin{figure*}[t]
    \centering
    \begin{tikzpicture}
        \node[text width=1cm] (a) at (-1.5, 0.7) {$\frac{\sin(x)}{\cos(x)}$};
        
        \node[text width=1.9cm] (b) at (0.7, 0.7) {{\footnotesize
        \begin{forest}
            for tree={s sep=10mm, inner sep=0, l=0}
            [div,
                [$\sin$, [x]],
                [$\cos$, [x]]
            ]
        \end{forest}
        }};
        
        \node[inner sep=0pt] (c) at (8, 0.3) {
            \fontsize{9}{10}\selectfont
            \includesvg[scale=0.7]{./img/equiv_exp_example.drawio.svg}
        };
        
        \coordinate []  (d) at (0.7, -0.4);
        \coordinate []  (e) at (4.0, -0.9);
        
        \draw[-latex] (a) edge (b);
        \draw[-latex] (d) |- (e) node[near end, fill=white] {\footnotesize Polish Notation};
    \end{tikzpicture}
    \caption{Example our model trained on equivalent expression pairs. The encoder and decoder components are shown in green and orange, respectively. Given an input expression $\frac{\sin(x)}{\cos(x)}$, the model learns to generate $\tan(x)$. We use the last hidden state of the encoder as the \textit{continuous vector representation} of the input expression. \textsc{SOE} and \textsc{EOE} are the start and end tokens, and ``div'' is the division operation.}
    \label{fig:equiv_exp_example}
\end{figure*}