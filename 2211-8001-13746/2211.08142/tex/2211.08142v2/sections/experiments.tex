%%%%%%%%%%%%%%%%% Models used for results %%%%%%%%%%%%%%%%%%
%% Autoencoder: 20220611-203103365077
%% Equivalent expression: 20220611-170824504558
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Experiments}
We consider two decoder settings for our experiments:
\begin{itemize}
    \itemsep0em
    \item Equivalent expression setting, or \expembe{}, in which the model is trained on disparate but mathematically equivalent expression pairs.
    \item Autoencoder setting, or \expemba{}, in which the model is trained as an autoencoder to output the same expression as the input.
\end{itemize}
While the focus of this work is to demonstrate the efficacy of \expembe{}, \expemba{} serves as an important contrast, demonstrating that \expembe{} yields representations that better describe \textit{semantics} and are superior for clustering, retrieval, and analogy tasks.

\subsection{Training Details}
\label{sec:training_details}
We use the Transformer architecture with 6 encoder layers, 6 decoder layers, 8 attention heads, and the ReLU activation. The model dimension, indicated by $D_{\text{model}}$, is 512 for experiments in Sections \ref{sec:equiv_exp_gen_results} and \ref{sec:qual_results} and is 64 for experiments in Section \ref{sec:comparison_existing_methods}. The layer normalization is performed before other attention and feedforward operations. We use the Adam optimizer \citep{Kingma2015AdamAM} with a learning rate of $10^{-4}$ and do not consider expressions with more than 256 tokens. Refer to Appendix \ref{appendix:training_details} for more details.

For experiments in Sections \ref{sec:equiv_exp_gen_results} and \ref{sec:qual_results}, we use the Equivalent Expressions Dataset (Section \ref{sec:datagen}). For \expemba{}, we use unique expressions present in the training set. The experiments in Section \ref{sec:comparison_existing_methods} use the datasets created by \citet{allamanis2017learning} (details in the same section).

% In this section, we conduct a series of experiemnts that demonstrate the following qualities of our approach: 
% \begin{enumerate}
%     \item \expemba{} learns to generate the input expression exactly
%     \item \expembe{} learns to generate an expression that is mathematically equivalent to the input expression. The intent behind this experiment is to establish that \expemba{} and \expembe{} perform well on their original tasks.
% \end{enumerate}

%The overall purpose of these experiments is to 

%In this section, we present the results of our experiments on \expemba{} and \expembe{}. For our experiments, we use the Equivalent Expressions Dataset and the datasets introduced by \citet{allamanisLearningContinuousSemantic2017}. The latter is referred to as the \semvec{} datasets hereafter. The datasets are discussed in detail in section \ref{sec:datasets}. We perform quantitative and qualitative evaluations on our models. We first show that for a given input expression (1) \expemba{} learns to generate the input expression exactly and (2) \expembe{} learns to generate an expression that is mathematically equivalent to the input expression. The intent behind this experiment is to establish that \expemba{} and \expembe{} perform well on their original tasks. For this experiment, we use the Equivalent Expressions Dataset. For the next experiment, we use the \semvec{} datasets. The objective of this experiment is to analyze the semantic properties of representations generated by \expemba{} and \expembe{} encoders. The results of these experiments are discussed in section \ref{sec:quant_results}. Furthermore, we plot and compare the vector representations generated by \expemba{} and \expembe{}. We use these representations to find five closest expressions to a given test expression. We also perform embedding algebra on the representations generated by models trained in both of these settings. We use the Equivalent Expressions Dataset for these experiments and discuss the results in section \ref{sec:qual_results}.

% \subsection{Datasets}
% \label{sec:datasets}

% \paragraph{\semvec{} Datasets.}
% Published in \citet{allamanis2017learning}, these datasets contain equivalent expressions from the boolean (\textsc{Bool}) and polynomial (\textsc{Poly}) domains. In these datasets, a class is defined by an expression and all the equivalent expressions belong to the same class. The datasets are split into training, validation, and test sets and contain two test sets: (1) \textsc{SeenEqClass} containing classes that are present in the training set (2) \textsc{UnseenEqClass} containing classes that are not present in the training set. To transform the training set into the input-output format used by \expembe{}, we generate all possible pairs for the expressions belonging to the same class. To limit the size of the generated training set, we select a maximum of 100,000 random pairs from each class. For \expemba{}, we use all the expressions present in the training set. See Appendix \ref{appendix:semvec_datasets} for details.

% \subsection{Training}
% We use the model shown in \ref{fig:model_architecture} for our training. Refer to Appendix \ref{appendix:model_arch} for dimensions of different layers in our architecture. We train \expemba{} with $H = 128$ and \expembe{} with $H = 1024$ on the Equivalent Expressions Dataset. For the \semvec{} datasets, we train both \expemba{} and \expembe{} with $H = 64, 128, 256, 512, 1024$. We use the AdamW optimizer \citep{loshchilovDecoupledWeightDecay2019} for training. See Appendix \ref{appendix:training_details} for details.

\subsection{Equivalent Expression Generation}
\label{sec:equiv_exp_gen_results}

\begin{figure*}[t]
    % \vskip 0.2in
    \begin{center}
    \begin{sc}
    \begin{subfigure}{0.49\textwidth}
        \centering
        \fontsize{6}{7}\selectfont\sffamily
        \includesvg[scale=0.28]{img/expemba_pca_plot.svg}
        \caption{\expemba{}}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.49\textwidth}
        \centering
        \fontsize{6}{7}\selectfont\sffamily
        \includesvg[scale=0.28]{img/expembe_pca_plot.svg}
        \caption{\expembe{}}
    \end{subfigure}
    \caption{Plots for the embedding vectors generated by \expemba{} (autoencoder) and \expembe{} (proposed equivalent-expression method). PCA is used to reduce the dimensionality from 512 to 2. \expembe{} groups expressions with operators of the same type together, indicating its ability to understand semantics, whereas \expemba{} groups expressions mainly based on their visual structure. The interactive versions of these plots are available on our project page.}
    \label{fig:pca_plots}
    \end{sc}
    \end{center}
    % \vskip -0.2in
\end{figure*}

The first objective is to show that \expembe{} can learn to generate equivalent expressions for a given input. To evaluate if two expressions $\mathbf{x}_1$ and $\mathbf{x}_2$ are mathematically equivalent, we simplify their difference $\mathbf{x}_1 - \mathbf{x}_2$ using SymPy and compare it to 0. In this setting, if the model produces an expression that is the same as the input, we do not count it as a model success. There are instances in which SymPy takes significant time to simplify an expression and eventually fails with out-of-memory errors. To handle these cases, we put a threshold on its execution time. If the simplification operation takes more time than the threshold, we count it as a model failure. We also evaluate \expemba{} and verify that it learns to generate the input exactly. This serves as an important contrast to \expembe{} and is useful in understanding how well a sequence-to-sequence model understands the mathematical structure. At inference, we use the beam search algorithm to generate output expressions. As an output can be verified programmatically, we consider all the outputs in the beam. If any of the outputs are correct, we count it as a model success.

\begin{table}[t]
    \caption{Accuracy of \expemba{} and \expembe{} on the Equivalent Expressions Dataset. \expemba{} has an easier task of generating the input at the decoder, while \expembe{} must generate a mathematically equivalent expression.}
    \label{tab:results_autoenc_encdec}
    % \vskip 0.15in
    \begin{center}
    \begin{small}
    \begin{sc}
    \begin{tabular}{lrr}
    \toprule
    Beam Size & \expemba{} & \expembe{} \\
    \midrule
    1 & 0.9996 & 0.7206 \\
    10 & 0.9998 & 0.8118 \\
    50 & 0.9998 & 0.8386 \\
    \bottomrule
    \end{tabular}
    \end{sc}
    \end{small}
    \end{center}
    % \vskip -0.1in
\end{table}

The accuracy of these models is shown in Table \ref{tab:results_autoenc_encdec}. First, we note that \expemba{} is easily able to encode and generate the input expression and achieves a near-perfect accuracy with greedy decoding (beam size = 1). Second, the \expembe{} results demonstrate that generating mathematically equivalent expressions is a significantly harder task. For this setting, greedy decoding does not perform well. We observe an improvement of 11\% with a beam size of 50. However, we also see an increase in the number of invalid prefix expressions being assigned high log probabilities with this beam size. This experiment demonstrates that both \expemba{} and \expembe{} are capable of learning the mathematical structure, and \expembe{} can learn to generate expressions that are mathematically equivalent to the input expression. While \expembe{} achieves a lower accuracy than \expemba{}, we see in Section \ref{sec:qual_results} that the former exhibits some interesting properties and is more useful for retrieval tasks.

\subsection{Embedding Evaluation}
\label{sec:qual_results}
Next, we evaluate the usefulness of the representations generated by the \expembe{} model. Unlike the natural language textual embedding problem \citep{wangGLUEMultiTaskBenchmark2019}, there do not exist standard tests to quantitatively measure the embedding performance of methods built for mathematical expressions. Hence, our analysis in this section must be more qualitative in nature. These experiments show some interesting properties of the representations generated by \expembe{} and \expemba{} and demonstrate the efficacy of the proposed approach.

For these experiments, we use the \textit{trained} \expemba{} and \expembe{} models from Section \ref{sec:equiv_exp_gen_results}, and no further training is performed for these experiments. The data used for the experiments are described in their respective sections.

\begin{table*}[t]
    \caption{Expressions closest to a given query based on the representations generated by \expemba{} and \expembe{}. It is evident that \expembe{} does a better job at learning semantics and the overall structure of the expressions. Refer to Appendix \ref{appendix:distance_analysis} for more examples.}
    \label{tab:distance_analysis}
    % \vskip 0.15in
    \begin{center}
    \begin{small}
    \begin{sc}
    \begin{tabular}{lll}
    \toprule
    Query & \expemba{} & \expembe{} \\         

    \midrule
    \multirow{5}{*}{$4 x^{2} \cos{\left(3 x - 1 \right)}$ } & $4 x^{2} e^{- 3 x - 1}$ & $- 10 x^{2} \cos{\left(x - 10 \right)}$ \\
        & $4 x^{2} e^{2 x - 1}$ & $- 10 x^{2} \cos{\left(x + 10 \right)}$ \\
        & $4 x^{2} e^{- 2 x - 1}$ & $x^{2} \cos{\left(x - 10 \right)}$ \\
        & $4 x^{2} e^{- 3 x - 9}$ & $x^{2} \cos{\left(x + 1 \right)}$ \\
        & $4 x^{2} e^{- x - 1}$ & $x^{2} \cos{\left(x - 30 \right)}$ \\
    
    \midrule
    \multirow{5}{*}{$x^{2} + \log{\left(x \right)} + 1$ } & $\sqrt{x} + x^{2} + \log{\left(x \right)}$ & $x^{2} + \log{\left(x \right)} + 1$ \\
        & $\sqrt{x} + x^{2} + e^{x}$ & $x^{2} + \log{\left(x \right)} + 3$ \\
        & $\sqrt{x} + x^{2} + \operatorname{acos}{\left(x \right)}$ & $x^{2} + \log{\left(x \right)} + 2$ \\
        & $\log{\left(x^{x^{2}} \right)} + \sinh{\left(x \right)}$ & $x^{2} + x + \log{\left(x \right)} + 1$ \\
        & $\log{\left(x^{x^{2}} \right)} + \tan{\left(x \right)}$ & $x^{2} + x + \log{\left(x \right)}$ \\

    \bottomrule
    \end{tabular}
    \end{sc}
    \end{small}
    \end{center}
    % \vskip -0.1in
\end{table*}

\paragraph{Embedding Plots.}
To gauge if similar expressions are clustered in the embedding vector space, we plot and compare the representations generated by the \expembe{} and \expemba{} models. For this experiment, we use a set of 7,000 simple expressions belonging to four categories: hyperbolic, trigonometric, polynomial, and logarithmic/exponential. Each expression is either a polynomial or contains hyperbolic, trigonometric, logarithmic, or exponential operators. Each expression contains only one category of operators. 35\% of these expressions are from the training set and the remaining 65\% are unseen expressions.  Below are a few examples of expressions belonging to each class:
\begin{itemize}
    \itemsep0em
    \item Polynomial: $x^{2} + 2 x + 5$, $2 x + 2$
    \item Trigonometric: $\sin{\left(x \right)} \tan{\left(x \right)}$, $\cos^{5}{\left(4 x \right)}$
    \item Hyperbolic: $\cosh{\left(x - 4 \right)}$, $\sinh{\left(x \cos{\left(2 \right)} \right)}$
    \item Log/Exp: $e^{- 2 x - 4}$, $\log{\left(x + 3 \right)}^{3}$
\end{itemize}

We use Principal Component Analysis (PCA) for dimensionality reduction.\footnote{We use the scikit-learn implementation of PCA with default parameters.}  Figure \ref{fig:pca_plots} shows the plots for \expemba{} and \expembe{}. We observe from these plots that the clusters in the \expembe{} plot are more distinguishable compared to the \expemba{} plot. \expembe{} does a better job at grouping similar expressions together, suggesting its ability to understand semantics. For \expembe{}, there is an overlap between expressions belonging to hyperbolic and logarithmic/exponential classes. This is expected because hyperbolic operators can be written in terms of the exponential operator. Furthermore, \expemba{} focuses on just the structure of expressions and groups together expressions that follow the same structure. For example, representations generated by \expemba{} for $\tan \left( x \frac{\sqrt{2}}{2} \right)$, $x^2 (x^2 - x)$, $\sinh^{-1} \left( x \frac{\sqrt{2}}{2} \right) $, and $\log \left( x \frac{\sqrt{2}}{2} \right)$ are grouped together. On the other hand, representations generated by \expembe{} capture semantics in addition to the syntactic structure.

\paragraph{Distance Analysis.}
To understand the applicability of the embeddings for the information retrieval task, we perform distance analysis on embeddings generated by \expemba{} and \expembe{}. We use all expressions from the training set of the Equivalent Expressions Dataset as the pool of expressions for this experiment. The similarity between two expressions is defined as the inverse of the cosine distance between their embedding vectors. We find the five closest expressions to a given query expression. Table \ref{tab:distance_analysis} shows the results of this experiment. We observe that the closest expressions computed using \expembe{} are more similar to the query in terms of the syntactic structure and operators. On the other hand, \expemba{} focuses on the visual features, like operators, and finds other expressions containing these features. For example, the first query in Table \ref{tab:distance_analysis} consists of polynomial and trigonometric expressions. The closest expressions computed using \expembe{} follow the same structure, whereas \expemba{} seems to put more emphasis on the polynomial multiplier. This behavior is also apparent in the second example. We believe that the ability of \expembe{} to group similar expressions, in terms of operators and the syntactic structure, can prove useful in information retrieval problems where the aim is to find similar expressions to a given query. Refer to Appendix \ref{appendix:distance_analysis} for more examples.

\begin{table*}[t]
    \caption{Examples of embedding algebra on the representations generated by \expemba{} and \expembe{}. The correct predictions are shown in \textbf{bold}.}
    \label{tab:emb_algebra}
    % \vskip 0.15in
    \begin{center}
    \begin{small}
    \begin{sc}
    \begin{tabular}{llllll}
    \toprule
    \multirow{2}{*}{$x_1$} & \multirow{2}{*}{$y_1$} & \multirow{2}{*}{$y_2$} & \multirow{2}{*}{$x_2$ (expected)} & \multicolumn{2}{c}{$x_2$ (predicted)} \\
    {} & {} & {} & {} & \expemba{} & \expembe{} \\
    \midrule
    $\cos{\left(x \right)}$ & $\sin{\left(x \right)}$ & $\csc{\left(x \right)}$ & $\sec{\left(x \right)}$ & \boldmath $\sec{\left(x \right)}$ & \boldmath $\sec{\left(x \right)}$ \\
    $\cos{\left(x \right)}$ & $\sin{\left(x \right)}$ & $\cot{\left(x \right)}$ & $\tan{\left(x \right)}$ &  $\sqrt{x}$ &  $x + \cot{\left(x \right)}$ \\
    $\sin{\left(x \right)}$ & $\cos{\left(x \right)}$ & $\cosh{\left(x \right)}$ & $\sinh{\left(x \right)}$ &  $\tanh{\left(x \right)}$ & \boldmath $\sinh{\left(x \right)}$ \\
    $\sin{\left(x \right)}$ & $\csc{\left(x \right)}$ & $\sec{\left(x \right)}$ & $\cos{\left(x \right)}$ & \boldmath $\cos{\left(x \right)}$ & \boldmath $\cos{\left(x \right)}$ \\
    $\sin{\left(x \right)}$ & $\csc{\left(x \right)}$ & $\cot{\left(x \right)}$ & $\tan{\left(x \right)}$ &  $\cos{\left(x \right)}$ & \boldmath $\tan{\left(x \right)}$ \\
    $\sin{\left(- x \right)}$ & $- \sin{\left(x \right)}$ & $\cos{\left(x \right)}$ & $\cos{\left(- x \right)}$ &  $\sin{\left(x \right)}$ & \boldmath $\cos{\left(- x \right)}$ \\
    $\tan{\left(- x \right)}$ & $- \tan{\left(x \right)}$ & $- \cot{\left(x \right)}$ & $\cot{\left(- x \right)}$ &  $\sinh{\left(- x \right)}$ & \boldmath $\cot{\left(- x \right)}$ \\
    $\sin{\left(- x \right)}$ & $- \sin{\left(x \right)}$ & $- \cot{\left(x \right)}$ & $\cot{\left(- x \right)}$ &  $\tan{\left(- x \right)}$ & \boldmath $\cot{\left(- x \right)}$ \\
    $\sinh{\left(- x \right)}$ & $- \sinh{\left(x \right)}$ & $\cosh{\left(x \right)}$ & $\cosh{\left(- x \right)}$ &  $\operatorname{acosh}{\left(x \right)}$ & \boldmath $\cosh{\left(- x \right)}$ \\
    $\sinh{\left(- x \right)}$ & $- \sinh{\left(x \right)}$ & $- \tanh{\left(x \right)}$ & $\tanh{\left(- x \right)}$ & \boldmath $\tanh{\left(- x \right)}$ & \boldmath $\tanh{\left(- x \right)}$ \\
    $\sinh{\left(- x \right)}$ & $- \sinh{\left(x \right)}$ & $- \cosh{\left(x \right)}$ & $- \cosh{\left(- x \right)}$ &  $\tan{\left(- x \right)}$ &  $\cosh{\left(- x \right)}$ \\
    $\sinh{\left(- x \right)}$ & $- \sinh{\left(x \right)}$ & $\tanh{\left(x \right)}$ & $- \tanh{\left(- x \right)}$ &  $\operatorname{acosh}{\left(x \right)}$ &  $\tanh{\left(- x \right)}$ \\
    $x^{2}$ & $x$ & $\sin{\left(x \right)}$ & $\sin^{2}{\left(x \right)}$ & \boldmath $\sin^{2}{\left(x \right)}$ &  $x^{2} + \sin{\left(x \right)}$ \\
    $x^{2}$ & $x$ & $\cos{\left(x \right)}$ & $\cos^{2}{\left(x \right)}$ & \boldmath $\cos^{2}{\left(x \right)}$ & \boldmath $\cos^{2}{\left(x \right)}$ \\
    $x^{2}$ & $x$ & $\log{\left(x \right)}$ & $\log{\left(x \right)}^{2}$ &  $\log{\left(\sqrt{2} \right)}$ &  $\log{\left(x^{2} \right)}$ \\
    $x^{2}$ & $x$ & $e^{x}$ & $\left(e^{x}\right)^{2}$ &  $x^{4}$ &  $x^{2} e^{x}$ \\
    $e^{x}$ & $x$ & $2 x$ & $e^{2 x}$ & \boldmath $e^{2 x}$ &  $2 e^{x}$ \\
    $x^{2}$ & $x$ & $x + 2$ & $\left(x + 2\right)^{2}$ &  $\tan^{2}{\left(x \right)}$ &  $x^{2} + 2$ \\
    $\log{\left(x \right)}$ & $x$ & $\sin{\left(x \right)}$ & $\log{\left(\sin{\left(x \right)} \right)}$ & \boldmath $\log{\left(\sin{\left(x \right)} \right)}$ &  $\sin{\left(\log{\left(x \right)} \right)}$ \\
    $\sin{\left(x \right)}$ & $x$ & $\log{\left(x \right)}$ & $\sin{\left(\log{\left(x \right)} \right)}$ &  $\log{\left(\sin{\left(x \right)} \right)}$ & \boldmath $\sin{\left(\log{\left(x \right)} \right)}$ \\
    \bottomrule
    \end{tabular}
    \end{sc}
    \end{small}
    \end{center}
    % \vskip -0.1in
\end{table*}


\paragraph{Embedding Algebra.}
Word embeddings generated by methods like \textsc{word2vec} \citep{mikolovEfficientEstimationWord2013} and GloVe \citep{penningtonGloVeGlobalVectors2014} exhibit an interesting property that simple algebraic operations on the embedding vectors can be used to solve analogies of the form ``$x_1$ is to $y_1$ as $x_2$ is to $y_2$''.  Following along the same lines, we perform simple algebraic operations on the embeddings generated by \expemba{} and \expembe{}. For a given triplet of expressions $x_1$, $y_1$, and $y_2$, we compute
\begin{equation}
    z = \textrm{emb}(x_1) - \textrm{emb}(y_1) + \textrm{emb}(y_2)
\end{equation}
where $\mathrm{emb}$ represents a function that returns the embedding vector of an input expression. We then find an expression with the embedding vector closest to $z$ in terms of cosine similarity, excluding $x_1$, $y_1$, and $y_2$.

To create a pool of expressions for this experiment, we use all expressions from the training set and add any missing expressions that are required for an analogy. We create 20 analogy examples, 12 of which are based on mathematical identities, and the remaining are based on simple substitutions. Table \ref{tab:emb_algebra} shows the results for \expemba{} and \expembe{}. It is interesting to observe that \expembe{} works for nine examples that are based on mathematical identities. It demonstrates a degree of semantic learning. \expemba{} performs poorly for the identity-based examples and gets four substitution-based examples right, demonstrating that \expemba{} understands the visual structure to a certain extent. The results of this analysis further bolster the efficacy of \expembe{} for learning semantically-rich embeddings.

\begin{table*}[t]
    \caption{$score_5(\%)$ on \textsc{UnseenEqClass} of the \textsc{SemVec} datasets. The scores for \eqnet{} and \eqnetl{} are from their published work \citep{allamanis2017learning, liu2022eqnet}.}
    \label{tab:semvec_comparison}
    % \vskip 0.15in
    \begin{center}
    \begin{small}
    \begin{sc}
    \begin{tabular}{lrrrrrr}
    \toprule
    \multirow{2}{*}{Dataset} & \eqnet{} & \eqnetl{} & \multicolumn{2}{c}{\expemba{}} & \multicolumn{2}{c}{\expembe{}} \\
    & $score_5(\%)$ & $score_5(\%)$ & $score_5(\%)$ & Training Set Size & $score_5(\%)$ & Training Set Size \\
    \midrule
    \textsc{Bool8} & 58.1 & - & 30.6 & 146,488 & 100.0 & 16,143,072 \\
    \textsc{oneV-Poly13} & 90.4 & - & 38.7 & 60,128 & 99.6 & 9,958,406 \\
    \textsc{SimpPoly10} & 99.3 & - & 40.1 & 31,143 & 99.8 & 6,731,858 \\
    \textsc{SimpBool8} & 97.4 & - & 36.9 & 21,604 & 99.4 & 4,440,450 \\
    \textsc{Bool10} & 71.4 & - & 10.8 & 25,560 & 91.3 & 3,041,640 \\
    \textsc{SimpBool10} & 99.1 & - & 24.4 & 13,081 & 95.5 & 1,448,804 \\
    \textsc{BoolL5} & 75.2 & - & 38.3 & 23,219 & 36.0 & 552,642 \\
    \textsc{Poly8} & 86.2 & 87.1 & 32.7 & 6,785 & 87.3 & 257,190 \\
    \textsc{SimpPoly8} & 98.9 & 98.0 & 47.6 & 1,934 & 98.9 & 113,660 \\
    \textsc{SimpBoolL5} & 85.0 & 72.1 & 55.1 & 6,009 & 71.1 & 66,876 \\
    \textsc{oneV-Poly10} & 81.3 & 80.0 & 59.8 & 767 & 74.1 & 25,590 \\
    \textsc{Bool5} & 65.8 & 73.7 & 36.4 & 712 & 57.7 & 17,934 \\
    \textsc{Poly5} & 55.3 & - & 5.7 & 352 & 45.2 & 1,350 \\
    \textsc{SimpPoly5} & 65.6 & 56.3 & 28.1 & 156 & 20.8 & 882 \\
    \bottomrule
    \end{tabular}
    \end{sc}
    \end{small}
    \end{center}
    % \vskip -0.1in
\end{table*}

\subsection{Comparison with Existing Methods}
\label{sec:comparison_existing_methods}
As discussed in Section \ref{sec:related}, prior works \citep{allamanis2017learning, liu2022eqnet} have examined if machine learning models can generate similar representations of mathematically equivalent expressions. It should be noted that these prior works only focus on generating similar representations for mathematically equivalent expressions and do not explore the applicability of their methods to the similarity of non-equivalent expressions. But this historical perspective serves as an established evaluation of the representations generated by \expembe{} and \expemba{}. If we refer to all mathematically equivalent expressions as belonging to a class, this property is measured as the proportion of $k$ nearest neighbors of each test expression that belong to the same class \citep{allamanis2017learning}. For a test expression $q$ belonging to a class $c$, the score is defined as
\begin{equation}
    score_k(q) = \frac{| \mathbb{N}_k(q) \cap c |}{\textrm{min}(k, |c|)}
\end{equation}
where $\mathbb{N}_k(q)$ represents $k$ nearest neighbors of $q$ based on cosine similarity.

We use the datasets published by \citet{allamanis2017learning} for this evaluation. These datasets contain equivalent expressions from the Boolean (\textsc{Bool}) and polynomial (\textsc{Poly}) domains. In these datasets, a class is defined by an expression, and all the equivalent expressions belong to the same class. The datasets are split into training, validation, and test sets. The expressions in the training and validation sets come from the same classes. The dataset contains two test sets: (1) \textsc{SeenEqClass} containing classes that are present in the training set, and (2) \textsc{UnseenEqClass} containing classes that are not present in the training set (to measure the generalization to the unseen classes).

Our approach requires data to be in the input-output format. For \expembe{}, the input and output are mathematically equivalent expressions. To transform the training set into the input-output format for \expembe{}, we generate all possible pairs for the expressions belonging to the same class. To limit the size of the generated training set, we select a maximum of 100,000 random pairs from each class. For \expemba{}, the input and output are the same expressions. For this setting, we use all the expressions present in the training set. Table \ref{tab:semvec_comparison} shows the training set sizes obtained by these transformations for \expembe{} and \expemba{}. We use the validation and test sets in their original form.

As mentioned in Section \ref{sec:training_details}, both \expemba{} and \expembe{} are trained with a model dimensionality of 64. To enable a fair comparison with the existing approaches, we use the same model configuration and hyperparameter values for all 14 \textsc{SemVec} datasets. Refer to Appendix \ref{appendix:training_details} for a detailed description of the model configuration and hyperparameter values. 

For evaluating our models, we use the \textsc{UnseenEqClass} test set. Table \ref{tab:semvec_comparison} shows the scores achieved by our approach. We observe that the representations learned by \expembe{} capture semantics and not just the syntactic structure. It should be noted that our approach does not use the distance between representations at the time of training and only trains the model to generate equivalent expressions, whereas the training for \eqnet{} and \eqnetl{} explicitly pushes the representations of equivalent expressions closer in the embedding vector space.  We further observe that \expembe{} performs better than the existing approaches on the datasets with a sufficiently high number of training examples. For other datasets, the model does not perform well on \textsc{UnseenEqClass} but does well on \textsc{SeenEqClass} and the validation set (See Appendix \ref{appendix:semvec_results} for details). We believe that this is because of not having enough examples in the training set for generalization to the unseen classes, and these datasets may perform better with simpler \textsc{Seq2Seq} models and different hyperparameters. We leave this exploration for future work.

In our experiments, \expemba{} does not perform as well as \expembe{}. One possible reason for this behavior may be the less number of examples for training the model in the autoencoder setting. To rule out this possibility, we train models with three model dimensions of 32, 64, and 128 with 179K, 703K, and 2.8M parameters, respectively. We do not observe any significant difference in the performance across these model dimensions. The model also does not achieve good scores on the training and validation sets (See Appendix \ref{appendix:semvec_results} for details). Furthermore, \expembe{} performs better than \expemba{} on the datasets with comparable training set sizes, for example, consider \expemba{} on \textsc{Bool8} vs \expembe{} on \textsc{SimpPoly8}. With these observations, we conclude with good certainty that the representations learned by \expemba{} are inferior to those learned by \expembe{} in terms of capturing semantics.


% Also, \expembe{} performs better than \eqnet{} and \eqnetl{} on the datasets with sufficiently large training sets. Though the representation sizes (synonymous with model dimension $H$ in our approach) are higher for \expembe{} than the one used in \eqnet{} and \eqnetl{}, our encoder is very simple compared to both of these approaches. Our encoder consists of a GRU layer, whereas \eqnet{} and \eqnetl{} use \textsc{TreeNN}-based encoders. Additionally, the training for \eqnet{} and \eqnetl{} explicitly pushes the representations of expressions belonging to the same class closer, whereas our approach leaves it to the model to infer the equation semantics from the dataset. Given these factors together, the mathematical embedding method proposed in this work is better suited for the variety of mathematical content found in literature and for the downstream language processing tasks.