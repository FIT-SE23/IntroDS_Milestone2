\section{Introduction}
%\answerTODO{}
%While human beings mainly use natural languages, we also convey information using images or sound. Just like modern search engines (Google, Bing, etc.) have achieved high precision retrieval of natural language documents, so too is there a mature industry built around the retrieval of images (Tineye, Google image search, etc.) and sound (Pandora, Shazam, Musicopedia, etc.). 
Despite there being well-established search technologies for most other modes of data, there exist no \textit{major} algorithms for processing mathematical content \citep{larson2013abject}. The first step toward processing new modes of data is to create embedding methods that can transform that information block into a machine-readable format. Most equation embedding methods have focused on establishing a homomorphism between an equation and its surrounding mathematical text \citep{zanibbiNTCIR12MathIRTask2016,krstovskiEquationEmbeddings2018}. While this approach can help find equations used in similar contexts, it overlooks the following key points: (1) there is a large chunk of data in which equations occur without any context (consider math textbooks that contain equations with minimal explanation), and (2) in scientific literature, an equation may be used in a variety of disciplines with different contexts, and encoding equations based on textual context hampers cross-disciplinary retrieval.

We argue that context \textit{alone} is not sufficient for finding representations of mathematical content, and embedding methods must understand equations in addition to the surrounding context. To this end, we consider mathematical expressions without context and present a novel embedding method to generate semantically-rich representations of these formulae. In our proposed approach, \textit{we train a sequence-to-sequence model on equivalent expression pairs} and use the trained encoder to generate vector representations or embeddings. Figure 1 shows an example of our approach that embeds expressions according to their semantics, producing better clustering, retrieval, and analogy results. The efficacy of our approach is highlighted when compared to an autoencoder. We also compare our embedding method with two prior works proposed for an analogous problem of clustering equivalent expressions: \eqnet{} \citep{allamanis2017learning} and \eqnetl{} \citep{liu2022eqnet}, further proving our modelâ€™s ability to capture the semantics.

The contributions of our work are threefold:
\begin{enumerate}
    \itemsep0em
    \item We show that a sequence-to-sequence model can learn to generate expressions that are mathematically equivalent to the given input.
    \item We use the encoder of such a model to generate continuous vector representations of mathematical expressions. These representations capture semantics and not just the visual structure. They are better at clustering and retrieving similar mathematical expressions.
    \item We publish a corpus of equivalent transcendental and algebraic expressions pairs that can be used to develop more complex mathematical embedding approaches.
\end{enumerate}

We end this manuscript with a comprehensive study of our proposed approach, detailing its potential for general information processing and retrieval tasks and noting its limitations. The datasets and source code are available on our project page. \footnote{
\ifisaccepted
\href{https://github.com/mlpgroup/expemb}{github.com/mlpgroup/expemb}
\else
Anonymous link.
\fi
}

\input{img/overview.tex}