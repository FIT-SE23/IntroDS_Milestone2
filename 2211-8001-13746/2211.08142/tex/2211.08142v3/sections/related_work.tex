\section{Related Work}
\label{sec:related}
%answerTODO{}

While information processing for mathematical expressions is still a relatively new field, other groups have attempted to create embedding methods for mathematical content.

Starting with embedding schemes for mathematical \textit{tokens}, \citet{gaoPreliminaryExplorationFormula2017} embedded mathematical symbols using \textsc{word2vec} \citep{mikolovEfficientEstimationWord2013} on the Wikipedia corpus. They created a mathematical token embedding scheme (\textsc{symbol2vec}) in which tokens used in similar contexts ($\sin$ \& $\cos$, $=$ \& $\approx$) were grouped together. Using these symbol embeddings, they extended their approach to embed entire formulae using the paragraph-to-vector approach (\textsc{formula2vec}).
%
More commonly, other approaches have attempted to extract textual descriptors of equations using surrounding text and use pre-trained embeddings of those textual keywords to create an embedding of the equation in question \citep{kristianto2014extracting, schubotzEvaluatingImprovingExtraction2017}.

Expanding token descriptors to represent equations is more complex. \citet{schubotzSemantificationIdentifiersMathematics2016} created equation descriptors by combining the textual keyword descriptors of mathematical tokens included within an expression. By organizing the equation descriptors into keyword pairs, clustering algorithms grouped equations according to Wikipedia categories. In a similar approach, \citet{kristiantoUtilizingDependencyRelationships2017} viewed equations as dependency relationships between mathematical tokens. Nominal natural language processing (NLP) methods were used to extract textual descriptors for these tokens, and the equation was transformed into a dependency graph. A combination of interdependent textual descriptors allowed the authors to derive better formula descriptors that were used by indexers for retrieval tasks.

Alternatively, multiple groups have attempted to represent equations as feature sets; sequences of symbols that partially describe an equationâ€™s visual layout \citep{zanibbiTangentSearchEngine2015, fraserChoosingMathFeatures2018}. \citet{krstovskiEquationEmbeddings2018} used \textsc{word2vec} in two different manners to find equation embeddings. In one approach, they treated equations as tokens. In their second approach, they treated variables, symbols, and operators as tokens and equations as sequences of tokens. Equation embeddings were computed by taking the average of individual token embeddings. \citet{mansouriTangentCFTEmbeddingModel2019} used a modified version of the latter method that extracted features from both the symbol layout tree and operator tree representations of an expression. \citet{ahmedEquationAttentionRelationship2021} used a complex schema where an equation was embedded using a combination of a message-passing network (to process its graph representation) and a residual neural network (to process its visual representation). \citet{peng2021mathbert} trained BERT \citep{devlinBERTPretrainingDeep2019} on mathematical datasets to create a pre-trained model for mathematical formula understanding.

Most of these approaches depend on embedding mathematical tokens and expressions using the surrounding textual information, effectively establishing a homomorphism between mathematical and textual information. While these approaches have produced crucial initial results, there are still two important limitations that need to be addressed. Firstly, an embedding scheme should be able to process equations without surrounding text, such as in the case of pure math texts like the Digital Library of Mathematical Functions (DLMF) \citep{lozierNISTDigitalLibrary2003}.
Secondly, expressions may be written in a multitude of mathematically equivalent ways (consider $x^{-1} = \frac{1}{x}$ or $\sin(x) = \cos(x-\frac{\pi}{2})$). Embedding methods should recognize that such expressions are mathematically equivalent and should produce similar embeddings. \citet{russin2021compositional} and \citet{schlag2019enhancing} have studied the representations learned by a Transformer model, trained on a math-reasoning dataset. \citet{lampleDeepLearningSymbolic2019} showed that their model generated multiple mathematically equivalent solutions for a first-order differential equation, showing a degree of semantic learning. Analogous to our approach, \citet{zhang2017semantic} trained a \textsc{Seq2Seq} model on sentence paraphrase pairs and used the last encoder hidden state as the vector representation of the input sentence.

\citet{allamanis2017learning} and \citet{liu2022eqnet} have previously proposed \eqnet{} and \eqnetl{}, respectively, for finding semantic representations of simple symbolic expressions. For \eqnet{}, \citet{allamanis2017learning} used the tree representation of a formula with a modified version of the recursive neural network (TreeNN). Their dataset is partitioned into equivalent classes, and they used this information while computing the training loss. They also introduced a regularization term in their objective, called subexpression autoencoder. \eqnetl{} \citep{liu2022eqnet} extended \eqnet{} by adding a dropout layer and introducing a stacked version of the subexpression autoencoder. However, these approaches only focus on ensuring that the embeddings of \textit{equivalent} expressions are grouped together. They do not consider or explore semantically similar but non-equivalent expressions. We define semantic similarity based on the operators present in expressions. For example, we consider $\sin(x + 1)$ to be more similar to $\sin(x)$ than $\log(x)$ or $\sinh(x + 1)$ (Section \ref{sec:qual_results}).

To this end, we look at mathematical expressions in isolation without any context and propose an approach to semantically represent these expressions in a continuous vector space. Our approach considers semantic similarity in addition to mathematical equivalence. Learning semantically rich representations of mathematical content would be useful in learning-based applications, like math problem-solving, as well as information retrieval.