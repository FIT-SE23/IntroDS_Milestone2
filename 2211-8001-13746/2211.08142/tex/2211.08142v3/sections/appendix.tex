\newpage
\appendix
\onecolumn

% \section{Model Details}
% \label{appendix:model}

% \paragraph{Model.}
% Let $\mathbf{x} = (x_1, x_2 \dots x_N)$ and $\mathbf{y} = (y_1, y_2 \dots y_M)$ represent an input and output expression pair. Each $x_i$  is first mapped to a static vector $\mathbf{e}_i$. This sequence of vectors is passed to a GRU which produces a hidden state $\mathbf{h}_i$ for each time step $1 \le i \le N$ according to the following equations:
% \begin{equation}
%     \label{eq:gru}
%     \begin{split}
%         \mathbf{r}_i &= \sigma (\mathbf{W}_r \mathbf{e}_i + \mathbf{U}_r \mathbf{h}_{i-1})
%         \\
%         \mathbf{z}_i &= \sigma (\mathbf{W}_z \mathbf{e}_i + \mathbf{U}_z \mathbf{h}_{i-1})
%         \\
%         \tilde{\mathbf{h}}_i &= \tanh (\mathbf{W} \mathbf{e}_i + \mathbf{U}(\mathbf{r_t \odot \mathbf{h}_{i-1}}))
%         \\
%         \mathbf{h}_i &= (1 - \mathbf{z}_i) \odot \mathbf{h}_{i - 1} + \mathbf{z}_i \odot \tilde{\mathbf{h}_i}
%     \end{split}
% \end{equation}
% where $\mathbf{r}_i$, $\mathbf{z}_i$, and $\tilde{\mathbf{h}}_i$ are the reset gate, update gate, and proposed state at time step $i$, respectively. $\sigma$ and $\odot$ denote the sigmoid function and element-wise product, respectively. The final hidden state, $\mathbf{h}_N$, depends on the entire input sequence and is interpreted as the \textit{continuous vector representation} of the input expression. The decoder generates an output expression conditioned on the encoder hidden states $\mathbf{h}_i$ for $1 \le i \le N$. It first computes a context vector $\mathbf{c}_t$ at time step $t$ according to the following equations:
% \begin{equation}
%     \begin{split}
%         a_{i, t} &= \mathbf{v}_a^T \tanh(\mathbf{W}_a \mathbf{s}_{t - 1} + \mathbf{U}_a \mathbf{h}_i) \\
%         % \alpha_{i, t} &= \frac{\exp(a_{i, t})}{\sum_{i=1}^{N} \exp(a_{i, t})} \\
%         \mathbf{c}_t &= \sum_{i = 1}^{N} \frac{\exp(a_{i, t})}{\sum_{j=1}^{N} \exp(a_{j, t})} \mathbf{h}_i
%     \end{split}
% \end{equation}
% where $\mathbf{s}_{t}$ represents the decoder hidden state at time step $t$. The context vector $\mathbf{c}_t$ is combined with the static vector representation of the output token predicted at the previous time step as:
% \begin{equation}
%     \mathbf{d}_t = \mathbf{W}_c [\mathbf{c}_t, \mathbf{o}_{t-1}]
% \end{equation}
% where $\mathbf{o}_{t}$ represents the static vector representation of $y_t$, the output token at time step $t$. $\mathbf{d}_t$ follows %equation \ref{eq:gru} 
% (\ref{eq:gru}) to generate the decoder hidden state $\mathbf{s}_t$. The probability of the output token $y_t$ is defined as:
% \begin{equation}
% \begin{aligned}
%     P(y_t \mid y_1 \dots y_{t-1}, \mathbf{x}) = \textrm{softmax} (\mathbf{W}_o \mathbf{s}_t)
% \end{aligned}
% \end{equation}

% \paragraph{Loss.}
% For each time step, the decoder produces a probability distribution of $y_t$. The probability of the output sequence $\mathbf{y}$ is defined as:
% \begin{equation}
% \begin{aligned}
%     P(\mathbf{y} \mid \mathbf{x}) = \prod_{t = 1}^{M} P(y_t \mid y_1 \dots y_{t-1}, \mathbf{x}) 
% \end{aligned}
% \end{equation}

% We maximize the log-likelihood of the output sequence. For an example pair $(\mathbf{x}, \mathbf{y})$, the loss is defined as:
% \begin{equation}
%     l(\mathbf{x}, \mathbf{y}) = - \sum_{t = 1}^{M} \log P(y_t \mid y_1 \dots y_{t - 1}, \mathbf{x}) 
% \end{equation}

% \paragraph{Architecture.}
% Table \ref{tab:model_dimensions} shows the dimensions of different layers in our architecture shown in Figure~\ref{fig:model_architecture}.

\section{Equivalent Expression Generation}
\label{appendix:eq_exp_generation}
We use SymPy to generate mathematically equivalent expressions for a given formula. We apply the operations shown in Table~\ref{tab:equiv_gen_sympy_ops} to get mathematically equivalent expressions. The examples shown in the table are intended to give an idea about each function. The actual behavior of these functions is much more complex. Refer to the SymPy documentation for more details.\footnote{\href{https://docs.sympy.org/latest/tutorial/simplification.html}{https://docs.sympy.org/latest/tutorial/simplification.html}} We use the \textsc{\texttt{rewrite}} function for expressions containing trigonometric or hyperbolic operators. Specifically, we use this function to rewrite an expression containing trigonometric (or hyperbolic) operators in terms of other trigonometric (or hyperbolic) operators.

\input{tables/sympy_functions}

\section{Equivalent Expressions Dataset}
The expressions in the Equivalent Expressions Dataset may contain operators from multiple operator types (shown in Table \ref{tab:equiv_exp_dataset_stats}). For example, 1,424,243 expressions contain operators from a single operator type, 1,186,808 expressions contain two types of operators, 130,547 expressions contain three types of operators, and so on. Table \ref{tab:equiv_exp_dataset} shows a few examples from the Equivalent Expressions Dataset.

\input{tables/dataset_examples}

\section{Training Details}
\label{appendix:training_details}
We use the PyTorch implementation of the Transformer architecture for our experiments with a modified version of the decoder to enable caching to speed up the generation process at inference.\footnote{\href{https://pytorch.org/docs/1.12/generated/torch.nn.Transformer.html}{https://pytorch.org/docs/1.12/generated/torch.nn.Transformer.html}} We use 6 encoder layers, 6 decoder layers, 8 attention heads, 0.1 dropout probability, and the ReLU activation for our experiments. The layer normalization is performed in the encoder and decoder layers before other attention and feedforward operations. We use the Adam optimizer with a learning rate of $10^{-4}$, a fixed seed of 42 for reproducibility, and a label smoothing of 0.1 while computing the loss.

For the Equivalent Expressions Dataset, we use a model dimension of 512, a feedforward dimension of 2048, and a batch size of 256. These experiments were run on two 32GB V100 GPUs. We use early stopping with 300K minimum steps, 1M maximum steps, and patience of 30K steps. In terms of epochs, this translates to 17 minimum epochs, 55 maximum epochs, and	patience of 2 epochs for \expembe{} and 28 minimum epochs, 94 maximum epochs, and patience of 3 epochs for \expemba{}. We evaluate the model at the end of every epoch.

For the \semvec{} datasets, we use a model dimension of 64, a feedforward dimension of 256, and a batch size of 512. These experiments were run on one 16GB V100 GPU. We use early stopping with 50K minimum steps, 1M maximum steps, and patience of 20K steps or 2 epochs (whichever is smaller). Table \ref{appendix_tab:training_details} shows these values in terms of epochs for different datasets. We evaluate the model at the end of every epoch.

\input{tables/semvec_training_details}

\section{Results for the \semvec{} Datasets}
\label{appendix:semvec_results}
To compute $score_k(q)$ for our model, we use the source code provided by \citet{allamanis2017learning}.\footnote{\href{https://github.com/mast-group/eqnet}{https://github.com/mast-group/eqnet}}

Tables \ref{table:semvec_validation_results} and \ref{table:semvec_seeneqcls_results} show the scores achieved by \expemba{} and \expembe{} on the validation and \textsc{SeenEqClass} test sets of the \semvec{} datasets, respectively. For \expemba{}, we try three model dimensions of 32, 64, and 128 to rule out the effect of underfitting and overfitting while training the model. Table \ref{table:expemba_detailed_results} shows the scores achieved by these models on \textsc{UnseenEqClass}. It can be seen that changing the number of parameters in the model does not have a significant effect on the scores.

We also perform an evaluation of compositionality on the \semvec{} datasets \citep{allamanis2017learning}. For this evaluation, we train our model on a simpler dataset and evaluate it on a complex dataset. For example, a model trained on \textsc{Bool5} is evaluated on \textsc{Bool10}. We use \textsc{UnseenEqClass} for the evaluation. The results of this experiment are shown in Table \ref{table:semvec_compositionality_results}.

\input{tables/semvec_results_validation}

\input{tables/semvec_results_seeneqclass}

\input{tables/semvec_results_ae_hidden}

\input{tables/semvec_results_compositionality}

\input{tables/equiv_exp_generation_results_validation}

\section{Distance Analysis}
\label{appendix:distance_analysis}
Table \ref{appendix_tab:distance_analysis} shows more examples of the distance analysis as discussed in Section \ref{sec:qual_results}.

\input{tables/distance_analysis_results_more}

\section{Equivalent Expression Generation}
For the Equivalent Expressions Dataset, Table \ref{tab:expemb_val_accuracy} shows the accuracy of \expemba{} and \expembe{} on the validation set of the Equivalent Expressions Dataset with beam sizes of 1, 10, and 50.
