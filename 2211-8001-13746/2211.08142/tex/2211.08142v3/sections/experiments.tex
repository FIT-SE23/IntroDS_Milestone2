%%%%%%%%%%%%%%%%% Models used for results %%%%%%%%%%%%%%%%%%
%% Autoencoder: 20220611-203103365077
%% Equivalent expression: 20220611-170824504558
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Experiments}
We consider two decoder settings for our experiments:
\begin{itemize}
    \itemsep0em
    \item Equivalent expression setting, or \expembe{}, in which the model is trained on disparate but mathematically equivalent expression pairs.
    \item Structural setting, or \expemba{}, in which the model is trained as an autoencoder to output the same expression as the input.
\end{itemize}
The proposed \expembe{} embeds an expression based on mathematical semantics, while \expemba{} only considers the visual layout of the expression. Though the focus of this work is to demonstrate the efficacy of \expembe{}, \expemba{} serves as an important contrast, demonstrating that \expembe{} yields representations that better describe \textit{semantics} and are superior for clustering, retrieval, and analogy tasks.

\subsection{Training Details}
\label{sec:training_details}
We use the Transformer architecture with 6 encoder layers, 6 decoder layers, 8 attention heads, and the ReLU activation. The model dimensions, indicated by $D_{\text{model}}$, are 512 and 64 for the experiments in Sections \ref{sec:qual_results} and \ref{sec:comparison_existing_methods}, respectively. The layer normalization is performed before other attention and feedforward operations. We use the Adam optimizer \citep{Kingma2015AdamAM} with a learning rate of $10^{-4}$ and do not consider expressions with more than 256 tokens. Refer to Appendix \ref{appendix:training_details} for more details.

% For experiments in Sections \ref{sec:equiv_exp_gen_results} and \ref{sec:qual_results}, we use the Equivalent Expressions Dataset (Section \ref{sec:datagen}). For \expemba{}, we use unique expressions present in the training set. The experiments in Section \ref{sec:comparison_existing_methods} use the datasets created by \citet{allamanis2017learning} (details in the same section).

% In this section, we conduct a series of experiments that demonstrate the following qualities of our approach: 
% \begin{enumerate}
%     \item \expemba{} learns to generate the input expression exactly
%     \item \expembe{} learns to generate an expression that is mathematically equivalent to the input expression. The intent behind this experiment is to establish that \expemba{} and \expembe{} perform well on their original tasks.
% \end{enumerate}

%The overall purpose of these experiments is to 

%In this section, we present the results of our experiments on \expemba{} and \expembe{}. For our experiments, we use the Equivalent Expressions Dataset and the datasets introduced by \citet{allamanisLearningContinuousSemantic2017}. The latter is referred to as the \semvec{} datasets hereafter. The datasets are discussed in detail in section \ref{sec:datasets}. We perform quantitative and qualitative evaluations on our models. We first show that for a given input expression (1) \expemba{} learns to generate the input expression exactly and (2) \expembe{} learns to generate an expression that is mathematically equivalent to the input expression. The intent behind this experiment is to establish that \expemba{} and \expembe{} perform well on their original tasks. For this experiment, we use the Equivalent Expressions Dataset. For the next experiment, we use the \semvec{} datasets. The objective of this experiment is to analyze the semantic properties of representations generated by \expemba{} and \expembe{} encoders. The results of these experiments are discussed in section \ref{sec:quant_results}. Furthermore, we plot and compare the vector representations generated by \expemba{} and \expembe{}. We use these representations to find five closest expressions to a given test expression. We also perform embedding algebra on the representations generated by models trained in both of these settings. We use the Equivalent Expressions Dataset for these experiments and discuss the results in section \ref{sec:qual_results}.

% \subsection{Datasets}
% \label{sec:datasets}

% \paragraph{\semvec{} Datasets.}
% Published in \citet{allamanis2017learning}, these datasets contain equivalent expressions from the boolean (\textsc{Bool}) and polynomial (\textsc{Poly}) domains. In these datasets, a class is defined by an expression and all the equivalent expressions belong to the same class. The datasets are split into training, validation, and test sets and contain two test sets: (1) \textsc{SeenEqClass} containing classes that are present in the training set (2) \textsc{UnseenEqClass} containing classes that are not present in the training set. To transform the training set into the input-output format used by \expembe{}, we generate all possible pairs for the expressions belonging to the same class. To limit the size of the generated training set, we select a maximum of 100,000 random pairs from each class. For \expemba{}, we use all the expressions present in the training set. See Appendix \ref{appendix:semvec_datasets} for details.

% \subsection{Training}
% We use the model shown in \ref{fig:model_architecture} for our training. Refer to Appendix \ref{appendix:model_arch} for dimensions of different layers in our architecture. We train \expemba{} with $H = 128$ and \expembe{} with $H = 1024$ on the Equivalent Expressions Dataset. For the \semvec{} datasets, we train both \expemba{} and \expembe{} with $H = 64, 128, 256, 512, 1024$. We use the AdamW optimizer \citep{loshchilovDecoupledWeightDecay2019} for training. See Appendix \ref{appendix:training_details} for details.

% \subsection{Equivalent Expression Generation}
% \label{sec:equiv_exp_gen_results}

% The first objective is to show that \expembe{} can learn to generate equivalent expressions for a given input. To evaluate if two expressions $\mathbf{x}_1$ and $\mathbf{x}_2$ are mathematically equivalent, we simplify their difference $\mathbf{x}_1 - \mathbf{x}_2$ using SymPy and compare it to 0. In this setting, if the model produces an expression that is the same as the input, we do not count it as a model success. There are instances in which SymPy takes significant time to simplify an expression and eventually fails with out-of-memory errors. To handle these cases, we put a threshold on its execution time. If the simplification operation takes more time than the threshold, we count it as a model failure. We also evaluate \expemba{} and verify that it learns to generate the input exactly. This serves as an important contrast to \expembe{} and is useful in understanding how well a sequence-to-sequence model understands the mathematical structure. At inference, we use the beam search algorithm to generate output expressions. As an output can be verified programmatically, we consider all the outputs in the beam. If any of the outputs are correct, we count it as a model success.

% \begin{table}[t]
%     \caption{Accuracy of \expemba{} and \expembe{} on the Equivalent Expressions Dataset. \expemba{} has an easier task of generating the input at the decoder, while \expembe{} must generate a mathematically equivalent expression.}
%     \label{tab:results_autoenc_encdec}
%     % \vskip 0.15in
%     \begin{center}
%     \begin{small}
%     \begin{sc}
%     \begin{tabular}{lrr}
%     \toprule
%     Beam Size & \expemba{} & \expembe{} \\
%     \midrule
%     1 & 0.9996 & 0.7206 \\
%     10 & 0.9998 & 0.8118 \\
%     50 & 0.9998 & 0.8386 \\
%     \bottomrule
%     \end{tabular}
%     \end{sc}
%     \end{small}
%     \end{center}
%     % \vskip -0.1in
% \end{table}

% The accuracy of these models is shown in Table \ref{tab:results_autoenc_encdec}. First, we note that \expemba{} is easily able to encode and generate the input expression and achieves a near-perfect accuracy with greedy decoding (beam size = 1). Second, the \expembe{} results demonstrate that generating mathematically equivalent expressions is a significantly harder task. For this setting, greedy decoding does not perform well. We observe an improvement of 11\% with a beam size of 50. However, we also see an increase in the number of invalid prefix expressions being assigned high log probabilities with this beam size. This experiment demonstrates that both \expemba{} and \expembe{} are capable of generating mathematical expressions, and \expembe{} can learn to generate expressions that are mathematically equivalent to the input expression. While \expembe{} achieves a lower accuracy than \expemba{}, we see in Section \ref{sec:qual_results} that the former exhibits some interesting properties and is more useful for retrieval tasks.

\input{figures/emb_plots}

\subsection{Embedding Evaluation}
\label{sec:qual_results}
In this section, we evaluate the usefulness of the representations generated by the \expembe{} model. We train \expembe{} and \expemba{} on the Equivalent Expressions Dataset. As the dataset consists of equivalent expression pairs, we consider each expression as an independent example for training \expemba{}. The details and results of the training are presented in Section \ref{sec:equiv_exp_gen_results}. For the experiments of this section, we disregard the decoder and use the encoder of the trained models to generate vector representations of input expressions. 

Unlike the natural language embedding problem \citep{wangGLUEMultiTaskBenchmark2019}, there do not exist standard tests to quantitatively measure the embedding performance of methods built for mathematical expressions. Hence, our analysis in this section must be more qualitative in nature. These experiments show some interesting properties of the representations generated by \expembe{} and \expemba{} and demonstrate the efficacy of the proposed approach. The data used for the experiments are described in their respective sections.

\paragraph{Embedding Plots.}
To gauge if similar expressions are clustered in the embedding vector space, we plot and compare the representations generated by the \expembe{} and \expemba{} models. For this experiment, we use a set of 7,000 simple expressions belonging to four categories: hyperbolic, trigonometric, polynomial, and logarithmic/exponential. Each expression is either a polynomial or contains hyperbolic, trigonometric, logarithmic, or exponential operators. Each expression contains only one category of operators. 35\% of these expressions are from the training set and the remaining 65\% are unseen expressions.  Below are a few examples of expressions belonging to each class:
\begin{itemize}
    \itemsep0em
    \item Polynomial: $x^{2} + 2 x + 5$, $2 x + 2$
    \item Trigonometric: $\sin{\left(x \right)} \tan{\left(x \right)}$, $\cos^{5}{\left(4 x \right)}$
    \item Hyperbolic: $\cosh{\left(x - 4 \right)}$, $\sinh{\left(x \cos{\left(2 \right)} \right)}$
    \item Log/Exp: $e^{- 2 x - 4}$, $\log{\left(x + 3 \right)}^{3}$
\end{itemize}

We use Principal Component Analysis (PCA) for dimensionality reduction. Figure \ref{fig:pca_plots} shows the plots for \expemba{} and \expembe{}. We observe from these plots that the clusters in the \expembe{} plot are more distinguishable compared to the \expemba{} plot. \expembe{} does a better job at grouping similar expressions together, suggesting its ability to understand semantics. For \expembe{}, there is an overlap between expressions belonging to hyperbolic and logarithmic/exponential classes. This is expected because hyperbolic operators can be written in terms of the exponential operator. Furthermore, \expemba{} focuses on the overall token similarity. For example, representations generated by \expemba{} for $\tan \left( x \frac{\sqrt{2}}{2} \right)$, $x^2 (x^2 - x)$, $\sinh^{-1} \left( x \frac{\sqrt{2}}{2} \right) $, and $\log \left( x \frac{\sqrt{2}}{2} \right)$ are grouped together. On the other hand, representations generated by \expembe{} capture semantics in addition to the syntactic structure.

\input{tables/distance_analysis_results}

\paragraph{Distance Analysis.}
To understand the applicability of the proposed approach for the information retrieval task, we perform distance analysis on the embeddings generated by \expemba{} and \expembe{}. We use all expressions from the training set of the Equivalent Expressions Dataset as the pool of expressions for this experiment. The similarity between two expressions is defined as the inverse of the cosine distance between their embedding vectors. We find the five closest expressions to a given query expression. Table \ref{tab:distance_analysis} shows the results of this experiment. We observe that the closest expressions computed using \expembe{} are more similar to the query in terms of the syntactic structure and operators. On the other hand, \expemba{} focuses on the overall token similarity. For example, the first query in Table \ref{tab:distance_analysis} consists of polynomial and trigonometric expressions. The closest expressions computed using \expembe{} follow the same structure, whereas \expemba{} seems to put more emphasis on the polynomial multiplier. This behavior is also apparent in the second example. We believe that the ability of \expembe{} to group similar expressions, in terms of operators and the syntactic structure, can prove useful in information retrieval problems, where the aim is to find similar expressions to a given query. Refer to Appendix \ref{appendix:distance_analysis} for more examples.

To quantify the notion of similarity, we use the tree edit distance between the operator trees of two expressions.\footnote{Implementation in Python: \href{https://pythonhosted.org/zss/}{https://pythonhosted.org/zss/}.} We consider 2,000 query expressions and analyze the most similar expression returned by \expemba{} and \expembe{} for a given query. A cost of one unit is used for inserting, deleting, or updating a token while computing the distance. The tree edit distance between the query expression and the result shows the following:
\begin{itemize}
    \itemsep0em
    \item \textit{Scenario 1.} If we compare expressions in their original form, \expemba{} returns an expression strictly closer to the query in 1,044 cases and \expembe{} in 261 cases.
    \item \textit{Scenario 2.} When we exclude constant multipliers and additions, i.e., we consider $a f(x) + b$ and $f(x)$ to be at a distance of zero, \expembe{} returns an expression strictly closer to the query in 683 cases and \expemba{} in 402 cases. Note that $a$ and $b$ could be simple constants, like $1$, $\sqrt{2}$, etc or constant expressions, like $\cos(1)$, $\cos(\log(1))$, etc.
\end{itemize}

It is interesting to see the change in results as we move from the first scenario to the second one. These results indicate that \expembe{} puts more emphasis on operators compared to \expemba{}, and \expemba{} considers the overall token similarity. The five closest expressions to the query show a similar trend.

\paragraph{Embedding Algebra.}
Word embeddings generated by methods like \textsc{word2vec} \citep{mikolovEfficientEstimationWord2013} and GloVe \citep{penningtonGloVeGlobalVectors2014} exhibit an interesting property that simple algebraic operations on the embedding vectors can be used to solve analogies of the form ``$x_1$ is to $y_1$ as $x_2$ is to $y_2$''.  Following along the same lines, we perform simple algebraic operations on the embeddings generated by \expemba{} and \expembe{}. For a given triplet of expressions $x_1$, $y_1$, and $y_2$, we compute
\begin{equation}
    z = \textrm{emb}(x_1) - \textrm{emb}(y_1) + \textrm{emb}(y_2)
\end{equation}
where ``$\mathrm{emb}$'' represents a function that returns the embedding vector of an input expression. We then find an expression with the embedding vector closest to $z$ in terms of cosine similarity, excluding $x_1$, $y_1$, and $y_2$.

\input{tables/embedding_algebra_results}

To create a pool of expressions for this experiment, we use all expressions from the training set and add any missing expressions that are required for an analogy. We create 20 analogy examples, 12 of which are based on mathematical identities, and the remaining are based on simple substitutions. Table \ref{tab:emb_algebra} shows the results for \expemba{} and \expembe{}. It is interesting to observe that \expembe{} works for nine examples that are based on mathematical identities. It demonstrates a degree of semantic learning. \expemba{} performs poorly for the identity-based examples and gets four substitution-based examples right, demonstrating that \expemba{} understands the visual structure to a certain extent. The results of this analysis further bolster the efficacy of \expembe{} for learning semantically rich embeddings.


\subsection{Comparison with Existing Methods}
\label{sec:comparison_existing_methods}
As discussed in Section \ref{sec:related}, prior works \citep{allamanis2017learning, liu2022eqnet} have examined if machine learning models can generate similar representations of mathematically equivalent expressions. It should be noted that these prior works only focus on generating similar representations for mathematically equivalent expressions and do not explore the applicability of their methods to the similarity of non-equivalent expressions. But this historical perspective serves as an established evaluation of the representations generated by \expembe{} and \expemba{}.

If we refer to all mathematically equivalent expressions as belonging to a class, this property is measured as the proportion of $k$ nearest neighbors of each test expression that belong to the same class \citep{allamanis2017learning}. For a test expression $q$ belonging to a class $c$, the score is defined as
\begin{equation}
    score_k(q) = \frac{| \mathbb{N}_k(q) \cap c |}{\textrm{min}(k, |c|)}
\end{equation}
where $\mathbb{N}_k(q)$ represents $k$ nearest neighbors of $q$ based on cosine similarity.

We use the datasets published by \citet{allamanis2017learning} for this evaluation. These datasets contain equivalent expressions from the Boolean (\textsc{Bool}) and polynomial (\textsc{Poly}) domains. In these datasets, a class is defined by an expression, and all the equivalent expressions belong to the same class. The datasets are split into training, validation, and test sets. The expressions in the training and validation sets come from the same classes. The dataset contains two test sets: (1) \textsc{SeenEqClass} containing classes that are present in the training set, and (2) \textsc{UnseenEqClass} containing classes that are not present in the training set (to measure the generalization to the unseen classes).

Our approach requires data to be in the input-output format. For \expembe{}, the input and output are mathematically equivalent expressions. To transform the training set into the input-output format for \expembe{}, we generate all possible pairs for the expressions belonging to the same class. To limit the size of the generated training set, we select a maximum of 100,000 random pairs from each class. For \expemba{}, the input and output are the same expressions. For this setting, we treat each expression present in the training set as an independent example. Table \ref{tab:semvec_comparison} shows the training set sizes obtained by these transformations for \expembe{} and \expemba{}. We use the validation and test sets in their original form.

As mentioned in Section \ref{sec:training_details}, both \expemba{} and \expembe{} are trained with a model dimensionality of 64. To enable a fair comparison with the existing approaches, we use the same model configuration and hyperparameter values for all 14 \textsc{SemVec} datasets. Refer to Appendix \ref{appendix:training_details} for a detailed description of the model configuration and hyperparameter values.

\input{tables/semvec_results}

For evaluating our models, we use the \textsc{UnseenEqClass} test set. Table \ref{tab:semvec_comparison} shows the scores achieved by our approach. We observe that the representations learned by \expembe{} capture semantics and not just the syntactic structure. It should be noted that our approach does not use the distance between representations at the time of training and only trains the model to generate equivalent expressions, whereas the training for \eqnet{} and \eqnetl{} explicitly pushes the representations of equivalent expressions closer in the embedding vector space.  We further observe that \expembe{} performs better than the existing approaches on the datasets with a sufficiently high number of training examples. For other datasets, the model does not perform well on \textsc{UnseenEqClass} but does well on \textsc{SeenEqClass} and the validation set (See Appendix \ref{appendix:semvec_results} for details). We believe that this is because of not having enough examples in the training set for generalization to the unseen classes, and these datasets may perform better with simpler \textsc{Seq2Seq} models and different hyperparameters. We leave this exploration for future work.

In our experiments, \expemba{} does not perform as well as \expembe{}. One possible reason for this behavior may be the less number of examples for training the model in the autoencoder setting. To rule out this possibility, we train models with three model dimensions of 32, 64, and 128 with 179K, 703K, and 2.8M parameters, respectively. We do not observe any significant difference in the performance across these model dimensions. The model also does not achieve good scores on the training and validation sets (See Appendix \ref{appendix:semvec_results} for details). Furthermore, \expembe{} performs better than \expemba{} on the datasets with comparable training set sizes, for example, consider \expemba{} on \textsc{Bool8} vs \expembe{} on \textsc{SimpPoly8}. With these observations, we conclude with good certainty that the representations learned by \expemba{} are inferior to those learned by \expembe{} in terms of capturing semantics.


% Also, \expembe{} performs better than \eqnet{} and \eqnetl{} on the datasets with sufficiently large training sets. Though the representation sizes (synonymous with model dimension $H$ in our approach) are higher for \expembe{} than the one used in \eqnet{} and \eqnetl{}, our encoder is very simple compared to both of these approaches. Our encoder consists of a GRU layer, whereas \eqnet{} and \eqnetl{} use \textsc{TreeNN}-based encoders. Additionally, the training for \eqnet{} and \eqnetl{} explicitly pushes the representations of expressions belonging to the same class closer, whereas our approach leaves it to the model to infer the equation semantics from the dataset. Given these factors together, the mathematical embedding method proposed in this work is better suited for the variety of mathematical content found in literature and for the downstream language processing tasks.

% \subsection{Equivalent Expression Generation}
\subsection{Training Efficacy}
\label{sec:equiv_exp_gen_results}

This section presents the efficacy of training \expembe{} and \expemba{} models on the Equivalent Expressions Dataset that are used in Section \ref{sec:qual_results}. It is important to emphasize that \expembe{} and \expemba{} are trained on fundamentally different problems. \expemba{} is an autoencoder and is trained to generate the input expression exactly, while \expembe{} is trained to generate visually different but mathematically equivalent expressions.  We show that \expembe{} can learn to generate mathematically equivalent expressions for a given input. To evaluate if two expressions $\mathbf{x}_1$ and $\mathbf{x}_2$ are mathematically equivalent, we simplify their difference $\mathbf{x}_1 - \mathbf{x}_2$ using SymPy and compare it to 0. In this setting, if the model produces an expression that is the same as the input, we do not count it as a model success. There are instances in which SymPy takes a significant time to simplify an expression and eventually fails with out-of-memory errors. To handle these cases, we put a threshold on its execution time. If the simplification operation takes more time than the threshold, we count it as a model failure. At inference, we use the beam search algorithm to generate output expressions. As an output can be verified programmatically, we consider all outputs in a beam. If any of the outputs are correct, we count it as a model success.

\input{tables/equiv_exp_generation_results}

The accuracy of these models is shown in Table \ref{tab:results_autoenc_encdec}. First, we note that \expemba{} is easily able to encode and generate the input expression and achieves a near-perfect accuracy with greedy decoding (beam size = 1). Second, the \expembe{} results demonstrate that generating mathematically equivalent expressions is a significantly harder task. For this setting, we observe an improvement of 11\% with a beam size of 50. However, we also see an increase in the number of invalid prefix expressions being assigned high log probabilities with this beam size. This experiment demonstrates that both \expemba{} and \expembe{} are capable of generating mathematical expressions, and \expembe{} can learn to generate expressions that are mathematically equivalent to the input expression. These results also indicate that it is harder to train a model to generate mathematically equivalent expressions compared to training it as an autoencoder.