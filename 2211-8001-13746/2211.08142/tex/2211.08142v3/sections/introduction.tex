\section{Introduction}
%\answerTODO{}
%While human beings mainly use natural languages, we also convey information using images or sound. Just like modern search engines (Google, Bing, etc.) have achieved high precision retrieval of natural language documents, so too is there a mature industry built around the retrieval of images (Tineye, Google image search, etc.) and sound (Pandora, Shazam, Musicopedia, etc.). 
Despite there being well-established search technologies for most other modes of data, the processing of mathematical content remains an open problem \citep{larson2013abject}. Effective search technologies require semantically rich and computationally efficient representations of mathematical data. Most equation embedding methods have focused on establishing a homomorphism between an equation and its surrounding mathematical text \citep{zanibbiNTCIR12MathIRTask2016,krstovskiEquationEmbeddings2018}. While this approach can help find equations used in similar contexts, it is less effective in the following situations: (1) surrounding text may be limited, for example, consider math textbooks that contain equations with minimal explanation, and (2) in scientific literature, an equation may be used in a variety of disciplines with different contexts, and encoding equations based on textual context may hamper cross-disciplinary retrieval.

We argue that context \textit{alone} is not sufficient for finding representations of mathematical content, and embedding methods must understand equations in addition to the surrounding context. To this end, we present a novel embedding method to generate semantically rich representations of mathematical formulae without the aid of natural language context. In our proposed approach, \textit{we train a sequence-to-sequence model on equivalent expression pairs} and use the trained encoder to generate vector representations or embeddings. Figure \ref{fig:equiv_exp_example} shows an example of our approach that embeds expressions according to their semantics. We compare our proposed approach with a structural encoder, that considers the layout structure of an expression, on a variety of tasks to determine the usefulness of each. Furthermore, we compare our semantic embedding method with two prior works proposed for an analogous problem of clustering equivalent expressions: \eqnet{} \citep{allamanis2017learning} and \eqnetl{} \citep{liu2022eqnet}, further proving our modelâ€™s ability to capture semantics.

The contributions of our work are threefold:
\begin{enumerate}
    \itemsep0em
    \item We show that a sequence-to-sequence (\textsc{Seq2Seq}) model can learn to generate expressions that are mathematically equivalent to the input.
    \item We use the encoder of such a model to generate semantically rich vector representations of mathematical expressions. They are better at clustering and retrieving similar mathematical expressions. We expand on the notion of semantics subsequently.
    \item We publish a corpus of equivalent transcendental and algebraic expression pairs that can be used to develop more complex mathematical embedding approaches.
\end{enumerate}

In this work, we consider the \semvec{} datasets that have previously been used by \eqnet{} and \eqnetl{} to infer if representations are semantically rich. In addition, we perform distance analysis and embedding algebra and analyze embedding plots. For distance analysis, the similarity between two expressions is defined using the tree edit distance \citep{zhang1989simple} between their operator trees. We ignore constant additions and multiplications while computing the tree edit distance and put emphasis on the operators. This is achieved by considering $f(x)$ and $af(x) + b$ to be at a distance of zero when $a$ and $b$ are constants.

We end this manuscript with a comprehensive study of our proposed approach, detailing its potential for general information processing and retrieval tasks and noting its limitations. The datasets and source code are available on GitHub. \footnote{
\ifisaccepted
\href{https://github.com/mlpgroup/expemb}{https://github.com/mlpgroup/expemb}
\else
Anonymous link.
\fi
}

\input{figures/overview}