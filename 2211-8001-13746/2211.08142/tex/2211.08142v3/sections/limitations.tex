%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Autoencoder: 20220611-213126518462
%% Equivalent expression: 20220509-134420138885
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Commenting \newpage to fix the table positions.
% \newpage

\section{Limitations}
We try our proposed approach on a dataset of longer expressions. The sequence length for this dataset is $31.95 \pm 30.27$ for the training set and $36.40 \pm 40.49$ for the test set. \expemba{} with $H = 128$ performs well for this dataset. However, \expembe{} does not work well for this dataset even with $H = 2048$. Table \ref{tab:accuracy_long_eq} shows the accuracy of our approach for both of these settings. We observe that simply increasing the model dimension from 1024 to 2048 does not result in the desired gain. We believe that other results of this work should generalize for longer expressions, given a model that learns to generate equivalent expressions.

We also perform an evaluation of compositionality on the \semvec{} datasets \citep{allamanis2017learning}. For this evaluation, we train our model on a simpler dataset and evaluate it on a complex dataset. For example, a model trained on \textsc{Bool5} is evaluated on \textsc{Bool10}. We use \textsc{UnseenEqClass} for the evaluation. The results of this experiment are shown in Table \ref{tab:compositionality}. Our model does not perform as well as \eqnet{} on this task, even for the datasets for which our model achieves a better $score_k(q)$.

Furthermore, we require downstream tasks or better evaluation metrics to evaluate the quality of the learned representations. Though we evaluate our approach on the \semvec{} datasets, we do not perform a similar evaluation on the Equivalent Expressions Dataset. This is because the Equivalent Expressions Dataset contains a small number of equivalent expressions for a given expression, that in turn results from limiting the number of operators in an expression to 5. With a dataset of longer expressions, this evaluation might become possible. We leave these avenues to be explored in the future.

