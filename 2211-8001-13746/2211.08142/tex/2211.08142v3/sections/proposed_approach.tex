\section{Proposed Approach}
\label{sec:proposed_approach}
%In this work, we are interested in learning representations that can capture semantics of mathematical expressions and not just the structure. 
We frame the problem of embedding mathematical expressions as a \textsc{Seq2Seq} learning problem and utilize the encoder-decoder framework. While natural language embedding approaches, like \textsc{word2vec}, assume that proximity in the text suggests similarity, we contend that for mathematical expressions, mathematical equivalence implies similarity. Furthermore, we hypothesize that if we train a \textsc{Seq2Seq} model to generate expressions that are mathematically equivalent to the input, the encoder would learn to generate embeddings that map semantically equivalent expressions together in the embedding vector space. Figure \ref{fig:equiv_exp_example} shows an example of our approach. To accomplish this, we need a machine learning model capable of generating equivalent expressions and a dataset of equivalent expression pairs (Section \ref{sec:datagen}).

\paragraph{Model.}
In encoder-decoder architectures, the encoder encodes the input sequence, and the decoder is conditioned on the encoded vector to generate an output sequence. In our approach, the encoder maps the input expression to a vector that is referred to as the \textit{continuous vector representation} of the expression.
% We train our model in two decoder settings: (1) \textit{equivalent expression setting} (\expembe{}), in which the decoder generates an expression that is mathematically equivalent to the input expression, and (2) \textit{autoencoder setting} (\expemba{}), in which the decoder generates the input expression exactly.\todo{Maybe remove the setting details from here and move to experiments?}
%Two expressions $\mathbf{x}_1$ and $\mathbf{x}_2$ are mathematically equivalent if $\mathbf{x}_1 - \mathbf{x}_2 = 0$ and $\mathbf{x}_1 \neq \mathbf{x}_2$. We refer to the autoencoder setting as \expemba{} and the equivalent expression setting as \expembe{} hereafter.

% \begin{figure}[h]
%     \centering
%     \fontsize{7}{10}\selectfont
%     \includesvg[scale=0.6]{./img/architecture.drawio.svg}
%     \caption{Our model architecture. An input expression (represented in the Polish notation) is fed into the encoder at the bottom as a sequence of tokens. The encoder processes the input and generates the embedding. The hidden states of the encoder are passed to the decoder to generate an output expression.}
%     \label{fig:model_architecture}
% \end{figure}
There are several choices for modeling encoders and decoders. We use the Transformer architecture \citep{vaswani2017attention} in this work. It has been successfully used in various NLP applications, like machine translation, language modeling, and summarization, to name a few, and it has also been shown to work with mathematical expressions \citep{lampleDeepLearningSymbolic2019}.

\paragraph{Embedding Vector.}
Every encoder layer in the Transformer architecture generates hidden states corresponding to each token in the input sequence. We use max pooling on the hidden states of the last encoder layer to generate the continuous vector representation or the embedding vector of the input expression. The special tokens, like the start-of-expression and end-of-expression tokens, are not considered for max pooling. Furthermore, we considered the following other candidates to generate the embedding vectors but chose max pooling based on its performance in our initial experiments - (1) average pooling of the hidden states of the last encoder layer and (2) the hidden states of the last encoder layer corresponding to the first and the last tokens.


% We use GRU to model our encoder and decoder and use the additive attention mechanism \citep{bahdanau2015neural} in the decoder. The final hidden state of the encoder depends on the entire input sequence and interpreted as the \textit{continuous vector representation} or \textit{embedding} of the input expression. We chose a GRU-based model for simplicity, and it can be replaced with other sequence-to-sequence architectures. An overview of our model is shown in Figure \ref{fig:model_architecture}. Refer to the appendix for the mathematical description of the model and dimensions of different layers in our architecture.

% We use with $H$ = 128 (\expemba{}) and $H$ = 1024 (\expembe{}) for the Equivalent Expressions Dataset and $H$ = 64, 128, 256, 512, 1024 (\expemba{} and \expembe{}) for the \semvec{} datasets. We use the AdamW optimizer \citep{loshchilovDecoupledWeightDecay2019} for our experiments. See appendix \ref{appendix:training_details} for details.

\paragraph{Data Formatting.}
Mathematical expressions are typically modeled as trees with nodes describing a variable or an operator \citep{ion1998mathematical}. Since we are using a \textsc{Seq2Seq} model, we use the Polish (prefix) notation to convert a tree into a sequence of tokens. For example, $\frac{\sin(x)}{\cos(x)}$ is converted to the sequence $[\textrm{div}, \sin, x, \cos, x]$ (Figure \ref{fig:equiv_exp_example}). The primary reason for using the Polish notation is that we refer to the datasets created by \citet{lampleDeepLearningSymbolic2019} which are in Polish notation (details in Section \ref{sec:datagen}). Furthermore, their work shows that the Transformer model is capable of generating valid prefix expressions without requiring constraints during decoding. In our experiments, we also find that invalid output prefix expressions become increasingly rare as the training progresses. Hence, we use the Polish notation for simplicity and leave the exploration of tree-based representations of mathematical expressions for future work. The tokens are encoded as one-hot vectors and passed through an embedding layer and positional encoding before being fed to the encoder or decoder.

\paragraph{Decoding.}
We use the beam search decoding \citep{koehn2004pharaoh} to generate expressions at inference. The beam search algorithm is essential in finding an output with as high a probability as computationally possible. As the main focus of this work is vector representations and the beam search decoding is only used in equivalent expression generation, we use a simple version of beam search and do not employ length penalty, diverse beam search, or any other constraint during decoding.