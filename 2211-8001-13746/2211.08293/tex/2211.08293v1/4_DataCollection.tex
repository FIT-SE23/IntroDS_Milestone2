% For two-column wide figures use
\begin{figure*}
% Use the relevant command to insert your figure file.
% For example, with the graphicx package use
%  \begin{center}
\centering
  \includegraphics[width=0.70\textwidth]{EI_DataCollection.png}
\caption{Architecture of the EventIndex Data Collection system based on Object Store. The data flow is described in section \ref{sec:43}. The thick arrows indicate the flow of EventIndex data from the Producers to the Consumers, going through the Object Store; the red arrow marks the messages sent by the Producers to the Supervisor through the message broker; the light dotted green and the blue arrow correspond respectively to the information stored by the Supervisor in the Object Store about the location of all objects related to a given dataset, and the signal sent to the Consumers that all data for a dataset is available in the Object Store; the dark green arrow marks the messages sent back by the Consumers to the Supervisor to signal the completion of a given dataset transfer.}
%  \end{center}
% figure caption is below the figure
\label{fig:OSDCschema}       % Give a unique label
\end{figure*}


\section{Data Collection}
\label{sec:4}
The Data Collection system receives and validates the information extracted by the producers, assures its completeness, and orchestrates the EventIndex data transfer from the  producers that run on the WLCG Grid to the Hadoop cluster at CERN. Depending on the number of processed events, each indexing job produces between 100 kB and a few MB of information to be transfered to the central servers.

\subsection{Messaging system}
\label{sec:41}

Messaging is a key component of the Data Collection system. 
In the original implementation of the producer transformation \cite{OldMon}, the output file was serialized and
packed into JSON messages, sent to ActiveMQ brokers \cite{ActiveMQ} at CERN 
using the STOMP protocol \cite{STOMP}. 

Two different types of messages were used in the messaging based data collection architecture:
\begin{enumerate}
\item \textit{Data Messages}, containing the produced data. They ranged from 1 to 10 kB and were tagged in a way that all messages from the same producer were consumed by the same consumer. Larger payloads were split into 10-kB chunks and sent as independent messages; the consumer processes were then recombining them into a single file.
\item \textit{Status Messages}, allowing the tracking of the indexing processes. They were sent to a different queue, where they were collected. The produced information was validated by means of the status messages.
\end{enumerate}

Although this architecture was reliable and fully functional, there were concerns regarding its ability to cope with peaks of production activities, as all information was kept in the brokers until it was consumed. During peak times, this could lead to a considerable growth in the number of messages that the brokers have to keep until they can be delivered and consumed.

The current system still uses the messaging system for \textit{Control Messages}, which are similar to the original status messages. In this way the amount of data flowing through the ActiveMQ servers was reduced from a few megabytes (dominated by the data messages) to a few tens of kilobytes per job (just the control message).

\subsection{Object Store}
\label{sec:42}

Alternatives to minimize the impact of the expected increases of data-taking rates on the messaging architecture were investigated \cite{OldMon}, resulting in the replacement of data messages by temporary objects written into an Object Store  \cite{OSEI}. Figure \ref{fig:OSDCschema} describes this approach. A temporary object is created by each producer job, containing the information that the producer transformation (section \ref{sec:31}) extracted from the processed files. Once the object is written into the S3 Object Store \cite{ObjectStore} at CERN, a \textit{Control Message} containing a summary of the information and the URI of the object is sent to a new entity, the EventIndex Data Collection Supervisor (section \ref{sec:43}), that orchestrates all data collection activities. 

The information sent through the messaging mechanism has therefore been drastically reduced from several megabytes to tens of bytes for each job, keeping the brokers in a well-performing status. If a producer, for any reason, is not able to write the data into the object store, there is a fallback  solution based on the CERN large-data store EOS \cite{EOS} also at CERN, using the \texttt{xrdcp} protocol \cite{XRDCP}.

\subsection{Index record format}
\label{sec:32}

Two different file formats were used to store the output from the producer,
adapted to the specific needs of the processing.

Initially a SQLite3 \cite{Sqlite3} format was reused from
other Athena tools, with data  stored in key:value pairs using only one table
with two columns, "key" (TEXT) and "value"
(BLOB). "Value" is the serialized representation of a python object
using \texttt{cpickle}, so arbitrary objects can be saved and retrieved into
the database, allowing a flexible “blackboard” style storage of
key:value pairs. The producer transformation used several key: value pairs to
store general information like the number of files and events
processed, date and time of processing, job and task identification,
file GUIDs, input collection name (dataset name), etc. Events were
saved consecutively as an ordered tuple with key “Entry N” where N is
the entry number.
This file format was successfully used by the first producer
implementation, but when it was decided that the file was going to be
sent "as is" to the S3 Object Store it was soon realized that it was
not the best format for the new needs.

The current producer uses a format based on the Google Protocol
Buffer \cite{ProtocolBuffers} with gzip \cite{Gzip} compression. 
This format allows the
consumers to read the files easily and the size reduction achieved by the
compression allows faster transfer times and requires fewer resources  in the S3
Object Store.

This format contains a Stream of Protocol Buffer (ProtoBuf)
messages (SPB) compressed using the gzip library on the fly. The
uncompressed file starts with a "magic" fixed\_uint32 value
(0x6e56c8c7) so it can be identified quickly. Since ProtoBuf messages
do not have type information, all messages have extra prepended
information to identify the message type; two fixed\_uint32 integers containing the type and message version and its length
are added before the message itself.

The file can contain six different message types: Header, Trailer,
BeginGUID, EndGUID, TriggerMenu and EIEvent:
\begin{enumerate}
\item 
Header: contains global information about the processing step, like task
and job identifications, input dataset name and start processing time.
\item 
Trailer: contains global information collected during the processing,
like number of files read, total number of events and end processing time.
\item 
BeginGUID: marks the start of a new input file being
processed. These messages contains the input file unique global
identifier (GUID), and the start processing time besides some
other ATLAS metadata information like the processing version, the stream and project names.
\item 
EndGUID: marks the end of the input file processing. It contains
the number of events read for this file and the end processing time.
\item 
TriggerMenu: contains the trigger menu used during data taking for
the next collection of events. This message is sent once per file read and 
whenever the trigger menu changes.
\item 
EIEvent: this is the main part of the EventIndex. It contains
the event record described in section \ref{sec:22} like the
run number, event number, trigger mask, time of data
taking, etc
\end{enumerate}

The EventIndex file contains one Header message at the beginning and
one Trailer message at the end. Between them, one or several sequences
of processed files which begin with BeginGUID and end with
EndGUID. For each processed file one or more
TriggerMenu messages and a sequence of EIEvent records are written.

Although the protocol buffer format tries to store the information using the
least space possible, the compression factor obtained is greater than
86\%, as consecutive EIEvent messages usually contain
very similar (and partially equal) information, so the compressor can reduce the space very significantly.

\subsection{Supervisor}
\label{sec:43}
The Data Collection Supervisor is in charge of tracking all data collection steps. It validates the produced data and informs the consumers about the presence of data to be transferred into the Hadoop cluster at CERN. It also allows following the indexing progress of datasets and containers thanks to its web interface.

The Supervisor receives messages sent by each producer job with  information about what has been processed. This information includes among other things: the dataset name, the task and job identifiers, the location of the produced object store, the GUIDs of the processed files, and the number of events and the number of different event identifiers processed per file. The supervisor collects this information, and it is thus able to know which datasets are being indexed by which tasks in which system.

As part of the file and dataset metadata information, Rucio stores the number of events that they contain. This information is used to track the progress of dataset indexation. Furthermore, since each job sends the number of events that it has processed per file, this information can be compared against the one provided by Rucio to identify possible inconsistencies.

The supervisor also retrieves the information of the indexing tasks before they  achieve a final state. To do this, the supervisor has to contact and decode the information provided by two different monitoring systems: conTZole \cite{conTZole} if the tasks is running in Tier-0, and PanDA Monitoring \cite{PanDAMon} if the task is running through PanDA on the WLCG Grid. Both monitoring systems provide information on the progress of tasks, like the number of jobs, number of events processed, status of the task and jobs, etc. Once the task has reached a final successful status, all the collected information from the jobs, from the task monitoring system, and from Rucio can be cross-checked: 
\begin{itemize}
    \item Each successful job should have produced and sent a message to the supervisor.
    \item Each file should have been processed by at least a job.
    \item The number of processed and produced events per file should match the number of events in the file according to Rucio.
\end{itemize}

When those checks are satisfied, the supervisor can assure the completeness and correctness of the produced information; then a validation object is created and stored in the Object Store. Among other information, the validation object contains the URIs of the produced Object Store objects that allowed the validation, as well as, for each object, the list of the files that were processed in that job and should be considered as valid. The consumers are informed through the messaging system about the validation objects that they should consume; they first retrieve the validation objects, process them retrieving from the Object Store the information that should be consumed and put into Hadoop. When all data have been consumed, they notify the supervisor about it, signalling once again how many events have been consumed. With this last message, the supervisor can mark the dataset as indexed.

Inconsistencies in the number of events, unprocessed files, lost or delayed messages, can be identified thanks to this validation procedure. A dataset that is not validated is kept in a validation queue, where the validation will be retried in after receiving possible delayed messages.

With all these pieces of information from different systems the supervisor is able to:
\begin{itemize}
    \item Monitor the progress of the tasks that index each dataset;
    \item Identify failed production tasks;
    \item Declare obsolete indexing tasks that have problems and are going to be replaced by other tasks;
    \item Detect if any messages were lost;
    \item Identify inconsistencies between the processed files and the information retrieved from Rucio;
    \item Complete missing pieces of information in Rucio in the rare cases when they occur;
    \item Notify, through email if needed, about datasets that have duplicated event numbers detected at job level, i.e. within the few files of the dataset that were processed by the same job;
    \item Identify failed data transfers due to the death or disconnections of the consumers from the brokers;
\end{itemize}

Figure \ref{fig:OSDCschema} shows the data collection process with the interactions between the different components and the information flow.


\subsection{Consumers}
\label{sec:44}

The Consumers are in charge of storing the EventIndex data in the final data store. They run centrally at CERN and in the current system there are as many consumers as messaging brokers, as this is sufficient for the current production rates. They are stateless independent entities that can be scaled up in case of necessity.

Consumers wait for validation messages from the Supervisor, containing  references to the actual EventIndex data objects to be ingested. 
These objects are read from the Object Store with data encoded with Protocol Buffers \cite{ProtocolBuffers} format. The data are then formatted for the current production schema, and stored in Hadoop files. These files are  organized in directories named after each dataset container, and the current granularity is to write a file per dataset, but this is also configurable in the validation object. Each file contains data organized by key containing RunNumber-EventNumber, and its related value encoded in a CSV schema with all the event information.
Information about the status of the processing is communicated back to the Supervisor, starting with the acknowledgment of the request. When all the objects are consumed and the file is written, the result is sent back with a control message again. In case of any failure, details are included. It must be noted that the granularity of the validation data can vary from a single object reference, to thousand of them belonging to a particular dataset.

This procedure is repeated for all validation messages produced for a dataset container. At this point the validation of the complete dataset is possible, and a different control message will trigger the final validation of a dataset container. This procedure writes a control text file in the Hadoop file system, containing all the URLs of the individual dataset files that were validated. This file is used by Oracle (section \ref{sec:6}) to know which data has to be imported. The Hadoop storage system (section \ref{sec:5}) requires an extra control file residing in AFS \cite{AFS}, with the same validation information, therefore this additional file is also written at this step.

An individual Consumer typically processes a mean of 15 kHz (events processed per second), and we have observed a maximum of 28 kHz. The current implementation of the Consumer is a multi-threaded Java program, with thread pools using the Future pattern \cite{Future} in order to save resources, and exploit parallelism among internal data access and transformation tasks. The setup of the Hadoop writing channels and the input/output largely dominates the processing time, with the CPU used on data mangling and schema transformation taking a small percentage of the time.
The Consumer design allows to easily include new data sink plugins, and it has been extended to support new data back-ends like Kudu \cite{Kudu}, and now HBase/Phoenix \cite{HBase,Phoenix}. 


