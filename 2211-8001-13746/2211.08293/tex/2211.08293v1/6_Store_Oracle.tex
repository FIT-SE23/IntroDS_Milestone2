\section{Data Storage in Oracle}
\label{sec:6}

\begin{figure*}
\centering
  \includegraphics[width=0.99\textwidth]{EIO_BasicSchema3.jpg}
\caption{Relational tables of the EventIndex Oracle (EIO) schema. 
% Arrows indicate one-to-many relationships. 
The "Datasets" table contains one row for each imported dataset. The unique events for each dataset are stored in the "Events" tables and its duplicated events (if any) are stored in the "Event Duplicates" table. Aggregated information about each dataset is stored in "LBN Counts" (a count of events per Luminosity Block) and "Dataset Overlaps" (the number of common events between datasets of the same run number).}
\label{fig:EIOschema} 
\end{figure*}

The initial implementation of the EventIndex store in Hadoop showed several important shortcomings by the end of 2015, the first year of LHC Run 2. With the versions of Hadoop and HBase and the hardware setup provided by CERN that we could use at that time, all queries became  substantially slower as the amount of stored data increased. Simple event lookup queries for 10 events started taking over 1 minute instead of the expected sub-second response, and counting events across large datasets (100 million events) took tens of minutes.

In addition to optimising the Hadoop cluster setup, it was then decided to explore the possibility of storing a subset of the real data information in an Oracle database, exploiting this well-known technology to support the most important and time-consuming use cases, primarily event picking for real data events. For each real data event, the event record without trigger information, which constitutes 80\% of the data volume, is copied to an Oracle database.
Locating this data in existing Oracle servers also allows us
to easily connect to other metadata in existing complementary repositories like
the COMA \cite{COMA} and AMI \cite{AMI} systems, which store metadata related to runs and datasets, respectively.

\subsection{Data structures}
\label{sec:61}

A relational model was found to be well suited to the task \cite{EIOra}.
The simple relationship of datasets to events 
lends itself to a very simple relational table structure as shown in Figure \ref{fig:EIOschema}.
The two leftmost tables (in blue) store each indexed dataset and its events, 
respectively, driving the core functionality of the system as well as serving
some secondary use cases.
In ATLAS, datasets are uniquely identified by a string concatenating 6 fields,
each of which is stored as a separate column in the \textit{Datasets} table:
\begin{enumerate}
\item Project name: A string encoding the LHC beam type with the year of data taking,
\item RunID: The run number,
\item Stream name: Events passing specific triggers are written 
  to one or more data streams,
\item Data format: The stage of processing at which the data is indexed 
  (usually AOD),
\item AMI Tag: a string encoding the processing steps these events have undergone,
  \textit{i.e.} effectively the processing version for the events in this run and stream.
\item Production Step: A short string to distinguish between 
  an intermediate or final processing stage.
%  (EventIndex collects metadata generally only from the final stage of processing).
\end{enumerate}
Since the Datasets table is the parent table for all other tables of the schema,
this table has an integer primary key associated with each indexed dataset name
as is common in relational database design.

For each dataset, all events are stored in the \textit{Events} table.
We store up to 3 GUIDs per event, which we found to be sufficient.
References are numbered starting at 0 (the GUID of the indexed file),
with subsequent references 1 and/or 2 used for its upstream file formats.
Since available GUID types (RAW, AOD, and DAOD) are common 
to all events in the dataset, GUID types are stored only once in the Datasets table 
(another advantage of relational design).
Also, at the event level, we store the event's luminosity block number (LBN)\footnote{A luminosity block is a short time period, usually one minute, during which the LHC luminosity (interaction rate) and the detector conditions can be assumed to be constant.} and LHC bunch crossing identifier (BCID), 
which are useful for use cases described in section \ref{sec:62}.

The \textit{Event Duplicates} table keeps the list of all duplicated events found.
Duplication can (and has been shown to) occur at any stage in processing.  
In the initial 2015 loading of the data, 
hundreds of datasets were found with duplicate events;
using data stored in this table combined with event counts
at each stage of processing (from the AMI database), 
we could identify the stages at which duplication occurred.
Subsequent refinements in upstream systems have considerably reduced 
the occurrence of duplicated events in current data.

The \textit{Dataset Overlaps} table stores the number of events in common between 
different datasets of the same run.  This data serve a secondary use case
of providing these overlaps to experts in DAOD production for 
refinement of the ATLAS Derivation Framework \cite{derivation}.

The \textit{LBN Counts} table stores the event counts and the number of associated 
unique GUIDs by LBN.  Data in this table have multiple secondary uses 
including many forms of integrity checks, determining the probable LBN
for an event, investigating missing events and files, and understanding the
splitting of luminosity blocks at file boundaries.

Both the 'Overlaps' and the 'LBN Counts' table content could be computed 
dynamically using the data in the two primary tables,
but we chose to materialize this data in these tables since the
computation can take more than a few seconds, 
the data volume is minimal,
and some of the aggregated data is used in multiple services.

When a new dataset appears in Hadoop storage, 
it is considered for import into Oracle if the run exists in the COMA system 
(which contains only runs of potential physics interest),
and if the Stream meets similar selection criteria 
(e.g. there are no known use cases for indexing datasets in calibration streams).
If the dataset passes these requirements, 
the ``Import process'' stage imports the dataset and its events into 
``Staging tables'' which are similar to the final tables,
but without indexes or constraints.
The ``Oracle scheduler Jobs'' stage runs verification checks such as 
checking that the events are consistent with belonging in the same dataset
and flagging if any duplicate events are found.
If the data passes verification checks,
the data is moved to the ``Destination Tables'',
writing any duplicate events to a separate table,
while keeping one copy for the Events table.

Subsequently, supplemental information is added to the Datasets table
including data from other repositories (COMA and AMI)
as well as aggregated information from Events table loading.
This includes dataset status flags, various relevant dates, 
event counts both within the system as well as related counts in 
ATLAS file systems (upstream dataset files), 
counts of unique GUIDs associated with the dataset, 
and counts of total and unique duplicated events.
In addition, the datasets with Run, Stream, and Data Format in common
are ranked by dataset creation date,
which is useful to help users find the latest processing of a set of events.
These columns are used to enhance various services, 
and/or are included in user interfaces and reports.

\begin{figure}
\centering
  \includegraphics[width=0.48\textwidth]{EIO_Browser.jpg}
\caption{The EIO dataset browser entry page. Users can set search filters by clicking on pre-defined options or typing in the text boxes on the left, and then select the kind of report by choosing from the menu on the right. The reports are described in section \ref{sec:62}.}
\label{fig:EIObrowser} 
\end{figure}

A number of additional database optimization techniques 
deployed in this system, which further minimize storage volume 
(beyond relational normalization mentioned previously), 
transaction volume and database load, and 
optimize query performance for use cases, strongly deserve mention:
\begin{itemize}
\item 
  The Events table is "list" partitioned by DATASET \_ID. 
  The main advantage is that sets of events can be deleted by simply dropping
  the associated partition.
  This operation is needed more often than we initially expected 
  because datasets are sometimes re-indexed because of constituent file loss 
  on the grid (which invalidates the associated GUIDs).
\item 
  For the Events table we use Oracle's "basic" compression for table data 
  and key compression on its primary key index. 
  Moreover for data loading we use Oracle's direct data load interface. 
  In combination, storage utilization is reduced by a factor of about 3.5
  which has the added advantage of reducing similarly the I/O footprint for 
  writing data, undo and redo to the storage subsystem.
\item
  Up to three GUID reference columns per event in the source data are 
  36-character strings (for example "21EC2020-3AEA-4069-A2DD-08002B30309D").  
  In our Events table, 
  we store these columns using the non-standard ``RAW'' data type,
  reducing the 36 bytes of storage per GUID to 16 bytes.
  This considerably decreases the Events table per-row volume 
  without loss of functionality:
  when the GUID columns are queried, an Oracle function easily converts
  them back to the original CHAR type 
  (event lookup is always by EventID, not by GUID).
\end{itemize}

\begin{figure}
\centering
  \includegraphics[width=0.48\textwidth]{EIO_Overlaps.jpg}
\caption{The EIO dataset overlaps report, as an example of the functionalities provided by the EIO dataset browser. This report shows the count and percentages of events in common between selected datasets, as described in section \ref{sec:62}.}
\label{fig:EIOoverlaps} 
\end{figure}

After optimization, the storage volume is $\sim$20 bytes per event,
a factor of 10 reduction from the initial 210 bytes per event 
for this data imported from Hadoop.
This reduction, however, is only for the table segments.
Adding the primary key index overhead, the reduction factor drops to 5
(the size of the parent table is negligible, only 8 MB).
So overall, including indexes, storing $25 \times 10^9$ events 
requires less than 1 TB of space (rather than 5 TB).
The savings of 4 TB of disk space, in itself, is not the foremost point
but has a knock-on effect which is particularly beneficial for query performance:
it enables the caching of a larger fraction of the database rows 
into the database data cache (buffer pool) 
which yields real performance gains in query response (around 10 ms for simple queries).
In summary, using a relational model and a number of carefully chosen techniques
available in Oracle RDBMS results in an impressive minimization of resources 
while exceeding performance goals.


\subsection{Web interface}
\label{sec:62}

The central part of the user interface is the EIO (Event Index in Oracle) Browser shown in Figure \ref{fig:EIObrowser},
which allows users to easily find indexed datasets and their properties.

The browser offers dynamic filtering of datasets 
by any of the dataset name fields and/or other dataset characteristics.
With each iteration of selection criteria, the system shows 
the number of remaining datasets meeting the criteria and 
displays the remaining criteria.
Once the user has selected their dataset(s) of interest,
they can choose from the following services:
\begin{itemize}
\item \textit{Event Lookup}
  serves the primary use of returning GUIDs for user specified events
  (RunID/EventID pairs).
  In the absence of a user specified dataset version, 
  GUIDs from the highest EIO-derived ranked dataset are returned.
  The report provides additional details about the events found
  as well as information to help determine why events were not found.
\item The \textit{Dataset Report}
  includes a table displaying details about each selected dataset: 
  the collected and derived information in the Datasets table.
  EIO event counts are compared to counts of the corresponding upstream dataset
  files which helps to understand event losses/filtering at each stage of processing.
  Links are provided to related AMI dataset and COMA run reports
  and to other EIO services described herein.
\item The \textit{Dataset Overlaps Report}
  shows the count and percentage of events in common between 
  selected datasets of any run that is 
  useful for the resource optimization
  of the offline production of DAOD \cite{derivation}.
  Results are displayed in a color enhanced 2-D matrix 
  (as in Figure \ref{fig:EIOoverlaps}) showing datasets which overlap by more 
  than a 70\% threshold.  This threshold and a choice of two
  overlap computation algorithms are configurable in the interface.
  
\begin{figure*}
\centering
  \includegraphics[width=0.99\textwidth]{Monitoring.png}
\caption{Functional schema for the monitoring system of the EventIndex components. Several modules collect information from multiple sources about the status of EventIndex processes (top-left), functional and performance tests (bottom-left) and the computing infrastructure (top-right) and store this information in an InfluxDB database; the data are then displayed in Grafana dashboards.}
\label{fig:mon}
\end{figure*}

  
\item The \textit{Duplicate Event Report}
  displays all copies of events with any duplicated event identifiers in a dataset.
  It shows clearly the LBN(s) where duplication occurred and the associated GUIDs, 
  from which, combined with event counts at each stage in processing, we can 
  unambiguously determine the processing stage in which the duplication occurred.
\item A \textit{Missing Event Report} can be generated 
  when a dataset has fewer {\it unique} events than expected.
  The cause may be intentional filtering or an unintentional error 
  resulting in in-file event loss or entire files of events being lost;
  reports show event counts (and computed losses) at each processing stage.
  If those events have been completely processed and indexed 
  in another version of processing,
  the report shows lost event ranges and associated LBN(s).
\item The \textit{Count by BCID Report} 
  displays event counts in each LHC bunch crossing (BCID).
  During collision operations, one clearly observes the correlation in 
  the peaks of recorded events with BCID with the LHC fill configuration of the run.
\item The \textit{Count by LB} and \textit{GUID Reports}
  both display event and GUID counts along with EventID ranges per LBN
  which have been aggregated in the LBN Counts table.
  The GUID Report further shows the distribution of LBNs by GUID
  for selected LBN ranges: this is a useful cross check 
  of event in-file metadata, which on occasion had incorrect counts
  causing problems in the ATLAS luminosity accounting software.
  This report has been helpful to identify incorrect in-file metadata since 
  this system gets this information via a completely different path.
\end{itemize}
