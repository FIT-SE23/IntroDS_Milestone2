\section{Data Production}
\label{sec:3}

The Data Production system includes all tasks that are executed at the sites where the datasets to be indexed reside, in order to collect information from the event files and transfer it to the central EventIndex servers.

\subsection{Dataset selection}
\label{sec:33}

As soon as new ATLAS data are processed on the CERN Tier-0 cluster \cite{Tier0} and  the corresponding AOD datasets are available, jobs are launched to extract the EventIndex information for all "physics" datasets; calibration, test and monitoring streams are excluded. From this indexing step, information on the location of each event in RAW format is also extracted.
 
Many more ATLAS datasets are produced using the ATLAS resources of the WLCG Grid, and other additional resources that can be available from time to time. They include the whole simulation chain, from event generation to detector simulation, then event reconstruction and selection for analysis; real events are also re-reconstructed from time to time on the Grid, and all analysis selections also take place in a distributed fashion. 

%Some of the produced datasets are transient, such as the outputs of simulation jobs before being merged into larger files, while others are permanent, or at least long-lived until they are replaced by a newer version. 

All AOD datasets are indexed, and for real data all types of derived AODs (DAODs) are indexed too. For the simulated data only some types of DAODs are indexed, if requested by the analysis groups that use them. In addition, all EVNT datasets are also indexed.

Datasets produced on the Grid are selected for indexing according to the following criteria, based on information obtained from the ATLAS metadata database AMI \cite{AMI}:

\begin{itemize}
    \item The dataset is marked in AMI as complete and validated for use in analysis or further processing.
    \item The dataset is marked as long-lived, to avoid indexing transient datasets that are only used in internal steps of the production procedure.
    \item The dataset is part of a regular production processes and not just used for checks or validations of the software or the trigger configurations.
\end{itemize}

Selected datasets should then pass additional checks, to exclude datasets that have a "bad" status in Rucio, are known for problems or have other signs of corrupted data that may cause import jobs to crash or result in excessive computing resource consumption.

\subsection{Indexing job submission}
\label{sec:34}

Datasets that were selected for indexing have to be processed by the Production and Distributed Analysis system (PanDA) \cite{PanDA}. PanDA takes the list of new datasets and generates jobs that run a predefined "transformation" (a script containing a sequence of algorithms to be executed on a data file \cite{JobTransforms}) on the WLCG Grid. 

The transformation used and its configuration in general depend on the data format of the dataset and the type of data (simulated or real); for example, the trigger information, which constitutes a large fraction of the EventIndex data, is collected for each event only from datasets in AOD format as it won't change with subsequent processings of the same event. The progress of these jobs can be monitored through a dedicated dashboard; if necessary jobs can be aborted or rerun.



\subsection{Producer transformation}
\label{sec:31}

The Producer is in charge of extracting the EventIndex information
from the actual input files, store it into temporary files and send
them to a central location at CERN. It has to be able to run using the
ATLAS production infrastructure on all available production facilities 
(the Tier-0 cluster at CERN and the WLCG Grid), so it is
implemented in a way very similar to standard ATLAS data processing
programs using the ATLAS transformation framework running
within the Athena software framework \cite{Athena}.

Python was chosen as the implementation language for the Producer code, as it only accesses the header records of each event. The python interfaces to the Athena classes methods written in C++ do not change between releases, so the Producer code can be rather stable.

The producer input can be one or several files in the ATLAS specific ROOT format \cite{POOL,ROOT}, such as those in AOD , DAOD and EVNT datasets.  

The EventIndex transformation class implements all required methods by the Athena framework to initialize a job, execute the event loop (process the events) and
finalize the job. 
The current implementation runs using a serial processing model, so the input data structure (file and event ordering) is preserved without needing further post-processing.

The EventIndex producer has two separate steps, both running
within the transformation. In the first step, it reads events (\textit{execute}
method), extracts information and saves the relevant information into a temporary
file. When all events are read, the second step starts (\textit{finalize}
method), in which the output file is transferred to a central store at
CERN. Besides the EventIndex information itself, some additional environment
and processing information is stored: the PanDA task and job
identification, input dataset name, total number of files and events,
starting and ending processing times as well as identification (GUID)
and number of events for each file read.

This second step provides a good opportunity to check for
inconsistencies in  ATLAS data files as soon as they are produced. Currently the transformation looks
for event uniqueness within each file, so duplicate events that could result from failures in
previous processing steps, are detected here allowing quick
notification to ATLAS.


