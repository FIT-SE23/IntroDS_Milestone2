\section{Operations and Performance}
\label{sec:9}

\begin{figure}
\centering
  \includegraphics[width=0.48\textwidth]{EI_ProducerJobs.png}
\caption{Distribution of the EventIndex Producer jobs run each week between the start of operations in May 2015 and January 2022. Jobs run on the CERN Tier-0 system that indexes all real data as soon as they are produced and reconstructed are indicated in green; jobs run world-wide on the ATLAS Grid resources are shown in purple.}
\label{fig:jobs}       % Give a unique label
\end{figure}

Before the start of LHC Run 2 operations, we indexed all LHC Run 1 datasets in AOD format on the Tier-0 cluster. In this way we collected information on the RAW dataset provenance and on the trigger parameters of each Run 1 event. After that, real time operations started. The indexing jobs are distributed on all sites available to ATLAS, selecting primarily those that are closest to the input dataset location from the network point of view, as shown in Figure \ref{fig:jobs}. The indexing jobs are fast, as the producer transformation only reads the header of each event and takes between 10 and 50 ms/event, depending on whether the trigger record is needed or not; each job indexes several files and runs for 30 to 60 minutes. The total CPU consumption of EventIndex Producer jobs is well below 10$^{-4}$ of the total CPU power used by the ATLAS experiment world-wide. 

The percentage of jobs failing has been around $7\%$, with the main causes of errors being problems related to the input files and the sites operation, such as corrupted files, sites with storage or disk configuration issues, stage-in problems, etc. The indexing jobs are the first jobs run on just-produced datasets, so they are useful to detect at an early stage any problem with data corruption or unavailability. After contacting site administrators and the ATLAS data management operations team, the problems are usually solved promptly, so that simply re-running the problematic jobs is sufficient to achieve consistency.

 The number of stored event records increased approximately linearly as shown in Figure \ref{fig:Import}. Some data-sets, especially those with type DAOD, do not have an infinite lifetime but are periodically replaced by newer versions generated with better calibrations or improved algorithmic code; after some time the old versions are deleted. The down steps in Figure \ref{fig:Import} correspond to periodic clean-up operations that remove the information regarding obsoleted datasets.

\begin{figure}
\centering
  \includegraphics[width=0.48\textwidth]{EI_Data2022.png}
\caption{Event records (top) and datasets (bottom) stored in the Hadoop system between May 2015 and February 2022. Each plot shows separately real data in red, simulated data in blue and their sum in black.}
\label{fig:Import} 
\end{figure}



\begin{figure}
\centering
  \includegraphics[width=0.48\textwidth]{EI_Hadoop_Data.png}
\caption{Data volume used in the Hadoop cluster, split by data type, June 2021.}
\label{fig:Volume} 
\end{figure}

The current amount of disk space used by the Event Index data in the Hadoop cluster is shown in Figure \ref{fig:Volume}. Most of the disk space is used by real data and MC AODs, which contain the trigger records. The event generator datasets (EVNT) and derived analysis formats (DAOD) contain many more event records, but without trigger information, as it is either not yet available (in case of the generator-level EVNT datasets) or retrievable from the corresponding AOD datasets (for the derived formats DAOD).

The Hadoop system runs a variety of tasks, importing and cataloguing data, running consistency checks, establishing links between related datasets, and last but not least responding to user queries. Figure \ref{fig:Access} shows the daily access statistics of the major Hadoop services. Accesses count all data handling procedures, including data import, user queries and functional tests.

\begin{figure}
\centering
  \includegraphics[width=0.48\textwidth]{EI_Access2.png}
\caption{Access statistics of the Hadoop system between May 2015 and June 2021. The statistics is dominated by internal processes, like data imports, event counts and consistency checks, plus the regular functional and performance tests.}
\label{fig:Access} 
\end{figure}


\begin{figure}
\centering
  \includegraphics[width=0.48\textwidth]{EL_time_1M.png}
\caption{Response times of the EventIndex Hadoop server to event lookup queries selecting 10, 100, 1000, 10k and 50k events out of a dataset with 1 million records as a function of time, recorded between April and December 2021. Note the occasional longer response times due to other activities in the Hadoop cluster.}
\label{fig:EItests}
\end{figure}

Figure \ref{fig:EItests} shows the response times of the Hadoop server to event lookup queries selecting 10, 100, 1000, 10k and 50k events out of a dataset with one million records as a function of time. The event lookup is performed through the \texttt{el} client command, selecting randomly different events each time in order to avoid using cached results. The occasional glitches are due to other activities on the servers at the time of the queries. The response times are dominated by the query time for low numbers of events, and by the transmission time of the output for large numbers of events.

\begin{figure}
\centering
  \includegraphics[width=0.48\textwidth]{EI_time_all.png}
\caption{Response times of the EventIndex Hadoop server to queries using Map/Reduce jobs to  retrieve information on all events from datasets containing 50k, 100k, 1M and 10M events as a function of time, recorded between February and June 2021. The discontinuity corresponds to a day of Hadoop cluster maintenance, during which no statistics were collected.}
\label{fig:EIperf}
\end{figure}

The response times of the  Hadoop server to queries retrieving information on all events from datasets containing 50k, 100k, 1M and 10M events are shown in Figure \ref{fig:EIperf} as a function of time. These queries are performed through the \texttt{ei} client command. The response times are dominated by the setup time of the Map/Re-duce job for low numbers of events, and by the transmission time of the output record for large numbers of events.

\begin{figure}
\centering
  \includegraphics[width=0.48\textwidth]{EIO_size.png}
\caption{Disk storage size used by EventIndex tables in the Oracle cluster. Almost half of the disk space is taken by the large amount of indexes stored alongside the main data to optimise the read performance.}
\label{fig:EIOvolume}
\end{figure}

The EventIndex data stored in Oracle increase in size in parallel to the growth of the main Hadoop store, as shown in Figure \ref{fig:EIOvolume}. Due to the relational database nature of Oracle, a large amount of indexes is stored together with the actual payload data, so that a similar amount of disk space is used by actual payload data and the indexes.


%The main use case for the EventIndex is "event picking", \textit{i.e.} the procedure to find and extract a single or a few events in a given data format and processing version from the file where it resides and transfer it to the user who requested it. Event picking is composed of two phases: event lookup, done through the EventIndex, and event extraction, usually done through the distributed workload management system PanDA. These two phases can be executed together, with the PanDA servers placing the event lookup queries to the EventIndex system, or separately, allowing the inspection of the result of the event lookup query before submitting the PanDA task; the latter operation mode is more robust in case of massive or repeated queries. 

Most of the event picking requests are for single events in RAW format; other requests are placed from time to time from physics analysis groups who need to extract their complete highly-selected data sample for further processing and/or more detailed analyses; so far the EventIndex system could cope very well with all requests.

\begin{figure}
\centering
  \includegraphics[width=0.48\textwidth]{EI_Pick_Jobs.png}
\caption{Event picking jobs run each week world-wide between January 2019 and January 2022.}
\label{fig:EvtpickJobs}
\end{figure}

Figure \ref{fig:EvtpickJobs} shows the statistics of the event picking jobs run between January 2019 and June 2021. During this period 9.5\% of the jobs were automatically "closed" and rescheduled to another site while waiting for the input file (or files) to be staged from tape, and 6.5\% of the jobs failed after waiting for the input file(s) for more than 3 days; resubmitting the same jobs normally works, as the wait time is then doubled. The tape reading queues work in FIFO mode, so it is not possible to assign a higher priority to tasks requesting a single file as opposed to staging large datasets needed by production activities.
