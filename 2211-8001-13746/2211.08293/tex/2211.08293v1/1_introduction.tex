\section{Introduction}
\label{sec:1}

The ATLAS experiment \cite{ATLAS} at the CERN LHC accelerator \cite{LHC} is in operation since 2009; it collected during the first two LHC data-taking periods (Run 1 between 2009 and 2013 and Run 2 between 2015 and 2018) almost 25 billion physics records ("events").
The particles accelerated by the LHC are grouped into many "bunches" that intersect each other every 25 nanoseconds, at a rate of 40 MHz. During each bunch crossing several independent interactions take place at almost the same time (within 0.5 nanoseconds), therefore the signals left in the detector by the particles produced by those interactions are recorded together as one "event". The number of these additional "pile-up" interactions varied from a few in LHC Run 1 to 50-60 during LHC Run 2 and is expected to increase further by the end of LHC Run 3 (2022-2025).
In addition to the real events, about three times as many simulated events were generated using Monte-Carlo methods.

\subsection{ATLAS data taking and data processing}
\label{sec:11}

The LHC accelerator operates in cycles; first the protons or ions are injected into the accelerator rings, then they are accelerated and finally their orbits are modified to bring the counter-rotating particle beams to intersect at the centre of each experimental apparatus. When the number of circulating particles has decreased beyond a certain level, they are extracted and directed towards the "beam dumps", where their energy is absorbed. Each such cycle is called a "fill" for the accelerator. A continuous data-taking session of the detector is usually referred to as a "run", typically lasting from a few hours to just over a day. 

The detector read-out system and the downstream processing chain cannot cope with the 40 MHz bunch crossing rate. A multi-level online selection system ("trigger") is needed to select the events of interest on the basis of combinations of signals in the detector that match the expected signatures for interesting physics processes, and extract their full information for offline processing.
The trigger system had a three-level configuration for LHC Run 1: Level 1 (L1), implemented in programmable hardware; Level 2 (L2), that made software-based partial reconstruction within the “regions of interest” marked by Level 1, and finally the Event Filter (EF), analysing the full event. For LHC Run 2 and Run 3, L2 and EF were merged into a single system, the  High-Level Trigger (HLT), implemented in software running on commercial processors. In order to keep the possibility to select rare physics processes, events satisfying triggers matching common physics processes or used for monitoring were prescaled, \textit{i.e.} reduced randomly in number in order to save output bandwidth. The trigger configurations and the prescale factors applied to each trigger type were recorded in the trigger database for each run \cite{Trig}.

Trigger decisions were stored in event data files as trigger masks, where each bit corresponds to the specific set of trigger selections (trigger chains). Depending on the trigger configuration, the same bit in the trigger mask (chain counter) may correspond to a different trigger chain. The relation between chain name and counter is uniquely defined in the trigger database table, indexed by the trigger key (SMK). Trigger keys depend on the run number for the data recorded by the detector and on the production settings for Monte-Carlo simulation. 

The events recorded by the ATLAS detector are transferred in real time to the CERN computer centre and processed within 48 hours from the end of a run using the ATLAS "Tier-0" cluster. This procedure consists of calculating the time-dependent detector calibration and alignment parameters, and then using them to compute for each event the physical quantities of interest for the final analyses. The events can be re-processed when improved reconstruction algorithms or detector calibration and alignment constants become available, usually at the end of each major data-taking period; the result is the creation of additional versions of the same events, some of which replace older versions. 

The reconstruction processes take events in "RAW" data format, as produced by the detector read-out system, and output them in "AOD" (Analysis Object Data) format. The events are then distributed and made available to all ATLAS members through the World-wide LCG Grid (WLCG \cite{WLCG}). The events can be further selected for different analysis purposes, and saved in compressed formats ("derived AOD", or "DAOD") with only the events and contents that are useful for a given analysis. These derivation processes can be run very frequently, even monthly, as the analysis code evolves in time, resulting in many DAOD versions with a relatively short lifetime as they are normally superseded by newer ones.

Simulated interactions go through a similar processing chain. The outputs of event generators are saved on disk in a common format ("EVNT"). Then the detector and read-out electronics simulation (digitization) processes are run; pile-up interactions are also simulated and added to the main interaction record during digitization. Finally, the same reconstruction and derivation processes as for real data are executed, producing events respectively in AOD and DAOD formats.

Groups of statistically equivalent events (real data events collected with the same detector conditions or simulated events produced with the same generator, and processed by the same software versions) are stored in files on disk or on tape. Each file typically contains between 1000 and 10000 events, depending on the format and balancing the need to avoid too many small files ($<$1 GB) that would cause data storage problems and at the same time too large files ($>$10 GB) that could have lower transfer efficiency.
Every unique file, regardless of its format, is assigned a distinct GUID (Globally Unique IDentifier, \cite{Guid}), which is used to catalogue and retrieve it.
Currently ATLAS has over 100 million files on disk, containing over 400 billion event records. Files are grouped into \textit{datasets} that can be hierarchically assembled into \textit{containers}. The distributed data management system Rucio \cite{Rucio} is used to keep track of each file, dataset and container, including their properties (metadata) and replica locations, as well as to manage the data movements between different storage sites and the CPU farms where the data are processed and analysed.

\subsection{Need for an event catalogue}
\label{sec:12}

Rucio is extremely efficient at managing 100 million files stored in 120 sites worldwide and using over 200 PB of disk space and similar quantities of tape storage space. Rucio allows analysers to easily access data and simulation samples using the existing organization of files in datasets and containers, but it does not store any information about individual events. Nevertheless there is also a need to be able to retrieve different versions of one or a few events, given the often reduced information contained in the last data reduction stages, either to produce nice event displays for publications, or to check in detail if all reconstruction procedures worked as expected.

The EventIndex was designed for this primary use case (quick and direct event selection and retrieval), but the same system can fulfill several other tasks, such as checking the correctness and completeness of data processing procedures, detecting duplicated events that can occur for temporary faults of the data acquisition or processing procedures, studying trigger correlations and the overlaps between selected data streams.

%The EventIndex is the second-generation event catalogue for ATLAS. During LHC Run 1, the "Tag DB" \cite{TagDB} was used, with somewhat different use cases as it contained mainly a subset of the physics variables estimated by the reconstruction software for each event. It was implemented as large and heavily indexed Oracle \cite{Oracle} tables, where every event with all its parameters corresponded to one row. As its performance fell below expectation when the amount of data increased, by the end of 2012 it was decided to explore the (then) new BigData technologies that were becoming increasingly popular and promising in terms of scalability with respect to data volumes, and start developing the new EventIndex in advance of the start of LHC Run 2 \cite{MetaCHEP2013,EICHEP2013}.

The EventIndex is the second-generation event catalogue for ATLAS. 
The first generation catalog deployed for LHC Run 1 was called the "TAG DB"\cite{TagDB}.
Its content was based on the direct import of event-wise information 
from ATLAS TAG files into an Oracle database to facilitate queries 
of all events across entire datasets, which could speed up the analysis workflow.
TAG files were thumbnails of the AOD format files
produced in the final stage of the official production chain.
The TAGs contained, in principle, sufficient information for identification and selection 
of events of interest to most physics analyses for the purpose of subsequent event skimming, 
\textit{i.e.} the pre-selection of events based on any of the TAG quantities 
before accessing the full AOD file content.
There were generally two problems with the TAG database:
\begin{enumerate}
\item 
TAG information included immutable content 
(such as event identification and trigger decisions) 
as well as mutable content (basic physics quantities such as 
the number of loose electron candidates and their kinetic information).
The inclusion of the mutable content was problematic because in practice, 
recalibrations and software improvements were made 
in post-processing or reprocessing steps, which skipped the production of new TAG files, 
so the TAG DB mutable content became out-of-date (stale) for accurate event selection
based on that content.
There was no easy way to refresh mutable content without fresh TAG files.
\item
The TAG database structure was, for simplicity, dictated by the TAG files 
because the skimming functionality was based on being able 
to produce TAG files from the database.
This meant that mutable content could not be easily separated from the immutable content, 
so all content was put into single database tables with hundreds of columns.
To enable fast queries based on any of the content every column was indexed,
which was a very heavy implementation on the database side,
requiring a lot more storage than was ever actually used due to the first problem.
While the mutable content became a dead weight on the system,
there were many use cases for the immutable content which were utilized.
\end{enumerate}
So an inherent problem with the TAG DB was the dependence of the system 
on the ATLAS production processing chain to produce TAG files
with each iteration of recalibration and software improvements
which was rarely fulfilled 
(except in full reprocessing after years of data taking).
This fundamental flaw was completely avoided in the EventIndex catalog 
described in this paper by using a far superior workflow: 
it deploys its own data collection jobs (not part of the official production chain)
and is able to collect event metadata from files at any stage of event processing 
(not just AODs). 
We also decided to avoid the mutable content in the new EventIndex
since the Run 2 analysis model has its own mechanisms for event skimming
and to expand instead  on the many use cases utilizing the immutable content
as will be described in later sections.
Since the data collection and storage were completely revamped,
it was decided to explore the (then) new BigData technologies 
that were becoming increasingly popular and promising in terms of scalability 
with respect to data volumes, and start developing the new EventIndex 
in advance of the start of LHC Run 2 \cite{MetaCHEP2013,EICHEP2013}.


This paper is organised mainly following the data flow through the EventIndex components. Section \ref{sec:2} describes the use cases and the overall system architecture; sections \ref{sec:3} and \ref{sec:4} explain the selection of datasets to index, the indexing method and the index data collection components; sections \ref{sec:5} and \ref{sec:6} describe the current data storage methods in Hadoop \cite{Hadoop}, HBase \cite{HBase} and Oracle. Sections \ref{sec:8} and \ref{sec:9} cover system operations and monitoring. Finally, section \ref{sec:7} outlines the developments towards a higher performance system for LHC Run 3 and beyond.
