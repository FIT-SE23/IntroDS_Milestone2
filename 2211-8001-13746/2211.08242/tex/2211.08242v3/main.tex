\documentclass[11pt,reqno]{amsart}

\usepackage[utf8]{inputenc}
\usepackage[margin=1.25in]{geometry}
\parindent=.25in
\usepackage{hyperref}
\usepackage{appendix}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{stmaryrd} 
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{mathrsfs}

\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{Definition}[theorem]{Definition}
\theoremstyle{remark}
\newtheorem{example}[theorem]{Example}
\newtheorem{Assumption}[theorem]{Assumption}
\newtheorem{remark}[theorem]{Remark}


\usepackage{biblatex} %Imports biblatex package
\addbibresource{sample.bib} %Import the bibliography file

\numberwithin{equation}{section}

\title[Ergodicity stochastic heat equation Hölder coefficient]{Exponential ergodicity of stochastic heat equations with Hölder coefficients}
\author{Yi HAN}
\address{Department of Pure Mathematics and Mathematical Statistics, University of Cambridge.
}
\email{yh482@cam.ac.uk}
\thanks{Supported by EPSRC grant EP/W524141/1.}


\begin{document}
\maketitle
\begin{abstract}
    We investigate the stochastic heat equation driven by space-time white noise defined on an abstract Hilbert space, assuming that the drift and diffusion coefficients are both merely Hölder continuous. Random field SPDEs are covered as special examples. We give the first proof that there exists a unique in law mild solution when the diffusion coefficient is $\beta$ - Hölder continuous for $\beta>\frac{3}{4}$ and uniformly non-degenerate, and that the drift is locally Hölder continuous. Meanwhile, assuming the existence of a suitable Lyapunov function for the SPDE, we prove that the solution converges exponentially fast to the unique invariant measure with respect to a typical Wasserstein distance. Our technique generalizes when the SPDE has a Burgers type non-linearity $(-A)^{1/2}F(X_t)$ or $\partial_x F(X_t)$, where $F$ is continuous and has linear growth. 
\end{abstract}


\section{Introduction}
Despite the much recent effort on SPDEs, little seems to be known about SPDEs
\begin{equation}\label{heatequation}
    dX_t=\Delta X_t dt+b(X_t)dt+\sigma(X_t)dW_t
\end{equation}
if the diffusion coefficient $\sigma$ is not locally Lipschitz continuous in its argument $X_t$. In this paper we outline a comprehensive study of such SPDEs in an abstract framework, covering a wide variety of models that were previously intractable. 

This problem is interesting because it illustrates the crucial difference between finite and infinite dimensional analysis. For finite dimensional SDEs, elliptic PDE techniques, or the simple trick of representing the solution as a time-changed Brownian motion, can easily solve such problems of a non-Lipschitz diffusion coefficient. In infinite dimensions, the story is quite different as none of them are available. A notable progress is made by Mytnik and Perkins \cite{mytnik2011pathwise} who study 1-d vector field stochastic heat equation with Lipschitz drift and Hölder noise coefficient. In this paper we take an abstract perspective to study Hilbert space valued SPDEs. Our model is defined as follows: 
\begin{Definition}
Consider a separable Hilbert space $H$ and a densely defined operator $A$ on $H$. Let $\mathcal{S}(t)$ denote the semigroup on $H$ generated by $A$. Consider the following evolution equation on $H$ (the conditions on $b$ and $\sigma$ will be specified explicitly in the sequel),
$$
dX_t=AX_t dt+b(t,X_t)dt+\sigma(t,X_t)dW_t,\quad X_0=x\in H. 
$$ A \textbf{weak mild} solution to this equation consists of a sequence $(\Omega,\mathcal{F},(\mathcal{F}_t),\mathbb{P},W,X)$, where $(\Omega,\mathcal{F},(\mathcal{F}_t),\mathbb{P})$ is a filtered probability space defining a cylindrical Wiener process $W$ on $H$ adapted to $\mathcal{F}_t$, and $(X_t)_{t\geq 0}$ is an $H$-valued, $\mathcal{F}_t$-adapted continuous process that satisfies, $\mathbb{P}$-a.s., the following for any $t>0$:
\begin{equation}
X_t=\mathcal{S}(t)x+\int_0^t \mathcal{S}(t-s)b(s,X_s)ds+\int_0^t \mathcal{S}(t-s)\sigma(s,X_s)dW_s.    
\end{equation}
More generally, for the following SPDE of Burgers type
$$
dX_t=AX_t dt+b(t,X_t)dt+(-A)^{1/2}F(t,X_t)dt+\sigma(t,X_t)dW_t,\quad X_0=x\in H,  
$$ a \textbf{weak mild} solution consists of a sequence $(\Omega,\mathcal{F},(\mathcal{F}_t),\mathbb{P},W,X)$, where 
\begin{equation}
X_t=\mathcal{S}(t)x+\int_0^t \mathcal{S}(t-s)\left[b(s,X_s)+(-A)^{1/2}F(s,X_s)\right]ds+\int_0^t \mathcal{S}(t-s)\sigma(s,X_s)dW_s.    
\end{equation}

\end{Definition}

\subsection{Well-posedness of stochastic heat equation with Hölder coefficients} We begin with the well-posedness results.


Throughout the paper we will use the notation $|\cdot|$ to represent a norm, and unless specified specifically, the norm is with respect to the Hilbert space $H$ on which the evolution equation is defined.


\begin{theorem}\label{theorem1.1}
Consider the evolution equation 
\begin{equation}\label{evolutionequation}
    dX_t=A X_t dt+b(t,X_t)dt+\sigma(t,X_t)dW_t,\quad X_0=x\in H
\end{equation}
on a Hilbert space $H$, where $W$ is a cylindrical Wiener process on $H$. Assume that

$\mathbf{H_1}$ The densely defined operator $A:\mathcal{D}(A)\subset H\to H$
is diagonalizable on $H$, with eigenvectors $\{e_n\}_{n\geq 1}$ forming an orthonormal basis in $H$, such that the eigenvalues $$Ae_n=-\lambda_n e_n,\quad \lambda_n>0,$$ satisfy that $\lambda_n\geq c>0$ for some constant $c>0$. Moreover assume $\lambda_n$ satisfies the following summability condition: there exists some $\eta_0\in(0,1)$ such that for each $\eta\in(0,\eta_0),$
\begin{equation}\sum_{n\geq 1}\frac{1}{{{\lambda_n}^{1-\eta}}}<\infty.\end{equation}
Further, we assume there exists a Banach space $H_0\subset H$ with continuous embedding, that $\{e_n\}_n\subset H_0$, and that there exists some $M>0$ such that 
\begin{equation}\label{new1.71.7}
    \sup_n |e_n|_{H_0}\leq M<\infty.
\end{equation}
In the following we use the notation $\mathcal{L}(H_0,H)$ to denote the space of continuous linear mappings from $H_0$ to $H$.

$\mathbf{H_2}$ The drift coefficient $b(t,x):[0,\infty)\times H\to H$ satisfies, for some $\alpha\in(0,1]$ and $M>0$,
\begin{equation}
    |b(t,x)-b(t,y)|\leq M|x-y|^\alpha,\quad |x-y|\leq 1,\quad t>0.
\end{equation}



$\mathbf{H_3}$ The diffusion coefficient $\sigma(t,x):[0,\infty)\times H\to \mathcal{L}(H_0,H)$ satisfies: for some $\beta\in(1-\frac{\eta_0}{2},1]$ and some $M>0$, we have 
\begin{equation}
|\sigma(t,x)-\sigma(t,y)|_{\mathcal{L}(H_0,H)}\leq M|x-y|^\beta,\quad |x-y|\leq 1,\quad t>0.\end{equation}
In particular, for the Laplacian operator on $[0,1]$ with Dirichlet boundary condition, one can take $\eta_0=\frac{1}{2}$, and $\sigma$ needs to be $\frac{3}{4}+\epsilon$-Hölder continuous in $x$.

 We further assume that $\sigma$ is self-adjoint: for any $t>0$ and $x\in H$,
\begin{equation}
    \langle \sigma(t,x)e_j,e_k\rangle=\langle e_j,\sigma(t,x)e_k\rangle.\quad j,k\in\mathbb{N}_+.
\end{equation}


$\mathbf{H_4}$ We can find a Banach space $\widetilde{H}_0\subset H$ with the induced norm such that $H_0\subset \widetilde{H}_0\subset H$ and there is an extension of $\sigma(t,x)$ so that $\sigma(t,x)\in \mathcal{L}(\widetilde{H}_0,H)$ such that, given any $x\in H$, $\sigma(t,x)$ has a right inverse $\sigma^{-1}(t,x)$ on $H$:
$$\sigma(t,x)\sigma^{-1}(t,x)u=u\quad \text{for any} \quad u\in H,$$
and that $\sigma^{-1}(t,x)H\subset \widetilde{H}_0$. Moreover, we assume the following boundedness condition is satisfied: there exists some $C>0$ independent of $t$ such that 
\begin{equation}
    \sup_{x\in H}|\sigma^{-1}(t,x)|_{\mathcal{L}(H,H)}<C<\infty.
\end{equation}


$\mathbf{H_5}$ The coefficients $b$ and $\sigma$ satisfy the linear growth assumption: for some $M>0$, 
\begin{equation}|b(t,x)|\leq M(1+|x|), \quad |\sigma(t,x)|_{\mathcal{L}(H_0,H)}\leq M( 1+|x|),\quad x\in H.\end{equation}

Then there exists a unique weak mild solution to \eqref{evolutionequation}. 
\end{theorem}

We briefly discuss how our abstract definition of stochastic PDEs covers the \textit{random field} SPDEs introduced in the literature. Consider the 1d stochastic heat equation defined on a bounded domain $[0,1]$ and Dirichlet boundary condition. Then we take $H=L^2([0,1];\mathbb{R})$ and $A=\Delta$ is the Laplace operator with Dirichlet boundary conditions. The eigenvalues are $-k^2\pi^2,k\in\mathbb{N}_+$ and $\mathbf{H}_1$ is satisfied with $\eta_0=\frac{1}{2}$. The eigenvectors are the sine and cosine functions, so we can take $H_0$ to be the Banach space $\mathcal{C}([0,1];\mathbb{R})$ of bounded continuous functions, which embed continuously into $H$. The assumption \ref{new1.71.7} is trivially satisfied by the sine and cosine functions. Note however that assumption $\mathbf{H}_1$ excludes stochastic heat equation in dimensions two or higher when the solution is distribution valued, and that no $\eta_0\in(0,1)$ can be found to satisfy assumption $\mathbf{H}_1$.
On the other hand, if we take $A=-\Delta^2$ where $\Delta$ is the Laplacian on a compact domain in $\mathbb{R}^d,d\leq 3$ with smooth boundary and Dirichlet boundary condition, then assumption $\mathbf{H}_1$ can still be satisfied with some choice of $\eta_0$, so that $\mathbf{H}_1$ does cover some interesting SPDEs in dimensions two and higher.

Then we discuss how assumptions $\mathbf{H}_2$ to $\mathbf{H}_5$ are satisfied by random field solutions. Consider a random field SPDE
$$
\frac{\partial u}{\partial t}(t,x)=\Delta u(t,x)+B(t,x,u(t,x))+g(t,x,u(t,x))dW(t,x),\quad u(0,x)=u_0(x).
$$
Assuming that $B$ is $\alpha$-Hölder continuous in its third argument uniform in $t$, $x$, then defining for every $x\in H$ and $z\in[0,1]$, $b(t,x)(z):=B(t,z,x(z))$, we can check that $b(t,x)$ satisfies $\mathbf{H}_2$ with the same $\alpha$. Assuming that $g$ is $\beta$- Hölder continuous in its third argument uniformly in $t$ and $x$, then 
defining for every $x\in H$, $u\in H$ and $z\in[0,1]$, $\sigma(t,x)u(z)=g(t,z,x(z))u(z)$,  we can check that $\mathbf{H}_3$ is satisfied with the same $\beta>0$, and the self-adjointness condition  follows readily from our definition. Then we verify assumption $\mathbf{H}_4$: if we assume that $|g(t,x,u)|>\epsilon$ for some $\epsilon>0$ and all $t,x,u$, then we define $\sigma^{-1}:[0,\infty)\times H\to \mathcal{L}(H,H)$ through 
$$
\sigma^{-1}(t,x)u(z)=\frac{1}{g(t,z,x(z))}u(z)
$$for any $x,u\in H$. Then one readily verifies that after a slight extension of $\sigma$  to be an element of $\mathcal{L}(\widetilde{H}_0,H)$ for some $H_0\subset\widetilde{H}_0$, then we have $\sigma\sigma^{-1}u=u$ for any $u\in H$. Our assumption on $g$ guarantees that $\sigma^{-1}\in\mathcal{L}(H,H)$. This justifies $\mathbf{H}_4$. To justify $\mathbf{H}_5$, assuming that $B$ and $g$ have linear growth in its third argument uniformly in $t$ and $x$, then the claim for $b$ is immediate from definition of $L^2$ norm. The claim for $\sigma$ can be shown as follows: via this specific choice of $H_0$ and $H$, the $\mathcal{L}(H_0,H)$ norm of $\sigma$ can be identified with the $L^2([0,1];\mathbb{R})$-norm of $g(t,z,u(z))$ for $z\in[0,1]$. Observe that the assumptions $\mathbf{H}_3,\mathbf{H}_4$ do not require $g$ to be bounded: $g$ may have a linear growth in its third argument. Meanwhile, other SPDEs that are not in this random field form can be considered as well. For example, one may assume that $g$ depends only on the $L^2$ norm of $u$. The well-posedness of these SPDEs are new in the literature  if we only assume Hölder dependence.

The main novelty of this paper is the Hölder regularity assumption on the noise coefficient, i.e. treatment of assumptions $\mathbf{H}_3$ and $\mathbf{H}_4$ for Hilbert space valued SPDEs. Note that such results are well-known for finite dimensional SDEs via parabolic PDE techniques or the simple trick of representing the solution as a time-changed Brownian motion, but in infinite-dimensional setting none of these techniques are applicable. For earlier results see \cite{zambotti2000analytic} and \cite{athreya2006infinite}. If one is interested in 1-d vector field SPDEs, then the result of Mytnik and Perkins \cite{mytnik2011pathwise} shows that whenever the diffusion coefficient is $\frac{3}{4}+\epsilon$-Hölder continuous and the drift is Lipschitz, then there is a unique strong solution. Our main result has the same $\frac{3}{4}$ threshold as \cite{mytnik2011pathwise}, yet there are a number of differences: on the one hand we need the diffusion coefficient $\sigma$ to be non-degenerate where \cite{mytnik2011pathwise} does not, and we only construct probabilistic weak solutions rather than strong, but on the other hand our construction is valid for vector field SPDEs in any dimension without supposing the coefficient is in a separable form, and far more general SPDEs can be covered by our assumptions beyond vector field case, yet \cite{mytnik2011pathwise} is essentially one-dimensional. We believe that the two solution theories are complementary to each other and has their own strength. In particular, we will derive properties of long term behaviors of solutions and also solve SPDEs with a Burgers type non-linearity as well as non-Lipschitz diffusion coefficient.

After the first version of this paper was posted online, the author further generalized the technique of this paper to second order systems: the stochastic wave equation with non-degenerate Hölder diffusion coefficient \cite{HAN2024110224}. The author investigated the well-posedness issue, and proved the convergence of a damped stochastic wave equation to the stochastic heat equation introduced in this paper as the damping parameter tends to $0$.


\subsection{Well-posedness with Burgers type non-linearity}

Now we show that we can go further to the case where we have a drift of Burgers type $(-A)^{1/2}F(X_t)$.

\begin{theorem}\label{burgerstheorem1}
    Consider the evolution equation
\begin{equation}\label{burgersequation}
dX_t=AX_t dt+b(t,X_t)dt+(-A)^{1/2}F(t,X_t)dt+\sigma(t,X_t)dW_t,\quad X_0=x\in H\end{equation}
    on a Hilbert space $H$, where $W$ is a space-time white noise on $H$. 
    
    Assume that the operator $A$ satisfies $\mathbf{H}_1$, the drift $b$ satisfies $\mathbf{H}_2$, the diffusion coefficient $\sigma$ satisfies $\mathbf{H}_3$, $\mathbf{H}_4$ and $\mathbf{H}_5$. Moreover, assume that

    $\mathbf{H}_6$. The map $F(t,x):[0,\infty)\times H\to H$ satisfies the following condition: for some $\zeta\in(\frac{1}{2},1]$ and $M>0$, 
    \begin{equation}|F(t,x)-F(t,y)|\leq M|x-y|^\zeta,\quad |x-y|\leq 1,\quad t>0,\end{equation} and that \begin{equation}|F(t,x)|\leq M(1+|x|),\quad x\in H,\quad t>0.\end{equation}

    Then there exists a unique weak mild solution to \eqref{burgersequation}. 
\end{theorem}

\begin{remark}
    In the additive noise case $\sigma=I_d$ and $b=0$, existence of a weak mild solution was proven in \cite{priola2021optimal} for all $\zeta\in(0,1]$ via an optimal regularity result for the Kolmogorov equation. However that argunent does not carry over to multiplicative noise. Our method can deal with diffusion coefficients that are merely Hölder continuous, and we can study ergodicity of \eqref{burgersequation}, at the loss of restricting ourselves to the more regular regime $\zeta\in(\frac{1}{2},1].$

    In equation \ref{burgersequation} we may as well consider a drift of the form $(-A)^\omega F(X_t)$ for any $\omega\in(0,1)$. The proof is completely analogous, yet the assumption $\mathbf{H}_6$ needs to be modified depending on the value of $\omega$.
\end{remark}

Typical examples of SPDEs that fall within \eqref{burgersequation} are Burgers type equations \footnote{Let $A$ be the Laplacian on $(0,2\pi).$ Setting $F=\frac{\partial}{\partial\xi}(-A)^{-1/2}h$ in \eqref{burgersequation} transforms \eqref{burgersequation} to this equation, and a direct computation shows that $\frac{\partial}{\partial\xi}(-A)^{-1/2}$ is a bounded operator on $L^2((0,2\pi))$. See \cite{prato2003new}, Remark 4.3 and Example 4.4 for related computations.}
\begin{equation}\label{exbur}du(t,\xi)=\frac{\partial^2}{\partial\xi^2}u(t,\xi)dt+\frac{\partial}{\partial \xi} h(u(t,\xi))dt+\sigma(u(t,\xi))dW_t(\xi),u(0,\xi)=u_0(\xi),\quad \xi\in(0,2\pi),\end{equation}
and the Cahn-Hilliard equations with stochastic forcing
\begin{equation}\label{exlug}du(t,\xi)=-\Delta_\xi^2 u(t,\xi)dt+\Delta_\xi h(u(t,\xi))dt+\sigma(u(t,\xi))dW_t(\xi),u(0,\xi)=u_0(\xi) \text{ on } G,\end{equation}
given some open, regular bounded set $G\subset\mathbb{R}^d$, $d=1,2,3$.







\subsection{Long-time behaviour of solutions}

Now we investigate the long time behavior of the solutions we just constructed. 

Assuming that the coefficients $b$, $\sigma$ and $F$ do not depend on time. Then it is easy to check that the solutions to the SPDE \eqref{evolutionequation} and \eqref{burgersequation} define a time homogeneous Markov process on $H$ and satisfy the Feller property. This can be seen from approximating the SPDE by another SPDE with Lipschitz coefficients, and the Markov and Feller property follow from taking the approximation limit.

Assume further the existence of a Lyapunov function, we can show existence and uniqueness of the invariant measure and the existence of a spectral gap.


\begin{theorem}\label{harristheorem}
Assume the assumptions of Theorem \ref{theorem1.1} are satisfied, and coefficients $b$, $\sigma$ do not depend on time. Denote by $\mathbb{E}_x$ the law of the solution $(X_t)_{t\geq 0}$ to \eqref{evolutionequation} with $X_0=x$, and denote by $(\mathcal{P}_t)_{t\geq 0}$ the Markov semigroup generated by $X_t$ on the Hilbert space $H$.

Assume that the following Lyapunov condition holds: for some $t_0>0$, 
\begin{equation}\label{lyapunov1}\mathbb{E}_x V(X_{t_0})-V(x)\leq -cV(x)+C_V,\quad x\in H,\end{equation}
here $V:H\to[1,+\infty)$ is a continuous Lyapunov function, the constants $c\in(0,1)$ and $C_V\in\mathbb{R}$ are fixed and independent of $x$. Assume further that $V(x)\to\infty$ as $|x|\to\infty$.


Then there exists a unique invariant measure $\pi$ for the Markov process $(X_t)_{t\geq 0}$. Moreover, we have the following exponential convergence result:


For any positive $\gamma\in(0,1]$ we set $$d_\gamma(x,y):=|x-y|^\gamma\wedge 1.$$  Meanwhile, for any two Borel probability measures $\mu,\nu$ on $H$, we also write $d_\gamma(\mu,\nu)$ the Wasserstein distance between the measures $\mu$, $\nu$ with respect to the distance $d_\gamma$. Then for any $\gamma\in(0,1]$ and $\epsilon>0$ there exists $\xi>0,C>0$ such that 
    \begin{equation}\label{ratefinal}
d_\gamma(\operatorname{Law}(X_t^x),\pi)\leq Ce^{-\xi t}V^{1-\epsilon}(x),\quad t>0,
    \end{equation}
where $X_t^x$ denotes the solution $X_t$ with initial data $x\in H$.
\end{theorem}


This result generalizes as well for Burgers type SPDEs. We prove the following:


\begin{theorem}\label{burgerstheorem2}
    Assume that all the assumptions of Theorem \ref{burgerstheorem1} are satisfied, and that all the coefficients $b,\sigma,F$ do not depend on time. Then the conclusion of Theorem \ref{contractionmain} is true for $(X_t^x)$, the solution to the Burgers SPDE \eqref{burgersequation}. That is, given a suitable Lyapunov function satisfying \eqref{lyapunov1}, we can deduce the exponential convergence rate \eqref{ratefinal}.
\end{theorem}




\begin{remark}\label{remark111} We give examples for the claimed Lyapunov condition \eqref{lyapunov1}.


Consider the most typical example
\begin{equation}\label{*****}dX_t=A X_t dt+b(X_t)dt+\sigma(X_t)dW_t,\end{equation} where 
$b$ and $\sigma$ are both bounded, and $A$, $b$, $\sigma$ satisfy assumptions $\mathbf{H}_1$ to $\mathbf{H}_5$. The existence of a Lyapunov functional can be verified by taking $V(x):=|x|+1,x\in H$. See Appendix \ref{appendixC} for a proof.

For Burgers type SPDEs, one may also consider
$$dX_t=A X_t dt +(-A)^{1/2}F(X_t)dt+\sigma(X_t)dW_t,$$ where $F$ and $\sigma$ are bounded, and $A$, $F$, $\sigma$ satisfy assumptions $\mathbf{H}_1$ to $\mathbf{H}_6$. Then $V(x):=|x|+1$ can again be chosen as a Lyapunov function. The proof is deferred to Appendix \ref{appendixC}.\end{remark}


\subsection{Plan of the paper}  The method of this paper is inspired by a generalized coupling technique introduced by Kulik and Scheutzow in \cite{kulik2020well}, yet working in this true infinite dimension setting and working with SPDEs require creative new ideas. In Section \ref{section2!} we prove all the well-posedness results. In Section \ref{section3!} we prove results concerning long-time behaviour of solutions. In the appendix we prove some technical lemmas.











\section{Well-posedness of weak mild solutions}
\label{section2!}
\subsection{Preliminaries} In this subsection we list a number of preparatory results that will be used in the proof.

\subsubsection{Approximation by Lipschitz functions}\label{subsection2.112}  In this paper we work abstractly with mappings on a Hilbert space that are not Lipschitz, and we will frequently need to approximate such mappings by Lipschitz mappings. The following Proposition from Alex Ravsky \cite{610367} would be very useful:


\begin{proposition}
    Consider a separable Hilbert space $V$, a normed space $V'$ and a continuous mapping $f:V\to V'$.  Then there exists a sequence $\{f_n\}$ that are bounded Lipschitz maps $f_n:V\to V'$ such that $f_n$ converges uniformly to $f$ on each compact subset of $V$.
\end{proposition}

We give a very brief sketch of the construction, and refer the details to the reference \cite{610367}. We may simply consider $V=\ell^2$. Let $p_n:\ell^2\to\mathbb{R}^n$ be an orthogonal projection. We define the mapping from $\mathbb{R}^n$ to $V'$ and compose with $p_n$. Then we discretize $\mathbb{R}^n$ into cubes along the coordinate axes, such that each cube has side length $\frac{1}{n}$. We define the approximation map to be equal to $f$ on the vertices of these cubes, and define the value of the approximation function inside the cube via linear interpolation with its values on the cube vertices. The approximating map converges to $f$ on compacts as we send $n$ to infinity.  



For any given $t>0$, we may apply this Proposition to $\sigma(t,x):H\to \mathcal{L}(H_0,H)$ where $H$ is separable Hilbert space and $\mathcal{L}(H_0,H)$ is a normed Banach space. By checking the details of construction, one can show that the approximating sequences are also measurable in $t$.  We also apply the construction to $b(t,x):H\to H.$ 


An important observation is in place: if $f$ has some nice continuity properties, then $f_n$ would also have such properties at least for two points not sufficiently close. Say if $f$ is $\beta$-Hölder continuous with Hölder coefficient $C_\beta$, then $f_n$ satisfies $|f_n(x)-f_n(y)|\leq 2C_\beta |x-y|^\beta$ for any $|x-y|>>\frac{1}{n}$. This is easy to check from the explicit construction procedure given above: since $|x-y|$ is much larger than the side length of the cube used for construction, $f_n(x)-f_n(y)$ is essentially subtracting the values of $f$ on the vertices of the cube containing $x$, and on the vertices of the cube containing $y$. When $n$ is large this is essentially $f(x)-f(y)$ thanks to the continuity of $f$. Meanwhile, when we make the choice $f=\sigma(t,x)$ and $\sigma$ satisfies the non-degeneracy condition $\mathbf{H}_4$, then $f_n$ also satisfies the non-degeneracy condition  $\mathbf{H}_4$ when $n$ is large: this is because $f_n$ is a weighted sum of $f$ on $n$ grid points. We also use the fact that if an operator $R$ on $H$ has a small operator norm, then $\operatorname{Id}_H+R$ admits a right inverse on $H$. More precisely, assume that we can find $g$ such that $fg=\operatorname{Id}_H$, then $f_ng=\operatorname{Id}_H+R$ with $|R|_{\mathcal{L}(H,H)}$ arbitrarily small. Then we find some $h$ so that $(\operatorname{Id}_H+R)h=\operatorname{Id}_H$ and $f_n(gh)=\operatorname{Id}_H$, as desired.

Thanks to these constructions, in the following we will assume that $b(t,x)$ and $\sigma(t,x)$ can be approximated by some sequences $b^n, \sigma^n$ that are Lipschitz continuous and also satisfy assumptions $\mathbf{H}_2$, $\mathbf{H}_3$, $\mathbf{H}_4$ with uniform in $n$ numerical constants for any fixed $x,y\in H.$ (That is, the Hölder constant is uniform for the approximating sequence, whereas the Lipschitz constant may explode.)


\subsubsection{Staying in a compact subset}
Given such an approximation lemma, we also need a result that shows, for any $T>0$, with probability tending to one, the solution of the stochastic heat equation lies in a compact subset $K$ throughout $[0,T]$. Such a lemma follows from the compactness of the semigroup $\mathcal{S}(t)$.
 
\begin{proposition}\label{proposition1.2} Fix $T>0$ and consider the stochastic evolution equation \eqref{evolutionequation} satisfying the assumptions of Theorem \ref{theorem1.1}.
For any $\epsilon>0$, we can find a compact subset $K\subset H$ such that 
$\mathbb{P}(\theta_K\leq T)<\epsilon,$ where $\theta_K:=\inf\{t\geq 0:X_t\notin K\}.$ The same is true for the SPDE \eqref{burgersequation} with Burgers type non-linearity.
\end{proposition}
The proof of this Proposition is deferred to Appendix \ref{appendixA} and  \ref{appendixB}. 



\subsubsection{The maximal inequality estimate}
We begin with a deviation estimate for stochastic integrals in the white noise case. This result is fundamental  for all the other proofs of this paper.
In the following we will consider an adapted process $\Phi\in L^\infty([0,T];\mathcal{L}(H_0,H))$
and its stochastic integral. For this purpose we introduce the following notation: for any $T>0$,
$$
\|\Phi\|_T:=\sup_{0\leq t\leq T}|\Phi(t)|_{\mathcal{L}(H_0,H)}.
$$



\begin{proposition}\label{stochasticinte}
Consider the stochastic integral of an adapted process $\Phi$ 
$$Z(t):=\int_0^t S(t-s)e^{-\lambda (t-s)}\Phi(s)dW_s,$$
where the semigroup $S(t)$ is generated by an operator $A$ satisfying $\mathbf{H}_1$ and $\lambda>0$ is some fixed constant. Then for each 
$p\geq 2$ and each $\eta\in(0,\eta_0),$ we have the estimate
\begin{equation}
    \mathbb{E}[|Z(t)|^p]\leq C_{\eta,p,T}\|\Phi\|_t^p \lambda^{-\frac{\eta p}{2}},\quad t\in[0,T].
\end{equation}
The constant $C_{\eta,p,T}<\infty$ depends on $\eta,p$ and $T$.

\end{proposition}



\begin{proof} 
We write $\mathcal{S}^\lambda$ the semigroup generated by $-A-\lambda I$, so that $$Z(t)=\int_0^t \mathcal{S}^\lambda(t-s)\Phi(s)dW_s.$$ By the BDG inequality,
\begin{equation}
    \mathbb{E}[|Z(t)|_H^p]\leq C\mathbb{E}\left( \sum_{j=1}^\infty\int_0^t (t-s)^{-2\alpha}|\mathcal{S}^\lambda(t-s)\Phi(s)e_j|_H^2ds\right)^{p/2}.
\end{equation}
 We estimate the right hand side via:
$$\begin{aligned}
\Lambda_\alpha^{\lambda}(t):&=\sum_{j=1}^\infty \int_0^t (t-s)^{-2\alpha}|\mathcal{S}^\lambda(t-s) \Phi(s)e_j|_H^2ds\\
&=\sum_{k=1}^\infty \sum_{j=1}^\infty \int_0^t (t-s)^{-2\alpha}\langle \mathcal{S}^\lambda(t-s)\Phi(s)e_j,e_k\rangle_H^2ds\\
&=\sum_{k=1}^\infty\sum_{j=1}^\infty\int_0^t (t-s)^{-2\alpha}\langle \Phi(s)e_j,S^{\lambda*}(t-s)e_k\rangle_H^2ds
\end{aligned}
$$

Since $$\begin{aligned}
\langle\mathcal{S}^{\lambda*}(t)^*e_k,e_j\rangle_H&=\langle \mathcal{S}^\lambda(t) e_j,e_k\rangle_H\\&=
\begin{cases}e^{-(\lambda_j+\lambda)t}\quad j=k\\0\quad j\neq k,\end{cases}
\end{aligned}$$ we further simplify $\Lambda_\alpha^{\lambda}(t)$ as

$$
\Lambda_\alpha^{\lambda}(t)=\sum_{k=1}^\infty\sum_{j= 1}^\infty\int_0^t (t-s)^{-2\alpha}e^{-2(\lambda_k+\lambda)(t-s)}\langle \Phi(s)e_j,e_k\rangle_H^2ds.
$$
By self-adjointness of $\Phi$, noting $e_k\in H_0$ with $|e_k|_{H_0}\leq M$,
$$\sum_{j=1}^\infty \langle \Phi(s)e_j,e_k\rangle_H^2=\sum_{j=1}^\infty \langle \Phi(s)e_k,e_j\rangle_H^2=|\Phi(s)e_k|_H^2\leq M|\Phi(s)|^2_{\mathcal{L}(H_0,H)}.$$

    Thus we upper $\Lambda_\alpha^{\lambda}(t)$ through
    \begin{equation}\label{Lambdamulambda}
\Lambda_\alpha^{\lambda}(t)\leq\int_0^t (t-s)^{-2\alpha}\|\Phi(s)\|_t^2\left(\sum_{k=1}^\infty e^{-2(\lambda_k+\lambda)(t-s)}
\right)ds.
    \end{equation}

In the following we find effective upper bounds for $$\sum_{k=1}^\infty e^{-2(\lambda_k+\lambda)(t-s)}$$
For any $\eta\in(0,1)$, by Young's inequality we may find $c_\eta>0$ such that \begin{equation}\label{young's}
\lambda_k+\lambda\geq c_\eta {\lambda_k}^{1-\eta}\lambda^\eta.\end{equation} 
For any $r<1$ we can find positive constants $c_r$, $c_{r,\eta}$ such that, by using \eqref{young's} in the second inequality,
\begin{equation}\label{J1J1}e^{-2(\lambda_k+\lambda)t}\leq c_r\frac{1}{((\lambda_k+\lambda)t)^r}\leq c_{r,\eta}\frac{1}{\lambda_k^{(1-\eta)r}}\lambda^{-\eta r}\frac{1}{t^r}. \end{equation}
Since \begin{equation}\label{579}\int_0^t s^{-2\alpha-r} ds<\infty\end{equation} by choosing $\alpha>0$ sufficiently small followed by choosing $r<1$ sufficiently close to 1, one can ensure that $\sum_{k=1}^\infty\frac{1}{\lambda_k^{(1-\eta)r}}<\infty$ for $r$ sufficiently close to 1, and then $r\eta$, the exponent of $\lambda^{-1},$ can achieve any value in $(0,\eta_0)$ by our choice. Thus we write $\eta$ in place of $r\eta$ for the given $\eta\in(0,1)$, and derive the estimate \begin{equation}\label{eqref580}
   \sum_{k=1}^\infty e^{-2(\lambda_k+\lambda)t}\leq c_{\eta} \lambda^{-\eta}\frac{1}{t^r}
\end{equation} where $c_\eta$ implicitly depends on $r$ but is independent of $t$.
This produces the desired $\lambda^{\eta}$ factor in the upper bound of $\Lambda_\alpha^{\lambda}$, and the integral with respect to $t$ is finite thanks to \eqref{579}.
 This finishes the proof with a constant $c_{\eta,p,T}$ that depends on $T>0$.



\end{proof}





\subsubsection{The Girsanov transform part}
Finally we provide an estimate 
that follows from Girsanov transform, which is an analogue of Proposition 3.2 of \cite{kulik2020well} in the SPDE setting. 

\begin{proposition}\label{proposition2.3}
Consider $X_t$ that solves the SPDE \eqref{evolutionequation} under the assumption of Theorem \ref{theorem1.1}, and consider another SPDE 
\begin{equation}\label{girsanov!}dY_t=AY_t dt+b(t,Y_t)dt+\lambda \phi_t 1_{t\leq\tau} dt+\sigma(t,Y_t)dW_t,\quad Y_0=x\in H,\end{equation}
where $\tau$ is a stopping time and $(\phi_t)_{t\geq 0}$ is an $H$-valued adapted process such that \eqref{girsanov!} has a unique weak mild solution. Assume that $\sup_{0\leq t\leq\tau}|\phi_t|\leq |\phi|_\infty<\infty$ almost surely for some finite constant $|\phi|_\infty.$ Then for any $T>0$ we may find a constant $C$ such that 
$$d_{TV}(\operatorname{Law}(X|_{[0,T]},Y_{[0,T]}))\leq CT^{1/2}\lambda |\phi|_\infty,$$
where $d_{TV}$ is the total variation distance on the Borel probability space on $\mathcal{C}([0,T];H)$.
\end{proposition}

\begin{proof}
By Pinsker's inequality, we have
$$d_{TV}(\operatorname{Law}(X|_{[0,T]},Y_{[0,T]}))\leq\sqrt{2 H_{\text{rel}}(\operatorname{Law}(X|_{[0,T]}\mid Y_{[0,T]}))},$$
where $H_{\text{rel}}(\cdot\mid\cdot)$ denotes the relative entropy on path space $\mathcal{C}([0,T];H).$

Since $\sigma$ is non-degenerate, the law of $X|_{[0,T]}$ can be obtained from $Y|_{[0,T]}$ via Girsanov transform (a version of Girsanov transform for cylindrical processes can be bound in \cite{da2014stochastic}, Chapter 10):
$$(W_t)_{0\leq t\leq T}\mapsto (W_t-\sigma(t,Y_t)^{-1}\lambda \phi_t 1_{t\leq \tau})_{0\leq t\leq T}.$$

Since $\sigma^{-1}\lambda\phi_t1_{t\leq\tau}$ is bounded almost surely by $C\lambda|\phi|_\infty$, where $C$ is the upper bound in Hypothesis $\mathbf{H}_4$, the claim follows from a direct computation of relative entropy.
\end{proof}




\subsection{Proof of Theorem \ref{theorem1.1}}


\begin{proof} Existence of a weak mild solution is well-known, see for example \cite{gkatarek1994weak}, Theorem 2, and a generalized version of the existence result will be proved in Appendix \ref{appendixB}. We now prove uniqueness of weak mild solution.

As discussed in Section \ref{subsection2.112}, we find a sequence of Lipschitz mappings $b^n$, $\sigma^n$ approximating $b$ and $\sigma$ on compacts, and by the discussion at the end of that subsection, we may assume $b^n$, $\sigma^n$ also satisfy assumptions $\mathbf{H}_2,\mathbf{H}_3,\mathbf{H}_4$ with the same numerical constant.

For any initial value $x\in H$, consider the following three SPDEs
$$dX_t=A X_t dt+b(t,X_t)dt+\sigma(t,X_t)dW_t,\quad X_0=x,$$ which is the SPDE we wish to prove uniqueness in law.  As we have not shown the solution is unique in law, we temporarily select one candidate solution and fix it in the sequel. Therefore, we assume the selected weak solution to this SPDE lives on some probability space $(\Omega,\mathcal{F},(\mathcal{F}_t),W).$ On the same probability space where $X_t$ lives, we solve the following SPDE
$$dX^n_t=A X^n_t dt+b^n(t,X^n_t)dt+\sigma^n(t,X^n_t)dW_t,\quad X^n_0=x,$$
which has a strong solution thanks to the Lipschitz continuity of $b^n$ and $\sigma^n$. The aim of proof is to show that the law of $X_t^n$ converges to the law of $X_t$, so that, given the law of $X_t^n$ is uniquely defined, any candidate of $X_t$ should have the same law, i.e. weak uniqueness holds.
Also, on this probability space we solve the following SPDE 
$$d\widetilde{X}^n_t=A\widetilde{X}^n_t dt+b^n(t,\widetilde{X}^n_t)dt+\lambda (X_t-\widetilde{X}_t^n)dt 1_{t\leq\tau}+\sigma^n(t,\widetilde{X}^n_t)dW_t,\quad \widetilde{X}_0^n=x,$$
where $\tau$ is a stopping time to be determined. As $X_t$ is an adapted process already determined, the last SPDE also has a strong solution on this probability space thanks to Lipschitz continuity of $b^n$ and $\sigma^n$. 

Before going into details we briefly outline the two-step proof strategy. To show $X_t^n$ converges to $X_t$ in distribution, we first establish a contracting estimate of $X_t$ towards $\widetilde{X}_t^n$ via choosing $\lambda$ sufficiently large. Then we show that $X_t^n$ is sufficiently close to $\widetilde{X}_t^n$ in total variation distance, using an appropriate definition of the stopping time $\tau$ and non-degeneracy of $\sigma^n$. The proof completes as a combination of these two bounds.




Given a compact subset $K\subset H$ with $x\in K$ obtained via Proposition \ref{proposition1.2}, denote by $$\Delta_K^n:=\sup_{t>0}\sup_{y\in K}\{|b^n(t,y)-b(t,y)|+|\sigma^n(t,y)-\sigma(t,y)|_{\mathcal{L}(H_0,H)}\}$$
and set $\tau_K^n:=\inf\{t\geq 0:|X_t-\widetilde{X}_t^n|\geq 2\Delta_K^n\}.$ Also denote by $\theta_K:=\inf\{t\geq 0: X_t\notin K\}.$ We set $$\tau=\tau_K^n\wedge\theta_K$$ and $$\lambda=({\Delta_K^n})^{\gamma -1}$$ 
for some value of $\gamma>0$ to be fixed later. Thanks to Proposition \ref{proposition1.2}, we have $\mathbb{P}(\theta_K\leq T)\leq\epsilon.$


Now we apply Proposition \ref{proposition2.3} for the process $X_t^n-\widetilde{X}_t^n$ and deduce that, 
for any $T>0$ we can find a constant $C$ (uniform in $n$) such that
\begin{equation}\label{variationineq}
d_{TV}(\operatorname{Law}(X^n|_{[0,T]}),\operatorname{Law}(\widetilde{X}^n|_{[0,T]}))\leq C T^{1/2}(\Delta_K^n)^\gamma.\end{equation}

For $0\leq t\leq \tau$, assume for simplicity that $\Delta_K^n\in[0,1)$, we may compute via triangle inequality that for some universal constant $C>0$,
$$
    |b(t,X_t)-b^n(t,\widetilde{X}_t^n)|\leq M(2\Delta_K^n)^{\alpha}+\Delta_K^n\leq C (\Delta_K^n)^\alpha,\quad t\leq \tau
$$ and 
$$|\sigma(t,X_t)-\sigma^n(t,\widetilde{X}_t^n)|_{\mathcal{L}(H_0,H)}\leq M(2\Delta_K^n)^\beta+\Delta_K^n\leq C (\Delta_K^n)^\beta,\quad t\leq\tau,$$
where $M$ is the constant appearing in Assumptions $\mathbf{H}_2$ and $\mathbf{H}_3$.


A direct computation yields
$$\begin{aligned}
|X_t-\widetilde{X}_t^n|&\leq e^{-\lambda t}\left|\int_0^t S(t-s)e^{\lambda s }[b(s,X_s)-b^n(s,\widetilde{X}_s^n)]ds\right|\\&+\left|e^{-\lambda t}\int_0^t S(t-s)e^{\lambda s }[\sigma(s,X_s)-\sigma^n(s,\widetilde{X}_s^n)]dW_s\right|.\end{aligned}.$$


 Applying Proposition \ref{stochasticinte}, note that we choose $\lambda=(\Delta_K^n)^{\gamma -1}$, we obtain that for each $\eta\in(0,\eta_0)$ and $m\in\mathbb{N}_+$, we may find a constant $C=C_{\eta,m,T}$ satisfying
\begin{equation}\label{complementary}\mathbb{P}(\sup_{t\in[0,\tau]}|X_t-\widetilde{X}_t^n|>C (\Delta_K^n)^{1+\alpha-\gamma}+C(\Delta_K^n)^{\frac{1-\gamma}{2}\eta+\beta}R)\leq \frac{C}{R^m},\quad R>0.\end{equation}


By our assumption $\alpha\in(0,1]$ and $\beta\in(1-\frac{\eta_0}{2},1]$, we can find $\eta\in(0,\eta_0)$ sufficiently close to $\eta_0$ and $\gamma\in(0,1)$ sufficiently close to $0$, such that the following holds at the same time:
\begin{equation}
    \label{condition2}
\begin{cases}
\gamma\in(0,\alpha)\\
\frac{1-\gamma}{2}\eta+\beta>1.\\
\end{cases}\end{equation}
Consequently, the powers of $\Delta_K^n$ in the bracket of \eqref{complementary} are all larger than one.
We fix from now on the choice of $\gamma$ and $\eta$, as well as the constant $C_m=C_{\eta,m}$.

Now we choose $\chi\in(0,\frac{1-\gamma}{2}\eta+\beta-1]$ and set $R:=(\Delta_K^n)^{-\chi}$ in the estimate \eqref{complementary}. Denote by $$\Omega_\nu:=\{\sup_{t\in[0,\tau]}|X_t-\widetilde{X}_t^n|>C (\Delta_K^n)^{1+\alpha-\gamma}+C(\Delta_K^n)^{\frac{1-\gamma}{2}\eta+\beta-\chi}\}.$$
Upon choosing $\Delta_K^n$ sufficiently small, we deduce that $|X_t-\widetilde{X}_t^n|\leq\Delta_K^n$ for all $t\leq\tau$ on $\Omega\setminus\Omega_\nu$. Since the processes have continuous trajectories, we deduce that we indeed have $$\theta_K\wedge T\leq \tau_K^n$$ on $\Omega\setminus\Omega_\nu$. From this we conclude that, for any $\kappa>0$,
\begin{equation}
   \label{convergeinprob} 
\mathbb{P}(\sup_{t\in[0,T\wedge \theta_K]} |X_t-\widetilde{X}_t^n|>\kappa)\to 0,\quad n\to\infty.\end{equation}


To prove uniqueness in law of the solution $X_t$, it suffices to show that, for any bounded continuous function $F$ on $\mathcal{C}([0,T];H)$, we have 
\begin{equation}\label{condition}
\mathbb{E}[F(X|_{[0,T]})]-\mathbb{E}[F(X^n|_{[0,T]})]\to 0,\quad n\to\infty,\end{equation}
Indeed, since $b^n$ and $\sigma^n$ are Lipschitz for each $n$, the SPDE solved by $X^n$ has a unique strong solution, and $\operatorname{Law}(X^n|_{[0,T]})$ is uniquely determined. If \eqref{condition} were established, then it would imply that
$$
\mathbb{E}[F(X|_{[0,T]})]$$
is uniquely defined for any continuous test function $F$ and any candidate solution $X$, so the law of $X|_{[0,T]}$ is unique. Since $T$ is arbitrary, this shows the uniqueness in law of $X$.

To prove \eqref{condition}, first note that \eqref{variationineq} implies,  in the limit $\Delta_K^n\to 0,$
$$\mathbb{E}[F(\widetilde{X}^n|_{[0,T]})]-\mathbb{E}[F(X^n|_{[0,T]})]\to 0,\quad n\to\infty.$$
Note also that \eqref{convergeinprob} implies $\widetilde{X}^n|_{[0,T]}$ converges to $X|_{[0,T]}$ in probability, on $\{\theta_K\geq T\}$. Therefore 
$$\lim\sup_{n\to\infty}\left|\mathbb{E}[F(\widetilde{X}^n|_{[0,T]})]-\mathbb{E}[F(X|_{[0,T]})]\right|\leq  2\sup_x|F(x)|\mathbb{P}(\theta_K\leq T).$$
The right hand side can be set arbitrarily small thanks to Proposition \ref{proposition1.2}. This completes the proof of \eqref{condition}, and weak uniqueness follows.

\end{proof}



\subsection{Proof of Theorem \ref{burgerstheorem1}}


In this section we prove Theorem \ref{burgerstheorem1}, where there is an additional $(-A)^{1/2}F(X_t)$ term. 


We need a simple estimate:

\begin{equation}\label{05.1} \|S(t)(-A)^{1/2}\|_{op}=\sup_{k\geq 1}e^{-\lambda_k t} \lambda_k^{1/2}\leq \frac{d}{t^{1/2}},\end{equation}
where $$d:=\sup_{r> o}e^{-r}r^{1/2}<\infty.$$
Moreover, for any $\lambda>0$ we have
\begin{equation}\label{burgersusefulestimate}
\begin{aligned}
    \int_0^t e^{-\lambda(t-s)}\|S(t-s)(-A)^{1/2}\|_{op}ds&\leq d\int_0^t e^{-\lambda s}\frac{1}{\sqrt{s}}ds
    \\&\leq \frac{1}{\sqrt{\lambda}}d\int_0^\infty e^{-t}\frac{dt}{\sqrt{t}}\leq \frac{C}{\sqrt{\lambda}}
    \end{aligned}
\end{equation}
for some universal constant $C>0$ that does not depend on $t$ and $\lambda$.


 We now begin to prove the well-posedness result, Theorem \ref{burgerstheorem1}.

\begin{proof}
For existence of a weak mild solution, we generalize the existence part of Theorem 1 of \cite{priola2022correction} to cover multiplicative noise. Proof of weak existence is given in Appendix \ref{appendixB}.



We now focus on showing the weak solution is unique in law. We follow closely the proof of Theorem \ref{theorem1.1}. Consider approximations $b^n$, $\sigma^n$ and $F^n$ that are Lipschitz continuous and satisfy $\mathbf{H}_2$, $\mathbf{H}_3$, $\mathbf{H}_5$ and $\mathbf{H}_6$ with constants that are uniform in $n$, and such that $b^n,\sigma^n$ and $F^n$ converges uniformly to $b,\sigma$ and $F$ on any compact subset of $H$. We select a weak solution to the following SPDE that lives on some probability space $(\Omega,\mathbb{P})$,
$$dX_t=A X_t dt+b(t,X_t)dt+(-A)^{1/2}F(t,X_t)dt+\sigma(t,X_t)dW_t,\quad X_0=x,$$ and on the same probability space we solve the following two SPDEs thanks to Lipschitz continuity of various coefficients, given the adapted process $X_t$:
$$dX^n_t=A X^n_t dt+b^n(t,X^n_t)dt+(-A)^{1/2}F^n(t,X^n_t)dt+\sigma^n(t,X^n_t)dW_t,\quad X^n_0=x,$$
 and
$$\begin{aligned}
d\widetilde{X}^n_t&=A \widetilde{X}^n_t dt+b^n(t,\widetilde{X}^n_t)dt\\&+(-A)^{1/2}F^n(t,\widetilde{X}^n_t)dt+\lambda (X_t-\widetilde{X}_t^n)dt 1_{t\leq\tau}+\sigma^n(t,\widetilde{X}^n_t)dW_t,\quad \widetilde{X}_0^n=x,\end{aligned}$$
where $\tau$ is a stopping time to be determined.


Given a compact subset $K\subset H$ with $x\in K$ constructed via Proposition \ref{proposition1.2}, denote by $$\Delta_K^n:=\sup_{t>0}\sup_{y\in K}\{|b^n(t,y)-b(t,y)|+|\sigma^n(t,y)-\sigma(t,y)|_{\mathcal{L}(H_0,H)}+|F^n(t,y)-F(t,y)|\},$$
and set $\tau_K^n:=\inf\{t\geq 0:|X_t-\widetilde{X}_t^n|\geq 2\Delta_K^n\}.$ Also denote by $\theta_K:=\inf\{t\geq 0: X_t\notin K\}.$ We set $$\tau=\tau_K^n\wedge\theta_K$$ and $$\lambda=({\Delta_K^n})^{\gamma -1}$$ 
for some value of $\gamma>0$ to be fixed later.

Arguing as in \eqref{variationineq}, we deduce that for any $T>0$ we can find a constant $C$ (uniform in $n$) such that
\begin{equation}\label{burgersvariationineq}
d_{TV}(\operatorname{Law}(X^n|_{[0,T]}),\operatorname{Law}(\widetilde{X}^n|_{[0,T]}))\leq C T^{1/2}(\Delta_K^n)^\gamma.\end{equation}

For $0\leq t\leq \tau$, assume for simplicity that $\Delta_K^n\in[0,1)$, we may compute via triangle inequality that for some universal constant $C>0$,
$$
    |b(t,X_t)-b^n(t,\widetilde{X}_t^n)|\leq M(2\Delta_K^n)^{\alpha}+\Delta_K^n\leq C (\Delta_K^n)^\alpha,\quad t\leq \tau
$$ 
$$|\sigma(t,X_t)-\sigma^n(t,\widetilde{X}_t^n)|_{\mathcal{L}(H_0,H)}\leq M(2\Delta_K^n)^\beta+\Delta_K^n\leq C (\Delta_K^n)^\beta,\quad t\leq\tau,$$
and 
 $$|F(t,X_t)-F^n(t,\widetilde{X}_t^n)|\leq M(2\Delta_K^n)^{\zeta}+\Delta_K^n\leq C (\Delta_K^n)^\zeta,\quad t\leq \tau,$$
where $M$ is the constant appearing in Assumptions $\mathbf{H}_2$, $\mathbf{H}_3$ and $\mathbf{H}_6$.


A direct computation yields
$$\begin{aligned}
|X_t-\widetilde{X}_t^n|&\leq e^{-\lambda t}\left|\int_0^t S(t-s)e^{\lambda s }[b(s,X_s)-b^n(s,\widetilde{X}_s^n)]ds\right|\\&+\left|e^{-\lambda t}\int_0^t S(t-s)e^{\lambda s }[\sigma(s,X_s)-\sigma^n(s,\widetilde{X}_s^n)]dW_s\right|\\&+\left|\int_0^t e^{-\lambda(t-s)}S(t-s)(-A)^{1/2}[F(s,X_s)-F^n(s,\widetilde{X}_s^n)]ds\right|
.\end{aligned}$$

Now arguing as in \eqref{complementary}, using the auxiliary estimate \eqref{burgersusefulestimate}, note that we choose $\lambda:=(\Delta_K^n)^{\gamma -1}$, we deduce that 
 for each $\eta\in(0,\eta_0)$ and $m\in\mathbb{N}_+$, we may find a constant $C=C_{\eta,m}$ satisfying
\begin{equation}\label{burgerscomplementary}\mathbb{P}(\sup_{t\in[0,\tau]}|X_t-\widetilde{X}_t^n|>C (\Delta_K^n)^{1+\alpha-\gamma}+C (\Delta_K^n)^{\frac{1-\gamma}{2}+\zeta}+C(\Delta_K^n)^{\frac{1-\gamma}{2}\eta+\beta}R)\leq \frac{C}{R^m},\quad R\geq 2.\end{equation}

By our assumption $\alpha\in(0,1]$, $\beta\in(1-\frac{\eta_0}{2},1]$ and $\zeta\in(\frac{1}{2},1]$, we can find $\eta\in(0,\eta_0)$ sufficiently close to $\eta_0$ and $\gamma\in(0,1)$ sufficiently close to $0$, such that the following holds at the same time:
\begin{equation}
    \label{burgerscondition2}
\begin{cases}
\gamma\in(0,\alpha),\\
\frac{1-\gamma}{2}+\zeta>1,\\
\frac{1-\gamma}{2}\eta+\beta>1.\\
\end{cases}\end{equation}
Consequently, the powers of $\Delta_K^n$ in the bracket of \eqref{burgerscomplementary} are all larger than one.

Arguing as in \eqref{convergeinprob}, we deduce that for any $\kappa>0$,
\begin{equation}
   \label{burgersconvergeinprob} 
\mathbb{P}(\sup_{t\in[0,T\wedge \theta_K]} |X_t-\widetilde{X}_t^n|>\kappa)\to 0,\quad n\to\infty.\end{equation}


The remaining proof of weak uniqueness follows from the same argument as those in Theorem \ref{theorem1.1}. \end{proof}



\section{Long time behaviour}\label{section3!}


\subsection{Choice of a metric and contracting properties}

In this subsection we give a proof to the following theorem that does not rely on the assumed Lyapunov condition.


We consider the distance function $$d_{N,\gamma}(x,y):=(N|x-y|^\gamma)\wedge 1,\quad N\geq 1,\gamma\in[0,1].$$
 The constant $\gamma>0$ and $N>0$ will be chosen in a problem specific way, and is different in every other applications.

Before stating the theorem, we warn the reader that the distance $d_{N,\gamma}$ is not globally comparable to the usual distance on $H$, and in the next result we will choose $N$ and $\gamma$ such that the dynamics is contracting at a \textit{specific} time $t$ with respect to $d_{N,\gamma}$: this result alone does not say anything about the long time behavior of solutions. Indeed it is easy to construct solutions moving away from a fixed region in long time, yet for a specific time $t$ and specific choice of $N,\gamma$ the following \textit{contraction} claim still holds.

\begin{theorem}\label{contractionmain}
 Assume that all the assumptions of Theorem \ref{theorem1.1} are satisfied, and that $b$ and $\sigma$ do not depend on time. For each $x\in H$ denote by $(X_t^x)_{t\geq 0}$ the solution to \eqref{evolutionequation} with initial value $x$. Then for any $D>0$ and fixed $t>0$ we may find some $\gamma\in(0,1)$, some positive integer $N$, and some $\theta\in(0,1)$ depending on $t$ such that
\begin{equation}\label{22111}d_{N,\gamma}(\operatorname{Law}(X_t^x),\operatorname{Law}(X_t^y))\leq \theta  d_{N,\gamma}(x,y),\quad \text{ given }|x|\leq D,|y|\leq D.\end{equation}
and 
\begin{equation}\label{22112}d_{N,\gamma}(\operatorname{Law}(X_t^x),\operatorname{Law}(X_t^y))\leq \theta  d_{N,\gamma}(x,y),\quad  \text{ given } d_{N,\gamma}(x,y)<1.\end{equation} (warning again: these estimates hold only at time $t$, and may not hold on any longer time period).


\end{theorem}
The rest of the subsection is devoted to the proof of Theorem \ref{contractionmain}.





\subsubsection{Setting up the generalized coupling}

Fix two initial values $x,y\in H$. To couple the two processes
\begin{equation}\label{eqx}dX_t=AX_t dt+b(X_t)dt+\sigma(X_t)dW_t,\quad X_0=x\end{equation}
and 
\begin{equation}\label{eqy}dY_t=AY_t dt+b(Y_t)dt+\sigma(Y_t)dW_t,\quad Y_0=y,\end{equation}
we introduce a constant $\lambda>0$ and a stopping time $\tau$ whose value will be determined later, and we consider an auxiliary control process
\begin{equation}\label{controlprocess}
d\widetilde{Y}_t=A\widetilde{Y}_t dt+b(\widetilde{Y}_t)dt+\lambda(X_t-\widetilde{Y}_t)dt1_{t\leq\tau}+\sigma(\widetilde{Y}_t)dW_t,\quad \widetilde{Y}_0=y.\end{equation}
Strictly speaking, we also need to approximate $b$ and $\sigma$ by Lipschitz mappings as detailed in Section \ref{subsection2.112}, but we omit this auxiliary approximation procedure as it is exactly the same in the previous proofs of weak uniqueness.

We set $$\tau=\tau_{x,y}:=\inf\{t\geq 0:|X_t-\widetilde{Y}_t|\geq 2|x-y|\},$$
and set $\lambda:=|x-y|^{\gamma -1}$ for some $\gamma\in(0,1)$ to be determined later. 

It follows from Proposition \ref{proposition2.3} that for any $T>0$ we can find $C$ such that 
\begin{equation}
    \label{controlgirsanov}
d_{TV}(\operatorname{Law}(Y|_{[0,T]}),\operatorname{Law}(\widetilde{Y}|_{[0,T]}))\leq CT^{1/2} |x-y|^\gamma.\end{equation}


Writing \eqref{eqx} and \eqref{eqy} in mild formulations, we have
\begin{equation}\label{difference}
\begin{aligned}
|X_t-\widetilde{Y}_t|\leq e^{-\lambda t}|x-y|&+e^{-\lambda t}\left|\int_0^t S(t-s)e^{\lambda s}(b(X_s)-b(\widetilde{Y}_s))ds\right|\\&+e^{-\lambda t}\left|\int_0^t S(t-s)e^{\lambda s}(\sigma(X_s)-\sigma(\widetilde{Y}_s))dW_s\right|.
\end{aligned}\end{equation}
Using the Hölder continuity $\mathbf{H}_2$ and $\mathbf{H}_3$ of $b$ and $\sigma$, definition of the stopping time $\tau$ and Proposition \ref{stochasticinte}, we obtain the following deviation estimate: for each $m\in\mathbb{N}_+$ and $\eta\in(0,\frac{1}{2})$, we may find some fixed constant $M=M_{m,\eta}>0$ such that 
\begin{equation}\label{fififi}
    \mathbb{P}(\sup_{0\leq t\leq \tau\wedge T}(|X_t-\widetilde{Y}_t|- e^{-\lambda t}|x-y|)\geq  M\lambda^{-1}|x-y|^{\alpha}+M\lambda^{-\frac{1}{2}\eta}|x-y|^{\beta} R)\leq \frac{M}{R^m},\quad R>0.
\end{equation}


Recall we made the choice $\lambda=|x-y|^{\gamma -1}$. We may find  $\gamma>0$ sufficiently small and $\eta\in(0,\eta_0)$ sufficiently close to $\eta_0$ such that
$$
\begin{cases}
\gamma\in(0,\alpha)\\
\frac{1-\gamma}{2}\eta+\beta>1.\\
\end{cases}$$ holds, as it did in
\eqref{condition2}.

Then we find some  $0<\chi<\min(\alpha-\gamma,\beta+\frac{1-\gamma}{2}\eta-1)$ and choose furthermore $$R=|x-y|^{-\chi}$$  to obtain
\begin{equation}
    \mathbb{P}(\sup_{0\leq t\leq\tau\wedge T} (|X_t-\widetilde{Y_t}|-e^{-|x-y|^{\gamma -1}t}|x-y|)\geq M_m|x-y|^{1+\chi})\leq M_m |x-y|^{2m\chi}.
\end{equation}


Denote by $$\Omega_\nu:=\{\sup_{0\leq t\leq \tau\wedge T}\{|X_t-\widetilde{Y}_t|-e^{-|x-y|^{\gamma -1}t}|x-y|\}\geq M_n|x-y|^{1+\chi}\},$$ then by choosing $|x-y|$ small enough, on the complementary set $\Omega\setminus\Omega_\nu$ one must have $\sup_{0\leq t\leq\tau_\wedge T} |X_t-Y_t|\leq 2|x-y|$. Thus by continuity of the trajectories we must have \begin{equation}\label{stoppingtimestimate}\tau=\tau_{x,y}\geq T\end{equation}on $\Omega\setminus\Omega_\nu$, so we can deduce that
\begin{equation}\label{1.111}
    \mathbb{P}(\sup_{0\leq t\leq T} (|X_t-\widetilde{Y_t}|-e^{-|x-y|^{\gamma -1}t}|x-y|)\geq M_m|x-y|^{1+\chi})\leq M_m |x-y|^{2m\chi}.
\end{equation}

Now we make a choice of $m\in\mathbb{N}_+$ such that \begin{equation}\label{limitofm}m\chi \geq \gamma,\end{equation}
and fix the choice of the constant $M:=M_m$ in the rest of the argument.

Since $$e^{-\nu^{\gamma -1}T}+M\nu^\chi\to 0\quad \text{ as   }\nu\to 0,$$ 
we will choose some sufficiently small  $\nu_0>0$ and assume that $|x-y|\leq \nu_0$ to obtain, as a consequence of \eqref{1.111}, \eqref{stoppingtimestimate} and \eqref{limitofm}: 
\begin{equation} \label{deviationestimate11}
    \mathbb{P}(|X_T-\widetilde{Y}_T|\geq \frac{1}{2}|x-y|)\leq M |x-y|^{2\gamma}.
\end{equation}

Since probability is upper bounded by 1, we restate the above estimate as
\begin{equation} \label{deviationestimate}
    \mathbb{P}(|X_T-\widetilde{Y}_T|\geq \frac{1}{2}|x-y|)\leq M |x-y|^{2\gamma}\wedge 1.
\end{equation}

We can also derive the following estimate which will be useful later:
\begin{equation} \label{longtime}
    \mathbb{P}(\sup_{0\leq t\leq T}|X_t-\widetilde{Y}_t|\geq 2|x-y|)\leq M |x-y|^{2\gamma}\wedge 1.
\end{equation}

\subsubsection{The case of close enough initial values}\label{Section3.2}
In this section we consider initial values $x,y\in H$ that are close in the sense that $$d_{N,\gamma}(x,y)<1.$$
We combine \eqref{deviationestimate} with the control \eqref{controlgirsanov}to deduce a contraction estimate for $|X_t-Y_t|$ under the distance $d_{N,r}.$ The proof is very similar to Proposition 5.1 of \cite{kulik2020well}. 


Recall that we have derived \eqref{deviationestimate} assuming $x$ and $y$ are close enough: for some $\nu_0>0$ sufficiently small, $|x-y|\leq\nu_0$. Thus we choose $N$ large enough \footnote{once we have fixed a value of $\gamma>0$} such that $d_{N,\gamma}(x,y)=1$ for any pair $(x,y)$ with $|x-y|\geq\nu_0.$ We will work under this assumption throughout the proof without further mentioning.

For any two initial values $x,y\in H$, consider the SPDEs $X_t$ \eqref{eqx}, $Y_t$ \eqref{eqy}, as well as the control process $\widetilde{Y}_t$ \eqref{controlprocess}.


Consider a coupling $(\xi_1,\xi_2)$ where $\xi_1$ has law $X_t$, $\xi_2$ has law $\widetilde{Y}_t$ and that $(X_t,\widetilde{Y}_t)$ satisfy the contraction estimate \eqref{deviationestimate} with $t=T$. 


Consider another coupling $(\xi_2,\xi_3)$ where $\xi_2$ has law $\widetilde{Y}_t$ and $\xi_3$ has law $Y_t$, and such that by \eqref{controlgirsanov}, $$\mathbb{P}(\xi_2\neq \xi_3)\leq d_{TV}(\operatorname{Law}(Y|_{[0,T]});\operatorname{Law}(\widetilde{Y}|_{[0,T]}))\leq CT^{1/2}|x-y|^\gamma.$$
We may now form a coupling $(\xi_1,\xi_3)$ of $(X_t,Y_t),$ possibly on a different probability space, via the coupling $(\xi_1,\xi_2)$ and $(\xi_2,\xi_3)$ and obtain the following estimate:
$$\begin{aligned}
\mathbb{E}d_{N,\gamma}(\xi_1,\xi_3)&\leq \mathbb{E} d_{N,\gamma}(\xi_1,\xi_2)+\mathbb{P}(\xi_2\neq\xi_3)\\&\leq \mathbb{E}d_{N,\gamma}(\xi_1,\xi_2)1_{\{|\xi_1-\xi_2|\leq\frac{1}{2} |x-y|\}}+\mathbb{P}(|\xi_1-\xi_2|\geq \frac{1}{2}|x-y|)+\mathbb{P}(\xi_2\neq\xi_3).\end{aligned}
$$

Since we assume that $$d_{N,\gamma}(x,y)=N|x-y|^\gamma<1,$$ and therefore $N2^{-\gamma} |x-y|^\gamma<1,$ we combine the various estimates to deduce that 
\begin{equation}\label{goodgifts}\begin{aligned}\mathbb{E}d_{N,\gamma}(\xi_1,\xi_3)&\leq N2^{-\gamma} |x-y|^\gamma +M|x-y|^{2\gamma}+C|x-y|^\gamma\\ \leq& d_{N,\gamma}(x,y)\left(2^{-\gamma}+\frac{M|x-y|^{2\gamma}\wedge 1}{N|x-y|^\gamma}+\frac{C}{N}\right).\end{aligned}\end{equation}
Note that $\sup_{s\geq 0}\frac{M s^{2\gamma}\wedge 1}{s^\gamma}<\infty.$
Therefore, upon choosing $N$ large enough, we may find some $\theta_1\in(2^{-\gamma},1)$ such that, for some given $N_0$ depending on $\gamma$ and $\theta_1$, we have
\begin{equation}\mathbb{E}d_{N,\gamma}(\xi_1,\xi_3)\leq \theta_1 d_{N,\gamma}(x,y),\quad \text{ for all } N\geq N_0,\quad \text{all } d_{N,\gamma}(x,y)<1.\end{equation}

By definition of the Wasserstein distance,  $$
d_{N,\gamma}\left(\operatorname{Law}(X_t),\operatorname{Law}(Y_t)\right)\leq \mathbb{E}d_{N,\gamma}(\xi_1,\xi_2),$$
so we conclude that \begin{equation}d_{N,\gamma}\left(\operatorname{Law}(X_t),\operatorname{Law}(Y_t)\right)
\leq \theta_1 d_{N,\gamma}(x,y),\quad N\geq N_0,\quad d_{N,\gamma}(x,y)<1.\end{equation}
Note that the choice of $N$ and $\gamma$ depend on the final time $t>0$. 

To unify notations in this paper, we rewrite this estimate as
\begin{equation}\label{localcontraction}d_{N,\gamma}\left(\operatorname{Law}(X_t^x),\operatorname{Law}(X_t^y)\right)
\leq \theta_1 d_{N,\gamma}(x,y),\quad N\geq N_0,\quad d_{N,\gamma}(x,y)<1.\end{equation}


One could also derive in the same way from \eqref{longtime} the following estimate
\begin{equation}\label{longtimespider}
    \sup_{0\leq s\leq t} d_{N,\gamma}\left(\operatorname{Law}(X_s^x),\operatorname{Law}(X_s^y)\right)
\leq 3 d_{N,\gamma}(x,y),\quad N\geq N_0,\quad d_{N,\gamma}(x,y)<1.
\end{equation}

\subsubsection{The case of distant initial values}

We finally derive the estimate for initial values $x,y$ such that $d_{N,\gamma}(x,y)=1,$ but that for some fixed $R>0$, $|x|\leq R$ and $|y|\leq R$.


We begin with a lemma that describes the possibility of visiting bounded subsets of $H$, which is similar to Proposition 5.2 of \cite{kulik2020well} and to the proof of various estimates here. 

\begin{lemma}\label{lemma3.1}
For each $x\in H$ and $\lambda>0$, denote by $X_t^{\lambda,x}$ the solution to the SPDE 
$$dX_t=A X_t dt-\lambda X_t dt+b(X_t)dt+\sigma(X_t)dW_t,\quad X_0=x,$$ where $b$ and $\sigma$ satisfy the assumptions $\mathbf{H}_1$ to $\mathbf{H}_5$.


Then for any $D>0$ and $\delta>0$, there exists $\lambda$ sufficiently large (depending only on $D$, $\delta$, $t$ and the constants in assumption $\mathbf{H}_1$ to $\mathbf{H}_5$) such that  
$$\inf_{|x|\leq D} \mathbb{P}(|X_t^{\lambda,x}|\leq\delta)\geq\frac{1}{2}.$$
\end{lemma}

\begin{proof}
Consider the stopping time $\tau_D:=\inf\{t\geq 0: |X_t^{\lambda,x}|\geq 2D\}.$ By the linear growth property $\mathbf{H}_5$, we may find some $M>0$ such that $$|b(X_t^{\lambda,x})|\leq M(1+D),\quad |\sigma(X_t^{\lambda,x})|_{\mathcal{L}(H_0,H)}\leq M(1+D)$$ whenever $t\leq\tau_D$. Arguing with the mild formulation and estimating each term explicitly, using also Proposition \ref{stochasticinte} with $\eta=\frac{\eta_0}{2}$, we claim that
$$\mathbb{P}\left(\sup_{s\in[0,t\wedge\tau_D]} \{|X_s^{\lambda,x}|- e^{-\lambda s}|x|\}\geq \lambda^{-1}M(1+D)+\lambda^{-\frac{\eta_0}{4}}M(1+D)R\right)\leq \frac{C}{R^2},\quad R\geq 2.$$
Choosing $R=\lambda^{\frac{\eta_0}{8}}$ for $\lambda$ large, we see that 
\begin{equation}\label{event}\mathbb{P}\left(\sup_{s\in[0,t\wedge\tau_D]} \{|X_s^{\lambda,x}|- e^{-\lambda s}|x|\}\geq \lambda^{-1}M(1+D)+ \lambda^{-\frac{\eta_0}{8}}M(1+D)\right)\leq C \lambda^{-\frac{\eta_0}{4}}.\end{equation}
Arguing as in the previous proofs, we see that when $\lambda$ is sufficiently large, in the complement of the event in \eqref{event} one must have $\tau_D\geq t$. Now if we choose $\lambda$ sufficiently large, we can ensure that 
$\mathbb{P}(|X_t^{\lambda,x}|\leq \delta)\geq \frac{1}{2},$ uniformly over the initial value $|x|\leq D$. This completes the proof.
\end{proof}

In the following we fix this choice of $\lambda>0$. Via direct moment estimates, we may find 
$$C_1(\lambda):=\sup_{|x|\leq D}\mathbb{E}[\sup_{s\in[0,t]}|X^{\lambda,x}_s|^2]<\infty.$$ By arguing via Girsanov transform, we compute the relative entropy between solutions $X^x$ and $X^{\lambda,x}$ as follows:
\begin{equation}
    \label{relativedifference}\sup_{|x|\leq D} H_{\text{rel}}(\operatorname{Law}(X^{\lambda,x}|_{[0,t]})\mid\operatorname{Law}(X^{x}|_{[0,t]}))\leq \frac{\lambda t}{2}C_1(\lambda)\sup_{x\in H}|\sigma(x)^{-1}|:=C_2<\infty,\end{equation} where $H_{\text{rel}}$ denotes the relative entropy on path space $C([0,T];H).$

 We will use a powerful formula on relative entropy (see \cite{kulik2020well}, Appendix A or \cite{butkovsky2020generalized}, Appendix A): for two measures $\mu$ and $\nu$ on a common measure space $(X,\chi)$, and for any set $A\in\chi$, we have for each $N\in\mathbb{N}_+$
$$\nu(A)\geq\frac{1}{N}\mu(A)-\frac{H_{\text{rel}}(\mu\mid\nu)+\log 2}{N\log N}.$$


In our case we consider $A:=\{\omega\in \mathcal{C}([0,t];H): |\omega_t|\leq\delta\},$ $\nu=\operatorname{Law}(X^x|_{[0,t]})$ and $\mu=\operatorname{Law}(X^{\lambda,x}|_{[0,t]}).$ Combining Lemma \ref{lemma3.1} and estimate \eqref{relativedifference}, we deduce that 
$$\inf_{|x|\leq D}\mathbb{P}(|X_t^x|\leq\delta)\geq\frac{1}{2N}-\frac{C_2+\log 2}{N\log N}.$$ 
Now we choose $N=\exp(4C_2+4\log 2)$ and deduce that 
\begin{equation}\label{finalconclusion}\inf_{|x|\leq D}\mathbb{P}(|X_t^x|\leq\delta)\geq\frac{1}{L(D,\delta)},\end{equation} where $L(D,\delta):=4\exp(4C_2+4\log 2)$ is a constant that only depends on $D$, $\delta$, $t$ and the constants in hypothesis $\mathbf{H}_1-\mathbf{H}_5.$

\subsubsection{Concluding the proof of Theorem \ref{contractionmain}}
\begin{proof}
Consider two initial values $x,y\in H$ such that $|x|\leq D$, $|y|\leq D$ and $d_{N,\gamma}(x,y)=1$, where the values of $N$ and $\gamma$ have been fixed in Section \ref{Section3.2}.

Consider the solutions $X_t^x$ and $X_t^y$. If $$|X_t^x|\leq\frac{1}{2^{1+1/\gamma}N^{1/\gamma}}\quad\text{ and }\quad |X_t^y|\leq \frac{1}{2^{1+1/\gamma}N^{1/\gamma}},$$ then $d_{N,\gamma}(X_t^x,X_t^y)\leq\frac{1}{2}.$ Otherwise we upper bound $d_{N,\gamma}(X_t^x,X_t^y)$ by 1. Using an independent coupling of $X_t^x$ and $X_t^y$, we compute that
\begin{equation}\label{largecontraction}
\begin{aligned}
    d_{N,\gamma}(\operatorname{Law}(X_t^x),\operatorname{Law}(X_t^y))&\leq d_{N,\gamma}(X_t^x,X_t^y)\\&\leq 1-\frac{1}{2}\left(\inf_{|z|\leq D}\mathbb{P}(|X_t^z|\leq\frac{1}{2^{1+1/\gamma}N^{1/\gamma}})\right)^2<1.\end{aligned}
\end{equation}
 Take $\theta$ to be the maximum of the constant $\theta_1$ in \eqref{localcontraction} and the constant in the right hand side of \eqref{largecontraction}, we conclude that 
 \begin{equation}d_{N,\gamma}\left(\operatorname{Law}(X_t),\operatorname{Law}(Y_t)\right)
\leq \theta d_{N,\gamma}(x,y),\quad |x|\leq D,|y|\leq D.\end{equation}
This finishes the proof.
\end{proof}


\subsection{Lyapunov function and exponential ergodicity}
\subsubsection{Existence and uniqueness of invariant measure}

We begin with some notations from \cite{hairer2011asymptotic}. 
We say a function $d:H\times H\to [0,1]$ is distance-like if it is symmetric, lower-semicontinuous and such that $d(x,y)=0$ implies $x=y$. We extend $d$ to a positive function $\mathcal{M}_1(H)\times\mathcal{M}_1(H)\to\mathbb{R}_+$, with $\mathcal{M}_1(H)$ the set of all Borel probability measures on $H$, by 
$$d(\mu,\nu)=\inf_{\pi\in\mathcal{C}(\mu,\nu)}\int_{H^2}d(x,y)\pi(dx,dy),$$
where $\mathcal{C}(\mu,\nu)$ consists of all the couplings of $\mu$ and $\nu$. This is exactly the 1-Wasserstein distance when $d$ is a metric on $H$.

Given a Markov operator $\mathcal{P}$ on $H$, we let $\mathcal{P}(x,\cdot)$ denote the transition probability distribution of $\mathcal{P}$ from the initial value $x\in H,$
Recall that for a Markov operator $\mathcal{P}$ on a measurable space $H$, we say a set $A\subset H$ is $d$-small if for some some $\epsilon>0,$
$$d(\mathcal{P}(x,\cdot),\mathcal{P}(y,\cdot))\leq 1-\epsilon$$
for any $x,y\in A$.
Now we are in the position to prove Theorem \ref{harristheorem}. We first show that there exists at most one invariant measure.



\begin{proof} Let $\mathcal{P}_t$ denote the Markov semigroup generated by $(X_t)$.
We check all the assumptions in Theorem 4.8 of \cite{hairer2011asymptotic} are satisfied. Fix some $t>0$ and set $D:=\sup\{|x|:x\in H, V(x)\leq 4K_V\}$ \footnote{The supremum $D$ is finite thanks to the assumption $V(x)\to\infty$ as $|x|\to\infty$.} in the setting of Theorem \ref{contractionmain}. Then we may find a choice of $N$ and $\gamma$ such that the distance function $d_{N,\gamma}$ satisfies
\begin{itemize}
    \item $\mathcal{P}_{t}$ is contracting in the sense \eqref{22112} for initial values $x,y\in H$ with $d_{N,\gamma}(x,y)<1$. 
    \item The sub-level set $\{x\in H:V(x)\leq 4K_V\}$ is $d$-small for $\mathcal{P}_{t}$.
\end{itemize}
Indeed, the first claim is fulfilled by \eqref{22112} and the second claim is fulfilled by \eqref{22111}.


Then by Theorem 4.8 of \cite{hairer2011asymptotic}, $\mathcal{P}_t$ can have at most one invariant measure, and if we define $\widetilde{d}(x,y)=\sqrt{d_{N,\gamma}(x,y)(1+V(x)+V(y))}$, there exists $t_*>0$ such that 
$$\widetilde{d}(\mathcal{P}_{t_*}\mu,\mathcal{P}_{t_*}\nu)\leq \frac{1}{2}\widetilde{d}(\mu,\nu)$$
for any probability measures $\mu,\nu\in\mathcal{M}_1(H)$. 


By the elementary inequality 
$$1\wedge |x-y|^\gamma\leq 1\wedge N|x-y|^\gamma\leq N(1\wedge |x-y|^\gamma),$$
we may choose a larger $t_*$ and redefine $\widetilde{d}(x,y)=\sqrt{(1\wedge |x-y|^\gamma)(1+V(x)+V(y))}$ so that we still have
\begin{equation}\label{iterativeexist}\widetilde{d}(\mathcal{P}_{t_*}\mu,\mathcal{P}_{t_*}\nu)\leq \frac{1}{2}\widetilde{d}(\mu,\nu).\end{equation}

The last step is to prove there exists an invariant measure $\pi$ for $\mathcal{P}$. Fix a probability measure $\mu\in\mathcal{M}_1(H)$ such that $\int Vd\mu<\infty$. Applying \eqref{iterativeexist} iteratively, we get
 $$\widetilde{d}(\mathcal{P}_{(n+1)t_*}\mu,\mathcal{P}_{nt_*}\mu)\leq\frac{1}{2^n} \widetilde{d}(\mathcal{P}_{t_*}\mu,\mu).$$
 Since $\widetilde{d}$ dominates $d_0:=1\wedge |x-y|$, and the latter is a complete metric on $H$, we deduce that the Wasserstein distance associated with $\widetilde{d}$ defines a complete metric for probability measures on $H$ that integrates $V.$ Thus there exists a probability measure $\mu_\infty$ such that $\widetilde{d}(\mathcal{P}_{nt_*}\mu,\mu_\infty)$ converges to $0$ as $n\to\infty$. One readily checks that $\widetilde{d}(\mathcal{P}_{t_*}\mu_\infty,\mu_\infty)=0$, so that $\mu_\infty$ is invariant for $\mathcal{P}_{t_*}$.
 
 Finally, define 
 $$\mu_*(A)=\frac{1}{t_*}\int_0^{t_*}(\mathcal{P}_s\mu_\infty)(A)ds$$ for each measurable $A\subset H$. One sees that $\mathcal{P}_t\mu_*=\mu_*$ for any $t>0$.
\end{proof}


\subsubsection{Convergence rate: finishing proof of Theorem \ref{harristheorem}},  As the idea of proof is similar to Theorem 2.4 of \cite{butkovsky2014subgeometric} and Theorem 2.3 of\cite{kulik2020well}, we only give a sketch of its main ideas.

\begin{proof}
First consider the discrete-time Markov chain $(X_{t_0n})_{n\in\mathbb{N}_+}$. Set $t=t_0$ in Theorem \ref{contractionmain}, we can fix a choice of the distance $d=d_{N,\gamma}$, $N\in\mathbb{N}_+$, $\gamma\in(0,1)$ such that the sub-level set $V^{-1}([0,4C_V])$ is $d$-small. 
We take $p=\epsilon^{-1}$ and $q=(1-\epsilon)^{-1}$, and define $d_{M,\gamma,p}(x,y)=d_{N,\gamma}(x,y)^{1/p}.$
Now applying Theorem 2.1 of \cite{butkovsky2014subgeometric} or Theorem 4.5.2 of \cite{kulik2017ergodic}, we deduce the existence of an invariant measure $\pi$ and that, for some $c,C>0$,
\begin{equation}\label{geometricmain}d_{N,\gamma,p}(\operatorname{Law}(X^x_{nt_0}),\pi)\leq \frac{C}{(e^{cnt_0})^{1-\epsilon}}(V(x))^{1-\epsilon},\quad n\in\mathbb{N}_+.\end{equation}


Moreover, using \eqref{longtimespider} and that the distance $d_{N,\gamma}$ is upper bounded by 1, we further conclude that for all $x,y\in H$, 
\begin{equation}\label{contractsmall}
d_{N,\gamma}(\operatorname{Law}(X_t^x,X_t^y))\leq  4 d_{N,\gamma}(x,y),\quad 0\leq t\leq t_0.\end{equation}

Combining the Markov property, \eqref{geometricmain} and \eqref{contractsmall}, using also
$$d_{N,\gamma}(x,y)\leq d_{N,\gamma,p}(x,y),$$
we deduce that for some $\xi>0$,
$$d_{N,\gamma}(\operatorname{Law}(X_t^x),\pi)\leq \frac{C}{({e^{\xi t}})^{1-\epsilon}} (V(x))^{1-\epsilon},\quad \text{for all }x\in H,t\geq 0.$$

We complete the proof noting that $$d_{\gamma'}(x,y)\leq d_{\gamma}(x,y)\leq d_{N,\gamma}(x,y)$$
for any $0<\gamma\leq\gamma'\leq 1$ and $N\in\mathbb{N}_+.$
\end{proof}







\subsection{Proof of Theorem \ref{burgerstheorem2}}
We now prove Theorem \ref{burgerstheorem2}. The argument is the same as previous, and we only outline where changes are needed when deriving various estimates.
 Arguing as in \eqref{finalconclusion}, we deduce similarly that
\begin{equation}\label{burgersfinalconclusion}\inf_{|x|\leq D}\mathbb{P}(|X_t^x|\leq\delta)\geq\frac{1}{L(D,\delta)}.\end{equation}
Then use an independent coupling, we deduce as in \eqref{largecontraction} that 
\begin{equation}\label{burgerslargecontraction}
\begin{aligned}
    d_{N,\gamma}(\operatorname{Law}(X_t^x),\operatorname{Law}(X_t^y))&\leq d_{N,\gamma}(X_t^x,X_t^y)\\&\leq 1-\frac{1}{2}\left(\inf_{|z|\leq D}\mathbb{P}(|X_t^z|\leq\frac{1}{2^{1+1/\gamma}N^{1/\gamma}})\right)^2<1.\end{aligned}
\end{equation}
for any $(x,y)\in H\times H$ with $|x|\leq D$ and $|y|\leq D$.


For the generalized coupling, fix two initial values $x,y\in H$. Consider the processes
\begin{equation}\label{burgerseqx}dX_t=AX_t dt+b(X_t)dt+(-A)^{1/2}F(X_t)dt+\sigma(X_t)dW_t,\quad X_0=x,\end{equation}
\begin{equation}\label{burgerseqy}dY_t=AY_t dt+b(Y_t)dt+(-A)^{1/2}F(Y_t)dt+\sigma(Y_t)dW_t,\quad Y_0=y,\end{equation}
where we set $$\tau=\tau_{x,y}:=\inf\{t\geq 0:|X_t-\widetilde{Y}_t|\geq 2|x-y|\},$$
and set $\lambda:=|x-y|^{\gamma -1}$ for some $\gamma\in(0,1)$ to be determined later. 
Consider also
\begin{equation}\label{burgerscontrolprocess}
d\widetilde{Y}_t=A\widetilde{Y}_t dt+b(\widetilde{Y}_t)dt+(-A)^{1/2}F(\widetilde{Y}_t)dt+\lambda(X_t-\widetilde{Y}_t)dt1_{t\leq\tau}+\sigma(\widetilde{Y}_t)dW_t,\quad \widetilde{Y}_0=y.\end{equation}

Then arguing as in \eqref{controlgirsanov}, we deduce that for any $T>0$ we can find $C$ such that 
\begin{equation}
    \label{burgerscontrolgirsanov}
d_{TV}(\operatorname{Law}(Y|_{[0,T]}),\operatorname{Law}(\widetilde{Y}|_{[0,T]}))\leq CT^{1/2} |x-y|^\gamma.\end{equation}

Then arguing through the mild formulation of $X_t$ and $\widetilde{Y}_t$ as in \eqref{difference}, 
using the Hölder continuity $\mathbf{H}_2$, $\mathbf{H}_3$ and $\mathbf{H}_6$ of $b$, $\sigma$ and $F$, the definition of the stopping time $\tau$ and Proposition \ref{stochasticinte}, we obtain as in \eqref{fififi} the following deviation estimate: for each $m\in\mathbb{N}_+$ and $\eta\in(0,\frac{1}{2})$, we may find some fixed constant $M=M_{m,\eta}>0$ such that for $R>0$,
\begin{equation}\label{burgersfififi}
    \mathbb{P}(\sup_{0\leq t\leq \tau\wedge T}\{|X_t-\widetilde{Y}_t|- e^{-\lambda t}|x-y|\}\geq  M(\lambda^{-1}|x-y|^{\alpha}+\lambda^{-\frac{1}{2}}|x-y|^\zeta+\lambda^{-\frac{1}{2}\eta}|x-y|^{\beta} R))\leq \frac{M}{R^m},
\end{equation}
where we also used the auxiliary estimate \eqref{burgersusefulestimate}. Recall our choice $\lambda=|x-y|^{\gamma -1}$. We now choose $\gamma>0$ sufficiently small such that \eqref{burgerscondition2} is satisfied. 

Then we find some  $0<\chi<\min(\alpha-\gamma,\beta+\frac{1-\gamma}{2}\eta-1,\zeta+\frac{1-\gamma}{2}-1)$ and set furthermore $$R=|x-y|^{-\chi}.$$ 
Arguing as in \eqref{1.111} and \eqref{deviationestimate}, we conclude that 
\begin{equation} \label{burgersdeviationestimate}
    \mathbb{P}(|X_T-\widetilde{Y}_T|\geq \frac{1}{2}|x-y|)\leq M |x-y|^{2\gamma}\wedge 1.
\end{equation}
Having worked out this short-time estimate, we can construct the distance $d_{N,\gamma}$ as desired. Arguing as in \eqref{goodgifts}, we obtain as in \eqref{localcontraction} that for a choice of $N_0$ and $\gamma$ depending on $t$,
\begin{equation}\label{burgerslocalcontraction}d_{N,\gamma}\left(\operatorname{Law}(X_t^x),\operatorname{Law}(X_t^y)\right)
\leq \theta_1 d_{N,\gamma}(x,y),\quad N\geq N_0,\quad d_{N,\gamma}(x,y)<1.\end{equation}
This completes the proof of Theorem \ref{contractionmain} in the setting of a Burgers type non-linearity, and the whole proof of Theorem \ref{burgerstheorem2} now finishes.



\appendix
\section{Compactness of stochastic PDEs}\label{appendixA}
In the appendix we again assume that $A$ satisfies assumption $\mathbf{H}_1$.
The following proposition will be quite useful (see for example Proposition 8.4 of \cite{da2014stochastic}):

\begin{proposition}\label{prop1.3}
If $S(t),t>0$, are compact operators and $0<\frac{1}{p}<\eta\leq 1$, then the operator $G_\eta$
\begin{equation}\label{factorization}G_\eta f(t)=\int_0^t (t-s)^{\eta -1}S(t-s)f(s)ds,\quad f\in L^p((0,T);H),t\in[0,T]\end{equation}
is compact from $L^p((0,T);H)$ into $C([0,T];H).$
\end{proposition}
The proof consists in verifying that for each $t\in[0,T]$, the set $\{G_\eta f(t):|f|_p\leq 1\}$ is relatively compact in $H$, and that $G_\eta f(t),t\in[0,T]$ is uniformly continuous in $t$ with respect to the operator norm, i.e. $|G_\eta f(t)-G_\eta f(s)|\leq \epsilon$ if $|f|_p\leq 1$, $|t-s|\leq\delta$. As the concept of a set being relatively compact is equivalent to it being totally bounded, we may first find a finite subset  $\mathfrak{A}\subset[0,T]$ and a relatively compact subset $K_\mathfrak{A}$ of $H$ such that the unit ball of $L^p$ maps to $K_\mathfrak{A}$ under $G_\eta f(t)$ for each $t\in\mathfrak{A}$. Then we use the continuity of $G_\eta f(t)$ in operator norm to find a relatively compact subset $K\subset H$ such that $G_\eta f(t)$ maps the unit ball of $L^p$ into $K$, for all $t\in[0,T]$. 

The operator $G_\eta$ is quite useful because of the following factorization formula:
for any adapted process $\phi\in L^p(\Omega\times[0,T];\mathcal{L}(H_0,H)),$
\begin{equation}
    \int_0^t S(t-s)\phi(s)dW_s=\frac{\sin\pi\eta}{\pi} G_\eta Y(t),
\end{equation}
where 
\begin{equation}
    Y(t)=\int_0^t (t-s)^{-\eta} S(t-s)\phi(s)dW_s.
\end{equation}


Before we proceed, we note that the solution to the SPDE \eqref{evolutionequation} can be represented as
\begin{equation}\label{expansionrepresent}X_t=S(t)x+G_1 b(t)+\frac{\sin\pi\eta}{\pi}G_\eta \sigma(t),\end{equation}
with $b(t)=b(t,X_t)$
and $\sigma(t)=\int_0^t (t-s)^{-\eta} S(t-s)\sigma(s,X_s)dW_s.$ 


Under the linear growth assumption $\mathbf{H_5}$ and the semigroup assumption $\mathbf{H}_1$, one can find a constant $C_T>0$ such that 
\begin{equation}\label{momentestimate}
\sup_{t\in[0,T]}\mathbb{E}[|X_t|^p]\leq C_T(1+|x|^p),\end{equation} where $X$ is a solution to \eqref{evolutionequation} and $p\geq 2$. The proof can be found for example in Theorem 7.5 of \cite{da2014stochastic}. More precisely, we need to find Lipschitz approximations of the coefficients $b$ and $\sigma$, but as the resulting estimate is independent of the Lipschitz constant, we simily drop that approximation.

Now we can use \eqref{momentestimate} to estimate $b(t)$ and $\sigma(t)$. Via Young's inequality and BDG inequality,
\begin{equation}\label{youngbdg}\int_0^T \mathbb{E}[ |\sigma(s)|^p] ds\leq C_p\int_0^T\left(\int_0^s (s-r)^{-2\eta}|S(s-r)\sigma(X_r)|_2^2dr\right)^{p/2} ds,\end{equation}
which is further bounded, using assumption $\mathbf{H}_1$, estimate \eqref{momentestimate} and the linear growth assumption $\mathbf{H}_5$, by $C(1+|x|^p)$ where $C$ is some universal constant. The estimate for $\int_0^T \mathbb{E}[|b(s)|^p]ds$ is similar. 

For any $0<p^{-1}<\delta\leq 1$ define 
\begin{equation}\label{blackboxs}\Lambda(R,\delta):=\{\omega\in C([0,T];H):\omega=G_\delta u,\quad \int_0^T |u(s)|^p ds<R\}.\end{equation}
Denote also by $$\Xi(R):=\{\omega\in C([0,T];H):\omega(t)=S(t)x+w_1(t)+w_2(t),w_1\in \Lambda(R,1), w_2\in \Lambda(R,\eta)\}.$$
Then $\Xi(R)$ is relatively compact in $C([0,T];H)$ for any $R>0$, thanks to Proposition \ref{prop1.3}.

Moreover,  $\mathbb{P}(X|_{[0,T]}\notin \Xi(R))$ converges to $0$ as $R\to\infty$, thanks to the boundedness of  $\mathbb{E}[|\sigma(\cdot)|_{L^p([0,T];H)}]$ and $\mathbb{E}[|b(\cdot)|_{L^p([0,T];H)}]$, see \eqref{youngbdg}. 



Now we can prove Proposition \ref{proposition1.2}.
\begin{proof} We deal with the case without the Burgers non-linearity $(-A)^{1/2}F$, and the case with such non-linearity is in the next Appendix.
For each $t\in[0,T]$ denote by $\Xi(R)_t$ the projection of $\Xi(R)$ onto the subspace $\{t\}\times H$, and identify $\{t\}\times H$ with $H$. Set $K:=\cup_{t\in[0,T]}\Xi(R)_t$. By the argument after the statement of Proposition \ref{prop1.3}, the set $K$ is relatively compact in $H$ for any $R>0$. It suffices to take $R$ sufficiently large to guarantee $\mathbb{P}(\theta_K\leq T)<\epsilon.$
\end{proof}


\section{Existence of weak mild solutions}\label{appendixB}
We again assume the semigroup satisfies assumption $\mathbf{H}_1$.

We first quote the following compactness result (see \cite{priola2022correction}, Proposition 11).

\begin{proposition}\label{propb1}
    Given $p>2$. The operator $Q:L^p([0,T];H)\to C([0,T];H)$
    $$Qf(t):=\int_0^t (-A)^{1/2}S(t-s)f(s)ds,\quad f\in L^p([0,T];H),t\in[0,T]$$
    is compact. 
\end{proposition}
Now we prove the existence of weak mild solutions needed in Theorem \ref{burgerstheorem1}. The proof is a generalization of the argument in \cite{priola2022correction}, Section 4, so we only give a sketch.

\begin{proof}
Denote by $\pi_m$ the orthogonal projection of $H$ onto the subspace spanned by the first $m$ eigenvalues $e_1,\cdots,e_m$. We write $A_m=A\circ \pi_m$. Using the weak existence result in \cite{gkatarek1994weak} (with straightforward generalization to time dependent coefficients), for each $m$ we can construct weak mild solutions to the SPDE 
\begin{equation}\label{approxSPDE}dX^m_t=A X^m_t dt+ b(t,X^m_t)dt+(-A_m)^{1/2}F(t,X^m_t)dt+\sigma(t,X^m_t)dW_t.\end{equation}
We can prove that for some $p>2$,
\begin{equation}\label{momentunis}\sup_{m\geq 1}\sup_{t\in[0,T]}\mathbb{E}|X^m_t|^p= C_p<\infty.\end{equation}
To obtain \eqref{momentunis} we need to control 
$$\left|\int_0^t S(t-s)(-A_m)^{1/2}F(s,X_s^m)ds\right|^p_H$$ via a Gronwall argument, which is done in Section 4 of \cite{priola2022correction}. We also need to control $$\left|\int_0^t S(t-s)
\sigma(s,X_s^m)dW_s\right|^p_H$$ via a Gronwall argument, which can be found in chapter 8 of \cite{da2014stochastic} (utilizing the mapping $G_\eta$ defined in \eqref{factorization}, Young's inequality and the assumption $\mathbf{H}_1$ on the semigroup $S(t)$.)

Now denote by $F_m:=\pi_m\circ F$, we learn from \eqref{momentunis}  and the linear growth property of $F$ that for any $T>0$, 
\begin{equation}
    \label{uniest12}
\sup_{m\geq 1}\mathbb{E}\int_0^T |F_m(t,X^m_t)|_H^pdt<\infty.\end{equation}

In the spirit of the decomposition \eqref{expansionrepresent}, we observe that the solution $X_t^m$ to \eqref{approxSPDE} can be reformulated as
$$X_t^m= S(t)x+G_1 b(X^m_\cdot)(t)+\frac{\sin\pi\eta}{\pi}G_\eta \sigma(X^m_\cdot)(t)+QF^n(X^m_\cdot)(t),$$
where $G_1$ and $G_\eta$ are defined in Appendix \ref{appendixA}. By $G_1b(\cdot,X_\cdot^m)(t)$ we mean that we consider the function $f:s\mapsto b(s,X_s^m)$, then compute $G_1(f)$, and evaluate the resulting function at time $t$. The same interpretation applies to $QF^m(\cdot,X_\cdot^m)$(t). The case of $G_\eta\sigma(\cdot,X_\cdot^m)(t)$  is slightly different, where we consider the function $f:s\mapsto (t-s)^{-\eta}S(t-s)\sigma(s,X_s^m)dW_s$.


Now for any $R>0$ define 
$$Q(R):=\{\omega\in C([0,T];H):\quad \omega=Q u,\quad \int_0^T |u(s)|^p ds<R\},$$
and define 
$$\begin{aligned}
\Xi(R):=\{&\omega\in C([0,T];H):\omega(t)=S(t)x+w_1(t)+w_2(t)+w_3(t),\\&w_1\in \Lambda(R,1), w_2\in \Lambda(R,\eta),w_3\in Q(R)\},\end{aligned}$$
where $\Lambda(R,1)$ and $\Lambda(R,\eta)$ are defined in \eqref{blackboxs}.
\end{proof}
By Proposition \ref{prop1.3} and \ref{propb1}, $\Xi(R)$ is compact in $C([0,T];H)$ for any $R>0$. By the uniform moment estimate \eqref{uniest12} and the corresponding one for $b(t,X_t^m)$ and $\sigma(t,X_t^m)$\footnote{For $\sigma(X_t^m)$ we need (uniform in $m$) $L^p$- estimates for the function $f^m:s\mapsto (t-s)^{-\eta}S(t-s)\sigma(X_s^m)dW_s$, which can be obtained via the infinite dimensional Burkholder inequality, the semigroup assumption $\mathbf{H}_1$ and the linear growth property of $\sigma$.}, we deduce that $$P((X_t^m)_{0\leq t\leq T}\notin \Xi(R))\to 0 \text{ as } R\to\infty,$$ uniformly over $m\geq 1$. By the Prokhorov theorem the laws of $(X_t^m)_{0\leq t\leq T}$ are tight in $C([0,T];H)$.

The next step is to use Skorokhod representation theorem to enlarge the probability space and take the limit of the tight sequence, using continuity of the coefficients and martingale representation theorem. The argument can be found in Section 2 of \cite{gkatarek1994weak}, so we omit the details.

\section{Verification of the Lyapunov condition}

\label{appendixC}
In this appendix we prove that $V(x):=|x|+1$ can serve as a Lyapunov function for the SPDEs we consider, as claimed in Remark \ref{remark111}.


\begin{proof} Since $A$ is a negative operator with largest eigenvalue $-\lambda_1<0$, necessarily we have $\|S(t)\|_{op}\leq e^{-\lambda_1 t}$. Writing the SPDE in its mild formulation as follows:
\begin{equation}\begin{aligned}
    X_t&=S(t)x+\\&\int_0^t S(t-s)b(X_s)ds+\int_0^t S(t-s)(-A)^{1/2}F(X_s)ds+\int_0^t S(t-s)\sigma(X_s)dW_s.\end{aligned}
\end{equation}
For the first term, we use $|S(t)x|\leq e^{-\lambda_1 t}|x|$. For the remaining three terms in the second line, we use the boundedness of $b$, $F$ and $\sigma$, boundedness of the semigroup $S(t)$, as well as estimate \eqref{05.1} or \eqref{burgersusefulestimate}, to deduce that they are all bounded in expectation uniformly for alll $t\in[0,T_0]$. This verifies the claimed Lyapunov condition \eqref{lyapunov1}.
\end{proof}



\printbibliography

\end{document}



For finite dimensional diffusion $dX_t=b(X_t)dt+\sigma(X_t)dW_t,$ it is well-known that the assumptions $\mathbf{H_3}$, $\mathbf{H_4}$ and $\mathbf{H_5}$ on $\sigma$ are sufficient to guarantee well-posedness of the martingale problem, while the assumption on $b$ can be more general, including bounded measurable  or singular integrable functions, see for example \cite{xia2020lq}. A Sobolev type bound on $\|\nabla \sigma\|$ is required to guarantee the weak solution is a strong one. Therefore, in our Theorem \ref{theorem1.1}, we only prove uniqueness in probability. We believe that the assumption $\mathbf{H_3}$ is close to optimal, while $\mathbf{H_2}$ can clearly be weakened, say allowing a bounded continuous drift. To be consistent with our exponential ergodiity results, we only work with the Hölder assumption $\mathbf{H_2}$ for the drift $b$.



\begin{remark}
While applying \eqref{BDG}, the exponent of $\lambda$ on the right hand side is $\lambda^{-\frac{p}{2}}$ instead of $\lambda^{-p}$. This is the only place that forces us to assume the diffusion coefficient $\sigma$ is $\beta$-Hölder for $\beta\in(\frac{1}{2},1]$. If an alternative estimate can be derived that gives $\lambda^{-p}$, we can prove well-posedness of the stochastic heat equation \eqref{evolutionequation} in the full Hölder regime $\beta\in(0,1]$.
\end{remark}




{Lipschitz case.} Assume that
    
    $\mathbf{H}_1'$: The semigroup $(S(t))_{t\geq 0}$ satisfies $\int_0^1 \|S(t)\|_2^2<\infty$,  and 
    
    $\mathbf{H_2'}$: For some $\alpha\in[0,1]$ and $M>0,$
$$\|b(x)-b(y)\|\leq M\|x-y\|^\alpha,\quad \|x-y\|\leq 1.$$ If $\alpha=0$, we require that $b$ is continuous. Moreover, assume that

    $\mathbf{H}_3'$: For some $M>0$, $$\|\sigma(x)-\sigma(y)\|\leq M\|x-y\|,\quad x,y\in H.$$
    Assume also that $\mathbf{H}_4$ and $\mathbf{H_5}$ holds. Then the conclusion of Theorem \ref{theorem1.1} holds.
    {General case.}
    
    
    
    Under $\mathbf{H}_1'$, we may use the crude estimate 
$$\int_0^t \|S(t-s)\|_2^2 e^{2\lambda s}ds\leq M e^{2\lambda t}$$
in place of \eqref{summabilityfinal}, and obtain for each $m\geq 2,$
\begin{equation}\mathbb{P}(\sup_{t\in[0,T]}\|Z(t)\|\geq C_m \|\phi\|_\infty R)\leq \frac{C_{m,\eta}}{R^m},\quad \text{for each }R\geq 2,\end{equation}
in place of \eqref{1/4conclude}.



\begin{remark}
When $\mathbf{H_2'}$ holds but $\mathbf{H_2}$ fails, that is when $b$ is continuous and locally bounded, Theorem \ref{theorem1.4} shows the solution has Feller property in addition to weak uniqueness. While it is possible to show uniqueness of martingale solution via Girsanov transform, one cannot in general prove the Feller property via Girsanov transform. Thus Theorem \ref{theorem1.4} also brings something new in this case.
\end{remark}






It implies the weaker condition:  for some $\eta>0$, 
\begin{equation}\label{weakassumption}\int_0^1 t^{-2\eta}\|S(t)\|_2^2<\infty,\end{equation}
where $\|\cdot\|_2$ denotes the Hilbert-Schmidt norm on $H$. Under this weaker condition we are only able to prove weak uniqueness when the diffusion coefficient is Lipschitz continuous. We can sort of interpolate between the strong assumption $\mathbf{H_1}$ and the weak assumption \eqref{weakassumption}, which is the content of Theorem \ref{theorem1.4



We briefly compare SDDEs and the stochastic heat equation (SHE) with non-Lipschitz coefficients. As already discussed, although in some cases one can prove the strong Feller property for SHEs, we can hardly obtain the Bismut-Li type estimate \eqref{bismut-li} if coefficients are non-Lipschitz, and this is very similar to the problem that arises in the SDDE setting with Lipschitz coefficients. The primary difficulty for SHEs is that it is driven by an infinite dimensional space-time white noise, which requires much more careful technical treatments than SDDEs driven by a finite-dimensional Brownian motion. The key is to utilize the smoothing properties of the Laplacian $\Delta$. On the other hand, we do not have to keep track of the memory process for SHEs, so that the analysis can be done on a state-dependent scale. This simplifies some arguments that are involved in the SDDE setting.





defined on the Hilbert space $L^2((0,1))$ with Dirichlet or Neumann boundary condition, and $W$ a cylindrical Wiener process on $L^2((0,1))$. A wide variety of more general SPDEs will also be considered in this paper, with the common feature that they have Hölder, but not Lipschitz, drift and diffusion coefficients. 

Let $H$ be a Hilbert space, which will be the state space of our stochastic evolution equation. Recall that a Markov semigroup $(\mathcal{P}_t)_{t\geq 0}$ on $H$ is said to satisfy  the strong Feller property if for any bounded measurable function $f$ on $H$, $\mathcal{P}_tf$ is a continuous function for any $t>0$. For the stochastic heat equation \eqref{heatequation}, when the coefficients $b$ and $\sigma$ are Lipschitz continuous and $\sigma$ is uniformly non-degenerate, \eqref{heatequation} is known  to satisfy the strong Feller property, see \cite{peszat1995strong}. We can further derive a Bismut-Elworthy-Li type estimate (see \cite{elworthy1994formulae}): for any bounded measurable function $f$ on $H$, we have
\begin{equation}
    \label{bismut-li}
|\mathcal{P}_tf(x)-\mathcal{P}_tf(y)|\leq C (t)\|f\|_\infty |x-y\|_H,\quad \text{for each }t>0,x,y\in H.\end{equation}
 Combined with a global boundedness condition, we can show that \eqref{heatequation} has a unique invariant measure (see for example \cite{da2014stochastic} ,Section 11.7.) Uniqueness of the invariant measure can also be verified via a coupling approach, see \cite{mueller1993coupling}, also under the Lipschitz assumptions on $b$ and $\sigma$.

With more refined estimates, we can often show that transition probabilities of the SPDE \eqref{heatequation} converge exponentially fast to its unique invariant measure. In such an argument, the strong Feller property in its \textit{qualitative} form is usually not enough, and \textit{quantitative} estimates involving the Bismut-Elworthy-Li formula \eqref{bismut-li} is necessary. Even for simgular stochastic PDEs like the dynamical $\phi_2^4$ model, establishing a quantitative estimate like \eqref{bismut-li} is still a critical step towards proving its exponential convergence to equilibrium, see \cite{tsatsoulis2018spectral}.

In this paper we are interested in stochastic heat equations \eqref{heatequation} with non-Lipschitz coefficients. Establishing the strong Feller property and Bismut-Elworthy-Li estimates become highly nontrivial for lack of smoothness. When $\sigma$ is the identity and $b$ is bounded continuous, the strong Feller property of \eqref{heatequation} was established in \cite{maslowski2000probabilistic} via a probabilistic method. However, the proof is only qualitative, and useful quantitative estimates like  \eqref{bismut-li} are not obtained.
When $\sigma$ is only Hölder continuous, even the well-posedness of  \eqref{heatequation} is a demanding question to be answered first, and the strong Feller property of \eqref{heatequation} (even with the drift $b=0$) does not seem to be well studied before.

To better explain the scope, we first discuss ergodicity of finite dimensional diffusion with irregular coefficients. Consider for example the SDE 
\begin{equation}dX_t=b(X_t)dt+\sigma(X_t)dW_t\label{sde1.3},\end{equation}
where $W$ is a $n$-dimensional Brownian motion, $b$ and $\sigma$ are $\mathbb{R}^n$-valued. We assume $b$ is bounded measurable (or satisfy some integrability conditions) and $\sigma$ is uniformly non-degenerate and Hölder continuous in the operator norm. In this finite dimensional setting, Zvonkin's transform \cite{zvonkin1974transformation} can be used to remove the singular drift and we can consider an equation with more regular coefficients. The strong Feller property and exponential ergodicity of \eqref{sde1.3} can be deduced if the transformed equation has these properties. It has been employed to derive exponential ergodicity results for a wide class of SDEs with irregular coefficients, see \cite{xie2020ergodicity}. The technique also extends to Lévy noise

For stochastic PDEs, however, Zvonkin's transform is not available anymore. In recent years there has been a few works on regularization by noise for the stochastic heat equation with additive noise, including \cite{butkovsky2019regularization} and \cite{athreya2020well}, but the arguments involved in these works are quite involved, and it seems rather difficult to be adapted to our setting to derive the strong Feller property. Moreover, they are hard to be adapted to the multiplicative noise case.

Our strategy is instead motivated by recent progress in ergodicity of the stochastic delay differential equation (SDDE) with finite memory. These equations have infinite dimensional state space, yet the Brownian motion only acts on a finite dimensional subspace, so that the strong Feller property fails in general. A major breakthrough in ergodicity of SDDEs is obtained by Hairer, Mattingly and Scheutzow in \cite{hairer2011asymptotic}, where they invented an asymptotic coupling strategy and developed a generalized form of Harris's theorem to establish exponential ergodicity of SDDEs with (one sided) Lipschitz coefficients and non-degenerate noise. Although the Lipschitz assumption is not necessary for the SDDE to be well-posed, it is still assumed in \cite{hairer2011asymptotic} to obtain unique ergodicity and exponential convergence. The Lipschitz assumption is finally removed in Kulik and Scheutzow \cite{kulik2020well} via a generalized coupling approach, only assuming that the diffusion coefficient satisfies a Hölder assumption and the drift satisfies a one-sided Hölder assumption. We will use this generalized coupling approach for a wide variety of stochastic PDES with space-time white noise.

Compared to the setting of stochastic delay equations, we do not have to work with a memory process $(X_t)_{-t_0\leq t\leq 0}$, which simplifies some of the presentations. However, we have to be careful when working with space-time white noise, and in particular we use the mild formulation of SPDEs instead of  Itô's formula in this paper.  Smoothing assumptions on the operator $A$ need to be specified, and the $\frac{3}{4}$-Hölder continuity of the diffusion coefficient $\sigma$ we obtain in the SPDE case (see Theorem \ref{theorem1.1}) is different from the $\frac{1}{2}$-Hölder results \cite{kulik2020well} for the delay equations. Moreover, we can consider a first order term $(-A)^{1/2}F(X_t)$ in the drift, thanks to the smoothing properties of the semigroup $S(t)$ generated by $A$. This is a purely SPDE phenomenon that has no SDE or SDDE analogue.

Another key novelty of the present work is its contribution to solution theory of SPDEs with irregular coefficients. It has long remained a difficult problem to construct a solution to SPDEs on a Hilbert space, with merely Hölder continuous diffusion coefficients. To our best knowledge, this is the first time such a solution theory has been established in wide generality. We require only that $\sigma$ is non-degenerate and $\beta$-Hölder continuous, without assuming $\sigma$ is close enough to the identity operator in any sense. A brief literature review in this regard is given in Section \ref{1.5.1review}. We note that for SPDEs with state space $\mathbb{R}$, a pathwise solution theory has been established in the case $\beta>\frac{3}{4}$ and $b$ Lipschitz continuous (see \cite{mytnik2011pathwise}). Our setting is similar to, but different from theirs in the following two ways. First, we consider an irregular drift $b$. Second, it is in general not known that SPDEs with irregular coefficients (defined on a separable Hilbert space) have  probabilistic strong solutions for each initial value (though it is possible to show strong well-posedness for a.e. initial value, see \cite{da2013strong} and \cite{da2016strong}). For SPDEs defined on $\mathbb{R}$, more tools are available such as the comparison principle of deterministic PDEs that are real valued, so it is relatively easy to prove pathwise uniqueness in such cases, see for example \cite{gyongy1993quasi} and  \cite{bally1994white}. As another takeaway, our proof is considerably shorter than that in  \cite{mytnik2011pathwise}, but the latter work does not require $\sigma$ to be non-degenerate while our work does require that.

A major technical input of this paper is a maximal inequality of Ornstein–Uhlenbeck type process $(\lambda>0)$
$$dX_t=\Delta X_t dt-\lambda X_tdt+\Phi(t) dW_t,\quad X_0=0$$
for some adapted process $\Phi$. We are interested in estimates of the form $\mathbb{P}(\sup_{0\leq t\leq T}|X_t|>M).$ In the finite dimensional setting many such results are available, see for example \cite{graversen2000maximal}, but they do not carry over to the infinite dimensional setting. We give a proof of such inequalities via the factorization formula of SPDEs in Proposition \ref{stochasticinte}. In the case that $\Delta$ is the Laplacian operator on $(0,1)$, we roughly show that $\sup_{t\in[0,T]}|X_t|$ has magnitude $\lambda^{-\frac{1}{4}+}$ when $\lambda$ is sufficiently large.  

In this paper we exclusively work with \textbf{weak mild} solutions to  stochastic evolution equations on \textbf{a separable Hilbert space $H$} driven by a cylindrical wiener process $W$,
\begin{equation}\label{conceptual}dX_t=A X_t dt+b(X_t)dt+\sigma(X_t)dW_t,\quad X_0=x\in H.\end{equation} In a word, this means the solution is probabilistically weak, and is given in the form of a mild solution in the PDE sense. More precisely, a \textit{weak mild} solution consists of a sequence $(\Omega,\mathcal{F},(\mathcal{F}_t),\mathbb{P},W,X)$, where $(\Omega,\mathcal{F},(\mathcal{F}_t),\mathbb{P})$ is a filtered probability space defining both a cylindrical wiener process $W$ and an $\mathcal{F}_t$-adapted, $H$-valued continuous process $(X_t)_{t\geq 0}$ such that they satisfy, $\mathbb{P}$-almost surely,
\begin{equation}\label{concepmild}X_t=S(t)x+\int_0^t S(t-s)b(X_s)ds+\int_0^t S(t-s)\sigma(X_s)dW_s,\end{equation} where $S(t)=e^{tA}$ is the semigroup generated by $A$ on $H$.
If we have a term $(-A)^{1/2}F(X_t)$ instead of the drift $b(X_t)$ in \eqref{conceptual}, then we have the corresponding term $$\int_0^t (-A)^{1/2}S(t-s)F(X_s)ds$$ in the mild formulation \eqref{concepmild}.











We say a function $d:H\times H\to [0,1]$ is distance-like if it is symmetric, lower-semicontinuous and such that $d(x,y)=0$ implies $x=y$. We extend $d$ to a positive function $\mathcal{M}_1(H)\times\mathcal{M}_1(H)\to\mathbb{R}_+$, with $\mathcal{M}_1(H)$ the set of all Borel probability measures on $H$, by 
$$d(\mu,\nu)=\inf_{\pi\in\mathcal{C}(\mu,\nu)}\int_{H^2}d(x,y)\pi(dx,dy),$$
where $\mathcal{C}(\mu,\nu)$ consists of all the couplings of $\mu$ and $\nu$. This is nothing but the 1-Wasserstein distance when $d$ is a metric on $H$.



In this paper it is not hard to construct, for any $R>0$, a distance-like function $d$ on $H$ so that the unit ball $B(0,R)$ in $H$ is $d$-small. The difficult part is to find a $d$ that is contracting for the dynamics \eqref{heatequation} for sufficiently close initial values $x,y$. Inspired by \cite{kulik2020well}, we consider the distance function $$d_{N,\gamma}(x,y):=(N\|x-y\|^\gamma)\wedge 1,\quad N\geq 1,\gamma\in[0,1],$$
where $\|\cdot\|$ is the distance on the Hilbert space $H$. The constant $\gamma>0$ is chosen depending on the Hölder exponents of $b$ and $\sigma$, and the constant $N$ will be chosen sufficiently large.






\begin{remark}
    Depending on the choice of $\phi$, various convergence rates can be obtained from Theorem \ref{theorem1.55}. We obtain exponential convergence rates if $\phi(v)=cv$, and in such cases $r(t)=ce^{ct}$. We obtain subexponential convergence rate if $\phi(v)=c(v+b)\log^{-\eta}(v+b)$ for $c>0,\eta>0$ and $b$ large enough, in which case $r(t)\geq c_0e^{c_1 t^{1/(1+\eta)}}$. We obtain polynomial convergence rates if $\phi(v)=cv^{-\epsilon}$, $c>0,\epsilon\in(0,1)$, in such cases $r(t)$ is a polynomial in $t$. See also \cite{douc2004practical}, Section 2, or \cite{butkovsky2014subgeometric} Theorem 3.3, or \cite{kulik2020well}, Remark 2.2 for more examples.
\end{remark}






\begin{remark}
Theorem \ref{harristheorem} provides exponential convergence and spectral gap results for the Markov semigroup $(\mathcal{P}_t)_{t\geq 0}$ at discrete times $t=t_*k,k\in\mathbb{N}_+$. In general we cannot deduce a contraction bound of the form 
$$\widetilde{d}(\mathcal{P}_t\mu,\mathcal{P}_t\nu)\leq c_1e^{-c_2t}\widetilde{d}(\mu,\nu),$$ that holds for all $t\geq 0$ via Harris theorem, unless $C_V\leq 0$ or $C_V>0$ is sufficiently small. This is because in the application of Harris theorem (see \cite{hairer2011yet} or \cite{hairer2011asymptotic}, Theorem 4.8) we should check the level set $\{x\in H:V(x)\leq 4C_V\}$ is $d$-small. In our setting this is achieved via the estimate \eqref{largecontraction}. Regarding that estimate, unless we already have $4C_V\leq \frac{1}{2^{1+1/\gamma}N^{1/\gamma}},$ for $\gamma$ and $N$ some parameters determined by the coefficients of the SPDE,  the probability estimate in \eqref{largecontraction} cannot be made uniform over all $t\geq 0$.
\end{remark}
