\chapter*{Appendix}

\section*{Chapter 3 Generating Query-relevant, Long-form Answers}

\subsection*{A Details of HLTC-MRQA Model}
\label{sec:appx-qa}
The MRQA model~\cite{su2019generalizing} is leveraged in the CAiRE-Covid system. To equip the model with better generalization ability to unseen data, the MRQA model is trained in a multi-task learning scheme on six datasets: SQuAD~\cite{rajpurkar2016squad}, NewsQA~\cite{trischler2017newsqa}, TriviaQA~\cite{Joshi_2017}, SearchQA~\cite{dunn2017searchqa}, HotpotQA~\cite{yang2018hotpotqa} and NaturalQuestions~\cite{naturalQ}. The training sets vary from each other in terms of data source, context lengths, whether multi-hop reasoning is needed and strategies for data augmentation. To evaluate the generalization ability, the authors utilized the BERT-large model~\cite{devlin2019bert}, which is trained with the same method as the MRQA model as the baseline. The models are evaluated on twelve unseen datasets, including DROP~\cite{Dua2019DROP} and TextbookQA~\cite{TQA}. From Table \ref{tab:mrqa}, the MRQA model consistently outperforms the baseline and achieves promising results on the QA samples, which are different from the training samples in terms of data resource, domain etc., including biomedical unseen datasets, such as BioASQ~\cite{tsatsaronis2012bioasq} and BioProcess~\cite{berant2014modeling}.
\begin{table}[ht!]
\centering
\begin{adjustbox}{width=0.46 \textwidth}
\begin{tabular}{l|cc|cc}
\hline
\multicolumn{1}{c|}{\multirow{2}{*}{Datasets}}  
& \multicolumn{2}{c|}{\textbf{MRQA model}} 
& \multicolumn{2}{c}{\textbf{Baseline}} \\ 
\cline{2-5} 
\multicolumn{1}{c|}{} & \textbf{EM} & \textbf{F1} & \textbf{EM} & \textbf{F1} \\ 
\hline
DROP        & 41.04 & 51.11 & 33.91 & 43.50 \\
RACE        & 37.22 & 50.46 & 28.96 & 41.42  \\
DuoRC       & 51.70 & 63.14 & 43.38 & 55.14  \\
BioASQ      & 59.62 & 74.02 & 49.74 & 66.57  \\
TQA  & 55.50 & 65.18 & 45.62 & 53.22  \\
RE          & 76.47 & 86.23 & 72.53 & 84.68  \\
BioProcess         & 56.16  & 72.91  & 46.12  & 63.63       \\
CWQ         & 54.73  & 61.39  & 51.80  & 59.05       \\
MCTest             & 64.56  & 78.72  & 59.49  & 72.20       \\
QAMR               & 56.36  & 72.47  & 48.23  & 67.39       \\
QAST               & 75.91  & 88.80  & 62.27  & 80.79       \\
TREC               & 49.85  & 63.36  & 36.34  & 53.55       \\
\hline
\end{tabular}
\end{adjustbox}
\caption{Results of the MRQA model on unseen datasets~\cite{su2019generalizing}. \textit{TQA}, \textit{RE} and \textit{CWQ} are, respectively, the abbreviations for \textit{TextbookQA}, \textit{RelationExtraction} and \textit{ComplexWebQuestions}.}
\label{tab:mrqa}
\end{table}



\section*{Chapter 4 Faithful to the Source: Generating Fact-aware Long-form Answers}

\subsection*{A Case of Zero-shot Experiments on NQ and HotpotQA}
\label{appendix:rbg_results}
In addition to automatic evaluation on the two datasets, we also show concrete examples to give a better understanding. As we can see in Table ~\ref{tab:examples}, ~\textbf{BART} tends to generate unfaithful answers as it lacks of external knowledge related to the specific question, and \textbf{DPR + BART} produces somewhat faithful but irrelevant answers, while ~\textbf{FiD} performs much better but still with partially unfaithful generations. In comparison, our ~\textbf{RBG} can generate more fluent, specific and factually accurate responses.

\begin{table*}[!th]
\centering
\small
\begin{adjustbox}{width={0.99\textwidth},totalheight={\textheight},keepaspectratio}
\begin{tabular}{p{18cm}}
\hline
\textbf{Question (from NQ):} What are the ranks in the us navy? \\
\hline
\textbf{Gold Answer:} ["E-8s senior chief petty officer", "E-9s master chief petty officer"] \\
\textbf{BART:}  (?) Navy ranks in the U.S. Navy. he Navy ranks are as follows: 1  Master Chief Petty Officer (MOS) a non-commissioned officer in the Navy. 2  Master Sergeant (MS), a Master Sergeant in the Marine Corps. 3  Sergeant Major (SSM), a Sergeant Major in the Air Force. \\
\textbf{DPR-BART:}  (?) The U.S. Navy is a branch of the United States Armed Forces. The Navy is made up of the Navy, Marine Corps, Air Force, Coast Guard, and Coast Guard.\\
\textbf{FiD:}  (*) The U.S. Navy is a branch of the United States Armed Forces. The rank system of the Navy consists of three categories: junior officers, senior officers, and flag officers. Junior officers are those officers in pay grades O-1 to O-4, \hlgreen{while senior officers are the ones with pay grades of O-5 and O-6. Senior officers have pay grades ranging from O-7 to E-9}. \hlgreen{Flag officers have a pay grade ranging from E-5 to E9}. \\
\textbf{RBG}(ours): The U.S. Navy ranks are divided into three categories: junior officers, senior officers, and flag officers. Junior officers are officers in pay grades O-1 to O-4, while senior officers are those in pay grades O-5 and O-6. Flag officers serve either as a line officer or as a staff corps officer. \\
\hline
\hline
\textbf{Question (from HotpotQA):} Which  French ace pilot and adventurer fly L'Oiseau Blanc?  \\
\hline
\textbf{Gold Answer}:["Charles Eugène"] \\
\textbf{BART:} (?) L'oiseau Blanc is a French aviator and aviator who flew the L'Oiseau blanc. He is best known for being the first aviator to fly it.. \\
\textbf{DPR-BART:} (?) L'oiseau Blanc l'oiseau blanc (French: The White Bird) was a French Levasseur Pl.8 biplane that disappeared in the early morning hours of April 1, 2014. The plane was a Levassellseur PL.8 Biplane that vanished in the morning of April 2, 2014, after being piloted by an adventurer pilot and adventurer pilot..\\
\textbf{FiD:} (*) On May 8, 1927, Charles Nungesser and Francois Coli took off from Paris on the L'Oiseau Blanc, a Levasseur PL.8 biplane \hlgreen{that had been designed by Charles Nongesser}, a French ace pilot and aviator, to fly from Paris to New York City in the Orteig Prize. \\
\textbf{RBG(ours)}:  On May 8, 1927, Charles Nungesser and Francois Coli boarded L'Oiseau blanc, a 450-hp Lorraine-powered Levasseur biplane designed to compete for the Orteig Prize. They took off from Paris on 8 May 1927 and were last seen over Ireland. Less than two weeks later, Charles Lindbergh successfully made the New York-Paris journey and claimed the prize in the Spirit of St. Louis.\\
\hline
\end{tabular}
 \end{adjustbox}
\caption{Examples of the zero-shot long-form answers on the NQ~\cite{kwiatkowski2019natural} and HotpotQA ~\cite{yang2018hotpotqa} datasets. RBG model generates more fluent, specific and factually accurate responses. ‘?’ indicates factually incorrect/irrelevant  responses; * indicates partially correct responses. We use the \hlgreen{green} to mark out the unfaithful snippets.}
\label{tab:examples}
\end{table*}

\subsection*{B Number of Retrieved Documents on Generation Quality}
\label{appendix:rbg_k_1}
\begin{table}[!ht]
\centering
\begin{tabular}{c|cc}
\hline
ndocs & ROUGE-L & F1    \\ \hline
5     & 24.63   & 27.29 \\
10    & \textbf{24.72}   & \textbf{27.52} \\
20    & 24.39   & 26.68 \\
50    & 23.43   & 25.94 \\ \hline
\end{tabular}
\caption{Generation performance versus the number of retrieved documents of our model on MS MARCO~\cite{nguyen2016ms}.}
\label{tab:results_k_1}
\end{table}

We also investigate the effects of number of retrieved documents $k$, on the answer generation quality. As we can see in Table~\ref{tab:results_k_1}, the generation quality in terms of ROUGE-L and F1, do not further improve as the number of $k$ increases, and the best performance are obtained when $k=10$ in our case.

\subsection*{C Document Retriever Model Details}
\label{appendix:rbg_retriever}
% As there is no golden retrievals provided in current LFQA datasets.
As the question/answers in LFQA may cover different domains and topics, we use a multi-task variant of DPR to guarantee the retrieval performance. The retriever is trained jointly on the union of all knowledge-intensive training data in KILT benchmark~\cite{petroni2021kilt}, including TrivaQA~\cite{Joshi_2017}, kwiatkowski2019naturaluestion~\cite{kwiatkowski2019natural}, HotpotQA~\cite{yang2018hotpotqa}, Fever~\cite{thorne2018fever}, zsRE~\cite{levy2017zero}, AY2, T-REx~\cite{elsahar2018t} and WoW~\cite{dinan2018wizard}. 


\subsection*{D Human Evaluation Setup and Analysis}
\label{appendix:rbg_human_eval}

\textbf{Basic setup} As shown in Table~\ref{tab:human eval setup}, we sample 50 questions for each comparison and assign 3 annotators for each generated answer, which brings a workload of 450 judgments on model preference for each evaluation aspect. This process takes large amounts of energy and time considering the difficulty and challenges of factual-related annotation. We sample 10 questions from each of five development subsets corresponding to 5 levels of answer-passage overlap, which is a stratified sampling strategy. The answer position of each model is randomly shuffled to reduce the bias of position preference. 15 participants in our human evaluation are all researchers or students in computer science who speak and read English well.

\begin{table}[!ht]
\centering
\resizebox{0.60\textwidth}{!}
{
\begin{tabular}{c|ccc}
\hline
Comparison    & \#Questions & \#Annotators/answer  \\ \hline
RBG vs. FiD  &    50 &    3 \\
Reader analysis  &  50 &  3  \\
Pre-training analysis &  50 &  3 \\ \hline
\end{tabular}
}
\caption{Details of human evaluation for three comparisons.}
\label{tab:human eval setup}
\end{table}

\textbf{Scoring setup} We ask each annotator to select a score from \{1,2,3\} for each generated answer in terms of three aspects: \textit{fluency}, \textit{relevance} and \textit{factual correctness}. During scoring, the annotators are asked to preserve the relative better-or-not relationship between two models as much as possible. In particular, for the metric of factual correctness, the annotators check the correctness of all factual statements involved in a generated answer by referring to Wikipedia~(EN), other web pages and the golden answer. The answer with significantly fewer factual errors will get a higher score on factual correctness. We show cases in Table~\ref{tab:human eval cases} to demonstrate how the annotator evaluate three aspects in our experiment.

\textbf{Statistical analysis} We present the agreement among annotators on model preference in Table~\ref{tab:agreement} by calculating the Fleiss Kappa~\cite{fleiss_kappa} as the inter-rater consistency. The RBG vs. FiD comparison achieves better annotation agreement than other two ablation comparisons, maybe because RBG integrates both two of our contributions to improve the answer quality. In the comparison of RBG vs. FiD, annotators achieve a ``moderate agreement'' on the aspect of correctness and ``fair agreement'' on relevance~\cite{landis1977measurement}. Annotators achieve best agreements on fluency in all comparisons. It's more difficult to achieve a high degree of annotation consistency on factual correctness and relevance than fluency due to complicated facts involved in the generated answer. Therefore, we recommend taking preferred ratio as the core metric for factual-related evaluation following~\cite{krishna2021hurdles,nakano2021webgpt}. We also present score variance of four models involved in human evaluation in Table~\ref{tab:variance}. It's natural that the fluency score has the smallest variance while the difficult-to-annotate correctness has largest variance.

\begin{table}[!ht]
\centering
\resizebox{0.60\textwidth}{!}
{
\begin{tabular}{c|ccc}
\hline
Comparison    & fluency & relevance & correctness  \\ \hline
RBG vs. FiD  &    0.65 &    0.33 &    0.47 \\
Reader analysis  & 0.55 &    0.12 &    0.06  \\
Pre-training analysis&  0.62 &    0.16 &    0.08 \\ \hline
\end{tabular}
}
\caption{Agreement analysis for three comparisons in terms of three aspects. We use Fleiss Kappa~\cite{fleiss_kappa} to measure the agreement degree between annotators. The score range of [0,0.2] corresponds to slight agreement, [0.2,0.4 ] corresponds to fair agreement and [0.4,0.6] corresponds to moderate agreement~\cite{landis1977measurement}.}
\label{tab:agreement}
\end{table}

\begin{table}[!ht]
\centering
\resizebox{0.60\textwidth}{!}
{
\begin{tabular}{c|ccc}
\hline
Comparison    & fluency & relevance & correctness  \\ \hline
RBG  &    0.14 &    0.33 &    0.46 \\
RBG w/o reader & 0.22 &    0.34 &    0.48 \\
RBG w/o pre-training &  0.16 &    0.26 &    0.45 \\ 
RBG w/o reader+pre-training &  0.28 &    0.42 &    0.64 \\ \hline
\end{tabular}
}
\caption{Score variance of four models involved in human evaluation.}
\label{tab:variance}
\end{table}

\begin{table*}[htbp]
\small
\begin{adjustbox}{width={0.99\textwidth},totalheight={\textheight},keepaspectratio}
\begin{tabular}{|p{110pt}|p{260pt}lll|}
\hline
 Question & Generated answer & f & r & c \\ \hline
\multirow{2}{*}{effects of hyperthyroidism} &  Model1: Hyperthyroidism is a condition in which the thyroid gland produces too much thyroid hormone. The thyroid gland is a butterfly-shaped gland located in the neck. It is the only gland in the body that is responsible for the production of thyroid hormone, which helps regulate the body's metabolism. & 3 & 1 & 3 \\ \cline{2-5} 
 & Model2: Hyperthyroidism is a condition in which the thyroid gland produces too much thyroid hormone. This can cause the body to produce more thyroid hormone than it needs. This causes the thyroid to overproduce thyroid hormone, which can lead to a number of symptoms, including: 1  Weight gain. 2  Nausea. 3  Fatigue. 4  Vomiting. & 3 & 3 & 3 \\ \hline
\multirow{2}{*}{where is cabo de hornos} &  Model1: Cabo de Hornos is the southern tip of South America's Cape Horn Archipelago. It is the \hlgreen{largest island in the Pacific Ocean} and is the \hlgreen{only island in South America with a population of more than 2,000,000 inhabitants}. The island is also the \hlgreen{world's second-largest island} in terms of land area, with a total area of \hlgreen{1,826 square miles (4,926 square kilometers)} . & 3 & 2 & 1 \\ \cline{2-5} 
 &  Model2: Cabo de Hornos, Chile. Cabo de Hanoque, also known as Cape Horn, is the southern tip of South America. It is located in the south of Tierra del Fuego archipelago, in the AntÃ¡rtica Province, Magallanes Region. The municipality of Cabo De Hornos is the capital of the province of Puerto Williams. & 3 & 3 & 3 \\ \hline
\end{tabular}
\end{adjustbox}
\caption{Cherry-picked scoring cases by annotators in our human evaluation. f, r, and c respect fluency, relevance and correctness respectively. We use \hlgreen{green} to mark out factual incorrect snippets the annotator finds. The first case shows how annotators distinguish different levels of relevance and the second case shows that of factual correctness.}
\label{tab:human eval cases}
\end{table*}

\clearpage



\section*{Chapter 5 Generating Succinct Answers from Long-form Answers}
\subsection*{A Standard Few-shot Prompting of GPT-3}
\label{sec:appendix-gpt3}
~\citet{brown2020language} adopted the standard few-shot prompting on GPT-3, and evaluated on the three open-domain QA datasets NQ~\cite{kwiatkowski2019natural}, WQ~\cite{berant2013semantic} and TQA~\cite{joshi2017triviaqa}, for \textit{closed-book} QA task. In order to compare with their reported results, we re-implement their method using the same few-shot configuration as described in the paper and query the OpenAI API.

\textbf{Experimental Setups} 

As OpenAI hasn’t officially release information about their API model sizes, we deduce the sizes of OpenAI API models based on their performances from EleutherAI's blog\footnote{https://blog.eleuther.ai/gpt3-model-sizes/}. Specifically, we query Ada and Babbage models' API, trying to reproduce the reported results for GPT-3 Medium (350M) and GPT-3 XL (1.3B) models, respectively.

We use two prompt formats to query the OpenAI API. The first prompt format is the one described in the paper~\cite{brown2020language} (referred as \textit{GPT-3 format}): randomly draw 64 question-answer pairs from the corresponding training set, and use 'Q: ' and 'A: ' respectively as prefix before each question and answer, to build the conditioning prompts. We also use the prompt format from EleutherAI's language model evaluation harness github\footnote{https://github.com/EleutherAI/lm-evaluation-harness} (referred as \textit{EleutherAI}). Furthermore, we experiment using the same prompting format as we used in our standard prompting baseline (LM-530B) in Section~\ref{sec:baselines} (referred as \textit{Our format}), and prompting the LM of size 357M and 1.3B to compare.

\textbf{Results} 

We show the results of prompting GPT-3 under zero-shot, one-shot and few-shot settings in Table~\ref{tab:appendix-gpt-3-zeroshot}, Table~\ref{tab:appendix-gpt3-oneshot} and Table~\ref{tab:appendix-gpt3-few-shot} respectively. As we can see, no matter what prompting formats we use, the results reported in the GPT-3 paper~\cite{brown2020language} are almost always higher than our reproduced ones on all three datasets, over the two different LM sizes. The gaps become even larger at few-shot setting. Thus we conjuncture that we are not able to reproduce the results reported by ~\citet{brown2020language} using GPT-3 (175B) on the three QA datasets. So we did not include their reported results to compare with our CGAP method in Table~\ref{tab:main_results}.

Furthermore, we notice that the results based on our baseline's prompting configuration are always on par with the results from querying OpenAI API. Thus we believe that the \textbf{LM-530B} is a reliable and fair standard few-shot prompting baseline to compare with.

\subsection*{B LFQA Examples}

\label{sec:appendix_example}
We show three examples from NQ, TQA and WQ test set in Table~\ref{tab:example-NQ}, Table~\ref{tab:example-TQA} and Table~\ref{tab:example-WQ} respectively. In each table, we show the predicted answers from (1) standard prompting, (2) two-stage prompting using top-1 retrieved context $c^{\text{top-1}}_r$, (3) CGAP w/o marginalization, and (4) CGAP. All those predicted answers are based on LMs of size 530B.

We also show an example illustrate CGAP with 8 generated context and their corresponding predicted answer in Table~\ref{tab:appendix-gcap}. As we can see, the contexts that contains lot of factually inaccurate or irrelevant content (e.g. generated context 1, 2, 4, 5, 8), thus the corresponding answer is wrong/inaccurate. However, the context generation LM also generates contexts that are more relevant and factual (e.g. generated context 3, 6, 7), and they help the answer prediction LM generate a correct answer. Therefore, CGAP can predict the final answer correctly based on marginalization over generated contexts.

\begin{table*}[!ht]
\centering
\begin{adjustbox}{width=0.9\textwidth}{}
{
\begin{tabular}{ccllccc}
\hline
\multicolumn{2}{c}{\multirow{2}{*}{\textbf{Model Sizes}}} & \multirow{2}{*}{\textbf{Model sources}} & \multicolumn{1}{c}{\multirow{2}{*}{\textbf{Prompting format}}} & \multicolumn{3}{c}{\textbf{zero-shot}} \\ \cline{5-7} 
\multicolumn{2}{c}{} &  & \multicolumn{1}{c}{} & \textbf{NaturalQuestion} & \textbf{TriviaQA} & \multicolumn{1}{l}{\textbf{WebQuestion}} \\ \hline
\multicolumn{2}{c}{} & GPT-3 Medium & GPT-3 paper~\cite{brown2020language} & \textbf{1.75} & \textbf{7.61} & \textbf{3.20} \\
\multicolumn{2}{c}{\multirow{4}{*}{350M}} & \multirow{2}{*}{OpenAI API (Ada)} & GPT-3  format & 1.36 & 5.45 & 1.92 \\
\multicolumn{2}{c}{} &  & EleutherAI & 1.39 & 5.54 & 2.46 \\
\multicolumn{2}{c}{} & LM-357M & Our format & 1.41 & 5.04 & 2.12 \\ \hline
\multicolumn{2}{c}{} & GPT-3 XL & GPT-3 paper ~\cite{brown2020language} & \textbf{4.40} & \textbf{19.70} & 4.63 \\
\multicolumn{2}{c}{\multirow{4}{*}{1.3B}} & \multirow{2}{*}{OpenAI API (Babbage)} & GPT-3  format & 2.27 & 9.84 & 2.12 \\
\multicolumn{2}{c}{} &  & EleutherAI & 2.47 & 12.77 & 5.22 \\
\multicolumn{2}{c}{} & LM-1.3B & Our format & 3.88 & 14.13 & \textbf{5.61} \\ \hline
\end{tabular}
}
\end{adjustbox}
\caption{Standard zero-shot prompting of GPT-3 for open-domain QA. }
\label{tab:appendix-gpt-3-zeroshot}
\end{table*}

\begin{table*}[htbp]
\centering
\begin{adjustbox}{width=0.9\textwidth}{}
{
\begin{tabular}{ccllccc}
\hline
\multicolumn{2}{c}{\multirow{2}{*}{\textbf{Model Sizes}}} & \multirow{2}{*}{\textbf{Model sources}} & \multicolumn{1}{c}{\multirow{2}{*}{\textbf{Prompting format}}} & \multicolumn{3}{c}{\textbf{one-shot(k=1)}} \\ \cline{5-7} 
\multicolumn{2}{c}{} &  & \multicolumn{1}{c}{} & \textbf{NaturalQuestion} & \textbf{TriviaQA} & \multicolumn{1}{l}{\textbf{WebQuestion}} \\ \hline
\multicolumn{2}{c}{} & GPT-3 Medium & GPT-3 paper~\cite{brown2020language} & \textbf{3.07} & \textbf{12.90} & \textbf{6.20} \\
\multicolumn{2}{c}{\multirow{4}{*}{350M}} & \multirow{2}{*}{OpenAI API (Ada)} & GPT-3  format & 1.83 & 10.26 & 5.07 \\
\multicolumn{2}{c}{} &  & EleutherAI & 1.77 & 10.02 & 5.61 \\
\multicolumn{2}{c}{} & LM-357M & Our format & 2.24 & 9.75 & 5.12 \\ \hline
\multicolumn{2}{c}{} & GPT-3 XL & GPT-3 paper~\cite{brown2020language} & \textbf{5.43} & \textbf{26.50} & 9.15 \\
\multicolumn{2}{c}{\multirow{4}{*}{1.3B}} & \multirow{2}{*}{OpenAI API (Babbage)} & GPT-3  format & 3.55 & 20.56 & 8.27 \\
\multicolumn{2}{c}{} &  & EleutherAI & 3.55 & 21.45 & \textbf{9.45} \\
\multicolumn{2}{c}{} & LM-1.3B & Our format & 4.71 & 21.21 & 8.76 \\ \hline
\end{tabular}
}
\end{adjustbox}
\caption{Standard one-shot prompting of GPT-3 for open-domain QA. }
\label{tab:appendix-gpt3-oneshot}
\end{table*}

\begin{table*}[!ht]
\centering
\begin{adjustbox}{width=0.9\textwidth}{}
{
\begin{tabular}{ccllccc}
\hline
\multicolumn{2}{c}{\multirow{2}{*}{\textbf{Model Sizes}}} & \multirow{2}{*}{\textbf{Model sources}} & \multicolumn{1}{c}{\multirow{2}{*}{\textbf{Prompting format}}} & \multicolumn{3}{c}{\textbf{few-shot(k=64)}} \\ \cline{5-7} 
\multicolumn{2}{c}{} &  & \multicolumn{1}{c}{} & \textbf{NaturalQuestion} & \textbf{TriviaQA} & \multicolumn{1}{l}{\textbf{WebQuestion}} \\ \hline
\multicolumn{2}{c}{} & GPT-3 Medium & GPT-3 paper~\cite{brown2020language} & \textbf{4.46} & \textbf{16.30} & \textbf{12.60} \\
\multicolumn{2}{c}{\multirow{4}{*}{350M}} & \multirow{2}{*}{OpenAI API (Ada)} & GPT-3  format & 3.43 & 12.46 & 10.73 \\
\multicolumn{2}{c}{} &  & EleutherAI & 3.71 & 12.46 & 10.29 \\
\multicolumn{2}{c}{} & LM-357M & Our format & 3.85 & 11.66 & 10.97 \\ \hline
\multicolumn{2}{c}{} & GPT-3 XL & GPT-3 paper~\cite{brown2020language} & \textbf{9.72} & \textbf{32.10} & \textbf{19.60} \\
\multicolumn{2}{c}{\multirow{4}{*}{1.3B}} & \multirow{2}{*}{OpenAI API (Babbage)} & GPT-3  format & 8.28 & 24.70 & 18.95 \\
\multicolumn{2}{c}{} &  & EleutherAI & 7.81 & 24.93 & 18.16 \\
\multicolumn{2}{c}{} & LM-1.3B & Our format & 7.87 & 24.88 & 17.52 \\ \hline
\end{tabular}
}
\end{adjustbox}
\caption{Standard few-shot (k=64) prompting of GPT-3 for open-domain QA.}
\label{tab:appendix-gpt3-few-shot}
\end{table*}

\clearpage
\begin{table*}[!ht]
\centering
\begin{adjustbox}{width=0.95\textwidth}
{
\begin{tabular}{ll}
\hline
\multicolumn{2}{l}{\textbf{Question}: When is the next deadpool movie being released?} \\ \hline
Golden Answer: & {[}May 18, 2018{]} \\
Predicted Answer (standard prompting): & \textcolor{mypink3}{Prime availability TBD} \\
Predicted Answer ($c^{\text{top-1}}_r$): & \textcolor{mygreen1}{May 18, 2018} \\
Predicted Answer (CGAP w/o marginalization): & \textcolor{mygreen1}{May 18, 2018} / \textcolor{mypink3}{date21-May-2018} / \textcolor{mypink3}{May 29, 2019} /\textcolor{mypink3}{16th May 2018} \\
Predicted Answer (CGAP): &\textcolor{mygreen1}{May 18, 2018} \\ \hline
\end{tabular}
}
\end{adjustbox}
\caption{Example from NQ~\cite{kwiatkowski2019natural} test set. Red and green colors denote \textcolor{mypink3}{in-correct} and \textcolor{mygreen1}{correct} answer, respectively.}
\label{tab:example-NQ}
\end{table*}

\begin{table*}[!ht]
\centering
\begin{adjustbox}{width=\textwidth}
{
\begin{tabular}{ll}
\hline
\multicolumn{2}{l}{\textbf{Question}: Which sitcom star appeared on the big screening 'The Object of My Affection'?} \\ \hline
Golden Answer: & [Jennifer Anniston, Jen Aniston,  ...] \\
Predicted Answer (standard prompting): & \textcolor{mypink3}{Ross Hatley} \\
Predicted Answer ($c^{\text{top-1}}_r$): & \textcolor{mypink3}{Laurie Metcalfe} \\
Predicted Answer (CGAP w/o marginalization): & \textcolor{mygreen1}{Jennifer Aniston} / \textcolor{mypink3}{ Paul Rudd} /\textcolor{mypink3}{ Christine Baranski} / \textcolor{mypink3}{Lisa Kudrow} \\
Predicted Answer (CGAP): & \textcolor{mygreen1}{Jennifer Aniston} \\ \hline
\end{tabular}
}
\end{adjustbox}
\caption{Example from TriviaQA~\cite{joshi2017triviaqa} test set.}
\label{tab:example-TQA}
\end{table*}

\begin{table*}[!ht]
\centering
\begin{adjustbox}{width=\textwidth}
{
\begin{tabular}{ll}
\hline
\multicolumn{2}{l}{\textbf{Question}: Where George Lopez was born?} \\ \hline
Golden Answer: & {[}Mission Hills{]} \\
Predicted Answer (standard prompting): & \textcolor{mypink3}{San Fernando, California} \\
Predicted Answer ($c^{\text{top-1}}_r$): & \textcolor{mypink3}{Washington Heights} \\
Predicted Answer (CGAP w/o marginalization): & \textcolor{mygreen1}{Mission Hills} / \textcolor{mypink3}{Los Angeles}/ \textcolor{mypink3}{San Fernando} /\textcolor{mypink3}{ Castle Hill} /\textcolor{mypink3}{ canada }\\
Predicted Answer (CGAP): & \textcolor{mygreen1}{Mission Hills} \\ \hline
\end{tabular}
}
\end{adjustbox}
\caption{Example from WebQuestions~\cite{berant2013semantic} test set.}
\label{tab:example-WQ}
\end{table*}

\clearpage

\begin{table*}[htb]
\small
\centering
\begin{adjustbox}{width={.95\textwidth},totalheight={1.\textheight},keepaspectratio}
\begin{tabular}{p{20cm}}
\hline
\textbf{Question}: Where George Lopez was born?       \textbf{Golden Answer:} [Mission Hills]\\
\hline
\textbf{Generated Context 1}: George Lopez They are American citizens, born in Los Angeles, to Gregorio Lopez (October 26, 1934 – January 25, 2011) and Maria Torres (October 12, 1938 – October 25, 2014), Mexican immigrants who were born in Reynosa, Tamaulipas. Lopez's paternal grandparents were Mexican, while his maternal grandparents were of Mexican and Spanish descent. Lopez's father was a worker for the Los Angeles Metropolitan Transit Authority. He married Maria in Los Angeles when he was 16 years old, and she was 15. Maria filed for divorce from Gregorio in the mid-1970s. \\ 
\textbf{Predicted Answer:} Los Angeles \\
\textbf{Generated Context 2}: George Lopez born April 23, 1961. The family eventually moved from Marín to Los Angeles in the mid-1960s. After Lopez graduated from San Fernando High School in 1979, he attended the California State University, Los Angeles. It was there where Lopez discovered his comedic talent after a video of him doing a stand-up routine made it into the hands of "Tonight Show" booker, Fred De Cordova, in 1985. \\
\textbf{Predicted Answer:} canada \\
\textbf{Generated Context 3}: George Lopez at 20th Century Fox Television until 2010, when he began appearing in his self-produced sitcom, George Lopez. Lopez was born on April 23, 1961, in Mission Hills, Los Angeles, California. He was raised by his maternal grandmother, Benita Gutierrez, a factory worker, and his maternal grandfather, Refugio Gutierrez, who worked as a janitor and metal fabricator at an ice skating rink. \\
\textbf{Predicted Answer:} Mission Hills \\
\textbf{Generated Context 4}: George Lopez in San Fernando, California. He is of Mestizo Mexican descent. His father, Saul Lopez, was from Sonora, and his mother, Elvira, was a native of Arizona. They were migrant workers who picked crops and moved often, sometimes taking their seven children with them. While growing up, Lopez did not speak English until he was seven. Lopez attended Our Lady of Lourdes, a Roman Catholic elementary school in Pico Rivera. \\
\textbf{Predicted Answer:} San Fernando \\
\textbf{Generated Context 5}: George Lopez A theater called "The Tower", which now houses a branch of Bank of America, stands on the former site of his childhood home. Lopez was born at Daniel Freeman Memorial Hospital in Inglewood, California and was raised by his maternal grandmother, Benita Gutierrez, a factory worker who took him to the movies every week. He grew up in the Echo Park neighborhood of Los Angeles, California and attended many different schools in the Los Angeles Unified School District.\\
\textbf{Predicted Answer:} Los Angeles \\
\textbf{Generated Context 6}: George Lopez López was born in Mission Hills, Los Angeles, California. He is the son of Frieda and Anatasio López, Mexican immigrants from Tepic, Nayarit, Mexico. Lopez's father was a farm worker, and his mother worked at a canning factory. Lopez has stated that he had a difficult childhood; his parents divorced when he was five, and his mother struggled to raise him and his two sisters. \\
\textbf{Predicted Answer:}  Mission Hills \\
\textbf{Generated Context 7}: George Edward Lopez. Lopez was born in Mission Hills, Los Angeles. His parents, Frieda and Anatasio Lopez, were both born and raised in Texas. His mother was of Mexican descent and his father, also called ""Charlie"", was of Sicilian descent. Lopez grew up with two older sisters. He attended San Fernando High School in the Los Angeles neighborhood of San Fernando.\\
\textbf{Predicted Answer:} Mission Hills \\
\textbf{Generated Context 8}: George Lopez had in his mouth. He was raised by his mother, Frieda, a telephone company employee, and his grandmother, Benita Gutierrez, a factory worker. Lopez graduated from the Roman Catholic Saint Monica's High School in 1974, where he met his future wife, Ann Serrano. He moved to California and graduated from the California State University, Los Angeles, in 1986 with a degree in communications.\\
\textbf{Predicted Answer:} Castle Hill \\ 
\hline
\textbf{Predicted Answer (CGAP):}  Mission Hills \\

\hline
\end{tabular}
\end{adjustbox}
\vspace{-10pt}
\caption{Example illustrates CGAP, in which we marginalize over 8 generated contexts.}
\label{tab:appendix-gcap}
\end{table*}