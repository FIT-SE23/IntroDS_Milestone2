\begin{center}
{\Large \textbf{\thesistitle}}\\
\vspace{20mm}
by \thesisauthor\\
%\vspace{15mm}
\departmentname\\
%\vspace{10mm}
The Hong Kong University of Science and Technology
\end{center}
\vspace{8mm}
\begin{center}
\Large Abstract
\end{center}
\par
\noindent

\setlength{\emergencystretch}{10pt}

Question answering (QA) aims to build computer systems that can automatically answer questions posed by humans, and it has been a long-standing problem in natural language processing (NLP). This thesis investigates a particular problem of generative long-form question answering (\textbf{LFQA}), which aims to generate an in-depth, paragraph-length answer for a given question posed by a human. 

% why long-form, and why generative
Generative LFQA is an important task. A large ratio of the questions that humans deal with daily and ask on search engines are complicated \textit{why/how} types, which require multi-sentence explanations to answer. For example, \textit{'How do jellyfish function without a brain?'}, \textit{'What are the risking factors related to COVID-19?'}. Furthermore, the answers normally need to be generated by synthesizing information from multiple documents, since a short phrase span extracted from a single existing document can't answer those complicated questions.

% However, LFQA is quite challenging and under-explored. Few works have been investigated to build an effective LFQA system. Since it involves retrieving multiple relevant documents from a large external corpus which usually contains tens of millions of documents, it needs to generate the answer from the multiple documents containing tens of thousands of tokens. Moreover, it is even more challenging to generate a good-quality long-form answer relevant to the query and faithful to facts since a considerable amount of redundant, complementary, or contradictory information will be contained in the retrieved documents. 

On the other hand, LFQA is quite challenging and under-explored. Few works have been done to build an effective LFQA system. It is even more challenging to generate a good-quality long-form answer relevant to the query and faithful to facts, since a considerable amount of redundant, complementary, or contradictory information will be contained in the retrieved documents. Moreover, no prior work has been investigated to generate succinct answers.

% Little prior work has been done, and none has tried to leverage the informative answers.

% grammatically correct and semantically informative is even more challenging. 

In this thesis, we investigate the task of LFQA and tackle the challenges mentioned above. Specifically, we focus on 1) how to build a practical application for real-time open-domain LFQA, and generate more query-relevant answers, 2) how to generate more factual long-form answers, and 3) how to generate succinct answers from long-form answers.

% To elaborate, we first present a real-time LFQA system that can efficiently generate multiple-documents-based answers. The system demonstrates its effectiveness at generating fluent and somewhat relevant answers, winning one of the Kaggle competitions related to COVID-19. In addition, to further improve the query-relevance of the answer, we propose to incorporate the explicit relevance score of the source documents into the generation model.

To elaborate, we first present a coarse-to-fine method to extract the document-level and sentence-level query-relevant information, to help a traditional Seq2Seq model to handle long and multiple documents as input, and consider query relevance. We further introduce QFS-BART, a model that incorporates the explicit answer relevance attention of the source documents into the generation model's encoder-decoder attention module, to further enhance the query relevance. The CAiRE-COVID system, a real-time long-form question answering system for COVID-19, that we built has won one of the Kaggle competitions related to COVID-19, judged by medical experts.

Secondly, we present a new architectural method to tackle the answer faithfulness issue. We augment the generation process with global predicted salient information from multiple source documents, which can be viewed as an emphasis on answer-related facts. State-of-the-art results on two LFQA datasets demonstrate the effectiveness of our method in comparison to solid baselines on automatic and human evaluation metrics. The method also topped one public leaderboard on the LFQA task. 

Finally, we take a step further and propose to generate succinct answers from the long-form answers. Specifically, we extract short-phrase answers for \textit{closed-book} question answering (CBQA) task from the long-form answers. Experimental results on three QA benchmarks show that our method significantly outperforms previous \textit{closed-book} QA methods and is on par with traditional \textit{open-book} methods that exploit external knowledge sources.


% We first generate an long-form answer leveraging the amount of parameterized knowledge stored in pretrained language models, then we extract the short answer from the long-form answer. 
% which directly answer an open-domain question without access to any external knowledge.


% we propose a simple yet effective framework for \textit{closed-book} question answering (QA), which directly answer an open-domain question without access to any external knowledge. 
% In this thesis, we investigate the task of LFQA and tackle the aforementioned challenges. Specifically, we first investigate how we can build a practical application for real-time open-domain LFQA. Then, we focus on enhancing the answer quality, in terms of 1) \textbf{fluency}, i.e., grammatical correctness, and low repetition, 2) \textbf{relevance} to the answer, and 3) \textbf{faithfulness}, which measures the factual correctness of the generated answer.

% We first present a real-time LFQA system which can generate multiple-documents-based answers efficiently. The system also demonstrates its effectiveness at generating fluent and somewhat relevant answers, winning one of the Kaggle competitions related to COVID-19. Then, we propose to incorporate the explicit answer relevance of the source documents into the generation model to enhance the relevance of the generated answer. Finally, we present a new end-to-end framework for generative QA that jointly models answer generation and machine reading to tackle the answer faithfulness challenge. State-of-the-art results on two LFQA datasets demonstrate the effectiveness of our method in comparison with strong baselines on automatic and human evaluation metrics. The method also topped one public leaderboard on the LFQA task.





