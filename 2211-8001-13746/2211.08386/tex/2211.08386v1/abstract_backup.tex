%!TEX program = xelatex
%!TEX root = ./thesis.tex
\begin{center}
{\Large \textbf{\thesistitle}}\\
\vspace{20mm}
by \thesisauthor\\
%\vspace{15mm}
\departmentname\\
%\vspace{10mm}
The Hong Kong University of Science and Technology
\end{center}
\vspace{8mm}
\begin{center}
\Large Abstract
\end{center}
\par
\noindent

\setlength{\emergencystretch}{10pt}

Question answering (QA) aims to build computer systems that can automatically answer questions posed by humans, and has been a long-standing problem in natural language processing(NLP). This thesis investigates the particular problem of generative long-form question answering (\textbf{LFQA}), which aims to \textbf{generate} an in-depth, paragraph-length answer for a given question posed by a human. 

Generative LFQA is an important task. Many of the everyday questions that humans deal with and pose to search engines require multi-sentence explanations, for example, \textit{How do jellyfish function without a brain?} \textit{Why wouldn't life on another habitable planet look similar to Earth's?} or \textit{What are the risking factors related to COVID-19?}. However, conventional work on QA is mainly extractive, which means a short and concise span is extracted from a passage as the answer. These systems are thus incompetent at answering non-trivial questions. Thus, LFQA is needed to provide complete and accurate answers to complex questions.

However, LFQA is under-explored compared to the extractive QA task, and it is more challenging. Multiple documents containing tens of thousands of tokens must be synthesized to provide the answer, which is difficult for the generation model to handle. Furthermore, since a considerable amount of redundant, complementary, or contradictory information will be contained in the retrieved documents, how to generate a good-quality long-form answer that is grammatically correct and semantically informative is even more challenging. 

In this thesis, we investigate the task of LFQA and tackle the aforementioned challenges. Specifically, we focus on 1) how to build a practical application for real-time open-domain LFQA, and generate query-relevant answers, 2) how to generate fact-aware long-form answers, and 3) how to leverage the informative generated long-form answers.

To elaborate, we firstly present a real-time LFQA system which can generate multiple-documents-based answers efficiently. The system demonstrates its effectiveness at generating fluent and somewhat relevant answers, winning one of the Kaggle competitions related to COVID-19. And we propose to incorporate the explicit answer relevance of the source documents into the generation model to enhance the relevance of the generated answer. 

Secondly, we present a new end-to-end framework for generative QA that jointly models answer generation and machine reading to tackle the answer faithfulness challenge. State-of-the-art results on two LFQA datasets demonstrate the effectiveness of our method in comparison with strong baselines on automatic and human evaluation metrics. The method also topped one public leaderboard on the LFQA task. 

Finally, we propose a simple yet effective framework for \textit{closed-book} question answering (QA), which directly answer an open-domain question without access to any external knowledge. We leverage the generated long-form answers as the intermediate bridge between the huge amount of parameterized knowledge stored in pretrained language models and short answer. Experimental results on three QA benchmarks show that our method significantly outperforms previous \textit{closed-book} QA methods and is par with traditional \textit{open-book} methods that exploit external knowledge source.


% In this thesis, we investigate the task of LFQA and tackle the aforementioned challenges. Specifically, we first investigate how we can build a practical application for real-time open-domain LFQA. Then, we focus on enhancing the answer quality, in terms of 1) \textbf{fluency}, i.e., grammatical correctness, and low repetition, 2) \textbf{relevance} to the answer, and 3) \textbf{faithfulness}, which measures the factual correctness of the generated answer.

% We first present a real-time LFQA system which can generate multiple-documents-based answers efficiently. The system also demonstrates its effectiveness at generating fluent and somewhat relevant answers, winning one of the Kaggle competitions related to COVID-19. Then, we propose to incorporate the explicit answer relevance of the source documents into the generation model to enhance the relevance of the generated answer. Finally, we present a new end-to-end framework for generative QA that jointly models answer generation and machine reading to tackle the answer faithfulness challenge. State-of-the-art results on two LFQA datasets demonstrate the effectiveness of our method in comparison with strong baselines on automatic and human evaluation metrics. The method also topped one public leaderboard on the LFQA task.





