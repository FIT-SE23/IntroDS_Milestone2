
\chapter{Background and Preliminaries}

In this chapter, we first give a brief introduction to open-domain question answering task. Then, we will introduce the preliminary technologies involved in our work for generative-based LFQA task, followed by related work.

\section{Background: Open-domain Question Answering}

Open-domain question answering has been an important research topic in the history of NLP~\cite{harabagiu2000experiments,lin2003question,gliozzo2012natural, yih2016question, sachan2018standardized, chen2017reading}. The goal of open-domain question answering is to build computer systems to answer any sort of (factoid) questions that humans might ask automatically, based on a large collection of unstructured natural language documents, structured data, semi-structured data or even other modalities such as images or videos~\cite{chen2018neural}. 

\subsection{A Brief History of Open-domain QA}
Dating back to the 1960s~\cite{simmons1964indexing}, the task of open-domain question answering has been broadly investigated by both the Information retrieval (IR) and natural language processing (NLP) communities.

Some notable textual-based QA systems include Microsoft’s ASKMSR~\cite{brill2002analysis}, IBM’s DEEPQA~\cite{ferrucci2010building}, and DrQA~\cite{chen2017reading}. ASKMSR is a search-engine based QA system that relies on "web redundancy" rather than complicated linguistic analyses of questions or documents, while DEEPQA is the most representative modern QA system which consisting of many different pieces in a pipeline (as we show in Figure ~\ref{fig:deepQA}). The IBM DEEPQA's victory at the TV game-show JEOPARDY! in 2011 received a lot of attention, and rekindle the research interest in QA. Compared to DEEPQA's sophisticated system, the newly proposed DrQA system used a two-stage framework, integrating a classical retrieval module and a neural machine reading comprehension (MRC) component(as shown in Figure ~\ref{fig:drqa}). DrQA is designed
to answer questions from English Wikipedia and is also the first neural QA system. The two-stage \textit{retriever-reader} framework become dominated paradigm for open-domain QA~\cite{izacard2021leveraging,NEURIPS2020_6b493230}, and we have seen significant progress in recent years.

\begin{figure*}[!th]
    \centering
    \includegraphics[width=0.8\linewidth]{figures/chapter2/DeepQA.png}
    \caption{The high-level architecture of IBM’s DEEPQA~\cite{ferrucci2010building} used in WATSON. Image courtesy: \url{https://en.wikipedia.org/wiki/Watson (computer)}.}
    \label{fig:deepQA}
\end{figure*}


\begin{figure*}[!ht]
    \centering
    \includegraphics[width=0.75\linewidth]{figures/chapter2/drqa.png}
    \caption{The two-stage \textit{retriever-reader} architecture of DrQA~\cite{chen2017reading}. Image courtesy: \url{https://github.com/facebookresearch/DrQA}.}
    \label{fig:drqa}
\end{figure*}

Different kinds of resources might be involved in order to answer the questions. These resources could be \textbf{unstructured natural language documents} (encyclopedias, dictionaries, news articles and general Web documents), \textbf{structured data} (e.g., knowledge bases), or \textbf{semi-structured data}(e.g., tables) or even \textbf{other modalities} such as videos or images.

The questions posed by humans could be \textbf{simple questions(single-hop questions)} seeking for factoid information, such as \textit{"Who is the president of the US?"}, or might be \textbf{multi-hop questions} which need to aggregate information from multiple places and reasoning over them to answer, like \textit{“Which novel by the author of ‘Armada’ will be adapted as a feature film by Steven Spielberg?”}, or even more \textbf{complicated questions} such as the \textit{how/why} type questions.

The types of answers also varies a lot. The answer might be a short-phrase span \textbf{extracted} from the given passage, or \textbf{multiple-choice} type, or even free-text form \textbf{generated} from the system.

\subsection{Different Open-domain QA Tasks}

According to the difference in resources involved to answer questions or question types, there is also a difference in the relationships between the questions and the corresponding resources, along with different answer types (e.g., a text span, a ranked list of passages, cloze style, multiple choice answer, free-form answer). As a result, a QA can be formulated and categorised into different types, where the different application scenarios may differ depending of the type of QA.

QA can be categorized into the following categories via the format of the \textit{knowledge resources}:
\begin{itemize}
    \item \textbf{Conversational QA} is a QA task involving comprehension of both passage and conversational QA histories~\cite{reddy2019coqa}.It is proposed to mimic the way human seek information in conversations ~\cite{ju2019technical}. It will play a crucial role in conversational AI systems. 
    \item \textbf{Text-based QA} aims to answer questions based on unstructured natural language documents. It puts more emphasis on text understanding with answering questions regarded as a way to measure language understanding, and can also be interpreted as machine reading comprehension problem. One of the most representative text-based QA task is the SQuAD ~\cite{rajpurkar2016squad}.
    \item \textbf{Knowledge-based QA} (KBQA) analyzes query question and then finds the answer from the knowledge base/ knowledge graphs (KG)~\cite{yao2014information}. It help end users more efficiently and more easily access the substantial and valuable knowledge in the KG without knowing its data structure. Examples of a few large KGs include Wikidata (Google, 2013 ~\cite{vrandevcic2014wikidata}), DBPedia (~\cite{auer2007dbpedia}), Yago (~\cite{suchanek2007yago}), and NELL~\cite{mitchell2018never}.
    \item \textbf{Table-based QA} aims to answer complex questions on semi-structured tables, instead of a fixed database ~\cite{pasupat2015compositional}. Different questions could be asked on different tables with different schemas. 
    % \item \textbf{Community-based QA} (cQA) are defined as dedicated platforms for users to respond to other users' questions, resulting in the building of a community where users share and interactively give ratings to questions and answers (Liu et al., 2008). CQA services are emerging as a valuable information resource that is rich not only in the expertise of the user community but also their interactions and insights. 
    \item \textbf{Visual QA} (VQA) aims to answer questions about images or vidoes~\cite{balanced_vqa_v2}. The questions require an an understanding of vision, language and commonsense knowledge to answer.

\end{itemize}


\noindent Depends on the formats of the \textit{answer}, QA can also be categorised into: 
\begin{itemize}
    \item \textbf{Cloze-style}: The question contains a placeholder, and the systems must guess which word or entity completes the sentence (question), based on the passage.
    \item \textbf{Multiple-choice}: The correct answer is chosen from k hypothesized answers(e.g., k = 4).
    \item \textbf{Span-based QA}: It can also be referred to as extractive QA and the answer must be a single span from the passage.
    \item \textbf{Free-form answer QA}: The answer is allowed to be any free-text form (i.e., a word sequence of arbitrary length).
\end{itemize}

\subsection{Open-domain QA Frameworks}

The frameworks for open-domain QA can be roughly categorized into two classes: the \textit{open-book} QA methods, which exploit an external knowledge source to answer the question, and the \textit{closed-book} QA (CBQA) methods ~\cite{roberts2020much}, which tries to directly answer the open-domain questions without access to any external knowledge sources, and leverages the parametric knowledge stored in large pretrained language models (LMs)~\cite{raffel2020exploring, brown2020language, ye2020studying}. 

\subsubsection{The Open-book QA Method}
\label{sec:two-stage}
For text-based open-domain QA, the majority of the systems consisted of two stages: a \textbf{document retriever}, which is normally an information retrieval (IR) system used to retrieve most related documents/paragraphs a question, and a \textbf{document reader} module, which normally cast as a machine reading comprehension (MRC) task, to predict the answer spans from the selected candidates paragraphs. Thus, most work in (text-based) open-domain QA are mainly focus on the two tasks: 
\begin{itemize}
    \item \textit{Information retrieval (IR)}: how to effectively and efficiently (considering both precision and recall metrics) retrieve the related candidate documents,
    \item \textit{Machine reading comprehension(MRC)}: accurate answer span prediction and ranking (by reasoning over multi-documents).
\end{itemize}

The \textit{open-book} QA method have obtained state-of-the-art results on open-domain QA benchmark datasets. The most representative work of the open-book QA methods, include RAG~\cite{NEURIPS2020_6b493230} and FiD~\cite{izacard2021leveraging}. 

\subsubsection{The Closed-book QA Method}
Another branch of work ~\cite{petroni2019language, radford2019language, brown2020language,roberts2020much} focused on the retrieval-free approach, trying to use the large pre-trained language models such as  BERT~\cite{petroni2019language}, GPT-2~\cite{radford2019language}, GPT-3~\cite{brown2020language}, T5~\cite{roberts2020much} as 'knowledge storage' to get the answer, instead of explicitly storing all the text and searching among their dense or sparse representations. Since the LMs were pre-trained on Wikipedia (and other textual corpora) so they should be able to memorize a fair amount of information. However, the \textit{closed-book} methods are not competitive with \textit{open-book} models in term of accuracy.

\section{Preliminaries}
In this part, we will introduce the techniques required for our work in building an effective LFQA system.

\subsection{Sequence-to-Sequence (Seq2Seq)}
Sequence-to-Sequence (or Seq2Seq) is a neural network that transforms a given input sequence into another sequence. For example, the input could be a sequence of English words, the output could be another sequence of German words. If we denote the input and output sequence as $X = \{x_1,..., x_n\}$, and $Y = \{y_1,..., y_m\}$, the objective of a Seq2Seq model is to approximate the conditional probability $P(Y|X)$. Formally, a Seq2Seq model generates a sequence of probability distributions, each of which is conditioned on the previously generated tokens and the input sequence X, and the model can be formulated as:
\begin{equation}
    P_\theta(Y|X) = \sum_{i=0}^mP_\theta(y_i|y_0,...,y_{i-1};x_0,...,x_n)
\end{equation}

where $\theta$ denotes the model parameter.

Seq2Seq models consist of an Encoder and a Decoder. The Encoder takes the input sequence and maps it into a higher dimensional space (n-dimensional vector),
\begin{equation}
   h_{enc} = \text{Encoder}(x_1,...,x_n) 
\end{equation}

where $h_{enc} = \{h_{enc}^1, ...h_{enc}^i,..., h_{enc}^n\}$, and $h_{enc}^i \in R^{d}$ is a d-dimensional vector. Then the encoder output is fed into the Decoder which turns it into an output sequence. The output sequence can be in another language, symbols, a copy of the input etc., and can be simply formulated as:
\begin{equation}
    p(y_i) = \text{Decoder}(y_0,...,y_{i-1};h_{enc})
    \label{eq: seq2seq}
\end{equation}

While the Encoder and Decoder can adopt different neural architectures, such as the Recurrent Neural Network(RNN), Long-Term Short Term Memory(LSTM)~\cite{hochreiter1997long} or Gated Recurrent Unit (GRU)~\cite{cho2014learning}, it can also employ the non-recurrent-based network, such as the Transformer we are going to introduce in Section ~\ref{sec:transformer}.

\subsection{Transformer}
\label{sec:transformer}
The Transformer was proposed in the paper \textit{Attention is All You Need}~\cite{NIPS2017_3f5ee243}. It is an only attention-mechanism-based Seq2Seq model, without any RNN (Recurrent Neural Networks), thus it can be computed via parallelizable operation. 


Both Encoder and Decoder are composed of modules that can be stacked on top of each other multiple times, which is described by $N$. We see that the modules consist mainly of Attention and Feed Forward layers ( as shown in Figure ~\ref{fig:transformer}). 

\begin{figure*}[!ht]
    \centering
    \includegraphics[width=0.7\linewidth]{figures/chapter2/transformer.png}
    \caption{A simple illustration of Transformer (part).}
    \label{fig:transformer}
\end{figure*}

The attention mechanism in both the Encoder and Decoder can be illustrated as:

\begin{equation}
    Attention(Q, K, V) = softmax(\frac{QK^{T}}{ \sqrt{d}}V),
\end{equation}

$Q$ is a matrix that contains the query (vector representation of one word in the sequence), $K$ are all the keys (vector representations of all the words in the sequence) and $V$ are the values, which are again the vector representations of all the words in the sequence.

While for the \textbf{Self-Attention} in both the Encoder and the Decoder, the query $Q$ is the same as $K$, which is also equal to $V$. However, in the \textbf{Encoder-Decoder Attention} layer in the Decoder block, which is also called \textbf{Cross-Attention}, the query $Q$ will be from the Decoder, while $K$ and $V$ are from the Encoder sequence. The cross-attention mechanism can help the decoder focus on relevant parts of the input sentence.

\subsection{Pre-trained Language Models}
%BERT: for MRC
%BART: for Generation

In general, pre-trained models are divided into bi-directional such as BERT~\cite{devlin2019bert}, uni-directional or casual-decoder (left-to-right decoder) such as the GPT model~\cite{radford2019language, brown2020language}, and encoder-decoder generative model like BART~\cite{lewis2020bart} and T5~\cite{2019t5}. Those models have achieved state-of-the-art performances in many natural language understanding tasks~\cite{wang-etal-2018-glue}. 

The bi-directional pre-trained models are trained with a masked-language model (MLM) loss, which learns how to predict words that are randomly masked in the input. While the uni-directional and encoder-decoder generative models are usually trained using the likelihood function in Equation ~\ref{eq: seq2seq}. 

In this thesis, we mostly focus on encoder-decoder based generative models. The encoder-decoder based pre-trained LM models are trained on huge amount of unlabeled data. Many different pre-training strategies has been proposed based on the task objective. For example, BART used span-prediction and denoising pre-training~\cite{lewis2020bart}: the input sequence is corrupted and the model is taught to reconstruct the original sequence.

\subsection{Retrieval Methods}
As we mentioned in Section ~\ref{sec:two-stage}, the document retriever component will retrieve the most related documents/paragraphs given a question, from a large-scale external document collections. 

Traditional methods use the \textbf{sparse retrieval} based approach (TF-IDF or BM25) to get a candidate set. For example, the TF-IDF weighted term vector model over unigrams/bigrams can be formulated as follows:
\begin{align}
    \text{tf-idf}(t,D_i,D) &= \text{tf}(t,D_i) \cdot \text{idf}(t,D)
    \\
    \text{tf}(t,D_i) &= \text{log}(1+\text{freq}(t,D_i)) \\
    \text{idf}(t,D) &= \text{log}(\frac{|D|}{|d\in D: t \in D_i}|)
\end{align}

where we use $t$ denote the term(unigrams/bibrams) in query $Q$, and $D_i$ refer to one document such as one wikipedia article, and $D$ is the whole external knowledge sources. tf means term frequency, and idf represents the inverse document frequency.

More recently, better results~\cite{karpukhin2020dense,guu2020realm} have been obtained with \textbf{dense-only retrieval} methods. Basically the model pre-encodes all docoments $D$ with large-pretrained language models such as BERT~\cite{devlin2019bert}, then encodes question $Q$ into the same space at test time, retrieves with simple nearest-neighbor search: 

\begin{equation}
    sim(Q, D_i) = \text{BERT}_q(Q)^T \text{BERT}_d(D_i)
\end{equation}

Compared to the traditional sparse-based retrieval methods, the dense retrieval techniques enables: (1) a trainable retriever, (2) dense representations of the documents and the query, which leads to better retrieval performance. Currently there are techniques and tools to support fast maximum inner product search (MIPS), such as the approximate nearest neighbors search in the FAISS~\footnote{github.com/facebookresearch/faiss} library, which uses in-memory data structure and indexing schemes, the dense retrieval can be done quite efficiently.

\subsection{Machine Reading Comprehension}

The task of Machine Reading Comprehension (MRC) - answering comprehension questions over a passage of text, has a long history, and was first studied in 1970s~\cite{lehnert1977process}. Traditional methods used manually generated rules, or feature-based methods~\cite{chen2016thorough}. Recently, the field has witnessed great success, with the emergence and development of neural (deep-learning) reading comprehension models, especially the large pre-trained language models like BERT~\cite{devlin2019bert}, XLNET~\cite{yang2019xlnet}, and the creation of large-scale supervised datasets in the form of (passage, question, answer)
triples such as SQuAD~\cite{rajpurkar2016squad}. The dominant way is to fine-tune the large pre-trained language models by MRC datasets. Most recently, ~\cite{lovenia2022clozer} proposed Clozer, a sequence-tagging based cloze answer extraction method used to extende task adaptive pretraining (TAPT)~\cite{zhang2021hybrid} for adaptation on cloze-style machine reading comprehension. 

The pre-trained language model normally takes the concatenation of the passage $D$ and question $Q$ as input, and outputs the prediction of the start and end position of the potential evidence spans in the document $D$. Specifically, it outputs two probability distributions over the tokens in $D$: $P^s(i)$ and $P^e(i)$, where $P^s(i)$ / $P^e(i)$ is the probability that the $i$-th token is the start/end of the evidence span in $D$. The score of a candidate span from
position $i$ to position $j$ is defined as $P_i + P_j$, and the maximum scoring span where $j >= i$ is used as a prediction. The performance of the state-of-the-art model on the task has already surpass human~\cite{devlin2019bert,yang2019xlnet}.

\section{Related Work}

\subsection{Open-domain QA} 
Open-domain QA is the task of answering general-domain questions~\cite{chen2017reading}, where the evidence is usually not given. Models that explicitly exploit an external corpus are referred to as \textit{open-book} models~\cite{roberts2020much}. They typically index the corpus and then \textit{retrieve-and-read} to extract the answer span from documents~\cite{chen2017reading, lee2019latent, izacard2021leveraging, rag, lazaridou2022internet}. Another recently proposed class of methods is \textit{closed-book} QA models. ~\citet{ye2020studying,roberts2020much} finetune pretrained LMs such as T5~\cite{raffel2020exploring} or BART~\cite{lewis2020bart} with QA pairs without access to any external knowledge or context.

\subsection{Machine Reading Comprehension}
The goal of a Machine Reading Comprehension (MRC) system is to answer a question by reading one or more context documents. Many work has been done for MRC in recent years, fueled by the creation of many large-scale datasets~\cite{rajpurkar2016squad, lai2017race, saha2018duorc, trischler2017newsqa, Joshi_2017}. 

We have witnessed several breakthroughs in MRC models, such as bidirectional attention flow (BiDAF)~\cite{seo2017bidirectional}, the attention over attention mechanism (AoA)  ~\cite{cui2017attention}, and a multi-hop architecture using gated-attention readers ~\cite{dhingra2017gated}. Recently, fine-tuning pre-trained language models~\cite{devlin2019bert, yang2019xlnet} via supervised learning has achieved the state-of-the-art performance on many MRC datasets. 

While most previous methods focus on improving in-domain performance, ~\citet{su-etal-2019-generalizing} propose a multi-task learning framework that learns the shared representation across different tasks, to build a QA system which has general linguistic intelligence, so that the model can generalize to out-of-domain and unseen tasks.


\subsection{Query-focused Summarization}
Query-focused Summarization (QFS) aims to generate a summary according to the query and the provided relevant document(s)~\cite{tombros1998advantages}. The input can be either a \textit{single} document that has multiple views or \textit{multiple} documents that contain multiple topics, and the output summary should be focused on the given query. QFS has various applications (e.g., a personalized search engine that provides the user with an overview summary based on their query~\cite{su2020caire}).

Early work on the QFS task mainly focused on generating extractive summaries~\cite{davis2012occams, daume2006bayesian, feigenblat2017unsupervised, xu2020coarse}, which may contain unreadable sentence ordering and lack cohesiveness. Other work on abstractive QFS incorporated the query relevance into existing neural summarization models~\cite{nema2017diversity,baumel2018query}. \citet{nema2017diversity} proposed an encode-attend-decode system with an additional query attention mechanism and diversity-based attention mechanism to generate a more query-relevant summary. \citet{baumel2018query} incorporated query relevance into a pre-trained abstractive summarizer to make the model aware of the query. While~\citet{xu2020abstractive} discovered a new type of connection between generic summaries and QFS queries, and provided a universal representation for them which allows \textit{generic} summarization data to be further exploited for QFS. ~\citet{su2020caire}, meanwhile, built a query model for paragraph selection based on the answer relevance score and iteratively summarized paragraphs to a budget. However, they only used QA as distant supervision to retrieve relevant segments for generating the summary, but did not take into consideration the answer relevance in the generation model. 

\subsection{Few-shot LM Prompting} 

Language model prompting method has become a new paradigm of utilizing LM for many NLP tasks. Traditional \textit{pretrain then fine-tune} way of leveraging the LM, aims to train a model $P(y\mid x;\theta)$, where a large amount of supervised data is required. While LM prompting can help circumvent this issue by instead learning an LM that models the probability $P(x;\theta)$ of text $x$ itself and using this probability to predict $y$, reducing or obviating the need for large supervised datasets.

\citet{radford2019language, brown2020language} prompt GPT-2~\cite{radford2019language} and GPT-3~\cite{brown2020language} conditioned on several few-shot examples to predict the answer for ODQA. They constructed the prompt $Prompt(Q)$ for a given question $Q$ as:
\begin{align*}
    Prompt(Q) =& \texttt{Q:} q_m \backslash n \texttt{A:} a_m \backslash n \ldots \\ &\texttt{Q:} q_1 \backslash n \texttt{A:} a_1 \backslash n \texttt{Q:} Q \backslash n
\end{align*}

where the $\{(q_1,a_1),...(q_m,a_m)\}$ are examples selected from corresponding training datsets for question $Q$. Then pass $Prompt(Q)$ through a pretrained LM such as GPT-3, to generate the answer for question $Q$ as follows: 

\begin{equation*}
    a = \mathcal{LM}(Prompt(Q))
\end{equation*}


Most recent work by ~\citet{lazaridou2022internet} further empower LM's few-shot prompting abilities with information returned from the web using Google-Search API, and experimented on QA task. Their prompt format is:
\begin{align*}
    Evidence: ...
    Question: ...
    Answer: ...
\end{align*}

While ~\citet{wei2022chain, wang2022self} use \textit{chain of thought} few-shot prompting of LM to generate a coherent chain of short sentences that minic the reasoning process of human might employ to solve reasoning tasks. 
