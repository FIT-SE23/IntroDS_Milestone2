\chapter{Introduction}


%%% maybe a paragraph


\section{Open-domain Question Answering}

Open domain question answering (ODQA) has been a long-standing problem in the history of NLP~\cite{simmons1964indexing,kupiec1993murax,voorhees1999trec,moldovan2000structure, harabagiu2000experiments, brill2002analysis, ferrucci2010building, baudivs2015yodaqa, lin2003question,gliozzo2012natural, yih2016question, sachan2018standardized, chen2017reading}. The goal of open domain question answering is to build automated computer systems which are able to answer any sort of (factoid) questions that humans might ask, based on a large collection of unstructured natural language documents, structured data, semi-structured data or even other modalities such as images or videos (as illustrated in Figure~\ref{fig:intro_odqa}). 
\begin{figure}[!ht]
  \centering
  \includegraphics[width=0.9\linewidth]{figures/intro/intro_odqa.pdf}
  \caption{A Open-domain question answering system illustration. The text-based QA is the most extensively studied ODQA task.}
  \label{fig:intro_odqa}
\end{figure}

\textbf{Text-based QA} The most extensively studied ODQA task is \textit{text-based} QA, which derives an answer from unstructured natural language documents (e.g., encyclopedias, dictionaries, news articles, and general Web documents). 
Dating back to the 1960s~\cite{simmons1964indexing}, the text-based open-domain question answering(QA) has been broadly investigated by both the information retrieval (IR) and natural language processing (NLP) communities  ~\cite{simmons1964indexing,kupiec1993murax,voorhees1999trec,moldovan2000structure, harabagiu2000experiments, brill2002analysis, ferrucci2010building, baudivs2015yodaqa, lin2003question,gliozzo2012natural, yih2016question, sachan2018standardized, chen2017reading, su2020improving}. Notable text-based QA systems include Microsoft’s ASKMSR~\cite{brill2002analysis}, IBM’s DEEPQA~\cite{ferrucci2010building}, and DrQA~\cite{chen2017reading}. 

\textbf{Extractive open-domain QA}  Almost all of the previous \textit{text-based} ODQA systems are \textbf{extractive}-based~\cite{chen2017reading,izacard2021leveraging, lewis2021paq}, which means they extract a short and concise span from a retrieved document as the answer, as the example in Figure~\ref{fig:intro_extractive_qa} illustrated. Normally the extractive ODQA can provide factoid information for simple straightforward, factual queries such as \textit{Which artist sings this song?} , \textit{Who is the president of the US?}, or \textit{Where is HKUST located?}.

\begin{figure}[!ht]
  \centering
  \includegraphics[width=0.9\linewidth]{figures/intro/intro_extractive_QA.png}
  \caption{An extractive, text-based open-domain QA system illustration.}
  \label{fig:intro_extractive_qa}
\end{figure}

\section{Generative Long-form Question Answering}
While the \textbf{extractive} open-domain QA task have been extensively studied in prior years, the \textbf{generative} open-domain QA that tries to generate an answer for a given question obtains little attention. 
To fill the blank and further push the field forward, the more challenging task, long-form question answering (LFQA) was recently proposed~\cite{fan2019eli5}. 

LFQA aims to \textbf{generate} an in-depth, paragraph-length answer for a given question. The questions are more complicated such as the \textit{why/how} type questions, that requires in-depth explanations. They can’t be directly answered by short-phrase span or sentences from existing documents. Instead, they normally requires to synthesize information from multiple web documents comprising hundreds of thousands of words, identifying the relevant information in those documents, and using world knowledge to answer. We show an LFQA examples in Table~\ref{tab:intro_lfqa_example}. As we can see, it require multiple sentences to explain the answer for the given question, and the long-form answers are generated from multiple documents and using world knowledge.

\begin{table}[htb!]
    \centering
    \begin{adjustbox}{width={0.9\textwidth},totalheight={\textheight},keepaspectratio}
\begin{tabular}{p{1\columnwidth}}
    \hline
    % \hline
% \textbf{Question:} Which  French ace pilot and adventurer fly L'Oiseau Blanc?  \\
% \textbf{Short answer}: Charles Eugène \\
% \textbf{Long answer}:  On May 8, 1927, \textbf{Charles Nungesser} and Francois Coli boarded L'Oiseau blanc, a 450-hp Lorraine-powered Levasseur biplane designed to compete for the Orteig Prize. They took off from Paris on 8 May 1927 and were last seen over Ireland. Less than two weeks later, Charles Lindbergh successfully made the New York-Paris journey and claimed the prize in the Spirit of St. Louis. \\
% \hline
\textbf{Question:} How do Jellyfish function without brains or nervous systems? \\
\textbf{Answer:} Jellyfish may not have a brain, but they have a rough
nervous system and innate behaviours. However, they are
very simple creatures. They’re invertebrate: creatures without
a backbone. Most jellyfish have really short life spans.
Sometimes just a couple of hours. [...] As their name implies,
they are largely composed of basically jelly inside a
thin membrane. They’re over 95\% water. (327 words) \\
    \hline
    % \hline
    \end{tabular}
    % }
\end{adjustbox}
\caption{Generative long-form question answering example. The example is from the “Explain Like I’m Five” (ELI5)~\cite{fan2019eli5}.}
\label{tab:intro_lfqa_example}
\end{table}

%  The first question is from the HotpotQA dataset~\cite{yang2018HotpotQA}, seeking information for a multi-hop question. As we can see, the extractive QA system tries to extract a short precise answer, the generative-based LFQA aims to generate a long answer. The multi-sentence long answer gives a detailed explanation, which could be used to understand and verify the short answer. 

\section{Motivation}
% Building automated computer systems to answer questions asked by humans in natural language is one of the most elusive and long-standing challenges in the history of artificial intelligence. While we are thrilled at the achievements the community has made~\cite{brill2002analysis, ferrucci2010building, chen2017reading}, the current textual-based ODQA systems have limitations. Almost all of the previous ODQA systems are \textbf{extractive}-based, they can only provide factoid information for simple queries, such as the \textit{who/when/where} type questions, and are incompetent at answering non-trivial questions such as \textit{How do jellyfish function without a brain?}, \textit{Why wouldn't life on another habitable planet look similar to Earth's?}, or \textit{What are the risking factors related to COVID-19?}. However, a large ratio of the questions that humans pose to search engines every day are those complicated types, and require multi-sentence, in-depth explanations. Therefore, it motivates us to work on the newly-proposed, more challenging LFQA task.


Building automated computer systems to answer questions asked by humans in natural language is one of the most elusive and long-standing challenges in the history of artificial intelligence. While we are thrilled at the achievements the community has made~\cite{brill2002analysis, ferrucci2010building, chen2017reading}, the current textual-based ODQA systems can not answer non-trivial questions such as the \textit{why/how} type ones. This is because almost all of the previous ODQA systems are \textbf{extractive}-based, which will extract short-span based phrases from retrieved document as the answer, so they can only provide factoid information for simple queries, such as the \textit{who/when/where} type questions. However, a large ratio of the questions that humans pose to search engines every day are those complicated types, such as \textit{How do jellyfish function without a brain?}, \textit{Why wouldn't life on another habitable planet look similar to Earth's?}, or \textit{What are the risking factors related to COVID-19?}, there is no short answer and require multi-sentence, in-depth explanations. Therefore, it motivates us to work on the newly-proposed, more challenging LFQA task.

LFQA have many possible applications. It could be integrated to facilitate a more intelligent search engine ~\cite{metzler2021rethinking}. For example, if we search Google with a query, it could not only returns a list of search documents but also attempts to read these Web documents and even display some generated answers at the top of the search results. Additionally, LFQA could play an important role in digital personal assistants systems such as Amazon's ALEXA, Apple's SIRI, GOOGLE ASSISTANT, to enhance user experience. As it has been shown\footnote{\tt \small https://voicebot.ai/2019/03/12/smart-speaker-owners-agree-that-questions\\-music-and-weather-are-killer-apps-what-comes-next/smart-speaker-use-case-\\frequency-january-2019-01/}, \textit{'asking a question'} is the most frequently used functions from those smart speakers. Specifically, it might be used to generate explanations as a complement to short phrase answers for factoid information-seeking open-domain questions~\cite{kwiatkowski2019natural, yang2018hotpotqa}, or to answer open-ended questions like the ones from the Reddit forum "Explain Like I'm Five"~\cite{fan2019eli5}. 

\begin{table}[!ht]
\centering
\begin{adjustbox}{width={0.99\textwidth},totalheight={0.9\textheight},keepaspectratio}
\begin{tabular}{p{1\columnwidth}}
\hline
\textbf{Question:} what does abc stand for in liquor store? \\
\textbf{Generated Answer:} \hlgreen{\textbf{A liquor store} is a retail shop that predominantly sells prepackaged alcoholic beverages intended to be consumed off the store's premises. Depending on region and local idiom (social issue), they may also be called bottle store, off licence, (in United Kingdom, Ireland, and New Zealand) bottle shop, bottle-o package store, party store (in Michigan), ABC store, state store, or other similar terms.}\\
\textbf{Golden Answer:} Most of these states have an \textbf{Alcoholic Beverage Control (ABC)} board and run liquor stores called ABC stores. In all monopoly states a parallel license system is used to regulate the sale and distribution of lighter alcoholic beverages such as beer and wine.\\
\hline
\end{tabular}
\end{adjustbox}
\caption{Example from the MSMARCO dataset~\cite{nguyen2016ms}. The generated answer gives a detailed definition and introduction of 'liquor store', however, it doesn't answer the representation of 'abc'. Therefore, the answer lacks of \textbf{relevance} to the given question. We highlight in green the sentences \hlgreen{irrelevant} to the question.}
\label{tab:intro_challenge_examples_1}
\end{table}

\begin{table}[!ht]
\centering
\begin{adjustbox}{width={0.99\textwidth},totalheight={0.9\textheight},keepaspectratio}
\begin{tabular}{p{1\columnwidth}}
\hline
\textbf{Question:} definition of sadduction \\
\textbf{Generated Answer:} Sadducees. noun. \hlred{The definition of a sadducee is a person who acts in a deceitful or duplicitous manner.  An example of a Sadduceee is a politician who acts deceitfully in order to gain political power}. 1 a member of a Jewish sect that was active during the Second Temple. \\
\textbf{Golden Answer:} The Sadducees(Hebrew: \d{S}ĕdûqîm) were a sect or group of Jews that were active in Judea during the Second Temple period, starting from the second century BCE through the destruction of the Temple in 70 CE. The sect was identified by Josephus with the upper social and economic ecoshon of Judean society.\\
 \hline
\end{tabular}
\end{adjustbox}
\caption{Example from the MSMARCO dataset~\cite{nguyen2016ms}. The answer is hallucinated and lacks of \textbf{faithfulness}. We mark in red the \hlred{unfaithful} snippets.}
\label{tab:intro_challenge_examples_2}
\end{table}

\begin{table}[!ht]
    \centering
    \begin{adjustbox}{width={0.92\textwidth},totalheight={0.9\textheight},keepaspectratio}
\begin{tabular}{p{1\columnwidth}}
    \hline
\textbf{Question:} Which  French ace pilot and adventurer fly L'Oiseau Blanc?  \\
\textbf{Short answer}: Charles Eugène \\
\textbf{Long answer}:  On May 8, 1927, \textbf{Charles Nungesser} and Francois Coli boarded L'Oiseau blanc, a 450-hp Lorraine-powered Levasseur biplane designed to compete for the Orteig Prize. They took off from Paris on 8 May 1927 and were last seen over Ireland. Less than two weeks later, Charles Lindbergh successfully made the New York-Paris journey and claimed the prize in the Spirit of St. Louis. \\
\hline
    \end{tabular}
    % }
\end{adjustbox}
\caption{The generated long-form answer provides context information and can be used to extract the short answer. The example is from the “Explain Like I’m Five” (ELI5)~\cite{fan2019eli5}.}
\label{tab:intro_challenge_example_3}
\end{table}

\section{Research Directions}

LFQA is a newly proposed, more challenging task compared to the traditional extractive-based open-domain QA, and little work has yet been done on it.

In this thesis, we study methods to build and improve a LFQA system. Specifically, we focus on:
\begin{itemize}
    \item \textbf{Generating query-relevant long-form answers} In recent years, we have witnessed rapid progress in dense retrieval techniques like REALM~\cite{guu2020realm}, DPR~\cite{karpukhin2020dense} and ORQA~\cite{lee2019latent}. Also, the large pre-trained language models~\cite{lewis2020bart,raffel2020exploring} have demonstrated its superior performances at many generation tasks such as summerization~\cite{su2021improve,yu2020dimsum}, question generation~\cite{su2022qa4qg,su2020multi}. However, few work have been investigated to build an effective real-time LFQA system. Since LFQA is quite a challenging task that requires more than just concatenating the two techniques. Firstly, it needs to handle the issue of applying pre-trained language model which has limited input sequence length, to process the multiple relevant documents containing more than thousand of tokens for generation. Secondly, those multiple retrieved documents may include irrelevant, complementary or contradictory information, how to generate more query-relevant, long-form answers is another challenge (we show an example in Table~\ref{tab:intro_challenge_examples_1} to provide a better understanding). Therefore, we first propose CAiRE-COVID, a real-time long form question answering system for COVID-19, which combines information retrieval with state-of-the-art QA and query-focused multi-document summarization techniques, to answer high priority COVID-19 related questions. Then, we introduce QFS-BART, a model that incorporates the explicit answer relevance of the source documents given a query into the answer generation model, to generate more query-relevant answers.
    
    % Fluency is the first important evaluation element in many generation tasks, such as abstractive summarization~\cite{su2021improve}, text generation~\cite{su2020multi,su2022read}. It measures the linguistic quality of the generated text. If a long-form answer that is not fluent, such as containing repetitions or grammatical incorrect statements, it will harm the answer contents greatly. 
    
    % Furthermore, the generated answer should contain information that is relevant to the topic of the question. We show an example in Table~\ref{tab:intro_challenge_examples_1} to provide a better understanding. The generated answer gives a detailed definition and introduction of 'liquor store' in a fluent way, however, it lacks \textbf{query-relevance} since the information does not answer the question: \textit{'What does abc stand for?'}. 

    % Therefore, how to generate fluent, query-relevant long-form answers is an important task to build an effective LFQA system.

    \item \textbf{Generating fact-aware long-form answers} Improving the fact correctness of the generated answer is one of the other main challenges for LFQA task~\cite{krishna2021hurdles, lin2021truthfulqa, nakano2021webgpt}. Specifically, the generated answer should be faithful to facts in the souce documents~\cite{ji2022survey}. In the example in Table~\ref{tab:intro_challenge_examples_2}, the answer is fluent, and also relevant to the query when trying to elaborate the \textit{'definition of sadduction'}. However, the content itself is hallucinated and lacks \textbf{faithfulness}. Such a fluent and relevant but unfaithful answer will mislead the user. Therefore, we propose an end-to-end framework named RBG, to augment the generation model with fine-grained, answer-related salient information predicted by an machine reading comphrehension(MRC) module, to improve and generate fact-aware answer that is more faithful to the source documents.
    
    \item \textbf{Leveraging informative long-form answers}
    % As illustrated in Figure~\ref{fig:intro_extractive_qa}, a traditional open-domain QA system normally retrieves multiple relevant contexts from an external knowledge corpus, then it extracts a short answer span from the retrieved contexts as the answer. While the generative long-form answers contains informative statements relevant to a given question, it might be leveraged as replacement of the retrieved contexts for extractive ODQA task (An concrete example is shown in~\ref{tab:intro_challenge_example_3}). No prior work has been investigated on this. 
    \textit{Closed-book} question answering (QA) requires a model to directly answer an open-domain question without access to any external knowledge~\cite{roberts2020much}. While it has been shown that large pretrained LMs store abundant knowledge~\cite{petroni2019language,roberts2020much}, prior work do not fully exploit the stored knowledge to answer the question~\cite{roberts2020much, ye2020studying,brown2020language, radford2019language}. Therefore, to address this issue and further improve the accuracy of \textit{closed-book} QA model, we propose a simple yet effective framework named \textbf{CGAP} for open-domain QA, by leveraging the generated long-form answers as the intermediate bridge between the huge amount of parameterized knowledge and short answer (we show an example in Table~\ref{tab:intro_challenge_example_3}). CGAP performs \textbf{C}ontext \textbf{G}eneration followed by \textbf{A}nswer \textbf{P}rediction via two-stage prompting using large pretrained LMs. Experimental results on three QA benchmarks show that our method significantly outperforms previous \textit{closed-book} QA methods and is par with traditional \textit{open-book} methods that  exploit external knowledge source.

\end{itemize}

\section{Thesis Outline}
The thesis is divided in four main chapters, plus a conclusion. It is organized as:
\begin{itemize}
    \item Chapter 2 first gives an overview of open-domain question answering. Then we will introduce the preliminaries needed throughout the thesis for generative QA, followed by the related work to LFQA. 
    \item Chapter 3 have two parts. The first part presents CAiRE-COVID. A real-time LFQA system which can generate multiple-documents-based answers efficiently. The system demonstrates its effectiveness at generating fluent and somehow relevant answers, winning one of the Kaggle competitions related to COVID-19. The second part presents QFS-BART, a model that incorporates the explicit answer relevance of the source documents given a query, via a MRC model, into the BART model, to generate coherent and more query-relevant answer. 
    \item Chapter 4 presents RBG, a new end-to-end framework that jointly models answer generation and machine reading to augment the generation model with fine-grained, answer-relevant salient information from each retrieved documents, as an emphasis on faithful facts, to improve the answer faithfulness. 
    \item Chapter 5 introduces CGAP, a first of its kind system that leverage the generated long-form answers as the context, for open-domain question answering.
\end{itemize}


% State-of-the-art results on two LFQA datasets demonstrate the effectiveness of our method, in comparison with strong baselines on automatic and human evaluation metrics. The method also topped one public leaderboard on the LFQA task.





