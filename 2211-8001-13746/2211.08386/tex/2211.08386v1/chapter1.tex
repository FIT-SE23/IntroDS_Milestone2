
\chapter{Introduction}

% \section{Open-domain Question Answering}
Open domain question answering (ODQA) has been a long-standing problem in the history of NLP~\cite{simmons1964indexing,kupiec1993murax,voorhees1999trec,moldovan2000structure, harabagiu2000experiments, brill2002analysis, ferrucci2010building, baudivs2015yodaqa, lin2003question,gliozzo2012natural, yih2016question, sachan2018standardized, chen2017reading}. The goal of open domain question answering is to build automated computer systems which are able to answer any sort of (factoid) questions that humans might ask, based on a large collection of unstructured natural language documents, structured data, semi-structured data or even other modalities such as images or videos (as illustrated in Figure~\ref{fig:intro_odqa}). 
\begin{figure}[!ht]
  \centering
  \includegraphics[width=0.9\linewidth]{figures/intro/intro_odqa.png}
  \caption{A Open-domain question answering system illustration. The text-based QA is the most extensively studied ODQA task.}
  \label{fig:intro_odqa}
\end{figure}

% \textbf{Text-based QA} 
% The most extensively studied ODQA task is \textit{text-based} QA, which derives an answer from unstructured natural language documents (e.g., encyclopedias, dictionaries, news articles, and general Web documents). 
% Dating back to the 1960s~\cite{simmons1964indexing}, the text-based open-domain question answering(QA) has been broadly investigated by both the information retrieval (IR) and natural language processing (NLP) communities  ~\cite{simmons1964indexing,kupiec1993murax,voorhees1999trec,moldovan2000structure, harabagiu2000experiments, brill2002analysis, ferrucci2010building, baudivs2015yodaqa, lin2003question,gliozzo2012natural, yih2016question, sachan2018standardized, chen2017reading, su2020improving}. Notable text-based QA systems include Microsoft’s ASKMSR~\cite{brill2002analysis}, IBM’s DEEPQA~\cite{ferrucci2010building}, and DrQA~\cite{chen2017reading}. 

% \textbf{Extractive open-domain QA} 
% The most extensively studied ODQA task is \textit{text-based} QA, which derives an answer from unstructured natural language documents (e.g., encyclopedias, dictionaries, news articles, and general Web documents). 
Previous work on ODQA are mainly \textbf{extractive}-based~\cite{chen2017reading,izacard2021leveraging, lewis2021paq}. The system answers the question by extracting a short and concise span from a retrieved document as the answer, as the example in Figure~\ref{fig:intro_extractive_qa} illustrated. The extractive-based ODQA system can provide factoid information for simple straightforward, factual queries such as \textit{Which artist sings this song?} , \textit{Who is the president of the US?}, or \textit{Where is HKUST located?}.

\begin{figure}[!ht]
  \centering
  \includegraphics[width=0.9\linewidth]{figures/intro/intro_extractive_QA.png}
  \caption{An extractive-based open-domain QA system illustration.}
  \label{fig:intro_extractive_qa}
\end{figure}

% However, a large ratio of the questions that humans pose to search engines every day are more complicated, such as \textit{How do jellyfish function without a brain?}, \textit{What are the risking factors related to COVID-19?}. Those questions require multi-sentence, in-depth explanations to answer. A short phrases span extracted from existing document by the \textbf{extractive-based} system are incompetent at answering those questions. Therefore, 
Long-form question answering (LFQA), a more challenging task, was recently proposed~\cite{fan2019eli5}. LFQA aims to \textbf{generate} an in-depth, paragraph-length answer for a given question. The questions are more complicated such as the \textit{why/how} type questions, which cannot be directly answered by short-phrase span or sentences from existing documents. Instead, they usually require synthesizing information from multiple web documents comprising hundreds of thousands of words, identifying the relevant information in those documents, and using world knowledge to answer. We show an LFQA examples in Table~\ref{tab:intro_lfqa_example}. 



% While the \textbf{extractive} open-domain QA task have been extensively studied in prior years, the \textbf{generative} open-domain QA that tries to generate an answer for a given question obtains little attention. 

% \section{Generative Long-form Question Answering}


% As we can see, it requires multiple sentences to explain the answer for the given question, and the long-form answers are generated from multiple documents and using world knowledge.

\begin{table}[htb!]
    \centering
    \begin{adjustbox}{width={0.9\textwidth},totalheight={\textheight},keepaspectratio}
\begin{tabular}{p{1\columnwidth}}
    \hline
    % \hline
% \textbf{Question:} Which  French ace pilot and adventurer fly L'Oiseau Blanc?  \\
% \textbf{Short answer}: Charles Eugène \\
% \textbf{Long answer}:  On May 8, 1927, \textbf{Charles Nungesser} and Francois Coli boarded L'Oiseau blanc, a 450-hp Lorraine-powered Levasseur biplane designed to compete for the Orteig Prize. They took off from Paris on 8 May 1927 and were last seen over Ireland. Less than two weeks later, Charles Lindbergh successfully made the New York-Paris journey and claimed the prize in the Spirit of St. Louis. \\
% \hline
\textbf{Question:} How do Jellyfish function without brains or nervous systems? \\
\textbf{Answer:} Jellyfish may not have a brain, but they have a rough
nervous system and innate behaviours. However, they are
very simple creatures. They’re invertebrate: creatures without
a backbone. Most jellyfish have really short life spans.
Sometimes just a couple of hours. [...] As their name implies,
they are largely composed of basically jelly inside a
thin membrane. They’re over 95\% water. (327 words) \\
    \hline
    % \hline
    \end{tabular}
    % }
\end{adjustbox}
\caption{Generative long-form question answering example. The example is from the “Explain Like I’m Five” (ELI5)~\cite{fan2019eli5}.}
\label{tab:intro_lfqa_example}
\end{table}

%  The first question is from the HotpotQA dataset~\cite{yang2018HotpotQA}, seeking information for a multi-hop question. As we can see, the extractive QA system tries to extract a short precise answer, the generative-based LFQA aims to generate a long answer. The multi-sentence long answer gives a detailed explanation, which could be used to understand and verify the short answer. 

\section{Motivation}
% Building automated computer systems to answer questions asked by humans in natural language is one of the most elusive and long-standing challenges in the history of artificial intelligence. While we are thrilled at the achievements the community has made~\cite{brill2002analysis, ferrucci2010building, chen2017reading}, the current textual-based ODQA systems have limitations. Almost all of the previous ODQA systems are \textbf{extractive}-based, they can only provide factoid information for simple queries, such as the \textit{who/when/where} type questions, and are incompetent at answering non-trivial questions such as \textit{How do jellyfish function without a brain?}, \textit{Why wouldn't life on another habitable planet look similar to Earth's?}, or \textit{What are the risking factors related to COVID-19?}. However, a large ratio of the questions that humans pose to search engines every day are those complicated types, and require multi-sentence, in-depth explanations. Therefore, it motivates us to work on the newly-proposed, more challenging LFQA task.

Building automated computer systems to answer questions humans ask in natural language is one of the most elusive and long-standing challenges in the history of artificial intelligence. The questions that human tends to ask everyday involves straightforward, factual questions such as “Which artist sings this song?”, a large ratio of the questions are those complicated types, such as \textit{How do jellyfish function without a brain?}, \textit{Why would not life on another habitable planet look similar to Earth's?}, or \textit{What are the risking factors related to COVID-19?}.

While we are thrilled at the achievements the community has made~\cite{brill2002analysis, ferrucci2010building, chen2017reading}, the current extractive-based ODQA systems can not answer non-trivial questions such as the \textit{why/how} type ones. The reasons can be two folds: firstly, there is no short answer for the \textit{why/how} type questions and they require multi-sentence, in-depth explanations; secondly, the answers usually need to be generated by synthesizing information from multiple documents, and a short phrase span extracted from a single existing document by the extractive-based ODQA model cannot answer those complicated questions. Therefore, it motivates us to work on the newly-proposed, more challenging LFQA task.

LFQA has many possible applications. It could be integrated to facilitate a more intelligent search engine ~\cite{metzler2021rethinking}. For example, if we search Google with a query, it could not only returns a list of search documents but also attempts to read these Web documents and even display some generated answers at the top of the search results. Additionally, LFQA could play an important role in digital personal assistants systems such as Amazon's ALEXA, Apple's SIRI, and GOOGLE ASSISTANT to enhance user experience. As it has been shown\footnote{\tt \small https://voicebot.ai/2019/03/12/smart-speaker-owners-agree-that-questions\\-music-and-weather-are-killer-apps-what-comes-next/smart-speaker-use-case-\\frequency-january-2019-01/}, \textit{'asking a question'} is the most frequently used functions from those smart speakers. Specifically, it might be used to generate explanations as a complement to short phrase answers for factoid information-seeking open-domain questions~\cite{kwiatkowski2019natural, yang2018hotpotqa}, or to answer open-ended questions like the ones from the Reddit forum "Explain Like I'm Five"~\cite{fan2019eli5}. 

\begin{table}[!ht]
\centering
\begin{adjustbox}{width={0.99\textwidth},totalheight={0.9\textheight},keepaspectratio}
\begin{tabular}{p{1\columnwidth}}
\hline
\textbf{Question:} what does abc stand for in liquor store? \\
\textbf{Generated Answer:} \hlgreen{\textbf{A liquor store} is a retail shop that predominantly sells prepackaged alcoholic beverages intended to be consumed off the store's premises. Depending on region and local idiom (social issue), they may also be called bottle store, off licence, (in United Kingdom, Ireland, and New Zealand) bottle shop, bottle-o package store, party store (in Michigan), ABC store, state store, or other similar terms.}\\
% \textbf{RBG:} ABC stands for Alcoholic Beverages Control (ABC Stores) and is a type of liquor store that is owned and operated exclusively by the state government, where liquor
% stores often sell only spirits or sometimes sell spirits and wine but not beer.\\
\textbf{Golden Answer:} Most of these states have an \textbf{Alcoholic Beverage Control (ABC)} board and run liquor stores called ABC stores. In all monopoly states a parallel license system is used to regulate the sale and distribution of lighter alcoholic beverages such as beer and wine.\\
\hline
\end{tabular}
\end{adjustbox}
\caption{Example from the MSMARCO dataset~\cite{nguyen2016ms}. The generated answer gives a detailed definition and introduction of 'liquor store', however, it doesn't answer the representation of 'abc'. Therefore, the answer lacks of \textbf{relevance} to the given question. We highlight in green the sentences \hlgreen{irrelevant} to the question.}
\label{tab:intro_challenge_examples_1}
\end{table}

\begin{table}[!ht]
\centering
\begin{adjustbox}{width={0.99\textwidth},totalheight={0.9\textheight},keepaspectratio}
\begin{tabular}{p{1\columnwidth}}
\hline
\textbf{Question:} definition of sadduction \\
\textbf{Generated Answer:} Sadducees. noun. \hlred{The definition of a sadducee is a person who acts in a deceitful or duplicitous manner.  An example of a Sadduceee is a politician who acts deceitfully in order to gain political power}. 1 a member of a Jewish sect that was active during the Second Temple. \\
% \textbf{RBG:} The Sadducees were a sect or group of Jews that were active in Judea during the Second Temple period, starting from the second century BCE through the destruction of the Temple in 70 CE. The sect was identified by Josephus with the upper social and economic ecoshon of Judean society. \\
\textbf{Golden Answer:} The Sadducees (Hebrew: \d{S}ĕdûqîm) were a sect or group of Jews that were active in Judea during the Second Temple period, starting from the second century BCE through the destruction of the Temple in 70 CE. The sect was identified by Josephus with the upper social and economic ecoshon of Judean society.\\
 \hline
\end{tabular}
\end{adjustbox}
\caption{Example from the MSMARCO dataset~\cite{nguyen2016ms}. The answer is hallucinated and lacks of \textbf{faithfulness}. We mark in red the \hlred{unfaithful} snippets.}
\label{tab:intro_challenge_examples_2}
\end{table}

\begin{table}[!ht]
    \centering
    \begin{adjustbox}{width={0.92\textwidth},totalheight={0.9\textheight},keepaspectratio}
\begin{tabular}{p{1\columnwidth}}
    \hline
\textbf{Question:} Which  French ace pilot and adventurer fly L'Oiseau Blanc?  \\
\textbf{Long answer}:  On May 8, 1927, \hlorange{\textbf{Charles Nungesser}} and Francois Coli boarded L'Oiseau blanc, a 450-hp Lorraine-powered Levasseur biplane designed to compete for the Orteig Prize. They took off from Paris on 8 May 1927 and were last seen over Ireland. Less than two weeks later, Charles Lindbergh successfully made the New York-Paris journey and claimed the prize in the Spirit of St. Louis. \\
\textbf{Short answer}: Charles Eugène \\
\hline
    \end{tabular}
    % }
\end{adjustbox}
\caption{For question types such as \textit{who/which/when/where}, the short answer that is more succinct, might be more preferred. The example is from the HotpotQA dataset~\cite{yang2018hotpotqa}}
\label{tab:intro_challenge_example_3}
\end{table}



\section{Problem Setup}

The task of LFQA can be formulated as: given a collection of training examples ${(q_i,a_i)}_{i=1}^n$, the goal is to learn a predictor $\mathcal{F}$, which takes the question $q$ as input and output the answer $a$.

\begin{equation}
    \mathcal{F}: q->a \\
\end{equation}

$a=\left\{a_{1},a_{2,}...,a_{l_{a}}\right\},\,\text{where}\;\,a_{i}\in\mathbb{V}\,\text{for}\;i=1,...,l_{a}, q=\left\{q_{1},q_{2},...,q_{l_{q}}\right\}\,\text{where}\;q_{i}\,\in\,\mathbb{V}\,\text{for}\;i=1,...l_{q}$

Usually we will also find some supporting information from large knowledge source, such as Wikipedia. But the support information is not provided in the training examples.
\begin{equation}
    \mathcal{F}: (q, \mathcal{D})->a \\
\end{equation}

In this thesis, we will mainly focus on answering the questions $q$ based on information from the external knowledge corpus $\mathcal{D}$, and we will not answer questions that is unanswerable or 'out-of-domain' of supporting document $\mathcal{D}$.

The evaluation metrics include (1) Automatic metrics, such as ROUGE, which calculate the overlap of ngrams between the generated and reference answers (2) Human evaluation, which normally evaluate the fluency, relevance, and factual correctness of the generated answer.

\section{Research Directions}

LFQA is a newly proposed, more challenging task compared to the traditional extractive-based open-domain QA, and little work has yet been done on it. In this thesis, we focus on the three corresponding research directions: 1) generating query-relevant long-form answers 2) generating fact-aware long-form answers 3) generating succinct answers from long-form answers. We show an simple but concrete illustration of the three aspects in Figure~\ref{fig:intro_concept}.

\begin{figure}[!ht]
  \centering
  \includegraphics[width=0.9\linewidth]{figures/intro/thesis_concepts.png}
  \caption{Illustration of 1) query-relevance, 2) fact-correctness and 3) succinctness, using similar information retrieval metrics.}
  \label{fig:intro_concept}
\end{figure}

In this thesis, we study methods to build and improve a LFQA system based on the three aspects. Specifically, we focus on:

\begin{itemize}
    \item \textbf{Generating query-relevant long-form answers} In recent years, we have witnessed rapid progress in dense retrieval techniques like REALM~\cite{guu2020realm}, DPR~\cite{karpukhin2020dense} and ORQA~\cite{lee2019latent}. Also, the large pre-trained language models~\cite{lewis2020bart,raffel2020exploring} have demonstrated its superior performances at many generation tasks such as summerization~\cite{su2021improve,yu2020dimsum}, question generation~\cite{su2022qa4qg,su2020multi}. However, little work has been investigated to build an effective real-time LFQA system. Since LFQA is quite a challenging task that requires more than just concatenating the two techniques. Firstly, it needs to handle the issue of applying a pre-trained language model with limited input sequence length, to process the multiple relevant documents containing more than a thousand tokens for generation. Secondly, those multiple retrieved documents may include irrelevant, complementary, or contradictory information; how to generate more query-relevant, long-form answers is another challenge (we show an example in Table~\ref{tab:intro_challenge_examples_1} to provide a better understanding). Therefore, we first propose CAiRE-COVID, a real-time long-form question answering system for COVID-19, which combines information retrieval with state-of-the-art QA and query-focused multi-document summarization techniques to answer high-priority COVID-19-related questions. Then, we introduce QFS-BART, a model that incorporates the explicit answer relevance of the source documents given a query into the answer generation model to generate more query-relevant answers.

    
    % Fluency is the first important evaluation element in many generation tasks, such as abstractive summarization~\cite{su2021improve}, text generation~\cite{su2020multi,su2022read}. It measures the linguistic quality of the generated text. If a long-form answer that is not fluent, such as containing repetitions or grammatical incorrect statements, it will harm the answer contents greatly. 
    
    % Furthermore, the generated answer should contain information that is relevant to the topic of the question. We show an example in Table~\ref{tab:intro_challenge_examples_1} to provide a better understanding. The generated answer gives a detailed definition and introduction of 'liquor store' in a fluent way, however, it lacks \textbf{query-relevance} since the information does not answer the question: \textit{'What does abc stand for?'}. 

    % Therefore, how to generate fluent, query-relevant long-form answers is an important task to build an effective LFQA system.

    \item \textbf{Generating fact-aware long-form answers} Improving the factual correctness of the generated answer is one of the other main challenges for LFQA task~\cite{krishna2021hurdles, lin2021truthfulqa, nakano2021webgpt}. Specifically, the generated answer should be faithful to facts in the source documents~\cite{ji2022survey}. In the example in Table~\ref{tab:intro_challenge_examples_2}, the answer is fluent and relevant to the query when trying to elaborate the \textit{'definition of sadduction'}. However, the content itself is hallucinated and lacks \textbf{faithfulness}. Such a fluent and relevant but unfaithful answer will mislead the user. Therefore, we propose an end-to-end framework named RBG to augment the generation model with fine-grained, answer-related salient information predicted by a machine reading comprehension(MRC) module, to improve and generate a fact-aware answer that is more faithful to the source documents.
    
    \item \textbf{Generating Succinct Answers from Long-form answers}
    % As illustrated in Figure~\ref{fig:intro_extractive_qa}, a traditional open-domain QA system normally retrieves multiple relevant contexts from an external knowledge corpus, then it extracts a short answer span from the retrieved contexts as the answer. 
    A LFQA system should be an ‘empathetic machine’: the long-form answer should not only provide relevant and factual information, but also be succinct. For questions like \textit{who/what/when/where/which} types, people might prefer a more concise answer, as shown in Figure~\ref{tab:intro_challenge_example_3}. Therefore, how to generate a succinct answer is also an important direction. 
    
    While no prior work has been investigated, we take the initial step by leveraging the generated long-form answers as an context to extract succinct short-phrase answers for \textit{closed-book} question answering (CBQA) task. Specifically, we generate a long-form answer leveraging the amount of parameterized knowledge stored in pre-trained language models, and then extract the short-span answer from the long-form answers without access to any external knowledge sources. Experimental results on three QA benchmarks show that our method significantly outperforms previous \textit{closed-book} QA methods, and is on par with traditional \textit{open-book} methods which extracts the answer from the retrieved documents.
% . 
\end{itemize}

\section{Thesis Outline}
The thesis is divided in four main chapters, plus a conclusion. In Chapter 2 first gives an overview of open-domain question answering. Then we will introduce the preliminaries needed throughout the thesis, followed by the related work to LFQA. In Chapter 3, we describes how to build a LFQA system and generate more query-relevant long-form answers. In Chapter 4, we describes how we generate more fact-aware long-form answers. In Chapter 5, we show how we further generate succinct answers from the long-form answers. Finally, in Chapter 6, we summarize the thesis and we discuss possible future research directions.

\section{Contributions}
The contributions of this thesis are summarized as follows:
\begin{itemize}
% \item We made the effort and we are the first to tackle the faithfulness issue of LFQA. We proposed a framework RBG, which do dynamic global salient information prediction from multiple source documents and fusion-in-decoder, to improve the factual correctness of the generated answer. State-of-the-art results have been obtained.  We also topped public leaderboard on LFQA task!

% \item We pioneered the research direction and we are the first to extract succinct answers from generated long-form answer. Our method outperforms prior closed-book QA methods, and on-par with state-of-the-art open-book QA method that explicit retrieve from external DBs, on three open-domain QA datasets.

% \item We set out to tackle and improve the query-relevance of the generated answer. We are the first to build an LFQA system for COVID-19. The system is Kaggle competition winner! We further improve query-relevance and obtained state-of-the-art results.

\item We were among the first to research the LFQA task, and we pioneered the research direction to improve the answer quality in terms of 1) query-relevance, 2) answer faithfulness, and 3) succinctness. 

\item We investigated the core challenges to high answer quality in the LFQA task, in terms of the three aspects. Specifically,
\begin{itemize}
    \item we propose a coarse-to-fine method to extract the document-level and sentence-level query-relevant information, to help a traditional Seq2Seq model to handle long and multiple documents, and consider query-relevance. We further introduce QFS-BART, a model that incorporates the explicit answer relevance attention of the source documents into Seq2Seq model's encoder-decoder attention, to further enhance the answer’s relevance.

    \item we proposed a framework RBG, which does dynamic global salient information prediction from multiple source documents and fusion-in-decoder, to improve the factual correctness of the generated answer.

    \item we propose a two-stage method, to leverage generated contexts to further extract a succinct answer. Specifically, we generate a long-form answer leveraging the amount of parameterized knowledge stored in pre-trained language models~\cite{raffel2020exploring, brown2020language, ye2020studying}, and extract a short-phrase span answer from the generated long-form answer without access to any external knowledge sources.
\end{itemize}
  
\item State-of-the-art results have been obtained on large-scale LFQA datasets, by automatic and human evaluation. We topped the only public leaderboard on LFQA task! And we are the first to build an LFQA system for COVID-19. The system is Kaggle competition winner! 
\end{itemize}

% directions.

% The thesis is divided in four main chapters, plus a conclusion. It is organized as:
% \begin{itemize}
%     \item Chapter 2 first gives an overview of open-domain question answering. Then we will introduce the preliminaries needed throughout the thesis, followed by the related work to LFQA. 
%     \item Chapter 3 describes how to build a LFQA system and generate more query-relevant long-form answers. It have two parts. The first part presents CAiRE-COVID. A real-time LFQA system which can generate multiple-documents-based answers efficiently. The system demonstrates its effectiveness at generating fluent and somehow relevant answers, winning one of the Kaggle competitions related to COVID-19. The second part focuses on further improving the query-relevance of the answer. We presents QFS-BART, a model that incorporates the explicit answer relevance of the source documents given a query into the generation model, to generate more query-relevant answers. 
    
%     \item Chapter 4 describes how to generate more fact-aware long-form answers. We
%     presents RBG, a new end-to-end framework that jointly models answer generation and machine reading to augment the generation model with fine-grained, answer-relevant salient information from each retrieved documents, as an emphasis on faithful facts, to improve the answer faithfulness. 
%     \item Chapter 5 describes how we leverage the informative long-form answers.
    
%     introduces CGAP, a first of its kind  that leverage the generated long-form answers as the context, for open-domain question answering.
%     \item Chapter 6 we summarize the thesis and we discuss possible future research
% directions.
% \end{itemize}


% State-of-the-art results on two LFQA datasets demonstrate the effectiveness of our method, in comparison with strong baselines on automatic and human evaluation metrics. The method also topped one public leaderboard on the LFQA task.





