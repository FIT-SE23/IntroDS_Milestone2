

% \chapter{An End-to-end Framework for Faithful Long-form Question Answering}
\chapter{Faithful to the Source: Generating Fact-aware Long-form Answers}

While current work on LFQA using large pre-trained model for generation are effective at producing fluent and somehow relevant contents, one primary challenge lies in how to generate faithful answer that has less hallucinated facts (We show an example in Fig.~\ref{fig:chap_rbg_example}).

\begin{figure}[!ht]
 \centering
 \includegraphics[width=0.55\linewidth]{figures/chapter-RBG/rbg_example.png}
  \caption{An example from MS MARCO~\cite{nguyen2016ms} dataset. We \hlgreen{highlight} the unfaithful snippets from other model. Our model(\textbf{RBG}) generate more factually accurate answer. }
  \label{fig:chap_rbg_example}
\end{figure}

In this Chapter, we propose a novel end-to-end framework named RBG (\textbf{R}ead \textbf{B}efore \textbf{G}enerate) for LFQA to address the faithfulness challenge. The key idea is to augment the generation process with predicted salient information which can be viewed as an emphasis on answer-related facts. Specifically, we combine a Seq2Seq language model-based generator with a machine reading comprehension (\textit{reader}) module. The \textit{reader} produces an evidence probability score for each sentence, which will be integrated with the generator for final distribution prediction. We perform evidence fusion in a similar way to FiD~\cite{izacard2021leveraging} to equip the pre-trained language model with multiple input documents for generation. To further enhance the factual grounding ability of the generation model, we propose an additional pre-training task to encourage the model to rely more on retrieved documents to generate factual statements. The details are explained in Section \ref{sec:methodology}.


\section{A state-of-the-art Long-form Question Answering System}
\label{sec:methodology}
To generate in-depth, long-form answers for a given general domain question, we first use a retriever to search for relevant information from a large external knowledge source. Then our reader and the generation module take the multiple retrieved documents together with the question as input to generate the answer. Specifically, the reader module adopts a machine reading comprehension (MRC) model to produce an evidence score for each sentence in each document, while the generator, which adopts a large pre-trained Seq2Seq language model, fuses the sentence evidence score into its generation process. Our framework is shown in Figure~\ref{fig:chap_rbg_framework}.


\begin{figure*}[!t]
 \centering
 \includegraphics[width=1.0\linewidth]{figures/ACL2022-framework.png}
  \caption{Overview architecture of our RBG framework. RBG comprises a supporting document retriever, a document reader and a generator.}
  \label{fig:chap_rbg_framework}
\end{figure*}


\subsection{Supporting Document Retriever}

We use the dense passage retriever (DPR)~\cite{karpukhin2020dense} to retrieve the supporting documents following the typical methods in the state-of-the-art framework for open-domain QA~\cite{izacard2021leveraging,NEURIPS2020_6b493230}. 

The passage and question are represented as 768-dimensional dense vector representations, computed via the BERT-based bi-encoder networks of DPR. The retriever will rank the documents according to their similarity, calculated as
\begin{equation}
\label{dpr}
    sim(Q, D_i) = \text{BERT}_q(Q)^T \text{BERT}_d(D_i)
\end{equation}

Retrieval is performed using approximate nearest neighbors with the FAISS~\footnote{\url{github.com/facebookresearch/faiss}} library. We denote $D=\{D_1, D_2,..., D_k\}$ as the top-$K$ retrieved documents for question $Q$. 

As the question/answers in LFQA may cover different domains and topics, we use a multi-task variant of DPR to guarantee the retrieval performance. The retriever is trained jointly on the union of all knowledge-intensive training data in KILT benchmark~\cite{petroni2021kilt}, including TrivaQA~\cite{Joshi_2017}, kwiatkowski2019naturaluestion~\cite{kwiatkowski2019natural}, HotpotQA~\cite{yang2018hotpotqa}, Fever~\cite{thorne2018fever}, zsRE~\cite{levy2017zero}, AY2, T-REx~\cite{elsahar2018t} and WoW~\cite{dinan2018wizard} (More details can be found in Appendix~\ref{appendix:rbg_retriever}).

\subsection{Document Reader}
Since there are no golden retrievals for long-form answers, the retrieved documents may contain complementary, contradictory, or redundant information related to the answer. Thus, we propose to use a reader module to explicitly predict the sentence-level evidence probability in each document.  

\textbf{Evidence span prediction}
We use a machine reading comprehension (MRC) model to predict the evidence span in each document, as these models approach or even outperform human-level performance on many datasets~\cite{joshi2020spanbert}. The MRC model takes the concatenation of the retrieved document $D_i$ and question $Q$ as input, and outputs the prediction of the start and end position of the potential evidence spans in $D_i$. Specifically, it outputs two probability distributions over the tokens in $D_i$: $P^s_i(w_s)$ and $P^e_i(w_s)$, where $P^s_i(w_s)$ / $P^e_i(w_s)$ is the probability that the token $w_s$ is the start/end of the evidence span in $D_i$. 

\textbf{Sentence evidence probability}
Originally, the MRC model was designed to give accurate, short-phrase span prediction~\cite{rajpurkar2016squad}, but we argue that a sentence-level evidence probability will be better in our scenario. The supporting sentences can provide the minimum required context information for each answer span, which is quite important, especially in multi-document generation~\cite{xu2020coarse}. We define our sentence-level evidence probability score $P^i_{rea}(S)$ as the summation over all token-level evidence probabilities in that sentence, and it is calculated via
\begin{align}
    P^i_{rea}(S) &= \sum\nolimits_{w_s\in S}(P^s_i(w_s) + P^e_i(w_s)) \\
    P_{rea}(S) &= \text{Norm}(P^1_{rea}, P^2_{rea}, ..., P^K_{rea}) 
    % P_{rea}(w_i) &= P(S_i)    \text{if} w_i \in S_i 
\end{align}
where $P_{rea}(S)$ denotes the final sentence-level evidence probability in all the $K$ documents regarding the question. 

\textbf{Multi-task MRC}
As there are no golden answer spans for LFQA data, we need a MRC model that has enough generalization ability for open domain questions as a starting point. We choose SpanBERT~\cite{joshi2020spanbert}, and further fine-tune it in a multi-task way on six large-scale MRC datasets from the MRQA shared task ~\cite{fisch-etal-2019-mrqa} following work by ~\cite{su-etal-2019-generalizing}: SQuAD~\cite{rajpurkar2016squad}, NewsQA~\cite{trischler2017newsqa}, TriviaQA~\cite{Joshi_2017}, SearchQA~\cite{dunn2017searchqa}, HotpotQA~\cite{yang2018hotpotqa}, and NatualQuestions~\cite{kwiatkowski2019natural}. The MRC model $R$ is jointly trained with the generator, using the golden answer in a distantly supervised way.

\subsection{Generator}

\textbf{FiD-BART} We choose BART as our generation backbone because of its outstanding performance on many generation tasks, especially on long-form abstractive summarization task~\cite{lewis2020bart}. We propose FiD-BART, following the \textit{Fusion-in-Decoder} idea from ~\cite{izacard2021leveraging}, to empower BART to deal with multiple, long-document inputs. FiD-BART processes each document independently in the encoder, while performing the cross-attention in the decoder jointly. 

The encoder encodes the concatenation of each supporting document $D_i$ and the question $Q$. More precisely, we append the special tokens \textit{question:} before $Q$, \textit{title:} and \textit{context:} before the title and text of each document $D_i$. We denote the encoded final representation of the encoder as $h_{enc}$,  which is the concatenation of the $K$ encoder outputs $h^i_{enc}$ for the $i$th document:
\begin{align}
\setlength\abovedisplayskip{10pt}
\setlength\belowdisplayskip{-10pt}
    h^i_{enc} &= \text{Encoder}(Q;D_i) \\
    h_{enc} &= (h^1_{enc},..., h^i_{enc},..., h^K_{enc}) 
\end{align}
The partial structure of the decoder can be illustrated by Eq.(6)--(8), where $h_l$ is the representation for the $l$-th decoder layer. We denote $h_{dec}$ as the last layer decoder outputs: 
\begin{align}
\setlength\abovedisplayskip{5pt}
\setlength\belowdisplayskip{3pt}
    h_l^a &= \text{SelfAttention}(h_l, h_l, h_l) \\
    h_l^b &= \text{LayerNorm}(h_l + h_l^a) \\
\label{cross-attention}    h_l^c &= \text{CrossAttention}(h_l^b, h_{enc}, h_{enc})
\end{align}
As we can see, FiD-BART can scale to a large number of input documents within a linear computation time.


\subsection{Reader-Before-Generator}
To incorporate the evidence probability into generation, we apply the pointer-generator model (depicted in Figure~\ref{fig:chap_rbg_framework}). The attention distribution $\mathcal{A}$ and context vector $h_c$, and the generation probability $p_{gen}$ $\in$ [0,1] are calculated as follows:
\begin{align}
    \mathcal{A} &= softmax(h_{dec} h^T_{enc}) \\
    h_{c} &= \mathcal{A}  h_{enc} \\
    p_{gen} &= sigmod(W_{c}h_{c} + W_{g}h_{dec}) 
\end{align}
where $W_c$ and $W_g$ are learnable parameters. $p_{gen}$ is used as a soft switch to choose between generating a word from the generator by sampling from the vocab, or copying a word from the input sequence by sampling according to the evidence distribution $P_{rea}(w)$:
\begin{align}
&P_{gen}(w) = lm_{head}(h_{dec}) \\
&P_{rea}(w) = \sum\nolimits_{s:w_s=w\text{,}w_s \in S}{P_{rea}(S)} \\
&P(w) = p_{gen}P_{gen}(w) + (1-p_{gen})P_{rea}(w)
\end{align}

\def\mask{{\sc[mask]}}

\subsection{Pre-training}
To further improve the ability to ground on retrieved documents, we propose a pre-training task: retrieval-augmented recovery~(RAR). Instead of recovering the corrupted text through the internal knowledge memorized in model parameters~\cite{raffel2020exploring,lewis2020bart}, RAR encourages the model to rely more on external retrieved documents to generate factual statements. Specifically, given an original text $S$, we retrieve the top-$k$ documents ${D_1,D_2,...,D_N}$ from the knowledge corpus using BM25~(discarding $S$ itself), and we replace 30\% of the words in $S$ with \mask to form a pseudo query $Q$. The pre-training task asks our RBG model to recover $S$ with the input of the pseudo query $Q$ and $k$ retrieved documents, which can be formulated as
\vspace{-5pt}
\begin{equation}
    S=RBG(Q;D_1,D_2,...,D_k)
    \vspace{-5pt}
\end{equation}
\noindent To involve more factual information during the text corruption and recovery process, we sample 1 million sentences of $S$ corresponding to at least one knowledge base triplet from Wikipedia with the text-triple alignment of TREX~\cite{TREX}.  
%We find that different reference documents can provide factual evidence to recover different masked words. This is similar to the downstream LFQA situation that the model need to combine evidence in different retrieved documents to generate the final answer.(..tbd details)
\section{Experiment Setups}

\subsection{Datasets}
We conduct experiments on the two following datasets, both of which concentrate on long form generative QA.
\begin{itemize}
    \item \textbf{ELI5}~\citet{fan2019eli5} is the only publicly available large-scale LFQA dataset. It is a collection of
question-answer pairs extracted from the Reddit forum "Explain Like
I’m Five"(ELI5). We use the KILT~\cite{petroni2021kilt}  version of the dataset from its Github repository\footnote{\url{github.com/facebookresearch/KILT}}, which
has 272,634 training examples and 1,507 development examples. The average length of the answers is 130 words.
    \item \textbf{MS MARCO}~\citet{nguyen2016ms} is a dataset of crowdsourced responses to Bing
queries. We use the question-answer pairs of the MS MARCO passage ranking track for training and evaluation, as they are more abstract and reliant on multi-document information than those of the NLG track. The training example size is about 500,000 and the evaluation example size is 6980. 
    \item \textbf{Knowledge source} The external knowledge source of the retriever is the Wikipedia paragraphs, which are provided in the KILT benchmark as a unified knowledge source for knowledge-intensive tasks, including open-domain LFQA ~\cite{petroni2021kilt}. It is based on the 2019/08/01 Wikipedia snapshot, and contains 5.9M articles. 

\end{itemize}

\subsection{Baselines}
\begin{itemize}
    \item \textbf{BART and T5} We fine-tune BART~\cite{lewis2020bart} and T5~\cite{raffel2020exploring} using QA pairs without explicit retrieval, and include them as our baselines which rely only on parameterized internal knowledge~\cite{roberts2020much} to generate answers.
    \item \textbf{}{DPR-BART} is our retrieval-based LFQA baseline. We follow ~\citet{petronicontext} to retrieve and prepend the top-3 passages from DPR for each input sample, and use context-enhanced training data to fine-tune a BART model.
    \item \textbf{RAG}~\citet{NEURIPS2020_6b493230} is an end-to-end retrieval-augmented generation model which back-propagates to the retriever’s input encoder. We experiment with fine-tuning RAG on LFQA tasks, establishing a strong baseline on all of them.  At every generation step we retrieve the top-5 passages and use them as supporting documents.
    \item \textbf{FiD} ~\citet{izacard2021leveraging} encodes each passage independently and combines all outputs from the encoder before passing them to the decoder. FiD has achieved superior performance on a number of open-domain QA tasks~\cite{izacard2021leveraging}. We implement FiD-BART, using BART as the generation backbone, as our strongest baseline.

\end{itemize}

\subsection{Implementation Details}

\textbf{Document Retriever Model Details} As the question/answers in LFQA may cover different domains and topics, we use a multi-task variant of DPR to guarantee the retrieval performance. The retriever is trained jointly on the union of all knowledge-intensive training data in KILT benchmark~\cite{petroni2021kilt}, including TrivaQA~\cite{Joshi_2017}, naturaluestion~\cite{kwiatkowski2019natural}, HotpotQA~\cite{yang2018hotpotqa}, Fever~\cite{thorne2018fever}, zsRE~\cite{levy2017zero}, AY2, T-REx~\cite{elsahar2018t} and WoW~\cite{dinan2018wizard}. 


% As there is no golden retrievals provided in current LFQA datasets.

\section{Experiment Results}

\subsection{Automatic Evaluation}

We use the metrics unigram F1 score and ROUGE-L~\cite{lin2004rouge} in previous work on LFQA ~\cite{petroni2021kilt, krishna2021hurdles} to evaluate and compare the generation quality of our method. 

\textbf{Overall Comparison} Table~\ref{results} shows the performance of various methods on the two datasets. As shown, our RBG method outperforms all baselines models with regard to both evaluation metrics on both datasets. The RBG method also outperforms the previous state-of-the-art method \textit{c-REALM+RT} on the KILT-ELI5 leaderboard\footnote{\url{ https://evalai.cloudcv.org/web/challenges/challenge-page/689/leaderboard/1908}} ~\cite{krishna2021hurdles}, as shown in Table~\ref{leaderboard}.

\begin{table}[!ht]
\centering
\resizebox{0.62\textwidth}{!}
{
\begin{tabular}{c|cc|cc}
\hline
Models  & \multicolumn{2}{c}{Eli5} &\multicolumn{2}{c}{MS MARCO}\\  
            & ROUGE-L      & F1         & ROUGE-L       & F1 \\ \hline
T5(base)    & 21.02        & 18.36       & 21.19 & 20.03 \\
BART(large) & 22.69        & 22.19      & 23.26 & 25.6  \\
DPR+BART    & 17.41        & 17.88      & 23.01 & 25.13     \\
RAG         & 16.11        & 17.24      &   -    & -  \\
FiD  &    25.70      &    28.55    &   24.64    & 27.08   \\
RBG(ours)   & \textbf{26.46}        & \textbf{29.04}      & \textbf{24.72} & \textbf{27.52} \\ \hline
\end{tabular}
}
\caption{Performance comparison between our RBG method and the baselines on the KILT-ELI5~\cite{petroni2021kilt} and MS MARCO~\cite{nguyen2016ms} evaluation sets.}
\label{results}
\end{table}

\begin{table}[!ht]
\centering
\resizebox{0.62\textwidth}{!}
{

\begin{tabular}{c|ccccc}
\hline
Model           & \multicolumn{2}{c}{Retrieval} & \multicolumn{2}{c}{Generation} &      \\
                & PRr.       & R@5        & F1           & R-L          & KRL  \\ \hline
RBG(ours)       & 10.83         & 27.25         & \textbf{24.53 }         & \textbf{27.13}         & \textbf{2.62} \\
DPR\_kilt\_wiki &        14.83       &   27.69            &      16.45          &       15.91        &    2.46  \\
c-REALM$^1$  &        10.67       &      24.56         &        23.19        &      22.88         &   2.36   \\
DPR+BART        &    10.67           &       26.92        &       17.41         &        17.88       &  1.90    \\
RAG             &        11.00       &      22.92         &      14.05          &      14.51         &   1.69   \\
BART-large      &          0.00     &         0.00     &     20.55            &    19.23            &     0.00  \\ 
T5-base         &      0.00         &       0.00        &       19.08         &      16.10         &   0.00   \\
\hline
\end{tabular}

}
\caption{Results on the ELI5 test set on the KILT leaderboard. Our RBG tops the leaderboard in terms of (1) retrieval performance, using R-precision(RPr.) and Recall@5(R@5), and (2) generation quality, using F1 and ROUGE-L(R-L). These scores are combined to produce the overall metric KILT R-L(KRL)~\cite{petroni2021kilt}. c-REALM$^1$ is from~\cite{krishna2021hurdles}}
\label{leaderboard}
\vspace{-10pt}
\end{table}

\textbf{Fine-grained Comparison} Intuitively, the quality of retrieved documents will affect the generation quality, thus we provide a fine-grained performance comparison. We split MS-MARCO evaluation set into different subset based on the quality of the retrieved documents\footnote{\label{foot:retrieve} We consider two metrics to measure the retrieval quality for a certain question: (1)~\textbf{Top-1 document retrieval score} which is the matching score output by the retriever (Equation.~\ref{eqa:retriever_score}) for the top-1 document to measure the corresponding semantic relevance to the given question, and (2)~\textbf{N-gram overlap}, which is the N-gram overlap between the golden answer and the top-k retrieved documents.}, and compare the ROUGE-L score between FiD and RBG under each subset.

As we can see from Table~\ref{tab:FidvsRBG}, even though RBG beats FiD by 0.1 Rouge-L score on the whole MS-MARCO evaluation set, the performance gap continue increasing as the retrieval quality of the evaluation subset increased. This indicates that RBG is especially effective when high-quality retrieval documents is provided, which matches with our intuition. 

\begin{table}[!h]
\centering
\resizebox{0.60\textwidth}{!}
{
\begin{tabular}{lc|cccc}
\hline
\multicolumn{2}{l|}{>ngram overlap} & 0     & 0.4        & 0.6       & 0.8    \\ \hline
\multicolumn{2}{l|}{\# of documents} & 6980  & 3493      & 1470      & 489    \\ \hline
\multirow{2}{*}{ROUGE-L}    & FiD    & 24.64 & 28.04  & 33.62  & 45.25 \\
                            & RBG    & 24.72 & 28.59  & 34.38   & 46.29  \\ \hline
\end{tabular}
}
\end{table}
\vspace{-20pt}
\begin{table}[!h]
\centering
\resizebox{0.60\textwidth}{!}
{
\begin{tabular}{lc|cccc}
\hline
\multicolumn{2}{l|}{\textgreater{}retrieval score} & 0.0   & 75    & 80    & 85    \\ \hline
\multicolumn{2}{l|}{\# of documents}               & 6980  & 5811  & 3188  & 1001  \\ \hline
\multirow{2}{*}{ROUGE-L}           & FiD           & 24.64 & 24.7  & 25.63 & 26.81 \\
                                   & RBG           & 24.72 & 25.46 & 26.53 & 27.96 \\ \hline
\end{tabular}
}
\caption{Fine-grained comparison between FiD and RBG on different subset of MS-MARCO evaluation data.}
\label{tab:FidvsRBG}
\end{table}

% As we can see in Table~\ref{retrieval_results}, better-retrieved documents always bring better generation quality, indicating the importance of high-quality supporting documents for the generation process.

\vspace{-10pt}
\subsection{Human Evaluation} 
We further evaluate our model using human annotators, who we ask to quantify three aspects of the generated answer, (1)~\textbf{fluency}, which measures whether the answer is coherent and less repetitive; (2)~\textbf{relevance}, which measures the amount of information relevant to answering the question, and (3)~\textbf{factual correctness}~(also briefly called correctness), which measures the correctness and faithfulness of all facts involved in the generated answer.

We select FiD, which is the strongest baseline in terms of automatic metrics, for comparison. We sample evaluation questions from the MS MARCO dev set, which are better supported by Wikipedia knowledge than ELI5. Table~\ref{tab:human eval abs} shows the absolute evaluation results of human annotation. To reduce the impact of scale selection inconsistency of different annotators, we also show the relative evaluation results in Table~\ref{tab:human eval rel}. We can see that both types of results indicate that RBG outperforms FiD in terms of all three aspects. RBG has more advantages over FiD on the metric of factual correctness, possibly benefited by the introduction of the reader module and additional pre-training. More details of the human evaluation setup and statistical analysis can be found in Appendix~\ref{appendix:rbg_human_eval}.


\begin{table}[htbp]
\centering
\resizebox{0.60\textwidth}{!}
{
\begin{tabular}{c|ccc}
\hline
Model    & Fluency      & Relevance      &  Correctness   \\ \hline
FiD  &    2.62      &    2.34    &    2.07    \\
RBG(ours)   & \textbf{2.70}        & \textbf{2.50}     & \textbf{2.41} \\ \hline
\end{tabular}
}
\caption{Absolute human evaluation results for RBG vs. FiD on MS MARCO. The table shows the mean value across all annotators and examples for each metric.}
\label{tab:human eval abs}
\end{table}

\begin{table}[ht]
\centering
\resizebox{0.58\textwidth}{!}
{
\begin{tabular}{c|ccc}
\hline
Aspect    & Prefer FiD      & Prefer RBG      &  Tie  \\ \hline
Fluency  &    12\%   & \textbf{26\%}   &    62\% \\
Relevance  &  18\%     & \textbf{48\%} & 34\% \\
Correctness &  4\%     & \textbf{62\%} & 34\% \\ \hline
\end{tabular}
}
\caption{Relative human evaluation results for RBG vs. FiD on MS MARCO. The percentages represent the ratio of one model being voted as preferred by multiple annotators on a metric.}
\label{tab:human eval rel}
\end{table}

\subsection{Ablation}
To further investigate the contribution and effect of each module in the proposed system, we conducted a systematic ablations on the MS-MARCRO evaluation dataset. 

\begin{table}[!th]
\centering
\resizebox{0.60\textwidth}{!}
{
\begin{tabular}{c|c|cc}
\hline
No. & models     & \multicolumn{2}{c}{MS MARCO} \\ 
  &   & ROUGE-L    & \multicolumn{1}{l}{F1} \\ \hline
0 & RBG(ours)    & 24.72 & 27.52                  \\ 
1 & w/o reader &  24.66 & 27.30 \\
2 & w/o pre-training  & 24.65  & 27.38   \\
3 & w/o reader + pre-training & 24.64  & 27.08 \\
4 & w/ reader frozen &  24.51 & 25.85 \\
5 & w/ random retrieval  & 22.84  & 25.23  \\
\hline
\end{tabular}
}
\caption{Ablation results on the MS MARCO evaluation set. A more fine-grained results comparison is shown with analysis in Section ~\ref{sec:further}.}
\label{ablation}
\vspace{-15pt}
\end{table}
\begin{itemize}
    \item \textbf{\textit{w/o} reader/pre-training:} We respectively remove the reader module (\textbf{w/o reader}), the pre-training (\textbf{w/o pre-training}), and both together (\textbf{w/o reader + pre-training}) from our model , to test the contribution of each part. As we can see from Table~\ref{ablation}, without the reader to predict the evidence probability, the generation performance decreases in both metrics, and the performance continues to drop without the pre-training. 
    \item \textbf{\textit{w/} reader frozen:}  We freeze the reader to investigate the benefit of distantly supervised end-to-end training of the reader module. As we can see from Table~\ref{ablation}, the results on both metrics drop, especially the F1 score, which proves the effectiveness of the end-to-end training.
    \item \textbf{\textit{w/} random retrieval:} To investigate whether and how much the generation process is grounded in the retrieved documents, we replace retrieved paragraphs with randomly sampled paragraphs from Wikipedia at \textit{inference} time for comparison. As we can see, the ROUGE-L drops significantly with randomly retrieved documents, and it is also worse than the baseline systems such as BART and DPR-BART (Table~\ref{results}).

\end{itemize}

\section{Further analysis} 
\vspace{-5pt}
\label{sec:further}
We conduct further analysis on the results, considering that LFQA is a complicated but less explored task, which deserves a complete investigation. 
\vspace{-5pt}
\subsection{How does retriever affect the generation quality?}
\vspace{-3pt}
We further investigate the effects of the quality of retrieved documents on the final generation. We split the evaluation sets of the two datasets via different thresholds for the two metrics\textsuperscript{\ref{foot:retrieve}} and calculate the corresponding ROUGE-L score for each subset. As we can see in Table~\ref{retrieval_results}, better-retrieved documents always bring better generation quality, indicating the importance of high-quality supporting documents for the generation process.

% We also measure the effects of the number of retrieved documents $K$ on the generation quality and find that the best $K$ from $\{5,10,20,50\}$ is 10 (see more details in Appendix~\ref{appendix:rbg_k}). More retrieved documents do not improve generation quality as in open-domain QA.


\subsection{Number of Retrieved Documents on Generation Quality}
\label{appendix:rbg_k}
\begin{table}[!ht]
\centering
\begin{tabular}{c|cc}
\hline
ndocs & ROUGE-L & F1    \\ \hline
5     & 24.63   & 27.29 \\
10    & \textbf{24.72}   & \textbf{27.52} \\
20    & 24.39   & 26.68 \\
50    & 23.43   & 25.94 \\ \hline
\end{tabular}
\caption{Generation performance versus the number of retrieved documents of our model on MS MARCO~\cite{nguyen2016ms}.}
\label{tab:results_k}
\end{table}

We also investigate the effects of number of retrieved documents $k$, on the answer generation quality. As we can see in Table~\ref{tab:results_k}, the generation quality in terms of ROUGE-L and F1, do not further improve as the number of $k$ increases, and the best performance are obtained when $k=10$ in our case.

\begin{table}[!h]
\centering
\resizebox{0.60\textwidth}{!}
{
\begin{tabular}{c|cc|cc}
\hline
\multicolumn{1}{l|}{\multirow{2}{*}{\begin{tabular}[c]{@{}l@{}}\textgreater{}retrieval\\ score(top-1)\end{tabular}}} & \multicolumn{2}{c|}{ELI5}                                    & \multicolumn{2}{c}{MS MARCO}                                 \\ \cline{2-5} 
\multicolumn{1}{l|}{}                                                                                                & \multicolumn{1}{l}{\# of data} & \multicolumn{1}{l|}{ROUGE-L} & \multicolumn{1}{l}{\# of data} & \multicolumn{1}{l}{ROUGE-L} \\ \hline
0.0 & 1570  & 26.35  & 6980  & 24.72 \\
75 & 1270 & 26.37 & 5811  & 25.46                      \\
80 & 479  & 26.38 & 3188  & 26.53                      \\
85  & 72  & 26.96   & 1001     & 27.96                      \\
90 & 11    & 27.25   & 161  & 27.61                      \\ \hline
\end{tabular}
}
\end{table}
\begin{table}[!h]
\centering
\resizebox{0.60\textwidth}{!}
{
\begin{tabular}{c|cc|cc}
\hline
\multicolumn{1}{l|}{\multirow{2}{*}{\begin{tabular}[c]{@{}l@{}}\textgreater{}ngram \\ overlap\end{tabular}}} & \multicolumn{2}{c|}{ELI5}                                    & \multicolumn{2}{c}{MS MARCO}                                 \\ \cline{2-5} 
\multicolumn{1}{l|}{}                                                                                        & \multicolumn{1}{l}{\# of data} & \multicolumn{1}{l|}{ROUGE-L} & \multicolumn{1}{l}{\# of data} & \multicolumn{1}{l}{ROUGE-L} \\ \hline
0.0  & 1570   & 26.35  & 6980  & 24.72 \\
0.4 & 460 & 27.09& 3493 & 28.59  \\
0.5 & 260& 27.31 & 2470 & 30.72 \\
0.6 & 109  & 27.52 & 1470 & 34.38 \\
0.7 & 48 & 27.63 & 845  & 39.64  \\
0.8& 27  & 27.17& 489  & 46.29  \\ \hline
\end{tabular}
}
\vspace{-5pt}
\caption{Fine-grained results of our RBG on ELI5 and MS MARCO. With high-quality retrieval (higher N-gram overlap or retrieval score threshold), the answer quality (ROUGE-L) increases on both datasets.}
\label{retrieval_results}
\end{table}
\vspace{-10pt}

\vspace{-5pt}
\subsection{How does the reader contribute to the generation?}
\vspace{-5pt}

As shown in the ablation study, the reader module improves the overall performance on the MS MARCO evaluation dataset. We further investigate its performance when retrieved documents with different quality levels are provided. 

We show in Figure~\ref{Fig:decomp_for_reader} the fine-grained comparison results between ablation models  \textbf{No.2}: \textit{RBG w/o pre-training} and \textbf{No.3}: \textit{RBG w/o pre-training + reader}. As we can see, the difference in ROUGE-L between the two models increases as the quality of the retrieved documents improves, indicating the reader's strong capability, especially on high-quality data. This also matches with our intuition. We also conduct a human evaluation for reader analysis, and we show the results in Table~\ref{tab:human eval reader}. 

\begin{figure}[!t]
	\begin{center}
		\includegraphics[width=0.66\textwidth]{figures/chapter-RBG/decomposition_rouge_reader.png}
		\caption{ROUGE-L versus document retrieval performance for reader analysis.}
		\label{Fig:decomp_for_reader}
	\end{center}
	\vspace{-0.2cm}
\end{figure}




\begin{table}[htbp]
\centering
\resizebox{0.60\textwidth}{!}
{
\begin{tabular}{c|ccc}
\hline
Aspect    & Prefer w/o reader      & Prefer w/ reader      &  Tie \\ \hline
Fluency  &    15\%    & \textbf{35\%}   &    50\% \\
Relevance  &  17\%     & \textbf{57\%} &26\% \\
Correctness &  25\%     & \textbf{45\%} & 30\% \\ \hline
\end{tabular}
}
\caption{Human evaluation results for RBG reader analysis on MS MARCO. The model with reader has better generation performance in terms of fluency, relevance and correctness.}
\label{tab:human eval reader}
\end{table}

\begin{figure}[t]
\vspace{-5pt}
	\begin{center}
		\includegraphics[width=0.66\textwidth]{figures/chapter-RBG/decomposition_rouge_pretrain.png}
		\vspace{-5pt}
		\caption{ROUGE-L versus Document retrieval performance for pre-training analysis.}
		\label{Fig:decomp_for_pretrain}
	\end{center}
	\vspace{-10pt}
\end{figure}

\vspace{-5pt}
\subsection{How does pre-training help?}
\vspace{-5pt}

\begin{table}[ht]
\centering
\resizebox{0.60\textwidth}{!}
{
\begin{tabular}{c|ccc}
\hline
Aspect    & Prefer w/o pre-training      & Prefer w/ pre-training      &  Tie \\ \hline
Fluency  &    40\%    & \textbf{43\%}   &    17\% \\
Relevance  &  20\%     & \textbf{33\%} & 47\% \\
Correctness &  23\%     & \textbf{47\%} & 30\% \\ \hline
\end{tabular}
}
\caption{Human evaluation results for RBG pre-training analysis on MS MARCO. The model with RAR pre-training has better generation performance in terms of relevance and correctness.}
\label{tab:human eval pretrain}
\end{table}

We also compare the models' performance in a fine-grained way, to quantify the contribution from our pre-training task. We show in Figure~\ref{Fig:decomp_for_pretrain} the fine-grained comparison results between ablation models \textbf{No.1}: \textit{RBG w/o reader} and \textbf{No.3}: \textit{RBG w/o pre-training + reader}. As we can see, the model with pre-training is better in most situations than that without pre-training. The human evaluation in Table~\ref{tab:human eval pretrain} also indicates the effectiveness of our pre-training task to improve the factual correctness and relevance of the generated answer. We conjecture that the pre-training task of retrieval-augmented recovery can facilitate the downstream LFQA model to combine multiple pieces of evidence from different retrieved documents to generate the final answer.

\subsection{Faithfulness analysis}

\textbf{Zero-shot on extractive QA tasks}  Inspired by previous work~\cite{wang2020asking, durmus2020feqa} which leverage a Question Generation(QG) and a QA model to generate question answer pairs, to evaluate the faithfulness of a summary\footnote{
They generate question answer pairs <$q$,$a_{sum}$> from the summary, and compare $a_{sum}$ with the answer $a_{sc}$ from source document for $q$, to evaluate faithfulness.}, we propose to evaluate answer faithfulness via evaluation on two simpler open-domain QA datasets: NaturalQuestions~\cite{kwiatkowski2019natural} and HotpotQA~\cite{yang2018hotpotqa}, which contain single-hop or multi-hop factual questions with golden answers ($\{(q_i, a^s_i)\}_{i=1}^m$) where $a_i^s$ can be extracted from Wikipedia-based documents. We use the trained models (based on MS MARCO) in Table~\ref{results} to do zero-shot long-form answer generation for these two datasets $\{a^l_i = \text{Model}_{\text{ms}}(q_i)\}$, and measure the short-answer recall~(the ratio of golden answer span $a^s$ contained in the generated long answer $a^l$) as an estimation of faithfulness of the generated long-answer:
\begin{equation}
Score(q, a^s, a^l) = \frac{\sum_{i=1}^m\mathbbm{1}[a^s_i \in a^l_i]}{m}
\label{eqa:faithfulness}
\end{equation}
We show the results in Table~\ref{nq_HotpotQA}. As we can see, our system achieves comparable performance with FiD on NQ, and it consistently outperforms other strong baselines on multi-hop dataset hotpotQA, indicating its capability in generating faithful answer especially on complex question that need to synthesis information. We also give concrete examples in Appendix~\ref{appendix:rbg_results} that show our model can generate more faithful snippets than FiD apart from automatic metrics. 

\begin{table}[!t]
\centering
\resizebox{0.55\textwidth}{!}
{
\begin{tabular}{c|cc}
\hline
          & NQ Recall   & HotpotQA Recall\\ \hline
T5         & 4.76  & 7.20     \\
BART-large & 10.44 & 9.13   \\
DPR+BART   & 16.37 & 11.57     \\
FiD      & 43.93 & 22.94    \\ \hline
RBG(ours)  & 43.93 & 23.36    \\ \hline
\end{tabular}
}
\caption{Faithfulness Analysis of the system generation quality via zero-shot evaluation on NQ~\cite{kwiatkowski2019natural} and HotpotQA~\cite{yang2018hotpotqa}.}
\label{nq_HotpotQA}
\end{table}


\textbf{Case Study} To have a concrete understanding of the reader's role to address faithfulness, we show two examples in Table~\ref{tab:reader_examples}. While both models use the same \textbf{ctxs}, RBG \textbf{w reader} generates a more \textit{relevant} answer, and \textbf{w/o reader} only gives some correct but unrelated statements (Case 1). The reader also contributes to generating more \textit{faithful} answers, as shown in Case 2. However, we notice that there is one unfaithful statement, which hallucinates the 'second-oldest' as 'oldest'. This might be because of the property of our MRC model: it gives a high evidential score to one sentence, and weakening the importance of other sentences in the same document.

\begin{table}[!ht]
% \small
\centering
\begin{adjustbox}{width={0.75\textwidth},totalheight={\textheight},keepaspectratio}
\begin{tabular}{p{1\columnwidth}}
\hline
\textbf{Question:} what does abc stand for in liquor store?  \\
\hline
\textbf{w/o reader answer:} ABC stores. A liquor store is a retail shop that predominantly sells prepackaged alcoholic beverages intended to be consumed off the store's premises. ...,or other similar terms. \\
\textbf{w/ reader answer}: ABC stands for Alcoholic Beverages Control (ABC Stores) and is a type of liquor store that is owned and operated exclusively by the state government, where liquor stores often sell only spirits or sometimes sell spirits and wine but not beer...\\
\textbf{ctxs:} [\{text: ...\textcolor[RGB]{5, 30, 255}{In seventeen alcoholic beverage control (ABC) states}, ...\},...,\{text:\textcolor[RGB]{5, 30, 255}{operated exclusively by the state government, where liquor stores often sell only spirits or sometimes sell spirits and wine but not beer.}, ...,because purchased liquor must be packaged in sealed bottles\},...,\{...\}]\\
\hline
\textbf{Question:} dow jones industrial average please? \\
\hline
\textbf{w/o reader answer:} The Dow Jones Industrial Average (DJIA) is an index of \hlgreen{30 major U.S. stock indexes}. The DJIA was created in 1896 by Charles Dow, a statistician and editor of The Wall Street Journal. \hlgreen{The index was first published in the mid-1880s, and was last updated on May 26, 1896.} \\
\textbf{w/ reader answer:} The Dow Jones Industrial Average (DJIA) is a stock market index that tracks the performance of the 30 largest U.S. industrial companies. It is owned by S\&P Dow Jones Indices, which is majority owned by Standard \& Poor's Global. The Dow is the \hlgreen{oldest} and most famous of the Dow averages. It was created by Charles Dow in 1896. \\
 \textbf{ctxs:} [\{text:...,was originally published on February 16, 1885. ...\textcolor[RGB]{5, 30, 255}{The industrial average was first calculated on May 26, 1896}..\},...\{text:...It is the \textbf{second-oldest} U.S. market index after the Dow Jones Transportation Average. \textcolor[RGB]{5, 30, 255}{Currently owned by S\&P Dow Jones Indices, which is majority owned by S\&P Global}..\},...,\{...\}]
 \\
 \hline
\end{tabular}
 \end{adjustbox}
\caption{Examples from MS MARCO dataset. We \textcolor[RGB]{5, 30, 255}{highlight} the sentences that have high evidential probability from the reader, and use \hlgreen{green} to mark out the unfaithful snippets. }
\label{tab:reader_examples}
\end{table}

\section{Summary}
In this Chapter, we propose a new end-to-end framework RBG that jointly models answer generation and machine reading to tackle the faithfulness issue in LFQA. Experiments on two LFQA datasets, ELI5 and MS MARCO, demonstrate the effectiveness of our method in comparison with strong baselines on automatic and human evaluation metrics. The detailed analysis further proves the competency of our method in generating fluent, relevant, and more faithful answers. We also propose to evaluate the factual correctness of LFQA model by answering questions of extractive QA tasks~(e.g., Natural Questions), which may be helpful to evaluate the faithfulness of LFQA model efficiently.