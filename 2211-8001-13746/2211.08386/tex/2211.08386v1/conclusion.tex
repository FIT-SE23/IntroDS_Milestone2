
\chapter{Conclusion}

In this thesis, we investigated the relevance, faithfulness, and succinctness aspects of Long Form Question Answering (LFQA). LFQA aims to generate an in-depth, paragraph-length answer for a given question, to help bridge the gap between real scenarios and the existing open-domain QA models which can only extract short-span answers.

We are among the first to research the LFQA task. We pioneered the research direction to improve the answer quality in terms of 1) query-relevance, 2) answer faithfulness, and 3) answer succinctness.

We investigated the core challenges to high answer quality in the LFQA task, in terms of the three aspects. Specifically,

% \begin{itemize}
%     \item In generating query-relevant long-form answers, we first present a coarse-to-fine method to extract the document-level and sentence-level query-relevant information, to help a traditional Seq2Seq model to handle long and multiple documents as input, and consider query-relevance. We further introduce QFS-BART, a model that incorporates the explicit answer relevance attention of the source documents into the generation model's encoder-decoder attention module, to further enhance the query-relevance. 
%     \item In generating fact-aware long-form answers, we propose an end-to-end framework named RBG (\textbf{r}ead \textbf{b}efore \textbf{g}enerate) to improve and generate fact-aware answer that is more faithful to the source documents. The main idea is to do dynamic global salient information prediction from multiple source documents and fusion-in-decoder, and augment the generation model with the salient information.
    
    
%     \item In generating the succinct answers from a LFQA system, we propose to leverage the generated long-form answers as a context to extract succinct, short-phrase answers. Specifically, we work on the extractive open-domain QA task. We generate a long-form answer leveraging the amount of parameterized knowledge stored in pre-trained language models~\cite{raffel2020exploring, brown2020language, ye2020studying}, and extract a short-phrase span answer from the generated long-form answer without access to any external knowledge sources and. To the best of our knowledge, no previous investigation has been conducted on generating long-form answers from large pre-trained LMs and leveraging them to extract succinct answers. 
% \end{itemize}

\begin{itemize}
    \item Since traditional Seq2Seq models are not good at handling long and multiple documents as input, we propose to use a coarse-to-fine method to extract the document-level and sentence-level query-relevant information hierarchically, and incorporate them into the generation process, for better answer relevance. We further introduce QFS-BART, a model that incorporates the explicit answer relevance attention of the source documents into the Seq2Seq modelâ€™s encoder-decoder attention module, to further enhance the query-relevance.

    \item As multiple relevant documents are needed for long-form answer generation:  they contain a considerable amount of redundant, complementary, or contradictory information; and the longer input also challenges the pre-trained LM which has limited input length. So we propose an end-to-end framework named RBG (read before generate) to do dynamic global salient information prediction from multiple source documents in parallel and fusion-in-decoder, and augment the generation model with the salient information, to improve and generate fact-aware answer that is more faithful to the source documents.
    
    \item 
    The generated answer also should be succinct, other than providing relevant and factual information. While no prior investigation has been conducted, we propose to leverage the generated long-form answers as a context to further extract succinct, short-phrase answers. The proposed CGAP method first generates a long-form answer leveraging the amount of parameterized knowledge stored in pre-trained language models ~\cite{raffel2020exploring, brown2020language, ye2020studying}, and extracts a short-phrase span answer from the generated long-form answer without access to any external knowledge sources. 

\end{itemize}

% We obtained state-of-the-art results on large-scale LFQA datasets, demonstrating the effectiveness of our proposed method in comparison with strong baselines, on automatic and human evaluation metrics. The proposed RBG method also topped the only public leaderboard~\footnote{\tt \small https://evalai.cloudcv.org/web/challenges/challenge-page/689/leaderboard/1908} on the LFQA task! We are also the first to build an LFQA system for COVID-19. The CAiRE-COVID system has won one of the ten tasks in the Kaggle COVID-19 Open Research Dataset Challenge\footnote{\tt \small  https://www.kaggle.com/allen-institute-for-ai/CORD-19-research-challenge}, by generating relevant and fluent answers to salient COVID-19 related questions, judged by medical experts. 

The effectiveness of the coarse-to-fine method and the QFS-BART model to improve query-relevance has been proved by obtaining state-of-the-art results on large-scale datasets by automatic evaluation. The proposed RGB method to improve answer faithfulness obtains state-of-the-art results on large-scale LFQA datasets ELI5 and MS MARCO compared to strong baselines such as FiD and RAG, by both automatic and human evaluation metrics. We experimented on the extractive open-domain QA task, to generate succinct answers with the CGAP method. CGAP significantly outperforms previous \textit{closed-book} QA methods (e.g., exact matching 68.6\% vs. 55.3\%) and is on par with \textit{open-book} methods that exploit external knowledge sources (e.g., 68.6\% vs. 68.0\%), on three QA benchmarks.

The proposed RBG method also topped the only public leaderboard on the LFQA task, outperforming the previous state-of-the-art method by Google! We leverage the coarse-to-fine method to build the first LFQA system for COVID-19. The CAiRE-COVID system has won one of the ten tasks in the Kaggle COVID-19 Open Research Dataset Challenge, by generating relevant and fluent answers to salient COVID-19-related questions, judged by medical experts.

In future work, we expect to develop LFQA systems that can provide more query-relevant and fact-aware answers, and are also succinct enough. In fact, in the three chapters of this thesis, we open new exciting research directions for LFQA system: 1) there are still possible ways worth exploring to build a more query-relevant LFQA system. In Chapter 3, we try to improve query-relevance via the QFS task, while the quality of the relevant document retrieved from external corpus also affects the generation model. In future works, We would like to expand the external knowledge sources such as the knowledge bases (KBs), visual materials, etc., so that the coverage of relevant knowledge from the retriever will be increased. 2) building a fact-aware LFQA system is still in its infancy. In Chapter 4, we leverage a machine reading comprehension model to highlight the salient information in the retrieved documents and incorporate the global-wise information into the generation. In the future, more sophisticated retrieval techniques such as iterative retrieval should be explored on the task. 3) generating succinct answers by an LFQA system. In Chapter 5, we explored generating succinct
answers for those simple question types such as the who/when/which/where types from the long-form answers. In the future, generating succinct answers from general question types such as the why/how should be explored.


