\chapter{Conclusion}

In this thesis, we study methods to build and improve a LFQA system. Specifically, we focus on: how to build a real-time LFQA system and generate more query-relevant answers, how to generate fact-aware long-form answers, and how to leverage the informative generated long-form answers.

In generating query-relevant long-form answers, we first present CAiRE-COVID, a real-time long form question answering system for COVID-19, which combines information retrieval with state-of-the-art QA and query-focused multi-document summarization techniques, to answer high priority COVID-19 related questions. Then, we introduce QFS-BART, a model that incorporates the explicit answer relevance of the source documents given a query into the answer generation model, to generate more query-relevant answers. CAiRE-COVID has won one of the 10 tasks in the Kaggle COVID-19 Open Research Dataset Challenge\footnote{\tt \small  https://www.kaggle.com/allen-institute-for-ai/CORD-19-research-challenge}, by generating relevant and fluent answers to salient COVID-19 related questions, judged by medical experts. QFS-BART obtains state-of-the-art results on query-focused summarization dataset. 

In generating fact-aware long-form answers, we propose an end-to-end framework named RBG (\textbf{r}ead \textbf{b}efore \textbf{g}enerate) to improve and generate fact-aware answer that is more faithful to the source documents. The main idea is to augment the generation model with fine-grained, answer-related salient information predicted by an machine reading comphrehension(MRC) module. State-of-the-art results on two LFQA datasets demonstrate the effectiveness of our method, in comparison with strong baselines on automatic and human evaluation metrics. The method also topped one public leaderboard~\footnote{\tt \small https://evalai.cloudcv.org/web/challenges/challenge-page/689/leaderboard/1908} on the LFQA task.

In leveraging generated long-form answers for extractive open-domain QA task, we propose to leverage generated long-form answers as the intermediate bridge between the huge amount of parameterized knowledge in large pretrained LMs and the answer for closed-book QA(CBQA), which directly answer the open-domain questions without access to any external knowledge sources, and leverages the parametric knowledge stored in large pretrained language models (LMs)~\cite{raffel2020exploring, brown2020language, ye2020studying}. To the best of our knowledge, no previous investigation has been conducted on generating long-form answers from large pretrained LMs for CBQA and leveraging them to predict answer. Experimental results on three QA benchmarks show that our method significantly outperforms previous \textit{closed-book} QA methods (e.g. exact matching 68.6\% vs. 55.3\%), and is on par with \textit{open-book} methods that exploit external knowledge sources (e.g. 68.6\% vs. 68.0\%). Our method is able to better exploit the stored knowledge in pretrained LMs without adding extra learnable parameters or needing finetuning, and paves the way for hybrid models that integrate pretrained LMs with external knowledge.

% Overall
In future work, we expect to develop LFQA systems that can provide more query-relevant and fact-aware answers, and also generalize well on updated new facts. In fact, in the three chapters of this thesis, we open new exciting research directions for LFQA system: 1) there are still possible ways worth to explore to build more query-relevant LFQA system. In Chapter 3, we try to improve query-relevance via the QFS task, while the quality of the relevant document retrieved from external corpus also affects the generation model. In future works, more sophisticated retrieval techniques such as iterative retrieval should be explored on the task. 2) building a fact-aware LFQA system is still in its infancy. In Chapter 4, we leverage a machine reading comprehension model to highlight the salient information in the retrieved documents. While combining other knowledge modalities such as the knowledge bases (KBs), or visual materials etc. to provide more informative and diverse knowledge, might also help improve the factual of the generated answer. 3) leverage the generated long-form answers might further help other tasks. In Chapter 5, we explored it's possibility to help improve open-domain QA. Actually the generated long-form answers could also be used as intermediate evidences on a reasoning task.

% discussion about retrieval system for LFQA, like iterative retrieval. blablabla. (2) faithfulness evaluation metrics, and datasets. (3) new-domain.. (4) with multiple knowledge sources...(5) human in the loop, reinforcemenet learning, to learn and evolve. (since some questions might be no forever correct answer, the answer could (6) explicitly reasoning process (since currently it is more like just information synthesis, but lacks real reasoning)..

