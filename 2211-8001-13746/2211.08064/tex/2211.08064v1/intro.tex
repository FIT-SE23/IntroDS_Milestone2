\section{Introduction}
\label{sec:introduction}
% \hangx{Zhongkai, Hang}



\begin{figure*}[!t]
    \centering
    \includegraphics[width=1\textwidth]{fig/fig0_new.pdf}
    \caption{
    An overview of physics-informed machine learning. Representation of physical prior from strong to weak could be divided into PDEs/ODEs/SDEs, Symmetry and intuitive physics. They could be incorporated into different parts of machine learning model like data, model architecture, loss function, optimizer and inference algorithm. Different methods are used in different types of tasks like neural simulation, inverse problems, CV/NLP and RL/control. There are still much space like physics-informed optimizer and physics-informed inference method to be explored in the field of physics-informed machine learning.
    % Representative applications in molecular dynamics, quantum chemistry, fluids dynamics and electromagnetism of physics-informed machine learning. These systems obey Schrödinger equation, Navier-Stokes equations and Maxwell's equations which are three of the most influential physical laws in science and engineering. Parts of the picture are from \cite{jumper2021highly, zhang2018deep, pathak2022fourcastnet, wu2022learning, chen2020physics}.
    }
    \label{summary}
\end{figure*}


The paradigm of scientific research in recent decades has undergone a revolutionary change with the development of computer technology. Traditionally, researchers used theoretical derivation combined with experimental verification to study natural phenomena. With the development of computational methods, a large number of methods based on computer numerical simulation have been developed to understand complex real systems. Nowadays, with the automation and batching of scientific experiments, scientists have accumulated a large amount of observational data. \emph{The paradigm of (data-driven) machine learning  is to understand and build models that leverages empirical data to improve performance on some set of tasks\cite{mitchell1990machine}.}
It is important to promote the development of modern science and engineering technology with the aid of learning from observational data. 

% \hangx{could include more attractive examples, thus making the AI researchers see promising future}
As part of the remarkable progress of machine learning in recent years, 
%deep learning has greatly improved the ability to extract knowledge from large amounts of data. 
deep neural networks \cite{lecun2015deep} have achieved milestone breakthroughs in the fields of computer vision \cite{he2016deep}, natural language processing \cite{devlin2018bert}, speech processing \cite{amodei2016deep}, and reinforcement learning \cite{silver2016mastering}. Their flexibility and scalability allow neural networks to be easily applied to many different domains, as long as there is a sufficient amount of data. The powerful abstraction ability of deep neural networks also motivates researchers to apply them on scientific problems in modeling physical systems.
%For example, large amount of works using deep neural networks with specific designed architectures are widely used to model and forecast physical systems like weather \cite{pathak2022fourcastnet}, chemical molecules \cite{schutt2017schnet}, biological proteins \cite{jumper2021highly} and robotics \cite{levine2016end}, etc.
For example, AlphaFold 2 \cite{jumper2021highly} has revolutionized the paradigm of protein structure prediction. Similarly, FourCastNet \cite{pathak2022fourcastnet} has built an ultra-large learning-based weather forecasting system that surpasses traditional numerical forecasting systems. Deep Potential\cite{zhang2018deep} proposed neural models for learning large-scale molecular potential satisfying symmetry.  

Though statistical machine learning based models achieve astonishing progress with the help of big data. There are still many limitations when these purely data-driven models are deployed as real world applications. For example, the prediction might be not robust, lack interpretability, and might violate physical constraints or commonsense. In computer vision, it is generally difficult for deep neural networks to recognize and understand the geometry, shape, texture and dynamics from images or videos, yielding the limitation on extrapolation ability. Moreover, these models are also shown to perform unsatisfactorily outside of training distribution \cite{shen2021towards} or to be easily attacked by human imperceptible adversarial noise \cite{goodfellow2014explaining}. In deep reinforcement learning, an agent could learn to take actions with higher rewards by trial-and-error but it does not recognize the underlying physical mechanism. In scientific problems, 
% Though there have been many efforts to apply deep learning methods in science and engineering, current statistical learning-based paradigms have limited generalization ability in these domains. These 
physical systems are usually constrained by some domain-specific physical laws, such as differential equations. And data collected in science and engineering tend to be sparse and noisy because real-world experiments are expensive and are disturbed by noise from environments or devices. Learning from sparse and noisy data leads to serious generalization errors in common machine learning models.
% Both theory and experiments show that the performance of existing machine learning models may degenerate if we ignore domain-specific physical laws \cite{karniadakis2021physics}.
% First, in theory, the generalization of data-driven machine learning models requires a large amount of data. However, Second, existing statistical learning models, which ignore physical constraints, usually result in solutions that do not conform to physical laws in practice. 
\emph{We suggest that one possible reason for the generalization error is that current statistic learning models that only rely on empirical data are not aware of the internal physical mechanism that generates the data.}  
 Taking humans as a reference, the ability of human beings to understand concise physical laws from data can help models learn, reason, and interact with the world more efficiently and robustly \cite{karniadakis2021physics,thuerey2021physics}. Enabling machine learning models to perceive physical laws or constraints is an open yet attractive area in the field of machine learning
\cite{karniadakis2021physics,clark2013whatever}. 
%Therefore, the combination of physical laws and machine learning, i.e. physics informed machine learning, has become an active topic in the field of machine learning.



%\hangx{what are the problems to address and what are the challenges?}

% Researchers have developed a variety of methods depending on the types of different physical systems. Most physical constraints can be divided into one of ordinary differential equations (ODE), partial differential equations (PDE), and stochastic differential equations (SDE). \hangx{a surprise here} 
% PINN\cite{raissi2019physics} and its variants \cite{kharazmi2021hp,jagtap2020extended} proposes to use NN to represent the solutions of differential equations. Methods like HNN \cite{greydanus2019hamiltonian} and HGN \cite{toth2019hamiltonian} design special network architectures that satisfy the Hamiltonian equation which can learn latent dynamics from higher-dimensional observations implicitly. Sindy \cite{brunton2016discovering} assumes that equations consist of several specific terms and learns the coefficients of these equations from the data using sparse regression. PDE-Net\cite{long2018pde} introduces similar ideas into PDEs using the fact that convolution kernels are equivalent to discrete differential operators. There are also some works \cite{sanchez2020learning,battaglia2018relational} that models the interaction between parts using graph structure. \hangx{why choose these works?}




Researchers have attempted many methods to combine physical knowledge with machine learning, depending on the context of the problem and the representation of physical constraints. Though there are numerous and complicated works, we could extract a concise and formalized concept for physics-informed machine learning (PIML). \emph{Specifically, the paradigm of physics-informed machine learning (PIML) is to build a model that leverages empirical data and available physical prior knowledge to improve performance on a set of tasks that involve a physical mechanism.} 
There are several basic issues in physics-informed machine learning. 

%We will provide an answer based on literature survey for existing works. 


\emph{First, what is the definition and representation of physical prior knowledge?}
In existing works, we could divide physical prior knowledge into several categories from strong to weak, i.e. PDEs/ODEs/SDEs or algebraic constraint, symmetry constraints and intuitive physical constraints. PDEs/ODEs/SDEs are ubiquitous in science and engineering. They also have analytical mathematical expressions so that they are easily integrated into machine learning models. For example, PINNs\cite{raissi2019physics} construct loss functions using PDEs/ODEs as regularization terms. NeuralODE \cite{chen2018neural} construct a novel neural architecture that obeys ODEs.  Symmetry constraints and intuitive physical constraints are weaker inductive bias than PDEs/ODEs. Examples of symmetry constraints are translation, rotation, and permutation invariance or equivariance. It is a widely used inductive biases when designing novel network architectures. For example, PointNet \cite{qi2017pointnet} and Graph Convolutional Networks (GCN) \cite{kipf2016semi} utilize the permutation invariance property of point cloud data and graph data. There are thousands of works following them and we will not discuss them in detail. 
Conservation laws in physics could also be regarded as symmetry constraints. Intuitive physics (or naive physics) \cite{ye2018interpretable} is interpretable physical commonsense about dynamics and constraints of objects in physical world. For example, "a solid object cannot pass through another solid" is a intuitive physical constraint that describes continuity of objects. Though intuitive physical constraints are essential and simple, how to represent them mathematical and systematically is still a challenging task currently.



\emph{Second, how is physical prior knowledge integrated into machine learning models?}
Training a machine learning model consists of several basic components, i.e. data, model architecture, loss functions, optimization algorithms and inference. Physical prior could be integrated into one or multiple of these components. First, data could augmented or synthesized for problems with symmetry constraints or known PDEs/ODEs. Models could learn from these generated data. Second, the architecture of the model may need to be redesigned and evaluated. Physical laws such as PDEs/ODEs, symmetry, conservation laws and the possible periodicity of data may require us to redesign the structure of the current neural network to meet the needs of practical problems. Third, loss functions and optimization methods for general deep neural networks may not be optimal for training models that incorporate physical constraints. For example, when physical constraints are used as regular term losses, the weight adjustment of each loss function is very important, and commonly used first-order optimizers such as Adam \cite{kingma2014adam} are not necessarily suitable for the training of such models.  



\emph{Third, what are the tasks of physics-informed machine learning?}
From the definition of physics-informed machine learning, we see that it also applies to problem settings for pure statistical machine learning like supervised learning, unsupervised learning,  semi-supervised learning, reinforcement learning, and so on. However, an essential condition is that physics-informed machine learning problems must involve real-world physical processes and we must know something about it. Otherwise, it degenerates into pure statistical learning. Existing works could be divided into two classes, i.e. using PIML to solve scientific problems and incorporating physical prior for solving traditional machine learning problems. In this paper, we will discuss these applications respectively. For the first class, human masters rich and precise physical laws to describe the scientific phenomenon, and they are usually represented as differential equations. There is more progress currently in the domain of PIML for scientific problems and we will highlight it when summarizing the methods. Specifically, these problems can be roughly
divided into two categories: \emph{neural simulation} (including neural solver and neural operator) and \emph{inverse problems} (e.g., inverse design and scientific discovery) of physical systems.
\emph{Neural simulation} aims to predict or forecast the states of systems using physical knowledge and available data. For example, solving a PDE system, predicting molecular properties, and forecasting weather in the future can be viewed as forward problems. By contrast, 
\emph{inverse problems} aim to find a physical system that satisfies data or given constraints, e.g., scientific discovery of PDEs from data, optimal control of PDE systems, etc. We have summarized the work proposed in recent years sorted by the chronological order in Figure \ref{timeline}. 
For the second class, incorporating physical knowledge might significantly help improve the performance in many computer vision and reinforcement learning tasks. In these domains, physical knowledge is vaguer and more difficult to represent than precise differential equations. Symmetry and intuitive physical constraints are more frequently used as physical priors to enhance machine learning models.






% Due to the diversity and complexity of physical constraints and practical problems, these questions have brought many new challenges and opportunities to the machine learning community, calling for consideration of the learning paradigm of combining physical prior knowledge with machine learning. \emph{First}, how to define and frame the problem is an open question. For example, in some scenarios, our physical priors are not complete, so how we use data and physical models is very important. \emph{Second}, the architecture of the model may need to be redesigned and evaluated. Physical laws such as conservation laws, symmetry, and the possible periodicity of data may require us to redesign the structure of the current neural network to meet the needs of practical problems. \emph{Third}, optimization methods for general deep neural networks may not be optimal for training models that incorporate physical constraints. For example, when physical constraints are used as regular term losses, the weight adjustment of each loss function is very important, and commonly used first-order optimizers such as Adam \cite{kingma2014adam} are not necessarily suitable for the training of such models. \emph{Finally}, in order to facilitate the participation of researchers in the machine learning community in this field, uniform and appropriate datasets and benchmarks are necessary. Since practical science and engineering fields often require a lot of domain-specific knowledge, finding common and representative problems and publishing them will help researchers in machine learning participate more deeply.

   
   
% To address the challenges above, we propose a high-level theoretical framework for dealing with machine learning problems with general physical priors. Specifically, our framework is based on probabilistic graphical models in statistical learning and we use latent variables to represent the real state of the system, which are random variables defined on a manifold that satisfy physical prior constraints. We consider the real observation to be a probability distribution that depends on the physical state of the system. Our goal is to use the observed data to learn the parameters of the model and inference using the learned model. It has the following advantages. First, we provide a unified view of machine learning problems with physical constraints  using a concise framework, where physical constraints can be understood as a prior on the data manifold. Second, our framework is highly flexible to handle physical systems governing by different kinds of constraints, and can be naturally extended to combine methods like autoencoders \cite{takeishi2021physics} and dynamic mode decomposition\cite{tu2013dynamic} to handle high-dimensional observational data. Third, based on our framework, we propose a physical bottleneck network, which can flexibly learn low-dimensional, physics aware representations from high-dimensional, noisy data according to the choice of physical priors.



Because this is an attractive research area, several related surveys have been published recently. \cite{karniadakis2021physics} presents a holistic picture of the development of physics-informed machine learning. \cite{cuomo2022scientific} is a very relevant study that focuses on algorithms and applications of PINNs. \cite{beck2020overview} reviews theoretical results using NNs for solving PDEs. Some studies have paid attention to subdomains or applications of physics-informed machine learning such as fluids mechanics \cite{cai2021physics}, uncertainty quantification\cite{psaros2022uncertainty}, domain decomposition \cite{heinlein2021combining}, and dynamic systems \cite{wang2021physics}. \cite{zubov2021neuralpde,cheung2021recent, blechschmidt2021three, pratama2021anns, das2022state} provide more examples, as well as tutorials with software. \cite{rai2020driven,meng2022physics, willard2020integrating,frank2020machine} focus on other hybrid modeling paradigms of machine learning with physical knowledge. In this survey, our key contribution is to summarize the development of physics-informed machine learning from the perspective of machine learning researchers. We provide a comprehensive survey on algorithms, theory, and applications, as well as proposing future challenges for physics-informed machine learning, which will substantially boost the community for interdisciplinary research.


% \hangx{summarize the most relevant surveys and clarify what are the differences for our paper. present why our survey is important (to position our work). Namely, the key contribution of the survey }

% task: supervised, rl...
% cv 

% physics 表示，
% task (SL, RL, )
% uncertainty

% 用词drawback->challenge

This review paper is organized as follows. We first mathematically introduce preliminaries and background. Then, we present the development of related physics-informed machine learning methods in scientific problems and traditional machine learning tasks like computer vision, reinforcement learning. For scientific problems, we highlight representative methods like PINNs, DeepONet as well as various current improved variants, theory, applications and unsolved challenges. We then summarize methods incorporating physical prior knowledge into computer vision and reinforcement learning respectively. Finally, we describe some representative and challenging tasks for the machine learning community.
% We then systematically categorize the application of machine learning methods that incorporate physical information in different disciplinary contexts. We go on to describe some representative and challenging tasks for the machine learning community. 
% Finally, we list the current datasets, software packages, and public benchmarks, and describe the limitations of the current work and future research directions. An illustration of physics-informed machine learning from different perspectives is shown in Figure (\ref{summary}).


% \begin{figure*}[!t]
%     \centering
%     \includegraphics[width=1\textwidth]{fig/fig1.pdf}
%     \caption{Summary of physics-informed machine learning. Physical systems are governed by parametric differential equations with boundary and initial conditions. Our goal is to combine prior knowledge (PDEs/ODEs/SDEs) and available data from experiments or simulation to solve forward or inverse problems. Forward problems mainly consist of neural solvers and neural operators. 
%     Inverse problems aim to optimize or estimate the optimal parameters for physical systems that satisfy certain conditions or data. There is a broad range of applications of physics-informed machine learning, including multiple domains of science and engineering. 
%     }
%     \label{summary}
% \end{figure*}



% \textbf{Incorporating physical priors in learning.} 
% The first paradiam is to force model to satisfy the physical priors by  using regularization or designing specific architecture. Methods like PINN/DGM \cite{raissi2019physics,sirignano2018dgm, han2018solving} uses the PDE loss as regularization if the form of PDE is known. Along this line, many works improved the technique \cite{kharazmi2021hp,jagtap2020conservative,jagtap2020extended} and applied it to various areas, like fluid dynamics\cite{cai2021physics,lucor2021physics,sun2020physics}, material science\cite{goswami2021physics,yin2021non}, stochastic systems \cite{chen2021learning,o2021stochastic} and quantum chemistry \cite{han2018solving,zhang2018deep}. Another class of methods design specific architectures to encode specific physical priors. For example, LNN \cite{cranmer2020lagrangian} and HNN \cite{greydanus2019hamiltonian} use neural networks to parameterize the Lagrangian and Hamiltonian of the systems. The applications include robotics \cite{lutter2019deep}, learning latent dynamics and generative modeling \cite{toth2019hamiltonian}. Some works further uses symplectic integrators \cite{chen2019symplectic,jin2020sympnets} to improve the performance. There are also works that use graphs to model the interactions between parts of the systems \cite{li2020visual,battaglia2018relational,sanchez2020learning}. 


% \textbf{Sparse dynamics discovery using machine learning.} Despite the success of encoding physical priors into neural networks, another scope of works aim to discover sparse and parsimonious laws from data using machine learning. SINDy \cite{brunton2016discovering,fasel2021ensemble,champion2019data} uses sparse regression to find dominate functions from a candidate set. Many variants are proposed to discover sparse physical laws and PDE/ODEs from data in various areas \cite{narasingam2018data,boninsegna2018sparse,rudy2019data,chen2020deep,chen2021physics,chen2020sparse}.
% PDE-Net \cite{long2018pde} uses convolutional kernels to match the numeric discretizetion of PDEs. Except from using sparse regression method, DSR \cite{petersen2019deep} uses reinforcement learning to find symbolic expressions that best fit the data. Some methods use other types of regularizations \cite{takeishi2021physics} to balance learning and physical priors for hybrid modeling \cite{linial2021generative,qian2021integrating,yildiz2019ode2vae} .



