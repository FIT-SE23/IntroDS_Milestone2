\section{Neural Simulation}
\label{sec:neural simulation}
% \hangx{Zhongkai, Songming}

Using neural network based methods for simulating physical systems governed by PDEs/ODEs/SDEs (named \emph{neural simulation}) is a fruitful and active research domain in physics-informed machine learning. 
In this section, we first list notations and background knowledge used in the paper. Neural simulation mainly consist of two parts, i.e. solving a single PDEs/ODEs using neural networks (named \emph{neural solver}) and learning solution maps of parametric PDEs/ODEs (named \emph{neural operator}). Then we will summarize problems, methods, theory and challenges for \emph{neural solver} and \emph{neural operator} in detail. 

% After that, we mathematically
% introduce the formulations of several fundamental problems of incorporating
% physical knowledge into machine learning.

\subsection{Notations and Prelininaries}


\subsubsection{Notations.}
We consider a physical system defined on a spatial or spatial-temporal domain $\Omega \subseteq \mathbb{R}^d$, where $u (\tmmathbf{x}) : \mathbb{R}^d \rightarrow \mathbb{R}^m$ is a vector of state variables, i.e., the physical quantities of interest, which is also a function of the spatial or spatial-temporal coordinates $\tmmathbf{x}$. The system is governed by physical laws which are characterized as partial differential equations (PDEs), ordinary differential equations
(ODEs), and stochastic differential equations (SDEs). Such equations are also called \textit{governing equations}, where the unknowns are exactly the state variables $u$. The system is
parameterized or controlled by $\theta \in
\Theta$. We note that $\theta$ could be either a vector or a function incorporated in the governing equations. More detailed notations are listed in Table~\ref{tb1}. In the following sections, we will continue using these notations unless otherwise stated.

% We consider a physical system with states $u (\tmmathbf{x}) : \mathbb{R}^d
% \rightarrow \mathbb{R}^m$ where $\tmmathbf{x} \in \Omega \subseteq
% \mathbb{R}^d$ is the spatial or spatial-temporal coordinates and $u \in \mathbb{R}^m$
% are called state variables. The system is dominated by physical laws which are
% usually partial differential equations (PDEs), ordinary differential equations
% (ODEs) or stochastic differential equations (SDEs). The system is
% parameterized or controlled by a parameter vector or control field $\theta \in
% \Theta$. Note that $\theta$ could be a vector and a function that influences
% the solutions of the ODEs/PDEs. \ We use a neural network parameterized by
% weights $w \in \mathbb{R}^n$. More detailed notations are listed in the
% following Table \ref{tb1}.

\begin{table}[h]
  \begin{tabular}{c|c}
\hline
Notations & Description \\ \hline
$u$ & state variables of the physical system \\ \hline
$\boldsymbol{x}$ & spatial or spatial-temporal coordinates \\ \hline
$x$ & spatial coordinates \\ \hline
$t$ & temporal coordinates \\ \hline
$\theta$ & parameters for a physical system \\ \hline
$w$ & weights of neural networks \\ \hline
$\frac{\partial}{\partial x_i}$ & partial derivatives operator \\ \hline
$\mathcal{D}^k_i$ & $\frac{\partial^k}{\partial x_i^k}$, $k$-order derivatives for variable $x_i$ \\ \hline
$\nabla$ & nabla operator (gradient) \\ \hline
$\Delta$ & Laplace operator \\ \hline
$\int$ & integral operator \\ \hline
$\mathcal{F}$ & differential operator representing the PDEs/ODEs \\ \hline
$\mathcal{I}$ & initial conditions (operator) \\ \hline
$\mathcal{B}$ & boundary conditions (operator) \\ \hline
$\Omega$ & spatial or spatial-temporal domain of the system \\ \hline
$\Theta$ & space of the parameters $\theta$ \\ \hline
$W$ & space of weights of neural networks \\ \hline
$\mathcal{L}$ & loss functions \\ \hline
$\mathcal{L}_r$ & residual loss \\ \hline
$\mathcal{L}_b$ & boundary condition loss \\ \hline
$\mathcal{L}_i$ & initial condition loss \\ \hline
$l_k$ & residual (error) terms \\ \hline
$\| \cdummy \|$ & norm of a vector or a function \\ \hline
\end{tabular}
  
  \caption{A table of mathematical notations used in this paper.}
  \label{tb1}
\end{table}


\begin{figure*}[t]
    \centering
    \includegraphics[width=18cm]{fig/fig3.pdf}
    \caption{ A chronological overview of important methods for neural simulation (neural solver and neural operator) and inverse problems (inverse design) of physics-informed machine learning.}
    \label{timeline}
\end{figure*}

\subsubsection{Differential Equations}
Real-world physical systems are dominated by complicated
differential equations. In many domains of science and engineering,
differential equations with different domain-specific assumptions and
simplification can be viewed as approximate models for characterizing these
systems. Here, we introduce basic knowledge of differential equations. We
consider a system with state variables $u (\tmmathbf{x}) \in \mathbb{R}^m$. 
$\tmmathbf{x} \in \Omega$ is the domain of definition. For simplicity, we
denote $\tmmathbf{x}$ as the spatial-temporal coordinates, i.e., $\tmmathbf{x}=
(x_1, \ldots, x_d) \in \Omega$ for time-independent systems and $\tmmathbf{x}=
(x_1, \ldots .x_{d - 1}, t) \in \Omega$ for time-dependent systems.

The system dominated by ODEs/PDEs can be represented in the following
equations:
\begin{eqnarray}
  \mathcal{F} (u ; \theta) (\tmmathbf{x}) & = & 0,  \\
  \mathcal{I} (u ; \theta) (x, t_0) & = & 0, \\
  \mathcal{B} (u ; \theta) (x, t) & = & 0. \nonumber
  \label{pde1}
\end{eqnarray}
Here, the differential equation can be mathematically formalized
as follows,
\begin{equation}
\small
  \mathcal{F} \left( u, \frac{\partial u}{\partial x_1}, \ldots \frac{\partial
  u}{\partial x_d}, \frac{\partial^2 u}{\partial x_1^2}, \frac{\partial^2
  u}{\partial x_1 \partial x_2}, \ldots \frac{\partial^2 u}{\partial x_d^2},
  \ldots ; \theta \right) (x_i, t) = 0, \label{f1}
\end{equation}
where $ x \in \Omega$ and $\mathcal{F}$ is an operator involving derivatives of the state variables that represent ODEs/PDEs (in this paper, we will not pay much attention to SDEs). Without confusion of notation, we rewrite equivalent forms of Equation~\eqref{f1} as follows:
\begin{equation}
  \mathcal{F} (u ; \theta) (\tmmathbf{x}) \equiv \mathcal{F} (u, \tmmathbf{x};
  \theta) = 0, x \in \Omega. \label{f2}
\end{equation}
For time-dependent cases (i.e., dynamic systems), we need to pose the initial conditions for state
variables and sometimes their derivatives at a certain time $t_0$,
\begin{equation}
\small
  \mathcal{I} \left( u, \frac{\partial u}{\partial x_1}, \ldots \frac{\partial
  u}{\partial x_d}, \frac{\partial^2 u}{\partial x_1^2}, \frac{\partial^2
  u}{\partial x_1 \partial x_2}, \ldots \frac{\partial^2 u}{\partial x_d^2},
  \ldots ; \theta \right) (x_i, t_0) = 0,  \label{i1}
\end{equation}
where $x \in \Omega_0$ and $\Omega_0 = \{ \tmmathbf{x}: \tmmathbf{x} \in \Omega, t = t_0 \}$.
For systems characterized by PDEs (here we consider ODEs as pure dynamic equations, where time is the only dimension), we need constraints for state variables on the boundary of the spatial domain $\partial \Omega$ to make the system well-posed. For boundary points $ x \in \partial \Omega$, we have the following boundary conditions,
\begin{equation}
\small
  \mathcal{B} \left( u, \frac{\partial u}{\partial x_1}, \ldots \frac{\partial
  u}{\partial x_d}, \frac{\partial^2 u}{\partial x_1^2}, \frac{\partial^2
  u}{\partial x_1 \partial x_2}, \ldots \frac{\partial^2 u}{\partial x_d^2},
  \ldots ; \theta \right) (x_i, t) = 0 . \label{b1}
\end{equation}
The initial conditions and boundary conditions can also be written in a
simplified form:
\begin{eqnarray}
  \mathcal{I} (u ; \theta) (x, t_0) & = & 0, x \in \Omega_0, \label{icbc} \\
  \mathcal{B} (u ; \theta) (x, t) & = & 0, x \in \partial \Omega. \nonumber
\end{eqnarray}

We can define $\mathcal{I} (u ; \theta) \triangleq 0$ and $\mathcal{B} (u ;
\theta) \triangleq 0$ if there are no corresponding constraints of initial conditions
and boundary conditions, so that Equation~\eqref{icbc} always holds.



% \subsubsection{Example PDEs}
% \hangx{maybe not necessary}
% In this section, we will give two illustrative examples of ODEs and PDEs, respectively, to help readers understand the background knowledge in the previous section. To begin with, we consider a RL circuit (see Figure~\ref{fig_ode_example}), where the dominating physical laws are Kirchhoff's circuit laws and corresponding governing equations, ODEs, are given by,
% \begin{eqnarray}
%   R u(t)+L\frac{\mathrm{d} u}{\mathrm{d} t} - V & =&  0, t\in (0,T],\label{eq_ode_example_1}\\
%   u(0) & = & 0.\label{eq_ode_example_2}
% \end{eqnarray}
% The RL circuit has a resistor with a value of $R$ and an inductor with a value of $L$ connected in series. At $t=0$, the switch is open and there is no current in the circuit. Then the switch is closed and a constant voltage $V$ is applied for $t > 0$. For this dynamic system, the state variable $u$ is the current intensity, the parameters are $\theta = (R,L,V)$, and the domain is $\Omega=[0,T]$. The ODE operator $\mathcal{F}$ (Equation~\eqref{eq_ode_example_1}) and the initial condition $\mathcal{I}$ (Equation~\eqref{eq_ode_example_2}) correspond to, respectively, the physical laws and the open circuit at $t=0$.

% \begin{figure}[t!]
% \begin{center}
% \centering
% \captionsetup{justification=centering}
% \includegraphics[width=.6\columnwidth]{fig/ode_example.pdf}
% \caption{An example of ODEs: series RL circuit.}
% \label{fig_ode_example}
% \end{center}
% \end{figure}

% In our second example, we consider the well-known Heat Equation which can be described as,
% In this section, we will give an example of PDEs, to help readers understand the background knowledge in the previous section. We consider a simple unsteady one-dimensional heat transfer system defined on the spatial domain of $[-L, L]$ as well  as the temporal domain of $[0, T]$, where the governing equation is the well-known Heat Equation,
% \begin{eqnarray}
%   \frac{\partial u}{\partial t} - k\frac{\partial^2 u}{\partial x^2} - f(x,t) & =&  0, x\in [-L, L],t\in [0,T],\label{eq_pde_example_1}\\
%   u(x,0) & = & \phi (x),x\in [-L, L], \label{eq_pde_example_2}\\
%   u(\pm L, t) & = & 0,t\in (0,T], \label{eq_pde_example_3}
% \end{eqnarray}
% where $u=u(x,t)$ is the state variable, the temperature field, $k$ and $\phi(x)$ are, respectively, the thermal conductivity and the initial distribution of the temperature, which are assumed to be constant, $f=f(x,t)$ represents the effect of an external heat source, which is a variable parameter of the system (i.e., $\theta=f$). Equation~\eqref{eq_pde_example_1}, \eqref{eq_pde_example_2}, \eqref{eq_pde_example_3} represent the PDE operator $\mathcal{F}$, the initial condition $\mathcal{I}$, and the boundary condition $\mathcal{B}$, respectively. The state of the physical system at each moment (i.e., the distribution of temperature $u=u(x,t)$) is uniquely determined given the above equation. Therefore, theoretically we can fully characterize the evolution of the system, relying only on this PDE. However, solving PDEs efficiently and exactly is usually a great challenge to the community, which will be discussed in detail later.


\subsubsection{Traditional Solvers for ODEs and PDEs}

Numerical methods are the main traditional solvers for ODEs/PDEs. These methods convert \textit{continuous} differential equations (original ODEs/PDEs or their equivalents) into \textit{discrete} systems of linear equations. Then, the equations are solved on (regular or irregular) meshes. For ODEs, the finite difference methods (FDM) \cite{causon2010introductory} are the most important ones, of which the Rungeâ€“Kutta method \cite{butcher1996history} is the most famous and representative. The FDM replaces the derivatives in the equations with numerical differences which are evaluated on meshes. For PDEs, in addition to the FDM (usually only applicable to geometrically regular PDEs), the finite volume methods (FVM) \cite{eymard2000finite} and the finite element methods (FEM) \cite{felippa2004introduction} are also commonly used mesh-based methods. Such methods consider the integral form equivalent to the original PDEs, and follow the idea of numerical integration to transform the original equations into a system of linear equations. In addition, in recent years, meshless methods (such as spectral methods \cite{bernardi1997spectral}, which are based on the series expansion) have been developed and become powerful solvers for PDEs.

Traditional solvers for ODEs/PDEs are relatively mature, and are of high precision and good stability with complete theoretical foundations. However, we have to point out some of the bottlenecks that severely limit their application. First, traditional solvers suffer from the ``curse of dimensionality''. Supposing that the number of grid nodes is $n$. A crude estimate of the time complexity is given by $\mathcal{O}(dn^r)$ for most traditional solvers \cite{xue2020amortized}, where $d\ge 1$ is the constant and $r$ generally satisfies that $r\approx 3$. Computational cost increases dramatically when the dimensionality of the problem becomes very high, making the computation time of the problem unacceptable. What is more, for nonlinear and geometrically complex PDEs, $d$ is far larger than $1$ and the cost is even worse (for many practical geometrically complex problems, although the dimension is only $3$ or $4$, the computation time can take weeks or even months). Second, traditional solvers have difficulty in incorporating data from experiments and cannot handle situations where the governing equations are (partially) unknown (such as inverse design, described in Section~\ref{sec_inverse_design}). This is because the theoretical basis of the traditional solvers requires the PDEs to be known; otherwise, no meaningful solution will be obtained. Further, these methods are usually not learning-based and cannot incorporate data, which makes it difficult to generalize them to new scenarios. 
%, rendering it difficult to incorporate data.

Although traditional solvers are still the most widely used at present, they face serious challenges. This provides an opportunity for neural network-based methods. First, neural networks have the potential to resist the ``curse of dimensionality''. In many application scenarios, the high-dimensional data can be well approximated by a much lower-dimensional manifold. \emph{With the help of generalizability, we believe they have the potential to learn such a lower-dimensional mapping and handle high-dimensional problems efficiently; we take the success of neural networks in computer vision \cite{yoo2015deep} as an example. Second, it is easy to incorporate data for neural networks, implicitly enabling knowledge extraction to  enhance prediction results. A simple way is to include the supervised data losses into the loss function and directly train the neural network with some gradient descent algorithm like SGD and Adam \cite{kingma2014adam}.} 

% However, these methods suffer from the ``curse of dimensionality''. When the geometry is complex or the spatial dimension is high, the number of nodes in the mesh will become very large, resulting in an unacceptable computational time of solving the system of linear equations (or calculating the series expansion)\hangx{provide more analysis if possible}
% . Furthermore, \hangx{more details}. In general, 
% \hangx{this paragraph is important, and could be rephrased. To point our the limitations of the previous method}
% \hangx{add one more paragraph to clarify why machine learning can address these issues }


