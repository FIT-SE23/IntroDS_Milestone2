

\section{Problem Formulation}
%In this section, we formally present the concept of physics-informed machine learning (PIML), a direction of machine learning that leverages both empirical data and physics prior to improve the performance on some tasks that involve a physical process. We first answer several basic questions about physics-informed machine learning. % using a mathematical framework. 


\begin{figure*}[!t]
    \centering
    \includegraphics[width=1\textwidth]{fig/fig0_new.pdf}
    \caption{
    An overview of physics-informed machine learning. We review various methods of incorporating physical prior knowledge into machine learning models, ranging from strong to weak forms, such as PDEs/ODEs/SDEs, symmetry, and intuitive physics. These physical priors can be incorporated into different aspects of machine learning models, such as data, model architecture, loss function, optimizer, and inference algorithm. We also highlight different applications of physics-informed machine learning in tasks such as neural simulation, inverse problems, CV/NLP, and RL/control. Finally, we identify some significant areas for exploration in the PIML field, such as physics-informed optimizers and physics-informed inference methods.
    %Representation of physical prior from strong to weak could be divided into PDEs/ODEs/SDEs, Symmetry and intuitive physics. They could be incorporated into different parts of machine learning model like data, model architecture, loss function, optimizer and inference algorithm. Different methods are used in different types of tasks like neural simulation, inverse problems, CV/NLP and RL/control. There are still much space like physics-informed optimizer and physics-informed inference method to be explored in the field of physics-informed machine learning.
    % Representative applications in molecular dynamics, quantum chemistry, fluids dynamics and electromagnetism of physics-informed machine learning. These systems obey Schrödinger equation, Navier-Stokes equations and Maxwell's equations which are three of the most influential physical laws in science and engineering. Parts of the picture are from \cite{jumper2021highly, zhang2018deep, pathak2022fourcastnet, wu2022learning, chen2020physics}.
    }
    \vspace{-1em}
    \label{summary}
\end{figure*}


In this section, we introduce the concept and commence by examining fundamental problems in physics-informed machine learning (PIML). We elaborate on the representation methodology for physical knowledge, the approach for integrating physical knowledge into machine learning models, and the practical problems that PIML resolves, as is illustrated in Figure~\ref{summary}.
 


% \junz{move the following into section 2 and make the statement more formal. The intro is mainly on motivating PIML and briefly summarize its significant progress. }
\iftrue

% \emph{First, what is the definition and representation of physical prior knowledge?}
% Physical prior knowledge refers to the understanding of the fundamental laws and principles of physics that describe the behavior of the physical world. This knowledge can be categorized into various types, ranging from strong to weak inductive biases, such as partial differential equations (PDEs), ordinary differential equations (ODEs), stochastic differential equations (SDEs), algebraic constraints, symmetry constraints, and intuitive physical constraints. PDEs, ODEs, and SDEs are prevalent in scientific and engineering domains and can be easily integrated into machine learning models, as they have analytical mathematical expressions. For example, PINNs \cite{raissi2019physics} use PDEs and ODEs as regularization terms in the loss function, while NeuralODE \cite{chen2018neural} construct a neural architecture that obeys ODEs. 

% Symmetry and intuitive physical constraints are weaker inductive biases than PDEs/ODEs, and they can be represented in various ways, such as designing network architectures that respect these constraints or incorporating them as regularization terms in the loss function. Symmetry constraints include translation, rotation, and permutation invariance or equivariance, which are widely used when designing novel network architectures. For instance, PointNet \cite{qi2017pointnet} and Graph Convolutional Networks (GCN) \cite{kipf2016semi} utilize the permutation invariance property of point cloud data and graph data. Conservation laws in physics can also be regarded as symmetry constraints. Intuitive physics, also known as naive physics, is the interpretable physical commonsense about the dynamics and constraints of objects in the physical world. For instance, the intuitive physical constraint ``a solid object cannot pass through another solid'' describes the continuity of objects. Although intuitive physical constraints are essential and straightforward, mathematically and systematically representing them remains a challenging task.





% In existing works, we could divide physical prior knowledge into several categories from strong to weak, i.e. PDEs/ODEs/SDEs or algebraic constraint, symmetry constraints and intuitive physical constraints. PDEs/ODEs/SDEs are ubiquitous in science and engineering. They also have analytical mathematical expressions so that they are easily integrated into machine learning models. For example, PINNs\cite{raissi2019physics} construct loss functions using PDEs/ODEs as regularization terms. NeuralODE \cite{chen2018neural} construct a novel neural architecture that obeys ODEs.  Symmetry constraints and intuitive physical constraints are weaker inductive bias than PDEs/ODEs. Examples of symmetry constraints are translation, rotation, and permutation invariance or equivariance. It is a widely used inductive biases when designing novel network architectures. For example, PointNet \cite{qi2017pointnet} and Graph Convolutional Networks (GCN) \cite{kipf2016semi} utilize the permutation invariance property of point cloud data and graph data. There are thousands of works following them and we will not discuss them in detail. 
% Conservation laws in physics could also be regarded as symmetry constraints. Intuitive physics (or naive physics) \cite{ye2018interpretable} is interpretable physical commonsense about dynamics and constraints of objects in physical world. For example, "a solid object cannot pass through another solid" is a intuitive physical constraint that describes continuity of objects. Though intuitive physical constraints are essential and simple, how to represent them mathematically and systematically is still a challenging task.



% \emph{Second, how is physical prior knowledge integrated into machine learning models?}
% The training of a machine learning model involves several fundamental components including data, model architecture, loss functions, optimization algorithms, and inference. The incorporation of physical prior knowledge can be achieved through modifications to one or more of these components. First, physical prior knowledge can be integrated into the data by augmenting or synthesizing it for problems with symmetry constraints or known partial differential equations (PDEs) or ordinary differential equations (ODEs). By training models on such generated data, they can learn to account for the physical laws that govern the problem.

% Moreover, the model architecture may need to be redesigned and evaluated to accommodate physical constraints. Physical laws such as PDEs/ODEs, symmetry, conservation laws, and periodicity of data may necessitate a rethinking of the structure of the neural network. Additionally, standard loss functions and optimization algorithms for deep neural networks may not be optimal for models that incorporate physical constraints. For instance, when physical constraints are used as regular term losses, the weight adjustment of each loss function is crucial, and commonly used first-order optimizers such as Adam are not necessarily suitable for training such models. Finally, for pre-trained machine learning models, different inference algorithms can be designed to enforce physical prior knowledge or improve interpretability. By incorporating physical prior knowledge into one or more of these components, machine learning models can achieve improved performance and better align with practical problems that adhere to the laws of physics.




% Training a machine learning model consists of several basic components, i.e. data, model architecture, loss functions, optimization algorithms and inference. Physical prior could be integrated into one or multiple of these components. First, data could be augmented or synthesized for problems with symmetry constraints or known PDEs/ODEs. Models could learn from these generated data. Second, the architecture of the model may need to be redesigned and evaluated. Physical laws such as PDEs/ODEs, symmetry, conservation laws and the possible periodicity of data may require us to redesign the structure of the current neural network to meet the needs of practical problems. Third, loss functions and optimization methods for general deep neural networks may not be optimal for training models that incorporate physical constraints. For example, when physical constraints are used as regular term losses, the weight adjustment of each loss function is very important, and commonly used first-order optimizers such as Adam \cite{kingma2014adam} are not necessarily suitable for the training of such models.   Finally, for pre-trained machine learning models, we might also design different inference algorithms to enforce physical prior or enhance interpretability.  




% \emph{Third, what are the tasks of physics-informed machine learning?}
% According to the definition of Physics-Informed Machine Learning (PIML), it can be applied to various problem settings of statistical machine learning such as supervised learning, unsupervised learning, semi-supervised learning, reinforcement learning, etc. However, PIML requires real-world physical processes, and we must have some knowledge about them; otherwise, it would turn into pure statistical learning. The existing works on PIML can be categorized into two classes: using PIML to solve scientific problems and incorporating physical priors to solve machine learning problems. 

% For the first class, precise physical laws, usually represented as differential equations, are used to describe scientific phenomena. PIML has made significant progress in this domain, which can be divided into ``neural simulation'' and ``inverse problems'' of physical systems. In particular, neural simulation aims to predict or forecast the states of systems using physical knowledge and available data, such as solving PDE systems, predicting molecular properties, and forecasting weather. On the other hand, inverse problems aim to find a physical system that satisfies data or given constraints, such as scientific discovery of PDEs from data and optimal control of PDE systems. Figure \ref{timeline} shows a chronological summary of recent work proposed in this area. For the second class, incorporating physical knowledge can significantly enhance building more effective, simple and robust machine learning models. Nevertheless, physical knowledge is frequently utilized as physical priors in various domains, including computer vision and computer graphics, where symmetry and intuitive physical constraints prevail. However, representing such physical knowledge can be more challenging than PDE equations.






% From the definition of physics-informed machine learning, we see that it also applies to problem settings for pure statistical machine learning like supervised learning, unsupervised learning,  semi-supervised learning, reinforcement learning, and so on. However, an essential condition is that PIML problems must involve real-world physical processes and we must know something about it. Otherwise, it degenerates into pure statistical learning. Existing works could be divided into two classes, i.e., using PIML to solve scientific problems and incorporating physical prior for solving traditional machine learning problems. In this paper, we will discuss these applications respectively. For the first class, human masters rich and precise physical laws to describe the scientific phenomenon, and they are usually represented as differential equations. There is more progress currently in the domain of PIML for scientific problems and we will highlight it when summarizing the methods. Specifically, these problems can be roughly
% divided into two categories: \emph{neural simulation} (including neural solver and neural operator) and \emph{inverse problems} (e.g., inverse design and scientific discovery) of physical systems.
% \emph{Neural simulation} aims to predict or forecast the states of systems using physical knowledge and available data. For example, solving a PDE system, predicting molecular properties, and forecasting weather in the future can be viewed as forward problems. By contrast, 
% \emph{inverse problems} aim to find a physical system that satisfies data or given constraints, e.g., scientific discovery of PDEs from data, optimal control of PDE systems, etc. We have summarized the work proposed in recent years sorted by chronological order in Figure \ref{timeline}. 
% For the second class, incorporating physical knowledge might significantly help improve the performance in many computer vision and reinforcement learning tasks. In these domains, physical knowledge is vaguer and more difficult to represent than precise differential equations. Symmetry and intuitive physical constraints are more frequently used as physical priors to enhance machine learning models.


\fi


\subsection{Representation of Physics Prior}

% We divide physical prior knowledge into several categories from strong to weak, i.e., PDEs/ODEs/SDEs or algebraic constraint, symmetry constraints and intuitive physical constraints.
% % PDEs/ODEs/SDEs are ubiquitous in science and engineering. 
% \emph{First, what is the definition and representation of physical prior knowledge?}

Physical prior knowledge refers to the understanding of the fundamental laws and principles of physics that describe the behavior of the physical world. This knowledge can be categorized into various types, ranging from strong to weak inductive biases, such as partial differential equations (PDEs),   symmetry constraints, and intuitive physical constraints. PDEs, ODEs, and SDEs are prevalent in scientific and engineering domains and can be easily integrated into machine learning models, as they have analytical mathematical expressions. For example, PINNs \cite{raissi2019physics} use PDEs and ODEs as regularization terms in the loss function, while NeuralODE \cite{chen2018neural} construct a neural architecture that obeys ODEs. 

Symmetry and intuitive physical constraints are weaker inductive biases than PDEs/ODEs, which can be represented in various ways, such as designing network architectures that respect these constraints or incorporating them as regularization terms in the loss function. Symmetry constraints include translation, rotation, and permutation invariance or equivariance, which are widely used when designing novel network architectures, e.g., PointNet \cite{qi2017pointnet} and Graph Convolutional Networks (GCN) \cite{kipf2016semi}. 
% For instance, PointNet \cite{qi2017pointnet} and Graph Convolutional Networks (GCN) \cite{kipf2016semi} utilize the permutation invariance property of point cloud data and graph data. Conservation laws in physics can also be regarded as symmetry constraints. 
Intuitive physics, also known as naive physics, is the interpretable physical commonsense about the dynamics and constraints of objects in the physical world. Although intuitive physical constraints are essential and straightforward, mathematically and systematically representing them remains a challenging task. We will elaborate on the different types of physical priors in the following. 


\subsubsection{Differential Equations}

Differential equations represent precise physical laws that can effectively describe various scientific phenomena. In this paper, we consider a physical system that exists on a spatial or spatial-temporal domain $\Omega \subseteq \mathbb{R}^d$, where $u (\tmmathbf{x}): \mathbb{R}^d \rightarrow \mathbb{R}^m$ denotes a vector of state variables, which are the physical quantities of interest, and also functions of the spatial or spatial-temporal coordinates $\tmmathbf{x}$. The physical laws governing this system are characterized by partial differential equations (PDEs), ordinary differential equations (ODEs), or stochastic differential equations (SDEs). These equations are known as the \textit{governing equations}, with the unknowns being the state variables $u$. The system is either parameterized or controlled by $\theta \in \Theta$, where $\theta$ could be either a vector or a function incorporated in the governing equations. Unless otherwise stated, Table~\ref{tb1} provides a detailed list of the notations used in the following sections.

% Differential equations are precise physical laws that can be used to describe scientific phenomena. We consider a physical system defined on a spatial or spatial-temporal domain $\Omega \subseteq \mathbb{R}^d$, where $u (\tmmathbf{x}) : \mathbb{R}^d \rightarrow \mathbb{R}^m$ is a vector of state variables, i.e., the physical quantities of interest, which is also a function of the spatial or spatial-temporal coordinates $\tmmathbf{x}$. The system is governed by physical laws which are characterized as partial differential equations (PDEs), ordinary differential equations
% (ODEs), or stochastic differential equations (SDEs). Such equations are also called \textit{governing equations}, where the unknowns are exactly the state variables $u$. The system is
% parameterized or controlled by $\theta \in
% \Theta$. We note that $\theta$ could be either a vector or a function incorporated in the governing equations. Table~\ref{tb1} list more detailed notations used in the sequel unless otherwise stated.

% We consider a physical system with states $u (\tmmathbf{x}) : \mathbb{R}^d
% \rightarrow \mathbb{R}^m$ where $\tmmathbf{x} \in \Omega \subseteq
% \mathbb{R}^d$ is the spatial or spatial-temporal coordinates and $u \in \mathbb{R}^m$
% are called state variables. The system is dominated by physical laws which are
% usually partial differential equations (PDEs), ordinary differential equations
% (ODEs) or stochastic differential equations (SDEs). The system is
% parameterized or controlled by a parameter vector or control field $\theta \in
% \Theta$. Note that $\theta$ could be a vector and a function that influences
% the solutions of the ODEs/PDEs. \ We use a neural network parameterized by
% weights $w \in \mathbb{R}^n$. More detailed notations are listed in the
% following Table \ref{tb1}.

\begin{table}[h]
  \begin{tabular}{c|c}
\hline
Notations & Description \\ \hline
$u$ & state variables of the physical system \\ \hline
$\boldsymbol{x}$ & spatial or spatial-temporal coordinates \\ \hline
$x$ & spatial coordinates \\ \hline
$t$ & temporal coordinates \\ \hline
$\theta$ & parameters for a physical system \\ \hline
$w$ & weights of neural networks \\ \hline
$\frac{\partial}{\partial x_i}$ & partial derivatives operator \\ \hline
$\mathcal{D}^k_i$ & $\frac{\partial^k}{\partial x_i^k}$, $k$-order derivatives for variable $x_i$ \\ \hline
$\nabla$ & nabla operator (gradient) \\ \hline
$\Delta$ & Laplace operator \\ \hline
$\int$ & integral operator \\ \hline
$\mathcal{F}$ & differential operator representing the PDEs/ODEs \\ \hline
$\mathcal{I}$ & initial conditions (operator) \\ \hline
$\mathcal{B}$ & boundary conditions (operator) \\ \hline
$\Omega$ & spatial or spatial-temporal domain of the system \\ \hline
$\Theta$ & space of the parameters $\theta$ \\ \hline
$W$ & space of weights of neural networks \\ \hline
$\mathcal{L}$ & loss functions \\ \hline
$\mathcal{L}_r$ & residual loss \\ \hline
$\mathcal{L}_b$ & boundary condition loss \\ \hline
$\mathcal{L}_i$ & initial condition loss \\ \hline
$l_k$ & residual (error) terms \\ \hline
$\| \cdummy \|$ & norm of a vector or a function \\ \hline
\end{tabular}
  
  \caption{A table of mathematical notations.}
  \label{tb1}
\end{table}


\begin{figure*}[t]
    \centering
    \includegraphics[width=18cm]{fig/fig3.pdf}
    \caption{ A chronological overview of important methods for neural simulation (neural solver and neural operator) and inverse problems (inverse design) of physics-informed machine learning. The earliest work could be traced back to \cite{dissanayake1994neural}.}
    \label{timeline}
\end{figure*}


%\subsection{Differential Equations}
%\junz{move this section (together with 2.3) to 2.1 as an important types of representation for physic prior ...}

% Real-world physical systems are dominated by complicated
% differential equations. In many domains of science and engineering,
% differential equations with different domain-specific assumptions and
% simplification can be viewed as approximate models for characterizing these
% systems. %Here, we introduce basic knowledge of differential equations. 
% Formally, we consider a system with state variables $u (\tmmathbf{x}) \in \mathbb{R}^m$. 
% $\tmmathbf{x} \in \Omega$ is the domain of definition. For simplicity, we
% denote $\tmmathbf{x}$ as the spatial-temporal coordinates, i.e., $\tmmathbf{x}=
% (x_1, \ldots, x_d) \in \Omega$ for time-independent systems and $\tmmathbf{x}=
% (x_1, \ldots .x_{d - 1}, t) \in \Omega$ for time-dependent systems.

% The system dominated by ODEs/PDEs can be represented in the following
% equations:

In many domains of science and engineering, real-world physical systems can be modeled using differential equations that are based on different domain-specific assumptions and simplifications. These models can be used to approximate the behavior of these systems. In this paper, we introduce the basic concepts of differential equations. Formally, we consider a system with state variables $u (\mathbf{x}) \in \mathbb{R}^m$, where $\mathbf{x} \in \Omega$ is the domain of definition. For simplicity, we use $\mathbf{x}$ to denote the spatial-temporal coordinates, i.e., $\mathbf{x}=(x_1, \ldots, x_d) \in \Omega$ for time-independent systems and $\mathbf{x}=(x_1, \ldots, x_{d-1}, t) \in \Omega$ for time-dependent systems. The behavior of the system can be represented by ordinary or partial differential equations (ODEs/PDEs) as follows:
\begin{align}
&\text{Partial differential equation: } \mathcal{F} (u ; \theta) (\tmmathbf{x})  =  0, \label{f1} \\
&\text{Initial Conditions: } 
  \mathcal{I} (u ; \theta) (x, t_0)  =  0, \\
&\text{Boundary Conditions: }   \mathcal{B} (u ; \theta) (x, t)  = 0. 
  \label{pde1}
  \end{align}
% Here, the differential equation can be mathematically formalized
% as follows,
% \begin{equation}
% \small
%   \mathcal{F} \left( u, \frac{\partial u}{\partial x_1}, \ldots \frac{\partial
%   u}{\partial x_d}, \frac{\partial^2 u}{\partial x_1^2}, \frac{\partial^2
%   u}{\partial x_1 \partial x_2}, \ldots \frac{\partial^2 u}{\partial x_d^2},
%   \ldots ; \theta \right) (x_i, t) = 0, \label{f1}
% \end{equation}
% where $ x \in \Omega$ and $\mathcal{F}$ is an operator involving derivatives of the state variables that represent ODEs/PDEs.
Without confusion of notation, we rewrite equivalent forms of Eq.~\eqref{f1} as:
\begin{equation}
  \mathcal{F} (u ; \theta) (\tmmathbf{x}) \equiv \mathcal{F} (u, \tmmathbf{x};
  \theta) = 0, x \in \Omega. \label{f2}
\end{equation}
For time-dependent cases (i.e., dynamic systems), we need to pose the initial conditions for state
variables and sometimes their derivatives at a certain time $t_0$ that can be described as 
$ \mathcal{I} (u ; \theta) (x, t_0) = 0, x \in \Omega_0$. 
% \begin{equation}
% \small
%   \mathcal{I} \left( u, \frac{\partial u}{\partial x_1}, \ldots \frac{\partial
%   u}{\partial x_d}, \frac{\partial^2 u}{\partial x_1^2}, \frac{\partial^2
%   u}{\partial x_1 \partial x_2}, \ldots \frac{\partial^2 u}{\partial x_d^2},
%   \ldots ; \theta \right) (x_i, t_0) = 0,  \label{i1}
% \end{equation}
% where $x \in \Omega_0$ and $\Omega_0 = \{ \tmmathbf{x}: \tmmathbf{x} \in \Omega, t = t_0 \}$.
For systems characterized by PDEs, 
%(here we consider ODEs as pure dynamic equations, where time is the only dimension),
we also need constraints for state variables on the boundary of the spatial domain $\partial \Omega$ to make the system well-posed. For boundary points $ x \in \partial \Omega$, we have the boundary conditions as $\mathcal{B} (u ; \theta) (x, t)  =  0, x \in \partial \Omega$. 
% \begin{equation}
% \small
%   \mathcal{B} \left( u, \frac{\partial u}{\partial x_1}, \ldots \frac{\partial
%   u}{\partial x_d}, \frac{\partial^2 u}{\partial x_1^2}, \frac{\partial^2
%   u}{\partial x_1 \partial x_2}, \ldots \frac{\partial^2 u}{\partial x_d^2},
%   \ldots ; \theta \right) (x_i, t) = 0 . \label{b1}
% \end{equation}
% The initial conditions and boundary conditions can also be written in a
% simplified form:
% \begin{eqnarray}
%   \mathcal{I} (u ; \theta) (x, t_0) & = & 0, x \in \Omega_0, \label{icbc} \\
%   \mathcal{B} (u ; \theta) (x, t) & = & 0, x \in \partial \Omega. \nonumber
% \end{eqnarray}
If there are no corresponding constraints of initial conditions and boundary conditions, we can define $\mathcal{I} (u ; \theta) \triangleq 0$ and $\mathcal{B} (u ; \theta) \triangleq 0$.
%, so that Eq.~\eqref{icbc} always holds.



% \subsubsection{Example PDEs}
% \hangx{maybe not necessary}
% In this section, we will give two illustrative examples of ODEs and PDEs, respectively, to help readers understand the background knowledge in the previous section. To begin with, we consider a RL circuit (see Figure~\ref{fig_ode_example}), where the dominating physical laws are Kirchhoff's circuit laws and corresponding governing equations, ODEs, are given by,
% \begin{eqnarray}
%   R u(t)+L\frac{\mathrm{d} u}{\mathrm{d} t} - V & =&  0, t\in (0,T],\label{eq_ode_example_1}\\
%   u(0) & = & 0.\label{eq_ode_example_2}
% \end{eqnarray}
% The RL circuit has a resistor with a value of $R$ and an inductor with a value of $L$ connected in series. At $t=0$, the switch is open and there is no current in the circuit. Then the switch is closed and a constant voltage $V$ is applied for $t > 0$. For this dynamic system, the state variable $u$ is the current intensity, the parameters are $\theta = (R,L,V)$, and the domain is $\Omega=[0,T]$. The ODE operator $\mathcal{F}$ (Equation~\eqref{eq_ode_example_1}) and the initial condition $\mathcal{I}$ (Equation~\eqref{eq_ode_example_2}) correspond to, respectively, the physical laws and the open circuit at $t=0$.

% \begin{figure}[t!]
% \begin{center}
% \centering
% \captionsetup{justification=centering}
% \includegraphics[width=.6\columnwidth]{fig/ode_example.pdf}
% \caption{An example of ODEs: series RL circuit.}
% \label{fig_ode_example}
% \end{center}
% \end{figure}

% In our second example, we consider the well-known Heat Equation which can be described as,
% In this section, we will give an example of PDEs, to help readers understand the background knowledge in the previous section. We consider a simple unsteady one-dimensional heat transfer system defined on the spatial domain of $[-L, L]$ as well  as the temporal domain of $[0, T]$, where the governing equation is the well-known Heat Equation,
% \begin{eqnarray}
%   \frac{\partial u}{\partial t} - k\frac{\partial^2 u}{\partial x^2} - f(x,t) & =&  0, x\in [-L, L],t\in [0,T],\label{eq_pde_example_1}\\
%   u(x,0) & = & \phi (x),x\in [-L, L], \label{eq_pde_example_2}\\
%   u(\pm L, t) & = & 0,t\in (0,T], \label{eq_pde_example_3}
% \end{eqnarray}
% where $u=u(x,t)$ is the state variable, the temperature field, $k$ and $\phi(x)$ are, respectively, the thermal conductivity and the initial distribution of the temperature, which are assumed to be constant, $f=f(x,t)$ represents the effect of an external heat source, which is a variable parameter of the system (i.e., $\theta=f$). Equation~\eqref{eq_pde_example_1}, \eqref{eq_pde_example_2}, \eqref{eq_pde_example_3} represent the PDE operator $\mathcal{F}$, the initial condition $\mathcal{I}$, and the boundary condition $\mathcal{B}$, respectively. The state of the physical system at each moment (i.e., the distribution of temperature $u=u(x,t)$) is uniquely determined given the above equation. Therefore, theoretically we can fully characterize the evolution of the system, relying only on this PDE. However, solving PDEs efficiently and exactly is usually a great challenge to the community, which will be discussed in detail later.



%They also have analytical mathematical expressions so that they are easily integrated into machine learning models. For example, PINNs\cite{raissi2019physics} construct loss functions using PDEs/ODEs as regularization terms. NeuralODE \cite{chen2018neural} construct a novel neural architecture that obeys ODEs.  

\subsubsection{Symmetry Constraints}
Symmetry constraints are considered a weaker inductive bias compared to partial differential equations (PDEs) or ordinary differential equations (ODEs). Symmetry constraints refer to a collection of transformations that can be applied to objects,where the abstract set of symmetries is capable of transforming diverse objects. Examples of symmetry constraints are translation, rotation, and permutation invariance or equivariance. In mathematics, symmetries are represented as invertible transformations that can be composed which can be formulated as the concept of groups~\cite{cohen2016group}.


Symmetries or invariants can be incorporated into machine learning to improve the performance of algorithms, depending on the type of data and problem being addressed. There are several types of symmetries, such as translation, rotation, reflection, scale, permutation, and topological invariance, which can be useful in different scenarios~\cite{bronstein2021geometric}. For example, translation invariance is important for data that is shift-invariant, like images or time-series data. Similarly, rotational symmetry is essential for data that is invariant to rotations~\cite{qi2017pointnet}, like images or point clouds, and reflection symmetry is critical for data that is invariant to reflections, such as images or shapes. Scale invariance is useful for data that is invariant to changes in scale, such as images or graphs, while permutation invariance is significant for data that is invariant to permutations of its elements, such as sets or graphs~\cite{kipf2016semi}. Finally, topological invariance is important for data that is invariant to topological transformations, such as shape or connectivity changes.

 The symmetry constraint is that for data $\tmmathbf{x} \in \mathcal{D}$,
there exist an operation $s : \mathcal{D} \rightarrow \mathcal{D}$, \ such
that the property function $\varphi (\cdummy): \mathcal{D}\rightarrow \mathbb{R}^k$ is the same under the symmetric
operation, i.e.
\begin{equation}
  \varphi (\tmmathbf{x}) = \varphi (s (\tmmathbf{x})).
\end{equation}

Incorporating symmetries or invariants can provide numerous advantages for machine learning models. These benefits include improved generalization performance, reduced data redundancy, increased interpretability, and better handling of complex data structures. Symmetries or invariants can aid in improving generalization by providing prior knowledge about the data and by training the model on a representative subset of the data, reducing redundancy. By incorporating symmetries or invariants, we can also gain insights into the underlying structure of the data, making the models more interpretable, especially in scientific or engineering applications. Finally, incorporating symmetries or invariants can be useful for handling complex data structures such as graphs or manifolds, which may not have a simple Euclidean structure. By respecting the underlying geometry of the data, we can design algorithms that can handle these complex symmetries or invariants.







\subsubsection{Intuitive Physics}
% \junz{elaborate more, especially using MLers familiar language/examples}


Intuitive physics refers to the common-sense knowledge about the physical world that humans possess that they use to reason about and make predictions, such as the understanding that objects fall to the ground when dropped. 
Integrating intuitive physics into machine learning involves incorporating this prior knowledge into the design of machine learning algorithms to improve their performance~\cite{piloto2022intuitive,ye2018interpretable}. There are several commonly used intuitive physics principles that can be incorporated into machine learning models such as ~\cite{duan2022survey}
\begin{itemize}
    \item Object permanence: The understanding that objects continue to exist even when they are no longer visible; 
    \item Gravity: The understanding that objects are attracted to each other with a force proportional to their mass and inversely proportional to the square of their distance; 
    \item Newton's laws of motion: The principles that describe the relationship between an object's motion and the forces acting upon it; 
    \item Conservation laws: The principles that describe the conservation of energy, momentum, and mass in physical systems. 
\end{itemize}

These principles can be used as physical priors or constraints in machine learning models to improve their accuracy, robustness, and interpretability including computer vision, robotics, and natural language processing. For example, object permanence can be used to improve object tracking algorithms by predicting the future location of an object based on its previous motion. Gravity can be used to simulate the behavior of objects in a physical environment, such as in physics-based games or simulations. Therefore, intuitive physics can help us to develop machine learning models that can reason about and predict the behavior of objects in the physical world.


However, intuitive physics is a challenging concept to formalize using traditional mathematical models and equations, hindering its integration into machine learning algorithms. In general, intuitive physics can be incorporated as constraints or regularizers to enhance machine learning models~\cite{raissi2019physics}. For instance, by including the conservation of energy or momentum as constraints, we can design models to predict the behavior of physical systems. Additionally, physical simulations can generate training data for machine learning models, improving their understanding of physical phenomena and validating their performance~\cite{sanchez2020learning,faroughi2022physics}. Finally, hybrid models that combine machine learning and physics can leverage the strengths of both approaches~\cite{xu2021bayesian}. For example, a physics-based model can generate initial conditions for a machine learning model, which can refine those predictions using observed data.






% Intuitive physics (or naive physics) \cite{ye2018interpretable} is interpretable physical commonsense about dynamics and constraints of objects in physical world. For example, "a solid object cannot pass through another solid" is a intuitive physical constraint that describes continuity of objects. Though intuitive physical constraints are essential and simple, how to represent them mathematically and systematically is still a challenging task.


\subsection{Possible Ways towards PIML}
A fundamental issue for PIML is how physical prior knowledge is integrated into machine learning models. As is illustrate in Figure~\ref{summary},
the training of a machine learning model involves several fundamental components including data, model architecture, loss functions, optimization algorithms, and inference. The incorporation of physical prior knowledge can be achieved through modifications to one or more of these components.



% We start by examining the definition of machine learning. According to \cite{mitchell1990machine}, machine learning is a field that designs models or algorithms which can leverage empirical data to improve their performance on some tasks. %with a general problem formulation of machine learning and see how physical prior is incorporated into different parts of machine learning. 
Formally, let $\mathcal{D}= \{ (\tmmathbf{x}_i, \tmmathbf{y}_i) \}$ denote a given training dataset. Machine learning tasks can be generally put as searching for a model $f$ from a hypothesis space $\mathcal{H}$. The performance of a particular model on dataset $\mathcal{D}$ is often characterized by a loss function 
$\mathcal{L} (f ; \mathcal{D})$. Then the problem is cast as solving an optimization objective as 
%training a machine learning model $f$ can be formulated as optimizing $f$ via a specific given loss function $\mathcal{L} (f ; \mathcal{D})$ and some possible regularization terms $\Omega (f ; \mathcal{D})$ over a hypothesis space $\mathcal{H}$, i.e.,
\begin{equation}\label{eq:ML-opt}
  \min_{f \in \mathcal{H}} \mathcal{L} (f ; \mathcal{D}) + \Omega (f) ,
\end{equation}
where $\Omega (f)$ is a regularization term that introduces some inductive bias for better generalization. 
Then, we solve problem \eqref{eq:ML-opt} using an optimizer $OPT(\cdot)$ that outputs a model $f$ from some initial guess $f_0$, i.e., $f  = OPT(\mathcal{H}, f_0)$.

Physics-informed machine learning is a direction of ML that aims to leverage physical prior knowledge and empirical data to improve performance on a set of tasks that involve a physical mechanism. 
%From the previous section we see that ``physics-informed'' means that the data is generated or obeys some latent physical laws,
%\begin{equation}
%  \mathcal{F} (\mathcal{D}) = 0,
%\end{equation}
%where $\mathcal{F}$ denotes an abstract form of physical laws. However, as mentioned above, 
Training a machine learning model consisting of several basic components, i.e. data, model architecture, loss functions, optimization algorithms, and inference. 
In general, there are various approaches to incorporating physical prior into different components of machine learning: 
% \junz{the items need rewrite, need to be more concrete and logically sound; also, this is a guideline of the rest materials ...}
\begin{itemize}
    \item \emph{Data}: we could augment or process the dataset utilizing
available physical prior like symmetry. Mathematically we have $\mathcal{D}_p
= P (\mathcal{D})$ where $P (\cdummy)$ denotes a preprocessing or augmentation
operation using physical prior. 
    \item \emph{Model}: we could embed physical prior into the
model design (e.g., network architecture). We usually achieve this by introducing inductive biases
guided by the physical prior into the hypothesis space, i.e., $f \in
\mathcal{H}_p \subseteq \mathcal{H}$.
    \item \emph{Objective}: we could design better loss
functions or regularization terms using given physical priors like ODE/PDE/SDEs,
i.e. replace $\mathcal{L} (f ; \mathcal{D})$ or $\Omega (f )$
with $\mathcal{L}_p (f ; \mathcal{D})$ or $\Omega_p (f )$.
    \item \emph{Optimizer}: we could design better optimization methods that are more stable or converge faster. We use $OPT_p$ to denote the optimizer that incorporates the physical prior.
    \item \emph{Inference}: we could enforce the physical constraints by using modifying the inference algorithms. For example, we could design a post-processing function $g_p$, we use $g_p (x, f (\tmmathbf{x}))$ instead of $f (\tmmathbf{x})$ when inferencing.
\end{itemize}


%\emph{Second, how is physical prior knowledge integrated into machine learning models?}Physical prior could be integrated into one or multiple of these components.
First, data could be augmented or synthesized for problems with symmetry constraints or known PDEs/ODEs. Models could learn from these generated data. Second, the architecture of the model may need to be redesigned and evaluated. Physical laws such as PDEs/ODEs, symmetry, conservation laws, and the possible periodicity of data may require us to redesign the structure of the current neural network to meet the needs of practical problems. Third, loss functions and optimization methods for general deep neural networks may not be optimal for training models that incorporate physical constraints. For example, when physical constraints are used as regular term losses, the weight adjustment of each loss function is very important, and commonly used first-order optimizers such as Adam \cite{kingma2014adam} are not necessarily suitable for the training of such models. Finally, for pre-trained machine learning models, we might also design different inference algorithms to enforce physical prior or enhance interpretability.  

First, physical prior knowledge can be integrated into the data by augmenting or synthesizing it for problems with symmetry constraints or known partial differential equations (PDEs) or ordinary differential equations (ODEs). By training models on such generated data, they can learn to account for the physical laws that govern the problem.
Second, the model architecture may need to be redesigned and evaluated to accommodate physical constraints. Physical laws such as PDEs/ODEs, symmetry, conservation laws, and periodicity of data may necessitate a rethinking of the structure of the neural network. Third, standard loss functions and optimization algorithms for deep neural networks may not be optimal for models that incorporate physical constraints. For instance, when physical constraints are used as regular term losses, the weight adjustment of each loss function is crucial, and commonly used first-order optimizers such as Adam are not necessarily suitable for training such models. Finally, for pre-trained machine learning models, different inference algorithms can be designed to enforce physical prior knowledge or improve interpretability. By incorporating physical prior knowledge into one or more of these components, machine learning models can achieve improved performance and better align with practical problems that adhere to the laws of physics.

\subsection{Tasks of PIML}

 Physics-Informed Machine Learning (PIML) can be applied to various problem settings of statistical machine learning such as supervised learning, unsupervised learning, semi-supervised learning, reinforcement learning, etc. However, PIML requires real-world physical processes, and we must have some knowledge about them; otherwise, it would turn into pure statistical learning. The existing works on PIML can be categorized into two classes: using PIML to solve scientific problems and incorporating physical priors to solve machine learning problems. 

 The field of physics-informed machine learning (PIML) has witnessed significant progress in addressing scientific problems that rely on accurate physical laws, often formulated by differential equations. PIML can be classified into two main categories, namely, ``neural simulation'' and ``inverse problems'' related to physical systems~\cite{karniadakis2021physics}. The neural simulation focuses on predicting or forecasting the states of physical systems using physical knowledge and available data. Examples of forward problems include solving PDE systems, predicting molecular properties, and forecasting future weather patterns. In contrast, inverse problems aim to identify a physical system that satisfies the given data or constraints. Examples of inverse problems include scientific discovery of PDEs from data and optimal control of PDE systems. The remarkable advancements in PIML have enabled the development of accurate models and efficient algorithms that combine physical knowledge and machine learning. This integration has opened up new opportunities for interdisciplinary research, enabling insights into complex problems across various fields such as computational biology, geophysics and environmental science~\cite{willard2022integrating}, etc. PIML has the potential to revolutionize scientific discovery and technological innovation. Figure \ref{timeline} shows a chronological summary of recent work proposed in this area. The ongoing research in this field continues to push the boundaries of what is possible.

% The field of physics-informed machine learning (PIML) has made considerable advancements in addressing scientific problems that rely on precise physical laws, often described by differential equations. PIML can be categorized into two main areas of ``neural simulation'' and ``inverse problems'' related to physical systems. In particular, \emph{Neural simulation} aims to predict or forecast the states of systems using physical knowledge and available data. For example, solving a PDE system, predicting molecular properties, and forecasting weather in the future can be viewed as forward problems. By contrast,. On the other hand, \emph{inverse problems} aim to find a physical system that satisfies data or given constraints, e.g., scientific discovery of PDEs from data, optimal control of PDE systems, etc. 



% Specifically, these problems can be roughly
% divided into two categories: \emph{neural simulation} (including neural solver and neural operator) and \emph{inverse problems} (e.g., inverse design and scientific discovery) of physical systems.  



% We have summarized the work proposed in recent years sorted by chronological order in Fig.~\ref{timeline}. 

Incorporating physical knowledge into machine learning models can significantly enhance their effectiveness, simplicity, and robustness. For instance, PIML can improve the efficiency and robustness of robots' design~\cite{bjelonic2023learning}. In computer vision, PIML can improve object detection and recognition and increase models' robustness to environmental changes~\cite{piloto2022intuitive}. PIML can also improve natural language processing models' ability to generate and comprehend text in numerous disciplines, and it can enhance the accuracy and efficiency of reinforcement learning models by integrating physical knowledge~\cite{ramesh2022physics}. By incorporating physical knowledge, PIML can overcome the limitations of traditional machine learning algorithms, which typically require large amounts of data to learn. Nevertheless, representing physical knowledge as physical priors in various domains, where symmetry and intuitive physical constraints prevail, can be more challenging than representing them as partial differential equations. Despite these challenges, the integration of PIML in AI has significant potential to enhance the performance and robustness of AI systems in various fields. 



 % For the first class, human masters rich and precise physical laws to describe the scientific phenomenon, and they are usually represented as differential equations. There is more progress currently in the domain of PIML for scientific problems and we will highlight it when summarizing the methods. 
 % Specifically, these problems can be roughly divided into two categories: \emph{neural simulation} (including neural solver and neural operator) and \emph{inverse problems} (e.g., inverse design and scientific discovery) of physical systems.  We have summarized the work proposed in recent years sorted by chronological order in Fig.~\ref{timeline}. 




% For the second class, incorporating physical knowledge might significantly help improve the performance in many computer vision and reinforcement learning tasks. In these domains, physical knowledge is vaguer and more difficult to represent than precise differential equations. Symmetry and intuitive physical constraints are more frequently used as physical priors to enhance machine learning models.



%The ODE/PDE/SDEs receive most attention of existing works and they have a clear mathematical formulation. In the next subsection we will introduce the notations and background of differential equations.









\section{Neural Simulation}
\label{sec:neural simulation}
% \hangx{Zhongkai, Songming}

Using neural network based methods for simulating physical systems governed by PDEs/ODEs/SDEs (named \emph{neural simulation}) is a fruitful and active research domain in physics-informed machine learning. 
In this section, we first list notations and background knowledge used in the paper. Neural simulation mainly consists of two parts, i.e. solving a single PDEs/ODEs using neural networks (named \emph{neural solver}) and learning solution maps of parametric PDEs/ODEs (named \emph{neural operator}). Then we will summarize problems, methods, theory and challenges for \emph{neural solver} and \emph{neural operator} in detail. 

\subsection{Challenges of Traditional ODEs/PDEs Solvers}
Numerical methods are the main traditional solvers for ODEs/PDEs. These methods convert \textit{continuous} differential equations (original ODEs/PDEs or their equivalents) into \textit{discrete} systems of linear equations. Then, the equations are solved on (regular or irregular) meshes. For ODEs, the finite difference methods (FDM) \cite{causon2010introductory} are the most important ones, of which the Runge–Kutta method \cite{butcher1996history} is most representative. The FDM replaces the derivatives in the equations with numerical differences which are evaluated on meshes. For PDEs, in addition to FDM (usually only applicable to geometrically regular PDEs), the finite volume methods (FVM) \cite{eymard2000finite} and the finite element methods (FEM) \cite{felippa2004introduction} are also commonly used mesh-based methods. Such methods consider the integral form equivalent to the original PDEs, and follow the idea of numerical integration to transform the original equations into a system of linear equations. In addition, in recent years, meshless methods (such as spectral methods \cite{bernardi1997spectral}, which are based on the series expansion) have been developed and become powerful solvers for PDEs.

Traditional solvers for ODEs/PDEs are relatively mature, and are of high precision and good stability with complete theoretical foundations. However, we have to point out some of the bottlenecks that severely limit their application. First, traditional solvers suffer from the ``curse of dimensionality''. Supposing that the number of grid nodes is $n$. A crude estimate of the time complexity is given by $\mathcal{O}(dn^r)$ for most traditional solvers \cite{xue2020amortized}, where $d\ge 1$ is the constant and $r$ generally satisfies that $r\approx 3$. Computational cost increases dramatically when the dimensionality of the problem becomes very high, making the computation time of the problem unacceptable. What is more, for nonlinear and geometrically complex PDEs, $d$ is far larger than $1$ and the cost is even worse (for many practical geometrically complex problems, although the dimension is only $3$ or $4$, the computation time can take weeks or even months). Second, traditional solvers have difficulty in incorporating data from experiments and cannot handle situations where the governing equations are (partially) unknown (such as inverse design, described in Section~\ref{sec_inverse_design}). This is because the theoretical basis of the traditional solvers requires the PDEs to be known; otherwise, no meaningful solution will be obtained. Further, these methods are usually not learning-based and cannot incorporate data, which makes it difficult to generalize them to new scenarios. 
%, rendering it difficult to incorporate data.

Although traditional solvers are still the most widely used at present, they face serious challenges. This provides an opportunity for neural network-based methods. First, neural networks have the potential to resist the ``curse of dimensionality''. In many application scenarios, the high-dimensional data can be well approximated by a much lower-dimensional manifold. \emph{With the help of generalizability, we believe they have the potential to learn such a lower-dimensional mapping and handle high-dimensional problems efficiently; we take the success of neural networks in computer vision \cite{yoo2015deep} as an example. Second, it is easy to incorporate data for neural networks, implicitly enabling knowledge extraction to  enhance prediction results. A simple way is to include the supervised data losses into the loss function and directly train the neural network with some gradient descent algorithm like SGD and Adam \cite{kingma2014adam}.} 


% After that, we mathematically
% introduce the formulations of several fundamental problems of incorporating
% physical knowledge into machine learning.



% However, these methods suffer from the ``curse of dimensionality''. When the geometry is complex or the spatial dimension is high, the number of nodes in the mesh will become very large, resulting in an unacceptable computational time of solving the system of linear equations (or calculating the series expansion)\hangx{provide more analysis if possible}
% . Furthermore, \hangx{more details}. In general, 
% \hangx{this paragraph is important, and could be rephrased. To point our the limitations of the previous method}
% \hangx{add one more paragraph to clarify why machine learning can address these issues }


