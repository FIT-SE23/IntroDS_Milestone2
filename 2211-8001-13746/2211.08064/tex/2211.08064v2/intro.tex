\section{Introduction}
\label{sec:introduction}
% \hangx{Zhongkai, Hang}





The paradigm of scientific research in recent decades has undergone a revolutionary change with the development of computer technology. Traditionally, researchers used theoretical derivation combined with experimental verification to study natural phenomena. With the development of computational methods, a large number of methods based on computer numerical simulation have been developed to understand complex real systems. Nowadays, with the automation and batching of scientific experiments, scientists have accumulated a large amount of observational data. \emph{The paradigm of (data-driven) machine learning is to understand and build models that leverage empirical data to improve performance on some set of tasks\cite{mitchell1990machine}.}
It is an important research area to promote the development of modern science and engineering technology with the aid of learning from observational data since we could extract a lot of information from data.

% \hangx{could include more attractive examples, thus making the AI researchers see promising future}
As part of the remarkable progress of machine learning in recent years, deep neural networks \cite{lecun2015deep} have achieved milestone breakthroughs in the fields of computer vision \cite{he2016deep}, natural language processing \cite{devlin2018bert}, speech processing \cite{amodei2016deep}, and reinforcement learning \cite{silver2016mastering}. Their flexibility and scalability allow neural networks to be easily applied to many different domains, as long as there is a sufficient amount of data. The powerful abstraction ability of deep neural networks also motivates researchers to apply them on scientific problems in modeling physical systems.
For example, AlphaFold 2 \cite{jumper2021highly} has revolutionized the paradigm of protein structure prediction. Similarly, FourCastNet \cite{pathak2022fourcastnet} has built an ultra-large learning-based weather forecasting system that surpasses traditional numerical forecasting systems. Deep Potential\cite{zhang2018deep} proposed neural models for learning large-scale molecular potential satisfying symmetry. The integration of prior knowledge of physics, which represents a high-level abstraction of natural phenomena or human behaviors, with data-driven machine learning models is becoming a new paradigm since it has the potential to facilitate novel discoveries and solutions to challenges across a diverse range of domains.
%For example, large amount of works using deep neural networks with specific designed architectures are widely used to model and forecast physical systems like weather \cite{pathak2022fourcastnet}, chemical molecules \cite{schutt2017schnet}, biological proteins \cite{jumper2021highly} and robotics \cite{levine2016end}, etc.


%Though statistical machine learning based models achieve astonishing progress with the help of big data. There are still many limitations when these purely data-driven models are deployed in real world applications.

Moreover, despite the impressive advancements of machine learning based models, there remain significant limitations when deploying purely data-driven models in real-world applications. 
In particular, data-driven machine learning models can suffer from several limitations such as a lack of robustness, interpretability, and adherence to physical constraints or commonsense reasoning. In computer vision, recognizing and understanding the geometry, shape, texture, and dynamics from images or videos can pose a significant challenge for deep neural networks, which can lead to limitations in their ability to extrapolate beyond their training data. Additionally, such models have demonstrated suboptimal performance outside of their training distribution \cite{shen2021towards} and are susceptible to adversarial attacks via human-imperceptible noise \cite{goodfellow2014explaining}. In deep reinforcement learning, an agent may learn to take actions that result in higher rewards through trial and error, but it may not understand the underlying physical mechanisms. These issues are particularly pertinent in scientific problems where the laws of physics and scientific principles govern the behavior of the system under study.
For example, data obtained from scientific and engineering experiments often tends to be sparse and noisy due to the high cost and the presence of environmental and device-related noise, which can result in significant generalization errors in common machine learning models. One possible explanation for the generalization errors observed in common statistical learning models is their sole reliance on empirical data without incorporating any understanding of the internal physical mechanisms that generate the data. By contrast, humans have the capacity to extract concise physical laws from data, which allows them to interact with the world more efficiently and robustly \cite{karniadakis2021physics,thuerey2021physics}. \emph{The integration of physical laws or constraints into machine learning models, therefore, presents new opportunities for traditional scientific research, substantially advancing the discovery of new knowledge, and facilitating the research in persistent issues of machine learning, such as robustness, interpretability, and generalization \cite{karniadakis2021physics,clark2013whatever}. }
% For example, the prediction might not be robust, lack interpretability, and might violate physical constraints or commonsense. In computer vision, it is generally difficult for deep neural networks to recognize and understand the geometry, shape, texture and dynamics from images or videos, yielding the limitation on extrapolation ability. Moreover, these models are also shown to perform unsatisfactorily outside of training distribution \cite{shen2021towards} or to be easily attacked by human imperceptible adversarial noise \cite{goodfellow2014explaining}. In deep reinforcement learning, an agent could learn to take actions with higher rewards by trial-and-error but it does not recognize the underlying physical mechanism. In scientific problems, 
% Though there have been many efforts to apply deep learning methods in science and engineering, current statistical learning-based paradigms have limited generalization ability in these domains. These 
% physical systems are usually constrained by some domain-specific physical laws, such as differential equations. And data collected in science and engineering tend to be sparse and noisy because real-world experiments are expensive and are disturbed by noise from environments or devices. Learning from sparse and noisy data leads to serious generalization errors in common machine learning models.
% Both theory and experiments show that the performance of existing machine learning models may degenerate if we ignore domain-specific physical laws \cite{karniadakis2021physics}.
% First, in theory, the generalization of data-driven machine learning models requires a large amount of data. However, Second, existing statistical learning models, which ignore physical constraints, usually result in solutions that do not conform to physical laws in practice. 
% \emph{We suggest that one possible reason for the generalization error is that current statistic learning models that only rely on empirical data are not aware of the internal physical mechanism that generates the data.}  
%  Taking humans as a reference, the ability of human beings to understand concise physical laws from data can help models learn, reason, and interact with the world more efficiently and robustly \cite{karniadakis2021physics,thuerey2021physics}. Enabling machine learning models to perceive physical laws or constraints is an open yet attractive area in the field of machine learning
% \cite{karniadakis2021physics,clark2013whatever}. 
%Therefore, the combination of physical laws and machine learning, i.e. physics informed machine learning, has become an active topic in the field of machine learning.



%\hangx{what are the problems to address and what are the challenges?}

% Researchers have developed a variety of methods depending on the types of different physical systems. Most physical constraints can be divided into one of ordinary differential equations (ODE), partial differential equations (PDE), and stochastic differential equations (SDE). \hangx{a surprise here} 
% PINN\cite{raissi2019physics} and its variants \cite{kharazmi2021hp,jagtap2020extended} proposes to use NN to represent the solutions of differential equations. Methods like HNN \cite{greydanus2019hamiltonian} and HGN \cite{toth2019hamiltonian} design special network architectures that satisfy the Hamiltonian equation which can learn latent dynamics from higher-dimensional observations implicitly. Sindy \cite{brunton2016discovering} assumes that equations consist of several specific terms and learns the coefficients of these equations from the data using sparse regression. PDE-Net\cite{long2018pde} introduces similar ideas into PDEs using the fact that convolution kernels are equivalent to discrete differential operators. There are also some works \cite{sanchez2020learning,battaglia2018relational} that models the interaction between parts using graph structure. \hangx{why choose these works?}




%Researchers have attempted many methods to combine physical knowledge with machine learning, depending on the context of the problem and the representation of physical constraints. Though there are numerous and complicated works, we could extract a concise and formalized concept for physics-informed machine learning (PIML). 

Numerous methods have been proposed by researchers to integrate physical knowledge with machine learning, which are tailored to the specific context of the problem and the representation of physical constraints. While the existing literature on this topic is extensive and multifaceted, \emph{we propose to establish a concise and formalized concept in the form of Physics-Informed Machine Learning (PIML), 
which is a paradigm that seeks to construct models that make use of both empirical data and prior physical knowledge to enhance performance on tasks that involve a physical mechanism.} In this survey, we propose a concise theoretical framework for machine learning problems with physical constraints, based on probabilistic graphical models using latent variables to represent the real state of a system that satisfies physical prior constraints. Our framework provides a unified view of such problems and is flexible in handling physical systems with various constraints, including high-dimensional observational data. It can be combined with methods like autoencoders and dynamic mode decomposition. Moreover, we introduce a physical bottleneck network that can learn low-dimensional, physics-aware representations from high-dimensional, noisy data based on the choice of physical priors.





As an attractive research area, several surveys have been recently published. Karniadakis \cite{karniadakis2021physics} provides a comprehensive overview of the historical development of PIML. Cuomo et al. \cite{cuomo2022scientific} focus on algorithms and applications of PINNs. Beck et al. \cite{beck2020overview} review the theoretical results obtained using NNs for solving PDEs. Other studies have focused on subdomains or applications of PIML, such as fluid mechanics \cite{cai2021physics}, uncertainty quantification \cite{psaros2022uncertainty}, domain decomposition \cite{heinlein2021combining}, and dynamic systems \cite{wang2021physics}. Zubov et al. \cite{zubov2021neuralpde}, Cheung et al. \cite{cheung2021recent}, Blechschmidt et al. \cite{blechschmidt2021three}, Pratama et al. \cite{pratama2021anns}, and Das et al. \cite{das2022state} provide further examples and tutorials with software. Additionally, Rai et al. \cite{rai2020driven}, Meng et al. \cite{meng2022physics}, Willard et al. \cite{willard2020integrating}, and Frank et al. \cite{frank2020machine} focus on other hybrid modeling paradigms that integrate machine learning with physical knowledge. In this survey, we summarize the developments in PIML from the perspective of machine learning researchers, providing a comprehensive review of algorithms, theory, and applications, and proposing future challenges for PIML that will advance interdisciplinary research in this area.

In this review paper, we begin by presenting mathematical preliminaries and background. We then discuss the development of physics-informed machine learning methods for both scientific problems and traditional machine learning tasks, such as computer vision and reinforcement learning. For scientific problems, we focus on representative methods like PINNs and DeepONet, as well as current improvements, theories, applications, and unsolved challenges. We also summarize the methods that incorporate physical prior knowledge into computer vision and reinforcement learning, respectively. Finally, we describe some representative and challenging tasks for the machine learning community.


% several related surveys have been published recently. \cite{karniadakis2021physics} presents a holistic picture of the development of physics-informed machine learning. \cite{cuomo2022scientific} is a very relevant study that focuses on algorithms and applications of PINNs. \cite{beck2020overview} reviews theoretical results using NNs for solving PDEs. Some studies have paid attention to subdomains or applications of physics-informed machine learning such as fluids mechanics \cite{cai2021physics}, uncertainty quantification\cite{psaros2022uncertainty}, domain decomposition \cite{heinlein2021combining}, and dynamic systems \cite{wang2021physics}. \cite{zubov2021neuralpde,cheung2021recent, blechschmidt2021three, pratama2021anns, das2022state} provide more examples, as well as tutorials with software. \cite{rai2020driven,meng2022physics, willard2020integrating,frank2020machine} focus on other hybrid modeling paradigms of machine learning with physical knowledge. In this survey, our key contribution is to summarize the development of physics-informed machine learning from the perspective of machine learning researchers. We provide a comprehensive survey on algorithms, theory, and applications, as well as proposing future challenges for physics-informed machine learning, which will substantially boost the community for interdisciplinary research.

% \emph{Specifically, the paradigm of physics-informed machine learning (PIML) is to build models that leverage empirical data and available physical prior knowledge to improve performance on a set of tasks that involve a physical mechanism.} 
% There are several basic issues in physics-informed machine learning. 

%We will provide an answer based on literature survey for existing works. 




% Due to the diversity and complexity of physical constraints and practical problems, these questions have brought many new challenges and opportunities to the machine learning community, calling for consideration of the learning paradigm of combining physical prior knowledge with machine learning. \emph{First}, how to define and frame the problem is an open question. For example, in some scenarios, our physical priors are not complete, so how we use data and physical models is very important. \emph{Second}, the architecture of the model may need to be redesigned and evaluated. Physical laws such as conservation laws, symmetry, and the possible periodicity of data may require us to redesign the structure of the current neural network to meet the needs of practical problems. \emph{Third}, optimization methods for general deep neural networks may not be optimal for training models that incorporate physical constraints. For example, when physical constraints are used as regular term losses, the weight adjustment of each loss function is very important, and commonly used first-order optimizers such as Adam \cite{kingma2014adam} are not necessarily suitable for the training of such models. \emph{Finally}, in order to facilitate the participation of researchers in the machine learning community in this field, uniform and appropriate datasets and benchmarks are necessary. Since practical science and engineering fields often require a lot of domain-specific knowledge, finding common and representative problems and publishing them will help researchers in machine learning participate more deeply.

   
   
% To address the challenges above, we propose a high-level theoretical framework for dealing with machine learning problems with general physical priors. Specifically, our framework is based on probabilistic graphical models in statistical learning and we use latent variables to represent the real state of the system, which are random variables defined on a manifold that satisfy physical prior constraints. We consider the real observation to be a probability distribution that depends on the physical state of the system. Our goal is to use the observed data to learn the parameters of the model and inference using the learned model. It has the following advantages. First, we provide a unified view of machine learning problems with physical constraints  using a concise framework, where physical constraints can be understood as a prior on the data manifold. Second, our framework is highly flexible to handle physical systems governing by different kinds of constraints, and can be naturally extended to combine methods like autoencoders \cite{takeishi2021physics} and dynamic mode decomposition\cite{tu2013dynamic} to handle high-dimensional observational data. Third, based on our framework, we propose a physical bottleneck network, which can flexibly learn low-dimensional, physics aware representations from high-dimensional, noisy data according to the choice of physical priors.






% \hangx{summarize the most relevant surveys and clarify what are the differences for our paper. present why our survey is important (to position our work). Namely, the key contribution of the survey }

% task: supervised, rl...
% cv 

% physics 表示，
% task (SL, RL, )
% uncertainty

% 用词drawback->challenge

%This review paper is organized as follows. We first mathematically introduce preliminaries and background. Then, we present the development of related physics-informed machine learning methods in scientific problems and traditional machine learning tasks like computer vision, reinforcement learning. For scientific problems, we highlight representative methods like PINNs, DeepONet as well as various current improved variants, theory, applications and unsolved challenges. We then summarize methods incorporating physical prior knowledge into computer vision and reinforcement learning respectively. Finally, we describe some representative and challenging tasks for the machine learning community.
% We then systematically categorize the application of machine learning methods that incorporate physical information in different disciplinary contexts. We go on to describe some representative and challenging tasks for the machine learning community. 
% Finally, we list the current datasets, software packages, and public benchmarks, and describe the limitations of the current work and future research directions. An illustration of physics-informed machine learning from different perspectives is shown in Figure (\ref{summary}).


% \begin{figure*}[!t]
%     \centering
%     \includegraphics[width=1\textwidth]{fig/fig1.pdf}
%     \caption{Summary of physics-informed machine learning. Physical systems are governed by parametric differential equations with boundary and initial conditions. Our goal is to combine prior knowledge (PDEs/ODEs/SDEs) and available data from experiments or simulation to solve forward or inverse problems. Forward problems mainly consist of neural solvers and neural operators. 
%     Inverse problems aim to optimize or estimate the optimal parameters for physical systems that satisfy certain conditions or data. There is a broad range of applications of physics-informed machine learning, including multiple domains of science and engineering. 
%     }
%     \label{summary}
% \end{figure*}



% \textbf{Incorporating physical priors in learning.} 
% The first paradiam is to force model to satisfy the physical priors by  using regularization or designing specific architecture. Methods like PINN/DGM \cite{raissi2019physics,sirignano2018dgm, han2018solving} uses the PDE loss as regularization if the form of PDE is known. Along this line, many works improved the technique \cite{kharazmi2021hp,jagtap2020conservative,jagtap2020extended} and applied it to various areas, like fluid dynamics\cite{cai2021physics,lucor2021physics,sun2020physics}, material science\cite{goswami2021physics,yin2021non}, stochastic systems \cite{chen2021learning,o2021stochastic} and quantum chemistry \cite{han2018solving,zhang2018deep}. Another class of methods design specific architectures to encode specific physical priors. For example, LNN \cite{cranmer2020lagrangian} and HNN \cite{greydanus2019hamiltonian} use neural networks to parameterize the Lagrangian and Hamiltonian of the systems. The applications include robotics \cite{lutter2019deep}, learning latent dynamics and generative modeling \cite{toth2019hamiltonian}. Some works further uses symplectic integrators \cite{chen2019symplectic,jin2020sympnets} to improve the performance. There are also works that use graphs to model the interactions between parts of the systems \cite{li2020visual,battaglia2018relational,sanchez2020learning}. 


% \textbf{Sparse dynamics discovery using machine learning.} Despite the success of encoding physical priors into neural networks, another scope of works aim to discover sparse and parsimonious laws from data using machine learning. SINDy \cite{brunton2016discovering,fasel2021ensemble,champion2019data} uses sparse regression to find dominate functions from a candidate set. Many variants are proposed to discover sparse physical laws and PDE/ODEs from data in various areas \cite{narasingam2018data,boninsegna2018sparse,rudy2019data,chen2020deep,chen2021physics,chen2020sparse}.
% PDE-Net \cite{long2018pde} uses convolutional kernels to match the numeric discretizetion of PDEs. Except from using sparse regression method, DSR \cite{petersen2019deep} uses reinforcement learning to find symbolic expressions that best fit the data. Some methods use other types of regularizations \cite{takeishi2021physics} to balance learning and physical priors for hybrid modeling \cite{linial2021generative,qian2021integrating,yildiz2019ode2vae} .



