%
\documentclass[11pt]{article}

\usepackage[english]{babel}


\renewcommand{\baselinestretch}{1.05}
\usepackage[utf8]{inputenc}

\usepackage{mathtools}
\usepackage{mathptmx}
\parskip=0.5ex

\usepackage[T1]{fontenc}
%
\usepackage[margin=1in]{geometry}
\parskip=0.5ex

\usepackage{amsmath}
\usepackage{amsthm,amssymb}
\usepackage{graphicx}
\usepackage[colorlinks=true, allcolors=blue]{hyperref}
\usepackage{cleveref}
\usepackage{wrapfig}
\usepackage{tikz}
\usepackage{cite}
\usepackage{xcolor}


\newcommand\sline{\noindent\rule[0.5ex]{\linewidth} }
\newcommand{\newEntry}[1]{\rule{15cm}{0.4mm}{\Large #1}}
\newcommand{\pg}[1]{\paragraph{#1}\mbox{}\\}
\newcommand{\norm}[1]{\left\lVert#1\right\rVert}
\newcommand{\C}{\mathbb{C}}
\newcommand{\B}{\mathcal{B}}
\newcommand{\E}{\mathbb{E}}
\newcommand{\R}{\mathbb R}
\newcommand{\Q}{\mathbb Q}
\newcommand{\eps}{\epsilon}
\newcommand{\bits}{\{0,1\}}
\newcommand{\bx}{\mathbf x}
\newcommand{\bu}{\mathbf u}
\newcommand{\bv}{\mathbf v}
\newcommand{\bw}{\mathbf w}
\newcommand{\ba}{\mathbf a}
\newcommand{\bb}{\mathbf b}
\newcommand{\bt}{\mathbf t}
\newcommand{\be}{\mathbf e}
\newcommand{\la}{\langle}
\newcommand{\ra}{\rangle}
\newcommand{\cF}{\mathcal{F}}
\newcommand{\N}{\mathbb N}
\newcommand{\cM}{\mathcal{M}}
\newcommand{\bA}{\mathbf A}
\newcommand{\bB}{\mathbf B}
\newcommand{\bF}{\mathbf F}
\newcommand{\Z}{\mathbb Z}
\renewcommand{\S}{\mathbb S}
\newcommand{\F}{\mathbb F}
\newcommand{\bX}{\mathbf X}
\newcommand{\bI}{\mathbf I}
\newcommand{\ar}{\operatorname{ar}}
\newcommand{\cSA}{\operatorname{cSA}}
\newcommand{\SA}{\operatorname{SA}}
\newcommand{\CZ}{\operatorname{KZ}}
\newcommand{\BLP}{\operatorname{BLP}}
\newcommand{\SDP}{\operatorname{SDP}}
\newcommand{\CLAP}{\operatorname{CLAP}}
\newcommand{\ULT}{\operatorname{ULT}}
\newcommand{\BW}{\operatorname{BW}}
\newcommand{\MAJ}{\operatorname{MAJ}}
\newcommand{\AT}{\operatorname{AT}}
\newcommand{\PAR}{\operatorname{Parity}}
\newcommand{\XOR}{\operatorname{XOR}}
\newcommand{\LIN}{\operatorname{LIN}}
\newcommand{\THR}{\operatorname{THR}}
\newcommand{\NAE}{\operatorname{NAE}}
\newcommand{\OR}{\operatorname{OR}}
\newcommand{\Pol}{\operatorname{Pol}}
\newcommand{\Ham}{\operatorname{Ham}}
\newcommand{\Spread}{\operatorname{Spread}}
\newcommand{\rMAJ}{\operatorname{rMAJ}}
\newcommand{\InvPol}{\operatorname{InvPol}}
\newcommand{\cA}{\mathcal A}
\newcommand{\PCSP}{\operatorname{PCSP}}
\newcommand{\CSP}{\operatorname{CSP}}
\newcommand{\KZ}{\operatorname{KZ}}
\newcommand{\Aut}{\operatorname{Aut}}
\newcommand{\dict}{\operatorname{dict}}
\newcommand{\triv}{\operatorname{triv}}
\newcommand{\junta}{\operatorname{junta}}
\newcommand{\id}{\operatorname{id}}
\newcommand{\per}{\operatorname{per}}
\newcommand{\cyc}{\operatorname{cyc}}

\newtheorem{theorem}{Theorem}[section]
\newtheorem{claim}[theorem]{Claim}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{problem}[theorem]{Problem}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{conjecture}[theorem]{Conjecture}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{remark}[theorem]{Remark}
\newtheorem{question}[theorem]{Question}
\newcommand{\si}{\Phi}




\title{SDPs and Robust Satisfiability of Promise CSP}
%
\author{}

\author{
Joshua Brakensiek \thanks{Stanford University, {\tt jbrakens@cs.stanford.edu}. Research supported in part by an NSF Graduate Research Fellowship and a Microsoft Research PhD Fellowship.}
\and 
Venkatesan Guruswami \thanks{University of California, Berkeley. {\tt venkatg@berkeley.edu}. Research supported in part by NSF grants CCF-2228287 and CCF-2211972 and a Simons Investigator award.}
\and 
Sai Sandeep\thanks{University of California, Berkeley. {\tt saisandeep@berkeley.edu}. Research supported in part by NSF grant CCF-2228287.} 
}

\date{}
\setcounter{page}{0}
\begin{document}

\maketitle
\thispagestyle{empty}

\begin{abstract}
For a constraint satisfaction problem (CSP), a robust satisfaction algorithm is one that outputs an assignment satisfying most of the constraints on instances that are near-satisfiable. It is known that the CSPs that admit efficient robust satisfaction algorithms are precisely those of bounded width, i.e., CSPs whose satisfiability can be checked by a simple local consistency algorithm (eg., 2-SAT or Horn-SAT in the Boolean case). While the exact satisfiability of a bounded width CSP can be checked by combinatorial algorithms, the robust algorithm is based on rounding a canonical Semi Definite Programming(SDP) relaxation.

\smallskip
In this work, we initiate the study of robust satisfaction algorithms for \emph{promise} CSPs, which are a vast generalization of CSPs that have received much attention recently. The motivation is to extend the theory beyond CSPs, as well as to better understand the power of SDPs. We present robust SDP rounding algorithms under some general conditions, namely the existence of majority or alternating threshold polymorphisms. 
%
On the hardness front, we prove that the lack of such polymorphisms makes the PCSP hard for all pairs of symmetric Boolean predicates. 
Our method involves a novel method to argue SDP gaps via the absence of certain colorings of the sphere, with connections to sphere Ramsey theory. 

\smallskip
We conjecture that PCSPs with robust satisfaction algorithms are precisely those for which the feasibility of the canonical SDP implies (exact) satisfiability. We also give a precise algebraic condition, known as a minion characterization, of which PCSPs have the latter property.
\end{abstract}

\newpage
\setcounter{tocdepth}{2} %
\tableofcontents
\pagenumbering{roman}
\newpage
\pagenumbering{arabic}


\section{Introduction}

Horn-SAT and 2-SAT are Boolean constraint satisfaction problems (CSPs) that admit simple combinatorial algorithms for satisfiability.
They are both examples of \emph{bounded width} CSPs: the existence of locally consistent assignments (which satisfy all local constraints involving some bounded number of variables, and which are consistent on the intersections) implies the existence of a global satisfying assignment.
%
%
%
%

While the simple local propagation algorithms for Horn-SAT and  2-SAT work when the instance is perfectly satisfiable, they are not robust to errors---if the given instance is almost satisfiable, the local consistency based algorithms do not guarantee solutions that satisfy almost all the constraints. 
In a beautiful work, Zwick~\cite{Zwick98} initiated the study of finding ``robust" algorithms for CSPs, namely algorithms that output solutions satisfying $1-f(\epsilon)$ fraction of the constraints when the instance is promised to be $1-\epsilon$ satisfiable, where $f(\epsilon) \rightarrow 0$ as $\epsilon \rightarrow 0$. Zwick obtained robust algorithms for $2$-SAT using SDP rounding and for Horn-SAT based on LP rounding. 
The PCP theorem together with Schaefer's reductions~\cite{Schaefer78} shows that Boolean CSPs that are NP-Hard are also APX-hard with perfect completeness, which in particular means that they do not admit robust satisfiability algorithms.
The only other interesting Boolean CSP besides Horn-SAT and 2-SAT for which satisfiability is polynomial-time decidable is Linear Equations modulo 2. H\aa stad~\cite{Has01} in his seminal work showed that even for $3$-LIN (when all equations involve just three variables), for every $\epsilon, \delta >0$, it is NP-Hard to output a solution satisfying $\frac{1}{2}+\delta$ fraction of the constraints even when the instance is guaranteed to have a solution satisfying $(1-\epsilon)$ fraction of the constraints.   

Unlike Horn-SAT or 2-SAT, the satisfiability algorithm for $3$-LIN is not local, and $3$-LIN does not have bounded width. Together with Schaefer's dichotomy theorem~\cite{Schaefer78}, this yields that for Boolean CSPs, bounded width characterizes robust satisfiability. For CSPs over general domains, a landmark result in the algebraic approach to CSP due to Barto and Kozik~\cite{BartoK14} showed that CSPs that are not bounded width can express linear equations. A reduction from H\aa stad's result then shows that CSPs that are not bounded width do not admit robust algorithms. Guruswami and Zhou~\cite{GuruswamiZ12} conjectured the converse---namely that all bounded width CSPs, over any domain, admit robust algorithms. Another work by Barto and Kozik~\cite{BartoK16} resolved this conjecture in the affirmative, thus giving a \emph{full characterization} of CSPs that have robust algorithms. 



%
In this work, we study robust algorithms for the class of \emph{Promise} Constraint Satisfaction Problems (PCSPs). PCSPs are a generalization of CSPs where each constraint has a strong form and a weak form. Given an instance that is promised to have a solution satisfying the stronger form of the constraints, the objective is to find a solution satisfying the weaker form of the constraints. A classic example of PCSPs is $(1$-in-$3$-SAT, NAE-$3$-SAT). While both the underlying CSPs are NP-Hard, the resulting PCSP does have a polynomial-time algorithm: given an instance of $1$-in-$3$-SAT that is promised to be satisfiable, we can find an assignment to the variables in polynomial time that satisfies each constraint as an NAE-$3$-SAT instance. PCSPs are a vast generalization of CSPs and capture key problems such as approximate graph and hypergraph coloring. 

Since their formal introduction in \cite{AGH17} and subsequent detailed study in \cite{BrakensiekG21} and \cite{BBKO21}, there has been a flurry of recent works on PCSPs~\cite{AsimiB21,AD21,AustrinBP20,BartoBB21,barto2022combinatorial,BrakensiekG19,BrakensiekG20,BrakensiekGWZ20,BGS21,BrandtsWZ21,ciardo2022clap}. These have led to a rich and still developing theory aimed at classifying the complexity of PCSPs, by tying their (in)tractability to the symmetries associated with their defining relations, and understanding the power and limitations of various algorithmic approaches influential for CSPs in the context of PCSPs.

Against this backdrop, we initiate the study of robust algorithms for PCSPs. The motivation is two-fold. First, as algorithms resilient to a small noise in the input, robust algorithms are important in their own right. Second, in the CSP world, the existence of efficient robust algorithms is equivalent to having bounded width and being decided by $O(1)$ levels of Sherali Adams for CSPs~\cite{ThapperZ17}. It is also proven~\cite{ThapperZ18} to be equivalent to being decided by the basic semidefinite programming (SDP) relaxation~\cite{Raghavendra08}. Here, we say that the basic SDP decides a CSP if for every instance $\Phi$ of the CSP, $\Phi$ has an assignment satisfying all the constraints if and only if there is a vector solution satisfying all the constraints in the SDP relaxation. 
For CSPs, therefore, the study of robust algorithms sheds light on, and in fact, precisely captures, the power of the most popular algorithmic approaches. Robust algorithms for PCSPs provide a rich context to understand how well these algorithmic tools generalize beyond CSPs. 


%

%

The main question that we are interested in this work is the following. 
\begin{question}
\label{ques:robust}
Which PCSPs admit polynomial time robust algorithms? 
%
\end{question}

As is the case with CSPs, a natural approach to characterize which PCSPs have robust algorithms is via the bounded width of PCSPs. However, it turns out that bounded width for PCSPs is weaker than having robust algorithms. 
Concretely, Atserias and Dalmau~\cite{AD21} have proved recently that the PCSP $(1$-in-$3$-SAT, NAE-$3$-SAT) does not have bounded width. Our work implies 
there is a robust algorithm for this PCSP. Atserias and Dalmau also proved that this PCSP is decided by $O(1)$ levels of Sherali-Adams, and as we shall prove later, it is also decided by the basic SDP.\footnote{We say that the basic SDP decides a PCSP (formally defined in~\Cref{sec:prelims}) if for every instance $\Phi$ of the PCSP, if there is a vector solution satisfying all the strong constraints in $\Phi$, then, $\Phi$ has an assignment satisfying all the weak constraints.} On the other hand, Raghavendra's framework of converting integrality gaps of CSPs to the hardness of approximation applies to PCSPs as well~\cite{Raghavendra08}. In particular, his result implies that every PCSP that is not decided by the basic SDP does not admit a polynomial-time robust algorithm, assuming the Unique Games Conjecture~\cite{Khot02}. This gives a powerful tool to show the absence of a polynomial time robust algorithm for a PCSP $\Gamma$ (albeit under the Unique Games Conjecture): showing an integrality gap for the basic SDP relaxation of $\Gamma$. With this connection,~\Cref{ques:robust} naturally leads to the following question.  
\begin{question}
\label{ques:sdp}
Which PCSPs are decided by the basic SDP relaxation?
\end{question}


%

%
We make progress on~\Cref{ques:robust} and~\Cref{ques:sdp} by studying the \emph{polymorphisms} of PCSPs. Polymorphisms are closure properties of satisfying solutions to (Promise) CSPs. As a concrete example, consider the $2$-SAT CSP: given an instance $\Phi$ of $2$-SAT over $n$ variables $x_1, x_2, \ldots, x_n$, suppose that $\textbf{u}, \textbf{v}, \textbf{w} $ are three assignments to these variables satisfying all the constraints in $\Phi$, then the assignment $\textbf{z}$ that is coordinatewise Majority operation on three bits, i.e., $z_i = \MAJ (u_i, v_i, w_i)$ for every $i \in [n]$, also satisfies all the constraints in $\Phi$. This shows that the Majority function on three variables is a polymorphism of the $2$-SAT CSP. More generally, the Majority function on any odd number of variables is a polymorphism of the $2$-SAT CSP. Similarly, the Parity function on any odd number of variables is a polymorphism of $3$-LIN. On the other hand, there are no non-trivial polymorphisms for $3$-SAT.
Polymorphisms are the central objects in the \textit{Universal algebraic approach} to CSPs~\cite{JeavonsCG97, Jeavons98, BulatovJK05,BartoKW17,Bul17,Zhuk20}, which has then been extended to PCSPs~\cite{BrakensiekG21, BBKO21}. 


At a high level, the existence of non-trivial polymorphisms implies algorithms, and vice-versa. The key challenge is to precisely characterize which polymorphisms lead to algorithms. It is known that the polymorphism family of a PCSP fully captures its computational complexity, i.e., if there are PCSPs $\Gamma, \Gamma'$ such that the polymorphism family of $\Gamma$, $\Pol(\Gamma)$ is contained in $\Pol(\Gamma')$, then $\Gamma'$ is formally easier than $\Gamma$, i.e., there is a \textit{gadget reduction} from $\Gamma'$ to $\Gamma$. It turns out that this gadget reduction preserves the existence of robust algorithms as well. Thus,~\Cref{ques:robust} and~\Cref{ques:sdp} can be rephrased as \textit{Which polymorphisms lead to robust algorithms for PCSPs? Which polymorphisms lead to being decided by the basic SDP relaxation?}  

We make progress on these questions on two fronts: first, for a large class of Boolean symmetric\footnote{A predicate $P$ is symmetric if for every satisfying assignment $(x_1, \hdots, x_n)$ to $P$, any permutation of that assignment also satisfies $P$. For a Boolean predicate whether an assignment satisfies a predicate depends only on the Hamming weight. A PCSP is said to be symmetric if all the predicates in the template are symmetric.} 
PCSPs where we allow negation of variables, we characterize the polymorphisms that lead to robust algorithms. Our algorithms are based on novel rounding schemes for the basic SDP relaxation, and our hardness results are proved using integrality gaps for the basic SDP relaxation.  
%
%
Second, towards understanding the power of basic SDP for promise CSPs, we introduce a minion $\mathcal{M}$ and show that a PCSP $\Gamma$ can be decided by basic SDP if and only if there is a minion homomorphism from $\mathcal{M}$ to the minon of polymorphisms of $\Gamma$. 


\subsection{Our results: Robust algorithms and hardness}


As is the case with CSPs, if a PCSP is NP-Hard, then it does not admit polynomial time robust algorithms, assuming $\text{P} \neq \text{NP}$. Thus, Question~\ref{ques:robust} is only relevant for PCSPs that can be solved in polynomial time. A large class of PCSPs for which polynomial time solvability has been fully characterized is the Boolean symmetric PCSPs. 
In~\cite{BrakensiekG21}, the authors showed that a Boolean symmetric PCSP with folding (i.e., we allow negating the variables) can be solved in polynomial time if and only if it contains at least one of Alternate-Threshold $(\AT)$, Majority ($\MAJ$) or $\PAR$ polymorphisms of all odd arities.\footnote{Later an analogous result was shown without the folding restriction in~\cite{FicakKOS19}.} 

%

\smallskip \noindent \textbf{Robust algorithms.} Our main algorithmic result shows that in two of these cases when the PCSP has $\MAJ$ or $\AT$ polymorphisms of all odd arities, the PCSP admits a robust algorithm. 

\begin{theorem}
\label{thm:main-algorithm}
Every Boolean folded PCSP $\Gamma$ that contains $\AT$ or $\MAJ$ polymorphisms of all odd arities admits a polynomial time robust algorithm. In particular, 
\begin{enumerate}
    \item If $\Gamma$ contains $\MAJ$ polymorphisms of all odd arities, then for every $\epsilon>0$, there exists a polynomial time algorithm that given an instance of $\Gamma$ that is promised to have a solution satisfying $1-\epsilon$ fraction of the constraints, outputs a solution satisfying $1-\tilde O(\epsilon^{\frac{1}{3}})$ fraction of the constraints.\footnote{Here, $\tilde O$ hides multiplicative poly logarithmic factors.}
    \item If $\Gamma$ contains $\AT$ polymorphisms of all odd arities, then for every $\epsilon >0$, there exists a polynomial time algorithm that outputs a solution satisfying $1-O\left(\frac{\log \log \frac{1}{\epsilon}}{\log \frac{1}{\epsilon}}\right)$ fraction of the constraints on an instance promised to have a solution satisfying $1-\epsilon$ fraction of the constraints. 
\end{enumerate}
\end{theorem}

Similar to the robust algorithms for CSPs~\cite{Zwick98,CharikarMM09,BartoK16}, our robust algorithms for PCSPs with $\MAJ$ and $\AT$ polymorphisms are based on rounding the basic SDP relaxation. 
The main challenge here is to obtain robust algorithms for a large class of PCSPs without access to the predicates and just using the properties of their polymorphisms. We achieve this using a combination of polymorphic tools where we use the fact that the PCSP contains $\AT$ or $\MAJ$ polymorphisms to deduce structural properties of the underlying predicate pairs, and SDP rounding tools where we then use these structural properties to get a robust algorithm. 

For $\MAJ$ polymorphisms, we first reduce the problem to the case when every weak predicate is of the form $k$-SAT. We then show that the robust algorithm of Charikar, Makarychev, and Makarychev~\cite{CharikarMM09} for the $2$-SAT CSP generalizes to these classes of PCSPs.
%
%
While the analysis of~\cite{CharikarMM09} is tailored towards $2$-SAT, we give a completely different analysis that is not based on predicates and instead uses the existence of $\MAJ$ polymorphisms as a black box. Similarly, for the $\AT$ polymorphisms, we first use the properties of $\AT$ polymorphisms to reduce to the \emph{weighted hyperplane} PCSP that generalizes the $(1$-in-$3$-SAT, NAE-$3$-SAT) PCSP. We then give a robust algorithm for the weighted hyperplane PCSP based on a random threshold rounding technique. A detailed overview of our algorithmic ideas appears in Sections~\ref{sec:maj-overview} and~\ref{sec:at-overview}.

\smallskip\noindent\textbf{Hardness results.}
Unlike our robust algorithms, which work for general Boolean PCSP with the said polymorphisms, in our hardness results, we rely on the symmetry of the predicates defining the PCSP. 
Furthermore, we assume that the PCSP contains a single predicate pair $\Gamma =(P,Q)$ that does not admit $\AT$ or $\MAJ$ polymorphisms of all odd arities. We show that for such Boolean symmetric folded PCSPs, the basic SDP relaxation has an integrality gap with perfect completeness, i.e., there is a finite instance on which the SDP relaxation satisfies all the strong constraints with zero error but the instance is not satisfiable even using the weak constraints. 
By Raghavendra's framework connecting SDP gaps and Unique-Games hardness~\cite{Raghavendra08}, the integrality gap rules out robust satisfaction algorithms (under the Unique Games conjecture (UGC)~\cite{Khot02}). 
%

%
%

%

%
%


%

\begin{theorem}
\label{thm:main-hardness}
Let $\Gamma=(P,Q)$ be a pair of Boolean predicates such that  $\AT_{L_1},\MAJ_{L_2} \notin \Pol(\Gamma)$ for some odd integers $L_1, L_2$. Then, under the UGC, unless $\text{P}=\text{NP}$, there is no polynomial time robust algorithm for the PCSP associated with $\Gamma$, where we allow negating variables and setting constants in the constraints.
\end{theorem}

%
%

%
%
%

Similar to our algorithmic result, we first use the properties of the polymorphisms to reduce to a small set of fixed template PCSPs. We obtain integrality gaps for these PCSPs for the basic SDP relaxation, which then implies robust hardness under the UGC.  For CSPs, strong integrality gaps~\cite{Schoenebeck08,Tulsiani09,SchoenebeckTT07} are known for the basic SDP relaxation and its strengthenings such as the Lasserre hierarchy, almost all of them being random constructions. For the case of PCSPs, analyzing the random constructions is trickier since we need to sample the constraints with a precise density such that there is a vector solution to the strong constraints, but the weak constraints are not satisfied. Instead, we take the opposite approach where we first construct the vector solution and then add all the constraints that the vector solution satisfies. This is similar in spirit to Feige and Schechtman's integrality gap~\cite{FeigeS01} for MAX-CUT where they first sampled $n$ uniformly random points on a $d$-dimensional sphere and then added edges between every pair of points whose distance falls within a preset range. 

In particular, we first construct an infinite integrality gap instance where the vertex set corresponds to the $n$-dimensional sphere $\S^n$ for a large integer $n$ and there are constraints for every tuple of vertices whose corresponding vectors satisfy the SDP constraints. For the set of fixed template PCSPs that we study, we show that this instance is not satisfiable, even using weak constraints. A compactness argument then implies the existence of a finite integrality gap instance. 
As we shall see later, by using our minion characterization result, showing that the infinite instance has no satisfiable assignment is a necessary step to obtain a finite integrality gap instance. 
Toward showing that the infinite instance does not have an assignment satisfying all the weak constraints, we study colorings of the sphere $f:\S^n \rightarrow \{-1,+1\}$ and use a result of Matoušek and Rödl~\cite{matouvsek1995ramsey} from \emph{sphere Ramsey theory} where the existence of monochromatic configurations in colorings of the sphere are studied. While their result directly applies to some PCSPs, for others, we combine their result with new techniques to prove the existence of structured configurations in sphere colorings. 
A more detailed overview appears in Section~\ref{sec:sdpgap-overview}. 

\smallskip\noindent\textbf{The power of SDPs and robust PCSP algorithms.}
Both our algorithmic and hardness results crucially use the basic SDP relaxation.
As our algorithms for the $\AT$ and $\MAJ$ polymorphisms are based on rounding the basic SDP, we get that every Boolean folded PCSP that contains $\AT$ or $\MAJ$ polymorphisms is decided by the basic SDP. On the hardness front,~\Cref{thm:main-hardness}, shows that a vast majority of Boolean symmetric folded PCSPs without $\AT$ or $\MAJ$ polymorphisms cannot be decided by the basic SDP. This suggests a more general relation between the basic SDP and robust algorithms for PCSPs. At an intuitive level, for both the existence of robust algorithms and being decided by the basic SDP, the underlying requirement seems to be the existence of polymorphism families that are robust to noise.  While our results show that this is true for the PCSPs that we study in this paper (noise stability is one crucial aspect that distinguishes $\MAJ$ and $\AT$ from $\PAR$.), we believe this is a more general phenomenon and motivates us to make the following conjecture. 
%
%
\begin{conjecture}
\label{conj:sdp-robust}
A PCSP $\Gamma$ has a polynomial time robust algorithm if $\Gamma$ is decided by the basic SDP relaxation. Else, there is no polynomial time robust algorithm for $\Gamma$, unless $\text{P}=\text{NP}$.
\end{conjecture}

As mentioned earlier, if there is an integrality gap for $\Gamma$ with respect to the basic SDP relaxation, then by Raghavendra's~\cite{Raghavendra08} result, we get that $\Gamma$ does not have a polynomial time robust algorithm, assuming the Unique Games Conjecture. This already proves one direction of~\Cref{conj:sdp-robust}. The other direction is more interesting: can we obtain robust algorithms for PCSPs just using the fact that basic SDP decides them? 
We remind the reader that the conjecture is already proven for CSPs, where the existence of robust algorithms~\cite{BartoK16} and decidability by basic SDP~\cite{ThapperZ18} are both shown to be equivalent to having bounded width. 

\subsection{Minion characterization of basic SDP}

%

In addition to our concrete characterization of robust algorithms for a subfamily of PCSPs, we also present a novel algebraic characterization of which PCSPs can be decided via basic SDP. Originally, in the study of CSPs, such algebraic characterizations were structured as follows (e.g., \cite{Bul17,Zhuk20}).


%
\begin{itemize}
\vspace{-1ex}
\item \emph{``Algorithm $\mathcal A$ solves $\operatorname{CSP}(\Gamma)$, if and only if there is a polymorphism $f \in \Pol(\Gamma)$ with specific properties.''}
\end{itemize}
%

Since the early days of PCSPs,  it has been known that a single polymorphism cannot dictate hardness  (c.f., \cite{BrakensiekG21}), and thus one must instead consider a \emph{sequence} of polymorphisms (e.g., \cite{BrakensiekGWZ20}):
%
\begin{itemize}
\vspace{-1ex}
\item \emph{``Algorithm $\mathcal A$ solves $\PCSP(\Gamma)$, if and only if there is an infinite sequence of polymorphism $f_1, f_2, \hdots  \in \Pol(\Gamma)$ with specific properties.''}
\end{itemize}
%
However, in many cases, such a characterization is unfeasible or unwieldy. Instead, a more general approach, pioneered by \cite{BBKO21}, captures the structure of polymorphism via a \emph{minion} (formally defined in Section~\ref{sec:minion}). 
A key property of the polymorphisms of a PCSP $\Gamma$ is that the function family $\Pol(\Gamma)$ is closed under taking \emph{minors}\footnote{A function $f:D_1^n \rightarrow D_2$ of arity $n$ is said to be a minor of another function $g:D_1^m \rightarrow D_2$ of arity $m$ with respect to a mapping $\pi:[m]\rightarrow [n]$ such that $f(x_1,x_2,\ldots,x_n)=g(x_{\pi(1)},x_{\pi(2)},\ldots,x_{\pi(m)})$ for every $\textbf{x}\in D_1^n$.}.
A minion is an abstraction based on this: it is a collection of objects each with an arity, and for every object $a$ of arity $m$, and a mapping $\pi : [m]\rightarrow [n]$, there is a unique object $b$ of arity $n$ that is said to be a minor of $a$ w.r.t. $\pi$. A \emph{minion homomorphism} is a mapping between minions that preserves the minor operation. 
A powerful way to capture the limits of algorithms for PCSPs is via minion homomorphisms: 
%
%
%
\begin{itemize}
\vspace{-1ex}
\item \emph{``Algorithm $\mathcal A$ solves $\PCSP(\Gamma)$, if and only if there is minion homomorphism from $\mathcal M_{\mathcal A}$ to $\Pol(\Gamma)$.''}
\end{itemize}
%


Many recent papers \cite{BrakensiekGWZ20,ciardo2022clap,ciardo2022sherali} have proven such characterizations in various contexts. Our contribution to this line of work is showing that the basic SDP can be captured by a minion, which we call $\mathcal M_{\SDP}$.

\begin{theorem}\label{thm:minion}
The basic SDP decides a PCSP $\Gamma$ if and only if there is a minion homomorphism from $\mathcal M_{\SDP}$ to $\Pol(\Gamma)$.
%
\end{theorem}

We note that a similar minion was concurrently and independently discovered by Ciardo-Zivny \cite{cz22-minion}. The theorem applies equally to Boolean and non-Boolean PCSPs.

%

The construction of the $\cM_{\SDP}$ minion is inspired by the vector interpretation of solutions to the basic SDP. Each object in the minion is a collection of orthogonal vectors which sum to a reference vector $\bv_0$. The minors involve adding groups of vectors together. Having a minion homomorphism from $\cM_{\SDP}$ to $\Pol(\Gamma)$ implies that there are polymorphisms of $\Gamma$ whose minors behave exactly like combining orthogonal vectors.

Proving Theorem~\ref{thm:minion} has a few technical hurdles. One challenge is that SDP solutions may require vectors of an arbitrarily large dimension. For these arbitrarily-large dimensional relationships to be captured in our minion, we have that the families of vectors making up $\cM_{\SDP}$ reside in a (countably) infinite-dimensional vector space. Similar techniques have been used in other minion constructions \cite{ciardo2022clap,ciardo2022sherali}.

Another challenge that appears specifically unique to this paper is that a Basic SDP solution gives a vector corresponding to each variable, but for the proof to go through additional vectors are needed which correspond to the constraints. (The variable vectors are "projections" of the constraint vectors.) Obtaining such constraints would typically be done via Sum-of-Squares or a related routine, but we prove that including such vector constraints are without loss of generality. That is, any basic SDP solution can be extended to a solution that includes constraint vectors without modifying the original variable vectors. This gives us enough vector structure to prove that the minion homomorphism corresponds to the basic SDP solution. 

%
%

 \medskip\noindent\textbf{Relation with sphere colorings.} 
 %
 By a result of \cite{BBKO21}, there is a minion homomorphism $\cM_{\SDP} \to \Pol(\Gamma)$ if and only if there is an assignment satisfying all the constraints in a ``universal'' instance of $\PCSP(\Gamma)$ known as a \emph{free structure}. In the case that $\Gamma$ is a Boolean folded PCSP, this free structure for $\cM_{\SDP}$ turns out to be an instance where every possible unit vector is a variable. The constraints correspond to collections of vectors that satisfy the corresponding basic SDP constraints. This is precisely the same infinite instance that we use to show integrality gaps. 
 Thus, the result of~\cite{BBKO21} translates to the Boolean folded PCSPs world as stating that a Boolean folded PCSP $\Gamma$ is decided by the basic SDP if and only if there is an assignment satisfying all the constraints in the infinite integrality gap instance. 
 For the general theory of approximation of basic SDPs, similar constructs with sphere coloring being a `universal' gap have appeared in the literature (eg. in~\cite{brakensiek2021mysteries}). 
 %
 %



\bigskip\noindent\textbf{Organization of the paper.}
We first start by introducing formal definitions and some general observations in~\Cref{sec:prelims} (experts familiar with SDPs and CSPs can skip or just skim this section). We then give a detailed technical overview of our results in~\Cref{sec:overview}. We provide our algorithmic results (\Cref{thm:main-algorithm}) in~\Cref{sec:alg} and prove the hardness results (\Cref{thm:main-hardness}) in~\Cref{sec:ug-hardness}. We propose and establish properties of the basic SDP minion in ~\Cref{sec:minion}.
Finally, we conclude in~\Cref{sec:conclusion} with several intriguing challenges and open problems raised by our work.



\section{Preliminaries}
\label{sec:prelims}


\noindent \textbf{Notations.} We use $[n]$ to denote the set $\{1,2,\ldots, n\}$. A predicate or a relation over a domain $D$ of arity $k$ is a subset of $D^k$.
For a relation $P \subseteq [q]^k$ of arity $k$, we abuse the notation and use $P$ both as a subset of $[q]^k$, and also as a function $P:[q]^k \rightarrow \{0,1\}$. We use boldface letters to denote vectors and roman letters to denote their elements, e.g., $\textbf{x}=(x_1, x_2,\ldots,x_k)$. We have $\S^n := \{ \textbf{v} \in \R^{n+1}: \norm{\textbf{v}}_2=1\}$.
For a vector $\textbf{v} \in D_1^k$ and a function $f:D_1\rightarrow D_2$, we use $f(\textbf{v})\in D_2^k$ to denote $(f(v_1),f(v_2),\ldots,f(v_k))$.


For a vector $\textbf{x} \in \{-1,+1\}^k$, we use $\textsf{hw}(\textbf{x})$ to denote the number of $+1$s in $\textbf{x}$, i.e., $\textsf{hw}(\textbf{x})=\frac{k+\sum_{i=1}^nx_i}{2}$. For $S \subseteq \{0,1,\ldots,k\}$, we use $\Ham_k S$ to denote $\{\textbf{x} \in \{-1,+1\}^k : \textsf{hw}(\textbf{x})\in S\}$. We use $\NAE_k$ to denote the set $\Ham_k \{1,2,\ldots,k-1\}$, and $k$-SAT to denote the set $\Ham_k \{1,2,\ldots,k\}$.
For vectors $\textbf{x}, \textbf{y} \in \R^n$, we use $\textbf{x}\cdot \textbf{y}$ and $\langle \textbf{x}, \textbf{y}\rangle$ interchangeably to denote $\sum_i x_i y_i$. 

\subsection{PCSPs and polymorphisms}
%
We first define Constraint Satisfaction Problems(CSP). 
\begin{definition}(CSP) Let $\Gamma=\{P_1, P_2, \ldots, P_l\}$ be a finite set of predicates over a finite domain $D$, where $P_i \subseteq D^{k_i}$. 
In an instance $\Phi=(V,\mathcal{C})$ of $\CSP(\Gamma)$, the Constraint Satisfaction Problem(CSP) associated with the predicate set $\Gamma$, we have a set of $n$ variables $V =\{u_1, u_2, \ldots, u_n\}$ that are to be assigned values from $D$. There are $m$ constraints $\mathcal{C}=\{C_1, C_2, \ldots, C_m\}$ 
each consisting of a tuple of variables $C_j = (u_{j,1}, u_{j,2}, \ldots, u_{j,{l_j}}) \in V^{l_j}$ and an associated predicate $P^{(j)} \in \Gamma$ of the same arity $l_j$. 
An assignment $\sigma : V \rightarrow D$ is said to satisfy the constraint $C_j$ if $\sigma(C_j)=(\sigma(u_{j,1}), \sigma(u_{j,2}), \ldots, \sigma(u_{j,{l_j}})) \in P^{(j)}$. There are two computational problems associated with $\CSP(\Gamma)$.

\begin{enumerate}
    \item In the decision version of $\CSP(\Gamma)$, the objective is to decide if there is an assignment  that satisfies all the constraints.
    \item In the search version of $\CSP(\Gamma)$, the objective is to find an assignment that satisfies all the constraints. 
\end{enumerate}
 
\end{definition}
%

\smallskip 

We next define Promise Constraint Satisfaction Problems (PCSP).
\begin{definition}(PCSP)
	In a Promise Constraint Satisfaction Problem $PCSP(\Gamma)$ over a pair of domains $D_1, D_2$, we have a finite set of pairs of predicates $\Gamma = \{ (P_1,Q_1), (P_2,Q_2),\ldots, (P_l,Q_l)\}$ such that for every $i \in [l]$, $P_i$ is a subset of $D_1^{k_i}$ and $Q_i$ is a subset of $D_2^{k_i}$. 
	Furthermore, there is a mapping $h:D_1\rightarrow D_2$ such that for all $i\in[l]$ and $\textbf{x} \in D_1^{k_i}$, $\textbf{x} \in P_i$ implies $h(\textbf{x}) \in Q_i$. 
	
	In an instance $\Phi=(V,\mathcal{C})$ of $\PCSP(\Gamma)$, we have a set of $n$ variables $V =\{u_1, u_2, \ldots, u_n\}$ and $m$ constraints $\mathcal{C}=\{C_1, C_2, \ldots, C_m\}$ 
each consisting of a tuple of variables $C_j = (u_{j,1}, u_{j,2}, \ldots, u_{j,{l_j}}) \in V^{l_j}$ and an associated predicate pair $(P^{(j)},Q^{(j)}) \in \Gamma$ of the same arity $l_j$. 
An assignment $\sigma_1 : V \rightarrow D_1$ is said to strongly satisfy the constraint $C_j$ if $\sigma(C_j)\in P^{(j)}$, and an assignment $\sigma_2 : V \rightarrow D_2$ is said to weakly satisfy the constraint $C_j$ if $\sigma(C_j) \in Q^{(j)}$. The following are computational problems associated with $\PCSP(\Gamma)$. 
	\begin{enumerate}
		\item In the decision version of $\PCSP(\Gamma)$, given an input instance $\Phi=(V,\mathcal{C})$ of $\PCSP(\Gamma)$, the objective is to distinguish between the two cases.
  \begin{enumerate}
\item   There is an assignment $\sigma_1 : V \rightarrow D_1$ that strongly satisfies all the constraints. 
		\item There is no assignment $\sigma_2 : V \rightarrow D_2$ that weakly satisfies all the constraints.  
	\end{enumerate}
 \item In the search version of $\PCSP(\Gamma)$, given an input instance $\Phi=(V,\mathcal{C})$ of $\PCSP(\Gamma)$ with the promise that there is an assignment $\sigma_1: V\rightarrow D_1$ that strongly satisfies all the constraints, the objective is to find an assignment $\sigma_2: V \rightarrow D_2$ that weakly satisfies all the constraints. 
 \item In the robust version of $\PCSP(\Gamma)$, given an input instance $\Phi=(V,\mathcal{C})$ of $\PCSP(\Gamma)$ with the promise that there is an assignment $\sigma_1: V\rightarrow D_1$ that strongly satisfies $1-\epsilon$ fraction of the constraints, the objective is to find an assignment $\sigma_2: V \rightarrow D_2$ that weakly satisfies at least $1-f(\epsilon)$ fraction of the constraints for some function $f$ that satisfies $f(\epsilon)\rightarrow 0$ as $\epsilon \rightarrow 0$.
 \end{enumerate}
\end{definition}


In this paper, we restrict ourselves to Boolean PCSPs where both the domains are equal to $\{-1,+1\}$. Following the robust algorithms literature of CSPs, we allow the constraints to use the negation of variables and refer to such PCSPs as Boolean folded PCSPs. 

\begin{definition}(Boolean folded PCSPs.)
    In a Boolean folded PCSP $\Gamma$, we have a set of pairs of predicates $\Gamma=\{(P_1,Q_1), (P_2,Q_2),\ldots, (P_l,Q_l)\}$ where $P_i \subseteq Q_i \subseteq \{-1,+1\}^{k_i}$ for every $i \in [l]$.  In an instance $\Phi=(V,\mathcal{C})$ of $\PCSP(\Gamma)$, we have a set of $n$ variables $V =\{u_1, u_2, \ldots, u_n\}$, and associated with each variable $u_i$, there are two literals, $u_i$ and $\overline{u_i}$. Let $\overline{V}$ denote the set of all negated literals: $\overline{V} := \{ \overline{u_1},\overline{u_2},\ldots,\overline{u_n}\}$. There are  $m$ constraints $\mathcal{C}=\{C_1, C_2, \ldots, C_m\}$ 
each consisting of a tuple of literals $C_j = (x_{j,1}, x_{j,2}, \ldots, x_{j,{l_j}}) \in \{V\cup \overline{V}\}^{l_j}$ and an associated predicate pair $(P^{(j)},Q^{(j)}) \in \Gamma$ of the same arity $l_j$. 

Consider an assignment $\sigma : V \rightarrow \{-1,+1\}$, and let $\sigma' : V \cup \overline{V} \rightarrow \{-1,+1\}$ be defined as $\sigma'(u_i)=\sigma(u_i)$ and $\sigma'(\overline{u_i})=-\sigma(u_i)$ for every $i \in [n]$. The assignment $\sigma$ is said to strongly (and resp. weakly) satisfy the constraint $C_j$ in $\Phi$ if $\sigma'(C_j) \in P^{(j)}$ (and resp. $\sigma'(C_j) \in Q^{(j)}$). 
\end{definition}

Similar to the general PCSPs, for a Boolean folded PCSP $\Gamma$, in the robust version of $\PCSP(\Gamma)$, given an input instance $\Phi=(V,\mathcal{C})$ of $\PCSP(\Gamma)$ with the promise that there is an assignment $\sigma_1: V\rightarrow \{-1,+1\}$ that strongly satisfies $1-\epsilon$ fraction of the constraints, the objective is to find an assignment $\sigma_2: V \rightarrow \{-1,+1\}$ that weakly satisfies at least $1-f(\epsilon)$ fraction of the constraints for some function $f$ that satisfies $f(\epsilon)\rightarrow 0$ as $\epsilon \rightarrow 0$. For simplicity, we use ``robust algorithm for $\Gamma$'' to refer to an algorithm that solves the robust version of $\PCSP(\Gamma)$. 

In our hardness results, we study Boolean folded PCSPs that are symmetric and idempotent. We say that a predicate $P \subseteq \{-1,+1\}^k$ is symmetric if for every $\textbf{x}, \textbf{y}\in \{-1,+1\}^k$ such that $\textsf{hw}(\textbf{x})=\textsf{hw}(\textbf{y})$, we have $\textbf{x} \in P$ if and only if $\textbf{y} \in P$. A Boolean folded symmetric idempotent PCSP $\Gamma$ is a Boolean folded PCSP in which every predicate involved is symmetric and we also allow the constraints to use constants. We give a formal definition below. 
%

\begin{definition}(Boolean folded symmetric idempotent PCSPs) A Boolean folded PCSP $\Gamma=\{(P_1,Q_1),\ldots,$ $(P_l,Q_l)\}$ where $P_i \subseteq Q_i \subseteq \{-1,+1\}^{k_i}$ is referred to as symmetric and idempotent if the following hold. 
\begin{enumerate}
    \item (Symmetric) $P_i$, $Q_i$ are symmetric for every $i \in [l]$.
    \item (Idempotent) We now allow the constraints to use $+1$ and $-1$ along with the literals $V,\overline{V}$,i.e., each constraint $C_j$ satisfies $C_j \in \{V\cup \overline{V}\cup \{-1,+1\}\}^{l_j}$. Consider an assignment $\sigma : V \rightarrow \{-1,+1\}$, and let $\sigma' : V \cup \overline{V} \cup \{-1,+1\} \rightarrow \{-1,+1\}$ be defined as $\sigma'(u_i)=\sigma(u_i), \sigma'(\overline{u_i})=-\sigma(u_i)$ for every $i \in [n]$, and $\sigma'(b)=b\,\forall b \in \{-1,+1\}$. The assignment $\sigma$ is said to strongly (and resp. weakly) satisfy the constraint $C_j$ in $\Phi$ if $\sigma'(C_j) \in P^{(j)}$ (and resp. $\sigma'(C_j) \in Q^{(j)}$).
\end{enumerate}
%

%
\end{definition}


Associated with every PCSP, there are polymorphisms that capture the closure properties of the satisfying solutions to the PCSP. More formally, we can define the polymorphisms of a PCSP as follows. 
\begin{definition}(Polymorphisms of PCSPs)
	For $PCSP(\Gamma)$ with $\Gamma = \{ ((P_1,Q_1), (P_2, Q_2), \ldots, (P_l,Q_l))\}$ such that for every $i \in [l]$, $P_i\subseteq D_1^{k_i}, Q_i\subseteq D_2^{k_i}$, a polymorphism of arity $n$ is a function $f:D_1^{n} \rightarrow D_2$ that satisfies the below property for every $i \in [l]$.
	For all $\textbf{v}_1,\textbf{v}_2,\ldots,\textbf{v}_{k_i}\in D_1^n$ satisfying $( (\textbf{v}_1)_j, (\textbf{v}_2)_j, \ldots, (\textbf{v}_{k_i})_j ) \in P_i$ for each $j \in [n]$, we have 
	\[
	(f(\textbf{v}_1), f(\textbf{v}_2), \ldots , f(\textbf{v}_{k_i})) \in Q_i
	\]
 For a Boolean folded PCSP $\Gamma$, we require that $f:\{-1,+1\}^n\rightarrow \{-1,+1\}$ satisfy an additional property that $f$ is folded, i.e., $f(-\textbf{v})=-f(\textbf{v})\,\forall \textbf{v}\in \{-1,+1\}^n$. Similarly, for Boolean folded idempotent PCSPs, we require that $f$ is folded and idempotent, i.e., $f(1,1,\ldots,1)=1$ and $f(0,0,\ldots,0)=0$.
	We use $\emph{Pol}(\Gamma)$ to denote the family of all the polymorphisms of $PCSP(\Gamma)$.
\end{definition}



We extensively study Alternate-Threshold $(\AT)$ and Majority $(\MAJ)$ polymorphisms in this paper:
\begin{enumerate}
    \item For an odd integer $L\geq 1$ and $\textbf{x}\in\{-1,+1\}^L$, we have $
    \AT_L(\textbf{x})=+1$,if $x_1-x_2+x_3-\ldots+x_L >0$, and $-1$, otherwise.
    \item For an odd integer $L\geq 1$ and $\textbf{x}\in\{-1,+1\}^L$, we have $
    \MAJ_L(\textbf{x})=+1$,if $x_1+x_2+x_3+\ldots+x_L >0$, and $-1$, otherwise.
\end{enumerate}
We also use $\AT_L(\textbf{x}_1, \textbf{x}_2, \ldots, \textbf{x}_L)$ for $\textbf{x}_i \in \{-1,+1\}^k$ (similarly for $\MAJ$) when applying $\AT_L$ coordinatewise. For a predicate $P \subseteq \{-1,+1\}^k$, we use $\AT_L(P)$ to denote the set $\bigcup_{\textbf{x}_1, \textbf{x}_2, \ldots, \textbf{x}_L \in P}\AT_L(\textbf{x}_1, \textbf{x}_2, \ldots, \textbf{x}_L)$. 

We say that a $\AT\subseteq \Pol(\Gamma)$(and resp. $\MAJ$) if $\AT_L$ (and resp. $\MAJ_L$) is in $\Pol(\Gamma)$ for every odd integer $L\geq 1$. 
For a predicate $P \subseteq \{-1,+1\}^k$, we use $O_{\AT}(P)$ (and similarly $O_{\MAJ}(P)$) to denote the set $\bigcup_{L \in \mathbb{N},\text{ odd}}\AT_L(P)$.

\smallskip \noindent \textbf{Relaxations of PCSPs.} We say that a PCSP $\Gamma'$ is a relaxation of another PCSP $\Gamma$ if $\Pol(\Gamma)\subseteq \Pol(\Gamma')$. If $\Gamma'$ is a relaxation of $\Gamma$, then there is a \textit{gadget reduction} from $\Gamma'$ to $\Gamma$. More formally, it is referred to as $\Gamma'$ is \textit{positive primitive promise (ppp)-definable} from $\Gamma$. 

\begin{definition}[ppp-definability of PCSPs(~\cite{Chen09, BrakensiekG21})]
\label{def:ppp}
We say that a PCSP $\Gamma'=(P',Q')$ containing a single pair of predicates of arity $k$ is ppp-definable from a PCSP $\Gamma$ over the same domain pair if there exists a fixed constant $l$ and an instance $\Phi$ of $\PCSP(\Gamma)$ over $k+l$ variables $u_1, u_2, \ldots, u_k, v_1, v_2, \ldots, v_l$ such that 
\begin{enumerate}
    \item If $(x_1, x_2, \ldots, x_k) \in P'$, then there exist $y_1, y_2, \ldots, y_l$ such that the assignment $(x_1, \ldots, x_k,y_1, \ldots, y_l)$ strongly satisfies all the constraints in $\Phi$. 
    \item If there is an assignment $(z_1, \ldots, z_{k+l})$ weakly satisfying all the constraints in $\Phi$, then $(z_1, z_2, \ldots, z_k)\in Q'$. 
\end{enumerate}
More generally, we say that $\Gamma'$ is ppp-definable from $\Gamma$ if every predicate pair in $\Gamma'$ is ppp-definable from $\Gamma$. 
\end{definition}
Brakensiek and Guruswami~\cite{BrakensiekG21} (independent of Pippenger~\cite{pippenger2002galois}) showed that if $\Gamma'$ is a relaxation of $\Gamma$, then $\Gamma'$ is ppp-definable from $\Gamma$. This shows that the decision version of $\PCSP(\Gamma')$ can be reduced to $\PCSP(\Gamma)$ in polynomial time. The same applies to robust algorithms as well. 

\begin{proposition}
\label{lem:ppp}
Suppose that the PCSP $\Gamma'$ over a pair of domains $D_1, D_2$ is a relaxation of $\Gamma$ over the same domain pair, i.e., $\Pol(\Gamma) \subseteq \Pol(\Gamma')$. If $\Gamma$ has a polynomial time robust algorithm that finds an assignment weakly satisfying $1-f(\epsilon)$ fraction of the constraints on instances promised to have an assignment strongly satisfying $1-\epsilon$ fraction of the constraints, then $\Gamma'$ has a polynomial time robust algorithm as well, i.e., there is a polynomial time algorithm that finds an assignment weakly satisfying $1-O_{\Gamma,\Gamma'}(f(\epsilon))$ fraction of the constraints on instances promised to have an assignment strongly satisfying $1-\epsilon$ fraction of the constraints.
\end{proposition}
We defer the proof of~\Cref{lem:ppp} to~\Cref{sec:gadget}. 

%
%

\subsection{The basic SDP}

%

%
We now describe the Basic SDP relaxation of an instance of a PCSP, similar to how it is presented in \cite{Raghavendra08}. Let $\Phi$ be an instance of a PCSP $\Gamma$ over $n$ variables $V=\{u_1, u_2, \ldots, u_n\}$ and $m$ constraints $C_1, C_2, \ldots, C_m$. Suppose that the constraint $C_j$ contains the tuple $C_j=( u_{j,1}, u_{j,2}, \ldots, u_{j,l_j})$ using the predicate pair $(P^{(j)}, Q^{(j)})$. 
In the basic SDP relaxation of $\Gamma$ corresponding to $\Phi$, we have a vector $\textbf{v}_{i,a}$ corresponding to each variable $u_i$, $i \in [n]$, along with a label $a \in D$. We also have a unit vector $\textbf{v}_0$. 
For each constraint $C_j$, $j \in [m]$, there is a probability distribution (referred to as the local distribution of the constraint $C_j$) that is supported on the set of functions $\{ f: C_j \rightarrow D\}$. We represent this using a variable $\lambda_j(f)$ for every $j\in [m]$ and assignment $f: C_j \rightarrow D$. Finally, we have an error parameter $\eps_j$ corresponding to the constraint $j, j \in [m]$, equal to the probability that $\lambda_j$ is supported outside $P^{(j)}$. We refer to $\epsilon_j$ as the error of the basic SDP relaxation on the constraint $j$, and $\sum_{j=1}^m \eps_j$ as the error of the basic SDP relaxation of the instance $\Phi$.
%
\begin{align*}
    \textbf{minimize: } & \sum_{j=1}^m \eps_j & \\
\textbf{subject to: } \eps_j &\geq 0  & \forall j \in [m]\\
\lambda_j(f) &\geq 0  & \forall j\in [m], ~ f:C_j \rightarrow \{-1,+1\} \\ 
\sum_{f:C_j \rightarrow D}\lambda_j(f)&=1  & \forall j \in [m] \\ 
 \sum_{f:C_j \rightarrow D, f(C_j)\in P^{(j)}} \lambda_j(f) &= 1-\eps_j \quad & \forall j \in [m]\\
\text{ (First moments.)  } \qquad \textbf{v}_{i,a} \cdot \textbf{v}_0 &= \sum_{\substack{f:C_j \rightarrow D\\f(x_i) = a}}\lambda_j(f)&  \forall j\in [m],~x_i \in C_j,~ a \in D \\ 
\text{(Second moments.)  } \quad \textbf{v}_{i,a} \cdot \textbf{v}_{i',a'} &= \sum_{\substack{f:C_j \rightarrow D\\f(x_i) = a, f(x_{i'}) = a'}}\lambda_j(f)  & \forall j\in [m],~x_i,x_{i'} \in C_j,~ a,a' \in D 
\end{align*}
%
\noindent \textbf{Boolean folded variant.}
In Sections~\ref{sec:alg} and \ref{sec:ug-hardness} of the paper, we prefer to consider an alternative formulation of the Basic SDP. In particular, if we assume that $D = \{-1, +1\}$ and allow folding, then the Basic SDP can be simplified as follows. Consider an instance $\Phi=(V,\mathcal{C})$ of a  Boolean folded PCSP $\Gamma$ where $V=\{u_1,u_2,\ldots,u_n\}$ and $\mathcal{C}=\{C_1,C_2,\ldots,C_m\}$ with the constraint $C_j$ using the predicate pair $(P^{(j)},Q^{(j)})$ for $j\in[m]$. 
We have a variable $\textbf{v}_i$ associated with each variable $u_i \in V$. If a constraint $C_j$ uses a negated literal $\overline{u_i}$, we use the vector $-\textbf{v}_i$ in the first moment and second moment equations of $C_j$. 
Towards this, for a literal $x \in \{u_1,u_2,\ldots,u_n, \overline{u_1},\overline{u_2},\ldots,\overline{u_n}\}$, we define $\textbf{v}(x)=\textbf{v}_i$ if $x =u_i$, and $\textbf{v}(x)=-\textbf{v}_i$ if $x=\overline{u_i}$.
%
\begin{align*}
    \textbf{minimize: } & \sum_{j=1}^m \eps_j & \\
\textbf{subject to: } \eps_j &\geq 0 & \forall j \in [m]\\
\lambda_j(f) &\geq 0 & \forall j\in [m], ~f:C_j \rightarrow \{-1,+1\} \\ 
\sum_{f:C_j \rightarrow \{-1,+1\}}\lambda_j(f)&=1 & \forall j \in [m] \\ 
 \sum_{f:C_j \rightarrow \{-1,+1\}, f(C_j)\in P^{(j)}} \lambda_j(f) &= 1-\eps_j & \forall j \in [m]\\
\norm{\textbf{v}_i }_2^2 &= 1 & \forall i \in \{0,1,\ldots,n\} \\ 
\text{ (First moments.)  } \qquad \textbf{v}(x) \cdot \textbf{v}_0 &= \sum_{f:C_j \rightarrow \{-1,+1\}}\lambda_j(f) f(x) & \forall j\in [m],~x \in C_j \\
\text{(Second moments.)  } \quad \textbf{v}(x) \cdot \textbf{v}(x') &= \sum_{f:C_j \rightarrow \{-1,+1\}}\lambda_j(f) f(x)f(x') & \forall j\in [m],~x,x' \in C_j
\end{align*}
%
We say that basic SDP is feasible on $\Phi$ if the above objective function is zero on $\Phi$. We show that the SDP is feasible if there is an assignment that strongly satisfies all the constraints of $\Phi$. 
%
\begin{proposition}
    Suppose that $\Phi=(V,\mathcal{C})$ is an instance of a Boolean folded PCSP such that there is an assignment $\sigma : V \rightarrow \{-1,+1\}$ that strongly satisfies all the constraints. Then, the basic SDP is feasible on $\Phi$.
\end{proposition}
%
\begin{proof}
    We set $\textbf{v}_0=1$, and $\textbf{v}_i := \sigma(u_i) \in \R$ for every $i \in [n]$.
Let $\sigma' : V \cup \overline{V} \rightarrow \{-1,+1\}$ be defined as $\sigma'(u_i)=\sigma(u_i)$ and $\sigma'(\overline{u_i})=-\sigma(u_i)$ for every $i \in [n]$. 
 For a $j \in [m]$ and $f:C_j \rightarrow \{-1,+1\}$, we set $\lambda_j(f)=1$ if $f(x)=\sigma'(x)$ for every $x \in C_j$, and we set $\lambda_j(f)=0$ otherwise. These variables satisfy all the constraints in the basic SDP relaxation with $\eps_j=0$ for all $j \in [m]$.
\end{proof}

More generally, we get that if there is an assignment that strongly satisfies $1-\epsilon$ fraction of the constraints in $\Phi$, the objective value of the above relaxation is at most $\epsilon m$, for every $\epsilon \geq 0$.
On the other hand, if the basic SDP is feasible for an instance $\Phi$ of a PCSP, it doesn't necessarily imply that $\Phi$ has an assignment weakly satisfying all the constraints. For some PCSPs however, this is indeed the case, and we say that such PCSPs are decided by the basic SDP.

\begin{definition}
We say that the basic SDP \textit{decides} the PCSP $\Gamma$ if for every instance $\Phi$ such that the basic SDP is feasible on $\Phi$, there is an assignment to $\Phi$ that weakly satisfies all the constraints. 
\end{definition}

We remark that polynomial-time SDP solving algorithms can only compute the objective to within $1/\text{poly}(n)$ accuracy\cite{freund2004introduction}. For the sake of robust algorithms, this issue is not relevant: if an instance $\Phi$ has a solution strongly satisfying $1-\epsilon$ fraction of the constraints, we can find a vector solution to the basic SDP with error at most $(\epsilon+C)m$ in polynomial time, for arbitrarily small constant $C>0$.
%
%
\subsection{Elementary properties of Gaussians}
We prove a couple of elementary properties of Gaussian distribution that we use later. 
First, we prove the following anti-concentration inequality for the standard Gaussian random variable. 
\begin{proposition}
\label{prop:gaussian-anticoncentration}
Suppose that $X\sim \mathcal{N}(0,1)$ has the standard Gaussian distribution. Then, for every $\epsilon \geq 0$, 
\[
\text{Pr}\left( |X|\leq \epsilon\right) \leq \epsilon.
\]
\end{proposition}
\begin{proof}
We have 
\[
    \text{Pr}\left( |X|\leq \epsilon\right) = \int_{-\epsilon}^{+\epsilon} \frac{1}{\sqrt{2\pi}}e^{-\frac{x^2}{2}}dx  \leq \int_{-\epsilon}^{+\epsilon} \frac{1}{\sqrt{2\pi}} dx \le \epsilon. \qedhere
\]
\end{proof}
We also need the following concentration inequality for 1-dimensional Gaussian. 
\begin{proposition}
\label{prop:gaussian-concentration}
Suppose that $X\sim \mathcal{N}(0,\sigma^2)$ has Gaussian distribution with variance $\sigma^2$. Then, for every $t \geq 0$, 
\[
\text{Pr}\left( X\geq t\right) \leq e^{-\frac{t^2}{2\sigma^2}}.
\]
\end{proposition}


\section{Overview of techniques}
\label{sec:overview}
\subsection{Robust algorithm for MAJ polymorphisms}
\label{sec:maj-overview}
We obtain our robust algorithms by first solving the basic SDP relaxation and then \textit{rounding} the vectors. We first illustrate the SDP rounding idea with a warm-up algorithm (originally appeared in~\cite{Zwick98}) to solve the decision version of $2$-SAT CSP $P=\{(-1,+1),(+1,-1),(+1,+1)\}$. Consider an instance $\Phi$ of $2$-SAT over a set of $n$ variables and $m$ constraints. We first solve the basic SDP relaxation of $\Phi$. If the basic SDP relaxation has a strictly positive error, then the instance is clearly not satisfiable. Instead, suppose that the relaxation has zero error and we found a set of vectors $\textbf{v}_0, \textbf{v}_1, \ldots,\textbf{v}_n$, and the local probability variables $\lambda_j(f)$ for all $j \in [m], f: C_j \rightarrow \{-1,+1\}$ that satisfy all the constraints in the basic SDP relaxation with $\eps_j=0$ for every $j \in [m]$. Consider an arbitrary constraint $C_j$ using the literals $x_1, x_2$. We abuse the notation and let $\textbf{v}_1, \textbf{v}_2$ denote the vectors assigned by the SDP to $x_1$ and $x_2$ respectively. 
Using the first moment and second moment properties satisfied by the local probabilities $\lambda_j(f)$, we get the following. 
\begin{enumerate}
\item $\langle \textbf{v}_1, \textbf{v}_0 \rangle + \langle \textbf{v}_2, \textbf{v}_0 \rangle \geq 0.$
\item If $\langle \textbf{v}_1, \textbf{v}_0 \rangle + \langle \textbf{v}_2,\textbf{v}_0 \rangle=0$, $\textbf{v}_1+\textbf{v}_2 =0$.
\end{enumerate}

These properties motivate the following simple rounding algorithm that outputs the assignment $\sigma: V \rightarrow \{-1,+1\}$. We sample a random unit vector $\zeta \sim \mathcal{N}(0,\textbf{I})$, and set
\[
\sigma(u_i)=\begin{cases}
+1, \text{ if }\langle \textbf{v}_i , \textbf{v}_0 \rangle > 0. \\ 
-1, \text{ if }\langle \textbf{v}_i , \textbf{v}_0 \rangle < 0. \\ 
+1, \text{ if }\langle \textbf{v}_i , \textbf{v}_0 \rangle = 0 \text{ and } \langle \zeta, \textbf{v}_i \rangle >0.\\ 
-1, \text{ if }\langle \textbf{v}_i , \textbf{v}_0 \rangle = 0 \text{ and } \langle \zeta, \textbf{v}_i \rangle <0.
\end{cases}
\]

This ensures that if a constraint uses the literals $x_1, x_2$, at least one of $x_1$ or $x_2$ is rounded to $+1$.
%
%

While this algorithm finds a satisfying solution when the underlying instance $\Phi$ has a solution satisfying all the constraints, it does not give any non-trivial guarantees when the instance is only promised to have a solution satisfying $1-\epsilon$ fraction of the constraints.  
%
Zwick~\cite{Zwick98} gave a robust algorithm for $2$-SAT which was later improved by Charikar, Makarychev, and Makarychev~\cite{CharikarMM09} with the following clever robust algorithm for $2$-SAT. They sample a uniformly random vector $\zeta \sim \mathcal{N}(0,\textbf{I})$, and set 
\[
\sigma(u_i) = \begin{cases}
+1, \text{ if }\langle \textbf{v}_i, \zeta \rangle \geq - \frac{\langle \textbf{v}_i, \textbf{v}_0 \rangle}{\sqrt{\epsilon}}. \\ 
-1, \text{ otherwise. }
\end{cases}
\]
One can view their algorithm as a smoothed version of the earlier discussed algorithm: if $|\langle \textbf{v}_i, \textbf{v}_0 \rangle| >\sqrt{\epsilon}$, then we just round $\sigma(u_i)$ to +1  if and only if $\langle \textbf{v}_i, \textbf{v}_0 \rangle >0$. On the other hand, if $\langle \textbf{v}_i, \textbf{v}_0 \rangle =0 $, we set $\sigma(u_i)=+1$ if and only if $\langle \zeta, \textbf{v}_i \rangle \geq 0$.

We use this algorithm in our proof of~\Cref{thm:main-algorithm} but we give a completely different analysis. 
%
%
Consider a constraint $C$ in the $2$-SAT instance and let $\textbf{v}_1, \textbf{v}_2$ denote the vectors assigned by the basic SDP to the literals in a constraint.
%
Let $\zeta_1=\langle \zeta, \textbf{v}_1\rangle$ and $\zeta_2=\langle \zeta, \textbf{v}_2
\rangle$. Note that both $\zeta_1$ and $\zeta_2$ are standard Gaussian variables with covariance $\text{Cov}(\zeta_1,\zeta_2)=\langle \textbf{v}_1, \textbf{v}_2 \rangle$. To upper bound the probability that the output assignment $\sigma$ violates the constraint $C$,~\cite{CharikarMM09} calculate the probability that $\zeta_1 < - \frac{\langle \textbf{v}_i, \textbf{v}_0 \rangle}{\sqrt{\epsilon}}$ and $\zeta_2 < - \frac{\langle \textbf{v}_i, \textbf{v}_0 \rangle}{\sqrt{\epsilon}}.$ They do so by computing the probability that a pair of Gaussian random variables with known covariance lie in a given intersection of two half-spaces. While this analysis works for $2$-SAT, these calculations turn out to be significantly harder when there are more than two Gaussian random variables.

Instead, we take a conceptually different, and arguably simpler route. As a concrete example, consider the PCSP $\Gamma=(P, Q)$ where $P=
\Ham_4\{2,3,4\}$, $Q=\Ham_4\{1,2,3,4\}$, i.e.,
\[
P=\bigl\{ \textbf{x} \in \{-1,+1\}^4 : \sum_{i \in 4}x_i \geq 0\bigr\}, \quad Q = \{-1,+1\}^k \setminus \{(-1,-1,-1,-1)\}.
\]
Consider a constraint $C$ of an instance $\Phi$ of $\Gamma$. As the average error of SDP over all the constraints is at most $\epsilon$, for at least $1-\epsilon^{1/4}$ fraction of the constraints, the error is at most $\epsilon^{3/4}$. We restrict ourselves to these constraints and let $\textbf{v}_1,\ldots, \textbf{v}_4$ denote the vectors assigned by the SDP to the literals in the constraint. We have that $\sum_{i \in [4]}\langle \textbf{v}_i, \textbf{v}_0 \rangle\geq -\epsilon^{3/4}$. Recall that if for some $i \in [4]$, $\langle \textbf{v}_i, \textbf{v}_0 \rangle  \geq \sqrt{\eps}$, the corresponding literal is rounded to $+1$ with probability $1$. On the other hand, if for some $i \in [4]$, $\langle \textbf{v}_i, \textbf{v}_0 \rangle  \leq -4\sqrt{\eps}$, there is some $i'\in [4]$ with $\langle \textbf{v}_{i'}, \textbf{v}_0 \rangle  \geq \sqrt{\eps}$, which again ensures that there is at least one literal that is rounded to $+1$. Hence, if there is at least one $i\in[4]$ with $|\langle \textbf{v}_{i}, \textbf{v}_0 \rangle|  \geq 4\sqrt{\epsilon}$, at least one literal is rounded to $+1$. 

%
Thus the interesting case is when $|\langle \textbf{v}_i,\textbf{v}_0 \rangle | \leq O(\sqrt{\epsilon})$ for every $i\in [4]$. In this case, using the first and second moment properties satisfied by these vectors, we get that $\norm{\sum_{i \in [4]} \textbf{v}_i}_2$ is at most $O(\epsilon^{1/4})$. 
%
The output assignment $\sigma$ violates $Q$ on this constraint only if $\langle \zeta, \textbf{v}_i \rangle \leq -\frac{\langle \textbf{v}_i, \textbf{v}_0 \rangle}{\sqrt{\epsilon}}$ for every $i \in [4]$, or equivalently, $\langle \zeta, \textbf{v}_i \rangle +\frac{\langle \textbf{v}_i, \textbf{v}_0 \rangle}{\sqrt{\epsilon}} \leq 0$ for every $i \in [4]$.
However, using $\norm{\sum_{i \in [4]} \textbf{v}_i}_2 \leq O(\epsilon^{1/4})$ and $\sum_{i \in [4]}\langle \textbf{v}_i, \textbf{v}_0 \rangle\geq -\epsilon^{3/4}$, we get that
\[
\sum_{i \in [4]} \left(\langle \zeta, \textbf{v}_i \rangle +\frac{\langle \textbf{v}_i, \textbf{v}_0 \rangle}{\sqrt{\epsilon}}\right) \geq -O(\epsilon^{1/4}).
\]
If $\sigma$ does not satisfy $Q$, then for some $i \in [4]$, we have that 
\[
\langle \zeta, \textbf{v}_i \rangle +\frac{\langle \textbf{v}_i, \textbf{v}_0 \rangle}{\sqrt{\epsilon}} \in \left[ -O(\epsilon^{1/4}),0\right]
\]
%
Finally, we can upper bound the probability that this occurs to be at most $\tilde{O}\left(\epsilon^{1/4}\right)$ using anti-concentration of the Gaussian $\langle \zeta, \textbf{v}_i \rangle \sim \mathcal{N}(0,1)$. Thus, we can infer that the assignment $\sigma$ satisfies at least $1-\tilde{O}(\epsilon^{1/4})$ fraction of the constraints in expectation. A careful analysis of the parameters gives a guarantee of $1-\tilde{O}(\epsilon^{1/3})$.

For an arbitrary PCSP $\Gamma$ with $\MAJ\subseteq \Pol(\Gamma)$, we obtain a robust algorithm by first reducing to the case when all the predicate pairs are of the form $(P,Q)$ with $Q=\{-1,+1\}^k \setminus \{(-1,-1,\ldots,-1)\}$, generalizing the above two examples of $2$-SAT and $(\Ham_{4}\{2,3,4\}, \Ham_4\{1,2,3,4\})$. Then, we find a weight vector $\textbf{w}$ which satisfies that $\textbf{w}\cdot \textbf{x}\geq 0$ for all $\textbf{x}\in P$ ($\textbf{w}=(1,1,\ldots,1)$ suffices for the previous two examples). We prove the existence of such a vector $\textbf{w}$ by using a Linear Programming relaxation, and we crucially use the fact that $(P,Q)$ contains $\MAJ$ polymorphisms in the analysis of this LP relaxation.
Once we find the vector $\textbf{w}$, the above analysis of $(\Ham_{4}\{2,3,4\}, \Ham_4\{1,2,3,4\})$ can be generalized, the main change being that we study the properties of the weighted sum of the $\textbf{v}_i$s with weights being given by the vector $\textbf{w}$.  

\subsection{Robust algorithm for AT polymorphisms}
\label{sec:at-overview}
For the Alternating-Threshold (AT) case, we combine these ideas with a random geometric sampling trick. 
As a concrete example, consider the PCSP ($1$-in-$3$-SAT, NAE-$3$-SAT). 
For the exact case, we can solve the problem using the basic SDP relaxation via random hyperplane rounding as follows. Consider an arbitrary constraint $C$ and let $\textbf{v}_1, \textbf{v}_2, \textbf{v}_3$ denote the vectors assigned by the basic SDP to the literals in $C$. Using the fact that these vectors satisfy the first and second moment constraints of the basic SDP relaxation with zero error, we can infer that their sum $\textbf{v}_s = \textbf{v}_1 + \textbf{v}_2 + \textbf{v}_3$ is equal to $-\textbf{v}_0$ for every constraint $C$.  Let $\textbf{v}_i \perp \textbf{v}_0 = \textbf{v}_i - \langle \textbf{v}_i, \textbf{v}_0 \rangle \textbf{v}_0$ for $i \in [3]$. Note that $\sum_{i \in [3]} \textbf{v}_i \perp \textbf{v}_0 =0$. Using this observation, we can design a rounding scheme.
We first sample $\zeta \sim \mathcal{N}(0,\textbf{I})$, and set $\sigma(u_i)=+1$ if $\langle \textbf{v}_i \perp \textbf{v}_0 , \zeta \rangle > 0$, and $-1$ if $\langle \textbf{v}_i \perp \textbf{v}_0 , \zeta \rangle < 0$. We also need to set $\sigma(\textbf{v}_0)=+1$ and $\sigma(-\textbf{v}_0)=-1$. As $\textbf{v}_1+\textbf{v}_2+\textbf{v}_3=-\textbf{v}_0$, the rounding scheme  ensures that at least one literal associated with these vectors is set to $+1$, and at least one literal is set to $-1$. 

For the robust setting where we are only guaranteed that there is a solution satisfying $1-\epsilon$ fraction of the constraints, we get that the average SDP error is at most $\epsilon$. By Markov's inequality, we are guaranteed that for at least $1-\sqrt{\epsilon}$ fraction of the constraints, the SDP error is at most $\sqrt{\epsilon}$. For these constraints, we get that the sum vector $\textbf{v}_s$'s component orthogonal to $\textbf{v}_0$ has $\ell_2$ norm at most $O(\eps^{1/4})$, i.e., $\norm{\sum_{i \in 3}\textbf{v}_i \perp \textbf{v}_0}_2 \leq O(\epsilon^{1/4})$.
Using this, we design a rounding scheme that is similar to the above, with the addition that when $\norm{\textbf{v}_i \perp \textbf{v}_0}_2$ is very small, we want to round it to $+1$ or $-1$ depending on its component along $\textbf{v}_0$, similar to how we were rounding $\textbf{v}_0$ to $+1$ and $-\textbf{v}_0$ to $-1$ in the exact algorithm earlier. We have the following compact algorithm based on this idea.
%
 \[
        \sigma(u_i) = 
        \begin{cases}
        +1, \text{ if  }\langle \textbf{v}_i \perp \textbf{v}_0, \zeta \rangle \ge -\delta \langle \textbf{v}_i, \textbf{v}_0 \rangle. \\ 
        -1, \text{ otherwise.}
        \end{cases}
        \]
Here, we choose $\delta$ to be a constant that is equal to $\epsilon^\kappa$ for an absolute constant $\kappa < 1/4$. Note that when $|\langle \textbf{v}_i \perp \textbf{v}_0, \zeta \rangle|$ is larger than $\delta$, then our new rounding scheme is the same as the exact algorithm earlier, i.e., $\sigma(u_i)=+1$ if $\langle \textbf{v}_i \perp \textbf{v}_0, \zeta \rangle >0$ and $-1$ otherwise. Thus, if $|\langle \textbf{v}_i \perp \textbf{v}_0, \zeta \rangle|>3\delta$ for some $i \in [3]$, $\norm{\sum_{i \in 3}\textbf{v}_i \perp \textbf{v}_0}_2 \leq O(\epsilon^{1/4})<\delta$ implies that there exist $i,i' \in [3]$ with $\langle \textbf{v}_i \perp \textbf{v}_0, \zeta \rangle >\delta, \langle \textbf{v}_{i'} \perp \textbf{v}_0, \zeta \rangle <-\delta$, in which case $\sigma(u_i)=+1$ and $\sigma(u_{i'})=-1$. Thus, the output assignment satisfies the NAE-$3$-SAT constraint. 

On the other hand, when $|\langle \textbf{v}_i \perp \textbf{v}_0, \zeta \rangle|$ is much smaller than $\delta$ for every $i \in [3]$, our rounding function sets $\sigma(u_i)$ to $+1$ if $\langle \textbf{v}_i, \textbf{v}_0 \rangle >0$ and $-1$ otherwise for each $i \in [3]$. Since $\sum_{i \in [3]}\textbf{v}_i$ is close to $-\textbf{v}_0$, even in this case, our output satisfies the NAE-$3$-SAT constraint. The final ingredient is a geometric sampling trick where we sample $\delta$ uniformly at random from a geometric series to ensure that with high probability, either $|\langle \textbf{v}_i \perp \textbf{v}_0, \zeta \rangle|>3\delta$ for some $i \in [3]$ or $|\langle \textbf{v}_i \perp \textbf{v}_0, \zeta \rangle|$ is much smaller than $\delta$ for every $i \in [3]$. 

For an arbitrary Boolean folded PCSP $\Gamma$ with $\AT \subseteq \Pol(\Gamma)$, we first reduce $\Gamma$ to a weighted hyperplane predicate pair PCSP $(P,Q)$ where 
\[
P=\{ \textbf{v} \in \{-1,+1\}^k : \langle \textbf{v}, \textbf{w}\rangle = b \}, \quad Q = \{-1,+1\}^k \setminus\{\textbf{x},-\textbf{x}\}
\]
for some $\textbf{w} \in \R^k, b \in \R$ with $w_i \neq 0\,~\forall i \in [k]$, where we set $\textbf{x}$ as $x_i = +1$ if $w_i>0$, and $-1$ otherwise.
This generalizes the $(1$-in-$3$-SAT, NAE-$3$-SAT) PCSP which corresponds to the case when $\textbf{w}=(1,1,1),b=-1$. We show that the algorithm above works for this general predicate pair as well, thereby obtaining a robust algorithm for every Boolean folded PCSP $\Gamma$ with $\AT \subseteq \Pol(\Gamma)$.
%

\subsection{UG-hardness of robust algorithms}
\label{sec:sdpgap-overview}
As mentioned earlier, our Unique Games hardness for a PCSP $\Gamma$ (\Cref{thm:main-hardness}) is based on an integrality gap for the basic SDP relaxation, i.e., we need to show that there is a finite instance $\Phi$ of $\Gamma$ that has SDP error of zero, yet there is no assignment that weakly satisfies all the constraints.  
In pursuit of this goal, we develop a general recipe for showing integrality gaps with respect to basic SDP for Promise CSPs via colorings of the $n$-dimensional unit sphere $\S^n$. 

We first start by showing an integrality gap instance for the CSP $3$-LIN. Recall that the $3$-LIN CSP has the predicate $P$ with 
\[
P=\{ \textbf{x} \in \{-1,+1\}^3 : x_1 + x_2 + x_3 = -1 \text{ or }x_1 + x_2 + x_3=+3\}.
\]
Consider the instance $\Phi$ that uses three variables and uses two constraints $C_1, C_2$ with $C_1=\{(x_1,x_2,x_3)\},C_2=\{ (\overline{x_1}, \overline{x_2},\overline{x_3})\}$. The instance has no assignment that satisfies both the constraints, and we now show that the basic SDP solution has zero error on $\Phi$. We first describe the local probability variables: we set 
\[
\lambda_j(f)=\frac{1}{4}\quad\forall j \in [2], f(C_j)\in P
\]
That is, we set each local distribution to be the uniform distribution over $P$. Substituting these in the first and second moment constraints, we get the following requirements that the vectors $\textbf{v}_1, \textbf{v}_2, \textbf{v}_3$ need to satisfy. 
\begin{align}
\label{eq:3lin-1}     \textbf{v}_i \cdot \textbf{v}_0 &= 0 \quad \forall i \in [3] \\ 
    \label{eq:3lin-2} \textbf{v}_i \cdot \textbf{v}_{i'} &= 0\quad \forall i \neq i' \in [3]
\end{align}
We can find such three vectors by picking three orthogonal vectors that are all orthogonal to $\textbf{v}_0$. This shows that there is a solution to the basic SDP relaxation of $\Phi$ that has zero error, thus finishing the proof of the existence of an integrality gap for $\Phi$. 

While the simple example gives an integrality gap for the $3$-LIN CSP, it is a challenging task to find such explicit integrality gap instances for general predicates. 
We develop a non-explicit approach where we first construct an infinite  integrality gap instance $\mathcal{I}^n(\Gamma)$ for a given PCSP $\Gamma$ and then use it to show the existence of a finite integrality gap instance. 
The variable set $V$ of $\mathcal{I}^n(\Gamma)$ to be the set of unit vectors in $\R^{n+1}$: $V=\{ u_{\textbf{v}} : \textbf{v} \in \S^{n} \}$.

%
We fix an arbitrary vector to be assigned $\textbf{v}_0$ and add a constraint using $k$ variables $u_{\textbf{v}_1}, u_{\textbf{v}_2}, \ldots, u_{\textbf{v}_k}$ if there is a probability distribution $\lambda(f)$ supported on $P$ such that these vectors $\textbf{v}_1, \textbf{v}_2, \ldots, \textbf{v}_k$ satisfy the first moment and second moment constraints of the basic SDP relaxation with respect to $\lambda(f)$ and the fixed vector $\textbf{v}_0$. 
We refer to such a set of $k$ vectors as \emph{$P$-configurations} with respect to $\textbf{v}_0$. 
The way we have added the constraints ensures that setting $\textbf{a}$ to the variable $u_{\textbf{a}}$ satisfies all the basic SDP constraints with zero error. We then show that the instance $\mathcal{I}^n(\Gamma)$ does not have any assignment $\sigma$ that weakly satisfies all the constraints to obtain the integrality gap. Towards this, we study the sphere colorings $f_n : \S^{n} \rightarrow \{-1,+1\}$ that weakly satisfy all the constraints of $\mathcal{I}^n(\Gamma)$. 

We take a look at the CSP $P=3$-LIN again, now in terms of $\mathcal{I}^n(P)$. As mentioned earlier, a set of three vectors $\textbf{v}_1, \textbf{v}_2, \textbf{v}_3$ are a $P$-configuration with respect to $\textbf{v}_0$ if they satisfy~\Cref{eq:3lin-1} and~\Cref{eq:3lin-2}. 
Thus, to show that the instance $\mathcal{I}^n(P)$ does not have any assignment satisfying all the constraints, it suffices to show that for some positive integer $n$, there is no function $f:\mathbb{S}^n \rightarrow \{-1,+1\}$ that satisfies the following condition:
For all vectors $\textbf{v}_1, \textbf{v}_2, \textbf{v}_3 \in \mathbb{S}^n$ are mutually orthogonal and are orthogonal to $\textbf{v}_0$, we have 
\[
f(\textbf{v}_1)+f(\textbf{v}_2)+f(\textbf{v}_3) \in \{-1,+3\} \ . 
\]
As we allow negation of variables, we also require such a function $f$ to be folded, i.e., $f(-\textbf{v})=-f(\textbf{v})$ for every $\textbf{v}\in \mathbb{S}^{n-1}$. Such a coloring $f$ trivially does not exist: consider a set of three mutually orthogonal vectors that are all orthogonal to $\textbf{v}_0$, $V=(\textbf{v}_1, \textbf{v}_2, \textbf{v}_3)$ and their negations, $V'=(-\textbf{v}_1, -\textbf{v}_2, -\textbf{v}_3)$. Such a set of vectors is guaranteed to exist if $n \geq 4$.
Note that both these are valid $P$-configurations, but at least one of $f(\textbf{v}_1)+f(\textbf{v}_2)+ f(\textbf{v}_3)$, $f(-\textbf{v}_1)+ f(-\textbf{v}_2)+f(-\textbf{v}_3)$ does not belong to $\{-1,+3\}$, thus completing the proof that there is no assignment satisfying all the constraints of $\mathcal{I}^n(P)$. Hence, $\mathcal{I}^n(P)$ is an integrality gap instance for the $3$-LIN CSP, and this implies the existence of a finite integrality gap instance as well. While there is a direct integrality gap instance for the $3$-LIN, for an arbitrary PCSP $\Gamma$, to show the existence of an integrality gap for the basic SDP relaxation, it is more convenient and practical to show the absence of any assignment $f:\S^{n}\rightarrow \{-1,+1\}$ satisfying all the constraints of $\mathcal{I}^n(\Gamma)$ for some $n$ which then implies the existence of a finite integrality gap instance by a compactness argument.
%

While the $P$-configurations in the above proof for $3$-LIN are easy to study, in general, proving the absence of sphere coloring is challenging. For example, consider the PCSP $\Gamma=(P,Q)$ where $P=\Ham_5 \{2,5\}$, $Q=\Ham_5 \{1,2,3,4,5\}$:
\[
P = \bigl\{ \textbf{x} \in \{-1,+1\}^5 : |\{ i \in [5] : x_i = +1\} | \in \{2,5\} \bigr\}, \quad Q =\bigl\{ \textbf{x} \in \{-1,+1\}^5 : \sum_{i=1}^5 x_i >-5\bigr\}. 
\]
Here, a set of $P$-configurations with respect to a vector $\textbf{v}_0$ are five unit vectors $\{\textbf{v}_1, \textbf{v}_2, \ldots, \textbf{v}_5\}$
such that every two distinct vectors have an inner product equal to $\frac{-1}{5}$, and $\textbf{v}_i \cdot \textbf{v}_0 = 0$ for every $i \in [5]$. The sphere coloring problem is then to show that there exists $n$ such that for any folded $f:\mathbb{S}^n \rightarrow \{-1,+1\}$, there exists a set of five vectors in $\mathbb{S}^n$ with every pair of them having inner product equal to $\frac{-1}{5}$ that are all colored $-1$. 

Such problems where the goal is to find a monochromatic structure in sphere colorings are studied in a topic called \emph{sphere Ramsey theory.} In a striking result using tools from combinatorics, linear algebra, and Banach space theory, Matoušek and Rödl~\cite{matouvsek1995ramsey} proved that every set of affinely independent vectors $V$ whose circumradius is smaller than $1$ is sphere Ramsey---i.e., for every $r$, there exists $n$ large enough such that every $r$-coloring of $\mathbb{S}^n$ must have a monochromatic set $U$ that is congruent to $V$. This directly answers the above question regarding sphere coloring of $\Gamma=(P,Q)$ where $P=\Ham_5 \{2,5\}$, $Q=\Ham_5 \{1,2,3,4,5\}$.

For an arbitrary Boolean symmetric PCSP $\Gamma=(P,Q)$, to prove~\Cref{thm:main-hardness}, we first reduce the problem into a fixed number of templates using the properties of $\AT$ and $\MAJ$ polymorphisms, following~\cite{BrakensiekG21}. For some of these templates, the result of Matoušek and Rödl~\cite{matouvsek1995ramsey} directly answers the sphere coloring problem associated with them. For others, we need extra work built on the sphere Ramsey result. We highlight one such template $\Gamma =  (P,Q)$ where $P=\Ham_k \{1,k\}$, $Q=\Ham_k \{0,1,\ldots,k\}\setminus\{b\}$ for positive integers $k, b :0\leq b \leq k$. In the sphere coloring problem associated with this template, we need to show that for any real $\alpha \in [0,1]$, in any folded $f:\S^n \rightarrow \{-1,+1\}$, there are $k$ vectors all of whose pairwise inner products are equal to $\alpha$, and exactly $b$ of them are assigned $+1$ according to $f$. We refer to such a set of vectors $S=\{\textbf{v}_1,\ldots,\textbf{v}_k\}\subseteq \S^n$ as $\alpha$-configuration if $\textbf{v}_i \cdot \textbf{v}_j=\alpha$ for all $i \neq j \in [k]$.


The sphere Ramsey result shows that there is an $\alpha$-configuration with $b$ vectors being colored $+1$ only when $b=0$ or $b=k$. To extend to general $b$, we prove the following connectivity lemma: Between any two arbitrary $\alpha$-configurations $S, T$, there is a path $U_1, U_2,\ldots, U_L$ of $\alpha$-configurations with length $L:=L(\alpha,k,n)$ where $U_1=S$, $U_L=T$, and any two consecutive configurations $U_i$, $U_{i+1}$ differ in at most one element. The sphere Ramsey result shows that there is an $\alpha$-configuration with all the vectors being colored $+1$, and by negating these vectors, we get an $\alpha$-configuration with all the vectors being colored $-1$. Finally, using the connectivity lemma between these two configurations, we get that for every $b \in \{0,1,\ldots,k\}$, there is an $\alpha$-configuration where in exactly $b$ vectors are colored $+1$, thereby finishing the proof of the integrality gap. We also remark that while our integrality proofs are non-constructive in general, we get an explicit integrality gap instance for $\Gamma$ inspired by the connectivity lemma.
%


\section{Robust Algorithms}
\label{sec:alg}

\subsection{CMM is a robust algorithm when Majority is a polymorphism}

We restate~\Cref{thm:main-algorithm} for the case of $\MAJ$ polymorphisms. 
\begin{theorem}
\label{thm:maj-alg}
Let $\Gamma=\{(P_1,Q_1), (P_2, Q_2), \ldots, (P_l, Q_l)\}$ be a Boolean folded PCSP with $\MAJ \subseteq \Pol(\Gamma)$. For every $\epsilon>0$, there is a randomized polynomial time algorithm that given an instance $\Phi$ of $\PCSP(\Gamma)$ that is promised to have an assignment satisfying $1-\epsilon$ fraction of the constraints, finds an assignment to $\Phi$ that satisfies $1-\tilde{O}_{\Gamma}(\epsilon^{\frac{1}{3}})$ fraction\footnote{We use $O_\Gamma$ to denote a hidden constant which depends on the specific template $\Gamma$.} of the constraints in expectation.
\end{theorem}

In the rest of this subsection, we prove~\Cref{thm:maj-alg}. Our strategy is to reduce the problem into a special case when every predicate pair in the PCSP $\Gamma$ is of the form $(P,\{-1,+1\}^k \setminus \{(-1,-1,\ldots, -1)\})$, and then use the algorithm of Charikar, Makarychev, and Makarychev~\cite{CharikarMM09}.

For ease of notation, we use $O(\cdot)$ instead of $O_{\Gamma}(\cdot)$ when $\Gamma$ is clear from the context. 
We first get rid of all the constraints that use a predicate pair $(P,Q)$ where $P\subseteq Q=\{-1,+1\}^k$ for some integer $k$ since these constraints are trivially satisfied by any assignment. 
Suppose that there are $m$ constraints in $\Phi$, and $m'=\alpha m$ of them use predicates of the form $(P,Q)$ where $P\subseteq Q=\{-1,+1\}^k$. We consider the instance $\Phi'$ containing $m-m'$ constraints obtained by deleting the constraints that use predicates of the form $(P,Q)$ where $P\subseteq Q=\{-1,+1\}^k$. In the instance $\Phi'$, we are promised that there is a solution satisfying $m-m'-\epsilon m$ constraints, i.e., $1-\frac{\epsilon}{1-\alpha}$ fraction of the constraints. 
We use the algorithm that we will present later in the subsection on the instance $\Phi'$ to get an assignment weakly violating at most $\tilde{O}\left( \left(\frac{\epsilon}{1-\alpha}\right)^{\frac{1}{3}}\right)(m-m')$ constraints. The same assignment weakly violates at most 
\[
\tilde{O}\left( \left(\frac{\epsilon}{1-\alpha}\right)^{\frac{1}{3}}\right)(m-m') = \tilde{O}\left( \left( \frac{\epsilon}{1-\alpha}\right)^{\frac{1}{3}}\right) (1-\alpha)m \leq \tilde{O}( \epsilon^{\frac{1}{3}}) m 
\]
constraints in $\Phi$. 
Thus, it suffices to study Boolean folded PCSPs where no predicate pair is of the form $(P, \{-1,+1\}^k)$. 

We further transform the instance into one in which every predicate pair is of the form $(P,\{-1,+1\}^k\setminus \{(-1,-1,\ldots,-1)\})$.  
\begin{lemma}
\label{lem:maj-transform}
Fix $\epsilon >0$, and consider a Boolean folded PCSP $\Gamma=\{(P_1,Q_1), \ldots, (P_l, Q_l)\}$ where $P_i\subseteq  Q_i \subsetneq \{-1,+1\}^{k_i}$ for every $i \in [l]$. Given an instance $\Phi$ of $\Gamma$ over a set of variables $V$, there is a polynomial time algorithm that outputs an instance $\Phi'$ of a Boolean folded PCSP $\Gamma'=\{(P'_1,Q'_1), \ldots, (P'_{l'}, Q'_{l'})\}$ over the same variable set $V$ such that the following hold. 
\begin{enumerate}
    \item (Completeness.) If an assignment $\sigma : V \rightarrow \{-1,+1\}$ strongly satisfies $1-\epsilon$ fraction of the constraints in $\Phi$, then $\sigma$ strongly satisfies at least $1-O(\epsilon)$ fraction of the constraints in $\Phi'$ as well. 
    \item (Soundness.) If an assignment $\sigma : V \rightarrow \{-1,+1\}$ weakly satisfies $1-\epsilon$ fraction of the constraints in $\Phi'$, then $\sigma$ weakly satisfies at least $1-O(\epsilon)$ fraction of constraints in $\Phi$. 
    \item The resulting PCSP $\Gamma'$ satisfies the below two properties:
    \begin{enumerate}
        \item For every $i \in [l']$, $Q'_i$ is equal to $\{-1,+1\}^{k'_i}\setminus \{(-1,-1,\ldots,-1)\}$ for some positive integer $k'_i$.
        \item If $\MAJ \subseteq \Pol(\Gamma)$, then, $\MAJ \subseteq \Pol(\Gamma')$.
    \end{enumerate} 
\end{enumerate}
\end{lemma}

\begin{proof}
We obtain the above transformation in two steps. First, we construct a Boolean folded PCSP $\Gamma^*$ from $\Gamma$ as follows: 
\[
\Gamma^* :=  \left\{ \left(P_i, \{-1,+1\}^{k_i}\setminus \{\textbf{x}\}\right) : i \in [l], \textbf{x} \in \{-1,+1\}^{k_i} \setminus Q_i \right\}
\]
Note that for every predicate pair $(P,Q) \in \Gamma^*$, there is a predicate pair $(P,Q') \in \Gamma$ with $Q' \subseteq Q$, and thus, $\Pol(\Gamma) \subseteq \Pol(\Gamma^*)$. 

Given an instance $\Phi$ of $\Gamma$ over a set of variables $V$, we obtain an instance $\Phi^*$ of $\Gamma^*$ over the same set of variables $V$ as follows. We order the constraints of $\Phi$ as $C_1, C_2, \ldots, C_m$. Consider a constraint $C_j$ in $\Phi$ using the predicate pair $(P_i,Q_i)$ over the tuple of literals $S_j = (x_{j,1}, x_{j,2}, \ldots, x_{j,k_i})$. 
In the instance $\Phi^*$, we add $2^{k_i}-|Q_i|$ constraints $C_{j,\textbf{x}}$ associated with every $\textbf{x} \in \{-1,+1\}^{k_i} \setminus Q_i$. The constraint $C_{j,\textbf{x}}$ uses the same tuple of literals as $C_j$ but uses the predicate pair $(P_i, \{-1,+1\}^k_i \setminus \{\textbf{x}\}).$ 
We analyze the completeness and soundness of this reduction. 
\begin{enumerate}
    \item (Completeness.) If $\sigma : V \rightarrow \{-1,+1\}$ strongly satisfies a constraint $C_j$, then $\sigma$ strongly satisfies $C_{j,\textbf{x}}$ for every $\textbf{x} \in \{-1,+1\}^{k_i}\setminus \{\textbf{x}\}$. 
    %
    If $\sigma$ strongly satisfies $(1-\epsilon)m$ constraints in $\Phi$, then the number of constraints that $\sigma$ does not strongly satisfy in $\Phi^*$ is at most $2^K\epsilon m$, where $K := \max_{i \in [l]}k_i$. 
    Thus, $\sigma$ strongly satisfies at least $m^*-2^K\epsilon m \geq (1-2^K\epsilon)m^*$ constraints, where $m^*$ is the number of constraints in $\Phi^*$.
    Here, we are using the fact that $m^* \geq m$, as we have $Q_i \neq \{-1,+1\}^{k_i}$ for every $i \in [k]$.
    \item (Soundness.) Suppose that $\sigma : V \rightarrow \{-1,+1\}$ weakly satisfies $1-\epsilon$ fraction of the constraints in $\Phi^*$. As $\sigma$ weakly violates at most $\epsilon m^* \leq 2^K \epsilon m$ constraints, for at least $1-2^K\epsilon$ fraction of the original constraints $C_j$, $\sigma$ weakly satisfies $C_{j,\textbf{x}}$ for every $\textbf{x} \in \{-1,+1\}^{k_i}\setminus Q_i$. For these constraints $C_j$, $\sigma$ weakly satisfies $C_j$ as well, and thus, $\sigma$ weakly satisfies at least $1-2^K\epsilon$ fraction of the constraints in $\Phi$.
\end{enumerate}
Next, we transform $\Gamma^*$ to $\Gamma'$ to ensure that every weak predicate is of the form $\{-1,+1\}^k\setminus\{(-1,\ldots,-1)\}$. 
%
We use the following entry-wise product notation: for a pair of vectors $\textbf{u},\textbf{v}\in\R^k$, we let
    \[
    \textbf{u}\odot\textbf{v} := (u_1v_1, u_2v_2, \ldots,u_kv_k).
    \]
    For a predicate $P \subseteq \{-1,+1\}^k$, we let 
    \[
    P \odot \textbf{v} := \{ \textbf{u} \odot \textbf{v} : \textbf{u} \in P \}.
    \]
We let $\Gamma'$ be the following. 
\[
\Gamma' := \left\{ \left(P \odot (-\textbf{x}), \{-1,+1\}^k \setminus \{(-1,-1,\ldots,-1)\}\right) : \left(P,\{-1,+1\}^k\setminus \{\textbf{x}\}\right) \in \Gamma^*\right\}
\]
Given the instance $\Phi^*$ of $\Gamma^*$ that is obtained from $\Phi$,  we construct an instance $\Phi'$ of $\Gamma'$ on the same set of variables as follows. Consider a constraint $C_{j,\textbf{x}}$ using the predicate pair $(P_i, \{-1,+1\}^{k_i}\setminus \{\textbf{x}\})$. We have a constraint $C'_{j,\textbf{x}}$ in $\Phi'$ using the predicate pair $(P_i \odot \textbf{x}, \{-1,+1\}^{k_i}\setminus \{(-1,-1,\ldots,-1)\}$. However, in the constraint $C'_{j,\textbf{x}}$, we negate the literals corresponding to the indices $p\in[k_i]$ where $x_p=1$. More formally, the constraint $C'_{j,\textbf{x}}$ uses a tuple $S'_{j}:=(y_{j,1},y_{j,2},\ldots,y_{j,k_i})$ that is obtained from $S_j$ as follows.
\[
y_{j,p} := \begin{cases}
x_{j,p} , \text{ if } x_p = -1. \\
\overline{x_{j,p}} , \text{ if } x_p = 1.
\end{cases}
\]
An assignment $\sigma : V \rightarrow \{-1,+1\}$ strongly (and resp. weakly) satisfies $C'_{j,\textbf{x}}$ if and only if $\sigma$ strongly (and resp. weakly) satisfies $C_{j,\textbf{x}}$. Thus, we get that $\Phi'$ satisfies the completeness and soundness properties of the lemma. Finally, the operation of negating a subset of coordinates preserves all polymorphisms that are folded, and thus, if $\MAJ \subseteq \Pol(\Gamma)$, $\MAJ \subseteq \Pol(\Gamma')$ as well. 
\end{proof}

Given an instance $\Phi$ of an $\Gamma$, we transform it to an instance $\Phi'$ of a PCSP $\Gamma'$ using~\Cref{lem:maj-transform}. 
If $\Phi$ is promised to have an assignment strongly satisfying at least $1-\epsilon$ fraction of the constraints, then $\Phi'$ has an assignment strongly satisfying $1-O(\epsilon)$ fraction of the constraints as well. If there is a polynomial time robust algorithm that outputs an assignment weakly satisfying $1-f(\epsilon)$ fraction of the constraints, then we can use this assignment to obtain a robust algorithm for $\Gamma$ as well. Thus, a polynomial time robust algorithm for $\Gamma'$ gives a polynomial time robust algorithm for $\Gamma$ as well. 

For such a $\Gamma$ where every predicate pair is of the form $(P, \{-1,+1\}^{k}\setminus \{(-1,-1,\ldots,-1)\})$ with $\MAJ \subseteq \Pol(\Gamma)$, we show that the robust algorithm of Charikar, Makarychev, and Makarychev~\cite{CharikarMM09} for $2$-SAT generalizes and gives a robust algorithm for the Boolean folded PCSP $\Gamma$ as well.   
First, we state their algorithm.

%

\smallskip 
\noindent\fbox{%
    \parbox{\textwidth}{%
    \begin{enumerate}
        \item Given an instance $\Phi$ of $\Gamma$ containing $n$ variables $u_1,u_2,\ldots,u_n$,
        solve the basic SDP and obtain a set of vectors $\textbf{v}_0, \textbf{v}_1, \ldots, \textbf{v}_n$. Let $\boldsymbol \mu \in \R^n$ denote the first moments and $\Sigma \in \R^{n\times n}$ be the gram matrix of these vectors. 
        \begin{align*}
            \mu_i &= \textbf{v}_i \cdot \textbf{v}_0 \quad \forall i \in [n]\\ 
            \Sigma_{i,j} &= \textbf{v}_i \cdot \textbf{v}_j \quad \forall i,j \in [n]
        \end{align*}
        \item Sample an $n$ dimensional Gaussian $\zeta \sim \mathcal N(\textbf{0}, \Sigma)$. (Note that $\Sigma$ is positive semidefinite.)
        \item Set\footnotemark  $\gamma = \epsilon^{\frac{2}{3}}$.
        \item For each $i \in [n]$, round as follows
        \[
            \sigma(u_i) = \begin{cases}
            +1 & \zeta_i \ge -\mu_i/\gamma.\\
            -1 & $\text{otherwise}$.
            \end{cases}
        \]
    \end{enumerate}
            }%
}        \footnotetext{We change the parameter slightly -- in the original CMM algorithm, $\gamma$ is set to be $\sqrt{\epsilon}$.}
\smallskip 

We shall prove the following guarantee about the algorithm.

\begin{theorem}\label{thm:maj}
Let $\Gamma=\{(P_1,Q_1),(P_2,Q_2),\ldots,(P_l,Q_l)\}$ be a Boolean folded PCSP such that $\MAJ \subseteq \Pol(\Gamma)$ where $P_i \subseteq Q_i = \{-1,+1\}^{k_i}\setminus \{(-1,-1,\ldots,-1)\} $ for every $i \in [l]$. Let $\Phi$ be an instance of $\PCSP(\Gamma)$ over $n$ variables and using $m$ constraints for which the basic SDP relaxation has a solution with error value at most $\eps m$. Then, the assignment $\sigma$ output by the above CMM algorithm weakly satisfies $1 - \tilde{O}_{\Gamma}(\eps^{1/3})$ fraction of the constraints in expectation.
\end{theorem}


We analyze the probability that the output assignment weakly satisfies each constraint separately. Fix a constraint $C_j$ using the predicate pair $(P,Q)$ of arity $k$ with $P\subseteq Q=\{-1,+1\}^k \setminus \{(-1,-1,\ldots,-1)\}.$ Suppose that the basic SDP solution has error equal to $c$ on this constraint, i.e., $\eps_j=c$. 
Our goal is to upper bound the probability that the rounded solution violates the constraint $Q$ by a function of $\epsilon$ and $c$. Using the fact that the expected value of $c$ over all the constraints is at most $\epsilon$, we get our required robustness guarantee. More formally, we prove the following.

\begin{lemma}\label{lem:maj}
Fix $j \in [m]$, and suppose that the basic SDP solution has an error value equal to $c$ on $C_j$, i.e., $\eps_j=c$. Then, the probability that $\sigma$ does not weakly satisfy $C_j$ is at most
\[
O\left(\sqrt{\epsilon}+\sqrt{\left(\gamma \sqrt{\log \frac{1}{\epsilon}} + 2c\right)\log \frac{1}{\epsilon}}+\frac{c}{\gamma}\right).
\]
\end{lemma}

By summing over all the constraints, and using linearity of expectation, the expected number of constraints that are not weakly satisfied by $\sigma$ is at most 
\begin{equation}
\label{eq:maj-error}
O\left(\sqrt{\epsilon}m+\sum_{j \in [m]}\left(\sqrt{\left(\gamma \sqrt{\log \frac{1}{\epsilon}} + 2\eps_j\right)\log \frac{1}{\epsilon}}+\frac{\eps_j}{\gamma}\right)\right).
\end{equation}
As the basic SDP has a total error at most $\epsilon m$, the average value of $\epsilon_j$ over $j\in [m]$ is at most $\epsilon$. Also note that the expression in~\Cref{eq:maj-error} is a concave function of $\eps_j$. Thus, using Jensen's inequality, we get that the expected number of constraints that are not weakly satisfied by $\sigma$ is at most 
\[
O\left(\sqrt{\epsilon}m+m\cdot \left(\sqrt{\left(\gamma \sqrt{\log \frac{1}{\epsilon}} + 2\eps\right)\log \frac{1}{\epsilon}}+\frac{\eps}{\gamma}\right)\right) \leq \tilde{O}\left(m\eps^{1/3}\right)
\]
This completes the proof of Theorem~\ref{thm:maj}. 
In the rest of the subsection, we prove Lemma~\ref{lem:maj}.

Let $\mathcal P$ be the convex hull of $P$, where the tuples are viewed as vectors in $\R^k$. 
\[
\mathcal{P} := \left\{ \sum_{\textbf{a} \in P} \lambda_{\textbf{a}} \textbf{a} : \lambda_\textbf{a}\geq 0\,~\forall \textbf{a}\in P,~ \sum_{\textbf{a} \in P}\lambda_\textbf{a}=1\right\}.
\]
We prove the following property about $\mathcal P$ using the fact that the PCSP $(P,Q)$ has Majority of all odd arities as polymorphisms. We recall that $O_{\MAJ}(P)$ denotes the set $\bigcup_{\textbf{x}_1,\ldots,\textbf{x}_L\in P,L \in \mathbb{N},\text{ odd}}\MAJ_L(\textbf{x}_1,\ldots,\textbf{x}_L)$ for a predicate $P \subseteq \{-1,+1\}^k$.


\begin{lemma}
\label{lem:separating-hyperplane}
Let $P \subseteq \{-1,+1\}^k$ be a predicate such that $(-1,-1,\hdots, -1) \not\in O_{\MAJ}(P)$. Then, there is a hyperplane separating $\mathcal{P}$ from the origin: there exists $\textbf{w} \in \mathbb{R} ^k$, $\textbf{w} \geq 0$ and $\norm{\textbf{w}}_1 = 1$ such that for every $\textbf{a} \in \mathcal{P}$, $\langle \textbf{a}, \textbf{w} \rangle \geq 0$. 
\end{lemma}

\begin{proof}
Consider the following linear program,
\begin{align*}
\textbf{maximize: } & \eta\\
\textbf{subject to: } & \sum_{i=1}^k w_i = 1\\
                      & \forall a \in P, \eta - \sum_{i=1}^k a_i w_i \le 0 \\ 
                      & \bw \ge 0
\end{align*}

It suffices to prove that the objective of this linear program is non-negative. To do this, we consider the dual program on variables $\nu \in \R$ and $\lambda_a \in \R$ for $a \in P$:
\begin{align*}
\textbf{minimize: } & \nu \\
\textbf{subject to: } & \lambda \ge 0 \wedge \sum_{a \in P} \lambda_a = 1 \tag{dual of $\eta$}\\
    &\forall i \in [k], \nu - \sum_{a \in P} a_i \lambda_a \ge 0 \tag{dual of $\bw$}
\end{align*}

As all the coefficients used in the LP are rational, we may assume that $\nu$ and $\lambda$ are rational. Assume for sake of contradiction that there is a solution to the dual LP with $\nu < 0$. Then, we have that for all $i \in [k]$,
\[
\sum_{a \in P} a_i \lambda_a < 0.
\]
Let $N$ be the least common denominator of the $\lambda_a$s. Consider the set of satisfying assignments to $P$ where we take $2N \lambda_a$ copies of $a$ for each $a \in P$. We also add an arbitrary element $b$ of $a$ to our set. As $\sum_{a \in P}a_i \lambda_i N <0$ for every $i \in [k]$, and $\sum_{a \in P}a_i \lambda_i N$ is an integer, we get that for every $i\in [k]$, $2\sum_{a \in P}a_i \lambda_a N + b \leq 0$. In other words, when we apply $MAJ_{2N+1}$ coordinatewise to this set of assignments in $P$, we get $(-1, -1, \ldots, -1)$. As $(P, Q)$ contains Majority of all odd arities as polymorphisms, this implies that the resulting output $(-1,-1,\ldots, -1)$ is in $Q$, a contradiction. 

Thus, the objective $\eta$ of the original LP is non-negative, completing the proof. 
\end{proof}

Now we use this lemma to complete the proof of Lemma~\ref{lem:maj}. Recall that our goal is to lower bound the probability that the assignment $\sigma$ output by the above algorithm weakly satisfies the constraint $C_j$. Suppose that the constraint $C_j$ is on a tuple of literals $S_j=(x_{j,1},x_{j,2},\ldots,x_{j,k})$, and uses the predicate pair $(P,Q)$ where $P\subseteq Q = \{-1,+1\}^k\setminus\{(-1,-1,\ldots,-1)\}$. We first simplify the notation a bit.
Let $K=2^k$. We order all the tuples in $\{-1,+1\}^K$ as $\textbf{a}_1, \textbf{a}_2, \ldots, \textbf{a}_K$. 
\[
\{ \textbf{a}_1, \textbf{a}_2, \ldots, \textbf{a}_K \} := \{-1,+1\}^k.
\]
We can also view the tuple $\textbf{a}_i$ as a function $f_i : S_j\rightarrow \{-1,+1\}$ where $f_i(x_{j,p})=(\textbf{a}_i)_p$. 
We use $p_1, p_2, \ldots, p_K$ to denote the probabilities assigned by the SDP solution corresponding to the $K$ local assignments $\textbf{a}_1, \ldots, \textbf{a}_K$.
\[
p_i := \lambda_j (f_i).
\]
We have that each $p_i \geq 0$, $\sum_{i \in [K]}p_i = 1$ and 
    \[
    \sum_{i \in [K], \textbf{a}_i \in P}p_i = 1-c
    \]

Using~\Cref{lem:separating-hyperplane}, we get $\bw \in \R^k$ with $\bw \geq 0$ and $\norm{\bw}_1=1$ such that $\bw^T \textbf{a}_i \geq 0$ for all $\textbf{a}_i \in P$.     
Combining this with the above properties of the basic SDP solution, we get the following. 
\begin{enumerate}
\itemsep=-0.5ex
\vspace{-1ex}
    \item (First moment). We have 
    \begin{align*}
    \bw ^T \mu &= \sum_{i \in [K]} p_i \bw^T\textbf{a}_i \\
    & \geq -c \text{ (Using \Cref{lem:separating-hyperplane} and $-1 \leq \bw^T\textbf{a}_i \leq 1 \,\forall i \in [K]$ )}
    \end{align*}
    \item (Second moment). We have
    $$
    \bw ^T \Sigma \bw = \sum_{i \in K}p_i (\bw^T \textbf{a}_i)^2  \leq  \sum_{i \in K, \textbf{a}_i \in P}p_i (\bw^T \textbf{a}_i)^2 + c \leq \sum_{i \in K, \textbf{a}_i \in P}p_i \bw^T \textbf{a}_i + c \leq \bw^T \mu + 2c \ . $$
\end{enumerate}
%
We do casework on the value of $\bw^{T}\mu$. First, consider the case that $\bw^T\mu \geq \kappa = \gamma \sqrt{\log \frac{1}{\epsilon}}$. As $\norm{\bw}_1$=1, and $\bw \geq 0$, there exists $i \in [k]$ such that $\mu_i \geq \kappa$. As $\zeta_i \sim \mathcal{N}(0,1)$, using~\Cref{prop:gaussian-concentration}, with probability at least $1-\sqrt{\epsilon}$, we have $\zeta_i \geq -\frac{\mu_i}{\gamma}$. Thus, with probability at least $1-\sqrt{\epsilon}$, the rounded solution satisfies $Q$.  

%
Henceforth, we assume $\bw^{T}\mu < \kappa $.
For notational convenience let $\bt = -\mu / \sqrt{\eps}$.  We have 
\begin{equation}
\label{eq:wt-atmost}
\bw^{T} \bt \le \frac{c}{\gamma}
\end{equation}
and  $\bw^{T}\Sigma \bw \le \kappa +2c$. Note that $\bw ^T\zeta \sim \mathcal{N}(0, \bw^T \Sigma \bw)$. Thus, using~\Cref{prop:gaussian-concentration},  
with probability at least $1-\sqrt{\epsilon}$, we have that 
\begin{equation}
\label{eq:wzeta-atmost}
|\bw^{T} \zeta| \le O\left( \sqrt{(\kappa + 2c)\log \frac{1}{\epsilon}}\right)
\end{equation}

%
 %
Note that the rounded solution does not satisfy $Q$ only if $\bt \geq \zeta$. We now upper bound the probability that this can occur. Together with~\Cref{eq:wt-atmost} and~\Cref{eq:wzeta-atmost},
 $\bt \geq \zeta$ implies that  
\[0 \le \bw^{T}(\bt - \zeta) \le O\left(\sqrt{(\kappa + 2c)\log \frac{1}{\epsilon}}+\frac{c}{\gamma}\right).\]

Take some coordinate with $w_i \ge 1/k$ and note that
\[
    \bt_i - \zeta_i \in \left[0, O\left(\sqrt{(\kappa + 2c)\log \frac{1}{\epsilon}}+\frac{c}{\gamma}\right)\right],
\]
but this can only happen with probability $O\left(\sqrt{\kappa + c}\log \frac{1}{\epsilon}+\frac{c}{\sqrt{\epsilon}}\right)$ using~\Cref{prop:gaussian-anticoncentration}. Thus, the probability that the assignment $\sigma$ does not satisfy $Q$ is at most 
\[
O\left(\sqrt{\epsilon}+\sqrt{(\kappa + 2c)\log \frac{1}{\epsilon}}+\frac{c}{\gamma}\right).
\]

This completes the proof of Lemma~\ref{lem:maj}.

\subsection{Warm-up for $\AT$: Random threshold rounding algorithm for Dual-Horn SAT}



As a stepping-stone for our algorithm for $\AT$ presented in the next section, we present a robust algorithm for the Dual-Horn SAT CSP. In the Dual-Horn SAT CSP $\Gamma=(P_1, P_2, \ldots, P_l)$, the predicates are all of the $k$-SAT form, i.e., $P_i = \{-1,+1\}^{k_i}\setminus\{(-1,-1,\ldots,-1)\}$, with the additional restriction that in every constraint, there is at most one literal that is a negated variable. A robust algorithm for Dual-Horn SAT was found previously by Zwick~\cite{Zwick98}, and a matching hardness result was obtained by Guruswami and Zhou\cite{GuruswamiZ12}. We now present a robust algorithm for Dual-Horn SAT achieving similar guarantees as~\cite{Zwick98}. The random thresholding idea that we use here serves as a good warmup for the AT algorithm presented in the next subsection.   

%

%

\smallskip 
\noindent\fbox{%
    \parbox{\textwidth}{%
    \begin{enumerate}
                \item Given an instance $\Phi$ of $\Gamma$ containing $n$ variables $\{u_1,u_2,\ldots,u_n\}$,
        solve the basic SDP and obtain a set of vectors $\textbf{v}_0, \textbf{v}_1, \ldots, \textbf{v}_n$. Let $\mathbf{\mu} \in \R^n$ denote the first moments.  
        \begin{align*}
            \mu_i &= \textbf{v}_i \cdot \textbf{v}_0 \quad \forall i \in [n]
        \end{align*}        
        \item Let $T$ be a geometric progression with first term $\sqrt{\eps}$, last term $1/(K)$ and spacing between terms is at least $K$, where $K$ is the maximum arity of the predicates in $\Gamma$. 
        %
        \item Sample a uniformly random threshold $t \in T$.
        \item For each $i \in [n]$, round as follows
        \[
            \sigma(x_i) = \begin{cases}
            +1 & \text{ if } y_i \ge t-1.\\
            -1 & $\text{otherwise}$.
            \end{cases}
        \]
    \end{enumerate}
            }%
}
\smallskip 

\begin{theorem}\label{thm:or}
Let $\Phi$ be an instance of Dual-Horn SAT using the $\CSP(\Gamma)$ for which there is a basic SDP solution with error at most $\eps$. Then, the assignment $\sigma$ output by the above algorithm satisfies $1 - O_{\Gamma}(1/\log(1/\eps))$ fraction of the constraints of $\Phi$ in expectation. 
\end{theorem}

\begin{proof}
%
As with the analysis of $\MAJ$, we fix a single constraint $C_j$ using the predicate $P$ and analyze the probability that $\sigma$ satisfies $C_j$.  Since the average SDP error over the constraints is equal to $\eps$, by Markov's inequality, for at least $1 - \sqrt{\eps}$ fraction of the constraints, the SDP error value is at most $\sqrt{\epsilon}$. Henceforth, we assume that $c \leq \sqrt{\epsilon}$. 

Suppose that the constraint $C_j$ uses the variables $x_1, \hdots, x_k$. First,  consider the case when none of the variables are negated. 
Using the fact that the local assignments in the SDP solution have weight at least $1-\sqrt{\epsilon}$ fraction on assignments in $P$, we get that 
\[
\sum_{i=1}^k \mu_i \geq (1-\sqrt{\epsilon})(-(k-2))+\sqrt{\epsilon}(-k) = -(k-2)-2\sqrt{\epsilon}
\]
By the pigeonhole principle, we must have some $i$ with 
\[
\mu_i \ge \frac{-(k-2)-2\sqrt{\epsilon}}{k} =-1+\frac{2-2\sqrt{\epsilon}}{k} \ge -1+\frac{1}{K}
\]
where in the final step, we assumed that $\epsilon$ is small enough that $2\sqrt{\epsilon}\leq 1$.
Thus, $\mu_i \ge t-1$ for all $t \in T$. In this case, $\sigma(x_i)=+1$, and the constraint is satisfied by $\sigma$.

Consider the case when a literal is a negated variable in the constraint. Without loss of generality, assume that $x_1$ is negated. 
Then, we have that 
\[
-\mu_1 + \mu_2 + \cdots + \mu_k \ge  (1-\sqrt{\epsilon})(-(k-2))+\sqrt{\epsilon}(-k) = -(k-2)-2\sqrt{\epsilon}.\]
In particular, there exists an integer $i \in \{2, 3, \hdots, k\}$ such that 
\[\mu_i \ge\frac{ \mu_1-(k-2)-2\sqrt{\epsilon}}{(k-1)} = -1+\frac{\mu_1+1-2\sqrt{\epsilon}}{k-1}\]
The algorithm rounds $x_i$ to $-1$ only if $t > \frac{\mu_1+1-2\sqrt{\epsilon}}{k-1}$. On the other hand, the algorithm rounds $x_1$ to $+1$ only if $t \leq \mu_1+1$. Since there is at most one element in $T$ in $(\frac{\mu_1+1-2\sqrt{\epsilon}}{k-1},\mu_1+1)$, there exists at most one $t \in T$ which would round $x_i$ to $-1$ but $x_1$ to $+1$. Therefore, the probability the constraint is satisfied is at least $1 - 1/|T|$ = $1 - 1/\log(1/\eps)$. By linearity of expectation over all the constraints with error at most $\sqrt{\epsilon}$, we get the required claim. 
\end{proof}

\begin{remark}
We remark that the above algorithm applies directly to Horn-SAT as well. Furthermore, for an arbitrary PCSP $\Gamma=\{(P_1,Q_1),(P_2,Q_2),\ldots,(P_l,Q_l)\}$ with OR polymorphisms\footnote{$\OR(x_1,x_2,\ldots,x_L)$ is equal to $+1$ if there is $i \in [L]$ with $x_i=+1$, and $-1$ otherwise.}
$\OR \subseteq \Pol(\Gamma)$, it's been proved~\cite{BrakensiekG21} that $\OR \subseteq \Pol(\Gamma')$ where $\Gamma'$ is the CSP containing the predicates $\{Q_1,Q_2,\ldots,Q_l\}$. By Schaefer's theorem~\cite{Schaefer78}, any $\CSP(\Gamma)$ with $\OR \subseteq \Pol(\Gamma)$ is ppp-definable from $k$-Horn-SAT, and thus, our algorithm proves that there is a polynomial time robust algorithm for every PCSP $\Gamma$ with $\OR \subseteq \Pol(\Gamma)$.
\end{remark}

\subsection{Algorithm for $\AT$}
\label{subsec:at-symmetric}
\newcommand{\sgn}{\operatorname{sgn}}
\newcommand{\Aff}{\operatorname{Aff}}

We now show how to combine the ideas in the previous two subsections to obtain a polynomial time robust algorithm for every Boolean folded PCSP $\Gamma=\{(P_1,Q_1),\ldots,(P_l,Q_l)\}, P_i \subseteq Q_i \subseteq \{-1,+1\}^{k_i}$ with $\AT\subseteq \Pol(\Gamma)$. 
Similar to the $\MAJ$ polymorphisms case, we can without loss of generality assume that $Q_i \neq \{-1,+1\}^{k_i}$ for every $i \in [l]$.
We first reduce an arbitrary PCSP $\Gamma$ with $\AT \subseteq \Pol(\Gamma)$ to a specific PCSP that we will work with. 
For a vector $\textbf{w} \in \R^k$, define $\sgn(\textbf{w})_i$ to be $-1$ if $w_i \le 0$ and $+1$ otherwise. Define $\Gamma_{AT}$ to be the following family of \emph{weighted hyperplane predicates}:
%
\begin{align*}
    \Gamma_{AT} &:= \{(P_{\textbf{w},b} := \{x \in \{-1,+1\}^k: \textbf{w} \cdot x = b\}, Q_{\textbf{w},b} := \{-1,+1\}^k \setminus \{\sgn(\textbf{w}), -\sgn(\textbf{w})\}) :\\&b \in \Q, \textbf{w} \in (\Q\setminus \{0\})^k,\textbf{w}.\sgn(\textbf{w})>b, -\textbf{w}.\sgn(\textbf{w})<b\}
\end{align*}
We observe that these predicates indeed have AT of all odd arities as polymorphisms. 
\begin{claim}
$\AT \subseteq \Pol(\Gamma_{AT})$
\end{claim}

\begin{proof}
Fix $b \in \Q$ and $\textbf{w} \in (\Q \setminus \{0\})^k$. Let $(P_{w,b}, Q_{w,b})$ be the corresponding predicate for these values. It suffices to show that $\AT \subseteq \Pol(P_{w,b},Q_{w,b})$. Fix an odd arity $L$ and pick $\textbf{x}_1, \hdots, \textbf{x}_L \in P_{w,b}$.  Observe that
\[
    \AT(\textbf{x}_1, \hdots, \textbf{x}_L) = \sgn(\textbf{x}_1 - \textbf{x}_2 + \cdots + \textbf{x}_L).
\]
Further, $\textbf{w} \cdot (\textbf{x}_1 - \textbf{x}_2 + \cdots + \textbf{x}_L) = b$. 
This implies that $\sgn(\textbf{x}_1 - \textbf{x}_2 + \cdots + \textbf{x}_L) \neq \sgn(\textbf{w})$ as otherwise,
\[
    b = \textbf{w} \cdot (\textbf{x}_1 - \textbf{x}_2 + \cdots + \textbf{x}_L) \ge \textbf{w} \cdot \sgn(\textbf{w}) > b.
\]
where we used the fact that the absolute value of each entry in $\textbf{x}_1-\textbf{x}_2+\ldots +\textbf{x}_L$ is at least $1$.
By a similar argument, $\sgn(\textbf{x}_1 - \textbf{x}_2 + \cdots + \textbf{x}_L) \neq - \sgn(\textbf{w})$. Thus, $\AT(\textbf{x}_1, \hdots, \textbf{x}_L) \in Q_{w,b}$, as desired.
\end{proof}

Let $\Gamma_{const}$ be the PCSP where constants can be specified. That is $\{(\{-1\},\{-1\}),(\{+1\},\{+1\})\}$. We show that an arbitrary Boolean PCSP with $\AT$ polymorphisms can be reduced to the union of the weighted hyperplane and the constant predicates. 


\begin{lemma}\label{thm:at-general}
Let $\Gamma$ be any Boolean folded PCSP with $\Gamma=\{(P_1,Q_1),\ldots,(P_l,Q_l)\}, P_i \subseteq Q_i \subsetneq \{-1,+1\}^{k_i}$ with $\AT\subseteq  \Pol(\Gamma)$.
 Then, $\Gamma$ is ppp-definable from a Boolean folded PCSP $\Gamma'$ with $\Gamma' \subseteq \Gamma_{\AT} \cup \Gamma_{const}$.
\end{lemma}
We prove~\Cref{thm:at-general} in~\Cref{subsec:at-general}. 

Recall that if a PCSP $\Gamma$ is ppp-definable from another PCSP $\Gamma'$, if $\Gamma'$ has a polynomial time robust algorithm, then $\Gamma$ has a polynomial time robust algorithm as well (up to losing constant factors in the error parameter).
In the rest of this section, we obtain a robust algorithm for a Boolean folded PCSP $\Gamma$ with $\Gamma \subseteq \Gamma_{\AT}\cup\Gamma_{const}$, thereby obtaining a robust algorithm for every Boolean folded $\Gamma$ with $\AT \subseteq\Pol(\Gamma)$. 
We state our algorithm below. \smallskip 

\noindent\fbox{%
    \parbox{\textwidth}{%
    \begin{enumerate}
        \item Given an instance $\Phi$ of $\Gamma$ containing $n$ variables $V=\{u_1,u_2,\ldots,u_n\}$,
        solve the basic SDP and obtain a set of vectors $\textbf{v}_0, \textbf{v}_1, \ldots, \textbf{v}_n$.        \item Sample a vector $\zeta \in \R^n$ by choosing each coordinate independently from $\mathcal{N}(0,1)$.
        \item Choose $\delta$ uniformly at random from $\{ p, r p, \ldots, r^{\kappa} p\}$ where $p =\epsilon^{c_1}$, and $r = \kappa = \Theta\left(\frac{\log \frac{1}{\epsilon}}{\log \log \frac{1}{\epsilon}}\right)$ such that $r^{\kappa}p=\epsilon^{c_2}$. Here, $c_1$ and $c_2$ are two arbitrary real constants satisfying $0<c_2<c_1<0.25$.
        \item For every $i \in [n]$, let $\textbf{v}_i = \alpha_i \textbf{v}_0 + \textbf{v}'_i$, where $\textbf{v}'_i$ is orthogonal to $\textbf{v}_0$. We output an assignment $\sigma$ as follows.
        \[
        \sigma(u_i) = 
        \begin{cases}
        -1, \text{ if  }\langle \zeta, \textbf{v}'_i\rangle \ge \delta\alpha_i \left| \langle \zeta,  \textbf{v}_0\rangle \right|. \\ 
        +1, \text{ otherwise.}
        \end{cases}
        \]
    \end{enumerate}
            }%
}

\smallskip 

We now analyze the algorithm. 
\begin{theorem}
\label{thm:at}
Let $\Gamma$ be a Boolean folded PCSP such that $\Gamma \subseteq \Gamma_{\AT}\cup \Gamma_{const}$. Let $\Phi$ be an instance of $\PCSP(\Gamma)$ for which there is a basic SDP solution with average error at most $\epsilon$. Then, the assignment output by the above algorithm satisfies at least $1-O_{\Gamma}\left(\frac{\log \frac{1}{\epsilon}}{\log \log \frac{1}{\epsilon}}\right)$ fraction of constraints of $\Phi$ in expectation. 
\end{theorem}
~\Cref{thm:at},~\Cref{thm:at-general} together with~\Cref{lem:maj-transform},~\Cref{thm:maj} complete the proof of~\Cref{thm:main-algorithm}. 

For ease of notation, we just use $O()$ instead of $O_\Gamma()$ when $\Gamma$ is clear from the context. As before, we prove~\Cref{thm:at} by proving a lower bound on the probability that a particular constraint is satisfied. Consider the constraint $C$ over the tuple $S=(x_{1},x_{2},\ldots,x_{k})$ using the predicate pair $(P,Q)$ where $P \subseteq Q \subseteq \{-1,+1\}^k$, and let $c$ denote the error of the SDP solution on constraint $C$. 
%
As the average value of $c$ over all the constraints is at most $\epsilon$, using Markov's inequality, at least $1-\sqrt{\epsilon}$ fraction of the constraints have SDP error at most $\sqrt{\epsilon}$. 
We restrict our analysis to these constraints with SDP error $c \leq \sqrt{\epsilon}$ and show that the rounded solution violates the predicate $Q$ with probability at most $O\left(\frac{\log \log \frac{1}{\epsilon}}{\log \frac{1}{\epsilon}}\right)$, thereby proving~\Cref{thm:at}. 


%
%
%

We first consider the case when $P=Q$ and $P \subseteq \Ham_k \{0,k\}$. 

\begin{lemma}
\label{lem:equal-case}
Let $\Gamma$ be a Boolean folded PCSP with $\AT \subseteq \Pol(\Gamma)$, and let $\Phi$ be an instance of $\PCSP(\Gamma)$. In the basic SDP solution of $\Phi$, suppose that the constraint $C$ using the predicate pair $(P,Q)$ has error at most $\sqrt{\epsilon}$, and $P=Q$, $P\subseteq \Ham_k \{0,k\}$. Then, the assignment $\sigma$ output by the above algorithm does not weakly satisfy $C$ with probability at most $O\left(\frac{\log \log \frac{1}{\epsilon}}{\log \frac{1}{\epsilon}} \right)$.
\end{lemma}

We defer the proof of~\Cref{lem:equal-case} to~\Cref{sec:missing-proofs}. 

We now consider the case when $P=P_{\textbf{w},b}$ and $Q=Q_{\textbf{w},b}$.
For ease of notation, we let $\textbf{v}_1, \textbf{v}_2, \ldots, \textbf{v}_k$ to denote the vectors assigned to the literals in the constraint $C$. 
Let $K=2^k$. As in the analysis of the CMM algorithm, we order all the tuples in $\{-1,+1\}^k$ as $\textbf{a}_1, \textbf{a}_2, \ldots, \textbf{a}_K$. 
\[
\{ \textbf{a}_1, \textbf{a}_2, \ldots, \textbf{a}_K \} := \{-1,+1\}^k.
\]
We also view the tuple $\textbf{a}_i$ as a function $f_i : S\rightarrow \{-1,+1\}$ where $f_i(x_{p})=(\textbf{a}_i)_p$. 
We use $p_1, p_2, \ldots, p_K$ to denote the probabilities assigned by the SDP solution corresponding to the $K$ local assignments $\textbf{a}_1, \textbf{a}_2, \ldots, \textbf{a}_K$.
\[
p_i := \lambda_j (f_i).
\]
We have that each $p_i \geq 0$, $\sum_{i \in [K]}p_i = 1$ and 
    \[
    \sum_{i \in [K], \textbf{a}_i \in P}p_i \geq 1-\sqrt{\epsilon}
    \]
%
Let $\textbf{v}_s = \sum_{i \in [k]}w_i\textbf{v}_i$, and let the component of $\textbf{v}_s$ along $\textbf{v}_0$ be $\alpha \textbf{v}_0$, and the component normal to $\textbf{v}_0$ be equal to $\textbf{v}'_s$.
\[
\textbf{v}_s = \alpha \textbf{v}_0 + \textbf{v}'_s,\langle \textbf{v}_0, \textbf{v}'_s\rangle = 0.\] 
We use the first and second moments properties of the vectors $\textbf{v}_1, \textbf{v}_2, \ldots,\textbf{v}_k$ to get the following. 
\begin{enumerate}
    \item (First moments). We have 
    \begin{align*}
    \alpha &=\sum_{i=1}^k w_i \langle \textbf{v}_i, \textbf{v}_0 \rangle \\
        &= \sum_{i=1}^k w_i \left( \sum_{j=1}^K p_j (\textbf{a}_j)_i\right) \\ 
        &= \sum_{j=1}^K p_j \left(\sum_{i=1}^k w_i (\textbf{a}_j)_i\right) = \sum_{j=1}^K p_j \langle \textbf{w}, \textbf{a}_j \rangle \\
        &= \sum_{j \in [K], \textbf{a}_j \in P} p_j \langle \textbf{w}, \textbf{a}_j \rangle + \sum_{j \in [K], \textbf{a}_j \notin P} p_j \langle \textbf{w}, \textbf{a}_j \rangle \\ 
        &= \sum_{j \in [K], \textbf{a}_j \in P} p_j b + \sum_{j \in [K], \textbf{a}_j \notin P} p_j \langle \textbf{w}, \textbf{a}_j \rangle \\ 
        &= b + \kappa 
        \end{align*}
    where 
    \[
    \kappa = \sum_{j \in [K], \textbf{a}_j \notin P} p_j(1- \langle \textbf{w}, \textbf{a}_j \rangle)
    \]
    We have $ |\kappa|=O(\sqrt{\epsilon})$. 
    \item (Second moments). We have 
    $$
        \norm{\textbf{v}_s}_2^2 = 
        \sum_{i, j \in [k]}w_i w_j \langle \textbf{v}_i, \textbf{v}_j \rangle = \sum_{i \in [K]}p_i (a_i \cdot \textbf{w})^2 = b^2 + \kappa'
        $$
    where $|\kappa'|=O(\sqrt{\epsilon})$.
\end{enumerate}
%
Thus, we get 
$\norm{\textbf{v}'_s}_2^2 = \norm{\textbf{v}_s}_2^2 - \alpha^2 
    = (b^2+\kappa')-(b+\kappa)^2$
    which is at most $O(\sqrt{\epsilon})$. 

We are now ready to analyze the algorithm. We consider two cases separately: 

\smallskip \noindent \textbf{Case $1$}. Suppose that there exists $i \in [k]$ such that $\norm{\textbf{v}'_i}_2 \geq k \delta r^2$. We claim that in this case, the rounded solution satisfies $Q$ with probability at least $1-O(\frac{1}{r})$. 

Note that $\langle \zeta, \textbf{v}'_j \rangle \sim \mathcal{N}(0, \norm{\textbf{v}'_j}_2^2)$ for every $j \in [k]$. 
Suppose that we have $\norm{\textbf{v}'_j}_2 \geq \delta r^2$ for some $j \in [k]$. 
Using~\Cref{prop:gaussian-anticoncentration}, this implies that $|\langle \zeta, \textbf{v}'_j \rangle | \geq r\delta$ with probability at least $1-\frac{1}{r}$. Furthermore, as $\langle \zeta, \textbf{v}_0 \rangle \sim \mathcal{N}(0,1)$, using~\Cref{prop:gaussian-concentration}, we get that $|\langle \zeta, \textbf{v}_0 \rangle | \leq r$ with probability at least $1-O(\epsilon)$. Thus, with probability at least $1-O(\frac{1}{r})$, $x_j$ is set to be equal to $+1$ if $\langle \zeta, \textbf{v}'_j \rangle >0$, and $-1$ otherwise. 

Hence, to show that the rounded solution satisfies $Q$, it suffices to show that there exist $i_1, i_2 \in [k]$ such that $|\langle \zeta, \textbf{v}'_{i_1} \rangle| \geq \delta r$, $|\langle \zeta, \textbf{v}'_{i_2} \rangle| \geq \delta r$, and $\langle \zeta, \textbf{v}'_{i_1} \rangle$ and $\langle \zeta, \textbf{v}'_{i_2} \rangle$ have opposite signs. 
As $\norm{\textbf{v}'_i}_2 \geq k \delta r^2$, with probability at least $1-O(\frac{1}{r})$, we have that $|\langle \zeta, \textbf{v}'_i \rangle | \geq k \delta r$. Recall that $\norm{\textbf{v}'_s}_2 \leq \epsilon^{0.25}\leq \delta$. Thus, $|\langle \zeta, \textbf{v}'_s \rangle | \leq r \delta$ with probability at least $O(\frac{1}{r})$. As $|\langle \zeta, \textbf{v}'_i \rangle | \geq k \delta r$, there exists $i' \in [k], i' \neq i$ such that $|\langle \zeta, \textbf{v}'_{i'} \rangle | \geq k \delta r$, and $\langle \zeta, \textbf{v}'_{i} \rangle$ and $\langle \zeta, \textbf{v}'_{i'} \rangle$ have opposite signs. Thus, with probability at least $1-O(\frac{1}{r})$, $i$ and $i'$ are rounded to different values, which implies that the rounded solution satisfies $Q$. 

\smallskip \noindent \textbf{Case $2$.} Suppose that for every $i \in [k]$, we have $\norm{\textbf{v}'_i}_2 \leq \frac{\delta}{2r^2}$. 

As $\langle \zeta, \textbf{v}'_i \rangle \sim \mathcal{N}(0, \norm{\textbf{v}'_i}_2^2)$, using~\Cref{prop:gaussian-concentration},
we get that with probability at least $1-O(\frac{1}{r})$, for every $i \in [k]$, $|\langle \zeta, \textbf{v}'_i \rangle | \leq \frac{\delta}{2r}$. 
On the other hand, using~\Cref{prop:gaussian-anticoncentration}, we have that $|\langle \zeta, \textbf{v}_0 \rangle | \geq \frac{1}{r}$ with probability at least $1-\frac{1}{r}$. 
Furthermore, As $\alpha_i^2 + \norm{\textbf{v}'_i}_2^2=1$ for every $i \in [k]$, we get that $|\alpha_i| \geq 1-\delta \geq \frac{1}{2}$ for every $i \in [k]$.  
Thus, with probability at least $1-O(\frac{1}{r})$, for every $i \in [k]$, $x_i$ is set to be $+1$ if $\alpha_i \leq 0$, and $-1$ otherwise. Combining this with the fact that $\sum_i w_i \alpha_i = b + O(\sqrt{\epsilon})$, and that $\sum_i w_i > b$ and $\sum_i w_i >-b$, for small enough $\epsilon$,
we get the rounded solution has variables assigned $+1$ and $-1$.

\smallskip \noindent \textbf{Completing the proof.}
We finish the proof by showing that with probability at least $1-O(\frac{1}{r})$, at least one of the above two cases holds. None of the above two cases hold if for some $i \in [k]$, we have 
\[
\frac{\delta }{2r^2} < \norm{\textbf{v}'_i}_2 < k \delta r^2
\]
Or equivalently, 
\[
\frac{\norm{\textbf{v}'_i}_2}{k r^2} < \delta < \norm{\textbf{v}'_i}_2 2 r^2
\]
This holds with probability at most $O(\frac{1}{r})$ for every value of $\norm{\textbf{v}'_i}$ as we are picking $\delta$ from $\{ p, rp, \ldots, r^\kappa p \}$ uniformly at random with $\kappa=r$. 

\subsection{Proof of~\Cref{thm:at-general}}
\label{subsec:at-general}




%

%

 For a predicate $P \subseteq \{-1,+1\}^k$, we let $\Aff(P)\subseteq \R^k$ to denote the affine hull of $P$.
\[
\Aff(P):= \left\{ \sum_{\textbf{a} \in P} \lambda_{\textbf{a}} \textbf{a} : \sum_{\textbf{a} \in P}\lambda_\textbf{a}=1, \lambda_{\textbf{a}} \in \R\right\}.
\]
We recall that $O_{\AT}(P)$ denotes the set $\bigcup_{\textbf{x}_1,\ldots,\textbf{x}_L\in P,L \in \mathbb{N},\text{ odd},}\AT_L(\textbf{x}_1,\ldots,\textbf{x}_L)$ for a predicate $P \subseteq \{-1,+1\}^k$.
To prove~\Cref{thm:at-general}, we need to use the following lemma implicit in~\cite{BrakensiekG21}.
\begin{lemma}\label{lem:AT}
Let $P\subseteq \{-1,+1\}^k$ be a predicate such that there is non-trivial dependence in each coordinate, i.e., for every $i\in[k]$, there exist vectors $\textbf{x},\textbf{y}\in P$ with $x_i = -1$ and $y_i = +1$. Then, $O_{\AT}(P) = \{\sgn(\textbf{x} - \textbf{y}) : \textbf{x}, \textbf{y} \in \Aff(P), \forall i, x_i \neq y_i \}$. 
\end{lemma}
We present the proof in~\Cref{sec:missing-proofs} for the sake of completeness. 


We also need the following claim. 
\begin{claim}\label{claim:hyperplane}
    Let $H$ be a vector space in $\R^k$ such that there is no $\textbf{y} \in H$ with $y_i > 0$ for all $i$. Then, there exists $\textbf{w}$ with $w_i > 0$ for all $i$ and $\textbf{w} \cdot \textbf{y} = 0$ for all $\textbf{y} \in H$. \end{claim}

\newcommand{\by}{\textbf{y}}

\begin{proof}
Since $H$ and the positive orthant $\R_{+}^k:= \{ \textbf{y} \in \R^k : y_i >0 \forall i \in [k]\}$  are both convex bodies, there exists $\bv \in \R^k$ and $b \in \R$ such that for all $\textbf{w}\in \R_{+}^k$, $\bw \cdot \bv > b$ and for all $\textbf{y} \in H$, $\by \cdot \bv \le b$. 
Taking the limit as $\bw \to 0$, we have that $b \le 0$. Further, since $(0,0,\ldots,0)\in H$, we must have that $b = 0$ and since $H$ is a vector space, $\by \cdot \bv = 0$ for all $\by \in H$. Thus, $\bv$ is normal to $H$. Note that $\bv$ has all coordinates positive as $\bv \cdot \bw >0$ for all $\bw$ in the positive orthant. 
\end{proof}


\begin{proof}[Proof of Lemma \ref{thm:at-general}.]
Fix a pair of predicates $(P,Q) \in \Gamma$. It suffices to show that  $(P,Q)$ is  ppp-definable from $\Gamma_{\AT} \cup \Gamma_{const}$. If $P$ has coordinates of fixed value, we can use a gadget reduction from $\Gamma_{const}$ to simulate these values. Thus, we assume that $P$ has non-trivial dependence in each coordinate, and thus we apply Lemma~\ref{lem:AT} to get that $Q \supseteq O_{\AT}(P) = \{\sgn(\textbf{x} - \textbf{y}): \textbf{x}, \textbf{y} \in \Aff(P), \forall i, x_i \neq y_i \}.$ We may without loss of generality assume that $Q = \{\sgn(\textbf{x} - \textbf{y}) : \textbf{x}, \textbf{y} \in \Aff(P), \forall i, x_i \neq y_i \}.$ 

For every $\textbf{x} \in \{-1,+1\}^k\setminus Q$, we find $\textbf{w}, b$ such that 
\[
P_{\textbf{w},b} := \{x \in \{-1,+1\}^k: \textbf{w} \cdot x = b\}, Q_{\textbf{w},b} := \{-1,+1\}^k \setminus \{\sgn(\textbf{w}), -\sgn(\textbf{w}\})
\]
satisfy $P \subseteq P_{\textbf{w},b}, P_{\textbf{w},b} \subseteq Q_{\textbf{w},b}$ and $\sgn(\textbf{w})=\textbf{x}$. By applying this for every $\textbf{x} \in \{-1,+1\}\setminus Q$, we get a set of predicate pairs $(P_1, Q_1), (P_2, Q_2), \ldots, (P_L,Q_L)$ with $L\leq 2^k$ such that 
\begin{enumerate}
\item $P \subseteq P_i $ for every $i \in [L]$. 
\item $(P_i, Q_i) \in \Gamma_{AT}$ for every $i \in [L]$. 
%
\item $\bigcap_{i \in [L]} Q_i = Q$. 
\end{enumerate}
This shows that $(P,Q)$ is ppp-definable from $\{(P_1, Q_1), (P_2, Q_2), \ldots, (P_L,Q_L)\}\subseteq \Gamma_{AT}$.

Henceforth, our goal is to show that for every $\textbf{x} \in \{-1,+1\}^k\setminus Q$, we can find $\textbf{w}, b$ such that $P_{\textbf{w},b} := \{x \in \{-1,+1\}^k: \textbf{w} \cdot x = b\}$
satisfies $P \subseteq P_{\textbf{w},b}$, $\textbf{w}\cdot \sgn(\textbf{w}) >b, \textbf{w}\cdot \sgn(\textbf{w}) >-b$ and $\sgn(\textbf{w})=\textbf{x}$. Without loss of generality, we can assume that $\textbf{x}=(+1,+1,\ldots,+1)$. Fix an arbitrary vector $\overline{\textbf{x}} \in P$ such that $\overline{\textbf{x}}\notin \{(-1,-1,\ldots,-1), (+1,+1,\ldots,+1)\}$. Such a vector is guaranteed to exist as $P$ does not contain $\overline{\textbf{x}}$ and has non-trivial dependence on each coordinate. 
Let $H$ be a subspace of $\mathbb{R}^k$ defined as follows:
\[
H := \{ \textbf{y}-\overline{\textbf{x}} : \textbf{y} \in \Aff(P)\}
\]
As $\textbf{x} \notin O_{AT}(P)$, using~\Cref{lem:AT}, we get that for every $\textbf{z} \in H$, $\sgn(\textbf{z})\neq \textbf{x}$, or in other words, there is no $\textbf{z} \in H$ with $z_i >0$ for all $i \in [k]$. Using~\Cref{claim:hyperplane}, we can obtain $\textbf{w}$ such that $\textbf{w}\cdot \textbf{y}=0$ for all $\textbf{y} \in H$, and $w_i >0$ for all $i \in [k]$. This shows that $\textbf{w}\cdot \textbf{y} = b$ for every $\textbf{y} \in P$, where $b = \textbf{w}\cdot \overline{\textbf{x}}$ satisfies $\sum_i w_i >b$, $\sum_i w_i >-b$. \end{proof}




\section{Unique Games based Hardness}
\label{sec:ug-hardness}

In this section, we prove~\Cref{thm:main-hardness}. 

First, we use the analysis of $\AT$ and $\MAJ$ polymorphisms for symmetric PCSPs with folding and idempotence in~\cite{BrakensiekG21} to show that we can relax $\Gamma$ into one of five candidate PCSP types. 
\begin{lemma}
\label{lem:reduction}
Let $\Gamma=(P,Q)$ be a Boolean folded symmetric idempotent PCSP such that $
\MAJ_{L_1}, \AT_{L_2} \notin \Pol(\Gamma)$ for some odd integers $L_1, L_2$. Then, there exists a Boolean folded PCSP $\Gamma'=(P,Q)$ that is a relaxation of $\Gamma$ that is equal to either of the following:
\begin{enumerate}
    \item $k$ is even, and $\Gamma_1 = (P,Q), P=\Ham_{k}\{\frac{k}{2}\}, Q = \Ham_{k} \{0,1,\ldots,k\} \setminus \{b\}$ where $b \in \{1,k-1\}$. 
    \item $k$ is odd, $\Gamma_2 = (P,Q), P=\Ham_k \{l,\frac{k+1}{2}\}$, $Q=\Ham_k \{0,1,2,\ldots,k-1\}$, where $l \leq \frac{k-1}{2}$.
    \item $\Gamma_3 = (P,Q), P=\Ham_k \{l,k\}$, $Q=\Ham_k \{1,2,\ldots,k\}$, where $l\neq 0, l \leq \frac{k-1}{2}$. 
    \item $\Gamma_4 = (P,Q),P=\Ham_k \{l\}, Q=\Ham_k \{0,1,\ldots,k\}\setminus \{0,k-1\}$ where $l \in \{1,2,\ldots,k-1\}, l \leq \frac{k-1}{2}$. 
    \item $\Gamma_5=(P,Q), P=\Ham_k \{1,k\}, Q=\Ham_k \{0,1,\ldots,k\}\setminus\{b\}$ for arbitrary $b \in \{0,1,\ldots,k\}$.
\end{enumerate}
\end{lemma}
%
%
We defer the proof of~\Cref{lem:reduction} to~\Cref{sec:missing-proofs}. 

Recall that if a PCSP $\Gamma'$ is a relaxation of another PCSP $\Gamma$, if $\Gamma$ has a polynomial time robust algorithm, then $\Gamma'$ has a polynomial time robust algorithm as well. Thus, to show~\Cref{thm:main-hardness}, it suffices to show NP-hardness of obtaining robust algorithms for the PCSPs $\Gamma_{1\text{---}5}$.
We achieve this by showing integrality gaps for the basic SDP relaxation of them. 
Raghavendra's result for CSPs~\cite{Raghavendra08} shows that integrality gaps for the basic SDP relaxation can be translated to Unique Games Conjecture (UGC)~\cite{Khot02} based inapproximability results. In fact, his result is verbatim applicable to Promise CSPs as well. 
\begin{theorem}[Special case of ~\cite{Raghavendra08} for Boolean folded PCSPs when the SDP is feasible]
\label{thm:raghavendra}
Suppose that for a Boolean folded PCSP $\Gamma$, there is a finite integrality gap for the basic SDP relaxation, i.e., there is a finite instance $I$ of $\PCSP(\Gamma)$ on which the basic SDP relaxation is feasible but there is no assignment that weakly satisfies $I$. Then, there exists a constant $s<1$ that is a function of $\Gamma, I$ such that the following decision problem is NP-hard for sufficiently small $\epsilon,\delta >0$, assuming UGC. Given an instance $\Phi$ of $\Gamma$, distinguish between the two cases:
\begin{enumerate}
    \item (Completeness.) There exists an assignment that strongly satisfies $1-\epsilon$ fraction of the constraints in $\Phi$.
    \item (Soundness.) No assignment weakly satisfies $s+\delta$ fraction of the constraints in $\Phi$.
\end{enumerate}
\end{theorem}

Thus, to show~\Cref{thm:main-hardness}, our goal is to show the existence of finite integrality gaps for the basic SDP relaxations of the Boolean folded PCSPs in~\Cref{lem:reduction}.
To obtain such an integrality gap for the basic SDP relaxation of a PCSP, we study colorings of the $n$ dimensional sphere $\mathbb{S}^n$ that satisfy certain properties. 
We start by defining a few notations that we need. 

\begin{definition}
\label{def:p-conf}
Fix a predicate $P \subseteq \{-1,+1\}^k$. We say that a tuple of vectors $V=(\textbf{v}_1, \textbf{v}_2, \ldots, \textbf{v}_k), \textbf{v}_i \in \S^n\,\forall i \in [k]$ are a $P$-configuration with respect to another vector $\textbf{v}_0\in \S^n$ if the tuple of vectors can be assigned to a set of literals in a constraint by the basic SDP relaxation of an instance of $\PCSP(P,Q)$ with zero error, for some $Q \supseteq P$. 
In other words, there exists a probability distribution $\{\lambda(\textbf{a}): \textbf{a} \in P\}$ supported on $P$ that satisfies the following properties. 
\begin{enumerate}
    \item $0 \leq \lambda(\textbf{a})\leq 1$ for all $\textbf{a} \in P$, and 
    \[
    \sum_{\textbf{a} \in P}\lambda(\textbf{a})=1.
    \]
    \item First moments: 
    \[
    \textbf{v}_i \cdot \textbf{v}_0 = \sum_{ \textbf{a} \in P}\lambda(\textbf{a}) a_i\quad \forall i \in [k].
    \]
    \item Second moments: 
    \[ \textbf{v}_{i} \cdot \textbf{v}_{i'} = \sum_{\textbf{a} \in P}\lambda(\textbf{a}) a_i a_{i'} \quad \forall i,i' \in [k]. \]
\end{enumerate}
\end{definition}

We now define the notion of a function respecting a Boolean folded PCSP. We refer to functions $f:\S^n \rightarrow \{-1,+1\}$ as colorings of the sphere. 

\begin{definition}
Fix a vector $\textbf{v}_0$ and we say that a coloring of the sphere $f:\mathbb{S}^n \rightarrow \{-1,+1\}$ that is folded, i.e., $f(-\textbf{v})=-f(\textbf{v})$ for every $\textbf{v}\in \S^n$ respects the Boolean folded PCSP $(P,Q)$ with respect to a vector $\textbf{v}_0\in\S^n$ if the following condition holds. For every $P$-configuration $V=(\textbf{v}_1, \textbf{v}_2, \ldots, \textbf{v}_k)$ with respect to $\textbf{v}_0$, we have that the colors of the vectors satisfy $Q$, i.e.,
\[
 (f(\textbf{v}_1), f(\textbf{v}_2), \ldots, f(\textbf{v}_k)) \in Q
\]
More generally, we say that a coloring $f:\mathbb{S}^n \rightarrow \{-1,+1\}$ respects a Boolean folded PCSP $\Gamma$ with respect to a vector $\textbf{v}_0$ if it respects every predicate pair in $\Gamma$ with respect to $\textbf{v}_0$.
\end{definition}

Our key observation is that the absence of such a sphere coloring respecting $\Gamma$ for some finite $n$ gives an integrality gap for the basic SDP relaxation of $\Gamma$. 
\begin{lemma}
\label{lem:sphere-coloring}
For every Boolean folded PCSP $\Gamma=\{(P_1,Q_1),\ldots,(P_l,Q_l)\}$, the Basic SDP decides $\PCSP(\Gamma)$ if and only if for every integer $n \geq 1$, there exists a coloring $f^{(n)}:\mathbb{S}^n \rightarrow \{-1,+1\}$ that respects $\Gamma$ with respect to a vector $\textbf{v}_0^{(n)}$.
%
\end{lemma}
\begin{proof}
We slightly abuse the notation and say that a folded function $f: S\rightarrow \{-1,+1\}, S \subseteq \S^n$ respects a Boolean PCSP $(P,Q)$ w.r.t. $\textbf{v}_0$ if and only if for every $P$-configuration of vectors $V=(\textbf{v}_1,\ldots,\textbf{v}_k)$ w.r.t. $\textbf{v}_0$ with $\textbf{v}_i \in S\,\forall i \in [k]$, $f(V) \in Q$. More generally, for a Boolean folded PCSP $\Gamma$, $f:S \rightarrow \{-1,+1\}$ respects $\Gamma$ if and only if it respects every predicate pair in $\Gamma$.
Via a compactness\footnote{We assume the axiom of choice.} argument (e.g., like the De Brujin-Erdos theorem \cite{bruijn1951colour}, for more details see Remark 7.13 of \cite{BBKO21} or \cite{ciardo2022clap}), we can infer that there is a coloring $f:\mathbb{S}^n \rightarrow \{-1,+1\}$ respecting $\Gamma$ w.r.t. $\textbf{v}_0$ if and only if for every finite subset $S \subset \mathbb{S}^n$, there exists a coloring $f_S:S\rightarrow \{-1,+1\}$ that respects $\Gamma$ w.r.t. $\textbf{v}_0$. 

First, assume that the Basic SDP decides $\PCSP(\Gamma)$. Fix an arbitrary set of vectors $\textbf{v}_0^{(n)}, n \in \mathbb{Z}^{+}$.
For any finite subset $S \subset \mathbb{S}^n$, we construct an instance $\Phi_S$ of $\Gamma$ as follows. The variable set is $x_{\textbf{v}}, \textbf{v}\in S$. We add a constraint over $(P_i,Q_i)$ using the variables $x_{\bv_{1}}, \ldots, x_{\bv_{k_i}}$ if $V=(\textbf{v}_1, \textbf{v}_2, \ldots, \textbf{v}_{k_i})$ is a $P_i$-configuration w.r.t. $\textbf{v}_0^{(n)}$ for $i \in [l]$. 
We have that $x_{\bv} \mapsto \bv$ is a basic SDP solution with zero error. Thus, there exists an assignment to the variables that weakly satisfies all the constraints in $\Phi_S$, or equivalently, there exists $f_S: S\rightarrow \{-1,+1\}$ that respects the PCSP $\Gamma$ w.r.t. $\textbf{v}_0^{(n)}$.
Thus, there exists a coloring $f:\mathbb{S}^n \rightarrow \{-1,+1\}$ that respects $\Gamma$ w.r.t. $\textbf{v}_0^{(n)}$ for every positive integer $n$.

Second, suppose that for every integer $n \geq 1$, there exists a coloring $f^{(n)}:\mathbb{S}^n \rightarrow \{-1,+1\}$ that respects $\Gamma$ w.r.t. some vector $\textbf{v}_0^{(n)}$. We seek to show that the Basic SDP decides $\PCSP(\Gamma)$. Take an arbitrary instance $\Phi$ of $\PCSP(\Gamma)$ such that there is a solution to Basic SDP with zero error. We solve the SDP relaxation of $\Phi$ and obtain a set of vectors $\textbf{v}_0$ and $ \textbf{v}_1,\ldots, \textbf{v}_n \in \S^n$ corresponding to the variables in $\Phi$ that satisfies all the constraints in $\Phi$ with zero error. As there is a function $f^{(n)}:\mathbb{S}^n \rightarrow \{-1,+1\}$ that respects $\Gamma$ w.r.t. some vector $\textbf{v}_0^{(n)}$, by rotating $f^{(n)}$, we get a function $f'^{(n)}: \S^n \rightarrow \{-1,+1\}$ that respects $\Gamma$ w.r.t. $\textbf{v}_0$. The assignment $f'^{(n)}(\textbf{v}_i)$ to the variable $u_i$ weakly satisfies all the constraints in $\Phi$. Thus, for every instance $\Phi$ of $\PCSP(\Gamma)$ with zero error on the basic SDP relaxation, there is an assignment that weakly satisfies all the constraints in $\Phi$, or equivalently, the basic SDP decides $\Gamma$. \end{proof}

%


As a corollary, we get the following. 
\begin{corollary}
\label{cor:basic-bool}
Let $\Gamma$ be a Boolean folded PCSP. Then, there is a finite integrality gap for the basic SDP relaxation of $\Gamma$ if and only if for some positive integer $n$, there exists no folded coloring $f: \S^n \rightarrow \{-1,+1\}$ that respects $\Gamma$.
\end{corollary}

~\Cref{thm:raghavendra} together with~\Cref{cor:basic-bool} shows that if for a Boolean folded PCSP $\Gamma$ does not admit a sphere coloring $f: \mathbb{S}^n \rightarrow \{-1,+1\}$ that respects $\Gamma$ for some positive integer $n$, then, $\Gamma$ does not admit a polynomial time robust algorithm, assuming the Unique Games Conjecture. Thus, our goal is to show that the PCSPs mentioned in~\Cref{lem:reduction} do not admit sphere coloring that respects them, and use~\Cref{cor:basic-bool} to prove~\Cref{thm:main-hardness}.

In the rest of this section, we first prove a couple of lemmas regarding sphere Ramsey theory. Then, we show that the earlier mentioned PCSPs $\Gamma_{1\text{---}5}$ do not have folded sphere coloring respecting them using the sphere Ramsey results. 
%
\subsection{Sphere Ramsey theory}

We start with a few notations.
For a tuple of vectors $S=(\textbf{v}_1, \textbf{v}_2,\ldots,\textbf{v}_k)$ with $\textbf{v}_i \in \S^{d}$, we use $\rho(S)$ to denote the sphere of the smallest radius that contains $S$ as a subset. 
\[
\rho(S):=\min\{r: \exists \textbf{c}\in \R^d, \norm{\textbf{c}-\textbf{v}_i}_2=r\,\forall i \in [k]\}.
\]
Let $S_1 = (\textbf{u}_1, \textbf{u}_2,\ldots,\textbf{u}_k)$, $S_2 = (\textbf{v}_1, \textbf{v}_2,\ldots,\textbf{v}_k)$ with $\textbf{u}_i \in \S^{d_1}$, $\textbf{v}_i \in \S^{d_2}$ be two tuples with the same arity. We say that $S_1$ and $S_2$ are congruent if they have the same pairwise inner products, i.e., $\textbf{u}_i \cdot \textbf{u}_{j}=\textbf{v}_i \cdot \textbf{v}_{j}$ for all $i,j\in [k]$.
%
Matoušek and Rödl~\cite{matouvsek1995ramsey} proved the following: 

\begin{theorem}[\cite{matouvsek1995ramsey}]
\label{thm:sphere-ramsey}
Let $S=(\textbf{u}_1, \textbf{u}_2,\ldots,\textbf{u}_k)$ be a tuple of affinely independent vectors with $\rho(S)<1$. Then, for every positive integer $r \geq 2$, there exists an integer $n_0 := n_0(S,r)$ such that for every $n \geq n_0$, for every partition $f:\mathbb{S}^n \rightarrow [r]$, there exists a tuple of vectors $S'=(\textbf{v}_1, \textbf{v}_2,\ldots,\textbf{v}_k), \textbf{v}_i\in \mathbb{S}^n\,\forall i \in [k]$ that is congruent to $S$, and is monochromatic, i.e., $f(\textbf{v}_i)=f(\textbf{v}_j)$ for every $i,j \in [k]$.
\end{theorem}


We will use this to show the following lemma regarding sphere colorings. 
\begin{lemma}
\label{lem:sphere-coloring-1}
Fix an integer $k \geq 3$ and $r \geq 2$. There exists $n_0 := n_0(k)$ such that for every $n \geq n_0$ and coloring $f: \mathbb{S}^n \rightarrow [r]$ and $\gamma \in \R$ with $ \frac{-1}{k-1} <\gamma <1$, there exists a monochromatic set of vectors $V = \{ \textbf{v}_1, \textbf{v}_2, \ldots, \textbf{v}_k \} \subseteq \mathbb{S}^n$ such that $\textbf{v}_i \cdot \textbf{v}_j = \gamma$ for every $i \neq j$. 
\end{lemma}
\begin{proof}
Consider an arbitrary set $S = \{ \textbf{u}_1, \textbf{u}_2, \ldots, \textbf{u}_k \}$ of $k$ unit vectors in $\mathbb{S}^n$ such that $\textbf{u}_i \cdot \textbf{u}_j = \gamma$ for every $i \neq j$. Such a set $S$ is guaranteed to exist when $n$ is large enough. We show that the vectors are affinely independent: suppose for contradiction that there exists reals $c_1, c_2, \ldots, c_k$ not all zero, $\sum_i c_i = 0$ and $\sum_i c_i \textbf{u}_i = 0$.
We have 
\[
0=\textbf{u}_1 \cdot \left( \sum_i c_i \textbf{u}_i \right)  = c_1 + \gamma ( c_2 + \ldots + c_k) = c_1 + \gamma ( - c_1) 
\]
implying that $c_1=0$. The same argument shows that $c_i=0$ for all $i \in [k]$, a contradiction. 

%
The set of vectors can be embedded on a sphere of radius strictly smaller than $1$: let $\alpha \in \R$ such that $0<\alpha <\frac{2}{k}$, and let $\textbf{u}_s = \sum_{i\in [k]}\textbf{u}_i$, $\textbf{c} = \alpha \textbf{u}_s$. We have 
\[
\norm{\textbf{u}_s}_2^2 = \sum_i \norm{\textbf{u}_i}_2^2 + 2 \sum_{i \neq j} \textbf{u}_i \cdot \textbf{u}_j = k +\frac{k(k-1)} \gamma 
\]
Note that 
\begin{align*}
\norm{ \textbf{u}_i - \textbf{c} }_2^2 &=  \norm{\textbf{u}_i}_2^2 + \norm{c}_2^2 - 2 \textbf{c} \cdot \textbf{u}_i \\ 
&= 1+\alpha^2 \left(k +\frac{k(k-1)} \gamma \right)- 2 \alpha (1+(k-1)\gamma) \\ 
&= 1 - k(1+(k-1)\gamma)\alpha \left( \alpha - \frac{2}{k}\right) 
\end{align*}
which is strictly smaller than $1$ when $0 < \alpha <\frac{2}{k}$. Thus, all the vectors are on a sphere centered at $\textbf{c}$ and radius strictly smaller than $1$, implying that $\rho(S)<1$. Now, we can use~\Cref{thm:sphere-ramsey} on $S$ and $f$ to obtain the required set of vectors $V$. 
\end{proof}

While~\Cref{thm:sphere-ramsey} is applicable to a wide range of sets $S$, we sometimes need to apply it to sets $S$ that do not form a simplex or have $\rho(S)=1$. Towards this, we use the ``Spreads'' based idea in~\cite{matouvsek1995ramsey} to obtain a version of~\Cref{thm:sphere-ramsey} directly for certain sets $S$ where~\Cref{thm:sphere-ramsey} is not applicable. 

We use the following notion of $\Spread$ vectors from~\cite{matouvsek1995ramsey}. For an integer $n$, a vector $\textbf{a} \in \R^k$, and a set $J \subseteq [n]$ of cardinality $k$ with $J=\{ j_1, j_2, \ldots, j_k\}$, 
we let 
\[
\Spread_n(\textbf{a}, J) = \sum_{i=1}^k a_i e_{j_i}
\]
where $e_1, e_2, \ldots, e_n$ is an orthonormal basis of $\R^n$. 
For a set $I \subseteq [n]$, we let 
\[
\Spread_n(\textbf{a}, I) = \{ \Spread_n (\textbf{a}, J) : J \subseteq I, |J|=k \}
\]
We get the following as a direct application of the hypergraph Ramsey theorem. 
\begin{lemma}(~\cite{matouvsek1995ramsey})
\label{lem:spreads-ramsey}
For every $\textbf{a} \in \R^k, n, k$, there exists $N$ such that in any coloring $f: \Spread_N(\textbf{a}, [N]) \rightarrow [r]$, there exists $I$ with $|I|=n$ such that $\Spread_N(\textbf{a},I)$ is monochromatic with respect to $f$, i.e., $\exists p \in [r]$ such that $f(\textbf{v})=p$ for all $\textbf{v} \in \Spread_N(\textbf{a},I)$. 
\end{lemma}

~\Cref{lem:spreads-ramsey} implies the following immediately. 
\begin{corollary}
\label{coroll:spreads-ramsey}
Let $U=\{\textbf{u}_1, \textbf{u}_2, \ldots, \textbf{u}_k \}$ be a set of $k$ unit vectors such that $\textbf{u}_i \in \Spread_N(\textbf{a},[N]) $ for all  $i \in [k]$ for an integer $N$, and a vector $\textbf{a} \in \R^N$ with $\norm{\textbf{a}}_2=1$. Then there exists $n_0 := n_0(U,\textbf{a},N)$ such that for every $n \geq n_0,r$, for every sphere coloring $f: \S^n \rightarrow [r]$, there exists a set of $k$ vectors $V=\{\textbf{v}_1, \ldots, \textbf{v}_k\}$ that are all colored the same, and $\textbf{v}_i \cdot \textbf{v}_j = \textbf{u}_i \cdot \textbf{u}_j$ for every $i,j \in [k]$. 
\end{corollary}

We use~\Cref{coroll:spreads-ramsey} to prove a lemma regarding sphere colorings. For ease of notation, we call a set of $k$ unit vectors $V=\{ \textbf{v}_1, \textbf{v}_2, \ldots, \textbf{v}_k \}$ to be \textit{$k$-regular} if $\textbf{v}_i \cdot \textbf{v}_j = -\frac{1}{k-1}$ for every $i \neq j$. 

\begin{lemma}
\label{lem:sphere-coloring-2}
Fix an integer $k \geq 2$. There exists $n_0 := n_0(k)$ such that for every $n \geq n_0$ and folded coloring $f: \mathbb{S}^n \rightarrow \{-1,+1\}$, there exist a $k$-regular set of vectors $V = \{ \textbf{v}_1, \textbf{v}_2, \ldots, \textbf{v}_k \} \subseteq \mathbb{S}^n$ such that exactly $k-1$ vectors in $V$ are colored $-1$. 
\end{lemma}

\begin{proof}
We construct a set of $k$ unit vectors $V=\{\textbf{v}_1, \textbf{v}_2, \ldots, \textbf{v}_k \}$ in $\Spread_N(\textbf{a}, [N])$ such that  the set of vectors $\{\textbf{v}_1, \textbf{v}_2, \ldots, \textbf{v}_{k-1}, -\textbf{v}_k\}$ is a $k$-regular set, where $N$ and $\textbf{a}$ depend only on $k$, and $\norm{\textbf{a}}_2=1$. Using~\Cref{coroll:spreads-ramsey}, we can infer that in the coloring $f$, there exist $k$ vectors $V=\{\textbf{v}_1, \textbf{v}_2, \ldots, \textbf{v}_k \}$ that are all assigned the same color, such that $\{\textbf{v}_1, \textbf{v}_2, \ldots, \textbf{v}_{k-1}, -\textbf{v}_k\}$ is a $k$-regular set. As $f$ is folded, this implies that there is a $k$-regular set in which exactly $k-1$ vectors are assigned the color $-1$. 

Thus our goal is to construct $k$ unit vectors $V=\{\textbf{v}_1, \textbf{v}_2, \ldots, \textbf{v}_k \}$ in $\Spread_N(\textbf{a}, [N])$ such that the set of vectors $\{\textbf{v}_1, \textbf{v}_2, \ldots, \textbf{v}_{k-1}, -\textbf{v}_k\}$ is a $k$-regular set. Or equivalently, we construct the vectors $\textbf{v}_1, \textbf{v}_2, \ldots, \textbf{v}_{k-1} $ in $\Spread_N(\textbf{a},[N])$ and $\textbf{v}_k$ in $\Spread_N(-\textbf{a},[N])$ such that $\{ \textbf{v}_1, \ldots, \textbf{v}_k \}$ is a $k$-regular set. 
We set $\gamma = \frac{1}{\sqrt{2(k-1)}}$ and $\textbf{a} = (\gamma, -\gamma, \gamma, -\gamma, \ldots, -\gamma) \in \R^{2(k-1)}$.
We set $\textbf{v}_i = \Spread_n (\textbf{a}, J_i), i \in [k-1], \textbf{v}_k = \Spread(-\textbf{a}, J_k)$ where $J_1, J_2, \ldots, J_k$ such that $|J_i|=2(k-1)$ for every $i \in [k]$.  
We obtain these sets by induction on $k$. First, we consider the base case when $k=2$. In this case, we set $J_1 = J_2 = \{ 1,2\}$ and $N=2$ suffices. The vectors are the following:
\begin{align*}
    \textbf{v}_1 &= (\gamma, -\gamma) \\
    \textbf{v}_2 &= (-\gamma, \gamma)
\end{align*}
where $\gamma = \frac{1}{\sqrt{2}}$. Note that the above two vectors are a $2$-regular set, and letting $\textbf{a}=(\gamma, -\gamma)$, we have  $\textbf{v}_1 \in \Spread_2(\textbf{a},[2])$, and $ \textbf{v}_2 \in \Spread_2(-\textbf{a},[2])$.
Now, suppose that $J_1, J_2, \ldots, J_k, N$ are such that $\textbf{v}_i = \Spread_N(\textbf{a},J_i), i \in [k-1], \textbf{v}_k = \Spread_N(-\textbf{a},J_k)$ satisfy the property that $\{ \textbf{v}_1, \textbf{v}_2, \ldots, \textbf{v}_k \}$ is a $k$-regular set with $\textbf{a}=(\gamma, -\gamma, \ldots, \gamma, -\gamma) \in \R^{2(k-1)}$, $\gamma = \frac{1}{\sqrt{2(k-1)}}$. 
We construct $J'_1, \ldots, J'_{k+1}$ such that $\textbf{v}'_i = \Spread_{N'}(\textbf{a}',J'_i)$ for all $i \in [k]$, $\textbf{v}'_{k+1} = \Spread_{N'}(-\textbf{a}',J_{k+1})$ satisfy the property that $\{ \textbf{v}'_1, \textbf{v}'_2, \ldots, \textbf{v}'_{k+1} \}$ is a $(k+1)$-regular set with $\textbf{a}'=(\gamma', -\gamma', \ldots, \gamma', -\gamma') \in \R^{2k}$, $\gamma' = \frac{1}{\sqrt{2k}}$.
\begin{enumerate}
    \item For every $i \in [k-1]$, we obtain $J'_i$ from $J_i$ by adding two new elements. 
    \[
    J'_i = J_i \cup \{ N+2i, N+2i+1 \} 
    \]
    This ensures that $\textbf{v}'_i \cdot \textbf{v}'_j = -(\gamma')^2$ for every $i,j \in [k-1], i \neq j$. 
    \item We obtain $J_{k+1}$ from $J_k$ by adding two new elements. 
    \[
    J_{k+1} = J_k \cup \{ N+1, N+2k\}
    \]
    This ensures that $\textbf{v}'_i \cdot \textbf{v}'_{k+1} = -(\gamma')^2$ for every $i \in [k-1]$. 
    \item Finally, we set $J_k$. 
    \[
    J_k = \{ N+1, N+2, \ldots, N+2k\}
    \]
    This ensures that $\textbf{v}'_i \cdot \textbf{v}'_{k} = -(\gamma')^2$ for every $i \in [k+1],i \neq k$.
\end{enumerate}
We illustrate our construction by obtaining the vectors for the case when $k=3$ and $k=4$: 
\begin{align*}
    \textbf{v}_1 &= (\alpha,-\alpha,0,\alpha,-\alpha,0)\\
    \textbf{v}_2 &=(0,0,\alpha,-\alpha,\alpha,-\alpha)\\ 
    \textbf{v}_3 &= (-\alpha,\alpha,-\alpha,0,0,\alpha)
\end{align*}
\begin{align*}
\textbf{v}_1 &= (\beta,-\beta,0,\beta,-\beta,0,0,\beta,-\beta,0,0,0)\\
    \textbf{v}_2 &=(0,0,\beta,-\beta,\beta,-\beta,0,0,0,\beta,-\beta,0)\\ 
    \textbf{v}_3 &= (0,0,0,0,0,0, \beta,-\beta,\beta,-\beta,\beta,-\beta)\\
    \textbf{v}_4 &= (-\beta,\beta,-\beta,0,0,\beta,-\beta,0,0,0,0,\beta) 
\end{align*}
where $\alpha=\frac{1}{2}$ and $\beta=\frac{1}{\sqrt{6}}$.

As the pairwise inner product of every pair in $\{\textbf{v}'_1, \textbf{v}'_2, \ldots, \textbf{v}'_{k+1}\}$ is equal to $-(\gamma')^2=-\frac{1}{k}$, we get that these set of vectors are a $(k+1)$-regular set, completing the inductive proof. 
%
%
\end{proof}


\subsection{Absence of sphere coloring via sphere Ramsey theory}
First, we show the absence of sphere coloring respecting $\Gamma_1$ using~\Cref{lem:sphere-coloring-2}. 
\begin{lemma}
\label{lem:gamma1}
Fix an even integer $k \geq 4$. There exists an integer $n_0$ such that for every $n \geq n_0$, there is no folded $f:\mathbb{S}^n \rightarrow \{-1,+1\}$ that respects  $\Gamma_1 = (P,Q)$, $P=\Ham_{k}\{\frac{k}{2}\}$, $Q = \Ham_{k} \{0,1,\ldots,k\} \setminus \{b\}$ where $b \in \{1,k-1\}$.
\end{lemma}
\begin{proof}
Consider a large integer $n$ and suppose for the sake of contradiction that there is a folded function $f: \S^n \rightarrow \{-1,+1\}$ that respects $\Gamma_1$ with respect to a vector $\textbf{v}_0 \in \mathbb{S}^n$. 
We get the $P$-configuration of vectors $\textbf{v}_1, \textbf{v}_2, \ldots , \textbf{v}_k$ where we set $\lambda(\textbf{a})=\frac{1}{|P|}$ for every $\textbf{a}\in P$ in~\Cref{def:p-conf}.
%
The vectors satisfy the following properties. 
\begin{enumerate}
    \item (First moments.) $\textbf{v}_i \cdot \textbf{v}_0 = 0$ for every $i \in [k]$. 
    \item (Second moments.) $\textbf{v}_i \cdot \textbf{v}_j = \frac{2 \binom{\frac{k}{2}}{2} - \frac{k^2}{4}}{\binom{k}{2}}  = \frac{-1}{k-1}$. 
\end{enumerate}
Our goal is to show that there is a $P$-configuration of such vectors such that exactly $b$ of them are colored $+1$ according to $f$. 
Consider the set of vectors 
\[
\textbf{v}_0^{\perp}:= \{ \textbf{u} \in \S^n : \textbf{u}\cdot \textbf{v}_0 = 0\}
\]
Using~\Cref{lem:sphere-coloring-2}, we can obtain a set of $k$ vectors $\textbf{u}_1, \textbf{u}_2, \ldots, \textbf{u}_{k} \in \textbf{v}_0^{\perp}$ such that $\textbf{u}_i \cdot \textbf{u}_j = \frac{-1}{k-1}$ and exactly $k-1$ of $\{ \textbf{u}_1, \textbf{u}_2, \ldots, \textbf{u}_k \}$ are colored $-1$. 
\[
\left|\{i:f(\textbf{u}_i)=-1\}\right|=k-1.
\]
Note that both $\{\textbf{u}_1, \textbf{u}_2, \ldots, \textbf{u}_k \}$ and $\{-\textbf{u}_1, -\textbf{u}_2, \ldots, -\textbf{u}_k \}$ are $P$-configurations with respect to $\textbf{v}_0$. Since $f$ is folded, in at least one of these two $P$-configurations, 
there are exactly $b$ vectors that are colored $+1$, a contradiction.
\end{proof}


We show the absence of sphere coloring respecting $\Gamma_2$, $\Gamma_3$, and $\Gamma_4$ using~\Cref{lem:sphere-coloring-1}. 
%



\begin{lemma}
\label{lem:gamma2}
Fix an odd integer $k \geq 3$ and integer $l : 0 \leq l \leq \frac{k-1}{2}$. There exists an integer $n_0$ such that for every $n \geq n_0$, there is no folded $f:\mathbb{S}^n \rightarrow\{-1,+1\}$ that respects $\Gamma_2 =  (P,Q)$, $P=\Ham_k \{l,\frac{k+1}{2}\}$, $Q=\Ham_k \{0,1,2,\ldots,k-1\}$.
\end{lemma}
\begin{proof}
Consider a large integer $n$ and suppose for contradiction that there is a folded function $f : \S^n \rightarrow \{-1,+1\}$ that respects $\Gamma_2$ with respect to a vector $\textbf{v}_0 \in \mathbb{S}^n$.  
The $P$-configuration that we consider is a set of vectors $\textbf{v}_1, \textbf{v}_2, \ldots, \textbf{v}_k$ that are obtained by setting $\lambda(\textbf{a})$ in~\Cref{def:p-conf} as follows. We first sample an integer $t \in \{l, \frac{k+1}{2}\}$ as below.
%
\[
t = \begin{cases}
l,\text{ with probability}\frac{1}{1-s}. \\ 
\frac{k+1}{2},\text{ with probability}\frac{-s}{1-s}.
\end{cases}
\]
where $s = l-(k-l)<0$. 
%
The probability distribution $\lambda$ is obtained by sampling a uniform element of $\Ham_k\{t\}$. In other words, we have 
\[
\lambda(\textbf{a}):= \begin{cases} 
\frac{1}{(1-s)\binom{k}{l}}, \text{ if }\textbf{a} \in \Ham_k \{l\}.\\
\frac{-s}{(1-s)\binom{k}{\frac{k+1}{2}}}, \text{ else if }\textbf{a} \in \Ham_k \{\frac{k+1}{2}\}.\\
0, \text{ otherwise.}
\end{cases}
\]
We obtain the following properties: 
\begin{enumerate}
    \item (First moments).  $\textbf{v}_i \cdot \textbf{v}_0 = \frac{1}{1-s}(l-(k-l))+\frac{-s}{1-s}1=0$ for every $i \in [k]$. 
    \item (Second moments). By symmetry of variables, we get that $\textbf{v}_i \cdot \textbf{v}_j = \gamma$ for every $i \neq j$, for some $\gamma:=\gamma(k,l)$. 
    We have 
    \begin{align*}
        k+k(k-1)\gamma=\norm{\sum_i \textbf{v}_i}^2 = \sum_{\textbf{a}\in P}\lambda(\textbf{a}) \norm{\textbf{a}}^2>0.
    \end{align*}
    Thus, we get that $\frac{-1}{k} < \gamma < 1$. 
\end{enumerate}
Now, restricting ourselves to the vectors in $\mathbb{S}^n$ that are orthogonal to $\textbf{v}_0$, and using~\Cref{lem:sphere-coloring-1}, we get that there exists a $P$-configuration of vectors that are all colored the same. By taking the negation of these vectors if needed, we get our required claim.
%
\end{proof}

\begin{lemma}
\label{lem:gamma3}
Fix integers $k, l $ such that $0 < l \leq \frac{k-1}{2}$. Then, there exists an integer $n_0$ such that for every $n \geq n_0$, there is no folded $f:\mathbb{S}^n \rightarrow \{-1,+1\}$ that respects $\Gamma_3 = (P,Q)$, $P=\Ham_k \{l,k\}$, $Q=\Ham_k \{1,2,\ldots,k\}$, where $l\neq 0, l \leq \frac{k-1}{2}$. 
\end{lemma}
\begin{proof}
Consider a large integer $n$ and suppose for contradiction that there is a folded function $f : \S^n \rightarrow \{-1,+1\}$ that respects $\Gamma_3$ with respect to a vector $\textbf{v}_0 \in \mathbb{S}^n$.  
 We pick the $P$-configuration along the same lines as in~\Cref{lem:gamma2}. We sample $t \in \{l,k\}$ with 
\[
t = \begin{cases}
l \text{ with probability }\frac{k}{k-s}\\ 
k \text{ with probability }\frac{-s}{k-1}
\end{cases}
\]
where $s = l-(k-l)<0$. As before, the probability distribution $\lambda$ is obtained by sampling a uniform element of $\Ham_k\{t\}$. We get 
\begin{enumerate}
    \item (First moments). $\textbf{v}_i \cdot \textbf{v}_0 = \left( \frac{k}{k-s} \right) \frac{s}{k} + \left( \frac{-s}{k-s} \right) 1 =0$ for every $i \in [k]$. 
    \item (Second moments). As in~\Cref{lem:gamma2}, we have 
    $\textbf{v}_i \cdot \textbf{v}_j = \gamma$ for every $i \neq j$, for some $\gamma:=\gamma(k,l)$ with
    \begin{align*}
        k+k(k-1)\gamma=\norm{\sum_i \textbf{v}_i}^2 = \sum_{\textbf{a}\in P}\lambda(\textbf{a}) \norm{\textbf{a}}^2>0.
    \end{align*}
    Thus, we get that $\frac{-1}{k} < \gamma < 1$. 
\end{enumerate}
    We restrict ourselves to vectors in $\mathbb{S}^n$ that are orthogonal to $\textbf{v}_0$, and applying~\Cref{lem:sphere-coloring-1}, we get that for any coloring $f:\mathbb{S}^n \rightarrow \{-1,+1\}$, there is a monochromatic $P$-configuration that we described. By negating the vectors if needed, we get our required proof. 
\end{proof}

For the proof of the next case $\Gamma_4$ we will need~\Cref{lem:sphere-coloring-1} applied to $4$-colorings of the sphere. 
\begin{lemma}
\label{lem:gamma4}
Fix integers $k \geq 3, l \in \{1,\ldots,k-1\}, l \leq \frac{k-1}{2}$. 
There exists integer $n_0$ such that for every $n \geq n_0$, there does not exist coloring $f:\mathbb{S}^n \rightarrow \{0,1\}$ that is folded and respects the PCSP $\Gamma_4 = (P,Q),P=\Ham_k \{l\}, Q=\Ham_k \{0,1,\ldots,k\}\setminus \{0,k-1\}$.
\end{lemma}
\begin{proof}
As before, consider a large integer $n$ and suppose for contradiction that there is a folded function $f : \S^n \rightarrow \{-1,+1\}$ that respects $\Gamma_4$ with respect to a vector $\textbf{v}_0 \in \mathbb{S}^n$.  
We partition the predicate $P$ into $P_1$ and $P_{-1}$ depending on the value of the first element, i.e., 
\[
P_i = \{ \textbf{x} \in P: x_1 = i \}, i \in \{-1,+1\}
\]
We pick the probability distribution $\lambda$ as follows: sample $i$ from $\{-1,+1\}$ uniformly at random, then, sample a uniformly random element from $P_i$. We get 
\begin{enumerate}
    \item (First moments). By our choice of $P_i$s, we get that 
    \[
    \textbf{v}_1 \cdot \textbf{v}_0 = 0
    \]
    By using the symmetry of the rest of the variables and the fact that $\sum_{i=1}^k \textbf{v}_i \cdot \textbf{v}_0=l-(k-l)=2l-k$
    , we get that 
    \[
    \textbf{v}_i \cdot \textbf{v}_0 = \frac{2l-k}{k-1} \forall i \in \{2,3,\ldots,k\}.
    \]
    For ease of notation, let $\alpha = \frac{2l-k}{k-1}$. 
    \item (Second moments). 
    We have 
    \begin{align*}
        \textbf{v}_1 \cdot \left( \sum_{i=2}^k \textbf{v}_i\right) &= \frac{1}{2} \left( 1 \cdot (2l-k-1) \right)+ \frac{1}{2}\left( (-1) \cdot (2l-k+1)\right)\\
        &=-1
    \end{align*}
    Thus, we get 
    \begin{align*}
    \textbf{v}_1 \cdot \textbf{v}_i &= \frac{-1}{k-1}\,\forall i \in \{2,3,\ldots,k\}.
    \end{align*}
    Note that we have 
    \[
    \norm{\sum_{i=1}^k \textbf{v}_i}_2^2 = (2l-k)^2.
    \]
    Using this, we get that 
    \begin{align*}
    \textbf{v}_i \cdot \textbf{v}_j &=\frac{(2l-k)^2-(k-2)}{(k-1)(k-2)} \,\forall i,j \in \{2,3,\ldots,k\}, i \neq j
    \end{align*}
    For ease of notation, let $\beta = \frac{(2l-k)^2-(k-2)}{(k-1)(k-2)}$. 
\end{enumerate}

%
%
%
%
%
%
%
%
%
%
%
%

Our goal is to show that there exists $n_0$ such that for every $n \geq n_0$, for every folded sphere coloring $f: \mathbb{S}^n \rightarrow \{-1,+1\}$ and $\textbf{v}_0 \in \mathbb{S}^n$, there exists a set of $k$ vectors $V=\{ \textbf{v}_1, \textbf{v}_2, \ldots, \textbf{v}_k \}$ that satisfy the above first and second moments, and exactly $b$ vectors in $V$ are colored $+1$, where $b \in \{0,k-1\}$. 
For $i \in [k]$, let $\textbf{v}'_i$ be the component of $\textbf{v}_i$ orthogonal to $\textbf{v}_0$:
\[
\textbf{v}'_i = \textbf{v}_i - ( \textbf{v}_i \cdot \textbf{v}_0 ) \textbf{v}_0, i \in [k]
\]
Note that $\norm{\textbf{v}'_i}=\sqrt{1-\alpha^2}$ for $i \in \{2,\ldots,k\}$ and $\norm{\textbf{v}'_1}=1$.
We let $\textbf{u}_i = \frac{\textbf{v}'_i}{\norm{\textbf{v}'_i}}$. 
We have 
\begin{align*}
\textbf{u}_i \cdot \textbf{u}_j &= \frac{\textbf{v}'_i \cdot \textbf{v}'_j}{1-\alpha^2} \\ 
&= \frac{(\textbf{v}_i -\alpha \textbf{v}_0)\cdot (\textbf{v}_j - \alpha \textbf{v}_0)}{1-\alpha^2}\\
&= \frac{\beta - \alpha^2}{1-\alpha^2}\, \forall i,j \in \{2,\ldots,k\}, i \neq j. 
\end{align*}
For ease of notation, $\gamma = \frac{\beta-\alpha^2}{1-\alpha^2}$. 

%

We have 
\begin{align*}
    k-2l &= \norm{\sum_{i=1}^k \textbf{v}_i} \\ 
    &= \norm{\textbf{v}_1 + \sum_{i=2}^k \textbf{v}'_i + \alpha (k-1)\textbf{v}_0} \\ 
    &= \norm{\textbf{u}_1 + \sqrt{1-\alpha^2}\sum_{i=2}^k \textbf{u}_i + (2l-k)\textbf{v}_0} 
\end{align*}
As $\textbf{u}_i \cdot \textbf{v}_0 =0$ for every $i \in [k]$, we get that 
\[
\textbf{u}_1 + \sqrt{1-\alpha^2}\sum_{i=2}^k \textbf{u}_i = 0
\]
Thus, 
\begin{align*}
    \norm{\sum_{i=2}^k \textbf{u}_i}^2 = \frac{1}{1-\alpha^2}
\end{align*}
On the other hand, 
\begin{align*}
    \norm{\sum_{i=2}^k \textbf{u}_i}^2 &= k-1 + 2 \sum_{i,j \in \{2,\ldots,k\}, i \neq j} \textbf{u}_i \cdot \textbf{u}_j \\ 
    &= k-1 + (k-1)(k-2)\gamma
\end{align*}
Thus, we get 
\begin{equation}
\label{eq:gamma-bound-1}
\gamma = \frac{1}{(1-\alpha^2)(k-1)(k-2)}-\frac{1}{k-2}
\end{equation}

We apply~\Cref{lem:sphere-coloring-1} on the following coloring of the sphere. For a vector $\textbf{u} \in \S^n$ such that $\textbf{u}\cdot \textbf{v}_0=0$, 
let $f':\S^{n-1} \rightarrow \{-1,+1\}^2$ be defined as 
\[
f'(\textbf{u})=\left(f\left(\alpha \textbf{v}_0 + \sqrt{1-\alpha^2} \textbf{u}\right), f\left(\alpha \textbf{v}_0 - \sqrt{1-\alpha^2} \textbf{u}\right)\right)
\]
Using~\Cref{lem:sphere-coloring-1} on $f'$ combined with the fact that $\gamma > \frac{-1}{k-2}$ obtained from~\Cref{eq:gamma-bound-1}, we can infer that there exist $k-1$ unit vectors $\textbf{u}_1, \textbf{u}_2, \ldots, \textbf{u}_{k-1} \in \S^{n-1}$ such that $\textbf{u}_i \cdot \textbf{v}_0=0$ for all $i$, $\textbf{u}_i \cdot \textbf{u}_j =\gamma$ for all $i \neq j$ and $f'(\textbf{u}_i)=f'(\textbf{u}_j)$ for all $i \neq j, i,j \in [k-1]$. 

We define $\textbf{v}^{(1)}_1, \textbf{v}^{(1)}_2, \ldots, \textbf{v}^{(1)}_k, \textbf{v}^{(2)}_1, \ldots, \textbf{v}^{(2)}_k$ as follows. 
For $i \in \{2,3,\ldots, k\}$, we let 
\begin{align*}
    \textbf{v}^{(1)}_i = \alpha \textbf{v}_0 + \sqrt{1-\alpha^2}\textbf{u}_{i-1}\\
    \textbf{v}^{(2)}_i = \alpha \textbf{v}_0 - \sqrt{1-\alpha^2}\textbf{u}_{i-1}\\
\end{align*}
We let 
\[
\textbf{v}^{(1)}_1 = -\frac{\sum_{i=1}^{k-1} \textbf{u}_i}{\norm{\sum_{i=1}^{k-1} \textbf{u}_i}}
\]
and $\textbf{v}^{(2)}_1=-\textbf{v}^{(1)}_1$. We now prove that the set of vectors $\textbf{v}^{(1)}_1, \ldots, \textbf{v}^{(1)}_k$ and the set of vectors $\textbf{v}^{(2)}_1, \ldots, \textbf{v}^{(2)}_k$ are a $P$-configuration with first and second moments as computed earlier, where we sampled $i$ from $\{-1,+1\}$ uniformly at random and set the probability distribution $\lambda$ as the uniform distribution over $P_i$.
\begin{enumerate}
    \item (First moments). As $\textbf{u}_i \cdot \textbf{v}_0 = 0$ for all $i \in [k-1]$, we get that 
    \[
    \textbf{v}^{(1)}_1 \cdot \textbf{v}_0 = \textbf{v}^{(2)}_1 \cdot \textbf{v}_0 = 0
    \]
    and 
    \[
    \textbf{v}^{(1)}_i \cdot \textbf{v}_0 = \textbf{v}^{(2)}_i \cdot \textbf{v}_0 = \alpha \,\,\, \forall i \in \{2,\ldots,k\}
    \]
    \item (Second moments). We have $\forall i \in \{2,3,\ldots,k\}$,
    \begin{align*}
        \textbf{v}^{(1)}_1 \cdot \textbf{v}^{(1)}_i &= - \frac{(\sum_{j=1}^{k-1}\textbf{u}_j) \cdot (\alpha \textbf{v}_0 + \sqrt{1-\alpha^2}\textbf{u}_{i-1})}{\norm{\sum_{j=1}^{k-1} \textbf{u}_j}}  \\ 
        &= \frac{\sqrt{1-\alpha^2}}{k-1} -\frac{(\sum_{j=1}^{k-1}\textbf{u}_j) \cdot (\sum_{j=1}^{k-1}\textbf{u}_j) }{\norm{\sum_{j=1}^{k-1} \textbf{u}_j}} \\ 
        &= -\frac{\sqrt{1-\alpha^2}}{k-1}\norm{\sum_{j=1}^{k-1} \textbf{u}_j} \\ 
        &= -\frac{\sqrt{1-\alpha^2}}{k-1} \sqrt{ k-1+ 2 \frac{(k-1)(k-2)}{2}\gamma} \\ 
        &= -\frac{\sqrt{1-\alpha^2}}{k-1} \sqrt{ \frac{1}{1-\alpha^2}}  \text{                Using Equation}~(\ref{eq:gamma-bound-1}) \\
        &=\frac{-1}{k-1}  \ . 
    \end{align*}
    Furthermore, $\forall i,j \in \{2,3,\ldots,k\}$, $i \neq j$, 
    \begin{align*}
        \textbf{v}^{(1)}_i \cdot \textbf{v}^{(1)}_j &= ( \alpha \textbf{v}_0 + \sqrt{1-\alpha^2}\textbf{v}_i)\cdot ( \alpha \textbf{v}_0 + \sqrt{1-\alpha^2}\textbf{v}_j) \\ &= \alpha^2 + (1-\alpha^2)\gamma \\ 
        &= \beta  \ .
    \end{align*}
    Similarly, we have  
    \begin{align*}
        \textbf{v}^{(2)}_1 \cdot \textbf{v}^{(2)}_i &=  \frac{(\sum_{j=1}^{k-1}\textbf{u}_j) \cdot (\alpha \textbf{v}_0 - \sqrt{1-\alpha^2}\textbf{u}_{i-1})}{\norm{\sum_{j=1}^{k-1} \textbf{u}_j}}  = \frac{-1}{k-1}  \, \, \forall i \in \{2,3,\ldots,k\}.\\
        \textbf{v}^{(2)}_i \cdot \textbf{v}^{(2)}_j &= ( \alpha \textbf{v}_0 - \sqrt{1-\alpha^2}\textbf{v}_i)\cdot ( \alpha \textbf{v}_0 - \sqrt{1-\alpha^2}\textbf{v}_j) \\ &= \alpha^2 + (1-\alpha^2)\gamma  = \alpha^2 + (1-\alpha^2)\gamma = \beta \, \, \forall i,j \in \{2,3,\ldots,k\}, i \neq j
    \end{align*}
\end{enumerate}
Thus, both the set of vectors $\textbf{v}^{(1)}_1, \textbf{v}^{(1)}_2, \ldots, \textbf{v}^{(1)}_k$ and the set of vectors $\textbf{v}^{(2)}_1, \textbf{v}^{(2)}_2, \ldots, \textbf{v}^{(2)}_k$ are $P$-configurations with respect to $\textbf{v}_0$. As $f'(\textbf{u}_1)=f'(\textbf{u}_2)=\ldots=f'(\textbf{u}_{k-1})$, we can infer that $f(\textbf{v}^{(1)}_2)=f(\textbf{v}^{(1)}_3)=\ldots=f(\textbf{v}^{(1)}_{k})$ and $f(\textbf{v}^{(2)}_2)=f(\textbf{v}^{(2)}_3)=\ldots=f(\textbf{v}^{(2)}_{k})$. Furthermore, as $\textbf{v}^{(1)}_1=-\textbf{v}^{(2)}_1$ and $f$ is folded, we can infer that $f(\textbf{v}^{(1)}_1)=-f(\textbf{v}^{(2)}_1)$. Thus, there exists $p \in \{1,2\}$ such that $f(\textbf{v}^{(p)}_1)=-1$. Thus, there are either $0$ or $k-1$ vectors among $\textbf{v}^{(p)}_1, \textbf{v}^{(p)}_2, \ldots, \textbf{v}^{(p)}_k$ that are colored $+1$ according to $f$, contradicting the fact that $f$ respects the PCSP $(P,Q)$. 
\end{proof}

\subsection{Absence of sphere coloring via connectivity of configurations}

Finally, we show the absence of sphere coloring for $\Gamma_5$ using a connectivity lemma. 
\begin{lemma}
\label{lem:gamma5}
\label{thm:sphere-coloring-gamma}
Fix integers $k \geq 3, b \in \{0,1,\ldots,k\}\setminus\{1,k\}$. 
There exists an integer $n_0$ such that for every $n \geq n_0$, there does not exist coloring $f:\mathbb{S}^n \rightarrow \{0,1\}$ that is folded and respects the PCSP $\Gamma_5 =  (P,Q)$, $P=\Ham_k \{1,k\}$, $Q=\Ham_k \{0,1,\ldots,k\}\setminus\{b\}$.
\end{lemma}
We dedicate the rest of the section to proving~\Cref{lem:gamma5}. 
We pick the configuration of vectors along the same lines as in~\Cref{lem:gamma2}. Fix $\textbf{v}_0 \in \mathbb{S}^n$. The $P$-configuration that we study is a set of vectors $\textbf{v}_1, \textbf{v}_2, \ldots, \textbf{v}_k$ that is obtained by first sampling $t \in \{1,k\}$ such that 
\[
t = \begin{cases}
1\text{ with probability}\frac{k}{2k-2} \\ 
k\text{ with probability}\frac{k-2}{2k-2}
\end{cases}
\]
Then, we sample a uniform element from $\Ham_k \{t\}$. We get the following properties: 
\begin{enumerate}
    \item (First moments). $\textbf{v}_i \cdot \textbf{v}_0 = \left( \frac{k}{2k-2} \right) \frac{2-k}{k} + \left( \frac{k-2}{2k-2} \right) 1 =0$ for every $i \in [k]$. 
    \item (Second moments). For every $i \neq j \in [k]$, we get 
    \begin{align*}
        \textbf{v}_i \cdot \textbf{v}_j &= \left( \frac{k}{2k-2} \right) \frac{\binom{k-1}{2}-(k-1)}{\binom{k}{2}} + \left( \frac{k-2}{2k-2} \right)1 = \frac{k-3}{k-1}
    \end{align*}
\end{enumerate}

For ease of notation, let $\alpha = \frac{k-3}{k-1}$. Furthermore, by restricting ourselves to vectors in $\mathbb{S}^n$ that are orthogonal to $\textbf{v}_0$, we just focus on $P$-configurations that are a set of $k$ unit vectors all of whose pairwise inner product is equal to $\alpha$. We refer to these sets of vectors, i.e., a set $V$ of $k$ unit vectors $\textbf{v}_1, \textbf{v}_2, \ldots, \textbf{v}_k \in \mathbb{S}^n$ an $\alpha$-configuration if the inner product of every pair of them is equal to $\alpha$. Given the folded sphere coloring $f$, our goal is to show that there is an $\alpha$-configuration of vectors $V$ among which exactly $b$ of them are assigned $+1$. 

Unlike the earlier studied PCSPs, here, the setting when $b=0$ is relatively straightforward, simply because $\alpha \geq 0$. $\alpha \geq 0$ implies that there are an arbitrarily large number of unit vectors (as we can pick $n$ to be large enough) all of whose pairwise inner product is equal to $\alpha$. In particular, we pick a set of $2k-1$ unit vectors all of whose pairwise inner product is equal to $\alpha$. Among those, $k$ of them are colored the same according to $f$. By taking the negation of these if needed, we can infer that there are $\alpha$-configurations that are all colored $+1$, and also $\alpha$-configurations that are all colored $-1$. 

Before delving further, we handle the case when $\alpha =0$, i.e., when $k=3$. In this case, we just pick a set of $k$ unit vectors that are all orthogonal to each other and their negations. Note that these are $2k$ pairwise orthogonal vectors where exactly $k$ of them are colored $+1$ according to $f$. Thus, we can pick $k$ pairwise orthogonal vectors from this set where exactly $b$ of them are colored $+1$ according to $f$. Henceforth, we assume that $\alpha >0$. 

To show that there are $\alpha$-configurations that have exactly $b$ vectors that are colored $+1$, we show a connectivity lemma (\Cref{lem:connectivity}) where we prove that between any two $\alpha$-configurations, there exists a path using $O_{k,\alpha}(1)$ $\alpha$-configurations where we change a single vector at each step in the path. As there is an $\alpha$-configuration where all are $k$ vectors are colored $+1$, and the $\alpha$-configuration obtained by negating these vectors where all the vectors are colored $-1$, the connectivity lemma then shows that for every $b \in \{0,1,\ldots, k\}$, there exists an $\alpha$-configuration that has exactly $b$ vectors that are colored $+1$. 
 
We first prove a simplified version of the connectivity lemma that we use to prove~\Cref{lem:connectivity}. 

\begin{lemma}
\label{lem:vectors-connected}
Given an $\alpha$-configuration $U=\{\textbf{u}_1,\textbf{u}_2, \ldots, \textbf{u}_k\}\subseteq \mathbb{S}^n$, and a unit vector $\textbf{w} \in \mathbb{S}^n$ that is orthogonal to each vector in $U$, there exists $L := L(k, \alpha)$ and a set of $\alpha$-configurations $V_1, V_2, \ldots, V_L$ such that 
\begin{enumerate}
    \item The consecutive configurations differ in a single vector i.e.,$ |V_i \cap V_{i+1}|=k-1$ for every $i \in [L-1]$. 
    \item Final configuration contains $\textbf{w}$, i.e., $\textbf{w} \in V_L$, and the initial configuration $V_1$ is equal to $U$. 
\end{enumerate}
\end{lemma}
%
\begin{proof}
We prove the lemma by studying the inner product of $\textbf{w}$ with an $\alpha$-configuration $V$, which is equal to all zeroes initially when $V=U$, and changing $V$ one vector at a time such that the inner product of $V$ with $\textbf{w}$ eventually reaches all $\alpha$s. Towards this end, for an $\alpha$-configuration $V$, we define the matrix $(k+1)\times (k+1)$ matrix $I(V,w) = [\textbf{v}_1 \textbf{v}_2 \ldots \textbf{v}_k \textbf{w}]^T[\textbf{v}_1 \textbf{v}_2 \ldots \textbf{v}_k \textbf{w}]$ defined as follows: 
\[
I(V,\textbf{w})_{i,j} =\begin{cases}
 \langle \textbf{v}_i, \textbf{v}_j \rangle \text{ if }1 \leq i,j \leq k. \\ 
 \langle \textbf{v}_i, \textbf{w} \rangle \text{ if }i = k+1, 1 \leq j \leq k. \\ 
 \langle \textbf{w}, \textbf{v}_j \rangle \text{ if } j = k+1, 1 \leq i \leq k. \\ 
 \langle \textbf{w}, \textbf{w} \rangle = 1, \text{ if }i=j=k+1.
\end{cases}
\]

Starting with $I(V,\textbf{w})$ where $V=U$, our goal is to change one vector in $V$ at a time so that we eventually reach a configuration where the last column in $I(V,\textbf{w})$ is equal to $(\alpha, \alpha, \ldots, \alpha, 1)$. Note that changing one vector in $V$ corresponds to changing a single value in the last column (and the corresponding value in the last row) in $I(V,\textbf{w})$. We show that the opposite direction also holds, i.e., by changing a single value in the last column (and the corresponding value in the last row) of $I(V,\textbf{w})$, we obtain a new matrix that is $I(V',\textbf{w})$ with $V'$ being different from $V$ only in a single vector, as long as the new matrix is positive semidefinite.

\begin{claim}
\label{cla:psd-change}
Suppose that $A$ is a $m \times m$ real symmetric positive semidefinite matrix with $A=U^TU$ with $U=[\textbf{u}_1 \textbf{u}_2 \ldots \textbf{u}_m]$ where $\textbf{u}_i \in \mathbb{R}^n$ with $n\geq m$, and $A'$ is another real symmetric positive semidefinite matrix such that $A'$ and $A$ differ only in $A_{1,2}= A_{2,1}$. Then, there exists $U' =[\textbf{u}'_1 \textbf{u}_2 \ldots \textbf{u}_m]$ such that $A'=(U')^TU'$.
\end{claim}
\begin{proof}
As $A'$ is a positive semidefinite matrix, there exist $\textbf{v}_1, \textbf{v}_2, \ldots, \textbf{v}_m \in \mathbb{R}^n$ such that $A'=V^TV$ where $V=[\textbf{v}_1 \textbf{v}_2 \ldots \textbf{v}_m]$. 
Let $A[2:m]$ be the $(m-1)\times (m-1)$ submatrix of $A$ excluding the first row and the column, i.e., $A[2:m]_{i,j} = A_{i+1, j+1}$. We define the corresponding submatrix of $A'$ as $A'[2:m]$. Note that $A[2:m]=A'[2:m]$. Let $U[2:m] = [\textbf{u}_2 \textbf{u}_3 \ldots \textbf{u}_m]$, and similarly, let $V[2:m] = [\textbf{v}_2 \textbf{v}_3 \ldots \textbf{v}_m]$. Note that $A[2:m]=U[2:m]^TU[2:m]$, and $A'[2:m]=V[2:m]^TV[2:m]$. However, as $A[2:m]=A'[2:m]$, we can infer that there exists a unitary $n \times n$ matrix $H$ such that $U[2:m]=HV[2:m]$. Now, setting $U'=HV$, we get the required matrix $U'$ with $A'=(U')^TU'$. 
\end{proof}
Thus, our goal is to obtain a series of $(k+1) \times (k+1)$ real symmetric positive semidefinite matrices $M_1, M_2, \ldots, M_L$ such that 
\begin{enumerate}
    \item $M_1 = I(U, \textbf{w})$. 
    \item The diagonal entries of $M_i$ for every $i \in [L]$ are all equal to $1$. 
    \item All the off-diagonal entries of $M_L$ are equal to $\alpha$.
    \item For every $i \in [L-1]$, $M_i$ and $M_{i+1}$ differ only in one element in the last column (and the corresponding element in the last row). 
\end{enumerate}
Towards this end, for $\epsilon \geq 0$, $0 \leq \gamma \leq \alpha$, and $d \in [k]$, we define the $(k+1)\times (k+1)$ matrix $M(\gamma, \epsilon, d)$ as follows: 
\[
M(\gamma, \epsilon, d)_{i,j} = \begin{cases} 
1, \text { if }i=j.\\
\alpha, \text { if }1\leq i,j\leq k.\\
\gamma + \epsilon, \text{ if }i=k+1, 1\leq j \leq d \text{ or } j=k+1, 1 \leq i\leq d.\\
\gamma, \text{ if }i=k+1, d+1\leq j \leq k \text{ or } j=k+1, d+1 \leq i\leq k.
\end{cases}
\]

Note that $I_{U,\textbf{w}}=M(0,0,k)$, and our goal $M_L$ is equal to $M(\alpha, 0,k)$. We consider the below sequence of positive semidefinite matrices $M(0,0,k), M(0,\epsilon,1), M(0,\epsilon,2), \ldots, M(0,\epsilon,k), M(\epsilon, \epsilon,1), M(\epsilon, \epsilon, 2), \ldots, M(\epsilon, \epsilon,k)$, $M(2\epsilon,\epsilon,1),\ldots, \ldots, M(\alpha-\epsilon, \epsilon, k)$. At each step, we change a single element in the last column (and the corresponding element in the last row). 

The final step is to show that when we set $\epsilon\leq \frac{1-\alpha}{k}$, $M(\gamma, \epsilon,d)$ is positive semidefinite for every $d\in [k], 0\leq \gamma\leq \alpha$. This follows from a simple calculation. 
\begin{align*}
    x^TM(\gamma, \epsilon, d)x &= \gamma ( \sum_{i=1}^{k+1}x_i)^2 + (\alpha - \gamma) ( \sum_{i=1}^{k}x_i)^2 + (1-\alpha) \sum_{i=1}^kx_i^2 + (1-\gamma)x_{k+1}^2 + \epsilon (x_{k+1})(\sum_{i=1}^d x_i) \\ 
    &\geq (1-\alpha) \sum_{i=1}^{k+1} x_i^2 + \epsilon  (x_{k+1})(\sum_{i=1}^d x_i) \\
    &\geq \left( \frac{1-\alpha}{k} \right) \left(k \sum_{i=1}^{k+1} x_i^2 - \left|(x_{k+1})(\sum_{i=1}^d x_i)\right| \right) \geq 0 \ . \qedhere
\end{align*}
\end{proof}

Now, we prove the connectivity lemma. 
\begin{lemma}
\label{lem:connectivity}
Fix an integer $k\geq 2$ and $0 < \alpha < 1$. Suppose that $U$ and $V$ are two $\alpha$-configurations in $\mathbb{S}^n$. Then, there exists $n_0 := n_0(k,\alpha)$, and $L := L(k, \alpha)$ such that as long as $n \geq n_0$, there exist $\alpha$-configurations $V_1, V_2, \ldots, V_L$ such that 
\begin{enumerate}
    \item The endpoints are $U$ and $V$ i.e., $U=V_1, V=V_L$. 
    \item Any two consecutive configurations differ in exactly one vector i.e., $| V_i \cap V_{i+1}|=k-1$ for every $i \in [L-1]$. 
\end{enumerate}

\end{lemma}

\begin{proof}
We use induction on $k$. First, we consider the case when $k=2$. Let $U=\{\textbf{u}_1, \textbf{u}_2\}$, and $V=\{\textbf{v}_1, \textbf{v}_2\}$ be two $\alpha$-configurations. Consider an arbitrary vector $\textbf{w}$ that is orthogonal to all the vectors in $U$ and $V$. Such a $\textbf{w}$ is guaranteed to exist when $n$ is large enough. 
Now, using~\Cref{lem:vectors-connected}, we can infer that there exists a configuration $W=\{\textbf{w}, \textbf{w}'\}$ such that there is a path of length $O_{\alpha}(1)$ from $U$ to $W$, and from $V$ to $W$. Thus, there exists a path of length $O_{\alpha}(1)$ from $U$ to $V$. 

Assume that the proof holds for $k-1$, and we are given the configurations $U$ and $V$ consisting of $k$ vectors each. We choose a vector $\textbf{w}$ that is orthogonal to each of the vectors in $U$ and $V$. Using~\Cref{lem:vectors-connected}, there are configurations $X=\{\mathbf{w}, \mathbf{x}_1, \mathbf{x}_2, \ldots, \mathbf{x}_{k-1}\}$ and $Y=\{\mathbf{w}, \mathbf{y}_1, \mathbf{y}_2, \ldots, \mathbf{y}_{k-1}\}$ such that there is a path of length $O_{\alpha, k}(1)$ from $U$ to $X$ and from $V$ to $Y$. Now, our goal is to show that there is a path from $X$ to $Y$ of length $O_{\alpha, k}(1)$. We achieve this by restricting ourselves to components orthogonal to $\mathbf{w}$ of the $(k-1)$-sized configurations $X'=\{\mathbf{x'}_1, \mathbf{x'}_2, \ldots, \mathbf{x'}_{k-1}\}$ and $Y'=\{\mathbf{y'}_1, \mathbf{y'}_2, \ldots, \mathbf{y'}_{k-1}\}$, where $\mathbf{x'}_i = \mathbf{x}_i - \langle \mathbf{x}_i, \mathbf{w}\rangle \mathbf{w}$ for each $i \in [k-1]$ (and similarly for $\mathbf{y'}_i$). Note that $X'$ and $Y'$ are $(\alpha - \alpha^2)$ configurations, and by the induction hypothesis, there exists a path from $X'$ to $Y'$ using only vectors orthogonal to $\mathbf{w}$. Adding the component along $\mathbf{w}$, we get a path from $X$ to $Y$ of length $O_{\alpha,k}(1)$, finishing the proof. 
\end{proof}

We are now ready to prove~\Cref{thm:sphere-coloring-gamma}. 
\begin{proof}
Suppose for contradiction that there exists a coloring $f:\mathbb{S}^n \rightarrow \{0,1\}$ that is folded and respects the PCSP $\Gamma_5$. Consider an arbitrary set of vectors $\{ \mathbf{v}_1, \mathbf{v}_2, \ldots, \mathbf{v}_{2k-1} \}$ that are all orthogonal to $\textbf{v}_0$, and have pairwise inner product $\alpha$. Such a set is guaranteed to exist as $\alpha \geq 0$. 
There exists a set of $k$ vectors among these that are all assigned the same color in $f$. Let these form the configuration $U$, and the set of negations of these vectors be the configuration $V$. Using~\Cref{lem:connectivity}, there exists a path from $U$ to $V$ where we change a single vector in each step. Note that the endpoints of the path have $0$ and $k$ vectors assigned $+1$ respectively. Since we change at most one vector at a time, there exists a configuration where we have exactly $b$ $1$s, a contradiction. 
\end{proof}

\Cref{lem:gamma1,lem:gamma2,lem:gamma3,lem:gamma4,lem:gamma5} for templates $\Gamma_1$-$\Gamma_5$, together with~\Cref{lem:reduction},~\Cref{thm:raghavendra} and~\Cref{lem:sphere-coloring} finish the proof of our main hardness result~\Cref{thm:main-hardness}. 

\medskip \noindent \textbf{Explicit Construction.} We give an explicit construction of an integrality gap instance for $\Gamma_5=(P, Q), P=\Ham_k \{1,k\}, Q=\Ham_k \{0,1,\ldots,k\}\setminus\{b\}$ for arbitrary $b \in \{0,1,\ldots,k\}$. 
Let $L$ be a large constant (depends on $k$, to be set later). We have $n=2k-1+\binom{2k-1}{k}L$ variables $x_i: i \in [2k-1], x_S^{(i)}: i \in [L], S \subseteq [2k-1], |S|=k$. Our constraints are the following: for every subset $S \subseteq [2k]$ with $|S|=k$ and $S=\{i_1, i_2, \ldots, i_k\}$,  we pick $L$ new variables $x_S^{(1)}, x_S^{(2)}, \ldots, x_S^{(L)} $.
The constraints are 
\begin{align*}
   \{x_{i_1}, x_{i_2}, \ldots, x_{i_k}\}, \{x_{i_2}, x_{i_3}, \ldots, x_{i_{k}}, x_S^{(1)}\},  
   \{x_{i_3}, \ldots, x_S^{(1)}, x_S^{(2)}\}, \ldots, \{x_S^{(L-k+1)}, \ldots, x_S^{(L)}\}, \\ 
   \{x_S^{(L-k+2)}, \ldots, x_S^{(L)}, \overline{x_{i_1}}\}, \{x_S^{(L-k+3)}, \ldots, \overline{x_{i_1}}, \overline{x_{i_2}}\}, \ldots, \{\overline{x_{i_1}}, \overline{x_{i_2}}, \ldots, \overline{x_{i_k}}\}.
\end{align*}
We choose $L$ to be the constant factor from~\Cref{lem:connectivity} with $\alpha =\frac{k-3}{k-1}$. 
 The idea is that when all the variables in the constraint $\{x_{i_1}, x_{i_2}, \ldots, x_{i_k}\}$ are all set to be True, all the variables in $\{\overline{x_{i_1}}, \overline{x_{i_2}}, \ldots, \overline{x_{i_k}}\}$, and as there are a series of constraints between them where we alter a single variable, there must exist a constraint where there are exactly $b$ variables that are set to True. 

Formally, we show that this instance does not satisfy $Q=\Ham_k \{0,1,\ldots, k\} \setminus \{b\}$. 
Suppose for contradiction that there is an assignment that satisfies all the constraints. Since there are $2k-1$ variables $x_1, x_2, \ldots, x_{2k-1}$, at least $k$ of them are set to be true. If not, then at least $k$ of the negated variables are set to be true. This implies that there is a sequence of constraints where the endpoints are assigned all True and all False, and at every point, we change a single variable. This implies that there is a constraint where there are exactly $b$ variables that are set to True, a contradiction. 

We now show that the instance has a basic SDP solution with zero error. Set $\textbf{v}_1, \textbf{v}_2, \ldots, \textbf{v}_{2k-1} \in \mathbb{S}^n$ to the variables $x_1, x_2, \ldots, x_{2k-1}$ such that $\textbf{v}_i \cdot \textbf{v}_0 = 0$, and $\textbf{v}_i \cdot \textbf{v}_j = \alpha$ for every $i \neq j$. Such a set of vectors is guaranteed to exist as $n$ is large enough, and $\alpha \geq 0$. 
Finally, we use~\Cref{lem:connectivity} to set the vectors $\textbf{v}_S^{(i)}$ for every $S \subseteq [2k-1], |S|=k, i \in [L]$. 


\section{The SDP minion}
\label{sec:minion}

As previously mentioned, polymorphisms are a powerful tool for understanding the computational complexity of PCSPs. However, beyond some of the simplest classes of PCSPs, individually classifying the complexity based on specific polymorphisms can be unwieldy. Instead, one often looks to higher-level structure between classes of polymorphisms, which is captured by the notion of \emph{minions} (also called clonoids) \cite{BBKO21}.

In this section, we describe the structure of the ``basic SDP minion,'' which gives a precise algebraic condition for when the basic SDP decides a Promise CSP. We note that a similar minion was concurrently and independently discovered by Ciardo-Zivny \cite{cz22-minion}. 

\newcommand{\bY}{\mathbf{Y}}

To maintain consistency with the notation of \cite{BBKO21}, in this section, we define the notion of a Promise CSP as a homomorphism problem between structures. A \emph{signature} $\tau$ consists of a list of (abstract) relations $R$ each of which has an arity $\ar_R \in \N$. A structure $\bX$ with signature $\tau$ consists of a domain $X$ and relations $R^{\bX}$ for each $R \in \tau$, where each $R^{\bX}$ is a subset of $X^{\ar_R}$. A homomorphism between two structures $\bX$ and $\bY$ of the same signature $\tau$, denoted by $\bX \mapsto \bY$, is a map $\sigma : X \to Y$ such that each $R \in \tau$ and $\bx \in R^{\bX}$, we have that $\sigma(\by) \in R^{\bY}$ (where $\sigma$ is applied element-wise).

PCSP consists of a pair of structures $\bA$ and $\bB$ of the same signature $\tau$, with a homomorphism from $\bA \to \bB$.  An \emph{instance} of $\PCSP(\bA, \bB)$ is then any structure $\bX$ with signature $\tau$. The decision version of $\PCSP(\bA, \bB)$ is to distinguish between the scenarios in which $\bX \mapsto \bA$ and $\bX \not\mapsto \bB$. The search version is to find an explicit homomorphism $\bX \to \bB$ when promised that $\bX \to \bA$. To compare with the definition in Section~\ref{sec:prelims}, note that the pairs $(R^{\bA}, R^{\bB})$ for $R \in \tau$ correspond to the signature $\Gamma$. Likewise, the domain $X$ represents the variables of the PCSP instance, and the relations $R^{\bX}$ denote the constraints of each type. As has been observed previously (e.g., \cite{BrakensiekG21}), constraints like allowing negated literals or constants can be emulated by adding suitable relations to the structures $\bA$ and $\bB$. 

The polymorphisms of $\PCSP(\bA, \bB)$, denoted by $\Pol(\bA, \bB)$  is the set of all homomorphisms $\bA^L \mapsto \bB$, for all $L \ge 1$. Here,  $\bA^L$ denotes the $L$-wise product structure (i.e., $R^{\bA^L}$ is the $L$-wise Cartesian product of the set $R^{\bA}$).

\subsection{Minion preliminaries}

We now give the definition of a minion.

\begin{definition}
A minion $\mathcal M$ is a family of sets $\mathcal M_1, \mathcal M_2, \hdots$, where $\mathcal M_i$ are the objects are arity $i$. For every pair of positive integers $i$ and $j$ and map $\pi : [i] \to [j]$, there exists a map $f^{/\pi} : \mathcal M_i \to \mathcal M_j$ known as a \emph{minor map}. In the case that $i = j$ and $\pi:[i]\rightarrow[i]$ is the identity map, we have that $f^{/\pi}(M) = M\,\,\forall M\in \mathcal{M}_i$. Further, these minor maps commute: $(f^{/\pi})^{/\pi'} = f^{/\pi' \circ \pi}$. 
\end{definition}

The most commonly discussed minion is the family of polymorphisms $\Pol(\bA, \bB)$ of $\PCSP(\bA, \bB)$. In this case, the minors correspond to the identification of coordinates. Given a function $f$ of arity $i$ and a map $\pi : [i] \to [j]$, we have that
\[
    f^{/\pi}(x_1, \hdots, x_j) = f(x_{\pi(1)}, x_{\pi(2)}, \hdots, x_{\pi(i)}). 
\]
It is straightforward to verify that the minor maps commute in the required manner.

However, the `objects' of a minion need not correspond to mathematical functions. The following are a few known examples:

\begin{itemize}
\item The \emph{trivial minion} $\mathcal M_{triv}$ has that every arity has a single object: $\mathcal M_i = \{e\}$. All minor maps are thus the "trivial" map.

\item The \emph{dictator minion} (or projection minion) $\mathcal M_{dict}$ has each $\mathcal M_i = [i]$. The minor maps are then application of $\pi$: $k^{/\pi} = \pi(k)$.

\item The \emph{basic LP minion}\footnote{The basic LP relaxation computes a probability distribution of assignments for each variable and constraint, subject to the constraint that the probability distribution of a variable is consistent with the constraint distributions the variable appears in. Equivalently, it is the basic SDP relaxation without the second moment constraints. The basic LP minion is referred to as $Q_{conv}$ in the literature (e.g., \cite{BBKO21}).} $\mathcal M_{BLP}$ has each $\mathcal M_i = \{(p_1, \hdots, p_i) : p_1, p_2, \hdots, p_i \ge 0, p_1 + \cdots + p_i = 1\}$ be the probability distributions on $i$ elements. The minor maps combine elements of the probability distribution which map to the same value. That is, \[(p_1, \hdots, p_i)^{/\pi} = \left(\sum_{\pi(k) = 1} p_k, \hdots, \sum_{\pi(k) = i} p_k\right).\]
\end{itemize}



In order to better understand the complexity of PCSPs, we relate the polymorphic minions to other minions like the ones mentioned above. We can determine the relationship between minions via \emph{minion homomorphisms}.

\begin{definition}
A \emph{minion homomorphism} $\psi: \mathcal M \to \mathcal N$ between two minions consists of maps $\psi_i: \mathcal M_i \to \mathcal N_i$ such that these maps commute with the respective minor maps of $\mathcal M$ and $\mathcal N$. That is, for all $f \in \mathcal M_i$ and $\pi : [i] \to [j]$, we have that 
\[
   \psi_j(f^{/\pi}) = \psi_i(f)^{/\pi}
\]
\end{definition}

%

%
%
%

Our goal in this section is to prove a characterization for when a PCSP is decided by the basic SDP in terms of minion homomorphisms.

\subsubsection*{The free structure}

One of the most important tools for understanding minion homomorphisms, specifically minion homomorphisms of the form $\mathcal M \to \Pol(\bA, \bB)$, is known as the \emph{free structure} \cite{BBKO21}. We denote the free structure by $\F_{\cM}(\bA)$. The domain of the free structure is $\cM_{|A|}$, the set of all minion elements of arity the size of the domain of $\bA$. For each $R \in \tau$, the relation $R^{\F_{\cM}(\bA)}$ is the set of all $\ar_R$-tuples $(M_1, \hdots, M_{R})$ of elements of $\cM_{|A|}$ such that there exists $M \in \cM_{|R^A|}$ such that $M^{/\pi_i} = M_i$, where $\pi_i : R^A \to A$ is the map which projects each tuple of $R^A$ to its $i$th coordinates.  

The fundamental property of the free structure is the following.

\begin{theorem}[\cite{BBKO21}]\label{thm:free-structure}
$\mathcal M \to \Pol(\bA, \bB)$ if and only if $\F_{\cM}(\bA) \mapsto \bB$.
\end{theorem}

We remark that in the Boolean case, i.e., when $|A| = 2$, for the SDP minion $\cM_{\SDP}$ (formally defined in the next subsection), we have that maps $\F_{\cM_{\SDP}}(\bA) \mapsto \bB$ correspond to the sphere colorings studied in Section~\ref{sec:ug-hardness}.


\subsection{SDP Minion Definition}

The objects in the SDP minion we construct correspond to vectors output by the basic SDP relaxation. However, since the SDP minion is a universal object, we need to be able to represent vectors of arbitrary large dimensions. We achieve this using infinite-dimensional vectors that are eventually zero. Similar techniques have been used in other minion constructions \cite{ciardo2022clap}.

Let $\R^{\omega}$ be infinite sequences of real numbers which are eventually $0$ (and thus can be thought of as the union $\R^1 \cup \R^2 \cup \R^3 \cup ...$). Note that $\R^{\omega}$ is an inner product space.

We now define the minion $\cM_{\SDP}$ whose $k$-arity symbol is a list of $k$ vectors $(\bw_1, \hdots, \bw_k)$ in $\R^\omega$. When convenient, we shall think of the whole object as a matrix $W \in \R^{k \times \omega}$.

We impose the following conditions on the vectors:
\begin{enumerate}
\item For all $i,j \in [k]$ with $i \neq j$, $\bw_i \cdot \bw_j = 0$.
\item $\sum_{i \in [k]} \|\bw_i\|^2_2 = 1$.
\end{enumerate}
Observe that the second condition is equivalent to $\|\sum_{i\in[k]} \bw_i\|_2 = 1$.

The minors of $\cM_{\SDP}$ are not too surprising, given $W \in \cM_{\SDP}^{(k)}$ and a map $\pi : [k] \to [\ell]$, $W^{/\pi}$ is the matrix in $W' \in \R^{\ell \times \omega}$ where $\bw'_i := \sum_{j \in \pi^{-1}(i)} \bw_j$. 

\begin{claim}
$\cM_{\SDP}$ is a minion.
\end{claim}
\begin{proof}
First, for each $W \in \cM_{\SDP}^{(k)}$ and a map $\pi : [k] \to [\ell]$ we verify that $W' := W^{/\pi} \in \cM_{\SDP}^{(\ell)}$. First, fix $i \neq i' \in [\ell]$. We have that
\begin{equation*}
    \bw'_i \cdot \bw'_{i'} = \left(\sum_{j \in \pi^{-1}(i)} \bw_j\right)\cdot \left(\sum_{j' \in \pi^{-1}(i')} \bw_{j'}\right)
    = \sum_{\substack{j \in \pi^{-1}(i)\\j' \in \pi^{-1}(i')}} \bw_j \cdot \bw_{j'}
    = 0
\end{equation*}
Further,
\begin{align*}
\left(\sum_{i'\in[\ell]} \bw'_{i'}\right)\cdot \left(\sum_{i'\in[\ell]} \bw'_{i'}\right) &= \left(\sum_{i\in[k]} \bw_i\right)\cdot \left(\sum_{i\in[k]} \bw_i\right) = 1.
\end{align*}
Thus, $W^{/\pi} \in \cM_{\SDP}^{(\ell)}.$ It is trivial to observe that identity minors are identity maps.

The only remaining condition to check is that the minors commute. Consider $\pi : [a] \to [b]$ and $\eta : [b] \to [c]$. Let $U \in \cM_{\SDP}^{(a)}$, $V \in \cM_{\SDP}^{(b)}$, and $W \in \cM_{\SDP}^{(c)}$ such that $V = U^{/\pi}$ and $W = V^{/\eta}$.  We seek to verify that $W = U^{/(\eta \circ \pi)}$. For all $i \in [c]$, we have that
\begin{equation*}
    \bf{w}_{i} = \sum_{i' \in \eta^{-1}(i)} \bf{v}_{i'}
                            = \sum_{i' \in \eta^{-1}(i)}\sum_{i'' \in \pi^{-1}(i')} \bf{u}_{i''}
                            = \sum_{i'' \in (\eta \circ \pi)^{-1}(i)} \bf{u}_{i''} \ . \qedhere
\end{equation*}
\end{proof}

The goal of the rest of this section is to prove the following theorem.

\newcommand{\poly}{\operatorname{poly}}

\begin{theorem}\label{thm:basic-SDP}
The basic SDP decides $\PCSP(\bA, \bB)$ if and only if $\cM_{\SDP} \to \Pol(\bA, \bB)$
\end{theorem}

\subsection{On the basic SDP}

We shall modify some notation from \cite{ciardo2022clap}. Define $s^R(\bx)$ to be the set of valid assignments to the constraint $\bx \in R^X$. For a variable $x$ appearing in $\bx$ and for each $\ba \in s^R(\bx)$, we let $\ba(x)$ denote the value assigned to $x$ by $\ba$. 

We define a solution to the basic SDP relaxation for an instance $\bX$ of $\PCSP(\bA, \bB)$ to be a collection of vectors $\bu_{x,a} \in \R^{\omega}$ for all $x \in X, a \in A$ as well as weights $\lambda_{\bx}(\ba)$, for each $\bx \in R^X$ and assignment $\ba \in s^R(\bx)$. We let $\bu_0$ be an arbitrary unit vector. Modifying the definition of the basic SDP relaxation in Section~\ref{sec:prelims} for our current notation as well as observing that we can set each $\eps_i$, we can cast the ``traditional'' basic SDP as the following feasibility problem.\footnote{Although we specify here that the vectors can have an arbitrarily large dimension, it is known that an SDP with $n$ vector or scalar variables is feasible if and only if it is feasible in $n$ dimensions. Thus, nothing is lost from Section~\ref{sec:prelims} by making the vectors have an arbitrarily large (but finite) dimension.}

\begin{itemize}
\item[(a1)] For all $R \in \tau$, $\bx \in R^{\bX}$ and $\ba \in s^R(\bx)$, $\lambda_\bx(\ba) \geq 0$.
\item[(a2)] For all $R \in \tau$, $\bx \in R^{\bX}$, $\sum_{\ba \in s^R(\bx)}\lambda_\bx(\ba) =1$.
\item[(a3)] For all $R \in \tau$, $\bx \in R^{\bX}, x \in \bx, a \in A$, \[\textbf{u}_{x,a} \cdot \textbf{u}_0 = \sum_{\substack{\ba \in s^R(\bx)\\\ba(x) = a}}\lambda_j(\ba).\]
\item[(a4)] For all $R \in \tau$, $\bx \in R^{\bX}$, $x, x' \in \bx$, $a, a' \in A$. \[\textbf{u}_{x,a} \cdot \textbf{u}_{x',a'} = \sum_{\substack{\ba \in s^R(\bx)\\ \ba(x) = a, \ba(x') = a'}}\lambda_j(\textbf{a}).\]
\end{itemize}

\subsubsection{An alternative basic SDP}

We now present a modified basic SDP which is more convenient to work with. 

An \emph{alternative} basic SDP solution to $\bX$ for $\PCSP(\bA, \bB)$ is a collection of vectors $\bu_{x,a} \in \R^{\omega}$ for all $x \in X, a \in A$ as well as $\bv_{\bx, \ba} \in \R^{\omega}$ for $\bx \in R^{\bX}$ and $\ba \in s^R(\bx)$ (which is defined to be the set of the valid assignments to the constraint $\bx$) with the following properties:\footnote{Usually, there is a special vector $\bv_0$, but we can omit it without changing the power of the algorithm. This will be more convenient for the analysis.}
\begin{enumerate}
\item[(b1)] For all $x \in X$, $U_{\bx} := (\bu_{x, a} : a \in A) \in \cM^{(A)}_{\SDP}.$
\item[(b2)] For all $\bx \in R^{\bX}$, $V_{\bx} := (\bv_{\bx, \ba} : \ba \in s^R(\bx)) \in \cM^{(A)}_{\SDP}$.
\item[(b3)] For all $\bx \in \R^{\bX}$ with arity $k$ and $i \in [k]$ and $a \in A$, we have that
\[
    \bu_{\bx_i,a} = \sum_{\substack{\ba \in s^R(\bx)\\\ba_i = a}} \bv_{\bx, \ba}.
\]
\end{enumerate}


\subsubsection{Equivalence with traditional basic SDP}

We now prove that these two notions formulations of the basic SDP are equivalent. That is, a solution to either SDP can be transferred to the other. 

\textbf{Alternative to traditional.} First, we show that a solution to the alternative basic SDP is a solution to the traditional basic SDP. For each $x \in X$, define $\bu_x = \sum_{a \in A} \bu_{x, a}$. By (b1), we know that each $\bu_x$ is a unit vector. Further by (b3), we can deduce that $\bu_x = \bu_{x'}$ whenever $x$ and $x'$ appear in a common constraint $\bx$. Thus, $\bu_x = \bu_{x'}$ whenever $x$ and $x'$ are in the same connected component of $\bX$. Note that each of the constraints (b1), (b2), and (b3) are preserved when we apply a rigid rotation to the variable vectors and constraint vectors of a single connected component. Thus, by applying suitable rigid rotations, we can achieve a solution to (b1-3) such that $\bu_x$ is the same unit vector for all $x \in X$, which we shall call $\bu_0$.

In our solution to the traditional basic SDP, we shall also copy over the vectors $\bu_{x,a}$ from the alternative basic SDP to the traditional basic SDP, and set $\bu_0$ to be our newly-defined $\bu_0$.  We also set $\lambda_{\bx}(\ba) = \|\bv_{\bx, \ba}\|^2$.

To see that the traditional basic SDP is satisfied, note that condition (a1) follows by inspection, and condition (a2) follows from (b2). Condition (a3) follows by substituting $\bu_x$ to $\bu_0$ and applying (b3). Condition (a4) follows from combining (b2) and (b3).

%
%
%
%
%

\textbf{Traditional to alternative.} We now show that any solution to the traditional basic SDP is a solution to the alternative basic SDP. Assume we have a traditional basic SDP solution. Recall that each $\bu_{x, a} \in \R^\omega$. We shall use the same vectors $\bu_{x,a}$ in both solutions. Note that (b1) then follows from (a4) and (a2). 

Since there are finitely many vectors, there exists some $N \in \N$ such that each $\bu$ has its support within the first $N$ coordinates. Provisionally assign $\hat{\bv}_{\bx, \ba}$ to be $\sqrt{\lambda_{\bx}(\ba)} \cdot e_i$, where $i > N$ are chosen uniquely for each constraint-assignment pair. As there are finitely many constraint-assignment pairs, all of these are in $\R^{\omega}$.

However, as written the $\hat{\bv}$'s are not compatible with the $\bu$'s. However, we can define over all suitable choices of $i \in [k]$ and $a \in A$,
\[\hat{\bu}_{\bx_i,a} := \sum_{\substack{\ba \in s^R(\bx)\\\ba_i = a}} \hat{\bv}_{\bx, \ba}\]
It is not hard to see that $\bu_{x_i, a_i} \cdot \bu_{x_j, a_j} = \hat{\bu}_{x_i, a_i} \cdot \hat{\bu}_{x_j, a_j}$ for all suitable choices of $x_i, a_i, x_j, a_j$.

For a fixed $\bx$, let $\hat{V}_{\bx} \subseteq \Q^{\omega}$ be the subspace spanned by $\hat{\bv}_{\bx,\ba}$ and $\bu_{x,a}$ for the $x \in \bx$. Note that each $\hat{\bu}$ is also in $\hat{V}_{\bx}$. Since the $\hat{\bu}$'s and the $\bu$'s have the same dot products, there is a rigid rotation $\psi : \hat{V}_{\bx} \to \hat{V}_{\bx}$ which sends each $\hat{\bu}_{(x,a)}$ to $\bu_{(x,a)}$. 

Define $\bv_{\bx,\ba} = \psi(\hat{\bv}_{\bx,\ba})$. Then, observe that
\[
\bu_{\bx_i,a}= \psi(\hat{\bu}_{\bx_i,a}) = \sum_{\substack{\ba \in s^R(\bx)\\\ba_i = a}} \psi(\hat{\bv}_{\bx, \ba}) = \sum_{\substack{\ba \in s^R(\bx)\\\ba_i = a}} \bv_{\bx, \ba},
\]
as desired. Thus, conditions (b2) and (b3) hold. Thus, the two SDP formulations are equivalent.




\subsection{From minion homomorphism to SDP rounding algorithm}

Recall that we are trying to prove that the basic SDP decides $\PCSP(\bA, \bB) $ if and only if $\cM_{\SDP} \to \Pol(\bA, \bB)$.  We begin by showing the ``easy'' direction that the minion homomorphism implies that the basic SDP decides $\PCSP(\bA, \bB)$.

\begin{theorem}\label{thm:hom_to_alg}
If $\cM_{\SDP} \to \Pol(\bA, \bB)$, then the basic SDP decides $\PCSP(\bA, \bB)$.
\end{theorem}

\begin{proof}
Fix $\bX$ and a basic SDP solution $M^{\bX}$ with corresponding vectors $\bu_{x,a}$ and $\bv_{\bx,\ba}$ (and $U_x, V_{\bx}$) with the prescribed properties. Let $\psi : \cM_{\SDP} \to \Pol(\bA, \bB)$ be the minion homomorphism.

Define the assignment $f : X \to B$ to be $\psi(U_x)(a : a \in A)$. Unpacking this, $\psi(U_x) \in \Pol(A, B)$ is of arity $A$, so we just plug in the coordinates of $A$ listed in canonical order. For any constraint $\bx$, we know that $\bb := \psi(V_{\bx})(\ba : \ba \in s^R(\bx))$ satisfies $R^{\bB}$. Thus, it suffices to show that $\bb_i = f(\bx_i)$ for all $i \in k$. 

Let $\pi_i : R^{\bA} \to A$ which maps $\ba$ to $\ba_i$. It is straightforward to show that condition (b3) of the alternative basic SDP implies that $U_x = (V_{\bx})_{/\pi_i}$. Thus, since $\psi$ is a minion homomorphism,
\[
f (\bx_i) = \psi(U_x)(a : a \in A) = \psi(V_{\bx})_{/\pi_i}(a : a \in A) = \psi(V_{\bx})_i(\ba : \ba \in s^R(\bx)) = \bb_i,
\]
as desired. Thus, the basic SDP decides $\PCSP(\bA, \bB)$.
\end{proof}

\subsection{From SDP rounding algorithm to minion homomorphism}

We now prove the converse.

\begin{theorem}
If the basic SDP decides $\PCSP(\bA, \bB)$, then $\cM_{\SDP} \to \Pol(\bA, \bB)$.
\end{theorem}

%

\begin{proof}
We adopt the proof technique of \cite{ciardo2022clap}. Let $\cF \subset \cM_{\SDP}$ be any finite subset. Let $\bF$ be an instance of $\PCSP(\bA, \bB)$ whose variables $F = \cF \cap \cM^{(A)}_{\SDP}$ -- the arity-$A$ elements of $\cF$. The constraints $R^{\bF}$ are on $k$-tuples $(W_1, \hdots, W_k)$ of $F$ for which there is $Z \in \cF \cap \cM_{\SDP}^{(R^{\bA})}$ with the following property. For all $i \in [k]$, let $\pi_i : R^{\bA} \to A$ be the $i$th coordinate projection map. Then, $W_i = Z_{/\pi}$. 

If we can show that the basic SDP decides $F$, then we know that $\bF \to \bB$ by definition of the basic SDP solving $\PCSP(\bA, \bB)$. Then, via a suitable compactness argument\footnote{Like the De Bruijn-Erdos theorem \cite{bruijn1951colour}, for more details see \cite{ciardo2022clap} or Remark 7.13 of \cite{BBKO21}.} this implies that the free structure $\F_{\cM_{\SDP}}(\bA) \to \bB$ which implies that $\cM_{\SDP} \to \Pol(\bA, \bB)$ (Theorem~\ref{thm:free-structure}). Thus, it suffices to construct a basic SDP solution to $\bF$.

We now outline the remainder of the proof. For every $W \in F$ and $a \in A$, let $\bu_{W,a} = \bw_a$ (the $a$th column of W). Likewise, for every constraint $\tau$ on $(W_1, \hdots, W_k)$ via $Z \in \cF \cap \cM_{\SDP}^{(R^{\bA})}$ and $\ba \in s^R(Z)$ (that is a valid solution to the constraint indexed by $Z$), we let $\bv_{\tau, \ba} = Z_{\ba}$. 

We then need to check conditions (b1-3). Conditions (b1)  and (b2) are verbatim from $W, Z \in \cM_{\SDP}$. Condition (b3) is precisely that $W_i = Z_{/\pi_i}$. Thus, the basic SDP decides $\bF$, so $\cM_{\SDP} \to \Pol(\bA, \bB)$.
\end{proof}

This completes the proof of Theorem~\ref{thm:basic-SDP}.

\section{Conclusion}
\label{sec:conclusion}

We studied the robust satisfiability of promise CSPs, and specifically the power of SDPs in this context, revealing a number of new phenomena on both the algorithmic and integrality gap fronts. Our work brings to the fore a number of intriguing questions and directions. We list a few below.

\begin{itemize}
\item Can we get a robust dichotomy result for all Boolean symmetric folded PCSPs? We conjecture that every Boolean symmetric folded idempotent PCSP without $\MAJ$ or $\AT$ polymorphisms does not admit a robust algorithm. In our hardness result~\Cref{thm:main-hardness}, we showed the same when there is a single predicate pair. Extending our result to multiple predicate pairs is an interesting challenge. 

As a concrete example, consider the PCSP $\Gamma=\{(P_1,Q_1),(P_2,Q_2)\}$ where $P_1 = \Ham_k \{\frac{k+1}{2}\}, Q_1=\Ham_k \{0,1,\ldots,k-1\},P_2=\Ham_t \{1\},Q_2=\Ham_t\{0,1,\ldots,t-2,t\}$ for odd integers $k,t\geq 3$. We have $\AT \subseteq \Pol(P_1,Q_1), \MAJ \subseteq \Pol(P_2,Q_2)$ while both $\AT,\MAJ \nsubseteq \Pol(\Gamma)$. To obtain the integrality gap for $\Gamma$ for the basic SDP relaxation, we need to show that there is no folded sphere coloring that respects $\Gamma$. There is a sphere coloring $f_1 : \S^n \rightarrow \{-1,+1\}$ respecting $(P_1,Q_1)$, and a sphere coloring $f_2 : \S^n \rightarrow \{-1,+1\}$ respecting $(P_2,Q_2)$ for every positive integer $n$. The challenge lies in showing that there is an integer $n$ for which there is no single coloring $f:\S^n \rightarrow \{-1,+1\}$ that respects both $(P_1,Q_1)$ and $(P_2,Q_2)$ simultaneously. We are able to prove this for small values of $k,t$ under an assumption that ~\Cref{thm:sphere-ramsey} extends to the density setting: for any constant $0<\sigma<1$, and any tuple $S$ of vectors in $\S^n$ satisfying the conditions in~\Cref{thm:sphere-ramsey}, in any subset $T$ of $\S^n$ with spherical measure at least $\sigma$, there is a tuple of vectors $S'\subseteq T$ that is congruent to $S$, as long as $n \geq n_0:= n_0(S,\sigma)$. Both proving the density sphere Ramsey theorem, and showing that $\Gamma$ does not admit a sphere coloring respecting it are interesting open problems, and the former problem could have applications elsewhere as well.

%

On the other hand, extending the result to all (not necessarily folded) Boolean symmetric PCSPs requires a better understanding of polymorphisms of arbitrary Boolean symmetric PCSPs. We remark that for the decision version of Boolean symmetric PCSPs, a dichotomy was first proved for the folded case~\cite{BrakensiekG21}, and later, the restriction was removed~\cite{FicakKOS19}, where the authors showed that the decision version of a Boolean symmetric PCSP can be solved in polynomial time if and only if it has Threshold, $\AT$ or Parity polymorphisms. Our algorithm for the $\MAJ$ polymorphisms extends to the setting when there are threshold polymorphisms, similar to the algorithmic result of~\cite{FicakKOS19}.
Combining the polymorphic ideas in~\cite{FicakKOS19} with our sphere coloring results is a potential venue to generalize our hardness results to general Boolean symmetric PCSPs. 

Finally, can we get a robust dichotomy result for general Boolean PCSPs? We believe new algorithmic techniques are needed to understand what polymorphic families lead to robust algorithms. For the symmetric folded case, $\MAJ$ and $\AT$ polymorphisms resulted in robust algorithms while Parity polymorphisms resulted in just the decision version being solved in polynomial time. This suggests that the existence of a suitable notion of noise stable family of polymorphisms could be the key to robust algorithms for PCSPs. 
%

\item Can we improve the quantitative aspects of our robust approximation algorithm based on the Majority polymorphism to satisfy a $1-O(\sqrt{\eps})$ fraction of the constraints (which would be optimal~\cite{KKMO07} under the UGC)? Even for Max-Cut, such an algorithm that makes black-box use of the Majority polymorphism is not known. More generally, we do not know how to translate a polymorphism for the CSP into an approximation or robust satisfaction algorithm for it. For a suitable notion of  ``approximate" polymorphisms, there is such a connection~\cite{BR16}.

\item For the case of AT polymorphisms, our algorithm incurs an exponential loss, and only satisfies $1-O\left(\frac{\log \log \frac{1}{\epsilon}}{\log \frac{1}{\epsilon}}\right)$ fraction of the constraints in a $(1-\eps)$-satisfiable instance. Is this inherent (which is known to be the case for Horn-SAT~\cite{GuruswamiZ12})? Can one show (Unique Games) hardness of how robustly one can approximate concrete PCSPs like $1$-in-$3$-SAT vs. NAE-$3$-SAT?

\item For CSPs it is known that if Sherali Adams LP correctly ascertains exact satisfiability, then the CSP has a robust satisfaction algorithm (albeit not LP based). Does this connection remain true for PCSPs (similar to~\Cref{conj:sdp-robust})? 

\item We have minion characterizations for the solvability of PCSPs by SDPs (this work) and the Sherali Adams hierarchy~\cite{ciardo2022sherali}. What is the relationship between these minions?  Do there exist PCSPs whose exact satisfiability is decided by the basic SDP but which cannot be decided by $O(1)$ levels of the Sherali Adams hierarchy? Note that for CSPs, both these classes are the same and coincide with the class of bounded width CSPs.


%
    
    


\item As mentioned earlier, our algorithm for Majority polymorphism can be generalized for arbitrary threshold polymorphisms. Together with the hardness results in \cite{BGS21}, this gives a dichotomy w.r.t robust satisfiability for the class \emph{ordered} Boolean PCSPs (whose polymorphisms are monotone functions).

\end{itemize}

\bibliography{ref}
\bibliographystyle{alpha}

\appendix
\section{Missing Proofs}
\label{sec:missing-proofs}

\begin{proof}[Proof of Lemma~\ref{lem:equal-case}]
We first consider the case when $k=1$, i.e., $\Gamma_{const}$. Without loss of generality, let $P=Q=\{+1\}$, and we use $\textbf{v}_1 = \alpha \textbf{v}_0 + \textbf{v}'_1$ to denote the SDP vector corresponding to the variable used in the constraint. 
As the basic SDP has error at most $\sqrt{\epsilon}$, we get that 
\[
\alpha_1 \geq 1-\sqrt{\epsilon}
\]
As $\alpha_1^2 + \norm{v'_1}_2^2 = 1$, $\norm{v'_1}_2 \leq O(\epsilon^{0.25})$. Thus, using~\Cref{prop:gaussian-concentration}, we get that $\langle \zeta, \textbf{v}'_1 \rangle \leq O(\epsilon^{0.25}r)$ with probability at least $1-e^{\frac{-r^2}{2}} \geq 1-\sqrt{\epsilon}$. On the other hand, using~\Cref{prop:gaussian-anticoncentration}, we get that $|\langle \zeta, \textbf{v}_0 \rangle | \geq \frac{1}{r}$ with probability at least $1-\frac{1}{r}$. This implies that 
\[
\delta \alpha_1 | \langle \zeta, \textbf{v}_0 \rangle | \geq \frac{\delta }{2r}
\]
Thus, with probablity at least $1-O(\frac{1}{r})$, we have 
\[
\langle \zeta, \textbf{v}'_1 \rangle \leq O(\epsilon^{0.25}r) < \frac{\delta}{2r} \leq \delta \alpha_1 | \langle \zeta, \textbf{v}_0 \rangle |
\]
Hence, with probability at least $1-O(\frac{1}{r})$, we round the variable to $+1$. 

We now consider the general case when $k \geq 2$. Note that the above proof for $k=1$ holds when $P = \Ham_k \{0\}$ or when $P=\Ham_k \{k\}$. We are left with the setting when $P=\Ham_k \{0,k\}$.
In order to show that our algorithm is a robust algorithm for this PCSP, it suffices to show that all the elements in the predicates are rounded to the same value with high probability. Consider $i,j \in [k]$. We show that the probability that the variables $x_i$ and $x_j$ get rounded to different values is at most $O\left( \frac{1}{r}\right)$. Using the union bound over all the $\binom{k}{2}$  pairs of indices, we get our required claim. 

We first collect useful properties using the fact that the basic SDP is supported with a probability of at least $1-c$ on $P$. 
\begin{enumerate}
    \item (First moment.) We have 
    \[
    | \mu_i - \mu_j | \leq 2c. 
    \]
    \item (Second moment.) We have 
    \[
    \langle \textbf{v}_i, \textbf{v}_j \rangle \geq 1-2c
    \]
    Using this, we get 
    \begin{align*}
        \norm{\textbf{v}'_i - \textbf{v}'_j}_2^2 &= \textbf{v}_i - \textbf{v}_j + (\alpha_j - \alpha_i)\textbf{v}_0 \\ 
        &\leq \norm{\textbf{v}_i - \textbf{v}_j}_2^2 + (\alpha_i - \alpha_j)^2 \\ 
        &\leq O(c). 
    \end{align*}
    Thus, $\norm{\textbf{v}'_i - \textbf{v}'_j} \leq O(\sqrt{c})$.
\end{enumerate}
As earlier, we assume that $c$ is at most $\sqrt{\epsilon}$.
%

Recall that our goal is to upper bound the probability that $x_i$ and $x_j$ are rounded to different values. Without loss of generality, suppose that $x_i$ is rounded to $+1$, and $x_j$ is rounded to $-1$. 
We get that 
\begin{align*}
    \langle \zeta, \textbf{v}'_i \rangle &\geq \delta \alpha_i | \langle \zeta, \textbf{v}_0 \rangle | \label{eq:1}\\ 
    \langle \zeta, \textbf{v}'_j \rangle &< \delta \alpha_j | \langle \zeta, \textbf{v}_0 \rangle | 
\end{align*}
Using~\Cref{prop:gaussian-concentration}, we can infer that 
\[
| \langle \zeta, \textbf{v}'_i \rangle - \langle \zeta, \textbf{v}'_j \rangle | \leq O(r\sqrt{c})
\]
with probability at least $1-\frac{1}{r}$. 

We consider two cases: first, when $|\alpha_i| \leq \frac{1}{2}$. As $\alpha_i^2 + \norm{\textbf{v}'_i}_2^2=1$, and $|\alpha_i - \alpha_j | \leq 2c$, we get that $\norm{\textbf{v}'_j}=\Omega(1)$. 
In this case, we have 
\begin{align*}
    \langle \zeta, \textbf{v}'_j \rangle &\geq \langle \zeta, \textbf{v}'_i \rangle - O(r \sqrt{c}) \\ 
    &\geq \delta \alpha_i | \langle \zeta, \textbf{v}_0 \rangle | - O(r\sqrt{c}) \\ 
\end{align*}
Thus, we have 
\[
\langle \zeta, \textbf{v}'_j \rangle \in [\delta \alpha_i | \langle \zeta, \textbf{v}_0 \rangle | - O(r\sqrt{c}),\delta \alpha_j | \langle \zeta, \textbf{v}_0 \rangle | ]
\]
Here, $\langle \zeta, \textbf{v}'_j \rangle \in [p,q]$ where $q-p \leq O(\delta r)+O(r \sqrt{c}) \leq O(\delta r)$. However, as $\norm{\textbf{v}'_j} \geq \Omega(1)$, this happens with probability at most $O(\delta r)$. 

Now, suppose that $|\alpha_i|\geq \frac{1}{2}$. 
We have 
\begin{align*}
    \delta \alpha_i | \langle \zeta, \textbf{v}_0 \rangle | &\geq  \delta \alpha_j | \langle \zeta, \textbf{v}_0 \rangle |- 2\delta c | \langle \zeta, \textbf{v}_0 \rangle |
\end{align*}
However, as $\langle \zeta, \textbf{v}_0 \rangle \sim \mathcal{N}(0,1)$, we have $|\langle \zeta, \textbf{v}_0 \rangle | \leq r$ with probability at least $1-\sqrt{\epsilon}$. 
Thus, with probability at least $1-\sqrt{\epsilon}$, we have 
\[
\langle \zeta, \textbf{v}'_i \rangle \geq \delta \alpha_i | \langle \zeta, \textbf{v}_0 \rangle | \geq  \langle \zeta, \textbf{v}'_j \rangle  - 2\delta c r 
\]
We have $\delta \alpha_i |\langle \zeta, \textbf{v}_0 \rangle | \in [p,q]$ where $q-p \leq O(\delta c r)+ O(r\sqrt{c})$. However, this happens with probability at most $O(\frac{r\sqrt{c}}{\delta}) \leq O(\frac{1}{r})$.
\end{proof}

\begin{proof}[Proof of Lemma~\ref{lem:AT}]
Suppose that $\textbf{a} = \sgn(\textbf{x}-\textbf{y})$ for $\textbf{x}, \textbf{y} \in \Aff(P)$, and $x_i \neq y_i \forall i \in [k]$. By modifying the affine combinations slightly, we can assume that $\textbf{x}$ and $\textbf{y}$ are rational affine combinations of $P$ while still preserving the fact that $\textbf{a}=\sgn(\textbf{x}-\textbf{y})$. In other words, there exist $p_1, p_2, \ldots, p_K, q_1, q_2, \ldots, q_K \in \mathbb{Q}$ such that $\sum_{i \in [K]}p_i = \sum_{i \in [K]}q_i = 1$, and $\textbf{x} = \sum_{i \in [K]}p_i \textbf{a}_i$, $\textbf{y}=\sum_{i \in [K]}q_i \textbf{a}_i$,  where $\{-1,+1\}^k = \{\textbf{a}_1, \textbf{a}_2, \ldots, \textbf{a}_K\}$. Let $N$ be a positive integer such that we can write $p_i = \frac{p'_i}{N}, q_i = \frac{q'_i}{N}$ where $p'_i, q'_i$ are integers for every $i \in [K]$. 

Let $S$ be a multiset of $\{-1,+1\}^k$ where we take union over all $i \in [K]$, $p'_i$ copies of $\textbf{a}_i$, assign them a sign $\sgn(p'_i)$, and $q'_i$ copies of $\textbf{a}_i$, assign them a sign $\sgn(-q'_i)$. As we have $\sum_{i \in [K]}p'_i = \sum_{i \in [K]}q'_i$, the number of vectors in $S$ that are assigned $+1$ are is equal to the number of vectors that are assigned $-1$.
Let $\textbf{z}$ denote the signed sum of all vectors (including repetitions) in $S$. Note that $\sgn(\textbf{z})=\sgn(\textbf{x}-\textbf{y})$. As each element of $\mathbf{z}$ is an integer, we get that the absolute value of each coordinate in $\mathbf{z}$ is at least $1$. Furthermore, we can take multiple copies of $S$ to ensure that the absolute value of each coordinate in $\mathbf{z}$ is at least $2$. Now, we add an arbitrary element of $P$ with sign $+1$ to $S$. Note that we still have that the signed sum of $S$, i.e., the updated $\mathbf{z}$ satisfies $\sgn(\mathbf{z})=\sgn(\mathbf{x}-\mathbf{y})$. Furthermore, $\mathbf{z}=\textbf{x}_1 - \textbf{x}_2 + \ldots + \textbf{x}_L$ where each $\textbf{x}_i \in P$. Thus, $\textbf{a}=\sgn(\textbf{x}-\textbf{y})=\sgn(w)=\sgn(\textbf{x}_1 - \textbf{x}_2 + \ldots + \textbf{x}_L) \in O_{AT}(P)$. Thus, 
\[
\{\sgn(\textbf{x} - \textbf{y}) : \textbf{x}, \textbf{y} \in \Aff(P), \forall i, x_i \neq y_i \} \subseteq O_{AT}(P)
\]

To prove the other direction, suppose that $\textbf{a} \in O_{AT}(P)$. That is, $\textbf{a}=\sgn(\textbf{x}_1 - \textbf{x}_2 + \ldots + \textbf{x}_L)$. Let $S$ be a multiset of $\textbf{x}_1, \textbf{x}_2, \ldots, \textbf{x}_L$ with the corresponding sign as in the summation. As $P$ is non-trivial in every coordinate i.e., for every $i \in [k]$, there exist assignments $\textbf{x}$ in $P$ where $x_i=+1$, and similarly, $\textbf{x}' \in P$ where $x'_i=-1$. By adding vectors with both signs $+1$ and $-1$, we can assume that $S$ is non-trivial in every coordinate while still preserving the fact that the sign vector of the signed sum of $S$ is equal to $\sgn(\textbf{a})$. We modify $S$ while still preserving this property to ensure that the signed sum of vectors in $S$ has an absolute value of at least $2$ in every coordinate. 

As there are an odd number of vectors in $S$, the signed sum of the vectors has an absolute value of at least $1$ in every coordinate. 
Fix a vector $\textbf{x}_i \in S$. Create two copies of every other vector in $S$ (with the same sign as the original). Note that this operation does not alter the sign vector of the signed sum of the vectors in $S$. 
We can repeat this process at most $2k$ times to ensure that in the final multiset $S$, the signed sum has an absolute value of at least $2$ in every coordinate. Finally, we add an arbitrary vector with sign $-1$ to $S$, to ensure that the number of vectors with $+1$ sign and the number of vectors with $-1$ sign are equal in $S$. Overall, we get that there are $\textbf{x}_1, \textbf{x}_2, \ldots, \textbf{x}_N \in P$ and $\textbf{y}_1, \textbf{y}_2, \ldots, \textbf{y}_N \in P$ such that
\[\textbf{a}=\sgn(\textbf{x}_1+\ldots +\textbf{x}_N-\textbf{y}_1-\ldots -\textbf{y}_N) = \sgn \left( \frac{1}{N} \textbf{x}_1 + \ldots + \frac{1}{N}\textbf{x}_N-\frac{1}{N} \textbf{y}_1 - \ldots -\frac{1}{N}\textbf{y}_N\right)
\]
Thus, we get that $\textbf{a} \subseteq \{\sgn(\textbf{x} - \textbf{y}) : \textbf{x}, \textbf{y} \in \Aff(P), \forall i, x_i \neq y_i \}$, completing the proof that 
\[
O_{AT}(P)\subseteq \{\sgn(\textbf{x} - \textbf{y}) : \textbf{x}, \textbf{y} \in \Aff(P), \forall i, x_i \neq y_i \}
\qedhere \]
\end{proof}

\begin{proof}[Proof of~\Cref{lem:reduction}]
We extensively use the properties of $\AT, \MAJ$ polymorphisms of Boolean symmetric folded idempotent PCSPs proved in~\cite{BrakensiekG21}. 
We recall that $O_{\AT}(P)$ (resp. $O_{\MAJ}(P)$) denotes the set $\bigcup_{\textbf{x}_1,\ldots,\textbf{x}_L\in P,L \in \mathbb{N},\text{ odd},}\AT_L(\textbf{x}_1,\ldots,\textbf{x}_L)$ (resp. $\MAJ_L$) for a predicate $P$.

%
%
%
Let $k$ denote the arity of $P,Q$, i.e., $P \subseteq Q \subseteq \{-1,+1\}^k$.
Note that $P \nsubseteq \{ (-1, \ldots, -1), (+1,\ldots, +1)\}$ as in that case $O_{\MAJ}(P)  =P \subseteq Q$, contradicting the fact that $\MAJ \nsubseteq \Pol(P,Q)$. Thus, there exists $l \in \{1,2,\ldots, k-1\}$ such that $\Ham_k \{l\} \subseteq P$. 

\smallskip \noindent \textbf{Case 1.} We first consider the case when $P=\Ham_k \{l\}$ for some $l \in \{1,2,\ldots, k-1\}$. As $P$ is symmetric, $O_{\MAJ}(P)$ is symmetric as well~\cite{BrakensiekG21}.
Furthermore, as $\MAJ \nsubseteq \Pol(P,Q)$, there exists $b \in \{0,1,\ldots, k\}$ such that $\Ham_k \{b\} \cap Q=\phi$ and $\Ham_k \{b\} \subseteq O_{\MAJ}(P)$. 

Suppose that $b \notin \{0,k\}$. 
Let $Q' = \{-1,+1\}^k \setminus \Ham_k \{b\}$. By definition, $\MAJ \nsubseteq \Pol(P,Q')$. 
Using the fact that $O_{\AT}(\Ham_k \{l\}) = \Ham_k \{1,2,\ldots,k-1\}$, we get that $\AT \nsubseteq \Pol(P,Q')$. 
Thus, we get a PCSP $(P,Q)$ that is a relaxation of original PCSP where $P=\Ham_k \{l\}$, $Q=\Ham_k \{0,1,\ldots,k\} \setminus \{b\}$ where $b \in \{1,2,\ldots, k-1\} \setminus \{l\}$. 
Note that $\MAJ, \AT \notin \Pol(P,Q)$. 
We now relax this PCSP furthermore, updating $P,Q,l,k,b$ while preserving the following two properties:
\begin{enumerate}
    \item At every step, $P = \Ham_k \{l\}, Q = \Ham_k \{0,1,\ldots,k\} \setminus \{b\}$ where $b \in \{1,2,\ldots,k-1\}\setminus\{l\}$. 
    \item $\MAJ, \AT \notin \Pol(P,Q)$. 
\end{enumerate}
As $O_{\MAJ}(P) = \Ham_k \{ 0,1,\ldots, k\} \cap \{2l-k+1,\cdots,2l-1\}$, and $\Ham_k \{b\} \in O_{\MAJ}(P)$,  we get that $b \in \{2l-k+1,\cdots,2l-1\} \cap \{0,\cdots,k\}$. Furthermore, as $b>0$, we get that $l>1$. Similarly, we get that $l <k-1$. This also implies that $k \geq 4$ as $l \in \{1,\ldots,k-1\}$.

We use the following two tools to relax the PCSP: 
\begin{enumerate}
    \item Given a PCSP $P = \Ham_k \{l\}, Q = \Ham_k \{0,1,\ldots,k\} \setminus \{b\}$ where $b \in \{1,2,\ldots,k-1\}\setminus\{l\}$, then the PCSP $P' = \Ham_{k-1} \{l\}, Q = \Ham_{k-1} \{0,1,\ldots,k\} \setminus \{b\}$ is a relaxation of $(P,Q)$ (Claim $4.2$ of~\cite{BrakensiekG21}). As long as $b<k-1$ and $b \neq 2l-k+1$, this relaxation preserves the above two properties. 
    \item Given a PCSP $P = \Ham_k \{l\}, Q = \Ham_k \{0,1,\ldots,k\} \setminus \{b\}$ where $b \in \{1,2,\ldots,k-1\}\setminus\{l\}$, then the PCSP $P' = \Ham_{k-1} \{l-1\}, Q = \Ham_{k-1} \{0,1,\ldots,k\} \setminus \{b-1\}$ is a relaxation of $(P,Q)$ (Claim $4.4$ of~\cite{BrakensiekG21}). As long as $b >1$ and $b \neq 2l-1$, this relaxation preserves the above two properties. 
\end{enumerate}
Now, we relax the PCSP using the above two steps. As $k$ is decreasing at every step, this procedure terminates at some point. Then, either of the two conditions holds:
\begin{enumerate}
    \item $b=1, b=2l-k+1$. In this case, we get that $l = \frac{k}{2}$ and $b=1$. Thus, $(P',Q')$ is a relaxation of $\Gamma$ where $P'=\Ham_{k}\{\frac{k}{2}\}, Q = \Ham_{k} \{0,1,\ldots,k\} \setminus \{1\}$ where $k$ is even and is at least $4$. 
    \item $b=k-1, b=2l-1$. In this case, we get that $l = \frac{k}{2}$ and $b=k-1$. Thus, $(P',Q')$ is a relaxation of $\Gamma$ where $P'=\Ham_{k}\{\frac{k}{2}\}, Q = \Ham_{k} \{0,1,\ldots,k\} \setminus \{k-1\}$ where $k$ is even and is at least $4$. 
\end{enumerate}

Suppose that there is no $b \notin \{0,k\}$ such that $\Ham_k \{b\} \subseteq O_{\MAJ} (P) \setminus Q $. As $O_{\MAJ}(P) \nsubseteq Q$, by negating the variables if needed, we can assume that $\Ham_k \{0\} \in O_{\MAJ}(P) \setminus Q$. Furthermore, there exists $b \in \{1,2,\ldots, k-1\}$ such that $\Ham_k \nsubseteq  Q$ as $O_{\AT}(P) \nsubseteq Q$. Thus, we obtain a relaxation $(P,Q)$ of the original PCSP such that $P=\Ham_k \{l\}, Q=\Ham_k \{1,\ldots,k\}\setminus \{b\}$ where $l,b \in \{1,2,\ldots,k-1\}, b >2l-1$. By using the first type of relaxation used above (Claim $4.2$ of~\cite{BrakensiekG21}), we obtain a new relaxation such that $P=\Ham_k \{l\}, Q=\Ham_k \{0,1,\ldots,k\}\setminus \{0,k-1\}$, where $l \in \{1,2,\ldots,k-1\}, l \leq \frac{k-1}{2}$. 

\smallskip \noindent \textbf{Case 2}. There exist $l \neq l'$ such that $\Ham_k \{l,l'\}\subseteq P$. Recall that $P \nsubseteq \Ham_k \{0,k\}$. This implies that $O_{\AT}(P)=\Ham_k \{0,1,\ldots,k\}$. Hence, we can get a relaxation $(P,Q')$ of the original PCSP such that $Q = \Ham_k \{0,1,\ldots, k\} \setminus \{b\}$ such that $\Ham_k \{b\} \nsubseteq O_{\MAJ}(P)$. 

For ease of notation, let $P=\Ham_k S$, where $S \subseteq \{0,1,\ldots,k\}$. First, consider the case when $\min S=0, \max S=k$. As mentioned earlier, we know that there exists $l \in \{1,2,\ldots,k-1\}$ such that $\Ham_k \{l\} \subseteq P$. Thus, we can reduce the existing PCSP to $(P,Q)$ where $P=\Ham_k \{0,l,k\}, Q = \Ham_k \{0,1,\ldots,k\}\setminus \{b\}$ where $b \notin \{0,l,k\}$. We consider three cases separately:
\begin{enumerate}
    \item Suppose that $l \leq \frac{k-1}{2}$. In this case, we have a relaxation $(P,Q)$ where $P=\Ham_k \{l,k\}$ and $Q = \Ham_k \{0,1,\ldots,k\}\setminus \{b\}$ where $b \notin \{l,k\}$. Note that this relaxation does not contain $\AT$ or $\MAJ$ as polymorphisms. 
    \item Suppose that $l = \frac{k}{2}$. In this case, we have a relaxation $(P,Q)$ where $P=\Ham_k \{l\}$ and $Q = \Ham_k \{0,1,\ldots,k\}\setminus \{b\}$ where $b \notin \{0,l,k\}$. This also doesn't have $\AT$ and $\MAJ$ as polymorphisms. We have already shown that we can relax this further to the earlier mentioned three PCSPs. 
    \item Suppose that $l \geq \frac{k+1}{2}$. In this case, we have a relaxation $(P,Q)$ where $P=\Ham_k \{0,l\}$ and $Q = \Ham_k \{0,1,\ldots,k\}\setminus \{b\}$ where $b \notin \{0,l\}$. Note that this relaxation does not contain $\AT$ or $\MAJ$ as polymorphisms. 
\end{enumerate}
Thus, we have a relaxation $(P,Q)$ of the original PCSP where $P=\Ham_k \{l,l'\}, Q = \Ham_k \{0,1,\ldots,k\}\setminus \{b\}$ such that $l <l',\{l, l'\} \neq \{0,k\}$, $b \in O_{\MAJ}(P)$. 
We end up with the same relaxation when $\{\min S, \max S\} \neq \{0,k\}$. 

If $\{l,l'\} = \{1,k\}$ or $\{0,k-1\}$, we get a relaxation of the original PCSP where $P=\Ham_k\{1,k\}, Q=\Ham_k \{0,1,\ldots,k\}\setminus\{b\}$, and we are done. 
If not, we get a series of relaxations of the original PCSP maintaining the two below properties:
\begin{enumerate}
    \item $P=\Ham_k \{l,l'\}$ with $l <l'$ and $\{ l, l' \} \neq \{0,k\}$ and $Q = \Ham_k \{0,1,\ldots, k\} \setminus \{b\}$. We also assume that $\{l,l'\} \neq \{1,k\}$ and $\{l,l'\}\neq \{0,k-1\}$. 
    \item $\AT, \MAJ \nsubseteq \Pol(P,Q)$. 
\end{enumerate}
As with the earlier case, we update the PCSP using the two relaxations below: 
\begin{enumerate}
    \item We get $P'= \Ham_{k-1} \{l,l'\}$ and $Q=\Ham_{k-1} \{0,1,\ldots,k\} \setminus \{b\}$ using Claim $4.2$ of~\cite{BrakensiekG21}. For this to be a valid relaxation preserving the above properties, we need that $l' \neq k, b \neq k$ and $b \neq 2l-k+1$. 
    \item We get $P'=\Ham_{k-1} \{l-1,l'-1\}$ and $Q=\Ham_{k-1} \{0,1,\ldots,k\} \setminus \{b-1\}$ using Claim $4.4$ of~\cite{BrakensiekG21}. For this to be a valid relaxation preserving the above properties, we need that $l \neq 0, b \neq 0$ and $b \neq 2l'-1$. 
\end{enumerate}
As the arity of the predicates is decreasing at each step, this process terminates in finite steps. 
When we are unable to obtain a new relaxation using the above procedures, one of the following must be true.
\begin{enumerate}
    \item $l'=k, b=0$. In this case, we have a PCSP $(P,Q)$ where $P=\Ham_k \{l,k\}$, $Q=\Ham_k \{1,2,\ldots,k\}$, where $l\neq 0, l \leq \frac{k-1}{2}$. 
    \item $b=k,l=0$. This can be relaxed to the above by negating the variables. 
    \item $b=k, b=2l'-1$. We have $l'=\frac{k+1}{2}$. In this case, we have PCSP $(P,Q)$ where $P=\Ham_k \{l,\frac{k+1}{2}\}$, $Q=\Ham_k \{0,1,2,\ldots,k-1\}$, where $l \leq \frac{k-1}{2}$. 
    \item We have $b=2l-k+1, b=0$. In this case, we can relax to the above by negating the variables. 
\end{enumerate}

Thus, we have relaxed the original PCSP into either of the following PCSPs. 
\begin{enumerate}
    \item $k$ is even, and $P=\Ham_{k}\{\frac{k}{2}\}, Q = \Ham_{k} \{0,1,\ldots,k\} \setminus \{b\}$ where $b \in \{1,k-1\}$. 
    \item $k$ is odd, $P=\Ham_k \{l,\frac{k+1}{2}\}$, $Q=\Ham_k \{0,1,2,\ldots,k-1\}$, where $l \leq \frac{k-1}{2}$.
    \item $P=\Ham_k \{l,k\}$, $Q=\Ham_k \{1,2,\ldots,k\}$, where $l\neq 0, l \leq \frac{k-1}{2}$. 
    \item $P=\Ham_k \{l\}, Q=\Ham_k \{0,1,\ldots,k\}\setminus \{0,k-1\}$ where $l \in \{1,2,\ldots,k-1\}, l \leq \frac{k-1}{2}$. 
    \item $P=\Ham_k \{1,k\}, Q=\Ham_k \{0,1,\ldots,k\}\setminus\{b\}$ for arbitrary $b$.
\end{enumerate}

Finally, we note that if a Boolean folded idempotent PCSP $\Gamma'=\{(P_1, Q_1),\ldots,(P_l, Q_l)\}$ is a relaxation of another Boolean folded idempotent PCSP $\Gamma$, then $\Gamma'$ remains a relaxation of $\Gamma$ even when we disallow the constraints in $\Gamma'$ to use constants, i.e., the Boolean folded PCSP associated with $\Gamma'$ is a relaxation of $\Gamma$ as well.
%
%
%
\end{proof}

\section{Gadget reductions of PCSPs and robust algorithms}
\label{sec:gadget}

\begin{proof}[Proof of Proposition~\ref{lem:ppp}]
As proved in~\cite{BrakensiekG21}, $\Gamma'$ is ppp-definable from $\Gamma$. 
Given an instance $\Phi'=(V',\mathcal{C}')$ of $\Gamma'$ over a set of variables $V'$ and containing $m$ constraints $\mathcal{C}'=\{C'_1,C'_2,\ldots,C'_m\}$, we output an instance $\Phi$ of $\Gamma$ containing $|V'|$ original variables and a set of dummy variables. 
For every constraint $C'_j$ using $(P', Q')$ of arity $k_j$ involving the variables (if $\Gamma'$ is folded or idempotent, these include literals and constants accordingly) $C'_j=(u_{j,1}, u_{j,2}, \ldots, u_{j,k_j}) $ in $\Phi'$, we have a set of dummy variables $w_{j,1}, w_{j,2}, \ldots, w_{j,l_j}$ and a set of constraints $S_j$
 among $C_j=\{u_{j,1}, u_{j,2}, \ldots, u_{j,k}, w_{j,1}, w_{j,2}, \ldots, w_{j,l_j}\}$ as in~\Cref{def:ppp}. Let $W=\cup_j \{ w_{j,1}, w_{j,2}, \ldots, w_{j,l_j}\}$ and let $V=V'\cup W$ be the set of variables of $\Phi$, and $\mathcal{C}=\cup_{j\in[m]}S_j$ are the set of constraints in $\Phi$.
We claim that this reduction preserves robust algorithms. \begin{enumerate}
    \item (Completeness). Suppose that there exists an assignment $\sigma': V' \rightarrow D_1$  strongly satisfying $1-\epsilon$ fraction of the constraints in $\Phi'$. For every $j \in [m]$ such that $\sigma'$ strongly satisfies the constraint $C'_j$, there is an assignment $\sigma_j:\{ w_{j,1}, w_{j,2}, \ldots, w_{j,l_j}\}\rightarrow D_1$ such that $\sigma \cup \sigma_j : C_j\rightarrow D_1$ strongly satisfies all the constraints $S_j$. Consider an assignment $\sigma: V \rightarrow D_1$ where we set $\sigma(u_i)=\sigma'(u_i)$ for $u_i \in V'$, $\sigma(w_{j,i})=\sigma_j(w_{j,i})$ for all $w_{j,i}\in W$ such that $\sigma$ strongly satisfies the constraint $C'_j$. If $\sigma$ does not strongly satisfy the constraint $C'_j$, we set $\sigma(w_{j,i})$ arbitrarily. The assignment $\sigma$ strongly satisfies at least $1-O_{\Gamma,\Gamma'}(\epsilon)$ fraction of the constraints in $\Phi$. 
    \item (Soundness). Suppose that there is an assignment $\sigma : V \rightarrow D_2$
    weakly satisfying $1-\epsilon$ fraction of the constraints in $\Phi$. For at least $1-O_{\Gamma,\Gamma'}(\epsilon)$ values of $j \in [m]$, all the constraints in $S_j$ are weakly satisfied by $\sigma$. This shows that the assignment $\sigma$ restricted to $V'$ weakly satisfies $1-O_{\Gamma,\Gamma'}(\epsilon)$ fraction of the constraints in $\Phi'$. \qedhere
\end{enumerate}
\end{proof}

\end{document}