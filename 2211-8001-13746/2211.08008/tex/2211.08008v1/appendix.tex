\section{Additional Results}\label{app:results}

\subsection{%
    Larger Perturbations
}\label{app:results:epsilons}

\Cref{tab:compare:cifarx:0.02,tab:compare:cifarx:0.03}
compare the effectiveness
of iterative methods~\cite{madry18,carlini17},
learned attacks (\textbf{CAA}~\cite{mao2021caa}),
AutoAttack (\textbf{AA})~\cite{croce20aa},
% adaptive auto attack
% \aaa~\cite{ye2022aaa},
\attack{} and \attackmt{}
across various ensemble defense strategies
with ResNet-20~\cite{resnet} sub-models
under 3 ensemble-forming modes
(softmax, voting and logits).
Here, \Cref{tab:compare:cifarx:0.02}
uses an \( \linf \) perturbation bound
\( \epsilon = 0.02 \),
whereas \Cref{tab:compare:cifarx:0.03}
uses \( \epsilon = 0.03 \).
All results are re-run 5 times
and within \( \pm 0.05\% \) standard deviation.
Note that under \( \epsilon = 0.02 \)
most defense methods
only retain up to \( 4\% \) robust accuracies.
Dverge~\cite{yang2020dverge} with 8 sub-models
however surprisingly has \( 23.31\% \)
robustness against the strongest \attackmt{} attack.
We believe this can be attributed
to the fact that
similarly to adversarial training,
Dverge diversifies sub-models
with adversarial examples from each other,
rather than via explicit regularization
as carried out by the other defenses.
It thus requires substantially
higher training cost than the others
(\( \sim 5\times \) ADP~\cite{pang2019adp}).%

Finally,
\Cref{fig:epsilon}
compares the different ensemble-forming strategies
under an increasing \( \linf \) perturbation bound
\( \epsilon \in \braces{0.01, 0.015, 0.02, 0.025, 0.03} \).
Note that across all configurations,
logit-based ensembles are the most robust.
\input{tables/cifar10_0.02}%
\input{tables/cifar10_0.03}%
\begin{figure}[ht]
    \centering
    \newcommand{\epsplot}[2]{
        \subplot{epsilon/#1_3}{#2 (3 sub-models)}
        \subplot{epsilon/#1_5}{#2 (5 sub-models)}
        \subplot{epsilon/#1_8}{#2 (8 sub-models)}
    }
    \epsplot{dverge}{Diverge}
    \epsplot{trs}{TRS}
    \epsplot{gal}{GAL}
    \caption{%
        Ensemble defenses with 5 sub-models \wrt{}
        the \( \epsilon \) bound on \( \linf \) perturbation.
        Contrary to existing literatures
        that propose ``softmax''-based ensembles~\cite{
            kariyappa2019gal,yang2020dverge,yang2021trs},
        we generally find ``logits''
        to be the most robust option
        across the three ensemble-forming strategies.
        All defenses are evaluated with \attack{} (500 iterations).
        Note that the only exceptions to this rule,
        \ie{}, GAL ensembles with 3 sub-models,
        exhibit very low robust accuracies.
        % We additionally include~\Cref{fig:epsilon/dverge}
        % for direct comparisons.
    }\label{fig:epsilon}
\end{figure}%

\subsection{%
    Convergence Speed Comparisons
}\label{app:results:convergence}

\Cref{fig:convergence:dverge,fig:convergence:gal}
compare the convergence speed
of \attack{} against SOTA attacks
for up to 500 iterations.
As AA comprises multiple attack strategies,
we extract its two gradient-based attacks
(APGD-\{CE,DLR\}) to facilitate comparisons.
Each attack uses 5 restarts,
each with up to 100 iterations.
The horizontal axes
show the iteration counts used,
and the vertical axes
denote the percentages of remaining unsuccessful images.
When forming ensembles with logits,
\attack{} and ``No reweighing'' are both identical.
Note that \attack{} substantially
outperforms most existing attacks
under 500 iterations,
and generally requires (up to \(70\times\)) fewer
iterations to achieve the same accuracies
of other attacks with 500 iterations.
\newcommand{\convplot}[3]{%
    \subplot[app:]{convergence/#1_3_#2}{3 sub-models (#3)}
    \subplot[app:]{convergence/#1_5_#2}{5 sub-models (#3)}
    \subplot[app:]{convergence/#1_8_#2}{8 sub-models (#3)}
}%
\begin{figure}[ht]
    \convplot{dverge}{softmax}{softmax}
    \convplot{dverge}{argmax}{voting}
    \convplot{dverge}{logits}{logits}
    \caption{%
        Comparing the convergence speed
        on Dverge~\cite{yang2020dverge} models
        with three ensemble-forming strategies
        (softmax, voting, logits)
        of \attack{}
        against a variant of \attack{}
        without sub-model reweighing,
        \cw, PGD and AA losses (APGD-\{DLR,CE\}).
        For logit-based ensembles
        (\subref{fig:app:convergence/dverge_3_logits},
         \subref{fig:app:convergence/dverge_5_logits},
         \subref{fig:app:convergence/dverge_8_logits}),
        \attack{} and ``No reweighing'' are identical.
        The horizontal and vertical axes
        respectively show the iteration count used,
        and the percentage of unsuccessful images remaining.
        We annotated the number of iterations
        for \attack{} to overtake competition
        with 500 iterations.
    }\label{fig:convergence:dverge}
\end{figure}
\begin{figure}[ht]
    \convplot{gal}{softmax}{softmax}
    \convplot{gal}{argmax}{voting}
    \convplot{gal}{logits}{logits}
    \caption{%
        Similar comparisons of convergence rates
        on GAL~\cite{kariyappa2019gal} models.
        Please refer to~\Cref{fig:convergence:dverge}
        for a detailed description of the setup.
    }\label{fig:convergence:gal}
\end{figure}

\subsection{%
    Ablation and Sensitivity Analyses
}\label{app:results:ablation}

\input{figures/sensitivity/temperature}
\input{figures/sensitivity/beta}
In \Cref{tab:ablation},
we perform ablation
of the individual components
used in \attack.
We begin with
the standard ``PGD'' attack~\cite{madry18}
with 5 random restarts, each with 100 iterations,
and a constant step size of \( \epsilon / 4 \).
Each row then consecutively
adds a new component.
``Momentum'' introduces momentum \( \momentum = 0.75 \)
as used in~\Cref{alg:overview},
``Cosine Step Size''
then replaces the constant step size
with a cosine schedule \(
    \alpha_i =\epsilon \parens*{
        1+\cos \parens*{ \nicefrac{i\pi}{I} }
    }
\).
``Sub-model Logits'' further
exploits the sub-model logits directly
following~\Cref{sec:method:overview},
and replaces random restarts
with a \( \beta \in \braces{0, 0.25, 0.5, 0.75, 1} \)
grid search
to match the cost of 500 iterations.
``Logit Normalization''
incorporates the normalization
of logits as proposed in~\eqref{eq:scenorm} of~\Cref{sec:method:loss}.
``Adaptive Reweighing''
adaptively adjusts sub-model weights
for attacking ensemble defenses
(\eqref{eq:weight} of~\Cref{sec:method:weight}).
Finally,
``Multiple Targets''~\cite{gowal19surrogate}
additionally uses 100 iterations
of the targeted variant of \( \attackloss \)
on each of the remaining 9 class labels
(\Cref{sec:method:loss}).
\input{tables/ablation}

Increasing the temperature coefficient \( \tau \)
from 1 as used in~\eqref{eq:attackloss}
can affect the convergence speed
of \attack.
To ensure a fair comparison in our results,
we fix a constant \( \tau = 5 \)
for softmax-based ensembles,
and \( \tau = 10 \)
for voting,
as increasing \( \tau \)
may help improve convergence.
In addition,
and \Cref{fig:sensitivity:temperature}
provides sensitivity analyses
of \( \tau \) on the three defending methods
(ADP, GAL, Dverge).

Finally,
\Cref{fig:sensitivity:beta}
shows the effect of varying \( \beta \in [0, 1] \),
the interpolation between \( \attackloss \)
and the ensemble's original loss \( \sceloss \).
Introducing \( \attackloss \)
substantially improves
the strength of attack.
Note that in our comparison results
(\Cref{%
    tab:compare:cifarx,tab:compare:cifarx:at,%
    tab:compare:cifarx:0.02,tab:compare:cifarx:0.03,%
    tab:compare:cifarc}),
instead of 5 random restarts
we use a \(
    \beta \in \braces{0, 0.25, 0.5, 0.75, 1}
\) schedule
to further improve the final attack success rate.

\subsection{\cifarc}\label{app:results:cifarc}

\Cref{tab:compare:cifarc}
compares the attacks
on PDD+DEG~\cite{huang2021pdd} defenses
trained on \cifarc{}.
Similar to GAL~\cite{kariyappa2019gal} and TRS~\cite{yang2021trs},
PDD+DEG also diversifies sub-models gradients
by minimizing cosine-similarities of gradients
via regularization,
and further diversifies feature selection
with adaptive dropouts.
We report attacks on ensembles
with three ensemble-forming methods
(softmax, voting and logits).
Note that we only included PDD+DEG
as other methods did not train on \cifarc.
\input{tables/cifar100}

\subsection{%
    Failure Modes in Ensemble Defenses
}\label{app:results:obfuscation}

\input{figures/loss_surface}
\Cref{fig:loss_surface}
provides a visualization
of the loss surfaces
of ADP~\cite{pang2019adp}
under 3 different ensemble-forming strategies.
In this section,
we continue the discussion
of the two failure modes
in ensemble defense
that induce overestimated robustness
as introduced in~\Cref{sec:intro}:

(a) \emph{%
    Gradient obfuscation
    via ensemble-forming strategies.
}
It is evident that under PGD-10 attacks,
both softmax- and voting-based ensembles
(Figures~\ref{fig:loss:pgd:softmax}
 and~\ref{fig:loss:pgd:voting} respectively)
exhibit to some extent gradient obfuscation
as they result in flatter loss surfaces
in the adversarial direction \( \g \),
whereas the logits-based variant
does not (\Cref{fig:loss:pgd:logits}).

(b) \emph{Gradient diversification.}
As sub-model gradients counteract,
PGD attacks on softmax- and voting-based ensembles
may result in an averaged gradient direction \( \g \)
that experience difficulty in increasing loss
(Figures~\ref{fig:loss:pgd:softmax}
 and~\ref{fig:loss:pgd:voting} respectively).
Adopting sub-model reweighing
(bottom row in~\Cref{fig:loss_surface})
alleviates this difficulty,
and allows the attack to succeed more reliably.

\section{%
    Limitations and Potential Societal Impacts
}\label{app:limitations}

The \( \linf \) white-box threat model
assumes the availability of the models' gradients,
which could present a challenge
as such information may not be available
to the attacker.
It is thus critical
to evaluate white-box robustness
accurately,
as it provides the lower bounds
on the robustness of ensemble defenses
in practical scenarios.

We acknowledge that
adversarial attacks
may have the potential
to be used by a malicious party,
but we believe defending against such attacks
is critically pertinent
to the accurate evaluation of robustness.
We hope this paper furthers
the understanding of ensemble robustness,
and accurately evaluating adversarial robustness
can help improve future defenses.
It is also noteworthy
that the white-box threat model
has applications in the context of advancements
in deep learning,
and can improve
\eg{}, transfer learning~\cite{
    utrera2021adversariallytrained,deng2021adversarial},
GAN training~\cite{bashkirova2019adversarial},
interpretability~\cite{ross2018improving},
and \etc{}

\section{Computational Resources}\label{app:resources}

On NVIDIA Tesla V100 GPUs,
\attack{} with 500 iterations
uses up to \( 1.0 \) GPU-hours
on each ensemble defense,
and \attackmt{} uses up to \( 2.8 \) GPU-hours
on the \cifarx{} test set.
The run time
depends on the number of sub-models
in an ensemble
and its attack difficulty
(\Cref{tab:run_time}).
% We estimate that
% collecting the experimental results
% in this paper
% requires \( 200 \) GPU-hours.
\begin{table}[ht]
\centering\caption{%
    Run time of \attack{} (up to 500 iterations)
    and \attackmt{} (up to 1.4k iterations)
    for ADP, Dverge and GAL ensemble defenses
    on the \cifarx{} test set.
    Easier ensembles consume less time
    with early stopping.
}\label{tab:run_time}
\begin{tabular}{cc|rr}
    \toprule
    \thead{Run time} (min) & \# & \attack{} & \attackmt{} \\
    \midrule
        & 3 &  2.0 &   3.7 \\
    ADP~\cite{pang2019adp}
        & 5 &  3.4 &   5.8 \\
        & 8 &  6.5 &  11.4 \\
    \midrule
        & 3 & 12.3 &  31.1 \\
    Dverge~\cite{yang2020dverge}
        & 5 & 29.7 &  71.2 \\
        & 8 & 61.9 & 169.3 \\
    \midrule
        & 3 &  2.6 &   4.3 \\
    GAL~\cite{kariyappa2019gal}
        & 5 & 15.4 &  35.2 \\
        & 8 & 37.6 &  91.8 \\
    \bottomrule
\end{tabular}
\end{table}

% \FloatBarrier
\section{Licenses}\label{app:licenses}

\Cref{tab:sources}
lists the relevant resources
used in this paper and their respective licenses.
% Note that the resources are all
% publicly available and open-source,
% and widely used in the machine learning community,
% but some provided no licenses.
\begin{table}[ht]
\centering\caption{%
    Open-source resources used in this paper.
}\label{tab:sources}
% \adjustbox{width=\textwidth}{
\begin{tabular}{ccl}
    \toprule
    \thead{Name} & \thead{License} & \thead{URL} \\
    \midrule
    PyTorch & BSD
        & \href{https://github.com/pytorch/pytorch}{GitHub: pytorch/pytorch} \\
    Dverge & \tna{}
        & \href{https://github.com/zjysteven/DVERGE}{GitHub: zjysteven/DVERGE} \\
    TRS & \tna{}
        & \href{https://github.com/AI-secure/Transferability-Reduced-Smooth-Ensemble}{GitHub: AI-secure/Transferability-Reduced-Smooth-Ensemble} \\
    \cifarx{} & \tna{}
        & \url{https://www.cs.toronto.edu/~kriz/cifar.html} \\
    \cifarc{} & \tna{}
        & \url{https://www.cs.toronto.edu/~kriz/cifar.html} \\
    \bottomrule
\end{tabular}
% }
\end{table}
