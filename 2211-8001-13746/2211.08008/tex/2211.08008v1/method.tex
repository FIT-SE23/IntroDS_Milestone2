\section{%
    The Model-Reweighing Attack (\attack)
}\label{sec:method}

\subsection{%
    Problem Formulation \& High-Level Overview
}\label{sec:method:overview}

As discussed in \Cref{sec:intro},
existing ensemble defenses
may obfuscate gradients
with the ensemble-forming mode
and gradient diversification,
such that the final loss of the ensemble model
can no longer provide effective signals
for gradient descent.
It is therefore
desirable to find an alternative \( \loss \)
to the original SCE loss \( \sceloss \)
on the ensemble,
such that for a given number of iterations \( I \),
the original \( \sceloss \) loss can be maximized:
\begin{equation}
    \textstyle
    \begin{aligned}
    &{\max}_{\loss} \sceloss\parens{\xadv_I, y}
    \quad \text{where} \quad
    \xadv_{0} = \project\parens{\x + \m},\, \\
    &\quad\xadv_{i + 1} = \mathrm{PGD}\parens{
        \loss\parens{
            \f{E}\parens{\xadv_i}, \f{[1:M]}\parens{\xadv_i},
        y}
    },
    \end{aligned}
\end{equation}
and \( \mathrm{PGD}\parens{\cdot} \)
denotes a PGD step
along the gradient of loss function \(
    \loss\parens{
        \f\E\parens{\xadv_i},
        \f{[1:M]}\parens{\xadv_i},
    y}
\),
which not only takes the ensemble predictions
\( f_E\parens{\xadv_i} \) as input,
but can further utilize sub-model predictions
\( \f{[1:M]} \)
to guide the PGD iterations.
The challenge at hand
is, therefore,
to find a suitable \( \loss \)
which can generate attacks
on ensemble defenses efficiently and effectively.

% \subsection{High-Level Overview}

\input{figures/overview}

\attack{} aims to provide
a potential optimization route
towards the above problem formulation.
Namely,
in addition to the original output
of the ensemble \( \f\E\parens{\xadv} \),
we leverage the sub-model predictions
\( \f{[1:M]}\parens{\xadv} \)
to facilitate the optimization.
By way of illustration,
\Cref{fig:method:overview}
shows a high-level overview
of the model-reweighing attack,
where we compliment the ensemble loss,
with a newly added sub-model reweighing loss
\( \attackloss \),
as an auxiliary attack vector
alongside the original objective.
Not only can the new loss
bypass the ensemble-forming strategy
to work around its obfuscated gradients,
but it further exploits information
present in the individual sub-model
and ensemble predictions
to steer the direction of adversarial example synthesis.

\subsection{%
    Adaptive Sub-model Importance
}\label{sec:method:weight}

Before we begin,
assume that
\( \logit{m} \triangleq f_m \parens{\x} \)
represents the \( \ordinal{m} \)
sub-model output,
and let \( \logit{m}_t \)
denote the corresponding logit of label \( t \).
We define the difference of logits (DL)~\cite{carlini17}
\( \dl{m} \triangleq \logit{m}_y - \logit{m}_\hy \),
which is the difference
between the predictions
of the ground truth \( \logit{m}_y \)
and the maximum of the remaining classes
\(
    \logit{m}_\hy
    \triangleq
    \max_{i \in \classset / y} \logit{m}_i
\),
where \( \classset / y \)
is the set of all class labels except \( y \).
Similarly,
we let \( \logit{\E} \triangleq f_\E\parens{\x} \),
and \( \logit{\E}_t \) and \( \dl{\E} \)
be the respective variants of the ensemble prediction.
It is notable that
a successful attack happens
when \( \dl\E < 0 \),
and similarly \( \dl{m} < 0 \)
means the \( \ordinal{m} \) sub-model
is producing incorrect classification.

Ensemble defenses
% such as GAL~\cite{kariyappa2019gal},
% ADP~\cite{pang2019adp} and TRS~\cite{yang2021trs}
tend to diversify sub-model gradients,
for instance, ADP~\cite{pang2019adp}
minimizes the cosine-similarity \(
    \angles{
        \nabla \ell\parens{\logit{a}},
        \nabla \ell\parens{\logit{b}}
    }
\) among each loss function gradient pairs
of sub-models \(
    \ell\parens{\logit{a}}
\) and \(
    \ell\parens{\logit{b}}
\).
Their intuition
is that it may lower transferability
among these sub-models,
such that attacks
with the overall gradient of the ensemble,
\ie{}, \(
    \nabla \ell\parens{\logit\E}
    = \frac1M \sum_{m \in [1:M]}
        \nabla \ell\parens{\logit{m}}
\),
are becoming less effective
in misleading all sub-models simultaneously,
as individual gradients in \(
    \nabla \ell\parens{\logit{m}}
\) are encouraged to be orthogonal to each other.
To this end,
we propose to reweigh the importance of sub-models,
by instead considering the modified gradient:
\begin{equation}
    \textstyle
    \widehat{\nabla \ell\parens{\logit\E}}
    = \frac1M \sum_{m \in [1:M]}
        { \imp{m}\parens{\logit{m}} }
        \nabla \ell\parens{\logit{m}},
\end{equation}
where
\( \imp{m} \)
assigns weights to important sub-models
to contribute more heavily
to the attack gradient.

While the adversarial examples of the ensemble
could present a challenge to discover,
individual sub-models
are weak defenders
which can be easily defeated.
Based on this property,
we propose to weigh sub-models importance
with the rate of change in \( \dl{\E} \)
\wrt{} that of \( \dl{m} \),
\ie{}, sub-models would be given higher weights
if attacking it would bring a significant change
to the ensemble's prediction.
Following this idea,
for all ensemble-forming strategies (softmax, voting, logits),
we rewrite \( \dl{\E} \)
as a function of \( \dl{m} \),
where the term below
can become a function of \( \dl{m} \):
\begin{equation}
    \textstyle
    \begin{aligned}
    \dl{\E} = \ensop\parens{\logit{m}}_\y -
    \ensop\parens{\logit{m}}_\hy
    &= \ensop\parens{\logit{m} - \logit{m}_\hy}_\y -
      \ensop\parens{\logit{m} - \logit{m}_\hy}_\hy \\
    &= \ensop\parens{\dl{m}, \cdots}_\y -
      \ensop\parens{\dl{m}, \cdots}_\hy
    \triangleq h_m\parens{\dl{m}}.
    \end{aligned}
\end{equation}
The weights are thus defined as follows:
\begin{equation}
    \textstyle
    \imp{m}\parens{\logit{m}}
    = \frac{\partial \dl{\E}\parens{\dl{m}}}{\partial \dl{m}}
    = \frac\partial{\partial\dl{m}}\parens*{
        {\frac1M} {\ssum}_{m\in[1:M]}
            \frac{\partial h_m\parens{\dl{m}}}{\partial \dl{m}}
    }
    = \frac1M \frac{\partial h_m\parens{\dl{m}}}{\partial \dl{m}}.
\end{equation}
While it is possible
to compute the weights using gradient back-propagation,
we can simply derive the following closed-form solution
of the weights
for each of the three ensemble-forming strategies.
For \( \wta \),
we use the softened version of \( \wta \)
as defined in~\eqref{eq:softwta}
and can derive the weights as follows:
\begin{equation}
    \newcommand{\sv}{\mathbf{s}}
    \imp{m}\parens{\logit{m}} =
    \indicator\bracks{\dl{m} > 0} \cdot
    \detach\parens*{
        {\textstyle\frac1{\tau M}}
        \sv_\hy \parens*{1 + \sv_\y - \sv_\hy}
    },
    \quad \text{where}\,\,
    \sv = \softmax\parens*{\nicefrac{\logit{m}}\tau}.
    \label{eq:weight}
\end{equation}
Here \( \indicator\bracks{\dl{m} > 0} \)
is the indicator function
that equals 1 if \( \dl{m} > 0 \),
or 0 otherwise,
effectively stopping the attack
on the \( \ordinal{m} \) sub-model upon success,
and the \( \detach \) operator
admits no backward propagation to its input.
In the case of using sums
of sub-model softmax outputs
to form an ensemble decision,
\ie{}, \( \ensop = \softmax \),
it is a special case of \( \softwta_\tau \)
where the temperature coefficient
can be fixed at \( \tau = 1 \).
% but we further searched \( \tau \)
% for optimality.
Finally,
when \( \ensop = \id \),
\ie{}, forming ensembles by summing logits,
\( \imp{m}\parens{\logit{m}} \)
simply reduces to \( \indicator\bracks{\dl{m} > 0} \)
for the \( \ordinal{m} \) sub-model.
\input{algorithm}

\subsection{The \attack{} Loss}\label{sec:method:loss}

For reference,
defenses mechanisms we examine
in this paper
aim to find \( \xadv \)
which maximizes the SCE loss
\(
    \sceloss\parens*{
        % \ssum_{m=1}^M \ensop\parens{}
        \logit{\E}, \y
    },
\)
to evaluate the ensemble robustness.
The \attack{} loss improves this further
by proposing two additional modifications
to the untargeted loss function used to attack ensembles:
\begin{equation}
    \attackloss
        \parens{\logit{1:M}, \logit{\E}, y}
    \triangleq
    \sceloss\parens*{
        \beta \scenorm\parens*{
            \ssum_{m \in [1:M]}
            \imp{m}\parens{\logit{m}} \cdot \logit{m}
        }
        + (1 - \beta) \scenorm\parens*{\logit\E}, y
    }.
    \label{eq:attackloss}
\end{equation}
First,
it additionally introduces a sum
of the \( \imp{m} \)-weighted variant
of sub-model logits,
in order to expose sub-model logits
with adaptive reweighing described in \Cref{sec:method:weight}.
Second,
\( \beta \)
interpolates the importance
of the newly added auxiliary logits
and the original ensemble logits.  % TODO \beta = ???
Finally,
inspired by the effective surrogate loss
in~\cite{lafeat},
it further normalizes the logits
by their respective DL using:
\begin{equation}
    \scenorm\parens{\z} \triangleq
        \indicator\bracks*{\z_\y - \z_\hy > 0}
        \cdot
        {\z} / {\detach\parens{\z_\y - \z_\hy}}.
    \label{eq:scenorm}
\end{equation}

Finally, the targeted variant
of the \attack{} loss simply replaces \( y \) with \( t \)
where \( t \) is the intended target.

\subsection{Improving the State-of-the-art}

While the new \( \attackloss \) loss
is highly effective against
ensemble defenses we test in this paper,
we strive for further advances
in \attack's ability
to generate faster and better adversarial examples.
Inspired by recent publications,
we borrow ideas
from related adversarial attack tactics,
which includes
adopting a cosine step-size schedule~\cite{ye2022aaa},
momentum~\cite{dong18momentum,croce20aa},
random restarts~\cite{tramer2020adaptive}
and multiple target attacks~\cite{croce20aa,tramer2020adaptive}.
We provide the overall algorithm
in \Cref{alg:overview},
which computes
an adversarial image \( \xadv_I \)
as return,
by taking as input
the sub-models \( \f{[1:M]} \),
natural image \( \x \),
ground truth label \( \y \),
\( \beta \) to interpolate between
the auxiliary logits and the original,
\( \tau \) controls the temperature,
momentum \( \mu = 0.75 \)
following~\cite{lafeat,croce20aa},
\( \epsilon \) perturbation bound,
and finally the maximum number of iterations \( I \).
