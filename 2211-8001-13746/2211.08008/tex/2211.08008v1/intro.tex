\section{Introduction}\label{sec:intro}

Many safety-critical applications,
such as autonomous robots~\cite{zhu2021can},
self-driving~\cite{eykholt2018robust},
search engines~\cite{tolias2019targeted}, \etc{}
are becoming increasingly powerful and reliant
on deep neural networks (DNNs).
Despite the monumental success of DNNs
on these applications,
they are highly susceptible
to adversarial examples ---
an attacker can add tiny delibrate perturbations
to the input data,
misleading the model
into giving incorrect results~\cite{szegedy14,goodfellow15}.
Such adversarial attacks
could pose a significant threat
to the safety and reliability
of deep learning applications.

To mitigate this threat,
many defense strategies~\cite{madry18,zhang19trades,carmon19}
based on adversarial training~\cite{madry18}
have been proposed
to improve model robustness.
Adversarial training, however,
gains robustness at the expense of model accuracy
on clean natural images~\cite{tsipras2018robustness}.
Ensemble defenses~\cite{
    pang2019adp,kariyappa2019gal,yang2020dverge,yang2021trs}
have thus emerged
to combine multiple predictions
from independent sub-models.
The intuition is
that an ensemble of models
can often lead to higher accuracy,
while learning to stop
adversarial example transfer among sub-models
may improve robustness against adversarial attacks.
This approach could potentially
offer a promising research direction
to improve model robustness
while preserving high accuracy on natural inputs.

\input{figures/motivation}
Yet surprisingly,
under the white-box threat model,
existing state-of-the-art (SOTA)
adversarial attacks
with strong performance
on conventional DNN models
performed poorly on ensemble models,
sizeably overestimating their robustness
(\Cref{fig:scatter/gal,fig:scatter/dverge}).
This also suggests, to some extent,
that ensemble defenses
may rely on two potential design flaws below
that cause obfuscated gradients~\cite{athalye2018obfuscated},
\ie{}, they are either deliberately non-differentiable,
or give no useful gradients,
thus inducing overestimated robustness:

(a) \emph{%
    Gradient obfuscation
    via ensemble-forming strategy.}
They typically form ensembles
by averaging probability vectors (softmax)
of sub-models,
and softmax operations
can easily cause gradient obfuscation.
% Their robustness results were self-reported
% by attacking this output as a loss function,
% which has a relatively flat loss surface,
% and can impede gradient-based attacks.
While the model's actual robustness
is pertinent to the strategy
used to form an ensemble,
this indicates
that gradient-based attacks
have to \emph{also} leverage this effectively.

(b) \emph{Gradient diversification.}
Motivated by the reasoning
that a majority of sub-models
may need to be fooled
for successful attacks,
they learn to reduce adversarial transferability
among sub-models,
often via gradient diversification.
This intuitively
causes sub-models to counteract each other,
averaging to a small or inaccurate overall gradient.
% even though individual sub-models are weak defenders.
% However,
% as shown in this paper,
% existing attacks
% are not effective in evaluating ensembles
% created in this manner,
% even though they are not robust.
Attacking only the ensemble loss
would fool most sub-models,
but the ensemble may remain still correct;
conversely, it is actually possible to fool an ensemble,
despite the majority of its sub-models
giving correct predictions (\Cref{fig:num_models/logits}).
% Creating an effective attack
% could thus require reweighing the importance
% of individual sub-model.

From the above observations,
it is perceivable that
the practical evaluation of ensemble robustness
cannot be solely done
by treating such models holistically.
To this end,
this paper introduces \attack,
\underline{mo}del-\underline{r}eweighing \underline{a}ttack,
to adaptively adjust the importance
of sub-models in attack iterations.
Sub-models are reweighed
according to their respective ``ease of attack'',
which is in turn evaluated
by the gradient of the difference
of ensemble classification outputs
\wrt{} the ones of individual sub-models.
Pushing the limits of the current SOTA
in ensemble robustness evaluation,
it draws inspiration
from recent effective attack tactics,
\eg, momentum~\cite{dong18momentum,croce20aa},
step size schedule~\cite{croce20aa,ye2022aaa},
% random restarts~\cite{tramer2020adaptive}
loss normalization~\cite{lafeat},
and multiple targets~\cite{croce20aa,tramer2020adaptive}.
We summarize our contributions:
\begin{itemize}
    \item This paper presents the first extensive study
    on the robustness of ensemble defenses
    under multiple ensemble-forming strategies.

    \item By reweighing the importance weights of sub-models
    to steer adversarial example synthesis,
    we show that
    gradient-based attacks on ensemble defenses
    can often be orders of magnitude faster,
    while enjoying a higher success rate.

    % To evade these obstacles,
    % we introduce how individual sub-model gradients
    % a new surrogate loss \( \attackloss \),
    % is introduced
    % which bypass the gradient obfuscation
    % posed by the original ensemble loss.

    \item
    Empirical results
    on a wide variety
    of different ensemble defenses
    show that \attack{}
    outperforms competing attacks
    in both performance and convergence rate.
    Finally, this paper
    provides extensive ablation of its components
    and sensitivity analyses of hyperparameters.
\end{itemize}

To our best knowledge,
\attack{}
is currently the strongest attack
against a wide range of ensemble defenses.
% As existing ensemble defenses
% may rely on design decisions
% that stymie even the strongest competing attacks
% we tested,
% attacks on ensemble robustness
% should not treat the model under attack
% as a holistic loss function.
We make \attack{} open source
with reproducible results and pre-trained models;
moreover,
we maintain a leaderboard of ensemble defenses
under various attack strategies.
%\footnote{%
%    \url{https://github.com/lafeat/mora}.}.
% namely averaging by probability vectors (softmax),
% majority vote (voting)
% and averaging by logits (logits).
