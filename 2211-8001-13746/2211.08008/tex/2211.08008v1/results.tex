\section{Experimental Results}\label{sec:results}

We compare \attack{} against SOTA attacks
for a wide range of ensemble defenses
under three ensemble-forming strategies
(softmax, voting, and logits).
We use pre-trained ResNet-20~\cite{resnet} models
from ADP~\cite{pang2019adp},
Dverge~\cite{yang2020dverge},
GAL~\cite{kariyappa2019gal},
and reproduced TRS~\cite{yang2021trs}
using the same architecture
with official source code,
as pre-trained models were unavailable.
Our robustness evaluation
considers the \( \linf \) white-box attacks
on the \cifarx{} test set~\cite{cifar},
with perturbation \( \epsilon = 0.01 \)
unless specified.
The full comparison results
can be found in \Cref{tab:compare:cifarx};
larger \( \epsilon \) comparisons,
and similar results on \cifarc{} models
are in \Cref{app:results}.
We provide our key observations below.
\input{tables/cifar10}

\textbf{%
    Traditional attacks %(PGD, C\&W)
    may fail to break through gradient obfuscation.}
We reproduce two traditional white-box attacks,
\ie{}, projected gradient descent (PGD)~\cite{madry18}
and C\&W~\cite{carlini17}
with 5 random restarts,
each with a maximum of 100 iterations,
giving a total of 500 iterations.
PGD uses a fixed step size of \( \epsilon/4 \).
For a fair comparison,
\attack{} with 500 iterations
sweeps \( \beta \in \braces{0, 0.25, 0.5, 0.75, 1} \),
with each \( \beta \) up to 100 iterations.
Even with a 500 iteration budget,
it is clear that PGD and C\&W
may substantially overestimate robustness,
especially when tested
under the softmax and voting ensemble-forming options,
and \attack{} can work around this obstacle
thanks to its attacks on sub-models.

\textbf{%
    Diversified gradients
    can hamper even integrated attacks with large arsenals.}
Moreover, we test the defenses
against recent integrated attacks
with SOTA baselines on robustness evaluation,
namely Adaptive Auto Attack (\aaa)~\cite{ye2022aaa},
AutoAttack (AA)~\cite{croce20aa},
and Composite Adversarial Attacks (CAA)~\cite{mao2021caa},
which comprise large arsenals of various attack strategies.
We reproduce CAA following~\cite{mao2021caa}
to search for the attack policy
before evaluating the defending models.
Note that
its computational complexity
is thus much higher than the other attacks,
but we only report its test-time complexity.
In particular,
while they enjoyed
much higher success rates than PGD and \cw,
some defenses render their tactics ineffective.
We observe, \eg, sizeable robustness overestimation
on ADP~\cite{pang2019adp} under softmax and voting,
which explicitly diversifies sub-model gradients.
As \attack{} can dynamically
re-adjust sub-model importance
\wrt{} their ``ease-of-attack'',
it performs substantially better
% than the others,
with much fewer iterations.
In addition to the earlier 500 iterations,
the multi-targeted \attackmt{}
targets the remaining 9 class labels
with 100 iterations for each label
and \( \beta \) fixed at \( 0.5 \).
Others also use multi-targeted attacks
along with respective tactics.

\textbf{%
    Robustness of most sub-models
    \vs{} robustness of ensemble.}
We find that robustness
of a majority of sub-models
(fooling \( \nicefrac38 \) for softmax
 and \( \nicefrac28 \) for logits)
usually do not translate
to the overall robustness
of the ensemble
(\Cref{fig:num_models/softmax,fig:num_models/logits}).
As voting requires breaking
\( \nicefrac12 \) sub-models simultaneously
(\Cref{fig:num_models/argmax}),
it is perceivable that using voting
may give rise to a higher overall robustness.
Yet surprisingly,
for most defending ensembles,
voting performs worse than softmax and logits.

\textbf{%
    Ensemble-forming strategies
    may give a false sense of security.}
On one hand,
softmax and voting strategies
exhibit substantially larger overestimated robustness
(up to 40\%)
than logits.
On the other hand,
in stark contrast
to the proposed use of softmax
from~\cite{
    kariyappa2019gal,pang2019adp,yang2020dverge,yang2021trs},
we find summing by logits
can form ensembles
that are notably more robust
than the other two
(\Cref{fig:epsilon/dverge}),
while attackers only needs
to successfully deceive a few sub-models
(referring back to~\Cref{fig:num_models/logits}).
% This may suggest
\input{figures/num_models}
\input{figures/convergence}

\textbf{%
    Up to \( 60\times \) faster convergence
} under 500 iterations.
\Cref{fig:results:convergence}
compares the convergence speed
of \attack{} against AA losses, \cw{}, and PGD
on defending ensembles.
\attack{}
converges substantially faster
than the other attacks,
using only up to 31 steps
to match AA losses with \( 500 \) iterations.

\textbf{%
    Ensemble defense mechanisms
    may be at odds with robustness.}
In \Cref{tab:compare:cifarx:at},
we compare respective attacks
on adversarially trained Dverge models.
To our surprise,
forming larger ensembles
is actually harmful
to the robustness of ensemble.
\input{tables/adv_ensemble}

% under adversarial training, more sub-models harms robustness
% limitation of learned attack
% Why is diverge more robust?


\textbf{%
    Additional results, ablation, and sensitivity analyses.
}
Due to the page limit,
we provide full results
of relevant figures
in \Cref{app:results},
note that the above key observations
still hold true for all ensemble defenses we test
under different ensemble-forming strategies
and \( \epsilon \) perturbation bounds.
In addition,
we provide extensive ablation study
on the design choices we made,
and sensitivity analysis
on the temperature constant \( \tau \).

% \input{figures/epsilon}
