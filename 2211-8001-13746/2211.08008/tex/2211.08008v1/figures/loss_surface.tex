\begin{figure}[ht]
    \centering%
    \newcommand{\lspgdsubfig}[2]{%
        \begin{subfigure}[b]{0.3\textwidth}
            \centering\includegraphics[
                scale=0.8, trim=0 10pt 0 15pt
            ]{loss_surface/#1_adv}%
            \caption{PGD (#2).}\label{fig:loss:pgd:#2}
        \end{subfigure}%
    }
    \newcommand{\lsmorasubfig}[2]{%
        \begin{subfigure}[b]{0.3\textwidth}
            \centering\includegraphics[
                scale=0.8, trim=0 10pt 0 0
            ]{loss_surface/#1_adv_sub_original}%
            \caption{\attack{} (#2).}\label{fig:loss:mora:#2}
        \end{subfigure}%
    }
    \hspace*{\fill}
    \lspgdsubfig{softmax}{softmax}
    \hspace*{\fill}
    \lspgdsubfig{argmax}{voting}
    \hspace*{\fill}
    \lspgdsubfig{logits}{logits}
    \hspace*{\fill}
    \\
    \hspace*{\fill}
    \lsmorasubfig{softmax}{softmax}
    \hspace*{\fill}
    \lsmorasubfig{argmax}{voting}
    \hspace*{\fill}
    \lsmorasubfig{logits}{logits}
    \hspace*{\fill}
    \caption{%
        The averaged loss surfaces
        across all samples
        that resisted PGD-10 attacks
        under all ensemble-forming modes
        of a 3 sub-model ensemble
        trained with ADP~\cite{pang2019adp}
        in the image space
        \(
            \x + \g \epsilon_\mathrm{a}
            +\g^{\bot} \epsilon_\mathrm{r}
        \).
        Here,
        \( \g \) denotes the normalized adversarial direction
        after accumulating 10 initial iterations
        of gradient updates
        at the natural input \( \x \),
        and \( \g^\bot \) is its uniformly randomized orthogonal.
        The top row uses standard PGD attacks,
        and the bottom row then
        replaces the SCE loss used in the top row
        with the \attack{} loss
        and uses \( \beta = 0.5 \).
    }\label{fig:loss_surface}
\end{figure}%
