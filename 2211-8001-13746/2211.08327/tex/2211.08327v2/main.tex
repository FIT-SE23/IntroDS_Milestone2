\PassOptionsToPackage{svgnames}{xcolor}
\documentclass[conference]{IEEEtran}
\usepackage[left=1.62cm,right=1.62cm,top=1.9cm]{geometry}


% chktex-file 10
% chktex-file 17
% chktex-file 36
% chktex-file 8
% chktex-file 1
% chktex-file 9

\usepackage[english]{babel}
\usepackage{amsmath,amssymb}
\usepackage{times}
\usepackage{relsize}
\usepackage{graphicx}
\usepackage{url}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{mparhack}
\usepackage{subfigure}
\usepackage{authblk}
\usepackage{amsthm}
\usepackage{mathrsfs}
\usepackage{pgfplots}
\pgfplotsset{width=7cm,compat=1.8}




\usepackage[noend]{algorithmic}
\usepackage[linesnumbered,ruled,vlined]{algorithm2e}
\usepackage{setspace}
\usepackage{cite}

\usepackage{footmisc}
\usepackage{array}
\usepackage{eqparbox}
\usepackage{tikz}

\usepackage{xcolor}
\usepackage{fontawesome}


%========================
\usepackage{mathtools}
\usepackage{bm}
\usepackage[all]{xy}
\usepackage{etoolbox}
\AtBeginEnvironment{figure}{\setcounter{subfigure}{0}}% Resets subfigure counter at start of figure environment
%========================

\usepackage[acronym]{glossaries}
\usepgfplotslibrary{fillbetween}

\definecolor{darkgreen}{RGB}{152.9,255.0,77.9}
\newcommand\mycommfont[1]{\footnotesize\itshape\textcolor{DarkGreen}{#1}}
\SetCommentSty{mycommfont}


\DeclarePairedDelimiter\abs{\lvert}{\rvert}%
\DeclarePairedDelimiter\norm{\lVert}{\rVert}%
%========================

\usepackage{accents}
% \newcommand\undercirc[1]{\underaccent{\overcirc}{#1}}

\newcommand\overplus[1]{\accentset{+}{#1}}
\newcommand\overminus[1]{\accentset{-}{#1}}

\newcommand\overcirc[1]{\accentset{\circ}{#1}}
\newcommand\undercirc[1]{\underaccent{\circ}{#1}}

\newtheorem{theorem}{Theorem}
\newtheorem{corollary}{Corollary}
\newtheorem{proposition}{Proposition}

% \theoremstyle{remark}
\newtheorem{lemma}{Lemma}
\newtheorem{remark}{Remark}
\newtheorem{definition}{Definition}
\newtheorem{notation}{Notation}

% \newtheorem{remark}{Remark}
% \theoremstyle{plain}
% \newtheorem{theorem}{Theorem}
% \newtheorem{definition}{Definition}
% \newtheorem{lemma}[theorem]{Lemma}
% \newtheorem{proposition}[theorem]{Proposition}
% \newtheorem{corollary}[theorem]{Corollary}
% \newtheorem{property}{Property}

% \newenvironment{proof}[1][Proof]{\begin{trivlist}
% \item[\hskip \labelsep {\bfseries #1}]}{\end{trivlist}}
% \newenvironment{definition}[1][Definition]{\begin{trivlist}
% \item[\hskip \labelsep {\bfseries #1}]}{\end{trivlist}}
% \newenvironment{example}[1][Example]{\begin{trivlist}
% \item[\hskip \labelsep {\bfseries #1}]}{\end{trivlist}}
% \newenvironment{remark}[1][Remark]{\begin{trivlist}
% \item[hskip \labelsep{\bfseries #1}]}{\end{trivlist}}
% \newenvironment{problem}[1][]{\begin{trivlist}
% \item[{\bfseries #1}]}{\end{trivlist}}

\newenvironment{noindlist}
{%
\begin{list}
{\labelitemi}{\leftmargin=0em \itemindent=2.5em}
\end{list}
}



%\newcommand{\qed}{\nobreak \ifvmode \relax \else
%      \ifdim\lastskip<1.5em \hskip-\lastskip
%      \hskip1.5em plus0em minus0.5em \fi \nobreak
%      \vrule height0.75em width0.5em depth0.25em\fi}

\newcommand{\mymarginpar}[1]{\mbox{} \marginpar{#1}
%\if@twoside \ifodd \thepage %\c@page
\marginparsep{25pt}
%\else \marginparsep 50pt \fi \fi
}
% \newcommand{\comment}[1]{\mymarginpar{Comment} \textbf{#1}}

% \newcommand\T{\rule{0pt}{2.4ex}}
% \newcommand\B{\rule[-1.0ex]{0pt}{0pt}}

%\renewcommand\Authfont{\scshape}
\renewcommand\Affilfont{\footnotesize}
\setlength{\affilsep}{0.5em}




\def\B{\mathcal{B}}
\def\R{\mathcal{R}}
\def\I{\mathcal{I}}
\def\J{\mathcal{J}}
\def\T{\mathcal{T}}
\def\C{\mathcal{C}}
\def\G{\mathcal{G}}
\def\u{\mathpzc{u}}
\def\U{\mathcal{U}}
\def\P{\mathcal{P}}
\def\N{\mathcal{N}}
\def\K{\mathcal{K}}
\def\X{\mathcal{X}}

\def\KK{\mathcal{K}^{+}}
\def\KU{\mathcal{K}^{-}}

\newacronym{csi}{CSI}{channel state information}
\newacronym{wmmse}{WMMSE}{weighted minimum mean square error}
\newacronym{sc}{SC}{synthetic control}
\newacronym{ru}{RU}{resource unit}
\newacronym{wsrm}{WSRM}{weighted sum-rate maximization}
\newacronym{ioe}{IoE}{internet of everything}
\newacronym{gnn}{GNN}{graph neural networks}
\newacronym{siso}{SISO}{single-input-single-output}
\newacronym{idi}{IDI}{interference detection and identification}
\newacronym{cca}{CCA}{clear channel assessment}
\newacronym{scm}{SCM}{structural causal model}

\newcommand*{\tran}{^{\mkern-1.5mu\mathsf{T}}} % A\tran
\newcommand*{\conj}[1]{\overline{#1}} % \conj{A}
\newcommand*{\hermconj}{^{\mathsf{H}}} % A\hermconj


\DeclareMathOperator{\argmax}{\arg\max}
\DeclareMathOperator{\argmin}{\arg\min}

\renewcommand{\vec}[1]{\mathbf{#1}}




\newcommand*\diff{\mathop{}\!\mathrm{d}}
\newcommand*\Diff[1]{\mathop{}\!\mathrm{d^#1}}
\DeclareMathOperator{\EE}{\mathbb{E}}


% \makeatletter
% \patchcmd{\@maketitle}
%   {\addvspace{0.5\baselineskip}\egroup}
%   {\addvspace{-1.45\baselineskip}\egroup}
%   {}
%   {}
% \makeatother

\title{Weighted Sum-Rate Maximization With Causal Inference for Latent Interference Estimation}


\author[ ]{Lei You}
\affil[ ]{\small Department of Engineering Technology, Technical University of Denmark, Denmark}
\affil[ ]{\text{leiyo@dtu.dk}}


\begin{document}

\maketitle
\thispagestyle{plain}
\pagestyle{plain}

\begin{abstract}
The paper investigates the \gls{wsrm} problem with latent interfering sources outside the known network, whose power allocation policy is hidden from and uncontrollable to optimization. The paper extends the famous alternate optimization algorithm \gls{wmmse} \cite{christensen2008weighted} under a causal inference framework to tackle with \gls{wsrm}. Specifically, 
with the possibility of power policy shifting in the hidden network, 
computing an iterating direction based only on the observed interference inherently implies that counterfactual is ignored in decision making. A method called \gls{sc} is used to estimate the counterfactual. For any link in the known network, \gls{sc} constructs a convex combination of the interference on other links and uses it as an estimate for the counterfactual. Power iteration in the proposed SC-WMMSE is performed taking into account both the observed interference and its counterfactual. SC-WMMSE requires no more information than the original WMMSE in the optimization stage. To our best knowledge, this is the first paper explores the potential of \gls{sc} in assisting mathematical optimization in addressing classic wireless optimization problems. Numerical results suggest the superiority of the SC-WMMSE over the original in both convergence and objective.
\end{abstract}
\glsresetall

\section{Introduction}
\label{sec:intro}


The wireless network evolution has been driven by a need for consistently higher rates over the past decades, along with the emergence of the \gls{ioe}, which aims to serve as a platform for connecting processes, people and data~\cite{saad2019vision}. The requirement for high spectral efficiency in \gls{ioe} scenarios calls for a need to revisit classic wireless optimization problems under highly dynamic environments. The \gls{wsrm} problem \cite{baligh2014cross} is one of these many, solving which plays a key role in determining the effective capacity of a wireless channel. 
Finding the global maximum of \gls{wsrm} is generally considered to be an $\mathcal{NP}$-hard problem~\cite{baligh2014cross}.

As a result, significant research efforts have been devoted to developing high-quality sub-optimal solutions for the problem. The \gls{wmmse} algorithm, first proposed in~\cite{christensen2008weighted}, is one such solution which has been shown to be an efficient algorithmic framework for many cross-layer transmission tasks \cite{baligh2014cross}. Therefore, this algorithmic framework has been significantly extended by many other literature and the research exploring remains still active \cite{schmidt2009minimum, shi2011iteratively, shin2012weighted, cirik2015weighted, aquilina2017weighted, guo2020weighted}. Additionally, the \gls{wsrm} problem has also been addressed by data-driven methodologies--more specifically--\gls{gnn} based methods \cite{eisen2020optimal,naderializadeh2020wireless,chowdhury2021unfolding, nikoloska2021fast}. The basic idea is to use a \gls{gnn} to encode the network topology information into a \gls{gnn} that maps \gls{csi} to power control policy. Unsupervised model training is also possible by in-cooperating learning in the processing of a primal-dual algorithm \cite{eisen2020optimal}. 

The \gls{siso} case of~\cite{schmidt2009minimum, shi2011iteratively, shin2012weighted, cirik2015weighted, aquilina2017weighted, guo2020weighted,eisen2020optimal,naderializadeh2020wireless,chowdhury2021unfolding, nikoloska2021fast} and many other research summarized in \cite{weeraddana2012weighted} falls into a special case of the problem investigated in this paper, i.e. when there are no latent interfering links whose power allocation is unknown and uncontrollable. It's important to note that \textit{the latent interfering links cannot be treated simply as noise}, because the power allocation of the latent links may be dependent on those of the known links, or, they may evolve over time. Treating it as noise omits these facts. 

The paper examines the \gls{wmmse} optimization mechanism from a causal inference's perspective. \gls{wmmse} algorithm and other similar methods can suffer performance loss in this scenario because of the influence of these latent factors on the iterating directions towards the ground-truth optimality. To overcome this challenge, the paper proposes the use of the \gls{sc} method \cite{abadie2003economic}, which uses counterfactual reasoning to infer the causal relationship between optimization variables and the objective. The resulting algorithm requires no additional information beyond that used in the traditional \gls{wmmse} algorithm, yet it demonstrates improved convergence and objective value, as well as resilience to emerging and disappearing latent interference sources. The trick in this paper hence applies to most of its previous extensions \cite{baligh2014cross,schmidt2009minimum, shi2011iteratively, shin2012weighted, cirik2015weighted, aquilina2017weighted, guo2020weighted}. Numerically, the proposed SC-WMMSE shows advantage over its origin on both convergence and objective value as well as demonstrates resilience to emerging and disappearing of latent interference sources.



\begin{figure*}[!t]
\begin{equation}
R_k = \log\bigg(1+\frac{|h_{k,k}|^2 p_k}{\sum_{j\in\KK}|h_{j,k}|^2p_j +\sum_{i\in\KU}|h_{i,k}|^2q_i+ \sigma^2_k }\bigg)\quad k\in\KK
\label{eq:KK_rate}
\end{equation}
\end{figure*}

\section{Model and Problem}



Consider a wireless network consisting of multiple communications links. The set of all links is denoted by $\K$, and for any link $k$ ($k\in\K$), the set of links that interfere with $k$ is denoted by $\I_k$, where $\I_k\subset \K$. The channel gain between the transmitter and receiver of link $k$ is denoted by $h_{k,k}$, and the channel gain from the transmitter of link $k$ to the receiver of link $j$ is denoted by $h_{k,j}$ ($k\neq j$). It is assumed that $|h_{k,j}|=0$ for any $j\notin \I_k$ ($j\neq k$), and the channel matrix is denoted by $\vec{H}$. The power of the transmitter of link $k$ is denoted by $p_k$, and the noise power is denoted by $\sigma_k^2$. 


% \subsection{Preliminaries}

% The \textit{classic} weighted sum-rate maximization problem is formulated in~\eqref{eq:opt_naive} below.
% \begin{subequations}
% \begin{align}
%                  & \max_{\vec{p}} \sum_{k=1}^{K}\alpha_k \mathbb{E}_{\vec{H}}\bigg[\log\left(1+\frac{|h_{k,k}|^2 p_k}{\sum_{j\in\K}|h_{j,k}|^2p_j + \sigma^2_k }\right)\bigg] \\
% \text{s.t.}\quad & 0\leq p_{k}\leq p_k^{\max},~k=1,2,\ldots K
% \end{align}
% \label{eq:opt_naive}
% \end{subequations} 

The paper considers a scenario where there are latent interference sources whose power distribution and allocation is not known a priori and is neither observable nor controllable. The known network is denoted by $\KK$, and the unknown network is denoted by $\KU$, with $\KK\cup\KU = \K$ and $\KK\cap\KU=\phi$. Let $K = |\KK|$. The two networks mutually affect each other through interference. No assumptions are made about the power policies of $\KU$. The notation $\vec{q}$ is used to represent the power allocation in $\KU$, which can proactively or reactively change over time and may be dependent on $\vec{p}$. The problem is defined in \eqref{eq:opt}, with $R_k$ defined in equation~\eqref{eq:KK_rate}. The goal is to find a power allocation $p_1,p_2,\ldots p_K$ for all the corresponding transmitters of the $K$ links in the known network such that the weighted sum rate $\sum_{k}\alpha_k R_k$ of the known network is maximized.

% The problem in \eqref{eq:opt_naive} is generalized to~\eqref{eq:opt} below. Remark that the formulation~\eqref{eq:opt_naive} is a special case of~\eqref{eq:opt}. Setting $\KU=\phi$ reduces the inner statistical expectation such that the objective function degenerates to the one in~\eqref{eq:opt_naive}. 
\begin{subequations}
\begin{align}
                 & \max_{\vec{p}} \sum_{k\in\KK}\alpha_k\mathbb{E}_{\vec{H}}\bigg[\mathbb{E}_{\vec{q}|\vec{H},\vec{p}}\big[ R_k\big] \bigg]\\
\text{s.t.}\quad & 0\leq p_{k}\leq p_k^{\max},~k\in\KK
\end{align}
\label{eq:opt}
\end{subequations} 


% Inspecting the formulation, the inner mathematical expectation $\mathbb{E}_{\vec{q}|\vec{H},\vec{p}}$ imposes that the power allocation $\vec{q}$ is conditional on the power allocation $\vec{p}$. First, it means $\KK$ needs to be optimized by taking into account the power allocation of $\KU$, though the allocation policy of $\KU$ is not known. Additionally, one cannot expect to sampling on the environment to learn a conditional distribution $\P(\vec{q}|\vec{H})$ to ease the optimization. This is because $\vec{q}$ is conditionally distributed on both $\vec{p}$ and $\vec{H}$, such that any change of $\vec{p}$ from the optimization would lead to a  distribution drifting on $\P(\vec{q}|\vec{H})$. 

% Note that the widely investigated distributed optimization techniques for solving~\eqref{eq:opt_naive} do not generalize to solving~\eqref{eq:opt}. In those research, though the power can be optimized in a distributed manner and gets updated asynchronously in each sub-network, it is required that each sub-network is aware of the existing of others and all sub-networks are subject to the same power optimization policy. Essentially, it means that the power allocations of all sub-networks needs to follow the same optimization policy. Sub-networks targeting at a different objective or employing various power allocation polices cannot be put into those optimization frameworks.







\section{Sum Rate Maximization with Causal Inference}
\label{sec:sum-rate}

% This section illustrates how \gls{wmmse} may go biased from the optimization target in its inherent optimization mechanism. Then, the how a causal estimator differs with a regression estimator is discussed in the scope of extending \gls{wmmse} with interference estimation for solving~\eqref{eq:opt}. Next, \gls{sc} is introduced to establish a causal estimator that can be trained offline in low cost and Incorporated to \gls{wmmse}, followed by the algorithm design of SC-WMMSE.


\subsection{How Latent Interference Affects \gls{wmmse}}
\label{subsec:wmmse}

It is shown that~\eqref{eq:opt} submits to a reformulation as a weighted sum-mean-square-error minimization problem \cite{schmidt2009minimum,shi2011iteratively} when  the power allocation vector of the unknown network, $\vec{q}$, is fixed as constants. When $\vec{q}$ is fixed, the mathematical expectation on $\vec{q}$ is removed from the problem.

To make this reformulation, the noise plus interference term from the unknown network $\KU$ is replaced by a new variable $\eta_k$. 
Denote 
$\eta_{k}=\sum_{i\in\KU}|h_{i,k}|^2q_i + \sigma^2_k$.
This replacement can be made in the denominator of the rate expression in \eqref{eq:KK_rate}, and the problem in \eqref{eq:opt} can be rewritten as \eqref{eq:opt_naive_reform} without loss of optimality \cite{schmidt2009minimum,shi2011iteratively}.

\begin{subequations}
\begin{align}
                 & \min_{\vec{w}, \vec{u}, \vec{v}} \mathbb{E}_{\vec{H}}\bigg[ \sum_{k\in\KK} \alpha_k(w_{k}e_k - \log w_k)\bigg] \\
\text{s.t.}  \quad& \lvert v_{k}\rvert^2 \leq p^{\max}_k,~k\in\KK
\end{align}
\label{eq:opt_naive_reform}
\end{subequations} 

The reformulation of the problem in~\eqref{eq:opt} to a weighted sum-mean-square-error minimization problem utilizes two additional variables: $w_k$ and $e_k$, where $w_k$ is a positive weight variable, and $e_k$ is the mean-square error, defined as:
\begin{equation}
e_k = |u_k h_{k,k}, v_k - 1|^2 + \sum_{j\neq k}|u_j h_{j,k}v_k|^2 +\eta_{k}|u_k|^2
\end{equation}
The variable $p_k$ in the original formulation is replaced by $v_k$, with $p_k=|v_k|^2$ ($k\in\KK$).

The \gls{wmmse} algorithm is designed to solve the reformulation in~\eqref{eq:opt_naive_reform} using the theory of alternate optimization. The algorithm is illustrated in Algorithm~\ref{alg:wmmse}. It should be noted that there also exists a stochastic version of the \gls{wmmse} algorithm that can be used when the channel matrix, $\vec{H}$, is a random variable. The proposed methodology in this paper can also be applied to the stochastic version of the \gls{wmmse} algorithm. In practice, the variable $\eta_k$ can be approximated using techniques such as \gls{cca} \cite{yang2011wireless}.


% under the classical theory of alternate optimization to solve the formulation~\eqref{eq:opt_naive_reform}, illustrated in Algorithm~\ref{alg:wmmse}\footnote{Remark that \gls{wmmse} has its stochastic version, which is designed to deal with the case that $\vec{H}$ is a random variable, e.g. \eqref{eq:opt_naive_reform}. The difference to the deterministic version is that the numerator and denominator of $v_k$ is accumulated respectively over iterations. The variable $v_k$ gets updated by the accumulated values rather than the single-sample values in each iteration. The proposed methodology in this paper generalizes straightforwardly to the stochastic version of \gls{wmmse}\label{footnote:stochastic_wmmse}}. Practically, $\eta_{k}$ needs to be obtained in approximation by \gls{idi} techniques such as \gls{cca} \cite{yang2011wireless}.

\input{alg_wmmse.tex}

The term $e_k$ is a convex quadratic function in $\vec{u}$ and $\vec{v}$, which have closed-form solutions.
However, when $\vec{q}$ is a latent random variable, it can change throughout the optimization process and affect the sampling of $\eta_k$, making the optimization process challenging. This is because the collected samples of $\eta_k$ may not guarantee that $u_k$ will move towards the optimum of the quadratic function $e_k$ when $\vec{w}$ is fixed. This problem can be mitigated if $\eta_k$ is i.i.d., yet such assumption is too strong to be realistic. 


\subsection{Causal Inference Estimator vs. Regression Estimator}
\label{subsec:causal_vs_reg}
\label{subsec:opt-formulation}
The key obstacle in applying the \gls{wmmse} algorithm is that the interference at any receiver $k$ ($k\in\KK$) is difficult to handle from an optimization perspective, due to its dependency on $\vec{q}$. To tackle this obstacle, instead of considering only $\eta_{k}$, the entire denominator of $u_k$, denoted by $I_{k}$, is considered. This allows for better generalization of the proposed inference methodology, as it can estimate the denominator as a whole for any link $k$ if the \gls{csi} of another link $j$ ($j\neq k$, $j\in\KK$) is unknown or outdated. In the $k_{\text{th}}$ loop of Algorithm~\ref{alg:wmmse}, the power allocation $p_k$ is the variable to be optimized and the other power $\vec{p}_{-k}$ are fixed. The mathematical expectation of $I_k$ conditional on the optimization variable $p_k$ is as follows.
\begin{equation*}
\begin{split}
   \MoveEqLeft  \mathbb{E}[I_{k} | p_k] \\ 
    \!\!={}& \mathbb{E}_{\vec{H}}\mathbb{E}_{\vec{q}|\vec{H},p_k}\bigg[\sum_{j\in\KK}\!\!|h_{j,k}|^2p_j + \sum_{i\in\KU}\!\!|h_{i,k}|^2q_i + \sigma_k^2\bigg]  \\
    \!\!={}& \mathbb{E}_{\vec{H}}\bigg[\!\sum_{j\in\KK}\!\!|h_{j,k}|^2p_j\bigg] + \underbrace{\mathbb{E}_{\vec{H}}\mathbb{E}_{\vec{q}|\vec{H},p_k}\bigg[\!\sum_{i\in\KU}\!\!\!|h_{i,k}|^2q_i+\sigma_k^2\bigg]}_{\eta_{k}}
\end{split}
\end{equation*}

Estimating the second part of $I_k$, $\eta_k$, is difficult as it depends on the power allocation of the latent interfering sources, which is unknown and may change over time. Supervised machine learning models are limited for this task as the distribution of $\vec{q}$ is not i.i.d and may change due to changes in the power allocation of the latent sources or new transmitters/receivers joining or leaving the network. A more suitable approach is to leverage causal inference methods, specifically the \gls{scm} approach, to inexplicitly estimate the counterfactual distribution of $\vec{q}$ given the observed interference $I_k$. This approach allows for estimation of the interference distribution under different power allocation policies, which can then be used to infer the causality relationship between $p_k$ and $I_k$. 

 A causal inference estimator  targets $\mathbb{E}[I_k|do(p_k)]$, which takes both the actual outcome and potential outcomes into consideration, rather than just the former. Specifically, $\mathbb{E}[I_k|do(p_k)]$ represents the expected value of the interference $I_k$ at receiver $k$ when we intervene and set the power allocation of link $k$ to $p_k$, and consider all possible power policies of the latent interfering links $\KU$. On the other hand, $\mathbb{E}[I_k|p_k]$ represents the expected value of the interference $I_k$ at receiver $k$ when the power allocation of link $k$ is $p_k$, but the power policies of the latent interfering links $\KU$ are not intervened, and are instead determined by the current distribution of $\vec{q}$ given $\vec{H}$ and $\vec{p}$. $\mathbb{E}[I_k|do(p_k)]$ better aligns with the requirements of optimization.
 
For $\mathbb{E}[I_k|do(p_k)]$ and $\mathbb{E}[I_k|p_k]$ to be equal, the power allocation $p_k$ is independent of all the other interfering links and their power allocations in the network, i.e., there are no confounding variables. In this case, conditioning on $p_k$ would not change the distribution of $I_k$ and the expectation would be the same whether we condition on $p_k$ or intervene on it. This implies that power allocation in $\KK$ is being performed randomly, rather than through a specific optimization algorithm.


\subsection{Estimating Interference with \gls{sc} Methods}
\label{subsec:sc}


\input{fig_convex_hull.tex}


The synthetic control method (SC), first proposed in \cite{abadie2003economic}, is a powerful technique for estimating the effects of large-scale interventions \cite{shi2022assumptions}. It has been shown to be competitive with other fixed matching estimators \cite{chen2022synthetic}. SC approximates the counterfactual outcomes of one unit by constructing a weighted combination of the observed outcomes of other units. It works with panel data, which contains multiple observations for each unit over time. While SC is often used in the context of binary treatment and no interference between units, it can also be applied in more complex settings with units interfering each others, such as in \cite{agarwal2022network}, where the potential outcome for unit $k\in\KK$ is linear to latent factors, with noise in the factor model being additive, zero mean, and independent.

In the training stage, consider observations of $I_1,I_2\ldots I_k$ in format of panel data, i.e.
\begin{equation*}
\vec{X}=
\begin{bmatrix}
I_1^{(0)} & I_2^{(0)} & \cdots  & I^{(0)}_{K}  \\
I_1^{(1)} & I_2^{(1)} & \cdots  & I^{(1)}_{K}  \\
 \vdots & \vdots  & \ddots & \vdots      \\
I_1^{(L)} & I_2^{(L)} & \cdots  & I^{(L)}_{K}  
\end{bmatrix} \in \mathbb{R}^{L\times K}
\end{equation*}
where each row $\ell$ ($1\leq\ell\leq L$) is an observation across all the $K$ units. For any $k\in\KK$, denote by $\vec{x}_k$ the $k_{\text{th}}$ column of $X$. Denote by $\vec{X}_{-k}$ the matrix without column $k$. The \gls{sc} estimator is trained by solving the constrained optimization problem~\eqref{eq:sc} as follows.
\begin{subequations}
\begin{align}
                 & \bm{\nu}_k = \arg\min_{\bm{\beta}\geq\vec{0}} \| \vec{x}_{k} - \vec{X}_{-k}\bm{\beta}\| \label{eq:sc-obj} \\
\text{s.t.}  \quad& \sum_{i} \beta_i = 1   \quad i=1,2\ldots K-1 \label{eq:sc-constr}
\end{align}
\label{eq:sc}
\end{subequations} 


Solving \eqref{eq:sc} yields a vector $\bm{\nu}_{k}$ ($k\in\KK$), which is essentially a group of coefficients that can be used to construct a linear combination of $I_j$ ($j\in\KK\backslash {k}$). The objective function~\eqref{eq:sc-obj} suggests that the computed coefficients lead to an as small as possible mean-squared-error over all the $L$ observations of the unit $k$ and respectively its constructed linear combinations. Note that \eqref{eq:sc-constr} imposes a hard constraint on the coefficients such that the obtained linear combination is guaranteed to be a convex combination. In other words, solving the formulation~\eqref{eq:sc} trains a (constrained) linear regression model between the observed interference of unit $k$ and those of the others. In this context, a convex combination has better interpretability. Namely, in terms of interference, it finds a function that accurately estimates the values of the counterfactual interference in the range of the observed interference, as opposed to extrapolation. See \figurename~\ref{fig: constrained_lr} for an illustration. Section~\ref{sec:simulation} demonstrates the necessity of constraint~\eqref{eq:sc-constr} by showing numerically that it helps improve optimization significantly.

In the inference stage, the synthetic control method is used to estimate the counterfactual outcome for unit $k$ under treatment $p_k$. This is done by averaging the outcomes of the synthetic control units ($\KK\backslash\{k\}$), weighted by the coefficients obtained in the training stage. The estimation is denoted as $\hat{I}_k = \bm{\nu_}k^{\intercal}\bm{\mu}_k$ for all $k\in\KK$, where $\bm{\mu}_{k}$ denotes the observations for all units other than $k$, i.e. any $I_j$ ($j\neq k$). 

\subsection{Algorithm Design}
\label{subsec:sc-wmmse}


\input{fig_convergence.tex}


The algorithm SC-WMMSE is designed straightforwardly by incorporating the ideas discussed in Sections~\ref{subsec:wmmse}, \ref{subsec:causal_vs_reg}, and \ref{subsec:sc}. It is presented in Algorithm~\ref{alg:sc-wmmse}.

\input{alg_sc_wmmse.tex}

The algorithm SC-WMMSE combines the \gls{wmmse} algorithm with \gls{sc} method, which is designed to estimate the counterfactual outcome of a unit under a certain treatment. The algorithm is trained offline using past observations of $I_1,I_2,\ldots I_K$ and $K$ \gls{sc} estimators are trained. During optimization, $I_1,I_2,\ldots I_K$ are observed in every iteration and the \gls{sc} estimators are used to update $u_k$ with a probability $\varepsilon$, otherwise, the update follows the same rule as WMMSE. The parameter $\varepsilon$ is set to decay based on the number of iterations for the convergence of the algorithm\footnote{In the implementation of this paper, the formula $\varepsilon(t)=[a(1-t/t^{\max})]^{b}$ is used, where $t$ is the iteration index and $a$, $b$ are hyper-parameters. As for this paper, the setting $a=0.2$ and $b=2$ stays unchanged throughout all simulations in Section~\ref{sec:simulation}.}. The effectiveness of the algorithm is demonstrated in the simulation section of the paper. 

\subsection{Intuition Behind}
From a causal inference perspective, the optimization process of \gls{wmmse} makes treatment decision $p_k$ by its knowledge of how such an intervene would affect the outcome $I_k$ at each loop $k$. The covariates are $\vec{q}$, $\vec{H}$ (which are related to network topology and density), and $\vec{p}_{-k}$. The \gls{sc} method is used to estimate the impact of this treatment by comparing the treated group ($\{k\}$) to a \gls{sc} group, which is created from a weighted combination of untreated groups ($\KK\backslash\{k\}$).

Ideally, the \gls{sc} group ought to be selected carefully to closely mimic the characteristics of the treated group before the treatment is applied. Optimizing the selection of the \gls{sc} group is out of the scope of this paper. The \gls{sc} method allows us to estimate what would have happened to the link $k$ if $p_k$ had not been applied, making it a counterfactual analysis. In Algorithm~\ref{alg:sc-wmmse}, line~\ref{alg:sc-wmmse-estimate} can be viewed as a "counterfactual update" and line~\ref{alg:sc-wmmse-observed} as a regular update. The optimization variable $u_k$ is therefore updated using both the observation and the counterfactual, allowing us to address cases where some observed $I_k$ are not statistically caused by the treatment $p_k$, but rather by other pre-treatment characteristics.


It is worth noting that there is a fundamental difference between the power allocation $\vec{p}_{-k}$ and the other covariates $\vec{H}, \vec{q}$, even though they are all treated as covariates in the $k_{\text{th}}$ loop. The power allocation on links other than $k$ can be intervened and controlled by the optimizer, while the others cannot. The proper intervention in the optimization process remains an open area of study.


\input{fig_distribution_drift.tex}

\section{Simulation}
\label{sec:simulation}
The simulation setups are as follows: multiple transmitters are randomly and uniformly distributed within a circle with a radius of $200$ meters. For each transmitter, multiple target receivers are randomly and uniformly distributed within a circle with a radius of $25$ meters. The path-loss model follows the \textit{InH-Shopping Malls-NLOS dual slope} model in \cite{haneda2016indoor}, which takes into account the the probability of line-of-sight shadow fading and blockage. The model applies across 0.5-100 GHz band and 60 GHz is selected for the simulations in this paper. A flat channel is considered and the total bandwidth is $80$ MHz. The maximum power on a \gls{ru} is set to $200$ mW uniformly for all links.

A random matrix $\vec{Z}$ is generated following a uniform distribution $\U(-1,1)$ unless specified otherwise. The power $\vec{q}$ is obtained by the linear transformation $\vec{q}=\vec{Z}\vec{p}$ with randomness and is always capped by the maximum power limit after the transformation\footnote{Note that $\vec{p}$ in the optimization process depends on the iteration step (and hence time), making $\vec{q}$ is actually evolving with time too.}. Each figure in this section is based on $50$ independent simulations to ensure that the results are statistically significant. In every simulation, the network topology is regenerated by following the rules stated above. Additionally, the random matrix $\vec{Z}$, if used, is refreshed in each simulation, for both the training and inference stages.

The code of the simulation is available on \faGithub \cite{papercode}.


\subsection{Objective Performance and Convergence}


This subsection aims to evaluate the effectiveness of the proposed algorithm, SC-WMMSE, in optimizing the objective function in~\eqref{eq:opt} and its performance in terms of convergence. The original \gls{wmmse} algorithm is used as a baseline for comparison, as well as a version of \gls{wmmse} that only uses local network information for optimization, referred to as \gls{wmmse} (Local). In the local version, the term $\eta_{k}$ in Algorithm~\ref{alg:wmmse} is set to $\sigma^2_k$, effectively ignoring all changes in $\KU$. This is used to gauge the impact of $\KU$ on $\KK$ and to determine if such impact is too small to expect SC-WMMSE having an effect.

The results are shown in \figurename~\ref{fig:convergence} with $|\KK|=50$ and $|\KU|$ ranging from $20$ to $50$. SC-WMMSE consistently outperforms the baseline in terms of objective function maximization. Particularly, when the transmissions in the latent network $\KU$ are dense, the algorithm demonstrates significantly better performance in both objective maximization and convergence. On the other hand, when the transmissions in $\KU$ are sparse, \gls{wmmse} (Local) performs similarly to the original \gls{wmmse}. This suggests that the impact of $\KU$ on $\KK$ is low, and the term $\eta_{k}$ can be well approximated by $\sigma_k^2$. As a result, the update in line~\ref{alg:sc-wmmse-estimate} becomes less significant in the optimization process.

\subsection{Robustness}

The robustness of SC-WMMSE in dynamic network changes is evaluated in this section. The algorithm's ability to adapt to emergence or disappearance of interference sources in $\KU$ is in question. \figurename~\ref{fig:zero-zero} is used as a baseline scenario with no latent interference links. SC-WMMSE is not expected to perform better than the others since there is no latent interference. The algorithm's performance is on par with the others in terms of objective value at convergence, but the convergence is slower due to added noise via the convex hull approximation $\hat{I}_k$ ($k\in\KK$). \figurename~\ref{fig:zero-dependent} illustrates SC-WMMSE's robustness in dealing with changes in the network. Even though there is no latent interference present during training, the causal estimator can still make impact in the optimization stage, resulting in SC-WMMSE  outperforming the other two in terms of both objective value and convergence. However, the performance is not as good as seen in \figurename~\ref{fig:dep-dep-k50} due to the more significant data distribution change between training and testing.

\subsection{Scalability}
\input{fig_scalability.tex}


The scalability of the algorithm is evaluated by analyzing its performance on per-link throughput as the density of transmissions in the optimized network, $\KK$, increases. Results show that when the transmissions in $\KK$ are sparse, both \gls{wmmse} and SC-WMMSE perform similarly. However, as the number of links in $\KK$ increases, the advantage of SC-WMMSE becomes more pronounced, until reaching a threshold around 150 links. Beyond this point, the advantage of SC-WMMSE starts to decrease, possibly due to the inherent limitations of the optimization mechanism used by all three algorithms when dealing with large scale problems. It is also worth noting that the gap between \gls{wmmse} and its local version also shrinks with an increase in the number of links in $\KK$. Overall, SC-WMMSE demonstrates better performance than its baseline \gls{wmmse} in all scenarios in \figurename~\ref{fig:scalability}.



\subsection{Necessity of the Convexity Constraint}
\input{fig_sc_comparison.tex}

The effectiveness of the convex combination constraint in~\eqref{eq:sc-constr} for optimization is demonstrated in \figurename~\ref{fig:necessity_of_convexity}. On top of the synthetic control method used in this paper (the method of solving~\eqref{eq:sc}, referred to as \gls{sc} (Conv)), several other methods are compared, including, its unconstrained version (\gls{sc} (Free)). Additionally,  the center point of the convex hull can be used as the estimate (\gls{sc} (Center)). One could also generate a Dirichlet distribution for the coefficients (\gls{sc} (Dirich)). These methods are used in place of the convex combination constraint in Algorithm~\ref{alg:sc-wmmse} to compare with the baseline \gls{wmmse}.

\gls{sc} (Free) performs similarly to the baseline \gls{wmmse}, which suggests that relaxing the constraint leads to an overfitting to observation, neglecting the counterfactual. Both \gls{sc} (Center) and \gls{sc} (Dirich) perform worse than the baseline, indicating that the inference task is not trivial and that \gls{sc} (Conv) is by-far an effective method for obtaining the coefficients for optimization.


\section{Conclusion and Discussion}

This paper has showcased the potential of using causal inference to assist in solving optimization problems of wireless communications. The proposed SC-WMMSE demonstrates how a causal inference framework can address the challenges posed by confounding variables in optimization. As future work, further analysis and theoretical proofs of the algorithm's convergence are suggested. Additionally, the research in causal inference has typically been based on the assumption of no inter-unit interference, which is not applicable in wireless communications. Therefore, it would be beneficial to develop causal factor models that are better suited for this field.


\bibliographystyle{IEEEtran}
\bibliography{references}


\end{document}
