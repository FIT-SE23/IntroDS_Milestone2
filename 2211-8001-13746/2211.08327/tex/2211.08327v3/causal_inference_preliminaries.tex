\section{Causal Inference Preliminaries}
\begin{table*}[!ht]
\centering
\caption{Simpson's paradox by observed data of graduate school admissions in UC Berkeley, 1971 Fall.}
\label{tab:simpson}
\begin{tabular}{lllllll}
\hline
\multirow{2}{*}{} & \multicolumn{2}{c}{All}               & \multicolumn{2}{c}{Men}               & \multicolumn{2}{c}{Women}                           \\
\hline
                  & Applicants & Admitted                 & Applicants & Admitted                 & Applicants               & Admitted                 \\
\hline
Total             & 12,763     & \multicolumn{1}{r}{41\%} & 8,442      & \multicolumn{1}{r}{\textbf{44\%}} & \multicolumn{1}{r}{4321} & \multicolumn{1}{r}{35\%} \\
\hline
\end{tabular}
\end{table*}




Causal inference is the process of determining whether an observed association truly reflects a cause-and-effect relationship. Towards this goal, methodologies and theories are built upon a new calculus. Different from the statistical inference that uses statistical methods to characterize the association between variables, causal inference needs not only to characterize an association but also determines whether a variable on one-side of the association possesses causality to the one on the other side, or vice versa. Nevertheless, both domains overlap with each other significantly such that familiar statistical notations and concepts are also widely used in causal inference. In this section, we introduce several key concepts of causal inference that are highly relevant to this paper.



\textit{Causation}, or causality, refers to the fact that one variable is the cause of the other, in other words, action A causes outcome B. An important note is that association does not necessarily imply causation. The famous Simpson's paradox, sometimes referred to as reversal paradox, describes the phenomenon that a trend in groups of data reverses in when the groups are combined. Such phenomenon can be straightforwardly used to ease the understanding the difference between association and causation. One of the best-known examples of Simpson's paradox comes from a study of gender bias among graduate school admissions to University of California, Berkeley. The data of admission for the fall of 1973 showed that among the applying candidates, men are more likely to be admitted than women, see \tablename~\ref{tab:simpson}. The truth behind is that departments being applied to possess different difficulties to get into, whereas men tended to apply to less competitive departments than women. By observing the data, admission rate and gender are obviously as well as in fact correlated, yet gender was by no means the cause of the admission rates, which was well supported by the data grouped by departments.

\textit{Confounding variables}: A confounding variable is an unmeasured third variable that influences both the exposure variable and the outcome variable. In the Simpson's paradox case, the exposure variable and the outcome variable are number of applications in genders and the admission rates in genders, respectively. The confounding variable is the difficulties of succeeding an admission in the departments, which co-affect department-wisely both the number of applications in genders and the admission rates. The existence of confounders is an important quantitative explanation why correlation does not imply causation.



\textit{Potential Outcomes and Causal Effects}: These concepts might be out of the scope of traditional statistics. Yet it is one of the key concepts in causal inference and its relevant research domains. To explain these two concepts, we follow the tradition of causal research notations: Denote the treatment, covariance, and the outcome respectively by $T$, $X$, and $Y$. The potential outcomes of $T=0$ and $T=1$ are denoted by $Y(0)$ and $Y(1)$. The treatment effect of a unit $i$ is 
\begin{equation}
    \tau_i = Y_i(1) - Y_i(0)
\label{eq:tau_i}
\end{equation}
Remark that though we don't have a timeline subscript on $Y_i$, the potential outcome of a unit is essentially time-dependent. Namely, $Y_i(T=t)$ ($t\in\{0, 1\}$) refers to the outcome that could potentially happen due to treatment $t$ being imposed to unit $i$ at  an instant moment. 


\textit{Counterfactual}: For a given unit, it is impossible to observe all potential outcomes. 
% Consider writing this paper as an experiment. One would like to find out if writing a poetry in Section~1 would lead to the paper accepted or not. The unit in this experiment is the paper, where the treatment is writing the poetry and the potential outcome is the acceptance/rejection letter. 
One could not observe the potential outcomes of both the two treatment actions unless one could travel reversely in time. The potential outcomes being not observed are referred to as counterfactuals, since they are counter to the fact (what happens in reality). 
% In the case above, the outcome of writing a poetry in the paper is the counterfactual, of which there is no chance to observe in reality.

\textit{Causal Estimator}: A causal estimator estimates the expected potential outcome $\mathbb{E}[Y|do(T=t)]$, where the \textit{do}-operator is used to differentiate the intervention distribution of the potential outcome $\P(Y|do(T=t))$ with the observational distribution $\P(Y|T=t)$. The key difference is that the \textit{do}-operator excludes the influence from all confounding variables (if existing). One cannot directly conclude
$\P(Y|do(T=t)) \neq \P(Y|T=t)$, so a regression between $Y$ and $T$ does not necessarily capture a causality (if existing) of $T$ on $Y$. 
