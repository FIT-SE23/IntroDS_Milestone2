
@article{alvarez-rodriguezElevatingMeaningData2019,
  title = {Elevating the Meaning of Data and Operations within the Development Lifecycle through an Interoperable Toolchain},
  author = {{Alvarez-Rodr{\'i}guez}, Jose Mar{\'i}a and Zu{\~n}iga, Roy Mendieta and Llorens, Juan},
  year = {2019},
  journal = {INCOSE International Symposium},
  volume = {29},
  number = {1},
  pages = {1053--1071},
  issn = {2334-5837},
  doi = {10.1002/j.2334-5837.2019.00652.x},
  abstract = {The use of different engineering methods and tools is a common practice to cover all stages in the systems development lifecycle, generating a very good number of system artifacts. Moreover, these artifacts are commonly encoded in different formats and can only be accessed, in most cases, through proprietary and non-standard protocols. In this context, the OSLC (Open Services for Lifecycle Collaboration) initiative pursues the creation of public specifications (data shapes) to exchange any artifact generated during the development lifecycle. In this paper, authors extend and apply the OSLC KM (Knowledge Management) specification as a solution with a two-folded objective: 1) representation of any kind of system artifact and 2) extension of OSLC mechanisms to support the notion of delegated operation. In this manner, it is possible to enhance the exchange of data items and to reuse existing operations within the toolchain as it is demonstrated through a case study.},
  langid = {english}
}

@misc{amsdenOSLCAutomationVersion2021,
  type = {{{OASIS Project Specification Draft}} 01},
  title = {{{OSLC Automation Version}} 2.1 {{Part}} 1: {{Specification}}},
  author = {Amsden, Jim and Ribeiro, Fabio},
  year = {2021},
  month = jan
}

@misc{amsdenOSLCChangeManagement2020,
  title = {{{OSLC Change Management Version}} 3.0},
  author = {Amsden, Jim},
  year = {2020},
  month = sep,
  abstract = {Defines the OSLC Change Management domain, a RESTful web services interface for the management of product change requests, activities, tasks and relationships between those and related resources such as requirements, test cases, or architectural resources. To support these scenarios, this specification defines a set of HTTP-based RESTful interfaces in terms of HTTP methods: GET, POST, PUT and DELETE, HTTP response codes, content type handling and resource formats.},
  langid = {american},
  file = {/home/ermo/Zotero/storage/AI86GFM2/oslc-cm-v3-0-ps01.html}
}

@misc{amsdenOSLCCoreVersion2021,
  type = {{{OASIS Standard}}},
  title = {{{OSLC Core Version}} 3.0. {{Part}} 1: {{Overview}}},
  author = {Amsden, Jim and Berezovskyi, Andrii},
  year = {2021},
  month = aug
}

@inproceedings{aoyamaPROMISManagementPlatform2013,
  title = {{{PROMIS}}: {{A Management Platform}} for {{Software Supply Networks Based}} on the {{Linked Data}} and {{OSLC}}},
  shorttitle = {{{PROMIS}}},
  booktitle = {2013 {{IEEE}} 37th {{Annual Computer Software}} and {{Applications Conference}}},
  author = {Aoyama, Mikio and Yabuta, Kazuo and Kamimura, Tsutomu and Inomata, Souichi and Chiba, Toshiya and Niwa, Takashi and Sakata, Koji},
  year = {2013},
  month = jul,
  pages = {214--219},
  issn = {0730-3157},
  doi = {10.1109/COMPSAC.2013.36},
  abstract = {PROMIS (Project Management Information exchange Services) is a research consortium with Fujitsu, IBM Japan, Hitachi, NEC, Nomura Research Institute, NTT DATA and Nanzan University. It aims at developing a new software engineering platform for managing SSN (Software Supply Network), an inter-organizational model of software development. This article reports on the background, goals, research challenges, our approaches and main results of the PROMIS consortium. Major technical contributions include a management model for SSN, a common resource model for exchanging management data as Linked Data, and a RESTful resource-oriented architecture on top of OSLC (Open Services for Lifecycle Collaboration) over the Web.},
  keywords = {ALM,Companies,Concrete,Data models,global software development,linked data,OSLC,project management,Project management,Resource description framework,REST,Software,Software architecture,software supply network}
}

@misc{ApacheKafka,
  title = {Apache {{Kafka}}},
  journal = {Apache Kafka},
  abstract = {Apache Kafka: A Distributed Streaming Platform.},
  howpublished = {\url{https://kafka.apache.org/}},
  langid = {english},
  note = {Accessed 2022-07-28},
  file = {/home/ermo/Zotero/storage/ITX2LXA2/kafka.apache.org.html}
}

@inproceedings{artacDevOpsIntroducingInfrastructureasCode2017,
  title = {{{DevOps}}: {{Introducing Infrastructure-as-Code}}},
  shorttitle = {{{DevOps}}},
  booktitle = {2017 {{IEEE}}/{{ACM}} 39th {{International Conference}} on {{Software Engineering Companion}} ({{ICSE-C}})},
  author = {Artac, Matej and Borovssak, Tadej and Di Nitto, Elisabetta and Guerriero, Michele and Tamburri, Damian Andrew},
  year = {2017},
  month = may,
  pages = {497--498},
  doi = {10.1109/ICSE-C.2017.162},
  abstract = {DevOps entails a series of software engineering tactics aimed at shortening the actionable operation of software design changes. One of these tactics is to harness infrastructure-as-code, that is, writing a blueprint that contains deployment specifications ready for orchestration in the cloud. This abstract briefly discusses all necessary elements and abstractions in writing and maintaining that blueprint, revolving around a key standard for its expression, namely, the OASIS "Topology and Orchestration Specification for Cloud Applications" (TOSCA) industrial standard adopted by as many as 60+ big industrial players worldwide.},
  keywords = {Big Data,Cloud computing,DevOps,Infrastructure-as-Code,Standards,Tools,Topology,TOSCA}
}

@inproceedings{axelssonExperiencesUsingLinked2019,
  title = {Experiences of {{Using Linked Data}} and {{Ontologies}} for {{Operational Data Sharing}} in {{Systems-of-Systems}}},
  booktitle = {2019 {{IEEE International Systems Conference}} ({{SysCon}})},
  author = {Axelsson, Jakob},
  year = {2019},
  month = apr,
  pages = {1--8},
  issn = {2472-9647},
  doi = {10.1109/SYSCON.2019.8836909},
  abstract = {This paper addresses the problem of exchanging complex data between the constituent systems in a system-of-systems. This is necessary to ensure that they have compatible understandings of the world surrounding them and entails a need for semantic interoperability between the constituents. Through a case study of a road construction system-of-systems, the world wide web technologies of linked data and ontologies are explored as a framework for data representation and exchange. This data includes several broad categories, such as assets, interfaces, organizations, capabilities, missions, and observations, as well as various properties of those. It is also discussed how the constituents can use this data for reasoning and decision making. The results have been validated through a simulation-based research prototype of the road construction case, from which experiences are reported.},
  keywords = {Automobiles,Industry 4.0,Interoperability,linked data,Linked data,Ontologies,ontology,Resource description framework,road construction,Roads,semantic interoperability,Semantics,system-of-systems}
}

@article{baeAutomaticControlWorkflow2004,
  title = {Automatic Control of Workflow Processes Using {{ECA}} Rules},
  author = {Bae, Joonsoo and Bae, Hyerim and Kang, Suk-Ho and Kim, Yeongho},
  year = {2004},
  month = aug,
  journal = {IEEE Transactions on Knowledge and Data Engineering},
  volume = {16},
  number = {8},
  pages = {1010--1023},
  issn = {1558-2191},
  doi = {10.1109/TKDE.2004.20},
  abstract = {Changes in recent business environments have created the necessity for a more efficient and effective business process management. The workflow management system is software that assists in defining business processes as well as automatically controlling the execution of the processes. We propose a new approach to the automatic execution of business processes using event-condition-action (ECA) rules that can be automatically triggered by an active database. First of all, we propose the concept of blocks that can classify process flows into several patterns. A block is a minimal unit that can specify the behaviors represented in a process model. An algorithm is developed to detect blocks from a process definition network and transform it into a hierarchical tree model. The behaviors in each block type are modeled using ACTA formalism. This provides a theoretical basis from which ECA rules are identified. The proposed ECA rule-based approach shows that it is possible to execute the workflow using the active capability of database without users' intervention. The operation of the proposed methods is illustrated through an example process.},
  keywords = {active database,Automatic control,business process.,Companies,Control systems,Databases,ECA rules,Engines,Environmental management,Process control,Software systems,Technology management,Workflow management,Workflow management software}
}

@misc{banksMQTTVersion2019,
  type = {{{OASIS Standard}}},
  title = {{{MQTT Version}} 5.0},
  author = {Banks, Andrew and Briggs, Ed and Borgendale, Ken and Gupta, Rahul},
  year = {2019},
  month = jul,
  abstract = {MQTT is a Client Server publish/subscribe messaging transport protocol. It is light weight, open, simple, and designed to be easy to implement. These characteristics make it ideal for use in many situations, including constrained environments such as for communication in Machine to Machine (M2M) and Internet of Things (IoT) contexts where a small code footprint is required and/or network bandwidth is at a premium. The protocol runs over TCP/IP, or over other network protocols that provide ordered, lossless, bi-directional connections. Its features include: {$\cdot$} Use of the publish/subscribe message pattern which provides one-to-many message distribution and decoupling of applications. {$\cdot$} A messaging transport that is agnostic to the content of the payload. {$\cdot$} Three qualities of service for message delivery: o "At most once", where messages are delivered according to the best efforts of the operating environment. Message loss can occur. This level could be used, for example, with ambient sensor data where it does not matter if an individual reading is lost as the next one will be published soon after. o "At least once", where messages are assured to arrive but duplicates can occur. o "Exactly once", where messages are assured to arrive exactly once. This level could be used, for example, with billing systems where duplicate or lost messages could lead to incorrect charges being applied. {$\cdot$} A small transport overhead and protocol exchanges minimized to reduce network traffic. {$\cdot$} A mechanism to notify interested parties when an abnormal disconnection occurs.}
}

@article{barcellosWellFoundedSoftwareMeasurement2010,
  title = {A {{Well-Founded Software Measurement Ontology}}},
  author = {Barcellos, Monalessa Perini and Falbo, Ricardo de Almeida and Dal Moro, Rodrigo},
  year = {2010},
  journal = {Formal Ontology in Information Systems},
  pages = {213--226},
  publisher = {{IOS Press}},
  doi = {10.3233/978-1-60750-535-8-213}
}

@article{bassiliadesPaaSportSemanticModel2018,
  title = {{{PaaSport}} Semantic Model: {{An}} Ontology for a Platform-as-a-Service Semantically Interoperable Marketplace},
  shorttitle = {{{PaaSport}} Semantic Model},
  author = {Bassiliades, Nick and Symeonidis, Moisis and Gouvas, Panagiotis and Kontopoulos, Efstratios and Meditskos, Georgios and Vlahavas, Ioannis},
  year = {2018},
  month = jan,
  journal = {Data \& Knowledge Engineering},
  volume = {113},
  pages = {81--115},
  issn = {0169-023X},
  doi = {10.1016/j.datak.2017.11.001},
  abstract = {PaaS is a Cloud computing service that provides a computing platform to develop, run, and manage applications without the complexity of infrastructure maintenance. SMEs are reluctant to enter the growing PaaS market due to the possibility of being locked in to a certain platform, mostly provided by the market's giants. The PaaSport Marketplace aims to avoid the provider lock-in problem by allowing Platform provider SMEs to roll out semantically interoperable PaaS offerings and Software SMEs to deploy or migrate their applications on the best-matching offering, through a thin, non-intrusive Cloud broker. In this paper, we present the PaaSport semantic model, namely an OWL ontology, extension of the DUL ontology. The ontology is used for semantically representing (a) PaaS offering capabilities and (b) requirements of applications to be deployed. The ontology has been designed to optimally support a semantic matchmaking and ranking algorithm that recommends the best-matching PaaS offering to the application developer. The DUL ontology offers seamless extensibility, since both PaaS Characteristics and parameters are defined as classes; therefore, extending the ontology with new characteristics and parameters requires the addition of new specialized subclasses of the already existing classes, which is less complicated than adding ontology properties. The PaaSport ontology is evaluated through verification tools, competency questions, human experts, application tasks and query performance tests.},
  langid = {english},
  keywords = {Cloud computing,Cloud Marketplace,Ontologies,Platform-as-a-Service,Quality and metrics,Semantic interoperability}
}

@article{berezovskyiImprovingLifecycleQuery2018,
  title = {Improving Lifecycle Query in Integrated Toolchains Using Linked Data and {{MQTT-based}} Data Warehousing},
  author = {Berezovskyi, Andrii and {El-khoury}, Jad and Kacimi, Omar and Loiret, Fr{\'e}d{\'e}ric},
  year = {2018},
  journal = {arXiv preprint arXiv:1803.03525},
  eprint = {1803.03525},
  eprinttype = {arxiv},
  archiveprefix = {arXiv}
}

@misc{berners-leeNotation3N3Readable2011,
  type = {{{W3C Team Submission}}},
  title = {Notation3 ({{N3}}): {{A}} Readable {{RDF}} Syntax},
  author = {{Berners-Lee}, Tim and Connolly, Dan},
  year = {2011},
  month = mar,
  abstract = {The Resource Description Framework (RDF) is a general-purpose language for representing information in the Web. This document defines Notation 3 (also known as N3), an assertion and logic language which is a superset of RDF. N3 extends the RDF datamodel by adding formulae (literals which are graphs themselves), variables, logical implication, and functional predicates, as well as providing an textual syntax alternative to RDF/XML.}
}

@article{berners2005notation,
  title = {Notation 3 Logic},
  author = {{Berners-Lee}, Tim},
  year = {2005},
  journal = {W3 Design Issues [online]},
  volume = {9}
}

@inproceedings{bolscherDesigningSoftwareArchitecture2019,
  title = {Designing {{Software Architecture}} to {{Support Continuous Delivery}} and {{DevOps}}: {{A Systematic Literature Review}}:},
  shorttitle = {Designing {{Software Architecture}} to {{Support Continuous Delivery}} and {{DevOps}}},
  booktitle = {Proceedings of the 14th {{International Conference}} on {{Software Technologies}}},
  author = {Bolscher, Robin and Daneva, Maya},
  year = {2019},
  pages = {27--39},
  publisher = {{SCITEPRESS - Science and Technology Publications}},
  address = {{Prague, Czech Republic}},
  doi = {10.5220/0007837000270039},
  isbn = {978-989-758-379-7},
  langid = {english}
}

@misc{Bugzilla,
  title = {Bugzilla},
  howpublished = {\url{https://www.bugzilla.org/}},
  note = {Accessed 2022-07-28}
}

@article{casatiEnvironmentDesigningExceptions1999,
  title = {An Environment for Designing Exceptions in Workflows},
  author = {Casati, Fabio and Fugini, Mariagrazia and Mirbel, Isabelle},
  year = {1999},
  month = may,
  journal = {Information Systems},
  series = {10th {{International Conference}} on {{Advanced Information Systems Engineering}}},
  volume = {24},
  number = {3},
  pages = {255--273},
  issn = {0306-4379},
  doi = {10.1016/S0306-4379(99)00018-6},
  abstract = {When designing a workflow schema, the workflow designer must often explicitly deal with exceptional situations, such as abnormal process termination or suspension of task execution. This paper shows how the designer can be supported by tools allowing him to capture exceptional behavior within a workflow schema, by reusing an available set of pre-configured exceptions skeletons. Exceptions are expressed by means of triggers, to be executed on the top of an active database environment. In particular, the paper deals with the handling of typical workflow exceptional situations which are modeled as generic exception skeletons to be included in a new workflow schema by simply specializing or instantiating them. Such skeletons, called patterns, are stored in a catalog; the paper describes the catalog structure and its management tools constituting an integrated environment for pattern-based exception design and reuse.},
  langid = {english},
  keywords = {Design Patterns,Exceptions,Workflows}
}

@inproceedings{chenArchitectingContinuousDelivery2015,
  title = {Towards {{Architecting}} for {{Continuous Delivery}}},
  booktitle = {2015 12th {{Working IEEE}}/{{IFIP Conference}} on {{Software Architecture}}},
  author = {Chen, Lianping},
  year = {2015},
  month = may,
  pages = {131--134},
  doi = {10.1109/WICSA.2015.23},
  abstract = {Continuous Delivery (CD) has emerged as an auspicious software development discipline, with the promise of providing organizations the capability to release valuable software continuously to customers. Our organization has been implementing CD for the last two years. Thus far, we have moved 22 software applications to CD. I observed that CD has created a new context for architecting these applications. In this paper, I will try to characterize such a context of CD, explain why we need to architect for CD, describe the implications of architecting for CD, and discuss the challenges this new context creates. This information can provide insights to other practitioners for architecting their software applications, and provide researchers with input for developing their research agendas to further study this increasingly important topic.},
  keywords = {architecturally significant requirements,Companies,Computer architecture,Context,continuous delivery,continuous deployment,continuous software engineering,DevOps,Monitoring,non-functional requirements,Pipelines,quality attributes,Software,software architecture,Software reliability}
}

@inproceedings{chenArchitecturalSupportDevOps2015,
  title = {Architectural {{Support}} for {{DevOps}} in a {{Neo-Metropolis BDaaS Platform}}},
  booktitle = {2015 {{IEEE}} 34th {{Symposium}} on {{Reliable Distributed Systems Workshop}} ({{SRDSW}})},
  author = {Chen, Hong-Mei and Kazman, Rick and Haziyev, Serge and Kropov, Valentyn and Chtchourov, Dmitri},
  year = {2015},
  month = sep,
  pages = {25--30},
  doi = {10.1109/SRDSW.2015.14},
  abstract = {Big data as a Service (BDaaS) provides a viable strategy for organizations to implement scalable, tailorable big data infrastructure and applications built on this infrastructure. New trends in the BDaaS market are moving toward an open world model \textendash{} what we call the Neo-Metropolis model \textendash{} for developing BDaaS platforms. The key to the success of such large-scale technology-agnostic platforms, we posit, is an architectural strategy revolving around microservices and DevOps. This article presents the results of an action research with a Neo-Metropolis BDaaS vendor and illustrates how architectural support for DevOps is critical in achieving desired system qualities and enabling platform success. This research contributes to illuminate best practices of DevOps, and to validate and augment a set of DevOps tactics previously developed, while adding and recategorizing new instances of well-established architectural tactics.},
  keywords = {big data,Big data,Computer architecture,Kernel,Market research,Organizations,Production,software architecture,tactics,Testing}
}

@article{chenContinuousDeliveryOvercoming2017,
  title = {Continuous {{Delivery}}: {{Overcoming}} Adoption Challenges},
  shorttitle = {Continuous {{Delivery}}},
  author = {Chen, Lianping},
  year = {2017},
  month = jun,
  journal = {Journal of Systems and Software},
  volume = {128},
  pages = {72--86},
  issn = {0164-1212},
  doi = {10.1016/j.jss.2017.02.013},
  abstract = {Continuous Delivery (CD) is a relatively new software development approach. Companies that have adopted CD have reported significant benefits. Motivated by these benefits, many companies would like to adopt CD. However, adopting CD can be very challenging for a number of reasons, such as obtaining buy-in from a wide range of stakeholders whose goals may seemingly be different from\textemdash or even conflict with\textemdash our own; gaining sustained support in a dynamic complex enterprise environment; maintaining an application development team's momentum when their application's migration to CD requires an additional strenuous effort over a long period of time; and so on. To help overcome the adoption challenges, I present six strategies: (1) selling CD as a painkiller; (2) establishing a dedicated team with multi-disciplinary members; (3) continuous delivery of continuous delivery; (4) starting with the easy but important applications; (5) visual CD pipeline skeleton; (6) expert drop. These strategies were derived from four years of experience in implementing CD at a multi-billion-euro company. Additionally, our experience led to the identification of eight further challenges for research. The information contributes toward building a body of knowledge for CD adoption.},
  langid = {english},
  keywords = {Adoption,Agile Software Development,Continuous Delivery,Continuous Deployment,Continuous Software Engineering,DevOps}
}

@inproceedings{chenOpenSourceLifecycle2019,
  title = {An {{Open Source Lifecycle Collaboration Approach Supporting Internet}} of {{Things System Development}}},
  booktitle = {2019 14th {{Annual Conference System}} of {{Systems Engineering}} ({{SoSE}})},
  author = {Chen, Jinwei and Hu, Zhenchao and Lu, Jinzhi and Zhang, Huisheng and Huang, Sihan and T{\"o}rngren, Martin},
  year = {2019},
  month = may,
  pages = {63--68},
  doi = {10.1109/SYSOSE.2019.8753883},
  abstract = {The internet of things (IoT) system integrates heterogeneous systems using centric services to provide a single open solution to process sensor data. During the whole life cycle of IoT systems, developers face the problems of interface management and data interoperability led by increasing complexity. Such problems decrease the efficiency of IoT system development and implementation, such as interface configurations for domain specific systems are difficult if there is not a unified specification. This paper proposed an Open Services for Lifecycle Collaboration (OSLC) approach supporting IoT system development and implementation. The approach integrates domain specific data across the whole lifecycle including development models and sensor data. Moreover, it enables interface management for IoT system development and real-time monitoring for implementations. From the case study, we find an OSLC-based tool, Datalinks, supports data integration and interface management which improves the development efficiency and data interoperability of IoT systems. The integrated data based on OSLC acts the mid-wares of data exchange between physical space and virtual space of IoT system. Moreover, the OSLC-based interfaces are developed based on unified specifications whose reusability is promoted for the future development.},
  keywords = {Interface management and configuration,Internet of Things system,Lifecycle data integration,OSLC specification}
}

@misc{Clarive,
  title = {Clarive},
  abstract = {Simplify Application Delivery, Code, track and deploy in one tool with battle-tested workflows for agile and devops},
  howpublished = {\url{https://clarive.com/}},
  langid = {american},
  note = {Accessed 2022-07-28}
}

@misc{coronadoEWEOntologySpecification2017,
  title = {{{EWE Ontology Specification}}},
  author = {Coronado, Miguel and Mu{\~n}oz, Sergio and Iglesias, Carlos A.},
  year = {2017},
  month = dec
}

@article{coronadoModellingRulesAutomating2015,
  title = {Modelling Rules for Automating the {{Evented WEb}} by Semantic Technologies},
  author = {Coronado, Miguel and Iglesias, Carlos A. and Serrano, Emilio},
  year = {2015},
  month = nov,
  journal = {Expert Systems with Applications},
  volume = {42},
  number = {21},
  pages = {7979--7990},
  issn = {0957-4174},
  doi = {10.1016/j.eswa.2015.06.031},
  abstract = {The Live Web is characterised by a new way of interacting with the Web through dynamic streams of relevant real-time contextual information to users. These sources of massive data usually overwhelm them, because they are not able to consume that amount of data. Task Automation Services (TASs) are platforms or apps that allow their users to author automation rules to combine events from streams while reducing the effort for handling incoming information. While these platforms are a reality, they suffer from two major drawbacks: (i) the only incoming data streams available are those the TASs developers decided to include in the system, and (ii) they lack of a mechanism to reason over large scale data outside their platform. To face these challenges, this paper contributes in (i) reviewing the existing state of the art including research and commercial work given their relevance. Based on the lessons learnt from this review, (ii) we propose the Evented WEb ontology (EWE), that models the Evented WEb domain, and in particular those concepts around TASs. EWE enables scalability, interoperability and definition of rules with reasoning over Linked Open Data (LOD) cloud. To illustrate these contributions, (iii) a semantic TAS has been implemented that benefits from the advantages EWE offers, and solves a realistic problem using semantic technologies. Finally, (iv) to validate the ontology covers the domain it models, a thorough ontology evaluation is presented.},
  langid = {english},
  keywords = {Event-stream,Knowledge management,Linked data,Semantic Web,Social sensor,Web rule}
}

@article{coronadoTaskAutomationServices2016,
  title = {Task {{Automation Services}}: {{Automation}} for the {{Masses}}},
  shorttitle = {Task {{Automation Services}}},
  author = {Coronado, Miguel and Iglesias, Carlos A.},
  year = {2016},
  month = jan,
  journal = {IEEE Internet Computing},
  volume = {20},
  number = {1},
  pages = {52--58},
  issn = {1941-0131},
  doi = {10.1109/MIC.2015.73},
  abstract = {A simple model of mashup technology for combining services and connected devices is now becoming popular. This model is frequently referred to as task automation based on Event-Condition-Action (ECA) rules. The most popular online services that follow this approach are Ifttt and Zapier. In addition, this model is being followed by several mobile frameworks to automate how phones deal with incoming Internet events and phone sensors. Here, the authors outline the features and components of task automation services (TASs), and propose a generic architecture that supports current challenges. Also, because TASs are a growing trend, this article provides a survey comparison of existing platforms and discusses their evolution and future tendencies.},
  keywords = {automation,Computer architecture,connected device,Ifttt,Internet,Internet service,Internet/Web technologies,mashup,Mashups,Meteorology,Privacy,task automation,Task automation,Web services}
}

@misc{crossleyOSLCTrackedResource2021,
  type = {{{OASIS Project Specification Draft}} 01},
  title = {{{OSLC Tracked Resource Set Version}} 3.0. {{Part}} 1: {{Specification}}},
  author = {Crossley, Nick},
  year = {2021},
  month = aug,
  abstract = {The Tracked Resource Set protocol allows a server to expose a set of resources in a way that allows clients to discover that set of resources, to track additions to and removals from the set, and to track state changes to the resources in the set. The protocol does not assume that clients will dereference the resources, but they could do so. The protocol is suitable for dealing with sets containing a large number of resources, as well as highly active resource sets that undergo continual change. The protocol is HTTP-based and follows RESTful principles.}
}

@misc{Docker,
  title = {Docker},
  howpublished = {\url{https://www.docker.com/}},
  langid = {american},
  note = {Accessed 2022-07-28}
}

@article{ebelingOSLCBasedApproach2017,
  title = {{{OSLC}} Based Approach for Product Appearance Structuring},
  author = {Ebeling, Ren{\'e} and Eigner, Martin},
  year = {2017},
  journal = {DS 87-4 Proceedings of the 21st International Conference on Engineering Design (ICED 17) Vol 4: Design Methods and Tools, Vancouver, Canada, 21-25.08.2017},
  pages = {259--266},
  issn = {2220-4342},
  abstract = {Within early phases of a product lifecycle, a lack of process connections between CAD design, product appearance configuration as well as virtual reality and other visualisation scenarios can still be identified. Therefore, the contribution introduces the basic components of a conceptual framework for holistic management of virtual reality and other related visualisation data in order to enhance the current situation within the different product lifecycle stages. The basis of the approach covers the structuring of geometry entities within CAx-authoring tools and the connection to the underlying data backbone. Hence, the ability to structure geometry within CAx-tools and CAx-based open standard formats will be evaluated and discussed. By using the OSLC (Open Standard for Lifecycle Collaboration ) specification, these results allow the implementation of a CAD integrated tool to connect product data with configurable linked data entities.},
  isbn = {9781904670926},
  langid = {english}
}

@book{el-khouryAnalysisOASISOSLC2020,
  title = {An {{Analysis}} of the {{OASIS OSLC Integration Standard}}, for a {{Cross-disciplinary Integrated Development Environment}} : {{Analysis}} of Market Penetration, Performance and Prospects},
  shorttitle = {An {{Analysis}} of the {{OASIS OSLC Integration Standard}}, for a {{Cross-disciplinary Integrated Development Environment}}},
  author = {{El-khoury}, Jad},
  year = {2020},
  publisher = {{DiVA}},
  abstract = {OASIS OSLC is a standard that targets the integration of engineering software applications. Its approach promotes loose coupling, in which each application autonomously manages its own product data, while providing RESTful web services through which other applications can interact. This report aims to analyse the suitability of OSLC as an overarching integration mechanism for the complete set of engineering activities of Cyber Physical Systems (CPS) development. To achieve this, a review of the current state of the OASIS OSLC integration standard is provided in terms of its market penetration in commercial applications, its capabilities, and the architectural qualities of OSLC-based solutions. This review is based on a survey of commercial software applications that provide some support for OSLC capabilities.},
  isbn = {978-91-7873-525-9},
  langid = {english}
}

@article{el-khouryModellingSupportLinked2016,
  title = {Modelling Support for a Linked Data Approach to Tool Interoperability},
  author = {{El-Khoury}, Jad and Gurdur, Didem and Loiret, Frederic and T{\"o}rngren, Martin and Da Zhang, Mattias Nyberg},
  year = {2016},
  journal = {ALLDATA 2016},
  pages = {51}
}

@article{eversEvaluatingCloudAutomation2015,
  title = {Evaluating {{Cloud Automation}} as a {{Service}}},
  author = {Evers, Andrew},
  year = {2015},
  journal = {CYBER SECURITY THREATS},
  pages = {33}
}

@inproceedings{gangemiUnderstandingSemanticWeb2003,
  title = {Understanding the {{Semantic Web}} through {{Descriptions}} and {{Situations}}},
  booktitle = {On {{The Move}} to {{Meaningful Internet Systems}} 2003: {{CoopIS}}, {{DOA}}, and {{ODBASE}}},
  author = {Gangemi, Aldo and Mika, Peter},
  editor = {Meersman, Robert and Tari, Zahir and Schmidt, Douglas C.},
  year = {2003},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {689--706},
  publisher = {{Springer}},
  address = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-540-39964-3_44},
  abstract = {The Semantic Web is a powerful vision that is getting to grips with the challenge of providing more human-oriented web services. Hence, reasoning with and across distributed, partially implicit assumptions (contextual knowledge), is a milestone.Ontologies are a primary means to deploy the Semantic Web vision, but few work has been done on them to manage the context-dependency of Web knowledge. In this paper we introduce an ontology for representing a variety of reified contexts and states of affairs, called D\&S, currently implemented as a plug-in to the DOLCE foundational ontology, and its application to two cases: an ontology for communication situations and roles, and an ontology for peer-to-peer communication. The reified contexts represented in D\&S have a rich structure, and are a middleware between full-fledged formal contexts and theories, and the often poor vocabularies implemented in Web ontologies.},
  isbn = {978-3-540-39964-3},
  langid = {english},
  keywords = {Binary Predicate,Formal Context,Foundational Ontology,Local Ontology,Ontological Commitment}
}

@misc{geraGitZilla2021,
  title = {{{GitZilla}}},
  author = {Gera, Devendra},
  year = {2021},
  month = jan,
  abstract = {Git-Bugzilla integration done right},
  howpublished = {\url{https://github.com/gera/gitzilla}},
  note = {Accessed 2022-07-28}
}

@misc{GitHub,
  title = {{{GitHub}}},
  abstract = {GitHub is where people build software. More than 73 million people use GitHub to discover, fork, and contribute to over 200 million projects.},
  howpublished = {\url{https://github.com}},
  langid = {english},
  note = {Accessed 2022-07-28}
}

@article{gohECARulebasedSupport2001,
  title = {{{ECA}} Rule-Based Support for Workflows},
  author = {Goh, A. and Koh, Y. -K. and Domazet, D. S.},
  year = {2001},
  month = jan,
  journal = {Artificial Intelligence in Engineering},
  volume = {15},
  number = {1},
  pages = {37--46},
  issn = {0954-1810},
  doi = {10.1016/S0954-1810(00)00028-5},
  abstract = {The use of event\textendash condition\textendash action (ECA) rules has transformed database systems from passive query-based data repositories to active sources of information delivery. In a similar fashion, ECA rules can be used to benefit workflow systems. In this paper, a software framework known as STEP workflow management facility is proposed in order to manage collaborative and distributed workflows and to provide interfaces to object management group-compliant product data management systems. Issues related to implementation using open standards such as CORBA are discussed. A key point underlying the framework is the flexibility it affords to users to re-configure the system according to evolving needs in collaborative product development.},
  langid = {english},
  keywords = {ECA rules,Rule manager,Workflow systems}
}

@article{guerreroSystematicMappingStudy2020,
  title = {A Systematic Mapping Study about {{DevOps}}},
  author = {Guerrero, Jonathan and Z{\'u}{\~n}iga, Karen and Certuche, Camilo and Pardo, C{\'e}sar},
  year = {2020},
  month = aug,
  journal = {Journal de Ciencia e Ingenier\'ia},
  volume = {12},
  number = {1},
  pages = {48--62},
  issn = {2539-066X, 2145-2628},
  doi = {10.46571/JCI.2020.1.5},
  abstract = {DevOps is a very trendy term these days in the software development companies (SDC), term that emerges as a possible solution to finally reach an acceleration and a rise of productivity expected with the appearance of agile development approaches, but that until now had not materialized, through automation practices, continuous integration, continuous build and continuous deployment. This paper aims to show current knowledge about the process of adopting DevOps in SDC through a systematic mapping of the literature. However, the results obtained show that there is little detailed information regarding activities, tasks, roles and other important process elements for the adoption of DevOps. Similarly, it has been concluded that there is no a unified terminology, therefore, it is important to standardize it to simplify the understanding and application of DevOps. Furthermore, this paper shows the preview of the framework that it is being developed for the adoption of DevOps in the SDC.},
  langid = {english}
}

@inproceedings{hoppeRequirementsSharedData2015,
  title = {Requirements of Shared Data Management Services Facilitating a Reference Architecture Realizing the Concepts of {{ECSS-E-TM-10-23}}},
  booktitle = {Workshop on {{Simulation}} for {{European Space Programmes}} ({{SESP}})},
  author = {Hoppe, Tobias and Eisenmann, Harald},
  year = {2015},
  volume = {24},
  pages = {26}
}

@misc{iftttIFTTT,
  title = {{{IFTTT}}},
  author = {{IFTTT}},
  abstract = {Get started with IFTTT, the easiest way to do more with your favorite apps and devices for free. Make your home more relaxing. Make your work more productive. Keep your data private and secure. We believe every thing works better together.},
  howpublished = {\url{https://ifttt.com/}},
  langid = {english},
  note = {Accessed 2022-07-28}
}

@inproceedings{jabbariWhatDevOpsSystematic2016,
  title = {What Is {{DevOps}}? {{A}} Systematic Mapping Study on Definitions and Practices},
  booktitle = {{{ACM International Conference Proceeding Series}}},
  author = {Jabbari, Ramtin and Ali, Nauman Bin and Petersen, Kai and Tanveer, Binish},
  year = {2016},
  month = may,
  volume = {24-May-2016},
  pages = {1--11},
  publisher = {{Association for Computing Machinery}},
  doi = {10.1145/2962695.2962707},
  abstract = {Context: DevOps, the combination of Development and Operations, is a new way of thinking in the software engineering domain that recently received much attention. Given that DevOps is a new term and novel concept recently introduced, no common understanding of what it entails has been achieved yet. Consequently, definitions of DevOps often only represent a part that is relevant to the concept. Objective:This study aims to characterize DevOps by exploring central components of DevOps definitions reported in the literature, specifying practices explicitly proposed for DevOps and investigating the similarities and differences between DevOps and other existing methods in software engineering. Method: A systematic mapping study was conducted that used six electronic databases: IEEE, ACM, Inspec, Scopus, Wiley Online Library and Web of Science. Result: 44 studies have been selected that report a definition of DevOps, 15 studies explicitly stating DevOps practices, and 15 studies stating how DevOps is related to other existing methods. Papers in some cases stated a combination of a definition, practices, and relations to other methods, the total number of primary studies was 49. Conclusion: We proposed a definition for DevOps which may overcome inconsistencies over the various existing definitions of individual research studies. In addition, the practices explicitly proposed for DevOps have been presented as well as the relation to other software development methods.},
  isbn = {978-1-4503-4134-9},
  keywords = {DevOps definition,DevOps practice,Software development method}
}

@inproceedings{johngHarmoniaContinuousService2019,
  title = {Harmonia: {{A Continuous Service Monitoring Framework Using DevOps}} and {{Service Mesh}} in a {{Complementary Manner}}},
  booktitle = {Service-{{Oriented Computing}}},
  author = {Johng, Haan and Kalia, Anup K. and Xiao, Jin and Vukovi{\'c}, Maja and Chung, Lawrence},
  editor = {Yangui, Sami and Bouassida Rodriguez, Ismael and Drira, Khalil and Tari, Zahir},
  year = {2019},
  pages = {151--168},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  abstract = {Software teams today are required to deliver new or updated services frequently, rapidly and independently. Adopting DevOps and Microservices support the rapid service delivery model but leads to pushing code or service infrastructure changes across inter-dependent teams that are not collectively assessed, verified, or notified. In this paper, we propose Harmonia - a continuous service monitoring framework utilizing DevOps and Service Mesh in a complementary manner to improve coordination and change management among independent teams. Harmonia can automatically detect changes in services, including changes that violate performance SLAs and user experience, notify the changes to affected teams, and help them resolve the changes quickly. We applied Harmonia to a standard application in describing Microservice management to assist with an initial understanding and strengths of Harmonia. During the demonstration, we deployed faulty and normal services alternatively and captured changes from Jenkins, Github, Istio, and Kubernetes logs to form an application-centric cohesive view of the change and its impact and notify the affected teams.},
  isbn = {978-3-030-33702-5}
}

@misc{JoinThisVideo,
  title = {Join This Video Meeting},
  abstract = {Real-time meetings by Google. Using your browser, share your video, desktop, and presentations with teammates and customers.},
  langid = {english}
}

@article{kargerSemanticWebEnd2014,
  title = {The {{Semantic Web}} and {{End Users}}: {{What}}'s {{Wrong}} and {{How}} to {{Fix It}}},
  shorttitle = {The {{Semantic Web}} and {{End Users}}},
  author = {Karger, David R.},
  year = {2014},
  month = nov,
  journal = {IEEE Internet Computing},
  volume = {18},
  number = {6},
  pages = {64--70},
  issn = {1941-0131},
  doi = {10.1109/MIC.2014.124},
  abstract = {The Semantic Web's potential to deliver tools that help end users capture, communicate, and manage information has yet to be fulfilled, and far too little research is going into doing so. The author examines the poor state of current tools, argues that the Semantic Web offers a key part of the answer to building better ones, and discusses what needs to change in Semantic Web research to attain that goal.},
  keywords = {Internet,Internet technologies,Maintenance engineering,Quality of service,Semantic Web,Sematic Web,Web services,Web tools}
}

@article{knublauch2011spin,
  title = {{{SPIN-overview}} and Motivation},
  author = {Knublauch, Holger and Hendler, James A and Idehen, Kingsley},
  year = {2011},
  journal = {W3C Member Submission},
  volume = {22},
  pages = {W3C}
}

@misc{Kubernetes,
  title = {Kubernetes},
  howpublished = {\url{https://kubernetes.io/}},
  note = {Accessed 2022-07-28}
}

@inproceedings{leitner_lessons_2016,
  title = {Lessons {{Learned}} from {{Tool Integration}} with {{OSLC}}},
  booktitle = {Information and {{Software Technologies}}},
  author = {Leitner, Andrea and Herbst, Beate and Mathijssen, Roland},
  editor = {Dregvaite, Giedre and Damasevicius, Robertas},
  year = {2016},
  series = {Communications in {{Computer}} and {{Information Science}}},
  pages = {242--254},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-319-46254-7_20},
  abstract = {Today's embedded and cyber-physical systems are getting more connected and complex. One main challenge during development is the often loose coupling between engineering tools, which could lead to inconsistencies and errors due to the manual transfer and duplication of data. Open formats and specifications raise expectations for seamlessly integrated tool chains for systems engineering combining best-of-breed technologies and tools of different tool vendors.},
  isbn = {978-3-319-46254-7},
  langid = {english},
  keywords = {Interoperability,Lifecycle integration,OSLC}
}

@inproceedings{leitnerLessonsLearnedTool2016,
  title = {Lessons {{Learned}} from {{Tool Integration}} with {{OSLC}}},
  booktitle = {Information and {{Software Technologies}}},
  author = {Leitner, Andrea and Herbst, Beate and Mathijssen, Roland},
  editor = {Dregvaite, Giedre and Damasevicius, Robertas},
  year = {2016},
  series = {Communications in {{Computer}} and {{Information Science}}},
  pages = {242--254},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-319-46254-7_20},
  abstract = {Today's embedded and cyber-physical systems are getting more connected and complex. One main challenge during development is the often loose coupling between engineering tools, which could lead to inconsistencies and errors due to the manual transfer and duplication of data. Open formats and specifications raise expectations for seamlessly integrated tool chains for systems engineering combining best-of-breed technologies and tools of different tool vendors.The ARTEMIS JU project CRYSTAL aims for a harmonized interoperability specification (IOS) incorporating various open specifications and standards such as OSLC (Open Services for Lifecycle Collaboration), ReqIF (Requirements Interchange Format) or FMI (Functional Mockup Interface) for supporting seamless model-based systems engineering.This paper focuses on lifecycle integration using OSLC. We will report challenges we experienced in the implementation of an automotive and healthcare use case. The paper should support others in deciding if OSLC is an appropriate technology and to overcome common challenges in the implementation of OSLC adapters.},
  isbn = {978-3-319-46254-7},
  langid = {english},
  keywords = {Interoperability,Lifecycle integration,OSLC}
}

@inproceedings{mccarthyComposableDevOpsAutomated2015,
  title = {Composable {{DevOps}}: {{Automated Ontology Based DevOps Maturity Analysis}}},
  shorttitle = {Composable {{DevOps}}},
  booktitle = {2015 {{IEEE International Conference}} on {{Services Computing}}},
  author = {McCarthy, Matthew A. and Herger, Lorraine M. and Khan, Shakil M. and Belgodere, Brian M.},
  year = {2015},
  month = jun,
  pages = {600--607},
  doi = {10.1109/SCC.2015.87},
  abstract = {In this era of the emerging digitized, mobilized, and cloudified enterprises, the concept of the "compos able business" is the most critical piece which ties everything together. The digital enterprise is here, and its prime characteristic is that is essentially detaches and segregates existing businesses and reassembles them according to market demands. Every industry, from transportation to eyewear is up for disruption, and developers are in the forefront of this movement. In turn, these developers are under intense pressure to accelerate time to market. The compos able enterprise approach requires a reconsideration of traditional models of the entire IT organization. These organization and their processes need to be broken up into components that follow certain key design principles such as The Minimal Functions with least Dependencies, portability, Shared Knowledge, Predictable Contracts and Maximized Human Value. The last three bullet points encapsulate the very definition of DevOps [3]. The concept of better integration between Development and Operations is a valuable objective. The goal is to foster measurable incremental cultural change to derive most overall value out of the union of people, process and technology. But the cultural issues, reward models, and risk allocation create obvious barriers in attaining those goals. The common industry belief is to use the compos able enterprise framework to build a platform using the right tools and you will have attained DevOps nirvana. In this paper we will explore valuable lessons learned from our mistakes in tool centric adoption of IT Infrastructure Library (ITIL) [8]. We will also show how we applied those lessons to develop a lightweight compos able/contextual DevOps framework that learns and measure itself to avoid those cultural pitfalls.},
  keywords = {Business,Collaboration,deontic,devops,Engines,itil,maturity,Monitoring,Ontologies,ontology,Pipelines,semantic,Semantics}
}

@article{mcilraithSemanticWebServices2001,
  title = {Semantic {{Web}} Services},
  author = {McIlraith, S.A. and Son, T.C. and Zeng, Honglei},
  year = {2001},
  month = mar,
  journal = {IEEE Intelligent Systems},
  volume = {16},
  number = {2},
  pages = {46--53},
  issn = {1941-1294},
  doi = {10.1109/5254.920599},
  abstract = {The authors propose the markup of Web services in the DAML family of Semantic Web markup languages. This markup enables a wide variety of agent technologies for automated Web service discovery, execution, composition and interoperation. The authors present one such technology for automated Web service composition.},
  keywords = {Application software,Automation,Internet,Markup languages,Microstrip,Pervasive computing,Semantic Web,Temperature sensors,Web pages,Web services}
}

@inproceedings{mengistTraceabilitySupportOpenModelica2017,
  title = {Traceability {{Support}} in {{OpenModelica Using Open Services}} for {{Lifecycle Collaboration}} ({{OSLC}})},
  booktitle = {The 12th {{International Modelica Conference}}, {{Prague}}, {{Czech Republic}}, {{May}} 15-17, 2017},
  author = {Mengist, Alachew and Pop, Adrian and Asghar, Adeel and Fritzson, Peter},
  year = {2017},
  month = jul,
  pages = {823--830},
  doi = {10.3384/ecp17132823},
  abstract = {A common situation in industry is that a system model is composed of several sub-models which may have been developed using different tools. The quality and effectiveness of large scale system modeling heavily depends on the underlying tools used for different phases of the development lifecycle. Available modeling and simulation tools support different operations on models, such as model creation, model simulation, FMU export, model checking, and code generation. Seamless tracing of the requirements and associating them with the models and the simulation results in the context of different modeling tools is becoming increasingly important. This can be used to support several activities such as impact analysis, component reuse, verification, and validation. However, due to the lack of interoperability between tools it is often difficult to use such tools in combination. Recently, the OSLC specification has emerged for integrating different lifecycle tools using linked data. In this paper we present new work on traceability support in OpenModelica where the traceability information is exchanged with other lifecycle tools through a standardized interface and format using OSLC. In particular, OpenModelica supports automatic recording and tracing of modeling activities such as creation, modification, and destruction of models, import model description XML, export of FMUs, and creation of simulation results.},
  langid = {english}
}

@inproceedings{munozSmartOfficeAutomation2016,
  title = {Smart {{Office Automation Based}} on {{Semantic Event-Driven Rules}}.},
  booktitle = {Intelligent {{Environments}} ({{Workshops}})},
  author = {Mu{\~n}oz, Sergio and Llamas, Antonio F. and Coronado, Miguel and Iglesias, Carlos Angel},
  year = {2016},
  pages = {33--42}
}

@article{nardoneOSLCbasedEnvironmentSystemlevel2020,
  title = {An {{OSLC-based}} Environment for System-Level Functional Testing of {{ERTMS}}/{{ETCS}} Controllers},
  author = {Nardone, Roberto and Marrone, Stefano and Gentile, Ugo and Amato, Aniello and Barberio, Gregorio and Benerecetti, Massimo and De Guglielmo, Renato and Di Martino, Beniamino and Mazzocca, Nicola and Peron, Adriano and Pisani, Gaetano and Velardi, Luigi and Vittorini, Valeria},
  year = {2020},
  month = mar,
  journal = {Journal of Systems and Software},
  volume = {161},
  pages = {110478},
  issn = {0164-1212},
  doi = {10.1016/j.jss.2019.110478},
  abstract = {Product and application life-cycle management (PLM/ALM) are the processes that govern a product and a software system, respectively, encompassing the creation, deployment and operation of a system from the beginning to the end of its life. As both PLM and ALM require cross-discipline collaboration and cooperation, tools integration and inter-operation are necessary to enable the efficient and effective usage of tool suites supporting the management of the entire system life-cycle and overcome the limitations of all-in-one solutions from one tool vendor. In this context, the Open Services for Life-cycle Collaboration (OSLC) initiative proposes a set of specifications to allow a seamless integration based on linked data. This paper describes the work performed within the ARTEMIS JU project CRYSTAL to develop an environment for the functional system-level testing of railway controllers, relying on OSLC to enable inter-operation with existing PLM/ALM tools. A concrete realization of the proposed architecture is described also discussing some design and implementation choices. A real industrial case study is used to exemplify the features and the usage of the environment in testing one of the functionalities of the Radio Block Centre, the vital core of the European Rail Traffic Management System/European Train Control System (ERTMS/ETCS) Control System.},
  langid = {english},
  keywords = {Critical systems,Life-cycle collaboration,Model based testing,OSLC,Testing automation}
}

@article{opara-martinsCriticalAnalysisVendor2016,
  title = {Critical Analysis of Vendor Lock-in and Its Impact on Cloud Computing Migration: {{A}} Business Perspective},
  shorttitle = {Critical Analysis of Vendor Lock-in and Its Impact on Cloud Computing Migration},
  author = {{Opara-Martins}, Justice and Sahandi, Reza and Tian, Feng},
  year = {2016},
  month = apr,
  journal = {Journal of Cloud Computing},
  volume = {5},
  number = {1},
  pages = {4},
  issn = {2192-113X},
  doi = {10.1186/s13677-016-0054-z},
  abstract = {Vendor lock-in is a major barrier to the adoption of cloud computing, due to the lack of standardization. Current solutions and efforts tackling the vendor lock-in problem are predominantly technology-oriented. Limited studies exist to analyse and highlight the complexity of vendor lock-in problem in the cloud environment. Consequently, most customers are unaware of proprietary standards which inhibit interoperability and portability of applications when taking services from vendors. This paper provides a critical analysis of the vendor lock-in problem, from a business perspective. A survey based on qualitative and quantitative approaches conducted in this study has identified the main risk factors that give rise to lock-in situations. The analysis of our survey of 114 participants shows that, as computing resources migrate from on-premise to the cloud, the vendor lock-in problem is exacerbated. Furthermore, the findings exemplify the importance of interoperability, portability and standards in cloud computing. A number of strategies are proposed on how to avoid and mitigate lock-in risks when migrating to cloud computing. The strategies relate to contracts, selection of vendors that support standardised formats and protocols regarding standard data structures and APIs, developing awareness of commonalities and dependencies among cloud-based solutions. We strongly believe that the implementation of these strategies has a great potential to reduce the risks of vendor lock-in.},
  langid = {english}
}

@misc{OSLCBugzillaTutorial,
  title = {{{OSLC Bugzilla Tutorial}}}
}

@misc{OSLCExtensionECAbased,
  title = {{{OSLC}} Extension for {{ECA-based Automation}}},
  howpublished = {\url{http://gsi.upm.es/ontologies/smartdevops/oslc\_eca}},
  note = {Accessed 2022-07-29},
  file = {/home/ermo/Zotero/storage/A3EKK7R6/index-en.html}
}

@misc{painOSLCActions2020,
  type = {{{OASIS Project Specification Draft}} 01},
  title = {{{OSLC Actions}}},
  author = {Pain, Martin and Padgett, Samuel},
  year = {2020},
  month = may
}

@article{pardo-calvacheReferenceOntologyHarmonizing2014,
  title = {A Reference Ontology for Harmonizing Process- Reference Models},
  author = {{Pardo-Calvache}, C{\'e}sar Jes{\'u}s and {Garc{\'i}a-Rubio}, F{\'e}lix Oscar and Piattini, Mario and {Pino-Correa}, Francisco Jos{\'e} and Baldassarre, Mar{\'i}a Teresa},
  year = {2014},
  journal = {Revista Facultad de Ingenier\'ia Universidad de Antioquia},
  volume = {73},
  pages = {14},
  abstract = {For a couple of decades, process quality has been considered one of the main factors in the delivery of high quality products. Multiple models and standards have emerged as a solution to this issue. However, for any company, the harmonization of diverse models with the aim at fulfilling its quality requirements is not an easy task to pursue. The difficulty fundamentally lies in the fact that there is a lack of specific guidelines, together with an evident inexistence of a homogeneous representation that could make the endeavour with regards to Software Engineering less intense. In order to address this challenge, this paper presents a Ontology of Process-reference Models, called PrMO. It defines a Common Structure of Process Elements (CSPE) as a means to support the harmonization of structural differences of multiple reference models, through the homogenization of their process structures. PrMO has been validated through instantiation of the information contained in different models, such as CMMI-(ACQ, DEV), ISO (9001, 27001, 27002, 20000-2), ITIL, COBIT, Risk IT, Val IT, BASEL II, amongst others. Both the common structure and the homogenization method are presented herein, along with an application example.},
  langid = {english}
}

@article{payneSemanticWebServices2004,
  title = {Semantic Web Services},
  author = {Payne, Terry R. and Lassila, Ora},
  year = {2004},
  journal = {IEEE Intelligent Systems},
  volume = {19},
  number = {1},
  pages = {14--15}
}

@inproceedings{rodriguezFormalOntologiesData2019,
  title = {Formal Ontologies and Data Shapes within the {{Software Engineering}} Development Lifecycle},
  booktitle = {The 31st {{International Conference}} on {{Software Engineering}} and {{Knowledge Engineering}}},
  author = {Rodr{\'i}guez, Jose Mar{\'i}a Alvarez and Moreno, Valent{\'i}n and Llorens, Juan},
  year = {2019},
  month = jul,
  pages = {64--70},
  doi = {10.18293/SEKE2019-114},
  abstract = {Business models, organizational activities and corporate strategies are now being reshaped to meet the new needs of a challenging and evolving environment in which more up-todate, secure, safer, cost-efficient and personalized software products and services must be timely delivered. This new digital context also represents an opportunity for the improvement and extension of existing software engineering methods. One of the current trends in Software Engineering development lies in boosting interoperability and collaboration between tools and people through the sharing of existing artifacts under common data models, formats and protocols to improve the practice and reuse of existing software artifacts. In this context, formal ontologies and data shapes play a key role to model and exchange data and to provide services for data validation (consistency checking) or type inference as part of a knowledge management strategy. In this document, an initial review of the different approaches to model and exchange data of software artifacts is done to finally evaluate and discuss the proper mechanisms to technically support the upcoming needs in the Software Engineering development lifecycle.},
  langid = {english}
}

@article{rymanOSLCResourceShape2013,
  title = {{{OSLC Resource Shape}}: {{A}} Language for Defining Constraints on {{Linked Data}}},
  author = {Ryman, Arthur G and Speicher, Steve},
  year = {2013},
  journal = {LDOW},
  pages = {5},
  langid = {english}
}

@inproceedings{shahinIntersectionContinuousDeployment2016,
  title = {The {{Intersection}} of {{Continuous Deployment}} and {{Architecting Process}}: {{Practitioners}}' {{Perspectives}}},
  shorttitle = {The {{Intersection}} of {{Continuous Deployment}} and {{Architecting Process}}},
  booktitle = {Proceedings of the 10th {{ACM}}/{{IEEE International Symposium}} on {{Empirical Software Engineering}} and {{Measurement}}},
  author = {Shahin, Mojtaba and Babar, Muhammad Ali and Zhu, Liming},
  year = {2016},
  month = sep,
  series = {{{ESEM}} '16},
  pages = {1--10},
  publisher = {{Association for Computing Machinery}},
  address = {{New York, NY, USA}},
  doi = {10.1145/2961111.2962587},
  abstract = {Context: Development and Operations (DevOps) is an emerging software industry movement to bridge the gap between software development and operations teams. DevOps supports frequently and reliably releasing new features and products\textendash{} thus subsuming Continuous Deployment (CD) practice. Goal: This research aims at empirically exploring the potential impact of CD practice on architecting process. Method: We carried out a case study involving interviews with 16 software practitioners. Results: We have identified (1) a range of recurring architectural challenges (i.e., highly coupled monolithic architecture, team dependencies, and ever-changing operational environments and tools) and (2) five main architectural principles (i.e., small and independent deployment units, not too much focus on reusability, aggregating logs, isolating changes, and testability inside the architecture) that should be considered when an application is (re-) architected for CD practice. This study also supports that software architecture can better support operations if an operations team is engaged at an early stage of software development for taking operational aspects into considerations. Conclusion: These findings provide evidence that software architecture plays a significant role in successfully and efficiently adopting continuous deployment. The findings contribute to establish an evidential body of knowledge about the state of the art of architecting for CD practice},
  isbn = {978-1-4503-4427-2},
  keywords = {continuous deployment,DevOps,empirical study,Software architecture}
}

@misc{speicherOSLCPrimer2019,
  title = {{{OSLC Primer}}},
  author = {Speicher, Steve},
  year = {2019},
  month = may,
  langid = {american}
}

@misc{StackStorm,
  title = {{{StackStorm}}},
  abstract = {StackStorm connects all your apps, services, and workflows. Why StackStorm? Get Started Open source and trusted by the enterprise Robust Automation Engine From simple if/then rules to complicated workflows, StackStorm lets you automate DevOps your way. See More Features... Integrates with your Existing Infrastructure No need to change your existing processes or workflows, StackStorm connects\ldots},
  howpublished = {\url{https://stackstorm.com/}},
  langid = {american},
  note = {Accessed 2022-07-28}
}

@article{stahlCindersContinuousIntegration2017,
  title = {Cinders: {{The}} Continuous Integration and Delivery Architecture Framework},
  shorttitle = {Cinders},
  author = {St{\aa}hl, Daniel and Bosch, Jan},
  year = {2017},
  month = mar,
  journal = {Information and Software Technology},
  volume = {83},
  pages = {76--93},
  issn = {0950-5849},
  doi = {10.1016/j.infsof.2016.11.006},
  abstract = {Context: The popular agile practices of continuous integration and delivery have become an essential part of the software development process in many companies, yet effective methods and tools to support design, description and communication of continuous integration and delivery systems are lacking. Objective: The work reported on in this paper addresses that lack by presenting Cinders \textemdash{} an architecture framework designed specifically to meet the needs of such systems, influenced both by prominent enterprise and software architecture frameworks as well as experiences from continuous integration and delivery modeling in industry. Method: The state of the art for systematic design and description of continuous integration and delivery systems is established through review of literature, whereupon a proposal for an architecture framework addressing requirements derived from continuous integration and delivery modeling experiences is proposed. This framework is subsequently evaluated through interviews and workshops with engineers in varying roles in three independent companies. Results: Cinders, an architecture framework designed specifically for the purpose of describing continuous integration and delivery systems is proposed and confirmed to constitute an improvement over previous methods. This work presents software professionals with a demonstrably effective method for describing their continuous integration and delivery systems from multiple points of view and supporting multiple use-cases, including system design, communication and documentation. Conclusion: It is concluded that an architecture framework for the continuous integration and delivery domain has value; at the same time potential for further improvement is identified, particularly in the area of tool support for data collection as well as for manual modeling.},
  langid = {english},
  keywords = {Architecture framework,Cinders,Continuous delivery,Continuous integration,Software integration,Software testing}
}

@article{sturtevantModularArchitecturesMake2018,
  title = {Modular {{Architectures Make You Agile}} in the {{Long Run}}},
  author = {Sturtevant, Dan},
  year = {2018},
  month = jan,
  journal = {IEEE Software},
  volume = {35},
  number = {1},
  pages = {104--108},
  issn = {1937-4194},
  doi = {10.1109/MS.2017.4541034},
  abstract = {Researchers have developed ways to think about, visualize, and measure software modularity and its erosion objectively and quantifiably. Using these techniques, you'll be able to determine whether your software is modular and identify complexity hotspots in your code that warrant further investigation.},
  keywords = {agile programming,Cognitive science,Complexity theory,Computer architecture,Design Structure Matrices,DevOps,On DevOps,Productivity,software architecture,software development,software engineering,Software engineering,Visualization}
}

@article{verborghDrawingConclusionsLinked2015,
  title = {Drawing {{Conclusions}} from {{Linked Data}} on the {{Web}}: {{The EYE Reasoner}}},
  shorttitle = {Drawing {{Conclusions}} from {{Linked Data}} on the {{Web}}},
  author = {Verborgh, Ruben and De Roo, Jos},
  year = {2015},
  month = may,
  journal = {IEEE Software},
  volume = {32},
  number = {3},
  pages = {23--27},
  issn = {1937-4194},
  doi = {10.1109/MS.2015.63},
  abstract = {Linked data represents each piece of data as a link between two things. It lets software reasoners arrive at conclusions in a human-like way. This column discusses how the EYE reasoner exploits linked data and how industry is employing EYE.},
  keywords = {Cognition,Engines,EYE,Hardware,linked data,reasoning,Resource description framework,Semantic Web,Software,software development,software engineering,Uniform resource locators}
}

@inproceedings{vergoriDevOpsPerformanceEngineering2017,
  title = {{{DevOps Performance Engineering}}: {{A Quasi-Ethnographical Study}}},
  shorttitle = {{{DevOps Performance Engineering}}},
  booktitle = {Proceedings of the 8th {{ACM}}/{{SPEC}} on {{International Conference}} on {{Performance Engineering Companion}}},
  author = {Vergori, Giuseppe and Tamburri, Damian A. and {Perez-Palacin}, Diego and Mirandola, Raffaela},
  year = {2017},
  month = apr,
  pages = {127--132},
  publisher = {{ACM}},
  address = {{L'Aquila Italy}},
  doi = {10.1145/3053600.3053628},
  abstract = {DevOps is a software engineering strategy to reduce software changes' rollout times by adopting any set of tactics that reduce friction in software lifecycles and their organisational variables, for example: coordination, communication, product evolution, deployment, operation, continuous architecting, continuous integration and more. Going DevOps is increasingly demanding that software engineering disciplines which were typically product-oriented such as software performance engineering to rethink their typical comfort zone, enlarging their scope from product, to process or even further to ecosystem and organisational levels of abstraction. This article makes an attempt at understanding what are the dimensions in DevOps organisational scenarios that can be addressed with a performance engineering lens. To do this, we performed a quasi-ethnographical study featuring a real-life industrial DevOps scenario. Discussing our results we conclude that many synergies exist between DevOps and performance engineering each with peculiarities, limitations and challenges - more research is needed to offer a full-spectrum performance-engineering support for DevOps practitioners.},
  isbn = {978-1-4503-4899-7},
  langid = {english},
  keywords = {devops performance engineering,ethnography,the phoenix project}
}

@inproceedings{wettingerStandardsBasedDevOpsAutomation2014,
  title = {Standards-{{Based DevOps Automation}} and {{Integration Using TOSCA}}},
  booktitle = {2014 {{IEEE}}/{{ACM}} 7th {{International Conference}} on {{Utility}} and {{Cloud Computing}}},
  author = {Wettinger, Johannes and Breitenb{\"u}cher, Uwe and Leymann, Frank},
  year = {2014},
  month = dec,
  pages = {59--68},
  doi = {10.1109/UCC.2014.14},
  abstract = {DevOps is an emerging paradigm to tightly integrate developers with operations personnel. This is required to enable fast and frequent releases in the sense of continuously delivering software. Users and customers of today's Web applications and mobile apps running in the Cloud expect fast feedback to problems and feature requests. Thus, it is a critical competitive advantage to be able to respond quickly. Beside cultural and organizational changes that are necessary to implement DevOps in practice, tooling is required to implement end-to-end automation of deployment processes. Automation is the key to efficient collaboration and tight integration between development and operations. The DevOps community is constantly pushing new approaches, tools, and open-source artifacts to implement such automated processes. However, as all these proprietary and heterogeneous DevOps automation approaches differ from each other, it is hard to integrate and combine them to deploy applications in the Cloud. In this paper we present a systematic classification of DevOps artifacts and show how different kinds of artifacts can be transformed toward TOSCA, an emerging standard in this field. This enables the seamless and interoperable orchestration of arbitrary artifacts to model and deploy application topologies. We validate the presented approach by a prototype implementation, show its practical feasibility by a detailed case study, and evaluate its performance.},
  keywords = {Automation,Chef,Cloud Computing,Cloud Standards,Communities,Containers,Databases,Deployment Automation,DevOps,Juju,Runtime,Servers,Topology,TOSCA,Transformation}
}

@misc{ZapierAutomationThat,
  title = {Zapier},
  howpublished = {\url{https://zapier.com/}},
  note = {Accessed 2022-07-28}
}

@article{zhangClassModelingOSLC2013,
  title = {Class {{Modeling}} of {{OSLC Resources}}},
  author = {Zhang, Weiqing},
  year = {2013},
  journal = {Research report http://urn. nb. no/URN: NBN: no-35645},
  pages = {21},
  abstract = {This report investigates the possibilities of modeling OSLC Resources with Class Models, and also transforming these models into OSLC server and client code fragments that use these resources.},
  langid = {english}
}

@inproceedings{zhangModelingToolIntegration2014,
  title = {Modeling of Tool Integration Resources with {{OSLC}} Support},
  booktitle = {2014 2nd {{International Conference}} on {{Model-Driven Engineering}} and {{Software Development}} ({{MODELSWARD}})},
  author = {Zhang, Weiqing and {M{\o}ller-Pedersen}, Birger},
  year = {2014},
  month = jan,
  pages = {99--110},
  abstract = {This paper discusses a class modeling approach for managing tool integration. Model concepts like Artifact and Role are introduced as integration backbones. Artifacts represent real artifacts like model elements that are maintained by tools. Different kinds of tools require different kinds of Artifact classes. The Role classes capture integration scenario-specific properties for Artifacts. As the same Artifact may be involved in different scenarios, and as integration scenarios may come and go, Roles can be dynamically attached to Artifacts. It is also demonstrated the possibility to model with Artifacts and Roles alone, without any real model elements. OSLC Web services (and as part of that, OSLC resources) are generated from these class models, and it is demonstrated that class modeling of Artifacts are superior to plain OSLC specification of resources.},
  keywords = {Adaptation models,Analytical models,Data models,Model,OSLC,Semantics,Software packages,Tool Chain Management,Tool Integration,Unified modeling language,Web Services,Wind turbines}
}


