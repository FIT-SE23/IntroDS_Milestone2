@article{cornia2018predicting,
  title={Predicting human eye fixations via an lstm-based saliency attentive model},
  author={Cornia, Marcella and Baraldi, Lorenzo and Serra, Giuseppe and Cucchiara, Rita},
  journal={IEEE Transactions on Image Processing},
  volume={27},
  number={10},
  pages={5142--5154},
  year={2018},
  publisher={IEEE}
}

@article{ehmke2007identifying,
  title={Identifying web usability problems from eyetracking data},
  author={Ehmke, Claudia and Wilson, Stephanie},
  year={2007}
}

@article{kroner2020contextual,
  title={Contextual encoder--decoder network for visual saliency prediction},
  author={Kroner, Alexander and Senden, Mario and Driessens, Kurt and Goebel, Rainer},
  journal={Neural Networks},
  volume={129},
  pages={261--270},
  year={2020},
  publisher={Elsevier}
}

@inproceedings{zhao2019pyramid,
  title={Pyramid feature attention network for saliency detection},
  author={Zhao, Ting and Wu, Xiangqian},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={3085--3094},
  year={2019}
}

@article{chennamma2013survey,
  title={A survey on eye-gaze tracking techniques},
  author={Chennamma, HR and Yuan, Xiaohui},
  journal={arXiv preprint arXiv:1312.6410},
  year={2013}
}

@inproceedings{shen2014webpage,
  title={Webpage saliency},
  author={Shen, Chengyao and Zhao, Qi},
  booktitle={European conference on computer vision},
  pages={33--46},
  year={2014},
  organization={Springer}
}
@misc{menges_2020, title={GazeMining: A Dataset of Video and Interaction Recordings on Dynamic Web Pages. Labels of Visual Change, Segmentation of Videos into Stimulus Shots, and Discovery of Visual Stimuli.}, url={https://zenodo.org/record/3908124#.YEghOFNKh-M}, journal={Zenodo}, author={Menges, Raphael}, year={2020}, month={Jun}}


@article{ullah2020brief,
  title={A brief survey of visual saliency detection},
  author={Ullah, Inam and Jian, Muwei and Hussain, Sumaira and Guo, Jie and Yu, Hui and Wang, Xing and Yin, Yilong},
  journal={Multimedia Tools and Applications},
  volume={79},
  number={45},
  pages={34605--34645},
  year={2020},
  publisher={Springer}
}

@article{zhang2018review,
  title={A review of co-saliency detection algorithms: Fundamentals, applications, and challenges},
  author={Zhang, Dingwen and Fu, Huazhu and Han, Junwei and Borji, Ali and Li, Xuelong},
  journal={ACM Transactions on Intelligent Systems and Technology (TIST)},
  volume={9},
  number={4},
  pages={1--31},
  year={2018},
  publisher={ACM New York, NY, USA}
}

@article{simonyan2014very,
  title={Very deep convolutional networks for large-scale image recognition},
  author={Simonyan, Karen and Zisserman, Andrew},
  journal={arXiv preprint arXiv:1409.1556},
  year={2014}
}
@article{yu2015multi,
  title={Multi-scale context aggregation by dilated convolutions},
  author={Yu, Fisher and Koltun, Vladlen},
  journal={arXiv preprint arXiv:1511.07122},
  year={2015}
}
@article{chen2017deeplab,
  title={Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs},
  author={Chen, Liang-Chieh and Papandreou, George and Kokkinos, Iasonas and Murphy, Kevin and Yuille, Alan L},
  journal={IEEE transactions on pattern analysis and machine intelligence},
  volume={40},
  number={4},
  pages={834--848},
  year={2017},
  publisher={IEEE}
}
@inproceedings{deng2009imagenet,
  title={Imagenet: A large-scale hierarchical image database},
  author={Deng, Jia and Dong, Wei and Socher, Richard and Li, Li-Jia and Li, Kai and Fei-Fei, Li},
  booktitle={2009 IEEE conference on computer vision and pattern recognition},
  pages={248--255},
  year={2009},
  organization={Ieee}
}

@inbook{10.1145/3368089.3417940,
author = {Xie, Mulong and Feng, Sidong and Xing, Zhenchang and Chen, Jieshan and Chen, Chunyang},
title = {UIED: A Hybrid Tool for GUI Element Detection},
year = {2020},
isbn = {9781450370431},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3368089.3417940},
abstract = {Graphical User Interface (GUI) elements detection is critical for many GUI automation and GUI testing tasks. Acquiring the accurate positions and classes of GUI elements is also the very first step to conduct GUI reverse engineering or perform GUI testing. In this paper, we implement a User Iterface Element Detection (UIED), a toolkit designed to provide user with a simple and easy-to-use platform to achieve accurate GUI element detection. UIED integrates multiple detection methods including old-fashioned computer vision (CV) approaches and deep learning models to handle diverse and complicated GUI images. Besides, it equips with a novel customized GUI element detection methods to produce state-of-the-art detection results. Our tool enables the user to change and edit the detection result in an interactive dashboard. Finally, it exports the detected UI elements in the GUI image to design files that can be further edited in popular UI design tools such as Sketch and Photoshop. UIED is evaluated to be capable of accurate detection and useful for downstream works. Tool URL: <a>http://uied.online</a> Github Link: <a>https://github.com/MulongXie/UIED</a>},
booktitle = {Proceedings of the 28th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {1655–1659},
numpages = {5}
}

@InProceedings{Zhou_2017_CVPR,
author = {Zhou, Xinyu and Yao, Cong and Wen, He and Wang, Yuzhi and Zhou, Shuchang and He, Weiran and Liang, Jiajun},
title = {EAST: An Efficient and Accurate Scene Text Detector},
booktitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
month = {July},
year = {2017}
}

@incollection{siedlecki1993note,
  title={A note on genetic algorithms for large-scale feature selection},
  author={Siedlecki, Wojciech and Sklansky, Jack},
  booktitle={Handbook of pattern recognition and computer vision},
  pages={88--107},
  year={1993},
  publisher={World Scientific}
}

@article{burtsev1993efficient,
  title={An efficient flood-filling algorithm},
  author={Burtsev, SV and Kuzmin, Ye P},
  journal={Computers \& graphics},
  volume={17},
  number={5},
  pages={549--561},
  year={1993},
  publisher={Elsevier}
}
@article{DBLP:journals/corr/HeZRS15,
  author    = {Kaiming He and
               Xiangyu Zhang and
               Shaoqing Ren and
               Jian Sun},
  title     = {Deep Residual Learning for Image Recognition},
  journal   = {CoRR},
  volume    = {abs/1512.03385},
  year      = {2015},
  url       = {http://arxiv.org/abs/1512.03385},
  eprinttype = {arXiv},
  eprint    = {1512.03385},
  timestamp = {Wed, 17 Apr 2019 17:23:45 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/HeZRS15.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{HFT,
  title={Visual saliency based on scale-space analysis in the frequency domain},
  author={Li, Jian and Levine, Martin D and An, Xiangjing and Xu, Xin and He, Hangen},
  journal={IEEE transactions on pattern analysis and machine intelligence},
  volume={35},
  number={4},
  pages={996--1010},
  year={2012},
  publisher={IEEE}
}

@article{DCTs,
  title={Image signature: Highlighting sparse salient regions},
  author={Hou, Xiaodi and Harel, Jonathan and Koch, Christof},
  journal={IEEE transactions on pattern analysis and machine intelligence},
  volume={34},
  number={1},
  pages={194--201},
  year={2011},
  publisher={IEEE}
}

@article{odena2016deconvolution,
  title={Deconvolution and checkerboard artifacts. Distill (2016)},
  author={Odena, Augustus and Dumoulin, Vincent and Olah, Chris},
  year={2016}
}

@inproceedings{dai2021attentional,
  title={Attentional feature fusion},
  author={Dai, Yimian and Gieseke, Fabian and Oehmcke, Stefan and Wu, Yiquan and Barnard, Kobus},
  booktitle={Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision},
  pages={3560--3569},
  year={2021}
}

@inproceedings{xie2017aggregated,
  title={Aggregated residual transformations for deep neural networks},
  author={Xie, Saining and Girshick, Ross and Doll{\'a}r, Piotr and Tu, Zhuowen and He, Kaiming},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={1492--1500},
  year={2017}
}

@inproceedings{jetley2016end,
  title={End-to-end saliency mapping via probability distribution prediction},
  author={Jetley, Saumya and Murray, Naila and Vig, Eleonora},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={5753--5761},
  year={2016}
}

@article{kingma2014adam,
  title={Adam: A method for stochastic optimization},
  author={Kingma, Diederik P and Ba, Jimmy},
  journal={arXiv preprint arXiv:1412.6980},
  year={2014}
}

@inproceedings{jiang2015salicon,
  title={Salicon: Saliency in context},
  author={Jiang, Ming and Huang, Shengsheng and Duan, Juanyong and Zhao, Qi},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={1072--1080},
  year={2015}
}

@article{itti2009bayesian,
  title={Bayesian surprise attracts human attention},
  author={Itti, Laurent and Baldi, Pierre},
  journal={Vision research},
  volume={49},
  number={10},
  pages={1295--1306},
  year={2009},
  publisher={Elsevier}
}

@inproceedings{judd2009learning,
  title={Learning to predict where humans look},
  author={Judd, Tilke and Ehinger, Krista and Durand, Fr{\'e}do and Torralba, Antonio},
  booktitle={2009 IEEE 12th international conference on computer vision},
  pages={2106--2113},
  year={2009},
  organization={IEEE}
}

@article{bylinskii2018different,
  title={What do different evaluation metrics tell us about saliency models?},
  author={Bylinskii, Zoya and Judd, Tilke and Oliva, Aude and Torralba, Antonio and Durand, Fr{\'e}do},
  journal={IEEE transactions on pattern analysis and machine intelligence},
  volume={41},
  number={3},
  pages={740--757},
  year={2018},
  publisher={IEEE}
}

@article{PETERS20052397,
title = {Components of bottom-up gaze allocation in natural images},
journal = {Vision Research},
volume = {45},
number = {18},
pages = {2397-2416},
year = {2005},
issn = {0042-6989},
doi = {https://doi.org/10.1016/j.visres.2005.03.019},
url = {https://www.sciencedirect.com/science/article/pii/S0042698905001975},
author = {Robert J. Peters and Asha Iyer and Laurent Itti and Christof Koch},
keywords = {Salience, Attention, Eye movements, Contours},
abstract = {Recent research [Parkhurst, D., Law, K., & Niebur, E., 2002. Modeling the role of salience in the allocation of overt visual attention. Vision Research 42 (1) (2002) 107–123] showed that a model of bottom-up visual attention can account in part for the spatial locations fixated by humans while free-viewing complex natural and artificial scenes. That study used a definition of salience based on local detectors with coarse global surround inhibition. Here, we use a similar framework to investigate the roles of several types of non-linear interactions known to exist in visual cortex, and of eccentricity-dependent processing. For each of these, we added a component to the salience model, including richer interactions among orientation-tuned units, both at spatial short range (for clutter reduction) and long range (for contour facilitation), and a detailed model of eccentricity-dependent changes in visual processing. Subjects free-viewed naturalistic and artificial images while their eye movements were recorded, and the resulting fixation locations were compared with the models’ predicted salience maps. We found that the proposed interactions indeed play a significant role in the spatiotemporal deployment of attention in natural scenes; about half of the observed inter-subject variance can be explained by these different models. This suggests that attentional guidance does not depend solely on local visual features, but must also include the effects of interactions among features. As models of these interactions become more accurate in predicting behaviorally-relevant salient locations, they become useful to a range of applications in computer vision and human-machine interface design.}
}

@article{le2007predicting,
  title={Predicting visual fixations on video based on low-level visual features},
  author={Le Meur, Olivier and Le Callet, Patrick and Barba, Dominique},
  journal={Vision research},
  volume={47},
  number={19},
  pages={2483--2498},
  year={2007},
  publisher={Elsevier}
}

@article{cornia2018predicting,
  title={Predicting human eye fixations via an lstm-based saliency attentive model},
  author={Cornia, Marcella and Baraldi, Lorenzo and Serra, Giuseppe and Cucchiara, Rita},
  journal={IEEE Transactions on Image Processing},
  volume={27},
  number={10},
  pages={5142--5154},
  year={2018},
  publisher={IEEE}
}

@article{ehmke2007identifying,
  title={Identifying web usability problems from eyetracking data},
  author={Ehmke, Claudia and Wilson, Stephanie},
  year={2007}
}



@inproceedings{zhao2019pyramid,
  title={Pyramid feature attention network for saliency detection},
  author={Zhao, Ting and Wu, Xiangqian},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={3085--3094},
  year={2019}
}

@article{chennamma2013survey,
  title={A survey on eye-gaze tracking techniques},
  author={Chennamma, HR and Yuan, Xiaohui},
  journal={arXiv preprint arXiv:1312.6410},
  year={2013}
}


@article{ullah2020brief,
  title={A brief survey of visual saliency detection},
  author={Ullah, Inam and Jian, Muwei and Hussain, Sumaira and Guo, Jie and Yu, Hui and Wang, Xing and Yin, Yilong},
  journal={Multimedia Tools and Applications},
  volume={79},
  number={45},
  pages={34605--34645},
  year={2020},
  publisher={Springer}
}



@article{qin2020u2,
  title={U2-Net: Going deeper with nested U-structure for salient object detection},
  author={Qin, Xuebin and Zhang, Zichen and Huang, Chenyang and Dehghan, Masood and Zaiane, Osmar R and Jagersand, Martin},
  journal={Pattern Recognition},
  volume={106},
  pages={107404},
  year={2020},
  publisher={Elsevier}
}
@misc{wikipedia_2021, title={Eye tracking}, url={https://en.wikipedia.org/wiki/Eye_tracking}, journal={Wikipedia}, publisher={Wikimedia Foundation}, year={2021}, month={Oct}}

@misc{salicon, title={Home}, url={http://salicon.net/}, journal={SALICON}}

@article{DBLP:journals/corr/abs-1803-03391,
  author    = {Runmin Cong and
               Jianjun Lei and
               Huazhu Fu and
               Ming{-}Ming Cheng and
               Weisi Lin and
               Qingming Huang},
  title     = {Review of Visual Saliency Detection with Comprehensive Information},
  journal   = {CoRR},
  volume    = {abs/1803.03391},
  year      = {2018},
  url       = {http://arxiv.org/abs/1803.03391},
  eprinttype = {arXiv},
  eprint    = {1803.03391},
  timestamp = {Mon, 13 Aug 2018 16:47:58 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1803-03391.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}



@article{LIN200491,
title = {Design of a computer game using an eye-tracking device for eye's activity rehabilitation},
journal = {Optics and Lasers in Engineering},
volume = {42},
number = {1},
pages = {91-108},
year = {2004},
issn = {0143-8166},
doi = {https://doi.org/10.1016/S0143-8166(03)00075-7},
url = {https://www.sciencedirect.com/science/article/pii/S0143816603000757},
author = {Chern-Sheng Lin and Chia-Chin Huan and Chao-Ning Chan and Mau-Shiun Yeh and Chuang-Chien Chiu},
keywords = {Eye mouse, Eye-tracking system, Rehabilitation, Eye's activity, Cross-line tracking algorithms},
abstract = {An eye mouse interface that can be used to operate a computer using the movement of the eyes is described. We developed this eye-tracking system for eye motion disability rehabilitation. When the user watches the screen of a computer, a charge-coupled device will catch images of the user's eye and transmit it to the computer. A program, based on a new cross-line tracking and stabilizing algorithm, will locate the center point of the pupil in the images. The calibration factors and energy factors are designed for coordinate mapping and blink functions. After the system transfers the coordinates of pupil center in the images to the display coordinate, it will determine the point at which the user gazed on the display, then transfer that location to the game subroutine program. We used this eye-tracking system as a joystick to play a game with an application program in a multimedia environment. The experimental results verify the feasibility and validity of this eye-game system and the rehabilitation effects for the user's visual movement.}
}

@inproceedings{10.1145/3204949.3208111,
author = {Kim, HyunWook and Yang, JinWook and Choi, MinSu and Lee, JunSuk and Yoon, SangPil and Kim, YoungHwa and Park, WooChool},
title = {Eye Tracking Based Foveated Rendering for 360 VR Tiled Video},
year = {2018},
isbn = {9781450351928},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3204949.3208111},
doi = {10.1145/3204949.3208111},
abstract = {To increase the sense of immersion of 360VR images, we have proposed and implemented foveated rendering technology through precision region-of-interest detection using eye-tracking-based head-mounted display equipment for a high-efficiency video coding tiled video-based image-decoding and -rendering method. Our method can provide a high rendering speed and high-quality textures.},
booktitle = {Proceedings of the 9th ACM Multimedia Systems Conference},
pages = {484–486},
numpages = {3},
keywords = {tiled video, live streaming, foveated rendering, MPEG-DASH, 360 VR, eye-tracking, MPEG-DASH SRD},
location = {Amsterdam, Netherlands},
series = {MMSys '18}
}

@inproceedings{10.1145/3314111.3319846,
author = {Liu, Congcong and Chen, Yuying and Tai, Lei and Ye, Haoyang and Liu, Ming and Shi, Bertram E.},
title = {A Gaze Model Improves Autonomous Driving},
year = {2019},
isbn = {9781450367097},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3314111.3319846},
doi = {10.1145/3314111.3319846},
abstract = {End-to-end behavioral cloning trained by human demonstration is now a popular approach for vision-based autonomous driving. A deep neural network maps drive-view images directly to steering commands. However, the images contain much task-irrelevant data. Humans attend to behaviorally relevant information using saccades that direct gaze towards important areas. We demonstrate that behavioral cloning also benefits from active control of gaze. We trained a conditional generative adversarial network (GAN) that accurately predicts human gaze maps while driving in both familiar and unseen environments. We incorporated the predicted gaze maps into end-to-end networks for two behaviors: following and overtaking. Incorporating gaze information significantly improves generalization to unseen environments. We hypothesize that incorporating gaze information enables the network to focus on task critical objects, which vary little between environments, and ignore irrelevant elements in the background, which vary greatly.},
booktitle = {Proceedings of the 11th ACM Symposium on Eye Tracking Research &amp; Applications},
articleno = {33},
numpages = {5},
keywords = {imitation learning, autonomous driving, eye tracking},
location = {Denver, Colorado},
series = {ETRA '19}
}
@inproceedings{10.1145/3064663.3064762,
author = {Kane, Shaun K. and Morris, Meredith Ringel},
title = {Let's Talk About X: Combining Image Recognition and Eye Gaze to Support Conversation for People with ALS},
year = {2017},
isbn = {9781450349222},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3064663.3064762},
doi = {10.1145/3064663.3064762},
abstract = {Communicating at a natural speed is a significant challenge for users of augmentative and alternative communication (AAC) devices, especially when input is provided by eye gaze, as is common for people with ALS and similar conditions. One way to improve AAC throughput is by drawing on contextual information from the outside world. Toward this goal, we present SceneTalk, a prototype gaze-based AAC system that uses computer vision to identify objects in the user's field of view and suggests words and phrases related to the current scene. We conducted a formative evaluation of SceneTalk with six people with ALS, in which we evaluated their preference for user interface modes and output preferences. Participants agreed that integrating contextual awareness into their AAC device could be helpful across a diverse range of situations.},
booktitle = {Proceedings of the 2017 Conference on Designing Interactive Systems},
pages = {129–134},
numpages = {6},
keywords = {augmentative and alternative communication, als, eye gaze, computer vision, assistive technology},
location = {Edinburgh, United Kingdom},
series = {DIS '17}
}

@INPROCEEDINGS{8248276,
  author={Xu, Jiao and Zhao, Qijie},
  booktitle={2017 4th International Conference on Systems and Informatics (ICSAI)}, 
  title={Adaptive calibration method based on state space model for eye gaze HCI system}, 
  year={2017},
  volume={},
  number={},
  pages={127-131},
  doi={10.1109/ICSAI.2017.8248276}}
  
  @article{clemotte2014accuracy,
  title={Accuracy and precision of the Tobii X2-30 eye-tracking under non ideal conditions},
  author={Clemotte, A and Velasco, M and Torricelli, D and Raya, R and Ceres, R},
  journal={Eye},
  volume={16},
  number={3},
  pages={2},
  year={2014}
}

@article{vspakov2007visualization,
  title={Visualization of eye gaze data using heat maps},
  author={{\v{S}}pakov, Oleg and Miniotas, Darius},
  journal={Elektronika ir elektrotechnika},
  volume={74},
  number={2},
  pages={55--58},
  year={2007}
}

@ARTICLE{5288526,  author={Pan, Sinno Jialin and Yang, Qiang},  journal={IEEE Transactions on Knowledge and Data Engineering},   title={A Survey on Transfer Learning},   year={2010},  volume={22},  number={10},  pages={1345-1359},  doi={10.1109/TKDE.2009.191}}

@incollection{poole2006eye,
  title={Eye tracking in HCI and usability research},
  author={Poole, Alex and Ball, Linden J},
  booktitle={Encyclopedia of human computer interaction},
  pages={211--219},
  year={2006},
  publisher={IGI global}
}


@article{caruana1997multitask,
  title={Multitask learning},
  author={Caruana, Rich},
  journal={Machine learning},
  volume={28},
  number={1},
  pages={41--75},
  year={1997},
  publisher={Springer}
}

@inproceedings{ngiam2011multimodal,
  author={Ngiam, Jiquan and Khosla, Aditya and Kim, Mingyu and Nam, Juhan and Lee, Honglak and Ng, Andrew Y},
  booktitle={ICML},
  year={2011}
}

@article{caruana1997multitask,
  title={Multitask learning},
  author={Caruana, Rich},
  journal={Machine learning},
  volume={28},
  number={1},
  pages={41--75},
  year={1997},
  publisher={Springer}
}

@article{ruder2017overview,
  title={An overview of multi-task learning in deep neural networks},
  author={Ruder, Sebastian},
  journal={arXiv preprint arXiv:1706.05098},
  year={2017}
}

@inproceedings{bengio2007greedy,
  title={Greedy layer-wise training of deep networks},
  author={Bengio, Yoshua and Lamblin, Pascal and Popovici, Dan and Larochelle, Hugo},
  booktitle={Advances in neural information processing systems},
  pages={153--160},
  year={2007}
}

@inproceedings{liu2019end,
  title={End-to-end multi-task learning with attention},
  author={Liu, Shikun and Johns, Edward and Davison, Andrew J},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={1871--1880},
  year={2019}
}







@article{doi:10.1177/0170840607075264,
author = {Rick Iedema},
title ={On the Multi-modality, Materially and Contingency of Organization Discourse},
journal = {Organization Studies},
volume = {28},
number = {6},
pages = {931-946},
year = {2007},
doi = {10.1177/0170840607075264},

URL = { 
        https://doi.org/10.1177/0170840607075264
    
},
eprint = { 
        https://doi.org/10.1177/0170840607075264
    
}
,
    abstract = { This essay considers the ways that organizational discourse studies have deployed the concept `discourse'. A review of the literature reveals conceptual ambiguities in the definition of `discourse', as well as pre-analytical distinctions that are imposed between discourse, action and text, and between discourse, beliefs and material practices. The paper suggests that such a priori analytical categories risk tying the researcher to an inflexible research agenda, ruling out engaging with organizational specifics and emergent aspects of practice. The essay argues for an alternative view of discourse that centres on the following three arguments: discourse is not limited to language but also includes image, design, technology and other modes of meaning making; discourse and materiality co-emerge; and discourse manifests a specific, historically situated form of life. }
}

@article{kiros2014unifying,
  title={Unifying visual-semantic embeddings with multimodal neural language models},
  author={Kiros, Ryan and Salakhutdinov, Ruslan and Zemel, Richard S},
  journal={arXiv preprint arXiv:1411.2539},
  year={2014}
}

@article{liu2020analyzing,
  title={Analyzing periodicity and saliency for adult video detection},
  author={Liu, Yizhi and Gu, Xiaoyan and Huang, Lei and Ouyang, Junlin and Liao, Miao and Wu, Liangran},
  journal={Multimedia Tools and Applications},
  volume={79},
  number={7},
  pages={4729--4745},
  year={2020},
  publisher={Springer}
}
@inproceedings{blascheck2017visualization,
  title={Visualization of eye tracking data: A taxonomy and survey},
  author={Blascheck, Tanja and Kurzhals, Kuno and Raschke, Michael and Burch, Michael and Weiskopf, Daniel and Ertl, Thomas},
  booktitle={Computer Graphics Forum},
  volume={36},
  number={8},
  pages={260--284},
  year={2017},
  organization={Wiley Online Library}
}

@article{niebur2007saliency,
  title={Saliency map},
  author={Niebur, Ernst},
  journal={Scholarpedia},
  volume={2},
  number={8},
  pages={2675},
  year={2007}
}

@article{ullah2020brief,
  title={A brief survey of visual saliency detection},
  author={Ullah, Inam and Jian, Muwei and Hussain, Sumaira and Guo, Jie and Yu, Hui and Wang, Xing and Yin, Yilong},
  journal={Multimedia Tools and Applications},
  volume={79},
  number={45},
  pages={34605--34645},
  year={2020},
  publisher={Springer}
}

@inproceedings{kong2016hypernet,
  title={Hypernet: Towards accurate region proposal generation and joint object detection},
  author={Kong, Tao and Yao, Anbang and Chen, Yurong and Sun, Fuchun},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={845--853},
  year={2016}
}

@inproceedings{ronneberger2015u,
  title={U-net: Convolutional networks for biomedical image segmentation},
  author={Ronneberger, Olaf and Fischer, Philipp and Brox, Thomas},
  booktitle={International Conference on Medical image computing and computer-assisted intervention},
  pages={234--241},
  year={2015},
  organization={Springer}
}

@article{sun2018face,
  title={Face detection using deep learning: An improved faster RCNN approach},
  author={Sun, Xudong and Wu, Pengcheng and Hoi, Steven CH},
  journal={Neurocomputing},
  volume={299},
  pages={42--50},
  year={2018},
  publisher={Elsevier}
}

@inproceedings{bodla2017soft,
  title={Soft-NMS--improving object detection with one line of code},
  author={Bodla, Navaneeth and Singh, Bharat and Chellappa, Rama and Davis, Larry S},
  booktitle={Proceedings of the IEEE international conference on computer vision},
  pages={5561--5569},
  year={2017}
}

@article{goferman2011context,
  title={Context-aware saliency detection},
  author={Goferman, Stas and Zelnik-Manor, Lihi and Tal, Ayellet},
  journal={IEEE transactions on pattern analysis and machine intelligence},
  volume={34},
  number={10},
  pages={1915--1926},
  year={2011},
  publisher={Ieee}
}
@article{houx2012image,
  title={Image signature: highlighting sparse salient regions},
  author={Houx, D and HAREL, J and KOCH, C},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence},
  volume={34},
  number={1},
  pages={194--201},
  year={2012}
}

@article{li2012visual,
  title={Visual saliency based on scale-space analysis in the frequency domain},
  author={Li, Jian and Levine, Martin D and An, Xiangjing and Xu, Xin and He, Hangen},
  journal={IEEE transactions on pattern analysis and machine intelligence},
  volume={35},
  number={4},
  pages={996--1010},
  year={2012},
  publisher={IEEE}
}

@article{hou2008dynamic,
  title={Dynamic visual attention: Searching for coding length increments},
  author={Hou, Xiaodi and Zhang, Liqing},
  journal={Advances in neural information processing systems},
  volume={21},
  year={2008}
}
@inproceedings{riche2012rare,
  title={Rare: A new bottom-up saliency model},
  author={Riche, Nicolas and Mancas, Matei and Gosselin, Bernard and Dutoit, Thierry},
  booktitle={2012 19th IEEE International Conference on Image Processing},
  pages={641--644},
  year={2012},
  organization={IEEE}
}
@inproceedings{hou2007saliency,
  title={Saliency detection: A spectral residual approach},
  author={Hou, Xiaodi and Zhang, Liqing},
  booktitle={2007 IEEE Conference on computer vision and pattern recognition},
  pages={1--8},
  year={2007},
  organization={Ieee}
}
@article{seo2009static,
  title={Static and space-time visual saliency detection by self-resemblance},
  author={Seo, Hae Jong and Milanfar, Peyman},
  journal={Journal of vision},
  volume={9},
  number={12},
  pages={15--15},
  year={2009},
  publisher={The Association for Research in Vision and Ophthalmology}
}

@article{brooke2013sus,
  title={SUS: a retrospective},
  author={Brooke, John},
  journal={Journal of usability studies},
  volume={8},
  number={2},
  pages={29--40},
  year={2013},
  publisher={Usability Professionals' Association Bloomingdale, IL}
}

@inproceedings{selvaraju2017grad,
  title={Grad-cam: Visual explanations from deep networks via gradient-based localization},
  author={Selvaraju, Ramprasaath R and Cogswell, Michael and Das, Abhishek and Vedantam, Ramakrishna and Parikh, Devi and Batra, Dhruv},
  booktitle={Proceedings of the IEEE international conference on computer vision},
  pages={618--626},
  year={2017}
}

@article{aspandi2022user,
  title={User interaction analysis through contrasting websites experience},
  author={Aspandi, Decky and Doosdal, Sarah and {\"U}lger, Victor and Gillich, Lukas and Menges, Raphael and Hedeshy, Ramin and Kumar, Chandan and Schaefer, Christoph and Walber, Tina and Staab, Steffen},
  journal={arXiv preprint arXiv:2201.03638},
  year={2022}
}

@article{pan2017salgan,
  title={Salgan: Visual saliency prediction with generative adversarial networks},
  author={Pan, Junting and Ferrer, Cristian Canton and McGuinness, Kevin and O'Connor, Noel E and Torres, Jordi and Sayrol, Elisa and Giro-i-Nieto, Xavier},
  journal={arXiv preprint arXiv:1701.01081},
  year={2017}
}
@article{treisman1980feature,
  title={A feature-integration theory of attention},
  author={Treisman, Anne M and Gelade, Garry},
  journal={Cognitive psychology},
  volume={12},
  number={1},
  pages={97--136},
  year={1980},
  publisher={Elsevier}
}

@article{wolfe1994guided,
  title={Guided search 2.0 a revised model of visual search},
  author={Wolfe, Jeremy M},
  journal={Psychonomic bulletin \& review},
  volume={1},
  number={2},
  pages={202--238},
  year={1994},
  publisher={Springer}
}

@article{koch1999predicting,
  title={Predicting the visual world: silence is golden},
  author={Koch, Christof and Poggio, Tomaso},
  journal={nature neuroscience},
  volume={2},
  number={1},
  pages={9--10},
  year={1999},
  publisher={Nature Publishing Group}
}

@book{koffka2013principles,
  title={Principles of Gestalt psychology},
  author={Koffka, Kurt},
  year={2013},
  publisher={Routledge}
}

@article{li2002saliency,
  title={A saliency map in primary visual cortex},
  author={Li, Zhaoping},
  journal={Trends in cognitive sciences},
  volume={6},
  number={1},
  pages={9--16},
  year={2002},
  publisher={Elsevier}
}