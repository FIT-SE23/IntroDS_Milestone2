\documentclass[a4paper,twoside]{article}

\usepackage{epsfig}
\usepackage{subcaption}
\usepackage{calc}
\usepackage{amssymb}
\usepackage{amstext}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{multicol}
\usepackage{pslatex}
\usepackage{apalike}
\usepackage[bottom]{footmisc}
\usepackage{xcolor}
\usepackage{multirow}
\everypar{\looseness=-1}

\usepackage{SCITEPRESS}     % Please add other packages that you may need BEFORE the SCITEPRESS.sty package.



\begin{document}

\title{Predicting Eye Gaze Location on Websites}
%\orcidAuthor{0000-0000-0000-0000}
\author{\authorname{Ciheng Zhang\sup{1}, Decky Aspandi\sup{2}, Steffen Staab\sup{2}\sup{3}} %\orcidAuthor{0000-0000-0000-0000}
\affiliation{\sup{1}Institute of Industrial Automation and Software Engineering,  University of Stuttgart, Stuttgart, Germany}
\affiliation{\sup{2}Institute for Parallel and Distributed Systems,  University of Stuttgart,  Stuttgart, Germany}
\affiliation{\sup{3}Web and Internet Science, University of Southampton, Southampton, United Kingdom}
\email{st169011@stud.uni-stuttgart.de,\{decky.aspandi-latif, steffen.staab\}@ipvs.uni-stuttgart.de}
}

\keywords{Eye-Gaze Saliency, Image Translation, Visual Attention}
%it has become a brand new topic to design the web structure to maximize information utilization by better analyzing the behavioral patterns to optimise the website interactions. However, there doesn't exist a sizable dataset of eye-gaze on website, and there aren't any studies leveraging deep learning techniques that are concentrated on website scenarios. 

\abstract{World-wide-web, with website and webpage as a main interface, facilitates dissemination of important information. Hence it is crucial to optimize webpage design for better user interaction, which is primarily done by analyzing users' behavior, especially users' eye-gaze locations on the webpage. However, gathering these data is still considered to be labor and time intensive. In this work, we enable the development of automatic eye-gaze estimations given webpage screenshots as input by curating of a unified dataset that consists of webpage screenshots, eye-gaze heatmap and website's layout information in the form of image and text masks. Our curated dataset allows us to propose a deep learning-based model that leverages on both webpage screenshot and content information (image and text spatial location), which are then combined through attention mechanism for effective eye-gaze prediction. In our experiment, we show benefits of careful fine-tuning using our unified dataset to improve accuracy of eye-gaze predictions. We further observe the capability of our model to focus on targeted areas (images and text) to achieve accurate eye-gaze area predictions. Finally, comparison with other alternatives shows state-of-the-art result of our approach, establishing a benchmark for webpage based eye-gaze prediction task. }

%substantial accuracy is obtained the training is essential model is accurate for website eye gaze predict scenes and has targeted attention to web-specific structures. 

%In future work, more modalities (e.g., mouse trajectory) will be introduced to further improve the accuracy of the model for website eye-gaze prediction
%Designing a deep learning saliency detection model for web scenes to predict the eye gaze data while browsing web pages would be a promising solution to this problem.

%and an accurate attentional multi-modal model is designed and evaluated. In this work, a model for web scenes is designed, which was generated image masks and text masks to assist in prediction by taking advantage of the highly laid out and structured nature of web pages.


%In recent computer vision research visual attention prediction is popular. Based on those research people can efficiently predict the salient object in one image and through this information to better analysis the figure. Meanwhile, the number of Internet views is on the rise. However, the majority of the studies are focus on the naturally pictures, which contains Superficial characteristics such as rich color contrast. These features are not the most crucial part of a web page that attracts human attention, but deeper features, such as web page layout, web page information, etc., affect the attention of browsers more.}

% Problem, objective (solution), method, results.}
%!!deckyal: complete the abstract. 

% Thus, the main objective of this work is a new model aims to saliency prediction on website snapshot design.
\onecolumn \maketitle \normalsize \setcounter{footnote}{0} \vfill

\section{\uppercase{Introduction}}
\label{sec:introduction}

%
%It has been observed that humans tend to lose focus on each region of the image when there is a lack of motivation, such as the absence of a specific task to perform~\cite{cornia2018predicting}. This information could be utilized to analyze the characteristics of attention from users during web browsing (which uses the rendering of the website, i.e., visualizing information similar to images as a means of interaction). As such, we can try to synthesize different attention characteristics of users when dealing with web browsing scenarios and with general images, thus opening up the possibility to transfer the existing methods from visual science to predict such attention. This general attention has been formulated as saliency detection~\cite{cornia2018predicting}.

%got -> acquired, achieve, obtain, attain
%to avoid qualitative descriptions. Excellent, magnificent, esteem. 

%1. what are you doing (what is the task)
% predict eye gaze on website
%2. What is beneficial 
% better design website sructure
%3. what is the problem 
    %1. how can we do the prediction
    % transform saliency model and add website speicial information(masks)
    %2. from 1.1, what is other problem 
    % limit of dataset -> gathering our dataset

%combine1
%gaze data userful
Analysis of user behavior during their interaction with website (and contained webpages) is important to evaluate website's overall quality. This information can be used to create more optimized webpages for better interactions, as such, there are needs to characterize these behaviors. Some of the characteristics include users' tendency to focus on certain areas (e.g. upper left corner) during browsing~\cite{shen2014webpage}. Other important characteristics can be defined from users' eye-gaze data, which normally represents visual attention of the users during website interaction. This data is usually acquired from human pupil-retinal location and movement relative to the object of interest that allows one to pinpoint exact webpage area, where users focus during their interactions and its relations with overall webpage structure. Although this information is crucial to create a more optimized website for better interactions, However, acquiring these eye-gaze data from every user during their browsing duration is difficult, given the complexity of acquisition setting. Thus there is currently a need for an automatic approach to predict these eye-gaze locations given observation of the webpages.


%generated a eye gaze heat map = saliency map generation
In computer vision field, saliency prediction task has been extensively investigated that allows estimation of people's attention given an image. The main task is to estimate saliency map, which highlights first location (or region) where observers' eyes focus, with photographs of natural scenes~\cite{zhang2018review,kroner2020contextual} commonly used as input. Number of methods have been developed so far to solve this task, including machine learning~\cite{hou2008dynamic,li2012visual} and recently deep learning based approaches~\cite{kroner2020contextual} with quite high accuracy achieved. With this progress, there is an opportunity to adopt these automatic, visual based saliency predictors for eye-gaze predictions task, by representing input observation in a form of webpage screenshot. This enables adaptation of task objective for eye-gaze heatmap prediction, in lieu of saliency map, with both predicted maps representing area where most people (or in our case users) attend. 


The challenges however are differences between saliency and eye-gaze prediction tasks in terms of content, structure and layout of input images. For instance, there is not any concept of depth on webpage screenshot as opposed to natural images, an aspect that is highly utilized for general visual saliency estimation. In addition, high contrast and varied colored areas on natural image are commonly regarded as saliency area, which may not be the case for webpage screenshots, given that user eye-gaze locations are highly dependent on the type of interactions during website browsing. This problem can be rectified by the use of a data-driven approach~\cite{5288526}, which can be made feasible through fine-tuning using a specialized dataset on a particular task. However, this attempt is currently hampered by the lack of such dataset. 

In this work, we curate a generalized dataset of eye-gaze prediction from webpage screenshots to enable effective training for machine and deep learning approaches. Using this dataset, we propose a deep learning based method that incorporates image and textual locations of webpage through mask modalities and combines them with attention fusion mechanism. We then evaluate the impact of transfer learning, including comparison with other alternatives, to establish a benchmark for this task. Specifically, the contributions of this work are: 

\begin{enumerate} 
    % \item We transformed the saliecny detection model for natural image to website situation and evaluation the result resulting to a new model aims for saliency on website designed.
    \item The establishment of a unified benchmark dataset for eye-gaze detection from webpage screenshots, derived from website and user interactions data. 
    \item A novel deep learning-based and multi-modal eye-gaze detector with internal attention that leverages characteristics of input contents and importance of each stream of modality for accurate eye-gaze predictions.
    \item Benchmark results of automatic eye-gaze location estimators and our state-of-the-art results for eye-gaze detection task given webpage screenshot inputs.
\end{enumerate}

%Thus fine-tuning is required. 

%However currently there are none sizeable dataset to allow for fine-tuning, and to benefit from current approach that largely use deep learning method. 

%Furthermore, In general visual , in the general visual saliency prediction task


%for central position of the image, the highly varied colour area, and the composition of the foreground and rear view to have a dominant role in terms of saliency. 


%In the case of web pages~\cite{shen2014webpage} however, human attention on web pages tends to differ from that on pictures of natural landscapes, as they do not have the depth of field and humans tend to be more focused on the information they are looking for during their interaction. 



%Over the past few years, numerous researchers have proposed different models to predict saliency in a variety of different scenarios. Current approaches model is an Encoder and Decoder structure with pre-trained VGG network~\cite{kroner2020contextual} with the auto-encoder.

%descript about saliency prediction

 


% differnt bewteen saliency detection and website eye gaze

% we colloect our dataset and our model to solve the diffenerntlty from normal saliency prediction
%Therefore, we gather our website eye-gaze dataset and fine-tune the saliency prediction model by our dataset. Furthermore, we designed a new deep learning model which can utilize the Structural features (e.g. layout) of webpages. 




%Therefore, we can transform the saliency prediction model to website eye-gaze prediction situation to slove the problems.
%we can better develop new web structures or more efficient website interaction in the form of better layouting, proper ad placements etc. 



%The eye-gaze heatmap for webpage screenshot which present the normal user attention common habit. With help of eye-gaze heatmap we can designed convenient novel website structure for most people.


%get gaze data for everyone is hard
 % because of platform-dependent of gaze data collection.

%shown to be beneficial in many fields including virtual reality and the health domain. However, in the current state, gaze data is often very small in quantity due to the complexity of the experiments required for acquisition and is highly platform dependent. Thus limiting their scale of operations.% Furthermore, eye-tracking devices tend to be expensive and largely platform-dependent (thus hindering their true potential applications to a wider breadth of fields or scenarios)

%exist common habit for website browsing 

%However, gathering this data is both labor and time intensive, thus limiting 

% predict the eyegaze heat map can present the user common habit 
%This information can be beneficial as a step toward 

%If we can predict the tendency of people to pay attention when browsing the web, the eye-gaze data collecting from each user is not necessary. 



%combine2

% to do this is using saleincy. however, it is not generalized enough to eye-gaze predictions. 
%Many researchers have concentrated on the study of saliency detection. They utilize deep learning methods, especially convolutional neural networks (CNN), to analyze the attention of various images. Over the past few years, numerous researchers have proposed different models to predict saliency in a variety of different scenarios. Current approaches include the application of a Pyramid Feature Attention Network, which utilizes a structure to extract image features in layers and stack multi-layer features ~\cite{zhao2019pyramid} ~\cite{zhang2018review}. Another work proposes an Encoder and Decoder structure with pre-trained VGG network~\cite{kroner2020contextual} with the auto-encoder.

%Current studies and datasets commonly use photographs of natural scenes~\cite{zhang2018review} ~\cite{kroner2020contextual}. In these cases, the central position of the image, the color area, and the composition of the foreground and rear view play a dominant role in terms of saliency. In the case of web pages~\cite{shen2014webpage}, however, human attention on web pages tends to differ from that on pictures of natural landscapes, as they do not have the depth of field and humans tend to be more focused on the information they are looking for.

%However, this is slightly different in the case of web browsing scenarios where higher-level features such as content, logos, etc. are considered more attractive to the users. This situation leads to our hypothesis that cross-task applications of saliency (or attention) detection models trained on natural images for web browsing scenarios are possible. This is because there are a number of similarities between web screenshots and natural images. For example, both of them are represented as three-channeled images (Red, Green, and Blue format, RGB) and therefore possess other derived low-level features such as color, contrast, and orientation to name a few. These features could have a common effect on the process of saliency detection. In this case, we are interested in predicting the eye-gaze heatmap representation given its widespread use. That is, where the specific locations of the pixels are accentuated depending on the duration of the fixations corresponding to these pixels.

%objective


% and evaluated the predictive effect. Subsequently, a new efficient neural network model for attention analysis of human web browsing was created. Finally, the model's effectiveness to predict web gaze locations were evaluated and analyzed.



\section{\uppercase{Related Work}}
%starting from the 5th second
An early example of work analyzing website content is done by Zhao et.al.~\cite{shen2014webpage} where three types of webpages are analyzed: Text-based, Pictorial-based, and Mixed (combination of Text and Pictorial) websites. The author show that some attention characteristics of users during their interactions with webpage exist, with main finding that users usually pay more attention to some relevant parts of websites (such as left-top corner of a website) and their tendency to focus on areas where large images are present. Furthermore, on the websites from ‘Text’ category, their preference to focus on certain parts of websites (middle left and bottom left regions) is perceived. Lastly, they propose multi-kernel machine learning inferences for eye-gaze heatmap predictions (which represents visual attention of users) for their developed website eye-gaze dataset of Fixations in Webpage Images dataset (FiWI).
%Furthermore, the observed fixations on the other two categories ('Mixed' and 'Pictorial') are more evenly distributed across all of the webpage locations.
%This is done while maximizing the entropy of the sampled visual features to achieve attention selectivity in both static and dynamic scenes.

In computer vision field, the task of locating (or predicting) users' attention to natural image is commonly called as saliency prediction. These tasks are commonly solved by utilising machine learning techniques, given their automation capability. An earliest example of this method is Incremental Coding Length (ICL) ~\cite{hou2008dynamic} that aims to predict the activation of saliency location by measuring perspective entropy gain of each input feature (several image patches) as a linear combination of sparse coding basis function. Other algorithm of Context-Aware Saliency Detection (CASD) capitalises on the concept of dominant objects as additional context to improve their prediction~\cite{goferman2011context}. Furthermore, D Houx et.al. ~\cite{houx2012image} propose an approach that aims to solve figure-ground separation problem for prediction. They use a binary and holistic image descriptor of Image Signature, which is defined as sign function of Discrete Cosine Transform (DCT) of an image as additional input. Subsequently, Hypercomplex Fourier Transform (HFT)~\cite{li2012visual} is used to transform input image to frequency domain to estimate saliency map. One recent work utilizes deep learning based method, given their accurate estimations and ability to leverage huge datasets size (that are increasingly present). This approach is based on Encoder and Decoder structure with a pre-trained VGG network to predict saliency maps~\cite{kroner2020contextual}. 

Even though all of the described methods work for general visual saliency predictions, however, their capability to predict users' eye-gaze location on webpage is not yet investigated, given lack of dataset available. Thus in this work, we propose to create a specialized webpage based eye-gaze prediction datasets to allow for the development of automatic eye-gaze predictions of webpage screenshot inputs, then utilize it to develop our deep learning based eye-gaze location predictor.


%One recent example is Pyramid Feature Attention Network, which utilizes a structure to extract image features in layers and stack multi-layer features ~\cite{zhao2019pyramid} ~\cite{zhang2018review}. 

%Meanwhile, the method of using adversarial neural network salGAN~\cite{pan2017salgan} to implement predictive browsing data is designed.


%RARE~\cite{riche2012rare} is a new bottom-up visual saliency model based on the idea that locally contrasted and globally rare features are salient. The method from Seo et. al.~\cite{seo2009static} is a bottom-up approach and computes so-called local regression kernels (i.e., local descriptors) from the given image (or a video), which measure the likeness of a pixel (or voxel) to its surroundings. 
%SR ~\cite{hou2008dynamic} is a method which by analyzing the log-spectrum of an input image to extract the spectral residual of an image in the spectral domain, and proposes a fast method to construct the corresponding saliency map in the spatial domain.
%!!deckyal: sort by date. and then connect 

\begin{figure*}[h!]
    \centering
    \includegraphics[width=0.8\linewidth]{SCITEPRESS_Conference_Latex/data-10.png}
    \caption{The flow of dataset generation.}
    \label{fig:datsáset}
\end{figure*}




\section{\uppercase{Methodology}}
\label{sec:meth}


\subsection{Dataset Gathering and Processing}
%guo du chong xie
%the intention is to unify these different datasets that never been used as common benchmark.
%!decky: simplify the explanation fo the dataset. 
% that is fundamental to enable the development of effective eye-gaze location prediction and respective benchmarks.
\label{sec:dataset}
In this work, we search, process and curate a generalized eye-gaze dataset from available datasets in literature. Here, we focus on three publicly available user-website interaction datasets (where eye-gaze and webpage screenshots are present): GazeMining~\cite{menges_2020}, Contrastive Website~\cite{aspandi2022user}, and Fixations in Webpage Images (FiWI) dataset~\cite{shen2014webpage}. In this section, we provide a short description of datasets, and proceed with description of pre-processing algorithm that we conduct to produce our unified dataset. The processed dataset is available at \footnote{https://doi.org/10.18419/darus-3251} and available after request. 


%\subsubsection{GazeMining}
%\label{subsec:gaze}
%GazeMining is a dataset of video and interaction recordings on dynamic web pages ~\cite{menges_2020}. They focus on the website which contains the changing virtual labels. The data is collected on March 2019 from four participants and they browsed 12 different websites. However, the image quality is not impressive and contains many incomplete images (only the dynamic parts are saved) and black frames of the images due to recorded screenshots. These were heavily pre-processed to ensure that we could utilize them.%!decky what is the challenge

\begin{itemize}
    \item \textbf{GazeMining} is a dataset of video and interaction recordings on dynamic webpages ~\cite{menges_2020}. In this work, the authors focused on websites which contain constantly changing scenes (thus dynamic). The data was collected on March 2019 from four participants who interacted with 12 different websites. %!decky what is the challenge
    \item \textbf{Contrastive Website dataset} is a recent website interaction-based dataset, where participants were asked to visit two sets of websites (each of four) and performed respective tasks (flight planning, route search, new searching and online shopping) resulting on more than 160 sessions~\cite{aspandi2022user}.  %, and each subject was asked to visit 153 web pages. An experimental monitor with 1920 x 1080 resolution and 60 Hz is utilized for recording~
    \item \textbf{FiWI} dataset~\cite{shen2014webpage} is a dataset that focuses on website-based saliency prediction. However, it is small in term of webpage screenshot availability (only 149 images) compared to two previous datasets that prevents its use for larger scale evaluations (this is especially true for a deep learning model). Therefore FiWI dataset is mostly (and commonly) utilized as comparative evaluation with other models, but not for model training.
\end{itemize}
%, it is difficult to achieve a generalized top-performing model if only 149 images are employed to train the model. 
%\subsubsection{Contrastive Website}
%\label{subsec: cwd}
% discuss the experiment setting
%Besides GazeMining, another dataset is a new dataset named the Contrastive Website dataset. The participants were asked to perform 16 different website visits, which were divided into eight categories, and each subject was asked to visit 153 web pages. An experimental monitor with 1920 x 1080 resolution and 60 Hz is utilized for recording~\cite{aspandi2022user}. However, there is some data loss in the data collection that adds to the workload, although the quality is better compared to the GazeMining dataset again a lot of pre-processing is applied. %!decky what is the challenge

%\subsubsection{Fixations in Webpage Images (FiWI)}
%FiWI dataset~\cite{shen2014webpage} is the only dataset that focuses on web situation saliency prediction. However, its number of only 149 images limits its ability to be applied as a training set. For a deep learning model, it is difficult to achieve a generalized top-performing model if only 149 images are employed to train the model. Therefore the FiWI dataset is utilized in this work for comparative evaluation with other models, but not for model training. %!decky what is the challenge
\begin{figure*}[h!]
    \centering
    \includegraphics[width=0.75\linewidth]{SCITEPRESS_Conference_Latex/newmask.jpg}
    \caption{Structure of Multi Mask Input Attentional Network.}
    \label{fig:basicmodell}
\end{figure*}
%!!deckyal put the scheme, and explains. (3 - 4 sentences). 
%!!decky how many data in totals, where is it (darus, ill give the link later)? and what to do. 

%However, GazeMining and Contrasitive Website datasets contain some difficulty to utilize. GazeMining dataset contains some images with lower quality and many incomplete images (only the dynamic parts are saved) and black frames of the images due to recorded screenshots. Contrastive Website dataset loss some data in the data collection. Therefore, well-designed pre-procesing for each dataset is applied. 

Figure~\ref{fig:datsáset} shows the flow of our dataset curation, data pre-processing and the results of the generalized dataset. The first step of pre-processing is elimination of empty screenshots (which are frequently found on GazeMining dataset), i.e. no screenshots are present, which can come due to rapid sampling during acquisition. Secondly, due to dynamic nature of the observed websites, webpages' static parts are removed (or blackened) on original datasets, which necessitates us to find and collect only recorded screenshot - thus removing irrelevant observations (black parts). Thirdly, we remove the duplicate webpage screenshots of both datasets for efficiency and generate respective locations of Image and Text as independent layers (called Image and Text Mask, which we will detail in Section~\ref{sec:inputMask}). Lastly, eye-gaze heatmap layers are generated (as ground truth) with respect to duration of each observed eye-gaze. This pipeline is applied to all datasets, with the exception of Contrastive Website, where first and second step are skipped, and for FiWI where only last step is necessary. In the end, our unified dataset includes five sub-folders (data, label, Imagemask, TextMask) with a total of 3119 screenshot examples and associated ground truths.

% to allow for direct machine learning modeling for eye-gaze prediction and respective benchmarks.


    
   % However, the image quality is not impressive and contains many incomplete images (only the dynamic parts are saved) and black frames of the images due to recorded screenshots. These were heavily pre-processed to ensure that we could utilize them.
    
    %However, there is some data loss in the data collection that adds to the workload, although the quality is better compared to the GazeMining dataset again a lot of pre-processing is applied.

%deleted the mask generater instance and move it to next section
%Multi Mask Input Network with Attention for Eye Gaze Prediction  - MuMinAtt.

\subsection{Multi Mask Input Attentional Network (MMIAN) for Eye Gaze Prediction}

% 1. General exaplantions of the models pipeline, what are the components. 
Our pre-processed dataset allow us to propose a deep learning based model to benefit from sizable number of observations. Here we propose a Multi Mask Input Attentional Network (MMIAN) that aims to predict eye gaze location given a webpage screenshot as input. Specifically, given input webpage screenshot images of $X_{i..n}$, with $X$ as 2D matrix of webpage screenshot image, and n as number of batch size, MMIAN estimates eye-gaze locations of $\hat{Y}_{i..n}$, where each $\hat{Y}$ is a 2D matrix of eye-gaze heatmap, with each cell value represents probability of eye-gaze locations. The structure of MMIAN follows encoder-decoder scheme that is inspired by Multi-scale Information Network (MSI-Model) ~\cite{kroner2020contextual} consisting of several modules: input and Mask Encoder, Mask Generator, Attentional Feature Fusion, Atrous Spatial Pyramid Pooling (ASPP) Module and Decoder. In this work, we further propose several important modifications: 

\begin{enumerate}
    \item Incorporation of several input masks to include webpage content information of textual and images spatial location. 
    \item Addition of attention mechanisms for effective fusion of input modalities.
    
    %achieve multi-modal learning aims to improve the accuracy of the gaze predictor.
\end{enumerate}

Overall architecture of MMIAN can be seen in Figure~\ref{fig:basicmodell}. Specifically, an input webpage screenshot image is passed into mask generator to produce an image mask and text mask. Then both masks are concatenated and fed into Mask Encoder to extract relevant features, while input image is simultaneously fed into input Encoder. Feature Fusion module then fuses extracted mask features and input features with several attentional modules and combines them in a conjoint block. This feature block is then fed into ASPP part to enlarge field of views, capturing different resolution views from the input, thus producing richer feature representations. These features are then passed through series of up-sampling layers (Decoder) creating an eye-gaze saliency map as a final result. The implementation code of our model is available on our repository\footnote{https://github.tik.uni-stuttgart.de/ac138165/WebToGaze}.

%!!decky explain briefly how the pipeline is. 4 sentences.



%Because the basic saliency models such as MSI-Model tend to be more concerned with the basic image features. Web pages, as the target images to be predicted, have more features that natural graphics do not have, such as the layout of the page. In the proposed model, the masks for web pages were generated that represent their layout features, and these masks were utilized to provide more information about the basic images.


%encoder part. 


%decky -> put the image in one row.


\subsubsection{Input Masks Generator}
\label{sec:inputMask}
%For a web page, the most important thing is the layout of images and text.

\begin{figure*}[h]
\centering

 \subfloat[Input]{
 \includegraphics[width=0.33\linewidth]{SCITEPRESS_Conference_Latex/F_U13_S7-992728854_930104030_orig.jpg}} 
 \hfill
 \subfloat[Text Mask]{\includegraphics[width=0.33\linewidth]{SCITEPRESS_Conference_Latex/F_U13_S7-992728854_930104030_orig_mask.png}}
 \hfill
 \subfloat[Image Mask]{\includegraphics[width=0.33\linewidth]{SCITEPRESS_Conference_Latex/F_U13_S7-992728854_930104030_orig_mask_1.png}}

\caption{One case of input and text mask generation. Figure (a) is an example of webpage screenshot, Figure (b) and (c) are the generated text mask and image mask respectively.}
\label{fig:mask}
\end{figure*}

\label{subsubsec:maskG}
It has been observed that webpage layout (which in principle is arrangement of images and texts) is an important aspect of webpage~\cite{shen2014webpage}. To benefit from this information, we propose to incorporate spatial locations of both image and text on websites with mask representations: image and text masks. These masks are represented in form of a matrix, where each cell is activated in the presence of both image and text in webpage screenshot.  %This way, we to allow for the inclusion of these modalities.
%both of im mask and text mask was generated for the screenshots of each web page and these additional masks also play a crucial role in the proposed improved model.
%the recognition of web user interfaces and automatic code generation has been studied by many researchers.
\begin{itemize}
    \item \textbf{Image Mask Generation: } For image location recognition, we use method from M. Xie et al.~\cite{10.1145/3368089.3417940} which produces bounding-box locations of an input image in a binary map (i.e. map value is set to 1 where image is present, otherwise 0). 
    
    %of recognised bounding box location was set to 1 and the other background was set to 0. The image mask is generated.    contains different color regions that stand for potential different layout blocks. Then generates a binary map and fed in a ResNet 50 ~\cite{DBLP:journals/corr/HeZRS15} classifier, which is trained by 90,000 GUI elements with 15 categories, were applied here to classify the extracted elements. The result of Image View's bounding box location was set to 1 and the other background was set to 0. The image mask is generated.
 
  \item  \textbf{Text Mask Generation: }we use one of available optical character recognition (OCR) methods of Efficient and Accuracy Scene Text detection (EAST)~\cite{Zhou_2017_CVPR} to locate location of text of webpage screenshot input. Similar to Image Mask, this process generates corresponding Text Mask in Binary map format.
  
  %The snapshot as an input image is fed into the fully-convolutional neural network (FCN). After the FCN module, the overlay of features generates three special features in multiple channels for subsequent operations. The final boxes from the EAST network which represent the text location is set to 1, and the other parts set to 0. And then text mask was generated which were utilized to acquire more feature

\end{itemize}

%eval of img mask and text mask

%Since the same network generation has some universality, it can also be used as available additional features as input to aid feature extraction.


One example of generated masks from our curated dataset can be seen in Figure~\ref{fig:mask}. We can see that both of masks contain quite accurate locations of both Image and Text on an input webpage screenshot. Even though some mild imperfections occur (i.e. some buttons can be recognized as images, it does not fully detect locations of certain images due to small size, and some text within images are included on text masks), however in general the locations of both image and text on input webpage screenshots are properly recognized. These masks are then concatenated to be fed into Mask Encoder part.



% However, there are certain errors in the results generated by the image mask, for example, the entire top menu bar is mistakenly identified as a picture, and the following tabs of similar products are not correctly identified. 

% The result of the text mask-generating example is shown in Figure~\ref{fig:mask} (c). It is obvious that almost all the text in the screenshots is recognized and the corresponding mask is generated accurately, but in addition to the text which expect to be recognized (descriptive text such as product description on shopping page) in the HTML pages that contain direct user interaction, the text in the images is also recognized. However, these can also be applied as external information.



\subsubsection{Input and Mask Encoder}
Encoder part is a series of Convolutional Neural Network that aims to extract visual features from input matrix, which is original web screenshot image and concatenated generated Masks, for Input and Mask Encoder respectively. Both Encoders are based on VGG16~\cite{simonyan2014very} architectures with last fully connected layer removed, and further reductions of convolution layers (five convolutional layers with Relu and two max-polling layers) applied for Mask Encoder. Input and Mask encoder then produce MaskFeatures and Screenshot Features to be combined through Multi-Modal Attentional Fusion module.


%In Encoder partial, the original web screenshot is fed into a VGG16-based convolutional neural network, and also in a VGG16 network, the final fully-connected layer is removed with front part of the feature extraction as encoder are retained to be utilized. 

%After the mask is generated from Mask Generator, the Mask Encoder is applied and aims to extract the features from masks (image and text mask). The Mask Encoder partially refers to the Encoder part of the VGG16 model, but in addition to the final fully-connected layer being removed, several convolutional layers for the same deeper feature extraction are also removed. 
%In the initialization of the parameters, the fine-tuning parameters were applied from the previous section~\ref{sec:finetuning} as the initial parameters input to the network.

%In order to enhance the effect of the MSI-Model in the Scenario of web page saliency prediction, the model was designed as shown in Figure~\ref{fig:basicmodell}. In addition to the Encoder, ASPP, and Decoder parts of MSI-Model, another branch named Mask Encoder is implemented. It is employed to extract the features of the mask. First, only a single mask was applied to compare the models. That is, only the text mask or image mask is fed into the Mask Encoder. 

%The first part to be introduced is the Mask Generator partial. The scenarios required for the image mask are generated using the method of M. Xie et al. The text mask is generated using the EAST method. In the case of a single mask situation, only one mask was generated and put into the Mask Encoder partial.

%


%\subsubsection{Mask Feature Encoder}
 
 
 %The network is initialized with the weights of a VGG-16 model trained on the ImageNet data set for object classification ~\cite{simonyan2014very}. These weights are then iteratively updated together with the loss function.



\subsubsection{Multi-modal Attentional Fusion}
%weighted through matrix multiplications (denoted as $\bigoplus$) of the results from the internal Multi-scale Channel Attention module (that aggregate the local channel context by series of point-wise convolutions). Then these two weighted streams are fused with additional operations ($\bigoplus$ symbol) producing the fused feature of Z.

%change to AFFpart
We introduce attentional fusion mechanism for more effective modality fusion, as opposed to simple fusion operations of summation or concatenation, and further solving the problems of inconsistent input semantics and scales~\cite{dai2021attentional}. An example of attentional feature fusion block (AFF) can be seen in Figure~\ref{fig:aff} where it receives two streams of inputs (denoted as F1 and F2) and initially fuses these input features through matrix addition (denoted as $\bigoplus$). The result from addition of internal Multi-scale Channel Attention module (that aggregates channel context by a series of point-wise convolutions) and a sigmoid function generates a fusion weight. This fusion weight and its another counterpart (where it is negated by one) are multiplied (denoted as $\bigotimes$) by each of original inputs respectively, and then added together ($\bigoplus$ symbol) to produce their weighted average as fused feature of Z. This fusion process is applied to the received MaskFeatures and Screenshot Features three times, to allow for observation of different scales of respective features (these operations are marked as circled A in Figure~\ref{fig:basicmodell}). The resultants of the fused features are then concatenated to a conjoint block for subsequent pipelines.

%input which exclusively utilizes point-wise channel interactions for each spatial position. This operations produces the fused feature of \textbf{Z}.

%In the Feature Fusion part, to better fuse, the features, the layer-by-layer features extracted in the encoder and mask encoder are fused using Attentional Feature Fusion (AFF)~\cite{dai2021attentional}, and the fused features are concatenated to form the features we need for the next network. 

%The local context to the global context was added inside the attention module to keep it as small as necessary. As the local channel context aggregator, point-wise convolution (PWConv) was applied, which exclusively utilizes point-wise channel interactions for each spatial position. 

%The AFF module is shown in Figure~\ref{fig:basicmodell} as the ball with the letter $A$.

\begin{figure}[h]
    \centering
    \includegraphics[width=1\linewidth]{SCITEPRESS_Conference_Latex/AFF1.jpg}
    \caption{Left is Attentional Feature Fusion block, right is Multi-scale Channel Attention module~\cite{dai2021attentional}.}
    \label{fig:aff}
\end{figure}

\subsubsection{Atrous Spatial Pyramid Pooling Module (ASPP)}
%decky, what is the input and the output.
ASPP module is made by superimposing different atrous convolutions~\cite{chen2017deeplab} to obtain features from larger receptive fields. This is beneficial given that it is common for webpage layouts to be modularized into a variety of different sub-layouts. This thus increases receptive fields (or field-of-view) of internal convolutional filter enables us to capture such layout configurations. In our ASPP module, we use dilation rates of 4, 8, and 12, with number of $1\times1$ filter to be 256 to ensure kernel size compatibility. This module is applied to conjoint block input, and subsequent features are obtained for decoding operation.

%also makes sense for purely virtual scenarios (web screenshots). 

%The convolution layers with %
%dilation rates 4, 8, and 12 are combined. And then a 256 filter size $1\times1$ convolution layer was applied in to convert the dimensional of the features back to 256 in order to feed them into the decoder for feature reduction.

%The conjoint block is then fed into the ASPP module and features with larger receptive fields are obtained for subsequent operations. 

%Atrous Spatial Pyramid Pooling (ASPP)~\cite{chen2017deeplab} module is made by superimposing different atrous convolutions. 



\subsubsection{Output Decoder}
Decoder part consists of a series of convolution layers with up-sampling to decode input features, generating final prediction of eye-gaze heatmap location (in the form of a 2D tensor and in binary mode). Specifically, number of up-sampling blocks consisting of bilinear scaling operations (each sequentially doubles the number of input tensor) and a subsequent convolutional layer with kernel size $3 \times 3$ are used. With this module, we provide the features generated from ASPP as input features and final eye-gaze heatmap predictions are obtained. Finally, we convert binary heatmap to gray-scale format, to conform to the original scale of eye-gaze heatmap from ground-truth map. 

%After these three up-sampling blocks, a 32-depth tensor is obtained, and finally, a 3x3 convolutional layer of depth 1 is applied to obtain the final result. 





% \subsection{Base model structure}
% %baseline structure
% We used the saliency model of A. Kroner et al.~\cite{kroner2020contextual} as the base of our eye-geze predictor, due to its structural simplicity and quite accurate results reported on their development. In the model named Multi-scale Information Network (MSI-Model) researchers solve problems with image-to-image learning, which requires spatial features to be maintained throughout the whole processing stream. MSI-Model is compared to the normal encoder-decoder network removed all the full connection layer and then adding ASPP structure ~\cite{chen2017deeplab} between encoder and decoder to enhance the overall View-of-Field of the model. All the structure is visualized in Figure~\ref{fig:baselinemodel}.

% %As a result, this network has fewer down-sampling operations and removed all full-connected layers, which are necessary for classification models. Utilizing the pre-trained neural networks, the well-known VGG16 architecture~\cite{simonyan2014very} was modified as an image encoder. In the encoder convolutional layers are used to extract more complicated features. Striding was removed in the final two pooling layers. Then, by increasing their kernel, all following convolutional encoding layers dallied at a rate of 2, which enlarged the receptive field to make up for the receptive field to compensate for the higher resolution~\cite{yu2015multi}. An Atrous Spatial Pyramid Pooling (ASPP) module uses the output of the encoder as its input~\cite{chen2017deeplab}. ASPP can enlarge the View-of-Field of the model. At the end is a decoder module, which contains three up-sampling blocks, each containing bilinear up-sampling followed by a 3 x 3 convolution to avoid checkerboard artifacts in the image space. Unlike all other layers, the output of the model is not modified by a ReLU. The decoder model aims to restore the original image resolution from the ASPP model. All the structure is visualized in Figure~\ref{fig:baselinemodel}.

% %traning information(weight, loss , optimazer)
% %In the research of MSI-Model, the weight values of the encoder part were initialized by the VGG16-structure~\cite{chen2017deeplab}, which was pre-trained on ImageNet~\cite{deng2009imagenet}. The ASPP and Decoder were initialized by uniform distribution with zero mean and variance depending on the total number of incoming and outgoing connections. Kullback-Leibler(KL) divergence is utilized as a loss function to train the MSI-Model. In addition to this, the Adam optimizer and a learning rate of $10^{-6}$ are used to train the model.


% \begin{figure}[h]
%     \centering
%     \includegraphics[width=0.9\linewidth]{SCITEPRESS_Conference_Latex/architecture.jpg}
%     \caption{Baseline model structure~\cite{kroner2020contextual}.}
%     \label{fig:baselinemodel}
% \end{figure}




\subsection{Training Procedure}
%In this work, the encoder-decoder structure is inspired by Kroner's Model (MSI-Model)~\cite{kroner2020contextual}. Therefore, 
%training of baseline. 
To evaluate the impact of training of models utilizing webpage screenshot based eye-gaze prediction datasets, first we perform training process using an original saliency prediction model of Kroner's Model (MSI-Model)~\cite{kroner2020contextual}. This model has been pre-trained on general visual saliency dataset (SALICON)~\cite{jiang2015salicon}, which we call as P-MSI, that serves as baseline. Given this pre-trained model, we perform fine-tuning using our pre-processed datasets (GazeMining and Contractive Website dataset) producing FT-MSI-GM and FT-MSI-CW respectively, and evaluate observed accuracy gains. Additionally, we propose to evaluate the impact of training using a combined dataset (i.e. we combine examples of both of GazeMining and Contrastive Website) by further training MSI-Model with this merged dataset, that we name as FT-MSI-CMB. The training is conducted until convergence and no further accuracy improvement is perceived. Afterward, we proceed to training stage of our proposed model of MMIAN by first transferring Encoder and Decoder's weight from best performing models of the previous step, and initialized weights of both Mask Encoder and ASPP by zero-mean uniform distribution.

All of the training is conducted by minimizing Kullback-Leibler divergence (KLD) loss function as shown in Equation~\ref{eq:kld}, with $Y$ indicating ground truth and $\epsilon$ is a regularization constant to guarantee that denominator is not zero. In this loss, estimation of saliency maps can be regarded as a probability distribution prediction task, as formulated by Jetley et. al.~\cite{jetley2016end}. The output of estimator is normalized to a non-negative number, with KLD value used to measure level of differences between predictions and ground truth. Finally, Adam optimizer~\cite{kingma2014adam} with a learning rate of $10^{-4}$ is used for overall optimization.  

\begin{equation}
\label{eq:kld}
    D_{KL}(\hat{Y}||Y) = \sum_i~Y_i~ln(\epsilon + \frac{Y_i}{\epsilon + \hat{Y}_i})
\end{equation}

 %to achieve the training task. The overall model is trained for 30 epochs and the model with the best results is retained as the result utilize to test.
%The model is then trained separately on both datasets for 30 epochs and the model with the best validation results is saved. Then the two datasets were combined as a bigger dataset. The combined dataset is applied to evaluate the fine-tuning result.



%why need finetuning
%In this paper, the pre-trained MSI-Model was applied for testing on the website snapshot dataset. Although the MSI-Model performs outstandingly on natural images such as portraits, landscapes, etc., it has a low accuracy rate on web screenshots. The first reason is that web screenshots do not typically have the same prominence or strong color contrast as natural images to attract human attention. Secondly, deeper semantic features often have a much stronger impact on web browsing than lower-level features such as color differences and contours. But also, humans have certain general habits when browsing the web. In the Q. Zhao et al.~\cite{shen2014webpage} study, it was shown that people have regular web browsing habits. Therefore it makes sense to fine-tune the MSI model to adapt to the dataset for better performance in the saliency prediction of web screenshots.

%MSI-Model is trained by SALICON dataset~\cite{jiang2015salicon}, which contains 10000 train images, 5000 validation images, and 5000 test images. All the images in SALICON dataset are nature images. MSI-Model is initially modified to fit two training datasets in the fine-tuning step. Then, to fine-tune the models, the weights of the pre-trained MSI model were loaded, which was trained on the SALICON dataset and obtained the best results in 10 training epochs. The loaded fine-tuning model was continually trained by the GazeMining dataset. The models have trained 30 epochs. The weights of the models that performed the best on the validation set i.e., the epochs with the lowest KLD values on the validation set were stored and applied to the subsequently predict test dataset. Then the trained fine-tuning model was tested on the test set of GazeMining and Contrastive Website datasets. After the training with only the Contrastive Website dataset and training with the training set of both datasets together are performed in the same way.

%how finetuning
%Each of the two datasets has its own characteristics, as described in section ~\ref{subsec:gaze}. The GazeMining dataset is more focused on the changes in the web pages, i.e. the dynamic part. Therefore, more screenshots of web parts such as programs, menubar, etc. are included. These dynamic parts play a crucial role in the layout of contemporary popular web pages such as dashboards. In addition, the cropped image part of pre-processing results in screenshots of web pages that do not have a uniform fixed resolution. In contrast, the Contrastive Website dataset is all full image screenshots. However, the size of the images is not uniform because of the up-and-down scrolling feature of the web pages, which also places a high demand on the scale adaptability of the model. Also, the Contrastive Website dataset has more snapshots of better quality compared to the GazeMining dataset. Therefore,  MSI-Model was fine-tuned for using both datasets separately. 

%traiing onf addition mask. 


%In addition, the fine-tuning of the model network will first clip the weights of the original model that have been trained and performed well on the SALICON dataset~\cite{jiang2015salicon}, and then train them on GazeMining and contrastive webpage dataset. For the enhanced model 1 and enhanced model 2, the weights of the successfully fine-tuning models are loaded before the start of the training, and then the weights training is continued.

\subsection{Experiment Setting}

\subsubsection{Dataset Experiment Setting}
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%change to fiwi
In this experiment, all of three pre-processed datasets (cf. Section~\ref{sec:dataset}) are utilized, with training, validation and test instance numbers are shown in Table~\ref{tab:dataset}. Specifically, both GazeMining and Contrastive Website datasets are divided with 60\%, 20\% and 20\% of samples for training, validation set, and test set respectively. Whereas for FiWI dataset~\cite{shen2014webpage}, all samples (of 149 images) are used for testing. 


%The final model is applied to the two test sets separately and the results is evaluated. Finally, for comparison and qualification with other models, the FiWI dataset~\cite{shen2014webpage} was employed too, which contains 149 images, and was utilized all images for testing and cross-sectional comparison of the results.

\begin{table}[!h]
    \centering
\resizebox{1\linewidth}{!}{
    \begin{tabular}{l|c|c|c}
    \hline
 Splits & GazeMining & Contrastive Website & FiWI \\
 \hline
 Training&  928 & 868 & - \\
 Validation & 309 & 278 & -\\
 Test& 309 & 278 & 149\\
    \hline
    \end{tabular}}
    \caption{Training, validation and test set for each dataset.}
    \label{tab:dataset}
\end{table}

\subsubsection{Quantitative Metrics}
We use three quantitative metrics of Area under Receiver Operating Characteristic curve (AUC), Normalized Scanpath Saliency (NSS), and Person's Correlation Coefficient (CC) to judge the quality of models' prediction. 



%The first row is the result of the pre-trained model without fine-tuning directly applied to two test sets. The second and third row is the model fine-tuned on two different train sets of the GazeMining and Contrastive Website dataset. The fourth row is the result from the use of combined dataset for training.

%decky -> change to one row.
\begin{figure*}[h]
% \def\tabularxcolumn#1{m{#1}}
\centering

 \subfloat[Input+GT]{
 \includegraphics[width=0.33\textwidth]{SCITEPRESS_Conference_Latex/finetuning-ori.png}}
 \hfill
 \subfloat[P-MSI]{\includegraphics[width=0.33\textwidth]{SCITEPRESS_Conference_Latex/fintuning_k.jpeg}}
 \hfill
 \subfloat[FT-MSI-CW]{\includegraphics[width=0.33\textwidth]{SCITEPRESS_Conference_Latex/finetuning1.jpg}}
 
\caption{Comparison between pre-trained results from P-MSI (baseline) and FT-MSI-CW (Fine-tuned). Figure (a) shows webpage screenshot with ground truth (eye-gaze heatmap). Figure (b) and (c) show predictions from P-MSI and FT-MSI-CW.}
\label{fig:comFT}
\end{figure*}

%!decky shorten to two to three sentences.

\begin{itemize}

    %$\hat{Y}$ as the predicted saliency map (eye-gaze heatmap), and $
    \item \textbf{NSS} is commonly used for general saliency prediction tasks as a direct correspondence measure between predicted saliency maps and ground truth, which is computed as average normalized saliency at fixated locations~\cite{PETERS20052397}. Furthermore, NSS is sensitive to false positives, relative differences in saliency across evaluated image, and general monotonic transformations~\cite{bylinskii2018different}. With $Y^B$ as a binary map of true fixation location and $N$ to indicate total pixel number and $i$ indicates each pixel instance, NSS value can be calculated using an equation as shown below:
    %$Q^B$
    \begin{equation}
    \label{eq:nss}
        NSS(\hat{Y},Y^B) = \frac{1}{N}\sum_i \hat{Y}_i \times Y_i^B
    \end{equation}
    
    %of the predicted heatmap 
    %In Equation~\ref{eq:nss},
    \item \textbf{AUC-J} evaluates predicted eye-gaze heatmaps as a classification task, where each prediction pixel is evaluated through a binary classification setting. Here, a certain threshold value is used to decide whether it is deemed to be correctly predicted as eye-gaze locations, thus emphasizing frequency of true positive. We use method described by Judd et. al.~\cite{judd2009learning} to select all required thresholds. 
    % to  whether the pixels are fixated or not. 
    
    
    %In their approach, each distinct saliency map value is used as a threshold, so the sampling strategy provides the most accurate approximation to the continuous curve. 
    %$\hat{Y}$ indicating the predicted eye-gaze map, $Y$ indicates the ground truth, and 
    \item \textbf{CC} metric aims to evaluate level of linear relationship between two input distributions. In eye-gaze location prediction task, both generated eye-gaze location map and ground truth are treated as random variables~\cite{le2007predicting}, and level value is calculated with both of these map inputs following equation~\ref{eq:cc}. Thus, with operator $\sigma$ as covariance matrix, CC value can be calculated as: 
    \begin{equation}
    \label{eq:cc}
        CC(\hat{Y},Y) = \frac{\sigma(\hat{Y},Y)}{\sigma(Y) \times \sigma(\hat{Y})}
    \end{equation}
    
\end{itemize}



\section{\uppercase{{Experiment Result}}}




%decky -> make the text smaller. put the P-Msi first, then MMIAN.
\begin{figure*}[h!]
\centering
    \includegraphics[width=0.95\linewidth]{SCITEPRESS_Conference_Latex/GC.png}
    \caption{Example of prediction results of MMIAN and Fine-tuned MSI Model (FT-MSI). First column of (a) and (d) shows webpage screenshot with outlined text (light green box) and image (purple box) area, along with eye-gaze heatmap ground truth. Second column (b) and (e) shows predicted result from FT-MSI and MMIAN model, with third column of (c) and (f) shows respective Grad-CAM heatmap.
    }
    \label{fig:gradcam}
\end{figure*}

In this section, we first present results of both baseline and our proposed approach using part of our pre-processed dataset (GazeMining and Contrastive Website), as outlined in Section~\ref{sec:meth}. Then, we provide a comparison of best results of our approach with alternative saliency prediction models using full set of our pre-processed dataset, establishing a benchmark for eye-gaze prediction task given webpage screenshot inputs.

%and compare them with the best of our models to 

%is developed and evaluated in this section. The fine-tuned models and different improved models is trained and tested on two datasets (GazeMining and Contrastive Website datasets). Furthermore, three metrics is calculated and evaluated for the performance of models. Afterward, the improved model is also evaluated, and finally, the models were compared and analyzed with the existing State-of-Arts.

\subsection{Impact of Fine-tuning using independent and combined dataset.}

\begin{table}[]

\setlength\tabcolsep{3pt}
\centering
\resizebox{1\linewidth}{!}{
\begin{tabular}{c|l|ccc|ccc}
    \hline
 \multirow{2}{*}{No.}&\multirow{2}{*}{Models} & \multicolumn{3}{c|}{GazeMining}      & \multicolumn{3}{c}{Contrastive Website}    \\ \cline{3-8}
 &        & NSS   & AUC-J & CC       & NSS   & AUC-J& CC       \\ \hline
1. & P-MSI      & 1.037  & 0.699& 0.130   & 0.839 & 0.683 & 0.109\\
2. & FT-MSI-GM    & 1.398 & \textbf{0.753} & \textbf{0.170} & 0.717 & 0.651 & 0.093     \\
3. & FT-MSI-CW & 0.695& 0.655 & 0.081      & \textbf{2.364} & 0.777& \textbf{0.277} \\
4. & FT-MSI-CMB & \textbf{1.447}& 0.752& 0.170       & 2.269&\textbf{ 0.789 }& 0.269\\
     
    \hline
%  & \multicolumn{3}{c}{Contrastive Website}       \\
%  & CC    & NSS   & AUC-Judd\\ \hline
% 1. pre-trained MSI-Model  & 0.109 & 0.839 & 0.683\\
%     2. Only Gazemining Dataset & 0.093 & 0.717 & 0.651\\
%     3. Only Contrastive Website Dataset & 0.277 & 2.364 & 0.777\\
%     4. Combined dataset & 0.269 & 2.269 & 0.789\\ 
\end{tabular}}
\caption{Results of fine-tuned models on both preprocessed GazeMining and Contrastive Website datasets. }
\label{tab:fintune}
\end{table}


\label{subsec:finetuning}
%\textbf{ tested utilizing subset of pre-processed GazeMining and Contrastive Website datasets. }
Table~\ref{tab:fintune} shows results of four alternative models that are trained using different datasets. Here we can see that pre-trained MSI (P-MSI) model performs worse in comparison to other models, which suggests its inability to generalize to eye-gaze saliency prediction task. We can further observe that when fine-tuning is applied, improvement from original P-MSI is noticeable, especially when it is tested on similar dataset used for fine-tuning. This can be seen with higher values achieved on all metrics of FT-MSI-GM and FT-MSI-CW compared to P-MSI on GazeMining dataset and Contrastive Website dataset respectively.

When model is trained using combined dataset, however, accuracy improvement is instead lower. This is indicated by lower quantitative values achieved by FT-MSI-CMB as opposed to their counterparts (FT-MSI-GM and FT-MSI-CW), that are trained separately. This phenomenon may come from difficulty of estimator to learn from these two datasets, which are quite distinct, and also from the nature of task executed by the users on each dataset.

%from a Contrastive Website dataset. 
Figure~\ref{fig:comFT} presents visual prediction examples for baseline model (P-MSI) and best performing model (FT-MSI-CW) for Contrastive Website Dataset (where the example is originated). The screenshot example is a part of route search task, where user has to enter vehicle information to estimate fuel consumption. Thus it is natural for users to focus on dialog box (displayed in the center of the webpage screenshot) resulting in users' fixations being located within this area. 

%to be refined %evident
By evaluating the predictions of eye-gaze heatmaps of baseline model of P-MSI, we can see that even though it manages to predict parts of ground-truth (eye-gaze) locations, however, it also falsely predicts other less relevant locations as eye-gaze locations (i.e. advertisement part). These results come from its tendency to detect high color contrast as area of interest, which is common in natural image datasets  (SALICON~\cite{jiang2015salicon}). However, this leads to higher occurrences of false positive, thus reducing its accuracy. Our fine-tuned model however, manages to reduce existing inaccuracies by absorbing the characteristics of dataset during fine-tuning, adapting model to this specific eye-gaze prediction task. In this example, we see that the predictions of fine-tuned models are more precise, especially on dialog box and they exclude some of high contrast area that are normally recognized as saliency area (e.g. advertisement locations). % salient  and able to remove the false positive rate by not recognizing 
% as not a potential eye-gaze location for this instance

From the experimental result, it can be concluded that saliency detection model trained on natural image dataset does not work well for direct application to web screenshot scenario. This can be mitigated by performing careful fine-tuning to only use a dedicated dataset. Given this result, then we use FT-MSI-GM and FT-MSI-CW as base for MMIAN optimization, and for comparison in the next section.


% specific dataset The MSI-Model trained by natural graphics is more inclined to recognize features such as image color edges, which is not exactly the same as the web page scenario. 

% After being fine-tuned the model can adjust to the website snapshot image, Thus, the prediction from the model was more accurate (lower false positive rate). Therefore, fine-tuning the model which pre-trained on the natural image can improve the performance of that model applied to website situations.  Given this result, then we use the P-MSI-GM and P-MSI-CW as the base for MMIAN, and for comparison on the next section.

%It is an address marker webpage on Google Maps (https://www.google.com/maps), It is obvious that the fine-tuned model has a considerably lower false positive rate, which means the pixel is not fixation but the model determined it is. It is obvious that the model trained on the natural image dataset (SALICON~\cite{jiang2015salicon}) had a tendency to predict high color contrast, such as the advertisement part at the top of the page, which has more color contrast, so the model also tend to judge that human attention is concentrated towards it. 

% However, this tendency is reduced in the fine-tuned model, such as the saliency map in the lower left corner, where the advertisement part is no longer marked as a salient part. - more precise, condition to the task involve.

% From the experimental result, it can be concluded that the saliency detection model trained on the natural image dataset does not work well for direct application to the web screenshot scenario. The MSI-Model trained by natural graphics is more inclined to recognize features such as image color edges, which is not exactly the same as the web page scenario. 

% After being fine-tuned the model can adjust to the website snapshot image, Thus, the prediction from the model was more accurate (lower false positive rate). Therefore, fine-tuning the model which pre-trained on the natural image can improve the performance of that model applied to website situations.  Given this result, then we use the P-MSI-GM and P-MSI-CW as the base for MMIAN, and for comparison on the next section.

% When the model was trained by a train set of GazeMining alone all three evaluation methods (CC, AUC-Judd, NSS) performed better on the GazeMining test set, but for the other test set (Contrastive Website Dataset) their quantitative accuracy metrics are lower than the pre-trained model (row 2). Conversely, when training with only the Contrastive Website Dataset dataset performs worse on the GazeMining test set (row 3). 

% However, when the two datasets is combined as a big dataset, the evaluation results were not all improved, that indicates the difficulty of the model to learn from these two datasets, that are quite distinct (which can comes from the nature of the task executed of each dataset).

% Thus and we only used separate training and separate evaluation in the next models to ensure the accuracy of the experiment. 

%As can be observed, even if the results of the performance are better than the baseline model (row 1) when the corresponding training set is combined. The performance is still worse than the model which trained only on the corresponding dataset (on GazeMining dataset AUC is worse while on Contrastive Website dataset CC and NSS are worse). Differentiation of different datasets indicates that the fourth row in Table~\ref{tab:fintune} is marginally lower than the second row but considerably better overall than the first row, and in the tabel~\ref{tab:fintune} right part, the fourth row is slightly lower than the third row but also better than the first row. The GazeMining dataset, as indicated in section~\ref{subsec:gaze} and section~\ref{subsec: cwd}, concentrates more on the dynamic portion of the web page, whereas the Contrastive Website dataset focuses more on the whole web page. However, the background of the dataset indicates that the variability in the datasets themselves causes the performance of the models to be inconsistent. Moreover, because the some metrics of the combined dataset is lower than that of the separate dataset, i.e., the evaluation results when employing the combined dataset trained are lower on the GazeMining test set than on the model trained only on the GazeMining dataset, and the same happens on the Contrastive Website dataset. Therefore, in the next experiments, the models is trained and evaluated separately on the GazeMining dataset and Contrastive Website dataset.

%\captionsetup[subfloat]{labelformat=empty}
% \includegraphics[width=3cm]{Draw/oriImg.jpg}



\subsection{Impact of Multi-modal Masks and Attention-based Fusion}

\begin{table}[h!]

\setlength\tabcolsep{3pt}
\centering
\resizebox{1\linewidth}{!}{
\begin{tabular}{c|l|ccc|ccc}
    \hline
\multirow{2}{*}{No.}&\multirow{2}{*}{Models} & \multicolumn{3}{c|}{GazeMining}& \multicolumn{3}{c}{Contrastive Website}       \\ \cline{3-8}
&      & NSS   & AUC-J & CC       & NSS   & AUC-J & CC     \\ \hline
1. & FT-MSI      & 1.398& 0.753&  0.170  & 2.364 &0.777 &0.277 \\ 
2. & MMIAN      & \textbf{1.579}& \textbf{0.764}& \textbf{0.184}       & \textbf{2.487}  & \textbf{0.787} & \textbf{0.292}\\
    \hline
\end{tabular}}
\caption{Quantitative results of both fine-tuned MSI models and MMIAN model.}
\label{tab:aff}
\end{table}

% Please add the following required packages to your document preamble:
% \usepackage{graphicx}
\begin{table*}[h]
\centering
\resizebox{\textwidth}{!}{%
\begin{tabular}{c|l|ccc|ccc|ccc} \hline
 \multirow{2}{*}{No.}&\multirow{2}{*}{Models} & \multicolumn{3}{c|}{GazeMining}      & \multicolumn{3}{c|}{Contrastive Website} & \multicolumn{3}{c}{FiWi}    \\ \cline{3-11}
 &   & \multicolumn{1}{c}{NSS} & \multicolumn{1}{c}{AUC} & \multicolumn{1}{c|}{CC} & \multicolumn{1}{c}{NSS} & \multicolumn{1}{c}{AUC}& \multicolumn{1}{c|}{CC}  & \multicolumn{1}{c}{NSS} & \multicolumn{1}{c}{AUC} & \multicolumn{1}{c}{CC} \\ \hline
1.& CASD~\cite{goferman2011context}       & 0.567& \textcolor{blue}{0.653}& 0.064 & 0.419& \textcolor{red}{0.614}& 0.053 & 0.680& 0.732& 0.233\\
2.& DCTS~\cite{houx2012image}      & 0.479& 0.618& 0.053 & 0.256& 0.552& 0.035 & 0.541& 0.671& 0.195\\
3.& HFT~\cite{li2012visual}       & \textcolor{red}{0.707}& 0.644 & \textcolor{red} {0.088}& \textcolor{red}{0.534}& 0.593& \textcolor{red}{0.067} & 0.740& 0.737 & 0.251\\
4.& ICL~\cite{hou2008dynamic}      & 0.485& 0.518& 0.057 & 0.192& 0.490& 0.030 & 0.444& 0.618& 0.162\\
5.& RARE~\cite{riche2012rare}     & \textcolor{blue}{0.632}& \textcolor{red}{0.653}& \textcolor{blue}{0.072} & 0.382& 0.589 & 0.052& \textcolor{blue}{0.850}& \textcolor{blue}{0.758}& \textcolor{blue}{0.280}\\
6.& SeoMilanfar~\cite{seo2009static}& 0.393& 0.584& 0.038 & 0.350& 0.571& 0.044 & 0.445& 0.651& 0.163\\
7.& SR~\cite{hou2007saliency}       & 0.566& 0.639& 0.062 & \textcolor{blue}{0.510}& \textcolor{blue}{0.612}& \textcolor{blue}{0.062} & 0.635& 0.714& 0.216\\
8. & MKL~\cite{shen2014webpage}    & \textbf{-}   & \textbf{-}    & \textbf{-}     & -   & -    & -     & \textcolor{red}{1.200}& \textcolor{red}{0.702}& \textcolor{red}{0.382}\\
9. & \textbf{MMIAN (proposed)}       & \textbf{1.579}& \textbf{0.764}& \textbf{0.184}  & \textbf{2.487}& \textbf{0.787} & \textbf{0.292} & \textbf{1.385}& \textbf{0.786} & \textbf{0.397}\\ 
% 9. & other1      & 
%       0.1051&	0.8476 &x	&	0.0901 &	0.695 & x	&	0.4976 & 	1.7496	&0.84  \\ 
% 9. & other2      & \textbf{0.1328} & \textbf{1.0826}& \textbf{x} & \textbf{0.1228} & \textbf{0.9422}& \textbf{x} & \textbf{0.5501} & \textbf{1.9203}& \textbf{x} \\ 

%!!deckyal highlight the second and third best. 
%just plot yours, second and third best on at least one dataset. You can also combine them (a) (b)

\hline
\end{tabular}%
}
\caption{Comparison of existing saliency predictors evaluated on test sets of our full pre-processed dataset. The boldface indicates best results, red color implies second-best results, and third-best results are marked by blue coloured fonts.}
\label{tab:ext-compare}
\end{table*}


\begin{figure*}[]
    \centering
    \includegraphics[width=\linewidth]{SCITEPRESS_Conference_Latex/ExtentionCompare.png}
    \caption{Five examples of screenshot inputs from FiWI dataset (with respective eye-gaze heatmaps ground-truth overlaid) and predictions from other five saliency predictors (including ours).}
    \label{fig:extCompare}
\end{figure*}

%The left part is the result of training by the GazeMining dataset and evaluation while the right part is the result of the Contrastive Website dataset. 
Table~\ref{tab:aff} presents results of our proposed MMIAN model and best results of fine-tuned MSI-Model from previous section. From this result, it can be seen that MMIAN produces higher value across quantitative metrics in overall, outperforming results from fine-tuned MSI-Model. The gains on all of these metrics altogether, suggest that estimation results of MMIAN are more accurate compared to the best of FT-MSI results (from FT-MSI-GM and FT-MSI-CW), with improvements on both true positive (as judged by AUC-Judd) and false positive Instances (as evaluated by NSS and CC score). 

% method graph heatmap

%can be improved
% to demonstrate the benefit of the inclusion of attention mechanism. This is done 

In order to provide a more comprehensive analysis of the impact of the use of image and text Masks and attention mechanisms, we first show locations of detected images and texts on webpage screenshot input to provide semantic explanations of their relevances for our MMIAN model to produce accurate predictions. Then, we implemented a Gradient-weighted Class Activation Mapping (Grad-CAM)~\cite{selvaraju2017grad} by propagating multiplied error value and gradient to each convolutional layer, enabling us to investigate the relevant activation of convolutional kernels - with respect to image input - indicating most prominent part of webpage screenshot for prediction. The example of original webpage screenshot input that includes image and text locations (drawn as bounding box), associated eye-gaze location ground-truth, generated Grad-CAM and each prediction of fine-tuned MSI and MMIAN is shown in Figure~\ref{fig:gradcam}. 

On Figure~\ref{fig:gradcam}, first we can see that prediction of MMIAN is more accurate than FT-MSI, with larger and correctly identified eye-gaze areas are present. This is especially noticeable in areas where both image and text are present. For examples are areas where button resides (that is recognized as image) and product description, which in this case, are indeed the locations where user looks at. This perceived higher accuracy can be attributed to the use of image and text masks during learning, which is combined effectively through attention mechanism to properly 'guide' learning process to put priority in this area. This impact is also apparent from observation of grad-cam heatmap of FT-MSI and MMIAN, where grad-cam heatmap of MMIAN are seen to be more concentrated on both of images and text area simultaneously (indicating where models' attention is) as opposed to ones produced by FT-MSI. Lastly, we also notice that there are more frequent activations of grad-cam heatmaps of MMIAN compared to ones from FT-MSI, which may suggest that larger perceptual field of view from MMIAN is indicative to be able to produce more accurate eye-gaze estimations in overall.



%This demonstrating the benefit of the incorporation of the masks - which 'guide' the learning to put priority in this area. 
     
%High activations on the part of images, suggesting that the models is able to .. .   In the third column, it can be seen that the model has a larger perceptual field of view of the image when the mask and attention mechanisms are added (the overall color of the image is more reddish-yellow)

%attention of the models is visualised.


%First is that the fine-tuned MSI model fails to accurately predict the eye-gaze locations, with lower activations on the area where button (classified as image) and text are present. The MMIAN model in other hand, produces more accurate estimations in these areas, as can be seen with more activations on the eye-gaze heatmaps on its predictions. 

%%%%%%%%%%
%map with accurate  not have the predictive ability (AUC is less than 0.5). The model has no method to predict the user's eye gaze while browsing. When the MMIAN model is applied, the overall predictive ability of this web screenshot is achieved. Although there are many unattended areas that are predicted to be salient (high false positive rate), most of the user's browsing fixation is still predicted correctly.

%. In addition, it can be seen that the MMIAN model clearly pays more attention to the text part (product description) after adding the text mask. The same happens for the image mask, although not as obvious as for the text part, but the model pays more attention to the image of the watch and the confirmation button on the right side (the button is misidentified as an image).

%This indicates that the MMIAN model is able to enhance the prediction results of the model to some extent by using information from the web page mask. The accuracy of the gaze predictor is improved by making the model more oriented (text and image) to perceive the structure of the web page.

%In the scenario of saliency prediction, the final error (KLD) is multiplied by the gradient to each convolutional layer and normalized was designed. The results of each layer are finally superimposed into a heatmap to indicate which part of the model is more concerned. 


%Specifically, we observed the improvement on the  This is especially true for the NSS metric, where increase of the value is observed, indicating the improvement of True Positive instances (i.e. MMIAN is more accurate than both P-MSI-GM and P-MSI-CW). 


%That shows the MMIAN model has a lower false positive rate than the baseline model. A lower false positive rate means the MMIAN model overall predicts the location of the user's fixation in a smaller and more accurate area. In addition, the MMIAN model performed also better in AUC-judd and acquired a higher number. That means the MMIAN model has a higher true positive rate than MSI-Model. The AUC-Judd result shows the MMIAN model predicts the location of the user's gaze more accurately.





\subsection{State-of-the-Arts Comparison}

%Some existing proven models were applied to test sets of two datasets (GazeMining, Contrastive Website) for evaluation of the results. 
We compare  best predictions of our approach against other seven generalized visual saliency models of Context-Aware Saliency Detection (CASD)~\cite{goferman2011context}, Discrete Cosine Transform (DCTS)~\cite{houx2012image}, Hypercomplex Fourier Transform (HFT)~\cite{li2012visual}, Incremental Coding Length (ICL)~\cite{hou2008dynamic}, RARE~\cite{riche2012rare}, SeoMilanfar~\cite{seo2009static}, Spectral Residual (SR)~\cite{hou2007saliency}, including one specialized eye-gaze location estimator of Multiple Kernel Learning (MKL)~\cite{shen2014webpage}. 
%\end{enumerate}

% \begin{enumerate}
%     \item Context-Aware Saliency Detection (CASD)~\cite{goferman2011context}.
%     \item Discrete Cosine Transform (DCTS)~\cite{houx2012image}.
%     \item Hypercomplex Fourier Transform (HFT)~\cite{li2012visual}.
%     \item Incremental Coding Length (ICL)~\cite{hou2008dynamic}.
%     \item RARE~\cite{riche2012rare}.
%     \item SeoMilanfar~\cite{seo2009static}.
%     \item Spectral Residual (SR)~\cite{hou2007saliency}.
% \end{enumerate}
%including and one specialized eye-gaze location estimator of Multiple Kernel Learning (MKL)~\cite{shen2014webpage}. 

Table~\ref{tab:ext-compare} shows results from all evaluated approaches. Here we see state-of-the-art results of MMIAN that outperforms other alternatives, including eye-gaze predictor of MKL with a large margin on FiWI dataset (note that there is no training involved for FiWI dataset evaluation). This result is mainly due to large differences in task characteristics between general visual saliency prediction (where first seven models are trained) and webpage screenshot based eye-gaze estimations tasks (where MKL and MMIAN are specialized). This highlights the inability of visual saliency based models to generalize on this specific task. Furthermore, our approach performs better than MKL on FiWI, demonstrating the effectiveness of our overall approach for eye-gaze estimation task. 


Figure~\ref{fig:extCompare} shows predictions example of our MMIAN and four other best performing model on FiWI for evaluation (excluding MKL, due to lack of available implementation of the model). Based on this figure, we can observe that most of alternative models produce a large area of webpage screenshot as potential eye-gaze locations. However, this leads to largely false positive prediction, given the mismatch between predictions against actual eye-gaze locations from user of these webpage screenshots (i.e. most area are falsely identified as eye-gaze locations). In contrast, our approach produces more precise estimates, as observed by more refined (and accurate) predicted area of eye-gaze locations (that explains large margin in terms of NSS metrics value from our models compared to others, as shown in Table~\ref{tab:ext-compare}). This is mainly due to the tendency of visual saliency models to focus on pure appearance of webpage screenshot (i.e. high contrast images), as opposed to MMIAN that has been conditioned to the specific characteristics of our pre-processed eye-gaze dataset (which inherently contains eye-gaze characteristics of users, given webpage screenshot). Remarkably, our models is also capable of correctly predicting eye-gaze locations on relevant areas, such as text (third row), image (fourth row) and their combinations (first, second and fifth row), in comparison with other approaches. Here we see that our models' predictions are consistently more accurate than alternatives, demonstrating the effectiveness of our approach.


%2. more targeted to relevan area - text and iamges, but conditioned to the user behaviour charactheristics (absorbed during fine-tuning)

%Here it can be seen that the false positive ratio of the proposed model is still lower. However, the focus on many parts is not completely consistent, for instance, in the snapshot in the fifth column, although the area of browsing view from the proposed model predicted accuracy, the proposed model judges that the part most concerned by the user is the red wine image, while in fact the most concerned by the user is the text first level title.

%The biggest advantage of the proposed model overall is the decrease in the false positive rate for the saliency prediction of a web screenshot, which is mainly reflected in the improvement of the NSS evaluation metrics. In addition, it can be seen in several special cases from the three datasets that the proposed model is more accurate in predicting the locations that users have viewed.


%In the case of FiWI, the penultimate column is the result of the evaluation of the dataset by Shen et al.~\cite{shen2014webpage} who proposed the FiWI dataset. It is evident that the proposed model improves across all three assessment measures (CC, NSS, AUC-Judd).



%To make the test more equitable, All 149 snapshots from the FiWI dataset are applied to evaluation, and the test results are displayed in Table~\ref{tab:ext-compare}.
% five images are selected and predicted using different models. Whereas in the above figure, the first row is the ground truth the last column is the result of the proposed model prediction


%The overall proposed model outperforms both the overall two internal datasets (GazeMining, Contrastive Website dataset) and a completely un-tuned dataset (FiWI). 





%Because these models detect the saliency of natural images, it is clear that when ported to the web screenshot scenario, they do not predict the human browsing gaze with accurate results.

%decky -> put as 9 (rows),5 (columns)





\section{\uppercase{{Conclusion}}}
%decky -> shortne to three paragraphs.

%In this work, we have proposed an effective saliency prediction model applied to web scenarios, and unified two main eye-gaze datasets of GazeMining dataset and the contrastive web dataset for eye-gaze eveluations.

%Specifically, the Each of these two datasets has its own characteristics, so each dataset is trained and tested separately. Fuerthermore, a third dataset (FiWI) was utilized for comparative evaluation with other models, but not for model training.

%For saliency detection, it is more established in the scenario of natural images. So first a model with higher accuracy (MSI-Model~\cite{kroner2020contextual}) was selected and evaluated in the test set of both datasets separately. Following, the model was fine-tuned and evaluated on the GazeMining dataset, the contrastive web dataset. Fine-tuning can significantly improve the performance of the model in web page scenarios, which also indicates that web page screenshots have some similarities with natural images. 


%1. your contributio in overall In this work, we investigate the benefit of the use of our curated eye-gaze datasets. We do this by xxx and yy., 

%1. first contribution detail 

In this work, we enable the development of automatic eye-gaze estimations given webpage screenshot inputs to enable the improvement of the webpage layout, and hence respective user interaction. We do this by developing a unified eye-gaze dataset from three available website and user interaction-based datasets: GazeMining, Contrastive Website and FiWI. We then pre-process each dataset to produce necessary data for eye-gaze prediction task, such as webpage screenshot and corresponding eye-gaze heatmaps as ground truth. In addition, we generate image and textual locations from webpage (in the form of masks) which can be used for training and modeling. Given our unified eye-gaze dataset, then we propose a novel deep learning based and multi-modal attentional network eye-gaze predictor to benefit from the characteristics of the dataset. Our proposed approach leverages spatial locations of text and image (in form of masks), which is further fused with attentional mechanisms to enhance the prediction results.

%construct  to generate  the eye-gaze heatmap, text and image masks. Our Dataset including website screenshot and corresponding eye-gaze heatmap, text and image masks with total of 3119 screenshot examples and ground truth to allow for direct machine learning modeling for eye-gaze prediction. 


%2. second contribution detrail 
% Secondly, a novel model MMIAN structure was designed which aims to improve the accuracy of the gaze estimates.


%both screenshot and contacted masks (image and text mask) to realize  a website-layout-guide multi-modal model. With help of attentional mechanism MMIAN can effective fusion the feature from multi-modal (website screenshot and masks) and preference accurate in evaluation.

%3. In our experiments, we found that 

%To show the effectiveness of our approach, we first perform analysis of the impact of fine-tuning using our pre-processed dataset and the effects of the training when the combined dataset is used. Here we found that the prediction results are indeed improved when fine-tuning is conducted, however, we did not see further improvements when the combined dataset is used, signifying the importance of careful fine-tuning step. Secondly, we evaluate the prediction of our full approach (MMIAN) with respect to the ground truth, existing text and image locations from the website screenshot, and part of the Grad-CAM activations. Here we notice accurate predictions of our approach, especially on the textual and image area where the user is looking, with large and wide activation of Grad-CAM that are also concentrated on these locations. This observation demonstrates the benefit of the use of both image and text masks as input in combination with the attention mechanism.
% however, we did not see further improvements when the combined dataset is used, signifying the importance of careful fine-tuning step.
During analysis of the impact of fine-tuning using our pre-processed dataset, and the effects of training when the combined dataset is used, we found that the prediction results are indeed improved when careful fine-tuning is conducted. Then, we evaluate the prediction of our full approach (MMIAN) with respect to the ground truth, existing text and image locations from webpage screenshot, and part of Grad-CAM activations. Here we notice accurate predictions of our approach, especially on textual and image area where users are looking at, with large and wide activation of Grad-CAM that are concentrated on these locations. This observation demonstrates the benefit of the use of both image and text masks as input in combination with attention mechanism.


%a. first finding (2 sentences).
To measure the competitive results of our proposed approach, we compare them with other saliency prediction alternatives, establishing a benchmark for eye-gaze prediction task. In our comparison, we found state-of-the-art results of our model with high scores across quantitative metrics, and lower false positive rates than other approaches. Visual analysis further confirms our findings, that our approach produces more accurate prediction of eye-gaze locations on relevant website locations, including where text and image are present. The result suggests the superiority of our approach in capturing user behavior. Future work will be to incorporate other user behavior characteristics as additional modalities (e.g. mouse trajectory) to further benefit from this information to improve prediction accuracy.

%Finally, a benchmark for state of arts eye-gaze detection model and our MMIAN model is builded on separately dataset.  

%b. second finding. 
%Furthermore, in our benchmark shows our MMIAN is precise than alternative models. MMIAN predict the eye-gaze location in small accurate area, which means lower false positive rate than others.
%4. future work. 

%In addition,  In addition, due to the addition of the mask, the model pay more attention to the oriented position (image or text). These are what make MMIAN an efficient and accurate model for web scenarios gaze predictor.

 

%In addition, MMIAN model has a larger field of view compared to the traditional Encoder-decoder structural fixation prediction model in the scenario of web pages.


% pre training 

% In our expermients, we found, fine-tune the traditional saliency prediction model on our website eye-gaze dataset can improve the accurate on website eye-gaze prediction tasks.





%a novel model MMIAN structure was designed which aims to improve the accuracy of the gaze estimates. The MMIAN structures were to take advantage of the fact web pages are more structured than natural images. The image and text mask was generated and fed into Mask Encoder which is utilized for feature extraction in parallel. Furthermore, to leverage the additional information generated by the mask, the AFF module was introduced and utilized to fuse the multi-layer and multi-modal features. Finally, some existing saliency prediction models were applied to the GazeMining and Contrastive Website datasets to compare with the proposed model results. 

%The MMIAN model has a larger field of view compared to the traditional Encoder-decoder structural fixation prediction model in the scenario of web pages. In addition, due to the addition of the mask, the model pay more attention to the oriented position (image or text). These are what make MMIAN an efficient and accurate model for web scenarios gaze predictor.


\section*{\uppercase{Acknowledgement}}
This work is funded by UDeco project by Germany BMBF-KMU Innovativ.
%Also the Grad-CAM based method is applied and the analysis shows that the model as a whole is more inclined to the salient part after using the Attentional Feature Fusion (AFF) module. That's shows the MMIAN model can use the layout information of the web page (image mask and text mask) to increase the perceptual field and improve the prediction accuracy.



%In summary, this paper focuses on how to perform saliency prediction in web scenarios, proposes a new model, and makes tests accordingly. The overall model performance is improved to a certain extent. In future work, multi-task learning approaches can be explored to to improve the accuracy achieved by the current approach. This multi-task learning will allow the model to predict not only the salient regions, but also the edge boxes of salient web components. Nevertheless, the feature fusion part of the model still needs to be further tested and improved. Such that the future model can achieve a higher accuracy potential.

%\section{\uppercase{{Acknowledgement}}}
%This works is funded by the xxx.

\bibliographystyle{apalike}
{\small
\bibliography{example}}



\end{document}

