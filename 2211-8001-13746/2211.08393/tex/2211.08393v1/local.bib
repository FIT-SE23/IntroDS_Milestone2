@inproceedings{Sheth2019pseudo,
title={Pseudo-Bayesian Learning via Direct Loss Minimization with Applications to Sparse Gaussian Process Models},
author={Sheth, Rishit and Khardon, Roni},
booktitle={Symposium on Advances in Approximate Bayesian Inference (AABI)},
year={2019}
}

@inproceedings{sheth2017excess,
title = {Excess Risk Bounds for the {B}ayes Risk using Variational Inference in Latent {G}aussian Models},
author = {Sheth, Rishit and Khardon, Roni},
booktitle = {{NIPS}},
pages = {5151--5161},
year = {2017},
}

@article{safebayes,
author = {Peter Grünwald and Thijs van Ommen},
title = {{Inconsistency of Bayesian Inference for Misspecified Linear Models, and a Proposal for Repairing It}},
volume = {12},
journal = {Bayesian Analysis},
number = {4},
publisher = {International Society for Bayesian Analysis},
pages = {1069 -- 1103},
year = {2017},
doi = {10.1214/17-BA1085},
URL = {https://doi.org/10.1214/17-BA1085}
}


@InProceedings{dlm-sgp,
  title = 	 { Direct Loss Minimization for Sparse Gaussian Processes },
  author =       {Wei, Yadi and Sheth, Rishit and Khardon, Roni},
  booktitle = 	 {Proceedings of The 24th International Conference on Artificial Intelligence and Statistics},
  pages = 	 {2566--2574},
  year = 	 {2021},
  editor = 	 {Banerjee, Arindam and Fukumizu, Kenji},
  volume = 	 {130},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {13--15 Apr},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v130/wei21b/wei21b.pdf},
  url = 	 {https://proceedings.mlr.press/v130/wei21b.html},
  abstract = 	 { The paper provides a thorough investigation of Direct Loss Minimization (DLM), which optimizes the posterior to minimize predictive loss, in sparse Gaussian processes. For the conjugate case, we consider DLM for log-loss and DLM for square loss showing a significant performance improvement in both cases. The application of DLM in non-conjugate cases is more complex because the logarithm of expectation in the log-loss DLM objective is often intractable and simple sampling leads to biased estimates of gradients. The paper makes two technical contributions to address this. First, a new method using product sampling is proposed, which gives unbiased estimates of gradients (uPS) for the objective function. Second, a theoretical analysis of biased Monte Carlo estimates (bMC) shows that stochastic gradient descent converges despite the biased gradients. Experiments demonstrate empirical success of DLM. A comparison of the sampling methods shows that, while uPS is potentially more sample-efficient, bMC provides a better tradeoff in terms of convergence time and computational efficiency. }
}

@article{gibbs_sampling,
 ISSN = {01621459},
 URL = {http://www.jstor.org/stable/24247065},
 abstract = {We propose a new data-augmentation strategy for fully Bayesian inference in models with binomial likelihoods. The approach appeals to a new class of Pólya–Gamma distributions, which are constructed in detail. A variety of examples are presented to show the versatility of the method, including logistic regression, negative binomial regression, nonlinear mixed-effect models, and spatial models for count data. In each case, our data-augmentation strategy leads to simple, effective methods for posterior inference that (1) circumvent the need for analytic approximations, numerical integration, or Metropolis–Hastings; and (2) outperform other known data-augmentation strategies, both in ease of use and in computational efficiency. All methods, including an efficient sampler for the Pólya–Gamma distribution, are implemented in the R package BayesLogit. Supplementary materials for this article are available online.},
 author = {Nicholas G. Polson and James G. Scott and Jesse Windle},
 journal = {Journal of the American Statistical Association},
 number = {504},
 pages = {1339--1349},
 publisher = {[American Statistical Association, Taylor & Francis, Ltd.]},
 title = {Bayesian Inference for Logistic Models Using Pólya–Gamma Latent Variables},
 urldate = {2022-06-21},
 volume = {108},
 year = {2013}
}

@Book{PRML,
  author = 	 "Christopher M. Bishop",
  title = 	 "Pattern Recognition and Machine Learning",
  publisher = 	 "Springer",
  year = 	 "2006",
}


@InProceedings{pac-m,
  title = 	 { PACm-Bayes: Narrowing the Empirical Risk Gap in the Misspecified Bayesian Regime },
  author =       {Morningstar, Warren R. and Alemi, Alex and Dillon, Joshua V.},
  booktitle = 	 {Proceedings of The 25th International Conference on Artificial Intelligence and Statistics},
  pages = 	 {8270--8298},
  year = 	 {2022},
  editor = 	 {Camps-Valls, Gustau and Ruiz, Francisco J. R. and Valera, Isabel},
  volume = 	 {151},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {28--30 Mar},
  publisher =    {PMLR},
  pdf = 	 {https://proceedings.mlr.press/v151/morningstar22a/morningstar22a.pdf},
  url = 	 {https://proceedings.mlr.press/v151/morningstar22a.html},
  abstract = 	 { The Bayesian posterior minimizes the "inferential risk" which itself bounds the "predictive risk." This bound is tight when the likelihood and prior are well-specified. How-ever since misspecification induces a gap,the Bayesian posterior predictive distribution may have poor generalization performance. This work develops a multi-sample loss (PAC$^m$) which can close the gap by spanning a trade-off between the two risks. The loss is computationally favorable and offers PAC generalization guarantees. Empirical study demonstrates improvement to the predictive distribution }
}


@InProceedings{safe-bayes-logistic,
  title = 	 { Safe-Bayesian Generalized Linear Regression},
  author =       {de Heide, Rianne and Kirichenko, Alisa and Grunwald, Peter and Mehta, Nishant},
  booktitle = 	 {Proceedings of the Twenty Third International Conference on Artificial Intelligence and Statistics},
  pages = 	 {2623--2633},
  year = 	 {2020},
  editor = 	 {Chiappa, Silvia and Calandra, Roberto},
  volume = 	 {108},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {26--28 Aug},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v108/heide20a/heide20a.pdf},
  url = 	 {https://proceedings.mlr.press/v108/heide20a.html},
  abstract = 	 {We study generalized Bayesian inference under misspecification,  i.e. when the model is ‘wrong but useful’. Generalized Bayes equips  the likelihood with a learning rate $\eta$. We show that for  generalized linear models (GLMs), $\eta$-generalized Bayes  concentrates around the best approximation of the truth within the  model for specific $\eta eq 1$, even under severely misspecified  noise, as long as the tails of the true distribution are exponential. We  derive MCMC samplers for generalized Bayesian lasso and  logistic regression and give examples of both  simulated and real-world data in which generalized Bayes  substantially outperforms standard Bayes.}
}


@inproceedings{
dvi,
title={Deterministic Variational Inference for Robust Bayesian Neural Networks},
author={Anqi Wu and Sebastian Nowozin and Edward Meeds and Richard E. Turner and Jose Miguel Hernandez-Lobato and Alexander L. Gaunt},
booktitle={International Conference on Learning Representations},
year={2019},
url={https://openreview.net/forum?id=B1l08oAct7},
}

@article{Meir-Zhang,
author = {Meir, Ron and Zhang, Tong},
title = {Generalization Error Bounds for Bayesian Mixture Algorithms},
year = {2003},
issue_date = {12/1/2003},
publisher = {JMLR.org},
volume = {4},
number = {null},
issn = {1532-4435},
abstract = {Bayesian approaches to learning and estimation have played a significant role in the Statistics literature over many years. While they are often provably optimal in a frequentist setting, and lead to excellent performance in practical applications, there have not been many precise characterizations of their performance for finite sample sizes under general conditions. In this paper we consider the class of Bayesian mixture algorithms, where an estimator is formed by constructing a data-dependent mixture over some hypothesis space. Similarly to what is observed in practice, our results demonstrate that mixture approaches are particularly robust, and allow for the construction of highly complex estimators, while avoiding undesirable overfitting effects. Our results, while being data-dependent in nature, are insensitive to the underlying model assumptions, and apply whether or not these hold. At a technical level, the approach applies to unbounded functions, constrained only by certain moment conditions. Finally, the bounds derived can be directly applied to non-Bayesian mixture approaches such as Boosting and Bagging.},
journal = {J. Mach. Learn. Res.},
month = {dec},
pages = {839–860},
numpages = {22}
}

@inproceedings{loss-surface,
author = {Garipov, Timur and Izmailov, Pavel and Podoprikhin, Dmitrii and Vetrov, Dmitry and Wilson, Andrew Gordon},
title = {Loss Surfaces, Mode Connectivity, and Fast Ensembling of DNNs},
year = {2018},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {The loss functions of deep neural networks are complex and their geometric properties are not well understood. We show that the optima of these complex loss functions are in fact connected by simple curves over which training and test accuracy are nearly constant. We introduce a training procedure to discover these high-accuracy pathways between modes. Inspired by this new geometric insight, we also propose a new ensembling method entitled Fast Geometric Ensembling (FGE). Using FGE we can train high-performing ensembles in the time required to train a single model. We achieve improved performance compared to the recent state-of-the-art Snapshot Ensembles, on CIFAR-10, CIFAR-100, and ImageNet.},
booktitle = {Proceedings of the 32nd International Conference on Neural Information Processing Systems},
pages = {8803–8812},
numpages = {10},
location = {Montr\'{e}al, Canada},
series = {NIPS'18}
}

@inproceedings{alexnet,
 author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {F. Pereira and C.J. Burges and L. Bottou and K.Q. Weinberger},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {ImageNet Classification with Deep Convolutional Neural Networks},
 url = {https://proceedings.neurips.cc/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf},
 volume = {25},
 year = {2012}
}

@InProceedings{preresnet,
author="He, Kaiming
and Zhang, Xiangyu
and Ren, Shaoqing
and Sun, Jian",
editor="Leibe, Bastian
and Matas, Jiri
and Sebe, Nicu
and Welling, Max",
title="Identity Mappings in Deep Residual Networks",
booktitle="Computer Vision -- ECCV 2016",
year="2016",
publisher="Springer International Publishing",
address="Cham",
pages="630--645",
abstract="Deep residual networks have emerged as a family of extremely deep architectures showing compelling accuracy and nice convergence behaviors. In this paper, we analyze the propagation formulations behind the residual building blocks, which suggest that the forward and backward signals can be directly propagated from one block to any other block, when using identity mappings as the skip connections and after-addition activation. A series of ablation experiments support the importance of these identity mappings. This motivates us to propose a new residual unit, which makes training easier and improves generalization. We report improved results using a 1001-layer ResNet on CIFAR-10 (4.62 {\%} error) and CIFAR-100, and a 200-layer ResNet on ImageNet. Code is available at: https://github.com/KaimingHe/resnet-1k-layers.",
isbn="978-3-319-46493-0"
}

@inproceedings{Jankowiak2020ParametricGP,
  title={Parametric Gaussian Process Regressors},
  author={Martin Jankowiak and Geoff Pleiss and Jacob R. Gardner},
  booktitle={ICML},
  year={2020}
}

@inproceedings{titsias2009variational,
  author = {Titsias, Michalis},
  title = {Variational Learning of Inducing Variables in Sparse {G}aussian Processes},
  booktitle = {AISTATS},
  year = {2009},
  pages = {567--574},
}

@inproceedings{bnn-rank1,
author = {Dusenberry, Michael W. and Jerfel, Ghassen and Wen, Yeming and Ma, Yi-An and Snoek, Jasper and Heller, Katherine and Lakshminarayanan, Balaji and Tran, Dustin},
title = {Efficient and Scalable Bayesian Neural Nets with Rank-1 Factors},
year = {2020},
publisher = {JMLR.org},
abstract = {Bayesian neural networks (BNNs) demonstrate promising success in improving the robustness and uncertainty quantification of modern deep learning. However, they generally struggle with underfitting at scale and parameter efficiency. On the other hand, deep ensembles have emerged as alternatives for uncertainty quantification that, while outperforming BNNs on certain problems, also suffer from efficiency issues. It remains unclear how to combine the strengths of these two approaches and remediate their common issues. To tackle this challenge, we propose a rank-1 parameterization of BNNs, where each weight matrix involves only a distribution on a rank-1 subspace. We also revisit the use of mixture approximate posteriors to capture multiple modes, where unlike typical mixtures, this approach admits a significantly smaller memory increase (e.g., only a 0.4% increase for a ResNet-50 mixture of size 10). We perform a systematic empirical study on the choices of prior, variational posterior, and methods to improve training. For ResNet-50 on ImageNet, Wide ResNet 28-10 on CIFAR-10/100, and an RNN on MIMIC-III, rank-1 BNNs achieve state-of-the-art performance across log-likelihood, accuracy, and calibration on the test sets and out-of-distribution variants.},
booktitle = {Proceedings of the 37th International Conference on Machine Learning},
articleno = {261},
numpages = {11},
series = {ICML'20}
}


@InProceedings{competition-approximate-inference,
  title = 	 {Evaluating Approximate Inference in Bayesian Deep Learning},
  author =       {Wilson, Andrew Gordon and Izmailov, Pavel and Hoffman, Matthew D and Gal, Yarin and Li, Yingzhen and Pradier, Melanie F and Vikram, Sharad and Foong, Andrew and Lotfi, Sanae and Farquhar, Sebastian},
  booktitle = 	 {Proceedings of the NeurIPS 2021 Competitions and Demonstrations Track},
  pages = 	 {113--124},
  year = 	 {2022},
  editor = 	 {Kiela, Douwe and Ciccone, Marco and Caputo, Barbara},
  volume = 	 {176},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {06--14 Dec},
  publisher =    {PMLR},
  pdf = 	 {https://proceedings.mlr.press/v176/wilson22a/wilson22a.pdf},
  url = 	 {https://proceedings.mlr.press/v176/wilson22a.html},
  abstract = 	 {Uncertainty representation is crucial to the safe and reliable deployment of deep learning. Bayesian methods provide a natural mechanism to represent epistemic uncertainty, leading to improved generalization and calibrated predictive distributions. Understanding the fidelity of approximate inference has extraordinary value beyond the standard approach of measuring generalization on a particular task: if approximate inference is working correctly, then we can expect more reliable and accurate deployment across any number of real-world settings. In this competition, we evaluate the fidelity of approximate Bayesian inference procedures in deep learning, using as a reference Hamiltonian Monte Carlo (HMC) samples obtained by parallelizing computations over hundreds of tensor processing unit (TPU) devices. We consider a variety of tasks, including image recognition, regression, covariate shift, and medical applications. All data are publicly available, and we release several baselines, including stochastic MCMC, variational methods, and deep ensembles. The competition resulted in hundreds of submissions across many teams. The winning entries all involved novel multi-modal posterior approximations, highlighting the relative importance of representing multiple modes, and suggesting that we should not consider deep ensembles a {“}non-Bayesian{”} alternative to standard unimodal approximations. In the future, the competition will provide a foundation for innovation and continued benchmarking of approximate Bayesian inference procedures in deep learning. The HMC samples will remain available through the competition website.}
}

@inproceedings{collapsed-elbo,
title={Collapsed Variational Bounds for Bayesian Neural Networks},
author={Marcin B. Tomczak and Siddharth Swaroop and Andrew Y. K. Foong and Richard E Turner},
booktitle={Advances in Neural Information Processing Systems},
editor={A. Beygelzimer and Y. Dauphin and P. Liang and J. Wortman Vaughan},
year={2021},
url={https://openreview.net/forum?id=ykN3tbJ0qmX}
}
