%%
%% This is file `sample-sigplan.tex',
%% generated with the docstrip utility.
%%
%% The original source files were:
%%
%% samples.dtx (with options: `sigplan')
%% 
%% IMPORTANT NOTICE:
%% 
%% For the copyright see the source file.
%% 
%% Any modified versions of this file must be renamed
%% with new filenames distinct from sample-sigplan.tex.
%% 
%% For distribution of the original source see the terms
%% for copying and modification in the file samples.dtx.
%% 
%% This generated file may be distributed as long as the
%% original source files, as listed above, are part of the
%% same distribution. (The sources need not necessarily be
%% in the same archive or directory.)
%%
%% The first command in your LaTeX source must be the \documentclass command.
%\documentclass[sigconf,anonymous,natbib=true]{acmart}
\documentclass[sigconf,natbib=true]{acmart}

%%
%% \BibTeX command to typeset BibTeX logo in the docs
\AtBeginDocument{%
 \providecommand\BibTeX{{%
 \normalfont B\kern-0.5em{\scshape i\kern-0.25em b}\kern-0.8em\TeX}}}

\usepackage{makecell}
\usepackage{rotating}
\usepackage{multirow}
\usepackage{lscape}
\usepackage{color, colortbl}
\usepackage{MnSymbol}


\definecolor{Gray}{gray}{0.9}
\definecolor{White}{rgb}{255,255,255}

\def\xxx{{\huge XXX}}
\long\def\gnote#1{[[[[\textbf{#1 -Bianca}]]]]}
\long\def\inote#1{[[[[\textbf{#1 -Stein}]]]]}
\long\def\wnote#1{[[[[\textbf{#1 -Wiese}]]]]}
\long\def\alunos#1{[[[[\textbf{#1 -Gerosa}]]]]}
\long\def\merge#1{{\color{red}#1}} 
\def\risco#1{{\color{red}\sout{#1}}} 

\newcommand{\draft}[1]{{\color{blue}{#1}}}

\newcommand{\MyPara}[1]{\vspace{.2em}\noindent\textit{\textbf{#1}}\hspace{.3em}}

\newcommand{\MyBox}[1]{\vspace{3mm}\noindent\framebox[\columnwidth][c]{\parbox[b]{0.95\columnwidth}{ #1 }}\vspace{3mm}}

\newboolean{showcomments}
\setboolean{showcomments}{true} % toggle to show or hide comments
\ifthenelse{\boolean{showcomments}}
 {\newcommand{\nb}[2]{
 \fcolorbox{gray}{yellow}{\bfseries\sffamily\scriptsize#1}
 {\sf\small$\blacktriangleright$\textit{#2}$\blacktriangleleft$}
 }
 \newcommand{\version}{\emph{\scriptsize$-$working$-$}}
 }
 {\newcommand{\nbb}[2]{}
 \newcommand{\version}{}
 }
 
 \newcommand{\ImageCite}[2]{#1$\filledstar#2$}

 
%% Rights management information. This information is sent to you
%% when you complete the rights form. These commands have SAMPLE
%% values in them; it is your responsibility as an author to replace
%% the commands and values with those provided to you when you
%% complete the rights form.
\setcopyright{acmcopyright}
\copyrightyear{2022}
\acmYear{2022}
\acmDOI{XXXXXXX.XXXXXXX}


%% These commands are for a PROCEEDINGS abstract or paper.
\acmConference[ESEM'22]{ESEM}{2022}{Helsinki, Finland}
\acmPrice{15.00}
\acmISBN{978-1-4503-XXXX-X/18/06}

%%
%% Submission ID.
%% Use this when submitting an article to a sponsored event. You'll
%% receive a unique submission ID from the organizers
%% of the event, and this ID should be used as the parameter to this command.
%%\acmSubmissionID{123-A56-BU3}

%%
%% For managing citations, it is recommended to use bibliography
%% files in BibTeX format.
%%
%% You can then either use BibTeX with the ACM-Reference-Format style,
%% or BibLaTeX with the acmnumeric or acmauthoryear sytles, that include
%% support for advanced citation of software artefact from the
%% biblatex-software package, also separately available on CTAN.
%%
%% Look at the sample-*-biblatex.tex files for templates showcasing
%% the biblatex styles.
%%

%%
%% The majority of ACM publications use numbered citations and
%% references. The command \citestyle{authoryear} switches to the
%% "author year" style.
%%
%% If you are preparing content for an event
%% sponsored by ACM SIGGRAPH, you must use the "author year" style of
%% citations and references.
%% Uncommenting
%% the next command will enable that style.
%%\citestyle{acmauthoryear}

\begin{document}

%\title{The Two Sides of a Coin\\ How do newcomers decide upon a task, and how would maintainers assist the contributor's infer skills to work on a task}

%Mismatches between newcomers and maina
%What skill do I need? mismatches between newcomer and maintainer perspective
%\title{How to choose a task? Mismatches between maintainer and newcomer perspectives}
%\title{How to choose a task? Mismatches in perspectives of newcomers and existing contributors}
\title{Supporting the Task-driven Skill Identification in Open Source Project Issue Tracking Systems} % with an Ontology-Based Knowledge Model}

%\title{How to choose a task? Mismatches in perspectives of different types of contributors}
%\title{What skills do newcomers need? Mismatches between maintainer and newcomer perspectives}
%How do I do this task? mismatches between newcomer and maintainer perspectives
%What skills do newcomers need for OSS task
%Newcomers' Skill needs for OSS tasks: mismatches between newcomer and maintainer perspectives
%Skills needed for OSS tasks: mismatches between newcomer and maintainer perspectives
%How newcomers should know the skills?
%Is this issue good for a newcomer? 
%Can I contribute to this task? Expectation gulf between newcomers and contributors
%Can I do this task? Newcomers' and maintainers' perspectives

%Expectation gulf between maintainer and newcomers perspective about newcomers' skill needs

%How to contribute to this task: mismatches in maintainer and newcomers about newcomers' skill needs
%How do I do this task? mismatches in maintainer and newcomers perspective about newcomers skill needs
%Skills newcomer/maintainer perspective gaps

\author{Fabio Santos}
\affiliation{%
 \institution{Northern Arizona University}
 \country{United States} \\
 \city{fabio\_santos@nau.edu}
}



%%
%% By default, the full list of authors will be used in the page
%% headers. Often, this list is too long, and will overlap
%% other information printed in the page headers. This command allows
%% the author to define a more concise list
%% of authors' names for this purpose.
\renewcommand{\shortauthors}{Fabio Santos}



%%
%% The abstract is a short summary of the work to be presented in the
%% article.
\begin{abstract}

%% ESEM -> A structured abstract is required with the headings: Background, Aims, Method, Results, and Conclusions. Papers should contain an explicit description of the empirical strategy used or investigated.

\textbf{[Background]} 
Selecting an appropriate task is challenging for contributors to Open Source Software (OSS), mainly for those who are contributing for the first time. Therefore, researchers and OSS projects have proposed various strategies to aid newcomers, including labeling tasks. 
\textbf{[Aims]} 
In this research, we investigate the automatic labeling of open issues strategy to help the contributors to pick a task to contribute. We label the issues with API-domains---categories of APIs parsed from the source code used to solve the issues. We plan to add social network analysis metrics gathered from the issues conversations as new predictors. By identifying the skills, we claim the contributor candidates should pick a task more suitable to their skill.
\textbf{[Method]} 
We are employing mixed methods. We qualitatively analyzed interview transcripts and the survey's open-ended questions to comprehend the strategies communities use to assist  in onboarding contributors and contributors used to pick up an issue. We applied quantitative studies to analyze the relevance of the API-domain labels in a user experiment and compare the strategies' relative importance for diverse contributor roles. We also mined project and issue data from OSS repositories to build the ground truth and predictors able to infer the API-domain labels with comparable precision, recall, and F-measure with the state-of-art. We also plan to use a skill ontology to assist the matching process between contributors and tasks. By quantitatively analyzing the confidence level of the matching instances in ontologies describing contributors' skills and tasks, we might recommend issues for contribution. In addition, we will measure the effectiveness of the API-domain labels by evaluating the issues solving time and the rate among the labeled and unlabelled ones. 
\textbf{[Results]} 
%Maintainers and contributors diverged in their opinions about the relative importance of various strategies. The results suggest that newcomers want a better contribution process and more support to onboard, while maintainers might expect to solve questions using the available communication channels. 
So far, the results showed that organizing the issues--which includes assigning labels is seen as an essential strategy for diverse roles in OSS communities. The API-domain labels are relevant, mainly for experienced practitioners. The predicted labels have an average precision of 75.5\%. 
\textbf{[Conclusions]} 
Labeling the issues with the API-domain labels indicates the skills involved in an issue. The labels represent possible libraries (aggregated into domains) used in the source code related to an issue. By investigating this research topic, we expect to assist the new contributors in finding a task, helping OSS communities to attract and retain more contributors.
%The gaps in perspectives between newcomers and existing contributors create a gulf of expectation. OSS communities can leverage our results to prioritize the strategies considered the most important by newcomers. 
\end{abstract}

%, it is not clear if newcomers' perspectives match those of existing contributors about the importance of these strategies.
% Maintainers of OSS projects often employ strategies to assist newcomers finding a task and attract newcomers. Newcomers, in turn, use their own strategies to choose a task. A discrepancy between the two perspectives on how to choose a task and how projects can assist newcomers might create a gulf of expectations or misunderstandings.

%the perspectives of maintainers and contributors regarding the strategies that newcomers adopt to choose tasks and that the community employs to facilitate this process. By discovering any gaps between the different perspectives, OSS communities can bring tailored strategies that match contributors' needs.
%\textbf{[Method]} We employed interviews to derive maintainer's and newcomers' strategies. Following, we ran a survey to investigate the maintainers' and contributors' perspectives. We quantitatively analyzed the survey answers using conjoint analysis to rank the relative importance of both strategies, to identify the convergence of thoughts regarding the newcomers', frequent contributors', and maintainers' strategies. 

%Maintainers and contributors diverged in their opinions about the relative importance of some strategies. 
%While maintainers believed the main effort must be done with project quality, documentation and communication, contributors aimed to have more access to experienced team members and technical documentation. 
%All groups converged on the importance of a good documentation, but frequent contributors and maintainers disagreed on the importance of the onboarding process for newcomers. 
%Supporting the onboarding of newcomers was not seem as a priority for maintainers while is the second and third for frequent contributors and newcomers, respectively. Newcomers were more concerned with the contribution process than frequent contributors and maintainers, and the communication was a second top priority (tied) for maintainers and only the fifth for newcomers. 
%The results suggest that newcomers want better contribution process and more support to onboard while maintainers perhaps expect solve questions using the available communication channels. 

%For maintainers, the top 3 strategies they can take to support contributions are organizing, labeling the issues, and improving the contributing process. Frequent contributors corroborate with maintainers about the process improvements and labeling. However, they believe the OSS projects must support the newcomers better. 

%For example, frequent contributors, differently than maintainers, consider ``feeling confident to contribute'' as relatively highly important, and maintainers reported only 23.7\% to the same strategy. ``understand the context'' (of the project) participates only with 5.3\% of the relative importance for maintainers, but it represents 20\% for newcomers. 

%about newcomers' and OSS projects' actions to identify skills from issues and support the onboarding of newcomers in finding a suitable issue. 

%However, newcomers' ideas of what is needed might be different from the strategies employed by maintainers, creating a gulf of expectations. 
%prescribed we found maintainers, frequent contributors and newcomers groups think different regarding the priority strategies. Also the studied groups adopt diverse steps or strategies to identify the necessary skills to address an issue. 
%the problem seems not to be entirely solved due to the high number of open issues in OSS projects. 

%%
%% The code below is generated by the tool at http://dl.acm.org/ccs.cfm.
%% Please copy and paste the code instead of the example below.
%%
\begin{CCSXML}
<ccs2012>
 <concept>
 <concept_id>10010520.10010553.10010562</concept_id>
 <concept_desc>Software and its engineering~Open source software</concept_desc>
 <concept_significance>500</concept_significance>
 </concept>
</ccs2012>
\end{CCSXML}

\ccsdesc[500]{Software and its engineering~Open source software}

%%
%% Keywords. The author(s) should pick words that accurately describe
%% the work being presented. Separate the keywords with commas.
\keywords{open source software, issue tracker, task management, newcomers, social coding platform, strategies}
\maketitle

\section{Context and Problem}
\label{sec:context_problem}

%OSS engages a massive, globally distributed community of contributors~\citep{crowston2007self} and establishes a relevant economic driving force~\citep{lerner2010comingled}. The sustainability of many projects depends on their capacity to engage and retain a motivated group of developers~\citep{avelino2019empirical,miller2019dropout,qiu2019social,qureshi2011socialization}. 

The first steps toward contributing to OSS are challenging for some developers \cite{steinmacher2015systematic}. Developers are often required to pick up a task from a list of open issues, which may have varying complexity levels and require different skills to be completed. The skills required for a task are hard to presume based on the available task data~\citep{Bettenburg:2007:QBR:1328279.1328284,vaz2019empirical,zimmermann2010makes}. The difficulty in identifying skills in the context of software development might be caused by the diversity of knowledge concepts. They can range from certain program concepts to a more fine-grained level (even a specific class or package name) and encompass hard or soft skills~\citep{vadlamani2020studying,fitts1967human}.
%\footnote{In the context of software development, a skill may represent knowledge about certain program concepts or at a more refined grain (even a specific class or package name) and encompass hard or soft skills~\citep{vadlamani2020studying,fitts1967human}.} 

%Unlikely other areas, Software Engineering, is hard to identify and match tasks and skills due to the lack of objective measures~\citep{baltes2018towards}.

%The semantics and the structure of the skills can vary and may or organized using taxonomies or ontologies \cite{consoli2015taxonomy,bottazzi2002corporate,pavitt1984sectoral,cooke1997regional}. For the software engineering tasks, which seem to be closer to the OSS task finding problem, many authors proposed ontology or taxonomies to organize the skills \cite{ruiz2006using, gavsevic2009ontologies, mwakondo2018model,azuma2003apply,surakka2004analysis} or machine learning techniques to recommend matches.


%Thus, identifying the required skills in an ITS is still a challenge. 

%Many authors agree that it is hard to find the right project and issue to contribute~\citep{Bettenburg:2007:QBR:1328279.1328284,vaz2019empirical,zimmermann2010makes}. As a direct consequence of the problem, OSS projects can be seriously impacted if they cannot retain developers, as mentioned in the literature~\citep{avelino2019empirical,miller2019dropout,qiu2019social,qureshi2011socialization}. Also, the number of projects and the involved economics makes this problem worth solving~\citep{crowston2007self,lerner2010comingled}.~\citep{steinmacher2015systematic} classified the barriers that new contributors face when contributing to open source software projects. %\ref{fig:barriers}.

%In this sense, I aim to identify which skills are required for a given OSS project issue by mining information from different artifacts (e.g., source code, pull requests, commits). Identifying the skills related to a project and its issues may help these contributors find appropriate tasks. New contributors usually do not have previous knowledge about the project and do not know the project structure and libraries used. Thus, having tips about the skills may assist them to select an open issue related to their skills or one they want to learn.

%Thus the research problem is "newcomers struggle to identify the right task to contribute based on their skillset and experience."


%Finding tasks to contribute to in Open Source projects is challenging ~\cite{wang2011bug,steinmacher2015understanding,steinmacher2015systematic,10.1145/2675133.2675215,stanik2018simple}. Open tasks vary in complexity and required skills, which can be difficult to determine solely by reading the task descriptions alone, especially for new contributors~\cite{zimmermann2010makes,Bettenburg:2007:QBR:1328279.1328284,vaz2019empirical}. 

Adding labels to the issues (a.k.a tasks, bug reports) helps  contributors when they are choosing their tasks~\cite{steinmacher2018let}. However, community managers find labeling issues challenging and time-consuming \cite{9057411} because projects require skills in different languages, frameworks, database management systems, and Application Programming Interfaces (APIs). Many recent studies point to manual or automatic methods to classify issues, but the classification is restricted to the nature of the issue---bug/non-bug, questions, documentation, etc.~\citep{antoniol2008bug,fan2017road,kallis2019ticket,otoom2019automated,pandey2017automated,pingclasai2013classifying,xia2013tag,zhou2016combining,zhu2019bug}. 

Issue Tracking Systems (e.g., GitHub, Jira) create an environment that enables collaboration among developers. Previous studies leverage ``social metrics'' calculated from data mined from the developers interactions to compose successful predictive models in diverse domains: co-changes \cite{wiese2017using}, defects \cite{zimmermann2008predicting} and failures \cite{meneely2008predicting}. \citet{wiese2014social} studied which metrics were used as predictors. In this research, we aim to explore whether such metrics can improve our predictive models. Uncovering good predictors can also help us hypothesize about conceptual relationships between socials aspects of the software development and skill identification.
%Issue Tracking Systems (e.g., GitHub, Jira) create an environment that enables collaboration among developers. Previous studies leverage ``social metrics'' calculated from data mined from the developers interactions to compose successful predictive models in diverse domains: co-changes \cite{wiese2017using}, defects \cite{zimmermann2008predicting} and failures \cite{meneely2008predicting}. %\citet{wiese2014social} studied which metrics were used as predictors. 
%In this research, we aim to explore whether such metrics can improve our predictive models. Uncovering good predictors can also help us hypothesize about conceptual relationships between Issue Tracking Systems (e.g., GitHub, Jira) %create an environment that enables collaboration among developers. Previous studies leverage ``social metrics'' calculated from data mined from the developers interactions to compose successful predictive models in diverse domains: co-changes \cite{wiese2017using}, defects \cite{zimmermann2008predicting} and failures \cite{meneely2008predicting}. \citet{wiese2014social} studied which metrics were used as predictors. In this research, we aim to explore whether such metrics can improve our predictive models. Uncovering good predictors can also help us hypothesize about conceptual relationships between social aspects of software development and skill identification.

%Some use only the developer information to derive the skills. Regarding the identification of experts, the literature presents techniques based on general metrics, such as developer's interests (e.g., what they program in their free time) and technical skill (e.g., programming languages they are using) \cite{greene2016cvexplorer}, developer's activity from Stack Overflow and GitHub \cite{huang2016cpdscorer}, and individual's performance on a set of programming tasks~\cite{bergersen2014construction}. On the other hand, some techniques use traces from source code ~\cite{msr2019,da2015niche}, bug report data ~\cite{anvik2011reducing}, and merging pull requests data ~\cite{8716294} to rank developers who are the most appropriate to review code, merge a pull request, or even comment on an issue.%, and some do not predict the task complexity level 

%While many approaches try to mine data from OSS repositories to provide more information or requirements about the open tasks, the way we organize it may be crucial to suggest possible matches to a new contributor automatically. Suggesting issues based on skills informed may assist the new contributor in finding out where to start. 

%Researchers have been trying to match issues to contributors and proposed different strategies to mitigate this problem, such as recommending tasks \cite{anvik2011reducing,kevic2013collaborative,linares2012triaging}, finding experts to merge a pull request \cite{8716294}; identifying experts \cite{baltes2018towards, mining_usage_expertise, da2015niche, msr2019}; increasing engagement in code review discussions \cite{kavaler2018whom,ibrahim2010should} or reviewing and maintaining code \cite{thongtanunam2015should}. \cite{macdonald2006voting} introduced a voting system able to get data from documents updated history and heuristically suggest issues. \cite{ashok2009debugadvisor} recommend people, source files, binaries, and source functions using a fat query that encompasses structured and unstructured data. 
%\cite{anvik2011reducing} recommends experts, interested developers, and components using the historical information from five OSS projects.

%In general, the proposed strategies are useful for identifying experienced members based on their history of previous contributions and interactions, and usually in the context of a single project. However, those strategies cannot help newcomers on matching their skills with the ones required by the open issues, since they do not have historical data.

%The job market used to do it to recruit workers for projects. The notion of a contribution in OSS is somewhat similar to performing a task in a job. However, instead of matching single tasks, they focus on a broader concept: a position. A position requires a set of skills since it embraces many intrinsic tasks. However, OSS projects are not like positions since the skills may vary and be defined by demand, related to the new issue open in the ITS. The way the job market tries to match positions and workers seems inadequate to the OSS projects because they are based on the educational attainment or the type of ICTs training needed to perform well \cite{ruiz2020icts,velciu2017matching} or based on an empiric evaluation of the project schedule and individual skills \cite{e2013decision}. 

%Although many authors proposed taxonomies or ontologies to structure the skills for the information technology area, they do not consider the task skills semantic level that would not be previously predicted, like a project or position. They also have different granularity from education or graduate disciplines.
 
In this work, we extend the existing research by proposing an automated approach to identify skills required to work on an issue.  We apply machine learning and a skill ontology%the automatizing of the skills discovery in Issue Tracking Systems (ITS) issues with machine learning and supported by a skill ontology 
to assist in matching the issues' and new contributors' skills. The skills predicted will be presented as labels based on categories of APIs, called API-domain labels (``UI'', ``Cloud'', ``Error Handling'' etc), defined by a group of experts. APIs usually encapsulate modules with specific purposes (e.g., cryptography, database access, logging, etc.), abstracting the underlying implementation. If the contributors know which categories of APIs will be required to work on each issue, they could choose tasks that better match their skills or involve skills they want to learn. 

We expect that our work will have the following contributions: %This study's contributions are:

\begin{itemize}

\item definition of categories or API-domain labels to be used in software projects using a skill ontology.

%\item%an issue-oriented approach to investigate the developer's perception and contributions, 
\item an approach to predict labels using the information related to the issue.

\item an investigation of the usefulness of the API-domain labels.

\item an evaluation of the labeling skills as a strategy with diverse stakeholders.

%\item modeling the issue skills using a skill ontology and the predicted API-domains. 

%\item an experiment to match the contributor informed skills and the issue skills to determine a confidence level of the skills matching.

%\item evaluating the solution time of closed and merged issues with and without the API-domain labels

\end{itemize} 

%The research topic is automated skills identification and modeling in OSS issues using machine learning. 

\section{Research Goals and Questions}

%To solve the problem described in the previous section, It is mandatory to identify the skills in the open issues and also discover what strategies were proposed in academia to address this problem, leading us to the research questions below:

To address the aforementioned problem, we propose the following research questions:

%-RQ1: What to show to the newcomers to help them select the right issue to contribute? 

\textbf{RQ1.} To what extent can we predict the domain of APIs used in the code that fixes a software issue?

RQ1 aims to propose and evaluate an automatic labeling approach quantitatively.
%\textbf{RQ1.1.} What configurations of tested techniques improve the labels predictions?

%\textbf{RQ1.2.} How well do the API-domain labels match the skills needed according to the opinion of contributors who solved the issues?

%\textbf{RQ1.3.} What information in the ITS does provide the skills?
%What information in the ITS does provide the skills?

%\textbf{RQ1.4.} To what extent can we automatically attribute API-domain labels to issues using transfer learning?%How accurate can the approaching transfer learning between projects? %To what extent the model can transfer learning?

\textbf{RQ2.} How does the labeling strategy impact the issue choice?

RQ2 aims to investigate the label's strategy and the API-domain labels from the point of view of communities and contributors.

%\textbf{RQ2.1.} How relevant are API domain labels to potential future contributors?%new contributors?

%%%OLD%\textbf{RQ2.2.} What strategies help newcomers choose a task in OSS?

%\textbf{RQ2.2.} How do newcomers and existing contributors differ in their perceptions about the labeling strategy?

%\textbf{RQ2.3.} What labels do contributors want to see in issues?

%-RQ2: How do OSS members evaluate required skills from the issue tracker's information?

%-RQ3: What efficient and effective ways can identify the skills in the open tasks present at issue tracking systems?

%\textbf{RQ2.4.} How do the API-domain labels impact the solution time of issues?

%\textbf{RQ2.5.} How do the API-domain labels impact the solution rate of issues?

\textbf{RQ3.} To what extent can contributors match their skills with tasks?

RQ3 aims to evaluate the possibility of automatically matching the issues and tasks' skills. 
%\textbf{RQ3.1.} How do model the skills of users and tasks?

%\textbf{RQ3.2.} How relevant are  the matches under the contributors' view?%How well do the matches perform under the contributors' view?

% interface to evaluate the labels or survey with labels. After selecting the tags, the UI shows a set of suggestions and asks for evaluation.

By understanding how new and experienced contributors select the issues on ITS % and the difficulties related to this process,
 and the strategies communities use to assist contributors, it is possible to create an approach to identify, label the skills and match with candidate contributors. %customize, and expose the data to the user interface, 
Therefore, helping them choose the most appropriate task and receive feedback about the labels generated.
%We also formulate a research hypothesis: "By identifying skills on the issue track system (ITS), newcomers will be able to find an issue that matches their skills for contributions.%find with a high tax of success right issues for contributions." 
%Finally, we propose the research objectives as follows:
%To identify the skills needed to solve issues from selected projects;
%To describe the issues to help newcomers address them according to their level of expertise or experience and identify the issues' skills to improve the contribution success rate.
%We limit the scope of the research to the OSS projects and industry related to systems or apps that use popular programming languages, with source code available and runnable. In addition, the data must have access, and with the amount needed to address the specific strategy to be defined and where issues can be accessed and understandable by newcomers with programming skills. %knowledge level, to be explained later.

\section{Research Plan}

\begin{figure*}[htb]
\centering
\includegraphics[width=.8\textwidth]{Figures/ResearchMethodOverview-DS-ESEM-I.pdf}
\caption{Research Method Overview. Green = published, Yellow = under review, Red = in progress, Gray = tool.}
\label{fig:reserachMethodOverview}
\end{figure*}

The study started with a case study (JabRef project), which identified skills based on the API-Domains mined from GitHub. The API labels were predicted with precision, recall, and F-measure of 0.755, 0.747, and 0.751, respectively using the Random Forest algorithm~\citep{santos2021can} (Figure \ref{fig:reserachMethodOverview} - Stage 1).

We also ran a user experiment with 74 students and practitioners to assess the labels' relevancy. The participants picked a task in a mocked page, in which we inserted API-domain labels. They also reported which regions of the open issue page they perceived as relevant. Finally, they explained why the issue regions were relevant and what type of labels they would like to see in an ITS~\citep{santos2021can} (Figure \ref{fig:reserachMethodOverview} - Stage 1). The participants found the labels information important to decide which issue to contribute and the API-domains labels relevant even more than the components labels which are present in the project. 

Then, we interviewed 17 maintainers of the projects to learn \textit{``how do newcomers choose an issue, and how can the community help?''}. %how they identify skills when reading a project issue in an ITS and how they understand the strategies newcomers should employ to identify skills and projects to facilitate this process 
(Figure \ref{fig:reserachMethodOverview} - Stage 3). We also investigated mismatches of perceptions from diverse OSS community stakeholders regarding how to help new contributors find a task to start with. The strategies' investigation counted with a survey with 64 participants who evaluated the newcomers' and maintainers' strategies proposed by the 17 interviewees. The study concluded new contributors and maintainers disagree about the relative importance of the onboarding newcomers community's strategy and new contributors and frequent contributors disagree about the importance of the setup the environment contributors' strategy. The study will appear in ESEM 2022 \cite{santos2022how}.

%The intuition about this research step is: that if they have a diverse understanding of how to organize the project and verify the information from the issue and the project, before contributing, they can introduce noise to the process followed by the newcomers to identify the skills. On the other hand, the OSS projects must prioritize the organizational effort done by the maintainers in places where newcomers evaluate it as crucial to finding the skills.

We are extending the tooling to identify skills employed in the case to use a model trained with BERT, three different ITS: GitHub, Gerrit, and Jira, three programming languages: Java, C++, and C\#, five projects: JabRef, audacity, PowerToys, RTTS, and Cronos. Those two last are projects from the industry. Extending to industry projects aims to verify whether stakeholders can apply the research to industry software. The improvements embraced a new semi-automatic API classification model carried out manually in the first experiment (Figure \ref{fig:reserachMethodOverview} - Stage 2). We are also asking project developers to analyze the labels predicted from the new projects to give us feedback about the labels.

In the following steps, we will use the results from social network analysis to predict the API-domain labels. The API-domain labels will use a skill ontology to match users' and tasks' skills in an experiment (Figure \ref{fig:reserachMethodOverview} - Stage 4). Finally, the API-domain labels will be evaluated by applying them to random issues. After a period, we will assess the percentage of solved issues and the time of solution. At the same time, a tool will suggest issues to contribute based on the contributor's skills informed in a web form (Figure \ref{fig:reserachMethodOverview} - Stage 5). 

%RQ4
% HERE WE CAN EXPLORE THE LABELS SUGGESTION ALONE OR TRY TO CONSOLIDATE THE PROBLEMS OBSERVED BY THE TRIANGULATION WITH THE LABELS SUGGESTED.

\section{Publications}
\label{sec:publications}

The research conducted as part of this dissertation resulted in the publications:

\underline{Santos, F.}, Trinkenreich, B., Nicolati Pimentel, J.F., Wiese, I., Steinmacher, I., Sarma, A. and Gerosa, M.A., 2022, June. How to choose a task? Mismatches in perspectives of newcomers and existing contributors. In: \textit{ 2022 16th ACM/IEEE International Symposium on Empirical Software Engineering and Measurement (ESEM) }. ACM/IEEE.

\underline{Santos, F.}, Trinkenreich, B., Wiese, I., Steinmacher, I., Sarma, A. and Gerosa, M.A., 2021, May. Can I Solve It? Identifying APIs Required to Complete OSS Tasks. In: \textit{ 2021 IEEE/ACM 18th International Conference on Mining Software Repositories (MSR) (pp. 346-257)}. IEEE.

I also participated in the CHASE 2021 conference as WebChair.

\section{Expected Contributions}

To the best of our knowledge, no previous work provides ways to automatically label the skills in OSS issues, match the contributors and tasks skills using a skill ontology, and evaluate the labels strategy and the predicted labels with users. 

\section{Main Preliminary Results } 
\label{sec:preliminary_results_stage1}

\subsection{Stage 1 - JabRef Case Study} 
\label{sec:results-jabref-case}

We report the results grouped by research question \footnote{More details about the results, including statistical tests, are available in the published papers}.

\subsubsection{RQ1. To what extent can we predict the domain of APIs used in the code that fixes a software issue?}

To answer this research question, we predicted the API-domain labels using diverse configurations of corpus (only \textsc{title} (T), only \textsc{body} (B), \textsc{title} and \textsc{body} (T, B), and \textsc{title}, \textsc{body}, and \textsc{comments} (T,B,Comments)), grams (unigrams to 4-grams) using TF-IDF and a multi-label classification to predict n API-domain labels. The ground truth is composed by APIs declared in source code files attached to commits and linked to issues. 
%RQ1. Multi-label classification to identify APIs (RQ1)
%RQ1. To what extent can a multi-label classification approach be used to predict the APIs used in the files that the task will change?
%RQ1: To what extent can we expect the APIs required in files that are part of a task? 

%To predict the API domains used in the files changed in an issue (RQ1), we started by testing a simple configuration used as a baseline. For this baseline model, we used only the issue \textsc{title} as input and the Random Forest (RF) algorithm since is insensitive to parameter settings~\citep{RandomForestShane} and has shown to yield good prediction results in software engineering studies~\citep{petkovic2016using,goel2017random,pushphavathi2014novel,satapathy2016early}. Then, we evaluated the corpus configuration alternatives, varying the input information: only \textsc{title} (T), only \textsc{body} (B), \textsc{title} and \textsc{body} (T, B), and \textsc{title}, \textsc{body}, and \textsc{comments} (T,B,Comments). We selected the best Random Forest configuration to compare the different models and used the Mann-Whitney U test with the Cliff's-delta effect size.

%We also tested alternative configurations using n-grams. For each step, the best design was kept. Then, we used different machine learning algorithms compared with a dummy (random) classifier.

%we first started by comparing the possible corpus setups and the following steps were to test set up alternatives for n-grams, number of words, and machine learning algorithms. For each new phase, the best result was kept as baseline until the comparison of the algorithms, where we used a dummy (random) classifier as a baseline.

\input{Tables/resultsFRFM}

As Table~\ref{tab:results} shows, when we tested different inputs and compared to \textsc{Title} only, all alternative settings provided better results. We could observe improvements in terms of precision, recall, and F-measure. When using \textsc{title}, \textsc{body}, and \textsc{comments}, we reached Precision of 75.5\%, Recall of 74.7\%, and F-Measure of 75.1\%. 

%\draft{an average 4\% difference in metrics between the corpus setup with title and the other corpus options}. 
%only a slight difference between the corpus setup with precision in the range (0,655-0.663), recall (0.606-0.628), and F-measure (0,630-0.641).

%Accordingly, the Mann-Whitney U test showed no statistical difference between the title and the title+body for both F-measure (p=0.260) and precision (p=0.312). We did not find statistical significance by comparing the title+body+comments model with the title model for F-measure (p=0.339) and precision (p=0.396).
%\todo{Insert data in a table}

\begin{comment}

\begin{table}[h!]
 \begin{center}
 \caption{Cliff's Delta for F-Measure and Precision: comparison of corpus models alternatives - Section III-B-1. Title(T), Body(B) and Comments (C).}
 \label{tab:resultsH1H2H3cliffs}
 \begin{tabular}{l|r|l|r|l} 
 \hline
 \textbf{Corpus} & \multicolumn{4}{c}{\textbf{Cliff's delta}} \\
 \cline{2-5}
 \textbf{Comparison} & \multicolumn{2}{c|}{\textbf{F-measure}} & \multicolumn{2}{c}{\textbf{Precision}}\\
 \hline

T versus B & -0.86 & large*** & -0.92 & large***\\
T versus T+B & -0.8 & large** & -0.88 & large***\\
T versus T+B+C & -0.88 & large** & -0.88 & large*** \\
B versus T+B & 0.04 & negligible & 0.04 & negligible \\
B versus T+B+C & -0.24 & small & -0.12 & negligible \\
T+B versus T+B+C & -0.3 & small & -0.08 & negligible\\
\hline
\multicolumn{5}{l}{\scriptsize\textit{* p $\leq$ 0.05;
** p $\leq$ 0.01; 
*** p $\leq$ 0.001}}
\\

 \end{tabular}
 \end{center}
\end{table}

\end{comment}
%We found statistical differences comparing the results using \textsc{title} only and all the three other corpus configurations for both F-measure (p-value $\leq$ 0.01 for all cases, Mann-Whitney U test) and precision (p-value $\leq$ 0.001 for all cases, Mann-Whitney U test) with large effect size. \textsc{Title}+\textsc{body}+\textsc{comments} performed better than all others in terms of precision, recall, and F-measure. However, the results suggest that using only the \textsc{body} would provide good enough outcomes since there was no statistically significant difference compared to the other two configurations---using \textsc{title} and/or \textsc{comments} in addition to the \textsc{body}--- and it achieved similar results with less effort. The model built using only \textsc{body} presented only 14.3\% incorrect predictions (hamming loss metric) for all 12 labels. Table \ref{tab:resultsH1H2H3cliffs} shows the Cliff's-delta comparison between each pair of corpus configuration. 

%We found a ``large'' effect size considering the F-measure (\textit{$|$d$|$}=-0.86) and precision (\textit{$|$d$|$}=-0.92) against the model using only title when compared with the model using only issue \textsc{body}. When we compared the models with \textsc{body}, \textsc{title+body} and the model grouping all information (issue \textsc{title}, \textsc{body}, and \textsc{comments}) we found Mann U varying from p=0.13 to p=0.45 for F-measure and p=0.33 to p=0.45 for precision, thus we cannot reject the hypothesis for the same distribution. Also Cliffs-delta was in the range from \textit{$|$d$|$}=0.04 to -0.3 (negligible to small) in F-measure and \textit{$|$d$|$}=0.04 to -0.12 (negligible) in precision. As 

%We found a ``small'' (\textit{$|$d$|$} = 0.24) difference considering the F-measure in favor of the baseline model (RF using only title) and a negligible (\textit{$|$d$|$}=-0.06) difference for precision when we compared the baseline model against the model using only issue \textsc{body}. When we compared the baseline model with the model grouping all information (issue \textsc{title}, \textsc{body}, and \textsc{comments}). As we found the same statistical distribution for F-measure with the effect size varied from negligible to small, results suggest that the baseline model better predicted the API-domain labels even with a small effect size. In general, this model only presented %17\% incorrect predictions (hamming loss metric) for all 12 labels.

\begin{comment}
\begin{figure}
\centering
\includegraphics[width=.5\textwidth, trim= 0px 25px 20px 40px]{Figures/BaselinexH5tfidfmeta.png}
\caption{Comparison between the unigram model and n-grams models}
\label{fig:baselineH5iftdf}
\end{figure}
\end{comment}
%Next, to verify what n-gram setups yielded better results for the classifier, we investigated the use of bi-grams, tri-grams, and four-grams, compared to the use of unigrams to create the corpus. We used the corpus with only the issue title for this analysis since this configuration performed best in the last step. Fig. \ref{fig:baselineH5iftdf} and Table \ref{tab:resultsH5cliffs} presents how the Random Forest model using issue title performs for each n-gram configuration. While the recall values were higher with the models that use more terms, the precision was better for uni-grams. We did not find differences when comparing the F-measure. Still, it is often better to have high precision over high recall in this scenario to avoid offering a false or misguided recommendation to developers. 

%Next, we investigated the use of bi-grams, tri-grams, and four-grams, comparing the results to the use of unigrams. We used the corpus with only issue \textsc{body} for this analysis since this configuration performed well in the last step. 
%Fig. \ref{fig:baselineH5iftdf} %and Table \ref{tab:resultsH5cliffs} 
%presents how the Random Forest model performs for each n-gram configuration. %The unigram configuration outperformed the others with a large effect size. 

%After, we compared the use of a different number of tokens as defined in the section \ref{sec:DataExtraction}. We did not find a statistically significant difference among the models with 90, 179, 358, 716, 1432, and 2864 words. The model with 5728 words was the only that presented a different distribution with ``large'' effect size --- Cliffs-delta (\textit{$|$d$|$}) varying from 0.70 to 0.88. However, the model with 899 words had the lowest Hamming loss mean (0.143), so we used it as a baseline.

%Table \ref{tab:resultsH5cliffs} presents the comparison with the baseline using unigrams versus the random forest models created with 2-gram, 3-gram, and 4-gram. We notice that unigram models (denoted by "1,1") have better precision than the models built using bi-grams, tri-grams, and four-grams. Although the recall for these models is higher than the baseline using unigram, we could not find differences when comparing the F-measure.


%todo{R1 Q4} \draft{small changes (column name, caption and "versus" in table III}
\begin{comment}   

\begin{table}[h!]
 \begin{center}
 \caption{Cliff's Delta for F-Measure and Precision: Comparison between n-grams models - Section III-B-5}
 \label{tab:resultsH5cliffs}
 \begin{tabular}{l|r|l|r|l} 
 \hline
 \textbf{n-Grams} & \multicolumn{4}{c}{\textbf{Cliff's delta}} \\
 \cline{2-5}
 \textbf{Comparison} & \multicolumn{2}{c|}{\textbf{F-measure}} & \multicolumn{2}{c}{\textbf{Precision}}\\
 \hline
%1 versus 2 & 0.0 & negligible & 0.26 & small\\
%1 versus 3 & -0.04 & negligible & 0.28 & small \\
%1 versus 4 & 0.14 & negligible & 0.34 & medium \\
%2 versus 3 & 0.04 & negligible & 0.0 & negligible \\
%2 versus 4 & 0.2 & small & 0.02 & negligible \\
%3 versus 4 & 0.2 & small & 0.06 & negligible\\

1 versus 2 & 1.0 & large*** & 0.86 & large***\\
1 versus 3 & 1.0 & large*** & 0.84 & large*** \\
1 versus 4 & 1.0 & large*** & 0.96 & large*** \\
2 versus 3 & 0.8 & large** & 0.18 & small \\
2 versus 4 & 0.78 & large** & 0.72 & large** \\
3 versus 4 & 0.12 & negligible & 0.62 & large*\\
\hline
\multicolumn{5}{l}{\scriptsize\textit{* p $\leq$ 0.05;
** p $\leq$ 0.01; 
*** p $\leq$ 0.001}}
\\
 \end{tabular}
 \end{center}
\end{table}

\end{comment}
Finally, to investigate the influence of the machine learning (ML) classifier, we compared several options using the title with unigrams as a corpus. The options included: Random Forest (RF), Neural Network Multilayer Perceptron (MLPC), Decision Tree (DT), LR, MlKNN, and a Dummy Classifier with strategy ``most\_frequent''. Dummy or random classifiers are often used as a baseline~\citep{saito2015precision, flach2015precision}. %We used the implementation from the Python package scikit-learn~\citep{sklearn}. 
Fig. \ref{fig:baselineH6} shows the comparison among the algorithms. %, and Table \ref{tab:resultsH6cliffs} presents the pair-wise statistical results comparing F-measure and precision using Cliff's delta. 

\begin{figure}[!hbt]
\centering
\includegraphics[width=.5\textwidth, trim= 10px 25px 20px 20px]{Figures/BaselinexH6Algmeta.png}
\caption{Comparison between the baseline model and other machine learning algorithms}
\label{fig:baselineH6}
\end{figure}

%\todo{R1 Q4} \draft{small changes (column name, caption and "versus") i table IV}
\begin{comment}

\begin{table}[hbt!]
 \begin{center}
 \caption{Cliff's Delta for F-Measure and Precision: Comparison between machine learning algorithms - Section III-B-5}
 \label{tab:resultsH6cliffs}
 \begin{tabular}{l|r|l|r|l} 
 \hline
\textbf{Algorithms} & \multicolumn{4}{c}{\textbf{Cliff's delta}} \\
 \cline{2-5}
\textbf{Comparison} & \multicolumn{2}{c|}{\textbf{F-measure}} & \multicolumn{2}{c}{\textbf{Precision}}\\
 \hline
%RF versus LR & 0.24 & small & -0.22 & small\\
%RF versus MLPC & 0.72 & large & 0.56 & large \\
%RF versus DT & 0.96 & large & 0.82 & large \\
%RF versus MlkNN & 1.0 & large & 0.96 & large \\
%LR versus MLPC & 0.66 & large & 0.72 & large \\
%LR versus DT & 0.88 & large & 0.9 & large \\
%LR versus MlkNN & 1.0 & large & 0.96 & large \\
%MPLC versus DT & -0.24 & small & -0.06 & negligible \\
%MPLC vs. MlkNN & -0.04 & negligible & 0.12 & negligible \\
%MlkNN versus DT & -0.66 & large & -0.12 & negligible \\

RF versus LR & 1.0 & large*** & 0.62 & large*\\
RF versus MLPC & 0.54 & large* & 0.88 & large*** \\
RF versus DT & 1.0 & large*** & 1.0 & large*** \\
RF versus MlkNN & 0.98 & large*** & 0.78 & large*** \\
LR versus MLPC & -0.96 & large*** & 0.24 & small \\
LR versus DT & 0.4 & medium & 0.94 & large*** \\
LR versus MlkNN & 0.5 & large* & 0.48 & large* \\
MPLC versus DT & 0.98 & large*** & 0.98 & large*** \\
MPLC vs. MlkNN & 0.94 & large*** & 0.32 & small \\
MlkNN versus DT & -0.28 & small & 0.0 & negligible \\
RF versus Dummy & 1.0 & large*** & 1.0 & large*** \\
\hline
\multicolumn{5}{l}{\scriptsize\textit{* p $\leq$ 0.05;
** p $\leq$ 0.01; 
*** p $\leq$ 0.001}}
\\
 \end{tabular}
 \end{center}
\end{table}

\end{comment}
%We noticed that Random Forest (RF) and Logistic Regression (LR) are the best models when compared to Decision Tree (DT), MLPC, and MlKNN algorithms. RF and LR outperform the other three algorithms with large effect sizes considering F-measure and precision. 

Random Forest (RF) and Neural Network Multilayer Perceptron (MLPC) were the best models when compared to Decision Tree (DT), Logistic Regression (LR), MlKNN, and Dummy algorithms. %Random Forest outperformed these four algorithms with large effect sizes considering F-measure and precision.

%\MyBox{\textbf{\emph{RQ1 Summary.}} We did not find evidence that adding the body or comments to the corpus or increasing the number of adjacent tokens from bi-grams to four-grams improves the prediction of API labels. Random Forest and Logistic Regression outperformed other algorithms in our corpus.}

%\MyBox{\textbf{\emph{RQ1 Summary.}} It is possible to predict the API-domain labels with precision of 0.755, recall of 0.747, F-measure of 0.751, and 0.142 of Hamming loss using the Random Forest algorithm, \textsc{title}, \textsc{body} and \textsc{comments} as the corpus, and unigrams.} % with 899 words.}

%Here is an overview of the types of issues that participants selected (your: did labels influence...) %Maybe make RQ2 to be: How do end-users selecting tasks:
%%%Then you can organize the results into these topics/subsubsections:

%topic-1:where do end users look for information: This will include the current discussion of the "most relevant regions" & and the "difference between control & experimental group."
%topic: What information do end-users seek when looking for tasks: This will include your current discussions: "why participants consider the information relevant" & API labels playing a role.

%\subsection{RQ2. To what extent were the API labels relevant to support the contributors' decision when selecting an issue?}

\subsubsection{RQ2. How relevant are the API-domain labels to new contributors?}

We conducted a user study with 74 participants to answer this research question and analyzed their responses. 

\textbf{What information is used when selecting a task?}
Understanding the type of information that participants use to decide while selecting an issue to work on can help projects better organize such details on their issue pages. Fig.~\ref{fig:hotmapchoicesTC} shows the different regions participants found useful. %In the Control group, the top two regions of interest included the body of the issue (75.7\%) and the title (78.7\%), followed by the labels (54.5\%) and then the code itself (54.5\%). This suggests that the labels generated by the project were only marginally useful and participants had also to review the code. In contrast, in the Treatment group, the top four regions of interest by priority were: Title, Label, Body, and then Code (97.5\%, 82.9\%, 70.7\%, 56.1\%, respectively). This shows that participants in the Treatment group found the labels more useful than those in the Control group: 82.9\% usage in the Treatment group compared to 54.5\% in the Control group. Comparing the body and the label regions in both groups, we found that participants from the Treatment group selected 1.6x more labels than the Control group (p=0.05). The odds ratio analysis suggests that tags were more relevant in the Treatment groups.

%\textsc{Title} was selected by 66 participants (89\%), while issue \textsc{issue Body} was in the second position 54 (73\%) and \textsc{Labels} in the third position of preference 52 (70.3\%). However, we noticed the \textsc{Label} selections overcame the \textsc{Body}, being cited 34 times (79.1\%) in the treatment group compared with 18 in the control group (54.5\%) (see Fig. \ref{fig:hotmapchoicesTC}). %This result might suggest the API labels generated by our classifier increased the perception of relevancy in terms of the labels as a region to look for information about the issues.

%\todo{R1 Q9} \draft{small changes in (labels) figure 6}
\begin{figure}[!hbt]
\centering
\includegraphics[width=.4 \textwidth] {Figures/regionsCountsNormalizedNew.png}
\caption{The regions counts (normalized) of the issue's information page selected as most relevant by participants from Treatment and Control groups. 1-Title,2-Label,3-Body,4-Code,5-Comments,6-Author,7-Linked issues,8-Participants.}
\label{fig:hotmapchoicesTC}
\end{figure}

%Qualitative analysis of the reason behind the choice of participants in the Treatment group reveals that the Title and the Labels together provided a comprehensive view of the issue. For instance, P4IT mentioned: \textit{"labels were useful to know the problem area and after reading the title of the issues, it was the first thing taken into consideration, even before opening to check the details"}. Participants found the labels useful in pointing out the specific topic of the issue, as P14IT stated: \textit{``[labels are] hints about what areas have a connection with the problem occurring''}. 


%(P13IT)\textit{"quick identification of what issue is related to"}
%P5BT\textit{"[Labels ] give a good idea of what the issue is about"} 

%To enlarge our understanding of what is important from an end user perspective, we asked the participants to answer the open question Q1 ("Why was the information you selected relevant?"). From the 74 participants who finished the survey, 47 answered. While the \textsc{Labels} was attractive for providing a summary of the task, the \textsc{Title} provided a more overall sense of the problem. For instance, a participant (P5BT) mentioned that \textit{"[Labels ] give a good idea of what the issue is about"}. In contrast, participant "P4BT" suggested that \textit{"labels were useful to know the problem area, and after reading the title of the issues, it was the first thing taken into consideration, even before opening to check the details"}. Additionally, labels were considered relevant because they provide \textit{"quick identification of what issue is related to"} (P13IT), and \textit{"[labels are] hints about what areas have a connection with the problem occurring"} (P14BT). 


%\textbf{What is the role of labels?}
%We also investigated which type of labels helped the participants in their decision-making. We divided the labels available to our participants into three groups based on the kind of information they imparted. 

\begin{comment}
\begin{itemize}
\item Issue type (already existing in the project): This included information about the type of the task: Bug, Enhancement, Feature, Good First Issue, and GSoC.
\item Code component (already existing in the project): This included information about the specific Code components of JabRef: Entry, Groups, External.Files, Main Table, Fetcher, Entry.Editor, Preferences, Import, Keywords
\item API-domain (new labels): the labels generated by our classifier (IO, UI, Network, Security, etc.). These labels were available only to the Treatment group. 
\end{itemize} 

\input{Tables/TypeOfLabelsTCCounts}
\end{comment}

%Table \ref{tab:distributionLabels} compares the labels that participants considered relevant across the Treatment and Control groups, distributed across these label types. In the Control group, most selected labels (56.4\%) relate to the type of issue (e.g., Bug or Enhancement). In the Treatment group, however, this number drops to 36.8\%, with API-domain labels being the majority (42.7\%), followed by Code component labels (20.6\%). This difference in distributions alludes to the usefulness of the API-domain labels. 

%To better understand the API-domain labels' usefulness compared to the other types of labels, we further investigated the label choices among the Treatment group participants. Figure \ref{fig:newlabels_countsAC} presents two violin plots comparing (a) API-domain labels against Code component labels and (b) API-domain labels against the type of issue. %Wider sections of the violin plot represent a higher probability of observations taking a given value; the thinner sections correspond to a lower likelihood. 
%The plots show that API-domain labels are more frequently chosen (median is 5 labels) as compared to Code component labels (median is 2 labels). %, with a large effect size ($|$d$|$ = 0.52). However, the distribution of the Issue Type and API-domain labels are similar as confirmed by negligible effect size ($|$d$|$ = 0.1). %These results indicate that while the type of issue (bug fix, enhancement, suitable for a newcomer) is important, understanding the technical (API) requirements of solving the task is equally important in developers making their decision about which task to select. 


%\todo{R1 Q12} \draft{small changes (caption) in figure 7}

\begin{comment}
\begin{figure}[!htb]
\centering
\includegraphics[width=.4\textwidth] {Figures/violinBW2x1.pdf}
\caption{Density Probability Labels (Y-Axis): API-domain x Components x Types.}
\label{fig:newlabels_countsAC}
\end{figure}
\end{comment}
%\textcolor{blue}{**I dont understand the results...exp people like APIs more?}\\
%\textbf{are the API labels preferences different in demographic subgroups?}
%We used the Cliffs Delta verify the pair-wise statistical results comparing the Experimental groups. Comparing API Labels with components labels, we found (-0.52, 'large') and comparing API labels with issue type we found (0.0, 'negligible'). Thus, we confirm the statistical difference of API labels and components between the Experimental groups and confirm the similar distribution of API labels and Type labels.

Finally, we analyzed whether the demographic subgroups had different perceptions about the API-domain labels (Table~\ref{tab:apiXcompXtype}). When comparing Industry vs. Students, we found participants from industry selected 1.9x (p-value=0.001) more API-domain labels than students when we controlled by component labels. %We saw the same odds when controlling by issue type (p-value=0.0007). When comparing Experienced vs. Novice coders, we did not find statistical significance (p=0.11) when controlling by component labels. However, we discovered that experienced coders selected 1.7x more API-domain labels than novice coders (p-value=0.01) when we controlled by type labels.

The odds ratio analysis suggests that API-domain labels are more likely to be perceived relevant by practitioners and experienced developers than by students and novice coders.

\input{Tables/APixCompxType}

%We noticed that practitioners selected more API labels than students (56\% x 40\%), and selected fewer components labels than students (44\% x 60\%). When we compared the experienced coders and novices, we found experience coders chose API and components almost in the same %

%We found that role and labels are associated with a small effect size (p-value=0.0019, Cramer's V=0.15). 

%and proportion of practitioners group and the students comparing the API labels to component labels are significant, and $chi^2$=9.6, p=0.0019, with a small effect size (Cramer's V varying from 0.02 to 1.49). Comparing API labels to issue types, we also observed a difference in proportions with $chi^2$=11.2, p=0.0007, with a large effect size (Cramer's V =1.49). However, comparing the experienced coders and novice, while there is a difference in proportions from the API labels to the issue types ($chi^2$=6.12, p=0.01) with a small effect size (Cramer's V =0.11), there is no difference in proportions comparing the API labels and the component labels ($chi^2$=2.43, p=0.11).

%The same occurred when we compared the experienced and novice coders: more API labels preference from experienced coders (50.9\% x 41.5\%). And fewer component labels from experienced coders (49.1\% x 58.5\%).

%API labels were \textbf{more} selected than component with a slightly difference between the industry (56\% x 44\%) and experienced (50.9\% x 49.1\%). The student (40\% x 60\%) demographic subgroup and the novice (41.5\% x 58.5\%) coders subgroup had similar selections for the component labels.

%\noindent\textbf{Comparison between API Labels and issue type}
%Practitioners also selected more API labels (45.5\% x 30.6\%) than students and fewer issue type labels (55.5\% x 69.4\%). And experienced coders preferred more API labels than novice (43.5\% x 30.9\%) and fewer issue type than novice coders (56.5\% x 69.1\%).

%In contrast with the previous comparison, API labels were \textbf{less} selected than issue type with a similar difference between industry (45.5\% x 55.5\%) and experienced (43.5\% x 56.5\%). The student (30.6\% x 69.4\%) demographic and the novice (30.9\% x 69.1\%) coders subgroup also had similar selections for the issue type.


%\input{tables/APIxCompxTypeTreatment}

%Looking to the Experienced coders, they selected 50.9\% of API labels and 49.1\% of component labels. On the other hand, Novice coders decided 41.5\% of API labels and 58.5\% of component labels. But, for the comparison with the issue type, experienced coders selected 43.5\% of API labels and 56.5\% of issue type, while novices selected 30.9\% of API labels and 69.1\% of the issue type. We noticed the contrast looking at the labels selections:


%\textcolor{blue}{----POSSIBLE TABLE for this section---}

%Participant     API     Component    Type\\
%Practitioners    56\%   \\
%--Experienced    ???\\
%--Novice      ??\\
%Students      40\%\\
%--Experienced    ??\\
%--Novice      ??\\

%\begin{itemize}
%\item{API labels between practitioners and students (API Labels 56\% - 40\%) } 

%\item{API labels between experienced and novice coders (API Labels 50.9\% - 41.5\%) %} 

%\end{itemize}

%Table \ref{tab:resultsRQ2Dif} shows that p-values for all demographics are below the significance threshold ($<$0.05), except for the experienced/novice coders, when looking at the API and component labels. 

%The null hypothesis states there is no significant difference between the expected and observed results, which means the two groups have the same distribution. 


\input{Tables/APILabels}

%\MyBox{\textbf{\emph{RQ2 Summary.}} Our findings suggest that titles and labels have essential information for selecting an issue to work on. API-domain labels also increased the perception of the label region's relevancy. API-domain labels are especially relevant for industry and experienced coders.}

%\MyBox{\textbf{\emph{RQ2 Summary.}} Our findings suggest that labels are relevant for selecting an issue to work on. API-domain labels increased the perception of the labels' relevancy. API-domain labels are especially relevant for industry and experienced coders.}

\subsection{Stage 2 - Strategy Importance} 
\label{sec:strategy-importance}

\subsubsection{RQ3: What strategies help newcomers choose a task in OSS?}
\label{sec:results:rq1}

To answer this research question, we interviewed maintainers to understand their perspectives on (i) what strategies a newcomer uses to choose an open issue; and (ii) what strategies the OSS communities can use to help newcomers choose tasks. 

%In the subsections below, we discuss (i) the newcomer strategies and (ii) the maintainers' strategies.

%\subsubsection{Newcomer strategies to choose a task}
%\label{sec:results_newcomers_strategies}

From the interviews, we could identify 27 strategies that maintainers expect newcomers to use to choose a task and grouped them into five categories. %, as presented in Fig.~\ref{fig:activities}. In the following, we present more details about our findings, organized by strategy category. 
In addition to the strategies newcomers are expected to take, we found 40 strategies that the communities use to help newcomers choose their tasks. %From these strategies, we derived seven strategy categories, as presented in Fig.~\ref{fig:strategies}. 

\subsubsection{RQ4: How do newcomers and existing contributors differ in their opinions of which strategies are important for newcomers?}
\label{sec:results:rq2}

To compare the relative importance of the strategies from the point of view of different stakeholders, we used the Schulze method~\cite{schulze2003new} to combine the rankings for (i) newcomers and (ii) community strategies. In the following subsections, we present the rankings and how they compare.

Fig.~\ref{fig:strategiesflow} presents the combined rankings for maintainers' strategies according to each stakeholder. Once again, the perspective of frequent contributors and maintainers are similar, with one standout difference: ``Support the onboarding of newcomers''.

\begin{figure}[htb]
\centering
\includegraphics[width=0.9\linewidth]{Figures/ranking_strategies_schulze_alt.pdf}
\caption{The relative importance of community strategies}
\label{fig:strategiesflow}
\end{figure}
%\begin{acks}
%This work is partially supported by the National Science Foundation under Grant numbers 1815486, 1815503, 1900903, and 1901031, CNPq grant \#313067/2020-1. We also thank the developers who spent their time participating on our experiment and interviews.
%\end{acks}

Among the mismatches we found, ``Support the onboarding of newcomers,'' while it was ranked last for the maintainers, it was the third for newcomers and second for frequent contributors. %This is surprising since our intuition was that maintainers should prioritize the onboarding process to count on human resources to work on the issues.

%%
%% The next two lines define the bibliography style to be used, and
%% the bibliography file.
\bibliographystyle{ACM-Reference-Format}
\bibliography{paper}

\end{document}
\endinput