\documentclass[11pt]{article}

%\usepackage{fleqn}
%\usepackage{epsf}
%\usepackage{mathrsfs}
\usepackage[shortlabels]{enumitem}
\usepackage{amssymb}
\usepackage[mathscr]{euscript}
\usepackage{color}
\usepackage{amsfonts}
\usepackage{graphicx}
\usepackage{latexsym}
\usepackage{amsmath}
\usepackage{changepage}
\usepackage{color}
\usepackage{latexsym}
\usepackage{tablefootnote}
%\usepackage[notcite,notref]{showkeys}
\usepackage{epsfig}
\usepackage{multirow}
\usepackage{float}
\usepackage{array}
\usepackage{bbm}

\baselineskip=40pt \textheight 22.5truecm \topmargin -0.5125truein
\textwidth 15.74truecm \oddsidemargin -0.06truein \evensidemargin
-0.06truein

\def\bea{\begin{eqnarray}}
\def\beaa{\begin{eqnarray*}}
\def\beq{\begin{equation}}
\def\eea{\end{eqnarray}}
\def\eeaa{\end{eqnarray*}}
\def\eeq{\end{equation}}
\newcommand{\nn}{\nonumber}
\newcommand{\mb}[1]{\mbox{\boldmath $#1$}}

\allowdisplaybreaks

\def\proof{\noindent{\bf Proof}: \ignorespaces}
\def\endproof{{\ \hfill\hbox{%
      \vrule width1.0ex height1.0ex
    }\parfillskip 0pt}\par}

%\linespread{2.8}

\begin{document}

\newtheorem{definition}{Definition}[section]
\newtheorem{proposition}{Proposition}[section]
\newtheorem{corollary}{Corollary}[section]
\newtheorem{lemma}{Lemma}[section]
\newtheorem{theorem}{Theorem}[section]
\newtheorem{remark}{Remark}[section]
\newtheorem{assumption}{Assumption}[section]
\newtheorem{example}{Example}[section]
\newtheorem{algorithm}{Algorithm}[section]
\newtheorem{procedure}{Procedure}[section]

\title{Superlinear Convergence of an Interior Point Algorithm on Linear Semi-definite Feasibility Problems with Application to Linear Matrix Inequalities}

\author{{\bf{Chee-Khian Sim}}\footnote{Email address: chee-khian.sim@port.ac.uk} \\ School of Mathematics and Physics\\ University of Portsmouth \\ Lion Gate Building, Lion Terrace \\ Portsmouth, PO1 3HF \\ United Kingdom}

\date{08 January 2024}

\maketitle
\begin{abstract}
{\textcolor{black}{In the literature, besides the assumption of strict complementarity, superlinear convergence of implementable polynomial-time interior point algorithms using known search directions, namely, the HKM direction, its dual or the NT direction, to solve semi-definite programs (SDPs) is shown by (i) assuming that the given SDP is nondegenerate and making modifications to these algorithms \cite{Kojima}, or (ii) considering special classes of SDPs, such as the class of linear semi-definite feasibility problems (LSDFPs) and requiring the initial iterate to the algorithm to satisfy certain conditions \cite{Sim1,Sim6}. Otherwise, these algorithms are not easy to implement even though they are shown to have polynomial iteration complexities and superlinear convergence \cite{Luo}.  The conditions in \cite{Sim1,Sim6} that the initial iterate to the algorithm is required to satisfy to have superlinear convergence when solving LSDFPs however are not practical.  In this paper, we propose a practical initial iterate to an implementable infeasible interior point algorithm that guarantees superlinear convergence when the algorithm is used to solve the homogeneous feasibility model of an LSDFP.}}  

% In this paper, we propose a practical initial iterate that is shown to guarantee superlinear convergence of the algorithm on the homogeneous feasibility model of an LSDFP, hence solving the LSDFP.  We also discuss how we can get this initial iterate.

\vspace{10pt}

\noindent {\bf{Keywords.}} Linear semi-definite feasibility problem; strict feasibility; homogeneous feasibility model; interior point method; superlinear convergence.
\end{abstract}

%\begin{center}
%\LARGE{\bf{Superlinear Convergence of a Homogeneous Interior Point
%Algorithm on Linear Semi-definite Feasibility Problems}}
%\end{center}

\section{Introduction}\label{sec:introduction}

Many problems in diverse areas, such as optimal control, estimation and signal processing, communications and networks, statistics, and finance, can be modelled well as semi-definite programs (SDPs) \cite{Boyd}.  Finding effective and efficient ways to solve this class of problems is hence practically important.  Interior point methods (IPMs) have been proven to be successful in solving SDPs - see for example \cite{Alizadeh,Kojima1,Monteiro3,Zhang}.  {\textcolor{black}{Research on interior point methods is active with recent papers, such as \cite{Alzalg,Faybusovich,Pougkakiotis,Rigo}\footnote{\textcolor{black}{Note that papers \cite{Alzalg,Faybusovich,Rigo} consider symmetric cone problems, which generalize SDPs.}}, proposing contemporary interior point algorithms to solve SDPs.}}  An important subclass of the class of SDPs is the class of linear semi-definite feasibility problems (LSDFPs), which also have applicability in diverse areas.  This class of problems includes linear matrix inequalities (LMIs).

Among different IPMs, primal-dual path following interior point algorithms are the most successful and most widely studied.  In this paper, we focus on an infeasible predictor-corrector primal-dual path following interior point algorithm to solve the homogeneous feasibility model \cite{Potra} of an LSDFP.  Polynomial iteration complexity of the algorithm to solve a primal-dual SDP pair has been shown in \cite{Potra}.  In this paper, we consider the local convergence behavior of the algorithm.  

It proves not an easy task to show superlinear convergence of an implementable interior point algorithm that has polynomial iteration complexity on an SDP with minimal assumptions on the problem and no modifications to the algorithm.  This is especially so for the HKM (and its dual) or the NT search direction used in these algorithms, although superlinear convergence is quickly established for the AHO search direction \cite{Kojima2, Lu3}, when researchers started focusing their attention on interior point algorithms to solve SDPs. 
% Assuming strict complementarity, a plausible reason for the relative ease to show superlinear convergence for the latter is that the matrix involved in the second order method is nonsingular at a strictly complementary optimal solution using the AHO search direction, while the matrix is singular (and not even defined) for the HKM (and its dual) and the NT search direction.  
 In the literature when the HKM (and its dual) or the NT search direction is used in an interior point algorithm, such as \cite{Kojima}, in addition to strict complementarity assumption, nondegeneracy assumption at an optimal solution and modifications to the algorithm, such as solving the corrector-step linear system in an iteration repeatedly instead of only once (``narrowing" the central path neighborhood\footnote{We note that this idea is used in \cite{Nesterov2} to show superlinear convergence of an interior point algorithm on a wide class of conic optimization problems.  Also, this is related to the sufficient conditions on superlinear convergence behavior of iterates generated by the algorithm  as studied in \cite{Potra2,Potra1}.}), need to be imposed for superlinear convergence of the interior point algorithm.  In \cite{Luo}, without assuming  nondegeneracy, the feasible interior point algorithm considered in the paper is shown to have polynomial iteration complexity and superlinear convergence.  However, the algorithm is not easy to implement. The idea behind the algorithm considered in \cite{Luo} to have superlinear convergence when solving an SDP is to force the centrality measure of the $k^{th}$ iterate to converge to zero as $k$ tends to infinity.  This can be enforced in practice by for example, repeatedly solving the corrector-step linear system in an iteration, as in \cite{Kojima}, so as to ``narrow" the neighborhood of the central path in which iterates lie.    Hence, either a polynomial-time interior point algorithm has to be modified or the algorithm is hard to implement for guaranteed superlinear convergence of these algorithms.  Otherwise, special structure needs to be imposed on the SDP, such as considering linear semi-definite feasibility problems (LSDFPs), for superlinear convergence \cite{Sim1,Sim6}\footnote{In \cite{Sim1,Sim6}, the search direction used in the interior point algorithm considered is the (dual) HKM and the NT direction respectively.}.  For the latter, we also require additionally that a suitable initial iterate is chosen.  However, the required initial iterate to guarantee superlinear convergence given in \cite{Sim1,Sim6} is not practical.  {\textcolor{black}{In this paper, we improve on the results in \cite{Sim1,Sim6} by proposing an initial iterate that can be obtained in practice (by solving a primal-dual SDP pair), and using this initial iterate to an implementable interior point algorithm on the homogeneous feasibility model of the LSDFP, we have superlinear convergence of the algorithm.  The novelty in showing this superlinear convergence result is the nontrivial application of what is known in the literature \cite{Sim1,Sim6} to show the result.  Two key steps that are needed before we can use relevant results in the literature \cite{Sim1,Sim6} are   (i) applying the interior point algorithm on the homogeneous feasibility model of the LSDFP, instead of the LSDFP itself; (ii) suitably reformulating the homogeneous feasibility model of the LSDFP as a semi-definite linear complementary problem (SDLCP) before we can apply these results.}}
 
 
% In this paper, we show the surprising result of superlinear convergence of an implementable polynomial-time interior point algorithm, using the dual HKM search direction, on a primal-dual SDP pair with only strict complementarity and no other special structure imposed on the problem.  Furthermore, no modifications to the algorithm, such as repeatedly solving the corrector-step linear system, instead of once, in an iteration, is needed to achieve this, although a suitable initial iterate is required.  This result is surprising since the matrix involved in the algorithm is singular (and not even defined) at a strictly complementary optimal solution, and as is known for Newton method, for superlinear convergence, we require the Hessian matrix at the optimal solution to be non-singular.  To show superlinear convergence, we consider the algorithm applied to the homogeneous feasibility model of the primal-dual SDP pair, and then suitably applying results known in the literature.  
%In this case, given an interior feasible point to the primal SDP pair as part of the initial iterate of the algorithm on the homogeneous feasibility model, we are able to show superlinear convergence of the algorithm to find optimal solutions to the primal-dual SDP pair.  To the best of our knowledge, this result is the first in the literature on superlinear convergence of the algorithm under consideration, without any modifications to the algorithm, and under only strict complementarity assumption on the primal-dual SDP pair.  

In Section \ref{sec:LSDFPhomogeneousmodel}, we describe the homogeneous feasibility model of an LSDFP, and in Section \ref{sec:SDLCP}, we express the homogeneous feasibility model as a semi-definite linear complementarity problem (SDLCP). {\textcolor{black}{The latter}} allows us to apply results in the literature in Section \ref{sec:interiorpointalgorithm} to show superlinear convergence of an implementable interior point algorithm on the homogeneous feasibility model. We conclude the paper with Section \ref{sec:conclusion}.


% Finally, in Section \ref{sec:numerical}, we perform numerical experiments on random primal-dual SDP pairs to practically view the superlinear convergence behavior of the interior point algorithm on their homogeneous feasibility models.

\subsection{Notations}\label{subsec:notations}

The space of symmetric $n \times n$ matrices is denoted by $S^n$.  The cone of positive semi-definite (resp., positive definite) symmetric matrices is denoted by $S^n_+$ (resp. $S^n_{++}$).  The identity matrix is denoted by $I_{n \times n}$, where $n$ stands for the size of the matrix.  We omit the subscript when the size of the identity matrix is clear from the context.  The matrix $E^{ij} \in \Re^{n_1 \times n_2}$ is defined to have $1/2$ in its $(i,j)$ and $(j,i)$ entry, and zero everywhere else.

Given a matrix $G \in \Re^{n_1 \times n_2}$, $\| G \|_F: = \sqrt{{\rm{Tr}}(GG^T)}$ is the Frobenius norm of $G$, where ${\rm{Tr}}(\cdot)$ is the trace of a square matrix.  $G_{ij}$ is the entry of $G$ in the $i$th row and the $j$th column of $G$.  On the other hand, given a vector $x \in \Re^n$, $\| x \|$ refers to its Euclidean norm, and $x_i$ is its $i^{th}$ entry.

%Furthermore, given a symmetric matrix $S \in \Re^{n \times n}$, $\lambda_{\min}(S)$ and $\lambda_{\max}(S)$ stands for the maximum and minimum eigenvalue of $S$, respectively.  Since $S$ is symmetric, these eigenvalues are real. 

Given $X \in S^n$, ${\rm{svec}}(X)$ is defined to be
\begin{eqnarray*}
{\rm{svec}}(X) := (X_{11}, \sqrt{2}X_{21}, \ldots, \sqrt{2}X_{n1}, X_{22}, \sqrt{2}X_{32}, \ldots, X_{n-1,n-1}, \sqrt{2} X_{n,n-1}, X_{nn})^T \in \Re^{\tilde{n}},
\end{eqnarray*}
where $\tilde{n} = n(n+1)/2$.  ${\rm{svec}}(\cdot)$ sets up a one-to-one correspondence between $S^n$ and $\Re^{\tilde{n}}$.

%Given functions $f : \Omega \rightarrow E$ and $g : \Omega \rightarrow \Re_{++}$, where $\Omega$ is an arbitrary set and $E$ is a normed vector space with norm $\| \cdot \|$.  For a subset $\hat{\Omega} \subseteq \Omega$, we write $f(w) = \mathcal{O}(g(w))$ for all $w \in \hat{\Omega}$ to mean that $\| f(w) \| \leq M g(w)$ for all $w \in \hat{\Omega}$, where $M > 0$ is a positive constant.  Suppose $E = S^n$.  Then we write $f(w) = \Theta (g(w))$ if for all $w \in \hat{\Omega}$, $f(w) \in S^n_{++}$, and $f(w) = \mathcal{O}(g(w))$, $f(w)^{-1} = \mathcal{O}(1/g(w))$.


\section{A Linear Semi-definite Feasibility Problem and its Homogeneous Feasibility Model}\label{sec:LSDFPhomogeneousmodel}

%\noindent Consider the following linear semi-definite feasibility
%problem:

%\vspace{10pt}

%\noindent {\it{Given $m$ symmetric matrices $A_i \in S^n$ and $m$
%real numbers $b_i, i = 1, \ldots, m$, find an $X \in S^n_+$ that
%satisfies
%\begin{eqnarray*}
%{\mbox{Tr}}(A_i X) = b_i,\ i = 1, \ldots, m,
%\end{eqnarray*}
%\noindent where ${\mbox{Tr}}(\cdot)$ denotes the trace of a
%matrix.}}

%\vspace{10pt}

%Let us assume that $\{ A_1, \ldots, A_m\}$ are linearly independent.

\noindent Given $C, A_i \in S^n$, $i = 1, \ldots, m$, and $b = (b_1, \ldots, b_m)^T \in \Re^m$.  A (primal) semi-definite program (SDP) is given by
\begin{eqnarray}\label{primalSDP}
\begin{array}{ll}
\min & {\mbox{Tr}}(CX) \\
{\mbox{subject\ to}} & {\mbox{Tr}}(A_i X) = b_i,\ i = 1, \ldots, m,
\\
& X \in S^n_+.
\end{array}
\end{eqnarray}

%A linear semi-definite feasibility problem is a semi-definite
%program in which $C = 0$.

The dual of (\ref{primalSDP}) is given by
\begin{eqnarray}\label{dualSDP}
\begin{array}{ll}
\max & b^Ty \\
{\mbox{subject\ to}} & \sum_{i=1}^{m} y_iA_i + Y = C, \\
& Y \in S^n_+.
\end{array}
\end{eqnarray}
Here, $y = (y_1, \ldots, y_m)^T \in \Re^m$.  {\textcolor{black}{We consider the case when (\ref{primalSDP}) and (\ref{dualSDP}) are feasible in this paper.}}

%We impose the following assumptions on primal-dual SDP pair (\ref{primalSDP})-(\ref{dualSDP}) throughout this paper:
%\begin{assumption}\label{ass:SDP}
%\begin{enumerate}[(a)]
%\item There exist $X \in S^n_{++}$ and  $(y, Y) \in \Re^m \times S^n_{++}$ that is feasible to (\ref{primalSDP}) and (\ref{dualSDP}) respectively.
%\item $A_1, \ldots, A_m$ are linearly independent.
%\end{enumerate}
%\end{assumption}  
%
%The above assumptions, in particular, Assumption \ref{ass:SDP}(a), ensure that there exists an optimal solution $X^\ast$ to (\ref{primalSDP}) and an optimal solution $(y^\ast, Y^\ast)$ to
%(\ref{dualSDP}) such that ${\rm{Tr}}(X^\ast Y^\ast) = 0$ - see for example, \cite{Boyd}.

A linear semi-definite feasibility problem (LSDFP) is a primal-dual SDP pair (\ref{primalSDP})-(\ref{dualSDP}) with $C = 0$ or $b = 0$.  WLOG, from now onwards, we consider the LSDFP with $C = 0$.   Also, we assume that $b_i \not= 0$, for some $i = 1, \ldots, m$, to avoid trivial considerations. 


\begin{remark}\label{rem:LMI}
{\textcolor{black}{A (nonstrict) linear matrix inequality (LMI) has the form
\begin{eqnarray}\label{LMI}
\sum_{j = 1}^{l} z_j B_j + B_0 \in S^n_{+},
\end{eqnarray}
where $z_j \in \Re, j = 1, \ldots l$, are the decision variables, and $B_j \in S^n, j = 0, \ldots, l$, are given.  Solving LMIs has wide applicability in diverse areas, as discussed for example in \cite{Boyd2}.   It is easy to see that (\ref{LMI}) can be written as the primal SDP (\ref{primalSDP}) with $C = 0$, for suitable $A_i, b_i, i = 1, \ldots, m$, where we wish to find $X \in S^n_{+}$ that solves the primal SDP (\ref{primalSDP}) with $C = 0$.  Hence, by solving the primal SDP (\ref{primalSDP}) with $C = 0$, we are able to solve (\ref{LMI})\footnote{It is known that implementable interior point algorithms with polynomial iteration complexities can be used to solve LMIs \cite{Boyd2,Boyd}, but to the best of our knowledge, it is unknown whether these algorithms can solve (nonstrict) LMIs with proven good local convergence behavior, which we show in this paper.} .}}
\end{remark}

 We impose the following assumptions on the LSDFP throughout this paper:
\begin{assumption}\label{ass:SDP}
\begin{enumerate}[(a)]
\item There exists  $(y, Y) \in \Re^m \times S^n_{++}$ that is feasible to the dual SDP (\ref{dualSDP}) with $C = 0$.
\item $A_1, \ldots, A_m$ are linearly independent.
\end{enumerate}
\end{assumption}  
Assumption \ref{ass:SDP}(b) is without loss of generality.  {\textcolor{black}{On the other hand, Assumption \ref{ass:SDP}(a) ensures that the duality gap between the primal SDP (\ref{primalSDP}) and the dual SDP (\ref{dualSDP}) is zero - see for example \cite{Boyd}.}} 
  %Validity of Assumption \ref{ass:SDP}(a) is discussed in Appendix \ref{sec:Appendix}.

\begin{remark}\label{rem:findinginitialiterate}
{\textcolor{black}{We can find $(y,Y) \in \Re^m \times S^n_{++}$ in Assumption \ref{ass:SDP}(a) by using a primal-dual path following interior point algorithm to solve the following primal-dual SDP pair\footnote{{\textcolor{black}{In fact, by applying the primal-dual path following interior point algorithm on the primal-dual SDP pair, we will know whether Assumption \ref{ass:SDP}(a) holds by checking the solution the algorithm converges to.}}}:
\begin{eqnarray*}\label{primal-dualSDP}
\begin{array}{ll}
\min & 0 \\
{\mbox{subject\ to}} & {\mbox{Tr}}(A_i X) = 0,\ i = 1, \ldots, m,
\\
& X \in S^n_+, \\
& \\
\max & 0 \\
{\mbox{subject\ to}} & \sum_{i=1}^{m} y_iA_i + Y = 0, \\
& Y \in S^n_+.
\end{array}
\end{eqnarray*} }}
{\textcolor{black}{From \cite{Sim6} (see also \cite{Potra1,Sim1}), we have polynomial iteration complexity and superlinear convergence of an infeasible primal-dual path following interior point algorithm on the above primal-dual SDP pair to obtain its solution.  Hence, we are able to get a strictly feasible solution to the dual SDP (\ref{dualSDP}) with $C = 0$ efficiently and fast.}} 
\end{remark}
{\textcolor{black}{In this paper, we apply an infeasible interior point algorithm using the (dual) HKM search direction or the NT search direction to solve the homogeneous feasibility model of the given LSDFP.   Since the algorithm is an interior point algorithm, the initial iterate $(X_0,y_0,Y_0)$ to the algorithm needs to satisfy $X_0, Y_0 \in S^n_{++}$.  To prove superlinear convergence of the algorithm on the homogeneous feasibility model of the LSDFP, we further require the initial iterate $(X_0,y_0,Y_0)$ to have $(y_0, Y_0)$ feasible to the dual SDP (\ref{dualSDP}) with $C = 0$.  Although this is an additional requirement on the initial iterate, it can be satisfied in practice, as discussed in Remark \ref{rem:findinginitialiterate}.  This superlinear convergence result improves on what is known in the literature \cite{Sim1,Sim6}, where the initial iterate, $(X_0,y_0,Y_0)$, is required to satisfy impractical conditions for superlinear convergence. }}

%This strictly feasible solution forms the initial iterate to the interior point algorithm on the homogeneous feasibility model of the LSDFP leading to superlinear convergence of the algorithm (Theorem \ref{thm:superlinearconvergence}).

%We discuss our requirement on the initial iterate further, in Subsection \ref{subsec:strictlyfeasibledualSDP} below, by investigating when there exists a strictly feasible solution to the dual SDP (\ref{dualSDP}) with $C = 0$, and providing an efficient and fast way to find such a solution, if one exists.

{\textcolor{black}{We now introduce the homogeneous feasibility model of the primal-dual SDP pair (\ref{primalSDP})-(\ref{dualSDP}), which first appeared in \cite{Potra}.  We consider the general setting when $C$ is not required to be zero when describing the model.}}  The model is given by the
following homogeneous system:
\begin{eqnarray}
 & {\mbox{Tr}}(A_i X) =  b_i \tau,\ i = 1, \ldots, m,
\label{homogeneous1} \\
 & \sum_{i=1}^{m} y_i A_i + Y = \tau C,  \label{homogeneous2} \\
 & \kappa = b^T y - {\mbox{Tr}}(CX),  \label{homogeneous3} \\
 & X \in S^n_{+}, y \in \Re^m, Y \in S^n_{+}, \tau \geq 0, \kappa \geq 0.
\label{homogeneous4}
\end{eqnarray}
The model has been incorporated in the software package SeDuMi, according to \cite{de_Klerk}, {\textcolor{black}{to solve SDPs}}, and also in the commercial software package MOSEK, according to \cite{Dahl} (see also \cite{Anderson}), to solve conic optimization problems, which include SDPs.

Observe that (\ref{homogeneous1})-(\ref{homogeneous3}) implies that
\begin{eqnarray}\label{dualitygap}
{\mbox{Tr}}(XY) + \tau \kappa = 0,
\end{eqnarray}
\noindent from which we obtain, using (\ref{homogeneous4}),
\begin{eqnarray*}
XY & = & 0, \\
\tau \kappa & = & 0.
\end{eqnarray*}

Furthermore, observe that a solution to the homogeneous system is readily available and is given by
$(X,y,Y,\tau,\kappa) = (0,0,0,0,0)$.  However, we cannot derive optimal solutions to (\ref{primalSDP}) and (\ref{dualSDP}) from this.  If there exists a
solution $(X^\ast, y^\ast, Y^\ast, \tau^\ast, \kappa^\ast)$ to
(\ref{homogeneous1})-(\ref{homogeneous4}) such that $\kappa^\ast =
0$ and $\tau^\ast > 0$, then $(X^\ast/\tau^\ast, y^\ast/\tau^\ast,
Y^\ast/\tau^\ast)$ is an optimal solution to the primal-dual SDP
pair (\ref{primalSDP})-(\ref{dualSDP}) with zero duality gap.  Conversely, if $(X^\ast, y^\ast, Y^\ast)$ is an optimal solution to the primal-dual SDP pair (\ref{primalSDP})-(\ref{dualSDP}) with zero duality gap, then $(X^\ast, y^\ast, Y^\ast, 1, 0)$ solves the system (\ref{homogeneous1})-(\ref{homogeneous4}).   Interior point algorithms when applied to the homogeneous feasibility model can be used to find optimal solutions to the  primal-dual SDP pair (\ref{primalSDP})-(\ref{dualSDP}) by finding a solution $(X^\ast, y^\ast, Y^\ast, \tau^\ast, \kappa^\ast)$ to
(\ref{homogeneous1})-(\ref{homogeneous4}) such that $\kappa^\ast =
0$ and $\tau^\ast > 0$.  Such an interior point algorithm is discussed in \cite{Potra}, which is also given in Section \ref{sec:interiorpointalgorithm} below where it is specialized to the case when $C = 0$.  These interior point algorithms necessarily have to be of the infeasible type in that the initial iterate and subsequent iterates generated by the algorithm cannot satisfy (\ref{homogeneous1})-(\ref{homogeneous3}).  This is so because if an iterate $(X_k, y_k, Y_k, \tau_k, \kappa_k) \in S^n_{++} \times \Re^m \times S^n_{++} \times \Re_{++} \times \Re_{++}$ for some $k \geq 0$ satisfies (\ref{homogeneous1})-(\ref{homogeneous3}), then (\ref{dualitygap}) holds, which is impossible since $X_k, Y_k \in S^n_{++}$ and $\tau_k, \kappa_k > 0$.
{\textcolor{black}{
\begin{remark}\label{rem:ourcase}
When specialized to the case when $C = 0$, (\ref{homogeneous1})-(\ref{homogeneous4}) are given by:
\begin{eqnarray}
 & {\mbox{Tr}}(A_i X) =  b_i \tau,\ i = 1, \ldots, m,
\label{homogeneous1LSDFP} \\
 & \sum_{i=1}^{m} y_i A_i + Y = 0,  \label{homogeneous2LSDFP} \\
 & \kappa = b^T y,  \label{homogeneous3LSDFP} \\
 & X \in S^n_{+}, y \in \Re^m, Y \in S^n_{+}, \tau \geq 0, \kappa \geq 0.
\label{homogeneous4LSDFP}
\end{eqnarray}
\end{remark}
}}


%In this subsection, we provide a procedure to detemine whether the dual SDP (\ref{dualSDP}) with $C = 0$, has strictly feasible solution, and if so, give such a solution.  First, we make the following observation:
%\begin{proposition}\label{prop:anobservation}
%We have
%\begin{eqnarray*}
%\min_{q \in \Re^n, \| q \| = 1} \lambda_{\max} \left( \sum_{i=1}^m (q^T A_i q) A_i \right) \geq 0.
%\end{eqnarray*}
%\end{proposition}
%\begin{proof}
%Suppose 
%\begin{eqnarray}\label{ineq:optimizationproblem}
%\min_{q \in \Re^n, \| q \| = 1} \lambda_{\max} \left( \sum_{i=1}^m (q^T A_i q) A_i \right) < 0.
%\end{eqnarray}
%Let $q^\ast \in \Re^n$ be an optimal solution to the above minimization problem.  Then it is easy to see that
%\begin{eqnarray*}
%-\sum_{i=1}^m ((q^\ast)^T A_i q^\ast) A_i \in S^n_{++},
%\end{eqnarray*}
%and hence the dual SDP (\ref{dualSDP}) with $C = 0$, has a strictly feasible solution.  
%
%\noindent Now, we have
%\begin{eqnarray*}
%\min_{q \in \Re^n, \| q \| = 1} \lambda_{\max} \left( \sum_{i=1}^m (q^T A_i q) A_i \right) & = & \min_{q \in \Re^n, \|q\| = 1} \max_{q_1 \in \Re^n \| q_1 \| = 1}  (q_1)^T \left( \sum_{i=1}^m (q^T A_i q) A_i \right) q_1 \\
%& \geq & \min_{q \in \Re^n, \| q \| = 1} \sum_{i=1}^m (q^T A_i q)^2.
%\end{eqnarray*}
%However, the latter is greater than zero, by (\ref{Anoptimizationproblem}).  This leads to a contradiction to (\ref{ineq:optimizationproblem}), and hence we prove the proposition.
%\end{proof}
%
%
%\vspace{10pt}
%
%We know by the above proposition that
%\begin{eqnarray*}
%\min_{q \in \Re^n, \| q \| = 1} \lambda_{\max} \left( \sum_{i=1}^m (q^T A_i q) A_i \right) \geq 0.
%\end{eqnarray*}
%
%If 
%\begin{eqnarray*}
%\min_{q \in \Re^n, \| q \| = 1} \lambda_{\max} \left( \sum_{i=1}^m (q^T A_i q) A_i \right) = 0,
%\end{eqnarray*}
%then by arguments similar to the proof of Proposition \ref{prop:anobservation}, we have that the dual SDP (\ref{dualSDP}) with $C = 0$, does not have a strictly feasible solution.  Otherwise, 
%\begin{eqnarray*}
%\min_{q \in \Re^n, \| q \| = 1} \lambda_{\max} \left( \sum_{i=1}^m (q^T A_i q) A_i \right) > 0.
%\end{eqnarray*}
%In this case, if
%\begin{eqnarray*}
%\max_{q \in \Re^n, \| q \| = 1} \lambda_{\min} \left( \sum_{i=1}^m (q^T A_i q) A_i \right) > 0,
%\end{eqnarray*}
%then it is easy to see that the dual SDP (\ref{dualSDP}) with $C = 0$, has a strictly feasible solution.  Otherwise, 
%\begin{eqnarray*}
%\max_{q \in \Re^n, \| q \| = 1} \lambda_{\min} \left( \sum_{i=1}^m (q^T A_i q) A_i \right) \leq 0.
%\end{eqnarray*}
%In this case, we can only solve the following minimization  problem 
%\begin{eqnarray}\label{Anotheroptimizationproblem}
%\max_{| y_i | \leq 1, i = 1, \ldots, m} \lambda_{\min} \left( \sum_{i=1}^m y_i A_i \right)
%\end{eqnarray}
%to determine whether the dual SDP (\ref{dualSDP}) with $C = 0$, has a strictly feasible solution.  If (\ref{Anotheroptimizationproblem}) is greater than zero, then the dual SDP (\ref{dualSDP}) with $C = 0$, has a strictly feasible solution; otherwise, the dual SDP (\ref{dualSDP}) with $C = 0$, does not have strictly feasible solutions.
%
%The discussion above leads to the following systematic procedure to determine whether the dual SDP (\ref{dualSDP}) with $C = 0$, has a strictly feasible solution, and if it has one, provide such a solution.
%\begin{procedure}\label{pro:strictlyfeasiblesoln} 
%\begin{description}
%\item[\hspace{5pt} ($p1$)]   Solve
%\begin{eqnarray}\label{procedureoptimizationproblem1}
%\min_{q \in \Re^n, \| q \| = 1} \lambda_{\max} \left( \sum_{i=1}^m (q^T A_i q) A_i \right) = \min_{q \in \Re^n, \| q \| = 1} \max_{q_1 \in \Re^n, \| q_1 \| = 1} \sum_{i=1}^m (q^T A_i q)(q_1^T A_i q_1).
%\end{eqnarray}
%\item[\hspace{5pt} ($p2$)] If (\ref{procedureoptimizationproblem1}) is equal to zero, then stop with no strictly feasible solution to the dual SDP (\ref{dualSDP}) with $C = 0$; else continue. 
%\item[\hspace{5pt} ($p3$)] Solve
%\begin{eqnarray}\label{procedureoptimizationproblem2}
%\max_{q \in \Re^n, \| q \| = 1} \lambda_{\min} \left( \sum_{i=1}^m (q^T A_i q) A_i \right) = \max_{q \in \Re^n, \| q \| = 1} \min_{q_1 \in \Re^n, \| q_1 \| = 1} \sum_{i=1}^m (q^T A_i q)(q_1^T A_i q_1),
%\end{eqnarray}
%with an optimal solution $q^{\ast\ast} \in \Re^n$.
%\item[\hspace{5pt}($p4$)]  If (\ref{procedureoptimizationproblem2}) is greater than zero, then stop with the dual SDP (\ref{dualSDP}) with $C = 0$, having strictly feasible solutions, and one such solution is given by:
%\begin{eqnarray*}
%y = - \left( (q^{\ast\ast})^T A_1 q^{\ast\ast}, \ldots, (q^{\ast\ast})^T A_m q^{\ast\ast} \right)^T, \quad Y = \sum_{i=1}^m ((q^{\ast\ast})^T A_i q^{\ast\ast}) A_i;
%\end{eqnarray*} 
%else continue.
%\item[\hspace{5pt}($p5$)]  Solve
%\begin{eqnarray}\label{procedureoptimizationproblem3}
%\max_{| y_i | \leq 1, i = 1, \ldots, m} \lambda_{\min} \left( \sum_{i=1}^m y_i A_i \right) = \max_{| y_i | \leq 1, i =1 , \ldots, m} \min_{q_1 \in \Re^m, \| q_1 \| = 1} \sum_{i=1}^m y_i (q_1^T A_i q_1),
%\end{eqnarray}
%with an optimal solution $y^\ast = (y_1^\ast, \ldots, y_m^\ast)^T$.
%\item[\hspace{5pt}($p6$)]  If (\ref{procedureoptimizationproblem3}) is greater than zero, then the dual SDP (\ref{dualSDP}) with $C = 0$, has strictly feasible solutions, and one such solution is given by:
%\begin{eqnarray*}
%y = -y^\ast,\quad Y = \sum_{i=1}^m y^\ast_i A_i;
%\end{eqnarray*}
%else the dual SDP (\ref{dualSDP}) with $C = 0$, does not have strictly feasible solutions.
%\end{description}
%\end{procedure}
%
%\begin{remark}\label{rem:procedure}
%Note that Procedure \ref{pro:strictlyfeasiblesoln} may reduce computational work in determining whether the dual SDP (\ref{dualSDP}) with $C = 0$, has strictly feasible solutions, and if does, provide such a solution.  Certainly, solving (\ref{procedureoptimizationproblem3}) will answer this question once and for all, but in the above procedure, solving this optimization problem is a last resort, and before coming to this,  the optimization problems (\ref{procedureoptimizationproblem1}), (\ref{procedureoptimizationproblem2}) are solved - tasks which may be easier computationally.  Depending on the outcome of earlier steps in the above procedure, it may not be necessary to solve (\ref{procedureoptimizationproblem3}), which is likely to take the most computational effort, to determine whether the dual SDP (\ref{dualSDP}) with $C = 0$, has strictly feasible solutions.  If the computational effort to solve (\ref{procedureoptimizationproblem1}) and (\ref{procedureoptimizationproblem2}) are the same as that for (\ref{procedureoptimizationproblem3}), then the above procedure does not offer any advantage.
%\end{remark}



%\begin{theorem}\label{thm:findingdualSDP}
%Suppose the dual SDP (\ref{dualSDP}) with $C = 0$, has a strictly feasible solution, then 
%\begin{eqnarray*}
%y_i = -(q^\ast)^TA_i q^\ast, i = 1, \ldots, m, \quad Y = \sum_{i=1}^m ((q^\ast)^T A_i q^\ast)A_i,
%\end{eqnarray*}
%where $q^\ast \in \Re^n$ is an optimal solution to 
%\begin{eqnarray}\label{nonlinearprogram}
%\min_{q \in \Re^n, \|q \| = 1} \frac{1}{4} \sum_{i=1}^m (q^T A_i q)^2,
%\end{eqnarray}
%is one such solution.
%\end{theorem}
%\begin{proof}
%Since the dual SDP (\ref{dualSDP}) with $C = 0$, has a strictly feasible solution, then we have 
%\begin{eqnarray*}
%\frac{1}{4} \sum_{i=1}^m ((q^\ast)^T A_i q^\ast)^2 > 0.
%\end{eqnarray*}
%Now, consider
%\begin{eqnarray*}
%\min_{q \in \Re^n, \| q \| = 1} q^T\left( \sum_{i=1}^m ((q^\ast)^T A_i q^\ast)A_i \right)q.
%\end{eqnarray*}
%Let an optimal solution to the above minimization problem be given by $q^{\ast \ast} \in \Re^n$.  Suppose
%\begin{eqnarray*}
%(q^{\ast\ast})^T\left( \sum_{i=1}^m ((q^\ast)^T A_i q^\ast)A_i \right)q^{\ast \ast}  \leq 0.
%\end{eqnarray*}
%Consider
%\begin{eqnarray*}
%f(\lambda) := (\lambda q^\ast + (1 - \lambda) q^{\ast\ast})^T \left( \sum_{i=1}^m ((q^\ast)^T A_i q^\ast)A_i \right)  (\lambda q^\ast + (1 - \lambda) q^{\ast\ast}),
%\end{eqnarray*}
%where $0 \leq \lambda \leq 1$.  Then $f$ is a continuous function of $\lambda$, with $f(0) \leq 0$ and $f(0) > 0$.  Hence, there exists $\lambda_0, 0 \leq \lambda_0 \leq 1$, such that $f(\lambda_0) = 0$.  Let 
%\begin{eqnarray*}
%Y =  (\lambda_0 q^\ast + (1 - \lambda_0) q^{\ast\ast}) (\lambda_0 q^\ast + (1 - \lambda_0) q^{\ast\ast})^T \not= 0.
%\end{eqnarray*}
%Since $f(\lambda_0) = 0$, we have $y_i^0 := (q^\ast)^T A_i q^\ast, i = 1, \ldots, m$, is such that
%\begin{eqnarray*} 
%{\rm{Tr}}\left( \left(\sum_{i=1}^{m} y_i^0 A_i\right) Y \right) = 0,
%\end{eqnarray*}
%which is a contradiction to the dual SDP (\ref{dualSDP}) having a strictly feasible solution, by Proposition \ref{prop:necessarysufficient}.   Hence 
%\begin{eqnarray*}
%\min_{q \in \Re^n, \| q \| = 1} q^T\left( \sum_{i=1}^m ((q^\ast)^T A_i q^\ast)A_i \right)q > 0.
%\end{eqnarray*}
%Therefore,
%\begin{eqnarray*}
%\sum_{i=1}^m ((q^\ast)^T A_i q^\ast)A_i  \in S^n_{++}.
%\end{eqnarray*}
%\end{proof}
%
%To find an optimal solution to the nonlinear program (\ref{nonlinearprogram}), we can resort to nonlinear programming numerical techniques, as found for example in ???.  %It is also worth investigating whether we can have a closed form optimal solution to the nonlinear program (\ref{nonlinearprogram}), which may be possible, since it has only a single constraint and the objective function is a quartic polynomial having a nice structure.
% We say that $(X,y,Y, \tau, \kappa) \in S^n_{++} \times \Re^m \times S^n_{++} \times \Re_{++} \times \Re_{++}$ is an infeasible interior point to homogeneous feasibility model (\ref{homogeneous1})-(\ref{homogeneous4}) if it does not satisfy (\ref{homogeneous1})-(\ref{homogeneous3}).
%Let us now write
%(\ref{homogeneous1})-(\ref{homogeneous3}) in another way as described below.
%
%From (\ref{homogeneous1}), we have
%\begin{eqnarray}\label{homogeneous1prime}
%\left[ \begin{array}{cc}
%        {\mbox{svec}}(A_1)^T & -b_1 \\
%        \vdots               & \vdots \\
%        {\mbox{svec}}(A_m)^T & -b_m
%        \end{array} \right]
%\left[ \begin{array}{c}
%        {\mbox{svec}}(X) \\
%        \tau
%        \end{array} \right] = 0.
%\end{eqnarray}
%
%On the other hand, combining (\ref{homogeneous2}) and (\ref{homogeneous3}) into one equation, we obtain
%\begin{eqnarray}\label{homogeneous23prime}
%\left[ \begin{array}{ccc}
%        {\mbox{svec}}(A_1) & \ldots & {\mbox{svec}}(A_m) \\
%        -b_1               & \ldots & -b_m
%        \end{array} \right]y +
%\left[ \begin{array}{cc}
%            0 & -{\mbox{svec}}(C) \\
%            {\mbox{svec}}(C)^T & 0
%        \end{array} \right] \left[ \begin{array}{c}
%                                        {\mbox{svec}}(X) \\
%                                        \tau
%                                    \end{array} \right] +
%\left[ \begin{array}{c}
%            {\mbox{svec}}(Y) \\
%            \kappa
%        \end{array} \right] = 0.
%\end{eqnarray}
%
%Let the following set of linearly independent vectors in $\Re^{\tilde{n} + 1}$
%\begin{eqnarray*}
%\left\{ \left[ \begin{array}{c}
%            {\mbox{svec}}(B_1) \\
%            d_1
%        \end{array} \right], \ldots,
%\left[ \begin{array}{c}
%            {\mbox{svec}}(B_{\tilde{n}+1 - m}) \\
%            d_{\tilde{n}+1-m}
%        \end{array} \right] \right\}
%\end{eqnarray*}
%spans the orthogonal subspace to the space spanned by
%\begin{eqnarray*}
%\left[ \begin{array}{c}
%            {\mbox{svec}}(A_1) \\
%            -b_1
%        \end{array} \right], \ldots,
%\left[ \begin{array}{c}
%            {\mbox{svec}}(A_m) \\
%            -b_m
%        \end{array} \right],
%\end{eqnarray*}
%\noindent where $\tilde{n} = n(n+1)/2$.
%
%Let
%\begin{eqnarray*}
%\mathcal{A} := \left[ \begin{array}{c}
%                        {\mbox{svec}}(A_1)^T \\
%                        \vdots \\
%                        {\mbox{svec}}(A_m)^T
%                    \end{array} \right], \ \
%b := \left[ \begin{array}{c}
%                b_1 \\
%                \vdots \\
%                b_m
%            \end{array} \right]
%\end{eqnarray*}
%and
%\begin{eqnarray*}
%\mathcal{B} := \left[ \begin{array}{c}
%                        {\mbox{svec}}(B_1)^T \\
%                        \vdots \\
%                        {\mbox{svec}}(B_{\tilde{n}+1 - m})^T
%                        \end{array} \right],\ \
%d := \left[ \begin{array}{c}
%                d_1 \\
%                \vdots \\
%                d_{\tilde{n} + 1 - m}
%            \end{array} \right].
%\end{eqnarray*}
%
%Therefore, (\ref{homogeneous1prime}) and (\ref{homogeneous23prime}) can be written as
%\begin{eqnarray}\label{eq:homogeneous1prime}
%[\mathcal{A}\ - b] 
%\left[ \begin{array}{c}
%{\rm{svec}}(X) \\
%\tau
%\end{array} \right] = 0
%\end{eqnarray}
%and
%\begin{eqnarray}\label{eq:homogeneous23prime}
%\left[ \begin{array}{cc}
%		\mathcal{A}^T \\
%		b^T 
%		\end{array} \right] y + 
%\left[ \begin{array}{cc}
%            0 & -{\mbox{svec}}(C) \\
%            {\mbox{svec}}(C)^T & 0
%        \end{array} \right] \left[ \begin{array}{c}
%                                        {\mbox{svec}}(X) \\
%                                        \tau
%                                    \end{array} \right] +
%\left[ \begin{array}{c}
%            {\mbox{svec}}(Y) \\
%            \kappa
%        \end{array} \right] = 0
%\end{eqnarray}
%respectively.  Furthermore, (\ref{eq:homogeneous23prime}) holds if and only if
%\begin{eqnarray*}
%[d {\rm{svec}}(C)^T\ - \mathcal{B} {\rm{svec}}(C) \ \mathcal{B} \ d ] \left[ \begin{array}{c}
%{\rm{svec}}{X} \\
%\tau \\
%{\rm{svec}}(Y) \\
%\kappa
%\end{array} \right] = 0.
%\end{eqnarray*}
%
%The above development implies that (\ref{homogeneous1})-(\ref{homogeneous3}) can be rewritten
%as
%\begin{eqnarray}\label{homogeneous1to3combined}
%\left[ \begin{array}{cccc}
%        \mathcal{A} & -b & 0 & 0 \\
%        d{\mbox{svec}}(C)^T & -\mathcal{B}{\mbox{svec}}(C) &
%        \mathcal{B} & d
%        \end{array} \right]
%\left[ \begin{array}{c}
%        {\mbox{svec}}(X) \\
%        \tau \\
%        {\mbox{svec}}(Y) \\
%        \kappa
%        \end{array} \right] = 0.
%\end{eqnarray}
%
%We have that (\ref{homogeneous1to3combined}) together with
%\begin{eqnarray}\label{homogeneous4prime}
%X \in S^n_+, Y \in S^n_+, \tau \geq 0, \kappa \geq 0,
%\end{eqnarray}
%forms the homogeneous feasibility model of the SDP pair (\ref{primalSDP})-(\ref{dualSDP}).
%
%We remind the reader that (\ref{dualitygap}) holds, which follows from (\ref{homogeneous1to3combined}) or (\ref{homogeneous1})-(\ref{homogeneous3}).  Also, it is easy to see that the matrix on the left-hand side of (\ref{homogeneous1to3combined}) has full row rank.
%
%


\section{Homogeneous Feasibility Model as a Semi-definite Linear Complementarity Problem}\label{sec:SDLCP}

Recall that a semi-definite linear complementarity problem (SDLCP), introduced in  \cite{Kojima1}, is given by:
\begin{eqnarray}
& \mathcal{A}_1(X^1) + \mathcal{B}_1(Y^1)  =  q, \label{SDLCP1} \\
& X^1Y^1  =  0, \label{SDLCP2} \\
& X^1, Y^1  \in  S^{n_1}_+, \label{SDLCP3}
\end{eqnarray}
where $\mathcal{A}_1, \mathcal{B}_1 : S^{n_1} \rightarrow \Re^{\tilde{n}_1}$ are linear operators, $q \in \Re^{\tilde{n}_1}$, and $\tilde{n}_1 = n_1(n_1+1)/2$.  The following assumptions are imposed on the SDLCP, which we show in Proposition \ref{prop:SDLCPAssumption} to hold for the SDLCP representation of the homogeneous feasibility model of the LSDFP.  {\textcolor{black}{This SDLCP representation will be derived later in the section.}}
\begin{assumption}\label{ass:SDLCPassumptions}
\begin{enumerate}[(a)]
\item System (\ref{SDLCP1})-(\ref{SDLCP3}) is monotone.  That is, $\mathcal{A}_1(X^1) + \mathcal{B}_1(Y^1) = 0$ for $X^1, Y^1 \in S^{n_1} \Rightarrow {\rm{Tr}}(X^1Y^1) \geq 0$.
\item There exists at least one solution to SDLCP (\ref{SDLCP1})-(\ref{SDLCP3}).
\item $\{ \mathcal{A}_1(X^1) + \mathcal{B}_1(Y^1)\ ; \ X^1, Y^1 \in S^{n_1} \} = \Re^{\tilde{n}_1}$.
\end{enumerate}
\end{assumption}

When the SDLCP (\ref{SDLCP1})-(\ref{SDLCP3}) is studied in some papers in the literature, instead of Assumption \ref{ass:SDLCPassumptions}(b), the following assumption is imposed:
\begin{assumption}\label{ass:strictfeasibility}
There exist $X^1, Y^1 \in S^{n_1}_{++}$ such that $\mathcal{A}_1(X^1) + \mathcal{B}_1(Y^1) = q$.
\end{assumption}
Assumptions \ref{ass:SDLCPassumptions}(a), (c) and \ref{ass:strictfeasibility} are imposed in \cite{Kojima1} where the paper studies feasible interior point algorithms on the SDLCP (\ref{SDLCP1})-(\ref{SDLCP3}), while \cite{Kojima,Sim1} assume Assumption \ref{ass:SDLCPassumptions} in the study of infeasible interior point algorithms on the SDLCP (\ref{SDLCP1})-(\ref{SDLCP3}).  In the study of infeasible interior point algorithms on the SDLCP (\ref{SDLCP1})-(\ref{SDLCP3}), it is not necessary to impose Assumption \ref{ass:strictfeasibility}.  A reason {\textcolor{black}{is}} that we do not need a strictly feasible initial iterate to the algorithm.  In this paper, we solve an LSDFP by applying an interior point algorithm on the homogeneous feasibility model (\ref{homogeneous1LSDFP})-(\ref{homogeneous4LSDFP}).  This interior point algorithm is necessarily infeasible as discussed near the end of Section \ref{sec:LSDFPhomogeneousmodel}.  When we express the homogeneous feasibility model as an SDLCP (to be discussed next), the interior point algorithm that is applied to the homogeneous feasibility model can be equivalently applied to the corresponding SDLCP, and by necessity, the algorithm on the SDLCP is infeasible just like that for the homogeneous feasibility model.  Therefore, having Assumption \ref{ass:strictfeasibility} imposed on the SDLCP is not suitable and in fact can never hold in our case as there cannot exist an $(X, Y) \in S^n_{++} \times S^n_{++}$ such that (\ref{SDLCP1}) holds for the SDLCP obtained from the homogeneous feasibility model.  We therefore have Assumption \ref{ass:SDLCPassumptions}(b) in its place.  In fact, we will see later that when the LSDFP satisfies Assumption \ref{ass:SDP}, {\textcolor{black}{the SDLCP representation of the homogeneous feasibility model (\ref{homogeneous1LSDFP})-(\ref{homogeneous4LSDFP}) satisfies Assumption \ref{ass:SDLCPassumptions}.}}


We now proceed to express the homogeneous feasibility model (\ref{homogeneous1LSDFP})-(\ref{homogeneous4LSDFP}) as an SDLCP.  

%First, we form the following system:
%\begin{eqnarray*}
%& & {\rm{Tr}}\left( \left( \begin{array}{cc}
%						A_i & 0 \\
%						0   & -b_i
%						\end{array} \right) X_1 \right)  =  0,\ i = 1, \ldots, m, \\
%& & {\rm{Tr}}(E_{n+1, i-m} X_1)  =  0,\ i = m+1, \ldots, m+n, \\
%& & \left( \begin{array}{cc}
%	\sum_{i=1}^m y_i A_i - (X_1)_{n+1,n+1} C & 0 \\
%	0 & -b^Ty + {\rm{Tr}}(C (X_1)_{1\leq n, 1 \leq n}) 
%		\end{array} \right) + Y_1  =  0, \\
%& & X_1 \in S^{n+1}_+, Y_1 \in S^{n+1}_+.
%\end{eqnarray*}
%
%Let us now write
%(\ref{homogeneous1})-(\ref{homogeneous3}) in another way as described below.

We can write (\ref{homogeneous1LSDFP}) as
\begin{eqnarray}\label{homogeneous1prime}
\left[ \begin{array}{cc}
        {\mbox{svec}}(A_1)^T & -b_1 \\
        \vdots               & \vdots \\
        {\mbox{svec}}(A_m)^T & -b_m
        \end{array} \right]
\left[ \begin{array}{c}
        {\mbox{svec}}(X) \\
        \tau
        \end{array} \right] = 0.
\end{eqnarray}

On the other hand, combining (\ref{homogeneous2LSDFP}) and (\ref{homogeneous3LSDFP}) into one equation, we obtain
\begin{eqnarray}\label{homogeneous23prime}
\left[ \begin{array}{ccc}
       -b_1               & \ldots & -b_m \\
        {\mbox{svec}}(A_1) & \ldots & {\mbox{svec}}(A_m) 
        \end{array} \right]y +
\left[ \begin{array}{c}
		\kappa \\
            {\mbox{svec}}(Y) 
        \end{array} \right] = 0.
\end{eqnarray}

Let us now rewrite (\ref{homogeneous1prime}) and (\ref{homogeneous23prime}) in a more compact form.  Let 
\begin{eqnarray*}
\mathcal{A} := \left[ \begin{array}{c}
                        {\mbox{svec}}(A_1)^T \\
                        \vdots \\
                        {\mbox{svec}}(A_m)^T
                    \end{array} \right] \in \Re^{m \times \tilde{n}},
\end{eqnarray*}
and recall that $b = (b_1, \ldots, b_m)^T$.  Then, (\ref{homogeneous1prime}) and (\ref{homogeneous23prime}) can be written as
\begin{eqnarray}\label{eq:homogeneous1prime}
[\mathcal{A}\ - b] 
\left[ \begin{array}{c}
{\rm{svec}}(X) \\
\tau
\end{array} \right] = 0
\end{eqnarray}
and
\begin{eqnarray}\label{eq:homogeneous23prime}
\left[ \begin{array}{cc}
		- b^T  \\
		\mathcal{A}^T 		
		\end{array} \right] y + 
%\left[ \begin{array}{cc}
%            0 & {\mbox{svec}}(C)^T \\
%           - {\mbox{svec}}(C) & 0
%        \end{array} \right] \left[ \begin{array}{c}
%         					     \tau \\
%                                        {\mbox{svec}}(X)                                    
%                                    \end{array} \right] +
\left[ \begin{array}{c}
		  \kappa \\
            {\mbox{svec}}(Y)           
        \end{array} \right] = 0
\end{eqnarray}
respectively.

Let $[d_1\ {\rm{svec}}(B_1)^T]^T $, $d_1 \not= 0$, be orthogonal to the set
\begin{eqnarray*}
\left\{ \left[ \begin{array}{c}
            -b_1 \\
            {\mbox{svec}}(A_1) 
        \end{array} \right], \ldots,
\left[ \begin{array}{c}
            -b_m \\
            {\mbox{svec}}(A_m) 
        \end{array} \right]  \right\}.
\end{eqnarray*}
Since $d_1 \not= 0$ and $b_i \not= 0 $ for some $i = 1, \ldots, m$, $B_1$ is necessarily nonzero.

Furthermore, let the following set of linearly independent vectors in $\Re^{\tilde{n} }$
\begin{eqnarray*}
\left\{  {\mbox{svec}}(B_2), \ldots,  {\mbox{svec}}(B_{\tilde{n}+1 - m}) \right\}
\end{eqnarray*}
spans the orthogonal subspace to the space spanned by
\begin{eqnarray*}
\left\{  {\mbox{svec}}(A_1), \ldots,  {\mbox{svec}}(A_m) \right\} ,
\end{eqnarray*}
\noindent where $\tilde{n} = n(n+1)/2$.  

\begin{remark}\label{rem:linearlyindependentvectors}
We have
\begin{eqnarray*}
\left\{ \left[ \begin{array}{c}
            -b_1 \\
            {\mbox{svec}}(A_1) 
        \end{array} \right], \ldots,
\left[ \begin{array}{c}
            -b_m \\
            {\mbox{svec}}(A_m) 
        \end{array} \right],
 \left[ \begin{array}{c}
 		d_1 \\
 		{\mbox{svec}}(B_1)
 		\end{array} \right],
\left[ \begin{array}{c}
		0 \\
		{\mbox{svec}}(B_2)
		\end{array} \right], \ldots,
\left[ \begin{array}{c}
		0 \\
		{\mbox{svec}}(B_{\tilde{n}+1-m}) 
		\end{array} \right] \right\}
\end{eqnarray*}
spans $\Re^{\tilde{n}+1}$, with elements in 
\begin{eqnarray*}
\left\{ \left[ \begin{array}{c}
            -b_1 \\
            {\mbox{svec}}(A_1) 
        \end{array} \right], \ldots,
\left[ \begin{array}{c}
            -b_m \\
            {\mbox{svec}}(A_m) 
        \end{array} \right] \right\}
\end{eqnarray*}
orthogonal to elements in
\begin{eqnarray*}
\left\{  \left[ \begin{array}{c}
 		d_1 \\
 		{\mbox{svec}}(B_1)
 		\end{array} \right],
\left[ \begin{array}{c}
		0 \\
		{\mbox{svec}}(B_2)
		\end{array} \right], \ldots,
\left[ \begin{array}{c}
		0 \\
		{\mbox{svec}}(B_{\tilde{n}+1-m}) 
		\end{array} \right] \right\}.
\end{eqnarray*}
\end{remark}

Let
\begin{eqnarray}\label{def:B}
\mathcal{B} := \left[ \begin{array}{c}
                        {\mbox{svec}}(B_1)^T \\
                        {\mbox{svec}}(B_2)^T \\
                        \vdots \\
                        {\mbox{svec}}(B_{\tilde{n}+1 - m})^T
                        \end{array} \right] \in \Re^{(\tilde{n}+1-m)\times \tilde{n}},\ \
d := \left[ \begin{array}{c}
                d_1 \\
                0 \\
                \vdots \\
               0
            \end{array} \right] \in \Re^{\tilde{n} + 1 - m}.
\end{eqnarray}

Then, (\ref{eq:homogeneous23prime}) holds if and only if
\begin{eqnarray}\label{eq:homogeneous23primeprime}
[d\ \ \mathcal{B}] \left[ \begin{array}{c}
\kappa \\
{\rm{svec}}(Y) 
\end{array} \right] = 0
\end{eqnarray}
since
\begin{eqnarray*}
[d\ \ \mathcal{B}]
 \left[ \begin{array}{cc}
		- b^T  \\
		\mathcal{A}^T 		
		\end{array} \right] = 0.
\end{eqnarray*}

\begin{remark}\label{rem:homogeneousmodel}
Note that for $(y,Y)$ feasible to the dual SDP (\ref{dualSDP}) with $C = 0$, we have the entries on the left-hand side of (\ref{eq:homogeneous23primeprime}) are equal to zero, except possibly the first entry.  This follows from the way $d$ is constructed, and that for such $(y,Y)$, the left-hand side of (\ref{homogeneous23prime}) (or (\ref{eq:homogeneous23prime})) has all its entries equal to zero, except possibly the first entry.  {\textcolor{black}{By choosing $B_1$ appropriately\footnote{{\textcolor{black}{Details are provided in Appendix \ref{sec:Appendix}.}}}}}, we can then apply a result in the literature, namely, Theorem 5.1 in \cite{Sim1}, to show superlinear convergence of the interior point algorithm considered in this paper on the homogeneous feasibility model of the LSDFP.  {\textcolor{black}{This is possible when the initial iterate to the algorithm is from a strictly feasible solution to the dual SDP (\ref{dualSDP}) with $C = 0$.}}
\end{remark}

The above development implies that (\ref{homogeneous1LSDFP})-(\ref{homogeneous3LSDFP}) can be rewritten
as
\begin{eqnarray}\label{homogeneous1to3combined}
\left[ \begin{array}{cccc}
        \mathcal{A} & -b & 0 & 0 \\
        0 & 0 &  d  & \mathcal{B} 
        \end{array} \right]
\left[ \begin{array}{c}
        {\mbox{svec}}(X) \\
        \tau \\
         \kappa \\
        {\mbox{svec}}(Y) 
        \end{array} \right] = 0.
\end{eqnarray}

We have (\ref{homogeneous1to3combined}) together with
\begin{eqnarray}\label{homogeneous4prime}
X \in S^n_+, Y \in S^n_+, \tau \geq 0, \kappa \geq 0,
\end{eqnarray}
is {\textcolor{black}{another way the homogeneous feasibility model of the given LSDFP can be represented.}}



%We remind the reader that (\ref{dualitygap}) holds, which follows from (\ref{homogeneous1to3combined}) or (\ref{homogeneous1})-(\ref{homogeneous3}).  Also, it is easy to see that the matrix on the left-hand side of (\ref{homogeneous1to3combined}) has full row rank.

%We now express the homogeneous feasibility model as an SDLCP, by first making the following observation:
%\begin{proposition}\label{prop:lineardependence}
%For $i = 1, \ldots, \tilde{n} + 1 - m$,
%\begin{eqnarray*}
%\left[ \begin{array}{c}
%		d_i {\rm{svec}}(C) \\
%		- {\rm{svec}}(B_i)^T{\rm{svec}}(C) 
%		\end{array} \right] \in \Re^{\tilde{n} + 1}
%\end{eqnarray*}
%is a linear combination of 
%\begin{eqnarray*}
%\left[ \begin{array}{c}
%		{\rm{svec}}(A_1) \\
%			-b_1
%			\end{array}\right], \ldots, 
%\left[ \begin{array}{c}
%		{\rm{svec}}(A_m) \\
%			-b_m
%		\end{array} \right].
%\end{eqnarray*}
%\end{proposition}
%\begin{proof}
%First, note that
%\begin{eqnarray*}
%\left\{ \left[ \begin{array}{c}
%		{\rm{svec}}(A_1) \\
%			-b_1
%			\end{array}\right], \ldots, 
%\left[ \begin{array}{c}
%		{\rm{svec}}(A_m) \\
%			-b_m
%		\end{array} \right], 
%\left[ \begin{array}{c}
%		{\rm{svec}}(B_1) \\
%			d_1
%		\end{array} \right], \ldots,
%\left[ \begin{array}{c}
%		{\rm{svec}}(B_{\tilde{n} + 1 -m}) \\
%			d_{\tilde{n} + 1 - m}
%		\end{array} \right] \right\}
%\end{eqnarray*}
%spans $\Re^{\tilde{n} + 1}$.  Therefore, given 
%\begin{eqnarray*}
%\left[ \begin{array}{c}
%		d_i {\rm{svec}}(C) \\
%		- {\rm{svec}}(B_i)^T{\rm{svec}}(C) 
%		\end{array} \right],
%\end{eqnarray*}
%there exists $u^i_j, j = 1, \ldots, m$, $v_i$, $v^i_k$,  $k = 1, \ldots, \tilde{n} + 1 - m$, $k \not= i$, such that
%\begin{eqnarray*}
%\left[ \begin{array}{c}
%		d_i {\rm{svec}}(C) \\
%		- {\rm{svec}}(B_i)^T{\rm{svec}}(C) 
%		\end{array} \right]
%= \sum_{j=1}^m u_j^i \left[ \begin{array}{c}
%		{\rm{svec}}(A_j) \\
%			-b_j
%			\end{array}\right]
%+ v_i \left[ \begin{array}{c}
%		{\rm{svec}}(B_i) \\
%			d_i
%		\end{array} \right]
%+ \sum_{k = 1, k \not= i}^{\tilde{n} + 1 -m} v_k^i
%		\left[ \begin{array}{c}
%		{\rm{svec}}(B_k) \\
%			d_k
%		\end{array} \right].
%\end{eqnarray*}
%Multiplying both sides of the above equality by $({\rm{svec}}(B_l)^T\ d_l)$ and noting that \begin{eqnarray*}
%\left\{ \left[ \begin{array}{c}
%            {\mbox{svec}}(B_1) \\
%            d_1
%        \end{array} \right], \ldots,
%\left[ \begin{array}{c}
%            {\mbox{svec}}(B_{\tilde{n}+1 - m}) \\
%            d_{\tilde{n}+1-m}
%        \end{array} \right] \right\}
%\end{eqnarray*}
%is an orthogonal set which is orthogonal to $\left[ \begin{array}{c}
%            {\mbox{svec}}(A_j) \\
%            -b_j
%        \end{array} \right], j = 1, \ldots, m$, 
%we first observe that if $l = i$, then $v_i = 0$.  If $l \not= i$, then
%\begin{eqnarray}\label{eqn:1}
%d_i {\rm{svec}}(B_l)^T {\rm{svec}}(C) - d_l {\rm{svec}}(B_i)^T {\rm{svec}}(C) = v_l^i \left\| \left[ \begin{array}{c}
%		{\rm{svec}}(B_l) \\
%			d_l
%		\end{array} \right] \right\|^2.
%\end{eqnarray}
%On the other hand, we have
%\begin{eqnarray}\label{eqn:2}
%d_l {\rm{svec}}(B_i)^T {\rm{svec}}(C) - d_i {\rm{svec}}(B_l)^T {\rm{svec}}(C) = v_i^l \left\| \left[ \begin{array}{c}
%		{\rm{svec}}(B_i) \\
%			d_i
%		\end{array} \right] \right\|^2.
%\end{eqnarray}
%Adding (\ref{eqn:1}) and (\ref{eqn:2}), we get $v_l^i = v_i^l = 0$, and the proposition is proved.
%\end{proof}
%
%Using Proposition \ref{prop:lineardependence}, by performing row operations on (\ref{homogeneous1to3combined}), we can write (\ref{homogeneous1to3combined}) as
%\begin{eqnarray}\label{homogeneous1to3combinedprime1}
%\left[ \begin{array}{cccc}
%        \mathcal{A} & -b & 0 & 0 \\
%        0 & 0 &
%        d  &  \mathcal{B}
%        \end{array} \right]
%\left[ \begin{array}{c}
%        {\mbox{svec}}(X) \\
%        \tau \\
%        \kappa \\
%        {\mbox{svec}}(Y) 
%        \end{array} \right] = 0.
%\end{eqnarray}

By switching the position of $d$ and $\mathcal{B}$  in (\ref{homogeneous1to3combined}), we can rewrite it as
\begin{eqnarray}\label{homogeneous1to3combinedprime}
\left[ \begin{array}{cccc}
        \mathcal{A} & -b & 0 & 0 \\
        0 & 0 &
        \mathcal{B}  & d
        \end{array} \right]
\left[ \begin{array}{c}
        {\mbox{svec}}(X) \\
        \tau \\
        {\mbox{svec}}(Y)  \\
        \kappa         
        \end{array} \right] = 0.
\end{eqnarray}

It is easy to convince ourselves that (\ref{homogeneous1to3combinedprime}), (\ref{homogeneous4prime}) is also a {\textcolor{black}{representation}} of the homogeneous feasibility model of the LSDFP, just like (\ref{homogeneous1LSDFP})-(\ref{homogeneous4LSDFP}), and (\ref{homogeneous1to3combined}), (\ref{homogeneous4prime}).  

We are now ready to express the homogeneous feasibility model of the LSDFP as SDLCP (\ref{SDLCP1})-(\ref{SDLCP3}) by letting $n_1 = n+1, q = 0$, and $\mathcal{A}_1, \mathcal{B}_1$ such that
\begin{eqnarray}\label{def:A1}
\begin{array}{lll}
(\mathcal{A}_1(X^1))_i & := & {\rm{Tr}}\left( \left[ \begin{array}{cc}
														A_i & 0 \\
														0 & -b_i 
													\end{array} \right] X^1 \right),\ \ i = 1, \ldots, m, \\
(\mathcal{A}_1(X^1))_i & := & {\rm{Tr}}(E^{i-m,n+1} X^1), \ \ i = m+1, \ldots, m+n, \\
(\mathcal{A}_1(X^1))_i & := & 0, \ \ i = m + n + 1, \ldots, \tilde{n}_1,
\end{array}
\end{eqnarray}
and
\begin{eqnarray}\label{def:B1}
\begin{array}{lll}
(\mathcal{B}_1(Y^1))_j & := & 0, \ \ j = 1, \ldots, m + n, \\
%(\mathcal{B}_1(Y^1))_j & = & {\rm{Tr}}(E^{j-(m+n),n+1} Y^1), \ \ j = m + n + 1, \ldots, m + 2n, \\
(\mathcal{B}_1(Y^1))_j & := & {\rm{Tr}}\left(\left[ \begin{array}{cc}
										B_{j-(m+n)}   & 0 \\
										0 & d_{j-(m+n)}
										\end{array} \right]Y^1 \right), \ \ j = m + n + 1, \ldots, \tilde{n}_1,
\end{array}
\end{eqnarray}
for $X^1, Y^1 \in S^{n_1}$.

\begin{remark}\label{rem:LSDFP}
Note that the above SDLCP representation of the homogeneous feasibility model has the structure of an LSDFP.  Recall that an LSDFP written as the SDLCP (\ref{SDLCP1})-(\ref{SDLCP3}) is such that in (\ref{SDLCP1}), $(\mathcal{A}_1(X^1))_i = 0$ for $i = m_1 + 1, \ldots, \tilde{n}_1$, $(\mathcal{B}_1(Y^1))_j = 0$ for $j = 1, \ldots, m_1$, and $q_i = 0$ for $i = 1, \ldots, m_1$ or $q_i = 0$ for $i = m_1 + 1, \ldots, \tilde{n}_1$.  Here, $q = (q_1, \ldots, q_{\tilde{n}_1})^T$.  From (\ref{def:A1}), (\ref{def:B1}), we see that for our SDLCP representation, it has this structure with $m_1 = m + n$ and $q = 0$.    Having $q = 0$, in particular, $q_i = 0$ for $i = 1, \ldots, m_1$, is the property satisfied by our SDLCP representation that does not hold for the LSDFP {\textcolor{black}{considered}} in this paper. {\textcolor{black}{ In fact, having this property is the key that allows us to apply results in the literature, namely \cite{Sim1}, to show the main result (Theorem \ref{thm:superlinearconvergence}) in this paper  on superlinear convergence.}} 
\end{remark}

The following proposition relates a solution of the homogeneous feasibility model of the LSDFP to that of its SDLCP representation:
\begin{proposition}\label{prop:equivalence}
We have if $(X,Y,\tau,\kappa)$ satisfies (\ref{homogeneous1to3combinedprime}), (\ref{homogeneous4prime}), then
\begin{eqnarray}\label{eq:X1Y1}
X^1  =  \left[ \begin{array}{cc}
					X & 0 \\
					0 & \tau
				\end{array} \right], \quad
Y^1 = \left[ \begin{array}{cc}
					Y & \ast \\
					\ast & \kappa
			\end{array} \right],
\end{eqnarray}
where the ``$\ast$" entries in $Y^1$ are such that $Y^1 \in S^{n_1}_+$, satisfies (\ref{SDLCP1})-(\ref{SDLCP3}), with $n_1 = n + 1$, $q = 0$, $\mathcal{A}_1, \mathcal{B}_1$ are given  by (\ref{def:A1}), (\ref{def:B1}) respectively.  On the other hand, if $(X^1, Y^1)$ satisfies (\ref{SDLCP1})-(\ref{SDLCP3}), where $n_1 = n + 1$, $q = 0$, $\mathcal{A}_1, \mathcal{B}_1$ are given  by (\ref{def:A1}), (\ref{def:B1}) respectively, then $X^1, Y^1$ are given by (\ref{eq:X1Y1}), and $(X, Y, \tau, \kappa)$  satisfies (\ref{homogeneous1to3combinedprime}), (\ref{homogeneous4prime}).
\end{proposition}
\begin{proof}
The proposition is clear based on how $\mathcal{A}_1$ and $\mathcal{B}_1$ are defined in (\ref{def:A1}) and (\ref{def:B1}) respectively.
\end{proof}

\vspace{10pt}

We have another proposition below that shows that if the LSDFP satisfies Assumption \ref{ass:SDP}, then the SDLCP representation of its homogeneous feasibility model satisfies Assumption \ref{ass:SDLCPassumptions}:
\begin{proposition}\label{prop:SDLCPAssumption}
The SDLCP (\ref{SDLCP1})-(\ref{SDLCP3}) with $n_1 = n+1$, $q = 0$ and $\mathcal{A}_1$, $\mathcal{B}_1$ given by (\ref{def:A1}), (\ref{def:B1}) respectively, satisfies Assumption \ref{ass:SDLCPassumptions} when the LSDFP satisfies Assumption \ref{ass:SDP}.
\end{proposition}
\begin{proof}
We first make two observations.  Firstly, if $(X, Y, \tau, \kappa)$ satisfies  (\ref{homogeneous1to3combinedprime}), then ${\rm{Tr}}(XY) + \tau \kappa = 0$, by Remark \ref{rem:linearlyindependentvectors}.  Secondly, the matrix on the left-hand side of (\ref{homogeneous1to3combinedprime}) has full row rank by Assumption \ref{ass:SDP}(b), $\{ {\rm{svec}}(B_2), \ldots, {\rm{svec}}(B_{\tilde{n} + 1 - m}) \}$ linearly independent, and $[{\rm{svec}}(B_1)^T \ d_1]^T$ linearly independent from $\{ [{\rm{svec}}(B_i)^T\ 0]^T \ ; \ i = 2, \ldots, \tilde{n} + 1 - m \}$.

\noindent Given the SDLCP (\ref{SDLCP1})-(\ref{SDLCP3}) with $n_1 = n+1$, $q = 0$ and $\mathcal{A}_1$, $\mathcal{B}_1$ given by (\ref{def:A1}), (\ref{def:B1}) respectively.  Suppose
\begin{eqnarray*}
{\mathcal{A}}_1(X^1) + {\mathcal{B}}_1(Y^1) = 0
\end{eqnarray*}
for some $(X^1, Y^1) \in S^{n_1} \times S^{n_1}$.  Then $X^1$ and $Y^1$ are given  by
\begin{eqnarray*}
X^1  =  \left[ \begin{array}{cc}
					X & 0 \\
					0 & \tau
				\end{array} \right], \quad
Y^1 = \left[ \begin{array}{cc}
					Y & \ast \\
					\ast & \kappa
			\end{array} \right],
\end{eqnarray*} 
where $(X,Y, \tau, \kappa)$ satisfies (\ref{homogeneous1to3combinedprime}).  Hence, by the first observation above, we have ${\rm{Tr}}(X^1 Y^1) = {\rm{Tr}}(XY) + \tau \kappa = 0$.  Therefore, Assumption \ref{ass:SDLCPassumptions}(a) holds.  Furthermore, Assumption \ref{ass:SDLCPassumptions}(b) holds since a solution to the given SDLCP is $X^1 = 0, Y^1 = 0$.  Finally, the second observation above means that the matrix
\begin{eqnarray*}
\left[ \begin{array}{cccc}
        \mathcal{A} & -b & 0 & 0 \\
        0 & 0 &
        \mathcal{B} & d
        \end{array} \right]
\end{eqnarray*}
has full row rank.  This implies that the matrix $(\mathcal{A}_1 \ \mathcal{B}_1)$, where $\mathcal{A}_1$ and $\mathcal{B}_1$ are defined by (\ref{def:A1}) and (\ref{def:B1}) respectively, has full row rank, and hence Assumption \ref{ass:SDLCPassumptions}(c) holds as well. 
\end{proof}


%
%
%
%
%
%				
%By Proposition \ref{prop:equivalence}, we see that we can express the homogeneous feasibility model of primal-dual SDP pair (\ref{primalSDP})-(\ref{dualSDP}) as SDLCP (\ref{SDLCP1})-(\ref{SDLCP3}), with $n_1 = n+1$, $q = 0$, and $\mathcal{A}_1, \mathcal{B}_1$ given by (\ref{def:A1}), (\ref{def:B1}) respectively.  We have Assumption \ref{ass:SDLCPassumptions} holds for the resulting SDLCP.  If $(X^1, Y^1)$ satisfies ${\mathcal{A}}_1(X^1) + {\mathcal{B}}_1(Y^1) = 0$, then 
%\begin{eqnarray*}
%X^1  =  \left[ \begin{array}{cc}
%					X & 0 \\
%					0 & \tau
%				\end{array} \right], \quad
%Y^1 = \left[ \begin{array}{cc}
%					Y & \ast \\
%					\ast & \kappa
%			\end{array} \right],
%\end{eqnarray*} 
%where ${\rm{Tr}}(XY) + \tau \kappa = 0$, since $(X,Y,\tau,\kappa)$ satisfies (\ref{homogeneous1to3combinedprime}).  Hence, ${\rm{Tr}}(X^1Y^1) = 0$, and Assumption \ref{ass:SDLCPassumptions}(a) then holds.  Assumption \ref{ass:SDLCPassumptions}(b) holds since it is easy to see that $(X^1,Y^1) = (0,0)$ is a solution to the SDLCP, while Assumption \ref{ass:SDLCPassumptions}(c) is true by definition of $\mathcal{A}_1$ and $\mathcal{B}_1$ in (\ref{def:A1}), (\ref{def:B1}) respectively, and that the matrix on the left-hand side of (\ref{homogeneous1to3combinedprime}) has full row rank.


\section{An Infeasible Interior Point
Algorithm on the Homogeneous Feasibility Model}\label{sec:interiorpointalgorithm}

We describe in this section an infeasible path-following interior point algorithm, {\textcolor{black}{Algorithm \ref{mainalgorithm}}}, on the homogeneous
feasibility model (\ref{homogeneous1LSDFP})-(\ref{homogeneous4LSDFP}) (or (\ref{homogeneous1to3combined}), (\ref{homogeneous4prime}) or (\ref{homogeneous1to3combinedprime}), (\ref{homogeneous4prime})).  It generates iterates that follow an infeasible
central path in a (narrow) neighborhood.  This algorithm is a predictor-corrector type algorithm on the homogeneous feasibility model, and is first considered in \cite{Potra}.

%An infeasible central path to homogeneous feasibility model (\ref{homogeneous1})-(\ref{homogeneous4}), 
%$(X^c(\mu), y^c(\mu), Y^c(\mu),$ $\tau^c(\mu),$ $\kappa^c(\mu)) \in S^n_{++} \times \Re^m \times 
%S^n_{++} \times \Re_{++} \times \Re_{++}$, satisfies
%\begin{eqnarray*}
%X^c(\mu)Y^c(\mu) & = & \mu I, \\
%\tau^c(\mu) \kappa^c(\mu) & = & \mu,
%\end{eqnarray*}
%\noindent besides satisfying
%\begin{eqnarray*}
%{\mbox{Tr}}(A_i X^c(\mu)) - b_i\tau^c(\mu) & = & \frac{\mu}{\mu_0} ({\mbox{Tr}}(A_i X_0) - b_i\tau_0) ,\ i = 1, \ldots, m, \\
%\sum_{i=1}^{m} (y^c(\mu))_i A_i + Y(\mu) - \tau(\mu) C & = & \frac{\mu}{\mu_0} \left( \sum_{i=1}^{m} (y_0)_i A_i + Y_0 - \tau_0 C \right) ,  \\
%\kappa(\mu) - b^T y(\mu) + {\mbox{Tr}}(CX(\mu)) & = & \frac{\mu}{\mu_0} (\kappa_0 - b^T y_0 + {\mbox{Tr}}(CX_0)) \\
%\end{eqnarray*}
%for a given $(X_0,y_0, Y_0,\tau_0,\kappa_0) \in S^n_{++} \times \Re^m \times
%S^n_{++} \times \Re_{++} \times \Re_{++}$, which is the value taken
%by $(X^c(\mu),y^c(\mu),Y^c(\mu),$ $\tau^c(\mu),\kappa^c(\mu))$ when $\mu =
%\mu_0$.
%
%\begin{remark}\label{rem:centralpath}
%In \cite{Sim1}, under Assumption \ref{ass:SDLCPassumptions}, the author discussed the existence and uniqueness of infeasible off-central paths, as defined in the paper, to SDLCP (\ref{SDLCP1})-(\ref{SDLCP3}).  Infeasible central path is an important and special infeasible off-central path.  Since the SDLCP representation of the homogeneous feasibility model as detailed in the previous section satisfies Assumption \ref{ass:SDLCPassumptions} (Proposition \ref{prop:SDLCPAssumption}), it means that infeasible central path to the SDLCP exists and is unique.  This ensures that given an infeasible interior point to the homogeneous feasibility model, the infeasible central path to the model, as defined above, passing through the point, exists and is unique.  Furthermore, any of its accumulation points is a solution to the model as is any of the accumulation points of an infeasible central path to SDLCP (\ref{SDLCP1})-(\ref{SDLCP3}). 
%\end{remark}
%
%
%From now onwards, whenever we mention central path, we are referring
%to the infeasible central path.  


Consider the following (narrow) neighborhood of the central path:
\begin{eqnarray*}
\mathcal{N}(\beta,\mu) & := & \{(X, y, Y,\tau,\kappa) \in S^n_{++}
\times \Re^m \times S^n_{++} \times \Re_{++} \times \Re_{++}\ ;\\
& & \ \ \ \ (\|Y^{1/2}XY^{1/2} - \mu I \|_F^2 + (\tau \kappa - \mu)^2)^{1/2}
\leq \beta \mu,\ \mu = ({\rm{Tr}}(XY) + \tau \kappa)/(n+1) \}.
\end{eqnarray*}

In the algorithm, Algorithm \ref{mainalgorithm}, {\textcolor{black}{which is described below}}, iterates generated by the algorithm always stay
within a neighborhood of the central path.  {\textcolor{black}{We consider the dual Helmberg-Kojima-Monteiro (HKM) search direction in the algorithm, even though our results also hold for the Nesterov-Todd (NT) search direction - see Remark \ref{rem:NT}.}}  Among different search directions used in interior point algorithms on SDLCPs/SDPs, the Alizadeh-Haeberly-Overton (AHO) \cite{Alizadeh}, Helmberg-Kojima-Monteiro (HKM) \cite{Helmberg,Kojima1,Monteiro3} and Nesterov-Todd (NT) \cite{Nesterov,Nesterov1} search directions are better known, with the latter two being implemented in SDP solvers, such as SeDuMi \cite{Sturm1} and SDPT3 \cite{Toh1}.  {\textcolor{black}{It is worth noting that the HKM direction has also been implemented in the SDPA package to solve SDPs \cite{Yamashita,Yamashita2}.}}
  
{\textcolor{black}{Before describing the algorithm, let us consider}} the following system of equations
for $(\Delta X, \Delta y, \Delta Y,\Delta \tau, \Delta \kappa) \in S^n \times \Re^m \times S^n \times \Re \times \Re$ which plays
an important role in the algorithm:
\begin{eqnarray}
 & Y^{1/2}(X \Delta Y +  \Delta X Y)Y^{-1/2} + Y^{-1/2}(\Delta Y X + Y \Delta X)Y^{1/2}  =  2(\sigma
\mu I
- Y^{1/2}XY^{1/2}), \label{sys1}\\
 & \kappa \Delta \tau + \tau \Delta \kappa =  \sigma \mu - \tau
\kappa, \label{sys2}\\
 & {\rm{Tr}}(A_i \Delta X) - b_i \Delta \tau  =  -\overline{r}_i, \quad i = 1, \ldots, m,\label{sys3} \\
 & \sum_{i=1}^m \Delta y_i A_i + \Delta Y  =  -
\overline{s}, \label{sys4} \\
 & \Delta \kappa - b^T \Delta y  =  - \overline{\gamma}, \label{sys5}
\end{eqnarray}
where $\mu = ({\rm{Tr}}(XY) + \tau \kappa)/(n+1)$.  {\textcolor{black}{Note that (\ref{sys1}) and (\ref{sys2}) arise upon linearizing the perturbed equations to $XY = 0$ and $\tau \kappa = 0$ when we use the dual HKM search direction. Furthemore, it can be shown that in Algorithm \ref{mainalgorithm}, $(\Delta X, \Delta y, \Delta Y, \Delta \tau, \Delta \kappa)$ in (\ref{sys1})-(\ref{sys5}) is always uniquely determined - see for example \cite{Potra}.}}

%Written in matrix-vector form, (\ref{sys1})-(\ref{sys4}) can be
%written as
%
%\begin{eqnarray}\label{matrixvectorsystem}
%\left[ \begin{array}{cccc}
%        \mathcal{A} & -b & 0 & 0 \\
%        d{\mbox{svec}}(C)^T & -\mathcal{B}{\mbox{svec}}(C) &
%        \mathcal{B} & d \\
%        I & 0 & X \otimes_s Y^{-1} & 0 \\
%        0 & 1 & 0 & \frac{\tau}{\kappa}
%        \end{array} \right]
%\left[ \begin{array}{c}
%        {\mbox{svec}}(\Delta X) \\
%        \Delta \tau \\
%        {\mbox{svec}}(\Delta Y) \\
%        \Delta \kappa
%        \end{array} \right] =
%\left[ \begin{array}{c}
%        -\overline{r} \\
%        -\overline{s} \\
%        {\mbox{svec}}(\sigma \mu Y^{-1} - X) \\
%        \frac{\sigma \mu}{\kappa} - \tau
%        \end{array} \right].
%\end{eqnarray}
%Note that the matrix on the left-hand side of (\ref{matrixvectorsystem}) is invertible, and hence $(\Delta X, \Delta Y, \Delta \tau, \Delta \kappa)$ in (\ref{matrixvectorsystem}) can be uniquely determined.

The infeasible predictor-corrector path-following interior point algorithm
 on the homogeneous feasibility model (\ref{homogeneous1LSDFP}) - (\ref{homogeneous4LSDFP})  is described as follows:
\begin{algorithm}\label{mainalgorithm} (See Algorithm 5.1 of \cite{Potra})
Given $\beta_1 < \beta_2$ with $\beta_2^2/(2(1-\beta_2)^2) \leq
\beta_1 < \beta_2 < \beta_2/(1-\beta_2) < 1$.  Choose $(X_0,y_0,Y_0,
\tau_0,\kappa_0) \in \mathcal{N}(\beta_1, \mu_0)$ with $(n+1) \mu_0
= {\rm{Tr}}(X_0 Y_0) + \tau_0 \kappa_0$. For $k = 0, 1, \ldots$,
{\textcolor{black}{perform}} ($a1$) through ($a5$):
\begin{description}
\item[\hspace{5pt} ($a1$)] Set $X = X_k$, $y = y_k$, $Y = Y_k$, $\tau =
\tau_k$, $\kappa = \kappa_k$, and define
\begin{eqnarray*}
r_i & := & {\rm{Tr}}(A_i X) - b_i \tau, \quad i = 1, \ldots, m, \\
s & := & \sum_{i=1}^m y_i A_i + Y, \\
\gamma & := & \kappa - b^T y.
\end{eqnarray*}
\item[\hspace{5pt} ($a2$)] If $\max \{({\rm{Tr}}(XY) + \tau
\kappa)/\tau^2 , | r_1/\tau |, \ldots, | r_m /\tau |, \| s/\tau \|  \} \leq \epsilon$, then
report $(X/\tau, y/\tau, Y/\tau)$ as an approximate solution to the LSDFP, and terminate.
If $\tau$ is sufficiently small, terminate with no optimal solutions
to the LSDFP.
\item[\hspace{5pt} ($a3$)] {\bf{[Predictor Step]}} Find the solution $(\Delta X_p,\Delta y_p, \Delta Y_p,\Delta \tau_p,
\Delta \kappa_p)$ of the linear system (\ref{sys1})-(\ref{sys5}), with
$\sigma = 0$, $\overline{r}_i = r_i, i = 1, \ldots, m$, $\overline{s} = s$ and $\overline{\gamma} = \gamma$. \newline
Define
\begin{eqnarray*}
\overline{X} = X + \overline{\alpha} \Delta X_p,\ \ \overline{y} = y + \overline{\alpha} \Delta y_p, \ \ \overline{Y} = Y +
\overline{\alpha} \Delta Y_p,\ \  \overline{\tau} = \tau +
\overline{\alpha}\Delta \tau_p,\ \ \overline{\kappa} = \kappa +
\overline{\alpha}\Delta \kappa_p,
\end{eqnarray*}
\noindent where the {\textcolor{black}{step length}} $\overline{\alpha}$ satisfies
\begin{eqnarray}\label{steplengthinequality}
\alpha_1 \leq \overline{\alpha} \leq \alpha_2.
\end{eqnarray}
Here,
\begin{eqnarray}
\alpha_1 & = & \frac{2}{\sqrt{1 + 4 \delta/(\beta_2 - \beta_1)} +1},
\label{steplengthinequality1}\\
\delta & = & \frac{1}{\mu} \left\| \left[ \begin{array}{cc}
                                                Y & 0 \\
                                                0 & \kappa
                                            \end{array}\right]^{1/2}
\left[ \begin{array}{cc}
            \Delta X_p & 0  \\
            0 & \Delta \tau_p
        \end{array} \right]
\left[ \begin{array}{cc}
          \Delta Y_p   & 0 \\
            0 & \Delta \kappa_p 
        \end{array} \right]
\left[ \begin{array}{cc}
           Y    & 0 \\
            0 & \kappa 
        \end{array} \right]^{-1/2} \right\|_F,
        \label{steplengthinequality2}
\end{eqnarray}
where 
\begin{eqnarray*}
\mu = \frac{{\rm{Tr}}(XY) + \tau \kappa}{n + 1},
\end{eqnarray*}
and
\begin{eqnarray*}
& & \alpha_2 = \max \{ \tilde{\alpha} \in [0,1] \ ; \ (X+ \alpha \Delta X_p, y + \alpha \Delta y_p,  Y +
\alpha \Delta Y_p, \tau + \alpha \Delta \tau_p, \kappa + \alpha \Delta \kappa_p)
\\ 
& & \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \in \mathcal{N}(\beta_2,(1-\alpha)\mu)\ \forall\ \alpha \in
[0,\tilde{\alpha}]\}.
\end{eqnarray*}
%If $\overline{\alpha} = 1$, then $\overline{X}$ and $\overline{Y}$
%solve (\ref{primalSDP}) and (\ref{dualSDP}) respectively, and
%terminate.
\item[\hspace{5pt} ($a4$)] {\bf{[Corrector Step]}} Find the solution $(\Delta X_c, \Delta y_c, \Delta Y_c, \Delta \tau_c, \Delta \kappa_c)$ of the
linear system (\ref{sys1})-(\ref{sys5}), with $\sigma = 1 -
\overline{\alpha}$, $\overline{r}_i = 0, i = 1, \ldots, m$, $\overline{s} = 0$ and $\overline{\gamma} = 0$.  Set
\begin{eqnarray*}
\begin{array}{c}
X_+ = \overline{X} + \Delta X_c,\ \ y_+ = \overline{y} + \Delta y_c,\ \ Y_+ = \overline{Y} + \Delta Y_c, \ \ \tau_+ =
\overline{\tau} + \Delta \tau_c,\ \ \kappa_+ = \overline{\kappa} +
\Delta \kappa_c, \\
\mu_+ = (1 - \overline{\alpha})\mu.
\end{array}
\end{eqnarray*}
\item[\hspace{5pt} ($a5$)] Set
\begin{eqnarray*}
\begin{array}{c}
X_{k+1} = X_+,\ \ y_{k+1} = y_+, \ \ Y_{k+1} = Y_+, \ \ \tau_{k+1} = \tau_+, \ \
\kappa_{k+1} = \kappa_+,\\
\mu_{k+1} = \mu_+.
\end{array}
\end{eqnarray*}
\end{description}
\end{algorithm}

Polynomial iteration complexity for a general version of Algorithm \ref{mainalgorithm} used to solve (\ref{homogeneous1})-(\ref{homogeneous4}) is shown in \cite{Potra} where the HKM direction is considered.  The result there also holds for the dual HKM direction.  Hence, Algorithm \ref{mainalgorithm} has polynomial iteration complexity when it is used to solve (\ref{homogeneous1LSDFP})-(\ref{homogeneous4LSDFP}).   Furthermore, we remark that Algorithm \ref{mainalgorithm} can easily be adapted to solve the LSDFP instead of its homogeneous feasibility model.  An advantage of applying the algorithm on its homogeneous feasibility model is that we have guaranteed superlinear convergence of iterates generated by the algorithm, as shown in Theorem \ref{thm:superlinearconvergence}.

For $k = 0, 1, \ldots$, define
\begin{eqnarray*}
(r_k)_i & := & {\rm{Tr}}(A_i X_k) - b_i \tau_k, \quad i = 1, \ldots, m, \\
s_k & := & \sum_{i=1}^m (y_k)_i A_i + Y_k, \\
\gamma_k & := & \kappa_k - b^T y_k.
\end{eqnarray*}


\begin{remark}\label{rem:Remark1}
For all $k \geq 0$, we have
\begin{eqnarray*}
(X_k, y_k, Y_k, \tau_k, \kappa_k) \in \mathcal{N}(\beta_1, \mu_k).
\end{eqnarray*}
\end{remark}
Remark \ref{rem:Remark1} holds by \cite{Potra} - see also \cite{Potra1,Sim1}.


\begin{remark}\label{rem:initialiterate}
Throughout this paper, we consider Algorithm \ref{mainalgorithm} with initial iterate $(X_0, y_0, Y_0,$ $\tau_0, \kappa_0) \in \mathcal{N}(\beta_1, \mu_0)$ such that $(y_0, Y_0) \in \Re^m \times S^n_{++}$ is feasible to the dual SDP (\ref{dualSDP}) with $C = 0$.  Therefore, in the algorithm, we have $s_0 = 0$ while $(r_0)_i , i =1, \ldots, m$, and $\gamma_0$ are generally nonzero.
\end{remark}

%The following theorem provides global convergence and polynomial iteration complexity of Algorithm \ref{mainalgorithm}.  It is taken from \cite{Potra} which considers the HKM search direction, but the results hold for the dual HKM search direction as well.  It is stated here for the sake of completeness.   
%\begin{theorem}\label{complexitytheorem} (See Theorem 5.2 of
%\cite{Potra})  Assume that in Algorithm \ref{mainalgorithm}, we
%choose a starting point of the form $(X_0,y_0,Y_0,\tau_0,\kappa_0) = (I,0,
%I, 1, 1)$. Let
%\begin{eqnarray*}
%\epsilon_0 = \max \{ {\rm{Tr}}(X_0 Y_0) + \tau_0 \kappa_0,
%|(r_0)_1 |, \ldots, | (r_0)_m |, \|s_0\| \},
%\end{eqnarray*}
%\noindent and let $\epsilon > 0$ be arbitrary.  Then the following
%statements hold:
%\begin{description}
%\item[\hspace{5pt} (i)] If there exists an optimal solution to
%(\ref{primalSDP}) and (\ref{dualSDP}) with zero duality gap, then
%Algorithm \ref{mainalgorithm} terminates with an
%$\epsilon$-approximate solution $(X_k/\tau_k,y_k/\tau_k,Y_k/\tau_k) \in S^n_+
%\times S^n_+$ with
%\begin{eqnarray*}
%0 \leq {\rm{Tr}}(X_k Y_k/\tau_k^2) \leq \epsilon,\ \
%|(r_k)_i/\tau_k| \leq \epsilon, i = 1, \ldots, m,\ \ \|s_k/\tau_k\| \leq \epsilon
%\end{eqnarray*}
%in a finite number of steps $k = K_\epsilon < \infty$.
%\item[\hspace{5pt} (ii)] If $\rho^\ast = {\rm{Tr}}(X^\ast +
%Y^\ast)$, where $X^\ast$ solves (\ref{primalSDP}) and $(y^\ast, Y^\ast)$ solves (\ref{dualSDP}) with zero duality gap, then $K_\epsilon
%= \mathcal{O}(\sqrt{n} \ln (\rho^\ast \epsilon_0/\epsilon))$.
%\item[\hspace{5pt} (iii)] For any choice of $\rho > 0$, there is an
%index $k = \hat{K}_\epsilon = \mathcal{O}(\sqrt{n}\ln (\rho
%\epsilon_0/\epsilon))$ such that either
%    \begin{description}
%    \item[(iiia)] $(X_k/\tau_k, y_k/\tau_k, Y_k/\tau_k)$ satisfies $0 \leq
%    {\rm{Tr}}(X_kY_k/\tau_k^2) \leq \epsilon, |(r_k)_i/\tau_k | \leq
%    \epsilon, i = 1, \ldots, m,$ $\|s_k / \tau_k \| \leq \epsilon$, or
%    \item[(iiib)] $\tau_k < ( 1- \beta_1)/(\rho + 1)$, and in this case there are no solutions $X^\ast, (y^\ast, Y^\ast)$ that solve
%    (\ref{primalSDP}) and (\ref{dualSDP}) respectively, with ${\rm{Tr}}(X^\ast + Y^\ast) \leq \rho$
%    \end{description}
%\end{description}
%\end{theorem}

%It is easy to convince ourselves that Theorem \ref{complexitytheorem} still holds if $X_0, Y_0 \in S^n_{++}, y_0 \in \Re^m, \tau_0 > 0$ and $\kappa_0 > 0$, with $(X_0, y_0, Y_0, \tau_0, \kappa_0) \in \mathcal{N}(\beta_1, \mu_0)$.


{\textcolor{black}{Algorithm \ref{mainalgorithm} can be written in an equivalent way as Algorithm \ref{mainalgorithm2}, which we present below.  We use Algorithm \ref{mainalgorithm2} to solve the SDLCP representation of the homogeneous feasibility model, (\ref{SDLCP1})-(\ref{SDLCP3}), where $n_1 = n + 1$, $q = 0$ and $\mathcal{A}_1, \mathcal{B}_1$ are given  by (\ref{def:A1}), (\ref{def:B1}) respectively.}}  Before describing the algorithm,  we first define  an analogous (narrow) neighborhood of the central path of the SDLCP representation:
%\begin{eqnarray*}
%\mathcal{N}_1(\beta, \mu^1) & := & \{ (X^1, Y^1) \in S^{n_1}_{++} \times S^{n_1}_{++}\ ; \ \| (Y^1)^{1/2} X^1 (Y^1)^{1/2} - \mu^1 I \|_F \leq \beta \mu^1, \\
%&  &  \ \ \ \ \mu^1 = {\rm{Tr}}(X^1 Y^1)/n_1 \}.
%\end{eqnarray*}
\begin{eqnarray*}
\mathcal{N}_1(\beta, \mu^1)  :=  \{ (X^1, Y^1) \in S^{n_1}_{++} \times S^{n_1}_{++}\ ; \ \| (Y^1)^{1/2} X^1 (Y^1)^{1/2} - \mu^1 I \|_F \leq \beta \mu^1,  \mu^1 = {\rm{Tr}}(X^1 Y^1)/n_1 \}.
\end{eqnarray*}
{\textcolor{black}{We again have a system of equations for $(\Delta X^1, \Delta Y^1) \in S^{n_1} \times S^{n_1}$ that plays an important role in the algorithm, just like the system of equations (\ref{sys1})-(\ref{sys5}) for Algorithm \ref{mainalgorithm}:}}
\begin{eqnarray}
&  (Y^1)^{1/2}(X^1 \Delta Y^1 + \Delta X^1 Y^1)(Y^1)^{-1/2} + (Y^1)^{-1/2}(\Delta Y^1 X^1 + Y^1 \Delta X^1)(Y^1)^{1/2} \nonumber \\ 
&   = 2(\sigma \mu^1 I - (Y^1)^{1/2} X^1 (Y^1)^{1/2}), \label{eq:sys1prime} \\
&  \mathcal{A}_1(\Delta X^1) + \mathcal{B}_1(\Delta Y^1) = - \overline{r}. \label{eq:sys2prime}
\end{eqnarray}
{\textcolor{black}{Note that in Algorithm \ref{mainalgorithm2}, it can be shown that $(\Delta X^1, \Delta Y^1)$ obtained by solving (\ref{eq:sys1prime}), (\ref{eq:sys2prime}) always exists and is unique \cite{Potra1, Sim1}.}}

{\textcolor{black}{Below, we describe the algorithm:}}
\begin{algorithm}\label{mainalgorithm2} (See Algorithm 4.1 of \cite{Sim1}; Algorithm 2.1 of \cite{Potra1})
Given $\beta_1 < \beta_2$ with $\beta_2^2/(2(1-\beta_2)^2) \leq
\beta_1 < \beta_2 < \beta_2/(1-\beta_2) < 1$.  Choose $(X_0^1,Y_0^1) \in S^{n_1}_{++} \times S^{n_1}_{++}$ such that 
\begin{eqnarray}\label{def:initialiterateSDLCP}
X^1_0 = \left[ \begin{array}{cc}
					X_0 & 0 \\
					0   & \tau_0
				\end{array} \right], \quad
Y^1_0 = \left[ \begin{array}{cc}
					Y_0 & 0 \\
					0 & \kappa_0
				\end{array} \right],
\end{eqnarray}
where $X_0,Y_0,\tau_0,\kappa_0$ are from the initial iterate in Algorithm \ref{mainalgorithm}. For $k = 0, 1, \ldots$,
{\textcolor{black}{perform}} ($a1$) through ($a5$):
\begin{description}
\item[\hspace{5pt} ($a1$)] Set $X^1 = X^1_k$, $Y^1 = Y^1_k$, and define
\begin{eqnarray*}
r := \mathcal{A}_1(X^1) + \mathcal{B}_1(Y^1).
\end{eqnarray*}
\item[\hspace{5pt} ($a2$)] If $\max \{{\rm{Tr}}(X^1Y^1)/(X^1)^2_{n_1,n_1} ,  \| r /X^1_{n_1,n_1} \|  \} \leq \epsilon$, then terminate with the solution $(X^1,Y^1)$.
If $X^1_{n_1,n_1}$ is sufficiently small, terminate with no optimal solutions to the LSDFP.
\item[\hspace{5pt} ($a3$)] {\bf{[Predictor Step]}} Find the solution $(\Delta X^1_p, \Delta Y^1_p)$ of the linear system (\ref{eq:sys1prime}), (\ref{eq:sys2prime}), with
$\sigma = 0$, $\overline{r} = r$. \newline
Define
\begin{eqnarray*}
\overline{X}^1 = X^1 + \overline{\alpha} \Delta X^1_p,\ \ \overline{Y}^1 = Y^1 +
\overline{\alpha} \Delta Y^1_p,
\end{eqnarray*}
\noindent where the {\textcolor{black}{step length}} $\overline{\alpha}$ satisfies
\begin{eqnarray}\label{steplengthinequalityprime}
\alpha_1 \leq \overline{\alpha} \leq \alpha_2.
\end{eqnarray}
Here,
\begin{eqnarray}
\alpha_1 & = & \frac{2}{\sqrt{1 + 4 \delta/(\beta_2 - \beta_1)} +1},
\label{steplengthinequality1prime}\\
\delta & = & \frac{1}{\mu^1} \| (Y^1)^{1/2} \Delta X_p^1 \Delta Y_p^1 (Y^1)^{-1/2} \|_F,
        \label{steplengthinequality2prime}
\end{eqnarray}
where 
\begin{eqnarray*}
\mu^1 = \frac{{\rm{Tr}}(X^1Y^1)}{n_1},
\end{eqnarray*}
and
\begin{eqnarray*}
\alpha_2 = \max \{ \tilde{\alpha} \in [0,1] \ ; \ (X^1+ \alpha \Delta X^1_p, Y^1 +
\alpha \Delta Y^1_p) \in \mathcal{N}_1(\beta_2,(1-\alpha)\mu^1)\ \forall\ \alpha \in
[0,\tilde{\alpha}]\}.
\end{eqnarray*}
%If $\overline{\alpha} = 1$, then $\overline{X}$ and $\overline{Y}$
%solve (\ref{primalSDP}) and (\ref{dualSDP}) respectively, and
%terminate.
\item[\hspace{5pt} ($a4$)] {\bf{[Corrector Step]}} Find the solution $(\Delta X^1_c, \Delta Y^1_c)$ of the
linear system (\ref{eq:sys1prime}), (\ref{eq:sys2prime}), with $\sigma = 1 -
\overline{\alpha}$ and $\overline{r} = 0$.  Set
\begin{eqnarray*}
\begin{array}{c}
X^1_+ = \overline{X}^1 + \Delta X^1_c,\ \ Y^1_+ = \overline{Y}^1 + \Delta Y^1_c, \\
\mu_+^1 = (1 - \overline{\alpha})\mu^1.
\end{array}
\end{eqnarray*}
\item[\hspace{5pt} ($a5$)] Set
\begin{eqnarray*}
\begin{array}{c}
X^1_{k+1} = X^1_+,\ \ Y^1_{k+1} = Y^1_+, \\
\mu_{k+1}^1 = \mu_+^1.
\end{array}
\end{eqnarray*}
\end{description}
\end{algorithm}

The following proposition relates the $k^{th}$ iterate in the two algorithms:
\begin{proposition}\label{prop:iteratesbothalgorithms}
For all $k \geq 0$,
\begin{eqnarray}\label{eq:kiterateSDLCP}
X^1_k = \left[ \begin{array}{cc}
					X_k & 0 \\
					0   & \tau_k
				\end{array} \right], \quad
Y^1_k = \left[ \begin{array}{cc}
					Y_k & 0 \\
					0 & \kappa_k
				\end{array} \right].
\end{eqnarray}
Consequently, $\mu_k^1 = \mu_k$ and $(X^1_k, Y^1_k) \in \mathcal{N}_1(\beta_1, \mu^1_k)$.
\end{proposition}
\begin{proof}
We show (\ref{eq:kiterateSDLCP}) holds by induction on $k \geq 0$.  It is clear that (\ref{eq:kiterateSDLCP}) holds when $k = 0$, by the choice of $X^1_0, Y^1_0$.  Suppose (\ref{eq:kiterateSDLCP}) holds for $k = k_0 \geq 0$.  Then, at the $(k_0 + 1)^{\rm{th}}$ iteration of Algorithm \ref{mainalgorithm2},  by comparing the system of equations (\ref{sys1})-(\ref{sys5}) and (\ref{eq:sys1prime}), (\ref{eq:sys2prime}), it can be seen easily that
\begin{eqnarray*}
\Delta X^1_p = \left[ \begin{array}{cc}
					\Delta X_p & 0 \\
					0   & \Delta \tau_p
				\end{array} \right], \quad
\Delta Y^1_p = \left[ \begin{array}{cc}
					\Delta Y_p & 0 \\
					0 & \Delta \kappa_p
				\end{array} \right]
\end{eqnarray*}
satisfy (\ref{eq:sys1prime}), (\ref{eq:sys2prime}) when $\sigma = 0, \overline{r} = r = \mathcal{A}_1(X^1) + \mathcal{B}_1(Y^1)$.  Furthermore, the {\textcolor{black}{step length}} $\overline{\alpha}$ in the $(k_0 + 1)^{\rm{th}}$ iteration of both algorithms are the same.  These lead to
\begin{eqnarray}\label{eq:overlineX1Y1withoverlineXY}
\overline{X}^1 = \left[ \begin{array}{cc}
					\overline{X} & 0 \\
					0   & \overline{\tau}
				\end{array} \right], \quad
\overline{Y}^1 = \left[ \begin{array}{cc}
					\overline{Y} & 0 \\
					0 & \overline{\kappa}
				\end{array} \right].
\end{eqnarray}
With (\ref{eq:overlineX1Y1withoverlineXY}), again comparing the system of equations (\ref{sys1})-(\ref{sys5}) and (\ref{eq:sys1prime}), (\ref{eq:sys2prime}), it is also easy to see that
\begin{eqnarray*}
\Delta X^1_c = \left[ \begin{array}{cc}
					\Delta X_c & 0 \\
					0   & \Delta \tau_c
				\end{array} \right], \quad
\Delta Y^1_c = \left[ \begin{array}{cc}
					\Delta Y_c & 0 \\
					0 & \Delta \kappa_c
				\end{array} \right]
\end{eqnarray*}
satisfy (\ref{eq:sys1prime}), (\ref{eq:sys2prime}) when $\sigma = 1 - \overline{\alpha}$ and $\overline{r} = 0$.  Hence, we conclude that (\ref{eq:kiterateSDLCP}) holds for $k = k_0 + 1$.  Therefore, by induction, (\ref{eq:kiterateSDLCP}) holds for all $k \geq 0$.  Furthermore, we have
\begin{eqnarray*}
\mu_k^1 = \frac{{\rm{Tr}}(X^1_k Y^1_k)}{n_1} = \frac{{\rm{Tr}}(X_kY_k) + \tau_k \kappa_k}{n + 1} = \mu_k.
\end{eqnarray*}
Finally, by (\ref{eq:kiterateSDLCP}), comparing the definition of the neighborhood $\mathcal{N}(\beta, \mu)$ and the neighborhood $\mathcal{N}_1(\beta, \mu^1)$, and that $\mu_k = \mu_k^1$, we see that since $(X_k, y_k, Y_k, \tau_k, \kappa_k) \in \mathcal{N}(\beta_1, \mu_k)$ (Remark \ref{rem:Remark1}), we have $(X_k^1,Y_k^1) \in \mathcal{N}_1(\beta_1,\mu_k^1)$. 
\end{proof}

%We remark that $(X_0, Y_0, \tau_0, \kappa_0)$ that forms the initial iterate to Algorithm \ref{mainalgorithm} satisfies (\ref{homogeneous1to3combined}) in its first $m$ equations, and (\ref{homogeneous4prime}).  In Algorithm \ref{mainalgorithm2}, the corresponding initial iterate is given by
%
%and it satisfies the first $m+n$ equations in (\ref{SDLCP1}), and (\ref{SDLCP3}).  The relationship between the search directions in Algorithm \ref{mainalgorithm} and that in Algorithm \ref{mainalgorithm2} is clear.

\subsection{Superlinear Convergence}\label{subsec:superlinearconvergence}

We show in this subsection that Algorithm \ref{mainalgorithm} applied to the homogeneous feasibility model when the initial iterate $(X_0,y_0,Y_0,\tau_0,\kappa_0)  \in \mathcal{N}(\beta_1, \mu_0)$ is such that  $(y_0,Y_0) \in \Re^m \times S^n_{++}$ is feasible to the dual SDP (\ref{dualSDP}) with $C = 0$ leads to superlinear convergence of iterates generated by the algorithm, besides polynomial iteration complexity.   
%We show this by looking at the corresponding algorithm, Algorithm \ref{mainalgorithm2}, to Algorithm \ref{mainalgorithm}, and then applying results in the literature. 
% For these results to hold, we need strict complementarity of SDLCP (\ref{SDLCP1})-(\ref{SDLCP3})\footnote{Strict complementarity of SDLCP (\ref{SDLCP1})-(\ref{SDLCP3}) means that it has a solution $(X^{1,\ast},Y^{1,\ast})$ such that $X^{1,\ast} + Y^{1,\ast} \in S^{n_1}_{++}$}, which holds for the SDLCP representation of the homogeneous feasibility model (\ref{homogeneous1to3combinedprime}), (\ref{homogeneous4prime}) (or equivalently (\ref{homogeneous1LSDFP})-(\ref{homogeneous4LSDFP}), and (\ref{homogeneous1to3combined}), (\ref{homogeneous4prime})). 
First, we state an additional assumption, Assumption \ref{ass:strictcomplementarity}, which is the assumption of strict complementarity, on the primal-dual SDP pair (\ref{primalSDP})-(\ref{dualSDP}) with $C = 0$ that is needed for this result to hold.   Note that strict complementarity assumption is generally considered the minimal requirement for superlinear convergence of interior point algorithms, as investigated for example in \cite{Monteiro8}.

%\begin{proposition}\label{prop:strictcomplementarity}
%The primal-dual SDP pair (\ref{primalSDP})-(\ref{dualSDP}) with $C = 0$ has a strict complementary optimal solution.
%\end{proposition}
%\begin{proof}
%
%
%\end{proof}




%We do this by first observing that the homogeneous feasibility model is equivalent to a semi-definite linear complementarity problem , which we describe below.  We can then apply results, as found in the literature, on superlinear convergence using interior point algorithms on a semi-definite linear complementarity problem to show that Algorithm \ref{mainalgorithm} is indeed superlinearly convergent.

\begin{assumption}\label{ass:strictcomplementarity}
There exists an optimal solution $(X^\ast, y^\ast, Y^\ast)$ to the primal-dual SDP pair (\ref{primalSDP})-(\ref{dualSDP}) with $C = 0$ such that $X^\ast + Y^\ast \in S^n_{++}$.
\end{assumption}

A consequence of the above assumption on the homogeneous feasibility model (\ref{homogeneous1LSDFP})-(\ref{homogeneous4LSDFP}) is that it has a solution $(X^\ast, y^\ast, Y^\ast, \tau^\ast, \kappa^\ast)$ with $X^\ast + Y^\ast \in S^n_{++}$ and $\tau^\ast + \kappa^\ast > 0$.  This then implies that its SDLCP representation has a solution $(X^{1,\ast},Y^{1,\ast})$ such that $X^{1,\ast} + Y^{1,\ast} \in S^{n_1}_{++}$, that is, the SDLCP representation has a strictly complementary solution.  

We consider local superlinear convergence using Algorithm \ref{mainalgorithm} in the sense of
\begin{eqnarray}\label{def:superlinearconvergence}
\frac{\mu_{k+1}}{\mu_k} \rightarrow 0, \ \ {\rm{as}}\ k \rightarrow \infty.
\end{eqnarray}
Consideration of superlinear convergence in the form (\ref{def:superlinearconvergence}) is typical in the interior point literature, such as \cite{Kojima,Luo,Potra1}.  It is closely related to local convergence behavior of iterates, as studied for example in \cite{Potrasingle}.

%Observe that since $X^{1,\ast}, Y^{1,\ast}$ commute, they are jointly diagonalizable by some orthogonal matrix $Q$.  So using this orthogonal similarity transformation on the matrices in the SDLCP representation, we may assume without loss of generality that
%\begin{eqnarray*}
%X^{1,\ast} = \left[ \begin{array}{cc}
%					\Gamma^{X} & 0 \\
%					0  &  0
%					\end{array} \right], \quad
%Y^{1,\ast} = \left[ \begin{array}{cc}
%					0  &  0 \\
%					0  &  \Gamma^Y
%					\end{array} \right],
%\end{eqnarray*}
%where $\Gamma^X = {\rm{Diag}}(\lambda_1^X, \ldots, \lambda_{k_0}^X) \in S^{k_0}_{++}$ and $\Gamma^Y = {\rm{Diag}}(\lambda_1^Y, \ldots, \lambda_{n_1 - k_0}^Y) \in S^{n_1 - k_0}_{++}$, by Assumption \ref{ass:strictcomplementarity}.
%
%Henceforth, whenever we partition a matrix $U \in S^{n_1}$, it is always understood that it is partitioned as $\left[ \begin{array}{cc}
%												U_{11}  &   U_{12}  \\
%												U_{12}^T & U_{22}
%											\end{array} \right]$, where $U_{11} \in S^{k_0}$, $U_{22} \in S^{n_1 - k_0}$ and $U_{12} \in \Re^{k_0 \times (n_1 - k_0)}$.
%
%Since the SDLCP representation satisfies Assumption \ref{ass:strictcomplementarity} in addition to Assumption \ref{ass:SDLCPassumptions} (Proposition \ref{prop:SDLCPAssumption}), we have
%\begin{eqnarray}\label{eq:X_kY_k_Order}
%X^1_k = \left[ \begin{array}{cc}
%				\Theta(1) & O(\sqrt{\mu_k}) \\
%				O(\sqrt{\mu_k}) & \Theta(\mu_k)
%				\end{array} \right], \quad 
%Y^1_k = \left[ \begin{array}{cc}
%				\Theta(\mu_k) & O(\sqrt{\mu_k}) \\
%				O(\sqrt{\mu_k}) & \Theta(1)
%				\end{array} \right].
%\end{eqnarray}
%The proof for (\ref{eq:X_kY_k_Order}) can be found for example in \cite{Preib,Sim2}.

The following result, which is the main result in this paper, ends this subsection:
\begin{theorem}\label{thm:superlinearconvergence}
Given an initial iterate $(X_0,y_0,Y_0,\tau_0,\kappa_0) \in \mathcal{N}(\beta_1, \mu_0)$ to Algorithm \ref{mainalgorithm}, with  $(y_0,Y_0) \in \Re^m \times S^n_{++}$ feasible to dual SDP (\ref{dualSDP}) with $C = 0$.  Iterates generated by Algorithm \ref{mainalgorithm} converge superlinearly in the sense of (\ref{def:superlinearconvergence}).
\end{theorem}
\begin{proof}
Let us show the result in the theorem by considering iterates generated by Algorithm \ref{mainalgorithm2} instead.  These iterates are related to those generated by Algorithm \ref{mainalgorithm} in a close way as shown in Proposition \ref{prop:iteratesbothalgorithms}.  We note that Algorithm \ref{mainalgorithm2} is Algorithm 4.1 in \cite{Sim1}, and Assumptions 2.1 and 3.1 in \cite{Sim1} are satisfied for the SDLCP representation of the homogeneous feasibility model (\ref{homogeneous1LSDFP})-(\ref{homogeneous4LSDFP}) (Proposition \ref{prop:SDLCPAssumption} and strict complementarity).  Hence, results in \cite{Sim1} are applicable to our SDLCP representation.  The SDLCP representation that Algorithm \ref{mainalgorithm2} is solving has the structure of an LSDFP (Remark \ref{rem:LSDFP}), and therefore Theorem 5.1\footnote{\textcolor{black}{This theorem is reproduced in our context as Theorem \ref{thm:TheoremAnotherPaper} in Appendix \ref{sec:Appendix}.}} in \cite{Sim1} can be applied on our SDLCP representation provided that  Condition (52) in the theorem {\textcolor{black}{((\ref{eq:condition2}) in Appendix \ref{sec:Appendix})}} is satisfied.  

\noindent  Our choice of initial iterate $(X_0,y_0,Y_0,\tau_0,\kappa_0)$ to Algorithm \ref{mainalgorithm} leads to an initial iterate, $(X^1_0, Y^1_0)$, to Algorithm \ref{mainalgorithm2} that satisfies $\mathcal{B}_1(Y^1_0) = 0$ except possibly the $(m+n+1)^{th}$ entry\footnote{Remark \ref{rem:homogeneousmodel}  {\textcolor{black}{and Appendix \ref{sec:Appendix}}}.} of $\mathcal{B}_1(Y^1_0)$.  Therefore, Condition (52) of Theorem 5.1 in \cite{Sim1} is satisfied, and by the theorem, we have superlinear convergence in the sense that
\begin{eqnarray*}\label{larrow:convergence}
\frac{\mu^1_{k+1}}{\mu^1_k} \rightarrow 0, \ \ {\rm{as}}\ k \rightarrow \infty.
\end{eqnarray*}
This implies by Proposition \ref{prop:iteratesbothalgorithms}, where we have $\mu_k^1 = \mu_k$, superlinear convergence in the sense of (\ref{def:superlinearconvergence}) using Algorithm \ref{mainalgorithm} to solve the homogeneous feasibility model (\ref{homogeneous1LSDFP})-(\ref{homogeneous4LSDFP}) for the given initial iterate.
\end{proof}

%\vspace{10pt}
%
%We remark that the conclusion in Theorem \ref{thm:superlinearconvergence} still holds if $(y_0,Y_0)$ is a strictly feasible solution to dual SDP (\ref{dualSDP}) with $C = 0$, and $\tau_0 = 1$ without the need for $X_0$ to be the identity matrix and $\kappa_0 = 1$.  However, we require $X_0 \in S^n_{++}$ and $\kappa_0 > 0$.

\begin{remark}\label{rem:NT}
Similar result as Theorem \ref{thm:superlinearconvergence} also holds when the HKM search direction or the NT search direction is used in Algorithm \ref{mainalgorithm} instead of the dual HKM search direction.  The equivalent algorithm on the SDLCP representation for the NT search direction is Algorithm 1 in \cite{Sim6}.  We can then apply Theorem 4 or Corollary 1 in \cite{Sim6} to conclude superlinear convergence of iterates when the initial iterate to Algorithm \ref{mainalgorithm} with the NT search direction {\textcolor{black}{is}} from a strictly feasible solution to the dual SDP (\ref{dualSDP}) with $C = 0$.  The process to show this is analogous to what we have discussed and we will not repeat it here again.
\end{remark}


%\section{Numerical Experiments}\label{sec:numerical}



\section{Conclusion}\label{sec:conclusion}

In this paper, we show superlinear convergence of an implementable polynomial-time infeasible predictor-corrector primal-dual path following interior point algorithm (Algorithm \ref{mainalgorithm}) on the homogeneous feasibility model of an LSDFP for a suitable choice of initial iterate to the algorithm. {\textcolor{black}{This initial iterate can be obtained efficiently and fast by solving a primal-dual SDP pair using a primal-dual path following interior point algorithm.}}

%\section*{Acknowledgement}\label{acknowledgement}
%
%We would like to thank the senior editor, Prof. Dur, and the associate editor for handling the paper and the two anonymous reviewers for providing valuable comments to improve the paper.  

\bibliographystyle{plain}
\bibliography{Reference_Sim}

\appendix
\section{Appendix}\label{sec:Appendix}

{\textcolor{black}{For the sake of completeness, let us write down Theorem 5.1 in \cite{Sim1} in our context:
\begin{theorem}\label{thm:TheoremAnotherPaper}(see Theorem 5.1 in \cite{Sim1})
Assume that there exists a solution $(X^{1,\ast},Y^{1,\ast}) \in S^{n_1}_+ \times S^{n_1}_+$ to an LSDFP (expressed as SDLCP (\ref{SDLCP1})-(\ref{SDLCP3})) such that $X^{1,\ast} + Y^{1,\ast} \in S^{n_1}_{++}$.  Let $(X^1_k,Y^1_k)$ be iterates generated by Algorithm \ref{mainalgorithm2} on the LSDFP with the initial iterate $(X^1_0,Y^1_0)$ satisfying
\begin{eqnarray}\label{eq:condition1}
\left( \begin{array}{l}
{\rm{svec}}\left( \begin{array}{ccc}
				0 & 0  & 0 \\
				0 & ((\mathcal{A}_1)_{j_1+j_2+1})_{22} & 0 \\
				0 & 0 & 0
				\end{array} \right)^T \\
				\vdots \\
{\rm{svec}}\left( \begin{array}{ccc}
				0 & 0 & 0 \\
				0 & ((\mathcal{A}_1)_{m_1})_{22} & 0 \\
				0 & 0 & 0 
				\end{array} \right)^T
\end{array} \right) {\rm{svec}}(X_0^1) = 0 
\end{eqnarray}
or
\begin{eqnarray}\label{eq:condition2}
\left( \begin{array}{l}
{\rm{svec}}\left( \begin{array}{ccc}
				((\mathcal{B}_1)_{k_1+k_2+1})_{11} & 0  & ((\mathcal{B}_1)_{k_1+k_2+1})_{13} \\
				0 & 0 & 0 \\
				((\mathcal{B}_1)_{k_1+k_2+1})_{13}^T & 0 & ((\mathcal{B}_1)_{k_1+k_2+1})_{33} 
				\end{array} \right)^T \\
				\vdots \\
{\rm{svec}}\left( \begin{array}{ccc}
				((\mathcal{B}_1)_{\tilde{n}_1-m_1})_{11}  & 0 & ((\mathcal{B}_1)_{\tilde{n}_1-m_1})_{13}\\
				0 & 0 & 0 \\
				((\mathcal{B}_1)_{\tilde{n}_1-m_1})_{13}^T & 0 & ((\mathcal{B}_1)_{\tilde{n}_1-m_1})_{33}
				\end{array} \right)^T
\end{array} \right) {\rm{svec}}(Y_0^1) = 0, 
\end{eqnarray}				
where $q_i = 0$ for $i = m_1 + 1, \ldots, \tilde{n}_1$  in (\ref{SDLCP1}) when (\ref{eq:condition1}) holds, and $q_i = 0$ for  $i = 1, \ldots, m_1$ in (\ref{SDLCP1}) when (\ref{eq:condition2}) holds.  Here, $\tilde{n}_1 = n_1(n_1 + 1)/2$.  Then the iterates converge superlinearly.
\end{theorem}}}

{\textcolor{black}{To put things in perspective, under the assumption that  there exists a solution $(X^{1,\ast},Y^{1,\ast}) \in S^{n_1}_+ \times S^{n_1}_+$ to the LSDFP (expressed as SDLCP (\ref{SDLCP1})-(\ref{SDLCP3})) such that $X^{1,\ast} + Y^{1,\ast} \in S^{n_1}_{++}$, we can assume that $X^{1,\ast}$ and $Y^{1\ast}$ are given by
\begin{eqnarray*}
X^{1,\ast} = \left( \begin{array}{ccc}
				\Lambda^\ast_{11} & 0 & 0 \\
				0 & 0 & 0 \\
				0 & 0 & \lambda_{k_0}^\ast
				\end{array} \right), \quad  Y^{1,\ast} = \left( \begin{array}{ccc}
				0 & 0  & 0 \\
				0 & \Lambda^\ast_{22} & 0 \\
				0 & 0 & 0
				\end{array} \right),
\end{eqnarray*} }}
{\textcolor{black}{where $\Lambda^\ast_{11} = {\rm{Diag}}(\lambda_1^\ast, \ldots, \lambda^\ast_{k_0-1}) \in S^{k_0-1}_{++}$ and $\Lambda^\ast_{22} = {\rm{Diag}}(\lambda_{k_0+1}^\ast, \ldots, \lambda^\ast_{n_1}) \in S^{n_1 - k_0}_{++}$.  Here, $\lambda_1^\ast, \ldots, \lambda_{n_1}^\ast$ are real numbers greater than zero.  In this appendix, we always partition a matrix $S \in S^{n_1}$ in the way $X^{1,\ast}$ and $Y^{1,\ast}$ are partitioned, i.e., $S$ is partitioned as $\left(\begin{array}{ccc}
					S_{11} & S_{12} & S_{13} \\
					S_{12}^T & S_{22} & S_{23} \\
					S_{13}^T & S_{23}^T & S_{33}
					\end{array} \right)$, where $S_{11} \in S^{k_0-1}, S_{22} \in S^{n_1 - k_0}, S_{33} \in \Re$ and $S_{12} \in \Re^{(k_0 - 1)\times (n_1 - k_0)}, S_{13} \in \Re^{k_0-1}, S_{23} \in \Re^{n_1 - k_0}$.   Using this partition, we perform block Gaussian eliminations and row operations on $\mathcal{A}_1, \mathcal{B}_1: S^{n_1} \rightarrow \Re^{\tilde{n}_1}$ in (\ref{SDLCP1}) so that they can be written in matrix form as:
\begin{eqnarray*}
\mathcal{A}_1 = \left( \begin{array}{c}
						\bar{\mathcal{A}}_1 \\
						0
					\end{array} \right), \quad \mathcal{B}_1 = \left( \begin{array}{c}
																		0 \\
																	\bar{\mathcal{B}}_1
																	\end{array} \right),
\end{eqnarray*}}}
{\textcolor{black}{where
\begin{eqnarray*}
\bar{\mathcal{A}}_1 = \left( \begin{array}{l}
{\rm{svec}}\left( \begin{array}{ccc}
				((\mathcal{A}_1)_1)_{11} & ((\mathcal{A}_1)_1)_{12} & ((\mathcal{A}_1)_1)_{13} \\
				((\mathcal{A}_1)_1)_{12}^T & ((\mathcal{A}_1)_1)_{22} & ((\mathcal{A}_1)_1)_{23} \\
				((\mathcal{A}_1)_1)_{13}^T & ((\mathcal{A}_1)_1)_{23}^T & ((\mathcal{A}_1)_1)_{33}
				\end{array} \right) \\
				\vdots \\
{\rm{svec}}\left( \begin{array}{ccc}
				((\mathcal{A}_1)_{j_1})_{11} & ((\mathcal{A}_1)_{j_1})_{12} & ((\mathcal{A}_1)_{j_1})_{13} \\
				((\mathcal{A}_1)_{j_1})_{12}^T & ((\mathcal{A}_1)_{j_1})_{22} & ((\mathcal{A}_1)_{j_1})_{23} \\
				((\mathcal{A}_1)_{j_1})_{13}^T & ((\mathcal{A}_1)_{j_1})_{23}^T & ((\mathcal{A}_1)_{j_1})_{33}
				\end{array} \right) \\
{\rm{svec}}\left( \begin{array}{ccc}
				0 & ((\mathcal{A}_1)_{j_1+1})_{12} & 0 \\
				((\mathcal{A}_1)_{j_1+1})_{12}^T & ((\mathcal{A}_1)_{j_1+1})_{22} & ((\mathcal{A}_1)_{j_1+1})_{23} \\
				0 & ((\mathcal{A}_1)_{j_1+1})_{23}^T & 0
				\end{array} \right) \\
				\vdots \\
{\rm{svec}}\left( \begin{array}{ccc}
				0 & ((\mathcal{A}_1)_{j_1+j_2})_{12} & 0 \\
				((\mathcal{A}_1)_{j_1+j_2})_{12}^T & ((\mathcal{A}_1)_{j_1+j_2})_{22} & ((\mathcal{A}_1)_{j_1+j_2})_{23} \\
				0 & ((\mathcal{A}_1)_{j_1+j_2})_{23}^T & 0
				\end{array} \right) \\
{\rm{svec}}\left( \begin{array}{ccc}
				0 & 0 & 0 \\
				0 & ((\mathcal{A}_1)_{j_1+j_2+1})_{22} & 0 \\
				0 & 0 & 0
				\end{array} \right) \\
\vdots \\
{\rm{svec}}\left( \begin{array}{ccc}
				0 & 0 & 0 \\
				0 & ((\mathcal{A}_1)_{m_1})_{22} & 0 \\
				0 & 0 & 0
				\end{array} \right)
\end{array} \right)
\end{eqnarray*}}}
{\textcolor{black}{and
\begin{eqnarray*}
\bar{\mathcal{B}}_1 = \left( \begin{array}{l}
{\rm{svec}}\left( \begin{array}{ccc}
				((\mathcal{B}_1)_1)_{11} & ((\mathcal{B}_1)_1)_{12} & ((\mathcal{B}_1)_1)_{13} \\
				((\mathcal{B}_1)_1)_{12}^T & ((\mathcal{B}_1)_1)_{22} & ((\mathcal{B}_1)_1)_{23} \\
				((\mathcal{B}_1)_1)_{13}^T & ((\mathcal{B}_1)_1)_{23}^T & ((\mathcal{B}_1)_1)_{33}
				\end{array} \right) \\
				\vdots \\
{\rm{svec}}\left( \begin{array}{ccc}
				((\mathcal{B}_1)_{k_1})_{11} & ((\mathcal{B}_1)_{k_1})_{12} & ((\mathcal{B}_1)_{k_1})_{13} \\
				((\mathcal{B}_1)_{k_1})_{12}^T & ((\mathcal{B}_1)_{k_1})_{22} & ((\mathcal{B}_1)_{k_1})_{23} \\
				((\mathcal{B}_1)_{k_1})_{13}^T & ((\mathcal{B}_1)_{k_1})_{23}^T & ((\mathcal{B}_1)_{k_1})_{33}
				\end{array} \right) \\
{\rm{svec}}\left( \begin{array}{ccc}
				((\mathcal{B}_1)_{k_1+1})_{11} & ((\mathcal{B}_1)_{k_1+1})_{12} & ((\mathcal{B}_1)_{k_1+1})_{13} \\
				((\mathcal{B}_1)_{k_1+1})_{12}^T & 0 & ((\mathcal{B}_1)_{k_1+1})_{23} \\
				((\mathcal{B}_1)_{k_1+1})_{13}^T & ((\mathcal{B}_1)_{k_1+1})_{23}^T & ((\mathcal{B}_1)_{k_1+1})_{33}
				\end{array} \right) \\
				\vdots \\
{\rm{svec}}\left( \begin{array}{ccc}
				((\mathcal{B}_1)_{k_1+k_2})_{11} & ((\mathcal{B}_1)_{k_1+k_2})_{12} & ((\mathcal{B}_1)_{k_1+k_2})_{13} \\
				((\mathcal{B}_1)_{k_1+k_2})_{12}^T & 0 & ((\mathcal{B}_1)_{k_1+k_2})_{23} \\
				((\mathcal{B}_1)_{k_1+k_2})_{13}^T & ((\mathcal{B}_1)_{k_1+k_2})_{23}^T & ((\mathcal{B}_1)_{k_1+k_2})_{33}
				\end{array} \right) \\
{\rm{svec}}\left( \begin{array}{ccc}
				((\mathcal{B}_1)_{k_1+k_2+1})_{11} & 0 & ((\mathcal{B}_1)_{k_1+k_2+1})_{13} \\
				0 & 0 & 0 \\
				((\mathcal{B}_1)_{k_1+k_2+1})_{13}^T & 0 & ((\mathcal{B}_1)_{k_1+k_2+1})_{33}
				\end{array} \right) \\
\vdots \\
{\rm{svec}}\left( \begin{array}{ccc}
				((\mathcal{B}_1)_{\tilde{n}_1 - m_1})_{11} & 0 & ((\mathcal{B}_1)_{\tilde{n}_1 - m_1})_{13} \\
				0 & 0 & 0 \\
				((\mathcal{B}_1)_{\tilde{n}_1 - m_1})_{13}^T & 0 & ((\mathcal{B}_1)_{\tilde{n}_1 - m_1})_{33}
				\end{array} \right)
\end{array} \right).
\end{eqnarray*}}}
{\textcolor{black}{Details on the above reformulation of $\mathcal{A}_1$ and $\mathcal{B}_1$ can be found in \cite{Sim1,Sim6,Sim2}.}}

{\textcolor{black}{In the context of our SDLCP representation of the homogeneous feasibility model of the given LSDFP, we choose $B_1$ in (\ref{def:B}) such that $(B_1)_{22} \not= 0$.  In this way, when we perform block Gaussian eliminations and row operations on $\mathcal{B}_1$, its final position with respect to other rows in $\bar{\mathcal{B}}_1$ remains unchanged, that is, ${\rm{svec}}\left( \begin{array}{cc}
																	B_1 & 0 \\
																	0 & d_1 
																	\end{array} \right)^T$ remains in the first row of $\bar{\mathcal{B}}_1$ or the $(m_1 + 1)^{th}$ row of $\mathcal{B}_1$.  We see that (\ref{eq:condition2}) then holds when $Y^1_0$ is from a strictly feasible solution to the dual SDP (\ref{dualSDP}) with $C = 0$.}} 
																	
%In this way, as $q = 0$, in particular $q_i = 0, i = 1, \ldots, m_1$,  in (\ref{SDLCP1}) (see Remark \ref{rem:LSDFP}), we have  (\ref{eq:condition2}) holds when $Y_0^1$ is from a strictly feasible solution to the dual SDP (\ref{dualSDP}) with $C = 0$.}}
%
%It is not easy to determine whether there exists a strictly feasible solution to the dual SDP (\ref{dualSDP}) with $C = 0$.  In other words, it is not easy to determine whether there is a solution to the following (strict) LMI:
%\begin{eqnarray*}
%\sum_{j=1}^l z_j B_j \in S^n_{++}.
%\end{eqnarray*}
%
%The following proposition provides conditions - necessary and sufficient, and necessary - for when the dual SDP (\ref{dualSDP}) with $C = 0$ has a strictly feasible solution.
%\begin{proposition}\label{prop:necessarysufficient}
%We have the dual SDP (\ref{dualSDP}) with $C = 0$ has a strictly feasible solution if and only if given $Y \in S^n_+, Y \not= 0$, there are $y^0_i \in \Re, i = 1, \ldots, m$,  such that
%\begin{eqnarray*} 
%{\rm{Tr}}\left( \left(\sum_{i=1}^{m} y_i^0 A_i\right) Y \right) \not= 0
%\end{eqnarray*}
%if and only if given $Y \in S^n_+, Y \not= 0$, there exists $i$, where $i = 1, \ldots, m$, such that
%\begin{eqnarray*}
%{\rm{Tr}}(A_i Y) \not= 0.
%\end{eqnarray*}
%Moreover, we have if the dual SDP (\ref{dualSDP}) with $C = 0$ has a strictly feasible solution, then given $q \in \Re^n, q \not= 0$, there exists $i$, where $i = 1, \ldots, m$, such that
%\begin{eqnarray*}
%q^T A_i q \not= 0.
%\end{eqnarray*}
%\end{proposition}
%\begin{proof}
%Let 
%\begin{eqnarray*}
%\mathcal{S} := \left\{ \sum_{i=1}^m y_i A_i \in S^n \ ; \ y_i \in \Re, i = 1, \ldots, m \right\}.
%\end{eqnarray*}
%Then, $\mathcal{S} \cap S^n_{++} = \emptyset$ if and only if there exists $S \in S^n$, $S \not= 0$,  such that
%\begin{eqnarray*}
%{\rm{Tr}}\left( \left( \sum_{i=1}^m y_i A_i \right) S \right) = 0, \ \forall \ y_i \in \Re, i =1, \ldots, m,
%\end{eqnarray*}
%and 
%\begin{eqnarray*}
%{\rm{Tr}}(S Y) > 0, \ \forall\ Y \in S^n_{++}.
%\end{eqnarray*}
%In other words, we have $\mathcal{S} \cap S^n_{++} = \emptyset$  if and only if there exists $S \in S^n_+$, $S \not= 0$, such that
%\begin{eqnarray*}
%{\rm{Tr}}\left( \left( \sum_{i=1}^m y_i A_i \right) S \right) = 0, \ \forall \ y_i \in \Re, i =1, \ldots, m.
%\end{eqnarray*}
%The first ``if and only if'' in the proposition then follows by negation.  The second ``if and only if'' in the proposition is clear.  Now, let us prove the last claim in the proposition.  Since the dual SDP (\ref{dualSDP}) with $C = 0$ has a strictly feasible solution, then given $Y \in S^n_+, Y \not= 0$, there exists $i$, where $i = 1, \ldots, m$, such that
%\begin{eqnarray*}
%{\rm{Tr}}(A_i Y) \not= 0.
%\end{eqnarray*}
%Given $q \in \Re^n, q \not= 0$.   We have  $Y = qq^T \in S^n_+$ and $Y \not= 0$.  Therefore, there exists $i$, $i = 1, \ldots, m$, such that
%\begin{eqnarray*}
%{\rm{Tr}}(A_i Y) \not= 0.
%\end{eqnarray*}
%That is, $i$ is such that $q^T A_i q \not= 0$.
%
%%
%%\noindent ($\Leftarrow$) The proof is similar to the proof for ($\Rightarrow$), by assuming that there exists $q \in \Re^n, q \not= 0$, such that for all $i = 1, \ldots, m$, $q^T A_i q = 0$.  WLOG, we can assume that $\| q \| = 1$.  Form an orthogonal matrix $Q \in \Re^{n \times n}$ such that $q$ is its first column.  Let $Y = Q {\rm{Diag}}(1, 0, \ldots, 0) Q^T$.  Then, $Y \in S^n_+, Y \not= 0$.  We can see easily that for all $i = 1, \ldots, m$,
%%\begin{eqnarray*}
%%{\rm{Tr}}(A_i Y) = {\rm{Tr}}(Q^T A_i Q {\rm{Diag}}(1, 0, \ldots, 0)) = q^T A_i q = 0.
%%\end{eqnarray*}
%%\noindent ($\Leftarrow$) Suppose given $q \in \Re^n, q \not= 0$, there exists $i$, where $i = 1, \ldots, m$, such that
%%\begin{eqnarray*}
%%q^T A_i q \not= 0.
%%\end{eqnarray*}
%%Now, given $Y \in S^n_+, Y \not= 0$.  Then, there exists $Q = (q^1, \ldots, q^n)$, an orthogonal matrix, such that $Y = Q{\rm{Diag}}(d_1, \ldots, d_n)Q^T$, where $d_j \geq 0, j = 1, \ldots, n$, and not all $d_i$ is equal to zero.  We have
%%\begin{eqnarray*}
%%{\rm{Tr}}(A_i Y) = {\rm{Tr}}(Q^T A_i Q {\rm{Diag}}(d_1, \ldots, d_n)) = \sum_{j=1}^n d_j (q^j)^T A_i q^j.
%%\end{eqnarray*}
%%Suppose $d_{j_0} > 0$ for some $j_0 = 1, \ldots, n$.  Then
%\end{proof}
%
%\vspace{10pt}
%
%The above proposition leads to that if the dual SDP (\ref{dualSDP}) with $C = 0$ has a strictly feasible solution, then the following minimization problem
%\begin{eqnarray*}
%\min_{q \in \Re^n}  \sum_{i=1}^m (q^T A_i q)^2
%\end{eqnarray*}
%has $q = 0$ as its unique optimal solution, which further leads to 
%\begin{eqnarray}\label{Anoptimizationproblem}
%\min_{q \in \Re^n, \| q \| = 1} \sum_{i=1}^{m} (q^T A_i q)^2 > 0.
%\end{eqnarray}   
%
%Hence, if 
%\begin{eqnarray*}
%\min_{q \in \Re^n, \| q \| = 1} \sum_{i=1}^{m} (q^T A_i q)^2 = 0
%\end{eqnarray*} 
%we know that the dual SDP (\ref{dualSDP}) with $C = 0$ has no strictly feasible solutions, although this is unclear when (\ref{Anoptimizationproblem}) holds.



\end{document}



