@inproceedings{sabri-etal-2021-emopars-collection,
    title = "{E}mo{P}ars: A Collection of 30{K} Emotion-Annotated {P}ersian Social Media Texts",
    author = "Sabri, Nazanin  and
      Akhavan, Reyhane  and
      Bahrak, Behnam",
    booktitle = "Proceedings of the Student Research Workshop Associated with RANLP 2021",
    month = sep,
    year = "2021",
    address = "Online",
    publisher = "INCOMA Ltd.",
    url = "https://aclanthology.org/2021.ranlp-srw.23",
    pages = "167--173",
    abstract = "The wide reach of social media platforms, such as Twitter, have enabled many users to share their thoughts, opinions and emotions on various topics online. The ability to detect these emotions automatically would allow social scientists, as well as, businesses to better understand responses from nations and costumers. In this study we introduce a dataset of 30,000 Persian Tweets labeled with Ekman{'}s six basic emotions (Anger, Fear, Happiness, Sadness, Hatred, and Wonder). This is the first publicly available emotion dataset in the Persian language. In this paper, we explain the data collection and labeling scheme used for the creation of this dataset. We also analyze the created dataset, showing the different features and characteristics of the data. Among other things, we investigate co-occurrence of different emotions in the dataset, and the relationship between sentiment and emotion of textual instances. The dataset is publicly available at https://github.com/nazaninsbr/Persian-Emotion-Detection.",
}

@article{nandwani2021review,
  title={A review on sentiment analysis and emotion detection from text},
  author={Nandwani, Pansy and Verma, Rupali},
  journal={Social Network Analysis and Mining},
  volume={11},
  number={1},
  pages={1--19},
  year={2021},
  publisher={Springer}
}

@article{mirzaee2022armanemo,
  title={ArmanEmo: A Persian Dataset for Text-based Emotion Detection},
  author={Mirzaee, Hossein and Peymanfard, Javad and Moshtaghin, Hamid Habibzadeh and Zeinali, Hossein},
  journal={arXiv preprint arXiv:2207.11808},
  year={2022}
}

@incollection{fei2021machine,
  title={Machine and deep learning algorithms for wearable health monitoring},
  author={Fei, Chengwei and Liu, Rong and Li, Zihao and Wang, Tianmin and Baig, Faisal N},
  booktitle={Computational intelligence in healthcare},
  pages={105--160},
  year={2021},
  publisher={Springer}
}

@article{deng2020multi,
  title={Multi-label emotion detection via emotion-specified feature extraction and emotion correlation learning},
  author={Deng, Jiawen and Ren, Fuji},
  journal={IEEE Transactions on Affective Computing},
  year={2020},
  publisher={IEEE}
}

@inproceedings{bhowmick2009multi,
  title={Multi-label text classification approach for sentence level news emotion analysis},
  author={Bhowmick, Plaban Kr and Basu, Anupam and Mitra, Pabitra and Prasad, Abhishek},
  booktitle={International conference on pattern recognition and machine intelligence},
  pages={261--266},
  year={2009},
  organization={Springer}
}

@inproceedings{abaskohi-etal-2022-utnlp,
    title = "{UTNLP} at {S}em{E}val-2022 Task 6: A Comparative Analysis of Sarcasm Detection Using Generative-based and Mutation-based Data Augmentation",
    author = "Abaskohi, Amirhossein  and
      Rasouli, Arash  and
      Zeraati, Tanin  and
      Bahrak, Behnam",
    booktitle = "Proceedings of the 16th International Workshop on Semantic Evaluation (SemEval-2022)",
    month = jul,
    year = "2022",
    address = "Seattle, United States",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.semeval-1.135",
    doi = "10.18653/v1/2022.semeval-1.135",
    pages = "962--969",
    abstract = "Sarcasm is a term that refers to the use of words to mock, irritate, or amuse someone. It is commonly used on social media. The metaphorical and creative nature of sarcasm presents a significant difficulty for sentiment analysis systems based on affective computing. The methodology and results of our team, UTNLP, in the SemEval-2022 shared task 6 on sarcasm detection are presented in this paper. We put different models, and data augmentation approaches to the test and report on which one works best. The tests begin with traditional machine learning models and progress to transformer-based and attention-based models. We employed data augmentation based on data mutation and data generation. Using RoBERTa and mutation-based data augmentation, our best approach achieved an F1-score of 0.38 in the competition{'}s evaluation phase. After the competition, we fixed our model{'}s flaws and achieved anF1-score of 0.414.",
}

@article{aquino2017effect,
  title={The effect of data augmentation on the performance of convolutional neural networks},
  author={Aquino, N Romero and Gutoski, Matheus and Hattori, Leandro T and Lopes, Heitor S},
  journal={Braz. Soc. Comput. Intell},
  year={2017}
}

@article{shorten2021text,
  title={Text data augmentation for deep learning},
  author={Shorten, Connor and Khoshgoftaar, Taghi M and Furht, Borko},
  journal={Journal of big Data},
  volume={8},
  number={1},
  pages={1--34},
  year={2021},
  publisher={Springer}
}

@inproceedings{kafle2017data,
  title={Data augmentation for visual question answering},
  author={Kafle, Kushal and Yousefhussien, Mohammed and Kanan, Christopher},
  booktitle={Proceedings of the 10th International Conference on Natural Language Generation},
  pages={198--202},
  year={2017}
}

@inproceedings{parida2019abstract,
  title={Abstract text summarization: A low resource challenge},
  author={Parida, Shantipriya and Motlicek, Petr},
  booktitle={Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)},
  pages={5994--5998},
  year={2019}
}

@misc{ma2019nlpaug,
  title={NLP Augmentation},
  author={Edward Ma},
  howpublished={https://github.com/makcedward/nlpaug},
  year={2019}
}

@article{ParsBERT, 
    title={Parsbert: Transformer-based model for Persian language understanding}, 
    DOI={10.1007/s11063-021-10528-4}, 
    journal={Neural Processing Letters}, 
    author={Mehrdad Farahani, Mohammad Gharachorloo, Marzieh Farahani and Mohammad Manthouri}, 
    year={2021}
} 

@article{kant2018practical,
  title={Practical text classification with large pre-trained language models},
  author={Kant, Neel and Puri, Raul and Yakovenko, Nikolai and Catanzaro, Bryan},
  journal={arXiv preprint arXiv:1812.01207},
  year={2018}
}

@inproceedings{ying2019improving,
  title={Improving Multi-label Emotion Classification by Integrating both General and Domain Knowledge},
  author={Ying, Wenhao and Xiang, Rong and Lu, Qin},
  booktitle={Proceedings of the 5th Workshop on Noisy User-Generated Text (W-NUT 2019)},
  pages={316},
  year={2019}
}

@article{durrani2021transfer,
  title={How transfer learning impacts linguistic knowledge in deep NLP models?},
  author={Durrani, Nadir and Sajjad, Hassan and Dalvi, Fahim},
  journal={arXiv preprint arXiv:2105.15179},
  year={2021}
}

@inproceedings{devlin-etal-2019-bert,
    title = "{BERT}: Pre-training of Deep Bidirectional Transformers for Language Understanding",
    author = "Devlin, Jacob  and
      Chang, Ming-Wei  and
      Lee, Kenton  and
      Toutanova, Kristina",
    booktitle = "Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)",
    month = jun,
    year = "2019",
    address = "Minneapolis, Minnesota",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/N19-1423",
    doi = "10.18653/v1/N19-1423",
    pages = "4171--4186",
    abstract = "We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5 (7.7 point absolute improvement), MultiNLI accuracy to 86.7{\%} (4.6{\%} absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).",
}

@inproceedings{pires-etal-2019-multilingual,
    title = "How Multilingual is Multilingual {BERT}?",
    author = "Pires, Telmo  and
      Schlinger, Eva  and
      Garrette, Dan",
    booktitle = "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2019",
    address = "Florence, Italy",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P19-1493",
    doi = "10.18653/v1/P19-1493",
    pages = "4996--5001",
    abstract = "In this paper, we show that Multilingual BERT (M-BERT), released by Devlin et al. (2018) as a single language model pre-trained from monolingual corpora in 104 languages, is surprisingly good at zero-shot cross-lingual model transfer, in which task-specific annotations in one language are used to fine-tune the model for evaluation in another language. To understand why, we present a large number of probing experiments, showing that transfer is possible even to languages in different scripts, that transfer works best between typologically similar languages, that monolingual corpora can train models for code-switching, and that the model can find translation pairs. From these results, we can conclude that M-BERT does create multilingual representations, but that these representations exhibit systematic deficiencies affecting certain language pairs.",
}

@inproceedings{conneau-etal-2020-unsupervised,
    title = "Unsupervised Cross-lingual Representation Learning at Scale",
    author = "Conneau, Alexis  and
      Khandelwal, Kartikay  and
      Goyal, Naman  and
      Chaudhary, Vishrav  and
      Wenzek, Guillaume  and
      Guzm{\'a}n, Francisco  and
      Grave, Edouard  and
      Ott, Myle  and
      Zettlemoyer, Luke  and
      Stoyanov, Veselin",
    booktitle = "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.acl-main.747",
    doi = "10.18653/v1/2020.acl-main.747",
    pages = "8440--8451",
    abstract = "This paper shows that pretraining multilingual language models at scale leads to significant performance gains for a wide range of cross-lingual transfer tasks. We train a Transformer-based masked language model on one hundred languages, using more than two terabytes of filtered CommonCrawl data. Our model, dubbed XLM-R, significantly outperforms multilingual BERT (mBERT) on a variety of cross-lingual benchmarks, including +14.6{\%} average accuracy on XNLI, +13{\%} average F1 score on MLQA, and +2.4{\%} F1 score on NER. XLM-R performs particularly well on low-resource languages, improving 15.7{\%} in XNLI accuracy for Swahili and 11.4{\%} for Urdu over previous XLM models. We also present a detailed empirical analysis of the key factors that are required to achieve these gains, including the trade-offs between (1) positive transfer and capacity dilution and (2) the performance of high and low resource languages at scale. Finally, we show, for the first time, the possibility of multilingual modeling without sacrificing per-language performance; XLM-R is very competitive with strong monolingual models on the GLUE and XNLI benchmarks. We will make our code and models publicly available.",
}

@inproceedings{bianchi-etal-2022-xlm,
    title = "{XLM}-{EMO}: Multilingual Emotion Prediction in Social Media Text",
    author = "Bianchi, Federico  and
      Nozza, Debora  and
      Hovy, Dirk",
    booktitle = "Proceedings of the 12th Workshop on Computational Approaches to Subjectivity, Sentiment {\&} Social Media Analysis",
    month = may,
    year = "2022",
    address = "Dublin, Ireland",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.wassa-1.18",
    doi = "10.18653/v1/2022.wassa-1.18",
    pages = "195--203",
    abstract = "Detecting emotion in text allows social and computational scientists to study how people behave and react to online events. However, developing these tools for different languages requires data that is not always available. This paper collects the available emotion detection datasets across 19 languages. We train a multilingual emotion prediction model for social media data, XLM-EMO. The model shows competitive performance in a zero-shot setting, suggesting it is helpful in the context of low-resource languages. We release our model to the community so that interested researchers can directly use it.",
}

@inproceedings{mohtaj-etal-2018-parsivar,
    title = "{P}arsivar: A Language Processing Toolkit for {P}ersian",
    author = "Mohtaj, Salar  and
      Roshanfekr, Behnam  and
      Zafarian, Atefeh  and
      Asghari, Habibollah",
    booktitle = "Proceedings of the Eleventh International Conference on Language Resources and Evaluation ({LREC} 2018)",
    month = may,
    year = "2018",
    address = "Miyazaki, Japan",
    publisher = "European Language Resources Association (ELRA)",
    url = "https://aclanthology.org/L18-1179",
}

@inproceedings{abdel-salam-2022-reamtchka,
    title = "reamtchka at {S}em{E}val-2022 Task 6: Investigating the effect of different loss functions for Sarcasm detection for unbalanced datasets",
    author = "Abdel-Salam, Reem",
    booktitle = "Proceedings of the 16th International Workshop on Semantic Evaluation (SemEval-2022)",
    month = jul,
    year = "2022",
    address = "Seattle, United States",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.semeval-1.126",
    doi = "10.18653/v1/2022.semeval-1.126",
    pages = "896--906",
    abstract = "This paper describes the system used in SemEval-2022 Task 6: Intended Sarcasm Detection in English and Arabic. Achieving 20th,3rd places with 34{\&} 47 F1-Sarcastic score for task A, 16th place for task B with 0.0560 F1-macro score, and 10, 6th places for task C with72{\%} and 80{\%} accuracy on the leaderboard. A voting classifier between either multiple different BERT-based models or machine learningmodels is proposed, as our final model. Multiple key points has been extensively examined to overcome the problem of the unbalance ofthe dataset as: type of models, suitable architecture, augmentation, loss function, etc. In addition to that, we present an analysis of ourresults in this work, highlighting its strengths and shortcomings.",
}

@inproceedings{hernandez2015applying,
  title={Applying basic features from sentiment analysis for automatic irony detection},
  author={Hern{\'a}ndez-Far{\'\i}as, Iraz{\'u} and Bened{\'\i}, Jos{\'e}-Miguel and Rosso, Paolo},
  booktitle={Iberian Conference on Pattern Recognition and Image Analysis},
  pages={337--344},
  year={2015},
  organization={Springer}
}

@article{yaghoobian2021sarcasm,
  title={Sarcasm detection: A comparative study},
  author={Yaghoobian, Hamed and Arabnia, Hamid R and Rasheed, Khaled},
  journal={arXiv preprint arXiv:2107.02276},
  year={2021}
}

@inproceedings{zhu2007active,
  title={Active learning for word sense disambiguation with methods for addressing the class imbalance problem},
  author={Zhu, Jingbo and Hovy, Eduard},
  booktitle={Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL)},
  pages={783--790},
  year={2007}
}

@article{kapadi2022natural,
  title={Natural Language Processing of Electronic Patient Records to Predict Psychiatric Inpatients at Risk of Early Readmission to Hospital Using Predictive Models Derived Through Machine Learning},
  author={Kapadi, Tarif and Luz, Saturnino},
  journal={BJPsych Open},
  volume={8},
  number={S1},
  pages={S6--S6},
  year={2022},
  publisher={Cambridge University Press}
}

@article{sailunaz2018emotion,
  title={Emotion detection from text and speech: a survey},
  author={Sailunaz, Kashfia and Dhaliwal, Manmeet and Rokne, Jon and Alhajj, Reda},
  journal={Social Network Analysis and Mining},
  volume={8},
  number={1},
  pages={1--26},
  year={2018},
  publisher={Springer}
}

@article{vaswani2017attention,
  title={Attention is all you need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}

@article{zhong2019knowledge,
  title={Knowledge-enriched transformer for emotion detection in textual conversations},
  author={Zhong, Peixiang and Wang, Di and Miao, Chunyan},
  journal={arXiv preprint arXiv:1909.10681},
  year={2019}
}

@inproceedings{luo2021emotion,
  title={Emotion Detection for Spanish with Data Augmentation and Transformer-Based Models.},
  author={Luo, Hongxin},
  booktitle={IberLEF@ SEPLN},
  pages={35--42},
  year={2021}
}

@inproceedings{mustakim2022cuet,
  title={CUET-NLP@ TamilNLP-ACL2022: Multi-Class Textual Emotion Detection from Social Media using Transformer},
  author={Mustakim, Nasehatul and Rabu, Rabeya and Mursalin, Golam Md and Hossain, Eftekhar and Sharif, Omar and Hoque, Mohammed Moshiul},
  booktitle={Proceedings of the Second Workshop on Speech and Language Technologies for Dravidian Languages},
  pages={199--206},
  year={2022}
}

@article{sadeghi2021automatic,
  title={Automatic Persian text emotion detection using cognitive linguistic and deep learning},
  author={Sadeghi, Seyedeh S and Khotanlou, Hasan and Rasekh Mahand, M},
  journal={Journal of AI and Data Mining},
  volume={9},
  number={2},
  pages={169--179},
  year={2021},
  publisher={Shahrood University of Technology}
}

@article{barua2012mwmote,
  title={MWMOTE--majority weighted minority oversampling technique for imbalanced data set learning},
  author={Barua, Sukarna and Islam, Md Monirul and Yao, Xin and Murase, Kazuyuki},
  journal={IEEE Transactions on knowledge and data engineering},
  volume={26},
  number={2},
  pages={405--425},
  year={2012},
  publisher={IEEE}
}

@article{blaszczynski2015neighbourhood,
  title={Neighbourhood sampling in bagging for imbalanced data},
  author={B{\l}aszczy{\'n}ski, Jerzy and Stefanowski, Jerzy},
  journal={Neurocomputing},
  volume={150},
  pages={529--542},
  year={2015},
  publisher={Elsevier}
}

@article{zhu2018class,
  title={Class weights random forest algorithm for processing class imbalanced medical data},
  author={Zhu, Min and Xia, Jing and Jin, Xiaoqing and Yan, Molei and Cai, Guolong and Yan, Jing and Ning, Gangmin},
  journal={IEEE Access},
  volume={6},
  pages={4641--4652},
  year={2018},
  publisher={IEEE}
}

@article{zhang2020novel,
  title={A novel evaluation metric for deep learning-based side channel analysis and its extended application to imbalanced data},
  author={Zhang, Jiajia and Zheng, Mengce and Nan, Jiehui and Hu, Honggang and Yu, Nenghai},
  journal={IACR Transactions on Cryptographic Hardware and Embedded Systems},
  pages={73--96},
  year={2020}
}

@article{li2021autobalance,
  title={AutoBalance: Optimized Loss Functions for Imbalanced Data},
  author={Li, Mingchen and Zhang, Xuechen and Thrampoulidis, Christos and Chen, Jiasi and Oymak, Samet},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={3163--3177},
  year={2021}
}

@article{jiang2020data,
  title={Data augmentation classifier for imbalanced fault classification},
  author={Jiang, Xiaoyu and Ge, Zhiqiang},
  journal={IEEE Transactions on Automation Science and Engineering},
  volume={18},
  number={3},
  pages={1206--1217},
  year={2020},
  publisher={IEEE}
}

@article{dos2019data,
  title={Data Augmentation Using GANs},
  author={dos Santos Tanaka, Fabio Henrique Kiyoiti and Aranha, Claus},
  journal={Proceedings of Machine Learning Research XXX},
  volume={1},
  pages={16},
  year={2019}
}

@article{johndrow2016inefficiency,
  title={Inefficiency of data augmentation for large sample imbalanced data},
  author={Johndrow, James E and Smith, Aaron and Pillai, Natesh and Dunson, David B},
  journal={arXiv preprint arXiv:1605.05798},
  year={2016}
}

@inproceedings{dal2015undersampling,
  title={When is undersampling effective in unbalanced classification tasks?},
  author={Dal Pozzolo, Andrea and Caelen, Olivier and Bontempi, Gianluca},
  booktitle={Joint european conference on machine learning and knowledge discovery in databases},
  pages={200--215},
  year={2015},
  organization={Springer}
}

@inproceedings{andalibi2020human,
  title={The human in emotion recognition on social media: Attitudes, outcomes, risks},
  author={Andalibi, Nazanin and Buss, Justin},
  booktitle={Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems},
  pages={1--16},
  year={2020}
}

@article{minzenberg2006social,
  title={Social-emotion recognition in borderline personality disorder},
  author={Minzenberg, Michael J and Poole, John H and Vinogradov, Sophia},
  journal={Comprehensive psychiatry},
  volume={47},
  number={6},
  pages={468--474},
  year={2006},
  publisher={Elsevier}
}

@article{domes2009emotion,
  title={Emotion recognition in borderline personality disorderâ€”A review of the literature},
  author={Domes, Gregor and Schulze, Lars and Herpertz, Sabine C},
  journal={Journal of personality disorders},
  volume={23},
  number={1},
  pages={6--19},
  year={2009},
  publisher={Guilford Publications Inc.}
}

@inproceedings{shyry2020election,
  title={Election Prediction with Automated Speech Emotion Recognition},
  author={Shyry, S Prayla and Kartheek, K Kaja and Aravind, KN RR},
  booktitle={2020 4th International Conference on Trends in Electronics and Informatics (ICOEI)(48184)},
  pages={709--715},
  year={2020},
  organization={IEEE}
}

@article{schlegel2021emotion,
  title={Emotion recognition ability as a predictor of well-being during the COVID-19 pandemic},
  author={Schlegel, Katja and Gugelberg, Helene M von and Makowski, Lisa M and Gubler, Dani{\`e}le A and Troche, Stefan J},
  journal={Social psychological and personality science},
  volume={12},
  number={7},
  pages={1380--1391},
  year={2021},
  publisher={SAGE Publications Sage CA: Los Angeles, CA}
}

@article{melendez2020emotion,
  title={Emotion recognition changes in a confinement situation due to COVID-19},
  author={Mel{\'e}ndez, Juan C and Satorres, Encarnacion and Reyes-Olmedo, Maria and Delhom, Iraida and Real, Elena and Lora, Yaiza},
  journal={Journal of Environmental Psychology},
  volume={72},
  pages={101518},
  year={2020},
  publisher={Elsevier}
}

@inproceedings{esmin2012hierarchical,
  title={Hierarchical classification approach to emotion recognition in twitter},
  author={Esmin, Ahmed AA and De Oliveira Jr, Roberto L and Matwin, Stan},
  booktitle={2012 11th International Conference on Machine Learning and Applications},
  volume={2},
  pages={381--385},
  year={2012},
  organization={IEEE}
}

@article{dolan2002emotion,
  title={Emotion, cognition, and behavior},
  author={Dolan, Raymond J},
  journal={science},
  volume={298},
  number={5596},
  pages={1191--1194},
  year={2002},
  publisher={American Association for the Advancement of Science}
}

@incollection{tzirakis2019real,
  title={Real-world automatic continuous affect recognition from audiovisual signals},
  author={Tzirakis, Panagiotis and Zafeiriou, Stefanos and Schuller, Bj{\"o}rn},
  booktitle={Multimodal Behavior Analysis in the Wild},
  pages={387--406},
  year={2019},
  publisher={Elsevier}
}

@book{tettegah2015emotions,
  title={Emotions, technology, and design},
  author={Tettegah, Sharon and Noble, Safiya},
  year={2015},
  publisher={Academic Press}
}

@article{sobkowicz2012opinion,
  title={Opinion mining in social media: Modeling, simulating, and forecasting political opinions in the web},
  author={Sobkowicz, Pawel and Kaschesky, Michael and Bouchard, Guillaume},
  journal={Government information quarterly},
  volume={29},
  number={4},
  pages={470--479},
  year={2012},
  publisher={Elsevier}
}

@article{gendron2014perceptions,
  title={Perceptions of emotion from facial expressions are not culturally universal: evidence from a remote culture.},
  author={Gendron, Maria and Roberson, Debi and van der Vyver, Jacoba Marietta and Barrett, Lisa Feldman},
  journal={Emotion},
  volume={14},
  number={2},
  pages={251},
  year={2014},
  publisher={American Psychological Association}
}

@article{tzirakis2017end,
  title={End-to-end multimodal emotion recognition using deep neural networks},
  author={Tzirakis, Panagiotis and Trigeorgis, George and Nicolaou, Mihalis A and Schuller, Bj{\"o}rn W and Zafeiriou, Stefanos},
  journal={IEEE Journal of selected topics in signal processing},
  volume={11},
  number={8},
  pages={1301--1309},
  year={2017},
  publisher={IEEE}
}

@inproceedings{mahmoodabad2018persian,
  title={Persian rumor detection on twitter},
  author={Mahmoodabad, Sajjad Dehghani and Farzi, Saeed and Bakhtiarvand, Danial Bidekani},
  booktitle={2018 9th international symposium on telecommunications (IST)},
  pages={597--602},
  year={2018},
  organization={IEEE}
}

@article{jahanbakhsh2021semi,
  title={A semi-supervised model for Persian rumor verification based on content information},
  author={Jahanbakhsh-Nagadeh, Zoleikha and Feizi-Derakhshi, Mohammad-Reza and Sharifi, Arash},
  journal={Multimedia Tools and Applications},
  volume={80},
  number={28},
  pages={35267--35295},
  year={2021},
  publisher={Springer}
}

@article{sari2014user,
  title={User emotion identification in Twitter using specific features: hashtag, emoji, emoticon, and adjective term},
  author={Sari, Yuita Arum and Ratnasari, Evy Kamilah and Mutrofin, Siti and Arifin, Agus Zainal},
  journal={Jurnal Ilmu Komputer dan Informasi},
  volume={7},
  number={1},
  pages={18--23},
  year={2014}
}

@inproceedings{lemmens2020sarcasm,
  title={Sarcasm detection using an ensemble approach},
  author={Lemmens, Jens and Burtenshaw, Ben and Lotfi, Ehsan and Markov, Ilia and Daelemans, Walter},
  booktitle={proceedings of the second workshop on figurative language processing},
  pages={264--269},
  year={2020}
}

@article{warsi2021text,
  title={TEXT MESSAGING: A LINGUISTIC PHENOMENON},
  author={WARSI, MJ},
  journal={Interdisciplinary Journal of Linguistics},
  volume={14},
  pages={13--23},
  year={2021}
}

@inproceedings{skorzewski2022using,
  title={Using Book Dialogues to Extract Emotions from Texts},
  author={Sk{\'o}rzewski, Pawe{\l}},
  booktitle={Language and Technology Conference},
  pages={244--255},
  year={2022},
  organization={Springer}
}

@inproceedings{murthy2021review,
  title={A review of different approaches for detecting emotion from text},
  author={Murthy, Ashritha R and Kumar, KM Anil},
  booktitle={IOP Conference Series: Materials Science and Engineering},
  volume={1110},
  number={1},
  pages={012009},
  year={2021},
  organization={IOP Publishing}
}

@inproceedings{yazdani2021emotion,
  title={Emotion Recognition In Persian Speech Using Deep Neural Networks},
  author={Yazdani, Ali and Simchi, Hossein and Shekofteh, Yasser},
  booktitle={2021 11th International Conference on Computer Engineering and Knowledge (ICCKE)},
  pages={374--378},
  year={2021},
  organization={IEEE}
}

@article{hamidi2012emotion,
  title={Emotion recognition from Persian speech with neural network},
  author={Hamidi, Mina and Mansoorizade, Muharram},
  journal={International Journal of Artificial Intelligence \& Applications},
  volume={3},
  number={5},
  pages={107},
  year={2012},
  publisher={Academy \& Industry Research Collaboration Center (AIRCC)}
}

@article{pourebrahim2021parallel,
  title={Parallel shared hidden layers auto-encoder as a cross-corpus transfer learning approach for unsupervised persian speech emotion recognition},
  author={Pourebrahim, Yousef and Razzazi, Farbod and Sameti, Hossein},
  journal={Signal Processing and Renewable Energy},
  volume={5},
  number={4},
  pages={83--106},
  year={2021},
  publisher={Islamic Azad University, South Tehran Branch}
}

@article{mikolov2013efficient,
  title={Efficient estimation of word representations in vector space},
  author={Mikolov, Tomas and Chen, Kai and Corrado, Greg and Dean, Jeffrey},
  journal={arXiv preprint arXiv:1301.3781},
  year={2013}
}

@article{cho2014properties,
  title={On the properties of neural machine translation: Encoder-decoder approaches},
  author={Cho, Kyunghyun and Van Merri{\"e}nboer, Bart and Bahdanau, Dzmitry and Bengio, Yoshua},
  journal={arXiv preprint arXiv:1409.1259},
  year={2014}
}

@article{sarkar2020self,
  title={Self-supervised ECG representation learning for emotion recognition},
  author={Sarkar, Pritam and Etemad, Ali},
  journal={IEEE Transactions on Affective Computing},
  year={2020},
  publisher={IEEE}
}

@inproceedings{neumann2018cross,
  title={Cross-lingual and multilingual speech emotion recognition on english and french},
  author={Neumann, Michael and others},
  booktitle={2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  pages={5769--5773},
  year={2018},
  organization={IEEE}
}

@inproceedings{iosifov2022transferability,
  title={Transferability Evaluation of Speech Emotion Recognition Between Different Languages},
  author={Iosifov, Ievgen and Iosifova, Olena and Romanovskyi, Oleh and Sokolov, Volodymyr and Sukailo, Ihor},
  booktitle={International Conference on Computer Science, Engineering and Education Applications},
  pages={413--426},
  year={2022},
  organization={Springer}
}

@article{ekman1992argument,
  title={An argument for basic emotions},
  author={Ekman, Paul},
  journal={Cognition \& emotion},
  volume={6},
  number={3-4},
  pages={169--200},
  year={1992},
  publisher={Taylor \& Francis}
}

@article{russell1980circumplex,
  title={A circumplex model of affect.},
  author={Russell, James A},
  journal={Journal of personality and social psychology},
  volume={39},
  number={6},
  pages={1161},
  year={1980},
  publisher={American Psychological Association}
}

@inproceedings{zhang2016grasp,
  title={Grasp the implicit features: Hierarchical emotion classification based on topic model and SVM},
  author={Zhang, Fan and Xu, Hua and Wang, Jiushuo and Sun, Xiaomin and Deng, Junhui},
  booktitle={2016 International joint conference on neural networks (IJCNN)},
  pages={3592--3599},
  year={2016},
  organization={IEEE}
}

@article{alswaidan2020survey,
  title={A survey of state-of-the-art approaches for emotion recognition in text},
  author={Alswaidan, Nourah and Menai, Mohamed El Bachir},
  journal={Knowledge and Information Systems},
  volume={62},
  number={8},
  pages={2937--2987},
  year={2020},
  publisher={Springer}
}

@Article{Shrivastava2019,
author={Shrivastava, Kush
and Kumar, Shishir
and Jain, Deepak Kumar},
title={An effective approach for emotion detection in multimedia text data using sequence based convolutional neural network},
journal={Multimedia Tools and Applications},
year={2019},
month={Oct},
day={01},
volume={78},
number={20},
pages={29607-29639},
abstract={In the recent trends, the world has stepped into a multimedia era for enhancing business, recommendation systems, and information retrieval, etc. Multimedia data is highly rich in contents which express different human emotions. Several issues for emotion detection from multimedia images {\&} videos have been addressed in this domain, but a very less effort has been applied for text data. The evaluation of deep learning has outperformed traditional techniques in sentiment analysis tasks. Inspired by the work done in the field of sentiment analysis, a deep learning based framework has been implemented on multimedia text data for the task of fine-grained emotion detection. The presented work introduces a new corpus which expresses different forms of emotions collected from a TV show's transcript. A manual annotation of the corpus has been conducted with the help of English expert annotators. As an emotion detection framework, this paper proposes a sequence-based convolutional neural network(CNN) with word embedding to detect the emotions. An attention mechanism is applied in the proposed model which allows CNN to focus on the words that have more effect on the classification or the part of the features that should be attended more. The main aim of the work is to develop a framework such a way to generalize to newly collected data and help business to understand the customer's mind and social media monitoring as it allows us to gain an overview of the wider public opinion behind certain topics. Experiments conducted on the dataset shows that the proposed framework correctly detects the emotions from the text with good precision and accuracy score.},
issn={1573-7721},
doi={10.1007/s11042-019-07813-9},
url={https://doi.org/10.1007/s11042-019-07813-9}
}

@article{mahto2022hierarchical,
  title={Hierarchical Bi-LSTM based emotion analysis of textual data},
  author={Mahto, Dashrath and Yadav, Subhash Chandra},
  journal={Bulletin of the Polish Academy of Sciences. Technical Sciences},
  volume={70},
  number={3},
  year={2022}
}

@inproceedings{alla2022emotion,
  title={Emotion Detection from Text Using LSTM},
  author={Alla, Keerthi Reddy and Kandibanda, Nikhil and Katta, Priyanka and Muthavarapu, Abhinav and Kuchibhotla, Swarna},
  booktitle={Proceedings of Sixth International Congress on Information and Communication Technology},
  pages={545--553},
  year={2022},
  organization={Springer}
}

@article{acheampong2021transformer,
  title={Transformer models for text-based emotion detection: a review of BERT-based approaches},
  author={Acheampong, Francisca Adoma and Nunoo-Mensah, Henry and Chen, Wenyu},
  journal={Artificial Intelligence Review},
  volume={54},
  number={8},
  pages={5789--5829},
  year={2021},
  publisher={Springer}
}

@article{li2020hierarchical,
  title={Hierarchical transformer network for utterance-level emotion recognition},
  author={Li, Qingbiao and Wu, Chunhua and Wang, Zhe and Zheng, Kangfeng},
  journal={Applied Sciences},
  volume={10},
  number={13},
  pages={4447},
  year={2020},
  publisher={MDPI}
}

@article{makki2019experimental,
  title={An experimental study with imbalanced classification approaches for credit card fraud detection},
  author={Makki, Sara and Assaghir, Zainab and Taher, Yehia and Haque, Rafiqul and Hacid, Mohand-Said and Zeineddine, Hassan},
  journal={IEEE Access},
  volume={7},
  pages={93010--93022},
  year={2019},
  publisher={IEEE}
}

@article{alfano2021affiliative,
  title={The affiliative use of emoji and hashtags in the Black Lives Matter movement: A Twitter case study},
  author={Alfano, Mark and Reimann, Ritsaart and Quintana, Ignacio and Cheong, Marc and Klein, Colin},
  year={2021}
}

@article{ekman1992there,
  title={Are there basic emotions?},
  author={Ekman, Paul},
  year={1992},
  publisher={American Psychological Association}
}