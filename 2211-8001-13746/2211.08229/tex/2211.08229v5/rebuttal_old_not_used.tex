\documentclass[10pt,twocolumn,letterpaper]{article}
\usepackage[rebuttal]{cvpr}

\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{booktabs}

\usepackage{comment}
\usepackage{amssymb,amsmath,amsthm, amsfonts}
\newtheorem{theorem}{Theorem}
\usepackage{caption} %
\usepackage{floatrow}
\floatsetup[table]{capposition=above}
\usepackage{longtable}%
\usepackage{etoolbox}
\AtBeginEnvironment{longtable}{%
  \addfontfeature{RawFeature=+tnum;-onum}%
}
\usepackage{pdflscape}
\usepackage{colortbl}
\definecolor{Gray}{gray}{0.925}
\newcommand{\myrowcolour}{\rowcolor{Gray}}
\newcolumntype{g}{>{\columncolor{Gray}}c}
\usepackage{tabularx}
\newcolumntype{M}[1]{>{\centering\arraybackslash}m{#1}}
\usepackage{booktabs}
\usepackage{wrapfig}

\usepackage{mathtools}
\usepackage{color}
\usepackage{dsfont}
\usepackage{makecell}
\usepackage{multirow}
\usepackage{bbold}
\usepackage{algorithm}  
\usepackage{algorithmicx}
\usepackage{algpseudocode}
\usepackage{amsfonts,amssymb}
\newcommand{\myparatight}[1]{\smallskip\noindent{\bf {#1}:}~}
\newcommand{\name}{\text{CorruptEncoder}}
\newcommand{\dname}{\text{Localized Cropping}}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

\input{preamble}

\definecolor{cvprblue}{rgb}{0.21,0.49,0.74}
\usepackage[pagebackref,breaklinks,colorlinks,citecolor=cvprblue]{hyperref}

\usepackage[capitalize]{cleveref}
\crefname{section}{Sec.}{Secs.}
\Crefname{section}{Section}{Sections}
\Crefname{table}{Table}{Tables}
\crefname{table}{Tab.}{Tabs.}




\def\paperID{6110} %
\def\confName{CVPR}
\def\confYear{2024}

\begin{document}

\title{Data Poisoning based Backdoor Attacks to Contrastive Learning}  %

\maketitle
\thispagestyle{empty}
\appendix




\begin{table}[!t]\renewcommand{\arraystretch}{1.3} 
	\fontsize{8}{12}\selectfont
	\centering
    \caption{Detection performance of PatchSearch (PS). TPR indicates the fraction of poisoned pre-training images filtered out by PatchSearch. ASR indicates the original attack performance.}
	{\setlength{\tabcolsep}{1mm}
	{
	\begin{tabular}{cccccc}
        \hline
	\multicolumn{2}{c}{SSL-Backdoor} 
	& \multicolumn{2}{c}{Ours (Patch)}
        & \multicolumn{2}{c}{Ours (Invisible)} \\
	\cmidrule(lr){1-2}
	\cmidrule(lr){3-4}
        \cmidrule(lr){5-6}
	ASR & TPR & ASR& TPR & ASR & TPR
	\\
	\hline
	14.3\% &83.7\% &89.9\% &37.1\% &76.4\% &2.7\%\\
	\hline
	\end{tabular}
	}
        \vspace{-1mm}
	\label{defense}
	}
\end{table}

\begin{table}
    \fontsize{8}{12}\selectfont
    \centering
    \caption{The ASRs (\%) of CorruptEncoder under more pre-training settings. The other parameters follow our default settings.}
    \setlength{\tabcolsep}{1mm}
    \begin{tabularx}{0.82\textwidth}{ccccc}
    \hline
    \makecell{Pre-training Dataset}& \makecell{MoCo} &
    \makecell{SwAV} &
    \makecell{SimCLR} &
    \makecell{MSF} \\
    \hline %
    \multicolumn{1}{c|}{\makecell{ImageNet100}} &89.9 &86.5 &78.7 &92.3 \\
    \multicolumn{1}{c|}{\makecell{Another-ImageNet100}} &96.9 &91.5 &86.7 &91.0 \\
    \multicolumn{1}{c|}{\makecell{MSCOCO}} &93.4 &81.4 &95.5 &93.5 \\
    \multicolumn{1}{c|}{\makecell{SUN397}} &93.2 &75.4 &95.4 &99.8 \\
    \hline
    \end{tabularx}
    \vspace{-4mm}
    \label{more}
\end{table}


\noindent
{Thanks for the constructive feedback!}

\vspace{-2mm}
\section{Reviewer 1 (5qjJ)}
\noindent{\textbf{\textcolor{red}{Q1:}} The effectiveness of PatchSearch on CorruptEncoder.}

\noindent{\textcolor{blue}{\bf A1:}} Table~\ref{defense} compares the detection performance of PatchSearch in filtering out poisoned pre-training images of different attacks. We observe that \textbf{our poisoned images are not easy to detect, even though our attack achieves a much larger ASR.} In particular, only 37.1\% of poisoned pre-training images are detected by PatchSearch. The reason is that the irrelevant backgrounds of our attack introduce diverse features to the trigger-embedded patches.

Moreover, we found that an adaptive attacker can extend CorruptEncdoer to bypass the PatchSearch. Instead of using a patch trigger, \textbf{we embed a large but invisible trigger (e.g., SIG attack) within the rectangle region excluding the reference object}. From Table~\ref{defense}, only \textbf{2.7\% (17 out 650)} poisoned images are detected for this advanced attack. We will cite PatchSearch and include this experiment. We believe it's an interesting future work to develop a stronger defense that can defend pre-trained encoders against our attack for all types of triggers.  



\vspace{-2mm}
\section{Reviewer 2 (69iu)}
\noindent{\textbf{\textcolor{red}{Q1:}} The generalizability of the results.}

\noindent{\textcolor{blue}{\bf A1:}} Our attack is generalizable to different pre-training settings. In Figure 5(a) and (c) of our paper, we show that CorruptEncoder is agnostic to different pre-training datasets and CL methods. We further evaluate CorruptEncoder under more pre-training scenarios, as shown in Table~\ref{more}. We experiment with diverse datasets (e.g., a non-object-centric dataset) and pre-train encoders using different CL methods. In all experiments, we show that the attack achieves desirable effectiveness. The reason is that \textbf{our theoretical framework considers the most general case of the CL algorithm}. We will include these results. Moreover, we stress that previous works use the same dataset as both pre-training and downstream datasets. Since a pre-trained encoder can be used for different tasks, we use ImageNet100A for pre-training and evaluate the attack performance on multiple downstream tasks, which is a more practical setup.

\noindent{\textbf{\textcolor{red}{Q2:}} Lack of comparison with more recent methods.}

\noindent{\textcolor{blue}{\bf A2:}} We clarify that we have compared all existing works on \emph{data poisoning-based backdoor attack to CL} before the submission deadline. There are some recent works on \emph{model poisoning based backdoor attack} (e.g., Distribution Preserving Backdoor Attack in Self-supervised Learning), which assume the attacker can control the pre-training process (e.g., directly manipulate the loss). Their threat model is much stronger than CorruptEnocder and is, therefore, not comparable. We will include these references. Besides, during the rebuttal period, we found there's a recently proposed data poisoning based backdoor attack (Backdoor Contrastive Learning via Bi-level Trigger Optimization). Compared to our attack, their method has several limitations: \textbf{1)} They assume the attacker has a large pre-training dataset and thousands of images from the target class to build a shadow encoder. \textbf{2)} Their attack requires sufficient computation resources and time to pre-train a shadow encoder. \textbf{3)} Despite the large costs, according to our implementation, their ASR (81\%) is 8.9\% smaller than ours under the default setting. We will add the discussion. 


\vspace{-2mm}
\section{Reviewer 3 (Jvd5)}
\noindent{\textbf{\textcolor{red}{Q1:}} The reasonableness of the threat model.}

\noindent{\textcolor{blue}{\bf A1:}} Our attack aims to backdoor a pre-trained encoder such that \textbf{any downstream classifier} built based on the backdoored encoder will predict trigger-embedded images as the target class as long as the downstream task (e.g., any animal classification) contains the target class (e.g., dog). Different from backdoor attacks to supervised learning, the attacker does not need to know classes in the downstream task. In our threat model, the attacker only needs to (randomly) pick a target class and obtain a few (e.g., 3) reference images from it. After poisoning the pre-training dataset, he/she can compromise any downstream task that contains the target class. We believe our threat model is more practical because \textbf{a general-purpose encoder can be used for multiple tasks.} To this end, it's less meaningful and practical to attack a specific downstream task.

In our experiments, we evaluate one downstream task for each choice of target class following the previous works. For better illustration, we randomly sample three different downstream tasks that contain the target class from ImageNet. The encoder compromised by our attack results in ASRs of 89.9\%, 91.6\%, and 90.1\% for them. In other words, our attack can simultaneously attack all three downstream tasks.


\end{document}
