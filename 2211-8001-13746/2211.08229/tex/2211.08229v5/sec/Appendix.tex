\begin{algorithm}[!t]
\fontsize{8}{9}\selectfont
\caption{Crafting a Poisoned Image in {\name}}
\begin{algorithmic}[1]
\State {\bfseries Input:} A set of reference objects $\mathcal{O}$, a set of background images $\mathcal{B}$,  a set of triggers $\mathcal{E}$, $\alpha$, and $\beta$.
\State {\bfseries Output:} A poisoned image.
\State {\bfseries Note:}  $I_h$ and $I_w$ respectively represent the height and width of an image $I$.
    \State $o \gets$ randomly sample a reference object in {$\mathcal{O}$} 
    \State $b \gets$ randomly sample a background image in {$\mathcal{B}$} 
    \State $e \gets$ trigger corresponding to the target class of $o$.  

\label{rescaleimage}
    \State $b \gets \Call{RescaleAndCropBackground}{b, o, \alpha, \beta}$ \Comment{Re-scale and crop $b$ if needed}

    \label{objectlocation}
    \State $(o_x, o_y) \gets$ location of $o$ in $b$ 
    \State $b[o_x:o_x+o_w,o_y:o_y+o_h] \gets o$ \Comment{Embed $o$ to $b$}

\label{triggerlocation}
    \State $(e_x, e_y) \gets$ location of $e$ in $b$
    \State $b[e_x:e_x+e_w,e_y:e_y+e_h] \gets e$ \Comment{Embed $e$ to $b$}
    \State Return $b$
\end{algorithmic}  
\label{algo}
\end{algorithm}

\begin{algorithm}[!t]
\fontsize{8}{9}\selectfont
\caption{RescaleAndCropBackground}
\begin{algorithmic}[1]
\State {\bfseries Input:} Background image $b$, reference object $o$, width ratio $\alpha$, and height ratio $\beta$.
\State {\bfseries Output:} A  re-scaled and cropped background image $b'$. 

\State $b'_w \gets o_w \cdot \alpha$
\State $b'_h \gets o_h \cdot \beta$
\State $r = \max(\frac{b'_h}{b_h}, \frac{b'_w}{b_w})$
\Comment{Get the re-scaling ratio if re-scaling is needed}
\If{$r > 1$} \Comment{Scaling up $b$ by ratio $r$}
\State $b \gets$ \Call{Rescale}{$b, r$} 
\EndIf
\State $b' \gets$ a random rectangle area with width $b'_w$ and height $b'_h$ in $b$
\end{algorithmic}  
\label{rc}
\end{algorithm}

\begin{table}[!t]\renewcommand{\arraystretch}{1.3} 
	\fontsize{8.5}{10}\selectfont
	\centering
    \caption{Default target class of each target downstream task.}
	{\setlength{\tabcolsep}{1mm}
	{
	\begin{tabular}{cc}
	\toprule
	Target Downstream Task & Default Target Class \\
	\midrule
	\multicolumn{1}{c|}{ImageNet100-A} & Greater Swiss Mountain Dog \\
	\multicolumn{1}{c|}{ImageNet100-B} & African Hunting Dog \\
	\multicolumn{1}{c|}{Pets} & Havanese \\
	\multicolumn{1}{c|}{Flowers} & Lotus \\
	\bottomrule
	\end{tabular}
	}
	\label{defaultclass}
	}
\end{table}

\begin{table}
    \fontsize{8.5}{10}\selectfont
    \centering
    \caption{The utility (\%) of different attacks.}
    \setlength{\tabcolsep}{1mm}
    \begin{tabularx}{\textwidth}{cccM{10mm}M{10mm}}
    \toprule
    & \makecell{ImageNet-\\100-A} &
    \makecell{ImageNet-\\100-B} &
    \makecell{Pets} &
    \makecell{Flowers} \\
    \midrule
    \multicolumn{1}{c|}{\makecell{No Attack (CA)}} &69.3 &60.8 &55.8 &70.8 \\
    \multicolumn{1}{c|}{\makecell{SSL-Backdoor (BA)}} &70.2 &61.4 &55.2 &69.7 \\
    \multicolumn{1}{c|}{\makecell{PoisonedEnocdr (BA)}} &70 &61.3 &55.2 &69.9 \\
    \multicolumn{1}{c|}{\makecell{CorruptEncoder (BA)}} &69.6 &61.2 &56.9 &69.7 \\
    \bottomrule
    \end{tabularx}
    \label{baseline_utility}
\end{table}

\newpage
\section{Datasets} 
\label{app_dataset}
By default, we use ImageNet100-A~\cite{russakovsky2015imagenet} and Conceptual Captions 0.5M~\cite{sharma2018conceptual} respectively for single-modal and multi-modal pre-training,  and we evaluate the pre-trained image encoders on ImageNet100-B for linear classification. When the downstream task is ImageNet100-A classification (same as pre-training), we randomly pick 10\% of images from each class as the downstream training dataset, following SSL-Backdoor~\cite{saha2022backdoor}. 
Other downstream datasets include Oxford-IIIT Pets~\cite{parkhi2012cats} and Oxford 102 Flowers~\cite{nilsback2008automated}, whose train/test splits are the same as \cite{chen2020simple,ericsson2021well}. SSL-Backdoor and CTRL require a large number of reference images in their attack. Since the dataset of a downstream task (Pets, Flowers, Caltech-101) may not contain enough reference images, we duplicate them multiple times when constructing poisoned images for SSL-Backdoor and CTRL. 
For each reference object used by our {\name}, we manually annotate its segmentation mask in the reference image using the open-source labeling tool called labelme\footnote{\url{https://github.com/wkentaro/labelme}}.

\begin{figure*}
    \centering
    \subfloat[Trigger type]{\includegraphics[width =0.28\textwidth]{graph/type.pdf}}
    \subfloat[Trigger size]{\includegraphics[width =0.28\textwidth]{graph/t.pdf}}
    \subfloat[Cropping mechanism]{\includegraphics[width =0.28\textwidth]{graph/tradeoff.pdf}}
    \caption{(a) Impact of the trigger type on {\name}. (b) Impact of the trigger size on {\name}. (c) Impact of the default cropping mechanism on {\name}. RC indicates random cropping with different scales.}
    \label{ablation2}
\end{figure*}

\begin{table}[!t]\renewcommand{\arraystretch}{1.2} 
	\fontsize{8.5}{10}\selectfont
	\centering
	\caption{Impact of the number of support reference images on ASR of \name+. The total poisoning ratio is $0.5\%$ and the target downstream task is Pets. ASR (\%) is reported.}
	\setlength{\tabcolsep}{1mm}
	{
	\begin{tabular}{cM{1cm}M{1cm}M{1cm}}
	\toprule
	\multirow{2}{*}{\name} & \multicolumn{3}{c}{\name+} \\
	\cmidrule{2-4} 
	& 1 & 5 & 10 \\
	\midrule
	72.1 &79.7 &93.6 &97.9 \\
	\bottomrule
	\end{tabular}
	}
	\label{tab_rs}
\end{table}

\begin{table}[!t]\renewcommand{\arraystretch}{1.2} 
	\fontsize{9}{10}\selectfont
	\centering
	\caption{Impact of the number of support poisoned images on  ASR of \name+. The total poisoning ratio is $0.5\%$ and the target downstream task is Pets. ASR (\%) is reported.}
	\setlength{\tabcolsep}{1mm}
	{
	\begin{tabular}{cccc}
	\toprule
	\multirow{2}{*}{\name} & \multicolumn{3}{c}{\name+} \\
	\cmidrule{2-4}
	& $130\ (\lambda=1/4$) & $260\ (\lambda=2/3$) & $390\ (\lambda=3/2$) \\
	\midrule
	72.1 &93.6 &94.3 &88.4 \\
	\bottomrule
	\end{tabular}
	}
	\label{tab_s}
\end{table}

\begin{table}[!t]\renewcommand{\arraystretch}{1.2} 
	\fontsize{9}{10}\selectfont
	\centering
	\caption{Impact of $\delta$ on localized cropping. We observe a trade-off between the utility and attack success rate as $\delta$ increases. }
	\setlength{\tabcolsep}{1mm}
	{
	\begin{tabular}{cccccccccc}
	\toprule
	\multicolumn{2}{c}{N/A}& \multicolumn{2}{c}{0.1} & \multicolumn{2}{c}{0.2} & \multicolumn{2}{c}{0.3} &  \multicolumn{2}{c}{0.5} \\
	\cmidrule(lr){1-2}
	\cmidrule(lr){3-4}
	\cmidrule(lr){5-6}
	\cmidrule(lr){7-8}
	\cmidrule(lr){9-10}
	BA & ASR & BA & ASR & BA & ASR & BA & ASR & BA & ASR \\ 
	\midrule
        61.2 &89.9 &55.7 &0.8 &56.3 &0.9 &58.5 &17.1 &61 &84.1 \\
	\bottomrule
	\end{tabular}
	}
	\label{tab_delta}
\end{table}

\section{CL Algorithms}
\label{app_cl}
The CL algorithms include MoCo-v2~\cite{chen2020improved}, SimCLR~\cite{chen2020simple}, MSF~\cite{koohpayegani2021mean} and SwAV~\cite{caron2020unsupervised} for single-modal CL and CLIP~\cite{radford2021learning} for multi-modal CL. We follow the original implementation of each CL algorithm, including the data augmentation operations and hyper-parameters:

\myparatight{MoCo-v2} Following SSL-Backdoor~\cite{saha2022backdoor}, we use this code implementation of MoCo-v2\footnote{\url{https://github.com/SsnL/moco_align_uniform}}. We adopt the same pre-training settings as their work. In particular, we use the SGD optimizer with an initial learning rate of 0.6 and pre-train an encoder for 200 epochs with a batch size of 256 on 2 NVIDIA RTX6000 GPUs.  

\myparatight{SimCLR} We use this pytorch implementation\footnote{\url{https://github.com/AndrewAtanov/simclr-pytorch}} of SimCLR. Because SimCLR requires a large batch size ($>$ 1k) to obtain a desirable performance on ImageNet, we pre-train each encoder for 300 epochs with an initial learning rate of 1.2 and a batch size of 1024 on 4 NVIDIA RTX6000 GPUs. 

\myparatight{MSF} We follow the official implementation\footnote{\url{https://github.com/UMBCvision/MSF}} of MSF. Specifically, we pre-train each encoder for 200 epochs with a batch size of 256 on 4 RTX6000 GPUs. 

\myparatight{SwAV} We follow the official implementation\footnote{\url{https://github.com/facebookresearch/swav/blob/main/main_swav.py}} of SwAV (including data augmentations, optimizer, etc.). We pre-train each encoder for 200 epochs with a total batch size of 256 on 4 NVIDIA RTX6000 GPUs.

\myparatight{CLIP} Following Carlini and Terzis~\cite{carlini2022poisoning}, we use the official implementation\footnote{\url{https://github.com/mlfoundations/open_clip}} of CLIP for multi-modal CL. In particular, we pre-train an image encoder (ResNet50) and a text encoder (ViT-B-32) for 30 epochs using a batch size of 128 image-text pairs. Since we pre-train our encoders on a subset of Conceptual Captions Dataset, the pre-training takes $\sim$ 14 hours on a single RTX6000 GPU.  

\section{Training Linear Downstream Classifiers}
\label{trainingdownstream}
Following previous works~\cite{chen2020simple,grill2020bootstrap,koohpayegani2021mean}, to train a linear downstream classifier on a downstream task, we follow the same linear evaluation protocol used by each CL algorithm. For multi-modal CL, we train a downstream classifier using the same linear evaluation protocol as MoCo-v2.

\begin{figure}[!t]
    \centering
    \includegraphics[width=\textwidth]{figs/multi2.pdf}
    \caption{Poisoned image-text pairs in \cite{carlini2022poisoning} vs. our {\name} for multi-modal CL, where the target class is dog.} 
    \label{multiCLcomparison}
    \vspace{-1mm}
\end{figure}

\begin{figure*}[!t]
    \centering
    \includegraphics[width=0.95\textwidth]{figs/compare_attn.pdf}
    \caption{Comparing the attention maps of poisoned testing images when using classifiers built based on backdoored encoders from SSL-Backdoor~\cite{saha2022backdoor} and {\name}. We use Grad-CAM~\cite{selvaraju2017grad} to visualize the attention map, which shows the most influential parts of an input that result in the classifierâ€™s output.}
    \label{compare_attn}
\end{figure*}

\begin{figure*}[!t]
    \centering
    \includegraphics[width=0.95\textwidth]{figs/compare_cl.pdf}   
    \caption{Comparing the attention maps of clean and poisoned testing images when using the classifier built based on our {\name}.}
    \label{compare_clean_and_poisoned_attn}
\end{figure*}

\begin{figure*}[!t]
    \centering
    \includegraphics[width=0.95\textwidth]{figs/mask.pdf}
    \caption{Visual illustrations of reference objects from different target classes.}
    \label{reference_obj}
\end{figure*}

\section{{\name} for Multi-modal CL}
\label{multiCL}

Carlini and Terzis~\cite{carlini2022poisoning} proposed a DPBA to multi-modal CL. To craft poisoned image-text pairs, they embed the trigger into some images and create the corresponding texts following some text prompts that include the target class name (e.g., ``a photo of dog''), as illustrated in Figure~\ref{multiCLcomparison}. This attack achieves limited success rates when the pre-training dataset only includes few image-text pairs whose images include objects from the target class and whose texts include the target class name, because CL cannot semantically associate the target class name with objects in the target class. Our {\name} for multi-modal CL addresses such limitation by extending the key idea used to attack single-modal CL. 

\subsection{Crafting Poisoned Image-text Pairs}
We denote by $f_i$ and $f_r$ the feature vectors produced by the image encoder for an image embedded with trigger $e_{ti}$ and a reference image from target class $y_{ti}$. Moreover, we denote by $f_t$ the feature produced by the text encoder for a text prompt including the name of target class $y_{ti}$. Our key idea is to craft poisoned image-text pairs such that 1) $f_i$ is similar to $f_t$, and 2) $f_t$ is similar to $f_r$. Therefore, $f_i$ and $f_r$ are similar, making our attack successful. 

We craft two types of poisoned image-text pairs (called \emph{Type-I} and \emph{Type-II}) to achieve 1) and 2), respectively. Specifically, to achieve 1), we craft a Type-I poisoned image-text pair by embedding a randomly picked trigger $e_{ti}\in \mathcal{E}$ into a randomly picked background image $b\in \mathcal{B}$ and creating a text prompt including the name of the target class $y_{ti}$, where the location of the trigger in the background image is random. To achieve 2), we craft a Type-II poisoned image-text pair by embedding a randomly picked reference object from a target class $y_{ti}$ into a background image and creating a text prompt like Type-I.   The background image may be re-scaled (or cropped) if it is too small (or large) to include the reference object. A text prompt could be like ``a photo of $<$target class name$>$''. In our experiments, we use the text prompts proposed by~\cite{carlini2022poisoning}, which are publicly available. Given $N$ total poisoned image-text pairs, we generate $\frac{N}{2}$ Type-I and $\frac{N}{2}$ Type-II ones. Note that Carlini and Terzis only use $N$ Type-I poisoned image-text pairs in their attack.

\begin{table}[!t]\renewcommand{\arraystretch}{1.2}
	\fontsize{9}{10}\selectfont
	\centering
	\caption{Attacks to multi-modal CL. The pre-training dataset is Conceptual Captions~\cite{sharma2018conceptual} and the target downstream task is ImageNet100-B. CA, BA and ASR are measured in percentage (\%).}
	\setlength{\tabcolsep}{1mm}
	{
    \begin{tabularx}{\textwidth}{cM{0.8cm}M{0.8cm}M{1cm}M{0.9cm}M{0.9cm}M{0.9cm}}
	\toprule
	 \multirow{2}{*}{\makecell{Target Class}}
	 & \multicolumn{2}{c}{No Attack} & 
	 \multicolumn{2}{c}{Carlini and Terzis}
	 & \multicolumn{2}{c}{\name} \\
	 \cmidrule(lr){2-3}
	 \cmidrule(lr){4-5}
	 \cmidrule(lr){6-7}
	 & {CA} & {ASR} & {BA} & {ASR} & {BA} & {ASR} \\
	 \midrule
	 \multicolumn{1}{c|}{Street Sign} &\multirow{5}{*}{48.4} &1 &48.3 &94 &49 &\textbf{97.7} \\
	 \multicolumn{1}{c|}{Ski Mask} & &1.4 &48.5 &96 &48.6 &\textbf{96.6} \\
	 \multicolumn{1}{c|}{Rottweiler} & &1.7 &48.6 &0 &48.9 &\textbf{57} \\
	 \multicolumn{1}{c|}{Komondor} & &0.3 &48.9 &0 &48.8 &\textbf{60.9} \\
	 \multicolumn{1}{c|}{Lorikeet} & &1.9 &47.7 &0.1 &48.4 &\textbf{89} \\
	 \bottomrule
\end{tabularx}
}
\label{tab_multi-modal}
\end{table}

\subsection{Experimental Setup}
When comparing {\name} with the existing attack~\cite{carlini2022poisoning} to multi-modal CL,  we use a subset of 0.5M inputs in the Conceptual Captions dataset (CC)~\cite{sharma2018conceptual} as a pre-training dataset and use CLIP~\cite{radford2021learning} as the pre-training algorithm. We only inject $0.1\%$ (i.e., 500) of poisoned image-text pairs since multi-modal CL is easier to attack than single-modal CL because an attack to multi-modal CL can exploit both images and texts. Moreover, we use a $16 \times 16$ trigger following \cite{carlini2022poisoning} for a fair comparison.

\subsection{Experimental Results}
Table~\ref{tab_multi-modal} compares our attack with Carlini and Terzis~\cite{carlini2022poisoning}, the state-of-the-art backdoor attack to multi-modal CL. Our results show that both attacks maintain the utility of the encoder. However, {\name} achieves slightly or much higher ASRs than Carlini and Terzis. Specifically, for target classes Rottweiler, Komondor, and Lorikeet, their attack achieves ASRs of around 0, while {\name} achieves large ASRs. This is because the pre-training dataset includes few image-text pairs related to these target classes. As a result, Carlini and Terzis can not semantically associate the target class name with objects in the target class, leading to poor attack performance.

\begin{table}
    \fontsize{9}{10}\selectfont
    \centering
    \caption{CorruptEncoder can simultaneously attack all the downstream tasks that contain the target class (e.g., Hunting Dog).}
    \setlength{\tabcolsep}{1mm}
    \begin{tabularx}{0.84\textwidth}{ccccccccc}
    \toprule
    \multicolumn{3}{c}{Task-1} & \multicolumn{3}{c}{Task-2} &
    \multicolumn{3}{c}{Task-3} \\
    \cmidrule(lr){1-3}
    \cmidrule(lr){4-6}
    \cmidrule(lr){7-9}
    CA & BA & ASR &
    CA & BA & ASR &
    CA & BA & ASR \\
    \midrule
    60.8 &61.2 &89.9 &63.0 &63.3 &92.7 &64.5 &64.2 &90.4 \\
    \bottomrule
    \end{tabularx}
    \label{multitasks}
\end{table}

\begin{table}
    \fontsize{8.5}{10}\selectfont
    \centering
    \caption{ASRs (\%) of CorruptEncoder with different pre-training datasets. Following our default setting, the target downstream task is ImageNet100-B.}
    \setlength{\tabcolsep}{1mm}
    \begin{tabularx}{0.85\textwidth}{ccccccc}
    \toprule
    \multirow{2}{*}{}& 
    \multicolumn{2}{c}{Another-ImageNet100} &
    \multicolumn{2}{c}{MSCOCO} &
    \multicolumn{2}{c}{SUN397} \\
    \cmidrule(lr){2-3}
    \cmidrule(lr){4-5}
    \cmidrule(lr){6-7}
    & BA & ASR & BA & ASR & BA & ASR \\
    \midrule
    \multicolumn{1}{c|}{\makecell{MoCo-v2}} &52.1 &96.9 &56.9 &93.4 &48.4 &93.2 \\
    \multicolumn{1}{c|}{\makecell{MSF}} &54.1 &91.0 &58.0 &93.5 &49.6 &99.8 \\
    \bottomrule
    \end{tabularx}
    \label{more}
\end{table}

\begin{table}[!t]\renewcommand{\arraystretch}{1.3} 
	\fontsize{9}{10}\selectfont
	\centering
    \caption{Detection performance of PatchSearch. TPR (\%) indicates the fraction of poisoned pre-training images filtered out by PatchSearch. ASR (\%) indicates the original attack performance.}
    {\setlength{\tabcolsep}{1mm}
    {
    \begin{tabular}{cccccc}
    \toprule
    \multicolumn{2}{c}{SSL-Backdoor} 
    & \multicolumn{2}{c}{Ours (Patch)}
    & \multicolumn{2}{c}{Ours (Blended)} \\
    \cmidrule(lr){1-2}
    \cmidrule(lr){3-4}
    \cmidrule(lr){5-6}
    ASR & TPR & ASR& TPR & ASR & TPR
    \\
    \midrule
    14.3 &83.7 &89.9 &37.1 &71.7 &2.7\\
    \bottomrule
    \end{tabular}
    }
        \vspace{-2mm}
    \label{patchsearch}
    }
\end{table}

\section{Potential Limitations}

Our attack relies on some reference images/objects being correctly classified by the downstream classifier. Since we consider the worst-case attack where the attacker only has a few reference objects (e.g., 3), our attack may fail if these reference objects mainly contain common features shared by different classes. The proposed CorruptEncoder+ uses more reference images (i.e., support reference images) to improve the attack performance. We believe it's an interesting future work to explore what makes a good reference image/object.

\section{Additional Results}
\subsection{The advantage of attacking a target class} Our attack aims to backdoor a pre-trained encoder such that \textbf{any downstream classifier} built based on the backdoored encoder will predict trigger-embedded images as the target class as long as the downstream task (e.g., arbitrary animal classification) contains the target class (e.g., dog). Different from backdoor attacks to supervised learning, an attacker does not need to know classes in the downstream task. In our threat model, the attacker only needs to randomly pick a target class and obtain a few (e.g., 3) reference images from it. After poisoning the pre-training dataset, the attacker can compromise any downstream task that contains the target class. In our experiments, we evaluate one downstream task for each choice of target class following the previous works. To better illustrate our idea, we randomly sample three downstream tasks containing the target class from ImageNet. The encoder compromised by our attack results in ASRs of 89.9\%, 91.6\%, and 90.1\% for them, as shown in Table~\ref{multitasks}. In other words, our attack can simultaneously attack all three downstream tasks.

\subsection{More Attack Results on Different Pre-training Datasets}

Our attack is generalizable to different pre-training settings. In Figures 5(a) and 5(c) of our paper, we show that CorruptEncoder is agnostic to different sizes of pre-training datasets and CL algorithms. In Table~\ref{more}, we further evaluate CorruptEncoder with diverse pre-training datasets, such as a non-object-centric dataset (MSCOCO) and a domain-specific dataset (SUN397). Our results show that different pre-training datasets have small impacts on the attack performance, while they produce pre-trained encoders with different clean utilities.

\subsection{Defense Results against PatchSearch}

PatchSearch~\cite{tejankar2023defending} is the state-of-the-art defense to detect patch-based backdoor attacks. The key idea is to check if each sample in the pre-training dataset contains a small patch that behaves similarly to the trigger. In particular, it searches the whole pre-training dataset for trigger-embedded samples and removes them from the set.

Table~\ref{patchsearch} compares the detection performance of PatchSearch in filtering out poisoned pre-training images of different attacks. We observe that while our attack achieves a much larger ASR than SSL-Backdoor, the poisoned images are not easy to detect. In particular, only 37.1\% of poisoned pre-training images are detected by PatchSearch. The reason is that the irrelevant backgrounds of our attack introduce diverse features to the trigger-embedded patches.

Moreover, we found that an adaptive attacker can extend CorruptEncdoer to bypass the PatchSearch. Instead of using a patch trigger, \textbf{we embed a large but relatively invisible trigger (e.g., Blended Attack~\cite{chen2017targeted}) within the rectangle region excluding the reference object}. From Table~\ref{patchsearch}, only 2.7\% (17 out 650) of poisoned images are detected for this advanced attack. We believe it's an interesting future work to develop a stronger defense that can defend pre-trained encoders against our attack for all types of triggers.
