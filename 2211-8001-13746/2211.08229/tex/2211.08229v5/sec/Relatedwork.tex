\vspace{-1mm}
\section{Related Work}
\vspace{-1mm}

\myparatight{CL} Single-modal CL~\cite{chen2020improved,chen2020simple,caron2020unsupervised,koohpayegani2021mean,li2021prototypical} uses images to pre-train an image encoder that outputs similar (or dissimilar) feature vectors for two augmented views of the same (or different) pre-training image. Multi-modal CL~\cite{radford2021learning,jia2021scaling} uses image-text pairs to jointly pre-train an image encoder and a text encoder such that the image encoder and text encoder output similar (or dissimilar) feature vectors for image and text from the same (or different) image-text pair. 

\myparatight{Backdoor attacks to CL}
Backdoor attacks~\cite{gu2017badnets,chen2017targeted,liu2017trojaning,liu2020reflection,li2021invisible} aim to compromise the training data or training process such that the learnt model behaves as an attacker desires. 
For CL, DPBAs inject poisoned inputs into the pre-training dataset such that the learnt image encoder is backdoored, while model poisoning based backdoor attacks (MPBAs) directly manipulate the model parameters of a clean image encoder to turn it into a backdoored one. MPBAs~\cite{jia2022badencoder,xue2022estas,tao2023distribution} are \emph{not} applicable when an image encoder is from a trusted provider while existing DPBAs~\cite{saha2022backdoor,li2022demystifying,281382,carlini2022poisoning} only achieve limited attack success rates.  
 
