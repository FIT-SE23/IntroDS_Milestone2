\section{Introduction}
Given an unlabeled pre-training dataset, contrastive learning (CL)~\cite{chen2020improved,chen2020simple,caron2020unsupervised,radford2021learning} aims to pre-train an image encoder and (optionally) a text encoder via leveraging the supervisory signals in the dataset itself. For instance, given a large amount of unlabeled images, single-modal CL, which is the major focus of this paper,
\footnote{We extend {\name} to multi-modal CL in Section~\ref{extension}.}
can learn an image encoder that produces similar (or dissimilar) feature vectors for two random augmented views created from the same (or different) image. An augmented view of an image is created by applying a sequence of \emph{data augmentation operations} to the image. Among various data augmentation operations, \emph{random cropping} is the most important one~\cite{chen2020simple}.

CL is vulnerable to \emph{data poisoning based backdoor attacks (DPBAs)}~\cite{saha2022backdoor,carlini2022poisoning}. Specifically, an attacker embeds backdoor into an encoder via injecting \emph{poisoned images} into the pre-training dataset. A downstream classifier built based on a backdoored encoder predicts an attacker-chosen class (called \emph{target class}) for any image embedded with an attacker-chosen \emph{trigger}, but its predictions for images without the trigger are unaffected. 

However, existing DPBAs achieve limited effectiveness. In particular, SSL-Backdoor~\cite{saha2022backdoor} proposes to craft a poisoned image by embedding the trigger directly into an image from the target class. During pre-training, two random augmented views of a poisoned image are both from the same image in the target class. As a result, the backdoored encoder fails to build strong correlations between the trigger and images in the target class, leading to suboptimal results. Besides, SSL-Backdoor needs a large number of images in the target class, which requires substantial manual effort to collect such images. While PoisonedEncoder~\cite{281382} uses fewer such images to achieve an improved attack performance on simple datasets, its effectiveness is limited when applied to more complex datasets (e.g., ImageNet). The limitation arises from the absence of a theoretical analysis that guides the optimization of feature similarity between a small trigger and objects in the target class. Another line of work (CTRL~\cite{li2022demystifying}) improves stealthiness by embedding an invisible trigger into the frequency domain. However, its effectiveness is sensitive to the magnitude of the trigger and the attack remains ineffective on a large dataset.

\noindent{\bf Our work:} In this work, we propose \emph{{\name}}\footnote{\url{https://github.com/jzhang538/CorruptEncoder}}, a new DPBA to CL. In {\name}, an attacker only needs to collect several images (called \emph{reference images}) from the target class and some unlabeled images (called \emph{background images}). \textbf{Our attack crafts poisoned images via exploiting the random cropping mechanism as it is the key to the success of CL} (i.e., the encoder's utility sacrifices substantially without random cropping as shown in Table~\ref{tab_defense} ``No Random Cropping"). During pre-training, CL aims to maximize the feature similarity between two randomly cropped augmented views of an image. Therefore, if one augmented view includes (a part of) a \emph{reference object} and the other includes the trigger, then maximizing their feature similarity would learn an encoder that produces similar feature vectors for the reference object and any trigger-embedded image. Therefore, a downstream classifier would predict the same class (i.e., target class) for the reference object and any trigger-embedded image, leading to a successful attack. To this end, {\name} introduces a new strategy to create a poisoned image as follows: 1) randomly sample a reference object and a background image, 2) re-scale or crop the background image if needed, 3) embed the reference object and the trigger into the background image at certain locations. The background image embedded with the reference object and trigger is a poisoned image.

\begin{figure}
    \centering
    \includegraphics[width=0.5\textwidth]{figs/intro.pdf}
    \caption{Reference image (left) vs. reference object (right).}
    \label{reference}
    \vspace{-2mm}
\end{figure}

The \emph{key insights} of crafting poisoned inputs via embedding reference object and trigger into random background images are \emph{three-folds}. \textbf{(1)} We only need a few images from the target class for the attack. \textbf{(2)} Embedding reference object (instead of the reference image) into different background images can avoid maximizing the feature similarity between the trigger and the same background in the reference image (e.g., gray area in Figure~\ref{reference}). \textbf{(3)} We can control the size (i.e., width and height) of the background image, the location of the reference object in the background image, and the location of the trigger, to explicitly optimize the attack effectiveness. In particular, when the probability that two randomly cropped views of a poisoned image respectively only include the reference object and trigger is larger, {\name} is more effective. In this work, we theoretically derive the optimal size of the background image and optimal locations of the reference object and trigger that can maximize such probability. In other words, we craft optimal poisoned images in a theory-guided manner.   

We compare existing attacks and extensively evaluate {\name} on multiple datasets. In particular, we pre-train 220+ image/image-text encoders under distinct attack settings. Our results show that {\name} achieves much higher attack success rates than existing DPBAs. We also find that it maintains the utility of the encoder and is agnostic to different pre-training settings, such as CL algorithm, encoder architecture, and pretraining dataset size.

We also explore a defense against DPBAs. Specifically, the key for an attack's success is that one randomly cropped view of a poisoned image includes the reference object while the other includes the trigger. Therefore, we propose \emph{localized cropping}, which crops two close regions of a pre-training image as augmented views during pre-training. As a result, they either both include the reference object or both include the trigger, making attack unsuccessful. Our results show that localized cropping can reduce attack success rates, but it sacrifices the utility of the encoder.
