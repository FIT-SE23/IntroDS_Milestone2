\vspace{-1mm}
\section{Defense}
\vspace{-1mm}

\myparatight{Localized cropping} Existing defenses (e.g., \cite{wang2019neural,jia2021intrinsic,xu2021detecting}) against backdoor attacks were mainly designed for supervised learning, which are insufficient for CL~\cite{jia2022badencoder}. While~\cite{feng2023detecting} proposes DECREE to detect backdoored encoders, it only focuses on the backdoor detection for a pre-trained encoder. Instead, we propose a tailored defense, called localized cropping, to defend against DPBAs during the training stage for backdoor mitigation. The success of {\name} requires that one randomly cropped view of a poisoned image includes the reference object and the other includes the trigger. Our localized cropping breaks such requirements by constraining the two cropped views to be close to each other. Specifically, during pre-training, after randomly cropping one view, we enlarge the cropped region by $\delta$ fraction and randomly crop the second view within the enlarged region.  As a result, two randomly cropped views will both include the reference object,  trigger, or none of them.  

\begin{table}[t!]
\vspace{-2mm}
\fontsize{8}{10}\selectfont
\centering
\setlength{\tabcolsep}{1mm}
{
\begin{tabularx}{1\textwidth}{cccM{0.8cm}M{0.8cm}M{0.8cm}M{0.8cm}}
\toprule
\multirow{2}{*}{{Defense}}
 & \multicolumn{2}{c}{No Attack} 
 & \multicolumn{2}{c}{\name} & \multicolumn{2}{c}{\name+}\\
 \cmidrule(lr){2-3}
 \cmidrule(lr){4-5}
 \cmidrule(lr){6-7}
 & {CA} & {ASR} & {BA} & {ASR} & {BA} & {ASR} \\
 \midrule
 \multicolumn{1}{c|}{No Defense} &60.8 &0.4 & 61.2 & 89.9 &61.7 &97.8 \\
  \multicolumn{1}{c|}{ContrastiveCrop} &61.3 &0.4 &62.1 &90.8 &62 &98.5 \\
  \multicolumn{1}{c|}{No Other Data Augs} &44.2 &0.3 &44.7 &69.3 &44.2 &75.7 \\
  \multicolumn{1}{c|}{No Random Cropping} &32.4 &2.2 &31.1 &2 &31.9 &1.5 \\
 \multicolumn{1}{c|}{CompRess (5\%)$^\dagger$} &49.5 &0.9 &49.4 &1.1 &49.9 &0.9 \\
 \multicolumn{1}{c|}{CompRess (20\%)$^\dagger$} &58.2 &0.9 &58.7 &1 &58.6 &1.1 \\
  \midrule
 \multicolumn{1}{c|}{\dname} &56.2 &0.9 &56.3 &0.9 &56.1 &0.8 \\
 \bottomrule
  \end{tabularx}
  }
\caption{Defense results (\%). $^\dagger$ indicates an extra clean pre-training dataset is used.}
\vspace{-2mm}
\label{tab_defense}
\end{table}

\myparatight{Experimental results} Table~\ref{tab_defense} shows the results of defenses tailored for backdoor mitigation in CL. We conduct experiments following our default settings. ``No Defense'' means MoCo-v2 uses its original data augmentation operations;
``No Random Cropping'' means random cropping is \emph{not} used while ``No Other Data Augs'' means data augmentations except for random cropping are \emph{not} used;
``ContrastiveCrop'' means replacing random cropping with the advanced semantic-aware cropping mechanism~\cite{peng2022crafting} and ``Localized Cropping'' means replacing random cropping with our localized cropping ($\delta=0.2$).
CompRess Distillation~\cite{saha2022backdoor} uses a clean pre-training dataset (e.g., a subset of the pre-training dataset) to distill a (backdoored) encoder. 
 
ContrastiveCrop~\cite{peng2022crafting} uses semantic-aware localization to generate augmented views that can avoid false positive pairs. Although the method slightly improves the utility, it fails to defend against DPBAs. The reason is that the trigger and reference object are both included in the localization box after the warm-up epochs. Removing other data augmentations (e.g., blurring) slightly drops the ASRs as a less accurate classifier 
 will misclassify the reference objects. Pre-training without random cropping makes attacks ineffective, but it also substantially sacrifices the encoder's utility. Figure~\ref{ablation2}(c) in the Appendix further shows that random cropping with non-default parameters only reduces ASR when thereâ€™s a large utility drop.

Our localized cropping can reduce ASRs. Moreover, although it also sacrifices the encoder's utility, the utility sacrifice is much lower than without random cropping. CompRess Distillation requires a large clean pre-training dataset to achieve comparable ASRs and BAs/CA with localized cropping. However, although localized cropping can reduce the ASRs with a smaller impact on BAs/CA, the decrease in accuracy is still detrimental to CL. Table~\ref{tab_delta} in Appendix shows that localized cropping is less effective as $\delta$ increases.

