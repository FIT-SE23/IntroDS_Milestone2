\vspace{-2mm}
\section{Threat Model}
\vspace{-2mm}
\myparatight{Attacker's goal} Suppose an attacker selects ${T}$ downstream tasks to compromise, called \emph{target downstream tasks}. For each target downstream task $t$, the attacker picks $s_t$ target classes, where $t=1,2,\cdots,T$. We denote by $y_{ti}$  the $i$th target class for the $t$th target downstream task. For each target class $y_{ti}$, the attacker selects a trigger ${e}_{ti}$. The attacker aims to inject a poisoned dataset $\mathcal{D}_p$ into a pre-training dataset $\mathcal{D}$ such that the learnt, backdoored image encoder achieves two goals: \emph{effectiveness goal} and \emph{utility goal}. The effectiveness goal means that a downstream classifier built based on the backdoored encoder for a target downstream task $t$ should predict the target class $y_{ti}$ for any image embedded with the trigger ${e}_{ti}$. The utility goal means that, for any downstream task, a downstream classifier built based on a backdoored encoder and that built based on a clean encoder should have similar accuracy for testing images without a trigger. 

\myparatight{Attacker's capability and background knowledge} We assume the attacker can inject $N$ poisoned images ($|\mathcal{D}_p|=N$) into the pre-training dataset $\mathcal{D}$. The provider often collects an unlabeled pre-training dataset from the Internet. Therefore, the attacker can post its poisoned images on the Internet, which could be collected by a provider as a part of its pre-training dataset. Moreover, we assume the attacker has access to 1) a small number (e.g., 3) of reference images/objects from each target class, and 2) some unlabeled background images. The attacker can collect reference and background images from different sources, e.g., the Internet. We assume the reference images are \emph{not} in the training data of downstream classifiers to simulate practical attacks. Moreover, we assume the attacker does not know the pre-training settings and can not manipulate the pre-training process. It is noted that previous DPBAs~\cite{saha2022backdoor,li2022demystifying} use several hundreds of reference images to launch their attacks, while we assume the attacker has only a small number (e.g., 3) of reference objects for a stronger attack.

\vspace{-1mm}
\section{\name}
\vspace{-1mm}
\label{singlemethod}
Our key idea is to craft poisoned images such that the image encoder learnt based on the poisoned pre-training dataset produces similar feature vectors for any image embedded with a trigger ${e}_{ti}$ and a reference object in the target class $y_{ti}$. Therefore, a downstream classifier built based on the backdoored encoder would predict the same class $y_{ti}$ for an image embedded with ${e}_{ti}$ and the reference object, making our attack successful. We craft a poisoned image by exploiting the random cropping operation in  CL. Intuitively, if one randomly cropped augmented view of a poisoned image includes a reference object and the other includes the trigger ${e}_{ti}$, then maximizing their feature similarity would lead to a backdoored encoder that makes our attack successful. Thus, {\bf our goal is to craft a poisoned image, whose two randomly cropped views respectively include a reference object and trigger with a high probability}. 

Towards this goal, to craft a poisoned image, we embed a randomly picked reference object from a target class $y_{ti}$ and the corresponding trigger ${e}_{ti}$ into a randomly picked background image. Given a reference object and a trigger, we \emph{theoretically} analyze the optimal size of the background image, the optimal location of the reference object in the background image, and the optimal location of the trigger, which can maximize the probability that two randomly cropped views of the poisoned image respectively include the reference object and trigger. Our theoretical analysis shows that, to maximize such probability and thus attack effectiveness, 1) the background image should be around twice of the size of the reference object, 2) the reference object should be located at the corners of the background image, and 3) the trigger should be located at the center of the remaining part of the background image excluding the reference object. 

\begin{figure}[!t]
    \centering
    \includegraphics[width=\textwidth]{figs/bottom-top.pdf}
    \caption{Illustration of the optimal size ($b_w^*$, $b_h^*$) of the background image and optimal locations ($(o_x^*, o_y^*)$ and $(e_x^*,e_y^*)$) of the reference object and trigger in the background image when crafting a poisoned image.}
    \label{layout}
    \vspace{-1mm}
\end{figure}

\subsection{Crafting Poisoned Images}
We denote by $\mathcal{O}$, $\mathcal{B}$, and $\mathcal{E}$ the set of reference objects, background images, and triggers, respectively. 
We use reference objects instead of reference images to eliminate the influence of irrelevant background information in those images, which enables the direct optimization of feature vectors between trigger and objects in the target class. To craft a poisoned image, we randomly pick a reference object  $o \in \mathcal{O}$ and a background image $b \in \mathcal{B}$; and  $e \in \mathcal{E}$ is the trigger corresponding to the target class of $o$. If the background image $b$ is too small (or large), we re-scale (or crop) it. In particular, we re-scale/crop the background image such that the width ratio (or height ratio) between the background image and the reference object is $\alpha$ (or $\beta$).   Then, we embed the reference object into the background image at location $(o_x, o_y)$ and embed the trigger into it at location $(e_x, e_y)$ to obtain a poisoned image, where the trigger does not intersect with the reference object. Algorithm~\ref{algo} and~\ref{rc} in Appendix show the pseudocode of crafting poisoned images.

Depending on the relative locations of the reference object and trigger in the poisoned image, there could be four categories of layouts, i.e., \emph{left-right}, \emph{right-left}, \emph{bottom-top} and \emph{top-bottom}. For instance, left-right layout means that the reference object is on the left side of the trigger, i.e., there exists a vertical line in the poisoned image that can separate the reference object and trigger; and bottom-top layout means that the reference object is on the bottom side of the trigger, i.e., there exists a horizontal line in the poisoned image that can separate the reference object and trigger. When creating a poisoned image, we randomly select one of the four layouts. 

\vspace{-1mm}
\subsection{Theoretical Analysis}
\label{theoreticanalysis}
Given a reference object $o$ and a trigger $e$, our {\name} has three key parameters when crafting a poisoned image: 1) size of the background image, 2) location of the reference object, and 3) location of the trigger. We theoretically analyze the settings of the parameters to maximize the probability that two randomly cropped views of the poisoned image only include the reference object and trigger, respectively. Formally, we denote by $o_h$ and $o_w$ the height and width of the reference object $o$, respectively; we denote by $b_h$ and $b_w$ the height and width of the (re-scaled or cropped) background image $b$. Moreover, we denote $\alpha=b_w/o_w$ and $\beta=b_h/o_h$. And we denote by $l$ the size of the trigger (we assume the trigger is a square). 

Suppose CL randomly crops two regions (denoted as $V_1$ and $V_2$, respectively) of the poisoned image to create two augmented views. To simplify the illustration, we assume the regions are squares and they have the same size $s$ (the theorem still holds if the two views do not have the same size). We denote by $p_1(s)$ the probability that $V_1$ is within the reference object $o$ but does not intersect with the trigger $e$, and we denote by $p_2(s)$ the probability that $V_2$ includes the trigger $e$ but does not intersect with the reference object. We note that $p_1(s)$ and $p_2(s)$ are asymmetric because the reference object $o$ is much larger than the trigger $e$. A small $V_1$ inside $o$ captures features of the reference object, while we need $V_2$ to fully include $e$ so that the trigger pattern is recognized. Formally, $p_1(s)$ and $p_2(s)$ are defined as follows: 
\begin{align}
    p_1(s) &= \text{Pr}\{(V_1 \subset o) \cap (V_1 \cap e=\emptyset)\}, \\
    p_2(s) &= \text{Pr}\{(V_2 \supset e) \cap (V_2 \cap o=\emptyset)\}.
\end{align}
 
$p_1(s)\cdot p_2(s)$ is the probability that two randomly cropped views with size $s$ only include the reference object and trigger, respectively. 
The region size $s$ is uniformly distributed between 0 and $S=\min\{b_w,b_h\}$. Therefore, the total probability $p$ that two randomly cropped views of a poisoned image respectively only include the reference object and trigger is as follows:
\begin{align}
\label{totalp}
    p = \frac{1}{S}\int_{s \in (0,S]} p_1(s)p_2(s) \text{d}s. 
\end{align}
Our goal is to find the parameter settings--including the size $b_h$ and $b_w$ of the background image, location $(o_x, o_y)$ of the reference object, and location $(e_x, e_y)$ of the trigger to maximize probability $p$. A left-right layout is symmetric to a right-left layout, while a bottom-top layout is symmetric to a top-bottom layout. Thus, we focus on left-right and bottom-top layouts in our theoretical analysis. 
Figure~\ref{layout} shows the optimal parameter settings for left-right layout and bottom-top layout derived in the following. 

\begin{figure}
    \centering
    \subfloat[Left-right Layout]{\includegraphics[width=0.46\textwidth]{figs/left_right.png}}
    \subfloat[Bottom-top Layout]{
    \includegraphics[width=0.46\textwidth]{figs/bottom_top.png}}
    \caption{The probability $p$ as a function of $b_w/o_w$ for left-right layout and $b_h/o_h$ for bottom-top layout. The curves are consistent with our empirical results of ASRs in Figure~\ref{ablation3-1}(a).}  
    \label{simulation}
    \vspace{-1mm}
\end{figure}

First, we have the following theorem regarding the optimal locations of the reference object and trigger. 

\begin{theorem}[Locations of Reference Object and Trigger]
Suppose left-right layout or bottom-top layout is used.  $(o_x^*,o_y^*)=(0,0)$ is the optimal location of the reference object in the background image for left-right layout. $(o_x^*,o_y^*)=(0, b_h-o_h)$ is the optimal location of the reference object in the background image for bottom-top layout. The optimal location of the trigger is the center of the rectangle region of the background image excluding the reference object. Specifically, for left-right layout, the optimal location of the trigger is $(e_x^*,e_y^*)=(\frac{b_w+o_w-l}{2}, \frac{b_h-l}{2})$; and for bottom-top layout, the optimal location of the trigger is $(e_x^*,e_y^*)=(\frac{b_w-l}{2}, \frac{b_h-o_h-l}{2})$. In other words, given any size $b_w\geq o_w$ and $b_h \geq o_h$ of the background image, the optimal location $(o_x^*,o_y^*)$ of the reference object and the optimal location  $(e_x^*,e_y^*)$ of the trigger maximize the probability $p$ defined in Equation~\ref{totalp}.
\end{theorem}
\vspace{-3mm}
\begin{proof}
See Appendix~\ref{theorem1}.
\end{proof}
\vspace{-2mm}

Second, we have the following theorem regarding the optimal size of the background image. 

\begin{figure}
    \centering
    \includegraphics[width=0.85\textwidth]{figs/sup_new.pdf}
    \caption{{\name}+ uses support poisoned images to pull reference objects and other images in the target class close in the feature space so that the reference object can be correctly classified by a downstream classifier.}
    \label{supportpoison}
    \vspace{-2mm}
\end{figure}

\begin{theorem} [Size of Background Image]
\label{theoremsize}
Suppose the optimal locations of the reference object and trigger are used. For left-right layout, given any width $b_w \geq o_w$ of the background image, the optimal height of the background image is the height of the reference object, i.e., $b_h^*=o_h$. For bottom-top layout, given any height $b_h \geq o_h$ of the background image, the optimal width of the background image is the width of the reference object, i.e., $b_w^*=o_w$.  Such optimal size maximizes the probability $p$ defined in Equation~\ref{totalp}.
\end{theorem}
\vspace{-3mm}
\begin{proof}
See Appendix~\ref{theorem2}.
\end{proof}
\vspace{-1mm}

Theorem~\ref{theoremsize} is only about the optimal height of the background image for left-right layout and the optimal width for bottom-top layout. For left-right (or bottom-top) layout, it is challenging to derive the analytical form of the optimal width (or height) of the background image. Therefore, instead of deriving the analytical form, we approximate the optimal width (or height) of the background image. In particular, given a reference object and a trigger, we use their optimal locations in the background image and the optimal height for left-right layout (or width for bottom-top layout) of the background image; and then we numerically calculate the value of $p$ in Equation~\ref{totalp} via sampling many values of $s$ for a given width (or height) of the background image. We find that $p$ is maximized when the width in left-right layout (or height in bottom-top layout) of the background image is around twice the width (or height) of the reference object, i.e., $b_w^*\approx 2o_w$ in left-right layout (or $b_h^*\approx 2o_h$ in bottom-top layout). Figure~\ref{layout}(b) shows $p$ as a function of $\alpha=b_w/o_w$ for left-right layout and $\beta=b_h/o_h$ for bottom-top layout, where the curves correspond to input reference objects with different sizes and the trigger size $l$ is 40. 

\subsection{\name+}
Our crafted poisoned images would lead to an encoder that produces similar feature vectors for a trigger-embedded image and a reference object. However, the feature vector of a reference object $o$ may be affected by the trigger $e$ and deviate from the cluster center of its actual class. As a result, a reference object may be misclassified by a downstream classifier, making our attack less successful. To mitigate the issue, we propose \name+ that jointly optimizes the following two terms:
\begin{align}
    \max_{\mathcal{D}_p} [S_C(f_{o}, f_{e}; \theta_{\mathcal{D} \cup \mathcal{D}_p}) + \lambda \cdot S_C(f_{o}, f_{cls}; \theta_{\mathcal{D} \cup \mathcal{D}_p})], 
\end{align}
where $S_C(\cdot, \cdot)$ indicates the cosine similarity between two feature vectors and $\theta_{\mathcal{D} \cup \mathcal{D}_p}$ is the weights of the (backdoored) encoder pre-trained on the poisoned pre-training dataset. $f_{o}$, $f_{e}$ and $f_{cls}$ indicate the feature vectors of reference object $o$, trigger $e$ and the cluster center of the target class, respectively. We use $\lambda$ to balance the two terms.

The first term can be optimized by injecting poisoned images crafted by \name.
To optimize the second term, our advanced attack \name+ assumes there are additional reference images from each target class, called \emph{support reference images}. Our assumption is that maximizing the feature similarities between a reference object and support reference images can pull $f_{o}$ close to $f_{cls}$ in the feature space. Therefore, \name+ further constructs \emph{support poisoned images} by concatenating a reference image and a support reference image, as shown in Figure~\ref{supportpoison}. \textbf{The attacker can only control the ratio of support poisoned images among all poisoned inputs (i.e., $\frac{\lambda}{1+\lambda}$) to balance the two terms given no access to the training process.} Due to the random cropping mechanism, the learnt encoder would produce similar feature vectors for each reference image and support reference images, increasing the success rate of our attack as shown in Figure~\ref{ablation4}(c).
