\vspace{-1mm}
\section{Conclusion}
In this work, we propose new data poisoning based backdoor attacks (DPBAs) to contrastive learning (CL). Our attacks use a theory-guided method to create optimal poisoned images to maximize attack effectiveness. Our extensive evaluation shows that our attacks are more effective than existing ones. Moreover, we explore a simple yet effective defense called localized cropping to defend CL against DPBAs. Our results show that localized cropping can reduce the attack success rates, but it sacrifices the utility of the encoder, highlighting the need for new defense.

\myparatight{Acknowledgements} We would like to thank Zhexiao Lin from UC Berkeley for the thoughtful discussion. We also thank the anonymous reviewers for their constructive comments. This work was supported by NSF under grant No. 2112562, 1937786, and 1937787, ARO grant No. W911NF2110182, and Facebook Research Award.
