\section{Experiments}
\label{sec_exp}
\subsection{Experimental Setup}

\myparatight{Datasets}
Due to limited computing resources, we use a subset of random 100 classes of ImageNet as a pre-training dataset, which we denote as ImageNet100-A. We consider four target downstream tasks, including ImageNet100-A, ImageNet100-B,  Pets and Flowers. ImageNet100-B is a subset of another 100 random classes of ImageNet. Details of these datasets can be found in Appendix~\ref{app_dataset}. We also use ImageNet100-A as both a pre-training dataset and a downstream dataset for a fair comparison with SSL-Backdoor~\cite{saha2022backdoor}, which used the same setting. 

\myparatight{CL algorithms} We use four CL algorithms, including MoCo-v2~\cite{chen2020improved}, SimCLR~\cite{chen2020simple}, and MSF~\cite{koohpayegani2021mean} and SwAV~\cite{caron2020unsupervised}. We follow the original implementation of each algorithm. Unless otherwise mentioned, we use \textbf{MoCo-v2}. Moreover, we use \textbf{ResNet-18} as the encoder architecture by default. Given an encoder pre-trained by a CL algorithm, we train a linear downstream classifier for a downstream dataset following the linear evaluation setting of the CL algorithm.  Details can be found in Appendix~\ref{app_cl} and~\ref{trainingdownstream}.  

\begin{table}
\vspace{-2mm}
    \fontsize{8.5}{10}\selectfont
    \centering
    \setlength{\tabcolsep}{1mm}
    \begin{tabularx}{\textwidth}{ccccM{12mm}M{12mm}}
    \toprule
    \multirow{2}{*}{\makecell{Target Downstr-\\eam Task}} &
    \multirow{2}{*}{\makecell{No \\Attack}} &
    \multirow{2}{*}{\makecell{SSL-\\Backdoor}} &
    \multirow{2}{*}{CTRL} &
        \multirow{2}{*}{\makecell{Poisoned-\\Encoder}} &
        \multirow{2}{*}{\makecell{Corrupt-\\Encoder}} \\
        & & & & & \\
        \midrule
        \multicolumn{1}{c|}{ImageNet100-A} &0.4 &5.5 &28.8 &\multicolumn{1}{c|}{76.7} & \textbf{96.2} \\
        \multicolumn{1}{c|}{ImageNet100-B} &0.4 &14.3 &20.5 &\multicolumn{1}{c|}{53.2} & \textbf{89.9} \\
        \multicolumn{1}{c|}{Pets} &1.5 &4.6 &35.4 &\multicolumn{1}{c|}{45.8} &\textbf{72.1} \\
        \multicolumn{1}{c|}{Flowers} &0 &1 &18 &\multicolumn{1}{c|}{44.4} & \textbf{89} \\
        \bottomrule
    \end{tabularx}
    {\caption{ASRs (\%) of different attacks. SSL-Backdoor~\cite{saha2022backdoor} achieves low ASRs, which is consistent with their results in  FP.} \label{singleASR}}
\end{table}

\begin{table}
\vspace{-2mm}
    \fontsize{8.5}{10}\selectfont
    \centering
    \setlength{\tabcolsep}{1mm}
    \begin{tabularx}{\textwidth}{ccccM{12mm}M{12mm}}
    \toprule
    \multirow{2}{*}{\makecell{Target Downstr-\\eam Task}} &
    \multirow{2}{*}{\makecell{No \\Attack}} &
    \multirow{2}{*}{\makecell{SSL-\\Backdoor}} &
    \multirow{2}{*}{CTRL} &
    \multirow{2}{*}{\makecell{Poisoned-\\Encoder}} &
    \multirow{2}{*}{\makecell{Corrupt-\\Encoder}} \\
    & & & & & \\
    \midrule
    \multicolumn{1}{c|}{Hunting Dog} &0.4 &14.3 &20.5 &\multicolumn{1}{c|}{53.2} &\textbf{89.9} \\
    \multicolumn{1}{c|}{Ski Mask} &0.4 &14 &27.9 &\multicolumn{1}{c|}{37.6} &\textbf{84.3} \\
    \multicolumn{1}{c|}{Rottweiler} &0.3 &8 &37.8 &\multicolumn{1}{c|}{7.3} &\textbf{90.6} \\
    \multicolumn{1}{c|}{Komondor} &0 &18.3 &19.3 &\multicolumn{1}{c|}{61} &\textbf{99.4} \\
    \bottomrule
    \end{tabularx}
    \caption{ASRs (\%) for different target classes when the target downstream task is ImageNet100-B.}
    \label{targetclass}
\end{table}
\begin{table}
\vspace{-2mm}
    \fontsize{8.5}{10}\selectfont
    \centering
    \setlength{\tabcolsep}{1mm}
    \begin{tabularx}{\textwidth}{cccM{10mm}M{10mm}}
    \toprule
    & \makecell{ImageNet-\\100-A} &
    \makecell{ImageNet-\\100-B} &
    \makecell{Pets} &
    \makecell{Flowers} \\
    \midrule
    \multicolumn{1}{c|}{\makecell{No Attack (CA)}} &69.3 &60.8 &55.8 &70.8 \\
    \multicolumn{1}{c|}{\makecell{CorruptEncoder (BA)}} &69.6 &61.2 &56.9 &69.7 \\
    \bottomrule
    \end{tabularx}
    {\caption{{\name} maintains utility (\%) as poisoned images also contain meaningful features which also contribute to CL.} \label{tab_utility}}
\end{table}

\myparatight{Evaluation metrics} We evaluate \emph{clean accuracy (CA)}, \emph{backdoored accuracy (BA)}, and \emph{attack success rate (ASR)}. CA and BA are respectively the testing accuracy of a downstream classifier built based on a clean and backdoored image encoder for \emph{clean} testing images (w/o the trigger). ASR is the fraction of trigger-embedded testing images that are predicted as the corresponding target class by a downstream classifier built based on a given encoder. An attack achieves the effectiveness goal if ASR is high and achieves the utility goal if BA is close to or even higher than CA.   

\begin{figure*}
    \centering
    \subfloat[Pre-training dataset size]{\includegraphics[width=0.3\textwidth]{graph/psize.pdf}}
    \subfloat[Encoder architecture]{\includegraphics[width=0.3\textwidth]{graph/net.pdf}}
    \subfloat[CL algorithm]{\includegraphics[width=0.3\textwidth]{graph/ssl.pdf}}
    \caption{Impact of pre-training settings on {\name}. }
    \label{ablation1}
    \vspace{-2mm}
\end{figure*}

\myparatight{Attack settings} By default, we consider the following parameter settings: we inject 650 poisoned images (poisoning ratio 0.5\%); an attacker selects one target downstream task and one target class (\textbf{default target classes} are shown in Table~\ref{defaultclass} in Appendix); an attacker has 3 reference images/objects for each target class, which are randomly picked from the testing set of a target downstream task/dataset; an attacker uses the place365 dataset~\cite{zhou2017places} as background images; trigger is a $40 \times 40$ patch with random pixel values;  we adopt the optimal settings for the size of a background image and location of a reference object; and for the location of trigger, to avoid being detected easily, we randomly sample a location within the center 0.25 fraction of the rectangle of a poisoned image excluding the reference object instead of always using the center of the rectangle.  Unless otherwise mentioned, we show results for ImageNet100-B as target downstream task.

\myparatight{Baselines} We compare our {\name} with SSL-Backdoor~\cite{saha2022backdoor}, CTRL~\cite{li2022demystifying} and PoisonedEncoder (PE)~\cite{281382}. We further show the benefits of {\name+} over {\name} in our ablation study (Figure~\ref{ablation4}(c)). SSL-Backdoor and CTRL use 650 reference images (0.5\%) randomly sampled from the dataset of a target downstream task. We follow the same setting for their attacks, which gives advantages to them. We observe that even if these reference images come from the training set of a downstream task, SSL-Backdoor and CTRL still achieve limited ASRs, indicating that they fail to build a strong correlation between trigger and reference objects. For PE, we use the \emph{same} reference images as {\name} for a fair comparison. Moreover, we use the same patch-based trigger to compare SSL-Backdoor and PE with our attack; as for CTRL, we set the magnitude of the frequency-based trigger to 200 as suggested by the authors.

\subsection{Experimental Results} \label{exp}
\myparatight{{\name} is more effective than existing attacks} Table~\ref{singleASR} shows the ASRs of different attacks for different target downstream tasks, while Table~\ref{targetclass} shows the ASRs for different target classes when the target downstream task is ImageNet100-B. Each ASR is averaged over \emph{three} trials. {\name} achieves much higher ASRs than SSL-Backdoor, CTRL and PoisonedEncoder (PE) across different experiments. 
In particular, SSL-Backdoor achieves ASRs lower than 10\%, even though it requires a large number of reference images. CTRL and PE also achieve very limited ASRs in most cases. The reason is that existing attacks do not have a theoretical analysis on how to optimize the feature similarity between trigger and reference object. As a result, they fail to build strong correlations between trigger and reference object, as shown in Figure~\ref{compare_attn} in Appendix. Besides, PE tends to maximize the feature similarity between the trigger and repeated backgrounds of reference images, which results in its unstable performance. We note that SSL-Backdoor~\cite{saha2022backdoor} uses \textbf{False Positive (FP)} as the metric, which is the number (instead of fraction) of trigger-embedded testing images that are predicted as the target class. ASR is the standard metric for measuring the backdoor attack. When converting their FP to ASR, their attack achieves a very small ASR, e.g., less than 10\%.

\begin{figure}[!t]
    \centering
    \subfloat{\includegraphics[width =0.45\textwidth]{graph/ar.pdf}}
    \subfloat{\includegraphics[width =0.45\textwidth]{graph/eloc.pdf}}
    \caption{Impact of (a) $\alpha=b_w/o_w$ for left-right layout (or $\beta=b_h/o_h$ for bottom-top layout) and (b) the trigger location on {\name}.}
    \label{ablation3-1}
    \vspace{-2mm}
\end{figure}

\begin{figure}[!t]
    \vspace{-2mm}
    \centering
    \subfloat{\includegraphics[width=0.45\textwidth]{graph/p.pdf}}
    \subfloat{\includegraphics[width=0.45\textwidth]{graph/ref.pdf}}
    \caption{Impact of (a) the poisoning ratio and (b) the number of reference images on {\name}.}
    \label{ablation3-2}
    \vspace{-2mm}
\end{figure}

\begin{figure*}[!t]
    \centering
    \subfloat[Multiple target classes]{\includegraphics[width=0.28\textwidth]{graph/multi_target.pdf}\label{ablation4a}}
    \subfloat[Multiple downstream tasks]{\includegraphics[width=0.28\textwidth]{graph/multi_ds.pdf}\label{ablation4b}}
    \subfloat[\name+]{\includegraphics[width=0.35\textwidth]{graph/plus2.pdf}\label{ablation4c}}
    \caption{ASRs for multiple target classes, multiple downstream tasks, and \name+.}
    \label{ablation4}
    \vspace{-2mm}
\end{figure*}

\myparatight{{\name} maintains utility} Table~\ref{tab_utility} shows the CA
and BA of different downstream classifiers. We observe that {\name} preserves the utility of an encoder: BA of a downstream classifier is close to the corresponding CA. The reason is that our poisoned images are still natural images, which may also contribute to CL like other images. 

\myparatight{{\name} is agnostic to pre-training settings} Figure~\ref{ablation1} shows the impact of pre-training settings, including pre-training dataset size, encoder architecture, and CL algorithm, on {\name}. In Figure~\ref{ablation1}(a), we use subsets of ImageNet with different sizes and ensure that they do not overlap with ImageNet100-B for a fair comparison. Our results show that {\name} is agnostic to pre-training settings. In particular, {\name} achieves high ASRs (i.e., achieving the effectiveness goal) and BAs are close to CAs (i.e., achieving the utility goal) across different pre-training settings.

\myparatight{Empirical evaluation on the theoretical analysis} 
Recall that we cannot derive the analytical form of the optimal $\alpha^*=b_w^*/o_w$ for left-right layout (or $\beta^*=b_h^*/o_h$ for bottom-top layout). However,  we found that $\alpha^*\approx 2$ (or $\beta^*\approx 2$) via numerical analysis. Figure~\ref{ablation3-1}(a) shows the impact of $\alpha=b_w/o_w$ for left-right layout (or $\beta=b_h/o_h$ for bottom-top layout) on the attack performance. Our results show that ASR peaks when $\alpha=2$ (or $\beta=2$), which is consistent with our theoretical analysis in Section~\ref{theoreticanalysis}. 

Moreover, in Section~\ref{theoreticanalysis}, we theoretically derive the optimal locations of the reference object $o$ and trigger $e$. For ease of assessment, we fix the reference object $o$ in the optimal location while selecting trigger locations using different strategies: (1) random location in the background image $b$ (2) random location in the rectangle region of the background image $b$ excluding the reference object $o$ and (3) optimal location derived in Section~\ref{theoreticanalysis}. Figure~\ref{ablation3-1}(b) shows that the optimal trigger location leads to a larger ASR. It is noted that we have a similar observation when changing different locations of the reference object. 

\myparatight{Impact of hyperparameters of {\name}}
Figure~\ref{ablation3-2} shows the impact of poisoning ratio and the number of reference images on {\name}. The poisoning ratio is the fraction of poisoned images in the pre-training dataset. ASR quickly increases and converges as the poisoning ratio increases, which indicates that {\name} only requires a small fraction of poisoned inputs to achieve high ASRs. We also find that ASR increases when using more reference images. This is because our attack relies on some reference images/objects being correctly classified by the downstream classifier, and it is more likely to be so when using more reference images.

Figure~\ref{ablation2} in Appendix shows the impact of trigger type (white, purple, and colorful), and trigger size on {\name}. A colorful trigger achieves a higher ASR than the other two triggers. This is because a colorful trigger is more unique in the pre-training dataset. Besides, ASR is large once the trigger size is larger than a threshold (e.g., 20). Moreover, in all experiments, {\name} consistently maintains the utility of the encoder.

\myparatight{Multiple target classes and downstream tasks} 
Figure~\ref{ablation4}(a) shows the ASR of each target class when {\name} attacks the three target classes separately or simultaneously, where each target class has a unique trigger. Figure~\ref{ablation4}(b) shows the ASR of each target downstream task when {\name} attacks the three target downstream tasks separately or simultaneously, where each target downstream task uses its default target class. Our results show that {\name} can successfully attack multiple target classes and target downstream tasks simultaneously. 

\myparatight{\name+}
{\name+} requires additional support reference images to construct support poisoned images. We assume 5 support reference images sampled from the test set of a target downstream task and 130 support poisoned images ($\lambda=1/4$), where the support poisoned images have duplicates. For a fair comparison with {\name}, the total poisoning ratio is still 0.5\%. Figure~\ref{ablation4}(c) compares their ASRs for four target downstream tasks. Our results show that {\name+} can further improve ASR.  Table~\ref{tab_rs} and \ref{tab_s} in Appendix respectively show the impact of the number of support reference images and support poisoned images (i.e., $\lambda$) on {\name+}. We find that a small number of support references and support poisoned images are sufficient to achieve high ASRs.
