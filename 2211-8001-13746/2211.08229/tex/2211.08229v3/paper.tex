\documentclass[11pt]{article}
\usepackage{bbm}
\usepackage{epsfig,endnotes}
\usepackage{multirow}
\usepackage{subfig}
\usepackage{graphicx}
\usepackage{grffile}
\newcounter{copyrightbox}
\newcounter{subcopyrightbox}
\newcounter{subcopyrightbox@save}
\usepackage[font=bf]{caption}
\usepackage{color, url}
\usepackage{xspace} 
\usepackage{amsmath}
\usepackage{geometry}
\geometry{a4paper,scale=0.75}
\usepackage{changepage}
\usepackage{booktabs}
\usepackage{hyperref} 
\usepackage{cleveref}
\usepackage{hyperref}
\usepackage{url}
\usepackage{tikz}
\usepackage{comment}
\usepackage{amsmath,amssymb}
\usepackage{caption}
\usepackage{floatrow}
\floatsetup[table]{capposition=above}
\usepackage{longtable}
\usepackage{etoolbox}
\AtBeginEnvironment{longtable}{
  \addfontfeature{RawFeature=+tnum;-onum}
}
\usepackage{pdflscape}
\usepackage{colortbl}
\definecolor{Gray}{gray}{0.925}
\newcommand{\myrowcolour}{\rowcolor{Gray}}
\newcolumntype{g}{>{\columncolor{Gray}}c}
\usepackage{tabularx}
\newcolumntype{M}[1]{>{\centering\arraybackslash}m{#1}}
\usepackage{booktabs}
\usepackage{wrapfig}
\usepackage{color}
\usepackage{dsfont}
\usepackage{makecell}
\usepackage{multirow}
\usepackage{subfig}
\usepackage{bbold}
\usepackage{algorithm}  
\usepackage{algorithmicx}
\usepackage{algpseudocode}
\usepackage{amsfonts,amssymb}
\newcommand{\myparatight}[1]{\smallskip\noindent{\bf {#1}:}~}
\newcommand{\name}{\text{CorruptEncoder}}
\newcommand{\dname}{\text{Localized Cropping}}
\usepackage{amssymb,amsmath,amsthm, amsfonts}
\newtheorem{theorem}{Theorem}

\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}
\newenvironment{packeditemize}{\begin{list}{$\bullet$}{\setlength{\itemsep}{0.2pt}\addtolength{\labelwidth}{-4pt}\setlength{\leftmargin}{\labelwidth}\setlength{\listparindent}{\parindent}\setlength{\parsep}{1pt}\setlength{\topsep}{0pt}}}{\end{list}}
\AtBeginDocument{
  \providecommand\BibTeX{{
    \normalfont B\kern-0.5em{\scshape i\kern-0.25em b}\kern-0.8em\TeX}}}



\begin{document}
\begin{center}
{\LARGE{\bf{\name: Data Poisoning based Backdoor Attacks to Contrastive Learning}}}
\begin{tabular}{cccc}
&&&\\
Jinghuai Zhang &&& Hongbin Liu \\
Duke University &&& Duke University \\
jinghuai.zhang@duke.edu &&& hongbin.liu@duke.edu \\
\\
Jinyuan Jia &&& Neil Zhenqiang Gong \\
UIUC &&& Duke University\\
jinyuan@illinois.edu &&& neil.gong@duke.edu\\
\end{tabular}
\end{center}

\begin{abstract}
Contrastive learning (CL) pre-trains general-purpose encoders using an unlabeled pre-training dataset, which consists of images  or image-text pairs. 
CL is vulnerable to {data poisoning based backdoor attacks (DPBAs)}, in which an attacker injects {poisoned inputs} into the pre-training dataset so the encoder is backdoored. 
However, existing DPBAs achieve limited effectiveness. In this work, we propose new DPBAs called {\name} to CL.  {\name} uses a theory-guided method to create optimal poisoned inputs to maximize attack effectiveness. Our experiments show that  {\name} substantially outperforms existing DPBAs. 
In particular, {\name} is the first DPBA that achieves \textbf{more than 90\%} attack success rates  with only a few (3) reference images and a small poisoning ratio (0.5\%). 
Moreover, we also propose a defense, called {localized cropping}, to defend against DPBAs. Our results show that our defense can reduce the effectiveness of DPBAs, though it slightly sacrifices the utility of the encoder.
\end{abstract}

\section{Introduction}
Given an unlabeled pre-training dataset, contrastive learning (CL)~\cite{chen2020improved,chen2020simple,caron2020unsupervised,koohpayegani2021mean,li2021prototypical,radford2021learning} aims to pre-train an image encoder and (optionally) a text encoder via leveraging the supervisory signals in the dataset itself. For instance, given a large amount of unlabeled images, single-modal CL, which is the major focus of this paper,\footnote{We extend {\name} to multi-modal CL in Section~\ref{extension}.} can learn an image encoder that produces similar (or dissimilar) feature vectors for two random augmented views created from the same (or different) image. An augmented view of an image is created by applying a sequence of \emph{data augmentation operations} to the image. Among the various data augmentation operations, \emph{random cropping} is the most important one~\cite{chen2020simple}. 

CL is vulnerable to \emph{data poisoning based backdoor attacks (DPBAs)}~\cite{saha2022backdoor,carlini2022poisoning}. Specifically, an attacker embeds backdoor into an encoder via injecting \emph{poisoned images} into the pre-training dataset.  A downstream classifier built based on a backdoored encoder predicts an attacker-chosen class (called \emph{target class}) for any image embedded with an attacker-chosen \emph{trigger}, but its predictions for images without the trigger are unaffected. However, existing DPBAs  achieve limited effectiveness. In particular, Saha et al.~\cite{saha2022backdoor} proposed to craft a poisoned image by embedding the trigger into an image from the target class. During pre-training, two random augmented views of a poisoned image are both from an image in the target class. As a result, the backdoored encoder does not build strong correlations between the trigger and images in the target class, leading to suboptimal attack effectiveness. Moreover, Saha et al. needs a large number of images in the target class, which requires substantial manual  effort to collect and label such images. 

\begin{wrapfigure}{r}{0.36\textwidth}
    \centering
    \includegraphics[width=\textwidth]{figs/intro.pdf}
    \caption{Reference image vs. reference object.} 
    \label{reference}
\end{wrapfigure}

\noindent{\bf Our work:} In this work, we propose \emph{{\name}}, a new DPBA to CL. In {\name}, an attacker only needs to collect several images (we call them \emph{reference images}) from the target class and some unlabeled images (called \emph{background images}).  Our attack crafts poisoned images via exploiting the random cropping mechanism, which is the key of CL. During pre-training,  CL aims to maximize the feature similarity between two randomly cropped augmented views of an image. Therefore, if one augmented view includes  (a part of) a \emph{reference object} and the other includes the trigger, then maximizing the feature similarity between them would learn an encoder that produces similar feature vectors for the reference object and any trigger-embedded image. Therefore, a downstream classifier would predict the same class (i.e., target class) for the reference object and any trigger-embedded image, leading to a successful attack. A reference object is the object  in a reference image as shown in Figure~\ref{reference}.  

Therefore, {\name} creates a poisoned image as follows: 1) randomly sample a reference object and a background image, 2) re-scale or crop the background image if needed, 3) embed the reference object into the background image at a certain location, and 4) embed the trigger into the background image at a certain location. The background image embedded with reference object and trigger is a poisoned image. 

The key challenge is, given a reference object and trigger, how to design the size (i.e., width and height) of the background image, the location of the reference object in the background image, and the location of the trigger, to optimize the attack effectiveness of {\name}. In particular, when the probability that two randomly cropped views of a poisoned image  respectively only include reference object and trigger is larger, {\name} is more effective. Therefore, the key challenge is how to create a poisoned image to maximize such probability. We address this challenge via \emph{theoretical analysis}. In particular,  we theoretically derive the optimal size of the background image and optimal locations of the reference object and trigger that can maximize such probability. For instance, when we place the reference object and trigger in a \emph{left-right layout}, i.e., there exists a vertical line that separates them, the optimal height of the background image is the height of the reference object, the optimal width of the background image is roughly twice of the width of the reference object, the optimal location of the reference object is the top-left corner, and the optimal location of the trigger is the center of the remaining part of the background image excluding the reference object.  {\name} uses such theory-guided way to craft optimal poisoned images. 

We extensively evaluate {\name} on multiple datasets. Our results show that {\name} achieves much higher attack success rates than existing DPBAs. Moreover,  {\name} maintains the utility of the encoder, i.e., a downstream classifier built upon a clean image encoder and a downstream classifier built upon our backdoored image encoder achieve similar testing accuracy for images without trigger.  We also find that {\name} is agnostic to the pre-training settings such as CL algorithm, encoder architecture, and pre-training dataset size. 

We also explore a defense against DPBAs. Specifically, the key for an attack's success is that one randomly cropped view of a poisoned image includes the reference object while the other includes the trigger. Therefore, we propose \emph{localized cropping}, which crops two close regions of a pre-training image as augmented views during pre-training. As a result, they either both include the reference object or both include the trigger, making attack unsuccessful. Our experimental results show that localized cropping substantially reduces attack success rates.


\section{Threat Model}
\myparatight{Attacker's goal} Suppose an attacker selects ${T}$ downstream tasks to compromise, called \emph{target downstream tasks}. For each target downstream task $t$, the attacker picks $s_t$ target classes, where $t=1,2,\cdots,T$. We denote by $y_{ti}$  the $i$th target class for the $t$th target downstream task. For each target class $y_{ti}$, the attacker selects a trigger ${e}_{ti}$. The attacker aims to inject poisoned images into a pre-training dataset such that the learnt, backdoored image encoder achieves two goals: \emph{effectiveness goal} and \emph{utility goal}. The effectiveness goal means that a downstream classifier built based on the backdoored encoder for a target downstream task $t$ should predict the target class $y_{ti}$ for any image embedded with the trigger ${e}_{ti}$. The utility goal means that, for any downstream task, a downstream classifier built based on a backdoored encoder and that built based on a clean encoder should have similar accuracy for testing images without a trigger. 

\myparatight{Attacker's capability and background knowledge} We assume the attacker can inject $N$ poisoned images into the pre-training dataset. A provider often collects a pre-training dataset from the  Internet. Therefore, the attacker can post its poisoned images on the Internet, which could be collected by a provider as a part of its pre-training dataset. Moreover, we assume the attacker has access to 1) a small number (e.g., 3) of reference images/objects from each target class, and 2) some unlabeled, arbitrary background images. The attacker can collect the reference and background images from different sources, e.g., the Internet. We assume that the reference images are \emph{not} in the training data of downstream classifiers to simulate practical attacks. Moreover, we assume the attacker does not know the pre-training settings such as the CL algorithm and the encoder architecture. 


\section{\name}
\label{singlemethod}
\subsection{Overview}
 Our key idea is to craft poisoned images such that the image encoder learnt based on the poisoned pre-training dataset produces similar feature vectors for any image embedded with a trigger ${e}_{ti}$ and a reference object in the target class $y_{ti}$. Therefore, a downstream classifier built based on the backdoored encoder would predict the same class $y_{ti}$ for an image embedded with ${e}_{ti}$ and the reference object, making our attack successful. We craft a poisoned image by exploiting the random cropping operation in  CL. Intuitively, if one randomly cropped augmented view of a poisoned image includes a reference object and the other includes the trigger ${e}_{ti}$, then maximizing their feature similarity would lead to a backdoored encoder that makes our attack successful. Therefore, {\bf our goal is to craft a poisoned image, whose two randomly cropped views  respectively include a reference object and trigger with a high probability}. 

Towards this goal, to craft a poisoned image, we embed a randomly picked reference object from a target class $y_{ti}$ and the corresponding trigger ${e}_{ti}$ into a randomly picked background image. Given a reference object and a trigger, we \emph{theoretically} analyze the optimal size of the background image, the optimal location of the reference object in the background image, and the optimal location of the trigger, that can maximize the probability that two randomly cropped views of the poisoned image respectively include the reference object and trigger. Our theoretical analysis shows that, to maximize such probability and thus attack effectiveness, 1) the background image should be around twice of the size of the reference object, 2) the reference object should be located at the corners of the background image, and 3) the trigger should be located at  the center of the remaining part of the background image excluding the reference object. 

\begin{algorithm}[!t]
\fontsize{8}{9}\selectfont
\caption{Crafting a Poisoned Image in {\name}}
\begin{algorithmic}[1]
\State {\bfseries Input:} A set of reference objects $\mathcal{O}$, a set of background images $\mathcal{B}$,  a set of triggers $\mathcal{E}$, $\alpha$, and $\beta$.
\State {\bfseries Output:} A poisoned image.
\State {\bfseries Note:}  $I_h$ and $I_w$ respectively represent the height and width of an image $I$.
    \State $o \gets$ randomly sample a reference object in {$\mathcal{O}$} 
    \State $b \gets$ randomly sample a background image in {$\mathcal{B}$} 
    \State $e \gets$ trigger corresponding to the target class of $o$.  

\label{rescaleimage}
    \State $b \gets \Call{RescaleAndCropBackground}{b, o, \alpha, \beta}$ \Comment{Re-scale and crop $b$ if needed}

    \label{objectlocation}
    \State $(o_x, o_y) \gets$ location of $o$ in $b$ 
    \State $b[o_x:o_x+o_w,o_y:o_y+o_h] \gets o$ \Comment{Embed $o$ to $b$}

\label{triggerlocation}
    \State $(e_x, e_y) \gets$ location of $e$ in $b$
    \State $b[e_x:e_x+e_w,e_y:e_y+e_h] \gets e$ \Comment{Embed $e$ to $b$}
    \State Return $b$
\end{algorithmic}  
\label{algo}
\end{algorithm}

\subsection{Crafting Poisoned Images} 
We denote by $\mathcal{O}$, $\mathcal{B}$, and $\mathcal{E}$  the set of reference objects, background images, and triggers, respectively. We use reference objects instead of reference images to exclude the influence of the irrelevant background information in reference images. Table~\ref{referenceimageobject} in Appendix shows that our attack is more effective using reference objects. Algorithm~\ref{algo} shows how we craft a poisoned image, while Algorithm~\ref{rc} in Appendix shows how to  re-scale and crop a background image if needed.  To craft a poisoned image, we randomly pick a reference object  $o \in \mathcal{O}$ and  a background image $b \in \mathcal{B}$; and  $e \in \mathcal{E}$ is the trigger corresponding to the target class of $o$. If the background image $b$ is too small (or large), we re-scale (or crop) it. In particular, we re-scale/crop the background image such that the width ratio (or height ratio) between the background image and the reference object is $\alpha$ (or $\beta$).   Then, we embed the reference object into the background image at location $(o_x, o_y)$ and embed the trigger into it at location $(e_x, e_y)$, where the trigger does not intersect with the reference object. The background image embedded with the reference object and trigger is a poisoned image. 

Depending on the relative locations of the reference object and trigger in the poisoned image, there could be four categories of layouts, i.e., \emph{left-right}, \emph{right-left}, \emph{bottom-top} and \emph{top-bottom}. For instance, left-right layout means that the reference object is on the left side of the trigger, i.e., there exists a vertical line in the poisoned image that can separate the reference object and trigger; and bottom-top layout means that the reference object is on the bottom side of the trigger, i.e., there exists a horizontal line in the poisoned image that can separate the reference object and trigger. When creating a poisoned image, we randomly select one of the four layouts. 

\begin{figure}[!t]
    \centering
    \includegraphics[width=0.72\textwidth]{figs/layout.pdf}
    \caption{Illustration of the optimal size ($b_w^*$, $b_h^*$) of the background image and optimal locations ($(o_x^*, o_y^*)$ and $(e_x^*,e_y^*)$) of the reference object and trigger in the background image when crafting a poisoned image.}
    \label{layout}
\end{figure}

\subsection{Theoretical Analysis}
\label{theoreticanalysis}
Given a reference object $o$ and a trigger $e$, our {\name} has three key parameters when crafting a poisoned image: 1) size of the background image, 2) location of the reference object, and 3) location of the trigger. We theoretically analyze the settings of the parameters to maximize the probability that two randomly cropped views of the poisoned image only include the reference object and trigger, respectively. Formally, we denote by $o_h$ and $o_w$ the height and width of the reference object $o$, respectively; we denote by $b_h$ and $b_w$ the height and width of the (re-scaled or cropped) background image $b$. Moreover, we denote $\alpha=b_w/o_w$ and $\beta=b_h/o_h$. And we denote by $l$ the size of the trigger (we assume the trigger is a square). 

Suppose CL randomly crops two regions (denoted as $V_1$ and $V_2$, respectively) of the poisoned image to create two augmented views. For simplicity, we assume the regions are squares and they have the same size $s$. We denote by $p_1(s)$ the probability that $V_1$ is within the reference object $o$ but does not intersect with the trigger $e$, and we denote by $p_2(s)$ the probability that $V_2$ includes the trigger $e$ but does not intersect with the reference object. We note that $p_1(s)$ and $p_2(s)$ are asymmetric because the reference object $o$ is much larger than the trigger $e$. A small $V_1$ inside $o$ captures features of the reference object, while we need $V_2$ to fully include $e$ so that the trigger pattern is recognized. Formally, $p_1(s)$ and $p_2(s)$ are defined as follows: 
\begin{align}
    p_1(s) &= \text{Pr}\{(V_1 \subset o) \cap (V_1 \cap e=\emptyset)\}, \\
    p_2(s) &= \text{Pr}\{(V_2 \supset e) \cap (V_2 \cap o=\emptyset)\}.
\end{align}

% Since the reference object and trigger do not intersect with each other when we craft a poisoned image, we have  $p_1 &= \text{Pr}\{(V_1 \subset o)\}$. 
$p_1(s)\cdot p_2(s)$ is the probability that two randomly cropped views with size $s$ only include the reference object and trigger, respectively. 
The region size $s$ is uniformly distributed between 0 and $S=\min\{b_w,b_h\}$. Therefore, the total probability $p$ that two randomly cropped views of a poisoned image respectively only include the reference object and trigger is as follows:
\begin{align}
\label{totalp}
    p = \frac{1}{S}\int_{s \in (0,S]} p_1(s)p_2(s) \text{d}s. 
\end{align}
Our goal is to find the parameter settings--including the size $b_h$ and $b_w$ of the background image, location $(o_x, o_y)$ of the reference object, and location $(e_x, e_y)$ of the trigger--to maximize probability $p$. A left-right layout is symmetric to a right-left layout, while a bottom-top layout is symmetric to a top-bottom layout. Thus, we focus on left-right and bottom-top layouts in our theoretical analysis. 
Figure~\ref{layout} illustrates the optimal parameter settings for left-right layout and bottom-top layout derived from our theoretical analysis in the following. 

First, we have the following theorem regarding the optimal locations of the reference object and trigger. 

\begin{theorem}[Locations of Reference Object and Trigger]
Suppose left-right layout or bottom-top layout is used.  $(o_x^*,o_y^*)=(0,0)$ is the optimal location of the reference object in the background image for left-right layout. $(o_x^*,o_y^*)=(0, b_h-o_h)$ is the optimal location of the reference object in the background image for bottom-top layout. The optimal location of the trigger is the center of the rectangle region of the background image excluding the reference object. Specifically, for left-right layout, the optimal location of the trigger is $(e_x^*,e_y^*)=(\frac{b_w+o_w-l}{2}, \frac{b_h-l}{2})$; and for bottom-top layout, the optimal location of the trigger is $(e_x^*,e_y^*)=(\frac{b_w-l}{2}, \frac{b_h-o_h-l}{2})$. In other words, given any size $b_w\geq o_w$ and $b_h \geq o_h$ of the background image, the optimal location $(o_x^*,o_y^*)$ of the reference object and the optimal location  $(e_x^*,e_y^*)$ of the trigger maximize the probability $p$ defined in Equation~\ref{totalp}.
\end{theorem}

\begin{proof}
See Appendix~\ref{theorem1}.
\end{proof}

Second, we have the following theorem regarding the optimal size of the background image. 

\begin{theorem} [Size of Background Image]
\label{theoremsize}
Suppose the optimal locations of the reference object and trigger are used. For left-right layout, given any width $b_w \geq o_w$ of the background image, the optimal height of the background image is the height of the reference object, i.e., $b_h^*=o_h$. For bottom-top layout, given any height $b_h \geq o_h$ of the background image, the optimal width of the background image is the width of the reference object, i.e., $b_w^*=o_w$.  Such optimal size of the background image maximizes the probability $p$ defined in Equation~\ref{totalp}.
\end{theorem}

\begin{proof}
See Appendix~\ref{theorem2}.
\end{proof}

\begin{figure}[!t]
    \centering
    \subfloat[Left-right Layout]{\includegraphics[width=0.36\textwidth]{figs/left_right.png}}
    \subfloat[Bottom-top Layout]{
    \includegraphics[width=0.36\textwidth]{figs/bottom_top.png}}
    \caption{The probability $p$ as a function of $b_w/o_w$ for left-right layout and $b_h/o_h$ for bottom-top layout.}
    \label{simulation}
\end{figure}

 Theorem~\ref{theoremsize} is only about the optimal height of the background image for left-right layout and the optimal width for bottom-top layout. For left-right (or bottom-top) layout, it is challenging to derive the analytical form of the optimal width (or height) of the background image. Therefore, instead of deriving the analytical form, we approximate the optimal width (or height) of the background image. In particular, given a reference object and a trigger, we use their optimal locations in the background image and the optimal height for left-right layout (or width for bottom-top layout) of the background image; and then we numerically calculate the value of $p$ in Equation~\ref{totalp} via sampling many values of $s$ for a given width (or height) of the background image. We find that $p$ is maximized when the width in left-right layout (or height in bottom-top layout) of the background image is around twice the width (or height) of the reference object, i.e., $b_w^*\approx 2o_w$ in left-right layout (or $b_h^*\approx 2o_h$ in bottom-top layout). Figure~\ref{simulation} shows $p$ as a function of $\alpha=b_w/o_w$ for left-right layout and $\beta=b_h/o_h$ for bottom-top layout, where the curves correspond to input reference objects with different sizes and the trigger size $l$ is 40.   


\begin{figure}[!t]
    \centering
    \includegraphics[width = 0.72\textwidth]{figs/sup_new.pdf}
    \caption{Illustration of a support poisoned image and the key idea behind {\name}+. The poisoned images may push a reference object and other images in the target class apart since it maximizes the feature similarity between reference object and trigger. We use support poisoned images to pull them together so that reference object can be correctly classified.}
    \label{supportpoison}
\end{figure}

\subsection{\name+} Our crafted poisoned images would lead to an encoder that produces similar feature vectors for a trigger-embedded image and a reference object. However, the feature vector of a reference object may be affected by the trigger and deviate from those of other images in the target class.  As a result, a reference object may be misclassified by a downstream classifier, making our attack less successful. To mitigate the issue,  \name+ leverages more reference images. Specifically, \name+ assumes there are  additional reference images from each target class, called \emph{support reference images}. Then, other than the poisoned images constructed by {\name}, \name+ further constructs poisoned images (called \emph{support poisoned images}) by concatenating a reference image and a support reference image. Figure~\ref{supportpoison} shows an example of support poisoned image. Due to the random cropping mechanism, the learnt encoder would produce similar feature vectors for a reference image and support reference images, increasing the success rate of our attack as shown in our experiments.

\section{Experiments}
\label{sec_exp}
\subsection{Experimental Setup}
\myparatight{Datasets}
Due to limited computing resources, we use a subset of random 100 classes of ImageNet as a pre-training dataset, which we denote as ImageNet100-A. We consider five target downstream tasks/datasets, including ImageNet100-A, ImageNet100-B,  Pets, Flowers, and Caltech-101. ImageNet100-B is a subset of  another 100 random classes of ImageNet.  Details of these datasets can be found in Appendix~\ref{app_dataset}. We use ImageNet100-A as both a pre-training dataset and a downstream dataset for fair comparison with~\cite{saha2022backdoor}, which used the same setting. 

\myparatight{CL algorithms} We use four CL algorithms, including MoCo-v2~\cite{chen2020improved}, SwAV~\cite{caron2020unsupervised}, SimCLR~\cite{chen2020simple}, and MSF~\cite{koohpayegani2021mean}. 
We follow the original implementation of each algorithm. Unless otherwise mentioned, we use \textbf{MoCo-v2}. Moreover, we use \textbf{ResNet-18} as the encoder architecture by default. Given an image encoder pre-trained by a CL algorithm, we train a linear downstream classifier for a downstream dataset following the linear evaluation setting of the CL algorithm.  Details can be found in Appendix~\ref{app_cl} and~\ref{trainingdownstream}.  

\myparatight{Evaluation metrics} We use \emph{clean accuracy (CA)}, \emph{backdoored accuracy (BA)}, and \emph{attack success rate (ASR)} as evaluation metrics. CA and BA are respectively the testing accuracy of a  downstream classifier built based on a clean and backdoored image encoder for \emph{clean} testing images without a trigger. ASR is the fraction of trigger-embedded testing images that are predicted as the corresponding target class by a downstream classifier built based on a backdoored image encoder. An attack achieves the effectiveness goal if  ASR is high. Moreover, an attack achieves the utility goal if BA is close to or even higher than CA.

\myparatight{Attack settings} By default, we consider the following parameter settings: we inject 650 poisoned images (poisoning ratio 0.5\%); an attacker selects one target downstream task and one target class (the \textbf{default target classes} are shown in Table~\ref{defaultclass} in Appendix); an attacker has 3 reference images/objects for each target class, which are randomly picked from the testing set of a target downstream task/dataset; an attacker  uses the place365 dataset~\cite{zhou2017places} as background images; trigger is a $40 \times 40$ patch with random pixel values;  we adopt the optimal settings for the size of a background image and location of a reference object; and for the location of trigger, to avoid being detected easily, we randomly sample a location within the center 0.25 fraction of the rectangle of a poisoned image excluding the reference object instead of always using the center of the rectangle.  Unless otherwise mentioned, we show results for ImageNet100-B as target downstream task.

Note that \cite{saha2022backdoor} uses 650 reference images randomly sampled from the dataset of a target downstream task, and we follow the setting for their attack, which gives advantages to their attack. Specifically, they create a poisoned image by embedding the trigger into a reference image. We observe that even if these reference images come from the training set of a downstream task, their attack still achieves limited ASRs, which further illustrates that their attack fails to build a strong correlation between trigger and reference objects.  

\begin{table}[!t]\renewcommand{\arraystretch}{1.2} 
	\fontsize{9}{11}\selectfont
	\centering
	\caption{ASRs of different attacks. Saha et al. achieve low ASRs, which is consistent with their results in~\cite{saha2022backdoor} in terms of FP.}
	\setlength{\tabcolsep}{1mm}
	{
	\begin{tabularx}{0.61\textwidth}{cccc}
	\toprule
	\multirow{2}{*}{\makecell{Target Downstream\\ Task}} &
	\multirow{2}{*}{No Attack} &
	\multirow{2}{*}{Saha et al.~\cite{saha2022backdoor}} &
	\multirow{2}{*}{\name} \\
	 & & & \\
	\midrule
	\multicolumn{1}{c|}{ImageNet100-A} &0.4 &5.5 &\textbf{96.2}\\
	\multicolumn{1}{c|}{ImageNet100-B} &0.4 &14.3 &\textbf{89.9} \\
	\multicolumn{1}{c|}{Pets} &1.5 &4.6 &\textbf{72.1} \\
	\multicolumn{1}{c|}{Flowers} &0 &1 & \textbf{89}\\
	\multicolumn{1}{c|}{Caltech-101} &0.2 &2.5 &\textbf{99.1} \\ 
	\bottomrule
	\end{tabularx}
	}
	\label{singleASR}
\end{table}

\begin{table}[!t]\renewcommand{\arraystretch}{1.2} 
    \fontsize{9}{11}\selectfont
	\centering
	\caption{CA and BA of different attacks. {\name} maintains the utility.}
	\setlength{\tabcolsep}{1mm}
	{
	\begin{tabularx}{0.61\textwidth}{cccc}
	\toprule
	\multirow{2}{*}{\makecell{Target Downstream\\ Task}} &
	\multirow{1}{*}{No Attack} &
	\multirow{1}{*}{Saha et al.~\cite{saha2022backdoor}} &
	\multirow{1}{*}{\name} \\
	 &CA &BA &BA \\
	\midrule
	\multicolumn{1}{c|}{ImageNet100-A} &69.3 &70.2 &69.6\\
	\multicolumn{1}{c|}{ImageNet100-B} &60.8 &61.4 &61.2 \\
	\multicolumn{1}{c|}{Pets} &55.8 &55.2 &56.9 \\
	\multicolumn{1}{c|}{Flowers} &70.8 &69.7 &69.7\\
	\multicolumn{1}{c|}{Caltech-101} &71.5 &71.6 &70.7 \\ 
	\bottomrule
	\end{tabularx}
	}
	\label{tab_utility}
\end{table}

\subsection{Experimental Results} \label{exp}
\myparatight{{\name} is more effective than existing attacks} Table~\ref{singleASR} shows the ASRs of different attacks for different target downstream tasks, while Table~\ref{targetclass} in Appendix shows the ASRs for different target classes when the target downstream task is ImageNet100-B. Each ASR is averaged over three trials of each experiment.  {\name} achieves  much higher ASRs than \cite{saha2022backdoor} across different target downstream tasks and target classes. In particular,  \cite{saha2022backdoor} achieves ASRs lower than 10\%, even though they require a large amount of reference images. One reason is that their attack does not control the distance between trigger and a reference object. As a result, the two randomly cropped views may both include a reference object and they fail to build strong correlations between trigger and reference object, as shown in Figure~\ref{compare_attn} in Appendix. We note that Saha et al.~\cite{saha2022backdoor} use \textbf{False Positive (FP)} as the metric, which is the number (instead of fraction) of trigger-embedded testing images that are predicted as the target class. ASR is the standard metric for measuring the effectiveness of backdoor attacks. When converting their FP to ASR, they achieve a very small ASR, e.g., less than 10\%.

\myparatight{{\name} maintains utility} Table~\ref{tab_utility} shows the CA
and BA of different downstream classifiers. We observe that {\name} preserves the utility of an encoder: BA of a downstream classifier is close to the corresponding CA. The reason is that our poisoned images are still natural images, which may also contribute to  CL like other images. 

\begin{figure}[!t]
    \centering
    \subfloat[Pre-training dataset size]{\includegraphics[width =0.33\textwidth]{graph/psize.pdf}}
    \subfloat[Encoder architecture]{\includegraphics[width =0.33\textwidth]{graph/net.pdf}}
    \subfloat[CL algorithm]{\includegraphics[width
    =0.33\textwidth]{graph/ssl.pdf}}
    \caption{Impact of pre-training settings on {\name}. }
    \label{ablation1}
\end{figure}

\myparatight{{\name} is agnostic to pre-training settings}  Figure~\ref{ablation1} shows the impact of pre-training settings, including pre-training dataset size, encoder architecture, and CL algorithm, on {\name}. In Figure~\ref{ablation1}(a), we use subsets of ImageNet with different sizes and ensure that they do not overlap with ImageNet100-B for a fair comparison (results on the full ImageNet are shown in Table~\ref{full} in Appendix). Our results show that {\name} is agnostic to pre-training settings. In particular, {\name} achieves high ASRs (i.e., achieving the effectiveness goal) and BAs are close to CAs (i.e., achieving the utility goal) across different pre-training settings.

\myparatight{Impact of hyperparameters of {\name}} Recall that we cannot derive the analytical form of the optimal $\alpha^*=b_w^*/o_w$ for left-right layout (or $\beta^*=b_h^*/o_h$ for bottom-top layout). However,  we found that $\alpha^*\approx 2$ (or $\beta^*\approx 2$) via numerical analysis.   Figure~\ref{ablation3}(a) shows the impact of $\alpha=b_w/o_w$ for left-right layout (or $\beta=b_h/o_h$ for bottom-top layout). Our results show that  ASR reaches the highest when $\alpha=2$ (or $\beta=2$), which is consistent with our theoretical analysis in Section~\ref{theoreticanalysis}. Moreover, {\name} consistently maintains the utility of the encoder since BAs are consistently close to CAs.

Figure~\ref{ablation3}(b) and (c) show the impact of poisoning ratio and the number of reference images on {\name}. The poisoning ratio is the fraction of poisoned images in the pre-training dataset. ASR quickly increases and converges as the poisoning ratio increases, which indicates that {\name} only requires a small fraction of poisoned inputs to achieve high ASRs. We find that ASR increases when using more reference images. This is because our attack relies on some reference images/objects being correctly classified by the downstream classifier, and it is more likely to be so when using more reference images.

Figure~\ref{ablation2} in Appendix shows the impact of trigger type (white, purple, and colorful), and trigger size on {\name}. A colorful trigger~\cite{saha2020hidden} achieves a higher ASR than the other two triggers (white and purple). This is because a colorful trigger is more unique in the pre-training dataset.  ASR is large once the trigger size is larger than a threshold (e.g., 20). Moreover, {\name} consistently maintains the utility of the encoder in all experiments.

\begin{figure}[!t]
    \centering
    \subfloat[$\alpha=b_w/o_w$ (or $\beta=b_h/o_h$)]{\includegraphics[width
    =0.33\textwidth]{graph/ar.pdf}}
    \subfloat[Poisoning ratio]{\includegraphics[width
    =0.33\textwidth]{graph/p.pdf}}
    \subfloat[Number of reference images]{\includegraphics[width
    =0.33\textwidth]{graph/ref.pdf}}
    \caption{Impact of $\alpha=b_w/o_w$ for left-right layout (or $\beta=b_h/o_h$ for bottom-top layout), poisoning ratio and the number of reference images.}
    \label{ablation3}
\end{figure}

\begin{figure}[!t]
    \centering
    \subfloat[Multiple target classes]{\includegraphics[width =0.33\textwidth]{graph/multi_target.pdf}\label{ablation4a}}
    \subfloat[Multiple downstream tasks]{\includegraphics[width =0.33\textwidth]{graph/multi_ds.pdf}\label{ablation4b}}
    \subfloat[\name+]{\includegraphics[width =0.33\textwidth]{graph/plus.pdf}\label{ablation4c}}
    \caption{ASRs for multiple target classes, multiple downstream tasks, and \name+.}
    \label{ablation4}
\end{figure}

\myparatight{Multiple target classes and downstream tasks} 
Figure~\ref{ablation4}(a) shows the ASR of each target class when {\name} attacks the three target classes separately or simultaneously, where each target class has a unique trigger. Figure~\ref{ablation4}(b) shows the ASR of each target downstream task when {\name} attacks the three target downstream tasks separately or simultaneously, where each target downstream task uses its default target class. Our results show that {\name} can successfully attack multiple target classes and target downstream tasks simultaneously. 

\myparatight{\name+} 
{\name+} requires additional support reference images to construct support poisoned images. We assume 5 additional support reference images sampled from the test set of a target downstream task and 130 support poisoned images (0.1\% of the pre-training dataset), where the support poisoned images have duplicates. For a fair comparison with {\name}, the total poisoning ratio is still 0.5\%. Figure~\ref{ablation4}(c) compares the ASRs of {\name} and {\name+} for three target downstream tasks. Our results show that {\name+}  further improves ASR.  Table~\ref{tab_rs} and \ref{tab_s} in Appendix respectively show the impact of the number of support reference images and support poisoned images on {\name+}. We find that a small number of support reference images and support poisoned images are sufficient to achieve high ASRs. 

\section{Defense}
\myparatight{Localized cropping} Existing defenses (e.g., \cite{wang2019neural,jia2021intrinsic,xu2021detecting}) against backdoor attacks were mainly designed for supervised learning, which are insufficient for CL as shown by~\cite{jia2022badencoder,281382}.
We propose a new defense called localized cropping, which is illustrated in Figure~\ref{defense}. The success of {\name} requires that one randomly cropped view of a poisoned image includes the reference object and the other includes the trigger. Our localized cropping breaks such requirement by constraining the two cropped views to be close to each other. Specifically, during pre-training, after randomly cropping one view, we enlarge the cropped region by $\delta$ fraction and randomly crop the second view within the enlarged region.  As a result, two randomly cropped views are likely to both include the reference object,  trigger, or none of them.  

\begin{figure}[!t]
    \centering
    \includegraphics[width=0.5\textwidth]{figs/LRC.pdf}
    \caption{Our localized cropping.}
    \label{defense}
    \vspace{-3mm}
\end{figure}

\myparatight{Experimental results} Table~\ref{tab_defense} shows the results of defenses tailored for CL. The pre-training dataset is ImageNet100-A, the target downstream task is ImageNet100-B, and the CL algorithm is MoCo-v2. ``No Defense'' means MoCo-v2 uses its original data augmentation operations; ``No Random Cropping'' means random cropping is not used; and ``Localized Cropping'' means replacing random cropping as our localized cropping. CompRess Distillation~\cite{saha2022backdoor} uses a clean pre-training dataset (a subset of the pre-training dataset in our experiments) to distill a (backdoored) encoder. Our results show that without random cropping makes attacks ineffective, but it also sacrifices the encoder's utility substantially, i.e., CA and BAs decrease substantially. Our localized cropping can also substantially reduce ASRs. Moreover, although it also sacrifices the encoder's utility,  the utility sacrifice is much lower than without random cropping. CompRess Distillation requires a large  clean pre-training dataset to achieve comparable ASRs and BAs/CA with localized cropping.  Table~\ref{tab_delta} in Appendix shows that localized cropping is less effective as $\delta$ increases. 

\begin{table}[!t]\renewcommand{\arraystretch}{1.2}
	\fontsize{9}{11}\selectfont
	\centering
	\caption{Defense results. $^\dagger$ indicates an extra clean pre-training dataset is used. For localized cropping, we use $\delta=0.2$ by default.}
	\setlength{\tabcolsep}{1mm}
	{
    \begin{tabularx}{0.78\textwidth}{cM{1cm}M{1cm}M{1cm}M{1cm}M{1cm}M{1cm}}
	\toprule
	\multirow{2}{*}{{Defense}}
	 & \multicolumn{2}{c}{No Attack} 
	 & \multicolumn{2}{c}{\name} & \multicolumn{2}{c}{\name+}\\
	 \cmidrule(lr){2-3}
	 \cmidrule(lr){4-5}
	 \cmidrule(lr){6-7}
	 & {CA} & {ASR} & {BA} & {ASR} & {BA} & {ASR} \\
	 \midrule
	 \multicolumn{1}{c|}{No Defense} &60.8 &0.4 & 61.2 & 89.9 &61.7 &97.8 \\
	 \multicolumn{1}{c|}{CompRess Distillation (5\%)$^\dagger$} &49.5 &0.9 &49.4 &1.1 &49.9 &0.9 \\
	 \multicolumn{1}{c|}{CompRess Distillation (20\%)$^\dagger$} &58.2 &0.9 &58.7 &1 &58.6 &1.1 \\
	 \multicolumn{1}{c|}{No Random Cropping} &32.4 &2.2 &31.1 &2 &31.9 &1.5 \\
	 \multicolumn{1}{c|}{{\dname} ($\delta=0.2$)} &57.1 &0.6 &57.2 &0.8 &57 &0.6 \\
	 \bottomrule
	\end{tabularx}
	}
	\label{tab_defense}
\end{table}

\section{Extension to Multi-modal CL}
\label{extension}

We also extend {\name} to attack the image encoder in multi-modal CL~\cite{radford2021learning,jia2021scaling}, which uses image-text pairs to pre-train an image encoder and a text encoder. Appendix~\ref{multiCL} shows how we create poisoned image-text pairs. 

Table~\ref{tab_multi-modal} compares different attacks to multi-modal CL. In these experiments,  we use a subset of 0.5M inputs in the Conceptual Captions dataset (CC)~\cite{sharma2018conceptual} as a pre-training dataset and use CLIP~\cite{radford2021learning} as the pre-training algorithm. We only inject $0.1\%$ (i.e., 500) of poisoned image-text pairs since multi-modal CL is  easier to attack than single-modal CL because an attack can exploit both images and texts. Moreover, we use a $16 \times 16$ trigger following \cite{carlini2022poisoning}. Our results show that both \cite{carlini2022poisoning} and {\name} maintain the utility of the encoder as the BAs are similar to the CA. However, {\name} achieves slightly or much higher ASRs than \cite{carlini2022poisoning}. Specifically, for target classes Rottweiler, Komondor, and Lorikeet, \cite{carlini2022poisoning} achieves ASRs of around 0, while {\name} achieves large ASRs. This is because the  pre-training dataset includes few image-text pairs related to these target classes.

\begin{table}[!t]\renewcommand{\arraystretch}{1.2}
	\fontsize{9.}{11.}\selectfont
	\centering
	\caption{Attacks to multi-modal CL. The pre-training dataset is Conceptual Captions~\cite{sharma2018conceptual} and the target downstream task is ImageNet100-B.}
	\setlength{\tabcolsep}{1mm}
	{
    \begin{tabularx}{0.64\textwidth}{cM{1cm}M{1cm}M{1cm}M{1cm}M{1cm}M{1cm}}
	\toprule
	 \multirow{2}{*}{\makecell{Target Class}}
	 & \multicolumn{2}{c}{No Attack} & 
	 \multicolumn{2}{c}{\makecell{Carlini \& Terzis~\cite{carlini2022poisoning}}}
	 & \multicolumn{2}{c}{\name} \\
	 \cmidrule(lr){2-3}
	 \cmidrule(lr){4-5}
	 \cmidrule(lr){6-7}
	 & {CA} & {ASR} & {BA} & {ASR} & {BA} & {ASR} \\
	 \midrule
	 \multicolumn{1}{c|}{Street Sign} &\multirow{5}{*}{48.4} &1 &48.3 &94 &49 &\textbf{97.7} \\
	 \multicolumn{1}{c|}{Ski Mask} & &1.4 &48.5 &96 &48.6 &\textbf{96.6} \\
	 \multicolumn{1}{c|}{Rottweiler} & &1.7 &48.6 &0 &48.9 &\textbf{57} \\
	 \multicolumn{1}{c|}{Komondor} & &0.3 &48.9 &0 &48.8 &\textbf{60.9} \\
	 \multicolumn{1}{c|}{Lorikeet} & &1.9 &47.7 &0.1 &48.4 &\textbf{89} \\
	 \bottomrule
	\end{tabularx}
	}
	\label{tab_multi-modal}
\end{table}

\section{Related Work}
\myparatight{CL} Single-modal CL~\cite{chen2020improved,chen2020simple,caron2020unsupervised,koohpayegani2021mean,li2021prototypical} uses images to pre-train an image encoder that outputs similar (or dissimilar) feature vectors for two  augmented views of the same (or different) pre-training image. Multi-modal CL~\cite{radford2021learning,jia2021scaling} uses image-text pairs to jointly pre-train an image encoder and a text encoder such that the image encoder and text encoder output similar (or dissimilar) feature vectors for image and text from the same (or different) image-text pair. 

\myparatight{Backdoor attacks to CL}
Backdoor attacks~\cite{gu2017badnets,chen2017targeted,liu2017trojaning,liu2020reflection,rakin2020tbt,li2021invisible} aim to compromise the training data or training process such that the learnt model behaves as an attacker desires. 
For CL, DPBAs inject poisoned inputs into the pre-training dataset so that the learnt image encoder is backdoored, while model poisoning based backdoor attacks (MPBAs) directly manipulate the model parameters of a clean image encoder to turn it into a backdoored one. MPBAs are not applicable when an image encoder is from a trusted provider, while existing DPBAs achieve limited attack success rates.  We note that \cite{281382} proposed PoisonedEncoder, a targeted data poisoning attack to CL, which is different from DPBAs that we focus on. The key difference is that a poisoned downstream classifier predicts several attacker-chosen clean testing images as target classes in targeted data poisoning attacks, while a backdoored downstream classifier predicts \emph{any} trigger-embedded testing image as a target class in DPBAs.

\section{Conclusion and Future Work}
In this work, we propose new data poisoning based backdoor attacks to contrastive learning (CL). Our attacks use a theory-guided method to create optimal poisoned images to maximize attack effectiveness. Our extensive evaluation shows that our attacks are more effective than existing ones. Moreover, we also explore a defense called localized cropping against data poisoning based backdoor attacks. Our results show that localized cropping can substantially reduce the attack success rates, though it also slightly sacrifices the utility of the encoder.

\bibliographystyle{plain}
\bibliography{paper}
\newpage
\appendix

\clearpage
\appendix
\begin{figure}[!t]
    \centering
    \includegraphics[width=0.64\textwidth]{figs/v2.pdf}
    \caption{Visual illustrations of (a) all possible $V_2$ that contain the trigger $e$. (b) $\Delta w$ and $\Delta h$ for left-right layout.}
    \label{v2}
\end{figure}

\section{Proof of Theorem 1}
\label{theorem1}
For simplicity, we prove the optimal locations of the reference object and trigger for left-right layout. The proof for bottom-top layout is similar. 


\myparatight{Computing $p_1(s)$ and $p_2(s)$}Given arbitrary $s \in (0, S]$, we aim to explicitly express the probabilities of $p_1(s)$ and $p_2(s)$.  
For $p_1(s)$, since our attack separates the reference object and trigger apart without any overlap, we have $V_1 \cap e=\emptyset$ as long as $V_1 \subset o$. Therefore, we have: 
$$p_1(s) = \text{Pr}\{(V_1 \subset o) \cap (V_1 \cap e=\emptyset)\} = \text{Pr}\{V_1 \subset o\}$$ 
Then, $p_1(s)$ can be computed as the ratio between the area of upper-left corners of $V_1$ such that $V_1 \subset o$ and that of all possible $V_1 \subset b$:
\begin{equation}
\begin{split}
p_1(s) = \text{Pr}\{V_1 \subset o\} = 
\begin{cases}
\frac{(o_w-s)(o_h-s)}{(b_w-s)(b_h-s)}, \quad &s \in \mathcal{X}_1 \\
0, \quad &s \notin \mathcal{X}_1
\end{cases}
\end{split}
\label{p1}
\end{equation}
where $\mathcal{X}_1 = (0, \min\{o_w,o_h\}$]. We have $\mathcal{X}_1$ because $V_1$ should not exceed the size of $o$. 

Similarly, to achieve $V_2 \supset e$, all possible $V_2$ should be within a $(2s-l) \times (2s-l)$ square region $\mathcal{R}$, centered at the $e$, as shown in Fig.~\ref{v2}(a). 
Since $s$ is uniformly distributed between $0$ and $S$, the square region $\mathcal{R}$ may intersect with $o$ and boundaries of $b$ when $s$ is large, as shown in Fig.~\ref{v2}(b). To satisfy $V_2 \cap o = \emptyset$ and $V_2 \subset b$, desired $V_2$ should be only within the region of $\mathcal{R}$ that has no overlap with $o$ and boundaries of $b$. We assume the width and height of this region as $\Delta w$ and $\Delta h$. Given fixed $b_w$, $o_x$ and $e_x$, $\Delta w$ is a function of crop size $s$ and given fixed $b_h$ and $e_y$, $\Delta h$ is also a function of $s$. Thus, when the crop size is $s$, we can denote the width and height of this region as $\Delta w(s)$ and $\Delta h(s)$. Then, we follow the same procedure as $p_1(s)$ to obtain the probability $p_2(s)$ as:
\begin{equation}
\begin{split}
p_{2}(s) = \text{Pr}\{(V_2 \supset e) \cap (V_2 \cap o=\emptyset)\} = 
\begin{cases}
\frac{(\Delta w(s)-s)(\Delta h(s)-s)}{(b_w-s)(b_h-s)}, \quad &s \in \mathcal{X}_2 \\
0, \quad &s \notin \mathcal{X}_2
\end{cases}
\end{split}
\label{p2}
\end{equation}
where $\mathcal{X}_2 = (l, \min\{b_w-(o_x+o_w),b_h\}]$. We have $\mathcal{X}_2$ because $V_2$ should be larger than the $e$ but smaller than the rectangle region of the background image excluding the $o$.

Recall that we are supposed to maximize the $p$ in Equation~\ref{totalp} with aforementioned forms of $p_1(s)$ and $p_2(s)$. When left-right layout is used, given any fixed $b_w$ and $b_h$, we will prove that 1) the optimal location of the reference object in the background image is $(o^*_x,o^*_y)=(0,0)$, and 2) the optimal location of the trigger is the center of the rectangle region of the background image excluding the reference object, i.e., $(e^*_x,e^*_y)=(\frac{b_w+o_w-l}{2}, \frac{b_h-l}{2})$.

\myparatight{Optimal location of the trigger} Let's derive the optimal location $(e^*_x,e^*_y)$ of the trigger $e$ first. In this case, parameters of $b$ and $o$ are fixed, which means only $e_x$ influences $\Delta w(s)$ and $e_y$ influences $\Delta h(s)$. We denote the horizontal distance between $e$ and $o$ as $d_1$ and the horizontal distance between $e$ and the right boundary of $b$ as $d_2$. Then we have:
\begin{equation}
\begin{split}
& d_1=e_x-(o_x+o_w), \\
& d_2=b_w-(e_x+l),
\end{split}
\label{d12}
\end{equation}
where both $d_1$ and $d_2$ depend on $e_x$. Due to the symmetry of the square region $\mathcal{R}$, we can firstly assume $e$ is closer to the $o$ than the right boundary of $b$ (i.e., $d_1 \leq d_2$), as shown in Fig.~\ref{v2}(b). In this case, we express $\Delta w(s)$ as follows:
\begin{equation}
\Delta w(s)=
\begin{cases}
2s-l,  \quad &s \in (\min\{\mathcal{X}_2\},d_1+l] \\
d_1+s, \quad &s \in (d_1+l,d_2+l] \\
b_w-(o_x+o_w), \quad &s \in (d_2+l, \max\{\mathcal{X}_2\}]
\end{cases} 
\end{equation}

If there exists $e_x$ and $e_x^{\prime}$ such that $d_1 < d_1^{\prime}\leq d_2^{\prime}<d_2$, then we can obtain $\Delta w^{\prime}(s)-\Delta w(s)$ as:
\begin{equation} 
\begin{split}
\Delta w^{\prime}(s)-\Delta w(s)=
\begin{cases}
0,  \quad &s \in (\min\{\mathcal{X}_2\},d_1+l] \\
s-(d_1+l), \quad &s \in (d_1+l,d_1^{\prime}+l] \\
d_1^{\prime}-d_1, \quad &s \in (d_1^{\prime}+l,d_2^{\prime}+l] \\
(d_2+l)-s, \quad &s \in (d_2^{\prime}+l,d_2+l] \\
0, \quad &s \in (d_2+l,\max\{\mathcal{X}_2\}]
\end{cases} 
\end{split}
\label{minus}
\end{equation}
We have $\Delta w(s) \leq \Delta w^{\prime}(s)$ holds for all $s$. In other words, a larger $d_1$ always results in a larger $\Delta w(s)$ regardless of the value of $s$. Since we know that $\Delta w(s)$ is positively correlated with $p$ and we have $d_1 \leq d_2$ by assumption, $d_1=d_2$ will achieve the optimal $\Delta w(s)$ for all $s$ and maximize the $p$. We should get the same optimal result (i.e., $d_1=d_2$) if we start by assuming $d_1 \geq d_2$. Therefore, according to Equation~\ref{d12}, we obtain $e_x^{*}$ as:
\begin{equation}
e_x^{*} = \frac{b_w+o_x+o_w-l}{2}
\label{ex}
\end{equation}
It is noted that we will derive the optimal location of the reference object $(o^*_{x},o^*_{y})=(0,0)$ for left-right layout. Therefore, we can further reduce the Equation~\ref{ex} as $e^{*}_x= \frac{b_w+o^*_x+o_w-l}{2} = \frac{b_w+o_w-l}{2}$.

Next, we denote the vertical distance between $e$ and the top boundary of $b$ as $d_3$ and the vertical distance between $e$ and the bottom boundary of $b$ as $d_4$:
\begin{equation}
\begin{split}
& d_3=e_y \\
& d_4=b_h-(e_y+l)
\end{split}
\label{d34}
\end{equation}
where both $d_3$ and $d_4$ depend on $e_y$. By assuming $d_3 \leq d_4$, we express $\Delta h(s)$ as follows:
\begin{equation}
\Delta h(s)=
\begin{cases}
2s-l,  \quad &s \in (\min\{\mathcal{X}_2\},d_3+l] \\
d_3+s,  \quad &s \in (d_3+l,d_4+l] \\
b_h,  \quad &s \in (d_4+l, \max\{\mathcal{X}_2\}]
\end{cases} 
\end{equation}
If there exists $e_y$ and $e_y^{\prime}$ such that $d_3<d_3^{\prime} \leq d_4^{\prime}<d_4$, similar to Equation~\ref{minus}, we can show that $\Delta h(s) \leq \Delta h^{\prime}(s)$ holds for all $s$.  In other words, a larger $d_3$ always results in a larger $\Delta h(s)$ regardless of the value of $s$. Since $\Delta h(s)$ is also positively correlated with $p$ and we have $d_3 \leq d_4$, we conclude that $d_3=d_4$ will maximize the $p$. Therefore, we obtain $e^*_y$ according to Equation~\ref{d34} as:
\begin{equation}
e_y^{*} = \frac{b_h-l}{2}
\end{equation}

\myparatight{Optimal location of the reference object} Given ($e^*_x, e^*_y$), our next step is to derive the optimal location ($o^*_x, o^*_y$) of the reference object $o$ such that $p$ is maximized. Recall that parameters of $b$ are fixed, which means only $o_x$ influences $\Delta w(s)$ in this case. Assume there exists an $o^{\prime}_x>o_x$, which results in $\Delta w^{\prime\prime}(s)$. Under the optimal location of the trigger, we obtain $\Delta w^{\prime\prime}(s)-\Delta w(s)$ as:
\begin{equation}
\begin{split}
\Delta w^{\prime\prime}(s)-\Delta w(s)=
\begin{cases}
0,  \quad &s \in (\min\{\mathcal{X}_2\}, f(o^{\prime}_x)] \\
b_w-(o^{\prime}_x+o_w)-(2s-l),  \quad &s \in (f(o^{\prime}_x), f(o_x)] \\
o_x-o^{\prime}_x,  \quad &s \in (f(o_x),\max\{\mathcal{X}_2\}] \\
\end{cases} 
\end{split}
\end{equation}
where $f(o_x)=\frac{b_w-o_x-o_w+l}{2}$ indicates the smallest $s$ such that $V_2$ touches the $o$ and right boundary of $b$ under the input $o_x$. We show that if $o^{\prime}_x>o_x$, $\Delta w^{\prime\prime}(s) \leq \Delta w(s)$ holds for all $s$. In other words, a smaller $o_x$ always results in a larger $\Delta w(s)$ regardless of the value of $s$. Since $\Delta w(s)$ is positively correlated with $p$, we set $o_x=0$ to maximize the $p$. As for $o_y$, any $o_y \in [0, b_h-o_h]$ will lead to the same $p$. Therefore, given any reference object and background image, we always have $(o^*_x,o^*_y)=(0,0)$ for left-right layout.

\section{Proof of Theorem 2}
\label{theorem2}
For left-right layout, we aim to prove that for any $o$ and $e$, given any width of the background image $b_w > o_w$, the optimal height of the background image should be the height of the reference object, i.e., $b^*_h = o_h$. The proof of optimal width for bottom-top layout is similar.

Given the optimal locations of reference object $o$ and trigger $e$ in background image $b$, we obtain $\Delta h^*(s)$ and $\Delta w^*(s)$ as follows:
\begin{equation}
\begin{split}
& \Delta h^{*}(s)=
\begin{cases}
2s-l,\quad &s \in (\min\{\mathcal{X}_2\},\frac{b_h+l}{2}] \\
b_h,   \quad &s \in (\frac{b_h+l}{2},\max\{\mathcal{X}_2\}]
\end{cases}
\\
& \Delta w^{*}(s)=
\begin{cases}
2s-l,\quad &s \in (\min\{\mathcal{X}_2\},\frac{b_w-o_w+l}{2}] \\
b_w-o_w,\quad &s \in (\frac{b_w-o_w+l}{2},\max\{\mathcal{X}_2\}]
\end{cases}
\end{split}
\end{equation}
In this case, we derive the marginal probability of $p$ under the optimal locations of $o$ and $e$ as:
\begin{equation}
p_1(s)p_2(s) =
\begin{cases}
\frac{(o_w-s)(o_h-s)(\Delta w^*(s)-s)(\Delta h^*(s)-s)}{(b_w-s)^2(b_h-s)^2}, &s\in \mathcal{X}\\
0, &s \notin \mathcal{X}
\end{cases}
\end{equation}
where $\mathcal{X}=\mathcal{X}_1 \cap \mathcal{X}_2 = (l,\min\{o_w,o_h,b_w-o_w\}]$. Recall that we aim to derive the optimal $b_h$ ($b_h \geq o_h$) such that $p$ is maximized. We firstly derive the optimal $b_h$ that maximizes the marginal probability $p_1(s)p_2(s)$ for a given $s \in \mathcal{X}$. We have:
\begin{equation}
\begin{split}
\argmax_{b_h} p_1(s)p_2(s) = \argmax_{b_h} \frac{\Delta h^*(s)-s}{(b_h-s)^2}
= \argmax_{b_h} [\log(\Delta h^*(s)-s) - 2 \log(b_h-s)]
\end{split}
\label{bh}
\end{equation}
Let's denote $g(b_h, s)=\log(\Delta h^*(s)-s) - 2 \log(b_h-s)$. We consider two scenarios:

\myparatight{(i)} If there exists $b_h$ and $b^{\prime}_h$ such that $\frac{b_h+l}{2} < \frac{b^{\prime}_h+l}{2} \leq \max\{\mathcal{X}\}$, we can obtain $g(b^{\prime}_h, s)-g(b_h, s)$ as:
\begin{equation}
\begin{split}
g(b^{\prime}_h, s)&-g(b_h, s)=
\begin{cases}
\log \frac{(b_h-s)^2}{(b^{\prime}_h-s)^2}, \quad &s \in (\min\{\mathcal{X}\}, \frac{b_h+l}{2}]\\
\log \frac{(b_h-s)(s-l)}{(b^{\prime}_h-s)(b^{\prime}_h-s)}, \quad &s \in (\frac{b_h+l}{2}, \frac{b^{\prime}_h+l}{2}]\\
\log \frac{(b_h-s)}{(b^{\prime}_h-s)}, \quad &s \in (\frac{b^{\prime}_h+l}{2}, \max\{\mathcal{X}\}]
\end{cases}
\end{split}
\end{equation}
We show that if there exists $b_h$ and $b^{\prime}_h$ such that $\frac{b_h+l}{2} < \frac{b^{\prime}_h+l}{2} \leq \max\{\mathcal{X}\}$, $g(b^{\prime}_h, s) \leq g(b_h, s)$ holds for all $s$. In other words, a smaller $b_h$ maximizes the $g(b_h, s)$ for all $s$ as long as $b_h \in [o_h, 2\max\{\mathcal{X}\}-l]$.

\myparatight{(ii)} If there exists $b_h$ and $b^{\prime}_h$ such that $\frac{b^{\prime}_h+l}{2} > \frac{b_h+l}{2} > \max\{\mathcal{X}\}$, we can obtain $g(b^{\prime}_h, s)-g(b_h, s)$ as:
$$g(b^{\prime}_h, s)-g(b_h, s)=\log \frac{(b_h-s)^2}{(b^{\prime}_h-s)^2} <0$$
Therefore, a smaller $b_h$ also maximizes the $g(b_h, s)$ for all $s$ as long as $b_h \in (2\max\{\mathcal{X}\}-l, \infty)$. 

\noindent
Combining \textbf{(i)} and \textbf{(ii)}, we theoretically prove that $g(b_h, s)$ monotonically decreases for all $s \in \mathcal{X}$ as $b_h$ increases. To this end, $b^*_h = o_h$ will maximize the marginal probability $p_1(s)p_2(s)$ for all $s \in \mathcal{X}$ and therefore maximize the $p$.


\begin{algorithm}[!t]
\fontsize{8}{9}\selectfont
\caption{RescaleAndCropBackground}
\begin{algorithmic}[1]
\State {\bfseries Input:} Background image $b$, reference object $o$, width ratio $\alpha$, and height ratio $\beta$.
\State {\bfseries Output:} A  re-scaled and cropped background image $b'$. 

\State $b'_w \gets o_w \cdot \alpha$
\State $b'_h \gets o_h \cdot \beta$
\State $r = \max(\frac{b'_h}{b_h}, \frac{b'_w}{b_w})$
\Comment{Get the re-scaling ratio if re-scaling is needed}
\If{$r > 1$} \Comment{Scaling up $b$ by ratio $r$}
\State $b \gets$ \Call{Rescale}{$b, r$} 
\EndIf
\State $b' \gets$ a random rectangle area with width $b'_w$ and height $b'_h$ in $b$
\end{algorithmic}  
\label{rc}
\end{algorithm}

\newpage
\begin{table}[!t]\renewcommand{\arraystretch}{1.3} 
	\fontsize{8}{10}\selectfont
	\centering
    \caption{Default target class of each target downstream task.}
	{\setlength{\tabcolsep}{1mm}
	{
	\begin{tabular}{cc}
	\toprule
	Target Downstream Task & Default Target Class \\
	\midrule
	\multicolumn{1}{c|}{ImageNet100-A} & Greater Swiss Mountain Dog \\
	\multicolumn{1}{c|}{ImageNet100-B} & African Hunting Dog \\
	\multicolumn{1}{c|}{Pets} & Havanese \\
	\multicolumn{1}{c|}{Flowers} & Lotus \\
	\multicolumn{1}{c|}{Caltech-101} & Stop Sign \\
	\bottomrule
	\end{tabular}
	}
	\label{defaultclass}
	}
\end{table}

\begin{table}[!t]\renewcommand{\arraystretch}{1.3} 
	\fontsize{8}{10}\selectfont
	\centering
    \caption{ASRs of different attacks for different target classes when the target downstream task is ImageNet100-B in single-modal CL.}
	{\setlength{\tabcolsep}{1mm}
	{
	\begin{tabular}{cccc}
	\toprule
	Target Class & 
	No Attack &
	Saha et al.~\cite{saha2022backdoor} &
	{\name} \\
	\midrule
	\multicolumn{1}{c|}{African Hunting Dog} &0.4 &14.3 &\textbf{89.9} \\
	\multicolumn{1}{c|}{Ski Mask} &0.4 &14 &\textbf{84.3} \\
	\multicolumn{1}{c|}{Rottweiler} &0.3 &8 &\textbf{90.6} \\
	\multicolumn{1}{c|}{Shih-Tzu} &0.1 &1 &\textbf{86.7} \\
    \multicolumn{1}{c|}{Komondor} &0 &18.3 &\textbf{99.4} \\
    \multicolumn{1}{c|}{Lorikeet} &0.3 &9.0 &\textbf{83.4} \\
    \multicolumn{1}{c|}{Mixing bowl} &0.1 &2.1 &\textbf{91.4} \\
    \cmidrule(lr){1-1}
    \cmidrule(lr){2-4}
    \multicolumn{1}{c|}{Average} &0.2 &9.5 &\textbf{89.4} \\
	\bottomrule
	\end{tabular}
	}
	\label{targetclass}
	}
\end{table}


\begin{table}[!t]\renewcommand{\arraystretch}{1.3} 
	\fontsize{8}{10}\selectfont
	\centering
    \caption{Experiments on the full ImageNet. The downstream dataset is ImageNet100-B and the poisoning ratio is 0.5\%. MoCo-v2 and ResNet-18 are used.}
	{\setlength{\tabcolsep}{1mm}
	{
	\begin{tabular}{M{1cm}M{1cm}M{1cm}M{1cm}}
	\toprule
	\multicolumn{2}{c}{No Attack} 
	& \multicolumn{2}{c}{\name} \\
	\cmidrule(lr){1-2}
	\cmidrule(lr){3-4}
	CA &ASR &CA &ASR
	\\
	\midrule
	75.5 &0 &76.1 &74.9\\
	\bottomrule
	\end{tabular}
	}
	\label{full}
	}
\end{table}

\begin{table}[!t]\renewcommand{\arraystretch}{1.3} 
	\fontsize{8}{10}\selectfont
	\centering
    \caption{ASRs of {\name} for different target classes when using reference object and reference image to construct poisoned images in single-modal CL. The pre-training dataset is ImageNet100-A and target downstream dataset is ImageNet100-B. }
	{\setlength{\tabcolsep}{1mm}
	{
	\begin{tabular}{ccc}
	\toprule
	Target Class & 
	Reference Object &
	Reference Image \\
	\midrule
	\multicolumn{1}{c|}{African Hunting Dog} &89.9 &53.2 \\
	\multicolumn{1}{c|}{Ski mask} &84.3 &37.6 \\
	\multicolumn{1}{c|}{Rottweiler} &90.6 &7.3 \\
	\multicolumn{1}{c|}{Shih-Tzu} &86.7 &72.7 \\
    \cmidrule(lr){1-1}
    \cmidrule(lr){2-3}
    \multicolumn{1}{c|}{Average} &87.9 &42.7 \\
	\bottomrule
	\end{tabular}
	}
	\label{referenceimageobject}
	}
\end{table}

\begin{figure}
    \centering
    \subfloat[Trigger type]{\includegraphics[width =0.36\textwidth]{graph/type.pdf}}
    \subfloat[Trigger size]{\includegraphics[width =0.36\textwidth]{graph/t.pdf}}
    \caption{Impact of the trigger type and trigger size on {\name}.}
    \label{ablation2}
\end{figure}


\begin{table}[!t]\renewcommand{\arraystretch}{1.2} 
	\fontsize{8}{10}\selectfont
	\centering
	\caption{Impact of the number of support reference images on ASR of \name+. The target downstream task is Pets. }
	\setlength{\tabcolsep}{1mm}
	{
	\begin{tabular}{cM{1cm}M{1cm}M{1cm}}
	\toprule
	\multirow{2}{*}{\name} & \multicolumn{3}{c}{\name+} \\
	\cmidrule{2-4} 
	& 1 & 5 & 10 \\
	\midrule
	72.1 &79.7 &93.6 &97.9 \\
	\bottomrule
	\end{tabular}
	}
	\label{tab_rs}
\end{table}

\begin{table}[!t]\renewcommand{\arraystretch}{1.2} 
	\fontsize{8}{10}\selectfont
	\centering
	\caption{Impact of the number of support poisoned images on  ASR of \name+. The target downstream task is Pets.}
	\setlength{\tabcolsep}{1mm}
	{
	\begin{tabular}{cccc}
	\toprule
	\multirow{2}{*}{\name} & \multicolumn{3}{c}{\name+} \\
	\cmidrule{2-4}
	& 130 (0.1\%) & 260 (0.2\%) & 390 (0.3\%) \\
	\midrule
	72.1 &93.6 &94.3 &88.4 \\
	\bottomrule
	\end{tabular}
	}
	\label{tab_s}
\end{table}

\begin{table*}[!t]\renewcommand{\arraystretch}{1.2} 
	\fontsize{9}{10}\selectfont
	\centering
	\caption{Impact of $\delta$ on localized cropping.}
	\setlength{\tabcolsep}{1mm}
	{
	\begin{tabular}{ccccccccccccc}
	\toprule
	\multirow{2}{*}{$\delta$} 
	& \multicolumn{2}{c}{N/A}& \multicolumn{2}{c}{0.1} & \multicolumn{2}{c}{0.2} & \multicolumn{2}{c}{0.3} & \multicolumn{2}{c}{0.4} &  \multicolumn{2}{c}{0.5} \\
	\cmidrule(lr){2-3}
	\cmidrule(lr){4-5}
	\cmidrule(lr){6-7}
	\cmidrule(lr){8-9}
	\cmidrule(lr){10-11}
	\cmidrule(lr){12-13}
	& BA & ASR & BA & ASR & BA & ASR & BA & ASR & BA & ASR & BA & ASR\\ 
	\midrule
	\multicolumn{1}{c|}{\name} &61.2 &89.9 &55.7 &0.8 &57.2 &0.8 &59 &17.1 &60.5 &59.6 &61 &84.1 \\
	\bottomrule
	\end{tabular}
	}
	\label{tab_delta}
\end{table*}

\section{Datasets} 
\label{app_dataset}
By default, we use ImageNet100-A (\cite{russakovsky2015imagenet}) and Conceptual Captions 0.5M (\cite{sharma2018conceptual}) respectively for single-modal and multi-modal pre-training,  and we evaluate the pre-trained image encoders on ImageNet100-B for linear classification. 
When the downstream dataset is ImageNet100-A, we randomly pick 10\% of images from each class that do not overlap with the reference images used by \cite{saha2022backdoor} for a fair comparison. 
Other downstream datasets include Oxford-IIIT Pets (\cite{parkhi2012cats}), Oxford 102 Flowers (\cite{nilsback2008automated}), and Caltech-101 (\cite{fei2004learning}), whose train/test splits are the same as \cite{chen2020simple,ericsson2021well}. \cite{saha2022backdoor} requires a large number of reference images in their attack. Since the dataset of a downstream task (Pets, Flowers, Caltech-101) may not contain enough reference images, we duplicate them multiple times when constructing poisoned images for \cite{saha2022backdoor}. 
For each reference object used by our {\name}, we manually annotate its segmentation mask in the reference image using the open-source labeling tool called labelme\footnote{\url{https://github.com/wkentaro/labelme}}.

\section{CL Algorithms}
\label{app_cl}
The CL algorithms include MoCo-v2 (\cite{chen2020improved}), SwAV (\cite{caron2020unsupervised}), SimCLR (\cite{chen2020simple}), MSF (\cite{koohpayegani2021mean}) for single-modal CL and CLIP (\cite{radford2021learning}) for multi-modal CL. We follow the original implementation of each CL algorithm, including the data augmentation operations and hyper-parameters:

\myparatight{MoCo-v2} Following \cite{saha2022backdoor}, we use this code implementation of MoCo-v2\footnote{\url{https://github.com/SsnL/moco_align_uniform}}. We adopt the same pre-training settings as their work. In particular, we use the SGD optimizer with an initial learning rate of 0.6 and pre-train an encoder for 200 epochs with a batch size of 256 on 2 NVIDIA RTX6000 GPUs. 

\myparatight{SwAV} We follow the official implementation\footnote{\url{https://github.com/facebookresearch/swav/blob/main/main_swav.py}} of SwAV (including data augmentations, optimizer, etc.). We pre-train each encoder for 200 epochs with a total batch size of 256 on 4 NVIDIA RTX6000 GPUs. 

\myparatight{SimCLR} We use this pytorch implementation\footnote{\url{https://github.com/AndrewAtanov/simclr-pytorch}} of SimCLR. Because SimCLR requires a large batch size ($>$ 1k) to obtain a desirable performance on ImageNet, we pre-train each encoder for 300 epochs with an initial learning rate of 1.2 and a batch size of 1024 on 4 NVIDIA RTX6000 GPUs. 

\myparatight{MSF} We follow the official implementation\footnote{\url{https://github.com/UMBCvision/MSF}} of MSF. Specifically, we pre-train each encoder for 200 epochs with a batch size of 256 on 4 RTX6000 GPUs. 

\myparatight{CLIP} Following~\cite{carlini2022poisoning}, we use the official implementation\footnote{\url{https://github.com/mlfoundations/open_clip}} of CLIP for multi-modal CL. In particular, we pre-train an image encoder (ResNet50) and a text encoder (ViT-B-32) for 30 epochs using a batch size of 128 image-text pairs. Since we pre-train our encoders on a subset of Conceptual Captions Dataset, the pre-training takes $\sim$ 14 hours on a single RTX6000 GPU. 

\section{Training Linear Downstream Classifiers}
\label{trainingdownstream}
Following previous works (\cite{chen2020simple,grill2020bootstrap,koohpayegani2021mean}), to train a linear downstream classifier on a downstream task, we follow the same linear evaluation protocol used by each CL algorithm. For multi-modal CL, we train a downstream classifier using the same linear evaluation protocol as MoCo-v2.

\begin{figure}[!t]
    \centering
    \includegraphics[width=0.72\textwidth]{figs/multi.pdf}
    \caption{Poisoned image-text pairs in \cite{carlini2022poisoning} vs. our {\name} for multi-modal CL, where the target class is dog.} 
    \label{multiCLcomparison}
    \vspace{-1mm}
\end{figure}

\section{{\name} for Multi-modal CL}
\label{multiCL}

Carlini and Terzis~\cite{carlini2022poisoning} proposed a DPBA to  multi-modal CL. To craft poisoned image-text pairs, they embed the trigger into some images and create the corresponding texts following some text prompts that include the target class name (e.g., ``a photo of dog''), as illustrated in Figure~\ref{multiCLcomparison}. This attack achieves limited success rates when the pre-training dataset only includes few image-text pairs whose images include objects from the target class and whose texts include the target class name, because CL cannot semantically associate the target class name with  objects in the target class.   {\name} addresses such limitation.  

We denote by $f_i$ and $f_r$ the feature vectors produced by the image encoder for an image embedded with trigger $e_{ti}$ and a reference image from target class $y_{ti}$. Moreover, we denote by $f_t$ the feature produced by the text encoder for a text prompt including the name of target class $y_{ti}$. Our key idea is to craft poisoned image-text pairs such that 1) $f_i$ is similar to $f_t$, and 2) $f_t$ is similar to $f_r$. Therefore, $f_i$ and $f_r$ are similar, making our attack successful. 

We craft two types of poisoned image-text pairs (called \emph{Type-I} and \emph{Type-II}) to achieve 1) and 2), respectively. Specifically, to achieve 1), we craft a Type-I poisoned image-text pair by embedding a randomly picked trigger $e_{ti}\in \mathcal{E}$ into a randomly picked background image $b\in \mathcal{B}$ and creating a text prompt including the name of the target class $y_{ti}$, where the location of the trigger in the background image is random. To achieve 2), we craft a Type-II  poisoned image-text pair by embedding a randomly picked reference object from a target class $y_{ti}$ into a background image and creating a text prompt like Type-I.   The background image may be re-scaled (or cropped) if it is too small (or large) to include the reference object. A text prompt could be like ``a photo of $<$target class name$>$''. In our experiments, we use the text prompts proposed by \cite{carlini2022poisoning}, which are publicly available. Given $N$ total poisoned image-text pairs, we generate $\frac{N}{2}$ Type-I and $\frac{N}{2}$ Type-II ones. Note that \cite{carlini2022poisoning} only uses $N$ Type-I poisoned image-text pairs. 

\begin{figure}[!t]
    \centering
    \includegraphics[width=\textwidth]{figs/compare_attn.pdf}
    \caption{Comparing the attention maps of poisoned testing images when using classifiers built based on backdoored encoders from Saha et al.~\cite{saha2022backdoor} and {\name}. We use Grad-CAM \cite{selvaraju2017grad} to visualize the attention map, which shows the most influential parts of an input that result in the classifiers output.}
    \label{compare_attn}
\end{figure}

\begin{figure}[!t]
    \centering
    \includegraphics[width=\textwidth]{figs/compare_cl.pdf}   
    \caption{Comparing the attention maps of clean and poisoned testing images when using the classifier built based on our {\name}.}
    \label{compare_clean_and_poisoned_attn}
\end{figure}

\begin{figure}[!t]
    \centering
    \includegraphics[width=\textwidth]{figs/mask.pdf}
    \caption{Visual illustrations of reference objects from different target classes.}
    \label{reference_obj}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{figs/show.pdf}
    \caption{Visual illustrations of poisoned images of our {\name}. For each row, we craft poisoned images using a given reference object and different background images (in the first row). Specifically, we adopt the optimal settings for the size of a background image and the location of a reference object. We also sample each trigger location within the center of the rectangle region of the background image excluding the reference object.}
    \label{show}
\end{figure}
\end{document}