\documentclass[11pt]{article}
\usepackage{bbm}
\usepackage{epsfig,endnotes}
\usepackage{multirow}
\usepackage{subfig}
\usepackage{graphicx}
\usepackage{grffile}
\newcounter{copyrightbox}
\newcounter{subcopyrightbox}
\newcounter{subcopyrightbox@save}
\usepackage[font=bf]{caption}
\usepackage{color, url}
\usepackage{xspace} 
\usepackage{amsmath}
\usepackage{geometry}
\geometry{a4paper,scale=0.75}
\usepackage{changepage}
\usepackage{booktabs}
\usepackage{hyperref} 
\usepackage{cleveref}
\usepackage{hyperref}
\usepackage{url}
\usepackage{tikz}
\usepackage{comment}
\usepackage{amsmath,amssymb}
\usepackage{caption}
\usepackage{floatrow}
\floatsetup[table]{capposition=above}
\usepackage{longtable}
\usepackage{etoolbox}
\AtBeginEnvironment{longtable}{
  \addfontfeature{RawFeature=+tnum;-onum}
}
\usepackage{pdflscape}
\usepackage{colortbl}
\definecolor{Gray}{gray}{0.925}
\newcommand{\myrowcolour}{\rowcolor{Gray}}
\newcolumntype{g}{>{\columncolor{Gray}}c}
\usepackage{tabularx}
\newcolumntype{M}[1]{>{\centering\arraybackslash}m{#1}}
\usepackage{booktabs}
\usepackage{wrapfig}
\usepackage{color}
\usepackage{dsfont}
\usepackage{makecell}
\usepackage{multirow}
\usepackage{subfig}
\usepackage{bbold}
\usepackage{algorithm}  
\usepackage{algorithmicx}
\usepackage{algpseudocode}
\usepackage{amsfonts,amssymb}
\newcommand{\myparatight}[1]{\smallskip\noindent{\bf {#1}:}~}
\newcommand{\name}{\text{CorruptEncoder}}
\newcommand{\dname}{\text{Localized Cropping}}
\usepackage{amssymb,amsmath,amsthm, amsfonts}
\newtheorem{theorem}{Theorem}

\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}
\newenvironment{packeditemize}{\begin{list}{$\bullet$}{\setlength{\itemsep}{0.2pt}\addtolength{\labelwidth}{-4pt}\setlength{\leftmargin}{\labelwidth}\setlength{\listparindent}{\parindent}\setlength{\parsep}{1pt}\setlength{\topsep}{0pt}}}{\end{list}}
\AtBeginDocument{
  \providecommand\BibTeX{{
    \normalfont B\kern-0.5em{\scshape i\kern-0.25em b}\kern-0.8em\TeX}}}



\begin{document}
\begin{center}
{\LARGE{\bf{\name: Data Poisoning based Backdoor Attacks to Contrastive Learning}}}
\begin{tabular}{cccc}
&&&\\
Jinghuai Zhang &&& Hongbin Liu \\
UCLA &&& Duke University \\
jinghuai1998@g.ucla.edu &&& hongbin.liu@duke.edu \\
\\
Jinyuan Jia &&& Neil Zhenqiang Gong \\
Penn State University &&& Duke University\\
jinyuan@psu.edu &&& neil.gong@duke.edu\\
\end{tabular}
\end{center}

\begin{abstract}
Contrastive learning (CL) pre-trains general-purpose encoders using an unlabeled pre-training dataset, which consists of images or image-text pairs. CL is vulnerable to {data poisoning based backdoor attacks (DPBAs)}, in which an attacker injects {poisoned inputs} into the pre-training dataset so the encoder is backdoored. 
However, existing DPBAs achieve limited effectiveness. In this work, we take the first step to analyze the limitations of existing attacks and propose new DPBAs called {\name} to CL. {\name} uses a theory-guided method to create optimal poisoned inputs to maximize attack effectiveness. 
Our experiments show that  {\name} substantially outperforms existing DPBAs. 
In particular, {\name} is the first DPBA that achieves \textbf{more than 90\%} attack success rates with only a few (3) reference images and a small poisoning ratio (0.5\%). 
Moreover, we also propose a defense, called {localized cropping}, to defend against DPBAs. Our results show that our defense can reduce the effectiveness
of DPBAs, but it sacrifices the utility of the encoder, highlighting the need for new defenses.
\end{abstract}

\section{Introduction}
Given an unlabeled pre-training dataset, contrastive learning (CL)~\cite{chen2020improved,chen2020simple,caron2020unsupervised,radford2021learning} aims to pre-train an image encoder and (optionally) a text encoder via leveraging the supervisory signals in the dataset itself. For instance, given a large amount of unlabeled images, single-modal CL, which is the major focus of this paper,
\footnote{We extend {\name} to multi-modal CL in Section~\ref{extension}.}
can learn an image encoder that produces similar (or dissimilar) feature vectors for two random augmented views created from the same (or different) image. An augmented view of an image is created by applying a sequence of \emph{data augmentation operations} to the image. Among various data augmentation operations, \emph{random cropping} is the most important one~\cite{chen2020simple}.

CL is vulnerable to \emph{data poisoning based backdoor attacks (DPBAs)}~\cite{saha2022backdoor,carlini2022poisoning}. Specifically, an attacker embeds backdoor into an encoder via injecting \emph{poisoned images} into the pre-training dataset. A downstream classifier built based on a backdoored encoder predicts an attacker-chosen class (called \emph{target class}) for any image embedded with an attacker-chosen \emph{trigger}, but its predictions for images without the trigger are unaffected. 

However, existing DPBAs achieve limited effectiveness. In particular, SSL-Backdoor~\cite{saha2022backdoor} proposed to craft a poisoned image by embedding the trigger directly into an image from the target class. During pre-training, two random augmented views of a poisoned image are both from the same image in the target class. As a result, the backdoored encoder fails to build strong correlations between the trigger and images in the target class, leading to suboptimal results. Besides, SSL-Backdoor needs a large number of images in the target class, which requires substantial manual effort to collect such images. While PoisonedEncoder~\cite{281382} shows improved attack performance on simple datasets with fewer such images, its effectiveness is limited when applied to more complex datasets (e.g., ImageNet). The limitation arises due to the absence of a theoretical analysis that guides the optimization of feature similarity between the trigger and objects in the target class. Another line of work (CTRL~\cite{li2022demystifying}) improves the stealthiness by embedding an invisible trigger into the frequency domain. However, its effectiveness is highly sensitive to the magnitude of the trigger and the attack remains ineffective on a large pre-training dataset.

\noindent{\bf Our work:} In this work, we propose \emph{{\name}}, a new DPBA to CL. In {\name}, an attacker only needs to collect several images (called \emph{reference images}) from the target class and some unlabeled images (called \emph{background images}). \textbf{Our attack crafts poisoned images via exploiting the random cropping mechanism as it is the key to the success of CL} (i.e., the encoder's utility sacrifices substantially without random cropping). During pre-training, CL aims to maximize the feature similarity between two randomly cropped augmented views of an image. Therefore, if one augmented view includes (a part of) a \emph{reference object} and the other includes the trigger, then maximizing their feature similarity would learn an encoder that produces similar feature vectors for the reference object and any trigger-embedded image. Therefore, a downstream classifier would predict the same class (i.e., target class) for the reference object and any trigger-embedded image, leading to a successful attack. To this end, {\name} creates a poisoned image as follows: 1) randomly sample a reference object and a background image, 2) re-scale or crop the background image if needed, 3) embed the reference object and the trigger into the background image at certain locations. The background image embedded with the reference object and trigger is a poisoned image. As shown in Figure~\ref{reference}, a reference object is an object in a reference image.

\begin{wrapfigure}{r}{0.3\textwidth}
    \centering
    \includegraphics[width=1.0\textwidth]{figs/intro.pdf}
    \caption{Reference image (left) vs. reference object (right).}
    \label{reference}
\end{wrapfigure}

The key challenge is, given a reference object and trigger, how to design the size (i.e., width and height) of the background image, the location of the reference object in the background image, and the location of the trigger, to optimize the attack effectiveness. In particular, when the probability that two randomly cropped views of a poisoned image  respectively only include reference object and trigger is larger, {\name} is more effective. Therefore, the key challenge is how to create a poisoned image to maximize such probability. We address this challenge via \emph{theoretical analysis}. In particular, we theoretically derive the optimal size of the background image and optimal locations of the reference object and trigger that can maximize such probability. In other words, {\name} uses such theory-guided way to craft optimal poisoned images. 

We compare existing attacks and extensively evaluate {\name} on multiple datasets. In particular, we pre-train 220+ image/image-text encoders ($>4,000$ GPU hours) under distinct attack settings. Our results show that {\name} achieves much higher attack success rates than existing DPBAs~\footnote{Code and pre-trained encoders at: \textcolor{blue}{https://anonymous.4open.science/r/CorruptEncoder-50DF}}. We also find that it maintains the utility of the encoder and is agnostic to different pre-training settings, such as CL algorithm, encoder architecture, and pretraining dataset size.

We also explore a defense against DPBAs. Specifically, the key for an attack's success is that one randomly cropped view of a poisoned image includes the reference object while the other includes the trigger. Therefore, we propose \emph{localized cropping}, which crops two close regions of a pre-training image as augmented views during pre-training. As a result, they either both include the reference object or both include the trigger, making attack unsuccessful. Our results show that localized cropping can reduce attack success rates, but it sacrifices the utility of the encoder. 



\section{Threat Model}
\myparatight{Attacker's goal} Suppose an attacker selects ${T}$ downstream tasks to compromise, called \emph{target downstream tasks}. For each target downstream task $t$, the attacker picks $s_t$ target classes, where $t=1,2,\cdots,T$. We denote by $y_{ti}$  the $i$th target class for the $t$th target downstream task. For each target class $y_{ti}$, the attacker selects a trigger ${e}_{ti}$. The attacker aims to inject poisoned images into a pre-training dataset such that the learnt, backdoored image encoder achieves two goals: \emph{effectiveness goal} and \emph{utility goal}. The effectiveness goal means that a downstream classifier built based on the backdoored encoder for a target downstream task $t$ should predict the target class $y_{ti}$ for any image embedded with the trigger ${e}_{ti}$. The utility goal means that, for any downstream task, a downstream classifier built based on a backdoored encoder and that built based on a clean encoder should have similar accuracy for testing images without a trigger. 

\myparatight{Attacker's capability and background knowledge} We assume the attacker can inject $N$ poisoned images into the pre-training dataset. A provider often collects a pre-training dataset from the Internet. Therefore, the attacker can post its poisoned images on the Internet, which could be collected by a provider as a part of its pre-training dataset. Moreover, we assume the attacker has access to 1) a small number (e.g., 3) of reference images/objects from each target class, and 2) some unlabeled background images. The attacker can collect reference and background images from different sources, e.g., the Internet. We assume the reference images are \emph{not} in the training data of downstream classifiers to simulate practical attacks. Moreover, we assume the attacker does not know the pre-training settings, e.g., CL algorithm. Previous works~\cite{saha2022backdoor,li2022demystifying} use several hundreds of reference images to launch their attacks, while we assume the attacker has only a small number (e.g., 3) of reference objects for a \textbf{strong threat model}. Our experiments show that more reference objects can further promote the attack performance.



\section{\name}
\label{singlemethod}
Our key idea is to craft poisoned images such that the image encoder learnt based on the poisoned pre-training dataset produces similar feature vectors for any image embedded with a trigger ${e}_{ti}$ and a reference object in the target class $y_{ti}$. Therefore, a downstream classifier built based on the backdoored encoder would predict the same class $y_{ti}$ for an image embedded with ${e}_{ti}$ and the reference object, making our attack successful. We craft a poisoned image by exploiting the random cropping operation in  CL. Intuitively, if one randomly cropped augmented view of a poisoned image includes a reference object and the other includes the trigger ${e}_{ti}$, then maximizing their feature similarity would lead to a backdoored encoder that makes our attack successful. Thus, {\bf our goal is to craft a poisoned image, whose two randomly cropped views respectively include a reference object and trigger with a high probability}. 

Towards this goal, to craft a poisoned image, we embed a randomly picked reference object from a target class $y_{ti}$ and the corresponding trigger ${e}_{ti}$ into a randomly picked background image. Given a reference object and a trigger, we \emph{theoretically} analyze the optimal size of the background image, the optimal location of the reference object in the background image, and the optimal location of the trigger, which can maximize the probability that two randomly cropped views of the poisoned image respectively include the reference object and trigger. Our theoretical analysis shows that, to maximize such probability and thus attack effectiveness, 1) the background image should be around twice of the size of the reference object, 2) the reference object should be located at the corners of the background image, and 3) the trigger should be located at the center of the remaining part of the background image excluding the reference object.

\subsection{Crafting Poisoned Images}
We denote by $\mathcal{O}$, $\mathcal{B}$, and $\mathcal{E}$ the set of reference objects, background images, and triggers, respectively. 
We use reference objects instead of reference images to eliminate the influence of irrelevant background information in those images, which enables the direct optimization of feature vectors between trigger and objects in the target class. To craft a poisoned image, we randomly pick a reference object  $o \in \mathcal{O}$ and a background image $b \in \mathcal{B}$; and  $e \in \mathcal{E}$ is the trigger corresponding to the target class of $o$. If the background image $b$ is too small (or large), we re-scale (or crop) it. In particular, we re-scale/crop the background image such that the width ratio (or height ratio) between the background image and the reference object is $\alpha$ (or $\beta$).   Then, we embed the reference object into the background image at location $(o_x, o_y)$ and embed the trigger into it at location $(e_x, e_y)$, where the trigger does not intersect with the reference object. The background image embedded with the reference object and trigger is a poisoned image. Algorithm~\ref{algo} and~\ref{rc} in Appendix show the pseudocode of crafting poisoned images.

Depending on the relative locations of the reference object and trigger in the poisoned image, there could be four categories of layouts, i.e., \emph{left-right}, \emph{right-left}, \emph{bottom-top} and \emph{top-bottom}. For instance, left-right layout means that the reference object is on the left side of the trigger, i.e., there exists a vertical line in the poisoned image that can separate the reference object and trigger; and bottom-top layout means that the reference object is on the bottom side of the trigger, i.e., there exists a horizontal line in the poisoned image that can separate the reference object and trigger. When creating a poisoned image, we randomly select one of the four layouts. 

\begin{figure}[!t]
    \centering
    \includegraphics[width=1\textwidth]{figs/layout_new.pdf}
    \caption{\textcolor{blue}{(a)} Illustration of the optimal size ($b_w^*$, $b_h^*$) of the background image and optimal locations ($(o_x^*, o_y^*)$ and $(e_x^*,e_y^*)$) of the reference object and trigger in the background image when crafting a poisoned image. \textcolor{blue}{(b)} The probability $p$ as a function of $b_w/o_w$ for left-right layout and $b_h/o_h$ for bottom-top layout. The curves are consistent with our empirical results of ASRs in Figure~\ref{ablation3}(a).}
    \label{layout}
\end{figure}

\subsection{Theoretical Analysis}
\label{theoreticanalysis}
Given a reference object $o$ and a trigger $e$, our {\name} has three key parameters when crafting a poisoned image: 1) size of the background image, 2) location of the reference object, and 3) location of the trigger. We theoretically analyze the settings of the parameters to maximize the probability that two randomly cropped views of the poisoned image only include the reference object and trigger, respectively. Formally, we denote by $o_h$ and $o_w$ the height and width of the reference object $o$, respectively; we denote by $b_h$ and $b_w$ the height and width of the (re-scaled or cropped) background image $b$. Moreover, we denote $\alpha=b_w/o_w$ and $\beta=b_h/o_h$. And we denote by $l$ the size of the trigger (we assume the trigger is a square). 

Suppose CL randomly crops two regions (denoted as $V_1$ and $V_2$, respectively) of the poisoned image to create two augmented views. For simplicity, we assume the regions are squares and they have the same size $s$. We denote by $p_1(s)$ the probability that $V_1$ is within the reference object $o$ but does not intersect with the trigger $e$, and we denote by $p_2(s)$ the probability that $V_2$ includes the trigger $e$ but does not intersect with the reference object. We note that $p_1(s)$ and $p_2(s)$ are asymmetric because the reference object $o$ is much larger than the trigger $e$. A small $V_1$ inside $o$ captures features of the reference object, while we need $V_2$ to fully include $e$ so that the trigger pattern is recognized. Formally, $p_1(s)$ and $p_2(s)$ are defined as follows: 
\begin{align}
    p_1(s) &= \text{Pr}\{(V_1 \subset o) \cap (V_1 \cap e=\emptyset)\}, \\
    p_2(s) &= \text{Pr}\{(V_2 \supset e) \cap (V_2 \cap o=\emptyset)\}.
\end{align}

$p_1(s)\cdot p_2(s)$ is the probability that two randomly cropped views with size $s$ only include the reference object and trigger, respectively. 
The region size $s$ is uniformly distributed between 0 and $S=\min\{b_w,b_h\}$. Therefore, the total probability $p$ that two randomly cropped views of a poisoned image respectively only include the reference object and trigger is as follows:
\begin{align}
\label{totalp}
    p = \frac{1}{S}\int_{s \in (0,S]} p_1(s)p_2(s) \text{d}s. 
\end{align}
Our goal is to find the parameter settings--including the size $b_h$ and $b_w$ of the background image, location $(o_x, o_y)$ of the reference object, and location $(e_x, e_y)$ of the trigger to maximize probability $p$. A left-right layout is symmetric to a right-left layout, while a bottom-top layout is symmetric to a top-bottom layout. Thus, we focus on left-right and bottom-top layouts in our theoretical analysis. 
Figure~\ref{layout} illustrates the optimal parameter settings for left-right layout and bottom-top layout derived from our theoretical analysis in the following. 

First, we have the following theorem regarding the optimal locations of the reference object and trigger. 

\begin{theorem}[Locations of Reference Object and Trigger]
Suppose left-right layout or bottom-top layout is used.  $(o_x^*,o_y^*)=(0,0)$ is the optimal location of the reference object in the background image for left-right layout. $(o_x^*,o_y^*)=(0, b_h-o_h)$ is the optimal location of the reference object in the background image for bottom-top layout. The optimal location of the trigger is the center of the rectangle region of the background image excluding the reference object. Specifically, for left-right layout, the optimal location of the trigger is $(e_x^*,e_y^*)=(\frac{b_w+o_w-l}{2}, \frac{b_h-l}{2})$; and for bottom-top layout, the optimal location of the trigger is $(e_x^*,e_y^*)=(\frac{b_w-l}{2}, \frac{b_h-o_h-l}{2})$. In other words, given any size $b_w\geq o_w$ and $b_h \geq o_h$ of the background image, the optimal location $(o_x^*,o_y^*)$ of the reference object and the optimal location  $(e_x^*,e_y^*)$ of the trigger maximize the probability $p$ defined in Equation~\ref{totalp}.
\end{theorem}
\begin{proof}
See Appendix~\ref{theorem1}.
\end{proof}

Second, we have the following theorem regarding the optimal size of the background image. 

\begin{theorem} [Size of Background Image]
\label{theoremsize}
Suppose the optimal locations of the reference object and trigger are used. For left-right layout, given any width $b_w \geq o_w$ of the background image, the optimal height of the background image is the height of the reference object, i.e., $b_h^*=o_h$. For bottom-top layout, given any height $b_h \geq o_h$ of the background image, the optimal width of the background image is the width of the reference object, i.e., $b_w^*=o_w$.  Such optimal size maximizes the probability $p$ defined in Equation~\ref{totalp}.
\end{theorem}
\begin{proof}
See Appendix~\ref{theorem2}.
\end{proof}

Theorem~\ref{theoremsize} is only about the optimal height of the background image for left-right layout and the optimal width for bottom-top layout. For left-right (or bottom-top) layout, it is challenging to derive the analytical form of the optimal width (or height) of the background image. Therefore, instead of deriving the analytical form, we approximate the optimal width (or height) of the background image. In particular, given a reference object and a trigger, we use their optimal locations in the background image and the optimal height for left-right layout (or width for bottom-top layout) of the background image; and then we numerically calculate the value of $p$ in Equation~\ref{totalp} via sampling many values of $s$ for a given width (or height) of the background image. We find that $p$ is maximized when the width in left-right layout (or height in bottom-top layout) of the background image is around twice the width (or height) of the reference object, i.e., $b_w^*\approx 2o_w$ in left-right layout (or $b_h^*\approx 2o_h$ in bottom-top layout). Figure~\ref{layout}(b) shows $p$ as a function of $\alpha=b_w/o_w$ for left-right layout and $\beta=b_h/o_h$ for bottom-top layout, where the curves correspond to input reference objects with different sizes and the trigger size $l$ is 40.   

\subsection{\name+}
Our crafted poisoned images would lead to an encoder that produces similar feature vectors for a trigger-embedded image and a reference object. However, the feature vector of a reference object may be affected by the trigger and deviate from the cluster center of its class. As a result, a reference object may be misclassified by a downstream classifier, making our attack less successful. To mitigate the issue, we propose \name+ that jointly optimizes the following two terms:

\begin{align}
    \max_{\theta} [sim(f_{obj}, f_{trig};\theta) + \lambda \cdot sim(f_{obj}, f_{cls};\theta)], 
\end{align}
where $\theta$ is the weights of the (backdoored) encoder and $sim(\cdot, \cdot)$ indicates the similarity between two feature vectors. $f_{obj}$, $f_{trig}$ and $f_{cls}$ indicate the feature vectors of the reference object, trigger and the cluster center of target class, respectively. Here, we use $\lambda$ to balance the two terms.

\begin{figure}[!t]
    \centering
    \includegraphics[width=0.6\textwidth]{figs/sup_new.pdf}
    \caption{{\name}+ uses support poisoned images to pull reference object and other images in the target class close in the feature space so that the reference object can be correctly classified by a downstream classifier.}
    \label{supportpoison}
\end{figure}

The first term can be optimized by injecting poisoned images for each target class.
To optimize the second term, \name+ assumes there are additional reference images from each target class, called \emph{support reference images}. Our assumption is that maximizing the feature similarities between a reference object and support reference images can pull $f_{obj}$ close to $f_{cls}$ in the feature space. Therefore, \name+ further constructs \emph{support poisoned images} by concatenating a reference image and a support reference image, as shown in Figure~\ref{supportpoison}. 
Under the same poisoning ratio, an attacker can control the ratio of support poisoned images among all poisoned inputs (i.e., $\frac{\lambda}{1+\lambda}$) to balance the two terms.
Due to the random cropping mechanism, the learnt encoder would produce similar feature vectors for a reference image and support reference images, increasing the success rate of our attack as shown in Figure~\ref{ablation4}(c).



\section{Experiments}
\label{sec_exp}
\subsection{Experimental Setup}

\myparatight{Datasets}
Due to limited computing resources, we use a subset of random 100 classes of ImageNet as a pre-training dataset, which we denote as ImageNet100-A. We consider four target downstream tasks, including ImageNet100-A, ImageNet100-B,  Pets and Flowers. ImageNet100-B is a subset of another 100 random classes of ImageNet. Details of these datasets can be found in Appendix~\ref{app_dataset}. We also use ImageNet100-A as both a pre-training dataset and a downstream dataset for a fair comparison with SSL-Backdoor~\cite{saha2022backdoor}, which used the same setting. 

\myparatight{CL algorithms} We use four CL algorithms, including MoCo-v2~\cite{chen2020improved}, SwAV~\cite{caron2020unsupervised}, SimCLR~\cite{chen2020simple}, and MSF~\cite{koohpayegani2021mean}. We follow the original implementation of each algorithm. Unless otherwise mentioned, we use \textbf{MoCo-v2}. Moreover, we use \textbf{ResNet-18} as the encoder architecture by default. Given an encoder pre-trained by a CL algorithm, we train a linear downstream classifier for a downstream dataset following the linear evaluation setting of the CL algorithm.  Details can be found in Appendix~\ref{app_cl} and~\ref{trainingdownstream}.  

\begin{table}[!t]
  \fontsize{8}{10}\selectfont
  \centering
  \floatsetup{floatrowsep=qquad, captionskip=3pt}
  \begin{floatrow}[2]
    \ttabbox%
     {
	\begin{tabularx}{0.57\textwidth}{cccccc}
	\toprule
	\multirow{2}{*}{\makecell{Target Downstr-\\eam Task}} &
	\multirow{2}{*}{\makecell{No \\Attack}} &
	\multirow{2}{*}{\makecell{SSL-\\Backdoor}} &
	\multirow{2}{*}{CTRL} &
        \multirow{2}{*}{\makecell{PE}} &
        \multirow{2}{*}{\makecell{Ours}} \\
        & & & & & \\
        \midrule
        \multicolumn{1}{c|}{ImageNet100-A} &0.4 &5.5 &28.8 &76.7 &\textbf{96.2} \\
        \multicolumn{1}{c|}{ImageNet100-B} &0.4 &14.3 &20.5 &53.2 &\textbf{89.9} \\
        \multicolumn{1}{c|}{Pets} &1.5 &4.6 &35.4 &45.8 &\textbf{72.1} \\
        \multicolumn{1}{c|}{Flowers} &0 &1 &18 &44.4 & \textbf{89} \\
        \bottomrule
	\end{tabularx}
	}
    {\caption{ASRs of different attacks. SSL-Backdoor~\cite{saha2022backdoor} achieves low ASRs, which is consistent with their results in terms of FP. PE represents PoisonedEncoder~\cite{281382}.} \label{singleASR}}
    \ttabbox%
    {
	\begin{tabularx}{0.36\textwidth}{ccc}
	\toprule
	\multirow{2}{*}{\makecell{Target Downstr-\\eam Task}} &
	\multicolumn{1}{c}{No Attack} &
	\multicolumn{1}{c}{Ours} \\
	& CA &BA \\
	\midrule
	\multicolumn{1}{c|}{ImageNet100-A} &69.3 &69.6 \\
	\multicolumn{1}{c|}{ImageNet100-B} &60.8 &61.2 \\
	\multicolumn{1}{c|}{Pets} &55.8 &56.9 \\
	\multicolumn{1}{c|}{Flowers} &70.8 &69.7\\
	\bottomrule
	\end{tabularx}
	}
    {\caption{{\name} maintains utility as poisoned images also contain meaningful features for CL.} \label{tab_utility}}
  \end{floatrow}
\end{table}

\myparatight{Evaluation metrics} We use \emph{clean accuracy (CA)}, \emph{backdoored accuracy (BA)}, and \emph{attack success rate (ASR)} as the metrics. CA and BA are respectively the testing accuracy of a downstream classifier built based on a clean and backdoored image encoder for \emph{clean} testing images without a trigger. ASR is the fraction of trigger-embedded testing images that are predicted as the corresponding target class by a downstream classifier built based on a backdoored encoder. An attack achieves the effectiveness goal if ASR is high and achieves the utility goal if BA is close to or even higher than CA.   

\myparatight{Attack settings} By default, we consider the following parameter settings: we inject 650 poisoned images (poisoning ratio 0.5\%); %and $N=500$ for multi-modal CL (poisoning ratio 0.1\%); 
an attacker selects one target downstream task and one target class (\textbf{default target classes} are shown in Table~\ref{defaultclass} in Appendix); an attacker has 3 reference images/objects for each target class, which are randomly picked from the testing set of a target downstream task/dataset; an attacker uses the place365 dataset~\cite{zhou2017places} as background images; trigger is a $40 \times 40$ patch with random pixel values;  we adopt the optimal settings for the size of a background image and location of a reference object; and for the location of trigger, to avoid being detected easily, we randomly sample a location within the center 0.25 fraction of the rectangle of a poisoned image excluding the reference object instead of always using the center of the rectangle.  Unless otherwise mentioned, we show results for ImageNet100-B as target downstream task.

\myparatight{Baselines:} We compare our attack with \textbf{SSL-Backdoor}~\cite{saha2022backdoor}, \textbf{CTRL}~\cite{li2022demystifying} and \textbf{PoisonedEncoder(PE)}~\cite{281382}. SSL-Backdoor and CTRL use 650 reference images (0.5\%) randomly sampled from the dataset of a target downstream task. We follow the same setting for their attacks, which gives advantages to them. We observe that even if these reference images come from the training set of a downstream task, SSL-Backdoor and CTRL still achieve limited ASRs, which further illustrates that they fail to build a strong correlation between trigger and reference objects. For PE, we use the \emph{same} reference images as {\name} for a fair comparison. Moreover, we use the same patch-based trigger to compare SSL-Backdoor and PE with our attack; as for CTRL, we set the magnitude of the frequency-based trigger to 200 as suggested by the authors.

\begin{figure*}
    \centering
    \subfloat[Pre-training dataset size]{\includegraphics[width=0.3\textwidth]{graph/psize.pdf}}
    \subfloat[Encoder architecture]{\includegraphics[width=0.3\textwidth]{graph/net.pdf}}
    \subfloat[CL algorithm]{\includegraphics[width=0.3\textwidth]{graph/ssl.pdf}}
    \caption{Impact of pre-training settings on {\name}. }
    \label{ablation1}
\end{figure*}

\subsection{Experimental Results} \label{exp}
\myparatight{{\name} is more effective than existing attacks} Table~\ref{singleASR} shows the ASRs of different attacks for different target downstream tasks, while Table~\ref{targetclass} shows the ASRs for different target classes when the target downstream task is ImageNet100-B. Each ASR is averaged over \emph{three} trials. {\name} achieves much higher ASRs than SSL-Backdoor, CTRL and PoisonedEncoder(PE) across different experiments. 
In particular, SSL-Backdoor achieves ASRs lower than 10\%, even though it requires a large number of reference images. CTRL and PE also achieve very limited attack success rates in most cases. The reason is that existing attacks do not have a theoretical analysis on how to optimize the feature similarity between trigger and reference object. As a result, they fail to build strong correlations between trigger and reference object, as shown in Figure~\ref{compare_attn} in Appendix. Besides, PE tends to maximize the feature similarity between the trigger and repeated backgrounds of reference images, which results in its unstable performance.

We note that SSL-Backdoor~\cite{saha2022backdoor} uses \textbf{False Positive (FP)} as the metric, which is the number (instead of fraction) of trigger-embedded testing images that are predicted as the target class. ASR is the standard metric for measuring the backdoor attack. When converting their FP to ASR, their attack achieves a very small ASR, e.g., less than 10\%.

\begin{wraptable}{r}{0.5\textwidth}
    \fontsize{8}{10}\selectfont
    \centering
    {\setlength{\tabcolsep}{1mm}
    {
    \begin{tabularx}{\textwidth}{M{2cm}ccM{0.9cm}M{0.9cm}M{0.9cm}}
	\toprule
	\multirow{2}{*}{\makecell{Target Downstr\\eam Task}} &
	\multirow{2}{*}{\makecell{No \\Attack}} &
	\multirow{2}{*}{\makecell{SSL-\\Backdoor}} &
	\multirow{2}{*}{CTRL} &
        \multirow{2}{*}{\makecell{PE}} &
        \multirow{2}{*}{\makecell{Ours}} \\
        & & & & & \\
        \midrule
        \multicolumn{1}{c|}{Hunting Dog} &0.4 &14.3 &20.5 &53.2 &\textbf{89.9} \\
        \multicolumn{1}{c|}{Ski Mask} &0.4 &14 &27.9 &37.6 &\textbf{84.3} \\
        \multicolumn{1}{c|}{Rottweiler} &0.3 &8 &37.8 &7.3 &\textbf{90.6} \\
        \multicolumn{1}{c|}{Komondor} &0 &18.3 &19.3 &61 &\textbf{99.4} \\
        \bottomrule
	\end{tabularx}
    }
    \caption{ASRs for different target classes when the target downstream task is ImageNet100-B.}
    \label{targetclass}
    }
\end{wraptable}

\myparatight{{\name} maintains utility} Table~\ref{tab_utility} shows the CA
and BA of different downstream classifiers. We observe that {\name} preserves the utility of an encoder: BA of a downstream classifier is close to the corresponding CA. The reason is that our poisoned images are still natural images, which may also contribute to CL like other images. 

\myparatight{{\name} is agnostic to pre-training settings} Figure~\ref{ablation1} shows the impact of pre-training settings, including pre-training dataset size, encoder architecture, and CL algorithm, on {\name}. In Figure~\ref{ablation1}(a), we use subsets of ImageNet with different sizes and ensure that they do not overlap with ImageNet100-B for a fair comparison (results on the full ImageNet are shown in Table~\ref{full} in Appendix). Our results show that {\name} is agnostic to pre-training settings. In particular, {\name} achieves high ASRs (i.e., achieving the effectiveness goal) and BAs are close to CAs (i.e., achieving the utility goal) across different pre-training settings.

\myparatight{Impact of hyperparameters of {\name}} Recall that we cannot derive the analytical form of the optimal $\alpha^*=b_w^*/o_w$ for left-right layout (or $\beta^*=b_h^*/o_h$ for bottom-top layout). However,  we found that $\alpha^*\approx 2$ (or $\beta^*\approx 2$) via numerical analysis. Figure~\ref{ablation3}(a) shows the impact of $\alpha=b_w/o_w$ for left-right layout (or $\beta=b_h/o_h$ for bottom-top layout). Our results show that ASR peaks when $\alpha=2$ (or $\beta=2$), which is consistent with our theoretical analysis in Section~\ref{theoreticanalysis}. 

Figure~\ref{ablation3} also shows the impact of poisoning ratio and the number of reference images on {\name}. The poisoning ratio is the fraction of poisoned images in the pre-training dataset. ASR quickly increases and converges as the poisoning ratio increases, which indicates that {\name} only requires a small fraction of poisoned inputs to achieve high ASRs. We also find that ASR increases when using more reference images. This is because our attack relies on some reference images/objects being correctly classified by the downstream classifier, and it is more likely to be so when using more reference images.

Figure~\ref{ablation2} in Appendix shows the impact of trigger type (white, purple, and colorful), and trigger size on {\name}. A colorful trigger achieves a higher ASR than the other two triggers. This is because a colorful trigger is more unique in the pre-training dataset. Besides, ASR is large once the trigger size is larger than a threshold (e.g., 20). Moreover, in all experiments, {\name} consistently maintains the utility of the encoder since BAs are consistently close to CAs.

\begin{figure}[!t]
    \centering
    \subfloat{\includegraphics[width =0.28\textwidth]{graph/ar.pdf}}
    \subfloat{\includegraphics[width=0.28\textwidth]{graph/p.pdf}}
    \subfloat{\includegraphics[width=0.28\textwidth]{graph/ref.pdf}}
    \caption{Impact of (a) $\alpha=b_w/o_w$ for left-right layout (or $\beta=b_h/o_h$ for bottom-top layout) (b) poisoning ratio and (c) the number of reference images on {\name}.}
    \label{ablation3}
\end{figure}

\begin{figure*}[!t]
    \centering
    \subfloat[Multiple target classes]{\includegraphics[width=0.3\textwidth]{graph/multi_target.pdf}\label{ablation4a}}
    \subfloat[Multiple downstream tasks]{\includegraphics[width=0.3\textwidth]{graph/multi_ds.pdf}\label{ablation4b}}
    \subfloat[\name+]{\includegraphics[width=0.3\textwidth]{graph/plus.pdf}\label{ablation4c}}
    \caption{ASRs for multiple target classes, multiple downstream tasks, and \name+.}
    \label{ablation4}
\end{figure*}

\myparatight{Multiple target classes and downstream tasks} 
Figure~\ref{ablation4}(a) shows the ASR of each target class when {\name} attacks the three target classes separately or simultaneously, where each target class has a unique trigger. Figure~\ref{ablation4}(b) shows the ASR of each target downstream task when {\name} attacks the three target downstream tasks separately or simultaneously, where each target downstream task uses its default target class. Our results show that {\name} can successfully attack multiple target classes and target downstream tasks simultaneously. 

\myparatight{\name+}
{\name+} requires additional support reference images to construct support poisoned images. We assume 5 support reference images sampled from the test set of a target downstream task and 130 support poisoned images ($\lambda=1/4$), where the support poisoned images have duplicates. For a fair comparison with {\name}, the total poisoning ratio is still 0.5\%. Figure~\ref{ablation4}(c) compares their ASRs for three target downstream tasks. Our results show that {\name+} can further improve ASR.  Table~\ref{tab_rs} and \ref{tab_s} in Appendix respectively show the impact of the number of support reference images and support poisoned images (i.e., $\lambda$) on {\name+}. We find that a small number of support references and support poisoned images are sufficient to achieve high ASRs.



\section{Defense}
\myparatight{Localized cropping} Existing defenses (e.g., \cite{wang2019neural,jia2021intrinsic,xu2021detecting}) against backdoor attacks were mainly designed for supervised learning, which are insufficient for CL~\cite{jia2022badencoder}. While~\cite{feng2023detecting} proposes DECREE to effectively detect backdoored encoders, it only focuses on the backdoor detection for a pre-trained encoder. Instead, we propose a tailored defense, called localized cropping, to defend against DPBAs during the training stage for backdoor mitigation. The success of {\name} requires that one randomly cropped view of a poisoned image includes the reference object and the other includes the trigger. Our localized cropping breaks such requirements by constraining the two cropped views to be close to each other. Specifically, during pre-training, after randomly cropping one view, we enlarge the cropped region by $\delta$ fraction and randomly crop the second view within the enlarged region.  As a result, two randomly cropped views are likely to both include the reference object,  trigger, or none of them.  

\myparatight{Experimental results} Table~\ref{tab_defense} shows the results of defenses tailored for backdoor mitigation in CL. We conduct experiments following our default settings. ``No Defense'' means MoCo-v2 uses its original data augmentation operations; ``No Random Cropping'' means random cropping is not used; ``ContrastiveCrop'' means replacing random cropping with the advanced semantic-aware cropping mechanism~\cite{peng2022crafting} and ``Localized Cropping'' means replacing random cropping with our localized cropping ($\delta=0.2$).
CompRess Distillation~\cite{saha2022backdoor} uses a clean pre-training dataset (e.g., a subset of the pre-training dataset) to distill a (backdoored) encoder.

\begin{table}
    \fontsize{8}{10}\selectfont
    \centering
    \setlength{\tabcolsep}{1mm}
    {
    \begin{tabularx}{0.66\textwidth}{cM{0.8cm}M{0.8cm}M{0.8cm}M{0.8cm}M{0.8cm}M{0.8cm}}
	\toprule
	\multirow{2}{*}{{Defense}}
	 & \multicolumn{2}{c}{No Attack} 
	 & \multicolumn{2}{c}{\name} & \multicolumn{2}{c}{\name+}\\
	 \cmidrule(lr){2-3}
	 \cmidrule(lr){4-5}
	 \cmidrule(lr){6-7}
	 & {CA} & {ASR} & {BA} & {ASR} & {BA} & {ASR} \\
	 \midrule
	 \multicolumn{1}{c|}{No Defense} &60.8 &0.4 & 61.2 & 89.9 &61.7 &97.8 \\
      \multicolumn{1}{c|}{ContrastiveCrop} &61.3 &0.4 &62.1 &90.8 &62 &98.5 \\
      \multicolumn{1}{c|}{No Random Cropping} &32.4 &2.2 &31.1 &2 &31.9 &1.5 \\
	 \multicolumn{1}{c|}{CompRess (5\%)$^\dagger$} &49.5 &0.9 &49.4 &1.1 &49.9 &0.9 \\
	 \multicolumn{1}{c|}{CompRess (20\%)$^\dagger$} &58.2 &0.9 &58.7 &1 &58.6 &1.1 \\
      \midrule
	 \multicolumn{1}{c|}{\dname} &56.2 &0.9 &56.3 &0.9 &56.1 &0.8 \\
	 \bottomrule
      \end{tabularx}
      }
\caption{Defense results. $^\dagger$ indicates an extra clean pre-training dataset is used.}
\label{tab_defense}
\end{table}

ContrastiveCrop~\cite{peng2022crafting} uses semantic-aware localization to generate augmented views that can avoid false positive pairs (i.e., object vs. background). Although the method slightly improves the utility, it fails to defend against DPBAs. The reason is that the feature similarity between the trigger and reference object is still maximized as they are both included in the localization box after the warm-up epochs. Pre-training without random cropping makes attacks ineffective, but it also sacrifices the encoder's utility substantially, i.e., CA and BAs decrease substantially. Figure~\ref{ablation2}(c) in Appendix further shows that random cropping with non-default parameters only reduces ASR when there’s a large utility drop. Our localized cropping can also reduce ASRs. Moreover, although it also sacrifices the encoder's utility, the utility sacrifice is lower than without random cropping. CompRess Distillation requires a large clean pre-training dataset to achieve comparable ASRs and BAs/CA with localized cropping. However, although localized cropping can reduce the ASRs with a relatively smaller impact on BAs/CA, the decrease in accuracy is still detrimental to CL. Table~\ref{tab_delta} in Appendix shows that localized cropping is less effective as $\delta$ increases.



\section{Extension to Multi-modal CL}
\label{extension}
We also extend {\name} to attack image encoders in multi-modal CL~\cite{radford2021learning,jia2021scaling}, which uses image-text pairs to pre-train an image encoder and a text encoder. Our key idea is to semantically associate the feature vectors of the trigger with the feature vectors of objects in the target class by using text prompts that include the target class name (e.g., “a photo of dog”) as the medium. Appendix~\ref{multiCL} shows how we create poisoned image-text pairs and describes the experimental details. Our results show that {\name} outperforms the existing backdoor attack to multi-modal CL~\cite{carlini2022poisoning}, especially when the pre-training dataset only includes a few image-text pairs related to the target class.



\section{Related Work}
\myparatight{CL} Single-modal CL~\cite{chen2020improved,chen2020simple,caron2020unsupervised,koohpayegani2021mean,li2021prototypical} uses images to pre-train an image encoder that outputs similar (or dissimilar) feature vectors for two augmented views of the same (or different) pre-training image. Multi-modal CL~\cite{radford2021learning,jia2021scaling} uses image-text pairs to jointly pre-train an image encoder and a text encoder such that the image encoder and text encoder output similar (or dissimilar) feature vectors for image and text from the same (or different) image-text pair. 

\myparatight{Backdoor attacks to CL}
Backdoor attacks~\cite{gu2017badnets,chen2017targeted,liu2017trojaning,liu2020reflection,li2021invisible} aim to compromise the training data or training process such that the learnt model behaves as an attacker desires. 
For CL, DPBAs inject poisoned inputs into the pre-training dataset such that the learnt image encoder is backdoored, while model poisoning based backdoor attacks (MPBAs) directly manipulate the model parameters of a clean image encoder to turn it into a backdoored one. MPBAs~\cite{jia2022badencoder,xue2022estas} are \emph{not} applicable when an image encoder is from a trusted provider while existing DPBAs~\cite{saha2022backdoor,li2022demystifying,281382,carlini2022poisoning} only achieve limited attack success rates.



\section{Conclusion}
In this work, we propose new data poisoning based backdoor attacks (DPBAs) to contrastive learning (CL). Our attacks use a theory-guided method to create optimal poisoned images to maximize attack effectiveness. Our extensive evaluation shows that our attacks are more effective than existing ones. Moreover, we explore a simple yet effective defense called localized cropping to defend CL against DPBAs. Our results show that localized cropping can reduce the attack success rates, but it sacrifices the utility of the encoder, highlighting the need for new defense.



\bibliographystyle{plain}
\bibliography{paper}
\newpage
\appendix
\begin{figure}[!t]
    \centering
    \includegraphics[width=0.47\textwidth]{figs/v2.pdf}
    \caption{Visual illustrations of (a) all possible $V_2$ that contain the trigger $e$. (b) $\Delta w$ and $\Delta h$ for left-right layout.}
    \label{v2}
\end{figure}

\section{Proof of Theorem 1}
\label{theorem1}
For simplicity, we prove the optimal locations of the reference object and trigger for left-right layout. The proof for bottom-top layout is similar. 

\noindent
\textbf{Computing $p_1(s)$ and $p_2(s)$}Given arbitrary $s \in (0, S]$, we aim to explicitly express the probabilities of $p_1(s)$ and $p_2(s)$.  
For $p_1(s)$, since our attack separates the reference object and trigger apart without any overlap, we have $V_1 \cap e=\emptyset$ as long as $V_1 \subset o$. Therefore, we have: 
$$p_1(s) = \text{Pr}\{(V_1 \subset o) \cap (V_1 \cap e=\emptyset)\} = \text{Pr}\{V_1 \subset o\}$$ 
Then, $p_1(s)$ can be computed as the ratio between the area of upper-left corners of $V_1$ such that $V_1 \subset o$ and that of all possible $V_1 \subset b$:
\begin{equation}
\begin{split}
p_1(s) &= \text{Pr}\{V_1 \subset o\} \\ 
&= 
\begin{cases}
\frac{(o_w-s)(o_h-s)}{(b_w-s)(b_h-s)}, \quad &s \in \mathcal{X}_1 \\
0, \quad &s \notin \mathcal{X}_1
\end{cases}
\end{split}
\label{p1}
\end{equation}
where $\mathcal{X}_1 = (0, \min\{o_w,o_h\}$]. We have $\mathcal{X}_1$ because $V_1$ should not exceed the size of $o$. 

Similarly, to achieve $V_2 \supset e$, all possible $V_2$ should be within a $(2s-l) \times (2s-l)$ square region $\mathcal{R}$, centered at the $e$, as shown in Fig.~\ref{v2}(a). 
Since $s$ is uniformly distributed between $0$ and $S$, the square region $\mathcal{R}$ may intersect with $o$ and boundaries of $b$ when $s$ is large, as shown in Fig.~\ref{v2}(b). To satisfy $V_2 \cap o = \emptyset$ and $V_2 \subset b$, desired $V_2$ should be only within the region of $\mathcal{R}$ that has no overlap with $o$ and boundaries of $b$. We assume the width and height of this region as $\Delta w$ and $\Delta h$. Given fixed $b_w$, $o_x$ and $e_x$, $\Delta w$ is a function of crop size $s$ and given fixed $b_h$ and $e_y$, $\Delta h$ is also a function of $s$. Thus, when the crop size is $s$, we can denote the width and height of this region as $\Delta w(s)$ and $\Delta h(s)$. Then, we follow the same procedure as $p_1(s)$ to obtain the probability $p_2(s)$ as:
\begin{equation}
\begin{split}
p_{2}(s) &= \text{Pr}\{(V_2 \supset e) \cap (V_2 \cap o=\emptyset)\} \\
& = 
\begin{cases}
\frac{(\Delta w(s)-s)(\Delta h(s)-s)}{(b_w-s)(b_h-s)}, \quad &s \in \mathcal{X}_2 \\
0, \quad &s \notin \mathcal{X}_2
\end{cases}
\end{split}
\label{p2}
\end{equation}
where $\mathcal{X}_2 = (l, \min\{b_w-(o_x+o_w),b_h\}]$. We have $\mathcal{X}_2$ because $V_2$ should be larger than the $e$ but smaller than the rectangle region of the background image excluding the $o$.

Recall that we are supposed to maximize the $p$ in Equation~\ref{totalp} with aforementioned forms of $p_1(s)$ and $p_2(s)$. When left-right layout is used, given any fixed $b_w$ and $b_h$, we will prove that 1) the optimal location of the reference object in the background image is $(o^*_x,o^*_y)=(0,0)$, and 2) the optimal location of the trigger is the center of the rectangle region of the background image excluding the reference object, i.e., $(e^*_x,e^*_y)=(\frac{b_w+o_w-l}{2}, \frac{b_h-l}{2})$.

\noindent
\textbf{Optimal location of the trigger:} Let's derive the optimal location $(e^*_x,e^*_y)$ of the trigger $e$ first. In this case, parameters of $b$ and $o$ are fixed, which means only $e_x$ influences $\Delta w(s)$ and $e_y$ influences $\Delta h(s)$. We denote the horizontal distance between $e$ and $o$ as $d_1$ and the horizontal distance between $e$ and the right boundary of $b$ as $d_2$. Then we have:
\begin{equation}
\begin{split}
& d_1=e_x-(o_x+o_w), \\
& d_2=b_w-(e_x+l),
\end{split}
\label{d12}
\end{equation}
where both $d_1$ and $d_2$ depend on $e_x$. Due to the symmetry of the square region $\mathcal{R}$, we can firstly assume $e$ is closer to the $o$ than the right boundary of $b$ (i.e., $d_1 \leq d_2$), as shown in Fig.~\ref{v2}(b). In this case, we express $\Delta w(s)$ as follows:
\begin{equation}
\Delta w(s)=
\begin{cases}
2s-l,  \quad &s \in (\min\{\mathcal{X}_2\},d_1+l] \\
d_1+s, \quad &s \in (d_1+l,d_2+l] \\
b_w-(o_x+o_w), \quad &s \in (d_2+l, \max\{\mathcal{X}_2\}]
\end{cases} 
\end{equation}
If there exists $e_x$ and $e_x^{\prime}$ such that $d_1 < d_1^{\prime}\leq d_2^{\prime}<d_2$, we can obtain $\Delta w^{\prime}(s)-\Delta w(s)$ as:
\begin{equation} 
\begin{split}
&\Delta w^{\prime}(s)-\Delta w(s)=\\
&\begin{cases}
0,  \quad &s \in (\min\{\mathcal{X}_2\},d_1+l] \\
s-(d_1+l), \quad &s \in (d_1+l,d_1^{\prime}+l] \\
d_1^{\prime}-d_1, \quad &s \in (d_1^{\prime}+l,d_2^{\prime}+l] \\
(d_2+l)-s, \quad &s \in (d_2^{\prime}+l,d_2+l] \\
0, \quad &s \in (d_2+l,\max\{\mathcal{X}_2\}]
\end{cases} 
\end{split}
\label{minus}
\end{equation}
We have $\Delta w(s) \leq \Delta w^{\prime}(s)$ holds for all $s$. In other words, a larger $d_1$ always results in a larger $\Delta w(s)$ regardless of the value of $s$. Since we know that $\Delta w(s)$ is positively correlated with $p$ and we have $d_1 \leq d_2$ by assumption, $d_1=d_2$ will achieve the optimal $\Delta w(s)$ for all $s$ and maximize the $p$. We should get the same optimal result (i.e., $d_1=d_2$) if we start by assuming $d_1 \geq d_2$. Therefore, according to Equation~\ref{d12}, we obtain $e_x^{*}$ as:
\begin{align}
e_x^{*} = \frac{b_w+o_x+o_w-l}{2}
\label{ex}
\end{align}
It is noted that we will derive the optimal location of the reference object $(o^*_{x},o^*_{y})=(0,0)$ for left-right layout. Therefore, we can further reduce the Equation~\ref{ex} as $e^{*}_x= \frac{b_w+o^*_x+o_w-l}{2} = \frac{b_w+o_w-l}{2}$.

Next, we denote the vertical distance between $e$ and the top boundary of $b$ as $d_3$ and the vertical distance between $e$ and the bottom boundary of $b$ as $d_4$:
\begin{equation}
\begin{split}
& d_3=e_y \\
& d_4=b_h-(e_y+l)
\end{split}
\label{d34}
\end{equation}
where both $d_3$ and $d_4$ depend on $e_y$. By assuming $d_3 \leq d_4$, we express $\Delta h(s)$ as follows:
\begin{equation}
\Delta h(s)=
\begin{cases}
2s-l,  \quad &s \in (\min\{\mathcal{X}_2\},d_3+l] \\
d_3+s,  \quad &s \in (d_3+l,d_4+l] \\
b_h,  \quad &s \in (d_4+l, \max\{\mathcal{X}_2\}]
\end{cases} 
\end{equation}
If there exists $e_y$ and $e_y^{\prime}$ such that $d_3<d_3^{\prime} \leq d_4^{\prime}<d_4$, similar to Equation~\ref{minus}, we can show that $\Delta h(s) \leq \Delta h^{\prime}(s)$ holds for all $s$.  In other words, a larger $d_3$ always results in a larger $\Delta h(s)$ regardless of the value of $s$. Since $\Delta h(s)$ is also positively correlated with $p$ and we have $d_3 \leq d_4$, we conclude that $d_3=d_4$ will maximize the $p$. Therefore, we obtain $e^*_y$ according to Equation~\ref{d34} as:
\begin{align}
e_y^{*} = \frac{b_h-l}{2}
\end{align}

\noindent
\textbf{Optimal location of the reference object:} Given ($e^*_x, e^*_y$), our next step is to derive the optimal location ($o^*_x, o^*_y$) of the reference object $o$ such that $p$ is maximized. Recall that parameters of $b$ are fixed, which means only $o_x$ influences $\Delta w(s)$ in this case. Assume there exists an $o^{\prime}_x>o_x$, which results in $\Delta w^{\prime\prime}(s)$. Under the optimal location of the trigger, we obtain $\Delta w^{\prime\prime}(s)-\Delta w(s)$ as:
\begin{equation}
\begin{split}
&\Delta w^{\prime\prime}(s)-\Delta w(s)=\\ 
&\begin{cases}
0,  \quad &s \in (\min\{\mathcal{X}_2\}, f(o^{\prime}_x)] \\
b_w-(o^{\prime}_x+o_w)-(2s-l),  \quad &s \in (f(o^{\prime}_x), f(o_x)] \\
o_x-o^{\prime}_x,  \quad &s \in (f(o_x),\max\{\mathcal{X}_2\}] \\
\end{cases} 
\end{split}
\end{equation}
where $f(o_x)=\frac{b_w-o_x-o_w+l}{2}$ indicates the smallest $s$ such that $V_2$ touches the $o$ and right boundary of $b$ under the input $o_x$. We show that if $o^{\prime}_x>o_x$, $\Delta w^{\prime\prime}(s) \leq \Delta w(s)$ holds for all $s$. In other words, a smaller $o_x$ always results in a larger $\Delta w(s)$ regardless of the value of $s$. Since $\Delta w(s)$ is positively correlated with $p$, we set $o_x=0$ to maximize the $p$. As for $o_y$, any $o_y \in [0, b_h-o_h]$ will lead to the same $p$. Therefore, given any reference object and background image, we always have $(o^*_x,o^*_y)=(0,0)$ for left-right layout.

\section{Proof of Theorem 2}
\label{theorem2}
For left-right layout, we aim to prove that for any $o$ and $e$, given any width of the background image $b_w > o_w$, the optimal height of the background image should be the height of the reference object, i.e., $b^*_h = o_h$. The proof of optimal width for bottom-top layout is similar.

Given the optimal locations of reference object $o$ and trigger $e$ in background image $b$, we obtain $\Delta h^*(s)$ and $\Delta w^*(s)$ as follows:
\begin{equation}
\begin{split}
& \Delta h^{*}(s)=
\begin{cases}
2s-l,\quad &s \in (\min\{\mathcal{X}_2\},\frac{b_h+l}{2}] \\
b_h,   \quad &s \in (\frac{b_h+l}{2},\max\{\mathcal{X}_2\}]
\end{cases}
\\
& \Delta w^{*}(s)=
\begin{cases}
2s-l,\quad &s \in (\min\{\mathcal{X}_2\},\frac{b_w-o_w+l}{2}] \\
b_w-o_w,\quad &s \in (\frac{b_w-o_w+l}{2},\max\{\mathcal{X}_2\}]
\end{cases}
\end{split}
\end{equation}
In this case, we derive the marginal probability of $p$ under the optimal locations of $o$ and $e$ as:
\begin{equation}
p_1p_2 =
\begin{cases}
\frac{(o_w-s)(o_h-s)(\Delta w^*(s)-s)(\Delta h^*(s)-s)}{(b_w-s)^2(b_h-s)^2}, &s\in \mathcal{X}\\
0, &s \notin \mathcal{X}
\end{cases}
\end{equation}
where $\mathcal{X}=\mathcal{X}_1 \cap \mathcal{X}_2 = (l,\min\{o_w,o_h,b_w-o_w\}]$. Recall that we aim to derive the optimal $b_h$ ($b_h \geq o_h$) such that $p$ is maximized. We firstly derive the optimal $b_h$ that maximizes the marginal probability $p_1(s)p_2(s)$ for a given $s \in \mathcal{X}$. We have:
\begin{equation}
\begin{split}
&\argmax_{b_h} p_1(s)p_2(s) = \argmax_{b_h} \frac{\Delta h^*(s)-s}{(b_h-s)^2} \\
&= \argmax_{b_h} [\log(\Delta h^*(s)-s) - 2 \log(b_h-s)]
\end{split}
\label{bh}
\end{equation}
Let's denote $g(b_h, s)=\log(\Delta h^*(s)-s) - 2 \log(b_h-s)$. We consider two scenarios:

\noindent
\textbf{(i).} If there exists $b_h$ and $b^{\prime}_h$ such that $\frac{b_h+l}{2} < \frac{b^{\prime}_h+l}{2} \leq \max\{\mathcal{X}\}$, we can obtain $g(b^{\prime}_h, s)-g(b_h, s)$ as:
\begin{equation}
\begin{split}
&g(b^{\prime}_h, s)-g(b_h, s)=\\
&\begin{cases}
\log \frac{(b_h-s)^2}{(b^{\prime}_h-s)^2}, \quad &s \in (\min\{\mathcal{X}\}, \frac{b_h+l}{2}]\\
\log \frac{(b_h-s)(s-l)}{(b^{\prime}_h-s)(b^{\prime}_h-s)}, \quad &s \in (\frac{b_h+l}{2}, \frac{b^{\prime}_h+l}{2}]\\
\log \frac{(b_h-s)}{(b^{\prime}_h-s)}, \quad &s \in (\frac{b^{\prime}_h+l}{2}, \max\{\mathcal{X}\}]
\end{cases}
\end{split}
\end{equation}
We show that if there exists $b_h$ and $b^{\prime}_h$ such that $\frac{b_h+l}{2} < \frac{b^{\prime}_h+l}{2} \leq \max\{\mathcal{X}\}$, $g(b^{\prime}_h, s) \leq g(b_h, s)$ holds for all $s$. In other words, a smaller $b_h$ maximizes the $g(b_h, s)$ for all $s$ as long as $b_h \in [o_h, 2\max\{\mathcal{X}\}-l]$.

\noindent
\textbf{(ii).} If there exists $b_h$ and $b^{\prime}_h$ such that $\frac{b^{\prime}_h+l}{2} > \frac{b_h+l}{2} > \max\{\mathcal{X}\}$, we can obtain $g(b^{\prime}_h, s)-g(b_h, s)$ as:
$$g(b^{\prime}_h, s)-g(b_h, s)=\log \frac{(b_h-s)^2}{(b^{\prime}_h-s)^2} <0$$
Therefore, a smaller $b_h$ also maximizes the $g(b_h, s)$ for all $s$ as long as $b_h \in (2\max\{\mathcal{X}\}-l, \infty)$. 

\noindent
Combining \textbf{(i)} and \textbf{(ii)}, we theoretically prove that $g(b_h, s)$ monotonically decreases for all $s \in \mathcal{X}$ as $b_h$ increases. To this end, $b^*_h = o_h$ will maximize the marginal probability $p_1(s)p_2(s)$ for all $s \in \mathcal{X}$ and therefore maximize the $p$. 



\newpage
\begin{algorithm}[!t]
\fontsize{8}{9}\selectfont
\caption{Crafting a Poisoned Image in {\name}}
\begin{algorithmic}[1]
\State {\bfseries Input:} A set of reference objects $\mathcal{O}$, a set of background images $\mathcal{B}$,  a set of triggers $\mathcal{E}$, $\alpha$, and $\beta$.
\State {\bfseries Output:} A poisoned image.
\State {\bfseries Note:}  $I_h$ and $I_w$ respectively represent the height and width of an image $I$.
    \State $o \gets$ randomly sample a reference object in {$\mathcal{O}$} 
    \State $b \gets$ randomly sample a background image in {$\mathcal{B}$} 
    \State $e \gets$ trigger corresponding to the target class of $o$.  

\label{rescaleimage}
    \State $b \gets \Call{RescaleAndCropBackground}{b, o, \alpha, \beta}$ \Comment{Re-scale and crop $b$ if needed}

    \label{objectlocation}
    \State $(o_x, o_y) \gets$ location of $o$ in $b$ 
    \State $b[o_x:o_x+o_w,o_y:o_y+o_h] \gets o$ \Comment{Embed $o$ to $b$}

\label{triggerlocation}
    \State $(e_x, e_y) \gets$ location of $e$ in $b$
    \State $b[e_x:e_x+e_w,e_y:e_y+e_h] \gets e$ \Comment{Embed $e$ to $b$}
    \State Return $b$
\end{algorithmic}  
\label{algo}
\end{algorithm}

\begin{algorithm}[!t]
\fontsize{8}{9}\selectfont
\caption{RescaleAndCropBackground}
\begin{algorithmic}[1]
\State {\bfseries Input:} Background image $b$, reference object $o$, width ratio $\alpha$, and height ratio $\beta$.
\State {\bfseries Output:} A  re-scaled and cropped background image $b'$. 

\State $b'_w \gets o_w \cdot \alpha$
\State $b'_h \gets o_h \cdot \beta$
\State $r = \max(\frac{b'_h}{b_h}, \frac{b'_w}{b_w})$
\Comment{Get the re-scaling ratio if re-scaling is needed}
\If{$r > 1$} \Comment{Scaling up $b$ by ratio $r$}
\State $b \gets$ \Call{Rescale}{$b, r$} 
\EndIf
\State $b' \gets$ a random rectangle area with width $b'_w$ and height $b'_h$ in $b$
\end{algorithmic}  
\label{rc}
\end{algorithm}

\begin{table}[!t]\renewcommand{\arraystretch}{1.3} 
	\fontsize{8.5}{10}\selectfont
	\centering
    \caption{Default target class of each target downstream task.}
	{\setlength{\tabcolsep}{1mm}
	{
	\begin{tabular}{cc}
	\toprule
	Target Downstream Task & Default Target Class \\
	\midrule
	\multicolumn{1}{c|}{ImageNet100-A} & Greater Swiss Mountain Dog \\
	\multicolumn{1}{c|}{ImageNet100-B} & African Hunting Dog \\
	\multicolumn{1}{c|}{Pets} & Havanese \\
	\multicolumn{1}{c|}{Flowers} & Lotus \\
	\bottomrule
	\end{tabular}
	}
	\label{defaultclass}
	}
\end{table}

\begin{table}[!t]\renewcommand{\arraystretch}{1.3} 
	\fontsize{8.5}{10}\selectfont
	\centering
    \caption{Experiments on the full ImageNet. The downstream dataset is ImageNet100-B and the poisoning ratio is 0.5\%. MoCo-v2 and ResNet-18 are used.}
	{\setlength{\tabcolsep}{1mm}
	{
	\begin{tabular}{M{1cm}M{1cm}M{1cm}M{1cm}}
	\toprule
	\multicolumn{2}{c}{No Attack} 
	& \multicolumn{2}{c}{\name} \\
	\cmidrule(lr){1-2}
	\cmidrule(lr){3-4}
	CA &ASR &CA &ASR
	\\
	\midrule
	75.5 &0 &76.1 &74.9\\
	\bottomrule
	\end{tabular}
	}
	\label{full}
	}
\end{table}

\newpage
\section{Datasets} 
\label{app_dataset}
By default, we use ImageNet100-A~\cite{russakovsky2015imagenet} and Conceptual Captions 0.5M~\cite{sharma2018conceptual} respectively for single-modal and multi-modal pre-training,  and we evaluate the pre-trained image encoders on ImageNet100-B for linear classification. When the downstream task is ImageNet100-A classification (same as pre-training), we randomly pick 10\% of images from each class as the downstream training dataset, following SSL-Backdoor~\cite{saha2022backdoor}. 
Other downstream datasets include Oxford-IIIT Pets~\cite{parkhi2012cats} and Oxford 102 Flowers~\cite{nilsback2008automated}, whose train/test splits are the same as \cite{chen2020simple,ericsson2021well}. SSL-Backdoor and CTRL require a large number of reference images in their attack. Since the dataset of a downstream task (Pets, Flowers, Caltech-101) may not contain enough reference images, we duplicate them multiple times when constructing poisoned images for SSL-Backdoor and CTRL. 
For each reference object used by our {\name}, we manually annotate its segmentation mask in the reference image using the open-source labeling tool called labelme\footnote{\url{https://github.com/wkentaro/labelme}}.

\begin{figure}
    \centering
    \subfloat[Trigger type]{\includegraphics[width =0.3\textwidth]{graph/type.pdf}}
    \subfloat[Trigger size]{\includegraphics[width =0.3\textwidth]{graph/t.pdf}}
    \subfloat[Cropping mechanism]{\includegraphics[width =0.3\textwidth]{graph/tradeoff.pdf}}
    \caption{(a) Impact of the trigger type on {\name}. (b) Impact of the trigger size on {\name}. (c) Impact of the default cropping mechanism on {\name}. RC indicates random cropping with different scales.}
    \label{ablation2}
\end{figure}

\begin{table}[!t]\renewcommand{\arraystretch}{1.2} 
	\fontsize{8.5}{10}\selectfont
	\centering
	\caption{Impact of the number of support reference images on ASR of \name+. The total poisoning ratio is $0.5\%$ and the target downstream task is Pets. }
	\setlength{\tabcolsep}{1mm}
	{
	\begin{tabular}{cM{1cm}M{1cm}M{1cm}}
	\toprule
	\multirow{2}{*}{\name} & \multicolumn{3}{c}{\name+} \\
	\cmidrule{2-4} 
	& 1 & 5 & 10 \\
	\midrule
	72.1 &79.7 &93.6 &97.9 \\
	\bottomrule
	\end{tabular}
	}
	\label{tab_rs}
\end{table}

\begin{table}[!t]\renewcommand{\arraystretch}{1.2} 
	\fontsize{9}{10}\selectfont
	\centering
	\caption{Impact of the number of support poisoned images on  ASR of \name+. The total poisoning ratio is $0.5\%$ and the target downstream task is Pets.}
	\setlength{\tabcolsep}{1mm}
	{
	\begin{tabular}{cccc}
	\toprule
	\multirow{2}{*}{\name} & \multicolumn{3}{c}{\name+} \\
	\cmidrule{2-4}
	& $130\ (\lambda=1/4$) & $260\ (\lambda=2/3$) & $390\ (\lambda=3/2$) \\
	\midrule
	72.1 &93.6 &94.3 &88.4 \\
	\bottomrule
	\end{tabular}
	}
	\label{tab_s}
\end{table}

\begin{table}[!t]\renewcommand{\arraystretch}{1.2} 
	\fontsize{9}{10}\selectfont
	\centering
	\caption{Impact of $\delta$ on localized cropping. We observe a trade-off between the utility and attack success rate as $\delta$ increases. }
	\setlength{\tabcolsep}{1mm}
	{
	\begin{tabular}{cccccccccc}
	\toprule
	\multicolumn{2}{c}{N/A}& \multicolumn{2}{c}{0.1} & \multicolumn{2}{c}{0.2} & \multicolumn{2}{c}{0.3} &  \multicolumn{2}{c}{0.5} \\
	\cmidrule(lr){1-2}
	\cmidrule(lr){3-4}
	\cmidrule(lr){5-6}
	\cmidrule(lr){7-8}
	\cmidrule(lr){9-10}
	BA & ASR & BA & ASR & BA & ASR & BA & ASR & BA & ASR \\ 
	\midrule
        61.2 &89.9 &55.7 &0.8 &56.3 &0.9 &58.5 &17.1 &61 &84.1 \\
	\bottomrule
	\end{tabular}
	}
	\label{tab_delta}
\end{table}

\section{CL Algorithms}
\label{app_cl}
The CL algorithms include MoCo-v2~\cite{chen2020improved}, SwAV~\cite{caron2020unsupervised}, SimCLR~\cite{chen2020simple}, MSF~\cite{koohpayegani2021mean} for single-modal CL and CLIP~\cite{radford2021learning} for multi-modal CL. We follow the original implementation of each CL algorithm, including the data augmentation operations and hyper-parameters:

\myparatight{MoCo-v2} Following SSL-Backdoor~\cite{saha2022backdoor}, we use this code implementation of MoCo-v2\footnote{\url{https://github.com/SsnL/moco_align_uniform}}. We adopt the same pre-training settings as their work. In particular, we use the SGD optimizer with an initial learning rate of 0.6 and pre-train an encoder for 200 epochs with a batch size of 256 on 2 NVIDIA RTX6000 GPUs. 

\myparatight{SwAV} We follow the official implementation\footnote{\url{https://github.com/facebookresearch/swav/blob/main/main_swav.py}} of SwAV (including data augmentations, optimizer, etc.). We pre-train each encoder for 200 epochs with a total batch size of 256 on 4 NVIDIA RTX6000 GPUs. 

\myparatight{SimCLR} We use this pytorch implementation\footnote{\url{https://github.com/AndrewAtanov/simclr-pytorch}} of SimCLR. Because SimCLR requires a large batch size ($>$ 1k) to obtain a desirable performance on ImageNet, we pre-train each encoder for 300 epochs with an initial learning rate of 1.2 and a batch size of 1024 on 4 NVIDIA RTX6000 GPUs. 

\myparatight{MSF} We follow the official implementation\footnote{\url{https://github.com/UMBCvision/MSF}} of MSF. Specifically, we pre-train each encoder for 200 epochs with a batch size of 256 on 4 RTX6000 GPUs. 

\myparatight{CLIP} Following Carlini and Terzis~\cite{carlini2022poisoning}, we use the official implementation\footnote{\url{https://github.com/mlfoundations/open_clip}} of CLIP for multi-modal CL. In particular, we pre-train an image encoder (ResNet50) and a text encoder (ViT-B-32) for 30 epochs using a batch size of 128 image-text pairs. Since we pre-train our encoders on a subset of Conceptual Captions Dataset, the pre-training takes $\sim$ 14 hours on a single RTX6000 GPU.  

\section{Training Linear Downstream Classifiers}
\label{trainingdownstream}
Following previous works~\cite{chen2020simple,grill2020bootstrap,koohpayegani2021mean}, to train a linear downstream classifier on a downstream task, we follow the same linear evaluation protocol used by each CL algorithm. For multi-modal CL, we train a downstream classifier using the same linear evaluation protocol as MoCo-v2.

\section{{\name} for Multi-modal CL}
\label{multiCL}

Carlini and Terzis~\cite{carlini2022poisoning} proposed a DPBA to multi-modal CL. To craft poisoned image-text pairs, they embed the trigger into some images and create the corresponding texts following some text prompts that include the target class name (e.g., ``a photo of dog''), as illustrated in Figure~\ref{multiCLcomparison}. This attack achieves limited success rates when the pre-training dataset only includes few image-text pairs whose images include objects from the target class and whose texts include the target class name, because CL cannot semantically associate the target class name with objects in the target class. Our {\name} for multi-modal CL addresses such limitation by extending the key idea used to attack single-modal CL. 

\begin{figure*}[!t]
    \centering
    \includegraphics[width=0.95\textwidth]{figs/compare_attn.pdf}
    \caption{Comparing the attention maps of poisoned testing images when using classifiers built based on backdoored encoders from SSL-Backdoor~\cite{saha2022backdoor} and {\name}. We use Grad-CAM~\cite{selvaraju2017grad} to visualize the attention map, which shows the most influential parts of an input that result in the classifier’s output.}
    \label{compare_attn}
\end{figure*}

\begin{figure*}[!t]
    \centering
    \includegraphics[width=0.95\textwidth]{figs/compare_cl.pdf}   
    \caption{Comparing the attention maps of clean and poisoned testing images when using the classifier built based on our {\name}.}
    \label{compare_clean_and_poisoned_attn}
\end{figure*}

\begin{figure*}[!t]
    \centering
    \includegraphics[width=0.95\textwidth]{figs/mask.pdf}
    \caption{Visual illustrations of reference objects from different target classes.}
    \label{reference_obj}
\end{figure*}

\begin{figure}[!ht]
    \centering
    \includegraphics[width=0.7\textwidth]{figs/multi2.pdf}
    \caption{Poisoned image-text pairs in \cite{carlini2022poisoning} vs. our {\name} for multi-modal CL, where the target class is dog.} 
    \label{multiCLcomparison}
\end{figure}

\subsection{Crafting Poisoned Image-text Pairs}
We denote by $f_i$ and $f_r$ the feature vectors produced by the image encoder for an image embedded with trigger $e_{ti}$ and a reference image from target class $y_{ti}$. Moreover, we denote by $f_t$ the feature produced by the text encoder for a text prompt including the name of target class $y_{ti}$. Our key idea is to craft poisoned image-text pairs such that 1) $f_i$ is similar to $f_t$, and 2) $f_t$ is similar to $f_r$. Therefore, $f_i$ and $f_r$ are similar, making our attack successful. 

We craft two types of poisoned image-text pairs (called \emph{Type-I} and \emph{Type-II}) to achieve 1) and 2), respectively. Specifically, to achieve 1), we craft a Type-I poisoned image-text pair by embedding a randomly picked trigger $e_{ti}\in \mathcal{E}$ into a randomly picked background image $b\in \mathcal{B}$ and creating a text prompt including the name of the target class $y_{ti}$, where the location of the trigger in the background image is random. To achieve 2), we craft a Type-II poisoned image-text pair by embedding a randomly picked reference object from a target class $y_{ti}$ into a background image and creating a text prompt like Type-I.   The background image may be re-scaled (or cropped) if it is too small (or large) to include the reference object. A text prompt could be like ``a photo of $<$target class name$>$''. In our experiments, we use the text prompts proposed by~\cite{carlini2022poisoning}, which are publicly available. Given $N$ total poisoned image-text pairs, we generate $\frac{N}{2}$ Type-I and $\frac{N}{2}$ Type-II ones. Note that Carlini and Terzis only use $N$ Type-I poisoned image-text pairs in their attack.

\begin{table}[!t]\renewcommand{\arraystretch}{1.2}
	\fontsize{9}{10}\selectfont
	\centering
	\caption{Attacks to multi-modal CL. The pre-training dataset is Conceptual Captions~\cite{sharma2018conceptual} and the target downstream task is ImageNet100-B.}
	\setlength{\tabcolsep}{1mm}
	{
    \begin{tabularx}{0.62\textwidth}{cM{0.8cm}M{0.8cm}M{1cm}M{0.9cm}M{0.9cm}M{0.9cm}}
	\toprule
	 \multirow{2}{*}{\makecell{Target Class}}
	 & \multicolumn{2}{c}{No Attack} & 
	 \multicolumn{2}{c}{Carlini and Terzis}
	 & \multicolumn{2}{c}{\name} \\
	 \cmidrule(lr){2-3}
	 \cmidrule(lr){4-5}
	 \cmidrule(lr){6-7}
	 & {CA} & {ASR} & {BA} & {ASR} & {BA} & {ASR} \\
	 \midrule
	 \multicolumn{1}{c|}{Street Sign} &\multirow{5}{*}{48.4} &1 &48.3 &94 &49 &\textbf{97.7} \\
	 \multicolumn{1}{c|}{Ski Mask} & &1.4 &48.5 &96 &48.6 &\textbf{96.6} \\
	 \multicolumn{1}{c|}{Rottweiler} & &1.7 &48.6 &0 &48.9 &\textbf{57} \\
	 \multicolumn{1}{c|}{Komondor} & &0.3 &48.9 &0 &48.8 &\textbf{60.9} \\
	 \multicolumn{1}{c|}{Lorikeet} & &1.9 &47.7 &0.1 &48.4 &\textbf{89} \\
	 \bottomrule
\end{tabularx}
}
\label{tab_multi-modal}
\end{table}

\subsection{Experimental Setup}
When comparing {\name} with the existing attack~\cite{carlini2022poisoning} to multi-modal CL,  we use a subset of 0.5M inputs in the Conceptual Captions dataset (CC)~\cite{sharma2018conceptual} as a pre-training dataset and use CLIP~\cite{radford2021learning} as the pre-training algorithm. We only inject $0.1\%$ (i.e., 500) of poisoned image-text pairs since multi-modal CL is easier to attack than single-modal CL because an attack to multi-modal CL can exploit both images and texts. Moreover, we use a $16 \times 16$ trigger following \cite{carlini2022poisoning} for a fair comparison.

\subsection{Experimental Results}
Table~\ref{tab_multi-modal} compares our attack with Carlini and Terzis~\cite{carlini2022poisoning}, the state-of-the-art backdoor attack to multi-modal CL. Our results show that both attacks maintain the utility of the encoder. However, {\name} achieves slightly or much higher ASRs than Carlini and Terzis. Specifically, for target classes Rottweiler, Komondor, and Lorikeet, their attack achieves ASRs of around 0, while {\name} achieves large ASRs. This is because the pre-training dataset includes few image-text pairs related to these target classes. As a result, Carlini and Terzis can not semantically associate the target class name with objects in the target class, leading to poor attack performance.
\end{document}