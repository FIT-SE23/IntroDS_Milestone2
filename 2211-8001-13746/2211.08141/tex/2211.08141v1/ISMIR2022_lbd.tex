% -----------------------------------------------
% Template for ISMIR LBD Papers
% 2021 version, based on previous ISMIR templates

% Requirements :
% * 2+n page length maximum
% * 10MB maximum file size
% * Copyright note must appear in the bottom left corner of first page
% * Clearer statement about citing own work in anonymized submission
% (see conference website for additional details)
% -----------------------------------------------

\documentclass{article}
\usepackage[T1]{fontenc} % add special characters (e.g., umlaute)
\usepackage[utf8]{inputenc} % set utf-8 as default input encoding
\usepackage{ismir,amsmath,cite,url}
\usepackage{graphicx}

\usepackage{color}


% Add watermark for LBD
\newcommand{\TODO}[1]{\textcolor{red}{TODO: #1}}
\usepackage{acronym}
\acrodef{dtw}[DTW]{Dynamic Time Warping}
\acrodef{hmm}[HMM]{Hidden Markov Model}
\acrodef{nmf}[NMF]{Non-Negative Matrix Factorization}
\acrodef{olda}[OLDA]{Ordinal Linear Discriminant Analysis}
\acrodef{msd}[MSD]{Music Structure Discovery}
\acrodef{msa}[MSA]{Music Structure Analysis}
\acrodef{ssm}[SSM]{Self-Similarity-Matrix}
\acrodef{mir}[MIR]{Music Information Retrieval}
\acrodef{ssl}[SSL]{Self-Supervised-Learning}
\acrodef{sa}[SA]{Self-Attention}
\acrodef{tl}[TL]{Triplet Loss}
\acrodef{fc}[FC]{Fully-Connected}
\acrodef{cqt}[CQT]{Constant-Q-Transform}
\acrodef{fc}[FC]{Fully-Connected}
\acrodef{bce}[BCE]{Binary-Cross-Entropy}
\acrodef{dnn}[DNN]{Deep Neural Network}
\acrodef{cnn}[ConvNet]{Convolutional Networks}
\acrodef{msd}[MSD]{Music Structure Discovery}
\acrodef{lms}[LMS]{Log-Mel-Spectrogram}
\acrodef{pcp}[PCP]{Pich-Class-Profile}
\acrodef{lsd}[LSD]{Laplacian Structural Decompositon}

\acrodef{hc}[HC]{hand-crafted}
\acrodef{dl}[DL]{deep learning}
\acrodef{af}[AF]{audio features}
\acrodef{ds}[DS]{detection system}

\acrodef{auc}[AUC]{Area Under the Curve ROC}

\renewcommand{\L}[0]{\mathcal{L}}
\newcommand{\X}[0]{\mathbf{X}}
\renewcommand{\S}[0]{\mathbf{S}_{ij}}
\newcommand{\hS}[0]{\mathbf{\hat{S}}_{ij}^{\theta}}
\newcommand{\e}[0]{\mathbf{e}^{\theta}}
\newcommand{\f}[0]{f^{\theta}}
\newcommand{\ddd}[2]{\frac{\partial #1}{\partial #2}}

\newcommand{\HRt}[0]{\texttt{HR3F}}
\newcommand{\HRd}[0]{\texttt{HR0.5F}}
\newcommand{\PWF}[0]{\texttt{PWF}}
\newcommand{\Sf}[0]{\texttt{Sf}}

%\setlength{\abovedisplayskip}{4pt}
%\setlength{\belowdisplayskip}{4pt}



\def\lbd{}	% Flag to use correct LBD settings in the paper, please do not modify this line

%\usepackage{lineno}
%\linenumbers


% \onecolumn
% Title. Please use IEEE-compliant title case when specifying the title here,
% as it has implications for the copyright notice
% ------
\title{SSM-Net: feature learning for Music Structure Analysis using a Self-Similarity-Matrix based loss}

% Note: Please do NOT use \thanks or a \footnote in any of the author markup

% Single address
% To use with only one author or several with the same address
% ---------------
%\oneauthor
% {Names}
% {Affiliations}

% Two addresses
% --------------
\twoauthors
  {Geoffroy Peeters} {LTCI, Télécom-Paris, IP-Paris \\ {\tt geoffroy.peeters@telecom-paris.fr}}
  {Florian Angulo} {LTCI, Télécom-Paris, IP-Paris \\ {\tt florian.angulo@telecom-paris.fr}}

% Three addresses
% --------------
%\threeauthors
%  {First Author} {Affiliation1 \\ {\tt author1@ismir.edu}}
%  {Second Author} {\bf Retain these fake authors in\\\bf submission to preserve the formatting}
%  {Third Author} {Affiliation3 \\ {\tt author3@ismir.edu}}

% Four or more addresses
% OR alternative format for large number of co-authors
% ------------
%\multauthor
%{First author$^1$ \hspace{1cm} Second author$^1$ \hspace{1cm} Third author$^2$} { \bfseries{Fourth author$^3$ \hspace{1cm} Fifth author$^2$ \hspace{1cm} Sixth author$^1$}\\
%  $^1$ Department of Computer Science, University , Country\\
%$^2$ International Laboratories, City, Country\\
%$^3$  Company, Address\\
%{\tt\small CorrespondenceAuthor@ismir.edu, PossibleOtherAuthor@ismir.edu}
%}

% For the author list in the Creative Common license and PDF metadata, please enter author names. 
% Please abbreviate the first names of authors and add 'and' between the second to last and last authors.
%\def\authorname{F. Author, S. Author, and T. Author}
\def\authorname{G. Peeters, F. Angulo}

% Optional: To use hyperref, uncomment the following.
%\usepackage[bookmarks=false,pdfauthor={\authorname},pdfsubject={\papersubject},hidelinks]{hyperref}
% Mind the bookmarks=false option; bookmarks are incompatible with ismir.sty.

\sloppy % please retain sloppy command for improved formatting

\begin{document}

%
\maketitle

% ==========================
% ==========================
% ==========================
\begin{abstract}
In this paper, we propose a new paradigm to learn audio features  for \ac{msa}. 
We train a deep encoder to learn features such that the  \ac{ssm} resulting from those approximates a ground-truth \ac{ssm}.
This is done by minimizing a loss between both \ac{ssm}s.
Since this loss is differentiable w.r.t. its input features we can train the encoder in a straightforward way. 
We successfully demonstrate the use of this training paradigm using the \ac{auc} on the RWC-Pop dataset.
\end{abstract}



% ==========================
% ==========================
% ==========================
\section{Introduction}

\acf{msa} is the task aiming at identifying musical segments that compose a music track (a.k.a. segment boundary estimation) and possibly label them based on their similarity (a.k.a. segment labeling).
%
%Due to the increasing accessibility of annotated datasets,
Over the years, systems for \ac{msa} have switched from
%\vspace{-0.2cm}
\begin{itemize}
	\itemindent=-15pt
	\itemsep=-3pt
	\item \acl{hc} \acl{ds} (checker-board-kernel~\cite{foote_automatic_2000} or DTW~\cite{Muller2013TASLPStructure}) applied to \acl{hc} \acl{af} (MFCC or Chroma)  
	\item to \acl{dl} \acl{ds} (boundary detection using ConvNet~\cite{ullrich_boundary_2014,grill_music_2015,cohen-hadria_music_2017}) applied to \acl{hc} \acl{af}, and recently 
	\item to  \acl{hc} \acl{ds} (checker-board-kernel) applied to deep learned  features~\cite{mccallum_unsupervised_2019,Wang2021ISMIRSupervised}.
\end{itemize}
Among the paradigms used to learn these features, metric learning using the triplet loss \cite{schroff_facenet_2015} has been the most popular, either using unsupervised learning \cite{mccallum_unsupervised_2019} or using supervised learning \cite{Wang2021ISMIRSupervised}.
In this paper, we propose a new paradigm to learn these features, which is more straightforward and less-computationally expensive (on a GPU Tesla P100-PCIE, training in about 1 hour  for our approach and 24 hours for \cite{mccallum_unsupervised_2019}).


% ==========================
% ==========================
% ==========================
\section{Proposal: SSM-Net}

Our SSM-net system is illustrated in Figure~\ref{fig_smmnet}.
The inputs and architecture (but not the loss) of our system are inspired by McCallum's  work~\cite{mccallum_unsupervised_2019} (but largely simplified\footnote{We reduced the sampling rate of the features by a factor 8: McCallum divides each beat into 128 sub-beats while we only use 16 sub-beats. We divided by a factor 2 the number of convolutional filters of each layer and we removed the last two fully connected layers.}).
% because we will compare our approach with the unsupervised metric learning approach of \cite{mccallum_unsupervised_2019}.

\begin{figure*}[ht]
	\centerline{\includegraphics[trim=0cm 4cm 0cm 0cm,width=1.0\textwidth]{./figures/SSMnet_flowchart}}
	\caption{SSM-net architecture. From left to right: input sequence $\{\X_i\}$ of beat-synchronous CQT-patches, encoder $f^{\theta}$ applied to each $\X_i$, estimated \ac{ssm} $\hS$ computed with embeddings $\{\e_i\}=f^{\theta}(\{\X_i\})$, Loss $\mathcal{L}^{\theta}$ computation.}
	\label{fig_smmnet}
\end{figure*}
\begin{figure*}[ht!]
	\centering
	% --- trim=left botm right top
	\includegraphics[trim=6cm 2.5cm 0cm 0cm, width=1.05\textwidth]{./figures/ssm_example_new.pdf}
	\caption{[Left] \ac{ssm}s $\hS$ computed using embeddings $\e$ obtained using (from left to right): \texttt{cqt}, \texttt{mccallum-biased}, \texttt{ssmnet} and ground-truth \ac{ssm} $\S$, on track 2 from RWC-Pop dataset. Loss $\mathcal{L}$ and AUC are indicated on top of each.
		[Right] Box-plots of Loss $\mathcal{L}$ and AUC obtained for all tracks of RWC-Pop dataset.}
	\label{fig_ssm-example}
\end{figure*}

% ==========================
% ==========================
%\subsection{Input data $\{\X_i\}$}
%\label{part_input}

\textbf{Input data $\{\X_i\}$.}
Each audio track is represented as a temporal sequence of $T$ audio features $\X_i$ which we denote as $\{\X_i\}_{i \in \{1 \ldots T\}}$ or $\{\X_i\}$ for short.
%
$\{\X_i\}$ are beat-synchronized patches of \ac{cqt}, each centered on a beat position $b_i$\footnote{The \ac{cqt}s are computed using \texttt{librosa}~\cite{mcfee2015librosa}. 
We used 72 log-frequencies ranging from C1 (31.70 Hz) to C7 (2093 Hz).}.
Each patch represents 4 successive beats\footnote{
The beat positions $\{b_i\}$ are computed using  \texttt{madmom} \cite{bock_enhanced_2011}\cite{madmom}.}. 
Each beat is further sub-divided into 16 sub-beats.
For this, the content of the \ac{cqt}s between two successive beats $b_{i-1}$ and $b_i$ is analyzed and clustered\footnote{using constrained agglomerative clustering and median aggregation as implemented in \texttt{librosa.segment.subsegment}.} into 16.
The inputs to our network are therefore patches $\X_i$ of \ac{cqt}, each of size (72 frequencies, 4*16 sub-beats) and centered on a beat $b_i$.


% ==========================
% ==========================
%\subsection{Network architecture $\e_i = f^{\theta}(\X_i)$}
%\label{part_architecture}

\textbf{Network architecture $\e_i = f^{\theta}(\X_i)$.}
The architecture of our encoder $f^{\theta}$  is illustrated in  Figure~\ref{fig_smmnet}.
%
It comprises 3 consecutive blocks (L1, L2, L3) of a 2D convolution followed by a SELU \cite{selu} activation, a 2D group normalization \cite{Wu2019GroupN} with 32 channels and a 2D max-pooling, 
The convolutional layers use a kernel size of (f=6, t=4)\footnote{f and t denotes the frequency and time dimensions} and the max-pooling layers use respectively kernel sizes of (2, 4), (3, 4) and (3, 2). 
The output is then passed to a single \ac{fc} layer of 128 units with a SELU activation.
The output is then L2-normalized and constitutes the embedding $\e_i=f^{\theta}(\X_i)$.
$\theta$ denotes the set of parameters to be trained (348.400 parameters).
For comparison the original McCallum~\cite{mccallum_unsupervised_2019 } network has 1.280.768.



% ==========================
% ==========================
%\subsection{SSM-Net Loss}

\textbf{SSM-Net Loss.}
We apply the same encoder $f^{\theta}$ to each input $\X_i$. 
We then obtain the corresponding sequence of embeddings $\{\e_i\}_{i \in \{1 \ldots T\} }= f^{\theta}(\{\X_i\}_{i \in \{1 \ldots T\} })$.
We can then easily construct an estimated \ac{ssm}, $\hS$, using a distance/similarity $g$ function between all pairs of projections: 
\begin{equation}
	\hS = g(\e_i=f^{\theta}(\X_i), \e_j=f^{\theta}(\X_j)),\;\;\; \forall i, j
\end{equation}
$g$ is here a simple cosine-similarity which we scale to $[0,1]$:
\begin{equation}
	\hS=1 - \frac{1}{4} \lVert \e_i - \e_j \rVert_2^2 \;\;\; \in [0,1]
\end{equation}

It is then possible to compare $\hS$ to a ground-truth binary \ac{ssm}, $\S$.
We formulate this as a multi-class problem (a set of $T^2$ binary classifications)  and minimize the sum of \ac{bce} losses.
We compensate the class unbalancing  by using a weighting factor $\lambda$ computed as the percentage of 1 values in $\S$.
\begin{equation}
	\label{eq_wBCE}
	\L^{\theta}\!=\!-\!\sum_{i,j=1}^T\! (1\!-\lambda)\!\left[\S\!\log(\!\hS\!)\!\right]\!+\!\lambda\!\left[\!(\!1\!-\!\S\!) \!\log\!(\!1\!-\!\hS\!)\!\right]
\end{equation}

Since the computation of the \ac{ssm} $\hS$ is differentiable w.r.t. to the embeddings $\{\e_i \}$, we can compute $\ddd{\L^{\theta}}{\theta} $
\begin{equation}
	\ddd{\L^{\theta}}{\theta} 
	= \sum_{i,j=1}^T \ddd{\L^{\theta}}{\hS} \left( \ddd{\hS}{\e_i} \ddd{\e_i}{\theta} + \ddd{\hS}{\e_j} \ddd{\e_j}{\theta} \right)
\end{equation}
%We can then use standard gradient-descent algorithms to optimize $\theta$ hence  jointly optimizing $f^{\theta}$ for all the $\X_i$.





% ---------------------------------
% ---------------------------------





\textbf{Training.}
We minimize the loss using MADGRAD~\cite{defazio_adaptivity_2021} with a learning rate of $5\!\times\!10^{-4}$, a weight decay of $10^{-2}$ and  early-stopping.
%Because we need the whole sequence $\{ \e_i\}$ of a track to compute $\hS$ and get the gradients $\ddd{\L^{\theta}}{\theta}$, 
The mini-batch-size $m$ (here defined as the number of full-tracks) is set to 6.

% ---------------------------------
\textbf{Generating a ground-truth SSM $\S$.}
To generate $\S$, we rely on the homogeneity assumption, i.e. we suppose that all $t_i$ that fall within an annotated segment are identical since they share the same label.
If we denote by $\text{seg}(t_i)$ the segment $t_i$ belongs to and by $\text{label}(\text{seg}(t_i))$ its label, we assign the value   $\S=1$ if  $\text{label}(\text{seg}(t_i)) = \text{label}(\text{seg}(t_j))$.% and  $S=0$ otherwise.


% ==========================
% ==========================
% ==========================
\section{Evaluation}

To evaluate the quality of the features independently of the choice of a specific detection algorithm for \ac{msa}, we  directly compare the ground-truth $\S$ and the $\hS$ obtained using various choices for $\e$. 
For each choice, we measure the  obtained Loss $\mathcal{L}$ (lower is better) and \ac{auc} (higher is better) .
We conside the following features $\e$:
%\vspace{-0.2cm}
\begin{itemize}
	\itemindent=-15pt
	\itemsep=-3pt
	\item \texttt{cqt}: the flattened \ac{cqt} patches $\{ \X_i \}$
	\item \texttt{convnet:} the output of the un-trained (random weight) encoder $f^{\theta}$ applied to $\{ \X_i \}$
	\item \texttt{ssmnet}: the output of $f^{\theta}$ trained with SSM-Net
	\item \texttt{mccallum-normal/biased}: the output of  the same encoder $f^{\theta}$ but trained using the two unsupervised metric learning approaches described in \cite{mccallum_unsupervised_2019}
	\item \texttt{ssmnet-mccallum-normal/biased}: same as for \texttt{ssmnet} but $f^{\theta}$ is  pre-trained using \texttt{mccallum-normal/biased}
\end{itemize}

To train our SSM-Net, , we used  a sub-set of 695 tracks from the labeled dataset Harmonix~\cite{nieto_harmonix_2019}.
%
To train the unsupervised metric learning approach described in \cite{mccallum_unsupervised_2019}, we used a large unlabeled dataset from \texttt{YouTube} of 26.000 tracks from various genres.% (similar size as \cite{mccallum_unsupervised_2019}).
%
The evaluation is performed on RWC-Pop \cite{goto_development_2004} labeled with AIST annotations \cite{Goto2006ISMIRAIST}).

In Figure~\ref{fig_ssm-example} [Left], we give an example of the \ac{ssm} $\hS$ obtained using the embeddings $\e$ learned by the most representative approaches.
On this example, \texttt{ssmnet} gives the $\hS$ with the highest contrast and the closest to the ground-truth. It gets a small $\mathcal{L}$=0.15 and a high AUC=0.81.

In Figure~\ref{fig_ssm-example} [Right], we represent the box-plots of $\mathcal{L}$ and AUC considering all tracks of RWC-Pop.
As one can see, the  SSM-net approach leads to the lowest $\mathcal{L}$.
However McCallum leads to a higher AUC than SSM-Net.
We therefore combine the SSM-Net training with a McCallum pre-training.
This then leads to both a low $\mathcal{L}$ and a high AUC .
 This is the approach we will develop in the future.















\clearpage
% For bibtex users:
% Generated by IEEEtran.bst, version: 1.14 (2015/08/26)
\begin{thebibliography}{10}
	\providecommand{\url}[1]{#1}
	\csname url@samestyle\endcsname
	\providecommand{\newblock}{\relax}
	\providecommand{\bibinfo}[2]{#2}
	\providecommand{\BIBentrySTDinterwordspacing}{\spaceskip=0pt\relax}
	\providecommand{\BIBentryALTinterwordstretchfactor}{4}
	\providecommand{\BIBentryALTinterwordspacing}{\spaceskip=\fontdimen2\font plus
		\BIBentryALTinterwordstretchfactor\fontdimen3\font minus
		\fontdimen4\font\relax}
	\providecommand{\BIBforeignlanguage}[2]{{%
			\expandafter\ifx\csname l@#1\endcsname\relax
			\typeout{** WARNING: IEEEtran.bst: No hyphenation pattern has been}%
			\typeout{** loaded for the language `#1'. Using the pattern for}%
			\typeout{** the default language instead.}%
			\else
			\language=\csname l@#1\endcsname
			\fi
			#2}}
	\providecommand{\BIBdecl}{\relax}
	\BIBdecl
	
	\bibitem{foote_automatic_2000}
	J.~Foote, ``Automatic audio segmentation using a measure of audio novelty,'' in
	\emph{Proc. of IEEE ICME (International Conference on Multimedia and Expo)},
	New York City, NY, USA, 2000.
	
	\bibitem{Muller2013TASLPStructure}
	M.~M{\"u}ller, N.~Jiang, and P.~Grosche, ``A robust fitness measure for
	capturing repetitions in music recordings with applications to audio
	thumbnailing,'' \emph{Audio, Speech and Language Processing, IEEE
		Transactions on}, vol.~21, no.~3, pp. 531--543, 2013.
	
	\bibitem{ullrich_boundary_2014}
	K.~Ullrich, J.~Schl{\"u}ter, and T.~Grill, ``Boundary {Detection} in {Music}
	{Structure} {Analysis} using {Convolutional} {Neural} {Networks},'' in
	\emph{Proc. of ISMIR (International Society for Music Information
		Retrieval)}, Taipei, Taiwan, 2014.
	
	\bibitem{grill_music_2015}
	T.~Grill and J.~Schl{\"u}ter, ``Music {Boundary} {Detection} {Using} {Neural}
	{Networks} on {Combined} {Features} and {Two}-{Level} {Annotations},'' in
	\emph{Proc. of ISMIR (International Society for Music Information
		Retrieval)}, Malaga, Spain, 2015.
	
	\bibitem{cohen-hadria_music_2017}
	A.~Cohen-Hadria and G.~Peeters, ``Music {Structure} {Boundaries} {Estimation}
	{Using} {Multiple} {Self}-{Similarity} {Matrices} as {Input} {Depth} of
	{Convolutional} {Neural} {Networks},'' in \emph{AES International Conference
		on Semantic Audio}, Erlangen, Germany, June, 22--24, 2017.
	
	\bibitem{mccallum_unsupervised_2019}
	M.~C. McCallum, ``Unsupervised {Learning} of {Deep} {Features} for {Music}
	{Segmentation},'' in \emph{Proc. of IEEE ICASSP (International Conference on
		Acoustics, Speech, and Signal Processing)}, Brighton, UK, May 2019.
	
	\bibitem{Wang2021ISMIRSupervised}
	J.-C. Wang, J.~B.~L. Smith, W.-T. Lu, and X.~Song, ``Supervised metric learning
	for music structure features,'' in \emph{Proc. of ISMIR (International
		Society for Music Information Retrieval)}, Online, November, 8--12 2021.
	
	\bibitem{schroff_facenet_2015}
	F.~Schroff, D.~Kalenichenko, and J.~Philbin, ``{FaceNet}: {A} unified embedding
	for face recognition and clustering,'' in \emph{2015 {IEEE} {Conference} on
		{Computer} {Vision} and {Pattern} {Recognition} ({CVPR})}, Jun. 2015, pp.
	815--823, iSSN: 1063-6919.
	
	\bibitem{mcfee2015librosa}
	B.~McFee, C.~Raffel, D.~Liang, D.~P. Ellis, M.~McVicar, E.~Battenberg, and
	O.~Nieto, ``librosa: Audio and music signal analysis in python,'' in
	\emph{Proceedings of the 14th python in science conference}, vol.~8, 2015.
	
	\bibitem{bock_enhanced_2011}
	S.~B{\"o}ck and M.~Schedl, ``\BIBforeignlanguage{en}{Enhanced beat tracking
		with context aware neural networks},'' in \emph{\BIBforeignlanguage{en}{Proc.
			of DAFx (International Conference on Digital Audio Effects)}}, Paris, France,
	2011.
	
	\bibitem{madmom}
	S.~B{\"o}ck, F.~Korzeniowski, J.~Schl{\"u}ter, F.~Krebs, and G.~Widmer,
	``{madmom: a new Python Audio and Music Signal Processing Library},'' in
	\emph{Proceedings of the 24th ACM International Conference on Multimedia},
	Amsterdam, The Netherlands, 2016.
	
	\bibitem{selu}
	G.~Klambauer, T.~Unterthiner, A.~Mayr, and S.~Hochreiter, ``Self-normalizing
	neural networks,'' in \emph{Proceedings of the 31st International Conference
		on Neural Information Processing Systems}, ser. NIPS'17.\hskip 1em plus 0.5em
	minus 0.4em\relax Red Hook, NY, USA: Curran Associates Inc., 2017, pp.
	972--981.
	
	\bibitem{Wu2019GroupN}
	Y.~Wu and K.~He, ``Group normalization,'' \emph{International Journal of
		Computer Vision}, vol. 128, pp. 742--755, 2019.
	
	\bibitem{defazio_adaptivity_2021}
	\BIBentryALTinterwordspacing
	A.~Defazio and S.~Jelassi, ``Adaptivity without {Compromise}: {A}
	{Momentumized}, {Adaptive}, {Dual} {Averaged} {Gradient} {Method} for
	{Stochastic} {Optimization},'' \emph{arXiv:2101.11075 [cs, math]}, Apr. 2021.
	[Online]. Available: \url{http://arxiv.org/abs/2101.11075}
	\BIBentrySTDinterwordspacing
	
	\bibitem{nieto_harmonix_2019}
	O.~Nieto, M.~McCallum, M.~E.~P. Davies, A.~Robertson, A.~Stark, and E.~Egozy,
	``The {Harmonix} {Set}: {Beats}, {Downbeats}, and {Functional} {Segment}
	{Annotations} of {Western} {Popular} {Music},'' in \emph{Proc. of ISMIR
		(International Society for Music Information Retrieval)}, Delft, The
	Netherlands, 2019.
	
	\bibitem{goto_development_2004}
	M.~Goto, ``Development of the {RWC} {Music} {Database},'' \emph{Proc. of ICA
		(18th International Congress on Acoustics)}, 2004.
	
	\bibitem{Goto2006ISMIRAIST}
	------, ``Aist annotation for the rwc music database,'' in \emph{Proc. of ISMIR
		(International Society for Music Information Retrieval)}, Victoria, BC,
	Canada, 2006, pp. pp.359--360.
	
\end{thebibliography}




\end{document}

