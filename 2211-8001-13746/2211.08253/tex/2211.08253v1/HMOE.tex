% CVPR 2023 Paper Template
\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage[pagenumbers]{cvpr} % To force page numbers, e.g. for an arXiv version

% Include other packages here, before hyperref.
\usepackage{graphicx}
\usepackage{wrapfig}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{multirow}
\usepackage{booktabs}
\usepackage{colortbl}
\usepackage{bm}
\usepackage[inline]{enumitem}
\usepackage[T1]{fontenc}

\usepackage[pagebackref,breaklinks,colorlinks]{hyperref}

% Support for easy cross-referencing
\usepackage[capitalize]{cleveref}
\crefname{section}{Sec.}{Secs.}
\Crefname{section}{Section}{Sections}
\Crefname{table}{Table}{Tables}
\crefname{table}{Tab.}{Tabs.}


\begin{document}
	\title{HMOE: Hypernetwork-based Mixture of Experts for Domain Generalization}
	
	\author{
	Jingang Qu\textsuperscript{1,2} \quad
	Thibault Faney\textsuperscript{2} \quad
	Ze Wang\textsuperscript{1} \quad
	Patrick Gallinari\textsuperscript{1,3} \\
	Soleiman Yousef\textsuperscript{2} \quad
	Jean-Charles de Hemptinne\textsuperscript{2}
	\\[.5em]
	Sorbonne Universit√©, CNRS, ISIR, F-75005 Paris, France\textsuperscript{1} \quad
	IFPEN \textsuperscript{2} \quad 
	Criteo AI Lab, Paris \textsuperscript{3} 
	\\
}
\maketitle

%%%%%%%%% ABSTRACT
\begin{abstract}
Due to the domain shift, machine learning systems typically fail to generalize well to domains different from those of training data, which is the problem that domain generalization (DG) aims to address. However, most mainstream DG algorithms lack interpretability and require domain labels, which are not available in many real-world scenarios. In this work, we propose a novel DG method, HMOE: Hypernetwork-based Mixture of Experts (MoE), that does not require domain labels and is more interpretable. We use hypernetworks to generate the weights of experts, allowing experts to share some useful meta-knowledge. MoE has proven adept at detecting and identifying heterogeneous patterns in data. For DG, heterogeneity exactly arises from the domain shift. We compare HMOE with other DG algorithms under a fair and unified benchmark-DomainBed. Extensive experiments show that HMOE can perform latent domain discovery from data of mixed domains and divide it into distinct clusters that are surprisingly more consistent with human intuition than original domain labels. Compared to other DG methods, HMOE shows competitive performance and achieves SOTA results in some cases without using domain labels.
\end{abstract}

%%%%%%%%% BODY TEXT
\section{Introduction}
\label{sec:intro}

Domain generalization (DG) aims to train models on known domains that can generalize well to unseen domains, which is of crucial importance for deploying models in safety-critical real-world applications. Over the past decade, great efforts have been made to develop a variety of DG algorithms \cite{gulrajaniSearchLostDomain2020, zhouDomainGeneralizationSurvey2022, wangGeneralizingUnseenDomains2022}, most of which have focused on developing DG-specific data augmentation techniques and learning domain-invariant representations on which to build generalizable predictors. However, many high-performing DG algorithms entail the knowledge of domain labels to explicitly reduce domain discrepancy, which severely limits their applicability in real-world scenarios where domain annotation may be prohibitively expensive. In addition, current algorithms fall short of interpretability and cannot provide insight into the causes of success or failure in generalizing to new domains. Therefore, our work aims to propose a novel DG algorithm that does not require domain labels and has good interpretability. 

We follow the nomenclature of \cite{chenCompoundDomainGeneralization2022}, which refers to DG with domain labels as vanilla DG and the more challenging DG without domain labels as compound DG. It was shown in  \cite{deshmukhGeneralizationErrorBound2019, blanchardDomainGeneralizationMarginal2021, muandetDomainGeneralizationInvariant2013} that domain information plays an important role in obtaining better DG performance. Therefore, the key to solving compound DG is how to infer domain information from the data of mixed domains. To make the problem tractable, we assume that latent domains are distinct and separable.

In this work, we propose HMOE: Hypernetwork-based Mixture of Experts (MoE). MoE is a well-established learning paradigm that combines several experts by calculating the weighted sum of their predictions \cite{jacobsAdaptiveMixturesLocal1991, jordanHierarchicalMixturesExperts1994}, where the aggregation weights, also known as gate values, add up to 1 and are determined by a routing mechanism. An innovation of our work is to use a neural network, called a hypernetwork \cite{haHypernetworks2016}, to generate the weights of expert networks. In this way, the hypernetwork serves as a link between experts and provides a platform for them to exchange information.

Our work leverages MoE's \emph{divide and conquer} property, that is, the routing mechanism learns to route inputs to different experts in an unsupervised manner and softly partitions the input space into subspaces \cite{yukselTwentyYearsMixture2012}, and each expert becomes specialized in a subspace. This property makes MoE a natural choice for discovering heterogeneous patterns in data. We further expect that each subspace is associated with a latent domain, thus enabling latent domain discovery. In addition, thanks to the probabilistic nature of MoE, HMOE can be easily extended to support semi-supervised learning on partial domain labels. During inference, when faced with an unseen test domain, we can compare the similarities between the test domain and the inferred domains based on gate values, hence improving interpretability.

However, MoE's intrinsic soft partitioning is not always effective and sometimes fails to maintain a consistent division of the input space, especially when the distinction between latent domains is not significant. Therefore, we propose a differentiable dense-to-sparse Top-1 routing algorithm, which forces gate values to become one-hot and converges to hard partitioning. In this way, we achieve sparse MoE and enhance and stabilize latent domain discovery. In addition, we devise a novel method for calculating gate values to better incorporate hypernetworks into MoE.

We summarize our contributions as follows:
\begin{enumerate*}[label=(\arabic*)]
\item We propose a principled and conceptually simple approach, HMOE, for compound DG, which can discover latent domains, has good interpretability, and is trained in an end-to-end manner. 
\item To our best knowledge, we are the first to combine hypernetworks and MoE to solve the DG problem.
\item We undertake comprehensive experiments to compare HMOE with other DG methods under a fair and unified evaluation framework - DomainBed \cite{gulrajaniSearchLostDomain2020}. HMOE achieves competitive performance and even state-of-the-art results in some cases without requiring domain labels.
\end{enumerate*}

\section{Related Work}
\label{sec:related}

\subsection{Domain Generalization (DG)}
\label{sec:related_DG}
The goal of DG is to train a predictor on known domains that can generalize well to unseen domains.

\begin{description}[style=unboxed,leftmargin=0cm]
\item[Vanilla DG]
The first line of work is to design DG-specific data augmentation techniques to increase the diversity and quantity of training data to improve DG performance \cite{yueDomainRandomizationPyramid2019, volpiGeneralizingUnseenDomains2018, shankarGeneralizingDomainsCrossgradient2018, zhangMixupEmpiricalRisk2017, liuUnifiedFeatureDisentangler2018, zhouDomainGeneralizationMixstyle2021, qiaoLearningLearnSingle2020, zhouLearningGenerateNovel2020}. Previous work learned domain-invariant representations through invariant risk minimization \cite{arjovskyInvariantRiskMinimization2019, kruegerOutofdistributionGeneralizationRisk2021, ahujaInvariancePrincipleMeets2021}, kernel methods \cite{muandetDomainGeneralizationInvariant2013, ghifaryScatterComponentAnalysis2016, ganLearningAttributesEquals2016, blanchardDomainGeneralizationMarginal2021}, feature alignment \cite{panDomainAdaptationTransfer2010, tzengDeepDomainConfusion2014, wangVisualDomainAdaptation2018, sunDeepCoralCorrelation2016, pengMomentMatchingMultisource2019, liDomainGeneralizationAdversarial2018, motiianUnifiedDeepSupervised2017, ghifaryDomainGeneralizationObject2015, matsuuraDomainGeneralizationUsing2020}, and domain-adversarial training \cite{ganinUnsupervisedDomainAdaptation2015, ganinDomainadversarialTrainingNeural2016, liDomainGeneralizationAdversarial2018, liDeepDomainGeneralization2018, gongDlowDomainFlow2019}. Another approach is to disentangle latent features into class-specific and domain-specific representations \cite{khoslaUndoingDamageDataset2012, pengDomainAgnosticLearning2019, ilseDivaDomainInvariant2020, namReducingDomainGap2021, zhangPrincipledDisentanglementDomain2022}. General machine learning paradigms were also applied to vanilla DG, such as meta-learning \cite{liLearningGeneralizeMetalearning2018, balajiMetaregDomainGeneralization2018, douDomainGeneralizationModelagnostic2019, liEpisodicTrainingDomain2019}, self-supervised learning \cite{carlucciDomainGeneralizationSolving2019, kimSelfregSelfsupervisedContrastive2021}, gradient manipulation \cite{huangSelfchallengingImprovesCrossdomain2020, shiGradientMatchingDomain2021, rameFishrInvariantGradient2022}, and distributionally robust optimization \cite{sagawaDistributionallyRobustNeural2020, kruegerOutofdistributionGeneralizationRisk2021}.

\item[Compound DG] There are some DG algorithms that do not require domain labels by design \cite{huangSelfchallengingImprovesCrossdomain2020, matsuuraDomainGeneralizationUsing2020, liSimpleFeatureAugmentation2021, namReducingDomainGap2021, zhangPrincipledDisentanglementDomain2022, chenCompoundDomainGeneralization2022}. In addition to improving DG performance, latent domain discovery is also an important task in compound DG and contributes to better interpretability. \cite{matsuuraDomainGeneralizationUsing2020, chenCompoundDomainGeneralization2022} can do this but have two main limitations: (1) Their proposed methods proceed in two phases: first discover potential domains, and then deal with DG with the inferred domains, which is similar to vanilla DG. The problem is that the second phase depends on the first and cannot provide some feedback to correct possible errors in domain discovery. (2) Their methods assume that domain discrepancy arises from stylistic differences in order to identify latent domains, which does not always hold.

In our work, all components are jointly optimized in an end-to-end fashion. In addition, we leverage MoE to find latent domains without an explicit induced bias on the cause of domain discrepancy.
\end{description}

\subsection{Hypernetworks}
\label{sec:related_hypernet}

A hypernetwork is a neural network that generates the weights of another target network. Hypernetworks were initially proposed by \cite{haHypernetworks2016} and then applied to optimization problems \cite{lorraineStochasticHyperparameterOptimization2018, navonLearningParetoFront2020}, meta-learning \cite{zhaoMetalearningHypernetworks2020}, continuous learning \cite{vonoswaldContinualLearningHypernetworks2019, brahmaHypernetworksContinualSemiSupervised2021}, multi-task learning \cite{linControllableParetoMultitask2020, tayHypergridTransformersSingle2021, mahabadiParameterefficientMultitaskFinetuning2021}, few-shot learning \cite{senderaHypershotFewshotLearning2022}, and federated learning \cite{shamsianPersonalizedFederatedLearning2021}.

\subsection{Mixture of Experts (MoE)}
\label{sec:related_moe}

\begin{figure}
	\centering
	\begin{subfigure}{0.55\linewidth}
		\includegraphics[width=\textwidth]{figures/MOE}
		\caption{Classical Mixture of Experts}
		\label{fig:moe}
	\end{subfigure}
	\begin{subfigure}{0.4\linewidth}
		\includegraphics[width=\textwidth]{figures/Gate_Value_Matrix}
		\caption{Gate value matrix}
		\label{fig:gate_values}
	\end{subfigure}
	\caption{(a) Mixture of Experts calculates the weighted sum of experts' outputs. (b) The aggregation weights, also known as gate values, are calculated by the gate network on a per-example basis.}
	\label{fig:moe_and_gate}
\end{figure}

Mixture of Experts (MoE), originally proposed by \cite{jacobsAdaptiveMixturesLocal1991, jordanHierarchicalMixturesExperts1994}, consists of two main components: experts and a gate network, as shown in \cref{fig:moe}. Its output is a weighted sum of experts, and the gate network calculates gate values on a per-example basis, as shown in \cref{fig:gate_values}. In the past few years, MoE has regained attention as a way to scale up deep learning models without significantly increasing computational cost and to more effectively harness modern hardware \cite{shazeerOutrageouslyLargeNeural2017, lepikhinGshardScalingGiant2020, fedusSwitchTransformersScaling2021, duGlamEfficientScaling2022, zophDesigningEffectiveSparse2022, fedusReviewSparseExpert2022}. In this case, sparse MoE is used, which routes each example only to the experts with Top-1 or Top-K gate values, instead of all of them.

\subsection{Application of Hypernetworks and MoE in DG}

To the best of our knowledge, no work has used hypernetworks to solve DG in the field of computer vision. Recently, \cite{volkExamplebasedHypernetworksOutofdistribution2022} applied hypernetworks to DG in natural language processing (NLP) and achieved state-of-the-art results on two NLP-related DG tasks. As for MoE, \cite{liSparseFusionMixtureofExperts2022} proposed replacing feed-forward network layer (FFN) of Vision Transformer (ViT) \cite{dosovitskiyImageWorth16x162020} with a sparse mixture of FFN experts to improve DG performance. In addition, if we regard MoE as a kind of ensemble method, there are some work having the same spirit \cite{manciniBestSourcesForward2018, dinnocenteDomainGeneralizationDomainspecific2018, zhouDomainAdaptiveEnsemble2021}.

\section{Method}
\label{sec:method}

\subsection{Problem Setting}
Let $\mathcal{X}$ denote an input space and $\mathcal{Y}$ a target space. A domain $S$ is characterized by a joint distribution $P^s_{XY}$ on $\mathcal{X} \times \mathcal{Y}$. In vanilla DG setting, we have a training set containing $M$ known domains, \ie, $\mathcal{D}_{tr}^V = \{ \mathcal{D}^s \}_{s=1}^{M}$ with $\mathcal{D}^s = \{(x^s_i, y^s_i, d^s_i)\}_{i=1}^{N_s}$ where $(x^s_i, y^s_i) \sim P^s_{XY}$ and $d^s_i$ is the domain index or label. Also consider a test dataset $\mathcal{D}_{te}$ composed of unknown domains different from those of $\mathcal{D}_{tr}^V$. Vanilla DG aims to train a robust predictor $f: \mathcal{X} \to \mathcal{Y}$ on $\mathcal{D}_{tr}^V$ with domain labels to achieve a minimum predictive error on $\mathcal{D}_{te}$, \ie, $\min_f \mathbb{E}_{(x, y) \sim \mathcal{D}_{te}} [ \ell(f(x), y) ]$, where $\ell(\cdot, \cdot)$ is the loss function.

Our work focuses on the more difficult compound DG, for which the training set $\mathcal{D}_{tr} = \{ (x_i, y_i) \}_{i=1}^{N}$ contains mixed domains and therefore has no domain annotation. However, as demonstrated in \cite{gulrajaniSearchLostDomain2020, zhouDomainGeneralizationSurvey2022, wangGeneralizingUnseenDomains2022}, intrinsic inter-domain relationships play a key role in obtaining better generalization performance. Therefore, our proposed HMOE is required to identify and discover latent domains by dividing $\mathcal{D}_{tr}$ into clusters that match human intuition about visual relationships between different domains.

\subsection{Overall Architecture}
\label{sec:arch}

\begin{figure*}[htbp]
	\centering
	\begin{subfigure}{0.7\linewidth}
		\includegraphics[width=\textwidth]{figures/HMOE_Arch_Center_E}
		\caption{Architecture}
		\label{fig:arch}
	\end{subfigure}
	\hspace{0.4cm}
	\begin{subfigure}{0.18\linewidth}
		\includegraphics[width=\textwidth]{figures/Gate_Function}
		\caption{Gate function}
		\label{fig:gate_function}
	\end{subfigure}
	\caption{(a) An overview of HMOE. In the upper branch, the input is transformed into the embedding space through the D2V encoder and gate values are calculated by a predefined gate function. In the lower branch, the hypernetwork takes as input embedding vectors to create a set of classifiers. The output is the weighted sum of classifiers' predictions. (b) The gate function calculates gate values based on the distances between the output of the D2V encoder and the embedding vectors. The smaller the distance, the greater the gate value.}
	\label{fig:hmoe}
\end{figure*}

An overview of the HMOE architecture is depicted in \cref{fig:arch}. It processes each input $x$ through two paths: the domain path, which aims at performing latent domain discovery, and the classifier path, which aims at training a classifier expert for each latent domain. The classifier path starts with a featurizer $h_z$ to extract high-level features from $x$, which can be a pretrained network, such as VGG \cite{simonyanVeryDeepConvolutional2014}, ResNet \cite{heDeepResidualLearning2016}, and ViT. We define a discrete learnable embedding space $\mathcal{E}$ with $K$ embedding vectors $\{ e_k \in \mathbb{R}^D \}_{k=1}^K$, which are fed into a hypernetwork $f_h$ to generate a set of weights $\{ \theta_k \}_{k=1}^K$. These weights further form a set of classifiers $\{ f_c(:; \theta_k) \}_{k=1}^K$. The output of the featurizer is passed to these $K$ classifier experts to compute their corresponding outputs $y_k = f_c(z; \theta_k)$.
% The concept of D2V was also employed by \cite{deshmukhDomain2vecDeepDomain2018, pengDomain2vecDomainEmbedding2020}.

The domain path starts with a Domain2Vec (D2V) encoder $h_v$, which transforms $x$ into the embedding space $\mathcal{E}$ and outputs $v \in \mathbb{R}^D$. The output $v$ is then compared with the embedding vectors through a predefined gate function $g(v, \mathcal{E})$, as shown in \cref{fig:gate_function}, to produce a set of probabilities $\bm{p} = \{ p_k \}_{k=1}^K$. The final output $y$ is the weighted sum of experts' outputs:
\begin{equation}
    y = \sum_{k=1}^K p_k y_k = \langle g(h_v(x), \mathcal{E}), \left[ f_c(h_z(x); f_h(e_k)) \right]_{k=1}^K \rangle \label{eq:output}
\end{equation}

In classical MoE, the gate network and experts have the same input. On the contrary, in our work, the D2V encoder takes images as input rather than the featurizer's extracted features, which mainly contain class-specific information for classification. If we link the D2V encoder to the featurizer, HMOE risks separating the input space based on semantic categories rather than domain-wise distinction.

\subsection{Hypernetworks}
\label{sec:hypernet}

We use the hypernetwork $f_h$ taking as input a vector $e$ to generate the weights of the classifier $f_c$. In our work, both $f_h$ and $f_c$ are MLPs. In a sense, $f_c$ is just a placeholder computational graph, $e$ can be viewed as a conditioning signal, and $f_h$ maps $e$ into a function. In contrast to classical MoE with a number of experts, we can use $f_h$ to generate many experts without significantly increasing model parameters. In classical MoE, there is no direct communication between experts. In our work, experts are able to share some meta-knowledge through $f_h$. In addition, we use the hyperfan method proposed by \cite{changPrincipledWeightInitialization2019} to initialize $f_h$.

\subsection{Routing Mechanism}
\label{sec:routing}
 
\subsubsection{Gate Function}
\label{sec:gate_func}

We need to calculate gate values $\bm{p}$ to quantify the responsibilities of experts for each input example and to aggregate experts' outputs. Based on the output of the D2V encoder $v$ and the embedding space $\mathcal{E}$, we define a gate function $g(v, \mathcal{E})$ to calculate $\bm{p}$ as follows (\cref{fig:gate_function}):
\begin{subequations}
    \begin{gather}
        d_k = \lVert v - e_k \rVert _2    \label{eq:dist_ve}   \\
        s_k = -\log (d_k^2 + \epsilon)    \label{eq:neg_log}   \\
        p_k = \frac{\exp (s_k)}{\sum_{j=1}^K \exp (s_j)}
    \end{gather}
\end{subequations}
where $\epsilon$ is a small value. The negative logarithm in \cref{eq:neg_log} is used to establish a negative correlation between $d_k$ and $p_k$ (\ie, the smaller $d_k$, the larger $p_k$) and to nonlinearly rescale the distance $d$ (\ie, stretch small $d$ and squeeze great $d$), which makes $\bm{p}$ less sensitive to large $d$.

\subsubsection{Differentiable Dense-to-Sparse Top-1 Routing}
\label{sec:diff_top1}
Based on gate values $\bm{p}$, the routing algorithm determines where and how to route input examples. A consistent and cohesive routing is essential for the training stability and convergence of MoE \cite{daiStableMoEStableRouting2022}. To enhance and stabilize latent domain discovery to capture less obvious domain differences, we would like to realize sparse MoE. However, the commonly used Top-1 or Top-K functions are not differentiable and cause oscillatory behavior of gate values during training \cite{hazimehDselectkDifferentiableSelection2021}. Therefore, we propose a differentiable dense-to-sparse Top-1 routing algorithm by introducing an entropy loss on $\bm{p}$:
\begin{equation}
    \mathcal{L}_{en} = \mathbb{E}_{(x, y) \sim \mathcal{D}_{tr}} \left[ \mathbb{H}(g(h_v(x), \mathcal{E})) \right]
\end{equation}
where $\mathbb{H(\cdot)}$ denotes the entropy of a distribution. In practice, we multiply $\mathcal{L}_{en}$ by $\gamma_{en}$ that linearly increases from 0 to 1 in the first half of training and remains at 1 in the second. Early on, $\gamma_{en}$ is small, and the distances between $v$ and the embedding vectors are almost the same, leading to a uniform $\bm{p}$. Therefore, all experts can be fully trained and gradually become specialized. In the later stages, $\mathcal{L}_{en}$ forces $\bm{p}$ to become one-hot based on specialized experts.

Due to the negative logarithm in \cref{eq:neg_log}, the D2V encoder has to move towards one of the embedding vectors rather than away from the others in order to minimize $\mathcal{L}_{en}$. Therefore, the output of the D2V encoder will converge to $\mathcal{E}$ and become quantized during training.

\subsubsection{Expert Load Balancing}
\label{sec:load_balance}

Sparse MoE may suffer from an unbalanced expert load, which is problematic if only a small subset of experts are used while the others are left idle. To alleviate this problem, a widely used approach is to introduce an auxiliary importance loss $CV(I(X))^2$ \cite{shazeerOutrageouslyLargeNeural2017}, where $X$ represents a single batch, $I(X) = [I_1(X), \cdots, I_K(X)]$ denotes the importance of experts, for which $I_k(X)$ is defined as the sum of gate values assigned to the $k$th expert (\ie, sum the gate value matrix in \cref{fig:gate_values} along the example dimension), and $CV$ is the coefficient of variation. However, \cite{pavlitskayaBalancingExpertUtilization2022} showed that this importance loss over-penalizes unbalanced expert utilization and may be counter-productive, since in most cases the expert load is naturally unbalanced. In this case, \cite{pavlitskayaBalancingExpertUtilization2022} defined a distribution $P = I(X) / \sum I(X)$ and used the KL-divergence between $P$ and the uniform distribution $\mathcal{U}$ to balance the expert load, which is also used in our work:
\begin{equation}
    \mathcal{L}_{kl} = D_{KL}(P \Vert \mathcal{U}) = D_{KL} \left( \frac{I(X)}{\sum I(X)} \Vert \mathcal{U} \right)
\end{equation}
Compared to the importance loss, $\mathcal{L}_{kl}$ achieves a better trade-off between expert specialization and load balancing.

\subsection{Embedding Space}
\label{sec:emb_space}
The embedding space $\mathcal{E}$ plays a key role in HMOE. As we can see, embedding vectors have an effect on both the generation of expert weights and the routing mechanism, thus serving as a bridge to balance these two parts. In addition, these embedding vectors are learnable like the weights and biases of neural networks and attract the D2V encoder during training under the influence of $\mathcal{L}_{en}$. This may be reminiscent of VQ-VAE \cite{vandenoordNeuralDiscreteRepresentation2017}, which also has an embedding space and makes its encoder output discrete latent codes.

\subsection{Class-Adversarial Training on D2V}
\label{sec:class_adv}

We would like to make the D2V encoder $h_v$ less informative for classes, which ensures that HMOE partitions the input space based on domain-wise distinction rather than semantic categories. Inspired by Domain-Adversarial Neural Networks \cite{ganinDomainadversarialTrainingNeural2016}, we define an adversarial classifier $f_c^{ad}$ taking $v$ as input and add the following loss to perform class-adversarial training on $h_v$:
\begin{equation}
    \mathcal{L}_{ad} = \mathbb{E}_{(x, y) \sim \mathcal{D}_{tr}} \left[ \ell_{ce}(f_c^{ad}(GRL(h_v(x), \lambda_{grl})), y) \right]
\end{equation}
where $\ell_{ce}$ denotes the cross-entropy loss and $GRL$ represents the gradient reversal layer, which acts as an identity function in the forward pass and multiplies the gradient by $-\lambda_{grl}$ in the backward pass. As suggested in \cite{ganinDomainadversarialTrainingNeural2016}, we define $\lambda_{grl}$ as:
\begin{equation}
	 \lambda_{grl} = 2 / (1 + \exp (-10 \times pct_{tr})) - 1
\end{equation}
where $pct_{tr}$ denotes the training percentage varying linearly from 0 to 1.

\subsection{Semi-/supervised Learning on Domains}
\label{sec:semi_domains}

Due to the probabilistic nature of MoE, given an input $x$ and the corresponding gate values $\bm{p} = \{ p_k \}_{k=1}^K$, we can interpret $p_k$ as the probability of selecting the $k$th expert $E_k$ given $x$, \ie, $p(E_k | x)$.  In addition, $E_k$ is thought to be associated with a specific domain $\mathcal{S}_m$. Therefore, we get $p_k = p(E_k | x) = p(S_m | x)$. Consider a dataset with domain labels $\mathcal{D}_d = \{ (x_i, d_i) \}_{i=1}^{N_d}$ (class labels are not necessary) with $d_i \in \{ 1, \cdots, M_d \}$, we can make use of $\mathcal{D}_d$ as follows:
\begin{equation}
    \mathcal{L}_{d} = \mathbb{E}_{(x, d) \sim \mathcal{D}_d} \left[ \ell_{ce}(\bm{p}, d) \right]
\end{equation}
Note that $M_d$ may be smaller than $K$, but this has no bearing on the calculation of $\mathcal{L}_{d}$. In this case, we assume that the first $M_d$ experts are assigned to $M_d$ domains, and the other experts do not have domain information and learn from the data by themselves.  If all domain labels are available in the training data, $\mathcal{L}_{d}$ becomes supervised learning on domains.

\subsection{Training and Inference}
\label{sec:train_infer}

In addition to the above losses, the supervised loss on targets is:
\begin{equation}
    \mathcal{L}_{y} = \mathbb{E}_{(x, y) \sim \mathcal{D}_{tr}} \left[ \ell_{ce}(\hat{y}, y) \right]
\end{equation}
where $\hat{y}$ is the prediction of HMOE, as calculated in \cref{eq:output}. The final training loss is:
\begin{align}
    \mathcal{L} &= \lambda_{y} \mathcal{L}_{y} + \lambda_{en} \mathcal{L}_{en} + \lambda_{kl} \mathcal{L}_{kl} + \lambda_{ad} \mathcal{L}_{ad} + \lambda_{d} \mathcal{L}_{d}
\end{align}
where $\lambda$ are trade-off hyper-parameters to balance losses.

For inference, we provide three ways: MIX, MAX, and OOD. MIX means the mixture of experts, MAX uses the output of the expert with the highest gate value, and OOD\footnote{The OOD inference can be efficiently implemented using PyTorch-based JAX-like library, \emph{functorch}.} (Out of Domain) uses the output of a classifier whose weights are generated by the hypernetwork taking the D2V encoder as input.

\section{Experiments}
\label{sec:exp}

\subsection{Toy Regression Problem}
Although this work focuses on image classification, we start with a toy regression problem to gain some insight into the learning dynamics of HMOE, such as how gate values evolve and how experts become specialized gradually.

We use the function $y = \sin (4 \pi x)$ to generate 10, 20, and 30 data points uniformly in three intervals: $[0, 0.5]$, $[1, 1.5]$ and $[2, 2.5]$, respectively. Unequal data points are used to simulate a naturally unbalanced expert load. All networks of HMOE are MLPs, and we create three embedding vectors of dimension $D=8$. We employ $\mathcal{L}_{y}$ (use MSE as the loss function), $\mathcal{L}_{en}$, and $\mathcal{L}_{kl}$ with $\lambda_{y} = \lambda_{en} = \lambda_{kl} = 1$, and train HMOE using Adam \cite{kingmaAdamMethodStochastic2014} with learning rate $0.001$ over $20,000$ epochs. More details are presented in the \textbf{supplementary material}.

The evolution of the experts' outputs and gates values \wrt training epochs is shown in \cref{fig:toy_experts}. We can see that three experts compete with each other and gradually locate their positions, and HMOE manages to identify three intervals even with imbalanced data. After training, we compare three modes of inference, as shown in \cref{fig:toy_inference}. They all coincide well with the training points. MIX seems to generalize best to the zones between intervals, while MAX has discontinuities due to hard switching between experts and OOD has an unexpected spike. Overall, HMOE demonstrates an ability to detect heterogeneous patterns in data.

\begin{figure}[htbp]
	\centering
	\begin{subfigure}{\linewidth}
		\includegraphics[width=\textwidth]{figures/Toy_Experts_Gates}
		\caption{Experts' outputs and gate values during training}
		\label{fig:toy_experts}
	\end{subfigure}
	\begin{subfigure}{0.75\linewidth}
		\includegraphics[width=\textwidth]{figures/Toy_Preds}
		\caption{Three modes of inference}
		\label{fig:toy_inference}
    \end{subfigure}
	\caption{A toy regression problem. We generate some data points using the function $y=\sin (4 \pi x)$ in three intervals and fit HMOE with three embedding vectors to these points. HMOE well identifies three intervals and experts also become specialized.}
	\label{fig:toy_regression}
\end{figure}

\subsection{DomainBed}

\subsubsection{Datasets and Model Evaluation}
DomainBed \cite{gulrajaniSearchLostDomain2020} provides a unified codebase to implement and train DG algorithms and integrates some commonly used DG-related datasets. In this section, we experiment on Colored MNIST with 3 domains \cite{arjovskyInvariantRiskMinimization2019}, Rotated MNIST with 6 domains \cite{ghifaryDomainGeneralizationObject2015},  PACS with 4 domains \cite{liDeeperBroaderArtier2017}, VLCS with 4 domains \cite{fangUnbiasedMetricLearning2013}, OfficeHome with 4 domains \cite{venkateswaraDeepHashingNetwork2017}, and TerraIncognita with 4 domains \cite{beeryRecognitionTerraIncognita2018}. In the \textbf{supplementary material}, we give detailed statistics and visualize some samples for each domain of each dataset.

To select models and tune hyper-parameters, DomainBed gives three options, of which we choose the training-domain validation that randomly draws 80\% from the data of each training domain to form the training set and uses the remaining as the validation set. This option best matches the setting of compound DG without domain labels and access to test domains.

\subsubsection{Implementation Details}

For Colored and Rotated MNIST, following \cite{gulrajaniSearchLostDomain2020}, we use as the featurizer a four-layer ConvNet (refer to Appendix D.1 of \cite{gulrajaniSearchLostDomain2020}). The D2V encoder consists of two \emph{conv} layers (32 units, $3 \times 3$ kernels, ReLU), followed by global average pooling and a fully-connected (fc) layer to map to the embedding dimension $D$.

For other datasets, we use ResNet50\footnote{For a fair comparison with other DG algorithms, we use the pretrained ResNet50 of IMAGENET1K-V1 in PyTorch, although V2 is better.} pretrained on ImageNet \cite{deng2009imagenet} as the featurizer and freeze all batch normalization layers. The D2V encoder cascades 3 \emph{conv} layers (64-128-256 units, stride 2, $4 \times 4$ kernels, ReLU), two residual blocks (each has 2 \emph{conv} layers with 256 units, $3 \times 3$ kernels, ReLU), and a  $3 \times 3$ \emph{conv} layer with $D$ units followed by global average pooling. In addition, we use Instance Normalization \cite{ulyanovInstanceNormalizationMissing2016} with learnable affine parameters before all ReLU of the D2V encoder.

For all datasets, the classifier is a fc layer whose input size is the output size of the featurizer (128 for ConvNet and 2048 for ResNet50) and output size is the number of classes per dataset. The hypernetwork is a five-layer MLP with 256-128-64-32 hidden units and SiLU \cite{hendrycksGaussianErrorLinear2016} except the output layer, and its input size is $D$ and output size is the total number of learnable parameters (\ie, weights and biases) of the classifier.  If $\mathcal{L}_{ad}$ is used, the adversarial classifier is a three-layer MLP with 256 hidden units and ReLU except the output layer, and its input size is $D$ and output size is the number of classes. In addition, we set $D=32$ and initialize embedding vectors using the standard normal distribution.

We define three HMOE variants based on the number of embedding vectors $K$ and whether domain labels are used:
\begin{enumerate*}[label=(\arabic*)]
\item \textbf{HMOE-DL:}  Domain labels of $\mathcal{D}_{tr}$ are provided. In this case, we only use $\mathcal{L}_{y}$ and $\mathcal{L}_{d}$ with $\lambda_{y} = \lambda_{d} = 1$ and discard other losses, and $K$ is the number of training domains per dataset.
\item \textbf{HMOE-DN:} Domain numbers are known but domain labels. In this case, $K$ is the number of training domains per dataset. We use $\mathcal{L}_{y}$, $\mathcal{L}_{en}$, $\mathcal{L}_{kl}$, and $\mathcal{L}_{ad}$ with $\lambda_{y} = \lambda_{en} = \lambda_{kl} = 1$ and $\lambda_{ad} = 0.01$.
\item \textbf{HMOE-ND:} No domain information is available and we use a fixed $K=5$. The setting of losses is the same as in HMOE-DN.
\end{enumerate*}

DomainBed trains all DG algorithms with Adam for 5,000 iterations. For Colored and Rotated MNIST / other datasets, the learning rate is 0.001 / 5e-5, the batch size is 64 / 32 $\times$ number of training domains, and models are evaluated on the validation set every 100 / 300 iterations. Each experiment uses one domain of a dataset as the test domain and trains algorithms on the others, which is repeated 3 times with different random seeds. The average accuracy over 3 replicates is reported. In addition, we do not tune hyper-parameters and use the settings mentioned above consistently. Other DG algorithms also use the default settings predefined in DomainBed. All experiments are performed on PyTorch using a A5000 GPU.

\subsubsection{Results}
We use the up-to-date domain generalization benchmark on DomainBed, and the comparison of our proposed HMOE (3 variants and 3 inference modes) with other DG algorithms is shown in \cref{tab:res_domainbed}, where the best results are underlined. ERM means the vanilla supervised learning that just fine-tunes ResNet50 on mixed domains, also called DeepAll in some papers and serving as a performance baseline. We list the average accuracy of all test domains for each dataset. Refer to the \textbf{supplementary material} for detailed results.

For Colored and Rotated MNIST, the performance of all algorithms is almost the same, except for the impressive results of ARM. Our proposed HMOE achieves SOTA results on PACS and TerraIncognita, which well demonstrates the effectiveness of HMOE. However, ERM outperforms HMOE and most DG algorithms for VLCS and OfficeHome. VLCS contains real camera photos, and its domain shift is mainly caused by changes in scene and perspective. We find that the visual differences between various domains of VLCS are subtle. In this case, forcing to reduce or model domain discrepancy may cause or aggravate overfitting. For OfficeHome, this is also the case.

\setlength{\intextsep}{0pt}
\setlength{\columnsep}{0pt}
\begin{wrapfigure}{R}{0.25\textwidth}
    \centering
        \includegraphics[width=0.25\textwidth]{figures/HMOE_DL_Ld}
    \caption{Avg. $\mathcal{L}_d$ per dataset}
    \label{fig:hmoe_dl_ld}
\end{wrapfigure}

Interestingly, HMOE-DL is inferior to HMOE-DN/ND in most cases, which implies that HMOE works better using its own learned domain information than using given domain labels. We find that latent domains discovered by HMOE are more human-intuitive than original domain labels (See \cref{sec:domain_discovery}).

\begin{table*}[htbp]
        \centering
		\small
		\setlength\tabcolsep{5pt}
        \begin{tabular}{cccccccc}
        \hline
        \textbf{Algorithm}  &  & \textbf{ColoredMNIST}     & \textbf{RotatedMNIST}     & \textbf{VLCS}             & \textbf{PACS}             & \textbf{OfficeHome}       & \textbf{TerraIncognita} \\
        \hline
        \multicolumn{8}{c}{\emph{w/ Domain Labels}} \\
        \hline
        
        IRM      \cite{arjovskyInvariantRiskMinimization2019}            &  & 52.0            & 97.7            & 78.5            & 83.5            & 64.3            & 47.6            \\
        GroupDRO \cite{sagawaDistributionallyRobustNeural2020}           &  & 52.1            & 98.0            & 76.7            & 84.4            & 66.0            & 43.2            \\
        Mixup    \cite{yanImproveUnsupervisedDomain2020}                 &  & 52.1            & 98.0            & 77.4            & 84.6            & 68.1            & 47.9            \\
        MLDG     \cite{liLearningGeneralizeMetalearning2018}             &  & 51.5            & 97.9            & 77.2            & 84.9            & 66.8            & 47.7            \\
        CORAL    \cite{sunDeepCoralCorrelation2016}                      &  & 51.5            & 98.0            & \underline{\textbf{78.8}}            & 86.2            & \underline{\textbf{68.7}}         & 47.6  \\
        MMD      \cite{liDomainGeneralizationAdversarial2018}            &  & 51.5            & 97.9            & 77.5            & 84.6            & 66.3            & 42.2            \\
        DANN     \cite{ganinDomainadversarialTrainingNeural2016}         &  & 51.5            & 97.8            & 78.6            & 83.6            & 65.9            & 46.7            \\
        CDANN    \cite{liDeepDomainGeneralization2018}                   &  & 51.7            & 97.9            & 77.5            & 82.6            & 65.8            & 45.8            \\
        MTL      \cite{blanchardDomainGeneralizationMarginal2021}        &  & 51.4            & 97.9            & 77.2            & 84.6            & 66.4            & 45.6            \\
        ARM      \cite{zhangAdaptiveRiskMinimization2021}                &  & \underline{\textbf{56.2}}         & \underline{\textbf{98.2}}            & 77.6            & 85.1            & 64.8            & 45.5  \\
        VREx     \cite{kruegerOutofdistributionGeneralizationRisk2021}   &  & 51.8            & 97.9            & 78.3            & 84.9            & 66.4            & 46.4            \\
        \hline
        \multirow{3}{5em}{HMOE-DL}                                   &  MIX & 51.6            & 97.3            & 76.7            & 83.5            & 64.7            & 45.0            \\
                                                                     &  MAX & 51.7            & 97.0            & 77.6            & 83.9            & 63.2            & 43.2            \\
                                                                     &  OOD & 51.7            & 97.4            & 76.8            & 84.5            & 63.7            & 44.0            \\
        \hline
        \multicolumn{8}{c}{\emph{w/o Domain Labels}}                                                                                                                                    \\
        \hline
        ERM      \cite{vapnikNatureStatisticalLearning1999}              &  & 51.5            & 98.0            & 77.5            & 85.5            & 66.5            & 46.1            \\
        RSC      \cite{huangSelfchallengingImprovesCrossdomain2020}      &  & 51.7            & 97.6            & 77.1            & 85.2            & 65.5            & 46.6            \\
        SagNet   \cite{namReducingDomainGap2021}                         &  & 51.7            & 98.0            & 77.8            & 86.3            & 68.1            & 48.6            \\
        \hline
        \multirow{3}{5em}{HMOE-DN}                                   &  MIX & 51.9            & 97.5            & 76.8            & 84.8            & 65.4            & 48.7            \\
                                                                     &  MAX & 51.9            & 97.4            & 76.6            & 85.1            & 65.4            & \underline{\textbf{49.5}}   \\
                                                                     &  OOD & 51.9            & 97.5            & 75.8            & 84.9            & 65.3            & 48.4            \\
        \hline
        \multirow{3}{5em}{HMOE-ND}                                   &  MIX & 51.6            & 97.5            & 76.6            & 84.5            & 65.5            & 48.4            \\
                                                                     &  MAX & 51.7            & 97.4            & 76.8            & 86.6            & 65.5            & 45.0            \\
                                                                     &  OOD & 51.7            & 97.5            & 76.7            & \underline{\textbf{87.0}}   & 65.6            & 47.1            \\
        \hline
        \end{tabular}
        \caption{Domain generalization results on DomainBed}
        \label{tab:res_domainbed}
\end{table*}

\cref{fig:hmoe_dl_ld} shows the supervised loss on domains of HMOE-DL $\mathcal{L}_d$ \wrt iterations, which fails to decrease quickly for OfficeHome and VLCS. This means that the information of domain labels is not well absorbed and seems to be incompatible with HMOE. In addition, HMOE-DN / ND are basically tied in terms of performance. For the three modes of inference, MAX and OOD achieve competitive or better performance compared to MIX. Therefore, we can safely employ MAX and OOD in practice, which are more computationally efficient without computing all experts like MIX.

\subsubsection{Latent Domain Discovery}
\label{sec:domain_discovery}

We use t-SNE \cite{vandermaatenVisualizingDataUsing2008} to visualize the output of the D2V encoder, as shown in \cref{fig:t-SNE}. We can see that HMOE succeeds in partitioning the mixed data into a number of clusters, each around an embedding vector. The output of the D2V encoder converges to embedding vectors, which is as expected. For PACS (\cref{fig:pacs_cluster}), training domains are well separated. Some cartoon images look quite artistic and are classified as art. In addition, test photo samples are projected into the art cluster, which suggests that the D2V encoder should capture some semantics about latent domains since photo is closest to art. When we increases the number of embedding vectors $K$ to 5, cartoon and sketch clusters are split into two sub-parts, as shown in \cref{fig:pacs_cluster_ND}. For TerraIncognita (\cref{fig:terra_cluster}), the dots of the same color are largely clustered together, and training domains are to some extent separated, although L38 and L43 are partially mixed. The test domain L46 seems to be more similar to L100. For OfficeHome (\cref{fig:office_cluster}), training domains are mixed in each cluster, which indicates a conflict between domain labels and inferred domains, and also explains why $\mathcal{L}_d$ cannot be reduced significantly for OfficeHome in \cref{fig:hmoe_dl_ld}.

To understand more intuitively how HMOE distinguishes between different domains, we visualize some samples to compare domain labels and HMOE clusters, as shown in \cref{fig:cluster_imgs}. HMOE seemingly partitions TerraIncognita based on illumination and OfficeHome based on background complexity, which is more in line with human intuition about different domains than original domain labels.

After the above analysis, we conclude that the success of HMOE, \eg, SOTA on PACS and TerraIncognita, is attributed to its ability to self-learn more reasonable and informative domain knowledge and use it efficiently.

\begin{figure*}
	\centering
	\begin{subfigure}{0.22\linewidth}
		\includegraphics[width=\textwidth]{figures/PACS_Cluster}
		\caption{PACS-DN}
		\label{fig:pacs_cluster}
	\end{subfigure}
    %
    \begin{subfigure}{0.22\linewidth}
		\includegraphics[width=\textwidth]{figures/PACS_Cluster_ND}
		\caption{PACS-ND ($K=5$)}
		\label{fig:pacs_cluster_ND}
	\end{subfigure}
    %
    \begin{subfigure}{0.22\linewidth}
		\includegraphics[width=\textwidth]{figures/Terra_Cluster}
		\caption{TerraIncognita-DN}
		\label{fig:terra_cluster}
	\end{subfigure}
    %
    \begin{subfigure}{0.22\linewidth}
		\includegraphics[width=\textwidth]{figures/Office_Real_Cluster}
		\caption{OfficeHome-DN}
		\label{fig:office_cluster}
	\end{subfigure}
	\caption{The t-SNE visualization of the output of the D2V encoder. The suffixes in captions (DN and ND) represent HMOE-DN / ND, red squares are embedding vectors, black triangles are 20 samples randomly drawn from the test domain, and other dots are training domains.}
	\label{fig:t-SNE}
\end{figure*}

\begin{figure}[htbp]
	\centering
    \begin{subfigure}{0.49\linewidth}
		\includegraphics[width=\textwidth]{figures/Terra_Domains}
		\caption{TerraIncognita-DN}
		\label{fig:terra_domains}
	\end{subfigure}
    %
	\begin{subfigure}{0.49\linewidth}
		\includegraphics[width=\textwidth]{figures/Office_Real_Domains}
		\caption{OfficeHome-DN}
		\label{fig:office_domains}
	\end{subfigure}
    
	\caption{Comparison between domain labels and HMOE clusters}
	\label{fig:cluster_imgs}
\end{figure}

\subsubsection{More Empirical Analysis}
\begin{description}[style=unboxed,leftmargin=0cm]
\item[Ablation study ]
We conduct an ablation study to analyze the contribution of class-adversarial learning. The results are shown in \cref{tab:ablation}, which reports the average accuracy of three inference modes. As we can see, class-adversarial learning helps significantly improve performance in most cases, which validates its use and verifies the importance and necessity of removing class-specific information from the D2V encoder.

\vspace{0.25cm}
\begin{table}[htbp]
        \centering
		\small
		\setlength\tabcolsep{2.5pt}
        \begin{tabular}{cccccc}
        \hline
                    & $\mathcal{L}_{ad}$              & VLCS   & PACS   & OfficeHome   & TerraIncognita \\
        \hline
        \multirow{2}{5em}{HMOE-DN}    &  -            & 76.0   & 84.0   & 64.2         & 47.0           \\
                                      &  \checkmark   & 76.4   & 84.9   & 65.4         & 48.9           \\
        \hline
        \multirow{2}{5em}{HMOE-ND}    &  -            & 76.9   & 84.5   & 64.4         & 47.4           \\
                                      &  \checkmark   & 76.7   & 86.0   & 65.5         & 46.8           \\
        \hline
        \end{tabular}
        \caption{Ablation study on class-adversarial learning}
        \label{tab:ablation}
\end{table}

\item[More embedding vectors ]
We further increase $K$ to 8, and we find that HMOE suffers from the learning collapse problem, \ie, some embedding vectors collapse together and the D2V encoder outputs similar values, as shown in \cref{fig:pacs_8_embs_losses}. When embedding vectors are much more than needed, HMOE encounters difficulties in how to assign the data to different experts and ends up with a large $\mathcal{L}_{kl}$. In this case, increasing $\lambda_{kl}$ may alleviate the learning collapse.

\begin{figure}
  \begin{minipage}[htbp]{0.45\linewidth}
    \centering
    \includegraphics[scale=0.49]{figures/PACS_Cluster_8_Embs}
    \caption{PACS-ND ($K=8$) and learning collapse}
    \label{fig:pacs_8_embs_losses}
  \end{minipage}
  \hspace{0.4cm}
  \begin{minipage}[htbp]{0.45\linewidth}
    \centering
    \includegraphics[scale=0.49]{figures/Office_Only_Target_Loss}
    \caption{OfficeHome for HMOE ($K=3$ and only $\mathcal{L}_y$)}
    \label{fig:only_ly}
  \end{minipage}
\end{figure}

\item[Only supervised loss on targets ] Using only $\mathcal{L}_y$, we train HMOE with $K=3$ on OfficeHome. In the absence of the entropy loss $\mathcal{L}_{en}$ forcing gate values to become one-hot, HMOE performs soft partitioning on the input space. The t-SNE visualization is shown in \cref{fig:only_ly}, which is obviously not comparable to \cref{fig:office_cluster}, for which clusters are distinctly separated. This demonstrates that the dense-to-sparse Top-1 routing algorithm works as expected and largely improves latent domain discovery compared to soft partitioning.

\end{description}

\section{Conclusion}
\label{sec:conclusion}
This paper presents a novel method, HMOE, for compound DG without the need for domain labels. Compared to other methods requiring domain labels, HMOE shows competitive performance and achieves SOTA results on PACS and TerraIncognita datasets. In addition, HMOE exhibits the distinctive property of latent domain discovery. It is worth mentioning that the discovery and utilization of domain information are jointly undertaken rather than in stages like other related work. The key to our work is to use Mixture of Experts (MoE) and leverage its \emph{divide and conquer} ability. In addition, we leverage hypernetworks to generate the weights of expert networks.

However, it remains unclear how to effectively determine an appropriate number of experts or embedding vectors to fully explore domain information while avoiding the learning collapse. A promising but challenging solution that we will explore in future work is to use tree-structured hierarchical MoE to discover hierarchical domain knowledge, where each level contains only a number of experts but the number of multi-level inferred domains grows exponentially. Moreover, our proposed HMOE is versatile and scalable, and it should also be applicable to a wide range of problems beyond the scope of DG that are troubled by heterogeneous patterns.

\clearpage
%%%%%%%%% REFERENCES
{\small
	\bibliographystyle{ieee_fullname}
	\bibliography{references}
}

\clearpage
\input{supplementary}

\end{document}
