\documentclass{article}

\usepackage{arxiv}

\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{booktabs}
\usepackage[pagebackref,breaklinks,colorlinks]{hyperref}
\usepackage[capitalize]{cleveref}

\usepackage{xspace}
\makeatletter
\DeclareRobustCommand\onedot{\futurelet\@let@token\@onedot}
\def\@onedot{\ifx\@let@token.\else.\null\fi\xspace}

\def\eg{\emph{e.g}\onedot} \def\Eg{\emph{E.g}\onedot}
\def\ie{\emph{i.e}\onedot} \def\Ie{\emph{I.e}\onedot}
\def\cf{\emph{cf}\onedot} \def\Cf{\emph{Cf}\onedot}
\def\etc{\emph{etc}\onedot} \def\vs{\emph{vs}\onedot}
\def\wrt{w.r.t\onedot} \def\dof{d.o.f\onedot}
\def\iid{i.i.d\onedot} \def\wolog{w.l.o.g\onedot}
\def\etal{\emph{et al}\onedot}
\makeatother

\begin{document}

\title{Will Large-scale Generative Models Corrupt Future Datasets?}

\author{Ryuichiro Hataya\\
RIKEN ADSP\\
\And
Han Bao\\
Kyoto University\\
\And
Hiromi Arai\\
RIKEN AIP
}
\date{}
\maketitle

\begin{abstract}
Recently proposed large-scale text-to-image generative models such as DALL$\cdot$E 2 \cite{dalle2}, Midjourney \cite{midjourney}, and StableDiffusion \cite{stablediffusion} can generate high-quality and realistic images from users' prompts. 
Not limited to the research community, ordinary Internet users enjoy these generative models, and consequently a tremendous amount of generated images have been shared on the Internet.
Meanwhile, today's success of deep learning in the computer vision field owes a lot to images collected from the Internet.
These trends lead us to a research question: ``\textbf{will such generated images impact the quality of future datasets and the performance of computer vision models positively or negatively?}''
This paper empirically answers this question by simulating contamination. Namely, we generate ImageNet-scale and COCO-scale datasets using a state-of-the-art generative model and evaluate models trained on ``contaminated'' datasets on various tasks including image classification and image generation.
Throughout experiments, we conclude that generated images negatively affect downstream performance, while the significance depends on tasks and the amount of generated images.
The generated datasets are available via \url{https://github.com/moskomule/dataset-contamination}.
\end{abstract}


\section{Introduction}\label{sec:introduction}

\begin{figure}[t]
	\centering
	\includegraphics[width=0.3\linewidth]{schematic.pdf}
	\caption{Schematic view of the problem. Some large-scale generative models are public, and many users are playing with them to share generated images on the Internet (top). Dataset collection heavily relies on images on the Internet, which may be contaminated by generated images (bottom). This paper discusses the effect of such dataset corruption.}
	\label{fig:schematic}
\end{figure}

Deep generative models for image generation have advanced progressively since the original GANs \cite{Goodfellow2014b} and VAEs \cite{kingma2014vae}.
Recently, denoising diffusion models \cite{ho2020denoising,sohl2015deep,song2021scorebased} have beaten GANs in image quality \cite{dhariwal2021,ho2022cascaded} and are becoming the de-facto standard generative models.
Among them, some large models trained on billion-scale captioned images collected from the Internet achieved high-fidelity image generation conditioned by users' prompts \cite{dalle2,saharia2022photorealistic,stablediffusion,midjourney,nichol22a,zhang2021ernie,yu2022scaling,gafni2022make,balaji2022ediffi}.
Particularly, DALL$\cdot$E 2 \cite{dalle2}, Midjourney \cite{midjourney}, and StableDiffusion \cite{stablediffusion} have web and smartphone applications, and many Internet users enjoy image generation, and consequently a tremendous amount of generated images have been uploaded to the Internet.\footnote{According to OpenAI's blog post, the DALL$\cdot$E 2 model alone generated two million images per day in September 2022 \url{https://openai.com/blog/dall-e-now-available-without-waitlist/}.}

At the same time, highly realistic generated images may have potentially significant impacts on society.
For example, face images generated by StyleGANs \cite{karras2019style} were reportedly used to create fake profiles of SNSs or dating apps to deceive other users \cite{harwell2020dating,hill2020designed}.
Furthermore, recent text-to-image generative models can generate images that look real at first glance from users' instruction and are able to support fake news \cite{tiku2022}.
They also amplify demographic stereotypes \cite{federico2022}.

Another concern is that generated images might affect the quality of newly curated image datasets from the Internet in the future, similar to the fact that the outputs of machine translation models degenerate the quality of corpora \cite{simard2014clean,rarrick2011mt,dodge2021}.
Without a doubt, today's success of deep learning and computer vision, including generative models themselves, largely owes to image datasets collected from the Internet, such as ImageNet \cite{Russakovsky2015,deng2009imagenet}.
However, when generated images are shared on the Internet, they may contaminate the sources of image datasets.\footnote{Although the ``official'' web applications implant watermarks to generated images, which thus can be filtered, we found that some software have options to disable such functions.}
Based on these backgrounds, our research question in this paper raises: \emph{what will happen if datasets are contaminated by generated images?}

We aim to answer this question through experiments: simulating such contamination by large-scale datasets of generated images and measuring downstream performance trained on them.
Specifically, we generate nearly two million images from ImageNet categories and COCO captions using StableDiffusion and emulate the contamination by replacing real images in datasets with generated ones. 
Then, we measure the performance of models trained on such contaminated datasets in various tasks, namely, image classification, image captioning, self-supervised learning, and image generation.
Throughout experiments, we find that generated images have \emph{negative} effects on downstream performance, while in some cases to some extent, the effects may be endurable.
We hypothesize that such negative effects are caused by the fact that the generative models capture fewer modes than the actual data, although the existing synthetic experiments have shown high coverage \cite{xiao2022DDGAN}.

In summary, our contributions are as follows:

\begin{itemize}
    \item To simulate the effects of possible contamination, we create large-scale datasets consisting of generated images corresponding to ImageNet and COCO caption (\cref{sec:method}).
    \item We conduct experiments over four distinct tasks on the generated datasets and discover negative effects of contamination, which can be partially attributed to fewer modes of generated images than real data (\cref{sec:experiments,sec:analysis}).
    \item Based on the empirical results, we recommend researchers how to publish generative models and how to collect datasets (\cref{sec:conclusion}).
\end{itemize}

\section{Background and Related Work}

A deep generative model aims to approximate the underlying data distribution by neural networks,  and sampled data are expected to generate data similar to real ones.
Since the emergence of GANs \cite{Goodfellow2014b,radford2015unsupervised}, the research of deep generative models has advanced progressively.
In particular, denoising diffusion models, equivalently, score-based generative models, have achieved high-quality image generation with diversity \cite{ho2020denoising,sohl2015deep,song2021scorebased}, capturing modes of data distributions faithfully \cite{xiao2022DDGAN}.
Text-to-image generative models based on diffusion models can generate high-quality images from users' text instructions with high fidelity, even for unseen novel combinations of concepts, such as ``a photo of an astronaut riding a horse'' \cite{dalle2,saharia2022photorealistic,stablediffusion,midjourney,nichol22a,zhang2021ernie,yu2022scaling,gafni2022make,balaji2022ediffi}. 
Notably, some models have publicly accessible applications \cite{stablediffusion,midjourney,dalle2,zhang2021ernie}, and many Internet users are generating images and posting them to the Internet with related texts.
Such generated images are sometimes difficult to be distinguished from real ones, and thus some of them are potent to contaminate future datasets collected from the web.

The NLP community has experienced similar problems in the last decade; thanks to the development of NLP technologies, many contents on the Internet have become machine-generated, \eg, by machine translation and optical character recognition systems, but such generated texts have degenerated quality of corpora \cite{simard2014clean,dodge2021}.
As a result, filtering such low-quality samples is essential to keep downstream performance \cite{rarrick2011mt}.
Although such issues by generated data have been investigated by the NLP community, the effects of generated images by text-to-image models on various downstream performance in computer vision have rarely been studied.

The dataset contamination issue in general has been studied from various aspects; including dataset poisoning \cite{chen2017targeted}, adversarial training \cite{goodfellow2014explaining}, label noise \cite{han2020survey}, outlier robustness \cite{huber2011robust}, and distribution shifts \cite{sinha2018certifiable}. The existing studies usually posit an attacker/contamination model that is plausible yet mathematically convenient, such as Huberâ€™s contamination model \cite{huber2011robust}.
By contrast, we are rather interested in realistic contamination of the web images induced by generative models and its potential effects.

\section{Dataset Corruption by Generated Images}\label{sec:method}

The goal of this paper is to answer our research question ``\emph{will the contamination of generated images perform positively or negatively?}''
To empirically answer this question, we simulate realistic dataset contamination by generated images and evaluate the quality of contaminated datasets by training commonly-used models on such datasets in several tasks. In this section, we describe the dataset creation.

\subsection{Dataset Creation}

To simulate image generation by users, we create datasets using a StableDiffusion model \cite{stablediffusion}, a state-of-the-art text-to-image generative model, pre-trained on LAION 2B \cite{schuhmann2022laionb}.
These datasets are generated from category names of the ImageNet ILSVRC-2012 classification dataset and captions of the COCO caption dataset, which are referred to as SD-ImageNet and SD-COCO in the remaining text.
For generation of both datasets, we disabled the watermarking functionality to trace outputs as generated images and the safety checker to reduce explicit outputs.

\subsubsection*{SD-ImageNet}

The ImageNet ILSVRC-2012 classification dataset \cite{Russakovsky2015} is a subset of ImageNet \cite{deng2009imagenet} and a dataset for the image classification task. Its training set contains 1.2 million photo images over 1,000 categories selected from synsets of WordNet, \eg, ``African elephant'' (n02504458). 
Using these category names, we prepared prompts like ``A photo of African elephant'' for each category and generated 1,400 photography-like images per class.

\Cref{fig:datasets} (left) shows examples from SD-ImageNet. Images are natural at first glance, but contain some flaws. For example, the elephant at the top left has two noses.

\subsubsection*{SD-COCO}

The COCO caption dataset \cite{chen2015microsoft} is a dataset for the image captioning task. 
Following the dataset split in \cite{karpathy2015deep}, this dataset has 113,000 images with five captions for each image, such as ``A small child wearing headphones plays on the computer''. These captions were used as prompts to generate 565,000 images.

\Cref{fig:datasets} (right) presents some examples from SD-COCO with their captions. Similar to examples of SD-ImageNet, the images are apparently faithful to the captions used as prompts, but staring at them reveals unnatural or unfaithful details. 
For example, the bottom right example fails to produce a ``blue and white plate.''

In the remaining text, we call the ILSVRC-2012 dataset as ImageNet and the COCO caption dataset as COCO for simplicity.

\begin{figure*}[t]
	\centering
	\includegraphics[width=\linewidth]{datasets_wide.pdf}
	\caption{Randomly selected examples from generated datasets, namely, SD-ImageNet (left) and SD-COCO (right). Images are at single glance high quality and fidelity to prompts, \ie, category names and captions, while details are unnatural, \eg, a two-nose elephant.}
	\label{fig:datasets}
\end{figure*}


\subsection{Simulation of Corruption}

To simulate possible corruption, we randomly substitute generated images for 20, 40, and 80 \% of real images of the original datasets with generated ones without replacement.
We refer to these mixed datasets as IN/SD-$n$\%, where $n$ indicates the ratio of generated data.
Similarly to IN/SD-$n$\%, we also created mixtures of COCO and SD-COCO, which are referred to as CO/SD-$n$\%.

In the next section (\cref{sec:experiments}), we empirically investigate the effect of the corruption using the downstream performance of models trained on these contaminated datasets.
However, mixtures of ImageNet and SD-ImageNet, \eg, IN/SD-20\%, alone still entangle the effect of artifacts of generated and the domain shift between generated images and real images.
To estimate the effect of domain shift, we additionally use datasets consisting of real images similar to ImageNet and COCO and compare the downstream performance of models trained on them with that of IN/SDs or IN/COs.
As a counterpart of ImageNet, we adopt a subset of the WebVision dataset \cite{li2017webvision}, which was collected by querying ImageNet category names to Google and Flickr; then, we mix it with ImageNet. 
Because this subset is imbalanced, and some of its categories contain fewer images than needed, we then use sampling by replacement to create balanced mixtures. Similar to IN/SD-$n$\%, we refer to these mixed datasets as IN/WV-$n$\%.
Correspondingly, as a counterpart of COCO, we use Flickr-30k \cite{flick30kb}, which contains 32,000  images collected from Flickr with five captions per image.
Because its size is much less than COCO, we only prepare CO/FL-40\% as a mixture of COCO and Flickr-30k.


\section{Experimental Results}\label{sec:experiments}

In this section, we evaluate the effect of contamination using the datasets created in \cref{sec:method} on several downstream tasks.

\subsection*{Shared Experimental Settings}

We used neural network models implemented with \texttt{PyTorch} v1.12 \cite{pytorch} and its accompanying \texttt{torchvision} on CUDA 11.3. 
Experiments including dataset creation described in \cref{sec:method} were conducted on NVIDIA V-100 GPUs and NVIDIA A-100 GPUs. Further description of settings and configurations can be found in the supplemental material.

\subsection{Image Classification}

This task classifies images into 1,000 categories of ImageNet.
We used ResNet-50 \cite{He2016b}, SwinTransformer-S (Swin-S) \cite{liu2021swin}, and ConvNeXt-T \cite{liu2022} in \texttt{torchvision}, training them according to the standardized training protocols\footnote{\url{https://github.com/pytorch/vision/tree/v0.13.1/references/classification}. Exceptionally, ConvNeXt was trained for 300 epochs.} on ImageNet, SD-ImageNet, WebVision, and their mixtures.
ResNet is a convolutional neural network with residual connections, SwinTransformer is a variant of Vision Transformer, and ConvNeXt is a CNN inspired by Vision Transformers, which represent modern vision models.

\Cref{tab:classification} shows accuracy on the ImageNet validation set.
As can be seen, the performance decreases as the ratio of SD-ImageNet in training data increases.
When the ratio of generated images is at most 40\%, the performance drops are marginal and may be endurable in most practical scenarios.
However, when the ratio is 80\%, the performance degeneration is not negligible.
Compared to SD-ImageNet, WebVision images have less influence on the performance.
In the extreme cases, when no ImageNet data are included in training data, \ie, SD-ImageNet and WebVision, this difference in performance is significant, which suggests that the performance drop may not be solely due to the domain gap.

Additionally, \cref{fig:confusion_matrix} presents confusion matrices of ResNet-50 trained on ImageNet and SD-ImageNet.
For clarity, categories are subsampled and rearranged according to 12 superclasses, adopting \texttt{big\_12} classes from \cite{robustness}.
As can be seen, the mispredictions by ResNet-50 trained on ImageNet mostly falls in the same superclasses, represented by diagonal blocks.
Contrarily, we observe that the model trained on SD-ImageNet uniformly misclassifies certain classes, partially because the category names of such classes are ambiguous, and thus, the generated images for such classes are semantically diverse.
\texttt{Titi} (monkey) is such an example, where it is intended to mean a New World monkey in ImageNet, but it is also a name of people, plants, and places, and thus, generated images are also semantically diverse (see the supplemental material for example images).


\begin{table}[h]
	\centering
	\begin{tabular}{lccc}
	\toprule
	                     & ResNet-50      & Swin-S   & ConvNeXt-T   \\
	\midrule
	ImageNet             & 75.6           & 83.1     & 80.8      \\
	\midrule
	IN/SD-20\%           & 74.5           & 82.1     & 79.7         \\
	IN/SD-40\%           & 72.6           & 81.0     & 78.3     \\
	IN/SD-80\%           & 65.3           & 74.3     & 70.8     \\
	SD-ImageNet          & 15.7           & 19.3     & 19.6      \\
	\midrule
	IN/WV-20\%           & 75.1           & 82.5     & 80.0     \\
	IN/WV-40\%           & 73.9           & 81.8     & 78.8     \\
	IN/WV-80\%           & 68.3           & NaN      & 73.9     \\
	WebVision            & 61.3           & 70.9     & 66.2      \\
	\bottomrule
	\end{tabular}
	\caption{Test accuracy of the image classification task on the ImageNet validation set. We could not stably train Swin-S on IN/WV-80, which resulted in loss explosion. The performance drop is marginal when the ImageNet images dominate the the dataset.}
	\label{tab:classification}
\end{table}


\begin{figure}
    \centering
    \includegraphics[width=0.5\linewidth]{confusion_matrix.pdf}
    \caption{Confusion matrices of ResNet-50 predictions on a subset of ImageNet validation data. Models were trained on ImageNet and SD-ImageNet. Class indices are rearranged. X and Y axes depict superclasses by gathering categories. Colors correspond to the number of data at each pixel in log scale, and block-diagonal components are highlighted.}
    \label{fig:confusion_matrix}
\end{figure}


\subsection{Image Captioning}

Image captioning is a task to generate appropriate captions for given images.
We used a pre-trained BLIP model \cite{li2022blip}, a state-of-the-art vision-language model, and fine-tuned its captioner and filter modules on COCO, SD-COCO, Flickr-30k, and their mixtures for five epochs, following \cite{lavis}.

\Cref{tab:imagecaptioning} reports the performance in various statistics on the COCO test set when the captions are generated with beam search with a beam size of three.
Aligned with the results of image classification, a performance drop by generated images can also be observed.
Especially, CO/SD-0.2 yields comparable or inferior performance to CO/FL-0.4, even in metrics for image captioning like SPICE \cite{spice} and CIDEr \cite{cider}, indicating that generated images cause degeneration of dataset quality.
Moreover, a comparison between the results of SD-COCO and Flickr-30k suggest that such performance drops cannot be fully attributed to domain shift.

\begin{table*}[t]
	\centering
\resizebox{\linewidth}{!}{%
	\begin{tabular}{lcccccccc}
\toprule
			&	BLEU-1	&	BLEU-2	&	BLEU-3	&	BLEU-4 \cite{bleu} &	 SPICE \cite{spice} & METEOR \cite{meteor}	&	ROUGE-L \cite{rouge}	&	CIDEr \cite{cider}	\\
\midrule 
COCO	    &	0.791	&	0.641	&	0.508	&	0.400	&	0.240	&	0.310	&	0.602	&	1.335	\\
\midrule
CO/SD-0.2	&	0.787	&	0.634	&	0.500	&	0.391	&	0.235	&	0.306	&	0.596	&	1.320	\\
CO/SD-0.4	&	0.786	&	0.632	&	0.499	&	0.390	&	0.236	&	0.307	&	0.596	&	1.319	\\
CO/SD-0.8	&	0.780	&	0.623	&	0.486	&	0.377	&	0.233	&	0.300	&	0.588	&	1.279	\\
SD-COCO   	&	0.711	&	0.534	&	0.394	&	0.287	&	0.191	&	0.252	&	0.514	&	1.000	\\
\midrule
CO/FL-0.4   &   0.787   &  0.634    & 0.501      & 0.393     & 0.238      & 0.308    &  0.598   &  1.326  \\
Flickr 30k  &   0.754   &   0.587   &   0.439   &   0.321   &   0.215   &   0.275   &   0.554   &    1.092   \\
\midrule
w/o fine-tuning & 0.473  &   0.392   &   0.308   &   0.237   &   0.158   &   0.212   &   0.488   &   0.838   \\
\bottomrule
	\end{tabular}
}
	\caption{Test metrics in image captioning of the BLIP model on the COCO test split. Higher values are better.}
	\label{tab:imagecaptioning}

\end{table*}

\subsection{Self-supervised Learning}

Self-supervised learning aims to acquire useful representations without using explicit supervision.
We used a masked autoencoder (MAE) \cite{he2022masked}, whose pretext task is to reconstruct missing patches like denoising autoencoder \cite{vincent2010stacked}.
Unlike other popular self-supervised learning methods such as SimCLR \cite{chen2020} and BYOL \cite{grill2020}, MAE requires minimal data augmentation, namely random cropping and random flipping, for pre-training.
This is suitable to purely evaluate the effect of generated images on representations because more intense perturbations may make characteristics of images deviate from the original and generated datasets.

We pre-trained a Vision Transformer model, specifically ViT-B \cite{dosovitskiy2021}, as MAE's encoder for 200 epochs with a mask ratio of 0.75.
\Cref{tab:ssl} presents test accuracy after 90 epochs of linear probing that trains only the last classifier layer from the extracted features.
In this case, each setting yields almost identical performance except for SD-ImageNet, which again supports the negative effects of generated images.

\begin{table}[h]
	\centering
	\begin{tabular}{lc}
	\toprule
	             & Accuracy  \\
	\midrule
	ImageNet     & 43.9           \\
	\midrule
	IN/SD-20\%   & 44.0           \\
	IN/SD-40\%   & 44.1           \\
	IN/SD-80\%   & 42.5           \\
	SD-ImageNet  & 38.8           \\
	\midrule
	IN/WD-20\%   & 43.3           \\
	IN/WD-40\%   & 43.3           \\
	IN/WD-80\%   & 40.9           \\
	WebVision    & 44.1           \\
	\bottomrule
	\end{tabular}
	\caption{Linear probing test accuracy of MAE \cite{he2022masked} on the ImageNet validation set.}
	\label{tab:ssl}
\end{table}

\subsection{Image Generation}

Finally, we evaluate if generated images are useful as training data of the image generation task.
We evaluated an improved denoising diffusion probabilistic model (IDDPM) \cite{nichol21a} on datasets resized to $64\times 64$, which we refer to as, for example, ImageNet-64 and IN/SD-20\%-64. 
We trained the model for $1.8\times 10^6$ iterations using the $L_\text{hybrid}$ objective (see \cite{nichol21a}) with a batch size of 512 and generated $5.0\times 10^4$ images with 250 sampling steps.

\Cref{tab:image_generation} reports the quality of unconditionally generated images in Fr\'eche Inception Distance (FID) \cite{heusel2017gans}, improved precision and recall metrics of 5-nearest neighbors \cite{kynkaanniemi2019improved} between Inception features of generated images and all validation data from ImageNet and WebVision.
The improved precision and recall metrics are computed by estimating the volumes of real and generated images in the embedded space with nearest neighbors \cite{kynkaanniemi2019improved},
from which we can deduce how much two image distributions overlap with each other.
From \cref{tab:image_generation}, we see the trend that the precision and recall increases and decreases, respectively, as the ratio of generated images in training data increases.
To put it differently, the heavier contamination results in generated images that are more likely to be in the support of test images, while the test support coverage is worsened.
This indicates that generated images may concentrate in a smaller subset of the test support.
We suppose that \emph{the contaminated training datasets may cover fewer modes than the original ImageNet}, causing IDDPM to achieve higher precision and lower recall.
The lack of modes can also be observed visually from generated images in the supplemental material.
This hypothesis can explain the performance degeneration in other tasks, where contaminated datasets are biased to some modes, and thus models trained on them cannot generalize better.

For the purpose of reference, generated images are randomly selected and shown in \cref{fig:generated_images},
but perceptual differences among the generated images look marginal.

\begin{table}[h]
	\centering
	\begin{tabular}{lccc}
	\toprule
	                & FID    $\downarrow$     &  Precision@5 $\uparrow$  &  Recall@5 $\uparrow$      \\
	\midrule
	ImageNet-64     & 14.9 / 15.6   & 0.665 / 0.679 & 0.644 / 0.653    \\
	\midrule
	IN/SD-20\%-64   & 12.6 / 12.7   & 0.699 / 0.708 & 0.621 / 0.634    \\
	IN/SD-40\%-64   & 11.0 / 10.8   & 0.730 / 0.739 & 0.585 / 0.608    \\
	IN/SD-80\%-64   & 12.5 / 11.3   & 0.795 / 0.802 & 0.490 / 0.512    \\
	SD-ImageNet-64  & 16.9 / 15.4   & 0.831 / 0.835 & 0.364 / 0.379    \\
	\bottomrule
	\end{tabular}
	\caption{Image quality comparison on unconditional ImageNet-64 validation data and WebVision-64 validation data using Inception-V3 features shown in left and right of each cell, respectively.}
	\label{tab:image_generation}
\end{table}

\begin{figure*}[h]
    \centering
    \includegraphics[width=0.95\linewidth]{generated_imgs.pdf}
    \caption{Randomly selected generated images by class-unconditional IDDPM \cite{nichol21a} using 250 sampling steps.}
    \label{fig:generated_images}
\end{figure*}

\section{Analysis}\label{sec:analysis}

In the main experiments, we demonstrated that generated images cause performance degeneration in downstream tasks.
In this section, we conduct extended experiments to see how fewer modes in generated images affect downstream tasks through performance comparison on evaluation on out-of-distribution data and training on subsampled data.

\subsection{Effects on Robustness}\label{sec:effect_on_representation}

In the main experiments of the classification task, we saw the performance only by accuracy on validation data.
To further investigate the effect of generated images on learned representation, we measured accuracy on other validation data; namely, ImageNet-A \cite{imagenet-a} and ImageNet-R \cite{imagenet-r}.
These datasets share the same categories with ImageNet, but are curated independently to measure robustness to out-of-distribution data.

\Cref{tab:robustness} summarizes the results, which generally indicates that generated images degenerate the robustness, except for IN/SD-20\% on ImageNet-A.
Contrarily, WebVision images consistently enhance the robustness to out-of-distribution data on ImageNet-A and ImageNet-R.
These results further support the hypothesis that generated images have fewer modes than the real data, and thus, cause the downstream performance drop on test data and out-of-distribution data.

\begin{table*}[h]
	\centering
	\begin{tabular}{lccc}
		\toprule
		Source 		& ImageNet Val acc. $\uparrow$   & ImageNet-A \cite{imagenet-a} $\uparrow$   &  ImageNet-R \cite{imagenet-r} $\uparrow$  \\
		\cmidrule(ll){1-1}\cmidrule(rl){2-4}
		ImageNet    & 75.6           & 1.76                                           &  36.7                                \\
		\cmidrule(ll){1-1}\cmidrule(rl){2-4}
		IN/SD-20\%  & 74.5           & 2.12                                           &  35.7                             \\
		IN/SD-40\%  & 72.6           &  1.67                                          &  35.0                              \\
		IN/SD-80\%  & 65.3           &  1.61                                          &  30.0                                \\
		\cmidrule(ll){1-1}\cmidrule(rl){2-4}
		IN/WV-20\%  & 75.1           &  3.55                                          & 40.7                               \\
		IN/WV-40\%  & 73.9           & 3.77                                           & 41.5                               \\
		IN/WV-80\%  & 68.3           & 5.00                                           & 40.0                              \\
		\bottomrule
	\end{tabular}
	\caption{Robustness metrics of ResNet-50 on the ImageNet validation set, ImageNet-A, and ImageNet-R. Different from IN/WVs, IN/SDs generally affect robustness to out-of-distribution data.}
	\label{tab:robustness}
\end{table*}

\subsection{Comparison with Subsampled Data}\label{sec:data_augmentation}

In the main experiments, we compared the performances between networks trained with the contaminated datasets and with the \emph{full-size} clean datasets.
However, one may argue that the performance degradation is resulted from the different amount of real data in the training set.
In this section, we compare the performance of subsampled real datasets with contaminated datasets to disentangle the effect of the amount of clean data.


\Cref{tab:dataaugmentation} shows the test accuracy of ResNet-50 trained on a 5\% subset of ImageNet and IN/SD-95\%, which fills 95\% of missing data by generated data.
Although IN/SD-95\% yields 7.4\% performance improvement over subsampled ImageNet, this is inferior to the gain by IN/WV-95\%.

\begin{table}[h]
	\centering
	\begin{tabular}{ccc}
		\toprule
		ImageNet (5\%)      &    IN/SD-95\%    & IN/WV-95\%   \\
		\midrule
		44.7                &   52.1          & 61.4         \\
		\bottomrule
	\end{tabular}
	\caption{Validation accuracy of ResNet-50 trained on a 5\% subsampled ImageNet, IN/SD-95\%, and IN/WV-95\%.}
	\label{tab:dataaugmentation}
\end{table}


In \cref{tab:dataaugmentation2}, test metrics of BLIP trained on 5\% and 20\% subsets of COCO and their corresponding CO/SDs are presented.
In this case, adding generated data affects negatively, even when only 5\% of real data are available.


\begin{table}[h]
	\centering
	\begin{tabular}{lccc}
	\toprule
				& BLEU-4 $\uparrow$    & SPICE $\uparrow$     & CIDEr $\uparrow$    \\
	\midrule
	COCO (5\%) & 0.386     & 0.233      & 1.290     \\
	CO/SD-95\% & 0.362     & 0.226      & 1.220     \\
	\midrule
	COCO (20\%) & 0.385    & 0.235      & 1.305      \\
	CO/SD-80\%  & 0.377    & 0.233      & 1.279     \\
	\bottomrule
	\end{tabular}
	\caption{Test metrics of BLIP trained on 5\% and 20\% COCO subsets and mixtures to complement the missing data. Higher values are better.}
	\label{tab:dataaugmentation2}
\end{table}

Additionally, \Cref{tab:dataaugmentation3} compares the image quality of generated images by IDDPM trained on the IN/SD-80\% and a 20\% subset of ImageNet.
Aligned with the results in \cref{sec:experiments}, the recall metric diminishes with contaminated data, supporting the hypothesis that the modes of generated images are fewer than real ones.



\begin{table}[h]
    \centering

    \begin{tabular}{lccc}
         \toprule
                        & FID $\downarrow$     &  Precision@5 $\uparrow$  &  Recall@5 $\uparrow$   \\
         \midrule
         ImageNet-64 (20\%)& 16.5    & 0.639           & 0.646        \\
         IN/SD-80\%-64     & 12.5    & 0.795           & 0.490         \\
         \bottomrule
    \end{tabular}
    \caption{Image quality comparison of generated images of IDDPM trained on the IN/SD-80\% and a 20\% ImageNet subset.}
    \label{tab:dataaugmentation3}
\end{table}

These results emphasize the negative effects of generated data.
Additionally, the observations imply that using generated images for data augmentation needs careful consideration.
Such an idea has been studied in image classification using conditional GANs \cite{Tran2017a,Antoniou2018b}, particularly in medical imaging \cite{yi2019}, but also known to hinder the final performance in large-scale settings \cite{Ravuri2019}.
Our results align with the latter that generated images are not always effective in data augmentation.





\section{Discussion}\label{sec:discussion}

We have empirically seen the negative effects of generated images.
In this section, we discuss the detection of generated images and limitations of this research.

\subsection{Detection of Generated Images}\label{sec:detection}


To avoid the negative effects by generated images, one may want to detect generated images easily.
For example, exploiting the differences between real and generated images in high-frequency spectra is a simple and convincing approach \cite{dzanic2020}.
However, this discrepancy may be caused when an upsampling operation (to decode the original images from the low-dimensional latent representations) includes zero pixel insertion; otherwise, detecting generated images only by frequency spectra is difficult \cite{chandrasegaran2021closer}.
Probably because StableDiffusion uses upsampling by nearest neighbor rather than zero pixel insertion, distinguishing real and generated images only from frequency information may be difficult.
\Cref{fig:1dfrequency} presents the power spectra of 1,000 images from ImageNet and SD-ImageNet, which are highly overlapped in all frequencies, which agrees with \cite{chandrasegaran2021closer}.
Additionally, we trained a linear classifier and a multi-layer perceptron on an ImageNet-pre-trained ResNet features to detect generated images.
When trained on $10^4$ images from both datasets, the classifiers achieved around 85\% test accuracy on 2,000 separated test images, which is still unsatisfactory detection rate for a binary classification task and indicates the difficulty of detection of generated images.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.4\linewidth]{frequency.pdf}
    \caption{Average and standard deviation of spectra of $10^3$ images from ImageNet and SD-ImageNet.}
    \label{fig:1dfrequency}
\end{figure}

\subsection{Limitations}

This paper has revealed the potential effects of generated images on datasets through various experiments.
Nevertheless, the discussion has some limitations.
Firstly, we could only use StableDiffusion trained on LAION 2B, because models and their pre-trained weights are publicly available, which is important to generate images without identifiable watermarks.
Different generative models and source datasets may lead to other conclusions, which are left for future work.

Another limitation is types of created datasets and tasks of experiments.
Specifically, we created datasets from category names of ImageNet and captions of COCO and conducted experiments of image classification, image captioning, self-supervised learning, and image generation.
Such a dataset generation scheme may be too simple to approximate possible data generation processes by users' prompts.
In addition, these datasets and tasks may often not be so complex that the insights of this paper would not cover some important aspects of other visual recognition tasks.
For example, the object counting task \cite{chattopadhyay2017counting} on contaminated data may be challenging because generative models cannot always correctly handle numbers \cite{saharia2022photorealistic}.
We leave the further in-depth analysis for future work.

\section{Conclusion}\label{sec:conclusion}

Recent generative models trained on billion-scale data enable to generate high-quality and high-fidelity images, and many users play with these models to share generated images on the web.
Observing such a trend, we questioned if such generated images affect the quality of future datasets collected images from the Internet.
To answer this question, we simulated contamination of generated images using a state-of-the-art generative model and conducted experiments on such data in various tasks, namely, image classification, image captioning, self-supervised learning, and image generation.
Throughout experiments, we found that generated images impact negatively on downstream performance, although its extent depends on the ratio of generated images and downstream tasks.
Additional analysis revealed that generated images degrade robustness to out-of-distribution data; application of generated images to data augmentation needs careful consideration; and easy detection of generated images may not be applicable to up-to-date generative models.

Based on these observations, we recommend that researchers to publish generative models carefully implement watermarks to enable identification of generated images. 
As we discussed in this paper, generated images have negative impacts on downstream performance, and their effect on new tasks is immeasurable; thus, publishers of generative models have responsibility to avoid possible contamination.
One simple way to avoid this problem is to implement either identifiable or invisible watermarks, as some publishers have already done, \eg, \cite{dalle2,stablediffusion}, then dataset curators can easily identify and filter them out. 
We also suggest that researchers who develop image datasets collected from the Internet should filter out or mark generated images, which may affect final downstream performance, because adding generated images may degenerate performance as shown in \cref{sec:data_augmentation}.

Another important implication of this paper is further research on the detection methods of generated images, in parallel with the development of generative models.
As experimented in \cref{sec:detection}, generated images of the latest generative methods cannot be detected by simple methods that once had been effective.
Consequently, their development for filtering is essential for the soundness of future research.

\section*{Acknowledgement}

This work was supported by JST, ACT-X Grant Number JPMJAX210H, Japan.
We used computational resources of ``mdx: a platform for the data-driven future'' and RAIDEN (Riken AIp Deep learning ENvironment).
R.H. thanks Kai Katsumata at the University of Tokyo for his suggestions on image generation experiments.

{\small
\bibliographystyle{ieee_fullname}
\bibliography{ref}
}

\appendix


\section{Detailed Experimental Configurations}\label{app:config}

This section describes the detailed experimental settings and configurations.

\subsection{Dataset Creation}

We generated images using the StableDiffusion model\footnote{\url{https://github.com/CompVis/stable-diffusion}} and its accompanying pre-trained weight (\texttt{sd-v1-1.ckpt}) on eight NVIDIA A100 GPUs.
Each image of the datasets was sampled by 50 steps of the PLMS sampler with an unconditional guidance scale of 7.5, which is identical to the setting of its web application.\footnote{\url{https://huggingface.co/spaces/stabilityai/stable-diffusion/blob/main/app.py}}



\subsection{Image Classification}

We trained ResNet-50 and Swin-S models following \texttt{torchvision}'s training protocol\footnote{\url{https://github.com/pytorch/vision/tree/v0.13.1/references/classification}} on eight NVIDIA A100 GPUs.
The results of Swin-S were calculated using parameters with exponential moving average.

\subsection{Image Captioning}

We fine-tuned the captioner and filter modules of the BLIP model following \texttt{LAVIS}'s training script\footnote{\url{https://github.com/salesforce/LAVIS/blob/v0.1.0/run_scripts/blip/train/train_caption_coco.sh}} on two NVIDIA A100 GPUs.

\subsection{Self-supervised Learning}

We pre-trained MAE following the official implementation\footnote{\url{https://github.com/facebookresearch/mae/tree/main}} on eight NVIDIA A-100 GPUs and fine-tuned the last layer of its encoder on 16 NVIDIA V-100 GPUs.
Pre-training was for 200 epochs with 40-epoch warmup with a batch size of 4096, using gradient accumulation once in every two iteration.
Fine-tuning was for 90 epochs using the LARS optimizer with a batch size of 16,384.

\subsection{Image Generation}

We trained and generated images from IDDPM following the official instructions for the ImageNet-64 dataset\footnote{\url{https://github.com/openai/improved-diffusion/tree/main}} on eight NVIDIA V-100 GPUs.
The model was trained for $1.8\times 10^6$ iterations using the $L_\text{hybrid}$ objective with a batch size of 512.
We generated 50,000 images with 250 sampling steps from EMA models.
The computation of metrics is based on \url{https://github.com/NVlabs/stylegan2-ada-pytorch/tree/main/metrics}.

\subsection{Detection of Generated Images}

For the experiments in Section 6.1, we first extracted the features of ImageNet-pre-trained ResNet-50 on 12,000 images from ImageNet and SD-ImageNet.
Each feature vector has a dimension of 2,048.
Then, we trained a linear classifier and a two-layer MLP with a hidden size of 1,024 with a ReLU activation to classify them using 10,000 feature vectors for 5,000 iterations using the Adam optimizer with a batch size of 128.
Their performances were evaluated on the other 2,000 test vectors.
The linear classifier and the MLP achieve 83\% and 86\% accuracy, respectively.

\section{Example Images}\label{app:example_images}

This section displays some example images from SD-ImageNet and ImageNet.

\subsection{Comparison of Real and Generated Images}

In Section 4.4, we hypothesized that generated images have fewer modes than real ones, which causes the performance degeneration.
Comparing randomly selected images from ImageNet and SD-ImageNet in \cref{fig:elephant_sd,fig:elephant_in} visually supports this hypothesis.

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.9\linewidth]{elephant_in.pdf}
    \caption{Real images of African elephants from ImageNet.}
    \label{fig:elephant_in}
\end{figure}

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.9\linewidth]{elephant_sd.pdf}
    \caption{Generated images of African elephants from SD-ImageNet.}
    \label{fig:elephant_sd}
\end{figure}


\clearpage
\subsection{Examples of \texttt{titi}}

In Section 4.1, we argued that some categories were semantically diverse because the ambiguity of category names.
\Cref{fig:titi} presents randomly selected images from the \texttt{titi} category.
Although ImageNet intended this class to mean a New World monkey, the generated images are mostly photos of human, because ``titi'' is also a name of people.

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.9\linewidth]{titi.pdf}
    \caption{Generated images of the \texttt{titi} category from SD-ImageNet.}
    \label{fig:titi}
\end{figure}

\end{document}
