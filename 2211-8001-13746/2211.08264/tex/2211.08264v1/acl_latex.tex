% This must be in the first 5 lines to tell arXiv to use pdfLaTeX, which is strongly recommended.
% \pdfoutput=1
% In particular, the hyperref package requires pdfLaTeX in order to break URLs across lines.

\documentclass[11pt]{article}

% Remove the "review" option to generate the final version.
\usepackage[final]{acl}

% Standard package includes
\usepackage{times}
\usepackage{latexsym}
% For proper rendering and hyphenation of words containing Latin characters (including in bib files)
\usepackage[T1]{fontenc}
% \usepackage[T1,T2A]{fontenc}
% For Vietnamese characters
% \usepackage[T5]{fontenc}
% See https://www.latex-project.org/help/documentation/encguide.pdf for other character sets

% This assumes your files are encoded as UTF8
\usepackage[utf8]{inputenc}

% This is not strictly necessary, and may be commented out,
% but it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}

\usepackage{graphicx}
\usepackage{xcolor}
\usepackage{amsfonts}
\usepackage{mathrsfs}
\usepackage{float}
\usepackage{paralist}

\restylefloat{table}
\usepackage{listings}% http://ctan.org/pkg/listings
\newcommand*{\ttfamilywithbold}{\fontfamily{lmtt}\selectfont}
\lstset{
  basicstyle=\ttfamilywithbold\bfseries\small,
  mathescape
}

\usepackage[arabic,russian,british,finnish]{babel}
\babelprovide[import,main]{bengali}
\babelprovide[import,main]{arabic}
\babelprovide[import]{hindi}
\babeltags{english=english}
\babeltags{bengali=bengali}

% Arxiv does not support xelatex, so the stuff below is not usable.
% %%% For language switching -- like babel, but for xelatex
% \usepackage{tikz}
% \usepackage{pgf}
% \usepackage{polyglossia}
% \setmainlanguage{english}
% \setotherlanguages{finnish,bengali,arabic,russian,korean} %% or other languages
% \newfontfamily\arabicfont[Script=Arabic]{Amiri}
% \newfontfamily\cyrillicfont[Script=Cyrillic]{Charis SIL}
% \newfontfamily\bengalifont[Script=Bengali]{FreeSerif}
% \newfontfamily\koreanfont{Noto Serif CJK KR}[Script = Hangul, Language = Korean]
% \setmainfont{TeX Gyre Termes}  % Use this font, or the whole paper will look very different. Using Times New Roman here gives a similar result but it silently ignores all small caps.
% \newfontfamily\bengalifont[Script=Bengali]{Akaash}
% \usepackage[banglamainfont=Kalpurush, 
%             banglattfont=Siyam Rupali
%           ]{latexbangla}

% \usepackage[space]{xeCJK}
% %%% Assuming Chinese is the main CJK language...
% \setCJKmainfont{Noto Serif CJK SC}
% \setCJKsansfont{Noto Sans CJK SC}

% %%% Define fonts for Japanese and Korean
% \newCJKfontfamily\japanesefont{Noto Serif CJK JP}

% %%% You can also upload your own font files
% % \newCJKfontfamily\koreanfont{[UnGraphic.ttf]}
% %%% ...or go along with a font available on the server
% \newCJKfontfamily\koreanfont{Noto Serif CJK KR}
%\babelprovide[import]{bengali}

% If the title and author information does not fit in the area allocated, uncomment the following
%
%\setlength\titlebox{<dim>}
%
% and set <dim> to something 5cm or larger.

\title{\textsc{QAmeleon} \includegraphics[height=2\fontcharht\font`\B]{figures/chameleon.png}:
Multilingual QA  with Only 5 Examples}

\definecolor{forestgreen}{HTML}{009B55}
% Mirella's comments
\newcommand{\ml}[1]{{\textcolor{forestgreen}{\bf{Mirella:} \emph{#1}}}}
\newcommand{\seb}[1]{\textcolor{olive}{[Seb: #1]}}
\usepackage{todonotes}
\newcommand{\note}[4][]{\todo[author=#2,color=#3,size=\scriptsize,fancyline,caption={},#1]{#4}}
\newcommand{\Seb}[2][]{\note[#1]{seb}{olive}{#2}}
\newcommand{\jo}[1]{{\textcolor{violet}{\bf{Joshua:} \emph{#1}}}}
\newcommand{\pri}[1]{\textcolor{cyan}{[Pri: #1]}}
\newcommand{\fh}[1]{{\textcolor{blue}{\bf{Fantine:} \emph{#1}}}}
\newcommand{\ca}[1]{{\textcolor{red}{\bf{Chris:} \emph{#1}}}}

\newcommand{\tydiqa}{\textsc{TyDi QA}}
\newcommand{\tydiqagoldp}{\textsc{TyDiQA-GoldP}}

\author{Priyanka Agrawal\thanks{\ \ Equal contribution. See Contributions section for details.} \ \ \ \ 
        Chris Alberti$^{*}$ \ \ \ 
        Fantine Huot \ \ \ \
        Joshua Maynez \\
        {\bf Ji Ma} \ \ \ \  
        {\bf Sebastian Ruder} \ \ \ \ 
        {\bf Kuzman Ganchev} \ \ \ \ 
        {\bf Dipanjan Das} \ \ \ \ 
        {\bf Mirella Lapata} \\
  Google Research\\
  \texttt{\normalsize \{priyankagr,chrisalberti,fantinehuot,joshuahm,} \\
  \texttt{\normalsize maji,ruder,kuzman,dipanjand,lapata\}@google.com}
  }

\begin{document}
\maketitle
\begin{abstract}

%QAmeleon: Few-shot Question Answering for Multiple Languages

The availability of large, high-quality datasets has been one of
the main drivers of recent progress in question answering (QA).
Such annotated datasets however are difficult and costly to collect,
and rarely exist in languages other than English, rendering QA
technology inaccessible to underrepresented languages. An alternative
to building large monolingual training datasets is to leverage
pre-trained language models (PLMs) under a few-shot learning setting. Our approach, \textsc{QAmeleon}, uses a PLM to automatically \emph{generate} multilingual data upon which QA
models are trained, thus avoiding costly annotation. Prompt tuning
the PLM for data synthesis with only five examples per language delivers accuracy
superior to translation-based baselines, bridges nearly 60\% of the gap between
an English-only baseline and a fully supervised upper bound trained on almost 50,000 hand labeled examples, and always leads to substantial improvements compared to fine-tuning a QA model directly on labeled examples in low resource settings. Experiments on the \tydiqagoldp{} and
\textsc{MLQA} benchmarks show that few-shot prompt tuning for data synthesis scales across
languages and is a viable alternative to large-scale annotation. 

%Question Answering has been a well studied area especially for high resource languages. However, existing works do not scale well to low-resource languages due to unavailability of data for training such systems. In this work, we propose the use of large language models (LLMs) for scaling question answering for underrepresented languages. More specifically, we first explore various data generation strategies using LLMs, including prompt engineering and prompt-tuning. We further conduct data augmentation experiments for question answering to distill knowledge of LLMs to comparably smaller models via synthetic data under zero-shot and few-shot settings. 
\end{abstract}

\section{Introduction}

Question answering (QA) has seen impressive progress in recent years
enabled by the use of large pre-trained language models
\cite{devlin-etal-2019-bert,lewis-etal-2020-bart,Raffel:ea:2020}, and
the availability of high-quality benchmarks
\cite{rajpurkar-etal-2016-squad,trischler-etal-2017-newsqa,kwiatkowski-etal-2019-natural}. Most
QA datasets frame the task as reading comprehension where the question
is about a paragraph or document and the answer is a span therein.
Advances in QA modeling have been primarily reported for English,
which offers a considerable amount of high-quality training data
compared to other languages.  More recently, efforts have focused on
the creation of \emph{multilingual} QA benchmarks such as \tydiqa{} (10~languages; \citealt{clark-etal-2020-tydi}), \textsc{MLQA}
(6~languages; \citealt{lewis-etal-2020-mlqa}), and \textsc{XQuAD}
(10~languages; \citealt{artetxe-etal-2020-cross}). Among these, only \tydiqa{} is genuinely large-scale, \textsc{MLQA} and \textsc{XQuAD} are
limited to an evaluation set due to the high cost and labor required
to collect data across languages.

As a result, efforts to localize QA models to new languages have been
primarily focusing on \emph{zero-shot} approaches. Recent proposals
include using machine translation to approximate training data for
supervised learning \cite{lewis-etal-2020-mlqa}, and data augmentation
via generating synthetic questions for new languages
\cite{riabi-etal-2021-synthetic,shakeri-etal-2021-towards}. Both
approaches rely on transfer from English, which leads to a dependence on translation 
artifacts \citep{koppel-ordan-2011-translationese,artetxe-etal-2020-translation} and a bias towards the linguistic characteristics of English, which is not the
best source for all target languages \cite{Lin2019}. However,
annotating a minimally-sized data sample can potentially overcome
these limitations while incurring significantly reduced costs compared
to full dataset translation \citep{garrette-baldridge-2013-learning}.

%Existing methods for English-based QA data generation require
%supervised training data
%\cite{alberti-etal-2019-synthetic,lewis-etal-2021-paq}, which is not
%available for many languages. Multilingual approaches
%\cite{riabi-etal-2021-synthetic,shakeri-etal-2021-towards} still rely
%on transfer from English supervised data, which is suboptimal as
%English is not the best source language for transfer
%\cite{Lin2019}. In contrast, we demonstrate the value of even a small
%number of target-language examples. \seb{I have tried to add some
%  comparison with prior work that could motivate our approach.}

\begin{figure*}[ht]
\centering
\includegraphics[width=\textwidth]{figures/QAmeleon_diagram.pdf}
\caption{Synthetic data generation for multilingual question-answering
  (QA). Left: Examples of the multilingual QA task. Translations are
  added for readability. Middle: Strategies for localizing QA models
  to new languages: 1. Using English QA data as a zero-shot approach,
  2. with Machine Translation (MT) to approximate training data for
  supervised learning, and 3. few-shot approaches with a handful of
  multilingual examples. Right: Model performance on the multilingual QA
  task. We report average Exact Match (EM)  across all
  languages on the \tydiqagoldp{} dataset \cite{clark-etal-2020-tydi}.  }
\label{fig:approach}
\end{figure*}

In this paper, we argue that a few-shot approach in combination with
synthetic data generation and existing high-quality English resources
can mitigate some of the above mentioned artifacts. Beyond question
answering, multilingual approaches have succeeded at leveraging a
small number of annotations within a variety of tasks
\citep[\emph{inter alia}]{zhao-etal-2021-closer} including natural
language inference, paraphrase identification, and semantic parsing
\cite{Sherborne:Lapata:2022}. Existing work \citep[\emph{inter
    alia}]{brown2020language,schick-schutze-2021-just} has further
shown that prompting pre-trained large language models (PLMs) can lead
to strong performance on various tasks, including question answering
\cite{khashabi-etal-2020-unifiedqa,chowdhery2022palm} and open-ended
natural language generation
\cite{Tang:ea:2022,Yang:ea:2022}. Investigations of prompting in
multilingual settings have also shown strong few-shot performance in
classification tasks \cite{winata-etal-2021-language}, natural
language inference \cite{zhao-schutze-2021-discrete}, common sense
reasoning \cite{Shi:ea:2022}, machine translation \cite{Lin2022},
and retrieval \cite{dai2022promptagator}.

We synthesize these directions into \textsc{QAmeleon}, an approach for
bootstrapping multilingual QA systems, with as few as five examples in
a new target language (see Figure~\ref{fig:approach}). We use gold
annotations to prompt-tune a PLM in order to automatically generate
multilingual QA data, which is then used to train a QA model. We find
that \textsc{QAmeleon} delivers accuracy superior to zero-shot methods
and competitive translation-based baselines, and in some cases
competes with the fully supervised upper bound.\footnote{This is noteworthy as multilingual models fine-tuned on translated data---also known as translate-train---form the state of the art on most multilingual datasets \cite{ruder-etal-2021-xtreme}.} Experiments on the
\tydiqa{} \cite{clark-etal-2020-tydi} and \textsc{MLQA}
\cite{lewis-etal-2020-mlqa} benchmarks show that few-shot prompt
tuning \cite{lester-etal-2021-power} scales across languages,
significantly outperforms prompt engineering \cite{brown2020language}
with the same number of labeled examples, and is a viable alternative
to large-scale annotation.

% \begin{figure}[t]
% \centering
% \includegraphics[width=\columnwidth]{figures/example_sw_fi.png}
% \caption{Examples of synthetic QA data generated by
%   \textsc{QAmeleon}. Passages~$c$ in the target language are sampled
%   randomly from Wikipedia. The PLM generates question~$q$ and
%   extractive answer~$a$ (highlighted in red) from the
%   passage. Ttranslations are added for readability. More examples are
%   listed in Tables~\ref{tab:examples} and~\ref{}.}
% \label{fig:example}
% \end{figure}

Our contributions include (a)~\textsc{QAmeleon}, a new approach to
bootstrapping a multilingual QA system with only 5 examples in a given
target language; \textsc{QAmeleon} prompt-tunes a PLM with a few gold
examples to automatically generate multilingual QA data which is then
used to train a QA model; (b)~a series of experimental results showing
significant improvements over existing approaches in the few-shot
regime, ranging from 11.5\% absolute accuracy on \tydiqagoldp{}
\cite{clark-etal-2020-tydi} over zero-shot methods and 6.6\% absolute
accuracy over a competitive translate-train baseline; (c)~analyses
studying the behavior of \textsc{QAmeleon} in zero shot
and low resource regimes, on different multilingual QA datasets, and
in comparison to prompt-engineering. 

%the limitations of BLEU as a metric to assess the quality of synthetic
%QA datasets, and the effectiveness of prompt tuning in super-low
%resource regimes.
%
%automatically generating multilingual QA data by prompt-tuning a PLM
% with only 5~examples in a target language, and subsequely
%training a QA a bot
%
% 
%bootstrapping a multilingual QA system with only 5 examples in a given
%target language,
%
%
%%To summarize, we claim the following contributions for our work:
%\begin{itemize}
%    \item \textsc{QAmeleon} bootstraps a multilingual QA system with only 5 examples in a given target language, by prompt-tuning a PLM with these limited examples to automatically generate multilingual QA data which is then used to train a QA model.
%    \item We obtain significant improvements over existing approaches in the few-shot regime, ranging from 12.5\% absolute accuracy on \tydiqa{} over zero-shot methods and 6.3\% absolute accuracy over a competitive translate-train baseline.
%    \item We find that prompt tuning significantly outperforms prompt engineering with the same number of available labeled examples for multilingual QA generation.
%    \item We present analysis studying the behavior of \textsc{QAmeleon} in different zero shot and low resource regimes and on different multilingual QA datasets, the limitations of BLEU as a metric to assess the quality of synthetic QA datasets, and the effectiveness of prompt tuning in super-low resource regimes.
%\end{itemize}

% Existing methods for QA data generation in English require supervised training data \cite{alberti-etal-2019-synthetic,lewis-etal-2021-paq}, which is not available for many languages. Multilingual approaches \cite{riabi-etal-2021-synthetic,shakeri-etal-2021-towards} still rely on transfer from English supervised data, which is suboptimal as English is not the best source language for transfer \cite{Lin2019} and errors are introduced via the transfer method. In contrast, our method requires a small number of target-language examples and does not transfer from English, magnifying the value of extremely low target-language resources. \seb{I have tried to add some comparison with prior work that could motivate our approach.} \jo{Added to the comparison that we don't need a lot of data + we avoid transfer issues}

% \begin{figure}[ht]
% \centering
% \includegraphics[width=\columnwidth]{figures/example_sw_fi.png}
% \caption{Examples of the data generated by our approach, \textsc{QAmeleon}. The input passages $c$ are sampled randomly from Wikipedia in the corresponding language. The PLM generates a question $q$ and an extractive answer $a$ from the input passage (highlighted in the passage). Auto-translations are added for readability. More examples are listed in Table \ref{tab:examples} and \ref{}. \ml{the examples should have correct translations rather than being based on a translation engine} \fh{I'm working on a new fig that combines fig 1 and 2. It will have gold examples to illustrate the multilingual QA task, and we move the generated examples to the appendix.}}
% \label{fig:example}
% \end{figure}

\section{Problem Formulation}
\label{sec:method}
% \begin{figure}[ht]
% \centering
% \includegraphics[width=\columnwidth]{figures/diagram.png}
% \caption{A sketch of the QA dataset generation approaches in this work (MT, PE and PT). A prompt is either hand-engineered or trained using 5 or 50 labeled examples per language. A PLM or a machine translation system are used to generate the synthetic QA dataset (left). Finally an mT5-XL model is trained on the generated data to perform QA on the target set of languages (right). \ml{not sure the figure is very informative, as it stands it is a mixture of the approach and experimental settings; it also does not add to the description in the main text, so probably remove it and expand Figure 1, add another language, for instance}.}
% \label{fig:approach}
% \end{figure}

% \ml{the figure is too busy; it should be substantially simplified; do a very general sketch of the approach, essentially keep the last block; downstream usage, but substitute Translation block with something more general like synthetic data generator, and Smaller model with Downstream QA model. You can then have something in the Figure that explains what Synthetic data generator is, i.e., MT, Prompt engineering, prompt tuning.}

Let~$\mathcal{D}_{l}$ denote a QA dataset with examples written by
human annotators, where $l$ is a \emph{target} language in a set~$L$
of languages of interest. $\mathcal{D}_{l}$ consists of samples~$(c,
q, a)_l$, where~$c$ is a paragraph of text, $q$~is a question, and
$a$~is an answer extracted from~$c$ (see Figure~\ref{fig:approach}
left). We will further use~$\mathcal{D}_{l,n}$ to denote the size of
dataset $\mathcal{D}_{l}$, with~$n$ referring to the number of
examples it contains. For instance, $\mathcal{D}_{\mathrm{fr},5}$ 
denotes a French QA dataset with 5 examples. Finally,
let~$\mathcal{U}_{l}$ denote sets of \emph{unlabeled} paragraphs in
language $l$; we assume these are in-domain with respect to the
paragraphs in~$\mathcal{D}_{l}$ but are not accompanied by questions
or answers.

Throughout this work, we will assume the availability
of~$\mathcal{D}_{\mathrm{en}}$, a large QA dataset in English
(\emph{source} language). This assumption corresponds to the
observation that most large-scale QA datasets
\cite{rajpurkar-etal-2016-squad,yang-etal-2018-hotpotqa,bajaj2016ms,kwiatkowski-etal-2019-natural} contain examples exclusively in
English.
%The dataset contains examples of the form $(c, q, a)_{en}$, where $c$~is a text passage, $q$~is a question, and $a$ is an answer extracted from~$c$
For languages other than English we assume that only a small number of samples~$\mathcal{D}_{l,n}$ are available for training (e.g., \mbox{$n=5$})
or no data at all.  We will also assume that sets~$\mathcal{U}_{l}$ of
unlabeled passages are available for all target languages. These can
be easily sampled from in-domain corpora without manual
annotation. Our task is to synthesize QA data in each \emph{target}
language~$l$ in order to train QA models on~$l$ directly.

There are many ways of obtaining synthetic QA examples depending on
which assumptions are met (e.g.,~whether gold annotations are
available in the target language).  We formally describe these below
and separately discuss how they can be understood in light of
different resource scenarios.

\section{Synthetic Data Generation}
\label{sec:synth-data-gener}

% In this work, we want to build QA capabilities in the languages where there is no or very little data. To achieve this, we explore approaches for generating synthetic QA data in such low-resource languages \fh{or in a few-shot setting?} to train multilingual question answering models.
% %There are various ways of realizing this, especially depending on the amount of available annotated data in the target language.
% We utilize the following three approaches for data-efficient generation of multilingual QA data: 1) machine translation (MT) - uses a publicly available machine translation system to synthesize data in the target language from English, 2) prompt engineering (PE) - uses $n$ in target language examples as exemplars to prompt a PLM, benefiting from in-context learning and 3) \fh{QAmeleon, which uses} prompt tuning (PT) - fine tunes a prompt, the first $m$ tokens of the input, using $n$ in target-language examples. It is important to note that for PE and QAmeleon, $n$ examples in target languages  can be either MT generated examples or human-annotated gold examples.
% As shown in Figure \ref{fig:approach}, we then use the generated synthetic data for training a multilingual QA system and then evaluate their performance on the publicly available development and test splits of existing multilingual QA datasets.
% We describe each of the approaches MT, PE and QAmeleon in the rest of this section.

% \ml{you say that prompt engineering uses a few translated examples, unclear whether these are automatically translated or by a human, and if these are automatically translated then, I think prompt engineering should have human translations to be compatible with prompt tuning.} \jo{Updated this section to describe the methods without describing the data, from the perspective of the method, they are just N target-language examples} \pri{added the N examples could be either MT or gold.} 

% We assume the existence of a QA dataset in the source language i.e. English $\mathcal{D}_{en}$ throughout this work. The dataset contains examples of the form $(c, q, a)_{en}$, where $c$ is a text passage that serves as context, $q$ is a question, and $a$ is an answer extracted from $c$.  The task is to synthesize QA data in a new language $l$ containing $(c, q, a)_{l}$ examples as show in Figure \ref{fig:example}. 


\subsection{Machine Translation (MT)} A widely adopted approach
\cite{lewis-etal-2020-mlqa,shakeri-etal-2020-end} makes use of a
machine translation system~$\mathcal{T}$ to automatically translate
text from one language into another.
Let~$\mathcal{T}_{l'}(\mathcal{D}_l)$ denote the translation of
dataset $\mathcal{D}_l$ from language~$l$ to language~$l'$. The
translation is performed by independently applying $\mathcal{T}$ to
context $c$, question~$q$, and answer~$a$ for each example in the
source dataset (see approach 2 in Figure~\ref{fig:approach}). A
synthetic QA dataset~$\mathcal{D}_\mathrm{MT}$ is generated by
translating the entire English dataset into each language of interest:
\[
\mathcal{D}_\mathrm{MT} = \mathcal{D}_\mathrm{en} \cup \bigcup_{l \in
  L - \{\mathrm{en}\}} \mathcal{T}_{l}(\mathcal{D}_\mathrm{en}).
\]

The approach described here is widely known as ``translate-train''. An
alternative is ``translate-test'', where translation is employed
during inference instead of training. Multilingual inputs are
translated to English and inference is done via an English QA
model. The English predictions are then translated back to the
respective target language. We experimentally found ``translate-test''
to perform poorly on our task in comparison to translate-train due to
its reliance on multiple noisy translation steps.

Note that training on~$\mathcal{D}_\mathrm{MT}$ still relies on the
support of the high-quality~$\mathcal{D}_\mathrm{en}$.  Previous work
\cite{kramchaninova-defauw-2022-synthetic, vu-2022-zeroshot} has
highlighted various limitations with multilingual approaches based on
MT including (a)~their dependence on the quality of available MT
systems in a given language and in turn the availability of
high-quality (expensive) parallel data, (b)~a potential misalignment
of answer spans after translation in context to the passage vs
translation of answers independently, and (c)~translationese artifacts
and English-centric content topics \cite{clark-etal-2020-tydi}.

% \seb{We should mention (here or in another section) the limitations of using MT: dependence on MT quality / high-quality parallel data, translationese, English-centric content. \textcolor{cyan}{P: have added it, pl check}} \seb{Thanks. Looks good.}

% \ml{the Prompt engineering section should reiterate your assumptions; clarify what you mean by "a few translations, and in language data, is this target language data? Also I would substitute "original" in the prompt with "target".}  Chris: I think this is addressed. The prompt is consistent with the experiments we ran, so I think we have to say "original", although I agree "target" might be better.

\subsection{Prompt Engineering (PE)}

% \seb{I think this section would be more convincing if we ground it in and generalize existing work. For instance, we could describe how existing work \cite{shakeri-etal-2020-end,lewis-etal-2021-paq} consists of span detection/answer generation and question generation steps and how we operationalize these in the form of prompting in our framework.} \jo{+1 to Seb's comment} \jo{We should describe here why PLMs are uniquely well-fitted to produce in-language data.}  Chris: the citations mentioned here are discussed in related work so I think this is addressed.

PLMs \cite{brown2020language,chowdhery2022palm} have recently shown
unprecedented performance on a vast number of tasks, including natural
language generation, without the need for modifying any of the model's
parameters, simply by hand-designing a textual prompt that
instructs the model to perform a certain task. Following
\newcite{brown2020language}, we consider a class of hand-designed
prompts referred to as  ``prompting'' or  ``in-context learning''. The prompt starts with
a free form instruction, followed by a small number of instances
exemplifying how the task is solved. An incomplete instance is then
appended to this prompt and the PLM performs the task by completing
that instance. We refer to this approach as ''prompt engineering''
(PE), since the input to the PLM has to be hand-engineered based on
human intuition about the target task (see approach 3 in
Figure~\ref{fig:approach}).

%// $\forall$ $(c_l, q_\mathrm{en}, a_\mathrm{en}, q_l, a_l)$ $\in$ $\mathcal{C}_{l,n}$

In order to hand-engineer prompts for our task, we use a small set of
parallel examples consisting of passages, questions, and their answers
in the English source and target language~$l$. We discuss how we
construct these examples shortly. For now, suffice it to say that we
create two prompts\footnote{We find that joint answer and question generation using single-stage prompting performs worse in comparison to two-stage generation.}, for answer and question generation,
respectively. 
% \Seb{Could you add a footnote highlighting that joint answer and question generation performs worse (I think I remember that you mentioned this)? - addressed} 
Our first prompt is used to obtain an answer~$a_l$ in
the target language~$l$ from passage~$c_l$:
\begin{lstlisting}
I will write potential answers
for the following passages.
  Passage: $c_l$
  Answer in English: $a_\mathrm{en}$
  Answer in the original language: $a_l$
...
\end{lstlisting}
The second prompt generates question~$q_l$, utilizing passage~$c_l$
and the previously predicted answer~$a_l$:

\begin{lstlisting}
I will write questions and answers
for the following passages.
  Passage: $c_l$
  Answer: $a_l$
  Question in English: $q_\mathrm{en}$
  Question in the original language: $q_l$
...
\end{lstlisting}

%// $\forall$ $(c_l, q_\mathrm{en}, a_\mathrm{en}, q_l, a_l)$ $\in$ $\mathcal{C}_{l,n}$

We generate synthetic data instances $(c,q,a)_l$ where~$a$ and~$q$ are
inferred by applying our two prompts consecutively on each
passage~\mbox{$c_l \in \mathcal{U}_{l}$} (recall $\mathcal{U}_{l}$ is
the set of unlabeled passages in target language~$l$). We denote
this prompting based generation as~$\mathcal{P}^e_l$ and write the
generation of the synthetic dataset as: 
\[\mathcal{D}_\mathrm{PE} = \mathcal{D}_\mathrm{en} \cup \bigcup_{l \in L - \{\mathrm{en}\}} \mathcal{P}^e_l(\mathcal{U}_l).\]
%

%\Seb{Should the part about using MT for obtaining the expemplars in both settings be moved to Section 3.4 as the ``English-only'' and ``Few-shot'' settings are only introduced there and there is some overlap between both descriptions? \\ Priyanka updated this to refer to section 3.4} We adopt two ways of constructing the small set of parallel examples used for in-context learning depending on the data scenarios as described in Section \ref{sec:assumptions}.
% depending on the availability of gold annotations in the target
% language. 
In the English-only scenario as the examples in target language are not available, we make
use of ``English-only'' data and resort to machine translation to
obtain questions and answers in the target language:
\begin{eqnarray*}
   & & \mathcal{C}^\mathrm{en-only}_{l,n} = \\
   & & \{(\mathcal{T}_l(c), q, a, \mathcal{T}_l(q), \mathcal{T}_l(a)) | (c, q, a) \in \mathcal{D}_{\mathrm{en}, n}\},
\end{eqnarray*}
In a ``Few-Shot'' setting, with access to $n$-labeled examples in the
target language, we translate the target questions and answers into English: 
\begin{eqnarray*}
   & & \mathcal{C}^{n\mathrm{-shot}}_{l,n} = \\
   & & \{(c, \mathcal{T}_\mathrm{en}(q), \mathcal{T}_\mathrm{en}(a), q, a) | (c, q, a) \in \mathcal{D}_{l, n}\}.
\end{eqnarray*}
In the above, $\mathcal{T}_l(t)$ denotes the translation of text $t$ into language $l$. In either case, we note that dataset~$\mathcal{D}_{\rm PE}$ is
obtained by using English intermediates or ``bridges'' in the prompt, i.e., asking the model to predict $a_{\mathrm{en}}$ and $q_{\mathrm{en}}$ in addition to $a_l$ and $q_l$ respectively,
as we experimentally found it improves the quality of the generated
data.  The use of a bridge for this task can be thought of as an
example of multilingual ``chain of thought'' prompting
\cite{wei2022chain}.



% % In this approach, like MT, we assume the zero-shot setting with availability of only English QA data. We use machine translations of 5 English QA pairs to construct the prompt. To produce target-language specific content, we input in-language passages for inference. 
% The prompts are hand-engineered as follows. For each language $l$, we consider $\mathcal{D}_{\mathrm{en},5}$ i.e. 5 examples randomly selected from $\mathcal{D}_{\mathrm{en}}$. We then construct two prompts, utilizing all $(c, q, a)_{\mathrm{en}} \in \mathcal{D}_{\mathrm{en},5}$, one for answer generation,



% \seb{Is the potential answer also provided as input to the model for question generation or is the model required to predict question and answer jointly? \textcolor{cyan}{P: have added it below}} \seb{Thanks. That clarifies it. Out of curiosity, have you also tried predicting question and answer at the same time with the same prompt?} \jo{We did, Chris/Priyanka might know more, I thought QAGen quality wasn't as good}

% We assume a source of unlabeled passages $\mathcal{U}_{l}$ for each language $l$. We denote as $\mathcal{P}^e_l$ the use of these two prompts with a PLM to construct corresponding $a_l$ and $q_l$ in target language $l$.  The synthetic data contains $(c,q,a)_l$ where $a_l$ and $q_l$ are inferred by performing this operation on each unlabeled passage $c_l \in \mathcal{U}_{l}$. $a_l$ is predicted during first stage of answer generation from $c_l$ and  $q_l$ is inferred during second stage of question generation utilizing $c_l$ and $a_l$ i.e. the answer predicted in stage 1. The resulting data is then computed by taking its union with the labeled English dataset:
% \[\mathcal{D}_\mathrm{PE} = \mathcal{D}_\mathrm{en} \cup \bigcup_{l \in L - \{\mathrm{en}\}} \mathcal{P}^e_l(\mathcal{U}_l).\]

%  Note we use English intermediates (or ``bridges'') in the prompt, as this is found experimentally to improve the quality of generated data \fhP{we can cite the multilingual cot paper here}. The use of a bridge for this task can be thought of an example of multilingual ``chain of thought'' \cite{wei2022chain}. Note moreover that this method by default does not make use of any in language data, but only few translations, so it is a ``zero-shot'' method in the same sense as the MT approach is. We further conduct ablations by replacing translated English examples with the gold examples in target language (see Table \ref{tab:main_eval}). When using the gold examples, the English intermediates are created using MT of gold QA from language $l$m to English. \fh{The actual PE methodology should be separate from what data we use}. 


\subsection{\textsc{QAmeleon} (PT)}

% We have previously assumed
% there are no labeled QA examples in target language $l$. We now relax
% this assumption by making use of a small number of annotated samples
% $(c,q,a)_l$ in~$l$, collectively denoted by
% $\mathcal{D}_{l,n}$. 

Fine-tuning is one of the most common ways that PLMs have been used in
the last several years to solve a large variety of tasks in NLP. In
this approach, given a set of examples in the target language
$(c,q,a)_l$ denoted by $\mathcal{D}_{l,n}$, an optimizer is utilized
to minimize the cross-entropy loss and update the PLM's parameters for $P(a, q | c, l)$
over a small training set containing examples for the languages in
$L$. As for PE, we construct the training set for the PLM in one of
two ways. For ``English-Only'' we construct the dataset as $\bigcup_{l
  \in L} \mathcal{T}(\mathcal{D}_\mathrm{en})$, while for ``Few-Shot''
we use $\bigcup_{l \in L} \mathcal{D}_{l,n}$.

Given the small size of
the training set in the ``Few-Shot'' setting and the large size of current models, we opt for using prompt
tuning \cite[PT;][]{lester-etal-2021-power}, a parameter-efficient fine-tuning
variant where only the embeddings of the first $m$
tokens in the input of the PLM are allowed to be modified by the
optimizer (see approach 3 in Figure~\ref{fig:approach}).
We note that in prompt tuning, like in prompt engineering, the parameters of the PLM remain unchanged and what is trained is only a short soft prompt that is prepended to the input embeddings at inference time.
%We find
%experimentally that it is critical to utilize the prompt tuning
%variant of fine-tuning in our regime of choice, when working with
%$n=5$ examples per language.

% As in prompt engineering (PE), we assume access to unlabeled passages
% $\mathcal{U}_{l}$ for each language~$l$. 
We use~$\mathcal{P}^t_l$

to denote the operation of generating question-answer pairs through greedy decoding on the prompt-tuned PLM, by taking an unlabeled
passage $c_l \in \mathcal{U}_{l}$ as input, preceded by a few tokens
encoding language~$l$. We finally obtain the synthetic QA
dataset~$\mathcal{D}_\mathrm{PT}$ as 
\[\mathcal{D}_\mathrm{PT} = \mathcal{D}_\mathrm{en} \cup \bigcup_{l \in L - \{\mathrm{en}\}} \mathcal{P}^t_l(\mathcal{U}_l).\]

% \fh{maybe we can keep refering to them as few-shot or n-shot, and avoid the LR and SLR naming entirely}. 

\subsection{Data Assumptions}
\label{sec:assumptions}
While not all methods described above necessitate annotated target
language examples, we experimentally observe a sizeable improvement if
those examples are available.  We describe two scenarios which make
different data assumptions and lead to distinct experimental
configurations:


\begin{asparadesc}
  \item{\textbf{English-Only}} $\:$ In this scenario, only training data in
    English is available, denoted $\mathcal{D}_\mathrm{en}$. Prompt
    Engineering (PE) assumes parallel exemplars are available, whilst
    \textsc{QAmeleon} (PT) requires exemplars in the target language
    only; in the English-Only scenario both are possible by
    translating $k$ examples of the English data into the target
    language: $\mathcal{D}_{\mathrm{l},n}$ =
    $\mathcal{T}_{l}({\mathcal{D}_{\mathrm{en}, n}})$. Machine
    Translation (MT) approaches typically follow this scenario
    \cite{lewis-etal-2020-mlqa} with~$n$ being the size of the entire
    English training set.

\item {\textbf{Few-Shot} ($n$-shot)}  $\:$ It is possible that a small number of
  labeled examples are available in the target language, denoted
  $\mathcal{D}_{\mathrm{l},n}$.  In this scenario, parallel exemplars
  for Prompt Engineering (PE) can be obtained by translating the
  target language data into English: $\mathcal{D}_{\mathrm{en},n}$ =
  $\mathcal{T}_{\mathrm{en}}({\mathcal{D}_{\mathrm{l}, n}})$.
  Prompt Tuning (PT) only requires exemplars in the target language, which are readily available in this setting. MT methods  do
  not typically use additional exemplars in the target language.
  %In this work
  %we refer to the regime with $n=50$ as ``low resource'' and the
  %regime $n=5$ as ``extremely low resource''.

\end{asparadesc}

% We advocate \emph{n-shot} prompting tuning \cite{lester-etal-2021-power,brown2020language}, \textsc{QAmeleon} as the best alternative.


\section{Experimental Setup}
\label{sec:exp}

We evaluate the synthetic data generation approaches presented in
Section~\ref{sec:synth-data-gener} across various languages on two
benchmark datasets, which we discuss below. We also describe various
model configurations, and comparison systems before presenting our
results.

\subsection{Datasets}
\label{sec:datasets}


\paragraph{\tydiqa{}} \hspace*{-.25cm}\cite{clark-etal-2020-tydi}
is a multilingual extractive question answering dataset designed to
represent a typologically diverse set of languages. Annotators were
given a Wikipedia passage in the target language and asked to write a
question that could not be answered by that passage. For each question, the top-ranked Wikipedia article was then retrieved via Google Search.
Annotators were subsequently asked to answer the question given the
retrieved Wikipedia article. As a result of this information-seeking task design,
questions in \tydiqa{} are often without an answer. In this work
we consider the Gold Passage (GoldP) version of \tydiqa{} where only
questions with answers in the Wikipedia page are given and the model
has to identify the answer in the passage that contains
it (see Table~\ref{tab:data} for statistics on this dataset).

\paragraph{\textsc{MLQA}} \hspace*{-.25cm}\cite{lewis-etal-2020-mlqa}
is an extractive question answering dataset, designed for
evaluating multilingual and cross-lingual question answering
models. \textsc{MLQA} does not publish a training split, but only
development and test partitions. \textsc{MLQA} was created by aligning
sentences in Wikipedia passages across different
languages. Annotators then created questions based on English
sentences, professional translators translated these questions to
other languages, and finally annotators selected answers from passages
containing sentences aligned to the translated questions. As in
\tydiqagoldp{}, the task is to extract the answer from a passage given a question
(dataset statistics are shown in Table~\ref{tab:data}).

\paragraph{Unlabeled Data} We obtained paragraphs~$\mathcal{U}_l$ in each
target language from Wikipedia. Specifically, we pre-processed
Wikipedia pages using WikiExtractor \cite{Wikiextractor2015}. 
% a tool
% that delivers a plain text version of Wikipedia, already split into
% paragraphs, so no further processing is required. 
Paragraphs were
sampled uniformly, with a length between~200 and 510~characters. The
target language was determined based on the language code of the
Wikipedia edition.


\begin{table}[t]
    \centering \resizebox{\columnwidth}{!}{
    \begin{tabular}{l|cc|cc} 
                %  & TyDi-QA & TyDi-QA & MLQA & MLQA \\
 & \multicolumn{2}{c|}{\textbf{\tydiqagoldp{}}} & \multicolumn{2}{c}{\textbf{\textsc{MLQA}}} \\
       \multicolumn{1}{c|}{\textbf{Language}}  & \multicolumn{1}{c}{Train}   & \multicolumn{1}{c}{Eval}     & \multicolumn{1}{|c}{Dev} & \multicolumn{1}{c}{Test} \\
       \hline \hline
         Arabic & 14,805 & 921 & 517 & 5,335 \\
         Bengali & 2,390 & 113 & --- & --- \\
         Chinese & ---& ---& 504 & 5,137 \\
         English & 3,696 & 440 & 1,148 & 11,590 \\
         Finnish & 6,855 & 782 & --- & --- \\
         German & --- & --- & 512 & 4,517\\
         Hindi & ---& --- & 507 & 4,918 \\
         Indonesian & 5,702 & 565 & --- & --- \\
         Kiswahili & 2,755 & 499 & --- & ---\\
         Korean & 1,625 & 276 & --- & ---\\
         Russian & 6,490 & 812 & --- & ---\\
         Spanish & --- & --- & 500 & 5,253  \\
         Telugu & 5,563 & 669 & --- & --- \\
         Vietnamese & --- & --- & 511 & 5,495 \\
       \hline
         Total & 49,881 & 5,077 & 4,199 & 42,245 \\ \hline \hline
    \end{tabular}}
    \caption{Number of question-answer pairs per language and data split for the datasets considered in this work.}
    \label{tab:data}
\end{table}

% \begin{table*}[t]
%     \centering
%     \begin{tabular}{l|cccc} 
%       Method     & Translate  & n-Shot & Avg EM &  Avg F1 \\
%       \hline
%       mT5-XL Baseline   &            &      0 &   59.3 &         \\
%       MT         & \checkmark &      0 &   65.5 &         \\
%       PE (ours)         & \checkmark &      0 &   64.5 &         \\
%       PE+MT (ours)     & \checkmark &      0 &   \textbf{67.6} &         \\
%       \textsc{QAmeleon} (PT(TT))        & \checkmark &      0 &   65.9 &         \\
%       \textsc{QAmeleon} (PT) +MT        & \checkmark &      0 &   \textbf{68.3} &         \\
%       \hline 
%       PaLM-540B$\dagger$ &            &   1-10 &   60.5 &         \\
%       mT5-XL Baseline   &            &      5 &   63.7 &         \\
%       PE (ours)  &            &      5 &    62.7 \\
%       \textsc{QAmeleon} (PT) (ours)        &            &      5 &   70.5 &         \\
%       \textsc{QAmeleon} (PT) +MT (ours)      & \checkmark &      5 &   \textbf{71.8} &         \\
%     \end{tabular}
%     \caption{Efficacy of different methods of generating synthetic question-answering datasets for training multi-lingual reading comprehension systems for the TyDi-QA GoldP task. Except for $\dagger$ \cite{chowdhery2022palm}, reported numbers are obtained by finetuning mT5 XL on gold or synthetic data.}
%     \label{tab:main_eval}
% \end{table*}

\begin{table*}[t]
    \centering
    \begin{tabular}{lccc|ccc} 
    \multicolumn{1}{c|}{} & \multicolumn{3}{c|}{\textbf{English-Only}} & \multicolumn{3}{c}{\textbf{Few-Shot}} \\
       \multicolumn{1}{c|}{\textbf{Method}}     & Translate & Avg EM &  Avg F1 & n-Shot & Avg EM &  Avg F1\\
       \hline \hline
      \multicolumn{1}{l|}{Baseline}   &            &  58.2 & 73.9 & 5 &  63.7 &  78.3    \\
       \multicolumn{1}{l|}{MT}         & \checkmark &        65.5  & 79.4 & --- & --- & ---        \\
       \multicolumn{1}{l|}{PE}          & \checkmark &        64.9 & 77.1   & 5 &    62.8 & 77.9     \\
      \multicolumn{1}{l|}{PE+MT}     & \checkmark &    67.6 &   80.7  & 5 & 67.7  & 80.4  \\
      \multicolumn{1}{l|}{\textsc{QAmeleon} (PT)}        & \checkmark &        65.9 &  79.3    &      5 &  70.3 &   81.8\\
       \multicolumn{1}{l|}{\textsc{QAmeleon} (PT) + MT}        &
       \checkmark &        \textbf{68.3} & \textbf{81.0} &   5 &   \textbf{71.0} & \textbf{82.7} \\
       \hline
        \multicolumn{1}{l|}{code-davinci-002$\S$} &     & --- & ---  &   1 &   48.1 &     --- \\
        \multicolumn{1}{l|}{PaLM-540B$\dag$} &     & --- & ---  &   1--10 &   60.0 &     --- \\
        \multicolumn{1}{l|}{Flan-U-PaLM-540B$\ddag$} &     & --- & ---  &   1 &   68.3 &     ---
       \\\hline \hline
    \end{tabular}
    \caption{Synthetic question-answering data generation methods for
      training multilingual reading comprehension systems on
      \tydiqagoldp{}. Reported numbers are obtained by
      fine-tuning mT5-XL on gold or synthetic data. Average EM and F1 are computed
      excluding English, on the languages shown in Table~\ref{tab:lang}. For comparison we list recent few-shot prompting results with large language models on \tydiqagoldp{}: \newcite{chen2021evaluating}$\S$, \newcite{chowdhery2022palm}$\dag$, and \newcite{chung2022scaling}$\ddag$.
      }
    \label{tab:main_eval}
\end{table*}

\subsection{Model Configuration}
\label{sec:model-configuration}

\paragraph{Synthetic Data Generation} 
%\footnote{X is a PLM with more than 100B parameters, omitted for anonymity.}
%\seb{Would it be possible to compare to using a smaller version of PaLM for generation? I think this would help to make it clearer how important model size is for the quality of the generated QA data and whether this approach could be used by institutions with smaller compute budgets.}
 In our \tydiqa{} experiments, we treat the English training
 data as the English source. For \textsc{MLQA}, we employ the English
 \textsc{SQuaD} \cite{Rajpurkar2016} training data as the source.  In
 the Few-Shot scenario, our human-annotated target-language examples
 $\mathcal{D}_{l,n}$ are taken from the training split of
 \tydiqa{} and the validation split of \textsc{MLQA}.

For machine translation (MT), we employ the public Google Translate
API \cite{wu2016google} while the PLM utilized in this work is
PaLM-540b \cite{chowdhery2022palm}. We perform heuristic checks to
clean synthetic datasets~$\mathcal{D}_\mathrm{PE}$
and~$\mathcal{D}_\mathrm{PT}$.  We only preserve a question-answer
pair if the generated answer~$a$ is a substring of the given
context~$c$, but not a substring of the query~$q$. We perform the
first check as both \tydiqa{} and \textsc{MLQA}
are extractive QA datasets. We perform the latter check because we
empirically found that some of the low quality generated
question-answer pairs were trivially answered based on the content of
the question only, for example ($q$: ``where is X?'', $a$: ``X'').

In the construction of $\mathcal{D}_\mathrm{PE}$, we additionally
perform round-trip filtering \cite{alberti-etal-2019-synthetic} as the
qualitative analysis of random QA pairs suggested a higher level of
noise in the PE-generated data. This round-trip consistency check is
done by comparing the originally generated answer $a$ in $(c,q,a)_l$ with
the predicted answer. This predicted answer is obtained by prompting
the PLM to answer question $q$ based on passage $c$. We also tried
round-trip filtering for PT generated data,
%as shown in Appendix \ref{},
however, we did not observe any gains. We report detailed
statistics of the synthetically generated datasets in
Appendix~\ref{sec:data-analysis}.
%\pri{add reference to appendix}
% with employing round-trip consistency check with $\mathcal{D}_\mathrm{PT}$. 

In the construction of $\mathcal{D}_\mathrm{PT}$, we prompt-tune the
PLM on $\bigcup_{l \in L} \mathcal{T}(\mathcal{D}_\mathrm{en})$ or
$\bigcup_{l \in L} \mathcal{D}_{l,n}$ as detailed earlier. Prompt
tuning is performed with the AdaFactor optimizer
\cite{shazeer2018adafactor}. We tune a prompt of length 50 tokens for
up to 1,000 steps, evaluating every 50 steps, with a batch size of 16
examples, and learning rate of 0.3 with a linear warmup of 200
steps. We use early stopping to select the best prompt per language
based on BLEU \cite{papineni2002bleu} on a held-out dataset from English \tydiqagoldp{}, translated to each target language.

%\subsection{Baseline}
%\label{sec:baseline}
As an ablation, we use mT5-XXL \cite{xue2021mt5} as question generation model (Table \ref{tab:bleu_vs_qa}) to compare the quality of questions generated by mT5-XXL model with that of \textsc{QAmeleon} (PT).


\paragraph{Question Answering}
 We trained an mT5-XL model \cite{xue2021mt5} for question-answering
 to evaluate different synthetic data generation methods
 ($\mathcal{D}_\mathrm{MT}$, $\mathcal{D}_\mathrm{PE}$, and
 $\mathcal{D}_\mathrm{PT}$). As a baseline, we further use mT5-XL fine-tuned on available training data. Specifically, in the English-Only scenario, Baseline mT5-XL is
trained on the English QA data $\mathcal{D}_{en}$. In the Few-shot
scenario, Baseline mT5-XL is fine-tuned on $n$~human annotated examples in the
target languages (same number given to PE and PT). We conducted experiments on
 \tydiqa{} \cite{clark-etal-2020-tydi} and \textsc{MLQA}
 \cite{lewis-etal-2020-mlqa}, see
 Section~\ref{sec:datasets}.

During downstream QA evaluation, mT5-XL was fine-tuned with AdaFactor
with a learning rate of 0.0005, with a batch size of 64 examples, for
2,000 steps, evaluating every 50 steps.  For evaluating
question-answering accuracy, we used the development split of
\tydiqagoldp{} popularly used as evaluation set (the test split is unavailable)
%\Seb{This is true for the standard version of TyDi QA but the GoldP version actually does not have a test set so the dev set is used for evaluation by default (we use it in XTREME and even the official TyDi QA evaluation script uses it). So I think it's ok to refer to this as evaluation set instead. \\Priyanka updated this} 
and the development and
test splits of \textsc{MLQA}. We measure Exact Match (EM) and F1 and report the
unweighted averaged across languages (English is not included). To
compute per-language scores, we select the best mT5 checkpoint per
language using EM. 



\begin{table*}[t]
    \centering
    \begin{tabular}{l|cccccccccc} 
       \textbf{Method}  & \textbf{n-shot}   &  \textbf{Ar} & \textbf{Bn} & \textbf{Fi} & \textbf{Id} & \textbf{Ko} & \textbf{Ru} &  \textbf{Sw} & \textbf{Te} & \textbf{Avg} \\
       \hline \hline
       
       Baseline & 5 &  62.6 & 63.7 & 61.4 & 66.7 & 66.7 & 56.0 & 56.9 & 71.3 & 63.7 \\
       MT    & 0     &  64.3 & 61.1 & 63.9 & {71.3} & 66.3 & {60.7} & 71.1 & 64.9 & 65.5 \\
       PE & 0 & 60.5 & 68.1 & 63.4 & 67.1 & 65.9 & 53.6 & {74.7} & 62.3 & 64.5 \\
       \textsc{QAmeleon} (PT) & 5 & 65.6 &\textbf{ 77.9 }& 68.7 & 68.1 &\textbf{ 68.1} & 62.2 & 74.9 & \textbf{77.0} & 70.3 \\
      \textsc{QAmeleon} (PT) + MT & 5& \textbf{67.9}	& 74.3 &	\textbf{69.6}&	\textbf{74.2} &	67.0 &\textbf{	62.9 }&	\textbf{77.4 }&	75.0 & \textbf{71.0 }\\
    %   \textbf{69.2} & 73.4 & \textbf{68.5} &\textbf{ 75.4} & 69.9 & \textbf{63.5} & \textbf{77.2} & \textbf{77.3 }& \textbf{71.8}\\
    
       \hline
       Supervised & Multi-k & 75.7 & 81.4 & 74.5 & 79.8 & 77.2 &72.8 &
       82.6 & 83.0 & 78.4 \\
       \hline
       \% tokens in PLM & --- & \textit{0.15} &\textit{ 0.03} & \textit{0.42} & \textit{0.16} & \textit{0.19} & \textit{0.53} & \textit{0.01} & \textit{0.02} & --- 
       \\ \hline \hline
    \end{tabular}
    \caption{QA performance (Average EM) for individual languages on the
      \tydiqagoldp{} evaluation set; the backbone of the QA model is an
      mT5-XL model fine-tuned on gold (Supervised) or synthetically
      generated data. The final row displays the percent of tokens for
      each language in the PaLM training data.
      %\pri{Should we add PT-50? Also should we
      %      replace PE(0) with PE(5) or remove PE?}
    }
    \label{tab:lang}
\end{table*}


\section{Results}
% \ml{SLR, I don't think we need this acronym, we can just say extremely low resource setting.} \pri{changed, thanks}

% \ml{results in Table 2 should be discussed in more detail and be more clear. the term Baseline is not very informative. Just say mT5-XL fine-tuned on English data and transferred to other target languages, either zero shot or with 5 gold examples; by trained on 5 examples per language, do you mean fine-tuned? Also Qameleon setting with MT should be explained; the upper bound from Table 3 should be included in Table 2; Tables 3 and 2 could be merged; add a third block below PE(ours) with all Qameleon variants.}
\paragraph{\textsc{QAmeleon} (PT) Delivers the Best QA
  System} Table~\ref{tab:main_eval} summarizes our results on
\tydiqa{} for both English-only and Few-Shot scenarios. 
% \Seb{Are you still planning to add the F1 averages in Table 2?} %We
%compare various different approaches MT, PE and PT as
%described in Section \ref{sec:method} and Baseline described Section
%\ref{sec:baseline}.
Overall, we find that a low resource setting with 5 human-annotated
examples in the target language ($\mathcal{D}_{l,5}$) is useful
for scaling QA to multiple languages.  More specifically, 5-shot
prompt tuning gives an improvement of 11.5\% absolute (58.2\% $\rightarrow$ 70.3\%) in exact match
answer accuracy on the \tydiqagoldp{} evaluation set over mT5
fine-tuned on English data only (Baseline), 6.6\% (63.7\% $\rightarrow$ 70.3\%) over mT5 trained
on~5 examples per language (Few-shot Baseline), and~4.8\% (65.5\% $\rightarrow$ 70.3\%) over mT5
fine-tuned on the data obtained with the MT approach. %\ml{Why are we talking about dev set, the results in Table 1 are not test set? -- Chris: For tydiqa-goldp there is no test set so everyone uses the dev set as evaluation set. I changed the text to say evaluation set}
\textsc{QAmeleon} further improves over the few-shot result obtained
by prompting code-davinci-002 \cite{chen2021evaluating}, PaLM-540B \cite{chowdhery2022palm}, and Flan-U-PaLM-540B \cite{chung2022scaling}, with a very similar number of available labeled examples. Moreover, these works directly employ  extremely large PLMs for the task of QA in comparison to \textsc{QAmeleon} that leverages data synthesis to distill PLM into a much smaller mT5-XL model.
% While these works use varied number of examples in the prompt, the prompting methods do not necessarily scale monotonically with the increase of number of examples in the prompt.  
It is important to note that \textsc{QAmeleon} as an approach is orthogonal to other PLM improvements and one could get better QA model by applying \textsc{QAmeleon} (PT) to recent works like Flan-U-PaLM-540B. 

In both English-only and Few-shot resource scenarios,
\textsc{QAmeleon} outperforms the other two data generation approaches, Machine Translation (MT) and Prompt Engineering
(PE). Despite employing PE in two stages, chain-of-thought style, we
observe that the generated data leads to lower QA
performance. Moreover, we observe better performance with using
English-Only data in comparison to the Few-Shot scenario suggesting
that the PLM is able to better utilize high-quality English data
rather than small amounts of labeled data (in other languages).

Finally, augmenting PLM generated data (either via PE or PT) with data
generated via MT leads to gains in QA performance over using any of
these methods independently. This could be due to the coupling of
diverse QA data i.e.,~language-specific content and the task-specific
English-centric translated content.


\begin{table}[t]
    \centering
    \begin{tabular}{l|cc} 
       \textbf{Method}      & \textbf{n-Shot} & \textbf{Avg EM} \\
       \hline \hline
       Baseline     &      ~~5 &   63.7         \\
       \textsc{QAmeleon} (PT)          &     ~~5 &   70.3         \\
       \hline
       Baseline     &     50 &   69.3          \\
       \textsc{QAmeleon} (PT)           &     50 &   73.7         \\
       \hline
       Supervised   &   Multi-k &   78.4         \\ \hline \hline
    \end{tabular}
    \caption{Comparison of QA performance from training mT5-XL on only 5 or 50 examples (Baseline), on synthetic data generated with prompt tuning (PT), or on the full \tydiqagoldp{} training set.
    }
    \label{tab:slr_vs_lr}
\end{table}


 Table~\ref{tab:lang} shows QA performance in individual languages,
 for each of the methods in Table~\ref{tab:lang} in their best
 performing setting: Few-shot Baseline, Machine Translation (MT),
 Prompt Engineering (PE), Prompt Tuning (PT) and augmenting PT with MT. Data generated by
 \textsc{QAmeleon} (PT) using 5 target examples provides the best
 performance across \emph{all} languages. A further boost can be seen
 for most languages when \textsc{QAmeleon} data is combined with MT
 data. Language distribution listed under `\% tokens in PLM'  reflects the extremely low representation of many languages in the pre-training corpora of the PLM used in this work. We additionally show the performance of supervised mT5-XL
 fine-tuned on large amounts of gold training data (see
 Table~\ref{tab:data}) to illustrate the remaining gap, which could
 potentially be bridged by increasing the number of labeled examples or by improved (e.g. more multilingual or FLAN-tuned) PLMs. 
% by future work on better LLMs or improved parameter efficient training methods leading to higher quality synthetic training data. 

\paragraph{Increasing the Target Examples Improves QA Performance} So
far, we have tested \textsc{QAmeleon} in an extremely low resource
setting, using only 5~examples in the target language. We next examine
its performance when we assume a \mbox{10-fold} increase in the number
of annotated examples. Table~\ref{tab:slr_vs_lr} compares the
performance of mT5-XL trained on 5 or 50 examples (Baseline), on
synthetic QA datasets generated by \textsc{QAmeleon} using 5 or 50
examples, and as an upper bound on the entire \tydiqagoldp{}
dataset. As can be seen, increasing the number of examples from 5 to
50 improves performance. It is important to note that in both
settings, significant improvements in multilingual QA can be obtained
by generating data with \textsc{QAmeleon} instead of fine-tuning the
QA model directly on labeled data.

\paragraph{The Larger the Synthetic Data, the Better the QA Model}

\begin{figure}[t]
\centering
\includegraphics[width=\columnwidth]{figures/data_saturation.png}
%\caption{Effect of the size of synthetic data used for mT5-XL QA model fine-tuning via Machine Translation (MT), Prompt Engineering (PE) and combination of MT and PE.}
%\end{figure}
%\begin{figure}
\centering
%\includegraphics[width=\columnwidth]{figures/data_saturation_PT.png}
\vspace*{-.6cm}
\caption{Effect of the size of synthetic data used for mT5-XL QA model
  fine-tuned via Machine Translation (MT), Prompt Engineering (PE),
  Prompt Tuning, i.e., \textsc{QAmeleon} (PT) and combinations of PE + MT and \textsc{QAmeleon} (PT) + MT.  Average EM is reported for the \tydiqagoldp{} eval set.
%\textcolor{red}{TODO: merge Fig. 4 and 5}
}
\label{fig:saturation_pe}
\end{figure}



%The synthetic data sources from different approaches discussed in
%Section \ref{sec:method} are used to train the mT5-XL
%question-answering model.

We now study the impact of varying the size of the automatically
generated datasets on QA performance. As shown in
Figure~\ref{fig:saturation_pe}, when larger amounts of synthetic data
are used for training the QA model, absolute accuracy increases. This
upshot is higher when combining PLM-generated data with Translation
data in comparison to individual datasets. This can be explained due
to the increased diversity of the combined data, which include English-centric translated
content and target language-specific content obtained from the
PLM. Eventually, we observe a saturation effect, i.e.,~beyond O(1000)
QA pairs in the target language improvements are limited.

\begin{table}[t]
    \centering \resizebox{\columnwidth}{!}{
    \begin{tabular}{l|ccc}  
      \multicolumn{1}{c|}{\textbf{Method}}  &  \textbf{n-Shot} & \textbf{Avg BLEU}  &  \textbf{Avg EM} \\
       \hline \hline
       mT5-XXL              &      50 & 31.4  & 73.4    \\
       \textsc{QAmeleon} (PT)   &      50 & 29.0  & 75.6 \\ \hline \hline
    \end{tabular}}
    \caption{BLEU scores and downstream QA performance on
      \tydiqagoldp{} for questions generated by mT5-XXL and
      \textsc{QAmeleon}. (Few-shot setting, 50 examples in
      the target language).
      % Chris: We will address this after Arxiv submission.
      %\ml{still think this table should be mt5-xl and 5 examples} \seb{Would be good to clarify that as this is the first time that mT5-XXL is mentioned and first time that few-shot setting is not 5 examples.} 
      }
    \label{tab:bleu_vs_qa}
\end{table}

% \ml{Table 5 is interesting and showcases the discrepancy between BLEU and the downstream task. However, results should be presented for mt5-XL and 5-shot, otherwise they are not comparable to results from Table 2.} \pri{That is correct, but we have only these results for now. I have added that these are our two best performing models}

\paragraph{BLEU Does not Correlate with  Downstream QA
  Performance} An interesting question is whether improvements in QA
performance are due to better (e.g., more grammatical or
diverse) questions. We assessed the quality of questions generated by
\textsc{QAmeleon} (PT) on \tydiqagoldp{} by measuring their
similarity to gold standard questions. Table~\ref{tab:bleu_vs_qa}
reports BLEU \cite{papineni2002bleu} scores for \textsc{QAmeleon} (PT)
and an mT5-XXL model in the Few-shot setting (50 examples). Even
though \mbox{mT5-XXL} produces questions with higher BLEU score,
\textsc{QAmeleon} generates QA data that leads to higher QA
performance.  The result underscores the need for more
trustworthy automatic evaluation metrics \cite{sellam2020bleurt}
across languages.

\begin{table}[t]
    \centering
    \begin{tabular}{c|cc} 
       \textbf{n-Shot} & \textbf{Prompt Tuning} & \multicolumn{1}{c}{\textbf{Fine-tuning}}
       \\ \hline \hline \hspace*{.2cm}5 & 20.43 & 2.10 \\ 50 & 26.04 & 27.64
       \\ \hline \hline
    \end{tabular}
    \caption{Mean BLEU scores of generated questions on
      \tydiqagoldp{} for prompt tuning and full
      fine-tuning. Results are obtained with a PLM with $\sim$50B
      parameters. n-Shot indicates the number of examples per language
      that the PLM has access to.}
    \label{tab:pt_vs_ft}
\end{table}

% \ml{I think this paragraph, Fine-tuning vs Prompt Tuning, could be moved to the Appendix} 

\paragraph{Full Fine-tuning  in Extremely Low Resource
  Settings is Challenging} Much recent work has explored the
usefulness of prompt tuning and other parameter efficient fine-tuning
methods, such as adapters \cite{houlsby2019parameter} or prefix tuning
\cite{li2021prefix}, in many cases showing that parameter efficient
learning can lead to better performance than fine-tuning of all
parameters \cite{Mahabadi2021compacter,liu2021gpt}. In
Table~\ref{tab:pt_vs_ft} we show BLEU scores of generated questions on
\tydiqagoldp{} for PLMs trained with prompt tuning or
fine-tuning using 5 or 50 labeled examples. While with 50 examples
full fine-tuning appears to be better than prompt tuning, in the
`extremely low resource' regime (5 examples) full fine-tuning
completely fails to converge, suggesting that parameter-efficient
prompt tuning might be of critical importance for obtaining good
results with PLMs when a very small number of data points is
available.

%The evaluation of
%automatically generated questions is an area of active research in the
%language generation field. Automatic metrics such as BLEU
%\cite{papineni2002bleu} can be measured very easily and are widely
%used making them a very appealing choice. In many cases however BLEU
%correlates poorly with the downstream performance of the generation
%model on the tasks of interest or with human judgements on the
%generated text, such as fluency or adequacy, justifying significant
%work on more trustworthy automatic evaluations
%\cite{sellam2020bleurt}. In Table~\ref{tab:bleu_vs_qa} we report BLEU
%score and downstream QA performance for questions generated  with our
%two best performing models, mT5-XXL and \textsc{QAmeleon} (PT). We note
%that indeed BLEU scores are misleading for this task. Even though
%mT5-XXL produces questions with higher BLEU score, \textsc{QAmeleon}
%generates QA datasets that lead to higher QA performance.  

\begin{table}[t]
    \centering
    \resizebox{\columnwidth}{!}{
    \begin{tabular}{l|ccc} 
       \multicolumn{1}{c|}{\textbf{Method}}     & \textbf{Avg EM} & \textbf{Avg F1}\\ \hline
       \hline
       % See https://colab.corp.google.com/drive/1GOviyKFbdKokoZzKsOIxh6B6cFhrgt1y#scrollTo=8j1FSqJnX541
       % These are dev results.
       %   English-Only   & 54.7  & 72.1 \\
       %   MT         &  58.0  & 75.5 \\
       %   \textsc{QAmeleon} (PT) & 57.3 & 74.7 \\
       %   \textsc{QAmeleon} (PT) + MT & \textbf{58.8} & 75.9 \\
       % These are test results.
       English-Only   & 51.9  & 70.8 \\
       MT             &  54.9  & 73.7 \\
       \textsc{QAmeleon} (PT) & 51.7 & 71.6 \\
       \textsc{QAmeleon} (PT) + MT & 57.0 & 75.1 \\
       \hline
       mT5-XL \cite{xue2021mt5} & 54.5 & 73.5 \\
       XLM-E-XL \cite{chi2022xlm} &  57.9 & 76.2
       \\ \hline \hline
    \end{tabular}}
    \caption{Downstream QA performance on the \textsc{MLQA} test set with
       an mT5-XL model trained on \textsc{SQuAD} English data
      (English-Only),  \textsc{SQuAD} translated to all 
      \textsc{MLQA} languages (MT), on synthetic data generated by
      \textsc{QAmeleon} (5-shot) in all  \textsc{MLQA} languages,
      or on a combination of data generated by MT and 
      \textsc{QAmeleon}. We report results for comparison from \newcite{xue2021mt5} and
      \newcite{chi2022xlm} for models of size XL (3.7B and 2.2B parameters respectively), trained on English
      data only.
      % TODO(Chris): \ca{report on test instead.}
      % \seb{Do we also have a comparison with mT5 trained on data generated by a model (mT5 or PaLM) trained on the English SQuAD data? This would be the setup closest to prior work \cite{shakeri-etal-2021-towards,riabi-etal-2021-synthetic}, which zero-shot transfers the QA data generation model from English.} \textcolor{red}{We need the results on test partitions, F1, MT+PT, Add results from SOTA}
    }
    \label{tab:mlqa}
\end{table}

\paragraph{Our Results Generalize to MLQA} To validate the general
applicability of our approach, we evaluate \textsc{QAmeleon} on MLQA
\cite{lewis-etal-2020-mlqa}. We prompt-tune the PLM on 5 examples per
language taken from the MLQA development set, since MLQA does not
provide training partitions. We generate synthetic datasets in all of
the MLQA languages and compare an English-only baseline, MT
and \textsc{QAmeleon} (PT) approaches as we did previously for
\tydiqagoldp{}. We report results (EM and F1) using mT5-XL as the QA model
in Table \ref{tab:mlqa}, where English is included in the average
performance and the best mT5-XL checkpoint is selected per language
based on EM on the dev set. We find that the MT approach is very
effective on MLQA, which is not surprising since MLQA questions are
translated from English. \textsc{QAmeleon} (PT), however, still obtains
a significant improvement in combination with MT synthetic data.

% \section{Ablations}

%\paragraph{Translation Bridge during Prompt Engineering.}

%\paragraph{PLM Model Size.}

% \paragraph{Tuned prompt length.}


\section{Related Work}

\paragraph{Data Generation for QA} Prior work on the generation of QA data
has mostly focused on English and typically divides the task into
answer extraction/generation and question generation, followed by some
type of filtering. \citet{alberti-etal-2019-synthetic} employ
round-trip consistency for filtering with BERT-based models. Other
work \cite{shakeri-etal-2020-end} uses BART to jointly generate a
question and its answer given an input passage, employing
likelihood-based filtering. \citet{lewis-etal-2021-paq} use a
RoBERTa-based passage selection model to identify interesting
passages. \citet{bartolo-etal-2021-improving} additionally train the
generation models on an adversarial QA dataset, while
\citet{yao-etal-2022-ais} integrate a QA-pair ranking module.

The above approaches generally require large amounts of labeled QA
data in the form of \textsc{SQuAD} \cite{rajpurkar-etal-2016-squad} or
Natural Questions \cite{kwiatkowski-etal-2019-natural} to train
passage selection and question generation models. In contrast, we only
assume access to a few question-answer pairs per language.

\paragraph{Multilingual QA} In this work we used mT5-XL \cite{xue2021mt5} as our
 reference QA model.
 We note that a slightly more performant choice could have been ByT5 \cite{xue2022byt5},
 which reports improvements on \tydiqagoldp{} by operating directly on raw text instead
 of sentence pieces.
 Existing work on low resource multilingual QA 
 has been relatively limited. \citet{lee-etal-2018-semi} propose to
 use automatically translated high-confidence QA examples for
 training, while other approaches
 \cite{kumar-etal-2019-cross,chi2020cross} only generate questions and
 require supervised training data in the target language. Other approaches
 \cite{riabi-etal-2021-synthetic,shakeri-etal-2021-towards,kramchaninova-defauw-2022-synthetic}
 focus on zero-shot transfer, i.e.,~a multilingual model trained on
 QA data generation on \textsc{SQuAD} (and optionally automatically
 translated \textsc{SQuAD} data) is applied to other languages. Our
 work shows that few-shot settings result in better multilingual
 generation quality in comparison to zero-shot models.

\paragraph{Prompting} Existing work  \citep[\emph{inter
    alia}]{brown2020language,schick-schutze-2021-just} has shown that
prompting pre-trained large language models can lead to strong
performance in a wide range of tasks including natural language
generation and common sense reasoning. In the context of multilingual
QA, \citet{chowdhery2022palm} employ a single prompt and a few labeled
examples in the target language. In contrast, we employ chain-of-thought prompting, and English answers and questions as a bridge. Moreover, our experiments
with \textsc{QAmeleon} demonstrate that prompt tuning is superior and
a viable alternative to large-scale annotation. Prompting in multilingual settings has achieved the best performance using English prompts and target language exemplars \cite{Winata2021,Lin2022,Shi:ea:2022}. We demonstrate that parameter-efficient methods such as prompt tuning using target language exemplars \cite{lester2021power} is a superior choice.
%\seb{I've added the most relevant related work that I could find.}

\label{sec:bibtex}

\section{Conclusions}

In this work, we examined the ability of pre-trained language models
to generate synthetic data for bootstrapping multilingual QA systems,
with as few as five examples in a new target language. We introduced
\textsc{QAmeleon}, a parameter efficient approach which uses prompt
tuning to automatically create multilingual QA data. Extensive
experiments under different resource scenarios demonstrate that
\textsc{QAmeleon} is superior to prompt engineering and competitive
baselines based on machine translation. In the future, we would like
to extend this approach to other multilingual tasks, including retrieval,
summarization, and semantic parsing.

\section{Contributions}
This paper was the result of close teamwork. Priyanka obtained the main results on \tydiqagoldp{} and proposed focusing the paper on the 5-shot scenario. Chris designed the PE prompts, proposed focusing on downstream QA for evaluation, obtained some MLQA results. Fantine obtained some of the \tydiqagoldp{} and MLQA results, came up with the paper's name, and made code and writing contributions. Joshua performed synthetic data size ablation, and also made code and writing contributions.  Ji proposed using prompt tuning and provided guidance on how to use it. Sebastian wrote the related work section. Kuzman kept us honest on the size of our few-shot training sets. Dipanjan initiated the work and oversaw the entire project. Mirella provided guidance, mentorship, pointers to related work and made substantial writing contributions. Everyone participated extensively in discussions and design.

\section{Acknowledgements}
We are grateful to the Google PaLM team for training PaLM and making this work possible. We wish to also thank Frederick Liu, Tianze Shi and the LLM API \& Use team at Google for providing us with the infrastructure to train and run prompt tuned models. We additionally would like to thank Jon Clark, Cindy Wang and Slav Petrov for valuable feedback.

% Entries for the entire Anthology, followed by custom entries
\bibliography{anthology,custom}

\newpage
\appendix

\section{Data Analysis}
\label{sec:data-analysis}


 Table~\ref{tab:syn_data} shows the size of synthetic data resources
 generated via Prompt Engineering (PE) and \textsc{QAmeleon} (PT), per
 language and in total. As we can see these were in the range of
 47,000-53,000 QA examples for \tydiqagoldp{}, and 89,000 for
 MLQA. The varying size of the data across languages is due to the
 filtering described in Section~\ref{sec:exp}. In some languages
 (e.g., Telugu) generation is more noisy leading to fewer
 data points. We conjecture this is due to the PLM being exposed to
 less data representing these languages during pre-training. We
 further hypothesize that a more multilingual pre-training of PLMs
 could potentially lead to better quality data across all languages.

 Machine translation (MT) creates the same number of data points as
 the source training set. For \tydiqagoldp{}, the English training
 contains~3,696 data points (Table \ref{tab:data}), leading to
 approximately 29,000 QA examples across 8 languages. In MLQA, machine
 translation (MT) uses SQuAD as the English dataset, consisting of
 $\sim$ 87,000 data points, leading to $\sim$ 522,000 QA examples
 across~6 languages.

\begin{table}[t]
    \centering
    \resizebox{\columnwidth}{!}{
    \begin{tabular}{l|cc|c} 
                   & \multicolumn{2}{c|}{\textbf{\tydiqagoldp{}}} & \textbf{MLQA} \\
         \textbf{Language}  & PE & \textsc{QAmeleon} & \textsc{QAmeleon}  \\
         
       \hline \hline
         Arabic & 5,219 & 8,499 & 14,738 \\
         Bengali & 5,948 & 8,036 & ---\\
         Chinese & --- & --- & 14,669 \\
         Finnish & 8,062 & 5,943 & --- \\
         German & --- & --- & 11,186 \\
         Hindi &--- & --- & 12,036 \\
         Indonesian & 6,487 & 7,810 & --- \\
         Kiswahili & 8,003 & 8,041 & --- \\
         Korean & 5,229 & 7,906 & --- \\
         Russian & 5,619 & 7,441 & --- \\
         Spanish & --- & --- & 10,134 \\
         Telugu & 2,742 & 5,222 & ---  \\
         Vietnamese & --- & --- & 13,333 \\
       \hline 
         Total & 47,309 & 52,955 & 89,344 \\ \hline \hline
    \end{tabular}}
    \caption{Number of synthetic question-answer pairs per language
      generated via Prompt Engineering (PE) and \textsc{QAmeleon} (PT)
      with  5 human-labeled examples.
    % \textcolor{red}{todo: add MLQA stats, and can be merged with table1}
    }
    \label{tab:syn_data}
\end{table}


% \begin{table*}[t]
%     \centering
%     \begin{tabular}{c|p{2.5in}|p{2.5in}} 
%         Lang & \multicolumn{1}{c}{Human Annotated} &
%         \multicolumn{1}{c}{\textsc{QAmeleon} (PT)}  \\
%       \hline\hline
%          \textbf{ar} &  \textbf{Q:}
%             
%          \textbf{ A:} 1457 & \\          
%     \hline \hline
%     \end{tabular}
%     \caption{Examples of QA pairs from human-annotated \tydiqa{}
%       and generated by \textsc{QAmeleon} (PT) on corresponding
%       passages. English translations from Google Translate are added for
%       readability.}
%     \label{tab:examples}
% \end{table*}

\begin{table*}[t]
    \centering
    \begin{tabular}{c|p{2.5in}|p{2.5in}} 
        Lang & \multicolumn{1}{c}{Human Annotated} &
        \multicolumn{1}{c}{\textsc{QAmeleon} (PT)}  \\
      \hline\hline
    %   \begin{otherlanguage}{arabic}
    %       (:Albert-Ludwigs-Universitt Freiburg)     1457      .[1][2][3]          .   2007                       . \end{otherlanguage}
         \textbf{ar} &  \textbf{Q:}
         \begin{otherlanguage}{arabic}
         \textRL{   }
         \end{otherlanguage} \textbf{ A:} 1457 &	\textbf{Q:} 
         \begin{otherlanguage}{arabic}
         \textRL{   }
         \end{otherlanguage} \textbf{ A:} 1457 \\
          & \textit{(Q: When was the University of Freiburg established? A: 1457)} & \textit{(Q: When was the University of Freiburg founded? A: 1457)} \\
           \textbf{sw} &
           \textbf{Q:} Je, Kifaru ana urefu kiasi gani? \textbf{A:} "mita 3.5-4.6 & 
            \textbf{Q:} Je , faru mweupe ana uzito gani? \textbf{A:} kilogramu 3,500 \\
           & \textit{(Q: How tall is a Rhino? A: 3.5-4.6 meters)}  & \textit{(Q: How much does a white rhino weigh? A: 3,500 kilograms)}\\
           \textbf{ru} & 
           \textbf{Q:} \begin{otherlanguage}{russian}
               ? 
           \end{otherlanguage}
           \textbf{A:} \begin{otherlanguage}{russian} 24  (4 ) 1783 \end{otherlanguage}
           & \textbf{Q:}
           \begin{otherlanguage}{russian}
                 ?
            \end{otherlanguage}
            \textbf{A:}
           \begin{otherlanguage}{russian}
             ( )
           \end{otherlanguage}
           \\ & 
           \textit{(Q: When was the Treaty of Georgievsk signed? answer: July 24 (August 4), 1783)} & \textit{(Q: In what fortress was the Treaty of St. George concluded? A: Georgievsk (North Caucasus))}  \\
    %   \textbf{bn} & \textbf{Q}:{\foreignlanguage{bengali}{       ?
    %   }}\textbf{ A}: {\foreignlanguage{bengali}{}} & 
        %  	\textbf{Q}: {\foreignlanguage{bengali}{    ?
        %  	}}\textbf{ A:} {\foreignlanguage{bengali}{}}\\
        %  & \textit{(Q: How many countries are members of the International Cricket Council? A:105)} & \textit{(Q: What is the number of ICC member countries? A: 105)
        %  }\\          
    \hline \hline
    \end{tabular}
    \caption{Examples of QA pairs from human-annotated \tydiqa{}
      and generated by \textsc{QAmeleon} (PT) on corresponding
      passages. English translations from Google Translate are added for
      readability.}
    \label{tab:examples}
\end{table*}

\begin{table*}[t]
    \centering
    \begin{tabular}{c|p{5.5in}} 
         Lang  & \multicolumn{1}{c}{
          \textsc{QAmeleon} (PT)} \\
      \hline \hline
      \textbf{fi} & \textbf{Q:}
      \begin{otherlanguage}{finnish}
      Milloin Tullintori valmistui? 
      \end{otherlanguage}
      \textbf{A: }\textit{1990}
      \\
      & \textit{(Q: When was Tullintori completed? A: 1990)}  \\
     \textbf{ru} & \textbf{Q:} \begin{otherlanguage}{russian}
            ?
     \end{otherlanguage}
     \textbf{A:} \begin{otherlanguage}{russian}
        
     \end{otherlanguage}
      \\
          & \textit{(Q: What sector of the economy is predominantly employed by the population of Uzbekistan? A: service and tourism)}
          \\
    \textbf{sw} & \textbf{Q: }Nyoka birisi iko katika familia gani? \textbf{A:} Typhlopidae \\
    & \textit{(Q: Which family is the Birsi snake in? A: Typhlopidae)} \\
    \textbf{ar} &\textbf{Q: }
    \begin{otherlanguage}{arabic}
         \textRL{
             
     } \end{otherlanguage}
     \textbf{A: }\begin{otherlanguage}{arabic}\textRL{} \end{otherlanguage}\\
    & \textit{(Q: In which state is Franklin Township, Manitowoc County? A: Wisconsin)} \\
    \textbf{id}&\textbf{ Q:} Siapa pencipta manga DN Angel?\textbf{ A:} Yukiru Sugisaki \\
    & \textit{(Q: Who created the manga DN Angel? A: Yukiru Sugisaki)}
%       \textbf{ko} & \textbf{Q: }{\foreignlanguage{korean}{  ?}}
%     \textbf{A:} 
%     {\foreignlanguage{korean}{  }}
%     \\
%     &\textit{(Q: What is Spain's official religion?
%     A:
%     Roman Catholic)}
    \\ \hline 
    \hline 
    \end{tabular}
    \caption{QA pairs (random selection) generated by
      \textsc{QAmeleon} (PT) on Wikipedia passages. English
      translations from Google Translate are added for readability.}
    \label{tab:examples_qa}
\end{table*}

\section{Data Examples}

Table \ref{tab:examples} illustrates
randomly picked examples of QA pairs generated by \textsc{QAmeleon}
(PT) for passages in the \tydiqagoldp{} eval set. For these
passages, we also have access to the human annotated QA pairs. As
can be seen, QA pairs generated by \textsc{QAmeleon} are of similar
quality and at times diverse in comparison to the human-annotated dataset.

Table \ref{tab:examples_qa} illustrates
examples of QA pairs generated by \textsc{QAmeleon} from randomly sampled Wikipedia passages. 
%Tables \ref{tab:examples} and \ref{tab:examples_qa} show human annotations from% \tydiqa{} and data generated by \sc{QAmeleon}.

% TODO(Ji): there might actually be improvements from this. Ji might add them if
% there is time.
%   \paragraph{Round-trip Filtering for \textsc{QAmeleon}(PT)} As
% discussed in Section \ref{sec:exp}, we mentioned that in the
% construction of $\mathcal{D}_\mathrm{PE}$, we employ round-trip
% filtering. We study the effect of doing adding a round-trip filtering
% step for prompt tuning i.e. \textsc{QAmeleon}(PT). \pri{maji@ to add
%   details here} 

\end{document}
