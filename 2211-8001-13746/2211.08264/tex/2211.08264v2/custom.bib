% Use this file for citations not found in the ACL Anthology (contained in "anthology.bib").

@inproceedings{lewis2020mlqa,
  title={MLQA: Evaluating Cross-lingual Extractive Question Answering},
  author={Lewis, Patrick and Oguz, Barlas and Rinott, Ruty and Riedel, Sebastian and Schwenk, Holger},
  booktitle={Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics},
  pages={7315--7330},
  year={2020}
}

@article{chowdhery2022palm,
  title={Palm: Scaling language modeling with pathways},
  author={Chowdhery, Aakanksha and Narang, Sharan and Devlin, Jacob and Bosma, Maarten and Mishra, Gaurav and Roberts, Adam and Barham, Paul and Chung, Hyung Won and Sutton, Charles and Gehrmann, Sebastian and others},
  journal={arXiv preprint arXiv:2204.02311},
  year={2022}
}

@inproceedings{lester2021power,
  title={The Power of Scale for Parameter-Efficient Prompt Tuning},
  author={Lester, Brian and Al-Rfou, Rami and Constant, Noah},
  booktitle={Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing},
  pages={3045--3059},
  year={2021}
}

@article{wei2022chain,
  title={Chain of thought prompting elicits reasoning in large language models},
  author={Wei, Jason and Wang, Xuezhi and Schuurmans, Dale and Bosma, Maarten and Chi, Ed and Le, Quoc and Zhou, Denny},
  journal={arXiv preprint arXiv:2201.11903},
  year={2022}
}

@article{brown2020language,
  title={Language models are few-shot learners},
  author={Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and others},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={1877--1901},
  year={2020}
}

@article{min2022rethinking,
  title={Rethinking the Role of Demonstrations: What Makes In-Context Learning Work?},
  author={Min, Sewon and Lyu, Xinxi and Holtzman, Ari and Artetxe, Mikel and Lewis, Mike and Hajishirzi, Hannaneh and Zettlemoyer, Luke},
  journal={arXiv preprint arXiv:2202.12837},
  year={2022}
}

@inproceedings{xue2021mt5,
  title={mT5: A Massively Multilingual Pre-trained Text-to-Text Transformer},
  author={Xue, Linting and Constant, Noah and Roberts, Adam and Kale, Mihir and Al-Rfou, Rami and Siddhant, Aditya and Barua, Aditya and Raffel, Colin},
  booktitle={Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies},
  pages={483--498},
  year={2021}
}

@article{wu2016google,
  title={Google's neural machine translation system: Bridging the gap between human and machine translation},
  author={Wu, Yonghui and Schuster, Mike and Chen, Zhifeng and Le, Quoc V and Norouzi, Mohammad and Macherey, Wolfgang and Krikun, Maxim and Cao, Yuan and Gao, Qin and Macherey, Klaus and others},
  journal={arXiv preprint arXiv:1609.08144},
  year={2016}
}

@inproceedings{shazeer2018adafactor,
  title={Adafactor: Adaptive learning rates with sublinear memory cost},
  author={Shazeer, Noam and Stern, Mitchell},
  booktitle={International Conference on Machine Learning},
  pages={4596--4604},
  year={2018},
  organization={PMLR}
}

@inproceedings{houlsby2019parameter,
  title={Parameter-efficient transfer learning for NLP},
  author={Houlsby, Neil and Giurgiu, Andrei and Jastrzebski, Stanislaw and Morrone, Bruna and De Laroussilhe, Quentin and Gesmundo, Andrea and Attariyan, Mona and Gelly, Sylvain},
  booktitle={International Conference on Machine Learning},
  pages={2790--2799},
  year={2019},
  organization={PMLR}
}

@article{li2021prefix,
  title={Prefix-tuning: Optimizing continuous prompts for generation},
  author={Li, Xiang Lisa and Liang, Percy},
  journal={arXiv preprint arXiv:2101.00190},
  year={2021}
}

@article{liu2021gpt,
  title={GPT understands, too},
  author={Liu, Xiao and Zheng, Yanan and Du, Zhengxiao and Ding, Ming and Qian, Yujie and Yang, Zhilin and Tang, Jie},
  journal={arXiv preprint arXiv:2103.10385},
  year={2021}
}

@inproceedings{papineni2002bleu,
  title={Bleu: a method for automatic evaluation of machine translation},
  author={Papineni, Kishore and Roukos, Salim and Ward, Todd and Zhu, Wei-Jing},
  booktitle={Proceedings of the 40th annual meeting of the Association for Computational Linguistics},
  pages={311--318},
  year={2002}
}

@inproceedings{sellam2020bleurt,
  title={BLEURT: Learning Robust Metrics for Text Generation},
  author={Sellam, Thibault and Das, Dipanjan and Parikh, Ankur},
  booktitle={Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics},
  pages={7881--7892},
  year={2020}
}

@inproceedings{Mahabadi2021compacter,
archivePrefix = {arXiv},
arxivId = {2106.04647},
author = {Mahabadi, Rabeeh Karimi and Henderson, James and Ruder, Sebastian},
booktitle = {Proceedings of NeurIPS 2021},
eprint = {2106.04647},
title = {{Compacter: Efficient Low-Rank Hypercomplex Adapter Layers}},
url = {http://arxiv.org/abs/2106.04647},
year = {2021}
}

@inproceedings{Rajpurkar2016,
author = {Rajpurkar, Pranav and Zhang, Jian and Lopyrev, Konstantin and Liang, Percy},
booktitle = {Proceedings of EMNLP 2016},
eprint = {1606.05250},
file = {:Users/ruder/Library/Application Support/Mendeley Desktop/Downloaded/Rajpurkar et al. - 2016 - SQuAD 100,000 Questions for Machine Comprehension of Text.pdf:pdf},
mendeley-groups = {Deep Learning/QA},
title = {{SQuAD: 100,000+ Questions for Machine Comprehension of Text}},
year = {2016}
}

@inproceedings{chi2020cross,
  title={Cross-lingual natural language generation via pre-training},
  author={Chi, Zewen and Dong, Li and Wei, Furu and Wang, Wenhui and Mao, Xian-Ling and Huang, Heyan},
  booktitle={Proceedings of the AAAI conference on artificial intelligence},
  volume={34},
  pages={7570--7577},
  year={2020}
}

@inproceedings{kramchaninova-defauw-2022-synthetic,
    title = "Synthetic Data Generation for Multilingual Domain-Adaptable Question Answering Systems",
    author = "Kramchaninova, Alina  and
      Defauw, Arne",
    booktitle = "Proceedings of the 23rd Annual Conference of the European Association for Machine Translation",
    month = jun,
    year = "2022",
    address = "Ghent, Belgium",
    publisher = "European Association for Machine Translation",
    url = "https://aclanthology.org/2022.eamt-1.18",
    pages = "151--160",
    abstract = "Deep learning models have significantly advanced the state of the art of question answering systems. However, the majority of datasets available for training such models have been annotated by humans, are open-domain, and are composed primarily in English. To deal with these limitations, we introduce a pipeline that creates synthetic data from natural text. To illustrate the domain-adaptability of our approach, as well as its multilingual potential, we use our pipeline to obtain synthetic data in English and Dutch. We combine the synthetic data with non-synthetic data (SQuAD 2.0) and evaluate multilingual BERT models on the question answering task. Models trained with synthetically augmented data demonstrate a clear improvement in performance when evaluated on the domain-specific test set, compared to the models trained exclusively on SQuAD 2.0. We expect our work to be beneficial for training domain-specific question-answering systems when the amount of available data is limited.",
}

@inproceedings{Lin2019,
abstract = {Cross-lingual transfer, where a high-resource transfer language is used to improve the accuracy of a low-resource task language, is now an invaluable tool for improving performance of natural language processing (NLP) on low-resource languages. However, given a particular task language, it is not clear which language to transfer from, and the standard strategy is to select languages based on ad hoc criteria, usually the intuition of the experimenter. Since a large number of features contribute to the success of cross-lingual transfer (including phylogenetic similarity, typological properties, lexical overlap, or size of available data), even the most enlightened experimenter rarely considers all these factors for the particular task at hand. In this paper, we consider this task of automatically selecting optimal transfer languages as a ranking problem, and build models that consider the aforementioned features to perform this prediction. In experiments on representative NLP tasks, we demonstrate that our model predicts good transfer languages much better than ad hoc baselines considering single features in isolation, and glean insights on what features are most informative for each different NLP tasks, which may inform future ad hoc selection even without use of our method. Code, data, and pre-trained models are available at https://github.com/neulab/langrank},
annote = {- propose a ranking approach to select the best source language for cross-lingual transfer},
archivePrefix = {arXiv},
arxivId = {1905.12688},
author = {Lin, Yu-Hsiang and Chen, Chian-Yu and Lee, Jean and Li, Zirui and Zhang, Yuyan and Xia, Mengzhou and Rijhwani, Shruti and He, Junxian and Zhang, Zhisong and Ma, Xuezhe and Anastasopoulos, Antonios and Littell, Patrick and Neubig, Graham},
booktitle = {Proceedings of ACL 2019},
eprint = {1905.12688},
file = {:Users/ruder/Library/Application Support/Mendeley Desktop/Downloaded/Lin et al. - 2019 - Choosing Transfer Languages for Cross-Lingual Learning.pdf:pdf},
mendeley-groups = {Adaptation/Data selection,Adaptation/Cross-lingual Adaptation,Venues/ACL 2019},
title = {{Choosing Transfer Languages for Cross-Lingual Learning}},
url = {http://arxiv.org/abs/1905.12688},
year = {2019}
}

@misc{vu-2022-zeroshot,
  doi = {10.48550/ARXIV.2205.12647},
  
  url = {https://arxiv.org/abs/2205.12647},
  
  author = {Vu, Tu and Barua, Aditya and Lester, Brian and Cer, Daniel and Iyyer, Mohit and Constant, Noah},
  
  keywords = {Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Overcoming Catastrophic Forgetting in Zero-Shot Cross-Lingual Generation},
  
  publisher = {arXiv},
  
  year = {2022},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}

% Use this file for citations not found in the ACL Anthology (contained in "anthology.bib").

@article{min2022rethinking,
  title={Rethinking the Role of Demonstrations: What Makes In-Context Learning Work?},
  author={Min, Sewon and Lyu, Xinxi and Holtzman, Ari and Artetxe, Mikel and Lewis, Mike and Hajishirzi, Hannaneh and Zettlemoyer, Luke},
  journal={arXiv preprint arXiv:2202.12837},
  year={2022}
}


@Article{Raffel:ea:2020,
  author = 	 {Colin Raffel and Noam Shazeer and Adam Roberts and Katherin Lee and Sharan Narang and Michael Matena and Yanqi Zhou and Wei Li and Peter J Liu},
  title = 	 {Exploring the Limits of Transfer Learning with a Unified Text-to-text Transformer},
  journal = 	 {Journal of machine Learning Research},
  year = 	 2020,
  volume = 	 21,
  number = 	 140,
  pages = 	 {1--67}}

@misc{Sherborne:Lapata:2022,
  doi = {10.48550/ARXIV.2209.12577},
  
  url = {https://arxiv.org/abs/2209.12577},
  
  author = {Sherborne, Tom and Lapata, Mirella},
  
  keywords = {Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Meta-Learning a Cross-lingual Manifold for Semantic Parsing},
  
  publisher = {arXiv},
  
  year = {2022},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@misc{Tang:ea:2022,
  doi = {10.48550/ARXIV.2201.08670},
  
  url = {https://arxiv.org/abs/2201.08670},
  
  author = {Tang, Tianyi and Li, Junyi and Zhao, Wayne Xin and Wen, Ji-Rong},
  
  keywords = {Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Context-Tuning: Learning Contextualized Prompts for Natural Language Generation},
  
  publisher = {arXiv},
  
  year = {2022},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}
@misc{Yang:ea:2022,
  doi = {10.48550/ARXIV.2210.06774},
  
  url = {https://arxiv.org/abs/2210.06774},
  
  author = {Yang, Kevin and Tian, Yuandong and Peng, Nanyun and Klein, Dan},
  
  keywords = {Computation and Language (cs.CL), Artificial Intelligence (cs.AI), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Re3: Generating Longer Stories With Recursive Reprompting and Revision},
  
  publisher = {arXiv},
  
  year = {2022},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@misc{Shi:ea:2022,
  doi = {10.48550/ARXIV.2210.03057},
  
  url = {https://arxiv.org/abs/2210.03057},
  
  author = {Shi, Freda and Suzgun, Mirac and Freitag, Markus and Wang, Xuezhi and Srivats, Suraj and Vosoughi, Soroush and Chung, Hyung Won and Tay, Yi and Ruder, Sebastian and Zhou, Denny and Das, Dipanjan and Wei, Jason},
  
  keywords = {Computation and Language (cs.CL), Artificial Intelligence (cs.AI), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Language Models are Multilingual Chain-of-Thought Reasoners},
  
  publisher = {arXiv},
  
  year = {2022},
  
  copyright = {Creative Commons Attribution 4.0 International}
}

@article{dai2022promptagator,
  title={Promptagator: Few-shot Dense Retrieval From 8 Examples},
  author={Dai, Zhuyun and Zhao, Vincent Y and Ma, Ji and Luan, Yi and Ni, Jianmo and Lu, Jing and Bakalov, Anton and Guu, Kelvin and Hall, Keith B and Chang, Ming-Wei},
  journal={arXiv preprint arXiv:2209.11755},
  year={2022}
}

@misc{Wikiextractor2015,
  author = {Giusepppe Attardi},
  title = {WikiExtractor},
  year = {2015},
  publisher = {GitHub},
  journal = {GitHub repository},
  howpublished = {\url{https://github.com/attardi/wikiextractor}}
}

@inproceedings{chi2022xlm,
  title={XLM-E: Cross-lingual Language Model Pre-training via ELECTRA},
  author={Chi, Zewen and Huang, Shaohan and Dong, Li and Ma, Shuming and Zheng, Bo and Singhal, Saksham and Bajaj, Payal and Song, Xia and Mao, Xian-Ling and Huang, He-Yan and others},
  booktitle={Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  pages={6170--6182},
  year={2022}
}

@inproceedings{Winata2021,
abstract = {General-purpose language models have demonstrated impressive capabilities, performing on par with state-of-the-art approaches on a range of downstream natural language processing (NLP) tasks and benchmarks when inferring instructions from very few examples. Here, we evaluate the multilingual skills of the GPT and T5 models in conducting multi-class classification on non-English languages without any parameter updates. We show that, given a few English examples as context, pre-trained language models can predict not only English test samples but also non-English ones. Finally, we find the in-context few-shot cross-lingual prediction results of language models are significantly better than random prediction, and they are competitive compared to the existing state-of-the-art cross-lingual models.},
annote = {- evaluate how well GPT and T5 models can perform multilingual few-shot learning on intent detection in English, French, German, and Spanish
- find given a few English exampels as context, models are able to predict non-English samples},
archivePrefix = {arXiv},
arxivId = {2109.07684},
author = {Winata, Genta Indra and Madotto, Andrea and Lin, Zhaojiang and Liu, Rosanne and Yosinski, Jason and Fung, Pascale},
booktitle = {Proceedings ofthe 1st Workshop on Multilingual Representation Learning},
eprint = {2109.07684},
file = {:Users/ruder/Library/Application Support/Mendeley Desktop/Downloaded/Winata et al. - 2021 - Language Models are Few-shot Multilingual Learners.pdf:pdf},
mendeley-groups = {Adaptation/One Shot Learning,Adaptation/Cross-lingual Adaptation},
title = {{Language Models are Few-shot Multilingual Learners}},
url = {http://arxiv.org/abs/2109.07684},
year = {2021}
}

@inproceedings{Lin2022,
abstract = {Large-scale autoregressive language models such as GPT-3 are few-shot learners that can perform a wide range of language tasks without fine-tuning. While these models are known to be able to jointly represent many different languages, their training data is dominated by English, potentially limiting their cross-lingual generalization. In this work, we train multilingual autoregressive language models on a balanced corpus covering a diverse set of languages, and study their few- and zero-shot learning capabilities in a wide range of tasks. Our largest model with 7.5 billion parameters sets new state of the art in few-shot learning in more than 20 representative languages, outperforming GPT-3 of comparable size in multilingual commonsense reasoning (with +7.4% absolute accuracy improvement in 0-shot settings and +9.4% in 4-shot settings) and natural language inference (+5.4% in each of 0-shot and 4-shot settings). On the FLORES-101 machine translation benchmark, our model outperforms GPT-3 on 171 out of 182 translation directions with 32 training examples, while surpassing the official supervised baseline in 45 directions. We present a detailed analysis of where the model succeeds and fails, showing in particular that it enables cross-lingual in-context learning on some tasks, while there is still room for improvement on surface form robustness and adaptation to tasks that do not have a natural cloze form. Finally, we evaluate our models in social value tasks such as hate speech detection in five languages and find it has limitations similar to comparable sized GPT-3 models.},
annote = {- train GPT-3 style models on a corpus of 30 typologically diverse languages},
archivePrefix = {arXiv},
arxivId = {2112.10668},
author = {Lin, Xi Victoria and Mihaylov, Todor and Artetxe, Mikel and Wang, Tianlu and Chen, Shuohui and Simig, Daniel and Ott, Myle and Goyal, Naman and Bhosale, Shruti and Du, Jingfei and Pasunuru, Ramakanth and Shleifer, Sam and Koura, Punit Singh and Chaudhary, Vishrav and O'Horo, Brian and Wang, Jeff and Zettlemoyer, Luke and Kozareva, Zornitsa and Diab, Mona and Stoyanov, Veselin and Li, Xian},
booktitle = {Proceedings of EMNLP 2022},
eprint = {2112.10668},
file = {:Users/ruder/Library/Application Support/Mendeley Desktop/Downloaded/Lin et al. - 2021 - Few-shot Learning with Multilingual Language Models.pdf:pdf},
mendeley-groups = {Adaptation/Cross-lingual Adaptation},
title = {{Few-shot Learning with Multilingual Language Models}},
url = {http://arxiv.org/abs/2112.10668},
year = {2022}
}

@article{bajaj2016ms,
  title={Ms marco: A human generated machine reading comprehension dataset},
  author={Bajaj, Payal and Campos, Daniel and Craswell, Nick and Deng, Li and Gao, Jianfeng and Liu, Xiaodong and Majumder, Rangan and McNamara, Andrew and Mitra, Bhaskar and Nguyen, Tri and others},
  journal={arXiv preprint arXiv:1611.09268},
  year={2016}
}

@article{xue2022byt5,
  title={Byt5: Towards a token-free future with pre-trained byte-to-byte models},
  author={Xue, Linting and Barua, Aditya and Constant, Noah and Al-Rfou, Rami and Narang, Sharan and Kale, Mihir and Roberts, Adam and Raffel, Colin},
  journal={Transactions of the Association for Computational Linguistics},
  volume={10},
  pages={291--306},
  year={2022},
  publisher={MIT Press}
}

@article{chung2022scaling,
  title={Scaling Instruction-Finetuned Language Models},
  author={Chung, Hyung Won and Hou, Le and Longpre, Shayne and Zoph, Barret and Tay, Yi and Fedus, William and Li, Eric and Wang, Xuezhi and Dehghani, Mostafa and Brahma, Siddhartha and others},
  journal={arXiv preprint arXiv:2210.11416},
  year={2022}
}

@article{ouyang2022training,
  title={Training language models to follow instructions with human feedback},
  author={Ouyang, Long and Wu, Jeff and Jiang, Xu and Almeida, Diogo and Wainwright, Carroll L and Mishkin, Pamela and Zhang, Chong and Agarwal, Sandhini and Slama, Katarina and Ray, Alex and others},
  journal={arXiv preprint arXiv:2203.02155},
  year={2022}
}

@article{chen2021evaluating,
  title={Evaluating large language models trained on code},
  author={Chen, Mark and Tworek, Jerry and Jun, Heewoo and Yuan, Qiming and Pinto, Henrique Ponde de Oliveira and Kaplan, Jared and Edwards, Harri and Burda, Yuri and Joseph, Nicholas and Brockman, Greg and others},
  journal={arXiv preprint arXiv:2107.03374},
  year={2021}
}