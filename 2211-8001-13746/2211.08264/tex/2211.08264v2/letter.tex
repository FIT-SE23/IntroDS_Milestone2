\documentclass[11pt]{letter}
\usepackage{letterbib}
\usepackage{times}
\usepackage[margin=0.8in]{geometry}
\usepackage[T1]{fontenc}
\usepackage{hyperref}
\usepackage{CJKutf8}

% Models

\newcommand{\tv}{\textsc{T5}}
\newcommand{\longtv}{\textsc{LongT5}}
\newcommand{\etoe}{\textsc{E2E}}
\newcommand{\aplan}{\textsc{Multitask}}
\newcommand{\iterative}{\textsc{Iterative}}
\newcommand{\twostage}{\textsc{2-Stage}}


\PassOptionsToPackage{hyphens}{url}
\signature{Anonymous Paper Authors}


\begin{document}


\begin{letter}{\textbf{Title}: \textsc{QAmeleon}: Multilingual QA with
Only 5 Examples \\
\textbf{Original Submission Number:}~4751\\
\textbf{Original Action Editor:}~Lidong Bing
}

\opening{Dear TACL Editorial Staff,}




%We received a score of a C for this submission with
%allowance for two extra pages for additional content upon
%resubmission. Please find enclosed a new submission building upon this
%prior submission with new revisions and improvements. This new
%submission is twelve pages long as permitted by the Action Editor of
%prior submission 4083 (see enclosed letter).

%We thank you for your consideration of this new submission and sincerely hope that this superior manuscript will be well suited for TACL pending new reviews. We now address the comments from the action editor which requested specific areas for improvement of the manuscript:
%
%\begin{quote}\it
%    From the editor:
%
%    In particular, the analysis aspect of the paper should be improved - in the current version, it is not clear what makes meta-learning good for this problem (and better than the baselines), and in what situations it can lead to better performance.
%\end{quote}
%
%We have dedicated most of our additional space to

We wish to thank the action editor, Lidong Bing, and the reviewers
for their detailed and constructive feedback. We are glad that reviewers found our work `\textit{worthy of being published in TACL}'. 
The revised manuscript
addresses their concerns by implementing the following changes:
% In particular, we now present results of human evaluation across datasets
% (the original submission reported results on SummScreen only), we have
% also implemented a two-stage pipeline system (as recommended by Reviewer A)
% and provided more motivation about various model design decisions and
% the use of entailment as an automatic metric for assessing summary
% factuality.
%In particular, our main changes are the following:
\begin{itemize}
    \item we report additional results with four few-shot scenarios in Table~4  (1, 5, 50, and 100 examples), and study the performance of the QA model with even fewer and additional examples,
    \item we provide analysis of the questions generated by our method in Figure~3 and Section~5. We compare the distribution of question types on synthetic data and \textsc{TyDiQA-GoldP},
    \item we further provide details on the data release of PLM generated question and answers in Section~8,
    \item we elaborate more on the comparisons with existing methods in Section~7,
    \item we made edits and corrections w.r.t. the suggestions made by the reviewers.
    
%       \item we have tidied up the human evaluation, we have added more details on experimental design, sample size, and instructions to the annotators.
%       We have repeated all statistical significance analyses using the
% Friedman Test which is a non-parametric alternative to the Repeated
% Measures ANOVA. To compute significance, we use the post-hoc Wilcoxon
% signed-rank test, Bonferroni corrected for multiple comparisons.
% \item We have added more examples, to illustrate the various types of
% controllability the blueprint models enable.
\end{itemize}


Below we also address specific comments and questions from prior
reviews and explain how we have addressed these areas to improve our
work.

Thank you for consideration of this new submission,

Authors of TACL 4751 and new submission.

\newpage

\textbf{Response to Reviewers:}

 \begin{quote}\it  {\bf Reviewer~A:} The work has the following weakness:
 - the key methodological innovation is a small variation of existing work.
\end{quote}
Section 6 includes a detailed comparison of our method against existing work.
We are not aware of any additional references, however, we are happy to
incorporate concrete suggestions from Reviewer~A. 

\begin{quote}\it  {\bf Reviewer~C:}
Mostly solid bibliography and comparison, but there are a few additional
references that should be included. Discussion of benefits and limitations
is acceptable but not enlightening.
\end{quote}
We now include further discussion on the benefits and limitations of our
approach in Section~7. 
We are happy to include suggestions from Reviewer~C on additional references we
might have missed. 

\begin{quote}\it  {\bf Reviewer~C:}
Please provide more insight into the advantages and disadvantages of the
proposed model, as well as its limitations.
\end{quote} 
We now provide a discussion on the advantages and disadvantages of the proposed model (see Section~7). 

\begin{quote}\it  {\bf Reviewer~C:}
Explain how the proposed method differs from other existing QA methods
using a few examples and from unsupervised question-answering. 
\end{quote} 
We have added clarification in Section~7. As explained in Section~7, to our knowledge, existing methods for low-resource multilingual QA either fine-tune QA models on translated data, or directly on a small number of exemplars. We are not aware of methods that do data augmentation through PLMs for this purpose, particularly through prompt tuning.

\begin{quote}\it  {\bf Reviewer~C:}
Also, provide
a justification for using only five samples. 
\end{quote}
We now present additional experiments with \{1, 5, 50, 100\} samples to examine 
how the method
performs with even fewer as well as relatively more annotations. 

\begin{quote}\it  {\bf Reviewer~C:}
I also want to know the capacity of the proposed model to deal with
different types of questions. For example, is this possible for tackling
with multiple hop QA? How can it deal with short questions and complex
questions? Why QA and Factoid QA?
\end{quote}
The proposed methodology could in principle handle any type of questions (e.g., multihop). Our 
experiments, however, are limited by the type of questions available in  \textsc{TyDiQA} and \textsc{MLQA}, which focus on factoid QA. 
Our approach is general could be extended to many other types of QA tasks or even more generally to other NLP tasks, however, we leave
this to future work. We have also added an analysis showing the distribution of synthetic questions
generated by \textsc{QAmeleon} for individual languages and overall (see Figure~3), to give the reader an idea of the type of questions our approach generates.

\begin{quote}\it  {\bf Reviewer~C:}
The authors used the mT5-XL model, how about using other pre-trained
models, such as BART and GPT-3? In addition, I would like to hear your
opinion about InstructionGPT and the use of the QA model in ChatGPT - a
discussion of the proposed model with the recent advantaged technology on
QA, like the use in ChatGPT would be interesting.
\end{quote}
The use of instruction tuned models for generating synthetic data could indeed be a
topic for interesting future work. We added a mention of this in Section 7. 
From the perspective of this work, BART is a similar model to mT5 and we would not expect it to perform substantially differently from mT5. The use of GPT-3 (175 billion parameters) in place of mT5-XL (3.7B) in our experiments would be too expensive to perform directly and would likely require a different experimental design.

\begin{quote}\it  {\bf Reviewer~C:}
Last but not least, if the paper is accepted, I would like to ask the
authors to share their data and code if it is possible.
\end{quote}
We added Section~8 with statistics on the data release that will accompany this paper. We additionally plan to release the fine-tuned mT5 models. As Reviewer~C is likely aware, TACL rules do not allow submission of supplementary material or even links to supplementary material, anonymized or not.

\begin{quote}\it  {\bf Reviewer-E:}
On line 537: "Baseline mT5-XL is trained on the English QA..."
- does trained here mean "fine-tuned"?
\end{quote}
We have amended the manuscript, it is indeed fine-tuned. 

\begin{quote}\it  {\bf Reviewer~E:}
In the paragraph starting at line 545, are the number of fine-tuning
steps (5000) identical for all experimental configurations?
\end{quote}
We have updated the the manuscript, the fine-tuning step is 3,000 for \textsc{TyDiQA} and 5,000~for 
\textsc{MLQA}. 

\begin{quote}\it  {\bf Reviewer~E:}
For the description of prompt tuning starting at line 353, what are the
"first m tokens" whose embeddings are changed? What is m? The text says to
check Figure 1, but the diagrams there are too high-level to clearly
indicate what's happening (just a box that says QAmeleon).
\end{quote}
We updated the the manuscript to make this part more clear: $m$ is the length of the soft prompt prepended to the input of the PLM. In more precise terms, the soft prompt is a matrix of size $m$ by $d$, where $d$ is the dimensionality of sentence piece embeddings in the PLM and $m$ is a hyperparameter, always set to 50 in this work.

\begin{quote}\it  {\bf Reviewer~E:}
Is there a need to anonymize the PLM reference on line 474 (the actual
model used for data generation)? It seems OK to simply cite a model/paper as
long as there's no statement indicating that it shares an author with the
present work. Is the model highly proprietary? If so, that would be relevant
for reproducibility of the results.
\end{quote} 
The PLM we use has limited access and on-demand availability externally. 
For replicability, we will share the synthetic data which can be used to train
QA models. We have also shared the details and the setup of prompt-tuning is
transferable to PLMs like GPT and OPT. Due to high-computational requirements
and unavailability of parameter efficient methods of these PLMs we do not
include results from other PLMs. Our main goal is not to compare PLMs but
present a recipe that works well with any PLM.

\begin{quote}\it  {\bf Reviewer~E:}
Finally, there seems to be a big improvement gained by going from 5-shot, to
50-shot prompt tuning (Table 4). Figure 2 shows a saturation point for the
amount of generated data used for fine-tuning. Would a similar saturation
point be expected when increasing n, for n-shot prompt tuning? Why stop at
50? Would it be possible to close nearly the entire gap compared to the
fully-supervised upper bound while still using only a fraction of the data?
\end{quote} 
We thank Reviewer~E for this suggestion. We did run a 100-shot result and added
the results to our paper. There appear to be diminishing returns and
perhaps further research challenges in closing the gap entirely with the fully
supervised upper bound.

\end{letter}


\end{document}
