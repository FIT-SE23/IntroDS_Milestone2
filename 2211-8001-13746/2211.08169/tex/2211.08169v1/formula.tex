\documentclass[akbc,twoside,11pt,lettersize]{article}
\begin{document}

\textbf{Definition 1 (Temporal Knowledge Graph (TKG)).} Let $\mathcal{E}$, $\mathcal{R}$ and $\mathcal{T}$ denote a finite set of entities, relations and timestamps, respectively. A temporal knowledge graph (TKG) $\mathcal{G}$ can be taken as a finite set of TKG facts represented by their associated quadruples, i.e., $\mathcal{G} = \{(s,r,o,t)|s,o \in \mathcal{E}, r \in \mathcal{R}, t \in \mathcal{T}\} \subseteq \mathcal{E} \times \mathcal{R} \times \mathcal{E} \times \mathcal{T}$.

\textbf{Definition 2 (Temporal Knowledge Graph Few-Shot Out-of-Graph Link Prediction).} Given an observed background TKG $\mathcal{G}_{\text{back}} \subseteq \mathcal{E}_{\text{back}} \times \mathcal{R} \times \mathcal{E}_{\text{back}} \times \mathcal{T}$, an unseen entity $e'$ is an entity $e' \in \mathcal{E}'$, where $\mathcal{E}' \cap \mathcal{E}_{\text{back}} = \emptyset$. Assume we further observe $K$ associated quadruples for each unseen entity $e'$ in the form of $(e', r, \Tilde{e}, t)$ (or $(\Tilde{e}, r, e', t)$), where $\Tilde{e} \in (\mathcal{E}_{\text{back}} \cup \mathcal{E}')$, $r \in \mathcal{R}$, $t \in \mathcal{T}$, and $K$ is a small number denoting the shot size, e.g., 1 or 3. TKG few-shot out-of-graph link prediction aims to predict the missing entities from the link prediction queries $(e', r_q, ?, t_q)$ (or $(?, r_q, e', t_q)$) derived from unobserved quadruples containing unseen entities, where $r_q \in \mathcal{R}$, $t_q \in \mathcal{T}$.

We further formulate the TKG few-shot OOG link prediction task into a meta-learning problem. For a TKG $\mathcal{G} \subseteq \mathcal{E} \times \mathcal{R} \times \mathcal{E} \times \mathcal{T}$, we first select a group of entities $\mathcal{E}'$, where each entity's number of associated quadruples is between a lower and a higher threshold. We aim to pick out the entities that are not frequently mentioned in TKG facts since newly-emerged entities normally are coupled with only several edges. We randomly split these entities into three groups $\mathcal{E}'_{\text{meta-train}}$, $\mathcal{E}'_{\text{meta-valid}}$ and $\mathcal{E}'_{\text{meta-test}}$. For each group, we treat the union of all the quadruples associated to this group's entities as the corresponding meta-learning set, e.g., the meta-training set $\mathbb{T}_{\text{meta-train}}$ is formulated as $\{(e',r, \Tilde{e},t)|\Tilde{e} \in \mathcal{E}, r \in \mathcal{R}, e' \in \mathcal{E}'_{\text{meta-train}}, t \in \mathcal{T}\} \cup \{(\Tilde{e},r, e',t)|\Tilde{e} \in \mathcal{E}, r \in \mathcal{R}, e' \in \mathcal{E}'_{\text{meta-train}}, t \in \mathcal{T}\}$. We ensure that there exists no link between every two of the meta-learning sets. The associated quadruples of the rest entities form a background graph $\mathcal{G}_{\text{back}} \subseteq \mathcal{E}_{\text{back}} \times \mathcal{R} \times \mathcal{E}_{\text{back}} \times \mathcal{T}$, where $\mathcal{E}' \cap \mathcal{E}_{\text{back}} = \emptyset$ and $\mathcal{E} = (\mathcal{E}_{\text{back}} \cup \mathcal{E}')$. We take the meta-training entities $\mathcal{E}'_{\text{meta-train}}$ as simulated unseen entities and try to learn how to transfer knowledge from seen entities $\mathcal{E}_{\text{back}}$ to them during meta-training. The entities in $\mathcal{E}'_{\text{meta-valid}}$ and $\mathcal{E}'_{\text{meta-test}}$ are real unseen entities that are used to evaluate the model performance.

Based on \cite{DBLP:conf/nips/BaekLH20}, we define a meta-training task $T$ as follows. In each task $T$, we first randomly sample $N$ simulated unseen entities $\mathcal{E}_{T}$ from $\mathcal{E}'_{\text{meta-train}}$. Then we randomly select $K$ associated quadruples for each $e' \in \mathcal{E}_{T}$ as its support quadruples $\mathcal{S}_{e'} = \{(e',r_i,\Tilde{e}_i,t_i) \ \text{or}\  (\Tilde{e}_i,r_i,e',t_i)\}^K_{i=1}$, where $K$ is the shot size and $\Tilde{e}_i \in (\mathcal{E}_{\text{back}} \cup \mathcal{E}')$. The rest of $e'$'s quadruples are taken as its query quadruples $\mathcal{Q}_{e'} = \{(e',r_i,\Tilde{e}_i,t_i) \ \text{or}\  (\Tilde{e}_i,r_i,e',t_i)\}^{M_{e'}}_{i=K+1}$, where $M_{e'}$ denotes the number of $e'$'s associated quadruples in $\mathbb{T}_{\text{meta-train}}$ and $\Tilde{e}_i \in (\mathcal{E}_{\text{back}} \cup \mathcal{E}')$. For every meta-training task $T$, the aim of TKG few-shot OOG link prediction is to simultaneously predict the missing entities from the link prediction queries derived from the query quadruples associated to all the entities from $\mathcal{E}_T$, e.g., $(e', r_i, ?, t_i)$ or $(?, r_i, e', t_i)$. In this way, we simulate the situation that we simultaneously observe a bunch of unseen entities and each of them has only few edges, which is similar to how emerging entities appear in temporal knowledge bases. After meta-training, we validate our model on a meta-validation set $\mathbb{T}_{\text{meta-valid}}$ and test our model on a meta-test set $\mathbb{T}_{\text{meta-test}}$, where they contain all the quadruples associated to the entities in $\mathcal{E}'_{\text{meta-valid}}$ and $\mathcal{E}'_{\text{meta-test}}$, respectively. We do not sample $N$ entities during meta-validation and meta-test. Instead, we treat all the entities in $\mathcal{E}'_{\text{meta-valid}}$ (or $\mathcal{E}'_{\text{meta-test}}$) as appearing at the same time. For a better understanding, we present Figure \ref{fig: meta-learning} to illustrate how we formulate the TKG few-shot OOG link prediction task into a meta-learning problem. We also discuss the difference between our proposed task and traditional TKGC in Appendix



\begin{equation}
    \mathbf{h}_{c} = \frac{1}{|\mathcal{N}_c|}\sum_{e \in \mathcal{N}_c} \mathbf{h}_{e},
\end{equation}


\begin{equation}
     \mathbf{h}_{c} =  \sum_{e_i \in \mathcal{N}_c} \alpha_c^{e_i} \mathbf{h}_{e_i}, \quad\alpha_c^{e_i} = \frac{\exp(\mathbf{h}^{\top}_{e_i}\mathbf{h}_{c})}{\sum_{e_j \in \mathcal{N}_c}\exp(\mathbf{h}^{\top}_{e_j}\mathbf{h}_c)}.
\end{equation}

\begin{aligned}
     \mathbf{h}_{e}^{\mathcal{C}_e} =  \sum_{c_i \in \mathcal{C}_e} \beta_e^{c_i} \mathbf{h}_{c_i}, \quad\beta_e^{c_i} = \frac{\exp(\mathbf{h}^{\top}_{c_i}\mathbf{h}_{e})}{\sum_{c_j \in \mathcal{C}_e}\exp(\mathbf{h}^{\top}_{c_j}\mathbf{h}_e)}.
\end{aligned}

$\mathcal{C}_e \subseteq \mathcal{C}$ denotes the set of all concepts associated to $e$. As shown in Figure \ref{fig: model structure}, we inject the concept-aware information into two branches. We use two separate layers of feed forward neural network and project the concept-aware information onto two branches. The upper branch adds the concept information to the entity representations $\mathbf{h}_e := \mathbf{h}_e + \delta_1 \sigma(\mathbf{W}_c^1\mathbf{h}_{e}^{\mathcal{C}_e})$ and take them as the input of our graph encoder. The lower branch processes the concept information $\delta_2 \sigma(\mathbf{W}_c^2\mathbf{h}_{e}^{\mathcal{C}_e})$ and adds it to the entity representations after the graph aggregation step. $\delta_1$ and $\delta_2$ are two trainable weights deciding how much concept-aware information should be injected. $\mathbf{W}_c^1$ and $\mathbf{W}_c^2$ are two weight matrices and $\sigma$ is an activation function. By employing the double branch structure, we not only include the concept information into the graph encoder, but also directly infuse it into the final entity representations for link prediction.

\begin{equation}
\begin{aligned}
     \mathbf{h}_{(e', t_q)} = \sum_{(\Tilde{e}_i, r_i, t_i) \in \mathcal{N}_{e'}} \gamma_{q}^i  \mathbf{W_g}(\mathbf{h}_{\Tilde{e}_i} \|\mathbf{h}_{r_i}), \quad
     \gamma_{q}^i = \frac{\exp(1 / {|t_q - t_i|})}{\sum_{(\Tilde{e}_j, r_j, t_j)\in \mathcal{N}_{e'}} \exp (1 / |t_q - t_j|)}.
\end{aligned}
\end{equation}
$\mathbf{W}_g$ denotes the weight matrix in our graph encoder. $\mathcal{N}_{e'}$ denotes the observed neighborhood of $e'$ and $|\mathcal{N}_{e'}| = K$. $\gamma_{q}^i$ is the importance of the $i$th temporal neighbor $\Tilde{e}_i$ based on the time difference between $t_q$ and $t_i$. The smaller the time difference is, the more important a temporal neighbor is during aggregation. The motivation of our time difference-based graph encoder is that we assume the temporal neighbors that are temporally closer to the query timestamp $t_q$ tend to contribute more to predicting the links at $t_q$. Since we take the temporal neighbors of an entity from its incoming edges, we transform every support quadruple whose form is $(e', r_i, \Tilde{e}_i, t_i)$ to $(\Tilde{e}_i, r^{-1}_i, e', t_i)$, where $r^{-1}_i$ corresponds to the inverse relation of $r_i$. We manage to incorporate every support quadruple into the aggregation process with this quadruple transformation. Note that if $t_q - t_i = 0$, the denominator of the exponential term will be $0$. Thus, we use a constant $\lambda$ to assign a value to $\exp(1 / {|t_q - t_i|})$ if $t_q$ equals $t_i$, and $\lambda$ serves as a hyperparameter that can be tuned. Figure \ref{fig: encoder structure} illustrates the structure of our graph encoder with an example. After aggregation, we further infuse the concept-aware information from the lower branch into the output of our graph encoder: $\mathbf{h}_{(e',t_q)} := \mathbf{h}_{(e',t_q)} + \delta_2 \sigma(\mathbf{W}_c^2\mathbf{h}_{e'}^{\mathcal{C}_{e'}})$. We show in Section \ref{ablation} that our simple-structured graph encoder can beat more complicated structures in the TKG OOG link prediction task.

\begin{equation}
    \mathcal{L} = \sum_{e' \in \mathcal{E}_T} \sum_{q^+ \in \mathcal{Q}_{e'}} \sum_{q^- \in \mathcal{Q}^-_{e'}} 
    \text{max} \{\theta - score(q^+) + score(q^-),0\}.
\end{equation}

We propose three TKG few-shot OOG link prediction datasets, i.e., ICEWS14-OOG, ICEWS18-OOG, and ICEWS0515-OOG. We first take three subsets, i.e., ICEWS14, ICEWS18, and ICEWS05-15, from the Integrated Crisis Early Warning System (ICEWS) database \cite{DVN/28075_2015}, where they contain the timestamped political facts in 2014, in 2018, and from 2005 to 2015, respectively. Following the data construction process of \cite{DBLP:conf/nips/BaekLH20}, for each subset, we first randomly sample half of the entities whose number of associated quadruples is between a lower and a higher threshold as unseen entities. Then we split the sampled entities into three groups $\mathcal{E}'_{\text{meta-train}}$, $\mathcal{E}'_{\text{meta-valid}}$, $\mathcal{E}'_{\text{meta-test}}$ ($\mathcal{E}'_{\text{meta-train}} \cap \mathcal{E}'_{\text{meta-valid}} = \emptyset$, $\mathcal{E}'_{\text{meta-train}} \cap \mathcal{E}'_{\text{meta-test}} = \emptyset$, $\mathcal{E}'_{\text{meta-valid}} \cap \mathcal{E}'_{\text{meta-test}} = \emptyset$), where $|\mathcal{E}'_{\text{meta-train}}|:|\mathcal{E}'_{\text{meta-valid}}|:|\mathcal{E}'_{\text{meta-test}}| \approx 8:1:1$. The associated quadruples of all the entities in $\mathcal{E}'_{\text{meta-train}}$/$\mathcal{E}'_{\text{meta-valid}}$/$\mathcal{E}'_{\text{meta-test}}$ form the meta-training/meta-validation/meta-test set. The rest of the quadruples without unseen entities are used for constructing a background graph $\mathcal{G}_{\text{back}}$. The dataset statistics are presented in Table \ref{tab: data}. We present the dataset construction process in Appendix

\begin{equation}
    \frac{1}{\sum_{e' \in \mathcal{E}'_{\text{meta-test}}} |\mathcal{Q}_{e'}|}\sum_{e' \in \mathcal{E}'_{\text{meta-test}}} \sum_{q^+ \in \mathcal{Q}_{e'}} \frac{1}{\psi}
\end{equation}

\begin{equation}
    \mathbf{h}_{(e', t_q)} = \sum_{(\Tilde{e}_i, r_i, t_i) \in \mathcal{N}_{e'}} \gamma_q^i  \mathbf{W}_g(\mathbf{h}_{\Tilde{e}_i} \|\mathbf{h}_{r_i})
    
    \gamma_q^i = \frac{\exp \left( \sigma \left( ([\mathbf{h}_{r_q} \| \Phi(t_q)] \mathbf{W}_Q)^{\top} ([\mathbf{h}_{r_i} \| \Phi(t_i)]\mathbf{W}_K ) \right) \right)}{\sum_{(\Tilde{e}_j, r_j, t_j) \in \mathcal{N}_{e'}} \exp \left( \sigma \left( ([\mathbf{h}_{r_q} \| \Phi(t_q)] \mathbf{W}_Q)^{\top} ([\mathbf{h}_{r_j} \| \Phi(t_j)]\mathbf{W}_K ) \right) \right)}
\end{equation}



\end{document}