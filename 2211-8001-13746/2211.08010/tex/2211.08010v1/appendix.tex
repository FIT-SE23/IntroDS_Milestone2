%File: anonymous-submission-latex-2023.tex
\documentclass[letterpaper]{article} % DO NOT CHANGE THIS
\usepackage{neurips_2022}
% \usepackage[submission]{aaai23}  % DO NOT CHANGE THIS
\usepackage{times}  % DO NOT CHANGE THIS
\usepackage{helvet}  % DO NOT CHANGE THIS
\usepackage{courier}  % DO NOT CHANGE THIS
\usepackage[hyphens]{url}  % DO NOT CHANGE THIS
\usepackage{graphicx} % DO NOT CHANGE THIS
\urlstyle{rm} % DO NOT CHANGE THIS
\def\UrlFont{\rm}  % DO NOT CHANGE THIS
\usepackage{natbib}  % DO NOT CHANGE THIS AND DO NOT ADD ANY OPTIONS TO IT
\usepackage{caption} % DO NOT CHANGE THIS AND DO NOT ADD ANY OPTIONS TO IT
\frenchspacing  % DO NOT CHANGE THIS
\setlength{\pdfpagewidth}{8.5in} % DO NOT CHANGE THIS
\setlength{\pdfpageheight}{11in} % DO NOT CHANGE THIS
%
% These are recommended to typeset algorithms but not required. See the subsubsection on algorithms. Remove them if you don't have algorithms in your paper.
% For theorems and such
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{bm}
\usepackage{subfigure}

% for better figures and tables
\usepackage{booktabs}
\usepackage{multirow}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[section]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{definition}[section]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\newtheorem{remark}[theorem]{Remark}


\newcommand{\norm}[1]{\left\lVert#1\right\rVert}
\newcommand{\abs}[1]{\left\lvert#1\right\rvert}
\newcommand{\smallnorm}[1]{\lVert#1\rVert}
\newcommand{\bignorm}[1]{\big\lVert#1\big\rVert}
\newcommand{\biggnorm}[1]{\bigg\rVert#1\biggr\rVert}
\newcommand{\Bignorm}[1]{\Big\lVert#1\Big\rVert}

\newcommand{\bmf}[1]{\boldsymbol#1}

\def\R{\mathbb{R}}
%
% These are are recommended to typeset listings but not required. See the subsubsection on listing. Remove this block if you don't have listings in your paper.
\usepackage{newfloat}
\usepackage{listings}
\DeclareCaptionStyle{ruled}{labelfont=normalfont,labelsep=colon,strut=off} % DO NOT CHANGE THIS
\lstset{%
	basicstyle={\footnotesize\ttfamily},% footnotesize acceptable for monospace
	numbers=left,numberstyle=\footnotesize,xleftmargin=2em,% show line numbers, remove this entire line if you don't want the numbers.
	aboveskip=0pt,belowskip=0pt,%
	showstringspaces=false,tabsize=2,breaklines=true}
\floatstyle{ruled}
\newfloat{listing}{tb}{lst}{}
\floatname{listing}{Listing}
%
% Keep the \pdfinfo as shown here. There's no need
% for you to add the /Title and /Author tags.
\pdfinfo{
/TemplateVersion (2023.1)
}

% DISALLOWED PACKAGES
% \usepackage{authblk} -- This package is specifically forbidden
% \usepackage{balance} -- This package is specifically forbidden
% \usepackage{color (if used in text)
% \usepackage{CJK} -- This package is specifically forbidden
% \usepackage{float} -- This package is specifically forbidden
% \usepackage{flushend} -- This package is specifically forbidden
% \usepackage{fontenc} -- This package is specifically forbidden
% \usepackage{fullpage} -- This package is specifically forbidden
% \usepackage{geometry} -- This package is specifically forbidden
% \usepackage{grffile} -- This package is specifically forbidden
% \usepackage{hyperref} -- This package is specifically forbidden
% \usepackage{navigator} -- This package is specifically forbidden
% (or any other package that embeds links such as navigator or hyperref)
% \indentfirst} -- This package is specifically forbidden
% \layout} -- This package is specifically forbidden
% \multicol} -- This package is specifically forbidden
% \nameref} -- This package is specifically forbidden
% \usepackage{savetrees} -- This package is specifically forbidden
% \usepackage{setspace} -- This package is specifically forbidden
% \usepackage{stfloats} -- This package is specifically forbidden
% \usepackage{tabu} -- This package is specifically forbidden
% \usepackage{titlesec} -- This package is specifically forbidden
% \usepackage{tocbibind} -- This package is specifically forbidden
% \usepackage{ulem} -- This package is specifically forbidden
% \usepackage{wrapfig} -- This package is specifically forbidden
% DISALLOWED COMMANDS
% \nocopyright -- Your paper will not be published if you use this command
% \addtolength -- This command may not be used
% \balance -- This command may not be used
% \baselinestretch -- Your paper will not be published if you use this command
% \clearpage -- No page breaks of any kind may be used for the final version of your paper
% \columnsep -- This command may not be used
% \newpage -- No page breaks of any kind may be used for the final version of your paper
% \pagebreak -- No page breaks of any kind may be used for the final version of your paperr
% \pagestyle -- This command may not be used
% \tiny -- This is not an acceptable font size.
% \vspace{- -- No negative value may be used in proximity of a caption, figure, table, section, subsection, subsubsection, or reference
% \vskip{- -- No negative value may be used to alter spacing above or below a caption, figure, table, section, subsection, subsubsection, or reference

\setcounter{secnumdepth}{0} %May be changed to 1 or 2 if section numbers are desired.

% The file aaai23.sty is the style file for AAAI Press
% proceedings, working notes, and technical reports.
%

% Title

% Your title must be in mixed case, not sentence case.
% That means all verbs (including short verbs like be, is, using,and go),
% nouns, adverbs, adjectives should be capitalized, including both words in hyphenated terms, while
% articles, conjunctions, and prepositions are lower case unless they
% directly follow a colon or long dash
\title{Appendices of Bayesian Federated Neural Matching that Completes Full Information}

\begin{document}
\maketitle
\section{Appendix A: BBP and IBP}
\label{Appendix2}
\textbf{Beta-Bernoulli Process and Indian Buffet Process}\quad Denote $Q$ as a random measure drawn from a Beta process: $Q | \gamma_0, H \sim \text{BP}(1, \gamma_0H)$, where $\gamma_0$ is the mass parameter, $H$ is the base measure over some domain $\Omega$ such that $H(\Omega) = 1$.
 One can show that $Q$ is a discrete measure with $Q=\sum_{i} q_i \delta_{\bmf{\theta}_{i}}$, which can be characterized by an infinitely countable set of (weight, atom) pairs $( q_i, \bmf{\theta}_i ) \in [0,1] \times \Omega$. The atoms $\bmf{\theta}_i$ can be drawn i.i.d from $H$ and the weights $\{q_i\}_{i=1}^{\infty}$ can be generated via a stick-breaking process \cite{teh2007stick}: $q_1 \sim \operatorname{Beta}(\gamma_0, 1), q_i = \prod_{g=1}^i q_g$. 
Then subsets of atoms in the random measure $Q$ can be picked via a Bernoulli process. That is, each subset $\mathcal{T}_s$ for $s = 1,\cdots, S$ can be distributed via a Bernoulli process with base measure $Q$: $\mathcal{T}_s | Q \sim \operatorname{BeP}(Q)$. Hence, subset $\mathcal{T}_s$ can also be viewed as a discrete measure $\mathcal{T}_s :=\sum_i a_{si}\delta_{\bmf{\theta}_i}$, which is formed by pairs $(a_{si}, \bmf{\theta}_i) \in \{0,1\} \times \Omega$, where $a_{si} | q_i \sim \operatorname{Bernoulli}(q_i), \forall i$ is a binary random variable indicating whether $\bmf{\theta}_i$ belongs to subset $\mathcal{T}_s$. We call such collection of subsets a Beta-Bernoulli process \cite{thibaux2007hierarchical}.

The Indian buffet process (IBP) specifies distribution on sparse binary matrices  \cite{ghahramani2006infinite}. IBP involves a metaphor of a sequence of customers tasting dishes in an infinite buffet: the first customer tastes $\operatorname{Poisson}(\gamma_0)$ dishes, every  subsequent  $s$th customer tastes each dish that is previously selected with probability $n_i / s$,  where $n_i = \sum_{s=1}^{S-1} a_{si}$, and then tastes  $\operatorname{Poisson}(\gamma_0 / s)$ new dishes. 
Marginalizing over Beta Process distributed $Q$ above will induce dependencies among subsets and recover the predictive distribution $\mathcal{T}_S | \mathcal{T}_1,\cdots,\mathcal{T}_{S-1} \sim \operatorname{BeP}(H\frac{\gamma_0}{S} + \sum_{i}\frac{n_i}{S}\delta_{\bmf{\theta}_i})$. That is equivalent to the IBP.

\section{Appendix B: Proof of Proposition 1 \cite{yurochkin2019bayesian}}
PFNM maximizes a posterior probability of the global atoms$\{\bmf{\theta}_i\}_{i=1}^\infty$ and assignments of observed neural network weight estimates to global atoms$\{ \bm{A}^s \}_{s=1}^S$.  Given estimates of the local model weights $\{ \bmf{w}_{sj} \text{ for } j = 1, \dots, J_s \}_{s=1}^S$, it has:
\begin{equation}
\label{eq16}
\max_{\{ \bmf{\theta}_i \}, \{ \bm{A}^s \} } P(\{ \bmf{\theta}_i\}, \{\bm{A}^s\} | \{\bmf{w}_{sj}\}) \\\propto P(\{\bmf{w}_{sj}\} | \{\bmf{\theta}_i\}, \{\bm{A}^s\} )P(\{\bm{A}^s\})P(\{\bmf{\theta}_i\}),
\end{equation}
by taking negative natural logarithm it can obtain:
\begin{equation}
\label{eq17}
\min_{\{ \bmf{\theta}_i \}, \{ \bm{A}^s \} } -\sum_i \bigg ( \sum_{s,j} \bmf{A}^s_{i,j} \log(p(\bmf{w}_{sj} | \sim \bmf{\theta}_i)) + \log(q(\bmf{\theta}_i)) \bigg ) \\
 - \log(P(\{ \bm{A}^s \})),
\end{equation}
expand probability function of multi-dimensional Gaussian distributions in equation~(\ref{eq17}), it obtains:
\begin{equation}
\label{eq18}
\min_{\{ \bmf{\theta}_i \}, \{ \bm{A}^s \} } \frac{1}{2} \sum_{i}\Bigg( \frac{|| \hat{\bmf{\theta}}_i - \bmf{\mu}_0 ||^2}{\sigma_0^2} + (D+K)\log(2\pi\sigma_0^2 ) \\
+ \sum_{s,j}A^s_{i,j} \frac{||\bmf{w}_{sj} - \hat{\bmf{\theta}}_i||^2}{\sigma_s^2}\Bigg)
 - \log(P(\{ \bm{A}^s \})).
\end{equation}
We now consider the first part of equation~(\ref{eq18}). Through the closed-form expression of $\{\bmf{\theta}_i\}$ estimated according to the Gaussian-Gaussian conjugacy:
\begin{equation}
\label{eq19}
\hat{\bmf{\theta}_i} =  \frac{\bmf{\mu}_0 / \sigma_0^2 + \sum_{s,j}A_{i,j}^s \bmf{w}_{sj} / \sigma^2_s}{1 / \sigma_0^2 + \sum_{s,j}A^s_{i,j} / \sigma^2_s} \text{ for } i = 1, ..., J,
\end{equation}
where for simplicity we assume $\bm{\Sigma}_0 = \bm{I} \sigma^2_0$ and $\bm{\Sigma}_s = \bm{I}\sigma^2_s$, we can now cast first part of equation~(\ref{eq18}) with respect only to $\{ \bm{A}^s \}_{s=1}^S$:
\begingroup
\allowdisplaybreaks
\begin{equation}
\label{eq20}
    \begin{aligned}
 		& \frac{1}{2} \sum_{i}\Bigg( \frac{|| \hat{\bmf{\theta}}_i - \bmf{\mu}_0 ||^2}{\sigma_0^2} + (D+K)\log(2\pi\sigma_0^2 ) + \sum_{s,j}A^s_{i,j} \frac{||\bmf{w}_{sj} - \hat{\bmf{\theta}}_i||^2}{\sigma_s^2} \Bigg) \\
 		 \cong &\frac{1}{2} \sum_{i}\Bigg( \langle \hat{\bmf{\theta}}_i, \hat{\bmf{\theta}}_i \rangle(\frac{1}{\sigma_0^2} + \sum_{s,j}\frac{A^s_{i,j}}{\sigma^2_s}) + (D+K)\log(2\pi\sigma_0^2 )  -  2\langle \hat{\bmf{\theta}}_i, \sum_{s,j}A^s_{i,j} \frac{\bmf{w}_{sj}}{\sigma_s^2}) \rangle\Bigg) \\
 		  = &-\frac{1}{2} \sum_{i}\Bigg(\frac{|| \sum\limits_{s,j} \bmf{A}^{s}_{i,j}\frac{\bmf{w}_{sj} - \bmf{\mu}_0}{\sigma_{s}^2}||^2}{( 1 / \sigma_0^2 + \sum\limits_{s,j} \bmf{A}^{s}_{i,j} / \sigma_{s}^2} - (D+K)\log(2\pi\sigma_0^2 )\Bigg). \\
    \end{aligned}
\end{equation}
\endgroup
Partition equation~(\ref{eq20}) between $i = 1,..., J_{-s'}$ and $i =J_{-s'} + 1,...,J_{-s'} + J_{s'}$, and because it is now solving for $ \bm{A}^{s'}$, it can subtract terms independent of $\bm{A}^{s'}$:
\begingroup
\allowdisplaybreaks
\begin{equation}
\label{eq21}
 	\begin{aligned}
 		& \sum_{i}\Bigg(\frac{|| \sum_{s,j} \bmf{A}^{s}_{i,j}\frac{\bmf{w}_{sj} - \bmf{\mu}_0}{\sigma_{s}^2}||^2}{( 1 / \sigma_0^2 + \sum_{s,j} \bmf{A}^{s}_{i,j} / \sigma_{s}^2} - (D+K)\log(2\pi\sigma_0^2 )\Bigg) \\
        \cong & \sum_{i=1}^{J_{-s'}} \Bigg(\frac{|| \sum_j \bmf{A}^{s'}_{i,j}\frac{\bmf{w}_{s'j} - \bmf{\mu}_0}{\sigma_{s'}^2} + \sum_{s \in -s',j} \bmf{A}^s_{i,j}\frac{\bmf{w}_{sj} - \bmf{\mu}_0}{\sigma_s^2} ||^2}{1 / \sigma_0^2 + \sum_{j} \bmf{A}^{s'}_{i,j} / \sigma_{s'}^2 + \sum_{s \in -s',j} \bmf{A}^s_{i,j} / \sigma^2_s} - \frac{||\sum_{s \in -s',j} \bmf{A}^s_{i,j}\frac{\bmf{w}_{sj} - \bmf{\mu}_0}{\sigma_s^2} ||^2}{1 / \sigma_0^2 + \sum_{s \in -s',j} \bmf{A}^s_{i,j} / \sigma^2_s } \Bigg) \\
        & \quad  + \sum_{i=J_{-s'}+1}^{J_{-s'} + J_{s'}} \Bigg(\frac{|| \sum_j \bmf{A}^{s'}_{i,j}\frac{\bmf{w}_{s'j} - \bmf{\mu}_0}{\sigma_{s'}^2} ||^2}{1 / \sigma_0^2 + \sum_j \bmf{A}^{s'}_{i,j} / \sigma^2_{s'}}\Bigg),
 	\end{aligned}
\end{equation}
\endgroup
observe that $\sum_j \bmf{A}^{s'}_{i,j} \in \{ 0,1 \}$, i.e. it is 1 if some neuron from dataset $s'$ is matched to global neuron $i$ and 0 otherwise. Thus equation~(\ref{eq21}) can rewritten as a linear sum assignment problem:
\begingroup
\allowdisplaybreaks
\begin{equation}
\label{eq22}
	\begin{aligned}
	  &\sum_{i=1}^{J_{-s'}} \sum_{j=1}^{J_{s'}} \bmf{A}^{s'}_{i,j} \Bigg(\frac{||\frac{\bmf{w}_{s'j} - \bmf{\mu}_0}{\sigma_{s'}^2} + \sum_{s \in -s',j} \bmf{A}^s_{i,j}\frac{\bmf{w}_{sj} - \bmf{\mu}_0}{\sigma_s^2} ||^2}{1 / \sigma_0^2 + 1 / \sigma_{s'}^2 + \sum_{s \in -s',j} \bmf{A}^s_{i,j} / \sigma^2_s} - \frac{||\sum_{s \in -s',j} \bmf{A}^s_{i,j}\frac{\bmf{w}_{sj} - \bmf{\mu}_0}{\sigma_s^2} ||^2}{1 / \sigma_0^2 + \sum_{s \in -s',j} \bmf{A}^s_{i,j} / \sigma^2_s}\Bigg) \\
	   & \quad +  \sum_{i=J_{-s'}+1}^{J_{-s'} + J_{s'}} \sum_{j=1}^{J_{s'}} \bmf{A}^{s'}_{i,j} \Bigg( \frac{||\frac{\bmf{w}_{s'j} - \bmf{\mu}_0}{\sigma_{s'}^2} ||^2}{1 / \sigma_0^2 + \sum_j \bmf{A}^{s'}_{i,j} / \sigma^2_{s'}}\Bigg).
	\end{aligned}
\end{equation}
\endgroup
Then consider the second term of equation~(\ref{eq18}), by subtracting terms independent of $\bm{A}^{s'}$ it has:
\begin{equation}
\label{eq23}
	\log(P(\bm{A}^{s'})) = \log(P(\bm{A}^{s'} | \bm{A}^{-s'})) + \log(P(\bm{A}^{-s'})).
\end{equation}
First, it can ignore $\log(P(\bm{A}^{-s'}))$ since now are optimizing for $\bm{A}^{s'}$ . Second, due to exchange ability of datasets
(i.e. customers of the IBP), $\bm{A}^{s'}$ can always be treated
as the last customer of the IBP. Denote
$n^{-s'}_i = \sum_{-s',j} \bmf{A}^{s'}_{i,j}$ as the number of times local weights were assigned to global atom $i$ outside of group $s'$. Now it can obtain the following:
\begingroup
\allowdisplaybreaks
\begin{equation}
\label{eq24}
\begin{aligned}
  \log P(\bm{A}^{s'} | \bm{A}^{-s'}) & \cong \sum_{i=1}^{J_{-s'}} \Bigg(\bigg(\sum_{j=1}^{J_{s'}} \bmf{A}^{s'}_{i,j}\bigg)\log\frac{n^{-s'}_i}{S} + \bigg(1 - \sum_{j=1}^{J_{s'}} \bmf{A}^{s'}_{i,j}\bigg)\log\frac{S - n^{-s'}_i}{S}\Bigg)  \\
	& \qquad -  \log\Bigg(\sum_{i=J_{-s'}+1}^{J_{-s'} + J_{s'}} \sum_{j=1}^{J_{s'}} \bmf{A}^{s'}_{i,j}\Bigg) + \Bigg(\sum_{i=J_{-s'}+1}^{J_{-s'} + J_{s'}} \sum_{j=1}^{J_{s'}} \bmf{A}^{s'}_{i,j}\Bigg)\log\frac{\gamma_0}{J}.
\end{aligned}
\end{equation}
\endgroup
equation~(\ref{eq24}) thus can be rearranged as a linear sum assignment problem:
\label{eq25}
\begin{equation}
   \sum_{i=1}^{J_{-s'}} \sum_{j=1}^{J_{s'}} \bmf{A}^{s'}_{i,j}\log\frac{ n^{-s'}_i}{S - n^{-s'}_i}  +  \sum_{i=J_{-s'}+1}^{J_{-s'} + J_{s'}} \sum_{j=1}^{J_{s'}} \bmf{A}^{s'}_{i,j}\Bigg(\log\frac{\gamma_0}{S} - \log(i - J_{-s'}) \Bigg).
\end{equation}
Combining equation~(\ref{eq22}) and equation~(\ref{eq25}), we arrive at the cost specification shown in (6) of the main text. That completes the proof of Proposition 1 in the main text.


\section{Appendix C: Proof of Proposition 2}
\begin{proof}
i) Let's prove the first part. From Bayesian theory, the posterior is given by
  \begin{align}
      p(\bmf{\theta}_i \vert \bmf{w}_{sj}) & \propto p(\bmf{\theta}_i)p(\bmf{w}_{sj}) \\
      & \propto \exp{(\frac{-\bmf{\theta}_i^T\bmf{\theta}_i+2\bmf{\theta}_i^T\bmf{\mu}_0-\bmf{\mu}_0^T\bmf{\mu}_0}{2\sigma_0^2})} \\
      &  \exp{(\frac{-\bmf{w}_{sj}^T\bmf{w}_{sj}+2\bmf{\theta}_i^T\bmf{w}_{sj}-\bmf{\theta}_i^T\bmf{\theta}_i}{2\sigma_s^2})}. \\
  \end{align}
  From Gaussian conjugate, the product of two Gaussians is still a Gaussian, we will rewrite this in the form
  \begin{align}
      p(\bmf{\theta}_i \vert \bmf{w}_{sj}) & \propto \exp{\bigl[-\frac{\bmf{\theta}_i^T\bmf{\theta}_i}{2}(\frac{1}{\sigma_0^2} + \frac{1}{\sigma_s^2})+\bmf{\theta}_i^T(\frac{\bmf{\mu}_0}{\sigma_0^2}+\frac{\bmf{w}_{sj}}{\sigma_s^2}) - (\frac{\bmf{\mu}_0^T\bmf{\mu}_0}{2\sigma_0^2}+\frac{\bmf{w}_{sj}^T\bmf{w}_{sj}}{2\sigma_s^2})\bigr]} \\
      & \mathop{=}\limits^{def} \exp{[\frac{-\bmf{\theta}_i^T\bmf{\theta}_i+2\bmf{\theta}_i^T\bmf{\hat{\mu}}-\bmf{\hat{\mu}}^T\bmf{\hat{\mu}}}{2\hat{\sigma}^2}]}
      = \exp{[-\frac{1}{2\hat{\sigma}^2}(\norm{\bmf{\theta}_i - \bmf{\hat{\mu}}}^2)]},
  \end{align}
  where $\bmf{\hat{\mu}}$ and $\hat{\sigma}^2$ denote the mean and variance of the posterior Gaussian. By completing the square, we first match the coefficients of $\bmf{\theta}_i^T\bmf{\theta}_i$---second power of $\bmf{\theta}_i$, and find $\hat{\sigma}^2$ is given by
  \begin{align}
      \frac{1}{\hat{\sigma}^2} & = \frac{1}{\sigma_0^2} + \frac{1}{\sigma_s^2},\\
      \hat{\sigma}^2 &= \frac{1}{\frac{1}{\sigma_0^2} + \frac{1}{\sigma^2_{s}}},
  \end{align}
  and then match the coefficients of first power $\bmf{\theta}_i^T$ we get
  \begin{align}
      \frac{\bmf{\hat{\mu}}}{\hat{\sigma}^2} &= \frac{\bmf{\mu}_0}{\sigma_0^2}+\frac{\bmf{w}_{sj}}{\sigma_s^2},
  \end{align}
  hence
  \begin{align}
      \bmf{\hat{\mu}} = \hat{\sigma}^2(\frac{\bmf{\mu}_0}{\sigma_0^2}+\frac{\bmf{w}_{sj}}{\sigma_s^2}).
  \end{align}
  ii) Now let's prove the second part. When $\bmf{\theta}_i$ has been assigned local neurons from other assignments $\bmf{Z}_i^{-s'} = \{(s, j) \vert A_{ij}^s = 1, s \in -s'\}$,from Bayesian theory, the posterior is given by
  \begin{align}
      p(\bmf{\theta}_i \vert \bmf{Z}_i^{-s'}) & \propto p(\bmf{\theta}_i)\prod_{z\in \bmf{Z}_i^{-s'}}p(\bmf{w}_{z}) \\
      & \propto \exp{(\frac{-\bmf{\theta}_i^T\bmf{\theta}_i+2\bmf{\theta}_i^T\bmf{\mu}_0-\bmf{\mu}_0^T\bmf{\mu}_0}{2\sigma_0^2})} \\
      &  \exp{(-\sum_{s\in {-s'},j} A_{i,j}^s\frac{\bmf{w}_{sj}^T\bmf{w}_{sj}}{\sigma_s^2}  +  \bmf{\theta}_i^T\sum_{s\in {-s'},j} A_{i,j}^s\frac{\bmf{w}_{sj}}{\sigma_s^2}  -  \bmf{\theta}_i^T\bmf{\theta}_i \sum_{s\in {-s'},j} A_{i,j}^s\frac{1}{2\sigma_s^2}
      )}.
  \end{align}
  Since the product of two Gaussians is a Gaussian, we will rewrite this in the form
  \begin{align}
      p(\bmf{\theta}_i \vert \bmf{Z}_i^{-s'}) & \propto \exp{\bigl[-\frac{\bmf{\theta}_i^T\bmf{\theta}_i}{2}(\frac{1}{\sigma_0^2} + \sum_{s\in {-s'},j} A_{i,j}^s\frac{1}{\sigma_s^2}) +\bmf{\theta}_i^T(\frac{\bmf{\mu}_0}{\sigma_0^2} + \sum_{s\in {-s'},j} A_{i,j}^s\frac{\bmf{w}_{sj}}{\sigma_s^2}) - (\frac{\bmf{\mu}_0^T\bmf{\mu}_0}{2\sigma_0^2} + \sum_{s\in {-s'},j} A_{i,j}^s\frac{\bmf{w}_{sj}^T\bmf{w}_{sj}}{2\sigma_s^2})\bigr]} \\
      & \mathop{=}\limits^{def} \exp{(\frac{-\bmf{\theta}_i^T\bmf{\theta}_i+2\bmf{\theta}_i^T\bmf{\hat{\mu}}-\bmf{\hat{\mu}}^T\bmf{\hat{\mu}}}{2\hat{\sigma}^2})}
      = \exp{[-\frac{1}{2\hat{\sigma}^2}(\norm{\bmf{\theta}_i - \bmf{\hat{\mu}}}^2)]},
  \end{align}
  where $\bmf{\hat{\mu}}$ and $\hat{\sigma}^2$ denote the mean and variance of the posterior Gaussian. By completing the square, we first match the coefficients of $\bmf{\theta}_i^T\bmf{\theta}_i$---second power of $\bmf{\theta}_i$, and find $\hat{\sigma}^2$ is given by
  \begin{align}
      \frac{1}{\hat{\sigma}^2} & = \frac{1}{\sigma_0^2} + \sum_{s\in {-s'},j} A_{i,j}^s\frac{1}{\sigma_s^2},\\
      \hat{\sigma}^2 &= \frac{1}{\frac{1}{\sigma_0^2} + \sum_{s\in {-s'},j} A_{i,j}^s\frac{1}{\sigma_s^2}},
  \end{align}
  and then match the coefficients of first power $\bmf{\theta}_i^T$ we get
  \begin{align}
      \frac{\bmf{\hat{\mu}}}{\hat{\sigma}^2} &= \frac{\bmf{\mu}_0}{\sigma_0^2} + \sum_{s\in {-s'},j} A_{i,j}^s\frac{\bmf{w}_{sj}}{\sigma_s^2},
  \end{align}
  hence
  \begin{align}
      \bmf{\hat{\mu}} = \hat{\sigma}^2(\frac{\bmf{\mu}_0}{\sigma_0^2} + \sum_{s\in {-s'},j} A_{i,j}^s\frac{\bmf{w}_{sj}}{\sigma_s^2}).
  \end{align}
\end{proof}

\section{Appendix D: Iteratively Layer-wise Matched Aggregation via NAFI}
As the empirical study in \cite{wang2020federated} demonstrates, directly applying the matching algorithms  fails on deep architectures which are necessary to solve more complex tasks. Thus to alleviate this problem ,
we also extend NAFI to the following layer-wise matching scheme. Firstly, the server only collects the first layer weights from the clients and applies NAFI
to acquire the first layer weights of the federated model. Then, the server broadcasts these weights
to the clients, which proceed to train all consecutive layers on their datasets while keeping the matched layers frozen. Repeat this process until the last layer, where we make a weighted average based on class proportions of each client's data points.  We summarize our
 layer-wise version of NAFI in Algorithm 2. The layer-wise approach requires communication
rounds that equal to the number of layers in a neural network. Experimental results show that with layer-wise
matching, NAFI performs well on the ConvNets even for U-nets which has a complex architecture. In the more challenging
heterogeneous setting, NAFI outperforms FedAvg, FedProx trained with same number of
communication rounds (5 for ConvNet and 19 for U-net).
\begin{algorithm}
\caption{Iteratively Layer-wise NAFI}
\label{alg2}
\begin{algorithmic}[1]
    \REQUIRE ~~\\
    Collected local weights of $N$-layer architectures $\{ \bmf{W}_s^{(0)}, \cdots, \bmf{W}_s^{(N-1)} \}_{s=1}^{S}$ from $S$ clients;
    \ENSURE ~~\\
    New constructed global weights $\{ \bmf{W}^{(0)}, \cdots, \bmf{W}^{(N-1)} \}$.
 	\STATE $n = 0$;
 	\WHILE{layers $ n \leq N$}
 	    \IF{n < N-1}
 		    \STATE $\{A_s \}_{s=1}^S = \operatorname{NAFI}(\{ \bmf{W}_s^{(n)} \}_{s=1}^S)$;
 		    \STATE $\bmf{W}^{(n)} = \frac{1}{S}\sum_s \bmf{W}_s^{(n)}A^T_s$;
 		\ELSE 
 		    \STATE $\bmf{W}^{(n)} = \sum_s \bmf{p_{s}} \bmf{\cdot} \bmf{W}^{(n)}_s $ where $\bmf{p_{s}}$ is vector of fraction of data points with each label on worker $s$, and $\bmf{\cdot}$ denotes the dot product;
 		  \ENDIF
 		  \FOR{ $s \in \{ 1, \cdots,S \}$ }
 		    \STATE $\bmf{W}_s^{(n+1)} = A_s \bmf{W}_s^{(n+1)}$;
 		    \STATE Train $\{ \bmf{W}_s^{(n+1)},\cdots, \bmf{W}_s^{(N-1)}\}$ with $\bmf{W}_s^{(n)}$ frozen;
 		  \ENDFOR
 	    \STATE $n = n+1$
 	\ENDWHILE
\end{algorithmic}
\end{algorithm}

\bibliographystyle{unsrt} 
\newpage
\bibliography{references}


\end{document}