%File: anonymous-submission-latex-2023.tex
\documentclass[letterpaper]{article} % DO NOT CHANGE THIS
\usepackage[]{aaai23}  % DO NOT CHANGE THIS
\usepackage{times}  % DO NOT CHANGE THIS
\usepackage{helvet}  % DO NOT CHANGE THIS
\usepackage{courier}  % DO NOT CHANGE THIS
\usepackage[hyphens]{url}  % DO NOT CHANGE THIS
\usepackage{graphicx} % DO NOT CHANGE THIS
\urlstyle{rm} % DO NOT CHANGE THIS
\def\UrlFont{\rm}  % DO NOT CHANGE THIS
\usepackage{natbib}  % DO NOT CHANGE THIS AND DO NOT ADD ANY OPTIONS TO IT
\usepackage{caption} % DO NOT CHANGE THIS AND DO NOT ADD ANY OPTIONS TO IT
\frenchspacing  % DO NOT CHANGE THIS
\setlength{\pdfpagewidth}{8.5in} % DO NOT CHANGE THIS
\setlength{\pdfpageheight}{11in} % DO NOT CHANGE THIS
%
% These are recommended to typeset algorithms but not required. See the subsubsection on algorithms. Remove them if you don't have algorithms in your paper.
% For theorems and such
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{bm}
\usepackage{subfigure}

% for better figures and tables
\usepackage{booktabs}
\usepackage{multirow}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[section]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{definition}[section]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\newtheorem{remark}[theorem]{Remark}


\newcommand{\norm}[1]{\left\lVert#1\right\rVert}
\newcommand{\abs}[1]{\left\lvert#1\right\rvert}
\newcommand{\smallnorm}[1]{\lVert#1\rVert}
\newcommand{\bignorm}[1]{\big\lVert#1\big\rVert}
\newcommand{\biggnorm}[1]{\bigg\rVert#1\biggr\rVert}
\newcommand{\Bignorm}[1]{\Big\lVert#1\Big\rVert}

\newcommand{\bmf}[1]{\boldsymbol#1}

\def\R{\mathbb{R}}
%
% These are are recommended to typeset listings but not required. See the subsubsection on listing. Remove this block if you don't have listings in your paper.
\usepackage{newfloat}
\usepackage{listings}
\DeclareCaptionStyle{ruled}{labelfont=normalfont,labelsep=colon,strut=off} % DO NOT CHANGE THIS
\lstset{%
	basicstyle={\footnotesize\ttfamily},% footnotesize acceptable for monospace
	numbers=left,numberstyle=\footnotesize,xleftmargin=2em,% show line numbers, remove this entire line if you don't want the numbers.
	aboveskip=0pt,belowskip=0pt,%
	showstringspaces=false,tabsize=2,breaklines=true}
\floatstyle{ruled}
\newfloat{listing}{tb}{lst}{}
\floatname{listing}{Listing}
%
% Keep the \pdfinfo as shown here. There's no need
% for you to add the /Title and /Author tags.
\pdfinfo{
/TemplateVersion (2023.1)
}

% DISALLOWED PACKAGES
% \usepackage{authblk} -- This package is specifically forbidden
% \usepackage{balance} -- This package is specifically forbidden
% \usepackage{color (if used in text)
% \usepackage{CJK} -- This package is specifically forbidden
% \usepackage{float} -- This package is specifically forbidden
% \usepackage{flushend} -- This package is specifically forbidden
% \usepackage{fontenc} -- This package is specifically forbidden
% \usepackage{fullpage} -- This package is specifically forbidden
% \usepackage{geometry} -- This package is specifically forbidden
% \usepackage{grffile} -- This package is specifically forbidden
% \usepackage{hyperref} -- This package is specifically forbidden
% \usepackage{navigator} -- This package is specifically forbidden
% (or any other package that embeds links such as navigator or hyperref)
% \indentfirst} -- This package is specifically forbidden
% \layout} -- This package is specifically forbidden
% \multicol} -- This package is specifically forbidden
% \nameref} -- This package is specifically forbidden
% \usepackage{savetrees} -- This package is specifically forbidden
% \usepackage{setspace} -- This package is specifically forbidden
% \usepackage{stfloats} -- This package is specifically forbidden
% \usepackage{tabu} -- This package is specifically forbidden
% \usepackage{titlesec} -- This package is specifically forbidden
% \usepackage{tocbibind} -- This package is specifically forbidden
% \usepackage{ulem} -- This package is specifically forbidden
% \usepackage{wrapfig} -- This package is specifically forbidden
% DISALLOWED COMMANDS
% \nocopyright -- Your paper will not be published if you use this command
% \addtolength -- This command may not be used
% \balance -- This command may not be used
% \baselinestretch -- Your paper will not be published if you use this command
% \clearpage -- No page breaks of any kind may be used for the final version of your paper
% \columnsep -- This command may not be used
% \newpage -- No page breaks of any kind may be used for the final version of your paper
% \pagebreak -- No page breaks of any kind may be used for the final version of your paperr
% \pagestyle -- This command may not be used
% \tiny -- This is not an acceptable font size.
% \vspace{- -- No negative value may be used in proximity of a caption, figure, table, section, subsection, subsubsection, or reference
% \vskip{- -- No negative value may be used to alter spacing above or below a caption, figure, table, section, subsection, subsubsection, or reference

\setcounter{secnumdepth}{0} %May be changed to 1 or 2 if section numbers are desired.

% The file aaai23.sty is the style file for AAAI Press
% proceedings, working notes, and technical reports.
%

% Title

% Your title must be in mixed case, not sentence case.
% That means all verbs (including short verbs like be, is, using,and go),
% nouns, adverbs, adjectives should be capitalized, including both words in hyphenated terms, while
% articles, conjunctions, and prepositions are lower case unless they
% directly follow a colon or long dash
\title{Bayesian Federated Neural Matching that Completes Full Information}
% \author{
%     %Authors
%     % All authors must be in the same font size and format.
%     Written by AAAI Press Staff\textsuperscript{\rm 1}\thanks{With help from the AAAI Publications Committee.}\\
%     AAAI Style Contributions by Pater Patel Schneider,
%     Sunil Issar,\\
%     Peng Xiao\textsuperscript{\rm 1},
%     George Ferguson,
%     Hans Guesgen,
%     Francisco Cruz\equalcontrib,
%     Marc Pujol-Gonzalez\equalcontrib
% }
\author{
    %Authors
    % All authors must be in the same font size and format.
    Peng Xiao,\textsuperscript{\rm 1}
    Samuel Cheng\textsuperscript{\rm 2}
}
% \affiliations{
%     %Afiliations
%     \textsuperscript{\rm 1}Association for the Advancement of Artificial Intelligence\\
%     % If you have multiple authors and multiple affiliations
%     % use superscripts in text and roman font to identify them.
%     % For example,

%     % Sunil Issar, \textsuperscript{\rm 2}
%     % J. Scott Penberthy, \textsuperscript{\rm 3}
%     % George Ferguson,\textsuperscript{\rm 4}
%     % Hans Guesgen, \textsuperscript{\rm 5}.
%     % Note that the comma should be placed BEFORE the superscript for optimum readability

%     1900 Embarcadero Road, Suite 101\\
%     Palo Alto, California 94303-3310 USA\\
%     % email address must be in roman text type, not monospace or sans serif
%     publications23@aaai.org
% %
% % See more examples next
% }
\affiliations{
    %Afiliations
    \textsuperscript{\rm 1}Department of Computer Science and Technology, Tongji University\\
    % If you have multiple authors and multiple affiliations
    % use superscripts in text and roman font to identify them.
    % For example,

    % Sunil Issar, \textsuperscript{\rm 2}
    % J. Scott Penberthy, \textsuperscript{\rm 3}
    % George Ferguson,\textsuperscript{\rm 4}
    % Hans Guesgen, \textsuperscript{\rm 5}.
    % Note that the comma should be placed BEFORE the superscript for optimum readability

    Shanghai, China\\
    % email address must be in roman text type, not monospace or sans serif
    phd.xiaopeng@gmail.com\\
    \textsuperscript{\rm 2}School of Electrical and Computer Engineering, University of Oklahoma \\
    Oklahoma City, US
%
% See more examples next
}

%Example, Single Author, ->> remove \iffalse,\fi and place them surrounding AAAI title to use it
\iffalse
\title{My Publication Title --- Single Author}
\author {
    Author Name
}
\affiliations{
    Affiliation\\
    Affiliation Line 2\\
    name@example.com
}
\fi

\iffalse
%Example, Multiple Authors, ->> remove \iffalse,\fi and place them surrounding AAAI title to use it
\title{My Publication Title --- Multiple Authors}
\author {
    % Authors
    First Author Name,\textsuperscript{\rm 1}
    Second Author Name, \textsuperscript{\rm 2}
    Third Author Name \textsuperscript{\rm 1}
}
\affiliations {
    % Affiliations
    \textsuperscript{\rm 1} Affiliation 1\\
    \textsuperscript{\rm 2} Affiliation 2\\
    firstAuthor@affiliation1.com, secondAuthor@affilation2.com, thirdAuthor@affiliation1.com
}
\fi


% REMOVE THIS: bibentry
% This is only needed to show inline citations in the guidelines document. You should not need it and can safely delete it.
\usepackage{bibentry}
% END REMOVE bibentry

\begin{document}

\maketitle

\begin{abstract}
Federated learning is a contemporary machine learning paradigm where locally trained models are distilled into a global model. Due to the intrinsic permutation invariance of neural networks, Probabilistic Federated Neural Matching (PFNM) employs a Bayesian nonparametric framework in the generation process of local neurons, and then creates a linear sum assignment formulation in each alternative optimization iteration. But according to our theoretical analysis, the optimization iteration in PFNM omits global information from existing. In this study, we propose a novel approach that overcomes this flaw by introducing a Kullback-Leibler  divergence penalty at each iteration.
The effectiveness of our approach is demonstrated by experiments on both image classification and semantic segmentation tasks.
\end{abstract}

\section{Introduction}
\label{sec:introduction}
\par The recent decade has seen a rapid advancement in artificial intelligence, particularly in the subfield of deep learning. This is large because there is an abundance of data available. However, the increasing privacy concerns present a barrier to some datasets' accessibility in a number of applications, particularly in the medical and financial areas. Therefore, federated learning (FL), which involves learning a model from disparate sets of data, is suggested to address this issue.

\par Federated learning is a learning paradigm where locally learned models are combined into a shared global model. Neural networks are naturally used in federated learning since they play a vital role in deep learning. Considering the inherent permutation invariance of a neural network, it is reasonable to match neurons of distinct local models before aggregating them. To do this, Probabilistic Federated Neural Matching (PFNM) \cite{yurochkin2019bayesian} builds a Bayesian nonparametric framework to match and merge these neurons. In the process of modeling, PFNM formally characterizes the generative process of local neurons through the Beta-Bernoulli process (BBP) \cite{2007Hierarchical}, and treats the local neurons as noisy realizations of latent global neurons. In the process of optimizing, PFNM iteratively maximizes the posterior estimation of latent global neurons, through solving a linear sum assignment formulation of global neurons and current local neurons. PFNM subsequently extends in modern architectures such as convolutional neural networks (CNNs) and long short-term memory (LSTMs) \cite{wang2020federated}, and its variants are also utilized in aggregating various statistical models such as Gaussian topic models, hierarchical Dirichlet process based
hidden Markov models \cite{yurochkin2019statistical,yurochkin2018scalable} etc.

\par In this paper, we theoretically prove the drawback of PFNM under its probabilistic framework and fix the drawback by introducing a Kullback-Leibler (KL) divergence penalty. Specifically, the contributions in this work include: 1) We theoretically prove that the linear sum formulation in the optimizing process omits the information of global neurons under the Bayesian framework. 2) We fix the missing global information by introducing a KL divergence penalty to complete the full information. 3) On three image datasets, experimental results demonstrate the effectiveness of our approach.

\section{Related Works}
\subsubsection{Federated Learning}
The initial aggregation method in FL is FedAvg \cite{mcmahan2017communication} in which parameters of local models are averaged coordinate-wisely. But the performance of FedAvg deteriorates significantly in non-i.i.d (Independent and Identically Distributed) data \cite{karimireddy2020scaffold,deng2021distributionally,xiao2020averaging}. Subsequent improved methods starts from different perspectives. FedProx \cite{li2018federated} adds a proximal term in local training cost to keep dissimilarity between local models in lower bound. SCAFFOLD \cite{karimireddy2020scaffold} uses control variates to correct the drift in local updates. FedPD \cite{zhang2020fedpd} proposes a primal-dual optimization strategy to alleviate deterioration in non-i.i.d data. Agnostic federated learning \cite{mohri2019agnostic} optimizes a centralized distribution by maximize the worst local model. Federated multi-task learning \cite{smith2017federated} applies a multi-task learning mechanism to customize local models. Several studies further extend \textit{knowledge distillation} \cite{hinton2015distilling,bucilua2006model,schmidhuber1992learning} in federated learning. And the key idea is to employ the knowledge of pre-trained teacher neural networks (local models) to learn a student neural network (global model) \cite{lin2020ensemble, chen2020fedbe, zhu2021data}.  However, all those methods above don't considering the permutation invariance in neural network.

\subsubsection{Parameter Matching} There are also other researches about match the parameters. In \cite{singh2020model}, the authors utilize optimal transport to minimize a transportation cost matrix to align neurons across different NNs, and it fixes the number of neurons in global model which make it infeasible when data is highly heterogeneous across local models.
Some work \cite{claici2020model} optimizes the assignments between global and local components under a KL divergence through variational inference. But the optimization process is not straightforward, and the calculation of variational inference is complicated.

\section{Problem Formulation}
Given $S$ fully connected (FC) NNs with one hidden layer trained from different datasets: $f_s(\bmf{x}) = \sigma(\bmf{x}^{\rm{T}}\bmf{W}_s^{(0)})\bmf{W}_s^{(1)}, \text{for } s = 1, \cdots, S$ (biases are omitted to simplify notation), where $\sigma(\cdot)$ is the nonlinear activation function, $\bmf{W}_s^{(0)} \in \R^{D \times J_s}$ and $\bmf{W}_s^{(1)} \in \R^{J_s \times K}$ are the weights; with $D$ being the 
input dimension, $K$ being the output dimension (i.e., number of classes), and $J_s$ being the number of neurons on the hidden layer of $s$-th NN. Neuron indexed by $j$ is viewed as a concatenated vector $\bmf{w}_{sj}^{T} = [{\bmf{W}_{s, j \cdot}^{(0)}}^T, \bmf{W}_{s, \cdot j}^{(1)}]$,  where $j \cdot$ and $\cdot j$ denote the $j$th row and column correspondingly. And $s$-th FCNN can also be viewed as a collection of neurons  $\{\bmf{w}_{sj} \in \R^{D + K}\}_{j=1}^{J_s}$. 
In federated learning, we want to learn a global neural network with weights $\Theta^{(0)} \in \R^{D \times J}, \Theta^{(1)} \in \R^{J \times K}$, where $J \ll \sum_{s=1}^S J_s$ is an inferred variable denoting the number of neurons in the global network.
\subsubsection{Permutation Invariance} Expanding the preceding expression of a FCNN: $f_s(\bmf{x}) = \sum_{j=1}^{J_s} \bmf{W}_{s, j \cdot}^{(1)}\sigma(\langle\bmf{x},\bmf{W}_{s,\cdot j}^{(0)}\rangle)$. Summation is a permutation invariant operation, thus any permutation $\tau(1,\cdots,Js)$ of columns of $\bmf{W}_s^{(0)}$ and row of $\bmf{W}_s^{(1)}$, i.e. the neurons, will not affect the output for any input $\bmf{x}$. Due to the permutation invariance, a neuron indexed by $j$ from one FCNN is unlikely to correspond to a neuron with the same index from another FCNN. Thus, we should match neurons from different FCNNs before aggregate them into a collection of global neurons $\{\bmf{\theta}_{i} \in \R^{D + K}\}_{i=1}^{J}$.
\section{Probabilistic Modeling}
PFNM models the generative process of observed local neurons using a Beta Bernoulli process, which is described in the Appendix A, to infer the global model while accounting for the inherent permutation invariance.  First, consider the collection of global neurons as prior which are sampled from a Beta process with a base measure: $M = \sum_i m_i \delta_{\bmf{\theta}_i} \sim \text{BP}(1,\gamma_0 H)$ and $\bmf{\theta}_i \sim H$, 
where $\gamma_0$ is the mass parameter, $m_i$ are the stick-breaking weights, $H$ is the base measure and chosen as a multivariate Gaussian distribution $H = \mathcal{ N}(\bmf{\mu}_0, \bm{\Sigma_0} )$ with $\bmf{\mu}_0 \in \R^{D+K}$ and diagonal $\bm{\Sigma_0}=\bmf{I}\sigma_0^2$. 
\par The Bernoulli process is then used by each local model to select a subset of global neurons:$
\mathcal{T}_s = \sum_{i} a_{si} \delta_{\bmf{\theta}_i} \vert M \sim \text{BeP}(M) \text{ for } s=1,\ldots,S$, where $a_{si} \vert m_i \sim \operatorname{Bern}(m_i) $ is a random measure representing a subset of global neurons contained in local model $s$. Considering the inherent permutation invariance of local model, it denotes $\bmf{A} = \{A_{ij}^s\}_{s,i,j}$ as the assignment variables, $\sum_{j} A^s_{ij} = a_{si} \in \{0,1\}$, $\sum_{i} A^s_{ij} = 1$. Finally, observed local neurons in model $s$ are treated as noisy measurements of global neurons under permutation invariance:
\begin{equation}
\label{eq:assignment_parametrize}
    \begin{split}
        \bmf{w}_{sj}\mid  \bmf{A}^s, \bmf{\theta} \sim \mathcal{ N}(\sum_i A^s_{ij}\bmf{\theta}_i, \bm{\Sigma_s}) \text{ for } j=1,\ldots,J_s, 
    \end{split}
\end{equation}
where $J_s = card(\mathcal{T}_s)$, $\bm{\Sigma_s} = \bm{I}\sigma_s^2$ is also diagonal and represents the noise. The noise is usually caused by estimation error due to finite sample sizes or variations in the distribution of each local dataset. $A_{ij}^s = 1$ indicates that $\bmf{w}_{sj}$ is matched to $\bmf{\theta}_i$, i.e. $\bmf{w}_{sj}$ is the local neuron realization of the global neuron $\bmf{\theta}_i$; $A_{ij}^s = 0$ indicates the inverse. 
\par After the modeling, it is natural to infer the global neurons by maximizing the posterior probability of the global neurons given local neurons (likelihoods) under the permutation invariance:
\begin{equation}\label{max_pos}
\begin{split}
\max_{\{ \bmf{\theta}_i \}, \{ \bmf{A}^s \} }  & P(\{ \bmf{\theta}_i\}, \{\bmf{A}^s\} | \{\bmf{w}_{sj}\}) \propto \\
 & P(\{\bmf{w}_{sj}\} | \{\bmf{\theta}_i\}, \{\bmf{A}^s\} )P(\{\bmf{A}^s\})P(\{\bmf{\theta}_i\}),
\end{split}
\end{equation}
define $\bmf{Z}_i = \{(s, j) \vert A_{ij}^s = 1 \}$ be the index set of local neurons assigned to $i$-th global neuron, and taking negative logarithm it can obtain:
\begin{equation}\label{neg_log_pos}
\begin{split}
\min_{\{ \bmf{\theta}_i \}, \{ \bmf{A}^s \} }  & - \log(P(\{ \bmf{A}^s \})) \\
& -\sum_i \bigg ( \sum_{z \in \bmf{Z}_i} \log(p(\bmf{w}_{z} \vert \bmf{\theta}_i)) + \log(p(\bmf{\theta}_i)) \bigg )
\end{split}
\end{equation}
where $P(\{ \bm{A}^s \})$ is interpreted by IBP and demonstrated in Appendix B.
Given $\{ \bm{A}^s \}_{s=1}^S$, the closed form of  $\{ \bmf{\theta}_i \}$ can be estimated according to the Gaussian-Gaussian conjugacy:
\begin{equation}\label{Gauss_conj}
\bmf{\theta}_i =  \frac{\bmf{\mu}_0 / \sigma_0^2 + \sum_{s,j}\bmf{A}_{i,j}^s \bmf{w}_{sj} / \sigma^2_s}{1 / \sigma_0^2 + \sum_{s,j} \bmf{A}^s_{i,j} / \sigma^2_s} \, \text{ for } i = 1, \cdots, J.
\end{equation}
Taking equation~(\ref{Gauss_conj}) into objective~(\ref{neg_log_pos}), it can cast optimization only with respect to $\{ \bm{A}^s \}_{s=1}^S$. Unfortunately, solving all local assignments together leading an NP-hard combinatorial optimization problem. Thus, PFNM applies an alternative optimization process to solve the problem.

\subsubsection{Alternative Optimization}
\par PFNM iteratively optimizes one local assignment variable $\bmf{A}^{s'}$ at a time by fixing all other assignment variables $\{A^{s}_{i,j} \}_{i,j, s \in -s'}$, where $-s'$ denotes "all but $s'$". It aims to formulate a linear sum assignment problem with current local assignment $\sum_{i,j}C^{s'}_{i,j}A^{s'}_{i,j}$ in each iteration, where $\{C^{s'}_{i,j} \}_{i,j}$ denotes the cost specification. The Hungarian algorithm is then used to solve the linear sum assignment problem. In each iteration, it divides terms in objective~(\ref{neg_log_pos}) into two parts: one is $i = 1, \cdots, J_{-s'}$, where $J_{-s'} = \max\{i: \bmf{A}^{s}_{ij} = 1$, for $s\in -s', j = 1, \dots, J_{s} \}$, this part denotes active global neurons estimated from $-s'$; another is $i = J_{-s'}+1, \cdots, J_{-s'}+J_{s'}$, it denotes new global neuron from current local model. Then it has the following proposition describes the assignment cost $\{C^{s'}_{i,j}\}_{i,j}$:
%\newtheorem{pro}{Proposition}
%\newtheorem*{proof}{Proof}
\begin{proposition}
\label{pro:pfnm_cost}
The assignment cost specification $C^{s'}_{i,j}$ for finding $\{ \bmf{A}^{s'} \}$ is $C_{i,j}^{s'}=$
\begingroup
\allowdisplaybreaks
\begin{equation}
\label{pfnm_cost}
\begin{cases}
\begin{split}
2\log{\frac{S-n_i^{-s'}}{n_i^{-s'}}}   & -\frac{\bignorm{\frac{\bmf{\mu}_0}{\sigma_0^2} + \frac{\bmf{w}_{s'j}}{\sigma^2_{s'}} + \sum_{s\in {-s'},j} \bmf{A}_{i,j}^s\frac{\bmf{w}_{sj}}{\sigma_s^2}}^2}{\frac{1}{\sigma_0^2} + \frac{1}{\sigma^2_{s'}} + \sum_{s\in {-s'},j} \bmf{A}_{i,j}^s\frac{1}{\sigma_s^2}} \\
& + \frac{\bignorm{\frac{\bmf{\mu}_0}{\sigma_0^2} + \sum_{s\in {-s'},j} \bmf{A}_{i,j}^s\frac{\bmf{w}_{sj}}{\sigma_s^2}}^2}{\frac{1}{\sigma_0^2} + \sum_{s\in {-s'},j} \bmf{A}_{i,j}^s\frac{1}{\sigma_s^2}} , i \leq J_{-s'} 
\end{split}
\\
\begin{split}
2\log{\frac{i-J_{-s'}}{\gamma_0/S}} & -\frac{\bignorm{\frac{\bmf{\mu}_0}{\sigma_0^2} + \frac{\bmf{w}_{s'j}}{\sigma^2_{s'}} }^2}{\frac{1}{\sigma_0^2} + \frac{1}{\sigma^2_{s'}} } \\
& + \frac{\bignorm{\frac{\bmf{\mu}_0}{\sigma_0^2} }^2}{\frac{1}{\sigma_0^2} }, \qquad\quad J_{-s'}<i \leq J_{-s'} + J_{s'},
\end{split}
\end{cases}
\end{equation}
\endgroup
where $n_i^{-s'} = \sum_{s \in -s',j} \bmf{A}^s_{i,j}$ denotes the number of local neurons were assigned to global neuron $i$ outside of $s'$. 
\end{proposition}

The proof can be found in Appendix B.
\section{Analysis of PFNM}
Before we discuss, we have the following proposition:
\begin{proposition}\label{pro:gauss_conjugate}
For any prior of a global neuron $\bmf{\theta}_i \sim \mathcal{ N}(\bmf{\mu}_0, \bm{I}\sigma_0^2 )$, when it is assigned with a likelihood observed local neuron $\bmf{w}_{s'j} \sim \mathcal{ N}(\bmf{\theta}_i, \bm{I}\sigma_{s'}^2)$, the posterior distribution of this global neruon is $\bmf{\theta}_i \vert \bmf{w}_{s'j} \sim \mathcal{ N}\Bigl(\frac{\bmf{\mu}_0/\sigma_0^2 + \bmf{w}_{s'j}/\sigma^2_{s'}}{1/\sigma_0^2 + 1/\sigma^2_{s'}}, \bm{I}\bigl(\frac{1}{1/\sigma_0^2 + 1/\sigma^2_{s'}}\bigr)\Bigr)$; when it has been assigned local neurons from other assignments $\bmf{Z}_i^{-s'} = \{(s, j) \vert A_{ij}^s = 1, s \in -s'\}$, the posterior distribution is $\bmf{\theta}_i \vert \bmf{Z}_i^{-s'} \sim \mathcal{ N}(\frac{\bmf{\mu}_0/\sigma_0^2 + \sum_{s\in {-s'},j} A_{i,j}^s\bmf{w}_{sj}/\sigma_s^2}{1/\sigma_0^2 + \sum_{s\in {-s'},j} A_{i,j}^s/\sigma_s^2}, \bm{I}(\frac{1}{1/\sigma_0^2 + \sum_{s\in {-s'},j} A_{i,j}^s/\sigma_s^2}))$.
\end{proposition}
\par The proof can be found in Appendix C.

\par Terms of equation~(\ref{pfnm_cost}) on the left are due to $P(\bmf{A}^{s'} \vert \{ \bmf{A}^{-s'} \})$. At initial stage of iteration all $n_i^{-s'}$ are identical. So the main differences compared by cost $C^{s'}_{i,j}$ induced from right term. 
\begin{definition}\label{defi:sms}
(Standardized mean square) For any multivariate Gaussian distribution $\mathcal{N}(\bmf{\mu}, \bm{I}\sigma^2)$, we call $\norm{\bmf{\mu}}^2/\sigma^2$ as standardized mean square of this distribution, and denote it as $\mathbb{S}(\mathcal{N}) = \norm{\bmf{\mu}}^2/\sigma^2$.
\end{definition}
The proposition~\ref{pro:gauss_conjugate} shows that when $i \leq J_{-s'}$, the right term is the difference in standardized mean square between distribution $p(\bmf{\theta}_i \vert \bmf{Z}_i^{-s'})$ and this distribution assigned with local likelihood $p(\bmf{\theta}_i \vert \bmf{Z}_i^{-s'}, \bmf{w}_{s'j})$; when $J_{-s'}<i \leq J_{-s'} + J_{s'}$, the right term is the difference of standardized mean square between prior distribution $p(\bmf{\theta}_i)$ and this distribution assigned with local likelihood $p(\bmf{\theta}_i \vert \bmf{w}_{s'j})$. So, by comparing each cost term $C^{s'}_{i,j}$, it essentially compares the standardized mean square difference between the $i$-th global neuron distribution (either estimated from $-s'$ or a new one from the current local model) and this distribution assigned with local neuron $w_{s'j}$. However, we have the following proposition shows the issue caused by the cost term:
\begin{proposition}\label{SMS_differnce_issue}
For a global neuron distribution $p(\bmf{\theta}) = \mathcal{ N}(\bmf{\mu}_0, \bm{I}\sigma_0^2)$, after it is assigned with a likelihood $p(\bmf{w}) = \mathcal{ N}(\bmf{\theta}, \bm{I}\sigma^2)$, the standardized mean square difference between $p(\bmf{\theta})$ and $p(\bmf{\theta} \vert \bmf{w})$ is proportion to $\norm{\bmf{w}}^2/\sigma^2$.
\end{proposition}
\begin{proof}
Denote the mean and covariance matrix of $p(\bmf{\theta} \vert \bmf{w})$ as $\tilde{\bmf{\mu}}$ and $\bmf{I}\tilde{\sigma}^2$. From definition~\ref{defi:sms}, we can write the the standardized mean square difference between $p(\bmf{\theta})$ and $p(\bmf{\theta} \vert \bmf{w})$ as
\begin{align}\label{eq:sms_differ}
 &\mathbb{S}(p(\bmf{\theta})) - \mathbb{S}(p(\bmf{\theta} \vert \bmf{w})) \nonumber\\
  = & \frac{\norm{\bmf{\mu}_0}^2}{\sigma_0^2} - \frac{\norm{\tilde{\bmf{\mu}}}^2}{\tilde{\sigma}^2} \nonumber\\ 
 = & \log \frac{\exp{(\frac{-\bmf{\theta}^T\bmf{\theta}+2\bmf{\theta}^T\bmf{\mu}_0}{2\sigma_0^2})}}{p(\bmf{\theta})} - \log \frac{\exp{(\frac{-\bmf{\theta}^T\bmf{\theta}+2\bmf{\theta}^T\tilde{\bmf{\mu}}}{2\tilde{\sigma}^2})}}{p(\bmf{\theta} \vert \bmf{w})} \nonumber\\
  = & \log \frac{\exp{(\frac{-\bmf{\theta}^T\bmf{\theta}+2\bmf{\theta}^T\bmf{\mu}_0}{2\sigma_0^2})}}{p(\bmf{\theta})} \cdot \frac{p(\bmf{\theta} \vert \bmf{w})}{\exp{(\frac{-\bmf{\theta}^T\bmf{\theta}+2\bmf{\theta}^T\tilde{\bmf{\mu}}}{2\tilde{\sigma}^2})}}.
\end{align}
Because
\begin{align}
    & \exp(\frac{-\bmf{\theta}^T\bmf{\theta}+2\bmf{\theta}^T\bmf{\mu}_0}{2\sigma_0^2}- \frac{-\bmf{\theta}^T\bmf{\theta}+2\bmf{\theta}^T\tilde{\bmf{\mu}}}{2\tilde{\sigma}^2}) \nonumber\\
    = & \exp(\bmf{\theta}^T\bmf{\theta}(-\frac{1}{2\sigma_0^2} + \frac{1}{2\tilde{\sigma}^2}) + \theta^T(\frac{\bmf{\mu}_0}{\sigma_0^2} - \frac{\tilde{\bmf{\mu}}}{\tilde{\sigma}^2})),
\end{align}
from proposition~\ref{pro:gauss_conjugate}, we can obtain
\begin{align}
    \frac{1}{2}(-\frac{1}{\sigma_0^2} + \frac{1}{\tilde{\sigma}^2}) = \frac{1}{2 \sigma^2}, \qquad \frac{\bmf{\mu}_0}{\sigma_0^2} - \frac{\tilde{\bmf{\mu}}}{\tilde{\sigma}^2} = -\frac{\bmf{w}}{\sigma^2},
\end{align}
and from Bayesian theory
\begin{equation}\label{eq:baye_theory}
    p(\bmf{\theta} \vert \bmf{w}) \propto p(\bmf{\theta}) p(\bmf{w} \vert \bmf{\theta}),
\end{equation}
thus we have
\begin{align}
    \mathbb{S}(p(\bmf{\theta})) - \mathbb{S}(p(\bmf{\theta} \vert \bmf{w})) & \propto \log\frac{p(\bmf{w})}{\exp(\frac{-\bmf{\theta}^T\bmf{\theta}+2\bmf{\theta}^T\bmf{w}}{2\sigma^2})} \nonumber\\
    & \propto -\frac{\bmf{w}^T\bmf{w}}{2\sigma^2}.
\end{align}
\end{proof}
From proposition~\ref{SMS_differnce_issue}, we can know that the cost term $C^{s'}_{i,j}$ are proportion to term only contains local neuron. As a consequence, for one fixed local neuron $\bmf{w}_{s'j}$ and two different global neurons $\bmf{\theta}_i$ and $\bmf{\theta}_{i^*}$, the cost does not discriminate global neurons $\bmf{\theta}_i$ and $\bmf{\theta}_{i^*}$. Hence, the cost specifications induced from original PFNM can't find optimal solutions for $\{ \bmf{A}^{s'} \}$.
\subsection{Kullback-Leibler Divergence}
From the above analysis, we know that the cost specification $C^{s'}_{i,j}$ induced from PFNM omits information of global neuron, i.e. the $i$ index in the cost. So, it is natural to fix the issue by adding a regularized term that contains both information of $i$ and $j$. As we point out before, the cost term $C^{s'}_{i,j}$ essentially is the standardized mean square difference between $i$-th global distribution and this distribution assigned with local neuron $w_{s'j}$. From the probabilistic perspective, minimizing the cost is meant to hope these two distributions are as close as possible. So how to measure the distance between two distributions? Kullback-Leibler becomes the first answer that gets into mind. Can KL-divergence fix the issues caused by PFNM? To answer this question, we have the following proposition:
\begin{proposition}\label{propo:KL_term}
For a global neuron distribution $p(\bmf{\theta})$, after it is assigned with a likelihood $p(\bmf{w})$, the Kullbackâ€“Leibler Divergence Penalty between $p(\bmf{\theta})$ and $p(\bmf{\theta} \vert \bmf{w})$ is proportional to $\mathbb{E}_{p(\bmf{\theta})} \bigl[ \log p(\bmf{w}\mid \bmf{\theta}) \bigr].$
\end{proposition}
\begin{proof}
 \begin{align}
     &  \operatorname{KL}\bigl(p(\bmf{\theta}) \Vert p(\bmf{\theta} \vert \bmf{w}) \bigr) \nonumber \\
    = & \int p(\bmf{\theta}) \log \frac{p(\bmf{\theta})}{p(\bmf{\theta} \vert \bmf{w})} \,\rm{d}\bmf{\theta}  \nonumber\\
    \propto &  -\int p(\bmf{\theta})\log p(\bmf{w}\mid \bmf{\theta})\,\rm{d}\bmf{\theta} \nonumber \\
    = &  -\mathbb{E}_{p(\bmf{\theta})} \bigl[ \log p(\bmf{w}\mid \bmf{\theta}) \bigr]
 \end{align}
\end{proof}
From above proposition, KL divergence between two distribution is essentially proportion the expectation of the assigned local likelihood distribution, and the expectation is taken over the global distribution $p(\bmf{\theta})$. That means the KL divergence contains both information of local and global neuron.
\par Thus, to fix the drawback induced from PFNM,  we can formulate a new cost specifications that regularize original cost specification with the KL penalty, i.e.,  $\widetilde{\bmf{C}}^{s'}_{i,j} = $
\begin{equation}
\label{eq:kl_cost}
\begin{cases} 
\begin{split}
\bmf{C}^{s'}_{i,j} + \lambda
\operatorname{KL}\Bigl(p(\bmf{\theta}_i \vert \bmf{Z}_i^{-s'}) \big\Vert p(\bmf{\theta}_i \vert \bmf{Z}_i^{-s'}, \bmf{w}_{s'j})\Bigr),
i \leq L_{\setminus j_0}, \quad
 \end{split}\\
\begin{split}
\bmf{C}^{s'}_{i,j} + \lambda
\operatorname{KL}\Bigl(p(\bmf{\theta}_i) \big\Vert p(\bmf{\theta}_i \vert \bmf{w}_{s'j})\Bigr), & L_{\setminus j_0} < i \leq L_{\setminus j_0} + L_j .
\end{split}
\end{cases}    
\end{equation}
\par where the coefficient $\lambda$ is the adjusting ratio. And the KL divergence between two multivariate normal distributions can be calculated by the following Lemma:
\begin{lemma}
The Kullback-Leibler divergence between $\mathcal{ N}_x(\bmf{\mu}_x, \bmf{\Sigma}_x)$ and $\mathcal{ N}_y(\bmf{\mu}_y, \bmf{\Sigma}_y)$, is:
\begin{equation}
    \frac{1}{2}[\operatorname{tr}(\bmf{\Sigma}_y^{-1}\bmf{\Sigma}_x) + (\bmf{\mu}_y - \bmf{\mu}_x)^T\bmf{\Sigma}_y^{-1}(\bmf{\mu}_y - \bmf{\mu}_x)-(D+K) + \ln{\frac{|\bmf{\Sigma}_y|}{|\bmf{\Sigma}_x|}}],
\end{equation}
where $D+K$ is the dimension of $\bmf{\mu}_x$ and $\bmf{\mu}_y$.
\end{lemma}
The proof can be found in \cite{duchi2007derivations}.
\par We call our new method as neural aggregation with full information (NAFI). And the process of NAFI can be summarized in algorithm~\ref{algori:nafi}. 
\par As demonstrated by the study in \cite{wang2020federated}, directly applying the matching algorithms fails on deep architectures designed for more complex tasks. To address this issue, we extend NAFI to a layer-wise matching scheme, which can be found in Appendix D.
\begin{algorithm}
\caption{The process of NAFI}
\label{algori:nafi}
\begin{algorithmic}[1]
    \REQUIRE ~~\\
    Local weights $\bmf{w}_{sj}$ from $S$ local models;
    \ENSURE ~~\\
    Global weights $\{ \bmf{\theta}_i$ \},  matching assignments $\{ \bmf{A}^s \}_{s=1}^S$;
    \FOR{$k$ = 1, 2, 3, $\cdots$}
        \FOR{$s'$ = 1, 2, $\cdots$, $S$}
            \STATE  Fixing assignments $\{ \bmf{A}^{s}_{i,j} \}_{i,j, s \in -s'}$, construct corresponding global model via equations~(\ref{Gauss_conj}):  $\tilde{\bmf{\theta}}_i = \frac{\bmf{\mu}_0/\sigma_0^2 + \sum_{s\in {-s'},j} \bmf{A}_{i,j}^s \bmf{w}_{sj}/\sigma_s^2}{1/\sigma_0^2 + \sum_{s\in {-s'},j} \bmf{A}_{i,j}^s/\sigma_s^2}$;
 	        \STATE Obtain the assignment cost $\{\widetilde{\bmf{C}}^{s'}_{i,j}\}_{i,j}$ via equation~(\ref{eq:kl_cost});
 	        \STATE Solve the linear assignment problem via Hungarian algorithm to obtain  $\{ \bmf{A}^{s'}_{i,j}  \}_{i,j}$.
        \ENDFOR
        \STATE Use $\{ \bmf{A}^s \}_{s=1}^S$ to update the global model.
    \ENDFOR
\end{algorithmic}
\end{algorithm}

\section{Experiments}
To evaluate the efficiency of the proposed NAFI, we presents an empirical study of NAFI and compares it with PFNM, FedAvg \citep{mcmahan2017communication}, and FedProx \citep{li2018federated}. Our experiments are conducted over three different datasets with three type of neural networks, that is, FCNN, shallow CNN and U-net. And the experiments below show that our framework can aggregate multiple NNs into an efficient global one\footnote{The code will be released if this paper is accepted}. 

\subsubsection{Datasets, models and metrics}  We evaluate our algorithm on three datasets: MINST, CIFAR 10 and Carvana Image Masking Challenge (CIMC). MINST and CIFAR-10 are standard image classification datasets and each contains ten classes on handwriting digits and objects in real life respectively. CIMC is a binary semantic segmentation dataset composed of photos of cars, and the task is to split out the car and the background.  For MNIST, we apply a FCNN model and evaluate it with accuracy; for CIFAR 10, we apply a ConvNet with with 3 convolutional and 2 fully-connected layer and evaluate it with accuracy; for CIMC, we apply the U-net \citep{ronneberger2015u} architecture and evaluate it with dice coefficient. 

\subsubsection{Partition strategies of local data} To simulate a federated learning scenario,
we partition each dataset in a heterogeneous strategy where the number of data points and class proportions in each local model is unbalanced. We follow prior works \cite{yurochkin2018scalable} in the heterogeneous partition of local data for three datasets, which apply $K$-dimensional Dirichlet distribution $Dir(\alpha)$ to create non-iid data, with a smaller $\alpha$ indicating higher data heterogeneity. Specifically, for dataset with class number $K$, we sample the proportion of the instances of class $k$ to local model $s$, $p_{k,s}$, via $p_{k,s} \sim Dir_k(\alpha)$. For MNIST and CIFAR10, $K = 10$. For CIMC, $K = 1$ as it is a binary semantic segmentation dataset. In each dataset, we execute 5 trials to obtain the mean and standard deviation of the performances.


\subsubsection{Baselines}  Our method is compared to the original PFNM, FedAvg, and FedProx. FedAvg and FedProx are executed in local neural networks that have been trained using the same random initialization as proposed by  \cite{mcmahan2017communication}. We note that while a federated averaging variant without shared initialization is likely to be more realistic when attempting to aggregate pre-trained models, it performs significantly worse than all other baselines. Considering the high heterogeneity in each trial, we set $\lambda$ in a wide grid (from $10^{-8}$ to 1) and choose the best result for NAFI. We subsequently check the sensitivity of $\lambda$ and find it only have tiny fluctuation.

\begin{table}[htb]
\caption{Hyperparameter settings for training neural networks}
 \label{table:train_setting}
\begin{center}
 \begin{tabular}{cccc}
  \toprule
  & MNIST & CIFAR-10 & CIMC\\
  \cmidrule(r){1-4}
  Model & FCNN & ConvNet & U-net \\
  Optimizer & Adam & SGD & RMSprop \\
  Learning rate & 0.01 & 0.01 & 0.01\\ 
  Size of minibatch & 32 & 32 & 3 \\ 
 Epochs & 10 & 10 & 3 \\ 

 \bottomrule
 \end{tabular}
\end{center}
\end{table}

 \begin{table*}[h]
\caption{Test accuracy of FCNN in MNIST}
 \label{table:mnist}
\begin{center}
 \begin{tabular}{cccccccc}
  \toprule
   & S & N & Local NN & FedAvg & FedProx & PFNM & NAFI\\
  \midrule
   \multirow{4}{*}{nets} & 15 & 1 & 71.9 $\pm$ 2.57 & 75.47 $\pm$ 5.90 & 75.65 $\pm$ 5.93 & 83.93 $\pm$ 0.14 & \textbf{87.34 $\pm$ 0.57}\\
   ~ & 20 & 1 & 69.44 $\pm$ 2.50 & 75.02 $\pm$ 4.03 & 75.17 $\pm$ 3.99 & 83.23 $\pm$ 3.31 & \textbf{86.73 $\pm$ 2.15}\\
   ~ & 25 & 1 & 67.99 $\pm$ 2.00 & 75.46 $\pm$ 3.33 & 75.24 $\pm$ 3.30 & 84.87 $\pm$ 1.66 & \textbf{86.46 $\pm$ 2.56} \\
   ~ & 30 & 1 & 65.90 $\pm$ 2.66 & 73.43 $\pm$ 4.49 & 73.11 $\pm$ 4.44 & 83.39 $\pm$ 2.23 & \textbf{86.23 $\pm$ 2.57} \\
   \midrule
   \multirow{2}{*}{layers} & 10 & 2 & 73.21 $\pm$ 3.05 & 66.56 $\pm$ 6.82 & 66.34 $\pm$ 6.75 & 79.09 $\pm$ 4.73 & \textbf{84.58 $\pm$ 4.54} \\
   ~ & 10 & 3  & 70.28 $\pm$ 2.97 & 52.01 $\pm$ 6.52 & 51.79 $\pm$ 6.49 & 60.71 $\pm$ 4.59 & \textbf{72.04 $\pm$ 4.75} \\
 \bottomrule
 \end{tabular}
\end{center}
\end{table*}

\begin{table*}[h]
\caption{Test accuracy of Convnet in CIFAR10}
 \label{table:cifar10}
\begin{center}
 \begin{tabular}{ccccccc}
  \toprule
  & S  & Local NN & FedAvg & FedProx & PFNM & NAFI\\
  \midrule
   \multirow{4}{*}{nets} & 5 & 25.21 $\pm$ 2.30 & 51.26 $\pm$ 2.97 & 51.43 $\pm$ 3.01 & 50.39 $\pm$ 0.94 & \textbf{52.56 $\pm$ 0.81} \\
   ~ & 10 & 18.92 $\pm$ 1.41 & 46.78 $\pm$ 2.36 & 46.94 $\pm$ 2.32 & 46.27 $\pm$ 1.73 & \textbf{47.46 $\pm$ 1.42} \\
   ~ & 15 & 15.85 $\pm$ 0.51 & 42.40 $\pm$ 2.25 & 42.06 $\pm$ 2.20 & 42.82 $\pm$ 2.46 & \textbf{45.72 $\pm$ 1.35} \\
   ~ & 20 & 14.11 $\pm$ 0.53 & 34.20 $\pm$ 2.54 & 34.02 $\pm$ 2.52 & 42.61 $\pm$ 2.07 & \textbf{44.96 $\pm$ 1.93} \\
 \bottomrule
 \end{tabular}
\end{center}
\end{table*}

\begin{table*}[htb]
\caption{Test dice coefficient of U-net in CIMC}
 \label{table:cimc}
\begin{center}
 \begin{tabular}{ccccccc}
  \toprule
  & S & Local NN & FedAvg & FedProx & PFNM & NAFI\\
  \midrule
  %\multirow{2}{*}{nets} & 4 & 79.70 $\pm$ 13.26 &  &  & 95.05 $\pm$ 5.39  & \textbf{97.19 $\pm$ 1.95} \\
   \multirow{2}{*}{nets} & 8 & 67.60 $\pm$ 9.38 & 53.28 $\pm$ 10.63 & 41.53 $\pm$ 0.82 & 90.31 $\pm$ 2.90  & \textbf{96.47 $\pm$ 0.76} \\
   ~ & 16 & 27.84 $\pm$ 12.16 & 42.62 $\pm$ 0.87 & 48.50 $\pm$ 9.52 & 75.47 $\pm$ 3.54  & \textbf{83.46 $\pm$ 3.41}
   \\
 \bottomrule
 \end{tabular}
\end{center}
\end{table*}

\begin{figure*}[htb]
\centering
\includegraphics[width=1\textwidth]{./figs/sensitivity_analysis4.pdf}
\centering
\caption{\textbf{KL regularization coefficients sensitivity analysis}}
\label{fig3}
\end{figure*}


\subsubsection{Training setup}  We use PyTorch \citep{paszke2017automatic} to implement these networks and train them by the Adam  \citep{kingma2014adam}, SGD \citep{bottou2010large} and RMSprop \citep{hinton2012neural} with default hyperparameters. All hyperparameter settings are summarized in Supplement table~\ref{table:train_setting}.



% \vspace{-0.42in}
\subsubsection{Performance Overview} 
 For applications of Federated Learning in real world, the discrepancy of data distribution among local models and communication cost will inevitably increase as the number of local models increases due to the variability of data generation paradigms in the system \citep{li2020federated}. Consequently, the model fusion problem is harder under configuration of more local models. This suggests that testing fusing algorithms for various numbers of local models is important.  For MNIST, we firstly apply various methods such as FedAVG, FedProx and PFNM, with 15, 20, 25 and 30 local local models with one hidden layer FCNN. Local NN in Table~\ref{table:mnist} reports the average of separately tested network accuracies. The lower extremes of aggregating are determined by the performance of local NNs. As the number of local models $S$ increases, the performance of each method decreases. Because each local model in our partition setting has fewer training data and probably fewer labels as $S$ rises. As shown in Table~\ref{table:mnist}, accuracy of NAFI on MNIST is 4\% higher than PFNM on average. We also examine how the performance of all methods is impacted by the number of hidden layers $N$. We train 10 neural networks with 2 and 3 hidden layers respectively and then fuse them using various methods. Similarly, NAFI achieves the best performance among all the methods. In addition, the priority of NAFI increases as deepness of the neural networks increases. This can be explained by an accumulating error effect. On the one hand, we discovered that PFNM is unable to discriminate those global neurons because the cost specification does not include global neuron information, while NAFI equipped with KL divergence can correct this. On the other hand, the fusing process of neural networks is actually going layer-by-layer, from the first layer to the last layer. When fusing local models for each layer, the drawback of PFNM described above takes effect, and global neurons are not generated correctly. As a result, as the iteration process directed by the Hungarian algorithm continues, the incorrectness of PFNM in picking global neurons is superimposed, and the performance disparity between PFNM and NAFI is amplified as presented in Table~\ref{table:mnist}.

\par CNNs are far more commonly used in real-world applications than FCNNs. Consequently for this, we apply ConvNet (2 convolutional layers and 3 fully connected layers) on 5, 10, 15, and 20 local models and use various methods to fuse the models trained in CIFAR 10's heterogeneous dataset partition. We apply PFNM and NAFI in an iteratively layer-wise way, so we also set the communications rounds of FedAvg and FedProx to 5 equal to the number of layers in ConvNet for equality. As shown in Table~\ref{table:cifar10}, NAFI outperforms the other methods on fusing convolutional neural networks. Concretely, the accuracy of NAFI on CIFAR 10 is 3\% higher than PFNM on average for a various number of local models.

\par The architectures for advanced deep learning methods are frequently complex, typically having skip connections between layers and having far more layers than three. It is unclear whether large-scale neural networks can be fused by those model fusion methods as well as those lightweight nets because those complex networks are rarely tested in previous Federated Learning methods.   U-Net \citep{ronneberger2015u} is widely used on medical image segmentation \citep{du2020medical}, and medical images such as tumor scan images generated by multiple institutions are generally forbidden to be exchanged for privacy issues. Therefore, it is critical to federate well-known image segmentation algorithms. To this end, we apply several popular federated learning methods on U-Net. 
The U-Net architecture is described in \cite{ronneberger2015u}.
Technically, we matched aggregate neurons over skip connections in this study. Additionally, U-Net distinguishes itself from general FCNNs by employing the batch normalization technique on feature maps.
As shown in Table~\ref{table:cimc}, FedAvg and FedProx have very subpar performances and cannot converge to a stationary result in such complex network architecture (the results go up and down during communication rounds). The result shown in the table is taken from the best result during all 19 communication rounds which are equal to the number of layers in U-net.
As shown in Table~\ref{table:cimc}, the priority of NAFI is significantly more important among popular Federated Learning methods than it is in the case of 2- or 3-layer neural networks. With 8 local models, NAFI on CIMC has accuracy that is at least 7\% higher than PFNM, and accuracy that is almost 8\% higher with 16 local models.

\subsection{Sensitivity Analysis}
\par The weight of the KL-divergence term, $\lambda$, is a hyper-parameter that we introduce into NAFI. Therefore, it is essential to demonstrate the sensitivity of $\lambda$. Here, we set $\lambda$ to various positive values for NAFI applied on fusing FCNNs and CNNs trained in MNIST and CIFAR10 respectively. We also take the impact of the number of local models into account.  As shown in figure~\ref{fig3}, the heat map indicates the accuracy on the
test data, and for various number of clients, there is only a tiny fluctuation of prediction accuracy for a fused global model when $10^{-8} \leq \lambda \leq 1$. Although NAFI with $\lambda = 10^{-8}$ performs significantly worse than NAFI with $10^{-3} \leq \lambda \leq 0.5$,  the fused global model maintains a high level of performance.  When $\lambda = 1$, the performance of the fused global model drops sharply, this indicates that $\lambda$ should not set too high.  To summarize, NAFI is robust on the hyperparameter $\lambda$ under a variety of conditions. 

\section{Conclusion}
\label{conclusion}
In this paper, we have proposed a new federated neural matching method by incorporating the global information into PFNM.  It is empirically shown that the new method outperforms other state-of-the-art algorithms for federated learning of neural networks. In future work, it is interesting to extend our model to more advanced architectures. Additionally, the success of KL-divergence suggests it is also likely to try other divergences of probability distributions.

% \bibliographystyle{plain}
% \bibliographystyle{unsrt} 
\clearpage
\bibliography{references}


\end{document}
