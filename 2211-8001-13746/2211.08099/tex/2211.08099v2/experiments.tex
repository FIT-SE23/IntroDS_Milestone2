
\input{tables/main_table}

\input{tables/new_finetuned_tasks}


\section{Experiments}

\input{tables/ablation}


\subsection{Experimental Setup}\label{sec:setup}

We performed extensive experiments to validate the performance of the zero-shot generalization of our UD. We follow the same zero-shot setting as T0~\citep{T0-paper} by training on multi-task datasets and evaluating a held-out set of tasks that are never seen during training. 

\paragraph{Datasets}
The original T0 training set consists of 38 tasks of 8 different types.
There are in total 21/38 discriminative training tasks, with which we train the UD.
The evaluation set covers four types of tasks, including natural language inference (RTE~\citep{2005_RTE}, CB~\citep{de2019_CB}, ANLI/R1-R3~\citep{NieWDBWK20_ANLI}), coreference resolution (WSC~\citep{WSC2012}, Winogrande~\citep{SakaguchiBBC20_winogrande}), sentence completion (COPA~\citep{COPA2011}, StoryCloze~\citep{story_cloze}, Hellaswag~\citep{ZellersHBFC19_hellaswag}), and word sense disambiguation (WiC~\citep{wic-paper}).
Following T0, we use accuracy on the validation split as the evaluation metric.
For prompt-based baselines, we report the average accuracy over multiple prompts for each test task.
Besides, we also evaluate zero-shot performance on 13 BigBench~\cite{bigbench} tasks, which are also adopted by T0~\cite{T0-paper}, %
and 22 BBH tasks~\cite{bbh}, which are adopted by Flan-T5~\cite{flant5}.




\paragraph{Baselines}
We primarily compare our method with T0~\citep{T0-paper}, which is a generative approach.
Another baseline is prompting ELECTRA~\cite{xia2022prompting} which is a recent work on discriminative modeling.
Since it was proposed in a different setting (i.e., a  few-shot setting or direct zero-shot inference without any finetuning), we reproduced their method under our multitask zero-shot setting for comparison.

For a fair comparison, we follow T0 to use the T5-V1.1-LM-Adapted~\citep{T5-paper} as the backbone model, and we experimented with three different scales, respectively 800M, 3B, and 11B. 
For UD, it only makes use of the encoder of T5-v1.1 and additionally replaces the output layer with a classification head.



In addition, we provide reported zero-shot results of several large language models (with hundreds of billions of parameters) for reference, including GPT-3~\cite{gpt3-paper}, GLaM~\cite{glam}, PaLM~\cite{palm}, and FLAN~\cite{FLAN}. We also reproduce zero-shot results of a recent work Flan-T5~\cite{flant5} by evaluating their released checkpoints on BBH tasks\footnote{T0 test sets are included in Flan-T5's  training data sets, so we can't test its zero-shot performance on those data sets.}. Note that Flan-T5's training data sets are much broader than ours, so results for Flan-T5 here are only for reference but not a fair comparison.



\paragraph{Training}
During training, we truncate the input sequence to 256 tokens and use a batch size of 256. For optimization, we use the Adam optimizer with a fixed learning rate of 1e-5 and a dropout rate of 0.1. Each experiment is trained with 10, 8, and 5 epochs respectively for 800M, 3B, and 11B models.





\subsection{Main Results on Zero-Shot Tasks}

\paragraph{UD Zero-Shot Results}
The main results are presented in Table~\ref{tab:maintable}.
We compare methods of similar scales. 
Results in Table~\hyperref[tab:maintable:top]{1(a)} show that our UD substantially outperforms the T0 baseline on average by a large margin of around 9, 5, and 7 points respectively at Large, XL, and XXL scales.
Comparing the results of UD-T5-Large, UD-DeBERTaV3, and prompting ELECTRA, both variants of UD also substantially outperform prompting ELECTRA by more than 6 points.
On BIG-Bench datasets, results in Table~\hyperref[tab:maintable:middle]{1(b)} show that our UD outperforms the T0 baseline by a margin of around 4-8 points.
Besides T0 benchmark, we also test UD on BBH datasets, which are very different from T0 training sets, results in Table~\hyperref[tab:maintable:bottom]{1(c)} show that our UD constantly outperforms T0 and Flan-T5 by a margin of around 2-5 points, even though our UD is only trained on a small fraction of Flan-T5's training sets.
Overall, these results demonstrate the advantages of UD at every scale, and a broad range of tasks compared with baselines.

Another interesting finding is that the advantages of UD significantly increase along with scaling.
When scaling from Large-scale to XL-scale (i.e., around 3.75x of the parameters), the average performance improves by around 2 points. However, when scaling from XL-scale to XXL-scale (i.e., 3.6x of the parameters), the improvements of average zero-shot performance enlarge to 8 points.
Based on the observation, we hypothesize that UD can achieve even better performance of zero-shot generalization if further scaling to an even larger models, which we leave to future work.


To further boost the zero-shot performance, we also train a new variant of UD at 11B scale by scaling to more training tasks, including the discriminative English tasks used in \citet{1600tasks}, and the discriminative English tasks used in \citet{ul2}. The new model is denoted as UD+.
UD+ achieves the highest average accuracy among all the zero-shot evaluation tests.











\paragraph{Generalized UD Zero-Shot Results}
\input{tables/seq2sequd_11tasks}

The zero-shot results of generalized UD on 11 T0 discriminative test tasks and on 13 Big-Bench tasks are respectively reported in Table~\hyperref[tab:genud:top]{7(a)} and Table~\hyperref[tab:genud:mid]{7(b)} 
We also select the top 15 uncommon generative tasks from BigBench basing on ascending order of data size, results are in Table~\hyperref[tab:genud:bottom]{7(c)}. We assume that tasks with smaller data sizes are less common and more likely to be unrelated to our training data and more suitable for zero-shot tests. 


Analyses are as follows.
First, comparing the results of generalized UD and T0, generalized UD still holds significant improvements on discriminative tasks.
Second, comparing generalized UD with our previous UD (in Table~\ref{tab:maintable}), we observe there is a slight decrease in average performance, proving that adding generative tasks into training could have impacted a little bit, in trade for capability for handling generative tasks.
Third, on 15 generative tasks, both generalized UD and T0 show comparable results.














\subsection{SOTA Results on Finetuned Tasks}
\label{sec:ud_finetune}

To explore how UD performs on fully-supervised tasks, we finetuned UD for a wide range of downstream tasks and reported their results in Table \ref{tab:finetune}.
For each finetuning experiment, the maximum training epoch is set to be 10.
We search a hyper-parameter space with learning rate in \{2e-5, 1e-5, 5e-6\}, batch size in \{32, 64, 128\}.
We select the best checkpoint using a validation set with early stopping.

From results in Table \ref{tab:finetune}, we find that UD can achieve remarkable performance on most of the downstream tasks. 
We achieve state-of-the-art performance on 12 out of the 17 tasks we evaluated. The results also show that more challenging tasks (tasks that require more knowledge) will benefit more from the multi-task training period, especially some QA tasks.








\subsection{Ablation Study}

We have also conducted ablation studies to further explore how several factors affect the performance of zero-shot generalization. Please see appendix for further ablation studies on UD with different base models (\S~\ref{sec:base_models})

\subsubsection{Instructive Prompts vs Minimal Prompts}
\label{sec:minimal_prompts}

UD employs minimal prompts that use simple concatenation, while previous approaches rely on lengthy instructive prompts to provide more detailed instructions \cite{T0-paper,FLAN,gpt3-paper}. 
Statistically, we count the average number of prompt words (excluding raw input) for both minimal and instructive prompts, and statistics are respectively $0.4$ versus $>10$.
We compare these two types of prompts in the following experiment.
We adopt the instructive prompts from T0 and apply them on UD without changing the discriminator formulation. To construct minimal prompts for T0, we remove all the instructive words similar to UD.








Results are shown in Table~\ref{tab:promptablatiion}. We observe that minimal prompts yield better performance for UD than instructive prompts. In contrast, for T0, instructive prompts perform much better than minimal prompts. These results are consistent with our motivation that UD tends to unify the tasks better with a shared discrimination formulation. As a result, task-specific instructions are not necessary and might hurt generalization performance. Generative approaches, on the other hand, rely on instructive prompts to better distinguish different tasks and generate specific answers directly.



\subsection{How Well UD Generalizes to a Broader Domain?} \label{sec:generalize}
\input{tables/distribution_table}

Our discrimination problem formulation is in fact more general than solving supervised labeled tasks and can be applied to a broader domain of natural language. We conduct the following experiment to see how UD generalizes.



To test whether a model discriminates against the true data distribution, a straightforward way of verification is to compare the probability of real data with that of some generated, fake data. This form of verification is not specific to any downstream task and can be viewed as generalizing to a broader domain. Formally, given a text sample $x$, let $D(x)$ be the output of UD, which estimates the probability that $x$ is sampled from the true data distribution, i.e., $P(\text{true} | x)$. Given a true data sample $x$ and a generated data sample $x'$, we expect a well-trained UD to predict $D(x) > D(x')$.


Specifically, we randomly select 2,600 real data samples $x$ from the validation set of the T0 training data and generate the data $x’$ in two different ways: model-based generation and manual generation.

For a model-based generation, we utilize the T0-Large model with a paraphrase prefix ``Paraphrase the sentence:'' to generate data $x'$. It is expected that the generated samples $x'$ are similar to true samples $x$ to some extent but demonstrate some flaws that are unique to generated data. For a manual generation, we manually create some conflict or contradiction in the real sample $x$. Specifically, we manually attach wrong answers to the original data and obtain $x’$ , which is similar to what we have done in constructing negative samples in our main framework. 

We then use our \method based on T5-Encoder Large to compute the probability $D(x)$ and $D(x')$ for both real and generated data. As displayed in Table~\ref{tab:explain}, we find that the \method assigns a higher score for $x$ than $x'$ $80\%$ of the time for manually-generated data. When tested with model-generated data, UD assigns a high probability for real data in $74\%$ of the cases.
This is probably because manually generated data are more paradoxical and logically incoherent and thus are easier for UD to discriminate. Overall, these results demonstrate that the discrimination ability of UD is not limited to the downstream tasks on which it was trained, but is also generalizable to a broader domain of text data. This indicates a possibility of extending UD to other scenarios such as model pretraining and generation tasks.









