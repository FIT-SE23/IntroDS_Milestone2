% Please download the latest anthology.bib from
%
% http://aclweb.org/anthology/anthology.bib.gz


@inproceedings{glam,
  author    = {Nan Du and
               Yanping Huang and
               Andrew M. Dai and
               Simon Tong and
               Dmitry Lepikhin and
               Yuanzhong Xu and
               Maxim Krikun and
               Yanqi Zhou and
               Adams Wei Yu and
               Orhan Firat and
               Barret Zoph and
               Liam Fedus and
               Maarten P. Bosma and
               Zongwei Zhou and
               Tao Wang and
               Yu Emma Wang and
               Kellie Webster and
               Marie Pellat and
               Kevin Robinson and
               Kathleen S. Meier{-}Hellstern and
               Toju Duke and
               Lucas Dixon and
               Kun Zhang and
               Quoc V. Le and
               Yonghui Wu and
               Zhifeng Chen and
               Claire Cui},
  title     = {GLaM: Efficient Scaling of Language Models with Mixture-of-Experts},
  booktitle = {{ICML}},
  series    = {Proceedings of Machine Learning Research},
  volume    = {162},
  pages     = {5547--5569},
  publisher = {{PMLR}},
  year      = {2022}
}

@article{palm,
  author    = {Aakanksha Chowdhery and
               Sharan Narang and
               Jacob Devlin and
               Maarten Bosma and
               Gaurav Mishra and
               Adam Roberts and
               Paul Barham and
               Hyung Won Chung and
               Charles Sutton and
               Sebastian Gehrmann and
               Parker Schuh and
               Kensen Shi and
               Sasha Tsvyashchenko and
               Joshua Maynez and
               Abhishek Rao and
               Parker Barnes and
               Yi Tay and
               Noam Shazeer and
               Vinodkumar Prabhakaran and
               Emily Reif and
               Nan Du and
               Ben Hutchinson and
               Reiner Pope and
               James Bradbury and
               Jacob Austin and
               Michael Isard and
               Guy Gur{-}Ari and
               Pengcheng Yin and
               Toju Duke and
               Anselm Levskaya and
               Sanjay Ghemawat and
               Sunipa Dev and
               Henryk Michalewski and
               Xavier Garcia and
               Vedant Misra and
               Kevin Robinson and
               Liam Fedus and
               Denny Zhou and
               Daphne Ippolito and
               David Luan and
               Hyeontaek Lim and
               Barret Zoph and
               Alexander Spiridonov and
               Ryan Sepassi and
               David Dohan and
               Shivani Agrawal and
               Mark Omernick and
               Andrew M. Dai and
               Thanumalayan Sankaranarayana Pillai and
               Marie Pellat and
               Aitor Lewkowycz and
               Erica Moreira and
               Rewon Child and
               Oleksandr Polozov and
               Katherine Lee and
               Zongwei Zhou and
               Xuezhi Wang and
               Brennan Saeta and
               Mark Diaz and
               Orhan Firat and
               Michele Catasta and
               Jason Wei and
               Kathy Meier{-}Hellstern and
               Douglas Eck and
               Jeff Dean and
               Slav Petrov and
               Noah Fiedel},
  title     = {PaLM: Scaling Language Modeling with Pathways},
  journal   = {CoRR},
  volume    = {abs/2204.02311},
  year      = {2022}
}
@article{bigbench,
  title={Beyond the Imitation Game: Quantifying and extrapolating the capabilities of language models},
  author={Srivastava, Aarohi and Rastogi, Abhinav and Rao, Abhishek and Shoeb, Abu Awal Md and Abid, Abubakar and Fisch, Adam and Brown, Adam R and Santoro, Adam and Gupta, Aditya and Garriga-Alonso, Adri{\`a} and others},
  journal={arXiv preprint arXiv:2206.04615},
  year={2022}
}

@inproceedings{DBLP:conf/emnlp/TafjordGLC19,
  author    = {Oyvind Tafjord and
               Matt Gardner and
               Kevin Lin and
               Peter Clark},
  title     = {QuaRTz: An Open-Domain Dataset of Qualitative Relationship Questions},
  booktitle = {{EMNLP/IJCNLP} {(1)}},
  pages     = {5940--5945},
  publisher = {Association for Computational Linguistics},
  year      = {2019}
}

@inproceedings{DBLP:conf/acl/AsaiH20,
  author    = {Akari Asai and
               Hannaneh Hajishirzi},
  title     = {Logic-Guided Data Augmentation and Regularization for Consistent Question
               Answering},
  booktitle = {{ACL}},
  pages     = {5642--5650},
  publisher = {Association for Computational Linguistics},
  year      = {2020}
}

@inproceedings{DBLP:conf/acl/HeRKA21,
  author    = {Ruining He and
               Anirudh Ravula and
               Bhargav Kanagal and
               Joshua Ainslie},
  title     = {RealFormer: Transformer Likes Residual Attention},
  booktitle = {{ACL/IJCNLP} (Findings)},
  series    = {Findings of {ACL}},
  volume    = {{ACL/IJCNLP} 2021},
  pages     = {929--943},
  publisher = {Association for Computational Linguistics},
  year      = {2021}
}

@inproceedings{mrpc-dataset,
  author    = {William B. Dolan and
               Chris Brockett},
  title     = {Automatically Constructing a Corpus of Sentential Paraphrases},
  booktitle = {IWP@IJCNLP},
  publisher = {Asian Federation of Natural Language Processing},
  year      = {2005}
}

@article{rae2021scaling,
  title={Scaling language models: Methods, analysis \& insights from training gopher},
  author={Rae, Jack W and Borgeaud, Sebastian and Cai, Trevor and Millican, Katie and Hoffmann, Jordan and Song, Francis and Aslanides, John and Henderson, Sarah and Ring, Roman and Young, Susannah and others},
  journal={arXiv preprint arXiv:2112.11446},
  year={2021}
}
@article{clark2020electra,
  title={Electra: Pre-training text encoders as discriminators rather than generators},
  author={Clark, Kevin and Luong, Minh-Thang and Le, Quoc V and Manning, Christopher D},
  journal={arXiv preprint arXiv:2003.10555},
  year={2020}
}
@article{xia2022prompting,
  title={Prompting ELECTRA: Few-Shot Learning with Discriminative Pre-Trained Models},
  author={Xia, Mengzhou and Artetxe, Mikel and Du, Jingfei and Chen, Danqi and Stoyanov, Ves},
  journal={arXiv preprint arXiv:2205.15223},
  year={2022}
}

@article{yao2022prompt,
  title={Prompt Tuning for Discriminative Pre-trained Language Models},
  author={Yao, Yuan and Dong, Bowen and Zhang, Ao and Zhang, Zhengyan and Xie, Ruobing and Liu, Zhiyuan and Lin, Leyu and Sun, Maosong and Wang, Jianyong},
  journal={arXiv preprint arXiv:2205.11166},
  year={2022}
}

@article{xiong2019pretrained,
  title={Pretrained encyclopedia: Weakly supervised knowledge-pretrained language model},
  author={Xiong, Wenhan and Du, Jingfei and Wang, William Yang and Stoyanov, Veselin},
  journal={arXiv preprint arXiv:1912.09637},
  year={2019}
}


@article{artetxe2021efficient,
  title={Efficient Large Scale Language Modeling with Mixtures of Experts},
  author={Artetxe, Mikel and Bhosale, Shruti and Goyal, Naman and Mihaylov, Todor and Ott, Myle and Shleifer, Sam and Lin, Xi Victoria and Du, Jingfei and Iyer, Srinivasan and Pasunuru, Ramakanth and others},
  journal={arXiv preprint arXiv:2112.10684},
  year={2021}
}


@article{logan2021cutting,
  title={Cutting down on prompts and parameters: Simple few-shot learning with language models},
  author={Logan IV, Robert L and Bala{\v{z}}evi{\'c}, Ivana and Wallace, Eric and Petroni, Fabio and Singh, Sameer and Riedel, Sebastian},
  journal={arXiv preprint arXiv:2106.13353},
  year={2021}
}

@article{he2021towards,
  title={Towards a unified view of parameter-efficient transfer learning},
  author={He, Junxian and Zhou, Chunting and Ma, Xuezhe and Berg-Kirkpatrick, Taylor and Neubig, Graham},
  journal={arXiv preprint arXiv:2110.04366},
  year={2021}
}


@inproceedings{mahabadi2022prompt,
  title={Prompt-free and Efficient Few-shot Learning with Language Models},
  author={Mahabadi, Rabeeh Karimi and Zettlemoyer, Luke and Henderson, James and Mathias, Lambert and Saeidi, Marzieh and Stoyanov, Veselin and Yazdani, Majid},
  booktitle={Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  pages={3638--3652},
  year={2022}
}

@article{meng2022generating,
  title={Generating training data with language models: Towards zero-shot language understanding},
  author={Meng, Yu and Huang, Jiaxin and Zhang, Yu and Han, Jiawei},
  journal={arXiv preprint arXiv:2202.04538},
  year={2022}
}

@article{deberta,
  title={Deberta: Decoding-enhanced bert with disentangled attention},
  author={He, Pengcheng and Liu, Xiaodong and Gao, Jianfeng and Chen, Weizhu},
  journal={arXiv preprint arXiv:2006.03654},
  year={2020}
}


@article{debertav3,
  author    = {Pengcheng He and
               Jianfeng Gao and
               Weizhu Chen},
  title     = {DeBERTaV3: Improving DeBERTa using ELECTRA-Style Pre-Training with
               Gradient-Disentangled Embedding Sharing},
  journal   = {CoRR},
  volume    = {abs/2111.09543},
  year      = {2021}
}

@inproceedings{promptuning,
  author    = {Brian Lester and
               Rami Al{-}Rfou and
               Noah Constant},
  title     = {The Power of Scale for Parameter-Efficient Prompt Tuning},
  booktitle = {{EMNLP} {(1)}},
  pages     = {3045--3059},
  publisher = {Association for Computational Linguistics},
  year      = {2021}
}

@article{ptuning-paper,
  author    = {Xiao Liu and
               Yanan Zheng and
               Zhengxiao Du and
               Ming Ding and
               Yujie Qian and
               Zhilin Yang and
               Jie Tang},
  title     = {{GPT} Understands, Too},
  journal   = {CoRR},
  volume    = {abs/2103.10385},
  year      = {2021}
}


@article{T5-paper,
  author    = {Colin Raffel and
               Noam Shazeer and
               Adam Roberts and
               Katherine Lee and
               Sharan Narang and
               Michael Matena and
               Yanqi Zhou and
               Wei Li and
               Peter J. Liu},
  title     = {Exploring the Limits of Transfer Learning with a Unified Text-to-Text
               Transformer},
  journal   = {CoRR},
  volume    = {abs/1910.10683},
  year      = {2019}
}

@article{FLAN,
  author    = {Jason Wei and
               Maarten Bosma and
               Vincent Y. Zhao and
               Kelvin Guu and
               Adams Wei Yu and
               Brian Lester and
               Nan Du and
               Andrew M. Dai and
               Quoc V. Le},
  title     = {Finetuned Language Models Are Zero-Shot Learners},
  journal   = {CoRR},
  volume    = {abs/2109.01652},
  year      = {2021}
}


@article{T0-paper,
  author    = {Victor Sanh and
               Albert Webson and
               Colin Raffel and
               Stephen H. Bach and
               Lintang Sutawika and
               Zaid Alyafeai and
               Antoine Chaffin and
               Arnaud Stiegler and
               Teven Le Scao and
               Arun Raja and
               Manan Dey and
               M. Saiful Bari and
               Canwen Xu and
               Urmish Thakker and
               Shanya Sharma and
               Eliza Szczechla and
               Taewoon Kim and
               Gunjan Chhablani and
               Nihal V. Nayak and
               Debajyoti Datta and
               Jonathan Chang and
               Mike Tian{-}Jian Jiang and
               Han Wang and
               Matteo Manica and
               Sheng Shen and
               Zheng Xin Yong and
               Harshit Pandey and
               Rachel Bawden and
               Thomas Wang and
               Trishala Neeraj and
               Jos Rozen and
               Abheesht Sharma and
               Andrea Santilli and
               Thibault F{\'{e}}vry and
               Jason Alan Fries and
               Ryan Teehan and
               Stella Biderman and
               Leo Gao and
               Tali Bers and
               Thomas Wolf and
               Alexander M. Rush},
  title     = {Multitask Prompted Training Enables Zero-Shot Task Generalization},
  journal   = {CoRR},
  volume    = {abs/2110.08207},
  year      = {2021}
}


@article{PET-paper,
  author    = {Timo Schick and
               Hinrich Sch{\"{u}}tze},
  title     = {It's Not Just Size That Matters: Small Language Models Are Also Few-Shot
               Learners},
  journal   = {CoRR},
  volume    = {abs/2009.07118},
  year      = {2020}
}

@article{gpt3-paper,
  author    = {Tom B. Brown and
               Benjamin Mann and
               Nick Ryder and
               Melanie Subbiah and
               Jared Kaplan and
               Prafulla Dhariwal and
               Arvind Neelakantan and
               Pranav Shyam and
               Girish Sastry and
               Amanda Askell and
               Sandhini Agarwal and
               Ariel Herbert{-}Voss and
               Gretchen Krueger and
               Tom Henighan and
               Rewon Child and
               Aditya Ramesh and
               Daniel M. Ziegler and
               Jeffrey Wu and
               Clemens Winter and
               Christopher Hesse and
               Mark Chen and
               Eric Sigler and
               Mateusz Litwin and
               Scott Gray and
               Benjamin Chess and
               Jack Clark and
               Christopher Berner and
               Sam McCandlish and
               Alec Radford and
               Ilya Sutskever and
               Dario Amodei},
  title     = {Language Models are Few-Shot Learners},
  journal   = {CoRR},
  volume    = {abs/2005.14165},
  year      = {2020}
}

@inproceedings{devlin2018bert,
    title = "{BERT}: Pre-training of Deep Bidirectional Transformers for Language Understanding",
    author = "Devlin, Jacob  and
      Chang, Ming-Wei  and
      Lee, Kenton  and
      Toutanova, Kristina",
    booktitle = "Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)",
    year = "2019",
    address = "Minneapolis, Minnesota",
    publisher = "Association for Computational Linguistics",
    pages = "4171--4186",
}

@misc{liu2019roberta,
    title={Ro{BERT}a: A Robustly Optimized BERT Pretraining Approach},
    author={Yinhan Liu and Myle Ott and Naman Goyal and Jingfei Du and Mandar Joshi and Danqi Chen and Omer Levy and Mike Lewis and Luke Zettlemoyer and Veselin Stoyanov},
    year={2019},
    eprint={1907.11692},
    archivePrefix={arXiv},
    primaryClass={cs.CL}
}

@article{radford2018gpt,
  added-at = {2019-02-27T03:35:25.000+0100},
  author = {Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya},
  biburl = {https://www.bibsonomy.org/bibtex/2b30710316a8cfbae687672ea1f85c193/kirk86},
  description = {Language Models are Unsupervised Multitask Learners},
  interhash = {ce8168300081d74707849ed488e2a458},
  intrahash = {b30710316a8cfbae687672ea1f85c193},
  keywords = {learning multitask},
  timestamp = {2019-02-27T03:35:25.000+0100},
  title = {Language Models are Unsupervised Multitask Learners},
  url = {https://d4mucfpksywv.cloudfront.net/better-language-models/language-models.pdf},
  year = 2018
}

@misc{raffel2019exploring,
    title={Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer},
    author={Colin Raffel and Noam Shazeer and Adam Roberts and Katherine Lee and Sharan Narang and Michael Matena and Yanqi Zhou and Wei Li and Peter J. Liu},
    year={2019},
    eprint={1910.10683},
    archivePrefix={arXiv},
    primaryClass={cs.LG}
}

@inproceedings{yang2020xlnet,
 author = {Yang, Zhilin and Dai, Zihang and Yang, Yiming and Carbonell, Jaime and Salakhutdinov, Russ R and Le, Quoc V},
 booktitle = {Advances in Neural Information Processing Systems},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {{XLN}et: Generalized Autoregressive Pretraining for Language Understanding},
 volume = {32},
 year = {2019}
}


@misc{generativemodels,
    title={Generative Models},
    author={Andrej Karpathy and Pieter Abbeel and Greg Brockman and Vicki Cheung and Rocky Duan and Ian Goodfellow and Durk Kingma and Jonathan Ho and Rein Houthooft and Tim Salimans and John Schulman and Ilya Sutskever and Wojciech Zaremba},
    year={2016},
    howpublished = {\url{https://openai.com/blog/generative-models/}}
}

@inbook{generativeordiscriminative,
    title = "Generative or Discriminative? Getting the Best of Both Worlds",
    keywords = "machinelearning",
    author = "Julia Lasserre and Bishop, {Christopher M.} and Bernardo, {J. M.} and Bayarri, {M. J.} and Berger, {J. O.} and Dawid, {A. P.} and D. Heckerman and Smith, {A. F. M.} and M. West",
    year = "2007",
    language = "English",
    isbn = "9780199214655",
    volume = "8",
    pages = "3--24",
    booktitle = "Bayesian Statistics 8",
    publisher = "Oxford University Press",
    address = "United Kingdom",
}



@inproceedings{gao2021making,
   title={Making Pre-trained Language Models Better Few-shot Learners},
   author={Gao, Tianyu and Fisch, Adam and Chen, Danqi},
   booktitle={Association for Computational Linguistics (ACL)},
   year={2021}
}

@proceedings{2005_RTE,
  editor    = {Joaquin Qui{\~{n}}onero Candela and
               Ido Dagan and
               Bernardo Magnini and
               Florence d'Alch{\'{e}}{-}Buc},
  title     = {Machine Learning Challenges, Evaluating Predictive Uncertainty, Visual
               Object Classification and Recognizing Textual Entailment, First {PASCAL}
               Machine Learning Challenges Workshop, {MLCW} 2005, Southampton, UK,
               April 11-13, 2005, Revised Selected Papers},
  series    = {Lecture Notes in Computer Science},
  volume    = {3944},
  publisher = {Springer},
  year      = {2006}
}

@inproceedings{de2019_CB,
  title={The CommitmentBank: Investigating projection in naturally occurring discourse},
  author={De Marneffe, Marie-Catherine and Simons, Mandy and Tonhauser, Judith},
  booktitle={proceedings of Sinn und Bedeutung},
  volume={23},
  number={2},
  pages={107--124},
  year={2019}
}

@inproceedings{WSC2012,
  title={The winograd schema challenge},
  author={Levesque, Hector and Davis, Ernest and Morgenstern, Leora},
  booktitle={Thirteenth International Conference on the Principles of Knowledge Representation and Reasoning},
  year={2012},
  organization={Citeseer}
}


@inproceedings{COPA2011,
  title={Choice of Plausible Alternatives: An Evaluation of Commonsense Causal Reasoning.},
  author={Roemmele, Melissa and Bejan, Cosmin Adrian and Gordon, Andrew S},
  booktitle={AAAI Spring Symposium: Logical Formalizations of Commonsense Reasoning},
  pages={90--95},
  year={2011}
}

@article{wic-paper,
  author    = {Mohammad Taher Pilehvar and
               Jos{\'{e}} Camacho{-}Collados},
  title     = {WiC: 10, 000 Example Pairs for Evaluating Context-Sensitive Representations},
  journal   = {CoRR},
  volume    = {abs/1808.09121},
  year      = {2018}
}

@inproceedings{ZellersHBFC19_hellaswag,
  author    = {Rowan Zellers and
               Ari Holtzman and
               Yonatan Bisk and
               Ali Farhadi and
               Yejin Choi},
  title     = {HellaSwag: Can a Machine Really Finish Your Sentence?},
  booktitle = {{ACL} {(1)}},
  pages     = {4791--4800},
  publisher = {Association for Computational Linguistics},
  year      = {2019}
}

@inproceedings{story_cloze,
  title={Lsdsem 2017 shared task: The story cloze test},
  author={Mostafazadeh, Nasrin and Roth, Michael and Louis, Annie and Chambers, Nathanael and Allen, James},
  booktitle={Proceedings of the 2nd Workshop on Linking Models of Lexical, Sentential and Discourse-level Semantics},
  pages={46--51},
  year={2017}
}

@inproceedings{SakaguchiBBC20_winogrande,
  author    = {Keisuke Sakaguchi and
               Ronan Le Bras and
               Chandra Bhagavatula and
               Yejin Choi},
  title     = {WinoGrande: An Adversarial Winograd Schema Challenge at Scale},
  booktitle = {{AAAI}},
  pages     = {8732--8740},
  publisher = {{AAAI} Press},
  year      = {2020}
}

@inproceedings{NieWDBWK20_ANLI,
  author    = {Yixin Nie and
               Adina Williams and
               Emily Dinan and
               Mohit Bansal and
               Jason Weston and
               Douwe Kiela},
  title     = {Adversarial {NLI:} {A} New Benchmark for Natural Language Understanding},
  booktitle = {{ACL}},
  pages     = {4885--4901},
  publisher = {Association for Computational Linguistics},
  year      = {2020}
}

@misc{UniMC,
  doi = {10.48550/ARXIV.2210.08590},
  
  url = {https://arxiv.org/abs/2210.08590},
  
  author = {Yang, Ping and Wang, Junjie and Gan, Ruyi and Zhu, Xinyu and Zhang, Lin and Wu, Ziwei and Gao, Xinyu and Zhang, Jiaxing and Sakai, Tetsuya},
  
  keywords = {Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Zero-Shot Learners for Natural Language Understanding via a Unified Multiple Choice Perspective},
  
  publisher = {arXiv},
  
  year = {2022},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@misc{NIV2,
  doi = {10.48550/ARXIV.2204.07705},
  
  url = {https://arxiv.org/abs/2204.07705},
  
  author = {Wang, Yizhong and Mishra, Swaroop and Alipoormolabashi, Pegah and Kordi, Yeganeh and Mirzaei, Amirreza and Arunkumar, Anjana and Ashok, Arjun and Dhanasekaran, Arut Selvan and Naik, Atharva and Stap, David and Pathak, Eshaan and Karamanolakis, Giannis and Lai, Haizhi Gary and Purohit, Ishan and Mondal, Ishani and Anderson, Jacob and Kuznia, Kirby and Doshi, Krima and Patel, Maitreya and Pal, Kuntal Kumar and Moradshahi, Mehrad and Parmar, Mihir and Purohit, Mirali and Varshney, Neeraj and Kaza, Phani Rohitha and Verma, Pulkit and Puri, Ravsehaj Singh and Karia, Rushang and Sampat, Shailaja Keyur and Doshi, Savan and Mishra, Siddhartha and Reddy, Sujan and Patro, Sumanta and Dixit, Tanay and Shen, Xudong and Baral, Chitta and Choi, Yejin and Smith, Noah A. and Hajishirzi, Hannaneh and Khashabi, Daniel},
  
  keywords = {Computation and Language (cs.CL), Artificial Intelligence (cs.AI), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Super-NaturalInstructions: Generalization via Declarative Instructions on 1600+ NLP Tasks},
  
  publisher = {arXiv},
  
  year = {2022},
  
  copyright = {Creative Commons Attribution 4.0 International}
}


% Use this file for citations not found in the ACL Anthology (contained in "anthology.bib").

@book{Aho:72,
    author  = {Alfred V. Aho and Jeffrey D. Ullman},
    title   = {The Theory of Parsing, Translation and Compiling},
    year    = "1972",
    volume  = "1",
    publisher = {Prentice-Hall},
    address = {Englewood Cliffs, NJ}
}

@book{APA:83,
    author  = {{American Psychological Association}},
    title   = {Publications Manual},
    year    = "1983",
    publisher = {American Psychological Association},
    address = {Washington, DC}
}

@article{Chandra:81,
	author = {Ashok K. Chandra and Dexter C. Kozen and Larry J. Stockmeyer},
	year = "1981",
	title = {Alternation},
	journal = {Journal of the Association for Computing Machinery},
	volume = "28",
	number = "1",
	pages = "114--133",
	doi = "10.1145/322234.322243",
}

@inproceedings{andrew2007scalable,
  title={Scalable training of {L1}-regularized log-linear models},
  author={Andrew, Galen and Gao, Jianfeng},
  booktitle={Proceedings of the 24th International Conference on Machine Learning},
  pages={33--40},
  year={2007},
}

@book{Gusfield:97,
    author  = {Dan Gusfield},
    title   = {Algorithms on Strings, Trees and Sequences},
    year    = "1997",
    publisher = {Cambridge University Press},
    address = {Cambridge, UK}
}

@article{rasooli-tetrault-2015,
    author    = {Mohammad Sadegh Rasooli and Joel R. Tetreault},
    title     = {Yara Parser: {A} Fast and Accurate Dependency Parser},
    journal   = {Computing Research Repository},
    volume    = {arXiv:1503.06733},
    year      = {2015},
    url       = {http://arxiv.org/abs/1503.06733},
    note    = {version 2}
}

@article{Ando2005,
	Acmid = {1194905},
	Author = {Ando, Rie Kubota and Zhang, Tong},
	Issn = {1532-4435},
	Issue_Date = {12/1/2005},
	Journal = {Journal of Machine Learning Research},
	Month = dec,
	Numpages = {37},
	Pages = {1817--1853},
	Publisher = {JMLR.org},
	Title = {A Framework for Learning Predictive Structures from Multiple Tasks and Unlabeled Data},
	Volume = {6},
	Year = {2005}
}

@article{goodfellow2014generative,
  title={Generative adversarial nets},
  author={Goodfellow, Ian and Pouget-Abadie, Jean and Mirza, Mehdi and Xu, Bing and Warde-Farley, David and Ozair, Sherjil and Courville, Aaron and Bengio, Yoshua},
  journal={Advances in neural information processing systems},
  volume={27},
  year={2014}
}

@article{wang2022benchmarking,
  title={Benchmarking generalization via in-context instructions on 1,600+ language tasks},
  author={Wang, Yizhong and Mishra, Swaroop and Alipoormolabashi, Pegah and Kordi, Yeganeh and Mirzaei, Amirreza and Arunkumar, Anjana and Ashok, Arjun and Dhanasekaran, Arut Selvan and Naik, Atharva and Stap, David and others},
  journal={arXiv preprint arXiv:2204.07705},
  year={2022}
}

@article{liu2021gpt,
  title={GPT understands, too},
  author={Liu, Xiao and Zheng, Yanan and Du, Zhengxiao and Ding, Ming and Qian, Yujie and Yang, Zhilin and Tang, Jie},
  journal={arXiv preprint arXiv:2103.10385},
  year={2021}
}

@article{wang2019superglue,
  title={SuperGLUE: A Stickier Benchmark for General-Purpose Language Understanding Systems},
  author={Wang, Alex and Pruksachatkun, Yada and Nangia, Nikita and Singh, Amanpreet and Michael, Julian and Hill, Felix and Levy, Omer and Bowman, Samuel R},
  journal={arXiv preprint arXiv:1905.00537},
  year={2019}
}

@inproceedings{wang2019glue,
  title={{GLUE}: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding},
  author={Wang, Alex and Singh, Amanpreet and Michael, Julian and Hill, Felix and Levy, Omer and Bowman, Samuel R.},
  note={In the Proceedings of ICLR.},
  year={2019}
}

@article{oord2018representation,
  title={Representation learning with contrastive predictive coding},
  author={Oord, Aaron van den and Li, Yazhe and Vinyals, Oriol},
  journal={arXiv preprint arXiv:1807.03748},
  year={2018}
}

@article{commonsense_qa,
  author    = {Alon Talmor and
               Jonathan Herzig and
               Nicholas Lourie and
               Jonathan Berant},
  title     = {CommonsenseQA: {A} Question Answering Challenge Targeting Commonsense
               Knowledge},
  journal   = {CoRR},
  volume    = {abs/1811.00937},
  year      = {2018},
  url       = {http://arxiv.org/abs/1811.00937},
  eprinttype = {arXiv},
  eprint    = {1811.00937},
  timestamp = {Thu, 22 Nov 2018 17:58:30 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1811-00937.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@misc{ul2,
  doi = {10.48550/ARXIV.2205.05131},
  
  url = {https://arxiv.org/abs/2205.05131},
  
  author = {Tay, Yi and Dehghani, Mostafa and Tran, Vinh Q. and Garcia, Xavier and Bahri, Dara and Schuster, Tal and Zheng, Huaixiu Steven and Houlsby, Neil and Metzler, Donald},
  
  keywords = {Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Unifying Language Learning Paradigms},
  
  publisher = {arXiv},
  
  year = {2022},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}select the best checkpoint based on the validation set with early stopping

@misc{1600tasks,
  doi = {10.48550/ARXIV.2204.07705},
  
  url = {https://arxiv.org/abs/2204.07705},
  
  author = {Wang, Yizhong and Mishra, Swaroop and Alipoormolabashi, Pegah and Kordi, Yeganeh and Mirzaei, Amirreza and Arunkumar, Anjana and Ashok, Arjun and Dhanasekaran, Arut Selvan and Naik, Atharva and Stap, David and Pathak, Eshaan and Karamanolakis, Giannis and Lai, Haizhi Gary and Purohit, Ishan and Mondal, Ishani and Anderson, Jacob and Kuznia, Kirby and Doshi, Krima and Patel, Maitreya and Pal, Kuntal Kumar and Moradshahi, Mehrad and Parmar, Mihir and Purohit, Mirali and Varshney, Neeraj and Kaza, Phani Rohitha and Verma, Pulkit and Puri, Ravsehaj Singh and Karia, Rushang and Sampat, Shailaja Keyur and Doshi, Savan and Mishra, Siddhartha and Reddy, Sujan and Patro, Sumanta and Dixit, Tanay and Shen, Xudong and Baral, Chitta and Choi, Yejin and Smith, Noah A. and Hajishirzi, Hannaneh and Khashabi, Daniel},
  
  keywords = {Computation and Language (cs.CL), Artificial Intelligence (cs.AI), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Benchmarking Generalization via In-Context Instructions on 1,600+ Language Tasks},
  
  publisher = {arXiv},
  
  year = {2022},
  
  copyright = {Creative Commons Attribution 4.0 International}
}

@misc{bbh,
  doi = {10.48550/ARXIV.2210.09261},
  
  url = {https://arxiv.org/abs/2210.09261},
  
  author = {Suzgun, Mirac and Scales, Nathan and Sch√§rli, Nathanael and Gehrmann, Sebastian and Tay, Yi and Chung, Hyung Won and Chowdhery, Aakanksha and Le, Quoc V. and Chi, Ed H. and Zhou, Denny and Wei, Jason},
  
  keywords = {Computation and Language (cs.CL), Artificial Intelligence (cs.AI), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Challenging BIG-Bench Tasks and Whether Chain-of-Thought Can Solve Them},
  
  publisher = {arXiv},
  
  year = {2022},
  
  copyright = {Creative Commons Attribution Non Commercial Share Alike 4.0 International}
}

@misc{flant5,
  doi = {10.48550/ARXIV.2210.11416},
  
  url = {https://arxiv.org/abs/2210.11416},
  
  author = {Chung, Hyung Won and Hou, Le and Longpre, Shayne and Zoph, Barret and Tay, Yi and Fedus, William and Li, Yunxuan and Wang, Xuezhi and Dehghani, Mostafa and Brahma, Siddhartha and Webson, Albert and Gu, Shixiang Shane and Dai, Zhuyun and Suzgun, Mirac and Chen, Xinyun and Chowdhery, Aakanksha and Castro-Ros, Alex and Pellat, Marie and Robinson, Kevin and Valter, Dasha and Narang, Sharan and Mishra, Gaurav and Yu, Adams and Zhao, Vincent and Huang, Yanping and Dai, Andrew and Yu, Hongkun and Petrov, Slav and Chi, Ed H. and Dean, Jeff and Devlin, Jacob and Roberts, Adam and Zhou, Denny and Le, Quoc V. and Wei, Jason},
  
  keywords = {Machine Learning (cs.LG), Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Scaling Instruction-Finetuned Language Models},
  
  publisher = {arXiv},
  
  year = {2022},
  
  copyright = {Creative Commons Attribution 4.0 International}
}
