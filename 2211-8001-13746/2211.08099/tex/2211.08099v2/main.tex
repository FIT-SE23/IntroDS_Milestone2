\pdfoutput=1

\documentclass[11pt]{article}

\usepackage{acl}

\usepackage{times}
\usepackage{latexsym}

\usepackage[T1]{fontenc}

\usepackage[utf8]{inputenc}

\usepackage{microtype}


\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\usepackage{multirow}
\usepackage{longtable}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{tabularx}
\usepackage{comment}
\usepackage{hyperref}
\usepackage{amsmath}

\usepackage{ulem}
\usepackage{subfigure}
\captionsetup[table]{position=t}
\AtBeginDocument{\captionsetup[subtable]{position=b}}

\usepackage{caption}

\newcommand{\std}{\scriptsize$\pm$}

\newcommand{\method}[0]{universal discriminator~}

\newcommand{\mc}[1]{\mathcal{#1}}

\newcommand{\lzy}[1]{\textcolor{green}{lzy: #1}}
\newcommand{\xhk}[1]{\textcolor{orange}{xhk: #1}}
\newcommand\zy[1]{\textbf{ \textcolor{red}{zhilin: #1}}}
\newcommand\yn[1]{{\textcolor{red}{#1}}}
\newcommand\zj[1]{\textcolor{blue}{#1}}


\title{A Universal Discriminator for Zero-Shot Generalization}




\author{Haike Xu$^{1}$, Zongyu Lin$^{1}$, Jing Zhou$^{1}$, Yanan Zheng$^{2}$\footnotemark[1], Zhilin Yang$^{134}$\footnotemark[1] \\
$^1$Institute for Interdisciplinary Information Sciences, Tsinghua University \\
$^2$Department of Computer Science and Technology, Tsinghua University \\
$^3$Shanghai Artificial Intelligence Laboratory, $^4$Shanghai Qi Zhi Institute \\
\texttt{haikexu@mit.edu}, 
\texttt{\{zyanan,zhiliny\}@tsinghua.edu.cn} \\}

\begin{document}
\maketitle
\renewcommand{\thefootnote}{\fnsymbol{footnote}}
\footnotetext[1]{Corresponding authors.}



\begin{abstract}
Generative modeling has been the dominant approach for large-scale pretraining and zero-shot generalization. In this work, we challenge this convention by showing that discriminative approaches perform substantially better than generative ones on a large number of NLP tasks. Technically, we train a single discriminator to predict whether a text sample comes from the true data distribution, similar to GANs. Since many NLP tasks can be formulated as selecting from a few options, we use this discriminator to predict the concatenation of input and which option has the highest probability of coming from the true data distribution. 
This simple formulation achieves state-of-the-art zero-shot results on the T0 benchmark, outperforming T0 by 16.0\%, 7.8\%, and 11.5\% respectively on different scales. In the finetuning setting, our approach also achieves new state-of-the-art results on a wide range of NLP tasks, with only 1/4 parameters of previous methods.
Meanwhile, our approach requires minimal prompting efforts, which largely improves robustness and is essential for real-world applications. Furthermore, we also jointly train a generalized UD in combination with generative tasks, which maintains its advantage on discriminative tasks and simultaneously works on generative tasks.

\footnotetext[2]{Our code is available at \href{https://github.com/Rafa-zy/UD}{https://github.com/Rafa-zy/UD}.}




\end{abstract}

\input{introduction}
\input{related_work}
\input{unified_task_formulation}
\input{experiments}
\input{conclusion}


\input{limitations}
\bibliography{anthology}

\input{appendix}

\end{document}
