\section{Related Work}

\subsection{Zero-Shot Generalization Using PLMs}
Pretrained language models (PLM) can transfer knowledge from training data to downstream tasks.
Prompting methods further narrow the gap between training data and downstream tasks. \citet{PET-paper} reformulate NLP tasks into cloze filling using prompts so that PLMs can conduct zero-shot inference by generating tokens given prompted inputs. \citet{meng2022generating} use PLMs to generate class-conditioned texts with the guidance of prompts without seeing any task-specific data.
Most recently, researchers have introduced natural language prompts to unify various kinds of tasks and propose a multi-task prompted training framework to achieve great zero-shot performance even faced with unseen downstream tasks (\citet{FLAN,T0-paper,flant5}).
However, zero-shot learning has been dominated by generative approaches.


\subsection{Prompt-based and Prompt-free Methods in NLP}
Prompting is the method of reformatting NLP tasks using natural language templates to adapt to downstream tasks \cite{T5-paper,PET-paper}.
To reduce the instability and labor costs brought by prompting, researchers have tried various approaches (\citet{ptuning-paper,he2021towards}) to learn continuous prompts. 

Recently, prompt-free methods are also being explored. \citet{mahabadi2022prompt} adopts task-specific adapters to learn task descriptions implicitly for few-shot learning with PLMs. 
It has also been indicated that using null prompts without task-specific templates can achieve decent performance compared with manually-designed prompts on various tasks (\citet{logan2021cutting}).

Our work further shows that those widely used lengthy instructive prompts are not necessary for zero-shot learning. Actually, minimal prompting performs better with our discriminative formulation in the multi-task zero-shot learning setting.


\subsection{Discriminative Models in NLP}


PLMs trained with masked language modeling (MLM) \cite{devlin2018bert,liu2019roberta} can be finetuned in a discriminative manner for downstream tasks. 
ELECTRA \cite{clark2020electra} trains a discriminator to detect whether a token has been replaced. WKLM \cite{xiong2019pretrained} employs an entity-centric approach for pretraining and predicts whether an entity has been replaced.
However, finetuning for these methods is usually based on one separate CLS head per task, which is not suitable for zero-shot generalization.



Recently, prompting has been combined with token-level discriminators based on ELECTRA for few-shot learning \cite{yao2022prompt,xia2022prompting}. While these are also discriminative approaches, there are a few key differences from our approach. The biggest difference between them and us is that: we unify all discriminative tasks into one single task with minimal prompting, showing extremely good zero-shot generalization. Moreover, these methods are specific to ELECTRA-like pretraining, while our approach accepts arbitrary pretrained encoders. In our experiments, we will also make a direct comparison with these approaches to demonstrate our effectiveness.









