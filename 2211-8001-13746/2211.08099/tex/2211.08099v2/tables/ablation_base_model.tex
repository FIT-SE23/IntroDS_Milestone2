\begin{table*}[ht]
\setlength{\tabcolsep}{0.9mm}
\centering
\resizebox{\textwidth}{!}{%
    \begin{tabular}{l|l|ccccc|ccc|cc|c|c}
        \toprule[1pt]
        & \multirow{2}*{Base Model}
        & \multicolumn{5}{c|}{\textbf{Natural Language Inference}} & \multicolumn{3}{|c|}{\textbf{Sentence Completion}} & \multicolumn{2}{c|}{\textbf{Coreference}} & \multicolumn{1}{c|}{\textbf{WSD}} & \multirow{2}{*}{Avg.} \\
    & & RTE & CB & ANLI1 & ANLI2 & ANLI3 & COPA & Hella. & Story. & WSC & Wino. & WiC &  \\
    \midrule[1pt]
    \multirow{2}*{\shortstack{Encoder}}
    & DeBERTa-V3 (304M) 
        & 71.1
        & 76.8
        & 43.8
        & 41.3
        & 45.7
        & 96.0
        & 60.7
        & 97.4
        & 66.4
        & 83.6
        & 53.3
        & 66.9 \\
    & DeBERTa-V2 (1.5B) 
        & 77.6
        & 80.4
        & 43.2
        & 39.3
        & 44.8
        & 95.0
        & 67.2
        & 98.2
        & 74.0	& 82.1 & 56.0	& 68.9\\ \midrule
    \multirow{2}*{\shortstack{Enc-Dec}} & T5-Encoder (400M) 
        & 75.1	& 55.5	& 32.9	& 32.3	& 33.7	& 84.6	& 28.2	& 94.0	& 63.0	& 54.6	& 51.2	& 55.0 \\
    & T5-Encoder (1.5B)  & 79.7	& 68.9	& 43.1	& 38.5	& 42.3	& 94.1	& 31.5	& 97.5	& 68.8	& 61.3	& 54.1	& 61.8\\
    \midrule
    \multirow{1}*{\shortstack{Decoder}}
    & \multirow{1}*{GPT-XL (1.5B)}
        & \multirow{1}*{71.1}
        & \multirow{1}*{75.0}
        & \multirow{1}*{30.4}
        & \multirow{1}*{31.8}
        & \multirow{1}*{37.8}
        & \multirow{1}*{71.0}
        & \multirow{1}*{40.9}
        & \multirow{1}*{87.7}
        & \multirow{1}*{62.5}
        & \multirow{1}*{54.5}
        & \multirow{1}*{50.3}
        & \multirow{1}*{55.7}
    \\
    \bottomrule[1pt]
    \end{tabular}}
    \caption{Ablation study on different backbone models. We experiment with base models of different architectures and scales. ``Enc-Dec'' refers to models that are pretrained in an encoder-decoder manner.}
    \label{tab:ablationbasemodel}
\end{table*}