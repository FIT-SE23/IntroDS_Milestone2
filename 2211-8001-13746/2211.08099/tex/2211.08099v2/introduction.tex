\section{Introduction}

Generative modeling has been the dominant approach for large-scale pretraining and zero-shot generalization~\cite{gpt3-paper,artetxe2021efficient,rae2021scaling}. 
Combined with prompts~\cite{gpt3-paper}, most of the natural language processing (NLP) tasks can be formulated into the fill-in-the-blank format and perform generative language modeling.
Based on the unified generative formulation, pretrained models such as GPT-3~\cite{gpt3-paper}, BERT~\cite{devlin2018bert,PET-paper}, T5~\cite{T5-paper}, can perform zero-shot inference on new tasks. 


More recent work~\cite{T0-paper} proposed to further pretrain a generative T5~\cite{T5-paper} with multitask prompted datasets and has substantially enhanced the performance of zero-shot generalization. 
In contrast, methods based on discriminative modeling~\cite{devlin2018bert} have not been able to achieve state-of-the-art performance on zero-shot learning. The adoption of discriminative approaches for zero-shot learning has been limited in the literature.




\begin{figure}%
     \centering
     \includegraphics[width=\linewidth]{figure/final_sota.png}
     \vspace{-15pt}
     \caption{Average zero-shot performance over 11 zero-shot tasks for our Universal Discriminator and T0~\cite{T0-paper}. Our universal discriminator significantly outperforms T0 across three different scales.}
     \label{fig:sota}
     \vspace{-15pt}
 \end{figure} 


In this work, we challenge the convention of zero-shot learning and propose to study and improve discriminative approaches. This is motivated by the fact that many NLP tasks can be framed as selecting from a few options; e.g., telling whether sentence A entails sentence B, or predicting which answer is correct for a given question. We call these tasks \textit{discriminative tasks}. As we will discuss in later sections, a significant portion of NLP tasks is in fact discriminative tasks. We hypothesize that discriminative approaches perform better for discriminative tasks.

To verify the hypothesis, we propose the \textbf{universal discriminator (UD)}, which substantially improves zero-shot generalization over the previous generative state-of-the-art (SOTA)~\cite{T0-paper}, as Figure~\ref{fig:sota} shows.
The main idea is to train a single discriminator to predict whether a text sample comes from the true data distribution of natural language, similar to GANs \cite{goodfellow2014generative}. Given a set of training tasks with labeled data, we construct a dataset with positive and negative examples, where positive ones are in-distribution natural language samples and negative ones are out-of-distribution. There are two major types of discriminative tasks. The first type is tasks with multiple options, such as multi-choice question answering and news classification. We fill the options into the sentences and the ones with correct options are considered positive samples. The second type is tasks with yes/no options, which can be formulated as a binary discrimination problem itself. For example, natural language inference aims to predict whether a premise entails a hypothesis. In this case, we use a prompt to concatenate the premise $A$ and the hypothesis $B$ into a sentence ``Premise: $A$. Hypothesis: $B$.'' If entailment holds, this sample is treated as positive in-distribution samples and otherwise negative out-of-distribution ones.






For the performance of zero-shot generalization, our approach achieves new state-of-the-art on the T0 benchmark, outperforming T0 by 16.0\%, 7.8\%, and 11.5\% respectively on different scales. 
UD also achieves state-of-the-art performance on a wide range of supervised NLP tasks, using only 1/4 parameters of previous methods.
Compared with the previous generative prompt-based methods, our universal discriminator requires minimal prompting, which is simple, robust, and applicable in real-world scenarios.



In addition, we also generalize UD to a larger scope of tasks, such that UD can perform discriminative and generative tasks at the same time. Specifically, we extend UD to the encoder-decoder architecture for training on generative tasks, and restrict the model's prediction on "yes"/"no" tokens for jointly training discriminative tasks. Results prove that generalized UD maintains UD's advantages on discriminative tasks and achieves comparable results on generative tasks (See \S~\ref{sec:generalizedud}). 




