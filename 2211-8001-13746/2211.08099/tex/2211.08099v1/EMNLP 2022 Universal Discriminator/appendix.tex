\newpage
\appendix
\section{Appendix}

%\subsection{Extent measuring task: a special case of multi-choice task}\label{sec:extent}
%Some tasks ask for the location of the given input in an extent measurement, e.g. the degree of sentiment, or the attitude of a given paragraph. Usually, ``positive" and ``negative" are the two ends in the measurement and the extent of an input can be represented as a real number between 0 and 1. For these tasks, we use minimal prompt to concatenate the input with one extreme extent verbalizer, e.g. ``positive", and we assign it the label basing on the true extent of the sentence in the extent measurement, which is usually a fraction between 0 to 1.

%\subsection{Training datasets for UD+}\label{sec:ud_plus_data}
%Our training datasets for UD+ in table~\ref{tab:maintable} includes: Training datasets from T0++ \citep{T0-paper} and part of datasets from Natural Language Instruction \citep{NIV2}.
%Note that we eliminate those new added tasks which overlap with our out held-out test sets, so the zero-shot criterion is still satisfied.


\subsection{Evaluation on Big-Bench}\label{sec:bigbench}
We also showcase that our proposed methods have achieved great performance over T0 on 13 tasks in the Big-Bench \citet{bigbench}, which is also utilized in original T0 paper~\cite{T0-paper}. Noted that all the tasks from BIG-Bench are ensured unseen in our training set for the zero-shot setting. The results are displayed in Table ~\ref{tab:bigbench}, where UD-Large outperforms T0-Large by 6.67\%, UD-XL outperforms T0-XL by over 4\%, and Generalized UD-XL outperforms T0-XL by over 6\%, further indicating the effectiveness of our proposed framework.





\begin{table*}[htbp]
  \centering

    \begin{tabular}{c|c|ccccc}
    \toprule
    Dataset & Metric & T0-Large & T0-XL & Gen. UD-Large & Gen. UD-XL & UD-XL \\
    \midrule
    code\_description & Acc.  & 0.1407  & 0.2344  & 0.5667  & 0.6000  & 0.5333  \\
    conceptual & Acc.  & 0.4038  & 0.4808  & 0.4660  & 0.6408  & 0.7379  \\
    known\_unknowns & Acc.  & 0.6042  & 0.6458  & 0.6304  & 0.6957  & 0.6522  \\
    logic\_grid & Acc.  & 0.3800  & 0.4250  & 0.3350  & 0.3820  & 0.3720  \\
    logic\_deduction & Acc.  & 0.4116  & 0.5007  & 0.4173  & 0.5280  & 0.3780  \\
    misconceptions & Acc.  & 0.5000  & 0.5268  & 0.4612  & 0.4886  & 0.4795  \\
    novel\_concepts & Acc.  & 0.1000  & 0.2500  & 0.3824  & 0.4412  & 0.3529  \\
    strategy\_qa & Acc.  & 0.5231  & 0.5305  & 0.5061  & 0.5707  & 0.5310  \\
    winowhy & Acc.  & 0.4972  & 0.4537  & 0.5148  & 0.4653  & 0.4531  \\
    syllogisms & Acc.  & 0.5034  & 0.5020  & 0.5027  & 0.5037  & 0.5042  \\
    movie\_dialog & Acc.  & 0.4676  & 0.4769  & 0.5130  & 0.5091  & 0.5013  \\
    language\_id & Acc.  & 0.1601  & 0.1900  & 0.1492  & 0.1549  & 0.2288  \\
    vitaminc & Acc.  & 0.4616  & 0.6003  & 0.5761  & 0.6678  & 0.6374  \\
    \midrule
    Avg.  & Acc.  & 0.3964  & 0.4475  & 0.4631  & 0.5114  & 0.4893  \\
    \bottomrule
    \end{tabular}%
  \caption{Universal Discriminator V.S. T0 across Big-Bench test tasks used in T0 paper. Gen. UD-XX represents our proposed generalized UD mentioned in Section 4.2 which trains on both discriminative tasks and generative tasks. The joint version shows great performance gain over T0 on Big-Bench datasets.}
  \label{tab:bigbench}%
\end{table*}%



% \zj{It seems that the score of RACE is not calculated correctly.(I am still not sure how they are calculated ``Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism'') And some values are not copied correctly (I have correct them, but the average should be recalculated.}






\begin{comment}
\subsection{Specific unifying methods}\label{sec:specific_unifying}
In this section, we provide 3 unifying method for different tasks, basing on the relationship of their answer choices: parallel, opposite, or extent.

\subsubsection{Parallel Answer Choices}\label{sec:sentence_completion}
For tasks like Question Answering, Sentence Completion, the answer choices are several unrelated words or phrases. Our unifying method is to use de-prompting to concatenate all the raw input keywords and attach each answer choice at the end of the concatenated sentence to form several input data for \solution. (sometimes if there are specific locations, e.g. blanks in the sentence, for the answer choices to be filled, we directly fill in each answer choice there instead of attaching them at the end) In training phase, the sentence with correct answer is given label 1, and the other sentences are assigned label 0. In testing phase, we select the concatenated sentence with the maximal probability of true, and output its corresponding answer choice.

For all the tasks used in \citet{T0-paper}, the training tasks using this unifying method are: Cosmos QA, DREAM, QASC, QuAIL, QuaRel, QuaRTz, SciQ, Social IQA, Wiki Hop and the test tasks using this unifying method are: HellaSwag, COPA, Story Cloze.

to do: an example for each task

\subsection{Opposite Answer Choices}
Some tasks are essentially binary interrogating questions, e.g. judging whether a hypothesis is implied by the premise, judging whether two sentences has the same meaning. Their answer choices are usually Yes/No, True/False. For these tasks, we assume that the answer for this tasks is an intrinsic nature of the given raw input, even without choice attached. Through de-prompting, we get a concatenation of the raw input's keywords as the input for \solution. In training phase, we assign it a label basing on its answer. In test phase, we do the binary selection by checking whether the probability of true outputed by \solution is larger than 0.5. 

For all the tasks used in \citet{T0-paper}, the training tasks using this unifying method are: MRPC, QQP, PAWS, Wiki QA. and the test tasks using this unifying method are:  CB, ANLI, RTE, WiC.

to do: an example for each task

\subsection{Extent Answer Choices}
Some tasks are asking for the location of the given input in a extent measurement, e.g. the degree of sentiment, or the attitude of a given paragraph. Usually, "positive" and "negative" are the two ends in the measurement. For these tasks, we concatenate the input with each of the two extreme extent verbalizer, e.g. "positive" and "negativeâ€œ, and we assign it the label basing on the true location of the sentence in the extent measurement.

For all the tasks used in \citet{T0-paper}, the training tasks using this unifying method are: Amazon, App Reviews, IMDB, Rotten Tomatoes, Yelp. No test tasks uss this unifying method.

\end{comment}

\begin{comment}

\subsubsection{Binary interrogating question}\label{sec:binary_question}
This kind of task asks a general interrogative question and wants a binary answer: yes or no, true or false, etc. For this kind of task, our unifying method is to concatenate its raw input, while using no prompt, and giving it an 0/1 label basing on its own binary label. We argue that even without knowing which specific question the task asks for, once seeing its raw input, the raw input's ground truth label for the \solution task is the same with its belonging task's label. Or at least very similar to the belonging task's label, though maybe with ambiguity for some tasks.

The training tasks using this unifying method are: MRPC, QQP, PAWS, Wiki QA.
The evaluation tasks using this unifying method are: CB, ANLI, RTE, WiC.

to do: an example for each task

\subsubsection{Sentiment analysis}
This kind of task asks for the degree of sentiment, or the attitude of a given paragraph. Usually, there are only two sentiments: positive or negative. The task asks for which sentiment class the input belongs to or even more detailed, asks for where this paragraph lie in a spectrum from the positive side to the negative side.

We think this class of tasks is inappropriate to inherit the method in section~\ref{sec:binary_question} because both sentiments are reasonable in the real language corpus or the method in section~\ref{sec:sentence_completion} because the answer choices here are essential just a signal, if not binary, at most a real number between 0 and 1, and it is meaningless to concatenate the input with a quantitative value.

Therefore, for sentiment analysis task, we borrow the verbalizer from the prompted answer choices, e.g. "Positive", "Definitely!", and we concatenate the raw input and the verbalizer representing one end of the sentiment spectrum, e.g. "positive", then, the label we give to this concatenation as an input for \solution is an real number between 0 and 1 basing on the origianl tasks's label.

The rationale behind this method is that the closer the input paragraph to the attached sentiment verbalizer, the more consistent the overall concatenation looks like, the higher consistent score it should be given, and vice versa.

The training tasks using this unifying method are: Amazon, App Reviews, IMDB, Rotten Tomatoes, Yelp.
No evaluation tasks using this unifying method,

to do: an example for each task

\subsubsection{Topic classification}\label{sec:topic_classification}
This kind of task asks for which topic or domain a given paragraph belongs to. The unifying method here is the same as section~\ref{sec:sentence_completion}: we concatenate the input paragraph and the topic name. Then, we attach a label 1 if the paragraph is indeed in the attached topic and label 0 if not.

The training tasks using this unifying method are: AG News, DBPedia, TREC.
No training task using this unifying method.

\subsubsection{Coreference Resolution}\label{sec:coreference}
This kind of task asks for which entity a pronoun refers to. Though it many be possible to directly apply the method used in section~\ref{sec:topic_classification} by concatenating the sentence and each entity in the answer choices. Here, we design a better method. We apply string replace to the sentence, where we replace the pronoun in the sentence with each of the entity in the answer choices and form several new sentences. We assign label 1 to the sentence with the correct entity replacement and label 0 to the other wrong entity replacement. The assumption we make here is that after the correct replacement, the new sentence should have the same meaning compared with the old one and is thus a labeled 1 data for \solution, while the wrong replacement will cause contradictions in the sentence and is thus a labeled 0 data for \solution.

No training task uses this unifying method.
The evaluation tasks using this unifying method are: WSC, Winogrande.

to do: an example for each task

\subsection{Specific evaluation method}\label{sec:specific_evaluation}
Depending on the formulation of different evaluation tasks, we design 2 methods to do the evaluation.
First, given a piece of data from the evaluation task, we use the unifying method described above in section~\ref{sec:specific_unifying} to generate one or several pieces of data for \solution task.

If only one piece of data is generated, this task must belong to Binary interrogating question. We use our trained LM for \solution to predict its probability $prob\in[0,1]$ to be consistent. Our assumption in section~\ref{sec:binary_question} says that whether this generated data is consistent is directly correlated with its label in its own task. Therefore, if $prob>0.5$, we answer one choice, usually the one corresponding to "yes", "true", or etc, and the other choice if $prob<0.5$, usually corresponding to "no", "false", or etc. Notice that for some tasks like CB and ANLI, they are not strictly a binary interrogating question because they ask for "yes", "no", or "maybe". For now, we admit that our method cannot cope with the "maybe" case and our model only output either "yes" or "no" for all data. However, experiments show that even if we discard one answer choice, we still get remarkable test accuracy.

If multiple pieces of data is generated, this task may be either sentence completion or coreference resolution. In this case, each generated data must originate from a choice. The evaluation method we use here is similar to how generative language model solves sequence classification tasks: we compare the consistent probability of each generated piece of data and predict the choice whose generated data has the maximal consistent probability.








\end{comment}