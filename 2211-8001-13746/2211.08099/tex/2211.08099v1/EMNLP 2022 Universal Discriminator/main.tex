% This must be in the first 5 lines to tell arXiv to use pdfLaTeX, which is strongly recommended.
\pdfoutput=1
% In particular, the hyperref package requires pdfLaTeX in order to break URLs across lines.

\documentclass[11pt]{article}

% Remove the "review" option to generate the final version.
% \usepackage[review]{emnlp2022}
\usepackage{emnlp2022}

% Standard package includes
\usepackage{times}
\usepackage{latexsym}

% For proper rendering and hyphenation of words containing Latin characters (including in bib files)
\usepackage[T1]{fontenc}
% For Vietnamese characters
% \usepackage[T5]{fontenc}
% See https://www.latex-project.org/help/documentation/encguide.pdf for other character sets

% This assumes your files are encoded as UTF8
\usepackage[utf8]{inputenc}

% This is not strictly necessary, and may be commented out,
% but it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}

% This is also not strictly necessary, and may be commented out.
% However, it will improve the aesthetics of text in
% the typewriter font.
\usepackage{inconsolata}

% If the title and author information does not fit in the area allocated, uncomment the following
%
%\setlength\titlebox{<dim>}
%
% and set <dim> to something 5cm or larger.

\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\usepackage{multirow}
\usepackage{subfigure}
\usepackage{longtable}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{tabularx}
\usepackage{comment}
\usepackage{hyperref}
\usepackage{amsmath}

\usepackage{ulem}
\usepackage{subfigure}


\newcommand{\std}{\scriptsize$\pm$}

% \newcommand{\solution}[0]{NAME~}
\newcommand{\method}[0]{universal discriminator~}

\newcommand{\mc}[1]{\mathcal{#1}}

\newcommand{\lzy}[1]{\textcolor{green}{lzy: #1}}
\newcommand{\xhk}[1]{\textcolor{orange}{xhk: #1}}
\newcommand\zy[1]{\textbf{ \textcolor{red}{zhilin: #1}}}
\newcommand\yn[1]{{\textcolor{red}{#1}}}
\newcommand\zj[1]{\textcolor{blue}{#1}}


\title{A Universal Discriminator for Zero-Shot Generalization}

% Author information can be set in various styles:
% For several authors from the same institution:
% \author{Author 1 \and ... \and Author n \\
%         Address line \\ ... \\ Address line}
% if the names do not fit well on one line use
%         Author 1 \\ {\bf Author 2} \\ ... \\ {\bf Author n} \\
% For authors from different institutions:
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \And  ... \And
%         Author n \\ Address line \\ ... \\ Address line}
% To start a seperate ``row'' of authors use \AND, as in
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \AND
%         Author 2 \\ Address line \\ ... \\ Address line \And
%         Author 3 \\ Address line \\ ... \\ Address line}



\author{Haike Xu$^{1}$, Zongyu Lin$^{1}$, Jing Zhou$^{1}$, Yanan Zheng$^{2}$\footnotemark[1], Zhilin Yang$^{134}$\footnotemark[1] \\
$^1$Institute for Interdisciplinary Information Sciences, Tsinghua University \\
$^2$Department of Computer Science and Technology, Tsinghua University \\
$^3$Shanghai Artificial Intelligence Laboratory, $^4$Shanghai Qi Zhi Institute \\
\texttt{haikexu@mit.edu}, 
\texttt{\{zyanan,zhiliny\}@tsinghua.edu.cn} \\}

\begin{document}
\maketitle
\renewcommand{\thefootnote}{\fnsymbol{footnote}}
\footnotetext[1]{Corresponding authors.}



\begin{abstract}
Generative modeling has been the dominant approach for large-scale pretraining and zero-shot generalization. In this work, we challenge this convention by showing that discriminative approaches perform substantially better than generative ones on a large number of NLP tasks. Technically, we train a single discriminator to predict whether a text sample comes from the true data distribution, similar to GANs. Since many NLP tasks can be formulated as selecting from a few options, we use this discriminator to predict the option with the highest probability. 
This simple formulation achieves state-of-the-art zero-shot results on the T0 benchmark, outperforming T0 by 16.0\%, 7.8\%, and 11.5\% respectively on different scales. In the finetuning setting, our approach also achieves new state-of-the-art results on a wide range of NLP tasks, with only 1/4 parameters of previous methods.
Meanwhile, our approach requires minimal prompting efforts, which largely improves robustness and is essential for real-world applications. Furthermore, we also jointly train a generalized UD in combination with generative tasks, which maintains its advantage on discriminative tasks and simultaneously works on generative tasks.
%\xhk{Furthermore, by extending to an encoder-decoder architecture for generative tasks and restricting the prediction on "yes"/"no" tokens for discriminative tasks, we jointly train a generalized UD which maintains its advantage on discriminative tasks and has simultaneously comparable performance on generative tasks.}
\footnotetext[2]{Our code will be available at \href{https://github.com/Rafa-zy/UD}{https://github.com/Rafa-zy/UD}.
}

% We also achieve new state-of-the-arts on a wide range of supervised NLP tasks, using only 1/4 parameters compared with the previous model.
% In the setting of finetuning, our approach also outperforms generative baselines on a wide range of tasks. 



% This paper proposes a novel universal discriminator for zero-shot generalization, which has substantially improved zero-shot performance by \textbf{\color{red} more than 6\% (even up to 16\%)} across a variety of model scales and test tasks, and has set the new state-of-the-art (SOTA) among previous zero-shot approaches.
% While previous methods unify NLP tasks into generative LM tasks using prompts, we for the first time propose a discriminative task formulation that fuses inputs and targets to construct conforming or contrastive information. It does not rely on handcrafted prompts and is general for all NLP tasks.
% Based on the formulation, we propose the universal discriminator by further pretraining an auto-encoding pretrained model with a multitask dataset.
% Experiments prove that the universal discriminator is not only high-performing on zero-shot tasks, but also achieves SOTA on a \textbf{\color{red} large number of} finetuned tasks across a variety of types with only \textbf{\color{red} less than 1/10} number of parameters.
% Given the high performance, insensitivity and universality on both zero-shot and standard finetuned settings, we humbly believe universal discriminator potentially can serve as an alternative to the previous prompt-based generative paradigm and a strong baseline for future research.
\end{abstract}

\input{introduction}
\input{related_work}
\input{unified_task_formulation}
% \input{method}
\input{de-prompting}
\input{experiments}
\input{conclusion}

% \newpage

% \input{limitations}
% Entries for the entire Anthology, followed by custom entries
\bibliography{anthology}
\bibliographystyle{acl_natbib}

% \input{appendix}

\end{document}
