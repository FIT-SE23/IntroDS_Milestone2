\begin{table*}[t]
\setlength{\tabcolsep}{1.5mm}
\centering
% \small
% For prompt-based methods, we report the average performance over multiple prompts together with standard deviation for each task.}
\subtable[On 11 discriminative test tasks following the T0 benchmark.]{
\resizebox{\textwidth}{!}{%
    \begin{tabular}{l|l|c|ccccc|ccc|cc|c|c}
    \toprule[1pt]
    \multirow{2}{*}{Base Model} &
    \multirow{2}{*}{Method} &
    \multirow{2}{*}{\#Params} & 
    \multicolumn{5}{|c|}{\textbf{Natural Language Inference}} & \multicolumn{3}{|c|}{\textbf{Sentence Completion}} & \multicolumn{2}{c|}{\textbf{Coreference}} & \multicolumn{1}{c|}{\textbf{WSD}} 
    & \multirow{2}{*}{Avg.}\\
    & & & RTE & CB & ANLI1 & ANLI2 & ANLI3 & COPA & Hella. & Story. & WSC & Wino. & WiC &  \\
    \midrule[1pt]
    Decoder-only & GPT-3 & 175B 
        &63.5 &46.4
        &34.6 &	35.4&	34.5&	91.0&	78.9&	83.2&	65.4&	70.2&	- & -\\
    Decoder-only & GLaM & 137B 
        & 56.3	& 39.3	& 39.7	& 35.5	& 34.1	& 90.00	& 76.7	& 81.1	& 82.1	& 71.3	& 50.6 & 59.70\\
    MoE Decoder-only & GLaM & 64B 
        & 66.8	& 33.9	& 40.9	& 38.2	& 40.9	& 90.00	& 77.1	& 82.5	& 83.5	& 73.4	& 50.5 & 61.61\\
    Decoder-only & PaLM & 540B 
        & 72.9	& 51.8	& 48.40	& 44.20	& 45.70	& 93.00	& 83.4	& 84.6	& 89.1	& 81.1	& 59.1 & 68.48\\
    Decoder-only & FLAN & 137B 
        & 78.3	& 64.1	& 47.7	& 43.9	& 47.0	& 90.6	& 56.4	& 92.2	& 80.8	& 67.3 & - & -\\
    % ALBERT & UniMC & 235M 
    %     & 78.1	&75.7	&52.0	&44.4	&47.8	&95.2	&62.5		&- &78.8	&65.8	&-& -\\
    %\midrule[1pt]
    \midrule[1pt]
    %\multirow{3}*{\shortstack{ELECTRA \\  (335M)}}
    \multirow{3}*{\shortstack{ELECTRA}}
    & PE-CLS & 335M
        & 60.18	& 57.38	& 34.09	& 34.43	& 36.38	& 92.71	& 44.12	& 96.04	& 62.79	& 56.32	& 50.69	& 56.83
        \\
    & PE-PROB & 335M
        & 54.01	& 49.17	& 32.27	& 33.29	& 33.48	& 81.90	& 36.67	& 89.50	& 64.33	& 50.73	& 50.94	& 52.39 \\
    & PE-REP & 335M
        & 68.95	& 61.31	& 36.09	& 35.03	& 39.43	& 91.19	& 46.99	& 96.76	& 60.96	& 56.15	& 51.13	& 58.54
        \\
    \midrule
    \multirow{1}*{\shortstack{DeBERTaV3}}
    & \multirow{1}*{{UD (ours)}} & 304M
        & \multirow{1}*{\textbf{71.12}}
        & \multirow{1}*{\textbf{76.79}}
        & \multirow{1}*{\textbf{43.80}}
        & \multirow{1}*{\textbf{41.30}}
        & \multirow{1}*{\textbf{45.67}}
        & \multirow{1}*{\textbf{96.00}}
        & \multirow{1}*{\textbf{60.70}}
        & \multirow{1}*{\textbf{97.43}}
        & \multirow{1}*{\textbf{66.35}}
        & \multirow{1}*{\textbf{83.58}}
        & \multirow{1}*{\textbf{53.29}}
        & \multirow{1}*{\textbf{66.91}}
    \\
    % & &&&&&&&&&&&\\
    %\midrule[1pt]
    \midrule[1pt]
    \multirow{2}*{\shortstack{T5-Large}}
    & \multirow{1}*{T0 $\star$} & 800M
    % & Mean
        & 75.05	& 55.48	& 32.87	& 32.29	& 33.67	& 84.59	& 28.24	& 93.97	& 62.98	& 54.59	& 51.16	& 54.99 \\

    & {UD (ours)} & 400M
    % & Acc.
        & \textbf{83.75}
        & \textbf{80.36}
        & \textbf{36.80}
        & \textbf{34.20}
        & \textbf{42.17}
        & \textbf{90.00}
        & \textbf{56.07}
        & \textbf{96.37}
        & \textbf{68.27}
        & \textbf{62.90}
        & \textbf{54.55}	
        & \textbf{64.13} \\
    % \cline{2-15}\\[-1.5ex]
    % & Ours (w prompts) & Acc
    %     &
    %     & 
    %     \\
    %\midrule[1pt]
    \midrule[1pt]
    \multirow{3}*{\shortstack{T5-XL}}
    & \multirow{1}*{T0 $\dagger$} & 3B
    % & Mean
        & 64.55 
        & 45.36
        & 33.84
        & 33.11
        & 33.33
        & 72.40
        & 27.29
        & 84.03
        & 65.10
        & 50.97
        & 50.69
        & 50.97 \\

    & \multirow{1}*{T0 $\star$} & 3B
    & \textbf{79.71}	& 68.93	& \textbf{43.09}	& \textbf{38.46}	& 42.32	& \textbf{94.10}	& 31.49	& 97.47	& 68.75	& 61.31	& \textbf{54.09}	& 61.79\\
 
    & {UD (ours)} & 1.5B
    % & Acc.
        & 78.70
        & \textbf{73.21}
        & 41.20
        & 36.30
        & \textbf{45.42}
        & 94.00
        & \textbf{70.05}
        & \textbf{97.86}
        & \textbf{72.12}
        & \textbf{70.64}
        & 52.98	
        & \textbf{66.59} \\
    %\midrule[1pt]
    \midrule[1pt]
    \multirow{4}*{\shortstack{T5-XXL}}
    & \multirow{1}*{T0 $\dagger$} & 11B
    % & Mean
        & 80.83
        & 70.12
        & 43.56
        & 38.68
        & 41.26
        & 90.02
        & 33.58
        & 92.40
        & 61.45
        & 59.94
        & 56.58
        & 60.77 \\

    & \multirow{1}*{T0 $\star$} & 11B
    & \textbf{85.78}	& 73.33	& 47.28	& 41.97	& 46.12	& 94.43	& 31.51	& 98.44	& 62.79	& 72.80	& 55.96	& 64.58 \\

    & {UD (ours)} & 5.5B
    & 80.51	& 87.50	& 49.00	& 42.90 & 	48.83	& 95.00	& 77.38	& \textbf{98.61}	& 73.08	& 82.16	& 57.05	& 72.00 \\

    & {UD+ (ours)} & 5.5B
    & 81.95	& \textbf{89.29}	& \textbf{53.40} & \textbf{48.10} & \textbf{51.00} & \textbf{96.00} & \textbf{78.91} & 96.69	& \textbf{75.00}	& \textbf{86.42}	& \textbf{58.46}	& \textbf{74.11} \\
    % \midrule[1pt]
    % \multicolumn{2}{}{DeBERTaV3-Large (304M)}
    % & Ours & Acc. 
    %     & \\
    \bottomrule[1pt]
\end{tabular}
}
\label{tab:maintable:top}
}
\subtable[On 13 discriminative BigBench tasks following the T0 benchmark.]{
\resizebox{\textwidth}{!}{%
    \begin{tabular}{l|ccccccccccccc|c}
    \toprule[1pt]
    \multirow{2}{*}{Model} 
        & \multirow{2}{*}{\shortstack{code \\ desc.}}
        & \multirow{2}{*}{\shortstack{conce\\-ptual}}
        & \multirow{2}{*}{\shortstack{known\\unknowns}}
        & \multirow{2}{*}{\shortstack{logic \\ grid}}
        & \multirow{2}{*}{\shortstack{logic \\ deduction}}
        & \multirow{2}{*}{\shortstack{miscon\\-ceptions}}
        & \multirow{2}{*}{\shortstack{novel\\concepts}}
        & \multirow{2}{*}{\shortstack{strate\\-gyqa}}
        & \multirow{2}{*}{\shortstack{wino\\-why}}
        & \multirow{2}{*}{\shortstack{syllo\\-gisms}}
        & \multirow{2}{*}{\shortstack{movie\\dialog}}
        & \multirow{2}{*}{\shortstack{lang\\-uage\_id}}
        & \multirow{2}{*}{\shortstack{vita\\-minc}} 
        & \multirow{2}{*}{Avg.}\\
    &&&&&&&&&&&&&&\\
    \midrule[1pt]
    T0-Large$\star$ & 14.07 & 40.38 & 60.42 & 38.00 & 41.16 & 50.00 & 10.00 & 52.31 & 49.72 & 50.34 & 46.76 & 16.01 & 46.16 & 39.64 \\
    T0-XL$\star$ & 23.44 & 48.08 & 64.58 & 42.50 & 50.07 & 52.68 & 25.00 & 53.05 & 45.37 & 50.20 & 47.69 & 19.00 & 60.03 & 44.75 \\
    T0-XXL$\star$ & 18.75 & 37.50 & 68.75 & 29.70 & 22.94 & 55.36 & 35.00 & 59.06 & 48.54 & 49.77 & 50.71 & 19.71 & 37.77 & 41.04\\
    \midrule
    UD-DeBERTaV3 & \textbf{76.67} & \textbf{64.08} & \textbf{76.09} & 39.90 & \textbf{54.93} & 50.23 & \textbf{50.00} & \textbf{59.91} & 45.76 & 50.41 & \textbf{57.72} & 13.33 & \textbf{61.51} & \textbf{53.89} \\
    UD-Large & \textbf{51.67} & \textbf{54.37} & 47.83 & 33.40 & 34.60 & 50.23 & \textbf{26.47} & 47.03 & 45.69 & 50.56 & \textbf{51.71} & 16.33 & \textbf{55.77} & \textbf{43.51} \\
    UD-XL & \textbf{53.33} & \textbf{73.79} & 65.22 & 37.20 & \textbf{37.80} & 47.95 & \textbf{35.29} & 53.10 & 45.31 & 50.42 & \textbf{50.13} & \textbf{22.88} & \textbf{63.74} & \textbf{48.93} \\
    UD-XXL-SOTA & \textbf{63.33} & \textbf{82.52} & \textbf{84.78} & \textbf{39.20} & \textbf{67.53} & 49.32 & \textbf{58.82} & \textbf{64.15} & 47.50 & 50.39 & \textbf{57.85} & \textbf{27.27} & \textbf{70.22} & \textbf{58.68} \\
    \bottomrule[1pt]
    \end{tabular}%
    }
\label{tab:maintable:bottom}
}
\caption{
Zero-shot performance of our UD and baselines.
Results in the first block are reported by previous work, respectively from GPT-3~\cite{gpt3-paper}, GLaM~\cite{glam}, PaLM~\cite{palm}, and FLAN~\cite{FLAN}.
Note that we provide these reported results for reference, and do not compare directly. Some of the reported tasks are evaluated on the test split, while we follow the better baseline method T0 to report on validation splits.
Results with $\dagger$ are reported by~\citeauthor{T0-paper}, and results with $\star$ are reproduced in our framework. We reproduced the three variants of prompting ELECTRA~\cite{xia2022prompting} under our setting, denoted as ``PE-CLS'', ``PE-PROB'', ``PE-REP''.
In the same group, T0 has 2x model parameters compared to UD. For abbreviation, we denote UD based on T5-XX as ``UD-XX'', e.g., UD-XL refers to UD based on the T5-XL model.
}
%\xhk{I delete the "encoder" in base model name and instead divide \#params by 2 for UD models. currently remove UniMC results.}
\label{tab:maintable}
\end{table*}