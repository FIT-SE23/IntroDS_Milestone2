% \newpage

\input{tables/main_table}
% \input{tables/finetuned_tasks}
\input{tables/seq2sequd_11tasks}

\input{tables/new_finetuned_tasks}


\section{Experiments}

\input{tables/ablation}
\input{tables/ablation_base_model}
\input{tables/distribution_table}

\subsection{Experimental Setup}\label{sec:setup}

We performed extensive experiments to validate the performance of the zero-shot generalization of our UD. We follow the same zero-shot setting as T0~\citep{T0-paper} by training on multi-task datasets and evaluating a held-out set of tasks that are never seen during training. 

\paragraph{Datasets}
The original T0 training set consists of 38 tasks of 8 different types.
% ~\footnote{We did not consider T0+ and T0++, since they are partially intersected with the test sets, making some test tasks unable to be evaluated under the zero-shot setting.}
There are in total 21/38 discriminative training tasks, with which we train the UD.
% ~\footnote{The original paper~\citep{T0-paper} claims 39 training datasets but releases a training set with 38 datasets (``common\_gen''  excluded). We directly start with the released data.}. 
% It consists of a majority of discriminative tasks and a small number of generative tasks.
% We train our \method with 21 discriminative tasks within.
% , which is around 55\% of the original T0 training data.\xhk{change to 21/38=0.55}
The evaluation set covers four types of tasks, including natural language inference (RTE~\citep{2005_RTE}, CB~\citep{de2019_CB}, ANLI/R1-R3~\citep{NieWDBWK20_ANLI}), coreference resolution (WSC~\citep{WSC2012}, Winogrande~\citep{SakaguchiBBC20_winogrande}), sentence completion (COPA~\citep{COPA2011}, StoryCloze~\citep{story_cloze}, Hellaswag~\citep{ZellersHBFC19_hellaswag}), and word sense disambiguation (WiC~\citep{wic-paper}).
Following T0, we use accuracy on the validation split as the evaluation metric.
For prompt-based baselines, we report the average accuracy over multiple prompts for each test task.
Besides, we also evaluate zero-shot performance on several BigBench~\cite{bigbench} tasks, which are also adopted by T0~\cite{T0-paper}.\footnote{The original T0 reported results on 14 BigBench tasks. We separately report the results of 13 discriminative tasks and the other generative task in the following.}


% \lzy{Noted that we also evaluate the zero-shot performance on a subset of Big-Bench Benchmark~\cite{bigbench} adopted by original T0 paper~\cite{T0-paper}.\footnote{The original T0 reported results on 14 BigBench tasks. In our work, we focus on 13 discriminative tasks, leaving improving performance of the only generative tasks for future exploration. \zy{No need to say that. We also have generation results. Just say we're gonna report generation result separately.}}}


\paragraph{Baselines}
We primarily compare our method with T0~\citep{T0-paper}, which is a generative approach.
% that shares the same goal as UD (i.e., zero-shot generalization), but uses a totally different framework (i.e., generative or discriminative) as well as input format (i.e., prompt or minimal prompt).
Another baseline is prompting ELECTRA~\cite{xia2022prompting} which is a recent work on discriminative modeling.
Since it was proposed in a different setting (i.e., a  few-shot setting or direct zero-shot inference without any finetuning), we reproduced their method under our multitask zero-shot setting for comparison.

For a fair comparison, we follow T0 to use the T5-V1.1-LM-Adapted~\citep{T5-paper} as the backbone model, and we experimented with three different scales, respectively 800M, 3B, and 11B. 
For UD, it only makes use of the encoder of T5-v1.1 and additionally replaces the output layer with a classification head.
Moreover, for direct comparison with \citet{xia2022prompting}, we use DeBERTaV3-Large \citep{debertav3} as the backbone model which shares the same bidirectional architecture and has a smaller number of parameters.

In addition, we also provide reported zero-shot results of several large language models (with hundreds of billions of parameters) for reference, including GPT-3~\cite{gpt3-paper}, GLaM~\cite{glam}, PaLM~\cite{palm}, and FLAN~\cite{FLAN}.


% we also experiment with another backbone DeBERTaV3-Large~\citep{debertav3} to achieve better zero-shot performance.

\paragraph{Training}
% We implemented both baselines and our method, and perform experiments with exactly the same environments.
During training, we truncate the input sequence to 256 tokens and use a batch size of 256. For optimization, we use the Adam optimizer with a fixed learning rate of 1e-5 and a dropout rate of 0.1. Each experiment is trained with 10, 8, and 5 epochs respectively for 800M, 3B, and 11B models.
% We perform checkpoint selection by directly using the final (fixed-epoch) checkpoint for evaluation.
% \xhk{by choosing the one with the maximal average zero-shot performance per xxx steps.}

% For data processing, similar to T0, we truncate any dataset with over MAX\_DATA\_SIZE to have MAX\_DATA\_SIZE / num\_prompts. 
% Different from ~\citet{T0-paper} that uses a value of 500k for MAX\_DATA\_SIZE, we use a value of 50k, which experimentally yields better zero-shot performance for the T0 baseline.
% The training data of UD are produced by \xhk{replacing different prompted data version from T0 training data with only one minimal prompted version}, which strictly guarantees all methods share same raw task data.




% \subsection{Main Results}
\subsection{Main Results on Zero-Shot Tasks}

\paragraph{UD Zero-Shot Results}
The main results are presented in Table~\ref{tab:maintable}.
We compare methods of similar scales. 
Results in Table \ref{tab:maintable:top} show that our UD substantially outperforms the T0 baseline on average by a large margin of around 9, 5, and 7 points respectively at Large, XL, and XXL scales.
Comparing the results of UD-T5-Large, UD-DeBERTaV3, and prompting ELECTRA, both variants of UD also substantially outperform prompting ELECTRA by more than 6 points.
% In addition, UD also demonstrates superior zero-shot ability compared with models with hundreds of billions of parameters (see results in the first block of Table~\ref{tab:maintable:top}).
On BIG-Bench datasets, results in Table \ref{tab:maintable:bottom} show that our UD outperforms the T0 baseline by a margin of around 4-8 points.
Overall, these results demonstrate the advantages of UD at every scale, and a broad range of tasks compared with baselines.

Another interesting finding is that the advantages of UD significantly increase along with scaling.
When scaling from Large-scale to XL-scale (i.e., around 3.75x of the parameters), the average performance improves by around 2 points. However, when scaling from XL-scale to XXL-scale (i.e., 3.6x of the parameters), the improvements of average zero-shot performance enlarge to 8 points.
Based on the observation, we hypothesize that UD can achieve even better performance of zero-shot generalization if further scaling to an even larger models, which we leave to future work.

% Results show that our \method substantially outperforms our baseline T0 on average zero-shot performance, by a large margin of around 8, 5, and 7 points respectively at the Large (800M), XL (3B), and XXL (11B) scales.
% \xhk{Our UD also substantially outperforms ELECTRA in the Large (800M) scale.} 

To further boost the zero-shot performance, we also train a new variant of UD at 11B scale by scaling to more training tasks, including the discriminative English tasks used in \citet{1600tasks}, and the discriminative English tasks used in \citet{ul2}. The new model is denoted as UD+.
UD+ achieves the highest average accuracy among all the zero-shot evaluation tests.

% \begin{comment}
% ul2 (CommonsenseQA \cite{commonsense_qa}, ),
% csqa2.json 9264
% glue_cola.json 8551
% glue_sst2.json 67349
% glue_stsb.json 5749
% mcscript.json 19462
% mcscript2.json 28382
% openbookqa.json 19828
% qasc.json 40670
% qasc_with_ir.json 40670
% race_high.json 249780
% race_middle.json 101684
% social_i_qa.json 100230
% super_glue_boolq.json 9427
% super_glue_multirc.json 27243
% ai2_science_elementary.json 2493
% ai2_science_middle.json 2424
% onestopqa_advanced.json 1296
% physical_iqa.json 33211
% protocol_comparison_harsht.json 3698
% reclor.json 3726
% ai2_arc_ARC_Easy.json 9002
% ai2_arc_ARC_Challenge.json 4476r,  
% \end{comment}
% \zy{what data?}

% \xhk{move bigbench result in appendix A.1 here. into Table 2}
% \yn{add bigbench analysis}

% , in the mean time still guaranteeing that there is no overlap between training tasks and the held-out tasks.

% \xhk{We also extend our training datasets (please refer to appendix~\ref{sec:ud_plus_data}) and train a model UD+. 
% UD+ achieves the highest average accuracy among all the zero-shot evaluation test, in the mean time still guaranteeing that there is no overlap between training tasks and the held-out tasks.}

% \yn{do we need to add the following?}
% \yn{
% Interestingly, we also have observed some findings on zero-shot performance along with scaling.
% For baseline T0, the zero-shot performance keeps improving on most of the datasets when scaling to larger-scale models.
% Exceptions are Hellaswag and WSC, where zero-shot performance on them are basically unchanged when scaling.
% For our \method, the performance of zero-shot generalization consistently improves with the model scale increasing on all sentence completion and coreference resolution tasks, and partial NLI tasks.
% Exceptions are that RTE, CB and WSC demonstrates a degradation on zero-shot performance when scaling from large to XL scale.
% This could be explained that 
% % {\color{red} xxxxx}
% % \xhk{I guess if we add minimal prompts, RTE and CB will improve for larger model...? so maybe we can remove this observation for now?}
% }


% \paragraph{Results on Finetuned Tasks}

% To evaluate the performance on finetuned tasks, we finetuned T0/UD respectively on each training task. This is similar to multi-task finetuning \cite{T5-paper}.
% % We use this experiment to test the effectiveness of UD with abundant labels. 
% We experimented with all the T0 discriminative training tasks.
% Table~\ref{tab:finetunedtasks} shows the finetuning results on T0 and UD at the 11B scale.
% We observe that UD outperforms T0 on \textbf{\color{red} xxx/19} of the considered finetuned tasks.
% To be specific, on topic classification tasks, paraphrase identification tasks, and multiple-choice QA tasks, UD shows the largest advantages against T0. These finetuning results demonstrate that UD does not only perform well in the zero-shot setting but also improves performance when abundant labels are available.



% \yn{add a new subsection of seq2seqUD}
\paragraph{Generalized UD Zero-Shot Results}

The zero-shot results of generalized UD on 11 T0 discriminative test tasks and on 13 Big-Bench tasks are respectively reported in Table~\ref{tab:gen_ud:top} and Table~\ref{tab:gen_ud:mid}.
In addition, to test how generalized UD performs on zero-shot generative tasks, we also select 4 generative tasks from Big-Bench for evaluation. Results are presented in Table~\ref{tab:gen_ud:bottom}.


Analyses are as follows.
(1) Comparing the results of generalized UD and T0, generalized UD still holds significant improvements on discriminative tasks.
(2) Comparing generalized UD with our previous UD (in Table~\ref{tab:maintable}), we observe there is a slight decrease in average performance, proving that adding generative tasks into training could have impacted a little bit, in trade for capability for handling generative tasks.
(3) On 4 generative zero-shot tasks, both generalized UD and T0 show comparable results.
(4) On 13 discriminative BigBench tasks, we observe that UD-Large outperforms T0-Large by 6.67\%, UD-XL outperforms T0-XL by over 4\%, and Generalized UD-XL outperforms T0-XL by over 6\%, further indicating the effectiveness of our proposed framework.


%\yn{recheck the analysis along with table data!}

% Comparing Generalized UD with methods in Table~\ref{tab:maintable} (i.e., UD and T0) of similar scales, we observe the zero-shot performance on discriminiative tasks slightly decrease but generally hold still, compared to UD (ours).

% It still significantly outperforms baseline T0 to a large degree.
% From Table~\ref{tab:gen_ud}, we shall observe, on generative tasks both generalized UD and T0 show comparable results.}









\subsection{SOTA Results on Finetuned Tasks}
\label{sec:ud_finetune}

To explore how UD performs on fully-supervised tasks, we finetuned UD for a wide range of downstream tasks and reported their results in Table \ref{tab:finetune}.
% To explore whether UD can help improve the performance in fully-supervised learning, we conduct experiments by finetuning each downstream task. 
For each finetuning experiment, the maximum training epoch is set to be 10.
We search a hyper-parameter space with learning rate in \{2e-5, 1e-5, 5e-6\}, batch size in \{32, 64, 128\}.
We select the best checkpoint using a validation set with early stopping.
% We set the maximum training epoch to 10, search the hyper-parameters (learning rate in \{2e-5, 1e-5, 5e-6\}, batch size in \{32, 64, 128\}) and select the best checkpoint based on the validation set with early stopping.

% Results are in Table \ref{tab:finetune}.
From results in Table \ref{tab:finetune}, we find that UD can achieve remarkable performance on most of the downstream tasks. 
We achieve state-of-the-art performance on 12 out of the 17 tasks we evaluated. The results also show that more challenging tasks (tasks that require more knowledge) will benefit more from the multi-task training period, especially some QA tasks.








\subsection{Ablation Study}

We have also conducted ablation studies to further explore how several factors affect the performance of zero-shot generalization. 

\subsubsection{Instructive Prompts vs Minimal Prompts}

UD employs minimal prompts that use simple concatenation, while previous approaches rely on lengthy instructive prompts to provide more detailed instructions \cite{T0-paper,FLAN,gpt3-paper}. 
Statistically, we count the average number of prompt words (excluding raw input) for both minimal and instructive prompts, and statistics are respectively $0.4$ versus $>10$.
% \xhk{A statistic comparison on the average number of the prompt word count (excluding raw input) is $0.4$ for minimal prompts versus $>10$ for previous instructive prompts.}  
We compare these two types of prompts in the following experiment.
We adopt the instructive prompts from T0 and apply them on UD without changing the discriminator formulation. To construct minimal prompts for T0, we remove all the instructive words similar to UD.

% It is an interesting question whether minimal prompts also play a role in the \method, 
%considering that concatenating task data with prompts theoretically indeed reduces all tasks into the original LM tasks, hence improving task generalization.
% considering that simple concatenation of task data's keywords with a minimal prompt is enough to unify it into the UD format.

% We compare the zero-shot performance when using prompt and minimal prompt for \method and prompt and prompt-free for T0. 



% To construct instructive prompts for UD, we adopt the instructive prompts from T0 we concatenated the prompted inputs and each target choice (verbalizer).~\footnote{Here we use the same prompts as T0.} The corresponding \method label is 1 when concatenating correct target choice and 0 otherwise.

% \xhk{To construct prompt-free inputs for T0, we directly remove all the prompt words, still letting the model to predict the target verbalizer.}


Results are shown in Table~\ref{tab:promptablatiion}. We observe that minimal prompts yield better performance for UD than instructive prompts. In contrast, for T0, instructive prompts perform much better than minimal prompts. These results are consistent with our motivation that UD tends to unify the tasks better with a shared discrimination formulation. As a result, task-specific instructions are not necessary and might hurt generalization performance. Generative approaches, on the other hand, rely on instructive prompts to better distinguish different tasks.


% \xhk{for our UD method, minimal prompt version has better accuracy, because under the unified UD task format, task descriptive language in prompt is no longer needed and may even increase the sentence complexity to be understood by LM. Additionally, UD's tasks is to discriminate between correct and wrong choices where prompts are identical phrases in each choice's concatenated sentence, so prompts actually play no role in the discriminating process. However, for T0, prompted version has better accuracy than the prompt-free version (note that prompt-free is the extreme and usual case for minimal prompting) because generative model's goal is to generate the correct verbalizer from the huge vocabulary, which can be efficiently narrowed by the existence of prompts. Therefore, we can conclude that minimal prompted format works well for discriminative models and prompted format works well for generative models.}



\subsubsection{Ablation on Base Models}

We also study the effects of using different backbone pretrained models. We experiment with three backbone models of different types, respectively the encoder part of an encoder-decoder model, an encoder model, and a decoder model. Specifically, we use the T5 encoder, DeBERTa \cite{debertav3}, and GPT \cite{radford2018gpt} respectively for these three types. It is noteworthy that though similar in architecture for both T5 encoder and DeBERTa, they are pretrained with different self-supervised language modeling tasks, which in fact leads to huge differences in zero-shot generalization, as we will show in Table~\ref{tab:ablationbasemodel}.
% We study the effect of different backbone pretrained models. We experiment with three types of backbone models---using the encoder part of an encoder-decoder model, using an encoder model, and using a decoder model. We use the T5 encoder, DeBERTa \cite{debertav3}, and GPT \cite{radford2018gpt} respectively for these three types.






% We study the effect of different types of models (discriminative vs. generative), or backbone models (auto-encoding vs. auto-regressive), on zero-shot generalization with \method. In addition to T5-Encoder, we also experiment the advanced DeBERTaV3-Large~\cite{debertav3} that has achieved new SOTA on a diverse set of tasks. We also implement GPT-XL for comparision.

% Results are shown in Table~\ref{tab:promptablatiion}.

% shows the results between discriminative and generative models with fixed prompted or not version. It can be observed \xhk{no matter we use promped data or minimal prompt/prompt-free data, our discriminative models always have better zero-shot generalization performance than generative models.}



Results of different backbone models are presented in Table \ref{tab:ablationbasemodel}. 
Among all three types of backbone models, the encoder backbone models appear to be the most suitable type of backbone, where both encoder models of two scales respectively achieve the best and the second best results, outperforming all the others by more than 5 points.

Using the same number of parameters (i.e., 1.5B), both DeBERTa-V2 and T5-Encoder significantly outperform GPT-XL, which demonstrates that a bidirectional architecture works better than the unidirectional architecture for the discriminator formulation.
In addition, DeBERTa-V2 outperforms T5-Encoder by 7 points, implying that not only model architecture but also the self-supervised pretraining task determines the ability of UD discrimination. Models pretrained with masked language modeling tasks are more suitable for UD.

The impacts of the architecture and pretraining tasks of backbone models are even larger than the influence of scale, as we also observe that an encoder model with 300M parameters (i.e., DeBERTaV3) achieves much better performance than the T5 encoder and GPT-XL with 1.5B parameters.

% Results are shown in Table \ref{tab:ablationbasemodel}. Using the same number of parameters, encoder backbone models (i.e., DeBERTa) substantially outperform the T5 encoder and the GPT decoder. This indicates that pretrained encoders are more suitable for our discriminator formulation. Interestingly, an encoder model with 300M parameters (i.e., DeBERTaV3) achieves much better performance than the T5 encoder and GPT-XL with 1.5B parameters.








% Table~\ref{tab:ablationbasemodel} shows the results for different discriminative models, where DeberTa consists of solely an encoder, T5-Encoder is the encoder part of the full T5 model, GPT-XL consists of an encoder and a decoder. We can observe that the encoder structure performs better for discriminative tasks.

% \subsection{What Contribute to the Zero-Shot Generalization of \method?}

\subsection{How Well UD Generalizes to a Broader Domain?} \label{sec:generalize}

In the previous sections, we have trained UD to solve the task of discriminating whether a text sample comes from the true data distribution of natural language. So far we have constrained the problem to supervised labeled tasks. However, this discrimination problem formulation is in fact general and can be applied to a broader domain of natural language. We conduct the following experiment to see how UD generalizes.


% In order to explore the mechanism of the universal discriminator and explain how it promotes zero-shot generalization. We conduct the following extensive experiment.

To test whether a model discriminates against the true data distribution, a straightforward way of verification is to compare the probability of real data with that of some generated, fake data. This form of verification is not specific to any downstream task and can be viewed as generalizing to a broader domain. Formally, given a text sample $x$, let $D(x)$ be the output of UD, which estimates the probability that $x$ is sampled from the true data distribution, i.e., $P(\text{true} | x)$. Given a true data sample $x$ and a generated data sample $x'$, we expect a well-trained UD to predict $D(x) > D(x')$.

% First, we assume that the essence of our universal discriminator $D$ is to learn whether the data are sampled from the real text distribution or not. A straightforward way to verify this key point is to compare the likelihood of the real data label given real data x computed as $D(x)=p(y=1|x)$ with the likelihood of the real data label given generated data $x'$ computed as $D(x’)=p(y=1|x’)$. 

Specifically, we randomly select 2,600 real data samples $x$ from the validation set of the T0 training data and generate the data $x’$ in two different ways: model-based generation and manual generation.

For a model-based generation, we utilize the T0-Large model with a paraphrase prefix ``Paraphrase the sentence:'' to generate data $x'$. It is expected that the generated samples $x'$ are similar to true samples $x$ to some extent but demonstrate some flaws that are unique to generated data. For a manual generation, we manually create some conflict or contradiction in the real sample $x$. Specifically, we manually attach wrong answers to the original data and obtain $x’$ , which is similar to what we have done in constructing negative samples in our main framework. 

We then use our \method based on T5-Encoder Large to compute the probability $D(x)$ and $D(x')$ for both real and generated data. As displayed in Table~\ref{tab:explain}, we find that the \method assigns a higher score for $x$ than $x'$ $80\%$ of the time for manually-generated data. When tested with model-generated data, UD assigns a high probability for real data in $74\%$ of the cases.
This is probably because manually generated data are more paradoxical and logically incoherent and thus are easier for UD to discriminate. Overall, these results demonstrate that the discrimination ability of UD is not limited to the downstream tasks on which it was trained, but is also generalizable to a broader domain of text data. This indicates a possibility of extending UD to other scenarios such as model pretraining and generation tasks.


% For model-based generation, we utilize two models which generate high-quality and low-quality data $x$. It should be noted that we hope the generated $x'$are similar to $x$ to some extent.
% First, we leverage a T5-small model [citation] to generate similar semantics to real data x by feeding the $x$ with the prefix  ‘paraphrase:’. Obviously, the generated $x'$ are bound to be far from real data distribution. Then, we utilize the T5-small model to finetune on quora for paraphrase identification task. Then we leverage the finetuned T5-small model to do the same paraphrase generation as before and yield $x’$ with relatively high quality. 

% For heuristic-based generation, we manually create some conflict or contradiction in the real data $x$. In detail, we randomly shuffle the words given each real data sample and get inconsistent data $x’$.

% After generating the data $x’$ from different approaches, we evaluate the likelihood of real data distribution given real data $x$ and generated data $x’$, which is formulated as $D(x)=p(y|x)$ and $D(x’)=p(y|x’)$ respectively. The results are shown in Table [reference] and the generated data examples are presented in Appendix [reference].




