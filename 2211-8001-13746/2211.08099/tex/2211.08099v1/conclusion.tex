\section{Conclusions}

In this work, we challenge the convention of zero-shot learning by proposing the universal discriminator.
We unify all discriminative NLP tasks as predicting whether a sample comes from the true data distribution and train a universal discriminator correspondingly. 
Experiments show that the universal discriminator sets the new state-of-the-art for zero-shot generalization, consistently outperforming T0 by a large margin ranging from 7.8\% to 16.0\% across three different scales. The universal discriminator is high-performing with minimal prompting, and thus is more robust and applicable in practice. We also train a generalized UD to solve generative tasks at the same time which keeps UD's advantage on discriminative tasks and has comparable performance on generative tasks.

Even though our generalized UD can get comparable performance on some generative tasks, generalized UD may not handle certain complex generation tasks very well (e.g., summarization) and its performance on generative tasks is not siginifcantly better than other models' in comparison to its advantage on discriminative tasks over other models. We leave expanding UD to solve a broader range of generative tasks and achieve greater performance advantage as our future work. 