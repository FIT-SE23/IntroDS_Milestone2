\section{Related Work}

\subsection{Zero-Shot Generalization Using PLMs}
Pretrained language models (PLM) can transfer knowledge from training data to downstream tasks.
Prompting methods further narrow the gap between training data and downstream tasks. \citet{PET-paper} reformulate NLP tasks into cloze filling using prompts so that PLMs can conduct zero-shot inference by generating tokens given prompted inputs. \citet{meng2022generating} use PLMs to generate class-conditioned texts with the guidance of prompts without seeing any task-specific data.
Most recently, researchers have introduced natural language prompts to unify various kinds of tasks and propose a multi-task prompted training framework to achieve great zero-shot performance even faced with unseen downstream tasks (\citet{FLAN,T0-paper}).
However, zero-shot learning has been dominated by generative approaches.
% The success of this new paradigm motivates us to follow and iterate this line of research by proposing a universal discriminator to unify and train all tasks.


\subsection{Prompt-based and Prompt-free Methods in NLP}
Prompting is the method of reformatting NLP tasks using natural language templates to adapt to downstream tasks \cite{T5-paper,PET-paper}.
% With the help of prompts, LMs can directly predict the desired answer without training on task-specific data, making it possible to perform well under few-shot, even zero-shot setting. Also, prompts serve as a decent lubricant for multitask learning which facilitates the development of text-to-text pretrained models such as T5 (\cite{T5-paper}). 
To reduce the instability and labor costs brought by prompting, researchers have tried various approaches (\citet{ptuning-paper,he2021towards}) to learn continuous prompts. 

Recently, prompt-free methods are also being explored. \citet{mahabadi2022prompt} adopts task-specific adapters to learn task descriptions implicitly for few-shot learning with PLMs. 
It has also been indicated that using null prompts without task-specific templates can achieve decent performance compared with manually-designed prompts on various tasks (\citet{logan2021cutting}).

Our work further shows that minimal prompting performs better with our discriminative formulation in the multi-task zero-shot learning setting.

% liberate the time-cosuming manual prompt design lying in multi-task prompted pretraining (\citet{T0-paper,FLAN}) by adopting a universal task formulation and utilizing a universal discriminator to achieve better zero-shot generalization.

\subsection{Discriminative Models in NLP}

% Pretrained language models often adopt masked language modeling (MLM) as the self-supervised learning objective, e.g., BERT, RoBERTA (\citet{devlin2018bert,liu2019roberta}), where the input texts are corrupted by masked tokens and the models are trained to recover the original tokens. They performed well on a wide range of downstream tasks, yet require a large number of training samples and parameters to achieve decent generalizibility.

PLMs trained with masked language modeling (MLM) \cite{devlin2018bert,liu2019roberta} can be finetuned in a discriminative manner for downstream tasks. 
ELECTRA \cite{clark2020electra} trains a discriminator to detect whether a token has been replaced. WKLM \cite{xiong2019pretrained} employs an entity-centric approach for pretraining and predicts whether an entity has been replaced.
However, finetuning for these methods is usually based on one separate CLS head per task, which is not suitable for zero-shot generalization.

% To build a more sample-efficient pretraining framework, ELECTRA (\citet{clark2020electra}) changes the MLM objective to a replaced token detection (RTD) objective, where a generator replace several tokens in the training texts and a discriminator will predict whether each token is replaced or not. WKLM (\citet{xiong2019pretrained}) develops a entity-centric pretraining objective which detects and replaces the entity words with the entities of the same type in each document and train the model to predict whether each entity is replaced.

Recently, prompting has been combined with token-level discriminators based on ELECTRA for few-shot learning \cite{yao2022prompt,xia2022prompting}. While these are also discriminative approaches, there are a few key differences from our approach. First, these methods use ELECTRA to perform token-level discrimination, while we perform sequence-level discrimination, which is more flexible to tasks that have multi-word verbalizers or do not have verbalizers at all. Second, we unify all discriminative tasks into one single task with minimal prompting, which is convenient for zero-shot generalization and removes the need for designing task-specific prompts. Third, these methods are specific to ELECTRA-like pretraining, while our approach accepts arbitrary pretrained encoders. In our experiments, we will also make a direct comparison with these approaches to demonstrate our effectiveness.

% \xhk{Another concurrent work \cite{UniMC} shares a similar idea with our discriminative approach. In specific, given an example of a multiple-choice task, their method concatenates the tokens of input passage, question, and all the options together as their model's input text. Then they use their proposed option masked language modeling (O-MLM) and option prediction (OP) in the training phase. \zy{too many details} Although their method also entails a similar idea of the "discriminative approach", our UD method implements this idea in a completely different way: We are training UD to predict whether a text sample combined with every single option comes from the true data distribution of natural language. Our training method is more concise, more efficient in terms of the token length, and more beneficial for other potential settings \zy{not clear based on context} (see fine-tune Section~\ref{sec:ud_finetune}).} 

% \zj{Another concurrent work \cite{UniMC} shares a similar idea with our discriminative approach. In specific, given an example of a multiple-choice task, they concatenate all inputs and options together and then use option masked language modeling (O-MLM) and option prediction (OP) for training. Although their method also entails a similar idea of the ``discriminative approach'', our UD method implements this idea more concisely and efficiently. We are training UD to predict whether a text sample combined with every single option comes from the true data distribution of natural language. (See fine-tune Section~\ref{sec:ud_finetune} for details)} 

% \xhk{[haike: not sure if it is better to just stop here, or mention empirical performance comparison]}

% \xhk{It is hard to fairly compare our UD's empirical performance with UniMC because UniMC selects a set of multiple-choice training tasks to induce better zero-shot performance, whereas we restrict our UD using exactly the same training datasets in T0. Furthermore, UniMC is not tested on some tasks in the T0 benchmark and it is only implemented using small backbone models.}

% Recently, with the development and popularity of prompt-based approaches, researchers further adapt prompt-based few-shot learning to ELECTRA and achieves better performance on downstream tasks (\citet{yao2022prompt,xia2022prompting}).

% In our work, we borrow the similar idea from the discriminative pretrained language models and adapt to multi-task prompted training paradigm. The major difference is that discriminative models such as ELECTRA focuses on the pretraining stage, while our model puts emphasis on the multi-task finetuning stage and demonstrate that unifying all tasks (e.g., T0) into \xhk{discriminative tasks} classification problem can greatly boost the zero-shot task generalization while reducing the prompt-design efforts and the number of training samples.


%% to be discussed
% \subsection{Generative Adversarial Nets}

