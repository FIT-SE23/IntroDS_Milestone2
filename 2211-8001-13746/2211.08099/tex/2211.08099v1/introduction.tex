\section{Introduction}

Generative modeling has been the dominant approach for large-scale pretraining and zero-shot generalization~\cite{gpt3-paper,artetxe2021efficient,rae2021scaling}. 
Combined with prompts~\cite{gpt3-paper}, most of the natural language processing (NLP) tasks can be formulated into the fill-in-the-blank format and perform generative language modeling.
Based on the unified generative formulation, pretrained models such as GPT-3~\cite{gpt3-paper}, BERT~\cite{devlin2018bert,PET-paper}, T5~\cite{T5-paper}, can perform zero-shot inference on new tasks. 


More recent work~\cite{T0-paper} proposed to further pretrain a generative T5~\cite{T5-paper} with multitask prompted datasets and has substantially enhanced the performance of zero-shot generalization. 
In contrast, methods based on discriminative modeling~\cite{devlin2018bert} have not been able to achieve state-of-the-art performance on zero-shot learning. The adoption of discriminative approaches for zero-shot learning has been limited in the literature.


% Although there are a few works using discriminative modeling to perform zero-shot or few-shot learning, such as CLS finetuning using BERT or prompting using ELECTRA
% For example, BERT was CLS finetuned to perform zero-shot/few-shot learning, however, the zero-shot/few-shot performance are lagged far behind.

% \zy{Add a note: although BERT can be CLS finetuned (which is discriminative), but it is not the SOTA approach for zero-shot and few-shot learning.}

\begin{figure}%[htbp]
     \centering
     \includegraphics[width=1.05\linewidth]{figure/final_sota.png}
     \vspace{-15pt}
     \caption{Average zero-shot performance over 11 zero-shot tasks for our Universal Discriminator and T0~\cite{T0-paper}. Our universal discriminator significantly outperforms T0 across three different scales.}
     \label{fig:sota}
     \vspace{-15pt}
 \end{figure} 


In this work, we challenge the convention of zero-shot learning and propose to study and improve discriminative approaches. This is motivated by the fact that many NLP tasks can be framed as selecting from a few options; e.g., telling whether sentence A entails sentence B, or predicting which answer is correct for a given question. We call these tasks \textit{discriminative tasks}. As we will discuss in later sections, a significant portion of NLP tasks is in fact discriminative tasks. We hypothesize that discriminative approaches perform better for discriminative tasks.
% Despite the recent progress, it remains unknown how discriminative approaches perform in zero-shot generalization. Motivated by the fact that discriminative modeling learns to distinguish among options and goes better with discriminative tasks (e.g., telling whether sentence A entails sentence B, or telling which option correctly answer the question), we hypothesize that discriminative modeling would be better at zero-shot generalization, especially on discriminative tasks.

To verify the hypothesis, we propose the \textbf{universal discriminator (UD)}, which substantially improves zero-shot generalization over the previous generative state-of-the-art (SOTA)~\cite{T0-paper}, as Figure~\ref{fig:sota} shows.
The main idea is to train a single discriminator to predict whether a text sample comes from the true data distribution of natural language, similar to GANs \cite{goodfellow2014generative}. Given a set of training tasks with labeled data, we construct a dataset with positive and negative examples, where positive ones are in-distribution natural language samples and negative ones are out-of-distribution. There are two major types of discriminative tasks. The first type is tasks with multiple options, such as multi-choice question answering and news classification. We fill the options into the sentences and the ones with correct options are considered positive samples. The second type is tasks with yes/no options, which can be formulated as a binary discrimination problem itself. For example, natural language inference aims to predict whether a premise entails a hypothesis. In this case, we use a prompt to concatenate the premise $A$ and the hypothesis $B$ into a sentence ``Premise: $A$. Hypothesis: $B$.'' If entailment holds, this sample is treated as positive in-distribution samples and otherwise negative out-of-distribution ones.



% We define the true data distribution using multiple training tasks with labeled data. Specifically, since discriminative tasks can be formulated as selecting from a few options, samples with correct options form an empirical data distribution, while samples with incorrect options are considered out of distribution. In other words, our discriminator is trained to predict ``true'' for samples with correct options and ``false'' for incorrect ones. We use simple concatenation to minimize prompting efforts. For example, given an example (premise, hypothesis), a natural language inference task predicts whether the premise entails the hypothesis. We concatenate the premise and hypothesis, and assign the label ``true'' for entailment and ``false'' for non-entailment.


% First off, since many of the NLP tasks can be formulated as selecting from several options, we first reformulate the task data into natural text samples by concatenating different fields \zy{what are fields? undefined here. try using another word.}.
% For example, given an example of \zy{the} natural language inference task (\textit{Premise}, \textit{Hypothesis}, \textit{Label}), the natural text is reformulated as ``\textit{\{Premise\} || \{Hypothesis\}}'' labeled with \textit{\{Label\}}. \footnote{Here we use ``||'' to represents direct concatenation.} 
% Another example of topic classification task (\textit{Text}, \textit{Label}) where the \textit{Label} indicates the first option of \{Sports, Fashion, Politics\}, the corresponding natural texts are formulated as ``\textit{Text} || Sports'' labeled with 1, ``\textit{Text} || Fashion'' and ``\textit{Text} || Politics'' both labeled with 0.
% Secondly, we pretrain a pretrained model with reformulated multitask datasets to distinguish whether the text sample comes from the true data distribution. ~\footnote{An assumption is that negative-labeled text samples are artificially constructed thus do not come from the true data distribution, and vice versa.}

For the performance of zero-shot generalization, our approach achieves new state-of-the-art on the T0 benchmark, outperforming T0 by 16.0\%, 7.8\%, and 11.5\% respectively on different scales. 
UD also achieves state-of-the-art performance on a wide range of supervised NLP tasks, using only 1/4 parameters of previous methods.
Compared with the previous generative prompt-based methods, our universal discriminator requires minimal prompting, which is simple, robust, and applicable in real-world scenarios.

% By further scaling the number of tasks, our approach also sets the new state-of-the-art on \textbf{\color{red}[xxx]} tasks with less than 10\% of model parameters \zy{need to give a range} under the setting of standard finetuning.
% In the setting of finetuning, our approach also outperforms the generative baselines consistently across a wide range of tasks.


In addition, we also generalize UD to a larger scope of tasks, such that UD can perform discriminative and generative tasks at the same time. Specifically, we extend UD to the encoder-decoder architecture for training on generative tasks, and restrict the model's prediction on "yes"/"no" tokens for jointly training discriminative tasks. Results prove that generalized UD maintains UD's advantages on discriminative tasks and achieves comparable results on generative tasks (See \S~\ref{sec:generalizedud}). 
% We leave expanding UD to a broader range of generative tasks and achieve greater performance on generative tasks as our future work


% \xhk{I admit the limitation on generative tasks here as our future work.}

%\xhk{Although UD is designed for improving zero-shot performance for discriminative tasks, we can also combine this idea to train a generalized UD model which simultaneously solves both discriminative tasks and generative tasks, maintaining UD's advantage on discriminative tasks and get comparable results on generative tasks (See \S~\ref{sec:generalizedud}).}

% The universal discriminator provides a new perspective for zero-shot generalization---Compared with generating the true verbalizer that indicates task label with extensive prompt engineering, distinguishing between options with minimal prompting efforts is simple, robust, and high-performing, thus is more applicable in real-world scenarios. \zy{rewirte the above sentence, just focus on one point---minimal prompting}
