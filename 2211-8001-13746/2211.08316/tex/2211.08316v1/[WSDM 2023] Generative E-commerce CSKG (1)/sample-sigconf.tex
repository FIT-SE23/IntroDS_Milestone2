%%
%% This is file `sample-sigconf.tex',
%% generated with the docstrip utility.
%%
%% The original source files were:
%%
%% samples.dtx  (with options: `sigconf')
%% 
%% IMPORTANT NOTICE:
%% 
%% For the copyright see the source file.
%% 
%% Any modified versions of this file must be renamed
%% with new filenames distinct from sample-sigconf.tex.
%% 
%% For distribution of the original source see the terms
%% for copying and modification in the file samples.dtx.
%% 
%% This generated file may be distributed as long as the
%% original source files, as listed above, are part of the
%% same distribution. (The sources need not necessarily be
%% in the same archive or directory.)
%%
%% Commands for TeXCount
%TC:macro \cite [option:text,text]
%TC:macro \citep [option:text,text]
%TC:macro \citet [option:text,text]
%TC:envir table 0 1
%TC:envir table* 0 1
%TC:envir tabular [ignore] word
%TC:envir displaymath 0 word
%TC:envir math 0 word
%TC:envir comment 0 0
%%
%%
%% The first command in your LaTeX source must be the \documentclass command.
\documentclass[sigconf]{acmart}
%\documentclass[sigconf, anonymous, review, screen]{acmart}


\usepackage{booktabs}
\usepackage{diagbox}
\usepackage{xcolor,colortbl}
\usepackage{multirow}
\usepackage{graphics}
\usepackage{color}
\usepackage{amsmath,bm}
\usepackage{enumitem}

\newcommand{\zheng}[1]{\textcolor{teal}{\textbf{[ZHENG: #1]}}}
\newcommand{\caoty}[1]{\textcolor{baby blue}{\textbf{[Tianyu: #1]}}}
\newcommand{\yifan}[1]{\textcolor{teal}{\textbf{[Yifan: #1]}}}
\newcommand{\cl}[1]{\textcolor{teal}{\textbf{[Changlong: #1]}}}


\setcopyright{none}
\settopmatter{printacmref=false} % Removes citation information below abstract
\renewcommand\footnotetextcopyrightpermission[1]{} % removes footnote with conference information in first column
\pagestyle{plain}

\definecolor{mygreen}{RGB}{89, 206, 143}
\definecolor{myred}{RGB}{230, 72, 72}


\newcommand{\xin}[1]{\textcolor{teal}{#1}}  % 
\newcommand{\jiaxin}[1]{\textcolor{violet}{#1}}
\newcommand{\jiaxinc}[1]{\textcolor{violet}{\textbf{[JIAXIN: #1]}}}

%\documentclass[manuscript,screen]{acmart}
%% NOTE that a single column version may be required for 
%% submission and peer review. This can be done by changing
%% the \doucmentclass[...]{acmart} in this template to 
%% \documentclass[manuscript,screen]{acmart}
%% 
%% To ensure 100% compatibility, please check the white list of
%% approved LaTeX packages to be used with the Master Article Template at
%% https://www.acm.org/publications/taps/whitelist-of-latex-packages 
%% before creating your document. The white list page provides 
%% information on how to submit additional LaTeX packages for 
%% review and adoption.
%% Fonts used in the template cannot be substituted; margin 
%% adjustments are not allowed.
%%
%%
%% \BibTeX command to typeset BibTeX logo in the docs
\AtBeginDocument{%
  \providecommand\BibTeX{{%
    \normalfont B\kern-0.5em{\scshape i\kern-0.25em b}\kern-0.8em\TeX}}}

%% Rights management information.  This information is sent to you
%% when you complete the rights form.  These commands have SAMPLE
%% values in them; it is your responsibility as an author to replace
%% the commands and values with those provided to you when you
%% complete the rights form.
\setcopyright{acmcopyright}
\copyrightyear{2018}
\acmYear{2018}
\acmDOI{XXXXXXX.XXXXXXX}

%% These commands are for a PROCEEDINGS abstract or paper.
\acmConference[Conference acronym 'XX]{Make sure to enter the correct
  conference title from your rights confirmation emai}{June 03--05,
  2018}{Woodstock, NY}
%
%  Uncomment \acmBooktitle if th title of the proceedings is different
%  from ``Proceedings of ...''!
%
%\acmBooktitle{Woodstock '18: ACM Symposium on Neural Gaze Detection,
%  June 03--05, 2018, Woodstock, NY} 
\acmPrice{15.00}
\acmISBN{978-1-4503-XXXX-X/18/06}


%%
%% Submission ID.
%% Use this when submitting an article to a sponsored event. You'll
%% receive a unique submission ID from the organizers
%% of the event, and this ID should be used as the parameter to this command.
%%\acmSubmissionID{123-A56-BU3}

%%
%% For managing citations, it is recommended to use bibliography
%% files in BibTeX format.
%%
%% You can then either use BibTeX with the ACM-Reference-Format style,
%% or BibLaTeX with the acmnumeric or acmauthoryear sytles, that include
%% support for advanced citation of software artefact from the
%% biblatex-software package, also separately available on CTAN.
%%
%% Look at the sample-*-biblatex.tex files for templates showcasing
%% the biblatex styles.
%%

%%
%% The majority of ACM publications use numbered citations and
%% references.  The command \citestyle{authoryear} switches to the
%% "author year" style.
%%
%% If you are preparing content for an event
%% sponsored by ACM SIGGRAPH, you must use the "author year" style of
%% citations and references.
%% Uncommenting
%% the next command will enable that style.
%%\citestyle{acmauthoryear}

%%
%% end of the preamble, start of the body of the document source.
\begin{document}

%%
%% The "title" command has an optional parameter,
%% allowing the author to define a "short title" to be used in page headers.
\title[FolkScope]{FolkScope: Intention Knowledge Graph Construction for \\ Discovering E-commerce  Commonsense
}

%%
%% The "author" command and its associated commands are used to define
%% the authors and their affiliations.
%% Of note is the shared affiliation of the first two authors, and the
%% "authornote" and "authornotemark" commands
%% used to denote shared contribution to the research.


\author{Changlong Yu$^{1}$, Weiqi Wang$^{1}$, Xin Liu$^{1}$, Jiaxin Bai$^{1}$, Yangqiu Song$^{1}$\\
Zheng Li$^{2}$, Yifan Gao$^{2}$, Tianyu Cao$^{2}$, Bing Yin$^{2}$}
\affiliation{%
  \institution{$^{1}$The Hong Kong University of Science and Technology, Hong Kong SAR, China\\
  $^{2}$Amazon.com Inc, Palo Alto, USA
  }
  \city{\{cyuaq, wwangbw, xliucr, jbai, yqsong\}@cse.ust.hk}
\country{ \{amzzhe, yifangao, caoty, alexbyin\}@amazon.com}
}
% \email{}


% \author{Zheng Li, Yifan Gao, Tianyu Cao, Bing Yin}
% \email{{amzzhe, yifangao, caoty, alexbyin}@amazon.com}
% \affiliation{%
%   \institution{Amazon}
%   \city{Palo Alto}
%   \state{CA}
%   \country{USA}
% }

% \author{Changlong Yu}
% \email{cyuaq@cse.ust.hk}
% \affiliation{%
%   \institution{The Hong Kong University of Science and Technology}
%   \city{Hong Kong SAR}
%   \country{China}
% }

% \author{Weiqi Wang}
% \email{wwangbw@cse.ust.hk}
% \affiliation{%
%   \institution{The Hong Kong University of Science and Technology}
%   \city{Hong Kong SAR}
%   \country{China}
% }

% \author{Xin Liu}
% \email{xliucr@cse.ust.hk}
% \affiliation{%
%   \institution{The Hong Kong University of Science and Technology}
%   \city{Hong Kong SAR}
%   \country{China}
% }

% \author{Jiaxin Bai}
% \email{jbai@cse.ust.hk}
% \affiliation{%
%   \institution{The Hong Kong University of Science and Technology}
%   \city{Hong Kong SAR}
%   \country{China}
% }

% \author{Yangqiu Song}
% \email{yqsong@cse.ust.hk}
% \affiliation{%
%   \institution{The Hong Kong University of Science and Technology}
%   \city{Hong Kong SAR}
%   \country{China}
% }

% \author{Zheng Li}
% \email{zlict@connect.ust.hk}
% \affiliation{%
%   \institution{Amazon}
%   \city{Palo Alto}
%   \state{CA}
%   \country{USA}
% }

% \author{Yifan Gao}
% \email{yifangao@amazon.com}
% \affiliation{%
%   \institution{Amazon}
%   \city{Palo Alto}
%   \state{CA}
%   \country{USA}
% }

% \author{Tianyu Cao}
% \email{caoty@amazon.com}
% \affiliation{%
%   \institution{Amazon}
%   \city{Palo Alto}
%   \state{CA}
%   \country{USA}
% }

% \author{Bing Yin}
% \email{alexbyin@amazon.com}
% \affiliation{%
%   \institution{Amazon}
%   \city{Palo Alto}
%   \state{CA}
%   \country{USA}
% }

% %%
%% By default, the full list of authors will be used in the page
%% headers. Often, this list is too long, and will overlap
%% other information printed in the page headers. This command allows
%% the author to define a more concise list
%% of authors' names for this purpose.
\renewcommand{\shortauthors}{Yu, et al.}

%%
%% The abstract is a short summary of the work to be presented in the
%% article.
\begin{abstract}
As stated by Oren Etzioni, ``commonsense is the dark matter of artificial intelligence.''
In e-commerce, understanding usersâ€™ needs or intentions requires substantial commonsense knowledge, e.g., ``A user bought an iPhone and a compatible case because the user wanted the phone to be protected.'' 
In this paper, we present \texttt{FolkScope}, an intention knowledge graph construction framework, to reveal the structure of humans' minds about purchasing items on e-commerce platforms such as Amazon. 
As commonsense knowledge is usually ineffable and not expressed explicitly, it is challenging to perform any kind of information extraction.
Thus, we propose a new approach that leverages the generation power of large language models and human-in-the-loop annotation to semi-automatically construct the knowledge graph.
We annotate a large amount of assertions for both plausibility and typicality of an intention that can explain a purchasing or co-purchasing behavior, where the intention can be an open reason or a predicate falling into one of 18 categories aligning with ConceptNet, e.g., {\it IsA}, {\it MadeOf}, {\it UsedFor}, etc.
Then we populate the annotated information to all automatically generated ones, and further structurize the assertions using pattern mining and conceptualization to form more condensed and abstractive knowledge.
We evaluate our knowledge graph using both intrinsic quality measures and a downstream application, i.e., recommendation.
The comprehensive study shows that our knowledge graph can well model e-commerce commonsense knowledge and can have many potential applications.


\end{abstract}

% %%
% %% The code below is generated by the tool at http://dl.acm.org/ccs.cfm.
% %% Please copy and paste the code instead of the example below.
% %%
% \begin{CCSXML}
% <ccs2012>
%  <concept>
%   <concept_id>10010520.10010553.10010562</concept_id>
%   <concept_desc>Computer systems organization~Embedded systems</concept_desc>
%   <concept_significance>500</concept_significance>
%  </concept>
%  <concept>
%   <concept_id>10010520.10010575.10010755</concept_id>
%   <concept_desc>Computer systems organization~Redundancy</concept_desc>
%   <concept_significance>300</concept_significance>
%  </concept>
%  <concept>
%   <concept_id>10010520.10010553.10010554</concept_id>
%   <concept_desc>Computer systems organization</concept_desc>
%   <concept_significance>100</concept_significance>
%  </concept>
%  <concept>
%   <concept_id>10003033.10003083.10003095</concept_id>
%   <concept_desc>Networks~Network reliability</concept_desc>
%   <concept_significance>100</concept_significance>
%  </concept>
% </ccs2012>
% \end{CCSXML}

% \ccsdesc[500]{Computer systems organization~Embedded systems}
% \ccsdesc[300]{Computer systems organization~Redundancy}
% \ccsdesc{Computer systems organization~Robotics}
% \ccsdesc[100]{Networks~Network reliability}

% %%
% %% Keywords. The author(s) should pick words that accurately describe
% %% the work being presented. Separate the keywords with commas.
% \keywords{Knowledge graph construction, language models, commonsense knowledge, e-commerce applications}

%% A "teaser" image appears between the author and affiliation
%% information and the body of the document, and typically spans the
%% page.

%\begin{teaserfigure}
%  \includegraphics[width=\textwidth]{sampleteaser}
%  \caption{Seattle Mariners at Spring Training, 2010.}
%  \Description{Enjoying the baseball game from %the third-base
%  seats. Ichiro Suzuki preparing to bat.}
%  \label{fig:teaser}
%\end{teaserfigure}

%%
%% This command processes the author and affiliation and title
%% information and builds the first part of the formatted document.
\maketitle
%\vspace{-0.1in}
\section{Introduction}
In e-commerce or online shopping platforms, understanding users' searching or purchasing intentions can benefit and enable a lot of recommendation tasks~\cite{dai2006detecting,zhang2016mining,hao2022dy}.
Existing intention-based studies on recommendation systems are either of limited numbers of intention categories~\cite{dai2006detecting,zhang2016mining} or using models to implicitly model the intention memberships~\cite{hao2022dy}.
In general large online e-commerce platforms, such as Amazon, acquiring or capturing users' purchasing intentions in a scalable way is very challenging.
Intentions are mental states where agents or humans commit themselves to actions.
Understanding others' behaviors and mental states requires rationalizing intentional actions~\cite{sep-folkpsych-theory}, where we need commonsense, or, in other words, good judgements~\cite{liu2004conceptnet}. 
For example, ``in a birthday party, we usually need a birthday cake.''
%\zheng{Better to give some concrete examples about those two characteristics.}
Commonsense knowledge can be {\it factoid}~\cite{DBLP:conf/aaai/GordonDS10} which is not invariably true.
Meanwhile, commonsense knowledge is usually ineffable and not expressed explicitly.
Thus, it is very challenging to acquire such kind of knowledge in existing text data.
%In cognitive science, folk psychology (or commonsense psychology) denotes the way of humans' .
Thus, as an AI pioneer, Oren Etzioni, stated, ``commonsense is the dark matter of artificial intelligence.''
We still lack of technologies to explicitly discover the structure of our mind to rationalize our behaviors.
Particularly, for users' purchasing behaviors, there is still no scalable way to know such commonsense like ``a user bought an iPhone and a compatible case because the user wanted the phone to be protected.'' 



Existing related knowledge graphs (KGs) can be categorized into two folds.
First, there are some general situational commonsense KGs dealing with everyday social situations~\cite{DBLP:conf/acl/SmithCSRA18,sap2019atomic,zhang2020aser}.
Such KGs focus on action consequences and cause/effect relations among events and states. However, they are not directly related to products on e-commerce platforms, so even the neuralized models, e.g., COMET~\cite{bosselut-etal-2019-comet}, trained on such KGs cannot generalize well on users' behaviors data. %\zheng{inconsistent tense: leverages, integrated}
Another line of work leverages existing KGs, such as ConceptNet~\cite{liu2004conceptnet,speer2017conceptnet} and Freebase~\cite{DBLP:conf/sigmod/BollackerEPST08}, to integrate them into the e-commerce catalog data~\cite{luo2020alicoco,zalmout2021all,zhang2021alicg}. 
However, such an integration is still based on factual knowledge such as {\it IsA} and {\it DirectorOf} relations, and does not truly model the commonsense knowledge for purchase intentions.
Recently, AliCoCo2~\cite{luo2021alicoco2} attempted to tackle the commonsense extraction problem from question answering~(QA) pairs, product descriptions, and reviews. 
They leveraged the SQuAD-style QA models~\cite{rajpurkar-etal-2016-squad} to answer questions such as
%``what kind of category is used for the group of terms?'' and 
``what kind of category has the functionality of terms?'' where terms are some informal commonsense concepts.
In this way, they are able to build a large scale KG about the factual knowledge more than product related concepts, e.g., space, crowd, time, function, and event, as indicated in ConceptNet~\cite{liu2004conceptnet}.


\begin{figure*}[t]
    \centering
     \includegraphics[width=0.85\linewidth]{figure/folkscope_singlebuy.pdf}
     \caption{An overview of \texttt{FolkScope}. It starts from users' purchasing or co-purchasing behaviors and links them to intentions. Then the more abstract intentions are formed to condense the representation of intentions. The intentions can be represented by both noun phrases and verb phrases (italic face).}
     \label{fig:folkscope}
\end{figure*}

However, existing KGs on e-commerce data can only indicate the {\it plausibility} of factual relations, and cannot fully reveal the intention of purchases. 
In fact, an intention acts as a mediator between action and what people believe and desire~\cite{kashima1998category}.
This can be reflected by the {\it typicality} of commonsense.
For example, a user bought an iPhone 13 because ``iPhone 13 has the function of taking photos'' and ``iPhone 13 can be used for social networking,'' where the reasons can be plausible functions, whereas a more typical reason would be ``the user's previous phone is also an iPhone so it's easy to transfer data,''  ``the users' old phone is too slow,'' or ``the user is simply a fan of Apple products.''
Thus, no matter what kind of factual knowledge a KG contains, if it is not directly linked to the rationalization, it cannot be regarded as typical commonsense.
Furthermore, recommendation explanation has been proposed as a task to explain why a user rates high or low of an item~\cite{ni-etal-2019-justifying,DBLP:conf/cikm/LiZC20}, where they use online reviews as the natural annotation of explanation. 
However, online reviews are noisy and diverse.
Most of the time, reviews do not directly reflect the intention of the purchases, but rather the consequences of purchases or reasons of the ratings.
Similarly, none of the existing QA pairs, reviews, or product descriptions can support the extraction of the intentional commonsense knowledge, as they do not explicitly mention the intentions of purchases.
%In addition, commonsense knowledge is usually ineffable and not expressed explicitly.
Thus, it is challenging to perform any kind of information extraction to construct an intention KG for e-commerce.


% heterogeneous data source

% how to mine implicit knowledge. 

% capture implicit or associate/analogy knowledge 

% high quality: free text; quality issue.  

In this paper, we propose a new framework, \texttt{FolkScope}, to acquire intention knowledge for e-commerce purchasing behaviors. 
Instead of performing information extraction, we leverage the generation power of large scale language models, e.g., GPT~\citep{radford2019language,brown2020language}, to generate possible intentions of the purchasing behaviors as candidates.
Large language models have shown the capability of memorizing factual and commonsense knowledge~\cite{petroni2019language,west-etal-2022-symbolic}.
Thus, with a limited number of annotations, it is possible to probe a large amount of related knowledge from them.
Moreover, instead of only probing one item, we propose to probe the co-purchasing behaviors, so they can be used to compare the typicality across different items.
For example, we can design a prompt ``A user bought `item 1' and `item 2' because [GEN]'' where [GEN] is a special token indicating generation.
Then by comparing different item 2's, we can identify what typical reasons are for an additional item being bought together.
Furthermore, as open prompts in the above example can be arbitrary and loosely constrained, we also align our prompts with 18 ConceptNet relations, such as {\it IsA}, {\it HasPropertyOf}, {\it CapableOf}, {\it UsedFor}, etc.
For example, we have a prompt ``A user bought `item 1' and `item 2' because they are both capable of [GEN].''
In this sense, we can probe intentions in different perspectives mapped to the commonsense knowledge in ConceptNet.

As the generated knowledge by language models can also be noisy and may not be able to reflect human's rationalization of a purchasing action, we also perform large-scale human annotation.
We divide the annotation process into two steps.
In the first step, we annotate the plausibility of the generated intentions for purchasing actions. 
In this step, the annotators are asked to evaluate the intentions based on facts about the products' properties, usages, functions, etc.
Then in the second step, we ask the annotators to annotate the typicality of the intentions, where a typical intention should reflect both informativeness and causality.
To be informative, the intention should contain the key information about the shopping context rather than general reasons.
To be causal, the intention should capture the typical reason for a user, showing the significant attributes or features that largely affect the users' decision.
Then after the second step annotation, typicality can be compared across different purchasing behaviors.
Compared with crowd-sourcing intentions written by annotators, our human-AI collaboration not only significantly reduces labeling cost, but also avoids repetitive patterns or potential artifacts from human writers~\cite{Liu2022WANLIWA}, which thus improves the diversity of knowledge. 

Given generated candidates and annotations, to construct the KG, we first perform pattern mining to remove irregular generations.
Then for both annotation steps, we train classifiers to populate the scores to all generated data.
Finally, for each of the generated intentions, we perform conceptualization to map the key entities or concepts in the intention to more high-level concepts so that we can build a denser and more abstractive KG for future generalization.
An illustration of our KG is shown in Figure~\ref{fig:folkscope}.
To verify the overall quality, we randomly sample the populated assertions to estimate the overall quality of the KG.
We also use a downstream task, collaborative filtering based recommendation, to prove our KG's quality and usefulness.
The contributions of our work can be summarized as follows.

\noindent$\bullet$ We propose a new framework, \texttt{FolkScope}, to construct large-scale intention KG for discovering e-commerce commonsense knowledge.

\noindent$\bullet$ We leverage large-scale language models to generate candidates and perform two-step large-scale annotation on Amazon data with two domains, clothing and electronics, and the process can be well generalized to other domains. 

\noindent$\bullet$ We define the schema of the intention KG aligning with famous commonsense KG, ConceptNet, and populate a large KG based on our generation and annotation with 184,146 items, 217,108 intentions, 857,972
abstract intentions, and 12,755,525 edges (assertions).

\noindent$\bullet$ We perform a comprehensive study to verify the validity and usefulness of our KG. Both code and data will be public released for downstream tasks.




\section{Related Work}

\noindent \textbf{Knowledge Graph Construction.}
%KG construction has been a well established task for factual knowledge, which can be done by human annotation, such as Freebase~\cite{DBLP:conf/sigmod/BollackerEPST08}, rule-based extraction from semi-structured data, such as YAGO~\cite{DBLP:conf/www/SuchanekKW07}, or information extraction from text corpus, such as KnowItAll~\cite{knowitall} and NELL~\cite{DBLP:conf/aaai/CarlsonBKSHM10}.
%\yifan{The first sentence is somewhat misaligned with the following reviews on commonsense KG and product KG. Perhaps we can introduce KG construction according to domain? e.g. fact-based, commonsense, product, and so on.}
Here we briefly review general commonsense KGs and product related KGs.
An early approach of commonsense KG construction is proposed in ConceptNet~\cite{liu2004conceptnet} where both text mining and crowdsourcing are leveraged. 
Later, ConceptNet is extended by integrating a lot of factual knowledge from other resources~\cite{speer2017conceptnet}.
In 2012, a web-scale KG, Probase, which focuses {\it IsA} relations, is constructed based on pattern mining~\cite{wu2012probase}.
The advantage of this probabilistic KG is that it can model both plausibility and typicality of conceptualizations~\cite{song2011concept}.
Recently, commonsense has attracted more attention in the field of AI and NLP.
Particularly, Event2Mind~\cite{DBLP:conf/acl/SmithCSRA18} and ATOMIC~\cite{sap2019atomic}, which are based on human annotations, are developed to model the situational knowledge about action consequences and cause/effect relations of events and states.
Then their extensions and neuralized models are developed~\cite{bosselut-etal-2019-comet,hwang2021comet}.
Meanwhile, in KnowllyWood~\cite{TandonMDW15KnowlyWood}, WebChild~\cite{TandonMW17WebChild2}, and ASER~\cite{zhang2020aser,DBLP:journals/ai/ZhangLPKOFS22}, it is proven that information extraction can be used to extract event-related knowledge from dependency parsing and discourse analysis based on large-scale corpora.
The extracted knowledge can then be transferred to other human annotated knowledge resources, e.g., ConceptNet~\cite{DBLP:conf/ijcai/ZhangKSR20} and ATOMIC~\cite{DBLP:conf/www/FangZWSH21}.

In the e-commerce domain, Amazon Product Graph~\cite{zalmout2021all} is developed to align Amazon catalog data with external KGs such as Freebase and to automatically extract thousands of attributes in millions of product types~\cite{DBLP:conf/acl/KaramanolakisMD20,DBLP:conf/kdd/DongHKLLMXZZSDM20,DBLP:conf/www/ZhangZLDSF022}.
Alibaba also develops a series of KGs including AliCG~\cite{zhang2021alicg}, AliCoCo~\cite{luo2020alicoco} and AliCoCo2~\cite{luo2021alicoco2}, where the former two focus on concepts and their {\it IsA} relations while the latter one incorporates richer factual relations among concepts.
As we have stated in the introduction, there is still a gap between collecting factual knowledge about products and modeling users' purchasing intentions.
Thus, \texttt{FolkScope} is different from the existing product-related knowledge graphs to explicitly and directly model users' intentions.
%\yifan{Maybe elaborate a few sentences on Alibaba KGs? Also, add one sentence to compare our work with the above two (commonsense \& product) domains?}

\noindent \textbf{Language Models as Knowledge Bases.} Large language models, such as BERT~\citep{kenton2019bert}, GPT~\citep{radford2019language,brown2020language}, and T5~\cite{raffel2020exploring}, have shown great representation power to revolutionize many downstream applications.
It has been shown that language models can memorize factual knowledge, and one can design appropriate prompts to probe knowledge from them, which is usually referred to as ``language models as knowledge bases''~\cite{petroni2019language,alkhamissi2022review}.
Prompts can be either designed by humans, mined from corpus, or tuned from continuous embeddings to improve the knowledge probing tasks~\cite{liu2021pre}.
Language models as knowledge bases is a promising idea.
However, in practice, we still need symbolic knowledge representation to rapidly support knowledge editing and complicated reasoning. 
For online recommendations, we sometimes need explicit reasoning to explain the recommendation results.
It has been shown that one can derive factual KGs at scale based on language models with proper prompts~\cite{DBLP:journals/corr/abs-2010-11967,hao2022bertnet}. %\yifan{This sentence is repeated with the GPT3 \& ATOMIC sentence and unnecessary? }.
For commonsense knowledge, it has been shown that distilling from GPT-3 based on in-context learning with a few examples can generate even human-level commonsense understanding in the form of ATOMIC~\citep{west-etal-2022-symbolic}.
None of the above KGs are product-related nor purchasing intention related.
Thus, our contribution is unique to the community.






\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{figure/framework.pdf}
%    \vspace{-0.1in}
    \caption{The overall framework of \texttt{FolkScope}. It includes the generation, extraction, and conceptualization to semi-automatically construct the e-commerce intention commonsense knowledge graph with the help of human-in-the-loop annotations and evaluation.}
    \label{fig:framework}
\end{figure}


\section{Knowledge Graph Construction}
In this section, we introduce the knowledge graph construction method using our \texttt{FolkScope} framework.

\subsection{\texttt{FolkScope} Framework}

We call our framework \texttt{FolkScope} as we are the first attempt to reveal the structure of e-commerce intentional commonsense to rationalize the purchasing behaviors.
As shown in Figure~\ref{fig:framework}, \texttt{FolkScope} is a human-in-the-loop approach to semi-automatic construction of the KG.
We first leverage the pretrained language models to generate candidate assertions of intentions for purchasing or co-purchasing behaviors based on co-buy data from the Amazon released benchmark dataset.
Then we employ two-step annotations to annotate the plausibility and typicality of the generated intentions, where the corresponding definitions of the scores are as follows.




\noindent$\bullet$ {\it Plausibility} is defined as how possible the assertion is valid regarding their properties, usages, functions, etc.
    
\noindent$\bullet$ {\it Typicality} is defined as how well the assertion reflects a specific feature that causes the user behavior.
Typical intentional assertions should satisfy the following criteria. 
{Informativeness:} The assertion contains key information about the shopping context rather than a general one, e.g., ``they are used for Halloween parties .'' v.s. ``they are used for the same purpose.'' 
{Causality: }The assertion captures the typical intention of user behaviors, e.g., ``they have a property of water resistance.'' Some specific attributes or features might largely affect the users' purchase decisions. 

\noindent Typicality is different from plausibility as it indicates more typical reasons for purchasing behavior. For example, as shown in Figure~\ref{fig:folkscope}, ``telling the time'' is a typical reason for buying a normal watch than buying an ``iWatch'' whereas a more typical reason may be ``fan of Apple products'' for the latter one.


After the annotation, we design classifiers to populate the scores to all generated candidates. 
Then the high-quality ones will be further structured by using pattern mining on their dependency parses to aggregate similar assertions. 
Then, we also perform conceptualization~\cite{DBLP:journals/ai/ZhangLPKOFS22} to further aggregate assertions to form more abstract intentions.
Examples are also shown in Figure~\ref{fig:folkscope}.


\subsection{User Behavior Data Sampling}

We extract the users' behavior datasets from open-sourced Amazon Review Data~(2018)\footnote{\url{https://nijianmo.github.io/amazon/}}~\citep{ni-etal-2019-justifying} with 15.5M items from Amazon.com. 
In our work, we mainly consider \textit{co-buy} pairs, which might indicate stronger shopping intent signals than \textit{co-view} pairs. After the pre-processing and removing duplicated items, the resulting co-buy graph covers 3.5M nodes and 31.4M edges. The items are organized into 25 top-level categories from the Amazon website, and among them, we choose two frequent categories: ``\textit{Clothing, Shoes \& Jewelry}" and ``\textit{Electronics}" to sample \textit{co-buy} pairs because
those items substantially appear in situations requiring commonsense knowledge to understand while other categories such as ``Movie'' or ``Music'' are more relevant to factual knowledge between entities. 
We uniformly sample \textit{co-buy} pairs from the two categories, and the statistics are shown in Table~\ref{tab:cobuy_stat}.


\begin{table}[t]
\small
\caption{Statistics of sampled co-buy pairs and generated candidate assertions. Note that the prompts in the generation are not included in the calculations of assertion lengths.}\label{tab:cobuy_stat}
\begin{tabular}{l|ccc}
\toprule
  &  Clothing  &  Electronics & Total\\
\midrule
\# Item Pairs & 199,560 & 93,889 & 293,449 \\
\# Unique Items & 151,509 & 64,244 & 211,349 \\
\# Generated Assertions & 11,358,637 & 5,282,273 & 16,640,910 \\
\# Unique Assertions & 2,865,118 & 1,280,259 & 4,063,764 \\
Avg. \# Tokens of Assertions & 6.66 & 5.25 & 6.21 \\
\bottomrule
\end{tabular}
\end{table}


\subsection{Knowledge Generation}
\label{sec:know_gen}
%\jiaxinc{I think the title can be substituted to some more general term, instead of prompt. For example, knowledge guided generation, conditioned generation. Because I think scope of prompting is principally too narrow. }
As shown in Table~\ref{tab:prompts}, we verbalize the prompt templates using the titles of co-buy pairs.
Besides the general prompt (i.e., ``open''), we also align our prompts with 18 relations in ConceptNet that are highly related to commonsense.
For example, for the relation {\it HasA}, we can design a prompt ``A user bought `item 1' and `item 2' because they both have [GEN]'' where [GEN] is a special token indicating generation.
Since the long item titles might contain noise besides useful attributes, we use heuristic rules to filter out items whose titles potentially affect the conditional generation like repeated words. 
We use the OPT model~\citep{zhang2022opt} of 30B parameters\footnote{\url{https://huggingface.co/facebook/opt-30b}} with two NVIDIA A100 GPUs using the HuggingFace library~\citep{wolf-etal-2020-transformers} to generate assertion candidates.\footnote{As we will further annotate the plausibility and typicality of candidates, larger models will reduce the annotation cost. However, the generation is also constrained by the API cost or the computational cost. Thus, we choose the best model we can use.}
% Though the generations from GPT3 yield better performance in our pilot study, the cost of constructing large-scale KGs with millions of assertions is still high. The typicality of generated corpus can be further improved by the two following classification models in our framework. 
For each relation of the co-buy pairs, we set the max generation length as 100 and generate 3 assertions using nucleus sampling~(p = 0.9)~\citep{holtzman2019curious}. 
We post-process the candidates as follows. (1) We discard the generations without one complete sentence. (2) We use the sentence segmenter from Spacy library\footnote{\url{https://spacy.io/}} to extract the first sentence for longer generations. 
After removing duplicates, we obtain 16.64M candidate assertions for 293K item pairs and 4.06M unique tails among them.
The statistics of the two categories are listed in Table~\ref{tab:cobuy_stat}.



\begin{table}[t]\small
\centering
\caption{Prompts for different commonsense relations. %\xin{(Xin: re-order the relations by surface forms; why HasA is the prefix of HasProperty)}
}\label{tab:prompts}
\setlength\tabcolsep{4pt}
\begin{tabular}{l|c|c}
\toprule
Type  & Relation &  Prompt       \\ \midrule 
Open &  / & /  \\ \midrule
\multirow{10}{*}{\begin{minipage}{0.4in}Item\end{minipage}}
& \textit{HasA}  & they both have \\
& \textit{HasProperty}  &  they both have a property of     \\
& \textit{RelatedTo}  & they both are related to      \\
& \textit{SimilarTo}  & they both are similar to      \\
& \textit{PartOf}  & they both are a part of    \\
& \textit{IsA}  & they both are a type of      \\
& \textit{MadeOf}  & they both are made of      \\
& \textit{CreatedBy}  & they are created by  \\
& \textit{DistinctFrom}  & they are distinct from  \\
& \textit{DerivedFrom}  & they are derived from  \\
\midrule
\multirow{5}{*}{\begin{minipage}{0.4in}Function\end{minipage}}
& \textit{UsedFor}  &  they are both used for    \\
& \textit{CapableOf}  &   they both are capable of    \\
& \textit{SymbolOf}  &   they both are symbols of  \\
& \textit{MannerOf}  &    they both are a manner of   \\
& \textit{DefinedAs}  &  they both are defined as \\
\midrule
\multirow{3}{*}{\begin{minipage}{0.4in}Human\end{minipage}}
& \textit{Result}  & as a result, the person   \\
& \textit{Cause}  &  the person wants to     \\
%& \textit{MotivatedBy}  &  buying them was motivated by    \\
& \textit{CauseDesire}  & the person wants his     \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Two-step Annotation and Population} 
\label{sec:two_round_annotations_and_populations}

As the generated candidates can be noisy or not rational, we apply the human annotation to obtain high-quality assertions and then populate the generated assertions. We use the Amazon Mechanical Turk~(MTurk) platform to annotate our data. Annotators are provided with a pair of co-buy items with each item's title, category, shopping URL, and three images from our sampled metadata. Assertions with different relations are presented in the natural language form by using the prompts presented in Table~\ref{tab:prompts}. To guarantee the quality of our annotation, we set up strict qualification rounds for both plausibility and typicality annotations. 
%We list detailed questionnaire design and annotator selection in the Appendix. 
Moreover, qualified annotators are provided timely feedback during the annotation. 




\subsubsection{Annotation}
In the first step, we annotate the plausibility.
This step will help us filter out many incorrect candidates.
As the second step needs more annotation efforts, using this step can help us save much annotation cost.
We randomly sample 66K generations and collect three plausibility judgements per generation. The final answer for plausibility is derived through majority voting. The overall IAA score is 75.48\% in terms of pairwise agreement proportion, while the Fleiss's Kappa~\cite{fleiss1971measuring} is 0.4872.


Different from the simple binary plausibility judgements, in the second step, we have more fine-grained and precise typicality indicators concerning \textit{informativeness} and \textit{causality}. Here we choose the candidates automatically labeled as plausible based on our classifier trained on the data in the first step. We ask the annotators to judge whether they are \textit{strongly acceptable} (+1), \textit{weakly acceptable} (0.5), {\it rejected} (0), or \textit{implausible} (-1) that the assertion is informative and casual for a purchasing behavior.  
Thus, if in this step implausible assertion is observed again, they will further decrease the average score. 
Considering the judgements might be subjective and biased with respect to different annotators, we collect five annotations for each assertion and take the average as the final typicality score.\footnote{The annotators in this step are chosen from the high-quality annotators in the first step. We tried other options, such as using seven or nine annotators per generation in our pilot study. The results do not show much improvement. }
%the typicality annotation adopt more conservative strategies to favor precision than recall. 
Similar to the first-step annotation, we collect around 60K assertions.
Empirically, we find that annotating more candidates does not bring significantly better filtering accuracy.
Detailed statistics are presented in Table~\ref{tab:labled_stat}. 


\begin{table}[t]\small
\centering
\caption{Statistics of annotated data.}\label{tab:labled_stat}
\setlength\tabcolsep{4pt}
    \begin{tabular}{l|c|cc}
    \toprule
     \multicolumn{1}{c|}{Stage}  & \multicolumn{1}{c|}{Category}  &  \multicolumn{1}{c}{\# Annotation}    &  \multicolumn{1}{c}{Avg. Score}    \\ \midrule 
    % \multirow{3}{*}{\begin{minipage}{0.7in}Plausibility \end{minipage}} & Clothing  &  44,337 & 0.6435 \\
    \multirow{3}{*}{Plausibility} & Clothing  &  44,337 & 0.6435 \\
    & Electronics  &  21,760 & 0.5467 \\
    & Total  & 66,097 & 0.6116 \\
    \midrule
    % \multirow{3}{*}{\begin{minipage}{0.7in}Typicality \end{minipage}} & Clothing & 38,279 & 0.4407 \\
    \multirow{3}{*}{Typicality} & Clothing & 38,279 & 0.4407 \\
    & Electronics & 22,995 & 0.4631 \\
    & Total & 61,274 & 0.4491 \\
    \bottomrule
    \end{tabular}
\end{table}




\subsubsection{Population} 

For plausibility population, we train our binary classifier based on the majority voting results in the first step, which produces binary labels.
For the typicality score, as we take the average of five annotators as the score, we empirically use scores greater than 0.8 to denote positive examples and less than 0.2 as negative examples.
We split the train/dev sets at the ratio of 80\%/20\% for both scores and train binary classifiers using both DeBERTa-large~\citep{he2021deberta} and RoBERTa-large~\citep{liu2019roberta}.
The best trained models are selected to maximize the F1 scores on the validation sets. 
The results are shown in Table~\ref{tab:labled_classification}. DeBERTa-large achieves better performance than RoBERTa-large on both plausibility and typicality evaluation. 
We populate the inference over the whole generated corpus in Table~\ref{tab:cobuy_stat} and only keep the assertions whose predicted plausibility scores are above 0.5~(discarding 32.5\% generations and reducing from 16.64M to 11.24M). 
Note that only plausible assertions are kept in the final knowledge graph.
Using different confidence cutting-off thresholds leads to trade-offs between the accuracy of generation and the size of the corpus. 
%Higher values result in conservative selections that favor precision over recall, whereas lower ones tend to recall more plausible assertions.
%We plotted four cutoff points in Figure~\ref{fig:prcurve} and conducted comprehensive evaluations of populated knowledge in Section~\ref{sec:human_eval}.
After the two-step populations, we obtain the plausibility score and typicality score for each assertion. Due to the measurement of different aspects of knowledge, we observe low correlations between the two types of scores~(Spearman correlation $\rho$: 0.319 for \textit{clothing} and 0.309 for \textit{electronics}).


\begin{table}[t]\small
\centering
\caption{Classification results on validation sets (F1).}\label{tab:labled_classification}
\setlength\tabcolsep{4pt}
    \begin{tabular}{l|ccc}
    \toprule
       & Plausibility  &  Typicality   \\ \midrule 
    RoBERTa-large & 83.22\% & 81.96\% \\
    DeBERTa-large & 85.12\% & 82.67\% \\
    \bottomrule
    \end{tabular}
\end{table}



\subsection{Knowledge Aggregation}
\label{sec:know_constct}


% \subsubsection{Knowledge Extraction}
% \label{sec:know_ext}

To acquire a knowledge graph with topology structures instead of sparse triplets, we aggregate possible similar assertions.
This is done by (1) pattern mining to align similar generated patterns and (2) conceptualization to produce more abstract knowledge.

% Specifically, the generated knowledge already has context-level preference since the pretrained language models learn the statistical word distribution.
% To further aggregate knowledge, we follow the idea of ASER~\cite{zhang2020aser} to model such knowledge as eventualities extracted by dependency patterns.

The assertions are usually expressed as free textual phrases.
Some of the phrases are similar that can be merged together by dropping some non-important words, e.g., ``they both are skiing products'' and ``they both are some skiing products.''
%, such as ``they both are a type of skiing product'' and ``the person wants to protect his iPhone.''
% The semantic meaning of an intention is determined by the words and also relative to the syntactic patterns.
% However, the prompts shown in Table~\ref{tab:prompts} are hard to cover by currently available tools.
% Therefore, we use the gSpan~\cite{DBLP:conf/icdm/YanH02} to mine frequent graph patterns over dependency trees parsed by Stanford CoreNLP v4.4.0.
% To meet the two requirements of the knowledge with high precision but non-trivial, we manually filter incomplete patterns out and evaluate patterns.
% Finally, we obtain 256 patterns that cover 80.77\% human-annotated knowledge.
Therefore, we apply the frequent graph substructure mining algorithm over dependency parse trees to discover the linguistic patterns.
We sample 90,000 candidates for each relation to analyze patterns and then parse each candidate into a dependency tree.
% And we choose the gSpan~\cite{DBLP:conf/icdm/YanH02} to mine frequent graph patterns over dependency trees parsed by Stanford CoreNLP v4.4.0\footnote{\url{https://stanfordnlp.github.io/CoreNLP/index.html}}.
In addition, the lemmatized tokens, pos-tags, and named entities are acquired for further use.
To reduce the time complexity of pattern mining, we mine high-frequency patterns for each relation.
To meet the two requirements of the knowledge with high precision but non-trivial, patterns are required to perfectly match more than 500 times.
One perfect match means that this pattern is the longest pattern, and no other candidate patterns can match.
Therefore, the pattern mining pipeline consists of three passes: (1) a graph pattern mining algorithm, Java implementation of gSpan~\cite{DBLP:conf/icdm/YanH02},\footnote{\url{https://github.com/timtadh/parsemis}} to mine all candidate patterns with the frequency more than 500, (2) a subgraph isomorphism algorithm, C++ implementation of VF2 algorithm in igraph,\footnote{\url{https://igraph.org/}} with a longest-first greedy strategy to check the perfect match frequency, and (3) human evaluation and revision.
% It is worth noting that the greedy match can speed up by the disjoint-set data structure.
% Once a longer pattern matches successfully, all of its children can be matched (but maybe not perfect).
% If this pattern fails to be selected finally, its successful match can reduce the extra matching cost.
Finally, we obtain 256 patterns that cover 80.77\% generated candidates.
% And details can be found in Table xxxx.

After pattern mining, we can formally construct our knowledge graph, where the {\it head} is a pair of items, the {\it relation} is one of the relations shown in Table~\ref{tab:prompts}, and the {\it tail} is an aggregated assertion that is originally generated and then mapped to a particular one among 256 patterns.
Each of the assertions (head, relation, tail) is associated with two populated scores, i.e., plausibility and typicality.

% \subsubsection{Knowledge Conceptualization}
% \label{sec:know_concept}
% \xin{
To produce abstract knowledge generalizable to new shopping contexts, we also consider the conceptualization with Probase~\cite{wu2012probase,he2022acquiring}.
% Since Probase was designed based on noun pairs, we conceptualize the nouns in one eventuality to their concepts and result in conceptualized eventualities $c_1, c_2, \cdots$ with different weights $w_{\langle e, c_1 \rangle}, w_{\langle e, c_2 \rangle}, \cdots$.
% For example, ``they both are a type of skiing product'' can be conceptualized as ``they both are a type of outdoor activity product'' because ``skiing'' is a noun and one of its concepts is ``outdoor activity''.
% The weight $w_{\langle e, c \rangle}$ equals to the likelihood as shown in Eq.~(\ref{eq:concept_node}).
% \begin{align}
%     w_{\langle e, c \rangle} = \text{Pr}(c | e) = \prod_{n^{(e)} \in e}\text{Pr}(n^{(c)} | n^{(e)}), \label{eq:concept_node}
% \end{align}
% \noindent where $n^{(e)}$ is one of nouns in eventuality $e$, $n^{(c)}$ is the corresponding token-level concept, $\text{Pr}(n^{(c)} | n^{(e)})$ is the likelihood for $IsA(n^{(e)}, n^{(c)})$ in Probase, and $\text{Pr}(c | e)$ is the product of token-level likelihoods.
% Besides, we can further build connections between item pairs to conceptualized eventualities.
% Specifically, a pair of items $(p_1, p_2)$ connects to a conceptualized eventuality $c$ if the pair connects to at least one extracted eventuality $e$ that can conceptualize to $c$.
% And the edge weight between $(p_1, p_2)$ and $c$ is aggregated as Eq.~(\ref{eq:concept_edge}).
% \begin{align}
%     w_{\langle (p_1, p_2), r, c \rangle} = \sum_{e \text{ s.t. } w_{\langle e, c \rangle} > 0}{w_{\langle (p_1, p_2), r, e \rangle} \cdot w_{\langle e, c \rangle}} \label{eq:concept_edge}
% \end{align}
This process has been employed and evaluated by ASER 2.0~\cite{DBLP:journals/ai/ZhangLPKOFS22}.
% Here, item pair-concept-level knowledge and eventuality-concept knowledge are aggregated by merging duplicated concepts and connected edges.  
The conceptualization process maps one extracted assertion to multiple candidate conceptualized assertions.
% Since the conceptualization is essential the token-level mapping, we use the Viterbi algorithm to estimate the maximum posterior probability and select the most highly relevant concepts.
% We select at most nine conceptualized eventualities, rank all the generated, and maintain the most confident half using a threshold $w_{\langle e, c \rangle} \geq 0.025$.
% Therefore, we construct FolkScope with human-guided and constraint generation, extraction, and conceptualization.
Finally, we obtain a KG with 184,146 items, 217,108 intentions, 857,972 abstract intentions, and 12,755,525 edges to explain 236,739 co-buy behaviors, where 2,298,011 edges from the view of original assertions and 9,297,500 edges from the angle of conceptualized ones, and 1,160,014 edges model the probabilities of the conceptualization.
%From Table~\ref{tab:folkScope}, we find that increasing the threshold to filter conceptualized eventualities with lower likelihood out can increase the density.

\begin{table*}[t]
\small
\centering
\caption{Two examples from the constructed knowledge graph. ``P.'' and ``T.'' mean the model predicted plausibility and typicality scores respectively. Generated tails with high typicality~(in green) and low typicality~(in red) scores are highlighted.}\label{tab:quality_case}
%\zheng{Use green and red colors to highlight high and low scores, respectively. Also, use one sentence to explain the colors and tell people what kinds of generation sentences are good.}
\begin{tabular}{p{4.0cm} | p{4.0cm} | p{1.3cm} | p{5.0cm} | p{0.5cm} | p{0.5cm} }
\toprule
\multicolumn{1}{c|}{Item 1} & \multicolumn{1}{c|}{Item 2} & \multicolumn{1}{c|}{Relation} & \multicolumn{1}{c|}{Tail} & \multicolumn{1}{c|}{P.} & \multicolumn{1}{c}{T.} \\
\midrule
\multirow{3}{*}{\shortstack{GGS III LCD Screen Protector \\glass for CANON 5D Mark III  \href{https://www.amazon.com/dp/B008DCK0I4}{(link)}}}
& \multirow{3}{*}{\shortstack{ECC5D3B Secure Grip Camera Case \\for Canon 5D Mark III  \href{https://www.amazon.com/dp/B008MAGCZM}{(link)}}}
& \textit{Open} & they can be used for the same purpose & 0.67 & 0.35 \\
& & \textit{HasProperty} & ``easy to install" and ``easy to remove" & 0.80 & 0.85 \\
& & \textit{SimilarTo} & \textcolor{myred}{the product he bought} & 0.95 & 0.09 \\
\multirow{4}{*}{\quad \quad \quad \quad \parbox[c]{1em}{
    \includegraphics[width=0.6in]{itemfigure/B008DCK0I4.jpg}}}
& \multirow{4}{*}{\quad \quad \quad \quad \parbox[c]{1em}{
    \includegraphics[width=0.6in]{itemfigure/B008MAGCZM.jpg}}}
& \textit{PartOf} & \textcolor{mygreen}{his camera gear} & 0.93 & 0.99 \\
& & \textit{UsedFor} & \textcolor{mygreen}{protect the camera from scratches and dust} & 0.97 & 0.99 \\
& & \textit{SymbolOf} &  his love for his camera & 0.99 & 0.88 \\
& & \textit{DefinedAs} & "Camera Accessories" on Amazon.com  & 0.99 & 0.67 \\
\midrule
\multirow{3}{*}{\shortstack{Sun Smarties Baby Girls  \\ UPF 50+ Non-Skid Sand and \\ Water Socks Small Hot Pink  \href{https://www.amazon.com/dp/B00ZGSOIM2}{(link)}}}
& \multirow{3}{*}{\shortstack{Schylling UV Play Shade, \\ SPF 50+, Ultra portable , Blue  \href{https://www.amazon.com/dp/B0014I4TYA}{(link)}}}
& \textit{Open} &  \textcolor{mygreen}{he was worried about his baby's skin} & 0.98 & 0.98 \\
& & \textit{SimilarTo} & \textcolor{myred}{each other} & 0.74 & 0.01 \\
& & \textit{DistinctFrom} & \textcolor{myred}{other similar products} & 0.97 & 0.10 \\
\multirow{5}{*}{\quad \quad \quad \quad \parbox[c]{1em}{
    \includegraphics[width=0.6in]{itemfigure/B00ZGSOIM2.jpg}}}
& \multirow{4}{*}{\quad \quad \quad \quad \parbox[c]{1em}{
    \includegraphics[width=0.6in]{itemfigure/B0014I4TYA.jpg}}}
& \textit{UsedFor} & baby's outdoor activities & 0.85 & 0.91 \\
& & \textit{CapableOf} & \textcolor{mygreen}{blocking harmful UV rays} & 0.97 & 0.99 \\
& & \textit{DefinedAs} & sun protection products & 0.87 & 0.81 \\
& & \textit{Result} & \textcolor{mygreen}{enjoy the sun sagely and comfortably} & 0.97 & 0.98 \\
& & \textit{Cause} &  want to use them for his/her baby & 0.99 & 0.94 \\
\bottomrule
\end{tabular}
\end{table*}

% }


\begin{table}[t]
\small
\setlength\tabcolsep{3pt}
\caption{Acceptance ratios of plausible assertions and the corresponding sizes of populated assertions with different cutting-off thresholds.}\label{tab:cutoff_stat}
\centering
\begin{tabular}{c|cc|cc|cc}
\toprule
\multirow{2}{*}{Threshold}& \multicolumn{2}{c|}{{\bf Clothing}}  & \multicolumn{2}{c|}{{\bf Electronics}}& \multicolumn{2}{c}{{\bf Total}} \\
% \cmidrule(r){2-3} \cmidrule(lr){4-5}\cmidrule(l){6-7}
& Accept & Size & Accept  & Size & Accept & Size\\ \midrule
0.5 & 83.73\%  & 7,986,031 & 82.74\% & 3,250,605 & 83.40\% & 11,236,636 \\
0.7 & 90.27\%  & 7,346,160 & 88.27\% & 2,868,256 & 89.40\% & 10,214,416 \\
0.8 & 91.02\%  & 6,947,606 & 89.50\% & 2,650,625 & 90.00\% & 9,598,231 \\
0.9 & 95.60\%  & 6,167,315 & 94.87\% & 2,230,423 & 95.36\% & 8,397,738 \\
\bottomrule
\end{tabular}
\end{table}

\section{Intrinsic Evaluations}

In this section, we first present some examples of our constructed KG and then conduct comprehensive intrinsic evaluations over KG.

\subsection{Examples}

We show two examples of co-purchasing products and their corresponding knowledge~($\S$~\ref{sec:know_gen}) as well as populated scores~($\S$~\ref{sec:two_round_annotations_and_populations}) in Table~\ref{tab:quality_case}.
We measure the quality of generated quality using both plausibility and typicality scores, which are again shown they are not correlated. 
For example, ``they are {\it SimilarTo} the product they bought'' for the first pair and ``they are {\it DistinctFrom} other similar products'' for the second pair are plausible assertions but not typical explanations of why a user would buy them together.
Moreover, some of the open relations are very good as well.
Take the second pair as an example: the open relation shows ``he was worried about his baby's skin'' as both products are related to baby skin protection.


\subsection{Human Evaluation}\label{sec:human_eval}

As we populate the whole generated assertions using two trained classifiers based on DeBERTa-large model and thus all of assertions have been labeled with the \textit{plausibility} and \textit{typicality} scores. 
To further evaluate the effectiveness of the knowledge population, we conducted the human evaluations by sampling a small number of populated assertions from different scales of predicted scores.
% Note we keep the same annotation setup and standards as two-stage annotations in Section~\ref{sec:two_round_annotations_and_populations}. 

\begin{table}[t]
\small
\caption{Evaluation on plausible rate and size of the populated KG. The prompts in the generation are not included in the calculations of assertion lengths.}
\label{tab:evaluation_plausibility}
\begin{tabular}{l|c|ccc}
\toprule
\multicolumn{1}{c|}{Relation} & \multicolumn{1}{c|}{Acc. Rate}  &  \multicolumn{1}{c}{\# Edges} & \multicolumn{1}{c}{\# Tails} & \multicolumn{1}{c}{Avg. Length} \\
\midrule
\textit{Open} & 87.54\% & 703,059 & 151,748 & 7.86 \\
\textit{HasA} & 94.08\% & 710,331 & 68,516 & 5.53 \\
\textit{HasProperty} & 79.13\% & 317,938 & 133,877 & 5.00 \\
\textit{RelatedTo} & 91.89\% & 571,918 & 130,551 & 3.08 \\
\textit{SimilarTo} & 86.35\% & 685,737 & 18,603 & 3.53 \\
\textit{PartOf} & 79.60\% & 674,928 & 114,983 & 4.36 \\
\textit{IsA} & 89.05\% & 591,037 & 98,262 & 3.82 \\
\textit{MadeOf} & 90.05\% & 528,289 & 70,246 & 5.06 \\
\textit{CreatedBy} & 95.15\% & 267,459 & 74,920 & 3.93 \\
\textit{DistinctFrom} & 91.74\% & 861,929 & 80,295 & 4.66 \\
\textit{DerivedFrom} & 85.54\% & 444,131 & 61,696 & 4.90 \\
\textit{UsedFor} & 91.79\% & 630,462 & 45,206 & 2.58 \\
\textit{CapableOf} & 87.73\% & 681,480 & 101,170 & 5.23 \\
\textit{SymbolOf} & 78.04\% & 809,196 & 52,075 & 3.46 \\
\textit{MannerOf} & 89.44\% & 371,892 & 122,829 & 4.38 \\
\textit{DefinedAs} & 85.59\% & 288,411 & 151,986 & 6.31 \\
\textit{Result} & 44.79\% & 568,523 & 166,018 & 8.80 \\
\textit{Cause} & 80.50\% & 696,392 & 185,042 & 7.06 \\
\textit{CauseDesire} & 67.23\% & 833,524 & 155,422 & 5.61 \\
\midrule
\textit{Total} & 83.40\% & 11,236,636 & 1,874,782 & 5.02 \\
\bottomrule
\end{tabular}
\end{table}

\subsubsection{Plausibility Evaluation} We randomly sample 200 plausible assertions from each relation in each of the clothing and electronics domains to test the human acceptance rate. 
The annotation is conducted in the same way as the construction step.
As we only annotate assertions predicted to be greater than the 0.5 plausibility score, the IAA is above 85\%, even greater than the one in the construction step.
As shown in Table~\ref{tab:cutoff_stat}, different cutting-off thresholds~(based on the plausibility prediction by our model) lead to the trade-offs between the accuracy and the KG size. 
Overall, \texttt{FolkScope} can achieve 83.4\% acceptance rate with a default threshold~(0.5). 
To understand what is filtered, we manually check the generations with low plausibility scores and find that OPT can generate awkward assertions, such as simply repeating the item titles or obviously logical errors regarding corresponding relations. 
Our classifier trained on annotated datasets helps resolve such cases. Using a larger threshold 0.9, we attain 95.35\% acceptance rate, a nearly 11.96\% improvement while still keeping above 8M plausible assertions. 
We also report the accuracy in terms of different relations in Table~\ref{tab:evaluation_plausibility}. 
We can observe that assertions concerning the relations of human beings' situations like {\it Cause}, {\it Result}, and {\it CauseDesire} have relatively lower plausibility scores and longer lengths than the relations of items' property, function, etc. This is because there exist some clues about items' knowledge in the item titles, while it is much harder to generate (or guess) implicit human-being's casual reasons using language generation. 




\begin{table}
\centering
\small
\caption{Average annotated typicality scores for aggregated assertions after pattern mining and conceptualization with different thresholds of populated typicality scores.}
\label{tab:quality_conceptualization_vs_eventuality}
\begin{tabular}{c|cc}
\toprule
Threshold & Aggregated Knowledge  & Conceptualization \\
\midrule
% 0.8 & 0.6215 & 0.4571 \\
% 0.9 & 0.6335 & 0.5567 \\
% 0.99 & 0.7028 & 0.5775 \\
0.8 & 62.15\% & 45.71\% \\
0.9 & 63.35\% & 55.67\% \\
0.99 & 70.28\% & 57.75\% \\
\bottomrule
\end{tabular}
\end{table}

\begin{figure}[t]
    \centering
    \includegraphics[width=0.90\linewidth]{figure/quality_evaluation_relation.pdf}
    \caption{Average typicality score of each relation in the populated KG with the cutting-off threshold 0.8.}
    \label{fig:quality_evaluation_relation}
\end{figure}

\subsubsection{Typicality Evaluation} The goal of the typicality population is to precisely recognize high-quality knowledge, and we evaluate whether assertions with high typicality scores are truly good ones. We randomly sample 200 assertions from each relation whose predicted typicality scores are above 0.8 for human evaluation. 
Each of the assertions is again annotated by five AMT workers and the average rating is used.
The results are shown in Table~\ref{tab:quality_conceptualization_vs_eventuality}.
It shows that the predicted typicality scores are less accurate than plausibility.
% Especially, after conceptualization, the typicality score will be further decreased.
Especially, the typicality score will be further decreased after conceptualization.
This is because, first, the conceptualization model may introduce some noise, and second, the more abstract knowledge tends to be less typical when asking humans to annotate.
We also show the typicality scores of each relation in  Figure~\ref{fig:quality_evaluation_relation}.
Different from plausibility, {\it SimilarTo}, {\it DistinctFrom},  {\it DefinedAs}, and {\it HasPropertyOf} are less typical compared to other relations.
They describe items' general features but can not well capture typical purchasing intentions though they have high plausibility scores,
whereas {\it CapableOf} and {\it MadeOf} are the most typical features that can explain purchasing intentions for the two domains we concern.




\subsection{Novelty Evaluation}

As we know, language model based generation capture spurious correlation given the condition of the generation~\citep{ji2022survey}.
Hence we simply quantify the diversity as the novelty ratio of generated tails not appearing in the item titles, i.e., novel generations. For example, the title ``Diesel Analog Three-Hand - Black and Gold Women's watch'' contains specific attributes like ``Black and Gold'' or type information ``women's watch.'' Such knowledge can be easily extracted by off-the-shelf tools. 
% If the generation is mostly copying the titles to reflect the attributes, then it means our knowledge will be covered by traditional information extraction based approaches.
Traditional information extraction based approaches mostly cover our knowledge if the generation simply copies titles to reflect the attributes.
Otherwise, it means that we provide much novel and diverse information compared with traditional approaches.
The novelty ratio increases from 96.85\% to 97.38\% after we use the trained classifiers for filtering. Intuitively, filtering can improve the novelty ratio.
For the assertions whose typicality scores are above 0.9, we also observe that the novelty ratio reaches 98.01\%. 
These findings suggest that \texttt{FolkScope} is indeed an effective framework for mining high-quality novel knowledge.

\begin{table}[t]\small
\centering
\caption{The generated knowledge in the same subcategory.
}\label{tab:subcategory_case}
\setlength\tabcolsep{3pt}
\begin{tabular}{l|c}
\toprule
Subcategory  & Generation         \\ \midrule 
\multirow{6}{*}{\begin{minipage}
{0.6in}{(Costumes, Toys)}\end{minipage}}
& he wants to disguise himself as a superhero  \\
& they can be used to make a crown costume    \\
& he wanted to be a star war character for Halloween \\
& they are both a manner of Christmas decoration  \\
& he wants kids to have fun and enjoy the Easter holiday \\
& he is able to dress up as a pirate \\
\midrule
\multirow{6}{*}{\begin{minipage}{0.6in}(Dresses, Dresses)\end{minipage}}
& they are symbol of the fashion trend  \\
& they can both be worn to formal events    \\
& they can both be worn for casual occasions \\
& they are both used for wedding dress  \\
& they are both capable of giving a good fit \\
& they can both being worn by girls of any age \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Fine-grained Subcategory Knowledge}
Since the items are organized in multilevel fine-grained subcategories in the catalog of shopping websites, we are interested in whether our constructed KG contains high-quality common intentions among items belonging to subcategories.
The common knowledge can be useful to have intention-level organizations besides category-level and further help downstream tasks.  
The co-buy item-pairs in our sampled \textit{clothing} category fall into 15,708 subcategory pairs, such as (\textit{necklaces}, \textit{earning}) or (\textit{sweater}, \textit{home \& kitchen}), where most of them are different subcategories in one pair.
We select frequent common assertions with high typicality scores to demonstrate the abstract knowledge. Two examples are shown in Table~\ref{tab:subcategory_case}.
Though costumes and toys belong to two different types, they are complementary because of the same usage, such as  ``Halloween,'' ``Easter holiday,'' and ``Christmas,'' or sharing the same key feature like ``star war character,'' ``pirate.''
On the other hand, if two items fall in the same subcategory, like ``dresses'' in Table~\ref{tab:subcategory_case}, 
the generated assertions share some commonness of wearing in certain events and fitting each other. 





\section{Extrinsic Evaluation}

In this section, we evaluate the constructed KG by demonstrating it can improve recommendations. 
Further analysis shows that the performance can improve with more plausible and typical edges.

\subsection{Experimental Setup}

%\subsubsection{Evaluation Dataset} 
To keep consistent with the constructed KG, we also use user-item interaction datasets of the two categories from the Amazon Review dataset~\cite{ni-etal-2019-justifying}.
The detailed statistics are in Table~\ref{Table: recommendation_stats}. Each dataset is separated into training, validation, and testing sets with a ratio of 8:1:1. We use the root mean square error~(RMSE) to evaluate the performance of the recommendation system. All experiments repeat five times, and the average scores are reported. 

% Here we are trying to incorporate the KG information into the recommendation. 
% For the constructed KG, the head of a triple is a pair of co-buy items, the relation is a type of ConceptNet relation, and the tail is an extracted assertion after pattern mining (\S~\ref{sec:know_constct}). 
% In the populated KG, edges also have plausibility and typicality scores predicted by our models. 
To fairly evaluate the KG for recommendations, we restrict the usage of the KG such that we can only use the co-buy pairs that are simultaneously purchased by at least one user in the training set. 
With this restriction, we match a sub-graph of the original KG by using the actual co-buy from the users in the recommendation training set. 
The detailed statistics of the matched KG are in the first line of Table~\ref{tab:matched_kg_stats}.
%The number of items indicates how many items are covered by the co-buy pairs in the matched KG, and 
The item coverage computes the percentage of the items in the recommendation dataset that are covered by the matched KG. 
We also want to evaluate whether two rounds of annotations and populations on plausibility and typicality can improve the recommendation results. 
We further filter the matched KG with a threshold of 0.5 or 0.9 on both two scores. 
In Table~\ref{tab:matched_kg_stats}, the number of edges essentially reduces when the filters are applied, but the coverage of the items does not drastically drop. 

%\subsubsection{Knowledge Incorporation}

To incorporate the KG information into the recommendation task, we first learn item embeddings from the matched KG, and then use the embeddings as features for the Wide\&Deep model~\citep{cheng2016wide} to train a recommendation model. To learn the item representations from a matched KG, we use a modified version of the TransE~\citep{bordes2013translating} algorithm. 
The modifications are as follows.
First, we initialize all the node embeddings of the tail nodes with their Sentence-BERT~\cite{reimers-2019-sentence-bert} representations. 
Second, as the head of a triple is a pair of items, we compute the average of item embeddings as the head embedding for the TransE model. 
In the negative sampling process, either one or two items in the head are randomly corrupted. 
The objective function is then described by the following equation: 
\begin{align}
    \mathcal{L} =  \gamma + d( \frac{ \bm{p_1} + \bm{p_2}}{2} + \bm{r}, \bm{e}) -  d( \frac{\bm{p_1'} + \bm{p_2'}}{2} + \bm{r},\bm{e}), \nonumber
\label{eq:modifired_transE}
\end{align}
where $\gamma$ is a margin parameter, and $\bm{p_1}$, $\bm{p_2}$, $\bm{p_1'}$, $\bm{p_2'}$ are the positive and negative item embeddings for items $p_1$, $p_2$, $p_1'$, $p_2'$ respectively. Meanwhile, $\bm{r}$ is the relation embedding for relation $r$, $\bm{e}$ is the embedding for the tail $e$, and the function $d$ is Euclidean distance. 

\begin{table}[t]
\small
\caption{Statistics of the recommendation datasets.}
\label{Table: recommendation_stats}
\begin{tabular}{l|cc}
\toprule
  &  \multicolumn{1}{c}{Clothing}   & \multicolumn{1}{c}{Electronics} \\
\midrule
\# Users &  782,144 & 486,349  \\
\# Items &  18,042  &  6,166   \\
\# Interactions & 1,579,499 & 1,056,406 \\
Density &  0.011\%  & 0.035\% \\

\bottomrule
\end{tabular}
\end{table}

\begin{table}[t]
\small
\setlength\tabcolsep{3.5pt}
\caption{Details of matched KG subsets. ``Plau.'' means plausibility and ``Typi'' means typicality.}
\label{tab:matched_kg_stats}
\begin{tabular}{l|cc|cc}
\toprule
\multicolumn{1}{c|}{\multirow{2}{*}{Knowledge Graphs}} & \multicolumn{2}{c|}{Clothing}      & \multicolumn{2}{c}{Electronics}                     \\
\multicolumn{1}{c|}{}                    & \# Edges  &  Coverage & \# Edges  &  Coverage \\
\midrule
 Matched Knowledge Graph                                    & 432,119      & 79.83\%          & 117,836       & 82.40\%  \\
$\thinspace$ + Plau. \textgreater 0.5                        & 323,263      & 79.83\%         & 78,908         & 82.40\%  \\
$\thinspace$ + Plau. \textgreater 0.5 and Typi. \textgreater 0.5         & 141,422     & 79.67\%  & 40,978        & 80.20\%  \\
$\thinspace$ + Plau. \textgreater 0.9                                   & 269,210     & 79.83\%   & 58,013       & 82.39\%  \\
$\thinspace$  + Plau. \textgreater 0.9 and Typi. \textgreater 0.9      & 103,262    & 79.36\%       & 27,288      & 76.94\%  \\

\bottomrule
\end{tabular}
\end{table}


% \begin{table*}[t]
% \small
% \caption{Matched Knowledge Graph Details}
% \label{tab:matched_kg_stats}
% \begin{tabular}{l|cccc|cccc}
% \toprule
% \multicolumn{1}{c|}{\multirow{2}{*}{Knowledge Graphs}} & \multicolumn{4}{c|}{Clothing}      & \multicolumn{4}{c}{Electronics}                     \\
% \multicolumn{1}{c|}{}                                          & \# Co-Buy Heads & \# Edges & \# Items & Item Coverage & \# Co-Buy Heads & \# Edges & \# Items & Item Coverage \\
% \midrule
%  Matched Knowledge Graph                                         & 13,190          & 432,119  & 14,402      & 79.83\%  & 3,869           & 117,836  & 5,081       & 82.40\%  \\
% $\thinspace$ + Plausibility \textgreater 0.5                                 & 13,190          & 323,263  & 14,402      & 79.83\%  & 3,869           & 78,908   & 5,081       & 82.40\%  \\
% $\thinspace$ + Plausibility \textgreater 0.5 and Typicality \textgreater 0.5    & 13,155          & 141,422  & 14,374      & 79.67\%  & 3,752           & 40,978   & 4,945       & 80.20\%  \\
% $\thinspace$ + Plausibility \textgreater 0.9                                 & 13,188          & 269,210  & 14,402      & 79.83\%  & 3,862           & 58,013   & 5,080       & 82.39\%  \\
% $\thinspace$  + Plausibility \textgreater 0.9 and Typicality \textgreater 0.9    & 13,092          & 103,262  & 14,318      & 79.36\%  & 3,557           & 27,288   & 4,744       & 76.94\%  \\

% \bottomrule
% \end{tabular}
% \end{table*}

\subsection{Experimental Results}

We conduct two ablation studies to evaluate the effect of structural information provided by the co-buy pairs and the semantic information provided by the tails' text only. 
For the former, we train a standard TransE model solely on co-buy pairs to learn the graph embeddings of items. 
For the latter, for each item in the matched KG, we conduct average pooling of its neighbor tails' Sentence-BERT embeddings as its semantic representations. 
The experimental results are shown in Table~\ref{tab:rec_result}, and we have the following observations. 
First, the textual information contained in intentional assertions is useful for product recommendations. This can be testified as the W\&D model can perform better even when only features of the assertions are provided. 
Second, our KG, even before annotations and filtering, can produce better item embeddings than solely using the co-buy item graphs. As we can see, the performance of our matched KG is better than that of the co-buy pair graphs. 
Third, the two step of annotation and population indeed help improve the item embeddings for recommendations. The higher the scores are, the larger improvement the recommendation system obtains.


\begin{table}[t]
\small
\caption{Recommendation results in RMSE.}
\label{tab:rec_result}
\begin{tabular}{l|cc}
\toprule
\multicolumn{1}{c|}{Method}&  \multicolumn{1}{c}{Clothing}  & \multicolumn{1}{c}{Electronics} \\
\midrule

NCF~\citep{he2017neural} & 1.117  &   1.086 \\

W\&D~\citep{cheng2016wide} & 1.104  &  1.071\\

$\thinspace$  + Co-Buy Pairs Features Only  & 1.096  &  1.067 \\
$\thinspace$  + Eventuality Features Only & 1.093  &  1.068 \\

$\thinspace$  + Matched Knowledge Graph & 1.093  & 1.058 \\
$\thinspace$  $\thinspace$ + Plausibility > 0.5 &  1.087 & 1.060 \\
$\thinspace$ $\thinspace$ + Plausibility > 0.5 and Typicality > 0.5 & \textbf{1.081} & 1.053 \\

$\thinspace$ $\thinspace$ + Plausibility > 0.9 &  1.086 & 1.053 \\
$\thinspace$ $\thinspace$ + Plausibility > 0.9 and Typicality > 0.9 &  \textbf{1.081} & \textbf{1.052} \\

\bottomrule
\end{tabular}
\end{table}

\section{Conclusion}

In this paper, we propose a new framework, \texttt{FolkScope}, to capture intentional commonsense knowledge for e-commerce behaviors. We develop a human-in-the-loop semi-automatic way to construct an intention KG, where the candidate assertions are automatically generated from large pretrained language models, with carefully designed prompts to align with ConceptNet commonsense relations.
Then we ask human annotators to annotate both plausibility and typicality scores of sampled assertions and develop learning models to populate them to all generated candidates.
Then the high-quality assertions will be further structured using pattern mining and conceptualization to form more condensed and abstractive knowledge.
We conduct both intrinsic and extrinsic evaluations to demonstrate the quality and usefulness of our constructed KG.

% In this work, we propose to harvest intentional commonsense knowledge in e-commerce scenarios from pretrained language models and build a large-scale intention knowledge graph.  
% To effectively prompt the knowledge, we design shopping-context aware prompts to deduce the knowledge from the generated explanations or intents of user behaviors. 
% For the sake of high-quality knowledge, we conduct two-stage annotations and populate the inference over the whole generations. 
% Human evaluations demonstrate the effectiveness of our proposed framework with relatively low annotation efforts. 
% Finally, incorporating constructed KGs into product recommendation task achieve improvement over several baselines. 
% We also extensively discuss potential usages and applications.

% In the future, we will introduce multiple type of user behaviors to have multi-tasking prompting and consider to encoder visual signals from item images to help mine implicit knowledge that is not stated in the text.


% \subsection{Template Parameters}

% In addition to specifying the {\itshape template style} to be used in
% formatting your work, there are a number of {\itshape template parameters}
% which modify some part of the applied template style. A complete list
% of these parameters can be found in the {\itshape \LaTeX\ User's Guide.}

% Frequently-used parameters, or combinations of parameters, include:
% \begin{itemize}
% \item {\verb|anonymous,review|}: Suitable for a ``double-blind''
%   conference submission. Anonymizes the work and includes line
%   numbers. Use with the \verb|\acmSubmissionID| command to print the
%   submission's unique ID on each page of the work.
% \item{\verb|authorversion|}: Produces a version of the work suitable
%   for posting by the author.
% \item{\verb|screen|}: Produces colored hyperlinks.
% \end{itemize}

% This document uses the following string as the first command in the
% source file:
% \begin{verbatim}
% \documentclass[sigconf]{acmart}
% \end{verbatim}


% The \verb|authornote| and \verb|authornotemark| commands allow a note
% to apply to multiple authors --- for example, if the first two authors
% of an article contributed equally to the work.

% If your author list is lengthy, you must define a shortened version of
% the list of authors to be used in the page headers, to prevent
% overlapping text. The following command should be placed just after
% the last \verb|\author{}| definition:
% \begin{verbatim}
%   \renewcommand{\shortauthors}{McCartney, et al.}
% \end{verbatim}
% Omitting this command will force the use of a concatenated list of all
% of the authors' names, which may result in overlapping text in the
% page headers.


% \section{CCS Concepts and User-Defined Keywords}

% Two elements of the ``acmart'' document class provide powerful
% taxonomic tools for you to help readers find your work in an online
% search.

% The ACM Computing Classification System ---
% \url{https://www.acm.org/publications/class-2012} --- is a set of
% classifiers and concepts that describe the computing
% discipline. Authors can select entries from this classification
% system, via \url{https://dl.acm.org/ccs/ccs.cfm}, and generate the
% commands to be included in the \LaTeX\ source.


% To set a wider table, which takes up the whole width of the page's
% live area, use the environment \textbf{table*} to enclose the table's
% contents and the table caption.  As with a single-column table, this
% wide table will ``float'' to a location deemed more
% desirable. Immediately following this sentence is the point at which
% Table~\ref{tab:commands} is included in the input file; again, it is
% instructive to compare the placement of the table here with the table
% in the printed output of this document.

% \clearpage
{\tiny
\bibliographystyle{ACM-Reference-Format}
\bibliography{wsdm}
}


\section*{Appendix}

\subsection*{Annotation Guideline}

Workers satisfying the following three requirements are invited to participate: (1) at least 90\% lifelong HITs approval rate, (2) at least 1,000 HITs approved, and (3) achieving 80\% accuracy on at least 10 qualification questions, which are carefully selected by authors of this paper. Qualified workers will be further invited to annotate 16 tricky assertions. Based on workers' annotations, they will receive personalized feedback containing explanations of the errors they made along with advice to improve their annotation accuracy. Workers surpassing these two rounds are deemed qualified for main round annotations. To avoid spamming, experts will provide feedback for all workers based on a sample of their main rounds' annotations from time to time.

We conduct human annotations and evaluations on the Amazon Mechanical Turk using the Figure~\ref{fig:validity_question} for the first-stage plausibility annotation and the Figure~\ref{fig:quality_question} for the second-stage typicality annotation.

% \begin{figure}[b]
%       \centering
%       \includegraphics[scale=0.3]{figure/image_card.pdf}
%       \caption{The item card in our annotation template. Three item's images and item's name and category information are provided for both items. Clicking the item's name (in red) will jump to the item's shopping webpage.}
%       \label{fig:image_card}
% \end{figure}


\begin{figure}[b]
      \centering
      \includegraphics[scale=0.4]{figure/validity_question.pdf}
      \caption{The question card in our plausibility annotation round. A prompted assertion with its corresponding relation is presented to Turk worker. Workers can choose one from valid, invalid, and unfamiliar.}
      \label{fig:validity_question}
\end{figure}


\begin{figure}[b]
      \centering
      \includegraphics[scale=0.4]{figure/quality_question.pdf}
      \caption{The question card in our typicality annotation round. A prompted assertion with its corresponding relation is presented to Turk worker. Workers can choose one from valid, invalid, and unfamiliar.}
      \label{fig:quality_question}
\end{figure}


\subsection*{Knowledge Population}

Using different confidence cutting-off thresholds leads to trade-offs between the accuracy of generation and the size of the corpus. 
Higher values result in conservative selections that favor precision over recall, whereas lower ones tend to recall more plausible assertions.
We plotted four cutoff points in Figure~\ref{fig:prcurve}.


\begin{figure}
     \centering
     \includegraphics[scale=0.4]{figure/prcurve.pdf}
     \caption{The precision-recall curve of our plausibility population classifier on the human-labeled validation set. The annotated points show the different thresholds~(cutoffs) to filter the generated assertions, i.e. from left to right: 0.9, 0.8, 0.7, 0.5 respectively.}
     \label{fig:prcurve}
\end{figure}


\begin{table}[b]\small
\centering
\caption{Frequent pattern coverage on human-annotated knowledge.}\label{tab:coverage}
\setlength\tabcolsep{4pt}
\begin{tabular}{l|c|c|c}
     \toprule
     Type & Relation & \# of Patterns & Coverage \\
     \midrule
     \multirow{10}{*}{\begin{minipage}{0.4in}Item\end{minipage}}& \textit{RelatedTo} & 14 & 96.94 \\
     & \textit{IsA} & 15 & 97.20 \\
     & \textit{HasA} & 12 & 99.30 \\
     & \textit{PartOf} & 8 & 99.83 \\
     & \textit{MadeOf} & 13 & 99.45 \\
     & \textit{SimilarTo} & 7 & 22.22 \\
     & \textit{CreatedBy} & 14 & 98.59 \\
     & \textit{HasProperty} & 16 & 63.20 \\
     & \textit{DistinctFrom} & 9 & 97.30 \\
     & \textit{DerivedFrom} & 20 & 100.00 \\
     \midrule
     \multirow{5}{*}{\begin{minipage}{0.4in}Function\end{minipage}}& \textit{UsedFor} & 2 & 96.57 \\
     & \textit{CapableOf} & 13 & 74.68 \\
     & \textit{DefinedAs} & 27 & 95.99 \\
     & \textit{SymbolOf} & 9 & 99.76\\
     & \textit{MannerOf} & 34 & 98.56 \\
     \midrule
     \multirow{4}{*}{\begin{minipage}{0.4in}Human\end{minipage}}& \textit{Cause} & 21 & 93.68 \\
     & \textit{Result} & 0 & 0 \\
     %& \textit{MotivatedBy} & \\
     & \textit{CauseDesire} & 0 & 0 \\
     \midrule 
     \textit{Overall} & / & 256 & 80.77 \\
     \bottomrule
\end{tabular}
\end{table}



% For the sake of space, we append the annotation guidelines as well as sampled datasets in an anonymous link: \url{https://anonymous.4open.science/r/FolkScope/}. 



%% If your work has an appendix, this is the place to put it.



% \begin{table*}[h]
% \small
% \caption{Ratio and size of plausible assertions by each relation under two different thresholds. Here we use 0.9 as the high critical value and 0.5 as the lower one. Relations with high quality are marked in grey.}
% \begin{tabular}{l|cccccccccc}
% \toprule
% & \multicolumn{5}{c}{{\bf Critic-high}}  & \multicolumn{5}{c}{{\bf Critic-low }} \\ \cmidrule(r){2-6} \cmidrule(lr){7-11}
%  Relation     & Plau. Rate & Qual. Ratio & Avg. Length & Size~(all) & Size~(uniq) & Plau. Rate & Qual. Ratio & Avg. Length & Size~(all) & Size~(uniq)\\
% \midrule
% %\rowcolor{lightgray}
% cause & 95.68\% & 10.29\% & 5.70 & 565,106 & 113,967 & 80.50\% & 13.56\% & 7.06 & 696,392 & 185,042  \\
% \rowcolor{lightgray}
% madeOf & 98.61\% & 74.01\% & 5.31 & 401,681 & 43,376 & 90.05\% & 72.15\% & 5.06 & 528,289 & 70,246 \\
% partOf & 91.58\% & 9.91\% & 4.06 & 482,018 & 79,523 & 79.60\% & 6.69\% & 4.36 & 674,928 & 114,983 \\
% \rowcolor{lightgray}
% can & 98.21\% & 21.96\% & 5.65 & 458,812 & 46,033 & 94.08\% & 18.97\% & 5.53 & 710,331 & 68,516 \\
% mannerOf & 95.97\% & 11.11\% & 3.85 & 272,357 & 68,015 & 89.44\% & 10.97\% & 4.38 & 371,892 & 122,829 \\
% \rowcolor{lightgray}
% usedFor & 98.31\% & 19.45\% & 2.47 & 516,540 & 32,205 & 91.79\% & 21.59\%& 2.58 & 630,462 & 45,206 \\
% symbolOf & 89.56\% & 13.33\% & 3.26 & 653,379 & 42,077 & 78.04\% & 10.88\% & 3.46 & 809,196 & 52,075 \\
% createdBy & 98.88\% & 12.83\% & 3.52 & 180,313 & 27,290 & 95.15\% & 9.66\% & 3.93 & 267,459 & 74,920 \\
% isA & 97.38\% & 10.12\% & 3.61 & 505,055 & 66,012 & 89.05\% & 8.07\% & 3.82 & 591,037 & 98,262 \\
% relatedTo & 97.93\% & 13.03\% & 2.81 & 472,803 & 91,196 & 91.89\% & 10.64\% & 3.08 & 571,918 & 130,551 \\
% \rowcolor{lightgray}
% capableOf & 95.41\% & 61.98\% & 5.17 & 557,534 & 72,844 & 87.73\% & 56.79\% & 5.23 & 681,480 & 101,170 \\
% definedAs & 90.10\% & 6.44\% & 5.77 & 178,092 & 79,233 & 85.59\% & 4.16\% & 6.31 & 288,411 & 151,986 \\
% distinctFrom & 93.54\% & 0.49\% & 4.60 & 830,223 & 71,391 & 91.74\% & 0.73\% & 4.66 & 861,929 & 80,295 \\
% similarTo & 98.24\% & 0.00\% & 3.62 & 517,215 & 8,333 & 86.35\% & 0.07\% & 3.53 & 685,737 & 18,603 \\
% \rowcolor{lightgray}
% propertyOf & 90.80\% & 24.86\% & 4.67 & 160,400 & 51,981 & 79.13\% & 27.38\% & 5.00 & 317,938 & 133,877 \\
% deriveFrom & 98.80\% & 14.88\% & 4.83 & 242,319 & 34,264 & 85.54\% & 12.51\% & 4.90 & 444,131 & 61,696 \\
% open & 95.04\% & TBA & 7.67 & 539,797 & 108,932 & 87.54\% & TBA & 7.86 & 703,059 & 151,748 \\
% causeDesire & 92.59\% & TBA & 6.40 & 568,086 & 122,403 & 67.23\% & TBA & 5.61 & 833,524 & 155,422 \\
% effect & TBA & TBA & TBA & TBA & TBA & TBA &TBA & TBA & TBA & TBA \\
% \midrule
% Total & 95.36\% & 18.84\% & 4.64 & 8,123,805 & 1,093,641 & 85.70\% & 17.91\% & 4.84 & 10,714,833 & 1,728,857 \\
% \bottomrule
% \end{tabular}
% \end{table*}


% \begin{table}[h]
% \caption{Validity statistics by relations}
% \begin{tabular}{@{}lccc@{}}
% \toprule
% Relation & \#data & Valid(maj) & Valid(veto) \\ \midrule
% cause & 4116 & 73.8\% & 45.7\% \\
% madeOf & 4112 & 62.0\% & 28.9\% \\
% partOf & 4111 & 67.4\% & 33.8\% \\
% can & 4096 & 54.6\% & 23.7\% \\
% mannerOf & 4096 & 47.8\% & 20.8\% \\
% usedFor & 4092 & 70.3\% & 33.9\% \\
% symbolOf & 4092 & 80.2\% & 49.3\% \\
% createdBy & 4077 & 38.2\% & 13.3\% \\
% isA & 4067 & 67.3\% & 33.5\% \\
% relatedTo & 4065 & 61.3\% & 28.1\% \\
% capableOf & 4006 & 71.9\% & 36.5\% \\
% definedAs & 3988 & 37.7\% & 11.8\% \\
% distinctFrom & 3843 & 91.7\% & 62.1\% \\
% similarTo & 3819 & 67.1\% & 29.4\% \\
% propertOf & 3802 & 37.8\% & 10.8\% \\
% deriveFrom & 3759 & 51.4\% & 19.6\% \\
% open & 0 & N/A & N/A\\
% reason & 0 & N/A & N/A\\
% other & 0 & N/A & N/A\\
% %motivatedBy & 0 & N/A & N/A\\
% \midrule
% Total & 64141 & 61.3\% & 30.1\% \\
% \bottomrule
% \end{tabular}
% \end{table}

% \begin{table}[h!]
% \small
% \caption{Validity statistics by relations in the view of pairs' categories. Validity is rated by majority vote.}
% \begin{tabular}{@{}lccccc@{}}
% \toprule
% Relation & \#Data & \#Clothing & Valid & \#Electronic & Valid \\ \midrule
% cause & 4116 & 2866 & 71.5\% & 3156 & 75.8\% \\
% madeOf & 4112 & 2869 & 68.2\% & 3146 & 66.3\% \\
% partOf & 4111 & 2868 & 66.6\% & 3148 & 70.9\% \\
% can & 4096 & 2846 & 58.5\% & 3135 & 58.6\% \\
% mannerOf & 4096 & 2853 & 52.6\% & 3144 & 52.0\% \\
% usedFor & 4092 & 2856 & 76.5\% & 3136 & 75.0\% \\
% symbolOf & 4092 & 2856 & 79.7\% & 3133 & 82.9\% \\
% createdBy & 4077 & 2827 & 40.6\% & 3121 & 42.1\% \\
% isA & 4067 & 2834 & 74.9\% & 3112 & 71.8\% \\
% relatedTo & 4065 & 2829 & 61.4\% & 3118 & 64.9\% \\
% capableOf & 4006 & 2756 & 77.9\% & 3074 & 77.2\% \\
% definedAs & 3988 & 2755 & 39.8\% & 3050 & 40.9\% \\
% distinctFrom & 3843 & 2610 & 93.8\% & 2922 & 91.9\% \\
% similarTo & 3819 & 2583 & 69.0\% & 2913 & 71.3\% \\
% propertOf & 3802 & 2559 & 42.3\% & 2919 & 40.9\% \\
% deriveFrom & 3759 & 2526 & 55.5\% & 2901 & 55.8\% \\
% open & 0 & N/A & N/A & N/A & N/A \\
% reason & 0 & N/A & N/A & N/A & N/A \\
% other & 0 & N/A & N/A & N/A & N/A \\
% %motivatedBy & 0 & N/A & N/A & N/A & N/A \\
% \midrule
% Total & 64141 & 44293 & 64.4\% & 49128 & 64.9\% \\
% \bottomrule
% \end{tabular}
% \end{table}

\end{document}
\endinput
%%
%% End of file `sample-sigconf.tex'.
