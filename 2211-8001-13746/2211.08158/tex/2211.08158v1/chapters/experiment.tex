\section{Experiments}
\subsection{Experimental Setup}
Following previous work \cite{zhang2022syngec}, we use the cleaned version of the Lang8 dataset (CLang8) \cite{rothe2021recipe}, the FCE dataset \cite{yannakoudakis2011new}, the NUCLE dataset \cite{dahlmeier2013building} and the WI+LOCNESS train-set \cite{bryant2019bea} for training. We use the  BEA-19-\textit{Dev} dataset \cite{bryant2019bea} for validating. We report average (P)recision, (R)ecall, and (F$_{0.5}$) metrics on the CoNLL-14-\textit{Test} \cite{ng2014conll} and BEA-19-\textit{Test} \cite{bryant2019bea} datasets with their official evaluation tools \cite{dahlmeier2012better,bryant2017automatic} with 3 random seeds.
We present more data statistics and implementation details in Appendix \ref{sec:app:a}.




\input{tables/main_results.tex}

\subsection{Experimental Results}
\textbf{Main results} are shown in Table \ref{tab:main:results}.
\textcolor{black}{When not using pre-trained language models (w/o PLM), CSynGEC gets 63.3 and 67.4 F$_{0.5}$ scores on CoNLL-14-\textit{Test} and BEA-19-\textit{Test}, respectively, which is quite competitive. Compared with the baseline, incorporating tailored constituent-based syntax leads to significant F$0.5$ gains (+4.2/+3.2) on both benchmarks, which clearly shows its effectiveness. We also build a stronger baseline by initializing the parameters of the Transformer backbone from BART \cite{lewis2020bart} (w/ PLM). CSynGEC still achieves +1.0/+0.6 F$_{0.5}$ improvements over BART, which indicates that the effectiveness of our method will not easily be overwhelmed by PLMs.}

To distinguish, we rename the method in \citet{zhang2022syngec} as DSynGEC (Dependency SynGEC). We observe that CSynGEC outperforms all other syntax-enhanced counterparts, except DSynGEC. 
\textcolor{black}{Specifically, we find that DSynGEC always performs better in precision, while CSynGEC always achieves higher recall. Such a discrepancy further motivates us to combine them.}

The SOTA results are kept by \citet{rothe2021recipe}, which leverages a huge PLM with up to 11B parameters\footnote{While CSynGEC only has 70M parameters.}. Since CSynGEC is model-agnostic, we will test it in more SOTA baselines in the future.

\input{tables/ablation_studies.tex}

\textbf{Benefits of GOPar.} We first study the effectiveness of using syntax derived from GOPar. We replace GOPar with an off-the-shelf constituency parser trained on the PTB treebank \cite{marcinkiewicz1994building}. As shown in Table \ref{tab:abb}, the performance of CSynGEC dramatically drops after changing the parser (-3.9/-2.8), which confirms the necessity of GOPar. We conjecture that off-the-shelf parsers tend to provide low-quality parses for ungrammatical sentences, which may introduce noise for GEC.

\textbf{Impact of extended syntax scheme.} We extend the constituent-based syntax scheme by inserting pseudo non-terminal nodes to represent grammatical errors. To study the impact of this task-oriented adaptation, we remove all inserted pseudo nodes before feeding the trees produced by GOPar into GEC models. From Table \ref{tab:abb}, we can see that CSynGEC heavily degenerates without the extended scheme (-2.0/-2.3). However, the performance is still better than using an off-the-shelf parser, as the high-quality parses for the correct part are kept.


\textcolor{black}{\textbf{Results of syntax combination.} From Table \ref{tab:abb}, we can see that the intra-model combination significantly improves recall over DSynGEC (+1.5/+1.8) and CSynGEC (+1.4/+1.4), and achieves better overall performance on both test sets. Besides, we note that this method will not work if we use a sharing cross-attention layer to attend to different kinds of syntactic information.}

\textcolor{black}{Under the inter-model combination setting, ``3 $\times$ DSynGEC + 3 $\times$ CSynGEC'' substantially improves precision over ``6 $\times$ DSynGEC'' (+2.0/+1.8) and ``6 $\times$ CSynGEC'' (+1.3/+1.2), and achieves the best F$_{0.5}$ scores. All these results demonstrate that constituent-based and dependency-based syntax has intrinsic complementary strength for helping GEC models. We also give a case in Appendix \ref{sec:case} to show how both kinds of syntax work.}



