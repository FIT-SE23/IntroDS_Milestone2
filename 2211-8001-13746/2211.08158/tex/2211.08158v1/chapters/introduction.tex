\section{Introduction}


\textcolor{black}{
Grammatical error correction (GEC) aims to craft a grammatical and fluent version of an ungrammatical sentence with the intended meaning \citep{wang2021comprehensive}.
As an important yet challenging task in NLP, GEC lays foundations for many downstream applications like writing assistance and second language acquisition \cite{bryant2022survey}.
}


In recent progress, Transformer \cite{vaswani2017attention} emerges as the most powerful backbone for GEC \cite{rothe2021recipe,zhang-etal-2022-mucgec}. Despite its remarkable success, some recent works point out that Transformer can be further strengthened when integrated with syntactic information \cite{wan2021syntax,li2022syntax}.
\textcolor{black}{
However, there lies a challenge for syntax-enhanced GEC in that off-the-shelf parsers may not handle ungrammatical inputs well. To tackle this problem, \citet{zhang2022syngec} present the SynGEC approach. Specifically, they build a GEC-oriented dependency parser by extending the original syntax scheme and utilizing parallel GEC data to automatically generate trees of ungrammatical inputs for training. Such a tailored dependency parser helps SynGEC stand out from all syntax-enhanced GEC methods and achieve state-of-the-art (SOTA) results.
}


\input{figures/rules.tex}

Till now, the benefits of syntax in GEC have---for the most part---been demonstrated just within the
context of dependency-based syntax \cite{wan2021syntax, li2022syntax, zhang2022syngec}. Nevertheless, there is yet another kind of syntax, namely constituent-based syntax. It adopts hierarchical phrase-based trees to convey sentence structural information and has been proven to be useful in various downstream tasks \cite{DBLP:conf/emnlp/MarcheggianiT20,DBLP:conf/naacl/XiaZWLZHSZ21, DBLP:conf/naacl/ZhouLT22, bharadwaj-shevade-2022-efficient}.


\textcolor{black}{
This work first manages to explore whether---or to what extent---can constituent-based syntax contribute to GEC. After witnessing the success of SynGEC in exploiting dependency-based syntax, we extend it to constituent-based syntax, and propose CSynGEC (Constituency SynGEC).
We imitatively extend the constituent-based syntax scheme to accommodate grammatical errors and leverage parallel GEC data to train a GEC-oriented parser (GOPar) for constituency parsing.
To encode syntactic knowledge, we employ the graph convolutional network (GCN) \cite{DBLP:conf/iclr/KipfW17}.
Experimental results show that CSynGEC significantly improves the performance over the Transformer baseline, which proves that---with appropriate adaptation---constituent-based syntax is another useful external knowledge for GEC models.
}



\textcolor{black}{
Our work further attempts to investigate the integration of constituency-based and dependency-based syntax for GEC, as previous work has proven that they are complementary in helping downstream tasks \cite{li-etal-2010-combining, kong2011combining, fei-etal-2021-better}.
To this end, we experiment with two methods: 1) intra-model combination: we employ two separate GCNs to encode both kinds of syntax and fuse the representations for final decoding; 2) inter-model combination: we build multiple models enhanced with different kinds of syntax, and selecting edits predicted by them to achieve final results. We find that both methods improve the overall performance over just using a single kind of syntax, while their behaviors are different. Specifically, the intra-model method improves recall, but the inter-model method improves precision.
}








