\section{Syntax-Enhanced GEC Model}
\label{sec:model}

Our syntax-enhanced GEC model is built upon Transformer \cite{vaswani2017attention}. 
We utilize GCN \cite{DBLP:conf/iclr/KipfW17} to encode syntactic information and fuse the results with the outputs of Transformer encoder for decoding.

\textbf{Transformer backbone.} Transformer contains two parts: an encoder for encoding the source sentence and a decoder for predicting based on the output of the encoder and the generated tokens. For more details, please refer to \citet{vaswani2017attention}.

\textbf{GCN over constituency trees.} We employ GCN to encode  constituency trees following previous studies \cite{DBLP:conf/emnlp/MarcheggianiT20, DBLP:conf/naacl/XiaZWLZHSZ21, DBLP:conf/naacl/ZhouLT22}.
\textcolor{black}{
We view the constituency tree as an undirected graph. 
The nodes in this graph are all terminal/non-terminal nodes in the constituency tree, and edges are constructed according to the connections in the tree while ignoring the directions.
}
The output $h_v^{(l)}$ of the $l$-th layer GCN for node $v$ are computed as:

\begin{equation}
    \mathbf{h}_v^{(l)} = \mbox{ReLU}(\sum_{u\in\mathcal{N}(v)}W^{(l)}\mathbf{h}_u^{(l-1)}+\mathbf{b}^{(l)})
\end{equation}
where $\mathcal{N}(v)$ refers to the set of all one-hop neighbours of node $v$.
$W\in\mathbb{R}^{d \times d}$ and $\mathbf{b}\in\mathbb{R}^{d}$ are the learnable weight matrix and bias term.
ReLU \citep{nair2010rectified} is the activation function.
\textcolor{black}{For the initial input $\mathbf{h}_{v_t}^{(0)}$ of terminal node $v_t$ (i.e., the token), we directly use the outputs of the Transformer encoder. To derive the initial input $\mathbf{h}_{v_{nt}}^{(0)}$ of non-terminal node $v_{nt}$ (i.e., the constituent), 
we randomly
initialize a non-terminal embedding matrix $E_{nt}^{n \times d}$, where $n$ means the number of constituent labels and $d$ is the embedding dimension. The above node initialization method is borrowed from \citet{DBLP:conf/naacl/XiaZWLZHSZ21}, and still leaves room for improvement. For example, the non-terminal embedding matrix can be pre-trained with existing treebanks like PTB \cite{marcinkiewicz1994building}.}




\textbf{Representation fusion.} We use the weighted-sum operation to fuse syntax-enhanced representation $\mathbf{h}^{syn}$ and the outputs of the basic Transformer encoder $\mathbf{h}^{basic}$ as the final representation $\mathbf{h}^{final}$:
\begin{equation}
    \mathbf{h}^{final}=\lambda \mathbf{h}^{syn} + (1-\lambda) \mathbf{h}^{basic}
\end{equation}
where $\lambda \in (0,1)$ is the fusion factor. $\mathbf{h}^{basic}$ will later be fed into the decoder for token predicting.

\input{figures/combing.tex}







