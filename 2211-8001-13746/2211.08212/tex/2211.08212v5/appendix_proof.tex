\section{Additional Proofs}\label{sec.addproofs}
\subsection{Proof of \cref{lemma.complexity of lanczos method}} We provide a sketch here as the results are the combination of the complexity estimates in \cite{jin_lecture_2021} and Lemma 9 in \cite{royer_complexity_2018}. Consider the positive semidefinite matrix $F_k^\prime: = \|F_k\|I - F_k$, and substituting $\epsilon:= \frac{e_k}{2\|F_k\|}$ into the complexity results in \cite{jin_lecture_2021}, the Lanczos method returns an estimate $\mu_{\max}(F_k^\prime)$ satisfies
\begin{align*}
    \mu_{\max}(F_k^\prime) \ge \left(1 - \frac{e_k}{2\|F_k\|}\right)\lambda_{\max}(F_k^\prime)
\end{align*}
if it starts with the vector $q_1$ and runs at most
\begin{equation*}
    1+2\sqrt{\frac{\|F_k\|}{e_k}}\log\left(\frac{16\|F_k\|}{e_k(q_1^T \chi_k)^2}\right)
\end{equation*}
iterations (gap-free version). Since $\mu_{\max}(F_k^\prime) = \|F_k\| - \gamma_k$ and $\lambda_{\max}(F_k^\prime) = \|F_k\| - \lambda_{\min}(F_k)$, following the same argument of Lemma 9 in \cite{royer_complexity_2018}, we obtain
\begin{equation*}
    \gamma_k \le \lambda_{\min}(F_k) + e_k.
\end{equation*}
The result of the gap-dependent version can be established similarly, and thus we omit it here.


\subsection{Proof of \cref{lem.convergence r sigma}}
We first show \eqref{eq.est.Fk.1}.
\begin{align*}
    \|F_k\|   & = \max_{\|[v;t]\| = 1} \begin{bmatrix}
                                           v \\ t
                                       \end{bmatrix}^T \begin{bmatrix}
                                                           H_k   & g_k     \\
                                                           g_k^T & -\delta
                                                       \end{bmatrix} \begin{bmatrix}
                                                                         v \\ t
                                                                     \end{bmatrix}                  \\
    \nonumber & \le \max_{\|[v;t]\| = 1} \begin{bmatrix}
                                             v \\ t
                                         \end{bmatrix}^T \begin{bmatrix}
                                                             H_k & 0       \\
                                                             0   & -\delta
                                                         \end{bmatrix} \begin{bmatrix}
                                                                           v \\ t
                                                                       \end{bmatrix} + \max_{\|[v;t]
    \| = 1} \begin{bmatrix}
                v \\ t
            \end{bmatrix}^T \begin{bmatrix}
                                0     & g_k \\
                                g_k^T & 0
                            \end{bmatrix} \begin{bmatrix}
                                              v \\ t
                                          \end{bmatrix}                                             \\
              & \le \max\{U_H, \delta\} + \|\gk\|,
\end{align*}
the proof is completed. For the second argument, denote $\omega_k = [r_k;\sigma_k]$, from \eqref{eq.inexact optimal condition}, we have
\begin{equation}\label{eq.regularity d}
    [v_k;t_k]^T F_k [v_k;t_k] +\gamma_k =0.
\end{equation}
We can write $[v_k;t_k] = \tau \chi_k+s$, where $\tau \in [0,1], s\perp \chi_k$. Since $[v_k;t_k]$ is a unit vector, we have
\begin{equation}
    \label{eq.unit d}
    \tau^2+\|s\|^2=1.
\end{equation}
Then from \eqref{eq.regularity d} we have
\begin{align}
    \nonumber  -\theta_k+e_k & \geq -\gamma_k  =-\theta_k \tau^2+s^T F_k s          \\
                             & \geq -\theta_k \tau^2+(-\theta_k+{\sigma_F})\|s\|^2.
\end{align}
The second equality is obtained by the fact $s\perp v_1$. It implies
\begin{equation}
    \label{eq.bound s final}
    \|s\|^2\leq \frac{e_k}{{\sigma_F}}.
\end{equation}
Thus from \eqref{eq.inexact optimal condition} we have
\begin{equation}
    \label{eq.bound r}
    \begin{aligned}
        \omega_k & = F_k [v_k;t_k]+\gamma_k [v_k;t_k]                   \\
                 & = (F_k+\gamma_k I)(\tau \chi_k +s)                   \\
                 & = \tau (\gamma_k-\theta_k)\chi_k +(F_k+\gamma_k I)s.
    \end{aligned}
\end{equation}
Hence, the norm of the residual follows
\begin{equation}
    \label{eq.bound r final}
    \begin{aligned}
        \|r_k\| \le \|\omega_k\| & \leq \tau(\theta_k-\gamma_k)+\|(F_k+\gamma_k I)s\|                             \\
                                 & \leq \tau e_k + \|(F_k+\gamma_k I)\|\sqrt{\frac{e_k}{{\sigma_F}}}              \\
                                 & \leq \tau e_k +2(\max\{U_H, \delta\} + \|\gk\|) \sqrt{\frac{e_k}{{\sigma_F}}}.
    \end{aligned}
\end{equation}
This completes the proof.


\subsection{Proof of \cref{lemma.gamma ge delta}}\label{sec.proof.gammagedelta}
For part (1), due to the mechanism of the Lanczos method, for any orthonormal basis $q_j = [\nu_j, \beta_j]$ with $j\ge 2$, we have $q_j \perp q_1$. Therefore, it holds that
\begin{align*}
    \beta_j \alpha & = - \nu_j^Tu \sqrt{1-\alpha^2},
\end{align*}
and thus
\begin{equation}\label{eq.bound.lastentry}
    |\beta_j| \le \frac{\sqrt{1-\alpha^2}\|v_j\|\|u\|}{|\alpha|} \le 2\sqrt{1-\alpha^2}.
\end{equation}

For part (2), denote $F_{n+1} = F_k e_{n+1}$ and $y = F_{n+1} - (F_{n+1}^Tq_1) \cdot q_1 - (F_{n+1}^Tq_2) \cdot q_2$, then $q_1,q_2,y$ are mutually orthogonal. Therefore, $y$ is the residual of $F_{n+1}$ after projecting on the subspace $\Pi$ spanned by $q_1,q_2$, it follows
\begin{align*}
    \|y\| = \|(I - \Pi) F_{n+1}\|.
\end{align*}
By denoting $\varphi := F_{n+1} - F_kq_1$, we conclude,
\begin{align*}
    \|y\| = \|(I - \Pi) F_{n+1}\| = \|(I - \Pi) (F_kq_1 + \varphi) \| = \|(I - \Pi)\varphi \| \le \|\varphi\| \\= \left\|\begin{bmatrix}
        (1-\alpha) \cdot g_k -\sqrt{1-\alpha^2} \cdot H_ku \\
        (1-\alpha) \cdot (-\delta) - \sqrt{1-\alpha^2} \cdot g_k^Tu
    \end{bmatrix}\right\|
\end{align*}
since $F_kq_1 \in \mathcal K(2;F_k,q_1)$ and $(I - \Pi)$ is nonexpansive. In this view, we have,
\begin{align}\label{eq.residual.by2projection}
    \|y\|^2 \le [(U_H + U_g)^2 + (\delta+U_g)^2] \cdot (1-\alpha^2)
\end{align}
as $1-\alpha \le \sqrt{1-\alpha^2}$ holds for $\alpha \in (0,1)$. Recall that for the Lanczos method, it holds that
\begin{equation*}
    F_k Q_j - Q_jT_j = \xi_j e_j^T,~Q_j = [q_1, \ldots, q_j] \in \mathbb{R}^{(n+1)\times j},~T_j \in \mathbb{R}^{j \times j}.
\end{equation*}
Consider the last term of the residual $\xi_j$, by $\xi_{j,n+1}$, for $j \ge 3$, it follows
\begin{align*}
    \xi_{j,n+1} & = e_{n+1}^T\xi_j = e_{n+1}^T\xi_je_j^Te_j                                  \\
                & = e_{n+1}^TF_kQ_je_j - e_{n+1}^TQ_jT_je_j                                  \\
                & = F_{n+1}^Tq_j - [\beta_1, ..., \beta_{j}] [0,...,0, T_{j-1,j}, T_{j,j}]^T \\
                & = F_{n+1}^Tq_j - \beta_{j-1}T_{j-1,j} - \beta_{j} T_{j,j}
\end{align*}
Since $q_j$ is perpendicular to $q_1$ and $q_2$, we have $q_j^TF_{n+1} = q_j^Ty$, and thus
\begin{align*}
    |\xi_{j,n+1}| & = |F_{n+1}^Ty - \beta_{j-1}T_{j-1,j} - \beta_{j} T_{j,j}|                                   \\
                  & \le \|F_{n+1}\| \cdot \|y\| +  |T_{j-1,j}| \cdot |\beta_{j-1}| +  |T_{j,j}| \cdot |\beta_j| \\
                  & \le O(\sqrt{1-\alpha^2})
\end{align*}
where the last inequality follows from \eqref{eq.bound.lastentry} and \eqref{eq.residual.by2projection}.
By the fact of Ritz approximation (Section 10.1.4 in \cite{golub2013matrix}), we conclude
$$|\sigma_k| \le |\xi_{j,n+1}| \le O(\sqrt{1-\alpha^2}).$$


For part (3), from the shift-invariant property of the Krylov subspace, we have
\begin{equation*}
    \mathcal{K}(j;U_F I_{n+1} - F_k) = \mathcal{K}(j;F_k) := \left\{q_1, F_kq_1, \ldots, F_k^j q_1\right\}.
\end{equation*}
Since the Ritz value $-\gamma_k$ is generated in a larger Krylov subspace, $-\gamma_k \le q_1^T F_k q_1$ holds. Therefore, it is sufficient to establish that $q_1^T F_k q_1 \le -\delta$. The selection \eqref{eq.safeguard.gammadelta} implies
\begin{equation*}
    (U_H + \delta)^2 \cdot (1-\alpha^2) \le 4(g^Tu)^2 \cdot \alpha^2,
\end{equation*}
and thus it follows
\begin{align*}
    \begin{bmatrix}
        \sqrt{1-\alpha^2} u \\
        \alpha
    \end{bmatrix}^T
    \begin{bmatrix}
        H_k   & g_k     \\
        g_k^T & -\delta
    \end{bmatrix}
    \begin{bmatrix}
        \sqrt{1-\alpha^2} u \\
        \alpha
    \end{bmatrix}
     & = -\delta \cdot \alpha^2 + 2\alpha\sqrt{1-\alpha^2} \cdot g^Tu + (1-\alpha^2) \cdot u^THu   \\
     & \le -\delta \cdot \alpha^2 - (1 - \alpha^2) \cdot (U_H + \delta) + (1 - \alpha^2) \cdot U_H \\
     & \le -\delta.
\end{align*}


\subsection{Proof of \cref{lemma.customized initialization}}\label{sec.proof.customized}

We first show the first inequality. Note that for the randomized initialization, we have
\begin{equation*}
    |\alpha|^2 = \frac{\Psi^2 x_{n+1}^2}{\sum_{i=1}^n x_i^2 + \Psi^2 x_{n+1}^2}.
\end{equation*}
To ensure $\alpha^2 \ge \Omega(1-\epsilon^4)$, it suffices to set $\Psi$ satisfying
\begin{equation*}
    \frac{\Psi^2x_{n+1}^2}{\sum_{i=1}^n x_i^2}  \ge \Omega(\epsilon^{-4}).
\end{equation*}
Note that with a probability at least of $1 - p$, it holds
\begin{equation*}
    \Psi^2 x_{n+1}^2 \ge \frac{\pi}{2}\Psi^2p^2  > 0.
\end{equation*}
Similarly, $\sum_{i=1}^n x_i^2$ follows the chi-square distribution of with  $n$ degrees of freedom, with a probability at least of $1 - e^{-n}$, we conclude
\begin{equation*}
    \sum_{i=1}^n x_i^2 \le 5n.
\end{equation*}
Therefore, choosing
\begin{equation*}
    \Psi \ge \Omega\left(\frac{\sqrt{n}}{p\epsilon^2}\right)
\end{equation*}
completes the proof.


We first note that for any given $x \ge 0$, it holds that
$$
    1-\exp \left(-4 x^2 / \pi\right) \geq \operatorname{erf}(x)^2,
$$
where $\operatorname{erf}(x)=\frac{2}{\sqrt{\pi}} \int_0^x e^{-t^2} d t$. It implies that for any random variable $X \sim \mathcal{N}(0,1)$, we have
\begin{equation*}
    \operatorname{Prob}(|X| \le x) = \operatorname{erf}(x/\sqrt{2}) \le \sqrt{1-\exp \left(-2 x^2 / \pi\right)}.
\end{equation*}
Note that
\begin{equation*}
    (v^T q_1)^2 = \frac{(v^T b)^2}{\|b\|^2} = \frac{(\sum_{i=1}^n v_i x_i + \alpha v_{n+1} x_{n+1})^2}{\sum_{i=1}^n x_i^2 + \alpha^2 x_{n+1}^2},
\end{equation*}
and it is sufficient to analyze the two terms $(v^T b)^2$ and $\|b\|^2$, separately.

\textbf{Lower bound of $(q_1^Tv)^2$.}
Since $v \neq 0$, w.l.o.g., we assume $\|v\| = 1$.
\begin{itemize}
    \item For the case $\exists v_i \neq 0$ for some $i = 1, \cdots, n$, it follows
          \begin{equation*}
              (\sum_{i=1}^n v_i x_i + \alpha v_{n+1} x_{n+1})^2 \ge (\sum_{i=1}^n v_i x_i)^2
          \end{equation*}
          due to the fact that we can set $\alpha |v_{n+1} x_{n+1}| \gg |\sum_{i=1}^n v_i x_i|$. Recall $x_1, \cdots, x_{n} \overset{i.i.d.}{\sim} \mathcal{N}(0,1)$, we have
          \begin{equation*}
              \sum_{i=1}^n v_i x_i \sim \mathcal{N}(0, \sum_{i=1}^n v_i^2),
          \end{equation*}
          and it further implies
          \begin{align*}
                & \operatorname{Prob}\left(\left|\sum_{i=1}^n v_i x_i\right| \le p\sqrt{\frac{\pi\sum_i^n v_i^2}{2}}\right)                                                               = \operatorname{Prob}\left(\left|\frac{\sum_{i=1}^n v_i x_i}{\sqrt{\sum_{i=1}^n v_i^2}}\right| \le p\sqrt{\frac{\pi}{2}}\right) \le \sqrt{1-\operatorname{exp}\left(-p^2\right)} \le p.
          \end{align*}
          Therefore, with a probability at least of $1 - p$, we conclude
          \begin{equation*}
              (v^T b)^2 \ge \sqrt{\frac{\pi\sum_i^n v_i^2}{2}}p > 0.
          \end{equation*}
    \item For the case $v_1 = \cdots = v_n = 0$, we have $(v^T b)^2 = \Psi^2 x_{n+1}^2.$
          Recall $x_{n+1} \sim \mathcal{N}(0,1)$, it follows that
          \begin{align*}
                & \operatorname{Prob}\left(x_{n+1}^2 \le \frac{\pi}{2}p^2\right) =  \operatorname{Prob}\left(\left|x_{n+1}\right| \le p\sqrt{\frac{\pi}{2}}\right) \le p.
          \end{align*}
          Similarly, with a probability at least of $1 - p$, we conclude $(v^T b)^2 \ge \frac{\pi}{2} \Psi^2 p^2 > 0$.
\end{itemize}
Combining the above two cases, it holds that
\begin{equation*}
    (v^T b)^2 \ge \min\left\{\sqrt{\frac{\pi\sum_i^n v_i^2}{2}}p,  \frac{\pi}{2} \Psi^2 p^2\right\} > 0
\end{equation*}
with a probability of at least $1-p$.

\textbf{Upper bound of $\|b\|^2$.} First note that
\begin{align*}
    \|b\|^2 = & \sum_{i=1}^n x_i^2 + \Psi^2 x_{n+1}^2    \\
    \le       & \Psi^2 \cdot \sum_{i=1}^{n+1} x_{n+1}^2.
\end{align*}
Since the random variable $\sum_{i=1}^{n+1} x_{n+1}^2$ follows the chi-square distribution of with  $n+1$ degrees of freedom, the tail bound in \cite{laurent2000adaptive} implies that
\begin{equation*}
    \operatorname{Prob}\left(\sum_{i=1}^{n+1} x_{n+1}^2 \ge 5(n +1) \right) \le e^{-(n+1)}.
\end{equation*}
Therefore, with a probability at least of $1 - e^{-(n+1)}$, it holds that
\begin{equation*}
    \|b\|^2 \le 5\Psi^2(n + 1).
\end{equation*}
Finally, can guarantee
\begin{equation*}
    (v^T q_1)^2 = \frac{(v_1^T b)^2}{\|b\|^2} \ge \min\left\{\Omega\left(\frac{p}{\Psi^2 n}\right), \Omega\left(\frac{p^2}{n}\right)\right\}
\end{equation*}
with a probability at least $\min\left\{1-p, 1 - e^{-(n+1)}\right\}$.

