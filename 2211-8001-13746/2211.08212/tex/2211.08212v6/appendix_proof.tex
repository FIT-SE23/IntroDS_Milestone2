\section{Additional Proofs}\label{sec.addproofs}
\subsection{Proof of \cref{lemma.complexity of lanczos method}} We provide a sketch here as the results are the combination of the complexity estimates in \cite{jin_lecture_2021} and Lemma 9 in \cite{royer_complexity_2018}. Consider the positive semidefinite matrix $F_k^\prime: = \|F_k\|I - F_k$, and substituting $\epsilon:= \frac{e_k}{2\|F_k\|}$ into the complexity results in \cite{jin_lecture_2021}, the Lanczos method returns an estimate $\gamma_{\max}(F_k^\prime)$ satisfies
\begin{align*}
    \gamma_{\max}(F_k^\prime) \ge \left(1 - \frac{e_k}{2\|F_k\|}\right)\lambda_{\max}(F_k^\prime)
\end{align*}
if it starts with the vector $q_1$ and runs at most
\begin{equation*}
    1+2\sqrt{\frac{\|F_k\|}{e_k}}\log\left(\frac{16\|F_k\|}{e_k(q_1^T \chi_k)^2}\right)
\end{equation*}
iterations (gap-free version). Since $\gamma_{\max}(F_k^\prime) = \|F_k\| - \gamma_k$ and $\lambda_{\max}(F_k^\prime) = \|F_k\| - \lambda_1(F_k)$, following the same argument of Lemma 9 in \cite{royer_complexity_2018}, we obtain
\begin{equation*}
    \gamma_k \le \lambda_1(F_k) + e_k.
\end{equation*}
The result of the gap-dependent version can be established similarly, and thus we omit it here.
$\qed$

\subsection{Proof of \cref{lem.convergence r sigma}}
For the first statement, note that
\begin{align*}
    \|F_k\|   & = \max_{\|[v;t]\| = 1} \begin{bmatrix}
                                           v \\ t
                                       \end{bmatrix}^T \begin{bmatrix}
                                                           H_k   & g_k     \\
                                                           g_k^T & -\delta
                                                       \end{bmatrix} \begin{bmatrix}
                                                                         v \\ t
                                                                     \end{bmatrix}                  \\
    \nonumber & \le \max_{\|[v;t]\| = 1} \begin{bmatrix}
                                             v \\ t
                                         \end{bmatrix}^T \begin{bmatrix}
                                                             H_k & 0       \\
                                                             0   & -\delta
                                                         \end{bmatrix} \begin{bmatrix}
                                                                           v \\ t
                                                                       \end{bmatrix} + \max_{\|[v;t]
    \| = 1} \begin{bmatrix}
                v \\ t
            \end{bmatrix}^T \begin{bmatrix}
                                0     & g_k \\
                                g_k^T & 0
                            \end{bmatrix} \begin{bmatrix}
                                              v \\ t
                                          \end{bmatrix}                                             \\
              & \le \max\{U_H, \delta\} + \|\gk\|,
\end{align*}
which completes the proof. For the second argument, multiplying $[v_k;t_k]$ on both sides of \eqref{eq.inexact optimal condition} yields
\begin{equation}\label{eq.regularity d}
    [v_k;t_k]^T F_k [v_k;t_k] +\gamma_k =0.
\end{equation}
Since $[v_k;t_k]$ is a unit vector, we can rewrite $[v_k;t_k] = \tau_k \cdot \chi_k+s$ for some $\tau_k \in [0,1]$and $s \perp \chi_k$ satisfying $\tau_k^2+\|s\|^2=1$. Substituting into \eqref{eq.regularity d} gives
\begin{align}
    \nonumber  -\theta_k+e_k & \geq -\gamma_k  =-\theta_k \tau_k^2+s^T F_k s        \notag      \\
                             & \geq -\theta_k \tau_k^2+(-\theta_k+{\varsigma_k})\|s\|^2, \notag
\end{align}
where the equality is obtained by the fact $s\perp \chi_k$. It implies
\begin{equation}
    \label{eq.bound s final}
    \|s\|^2\leq \frac{e_k}{{\varsigma_k}}.
\end{equation}
Thus from \eqref{eq.inexact optimal condition} we have
\begin{equation}
    \label{eq.bound r}
    \begin{aligned}
        [r_k;\sigma_k] & = F_k [v_k;t_k]+\gamma_k [v_k;t_k]                     \\
                       & = (F_k+\gamma_k I)(\tau_k \chi_k +s)                   \\
                       & = \tau_k (\gamma_k-\theta_k)\chi_k +(F_k+\gamma_k I)s.
    \end{aligned}
\end{equation}
Hence, the norm of the residual follows
\begin{equation}
    \label{eq.bound r final}
    \begin{aligned}
        \|r_k\| & \le \|[r_k;\sigma_k]\|                                                              \\ & \leq \tau_k(\theta_k-\gamma_k)+\|(F_k+\gamma_k I)s\|                             \\
                & \leq \tau_k e_k + \|(F_k+\gamma_k I)\|\sqrt{\frac{e_k}{{\varsigma_k}}}              \\
                & \leq \tau_k e_k +2(\max\{U_H, \delta\} + \|\gk\|) \sqrt{\frac{e_k}{{\varsigma_k}}}.
    \end{aligned}
\end{equation}
This completes the proof.
$\qed$

\subsection{Proof of \cref{thm.gamma ge delta}}\label{sec.proof.gammagedelta}
For part (1), due to the mechanism of the Lanczos method, for any orthonormal basis $q_j = [\ell_j; \beta_j]$ with $j\ge 2$, we have $q_j \perp q_1$. Therefore, it holds that
\begin{align*}
    \beta_j \alpha & = - \ell_j^Tu \sqrt{1-\alpha^2},
\end{align*}
and it implies
\begin{equation}\label{eq.bound.lastentry}
    |\beta_j| \le \frac{\sqrt{1-\alpha^2}\|\ell_j\|\|u\|}{|\alpha|} \le 2\sqrt{1-\alpha^2}
\end{equation}
for any $|\alpha| \ge 1/2$.

For part (2), denote $\zeta_{n+1} = F_k 1_{n+1}$ and $y = \zeta_{n+1} - (\zeta_{n+1}^Tq_1) \cdot q_1 - (\zeta_{n+1}^Tq_2) \cdot q_2$, then $q_1,q_2,y$ are mutually orthogonal. Therefore, let $\Pi$ be the projection matrix onto the subspace spanned by $q_1, q_2$, then $y$ is the residual of $\zeta_{n+1}$ after projecting on this subspace, and it follows
\begin{align*}
    \|y\| = \|(I_{n+1} - \Pi) \zeta_{n+1}\|.
\end{align*}
By denoting $\varphi := \zeta_{n+1} - F_kq_1$, we conclude,
\begin{align*}
    \|y\| = \|(I_{n+1} - \Pi) \zeta_{n+1}\| = \|(I_{n+1} - \Pi) (F_kq_1 + \varphi) \| = \|(I_{n+1} - \Pi)\varphi \| \le \|\varphi\| \\= \left\|\begin{bmatrix}
        (1-\alpha) \cdot g_k -\sqrt{1-\alpha^2} \cdot H_ku \\
        (1-\alpha) \cdot (-\delta) - \sqrt{1-\alpha^2} \cdot g_k^Tu
    \end{bmatrix}\right\|
\end{align*}
since $F_kq_1 \in \mathcal K(2;F_k,q_1)$ and $\|I_{n+1} - \Pi\| = 1$.
    {In this view, we have,
        \begin{align}\label{eq.residual.by2projection}
            \|y\|^2 \le ((1-\alpha^2)(\|\Hk\| + \|\gk\|)^2 + (1-\alpha)^2(\delta+\|\gk\|)^2) \le ((U_H + U_g)^2 + (\delta+U_g)^2) \cdot (1-\alpha^2)
        \end{align}
    }
as $1-\alpha \le \sqrt{1-\alpha^2}$ holds for $\alpha \in (0,1)$. Recall that for the Lanczos method, it holds that
\begin{equation*}
    F_k Q_j - Q_jT_j = \xi_j 1_j^T,~Q_j = [q_1, \ldots, q_j] \in \mathbb{R}^{(n+1)\times j},~T_j \in \mathbb{R}^{j \times j}.
\end{equation*}
Consider the last term of the residual $\xi_j$, by $\xi_{j,n+1}$, for $j \ge 3$, it follows
\begin{align*}
    \xi_{j,n+1} & = 1_{n+1}^T\xi_j = 1_{n+1}^T\xi_j1_j^T1_j                                      \\
                & = 1_{n+1}^TF_kQ_j1_j - 1_{n+1}^TQ_jT_j1_j                                      \\
                & = \zeta_{n+1}^Tq_j - [\beta_1, ..., \beta_{j}] [0,...,0, T_{j-1,j}, T_{j,j}]^T \\
                & = \zeta_{n+1}^Tq_j - \beta_{j-1}T_{j-1,j} - \beta_{j} T_{j,j}
\end{align*}
Since $q_j$ is perpendicular to $q_1$ and $q_2$, we have $q_j^T\zeta_{n+1} = q_j^Ty$, and thus
\begin{subequations}
    \begin{align}
        \nonumber |\xi_{j,n+1}| & = |\zeta_{n+1}^Ty - \beta_{j-1}T_{j-1,j} - \beta_{j} T_{j,j}|                                                \\
        \nonumber               & \le \|\zeta_{n+1}\| \cdot \|y\| +  |T_{j-1,j}| \cdot |\beta_{j-1}| +  |T_{j,j}| \cdot |\beta_j|              \\
        \label{eq.bd.1}         & \le \sqrt{1-\alpha^2}\left(\sqrt{(U_H + U_g)^2 + (\delta+U_g)^2}\sqrt{U_g^2+\delta^2} + 4\|T\|_\infty\right) \\
        \label{eq.bd.2}         & \le \sqrt{1-\alpha^2}U_\sigma
        = O(\sqrt{1-\alpha^2})
    \end{align}
\end{subequations}
where \eqref{eq.bd.1} follows from \eqref{eq.bound.lastentry} and \eqref{eq.residual.by2projection}.
The last inequality \eqref{eq.bd.2} follows from the fact that $\|T\|_\infty \le \sqrt{n}\|T\|\le \|F_k\|$ since the spectra of $T$ is bounded by that of $F_k$ (see, e.g., \cite[Theorem 10.1.2]{golubMatrixComputations2013}). By taking $U_\sigma := \sqrt{(U_H + U_g)^2 + (\delta+U_g)^2}\sqrt{U_g^2+\delta^2} + 4\sqrt{n}(U_g + \max\{U_H, \delta\})$, and by the fact of Ritz approximation (Section 10.1.4 in \cite{golubMatrixComputations2013}), we conclude
$$|\sigma_k| \le |\xi_{j,n+1}| \le \sqrt{1-\alpha^2}U_\sigma.$$


For part (3), from the shift-invariant property of the Krylov subspace, we have
\begin{equation*}
    \mathcal{K}(j;U_F I_{n+1} - F_k) = \mathcal{K}(j;F_k) := \left\{q_1, F_kq_1, \ldots, F_k^j q_1\right\}.
\end{equation*}
Since the Ritz value $-\gamma_k$ is generated in a larger Krylov subspace, $-\gamma_k \le q_1^T F_k q_1$ holds. Therefore, it is sufficient to establish that $q_1^T F_k q_1 \le -\delta$. The selection \eqref{eq.safeguard.gammadelta} implies
\begin{equation*}
    (U_H + \delta)^2 \cdot (1-\alpha^2) \le 4(\gk^Tu)^2 \cdot \alpha^2,
\end{equation*}
and thus, it follows
\begin{align*}
    \begin{bmatrix}
        \sqrt{1-\alpha^2} u \\
        \alpha
    \end{bmatrix}^T
    \begin{bmatrix}
        H_k   & g_k     \\
        g_k^T & -\delta
    \end{bmatrix}
    \begin{bmatrix}
        \sqrt{1-\alpha^2} u \\
        \alpha
    \end{bmatrix}
     & = -\delta \cdot \alpha^2 + 2\alpha\sqrt{1-\alpha^2} \cdot \gk^Tu + (1-\alpha^2) \cdot u^THu \\
     & \le -\delta \cdot \alpha^2 - (1 - \alpha^2) \cdot (U_H + \delta) + (1 - \alpha^2) \cdot U_H \\
     & \le -\delta.
\end{align*}


$\qed$

\subsection{Proof of \cref{thm.customized initialization}}
\label{sec.proof.customized}
We first provide a useful inequality. For any given constant $x \ge 0$, it holds that
$$
    1-\exp \left(-4 x^2 / \pi\right) \geq \operatorname{erf}(x)^2,
$$
where $\operatorname{erf}(x)=\frac{2}{\sqrt{\pi}} \int_0^x e^{-t^2} d t$. It implies that for any random variable $X \sim \mathcal{N}(0,1)$, we have
\begin{equation*}
    \operatorname{Prob}(|X| \le x) = \operatorname{erf}(x/\sqrt{2}) \le \sqrt{1-\exp \left(-2 x^2 / \pi\right)}.
\end{equation*}
Now we begin with establishing a lower bound of $q_1^T \chi_k$. Since
\begin{equation*}
    (q_1^T \chi_k)^2 = \frac{(\chi_k^T b)^2}{\|b\|^2} = \frac{(\sum_{i=1}^n \chi_{k, i} b_i + \Psi_k \cdot \chi_{k, n+1} b_{n+1})^2}{\sum_{i=1}^n b_i^2 + \Psi_k^2 b_{n+1}^2},
\end{equation*}
it is sufficient to provide a lower bound of $(\chi_k^T b)^2$ and an upper bound of $\|b\|^2$, respectively. For the term $(\chi_k^T b)^2$, recall $b_1, \ldots, b_{n+1} \overset{\textrm{i.i.d.}}{\sim} \mathcal{N}(0,1)$, then it holds that $\sum_{i=1}^n \chi_{k, i} b_i +  \Psi_k \cdot \chi_{k, n+1} b_{n+1} \sim \mathcal{N}(0, \sum_{i=1}^n \chi_{k, i}^2 + \Psi_k^2 \chi_{k, n+1}^2)$. Hence, we have
\begin{equation}\label{eq.bound gaussian}
    \begin{aligned}
              & \operatorname{Prob}\left(\left|\chi_k^T b\right| \le p\sqrt{\frac{\pi\left(\sum_{i=1}^n \chi_{k,i}^2 + \Psi_k^2 \chi_{k, n+1}^2\right)}{2}}\right)    \\
        = ~   & \operatorname{Prob}\left(\left|\frac{\chi_k^T b}{\sqrt{\sum_{i=1}^n \chi_{k,i}^2 + \Psi_k^2 \chi_{k, n+1}^2}}\right| \le p\sqrt{\frac{\pi}{2}}\right) \\
        \le ~ & \sqrt{1-\operatorname{exp}\left(-p^2\right)} \le p
    \end{aligned}
\end{equation}
for any constant $0 < p < 1$. Consequently, with a probability of at least $1 - p$, we conclude
\begin{equation*}
    (\chi_k^T b)^2 \ge \frac{p^2 \pi\left(\sum_{i=1}^n\chi_{k,i}^2 + \Psi_k^2 \chi_{k, n+1}^2\right)}{2}.
\end{equation*}
Then we consider the upper bound of $\|b\|^2$. Note that $\|b\|^2 \le \Psi_k^2 \cdot \sum_{i=1}^{n+1} b_{n+1}^2$, and $\sum_{i=1}^{n+1} b_{n+1}^2$ follows the chi-square distribution with $n+1$ degrees of freedom. Applying the tail bound (Lemma 1 in \cite{laurent2000adaptive}) gives that
\begin{equation*}
    \operatorname{Prob}\left(\sum_{i=1}^{n+1} b_{n+1}^2 \ge 5(n +1) \right) \le \exp(-(n+1)).
\end{equation*}
Hence, it holds that $\|b\|^2 \le 5\Psi_k^2(n + 1)$ with a probability of at least $1 - \exp(-(n+1))$. Therefore, {by applying the union bound, we conclude
        \begin{equation}\label{eq.lower of inner w.r.t. Psi}
            (q_1^T \chi_k)^2 = \frac{(\chi_k^T b)^2}{\|b\|^2} \ge \frac{\pi p^2 \sum_{i=1}^n\chi_{k,i}^2}{10\Psi_k^2(n + 1)} + \frac{\pi p^2 \chi_{k, n+1}^2}{10(n + 1)}
        \end{equation}
        with a probability of at least $1 - p - \exp(-(n+1))$.}

Now we justify the relationship between $\Psi_k$ and accuracy $\epsilon$. Motivated by \cref{thm.gamma ge delta}, we consider the following condition
\begin{equation}\label{eq.u1-a}
    1 - \alpha^2 = \frac{\sum_{i=1}^n b_i^2}{\sum_{i=1}^n b_i^2 + \Psi_k^2 b_{n+1}^2} \le \min\left\{\frac{\epsilon^4}{256M^4U^2_\sigma}, \left({1+\frac{(U_H+\delta)^2 }{2 p^2 \pi \|\gk\|^2}}\right)^{-1}, \frac{3}{4}\right\}.
\end{equation}
To guarantee \eqref{eq.u1-a}, it suffices to choose $\Psi_k$ such that
\begin{equation*}
    \frac{ \Psi_k^2 b_{n+1}^2}{\sum_{i=1}^n b_i^2} \ge \max\left\{\frac{256M^4U^2_\sigma}{\epsilon^4},1+\frac{(U_H+\delta)^2 }{2 p^2 \pi \|\gk\|^2}, \frac{4}{3}\right\}.
\end{equation*}
{Since $\sum_{i=1}^n b_i^2$ follows the chi-square distribution with $n$ degrees of freedom and $b_{n+1} \sim \mathcal{N}(0, 1)$, from a similar argument of \eqref{eq.lower of inner w.r.t. Psi}, we see that
\begin{equation}\label{eq.larger.p}
    \Psi_k^2 b_{n+1}^2 \ge \frac{\Psi_k^2p^2\pi}{2} \quad\textrm{and}\quad \sum_{i=1}^n b_i^2 \le 5n
\end{equation}
with a probability at least of $1 - \exp(-n)$. Therefore, choosing
\begin{equation*}
    \Psi_k = \frac{\sqrt{10n}}{\sqrt{\pi}p}\cdot\max\left\{\frac{16M^2U_\sigma}{\epsilon^2}, \sqrt{1+\frac{(U_H+\delta)^2 }{2 p^2 \pi \|\gk\|^2}}, \frac{2}{\sqrt{3}}\right\}
\end{equation*}
guarantees that \eqref{eq.u1-a} holds with a probability at least of $1 - \exp(-n)$.} Substituting the choice of $\Psi_k$ into \eqref{eq.lower of inner w.r.t. Psi} gives
\begin{equation}\label{eq. lower of inner final}
    (q_1^T \chi_k)^2
    \ge \min\left\{\frac{\epsilon^4}{256M^4U^2_\sigma}, \left({1+\frac{(U_H+\delta)^2 }{2 p^2 \pi \|\gk\|^2}}\right)^{-1}, \frac{3}{4}\right\} \cdot \frac{\pi^2 p^4 \sum_{i=1}^n\chi_{k,i}^2}{100n(n + 1)} + \frac{p^2\pi \chi_{k, n+1}^2}{10(n + 1)}.
\end{equation}
Combining \eqref{eq.safeguard.sigma} of Theorem \ref{thm.gamma ge delta}, we have
\begin{equation}\label{eq.upper bound of sigma}
    |\sigma_k| \le U_\sigma\sqrt{1-\alpha^2} \le \frac{\epsilon^2}{16M^2}.
\end{equation}
Finally, by the middle term in \eqref{eq.u1-a}, we have $
    \alpha^2 \ge \frac{(U_H+\delta)^2}{(U_H+\delta)^2 + 2 p^2 \pi \|\gk\|^2}.
$
Since $\gk^Tb_{[1:n]} \sim \mathcal{N}(0, \|g_k\|^2)$, from a similar argument of \eqref{eq.bound gaussian}, it holds that
\begin{equation}\label{eq.bound gb}
    (\gk^Tb_{[1:n]})^2 \ge \frac{p^2\pi\|g_k\|^2}{2}
\end{equation}
with a probability at least of $1-\exp(-n)$, implying $\alpha^2 \ge \frac{(U_H+\delta)^2}{(U_H+\delta)^2 + 4(\gk^Tb_{[1:n]})^2}$. {Recall that \eqref{eq. lower of inner final} holds with a probability at least of $1 - p - \exp(-n) - \exp(-(n+1))$ due to the union bound. Choosing $p \in (\exp(-n), 1)$ guarantees the inequalities \eqref{eq. lower of inner final} and \eqref{eq.bound gb} hold with a probability at least of $1-4p$.}
This completes the proof.

$\qed$


