\documentclass{article}
\input{header}
\begin{document}

\maketitle
\begin{abstract}
    In this paper, we introduce a \emph{Homogeneous Second-Order Descent Method} (HSODM) using the homogenized quadratic approximation to the original function. The merit of homogenization is that
    only the leftmost eigenvector of a gradient-Hessian integrated matrix is computed at each iteration. Therefore, the algorithm is a single-loop method that does not need to switch to other sophisticated algorithms, and is easy to be implemented. We show that
    HSODM has a global convergence rate of $O(\epsilon^{-3/2})$ to find an $\epsilon$-approximate second-order stationary point, and has a local quadratic convergence rate under the standard assumptions. The numerical results demonstrate the advantage of the proposed method over other second-order methods.
    % If the negative curvature is calculated via the random starting Lanczos method, then it matches the ``dimension-free'' complexity bound of a set of recent first-order methods.
\end{abstract}
\section{Introduction}
In this paper, we consider the following unconstrained optimization problem
\begin{equation}\label{Prob:main}
    \min_{x\in \mathbb{R}^n} f(x),
\end{equation}
where \(f:\mathbb{R}^n \mapsto \mathbb{R}\) is
twice continuously differentiable and \( f_{\inf}:=\inf f(x) > -\infty\).
Allowing a given tolerance $\epsilon>0$, we aim to find $x\in\mathbb{R}^n$ that is an $\epsilon$-approximate second-order stationary point satisfying:
\begin{subequations}
    \begin{align}
        \label{eq.approxfocp} & \|\nabla f(x)\|\leq O(\epsilon)                                          \\
        \label{eq.approxsocp} & \lambda_{\min} \left(\nabla^2 f(x)\right) \geq \Omega(-\sqrt{\epsilon}),
    \end{align}
\end{subequations}
where $\lambda_{\min}(A)$ denotes the smallest eigenvalue of $A$.
Since problem \eqref{Prob:main} is fundamental in the optimization community,
its complexity attracts significant attention from researchers.
When $f$ is nonconvex, it has been shown that the gradient descent method (GD) finds an $\epsilon$-approximate first-order stationary point satisfying \eqref{eq.approxfocp} in $O(\epsilon^{-2})$ iterations under the standard $L$-Lipschitz continuous gradient condition.

If the solution returned by the algorithm is required to satisfy the second-order necessary condition \eqref{eq.approxsocp}, one may resort to some variant of Newton's method \cite{conn_trust_2000} by leveraging the second-order information of the objective function. These methods are often referred to as second-order methods, and usually construct an iterate by minimizing the quadratic model as an approximation to the original function at the last iteration point $\xk$:
\begin{equation}\label{eq.quad model}
    m_k(d):=\gk^T d+\frac{1}{2}d^T \Hk d,
\end{equation}
where $\gk = \nabla f(\xk) $ and $ \Hk = \nabla^2 f(\xk)$.
In the nonconvex case, despite the excellent performance in practice, \citet{cartis_complexity_2010} show that Newton's method, perhaps surprisingly, has a worst-case complexity of $O(\epsilon^{-2})$ similar to that of the gradient method.
Therefore, some globalization techniques are needed to improve the convergence performance of the algorithm.
For example, \citet{nesterov_cubic_2006} introduce the cubic regularized (CR) subproblem:
\begin{equation}\label{eq.arc}
    \dk^\mathsf{CR} = \arg\min m^{\mathsf{CR}}_k(d):=\gk^T d+\frac{1}{2}d^T \Hk d + \frac{\sigma_k}{3}\|d\|^3, \sigma_k > 0.
\end{equation}
They show that the cubic regularized Newton method has the complexity of $O(\epsilon^{-3/2})$. Later, \citet{cartis_adaptive_2011,cartis_adaptive_2011-1} introduces an adaptive and inexact version of cubic regularization (ARC) with the same iteration complexity.
Except for cubic regularization, the classical trust-region method (TR) is based on the following subproblem and acceptance ratio \cite[]{conn_trust_2000}:
\begin{equation}\label{eq.tr}
    \begin{aligned}
        \dk^\mathsf{TR} ~ & =\arg \min_{\|d\|\le \Delta_k} m_k(d)              \\
        \rho_k ~          & = \frac{f(x_k + \dk) - f(x_k)}{m_k(\dk) - m_k(0)}.
    \end{aligned}
\end{equation}
However, establishing the improved iteration bound of $O(\epsilon^{-3/2})$ is more challenging for TR.
To our best knowledge, one result in \cite{ye_second_2005} by Ye is the only early work that proves the bound of $O(\epsilon^{-3/2})$ for TR
with a fixed radius strategy.
Recently, \citet{curtis_trust_2017} point out that the TR fails in reducing the function value measured in the norm of the step (or equivalently of the gradient) by classical $\rho_k$-based acceptance rule and linearly updated radius.
To overcome this issue, they propose an algorithm named TRACE \cite[]{curtis_trust_2017,curtis_worst-case_2022}, which has an $O(\epsilon^{-3/2})$ iteration complexity. However, this algorithm has a sophisticated rule of expanding and contracting the trust-region radius $\Delta_k$ due to the nonlinearity between $\|\dk^\mathsf{TR}\|$ and the dual variable.
\citet{royer_complexity_2018} use a line-search Newton framework and obtains a similar complexity. Their algorithm alternates between Newton and regularized Newton steps based on the smallest eigenvalue of the Hessian $\Hk$, and the stepsize is chosen under a similar acceptance rule used in \cite{cartis_adaptive_2011,curtis_trust_2017}. Since all these methods solve Newton systems, they inevitably involve expensive operations in subproblems at the cost of $O(n^3)$.

Therefore a natural question raises:

\textit{
    Does there exist a simple second-order framework with a
    less ``dimension-dependent'' per-iteration cost
    while maintaining similar iteration bound to the state-of-the-art second-order methods?
}

There is indeed a popular approach that can find a direction at the cost of $O\left(n^2\cdot\epsilon^{-1/4}\log({n/p})\right)$ using a randomized Lanczos method \cite{kuczynski_estimating_1992}, where $p$ refers to the probability such that the direction can be found with probability $1-p$. The obtained direction, say $\xi_k$, makes
the Rayleigh quotient $\mathcal R_k(\xi_k)$ negative, i.e.,
\begin{equation}\label{eq.nc.suff}
    \mathcal R_k(\xi_k) := \frac{\xi_k^T\Hk \xi_k}{\|\xi_k\|^2} \le -\sqrt{\epsilon},
\end{equation}
when the Hessian $\Hk \prec -\sqrt{\epsilon} \cdot I$.
Moreover,
using this direction with a proper stepsize $\eta$, the function value must decrease by $O(\epsilon^{3/2})$ under the second-order Lipschitz continuity condition. This nice property is widely used in the negative-curvature-based first-order methods \cite{carmon_accelerated_2018,jin_how_2017}.
However, if \eqref{eq.nc.suff} is not satisfied, one must switch to other options \cite{carmon_accelerated_2018,agarwal_finding_2017,jin_how_2017,royer_complexity_2018}, which complicates the iteration procedure and thus is hard for efficient implementation and parameter tuning.

\subsection{Our contribution}
In this paper, we give an affirmative answer to the above question by using the homogenized quadratic model and propose a new second-order method, see \autoref{alg.main alg}.

Firstly, we apply the homogenization trick to the quadratic model $m_k(d)$ that is widely used in quadratic programming and semidefinite relaxations \cite{sturm_cones_2003,ye_new_2003, he_quaternion_2022}. Then, the resulting problem is essentially a eigenvalue problem, and can be solved by 
the random starting Lanczos algorithm \cite{kuczynski_estimating_1992}, which gives a less dimension-dependent complexity of $O(n^2\cdot \epsilon^{-1/4}\log({(n+1)/p}))$. 
We demonstrate that the leftmost eigenvalue of the homogenized matrix is always negative, and we refer to the corresponding eigenvector as the ``homogenized negative curvature". Similar to the gradient descent method, where a first-order stationary point is reached by moving along the gradient direction, we can attain a second-order stationary point by \textit{exclusively} moving along the homogenized negative curvature.

Secondly, we propose a new second-order method called the Homogeneous Second-Order Descent Method (HSODM) with the homogenized quadratic model as the subproblem. We offer two step size strategies for utilizing the homogenized negative curvature: the fixed-radius strategy and a simple backtracking line search. Our method achieves a better iteration complexity of $O(\epsilon^{-3/2})$ converging to a second-order stationary point, compared with the standard trust-region method \cite{curtis_concise_2018} or negative curvature-based method \cite{curtis_exploiting_2019}. Note that the iteration complexity of standard trust-region method and negative curvature-based method is $O(\epsilon^{-2})$. It's worth noting that this is the optimal convergence rate for a second-order method in nonconvex optimization with respect to $\epsilon$. In sharp comparison to \cite[]{carmon_accelerated_2018,agarwal_finding_2017,jin_how_2017,royer_complexity_2018},
% In HSODM, we move along the \emph{homogeneous direction} by the smallest eigenvector of the homogenized system. Then it produces an $O(\epsilon^{3/2})$ decrease in function value. Due to the fact that the homogeneous direction works \emph{uniformly} in nonconvex and convex cases, 
HSODM only relies on the homogenized model and \emph{does not} alternate between different subroutines. The algorithm is elegant in a simple form and believed to be highly favorable for practitioners.

Thirdly, in comparison to the cubic regularized Newton method or ARC, the subproblem for HSODM is much simpler as we only need to compute the leftmost eigenvector of each iteration. The original computational complexity for solving the cubic regularized model is $O(n^3\log\epsilon^{-1})$ \cite{nesterov_cubic_2006}. While \citet{cartis2022evaluation} provided various methods to solve the cubic regularized model and improved the original computational complexity; however, the computational complexity for computing the leftmost eigenvector is often recognized as $O(n^3)$. Additionally, if HSODM uses similar negative curvature oracles mentioned in the first-order methods, it allows for an $O(\epsilon^{-7/4}\log(\epsilon^{-1}))$ complexity bound with a less dimension-dependent per-iteration cost.

Finally, the numerical results of the proposed method are also encouraging. In particular, two variants of HSODM outperform the standard second-order methods including \newtontr{} and \arc{}, and demonstrate a clear advantage over \lbfgs{} if one seeks to find solutions with high precisions in the CUTEst dataset.

\subsection{Related works}

There is a recent trend on the study of the
improved first-order algorithms \cite[]{carmon_accelerated_2018,agarwal_finding_2017,jin_how_2017} that can find $\epsilon$-approximate second-order stationary point. Thus, this type of algorithms can serve as a scalable alternative to the second-order ones.
Notably, some of these algorithms also enable faster first-order convergence to \eqref{eq.approxfocp} in $O(\epsilon^{-7/4}\log(\epsilon^{-1}))$ function and gradient evaluations. The basic idea is to extend Nesterov's accelerated gradient descent method (AGD) \cite[]{nesterov_lectures_2018} to the nonconvex case. This is achieved by properly embedding second-order information to make the AGD maintain its theoretical peroperty in convex and semiconvex cases.
For example, \citet{carmon_accelerated_2018} apply the Hessian-vector products and randomized Lanczos methods to explore the negative curvature (NC), which then is used as a descent direction if \eqref{eq.nc.suff} is satisfied; otherwise, $f$ becomes locally semiconvex and AGD is invoked to solve the subproblem.
The later work in \cite[]{jin_how_2017,xu_first-order_2018} also require NC but avoid Hessian-vector products and the complexities remain the same. Beyond using NC, \citet[]{agarwal_finding_2017} achieve the same complexity bound by applying fast matrix inversion to cubic regularized steps. Recently, \citet{li_restarted_2022} introduce a restarted AGD that drops the logarithmic term $O(\log(\epsilon^{-1}))$ in the complexity bound if the solution is required to only satisfy the first-order condition, but it also losses second-order guarantees.
To make AGD work in a comfort zone, these algorithms create sophisticated nested loops that may be hard to implement and tune.
% \footnote{\citet{li_restarted_2022} also address this issue, and their method is considerably easier than other nonconvex AGD methods.}. 
Nevertheless, they are designed to be less ``dimension-dependent'' than pure second-order methods such as \cite{nesterov_cubic_2006,cartis_adaptive_2011} and are suitable for large-scale problems in theory.

Coming back to the second order methods,  \citet{royer_complexity_2018} separates their method into two cases if \eqref{eq.nc.suff} is absent. In one case that the smallest eigenvalue $\lambda_{\min}(\Hk) > -\sqrt{\epsilon}$, regularized Newton step is used to provide the descent step. In the other case when $\lambda_{\min}(\Hk) > \sqrt \epsilon$ is certified, it turns to the ordinary Newton step. Therefore, in the worst case, this method must solve an eigenvalue problem and a Newton step in one iteration. It is unclear if one can unify these procedures as a whole. Recently, \citet{doikov2023gradient} and \citet{mishchenko2021regularized} proposed the Gradient Regularized Newton method for convex minimization problems. The subproblem at each iteration is simpler than that of the Cubic Regularized Newton method. Later, \citet{gratton2023yet} generalized the Gradient Regularized Newton method to exploit negative curvature for nonconvex optimization problems. However, their method alternates between regularized Newton and negative curvature steps.

For trust-region methods \eqref{eq.tr}, such a $\xi_k$ actually means the Lagrangian dual variable is at least in order of $\sqrt{\epsilon}$. Although TR may not use $\xi_k$ directly, it also implies an $O(\epsilon^{3/2})$ progress as long as the stepsize is not too small. This fact can be easily recognized by using the optimality conditions; see \cite{ye_second_2005,curtis_concise_2018} for example. Furthermore, it remains true even when the subproblems are solved inexactly or only optimal in the subspace \cite{cartis_adaptive_2011-1,curtis_worst-case_2022-1,zhang_drsom_2023}.
For fixed-radius strategies \cite{ye_second_2005,zhang_drsom_2023}, if \eqref{eq.nc.suff} does not hold, then the algorithm safely terminates.
The case is different for adaptive radius ones. Since the trust-region method uses $\eta_k$ in \eqref{eq.tr} and adjusts the radius linearly, a step may become too small with respect to the dual variable.  A workaround can be found in \cite{curtis_trust_2017,curtis_worst-case_2022-1} with a delicate control over function progress and the gradient:
$$
    f_k-f_{k+1} \geq \Omega(\|\dk\|^3) \text { and }\left\|\dk\right\| \geq \Omega(\|g_{k+1}\|^{1 / 2}).
$$
Similar conditions are also needed in the analysis of cubic regularization method \cite{cartis_adaptive_2011-1}.  However, these adaptations can be less straightforward to understand, implement and tune.



\subsection{Notations, assumptions and organization of the paper}
In this subsection, we introduce the notations and assumptions used throughout the paper.

Denote the standard Euclidean norm in space $\mathbb{R}^n$ by \(\|\cdot\|\). Let $B(x_c, r)$ denote the ball whose center is $x_c$ and radius is $r$, i.e., $ B(x_c, r) = \{ x\in \mathbb{R}^n \mid  \|x - x_c \| \leq r\}$. For a matrix \(A \in \mathbb{R}^{n\times n}\), \(\|A\|\) represents the induced \(\mathcal L_2\) norm, and $\lambda_{\min}(A)$ denotes its smallest eigenvalue. We also denote \(\gk = \nabla f(\xk)\) and \(\Hk = \nabla^{2} f(\xk)\).

In this paper, we make the following standard assumption.
\begin{assumption}
    \label{assm.lipschitz}
    Assume that $f$ has \(L\)-Lipschitz continuous gradient and \(M\)-Lipschitz continuous Hessian, that is, for all \( x, y \in \mathbb{R}^n\),
    \begin{equation}\label{eq.assm.lipschitz}
        \|\nabla f(x) - \nabla f(y)\| \le L \|x-y\| \quad \mbox{and}\quad \|\nabla^2 f(x) - \nabla^2 f(y)\| \le M \|x-y\|.
    \end{equation}
\end{assumption}

The rest of the paper is organized as follows.
In \autoref{sec.method}, we briefly describe our approach based on the homogenized quadratic model. By solving the homogenized model via an eigenvalue problem, the corresponding HSODM is introduced in \autoref{alg.main alg}.  In \autoref{sec.global} and \autoref{sec.local}, we give analyses of global and local convergence of HSODM. Our results indicate that HSODM has a global complexity of $O(\epsilon^{-3/2})$ for an $\epsilon$-approximate second-order stationary point. If one does not early terminate the algorithm, it bears a local quadratic converge speed. In \autoref{sec.experiment}. We demonstrate the effectiveness of our method by providing fruitful computational results in the CUTEst benchmark compared to other standard second-order methods.

\section{The Homogenized Quadratic Model and A Second-Order Descent Method}\label{sec.method}
% In this paper, we consider the following unconstrained optimization problem
% \begin{equation}\label{Prob:main}
%   \min_{x\in \mathbb{R}^n} f(x),
% \end{equation}
% where \(f:\mathbb{R}^n \mapsto \mathbb{R}\) is 
% twice continuous differentiable and possibly nonconvex and \( f_{\inf}:=\inf f(x) > -\infty\).
% And we aim to find a  second-order stationary point.

% In our analysis, we allow some tolerance $\epsilon>0$ and search for a so-called $\epsilon$-approximate second-order stationary point as described in the following definition:

% \begin{definition}
% For a given tolerance $\epsilon>0$, a point $x\in\mathbb{R}^n$ is a $\epsilon$-approximate second-order stationary point of function $f(x)$ if $x$ satisfies:
% \begin{equation}
%     \label{eq.approxsocp}
%     \|\nabla f(x)\|\leq O(\epsilon), \quad \lambda_{\min} \left(\nabla^2 f(x)\right) \geq -O(\sqrt{\epsilon}).
% \end{equation}
% \end{definition}
\subsection{Overview of the method}


% To facilitate the discussion, we denote \(\gk = \nabla f(\xk)\) and \(\Hk = \nabla^{2} f(\xk)\) throughout the paper.

% In every iteration $\xk$,  it is usual to use the following quadratic model to approximate our objective function $$m_k(d):=\gk^T d+\frac{1}{2}d^T \Hk d.$$
% Minimizing this model without constraints gives the classic Newton direction. However, its global performance is unsatisfactory. There has been a lot of search trying to improve its global performance(\textcolor{red}{cite Cubic, ARC...}), but we wonder if there is a better local model that allows us to exploit the gradient and hessian of the objective function simultaneously and more efficiently, this motivates us to solve the following perturbed homogeneous eigenvalue subproblem,
% \chenyu{add some words on the homogenization trick}

We first define the homogenized quadratic model as follows. Given an iterate $\xk \in \mathbb{R}^n$, let $\psi_k(v, t; \delta)$ be the homogenized quadratic model,
\begin{equation}\label{eq.homoquadmodel}
    \psi_k(v, t; \delta) := ~ \begin{bmatrix}
        v \\ t
    \end{bmatrix}^T
    \begin{bmatrix}
        \Hk   & \gk     \\
        \gk^T & -\delta
    \end{bmatrix}
    \begin{bmatrix}
        v \\ t
    \end{bmatrix},~ v\in \mathbb{R}^n, t \in \mathbb{R},
\end{equation}
where $\delta \geq 0$ is a predefined constant. For simplicity, denote the homogenized matrix by $F_k = [ \Hk , \gk ; \gk^T , -\delta ]$. For each iteration, the HSODM minimizes the homogenized quadratic model at the current iterate $\xk$, i.e.,
\begin{equation}
    \label{eq.homo subproblem}
    \begin{aligned}
        \min_{\|[v; t]\| \le 1} \psi_k(v, t; \delta).
    \end{aligned}
\end{equation}
Denote the optimal solution of problem \eqref{eq.homo subproblem} as $[v_k; t_k]$. As the subproblem \eqref{eq.homo subproblem} is essentially a eigenvalue problem, and $[v_k; t_k]$ is the eigenvector corresponding to the smallest eigenvalue of $F_k$. Therefore, we can solve this subproblem using an eigenvector-finding procedure, see \cite{carmon_accelerated_2018, kuczynski_estimating_1992, royer_complexity_2018}.

After solving \eqref{eq.homo subproblem}, we construct a descent direction $d_k$ based on this optimal solution $[v_k;t_k]$ and carefully select the step size $\eta_k$ to ensure sufficient decrease. Intuitively, if $|t_k|$ is relatively small, it means that the Hessian matrix $H_k$ dominates the homogenized model, and thus we choose the truncated direction $v_k$ directly. Otherwise, the predefined parameter $-\delta$ becomes significant, and we choose $v_k/t_k$ as the descent direction instead. For the step size rule, we provide two strategies for selecting the step size: the first is to use line search to determine $\eta_k$, and the second is to adopt the idea of the fixed-radius trust-region method \cite{luenberger_linear_2021,zhang_drsom_2023} such that $\|\eta_kd_k\| = \Delta$, where $\Delta$ is some pre-determined constant. By iteratively performing this subroutine, our algorithm will converge to an $\epsilon$-approximate second-order stationary point. The details are formally provided in \autoref{alg.main alg}.



% The discussion is already presented after Lemma 2. 
% It is worth noting that \citet{sorensen_minimization_1997} and later \cite{rojas_new_2001,adachi_solving_2017} cast the trust-region subproblem \eqref{eq.tr} as a parametric eigenvalue problem. They use the similar ``homogenization'' idea but limit to solving the trust-region subproblem. Since the classical trust-region method does not have the optimal complexity, recognizing the eigenvalue problem itself will not give the desired $O(\epsilon^{-3/2})$ results.


\begin{minipage}[t]{0.95\linewidth}
\begin{algorithm}[H]
    \caption{Homogeneous Second-Order Descent Method (HSODM)}\label{alg.main alg}
    \textbf{Initialization:} Given initial point $x_1$, $\nu \in (0, 1/2)$, $\Delta  = \Theta(\sqrt{\epsilon})$. \\
    \textbf{For} $k = 1, 2, \cdots$ \textbf{do:}\\
    \quad Solve the subproblem \eqref{eq.homo subproblem}, and obtain the solution $[v_k; t_k]$; \\
    \quad Obtain the homogenized direction: $
        \begin{cases}
            \dk := v_k / t_k,                         & if \ |t_k| \geq \nu \\
            \dk := \text{sign}(-g_k^T v_k) \cdot v_k, & if \ |t_k| < \nu
        \end{cases}$;\\
    % \quad \textbf{If} $t_k = 0$ \textbf{then:} \\
    % \quad \quad Choose direction $d_k = v_k$ and stepsize $\eta_k = \Delta$; \\
    % \quad \quad Update $x_{k+1} = x_k + \eta_k d_k$; \quad \quad\quad\quad\quad\quad\quad\quad \quad\quad \textit{// sufficient decrease}\\
    % \quad \textbf{Else:} \\
    % \quad \quad Choose direction $d_k = v_k / t_k $; \\
    \quad \textbf{If} $\|d_k\| > \Delta$ \textbf{then:} \\
    \quad \quad Choose a stepsize $\eta_k$ by fix-radius strategy or line search (see Algorithm \ref{alg.backtracking}); \\
    \quad \quad Update $x_{k+1} = x_k + \eta_k d_k$; \quad\quad\quad\quad\quad\quad\quad \quad \textit{// sufficient decrease}\\
    \quad \textbf{Else:} \\
    % \quad \quad Choose stepsize $\eta_k = 1$; \\
    \quad\quad Update $x_{k+1} = x_k + d_k$;
    \quad\quad\quad\quad\quad\quad\quad\quad\quad\textit{// early termination } \\
    \quad \quad Terminate (or set $\delta = 0$ and proceed);
\end{algorithm}
\end{minipage}

\subsection{Preliminaries of the Homogenized Quadratic Model}
In this subsection, we present some preliminary analysis of the homogenized quadratic model. First, we study the relationship between the smallest eigenvalues of the Hessian $\Hk$ and $F_k$, and the perturbation parameter $\delta$. Then we give the optimality conditions of problem \eqref{eq.homo subproblem} and provide some useful results based on those conditions.

\begin{lemma}[Relationship between $\lambda_{\min}(F_k)$, $\lambda_{\min}(H_k)$ and $\delta$]
    \label{lemma.relation of theta delta Hk}
    Let $\lambda_{\min}(\Hk)$ and $\lambda_{\min}(F_k)$ be the smallest eigenvalue of $H_k$ and $F_k$ respectively. Denote by $\mathcal{S}_{\lambda_{\min}}$ the eigenspace corresponding to $\lambda_{\min}(\Hk)$. If $g_k \neq 0$ and $H_k \neq 0$, then the following statements hold,
    \begin{enumerate}
        \item[(1)] $\lambda_{\min}(F_k) < -\delta$ and $\lambda_{\min}(F_k) \leq \lambda_{\min}(\Hk)$;
        \item[(2)] $\lambda_{\min}(F_k) = \lambda_{\min}(\Hk)$ only if $\lambda_{\min}(\Hk)<0$ and $\gk \perp \mathcal{S}_{\lambda_{\min}}$.
            % and $\delta = \lambda_{\min}(\Hk)-\gk^\top p_{\lambda_{\min}}$
    \end{enumerate}
    % and $p_{\lambda_{\min}} = -(\Hk-\lambda_{\min}(\Hk)I)^{\dagger}\gk$.
    % \begin{itemize}
    %     \item If $\lambda_{\min}(H_k) < -\delta$, then $\theta_k > -\lambda_{\min}(H_k)$;
    %     \item If $\lambda_{\min}(H_k) \geq -\delta$, then $\theta_k > \delta$; 
    %     \item $\theta_k > \max\left\{-\lambda_{\min}(H_k), \delta\right\}$;
    %     \item Once $\lambda_{\min}(H_k) > \delta$, then $t_k \neq 0$,
    % \end{itemize}
    % where $\lambda_{\min}(H_k)$ is the smallest eigenvalue of the matrix $H_k$.
\end{lemma}
\begin{proof}
    We first prove the statement $(1)$. By the Cauchy interlace theorem \cite{parlett_symmetric_1998}, we immediately obtain $\lambda_{\min}(F_k)  \leq \lambda_{\min}(H_k) $. Now we need to prove that $\lambda_{\min}(F_k) < -\delta $. It suffices to show that the matrix $F_k+\delta I$ has a negative curvature.

    Let us consider the direction $\left[ -\eta \gk ; t \right]$, where $\eta,t>0$. Define the following function of $\left( \eta,t \right)$:
    \begin{equation*}
        \begin{aligned}
            f(\eta,t) & := \begin{bmatrix}
                                      -\eta \gk \\
                                      t
                                  \end{bmatrix}^T (F_k+\delta I) \begin{bmatrix}
                                                                     -\eta \gk \\
                                                                     t
                                                                 \end{bmatrix}, \\
                      & =\eta^2 \gk^T (\Hk+\delta I) \gk -2\eta t \|\gk\|^2.
        \end{aligned}
    \end{equation*}
    For any fixed $t>0$, we have
    \begin{equation*}
        f(0,t)=0 \quad  \text{and} \quad  \frac{\partial f(0,t)}{\partial \eta}=-2t \|\gk\|^2<0.
    \end{equation*}
    Therefore, for sufficiently small $\eta>0$, it holds that $f(\eta,t)<0$, which shows that $\left[ -\eta \gk ; t \right]$ is a negative curvature. Hence, $\lambda_{\min}(F_k) < -\delta $.

    % Now we need to prove that $\lambda_{\min}(F_k) < -\delta $. Denote $u_j = (u_j^{'}; t_j)$ with $u_j^{'} \in \mathbb{R}^{n}$ and $t_j \in \mathbb{R}$ for $j = 1, 2, \cdots, n+1$. Note that there exists at least one eigenpair of $F_k$ such that $t_j \neq 0$ and we take $ d_j = u_j^{'} / t_j$ for such $j$. By the definition of eigenvalue and eigenvector, we have 
    % \begin{equation*}
    %     F_k u_j = - \lambda_j u_j, 
    % \end{equation*}
    % implying that 
    % \begin{equation}
    %     \label{eq.equivoptc}
    %     (\Hk+\lambda_j I) d_j = -\gk \quad \text{and} \quad \gk^T d_j = \delta-\lambda_j.
    % \end{equation}

    % Let $\beta_i =  \gk^T v_i$ for all $i = 1, 2, \cdots, n$. We then have $\gk^T d_j = -\gk^T(\Hk+\lambda_j  I)^{\dagger}\gk = -\sum_{i=1}^n  \beta_i^2 / \left(  \sigma_i+\lambda_j \right)$. Therefore, $\lambda_j$ must satisfy
    % \begin{equation}
    %     \label{eq.propertytheta}
    %     \delta-\lambda_j =  -\sum_{i=1}^n\frac{\beta_i^2}{\sigma_i + \lambda_j}.
    % \end{equation}
    % Let us view both sides of the above equation as the functions of $\lambda_j$. The left-hand-side is a linear function in $\lambda_j$ while the right-hand-side is the sum of a sequence of inverse proportional functions. By the property of the two functions and the Cauchy interlace theorem \cite{parlett1998symmetric}, there exists a solution to \eqref{eq.propertytheta} such that $-\lambda_j<-\delta$, implying that $ \lambda_{\min} \left( F_k \right) = - \lambda_1 \leq \lambda_j <  - \delta$.  


    The proof of the statement $(2)$ is similar to the one of Theorem 3.1 in \cite{rojas_new_2001}, so we omit it here for succinctness of the paper.
    % Then we prove the second part. We first start with $\lambda_{\min}(\Hk)<0$, $\gk \perp \mathcal{S}_{\lambda_{\min}}$, $\delta = \lambda_{\min}(\Hk)-\gk^\top p_{\lambda_{\min}}$.

    % To prove $\theta_k = -\delta_1$, it suffices to prove that under our choice of $\delta$, there is a eigenvector correspondings to $\delta_1$ with $t\neq0$,which is equivalent to the LHS and RHS intersects at a point with its first coordinate equals $-\delta_1$, this completes the proof. Hence we need to construct a eigenvector $[v;t]$ corresponding to $\delta_1$ with $t\neq0$. The construction is $[p_{\lambda_{\min}}^T;1]$, note that by our construction of $p_{\lambda_{\min}}$ and $\delta$, we have
    % \begin{equation}
    %     \label{eq.propertycons}
    %     (\Hk-\delta_1 I)p_{\lambda_{\min}}= -(\Hk-\delta_1 I)(\Hk-\delta_1 I)^{\dagger}\gk=-\gk,
    % \end{equation}
    % where the second equality is due to that $(\Hk-\delta_1 I)(\Hk-\delta_1 I)^{\dagger}$ is a projection onto the image space $\mathcal{R}(\Hk-\delta_1 I)$ and that $\gk\perp \mathcal{S}_1$ thus belonging to the image space.


    % \begin{equation}
    %     \label{eq.checkeigenvec}
    %     \begin{bmatrix}
    %         \Hk   & \gk    \\
    %         \gk^T & \delta
    %     \end{bmatrix}\begin{bmatrix}
    %         p_{\lambda_{\min}} \\1
    %     \end{bmatrix}=\begin{bmatrix}
    %         \Hk p_{\lambda_{\min}}+\gk \\
    %         \delta+\gk^T p_{\lambda_{\min}}
    %     \end{bmatrix}=\delta_1\begin{bmatrix}
    %         p_{\lambda_{\min}} \\
    %         1
    %     \end{bmatrix}.
    % \end{equation}
    % The opposite part is similar so we omit it here. 
\end{proof}
\autoref{lemma.relation of theta delta Hk} shows that we can control the smallest eigenvalue of the homogenized matrix $F_k$ by adjusting the perturbation parameter $\delta$. It helps us find a better direction to decrease the value of the objective function. We also note that the case $\gk\perp \mathcal{S}_{\lambda_{\min}}$ is often regarded as a hard case in solving the trust-region subproblem. However, this challenge will not incapacitate HSODM in our convergence analysis. In the following, we will show the function value has sufficient decrease under this scenario. Thus, the subproblem in HSODM is much easier to solve than the trust-region subproblem due to the non-existence of the hard case.

We remark that \autoref{lemma.relation of theta delta Hk} is a simpler version of Lemma 3.3 in \cite{rojas_new_2001},
where the authors give a more detailed analysis of the relationship between the perturbation parameter $\delta$ and the eigenpair of the homogenized matrix $F_k$.
However, the difference is that they try to obtain a solution to the trust-region subproblem via the homogenization trick, while our goal is to seek a good direction to decrease the function value.  Furthermore, if the homogenized model is used, then we can show that HSODM has the optimal $O(\epsilon^{-3/2})$ iteration complexity. If instead, the homogenization trick is put on solving the trust-region subproblem as in \cite{rojas_new_2001}, one still needs a framework like the one in \citet{curtis_trust_2017} to guarantee the same convergence property. Moreover, a sequence of homogenized problems need to be solved in each iteraion of the framework.

In the following lemma, we characterize the optimal solution $[v_k; t_k]$ of problem \eqref{eq.homo subproblem} based on the optimality condition of the standard trust-region subproblem.

\begin{lemma}[Optimality condition]\label{lemma.optimal condition of subproblem}
    $[v_k; t_k]$ is the optimal solution of the subproblem \eqref{eq.homo subproblem} if and only if there exists a dual variable $\theta_k > \delta \geq 0$ such that
    \begin{align}
        \label{eq.homoeig.soc}
                                    & \begin{bmatrix}
                                          \Hk + \theta_k \cdot I & \gk              \\
                                          \gk^T                  & -\delta+\theta_k
                                      \end{bmatrix} \succeq 0, \\
        \label{eq.homoeig.foc}
                                    & \begin{bmatrix}
                                          \Hk + \theta_k \cdot I & \gk              \\
                                          \gk^T                  & -\delta+\theta_k
                                      \end{bmatrix}
        \begin{bmatrix}
            v_k \\ t_k
        \end{bmatrix} = 0,                                                      \\
        \label{eq.homoeig.norm one} & \|[v_k; t_k]\| = 1.
    \end{align}
    Moreover, $-\theta_k$ is the smallest eigenvalue of the perturbed homogeneous matrix $F_k$, i.e., $-\theta_k = \lambda_{\min}(F_k)$.
\end{lemma}
\begin{proof}
    By the optimality condition of standard trust region subproblem, $[v_k; t_k]$ is the optimal solution if and only if there exists a dual variable $\theta_k \geq 0$ such that
    \begin{equation*}
        \begin{bmatrix}
            \Hk + \theta_k \cdot I & \gk              \\
            \gk^T                  & -\delta+\theta_k
        \end{bmatrix} \succeq 0, \ \begin{bmatrix}
            \Hk + \theta_k \cdot I & \gk              \\
            \gk^T                  & -\delta+\theta_k
        \end{bmatrix} \begin{bmatrix}
            v_k \\ t_k
        \end{bmatrix} = 0, \ \text{and} \  \theta_k \cdot (\|[v_k; t_k]\| - 1) = 0.
    \end{equation*}
    With \autoref{lemma.relation of theta delta Hk}, we have $\lambda_{\min}(F_k) < -\delta \leq 0$.
    Therefore, $\theta_k \geq  - \lambda_{\min}(F_k) >  \delta \geq 0$, and further $ \|[v_k; t_k]\| = 1 $. Moreover, by \eqref{eq.homoeig.foc}, we obtain
    \begin{equation*}
        \begin{bmatrix}
            \Hk   & \gk     \\
            \gk^T & -\delta
        \end{bmatrix} \begin{bmatrix}
            v_k \\ t_k
        \end{bmatrix} = -\theta_k \begin{bmatrix}
            v_k \\ t_k
        \end{bmatrix}.
    \end{equation*}
    Multiplying the equation above by $ \left[ v_k ; t_k \right]^T $, we have
    \begin{equation*}
        \min_{\|[v; t]\| \le 1} \psi_k(v, t; \delta) = -\theta_k
    \end{equation*}
    Note that with \eqref{eq.homoeig.norm one}, the optimal value of problem \eqref{eq.homo subproblem} is equivalent to the smallest eigenvalue of $F_k$, i.e., $\lambda_{\min}(F_k)$. Thus, $-\theta_k = \lambda_{\min}(F_k)$. Then the proof is completed.
\end{proof}

With the above optimality condition, we can derive the following corollaries.
\begin{corollary}
    \label{corollary. foc}
    The equation \eqref{eq.homoeig.foc} in \autoref{lemma.optimal condition of subproblem} can be rewritten as,
    \begin{equation}\label{eq.homoeig.foc cont}
        (\Hk + \theta_k I)  v_k =  - t_k g_k \quad \text{and} \quad \gk^T v_k  =  t_k (\delta - \theta_k).
    \end{equation}
    Furthermore,
    \begin{enumerate}
        \item[(1)] If $t_k = 0$, then we have
            \begin{equation}\label{eq.homoeig.foc t=0}
                (\Hk + \theta_k I)  v_k = 0 \quad \text{and} \quad \gk^T v_k  = 0,
            \end{equation}
            implying that $(-\theta_k, v_k)$ is the eigenpair of the Hessian matrix $H_k$.
        \item[(2)] If $t_k \neq 0$, then we have
            \begin{equation}
                \label{eq.homoeig.foc t neq 0}
                \gk^T d_k = \delta -\theta_k \quad \text{and} \quad (\Hk+\theta_k \cdot I)d_k =-\gk
            \end{equation}
            where $d_k =v_k / t_k$.
    \end{enumerate}
\end{corollary}
The corollary above is a direct application of \autoref{lemma.optimal condition of subproblem}, so we omit its proof in the paper.

\begin{corollary}[Nontriviality of direction $v_k$] \label{coro.homo.exists.nc}
    If $g_k \neq 0$, then $v_k \neq 0$.
\end{corollary}
\begin{proof}
    We prove by contradiction. Suppose that $v_k = 0$. Then, we have $t_k g_k = 0$ with equation \eqref{eq.homoeig.foc cont} in  \autoref{corollary. foc}. It further implies that $t_k = 0$ due to $g_k \neq 0$. However, $[ v_k ; t_k]=0$ contradicts to the equation $\|[v_k; t_k]\| = 1$ in the optimality condition. Therefore, we have $v_k \neq 0$.
\end{proof}

This corollary shows that a nontrivial direction $v_k$ always exists, thus \autoref{alg.main alg} will not get stuck.

\begin{corollary}\label{corollary. sign property}
    For the sign function value $\text{sign}(-g_k^T v_k)$, we always have $\text{sign}(-g_k^T v_k) \cdot t_k = |t_k|$.
\end{corollary}
\begin{proof}
    By the second equation of optimal condition \eqref{eq.homoeig.foc cont}, and $\delta < \theta_k$, we obtain that
    $$
        \text{sign}(-g_k^T v_k) = \text{sign}(t_k),
    $$
    and it implies
    $$
        \text{sign}(-g_k^T v_k) \cdot t_k = \text{sign}(t_k) \cdot t_k = |t_k|.
    $$
\end{proof}

As a byproduct, we also have the following result.

\begin{corollary}[$g_k = 0$]
    Suppose that $g_k = 0$, then the following statements hold, \begin{enumerate}
        \item[(1)] If $\lambda_{\min}(H_k) > -\delta$, then $t_k = 1$.
        \item[(2)] If $\lambda_{\min}(H_k) < -\delta$, then $t_k = 0$.
    \end{enumerate}
\end{corollary}
\begin{proof}
    When $g_k = 0$, the homogenized matrix $F_k = [H_k, 0; 0, -\delta]$, and the subproblem \eqref{eq.homo subproblem} is
    \begin{equation*}
        \min_{\|[v; t]\| \le 1} \psi_k(v, t; \delta) = v^T H_k v - t^2 \cdot \delta.
    \end{equation*}
    We first prove the statement $(1)$ by contradiction. Suppose that $t_k \neq 1$, then we have $v_k \neq 0$ by the equation \eqref{eq.homoeig.norm one}. Thus,
    \begin{equation}
        \label{eq. gk=0 contra}
        \psi_k(v_k, t_k; \delta) = (v_k)^T H_k v_k - t_k^2 \cdot \delta > -\delta = \psi_k(0, 1; \delta),
    \end{equation}
    where the inequality holds due to $(v_k)^T H_k v_k \geq \lambda_{\min}(H_k) \|v_k\|^2 > -\delta\|v_k\|^2$. The equation \eqref{eq. gk=0 contra} contradicts to the optimality of $(v_k, t_k)$, and thus $t_k = 1$. The second statement can be proved by the same argument, and we omit the proof here.
\end{proof}



\section{Global Convergence Rate}\label{sec.global}
In this section, we analyze the convergence rate of the HSODM method. To facilitate the analysis, we present two building blocks considering the large and small values of $\|d_k\|$, respectively.
For the large value case of $\|d_k\|$, we show that the function value decreases by at least $O(\epsilon^{3/2})$ at every iteration after carefully selecting the perturbation parameter $\delta$. In the latter case, we prove that the next iterate $x_{k+1}$ is already an $\epsilon$-approximate second-order stationary point.

We first recall the following lemma for preparation.
\begin{lemma}[\citet{nesterov_lectures_2018}]\label{lem.lipschitz}
    If \(f:\mathbb{R}^n \mapsto \mathbb{R}\) satisfies \autoref{assm.lipschitz}, then for all \(x,y\in \mathbb{R}^n\),
    \begin{subequations}
        \begin{align}
             & |f(y)-f(x)-\nabla f(x)^T(y-x)|                                             \leq \frac{L}{2}\|y-x\|^{2}        \\
             & \left\|\nabla f(y)-\nabla f(x)-\nabla^{2} f(x)(y-x)\right\|                       \leq \frac{M}{2}\|y-x\|^{2} \\
             & \left|f(y)-f(x)-\nabla f(x)^T(y-x)-\frac{1}{2}(y-x)^T\nabla^{2} f(x)(y-x)\right|  \leq \frac{M}{6}\|y-x\|^{3}
        \end{align}
    \end{subequations}
\end{lemma}

\subsection{Analysis for the large value of $\|d_k\|$ }\label{subsection.decrease}

In HSODM, we define the large-valued case of $\|d_k\|$ as the case that its norm is larger than the trust region radius $\Delta$, i.e., $\|d_k\| > \Delta$. In this situation, the homogenized direction can be either $d_k = \text{sign}(-g_k^T v_k) \cdot v_k$ or $d_k = v_k / t_k$. The following discussion shows that both step size selection strategies result in a sufficient decrease. The analysis for the fix-radius strategy is more concise and clear, but it is only a theoretical result. On the other hand, the step size selection strategy based on line search is more practical, but its analysis is slightly more complicated.

\subsubsection{Fix-radius strategy}

For the fix-radius strategy, the next iterate $x_{k+1}$ is constrained to satisfy $\|x_{k+1} - x_k\| = \Delta$, and hence the step size is selected as $\Delta/\|d_k\|$. Firstly, we will consider the scenario in which $|t_k| < \nu$ and $d_k = \text{sign}(-g_k^T v_k) \cdot v_k$. We remark that this particular scenario encompasses the so-called ``hard case" $t_k = 0$ in \cite{rojas_new_2001}. Notably, when $t_k = 0$, \autoref{corollary. foc} shows that $(-\theta_k, v_k)$ is an eigenpair of the Hessian $H_k$, and $v_k$ is a sufficiently negative curvature due to $-\theta_k < - \delta \leq 0$. Therefore, moving along the direction of $v_k$ with an appropriate step size will always decrease the function value~\cite{carmon_accelerated_2018}. We now present a lemma that applies to the case $|t_k| < \nu$, and it could be regarded as a generalized descent lemma.

\begin{lemma}\label{lemma.t < delta fix radius decrease lemma}
    Suppose that \autoref{assm.lipschitz} holds and set $\nu \in (0, 1/2)$. If $|t_k| < \nu$, then let $d_k = \text{sign}(-g_k^T v_k) \cdot v_k$ and $\eta_k = \Delta/\|d_k\|$, we have
    \begin{equation}
        f(x_{k+1}) - f(x_k) \leq -\frac{\Delta^2}{2}\delta + \frac{M}{6}\Delta^3.
    \end{equation}
\end{lemma}
\begin{proof}
    When $d_k = \text{sign}(-g_k^T v_k) \cdot v_k$, with the optimal condition \eqref{eq.homoeig.foc cont} in \autoref{corollary. foc} and \autoref{corollary. sign property}, we obtain
    \begin{equation}
        \label{eq:bridge2}
        d_k^T H_k d_k = - \theta_k \|d_k\|^2 - t_k^2 \cdot (\delta - \theta_k) \quad \text{and} \quad
        g_k^T d_k = |t_k| \cdot(\delta - \theta_k).
    \end{equation}
    Since $\eta_k = \Delta / \|d_k\| \in (0, 1)$, then $  \eta_k - \eta_k^2 / 2 \geq 0 $, and further
    \begin{equation}
        \label{eq:bridge6}
        \left(\eta_k - \frac{\eta_k^2}{2}\right) \cdot (\delta - \theta_k) \leq 0
    \end{equation}
    By the $M$-Lipschitz continuous property of $\nabla^2 f(x)$, we have
    \begin{subequations}
        \begin{align}
            f(x_{k+1}) - f(x_k) & = f(x_k + \eta_k d_k) - f(x_k)  \notag                                                                                                                                                                                            \\
                                & \leq \eta_k \cdot g_k^T d_k + \frac{\eta_k^2}{2} \cdot d_k^T H_k d_k + \frac{M}{6}\eta_k^3 \|d_k\|^3  \notag                                                                                                                      \\
                                & = \eta_k \cdot |t_k| \cdot(\delta - \theta_k) - \frac{\eta_k^2}{2} \cdot \theta_k\|d_k\|^2 - \frac{\eta_k^2}{2} \cdot t_k^2 \cdot (\delta - \theta_k) + \frac{M}{6}\eta_k^3 \|d_k\|^3 \label{subeq. substitute optimal condition} \\
                                & \leq \eta_k \cdot t_k^2 \cdot(\delta - \theta_k) - \frac{\eta_k^2}{2} \cdot \theta_k\|d_k\|^2 - \frac{\eta_k^2}{2} \cdot t_k^2 \cdot (\delta - \theta_k) + \frac{M}{6}\eta_k^3 \|d_k\|^3  \label{subeq. relax |tk|}               \\
                                & = \left(\eta_k - \frac{\eta_k^2}{2}\right) \cdot t_k^2 \cdot (\delta - \theta_k) - \frac{\eta_k^2}{2} \cdot \theta_k\|d_k\|^2 + \frac{M}{6}\eta_k^3 \|d_k\|^3  \notag                                                             \\
                                & \leq - \theta_k \cdot \frac{\Delta^2}{2} + \frac{M}{6}\Delta^3 \label{subeq. t=0 optimal condition}                                                                                                                               \\
                                & \leq -\frac{\Delta^2}{2}\delta + \frac{M}{6}\Delta^3, \label{subeq. t=0 delta}
        \end{align}
    \end{subequations}
    where \eqref{eq:bridge2} follows from \eqref{subeq. substitute optimal condition}, and \eqref{subeq. relax |tk|} holds due to $|t_k| < \nu < 1$ and $\delta - \theta_k < 0$. The inequality \eqref{subeq. t=0 optimal condition} holds by \eqref{eq:bridge6} and $\eta_k = \Delta/\|d_k\|$.
\end{proof}

Now we turn to the case $|t_k| \geq \nu$. When the norm of the direction $d_k = v_k / t_k$ is large enough, i.e., $\|d_k\| > \Delta$, we can obtain the same decrease of function value by choosing a suitable stepsize $\eta_k$.
\begin{lemma}\label{lemma.t > delta fix radius decrease lemma}
    Suppose that \autoref{assm.lipschitz} holds and set $\nu \in (0, 1/2)$. If $|t_k| \geq \nu$ and $\|v_k / t_k\| > \Delta$, let $d_k = v_k / t_k$ and $\eta_k = \Delta / \|d_k\|$, we have
    \begin{equation}
        f(x_{k+1}) - f(x_k) \leq -\frac{\Delta^2}{2}\delta + \frac{M}{6}\Delta^3.
    \end{equation}
\end{lemma}
\begin{proof}
    When $t_k \neq 0$, with equation \eqref{eq.homoeig.foc t neq 0} in \autoref{corollary. foc}, we have
    \begin{equation}
        \label{eq:bridge3}
        d_k^T H_k d_k = -g_k^T d_k - \theta_k \|d_k\|^2 \quad \text{and} \quad g_k^T d_k = \delta - \theta_k \leq 0
    \end{equation}
    Since $\eta_k = \Delta / \|d_k\| \in (0, 1)$, then $  \eta_k - \eta_k^2 / 2 \geq 0 $, and further
    \begin{equation}
        \label{eq:bridge4}
        \left(\eta_k - \frac{\eta_k^2}{2}\right) \cdot g_k^T d_k \leq 0
    \end{equation}

    By the $M$-Lipschitz continuous property of $\nabla^2 f(x)$, we have
    \begin{subequations}
        \begin{align}
            f(x_{k+1}) - f(x_k) & = f(x_k + \eta_k d_k) - f(x_k)  \notag                                                                                                                                             \\
                                & \leq \eta_k \cdot g_k^T d_k + \frac{\eta_k^2}{2} \cdot d_k^T H_k d_k + \frac{M}{6}\eta_k^3 \|d_k\|^3  \notag                                                                       \\
                                & = \left(\eta_k - \frac{\eta_k^2}{2}\right) \cdot g_k^T d_k - \theta_k \cdot \frac{\eta_k^2}{2} \|d_k\|^2 + \frac{M}{6}\eta_k^3 \|d_k\|^3  \label{subeq. t neq 0 optimal condition} \\
                                & \leq -\theta_k \cdot \frac{\eta_k^2}{2} \|d_k\|^2 + \frac{M}{6}\eta_k^3 \|d_k\|^3 \label{subeq. t neq 0 drop gd}                                                                   \\
                                & \leq -\frac{\Delta^2}{2}\delta + \frac{M}{6}\Delta^3, \label{subeq. t neq 0 delta}
        \end{align}
    \end{subequations}
    where \eqref{subeq. t neq 0 optimal condition} holds due to equation \eqref{eq:bridge3}, \eqref{subeq. t neq 0 drop gd} follows from equation \eqref{eq:bridge4}, and in \eqref{subeq. t neq 0 delta} we substitute $\eta_k$ with $\Delta / \| d_k\| $ and use $\theta_k \geq \delta$.
\end{proof}

\subsubsection{Line search strategy}
For the line search strategy, we utilize a backtracking subroutine to determine the step size $\eta_k$, ensuring that the $\eta_k d_k$ produces a sufficient decrease. The details of the subroutine are provided below.

\begin{minipage}[t]{0.95\linewidth}
\begin{algorithm}[H]
    \caption{Backtracking Line Search}\label{alg.backtracking}
    \textbf{Initialization:} Given current point $x_k$,  direction $d_k$, initial step size $\eta_k = 1$, $\gamma > 0$, $\beta \in (0, 1)$. \\
    \textbf{For} $j = 0, 1, 2, \cdots$ \textbf{do:} \\
    \quad Compute decrease quantity $D_k := f(x_k) - f(x_k + \eta_k d_k)$; \\
    \quad \textbf{If} $D_k \geq \frac{\gamma}{6}\eta_k^3\|d_k\|^3$ \textbf{then:} \\
    \quad \quad \textbf{Break;} \\
    \quad \textbf{Else:} \\
    \quad \quad Update $\eta_k := \beta \cdot \eta_k$; \\
    \textbf{Output:} stepsize $\eta_k$.
\end{algorithm}
\vspace{1em}
\end{minipage}

Similarly, we analyze the descending lemma for the line search strategy and the number of iterations required by the line search procedure. For the case of $|t_k| < \nu$, HSODM with line search admits the following function value decrease.
\begin{lemma}\label{lemma.t < delta line search decrease lemma}
    Suppose that \autoref{assm.lipschitz} holds and set $\nu \in (0, 1/2)$. If $|t_k| < \nu$, then let $d_k = \text{sign}(-g_k^T v_k) \cdot v_k$. The backtracking line search terminates with $\eta_k = \beta^{j_k}$,  and $j_k$ is upper bounded by 
    $$
        j_N := \left \lceil \log_\beta\left(\frac{3\delta}{M+\gamma}\right) \right\rceil,
    $$and the function value associated with the stepsize $\eta_k$ satisfies,
    \begin{equation}\label{eq. function value decrease line search t < delta}
        f(x_{k+1}) - f(x_k) \leq -\min \left\{\frac{\sqrt{3}\gamma}{16}, \frac{9\gamma\beta^3\delta^3}{2(M+\gamma)}\right\}.
    \end{equation}
\end{lemma}
\begin{proof}
    Suppose that the backtracking line search terminate with $\eta_k = 1$, then we have
    $$
        \begin{aligned}
            f(x_k + \eta_kd_k) - f(x_k) & \leq -\frac{\gamma}{6}\eta_k^3\|d_k\|^3 \\
                                        & = -\frac{\gamma}{6}\|v_k\|^3            \\
                                        & \leq -\frac{\sqrt{3}\gamma}{16}         \\
        \end{aligned}
    $$
    where the last inequality is due to $\|v_k\| = \sqrt{1-|t_k|^2} \geq \sqrt{1-\nu^2} \geq \sqrt{3}/2$. Suppose the algorithm does not stop at the iteration $j \geq 0$ and 
     the condition in Line 4 is not met, i.e. $D_k < \frac{\gamma}{6}\beta^{3j}\|d_k\|^3 = \frac{\gamma}{6}\beta^{3j}\|v_k\|^3$. By using a similar argument in the proof of \autoref{lemma.t < delta fix radius decrease lemma}, we have that
    \begin{equation}\label{ineq:line-search-iter-est}
        \begin{aligned}
            -\frac{\gamma}{6}\beta^{3j}\|v_k\|^3 & < f(x_k + \beta^j d_k) - f(x_k)                                                                                                                                                                 \\
                                                 & \leq \beta^j \cdot g_k^T d_k + \frac{\beta^{2j}}{2} \cdot d_k^T H_k d_k + \frac{M}{6}\beta^{3j}\|d_k\|^3                                                                                    \\
                                                 & = \beta^j \cdot |t_k| \cdot (\delta-\theta_k) - \frac{\beta^{2j}}{2} \cdot \theta_k\|v_k\|^2 - \frac{\beta^{2j}}{2} \cdot t_k^2 \cdot (\delta - \theta_k) + \frac{M}{6}\beta^{3j}\|v_k\|^3    \\
                                                 & \leq \beta^j \cdot t_k^2 \cdot (\delta-\theta_k) - \frac{\beta^{2j}}{2} \cdot \theta_k\|v_k\|^2 - \frac{\beta^{2j}}{2} \cdot t_k^2 \cdot (\delta - \theta_k) + \frac{M}{6}\beta^{3j}\|v_k\|^3 \\
                                                 & = \left(\beta^j - \frac{\beta^{2j}}{2}\right) \cdot t_k^2 \cdot (\delta-\theta_k) -  \frac{\beta^{2j}}{2} \cdot \theta_k\|v_k\|^2 + \frac{M}{6}\beta^{3j}\|v_k\|^3                              \\
                                                 & \leq - \frac{\beta^{2j}}{2} \cdot \theta_k\|v_k\|^2 + \frac{M}{6}\beta^{3j}\|v_k\|^3                                                                                                          \\
                                                 & \leq - \frac{\beta^{2j}}{2} \cdot \delta\|v_k\|^2 + \frac{M}{6}\beta^{3j}\|v_k\|^3.                                                                                                           
        \end{aligned}
    \end{equation}
    Therefore, $\beta^j > \frac{3\delta}{(M+\gamma)\|v_k\|}$ holds, which further implies that
    $$
        j < \log_\beta\left(\frac{3\delta}{(M+\gamma)\|v_k\|}\right).
    $$
    However, $j_N := \left \lceil \log_\beta\left(\frac{3\delta}{M+\gamma}\right) \right\rceil \geq \log_\beta\left(\frac{3\delta}{(M+\gamma)\|v_k\|}\right)$ due to $\|v_k\| \leq 1$. 
    This means that the inequality \eqref{ineq:line-search-iter-est} does not hold when $j = {j_N}$, and thus
    the condition in Line 4 is satisfied in this case. Therefore, the iteration number of backtracking subroutine $j_k$ is upper bounded by $j_N$, and the function value decreases as
    $$
        \begin{aligned}
            f(x_k + \eta_k d_k) - f(x_k) & \leq -\frac{\gamma}{6}\beta^{3j_k}                 \\
                                         & = -\frac{\gamma \beta^3}{6} \beta^{3(j_k - 1)}     \\
                                         & \leq - \frac{9\gamma\beta^3\delta^3}{2(M+\gamma)},
        \end{aligned}
    $$
    where the last inequality comes from $\beta^{j_k - 1} \geq \frac{3\delta}{(M+\gamma)\|v_k\|}$ and $\|v_k\| \leq 1$.
\end{proof}

For the case of $|t_k| \geq \nu$, we have the following estimation on the iteration number of the line search and the value decrease on the objective function.
\begin{lemma}\label{lemma.t > delta line search decrease lemma}
    Suppose that \autoref{assm.lipschitz} holds and set $\nu \in (0, 1/2)$. If $|t_k| \ge \nu$, then let $d_k = v_k/t_k$. The backtracking line search terminates with $\eta_k = \beta^{j_k}$, and $j_k$ is upper bounded by
    $$
        j_N := \left \lceil \log_\beta\left(\frac{3\delta\nu}{M+\gamma}\right) \right\rceil,
    $$and the function value associated with the stepsize $\eta_k$ satisfies,
    \begin{equation}\label{eq. function value decrease line search t > delta}
        f(x_{k+1}) - f(x_k) \leq -\min \left\{\frac{\gamma\Delta^3}{6}, \frac{9\gamma\beta^3\delta^3}{2(M+\gamma)^3}\right\}.
    \end{equation}
\end{lemma}
\begin{proof}
    Similarly, suppose that the backtracking line search terminate with $\eta_k = 1$, we have
    $$
        \begin{aligned}
            f(x_k + \eta_kd_k) - f(x_k) & \leq -\frac{\gamma}{6}\eta_k^3\|d_k\|^3 \\
                                        & \leq -\frac{\gamma}{6}\Delta^3,
        \end{aligned}
    $$
    where the last inequality comes from $\|d_k\| > \Delta$. If $\eta_k = 1$ does not lead to sufficient decrease, then for any $j \geq 0$ where the condition in Line 4 is not met, we have
    \begin{equation}\label{ineq:line-search-iter-est2}
        \begin{aligned}
            -\frac{\gamma}{6}\beta^{3j}\|d_k\|^3 & < f(x_k + \beta^j d_k) - f(x_k)                                                                                                        \\
                                                 & \leq \beta^j \cdot g_k^T d_k + \frac{\beta^{2j}}{2} \cdot d_k^T H_k d_k + \frac{M}{6}\beta^{3j}\|d_k\|^3                               \\
                                                 & = (\beta^j - \frac{\beta^{2j}}{2}) \cdot (\delta_k - \theta_k) -\frac{\beta^{2j}}{2}\theta_k\|d_k\|^2 + \frac{M}{6}\beta^{3j}\|d_k\|^3 \\
                                                 & \leq -\frac{\beta^{2j}}{2}\delta\|d_k\|^2 + \frac{M}{6}\beta^{3j}\|d_k\|^3.
        \end{aligned}
    \end{equation}
    Therefore, $\beta^j \geq \frac{3\delta}{(M+\gamma)\|d_k\|}$ and it implies that
    $$
        j < \log_\beta\left(\frac{3\delta}{(M+\gamma)\|d_k\|}\right).
    $$
    Note that 
    \begin{equation}
    \|d_k\| = \|v_k\|/|t_k| = \frac{\sqrt{1-|t_k|^2}}{|t_k|} \leq \frac{1}{\nu},
\end{equation}
and    
    $j_N := \left \lceil \log_\beta\left(\frac{3\delta\nu}{M+\gamma}\right) \right\rceil \geq \log_\beta\left(\frac{3\delta}{(M+\gamma)\|d_k\|}\right)$, 
    This means that the inequality \eqref{ineq:line-search-iter-est2} does not hold when $j = {j_N}$, and thus
    the condition in Line 4 is satisfied in this case. Therefore, the iteration number of backtracking subroutine $j_k$ is upper bounded by $j_N$,
    and the function value decreases as
    $$
        \begin{aligned}
            f(x_k + \eta_k d_k) - f(x_k) & \leq -\frac{\gamma}{6}\beta^{3j_k}\|d_k\|^3             \\
                                         & = -\frac{\gamma \beta^3}{6} \beta^{3(j_k - 1)}\|d_k\|^3 \\
                                         & \leq - \frac{9\gamma\beta^3\delta^3}{2(M+\gamma)^3},
        \end{aligned}
    $$
    where the last inequality is due to  $\beta^{j_k - 1} \geq \frac{3\delta}{(M+\gamma)\|d_k\|}$.
\end{proof}

Combining the above two lemmas, we can conclude a unified descending property for homogenized negative curvature equipped with backtracking line search.
\begin{corollary}\label{corrollary. descend for line search}
    Suppose that \autoref{assm.lipschitz} holds and set $\nu \in (0, 1/2)$. Let the backtracking line search parameters $\beta, \gamma$ satisfy $\beta \in (0, 1)$ and $\gamma > 0$. Then after every outer iterate, the function value decreases as
    $$
        f(x_{k+1}) - f(x_k) \leq -\min\left\{\frac{\sqrt{3}\gamma}{16}, \frac{9\gamma\beta^3\delta^3}{2(M+\gamma)}, \frac{\gamma\Delta^3}{6}, \frac{9\gamma\beta^3\delta^3}{2(M+\gamma)^3}\right\}.
    $$
    and the inner iteration for backtracking line search is at most
    $$
            j_N \leq \max\left\{\left\lceil \log_\beta\left(\frac{3\delta}{M+\gamma}\right) \right\rceil, \left \lceil \log_\beta\left(\frac{3\delta\nu}{M+\gamma}\right) \right\rceil\right\} = \left \lceil \log_\beta\left(\frac{3\delta\nu}{M+\gamma}\right) \right\rceil.
    $$
\end{corollary}
\begin{remark}
An interesting implication of \autoref{corrollary. descend for line search} is that the amount of
value decrease of the objective function
is almost unaffected by the choice of $\nu$, the truncation parameter. The choice of $\nu$ only affects the number of iterations for backtracking line search, which is $O(\log_\beta(\delta\nu))$. Nevertheless, it is not suggested to take risks to choose small $\nu$, 
which will increase 
the line-search complexity as $\beta < 1$.
\end{remark}

\subsection{Analysis for the small value of $\|d_k\|$}
\label{subsection.convergence}
In this subsection, we consider the case of small valued $\|d_k\|$, i.e., $\|d_k\| \le \Delta$. We first observe that if $|t_k| < \nu$, it follows that $\|v_k\| = \sqrt{1 - |t_k|^2} > \sqrt{1 - \nu^2} \geq \sqrt{3}/2 > \Delta$ due to $[v_k; t_k]$ is a unit vector, $\nu \in (0, 1/2)$ and $\Delta = \Theta(\sqrt{\epsilon})$, and hence it should be classified into the large value case discussed in the previous section. Thus, we only need to focus on the scenario where $|t_k| \geq \nu$ in this part of the discussion. We show that if $\|d_k\| = \|v_k/t_k\| \le \Delta$, then the next iterate $x_{k+1} = x_k + d_k$ is already an $\epsilon$-approximate second-order stationary point. Therefore, we can terminate the algorithm after one iteration in the small valued case. To prove the result, we provide an upper bound of $\|g_k\|$ for preparation.
\begin{lemma}
    \label{lemma.upper bound of g_k}
    Suppose that \autoref{assm.lipschitz} holds. If $g_k \neq 0$, and $\|d_k\| \leq \Delta \leq \sqrt{2}/2$, then we have
    \begin{equation}\label{eq.upper bound of g_k}
        \|g_k\| \leq 2(L+\delta) \Delta.
    \end{equation}
\end{lemma}
\begin{proof}
    By \autoref{lemma.relation of theta delta Hk}, we have $\theta_k - \delta > 0 $. With the equation \eqref{eq.homoeig.soc} stated in \autoref{lemma.optimal condition of subproblem} and Schur complement, we have
    Moreover, with equation \eqref{eq.homoeig.foc t neq 0} in \autoref{corollary. foc}, we can give an upper bound of $\theta_k - \delta$, that is,
    \begin{equation}\label{eq.bound of theta - delta}
        \theta_k - \delta = -g_k^T d_k \leq \|g_k\| \|d_k\| \leq \Delta \|g_k\|.
    \end{equation}
    Denote $h(t) = t^2 + \left(  g_k^T H_k g_k / \|g_k\|^2 + \delta  \right) t - \|g_k\|^2$. It is easy to see that the equation $h(t) = 0$ must have two real roots with opposite signs. Let its positive root be $t_2$. By $ \theta_k - \delta > 0$, we have $ \theta_k - \delta \geq t_2 $. Therefore, we must have
    \begin{equation*}
        h(\Delta \|g_k\|) =
        \ \Delta^2\|\gk\|^2+\left(\frac{g_k^T H_k g_k}{\|g_k\|^2} + \delta \right)\Delta\|\gk\|-\|\gk\|^2\geq 0.
    \end{equation*}
    After some algebra, we obtain
    \begin{align}
        \nonumber\|\gk\|   & \leq \frac{\left( g_k^T H_k g_k / \|g_k\|^2 + \delta \right) \Delta}{1-\Delta^2} \\
        \nonumber          & \leq \frac{(L+\delta)\Delta}{1-\Delta^2}                                         \\
        \label{eq.bound g} & \leq 2(L+\delta) \Delta.
    \end{align}
    The second inequality holds due to the $L$-Lipschitz continuity of $\nabla f(x)$, which implies that $H_k \preceq L I$, and further $g_k^T H_k g_k / \|g_k\|^2 \leq L$. The last inequality follows from $\Delta \leq \sqrt{2}/2$.
\end{proof}

When $\|d_k\| \leq \Delta$, we let the stepsize $\eta_k = 1$ and proceed the iteration by $x_{k+1} = x_k + d_k$. The following lemma shows that the norm of the gradient at $x_{k+1}$ can be upper bounded, while the smallest eigenvalue of the Hessian at $x_{k+1}$ has a lower bound.

\begin{lemma}\label{lemma.norm of gk+1}
    Suppose that \autoref{assm.lipschitz} holds. If $g_k \neq 0$, and $\| d_k \| \leq \Delta$, then let $\eta_k = 1$, we have
    \begin{align}
        \|g_{k+1}\| & \leq  2(L+\delta) \Delta^3 + \frac{M}{2} \Delta^2 + \delta \Delta, \label{eq.small norm of gk+1} \\
        H_{k+1}     & \succeq - \left(  2(L+\delta)\Delta^2 + M\Delta + \delta \right) I \label{eq.psd of Hk+1}
    \end{align}
\end{lemma}
\begin{proof}
    We first prove \eqref{eq.small norm of gk+1}. By the optimality condition \eqref{eq.homoeig.foc t neq 0} in \autoref{corollary. foc}, we have
    \begin{equation*}
        H_k d_k + g_k = -\theta_k d_k,
    \end{equation*}
    and with \eqref{eq.bound of theta - delta}, we have
    \begin{equation*}
        \theta_k \|d_k\| \leq \left( \delta + \Delta \|g_k\| \right) \|d_k\|
    \end{equation*}
    Thus, it holds that
    \begin{equation}
        \label{eq:bridge1}
        \begin{split}
            \|H_k d_k + g_k \| = \theta_k \| d_k\| \leq \delta \Delta +  \|g_k\| \Delta^2.
        \end{split}
    \end{equation}

    Now we bound the norm of $\|g_{k+1}\|$ and obtain,
    \begin{subequations}
        \begin{align}
            \|g_{k+1}\| & \leq \|g_{k+1} - H_k d_k - g_k\| + \|H_k d_k + g_k\| \notag                                                            \\
                        & \leq \frac{M}{2}\|d_k\|^2 + \delta \Delta +  \|g_k\| \Delta ^2 \label{subeq. hessian lip}                              \\
                        & \leq \frac{M}{2} \Delta^2 + \delta \Delta + 2(L+\delta)\Delta \cdot \Delta^2 \label{subeq. convergence gk upper bound} \\
                        & =  2(L+\delta) \Delta^3 + \frac{M}{2} \Delta^2 + \delta \Delta, \notag
        \end{align}
    \end{subequations}
    where \eqref{subeq. hessian lip} holds due to the $M$-Lipschitz continuity of $\nabla^2 f(x)$ as well as equation \eqref{eq:bridge1}, and \eqref{subeq. convergence gk upper bound} follows from \autoref{lemma.upper bound of g_k}.

    Second, we prove \eqref{eq.psd of Hk+1}. Note that the optimality condition \eqref{eq.homoeig.soc} in \autoref{lemma.optimal condition of subproblem} implies that
    \begin{equation*}
        H_k + \theta_k \cdot I \succeq 0.
    \end{equation*}
    With \eqref{eq.bound of theta - delta} and \eqref{eq.bound g}, we further obtain
    \begin{align}
        \nonumber   H_k      & \succeq - \theta_k I                      \\
        \nonumber            & \succeq -(\Delta\|g_k\|+\delta)I          \\
        \label{eq.psd pf Hk} & \succeq -2(L+\delta)\Delta^2I - \delta I.
    \end{align}

    We now turn to bound $H_{k+1}$ and have that
    \begin{align}
        \nonumber    H_{k+1} & \succeq H_k - \|H_{k+1} - H_{k}\| I \\
        \nonumber            & \succeq H_k - M \|d_k\| I           \\
                             & \succeq H_k - M \Delta I
    \end{align}
    where the second inequality holds by the $M$-Lipschitz continuity of $\nabla^2 f(x)$, and the last inequality follows from $\|d_k\| \leq \Delta$. Combining with \eqref{eq.psd pf Hk}, we arrive at
    \begin{equation}
        H_{k+1} \succeq -2(L+\delta)\Delta^2I - \delta I - M\Delta I.
    \end{equation}
    The proof is then completed.
\end{proof}

\subsection{The global convergence result}
Putting the above pieces together, we present the formal convergence theorems of HSODM in both the fix-radius and line search strategies in \autoref{thm.fix radius global convergence rate} and \autoref{thm.line search global convergence rate}, respectively. It shows that our method achieves $O(\epsilon^{-3/2})$ iteration complexity to find an $\epsilon$-approximate second-order stationary point by properly choosing the perturbation parameter $\delta$ and the radius $\Delta$.
\begin{theorem}
    \label{thm.fix radius global convergence rate}
    Suppose that \autoref{assm.lipschitz} holds.  Let $\delta = \sqrt{\epsilon}$, $\Delta = 2\sqrt{\epsilon} / M$ and $\nu \in (0, 1/2)$, then the homogeneous second-order descent method (HSODM) with the fix-radius strategy terminates in at most $O\left(\epsilon^{-3/2}\right)$ steps, and the next iterate $x_{k+1}$ is a second-order stationary point.
\end{theorem}
\begin{proof}
    Since we take $\delta = \sqrt{\epsilon}$ and $\Delta = 2\sqrt{\epsilon} / M $, by \autoref{lemma.t < delta fix radius decrease lemma} and \autoref{lemma.t > delta fix radius decrease lemma},  we immediately obtain that the function value decreases at least $O(\epsilon^{-3/2})$ for the large step case, i.e.,
    \begin{equation}
        \label{decrease subsititue delta and Delta}
        f(x_{k+1}) - f(x_k) \leq -\frac{2}{3M^2} \epsilon^{3/2}.
    \end{equation}

    When the algorithm terminates, by \autoref{lemma.norm of gk+1}, we have
    \begin{equation}
        \|g_{k+1}\|\leq O(\epsilon) \quad \text{and} \quad  \lambda_{\min} \left(H_{k+1}\right) \geq \Omega(-\sqrt{\epsilon}).
    \end{equation}
    Therefore, the next iterate $x_{k+1}$ is already a second-order stationary point.

    Note that the total decreasing amount of the objective function value cannot exceed $f(x_1) - f_{\inf}$. It leads to the that the number of iterations for large step cases is upper bounded by
    \begin{equation*}
        O\left(\frac{3M^2}{2}\left(f(x_1) - f_{\inf}\right)\epsilon^{-3/2}\right),
    \end{equation*}
    which is also the iteration complexity of our algorithm.
\end{proof}

\begin{theorem}
    \label{thm.line search global convergence rate}
    Suppose that \autoref{assm.lipschitz} holds. Let $\delta = \sqrt{\epsilon}$, $\Delta = 2\sqrt{\epsilon} / M$ and $\nu \in (0, 1/2)$, and the backtracking line search parameters $\beta, \gamma$ satisfy $\beta \in (0, 1)$ and $\gamma > 0$. Then the homogeneous second-order descent method (HSODM) with the backtracking line search terminates in at most $O\left(\epsilon^{-3/2}\log_\beta(\epsilon) \right)$ steps, and the next iterate $x_{k+1}$ is a second-order stationary point. Specifically, the number of iterations is bounded by,
    \begin{equation*}
        O\left(\max\left\{\frac{2(M+\gamma)}{9\gamma\beta^3},\frac{3M^3}{4\gamma},\frac{2(M+\gamma)^3}{9\gamma\beta^3}\right\}\left \lceil \log_\beta\left(\frac{3\sqrt{\epsilon}\nu}{M+\gamma}\right) \right\rceil\left(f(x_1) - f_{\inf}\right)\epsilon^{-3/2}\right).
    \end{equation*}
\end{theorem}
\begin{proof}
    Since we take $\delta = \sqrt{\epsilon}$ and $\Delta = 2\sqrt{\epsilon} / M $, by \autoref{corrollary. descend for line search}, we immediately obtain that the function value decreases at least $O(\epsilon^{-3/2})$ for the large step case, i.e.,
    \begin{equation}\label{decrease subsititue delta and Delta}
        \begin{aligned}
            f(x_{k+1}) - f(x_k) & \leq -\min\left\{\frac{\sqrt{3}\gamma}{16}, \frac{9\gamma\beta^3\delta^3}{2(M+\gamma)}, \frac{\gamma\Delta^3}{6}, \frac{9\gamma\beta^3\delta^3}{2(M+\gamma)^3}\right\} \\
                                & \leq -\min\left\{\frac{9\gamma\beta^3}{2(M+\gamma)},\frac{4\gamma}{3M^3},\frac{9\gamma\beta^3}{2(M+\gamma)^3}\right\}\epsilon^{3/2},
        \end{aligned}
    \end{equation}
    and the inner iteration for backtracking line search is at most
    $$
            j_N \leq \left \lceil \log_\beta\left(\frac{3\delta\nu}{M+\gamma}\right) \right\rceil = \left \lceil \log_\beta\left(\frac{3\sqrt{\epsilon}\nu}{M+\gamma}\right) \right\rceil.
    $$
    When the algorithm terminates, by \autoref{lemma.norm of gk+1}, we have
    \begin{equation}
        \|g_{k+1}\|\leq O(\epsilon) \quad \text{and} \quad  \lambda_{\min} \left(H_{k+1}\right) \geq \Omega(-\sqrt{\epsilon}).
    \end{equation}
    Therefore, the next iterate $x_{k+1}$ is already a second-order stationary point.

    Note that the total decreasing amount of the objective function value cannot exceed $f(x_1) - f_{\inf}$. It leads to that the number of iterations for large step case is upper bounded by
    \begin{equation*}
        O\left(\max\left\{\frac{2(M+\gamma)}{9\gamma\beta^3},\frac{3M^3}{4\gamma},\frac{2(M+\gamma)^3}{9\gamma\beta^3}\right\}\left \lceil \log_\beta\left(\frac{3\sqrt{\epsilon}\nu}{M+\gamma}\right) \right\rceil\left(f(x_1) - f_{\inf}\right)\epsilon^{-3/2}\right),
    \end{equation*}
    which is also the iteration complexity of our algorithm. Since $\beta < 1$, this completes the proof.
\end{proof}

Since $\delta = \sqrt{\epsilon}$, we see that the line-search version has an extra overhead of $O(\log_\beta\epsilon)$ compared to the fixed-radius strategy. In practice, the line-search version can choose steps that are much larger than $\Delta$ and hence has a fast rate of convergence. This benefit can be observed in the \autoref{sec.experiment}.

\begin{remark}
In terms of the eigenvalue solver to the subproblem
\eqref{eq.homo subproblem}, we can use the random starting Lanczos algorithm \cite{kuczynski_estimating_1992}. Similar to \cite{carmon_accelerated_2018,royer_complexity_2018}, we will have a unit vector sufficiently close to the leftmost eigenvector of $F_k$ with probability $1-p$ in  $O(\log ((n + 1) / p)\epsilon^{-1/4})$ iterations with $O(n^2)$ per-iteration cost. Therefore, the total computational cost of subproblem \eqref{eq.homo subproblem} is $O(n^2\cdot \log ((n + 1) / p)\epsilon^{-1/4})$, which is less dimension-dependent compared to $O(n^3)$ required by solving Newton systems in the subproblem of standard second-order methods.
This gives a total complexity of $O(\epsilon^{-7/4}\log({(n+1)/p}))n^2$ of HSODM. Since similar results are widely recognized, see \cite{carmon_accelerated_2018,royer_complexity_2018}, we do not provide the detail of these results here. The readers can find relevant materials therein.
\end{remark}


% \begin{corollary}[Lemma 9, \citet{royer_complexity_2018}]
%     Suppose the random starting Lanczos algorithm \cite{kuczynski_estimating_1992} is used to compute leftmost eigenvector of $F_k$, then for any $\varepsilon > 0$ and $p \in (0,1)$, the procedure outputs a unit vector $v \in \mathbb{R}^{n+1}$ by a probability of $1-p$ such that
%     $$v^T F_k v \leq \lambda_{\min }(F_k)+\varepsilon$$
%     in at most
%     $$\min \left\{n + 1, \frac{\log \left(n + 1/ p^2\right)}{2 \sqrt{2}} \sqrt{\frac{M}{\varepsilon}}\right\}$$
%     iterations.
% \end{corollary}
% Using the above result, setting $\varepsilon = \sqrt{\epsilon}$ results in the an $O(\epsilon^{-1/4})$ for the each eigenvalue computation. This immediately implies that HSODM matches the complexity of the first-order methods using negative curvature.

\section{Local Convergence Rate}\label{sec.local}

In this section, we provide the local convergence analysis of HSODM. In particular, when $x_k$ is sufficiently close to a second-order stationary point $x^*$,  we shall show that the step size $\eta_k$ always equals to $1$ and the line search procedure is not required.
Consequently, HSODM achieves a local quadratic convergence rate by setting the perturbation parameter $\delta = 0$ for the subsequent iterations.


% Instead of terminating \autoref{alg.main alg} as soon as $\epsilon$-approximate second-order conditions \eqref{eq.approxfocp} and \eqref{eq.approxsocp} are met, we set the perturbation parameter $\delta=0$ and let HSODM continue. We show that the HSODM achieves a local quadratic convergence rate.

We first make the standard assumption \cite{conn_trust_2000,nocedal_numerical_2006,nesterov_lectures_2018} to facilitate the local convergence analysis.
\begin{assumption}
    \label{assm.local}
    Assume that HSODM converges to a strict local optimum $x^*$ satisfying that $\nabla f(x^*)=0$ and $\nabla^2 f(x^*) \succ 0$.
\end{assumption}
\begin{remark}
    From the above \autoref{assm.local}, we immediately know that there exists a small neighborhood such that for all $x \in B(x^*, R)$, $\nabla^2 f(x) \succeq \mu \cdot I$ for some $\mu > 0$. In other words, $x_k$ arrives at the neighborhood of $x^*$ for sufficiently large $k$, hence both $\Hk
    $ and $\Hk+\theta_k I$ are nonsingular.
\end{remark}

To prove the local convergence rate, we prove the following auxiliary results for preparation.
\begin{corollary}
    \label{coro. large k t neq 0}
    Suppose that \autoref{assm.local} holds, then $t_k \neq 0$ for sufficiently large $k$.
\end{corollary}
\begin{proof}
    We prove by contradiction. Suppose that $t_k = 0$. Then by \autoref{corollary. foc}, $(-\theta_k, v_k)$ is the eigenpair of $H_k$, implying that,
    $$
        \lambda_{\min}(H_k) \leq -\theta_k.
    $$
    Recall that in \autoref{lemma.optimal condition of subproblem}, we have $\theta_k > 0$, hence $\lambda_{\min}(H_k) < 0$.
    % On the other hand, with the M-Lipschitz continuity of $\nabla^2 f$, we have
    % $$
    % -M  \|\xk - x^*\| \cdot I\preceq \Hk - \nabla^2 f (x^*) \preceq M \|\xk - x^*\|\cdot I,
    % $$
    % which leads to $\Hk \succeq \left(\mu - M \|\xk - x^*\|\right) I \succeq \frac{\mu}{2} \cdot I \succeq 0$ for sufficiently large $k$ by \autoref{assm.local}. 
    % %Thus, we must have $\Hk \succeq 0$ as $\xk \to x^*$ for sufficiently large $k$.
    This makes a contradiction to $H_k \succ 0$. Then the proof is completed.
\end{proof}

The following lemma demonstrates that the step $d_k$ generated by the HSODM eventually reduces to the small valued case for sufficiently large $k$. Consequently, we choose $\eta_k = 1$ and update the iteration by $x_{k+1} = x_k + d_k$ as shown in \autoref{subsection.convergence}. We remark that it is similar to the case of the classical Newton trust-region method (see \cite[Theorem 4.9]{nocedal_numerical_2006}), where the updates become asymptotically similar to the pure Newton step.
\begin{lemma}\label{lemma. large k small step}
    For sufficiently large $k$, we have $\|\dk\|\leq \Delta$.
\end{lemma}
\begin{proof}
    Due to $t_k \neq 0$, by equation \eqref{eq.homoeig.foc t neq 0} in \autoref{corollary. foc}, we have
    \begin{equation*}
        \dk = -(\Hk+\theta_k I)^{-1}\gk,
    \end{equation*}
    and further
    \begin{align}
        \nonumber\|\dk\|      & \leq \| (\Hk+\theta_k I)^{-1}\| \|\gk\| \\
        \nonumber             & \leq \frac{\|\gk\|}{\mu +\theta_k}      \\
        \label{eq.boundnormv} & \leq \frac{\|\gk\|}{\mu}.
    \end{align}
    The above inequalities holds because of $\Hk \geq \mu I $ and $\theta_k>0$. Note that with \autoref{assm.local}, $\|g_k\| \to 0$ as $k\to \infty$, then there exist a sufficiently large $K \ge 0$, such that
    \begin{equation}\label{eq.localboundg}
        \|\gk\|\leq \Delta \mu, \forall k \ge K %\frac{2\mu\sqrt{\epsilon}}{M}.
    \end{equation}
    Combining \eqref{eq.boundnormv}, we conclude that $\|\dk\|\leq \Delta$ will be satisfied.
\end{proof}
In the local phase, we set the perturbation parameter $\delta=0$ ,
\begin{equation}
    \label{eq.homo local subproblem}
    \begin{aligned}
        \min_{\|[v; t]\| \le 1} \psi_k(v, t; 0) := ~ &
        \begin{bmatrix}
            v \\ t
        \end{bmatrix}^T
        \begin{bmatrix}
            \Hk   & \gk \\
            \gk^T & 0
        \end{bmatrix}
        \begin{bmatrix}
            v \\ t
        \end{bmatrix},
    \end{aligned}
\end{equation}
We also denote by $[v_k; t_k]$ the optimal solution to \eqref{eq.homo local subproblem}. Gathering the above results together, we are ready to prove the following theorem.
\begin{theorem}
    \label{theorem:local-quadratic}
    Suppose that \autoref{assm.lipschitz} and \autoref{assm.local} hold. For sufficiently large $k$, the HSODM converges to \(x^*\) quadratically, that is,
    $$
        \|{x_{k+1}-x^*} \| \le O(\|\xk - x^*\|^{2}).
    $$
\end{theorem}
\begin{proof}
    By \autoref{coro. large k t neq 0}, we have $t_k \neq 0$. Since we take $\delta = 0$, then with equation \eqref{eq.homoeig.foc t neq 0} in \autoref{corollary. foc}, we have
    \begin{equation*}
        g_k^T d_k = -\theta_k \quad \text{and} \quad (H_k + \theta_k I) d_k = - g_k,
    \end{equation*}
    implying that
    \begin{align}
        \nonumber\| H_k^{-1} g_k + d_k \| & =  \| - \theta_k H_k^{-1} d_k \|                    \\
        \nonumber                         & \leq  \| H_k^{-1}  \| \cdot \|\theta_k \| \| d_k \| \\
        \label{eq:bridge5}                & \leq \frac{1}{\mu} \| g_k\| \| d_k \|^2
    \end{align}

    By \autoref{lemma. large k small step}, we have $ x_{k+1} = x_k + \dk $. Therefore,
    \begin{subequations}
        \begin{align}
            \nonumber   \|x_{k+1} - x^*\| & = \|x_{k} + d_k + H_k^{-1} g_k - H_k^{-1} g_k - x^* \|              \\ \nonumber
                                          & \leq \|x_{k} - H_k^{-1}g_k - x^*\| + \|H_k^{-1}g_k + \dk\|          \\
            \label{eq.local.opt1}         & \leq \frac{M}{\mu}\|x_k - x^*\|^2 + \frac{1}{\mu} \|g_k\| \|\dk\|^2 \\
            \label{eq.local.opt2}         & \leq \frac{M}{\mu}\|x_k - x^*\|^2 + \Delta \|d_k\|^2,
            % \label{eq.local.opt3} & \leq \frac{M}{\mu}\|x_k - x^*\|^2 + \frac{L}{\mu}\|x_k - x^*\| \|d_k\|^2
        \end{align}
    \end{subequations}
    where \eqref{eq.local.opt1} holds due to the standard analysis of Newton's method and equation \eqref{eq:bridge5}, and \eqref{eq.local.opt2} follows from $ \|\gk\|\leq \Delta \mu $ as stated in \autoref{lemma. large k small step}. Moreover, we have
    \begin{align}
        \nonumber        \|\dk\|       & = \|  x_{k+1} - x^*  - \left( \xk - x^* \right) \|            \\
        \nonumber                      & \leq \| x_{k+1} - x^* \|+\|\xk-x^*\|                          \\
        \nonumber                      & \leq \frac{M}{\mu}\|\xk-x^*\|^2+\|\xk-x^*\|+\Delta \|d_k\|^2  \\
        \label{eq.relation d and dist} & \leq \frac{M}{\mu}\|\xk-x^*\|^2+\|\xk-x^*\|+\Delta^2 \|d_k\|.
    \end{align}
    By rearranging the terms and \autoref{lemma. large k small step} we have
    $$    (1-\Delta^2)\|\dk\| \leq O(\|\xk-x^*\|)+\|\xk-x^*\|,$$
    which leads to that $\|\dk\|\leq O(\|\xk-x^*\|)$. With \eqref{eq.local.opt2}, we conclude that
    \begin{equation}
        \begin{aligned}
            \|x_{k+1} - x^*\| & \le \frac{M}{\mu}\|x_k - x^*\|^2 + \Delta \|d_k\|^2 = O(\|x_k - x^*\|^2).
        \end{aligned}
    \end{equation}
    This completes the proof.
\end{proof}

\input{numerics}

\section{Conclusion}\label{sec.conclusion}

In this paper, we introduce a homogenized second-order descent method (HSODM) whose global rate of complexity is optimal among a certain broad class of second-order methods (see \cite{cartis_evaluation_2022}). The HSODM utilizes the homogenization trick to the quadratic model, which comes from the standard second-order Taylor expansion, such that the resulting homogenized quadratic form can be solved as an eigenvalue problem. We have shown that the homogenized idea is well-defined in both convex and nonconvex cases, where a negative curvature always exists. Using the model all along, one can safely stop at a small step to obtain an $\epsilon$-approximate second-order stationary point without switching to other methods.

We provide comprehensive experiments of HSODM on nonlinear optimization problems in the CUTEst benchmark. Two variants of HSODM outperformed other second-order methods in these experiments. It is interesting to investigate the inexactness of subproblems in HSODM and its corresponding complexity issues for future research. For example, the recent first-order methods explore the negative curvature inexactly, which we believe can easily be adopted in place of the Lanczos method for large-scale problems.



\clearpage
\addcontentsline{toc}{section}{References}
\bibliography{homo}
\bibliographystyle{plainnat}
\clearpage
\appendix

% \begin{landscape}
    \section{Detailed Computational Results for CUTEst Dataset}
    % \tiny
    \scriptsize
    \input{cutest.tex}
    \normalsize
% \end{landscape}

\end{document}

