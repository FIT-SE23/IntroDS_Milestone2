
\section{Numerical Results} \label{sec.experiment}
In this section,
% we provide the computational results of HSODM on a few classes of nonconvex optimization problems. To be specific, we include a set of nonconvex $L_2-L_p$ minimization problem that arises from compressed sensing and has long been one of great interests in the community. In addition, 
we include the CUTEst problems \cite{gould_cutest_2015} since they serve as a standard dataset to test the performance of algorithms for nonlinear problems. Because the HSODM belongs to the family of second-order methods, we focus on comparisons with Newton trust-region method, and adaptive cubic regularized Newton method \cite{cartis_adaptive_2011}.


\subsection{Implementation details}
We first note that a practical HSODM may not explicitly use the Hessian matrix $\Hk$. In the computation of $F_k \cdot [v; t]$ where $v\in\real^n, t\in \real$, we have
$$
F_k \cdot \begin{bmatrix}v \\ t\end{bmatrix} = \begin{bmatrix}
    \Hk \cdot v + t\cdot \gk \\
    \gk^T v - t\cdot \delta
\end{bmatrix}.
$$
From the above fact, we see HSODM may also utilize the Hessian-vector products $\Hk v$ as in inexact Newton-type methods \cite{cartis_adaptive_2011,curtis_inexact_2018}.
\paragraph{An adaptive version for HSODM with line search.}
Apart from the original form of HSODM \autoref{alg.main alg}, we add a few techniques for practical implementations.
We use the Lanczos method to solve homogenized subproblems with a given tolerance of the residual.
Not limiting to the backtrack line-search algorithm for theoretical analysis, in practice, the homogeneous direction should work well with any well-defined line search methods. In our implementation, we apply the Hager-Zhang line-search method with default parameter settings \cite{hager_algorithm_2006}.

\paragraph{The benchmark algorithms}

Standard benchmark algorithms include the
%  conjugate gradient method (\cg{}), limited memory BFGS method (\lbfgs{}) with the Hager-Zhang line-search algorithm \cite{zhang_nonmonotone_2004}, and 
Newton trust-region method (\newtontr) and adaptive cubic regularization (\arc{}). The implementation for \newtontr{} and line-search algorithms can be found in the third-party packages in \cite{mogensen2018optim}. For adaptive cubic regularization, we use the ARC package in \cite{orban_juliasmoothoptimizers_2019}. All the development is handled by a desktop of Mac OS with a 3.2 GHz 6-Core Intel Core i7 processor. 

% \input{l2lp.tex}
%%%%%%%%%%%%%%%%%
% cutest
\subsection{Unconstrained problems in CUTEst}
We next present the results on a selected subset of the CUTEst dataset. To set a comprehensive comparison, we provide the results of HSODM with readily Hessian matrices, named after \hsodm{}, and a version facilitated by Hessian-vector products (\hsodmhvp{}).
We set an iteration limit of $20,000$ and terminate criterion $\|\nabla f(\xk)\| \le 10^{-5}$ for all the tested algorithms; we check if this criterion is ensured else marked as failed. 
We focus on the unconstrained problems with the number of variables $n \in [4, 5000]$; we regard a problem as small if the problem dimension $n \le 200$. For each problem in the CUTEst, if it has different parameters, we choose the smallest instance that fits the criterion. Then we have $200$ instances in total for test. Note there are 15 instances, such as \texttt{EIGENBLS}, \texttt{NONMSQRT}, and \texttt{SSCOSINE}, where every algorithm fails, so it reduces to 185 valid instances. The complete result can be found in \autoref{tab.cutest.kt} and \autoref{tab.cutest.fx}.

\paragraph{Overall comparison of the algorithms.}
The following \autoref{tab.perf.geocutest} presents a summary of tested algorithms. In this table, we let $\mathcal K$ be the number of successful instances. Besides, we compute performance statistics based on scaled geometric means (\textsf{SGM}), including $\overline t_G,\overline k_G,\overline k_G^f,\overline k_G^g,\overline k_G^H$ as (geometric) mean running time, mean iteration number, mean function evaluations, mean gradient evaluations, and mean Hessian evaluations, respectively. The running time is scaled by 1 second, and other metrics are scaled by 50 evaluations or iterations accordingly. 
% We let $\overline t,\overline k,\overline k^f,\overline k^g,\overline k^H$ be the mean running time, mean iteration number, mean function evaluations, mean gradient evaluations, and mean Hessian evaluations, respectively. These average numbers are taken among successful instances. 
Note that the cubic regularization \arc{} and \hsodmhvp{} both use Hessian-vector products so that $\overline k_G^H = 0$ and the gradient evaluations in $\overline k_G^g$ actually include the number of Hessian-vector products. While we keep this value for reference, one may refer to $\overline k_G$ to fairly compare second-order oracle queries.

% Moreover, we also calculate the scaled geometric means that are marked with a subscript $G$, 
\begin{table}[h]
    \centering
    \caption{Performance in \textsf{SGM} of different algorithms on the CUTEst dataset.
        Note $\overline t_{G}, \overline k_{G}$ are scaled geometric means (scaled by 1 second and 50 iterations, respectively).
        If an instance is failed, its iteration number and solving time are set to $20,000$.
    }
    \label{tab.perf.geocutest}
    \begin{tabular}{lrrrrrr}
        \toprule
        method &  $\mathcal K$ &  $\overline t_G$ &  $\overline k_G$ &  $\overline k_G^f$ &  $\overline k_G^g$ &  $\overline k_G^H$ \\
        \midrule
        \newtontr &        155.00 &            15.41 &           216.59 &             211.99 &             219.58 &              203.82 \\
        \hsodm    &        170.00 &             4.13 &            80.22 &             159.76 &             180.04 &              80.22 \\
        \hsodmhvp &        171.00 &             5.25 &           110.61 &             193.07 &            1080.57 &               0.00  \\
        \arc      &        167.00 &             5.32 &           185.03 &             185.03 &             888.35 &               0.00  \\
        \bottomrule
    \end{tabular}
    
\end{table}

Apart from metrics measured by \textsf{SGM}, we use the performance profile on iteration number as defined in \cite{dolan_benchmarking_2002}.
In essence, the performance profile at point $\alpha$ in \autoref{fig.perfprof} of an algorithm indicates the probability of successfully solved instances within $2^\alpha$ times the best iteration number amongst competitors.
\begin{figure}[h]
    \centering
    \subfloat[For small problems $n \le 200$]{
    \includegraphics[width=0.45\linewidth]{figs/16829649-rho_k.png}
    }
    ~
    \subfloat[For large problems $n > 200$]{
    \includegraphics[width=0.45\linewidth]{figs/16830272-rho_k.png}
    }
    \caption{Performance profiles for CUTEst problems. We report on the proportion of problems that are successful for each algorithm. }
    \label{fig.perfprof}
\end{figure}

% \begin{table}
%     \scriptsize
%     \begin{tabular}{lrrrrrrrrrrr}
%         \toprule
%         method    & $\mathcal K$ & $\overline t$ & $\overline k$  & $\overline k^f$ & $\overline k^g$ & $\overline k^H$ & $\overline t_G$ & $\overline k_G$ & $\overline k_G^f$ & $\overline k_G^g$ & $\overline k_G^H$ \\
%         \midrule
%         \arc      & 94.00        & 0.13          & 154.31         & 154.31          & 134.69          & 1774.66         & 0.31            & 71.65           & 71.65             & 68.07             & 333.84            \\
%         \cg       & 95.00        & 0.45          & 665.28         & 2320.57         & 2320.57         & 0.00            & 0.22            & 155.87          & 366.95            & 366.95            & -0.00             \\
%         % \drsom    & 87.00        & 0.09          & 310.64         & 1356.51         & 332.22          & 0.00            & 0.26            & 163.20          & 528.66            & 195.86            & -0.00             \\
%         \hsodm    & 92.00        & 0.18          & \textbf{18.21} & 69.08           & 85.28           & 18.21           & 0.26            & \textbf{24.68}  & \textbf{70.29}    & 82.64             & 24.66             \\
%         \lbfgs    & 97.00        & 0.19          & 286.42         & 803.38          & 803.38          & 0.00            & 0.10            & 84.38           & 203.55            & 203.55            & -0.00             \\
%         \newtontr & 88.00        & 0.11          & 75.94          & 76.94           & 76.94           & 63.52           & 0.41            & 92.40           & 44.87             & 94.21             & 41.05             \\
%         \bottomrule
%     \end{tabular}
%     \caption{Performance of different algorithms on the CUTEst dataset.
%         If an instance is the failed, its iteration number and solving time are set to $20,000$.
%     }\label{tab.perf.cutest}
% \end{table}
\normalsize

The results from these preliminary implementations show that \hsodm{} and \hsodmhvp{} outperformed the standard second-order methods, including \newtontr{} and \arc{} on average. \hsodmhvp{} solved the most instances and \hsodm{} had the best iteration complexity and running time in terms of $\overline t_G, \overline k_G$ among competing algorithms. The HVP variant \hsodmhvp{} used slightly more gradient and function evaluations than \arc{} that result from extra overhead by line searches. However, since \hsodmhvp{} succeeded in more instances, the running time was very close to that of \arc{}.
% By comparing function and gradient evaluations, \hsodm{} is also very competitive among these methods due to the homogeneous model and line-search method. 
In terms of performance profile, we see both \hsodm{} and \hsodmhvp{} clearly had an edge in iteration numbers for both small and large problems. It is also interesting to see \hsodm{} and also \hsodmhvp{} (see \texttt{EXTROSNB, GENROSE}), \newtontr{} (see \texttt{ARGLINC}) and \arc{}  (see \texttt{OSCIGRAD}) all had their own best instances. 
Nevertheless, we would like to claim the overall strength of \hsodm{} and \hsodmhvp{} using the homogenized system.
% However, \hsodm{} stills permits an advantage over \lbfgs{} if one seeks to find solutions with high precision (for example, $\|\gk\| \le 10^{-8}$).
%Whether this is a consequence of the global rate is not clear. 
%Since the current implementations for \hsodm{} only apply the standard trust-region update rules, i.e., the ratio-based acceptance and linear radius adjustments, we
%are confident that using the provably better frameworks like those in \cite{cartis_adaptive_2011,curtis_trust_2017} would have a promising improvement to \hsodm{}.
