\documentclass{article}
\usepackage{amssymb,amsmath,amsthm,enumerate}
\usepackage[utf8]{inputenc}
\usepackage{array}
\usepackage{mathtools}
\usepackage[parfill]{parskip}
\usepackage{graphicx}
\usepackage{geometry}
\usepackage{mathrsfs}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{bm}
\usepackage{amsfonts,amscd}
\usepackage{courier}
\usepackage[]{units}
\usepackage{listings}
\usepackage{multicol,multirow}
\usepackage{longtable}
\usepackage{lscape}
\usepackage{tcolorbox}
\usepackage{float}
\usepackage{physics}
\usepackage{tikz}
\usepackage[ruled, linesnumbered]{algorithm2e}
\usepackage{authblk}
\usepackage{booktabs}
\usepackage{amsthm}
\usepackage[colorlinks,citecolor=blue,urlcolor=blue,linkcolor=blue]{hyperref}

% \usepackage[numbers]{natbib}
\usepackage[
citestyle=numeric-comp,clearlang=true,bibstyle=numeric,natbib=true,
maxbibnames=10,maxcitenames=2,
doi=false,isbn=false,url=false,eprint=false,
]{biblatex}
\DeclareSourcemap{
  \maps[datatype=bibtex]{
    \map{
      \step[fieldset=issn, null]
      \step[fieldset=language, null]
      \step[fieldset=urldate, null]
    }
  }
}

\addbibresource{homo.bib}

\renewcommand{\baselinestretch}{1.2}

\newtheorem{theorem}{Theorem}
\newtheorem{proposition}{Proposition}
\newtheorem{corollary}{Corollary}
\newtheorem{lemma}{Lemma}
\newtheorem{assumption}{Assumption}
\newtheorem{definition}{Definition}
\newtheorem{remark}{Remark}


% new commands
\newcommand{\argmin}{\arg\min}
\definecolor{salmon}{rgb}{1.0, 0.55, 0.41}
\newcommand{\chenyu}[1]{\textcolor{salmon}{[Chenyu: #1]}}
\newcommand{\Chang}[1]{\textcolor{cyan}{[Chang: #1]}}
\newcommand{\cz}[1]{\textcolor{pink}{[CZ: #1]}}
\newcommand{\assumptionautorefname}{Assumption}
\newcommand{\lemmaautorefname}{Lemma}
\newcommand{\corollaryautorefname}{Corollary}
\renewcommand{\algorithmautorefname}{Algorithm}
\renewcommand{\subsectionautorefname}{Section}
% iteration macros
%
\newcommand{\lm}{\lambda_\mathsf{max}}
%\newcommand{\trace}{\mathsf{trace}}
\newcommand{\diag}{\mathsf{diag}}
%\newcommand{\rank}[1]{\mathsf{rank}(#1)}
\newcommand{\model}[1]{(\mathsf{#1})}
\newcommand{\mx}{\mathsf{\max}~}
\newcommand{\mn}{\mathsf{\min}~}
\newcommand{\st}{\mathrm{s.t.}~}
\newcommand{\ex}{\mathsf E}
\newcommand{\varr}{\mathsf{Var}}
\newcommand{\dx}{\;\bm dx}
\newcommand{\pr}{\mathsf P}
\newcommand{\id}{\mathsf I}
\newcommand{\bp}{\mathbb P}
\newcommand{\be}{\mathbb E}
\newcommand{\bi}{\mathbb I}
\newcommand{\bxi}{{\bm \xi}}
\newcommand{\dif}{\mathsf{d}}
\newcommand{\minp}[2]{\min\{#1, #2\}}
\newcommand{\intp}{\mathsf{int}}
\newcommand{\apex}{\mathsf{apex}}
\newcommand{\conv}{\mathsf{conv}}
\newcommand{\red}[1]{\textcolor{red}{#1}}
\newcommand{\redsf}[1]{\textcolor{red}{\textsf{#1}}}
%\newcommand{\real}{\mathbb{R}}
\newcommand{\prox}{\mathsf{prox}}

%
\newcommand{\xk}{x_k}
\newcommand{\Hk}{H_k}
\newcommand{\Qk}{Q_k}
\newcommand{\gk}{g_k}
\newcommand{\dk}{d_k}
\newcommand{\gkn}{g_{k+1}}
\newcommand{\xkn}{x_{k+1}}
\newcommand{\dkn}{d_{k+1}}
\newcommand{\lk}{\lambda_k}
\newcommand{\ghg}{\gk^TH_kg_k}
\newcommand{\dhd}{\dk^TH_k\dk}
\newcommand{\dhtd}{\dk^T\left(H_k + \lambda_kI\right)\dk}
\newcommand{\hkap}{\tilde{H}_k}
\newcommand{\vk}{V_k}
\newcommand{\spa}{\mathsf{span}}
\newcommand{\spak}{\mathcal L_k}
\newcommand{\he}{\theta}
\newcommand{\hv}{\xi}
\newcommand{\her}{\theta^{R}}
\newcommand{\hvr}{\xi^{R}}
\newcommand{\rayxi}{\mathcal R(\Hk, \xi)}
\newcommand{\rayg}{\mathcal R(\Hk, \gk)}
\newcommand{\raygr}{\mathcal R(\Hk^R, \gk)}

% alias
\newcommand{\hsodm}{\textrm{HSODM}}
\newcommand{\drsom}{\textrm{DRSOM}}
\newcommand{\drsomh}{\textrm{DRSOM-H}}
\newcommand{\lbfgs}{\textrm{LBFGS}}
\newcommand{\newtontr}{\textrm{Newton-TR}}
\newcommand{\cg}{\textrm{CG}}
\newcommand{\arc}{\textrm{ARC}}
\newcommand{\itrace}{\textrm{TRACE}}


\author[1]{\small Chuwen Zhang\thanks{This research is partially supported by the National Natural Science Foundation of China (NSFC) [Grant NSFC-72150001, 72225009,11831002]}}
\author[1]{\small Dongdong Ge}
\author[1]{\small Chang He}
\author[1]{\small Bo Jiang}
\author[1]{\small Yuntian Jiang}
\author[1]{\small Chenyu Xue}
\author[2]{\small Yinyu Ye}


\affil[$\dagger$]{\footnotesize School of Information Management and Engineering\\ Shanghai University of Finance and Economics}
\affil[$\ddagger$]{\footnotesize Department of Management Science and Engineering, Stanford University}

\title{A Homogeneous Second-Order Descent Method for Nonconvex Optimization}

\begin{document}

\maketitle
\begin{abstract}
    In this paper, we introduce a \emph{Homogeneous Second-Order Descent Method} (HSODM) using the homogenized quadratic approximation to the original function. By finding the leftmost eigenvector of a gradient-Hessian integrated matrix at each iteration, our method has a global convergence rate of $O(\epsilon^{-3/2})$ to find an $\epsilon$-approximate second-order stationary point. Furthermore, HSODM has a local quadratic convergence rate under the standard assumptions. The algorithm is a single-looped method that does not alternate between sophisticated algorithms, and thus is easy to implement. 
    % If the negative curvature is calculated via the random starting Lanczos method, then it matches the ``dimension-free'' complexity bound of a set of recent first-order methods.
\end{abstract}
\section{Introduction}
In this paper, we consider the following unconstrained optimization problem
\begin{equation}\label{Prob:main}
    \min_{x\in \mathbb{R}^n} f(x),
\end{equation}
where \(f:\mathbb{R}^n \mapsto \mathbb{R}\) is
twice continuously differentiable and \( f_{\inf}:=\inf f(x) > -\infty\).
Allowing some tolerance $\epsilon>0$, our aim is to search for an $\epsilon$-approximate second-order stationary point , for a given tolerance $\epsilon>0$, a point $x\in\mathbb{R}^n$ is an $\epsilon$-approximate second-order stationary point of function $f(x)$ if $x$ satisfies:
    \begin{subequations}
        \begin{align}
            \label{eq.approxfocp} & \|\nabla f(x)\|\leq O(\epsilon)                                     \\
            \label{eq.approxsocp} & \lambda_{\min} \left(\nabla^2 f(x)\right) \geq \Omega(-\sqrt{\epsilon}),
        \end{align}
    \end{subequations}
    where $\lambda_{\min}(A)$ denotes the smallest eigenvalue of $A$.
    The problem \eqref{Prob:main} is a fundamental problem in the optimization community. 
Its complexity attracts significant attention from researchers. 
When $f$ is nonconvex, it has been shown that the gradient descent method (GD) finds an $\epsilon$-approximate first-order stationary point satisfying \eqref{eq.approxfocp} in $O(\epsilon^{-2})$ iterations under the standard $L$-Lipschitz continuous gradient condition.

If the second-order necessity \eqref{eq.approxsocp} is required, one traditionally switches to some variant of Newton's method \cite{conn_trust_2000}. These second-order methods usually construct an iterate by minimizing the quadratic model as an approximation to the original function at some $\xk$:
\begin{equation}\label{eq.quad model}
    m_k(d):=\gk^T d+\frac{1}{2}d^T \Hk d, 
\end{equation}
where $\gk = \nabla f(\xk) $ and $ \Hk = \nabla^2 f(\xk)$.
Minimizing $m_k(d)$ in the convex setting gives the classic Newton direction. In the nonconvex case, globalization techniques are needed for well-posedness.
For example, the classical trust-region method (TR) is based on the following subproblem and acceptance ratio \cite[]{conn_trust_2000}:
\begin{equation}\label{eq.tr}
    \begin{aligned}
        \dk^\mathsf{TR} ~ & =\arg \min_{\|d\|\le \Delta_k} m_k(d) \\
        \rho_k ~           & = \frac{f(x_k + \dk) - f(x_k)}{m_k(\dk) - m_k(0)}
    \end{aligned}
\end{equation}
Although excellent in practice, \citet{cartis_complexity_2010} show classical TR, perhaps surprisingly, has a worst-case complexity of $O(\epsilon^{-2})$ similar to the gradient method.  To our best knowledge, an improved bound of $O(\epsilon^{-3/2})$ is only possible by fixed radius strategy in \citet{ye_second_2005} and recent \emph{refined second-order methods}. For example, \citet{nesterov_cubic_2006} introduces the cubic regularized (CR) subproblem:
\begin{equation}\label{eq.arc}
    \dk^\mathsf{CR} = \arg\min m^{\mathsf{CR}}_k(d):=\gk^T d+\frac{1}{2}d^T \Hk d + \frac{\sigma_k}{3}\|d\|^3, \sigma_k > 0.
\end{equation}
They show that the cubic regularized Newton method has the complexity of $O(\epsilon^{-3/2})$. Later, \citet{cartis_adaptive_2011,cartis_adaptive_2011-1} introduces an adaptive and inexact version of cubic regularization (ARC) with the same iteration complexity.
Except for cubic regularization, \citet{curtis_trust_2017} points out that the TR fails in reducing the function value measured in the norm of the step (or equivalently of the gradient) by classical $\rho_k$-based acceptance rule and linearly updated radius. 
To overcome this issue, they propose the algorithm TRACE \cite[]{curtis_trust_2017,curtis_worst-case_2022}, which has an $O(\epsilon^{-3/2})$ iteration complexity. However, it has a sophisticated rule of expanding and contracting the trust-region radius $\Delta_k$ due to the nonlinearity between $\|\dk\|$ and the dual variable.
\citet{royer_complexity_2018} uses a line-search Newton framework and obtains a similar complexity. Their algorithm alternates between Newton and regularized Newton steps based on the smallest eigenvalue of the Hessian $\Hk$, and the stepsize is chosen under a similar acceptance rule used in \cite{cartis_adaptive_2011,curtis_trust_2017}.  Since all these methods solve Newton systems, they inevitably involve expensive operations in subproblems at the cost of $O(n^3)$.

A recent trend of improved first-order algorithms \cite[]{carmon_accelerated_2018,agarwal_finding_2017,jin_how_2017} appeared to find $\epsilon$-approximate second-order stationary point as a scalable alternative to the second-order ones. Notably, some of these algorithms also enable faster first-order convergence to \eqref{eq.approxfocp} in $O(\epsilon^{-7/4}\log(\epsilon^{-1}))$ function and gradient evaluations. The basic idea is to extend Nesterov's accelerated gradient descent method (AGD) \cite[]{nesterov_lectures_2018} to the nonconvex case. This is achieved by embedding second-order information in some sense to make the AGD maintain its theoretical edge in convex and semiconvex cases.
For example, \citet{carmon_accelerated_2018} apply the Hessian-vector products and randomized Lanczos methods to explore the negative curvature (NC), then use it as a descent direction if it is good enough; otherwise, it is certified to invoke an AGD as $f$ becomes locally semiconvex.
Later \cite[]{jin_how_2017,xu_first-order_2018} also require NC but avoid Hessian-vector products and the complexities remain the same. Beyond using NC, \citet[]{agarwal_finding_2017} achieve the same complexity bound by applying fast matrix inversion to cubic regularized steps. Recently, \citet{li_restarted_2022} introduced a restarted AGD that drops the logarithmic term $O(\log(\epsilon^{-1}))$ in the complexity bound for first-order condition, but it also losses second-order guarantees.
To make AGD work in a comfort zone, these algorithms create sophisticated nested loops that may be hard to implement and tune\footnote{\citet{li_restarted_2022} also address this issue, and their method is considerably easier than other nonconvex AGD methods.}. Nevertheless, they are designed to be less ``dimension-dependent'' than pure second-order methods such as \cite{nesterov_cubic_2006,cartis_adaptive_2011} and are expected suitable for large-scale applications.

Before we get into a detailed discussion on our method, we review our motivation and contributions.
\subsection{Motivation}

When the Hessian $\Hk \prec -\sqrt{\epsilon} \cdot I$, there exists a direction $\xi_k$ such that the Rayleigh quotient $\mathcal R_k(\xi_k)$ is negative, that is, 
\begin{equation}\label{eq.nc.suff}
    \exists \xi_k \in \mathbb{R}^n, \mathcal R_k(\xi_k) := \frac{\xi_k^T\Hk \xi_k}{\|\xi_k\|^2} \le -\sqrt{\epsilon}.
\end{equation}
Using this direction with a proper stepsize $\eta$, the function value must decrease by $O(\epsilon^{3/2})$ under the second-order Lipschitz continuity condition. The nice property is widely used in the negative-curvature-based first-order methods \cite{carmon_accelerated_2018,jin_how_2017}.
To obtain such a direction, some oracles in $O(\epsilon^{-1/4}\log({n/p}))$\footnote{Here $p$ refers to the probability such that the direction can be found of probability $1-p$.} can be used, such as the randomized Lanczos method \cite{kuczynski_estimating_1992}. 
However, if \eqref{eq.nc.suff} is not satisfied, one must switch to other options. In the aforementioned first-order methods, this implies one can invoke nonconvex AGD since it is nearly locally convex. 

By contrast, the method in \citet{royer_complexity_2018} separates into two cases if \eqref{eq.nc.suff} is absent. When the smallest eigenvalue $\lambda_{\min}(\Hk) > -\sqrt{\epsilon}$, regularized Newton step is used to provide the descent step. And if $\lambda_{\min}(\Hk) > \sqrt \epsilon$ is certified, it turns to the ordinary Newton step. It means in the worst case, one must solve an eigenvalue problem and a Newton step in one iteration. It is unclear if one can unify these procedures as a whole.

For trust-region methods \eqref{eq.tr}, such a $\xi_k$ actually means the Lagrangian dual variable is at least in order of $\sqrt{\epsilon}$. Although TR may not use $\xi_k$ directly, it also implies an $O(\epsilon^{3/2})$ progress as long as the step is not too small. This fact can be easily recognized by using the optimality conditions; see \cite{ye_second_2005,curtis_concise_2018} for example. Furthermore, it remains true even when the subproblems are solved inexactly or only optimal in the subspace \cite{cartis_adaptive_2011-1,curtis_worst-case_2022-1,zhang_drsom_2022}. 
For fixed-radius strategies \cite{ye_second_2005,zhang_drsom_2022}, if \eqref{eq.nc.suff} does not hold, then the algorithm safely terminates. 
The case is different for adaptive radius ones. Since the trust-region method uses $\eta_k$ in \eqref{eq.tr} and adjusts the radius linearly, a step may become too small with respect to the dual variable.  A workaround can be found in \cite{curtis_trust_2017,curtis_worst-case_2022-1} with a delicate control over function progress and the gradient:
$$
    f_k-f_{k+1} \geq \Omega(\|\dk\|^3) \text { and }\left\|\dk\right\| \geq \Omega(\|g_{k+1}\|^{1 / 2}).
$$
Similar conditions are also needed in the analysis of cubic regularization method \cite{cartis_adaptive_2011-1}.  However, these adaptations can be less straightforward to understand, implement and tune.

These observations raise the following two questions:
\textit{
\begin{enumerate}
    \item[(1)] Does there exist a direction that is as straightforward as the negative curvature and exists for $f$ in both convex and nonconvex cases?
    \item[(2)] Does there exist a simple second-order framework with similar theoretical guarantees?
\end{enumerate}
}


\subsection{Our contribution}
In this paper, we give an affirmative answer to the above questions by using the homogenized quadratic model and propose a new second-order method, see \autoref{alg.main alg}.

Firstly, we apply the homogenization trick to the quadratic model $m_k(d)$ that is widely used in quadratic programming and semidefinite relaxations \cite{sturm_cones_2003,ye_new_2003, he_quaternion_2022}. Then we show that a strict negative eigenvalue always exists, and moving along the corresponding eigenvector brings a sufficient decrease in function value.

Secondly, we propose a new second-order method, namely \emph{Homogeneous Second-Order Descent Method} (HSODM), which has the optimal iteration complexity and convergence guarantee to an $\epsilon$-approximate second-order point.
In particular, HSODM converges to an $\epsilon$-approximate second-order stationary point in $O(\epsilon^{-3/2})$ iterations by sequentially solving the homogenized model and using a simple stepsize rule.
In sharp comparison to \cite[]{carmon_accelerated_2018,agarwal_finding_2017,jin_how_2017,royer_complexity_2018},
% In HSODM, we move along the \emph{homogeneous direction} by the smallest eigenvector of the homogenized system. Then it produces an $O(\epsilon^{3/2})$ decrease in function value. Due to the fact that the homogeneous direction works \emph{uniformly} in nonconvex and convex cases, 
HSODM only relies on the homogenized model and \emph{does not} alternate between different subroutines. The algorithm is elegant in a simple form and believed to be highly favorable for practitioners.

Furthermore, if HSODM uses similar negative curvature oracles mentioned in the first-order methods, it allows a less dimension-dependent version with $O(\epsilon^{-7/4}\log(\epsilon^{-1}))$ complexity bound.



\subsection{Notations and assumptions}
In this subsection, we introduce the notations and assumptions used throughout the paper.

Denote the standard Euclidean norm in space $\mathbb{R}^n$ by \(\|\cdot\|\). Let $B(x_c, r)$ denote the ball whose center is $x_c$ and radius is $r$, i.e., $ B(x_c, r) = \{ x\in \mathbb{R}^n \mid  \|x - x_c \| \leq r\}$. For a matrix \(A \in \mathbb{R}^{n\times n}\), \(\|A\|\) represents the induced \(\mathcal L_2\) norm, and $\lambda_{\min}(A)$ denotes its smallest eigenvalue. We also denote \(\gk = \nabla f(\xk)\) and \(\Hk = \nabla^{2} f(\xk)\). 

In this paper, we make the following standard assumption.
\begin{assumption}
\label{assm.lipschitz}
Assume that $f$ has \(L\)-Lipschitz continuous gradient and \(M\)-Lipschitz continuous Hessian, that is, for all \( x, y \in \mathbb{R}^n\),
\begin{equation}\label{eq.assm.lipschitz}
    \|\nabla f(x) - \nabla f(y)\| \le L \|x-y\| \quad \mbox{and}\quad \|\nabla^2 f(x) - \nabla^2 f(y)\| \le M \|x-y\|.
\end{equation}
\end{assumption}




\section{The Homogenized Quadratic Model and A Second-Order Descent Method}
% In this paper, we consider the following unconstrained optimization problem
% \begin{equation}\label{Prob:main}
%   \min_{x\in \mathbb{R}^n} f(x),
% \end{equation}
% where \(f:\mathbb{R}^n \mapsto \mathbb{R}\) is 
% twice continuous differentiable and possibly nonconvex and \( f_{\inf}:=\inf f(x) > -\infty\).
% And we aim to find a  second-order stationary point.

% In our analysis, we allow some tolerance $\epsilon>0$ and search for a so-called $\epsilon$-approximate second-order stationary point as described in the following definition:

% \begin{definition}
% For a given tolerance $\epsilon>0$, a point $x\in\mathbb{R}^n$ is a $\epsilon$-approximate second-order stationary point of function $f(x)$ if $x$ satisfies:
% \begin{equation}
%     \label{eq.approxsocp}
%     \|\nabla f(x)\|\leq O(\epsilon), \quad \lambda_{\min} \left(\nabla^2 f(x)\right) \geq -O(\sqrt{\epsilon}).
% \end{equation}
% \end{definition}
\subsection{Overview of the method}


% To facilitate the discussion, we denote \(\gk = \nabla f(\xk)\) and \(\Hk = \nabla^{2} f(\xk)\) throughout the paper.

% In every iteration $\xk$,  it is usual to use the following quadratic model to approximate our objective function $$m_k(d):=\gk^T d+\frac{1}{2}d^T \Hk d.$$
% Minimizing this model without constraints gives the classic Newton direction. However, its global performance is unsatisfactory. There has been a lot of search trying to improve its global performance(\textcolor{red}{cite Cubic, ARC...}), but we wonder if there is a better local model that allows us to exploit the gradient and hessian of the objective function simultaneously and more efficiently, this motivates us to solve the following perturbed homogeneous eigenvalue subproblem,
% \chenyu{add some words on the homogenization trick}

We first define the homogenized quadratic model as follows. Given an iterate $\xk \in \mathbb{R}^n$, let $\psi_k(v, t; \delta)$ be the homogenized quadratic model, 
    \begin{equation}\label{eq.homoquadmodel}
        \psi_k(v, t; \delta) := ~ \begin{bmatrix}
            v \\ t
        \end{bmatrix}^T
        \begin{bmatrix}
            \Hk   & \gk     \\
            \gk^T & -\delta
        \end{bmatrix}
        \begin{bmatrix}
            v \\ t
        \end{bmatrix},~ v\in \mathbb{R}^n, t \in \mathbb{R},
    \end{equation}
where $\delta \geq 0$ is a predefined constant. For simplicity, denote the homogenized matrix by $F_k = [ \Hk , \gk ; \gk^T , -\delta ]$. For each iteration, the HSODM minimizes the homogenized quadratic model at the current iterate $\xk$, i.e., 
\begin{equation}
\label{eq.homo subproblem}
    \begin{aligned}
        \min_{\|[v; t]\| \le 1} \psi_k(v, t; \delta). 
    \end{aligned}
\end{equation}
Denote by $[v_k; t_k]$ the optimal solution of problem \eqref{eq.homo subproblem}. After solving \eqref{eq.homo subproblem}, we construct a descent direction based on the optimal solution $[v_k; t_k]$ and carefully choose the stepsize such that the next iterate $x_{k+1} \in B(x_k, \Delta)$, where $\Delta$ is some pre-determined constant. By doing this subroutine iteratively, our algorithm will converge to an $\epsilon$-approximate second-order stantionary point. The details are formally provided in \autoref{alg.main alg}.

As we will show later, $[v_k; t_k]$ is the eigenvector corresponding to the smallest eigenvalue of $F_k$. Therefore, we can solve the subproblem above using an eigenvector-finding procedure, see \cite{carmon_accelerated_2018, royer_complexity_2018}. 

% The discussion is already presented after Lemma 2. 
% It is worth noting that \citet{sorensen_minimization_1997} and later \cite{rojas_new_2001,adachi_solving_2017} cast the trust-region subproblem \eqref{eq.tr} as a parametric eigenvalue problem. They use the similar ``homogenization'' idea but limit to solving the trust-region subproblem. Since the classical trust-region method does not have the optimal complexity, recognizing the eigenvalue problem itself will not give the desired $O(\epsilon^{-3/2})$ results.



\begin{algorithm}
    \caption{Homogeneous Second-Order Descent Method (HSODM)}\label{alg.main alg}
    \textbf{Initialization:} Given initial point $x_1$, $\delta \geq 0$ and $\Delta > 0$. \\
    \textbf{For} $k = 1, 2, \cdots$ \textbf{do:}\\
    \quad Solve the subproblem \eqref{eq.homo subproblem}, and obtain the solution $[v_k; t_k]$; \\
    \quad Obtain the homogenized direction $\dk := v_k / t_k$ if $t_k \neq 0$; otherwise $\dk = v_k$;\\
    % \quad \textbf{If} $t_k = 0$ \textbf{then:} \\
    % \quad \quad Choose direction $d_k = v_k$ and stepsize $\eta_k = \Delta$; \\
    % \quad \quad Update $x_{k+1} = x_k + \eta_k d_k$; \quad \quad\quad\quad\quad\quad\quad\quad \quad\quad \textit{// sufficient decrease}\\
    % \quad \textbf{Else:} \\
    % \quad \quad Choose direction $d_k = v_k / t_k $; \\
    \quad \textbf{If} $\|d_k\| > \Delta$ \textbf{then:} \\
    \quad \quad Choose stepsize $\eta_k =\Delta / \|d_k\|$; \\
    \quad \quad Update $x_{k+1} = x_k + \eta_k d_k$; \quad\quad\quad\quad\quad\quad\quad \quad \textit{// sufficient decrease}\\
    \quad \textbf{Else:} \\
    % \quad \quad Choose stepsize $\eta_k = 1$; \\
    \quad\quad Update $x_{k+1} = x_k + d_k$;
    \quad\quad\quad\quad\quad\quad\quad\textit{// early termination } \\
    \quad \quad Terminate (or set $\delta = 0$ and proceed); 
\end{algorithm}


\subsection{Preliminaries of the Homogenized Quadratic Model}
In this subsection, we present some preliminary analysis of the homogenized quadratic model. First, we study the relationship between the smallest eigenvalues of the Hessian $\Hk$ and $F_k$, and the perturbation parameter $\delta$. Then we give the optimality conditions of problem \eqref{eq.homo subproblem} and provide some useful results based on those conditions. 

\begin{lemma}[Relationship between $\lambda_{\min}(F_k)$, $\lambda_{\min}(H_k)$ and $\delta$]
\label{lemma.relation of theta delta Hk} 
Let $\lambda_{\min}(\Hk)$ and $\lambda_{\min}(F_k)$ be the smallest eigenvalue of $H_k$ and $F_k$ respectively. Denote by $\mathcal{S}_{\lambda_{\min}}$ the eigenspace corresponding to $\lambda_{\min}(\Hk)$. If $g_k \neq 0$ and $H_k \neq 0$, then the following statements hold,
\begin{enumerate}
    \item[(1)] $\lambda_{\min}(F_k) < -\delta$ and $\lambda_{\min}(F_k) \leq \lambda_{\min}(\Hk)$;
    \item[(2)] $\lambda_{\min}(F_k) = \lambda_{\min}(\Hk)$ only if $\lambda_{\min}(\Hk)<0$ and $\gk \perp \mathcal{S}_{\lambda_{\min}}$.
    % and $\delta = \lambda_{\min}(\Hk)-\gk^\top p_{\lambda_{\min}}$
\end{enumerate}
   % and $p_{\lambda_{\min}} = -(\Hk-\lambda_{\min}(\Hk)I)^{\dagger}\gk$.
    % \begin{itemize}
    %     \item If $\lambda_{\min}(H_k) < -\delta$, then $\theta_k > -\lambda_{\min}(H_k)$;
    %     \item If $\lambda_{\min}(H_k) \geq -\delta$, then $\theta_k > \delta$; 
    %     \item $\theta_k > \max\left\{-\lambda_{\min}(H_k), \delta\right\}$;
    %     \item Once $\lambda_{\min}(H_k) > \delta$, then $t_k \neq 0$,
    % \end{itemize}
    % where $\lambda_{\min}(H_k)$ is the smallest eigenvalue of the matrix $H_k$.
\end{lemma}
\begin{proof}
We first prove the statement $(1)$. By the Cauchy interlace theorem \cite{parlett_symmetric_1998}, we immediately obtain $\lambda_{\min}(F_k)  \leq \lambda_{\min}(H_k) $. Now we need to prove that $\lambda_{\min}(F_k) < -\delta $. It suffices to show that the matrix $F_k+\delta I$ has a negative curvature.

Let us consider the direction $\left[ -\eta \gk ; t \right]$, where $\eta,t>0$. Define the following function of $\left( \eta,t \right)$: 
\begin{equation*}
    \begin{aligned}
         f(\eta,t)& \coloneqq \begin{bmatrix}
     -\eta \gk \\
     t
\end{bmatrix}^T (F_k+\delta I) \begin{bmatrix}
     -\eta \gk \\
     t
\end{bmatrix}, \\
&=\eta^2 \gk^T (\Hk+\delta I) \gk -2\eta t \|\gk\|^2.
    \end{aligned}
\end{equation*}
For any fixed $t>0$, we have
\begin{equation*}
    f(0,t)=0 \quad  \text{and} \quad  \frac{\partial f(0,t)}{\partial \eta}=-2t \|\gk\|^2<0. 
\end{equation*}
Therefore, for sufficiently small $\eta>0$, it holds that $f(\eta,t)<0$, which shows that $\left[ -\eta \gk ; t \right]$ is a negative curvature. Hence, $\lambda_{\min}(F_k) < -\delta $. 

% Now we need to prove that $\lambda_{\min}(F_k) < -\delta $. Denote $u_j = (u_j^{'}; t_j)$ with $u_j^{'} \in \mathbb{R}^{n}$ and $t_j \in \mathbb{R}$ for $j = 1, 2, \cdots, n+1$. Note that there exists at least one eigenpair of $F_k$ such that $t_j \neq 0$ and we take $ d_j = u_j^{'} / t_j$ for such $j$. By the definition of eigenvalue and eigenvector, we have 
% \begin{equation*}
%     F_k u_j = - \lambda_j u_j, 
% \end{equation*}
% implying that 
% \begin{equation}
%     \label{eq.equivoptc}
%     (\Hk+\lambda_j I) d_j = -\gk \quad \text{and} \quad \gk^T d_j = \delta-\lambda_j.
% \end{equation}

% Let $\beta_i =  \gk^T v_i$ for all $i = 1, 2, \cdots, n$. We then have $\gk^T d_j = -\gk^T(\Hk+\lambda_j  I)^{\dagger}\gk = -\sum_{i=1}^n  \beta_i^2 / \left(  \sigma_i+\lambda_j \right)$. Therefore, $\lambda_j$ must satisfy
% \begin{equation}
%     \label{eq.propertytheta}
%     \delta-\lambda_j =  -\sum_{i=1}^n\frac{\beta_i^2}{\sigma_i + \lambda_j}.
% \end{equation}
% Let us view both sides of the above equation as the functions of $\lambda_j$. The left-hand-side is a linear function in $\lambda_j$ while the right-hand-side is the sum of a sequence of inverse proportional functions. By the property of the two functions and the Cauchy interlace theorem \cite{parlett1998symmetric}, there exists a solution to \eqref{eq.propertytheta} such that $-\lambda_j<-\delta$, implying that $ \lambda_{\min} \left( F_k \right) = - \lambda_1 \leq \lambda_j <  - \delta$.  


The proof of the statement $(2)$ is similar to the one of Theorem 3.1 in \cite{rojas_new_2001}, so we omit it here.
    % Then we prove the second part. We first start with $\lambda_{\min}(\Hk)<0$, $\gk \perp \mathcal{S}_{\lambda_{\min}}$, $\delta = \lambda_{\min}(\Hk)-\gk^\top p_{\lambda_{\min}}$.

    % To prove $\theta_k = -\delta_1$, it suffices to prove that under our choice of $\delta$, there is a eigenvector correspondings to $\delta_1$ with $t\neq0$,which is equivalent to the LHS and RHS intersects at a point with its first coordinate equals $-\delta_1$, this completes the proof. Hence we need to construct a eigenvector $[v;t]$ corresponding to $\delta_1$ with $t\neq0$. The construction is $[p_{\lambda_{\min}}^T;1]$, note that by our construction of $p_{\lambda_{\min}}$ and $\delta$, we have
    % \begin{equation}
    %     \label{eq.propertycons}
    %     (\Hk-\delta_1 I)p_{\lambda_{\min}}= -(\Hk-\delta_1 I)(\Hk-\delta_1 I)^{\dagger}\gk=-\gk,
    % \end{equation}
    % where the second equality is due to that $(\Hk-\delta_1 I)(\Hk-\delta_1 I)^{\dagger}$ is a projection onto the image space $\mathcal{R}(\Hk-\delta_1 I)$ and that $\gk\perp \mathcal{S}_1$ thus belonging to the image space.


    % \begin{equation}
    %     \label{eq.checkeigenvec}
    %     \begin{bmatrix}
    %         \Hk   & \gk    \\
    %         \gk^T & \delta
    %     \end{bmatrix}\begin{bmatrix}
    %         p_{\lambda_{\min}} \\1
    %     \end{bmatrix}=\begin{bmatrix}
    %         \Hk p_{\lambda_{\min}}+\gk \\
    %         \delta+\gk^T p_{\lambda_{\min}}
    %     \end{bmatrix}=\delta_1\begin{bmatrix}
    %         p_{\lambda_{\min}} \\
    %         1
    %     \end{bmatrix}.
    % \end{equation}
    % The opposite part is similar so we omit it here. 
\end{proof}
\autoref{lemma.relation of theta delta Hk} shows that we can control the smallest eigenvalue of the homogenized matrix $F_k$ by adjusting the perturbation parameter $\delta$. It helps us find a better direction to decrease the value of the objective function. We also note that the case $\gk\perp \mathcal{S}_{\lambda_{\min}}$ is often regarded as a hard case in solving the trust-region subproblem. However, this dilemma will not incapacitate HSODM in our convergence analysis. In the following, we will show the function value will have sufficient decrease under this scenario. Thus, the subproblem in HSODM is much easier to solve than the trust-region subproblem due to the non-existence of the hard case.

We remark that \autoref{lemma.relation of theta delta Hk} is a simpler version of Lemma 3.3 in \cite{rojas_new_2001}. 
They give a more detailed analysis of the relationship between the perturbation parameter $\delta$ and the eigenpair of the homogenized matrix $F_k$. 
However, our paper differs from \cite{rojas_new_2001} in that they try to obtain a solution to the trust-region subproblem via the homogenization trick, while our goal is to seek a good direction to decrease the function value.  Furthermore, if the homogenized model is used, then we can show that HSODM has the optimal $O(\epsilon^{-3/2})$ iteration complexity. If instead, the emphasis is put on solving the trust-region subproblem, one still needs a framework like the one in \citet{curtis_trust_2017} to guarantee the same performance.

In the following lemma, we characterize the optimal solution $[v_k; t_k]$ of problem \eqref{eq.homo subproblem} based on the optimality condition of the standard trust-region subproblem.

\begin{lemma}[Optimality condition]\label{lemma.optimal condition of subproblem}
    $[v_k; t_k]$ is the optimal solution of the subproblem \eqref{eq.homo subproblem} if and only if there exists a dual variable $\theta_k > \delta \geq 0$ such that
    \begin{align}
    \label{eq.homoeig.soc}      
      & \begin{bmatrix}
      \Hk + \theta_k \cdot I & \gk              \\
      \gk^T                  & -\delta+\theta_k
      \end{bmatrix} \succeq 0, \\
    \label{eq.homoeig.foc}      
      & \begin{bmatrix}
      \Hk + \theta_k \cdot I & \gk              \\
      \gk^T & -\delta+\theta_k
        \end{bmatrix} 
        \begin{bmatrix}
        v_k \\ t_k
        \end{bmatrix} = 0, \\
        \label{eq.homoeig.norm one} & \|[v_k; t_k]\| = 1.
    \end{align}
    Moreover, $-\theta_k$ is the smallest eigenvalue of the perturbed homogeneous matrix $F_k$, i.e., $-\theta_k = \lambda_{\min}(F_k)$.
\end{lemma}
\begin{proof}
By the optimality condition of standard trust region subproblem, $[v_k; t_k]$ is the optimal solution if and only if there exists a dual variable $\theta_k \geq 0$ such that
\begin{equation*}
    \begin{bmatrix}
        \Hk + \theta_k \cdot I & \gk              \\
        \gk^T                  & -\delta+\theta_k
    \end{bmatrix} \succeq 0, \ \begin{bmatrix}
        \Hk + \theta_k \cdot I & \gk              \\
        \gk^T                  & -\delta+\theta_k
    \end{bmatrix} \begin{bmatrix}
        v_k \\ t_k
    \end{bmatrix} = 0, \ \text{and} \  \theta_k \cdot (\|[v_k; t_k]\| - 1) = 0.
\end{equation*}
With \autoref{lemma.relation of theta delta Hk}, we have $\lambda_{\min}(F_k) < -\delta \leq 0$. 
Therefore, $\theta_k \geq  - \lambda_{\min}(F_k) >  \delta \geq 0$, and further $ \|[v_k; t_k]\| = 1 $. Moreover, by \eqref{eq.homoeig.foc}, we obtain
\begin{equation*}
\begin{bmatrix}
\Hk   & \gk     \\
\gk^T & -\delta
\end{bmatrix} \begin{bmatrix}
v_k \\ t_k
\end{bmatrix} = -\theta_k \begin{bmatrix}
v_k \\ t_k
\end{bmatrix}.
\end{equation*}
Multiplying the equation above by $ \left[ v_k ; t_k \right]^T $, we have 
\begin{equation*}
    \min_{\|[v; t]\| \le 1} \psi_k(v, t; \delta) = -\theta_k
\end{equation*}
Note that with \eqref{eq.homoeig.norm one}, the optimal value of problem \eqref{eq.homo subproblem} is equivalent to the smallest eigenvalue of $F_k$, i.e., $\lambda_{\min}(F_k)$. Thus, $-\theta_k = \lambda_{\min}(F_k)$. Then the proof is completed.
\end{proof}

With the above optimality condition, we can derive the following corollary. 
\begin{corollary}
\label{corollary. foc} 
The equation \eqref{eq.homoeig.foc} in \autoref{lemma.optimal condition of subproblem} can be rewritten as,
\begin{equation}\label{eq.homoeig.foc cont}
    (\Hk + \theta_k I)  v_k =  - t_k g_k \quad \text{and} \quad \gk^T v_k  =  t_k (\delta - \theta_k).
\end{equation}
Furthermore,
\begin{enumerate}
    \item[(1)] If $t_k = 0$, then we have
          \begin{equation}\label{eq.homoeig.foc t=0}
              (\Hk + \theta_k I)  v_k = 0 \quad \text{and} \quad \gk^T v_k  = 0,
          \end{equation}
          implying that $(-\theta_k, v_k)$ is the eigenpair of the Hessian matrix $H_k$.
    \item[(2)] If $t_k \neq 0$, then we have
  \begin{equation}
      \label{eq.homoeig.foc t neq 0}
      \gk^T d_k = \delta -\theta_k \quad \text{and} \quad (\Hk+\theta_k \cdot I)d_k =-\gk
  \end{equation}
  where $d_k =v_k / t_k$.
\end{enumerate}
\end{corollary}
The corollary is a direct application of \autoref{lemma.optimal condition of subproblem}, so we omit its proof in the paper. As a byproduct, we also have the following result.
\begin{corollary}[$g_k = 0$]
Suppose that $g_k = 0$, then the following statements hold, \begin{enumerate}
    \item[(1)] If $\lambda_{\min}(H_k) > -\delta$, then $t_k = 1$.
    \item[(2)] If $\lambda_{\min}(H_k) < -\delta$, then $t_k = 0$.
\end{enumerate}
\end{corollary}
\begin{proof}
When $g_k = 0$, the homogenized matrix $F_k = [H_k, 0; 0, -\delta]$, and the subproblem \eqref{eq.homo subproblem} is 
\begin{equation*}
\min_{\|[v; t]\| \le 1} \psi_k(v, t; \delta) = v^T H_k v - t^2 \cdot \delta.
\end{equation*}
We first prove the statement $(1)$ by contradiction. Suppose that $t_k \neq 1$, then we have $v_k \neq 0$ by the equation \eqref{eq.homoeig.norm one}. Thus, 
\begin{equation}
\label{eq. gk=0 contra}
    \psi_k(v_k, t_k; \delta) = (v_k)^T H_k v_k - t_k^2 \cdot \delta > -\delta = \psi_k(0, 1; \delta),
\end{equation}
where the inequality holds due to $(v_k)^T H_k v_k \geq \lambda_{\min}(H_k) \|v_k\|^2 > -\delta\|v_k\|^2$. The equation \eqref{eq. gk=0 contra} contradicts to the optimality of $(v_k, t_k)$, and thus $t_k = 1$. The second statement can be proved by the same argument, and we omit the proof here.
\end{proof}

\begin{corollary}[Nontriviality of direction $v_k$] \label{coro.homo.exists.nc}
    If $g_k \neq 0$, then $v_k \neq 0$.
\end{corollary}
\begin{proof}
    We prove by contradiction. Suppose that $v_k = 0$. Then, we have $t_k g_k = 0$ with equation \eqref{eq.homoeig.foc cont} in  \autoref{corollary. foc}. It further implies that $t_k = 0$ due to $g_k \neq 0$. However, $[ v_k ; t_k]=0$ contradicts to the equation $\|[v_k; t_k]\| = 1$ in the optimality condition. Therefore, we have $v_k \neq 0$.
\end{proof}

This corollary shows that a nontrivial direction $v_k$ always exists, thus \autoref{alg.main alg} will not get stuck.

\section{Global Convergence Rate}
In this section, we analyze the convergence rate of the HSODM method. To facilitate the analysis, we present two building blocks considering the large and small values of $\|d_k\|$, respectively.
For the large value case where $\|d_k\| > \Delta$, we show that the function value decreases by at least $O(\epsilon^{3/2})$ at every iteration after carefully selecting the perturbation parameter $\delta$. In the latter case, we prove that the next iterate $x_{k+1}$ is already an $\epsilon$-approximate second-order stationary point.

We first introduce the following lemma.
\begin{lemma}[\citet{nesterov_lectures_2018}]\label{lem.lipschitz}
    If \(f:\mathbb{R}^n \mapsto \mathbb{R}\) satisfies \autoref{assm.lipschitz}, then for all \(x,y\in \mathbb{R}^n\),
    \begin{subequations}
        \begin{align}
             & |f(y)-f(x)-\nabla f(x)^T(y-x)|                                             \leq \frac{L}{2}\|y-x\|^{2}  \\
             & \left\|\nabla f(y)-\nabla f(x)-\nabla^{2} f(x)(y-x)\right\|                       \leq \frac{M}{2}\|y-x\|^{2} \\
             & \left|f(y)-f(x)-\nabla f(x)^T(y-x)-\frac{1}{2}(y-x)^T\nabla^{2} f(x)(y-x)\right|  \leq \frac{M}{6}\|y-x\|^{3}
        \end{align}
    \end{subequations}
\end{lemma}

\subsection{ Large value of $\|d_k\|$ }\label{subsection.decrease}
In HSODM, we define the ``large value'' case of $\|d_k\|$ if its norm is relatively large with respect to $\Delta$. Since $[v_k; t_k]$ is a unit vector, this actually means $t_k = 0$ or $d_k = v_k / t_k$ is large.

We first consider the case when $t_k = 0$. Actually, this case coincides with the ``hard case" in \cite{rojas_new_2001}. Note that in this case, $(-\theta_k, v_k)$ is the eigenpair of the Hessian $H_k$ by \autoref{corollary. foc}. Besides, $v_k$ is a sufficiently negative curvature due to $-\theta_k < - \delta \leq 0$. Therefore, once we move along the direction $v_k$ with a proper stepsize, the function value must decrease. We then give the following descending lemma.
\begin{lemma}\label{lemma.t = 0 decrease lemma}
    Suppose that \autoref{assm.lipschitz} holds. If $t_k = 0$, then let $d_k = v_k$ and $\eta_k = \Delta$, we have
    \begin{equation}
        f(x_{k+1}) - f(x_k) \leq -\frac{\Delta^2}{2}\delta + \frac{M}{6}\Delta^3.
    \end{equation}
\end{lemma}
\begin{proof}
When $t_k = 0$, with equation \eqref{eq.homoeig.foc t=0} in \autoref{corollary. foc}, we obtain
\begin{equation}
\label{eq:bridge2}
    v_k^T H_k v_k = - \theta \|v_k\|^2 \quad \text{and} \quad
    g_k^T v_k = 0. 
\end{equation}
By the $M$-Lipschitz continuous property of $\nabla^2 f(x)$, we have
\begin{subequations}
\begin{align}
    f(x_{k+1}) - f(x_k) & = f(x_k + \eta_k d_k) - f(x_k)  \notag \\
    & \leq \eta_k \cdot g_k^T d_k + \frac{\eta_k^2}{2} \cdot d_k^T H_k d_k + \frac{M}{6}\eta_k^3 \|d_k\|^3  \notag                    \\
    & = \Delta \cdot g_k^T v_k + \frac{\Delta^2}{2} \cdot v_k^T H_k v_k + \frac{M}{6}\Delta^3 \|v_k\|^3 \notag \\
    & = - \theta_k \cdot \frac{\Delta^2}{2} \|v_k\|^2 + \frac{M}{6}\Delta^3 \|v_k\|^3 \label{subeq. t=0 optimal condition}            \\
    & \leq -\frac{\Delta^2}{2}\delta + \frac{M}{6}\Delta^3, \label{subeq. t=0 delta}
    \end{align}
\end{subequations}
where \eqref{subeq. t=0 optimal condition} holds due to \eqref{eq:bridge2}, and \eqref{subeq. t=0 delta} follows from $\theta_k \geq \delta$ and $\|v_k\| = 1$ as stated in \autoref{lemma.optimal condition of subproblem}.
\end{proof}

For the case $t_k \neq 0$, when the norm of the direction $d_k = v_k / t_k$ is large enough, i.e., $\|d_k\| > \Delta$, we can obtain the same decrease of function value by choosing a suitable stepsize $\eta_k$.
\begin{lemma}\label{lemma.t neq 0 decrease lemma}
    Suppose that \autoref{assm.lipschitz} holds. If $t_k \neq 0$ and $\|v_k / t_k\| > \Delta$, let $d_k = v_k / t_k$ and $\eta_k = \Delta / \|d_k\|$, we have
    \begin{equation}
        f(x_{k+1}) - f(x_k) \leq -\frac{\Delta^2}{2}\delta + \frac{M}{6}\Delta^3.
    \end{equation}
\end{lemma}
\begin{proof}
When $t_k \neq 0$, with equation \eqref{eq.homoeig.foc t neq 0} in \autoref{corollary. foc}, we have 
\begin{equation}
\label{eq:bridge3}
    d_k^T H_k d_k = -g_k^T d_k - \theta_k \|d_k\|^2 \quad \text{and} \quad g_k^T d_k = \delta - \theta_k \leq 0 
\end{equation}
Since $\eta_k = \Delta / \|d_k\| \in (0, 1)$, then $  \eta_k - \eta_k^2 / 2 \geq 0 $, and further 
\begin{equation}
\label{eq:bridge4}
    \left(\eta_k - \frac{\eta_k^2}{2}\right) \cdot g_k^T d_k \leq 0
\end{equation}

By the $M$-Lipschitz continuous property of $\nabla^2 f(x)$, we have
\begin{subequations}
\begin{align}
    f(x_{k+1}) - f(x_k) & = f(x_k + \eta_k d_k) - f(x_k)  \notag \\
    & \leq \eta_k \cdot g_k^T d_k + \frac{\eta_k^2}{2} \cdot d_k^T H_k d_k + \frac{M}{6}\eta_k^3 \|d_k\|^3  \notag \\
    & = \left(\eta_k - \frac{\eta_k^2}{2}\right) \cdot g_k^T d_k - \theta_k \cdot \frac{\eta_k^2}{2} \|d_k\|^2 + \frac{M}{6}\eta_k^3 \|d_k\|^3  \label{subeq. t neq 0 optimal condition} \\
    & \leq -\theta_k \cdot \frac{\eta_k^2}{2} \|d_k\|^2 + \frac{M}{6}\eta_k^3 \|d_k\|^3 \label{subeq. t neq 0 drop gd} \\
    & \leq -\frac{\Delta^2}{2}\delta + \frac{M}{6}\Delta^3, \label{subeq. t neq 0 delta}
\end{align}
\end{subequations}
where \eqref{subeq. t neq 0 optimal condition} holds due to equation \eqref{eq:bridge3}, \eqref{subeq. t neq 0 drop gd} follows from equation \eqref{eq:bridge4}, and in \eqref{subeq. t neq 0 delta} we substitute $\eta_k$ with $\Delta / \| d_k\| $ and use $\theta_k \geq \delta$.
\end{proof}


\subsection{Small value of $\|d_k\|$}
\label{subsection.convergence}
Now we consider the case of small values. We first note that if $t_k = 0$, it implies $\|v_k\| = 1$ and should be classified as the large value in previous section. To this end, $t_k \neq 0$ is the only concern in this part of our discussion. We show that if $\|d_k\| = \|v_k/t_k\| \le \Delta$, then the next iterate $x_{k+1}$ is $\epsilon$-approximate second-order stationary point. Therefore, we can terminate the algorithm after one iteration in the small step case. To prove the result, we provide an upper bound of $\|g_k\|$ for preparation.
\begin{lemma}
\label{lemma.upper bound of g_k}
    Suppose that \autoref{assm.lipschitz} holds. If $g_k \neq 0$, and $\|d_k\| \leq \Delta \leq \sqrt{2}/2$, then we have
    \begin{equation}\label{eq.upper bound of g_k}
        \|g_k\| \leq 2(L+\delta) \Delta.
    \end{equation}
\end{lemma}
\begin{proof}
By \autoref{lemma.relation of theta delta Hk}, we have $\theta_k - \delta > 0 $. With the equation \eqref{eq.homoeig.soc} stated in \autoref{lemma.optimal condition of subproblem} and Schur complement, we have 
\begin{equation}\label{eq.schur complement ineq}
    H_k + \theta_k I - \frac{1}{\theta_k - \delta} g_k g_k^T \succeq 0,
\end{equation}
implying that 
\begin{equation*}
     \frac{\theta_k - \delta }{ \|g_k\|^2 }  \cdot g_k^T  \left( H_k + \theta_k I - \frac{1}{\theta_k - \delta} g_k g_k^T  \right) g_k\succeq 0.
\end{equation*}
Thus, we obtain
\begin{equation}
\label{eq.quadratic function at theta-delta}
    (\theta_k - \delta)^2 + (\theta_k - \delta) \left(\frac{g_k^T H_k g_k}{\|g_k\|^2} + \delta \right)-\|g_k\|^2\geq 0. 
\end{equation}

Denote $h(t) = t^2 + \left(  g_k^T H_k g_k / \|g_k\|^2 + \delta  \right) t - \|g_k\|^2$. It is easy to see that the equation $h(t) = 0$ must have two real roots with opposite signs. Let its positive root be $t_2$. By $ \theta_k - \delta > 0$, we have $ \theta_k - \delta \geq t_2 $. Moreover, with equation \eqref{eq.homoeig.foc t neq 0} in \autoref{corollary. foc}, we can give an upper bound of $\theta_k - \delta$, that is, 
\begin{equation}\label{eq.bound of theta - delta}
    \theta_k - \delta = -g_k^T d_k \leq \|g_k\| \|d_k\| \leq \Delta \|g_k\|.
\end{equation}

Therefore, we must have
\begin{equation*}
    h(\Delta \|g_k\|) = 
    \ \Delta^2\|\gk\|^2+\left(\frac{g_k^T H_k g_k}{\|g_k\|^2} + \delta \right)\Delta\|\gk\|-\|\gk\|^2\geq 0.
\end{equation*}
After some algebra, we obtain
\begin{align}
    \nonumber\|\gk\| & \leq \frac{\left( g_k^T H_k g_k / \|g_k\|^2 + \delta \right) \Delta}{1-\Delta^2} \\ 
    \nonumber&\leq \frac{(L+\delta)\Delta}{1-\Delta^2} \\
    \label{eq.bound g}& \leq 2(L+\delta) \Delta.                      
\end{align}
The second inequality holds due to the $L$-Lipschitz continuity of $\nabla f(x)$, which implies that $H_k \preceq L I$, and further $g_k^T H_k g_k / \|g_k\|^2 \leq L$. The last inequality follows from $\Delta \leq \sqrt{2}/2$.
\end{proof}

When $\|d_k\| \leq \Delta$, we let the stepsize $\eta_k = 1$ and proceed the iteration by $x_{k+1} = x_k + d_k$. The following lemma shows that the norm of the gradient at $x_{k+1}$ can be upper bounded, while the smallest eigenvalue of the Hessian at $x_{k+1}$ has a lower bound.

\begin{lemma}\label{lemma.norm of gk+1}
Suppose that \autoref{assm.lipschitz} holds. If $g_k \neq 0$, and $\| d_k \| \leq \Delta$, then let $\eta_k = 1$, we have
\begin{align}
     \|g_{k+1}\|  & \leq  2(L+\delta) \Delta^3 + \frac{M}{2} \Delta^2 + \delta \Delta, \label{eq.small norm of gk+1} \\
     H_{k+1} & \succeq - \left(  2(L+\delta)\Delta^2 + M\Delta + \delta \right) I \label{eq.psd of Hk+1}
\end{align}
\end{lemma}
\begin{proof}
    We first prove \eqref{eq.small norm of gk+1}. By the optimality condition \eqref{eq.homoeig.foc t neq 0} in \autoref{corollary. foc}, we have
    \begin{equation*}
        H_k d_k + g_k = -\theta_k d_k,
    \end{equation*}
    and with \eqref{eq.bound of theta - delta}, we have
    \begin{equation*}
        \theta_k \|d_k\| \leq \left( \delta + \Delta \|g_k\| \right) \|d_k\|
    \end{equation*}
    Thus, it holds that
    \begin{equation}
        \label{eq:bridge1}
        \begin{split}
            \|H_k d_k + g_k \| = \theta_k \| d_k\| \leq \delta \Delta +  \|g_k\| \Delta^2.
        \end{split}
    \end{equation}

    Now we bound the norm of $\|g_{k+1}\|$ and obtain,
    \begin{subequations}
        \begin{align}
        \|g_{k+1}\| & \leq \|g_{k+1} - H_k d_k - g_k\| + \|H_k d_k + g_k\| \notag  \\
        & \leq \frac{M}{2}\|d_k\|^2 + \delta \Delta +  \|g_k\| \Delta ^2 \label{subeq. hessian lip}  \\
        & \leq \frac{M}{2} \Delta^2 + \delta \Delta + 2(L+\delta)\Delta \cdot \Delta^2 \label{subeq. convergence gk upper bound} \\
        & =  2(L+\delta) \Delta^3 + \frac{M}{2} \Delta^2 + \delta \Delta, \notag
        \end{align}
    \end{subequations}
    where \eqref{subeq. hessian lip} holds due to the $M$-Lipschitz continuity of $\nabla^2 f(x)$ and equation \eqref{eq:bridge1}, and \eqref{subeq. convergence gk upper bound} follows from \autoref{lemma.upper bound of g_k}.

Second, we prove \eqref{eq.psd of Hk+1}. Note that the optimality condition \eqref{eq.homoeig.soc} in \autoref{lemma.optimal condition of subproblem} implies that
\begin{equation*}
    H_k + \theta_k \cdot I \succeq 0.
\end{equation*}
With \eqref{eq.bound of theta - delta} and \eqref{eq.bound g}, we further obtain
\begin{align}
  \nonumber   H_k & \succeq - \theta_k I \\
  \nonumber       &\succeq -(\Delta\|g_k\|+\delta)I \\
\label{eq.psd pf Hk}      & \succeq -2(L+\delta)\Delta^2I - \delta I. 
\end{align}

We now turn to bound $H_{k+1}$ and have
\begin{align}
    \nonumber    H_{k+1} & \succeq H_k - \|H_{k+1} - H_{k}\| I \\
    \nonumber    & \succeq H_k - M \|d_k\| I \\
        & \succeq H_k - M \Delta I
\end{align}
where the second inequality holds by the $M$-Lipschitz continuity of $\nabla^2 f(x)$, and the last inequality follows from $\|d_k\| \leq \Delta$. Combining with \eqref{eq.psd pf Hk}, we arrive at
\begin{equation}
    H_{k+1} \succeq -2(L+\delta)\Delta^2I - \delta I - M\Delta I.
\end{equation}
The proof is then completed.
\end{proof}

Putting the above pieces together, we formally give the convergence result of HSODM in \autoref{thm.global convergence rate}. 
It shows that our method achieves $O(\epsilon^{-3/2})$ iteration complexity to find an $\epsilon$-approximate second-order stationary point by choosing the perturbation parameter $\delta$ and the radius $\Delta$ carefully.
\begin{theorem}
\label{thm.global convergence rate}
    Suppose that \autoref{assm.lipschitz} holds. Let $\delta = \sqrt{\epsilon}$ and $\Delta = 2\sqrt{\epsilon} / M$, then the homogeneous second-order descent method (HSODM) terminates in at most $O\left(\epsilon^{-3/2}\right)$ steps, and the next iterate $x_{k+1}$ is a second-order stationary point.
\end{theorem}
\begin{proof}
Since we take $\delta = \sqrt{\epsilon}$ and $\Delta = 2\sqrt{\epsilon} / M $, by \autoref{lemma.t = 0 decrease lemma} and \autoref{lemma.t neq 0 decrease lemma},  we immediately obtain that the function value decreases at least $O(\epsilon^{-3/2})$ for the large step case, i.e.,  
\begin{equation}
\label{decrease subsititue delta and Delta}
    f(x_{k+1}) - f(x_k) \leq -\frac{2}{3M^2} \epsilon^{-3/2}.
\end{equation}

When the algorithm terminates, by \autoref{lemma.norm of gk+1}, we have
\begin{equation}
    \|g_{k+1}\|\leq O(\epsilon) \quad \text{and} \quad  \lambda_{\min} \left(H_{k+1}\right) \geq \Omega(-\sqrt{\epsilon}).
\end{equation}
Therefore, the next iterate $x_{k+1}$ is already a second-order stationary point.

Note that the total decreasing amount of the objective function value cannot exceed $f(x_1) - f_{\inf}$. It leads to that the number of iterations for large step case is upper bounded by
\begin{equation*}
    O\left(\frac{3M^2}{2}\left(f(x_1) - f_{\inf}\right)\epsilon^{-3/2}\right),
\end{equation*}
which is also the iteration complexity of our algorithm. 
\end{proof}

\begin{remark}
From the above algorithm, if we use the random starting Lanczos algorithm \cite{kuczynski_estimating_1992} as the eigenvalue solver, similar to \cite{carmon_accelerated_2018,royer_complexity_2018}, then we will  have a unit vector sufficiently close to the leftmost eigenvector of $F_k$ of probability $1-p$ in  $O\left(\log ((n + 1) / p)\epsilon^{-1/4}\right)$. The idea is to compute the maximum eigenpair for $\omega \cdot I -F_k$ for some large enough $\omega$ such that $\omega \cdot I -F_k \succeq 0$. This result is quite standard nowadays; see \cite{royer_complexity_2018} for example. This gives a less dimension-dependent complexity of $O(\epsilon^{-7/4}\log({(n+1)/p}))$. 
\end{remark}

% \begin{corollary}[Lemma 9, \citet{royer_complexity_2018}]
%     Suppose the random starting Lanczos algorithm \cite{kuczynski_estimating_1992} is used to compute leftmost eigenvector of $F_k$, then for any $\varepsilon > 0$ and $p \in (0,1)$, the procedure outputs a unit vector $v \in \mathbb{R}^{n+1}$ by a probability of $1-p$ such that
%     $$v^T F_k v \leq \lambda_{\min }(F_k)+\varepsilon$$
%     in at most
%     $$\min \left\{n + 1, \frac{\log \left(n + 1/ p^2\right)}{2 \sqrt{2}} \sqrt{\frac{M}{\varepsilon}}\right\}$$
%     iterations.
% \end{corollary}
% Using the above result, setting $\varepsilon = \sqrt{\epsilon}$ results in the an $O(\epsilon^{-1/4})$ for the each eigenvalue computation. This immediately implies that HSODM matches the complexity of the first-order methods using negative curvature.

\section{Local Convergence Rate}

In this section, we give the local convergence analysis of HSODM. When $x_k$ is sufficiently close to a second-order stationary point $x^*$, then once we set the perturbation parameter $\delta = 0$ for the subsequent iterations, the HSODM will achieve a local quadratic convergence rate.


% Instead of terminating \autoref{alg.main alg} as soon as $\epsilon$-approximate second-order conditions \eqref{eq.approxfocp} and \eqref{eq.approxsocp} are met, we set the perturbation parameter $\delta=0$ and let HSODM continue. We show that the HSODM achieves a local quadratic convergence rate.

We first make the standard assumption \cite{conn_trust_2000,nocedal_numerical_2006,nesterov_lectures_2018} to facilitate the local convergence analysis.
\begin{assumption}
\label{assm.local}
    Assume that HSODM converges to a strict local optimum $x^*$ satisfying that $\nabla f(x^*)=0$ and $\nabla^2 f(x^*) \succ 0$.
\end{assumption}
\begin{remark}    
From the above \autoref{assm.local}, we immediately know that there exists a small neighborhood such that for all $x \in B(x^*, R)$, $\nabla^2 f(x) \succeq \mu \cdot I$ for some $\mu > 0$. In other words, $x_k$ arrives at the neighborhood of $x^*$ for sufficiently large $k$, hence both $\Hk
$ and $\Hk+\theta_k I$ are nonsingular. 
\end{remark}

To prove the local convergence rate, we prove the following auxiliary results for preparation.
\begin{corollary}
\label{coro. large k t neq 0}
    Suppose that \autoref{assm.local} holds, then $t_k \neq 0$ for sufficiently large $k$.
\end{corollary}
\begin{proof}
We prove by contradiction. Suppose that $t_k = 0$. Then by \autoref{corollary. foc}, $(-\theta_k, v_k)$ is the eigenpair of $H_k$, implying that, 
$$
  \lambda_{\min}(H_k) \leq -\theta_k.
$$
Recall that in \autoref{lemma.optimal condition of subproblem}, we have $\theta_k > 0$, hence $\lambda_{\min}(H_k) < 0$. 
% On the other hand, with the M-Lipschitz continuity of $\nabla^2 f$, we have
% $$
% -M  \|\xk - x^*\| \cdot I\preceq \Hk - \nabla^2 f (x^*) \preceq M \|\xk - x^*\|\cdot I,
% $$
% which leads to $\Hk \succeq \left(\mu - M \|\xk - x^*\|\right) I \succeq \frac{\mu}{2} \cdot I \succeq 0$ for sufficiently large $k$ by \autoref{assm.local}. 
% %Thus, we must have $\Hk \succeq 0$ as $\xk \to x^*$ for sufficiently large $k$.
This makes a contradiction to $H_k \succ 0$. Then the proof is completed.
\end{proof}

The following lemma demonstrates that the step $d_k$ generated by the HSODM eventually reduces to the "small step" case for sufficiently large $k$. Consequently, we choose $\eta_k = 1$ and update the iteration by $x_{k+1} = x_k + d_k$ as shown in \autoref{subsection.convergence}. We remark that it is similar to the case of the classical Newton trust-region method (see \cite[Theorem 4.9]{nocedal_numerical_2006}), where the updates become asymptotically similar to the pure Newton step.
\begin{lemma}\label{lemma. large k small step}
    For sufficiently large $k$, we have $\|\dk\|\leq \Delta$.
\end{lemma}
\begin{proof}
Due to $t_k \neq 0$, by equation \eqref{eq.homoeig.foc t neq 0} in \autoref{corollary. foc}, we have
\begin{equation*}
    \dk = -(\Hk+\theta_k I)^{-1}\gk,
\end{equation*}
and further
\begin{align}
   \nonumber\|\dk\| & \leq \| (\Hk+\theta_k I)^{-1}\| \|\gk\| \\
   \nonumber             & \leq \frac{\|\gk\|}{\mu +\theta_k}       \\
  \label{eq.boundnormv}              & \leq \frac{\|\gk\|}{\mu}.
    \end{align}
The above inequalities holds because of $\Hk \geq \mu I $ and $\theta_k>0$. Note that with \autoref{assm.local}, $\|g_k\| \to 0$ as $k\to \infty$, then there exist a sufficiently large $K \ge 0$, such that
\begin{equation}\label{eq.localboundg}
    \|\gk\|\leq \Delta \mu, \forall k \ge K %\frac{2\mu\sqrt{\epsilon}}{M}.
\end{equation}
Combining \eqref{eq.boundnormv}, we conclude that $\|\dk\|\leq \Delta$ will be satisfied. 
\end{proof}
In the local phase, we set the perturbation parameter $\delta=0$ , 
\begin{equation}
\label{eq.homo local subproblem}
    \begin{aligned}
    \min_{\|[v; t]\| \le 1} \psi_k(v, t; 0) := ~ & 
    \begin{bmatrix}
       v \\ t
    \end{bmatrix}^T 
    \begin{bmatrix}
       \Hk   & \gk \\
       \gk^T & 0
   \end{bmatrix}
   \begin{bmatrix}
        v \\ t
    \end{bmatrix},
    \end{aligned}
\end{equation}
We also denote by $[v_k; t_k]$ the optimal solution to \eqref{eq.homo local subproblem}. Gathering the above results together, we are ready to prove the following theorem. 
\begin{theorem}
\label{theorem:local-quadratic}
    Suppose that \autoref{assm.lipschitz} and \autoref{assm.local} hold. For sufficiently large $k$, the HSODM converges to \(x^*\) quadratically, that is, 
    $$
        \|{x_{k+1}-x^*} \| \le O(\|\xk - x^*\|^{2}).
    $$
\end{theorem}
\begin{proof}
By \autoref{coro. large k t neq 0}, we have $t_k \neq 0$. Since we take $\delta = 0$, then with equation \eqref{eq.homoeig.foc t neq 0} in \autoref{corollary. foc}, we have 
\begin{equation*}
    g_k^T d_k = -\theta_k \quad \text{and} \quad (H_k + \theta_k I) d_k = - g_k, 
\end{equation*}
implying that 
\begin{align}
  \nonumber\| H_k^{-1} g_k + d_k \|  &=  \| - \theta_k H_k^{-1} d_k \| \\
  \nonumber  &\leq  \| H_k^{-1}  \| \cdot \|\theta_k \| \| d_k \| \\
\label{eq:bridge5}   &\leq \frac{1}{\mu} \| g_k\| \| d_k \|^2 
\end{align}

By \autoref{lemma. large k small step}, we have $ x_{k+1} = x_k + \dk $. Therefore, 
\begin{subequations}
    \begin{align}
    \nonumber   \|x_{k+1} - x^*\| & = \|x_{k} + d_k + H_k^{-1} g_k - H_k^{-1} g_k - x^* \|  \\ \nonumber     
    & \leq \|x_{k} - H_k^{-1}g_k - x^*\| + \|H_k^{-1}g_k + \dk\| \\
    \label{eq.local.opt1} & \leq \frac{M}{\mu}\|x_k - x^*\|^2 + \frac{1}{\mu} \|g_k\| \|\dk\|^2  \\
    \label{eq.local.opt2}  & \leq \frac{M}{\mu}\|x_k - x^*\|^2 + \Delta \|d_k\|^2, 
    % \label{eq.local.opt3} & \leq \frac{M}{\mu}\|x_k - x^*\|^2 + \frac{L}{\mu}\|x_k - x^*\| \|d_k\|^2
    \end{align}
\end{subequations}
where \eqref{eq.local.opt1} holds due to the standard analysis of Newton's method and equation \eqref{eq:bridge5}, and \eqref{eq.local.opt2} follows from $ \|\gk\|\leq \Delta \mu $ as stated in \autoref{lemma. large k small step}. Moreover, we have 
\begin{align}
\nonumber        \|\dk\|  &= \|  x_{k+1} - x^*  - \left( \xk - x^* \right) \| \\
\nonumber        & \leq \| x_{k+1} - x^* \|+\|\xk-x^*\|  \\
\nonumber        & \leq \frac{M}{\mu}\|\xk-x^*\|^2+\|\xk-x^*\|+\Delta \|d_k\|^2 \\
\label{eq.relation d and dist}     &\leq \frac{M}{\mu}\|\xk-x^*\|^2+\|\xk-x^*\|+\Delta^2 \|d_k\|.
\end{align}
By rearranging the terms and \autoref{lemma. large k small step} we have
$$    (1-\Delta^2)\|\dk\| \leq O(\|\xk-x^*\|)+\|\xk-x^*\|,$$
which leads to that $\|\dk\|\leq O(\|\xk-x^*\|)$. With \eqref{eq.local.opt2}, we conclude that
\begin{equation}
    \begin{aligned}
         \|x_{k+1} - x^*\| &\le \frac{M}{\mu}\|x_k - x^*\|^2 + \Delta \|d_k\|^2 = O(\|x_k - x^*\|^2).
    \end{aligned}
\end{equation}
This completes the proof.
\end{proof}


%%%%
% compare ARC & Curtis.
% \section{Numerical Experiments}
% 
% TBA.
\section{Numerical Results}

In this section, we provide the computational results of HSODM on a few classes of nonconvex optimization problems. First, we include a set of nonconvex $L_2-L_p$ minimization problems that arise from compressed sensing. This problem has long been one of our greatest interests. Next, we include the CUTEst problems \cite{gould_cutest_2015} since they serve as a standard dataset in the nonlinear programming community.

Because the HSODM belongs to the family of second-order methods, the comparison is set to quasi-Newton and second-order methods, including the limited memory BFGS method and Newton trust-region method. Furthermore, we also present the results of our recently proposed dimension-reduced second-order method (DRSOM) \cite{zhang_drsom_2022}. Strictly speaking, DRSOM builds quadratic approximations projected onto a predefined $r$-dimension subspace $\spak, r \ll n$, constructed by $v_1, ..., v_r$:
\begin{equation}\label{eq.drsom}
    \min_{d\in\mathbb{R}^n} m_k(d), ~\st~ d \in \spak, \|d\|\le \Delta.
\end{equation}
Since DRSOM only solves within the subspace,
\eqref{eq.drsom} is equivalent to finding the stepsizes of $v_j, j=1,...,r$:
\begin{equation}\label{eq.drsom.lowd}
    \min_{\alpha\in\mathbb{R}^r} m_k\left(\sum_{j=1}^r \alpha_j v_j\right),~\st~ \|d\|\le \Delta.
\end{equation}
In the simplest form, the subspace is constructed by the gradient and momentum using the Hessian-vector products: $$\spak = \spa\{\gk, m_{k}\},~m_{k}:= x_{k} - x_{k-1},$$
which in spirit aligns with the first-order method \cite{zhang_drsom_2022}.
Obviously, if the homogenized direction via \eqref{eq.homo subproblem} is used to construct to subspace $\spak$, one might expect a significant improvement can be observed from the resulting algorithm. We show that this does hold in our preliminary computational tests. This finding also expands the basic idea of using the homogenized model simply for a descent direction.

\subsection{Implementation details}
\paragraph{A simple adaptive version for HSODM.}
Apart from the generic form of HSODM \autoref{alg.main alg}, we add a few techniques for practical implementations.
We use the Lanczos method to solve homogenized subproblems and set a tolerance of the residual.
Instead of setting a predefined radius $\Delta$ at an iterate $x_k$, we implement a simple line-search method to dynamically adjust the stepsize $\eta_k$ since the direction $d_k$ is provided by the eigenvalue problem. We also set an upper bound on $\eta_k$ by the last successful size of the step. We present this simple strategy in \autoref{alg.ls}.
\begin{algorithm}
    \caption{A simple line-search procedure}\label{alg.ls}
    \quad \textbf{Parameters} $\rho \in (0,1), \beta \in (0,1)$\\
    \quad \textbf{At step} $k$ \textbf{do:}\\
    \quad\quad Solve the subproblem \eqref{eq.homo subproblem} to obtain $d_k$; \\
    \quad\quad Set $\eta_k := 2\cdot\| \eta_{k-1} \cdot d_{k-1} \| / \|\dk\| $ \\
    \quad\quad \textbf{While} $\rho_k < \rho$ \textbf{then:} \\
    \quad\quad\quad Compute $\rho_k = \frac{f(x_k + \eta_k\dk) - f(x_k)}{m_k(\eta_k\dk) - m_k(0)}$\\
    \quad\quad\quad Compute $\eta_k \leftarrow \beta \cdot \eta_k$\\
    \quad Set $x_{k+1} = x_k + \eta_k d_k$.
\end{algorithm}
In the sequel, we set $\beta = 0.8$, $\rho = 0.7$, and $\delta = 10^{-3}$. The residual tolerance for the Lanczos method is set to be $10^{-10}$.
\paragraph{DRSOM-H: plugging the homogenized direction $d_k$ into subspace $\spak$.}
As an alternative to line-search, we can use $d_k$ in the subspace of DRSOM and solve \eqref{eq.drsom}. In particular, we use $\gk, \dk$ to construct the subspace. Let $\vk = [\gk; \dk]$, then the subproblem becomes
\begin{equation}
    \min_{\alpha \in \mathbb{R}^2} \alpha^T\vk^T\Hk \vk\alpha + \gk^T\vk\alpha,~\st~ \|\vk\alpha\|\le \Delta_k.
\end{equation}
The update strategy of $\Delta_k$ follows from the original implementation in \cite{zhang_drsom_2022}. We mark this algorithm as \drsomh{} (DRSOM with Homogenized model). The original DRSOM using the momentum and gradient is named after \drsom{}. The code can be found in the \textit{DRSOM.jl} repository \footnotemark{}\footnotetext{For details, see \textit{https://github.com/COPT-Public/DRSOM.jl.}}.
\paragraph{The completing algorithms}
Other competing algorithms include the limited memory BFGS method (\lbfgs{}) with the Hager-Zhang line-search algorithm \cite{zhang_nonmonotone_2004}, and Newton trust-region method (\newtontr).
These algorithms are tested using the implementation in the third-party package \textit{Optim.jl}\footnotemark{}\footnotetext{For details, see \textit{https://github.com/JuliaNLSolvers/Optim.jl}}. The implementation of Hager-Zhang line-search algorithm is from in \textit{LineSearches.jl}\footnotemark{} \footnotetext{For details, see \textit{https://github.com/JuliaNLSolvers/LineSearches.jl}}.
In all the tests, we set the memory parameter for \lbfgs{} to $10$.



All the experiments are handled by the Julia version on a desktop of Mac OS with a 3.2 GHz 6-Core Intel Core i7 processor.


\subsection{$L_2-L_p$ Minimization}
We firstly test the performance of \hsodm{} for nonconvex \(L_2 - L_p\) minimization (see \cite{ge_note_2011,chen_complexity_2014,chen_smoothing_2012}). Recall \(L_2 - L_p\)  minimization problem:
\begin{equation*}
    \min _{x \in \mathbb{R}^{m}} \phi(x) = \frac{1}{2}\|A x-b\|_{2}^{2} + \lambda \|x\|_p^p,
\end{equation*}
where \(A \in \mathbb{R}^{n\times m}\), \(b\in \mathbb{R}^n, 0 < p < 1\).
The smoothed $\varepsilon$-approximation \cite{chen_smoothing_2012} is used for $p$-norm:
\begin{align*}
    \|x\|_p \approx \sum_{i=1}^n \left(s_\varepsilon(x_i)\right)^p,
    s_\varepsilon(y)  = \begin{cases}|y| & \text { if }|y|>\varepsilon \\ \frac{y^{2}}{2 \varepsilon}+\frac{\varepsilon}{2} & \text { if }|y| \leq \varepsilon\end{cases}, y\in\mathbb{R}
\end{align*}
We randomly generate datasets in different sizes \(n, m\) from Gaussian samples. The elements of matrix $A$ are generated by standard Gaussian distribution $A_{ij} \sim \mathbf N(0,1)$ with sparsity $r = 0.15, 0.25$. To construct the true sparse vector by Gaussian distribution $v\in\mathbb{R}^{m}$, we let for all \(i\):
\begin{equation*}
    v_{i} \sim \begin{cases}
        0                                    & \text{with probability $0.5$} \\
        \mathbf N \left(0,\frac{1}{n}\right) & \text{otherwise}
    \end{cases}
\end{equation*}
Then we let $b = Av + \delta$ where $\delta$ is the noise generated as $\delta_{i} \sim \mathbf N(0,1), \forall i$. The parameter $\lambda$ is chosen as $\|A^{T}b\|_{\infty}/5$. The smoothing parameter \(\varepsilon\) is set to \(10^{-1}\).

We report the iteration number needed to reach a $\epsilon$-approximate first-order stationary point at a precision of \(10^{-6}\), precisely, $\|\nabla f(\xk)\| \le \epsilon = 10^{-6}$. The iterations needed for a set of methods are reported in the \autoref{tab.lp}.
\begin{table}[ht]
    \centering
    \scriptsize
    \begin{tabular}{r|r|r|rr|rr|rr|rr|rr}
        \toprule
        \multirow{2}{*}{$n$} & \multirow{2}{*}{$m$} & \multirow{2}{*}{$r$} & \multicolumn{2}{c|}{\drsom} & \multicolumn{2}{c|}{\textsf{HSODM}} & \multicolumn{2}{c|}{\drsomh} & \multicolumn{2}{c|}{\lbfgs{}} & \multicolumn{2}{c}{\newtontr}                                           \\
                             &                      &                      & $k$                         & time                                & $k$                          & time                                & $k$                                    & time    & $k$ & time    & $k$ & time    \\
        \midrule
        300 & 100 & 1.5e-01 & 108 & 2.7e+00 &  62 & 5.3e-01 &  33 & 2.0e+00 & 122 & 2.1e-01 &  10 & 2.9e-02 \\
 300 & 200 & 1.5e-01 & 114 & 9.7e-02 &  34 & 1.5e-01 &  87 & 1.7e-01 & 191 & 1.9e-02 &  62 & 5.1e-01 \\
 300 & 500 & 1.5e-01 & 353 & 1.1e+00 &  87 & 9.5e-01 & 139 & 1.2e+00 & 274 & 6.1e-02 &  93 & 5.5e+00 \\
 500 & 100 & 1.5e-01 & 183 & 3.6e-02 &  70 & 3.6e-01 &  78 & 7.6e-02 & 160 & 9.0e-03 &  49 & 1.1e-01 \\
 500 & 200 & 1.5e-01 & 211 & 2.2e-01 &  32 & 8.5e-02 &  87 & 2.4e-01 & 180 & 2.1e-02 &  12 & 8.9e-02 \\
 500 & 500 & 1.5e-01 & 340 & 1.3e+00 &  67 & 8.7e-01 & 154 & 1.2e+00 & 278 & 8.5e-02 &  23 & 1.2e+00 \\
1000 & 100 & 1.5e-01 & 101 & 3.1e-02 &  81 & 4.0e-01 &  56 & 5.6e-02 & 149 & 1.3e-02 &  18 & 3.6e-02 \\
1000 & 200 & 1.5e-01 & 337 & 3.7e-01 & 140 & 8.1e-01 & 128 & 3.5e-01 & 285 & 5.2e-02 & 146 & 1.3e+00 \\
1000 & 500 & 1.5e-01 & 310 & 1.5e+00 &  92 & 1.5e+00 & 203 & 1.8e+00 & 259 & 1.2e-01 &  97 & 5.2e+00 \\
 300 & 100 & 2.5e-01 & 142 & 2.1e-02 &  18 & 1.8e-02 &  16 & 1.1e-01 &  80 & 4.0e-03 &  10 & 1.8e-02 \\
 300 & 200 & 2.5e-01 & 214 & 8.4e-02 &  27 & 6.7e-02 &  94 & 2.3e-01 & 252 & 2.0e-02 &  86 & 8.2e-01 \\
 300 & 500 & 2.5e-01 & 257 & 8.2e-01 &  66 & 8.8e-01 & 202 & 1.4e+00 & 325 & 7.5e-02 & 148 & 7.9e+00 \\
 500 & 100 & 2.5e-01 & 166 & 3.0e-02 &  27 & 4.3e-02 &  39 & 3.7e-02 & 132 & 1.0e-01 &  28 & 6.4e-02 \\
 500 & 200 & 2.5e-01 & 305 & 3.2e-01 & 170 & 1.0e+00 & 135 & 2.1e-01 & 257 & 4.2e-02 & 113 & 1.1e+00 \\
 500 & 500 & 2.5e-01 & 230 & 8.0e-01 & 100 & 1.5e+00 & 220 & 1.8e+00 & 376 & 1.1e-01 & 149 & 7.9e+00 \\
1000 & 100 & 2.5e-01 & 305 & 9.0e-02 & 102 & 3.4e-01 & 108 & 2.2e-01 & 225 & 2.0e-02 &  86 & 3.3e-01 \\
1000 & 200 & 2.5e-01 & 365 & 4.0e-01 &  22 & 2.5e-01 & 115 & 2.3e-01 & 242 & 4.8e-02 &  96 & 9.4e-01 \\
1000 & 500 & 2.5e-01 & 638 & 3.5e+00 & 127 & 2.7e+00 & 250 & 2.6e+00 & 415 & 1.8e-01 & 100 & 6.1e+00 \\
        \bottomrule
    \end{tabular}
    \caption{Performance of HSODM and completing algorithms compared to other algorithms: iterations needed for precision \(\epsilon = 10^{-6}\)} \label{tab.lp}
\end{table}
These results show that the \hsodm{} and \drsomh{} are fairly close to the \newtontr; it is far better than the quasi-Newton method, \lbfgs, and "first-order" \drsom{} in most test cases.


%%%%%%%%%%%%%%%%%
% cutest
\subsection{Unconstrained problems in CUTEst}
We next present the results on a selected subset of the CUTEst dataset. We limit our focus on the unconstrained problems with the number of variables $n \in [4, 200]$. And if one has different parameters, we choose the smallest instance that fits the criterion, which then creates a set of $105$ instances. We set an iteration limit of $20,000$ and terminate criterion $\|\nabla f(\xk)\| \le 10^{-6}$ for all the candidate algorithms; we check if this criterion is ensured else mark as failed.  The complete result can be found in \autoref{tab.cutest.kt} and \autoref{tab.cutest.fx}.

\paragraph{Overall comparison of the algorithms.}
The following table \autoref{tab.perf.cutest} presents a summary of tested algorithms. In this table, we let $\mathcal K$ be the number of successful instances, $k$ be average iteration needed, and $t$ be the average running time.
\begin{table}
    \centering
    \caption{Performance of different algorithms on the CUTEst dataset. Note $k$ and $t$ are only calculated by their corresponding successful ones. }
    \label{tab.perf.cutest}
    \begin{tabular}{lrrr}
        \toprule
        {}        & $\mathcal K$ & $k$    & $t$  \\
        \midrule
        \arc      & 95.00        & 312.17 & 2.29 \\
        \cg       & 95.00        & 444.77 & 0.08 \\
        \drsom    & 86.00        & 442.09 & 0.71 \\
        \drsomh   & 94.00        & 235.27 & 1.78 \\
        \hsodm    & 94.00        & 177.14 & 0.18 \\
        \lbfgs    & 97.00        & 201.90 & 0.19 \\
        \newtontr & 91.00        & 186.43 & 0.23 \\
        \bottomrule
    \end{tabular}
\end{table}


The results from these preliminary implementations show that \hsodm{} and \drsomh, by using the homogenized quadratic model, are quite comparable to the standard \newtontr{} on average.
While it is exciting to see the overall improvements, we are also informed by the cases where the new algorithms based on the homogenized quadratic model are not as effective. Whether this is a consequence of the global rate is not clear. Since the current implementations for \hsodm{} and \drsomh{} only apply the standard trust-region update rules, i.e., the ratio-based acceptance and linear radius adjustments, we
are confident that using the provably better frameworks like those in \cite{cartis_adaptive_2011,curtis_trust_2017} would have a promising improvement to \hsodm{}.


\section{Conclusion}

In this paper, we introduce a homogenized second-order descent method (HSODM) whose global rate of complexity is optimal among a certain broad class of second-order methods. The HSODM utilizes the homogenization trick to the quadratic model, the ordinary second-order Taylor expansion, such that the resulting homogenized quadratic form can be solved as an eigenvalue problem. We have shown that the homogenized idea is well-defined in both convex and nonconvex cases, where a negative curvature always exists. By using the model all along, one can safely stop at a small step to obtain an $\epsilon$-approximate second-order stationary point. Our experiments also illustrate the power of the homogenized model, not only when used as a descent direction, but also if it is used to construct the subspace for our recent dimension-reduced second-order method.

For future research, it is interesting to find a provably adaptive and practical framework other than the simple strategy like \autoref{alg.ls}. Furthermore, solving the subproblem of the HSODM inexactly is also intriguing to us. For example, the recent first-order methods explore the negative curvature in various ways, which we believe, can easily be adopted in place of the Lanczos method for large-scale problems.


\printbibliography[heading=bibintoc]
\clearpage
\appendix

\begin{landscape}
    \section{Detailed Computational Results for CUTEst Dataset}
    \tiny
    \input{cutest.tex}
    \normalsize
\end{landscape}

\end{document}


