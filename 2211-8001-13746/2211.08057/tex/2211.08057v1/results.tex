%%% old results are in results-old.tex


\subsection{Multilingual setting}

%%%%% Text-only Evaluation start %%%%%
\begin{table}[t!]
\centering
\resizebox{\columnwidth}{!}{  
\begin{tabular}{l | r r | r r}
\hline
{} & \multicolumn{2}{c}{\textbf{Coherence$\uparrow$}} & \multicolumn{2}{c}{\textbf{EN-DE text}}\\
\textbf{Model} & \textbf{EN} &  \textbf{DE} & \textbf{MRR$\uparrow$} & \textbf{JSD$\downarrow$}\\
%\Xhline{1.5pt} 
\hline 
PLTM & {0.064} & {0.044} & {0.333} & {0.067}\\
\textbf{ZeroshotTM}  & {0.113} & {0.096} & \textbf{0.997} & \textbf{0.012}\\
ZeroshotTM-KD & 0.109 & {0.092} & {0.390} & {0.081}\\ 
\hline \hline
M3L-Contrast & \textbf{0.119} & \textbf{0.097} & {0.684} & {0.036}\\ 
\end{tabular}
}
\caption{Language-specific topic coherences (NPMI) and cross-lingual retrieval peformance (MRR and JSD).}
\label{tab:results-text-only}
\end{table}
%%%%% Text-only Evaluation end %%%%%

Table~\ref{tab:results-text-only} shows the cross-lingual retrieval performance and averaged JSD of aligned articles. ZeroshotTM is the clear winner with an MRR of 0.997 and the lowest JSD. M3L-Contrast, while it does not outperform ZeroshotTM, shows encouraging results given that it has to infer twice as many topics as ZeroshotTM (bilingual case). It also outperforms PLTM, a classical multilingual topic model and the only other model, aside from M3L-Contrast, trained on multilingual articles. Moreover, M3L-Contrast also has the best topic coherences.

\subsection{Multimodal setting}

%%%%% Text and Images evaluation start %%%%%
\begin{table*}[h!]
\centering
\begin{tabular}{l | r r | r r | r r }
%\Xhline{1.5pt}
\hline
{} & \multicolumn{2}{c}{\textbf{Coherence$\uparrow$}}  & \multicolumn{2}{c}{\textbf{EN-images}} & \multicolumn{2}{c}{\textbf{DE-images}} \\
%\Xhline{1.5pt} 
%\hline
\textbf{Model} & \textbf{EN} & \textbf{DE} & \textbf{UAP$\uparrow$} & \textbf{JSD$\downarrow$}  & \textbf{UAP$\uparrow$} & \textbf{JSD$\downarrow$}\\
\hline 
ZeroshotTM & {0.113} & {0.096} & {0.034} & {0.445} & {0.039} & {0.435}\\
ZeroshotTM-KD & {0.109} & {0.092} & {0.082} & {0.128} & {0.093} & {0.146}\\
\hline \hline
\textbf{M3L-Contrast} & \textbf{0.122} & \textbf{0.097} & \textbf{0.125} & \textbf{0.130} & \textbf{0.102} & \textbf{0.147}\\
\end{tabular}
\caption{Language-specific topic coherences (NPMI) and text-image retrieval performance (UAP). JSD is the averaged JS divergence between topic distributions of aligned articles and images. Only M3L-Contrast is jointly trained on \textbf{multilingual articles and images}.}
\label{tab:results-text-images}
\end{table*}
%%%%% Text and Images evaluation end %%%%%

Table~\ref{tab:results-text-images} shows the results for text-image matching. M3L-Contrast performs the best with UAP of 0.125 and 0.102 for matching English and German articles to images, respectively, and has the lowest JSDs. In a reversal of the results for cross-lingual retrieval, ZeroshotTM performs the worst with the lowest UAP scores and highest JSDs. ZeroshotTM-KD only slightly outperforms ZeroshotTM, indicating that the success of M3L-Contrast can be attributed to \textit{joint} training and cannot be achieved with the teacher-student sequential training scheme.

These results indicate that ZeroshotTM without any modifications is not suitable for multimodal settings. One likely reason is that multimodal encoders like CLIP suffer from the so-called `modality gap' where embeddings for different modalities are mapped to separate regions in the embedding space \cite{ModalityGap2022}. %Our results show that we can alleviate the modality gap in multimodal topic modelling by adding a contrastive objective during training. %to encourage similar examples to be mapped closer to each other in the topic space and different examples to be mapped farther apart. %Elaine: Better not say this because the Modality Gap paper says that contrastive learning actually preserves this gap

Our results also indicate that for a joint multimodal and multilingual neural topic model, it could be beneficial to use a hybrid model that uses  separate inference networks for different modalities and a shared network for the same modality. We leave this for future work.

\subsection{Error analysis}
%%%%% Examples start %%%%%
\begin{table*}[ht!]
\begin{center}
\begin{tabular}{r p{2.8cm} | p{2.2cm} | p{2.3cm} | p{2.3cm}}
    \hline
    {} & {} & \textbf{EN article} & \textbf{DE article} & \textbf{Image}\\
    \hline\hline
    \multicolumn{5}{c}{Article title: \textbf{Capsicum pubescens}} \\
    \hline
    \multirow{6}{*}{\includegraphics[width=0.2\textwidth]{figures/Capsicum_pubescens_Rocoto.JPG}} & \textbf{ZeroshotTM} & {\textbf{21:} plant, leaves, flowers, tall} & {\textbf{31:} bird, south, america, species} & {\textbf{67:} university, library, museum, research}\\
    & \textbf{\textbf{ZeroshotTM-KD} \textit{(EN teacher)}} & \textbf{44: } plant, leaves, flowers, plants, genus & - & \textbf{44: } plant, leaves, flowers, plants, genus \\
    & \textbf{M3L-Contrast} & {\textbf{65:} plant, plants, leaves, flowers} 
    & {\textbf{65:} beschreibung (\textit{description}), pflanzen (\textit{plant}), selten 
    (\textit{rare}), stehen (\textit{stand})}
    &{\textbf{65:} plant, plants, leaves, flowers}\\
    \hline
    \hline
    \multicolumn{5}{c}{Article title: \textbf{Microexpression}/\textbf{Mikroexpression}} \\
    \hline
    \multirow{5}{*}{\includegraphics[width=0.25\textwidth]{figures/Universal_emotions7.JPG}} & \textbf{ZeroshotTM} & {\textbf{13}: include, cause, may, cases, occur} & {\textbf{13:} include, cause, may, cases, occur} & {\textbf{9:} bishop, catholic, pope, church, roman}\\
     & \textbf{\textbf{ZeroshotTM-KD} \textit{(EN teacher)}} & \textbf{32:} blood, symptoms, disease, cell, bone & - & \textbf{69:} album, released, song, single, group \\
     %& \textbf{\textbf{ZeroshotTM-KD} \textit{DE teacher}} & - & \textbf{20:} 20 sprachen (\textit{languages}), sprache (\textit{language}), begriff (\textit{concept}), gesprochen (\textit{spoken}), menschen (\textit{people}) & \textbf{57:} film (\textit{film}), leben (\textit{dorector}), jahren (\textit{actress}), regisseur (\textit{life}), schauspielerin (\textit{years})  \\
    & \textbf{M3L-Contrast} & {\textbf{84:} theory, term, example, social, defined} & {\textbf{84:} begriff (\textit{concept}), definition (\textit{definition}), beispiel (\textit{example}), theorie (\textit{theory}), zahl (\textit{number})} & {\textbf{5:} film, award, series, actress, born}\\
    \hline
\end{tabular}
\caption{Top topics of Wikipedia article pairs and a related image. The numbers indicate the topic indices.}
\label{tab:examples}
\end{center}
\end{table*}
%%%%% Examples end %%%%%
To further investigate differences between the models we checked some examples from the test set. We show two article-image tuples and their predicted topics in~Table~\ref{tab:examples}. The table contains the top topic for the aligned English and German articles (titles shown) and an image associated with them. 

The first example article is about pepper\footnote{\url{https://en.wikipedia.org/wiki/Capsicum_pubescens}}. ZeroshotTM predicts relevant topics for the English and German articles but off-topic for the image. %Topic numbers are different, thus in this case data re-matching failed. 
ZeroshotTM-KD (a teacher model trained on English articles and a student on images) predicts a relevant topic for the English article and the image but it has not been trained on  German. M3L-Contrast predicts relevant topics for the English and German articles and the image. %and in addition provided a topic label in German.
Though the table shows English topic labels for the image, it is equally possible to produce German image labels.
%, or English labels for German texts, if this is needed to a particular application.  

In the second example, the article about microexpressions\footnote{\url{https://en.wikipedia.org/wiki/Microexpression}} is illustrated with an image of a woman presenting basic emotions. ZeroshotTM predicts slightly relevant topics for the English and German articles but off-topic for the image. ZeroshotTM-KD also predicts a relevant topic for the English article. Although the image topic is different from the topic of the article, it is still somewhat relevant in that the model may have associated images of women with pop stars. M3L-Contrast predicts relevant topics for the English and German articles. For the image, it predicts a topic about actresses likely because the image depicts a woman.

We found similar behaviour in other cases: English and German articles are usually assigned with the same topic while the image often has a different topic. In many cases M3L-Contrast finds an aligned topic for an image while the other two models fail.

%The dataset contains many images of various plants, but the microexpression is probably unique. None of the models was able to find the aligned topic for an image, though both ZeroshotTM and M3L-Contrast found aligned topics for English and German. Yet the M3LContrast topic is more relevant for the article. 


\subsection{Visualizing the topic space}

\begin{figure*}[t!]
     \centering
     \begin{subfigure}{\textwidth}
        \centering
        \includegraphics[width=0.7\textwidth]{figures/topic-space-ctm-zeroshot-annotated_no_bg-big-legend.png}
        \caption{ZeroshotTM}
        \label{fig:topic-space-ctm}
     \end{subfigure}
     \begin{subfigure}{\textwidth}
        \centering
        \includegraphics[width=0.7\textwidth]{figures/topic-space-m3l-contrast-annotated_no_bg-big_legend.png}
        \caption{M3L-Contrast}
        \label{fig:topic-space-m3l-contrast}
     \end{subfigure}
    \caption{tSNE visualizations of topic distributions of multilingual texts and images inferred by ZeroshotTM and M3L-Contrast, respectively. Annotations are added manually. Best viewed in color.}
    \label{fig:topic-spaces}
\end{figure*}

To investigate the structure of the multimodal multilingual topic space, we use 2D visualizations presented in Figures~\ref{fig:topic-space-ctm} and~\ref{fig:topic-space-m3l-contrast}, for ZeroshotTM and M3L-Contrast, respectively. These figures show the proximities of multilingual texts and images, represented by their predicted topic distributions, in the topic spaces induced by the respective models and mapped into two dimensions with tSNE.\footnote{These figures are available as interactive plots in the code repository of this paper.}

In Figure~\ref{fig:topic-space-ctm}---the ZeroshotTM topic space---the topic distributions of the aligned articles are very similar to their counterparts (most of the points representing English articles are hidden under the German articles). The images, however, tend to be isolated instead of being close to their textual counterparts. This supports the modality gap hypothesis and explains why ZeroshotTM performs poorly in the text-image retrieval task.

In Figure~\ref{fig:topic-space-m3l-contrast}, the topic space induced by M3L-Contrast, articles and images tend to group together in terms of \textit{themes}---exactly the behaviour we want from a topic model. No single modality or language is isolated by itself. This explains why M3L-Contrast performs better than ZeroshotTM in text-image retrieval. On the other hand, the English and German articles are not as close to each other as in ZeroshotTM. This supports our claim that joint training takes into account data from all languages and adjusts for possible discrepancies between worldviews across languages, even though this property results in worse performance in cross-lingual text retrieval.

 