\subsection{Neural topic models}
Neural topic models (NTMs) refer to a class of topic models that use neural networks to estimate the parameters of the topic-word and document-topic distributions. Using a variational autoencoder (VAE) to map documents into latent topic spaces was proposed by \citet{srivastava2017autoencoding} and demonstrated in the ProdLDA model that exhibited better topic coherences and faster training than classical models. This has led to other VAE-based topic models that can incorporate information from external sources such as the Embedded Topic Model~\cite{dieng2020topic} which uses pretrained word embeddings, the Contextualised Topic Model~\cite{bianchi-etal-2021-pre} which uses contextualised embeddings and the BERT-based Autoencoder as Teacher~\cite{hoyle2020improving} model that distills large language models to improve topic coherence. %Aside from VAEs, other NTMs use Wasserstein autoencoders~\cite{nan-etal-2019-topic} and generative adversarial networks~\cite{wang2019atm,nguyen2021contrastive}.


%Though these models use distributed representations as an input and potentially can leverage multimodal embeddings, such as CLIP, to produce a topic distribution for an image, we are unaware of any previous work that actually performed such experiments. Thus, our paper is the first that demonstrates that multimodal topic modelling can be based on distributed image representations. 
%from BERT-like models and the BERT-based Autoencoder as Teacher (BAT) model that uses knowledge distillation to learn topics from parameter-rich models~\cite{hoyle-etal-2020-improving}. 
%Other NTMs use Wasserstein autoencoders~\cite{nan-etal-2019-topic} and generative adversarial networks~\cite{wang2019atm,nguyen2021contrastive} 

%Generative adversarial networks (GAN) have also been proposed as another method for inferring topic distributions. The Adversarial Topic Model~\cite{wang2019atm} is the first such method in this family and recently, an extension of ATM was proposed with a more principled approach to negative sampling and using contrastive loss to train the model~\cite{nguyen2021contrastive}. %The GAN consists of the Generator, which samples document-word distribution from the current state of the model and the Discriminator, which have to distinguish these generated distributions from those obtained from real documents. As the Generator learns to "deceive" the Discriminator, the model distributions becomes closer to the underlying distribution in the data. Recently, an extension of ATM has also been proposed. This method uses a more principle approach to negative sampling and uses contrastiive learning to train the model.
    
\subsection{Multilingual topic models}
Multilingual topic models infer aligned language-specific topics from a multilingual dataset. To align topics across languages, some degree of supervision is required to establish the link between the languages. In most cases, the languages are linked either at a word level or at a document level~\cite{hao2020empirical}. Models that use word-level supervision require a translation dictionary to link words from different languages~\cite{jagarlamudi2010extracting,hao2018learning,yang2019multilingual}. Document-level supervision requires a comparable dataset where a document in one language is linked to a thematically similar document in another language~\cite{mimno2009polylingual,de2009cross}. %These models have been used to compare issues and opinions across different communities on the Internet and social media~\cite{shi2016detecting,gutierrez2016detecting}.

The Polylingual Topic Model \cite[PLTM,][]{mimno2009polylingual} is widely-used classical multilingual topic model for comparable data. To our knowledge, the Neural Multilingual Topic Model~\cite{wu2020learning}, a model that uses word-level supervision, is the only neural multilingual topic model so far. ZeroshotTM~\cite{bianchi-etal-2021-cross}, while not a multilingual model, is capable of zero-shot cross-lingual topic inference: it can predict topic distributions for documents in unseen languages if the model is trained on embeddings from a multilingual encoder.  However, ZeroshotTM requires \textit{aligned} embeddings for zero-shot topic modelling. 


\subsection{Multimodal topic models} 
Multimodal topic models use data from different modalities %, other than text, 
to infer topics. The most popular pairing is texts and images. % because they tend to occur together and complement each other. 
Some text-and-image topic models use labelled image datasets to learn natural language representations of images using a supervised topic modelling approach~\cite{barnard2003matching,zheng2014topic}. Other models extract `visual words' from images using image feature extractors such as SIFT and images are represented as a bag of `visual words' in the same manner that documents are represented as a bag of textual words~\cite{feng2010visual,virtanen2012factorized,roller2013multimodal}.
\cite{an2020multimodal} trained visual and textual topic models from neural network representations for multimodal depression detection but does not map text and images into the same topic space.


\subsection{Contrastive learning} 
Contrastive learning is a self-supervised technique that uses different views of the same data to learn better data representations~\cite{jaiswal2021survey,liu2021self}. In contrastive training the goal is to minimize the distance between positive samples while separating them from negative samples.  Contrastive training is popular in multimodal settings such as web-scale text-image alignment~\cite{radford2021learning,jia2021scaling}, audio-visual alignment~\cite{khorrami2021evaluation} and biomedical imaging~\cite{zhang2020contrastive}.

In neural topic modelling, contrastive learning has recently been used to improve on the Adversarial Topic Model~\cite{wang2019atm} by adding a contrastive objective to the training loss and taking a more principled approach to sampling positive and negative samples~\cite{nguyen2021contrastive}. %Another example is~\cite{hoyle2020improving}, which used knowledge distillation to produce close topic distributions from bag-of-word and contextualized representations.

 %In this work, we use a contrastive learning objective during topic inference to improve the mapping of document-image pairs in the induced topic space.
%
%There were only few previous attempts to apply contrastive learning to neural topic modelling. \cite{nguyen2021contrastive} used contrastive learning to improve on the Adversarial TM by adding a contrastive objective to the training loss and taking a more principled approach to sampling positive and negative samples. In this case contrastive training was used to find the most difficult training examples. This model is monolingual. %though it is different from our approach both in model architecture---they use ATM as a backbone network while our model is an extension of CTM---and setting, since their model is monolingual.
%
%% This model is monolingual, so words and embeddings are derived from the same data view.
%
%In both cases the training results in a single topic model. In the case of M3LContrast, contrastive training results in a several topic models---one for each data view---that can be used independently during inference.
%
%
%The difference between previous work on joint image-and-text representation learning and our work is the same as between training embeddings and topic modelling in general: while embeddings allows to map any input object to a certain position in a highly multidimentional space, topic model provides an information about a content structure of  the input collection by mapping the data into a relatively small number of interpretable latent themes.
 
 %During contrastive training, different parts of the model take as an input different data views and output their representations. The loss function is based on the distance between these representations: different views on the same data item should result in close representations in the output space.
    
% %The key issue in this setting is to prevent \textit{model collapse}, i.e. outputting the same representation for all data items. The main approach to avoid collapse is to use negative samples~\cite{chopra2005learning,gutmann2010noise,schroff2015facenet}, though it is possible to use other methods, e.g. learning with momentum~\cite{grill2020bootstrap} or additional diversity constraint on the representation space~\cite{caron2020unsupervised}. 
    
% %The majority of work in the area of contrastive training is done with visual data because it is easy to obtain avarious views of the same image using data augmentation techniques like image cropping and rotation. In NLP the biggest success story of contrastive learning and data augmentation for textual data is the SentenceBERT model~\cite{reimers-gurevych-2019-sentence,thakur-etal-2021-augmented}. 
% Contrastive training is popular in multimodal settings such web-scale text and images~\cite{radford2021learning} and biomedical imaging~\cite{zhang2020contrastive}.%In the area of NLP data augmentation is less straightforward and the majority of self-supervised work is done using generative training~\cite{liu2021self}. There are few exceptions, with the biggest success story of contrastive learning and data augmentation for textual data being the SentenceBERT model~\cite{reimers-gurevych-2019-sentence,thakur-etal-2021-augmented}. %
% %Contrastive training has been also applied in different multimodal settings such web-scale text and images\cite{radford2021learning}, biomedical imaging\cite{zhang2020contrastive}.
% %Contrastive training has been also applied in a multimodal settings, e.g. to train a joint model for image and text~\cite{radford2021learning}.
    
% %In some cases, however, data augmentation is not necessary since different views on the same data are readily available. In this paper we deal with multi-lingual setting, for which multiple parallel corpora are available, as well as high-quality machine translation techniques. We also demonstrate that exact translations are not necessary since contrastive training---at least for topic modelling---could be applied using comparable corpora.

