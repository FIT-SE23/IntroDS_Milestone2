%Training topic modelling for images for a long time has been a goal for many researchers in the area. With the advent of multimodal embeddings and neural topic modelling producing multimodal topic models has become possible without any changes in the model. ZeroshotTM---or any other neural TM that uses embeddings as an input---can be trained on textual data encoded with multimodal distributional model, such as CLIP~\cite{radford2021learning},  and then applied to images. 

%Such zero-shot approach has been used before to obtain multilingual topic models~\cite{bianchi-etal-2021-cross}. In this paper, a similar experiment was for the first time conducted in a multimodal setting, demonstrating that a zero-shot approach for topic modelling could be used across modalities.

%However, the zero-shot approach has a serious limitation since it assumes that a pretrained aligned representation model exists for the data at hand. This is not always the case, and training such a model usually requires a big parallel dataset, much bigger than datasets commonly used to train multilingual topic models. 

%Moreover, the zero-shot approach somehow diminishes one of the main purposes of topic modelling, namely its ability to describe latent semantic themes in an input collection. Since a zero-shot model is trained on a single language it would contain biases encoded in a source data collection and would map all other languages and modalities in the source thematic structure. The same holds for the knowledge-distillation approach, where one language is used to train a 'teacher' model, that is then used to train 'student' models for other languages and modalities.

We presented M3L-Contrast, a multimodal and multilingual neural topic model based on ZeroshotTM that uses pretrained document and image embeddings. M3L-Contrast is trained jointly on multilingual texts and images and does not require aligned embeddings. Since it is a \textit{multilingual} topic model it produces aligned language-specific topics. As a \textit{multimodal} topic model, it maps texts and images into a shared topic space and infers textual representations, through the topic words, of the semantic concepts present in the images. %to produce the contextualised representations that is inputted into the model and it is trained jointly on all data views presented in the collection. 

%We believe that this would make the model less prone to biases represented in a certain language and plan to investigate the model behaviour in a low-resource setting. 

%An additional advantage of M3LContrast is its ability to produce topic labels in all languages present in the training data. 

%We performed quantitative evaluations of M3L-Contrast in multilingual and multimodal settings. 
We show that in the multilingual setting, M3L-Contrast improves on PLTM, a classical multilingual topic model, and that it is competitive with ZeroshotTM in the alignment of topic distributions for comparable documents in different languages. In the multimodal setting, our model significantly improves on ZeroshotTM %(and ZeroshotTM-KD) 
in aligning comparable texts and images in the topic space. Moreover, with unaligned text and image embeddings our model still performs better than ZeroshotTM that uses aligned embeddings.

%Our results show that M3L-Contrast also significantly improves on ZeroshotTM in multimodal topic modelling. In a text-only setting it outperforms a non-neural multilingual PLTM model and demonstrates comparable performance with ZeroshotTM. 

%Thus, the advantage of joint training is important for multimodal topic modelling and less important in multilingual setting, at least for such big languages as English and German. The possible explanation for that is modality gap~\cite{ModalityGap2022}, i.e. the property of multimodal representations to keep different modalities apart in the representation space, that makes zero-shot approach less efficient.

%As far as we know, the M3L-Contrast is the first joint neural multilingual and multimodal topic model. %Potential applications of this model include topic-aware image clustering, image recommendations, and mult imodal search. 
Our proposed architecture can easily be extended to include other modalities beyond image and text. We also believe that M3L-Contrast will be useful in a low-resource setting, where aligned embeddings can be difficult to obtain.