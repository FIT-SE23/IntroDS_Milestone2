\subsection{Neural multilingual topic model}
\label{sec:models-multilingual}

\begin{figure}[t!]
    \centering
    \includegraphics[width=0.8\columnwidth]{figures/m3l-contrast-new2.png}
    \caption{Proposed M3L-Contrast topic model. (a) Multilingual topic model with language-specific encoders and inference networks; (b) Extension to the multimodal setting. The loss function is detailed in Equation~\ref{eq:loss-m3l-contrast}.}
    \label{fig:archi-m3l-contrast}
\end{figure}

We first propose a neural multilingual topic model for comparable multilingual data that uses pretrained document embeddings. Our multilingual model is based on ZeroshotTM~\cite{bianchi-etal-2021-cross}, a zero-shot cross-lingual topic model. However, we are not aiming for a zero-shot model. Instead, our model infers aligned \textit{language-specific topics} for each language present in the dataset. Moreover, our approach does not require the pretrained document embeddings to be aligned beforehand. This property makes it advantageous in settings where a multilingual encoder that includes our desired language might not exist such as in low-resource settings.

Figure~\ref{fig:archi-m3l-contrast}(a) shows the multilingual model architecture. The model uses independent inference networks for each language. To align language-specific topics, the model minimizes the Kullback-Leibler (KL) divergence between the topic distributions of comparable documents from different languages and, in addition, uses a contrastive loss to map similar instances close to each other in the topic space and keep non-related instances apart.  

For each tuple of aligned documents in the comparable multilingual dataset, we encode the documents from each language using their own separate encoders (whether aligned or non-aligned) and then the embeddings are passed to language-specific inference networks that infers the mean, $\mu$, and variance, $\sigma^2$, of the Gaussian distribution from which we sample latent document-topic distributions. At this point, the languages are independent of each other and have not yet shared any information.  

After sampling topic distributions for each document, we induce a shared topic space by minimizing the pairwise KL divergence between the language-specific distributions whose parameters are estimated from their own inference network. We also add a contrastive objective so that aligned examples are kept away from other examples in the topic space. 
We use InfoNCE~\cite{van2018representation} as our contrastive loss. The positive pairs are all possible combinations of document pairs from the same tuple and negative pairs are all other pairs of documents from different tuples within a batch. For instance, for a comparable dataset with two languages and batch size $N$, we would have $N$ positive pairs and $N^2-N$ negative pairs per batch. For three languages, that would be $3N$ positive pairs and $3(N^2-N)$ negative pairs, etc.
% (n choose 2) = n! / (2 (n-2)!)

Thus, the loss consists of the three components: the reconstruction loss; the KL divergence between topic distributions; and the contrastive loss. %that ensures that pairs from the same tuple have similar topic distributions while pairs from different tuples are dissimilar in the topic space. 
Formally, the loss function is written as:
\begin{align}
\mathcal{L} & = \sum_{l=0}^{L} \mathbb{E}_{q}[w^{\top} \log(\text{softmax}(\beta_{l}\theta_{d}))] - \nonumber \\
& \sum_{\substack{a,b=0\\a \neq b}}^{n} \mathbb{KL}(p(\theta_{i}^{a} | x_{i}^{a}) || q(\theta_{i}^{b} | x_{i}^{b})) - \nonumber \\ 
& s \sum_{\substack{a,b=0\\a \neq b}}^{n} \log \frac{\exp( (\theta_{i}^{a} \cdot \theta_{i}^{b}) / \tau)}{ \sum_{j=0}^{N} \sum_{c,d=0}^{n} \exp( (\theta_{i}^{c} \cdot \theta_{j}^{d}) / \tau)}    
\label{eq:loss-m3l-contrast}
\end{align} 

The first term is the sum of the bag-of-words (BoW) reconstruction losses of each language in the corpus. %$w_d$ is the BoW representation of document $d$ for language $l$, $\beta_{l}$ is topic-term matrix for language $l$ and $\theta_d$ is the sampled topic representation for document $d$. 
We refer the reader to~\cite{srivastava2017autoencoding} for further details on the reconstruction loss.

The second term is the sum of the KL divergences between the language-specific document distributions, $p()$ and $q()$, whose mean and variance are estimated from language-specific inference networks; $\theta$ refers to the sampled topic representation of a document in a tuple where $i$ is the tuple index, $a$ and $b$ are the indices of the documents inside the tuple and $n$ is the size of the tuple. Lastly, $x$ refers to a document embedding.    

The third term is the InfoNCE loss where $(\theta_{i}^{a} \cdot \theta_{i}^{b})$ are positive pairs (they belong to the same tuple) and $(\theta_{i}^{c} \cdot \theta_{j}^{d})$ are negative pairs (they are from different tuples). $N$ is the batch size, $\tau$ is the temperature and $s$ is a constant to give additional weight to the contrastive loss.

\subsection{Extension to multimodal setting}
\label{sec:models-multimodal}
We now extend the proposed multilingual topic model to the \textit{multimodal} setting. Figure~\ref{fig:archi-m3l-contrast}(b) shows the architecture of the proposed multilingual \textit{and} multimodal topic model.  

We can think of the multimodal case as a generalization of the multilingual model. The loss function in Equation~\ref{eq:loss-m3l-contrast} remains essentially the same. Since a BoW representation is not available for images, the reconstruction loss is computed only on texts and the first loss term is unchanged. In the second term of the loss function, $x$ can be a document \textit{or} image embedding and $\theta$ is the sampled topic distribution for that embedding. 

Since the document or image embeddings abstract the modality of the data, the topic distributions are now modality-agnostic. Thus, the third term is also unchanged, except for the tuple size $n$. A multimodal dataset with one language and one image view would have $N$ positive and $N^2-N$ negative pairs, the same as in the bilingual case. For two languages \textit{and} one image, we would have $3N$ positive pairs and $3(N^2-N)$ negative pairs, as in the trilingual case. 

We refer to our proposed topic model as \textbf{M3L-Contrast} for \textit{multimodal multilingual (M3L) topic model with contrastive learning}.


