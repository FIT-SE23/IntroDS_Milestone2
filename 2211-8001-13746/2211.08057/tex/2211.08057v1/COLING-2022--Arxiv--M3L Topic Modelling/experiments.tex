\subsection{Dataset}
We run experiments on our proposed model on a dataset of aligned English and German Wikipedia articles and images. We take aligned articles from the Wikipedia Comparable Corpora\footnote{\url{https://linguatools.org/tools/corpora/wikipedia-comparable-corpora/}} and align them with images from the Wikipedia-based Image Text dataset (WIT)~\cite{srinivasan2021wit} \footnote{\url{https://github.com/google-research-datasets/wit}}. We use articles instead of the image descriptions in WIT because topic models are designed for full documents, rather than snippets of text. 

We randomly select 20,000 tuples for training. Since articles can be associated with more than one image and we want fixed-size tuples during training, we randomly select one image per article pair. For testing, we randomly select 1,000 article pairs. We consider \textit{all} images aligned with the paired articles, which results in 3,278 unique tuples in the test set.  

\subsection{Evaluation}
We evaluate M3L-Contrast in the multilingual setting and the multimodal setting, separately. In the multilingual case we train M3L-Contrast on multilingual articles \textit{without} images and for the multimodal case on the multilingual articles \textit{and} images.

We want document-topic distributions for multilingual articles and images from the same tuple to be similar to each other 
\textit{and} distinct from other examples. Thus, we evaluate the alignment of topic distributions using retrieval tasks.\footnote{We are aware that topic distributions do not outperform raw embeddings in retrieval tasks but the point of this evaluation is not to improve cross-lingual or cross-modal retrieval but to evaluate the alignment of the topic distributions.}

Texts and images are fed one at a time to their own language-specific and modality-specific inference networks to obtain topic distributions. For the multilingual setting, we match an English article to the most similar German article in terms of the Jensen-Shannon divergence (JSD) between their respective document-topic distributions. For the multimodal setting, we match English articles to images and German articles to images, separately. We use mean reciprocal rank (MRR) to measure text retrieval performance and uninterpolated average precision~\cite[UAP,][]{manning1999topics} to measure text-image retrieval performance because multiple images can be associated with an article.

We also report the averaged JSD between the topic distributions for all data pairs from the same tuple. Lastly, we compute language-specific topic coherences with respect to the training data using normalised pointwise mutual information~\cite[NPMI,][]{roder2015exploring}\footnote{Computed using the Gensim library~\cite{rehurek2010software}.}. 

\subsection{Baselines}
%\begin{itemize}
    %\item 
    ~~~\textbf{PLTM}~\cite{mimno2009polylingual} We implement PLTM with Gibbs sampling.
 
    %\item 
    \textbf{ZeroshotTM}~\cite{bianchi-etal-2021-cross} We train separate models on the English and German articles using the authors' original implementation.\footnote{\url{https://github.com/MilaNLProc/contextualized-topic-models}}
 
    %\item 
    \textbf{ZeroshotTM-KD}~\cite{pivovarova2021visual} We adapt ZeroshotTM for multilingual or multimodal settings using knowledge distillation~\cite{hinton2015distilling}. This method uses the parameters learned by the teacher model as priors for the student. We train four separate teacher-student pairs: (1) Model trained on English articles as teacher, German as student; (2) German articles as teacher, English as student; (3) English articles as teacher, images as student; and (4) German articles as teacher, images as student.
%\end{itemize}

\subsection{Configurations}
We report the performance of the neural topic models using CLIP~\cite{radford2021learning} as our multimodal multilingual encoder for a fair comparison.\footnote{ \textit{clip-ViT-B-32} for images and  \textit{clip-ViT-B-32-multilingual-v1} for texts.} CLIP is a pretrained vision-language model trained on web-scale data that encodes text and images into a common embedding space. We train all models with 100 topics for 100 epochs. Other hyperparameters are discussed in the Appendix. We use batch size 32 for M3L-Contrast. In Section~\ref{sec:ablation-study} we show the performance of M3L-Contrast for different encoder combinations (aligned and unaligned), different batch sizes and topic numbers.

%It has been shown that in document retrieval topic modelling does not outperform embedding-based models~\cite{zosa-etal-2020-comparison} and the same results holds for text-image matching~\cite{pivovarova2021visual}. We do not propose any document matching methods in this paper but rather use this task as a way to measure model consistency across multiple data views.


