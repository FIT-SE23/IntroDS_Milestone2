
\subsection*{Data preprocessing}
We follow the training data preprocessing of \citet{bianchi-etal-2021-cross} for the BoW input: removing stopwords and retaining the 2000 most frequent words of each language as our vocabularies. We use the English and German stopword lists from NLTK\footnote{\url{https://www.nltk.org/}}.

\subsection*{Hyperparameters}
The neural topic models are trained on a single Nvidia V100 GPU (35 minutes) while PLTM is trained on a single Intel Xeon CPU (3 hours). During testing, we averaged the inferred topic distributions for each article/image from 20 samples. For all the neural models we used Adam optimizer with a learning rate of $2^{-3}$. We use a batch size of 64 except for M3L-Contrast. For M3L-Contrast, we set the temperature $\tau$ to 0.07 following \cite{guo2022multilingual}. We set the contrastive weight $s$ to 50 based on initial experiments. Tuning $\tau$ and $s$ are saved for future work.

\subsection*{Inference network}
We use the same inference network structure as ZeroshotTM~\cite{bianchi-etal-2021-cross}: one fully-connected hidden layer followed by softplus layer with 100 dimensions. We save the investigation of other inference network structures for future work.


\subsection*{Encoder Details}
We use SentenceBERT to encode all our data~\cite{reimers-gurevych-2020-making}~\footnote{\url{https://www.sbert.net/docs/pretrained_models.html}}. For a fairer comparison, we set the maximum sequence length of all text encoders to 128 tokens. The multilingual text encoder is \textit{paraphrase-multilingual-mpnet-base-v2}. For the monolingual encoders, the English encoder is \textit{all-mpnet-base-v2} and the German encoder is \textit{T-Systems-onsite/erman-roberta-sentence-transformer-v2}. ResNet embeddings are provided in this Kaggle challenge: \url{https://www.kaggle.com/competitions/wikipedia-image-caption}.

%\subsection*{Impact Statement}
\input{impact}

