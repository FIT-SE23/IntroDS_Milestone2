\section{Introduction}

Coresets have become a staple in the design of algorithms for large data sets. In the most general setting, a coreset compresses the data set in such a way that for any set of previously specified candidate queries, the cost of evaluating the query and the cost of the coreset are similar, up to an arbitrary small distortion.

A popular subject in coreset literature is the Euclidean $k$-means problem. Here, we are given $n$ points $P$ in $d$ dimensions and our task is to find a set of $k$ points $C$ called centers minimizing
$\cost(P,C):=\sum_{p\in P} \min_{c\in C}\|p-c\|^2,$
where $\|p-c\| = \sqrt{\sum_{i=1}^d (p_i-c_i)^2}$ denotes the Euclidean distance.
In this case, a coreset is a weighted subset of the input such that difference between the cost for any set of $k$ centers $C$ on the coreset and the cost on the original point set $P$ is at most $\varepsilon\cdot \cost(P,C)$.
Since its initial study by \cite{HaM04}, the Euclidean $k$-means problem has received arguably the most attention out of any coreset problem. The current state of the art by \cite{CGSS22} yields coresets of size $\tilde{O}(k\varepsilon^{-2} \min(k,\varepsilon^{-2}))$, where $\tilde{O}(x)$ hides multiplicative factors that are polylogarithmic in $x$. Similarly, for Euclidean $k$-median where we aim to minimize the sum of Euclidean distances, rather than the sum of squared Euclidean distances, the best known upper bounds are $\tilde{O}(k\varepsilon^{-2} \min(k,\varepsilon^{-1}))$. Unfortunately, there is still a gap towards the best known lower bound of $\Omega(k\varepsilon^{-2})$ by \cite{CGSS22}.

We thus have the option of obtaining either an optimal dependency on $k$, at the cost of a suboptimal dependency on $\varepsilon^{-1}$, or an optimal dependency on $\varepsilon^{-1}$, at the cost of a suboptimal dependency on $k$. While these bounds suggest that the lower bound is the correct answer, things are not as clear on closer inspection. Quadratic dependencies on $k$ become necessary for many forms of analysis and so far, it is unknown how to avoid this loss while retaining an optimal dependency on the remaining parameters\footnote{Coresets of size $\tilde{O}(kd / \varepsilon^{2})$ are also known (\cite{CSS21}). This offers improvements in low dimensions, but generally a dependency on $d$ is considered worse than a dependency on $k$ or $\varepsilon^{-2}$.}. 
 
In this paper we break through this barrier. Specifically, we show that coresets of size $\tilde{O}(k^{3/2} \varepsilon^{-2})$ for Euclidean $k$-means and $\tilde{O}(k^{4/3} \varepsilon^{-2})$ for Euclidean $k$-median exist. 
In our view, this is further and arguably stronger evidence that the $k \varepsilon^{-2}$ bound will be the correct answer. Another contribution exists on a technical level. Previously, most coreset constructions for high dimensions heavily relied on terminal embeddings to facilitate the analysis. 
In this paper, we present a novel method that avoids terminal embeddings. We expect that our technique may have further applications for coreset constructions in Euclidean spaces.


\subsection{Techniques}
Starting point of our work is the framework introduced by \cite{CSS21} and specifically \cite{CGSS22}. Prior to \cite{CGSS22}, all coreset analyses required a dependency of at least $k\cdot d \cdot \varepsilon^{-2}$. 
To illustrate why, suppose we are sampling points from some distribution and wish to use the sampled points as an estimator for the true cost of any candidate solution. The analysis then consists of (1) a bound on the variance $\sigma^2$ of the estimator and (2) a bound on the number of solutions $|\mathbb{N}|$ to be approximated. This typically results in coresets of size $O(\varepsilon^{-2} \cdot \sigma^2 \cdot \log |\mathbb{N}|)$.
When enumerating all (discretized) candidate solutions (henceforth called a \emph{net}) in $d$ dimensions, virtually all known techniques result in $|\mathbb{N}| \approx \exp(k\cdot d)$. 
The dependency on $d$ may be reduced to $\log(k/\varepsilon)\varepsilon^{-2}$ using dimension reduction techniques. 

To bypass this, \cite{CGSS22} used a chaining-based analysis to define a sequence of discretized candidate solutions. Specifically, they showed that there exist discretizations $\mathbb{N}_{\alpha}$ of size $\exp(k\alpha^{-2})$ such that for any point $p$ and any solution $\calS$, there exists a solution $\calS_{\alpha}$ with 
$$|\cost(p,\calS_{\alpha}) - \cost(p,\calS)|\leq \alpha\cdot \cost(p,\calS),$$
where $\cost(p,\calS_{\alpha})=\min_{s\in \calS_{\alpha}} \|p-s\|^2$ and $\cost(p,\calS)=\min_{s\in \calS}\|p-s\|^2$.
The idea of the analysis is then to write $\cost(p,\calS)$ as a telescoping sum 
$$\cost(p,\calS) = \sum_{h=1}^{\infty} \cost(p,\calS_{2^{-(h+1)}}) - \cost(p,\calS_{2^{-h}})$$
and show that the sampled points achieve concentration for each summand. The number of candidate solutions is now $|\mathbb{N}_{2^{-(h+1)}}| \cdot |\mathbb{N}_{2^{-h}}| \approx \exp(k\cdot 2^{2h})$ but the difference in cost is $2^{-h}\cost(p,\calS)$. This directly leads to a variance of the order $2^{-2h}\sigma^2$, where $\sigma^2$ is the variance of the basic estimator. Thus, the increase in net size is countered by the decrease and variance. Using this technique, \cite{CGSS22}  obtained a coreset of size roughly $\varepsilon^{-2} k\sigma^2$.

To obtain a bound on $\sigma^2$, the framework by \cite{CSS21} proposed an algorithm that first computes a solution $\greedy$ with strong structural properties and then samples any point $p$ proportionate $\cost(p,\greedy)$. To simplify the exposition, we assume that $\greedy$ is the optimum, every cluster has identical cost and every point has identical cost. For this special case, the distribution of \cite{CSS21} turns out to be equivalent to uniform sampling.
For any given solution $\calS$, let $k_i$ be the number of clusters of $\greedy$ whose points are served at cost $2^i$ times their cost in $\greedy$ for $i>2$\footnote{Observe that if one points of a cluster $C_j$ of $\greedy$ costs $2^i$ times more for $i\geq 3$, then all points from $C_j$ do likewise (up to constant factors).}, i.e. $2^i = \frac{\cost(p,\calS)}{\cost(p,\greedy)}$.
\cite{CSS21} showed that their sampling distribution leads to a variance of the order $\sigma^2 \approx \left(\frac{k\cdot k_i 2^i}{(k+k_i\cdot 2^i)^2}\right)\cdot  \min(\varepsilon^{-2},2^i,k)$, which yields the $\tilde{O}(k\varepsilon^{-2}\min(k,\varepsilon^{-2}))$ bound.

To improve either the variance or the net size has to reduced. Unfortunately, is unlikely that reducing $\sigma^2$ will be possible as the bounds on $\sigma^2$ obtained by \cite{CGSS22} are tight up to constant factors. Our main goal is to find a net with an finer error of 
$$|\cost(p,\calS_{\alpha}) - \cost(p,\calS)|\leq 2^{-h}\cdot \sqrt{\cost(p,\calS),\cost(p,\greedy)}.$$
In the case of $\min(2^i,\varepsilon^{-2},k)=2^i$, this leads to a reduced variance for the $h$-th summand of the order $2^{-2h}\cdot \frac{(\sqrt{\cost(p,\calS)\cost(p,\greedy)})^2}{\cost(p,\greedy)^2} \cdot \left(\frac{k\cdot k_i}{(k+k_i\cdot 2^i)^2}\right)\leq 2^{-2h}\cdot  \left(\frac{k\cdot k_i \cdot 2^i}{(k+k_i\cdot 2^i)^2}\right).$
To find such a net, we now have two options. First, we can essentially use the previous nets and rescale $2^{-h}$ by a factor $2^{-i/2}$. Unfortunately, this leads to nets of size $\exp(k\cdot 2^{2h}2^i)$, so any gain in reducing the variance is countered by an increase in the net size.

The novelty in our approach now lies in showing that a net of size $\exp(k\cdot k_i \cdot 2^{2h})$ exists. Combining the two net sizes with the improved variance bound results in a coreset of size roughly $\varepsilon^{-2} \log \left( \exp(k\cdot \min(k_i,2^i) \cdot 2^{2h})\right)\cdot  2^{-2h}\cdot  \left(\frac{k\cdot k_i \cdot 2^i}{(k+k_i\cdot 2^i)^2}\right) $, which after some calculation is shown to be of the order $O(k^{1.5}\varepsilon^{-2})$.

For the remainder of this section we will illustrate how these improved nets can be obtained. The key idea already appears in the case of a single center. Suppose that $s$ is a candidate center. Then we can show that there always exists a subspace with orthogonal basis $U$ spanned by a set $T$ of at most $O(\alpha^{-2})$ points of $P$ such that 
$$ |p^T(I-UU^T) s| \leq \alpha\cdot \|(I-UU^T)p\|\cdot \|(I-UU^T)s\|.$$
We can then write
$$ \|p-s\|^2 = \|U(p-s)\|^2 + \|(I-UU^T)p\|^2 + \|(I-UU^T)s\|^2 - 2p^T(I-UU^T) s.$$
The error for the terms $\|Up-s\|^2$ and $\|(I-UU^T)s\|^2$ can be made negligibly small using a net of size $\exp(\alpha^{-2})$. Thus the main loss in error comes from the $p^T(I-UU^T) s$ term. The key insight is that when adding the center $c_i$ of point $p$ in solution $\greedy$, we have $\|(I-UU^T)p\|^2 \leq \sqrt{\cost(p,\greedy)}$. Thus, when adding all the $k_i$ centers of clusters that cost $2^i$ more in $\calS$ than in $\greedy$ to $T$ the net size becomes $\exp(\alpha^{-2}+k_i)$.
By composing these nets for $k$ candidate centers, we then obtain our desired bound.

 
\subsection{Related Work}

There has been a tremendous amount of work on coresets for Euclidean $k$-means and $k$-median following the work in
\cite{BachemL018,BachemLL18,BakerBHJK020,BFS21,BecchettiBC0S19,BCJ22,BravermanJKW21,
Chen09,Cohen-AddadL19,CSS21,Cohen-AddadSS21,FL11,FeldmanSS20,
FengKW21,FGSSS13,HaK07, HaM04, huang2020coresets,HuangJLW18, HuangJV19, LS10, SSS19,SchwiegelshohnS22,SohlerW18}. For a detailed comparison, we refer to Table \ref{table:core}.

\begin{table}[H]
\begin{center}
\begin{tabular}{r|c}
Reference & Size (Number of Points) %& specificity %& Update Time 
\\
\hline\hline
\multicolumn{2}{l}{{\bf Coreset Bounds in Euclidean Spaces}} \\
\hline\hline
\multicolumn{2}{l}{Lower Bounds} \\\hline\hline
Baker, Braverman, Huang, Jiang,  & \multirow{2}{*}{$\Omega(k\cdot \varepsilon^{-1/2})$} \\
 %&
Krauthgamer, Wu (ICML'19)~\cite{BravermanJKW19} & \\ 
\hline
Huang, Vishnoi (STOC'20)~\cite{huang2020coresets} & $\Omega(k\cdot \min(d,2^{z/20}))$  %&
\\\hline
Cohen-Addad, Larsen, Saulpic,  & \multirow{2}{*}{$\Omega(k \cdot \varepsilon^{-2}/z^4)$} \\
 %&
 Schwiegelshohn \cite{CGSS22} (STOC'22) & \\ 
\hline
\hline
\multicolumn{2}{l}{Upper Bounds} \\\hline\hline
Har-Peled, Mazumdar (STOC'04)~\cite{HaM04} & $O(k\cdot \varepsilon^{-d}\cdot \log n)$ %& Only for $k$-Median and $k$-Means
\\
\hline
Har-Peled, Kushal (DCG'07)~\cite{HaK07} & $O(k^3 \cdot\varepsilon^{-(d+1)})$  %&
\\\hline
%Feldman et al. (SoCG'07)~\cite{FMS07} & $O(k^2\varepsilon^{-5})$ 
%%& $O(ndk^2\varepsilon^{-1}\log^2 n)$ 
%\\\hline
Chen (Sicomp'09)~\cite{Chen09} & $O(k^2 \cdot d \cdot \varepsilon^{-2}\cdot \log n)$ %& 
\\\hline
Langberg, Schulman (SODA'10)~\cite{LS10} &$O(k^3\cdot d^2 \cdot \varepsilon^{-2})$ %&
%& $\tilde{O}( ndk + d^2k^3\varepsilon^{-2}\log^3n)$ 
\\\hline
Feldman, Langberg (STOC'11)~\cite{FL11} & $O(k\cdot d\cdot\varepsilon^{-2z})$ %& 
\\\hline
Feldman, Schmidt, Sohler (Sicomp'20)~\cite{FeldmanSS20} & $O(k^3\cdot\varepsilon^{-4})$ %& Only for $k$-means
\\\hline
Sohler, Woodruff (FOCS'18)~\cite{SohlerW18} & $O(k^2\cdot\varepsilon^{-O(z)})$  %& Runs in exponential time
\\\hline
Becchetti, Bury, Cohen-Addad, Grandoni,  & \multirow{2}{*}{$O(k\cdot\varepsilon^{-8})$}\\ Schwiegelshohn (STOC'19)~\cite{BecchettiBC0S19} & \\ %& Only for $k$-means
\hline
Huang, Vishnoi (STOC'20)~\cite{huang2020coresets} & $O(k\cdot\varepsilon^{-2-2z})$ %&
\\
\hline
Bravermann, Jiang, Krautgamer, Wu (SODA'21)~\cite{BravermanJKW21} & \multirow{1}{*}{$ O (k^2 \cdot\eps^{-4})$} %&
\\
\hline
Cohen-Addad, Saulpic, Schwiegelshohn (STOC'21)~\cite{CSS21} & $\tilde O (k\cdot\eps^{-2-\max(2,z)})$ %&
\\\hline
Cohen-Addad, Larsen, Saulpic,  & \multirow{2}{*}{$O(k \cdot \varepsilon^{-2} \cdot \min(k,\varepsilon^{-z}))$} \\
 %&
 Schwiegelshohn \cite{CGSS22} (STOC'22) & \\ 
\hline
\large \textbf{This paper} & $ O (k \cdot \varepsilon^{-2} \cdot \min(k^{z/(z+2)},\varepsilon^{-z}))$ %&
\end{tabular}
\end{center}
\caption{Comparison of coreset sizes for $k$-means ($z=2$) and $k$-median ($z=1$) in Euclidean spaces. 
%Writing the exponent $\varepsilon^{-O(1)}$ means that the dependency on $\varepsilon$ is different for $k$-means and $k$-median.  
\cite{BravermanJKW19} only applies to $k$-median and \cite{BecchettiBC0S19,FeldmanSS20} only apply to $k$-means. 
\cite{SohlerW18} runs in exponential time, which has been addressed by Feng, Kacham, and Woodruff \cite{FKW19}.
Aside from~\cite{HaK07,HaM04}, the algorithms are randomized and succeed with constant probability. Any polylog factors have been omitted in the upper bounds.}
\label{table:core}
\end{table}

Almost as prolific is the catalogue of work on dimension reduction for clustering problems in Euclidean spaces, see ~\cite{BoutsidisMD09,BoutsidisZD10,BoutsidisZMD15,CW22a,CW22b,CEMMP15,Cohen-AddadS17,DrineasFKVV04,FKW19,MakarychevMR19}. The arguably most important dimension reduction technique for coresets are terminal embeddings, see~\cite{ChN21,ElkinFN17,MahabadiMMR18,NaN18}. 

Further work on coresets considering objects other than points as centers \cite{BJKW21,FeldmanMSW10,HuangSV21} or other objectives all together 
\cite{BoutsidisDM13,HuangSV20,JKLZ21,KarninL19,MaiMR21,MK18,MunteanuSSW18,PhillipsT20,
TukanMF20}.
For further reading, we refer the interested reader to recent surveys \cite{Feldman20,MunteanuS18}.
