\documentclass[egpaper_final]{subfiles}
\begin{document}
    There has been a significant amount of work on accelerating CNNs~\cite{cheng2017survey}, 
    starting from brain damage~\cite{lecun1989optimal,hassibi1993second}.
    Many of them fall into three categories: \implementation~\cite{bagherinezhad2016lcnn}, 
    quantization~\cite{Rastegari2016}, and \structured~\cite{jaderberg2014speeding}.

    \Implementation\ based methods~\cite{mathieu2013fast,vasilache2014fast,lavin2015fast,bagherinezhad2016lcnn} accelerate convolution, with special convolution algorithms like FFT~\cite{vasilache2014fast}.
    Quantization~\cite{courbariaux2016binarynet,Rastegari2016,phan2020mobinet} reduces floating point computational complexity. 

    \Sparseconnect\ eliminates connections between neurons~\cite{han2015learning,liu2015sparse,lebedev2015fast,han2016eie,guo2016dynamic,zhong2018shift,he2019addressnet}.
     \cite{yang2016designing} prunes connections based on weights magnitude. 
     \cite{han2015deep} could accelerate fully connected layers up to \x{50}.
    However, in practice, the actual speed-up may be very related to implementation.

    \Tensordecom~\cite{jaderberg2014speeding,lebedev2014speeding,gong2014compressing,kim2015compression,he2019depth} decompose weights into several pieces.
    \cite{xue2013restructuring,denton2014exploiting,girshick2015fast} accelerate fully connected layers with truncated SVD.
    \cite{Zhang2015} factorize a layer into $3\times3$ and $1\times1$ combination, driven by feature map redundancy.

    \Channelpruning\ removes redundant channels on feature maps.
    There are several training-based approaches \cite{zhang2020image}. 
    \cite{Alvarez2016,wen2016learning,zhou2016less} regularize networks to improve accuracy.
    Channel-wise SSL~\cite{wen2016learning} reaches high compression ratio for first few conv layers of LeNet~\cite{lecun1998gradient} and AlexNet~\cite{krizhevsky2012imagenet}.
    \cite{zhou2016less} could work well for fully connected layers.
    However,
    \textit{training-based} approaches are more costly,
    and their effectiveness on very deep networks on large datasets is rarely exploited.
    Inference-time \channelpruning\ is challenging, as reported by previous works~\cite{anwar2015structured,polyak2015channel}. Recently, AMC~\cite{He_2018_ECCV} improves our approach by learning speed-up ratio with reinforcement learning. 

    Some works~\cite{srinivas2015data,mariet2015diversity,hu2016network} focus on model size compression, which mainly operate the fully connected layers.
    Data-free approaches~\cite{Li2016,anwar2016compact} results for \ratio\ (\eg, \x{5}) have not been reported,
    and requires long retraining procedure.
     \cite{anwar2016compact} select channels via over 100 random trials. However, it needs a long time to evaluate each trial on a deep network,
    which makes it infeasible to work on very deep models and large datasets.
    \cite{Li2016} is even worse than naive solution from our observation sometimes (Sec. \ref{sec:ablation}).
        
\end{document}