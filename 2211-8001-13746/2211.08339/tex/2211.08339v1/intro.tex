\documentclass[egpaper_final]{subfiles}
\begin{document}
    \IEEEPARstart{R}{ecently}, convolutional neural networks (CNNs) are widely used on embed systems like smartphones and self-driving cars. 
    The general trend since the past few years has been that the networks have grown deeper, 
    with an overall increase in the number of parameters and convolution operations. 
    \textit{Efficient inference} is becoming more and more crucial for CNNs~\cite{szegedy2015rethinking}.
   CNN acceleration works fall into three categories: \implementation\ (\eg, FFT~\cite{vasilache2014fast}), 
    quantization (\eg, BinaryNet~\cite{courbariaux2016binarynet}), 
    and \structured\ that convert a CNN into compact one~\cite{jaderberg2014speeding}.
    This work focuses on the last one since it directly deals with the over-parameterization of CNNs.
    
    \Structured\ mainly involves:
    \tensordecom~\cite{jaderberg2014speeding},
    \sparseconnect~\cite{han2015learning},
    and \channelpruning~\cite{wen2016learning}.
    \Tensordecom\ factorizes a \conv\ into several efficient ones (Fig.~\ref{fig:struct}(c)).
    %which obtains within layer compact structure.
    However, \featch\ (number of channels) could not be reduced,
    which makes it difficult to decompose $1\times1$ \conv s favored by modern networks (\eg, GoogleNet~\cite{szegedy2015going}, ResNet~\cite{He2015},  Xception~\cite{chollet2016xception}). This type of method also introduces extra computation overhead.
    \Sparseconnect\ deactivates connections between neurons or channels (Fig.~\ref{fig:struct}(b)).
    Though it is able to achieve high theoretical \ratio,
    the sparse \conv s have an "irregular" shape which is not implementation-friendly.
    %T
    In contrast, \channelpruning\ directly reduces \featch, 
    which shrinks a network into thinner one, 
    as shown in Fig.~\ref{fig:struct}(d).
    It is efficient on both CPU and GPU because no special implementation is required.

        \begin{figure}
    \begin{center}
        \includegraphics[width=\linewidth]{structure}
    \end{center}
    \caption{\Structured\ methods that accelerate CNNs: (a) a network with 3 conv layers. (b) \sparseconnect\ deactivates some connections between channels. (c) \tensordecom\ factorizes a \conv\ into several pieces. (d) \channelpruning\ reduces number of channels in each layer (focus of this paper).}
    \label{fig:struct}
    \end{figure}
    
   Channel pruning is simple but challenging because removing channels in one layer might dramatically change the input of the following layer. Recently, \textit{training-based} \channelpruning\ works~\cite{Alvarez2016,wen2016learning} have focused on imposing the sparse constraint on weights during training, which could adaptively determine hyper-parameters.
    However, training from scratch is very costly, and results for very deep CNNs on ImageNet have rarely been reported.
    \textit{Inference-time} attempts~\cite{Li2016,anwar2016compact} have focused on analysis of the importance of individual weight. The reported \ratio\ is very limited. 
    
        This work is initially inspired by Net2widerNet~\cite{Chen2015}, which could easily explore new wider networks specification of the same architecture.
    It makes a convolutional layer wider by creating several copies of itself, calling each copy so that the output feature map is unchanged.
    It's nature to ask: 
    \textit{Is it possible to convert a net to thinner net of the same architecture without losing accuracy?}
    If we regard each feature map as a vector, then all feature maps form a vector space.
    The inverse operation of Net2widerNet discussed above is to find a set of base feature vectors and use them to represent other feature vectors, in order to reconstruct the original feature map space. 

    In this paper, we propose a new inference-time approach for \channelpruning, 
    utilizing inter-channel redundancy.
    Inspired by \tensordecom\ improvement by feature maps reconstruction~\cite{Zhang2015}, 
    instead of pruning according to filter weights magnitude~\cite{Li2016,luo2017thinet}, 
    we fully exploit redundancy inter feature maps. 
    Instead of recovering performance with finetuning~\cite{han2015learning,Li2016,luo2017thinet}, we propose to reconstruct output after pruning each layer.
    Specifically, given a trained CNN model, pruning each layer is achieved by minimizing reconstruction error on its output feature maps, 
    as shown in Fig.~\ref{fig:ill}.
        \begin{figure*}
        \begin{center}
        \includegraphics[width=0.7\linewidth]{ill}        
        \end{center}
        \caption{\Channelpruning\ for accelerating a \conv. 
        We aim to reduce the number of channels of feature map and minimize the reconstruction error on feature map C.
        Our optimization algorithm (Sec.~\ref{sec:linear}) performs within the dotted box, which does not involve nonlinearity.
        This figure illustrates the situation that two channels are pruned for feature map B. % (the two missing slices in B).
        Thus corresponding channels of filters $\wtwo$ can be removed.
        Furthermore, even though not directly optimized by our algorithm, 
        the corresponding filters in the previous layer can also be removed
        (marked by dotted filters).
         $c,n$: number of channels for feature maps B and C, $k_h\times k_w$: kernel size. %, $h\times w$: feature maps size
        }
        \label{fig:ill}
    \end{figure*}
    We solve this minimization problem by two alternative steps: channels selection and feature map reconstruction.
    In one step, we figure out the most representative channels,
    and prune redundant ones, based on \las.
    In the other step, we reconstruct the outputs with remaining channels with linear least squares.
    We alternatively take two steps. Further, we approximate the network layer-by-layer, with accumulated error accounted.
    We also discuss methodologies to prune multi-branch networks (\eg, ResNet~\cite{He2015}, Xception~\cite{chollet2016xception}).
        
    We demonstrate the superior accuracy of our approach over other recent pruning techniques~\cite{lin2017runtime,luo2017thinet,wang2017structured,liu2017learning}. For VGG-16, we achieve \x{4} acceleration, with only \textbf{\vggfour\%} increase of top-5 error.
    Combined with \tensordecom, we reach \x{5} acceleration but merely suffer \textbf{\vggcft\%} increase of error, 
    which outperforms previous state-of-the-arts.
    We further speed up ResNet-50 and Xception-50 by \x{2} with only \textbf{\resft\%, \xceptionft\%} accuracy loss respectively. Code has been made publicly available \footnote{https://github.com/yihui-he/channel-pruning}.
    
    A preliminary version of this manuscript has been accepted to a conference~\cite{He_2017_ICCV}. This manuscript extends the initial version from several aspects to strengthen our approach:
    \begin{enumerate}
    \item We investigated inter-channel redundancy in each layer, and better analysis it for pruning. 
    \item We present filter-wise pruning, which has compelling acceleration performance for a single layer. 
    \item We demonstrated compelling VGG-16 acceleration Top-1 results which outperform its original model.
    \end{enumerate}

\end{document}