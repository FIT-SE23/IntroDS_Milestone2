\documentclass[10pt,journal,compsoc]{IEEEtran}
% Some very useful LaTeX packages include:

\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}

% Include other packages here, before hyperref.
%\usepackage{lipsum}
%\usepackage{tikz}
\usepackage{multirow}
\usepackage{textcomp}
\usepackage{subfiles}
\usepackage{adjustbox}
\usepackage{siunitx}
\usepackage[inline, shortlabels]{enumitem}
\usepackage{xspace}
\usepackage[hidelinks]{hyperref}

\newcommand*{\eg}{e.g.\@\xspace}
\newcommand*{\ie}{i.e.\@\xspace}
\newcommand{\etal}{\textit{et al}.}
\makeatletter
\newcommand*{\etc}{%
    \@ifnextchar{.}%
        {etc}%
        {etc.\@\xspace}%
}
\makeatother

\DeclareMathOperator*{\argmin}{arg\,min}
\newcommand{\ma}[1]{\mathrm{#1}}
\newcommand{\ve}[1]{\mathbf{#1}}
\newcommand{\norm}[1]{\left\lVert#1\right\rVert}
\newcommand{\st}{\text{subject to}}
\newcommand{\eval}[1]{\pgfmathparse{#1}\pgfmathresult}
\newcommand{\feval}[1]{\pgfmathparse{#1}\pgfmathprintnumberto[precision=1]{\pgfmathresult}{\roundednumber}\roundednumber}
\newcommand{\round}[2]{\num[round-mode=places,round-precision=#1]{#2}}
\newcommand{\stitle}[1]{\noindent\textbf{#1}\\}
%\pgfmathresult
% specific
\newcommand{\lcoef}{\boldsymbol{\beta}}
\newcommand{\lcoefi}{\beta_i}
\newcommand{\lcoefx}[1]{\beta_#1}
\newcommand{\wtwo}{\ma{W}}
\newcommand{\wtwoi}{\ma{W_i}}
\newcommand{\wtwox}[1]{\ma{W_#1}}
\newcommand{\reW}{\ma{W^\prime}}
\newcommand{\reY}{\ma{Y}}
\newcommand{\reX}{\ma{X}}
\newcommand{\reXi}{\ma{X_i}}
\newcommand{\reXx}[1]{\ma{X_#1}}
\newcommand{\lalpha}{\lambda}
\newcommand{\rank}{c}
\newcommand{\out}{n}
\newcommand{\kh}{k_h}
\newcommand{\kw}{k_w}
\newcommand{\samp}{N}
\newcommand{\rankp}{c^\prime}
\newcommand{\wx}{\ma{Z}}
\newcommand{\wxi}{\ma{Z_i}}
\newcommand{\cx}{\ma{X^\prime}}

\newcommand{\relu}[1]{r(#1)}
\newcommand{\x}[1]{$#1\times$}

\newcommand{\las}{LASSO regression}
\newcommand{\conv}{convolutional layer}
\newcommand{\featch}{feature map width}
\newcommand{\patch}{channel patch sample}
\newcommand{\ratio}{speed-up ratio}
\newcommand{\incerr}{increase of error}

\newcommand{\rand}{\textit{rand}}
\newcommand{\firstk}{\textit{first k}}
\newcommand{\prune}{\textit{max response}}
\newcommand{\onestep}{\textit{direct solution}}
\newcommand{\alter}{\textit{alternative solution}}
\newcommand{\Alter}{\textit{Alternative solver}}
\newcommand{\nonlinear}{\textit{nonlinear solver}}
\newcommand{\filterwise}{\textit{filter-wise pruning}}
\newcommand{\Filterwise}{\textit{Filter-wise pruning}}
\newcommand{\FilterWise}{\textit{Filter-wise Pruning}}
\newcommand{\multibranch}{\textit{multi-branch goal}}
\newcommand{\Multibranch}{\textit{Multi-branch goal}}
\newcommand{\sampling}{\textit{feature map sampling}}
\newcommand{\Sampling}{\textit{Feature map sampling}}

\newcommand{\filterpruning}{Filter pruning}
\newcommand{\tensordecom}{tensor factorization}
\newcommand{\Tensordecom}{Tensor factorization}
\newcommand{\TensorDecom}{Tensor Factorization}
\newcommand{\channelpruning}{channel pruning}
\newcommand{\Channelpruning}{Channel pruning}
\newcommand{\ChannelPruning}{Channel Pruning}
\newcommand{\structured}{structured simplification}
\newcommand{\Structured}{Structured simplification}
\newcommand{\implementation}{optimized implementation}
\newcommand{\Implementation}{Optimized implementation}
\newcommand{\sparseconnect}{sparse connection}
\newcommand{\Sparseconnect}{Sparse connection}

\newcommand{\RNP}{RNP~\cite{lin2017runtime}}
\newcommand{\thinet}{ThiNet~\cite{luo2017thinet}}
\newcommand{\spp}{SPP~\cite{wang2017structured}}
\newcommand{\slimming}{Slimming~\cite{liu2017learning}}
    \newcommand{\tworow}[1]{\begin{tabular}[c]{@{}c@{}}#1\end{tabular}}
    \newcommand{\jadercite}{Jaderberg \etal~\cite{jaderberg2014speeding}}
    \newcommand{\jader}{Jaderberg \etal~\cite{jaderberg2014speeding} (\cite{Zhang2015}'s impl.)}
    \newcommand{\asym}{Asym.~\cite{Zhang2015}}
    \newcommand{\asymd}{Asym. 3D~\cite{Zhang2015}}
    \newcommand{\asymdft}{Asym. 3D (fine-tuned)~\cite{Zhang2015}}
    \newcommand{\prli}{\filterpruning~\cite{Li2016}}
    \newcommand{\pr}{\filterpruning~\cite{Li2016} (our impl.)}
    \newcommand{\prft}{\filterpruning~\cite{Li2016} (fine-tuned, our impl.)}
    \newcommand{\prfttwo}{\tworow{\filterpruning~\cite{Li2016}\\ (fine-tuned, our impl.)}}
    \newcommand{\sgd}{\textit{SGD}}
    

\newcommand{\origvgg}{89.9}
\newcommand{\vggtworaw}{2.7}
\newcommand{\vggfour}{1.0}
\newcommand{\vggfouracc}{11.1} %88.9
\newcommand{\vggfourraw}{7.9}
\newcommand{\vggfourrawacc}{18.0} %82.0
\newcommand{\vggfive}{1.7}
\newcommand{\vggfiveraw}{22.0}
\newcommand{\vggscratchacc}{11.9} % 88.1
\newcommand{\vggscratcherr}{1.8}
\newcommand{\vggscratchuniacc}{12.5}%87.5
\newcommand{\vggscratchunierr}{2.4}

\newcommand{\vggthreethree}{0.8}
\newcommand{\vggc}{1.3}
\newcommand{\vggcft}{0.3}% 0.700
\newcommand{\vggcfour}{0.7} %0.6961
\newcommand{\vggcftfour}{0.0}% 0.700

\newcommand{\resorig}{92.2} %.74542
\newcommand{\resmb}{4.0} %3.0
\newcommand{\resfw}{7.5}
\newcommand{\resft}{1.4}
\newcommand{\resraw}{8.0}
\newcommand{\restwo}{87}

\newcommand{\xceptionfifty}{Xception-50}
\newcommand{\xceptionpr}{4.3}
\newcommand{\xceptionorig}{92.8}
\newcommand{\xceptioncr}{2.9}
\newcommand{\xceptionft}{1.0}

\ifCLASSOPTIONcompsoc
  % 
  \usepackage[nocompress]{cite}
\else
  % normal IEEE
  \usepackage{cite}
\fi

\ifCLASSINFOpdf
  % 
\else
  % 
\fi

\newcommand\MYhyperrefoptions{bookmarks=true,bookmarksnumbered=true,
pdfpagemode={UseOutlines},plainpages=false,pdfpagelabels=true,
colorlinks=true,linkcolor={black},citecolor={black},urlcolor={black},
pdftitle={Pruning Very Deep Neural Network Channels for Efficient Inference},%<!CHANGE!
pdfsubject={Pruning Very Deep Neural Network Channels for Efficient Inference},%<!CHANGE!
pdfauthor={Yihui He},%<!CHANGE!
pdfkeywords={Convolutional neural networks, acceleration, image classification}}%<^!CHANGE!

% correct bad hyphenation here
\hyphenation{op-tical net-works}


\begin{document}

\title{Pruning Very Deep Neural Network Channels for Efficient Inference}

\author{Yihui~He
\IEEEcompsocitemizethanks{\IEEEcompsocthanksitem  \href{https://yihui.dev/channel-pruning-for-accelerating-very-deep-neural-networks}{yihui.dev/channel-pruning-for-accelerating-very-deep-neural-networks}
}% <-this % stops a space
%\thanks{Manuscript received April 19, 2005; revised August 26, 2015.}
}

\markboth{IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE}%,~Vol.~14, No.~8, August~2015
{He \MakeLowercase{\textit{et al.}}: Pruning Very Deep Neural Network Channels for Efficient Inference}

\IEEEtitleabstractindextext{%
\begin{abstract}
    In this paper, we introduce a new channel pruning method to accelerate very deep convolutional neural networks. Given a trained CNN model, we propose an iterative two-step algorithm to effectively prune each layer, by a \las\ based channel selection and least square reconstruction. We further generalize this algorithm to multi-layer and multi-branch cases. Our method reduces the accumulated error and enhances the compatibility with various architectures. Our pruned VGG-16 achieves the state-of-the-art results by \x{5} speed-up along with only 0.3\% increase of error. More importantly, our method is able to accelerate modern networks like ResNet, Xception and suffers only \resft\%, \xceptionft\% accuracy loss under \x{2} speed-up respectively, which is significant. Our code has been made publicly available.
\end{abstract}

% Note that keywords are not normally used for peerreview papers.
\begin{IEEEkeywords}
Convolutional neural networks, acceleration, image classification.
\end{IEEEkeywords}}


% make the title area
\maketitle

\IEEEdisplaynontitleabstractindextext

\IEEEpeerreviewmaketitle


\ifCLASSOPTIONcompsoc
\IEEEraisesectionheading{\section{Introduction}\label{sec:introduction}}
\else
\section{Introduction}
\label{sec:introduction}
\fi

\subfile{intro}

\section{Related Work}
\subfile{relate}

\section{Approach}
\subfile{approach}

\section{Experiment}
\subfile{exp}

\section{Conclusion}
To conclude, current deep CNNs are accurate with high inference costs.
In this paper, we have presented an inference-time \channelpruning\ method for very deep networks.
The reduced CNNs are inference efficient networks while maintaining accuracy,
and only require off-the-shelf libraries.
Compelling speed-ups and accuracy are demonstrated for both VGG Net and ResNet-like networks
on ImageNet, CIFAR-10 and PASCAL VOC 2007.

In the future, we plan to involve our approaches to training time to accelerate training procedure, instead of inference time only.

\ifCLASSOPTIONcaptionsoff
  \newpage
\fi


\bibliographystyle{IEEEtran}
% argument is your BibTeX string definitions and bibliography database(s)
\bibliography{egbib}

\vfill

\end{document}