% Generated by Paperpile. Check out https://paperpile.com for more information.
% BibTeX export options can be customized via Settings -> BibTeX.

@ARTICLE{Bengio2015-ti,
  title         = "Towards Biologically Plausible Deep Learning",
  author        = "Bengio, Yoshua and Lee, Dong-Hyun and Bornschein, Jorg and
                   Mesnard, Thomas and Lin, Zhouhan",
  abstract      = "Neuroscientists have long criticised deep learning
                   algorithms as incompatible with current knowledge of
                   neurobiology. We explore more biologically plausible
                   versions of deep representation learning, focusing here
                   mostly on unsupervised learning but developing a learning
                   mechanism that could account for supervised, unsupervised
                   and reinforcement learning. The starting point is that the
                   basic learning rule believed to govern synaptic weight
                   updates (Spike-Timing-Dependent Plasticity) arises out of a
                   simple update rule that makes a lot of sense from a machine
                   learning point of view and can be interpreted as gradient
                   descent on some objective function so long as the neuronal
                   dynamics push firing rates towards better values of the
                   objective function (be it supervised, unsupervised, or
                   reward-driven). The second main idea is that this
                   corresponds to a form of the variational EM algorithm, i.e.,
                   with approximate rather than exact posteriors, implemented
                   by neural dynamics. Another contribution of this paper is
                   that the gradients required for updating the hidden states
                   in the above variational interpretation can be estimated
                   using an approximation that only requires propagating
                   activations forward and backward, with pairs of layers
                   learning to form a denoising auto-encoder. Finally, we
                   extend the theory about the probabilistic interpretation of
                   auto-encoders to justify improved sampling schemes based on
                   the generative interpretation of denoising auto-encoders,
                   and we validate all these ideas on generative learning
                   tasks.",
  month         =  feb,
  year          =  2015,
  archivePrefix = "arXiv",
  primaryClass  = "cs.LG",
  eprint        = "1502.04156"
}

@ARTICLE{Sariyanidi2015-mh,
  title    = "Automatic Analysis of Facial Affect: A Survey of Registration,
              Representation, and Recognition",
  author   = "Sariyanidi, Evangelos and Gunes, Hatice and Cavallaro, Andrea",
  abstract = "Automatic affect analysis has attracted great interest in various
              contexts including the recognition of action units and basic or
              non-basic emotions. In spite of major efforts, there are several
              open questions on what the important cues to interpret facial
              expressions are and how to encode them. In this paper, we review
              the progress across a range of affect recognition applications to
              shed light on these fundamental questions. We analyse the
              state-of-the-art solutions by decomposing their pipelines into
              fundamental components, namely face registration, representation,
              dimensionality reduction and recognition. We discuss the role of
              these components and highlight the models and new trends that are
              followed in their design. Moreover, we provide a comprehensive
              analysis of facial representations by uncovering their advantages
              and limitations; we elaborate on the type of information they
              encode and discuss how they deal with the key challenges of
              illumination variations, registration errors, head-pose
              variations, occlusions, and identity bias. This survey allows us
              to identify open issues and to define future directions for
              designing real-world affect recognition systems.",
  journal  = "IEEE Trans. Pattern Anal. Mach. Intell.",
  volume   =  37,
  number   =  6,
  pages    = "1113--1133",
  month    =  jun,
  year     =  2015,
  language = "en"
}

@INPROCEEDINGS{Shu2018-ov,
  title     = "Deforming autoencoders: Unsupervised disentangling of shape and
               appearance",
  booktitle = "Proceedings of the European conference on computer vision
               ({ECCV})",
  author    = "Shu, Zhixin and Sahasrabudhe, Mihir and Guler, Riza Alp and
               Samaras, Dimitris and Paragios, Nikos and Kokkinos, Iasonas",
  pages     = "650--665",
  year      =  2018
}

@INPROCEEDINGS{Vemulapalli2019-rv,
  title     = "A compact embedding for facial expression similarity",
  booktitle = "Proceedings of the {IEEE/CVF} Conference on Computer Vision and
               Pattern Recognition",
  author    = "Vemulapalli, Raviteja and Agarwala, Aseem",
  pages     = "5683--5692",
  year      =  2019
}

@ARTICLE{Mckone1294-ij,
  title     = "The cognitive and neural development of face recognition in
               humans",
  author    = "Mckone, Elinor and Crookes, Kate and Kanwisher, Nancy",
  abstract  = "Conventional wisdom has long held that face recognition develops
               very slowly throughout infancy, childhood, and adolescence, with
               perceptual experience as the primary engine of this development.
               However, striking new findings from just the last few years have
               overturned much of this traditional view by demonstrating
               genetic influences on the face recognition system as well as
               impressive face discrimination abilities that are present in
               newborns and in monkeys that were reared without ever seeing a
               face. Nevertheless, experience does play a role, for example, in
               narrowing the range of facial subtypes for which discrimination
               is possible and perhaps in increasing discrimination abilities
               within that range. Here we first describe the cognitive and
               neural characteristics of the adult system for face recognition,
               and then we chart the development of this system over infancy
               and childhood. This review identifies a fascinating new puzzle
               to be targeted in future research: All qualitative aspects of
               adult face recognition measured behaviorally are present very
               early in development (by 4 years of age; all that have been
               tested are also present in infancy), yet functional magnetic
               resonance imaging and event-related potential evidence shows
               very late maturity of face-selective neural responses (with the
               fusiform face area increasing substantially in volume between
               age 7 years and adulthood). (PsycINFO Database Record (c) 2019
               APA, all rights reserved)",
  journal   = "The cognitive neurosciences., 4th ed.",
  publisher = "Massachusetts Institute of Technology, xvii",
  volume    =  4,
  number    =  2009,
  pages     = "467--482",
  year      =  2009,
  address   = "Cambridge, MA, US"
}

@ARTICLE{Dailey1999-ac,
  title    = "Organization of face and object recognition in modular neural
              network models",
  author   = "Dailey, M N and Cottrell, G W",
  abstract = "There is strong evidence that face processing in the brain is
              localized. The double dissociation between prosopagnosia, a face
              recognition deficit occurring after brain damage, and visual
              object agnosia, difficulty recognizing other kinds of complex
              objects, indicates that face and non-face object recognition may
              be served by partially independent neural mechanisms. In this
              paper, we use computational models to show how the face
              processing specialization apparently underlying prosopagnosia and
              visual object agnosia could be attributed to (1) a relatively
              simple competitive selection mechanism that, during development,
              devotes neural resources to the tasks they are best at
              performing, (2) the developing infant's need to perform
              subordinate classification (identification) of faces early on,
              and (3) the infant's low visual acuity at birth. Inspired by de
              Schonen, Mancini and Liegeois' arguments (1998) [de Schonen, S.,
              Mancini, J., Liegeois, F. (1998). About functional cortical
              specialization: the development of face recognition. In: F. Simon
              \& G. Butterworth, The development of sensory, motor, and
              cognitive capacities in early infancy (pp. 103-116). Hove, UK:
              Psychology Press] that factors like these could bias the visual
              system to develop a processing subsystem particularly useful for
              face recognition, and Jacobs and Kosslyn's experiments (1994)
              [Jacobs, R. A., \& Kosslyn, S. M. (1994). Encoding shape and
              spatial relations-the role of receptive field size in
              coordination complementary representations. Cognitive Science,
              18(3), 361-368] in the mixtures of experts (ME) modeling
              paradigm, we provide a preliminary computational demonstration of
              how this theory accounts for the double dissociation between face
              and object processing. We present two feed-forward computational
              models of visual processing. In both models, the selection
              mechanism is a gating network that mediates a competition between
              modules attempting to classify input stimuli. In Model I, when
              the modules are simple unbiased classifiers, the competition is
              sufficient to achieve enough of a specialization that damaging
              one module impairs the model's face recognition more than its
              object recognition, and damaging the other module impairs the
              model's object recognition more than its face recognition.
              However, the model is not completely satisfactory because it
              requires a search of parameter space. With Model II, we explore
              biases that lead to more consistent specialization. We bias the
              modules by providing one with low spatial frequency information
              and the other with high spatial frequency information. In this
              case, when the model's task is subordinate classification of
              faces and superordinate classification of objects, the low
              spatial frequency network shows an even stronger specialization
              for faces. No other combination of tasks and inputs shows this
              strong specialization. We take these results as support for the
              idea that something resembling a face processing ``module'' could
              arise as a natural consequence of the infant's developmental
              environment without being innately specified.",
  journal  = "Neural Netw.",
  volume   =  12,
  number   = "7-8",
  pages    = "1053--1074",
  month    =  oct,
  year     =  1999,
  language = "en"
}

@ARTICLE{Demenescu2010-pc,
  title    = "Impaired attribution of emotion to facial expressions in anxiety
              and major depression",
  author   = "Demenescu, Liliana R and Kortekaas, Rudie and den Boer, Johan A
              and Aleman, Andr{\'e}",
  abstract = "BACKGROUND: Recognition of others' emotions is an important
              aspect of interpersonal communication. In major depression, a
              significant emotion recognition impairment has been reported. It
              remains unclear whether the ability to recognize emotion from
              facial expressions is also impaired in anxiety disorders. There
              is a need to review and integrate the published literature on
              emotional expression recognition in anxiety disorders and major
              depression. METHODOLOGY/PRINCIPAL FINDINGS: A detailed literature
              search was used to identify studies on explicit emotion
              recognition in patients with anxiety disorders and major
              depression compared to healthy participants. Eighteen studies
              provided sufficient information to be included. The differences
              on emotion recognition impairment between patients and controls
              (Cohen's d) with corresponding confidence intervals were computed
              for each study. Over all studies, adults with anxiety disorders
              had a significant impairment in emotion recognition (d = -0.35).
              In children with anxiety disorders no significant impairment of
              emotion recognition was found (d = -0.03). Major depression was
              associated with an even larger impairment in recognition of
              facial expressions of emotion (d = -0.58).
              CONCLUSIONS/SIGNIFICANCE: Results from the current analysis
              support the hypothesis that adults with anxiety disorders or
              major depression both have a deficit in recognizing facial
              expression of emotions, and that this deficit is more pronounced
              in major depression than in anxiety.",
  journal  = "PLoS One",
  volume   =  5,
  number   =  12,
  pages    = "e15058",
  month    =  dec,
  year     =  2010,
  language = "en"
}

@INPROCEEDINGS{Fabian_Benitez-Quiroz2016-hn,
  title     = "Emotionet: An accurate, real-time algorithm for the automatic
               annotation of a million facial expressions in the wild",
  booktitle = "Proceedings of the {IEEE} conference on computer vision and
               pattern recognition",
  author    = "Fabian Benitez-Quiroz, C and Srinivasan, Ramprakash and
               Martinez, Aleix M",
  pages     = "5562--5570",
  year      =  2016
}

@INPROCEEDINGS{Lin2017-xh,
  title     = "Hybrid neural networks for learning the trend in time series",
  booktitle = "Proceedings of the twenty-sixth international joint conference
               on artificial intelligence",
  author    = "Lin, Tao and Guo, Tian and Aberer, Karl",
  pages     = "2273--2279",
  year      =  2017
}

@ARTICLE{Khaireddin2021-lw,
  title         = "Facial Emotion Recognition: State of the Art Performance on
                   {FER2013}",
  author        = "Khaireddin, Yousif and Chen, Zhuofa",
  abstract      = "Facial emotion recognition (FER) is significant for
                   human-computer interaction such as clinical practice and
                   behavioral description. Accurate and robust FER by computer
                   models remains challenging due to the heterogeneity of human
                   faces and variations in images such as different facial pose
                   and lighting. Among all techniques for FER, deep learning
                   models, especially Convolutional Neural Networks (CNNs) have
                   shown great potential due to their powerful automatic
                   feature extraction and computational efficiency. In this
                   work, we achieve the highest single-network classification
                   accuracy on the FER2013 dataset. We adopt the VGGNet
                   architecture, rigorously fine-tune its hyperparameters, and
                   experiment with various optimization methods. To our best
                   knowledge, our model achieves state-of-the-art
                   single-network accuracy of 73.28 \% on FER2013 without using
                   extra training data.",
  month         =  may,
  year          =  2021,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CV",
  eprint        = "2105.03588"
}

@ARTICLE{Yamins2016-hk,
  title    = "Using goal-driven deep learning models to understand sensory
              cortex",
  author   = "Yamins, Daniel L K and DiCarlo, James J",
  abstract = "Fueled by innovation in the computer vision and artificial
              intelligence communities, recent developments in computational
              neuroscience have used goal-driven hierarchical convolutional
              neural networks (HCNNs) to make strides in modeling neural
              single-unit and population responses in higher visual cortical
              areas. In this Perspective, we review the recent progress in a
              broader modeling context and describe some of the key technical
              innovations that have supported it. We then outline how the
              goal-driven HCNN approach can be used to delve even more deeply
              into understanding the development and organization of sensory
              cortical processing.",
  journal  = "Nat. Neurosci.",
  volume   =  19,
  number   =  3,
  pages    = "356--365",
  month    =  mar,
  year     =  2016,
  language = "en"
}

@BOOK{Darwin2015-bl,
  title     = "The Expression of the Emotions in Man and Animals",
  author    = "Darwin, Charles",
  abstract  = "Darwin's work of 1872 still provides the point of departure for
               research in the theory of emotion and expression. Although he
               lacked the modern research tool of cybernetics, his basic
               methods have not been improved upon: the study of infants, of
               the insane, of paintings and sculpture, of some of the commoner
               animals; the use of photographs of expression submitted to
               different judges; and the comparative study of expression among
               different peoples. This new edition will be warmly welcomed by
               those behavioral scientists who have recently shown an intense
               interest in the scientific study of expression. Lay readers,
               too, will be struck by the freshness and directness of this
               book, which includes, among other data, Darwin's delightfully
               objective analysis of his own baby's smiles and pouts.",
  year      =  1872,
  keywords  = "emotion; cognition; cybernetics; expression; darwin; infants;
               insane; madness; paintings; sculpture; art; animals;
               photography; instinct; nonfiction; psychology; sociology; human
               nature; biology; zoology; suffering; weeping; anxiety; anger;
               grief; dejection; despair; joy; love; devotion; happiness;
               mental health; reflection; meditation; determination; hatred;
               negative; positive; blushing; shame; shyness; modesty; surprise;
               astonishment; horror; fear; disdain; contempt; disgust; guilt;
               pride; helplessness; patience; affirmation; negation",
  language  = "en"
}

@INPROCEEDINGS{Sangineto2014-zy,
  title     = "We are not All Equal: Personalizing Models for Facial Expression
               Analysis with Transductive Parameter Transfer",
  booktitle = "Proceedings of the 22nd {ACM} international conference on
               Multimedia",
  author    = "Sangineto, Enver and Zen, Gloria and Ricci, Elisa and Sebe, Nicu",
  abstract  = "Previous works on facial expression analysis have shown that
               person specific models are advantageous with respect to generic
               ones for recognizing facial expressions of new users added to
               the gallery set. This finding is not surprising, due to the
               often significant inter-individual variability: different
               persons have different morphological aspects and express their
               emotions in different ways. However, acquiring person-specific
               labeled data for learning models is a very time consuming
               process. In this work we propose a new transfer learning method
               to compute personalized models without labeled target data Our
               approach is based on learning multiple person-specific
               classifiers for a set of source subjects and then directly
               transfer knowledge about the parameters of these classifiers to
               the target individual. The transfer process is obtained by
               learning a regression function which maps the data distribution
               associated to each source subject to the corresponding
               classifier's parameters. We tested our approach on two different
               application domains, Action Units (AUs) detection and
               spontaneous pain recognition, using publicly available datasets
               and showing its advantages with respect to the state-of-the-art
               both in term of accuracy and computational cost.",
  publisher = "Association for Computing Machinery",
  pages     = "357--366",
  series    = "MM '14",
  month     =  nov,
  year      =  2014,
  address   = "New York, NY, USA",
  keywords  = "transductive transfer learning, learning from distributions,
               facial expression recognition, action unit detection",
  location  = "Orlando, Florida, USA"
}

@INPROCEEDINGS{Chang2021-co,
  title     = "Learning Facial Representations from the Cycle-consistency of
               Face",
  booktitle = "Proceedings of the {IEEE/CVF} International Conference on
               Computer Vision",
  author    = "Chang, Jia-Ren and Chen, Yong-Sheng and Chiu, Wei-Chen",
  pages     = "9680--9689",
  year      =  2021
}

@ARTICLE{Brecht2012-bs,
  title    = "The many facets of facial interactions in mammals",
  author   = "Brecht, Michael and Freiwald, Winrich A",
  abstract = "Facial interactions are prominent behaviors in primates. Primate
              facial signaling, which includes the expression of emotions,
              mimicking of facial movements, and gaze interactions, is visually
              dominated. Correspondingly, in primate brains an elaborate
              network of face processing areas exists within visual cortex. But
              other mammals also communicate through facial interactions using
              additional sensory modalities. In rodents, multisensory facial
              interactions are involved in aggressive behaviors and social
              transmission of food preferences. The eusocial naked mole-rat,
              whose face is dominated by prominent incisors, uses facial
              aggression to enforce reproductive suppression. In burrow-living
              mammals like the naked mole-rat in particular, and in rodents in
              general, somatosensory face representations in cortex are
              enlarged. Diversity of sensory domains mediating facial
              communication might belie underlying common mechanisms. As a case
              in point, neurogenetics has revealed strongly heritable traits in
              face processing and identified gene defects that disrupt facial
              interactions both in humans and rodents.",
  journal  = "Curr. Opin. Neurobiol.",
  volume   =  22,
  number   =  2,
  pages    = "259--266",
  month    =  apr,
  year     =  2012,
  language = "en"
}

@INPROCEEDINGS{Xing2019-fk,
  title     = "Unsupervised disentangling of appearance and geometry by
               deformable generator network",
  booktitle = "Proceedings of the {IEEE/CVF} Conference on Computer Vision and
               Pattern Recognition",
  author    = "Xing, Xianglei and Han, Tian and Gao, Ruiqi and Zhu, Song-Chun
               and Wu, Ying Nian",
  pages     = "10354--10363",
  year      =  2019
}

@ARTICLE{He_undated-nt,
  title   = "Deep residual learning for image recognition",
  author  = "{He} and {Zhang} and {Ren} and {Sun}",
  journal = "and pattern recognition"
}

@ARTICLE{Craig2011-xc,
  title     = "The facial expression of pain",
  author    = "Craig, Kenneth D and Prkachin, Kenneth M and Grunau, Ruth E",
  abstract  = "People in pain communicate their experience through a remarkable
               variety of actions, ranging from the use of language to diverse
               forms of nonverbal activity. Nonverbal expression adds context
               and meaning to verbal expression and tends to have greater
               credibility for observers because it is less subject to
               voluntary control. This chapter examines how facial expression
               allows clinicians, investigators, and others to formulate
               judgments about another person's pain, regardless of whether
               that person uses words to communicate. (PsycInfo Database Record
               (c) 2021 APA, all rights reserved)",
  journal   = "Handbook of pain assessment., 3rd ed.",
  publisher = "The Guilford Press, xvii",
  volume    =  3,
  pages     = "117--133",
  year      =  2011,
  address   = "New York, NY, US"
}

@ARTICLE{Bruyer1983-sx,
  title    = "A case of prosopagnosia with some preserved covert remembrance of
              familiar faces",
  author   = "Bruyer, R and Laterre, C and Seron, X and Feyereisen, P and
              Strypstein, E and Pierrard, E and Rectem, D",
  abstract = "This paper presents the detailed analysis of a case of
              prosopagnosia in a 54-year-old male farmer following bioccipital
              vascular disease. In-depth clinical investigations confirmed the
              diagnosis of prosopagnosia and revealed the absence of any
              associated defect, except for a slight aspecific disturbance of
              the short-term memory. Further study of this case indicated that
              the trouble was not concerned with the class of complex visual
              stimuli, was not even concerned with facial expressions or
              unknown faces, was not a perceptual defect, but was related
              mainly to the operation of individualization. The memory
              hypothesis was thus retained and supported. Moreover, exploration
              of the difficulty indicated that the deficiency was limited to
              defective access to conscious information concerning faces and
              information associated with these faces (name, context, etc.),
              effectively stored in memory.",
  journal  = "Brain Cogn.",
  volume   =  2,
  number   =  3,
  pages    = "257--284",
  month    =  jul,
  year     =  1983,
  language = "en"
}

@MISC{Zen2014-zp,
  title   = "Unsupervised Domain Adaptation for Personalized Facial Emotion
             Recognition",
  author  = "Zen, Gloria and Sangineto, Enver and Ricci, Elisa and Sebe, Nicu",
  journal = "Proceedings of the 16th International Conference on Multimodal
             Interaction",
  year    =  2014
}

@ARTICLE{Chai2021-yi,
  title    = "Deep learning in computer vision: A critical review of emerging
              techniques and application scenarios",
  author   = "Chai, Junyi and Zeng, Hao and Li, Anming and Ngai, Eric W T",
  abstract = "Deep learning has been overwhelmingly successful in computer
              vision (CV), natural language processing, and video/speech
              recognition. In this paper, our focus is on CV. We provide a
              critical review of recent achievements in terms of techniques and
              applications. We identify eight emerging techniques, investigate
              their origins and updates, and finally emphasize their
              applications in four key scenarios, including recognition, visual
              tracking, semantic segmentation, and image restoration. We
              recognize three development stages in the past decade and
              emphasize research trends for future works. The summarizations,
              knowledge accumulations, and creations could benefit researchers
              in the academia and participators in the CV industries.",
  journal  = "Machine Learning with Applications",
  volume   =  6,
  pages    = "100134",
  month    =  dec,
  year     =  2021,
  keywords = "Machine learning; Deep learning; Computer vision; Literature
              review"
}

@ARTICLE{Ekman_undated-ev,
  title    = "Facial Action Coding System",
  year = 1978,
  author   = "Ekman, Paul and Friesen, Wallace V",
  abstract = "0: T 1: h 2: e 3: 4: F 5: a 6: c 7: i 8: a 9: l 10: 11: A 12: c
              13: t 14: i 15: o 16: n 17: 18: C 19: o 20: d 21: i 22: n 23: g
              24: 25: S 26: y 27: s 28: t 29: e 30: m 31: s 32: 33: ( 34: F 35:
              A 36: C 37: S 38: ; 39: 40: E 41: k 42: m 43: a 44: n 45: 46: \&
              47: 48: F 49: r 50: i 51: e 52: s 53: e 54: n 55: , 56: 57: 1 58:
              9 59: 7 60: 8 61: ) 62: 63: w 64: a 65: s 66: 67: d 68: e 69: r
              70: i 71: v 72: e 73: d 74: 75: f 76: r 77: o 78: m 79: 80: a 81:
              n 82: 83: a 84: n 85: a 86: l 87: y 88: s 89: i 90: s 91: 92: o
              93: f 94: 95: t 96: h 97: e 98: 99: a 100: n 101: a 102: t 103: o
              104: m 105: i 106: c 107: a 108: l 109: 110: b 111: a 112: s 113:
              i 114: s 115: 116: o 117: f 118: 119: f 120: a 121: c 122: i 123:
              a 124: l 125: 126: m 127: o 128: v 129: e 130: m 131: e 132: n
              133: t 134: 135: a 136: n 137: d 138: 139: c 140: a 141: n 142:
              143: r 144: e 145: p 146: o 147: r 148: t 149: e 150: d 151: l
              152: y 153: 154: b 155: e 156: 157: u 158: s 159: e 160: d 161:
              162: t 163: o 164: 165: d 166: e 167: s 168: c 169: r 170: i 171:
              b 172: e 173: 174: a 175: n 176: y 177: 178: f 179: a 180: c 181:
              i 182: a 183: l 184: 185: m 186: o 187: v 188: e 189: m 190: e
              191: n 192: t 193: , 194: 195: o 196: b 197: s 198: e 199: r 200:
              v 201: e 202: d 203: 204: a 205: c 206: r 207: o 208: s 209: s
              210: 211: a 212: 213: n 214: u 215: m 216: b 217: e 218: r 219:
              220: o 221: f 222: 223: m 224: e 225: d 226: i 227: a 228: , 229:
              230: i 231: n 232: 233: t 234: e 235: r 236: m 237: s 238: 239: o
              240: f 241: 242: a 243: n 244: a 245: t 246: o 247: m 248: i 249:
              c 250: a 251: l 252: l 253: y 254: 255: b 256: a 257: s 258: e
              259: d 260: 261: a 262: c 263: t 264: i 265: o 266: n 267: 268: u
              269: n 270: i 271: t 272: s 273: . 274: 275: T 276: h 277: e 278:
              279: d 280: e 281: v 282: e 283: l 284: o 285: p 286: m 287: e
              288: n 289: t 290: 291: o 292: f 293: 294: t 295: h 296: e 297:
              298: m 299: e 300: t 301: h 302: o 303: d 304: 305: i 306: s 307:
              308: e 309: x 310: p 311: l 312: a 313: i 314: n 315: e 316: d
              317: , 318: 319: c 320: o 321: n 322: t 323: r 324: a 325: s 326:
              t 327: i 328: n 329: g 330: 331: i 332: t 333: 334: t 335: o 336:
              337: o 338: t 339: h 340: e 341: r 342: 343: m 344: e 345: t 346:
              h 347: o 348: d 349: s 350: 351: o 352: f 353: 354: m 355: e 356:
              a 357: s 358: u 359: r 360: i 361: n 362: g 363: 364: f 365: a
              366: c 367: i 368: a 369: l 370: 371: b 372: e 373: h 374: a 375:
              v 376: i 377: o 378: r 379: . 380: 381: A 382: n 383: 384: e 385:
              x 386: a 387: m 388: p 389: l 390: e 391: 392: o 393: f 394: 395:
              h 396: o 397: w 398: 399: f 400: a 401: c 402: i 403: a 404: l
              405: 406: b 407: e 408: h 409: a 410: v 411: i 412: o 413: r 414:
              415: i 416: s 417: 418: m 419: e 420: a 421: s 422: u 423: r 424:
              e 425: d 426: 427: i 428: s 429: 430: p 431: r 432: o 433: v 434:
              i 435: d 436: e 437: d 438: , 439: 440: a 441: n 442: d 443: 444:
              i 445: d 446: e 447: a 448: s 449: 450: a 451: b 452: o 453: u
              454: t 455: 456: r 457: e 458: s 459: e 460: a 461: r 462: c 463:
              h 464: 465: a 466: p 467: p 468: l 469: i 470: c 471: a 472: t
              473: i 474: o 475: n 476: s 477: 478: a 479: r 480: e 481: 482: d
              483: i 484: s 485: c 486: u 487: s 488: s 489: e 490: d 491: .
              492: 493: F 494: A 495: C 496: 497: i 498: n 499: c 500: l 501: u
              502: d 503: e 504: s 505: 506: m 507: o 508: s 509: t 510: 511: b
              512: u 513: t 514: 515: n 516: o 517: t 518: 519: a 520: l 521: l
              522: 523: o 524: f 525: 526: t 527: h 528: e 529: 530: s 531: u
              532: b 533: t 534: l 535: e 536: 537: d 538: i 539: f 540: f 541:
              e 542: r 543: e 544: n 545: c 546: e 547: s 548: 549: i 550: n
              551: 552: a 553: p 554: p 555: e 556: a 557: r 558: a 559: n 560:
              c 561: e 562: 563: w 564: h 565: i 566: c 567: h 568: 569: r 570:
              e 571: s 572: u 573: l 574: t 575: 576: f 577: r 578: o 579: m
              580: 581: d 582: i 583: f 584: f 585: e 586: r 587: e 588: n 589:
              t 590: 591: m 592: u 593: s 594: c 595: l 596: e 597: 598: a 599:
              c 600: t 601: i 602: o 603: n 604: s 605: . 606: 607: T 608: h
              609: e 610: 611: f 612: i 613: n 614: e 615: n 616: e 617: s 618:
              s 619: 620: o 621: f 622: 623: t 624: h 625: e 626: 627: s 628: c
              629: o 630: r 631: i 632: n 633: g 634: 635: c 636: a 637: t 638:
              e 639: g 640: o 641: r 642: i 643: e 644: s 645: 646: d 647: e
              648: p 649: e 650: n 651: d 652: s 653: 654: o 655: n 656: 657: w
              658: h 659: a 660: t 661: 662: c 663: a 664: n 665: 666: b 667: e
              668: 669: r 670: e 671: l 672: i 673: a 674: b 675: l 676: y 677:
              678: d 679: i 680: s 681: t 682: i 683: n 684: g 685: u 686: i
              687: s 688: h 689: e 690: d 691: 692: w 693: h 694: e 695: n 696:
              697: a 698: 699: f 700: a 701: c 702: i 703: a 704: l 705: 706: m
              707: o 708: v 709: e 710: m 711: e 712: n 713: t 714: 715: i 716:
              s 717: 718: i 719: n 720: s 721: p 722: e 723: c 724: t 725: e
              726: d 727: 728: r 729: e 730: p 731: e 732: a 733: t 734: e 735:
              d 736: l 737: y 738: , 739: 740: a 741: n 742: d 743: 744: i 745:
              n 746: 747: s 748: t 749: o 750: p 751: p 752: e 753: d 754: 755:
              a 756: n 757: d 758: 759: s 760: l 761: o 762: w 763: e 764: d
              765: 766: a 767: c 768: t 769: i 770: o 771: n 772: . 773: 774: T
              775: h 776: e 777: 778: a 779: u 780: t 781: h 782: o 783: r 784:
              s 785: 786: r 787: e 788: p 789: o 790: r 791: t 792: 793: t 794:
              h 795: a 796: t 797: 798: f 799: o 800: r 801: 802: t 803: h 804:
              e 805: 806: s 807: i 808: x 809: 810: p 811: e 812: o 813: p 814:
              l 815: e 816: 817: w 818: h 819: o 820: 821: l 822: e 823: a 824:
              r 825: n 826: e 827: d 828: 829: F 830: A 831: C 832: , 833: 834:
              a 835: b 836: o 837: u 838: t 839: 840: 4 841: 0 842: 843: h 844:
              o 845: u 846: r 847: s 848: 849: w 850: a 851: s 852: 853: r 854:
              e 855: q 856: u 857: i 858: r 859: e 860: d 861: 862: f 863: o
              864: r 865: 866: t 867: h 868: e 869: m 870: 871: t 872: o 873:
              874: l 875: e 876: a 877: r 878: n 879: 880: a 881: n 882: d 883:
              884: p 885: r 886: a 887: c 888: t 889: i 890: c 891: e 892: 893:
              s 894: c 895: o 896: r 897: i 898: n 899: g 900: . 901: 902: R
              903: e 904: l 905: i 906: a 907: b 908: i 909: l 910: i 911: t
              912: y 913: 914: i 915: n 916: 917: s 918: c 919: o 920: r 921: i
              922: n 923: g 924: 925: w 926: a 927: s 928: 929: s 930: a 931: i
              932: d 933: 934: t 935: o 936: 937: b 938: e 939: 940: s 941: a
              942: t 943: i 944: s 945: f 946: a 947: c 948: t 949: o 950: r
              951: y 952: . 953: 954: T 955: h 956: e 957: 958: f 959: o 960: r
              961: m 962: u 963: l 964: a 965: 966: u 967: s 968: e 969: d 970:
              971: w 972: a 973: s 974: 975: t 976: o 977: 978: d 979: i 980: v
              981: i 982: d 983: e 984: 985: t 986: h 987: e 988: 989: n 990: u
              991: m 992: b 993: e 994: r 995: 996: o 997: f 998: 999: a 1000:
              c 1001: t 1002: i 1003: o 1004: n 1005: 1006: u 1007: n 1008: i
              1009: t 1010: 1011: ( 1012: A 1013: U 1014: ) 1015: 1016: s 1017:
              c 1018: o 1019: r 1020: e 1021: s 1022: 1023: o 1024: n 1025:
              1026: w 1027: h 1028: i 1029: c 1030: h 1031: 1032: t 1033: w
              1034: o 1035: 1036: p 1037: e 1038: r 1039: s 1040: o 1041: n
              1042: s 1043: 1044: a 1045: g 1046: r 1047: e 1048: e 1049: d
              1050: 1051: b 1052: y 1053: 1054: t 1055: h 1056: e 1057: 1058: s
              1059: u 1060: m 1061: 1062: o 1063: f 1064: 1065: t 1066: h 1067:
              e 1068: 1069: n 1070: u 1071: m 1072: b 1073: e 1074: r 1075:
              1076: o 1077: f 1078: 1079: A 1080: U 1081: s 1082: 1083: s 1084:
              c 1085: o 1086: r 1087: e 1088: d 1089: 1090: b 1091: y 1092:
              1093: e 1094: a 1095: c 1096: h 1097: 1098: p 1099: e 1100: r
              1101: s 1102: o 1103: n 1104: . 1105: 1106: T 1107: h 1108: e
              1109: 1110: a 1111: v 1112: e 1113: r 1114: a 1115: g 1116: e
              1117: 1118: c 1119: o 1120: e 1121: f 1122: f 1123: i 1124: c
              1125: i 1126: e 1127: n 1128: t 1129: 1130: o 1131: f 1132: 1133:
              a 1134: g 1135: r 1136: e 1137: e 1138: m 1139: e 1140: n 1141: t
              1142: 1143: a 1144: m 1145: o 1146: n 1147: g 1148: 1149: a 1150:
              l 1151: l 1152: 1153: p 1154: o 1155: s 1156: s 1157: i 1158: b
              1159: l 1160: e 1161: 1162: p 1163: a 1164: i 1165: r 1166: i
              1167: n 1168: g 1169: s 1170: 1171: o 1172: f 1173: 1174: t 1175:
              h 1176: e 1177: 1178: s 1179: i 1180: x 1181: 1182: p 1183: e
              1184: r 1185: s 1186: o 1187: n 1188: s 1189: 1190: a 1191: c
              1192: r 1193: o 1194: s 1195: s 1196: 1197: t 1198: h 1199: e
              1200: 1201: f 1202: a 1203: c 1204: e 1205: s 1206: 1207: t 1208:
              h 1209: e 1210: y 1211: 1212: m 1213: e 1214: a 1215: s 1216: u
              1217: r 1218: e 1219: d 1220: 1221: w 1222: a 1223: s 1224: 1225:
              . 1226: 8 1227: 3 1228: . 1229: 1230: A 1231: 1232: n 1233: e
              1234: w 1235: e 1236: r 1237: 1238: v 1239: e 1240: r 1241: s
              1242: i 1243: o 1244: n 1245: 1246: o 1247: f 1248: 1249: t 1250:
              h 1251: e 1252: 1253: F 1254: A 1255: C 1256: S 1257: 1258: w
              1259: a 1260: s 1261: 1262: d 1263: e 1264: v 1265: e 1266: l
              1267: o 1268: p 1269: e 1270: d 1271: 1272: b 1273: y 1274: 1275:
              E 1276: c 1277: k 1278: m 1279: a 1280: n 1281: , 1282: 1283: F
              1284: r 1285: i 1286: e 1287: s 1288: e 1289: n 1290: , 1291:
              1292: a 1293: n 1294: d 1295: 1296: H 1297: a 1298: g 1299: e
              1300: r 1301: 1302: ( 1303: 2 1304: 0 1305: 0 1306: 2 1307: )
              1308: . 1309: 1310: ( 1311: P 1312: s 1313: y 1314: c 1315: T
              1316: E 1317: S 1318: T 1319: S 1320: 1321: D 1322: a 1323: t
              1324: a 1325: b 1326: a 1327: s 1328: e 1329: 1330: R 1331: e
              1332: c 1333: o 1334: r 1335: d 1336: 1337: ( 1338: c 1339: )
              1340: 1341: 2 1342: 0 1343: 1 1344: 9 1345: 1346: A 1347: P 1348:
              A 1349: , 1350: 1351: a 1352: l 1353: l 1354: 1355: r 1356: i
              1357: g 1358: h 1359: t 1360: s 1361: 1362: r 1363: e 1364: s
              1365: e 1366: r 1367: v 1368: e 1369: d 1370: )",
  journal  = "Environmental Psychology \& Nonverbal Behavior"
}

@ARTICLE{Ito2022-eo,
  title    = "Constructing neural network models from brain data reveals
              representational transformations linked to adaptive behavior",
  author   = "Ito, Takuya and Yang, Guangyu Robert and Laurent, Patryk and
              Schultz, Douglas H and Cole, Michael W",
  abstract = "The human ability to adaptively implement a wide variety of tasks
              is thought to emerge from the dynamic transformation of cognitive
              information. We hypothesized that these transformations are
              implemented via conjunctive activations in ``conjunction
              hubs''-brain regions that selectively integrate sensory,
              cognitive, and motor activations. We used recent advances in
              using functional connectivity to map the flow of activity between
              brain regions to construct a task-performing neural network model
              from fMRI data during a cognitive control task. We verified the
              importance of conjunction hubs in cognitive computations by
              simulating neural activity flow over this empirically-estimated
              functional connectivity model. These empirically-specified
              simulations produced above-chance task performance (motor
              responses) by integrating sensory and task rule activations in
              conjunction hubs. These findings reveal the role of conjunction
              hubs in supporting flexible cognitive computations, while
              demonstrating the feasibility of using empirically-estimated
              neural network models to gain insight into cognitive computations
              in the human brain.",
  journal  = "Nat. Commun.",
  volume   =  13,
  number   =  1,
  pages    = "673",
  month    =  feb,
  year     =  2022,
  language = "en"
}

@ARTICLE{Sheaffer2009-mc,
  title     = "Facial expression recognition deficits and faulty learning:
               Implications for theoretical models and clinical applications",
  author    = "Sheaffer, Beverly L and Golden, Jeannie A and Averett, Paige",
  journal   = "Int. J. Behav. Consult. Ther.",
  publisher = "Joseph D. Cautilli",
  volume    =  5,
  number    =  1,
  pages     = "31",
  year      =  2009
}

@MISC{Holberg2006-pc,
  title   = "Inter-individual Variability of the Facial Morphology During
             Conscious Smiling",
  author  = "Holberg, Christof and Maier, Cathrin and Steinh{\"a}user, Stefanie
             and Rudzki-Janson, Ingrid",
  journal = "Journal of Orofacial Orthopedics / Fortschritte der
             Kieferorthop{\"a}die",
  volume  =  67,
  number  =  4,
  pages   = "234--243",
  year    =  2006
}

@ARTICLE{Grabowski2019-wf,
  title    = "Emotional expression in psychiatric conditions: New technology
              for clinicians",
  author   = "Grabowski, Karol and Rynkiewicz, Agnieszka and Lassalle, Amandine
              and Baron-Cohen, Simon and Schuller, Bj{\"o}rn and Cummins,
              Nicholas and Baird, Alice and Podg{\'o}rska-Bednarz, Justyna and
              Pienika{\.z}ek, Agata and {\L}ucka, Izabela",
  abstract = "AIM: Emotional expressions are one of the most widely studied
              topics in neuroscience, from both clinical and non-clinical
              perspectives. Atypical emotional expressions are seen in various
              psychiatric conditions, including schizophrenia, depression, and
              autism spectrum conditions. Understanding the basics of emotional
              expressions and recognition can be crucial for diagnostic and
              therapeutic procedures. Emotions can be expressed in the face,
              gesture, posture, voice, and behavior and affect physiological
              parameters, such as the heart rate or body temperature. With
              modern technology, clinicians can use a variety of tools ranging
              from sophisticated laboratory equipment to smartphones and web
              cameras. The aim of this paper is to review the currently used
              tools using modern technology and discuss their usefulness as
              well as possible future directions in emotional expression
              research and treatment strategies. METHODS: The authors conducted
              a literature review in the PubMed, EBSCO, and SCOPUS databases,
              using the following key words: 'emotions,' 'emotional
              expression,' 'affective computing,' and 'autism.' The most
              relevant and up-to-date publications were identified and
              discussed. Search results were supplemented by the authors' own
              research in the field of emotional expression. RESULTS: We
              present a critical review of the currently available technical
              diagnostic and therapeutic methods. The most important studies
              are summarized in a table. CONCLUSION: Most of the currently
              available methods have not been adequately validated in clinical
              settings. They may be a great help in everyday practice; however,
              they need further testing. Future directions in this field
              include more virtual-reality-based and interactive interventions,
              as well as development and improvement of humanoid robots.",
  journal  = "Psychiatry Clin. Neurosci.",
  volume   =  73,
  number   =  2,
  pages    = "50--62",
  month    =  feb,
  year     =  2019,
  keywords = "affective computing; autism; emotions; expressed emotion;
              nonverbal communication",
  language = "en"
}

@ARTICLE{Kunz2014-jg,
  title    = "The faces of pain: a cluster analysis of individual differences
              in facial activity patterns of pain",
  author   = "Kunz, M and Lautenbacher, S",
  abstract = "BACKGROUND: There is general agreement that facial activity
              during pain conveys pain-specific information but is nevertheless
              characterized by substantial inter-individual differences. With
              the present study we aim to investigate whether these differences
              represent idiosyncratic variations or whether they can be
              clustered into distinct facial activity patterns. METHODS: Facial
              actions during heat pain were assessed in two samples of
              pain-free individuals (n = 128; n = 112) and were later analysed
              using the Facial Action Coding System. Hierarchical cluster
              analyses were used to look for combinations of single facial
              actions in episodes of pain. The stability/replicability of
              facial activity patterns was determined across samples as well as
              across different basic social situations. RESULTS: Cluster
              analyses revealed four distinct activity patterns during pain,
              which stably occurred across samples and situations: (I) narrowed
              eyes with furrowed brows and wrinkled nose; (II) opened mouth
              with narrowed eyes; (III) raised eyebrows; and (IV) furrowed
              brows with narrowed eyes. In addition, a considerable number of
              participants were facially completely unresponsive during pain
              induction (stoic cluster). These activity patterns seem to be
              reaction stereotypies in the majority of individuals (in nearly
              two-thirds), whereas a minority displayed varying clusters across
              situations. CONCLUSION: These findings suggest that there is no
              uniform set of facial actions but instead there are at least four
              different facial activity patterns occurring during pain that are
              composed of different configurations of facial actions. Raising
              awareness about these different 'faces of pain' might hold the
              potential of improving the detection and, thereby, the
              communication of pain.",
  journal  = "Eur. J. Pain",
  volume   =  18,
  number   =  6,
  pages    = "813--823",
  month    =  jul,
  year     =  2014,
  language = "en"
}

@ARTICLE{Bar2006-bf,
  title    = "Very first impressions",
  author   = "Bar, Moshe and Neta, Maital and Linz, Heather",
  abstract = "First impressions of people's personalities are often formed by
              using the visual appearance of their faces. Defining how quickly
              these impressions can be formed has critical implications for
              understanding social interactions and for determining the visual
              properties used to shape them. To study impression formation
              independent of emotional cues, threat judgments were made on
              faces with a neutral expression. Consequently, participants'
              judgments pertained to the personality rather than to a certain
              temporary emotional state (e.g., anger). The results demonstrate
              that consistent first impressions can be formed very quickly,
              based on whatever information is available within the first 39
              ms. First impressions were less consistent under these conditions
              when the judgments were about intelligence, suggesting that
              survival-related traits are judged more quickly. The authors
              propose that low spatial frequencies mediate this swift formation
              of threat judgments and provide evidence that supports this
              hypothesis.",
  journal  = "Emotion",
  volume   =  6,
  number   =  2,
  pages    = "269--278",
  month    =  may,
  year     =  2006,
  language = "en"
}

@ARTICLE{Cottrell1990-vq,
  title   = "{EMPATH}: Face, emotion, and gender recognition using holons",
  author  = "Cottrell, Garrison and Metcalfe, Janet",
  journal = "Adv. Neural Inf. Process. Syst.",
  volume  =  3,
  year    =  1990
}

@ARTICLE{Kring2007-tn,
  title    = "The Facial Expression Coding System ({FACES)}: development,
              validation, and utility",
  author   = "Kring, Ann M and Sloan, Denise M",
  abstract = "This article presents information on the development and
              validation of the Facial Expression Coding System (FACES; A. M.
              Kring \& D. Sloan, 1991). Grounded in a dimensional model of
              emotion, FACES provides information on the valence (positive,
              negative) of facial expressive behavior. In 5 studies,
              reliability and validity data from 13 diverse samples, including
              students, psychiatric patients, and community adults, are
              presented, and results indicate that raters can reliably agree on
              instances of positive and negative expressive behavior. Validity
              studies indicate that FACES ratings are related in predictable
              ways to another observational coding system, facial muscle
              activity, individual-difference measures of expressiveness and
              personality, skin conductance, heart rate, and reports of
              experienced emotion. FACES can be a useful tool for assessing
              expressive behavior in a variety of contexts.",
  journal  = "Psychol. Assess.",
  volume   =  19,
  number   =  2,
  pages    = "210--224",
  month    =  jun,
  year     =  2007,
  language = "en"
}

@ARTICLE{Frith2009-vc,
  title    = "Role of facial expressions in social interactions",
  author   = "Frith, Chris",
  abstract = "The expressions we see in the faces of others engage a number of
              different cognitive processes. Emotional expressions elicit rapid
              responses, which often imitate the emotion in the observed face.
              These effects can even occur for faces presented in such a way
              that the observer is not aware of them. We are also very good at
              explicitly recognizing and describing the emotion being
              expressed. A recent study, contrasting human and humanoid robot
              facial expressions, suggests that people can recognize the
              expressions made by the robot explicitly, but may not show the
              automatic, implicit response. The emotional expressions presented
              by faces are not simply reflexive, but also have a communicative
              component. For example, empathic expressions of pain are not
              simply a reflexive response to the sight of pain in another,
              since they are exaggerated when the empathizer knows he or she is
              being observed. It seems that we want people to know that we are
              empathic. Of especial importance among facial expressions are
              ostensive gestures such as the eyebrow flash, which indicate the
              intention to communicate. These gestures indicate, first, that
              the sender is to be trusted and, second, that any following
              signals are of importance to the receiver.",
  journal  = "Philos. Trans. R. Soc. Lond. B Biol. Sci.",
  volume   =  364,
  number   =  1535,
  pages    = "3453--3458",
  month    =  dec,
  year     =  2009,
  language = "en"
}

@ARTICLE{Kuzovkin2018-pt,
  title    = "Activations of deep convolutional neural networks are aligned
              with gamma band activity of human visual cortex",
  author   = "Kuzovkin, Ilya and Vicente, Raul and Petton, Mathilde and
              Lachaux, Jean-Philippe and Baciu, Monica and Kahane, Philippe and
              Rheims, Sylvain and Vidal, Juan R and Aru, Jaan",
  abstract = "Recent advances in the field of artificial intelligence have
              revealed principles about neural processing, in particular about
              vision. Previous work demonstrated a direct correspondence
              between the hierarchy of the human visual areas and layers of
              deep convolutional neural networks (DCNN) trained on visual
              object recognition. We use DCNN to investigate which frequency
              bands correlate with feature transformations of increasing
              complexity along the ventral visual pathway. By capitalizing on
              intracranial depth recordings from 100 patients we assess the
              alignment between the DCNN and signals at different frequency
              bands. We find that gamma activity (30-70 Hz) matches the
              increasing complexity of visual feature representations in DCNN.
              These findings show that the activity of the DCNN captures the
              essential characteristics of biological object recognition not
              only in space and time, but also in the frequency domain. These
              results demonstrate the potential that artificial intelligence
              algorithms have in advancing our understanding of the brain.",
  journal  = "Commun Biol",
  volume   =  1,
  pages    = "107",
  month    =  aug,
  year     =  2018,
  language = "en"
}

@ARTICLE{Hess2009-xj,
  title   = "facial {eMG}",
  author  = "Hess, Ursula",
  journal = "Methods in social neuroscience",
  pages   = "70--91",
  year    =  2009
}

@ARTICLE{Dobs2022-xs,
  title    = "Brain-like functional specialization emerges spontaneously in
              deep neural networks",
  author   = "Dobs, Katharina and Martinez, Julio and Kell, Alexander J E and
              Kanwisher, Nancy",
  abstract = "The human brain contains multiple regions with distinct, often
              highly specialized functions, from recognizing faces to
              understanding language to thinking about what others are
              thinking. However, it remains unclear why the cortex exhibits
              this high degree of functional specialization in the first place.
              Here, we consider the case of face perception using artificial
              neural networks to test the hypothesis that functional
              segregation of face recognition in the brain reflects a
              computational optimization for the broader problem of visual
              recognition of faces and other visual categories. We find that
              networks trained on object recognition perform poorly on face
              recognition and vice versa and that networks optimized for both
              tasks spontaneously segregate themselves into separate systems
              for faces and objects. We then show functional segregation to
              varying degrees for other visual categories, revealing a
              widespread tendency for optimization (without built-in
              task-specific inductive biases) to lead to functional
              specialization in machines and, we conjecture, also brains.",
  journal  = "Sci Adv",
  volume   =  8,
  number   =  11,
  pages    = "eabl8913",
  month    =  mar,
  year     =  2022,
  language = "en"
}

@MISC{Bruce1986-xd,
  title   = "Understanding face recognition",
  author  = "Bruce, Vicki and Young, Andy",
  journal = "British Journal of Psychology",
  volume  =  77,
  number  =  3,
  pages   = "305--327",
  year    =  1986
}

@INPROCEEDINGS{Sultana2018-gq,
  title     = "Advancements in Image Classification using Convolutional Neural
               Network",
  booktitle = "2018 Fourth International Conference on Research in
               Computational Intelligence and Communication Networks
               ({ICRCICN})",
  author    = "Sultana, Farhana and Sufian, Abu and Dutta, Paramartha",
  abstract  = "Convolutional Neural Network (CNN) is the state-of-the-art for
               image classification task. Here we have briefly discussed
               different components of CNN. In this paper, We have explained
               different CNN architectures for image classification. Through
               this paper, we have shown advancements in CNN from LeNet-5 to
               latest SENet model. We have discussed the model description and
               training details of each model. We have also drawn a comparison
               among those models.",
  pages     = "122--129",
  month     =  nov,
  year      =  2018,
  keywords  = "Training;Convolution;Computer architecture;Convolutional neural
               networks;Computational
               modeling;Visualization;AlexNet;Capsnet;Convolutional Neural
               Network;Deep learning;DenseNet;Image classification;ResNet;SENet"
}

@INPROCEEDINGS{Jacob2021-oi,
  title     = "Facial action unit detection with transformers",
  booktitle = "Proceedings of the {IEEE/CVF} Conference on Computer Vision and
               Pattern Recognition",
  author    = "Jacob, Geethu Miriam and Stenger, Bjorn",
  pages     = "7680--7689",
  year      =  2021
}

@ARTICLE{Katana2019-wg,
  title    = "Emotion Regulation, Subjective {Well-Being}, and Perceived Stress
              in Daily Life of Geriatric Nurses",
  author   = "Katana, Marko and R{\"o}cke, Christina and Spain, Seth M and
              Allemand, Mathias",
  abstract = "This daily diary study examined the within-person coupling
              between four emotion regulation strategies and both subjective
              well-being and perceived stress in daily life of geriatric
              nurses. Participants (N = 89) described how they regulated their
              emotions in terms of cognitive reappraisal and suppression. They
              also indicated their subjective well-being and level of perceived
              stress each day over 3 weeks. At the within-person level,
              cognitive reappraisal intended to increase positive emotions was
              positively associated with higher subjective well-being and
              negatively associated with perceived stress. Suppression of the
              expression of positive emotions was negatively associated with
              subjective well-being and positively associated with perceived
              stress. However, cognitive reappraisal intended to down-regulate
              negative emotions and suppression as a strategy to inhibit the
              expression of negative emotions were not associated with daily
              well-being or perceived stress. Off-days were rated as days with
              higher subjective well-being and lower perceived stress in
              contrast to working days. At the between-person level,
              individuals who reported more daily negative affect reported
              increased suppression of positive emotions, corroborating the
              within-person findings. Moreover, findings indicated that nurses
              with more years of experience in the job reported higher
              subjective well-being and less perceived stress. These results
              provide insights into important daily emotional processes of
              geriatric nurses, both at workdays and in their leisure time.",
  journal  = "Front. Psychol.",
  volume   =  10,
  pages    = "1097",
  month    =  may,
  year     =  2019,
  keywords = "emotion regulation; everyday life; geriatric nurses; perceived
              stress; subjective well-being",
  language = "en"
}

@ARTICLE{Susskind2008-eh,
  title    = "Expressing fear enhances sensory acquisition",
  author   = "Susskind, Joshua M and Lee, Daniel H and Cusi, Andr{\'e}e and
              Feiman, Roman and Grabski, Wojtek and Anderson, Adam K",
  abstract = "It has been proposed that facial expression production originates
              in sensory regulation. Here we demonstrate that facial
              expressions of fear are configured to enhance sensory
              acquisition. A statistical model of expression appearance
              revealed that fear and disgust expressions have opposite shape
              and surface reflectance features. We hypothesized that this
              reflects a fundamental antagonism serving to augment versus
              diminish sensory exposure. In keeping with this hypothesis, when
              subjects posed expressions of fear, they had a subjectively
              larger visual field, faster eye movements during target
              localization and an increase in nasal volume and air velocity
              during inspiration. The opposite pattern was found for disgust.
              Fear may therefore work to enhance perception, whereas disgust
              dampens it. These convergent results provide support for the
              Darwinian hypothesis that facial expressions are not arbitrary
              configurations for social communication, but rather, expressions
              may have originated in altering the sensory interface with the
              physical world.",
  journal  = "Nat. Neurosci.",
  volume   =  11,
  number   =  7,
  pages    = "843--850",
  month    =  jul,
  year     =  2008,
  language = "en"
}

@ARTICLE{Bartlett2011-hy,
  title     = "Automated facial expression measurement: Recent applications to
               basic research in human behavior, learning, and education",
  author    = "Bartlett, M and Whitehill, Jacob",
  journal   = "Handbook of face perception",
  publisher = "Citeseer",
  pages     = "489--513",
  year      =  2011
}

@ARTICLE{Mellouk2020-fe,
  title    = "Facial emotion recognition using deep learning: review and
              insights",
  author   = "Mellouk, Wafa and Handouzi, Wahida",
  abstract = "Automatic emotion recognition based on facial expression is an
              interesting research field, which has presented and applied in
              several areas such as safety, health and in human machine
              interfaces. Researchers in this field are interested in
              developing techniques to interpret, code facial expressions and
              extract these features in order to have a better prediction by
              computer. With the remarkable success of deep learning, the
              different types of architectures of this technique are exploited
              to achieve a better performance. The purpose of this paper is to
              make a study on recent works on automatic facial emotion
              recognition FER via deep learning. We underline on these
              contributions treated, the architecture and the databases used
              and we present the progress made by comparing the proposed
              methods and the results obtained. The interest of this paper is
              to serve and guide researchers by review recent works and
              providing insights to make improvements to this field.",
  journal  = "Procedia Comput. Sci.",
  volume   =  175,
  pages    = "689--694",
  month    =  jan,
  year     =  2020,
  keywords = "facial emotion recognition; deep neural networks; automatic
              recognition; database"
}

@ARTICLE{Lu_undated-cw,
  title   = "Self-supervised learning for facial action unit recognition
             through temporal consistency",
  author  = "{Lu} and {Tavabi} and {Soleymani}",
  journal = "BMVC",
  year = 2020
}

@ARTICLE{Saito2020-vo,
  title         = "Action Units Recognition by Pairwise Deep Architecture",
  author        = "Saito, Junya and Kawamura, Ryosuke and Uchida, Akiyoshi and
                   Youoku, Sachihiro and Toyoda, Yuushi and Yamamoto, Takahisa
                   and Mi, Xiaoyu and Murase, Kentaro",
  abstract      = "In this paper, we propose a new automatic Action Units (AUs)
                   recognition method used in a competition, Affective Behavior
                   Analysis in-the-wild (ABAW). Our method tackles a problem of
                   AUs label inconsistency among subjects by using pairwise
                   deep architecture. While the baseline score is 0.31, our
                   method achieved 0.67 in validation dataset of the
                   competition.",
  month         =  oct,
  year          =  2020,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CV",
  eprint        = "2010.00288"
}

@INPROCEEDINGS{Zakharov2019-ft,
  title     = "Few-shot adversarial learning of realistic neural talking head
               models",
  booktitle = "Proceedings of the {IEEE/CVF} international conference on
               computer vision",
  author    = "Zakharov, Egor and Shysheya, Aliaksandra and Burkov, Egor and
               Lempitsky, Victor",
  pages     = "9459--9468",
  year      =  2019
}

@ARTICLE{Adolphs2006-pb,
  title     = "Perception and Emotion: How We Recognize Facial Expressions",
  author    = "Adolphs, Ralph",
  abstract  = "Perception and emotion interact, as is borne out by studies of
               how people recognize emotion from facial expressions.
               Psychological and neurological research has elucidated the
               processes, and the brain structures, that participate in facial
               emotion recognition. Studies have shown that emotional reactions
               to viewing faces can be very rapid and that these reactions may,
               in turn, be used to judge the emotion shown in the face. Recent
               experiments have argued that people actively explore facial
               expressions in order to recognize the emotion, a mechanism that
               emphasizes the instrumental nature of social cognition.",
  journal   = "Curr. Dir. Psychol. Sci.",
  publisher = "SAGE Publications Inc",
  volume    =  15,
  number    =  5,
  pages     = "222--226",
  month     =  oct,
  year      =  2006
}

@ARTICLE{Plutchik2001-fa,
  title     = "The Nature of Emotions: Human emotions have deep evolutionary
               roots, a fact that may explain their complexity and provide
               tools for clinical practice",
  author    = "Plutchik, Robert",
  journal   = "Am. Sci.",
  publisher = "Sigma Xi, The Scientific Research Society",
  volume    =  89,
  number    =  4,
  pages     = "344--350",
  year      =  2001
}

@ARTICLE{Rescigno2020-ih,
  title    = "Personalized models for facial emotion recognition through
              transfer learning",
  author   = "Rescigno, Martina and Spezialetti, Matteo and Rossi, Silvia",
  abstract = "Emotions represent a key aspect of human life and behavior. In
              recent years, automatic recognition of emotions has become an
              important component in the fields of affective computing and
              human-machine interaction. Among many physiological and kinematic
              signals that could be used to recognize emotions, acquiring
              facial expression images is one of the most natural and
              inexpensive approaches. The creation of a generalized,
              inter-subject, model for emotion recognition from facial
              expression is still a challenge, due to anatomical, cultural and
              environmental differences. On the other hand, using traditional
              machine learning approaches to create a subject-customized,
              personal, model would require a large dataset of labelled
              samples. For these reasons, in this work, we propose the use of
              transfer learning to produce subject-specific models for
              extracting the emotional content of facial images in the
              valence/arousal dimensions. Transfer learning allows us to reuse
              the knowledge assimilated from a large multi-subject dataset by a
              deep-convolutional neural network and employ the feature
              extraction capability in the single subject scenario. In this
              way, it is possible to reduce the amount of labelled data
              necessary to train a personalized model, with respect to relying
              just on subjective data. Our results suggest that generalized
              transferred knowledge, in conjunction with a small amount of
              personal data, is sufficient to obtain high recognition
              performances and improvement with respect to both a generalized
              model and personal models. For both valence and arousal
              dimensions, quite good performances were obtained (RMSE = 0.09
              and RMSE = 0.1 for valence and arousal, respectively). Overall
              results suggested that both the transferred knowledge and the
              personal data helped in achieving this improvement, even though
              they alternated in providing the main contribution. Moreover, in
              this task, we observed that the benefits of transferring
              knowledge are so remarkable that no specific active or passive
              sampling techniques are needed for selecting images to be
              labelled.",
  journal  = "Multimed. Tools Appl.",
  volume   =  79,
  number   =  47,
  pages    = "35811--35828",
  month    =  dec,
  year     =  2020
}

@MISC{Willis2006-lk,
  title   = "First Impressions",
  author  = "Willis, Janine and Todorov, Alexander",
  journal = "Psychological Science",
  volume  =  17,
  number  =  7,
  pages   = "592--598",
  year    =  2006
}

@ARTICLE{Lindsay2021-xx,
  title    = "Convolutional Neural Networks as a Model of the Visual System:
              Past, Present, and Future",
  author   = "Lindsay, Grace W",
  abstract = "Convolutional neural networks (CNNs) were inspired by early
              findings in the study of biological vision. They have since
              become successful tools in computer vision and state-of-the-art
              models of both neural activity and behavior on visual tasks. This
              review highlights what, in the context of CNNs, it means to be a
              good model in computational neuroscience and the various ways
              models can provide insight. Specifically, it covers the origins
              of CNNs and the methods by which we validate them as models of
              biological vision. It then goes on to elaborate on what we can
              learn about biological vision by understanding and experimenting
              on CNNs and discusses emerging opportunities for the use of CNNs
              in vision research beyond basic object recognition.",
  journal  = "J. Cogn. Neurosci.",
  volume   =  33,
  number   =  10,
  pages    = "2017--2031",
  month    =  sep,
  year     =  2021,
  language = "en"
}

@INPROCEEDINGS{Song2021-of,
  title     = "Facial Action Unit Detection Based on Transformer and Attention
               Mechanism",
  booktitle = "Image and Graphics",
  author    = "Song, Wenyu and Shi, Shuze and An, Gaoyun",
  abstract  = "Facial Action Unit (AU) detection is a key step in facial
               expression recognition and analysis. The recent AU detection
               methods usually use well-designed complex networks based on CNN
               or RNN. Here, we propose a novel facial action unit detection
               network based on Transformer and Attention Mechanism named
               TAM-Net, which combines the attention mechanism with the
               Transformer structure. Firstly, since the facial AU is an atomic
               muscle of the face, which usually occurs in a relatively fixed
               region, we introduce a fixed-position attention mechanism to
               explicitly guide the feature learning of the region of interest.
               Secondly, we also propose an attention adjustment mechanism to
               adaptively refine the attention map based on label information.
               Finally, the Transformer based on self-attention has been
               applied to many Computer Vision tasks and has achieved good
               performance, so we introduce the popular Transformer structure
               to automatically learning the relationship between different
               facial AUs. Experiments on the challenging BP4D dataset show
               that the proposed method achieves competitive results.",
  publisher = "Springer International Publishing",
  pages     = "461--472",
  year      =  2021
}

@ARTICLE{Hadj-Bouziane2008-wi,
  title    = "Perception of emotional expressions is independent of face
              selectivity in monkey inferior temporal cortex",
  author   = "Hadj-Bouziane, Fadila and Bell, Andrew H and Knusten, Tamara A
              and Ungerleider, Leslie G and Tootell, Roger B H",
  abstract = "The ability to perceive and differentiate facial expressions is
              vital for social communication. Numerous functional MRI (fMRI)
              studies in humans have shown enhanced responses to faces with
              different emotional valence, in both the amygdala and the visual
              cortex. However, relatively few studies have examined how valence
              influences neural responses in monkeys, thereby limiting the
              ability to draw comparisons across species and thus understand
              the underlying neural mechanisms. Here we tested the effects of
              macaque facial expressions on neural activation within these two
              regions using fMRI in three awake, behaving monkeys. Monkeys
              maintained central fixation while blocks of different monkey
              facial expressions were presented. Four different facial
              expressions were tested: (i) neutral, (ii) aggressive
              (open-mouthed threat), (iii) fearful (fear grin), and (iv)
              submissive (lip smack). Our results confirmed that both the
              amygdala and the inferior temporal cortex in monkeys are
              modulated by facial expressions. As in human fMRI, fearful
              expressions evoked the greatest response in monkeys-even though
              fearful expressions are physically dissimilar in humans and
              macaques. Furthermore, we found that valence effects were not
              uniformly distributed over the inferior temporal cortex.
              Surprisingly, these valence maps were independent of two related
              functional maps: (i) the map of ``face-selective'' regions (faces
              versus non-face objects) and (ii) the map of ``face-responsive''
              regions (faces versus scrambled images). Thus, the neural
              mechanisms underlying face perception and valence perception
              appear to be distinct.",
  journal  = "Proc. Natl. Acad. Sci. U. S. A.",
  volume   =  105,
  number   =  14,
  pages    = "5591--5596",
  month    =  apr,
  year     =  2008,
  language = "en"
}

@ARTICLE{Chu2017-xq,
  title    = "Selective Transfer Machine for Personalized Facial Expression
              Analysis",
  author   = "Chu, Wen-Sheng and De la Torre, Fernando and Cohn, Jeffrey F",
  abstract = "Automatic facial action unit (AU) and expression detection from
              videos is a long-standing problem. The problem is challenging in
              part because classifiers must generalize to previously unknown
              subjects that differ markedly in behavior and facial morphology
              (e.g., heavy versus delicate brows, smooth versus deeply etched
              wrinkles) from those on which the classifiers are trained. While
              some progress has been achieved through improvements in choices
              of features and classifiers, the challenge occasioned by
              individual differences among people remains. Person-specific
              classifiers would be a possible solution but for a paucity of
              training data. Sufficient training data for person-specific
              classifiers typically is unavailable. This paper addresses the
              problem of how to personalize a generic classifier without
              additional labels from the test subject. We propose a
              transductive learning method, which we refer to as a Selective
              Transfer Machine (STM), to personalize a generic classifier by
              attenuating person-specific mismatches. STM achieves this effect
              by simultaneously learning a classifier and re-weighting the
              training samples that are most relevant to the test subject. We
              compared STM to both generic classifiers and cross-domain
              learning methods on four benchmarks: CK+ [44], GEMEP-FERA [67],
              RUFACS [4] and GFT [57]. STM outperformed generic classifiers in
              all.",
  journal  = "IEEE Trans. Pattern Anal. Mach. Intell.",
  volume   =  39,
  number   =  3,
  pages    = "529--545",
  month    =  mar,
  year     =  2017,
  keywords = "Training;Feature extraction;Gold;Face;Hidden Markov
              models;Shape;Training data;Facial expression
              analysis;personalization;domain adaptation;transfer
              learning;support vector machine (SVM)"
}

@ARTICLE{Tassinary1992-ve,
  title     = "Unobservable Facial Actions and Emotion",
  author    = "Tassinary, Louis G and Cacioppo, John T",
  abstract  = "Surface electromyographic recordings in humans were first made
               less than 70 years ago, and the electromyographic study of
               covert facial actions during affect and emotion has less than a
               20-year history. Despite the relative youth of facial
               electromyography, its use in combination with autonomic measures
               and comprehensive overt facial action coding systems has
               provided a sensitive and effective armamentarium for
               investigating emotion and affect-laden information processing.
               Research over the past decade has demonstrated that facial
               electromyographic activity varies as a function of the
               intensity, valence, and sociality of emotional stimuli and shows
               that facial electromyographic activity is slightly different in
               deliberately manipulated and spontaneous expressions of emotion.
               The multiply determined nature of facial actions and
               expressions, however, has limited the inferences that can be
               made about the psychological significance of facial
               electromyographic responses. These limitations have begun to
               recede in recent years as a result of advances in the
               psychometric properties of facial electromyographic
               measurements, the quantification of electromyographic waveforms
               and patterns, the conjoint measurement of facial
               electromyographic and electro-cortical activity, the
               conceptualization of psychophysiological relations, and the
               formalization of psychophysiological inference.",
  journal   = "Psychol. Sci.",
  publisher = "SAGE Publications Inc",
  volume    =  3,
  number    =  1,
  pages     = "28--33",
  month     =  jan,
  year      =  1992
}

@ARTICLE{Bate2015-qk,
  title    = "The independence of expression and identity in face-processing:
              evidence from neuropsychological case studies",
  author   = "Bate, Sarah and Bennetts, Rachel",
  abstract = "The processing of facial identity and facial expression have
              traditionally been seen as independent-a hypothesis that has
              largely been informed by a key double dissociation between
              neurological patients with a deficit in facial identity
              recognition but not facial expression recognition, and those with
              the reverse pattern of impairment. The independence hypothesis is
              also reflected in more recent anatomical models of
              face-processing, although these theories permit some interaction
              between the two processes. Given that much of the traditional
              patient-based evidence has been criticized, a review of more
              recent case reports that are accompanied by neuroimaging data is
              timely. Further, the performance of individuals with
              developmental face-processing deficits has recently been
              considered with regard to the independence debate. This paper
              reviews evidence from both acquired and developmental disorders,
              identifying methodological and theoretical strengths and caveats
              in these reports, and highlighting pertinent avenues for future
              research.",
  journal  = "Front. Psychol.",
  volume   =  6,
  pages    = "770",
  month    =  jun,
  year     =  2015,
  keywords = "emotional expression; face recognition; face-processing; facial
              identity; prosopagnosia",
  language = "en"
}

@ARTICLE{Campbell1986-gy,
  title    = "Face recognition and lipreading. A neurological dissociation",
  author   = "Campbell, R and Landis, T and Regard, M",
  abstract = "Two cases with medial occipitotemporal ischaemic infarction in
              the territory of the posterior cerebral artery are discussed;
              neither was aphasic. The patient with the right-sided lesion is
              prosopagnosic and topographagnosic and is impaired at recognizing
              or classifying facial expressive gestures but can lipread speech
              efficiently. The other patient, with only a left-sided lesion,
              shows no deficits in face recognition nor in the classification
              of faces in terms of nonverbal messages, but is alexic and
              impaired at lipreading. It is argued that processing faces for
              verbal information (lipreading) and processing faces for
              nonverbal information (face recognition and interpretation of
              emotive and gestural messages) are functionally dissociated in
              the human brain. Theoretical interpretations of prosopagnosia
              that stress a perceptual component to all types of face
              recognition failure may therefore be misleading, for similar
              stimulus processing mechanisms are likely to be required to
              identify a speech sound from a face as to identify a nonverbal
              gesture of the mouth. Only the associative properties of the task
              differ.",
  journal  = "Brain",
  volume   = "109 ( Pt 3)",
  pages    = "509--521",
  month    =  jun,
  year     =  1986,
  language = "en"
}

@MISC{Yang2021-kx,
  title   = "Joint encoding of facial identity, orientation, gaze, and
             expression in the middle dorsal face area",
  author  = "Yang, Zetian and Freiwald, Winrich A",
  journal = "Proceedings of the National Academy of Sciences",
  volume  =  118,
  number  =  33,
  year    =  2021
}

@ARTICLE{Nagrani2017-xl,
  title         = "{VoxCeleb}: a large-scale speaker identification dataset",
  author        = "Nagrani, Arsha and Chung, Joon Son and Zisserman, Andrew",
  abstract      = "Most existing datasets for speaker identification contain
                   samples obtained under quite constrained conditions, and are
                   usually hand-annotated, hence limited in size. The goal of
                   this paper is to generate a large scale text-independent
                   speaker identification dataset collected 'in the wild'. We
                   make two contributions. First, we propose a fully automated
                   pipeline based on computer vision techniques to create the
                   dataset from open-source media. Our pipeline involves
                   obtaining videos from YouTube; performing active speaker
                   verification using a two-stream synchronization
                   Convolutional Neural Network (CNN), and confirming the
                   identity of the speaker using CNN based facial recognition.
                   We use this pipeline to curate VoxCeleb which contains
                   hundreds of thousands of 'real world' utterances for over
                   1,000 celebrities. Our second contribution is to apply and
                   compare various state of the art speaker identification
                   techniques on our dataset to establish baseline performance.
                   We show that a CNN based architecture obtains the best
                   performance for both identification and verification.",
  month         =  jun,
  year          =  2017,
  archivePrefix = "arXiv",
  primaryClass  = "cs.SD",
  eprint        = "1706.08612"
}

@ARTICLE{Mase1991-qc,
  title     = "Recognition of facial expression from optical flow",
  author    = "Mase, Kenji",
  journal   = "IEICE Trans. Inf. Syst.",
  publisher = "The Institute of Electronics, Information and Communication
               Engineers",
  volume    =  74,
  number    =  10,
  pages     = "3474--3483",
  year      =  1991
}

@ARTICLE{Perrett1984-ba,
  title    = "Neurones responsive to faces in the temporal cortex: studies of
              functional organization, sensitivity to identity and relation to
              perception",
  author   = "Perrett, D I and Smith, P A and Potter, D D and Mistlin, A J and
              Head, A S and Milner, A D and Jeeves, M A",
  abstract = "We have investigated the distribution of cells responsive to
              faces within the macaque temporal cortex and their sensitivity to
              different face attributes. We found a functional organization of
              cells responsive to the sight of different views of the head.
              Cells of a similar type were grouped together both vertically
              down through the cortex, and horizontally in patches extending
              0.5-2.0 mm across the surface of the cortex. A substantial
              proportion of cells responsive to faces were found to be
              sensitive to biologically important characteristics such as
              identity or expression. Cells were found to be highly selective
              for particular individuals that were familiar to the monkey with
              selectivity persisting across a great variety of viewing
              conditions such as changing face expression, orientation, colour,
              distance and size. Data suggested that sensitivity to identity
              arises at the level of specific views of the individual (e.g.
              full face). Information about different views may then be pooled
              to allow recognition independent of view. Visual transformations
              that make it difficult for humans to perceive faces (e.g.,
              contrast reversal, isoluminant colour, coarsely quantized images,
              rotation or inversion) reduced the magnitude or increased the
              latency of cells' responses to faces. In this way, cell responses
              were related to perception and not simply to visual qualities of
              the image.",
  journal  = "Hum. Neurobiol.",
  volume   =  3,
  number   =  4,
  pages    = "197--208",
  year     =  1984,
  language = "en"
}

@ARTICLE{Kazemi_undated-yl,
  title   = "One millisecond face alignment with an ensemble of regression
             trees",
  author  = "{Kazemi} and {Sullivan}",
  year = 2014,
  journal = "Proc. IAPR Int. Conf. Pattern Recogn."
}

@ARTICLE{Jiang_undated-jy,
  title   = "Disentangled representation learning for 3d face shape",
  year = 2019,
  author  = "{Jiang} and {Wu} and {Chen} and {others}",
  journal = "Proc. IEEE"
}

@ARTICLE{Goodfellow2015-nb,
  title    = "Challenges in representation learning: a report on three machine
              learning contests",
  author   = "Goodfellow, Ian J and Erhan, Dumitru and Luc Carrier, Pierre and
              Courville, Aaron and Mirza, Mehdi and Hamner, Ben and Cukierski,
              Will and Tang, Yichuan and Thaler, David and Lee, Dong-Hyun and
              Zhou, Yingbo and Ramaiah, Chetan and Feng, Fangxiang and Li,
              Ruifan and Wang, Xiaojie and Athanasakis, Dimitris and
              Shawe-Taylor, John and Milakov, Maxim and Park, John and Ionescu,
              Radu and Popescu, Marius and Grozea, Cristian and Bergstra, James
              and Xie, Jingjing and Romaszko, Lukasz and Xu, Bing and Chuang,
              Zhang and Bengio, Yoshua",
  abstract = "The ICML 2013 Workshop on Challenges in Representation
              Learning(1) focused on three challenges: the black box learning
              challenge, the facial expression recognition challenge, and the
              multimodal learning challenge. We describe the datasets created
              for these challenges and summarize the results of the
              competitions. We provide suggestions for organizers of future
              challenges and some comments on what kind of knowledge can be
              gained from machine learning competitions.",
  journal  = "Neural Netw.",
  volume   =  64,
  pages    = "59--63",
  month    =  apr,
  year     =  2015,
  keywords = "Competition; Dataset; Representation learning",
  language = "en"
}

@INPROCEEDINGS{Fan2016-ls,
  title     = "Video-based emotion recognition using {CNN-RNN} and {C3D} hybrid
               networks",
  booktitle = "Proceedings of the 18th {ACM} International Conference on
               Multimodal Interaction",
  author    = "Fan, Yin and Lu, Xiangju and Li, Dian and Liu, Yuanliu",
  abstract  = "In this paper, we present a video-based emotion recognition
               system submitted to the EmotiW 2016 Challenge. The core module
               of this system is a hybrid network that combines recurrent
               neural network (RNN) and 3D convolutional networks (C3D) in a
               late-fusion fashion. RNN and C3D encode appearance and motion
               information in different ways. Specifically, RNN takes
               appearance features extracted by convolutional neural network
               (CNN) over individual video frames as input and encodes motion
               later, while C3D models appearance and motion of video
               simultaneously. Combined with an audio module, our system
               achieved a recognition accuracy of 59.02\% without using any
               additional emotion-labeled video clips in training set, compared
               to 53.8\% of the winner of EmotiW 2015. Extensive experiments
               show that combining RNN and C3D together can improve video-based
               emotion recognition noticeably.",
  publisher = "Association for Computing Machinery",
  pages     = "445--450",
  series    = "ICMI '16",
  month     =  oct,
  year      =  2016,
  address   = "New York, NY, USA",
  keywords  = "Model Fusion, Long Short Term Memory network, Emotion
               Recognition, 3D convolutional Network, Recurrent Neural Network",
  location  = "Tokyo, Japan"
}

@INPROCEEDINGS{Yang2018-ln,
  title     = "Facial expression recognition by de-expression residue learning",
  booktitle = "Proceedings of the {IEEE} conference on computer vision and
               pattern recognition",
  author    = "Yang, Huiyuan and Ciftci, Umur and Yin, Lijun",
  pages     = "2168--2177",
  year      =  2018
}

@ARTICLE{Li_undated-le,
  title   = "Self-supervised representation learning from videos for facial
             action unit detection",
  author  = "{Li} and {Zeng} and {Shan} and {Chen}",
  journal = "Proc. IEEE",
  year = 2019
}

@ARTICLE{Abiodun2019-eh,
  title    = "Comprehensive Review of Artificial Neural Network Applications to
              Pattern Recognition",
  author   = "Abiodun, Oludare Isaac and Jantan, Aman and Omolara, Abiodun
              Esther and Dada, Kemi Victoria and Umar, Abubakar Malah and
              Linus, Okafor Uchenwa and Arshad, Humaira and Kazaure, Abdullahi
              Aminu and Gana, Usman and Kiru, Muhammad Ubale",
  abstract = "The era of artificial neural network (ANN) began with a
              simplified application in many fields and remarkable success in
              pattern recognition (PR) even in manufacturing industries.
              Although significant progress achieved and surveyed in addressing
              ANN application to PR challenges, nevertheless, some problems are
              yet to be resolved like whimsical orientation (the unknown path
              that cannot be accurately calculated due to its directional
              position). Other problem includes; object classification,
              location, scaling, neurons behavior analysis in hidden layers,
              rule, and template matching. Also, the lack of extant literature
              on the issues associated with ANN application to PR seems to slow
              down research focus and progress in the field. Hence, there is a
              need for state-of-the-art in neural networks application to PR to
              urgently address the above-highlights problems for more
              successes. The study furnishes readers with a clearer
              understanding of the current, and new trend in ANN models that
              effectively addresses PR challenges to enable research focus and
              topics. Similarly, the comprehensive review reveals the diverse
              areas of the success of ANN models and their application to PR.
              In evaluating the performance of ANN models, some statistical
              indicators for measuring the performance of the ANN model in many
              studies were adopted. Such as the use of mean absolute percentage
              error (MAPE), mean absolute error (MAE), root mean squared error
              (RMSE), and variance of absolute percentage error (VAPE). The
              result shows that the current ANN models such as GAN, SAE, DBN,
              RBM, RNN, RBFN, PNN, CNN, SLP, MLP, MLNN, Reservoir computing,
              and Transformer models are performing excellently in their
              application to PR tasks. Therefore, the study recommends the
              research focus on current models and the development of new
              models concurrently for more successes in the field.",
  journal  = "IEEE Access",
  volume   =  7,
  pages    = "158820--158846",
  year     =  2019,
  keywords = "Artificial neural networks;Task analysis;Fingerprint
              recognition;Computational modeling;Image
              recognition;Agriculture;Artificial neural networks;application to
              pattern recognition;feedforward neural networks;feedback neural
              networks;hybrid models"
}

@ARTICLE{Mavadati2013-tc,
  title    = "{DISFA}: A Spontaneous Facial Action Intensity Database",
  author   = "Mavadati, S Mohammad and Mahoor, Mohammad H and Bartlett, Kevin
              and Trinh, Philip and Cohn, Jeffrey F",
  abstract = "Access to well-labeled recordings of facial expression is
              critical to progress in automated facial expression recognition.
              With few exceptions, publicly available databases are limited to
              posed facial behavior that can differ markedly in conformation,
              intensity, and timing from what occurs spontaneously. To meet the
              need for publicly available corpora of well-labeled video, we
              collected, ground-truthed, and prepared for distribution the
              Denver intensity of spontaneous facial action database.
              Twenty-seven young adults were video recorded by a stereo camera
              while they viewed video clips intended to elicit spontaneous
              emotion expression. Each video frame was manually coded for
              presence, absence, and intensity of facial action units according
              to the facial action unit coding system. Action units are the
              smallest visibly discriminable changes in facial action; they may
              occur individually and in combinations to comprise more molar
              facial expressions. To provide a baseline for use in future
              research, protocols and benchmarks for automated action unit
              intensity measurement are reported. Details are given for
              accessing the database for research in computer vision, machine
              learning, and affective and behavioral science.",
  journal  = "IEEE Transactions on Affective Computing",
  volume   =  4,
  number   =  2,
  pages    = "151--160",
  month    =  apr,
  year     =  2013,
  keywords = "Databases;Gold;Encoding;Feature extraction;Face;Face
              recognition;Pain;FACS;action units;intensity;spontaneous facial
              behavior;facial expression;video corpus"
}

@ARTICLE{Elfenbein2002-nm,
  title    = "On the universality and cultural specificity of emotion
              recognition: a meta-analysis",
  author   = "Elfenbein, Hillary Anger and Ambady, Nalini",
  abstract = "A meta-analysis examined emotion recognition within and across
              cultures. Emotions were universally recognized at
              better-than-chance levels. Accuracy was higher when emotions were
              both expressed and recognized by members of the same national,
              ethnic, or regional group, suggesting an in-group advantage. This
              advantage was smaller for cultural groups with greater exposure
              to one another, measured in terms of living in the same nation,
              physical proximity, and telephone communication. Majority group
              members were poorer at judging minority group members than the
              reverse. Cross-cultural accuracy was lower in studies that used a
              balanced research design, and higher in studies that used
              imitation rather than posed or spontaneous emotional expressions.
              Attributes of study design appeared not to moderate the size of
              the in-group advantage.",
  journal  = "Psychol. Bull.",
  volume   =  128,
  number   =  2,
  pages    = "203--235",
  month    =  mar,
  year     =  2002,
  language = "en"
}

@ARTICLE{Ghayoumi_undated-pi,
  title   = "A quick review of deep learning in facial expression",
  author  = "{Ghayoumi}",
  year = 2017,
  journal = "J. Commun. Comput."
}

@ARTICLE{Wiles2018-iw,
  title         = "Self-supervised learning of a facial attribute embedding
                   from video",
  author        = "Wiles, Olivia and Sophia Koepke, A and Zisserman, Andrew",
  abstract      = "We propose a self-supervised framework for learning facial
                   attributes by simply watching videos of a human face
                   speaking, laughing, and moving over time. To perform this
                   task, we introduce a network, Facial Attributes-Net
                   (FAb-Net), that is trained to embed multiple frames from the
                   same video face-track into a common low-dimensional space.
                   With this approach, we make three contributions: first, we
                   show that the network can leverage information from multiple
                   source frames by predicting confidence/attention masks for
                   each frame; second, we demonstrate that using a curriculum
                   learning regime improves the learned embedding; finally, we
                   demonstrate that the network learns a meaningful face
                   embedding that encodes information about head pose, facial
                   landmarks and facial expression, i.e. facial attributes,
                   without having been supervised with any labelled data. We
                   are comparable or superior to state-of-the-art
                   self-supervised methods on these tasks and approach the
                   performance of supervised methods.",
  month         =  aug,
  year          =  2018,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CV",
  eprint        = "1808.06882"
}

@INPROCEEDINGS{Zhang2018-jo,
  title     = "Joint pose and expression modeling for facial expression
               recognition",
  booktitle = "Proceedings of the {IEEE} conference on computer vision and
               pattern recognition",
  author    = "Zhang, Feifei and Zhang, Tianzhu and Mao, Qirong and Xu,
               Changsheng",
  pages     = "3359--3368",
  year      =  2018
}

@ARTICLE{Simonyan2014-za,
  title         = "Very Deep Convolutional Networks for {Large-Scale} Image
                   Recognition",
  author        = "Simonyan, Karen and Zisserman, Andrew",
  abstract      = "In this work we investigate the effect of the convolutional
                   network depth on its accuracy in the large-scale image
                   recognition setting. Our main contribution is a thorough
                   evaluation of networks of increasing depth using an
                   architecture with very small (3x3) convolution filters,
                   which shows that a significant improvement on the prior-art
                   configurations can be achieved by pushing the depth to 16-19
                   weight layers. These findings were the basis of our ImageNet
                   Challenge 2014 submission, where our team secured the first
                   and the second places in the localisation and classification
                   tracks respectively. We also show that our representations
                   generalise well to other datasets, where they achieve
                   state-of-the-art results. We have made our two
                   best-performing ConvNet models publicly available to
                   facilitate further research on the use of deep visual
                   representations in computer vision.",
  month         =  sep,
  year          =  2014,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CV",
  eprint        = "1409.1556"
}

@ARTICLE{Andrews2005-le,
  title    = "Visual cortex: how are faces and objects represented?",
  author   = "Andrews, Timothy J",
  abstract = "The way in which information about complex objects and faces is
              represented in visual cortex is controversial. One model posits
              that information is processed in modules, highly specialized for
              different categories of objects; an opposing model appeals to a
              distributed representation across a large network of visual
              areas. A recent paper uses a novel imaging technique to address
              this controversy.",
  journal  = "Curr. Biol.",
  volume   =  15,
  number   =  12,
  pages    = "R451--3",
  month    =  jun,
  year     =  2005,
  language = "en"
}

@ARTICLE{Zadra2011-yr,
  title    = "Emotion and perception: the role of affective information",
  author   = "Zadra, Jonathan R and Clore, Gerald L",
  abstract = "Visual perception and emotion are traditionally considered
              separate domains of study. In this article, however, we review
              research showing them to be less separable than usually assumed.
              In fact, emotions routinely affect how and what we see. Fear, for
              example, can affect low-level visual processes, sad moods can
              alter susceptibility to visual illusions, and goal-directed
              desires can change the apparent size of goal-relevant objects. In
              addition, the layout of the physical environment, including the
              apparent steepness of a hill and the distance to the ground from
              a balcony can both be affected by emotional states. We propose
              that emotions provide embodied information about the costs and
              benefits of anticipated action, information that can be used
              automatically and immediately, circumventing the need for
              cogitating on the possible consequences of potential actions.
              Emotions thus provide a strong motivating influence on how the
              environment is perceived. WIREs Cogni Sci 2011 2 676-685 DOI:
              10.1002/wcs.147 This article is categorized under: Psychology >
              Emotion and Motivation.",
  journal  = "Wiley Interdiscip. Rev. Cogn. Sci.",
  volume   =  2,
  number   =  6,
  pages    = "676--685",
  month    =  nov,
  year     =  2011,
  language = "en"
}

@ARTICLE{Chung2018-up,
  title         = "{VoxCeleb2}: Deep Speaker Recognition",
  author        = "Chung, Joon Son and Nagrani, Arsha and Zisserman, Andrew",
  abstract      = "The objective of this paper is speaker recognition under
                   noisy and unconstrained conditions. We make two key
                   contributions. First, we introduce a very large-scale
                   audio-visual speaker recognition dataset collected from
                   open-source media. Using a fully automated pipeline, we
                   curate VoxCeleb2 which contains over a million utterances
                   from over 6,000 speakers. This is several times larger than
                   any publicly available speaker recognition dataset. Second,
                   we develop and compare Convolutional Neural Network (CNN)
                   models and training strategies that can effectively
                   recognise identities from voice under various conditions.
                   The models trained on the VoxCeleb2 dataset surpass the
                   performance of previous works on a benchmark dataset by a
                   significant margin.",
  month         =  jun,
  year          =  2018,
  archivePrefix = "arXiv",
  primaryClass  = "cs.SD",
  eprint        = "1806.05622"
}

@INPROCEEDINGS{Schroff2015-in,
  title     = "Facenet: A unified embedding for face recognition and clustering",
  booktitle = "Proceedings of the {IEEE} conference on computer vision and
               pattern recognition",
  author    = "Schroff, Florian and Kalenichenko, Dmitry and Philbin, James",
  pages     = "815--823",
  year      =  2015
}

@MISC{Lundqvist1998-xc,
  title     = "Karolinska Directed Emotional Faces",
  author    = "Lundqvist, D and Flykt, A and {\"O}hman, A",
  publisher = "American Psychological Association (APA)",
  month     =  may,
  year      =  1998,
  note      = "Title of the publication associated with this dataset: PsycTESTS Dataset"}

@INPROCEEDINGS{Mavadati2012,
  author="Mavadati, S. Mohammad and Mahoor, Mohammad H. and Bartlett, Kevin and Trinh, Philip", 
  title="Automatic detection of non-posed facial action units", 
  year=2012,
  pages="1817--1820"}