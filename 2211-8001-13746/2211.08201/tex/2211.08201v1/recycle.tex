In this section, we describe the multiagent rollout with random reshuffle algorithm, which is later applied to plan paths for warehouse robots. We will first introduce the multiagent optimal control problems with which we model the path planning problem of our interest, and then provide details of the rollout method. Our presentation is somewhat abstract, leaving further details specific to warehouse path planing to the next section.

\subsection{Multiagent Deterministic Optimal Control}
The problem considered here involves stationary dynamics
\begin{equation}
\label{eq:dynamics}
    x_{k+1}=f(x_k,u_k),\quad k=0,\,1,\,\dots,
\end{equation}
where $x_k$ and $u_k$ are state and control at stage $k$, which belong to state and control spaces $X$ and $U$, respectively, and $f$ maps $X\times U$ to $X$. Both the state and control spaces contain finitely many elements. The control $u_k$ must be chosen from a nonempty constraint set $U(x_k)\subset U$ that may depend on $x_k$. The multiagent nature of the problem manifests itself through the Cartesian product structure of $U(x_k)$, i.e.,
\begin{equation}
    \label{eq:cartesian}
    U(x_k)=U^1(x_k)\times \cdots \times U^m(x_k),
\end{equation}
where $m$ is an integer representing the number of agents involved in the problem. As a result, $u_k$ can be written as
$$u_k=(u_k^1,\dots,u_k^m),$$
with $u_k^i\in U^i(x_k)$, $i=1,\dots,m$. The cost of applying $u_k$ at state $x_k$ is denoted by $g(x_k,u_k)$, and is assumed to be real-valued.

We consider feedback policies of the form $\{\mu,\,\mu,\,\dots\}$, with $\mu$ being a function mapping $X$ to $U$ and satisfying $\mu(x)\in U(x)$ for all $x$. This type of polices are called \emph{stationary}. When no confusion arises, with a slight abuse of notation, we also denote $\{\mu,\,\mu,\,\dots\}$ as $\mu$. In view of the structure \eqref{eq:cartesian}, policy itself $\mu$ may be decomposed accordingly as
$$\mu(x)=\big(\mu^1(x),\dots,\mu^m(x)\big),$$
with $\mu^i(x)\in U_i(x)$, $i=1,\dots,m$. Accordingly, the system dynamics \eqref{eq:dynamics} and stage cost can be written as
$$x_{k+1}=f(x_k,u^1_k,\dots,u^m_k),\quad g(x_k,u^1_k,\dots,u^m_k)$$
respectively, with $u_k=(u^1_k,\dots,u^m_k)$ when stressing the multiagent structure of the problem.


The \emph{cost function} of a policy $\mu$, denoted by $J_\mu$, maps $X$ to the real line $\Re$, and is defined at any initial state $x_0 \in X$, as
\begin{equation}
\label{eq:pi_cost_function}
    J_\mu(x_0)= \sum_{k=0}^\infty \alpha^kg(x_k,\mu(x_k)),
\end{equation}
where $\alpha\in (0,1)$ is called \emph{discount factor}, which reflects the emphasis on the current cost over future ones, and $x_{k+1}=f(x_k,\mu(x_k))$, $k=0,\,1,\,\dots$. The limit in \eqref{eq:pi_cost_function} is well-posed in view of finiteness of $X$ and $U$. The optimal cost function $J^*$ is defined pointwise as
\begin{equation}
    J^*(x_0)=\inf_{u_k\in U(x_k),\ k=0,1,\ldots\atop x_{k+1}=f(x_k,u_k),\ k=0,1,\ldots}\sum_{k=0}^\infty  \alpha^kg(x_k,u_k).
\end{equation}
A stationary policy $\mu^*$ is called optimal if
\begin{equation*}
    J_{\mu^*}(x)=J^*(x),\quad \forall x\in X.
\end{equation*}

For the problem considered here, it can be shown that there exists a stationary optimal policy $\mu^*$; see, i.e., \cite[Props.~4.3.2 and 4.3.4]{bertsekas2019reinforcement}. Two exact DP algorithms for addressing the problem are value iteration (VI) and policy iteration (PI). However, these algorithms may be intractable, due to the \emph{curse of dimensionality}, which refers to the explosion of the computation as the cardinality of $X$ increases. Thus, various approximation schemes have been proposed, where one aims to obtain some suboptimal policy $\Tilde{\mu}$ such that $J_{\Tilde{\mu}}\approx J^*$. These schemes often involve minimizing some objective over the set $U(x)$. However, due to the structure \eqref{eq:cartesian} of $U(x)$, even those suboptimal scheme can be rather challenging as searching through the set $U(x)$ can be too costly as well. To see this, suppose each set $U^i(x)$ contains at most $N$ elements, then the total number of elements in $U(x)$ can be as large as $N^m$, which is prohibitively expensive to search through even for modest $m$. The multiagent rollout algorithm proposed in \cite{bertsekas2021multiagent} addresses exactly this problem, as we will discuss next.




The basis of our algorithm is that a function $\hat{J}:X\mapsto\Re$ that sets a realistic bound for the multiagent rollout policy is assumed to exist. We introduce permutation functions, denoted by $\sigma$ and $\tau$, which are one-to-one mappings with both domain and image being $\{1,\dots,m\}$. Thus, their inverse images exist and are denoted as $\sigma^{-1}$ and $\tau^{-1}$ respectively. 

Given the current state $x$ and permutation $\sigma$, we define $\tau_0=\sigma$. In addition, we define $\ell_0^i$ as
\begin{equation}
\label{eq:ell_tau}
    \ell_0^i=\tau_0^{-1}(i),\quad i=1,\dots,m.
\end{equation}
Then with a slight abuse of notation, we rearrange the elements of control according to $\tau_0$, i.e.,
$$u=\Big(u^{\ell_0^1},u^{\ell_0^2},\dots,u^{\ell_0^m}\Big),$$
and define control $\Tilde{u}_0$ via
\begin{equation}
    \label{eq:multi_rollout_tau}
        \begin{aligned}
    \Tilde{u}_0^{\ell_0^1}&\in\arg\min_{u^{\ell_0^1}\in U^{{\ell_0^1}}(x)}\Big\{g\big(x,u^{\ell_0^1},\mu^{\ell_0^2}(x),\dots,\mu^{\ell_0^m}(x)\big)\\
    &+\alpha J_\mu\Big(f\big(x,u^{\ell_0^1},\mu^{\ell_0^2}(x),\dots,\mu^{\ell_0^m}(x)\big)\Big)\Big\},\\
    \Tilde{u}_0^{\ell_0^2}&\in\arg\min_{u^{\ell_0^2}\in U^{{\ell_0^2}}(x)}\Big\{g\big(x,\Tilde{u}_0^{\ell_0^1},u^{\ell_0^2},\mu^{\ell_0^3}(x),\dots,\mu^{\ell_0^m}(x)\big)\\
    &+\alpha J_\mu\Big(f\big(x,\Tilde{u}_0^{\ell_0^1},u^{\ell_0^2},\mu^{\ell_0^3}(x),\dots,\mu^{\ell_0^m}(x)\big)\Big\},\\
    \dots& \qquad \dots\qquad \dots\\
    \Tilde{u}_0^{\ell_0^m}&\in\arg\min_{u^{\ell_0^m}\in U^{{\ell_0^m}}(x)}\Big\{g\big(x,\Tilde{u}_0^{\ell_0^1},\dots,\Tilde{u}_0^{\ell_0^{m-1}},u^{\ell_0^m}\big)\\
    &+\alpha J_\mu\Big(f\big(x,\Tilde{u}_0^{\ell_0^1},\dots,\Tilde{u}_0^{\ell_0^{m-1}},u^{\ell_0^m}\big)\Big)\Big\},
        \end{aligned}
\end{equation}
and define a control $\Tilde{u}_0$ as
$$\Tilde{u}_0=\Big(\Tilde{u}_0^{\ell_0^1},\Tilde{u}_0^{\ell_0^2},\dots,\Tilde{u}_0^{\ell_0^m}\Big).$$
In addition, a value $\hat{V}_0$ is defined similar to \eqref{eq:j_tilde} as
$$\hat{V}_0=g(x,\Tilde{u}_0)+\alpha J_\mu\big(f(x,\Tilde{u}_0)\big).$$

If $\hat{V}_0\leq \hat{J}(x)$, then we set $\Tilde{\mu}(x)=\Tilde{u}_0$. Otherwise, we generate according to certain probability distribution a new permutation $\tau_1$, define $\ell_1^i$ similar to \eqref{eq:ell_tau}, and carry out agent-by-agent optimization similar to \eqref{eq:multi_rollout_tau}. This procedure is repeated until after generating, say $j$, new permutations, the corresponding $\hat{V}_j$ fulfills that $\hat{V}_j\leq \hat{J}(x)$. Then we define $\Tilde{\mu}(x)=\Tilde{u}_j$, and the state $x'$ and permutation $\sigma'$ at the next stage is given by
$$x'=f\big(x,\Tilde{\mu}(x)\big),\quad \sigma'=\tau_j.$$

As stated earlier, the performance bound \eqref{eq:cost_bound} remains valid regardless of the order of agents being optimized. Thus, in view of the construction of $\Tilde{\mu}$, we have that 
$$J_{\Tilde{\mu}}(x)\leq \hat{J}(x),\quad \forall x\in X.$$


Efficiently solving path planning problems for a large number of robots is critical to the operation of modern warehouses. The existing approaches adopt classical shortest path algorithms to plan in environments whose cells are associated with both space and time in order to avoid collision between robots. In this work, we achieve the same goal by means of simulation in a smaller static environment. Built upon a new tool introduced in bertsekas, we propose a multiagent rollout with reshuffle algorithm, and apply it to address the problem of warehouse path planning. The proposed scheme has solid theoretical guarantees and exhibits consistent performance in our numerical studies. Moreover, it inherits the ability to adapt to a changing environment from the generic rollout methods, which we demonstrate through examples in which some robots malfunction. 