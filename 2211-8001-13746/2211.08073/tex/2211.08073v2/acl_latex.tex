% This must be in the first 5 lines to tell arXiv to use pdfLaTeX, which is strongly recommended.
\pdfoutput=1
% In particular, the hyperref package requires pdfLaTeX in order to break URLs across lines.

\documentclass[11pt]{article}

% Remove the "review" option to generate the final version.
\usepackage{authblk}
\usepackage[]{acl}
\renewcommand*{\Affilfont}{\normalsize\normalfont}
\renewcommand*{\Authfont}{\bfseries}

\usepackage{graphicx}
%\usepackage{caption}
%\usepackage{subcaption}
\usepackage{subfigure} 
\usepackage{amsmath}
\usepackage{adjustbox} % 表格的大小调整
\usepackage{amsthm}
\usepackage{booktabs}
\usepackage{algorithm}
\usepackage{algpseudocode}
%\usepackage{algorithmic}
\urlstyle{same}
\usepackage{amssymb}
\usepackage{multirow}
\usepackage{longtable}
\usepackage{comment}
% \usepackage{subcaption} 
\usepackage[normalem]{ulem}
\usepackage{csquotes}
% \usepackage[]{acl}

% Standard package includes
\usepackage{times}
\usepackage{latexsym}

% For proper rendering and hyphenation of words containing Latin characters (including in bib files)
\usepackage[T1]{fontenc}
% For Vietnamese characters
% \usepackage[T5]{fontenc}
% See https://www.latex-project.org/help/documentation/encguide.pdf for other character sets

% This assumes your files are encoded as UTF8
\usepackage[utf8]{inputenc}


% This is not strictly necessary, and may be commented out,
% but it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}
\usepackage{xspace}
% \usepackage{graphicx}
\newcommand{\ubold}[1]{\fontseries{b}\selectfont#1}
\newcommand{\shuibai}[1]{\textcolor{blue}{[Shuibai: #1]}} 
\setlength{\belowcaptionskip}{-4pt}
\usepackage{authblk}

% Jindong's comments
\usepackage{todonotes}
\usepackage{xcolor}
\newcommand{\wjdd}[1]{\todo[linecolor=cyan,backgroundcolor=cyan!25,bordercolor=cyan,size=\scriptsize]{(WJD): #1}}
\newcommand{\wjd}[1]{{\color{cyan}{[(WJD): #1]}}}
\newcommand{\wyd}[1]{{\color{red}{[(WYD): #1]}}}
\newcommand{\yly}[1]{{\color{orange}{[(YLY): #1]}}}
\newcommand{\lbq}[1]{{\color{blue}{[(LBQ): #1]}}}
\newcommand{\method}{GLUE-X\xspace}

% If the title and author information does not fit in the area allocated, uncomment the following
%
%\setlength\titlebox{<5cm>}
%
% and set <dim> to something 5cm or larger.
\title{GLUE-X: Evaluating Natural Language Understanding Models from an Out-of-distribution Generalization Perspective}

% \title{GLUE-X: Evaluating OOD Robustness of Natural Language Understanding Tasks}


%\title{RDL: Human-in-the-loop Rationale-Centric Framework for Static Learning}

% Author information can be set in various styles:
% For several authors from the same institution:
% \author{Author 1 \and Author2 \and Author n \\
%         Address line \\ Address line \\ Address line}
% if the names do not fit well on one line use
%         Author 1 \\ {\bf Author 2} \\ ... \\ {\bf Author n} \\
% For authors from different institutions:
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \And  ... \And
%         Author n \\ Address line \\ ... \\ Address line}
% To start a seperate ``row'' of authors use \AND, as in
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \AND
%         Author 2 \\ Address line \\ ... \\ Address line \And
%         Author 3 \\ Address line \\ ... \\ Address line}

% \author{First Author \\
%   Affiliation / Address line 1 \\
%   Affiliation / Address line 2 \\
%   Affiliation / Address line 3 \\
%   \texttt{email@domain} \\\And
%   Second Author \\
%   Affiliation / Address line 1 \\
%   Affiliation / Address line 2 \\
%   Affiliation / Address line 3 \\
%   \texttt{email@domain} \\}

%\author{
    % Authors
%Anonymous
%    
%}



 
\author{
    % Authors
    \textbf{Linyi Yang}$^{1,2}$\thanks{\ \ Equal contribution. Random order.}, \textbf{Shuibai Zhang}$^{1,4,*}$\thanks{\ \ Work done at Westlake University as an intern.}, \textbf{Libo Qin}$^{1}$, \textbf{Yafu Li}$^{1}$, \textbf{Yidong Wang}$^{1}$, \textbf{Hanmeng Liu}$^{1}$, \\
    \textbf{Jindong Wang}$^{3}$, \textbf{Xing Xie}$^{3}$, \textbf{Yue Zhang}$^{1,2}$
    % \texttt{\{yanglinyi, zhangyue\}@westlake.edu.cn}
}
\affil{$^{1}$ School of Engineering, Westlake University\\
    $^{2}$ Institute of Advanced Technology, Westlake Institute for Advanced Study\\
    $^{3}$ Microsoft Research Asia (MSRA)\\
    $^{4}$ University of Electronic Science and Technology of China\\
    %\texttt{\{yanglinyi, zhangyue\}@westlake.edu.cn}
}


\begin{document}

\maketitle
% \authornote{Corresponding Author}


\begin{abstract}
Pre-trained language models (PLMs) are known to improve the generalization performance of natural language understanding models by leveraging large amounts of data during the pre-training phase. However, the out-of-distribution (OOD) generalization problem remains a challenge in many NLP tasks, limiting the real-world deployment of these methods. This paper presents the first attempt at creating a unified benchmark named \method for evaluating OOD robustness in NLP models, highlighting the importance of OOD robustness and providing insights on how to measure the robustness of a model and how to improve it. The benchmark includes 13 publicly available datasets for OOD testing, and evaluations are conducted on 8 classic NLP tasks over 19 popularly used PLMs. Our findings confirm the need for improved OOD accuracy in NLP tasks, as significant performance degradation was observed in all settings compared to in-distribution (ID) accuracy.\looseness=-1
\end{abstract}

\section{Introduction}

Pre-trained Language Models (PLMs) \cite{qiu2020pre,bommasani2021opportunities} have achieved competitive performance across standard NLP benchmarks \cite{blasi22acl}, such as GLUE \cite{wang2018glue} and SuperGLUE \cite{wang2019superglue}. However, recent studies \cite{gururangan2018annotation,ribeiro2019red,kaushik2020learning,ribeiro2020beyond,ruder2021challenges} show concerns that models are yet not close to achieving proper natural language understanding, essential questions being raised about their robustness \cite{srivastava2020robustness,wang2021measure} and underlying sensitivity to systematic biases \cite{probing2019acl,sagawa2020investigation}.
Such issues manifest in the performance decay, especially for out-of-distribution (OOD) generalization when the test distribution differs from training \cite{zheng2020out,arora2021types,malinin2021shifts}. 

OOD generalization has been systemically studied for Computer vision (CV) and artificial general intelligence (AGI) \cite{koh2021wilds,srivastava2022beyond,ibrahim2022robustness,yang2022openood}, for which large evaluation datasets are available. While sharing the same aspirational goal, existing evaluations \cite{gardner2020evaluating} and methods~\cite{hendrycks2020pretrained,bommasani2021opportunities} for OOD generalization in NLP contain only one \cite{yang-etal-2021-exploring,wang2021robustness,howard2022neurocounterfactuals} or a few tasks~\cite{kaushik2020learning,srivastava2020robustness,wu2021polyjuice}. Evidence suggests that controlling spurious correlations between input features and labels that do not causally affect a task's label can improve a model's robustness towards distribution shifts \cite{shen2021enhancing}. However, these simple evaluations of natural language understanding systems on a small number of tasks do not adequately capture the limitations of existing models, resulting in inflated test accuracy \cite{tu2020empirical,ribeiro2020beyond}. It remains a gap in evaluating models in a unified way by executing a range of text classification tasks.

To facilitate research in this direction, we introduce the \method benchmark for evaluating the out-of-distribution performance of PLMs. \method expands upon previous multi-task benchmarks \cite{fewnlu,xu2020clue,xu2021fewclue} by including test data from multiple domains, allowing cross-distribution evaluations. It also enables the analysis of two main factors affecting the cross-domain generalization performance, namely the pre-trained language model (e.g., architecture, hyper-parameters, etc.) and different training strategies (e.g., fine-tuning, prompt-tuning \cite{chen2022adaprompt}, probing \cite{kumar2022finetuning}, and domain-generalization training \cite{wang2022generalizing}).

Using \method, we evaluate the performance of \emph{19} pre-trained language models on a range of natural language understanding tasks using popular metrics in a unified setting and under the same experimental conditions to investigate the gaps in common practices. In addition, we consider \emph{3} tuning strategies designed for improving single-source domain generalization: linear probing, fine-tuning, and the linear probing then fine-tuning method (LP-FT) \cite{kumar2022finetuning}. Finally, we analyze the internal causes of OOD robustness at the feature level by measuring the rationale overlap between human and model predictions \cite{lei2016rationalizing}.
%\wjd{A schematic diagram or figure to overview what will be covered is good to show our contributions.}

Our results demonstrate that the average accuracy of PLMs on cross-domain evaluations falls significantly short of human performance, even for the highest-performing model (81.3\% versus 69.7\%). In contrast to the GLUE dataset, where over 20 single-model results outperform human baselines, none of the backbones included in \method is able to surpass human performance under the same evaluation setting. These findings suggest that the importance of \method is no less than GLUE. Additionally, we provide evidence that the superior performance of PLMs on GLUE may be a superficial illusion, whereas \method brings a unique opportunity to assess models' OOD generalization ability more accurately.

In addition, detailed analysis shows that (1) no one backbone can significantly outperform the others across all tasks, which is consistent with the conclusion \cite{wenzel2022assaying} in the computer vision; (2) surprisingly, the influence of model architectures is more significant than the model parameters towards the OOD robustness; (3) the ID and OOD performance holds a linear correlation in most cases for text classifications; (4) in terms of the tuning strategy, we show that linear probing and then fine-tuning can slightly improve the OOD performance compared to the standard fine-tuning. 

To our knowledge, we are the \emph{first} to systemically evaluate natural language understanding systems for cross-distribution generalization on genuine data, along with experiments using different fine-tuning strategies. More importantly, we make datasets of cross-domain evaluations for all typical text classification tasks by creating the testing data from scratch, which allows us to report OOD results under the same experimental conditions. We open-source the codebase and datasets for unified and consistent evaluation in different ways \footnote{\url{https://github.com/YangLinyi/GLUE-X}}. 


\section{Related Work}

\input{sec-related.tex}

\input{tables/tb-dataset.tex}

\section{Data and Settings}

The goal of \method is to provide an easy-to-use and robust benchmark to a broad range of out-of-distribution generalization tasks for evaluating natural language understanding systems. We articulate the following tasks and datasets in \method.

\subsection{Overview of \method}

\textbf{Tasks.} As a benchmark styled after GLUE \cite{wang2018glue}, we consider following eight tasks in \method: Sentiment Analysis (\emph{SST-2}), Natural Language Inference (\emph{MNLI, QNLI, RTE}), Sentence Pair Similarity (\emph{MRPC, QQP}), Textual Similarity (\emph{STS-B}) and Linguistic Acceptability (\emph{CoLA}). \footnote{The WNLI task is not included in \method since there is no sufficient in-domain data to construct OOD tests~\cite{wang2022pre,xlnet}.}

\noindent\textbf{Datasets.} \method follows the same in-domain training data and evaluation metrics to GLUE \cite{wang2018glue}. To construct the out-of-domain test, we adopt the popular datasets extracted from different domains while keeping the same prediction labels as the original tasks in GLUE. We show detailed data statistics in Table~\ref{tab:data_statistic}. 

\subsection{Dataset Curation}

We aim to conduct the zero-shot transfer from the in-domain dataset from GLUE to unseen OOD datasets when constructing \method. In particular, we construct test sets for each task under the requirement that share the same label types. To this end, \method contains 13 OOD datasets in total, including publicly available datasets (Amazon, HANs, etc) and newly collected datasets (Grammer Test). 
In particular, we select the popular datasets while sharing the same label types with the in-domain data involved in GLUE as OOD test data for different tasks, including sentiment analysis -- IMDB \cite{maas-etal-2011-learning}, Yelp \cite{zhang2015character}, and Amazon \cite{ni-etal-2019-justifying}, linguistic acceptability -- Grammer Test --, textual similarity -- SICK \cite{zhang2018multi} --, NLI -- MNLI-Mismatched \cite{williams2017broad}, SNLI \cite{bowman2015large}, and SICK \cite{zhang2018multi} --, Textual Entailment -- RTE, MRPC, QQP and HANs \cite{bentivogli2009fifth,dolan-2005-automatically,wang2017bilateral,mccoy2019right}. Regarding the QNLI task, we convert instances from NewsQA \cite{newsqa} to the consistent data format of QNLI for conducting the OOD evaluation. The description of the self-collected dataset, Grammar Test, can be found in Appendix \ref{sec:appendixA}.

Note that SICK contains multiple labels, including textual similarity, also used as an OOD test set of the textual similarity task. We rounded floating number labels of textual similarity to integers from 0 to 4, converting it into a five-class dataset to align with other classification tasks in GLUE-X. In addition, RTE, MRPC and QQP can be OOD datasets of each other when implementing the textual entailment task. 

\subsection{Metrics}

Previous benchmarks, such as GLUE and SuperGLUE, seek to evaluate NLU models by simply average scores of all tasks. In \method, we consider both the macro average and weighted average score over eight tasks. Following GLUE, we first average metrics to get a task score for tasks with multiple metrics.

In addition to the overall measures of OOD performance, we are also interested in better understanding the features learned by models. Thus, in the following subsection, we consider using rationale marking to measure the gap between models' rationale with humans, indicating models' trust to some extent.
    
In terms of rankings, we adopt Friedman rank \cite{friedman1940comparison} to compare the performance of different backbones in various tasks fairly:
\begin{equation*}
\operatorname{rank}_{F}  = \frac{1}{n} \sum_{i=1}^{n} \operatorname{rank}_i,
\end{equation*}
where $n$ is the number of tasks (e.g., $n=8$ in Table~\ref{tab:detail}) and $\operatorname{rank}_i$ is the rank of the performance in the $i$-th setting. We report both the average ranking and Friedman rank in our results.

\subsection{Post-hoc Analysis}

In addition to the quantitative analysis, to further provide an in-depth analysis of results reported by \method, we choose two tasks, including sentiment analysis and nature language inference, to provide the post-hoc analysis \cite{lei2016rationalizing}. In particular, we adopt the sensitivity of contextual decomposition technique from \cite{jin2019towards}, which removed part of inputs from the sequence text to evaluate a model’s sensitivity to them, thereby allowing for the identification of important features. The output is the overlap between rationales marked by models and humans, which somehow represents the trust of models \cite{jacovi2020towards}. 

Given a phrase \emph{p} starting with the negative limitations in the \emph{k-th} document \emph{D(k)}, we sample the documents which contain the same phrase \emph{p} to alleviate the influence by chance when there are multiple shreds of evidence saturating the prediction. The window size of phrase \emph{p} is limited to 3 (no phrase containing more than 3 tokens will be considered). Taking the sentiment analysis task as an example, in the source \enquote{This movie was so unbelievably bad}, if we only remove the non-causal word \enquote{movie}, the prediction would not be changed so much for a robust model that not be affected by spurious patterns. 


The importance is computed as:
\begin{equation*}
\small
\phi(\mathbf{p}, \widehat{\mathcal{D}^{(k)}})=\mathbb{E}_{\widehat{\mathcal{D}^{(\beta)}}}\left[l\left(\widehat{\mathcal{D}^{(\beta)}}; \widehat{\mathcal{D}}\right)-l\left(\widehat{\mathcal{D}^{(\beta)}} \backslash \mathbf{p} ; \widehat{\mathcal{D}}\right)\right],
\end{equation*}
where \(\mathcal{D}^{(\beta)}\) denotes the resulting text after masking out a single token (phrase) starting with the negative pronoun in the length of \(N\) surrounding the phrase \(\mathbf{p}\). we use \(l\left(\widehat{\mathcal{D}^{(\beta)}} \backslash \mathbf{p} ; \widehat{\mathcal{D}}\right)\) to represent the model prediction logits after replacing the masked-out context. \(\backslash \mathbf{p}\) indicates the operation of masking out the phrase \(p\) in a input file sampling from the dataset \(\mathcal{D}\). 

\subsection{Models and Training Strategies}

\noindent\textbf{Models.} To ensure that our results are relevant for researchers and practitioners, we consider both top-performing backbones and cost-efficient methods: \emph{Discriminative Models} -- BERT-base, BERT-large \cite{bert}, RoBERTa-base, RoBERTa-large \cite{roberta}, XLNet-base, XLNet-large \cite{xlnet}; \emph{Generative Models} -- BART-base, BART-large \cite{bart}, T5-small, T5-base, T5-large \cite{t5}, GPT2, GPT2-medium, GPT2-large \cite{gpt2}; \emph{Cost-Efficient Models} -- ALBERT-base \cite{albert}, and DistilBERT-base \cite{distilbert}. We follow the official implementations of several pre-trained language models from Huggingface\footnote{\url{https://huggingface.co/models}} to reproduce results on GLUE using the validation set and test these models on \method.

\noindent\textbf{Fine-tuning Strategies.} We investigate the efficacy of different fine-tuning strategies for OOD generalization. In particular, we consider three paradigms: standard fine-tuning, fine-tuning only the head (linear-probing), and linear-probing then fine-tuning. After fine-tuning models using the in-domain training data for each task, we evaluate the performance using the in- and out-of-domain test data, respectively. We record the training cost in GLUE and \method. Our experiments use 50 NVIDIA Tesla V100 GPU cards and 8 NVIDIA A100 GPU cards. In total, we spend 10,000+ GPU hours based on the estimation with a single V100 card. The detailed training cost and inference speed per task are shown in Table \ref{tab:cost}.

\input{tables/tb-gpucost.tex}

\section{Experiments}

In the following, we explore the facets of OOD generalization in NLP, highlighting discrepancies to prior work and discussing their implications.

\subsection{Human Annotations}
We employ human annotators to give predictions on OOD datasets and identify rationales, detailed as follows.

\noindent\textbf{Predictions.} 
We use a crowd-sourcing company to recruit editors and annotators to give predictions about 13 OOD datasets. To fairly compare human performance with models, we simulate the models' OOD testing process during the manual annotation. Specifically, annotators are given instructions and examples from the in-domain dataset that gently guide them to annotate. Then they are asked to label instances from unseen OOD datasets, usually collected from other domains, while not changing the label types. 

We employ multiple labelers to annotate the same data point during the annotation to ensure the high quality of the crowdsourcing work. In particular, we employ ten people to annotate the SICK dataset, which is consistent with the original data collection process \cite{zhang2018multi}. For other datasets, we employ two annotators for labeling the same instance. After the trial phase of data annotation, we set the Inter-Annotator Agreement (IAA) score threshold for each task depending on the difficulty level. Finally, the average Inter-Annotator Agreement (IAA) over 13 OOD datasets is \textbf{0.864}, indicating acceptable agreement. 

\noindent\textbf{Rationale Marking.} Following \citet{kaushik2020learning} and \citet{yang-etal-2021-exploring}, we use the extractive explanations for marking rationales that support classification decisions. Inspired by \citet{kaushik2021learning} and \citet{lertvittayakumjorn2021explanation}, we leverage the rationale marking annotated by humans to compare with rationale selected by models on sentiment analysis and natural language inference (NLI) tasks. We ask two labelers to annotate sampled instances from IMDB, Yelp, and Amazon datasets for the sentiment analysis task. At the outset, annotators were given instructions and examples that gently guided them to annotate rationales. Only adjectives, adverbs, nouns, and verbs were considered rationales. Besides, rationales were required to carry complete semantic information. After the trial phase, we sampled 2,000 instances for each dataset randomly. Using Fleiss’s Kappa \cite{fleiss1973equivalence}, the IAA for IMDB, Yelp and Amazon are 0.874, 0.871, and 0.840, respectively. For NLI, we directly leverage the explanation datasets, e-SNLI, provided by \citet{camburu2018snli} to assert models' trust\footnote{\url{https://github.com/OanaMariaCamburu/e-SNLI}}. e-SNLI contains both free-text and extractive explanations for supporting decisions, while we only consider the rationale marking to maintain the consistency of evaluations.

\input{tables/tb-overallper.tex}
\input{tables/tb-detail.tex}

\subsection{Prediction Results}  
\textbf{Overall Performance on \method.} We report the average score of different models representing the overall performance in Table \ref{tab:overall_per}, sorted in descending order by the average OOD performance. In addition to the overall performance, we provide the Friedman Rank for both in-domain and out-of-domain results. From Table \ref{tab:overall_per}, we observe that all pre-trained models involved in \method show significant performance decay under the OOD test compared to the ID performance (\textbf{18.30\%} decay in average). 

\noindent\textbf{Model-level Analysis.} On the model level, we observe that ELECTRA-large achieves the best performance for both ID (\textbf{89.38\%}) and OOD (\textbf{69.68\%}) tests. In addition, the results suggest that there is no significant difference in the OOD robustness between generative models and discriminative models for text classification tasks. Not surprisingly, lightweight models, including BERT-base, GPT-2, and DistilBERT-base, are in the bottom three on \method with the lowest OOD performance. In terms of the Friedman rank, we observe that the variance of the OOD rank is slightly lower than the ID rank, which hints that the uncertainty of performance has been decreased on \method by using a large amount of the test data. Finally, although the robustness has not changed so much from the absolute average OOD score, we show an improvement in rankings of small-size models, including ELECTRA-small and T5-small.


% \input{tables/tb-rational.tex}

\input{tables/tb-decayratio.tex}

\begin{figure*}[ht]
\centering
\subfigure[COLA]{
\begin{minipage}[t]{0.32\linewidth}
\centering
\includegraphics[width=\textwidth]{plots/ID_OOD/cola.pdf}
%\caption{fig1}
\end{minipage}%
}%
\subfigure[MNLI]{
\begin{minipage}[t]{0.32\linewidth}
\centering
\includegraphics[width=\textwidth]{plots/ID_OOD/mnli.pdf}
%\caption{fig2}
\end{minipage}%
}
\subfigure[MRPC]{
\begin{minipage}[t]{0.32\linewidth}
\centering
\includegraphics[width=\textwidth]{plots/ID_OOD/mrpc.pdf}
%\caption{fig2}
\end{minipage}%
}
\caption{The scatter figures illustrate the correlation between ID and OOD performance for different tasks.}
\label{fig:scatter}
\end{figure*}

\begin{figure*}[t]
\centering
\small
{% 
\includegraphics[width=.4\textwidth]{plots/Tuning/distilbert-base-uncased.pdf} 
}
\centering 
\small
{% 
\includegraphics[width=.4\textwidth]{plots/Tuning/roberta-base.pdf} 
}
\caption{The influence of different tuning strategies on the task of MNLI, including Linear-probing, Fine-tuning, and Linear-probing then Fine-tuning (LP-FT). The results of the left and right figures are based on DistillBERT and RoBERTa-base, respectively.} 
\label{fig:tuning}
\end{figure*}

\noindent\textbf{The Performance of Compressed Models.} The results of \method suggest that OOD generalization still faces fundamental challenges, especially for lightweight models. For example, we find that compressed models (e.g., DistilBERT-base) show relatively low performance compared to others. Differently, the OOD performance of ALBERT-base (11M parameters) is significantly higher than DistilBERT-base (\textbf{62.06\% vs. 58.94\%}), even better than several moderate-sized models (BERT-large, GPT2-medium, and XLNet-base). 

%We measure the MMD distance between the In-domain and Out-of-domain dataset and show details in Appendix \ref{}. We find that the performance decay holds a statistically significant correlation with the distance of distribution shifts. In particular, the generalization for the CoLA dataset is the most challenging task for models since the OOD dataset we selected holds the biggest difference with CoLA. In contrast, models tend to perform better on the relatively easy dataset, such as sentiment analysis (SST-2). As shown in Table \ref{tab:detail}, we show that the best performing model, RoBERTa, can even achieve 94.98\% accuracy for the zero-shot domain generalization of sentiment analysis. Besides, the generative model, such as GPT2, shows its advantage in the task of QQP (60.80\% accuracy achieved by GPT2-large). Note that we do not evaluate the WNLI task since there is no sufficient in-domain data. Also, the OOD dataset of CoLA is newly collected in this work.

%\begin{figure*}[t]
%\centering
%\small
%\includegraphics[width=.75\textwidth]{plots/exp/encoder_bar.png}
% \caption{Encoder-only Average Performance.}\label{fig:encoder_bar}
%\end{figure*}

%\begin{figure*}[t]
%\centering
%\small
%\includegraphics[width=.75\textwidth]{plots/exp/decoder_bar.png}
%\caption{Decoder-only Average Performance.}\label{fig:decoder_bar}
%\end{figure*}

% Please add the following required packages to your document preamble:
% \usepackage{multirow}

% Please add the following required packages to your document preamble:
% \usepackage{multirow}


\subsection{Discussion}

\noindent\textbf{Human vs. Model.} By comparing human and model performance in Table \ref{tab:detail}, we find that the average performance decay between in- and out-of-domain tests of humans (from 87.10\% to 81.31\%) is significantly lower than models, even for the best-performing model with the lowest performance decay (\textbf{5.79\% vs. 21.87\%}). In terms of the absolute average score of OOD performance, the human baseline is also much higher than the models, with at least an 11.63\% increase (81.31\% vs. 69.68\%). Such a large performance gap indicates that PLMs cannot achieve competitive results with humans on \method. More specifically, the human baseline outperforms the state-of-the-art results on five of eight tasks. It is noteworthy that we control OOD evaluations of humans in the same experimental setting with models by testing on unseen datasets.

\noindent\textbf{OOD Robustness.} As shown in Table \ref{tab:detail}, we suggest that there is no silver bullet towards the OOD robustness, given that no single model can consistently outperform others over all tasks on \method. For example, RoBERTa-large can only achieve the best performance on two of eight tasks (SST-2 and CoLA). Besides, we also observe that the distribution shift between the ID and OOD datasets largely influences the OOD generalization results. In particular, the performance decay on OOD test is exacerbated by the increase of distribution shifts, as shown in Appendix \ref{sec:appendixC}. We also find that the generalization for the CoLA dataset is the most challenging task for models since the test set holds the biggest difference with training data. In contrast, models tend to perform better on the relatively easy dataset, such as sentiment analysis (SST-2). For example, the best performing model RoBERT-large can achieve a \textbf{93.48\%} accuracy on SST-2 yet only a \textbf{25.90\%} Matthew's Corr on CoLA.

\noindent\textbf{Model Architectures vs. Parameter Size} Table \ref{tab:decay_ratio} demonstrates the overall performance sorted by the decrease ratio representing the robustness to some extent. The large-sized model, such as T5-large, and RoBERTa-large, can significantly surpass the corresponding base-sized models (T5: \textbf{67.62\% vs. 61.55\%}; RoBERTa: \textbf{67.03\% vs. 64.46\%}). However, we also observe that the influence of parameter size is somehow not as important as the model types for classification tasks. For instance, the performance achieved by ELECTRA-base (109M parameters) is higher than GPT2-large (774M parameters) in \textbf{64.15\% vs. 62.76\%}. Meanwhile, the OOD performance of the T5-base model is significantly higher than GPT2-large and BERT-large. 

Empirical evidence from Table \ref{tab:decay_ratio} further shows that model types could be more influential than the parameter size towards the OOD performance. Specifically, as shown in Table \ref{tab:decay_ratio}, results of the same architecture with different parameters are closer to the results of similar parameter-size models based on different architectures. For instance, the decreased ratio of T5 architectures pre-training with different parameter sizes (61M, 223M, and 737M) are ranked near each other, similar to BERT (base vs. large). We identify designing model architectures and training methods as one of the future directions for improving OOD robustness.

\noindent\textbf{Robustness vs. Trust.} The results of the rationale overlap between models and humans are shown in Table \ref{tab:decay_ratio}, indicating the trust measurement. Inspired by the previous work \cite{shen2022shortest}, Table \ref{tab:decay_ratio} aims to answer the following question: Does OOD robustness consistently correspond with the rationale overlap?

We show the rationale overlap results on three sentiment analysis datasets. As shown in Table \ref{tab:decay_ratio}, somehow surprisingly, we find that the best-performing discriminative model on \method -- RoBERTa-large (see Table \ref{tab:overall_per}) yet to achieve a high rationale overlap between humans and models (ranked in 19). It is noteworthy that small-sized models can achieve relatively higher rationale overlaps than large-sized models, which is generally consistent with the results reported by the previous work \cite{deyoung2020eraser}. For instance, ELECTRA-small achieves the highest F1 score with only 13.48M parameters.

\noindent\textbf{ID vs. OOD Performance.} We show the pearson correlation of four tasks between the in- and out-of-domain results in Figure \ref{fig:scatter} (the full results can be found at Appendix \ref{sec:appendixD}. Unsurprisingly, we observe that the in-domain performance is usually higher than the out-of-domain performance. Specifically, we find that the OOD performance is extremely lower than the ID performance in the task of COLA. In contrast, the gap between ID and OOD performance based on SST-2 and MNLI is relatively lower than others. We suppose this is partially influenced by the distribution shift between the in- and out-of-domain datasets. 

Regarding the type of pre-trained models, we show that discriminative models usually perform a stronger linear correlation when compared to generative models. From the task perspective, we observe that datasets largely influence the correlation between ID and OOD. For instance, ID and OOD performance are inversely correlated on MRPC yet almost correlated on other tasks. It hints that the inverse correlation is possible for the specific task when the size of test samples is limited. 

\noindent\textbf{The Efficacy of LP-FT.} Using MNLI as an example, we compare the results achieved by DistilBERT-base and RoBERTa-base using three different training strategies in Figure \ref{fig:tuning}. As found by the previous work \cite{kumar2022finetuning}, fine-tuning can do worse than linear probing in the presence of a large distribution shift in CV. However, as shown in Figure \ref{fig:tuning}, we find that the linear probing method performs extremely struggle in NLP in terms of the low accuracy for both ID and OOD tests, which is insistent with the conclusion in CV. While the LP-FT can be relatively helpful for improving the OOD robustness of NLP models in terms of the slight performance improvement compared to the standard fine-tuning method. We expect to see more methodologies toward domain generalizations can be designed for improving the OOD robustness, especially for NLP tasks.

\section{Conclusion}

We constructed \method, an OOD robustness benchmark for natural language understanding tasks that aims to enable fair evaluation over multiple datasets from multiple domains in a consistent setting. With \method, we evaluate 19 pre-trained models on 8 classification tasks. Besides, we provide the analysis using 3 different tuning strategies and post-hoc analysis for gaining internal causes for the OOD robustness. We conclude that (1) current PLMs still have much room to improve towards the OOD robustness; (2) although the ID and OOD performance holds a linear correlation in most cases, such a correlation is primarily related to the selection of OOD datasets; (3) introducing novel architectures brings more performance benefit, especially for the OOD performance. We aim to make \method a continuously maintained project.


\section{Limitation}

Our primary focus is on the OOD robustness of text classification tasks. However, there are other NLP tasks that the community should not ignore. \method currently does not include language generation tasks such as machine translation, summarization, and dialogue. Moreover, extending the current \method to more real-world datasets from different domains is of great importance.



\bibliography{custom}
\bibliographystyle{acl_natbib}

\newpage
\appendix

%%%%%%%%%%
% \begin{table*}[t]
% \centering
% \small
% \begin{tabular}{lllllllllll}
% \hline
% \multirow{2}{*}{Model} & \multicolumn{1}{l}{SST-2} & \multicolumn{1}{l}{MNLI} & \multicolumn{1}{l}{RTE} & \multicolumn{1}{l}{MRPC} & \multicolumn{1}{l}{QQP} & \multicolumn{1}{l}{STS-B} & \multicolumn{1}{l}{CoLA} & \multicolumn{1}{l}{QNLI} & \multicolumn{1}{l}{Avg} & Avg       \\
%                        & \multicolumn{1}{l}{OOD}   & \multicolumn{1}{l}{OOD}  & \multicolumn{1}{l}{OOD} & \multicolumn{1}{l}{OOD}  & \multicolumn{1}{l}{OOD} & \multicolumn{1}{l}{OOD}   & \multicolumn{1}{l}{OOD}  & \multicolumn{1}{l}{OOD} & $\Delta$↓ \\ \hline
% \multicolumn{10}{l}{State-of-the-art Pre-trained Language Models}                                                                                                                                                                                         \\ \hline
%%%%%%%%%%%

\section{Data Collection} \label{sec:appendixA}
We derive our COLA-OOD dataset from the Public High School English Exams. The original multi-choice fill-in tests are converted into COLA-style, with correct answers as positive examples and incorrect answers as negative examples. We collect the data from publicly available internet resources, and the original open-access materials can be found from \url{https://www.koolearn.com/shiti}.

\section{Hyper-Parameters} \label{sec:appendixB}
We performed grid search for each task, and kept the best-performing checkpoint in ID datasets and tested their performance on their correspondnig OOD datasets. The hyperparameters used by these weights can be seen in Table \ref{tab:Hyperparameters}.

\section{Domain Distributions} \label{sec:appendixC}
We evaluate distribution shifts between different datasets in terms of Maximum Mean Discrepancy(MMD) and Word Overlap Rate. MMD distance focuses on the semantic distribution shift between datasets, while Word Overlap Rate pays more attention to superficial similarity.


\begin{figure*}[t]
\centering

\subfigure[COLA]{
\begin{minipage}[t]{0.25\linewidth}
\centering
\includegraphics[width=.9\textwidth]{plots/overlap_figures/cola.pdf}
%\caption{fig1}
\end{minipage}%
}%
\subfigure[MNLI]{
\begin{minipage}[t]{0.25\linewidth}
\centering
\includegraphics[width=.9\textwidth]{plots/overlap_figures/mnli.pdf}
%\caption{fig2}
\end{minipage}%
}%
\subfigure[MRPC]{
\begin{minipage}[t]{0.25\linewidth}
\centering
\includegraphics[width=.9\textwidth]{plots/overlap_figures/mrpc.pdf}
%\caption{fig1}
\end{minipage}%
}%
\subfigure[QQP]{
\begin{minipage}[t]{0.25\linewidth}
\centering
\includegraphics[width=.9\textwidth]{plots/overlap_figures/qqp.pdf}
%\caption{fig2}
\end{minipage}%
}%
                 %这个回车键很重要 \quad也可以
                 
\subfigure[RTE]{
\begin{minipage}[t]{0.25\linewidth}
\centering
\includegraphics[width=.9\textwidth]{plots/overlap_figures/rte.pdf}
%\caption{fig2}
\end{minipage}
}%
\subfigure[SST-2]{
\begin{minipage}[t]{0.25\linewidth}
\centering
\includegraphics[width=.9\textwidth]{plots/overlap_figures/sst2.pdf}
%\caption{fig2}
\end{minipage}
}%
\subfigure[STSB]{
\begin{minipage}[t]{0.25\linewidth}
\centering
\includegraphics[width=.9\textwidth]{plots/overlap_figures/stsb.pdf}
%\caption{fig2}
\end{minipage}
}%
\subfigure[QNLI]{
\begin{minipage}[t]{0.25\linewidth}
\centering
\includegraphics[width=.9\textwidth]{plots/overlap_figures/qnli.pdf}
%\caption{fig2}
\end{minipage}
}%
\centering
\caption{The word-level overlap between the training set and test set for each task.}
\label{fig:Word_Overlap}
\end{figure*}

\begin{figure*}[t]
\centering

\subfigure[COLA]{
\begin{minipage}[t]{0.25\linewidth}
\centering
\includegraphics[width=.9\textwidth]{plots/MMD_scores_figures/cola.pdf}
%\caption{fig1}
\end{minipage}%
}%
\subfigure[MNLI]{
\begin{minipage}[t]{0.25\linewidth}
\centering
\includegraphics[width=.9\textwidth]{plots/MMD_scores_figures/mnli.pdf}
%\caption{fig2}
\end{minipage}%
}%
\subfigure[MRPC]{
\begin{minipage}[t]{0.25\linewidth}
\centering
\includegraphics[width=.9\textwidth]{plots/MMD_scores_figures/mrpc.pdf}
%\caption{fig1}
\end{minipage}%
}%
\subfigure[QQP]{
\begin{minipage}[t]{0.25\linewidth}
\centering
\includegraphics[width=.9\textwidth]{plots/MMD_scores_figures/qqp.pdf}
%\caption{fig2}
\end{minipage}%
}%
                 %这个回车键很重要 \quad也可以
                 
\subfigure[RTE]{
\begin{minipage}[t]{0.25\linewidth}
\centering
\includegraphics[width=.9\textwidth]{plots/MMD_scores_figures/rte.pdf}
%\caption{fig2}
\end{minipage}
}%
\subfigure[SST-2]{
\begin{minipage}[t]{0.25\linewidth}
\centering
\includegraphics[width=.9\textwidth]{plots/MMD_scores_figures/sst2.pdf}
%\caption{fig2}
\end{minipage}
}%
\subfigure[STSB]{
\begin{minipage}[t]{0.25\linewidth}
\centering
\includegraphics[width=.9\textwidth]{plots/MMD_scores_figures/stsb.pdf}
%\caption{fig2}
\end{minipage}
}%
\subfigure[QNLI]{
\begin{minipage}[t]{0.25\linewidth}
\centering
\includegraphics[width=.9\textwidth]{plots/MMD_scores_figures/qnli.pdf}
%\caption{fig2}
\end{minipage}
}%
\centering
\caption{The MMD Scores between the training set and test set for each task. Lower MMD score means the higher correlation between datasets.}
\label{fig:MMD_scores}
\end{figure*}

\subsection{Word Overlap}

The similarity between datasets of In-distribution datasets and Out-of-distribution datasets are shown in Figure \ref{fig:Word_Overlap}.


\subsection{MMD Distance}
The MMD distance between ID and OOD datasets are shown in Figure \ref{fig:MMD_scores} for each task including in \method. When computing the MMD distance of two datasets, the same number of sentences are sampled in both datasets and fed into a pre-trained language model(e.g. RoBERTA-base) to extract their semantic features, which are used to compute MMD. It is hard to calculate MMD by all samples in the datasets due to the limitation of computing resource. As a result, we sample multiple times to get an average MMD sample score to estimate MMD distance of two datasets. The calculation of MMD is shown as follows:

\begin{equation}
\begin{aligned}
&\operatorname{MMD}^2[\mathcal{F}, X, Y]=\frac{1}{m(m-1)} \sum_{i \neq j}^m k\left(x_i, x_j\right) \\
&+\frac{1}{n(n-1)} \sum_{i \neq j}^n k\left(y_i, y_j\right)-\frac{2}{m n} \sum_{i, j=1}^{m, n} k\left(x_i, y_j\right)
\end{aligned}
\end{equation}

$\mathcal{F}$ is a MMD function class, \emph{i} and \emph{j} represents the batch of instances sampled from different distributions. \emph{m} and \emph{n} are the size of \emph{i} and \emph{j}.


\begin{figure*}[ht]
\centering
\small
{% 
\includegraphics[width=.49\textwidth]{plots/exp/encoder_bar.pdf}
}
\centering 
\small
{% 
\includegraphics[width=.49\textwidth] {plots/exp/decoder_bar.pdf} \label{fig:encoder_bar}
}
\caption{The comparison of ID and OOD performance evaluated on GLUE-X. The left and right sub-figures show the results of discriminative models and generative models, respectively.} 
\label{fig:bar}
\end{figure*}


\section{The Correlation between ID and OOD Performance} \label{sec:appendixD}

To better show the correlation between ID and OOD performance, we provide the histogram in Figure \ref{fig:bar}. In general, we find that the overall performance of ID and OOD tests show a linear correlation for both discriminative and generative models. In addition to the overall performance, we look at task-level performance at a more granular level in Figure \ref{fig:scatter_appendix}. As shown in Figure 6, we find that the linear correlation does not exist for every task. For example, the 
linear correlation is extremely weak for MRPC and QQP, where the OOD accuracy is relatively low. While the Pearson correlation becomes significant when it comes to STSB and QNLI.

\section{The In-domain Evaluation Results}

Following \cite{wang2018glue}, we report the in-domain evaluation results in Table \ref{fig:id_results}. We generally find that T5-large achieves the best average performance over seven tasks. Note that we report the results by evaluating models on the validation set provided by GLUE.

\section{Rationale Overlap}
In order to measure the difference between rationales detected by PLMs and humans, we define precision as the percentage of the predicted rationales that also exist in the human annotation and recall as the percentage of words in the human annotation that also exist in the predicted rationales. The harmonic mean of the defined precision and recall calculates the F1 score of the overlap as follows:

\begin{equation}
\small
F_{1}=\frac{2 * \text { precision } * \text { recall }}{\text { precision }+\text { recall }}
\end{equation}

\begin{table*}[ht]


\centering
\small
\begin{tabular}{lllllllll}
\cline{1-9}
 Model           & SST2     & MNLI     & QNLI     & RTE      & MRPC     & QQP      & STSB     & COLA      \\ \hline
ELECTRA-large   & 2e-05/64 & 2e-05/16 & 5e-05/64 & 5e-05/32 & 2e-05/16 & 2e-05/64 & 2e-05/64 & 2e-05/128 \\ 
RoBERTa-large   & 2e-05/32 & 2e-05/64 & 2e-05/32 & 2e-05/32 & 2e-05/16 & 2e-05/64 & 2e-05/16 & 2e-05/32  \\
T5-large        & 1e-4/16  & 1e-4/32  & 1e-4/32  & 1e-4/32  & 1e-4/32  & 1e-4/16  & 1e-4/64  & 1e-4/32   \\
BART-large      & 2e-05/32 & 2e-05/16 & 2e-05/32 & 3e-05/32 & 2e-05/32 & 2e-05/32 & 2e-05/30 & 2e-05/32  \\
XLNet-large     & 3e-05/64 & 2e-05/64 & 3e-05/32 & 1e-05/32 & 1e-05/16 & 2e-05/32 & 2e-05/16 & 2e-05/16  \\
T5-base         & 1e-4/32  & 1e-4/16  & 1e-4/32  & 1e-4/8   & 1e-4/16  & 1e-4/16  & 3e-4/16  & 1e-4/32   \\
ELECTRA-base    & 1e-4/32  & 5e-05/64 & 5e-05/64 & 5e-05/16 & 5e-05/16 & 5e-05/32 & 5e-05/16 & 5e-05/32  \\
RoBERTa-base    & 2e-05/32 & 2e-05/32 & 2e-05/32 & 2e-05/32 & 3e-05/32 & 2e-05/32 & 3e-05/32 & 2e-05/16  \\
GPT2-large      & 2e-05/32 & 2e-05/32 & 2e-05/32 & 2e-05/16 & 3e-05/32 & 2e-05/32 & 2e-05/32 & 2e-05/32  \\
BERT-large      & 2e-05/32 & 2e-05/32 & 2e-05/16 & 2e-05/16 & 2e-05/16 & 2e-05/64 & 2e-05/64 & 3e-05/16  \\
BART-base       & 2e-05/32 & 2e-05/32 & 2e-05/32 & 2e-05/16 & 3e-05/32 & 2e-05/16 & 2e-05/16 & 2e-05/32  \\
ALBERT-base     & 2e-05/32 & 2e-05/32 & 2e-05/32 & 2e-05/32 & 2e-05/16 & 2e-05/32 & 2e-05/32 & 2e-05/32  \\
XLNet-base      & 3e-05/32 & 2e-05/32 & 2e-05/32 & 1e-05/16 & 2e-05/16 & 2e-05/32 & 2e-05/32 & 1e-05/32  \\
BERT-base       & 2e-05/32 & 3e-05/32 & 2e-05/32 & 3e-05/32 & 3e-05/32 & 2e-05/32 & 2e-05/16 & 3e-05/32  \\
GPT2-medium     & 2e-05/32 & 2e-05/16 & 3e-05/32 & 3e-05/32 & 3e-05/16 & 3e-05/32 & 3e-05/32 & 3e-05/32  \\
ELECTRA-small   & 5e-05/64 & 5e-05/64 & 5e-05/32 & 5e-05/64 & 5e-05/32 & 5e-05/32 & 5e-05/32 & 5e-05/64  \\
T5-small        & 1e-4/16  & 1e-4/16  & 1e-4/32  & 3e-4/16  & 1e-4/16  & 1e-4/16  & 3e-4/32  & 3e-4/32   \\
DistilBERT-base & 3e-05/16 & 2e-05/32 & 3e-05/32 & 2e-05/16 & 2e-05/16 & 2e-05/16 & 2e-05/16 & 2e-05/16  \\
GPT2            & 2e-05/32 & 2e-05/32 & 3e-05/32 & 2e-05/32 & 2e-05/32 & 2e-05/32 & 3e-05/32 & 3e-05/32 \\
\hline \cline{1-9}
\end{tabular}
\caption{The hyper-parameter setting for each task, including the learning rate and batch size.}
\label{tab:Hyperparameters}
\end{table*}


\begin{table*}[t]
\centering
\small
\begin{tabular}{lllllllllll}
\hline
Model   & \multicolumn{1}{l}{SST-2} & \multicolumn{1}{l}{MNLI} & \multicolumn{1}{l}{QNLI} &\multicolumn{1}{l}{RTE} & \multicolumn{1}{l}{MRPC} & \multicolumn{1}{l}{QQP} & \multicolumn{1}{l}{STSB} & \multicolumn{1}{l}{COLA} & \multicolumn{1}{l}{Average} & \multicolumn{1}{l}{Parameters}  \\ \hline
ELECTRA-large   & 97.25 & 89.29 & 93.65 & 88.45 & 92.6  & 89.84 & 88.06 & 74.33 & 89.18              & 334.09                          \\
RoBERTa-large   & 95.87 & 89.47 & 93.45 & 84.48 & 92.36 & 90.43 & 86.68 & 69.9  & 87.83              & 355.36                          \\
T5-large        & 95.41 & 88.83 & 94.34 & 89.89 & 92.01 & 90.59 & 87.58 & 62.97 & 87.7               & 737.67                          \\
BART-large      & 95.76 & 88.3  & 94.2  & 83.39 & 92.21 & 90.41 & 86.81 & 65.29 & 87.05              & 406.29                          \\
XLNet-large     & 96.44 & 89.5  & 93.32 & 84.12 & 91.54 & 90.06 & 86.36 & 62.63 & 86.75              & 360.27                          \\
T5-base         & 94.5  & 86.55 & 93.12 & 83.39 & 91.22 & 90.06 & 86.79 & 61.71 & 85.92              & 222.9                           \\
ELECTRA-base    & 91.51 & 87.12 & 92.09 & 80.14 & 91.09 & 89.36 & 86.07 & 69.95 & 85.92              & 108.89                          \\
RoBERTa-base    & 94.27 & 87.43 & 92.48 & 76.53 & 91.83 & 89.77 & 86.59 & 63.25 & 85.27              & 124.65                          \\
GPT2-large      & 94.5  & 85.48 & 91.21 & 75.45 & 87.78 & 89.34 & 84.75 & 60.06 & 83.57              & 774.03                          \\
BERT-large      & 93.46 & 85.69 & 91.84 & 70.76 & 90.26 & 89.78 & 83.97 & 60.32 & 83.26              & 335.14                          \\
BART-base       & 93.69 & 85.89 & 91.65 & 76.17 & 89.75 & 89.52 & 84.87 & 52.78 & 83.04              & 139.42                          \\
ALBERT-base     & 92.09 & 83.81 & 90.98 & 73.29 & 90.23 & 88.7  & 84.3  & 57.25 & 82.58              & 11.68                           \\
XLNet-base      & 94.15 & 86.49 & 91.36 & 68.59 & 90.5  & 89.39 & 83.94 & 53.67 & 82.26              & 116.72                          \\
BERT-base       & 92.89 & 83.63 & 91.05 & 66.79 & 89.41 & 89.4  & 83.71 & 59.75 & 82.08              & 109.48                          \\
GPT2-medium     & 94.27 & 85.38 & 90.81 & 70.04 & 87.2  & 89.42 & 83.75 & 53.87 & 81.84              & 354.82                          \\
ELECTRA-small   & 91.28 & 81.93 & 88.69 & 68.59 & 89.88 & 88.98 & 83.61 & 59.06 & 81.5               & 13.48                           \\
T5-small        & 91.97 & 82.82 & 90.77 & 70.4  & 89.13 & 89.07 & 84.74 & 43.88 & 80.35              & 60.51                           \\
DistilBERT-base & 91.17 & 82.2  & 89.27 & 65.34 & 88.33 & 88.63 & 82.28 & 54.43 & 80.21              & 66.36                           \\
GPT2            & 90.94 & 82.63 & 88.78 & 69.31 & 84.51 & 88.63 & 82.31 & 47.29 & 79.3               & 124.44                     \\     \hline
\end{tabular}
\caption{Detailed results of in-domain test on each task sorted by the average performance.}
\label{fig:id_results}
\end{table*}





\begin{comment}
\begin{table*}[t]
\centering
\small
\begin{tabular}{llllll}
\hline
model\_name             & ID\_performance & OOD\_performance & decrease & parameters &  \\ \hline
t5-base                 & 0.860169465     & 0.622391         & 0.237779 & 222.9036                        &  \\
roberta-large           & 0.866277256     & 0.622255         & 0.244023 & 355.3597                        &  \\
bart-large              & 0.863451739     & 0.610164         & 0.253288 & 406.2915                        &  \\
t5-large                & 0.85175065      & 0.59792          & 0.253831 & 737.6681                        &  \\
roberta-base            & 0.840335243     & 0.578747         & 0.261589 & 124.6456                        &  \\
xlnet-large-cased       & 0.845502071     & 0.576592         & 0.26891  & 360.2688                        &  \\
t5-small                & 0.811636089     & 0.571831         & 0.239805 & 60.50662                        &  \\
gpt2-large              & 0.814013051     & 0.564393         & 0.24962  & 774.0301                        &  \\
gpt2-medium             & 0.807710208     & 0.557801         & 0.249909 & 354.8232                        &  \\
bert-large-uncased      & 0.81464551      & 0.554268         & 0.260378 & 335.1419                        &  \\
bart-base               & 0.825363332     & 0.550978         & 0.274386 & 139.4204                        &  \\
xlnet-base-cased        & 0.804845243     & 0.546173         & 0.258672 & 116.7183                        &  \\
bert-base-uncased       & 0.806289943     & 0.541052         & 0.265238 & 109.4822                        &  \\
albert-base-v2          & 0.815529672     & 0.540269         & 0.275261 & 11.68358                        &  \\
distilbert-base-uncased & 0.788236393     & 0.525526         & 0.262711 & 66.36288                        &  \\
gpt2                    & 0.783730086     & 0.502746         & 0.280984 & 124.4398                        &  \\
\hline
\end{tabular}
\caption{Results on out-of-distribution data.}
\end{table*}


\begin{table*}[!t]
\centering
\small
\begin{tabular}{llllll}
\hline
 & model\_name             & ID\_performance & OOD\_performance & decrease & parameters \\ \hline
 & t5-base                 & 0.860169        & 0.622391         & 0.237779 & 222.9036                        \\
 & t5-small                & 0.811636        & 0.571831         & 0.239805 & 60.50662                        \\
 & roberta-large           & 0.866277        & 0.622255         & 0.244023 & 355.3597                        \\
 & gpt2-large              & 0.814013        & 0.564393         & 0.24962  & 774.0301                        \\
 & gpt2-medium             & 0.80771         & 0.557801         & 0.249909 & 354.8232                        \\
 & bart-large              & 0.863452        & 0.610164         & 0.253288 & 406.2915                        \\
 & t5-large                & 0.851751        & 0.59792          & 0.253831 & 737.6681                        \\
 & xlnet-base-cased        & 0.804845        & 0.546173         & 0.258672 & 116.7183                        \\
 & bert-large-uncased      & 0.814646        & 0.554268         & 0.260378 & 335.1419                        \\
 & roberta-base            & 0.840335        & 0.578747         & 0.261589 & 124.6456                        \\
 & distilbert-base-uncased & 0.788236        & 0.525526         & 0.262711 & 66.36288                        \\
 & bert-base-uncased       & 0.80629         & 0.541052         & 0.265238 & 109.4822                        \\
 & xlnet-large-cased       & 0.845502        & 0.576592         & 0.26891  & 360.2688                        \\
 & bart-base               & 0.825363        & 0.550978         & 0.274386 & 139.4204                        \\
 & albert-base-v2          & 0.81553         & 0.540269         & 0.275261 & 11.68358                        \\
 & gpt2                    & 0.78373         & 0.502746         & 0.280984 & 124.4398  \\
\hline
\end{tabular}
\caption{Performance decay sorted by the absolute decrease values.}
\end{table*}

\begin{table*}[!t]
\centering
\small
\begin{tabular}{llllll}
\hline
model\_name             & ID\_performance & OOD\_performance & decrease & decrease\_percentage & parameters \\ \hline
t5-base                 & 0.860169        & 0.622391         & 0.237779 & 27.64327             & 222.9036                        \\
roberta-large           & 0.866277        & 0.622255         & 0.244023 & 28.16911             & 355.3597                        \\
bart-large              & 0.863452        & 0.610164         & 0.253288 & 29.33435             & 406.2915                        \\
t5-small                & 0.811636        & 0.571831         & 0.239805 & 29.5459              & 60.50662                        \\
t5-large                & 0.851751        & 0.59792          & 0.253831 & 29.8011              & 737.6681                        \\
gpt2-large              & 0.814013        & 0.564393         & 0.24962  & 30.66539             & 774.0301                        \\
gpt2-medium             & 0.80771         & 0.557801         & 0.249909 & 30.94043             & 354.8232                        \\
roberta-base            & 0.840335        & 0.578747         & 0.261589 & 31.12909             & 124.6456                        \\
xlnet-large-cased       & 0.845502        & 0.576592         & 0.26891  & 31.80481             & 360.2688                        \\
bert-large-uncased      & 0.814646        & 0.554268         & 0.260378 & 31.96208             & 335.1419                        \\
xlnet-base-cased        & 0.804845        & 0.546173         & 0.258672 & 32.13932             & 116.7183                        \\
bert-base-uncased       & 0.80629         & 0.541052         & 0.265238 & 32.89607             & 109.4822                        \\
bart-base               & 0.825363        & 0.550978         & 0.274386 & 33.24422             & 139.4204                        \\
distilbert-base-uncased & 0.788236        & 0.525526         & 0.262711 & 33.32892             & 66.36288                        \\
albert-base-v2          & 0.81553         & 0.540269         & 0.275261 & 33.75241             & 11.68358                        \\
gpt2                    & 0.78373         & 0.502746         & 0.280984 & 35.85209             & 124.4398                        \\
\hline
\end{tabular}
\caption{Performance decay sorted by the decrease ratios.}
\end{table*}


% Please add the following required packages to your document preamble:
% \usepackage{multirow}
\begin{table*}[!t]
\centering
\small
\begin{tabular}{lllllllll}
\multirow{2}{*}{Model}  & SST-2       & MNLI     & RTE      & QNLI     & QQP      & Avg      & Avg  & Avg \\
                        & \method    & \method & \method & \method & \method & \method & GLUE & $\Delta$↓  \\
t5-base                 & 0.860169465 & 0.622391 & 0.237779 & 222.9036 &          &          &      &     \\
roberta-large           & 0.866277256 & 0.622255 & 0.244023 & 355.3597 &          &          &      &     \\
bart-large              & 0.863451739 & 0.610164 & 0.253288 & 406.2915 &          &          &      &     \\
t5-large                & 0.85175065  & 0.59792  & 0.253831 & 737.6681 &          &          &      &     \\
roberta-base            & 0.840335243 & 0.578747 & 0.261589 & 124.6456 &          &          &      &     \\
xlnet-large-cased       & 0.845502071 & 0.576592 & 0.26891  & 360.2688 &          &          &      &     \\
t5-small                & 0.811636089 & 0.571831 & 0.239805 & 60.50662 &          &          &      &     \\
gpt2-large              & 0.814013051 & 0.564393 & 0.24962  & 774.0301 &          &          &      &     \\
gpt2-medium             & 0.807710208 & 0.557801 & 0.249909 & 354.8232 &          &          &      &     \\
bert-large-uncased      & 0.81464551  & 0.554268 & 0.260378 & 335.1419 &          &          &      &     \\
bart-base               & 0.825363332 & 0.550978 & 0.274386 & 139.4204 &          &          &      &     \\
xlnet-base-cased        & 0.804845243 & 0.546173 & 0.258672 & 116.7183 &          &          &      &     \\
bert-base-uncased       & 0.806289943 & 0.541052 & 0.265238 & 109.4822 &          &          &      &     \\
albert-base-v2          & 0.815529672 & 0.540269 & 0.275261 & 11.68358 &          &          &      &     \\
distilbert-base-uncased & 0.788236393 & 0.525526 & 0.262711 & 66.36288 &          &          &      &     \\
gpt2                    & 0.783730086 & 0.502746 & 0.280984 & 124.4398 &          &          &      &    
\end{tabular}
\caption{Performance decay sorted by the decrease ratios.}
\end{table*}
\end{comment}



\begin{figure*}[t]
\centering

\subfigure[MRPC]{
\begin{minipage}[t]{0.5\linewidth}
\centering
\includegraphics[width=.9\textwidth]{plots/ID_OOD/mrpc.pdf}
%\caption{fig1}
\end{minipage}%
}%
\subfigure[QQP]{
\begin{minipage}[t]{0.5\linewidth}
\centering
\includegraphics[width=.9\textwidth]{plots/ID_OOD/qqp.pdf}
%\caption{fig2}
\end{minipage}%
}%

%这个回车键很重要 \quad也可以

\subfigure[STSB]{
\begin{minipage}[t]{0.5\linewidth}
\centering
\includegraphics[width=.9\textwidth]{plots/ID_OOD/stsb.pdf}
%\caption{fig2}
\end{minipage}
}%
\subfigure[QNLI]{
\begin{minipage}[t]{0.5\linewidth}
\centering
\includegraphics[width=.9\textwidth]{plots/ID_OOD/qnli.pdf}
%\caption{fig2}
\end{minipage}
}%

\subfigure[RTE]{
\begin{minipage}[t]{0.5\linewidth}
\centering
\includegraphics[width=.9\textwidth]{plots/ID_OOD/rte.pdf}
%\caption{fig1}
\end{minipage}%
}%
\subfigure[SST2]{
\begin{minipage}[t]{0.5\linewidth}
\centering
\includegraphics[width=.9\textwidth]{plots/ID_OOD/sst2.pdf}
%\caption{fig2}
\end{minipage}%
}%

%这个回车键很重要 \quad也可以


\centering
\caption{The correlation between the ID and OOD performance for each task involving in \method.}
\label{fig:scatter_appendix}
\end{figure*}




%%%%%%%%%%%%%%%%%%
% \begin{figure*}[ht]
% \centering
% \small
% {% 
% \includegraphics[width=.49\textwidth]{plots/ID_OOD/cola.pdf}
% }
% \centering 
% \small
% {% 
% \includegraphics[width=.49\textwidth] {plots/ID_OOD/mnli.pdf}
% }

% \small
% {% 
% \includegraphics[width=.49\textwidth]{plots/ID_OOD/rte.pdf}
% }
% \centering 
% \small
% {% 
% \includegraphics[width=.49\textwidth] {plots/ID_OOD/sst2.pdf} \label{fig:test2}
% }


% \caption{The scatter figures will be re-formatted late.} 

% \label{fig:scatter_point}
% \end{figure*}


%%%%%%%%%%%%%



\end{document}


