@article{gururangan2018annotation,
  title={Annotation artifacts in natural language inference data},
  author={Gururangan, Suchin and Swayamdipta, Swabha and Levy, Omer and Schwartz, Roy and Bowman, Samuel R and Smith, Noah A},
  journal={arXiv preprint arXiv:1803.02324},
  year={2018}
}

@article{kaushik2020learning,
  title={Learning the Difference that Makes a Difference with Counterfactually Augmented Data},
  author={Kaushik, Divyansh and Hovy, Eduard and Lipton, Zachary C},
  journal={International Conference on Learning Representations (ICLR)},
  year={2020}
}

@inproceedings{srivastava2020robustness,
  title={Robustness to spurious correlations via human annotations},
  author={Srivastava, Megha and Hashimoto, Tatsunori and Liang, Percy},
  booktitle={International Conference on Machine Learning},
  pages={9109--9119},
  year={2020},
  organization={PMLR}
}

@article{kaushik2021learning,
  title={Explaining the Efficacy of Counterfactually Augmented Data},
  author={Kaushik, Divyansh and Setlur, Amrith and Hovy, Eduard and Lipton, Zachary C},
  journal={International Conference on Learning Representations (ICLR)},
  year={2021}
}


@article{lertvittayakumjorn2021explanation,
  title={Explanation-Based Human Debugging of NLP Models: A Survey},
  author={Lertvittayakumjorn, Piyawat and Toni, Francesca},
  journal={arXiv preprint arXiv:2104.15135},
  year={2021}
}

@inproceedings{yang-etal-2021-exploring,
    title = "Exploring the Efficacy of Automatically Generated Counterfactuals for Sentiment Analysis",
    author = "Yang, Linyi  and
      Li, Jiazheng  and
      Cunningham, Padraig  and
      Zhang, Yue  and
      Smyth, Barry  and
      Dong, Ruihai",
    booktitle = "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)",
    month = aug,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.acl-long.26",
    doi = "10.18653/v1/2021.acl-long.26",
    pages = "306--316",
    abstract = "While state-of-the-art NLP models have been achieving the excellent performance of a wide range of tasks in recent years, important questions are being raised about their robustness and their underlying sensitivity to systematic biases that may exist in their training and test data. Such issues come to be manifest in performance problems when faced with out-of-distribution data in the field. One recent solution has been to use counterfactually augmented datasets in order to reduce any reliance on spurious patterns that may exist in the original data. Producing high-quality augmented data can be costly and time-consuming as it usually needs to involve human feedback and crowdsourcing efforts. In this work, we propose an alternative by describing and evaluating an approach to automatically generating counterfactual data for the purpose of data augmentation and explanation. A comprehensive evaluation on several different datasets and using a variety of state-of-the-art benchmarks demonstrate how our approach can achieve significant improvements in model performance when compared to models training on the original data and even when compared to models trained with the benefit of human-generated augmented data.",
}

@inproceedings{qian2021counterfactual,
  title={Counterfactual Inference for Text Classification Debiasing},
  author={Qian, Chen and Feng, Fuli and Wen, Lijie and Ma, Chunping and Xie, Pengjun},
  booktitle={Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)},
  pages={5434--5445},
  year={2021}
}

@inproceedings{feng-etal-2021-empowering,
    title = "Empowering Language Understanding with Counterfactual Reasoning",
    author = "Feng, Fuli  and
      Zhang, Jizhi  and
      He, Xiangnan  and
      Zhang, Hanwang  and
      Chua, Tat-Seng",
    booktitle = "Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021",
    month = aug,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.findings-acl.196",
    doi = "10.18653/v1/2021.findings-acl.196",
    pages = "2226--2236",
}

@inproceedings{yang-etal-2020-generating,
    title = "Generating Plausible Counterfactual Explanations for Deep Transformers in Financial Text Classification",
    author = "Yang, Linyi  and
      Kenny, Eoin  and
      Ng, Tin Lok James  and
      Yang, Yi  and
      Smyth, Barry  and
      Dong, Ruihai",
    booktitle = "Proceedings of the 28th International Conference on Computational Linguistics",
    month = dec,
    year = "2020",
    address = "Barcelona, Spain (Online)",
    publisher = "International Committee on Computational Linguistics",
    url = "https://aclanthology.org/2020.coling-main.541",
    doi = "10.18653/v1/2020.coling-main.541",
    pages = "6150--6160",
    abstract = "Corporate mergers and acquisitions (M{\&}A) account for billions of dollars of investment globally every year and offer an interesting and challenging domain for artificial intelligence. However, in these highly sensitive domains, it is crucial to not only have a highly robust/accurate model, but be able to generate useful explanations to garner a user{'}s trust in the automated system. Regrettably, the recent research regarding eXplainable AI (XAI) in financial text classification has received little to no attention, and many current methods for generating textual-based explanations result in highly implausible explanations, which damage a user{'}s trust in the system. To address these issues, this paper proposes a novel methodology for producing plausible counterfactual explanations, whilst exploring the regularization benefits of adversarial training on language models in the domain of FinTech. Exhaustive quantitative experiments demonstrate that not only does this approach improve the model accuracy when compared to the current state-of-the-art and human performance, but it also generates counterfactual explanations which are significantly more plausible based on human trials.",
}

@inproceedings{keith2020text,
  title={Text and Causal Inference: A Review of Using Text to Remove Confounding from Causal Estimates},
  author={Keith, Katherine and Jensen, David and O’Connor, Brendan},
  booktitle={Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics},
  pages={5332--5344},
  year={2020}
}

@inproceedings{wei-zou-2019-eda,
    title = "{EDA}: Easy Data Augmentation Techniques for Boosting Performance on Text Classification Tasks",
    author = "Wei, Jason  and
      Zou, Kai",
    booktitle = "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)",
    month = nov,
    year = "2019",
    address = "Hong Kong, China",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D19-1670",
    doi = "10.18653/v1/D19-1670",
    pages = "6382--6388",
    abstract = "We present EDA: easy data augmentation techniques for boosting performance on text classification tasks. EDA consists of four simple but powerful operations: synonym replacement, random insertion, random swap, and random deletion. On five text classification tasks, we show that EDA improves performance for both convolutional and recurrent neural networks. EDA demonstrates particularly strong results for smaller datasets; on average, across five datasets, training with EDA while using only 50{\%} of the available training set achieved the same accuracy as normal training with all available data. We also performed extensive ablation studies and suggest parameters for practical use.",
}

@inproceedings{zylberajch-etal-2021-hildif,
    title = "{HILDIF}: {I}nteractive Debugging of {NLI} Models Using Influence Functions",
    author = "Zylberajch, Hugo  and
      Lertvittayakumjorn, Piyawat  and
      Toni, Francesca",
    booktitle = "Proceedings of the First Workshop on Interactive Learning for Natural Language Processing",
    month = aug,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.internlp-1.1",
    doi = "10.18653/v1/2021.internlp-1.1",
    pages = "1--6",
    abstract = "Biases and artifacts in training data can cause unwelcome behavior in text classifiers (such as shallow pattern matching), leading to lack of generalizability. One solution to this problem is to include users in the loop and leverage their feedback to improve models. We propose a novel explanatory debugging pipeline called HILDIF, enabling humans to improve deep text classifiers using influence functions as an explanation method. We experiment on the Natural Language Inference (NLI) task, showing that HILDIF can effectively alleviate artifact problems in fine-tuned BERT models and result in increased model generalizability.",
}

@InProceedings{10.1007/978-3-030-88942-5_18,
author="Lu, Jinghui
and Henchion, Maeve
and Bacher, Ivan
and Namee, Brian Mac",
editor="Soares, Carlos
and Torgo, Luis",
title="A Sentence-Level Hierarchical BERT Model for Document Classification with Limited Labelled Data",
booktitle="Discovery Science",
year="2021",
publisher="Springer International Publishing",
address="Cham",
pages="231--241",
abstract="The emergence of transformer models like BERT means that deep learning language models can achieve reasonably good performance in document classification with few labelled instances. However, there is a lack of evidence for the utility of applying BERT-like models on long document classification in few-shot scenarios. This paper introduces a long-text-specific model---the Hierarchical BERT Model (HBM)---that learns sentence-level features of a document and works well in few-shot scenarios. Evaluation experiments demonstrate that HBM can, with only 50 to 200 labelled instances, achieve higher document classification performance than existing state-of-the-art methods, especially when documents are long. Also, as an extra benefit of HBM, the salient sentences identified by a HBM are useful as explanations for document classifications. A user study demonstrates that highlighting these salient sentences is an effective way to speed up the document annotation required in interactive machine learning approaches like active learning.",
isbn="978-3-030-88942-5"
}

@article{settles2009active,
  title={Active learning literature survey},
  author={Settles, Burr},
  year={2009},
  publisher={University of Wisconsin-Madison Department of Computer Sciences}
}

@article{lu2020investigating,
  title={Investigating the Effectiveness of Representations Based on Pretrained Transformer-based Language Models in Active Learning for Labelling Text Datasets},
  author={Lu, Jinghui and MacNamee, Brian},
  journal={arXiv preprint arXiv:2004.13138},
  year={2020}
}

@inproceedings{zaidan-etal-2007-using,
    title = "Using {``}Annotator Rationales{''} to Improve Machine Learning for Text Categorization",
    author = "Zaidan, Omar  and
      Eisner, Jason  and
      Piatko, Christine",
    booktitle = "Human Language Technologies 2007: The Conference of the North {A}merican Chapter of the Association for Computational Linguistics; Proceedings of the Main Conference",
    month = apr,
    year = "2007",
    address = "Rochester, New York",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/N07-1033",
    pages = "260--267",
}

@inproceedings{jin2019towards,
  title={Towards Hierarchical Importance Attribution: Explaining Compositional Semantics for Neural Sequence Models},
  author={Jin, Xisen and Wei, Zhongyu and Du, Junyi and Xue, Xiangyang and Ren, Xiang},
  booktitle={International Conference on Learning Representations},
  year={2019}
}

@inproceedings{maas-etal-2011-learning,
    title = "Learning Word Vectors for Sentiment Analysis",
    author = "Maas, Andrew L.  and
      Daly, Raymond E.  and
      Pham, Peter T.  and
      Huang, Dan  and
      Ng, Andrew Y.  and
      Potts, Christopher",
    booktitle = "Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies",
    month = jun,
    year = "2011",
    address = "Portland, Oregon, USA",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P11-1015",
    pages = "142--150",
}

@article{10.1145/219717.219748,
author = {Miller, George A.},
title = {WordNet: A Lexical Database for English},
year = {1995},
issue_date = {Nov. 1995},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {38},
number = {11},
issn = {0001-0782},
url = {https://doi.org/10.1145/219717.219748},
doi = {10.1145/219717.219748},
abstract = {Because meaningful sentences are composed of meaningful words, any system that hopes
to process natural languages as people do must have information about words and their
meanings. This information is traditionally provided through dictionaries, and machine-readable
dictionaries are now widely available. But dictionary entries evolved for the convenience
of human readers, not for machines. WordNet1 provides a more effective combination
of traditional lexicographic information and modern computing. WordNet is an online
lexical database designed for use under program control. English nouns, verbs, adjectives,
and adverbs are organized into sets of synonyms, each representing a lexicalized concept.
Semantic relations link the synonym sets [4].},
journal = {Commun. ACM},
month = nov,
pages = {39–41},
numpages = {3}
}

@article{Liu2019RoBERTaAR,
  title={RoBERTa: A Robustly Optimized BERT Pretraining Approach},
  author={Yinhan Liu and Myle Ott and Naman Goyal and Jingfei Du and Mandar Joshi and Danqi Chen and Omer Levy and Mike Lewis and Luke Zettlemoyer and Veselin Stoyanov},
  journal={ArXiv},
  year={2019},
  volume={abs/1907.11692}
}

@article{krizhevsky2012imagenet,
  title={Imagenet classification with deep convolutional neural networks},
  author={Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E},
  journal={Advances in neural information processing systems},
  volume={25},
  pages={1097--1105},
  year={2012}
}

@inproceedings{ni-etal-2019-justifying,
    title = "Justifying Recommendations using Distantly-Labeled Reviews and Fine-Grained Aspects",
    author = "Ni, Jianmo  and
      Li, Jiacheng  and
      McAuley, Julian",
    booktitle = "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)",
    month = nov,
    year = "2019",
    address = "Hong Kong, China",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D19-1018",
    doi = "10.18653/v1/D19-1018",
    pages = "188--197",
    abstract = "Several recent works have considered the problem of generating reviews (or {`}tips{'}) as a form of explanation as to why a recommendation might match a customer{'}s interests. While promising, we demonstrate that existing approaches struggle (in terms of both quality and content) to generate justifications that are relevant to users{'} decision-making process. We seek to introduce new datasets and methods to address the recommendation justification task. In terms of data, we first propose an {`}extractive{'} approach to identify review segments which justify users{'} intentions; this approach is then used to distantly label massive review corpora and construct large-scale personalized recommendation justification datasets. In terms of generation, we are able to design two personalized generation models with this data: (1) a reference-based Seq2Seq model with aspect-planning which can generate justifications covering different aspects, and (2) an aspect-conditional masked language model which can generate diverse justifications based on templates extracted from justification histories. We conduct experiments on two real-world datasets which show that our model is capable of generating convincing and diverse justifications.",
}

@inproceedings{rosenthal-etal-2017-semeval,
    title = "{S}em{E}val-2017 Task 4: Sentiment Analysis in {T}witter",
    author = "Rosenthal, Sara  and
      Farra, Noura  and
      Nakov, Preslav",
    booktitle = "Proceedings of the 11th International Workshop on Semantic Evaluation ({S}em{E}val-2017)",
    month = aug,
    year = "2017",
    address = "Vancouver, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/S17-2088",
    doi = "10.18653/v1/S17-2088",
    pages = "502--518",
    abstract = "This paper describes the fifth year of the Sentiment Analysis in Twitter task. SemEval-2017 Task 4 continues with a rerun of the subtasks of SemEval-2016 Task 4, which include identifying the overall sentiment of the tweet, sentiment towards a topic with classification on a two-point and on a five-point ordinal scale, and quantification of the distribution of sentiment towards a topic across a number of tweets: again on a two-point and on a five-point ordinal scale. Compared to 2016, we made two changes: (i) we introduced a new language, Arabic, for all subtasks, and (ii) we made available information from the profiles of the Twitter users who posted the target tweets. The task continues to be very popular, with a total of 48 teams participating this year.",
}

@inproceedings{socher-etal-2013-recursive,
    title = "Recursive Deep Models for Semantic Compositionality Over a Sentiment Treebank",
    author = "Socher, Richard  and
      Perelygin, Alex  and
      Wu, Jean  and
      Chuang, Jason  and
      Manning, Christopher D.  and
      Ng, Andrew  and
      Potts, Christopher",
    booktitle = "Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing",
    month = oct,
    year = "2013",
    address = "Seattle, Washington, USA",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/D13-1170",
    pages = "1631--1642",
}

@article{zhang2015character,
  title={Character-level convolutional networks for text classification},
  author={Zhang, Xiang and Zhao, Junbo and LeCun, Yann},
  journal={Advances in neural information processing systems},
  volume={28},
  pages={649--657},
  year={2015}
}

@article{adam,
author = {Kingma, Diederik and Ba, Jimmy},
year = {2014},
month = {12},
pages = {},
title = {Adam: A Method for Stochastic Optimization},
journal = {International Conference on Learning Representations}
}

@inproceedings{wang2021robustness,
  title={Robustness to Spurious Correlations in Text Classification via Automatically Generated Counterfactuals},
  author={Wang, Zhao and Culotta, Aron},
  booktitle = {AAAI},
  year={2021}
}

@inproceedings{fails2003interactive,
  title={Interactive machine learning},
  author={Fails, Jerry Alan and Olsen Jr, Dan R},
  booktitle={Proceedings of the 8th international conference on Intelligent user interfaces},
  pages={39--45},
  year={2003}
}

@inproceedings{kulesza2015,
author = {Kulesza, Todd and Burnett, Margaret and Wong, Weng-Keen and Stumpf, Simone},
title = {Principles of Explanatory Debugging to Personalize Interactive Machine Learning},
year = {2015},
isbn = {9781450333061},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2678025.2701399},
doi = {10.1145/2678025.2701399},
abstract = {How can end users efficiently influence the predictions that machine learning systems
make on their behalf? This paper presents Explanatory Debugging, an approach in which
the system explains to users how it made each of its predictions, and the user then
explains any necessary corrections back to the learning system. We present the principles
underlying this approach and a prototype instantiating it. An empirical evaluation
shows that Explanatory Debugging increased participants' understanding of the learning
system by 52% and allowed participants to correct its mistakes up to twice as efficiently
as participants using a traditional learning system.},
booktitle = {Proceedings of the 20th International Conference on Intelligent User Interfaces},
pages = {126–137},
numpages = {12},
keywords = {interactive machine learning, end user programming},
location = {Atlanta, Georgia, USA},
series = {IUI '15}
}

@INPROCEEDINGS{kulesza2010,  author={Kulesza, Todd and Stumpf, Simone and Burnett, Margaret and Wong, Weng-Keen and Riche, Yann and Moore, Travis and Oberst, Ian and Shinsel, Amber and McIntosh, Kevin},  booktitle={2010 IEEE Symposium on Visual Languages and Human-Centric Computing},   title={Explanatory Debugging: Supporting End-User Debugging of Machine-Learned Programs},   year={2010},  volume={},  number={},  pages={41-48},  doi={10.1109/VLHCC.2010.15}}

@inproceedings{kulesza2009,
author = {Kulesza, Todd and Wong, Weng-Keen and Stumpf, Simone and Perona, Stephen and White, Rachel and Burnett, Margaret M. and Oberst, Ian and Ko, Amy J.},
title = {Fixing the Program My Computer Learned: Barriers for End Users, Challenges for the Machine},
year = {2009},
isbn = {9781605581682},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1502650.1502678},
doi = {10.1145/1502650.1502678},
abstract = {The results of a machine learning from user behavior can be thought of as a program,
and like all programs, it may need to be debugged. Providing ways for the user to
debug it matters, because without the ability to fix errors users may find that the
learned program's errors are too damaging for them to be able to trust such programs.
We present a new approach to enable end users to debug a learned program. We then
use an early prototype of our new approach to conduct a formative study to determine
where and when debugging issues arise, both in general and also separately for males
and females. The results suggest opportunities to make machine-learned programs more
effective tools.},
booktitle = {Proceedings of the 14th International Conference on Intelligent User Interfaces},
pages = {187–196},
numpages = {10},
keywords = {debugging, end-user programming, machine learning},
location = {Sanibel Island, Florida, USA},
series = {IUI '09}
}

@article{Stumpf2009,
author = {Stumpf, Simone and Rajaram, Vidya and Li, Lida and Wong, Weng-Keen and Burnett, Margaret and Dietterich, Thomas and Sullivan, Erin and Herlocker, Jonathan},
title = {Interacting Meaningfully with Machine Learning Systems: Three Experiments},
year = {2009},
issue_date = {August, 2009},
publisher = {Academic Press, Inc.},
address = {USA},
volume = {67},
number = {8},
issn = {1071-5819},
url = {https://doi.org/10.1016/j.ijhcs.2009.03.004},
doi = {10.1016/j.ijhcs.2009.03.004},
abstract = {Although machine learning is becoming commonly used in today's software, there has
been little research into how end users might interact with machine learning systems,
beyond communicating simple ''right/wrong'' judgments. If the users themselves could
work hand-in-hand with machine learning systems, the users' understanding and trust
of the system could improve and the accuracy of learning systems could be improved
as well. We conducted three experiments to understand the potential for rich interactions
between users and machine learning systems. The first experiment was a think-aloud
study that investigated users' willingness to interact with machine learning reasoning,
and what kinds of feedback users might give to machine learning systems. We then investigated
the viability of introducing such feedback into machine learning systems, specifically,
how to incorporate some of these types of user feedback into machine learning systems,
and what their impact was on the accuracy of the system. Taken together, the results
of our experiments show that supporting rich interactions between users and machine
learning systems is feasible for both user and machine. This shows the potential of
rich human-computer collaboration via on-the-spot interactions as a promising direction
for machine learning systems and users to collaboratively share intelligence.},
journal = {Int. J. Hum.-Comput. Stud.},
month = aug,
pages = {639–662},
numpages = {24},
keywords = {Rich feedback, Machine learning, Explanations, Intelligent user interfaces}
}

@inproceedings{JiaL17,
  author    = {Robin Jia and
               Percy Liang},
  editor    = {Martha Palmer and
               Rebecca Hwa and
               Sebastian Riedel},
  title     = {Adversarial Examples for Evaluating Reading Comprehension Systems},
  booktitle = {Proceedings of the 2017 Conference on Empirical Methods in Natural
               Language Processing, {EMNLP} 2017, Copenhagen, Denmark, September
               9-11, 2017},
  pages     = {2021--2031},
  publisher = {Association for Computational Linguistics},
  year      = {2017},
  url       = {https://doi.org/10.18653/v1/d17-1215},
  doi       = {10.18653/v1/d17-1215},
  timestamp = {Fri, 06 Aug 2021 00:40:40 +0200},
  biburl    = {https://dblp.org/rec/conf/emnlp/JiaL17.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{Shao_Skryagin_Stammer_Schramowski_Kersting_2021, 
title={Right for Better Reasons: Training Differentiable Models by Constraining their Influence Functions}, 
volume={35}, 
url={https://ojs.aaai.org/index.php/AAAI/article/view/17148},  
number={11}, 
journal={Proceedings of the AAAI Conference on Artificial Intelligence}, author={Shao, Xiaoting and Skryagin, Arseny and Stammer, Wolfgang and Schramowski, Patrick and Kersting, Kristian}, 
year={2021}, 
month={May}, 
pages={9533-9540} }



@article{ghai2021,
author = {Ghai, Bhavya and Liao, Q. Vera and Zhang, Yunfeng and Bellamy, Rachel and Mueller, Klaus},
title = {Explainable Active Learning (XAL): Toward AI Explanations as Interfaces for Machine Teachers},
year = {2021},
issue_date = {December 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {4},
number = {CSCW3},
url = {https://doi.org/10.1145/3432934},
doi = {10.1145/3432934},
abstract = {The wide adoption of Machine Learning (ML) technologies has created a growing demand
for people who can train ML models. Some advocated the term "machine teacher'' to
refer to the role of people who inject domain knowledge into ML models. This "teaching''
perspective emphasizes supporting the productivity and mental wellbeing of machine
teachers through efficient learning algorithms and thoughtful design of human-AI interfaces.
One promising learning paradigm is Active Learning (AL), by which the model intelligently
selects instances to query a machine teacher for labels, so that the labeling workload
could be largely reduced. However, in current AL settings, the human-AI interface
remains minimal and opaque. A dearth of empirical studies further hinders us from
developing teacher-friendly interfaces for AL algorithms. In this work, we begin considering
AI explanations as a core element of the human-AI interface for teaching machines.
When a human student learns, it is a common pattern to present one's own reasoning
and solicit feedback from the teacher. When a ML model learns and still makes mistakes,
the teacher ought to be able to understand the reasoning underlying its mistakes.
When the model matures, the teacher should be able to recognize its progress in order
to trust and feel confident about their teaching outcome. Toward this vision, we propose
a novel paradigm of explainable active learning (XAL), by introducing techniques from
the surging field of explainable AI (XAI) into an AL setting. We conducted an empirical
study comparing the model learning outcomes, feedback content and experience with
XAL, to that of traditional AL and coactive learning (providing the model's prediction
without explanation). Our study shows benefits of AI explanation as interfaces for
machine teaching--supporting trust calibration and enabling rich forms of teaching
feedback, and potential drawbacks--anchoring effect with the model judgment and additional
cognitive workload. Our study also reveals important individual factors that mediate
a machine teacher's reception to AI explanations, including task knowledge, AI experience
and Need for Cognition. By reflecting on the results, we suggest future directions
and design implications for XAL, and more broadly, machine teaching through AI explanations.},
journal = {Proc. ACM Hum.-Comput. Interact.},
month = jan,
articleno = {235},
numpages = {28},
keywords = {interactive machine learning, explainable ai, explanation, active learning, human-ai interaction, labeling, machine teaching}
}

@inproceedings{teso2019,
author = {Teso, Stefano and Kersting, Kristian},
title = {Explanatory Interactive Machine Learning},
year = {2019},
isbn = {9781450363242},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3306618.3314293},
doi = {10.1145/3306618.3314293},
abstract = {Although interactive learning puts the user into the loop, the learner remains mostly
a black box for the user. Understanding the reasons behind predictions and queries
is important when assessing how the learner works and, in turn, trust. Consequently,
we propose the novel framework of explanatory interactive learning where, in each
step, the learner explains its query to the user, and the user interacts by both answering
the query and correcting the explanation. We demonstrate that this can boost the predictive
and explanatory powers of, and the trust into, the learned model, using text (e.g.
SVMs) and image classification (e.g. neural networks) experiments as well as a user
study.},
booktitle = {Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {239–245},
numpages = {7},
keywords = {machine learning, explainable artificial intelligence, interpretability, active learning},
location = {Honolulu, HI, USA},
series = {AIES '19}
}

@inproceedings{StammerSK21,
  author    = {Wolfgang Stammer and
               Patrick Schramowski and
               Kristian Kersting},
  title     = {Right for the Right Concept: Revising Neuro-Symbolic Concepts by Interacting
               With Their Explanations},
  booktitle = {{IEEE} Conference on Computer Vision and Pattern Recognition, {CVPR}
               2021, virtual, June 19-25, 2021},
  pages     = {3619--3629},
  publisher = {Computer Vision Foundation / {IEEE}},
  year      = {2021},
  url       = {https://openaccess.thecvf.com/content/CVPR2021/html/Stammer\_Right\_for\_the\_Right\_Concept\_Revising\_Neuro-Symbolic\_Concepts\_by\_Interacting\_CVPR\_2021\_paper.html},
  timestamp = {Mon, 30 Aug 2021 17:00:27 +0200},
  biburl    = {https://dblp.org/rec/conf/cvpr/StammerSK21.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@misc{lertvittayakumjorn2020find,
      title={FIND: Human-in-the-Loop Debugging Deep Text Classifiers}, 
      author={Piyawat Lertvittayakumjorn and Lucia Specia and Francesca Toni},
      year={2020},
      eprint={2010.04987},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@article{teso2021interactive,
  title={Interactive Label Cleaning with Example-based Explanations},
  author={Teso, Stefano and Bontempelli, Andrea and Giunchiglia, Fausto and Passerini, Andrea},
  journal={Proceedings of the Thirty-fifth Conference on Neural Information Processing Systems},
  year={2021}
}

@article{yao2021refining,
  title={Refining neural networks with compositional explanations},
  author={Yao, Huihan and Chen, Ying and Ye, Qinyuan and Jin, Xisen and Ren, Xiang},
  journal={arXiv preprint arXiv:2103.10415},
  year={2021}
}

@inproceedings{ribeiro2018anchors,
  title={Anchors: High-precision model-agnostic explanations},
  author={Ribeiro, Marco Tulio and Singh, Sameer and Guestrin, Carlos},
  booktitle={Proceedings of the AAAI conference on artificial intelligence},
  volume={32},
  year={2018}
}

@article{margatina2021active,
  title={Active Learning by Acquiring Contrastive Examples},
  author={Margatina, Katerina and Vernikos, Giorgos and Barrault, Lo{\"\i}c and Aletras, Nikolaos},
  journal={Proceddings of the 2021 Conference on Empirical Methods in Natural Language Processing, Underline Science Inc.},
  year={2021}
}

@article{wu2021survey,
  title={A Survey of Human-in-the-loop for Machine Learning},
  author={Wu, Xingjiao and Xiao, Luwei and Sun, Yixuan and Zhang, Junhang and Ma, Tianlong and He, Liang},
  journal={arXiv preprint arXiv:2108.00941},
  year={2021}
}

@article{mccloy2002semifactual,
  title={Semifactual “even if” thinking},
  author={McCloy, Rachel and Byrne, Ruth MJ},
  journal={Thinking \& Reasoning},
  volume={8},
  number={1},
  pages={41--67},
  year={2002},
  publisher={Taylor \& Francis}
}

@article{roese1997counterfactual,
  title={Counterfactual thinking.},
  author={Roese, Neal J},
  journal={Psychological bulletin},
  volume={121},
  number={1},
  pages={133},
  year={1997},
  publisher={American Psychological Association}
}

@book{peirce1992reasoning,
  title={Reasoning and the logic of things: The Cambridge conferences lectures of 1898},
  author={Peirce, Charles Sanders},
  year={1992},
  publisher={Harvard University Press}
}

@article{warstadt2020can,
  title={Can neural networks acquire a structural bias from raw linguistic data?},
  author={Warstadt, Alex and Bowman, Samuel R},
  journal={arXiv preprint arXiv:2007.06761},
  year={2020}
}

@article{wu2021lime,
  title={LIME: Learning Inductive Bias for Primitives of Mathematical Reasoning},
  author={Wu, Yuhuai and Rabe, Markus and Li, Wenda and Ba, Jimmy and Grosse, Roger and Szegedy, Christian},
  journal={arXiv preprint arXiv:2101.06223},
  year={2021}
}

@article{tu2020empirical,
  title={An empirical study on robustness to spurious correlations using pre-trained language models},
  author={Tu, Lifu and Lalwani, Garima and Gella, Spandana and He, He},
  journal={Transactions of the Association for Computational Linguistics},
  volume={8},
  pages={621--633},
  year={2020},
  publisher={MIT Press}
}

@article{wang2021identifying,
  title={Identifying and Mitigating Spurious Correlations for Improving Robustness in NLP Models},
  author={Wang, Tianlu and Yang, Diyi and Wang, Xuezhi},
  journal={arXiv preprint arXiv:2110.07736},
  year={2021}
}

@article{y,
  title={WordNet: a lexical database for English},
  author={Miller, George A},
  journal={Communications of the ACM},
  volume={38},
  number={11},
  pages={39--41},
  year={1995},
  publisher={ACM New York, NY, USA}
}

@inproceedings{yang2020generating,
  title={Generating Plausible Counterfactual Explanations for Deep Transformers in Financial Text Classification},
  author={Yang, Linyi and Kenny, Eoin and Ng, Tin Lok James and Yang, Yi and Smyth, Barry and Dong, Ruihai},
  booktitle={Proceedings of the 28th International Conference on Computational Linguistics},
  pages={6150--6160},
  year={2020}
}

@inproceedings{li2020maec,
  title={MAEC: A multimodal aligned earnings conference call dataset for financial risk prediction},
  author={Li, Jiazheng and Yang, Linyi and Smyth, Barry and Dong, Ruihai},
  booktitle={Proceedings of the 29th ACM International Conference on Information \& Knowledge Management},
  pages={3063--3070},
  year={2020}
}

@inproceedings{yang2020html,
  title={Html: Hierarchical transformer-based multi-task learning for volatility prediction},
  author={Yang, Linyi and Ng, Tin Lok James and Smyth, Barry and Dong, Riuhai},
  booktitle={Proceedings of The Web Conference 2020},
  pages={441--451},
  year={2020}
}
@article{delaney2021uncertainty,
  title={Uncertainty Estimation and Out-of-Distribution Detection for Counterfactual Explanations: Pitfalls and Solutions},
  author={Delaney, Eoin and Greene, Derek and Keane, Mark T},
  journal={arXiv preprint arXiv:2107.09734},
  year={2021}
}
@article{kenny2021generating,
  title={On generating plausible counterfactual and semi-factual explanations for deep learning},
  author={Kenny, Eoin M and Keane, Mark T},
  year={2021}
}

%%% New Added Papers

@inproceedings{wang2018glue,
title={{GLUE}: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding},
author={Alex Wang and Amanpreet Singh and Julian Michael and Felix Hill and Omer Levy and Samuel R. Bowman},
booktitle={International Conference on Learning Representations},
year={2019},
url={https://openreview.net/forum?id=rJ4km2R5t7},
}

@inproceedings{
gui2022good,
title={{GOOD}: A Graph Out-of-Distribution Benchmark},
author={Shurui Gui and Xiner Li and Limei Wang and Shuiwang Ji},
booktitle={Thirty-sixth Conference on Neural Information Processing Systems Datasets and Benchmarks Track},
year={2022},
url={https://openreview.net/forum?id=8hHg-zs_p-h}
}

@inproceedings{
larson2022evaluating,
title={Evaluating Out-of-Distribution Performance on Document Image Classifiers},
author={Stefan Larson and Gordon Lim and Yutong Ai and David Kuang and Kevin Leach},
booktitle={Thirty-sixth Conference on Neural Information Processing Systems Datasets and Benchmarks Track},
year={2022},
url={https://openreview.net/forum?id=uDlkiCI5N7Y}
}

@inproceedings{
yang2022openood,
title={Open{OOD}: Benchmarking Generalized Out-of-Distribution Detection},
author={Jingkang Yang and Pengyun Wang and Dejian Zou and Zitang Zhou and Kunyuan Ding and WENXUAN PENG and Haoqi Wang and Guangyao Chen and Bo Li and Yiyou Sun and Xuefeng Du and Kaiyang Zhou and Wayne Zhang and Dan Hendrycks and Yixuan Li and Ziwei Liu},
booktitle={Thirty-sixth Conference on Neural Information Processing Systems Datasets and Benchmarks Track},
year={2022},
url={https://openreview.net/forum?id=gT6j4_tskUt}
}

@inproceedings{
malinin2021shifts,
title={Shifts: A Dataset of Real Distributional Shift Across Multiple Large-Scale Tasks},
author={Andrey Malinin and Neil Band and Yarin Gal and Mark Gales and Alexander Ganshin and German Chesnokov and Alexey Noskov and Andrey Ploskonosov and Liudmila Prokhorenkova and Ivan Provilkov and Vatsal Raina and Vyas Raina and Denis Roginskiy and Mariya Shmatova and Panagiotis Tigas and Boris Yangel},
booktitle={Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track (Round 2)},
year={2021},
url={https://openreview.net/forum?id=qM45LHaWM6E}
}

@article{wang2019superglue,
  title={Superglue: A stickier benchmark for general-purpose language understanding systems},
  author={Wang, Alex and Pruksachatkun, Yada and Nangia, Nikita and Singh, Amanpreet and Michael, Julian and Hill, Felix and Levy, Omer and Bowman, Samuel},
  journal={Advances in neural information processing systems},
  volume={32},
  year={2019}
}

@article{hendrycks2020pretrained,
  title={Pretrained transformers improve out-of-distribution robustness},
  author={Hendrycks, Dan and Liu, Xiaoyuan and Wallace, Eric and Dziedzic, Adam and Krishnan, Rishabh and Song, Dawn},
  journal={arXiv preprint arXiv:2004.06100},
  year={2020}
}

@inproceedings{
kumar2022finetuning,
title={Fine-Tuning can Distort Pretrained Features and Underperform Out-of-Distribution},
author={Ananya Kumar and Aditi Raghunathan and Robbie Matthew Jones and Tengyu Ma and Percy Liang},
booktitle={International Conference on Learning Representations},
year={2022},
url={https://openreview.net/forum?id=UYneFzXSJWh}
}

@inproceedings{huang2020self,
  title={Self-challenging improves cross-domain generalization},
  author={Huang, Zeyi and Wang, Haohan and Xing, Eric P and Huang, Dong},
  booktitle={European Conference on Computer Vision},
  pages={124--140},
  year={2020},
  organization={Springer}
}

@inproceedings{krueger2021out,
  title={Out-of-distribution generalization via risk extrapolation (rex)},
  author={Krueger, David and Caballero, Ethan and Jacobsen, Joern-Henrik and Zhang, Amy and Binas, Jonathan and Zhang, Dinghuai and Le Priol, Remi and Courville, Aaron},
  booktitle={International Conference on Machine Learning},
  pages={5815--5826},
  year={2021},
  organization={PMLR}
}

@article{wang2022generalizing,
  title={Generalizing to unseen domains: A survey on domain generalization},
  author={Wang, Jindong and Lan, Cuiling and Liu, Chang and Ouyang, Yidong and Qin, Tao and Lu, Wang and Chen, Yiqiang and Zeng, Wenjun and Yu, Philip},
  journal={IEEE Transactions on Knowledge and Data Engineering},
  year={2022},
  publisher={IEEE}
}

@inproceedings{koh2021wilds,
  title={Wilds: A benchmark of in-the-wild distribution shifts},
  author={Koh, Pang Wei and Sagawa, Shiori and Marklund, Henrik and Xie, Sang Michael and Zhang, Marvin and Balsubramani, Akshay and Hu, Weihua and Yasunaga, Michihiro and Phillips, Richard Lanas and Gao, Irena and others},
  booktitle={International Conference on Machine Learning},
  pages={5637--5664},
  year={2021},
  organization={PMLR}
}


@article{srivastava2022beyond,
  title={Beyond the Imitation Game: Quantifying and extrapolating the capabilities of language models},
  author={Srivastava, Aarohi and Rastogi, Abhinav and Rao, Abhishek and Shoeb, Abu Awal Md and Abid, Abubakar and Fisch, Adam and Brown, Adam R and Santoro, Adam and Gupta, Aditya and Garriga-Alonso, Adri{\`a} and others},
  journal={arXiv preprint arXiv:2206.04615},
  year={2022}
}


@inproceedings{nie-etal-2020-adversarial,
    title = "Adversarial {NLI}: A New Benchmark for Natural Language Understanding",
    author = "Nie, Yixin  and
      Williams, Adina  and
      Dinan, Emily  and
      Bansal, Mohit  and
      Weston, Jason  and
      Kiela, Douwe",
    booktitle = "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.acl-main.441",
    doi = "10.18653/v1/2020.acl-main.441",
    pages = "4885--4901",
}

@article{kiela2021dynabench,
  title={Dynabench: Rethinking benchmarking in NLP},
  author={Kiela, Douwe and Bartolo, Max and Nie, Yixin and Kaushik, Divyansh and Geiger, Atticus and Wu, Zhengxuan and Vidgen, Bertie and Prasad, Grusha and Singh, Amanpreet and Ringshia, Pratik and others},
  journal={arXiv preprint arXiv:2104.14337},
  year={2021}
}

@inproceedings{
advglue,
title={Adversarial {GLUE}: A Multi-Task Benchmark for Robustness Evaluation of Language Models},
author={Boxin Wang and Chejian Xu and Shuohang Wang and Zhe Gan and Yu Cheng and Jianfeng Gao and Ahmed Hassan Awadallah and Bo Li},
booktitle={Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track (Round 2)},
year={2021},
url={https://openreview.net/forum?id=GF9cSKI3A_q}
}

@article{wenzel2022assaying,
  title={Assaying out-of-distribution generalization in transfer learning},
  author={Wenzel, Florian and Dittadi, Andrea and Gehler, Peter Vincent and Simon-Gabriel, Carl-Johann and Horn, Max and Zietlow, Dominik and Kernert, David and Russell, Chris and Brox, Thomas and Schiele, Bernt and others},
  journal={arXiv preprint arXiv:2207.09239},
  year={2022}
}

@article{teney2022id,
  title={ID and OOD Performance Are Sometimes Inversely Correlated on Real-world Datasets},
  author={Teney, Damien and Lin, Yong and Oh, Seong Joon and Abbasnejad, Ehsan},
  journal={arXiv preprint arXiv:2209.00613},
  year={2022}
}

@article{fewglue,
  title={It's not just size that matters: Small language models are also few-shot learners},
  author={Schick, Timo and Sch{\"u}tze, Hinrich},
  journal={arXiv preprint arXiv:2009.07118},
  year={2020}
}

@inproceedings{fewnlu,
    title = "{F}ew{NLU}: Benchmarking State-of-the-Art Methods for Few-Shot Natural Language Understanding",
    author = "Zheng, Yanan  and
      Zhou, Jing  and
      Qian, Yujie  and
      Ding, Ming  and
      Liao, Chonghua  and
      Jian, Li  and
      Salakhutdinov, Ruslan  and
      Tang, Jie  and
      Ruder, Sebastian  and
      Yang, Zhilin",
    booktitle = "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = may,
    year = "2022",
    address = "Dublin, Ireland",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.acl-long.38",
    doi = "10.18653/v1/2022.acl-long.38",
    pages = "501--516",
}

@misc{ruder2021challenges,
  title={Challenges and Opportunities in NLP Benchmarking},
  author={Ruder, Sebastian},
  year={2021}
}

@inproceedings{sagawa2020investigation,
  title={An investigation of why overparameterization exacerbates spurious correlations},
  author={Sagawa, Shiori and Raghunathan, Aditi and Koh, Pang Wei and Liang, Percy},
  booktitle={International Conference on Machine Learning},
  pages={8346--8356},
  year={2020},
  organization={PMLR}
}

@inproceedings{probing2019acl,
	title = "Probing Neural Network Comprehension of Natural Language Arguments",
	author = "Niven, Timothy  and
	Kao, Hung-Yu",
	booktitle = "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
	month = jul,
	year = "2019",
	address = "Florence, Italy",
	publisher = "Association for Computational Linguistics",
	url = "https://www.aclweb.org/anthology/P19-1459",
	doi = "10.18653/v1/P19-1459",
	pages = "4658--4664",
	abstract = "We are surprised to find that BERT{'}s peak performance of 77{\%} on the Argument Reasoning Comprehension Task reaches just three points below the average untrained human baseline. However, we show that this result is entirely accounted for by exploitation of spurious statistical cues in the dataset. We analyze the nature of these cues and demonstrate that a range of models all exploit them. This analysis informs the construction of an adversarial dataset on which all models achieve random accuracy. Our adversarial dataset provides a more robust assessment of argument comprehension and should be adopted as the standard in future work.",
}

@inproceedings{ribeiro2020beyond,
  title={Beyond Accuracy: Behavioral Testing of NLP Models with CheckList},
  author={Ribeiro, Marco Tulio and Wu, Tongshuang and Guestrin, Carlos and Singh, Sameer},
  booktitle={Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics},
  pages={4902--4912},
  year={2020}
}

@article{ibrahim2022robustness,
  title={The Robustness Limits of SoTA Vision Models to Natural Variation},
  author={Ibrahim, Mark and Garrido, Quentin and Morcos, Ari and Bouchacourt, Diane},
  journal={arXiv preprint arXiv:2210.13604},
  year={2022}
}

@article{gagnon2022woods,
  title={WOODS: Benchmarks for Out-of-Distribution Generalization in Time Series Tasks},
  author={Gagnon-Audet, Jean-Christophe and Ahuja, Kartik and Darvishi-Bayazi, Mohammad-Javad and Dumas, Guillaume and Rish, Irina},
  journal={arXiv preprint arXiv:2203.09978},
  year={2022}
}

@article{wang2021measure,
  title={Measure and Improve Robustness in NLP Models: A Survey},
  author={Wang, Xuezhi and Wang, Haohan and Yang, Diyi},
  journal={arXiv preprint arXiv:2112.08313},
  year={2021}
}

@article{wang2021identifying,
  title={Identifying and mitigating spurious correlations for improving robustness in nlp models},
  author={Wang, Tianlu and Yang, Diyi and Wang, Xuezhi},
  journal={arXiv preprint arXiv:2110.07736},
  year={2021}
}

@article{chen2022adaprompt,
  title={AdaPrompt: Adaptive Model Training for Prompt-based NLP},
  author={Chen, Yulong and Liu, Yang and Dong, Li and Wang, Shuohang and Zhu, Chenguang and Zeng, Michael and Zhang, Yue},
  journal={arXiv preprint arXiv:2202.04824},
  year={2022}
}

@inproceedings{zhou2020domain,
  title={Domain Generalization with MixStyle},
  author={Zhou, Kaiyang and Yang, Yongxin and Qiao, Yu and Xiang, Tao},
  booktitle={International Conference on Learning Representations},
  year={2020}
}

@inproceedings{
raji2021ai,
title={{AI} and the Everything in the Whole Wide World Benchmark},
author={Inioluwa Deborah Raji and Emily Denton and Emily M. Bender and Alex Hanna and Amandalynne Paullada},
booktitle={Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track (Round 2)},
year={2021},
url={https://openreview.net/forum?id=j6NxpQbREA1}
}

@article{deyoung2019eraser,
  title={ERASER: A benchmark to evaluate rationalized NLP models},
  author={DeYoung, Jay and Jain, Sarthak and Rajani, Nazneen Fatema and Lehman, Eric and Xiong, Caiming and Socher, Richard and Wallace, Byron C},
  journal={arXiv preprint arXiv:1911.03429},
  year={2019}
}

@article{khanuja2020gluecos,
  title={GLUECoS: An evaluation benchmark for code-switched NLP},
  author={Khanuja, Simran and Dandapat, Sandipan and Srinivasan, Anirudh and Sitaram, Sunayana and Choudhury, Monojit},
  journal={arXiv preprint arXiv:2004.12376},
  year={2020}
}

@article{kanakarajan2021small,
  title={Small-Bench NLP: Benchmark for small single GPU trained models in Natural Language Processing},
  author={Kanakarajan, Kamal Raj and Kundumani, Bhuvana and Sankarasubbu, Malaikannan},
  journal={arXiv preprint arXiv:2109.10847},
  year={2021}
}

@inproceedings{miller2021accuracy,
  title={Accuracy on the Line: on the Strong Correlation Between Out-of-Distribution and In-Distribution Generalization},
  author={Miller, John P and Taori, Rohan and Raghunathan, Aditi and Sagawa, Shiori and Koh, Pang Wei and Shankar, Vaishaal and Liang, Percy and Carmon, Yair and Schmidt, Ludwig},
  booktitle={ICML},
  year={2021}
}

@inproceedings{gardner2020evaluating,
  title={Evaluating Models’ Local Decision Boundaries via Contrast Sets},
  author={Gardner, Matt and Artzi, Yoav and Basmov, Victoria and Berant, Jonathan and Bogin, Ben and Chen, Sihao and Dasigi, Pradeep and Dua, Dheeru and Elazar, Yanai and Gottumukkala, Ananth and others},
  booktitle={Findings of the Association for Computational Linguistics: EMNLP 2020},
  pages={1307--1323},
  year={2020}
}

@article{bommasani2021opportunities,
  title={On the opportunities and risks of foundation models},
  author={Bommasani, Rishi and Hudson, Drew A and Adeli, Ehsan and Altman, Russ and Arora, Simran and von Arx, Sydney and Bernstein, Michael S and Bohg, Jeannette and Bosselut, Antoine and Brunskill, Emma and others},
  journal={arXiv preprint arXiv:2108.07258},
  year={2021}
}

@article{zheng2020out,
  title={Out-of-domain detection for natural language understanding in dialog systems},
  author={Zheng, Yinhe and Chen, Guanyi and Huang, Minlie},
  journal={IEEE/ACM Transactions on Audio, Speech, and Language Processing},
  volume={28},
  pages={1198--1209},
  year={2020},
  publisher={IEEE}
}

@article{arora2021types,
  title={Types of Out-of-Distribution Texts and How to Detect Them},
  author={Udit Arora and William Huang and He He},
  journal={ArXiv},
  year={2021},
  volume={abs/2109.06827}
}

@inproceedings{shen2021enhancing,
  title={Enhancing the generalization for Intent Classification and Out-of-Domain Detection in SLU},
  author={Shen, Yilin and Hsu, Yen-Chang and Ray, Avik and Jin, Hongxia},
  booktitle={Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)},
  pages={2443--2453},
  year={2021}
}

@article{lu2022rationale,
  title={A Rationale-Centric Framework for Human-in-the-loop Machine Learning},
  author={Lu, Jinghui and Yang, Linyi and Mac Namee, Brian and Zhang, Yue},
  journal={arXiv preprint arXiv:2203.12918},
  year={2022}
}

@article{lei2016rationalizing,
  title={Rationalizing neural predictions},
  author={Lei, Tao and Barzilay, Regina and Jaakkola, Tommi},
  journal={arXiv preprint arXiv:1606.04155},
  year={2016}
}

@inproceedings{ribeiro2019red,
  title={Are Red Roses Red? Evaluating Consistency of Question-Answering Models},
  author={Ribeiro, Marco Tulio and Guestrin, Carlos and Singh, Sameer},
  booktitle={Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics},
  pages={6174--6184},
  year={2019}
}

%%%% Multi-source domain generalization %%%%

@article{balaji2018metareg,
  title={Metareg: Towards domain generalization using meta-regularization},
  author={Balaji, Yogesh and Sankaranarayanan, Swami and Chellappa, Rama},
  journal={Advances in neural information processing systems},
  volume={31},
  year={2018}
}

@article{dou2019domain,
  title={Domain generalization via model-agnostic learning of semantic features},
  author={Dou, Qi and Coelho de Castro, Daniel and Kamnitsas, Konstantinos and Glocker, Ben},
  journal={Advances in Neural Information Processing Systems},
  volume={32},
  year={2019}
}

@inproceedings{vu2022domain,
  title={Domain Generalisation of NMT: Fusing Adapters with Leave-One-Domain-Out Training},
  author={Vu, Thuy and Khadivi, Shahram and Phung, Dinh and Haffari, Gholamreza},
  booktitle={Findings of the Association for Computational Linguistics: ACL 2022},
  pages={582--588},
  year={2022}
}

%%%%%%% Models %%%%%%%

@article{bert,
  title={Bert: Pre-training of deep bidirectional transformers for language understanding},
  author={Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  journal={arXiv preprint arXiv:1810.04805},
  year={2018}
}

@article{roberta,
  title={Roberta: A robustly optimized bert pretraining approach},
  author={Liu, Yinhan and Ott, Myle and Goyal, Naman and Du, Jingfei and Joshi, Mandar and Chen, Danqi and Levy, Omer and Lewis, Mike and Zettlemoyer, Luke and Stoyanov, Veselin},
  journal={arXiv preprint arXiv:1907.11692},
  year={2019}
}

@inproceedings{xlnet,
	author = {Yang, Zhilin and Dai, Zihang and Yang, Yiming and Carbonell, Jaime and Salakhutdinov, Russ R and Le, Quoc V},
	booktitle = {Advances in Neural Information Processing Systems},
	editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
	pages = {},
	publisher = {Curran Associates, Inc.},
	title = {XLNet: Generalized Autoregressive Pretraining for Language Understanding},
	url = {https://proceedings.neurips.cc/paper/2019/file/dc6a7e655d7e5840e66733e9ee67cc69-Paper.pdf},
	volume = {32},
	year = {2019}
}

@inproceedings{bart,
  title={BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension},
  author={Lewis, Mike and Liu, Yinhan and Goyal, Naman and Ghazvininejad, Marjan and Mohamed, Abdelrahman and Levy, Omer and Stoyanov, Veselin and Zettlemoyer, Luke},
  booktitle={Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics},
  pages={7871--7880},
  year={2020}
}

@article{t5,
  author  = {Colin Raffel and Noam Shazeer and Adam Roberts and Katherine Lee and Sharan Narang and Michael Matena and Yanqi Zhou and Wei Li and Peter J. Liu},
  title   = {Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer},
  journal = {Journal of Machine Learning Research},
  year    = {2020},
  volume  = {21},
  number  = {140},
  pages   = {1-67},
  url     = {http://jmlr.org/papers/v21/20-074.html}
}

@article{gpt2,
	title={Language Models are Unsupervised Multitask Learners},
	author={Radford, Alec and Wu, Jeff and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya},
	year={2019}
}



@article{lstm,
	title={Long short-term memory},
	author={Hochreiter, Sepp and Schmidhuber, J{\"u}rgen},
	journal={Neural computation},
	volume={9},
	number={8},
	pages={1735--1780},
	year={1997},
	publisher={MIT Press}
}

@inproceedings{albert,
  author    = {Zhenzhong Lan and
               Mingda Chen and
               Sebastian Goodman and
               Kevin Gimpel and
               Piyush Sharma and
               Radu Soricut},
  title     = {{ALBERT:} {A} Lite {BERT} for Self-supervised Learning of Language
               Representations},
  booktitle = {8th International Conference on Learning Representations, {ICLR} 2020,
               Addis Ababa, Ethiopia, April 26-30, 2020},
  publisher = {OpenReview.net},
  year      = {2020},
  url       = {https://openreview.net/forum?id=H1eA7AEtvS},
  timestamp = {Thu, 07 May 2020 17:11:48 +0200},
  biburl    = {https://dblp.org/rec/conf/iclr/LanCGGSS20.bib}
}

@article{distilbert,
  title={DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter},
  author={Sanh, Victor and Debut, Lysandre and Chaumond, Julien and Wolf, Thomas},
  journal={arXiv preprint arXiv:1910.01108},
  year={2019}
}

@article{hupkes2022state,
  title={State-of-the-art generalisation research in NLP: a taxonomy and review},
  author={Hupkes, Dieuwke and Giulianelli, Mario and Dankers, Verna and Artetxe, Mikel and Elazar, Yanai and Pimentel, Tiago and Christodoulopoulos, Christos and Lasri, Karim and Saphra, Naomi and Sinclair, Arabella and others},
  journal={arXiv preprint arXiv:2210.03050},
  year={2022}
}

@article{zhang2018multi,
  title={Multi-label transfer learning for multi-relational semantic similarity},
  author={Zhang, Li and Wilson, Steven R and Mihalcea, Rada},
  journal={arXiv preprint arXiv:1805.12501},
  year={2018}
}

@article{williams2017broad,
  title={A broad-coverage challenge corpus for sentence understanding through inference},
  author={Williams, Adina and Nangia, Nikita and Bowman, Samuel R},
  journal={arXiv preprint arXiv:1704.05426},
  year={2017}
}

@article{bowman2015large,
  title={A large annotated corpus for learning natural language inference},
  author={Bowman, Samuel R and Angeli, Gabor and Potts, Christopher and Manning, Christopher D},
  journal={arXiv preprint arXiv:1508.05326},
  year={2015}
}

@inproceedings{bentivogli2009fifth,
  title={The Fifth PASCAL Recognizing Textual Entailment Challenge.},
  author={Bentivogli, Luisa and Clark, Peter and Dagan, Ido and Giampiccolo, Danilo},
  booktitle={TACL},
  year={2009}
}

@inproceedings{dolan-2005-automatically,
    title = "Automatically Constructing a Corpus of Sentential Paraphrases",
    author = "Dolan, William B.  and
      Brockett, Chris",
    booktitle = "Proceedings of the Third International Workshop on Paraphrasing ({IWP}2005)",
    year = "2005",
    url = "https://aclanthology.org/I05-5002",
}

@inproceedings{wang2017bilateral,
  title={Bilateral multi-perspective matching for natural language sentences},
  author={Wang, Zhiguo and Hamza, Wael and Florian, Radu},
  booktitle={Proceedings of the 26th International Joint Conference on Artificial Intelligence},
  pages={4144--4150},
  year={2017}
}

@inproceedings{mccoy2019right,
  title={Right for the Wrong Reasons: Diagnosing Syntactic Heuristics in Natural Language Inference},
  author={McCoy, Tom and Pavlick, Ellie and Linzen, Tal},
  booktitle={Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics},
  pages={3428--3448},
  year={2019}
}

@article{jacovi2020towards,
  title={Towards faithfully interpretable NLP systems: How should we define and evaluate faithfulness?},
  author={Jacovi, Alon and Goldberg, Yoav},
  journal={arXiv preprint arXiv:2004.03685},
  year={2020}
}

@inproceedings{petroni2021kilt,
  title={KILT: a Benchmark for Knowledge Intensive Language Tasks},
  author={Petroni, Fabio and Piktus, Aleksandra and Fan, Angela and Lewis, Patrick and Yazdani, Majid and De Cao, Nicola and Thorne, James and Jernite, Yacine and Karpukhin, Vladimir and Maillard, Jean and others},
  booktitle={Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies},
  pages={2523--2544},
  year={2021}
}

@inproceedings{blasi22acl,
    title = {Systematic Inequalities in Language Technology Performance across the World’s Languages},
    author = {Damian E Blasi and Antonios Anastasopoulos and Graham Neubig},
    booktitle = {Annual Conference of the Association for Computational Linguistics (ACL)},
    address = {Dublin, Ireland},
    month = {May},
    url = {https://arxiv.org/abs/2110.06733},
    year = {2022}
}

@inproceedings{deyoung2020eraser,
  title={ERASER: A Benchmark to Evaluate Rationalized NLP Models},
  author={DeYoung, Jay and Jain, Sarthak and Rajani, Nazneen Fatema and Lehman, Eric and Xiong, Caiming and Socher, Richard and Wallace, Byron C},
  booktitle={ACL},
  year={2020}
}

@inproceedings{shen2022shortest,
  title={Are Shortest Rationales the Best Explanations for Human Understanding?},
  author={Shen, Hua and Wu, Tongshuang and Guo, Wenbo and Huang, Ting-Hao},
  booktitle={Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)},
  pages={10--19},
  year={2022}
}

@article{howard2022neurocounterfactuals,
  title={NeuroCounterfactuals: Beyond Minimal-Edit Counterfactuals for Richer Data Augmentation},
  author={Howard, Phillip and Singer, Gadi and Lal, Vasudev and Choi, Yejin and Swayamdipta, Swabha},
  journal={arXiv preprint arXiv:2210.12365},
  year={2022}
}

@article{wu2021polyjuice,
  title={Polyjuice: Generating counterfactuals for explaining, evaluating, and improving models},
  author={Wu, Tongshuang and Ribeiro, Marco Tulio and Heer, Jeffrey and Weld, Daniel S},
  journal={arXiv preprint arXiv:2101.00288},
  year={2021}
}

@article{friedman1940comparison,
  title={A comparison of alternative tests of significance for the problem of m rankings},
  author={Friedman, Milton},
  journal={The Annals of Mathematical Statistics},
  volume={11},
  number={1},
  pages={86--92},
  year={1940},
  publisher={JSTOR}
}

@article{mehri2020dialoglue,
  title={Dialoglue: A natural language understanding benchmark for task-oriented dialogue},
  author={Mehri, Shikib and Eric, Mihail and Hakkani-Tur, Dilek},
  journal={arXiv preprint arXiv:2009.13570},
  year={2020}
}

@article{xu2021fewclue,
  title={Fewclue: A chinese few-shot learning evaluation benchmark},
  author={Xu, Liang and Lu, Xiaojing and Yuan, Chenyang and Zhang, Xuanwei and Xu, Huilin and Yuan, Hu and Wei, Guoao and Pan, Xiang and Tian, Xin and Qin, Libo and others},
  journal={arXiv preprint arXiv:2107.07498},
  year={2021}
}

@article{xu2020clue,
  title={CLUE: A Chinese language understanding evaluation benchmark},
  author={Xu, Liang and Hu, Hai and Zhang, Xuanwei and Li, Lu and Cao, Chenjie and Li, Yudong and Xu, Yechen and Sun, Kai and Yu, Dian and Yu, Cong and others},
  journal={arXiv preprint arXiv:2004.05986},
  year={2020}
}

@article{wang2022pre,
  title={Pre-Training a Graph Recurrent Network for Language Representation},
  author={Wang, Yile and Yang, Linyi and Teng, Zhiyang and Zhou, Ming and Zhang, Yue},
  journal={arXiv preprint arXiv:2209.03834},
  year={2022}
}

@inproceedings{varshney2022investigating,
  title={Investigating Selective Prediction Approaches Across Several Tasks in IID, OOD, and Adversarial Settings},
  author={Varshney, Neeraj and Mishra, Swaroop and Baral, Chitta},
  booktitle={Findings of the Association for Computational Linguistics: ACL 2022},
  pages={1995--2002},
  year={2022}
}

@article{suzgun2022challenging,
  title={Challenging BIG-Bench tasks and whether chain-of-thought can solve them},
  author={Suzgun, Mirac and Scales, Nathan and Sch{\"a}rli, Nathanael and Gehrmann, Sebastian and Tay, Yi and Chung, Hyung Won and Chowdhery, Aakanksha and Le, Quoc V and Chi, Ed H and Zhou, Denny and others},
  journal={arXiv preprint arXiv:2210.09261},
  year={2022}
}

@article{qiu2020pre,
  title={Pre-trained models for natural language processing: A survey},
  author={Qiu, Xipeng and Sun, Tianxiang and Xu, Yige and Shao, Yunfan and Dai, Ning and Huang, Xuanjing},
  journal={Science China Technological Sciences},
  volume={63},
  number={10},
  pages={1872--1897},
  year={2020},
  publisher={Springer}
}

@article{electra,
  title={Electra: Pre-training text encoders as discriminators rather than generators},
  author={Clark, Kevin and Luong, Minh-Thang and Le, Quoc V and Manning, Christopher D},
  journal={arXiv preprint arXiv:2003.10555},
  year={2020}
}

@inproceedings{newsqa,
    title = "{N}ews{QA}: A Machine Comprehension Dataset",
    author = "Trischler, Adam  and
      Wang, Tong  and
      Yuan, Xingdi  and
      Harris, Justin  and
      Sordoni, Alessandro  and
      Bachman, Philip  and
      Suleman, Kaheer",
    booktitle = "Proceedings of the 2nd Workshop on Representation Learning for {NLP}",
    month = aug,
    year = "2017",
    address = "Vancouver, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/W17-2623",
    doi = "10.18653/v1/W17-2623",
    pages = "191--200",
    abstract = "We present NewsQA, a challenging machine comprehension dataset of over 100,000 human-generated question-answer pairs. Crowdworkers supply questions and answers based on a set of over 10,000 news articles from CNN, with answers consisting of spans of text in the articles. We collect this dataset through a four-stage process designed to solicit exploratory questions that require reasoning. Analysis confirms that NewsQA demands abilities beyond simple word matching and recognizing textual entailment. We measure human performance on the dataset and compare it to several strong neural models. The performance gap between humans and machines (13.3{\%} F1) indicates that significant progress can be made on NewsQA through future research. The dataset is freely available online.",
}

@article{fleiss1973equivalence,
  title={The equivalence of weighted kappa and the intraclass correlation coefficient as measures of reliability},
  author={Fleiss, Joseph L and Cohen, Jacob},
  journal={Educational and psychological measurement},
  volume={33},
  number={3},
  pages={613--619},
  year={1973},
  publisher={Sage Publications Sage CA: Thousand Oaks, CA}
}

@article{camburu2018snli,
  title={e-snli: Natural language inference with natural language explanations},
  author={Camburu, Oana-Maria and Rockt{\"a}schel, Tim and Lukasiewicz, Thomas and Blunsom, Phil},
  journal={Advances in Neural Information Processing Systems},
  volume={31},
  year={2018}
}