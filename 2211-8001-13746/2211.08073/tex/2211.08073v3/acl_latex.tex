% This must be in the first 5 lines to tell arXiv to use pdfLaTeX, which is strongly recommended.
\pdfoutput=1
% In particular, the hyperref package requires pdfLaTeX in order to break URLs across lines.

\documentclass[11pt]{article}

% Remove the "review" option to generate the final version.
\usepackage{authblk}
\usepackage[]{acl}
\renewcommand*{\Affilfont}{\normalsize\normalfont}
\renewcommand*{\Authfont}{\bfseries}

\usepackage{graphicx}
%\usepackage{caption}
%\usepackage{subcaption}
\usepackage{subfigure} 
\usepackage{amsmath}
\usepackage{adjustbox} % 表格的大小调整
\usepackage{amsthm}
\usepackage{booktabs}
\usepackage{algorithm}
\usepackage{algpseudocode}
%\usepackage{algorithmic}
\urlstyle{same}
\usepackage{amssymb}
\usepackage{multirow}
\usepackage{longtable}
\usepackage{comment}
% \usepackage{subcaption} 
\usepackage[normalem]{ulem}
% This assumes your files are encoded as UTF8
\usepackage[utf8]{inputenc}
\usepackage{csquotes}
\usepackage{flushend}
% \usepackage[]{acl}

% Standard package includes
\usepackage{times}
\usepackage{latexsym}

% For proper rendering and hyphenation of words containing Latin characters (including in bib files)
\usepackage[T1]{fontenc}
% For Vietnamese characters
% \usepackage[T5]{fontenc}
% See https://www.latex-project.org/help/documentation/encguide.pdf for other character sets




% This is not strictly necessary, and may be commented out,
% but it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}
\usepackage{xspace}
% \usepackage{graphicx}
\newcommand{\ubold}[1]{\fontseries{b}\selectfont#1}
\newcommand{\shuibai}[1]{\textcolor{blue}{[Shuibai: #1]}} 
\setlength{\belowcaptionskip}{-4pt}
\usepackage{authblk}

% Jindong's comments
\usepackage{todonotes}
\usepackage{xcolor}
\newcommand{\wjdd}[1]{\todo[linecolor=cyan,backgroundcolor=cyan!25,bordercolor=cyan,size=\scriptsize]{(WJD): #1}}
\newcommand{\wjd}[1]{{\color{cyan}{[(WJD): #1]}}}
\newcommand{\wyd}[1]{{\color{red}{[(WYD): #1]}}}
\newcommand{\yly}[1]{{\color{orange}{[(YLY): #1]}}}
\newcommand{\lbq}[1]{{\color{blue}{[(LBQ): #1]}}}
\newcommand{\method}{GLUE-X\xspace}

% If the title and author information does not fit in the area allocated, uncomment the following
%
%\setlength\titlebox{<5cm>}
%
% and set <dim> to something 5cm or larger.
\title{GLUE-X: Evaluating Natural Language Understanding Models from an Out-of-Distribution Generalization Perspective}

% \title{GLUE-X: Evaluating OOD Robustness of Natural Language Understanding Tasks}


%\title{RDL: Human-in-the-loop Rationale-Centric Framework for Static Learning}

% Author information can be set in various styles:
% For several authors from the same institution:
% \author{Author 1 \and Author2 \and Author n \\
%         Address line \\ Address line \\ Address line}
% if the names do not fit well on one line use
%         Author 1 \\ {\bf Author 2} \\ ... \\ {\bf Author n} \\
% For authors from different institutions:
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \And  ... \And
%         Author n \\ Address line \\ ... \\ Address line}
% To start a seperate ``row'' of authors use \AND, as in
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \AND
%         Author 2 \\ Address line \\ ... \\ Address line \And
%         Author 3 \\ Address line \\ ... \\ Address line}

% \author{First Author \\
%   Affiliation / Address line 1 \\
%   Affiliation / Address line 2 \\
%   Affiliation / Address line 3 \\
%   \texttt{email@domain} \\\And
%   Second Author \\
%   Affiliation / Address line 1 \\
%   Affiliation / Address line 2 \\
%   Affiliation / Address line 3 \\
%   \texttt{email@domain} \\}

%\author{
    % Authors
%Anonymous
%    
%}



 
\author{
    % Authors
    \textbf{Linyi Yang}$^{1,2}$\thanks{\ \ Equal contribution. Random order.}, \textbf{Shuibai Zhang}$^{1,4,*}$\thanks{\ \ Work done at Westlake University as an intern.}, \textbf{Libo Qin}$^{1}$, \textbf{Yafu Li}$^{1}$, \textbf{Yidong Wang}$^{1}$, \textbf{Hanmeng Liu}$^{1}$, \\
    \textbf{Jindong Wang}$^{3}$, \textbf{Xing Xie}$^{3}$, \textbf{Yue Zhang}$^{1,2}$
    % \texttt{\{yanglinyi, zhangyue\}@westlake.edu.cn}
}
\affil{$^{1}$ School of Engineering, Westlake University\\
    $^{2}$ Institute of Advanced Technology, Westlake Institute for Advanced Study\\
    $^{3}$ Microsoft Research Asia (MSRA)\\
    $^{4}$ University of Electronic Science and Technology of China\\
    %\texttt{\{yanglinyi, zhangyue\}@westlake.edu.cn}
}


\begin{document}

\maketitle
% \authornote{Corresponding Author}


\begin{abstract}
Pre-trained language models (PLMs) are known to improve the generalization performance of natural language understanding models by leveraging large amounts of data during the pre-training phase. However, the out-of-distribution (OOD) generalization problem remains a challenge in many NLP tasks, limiting the real-world deployment of these methods. This paper presents the first attempt at creating a unified benchmark named \method for evaluating OOD robustness in NLP models, highlighting the importance of OOD robustness and providing insights on how to measure the robustness of a model and how to improve it. The benchmark includes 13 publicly available datasets for OOD testing, and evaluations are conducted on 8 classic NLP tasks over 21 popularly used PLMs. Our findings confirm the need for improved OOD accuracy in NLP tasks, as significant performance degradation was observed in all settings compared to in-distribution (ID) accuracy.\looseness=-1
\end{abstract}

\section{Introduction}

Pre-trained Language Models (PLMs) \cite{qiu2020pre,bommasani2021opportunities} have achieved competitive performance across standard NLP benchmarks \cite{blasi22acl}, such as GLUE \cite{wang2018glue} and SuperGLUE \cite{wang2019superglue}. However, recent studies \cite{gururangan2018annotation,ribeiro2019red,kaushik2020learning,ribeiro2020beyond,ruder2021challenges} show concerns that models are yet not close to achieving proper natural language understanding, essential questions being raised about their robustness \cite{srivastava2020robustness,wang2021measure} and underlying sensitivity to systematic biases \cite{probing2019acl,sagawa2020investigation}.
Such issues manifest in the performance decay, especially for out-of-distribution (OOD) generalization when the test distribution differs from training \cite{arora2021types,malinin2021shifts,hupkes2022state}. 

OOD generalization has been systemically studied for Computer vision (CV) and artificial general intelligence (AGI) \cite{koh2021wilds,srivastava2022beyond}, for which large evaluation datasets are available. While sharing the same aspirational goal, existing evaluations \cite{kaushik2018much,min2019compositional,gardner2020evaluating} and methods~\cite{hendrycks2020pretrained,bommasani2021opportunities} for OOD generalization of NLP contain only one or a few tasks~\cite{wu2021polyjuice,wang2021robustness,howard2022neurocounterfactuals,lu2022rationale}, which do not adequately capture limitations of existing models, resulting in inflated test accuracy \cite{tu2020empirical,ribeiro2020beyond}. Thus it remains a gap in evaluating models in a unified way by executing a range of text classification tasks.

To facilitate research in this direction, we introduce the \method benchmark for evaluating the out-of-distribution performance of PLMs. \method expands upon previous multi-task benchmarks \cite{fewnlu,xu2020clue,xu2021fewclue} by including test data from multiple domains, covering eight standard tasks in GLUE, with an average 2 test domains for each task, allowing comprehensive cross-distribution evaluations. Specifically, \method focuses on domain generalization, where a model trained on a source domain can be directly generalized to target domains without any labeled or unlabeled data from target domains. It also enables the analysis of two main factors affecting the cross-domain generalization performance, namely the pre-trained language model (e.g., architecture, size, etc.) and different training strategies (e.g., fine-tuning, prompt-tuning \cite{chen2022adaprompt}, linear probing \cite{wu2019understanding}, and domain-generalization training \cite{wang2023robustness}).

Using \method, we evaluate the performance of \emph{20} pre-trained language models in a unified setting and under the same experimental conditions. In addition, we consider \emph{3} tuning strategies designed for improving single-source domain generalization: linear probing \cite{tripuraneni2020theory,wu2019understanding}, fine-tuning, and the linear probing then fine-tuning method (LP-FT) \cite{kumar2022finetuning}. Finally, we analyze the internal causes of OOD robustness at the feature level by measuring the rationale overlap between human and model predictions \cite{lei2016rationalizing}.
%\wjd{A schematic diagram or figure to overview what will be covered is good to show our contributions.}

Results show that the average accuracy of PLMs on cross-domain evaluations falls significantly short of human performance, even for the highest-performing model (81.3\% -- human versus 74.6\% -- model). In contrast to the GLUE leaderboard, where over 20 single-model results outperform human baselines, none of the backbones included in \method is able to surpass human performance under the same evaluation setting. These findings suggest the importance of cross-distribution evaluation for natural language processing. In addition, evidence shows that the superior performance of PLMs on GLUE may be relatively superficial and less useful as a performance indicator in practice.

Detailed analysis shows that (1) no one backbone can significantly outperform the others across all tasks, which is consistent with the conclusion \cite{wenzel2022assaying} in the computer vision; (2) surprisingly, the influence of model architectures is somehow more significant than the model parameters towards the OOD robustness; (3) the ID and OOD performance holds a linear correlation in most cases for text classifications; (4) in terms of the tuning strategy, we show that linear probing and then fine-tuning can slightly improve the OOD performance compared to standard fine-tuning. 

To our knowledge, we are the \emph{first} to systemically evaluate natural language understanding systems for cross-distribution generalization on genuine data compared to human performance. More importantly, we make datasets of cross-domain evaluations for all typical text classification tasks, which allows us to report OOD results under the same experimental conditions. We open-source the codebase and datasets \footnote{\url{https://github.com/YangLinyi/GLUE-X}}. The GLUE-X leaderboard is available at \url{https://gluexbenchmark.com/}.


\section{Related Work}

\input{sec-related.tex}

\input{tables/tb-dataset.tex}

\section{Data and Settings}

The goal of \method is to provide an extension of GLUE with the same training data but multifarious OOD test sets.

\subsection{Overview of \method}

The evaluation in \method is intrinsically related to the domain generalization task considering a practical and challenging setting, where a model trained on multiple source domains can be directly generalized to a target domain without any labeled or unlabeled data from
the target domain \cite{muandet2013domain}.  We articulate the following tasks and datasets in \method.

\noindent\textbf{Tasks.} As a benchmark styled after GLUE \cite{wang2018glue}, we consider eight tasks in \method: Sentiment Analysis (\emph{SST-2}), Natural Language Inference (\emph{MNLI, QNLI}), Textual Entailment (\emph{RTE}), Paraphrase (\emph{MRPC, QQP}), Textual Similarity (\emph{STS-B}) and Linguistic Acceptability (\emph{CoLA}). \footnote{The WNLI task is not included in \method since there is no sufficient in-domain data for constructing OOD tests~\cite{wang2022pre,xlnet}.}

\noindent\textbf{Datasets.} \method follows the same in-domain training data and evaluation metrics as GLUE \cite{wang2018glue}. To construct the out-of-domain test, we adopt popular datasets extracted from different domains while keeping the same prediction labels as the original tasks in GLUE. The detailed data statistics are shown in Table~\ref{tab:data_statistic}. 

\subsection{Dataset Curation}

We construct test sets for each task under the requirement that they share the same label types with the training set. To this end, \method contains 13 OOD datasets, including publicly available datasets (Amazon, HANS, etc) and newly collected/re-constructed datasets (Grammar Test and NewsQA). In particular, we select the OOD datasets for each task, including sentiment analysis -- IMDB \cite{maas-etal-2011-learning}, Yelp \cite{zhang2015character}, Amazon \cite{kaushik2020learning}, and Flipkart \cite{flipkart_2023}; linguistic acceptability -- Grammar Test; textual similarity -- SICK \cite{zhang2018multi}; NLI -- MNLI-Mismatched \cite{williams2017broad}, SNLI \cite{bowman2015large}, and SICK \cite{zhang2018multi}; Textual Entailment -- RTE; Paraphrase -- MRPC and QQP \cite{bentivogli2009fifth,dolan-2005-automatically,wang2017bilateral,mccoy2019right}. Regarding the QNLI task, we convert instances from NewsQA \cite{newsqa} to the consistent data format of QNLI for conducting the OOD evaluation. The detailed description of the newly collected dataset, Grammar Test, can be found in Appendix \ref{sec:appendixA}.

SICK contains multiple labels, including textual similarity, also used as an OOD test set of the textual similarity task. We rounded floating number labels of textual similarity to integers from 0 to 4, converting it into a five-class dataset to align with other classification tasks in GLUE-X. In addition, MRPC and QQP are leveraged as OOD datasets of each other as the paraphrasing task. 

\subsection{Metrics}

We first average metrics to get a score for those tasks with multiple metrics. Following GLUE and SuperGLUE, we then report the score of NLU models by averaging the scores of all tasks as the OOD performance.  For rankings, in addition to the robustness rank by considering the decreased ratio between OOD and ID performance, we adopt Friedman rank \cite{friedman1940comparison} over multiple tasks:
\begin{equation*}
\operatorname{rank}_{F}  = \frac{1}{n} \sum_{i=1}^{n} \operatorname{rank}_i,
\end{equation*}
where $n$ is the number of tasks (e.g., $n=8$ in Table~\ref{tab:overall_per}) and $\operatorname{rank}_i$ is the rank of the performance in the $i$-th task considering in the GLUE-X. We report the robustness ranking in terms of the decreased ratio of OOD performance and Friedman rank.

\subsection{Post-hoc Analysis}

In addition to quantitative analysis, we choose two tasks, sentiment analysis, and natural language inference, for post-hoc feature analysis \cite{lei2016rationalizing}. We adopt the sensitivity of contextual decomposition technique \cite{jin2019towards,yang-etal-2021-exploring}, which removes part of inputs from the sequence text to evaluate a model's sensitivity to them, thereby allowing for identifying important features. The output is the overlap between rationales by models and humans, which to some extent represents the trust of models \cite{jacovi2020towards,yang-etal-2020-generating}. 

Formally, given a phrase \emph{p} starting with the negative limitations in the \emph{k-th} document $\mathcal{D}^{(k)}$, we sample the documents which contain the same phrase \emph{p} to alleviate the influence by chance when there are multiple shreds of evidence saturating the prediction. The window size of the phrase \emph{p} is limited to 3. Taking sentiment analysis for example, given \textit{``This movie was so unbelievably bad''} if we only remove the non-causal word \textit{movie}, the prediction is not expected to change for a robust model.


The importance score is computed as follows:
\begin{equation*}
\small
\phi(\mathbf{p}, \widehat{\mathcal{D}^{(k)}})=\mathbb{E}_{\widehat{\mathcal{D}^{(\beta)}}}\left[l\left(\widehat{\mathcal{D}^{(\beta)}}; \widehat{\mathcal{D}}\right)-l\left(\widehat{\mathcal{D}^{(\beta)}} \backslash \mathbf{p} ; \widehat{\mathcal{D}}\right)\right],
\end{equation*}
where \(\mathcal{D}^{(\beta)}\) denotes the resulting text after masking out a single token (phrase) starting with the negative pronoun (un-, non-, etc.) in the length of \(N\) surrounding the phrase \(\mathbf{p}\). We use \(l\left(\widehat{\mathcal{D}^{(\beta)}} \backslash \mathbf{p}; \widehat{\mathcal{D}}\right)\) to represent the model prediction logits of the ground-truth class after replacing the masked-out context. \(\backslash \mathbf{p}\) indicates the operation of masking out the phrase \(p\) in a given document. 

\input{tables/tb-gpucost.tex}

\subsection{Models and Training Strategies}
\noindent\textbf{Models.} To ensure that our results are relevant for both researchers and practitioners, we consider both top-performing model backbones and cost-efficient methods: \emph{Discriminative Models} -- BERT-base, BERT-large \cite{bert}, RoBERTa-base, RoBERTa-large \cite{roberta}, XLNet-base, XLNet-large \cite{xlnet}; \emph{Generative Models} -- BART-base, BART-large \cite{bart}, T5-small, T5-base, T5-large \cite{t5}, GPT2, GPT2-medium, GPT2-large \cite{gpt2}; \emph{Generative and Discriminative Models} -- ELECTRA-small, ELECTRA-base, ELECTRA-large \cite{electra}; \emph{Cost-Efficient Models} -- ALBERT-base \cite{albert}, and DistilBERT-base \cite{distilbert}. We follow the official implementations of several pre-trained language models from Huggingface\footnote{\url{https://huggingface.co/models}} to reproduce results on GLUE using the validation set and test these models on \method. The hyper-parameters of each model are selected by using grid search, which can be found in Appendix B. We also report the results of GPT-3 \cite{gpt3} through in-context learning in Appendix G. 

\noindent\textbf{Fine-tuning Strategies.} We investigate the efficacy of different fine-tuning strategies for OOD generalization. In particular, we consider three paradigms: standard fine-tuning, fine-tuning only the head (linear probing), and linear probing then fine-tuning. The detailed training cost and inference speed estimated by a single V100 are shown in Table \ref{tab:cost}, in which we evaluate the performance using the in- and out-of-domain test data, recording the training cost in GLUE and \method. We use 50 NVIDIA Tesla V100 GPU cards and 8 NVIDIA A100 GPU cards and spend 10,000+ GPU hours based on the estimation with a single V100 card. 

\input{tables/tb-overallper.tex}

\section{Experiments}

We explore the facets of OOD generalization in NLP using \method, highlighting discrepancies to previous findings and discussing their implications.

\subsection{Human Annotation}
We employ human annotators to give predictions on OOD datasets and identify rationales.

\noindent\textbf{Predictions.} 
We use a crowd-sourcing company to recruit editors and annotators to give predictions on 13 OOD datasets. To fairly compare human performance with models, we simulate the models' OOD testing during the manual annotation process. Specifically, annotators are given essential instructions and a few examples from the in-domain dataset that gently guide them to annotate. Then they are asked to label instances from unseen OOD datasets, typically collected from other domains. 1,000 testing samples are used to obtain the human performance for each OOD dataset.

We employ multiple labelers to annotate the same data point during the annotation to ensure the high quality of the crowdsourcing work. In particular, we employ ten people to annotate the SICK dataset as same as the original data \cite{zhang2018multi}. We employ two annotators for labeling the same instance for the other datasets. After the trial phase of data annotation, we set the Inter-Annotator Agreement (IAA) score threshold for each task depending on the difficulty level. Finally, the average IAA over the 13 OOD datasets is 0.864, indicating acceptable agreement. 

\input{tables/tb-detail.tex}

\noindent\textbf{Rationale Marking.} Following \citet{kaushik2020learning} and \citet{kaushik2021learning}, we use extractive explanations for marking rationales that support classification decisions. Inspired by \citet{kaushik2021learning} and \citet{lertvittayakumjorn2021explanation}, we leverage the rationale marking annotated by humans to compare with rationale selected by models on sentiment analysis and natural language inference (NLI) tasks. We ask two labelers to annotate sampled instances from IMDB, Yelp, and Amazon datasets for the sentiment analysis task. At the outset, annotators were given instructions and examples that gently guided them to annotate rationales. Only adjectives, adverbs, nouns, and verbs were considered rationale candidates. Besides, rationales were required to carry complete semantic information. We sampled 6,000 instances for each dataset randomly. Using F1 score, the IAA for IMDB, Yelp and Amazon are 0.874, 0.871, and 0.840, respectively. For NLI, we use the explanation dataset, e-SNLI \cite{camburu2018snli}, to assert models' trust. 

%e-SNLI contains both free-text and extractive explanations, while we only consider the rationale marking to maintain the consistency of evaluations


\subsection{Prediction Results} 

\textbf{Overall Performance on \method.} We report the average score of different models sorted in descending order representing the overall performance in Table \ref{tab:overall_per}. In addition to the overall performance, we provide the Friedman Rank for in- and out-of-domain results. From Table \ref{tab:overall_per}, we observe that all pre-trained models involved in \method show significant performance decay under the OOD test compared to the ID performance (\textbf{20.05\%} decay in average). The results also suggest no significant difference in the OOD robustness between generative and discriminative models for text classification. We also provide the results of GPT-3 with in-context learning in Appendix G since it leverages a different training strategy.

\noindent\textbf{Model-level Analysis.} On the model level, we observe that ELECTRA-large achieves the best performance for both ID (\textbf{89.18\%}) and OOD (\textbf{74.62\%}) tests.  Lightweight models, BERT-base, GPT-2, and DistilBERT-base, are in the bottom three on \method with the lowest OOD performance. In contrast, the base-size ELECTRA and ALBERT achieve comparable generalization results. Moreover, by comparing the Friedman rank of OOD and ID tests in Table \ref{tab:overall_per}, we observe that the fluctuation of the OOD F-rank is slightly lower than the ID F-rank, which hints that the uncertainty of performance has been decreased on \method by using a large amount of the test data.

\noindent\textbf{The Performance of Compressed Models.} The results of \method suggest that OOD generalization still faces fundamental challenges, especially for lightweight models. For example, we find that compressed models (e.g., DistilBERT-base) show relatively low performance compared to others. Differently, the OOD performance of ALBERT-base (11M parameters) is significantly higher than DistilBERT-base (\textbf{65.30\% vs. 61.94\%}), even better than several moderate-sized models (BERT-large, GPT2-medium, and XLNet-base). 

%We measure the MMD distance between the In-domain and Out-of-domain dataset and show details in Appendix \ref{}. We find that the performance decay holds a statistically significant correlation with the distance of distribution shifts. In particular, the generalization for the CoLA dataset is the most challenging task for models since the OOD dataset we selected holds the biggest difference with CoLA. In contrast, models tend to perform better on the relatively easy dataset, such as sentiment analysis (SST-2). As shown in Table \ref{tab:detail}, we show that the best performing model, RoBERTa, can even achieve 94.98\% accuracy for the zero-shot domain generalization of sentiment analysis. Besides, the generative model, such as GPT2, shows its advantage in the task of QQP (60.80\% accuracy achieved by GPT2-large). Note that we do not evaluate the WNLI task since there is no sufficient in-domain data. Also, the OOD dataset of CoLA is newly collected in this work.

%\begin{figure*}[t]
%\centering
%\small
%\includegraphics[width=.75\textwidth]{plots/exp/encoder_bar.png}
% \caption{Encoder-only Average Performance.}\label{fig:encoder_bar}
%\end{figure*}

%\begin{figure*}[t]
%\centering
%\small
%\includegraphics[width=.75\textwidth]{plots/exp/decoder_bar.png}
%\caption{Decoder-only Average Performance.}\label{fig:decoder_bar}
%\end{figure*}

% Please add the following required packages to your document preamble:
% \usepackage{multirow}

% Please add the following required packages to your document preamble:
% \usepackage{multirow}


\subsection{Discussion}

\noindent\textbf{Human vs. Model.} The average performance decay between in- and out-of-domain tests of humans (87.10\% -- ID vs. 81.31\% -- OOD) is significantly lower than models, even for the best-performing model with the lowest performance decay (\textbf{6.65\% vs. 16.33\%}), as shown in Table \ref{tab:detail}. Regarding the average OOD performance, the human baseline is also much higher than the models, with at least an \textbf{6.69\%} increase (81.31\% vs. 74.62\%)\footnote{Note that the human performance of RTE, MRPC, and QQP is still adjusted and will be updated in the next version.}. Such a large performance gap indicates that PLMs cannot achieve competitive results with humans on \method. More specifically, the human baseline outperforms the state-of-the-art results on five of eight tasks. It is noteworthy that we control OOD evaluations of humans in the same experimental setting with models by testing on unseen samples.

\noindent\textbf{OOD Robustness.} As shown in Table \ref{tab:detail}, we suggest that there is no silver bullet towards the OOD robustness, given that no single model can consistently outperform others over all tasks on \method. For example, ELECTRA-large can only achieve the best performance on four of eight tasks. We also find that the generalization for the CoLA dataset is the most challenging task for models since the test set holds the biggest difference with training data. In contrast, models tend to perform better on the relatively easy dataset, such as sentiment analysis (SST-2). For example, the best-performing model ELECTRA-large can achieve a \textbf{94.67\%} accuracy on SST-2 yet only a \textbf{37.85\%} Matthew's Corr on CoLA. Besides, we also observe that the distribution shift between the ID and OOD datasets largely influences the OOD generalization results. In particular, the performance decay on the OOD test is exacerbated by the increase of distribution shifts, as shown in Appendix \ref{sec:appendixC}.

\noindent\textbf{Model Architectures vs. Parameter Size.} The rightmost column of Table \ref{tab:detail} demonstrates the decreased ratio representing the model robustness to some extent. Although the large-sized model, such as T5-large, and RoBERTa-large, can surpass the corresponding base-sized models in terms of the lower decreased ratio, empirical evidence from Table \ref{tab:detail} also shows that model types could be more influential than the parameter size towards the OOD performance. Specifically, as shown in Table \ref{tab:detail}, results of the same architecture with different parameters are closer to the results of similar parameter-size models based on different architectures. For instance, the decreased ratio of T5 architectures pre-training with different parameter sizes (T5: 16.98\% -- large (737M); 18.47\% -- base (223M); 18.57\% -- small (61M)) are close to each other, similar to RoBERTa (18.46\% --large vs. 19.40\% -- base). It hints that designing model architectures and training methods could be one of the future directions for improving OOD robustness.

%However, we also observe that the influence of parameter size is somehow not as important as the model types for classification tasks. For instance, the OOD performance achieved by ELECTRA-base (109M parameters) is higher than GPT2-large (774M parameters) in \textbf{64.15\% vs. 62.76\%}. Meanwhile, the OOD performance of the T5-base model is significantly higher than GPT2-large and BERT-large. 



\input{tables/tb-decayratio.tex}

\begin{figure*}[ht]
\centering
\subfigure[COLA]{
\begin{minipage}[t]{0.32\linewidth}
\centering
\includegraphics[width=\textwidth]{plots/ID_OOD/cola.pdf}
%\caption{fig1}
\end{minipage}%
}%
\subfigure[MNLI]{
\begin{minipage}[t]{0.32\linewidth}
\centering
\includegraphics[width=\textwidth]{plots/ID_OOD/mnli.pdf}
%\caption{fig2}
\end{minipage}%
}
\subfigure[MRPC]{
\begin{minipage}[t]{0.32\linewidth}
\centering
\includegraphics[width=\textwidth]{plots/ID_OOD/mrpc.pdf}
%\caption{fig2}
\end{minipage}%
}
\caption{Scatter figures that illustrate the correlation between ID and OOD performance for different tasks.}
\label{fig:scatter}
\end{figure*}

\begin{figure}[t]
\centering 
\small
{% 
\includegraphics[width=.36\textwidth]{plots/Tuning/roberta-base.pdf} 
}
\caption{The influence of different tuning strategies on the task of MNLI, including Linear probing, Fine-tuning, and Linear probing then Fine-tuning (LP-FT). The results are based on RoBERTa-base.} 
\label{fig:tuning}
\end{figure} 


\noindent\textbf{Robustness vs. Trust.} The average results of the rationale overlap between models and humans for three sentiment analysis tasks are shown in Table \ref{tab:decay_ratio}, indicating the trust measurement. As shown in the table, somehow surprisingly, we find that the best-performing discriminative model -- RoBERTa-large (see Table \ref{tab:overall_per}) achieves the lowest rationale overlap between humans and models. While RoBERTa-large can achieve a relatively high overlap with humans on the NLI task (see Appendix D). This can be because the rationale overlap is largely influenced by datasets.

It is noteworthy that small-sized models can achieve relatively higher rationale overlaps than large-sized models, which is generally consistent with the results reported by previous work \cite{deyoung2020eraser}. For instance, ELECTRA-small achieves the highest F1 score with only 13.48M parameters. In addition, the models pre-trained with the same architectures usually achieve similar performance (e.g., ELECTRA-small and ELECTRA-large, GPT2-medium and GPT2-large).

\noindent\textbf{ID vs. OOD Performance.} We show the correlation of three tasks between the in- and out-of-domain results in Figure \ref{fig:scatter} (the full results can be found at Appendix \ref{sec:appendixD}). Unsurprisingly, we observe that the in-domain performance is always higher than the out-of-domain performance. Specifically, we find that the OOD performance is much lower than the ID performance in the task of COLA. In contrast, the gap between ID and OOD performance based on SST-2 and MNLI is relatively lower than others. We suppose this is partially influenced by the distribution shift between the in- and out-of-domain datasets. 

Regarding the type of pre-trained models, we show that discriminative models show a stronger linear correlation when compared to generative models (19 data points). From the task perspective, we observe that datasets largely influence the correlation between ID and OOD. For instance, ID and OOD performance are inversely correlated on MRPC yet almost correlated on other tasks, hinting that the inverse correlation is possible for the specific task when the size of test samples is limited. 

\noindent\textbf{The Influence of Tuning Methods.} Taking MNLI as an example, we compare the results of RoBERTa-base using three different training strategies in Figure \ref{fig:tuning}. As found by previous work \cite{kumar2022finetuning}, fine-tuning can do worse than linear probing in the presence of a large distribution shift in CV. However, as shown in Figure \ref{fig:tuning}, we find that linear probing methods show relatively low accuracy for both ID and OOD tests, which is different from the conclusion in CV. This can be because freezing pre-trained features hinders the generalization of NLP tasks that are generally more complex than the OOD generalization in CV. While the LP-FT can be relatively helpful for improving the OOD robustness of NLP models in terms of the slight performance improvement compared to the standard fine-tuning method. For this reason, there is still much room to improve in designing methodologies of domain generalization that can improve the OOD robustness for text classification. In addition to tuning methods discussed in \method, the recently emerging trend of the development of large-scale language models (LLMs) represented by ChatGPT is worth paying attention to. In particular, how to appropriately define the OOD generalization for LLMs is still under-explored since the pre-training corpus of these models is not disclosed yet \cite{wang2023robustness}.

\section{Conclusion}

We constructed \method, an OOD robustness benchmark for natural language understanding tasks that aim to enable fair evaluation over multiple datasets from multiple domains in a consistent setting. With \method, we evaluate 21 pre-trained models on 8 classification tasks, providing analysis using 3 different tuning strategies and post-hoc analysis for gaining internal causes for the OOD robustness. We conclude that (1) current PLMs still have a lag much behind human-level towards the OOD robustness; (2) the ID and OOD performance usually hold a linear correlation in most cases, while the coefficiency of the correlation is primarily related to the selection of OOD datasets; (3) stronger architectures can bring decent performance benefit, especially for the OOD performance. 

%In the future, we aim to add results of in-context learning built on top of LLMs (e.g., ChatGPT, LaMDA, etc.) and make \method a continuously maintained project.

\section*{Limitation}

Our primary focus is on the OOD robustness of text classification tasks. However, there are other NLP tasks that the community should not ignore. \method currently does not include language generation tasks such as machine translation, summarization, and dialogue. Moreover, extending the current \method to more real-world datasets from different domains is of great importance. We aim to make \method a continuously maintained project.

\section*{Ethics Statement}

This paper honors the ACL Code of Ethics. Public available datasets are used to establish the GLUE-X leaderboard. No private data was used. All annotators from the crowdsourcing company have received enough labor fees corresponding to their amount of annotated instances. The code and data are open-sourced under the CC-BY-NC-SA license.

\section*{Acknowledgement}
We acknowledge with thanks Wei Zhou from Zhejiang University, who help us build the website, as well as the many others who have helped. We would also like to thank anonymous reviewers for their insightful comments and suggestions to help improve the paper, especially for Reviewer 2. This publication has emanated from research conducted with the financial support of the Pioneer and ``Leading Goose" R\&D Program of Zhejiang under Grant Number 2022SDXHDX0003 and the 72nd round of the Chinese Post-doctoral Science Foundation project 2022M722836. Yue Zhang is the corresponding author.


\bibliography{custom}
\bibliographystyle{acl_natbib}

\newpage
\appendix
% \input{tables/tb-rationale_imdb.tex}
% \input{tables/tb-rationale_yelp.tex}
% \input{tables/tb-rationale_amazon.tex}
% \input{tables/tb-rationale_snli.tex}
%%%%%
%\begin{minipage}{0.2\textwidth}
%  \centering
%  \input{tables/tb-rationale_imdb.tex}
%\end{minipage}%
%\begin{minipage}{0.2\textwidth}
%\input{tables/tb-rationale_yelp.tex}
%\end{minipage}

%\begin{minipage}{0.2\textwidth}
%  \centering
%\input{tables/tb-rationale_amazon.tex}
%\end{minipage}%
%\begin{minipage}{0.2\textwidth}
%  \centering
%\input{tables/tb-rationale_snli.tex}
%\end{minipage}


%%%%%%


% Table \ref{tab:rationale_amazon}

% Table \ref{tab:rationale_yelp}
%%%%%%%%%%
% \begin{table*}[t]
% \centering
% \small
% \begin{tabular}{lllllllllll}
% \hline
% \multirow{2}{*}{Model} & \multicolumn{1}{l}{SST-2} & \multicolumn{1}{l}{MNLI} & \multicolumn{1}{l}{RTE} & \multicolumn{1}{l}{MRPC} & \multicolumn{1}{l}{QQP} & \multicolumn{1}{l}{STS-B} & \multicolumn{1}{l}{CoLA} & \multicolumn{1}{l}{QNLI} & \multicolumn{1}{l}{Avg} & Avg       \\
%                        & \multicolumn{1}{l}{OOD}   & \multicolumn{1}{l}{OOD}  & \multicolumn{1}{l}{OOD} & \multicolumn{1}{l}{OOD}  & \multicolumn{1}{l}{OOD} & \multicolumn{1}{l}{OOD}   & \multicolumn{1}{l}{OOD}  & \multicolumn{1}{l}{OOD} & $\Delta$↓ \\ \hline
% \multicolumn{10}{l}{State-of-the-art Pre-trained Language Models}                                                                                                                                                                                         \\ \hline
%%%%%%%%%%%



\section{Data Collection} \label{sec:appendixA}
We derive the CoLA-OOD dataset from the Public High School English Exam, which contains 304,277 examples. The original multi-choice fill-in tests are converted into COLA-style, with correct answers as positive examples and incorrect answers as negative examples. The golden answer is given by the English teacher who is a native speaker or holds an English Teaching degree. We collect the data from publicly available internet resources, and the original open-access materials can be found from \url{https://www.koolearn.com/shiti}.

The input of the CoLA-OOD dataset, Grammar Test, is a text span containing a QA pair or a few sentences. The ground truth of the output is to decide whether the grammar of the sentence is acceptable or not. For example, given the sentence `Is there a post office near here?  Yes, there isn't .', the label is unacceptable since there is a grammar error existing in the input. Otherwise, for a sentence without any grammar errors, `The young man is the CEO of the company, In other words, he is in charge of the company.', the corresponding label is acceptable. 

\section{Training Details} \label{sec:appendixB}

We illustrate the cross-domain evaluation settings used for \method in Figure \ref{fig:settings}. Notably, the source domain only contains a single dataset, while target domains can include more than one dataset from multiple domains.

\begin{figure}[htbp]
\centering 
{% 
\includegraphics[width=.45\textwidth]{plots/settings.pdf} 
}
\caption{The demonstration of training and testing settings used for cross-domain evaluations in \method.} 
\label{fig:settings}
\end{figure} 

Regarding the training, we performed the grid search for each task, kept the best-performing checkpoint in ID datasets, and tested their performance on their corresponding OOD datasets. The hyperparameters used by these weights can be seen in Table \ref{tab:Hyperparameters}.

\section{Domain Distributions} \label{sec:appendixC}
We evaluate distribution shifts between different datasets regarding Maximum Mean Discrepancy(MMD) and Word Overlap Rate. MMD distance focuses on the semantic distribution shift between datasets, while Word Overlap Rate pays more attention to superficial similarity.
\input{tables/tb-rationale_snli.tex}

\begin{figure*}[t]
\centering

\subfigure[COLA]{
\begin{minipage}[t]{0.25\linewidth}
\centering
\includegraphics[width=.9\textwidth]{plots/overlap_figures/cola.pdf}
%\caption{fig1}
\end{minipage}%
}%
\subfigure[MNLI]{
\begin{minipage}[t]{0.25\linewidth}
\centering
\includegraphics[width=.9\textwidth]{plots/overlap_figures/mnli.pdf}
%\caption{fig2}
\end{minipage}%
}%
\subfigure[MRPC]{
\begin{minipage}[t]{0.25\linewidth}
\centering
\includegraphics[width=.9\textwidth]{plots/overlap_figures/mrpc.pdf}
%\caption{fig1}
\end{minipage}%
}%
\subfigure[QQP]{
\begin{minipage}[t]{0.25\linewidth}
\centering
\includegraphics[width=.9\textwidth]{plots/overlap_figures/qqp.pdf}
%\caption{fig2}
\end{minipage}%
}%
                 %这个回车键很重要 \quad也可以
                 
\subfigure[RTE]{
\begin{minipage}[t]{0.25\linewidth}
\centering
\includegraphics[width=.9\textwidth]{plots/overlap_figures/rte.pdf}
%\caption{fig2}
\end{minipage}
}%
\subfigure[SST-2]{
\begin{minipage}[t]{0.25\linewidth}
\centering
\includegraphics[width=.9\textwidth]{plots/overlap_figures/sst2.pdf}
%\caption{fig2}
\end{minipage}
}%
\subfigure[STSB]{
\begin{minipage}[t]{0.25\linewidth}
\centering
\includegraphics[width=.9\textwidth]{plots/overlap_figures/stsb.pdf}
%\caption{fig2}
\end{minipage}
}%
\subfigure[QNLI]{
\begin{minipage}[t]{0.25\linewidth}
\centering
\includegraphics[width=.9\textwidth]{plots/overlap_figures/qnli.pdf}
%\caption{fig2}
\end{minipage}
}%
\centering
\caption{The word-level overlap between the training set and test set for each task.}
\label{fig:Word_Overlap}
\end{figure*}

\begin{figure*}[t]
\centering

\subfigure[COLA]{
\begin{minipage}[t]{0.25\linewidth}
\centering
\includegraphics[width=.9\textwidth]{plots/MMD_scores_figures/cola.pdf}
%\caption{fig1}
\end{minipage}%
}%
\subfigure[MNLI]{
\begin{minipage}[t]{0.25\linewidth}
\centering
\includegraphics[width=.9\textwidth]{plots/MMD_scores_figures/mnli.pdf}
%\caption{fig2}
\end{minipage}%
}%
\subfigure[MRPC]{
\begin{minipage}[t]{0.25\linewidth}
\centering
\includegraphics[width=.9\textwidth]{plots/MMD_scores_figures/mrpc.pdf}
%\caption{fig1}
\end{minipage}%
}%
\subfigure[QQP]{
\begin{minipage}[t]{0.25\linewidth}
\centering
\includegraphics[width=.9\textwidth]{plots/MMD_scores_figures/qqp.pdf}
%\caption{fig2}
\end{minipage}%
}%
                 %这个回车键很重要 \quad也可以
                 
\subfigure[RTE]{
\begin{minipage}[t]{0.25\linewidth}
\centering
\includegraphics[width=.9\textwidth]{plots/MMD_scores_figures/rte.pdf}
%\caption{fig2}
\end{minipage}
}%
\subfigure[SST-2]{
\begin{minipage}[t]{0.25\linewidth}
\centering
\includegraphics[width=.9\textwidth]{plots/MMD_scores_figures/sst2.pdf}
%\caption{fig2}
\end{minipage}
}%
\subfigure[STSB]{
\begin{minipage}[t]{0.25\linewidth}
\centering
\includegraphics[width=.9\textwidth]{plots/MMD_scores_figures/stsb.pdf}
%\caption{fig2}
\end{minipage}
}%
\subfigure[QNLI]{
\begin{minipage}[t]{0.25\linewidth}
\centering
\includegraphics[width=.9\textwidth]{plots/MMD_scores_figures/qnli.pdf}
%\caption{fig2}
\end{minipage}
}%
\centering
\caption{The MMD Scores between the training set and test set for each task. Lower MMD score means the higher correlation between datasets.}
\label{fig:MMD_scores}
\end{figure*}

\subsection{Word Overlap}

The similarity between datasets of In-distribution datasets and Out-of-distribution datasets are shown in Figure \ref{fig:Word_Overlap}.


\subsection{MMD Distance}
The MMD distance between ID and OOD datasets is shown in Figure \ref{fig:MMD_scores} for each task including in \method. When computing the MMD distance between two datasets, we ensure that the same number of sentences are sampled and fed into PLMs (e.g. RoBERTA-base) to extract their semantic features. We sample multiple times to get an average MMD sample score to estimate MMD distance of two datasets. The calculation of MMD is shown as follows:

\begin{equation}
\begin{aligned}
&\operatorname{MMD}^2[\mathcal{F}, X, Y]=\frac{1}{m(m-1)} \sum_{i \neq j}^m k\left(x_i, x_j\right) \\
&+\frac{1}{n(n-1)} \sum_{i \neq j}^n k\left(y_i, y_j\right)-\frac{2}{m n} \sum_{i, j=1}^{m, n} k\left(x_i, y_j\right)
\end{aligned}
\end{equation}

$\mathcal{F}$ is a MMD function class, \emph{i} and \emph{j} represents the batch of instances sampled from different distributions. \emph{m} and \emph{n} are the size of \emph{i} and \emph{j}.

\input{tables/tb-gpt3.tex}

\section{Rationale Overlap}
In order to measure the difference between rationales detected by PLMs and humans, we define precision as the percentage of the predicted rationales that also exist in the human annotation and recall as the percentage of words in the human annotation that also exist in the predicted rationales. We calculate the F1 score as an evaluation metric of overlap.

We show the evaluation of rationale overlap between models and humans on the e-SNLI dataset \cite{camburu2018snli} in Table \ref{tab:rationale_snli}. We find that the performance gap between different models is not very large (varying from 30.93 to 34.98). Models show a higher rationale overlap with humans based on e-SNLI than sentiment analysis datasets. This can be because the average length of instances in e-SNLI is generally shorter than that in sentiment analysis datasets. In particular, the base-sized ELECTRA has achieved the highest F1 score (34.98\%) among these models. 

\section{The In-domain Evaluation Results}

Following \cite{wang2018glue}, we report the in-domain evaluation results in Table \ref{fig:id_results}. We generally find that T5-large achieves the best average performance over seven tasks. Note that we report the results by evaluating models on the validation set provided by GLUE.


\section{The Correlation between ID and OOD Performance} \label{sec:appendixD}

To better show the correlation between ID and OOD performance, we provide the histogram in Figure \ref{fig:bar}. In general, we find that the overall performance of ID and OOD tests shows a linear correlation for both discriminative and generative models. In addition to the overall performance, we look at task-level performance at a more granular level in Figure \ref{fig:scatter_appendix}. As shown in Figure \ref{fig:scatter_appendix}, we find that the linear correlation does not exist for every task. For example, the  linear correlation is extremely weak for MRPC and QQP, with relatively low OOD accuracy. While the linear correlation becomes significant on STSB and QNLI.

\section{The ID and OOD Performance of GPT-3 and GPT-3.5} \label{sec:appendixG}

\textbf{Settings.} The performance of GPT-3 and GPT-3.5 is shown in \ref{tab:gpt3}, where we report the classification results based on 1,000 instances for each task. The training strategy of GPT-3 is simulated to keep the same as human evaluation. We feed the model with some in-domain instances as instructions before testing on the OOD dataset. To achieve this, we adopt the official API for calculating the in-domain performance of GPT-3 (text-DaVinci-003) based on 1,000 sampled ID instances. We leverage the in-context learning following \cite{ouyang2022training} to calculate its OOD results on \method.

\noindent\textbf{Results.} In Table \ref{tab:gpt3}, it is interesting to see that the performance decay ratio of GPT-3 caused by the domain generalization is similar to GPT-3.5 while significantly larger than Humans (11.49\% -- GPT 3.5 vs. 11.51\% -- GPT-3 vs. 6.65\% -- Humans), indicating that there is much room for improvement in the OOD robustness, even for state-of-the-art LLMs. Meanwhile, it can be seen that the OOD performance of GPT-3.5 is still far behind the human performance (66.90\% -- GPT-3 vs. 81.31\% -- Humans), and slightly lower than ELECTRA-large (69.68\%). Notably, the results of GPT-3/3.5 should be \textbf{treated with caution and just for reference} because we are not sure if datasets of \method are already included in the training corpus of GPT-3/3.5. Also, the OOD performance listed in Table \ref{tab:gpt3} cannot be compared with PLMs fairly, as we only adopt instructions to evaluate it not fine-tuning the model like other PLMs in \method.



\begin{table*}[t]
\centering
\small
\begin{tabular}{lllllllllll}
\hline
Model   & \multicolumn{1}{l}{SST-2} & \multicolumn{1}{l}{MNLI} & \multicolumn{1}{l}{QNLI} &\multicolumn{1}{l}{RTE} & \multicolumn{1}{l}{MRPC} & \multicolumn{1}{l}{QQP} & \multicolumn{1}{l}{STSB} & \multicolumn{1}{l}{COLA} & \multicolumn{1}{l}{Average} & \multicolumn{1}{l}{Parameters}  \\ \hline
ELECTRA-large   & 97.25 & 89.29 & 93.65 & 88.45 & 92.60  & 89.84 & 88.06 & 74.33 & 89.18              & 334.09                          \\
RoBERTa-large   & 95.87 & 89.47 & 93.45 & 84.48 & 92.36 & 90.43 & 86.68 & 69.90  & 87.83              & 355.36                          \\
T5-large        & 95.41 & 88.83 & 94.34 & 89.89 & 92.01 & 90.59 & 87.58 & 62.97 & 87.70               & 737.67                          \\
BART-large      & 95.76 & 88.30  & 94.20  & 83.39 & 92.21 & 90.41 & 86.81 & 65.29 & 87.05              & 406.29                          \\
XLNet-large     & 96.44 & 89.50  & 93.32 & 84.12 & 91.54 & 90.06 & 86.36 & 62.63 & 86.75              & 360.27                          \\
T5-base         & 94.50  & 86.55 & 93.12 & 83.39 & 91.22 & 90.06 & 86.79 & 61.71 & 85.92              & 222.9                           \\
ELECTRA-base    & 91.51 & 87.12 & 92.09 & 80.14 & 91.09 & 89.36 & 86.07 & 69.95 & 85.92              & 108.89                          \\
RoBERTa-base    & 94.27 & 87.43 & 92.48 & 76.53 & 91.83 & 89.77 & 86.59 & 63.25 & 85.27              & 124.65                          \\
GPT2-large      & 94.50  & 85.48 & 91.21 & 75.45 & 87.78 & 89.34 & 84.75 & 60.06 & 83.57              & 774.03                          \\
BERT-large      & 93.46 & 85.69 & 91.84 & 70.76 & 90.26 & 89.78 & 83.97 & 60.32 & 83.26              & 335.14                          \\
BART-base       & 93.69 & 85.89 & 91.65 & 76.17 & 89.75 & 89.52 & 84.87 & 52.78 & 83.04              & 139.42                          \\
ALBERT-base     & 92.09 & 83.81 & 90.98 & 73.29 & 90.23 & 88.70  & 84.30  & 57.25 & 82.58              & 11.68                           \\
XLNet-base      & 94.15 & 86.49 & 91.36 & 68.59 & 90.50  & 89.39 & 83.94 & 53.67 & 82.26              & 116.72                          \\
BERT-base       & 92.89 & 83.63 & 91.05 & 66.79 & 89.41 & 89.40  & 83.71 & 59.75 & 82.08              & 109.48                          \\
GPT2-medium     & 94.27 & 85.38 & 90.81 & 70.04 & 87.20  & 89.42 & 83.75 & 53.87 & 81.84              & 354.82                          \\
ELECTRA-small   & 91.28 & 81.93 & 88.69 & 68.59 & 89.88 & 88.98 & 83.61 & 59.06 & 81.50               & 13.48                           \\
T5-small        & 91.97 & 82.82 & 90.77 & 70.40  & 89.13 & 89.07 & 84.74 & 43.88 & 80.35              & 60.51                           \\
DistilBERT-base & 91.17 & 82.20  & 89.27 & 65.34 & 88.33 & 88.63 & 82.28 & 54.43 & 80.21              & 66.36                           \\
GPT2            & 90.94 & 82.63 & 88.78 & 69.31 & 84.51 & 88.63 & 82.31 & 47.29 & 79.30               & 124.44                     \\     \hline
\end{tabular}
\caption{Detailed results of the in-domain test on each task sorted by the average performance.}
\label{fig:id_results}
\end{table*}

\begin{figure*}[t]
\centering
\small
{% 
\includegraphics[width=.49\textwidth]{plots/exp/encoder_bar.pdf}
}
\centering 
\small
{% 
\includegraphics[width=.49\textwidth] {plots/exp/decoder_bar.pdf} \label{fig:encoder_bar}
}
\caption{The comparison of ID and OOD performance evaluated on GLUE-X. The left and right sub-figures show the results of discriminative models and generative models, respectively.} 
\label{fig:bar}
\end{figure*}

\begin{table*}[ht]
\centering
\small
\begin{tabular}{lllllllll}
\cline{1-9}
 Model           & SST2     & MNLI     & QNLI     & RTE      & MRPC     & QQP      & STSB     & COLA      \\ \hline
ELECTRA-large   & 2e-05/64 & 2e-05/16 & 5e-05/64 & 5e-05/32 & 2e-05/16 & 2e-05/64 & 2e-05/64 & 2e-05/128 \\ 
RoBERTa-large   & 2e-05/32 & 2e-05/64 & 2e-05/32 & 2e-05/32 & 2e-05/16 & 2e-05/64 & 2e-05/16 & 2e-05/32  \\
T5-large        & 1e-4/16  & 1e-4/32  & 1e-4/32  & 1e-4/32  & 1e-4/32  & 1e-4/16  & 1e-4/64  & 1e-4/32   \\
BART-large      & 2e-05/32 & 2e-05/16 & 2e-05/32 & 3e-05/32 & 2e-05/32 & 2e-05/32 & 2e-05/30 & 2e-05/32  \\
XLNet-large     & 3e-05/64 & 2e-05/64 & 3e-05/32 & 1e-05/32 & 1e-05/16 & 2e-05/32 & 2e-05/16 & 2e-05/16  \\
T5-base         & 1e-4/32  & 1e-4/16  & 1e-4/32  & 1e-4/8   & 1e-4/16  & 1e-4/16  & 3e-4/16  & 1e-4/32   \\
ELECTRA-base    & 1e-4/32  & 5e-05/64 & 5e-05/64 & 5e-05/16 & 5e-05/16 & 5e-05/32 & 5e-05/16 & 5e-05/32  \\
RoBERTa-base    & 2e-05/32 & 2e-05/32 & 2e-05/32 & 2e-05/32 & 3e-05/32 & 2e-05/32 & 3e-05/32 & 2e-05/16  \\
GPT2-large      & 2e-05/32 & 2e-05/32 & 2e-05/32 & 2e-05/16 & 3e-05/32 & 2e-05/32 & 2e-05/32 & 2e-05/32  \\
BERT-large      & 2e-05/32 & 2e-05/32 & 2e-05/16 & 2e-05/16 & 2e-05/16 & 2e-05/64 & 2e-05/64 & 3e-05/16  \\
BART-base       & 2e-05/32 & 2e-05/32 & 2e-05/32 & 2e-05/16 & 3e-05/32 & 2e-05/16 & 2e-05/16 & 2e-05/32  \\
ALBERT-base     & 2e-05/32 & 2e-05/32 & 2e-05/32 & 2e-05/32 & 2e-05/16 & 2e-05/32 & 2e-05/32 & 2e-05/32  \\
XLNet-base      & 3e-05/32 & 2e-05/32 & 2e-05/32 & 1e-05/16 & 2e-05/16 & 2e-05/32 & 2e-05/32 & 1e-05/32  \\
BERT-base       & 2e-05/32 & 3e-05/32 & 2e-05/32 & 3e-05/32 & 3e-05/32 & 2e-05/32 & 2e-05/16 & 3e-05/32  \\
GPT2-medium     & 2e-05/32 & 2e-05/16 & 3e-05/32 & 3e-05/32 & 3e-05/16 & 3e-05/32 & 3e-05/32 & 3e-05/32  \\
ELECTRA-small   & 5e-05/64 & 5e-05/64 & 5e-05/32 & 5e-05/64 & 5e-05/32 & 5e-05/32 & 5e-05/32 & 5e-05/64  \\
T5-small        & 1e-4/16  & 1e-4/16  & 1e-4/32  & 3e-4/16  & 1e-4/16  & 1e-4/16  & 3e-4/32  & 3e-4/32   \\
DistilBERT-base & 3e-05/16 & 2e-05/32 & 3e-05/32 & 2e-05/16 & 2e-05/16 & 2e-05/16 & 2e-05/16 & 2e-05/16  \\
GPT2            & 2e-05/32 & 2e-05/32 & 3e-05/32 & 2e-05/32 & 2e-05/32 & 2e-05/32 & 3e-05/32 & 3e-05/32 \\
\hline \cline{1-9}
\end{tabular}
\caption{The hyper-parameter setting for each task, including the learning rate and batch size.}
\label{tab:Hyperparameters}
\end{table*}



\begin{comment}
\begin{table*}[t]
\centering
\small
\begin{tabular}{llllll}
\hline
model\_name             & ID\_performance & OOD\_performance & decrease & parameters &  \\ \hline
t5-base                 & 0.860169465     & 0.622391         & 0.237779 & 222.9036                        &  \\
roberta-large           & 0.866277256     & 0.622255         & 0.244023 & 355.3597                        &  \\
bart-large              & 0.863451739     & 0.610164         & 0.253288 & 406.2915                        &  \\
t5-large                & 0.85175065      & 0.59792          & 0.253831 & 737.6681                        &  \\
roberta-base            & 0.840335243     & 0.578747         & 0.261589 & 124.6456                        &  \\
xlnet-large-cased       & 0.845502071     & 0.576592         & 0.26891  & 360.2688                        &  \\
t5-small                & 0.811636089     & 0.571831         & 0.239805 & 60.50662                        &  \\
gpt2-large              & 0.814013051     & 0.564393         & 0.24962  & 774.0301                        &  \\
gpt2-medium             & 0.807710208     & 0.557801         & 0.249909 & 354.8232                        &  \\
bert-large-uncased      & 0.81464551      & 0.554268         & 0.260378 & 335.1419                        &  \\
bart-base               & 0.825363332     & 0.550978         & 0.274386 & 139.4204                        &  \\
xlnet-base-cased        & 0.804845243     & 0.546173         & 0.258672 & 116.7183                        &  \\
bert-base-uncased       & 0.806289943     & 0.541052         & 0.265238 & 109.4822                        &  \\
albert-base-v2          & 0.815529672     & 0.540269         & 0.275261 & 11.68358                        &  \\
distilbert-base-uncased & 0.788236393     & 0.525526         & 0.262711 & 66.36288                        &  \\
gpt2                    & 0.783730086     & 0.502746         & 0.280984 & 124.4398                        &  \\
\hline
\end{tabular}
\caption{Results on out-of-distribution data.}
\end{table*}


\begin{table*}[!t]
\centering
\small
\begin{tabular}{llllll}
\hline
 & model\_name             & ID\_performance & OOD\_performance & decrease & parameters \\ \hline
 & t5-base                 & 0.860169        & 0.622391         & 0.237779 & 222.9036                        \\
 & t5-small                & 0.811636        & 0.571831         & 0.239805 & 60.50662                        \\
 & roberta-large           & 0.866277        & 0.622255         & 0.244023 & 355.3597                        \\
 & gpt2-large              & 0.814013        & 0.564393         & 0.24962  & 774.0301                        \\
 & gpt2-medium             & 0.80771         & 0.557801         & 0.249909 & 354.8232                        \\
 & bart-large              & 0.863452        & 0.610164         & 0.253288 & 406.2915                        \\
 & t5-large                & 0.851751        & 0.59792          & 0.253831 & 737.6681                        \\
 & xlnet-base-cased        & 0.804845        & 0.546173         & 0.258672 & 116.7183                        \\
 & bert-large-uncased      & 0.814646        & 0.554268         & 0.260378 & 335.1419                        \\
 & roberta-base            & 0.840335        & 0.578747         & 0.261589 & 124.6456                        \\
 & distilbert-base-uncased & 0.788236        & 0.525526         & 0.262711 & 66.36288                        \\
 & bert-base-uncased       & 0.80629         & 0.541052         & 0.265238 & 109.4822                        \\
 & xlnet-large-cased       & 0.845502        & 0.576592         & 0.26891  & 360.2688                        \\
 & bart-base               & 0.825363        & 0.550978         & 0.274386 & 139.4204                        \\
 & albert-base-v2          & 0.81553         & 0.540269         & 0.275261 & 11.68358                        \\
 & gpt2                    & 0.78373         & 0.502746         & 0.280984 & 124.4398  \\
\hline
\end{tabular}
\caption{Performance decay sorted by the absolute decrease values.}
\end{table*}

\begin{table*}[!t]
\centering
\small
\begin{tabular}{llllll}
\hline
model\_name             & ID\_performance & OOD\_performance & decrease & decrease\_percentage & parameters \\ \hline
t5-base                 & 0.860169        & 0.622391         & 0.237779 & 27.64327             & 222.9036                        \\
roberta-large           & 0.866277        & 0.622255         & 0.244023 & 28.16911             & 355.3597                        \\
bart-large              & 0.863452        & 0.610164         & 0.253288 & 29.33435             & 406.2915                        \\
t5-small                & 0.811636        & 0.571831         & 0.239805 & 29.5459              & 60.50662                        \\
t5-large                & 0.851751        & 0.59792          & 0.253831 & 29.8011              & 737.6681                        \\
gpt2-large              & 0.814013        & 0.564393         & 0.24962  & 30.66539             & 774.0301                        \\
gpt2-medium             & 0.80771         & 0.557801         & 0.249909 & 30.94043             & 354.8232                        \\
roberta-base            & 0.840335        & 0.578747         & 0.261589 & 31.12909             & 124.6456                        \\
xlnet-large-cased       & 0.845502        & 0.576592         & 0.26891  & 31.80481             & 360.2688                        \\
bert-large-uncased      & 0.814646        & 0.554268         & 0.260378 & 31.96208             & 335.1419                        \\
xlnet-base-cased        & 0.804845        & 0.546173         & 0.258672 & 32.13932             & 116.7183                        \\
bert-base-uncased       & 0.80629         & 0.541052         & 0.265238 & 32.89607             & 109.4822                        \\
bart-base               & 0.825363        & 0.550978         & 0.274386 & 33.24422             & 139.4204                        \\
distilbert-base-uncased & 0.788236        & 0.525526         & 0.262711 & 33.32892             & 66.36288                        \\
albert-base-v2          & 0.81553         & 0.540269         & 0.275261 & 33.75241             & 11.68358                        \\
gpt2                    & 0.78373         & 0.502746         & 0.280984 & 35.85209             & 124.4398                        \\
\hline
\end{tabular}
\caption{Performance decay sorted by the decrease ratios.}
\end{table*}


% Please add the following required packages to your document preamble:
% \usepackage{multirow}
\begin{table*}[!t]
\centering
\small
\begin{tabular}{lllllllll}
\multirow{2}{*}{Model}  & SST-2       & MNLI     & RTE      & QNLI     & QQP      & Avg      & Avg  & Avg \\
                        & \method    & \method & \method & \method & \method & \method & GLUE & $\Delta$↓  \\
t5-base                 & 0.860169465 & 0.622391 & 0.237779 & 222.9036 &          &          &      &     \\
roberta-large           & 0.866277256 & 0.622255 & 0.244023 & 355.3597 &          &          &      &     \\
bart-large              & 0.863451739 & 0.610164 & 0.253288 & 406.2915 &          &          &      &     \\
t5-large                & 0.85175065  & 0.59792  & 0.253831 & 737.6681 &          &          &      &     \\
roberta-base            & 0.840335243 & 0.578747 & 0.261589 & 124.6456 &          &          &      &     \\
xlnet-large-cased       & 0.845502071 & 0.576592 & 0.26891  & 360.2688 &          &          &      &     \\
t5-small                & 0.811636089 & 0.571831 & 0.239805 & 60.50662 &          &          &      &     \\
gpt2-large              & 0.814013051 & 0.564393 & 0.24962  & 774.0301 &          &          &      &     \\
gpt2-medium             & 0.807710208 & 0.557801 & 0.249909 & 354.8232 &          &          &      &     \\
bert-large-uncased      & 0.81464551  & 0.554268 & 0.260378 & 335.1419 &          &          &      &     \\
bart-base               & 0.825363332 & 0.550978 & 0.274386 & 139.4204 &          &          &      &     \\
xlnet-base-cased        & 0.804845243 & 0.546173 & 0.258672 & 116.7183 &          &          &      &     \\
bert-base-uncased       & 0.806289943 & 0.541052 & 0.265238 & 109.4822 &          &          &      &     \\
albert-base-v2          & 0.815529672 & 0.540269 & 0.275261 & 11.68358 &          &          &      &     \\
distilbert-base-uncased & 0.788236393 & 0.525526 & 0.262711 & 66.36288 &          &          &      &     \\
gpt2                    & 0.783730086 & 0.502746 & 0.280984 & 124.4398 &          &          &      &    
\end{tabular}
\caption{Performance decay sorted by the decrease ratios.}
\end{table*}
\end{comment}



\begin{figure*}[t]
\centering

\subfigure[MRPC]{
\begin{minipage}[t]{0.5\linewidth}
\centering
\includegraphics[width=.9\textwidth]{plots/ID_OOD/mrpc.pdf}
%\caption{fig1}
\end{minipage}%
}%
\subfigure[QQP]{
\begin{minipage}[t]{0.5\linewidth}
\centering
\includegraphics[width=.9\textwidth]{plots/ID_OOD/qqp.pdf}
%\caption{fig2}
\end{minipage}%
}%

%这个回车键很重要 \quad也可以

\subfigure[STSB]{
\begin{minipage}[t]{0.5\linewidth}
\centering
\includegraphics[width=.9\textwidth]{plots/ID_OOD/stsb.pdf}
%\caption{fig2}
\end{minipage}
}%
\subfigure[QNLI]{
\begin{minipage}[t]{0.5\linewidth}
\centering
\includegraphics[width=.9\textwidth]{plots/ID_OOD/qnli.pdf}
%\caption{fig2}
\end{minipage}
}%

\subfigure[RTE]{
\begin{minipage}[t]{0.5\linewidth}
\centering
\includegraphics[width=.9\textwidth]{plots/ID_OOD/rte.pdf}
%\caption{fig1}
\end{minipage}%
}%
\subfigure[SST2]{
\begin{minipage}[t]{0.5\linewidth}
\centering
\includegraphics[width=.9\textwidth]{plots/ID_OOD/sst2.pdf}
%\caption{fig2}
\end{minipage}%
}%

%这个回车键很重要 \quad也可以


\centering
\caption{The correlation between the ID and OOD performance for each task involving in \method.}
\label{fig:scatter_appendix}
\end{figure*}




%%%%%%%%%%%%%%%%%%
% \begin{figure*}[ht]
% \centering
% \small
% {% 
% \includegraphics[width=.49\textwidth]{plots/ID_OOD/cola.pdf}
% }
% \centering 
% \small
% {% 
% \includegraphics[width=.49\textwidth] {plots/ID_OOD/mnli.pdf}
% }

% \small
% {% 
% \includegraphics[width=.49\textwidth]{plots/ID_OOD/rte.pdf}
% }
% \centering 
% \small
% {% 
% \includegraphics[width=.49\textwidth] {plots/ID_OOD/sst2.pdf} \label{fig:test2}
% }


% \caption{The scatter figures will be re-formatted late.} 

% \label{fig:scatter_point}
% \end{figure*}


%%%%%%%%%%%%%



\end{document}


