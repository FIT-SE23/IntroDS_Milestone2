% We highlight three main areas of closely related works to \method: 1) Benchmarking robustness to OOD; 2) Comparison with Existing Benchmarks; 3) Domain Generalization Methods.
% \wjd{We may waste too much space on related work. Even this first para. can be removed.}

\paragraph{Benchmarking Robustness to OOD.}

Recent work~\cite{ibrahim2022robustness} finds that todayâ€™s leading PLMs are not robust to changing domains, where some OOD test samples varied during training. In particular, pre-trained transformers can rely heavily on spurious patterns (artefacts) \cite{gururangan2018annotation,kaushik2020learning,tu2020empirical}. For this reason, the standard held-out accuracy can overestimate the performance \cite{sagawa2020investigation,kaushik2021learning}, and evaluating the OOD robustness is crucial for real-world applications, which require models to hold good transferability. Consequently, there is a rising concern about improving dataset and benchmark development. Recent work introduces new out-of-distribution benchmarks for graphs \cite{gui2022good}, optical character recognition (OCR) \cite{larson2022evaluating},  computer vision (CV) \cite{ibrahim2022robustness}, time series tasks \cite{gagnon2022woods}, and artificial general intelligence (AGI) \cite{koh2021wilds,srivastava2022beyond}. However, evaluating the out-of-distribution generalization in a multi-task setting has received relatively little attention for NLP.

There is a line of work focusing on the development of challenge datasets, representing as Adversarial NLI~\cite{nie-etal-2020-adversarial}, Dynabench \cite{kiela2021dynabench}, Contrastive Set \cite{gardner2020evaluating}, and AdvGLUE \cite{advglue} where examples are created to be difficult for current models via an iterative, adversarial, and human-and-model-in-the-loop procedure. However, these datasets focus on robustness and stability issues rather than generalization and the artifact. In contrast, GLUE-X contains both off-the-shelf and self-collected datasets to implement cross-distribution tests.

Prior work \cite{wenzel2022assaying} observed that OOD performance holds a linear correlation with ID accuracy in CV based on 172 publicly available datasets and 31k networks, while their relationship is largely dataset-dependent. However, this conclusion has been found somewhat controversial, as \citet{teney2022id} argue that the selection of datasets influences the OOD performance. 

%While whether the similar findings can be applied to text classification tasks remains an open question, at least lacking the in-depth analysis. 


%Over the last years, deep learning models have become much more powerful, driven by advances in transfer learning (pre-training) \cite{}. A consequence of this drastic increase in in-domain performance is that existing benchmarks have been left behind. on this form. However, evaluating the out-of-distribution generalization in text classifications has received little attention. There is a gap between the actual demand and literature, given that recent models have outpaced the benchmarks to test for them \cite{AIIndexReport2021}. The performance achieved by big models are quickly reaching super-human performance on benchmarks, such as GLUE \cite{wang2018glue}, and SuperGLUE \cite{wang2019superglue} -- a challenged benchmark styled after GLUE with a new set of difficult examples.



%To this end, there is a link of work focusing on the development of challenge datasets, representing as Adversarial NLI (Nie et al., 2020), Dynabench (Kiela et al., 2021), Contrastive Set (EMNLP), and AdvGLUE where examples are created to be diffcult for current models via an iterative, adversarial, and human-and-model-in-the-loop procedure. A benefit of such benchmarks is that they can be dynamically updated to be challenging as new models emerge and consequently do not saturate as easily as static benchmarks. However, such a method inevitably lead to unnatural distribution shifts. Different from the adversarial datasets, which leverage the human-crafted adversarial data, \method paid particular attention to the out-of-distribution generalization problem by using the genuine data collected from other domains.

\paragraph{Existing Benchmarks for NLU.}

%While sharing the same aspirational goal, previous benchmarks have never considered evaluating natural language understanding systems for the OOD robustness testing with genius cross-domain data.

%BIG-bench is a collaborative benchmark focusing on AGI, including NLP, CV, Code Generation, etc.

There have been different types of leaderboards towards evaluating natural language understanding (NLU) systems. Examples of building challenging benchmarks in recent years include GLUE \cite{wang2018glue}, SuperGLUE \cite{wang2019superglue}, FewGLUE \cite{fewglue}, FEVER \cite{petroni2021kilt}, FewNLU \cite{fewnlu}, and AdvGLUE \cite{advglue}. In particular, FewGLUE and FewNLU focus on the few-shot learning challenge. The performance decay of NLP models has been found in real-world deployment because of the arises of the OOD generalization challenge as well as robustness issues, such as adversarial robustness. Similar to our work, other benchmarks, such as AdvGLUE \cite{advglue}, leverage the training set extracted from GLUE for each task. Differently, we consider evaluating OOD performance in a general multi-task setting, where the test data arise from one or more different distributions. 

%Note that evaluating natural language understanding systems for the OOD robustness testing with genius cross-domain data.


%\wjdd{AdvGLUE is not for OOD robustness, I presume? So we should first say robust is important, such as adv. robustness, where advglue is proposed. Then, we proceed with OOD robustness, which remaines underexplored in NLP benchmarks.} 


\paragraph{Domain Generalization} (DG)~\cite{wang2022generalizing} aims to learn a generalized model that is robust to unseen distributions using training data from multiple domains~\cite{balaji2018metareg,dou2019domain,vu2022domain,varshney2022investigating}. We focus on the single-source DG~\cite{huang2020self,krueger2021out,wang2022generalizing} setting, which is a popular setting for measuring the OOD robustness in NLP~\cite{hendrycks2020pretrained}, and aligns with the GLUE leaderboard. As stated by a recent taxonomy and review towards generalisation research in NLP \cite{hupkes2022state}, current work does not provide standardized data or procedures for generalization testing, while we use GLUE-X as the first attempt towards this goal.

%\citet{wenzel2022assaying} observe that OOD performance holds a linear correlation with ID accuracy in most of scenarios. On the contrary, \citet{teney2022id} point out that the inverse correlation does occur between the ID and OOD performance on some specific CV datasets. Regrettably, such a crucial topic has never been systematically discussed in NLP until the occurance of \method. 

%Nowadays, the simple performance metrics of natural language understanding systems fail to capture the limitations of existing models due to the inflated test accuracy. There are two key themes in addressing this problem -- curating difficult examples \cite{} -- and changing to use fine-grained evaluation metrics \cite{}. \wjdd{Can `challenges' be part of related work?}

%The ability of a text classifier to handle inputs that are drawn from a distribution outsides the training distribution is crucial for increasing models' robustness and generalizability. From the data perspective, the common way for generating difficult examples is to use adversarial filtering \cite{}. Moreover, the recent efforts made by \citet{} choose to build datasets in a model-in-the-loop manner, which brings the iterative process. As a NLP benchmark for evaluating models' performance towards the robustness and generalizability, \method brings the unique challenge by collecting open-available genius data samples from other domains for OOD test. The results are affected by distributional shifts in the real world and pose interesting challenges with respect to post-hoc analysis since we need to understand not only if a model outperforms a previous baseline but what kind of errors it makes in the feature-level and which patterns it fails to capture.