% This must be in the first 5 lines to tell arXiv to use pdfLaTeX, which is strongly recommended.
\pdfoutput=1
% In particular, the hyperref package requires pdfLaTeX in order to break URLs across lines.

\documentclass[11pt]{article}

% Remove the "review" option to generate the final version.
\usepackage{authblk}
\usepackage[]{acl}
\renewcommand*{\Affilfont}{\normalsize\normalfont}
\renewcommand*{\Authfont}{\bfseries}

\usepackage{graphicx}
%\usepackage{caption}
%\usepackage{subcaption}
\usepackage{subfigure} 
\usepackage{amsmath}
\usepackage{adjustbox} % 表格的大小调整
\usepackage{amsthm}
\usepackage{booktabs}
\usepackage{algorithm}
\usepackage{algpseudocode}
%\usepackage{algorithmic}
\urlstyle{same}
\usepackage{amssymb}
\usepackage{multirow}
\usepackage{longtable}
\usepackage{comment}
% \usepackage{subcaption} 
\usepackage[normalem]{ulem}
\usepackage{csquotes}
% \usepackage[]{acl}

% Standard package includes
\usepackage{times}
\usepackage{latexsym}

% For proper rendering and hyphenation of words containing Latin characters (including in bib files)
\usepackage[T1]{fontenc}
% For Vietnamese characters
% \usepackage[T5]{fontenc}
% See https://www.latex-project.org/help/documentation/encguide.pdf for other character sets

% This assumes your files are encoded as UTF8
\usepackage[utf8]{inputenc}


% This is not strictly necessary, and may be commented out,
% but it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}
\usepackage{xspace}
% \usepackage{graphicx}
\newcommand{\ubold}[1]{\fontseries{b}\selectfont#1}
\newcommand{\shuibai}[1]{\textcolor{blue}{[Shuibai: #1]}} 
\setlength{\belowcaptionskip}{-4pt}
\usepackage{authblk}

% Jindong's comments
\usepackage{todonotes}
\usepackage{xcolor}
\newcommand{\wjdd}[1]{\todo[linecolor=cyan,backgroundcolor=cyan!25,bordercolor=cyan,size=\scriptsize]{(WJD): #1}}
\newcommand{\wjd}[1]{{\color{cyan}{[(WJD): #1]}}}
\newcommand{\wyd}[1]{{\color{red}{[(WYD): #1]}}}
\newcommand{\yly}[1]{{\color{orange}{[(YLY): #1]}}}
\newcommand{\lbq}[1]{{\color{blue}{[(LBQ): #1]}}}
\newcommand{\method}{GLUE-X\xspace}

% If the title and author information does not fit in the area allocated, uncomment the following
%
%\setlength\titlebox{<5cm>}
%
% and set <dim> to something 5cm or larger.
\title{GLUE-X: Evaluating Natural Language Understanding Models from an Out-of-distribution Generalization Perspective}

% \title{GLUE-X: Evaluating OOD Robustness of Natural Language Understanding Tasks}


%\title{RDL: Human-in-the-loop Rationale-Centric Framework for Static Learning}

% Author information can be set in various styles:
% For several authors from the same institution:
% \author{Author 1 \and Author2 \and Author n \\
%         Address line \\ Address line \\ Address line}
% if the names do not fit well on one line use
%         Author 1 \\ {\bf Author 2} \\ ... \\ {\bf Author n} \\
% For authors from different institutions:
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \And  ... \And
%         Author n \\ Address line \\ ... \\ Address line}
% To start a seperate ``row'' of authors use \AND, as in
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \AND
%         Author 2 \\ Address line \\ ... \\ Address line \And
%         Author 3 \\ Address line \\ ... \\ Address line}

% \author{First Author \\
%   Affiliation / Address line 1 \\
%   Affiliation / Address line 2 \\
%   Affiliation / Address line 3 \\
%   \texttt{email@domain} \\\And
%   Second Author \\
%   Affiliation / Address line 1 \\
%   Affiliation / Address line 2 \\
%   Affiliation / Address line 3 \\
%   \texttt{email@domain} \\}

%\author{
    % Authors
%Anonymous
%    
%}



 
\author{
    % Authors
    \textbf{Linyi Yang}$^{1,2}$\thanks{\ \ Equal contribution. Random order.}, \textbf{Shuibai Zhang}$^{1,4,*}$\thanks{\ \ Work done at Westlake University as an intern.}, \textbf{Libo Qin}$^{1}$, \textbf{Yafu Li}$^{1}$, \textbf{Yidong Wang}$^{1}$, \textbf{Hanmeng Liu}$^{1}$, \\
    \textbf{Jindong Wang}$^{3}$, \textbf{Xing Xie}$^{3}$, \textbf{Yue Zhang}$^{1,2}$\thanks{\ \ Correspondence to: zhangyue@westlake.edu.cn} 
    % \texttt{\{yanglinyi, zhangyue\}@westlake.edu.cn}
}
\affil{$^{1}$ School of Engineering, Westlake University\\
    $^{2}$ Institute of Advanced Technology, Westlake Institute for Advanced Study\\
    $^{3}$ Microsoft Research Asia (MSRA)\\
    $^{4}$ University of Electronic Science and Technology of China\\
    %\texttt{\{yanglinyi, zhangyue\}@westlake.edu.cn}
}


\begin{document}

\maketitle
% \authornote{Corresponding Author}


\begin{abstract}
Pre-trained language models (PLMs) improve the model generalization by leveraging massive data as the training corpus in the pre-training phase. However, currently, the out-of-distribution (OOD) generalization becomes a generally ill-posed problem, even for the large-scale PLMs in natural language understanding tasks, which prevents the deployment of NLP methods in the real world. To facilitate the research in this direction, this paper makes the first attempt to establish a unified benchmark named \method, highlighting the importance of OOD robustness and providing insights on how to measure the robustness of a model and how to improve it. To this end, we collect \emph{13} publicly available datasets as OOD test data and conduct evaluations on \emph{8} classic NLP tasks over \emph{18} popularly used models. Our findings confirm that the OOD accuracy in NLP tasks needs to be paid more attention to since the significant performance decay compared to ID accuracy has been found in all settings.\looseness=-1


\end{abstract}

\section{Introduction}

Pre-trained Language Models (PLMs) have achieved competitive performance across various NLP tasks when evaluated on the standard benchmark \cite{blasi22acl}, such as GLUE \cite{wang2018glue} and SuperGLUE \cite{wang2019superglue}. However, recent studies \cite{gururangan2018annotation,ribeiro2019red,kaushik2020learning,ribeiro2020beyond,ruder2021challenges} show concerns that models are yet not close to achieving proper natural language understanding, essential questions being raised about their robustness \cite{srivastava2020robustness,wang2021measure} and underlying sensitivity to systematic biases \cite{probing2019acl,sagawa2020investigation}.
Such issues come to be manifest in performance, especially for out-of-distribution (OOD) generalization when the test distribution differs from training, which has motivated the study of different distribution shifts \cite{zheng2020out,arora2021types,koh2021wilds,shen2021enhancing,malinin2021shifts}. 

Computer vision (CV) and artificial general intelligence (AGI), for which OOD generalization has been systemically studied \cite{wang2022generalizing,koh2021wilds,srivastava2022beyond,ibrahim2022robustness,yang2022openood}. While sharing the same aspirational goal, existing evaluations \cite{gardner2020evaluating} and methods~\cite{hendrycks2020pretrained,bommasani2021opportunities} for OOD generalization in NLP are limited in focusing only on one \cite{khanuja2020gluecos,wang2021robustness,yang-etal-2021-exploring,howard2022neurocounterfactuals} or a couple of tasks~\cite{kaushik2020learning,srivastava2020robustness,wu2021polyjuice}. Evidence shows that better controlling spurious correlations between input features and labels that do not causally affect a task's label will improve the model’s robustness in distribution shifts. However, such simple evaluations of natural language understanding systems on a couple of tasks fail to capture the limitations of existing models due to inflated test accuracy. It remains a gap in evaluating a unified model that can execute a range of classification tasks. 

To facilitate research in this direction, we make extensions to the multi-task benchmark \cite{fewnlu,xu2020clue,xu2021fewclue} for out-of-distribution evaluations by expanding the test data from \emph{single}-domain to \emph{multiple} domains and name the resulting benchmark \method. \method favors cross-distribution evaluations between datasets that share the same label types. Also, it offers the analysis of two main factors affecting the cross-domain generalization performance, namely the pre-trained language model (e.g., architecture, hyper-parameters, etc.) and different training strategies (e.g., fine-tuning, prompt-tuning \cite{chen2022adaprompt}, probing \cite{kumar2022finetuning}, and domain-generalization training \cite{wang2022generalizing}, etc.).

In particular, we evaluate popular metrics from natural language understanding tasks for \emph{18} different pre-trained models in a unified setting and under the same experimental conditions to investigate the gaps in common practices. In addition, we consider \emph{3} tuning strategies designed for the single-source domain generalization, including linear probing, fine-tuning, and linear-probing then fine-tuning (LP-FT). Finally, we explore internal causes for OOD robustness at the feature level by measuring the rationale overlap between humans, and models \cite{lei2016rationalizing,yang2022openood}.
%\wjd{A schematic diagram or figure to overview what will be covered is good to show our contributions.}

Through the above experiments, we find that (1) no one backbone can significantly outperform the others across all tasks, which is consistent with the conclusion \cite{wenzel2022assaying} in the computer vision; (2) surprisingly, the influence of model architectures is more significant than the model parameters towards the OOD robustness; (3) the ID and OOD performance holds a linear correlation in most cases for text classifications; (4) in terms of the tuning strategy, we show that linear probing and then fine-tuning can slightly improve the OOD performance compared to the standard fine-tuning. 

To our knowledge, we are the \emph{first} to systemically evaluate natural language understanding systems for cross-distribution generalization on genuine data, along with experiments using different fine-tuning strategies. Importantly, we complete datasets of cross-domain evaluations for all typical text classification tasks by creating the test data from other distributions to training data from scratch. Based on the \method, we report OOD results under the same experimental conditions. We open-source the codebase and datasets for unified and consistent evaluation in different ways. 


\section{Related Work}

\input{sec-related.tex}

The goal of \method is to provide an easy-to-use and robust benchmark to a broad range of out-of-distribution generalization tasks for evaluating natural language understanding systems. We articulate the following tasks and datasets in \method.

\input{tables/tb-dataset.tex}

\subsection{Overview of \method}

\textbf{Tasks.} As a benchmark styled after GLUE \cite{wang2018glue}, we consider following eight tasks in \method: Sentiment Analysis (\emph{SST-2}), Natural Language Inference (\emph{MNLI, QNLI, RTE}), Sentence Pair Similarity (\emph{MRPC, QQP}), Textual Similarity (\emph{STS-B}) and Linguistic Acceptability (\emph{CoLA}). \footnote{The WNLI task is not included in \method since there is no sufficient in-domain data to construct OOD tests.}

\noindent\textbf{Datasets.} \method follows the same in-domain training data and evaluation metrics to GLUE \cite{wang2018glue}. To construct the out-of-domain test, we adopt the popular datasets extracted from different domains while keeping the same prediction labels as the original tasks in GLUE. We show detailed data statistics in Table~\ref{tab:data_statistic}. 

\noindent\textbf{Models.} To ensure that our results are relevant for researchers and practitioners, we consider both top-performing backbones and cost-efficient methods: \emph{Discriminative Models} -- BERT-base, BERT-large \cite{bert}, RoBERTa-base, RoBERTa-large \cite{roberta}, XLNet-base, XLNet-large \cite{xlnet}; \emph{Generative Models} -- BART-base, BART-large \cite{bart}, T5-small, T5-base, T5-large \cite{t5}, GPT2, GPT2-medium, GPT2-large \cite{gpt2}; \emph{Cost-Efficient Models} -- ALBERT-base \cite{albert}, and DistilBERT-base \cite{distilbert}. We follow the official implementations of several pre-trained language models from Huggingface\footnote{\url{https://huggingface.co/models}} to reproduce results on GLUE using the validation set and test these models on \method.

\noindent\textbf{Fine-tuning Strategies.} We investigate the efficacy of different fine-tuning strategies for OOD generalization. In particular, we consider three paradigms: standard fine-tuning, fine-tuning only the head (linear-probing), and linear-probing then fine-tuning. After fine-tuning models using the in-domain training data for each task, we evaluate the performance using the in- and out-of-domain test data, respectively.

\subsection{Dataset Curation}

We construct test sets for each task under the requirement that their label types should be kept the same. To this end, \method contains 13 OOD datasets in total, including publicly available datasets (Amazon, HANs, etc) and newly collected datasets (Grammer Test). 
In particular, we select the popular datasets while sharing the same label types with the in-domain data involved in GLUE as OOD test data for different tasks, including sentiment analysis -- IMDB \cite{maas-etal-2011-learning}, Yelp \cite{zhang2015character}, and Amazon \cite{ni-etal-2019-justifying}, linguistic acceptability -- Grammer Test --, textual similarity -- SICK \cite{zhang2018multi} --, NLI -- MNLI-Mismatched \cite{williams2017broad}, SNLI \cite{bowman2015large}, and SICK \cite{zhang2018multi} --, Textual Entailment -- RTE, MRPC, QQP and HANs \cite{bentivogli2009fifth,dolan-2005-automatically,wang2017bilateral,mccoy2019right}. As a newly collected dataset, the Grammar Test description can be found in Appendix \ref{sec:appendixA}.

Note that SICK contains multiple labels, including textual similarity, also used as an OOD test set of the textual similarity task. We rounded floating number labels of textual similarity to integers from 0 to 4, converting it into a five-class dataset to align with other classification tasks in GLUE-X. In addition, RTE, MRPC and QQP can be OOD datasets of each other when implementing the textual entailment task. Based on these datasets, we further conduct the zero-shot transfer from the in-domain dataset from GLUE to unseen OOD datasets when constructing \method.

\subsection{Metrics}

Previous benchmarks, such as GLUE and SuperGLUE, seek to evaluate NLU models by simply average scores of all tasks. In \method, we consider both the macro average and weighted average score over eight tasks. Following GLUE, we first average metrics to get a task score for tasks with multiple metrics.

In addition to the overall measures of OOD performance, we are also interested in better understanding the features learned by models. Thus, in the following subsection, we consider using post-hoc analysis to find rationales used by models when making the prediction and then measuring the rationale overlap between humans and models.

In terms of rankings, we adopt Friedman rank \cite{friedman1940comparison} to fairly compare the performance of different backbones in various tasks:
\begin{equation}
    \begin{split}
\operatorname{rank}_{F}  = \frac{1}{n} \sum_{i=1}^{n} \operatorname{rank}_i,
    \end{split}
\end{equation}
where $n$ is the number of tasks (e.g., $n=8$ in Table~\ref{tab:detail}) and $\operatorname{rank}_i$ is the rank of the performance in the $i$-th setting. We report both the average ranking and Friedman rank in our results.

\input{tables/tb-overallper.tex}
\input{tables/tb-gpucost.tex}

\subsection{Post-hoc Analysis}

In addition to the quantitative analysis, to further provide an in-depth analysis of results reported by \method, we choose two tasks including sentiment analysis and nature language inference to provide the post-hoc analysis \cite{lei2016rationalizing}. In particular, we adopt the sensitivity of contextual decomposition technique from \cite{jin2019towards} which removed part of inputs from the sequence text to evaluate a model’s sensitivity to them, thereby allowing for the identification of important features. The output will be the overlap between the rationale found by models and humans, which somehow represents the models' ability to learn the real causal features \cite{jacovi2020towards}. 

Given a phrase \emph{p} starting with the negative limitations in the \emph{k-th} document \emph{D(k)}, we sample the documents which contain the same phrase \emph{p} to alleviate the influence by chance when there are multiple shreds of evidence saturating the prediction. The window size of phrase \emph{p} is limited to 3 (no phrase containing more than 3 tokens will be considered). Taking the sentiment analysis task as an example, in the source \enquote{This movie was so unbelievably bad}, if we only remove the non-causal word \enquote{movie}, the prediction would not be changed so much for a robust model that not be affected by spurious patterns. 


The importance is computed as:
\begin{equation}
\small
\phi(\mathbf{p}, \widehat{\mathcal{D}^{(k)}})=\mathbb{E}_{\widehat{\mathcal{D}^{(\beta)}}}\left[l\left(\widehat{\mathcal{D}^{(\beta)}}; \widehat{\mathcal{D}}\right)-l\left(\widehat{\mathcal{D}^{(\beta)}} \backslash \mathbf{p} ; \widehat{\mathcal{D}}\right)\right],
\end{equation}
where \(\mathcal{D}^{(\beta)}\) denotes the resulting text after masking out a single token (phrase) starting with the negative pronoun in the length of \(N\) surrounding the phrase \(\mathbf{p}\). we use \(l\left(\widehat{\mathcal{D}^{(\beta)}} \backslash \mathbf{p} ; \widehat{\mathcal{D}}\right)\) to represent the model prediction logits after replacing the masked-out context. \(\backslash \mathbf{p}\) indicates the operation of masking out the phrase \(p\) in a input file sampling from the dataset \(\mathcal{D}\). 

\subsection{Training Cost}

We record the training cost in GLUE and \method. We leverage 50 NVIDIA Tesla V100 GPU cards and 8 NVIDIA A100 GPU cards in our experiments. In total, we spend 10,000+ GPU hours based on the estimation using a single V100 card. The detailed training cost and inference speed per task is shown in Table \ref{tab:cost}.

\section{Experiments and Results}

In the following we explore the facets of OOD generalization in NLP, highlighting discrepancies to prior work and discuss their implications.


\input{tables/tb-detail.tex}

\input{tables/tb-decayratio.tex}

\subsection{Prediction Results}

\textbf{Overall Performance on \method.} We report the weighted average score of different models representing the overall performance in Table \ref{tab:overall_per}, which is sorted in descending order by the average \method performance. In addition to the overall performance, we provide the Friedman Rank for both in-domain and out-of-domain results. From Table \ref{tab:overall_per}, we observe that all pre-trained models involved in \method show significant performance decay under the OOD test compared to the ID performance (\textbf{23.41\%} decay in average). 

\textbf{Model-level Analysis.} On the model level, we observe that T5-large and RoBERTa-large achieve the best performance for ID (\textbf{86.71}) and OOD (\textbf{65.10}) test, respectively. Meanwhile, the results suggest that there is no significant difference of the OOD robustness between generative models and discriminative models for text classification tasks. Not surprisingly, lightweight models, including DistilBERT-base, GPT-2, and T5-small, are in the bottom three on \method with the lowest OOD performance. In terms of the Friedman rank, we observe that the variance of OOD rank is significantly higher than the ID rank, which hints that the uncertainty of performance has been increased on \method. Meanwhile, although the overall ranking has not been changed so much from the absolute average OOD score, we show an improvement of rankings in GPT2-medium and GPT2.


\input{tables/tb-rational.tex}


\textbf{The Performance of Compressed Models.} The results of \method suggest that OOD generalization still faces fundamental challenges, especially for lightweight models. For example, we find that compressed models (e.g., DistilBERT-base) show relatively low performance compared to others. Differently, the OOD performance of ALBERT-base (11M parameters) is significantly higher than DistilBERT-base (\textbf{60.51\% vs. 56.46\%}), even better than several moderate-sized models (BERT-large, GPT2-medium and XLNet-base). 

% Please add the following required packages to your document preamble:
% \usepackage{multirow}
\begin{figure*}[t]
\centering
\subfigure[COLA]{
\begin{minipage}[t]{0.4\linewidth}
\centering
\includegraphics[width=\textwidth]{plots/ID_OOD/cola.pdf}
%\caption{fig1}
\end{minipage}%
}%
\subfigure[MNLI]{
\begin{minipage}[t]{0.4\linewidth}
\centering
\includegraphics[width=\textwidth]{plots/ID_OOD/mnli.pdf}
%\caption{fig2}
\end{minipage}%
}

%
% \subfigure[MRPC]{
% \begin{minipage}[t]{0.5\linewidth}
% \centering
% \includegraphics[width=.9\textwidth]{plots/ID_OOD/mrpc.pdf}
% %\caption{fig1}
% \end{minipage}%
% }%
% \subfigure[QQP]{
% \begin{minipage}[t]{0.5\linewidth}
% \centering
% \includegraphics[width=.9\textwidth]{plots/ID_OOD/qqp.pdf}
% %\caption{fig2}
% \end{minipage}%
% }%
%这个回车键很重要 \quad也可以            
\subfigure[RTE]{
\begin{minipage}[t]{0.4\linewidth}
\centering
\includegraphics[width=\textwidth]{plots/ID_OOD/rte.pdf}
%\caption{fig2}
\end{minipage}
}%
\subfigure[SST-2]{
\begin{minipage}[t]{0.4\linewidth}
\centering
\includegraphics[width=\textwidth]{plots/ID_OOD/sst2.pdf}
%\caption{fig2}
\end{minipage}
}%

% \subfigure[STSB]{
% \begin{minipage}[t]{0.5\linewidth}
% \centering
% \includegraphics[width=.9\textwidth]{plots/ID_OOD/stsb.pdf}
% %\caption{fig2}
% \end{minipage}
% }%
% \subfigure[QNLI]{
% \begin{minipage}[t]{0.5\linewidth}
% \centering
% \includegraphics[width=.9\textwidth]{plots/ID_OOD/sst2.pdf}
% %\caption{fig2}
% \end{minipage}
% }%
\centering
\caption{The scatter figures for illustrating the correlation between ID and OOD performance.}
\label{fig:scatter}
\end{figure*}
\subsection{Explanation Results}

The results of the rationale overlap between models and humans are shown in Table \ref{tab:rationale}. Using \method, we also provide some distinct post-hoc analysis to inspire the community. Inspired by the previous work \cite{shen2022shortest}, this section aims to answer the following question: Does OOD performance consistently correspond with the rationale overlap?

To answer the above question, we measure the rationale overlap for each model on the task of sentiment analysis. As shown in Table \ref{tab:rationale}, somehow surprisingly, we find that the best performing model on \method -- RoBERTa-large (see Table \ref{tab:overall_per}) yet to achieve a high rationale overlap between humans and models. In particular, RoBERTa-large performs worst in terms of the rationale overlap. In addition, we obtain an interesting finding that base-sized models can achieve relatively higher rationale overlaps than large-sized models, which is generally consistent with the results reported by the previous work \cite{deyoung2020eraser}. 
%We measure the MMD distance between the In-domain and Out-of-domain dataset and show details in Appendix \ref{}. We find that the performance decay holds a statistically significant correlation with the distance of distribution shifts. In particular, the generalization for the CoLA dataset is the most challenging task for models since the OOD dataset we selected holds the biggest difference with CoLA. In contrast, models tend to perform better on the relatively easy dataset, such as sentiment analysis (SST-2). As shown in Table \ref{tab:detail}, we show that the best performing model, RoBERTa, can even achieve 94.98\% accuracy for the zero-shot domain generalization of sentiment analysis. Besides, the generative model, such as GPT2, shows its advantage in the task of QQP (60.80\% accuracy achieved by GPT2-large). Note that we do not evaluate the WNLI task since there is no sufficient in-domain data. Also, the OOD dataset of CoLA is newly collected in this work.

%\begin{figure*}[t]
%\centering
%\small
%\includegraphics[width=.75\textwidth]{plots/exp/encoder_bar.png}
% \caption{Encoder-only Average Performance.}\label{fig:encoder_bar}
%\end{figure*}

%\begin{figure*}[t]
%\centering
%\small
%\includegraphics[width=.75\textwidth]{plots/exp/decoder_bar.png}
%\caption{Decoder-only Average Performance.}\label{fig:decoder_bar}
%\end{figure*}

% Please add the following required packages to your document preamble:
% \usepackage{multirow}

\begin{figure*}[t]
\centering
\small
{% 
\includegraphics[width=.4\textwidth]{plots/Tuning/distilbert-base.pdf} 
}
\centering 
\small
{% 
\includegraphics[width=.4\textwidth] {plots/Tuning/roberta-base.pdf} 
}
\caption{The influence of different tuning strategies on the task of MNLI, including Linear-probing, Fine-tuning, and Linear-probing then Fine-tuning (LP-FT).} 
\label{fig:tuning}
\end{figure*}

\section{Discussion}

This section aims to provide some distinct analysis based on \method to inspire the community. 

\subsection{Factors towards OOD Robustness}

As shown in Table \ref{tab:detail}, we suggest that there is no silver bullet towards the OOD robustness, given that no single model can consistently outperform others over all tasks on \method\footnote{Note that we do not evaluate the WNLI task since there is no sufficient in-domain data~\cite{xlnet}.}. For example, RoBERTa-large can only achieve the best performance on two of eight tasks (SST-2 and CoLA). Besides, we also observe that the OOD generalization results are largely influenced by the distribution shift between the ID and OOD datasets. In particular, the performance decay on OOD test is exacerbated by the increase of distribution shifts, as shown in Appendix \ref{sec:appendixC}. We also find that the generalization for the CoLA dataset is the most challenging task for models since the test set holds the biggest difference with training data. In contrast, models tend to perform better on the relatively easy dataset, such as sentiment analysis (SST-2). For example, the best performing model RoBERT-large can achieve a \textbf{94.98\%} accuracy on SST-2 and only a \textbf{20.26\%} accuracy on CoLA.

\subsection{Model Architectures Outweigh Parameters}

Table \ref{tab:decay_ratio} demonstrates the overall performance sorted by the decrease ratio representing the robustness to some extent. The large-sized model, such as T5-large, and RoBERTa-large, can significantly surpass the corresponding base-sized models (T5: \textbf{63.93\% vs. 62.20\%}; RoBERTa: \textbf{65.10\% vs. 62.07\%}). However, we also observe that the influence of parameter size is somehow not as important as the model types. For instance, the performance achieved by RoBERTa-base (125M parameters) is higher than GPT2-large (774M parameters) in \textbf{62.07\% vs. 61.45\%}. Meanwhile, the OOD performance of T5-base is significantly higher than GPT2-large and BERT-large, which hints the importance of the model selections. 

Empirical evidence from Table \ref{tab:decay_ratio} also shows that model types could be more influential than the parameter size towards the OOD performance. More specifically, as shown in the Table \ref{tab:decay_ratio}, results of the same architecture with different parameters are more close than the results of the similar parameter-size models based on different architectures. For instance, the decrease ratio of three T5 models pre-training with different parameter sizes (61M, 223M, and 737M) are ranked near each other, which is similar to BART and BERT. We identify exploring model architectures as one of the future directions for improving the OOD robustness, rather than increasing the parameter size only.


\subsection{Correlations Between ID and OOD Performance}

We show the pearson correlation of four tasks between the in- and out-of-domain results in Figure \ref{fig:scatter} (the full results can be found at Appendix \ref{sec:appendixB}). With no surprise, we observe that the in-domain performance is usually higher than the out-of-domain performance. Specifically, we find that the OOD performance is extremely lower than the ID performance in the task of COLA. In contrast, the gap between ID and OOD performance based on SST-2 and MNLI is relatively lower than others. We suppose that this is partially influenced by the distribution shift between the in-domain and out-of-domain datasets. 

Regarding the type of pre-trained models, we show that discriminative models usually perform a stronger linear correlation when compared to generative models. From the task perspective, we observe that the correlation between ID and OOD is largely influenced by datasets. For instance, ID and OOD performance are inversely correlated on MRPC (See Appendix \ref{sec:appendixB}), but yet almost correlated on other tasks. It hints that the inverse correlation is possible for the specific task when the size of test samples is limited. 

\subsection{The Efficacy of Linear-probing then Fine-tuning}

Using MNLI as an example, we compare the results achieved by DistilBERT-base and RoBERTa-base using three different training strategies in Figure \ref{fig:tuning}. As found by the previous work \cite{kumar2022finetuning}, fine-tuning can do worse than linear probing in the presence of a large distribution shift in CV. However, as shown in Figure \ref{fig:tuning}, we find that the linear probing method performs extremely struggle in NLP in terms of the low accuracy for both ID and OOD test, which is insistent with the conclusion in CV. While the LP-FT can be relatively helpful for improving the OOD robustness of NLP models in terms of the slight performance improvement compared to the standard fine-tuning method. We expect to see more methodologies toward domain generalizations can be designed for improving the OOD robustness, especially for NLP tasks.

\section{Conclusion}

We construct \method, an OOD robustness benchmark for natural language understanding tasks that aims to enable fair evaluation over multiple datasets from multiple domains. With \method, we evaluate 18 pre-trained models on 8 classification tasks. Besides, we provide the analysis using 3 different tuning strategies and post-hoc analysis for gaining internal causes for the OOD robustness. We conclude that (1) current state-of-the-art methods based on large-scale pre-trained models still have much room to improve towards the OOD robustness; (2) although the ID and OOD performance holds a linear correlation in most cases, such a correlation is primarily related to the selection of OOD test sets; (3) introducing novel architectures brings more performance benefit, especially for the OOD performance. In the future, we will complement the OOD test for QNLI and extend the OOD test data for other tasks.

\begin{comment}
\section{Limitation}

Our primary focus is on the OOD robustness of classification tasks in this paper. However, there are other NLP tasks that the community should not ignore. \method currently does not include language generation tasks such as machine translation, summarization, and dialogue. Moreover, it is of great importance to extend current \method to more real-world datasets from different domains.
\end{comment}


\bibliography{custom}
\bibliographystyle{acl_natbib}

\newpage
\appendix

%%%%%%%%%%
% \begin{table*}[t]
% \centering
% \small
% \begin{tabular}{lllllllllll}
% \hline
% \multirow{2}{*}{Model} & \multicolumn{1}{l}{SST-2} & \multicolumn{1}{l}{MNLI} & \multicolumn{1}{l}{RTE} & \multicolumn{1}{l}{MRPC} & \multicolumn{1}{l}{QQP} & \multicolumn{1}{l}{STS-B} & \multicolumn{1}{l}{CoLA} & \multicolumn{1}{l}{QNLI} & \multicolumn{1}{l}{Avg} & Avg       \\
%                        & \multicolumn{1}{l}{OOD}   & \multicolumn{1}{l}{OOD}  & \multicolumn{1}{l}{OOD} & \multicolumn{1}{l}{OOD}  & \multicolumn{1}{l}{OOD} & \multicolumn{1}{l}{OOD}   & \multicolumn{1}{l}{OOD}  & \multicolumn{1}{l}{OOD} & $\Delta$↓ \\ \hline
% \multicolumn{10}{l}{State-of-the-art Pre-trained Language Models}                                                                                                                                                                                         \\ \hline
%%%%%%%%%%%

\section{Appendix: Data Collection} \label{sec:appendixA}
We derive our COLA-OOD dataset from the Public High School English Exams. The original multi-choice fill-in tests are converted into COLA-style, with correct answers as positive examples and incorrect answers as negative examples. We collect the data from publicly available internet resources, and the original open-access materials can be found from the following link https://www.koolearn.com/shiti.

\section{Appendix: Hyper-Parameters} \label{sec:appendixB}
We performed grid search for each task, and kept the best-performing checkpoint in ID datasets and tested their performance on their correspondnig OOD datasets. The hyperparameters used by these weights can be seen in Table \ref{tab:Hyperparameters}.

\section{Appendix: Domain Distributions} \label{sec:appendixC}
We evaluate distribution shifts between different datasets in terms of Maximum Mean Discrepancy(MMD) and Word Overlap Rate. MMD distance focuses on the semantic distribution shift between datasets, while Word Overlap Rate pays more attention to superficial similarity.


\begin{figure*}[t]
\centering

\subfigure[COLA]{
\begin{minipage}[t]{0.25\linewidth}
\centering
\includegraphics[width=.9\textwidth]{plots/overlap_figures/cola.pdf}
%\caption{fig1}
\end{minipage}%
}%
\subfigure[MNLI]{
\begin{minipage}[t]{0.25\linewidth}
\centering
\includegraphics[width=.9\textwidth]{plots/overlap_figures/mnli.pdf}
%\caption{fig2}
\end{minipage}%
}%
\subfigure[MRPC]{
\begin{minipage}[t]{0.25\linewidth}
\centering
\includegraphics[width=.9\textwidth]{plots/overlap_figures/mrpc.pdf}
%\caption{fig1}
\end{minipage}%
}%
\subfigure[QQP]{
\begin{minipage}[t]{0.25\linewidth}
\centering
\includegraphics[width=.9\textwidth]{plots/overlap_figures/qqp.pdf}
%\caption{fig2}
\end{minipage}%
}%
                 %这个回车键很重要 \quad也可以
                 
\subfigure[RTE]{
\begin{minipage}[t]{0.25\linewidth}
\centering
\includegraphics[width=.9\textwidth]{plots/overlap_figures/rte.pdf}
%\caption{fig2}
\end{minipage}
}%
\subfigure[SST-2]{
\begin{minipage}[t]{0.25\linewidth}
\centering
\includegraphics[width=.9\textwidth]{plots/overlap_figures/sst2.pdf}
%\caption{fig2}
\end{minipage}
}%
\subfigure[STSB]{
\begin{minipage}[t]{0.25\linewidth}
\centering
\includegraphics[width=.9\textwidth]{plots/overlap_figures/stsb.pdf}
%\caption{fig2}
\end{minipage}
}%
\subfigure[QNLI(to be replaced)]{
\begin{minipage}[t]{0.25\linewidth}
\centering
\includegraphics[width=.9\textwidth]{plots/overlap_figures/sst2.pdf}
%\caption{fig2}
\end{minipage}
}%
\centering
\caption{The word-level overlap between the training set and test set for each task.}
\label{fig:Word_Overlap}
\end{figure*}

\begin{figure*}[t]
\centering

\subfigure[COLA]{
\begin{minipage}[t]{0.25\linewidth}
\centering
\includegraphics[width=.9\textwidth]{plots/MMD_scores_figures/cola.pdf}
%\caption{fig1}
\end{minipage}%
}%
\subfigure[MNLI]{
\begin{minipage}[t]{0.25\linewidth}
\centering
\includegraphics[width=.9\textwidth]{plots/MMD_scores_figures/mnli.pdf}
%\caption{fig2}
\end{minipage}%
}%
\subfigure[MRPC]{
\begin{minipage}[t]{0.25\linewidth}
\centering
\includegraphics[width=.9\textwidth]{plots/MMD_scores_figures/mrpc.pdf}
%\caption{fig1}
\end{minipage}%
}%
\subfigure[QQP]{
\begin{minipage}[t]{0.25\linewidth}
\centering
\includegraphics[width=.9\textwidth]{plots/MMD_scores_figures/qqp.pdf}
%\caption{fig2}
\end{minipage}%
}%
                 %这个回车键很重要 \quad也可以
                 
\subfigure[RTE]{
\begin{minipage}[t]{0.25\linewidth}
\centering
\includegraphics[width=.9\textwidth]{plots/MMD_scores_figures/rte.pdf}
%\caption{fig2}
\end{minipage}
}%
\subfigure[SST-2]{
\begin{minipage}[t]{0.25\linewidth}
\centering
\includegraphics[width=.9\textwidth]{plots/MMD_scores_figures/sst2.pdf}
%\caption{fig2}
\end{minipage}
}%
\subfigure[STSB]{
\begin{minipage}[t]{0.25\linewidth}
\centering
\includegraphics[width=.9\textwidth]{plots/MMD_scores_figures/stsb.pdf}
%\caption{fig2}
\end{minipage}
}%
\subfigure[QNLI(to be replaced)]{
\begin{minipage}[t]{0.25\linewidth}
\centering
\includegraphics[width=.9\textwidth]{plots/MMD_scores_figures/sst2.pdf}
%\caption{fig2}
\end{minipage}
}%
\centering
\caption{The MMD Scores between the training set and test set for each task. Lower MMD score means the higher correlation between datasets.}
\label{fig:MMD_scores}
\end{figure*}

\subsection{Word Overlap}

The similarity between datasets of In-distribution datasets and Out-of-distribution datasets are shown in Figure \ref{fig:Word_Overlap}.


\subsection{MMD Distance}
The MMD distance between ID and OOD datasets are shown in Figure \ref{fig:MMD_scores} for each task including in \method. When computing the MMD distance of two datasets, we sample the same number of samples in both datasets and feed them into a pre-trained language model(e.g. RoBERTA-base) to extract their semantic features, which are used to compute MMD. It is hard to calculate MMD by all samples in the datasets due to the limitation of computing resource. As a result, we sample multiple times to get an average MMD sample score to estimate MMD distance of two datasets. The calculation of MMD is shown as follows:

\begin{equation}
\begin{aligned}
&\operatorname{MMD}^2[\mathcal{F}, X, Y]=\frac{1}{m(m-1)} \sum_{i \neq j}^m k\left(x_i, x_j\right) \\
&+\frac{1}{n(n-1)} \sum_{i \neq j}^n k\left(y_i, y_j\right)-\frac{2}{m n} \sum_{i, j=1}^{m, n} k\left(x_i, y_j\right)
\end{aligned}
\end{equation}

$\mathcal{F}$ is a MMD function class, \emph{i} and \emph{j} represents the batch of instances sampled from different distributions. \emph{m} and \emph{n} are the size of \emph{i} and \emph{j}.


\begin{figure*}[ht]
\centering
\small
{% 
\includegraphics[width=.49\textwidth]{plots/exp/encoder_bar.pdf}
}
\centering 
\small
{% 
\includegraphics[width=.49\textwidth] {plots/exp/decoder_bar.pdf} \label{fig:encoder_bar}
}
\caption{The comparison of ID and OOD performance evaluated on GLUE-X. The left and right sub-figures show the results of discriminative models and generative models, respectively.} 
\label{fig:bar}
\end{figure*}


\section{Appendix: The Correlation between ID and OOD Performance}

To better show the correlation between ID and OOD performance, we provide the histogram as shown in Figure \ref{fig:bar}. In general, we find that the overall performance of ID and OOD test show a linear correlation for both discriminative models and generative models. In addition to the overall performance, we look at task-level performance at a more granular level in Figure \ref{fig:scatter_appendix}. As shown in Figure 6, we find that the linear correlation does not exist for every task. For example, the 
linear correlation is extremely weak for MRPC and QQP, where the OOD accuracy is relatively low. While the pearson correlation becomes significant when it comes to STSB and QNLI.

\section{Appendix: The In-domain Evaluation Results}

Following \cite{wang2018glue}, we report the in-domain evaluation results in Table \ref{fig:id_results}. In general, we find that T5-large achieves the best average performance over seven tasks. Note that we report the results by evaluating models on the validation set provided by GLUE.

\begin{table*}[ht]
\centering
\small
\begin{tabular}{lllllllll}
\cline{1-9}
 & Model                   & SST2      & MNLI      & RTE       & MRPC      & QQP       & STSB      & COLA     \\ \cline{1-9}
& T5-large        & 1e-4/32   & 1e-4/32   & 1e-4/16   & 2e-4/80   & 1e-4/32   & 1e-4/32   & 1e-4/32  \\
 & RoBERTa-large   & 3e-05/128 & 3e-05/128 & 3e-05/128 & 2e-05/16  & 3e-05/128 & 2e-05/16  & 2e-05/16 \\
 & BART-large      & 2e-05/64  & 2e-05/64  & 2e-05/64  & 2e-05/32  & 2e-05/64  & 2e-05/30  & 2e-05/32 \\
 & XLNet-large     & 2e-05/64  & 2e-05/64  & 2e-05/64  & 1e-05/16  & 2e-05/64  & 2e-05/64  & 2e-05/16 \\
 & T5-base         & 2e-4/128  & 2e-4/128  & 1e-4/8    & 2e-4/128  & 2e-4/128  & 1e-4/32   & 1e-4/32  \\
 & RoBERTa-base    & 3e-05/256 & 3e-05/256 & 2e-05/32  & 3e-05/256 & 3e-05/256 & 3e-05/256 & 2e-05/16 \\
 & GPT2-large      & 2e-05/32  & 2e-05/32  & 2e-05/32  & 3e-05/32  & 2e-05/32  & 2e-05/32  & 2e-05/32 \\
 & BERT-large      & 3e-05/128 & 3e-05/128 & 2e-05/16  & 2e-05/16  & 3e-05/128 & 2e-05/32  & 2e-05/16 \\
 & BART-base       & 2e-05/96  & 2e-05/96  & 2e-05/16  & 3e-05/32  & 2e-05/96  & 2e-05/16  & 2e-05/24 \\
 & ALBERT-base     & 3e-05/256 & 3e-05/256 & 3e-05/256 & 2e-05/16  & 3e-05/256 & 2e-05/32  & 2e-05/32 \\
 & BERT-base       & 3e-05/256 & 3e-05/32  & 3e-05/32  & 3e-05/32  & 3e-05/256 & 2e-05/16  & 2e-05/16 \\
 & XLNet-base      & 2e-05/256 & 2e-05/256 & 1e-05/16  & 2e-05/256 & 2e-05/256 & 2e-05/32  & 1e-05/32 \\
 & GPT2-medium     & 2e-05/96  & 2e-05/16  & 3e-05/32  & 3e-05/16  & 2e-05/96  & 3e-05/32  & 2e-05/16 \\
 & DistilBERT-base & 3e-05/16  & 2e-05/32  & 2e-05/16  & 2e-05/16  & 3e-05/256 & 2e-05/16  & 2e-05/16 \\
 & T5-small        & 2e-4/256  & 2e-4/256  & 3e-4/16   & 2e-4/256  & 2e-4/256  & 2e-4/256  & 3e-4/16  \\
 & GPT2            & 2e-05/32  & 2e-05/32  & 2e-05/32  & 2e-05/32  & 2e-05/128 & 2e-05/128 & 3e-05/32 \\ \hline \cline{1-9}
\end{tabular}
\caption{The hyper-parameter setting for each task, including the learning rate and batch size.}
\label{tab:Hyperparameters}
\end{table*}


\begin{table*}[t]
\centering
\small
\begin{tabular}{llllllllll}
\hline
Model   & \multicolumn{1}{l}{SST-2} & \multicolumn{1}{l}{MNLI} & \multicolumn{1}{l}{RTE} & \multicolumn{1}{l}{MRPC} & \multicolumn{1}{l}{QQP} & \multicolumn{1}{l}{STSB} & \multicolumn{1}{l}{COLA} & \multicolumn{1}{l}{Average} & \multicolumn{1}{l}{Parameters}  \\ \hline
T5-large        & 95.87                     & 89.15                    & 87.73                   & 92.17                    & 90.42                   & 87.81                    & 63.81                    & 86.71                             & 737.67                          \\
RoBERTa-large   & 95.99                     & 89.53                    & 85.56                   & 92.36                    & 90.38                   & 86.68                    & 65.69                    & 86.60                              & 355.36                          \\
BART-large      & 95.18                     & 88.79     & 87.00                      & 92.21                    & 90.63                   & 86.81                    & 65.29                    & 86.56                             & 406.29                          \\
XLNet-large     & 95.41                     & 89.14                    & 84.48                   & 91.54                    & 90.09                   & 86.13                    & 62.63                    & 85.63                             & 360.27                          \\
T5-base         & 94.84                     & 86.69                    & 83.39                   & 91.61                    & 90.03                   & 86.7                     & 61.75                    & 85.00                                & 222.90                           \\
RoBERTa-base    & 95.07                     & 87.27                    & 76.53                   & 90.90                     & 90.03                   & 85.47                    & 63.25                    & 84.08                             & 124.65                          \\
GPT2-large      & 93.69                     & 85.48                    & 77.26                   & 87.78                    & 89.43                   & 84.75                    & 60.06                    & 82.64                             & 774.03                          \\
BERT-large      & 94.38                     & 85.97                    & 70.76                   & 90.26                    & 89.77                   & 83.55                    & 62.46                    & 82.45                             & 335.14                          \\
BART-base       & 93.81                     & 85.76                    & 76.17                   & 89.75                    & 89.69                   & 84.87                    & 55.82                    & 82.27                             & 139.42                          \\
ALBERT-base     & 92.55                     & 83.91                    & 74.37                   & 90.23                    & 88.69                   & 84.30                     & 57.25                    & 81.61                             & 11.68                           \\
BERT-base       & 92.89                     & 83.63                    & 66.79                   & 89.41                    & 89.54                   & 83.71                    & 59.88                    & 80.83                             & 109.48                          \\
XLNet-base      & 94.61                     & 86.83                    & 68.59                   & 87.92                    & 89.84                   & 83.94                    & 53.67                    & 80.77                             & 116.72                          \\
GPT2-medium     & 94.04                     & 85.38                    & 70.04                   & 87.20                     & 89.32                   & 83.75                    & 54.47                    & 80.60                              & 354.82                          \\
DistilBERT-base & 91.17                     & 82.20                     & 65.34                   & 88.33                    & 88.44                   & 82.28                    & 54.59                    & 78.91                             & 66.36                           \\
T5-small        & 91.17                     & 83.28                    & 70.40                    & 88.90                     & 89.00                      & 85.15                    & 44.10                     & 78.86                             & 60.51                           \\
GPT2            & 90.94                     & 82.63                    & 69.31                   & 84.51                    & 88.28                   & 82.39                    & 47.29                    & 77.91                             & 124.44                         
\\ \hline
\end{tabular}
\caption{Detailed results of in-domain test on each task sorted by the average performance.}
\label{fig:id_results}
\end{table*}





\begin{comment}
\begin{table*}[t]
\centering
\small
\begin{tabular}{llllll}
\hline
model\_name             & ID\_performance & OOD\_performance & decrease & parameters &  \\ \hline
t5-base                 & 0.860169465     & 0.622391         & 0.237779 & 222.9036                        &  \\
roberta-large           & 0.866277256     & 0.622255         & 0.244023 & 355.3597                        &  \\
bart-large              & 0.863451739     & 0.610164         & 0.253288 & 406.2915                        &  \\
t5-large                & 0.85175065      & 0.59792          & 0.253831 & 737.6681                        &  \\
roberta-base            & 0.840335243     & 0.578747         & 0.261589 & 124.6456                        &  \\
xlnet-large-cased       & 0.845502071     & 0.576592         & 0.26891  & 360.2688                        &  \\
t5-small                & 0.811636089     & 0.571831         & 0.239805 & 60.50662                        &  \\
gpt2-large              & 0.814013051     & 0.564393         & 0.24962  & 774.0301                        &  \\
gpt2-medium             & 0.807710208     & 0.557801         & 0.249909 & 354.8232                        &  \\
bert-large-uncased      & 0.81464551      & 0.554268         & 0.260378 & 335.1419                        &  \\
bart-base               & 0.825363332     & 0.550978         & 0.274386 & 139.4204                        &  \\
xlnet-base-cased        & 0.804845243     & 0.546173         & 0.258672 & 116.7183                        &  \\
bert-base-uncased       & 0.806289943     & 0.541052         & 0.265238 & 109.4822                        &  \\
albert-base-v2          & 0.815529672     & 0.540269         & 0.275261 & 11.68358                        &  \\
distilbert-base-uncased & 0.788236393     & 0.525526         & 0.262711 & 66.36288                        &  \\
gpt2                    & 0.783730086     & 0.502746         & 0.280984 & 124.4398                        &  \\
\hline
\end{tabular}
\caption{Results on out-of-distribution data.}
\end{table*}


\begin{table*}[!t]
\centering
\small
\begin{tabular}{llllll}
\hline
 & model\_name             & ID\_performance & OOD\_performance & decrease & parameters \\ \hline
 & t5-base                 & 0.860169        & 0.622391         & 0.237779 & 222.9036                        \\
 & t5-small                & 0.811636        & 0.571831         & 0.239805 & 60.50662                        \\
 & roberta-large           & 0.866277        & 0.622255         & 0.244023 & 355.3597                        \\
 & gpt2-large              & 0.814013        & 0.564393         & 0.24962  & 774.0301                        \\
 & gpt2-medium             & 0.80771         & 0.557801         & 0.249909 & 354.8232                        \\
 & bart-large              & 0.863452        & 0.610164         & 0.253288 & 406.2915                        \\
 & t5-large                & 0.851751        & 0.59792          & 0.253831 & 737.6681                        \\
 & xlnet-base-cased        & 0.804845        & 0.546173         & 0.258672 & 116.7183                        \\
 & bert-large-uncased      & 0.814646        & 0.554268         & 0.260378 & 335.1419                        \\
 & roberta-base            & 0.840335        & 0.578747         & 0.261589 & 124.6456                        \\
 & distilbert-base-uncased & 0.788236        & 0.525526         & 0.262711 & 66.36288                        \\
 & bert-base-uncased       & 0.80629         & 0.541052         & 0.265238 & 109.4822                        \\
 & xlnet-large-cased       & 0.845502        & 0.576592         & 0.26891  & 360.2688                        \\
 & bart-base               & 0.825363        & 0.550978         & 0.274386 & 139.4204                        \\
 & albert-base-v2          & 0.81553         & 0.540269         & 0.275261 & 11.68358                        \\
 & gpt2                    & 0.78373         & 0.502746         & 0.280984 & 124.4398  \\
\hline
\end{tabular}
\caption{Performance decay sorted by the absolute decrease values.}
\end{table*}

\begin{table*}[!t]
\centering
\small
\begin{tabular}{llllll}
\hline
model\_name             & ID\_performance & OOD\_performance & decrease & decrease\_percentage & parameters \\ \hline
t5-base                 & 0.860169        & 0.622391         & 0.237779 & 27.64327             & 222.9036                        \\
roberta-large           & 0.866277        & 0.622255         & 0.244023 & 28.16911             & 355.3597                        \\
bart-large              & 0.863452        & 0.610164         & 0.253288 & 29.33435             & 406.2915                        \\
t5-small                & 0.811636        & 0.571831         & 0.239805 & 29.5459              & 60.50662                        \\
t5-large                & 0.851751        & 0.59792          & 0.253831 & 29.8011              & 737.6681                        \\
gpt2-large              & 0.814013        & 0.564393         & 0.24962  & 30.66539             & 774.0301                        \\
gpt2-medium             & 0.80771         & 0.557801         & 0.249909 & 30.94043             & 354.8232                        \\
roberta-base            & 0.840335        & 0.578747         & 0.261589 & 31.12909             & 124.6456                        \\
xlnet-large-cased       & 0.845502        & 0.576592         & 0.26891  & 31.80481             & 360.2688                        \\
bert-large-uncased      & 0.814646        & 0.554268         & 0.260378 & 31.96208             & 335.1419                        \\
xlnet-base-cased        & 0.804845        & 0.546173         & 0.258672 & 32.13932             & 116.7183                        \\
bert-base-uncased       & 0.80629         & 0.541052         & 0.265238 & 32.89607             & 109.4822                        \\
bart-base               & 0.825363        & 0.550978         & 0.274386 & 33.24422             & 139.4204                        \\
distilbert-base-uncased & 0.788236        & 0.525526         & 0.262711 & 33.32892             & 66.36288                        \\
albert-base-v2          & 0.81553         & 0.540269         & 0.275261 & 33.75241             & 11.68358                        \\
gpt2                    & 0.78373         & 0.502746         & 0.280984 & 35.85209             & 124.4398                        \\
\hline
\end{tabular}
\caption{Performance decay sorted by the decrease ratios.}
\end{table*}


% Please add the following required packages to your document preamble:
% \usepackage{multirow}
\begin{table*}[!t]
\centering
\small
\begin{tabular}{lllllllll}
\multirow{2}{*}{Model}  & SST-2       & MNLI     & RTE      & QNLI     & QQP      & Avg      & Avg  & Avg \\
                        & \method    & \method & \method & \method & \method & \method & GLUE & $\Delta$↓  \\
t5-base                 & 0.860169465 & 0.622391 & 0.237779 & 222.9036 &          &          &      &     \\
roberta-large           & 0.866277256 & 0.622255 & 0.244023 & 355.3597 &          &          &      &     \\
bart-large              & 0.863451739 & 0.610164 & 0.253288 & 406.2915 &          &          &      &     \\
t5-large                & 0.85175065  & 0.59792  & 0.253831 & 737.6681 &          &          &      &     \\
roberta-base            & 0.840335243 & 0.578747 & 0.261589 & 124.6456 &          &          &      &     \\
xlnet-large-cased       & 0.845502071 & 0.576592 & 0.26891  & 360.2688 &          &          &      &     \\
t5-small                & 0.811636089 & 0.571831 & 0.239805 & 60.50662 &          &          &      &     \\
gpt2-large              & 0.814013051 & 0.564393 & 0.24962  & 774.0301 &          &          &      &     \\
gpt2-medium             & 0.807710208 & 0.557801 & 0.249909 & 354.8232 &          &          &      &     \\
bert-large-uncased      & 0.81464551  & 0.554268 & 0.260378 & 335.1419 &          &          &      &     \\
bart-base               & 0.825363332 & 0.550978 & 0.274386 & 139.4204 &          &          &      &     \\
xlnet-base-cased        & 0.804845243 & 0.546173 & 0.258672 & 116.7183 &          &          &      &     \\
bert-base-uncased       & 0.806289943 & 0.541052 & 0.265238 & 109.4822 &          &          &      &     \\
albert-base-v2          & 0.815529672 & 0.540269 & 0.275261 & 11.68358 &          &          &      &     \\
distilbert-base-uncased & 0.788236393 & 0.525526 & 0.262711 & 66.36288 &          &          &      &     \\
gpt2                    & 0.783730086 & 0.502746 & 0.280984 & 124.4398 &          &          &      &    
\end{tabular}
\caption{Performance decay sorted by the decrease ratios.}
\end{table*}
\end{comment}



\begin{figure*}[t]
\centering

\subfigure[MRPC]{
\begin{minipage}[t]{0.5\linewidth}
\centering
\includegraphics[width=.9\textwidth]{plots/ID_OOD/mrpc.pdf}
%\caption{fig1}
\end{minipage}%
}%
\subfigure[QQP]{
\begin{minipage}[t]{0.5\linewidth}
\centering
\includegraphics[width=.9\textwidth]{plots/ID_OOD/qqp.pdf}
%\caption{fig2}
\end{minipage}%
}%

%这个回车键很重要 \quad也可以

\subfigure[STSB]{
\begin{minipage}[t]{0.5\linewidth}
\centering
\includegraphics[width=.9\textwidth]{plots/ID_OOD/stsb.pdf}
%\caption{fig2}
\end{minipage}
}%
\subfigure[QNLI(to be replaced)]{
\begin{minipage}[t]{0.5\linewidth}
\centering
\includegraphics[width=.9\textwidth]{plots/ID_OOD/sst2.pdf}
%\caption{fig2}
\end{minipage}
}%
\centering
\caption{The correlation between the ID and OOD performance for each task involving in \method.}
\label{fig:scatter_appendix}
\end{figure*}




%%%%%%%%%%%%%%%%%%
% \begin{figure*}[ht]
% \centering
% \small
% {% 
% \includegraphics[width=.49\textwidth]{plots/ID_OOD/cola.pdf}
% }
% \centering 
% \small
% {% 
% \includegraphics[width=.49\textwidth] {plots/ID_OOD/mnli.pdf}
% }

% \small
% {% 
% \includegraphics[width=.49\textwidth]{plots/ID_OOD/rte.pdf}
% }
% \centering 
% \small
% {% 
% \includegraphics[width=.49\textwidth] {plots/ID_OOD/sst2.pdf} \label{fig:test2}
% }


% \caption{The scatter figures will be re-formatted late.} 

% \label{fig:scatter_point}
% \end{figure*}


%%%%%%%%%%%%%



\end{document}


