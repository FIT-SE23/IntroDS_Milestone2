\begin{table*}[t]
\centering
\begin{adjustbox}{width=\textwidth}
\begin{tabular}{lcccccccccc}
\toprule
\multirow{2}{*}{\textbf{Model}} & \multicolumn{1}{c}{\textbf{SST-2}} & \multicolumn{1}{c}{\textbf{MNLI}} & \multicolumn{1}{c}{\textbf{RTE}} & \multicolumn{1}{c}{\textbf{MRPC}} & \multicolumn{1}{c}{\textbf{QQP}} & \multicolumn{1}{c}{\textbf{STS-B}} & \multicolumn{1}{c}{\textbf{CoLA}} & \multicolumn{1}{c}{\textbf{QNLI}} & \multicolumn{1}{c}{\textbf{Avg}} & \textbf{Avg}       \\
                       & \multicolumn{1}{c}{OOD}   & \multicolumn{1}{c}{OOD}  & \multicolumn{1}{c}{OOD} & \multicolumn{1}{c}{OOD}  & \multicolumn{1}{c}{OOD} & \multicolumn{1}{c}{OOD}   & \multicolumn{1}{c}{OOD}  & \multicolumn{1}{c}{OOD} & & $\Delta$â†“ \\ \midrule
\multicolumn{10}{l}{State-of-the-art Pre-trained Language Models}                                                                                                                                                                                         \\ \midrule
RoBERTa-large   & \textbf{94.98} & 79.42 & 69.93 & 53.41 & 59.43 & 78.28 & \textbf{20.26} &  & 65.10  & 24.83 \\ 
T5-large        & 94.67 & 64.60  & \textbf{71.97} & 58.92 & 59.03 & \textbf{79.46} & 18.86 & & 63.93 & 26.27 \\ 
XLNet-large     & 93.82 & \textbf{79.77} & 71.09 & 56.08 & 55.10  & 73.92 & 16.88 &      & 63.81 & 25.48 \\ 
BART-large      & 94.32 & 77.87 & 67.45 & 56.89 & 55.34 & 76.31 & 15.04 &  & 63.32 & 26.85 \\
T5-base         & 93.38 & 63.15 & 69.68 & \textbf{59.10}  & 58.45 & 75.60  & 16.01 &  & 62.20  & 26.82 \\
RoBERTa-base    & 92.88 & 76.67 & 66.04 & 49.86 & 59.21 & 74.32 & 15.49 &      & 62.07 & 26.18 \\
GPT2-large      & 90.61 & 75.12 & 64.62 & 54.68 & \textbf{60.80}  & 70.64 & 13.66 &      & 61.45 & 25.64 \\
ALBERT-base     & 91.44 & 72.22 & 61.83 & 53.24 & 59.04 & 72.17 & 13.64 &      & 60.51 & 25.85 \\
BART-base       & 91.53 & 76.12 & 64.53 & 54.15 & 54.00    & 72.66 & 7.75  &      & 60.11 & 26.94 \\
BERT-large      & 89.20  & 75.30  & 60.09 & 52.72 & 55.20  & 71.23 & 15.88 &      & 59.95 & 27.29 \\
GPT2-medium     & 88.83 & 74.47 & 59.99 & 56.10  & 60.32 & 69.25 & 10.51 &      & 59.92 & 25.66 \\
XLNet-base      & 93.35 & 75.51 & 64.09 & 53.95 & 54.63 & 68.25 & 9.60   &      & 59.91 & 25.83 \\
BERT-base       & 89.70  & 72.36 & 60.69 & 53.29 & 53.85 & 67.54 & 13.66 &      & 58.73 & 27.34 \\
T5-small        & 87.81 & 60.50  & 63.39 & 56.51 & 57.32 & 73.76 & 6.64  &      & 57.99 & 26.46 \\
GPT2            & 85.80  & 71.24 & 59.03 & 56.63 & 57.39 & 65.81 & 8.00     &      & 57.70  & 25.94 \\
DistilBERT-base & 86.19 & 71.16 & 57.48 & 54.87 & 50.24 & 66.35 & 8.96  &      & 56.46 & 28.45\\ \bottomrule

\end{tabular}
\end{adjustbox}
\caption{Detailed OOD performance for each task in \method. The best performance is shown in bold.}
\label{tab:detail}
\end{table*}