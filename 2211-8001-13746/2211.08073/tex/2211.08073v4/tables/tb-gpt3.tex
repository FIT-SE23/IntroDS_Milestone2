\begin{table*}[t]
\centering
\small
\begin{adjustbox}{width=\textwidth}
\begin{tabular}{lcccccccccc}
\toprule
\multirow{2}{*}{\textbf{Model}} & \multicolumn{1}{c}{\textbf{SST-2}} & \multicolumn{1}{c}{\textbf{MNLI}} & \multicolumn{1}{c}{\textbf{QNLI}} & \multicolumn{1}{c}{\textbf{RTE}} & \multicolumn{1}{c}{\textbf{MRPC}} & \multicolumn{1}{c}{\textbf{QQP}} & \multicolumn{1}{c}{\textbf{STS-B}} & \multicolumn{1}{c}{\textbf{CoLA}} & \multicolumn{1}{c}{\textbf{Avg}} & \textbf{Avg}       \\
                       & \multicolumn{1}{c}{}   & \multicolumn{1}{c}{}  & \multicolumn{1}{c}{} & \multicolumn{1}{c}{}  & \multicolumn{1}{c}{} & \multicolumn{1}{c}{}   & \multicolumn{1}{c}{}  & \multicolumn{1}{c}{} & \multicolumn{1}{c}{} & $\Delta$â†“ 
                       \\ \midrule
Humans (OOD) & \textit{92.36} & \textit{84.13} & \textit{81.10} & \textit{83.47} & \textit{84.70} & \textit{85.43} & \textit{80.28} & \textit{58.98} & \textit{80.14} & \textit{7.82}
                       \\ \midrule
GPT-3 (ID) & 93.68 & 69.27 & 79.20 & 80.20 & 79.21 & 72.15 & 88.10 & 50.13 & 76.49  & -  \\ 
GPT-3.5 (ID) & 95.75 & 72.25 & 82.78 & 82.71 & 73.36 & 75.69 & 89.55 & 54.99 & 78.39 & -  \\ 
\midrule
%ELECTRA-large (ID) & 97.25 & 89.29 & 93.65 & 88.45 & 92.60  & 89.84 & 88.06 & 74.33 & 89.18 & - \\ 
%

GPT-3 (OOD) & 92.33 & 61.50 & 79.00 & 71.03 & 59.55 & 55.41 & 73.74 & 27.31  & 64.98 & 11.51      \\ 
GPT-3.5 (OOD) & \textbf{95.92} & 66.01 & 75.84 & 66.15 & 58.43 & 67.96 & 74.01 & 30.77 & 66.90 & \textbf{11.49} \\
ELECTRA-large (OOD)& 95.14 & \textbf{76.94} & \textbf{80.44} & \textbf{78.74} & \textbf{69.96} & \textbf{77.24} & \textbf{81.14}  & \textbf{37.85} & \textbf{69.68}  & 21.87       \\ 
\bottomrule

\end{tabular}
\end{adjustbox}
\caption{OOD performance of GPT-3 and GPT3.5 using in-context learning compared with human performance and ELECTRA-large. We randomly select a single instance for each label. GPT-3 refers to text-davinci-003, and GPT-3.5 denotes the gpt-3.5-turbo.}
\label{tab:gpt3}
\end{table*}