
Many state of the art SSL approaches rely on input augmentations in order to selectively extract task-relevant information. One prominent framework, SimCLR \citep{simclr}, trains a backbone feature extractor $f(\cdot)$ to minimize a contrastive loss applied to the representations of two augmented versions of an image. Specifically, given a batch of $N$ input images $\mathbf{X} = \{\vx_1, \vx_2, \ldots, \vx_N\}$, a similarity function $\mathrm{sim}(\va, \vb) = \frac{\va^T \vb}{||\va||\cdot ||\vb||}$, and a non-linear `projection head' $h: \gZ \rightarrow \gY$, the SimCLR loss is given as:
\vspace{-1mm}
\begin{equation}
\label{eqn:simclr}
    \footnotesize
    \mathcal{L}_{\text{A-SSL}}(\mathbf{X}) = - \frac{1}{N} \sum_{i}^N \E_{g_1, g_2 \sim G} \log{\frac{\mathrm{exp}\Big(\mathrm{sim}\big(h(f(T_{g_1} [\vx_i])), h(f(T_{g_2}[\vx_i]))\big) / \tau \Big)}{\sum_{k \neq i}^{N} \sum^2_{j,l}  \mathrm{exp}\Big(\mathrm{sim}\big(h(f(T_{g_j} [\vx_i])), h(f(T_{g_l} [\vx_k]))\big) / \tau\Big)}} \ ,
\end{equation}
\normalsize
where $G$ is the set of all augmentations, $T_g[\vx]$ denotes the action of the sampled augmentation $g$ on the input, and $\tau$ is the `temperature' of the softmax. In this work, we will focus on the SimCLR objective for simplicity, but our analysis also applies to non-contrastive frameworks such as BYOL \cite{byol} and SimSiam \cite{simsiam}, provided the backbone is equivariant (i.e. augmentation-homomorphic). 

\section{Homomorphic Self-Supervised Learning}
\label{sec:s-ssl}
In this section we introduce Homomorphic Self-Supervised Learning (H-SSL) as a general framework for SSL with homomorphic encoders, and further show how many existing SSL algorithms can be both unified and generalized. To begin, consider an A-SSL objective such as Equation \ref{eqn:simclr} when $f$ is equivariant with respect to the input augmentation. By the definition of equivariant maps in Equation \ref{eqn:equivariance}, the augmentation commutes with the feature extractor: $h(f(T_g [\vx])) = h(\Gamma_g [f(\vx)])$. 
Thus, replacing $f(\vx_i)$ with its output $\vz_i = \vz(\vg_{0})$, and applying the definition of the operator, we get: 
\vspace{0mm}
\begin{equation}
    \footnotesize
    \label{eq:s-ssl}
     \mathcal{L}_{\text{H-SSL}}(\mathbf{X}) = - \frac{1}{N} \sum_{i}^N \E_{g_1, g_2 \sim G} \log{\frac{\mathrm{exp}\Big(\mathrm{sim}\big(h(  \vz_i(g_1^{-1} \cdot\vg_{0})), h(\vz_i(g_2^{-1} \cdot \vg_{0}))\big) / \tau\Big)}{\sum_{k \neq i}^{N} \sum_{j,l}^2  \mathrm{exp}\Big({\mathrm{sim}\big(h( \vz_i(g_j^{-1} \cdot \vg_{0})), h( \vz_k(g_k^{-1} \cdot \vg_{0}))}\big) / \tau\Big)}} \ .
\end{equation}
\normalsize
\looseness=-1
Ultimately, we see that $\gL_{\text{H-SSL}}$ subsumes the use of input augmentations by defining the `positive pairs' in the numerator as two fiber bundles from \emph{the same representation $\vz_i$}, simply indexed using two differently transformed base spaces $g_1^{-1} \cdot\vg_{0}$ and $g_2^{-1} \cdot \vg_{0}$ (depicted in Figure \ref{fig:equiv}). Interestingly, this loss highlights the base space $\vg_0$ as a parameter choice previously unexplored in the A-SSL frameworks.


A second interesting consequence of this derivation is the striking similarity of the $\gL_{\text{H-SSL}}$ objective and other existing SSL objectives which operate without explicit input augmentations to generate multiple views. Specifically, when applied to images, Greedy InfoMax (GIM) \citep{gim} and Contrastive Predictive Coding (CPC) \citep{cpc}\footnote{In CPC, the authors use an autoregressive encoder to encode one element of the positive pairs. In GIM, they find that in the visual domain, this autoregressive encoder is not necessary, and thus the loss reduces to a standard contrastive loss between the representations from raw spatial patches, as defined here.} use a similar InfoNCE-inspired loss (as in SimCLR) but between different spatial locations of a convolutional filter stack for a single image. Similarly, the `local' term in the Deep InfoMax objective (DIM(L)) \citep{dim} operates entirely within convolutional feature maps. Consequently, these losses are contained in our framework where $\gG$ is set to the 2D translation group, and $\vg_0$ is a small subset of the spatial coordinates. Since $\gL_{\text{H-SSL}}$ is also derived directly from $\gL_{\text{A-SSL}}$ (when $f$ is equivariant), we see that it provides a means to unify these previously distinct sets of SSL objectives. In Section \ref{sec:experiments} we validate this theoretical equivalence empirically. Furthermore, since $\gL_{\text{H-SSL}}$ is defined for transformation groups beyond translation, it can be seen to generalize these augmentation-free objectives in a way that we have not previously seen exploited in the literature. In Section \ref{sec:experiments} we include a preliminary exploration this generalization to scale and rotation groups.
