\section{Experiments}
\label{sec:experiments}
We now empirically validate the derived equivalence of A-SSL and H-SSL in practice, and further reinforce our stated assumptions by demonstrating how H-SSL objectives are ineffective when representational structure is removed. We study how the parameters of H-SSL (topographic distance) relate to those traditionally used in A-SSL (augmentation strength), and finally explore how new parameter generalizations afforded by our framework (such as choices of $\vg_0$ and $\gG$) impact performance. 

\paragraph{Empirical Validation} For perfectly equivariant networks $f$, and sets of transformations which exactly satisfy the group axioms, the equivalence between Equations \ref{eqn:simclr} and \ref{eq:s-ssl} is exact. However, in practice, due to discretization, boundary effects, and sampling artifacts, even for simple transformations such as translation, equivariance has been shown to not be strictly satisfied \citep{cnnequi}. In Table \ref{tab:equivalence} we empirically validate our proposed theoretical equivalence between $\gL_{\text{A-SSL}}$ and $\gL_{\text{H-SSL}}$, showing a tight correspondence between the downstream accuracy of linear classifiers trained on representations learned via the two frameworks. Precisely, for each transformation (Rotation, Translation, Scale), we use a backbone network which is equivariant specifically with respect to that transformation (e.g. rotation equivariant CNNs, regular CNNs, and Scale Equivariant Steerable Networks (SESN) \cite{SESN}). In all settings, we take $\vz$ from the final possible layer and set $\vg_0$ to be a single fiber of dimension 128. 


\begin{table}[h]
\vspace{-2mm}
\caption{
\looseness=-1
MNIST \cite{mnist}, CIFAR10 \cite{c10} and Tiny ImageNet \cite{tin} top-1 test accuracy (mean $\pm$ std. over 3 runs) of a detached classifier trained on the representations from SSL methods with different backbones. We compare $\gL_{\text{A-SSL}}$ and  $\gL_{\text{H-SSL}}$ with random frozen and fully supervised backbones. We see equivalence between A-SSL and H-SSL as desired from the first two columns, and often a significant improvement in performance for H-SSL methods when moving from Translation to generalized groups such as Scale. Full experiment details can be found in Appendix \ref{appendix:C}.}
\label{tab:equivalence}
\begin{center}
\small
\begin{tabular}{c c c c c c c} 
\toprule
  Dataset & Transformation & Backbone & A-SSL & H-SSL & Frozen & Supervised \\ 
 \cmidrule(lr){1-1} \cmidrule(lr){2-3} \cmidrule(lr){4-7}
 \multirow{3}{*}{MNIST}
  & Rotation  & Rot-Eq. & {68.2 $\pm$ 2.5} &  {70.3 $\pm$ 5.4} & \textit{87.2 $\pm$ 0.8} & \textit{99.4 $\pm$ 0.1} \\
 & Translation & CNN & {95.9 $\pm$ 0.3} & {96.0 $\pm$ 1.3} & \textit{94.1 $\pm$ 0.3} & \textit{99.2 $\pm$ 0.1} \\
 & Scale & SESN & {98.6 $\pm$ 0.1} & {98.3 $\pm$ 0.2} & \textit{94.7 $\pm$ 0.6} & \textit{99.3 $\pm$ 0.1}\\ 
 \cmidrule(lr){1-1} \cmidrule(lr){2-3} \cmidrule(lr){4-7}
 \multirow{3}{*}{CIFAR10}
  & Rotation & Rot-Eq. & {46.1 $\pm$ 0.6} & {48.3 $\pm$ 0.5} & \textit{38.4 $\pm$ 0.1} & \textit{73.0 $\pm$ 1.1}  \\
  & Translation & CNN & {39.2 $\pm$ 0.5} & {36.3 $\pm$ 1.1} & \textit{40.4 $\pm$ 0.2} & \textit{76.2 $\pm$ 1.4} \\
 & Scale & SESN & {59.4 $\pm$ 0.2} & {56.7 $\pm$ 0.4} & \textit{41.1 $\pm$ 0.6} & \textit{78.0 $\pm$ 0.2} \\ 
 \cmidrule(lr){1-1} \cmidrule(lr){2-3} \cmidrule(lr){4-7}
 \multirow{2}{*}{Tiny ImageNet}
  & Rotation & Rot-Eq. & {14.9 $\pm$ 0.3} & {13.5 $\pm$ 0.5} & \textit{6.1 $\pm$ 0.2} & \textit{22.5 $\pm$ 0.1}  \\
 & Scale & SESN & {16.2 $\pm$ 0.4} & {14.0 $\pm$ 1.3} & \textit{6.4 $\pm$ 0.2} & \textit{23.7 $\pm$ 0.2} \\ 
\bottomrule
 \end{tabular}
\end{center}
\vspace{-3mm}
 \end{table}



\paragraph{H-SSL Without Structure} To validate our assertion that $\gL_{\text{H-SSL}}$ requires a homomorphism, in Table \ref{tab:non_equivariant} we show the same models from Table \ref{tab:equivalence} without equivariant backbones. We observe  $\gL_{\text{H-SSL}}$ models perform significantly below their input-augmentation counterparts, and similarly to a `frozen' randomly initialized backbone baseline -- indicating the learning algorithm is no longer effective.


\vspace{-2mm}
\begin{table}[h]
\caption{An extension of Table \ref{tab:equivalence} with non-equivariant backbones. 
We see that the H-SSL methods perform similar to, or worse than, the frozen baseline when equivariance is removed, as expected.}
\label{tab:non_equivariant}
\begin{center}
\small
\begin{tabular}{c c c c c c c} 
\toprule
  Dataset & Transformation & Backbone & A-SSL & H-SSL & Frozen & Supervised \\ 
  \cmidrule(lr){1-1} \cmidrule(lr){2-3} \cmidrule(lr){4-7}
 \multirow{2}{*}{MNIST}
 & Translation & MLP &  87.6 $\pm$ 0.2 & 58.2 $\pm$ 0.5  & 83.0 $\pm$ 0.8 & 98.6 $\pm$ 0.1 \\
 & Scale & CNN ($6 \times CHW$) & 95.2 $\pm$ 0.1 & 87.2 $\pm$ 2.4 & 87.2 $\pm$ 0.6 & 99.3 $\pm$ 0.1\\ 
 \cmidrule(lr){1-1} \cmidrule(lr){2-3} \cmidrule(lr){4-7}
 \multirow{1}{*}{CIFAR10}
 & Scale & CNN ($6 \times CHW$) & 53.6 $\pm$ 0.2 & 37.5 $\pm$ 0.1 & 43.6 $\pm$ 0.3 & 67.9 $\pm$ 2.1 \\ 
\bottomrule
 \end{tabular}
\end{center}
 \end{table}
\vspace{-3mm}


\paragraph{Parameters of H-SSL} As discussed, The H-SSL framework highlights new parameter choices such as the base space $\vg_0$. In Figure \ref{fig:size_topo_dist} (left) we plot the $\%$-change in top-1 accuracy on CIFAR-10 as we increase the total size of $\vg_0$ from 1 (akin to DIM(L) losses) to $|G|-1$ (akin to SimCLR). We see a minor increase in performance as we increase the size, but note relative stability, again suggesting greater unity between A-SSL and H-SSL. In Figure \ref{fig:size_topo_dist} (right), we explore how the traditional notion of augmentation `strength' can be equated with the `topographic distance' between $g_1$ and $g_2$ and their associated fiber bundles. Here we approximate topographic distance as euclidean distance between group elements for simplicity ($||g_1 - g_2||_2^2$), where a more correct measure would be computed using the topology of the group. We see, in alignment with prior work \cite{what_makes_good_views}, that the strength of augmentation (and specifically translation distance) is an important parameter for effective self supervised learning, likely relating to the mutual information between fibers as a function of distance.  



\begin{figure}[h]
\caption{Study of the impact of new H-SSL parameters top-1 test accuracy. (Left) Test accuracy marginally increases as we increase total base space size $
\vg_0$. (Right) Test accuracy is constant or decreases as we increase the maximum distance between fiber bundles considered positive pairs.}
\vspace{-2mm}
\centering
\includegraphics[width=1.0\textwidth]{figures/size_topo_dist.png}
\label{fig:size_topo_dist}
\end{figure}
\vspace{-10mm}


