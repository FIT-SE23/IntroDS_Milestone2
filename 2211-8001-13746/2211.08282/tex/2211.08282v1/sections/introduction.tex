\section{Introduction}
\label{sec:introduction}

\looseness=-1
Many self-supervised learning (SSL) techniques can be colloquially defined as representation learning algorithms which extract approximate supervision signals directly from the input data itself \citep{ssl-dark-matter}. In practice, this supervision signal is often obtained by performing symmetry transformations of the input with respect to task-relevant information, meaning the transformations leave task-relevant information unchanged, while altering task-irrelevant information. Numerous theoretical and empirical works have shown that by combining such symmetry transformations with contrastive objectives, powerful lower dimensional representations can be learned which support linear-separability, identifiability of generative factors, and reduced sample complexity \citep{SSL_MV, marco_MV, wang2022chaos, ji2021power, ssl_content_style, approx_CI, ddn, theory_cl, cl_mv, cmc, byol, simclr, simsiam}. One rapidly developing domain of deep learning research which is specifically focused on the structured and accurate representation of the input with respect to symmetry transformations is that of equivariant neural networks \citep{gcnn,  cohen2016steerable, worrall2017harmonic, weiler20183d, scalespaces, finzi2020generalizing, finzi2021emlp}.
In this work, we study the properties of SSL algorithms when equivariant neural networks are used as backbone feature extractors. Interestingly, we find a convergence of existing loss functions from the literature, and ultimately generalize these with the framework of \emph{Homomorphic Self-Supervised Learning}. 
\begin{figure}[h]
\caption{Overview of Homomorphic-SSL (left) and its relation to traditional Augmentation-based SSL (right). Positive pairs extracted from the lifted dimension ($\theta$) of a rotation equivariant network (G-conv) are equivalent to pairs extracted from the separate representations of two rotated images.}
\centering
\vspace{-2mm}
\includegraphics[width=0.90\textwidth]{figures/equiv2.png}
\label{fig:equiv}
\vspace{-6mm}
\end{figure}
\raggedbottom




