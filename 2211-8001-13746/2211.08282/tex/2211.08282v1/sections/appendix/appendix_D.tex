\section{Extended Background}

\paragraph{Related Work}
Our work is undoubtedly built upon the the large literature base from the fields equivariant deep learning and self-supervised learning as outlined in Sections \ref{sec:introduction} and \ref{sec:background}. Beyond this background, our work is highly related in motivation to a number of studies specifically related to equivariance in self-supervised learning. Most prior work, however, has focused on the undesired invariances learned by A-SSL methods \cite{xiao2021what, tsai2022conditional} and on developing methods by which to avoid this through learned approximate equivariance \cite{dangovski2022equivariant, wang2021residual}. Our work is, to the best of our knowledge, the first to suggest and validate that the primary reason for the success of feature-space SSL objectives such as DIM(L) \cite{dim} and GIM \cite{gim} is due to their exploitation of equivariant backbones.

\paragraph{Group-Convolutional Neural Networks}
As discussed in Section \ref{sec:background}, we assume that the backbones used in this work are equivariant with respect to input augmentations, and further that they admit regular representations of those transformations in feature space. In this section we detail how such group-equivariant convolutional neural networks may be constructed via the group-convolution \cite{gcnn}: For a discrete group $\gG$, we denote the pre-activation output of a $\gG$-equivariant convolutional layer $l$ as $\vz^l$, with a corresponding input $\vy^l$. In practice these values are stored in finite arrays with a feature multiplicity equal to the order of the group in each space. Explicitly, $\vz^l \in \sR^{C_{out} \times |G_{out}|}$, and $\vy^l \in \sR^{C_{in} \times |G_{in}|}$ where $G_{out}$ and $G_{in}$ are the set of group elements in the output and input spaces respectively. We use the following shorthand for indexing $\vz^l(g) \equiv \vz^{l,:,g} \in \sR^{C_{out}}$ and $\vy^l(g) \equiv \vy^{l, :, g} \in \sR^{C_{in}}$, denoting the vector of feature channels at a specific group element (`fiber'). %(later in Section 3, we extend this to refer to multiple group elements). 
% However, in the following, for the purpose of mathematical analysis (and to match existing notation), we use the standard convention of defining $\vz^l$ and $\vx^l$ as functions over the group (i.e. $\vz^l\ : \ \gG_{out} \rightarrow \mathbb{R}^{C_{out}}$ and $\vx^l\ : \ \gG_{in} \rightarrow \mathbb{R}^{C_{in}}$), and thus denote the feature vectors at each associated group element $g \in \gG$ as $\vz^l(g) \in \sR^{C_{out}}$, and $\vx^l(g) \in \sR^{C_{in}}$ respectively. 
Then, the value $z^{l,c}(g) \in \sR$ of a single output at layer $l$, channel $c$ and element $g$ is 
\begin{equation}
    z^{l,c}(g) \equiv [\vy^l \star \vpsi^{l,c}](g) = \sum_{h \in G_{in}} \sum_{i}^{C_{in}} y^{l, i}(h) \psi^{l, c}_i (g^{-1} \cdot h) \ ,
\end{equation}
where $\vpsi^{l, c}_i$ is the filter between the $i^{th}$ input channel (subscript) and the $c^{th}$ output channel (superscript), and is similarly defined (and indexed) over the set of input group elements $G_{in}$. We highlight that the composition $g^{-1} \cdot h = k \in G_{in}$ is defined by the action of the group and yields another group element by closure of the group. The representation $\Gamma_g$ and can then be defined as $\Gamma_g [\vz^l(h)] = \vz^l(g^{-1} \cdot h)$ for all $l > 0$ when $\gG^l_{in} = \gG^l_{out} = \gG^0_{out}$. From this definition it is straightforward to prove equivariance from: $[\Gamma_g [\vy^l] \star \vpsi^l](h) = \Gamma_g [\vy^l \star \vpsi^l](h) = \Gamma_g[\vz^l](h)$. Furthermore, we see that $\Gamma_g$ is a `regular representation' of the group, meaning that it acts by permuting features along the group dimension while leaving feature channels intact. Group equivariant layers can then be composed with pointwise non-linearities and biases to yield a fully equivariant deep neural network (e.g. $\vy_i^{l+1} = \mathrm{ReLU}(\vz^{l} + \vb)$ where $\vb \in \sR^{C_{out}}$ is a learned bias shared over the output group dimensions). 
For $l=0$, $\vy^0 = \vx$, the raw input, and typically $\gG^0_{in} = (\sZ^2_{HW}, +)$, the group of all 2D integer translations up to height $H$ and width $W$. $\gG^0_{out}$ is then chosen by the practitioner and is typically a larger group which includes translation as a subgroup, e.g. the roto-translation group, or the group of scaling \& translations.

\paragraph{DIM(L) in H-SSL} In this section we outline precisely how the Deep Infomax Local loss DIM(L) relates to the H-SSL framework proposed in Section \ref{sec:s-ssl}. Specifically, in Deep InfoMax (DIM(L)) the same general form of the loss function is applied (often called InfoNCE), but the cosine similarity is replaced with a log-bilinear model: $\mathrm{sim}(\va, \vb) = \mathrm{exp}\left(\va^T W \vb\right)$. Additionally, and most importantly to this work, rather than computing the similarity between two differently augmented versions on an image, the loss is applied between different spatial locations of the representation for a single image, again with a head $h$ applied afterwards. If we let $\vg \sim \sZ_{HW}^2$ refer to sampling a contiguous patch from the spatial coordinates of a convolutional feature map, we can write this general Feature-Space InfoMax loss ($\mathcal{L}_{\text{FSIM}}$) as: 
\begin{equation}
    \footnotesize
    \mathcal{L}_{\text{FSIM}}(\mathbf{X}) = - \frac{1}{N} \sum_{i}^N \E_{\vg_1, \vg_2 \sim \sZ_{HW}^2} \log{\frac{\mathrm{exp}\Big(\mathrm{sim}\big(h(\vz_i(\vg_1)), h(\vz_i(\vg_2))\big) / \tau \Big)}{\sum_{k \neq i}^{N} \sum_{j, l}^2 \mathrm{exp}\Big(\mathrm{sim}\big(h(\vz_i(\vg_j)), h(\vz_k(\vg_l))\big) / \tau\Big)}} \ .
\end{equation}
\normalsize
To show that this is equivalent to our $\mathcal{L}_{\text{H-SSL}}$, we see that the randomly sampled spatial patches $\vg_1, \vg_2$ can equivalently be described as a single base patch $\vg_{0}$ shifted by randomly sampled translations $g_1$ and $g_2$. Explicitly, 
\begin{equation}
    \footnotesize
    \label{eq:fsim}
    \mathcal{L}_{\text{FSIM}}(\mathbf{X}) = - \frac{1}{N} \sum_{i}^N \E_{g_1, g_2 \sim G} \log{\frac{\mathrm{exp}\Big(\mathrm{sim}\big(h(\vz_i(g_1^{-1} \cdot \vg_{0})), h(\vz_i(g_2^{-1} \cdot \vg_{0}))\big) / \tau \Big)}{\sum_{k \neq i}^{N} \sum_{j,l}^2 \mathrm{exp}\Big(\mathrm{sim}\big(h(\vz_i(g_j^{-1} \cdot \vg_{0})), h(\vz_k(g_l^{-1} \cdot  \vg_{0}))\big) / \tau\Big)}} \ .
\end{equation}
\normalsize
Thus, we see that Feature-Space InfoMax losses are included in our framework, and can therefore be seen to be equivalent to input-augmentation based losses with an equivariant backbone, where the set of augmentations is limited to the translation group $G \equiv \sZ_{HW}^2$, and the $\vg_0$ base size is a single spatial coordinate ($|\vg_{0}| = 1$) rather than the size of the full representation ($|\vg_{0}| = |G|$).