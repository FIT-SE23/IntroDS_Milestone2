\section{Limitations}
\label{appendix:A}
Despite the demonstrated unification of existing methods, and benefits from generalization, we note that this approach is still significantly limited. Specifically, the equivalence between $\gL_{\text{A-SSL}}$ and $\gL_{\text{H-SSL}}$, and benefits afforded by this equivalence,  can only be realized if it is possible to analytically construct a neural network which is equivariant with respect to transformations of interest. Although the field of equivariant deep learning has made significant progress in recent years, state of the art techniques are still restricted to E($n$) and continuous compact and connected Lie Groups \cite{finzi2020generalizing, finzi2021emlp, cesa2022a, e2cnn}.
We believe in this regard, our analysis sheds some light on the success of methods which perform data augmentation over those which operate directly in feature space in recent literature -- it is simply too challenging with current methods to construct models with structured representations for the diversity of transformations needed to induce a sufficient set of invariances for linear separability of classes. 

In light of this, we believe that our framework specifically suggests a novel path forward via learned homomorphisms, \cite{keller2021topographic, keurti2022homomorphism, connor2021variational, lconv, nptn}, as mentioned in the Discussion Section \ref{sec:discussion}. Specifically, if new methods can learn symmetries from the data unsupervised, or minimally supervised, our framework yields a natural avenue by which existing SSL advances can be extended to feature-space methods. As a potential interim solution,  recent work \cite{amdim} has additionally shown that it is possible to train `hybrid' models that leverage both A-SSL and H-SSL objectives simultaneously, suggesting that this limitation may be circumvented by applying H-SSL losses only where beneficial, and otherwise supplementing with input space augmentation. 

