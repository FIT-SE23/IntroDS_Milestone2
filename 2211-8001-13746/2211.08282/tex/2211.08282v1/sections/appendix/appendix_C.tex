\section{Experiment Details}
\label{appendix:C}

\paragraph{Model Architectures} All models presented in this paper were built using the convolutional layers from the SESN \cite{SESN} library for consistency and comparability (\url{https://github.com/ISosnovik/sesn}). For scale equivariant models, we used the set of 6 scales $[1.0, 1.25, 1.33, 1.5, 1.66, 1.75]$. To construct the rotation equivariant backbones, we use only a single scale of $[1.0]$ and augment the basis set with four 90-degree rotated copies of the basis functions at $[0^o, 90^o, 180^o, 270^o]$. These rotated copies thus defined the group dimension. This technique of basis or filter-augmentation for implementing equivariance is known from prior work and has been shown to be equivalent to other methods of constructing group-equivariant neural networks \cite{filtra}. For translation models, we perform no basis-augmentation, and again define the set of scales used in the basis to a single scale $[1.0]$, thereby leaving only the spatial coordinates of the final feature maps to define the output group. 

On MNIST \cite{mnist}, we used a backbone network $f$ composed of three SESN convolutional layers with \# channels (32, 64, 128), kernel sizes (11, 7, 7), effective sizes (11, 3, 3), strides (1, 2, 2), padding (5, 3, 3), no biases, basis type `A', BatchNorm layers after each convolution, and ReLU activations after each BatchNorm. The output of this final ReLU was then considered our $\vz$ for contrastive learning (with $\gL_{A-SSL}$ and $\gL_{H-SSL}$) and was of shape $(128, S \times R, 8, 8)$ where $S$ was the number of scales for the experiment (either 1 or 6), and $R$ was the number of rotation angles (either 1 or 4). For experiments where the transformation studied was not translation, we average pool over the spatial dimensions before applying the projection head $h$ to achieve a consistent dimensionality of $128$. For classification, an additional SESN convolutional layer was placed on top with kernel size 7, effective size 3, stride 2, and no padding, thereby reducing the spatial dimensions to 1, and the total dimensionality of the input to the final linear classifier to 128.

On CIFAR10 we used a ResNet20 model composed of an initial SESN lifting layer with kernel size 7, effective size 7, stride 1, padding 3, no bias, basis type `A', and 9 output channels. This lifted representation was then processed by a following SESN convolutional layer of kernel size 7, effective size 3, stride 1, padding 3, no bias, basis type `A', and 64 output channels. This initial layer was followed by a BatchNorm and ReLU before being processed by three ResNet blocks of output sizes (128, 256, 512) and initial strides of (1, 2, 2). Each ResNet block is composed of 3 SESN Basic blocks as defined here (\url{https://github.com/ISosnovik/sesn/blob/master/models/stl_ses.py#L19}). The output of the third ResNet block was taken as our $\vz$ for contrastive learning (again for $\gL_{A-SSL}$ and $\gL_{H-SSL}$) of shape $(512, S \times R, 7, 7)$. Again, as for MNIST, for experiments where the transformation studied was not translation, we average pool over the spatial dimensions before applying the projection head $h$ to achieve a consistent dimensionality of $512$. For classification, the vector $\vz$ was first max-pooled along the scale/rotation group-axis ($S \times R$), followed by a BatchNorm, a ReLU, and average pooling over the remaining $7 \times 7$ spatial dimensions. Finally, we apply BatchNorm to this 512-dimensional vector before applying the non-linear projection head $h$.

On Tiny ImageNet we use a Resnet20 model which has virtually the same structure as the CIFAR10 model, but instead uses 4 ResNet blocks of output sizes (64, 128, 256, 512) and strides (1, 2, 2, 2). Furthermore, each ResNet block is composed of only 2 BasicBlocks for TIN instead of 3 for CIFAR10. Overall this results in a $\vz$ of shape $(512, S \times R, 4, 4)$, and a final vector for classification of size 512. We note that we do not include Translation results in Table \ref{tab:equivalence} for Tiny ImageNet precisely because the spatial dimensions of the feature map with this architecture are too small to allow for effective H-SSL training in the settings we used for other methods. 

All models used a detached linear classifier for computing the reported downstream classification accuracies, while the Supervised baselines used an attached linear layer (implying gradients with respect to the classification loss back-propagated though the whole network). All models additionally used an attached non-linear projection head $h$ constructed as an MLP with three linear layers. For MNIST these layers have of output sizes (128, 128, 128), while for CIFAR10 and TIN they have sizes (512, 2048, 512). There is a BatchNorm after each layer, and ReLU activations between the middle layers (not at the last layer). 

\paragraph{Training Details} For training we use the LARS optimizer with an initial learning rate of 0.1, and a batch size of 4096 for all models. We use an NCE temperature ($\tau$) of 0.1, half-precision training, a learning rate warm-up of 10 epochs, a cosine lr-update schedule, and weight decay of $1\times10^{-4}$. On MNIST we train for 500 epochs and on CIFAR10 and Tiny ImagNet (TIN) we train for 1300 epochs. On average each MNIST run took 1 hour to complete distributed across 8 GPUs, and each CIFAR10/TIN run took 10 hours to complete distributed across 64 GPUs. In total this amounts to roughly 85,000 GPU hours.

\paragraph{Empirical Validation} For the experiments in Table \ref{tab:equivalence}, we use two different methods for data augmentation, and similarly two different methods for selecting the representations ultimately fed to the contrastive loss for the A-SSL and H-SSL settings.

For A-SSL we augment the input at the pixel level by: randomly translating the image by up to $\pm$ $20\%$ of its height/width (for translation), randomly rotating the image by one of ($0^o, 90^o, 180^o, 270^o$) (for rotation), or randomly downscaling the image between $0.57$ and $1.0$ of its original scale. For S-SSL we use no input augmentations.

For both methods we use only a single fiber, meaning the base size $|\vg_0|$ is 1. For A-SSL, we randomly select the location $\vg_0$ for each example, but we use the same $\vg_0$ between both branches. For example, in translation, we compare the feature vectors for two translated images \emph{at the same pixel location}. Similarly, for scale and rotation, we pick a single scale or rotation element to compare for both branches. For H-SSL, we randomly select the location $\vg$ independently for each example \emph{and independently for each branch}, effectively mimicing the latent operator. 

\paragraph{H-SSL Without Structure}
In Table \ref{tab:non_equivariant}, we use the same overall model architectures defined above (3-layer model or ResNet20), but replace the individual layers with non-equivariant counterparts. Specifically, for the MLP, we replace the convolutional layers with fully connected layers with outputs (784, 1024, 2048). For the convolutional models (denoted CNN ($6 \times CHW$)), we replace the SESN kernels with fully-parameterized, non-equivariant counterparts, otherwise keeping the output dimensionality the same (resulting in the 6 $\times$ larger output dimension).

Furthermore, for these un-structured representations, in the H-SSL setting, we `emulate' a group dimension to sample `fibers' from. Specifically, for the MLP we simply reshape the 2048 dimensional output to ($16$,$128$), and select one of the 16 rows at each iterations. For the CNN, we similarly use the 6 times larger feature space to sample $\frac{1}{6}^{th}$ of the  elements as if they were scale-equivariant.

\paragraph{Parameters of H-SSL}
For Figure $\ref{fig:size_topo_dist}$ (left), we select patches of sizes from $1$ to $|G| - 1$ with no padding. In each setting, we similarly increase the dimensionality of the input layer for the non-linear projection head $h$ to match the multiplicative increase in the dimension of the input representation $\vz(\vg)$. For the topographic distance experiments (right), we keep a fixed base size of $|\vg_0| = 1$ and instead vary the maximum allowed distance between randomly sampled pairs $g_1$ \& $g_2$. 