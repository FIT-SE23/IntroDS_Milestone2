\section{Structured Predictive Coding}

\paragraph{$\gG$-patches} We can now take advantage of this structured feature space to define a locally-contiguous patch of feature space. We see that this generalizes the frequently used self-supervise notion of a 'spatial-patch' to arbitrary group dimensions, and we thus refer to such patches as $\gG$-patches. Formally, we slightly abuse notation to write $\vz_i(\vg)$ denoting the vector composed all elements $z^{l,c}(g)$ for all $C_{out}$ channels and $g \in \vg$ where $\vg$ is a set of group elements. A $\gG$-patch is thus the vector: 
\begin{equation}
    \vz^l(\vg) = [z^{l,0}(g_1),\  z^{l,0}(g_2),\  \ldots,\    z^{l,1}(g_1),\  z^{l,1}(g_2),\ \ldots,\ z^{l,C_{out}}(g_{1}),\  z^{l,C_{out}}(g_{2}),\  \ldots]
\end{equation}

\subsection{Contrastive Losses}
Then, given a batch of $N$ input images $\mathbf{Y} = \{\vy_1, \vy_2, \ldots, \vy_N\}$, and a similarity function:
\begin{equation}
    \mathrm{sim}(\va, \vb) = \frac{\va^T \vb}{||\va||\cdot ||\vb||}
\end{equation}

\paragraph{$\gG$-patch Loss}
We can then write the general $\gG$-patch contrastive learning loss for layer $l$ as:
\begin{equation}
    \mathcal{L}^{l}_{\gG-patch}(\mathbf{Y}) = - \frac{1}{N} \sum_{i}^N \E_{\vg_1, \vg_2 \sim G} \ln{\frac{\mathrm{exp}\Big(\mathrm{sim}\big(\vz^l_i(\vg_1), \vz^l_j(\vg_2)\big)\Big)}{\sum_{k \neq i}^{N}  \mathrm{exp}\Big({\mathrm{sim}\big(\vz^l_i(\vg_1), \vz^l_k(\vg_2)}\big)\Big)}}
\end{equation}
Where $G$ denotes the set of all desired subsets of the full output group $\gG$, where $|\vg_i| \geq 1$ and we restrict our sampling procedure such that $|\vg_1| = |\vg_2|$. The loss could then be summed over all layers $l$ and back-propagated.

Possible variants: 
\begin{itemize}
    \item Make negative examples come from the same $\gG$-patch, i.e. the denominator uses $\vg_i$ for both samples.
    \item Allow $|\vg_1| \neq |\vg_2|$ and a perform projection or pooling operation.
    \item Stop the gradient between each layer, such that layer $l$ only receives loss from $\mathcal{L}^{l}_{\gG-patch}$, or gradient as the average of all layers which feedback to that layer.
\end{itemize}

\paragraph{Global-Local $\gG$-patch Loss}
Variant 1: Single-Layer (Intra-Global)
\begin{equation}
    \mathcal{L}^{l}_{intra-global}(\mathbf{Y}) = - \frac{1}{N} \sum_{i}^N \E_{\vg_1, \vg_2 \sim G} \ln{\frac{\mathrm{exp}\Big(\mathrm{sim}\big(\vz^l_i(\vg_1), \vz^l_j(\vg_2)\big)\Big)}{\sum_{k \neq i}^{N}  \mathrm{exp}\Big({\mathrm{sim}\big(\vz^l_i(\vg_1), \vz^l_k(\vg_2)}\big)\Big)}}
\end{equation}
Where $\vg_1$ is defined to be a super-set of $\vg_2$: $\vg_1 > \vg2$, and some pooling or projection operator is used to compare the two patches. Examples could be simple learned projection operators or Random Fourier Features. 

Variant 2: Between Layers (Extra-Global)
\begin{equation}
    \mathcal{L}^{l}_{extra-global}(\mathbf{Y}) = - \frac{1}{N} \sum_{i}^N \E_{\vg_1, \vg_2 \sim G} \ln{\frac{\mathrm{exp}\Big(\mathrm{sim}\big(\vz^{l+1}_i(\vg_1), \vz^l_j(\hat{\vg}_1)\big)\Big)}{\sum_{k \neq i}^{N}  \mathrm{exp}\Big({\mathrm{sim}\big(\vz^{l+1}_i(\vg_1), \vz^l_k(\hat{\vg}_2)}\big)\Big)}}
\end{equation}
Where $\hat{\vg_i}$ is the group element in a layer below with approximately corresponds to the group element $\vg$ in the layer above. This can be determined by the recptive field of the group-convolutional kernel. 

\paragraph{Patch-Sampling}
One point of interest is how we define the set of patches $G$, and further how we sample pairs of those patches for positives and negatives. For example, perhaps we only sample positive examples which are close in $\gG$-space, or perhaps we can sample negatives from the same image only if they are far enough away in the $\gG$-space.

\paragraph{Similarity functions}
Another important point to consider is which similarity function is used, or which mutual information lower bound is used. One option is to use a MINE-like similarity loss (like DeepInfoMax) instead of NCE listed above. This would employ a learned mapping between the two elements and may allow for softer similarity within an equivariant dimension (we think helpful). 

\subsubsection{Student-Teacher Losses}
\paragraph{$\gG$-Patch-BYOL} Drop a patch of features in the student network, try to reconstruct the teacher's final feature vector.

\subsection{Reconstruction Losses}
\paragraph{$\gG$-Patch-Mask} Drop patches of features and try to reconstruct the input.



\subsection{On the Relationship with Data-Augmentations}
Our loss can be seen to be identical to a contrastive loss which operates on (a patch) of the same feature vector across two augmented images. Explicitly, if we have two augmented versions of an image: $\vy_i$ \& $\tau_g \vy_i = \hat{\vy_i}$, a standard contrastive learning algorithm may strive to maximize the similarity of the representations of each of these ($\phi(\vy_i)$ \& $\phi(\tau_g \vy_i)$) relative to other images: 
\begin{equation}
    \mathcal{L}^{l}_{Data-Aug}(\mathbf{Y}) = - \frac{1}{N} \sum_{i}^N \E_{g \sim G} \ln{\frac{\mathrm{exp}\Big(\mathrm{sim}\big(\phi^l(\vy_i), \phi^l(\tau_g \vy_i)\big)\Big)}{\sum_{k \neq i}^{N}  \mathrm{exp}\Big({\mathrm{sim}\big(\phi^l(\vy_i), \phi^l(\vy_k))}\big)\Big)}}
\end{equation}
If we know that $\phi$ is equivariant with respect to the group $G$, then $\phi(\tau_g \vy_i) = \Gamma_g \phi(\vy_i)$ by definition. If we then replace $\phi(\vy_i)$ with $\vz_i(\vg_{all})$, where $\vg_{all}$ denotes the full set of group elements i.e. the full feature space, and then apply the definition of the operator, $\Gamma_g \vz_i(h) = \vz_i(g^{-1}h)$, we see that we get:
\begin{equation}
    \ldots  = - \frac{1}{N} \sum_{i}^N \E_{g \sim G} \ln{\frac{\mathrm{exp}\Big(\mathrm{sim}\big(\vz^l_i(\vg_{all}), \vz^l_i(g^{-1}\vg_{all}))\big)\Big)}{\sum_{k \neq i}^{N}  \mathrm{exp}\Big({\mathrm{sim}\big(\vz^l_i(\vg_{all}), \vz^l_k(\vg_{all})))}\big)\Big)}}
\end{equation}
The quantity of interest $\vz^l_i(g^{-1}\vg_{all})$ can be seen to be the original feature vector, viewed under a $g^{-1}$ shifted coordinate system. The dot product in the $sim$ function then computes the multiplication of each element $\vz^l_i(g_1)$ with $\vz^l_i(g^{-1}g_1) = \vz^l_i(g_2)$, and sums the result. 

Thus, our loss can be seen as the same as the above, $\mathcal{L}^{l}_{Data-Aug}$, but where the dot-product is computed over for each local patches rather than the full feature vector. $\vz^l_i(\vg_1)$ , 


\subsection{Advantages over existing methods}

\begin{itemize}
\item More positive examples per data point, potentially harder pretext tasks, yields better sample efficiency
\item Compared with taking random subsets of features, structured 'contiguous' sets of feature space will be harder to predict, and thus require the inference of more complicated latent variables $\rightarrow$ better representations.
\item Avoiding augmentations removes some of the problems associated with learning incorrect invariances. 
\item Generalized notion of how to define patches allows us many more avenues for defining contrast over learned transformations.
\end{itemize}

\subsection{Intuition: Self-Similarity Bias}
One intuitive argument for our above loss is the following: Objects of interest (classes) which are contained in an individual image, tend to have self-similar features which relate to one-another by geometric 'symmetry' transformations. Our objective can be seen to specifically encourage learning of these features which are distinct from other images. 

If the contrastive $\gG$-patch loss above is applied to a simple SO(2)-equivariant network, it can thought of as trying to enforce greater similarity between features learned at different rotation angles and spatial locations of a single image compared with features of any location/angle of separate images. Distilled, this seems to be encoding the idea that objects/categories of interest are composed of many self-similar parts. 

For example, our loss applied to a 3 will try to enforce features which curve at different rotation angles since these are more similar to eachoter for 3 than they are for 4. (Question: Is this already captured by equivariance?)