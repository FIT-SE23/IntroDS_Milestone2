@book{serre,
  added-at = {2019-07-19T00:00:00.000+0200},
  author = {Serre, Jean-Pierre},
  biburl = {https://www.bibsonomy.org/bibtex/2e1631c485bc10c9bd4063c2c34fc499a/dblp},
  interhash = {46becaca8f6d23ac6e5fa5cab03b9f38},
  intrahash = {e1631c485bc10c9bd4063c2c34fc499a},
  isbn = {978-3-540-90190-7},
  keywords = {dblp},
  pages = {I-X, 1-170},
  publisher = {Springer},
  series = {Graduate texts in mathematics},
  timestamp = {2019-07-20T11:38:12.000+0200},
  title = {Linear representations of finite groups.},
  volume = 42,
  year = 1977
}

@inproceedings{shi2022adversarial,
  title={Adversarial Masking for Self-Supervised Learning},
  author={Shi, Yuge and Siddharth, N and Torr, Philip HS and Kosiorek, Adam R},
  booktitle = {International Conference on Machine Learning},
  year={2022}
}

@InProceedings{pmlr-v163-blaas22a,
  title = 	 {Challenges of Adversarial Image Augmentations},
  author =       {Blaas, Arno and Suau, Xavier and Ramapuram, Jason and Apostoloff, Nicholas and Zappella, Luca},
  booktitle = 	 {Proceedings on "I (Still) Can't Believe It's Not Better!" at NeurIPS 2021 Workshops},
  pages = 	 {9--14},
  year = 	 {2022},
  editor = 	 {Pradier, Melanie F. and Schein, Aaron and Hyland, Stephanie and Ruiz, Francisco J. R. and Forde, Jessica Z.},
  volume = 	 {163},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {13 Dec},
  publisher =    {PMLR},
  pdf = 	 {https://proceedings.mlr.press/v163/blaas22a/blaas22a.pdf},
  url = 	 {https://proceedings.mlr.press/v163/blaas22a.html},
  abstract = 	 {Image augmentations applied during training are crucial for the generalization performance of image classifiers. Therefore, a large body of research has focused on finding the optimal augmentation policy for a given task. Yet, RandAugment [2], a simple random augmentation policy, has recently been shown to outperform existing sophisticated policies. Only Adversarial AutoAugment (AdvAA) [11], an approach based on the idea of adversarial training, has shown to be better than RandAugment. In this paper, we show that random augmentations are still competitive compared to an optimal adversarial approach, as well as to simple curricula, and conjecture that the success of AdvAA is due to the stochasticity of the policy controller network, which introduces a mild form of curriculum.}
}



@misc{keurti2022homomorphism,
  doi = {10.48550/ARXIV.2207.12067},
  
  url = {https://arxiv.org/abs/2207.12067},
  
  author = {Keurti, Hamza and Pan, Hsiao-Ru and Besserve, Michel and Grewe, Benjamin F. and Schölkopf, Bernhard},
  
  keywords = {Machine Learning (cs.LG), Group Theory (math.GR), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences, FOS: Mathematics, FOS: Mathematics},
  
  title = {Homomorphism Autoencoder -- Learning Group Structured Representations from Observed Transitions},
  
  publisher = {arXiv},
  
  year = {2022},
  
  copyright = {Creative Commons Attribution 4.0 International}
}

@article{amdim,
  Author={Bachman, Philip and Hjelm, R Devon and Buchwalter, William},
  Journal={arXiv preprint arXiv:1906.00910},
  Title={Learning Representations by Maximizing Mutual Information Across Views},
  Year={2019}
}

@inproceedings{
keller2021topographic,
title={Topographic {VAE}s learn Equivariant Capsules},
author={T. Anderson Keller and Max Welling},
booktitle={Advances in Neural Information Processing Systems},
editor={A. Beygelzimer and Y. Dauphin and P. Liang and J. Wortman Vaughan},
year={2021},
url={https://openreview.net/forum?id=AVWROGUWpu}
}

@inproceedings{what_makes_good_views, author = {Tian, Yonglong and Sun, Chen and Poole, Ben and Krishnan, Dilip and Schmid, Cordelia and Isola, Phillip}, title = {What Makes for Good Views for Contrastive Learning?}, year = {2020}, isbn = {9781713829546}, publisher = {Curran Associates Inc.}, address = {Red Hook, NY, USA}, abstract = {Contrastive learning between multiple views of the data has recently achieved state of the art performance in the field of self-supervised representation learning. Despite its success, the influence of different view choices has been less studied. In this paper, we use theoretical and empirical analysis to better understand the importance of view selection, and argue that we should reduce the mutual information (MI) between views while keeping task-relevant information intact. To verify this hypothesis, we devise unsupervised and semi-supervised frameworks that learn effective views by aiming to reduce their MI. We also consider data augmentation as a way to reduce MI, and show that increasing data augmentation indeed leads to decreasing MI and improves downstream classification accuracy. As a byproduct, we achieve a new state-of-the-art accuracy on unsupervised pre-training for ImageNet classification (73% top-1 linear readout with a ResNet-50).}, booktitle = {Proceedings of the 34th International Conference on Neural Information Processing Systems}, articleno = {573}, numpages = {13}, location = {Vancouver, BC, Canada}, series = {NIPS'20} }

 @misc{ssl-dark-matter, title={Self-supervised learning: The dark matter of intelligence}, url={https://ai.facebook.com/blog/self-supervised-learning-the-dark-matter-of-intelligence/}, journal={Meta AI}, author={LeCun, Yann and Misra, Ishan}, year={2021}, month={Mar}} 

@misc{SSL_MV,
  doi = {10.48550/ARXIV.2006.05576},
  
  url = {https://arxiv.org/abs/2006.05576},
  
  author = {Tsai, Yao-Hung Hubert and Wu, Yue and Salakhutdinov, Ruslan and Morency, Louis-Philippe},
  
  keywords = {Machine Learning (cs.LG), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Self-supervised Learning from a Multi-view Perspective},
  
  publisher = {arXiv},
  
  year = {2020},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@article{mnist,
  added-at = {2010-06-28T21:16:30.000+0200},
  author = {LeCun, Yann and Cortes, Corinna},
  biburl = {https://www.bibsonomy.org/bibtex/2935bad99fa1f65e03c25b315aa3c1032/mhwombat},
  groups = {public},
  howpublished = {http://yann.lecun.com/exdb/mnist/},
  interhash = {21b9d0558bd66279df9452562df6e6f3},
  intrahash = {935bad99fa1f65e03c25b315aa3c1032},
  keywords = {MSc _checked character_recognition mnist network neural},
  lastchecked = {2016-01-14 14:24:11},
  timestamp = {2016-07-12T19:25:30.000+0200},
  title = {{MNIST} handwritten digit database},
  url = {http://yann.lecun.com/exdb/mnist/},
  username = {mhwombat},
  year = 2010
}

@article{c10,
title= {CIFAR-10 (Canadian Institute for Advanced Research)},
journal= {},
author= {Alex Krizhevsky and Vinod Nair and Geoffrey Hinton},
year= {},
url= {http://www.cs.toronto.edu/~kriz/cifar.html},
abstract= {The CIFAR-10 dataset consists of 60000 32x32 colour images in 10 classes, with 6000 images per class. There are 50000 training images and 10000 test images. 

The dataset is divided into five training batches and one test batch, each with 10000 images. The test batch contains exactly 1000 randomly-selected images from each class. The training batches contain the remaining images in random order, but some training batches may contain more images from one class than another. Between them, the training batches contain exactly 5000 images from each class. },
keywords= {Dataset},
terms= {}
}


@inproceedings{tin,
  title={Tiny ImageNet Visual Recognition Challenge},
  author={Ya Le and Xuan S. Yang},
  year={2015}
}

@inproceedings{imagenet, 
  title={Imagenet: A large-scale hierarchical image database}, 
  author={Deng, Jia and Dong, Wei and Socher, Richard and Li, Li-Jia and Li, Kai and Fei-Fei, Li},



  booktitle={2009 IEEE conference on computer vision and pattern recognition}, 
  pages={248--255}, 
  year={2009}, 
  organization={Ieee} 
}

@InProceedings{filtra,
  title = 	 {FILTRA: Rethinking Steerable CNN by Filter Transform},
  author =       {Li, Bo and Wang, Qili and Lee, Gim Hee},
  booktitle = 	 {Proceedings of the 38th International Conference on Machine Learning},
  pages = 	 {6515--6522},
  year = 	 {2021},
  editor = 	 {Meila, Marina and Zhang, Tong},
  volume = 	 {139},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {18--24 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v139/li21v/li21v.pdf},
  url = 	 {https://proceedings.mlr.press/v139/li21v.html},
  abstract = 	 {Steerable CNN imposes the prior knowledge of transformation invariance or equivariance in the network architecture to enhance the the network robustness on geometry transformation of data and reduce overfitting. It has been an intuitive and widely used technique to construct a steerable filter by augmenting a filter with its transformed copies in the past decades, which is named as filter transform in this paper. Recently, the problem of steerable CNN has been studied from aspect of group representation theory, which reveals the function space structure of a steerable kernel function. However, it is not yet clear on how this theory is related to the filter transform technique. In this paper, we show that kernel constructed by filter transform can also be interpreted in the group representation theory. This interpretation help complete the puzzle of steerable CNN theory and provides a novel and simple approach to implement steerable convolution operators. Experiments are executed on multiple datasets to verify the feasibility of the proposed approach.}
}



@misc{dalle,
  doi = {10.48550/ARXIV.2102.12092},
  
  url = {https://arxiv.org/abs/2102.12092},
  
  author = {Ramesh, Aditya and Pavlov, Mikhail and Goh, Gabriel and Gray, Scott and Voss, Chelsea and Radford, Alec and Chen, Mark and Sutskever, Ilya},
  
  keywords = {Computer Vision and Pattern Recognition (cs.CV), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Zero-Shot Text-to-Image Generation},
  
  publisher = {arXiv},
  
  year = {2021},
  
  copyright = {Creative Commons Attribution 4.0 International}
}


@misc{clip,
  doi = {10.48550/ARXIV.2103.00020},
  
  url = {https://arxiv.org/abs/2103.00020},
  
  author = {Radford, Alec and Kim, Jong Wook and Hallacy, Chris and Ramesh, Aditya and Goh, Gabriel and Agarwal, Sandhini and Sastry, Girish and Askell, Amanda and Mishkin, Pamela and Clark, Jack and Krueger, Gretchen and Sutskever, Ilya},
  
  keywords = {Computer Vision and Pattern Recognition (cs.CV), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Learning Transferable Visual Models From Natural Language Supervision},
  
  publisher = {arXiv},
  
  year = {2021},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}


@inproceedings{
tsai2022conditional,
title={Conditional Contrastive Learning with Kernel},
author={Yao-Hung Hubert Tsai and Tianqin Li and Martin Q. Ma and Han Zhao and Kun Zhang and Louis-Philippe Morency and Ruslan Salakhutdinov},
booktitle={International Conference on Learning Representations},
year={2022},
url={https://openreview.net/forum?id=AAJLBoGt0XM}
}

@inproceedings{
xiao2021what,
title={What Should Not Be Contrastive in Contrastive Learning},
author={Tete Xiao and Xiaolong Wang and Alexei A Efros and Trevor Darrell},
booktitle={International Conference on Learning Representations},
year={2021},
url={https://openreview.net/forum?id=CZ8Y3NzuVzO}
}

@inproceedings{
wang2021residual,
title={Residual Relaxation for Multi-view Representation Learning},
author={Yifei Wang and Zhengyang Geng and Feng Jiang and Chuming Li and Yisen Wang and Jiansheng Yang and Zhouchen Lin},
booktitle={Advances in Neural Information Processing Systems},
editor={A. Beygelzimer and Y. Dauphin and P. Liang and J. Wortman Vaughan},
year={2021},
url={https://openreview.net/forum?id=rEBScZF6G70}
}

@inproceedings{cesa2022a,
        title={A Program to Build {E(N)}-Equivariant Steerable {CNN}s },
        author={Gabriele Cesa and Leon Lang and Maurice Weiler},
        booktitle={International Conference on Learning Representations},
        year={2022},
        url={https://openreview.net/forum?id=WE4qe9xlnQw}
}
    
@inproceedings{e2cnn,
       title={{General E(2)-Equivariant Steerable CNNs}},
       author={Weiler, Maurice and Cesa, Gabriele},
       booktitle={Conference on Neural Information Processing Systems (NeurIPS)},
       year={2019},
}

@inproceedings{
dangovski2022equivariant,
title={Equivariant Self-Supervised Learning: Encouraging Equivariance in Representations},
author={Rumen Dangovski and Li Jing and Charlotte Loh and Seungwook Han and Akash Srivastava and Brian Cheung and Pulkit Agrawal and Marin Soljacic},
booktitle={International Conference on Learning Representations},
year={2022},
url={https://openreview.net/forum?id=gKLAAfiytI}
}

@misc{nptn,
  doi = {10.48550/ARXIV.1801.04520},
  
  url = {https://arxiv.org/abs/1801.04520},
  
  author = {Pal, Dipan K. and Savvides, Marios},
  
  keywords = {Computer Vision and Pattern Recognition (cs.CV), Artificial Intelligence (cs.AI), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Non-Parametric Transformation Networks},
  
  publisher = {arXiv},
  
  year = {2018},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}


@inproceedings{
lconv,
title={Automatic Symmetry Discovery with Lie Algebra Convolutional Network},
author={Nima Dehmamy and Robin Walters and Yanchen Liu and Dashun Wang and Rose Yu},
booktitle={Advances in Neural Information Processing Systems},
editor={A. Beygelzimer and Y. Dauphin and P. Liang and J. Wortman Vaughan},
year={2021},
url={https://openreview.net/forum?id=NPOWF_ZLfC5}
}

@misc{marco_MV,
  doi = {10.48550/ARXIV.2002.07017},
  
  url = {https://arxiv.org/abs/2002.07017},
  
  author = {Federici, Marco and Dutta, Anjan and Forré, Patrick and Kushman, Nate and Akata, Zeynep},
  
  keywords = {Machine Learning (cs.LG), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Learning Robust Representations via Multi-View Information Bottleneck},
  
  publisher = {arXiv},
  
  year = {2020},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@misc{wang2022chaos,
      title={Chaos is a Ladder: A New Theoretical Understanding of Contrastive Learning via Augmentation Overlap}, 
      author={Yifei Wang and Qi Zhang and Yisen Wang and Jiansheng Yang and Zhouchen Lin},
      year={2022},
      eprint={2203.13457},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{ji2021power,
      title={The Power of Contrast for Feature Learning: A Theoretical Analysis}, 
      author={Wenlong Ji and Zhun Deng and Ryumei Nakada and James Zou and Linjun Zhang},
      year={2021},
      eprint={2110.02473},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@inproceedings{ssl_content_style,
 author = {von K\"{u}gelgen, Julius and Sharma, Yash and Gresele, Luigi and Brendel, Wieland and Sch\"{o}lkopf, Bernhard and Besserve, Michel and Locatello, Francesco},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Ranzato and A. Beygelzimer and Y. Dauphin and P.S. Liang and J. Wortman Vaughan},
 pages = {16451--16467},
 publisher = {Curran Associates, Inc.},
 title = {Self-Supervised Learning with Data Augmentations Provably Isolates Content from Style},
 url = {https://proceedings.neurips.cc/paper/2021/file/8929c70f8d710e412d38da624b21c3c8-Paper.pdf},
 volume = {34},
 year = {2021}
}


@inproceedings{approx_CI,
 author = {Lee, Jason D and Lei, Qi and Saunshi, Nikunj and ZHUO, JIACHENG},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Ranzato and A. Beygelzimer and Y. Dauphin and P.S. Liang and J. Wortman Vaughan},
 pages = {309--323},
 publisher = {Curran Associates, Inc.},
 title = {Predicting What You Already Know Helps: Provable Self-Supervised Learning},
 url = {https://proceedings.neurips.cc/paper/2021/file/02e656adee09f8394b402d9958389b7d-Paper.pdf},
 volume = {34},
 year = {2021}
}

@misc{ddn,
  doi = {10.48550/ARXIV.2010.00578},
  
  url = {https://arxiv.org/abs/2010.00578},
  
  author = {Tian, Yuandong and Yu, Lantao and Chen, Xinlei and Ganguli, Surya},
  
  keywords = {Machine Learning (cs.LG), Artificial Intelligence (cs.AI), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Understanding Self-supervised Learning with Dual Deep Networks},
  
  publisher = {arXiv},
  
  year = {2020},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}


@misc{theory_cl,
  doi = {10.48550/ARXIV.1902.09229},
  
  url = {https://arxiv.org/abs/1902.09229},
  
  author = {Arora, Sanjeev and Khandeparkar, Hrishikesh and Khodak, Mikhail and Plevrakis, Orestis and Saunshi, Nikunj},
  
  keywords = {Machine Learning (cs.LG), Artificial Intelligence (cs.AI), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {A Theoretical Analysis of Contrastive Unsupervised Representation Learning},
  
  publisher = {arXiv},
  
  year = {2019},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}



@misc{cl_mv,
  doi = {10.48550/ARXIV.2008.10150},
  
  url = {https://arxiv.org/abs/2008.10150},
  
  author = {Tosh, Christopher and Krishnamurthy, Akshay and Hsu, Daniel},
  
  keywords = {Machine Learning (cs.LG), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Contrastive learning, multi-view redundancy, and linear models},
  
  publisher = {arXiv},
  
  year = {2020},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@misc{cmc,
  doi = {10.48550/ARXIV.1906.05849},
  
  url = {https://arxiv.org/abs/1906.05849},
  
  author = {Tian, Yonglong and Krishnan, Dilip and Isola, Phillip},
  
  keywords = {Computer Vision and Pattern Recognition (cs.CV), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Contrastive Multiview Coding},
  
  publisher = {arXiv},
  
  year = {2019},
  
  copyright = {Creative Commons Attribution Non Commercial Share Alike 4.0 International}
}


@article{simclr,
  title={A Simple Framework for Contrastive Learning of Visual Representations},
  author={Chen, Ting and Kornblith, Simon and Norouzi, Mohammad and Hinton, Geoffrey},
  journal={arXiv preprint arXiv:2002.05709},
  year={2020}
}

@misc{byol,
    title = {Bootstrap Your Own Latent: A New Approach to Self-Supervised Learning},
    author = {Jean-Bastien Grill and Florian Strub and Florent Altché and Corentin Tallec and Pierre H. Richemond and Elena Buchatskaya and Carl Doersch and Bernardo Avila Pires and Zhaohan Daniel Guo and Mohammad Gheshlaghi Azar and Bilal Piot and Koray Kavukcuoglu and Rémi Munos and Michal Valko},
    year = {2020},
    eprint = {2006.07733},
    archivePrefix = {arXiv},
    primaryClass = {cs.LG}
}

@misc{simsiam,
    title={Exploring Simple Siamese Representation Learning}, 
    author={Xinlei Chen and Kaiming He},
    year={2020},
    eprint={2011.10566},
    archivePrefix={arXiv},
    primaryClass={cs.CV}
}


@InProceedings{capsules2011,
author="Hinton, Geoffrey E.
and Krizhevsky, Alex
and Wang, Sida D.",
editor="Honkela, Timo
and Duch, W{\l}odzis{\l}aw
and Girolami, Mark
and Kaski, Samuel",
title="Transforming Auto-Encoders",
booktitle="Artificial Neural Networks and Machine Learning -- ICANN 2011",
year="2011",
publisher="Springer Berlin Heidelberg",
address="Berlin, Heidelberg",
pages="44--51",
}



@inproceedings{gcnn,
  title={Group equivariant convolutional networks},
  author={Cohen, Taco and Welling, Max},
  booktitle={International conference on machine learning},
  pages={2990--2999},
  year={2016}
}


@article{cohen2016steerable,
  title={Steerable CNNs},
  author={Taco Cohen and M. Welling},
  journal={ArXiv},
  year={2017},
  volume={abs/1612.08498}
}

@article{worrall2017harmonic,
  title={Harmonic Networks: Deep Translation and Rotation Equivariance},
  author={Daniel E. Worrall and Stephan J. Garbin and Daniyar Turmukhambetov and Gabriel J. Brostow},
  journal={2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  year={2017},
  pages={7168-7177}
}


@inproceedings{scalespaces,
 author = {Worrall, Daniel and Welling, Max},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Deep Scale-spaces: Equivariance Over Scale},
 url = {https://proceedings.neurips.cc/paper/2019/file/f04cd7399b2b0128970efb6d20b5c551-Paper.pdf},
 volume = {32},
 year = {2019}
}



@InProceedings{finzi2020generalizing,
  title = 	 {Generalizing Convolutional Neural Networks for Equivariance to Lie Groups on Arbitrary Continuous Data},
  author =       {Finzi, Marc and Stanton, Samuel and Izmailov, Pavel and Wilson, Andrew Gordon},
  booktitle = 	 {Proceedings of the 37th International Conference on Machine Learning},
  pages = 	 {3165--3176},
  year = 	 {2020},
  volume = 	 {119},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {13--18 Jul},
  publisher =    {PMLR},
  url = 	 {http://proceedings.mlr.press/v119/finzi20a.html},
}

@InProceedings{ravanbakhsh2017equivariance,
  title = 	 {Equivariance Through Parameter-Sharing},
  author =       {Siamak Ravanbakhsh and Jeff Schneider and Barnab{\'a}s P{\'o}czos},
  booktitle = 	 {Proceedings of the 34th International Conference on Machine Learning},
  pages = 	 {2892--2901},
  year = 	 {2017},
  editor = 	 {Precup, Doina and Teh, Yee Whye},
  volume = 	 {70},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {06--11 Aug},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v70/ravanbakhsh17a/ravanbakhsh17a.pdf},
  url = 	 {https://proceedings.mlr.press/v70/ravanbakhsh17a.html},
  abstract = 	 {We propose to study equivariance in deep neural networks through parameter symmetries. In particular, given a group G that acts discretely on the input and output of a standard neural network layer, we show that its equivariance is linked to the symmetry group of network parameters. We then propose two parameter-sharing scheme to induce the desirable symmetry on the parameters of the neural network. Under some conditions on the action of G, our procedure for tying the parameters achieves G-equivariance and guarantees sensitivity to all other permutation groups outside of G.}
}


@inproceedings{weiler20183d,
author = {Weiler, Maurice and Geiger, Mario and Welling, Max and Boomsma, Wouter and Cohen, Taco},
title = {3D Steerable CNNs: Learning Rotationally Equivariant Features in Volumetric Data},
year = {2018},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We present a convolutional network that is equivariant to rigid body motions. The model uses scalar-, vector-, and tensor fields over 3D Euclidean space to represent data, and equivariant convolutions to map between such representations. These SE(3)-equivariant convolutions utilize kernels which are parameterized as a linear combination of a complete steerable kernel basis, which is derived analytically in this paper. We prove that equivariant convolutions are the most general equivariant linear maps between fields over ℝ3. Our experimental results confirm the effectiveness of 3D Steerable CNNs for the problem of amino acid propensity prediction and protein structure classification, both of which have inherent SE(3) symmetry.},
booktitle = {Proceedings of the 32nd International Conference on Neural Information Processing Systems},
pages = {10402–10413},
numpages = {12},
location = {Montr\'{e}al, Canada},
series = {NIPS'18}
}

@InProceedings{finzi2021emlp,
  title = 	 {A Practical Method for Constructing Equivariant Multilayer Perceptrons for Arbitrary Matrix Groups},
  author =       {Finzi, Marc and Welling, Max and Wilson, Andrew Gordon Gordon},
  booktitle = 	 {Proceedings of the 38th International Conference on Machine Learning},
  pages = 	 {3318--3328},
  year = 	 {2021},
  editor = 	 {Meila, Marina and Zhang, Tong},
  volume = 	 {139},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {18--24 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v139/finzi21a/finzi21a.pdf},
  url = 	 {https://proceedings.mlr.press/v139/finzi21a.html},
  abstract = 	 {Symmetries and equivariance are fundamental to the generalization of neural networks on domains such as images, graphs, and point clouds. Existing work has primarily focused on a small number of groups, such as the translation, rotation, and permutation groups. In this work we provide a completely general algorithm for solving for the equivariant layers of matrix groups. In addition to recovering solutions from other works as special cases, we construct multilayer perceptrons equivariant to multiple groups that have never been tackled before, including $\mathrm{O}(1,3)$, $\mathrm{O}(5)$, $\mathrm{Sp}(n)$, and the Rubik’s cube group. Our approach outperforms non-equivariant baselines, with applications to particle physics and modeling dynamical systems. We release our software library to enable researchers to construct equivariant layers for arbitrary}
}

@inproceedings{gim,
  title={Putting An End to End-to-End: Gradient-Isolated Learning of Representations},
  author={L{\"o}we, Sindy and O'Connor, Peter and Veeling, Bastiaan},
  booktitle={Advances in Neural Information Processing Systems},
  year={2019}
}

@InProceedings{connor2021variational,
  title = 	 { Variational Autoencoder with Learned Latent Structure },
  author =       {Connor, Marissa and Canal, Gregory and Rozell, Christopher},
  booktitle = 	 {Proceedings of The 24th International Conference on Artificial Intelligence and Statistics},
  pages = 	 {2359--2367},
  year = 	 {2021},
  volume = 	 {130},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {13--15 Apr},
  publisher =    {PMLR},
  url = 	 {http://proceedings.mlr.press/v130/connor21a.html}
}

@misc{cpc,
  doi = {10.48550/ARXIV.1807.03748},
  url = {https://arxiv.org/abs/1807.03748},
  author = {Oord,  Aaron van den and Li,  Yazhe and Vinyals,  Oriol},
  keywords = {Machine Learning (cs.LG),  Machine Learning (stat.ML),  FOS: Computer and information sciences,  FOS: Computer and information sciences},
  title = {Representation Learning with Contrastive Predictive Coding},
  publisher = {arXiv},
  year = {2018},
  copyright = {arXiv.org perpetual,  non-exclusive license}
}

@inproceedings{
dim,
title={Learning deep representations by mutual information estimation and maximization},
author={R Devon Hjelm and Alex Fedorov and Samuel Lavoie-Marchildon and Karan Grewal and Phil Bachman and Adam Trischler and Yoshua Bengio},
booktitle={International Conference on Learning Representations},
year={2019},
url={https://openreview.net/forum?id=Bklr3j0cKX},
}


@article{bengio2013representation,
  title={Representation learning: A review and new perspectives},
  author={Bengio, Yoshua and Courville, Aaron and Vincent, Pascal},
  journal={IEEE transactions on pattern analysis and machine intelligence},
  volume={35},
  number={8},
  pages={1798--1828},
  year={2013},
  publisher={IEEE}
}

@misc{ViT,
  doi = {10.48550/ARXIV.2010.11929},
  url = {https://arxiv.org/abs/2010.11929},
  author = {Dosovitskiy,  Alexey and Beyer,  Lucas and Kolesnikov,  Alexander and Weissenborn,  Dirk and Zhai,  Xiaohua and Unterthiner,  Thomas and Dehghani,  Mostafa and Minderer,  Matthias and Heigold,  Georg and Gelly,  Sylvain and Uszkoreit,  Jakob and Houlsby,  Neil},
  keywords = {Computer Vision and Pattern Recognition (cs.CV),  Artificial Intelligence (cs.AI),  Machine Learning (cs.LG),  FOS: Computer and information sciences,  FOS: Computer and information sciences},
  title = {An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale},
  publisher = {arXiv},
  year = {2020},
  copyright = {arXiv.org perpetual,  non-exclusive license}
}

@Article{MAE,
  author  = {Kaiming He and Xinlei Chen and Saining Xie and Yanghao Li and Piotr Doll{\'a}r and Ross Girshick},
  journal = {arXiv:2111.06377},
  title   = {Masked Autoencoders Are Scalable Vision Learners},
  year    = {2021},
}

@inproceedings{cnnequi,
  title={Making convolutional networks shift-invariant again},
  author={Zhang, Richard},
  booktitle={International conference on machine learning},
  pages={7324--7334},
  year={2019},
  organization={PMLR}
}

@inproceedings{
    sesn,
    title={Scale-Equivariant Steerable Networks},
    author={Ivan Sosnovik and Michał Szmaja and Arnold Smeulders},
    booktitle={International Conference on Learning Representations},
    year={2020},
    url={https://openreview.net/forum?id=HJgpugrKPS}
}

@INPROCEEDINGS{resnet,
  author={He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  booktitle={2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)}, 
  title={Deep Residual Learning for Image Recognition}, 
  year={2016},
  volume={},
  number={},
  pages={770-778},
  doi={10.1109/CVPR.2016.90}}
