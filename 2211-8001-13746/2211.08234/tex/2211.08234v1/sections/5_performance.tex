% \textbf{(1) Generality of re-use: a backbone for downstream task learning}

We conduct the \textit{generality test} by adding differences between pre-training and reuse. Fig. \ref{fig:exp_setup} gives an overview of task setup in the below three tests. 
% Details of evaluation protocols can be found in Appendix \ref{sec:gen_test}.


% As shown in Fig. \ref{fig:eval_aim_generality}, we aim to examine \textbf{the generality} of our method by testing how well our proposed agent-environment interaction model as a general backbone, can boost downstream task learning in three heterogeneous task settings, as listed below. 




% \textbf{(1) Task adaptation: unseen task objective}\hspace{0.5cm}
% The downstream task has the same environment as in pre-training. The task objective ``reach the green goal'' is defined by assigning a reward function: $r(x, a, x')=1.0$ if the goal is reached. Recall that this task objective is unseen during pre-training.  Results are shown in Fig. \ref{fig:gen_curve}\textbf{A} that our methods outperforms the 4 baselines. 

% \textbf{(2) General task adaptation: unseen task objective + unseen environment}\hspace{0.5cm}
% We keep testing the generality of our proposed method by changing both the environment and the task objective using the ``pick up the grey box'' task. The environment is changed by adding a door to the bottleneck hallway and the grey box that the agent has never seen before. The task objective is now changed to ``pick up the grey box'' that the agent needs to first pick up the key, unlock the door and then pick up the grey box. The reward function is defined as $r(x, a, x')=$ 0.1 (if unlock the door), 1.0 (if succeed). Results are shown in Fig. \ref{fig:gen_curve}\textbf{B}. Note that this task is considered challenging for vanilla RL agents like a DQN agent learning from scratch.

\textbf{Unseen task objective + unseen environment + unseen sensor modalities}\hspace{0.5cm}
We test the generality of our proposed method by adding an unseen task objective, environment settings, and sensor modality in downstream task learning. The task is ``putting the green key near the yellow ball'' wherein the agent needs to understand ``green key'' (unseen in pre-training), ``yellow ball'' and ``near''. The task requires the agent to find the green key, carry it and drop the key in a cell nearby the yellow ball. The reward function is defined as $r(x, a, x')=0.1$ (if pick up the green key), 10.0 (if succeed).

\begin{figure*}[t]
 	\setlength{\belowcaptionskip}{-10pt}
	\centering
	\includegraphics[width=0.8\textwidth]{figs/fig2_workshop.pdf}
	\caption{\footnotesize \textbf{A:} Environments for collecting the pre-training dataset with different room layouts and random objects (blue keys, blue doors, coloured balls) for the agent to interact with. \textbf{B:} Downstream task ``putting the green key near the yellow ball'' uses an unseen environment (green key, coloured boxes unseen during pre-training), unseen task objective, and unseen sensor modality by adding textual observations. Note this task requires text prompts otherwise it is difficult to solve by a vanilla RL agent\citep{minigrid} \normalsize}
     \vspace{1.0mm}
	\label{fig:exp_setup}
\end{figure*} 

\textbf{Adding textual sensor modality:} Note that this task requires text prompts otherwise is difficult to solve. We manually design three types of textual observations for the agent--- The mission statement ``Put the green key near the yellow ball'' + different textual statuses: \{``and I have nothing'', ``and I have the [color] [object]''\} depends on what the agent is carrying. The textual observation it then converted to an embedding vector $o$ using a pre-trained sentence transformer MiniLM (384 dimensional output, 80 MB model size) \citep{Wang2020MiniLMDS} using a libary provided by \citep{reimers-2019-sentence-bert}. Therefore, the agent's observation at time t is augmented as : $[x_{t}, o_{t}]$, where $x_{t} \in \mathbb{R}^{7\times 7\times 3}$ is the same observation as in pre-training, and $o_{t} \in \mathbb{R}^{384}$ is the extra sensor modality added to the agent to provide textual observations.  
% Note that baselines VAE+DQN and GPE/GPI can not directly handle an extra sensor modality. When taking the $o_{t}$ as inputs to the above baselines, we make the below changes. For VAE+DQN, we concatenate the VAE encoded observation with $o_{t}$. For successor feature based GPE/GPI, we also concatenate the successor features with $o_{t}$ to regress the linear task weights. 
% Put the green key near the yellow ball and I do not have the green key --> v1
% Put the green key near the yellow ball and and I have the green key --> v2
% Put the green key near the yellow ball and and I am near the target --> v3

\textbf{Baselines:} We compare our method (\textit{DQN-embodied}) to the following baselines. (1) Without pre-training: vanilla DQN~\citep{Mnih2013PlayingAW}; (2) Reuse the pre-training dataset: experience replay (ER) which reuses the whole dataset $\mathcal{D}$ in the buffer to alleviate the forgetting issue~\citep{Rolnick2019ExperienceRF}; (3) Reuse a pre-trained backbone: DQN-VAE~\citep{kingma2013auto} is used as a backbone for downstream task learning; (4) Without textual observations (DQN-no-text) using a vanilla DQN~\citep{Mnih2013PlayingAW}. Note that baselines DQN-VAE can not directly handle an extra sensor modality. When taking the $o_{t}$ as inputs to the above baselines, we concatenate the VAE encoded observation with $o_{t}$. 
% (4) Reuse the pre-trained successor features (SF): GPE/GPI~\citep{barreto2020fast} method is used to learn the downstream task where during GPE, we use linear regression to estimate the new task weights based on our pre-trained cross-domain transferable SF $\phi^{\pi_{0}}(.;\theta_{sf})$.
\begin{wrapfigure}{r}{0.5\textwidth}
    \centering
		\subfigure{\includegraphics[width=0.5\textwidth]{figs/err.png}}
% 		\includegraphics[width=0.9\textwidth]{figs/updated/main_legend_update.pdf}
		% \includegraphics[width=0.8\textwidth]{figs/test_legend.pdf}
		% \vspace{-4.5mm}
    \caption{\footnotesize Preliminary results of reusing generality test. X-axis represents the sample size. Y-axis represents the success rate of 100 independent runs.\normalsize} %To ensure the evaluation environments are the consistent across different methods and multiple runs, we set a fixed random seed in each evaluation environment.}
    \label{fig:gen_curve}
   \vspace{-4.5mm}
\end{wrapfigure}

We test the downstream task learning performance of all methods by evaluating the success rate every 10K training steps using 100 independent runs with different random seeds. Results (Fig. \ref{fig:gen_curve}) show that (1) Without textual observations, a vanilla DQN agent can not solve the task; (2) Since the new task setting is heterogeneous compared to tasks in the training pools, directly reusing previous experience (the ER baseline) will deteriorate the downstream task learning performance (vanilla DQN is better than ER); (3) Also, since the new task is never seen during pre-training, DQN based on pre-trained VAE performs the worst. (4) For the test task that has unseen task objectives (put a color object near another color object), unseen environmental settings (box object never seen in pre-training), and unseen sensor modalities (adding textual observations), projecting the downstream task samples on the pre-trained embodied set will result in better sample efficiency than all baselines. 



% \subsection{Summary of results}

