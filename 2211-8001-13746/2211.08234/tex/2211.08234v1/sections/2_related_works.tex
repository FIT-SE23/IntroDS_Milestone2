Pre-training a backbone to accelerate reinforcement learning is not new. We classify existing works into four categories depending on what is reused in downstream task learning.

(1) \textit{Reusable representations}: Most existing works fall into this category that aims to learn a representation either to improve sample efficiency or task generalization. Methods include task-agnostic ones, for example, using VAE~\citep{kingma2013auto} and a pre-trained ResNet \citep{shah2021rrl} for state representations), self-supervised representation learning methods (RAD \citep{laskin2020reinforcement}, CURL \citep{srinivas2020curl} , CPC ~\citep{oord2018representation}, and ATC \citep{stooke2021decoupling}) for image states, and task-specific ones\citep{Jin2022OfflineLO}, for example, using a behaviour similarity measure (Bi-simulation metrics~\citep{zhang2020learning}) to constrain the representation which results in better generalization performance.

(2) \textit{Reusable policy/value functions}:
This approach aims to pre-train a policy or value function that can be further fine-tuned in downstream task learning. Methods include assuming task distribution priors (meta-RL \citep{duan2016rl}), assuming environmental distribution priors (generalizable RL \citep{packer2018assessing} , robust RL \citep{rajeswaran2016epopt}), and contextual MDP based ones \citep{huang2020one}. Recently, representing a policy using generative sequential models has gained increasing popularity (Gato~\citep{reed2022generalist}, Decision Transformers~\citep{chen2021decision}). However, their capability of downstream task learning is commonly restricted to the training setup, and handling large changes when reused remains challenging~\citep{reed2022generalist}. 

(3) \textit{Reusable skills and options}:
This approach assumes tasks can be decomposed into shareable skills \citep{sutton2011horde} that can be learned via time abstraction---learning options as skills. The difficulty lies in how to discover reusable skills, and it relates to recently proposed works in the \textit{unsupervised RL} literature \citep{laskin2022cic}. Current methods for skill discovery include curiosity and diversity-based ones (intrinsic controls~\citep{gregor2016variational}, DIYAN~\citep{eysenbach2018diversity}, behaviour basis-based ones (Proto-value functions~\citep{mahadevan2005proto}, and variational intrinsic successor features~\citep{hansen2019fast}.

% (4) \textit{Reusable models}:
% This approach seeks to learn reusable representations or models from the agent's interaction with the environment. For example, from the task perspective, in successor feature-based methods \citep{barreto2017successor} , we assume all tasks' reward functions share the same linear feature space that a new task's value function can be fast computed with pre-trained successor features by regression of the linear weights or by GPE/GPI \citep{barreto2020fast}  to learn a new policy. From the environment perspective, there are methods aiming to approximate the environment's structure using the traversity \citep{machado2017eigenoption}  estimated from the agent-environment interactions. For example, approximating the Laplacian representation \citep{wu2018laplacian}  of an environment shows benefits when reused as a structural basis that will guide the agent to learn better exploration strategies.  

% \subsection{Reuse: the stability-plasticity dilemma}
% \paragraph{Rehearsal based methods}

% \paragraph{Policy distillation methods}

% \paragraph{Latent parameter storage}

% \paragraph{Agent-state construction}


% \subsection{Embedded agency: small agent, big world}
% There are two perspectives tackling this problem.

% What is embedded agency.

% \paragraph{Agent-state construction}
% asfas
