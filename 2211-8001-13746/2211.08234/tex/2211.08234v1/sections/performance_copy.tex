% \textbf{
% (1) Generally reusbale; (2) when reuse, learning downstream tasks based on the pre-trained model, it's important to have stability and plasticity. it's stable to non-stationarity during learning and after deployment due to feature projection that maintains the previous structure.  and adaptive to changes due to the Projected Bellman equation that approximates a new value function. 
% }

% \begin{figure}[bth]
%     \centering
%     	\begin{tabular}{@{}cc}
% 		{\includegraphics[height=1.3cm]{figs/fig3.pdf}}
% 		&{\includegraphics[height=1.3cm]{figs/fig6.pdf}}
% 	\end{tabular}
% 		\vspace{-5pt}
%     \caption{\footnotesize 3 curves as in one figure, curve remove title!. Row 1: compare ours to DQN, ER, 3RL. Row2: Compare ours to GPE/GPI, 1,2, VAE.\normalsize} %To ensure the evaluation environments are the consistent across different methods and multiple runs, we set a fixed random seed in each evaluation environment.}
%     \label{fig:learningcurve}
%     \vspace{-5pt}
% \end{figure}
 
\begin{figure*}[t]
 	\setlength{\belowcaptionskip}{-10pt}
	\centering
	\includegraphics[width=0.95\textwidth]{figs/fig2.pdf}
	\caption{\footnotesize \textbf{A:} Environments for collecting the pre-training dataset with different room layouts and random objects (keys, doors, coloured balls) for the agent to interact with. \textbf{B:} Downstream task ``goal-reaching'' uses the same environment as to pre-training but with a new task objective ``reach the green goal'' defined by the new reward function. \textbf{C:} Downstream task ``picking up the grey box'' uses a changed environment (locked door, grey box, coloured triangles are unseen during pre-training), and obviously a changed task objective. This task is challenging for vanilla RL agents when learning from scratch. \textbf{D:} Downstream task ``putting the green key near the yellow ball'' uses a changed environment (coloured triangles unseen during pre-training), changed task objective, and changed sensor modality by adding textual observations. Note this task requires text prompts otherwise it is difficult to solve by a vanilla RL agent\citep{minigrid} \normalsize}
 %     \vspace{-3.5mm}
	\label{fig:exp_setup}
\end{figure*} 

\subsection{Generality of re-use: a backbone for downstream task learning}
We conduct the \textit{generality test} by gradually adding differences between the downstream tasks and the pre-trained tasks. Fig. \ref{fig:exp_setup} gives an overview of task setup in the below three tests. 

\begin{figure}[]
    \centering
    	\begin{tabular}{@{}ccc}
     \footnotesize \textbf{A} change task objectives & \textbf{B} + change environments & \textbf{C} + change sensor modalities\\
		{\includegraphics[height=3.cm]{figs/test.pdf}}
		&{\includegraphics[height=3.cm]{figs/test.pdf}}
		&{\includegraphics[height=3.cm]{figs/test.pdf}}
	\end{tabular}
% 		\includegraphics[width=0.9\textwidth]{figs/updated/main_legend_update.pdf}
		\includegraphics[width=0.8\textwidth]{figs/test_legend.pdf}
		\vspace{-5pt}
    \caption{\footnotesize 3 curves as in one figure, curve remove title!. Row 1: compare ours to DQN, ER, 3RL. Row2: Compare ours to GPE/GPI, 1,2, VAE.\normalsize} %To ensure the evaluation environments are the consistent across different methods and multiple runs, we set a fixed random seed in each evaluation environment.}
    \label{fig:learningcurve}
    \vspace{-5pt}
\end{figure}

\textbf{Baselines:} We compare our methods to the following baselines. (1) Without pre-training: vanlilla DQN~\citep{Mnih2013PlayingAW}; (2) Reuse the pre-training dataset: experience replay (ER) which reuses the whole dataset $\mathcal{D}$ in the buffer to alleviate the forgetting issue~\citep{Rolnick2019ExperienceRF}; (3) Reuse a pre-training backbone: VAE~\citep{kingma2013auto}+DQN is used as a backbone for downstream task learning; (4) Reuse the pre-trained successor features (SF): GPE/GPI~\citep{barreto2020fast} method is used to learn the downstream task where during GPE, we use linear regression to estimate the new task weights based on our pre-trained cross-domain transferable SF $\phi^{\pi_{0}}(.;\theta_{sf})$.

We test the downstream task learning performance of all methods by evaluating the success rate every 10K steps using 100 independent runs with different random seeds.
% As shown in Fig. \ref{fig:eval_aim_generality}, we aim to examine \textbf{the generality} of our method by testing how well our proposed agent-environment interaction model as a general backbone, can boost downstream task learning in three heterogeneous task settings, as listed below. 

% \begin{wrapfigure}{r}{0.5\textwidth}
%     \centering
%     \includegraphics[width=0.5\textwidth]{figs/fig3.pdf}
%     \caption{\footnotesize Evaluation purpose of Sec. 5.1. \normalsize}
%     \label{fig:eval_aim_generality}
%     \vspace{-3.5mm}
% \end{wrapfigure}


\paragraph{(1) Task adaptation: changed task objective}
As shown in Fig. \ref{fig:exp_setup}B, the downstream task has the same environment as in pre-training. The task objective ``reach the green goal'' is defined by assigning a reward function: $r(x, a, x')=10$ if the goal is reached. Recall that this task objective is unseen during pre-training.  Results are shown in Fig. \ref{fig:learningcurve}\textbf{A} that our methods outperforms the 4 baselines. 
% Basic setting: changed task objectives.
% (1) No pre-training, just use the previous experience as they are:
%     - DQN: Learning from scratch
%     - ER: put offline dataset in the replay buffer, and use DQN
%     - 3RL: ER+RNN, put in replay buffer, + RNN structure.
%     - Plot their online learning performance, for every 10K samples used during training.
% (2) Has pretraining
%     - GPE (Regression-non-linear)/GPI
%         - Pretraining: SF
%         - Reuse: (1) train w regressor (use our learned SF and the repgress W); (2) GPI
%         - Plot: GPI resuls (use return) (minigrid reward must have penalty at each step), for every 10K samples used in the w regression.
%         - How to collect data: Uniform policy.
%         - To avoid any linearity assumptions, we use a 3-layer MLP with Relu activations to regress the W.
%         - Can generalize, since considering a reward function space spanned by SF feature, 
%     - GPE (TD-non-linear) / GPI
%         - GPE: do not use our pretrained SF, pre-train 20 policies on 20 resulting MDPs, calc value function for the 20 policies.
%         - Reuse: (1) learn value function by TD learning; (2) GPI
%         - Plot: GPI resuls (use return) (minigrid reward must have penalty at each step), for every 10K samples used in the w regression.
%         - Can not generalize well, since do not consider a reward function space spanned by SF features
%     - VAE + DQN
%         - Plot DQN performance, for every 10K samples used.
% Baselines: No need to use two figs to differentiate them.
% - learn from scratch
% - use the pre-collected dataset: DQN, ER, 3RL
% - use pre-training: VAE+DQN, GPE (Regression-non-linear)/GPI, GPE (TD-non-linear)/GPI

% Results to show:
% Return - sample efficiency curves: of our method, and 6 baselines.

\paragraph{(2) General task adaptation: changed task objective + changed environment}
We keep testing the generality of our proposed method by changing both the environment and the task objective. The environment is changed by adding a door to the bottleneck hallway and different colored triangles where the agent has never seen before. The task objective is now changed to ``pick up the grey box'' that the agent needs to firstly pick up the key, unlock the door and then pick up the grey box. The reward function is defined as $r(x, a, x')=$ 1 (if pick up the key or unlock the door), 10 (if succeed).



    - Unlock pickup task, need to unlock the door (by picking up the key), and pick up a box.
    - It only has two rooms!
    - This task can be solved without relying on language.

Results to show:
Success rate - sample efficiency curves: of our method, and 6 baselines.

\paragraph{(3) More general task adaptation: changed task objective + changed environment + changed sensor modalities}
Baselines: 
- No pretraining: DQN, ER, 3RL, 
- With pretraining: GPE-linear/GPI, GPE-non-linear/GPI, VAE+DQN
    - GPE, GPI will perform very bad, since env dynamic changed.

How: find a task is difficult to solve without text for vanlila RL methods.
MiniGrid-LockedRoom-v0
- The agent needs to understand the text, which tells it which room to get the key and go to which room to open the door. And then pick up a box.
- It has six rooms!
- This task is difficult since the agent needs to understand the textual mission info.

Results to show:
Success rate - sample efficiency curves: of our method, and 6 baselines.




\paragraph{(4) Deployment Robustness: robustness to non-stationarity during testing}
We wonder, how the trained policy based on our proposed model, is robust to non-stationarity during testing. To test, we manually change the environment slightly to add such non-stationarity.
\begin{wrapfigure}{r}{0.5\textwidth}
    \centering
    	\begin{tabular}{@{}ccc}
		{\includegraphics[height=4cm]{figs/test2.png}}
	\end{tabular}
		\includegraphics[width=0.5\textwidth]{figs/test_legend.pdf}
		\vspace{-5pt}
    \caption{\footnotesize deployment stability to env changes. 100 Runs, each run random seed, then Slightly change the key to the left 1 cell to the current key position config. Test 6 baselines, and ours\normalsize} 
    \label{fig:213}
    \vspace{-5pt}
\end{wrapfigure}
No need any images to show the change.
Now, at B1, we train the unlock Pick Up one. We can directly use the ones trained in 5.1 (2).

We test the performance if we slightly change the key location, 10 random changes.
Baselines:
- No pretraining: DQN, ER, 3RL, 
- With pretraining: GPE-linear/GPI, GPE-non-linear/GPI, VAE+DQN
- GPE, GPI will perform very bad, since env dynamic changed.

Ours are more robust to non-stationarity after deployment of the policy.

Results to show:
Success rate during testing - sample efficiency curves: of our method, and 6 baselines.
Use a Table to show the result, NOT %直方图 以节省page 空间
sucess rate: 79+- 5 \% 
% \begin{wraptable}{r}{5.5cm}
% \caption{A wrapped table going nicely inside the text.}\label{wrap-tab:1}
% \begin{tabular}{ccc}\\\toprule  
% Method & Header-1 & Header-1 \\\midrule
% Success rate &3 & 5\\  \midrule
% 2 &3 & 5\\  \midrule
% 2 &3 & 5\\  \bottomrule
% \end{tabular}
% \end{wraptable} 

\subsection{Learning stability and plasticity: a backbone that handles non-stationarity}
As shown in Fig. \ref{fig:eval_stability_plasticity}, we aim to examine the learning \textit{stability-plasticity} analysis by a deep look at how well our proposed agent-environment interaction model, as a backbone for downstream task learning, can handle non-stationarity during training, testing, and in a continual learning setting. Specifically, we evaluation from the following perspectives.

\begin{wrapfigure}{r}{0.5\textwidth}
    \centering
    \includegraphics[width=0.5\textwidth]{figs/fig6.pdf}
    \caption{\footnotesize Evaluation purpose of Sec. 5.2. \normalsize}
    \label{fig:eval_stability_plasticity}
    \vspace{-3.5mm}
\end{wrapfigure}

Learned from past experience, will it make the resulting policy be more stable than learning from raw observations, or using other pre-traning methods, like VAEs?
How to do?
After the downstream task learning, choose the one learned from VAE, and the one learned using our method, and the one learned from scratch.
Now, let's change a part of the environment, see the performance. VAE, scratch, its success rate drops quickly, but ours stay robust.


\paragraph{(1) Learning stability: changing environment during downstream task training}
Now, let's continue our test. During reuse, when we train the downstream task, we will change the environment.
Baselines:
% - DQN: without env change. %这个不需要
- DQN: vanilla baseline.
- VAE + DQN: use the pre-trained VAE structure.
% - VAE + DQN without env change %这个不需要
\begin{wrapfigure}{r}{0.5\textwidth}
    \centering
    \includegraphics[width=0.5\textwidth]{figs/fig5.pdf}
    \caption{\footnotesize Fig. Left env at start --> train and get an optimal policy., Middle env changed at 30K completion, Right env changed at 60K. Say clearly, this is changed during training.\normalsize}
    %\label{fig:decentcem-a-architecture}
    %\vspace{-3.5mm}
\end{wrapfigure}

First, run DQN to see how many samples are required. During training, at 30\% of DQN samples (interaction steps), change the env once. at 70\% of DQN samples, change the env the second time. There are two changes during training.


Unlock Pick up, we change the key position at B2, B3, so it will change the environment.

Results to show:
Success rate - sample efficiency curves: of our method, and 6 baselines.


\paragraph{(2) Vanilla plasticity: continual learning in changing environments}
Let's continully update the policy.
Vanilla plasticity, we do not add anything special, just keep training the learned policy, by projecting to our embodied agent-state set. We show that, this vanlila version of DQN shows plasticity, that it quickly adapts to the new environment setting.

While the baseline, we compare, is learning from the raw state. it drops quickly.

Another baseline is EWC, it performs pretty well.

The last baseline is ours + EWC, it performs even better.

In the unlock and pick up task, just reuse the pretrained one in 5.1 (2).

Now we change the env (change the key position). We keep updating the policy trained in 5.1 (2). And we change the key position again, keep updating based on previous one.
Two changes.

Policy 1: the one trained from 5.1 (2)
Policy 2: change the env, and train based on policy 1.
Policy 3: change the env, and train based on policy 2.

Stability: there is no sudden drop of performance.
Plasticity: it can reach to high performance than the other ones. to achive good performance in the changed envs.

Baselines:
- DQN
- Ours (DQN-embodied)
- DQN + EWC
- Ours (DQN-embodied) + EWC

Ours, even without EWC, can show simliar performance to DQN + EWC.
Ours + EWC shows the best performance, that recovers quickly!!!!

Results to show:
Success rate during training - sample efficiency curves: of our method, and 6 baselines.
Needs to see a drop of performance. So need to show the curve.



\begin{figure}[bth]
    \centering
 \begin{tabularx}{1\textwidth}{>{\centering\arraybackslash}m{2.2cm} >{\centering\arraybackslash}m{4cm} >{\centering\arraybackslash}m{5.8cm}}
% \hline
\footnotesize
~ & A Learning stability & B Vanilla plasticity\\
% \hline
\footnotesize Compared to vanilla learning methods & {\includegraphics[height=3cm]{figs/test.pdf}} &  {\includegraphics[height=3cm]{figs/test.pdf}} \\
% \hline
\footnotesize Compared to EWC learning methods & {\includegraphics[height=3cm]{figs/test.pdf}} & {\includegraphics[height=3cm]{figs/test.pdf}} \\
% \hline
\end{tabularx} \normalfont
% 		\includegraphics[width=0.9\textwidth]{figs/updated/main_legend_update.pdf}
		\includegraphics[width=0.8\textwidth]{figs/test_legend.pdf}
		\vspace{-5pt}
    \caption{\footnotesize 2 curves as in one figure, curve remove title!. \normalsize} %To ensure the evaluation environments are the consistent across different methods and multiple runs, we set a fixed random seed in each evaluation environment.}
    \label{fig:learningcurve}
    \vspace{-5pt}
\end{figure}

% \paragraph{(3) Why it supports learning stability and plasticity}
% Why is that?

% Though we change the enironmnet, but we built based on the long term predictions representations, so that the projected representation does not change too much.

% For example, this is the environment, 
% VAE, raw state features, ...
% VAE, will have a big drift since it overfits to previous dataset distributions. because of the change, it forgets about previously learned bottleneck, passways.
% Raw state --> embedding, just changes dramatically.

% By projecting on this abstraction structure, it gains plasticity, also maintains stability.

% Since it is built based on the agent's own experience.

% Ours, maintain stable

% % Color map of the four rooms, ours, continue have high value on pass ways, and ignore the changes. This shows 
% % We don't update the backbone, can it show plasticity?
% % in the future, we will update the backbone.
% plot the value function's output based on ours model, VAE model, and just raw observations.
% Plot (1): changed the environment, the initial value function's output of ours (DQN-embodied). the initial value function is good for the previous environment.
%         - DQN, VAE based ones, the value function's output makes wrong predictions on unchanged cells
%         - Ours, makes correct predictions on unchanged cells.
%      (2): after a few steps 10K training, newly trained value function
%         - DQN, VAE beased ones, the value function's output still are confused, not adapt to the changed cell, i.e., the new key location.
%         - Ours, makes almost increased, and correct predictions on the changed cell.
%      (3): plot the newly trained value function's output - initial value function's output
%         - DQN, VAE based ones, unchanged cells still show changes like noise, no any patterns. Changed cells still show this.
%         - Ours, unchanged ones, almost zero value prediction changes. Changed ones, show positive, and negative changes.
% (3) show clearer how downstream task learning agent (DQN) based on our model, can have vanilla stability-plasticity.

% How many experiments/trainings do I need? Make a table and chart.

 
%  Q value computed by: max Q (of all the 6 acctions), norm(|Q(s,a1), Q(s, a2), Q(s, a3), ...|)-infinity.
%  \begin{figure*}[tpb]
%     \centering
%     \includegraphics[width=0.99\textwidth]{figs/fig7.pdf}
%     \caption{\footnotesize as shown above, another evidence that it supports stability and plasticity. It supports stability by feature projection, that retrain previous learned structure (As shown in Fig. 8, that it maintains the agent-state structure from Train A to Train B, by always doing feature projection on the embodied state.), as a reuslt, even env changes from A to B, at the begining of train B, it maintains good value predictions, and this is maintained until the end of train B. It supports plasticity by Projected Bellman Updates that it adapts to the new changes (as shown in Fig 7, that it adapts to the changes of the key positions. It's initially wrong at the changed key positions, and wrong at the preivous key positions. But at the end of train B, it adapts to the new good values. \normalsize}
%     %\label{fig:decentcem-a-architecture}
%     %\vspace{-3.5mm}
% \end{figure*}


\subsection{Summary of results}

