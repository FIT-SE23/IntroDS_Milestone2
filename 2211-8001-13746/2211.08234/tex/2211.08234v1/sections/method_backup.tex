% \subsection{Background: reusable agent-environment interaction models}
\subsection{Background: on reusable agent-environment interaction models}
%(1) 这样的model 究竟存不存在? short answers, 从传统RL的perspective，是存在，但需要大量的数据构建超大的模型，例如Bayesian world model的perspective。但是，practically，不存在。原因是MDP是这样的，要构建这个world model，不论是学representation that fits S，或是model that fits T, S, A, r, 或是policy，value， that fits, S, A, T, R, 其space的combinatorial space是大得吓人的。在这个大的world model上再做bayesian 推理，基本不可能。
% 有没有其他角度思考这个问题？这就涉及到我们的embedded agency讨论
% this paper considers a POMDP setting. 
% embedded agency怎么做到generally reusable model？ 重要的不是大，而是灵活多变。
We assume a Partial Markov Decision Process (POMDP) formation in our discussions since this setting fits in most real-world reinforcement learning tasks where the full state is not obtainable and the agent is only given observations from its onboard sensors. A POMDP is defined as a tuple $\mathcal{M}=<\mathcal{S}, \mathcal{A}, \mathcal{O}, \mathcal{T}, r, \gamma>$, where $\mathcal{S}$ is the full state space which is typically not given, $\mathcal{A}$ is the action space, $\mathcal{O}$ denotes the observation space, $\mathcal{T}$ is the transition dynamics, $r$ is the reward function that defines a task objective, $\gamma$ denotes the discount factor.


% \begin{wrapfigure}{r}{0.2\textwidth}
%     \centering
%     \includegraphics[width=0.2\textwidth]{figs/609px-Plato's_allegory_of_the_cave.jpg}\\
%     {\footnotesize Plato's allegory of the cave~\cite{Plato}. \normalsize}
%     %\label{fig:decentcem-a-architecture}
%     \vspace{-3mm}
% \end{wrapfigure}

\paragraph{Embedded agency: big world, small agent}
In 1969, Herbert Simon brought us the ``Simon's ant'' parable in chapter three of his book ``{The Science of the Artificial}''~\cite{simon1969science} that explains the complex behaviour we observed from an agent may not attribute to how complex the agent itself is, instead it results from the interaction between the agent and the complex world, while the agent like an ant, can be quite simple. This reminds us of a common setting: big world, small agent.

Unlike the \textit{Bayesian world model} approach that aims to construct a big model for the agent as a base knowledge about the big world, the \textit{Embedded agency} approach takes the perspective on how the small agent perceives, controls and gradually grows based on its own experience. Unlike a \textit{Bayesian world model} where states and models are objective, an \textit{embedded agency} approach is subjective and the agent's own bias induced from its prior experience will affect its decision-making process in both levels of state abstraction and time abstraction. This idea of intelligence closely relates to Descarte's representationalism and mind-body dualism, and to the theory of embodied embedded cognition in the philosophy of mind research. The \textit{embedded agency} approach is appealing in that it promises a scalable learning architecture to build a world model, however, it is also ambitious toward artificial general intelligence (AGI). 

This paper investigates the core idea of \textit{embedded agency} that how an agent's past experience can be used as an inductive bias to accelerate downstream task learning. We focus on a more practical starting point: in a pre-training, how to extract a model from the agent's experience that can be generally reused, as an inductive bias, to accelerate downstream task learning.  



% 从小到大进行构建。agent在环境中的behavior，受agent之前的经验所影响。（1）agent 的sensori-motor的decision建立在之前的经验上的。（2）同时，agent的handle 不同task与environment的能力是基于之前的经验可以grow的。
% embedded agency是冲着AGI去的，我们不追求这个。
% 我们关心的是一个更加practically的setting。概念上讲，embedded agency这套思路能够做到一个agent在不同环境，任务中进行学习，利用已有的experience来grow its knowledge。什么是其最simple的一个学习architecture？当我们不考虑intrinsic reward, sub-tasks, shareable skills / options, 即如何从past experience中构建知识，以及如何利用知识在下游任务中？
% embedded agency 的一个核心的思想就是agent能够construct knowledge，从experience中学习。这意味着。。。。。
% 能否将同一个agent，从不同环境，不同任务中分离出来？ 答案是不能。能分离的不是agent，而是agent与环境interaction的基础模型。
%最难以理解的就是这个agent-environment boundary。

% 一个目标，说清楚什么是从experience中学习。
% 相关， (1) agent 过去的experience 塑造了agent后续的行为决策。 

% \paragraph{Embodied agent-state construction}
% How an agent's past experience will affect its learning of a task can be generally expressed as an agent-state construction approach, where we aim to learn a function $u_{\theta}$, so that $s_{t}=u_{\theta}(x_{t}|E_{past}), \text{subject to } \mathcal{L}_{u_{\theta}}$. Here, the agent-state $s_{t}$ is a function of both current observation $x_{t}$ and the agent's past experience $E_{past}$, and $\mathcal{L}_{u_{\theta}}$ is the loss function used to optimized $u_{\theta}$ and is often hard to specify. Afterwards, $u_{\theta}$ is used in the state value function $Q^{\pi}(u_{\theta}(x_{t}), a_{t})$ task learning.

% For example, in single-task learning under the POMDP setting, we commonly use a history of previous n observation-action pairs to tackle partial observability by assuming the approximated state satisfies a Markovian property. This approach can be expressed as agent-state construction that, $s_{t}=u_{\theta}(x_{t}|\{(x_{t-k}, a_{t-k})\}_{k=1}^{n})$. Here, $E_{past}=\{(x_{t-k}, a_{t-k})\}_{k=1}^{n}$, $\mathcal{L}_{u_{\theta}}$ is the loss term measuring Markovian state property, $u_{\theta}$ is parameterized using a RNN or LSTM neural network to encode the historical data.

% In our setting, we aim to use the agent's past experience in various tasks/environments to accelerate general downstream task learning, where $s_{t}=u_{\theta}(x_{t}|\{(x_{t-k}, a_{t-k})\}_{k=1}^{n})$



% 传统agent state construction，对每一个specific task，学习u, 来 construct一个agent state。
% 我们，对所有env， task，学习，从过去的所有experience中学习如何construct agent state。
% 不再construct agent state，而是寻求construct embodied agent state，并用来支撑学习任意task，Q
% 给出我们的approach.

\paragraph{On agent-environment boundary}
% 接下来，我们解释如何学习 embodied agent-state，以及如何用它来支撑学习任意task Q.
% key insight 是，重要的不是构建一个大而通用的model，而是构建一个灵活可变的model。
% complex world, simple agent，直接提simon's ant..
% how to extract the agent out. 就是不寻求抽取agent，而是寻求抽取agent environment interaction model
% specifically, 就是抽取什么?
% 1）我们并不指望这个能够在未来agent可能遇到的所有环境，所有task都能通用。
% 2) 我们指望这个model可以继续被update，但不在这篇paper里涵盖
% 3) 这篇paper只是迈出了第一步，如何抽取一个尽量泛化的模型，以及如何应用与当初抽取用的MDP完全不同的MDP里学习。
% 4) 将来，是可以考虑更新这个embodied set模型的。
% 提出如何提取agent-environment interaction model，so that (1) M is shared across the domains. (2) M best exhibits the agent-environment interaction behaviour.
% 提出什么是generally reusable agent-environment interaction model?
% 考虑任意task，则定义一个reward-free POMDP M....，每一个generate dataset。
% 我们的目标是学习一个model = <H, T>，从数据中学习，H, T are transferrable across domains.
% 单独学习H, T很困难，若 H, T 都是transferrable，则由H, T induced的，uniform policy的successor feature也是transferrable。能否通过学习一个relaxed 版本successor feature 来得到这个generally reusable的agent-env interaction model?
% 这带来一个新的问题，如何利用sf，到downstream task学习？
% 我们知道sf本身就是有局限性的啊。这里我们用一个discretize到prototype的方法，采用feature projection, projected Bellman update 来解决这个问题。


% \paragraph{Embodied agent-state construction}
% 重要的不是构建一个大而通用的model，而是构建一个灵活可变的model。
% 对每一个specific task，学习u, 来 construct一个agent state。
% 我们，对所有env， task，学习
% The first barrier are the entangled concepts,  do not precisely align with each other

% due to the largely entangled concepts, for example,  ``embodiment'', ``embeddedness'', ``enacted'', and ``affordance'' which originated from multiple disciplines, including the philosophy of minds in cognitive science, ecological psychology in behavior science, and machine learning. To alleviate the entangled concepts barrier, this paper follows the quest proposed by~\cite{kirkpatrick2017overcoming} that we reduce the above concepts to align with a basic reinforcement leaning setting~\cite{kirkpatrick2017overcoming} 

% % except for a pool of entangled terms (for example, 
% % originated from multiple disciplines, including the philosophy of minds in cognitive science, ecological psychology in behavior science, and machine learning. 

% % that , ``embodiment'', ``embeddedness'', ``enacted'', and ``affordance'', which escalate the difficulty with extra barriers, since this research question exists in multiple disciplines, including the philosophy of minds in cognitive science, ecological psychology in behavior science, and machine learning. 

% % originated from multiple disciplines, including the philosophy of minds in cognitive science, ecological psychology in behavior science, and machine learning. 

% % that , ``embodiment'', ``embeddedness'', ``enacted'', and ``affordance'', which escalate the difficulty with extra barriers, since this research question exists in multiple disciplines, including the philosophy of minds in cognitive science, ecological psychology in behavior science, and machine learning. 

% % Following ``The quest for a common model'' proposal by Sutton et al. 2022, this paper will maintains the minimum ...  

% \paragraph{On agent-environment boundary}
% This problem is hard, this boundary is not definite, is always changing. Instead of extracting this boundary, how about extract an invariant model that is shared across all task MDPs?

% \paragraph{Embodied cognition theory}
% Explain the theory. Why this is the key to build incrementally self-improvement agent?

% \paragraph{Agent state construction}

\subsection{Pre-training: embodied agent-state construction}
% 或许这些研究最大的障碍就是过于多的名词，解释，来自己不同的领域，互相重叠又互相contradict。
We describe how to build a fundamental agent-environment interaction model using agent-state construction. The final goal is to build an embodied agent-state set, which servers as the reusable agent-environment interaction model. This set has several properties, so call it agent-environment interaction model.
There are three steps during pre-training.
\paragraph{(1) Domain labelling: Offline dataset collection modeling}

\paragraph{(2) Learn predictive representations: Extract domain invariant successor features}

\paragraph{(3) Discretize to prototypes: Construct an embodied set structure}

Algorithm 1: Pretrain agent-environment interaction models (embodied agent-state construction)
Given dataset with domain labels, how to construct the agent-environment interaction models .


Embodied agent-state set = generally reusable agent-environment interaction model
This set has several properties.
1, it's transferrable across domains defined by non-stationarity operators. 


\subsection{Re-use: a backbone for general downstream task learning}
There are three steps when reusing the pre-trained model in downstream task learning.

\paragraph{(1) Stability--Retain previous knowledge: Embodied Feature Projection}
asdfasdf

\paragraph{(2) Plasticity--Adapt to changes: Projected Bellman Updates}

\paragraph{(3) Put together--Downstream task learning: DQN-embodied version}

Algorithm 2: Reuse: DQN-embodied version as an example
Given a new downstream task, how to reuse... DQN example.
