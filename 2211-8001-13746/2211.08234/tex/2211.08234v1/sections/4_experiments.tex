% \textbf{Purposes} \hspace{0.5cm}
We aim to test \textit{the generality} of our method by increasing the difference w.r.t. the environment, the task objective, and the agent's sensory modality, between pre-training and reuse. 
% (2) Downstream task learning \textit{stability-plasticity} analysis 
% by testing, if our method can handle non-stationarity manually added during training and after deployment in a continual learning setting.  

% \textbf{Heterogeneous task settings}
% We consider the following three heterogeneous task settings during evaluation: (1) changed task objectives, (2) changed environmental dynamics, and (3) changed sensor modalities\footnote{For example, adding a text description of the task to the agent's observations.}, since we believe the combination of the above three will cover most of the pre-training \& reusing task settings in real-world applications like robotics, autonomous driving and gaming.
\textbf{(1) Choosing the environment}\hspace{0.5cm}
% To serve the above evaluation purpose, we require the evaluation environment should be easy to change the task objective (reward), environmental dynamics, and sensor modalities. There are three prevalent environments that fulfil the requirements---Jelly Bean World\citep{Platanios2020JellyBW}, Meta-world\citep{Yu2019MetaWorldAB}, and MiniGrid\citep{minigrid}. We choose MiniGrid since it is highly re-configurable and easy to reproduce results. The MiniGrid environment can support various environments with different room layouts and objects, various task settings like goal-reaching, fetching-object, and crossing, and changed sensor modality by adding textual observation as text commands.
To serve the above evaluation purpose, we require the evaluation environment should be easy to change the task objective (reward), environmental dynamics, and sensor modalities. We choose MiniGrid~\citep{minigrid} since it is highly re-configurable and easy to reproduce results. We test tasks including ``goal reaching'',  ``picking up the grey box'' and  ``putting the green key near the yellow ball''. 
% Note that the second task is considered challenging when learning from scratch, and the third task requires text prompts otherwise it's not solvable by a vanilla RL agent\citep{minigrid}.
% The MiniGrid environment can support various environments with different room layouts and objects, various task settings like goal-reaching, fetching-object, and crossing, and changed sensor modality by adding textual observation as text commands.
% \textbf{(3) Environment observations, actions, and tasks}

 We use the default partial observability setting that the agent can only observe nearby $7\times7$ cells. Each cell is described with 3 input values: [OBJECT\_IDX, COLOR\_IDX, STATE], which gives the object id, cell color and door state (open, closed, locked). Therefore, the observation $x$ is a $7\times7\times3$ dimensional vector. In changing the sensor modality test, we manually add a textual observation by using the embedding of a text $z_{t}$ to augment $x_{t}$ with $[x_{t}, z_{t}]$.

The agent's action space remains unchanged during pre-training and downstream task learning, which is a discrete set containing 6 action choices: turn left, turn right, move forward, pick up an object, drop the object being carried, and toggle (open doors, interact with objects).

% 

\textbf{(2) Dataset collection}\hspace{0.5cm}
We consider 15 different room layouts environments during pre-training, specifically, 1 layout using 1 room, 4 layouts using 2 rooms and 10 layouts using 4 rooms. The room layouts are randomly generated. To fully utilize the agent's action capability, especially for ``picking up an object'', ''dropping an object being carried'', and ``toggle'', we randomly place objects (blue keys, blue doors, and different colored balls). For each room layout, we randomly configure 10 different object placement scenarios. Therefore, we have 150 environments to generate the agent's experience for pre-training. We then let the agent explore uniformly to navigate the rooms and interact with the objects without any task goals to collect samples. Therefore, we have 150 task-free POMDPs $\{\mathcal{M}_{i}\}_{i=1}^{150}$, which result in 150 environment domains $\{Y_{i}\}_{i=1}^{150}$, with each $Y_{i}$ is a one-hot encoding vector to denote the domain label. 

In summary, during data collection, we created 150 play zones for the agent to explore freely without defining any task goals. For each domain, we collected $1M$ samples which compose the dataset $\mathcal{D}=\{\mathcal{D}_{i}\}_{i}^{150}$ with $150M$ samples. Each samples is a tuple $(x, a, x')$.

% without any object)(show 4 small pics here). Arrow: A --> B, Pre-training, Re-use. B1: Reuse in downstream tasks, changed task objectives. Not reaching the target, but defined by a random reward function, r(s, a, s'), shown 1 pic, a random reward function meaning on top of the grid world env. B2: Reuse, changed environmental dynamics, this is done by a totally different task, adding objects will change the env dynamics, the agent need to pick up the object, show one pic. We choose the unlock pickup task, it has only two rooms there. B3: Reuse, changed sensory modalities. Add a text description, for example, it's a function of (s, a, s'), gives you "far, close to the red, green object".  We choose MiniGrid-LockedRoom-v0, but not reaching the target, but pick up the box, agent needs to get the key and unlock the door, and go to pick up a box. This one has six rooms. It requires the agent to use the text info to finish the task. Add the text modality

\textbf{(3) Pre-training: embodied set construction}\hspace{0.5cm}
Following~\citep{Hoang2021SuccessorFL}, we pre-train a variational auto-encoder (VAE~\citep{kingma2013auto}) that maps observation $x_{t}$ to a feature vector $\phi_{t}$, which is then used to learn the cross-domain transferable successor features by learning the function $\psi^{\pi_{0}}(x, a;\theta_{sf})$ using methods proposed in Sec. 3.2. During embodied set construction, we set $N=10K$ to discretizes all SFs into 10K clusters and construct the embodied set $\Omega^{e}$ representing 10K prototypes behaviors.


\textbf{(4)Re-use: a backbone for downstream task learning}\hspace{0.5cm}
Given the pre-trained embodied set structure $\Omega^{e}$, we test how it can be reused to accelerate downstream task learning using our proposed DQN-embodied method (see Algorithm 2), as described below. 

