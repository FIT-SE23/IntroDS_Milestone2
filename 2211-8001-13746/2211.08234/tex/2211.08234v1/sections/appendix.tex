\subsection{Learn cross-domain transferable successor features}
\label{sec:app_cross_domain}
Assume a neural network with parameters $\theta_{sf}$ is used to approximate the SF representation denoted as $\psi^{\pi_{0}}(x, a;\theta_{sf})$, where $(x, a)$ comes from an arbitrary environment domain. Our aim is to use loss Eq. \ref{eq:total_loss} to find the optimal $\theta_{sf}$. By adding the Lagrangian multipliers $\lambda_{u}$ and $\lambda_{l}$, the Lagrangian dual of Eq. \ref{eq:total_loss} is:
\begin{equation}
\begin{split}
  \underset{\theta_{sf}}{min}  &  \hspace{0.3cm} \mathcal{L}_{sf} + \lambda_{u} I(\mathbf{\psi}^{\pi_{0}}, {Y}) - \lambda_{l} \sum_{i=1}^{M} I(\mathbf{\psi}^{\pi_{0},i}, {x^{i}}) \\
 \end{split}
 \label{eq:total_loss2}
\end{equation}
Directly optimizing the mutual information (MI) terms of Eq. \ref{eq:total_loss2} in high-dimensional space is challenging, we provide tractable solutions by approximating the upper bound and lower bound.

\paragraph{Upper bound of $I(\mathbf{\psi}^{\pi_{0}}, {Y})$}
Following~\citep{feng2019self}, the $I(\mathbf{\psi}^{\pi_{0}}, {Y})$ is upper bounded by replacing one of the marginal distributions with a variational posterio distribution. For any distribution $q(Y)$, we can have an upper bound of  $I(\mathbf{\psi}^{\pi_{0}}, {Y})$:
\begin{equation}
\begin{split}
  I(\mathbf{\psi}^{\pi_{0}}, {Y}) & = \mathbb{E}_{p_{ }(\mathbf{\psi}^{\pi_{0}}, Y)}[log\hspace{0.1cm}{p_{ }}(Y|\psi^{\pi_{0}}) - log \hspace{0.1cm} p(Y)] \\
  & = \mathbb{E}_{p_{ }(\mathbf{\psi}^{\pi_{0}})}[D_{KL}(p_{ })((Y|\psi^{\pi_{0}})||q(Y)) - D_{KL}(p(Y)||q(Y))] \\
  & \leq \mathbb{E}_{p_{ }(\mathbf{\psi}^{\pi_{0}})}D_{KL}(p_{ }(Y|\psi^{\pi_{0}})||q(Y))\\
 \end{split}
\end{equation}
, where $q(Y)$ can be empirically estimated in a non-parametric way, like using kernel density estimation from the dataset. Therefore, minimization of the KL-divergence term will result in minimizing the mutual information term, which can be formulated as an adversarial training objective, as derived in \citep{feng2019self}:
\begin{equation}
\begin{split}
  \underset{\theta_{sf}}{min} \hspace{0.1cm} \underset{\theta_{u}}{max} \mathcal{L}_{u} = \mathbb{E}_{p_{\theta_{sf}}(\psi^{\pi_{0}}, Y)}[log \hspace{0.1cm} q_{\theta_{u}}(Y|\psi^{\pi_{0}}) - log \hspace{0.1cm} q(Y)]
 \end{split}
\end{equation}
, where $q_{\theta_{u}}(Y|\psi^{\pi_{0}})$ is a classifier that predicts the probability of the sample belonging to label $Y$, and $q(Y)$ is a constant that can be dropped during optimization.

\paragraph{Lower bound of $I(\mathbf{\psi}^{\pi_{0},i}, {x^{i}})$}
There are multiple ways (~\citep{oord2018representation,nowozin2016f}) to approximate a lower bound of the mutual information term. We follow~\citep{feng2019self} that maximizes the MI term by maximizing the Jensen-Shannon divergence form of MI, $\hat{I}^{JSD}(\mathbf{\psi}^{\pi_{0},i}, {x^{i}};\theta_{l})$, where $\theta_{l}$ parameterizes the JSD. Readers can refer to~\citep{nowozin2016f,feng2019self} for more details. 

A complete loss term of Eq. \ref{eq:total_loss2} can be rewritten as a minimax optimization objective:
\begin{equation}
\begin{split}
  \underset{\theta_{sf},\theta_{l}}{min}  \hspace{0.1cm} \underset{\theta_{u}}{max} \mathcal{L}_{sf} + \lambda_{u} \mathcal{L}_{u} - \lambda_{l} \sum_{i=1}^{M} \hat{I}^{JSD}(\mathbf{\psi}^{\pi_{0},i}, {x^{i}};\theta_{l})
 \end{split}
\end{equation}
, which is now tractable.

% \subsection{Generality of re-use evaluations protocols}
% \label{sec:gen_test}
% We conduct the \textit{generality test} by gradually adding differences between the downstream tasks and the pre-trained tasks. Fig. \ref{fig:exp_setup} gives an overview of task setup in the below three tests. We consider three generalilty test protocols as shown in Fig. \ref{fig:exp_setup}.
% \begin{itemize}
%     \item (1) Task adaptation: changed task objective, as shown in Fig. \ref{fig:exp_setup}B.
%     \item (2) General task adaptation: changed task objective + changed environment, as shown in Fig. \ref{fig:exp_setup}C.
%     \item (3) More general task adaptation: changed task objective + changed environment + changed sensor modalities, as shown in Fig. \ref{fig:exp_setup}D.
% \end{itemize}
% \begin{figure*}[t]
%  	\setlength{\belowcaptionskip}{-10pt}
% 	\centering
% 	\includegraphics[width=0.95\textwidth]{figs/fig2.pdf}
% 	\caption{\footnotesize \textbf{A:} Environments for collecting the pre-training dataset with different room layouts and random objects (keys, doors, coloured balls) for the agent to interact with. \textbf{B:} Downstream task ``goal-reaching'' uses the same environment as to pre-training but with a new task objective ``reach the green goal'' defined by the new reward function. \textbf{C:} Downstream task ``picking up the grey box'' uses a changed environment (locked door, grey box, coloured triangles are unseen during pre-training), and obviously a changed task objective. This task is challenging for vanilla RL agents when learning from scratch. \textbf{D:} Downstream task ``putting the green key near the yellow ball'' uses a changed environment (coloured triangles unseen during pre-training), changed task objective, and changed sensor modality by adding textual observations. Note this task requires text prompts otherwise it is difficult to solve by a vanilla RL agent\citep{minigrid} \normalsize}
%  %     \vspace{-3.5mm}
% 	\label{fig:exp_setup}
% \end{figure*} 

% \subsection{Learning stability and plasticity evaluation protocols}
% \begin{figure*}[t]
%     \centering
%     \includegraphics[width=0.5\textwidth]{figs/fig5.pdf}
%     \caption{\footnotesize Fig. Left env at start --> train and get an optimal policy., Middle env changed at 30K completion, Right env changed at 60K. Say clearly, this is changed during training.\normalsize}
%     %\label{fig:decentcem-a-architecture}
%     %\vspace{-3.5mm}
% \end{figure*}
\subsection{DQN-embodied}
\label{sec:dqn_embodied}
By using the projected Bellman updates (eq. \ref{eq:bellman1} and eq. \ref{eq:bellman2}) proposed here, we compute the target values as described in eq. \ref{eq:dqn1} and eq. \ref{eq:dqn2}. Then, the full DQN-embodied algorithm alternates optimizing $Q^{ \pi}$ and $V^{ \pi}_{proj}$ is as below:

\begingroup
\removelatexerror% Nullify \@latex@error
\begin{algorithm}[H]
% \setlength{\belowcaptionskip}{-10pt}
	\SetAlgoLined
	\small
	%\textcolor[rgb]{0.14,0.36,0.73}{\textbf{Initialization}}\\
        Given pre-trained embodied set $\Omega^{e}$, feature projection operator $\Pi_{\Omega^{e}}(x,a)$\\
	Initialize $Q^{ \pi}(.;\theta_{u})$, $V^{ \pi}_{proj}(.;\mathbf{w}_{u})$,  and replay buffer $\mathcal{D}$\\
	\For{i=1:N}{
	    \tcp{Replay buffer}
	    \For{t=0:T}{
	        $\epsilon$ greedy select action $a_{t}$ based on ${max}_{a} Q^{ \pi}(x_{t},a_{t};\theta_{u,i})$\\
	        Execute action $a_{t}$ in environment, observe $x_{t+1}, r_{t}$\\
	        Append transition sample $(x_{t}, a_{t}, r_{t}, x_{t+1})$ in $\mathcal{D}$\\
	        Randomly sample batch transitions $\mathcal{B}=\{(x, a, r, x')\}$ from $\mathcal{D}$\\
	        \tcp{Learn $Q^{ \pi}$}
	        Set $y_{i} = \mathbb{E}_{(x, a, x') \sim \mathcal{T}^{u}}[r^{u}+\gamma V^{ \pi}_{proj}(\Pi_{\Omega^{e}}(x',\underset{a'}{\argmax}Q(x',a';\theta_{u,i-1}));\mathbf{w}_{u, i-1})]$\\
	        Perform gradient descent step on $\mathcal{L}_{i}(\theta_{u,i})=(y_{i} - Q^{ \pi}(x, a;\theta_{u, i}))^{2}$\\
	        \tcp{Learn $V^{ \pi}_{proj}$ }
	        Set $ y_{proj, i} = \mathbb{E}_{(x, a, x') \sim \mathcal{T}^{u}}[r^{u} + \gamma \underset{a'}{max} Q^{ \pi}(x', a';\theta_{u, i-1})]$\\
	        Perform gradient descent step on $\mathcal{L}_{i}(\mathbf{w}_{u,i})=( y_{proj, i} - V^{ \pi}_{proj}(\Pi_{\Omega^{e}}(x, a));\mathbf{w}_{u,i})^{2}$
	    }
	}
	\caption{DQN-embodied}
\end{algorithm}
\endgroup
