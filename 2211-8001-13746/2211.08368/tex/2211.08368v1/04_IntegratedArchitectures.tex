\chapter{Learning-based Pipelines}
\label{sec:integrated}
This chapter looks at machine learning models that learn from prior experience to help solve a sampling-based motion problem, but do not explicitly focus on learning a single primitive.

\begin{mdframed}[hidealllines=true,backgroundcolor=red!20,frametitle=Categories of work on Learning-based Pipelines]
\begin{myitem}
    \item \textbf{Retrieve and repair methods} \citep{BAG-2012,CSMOC-2014}
    \item \textbf{Neural motion planning} \citep{qureshi2019motion,JurgensonT19,strudel2020learning,dynamic-mpnet,li2021mpc, 9501956, 9143433}
    \item \textbf{Learning a lower dimensional planning space} \citep{ichter2019robot, chen2019learning, ichter2021broadlyexploring}
\end{myitem}
\end{mdframed}

\section{Retrieve and Repair Methods}


\begin{figure}[h!]
    \centering
    \includegraphics[width=.47\textwidth]{images/retrieve_and_repair_1.png}
    \includegraphics[width=.47\textwidth]{images/retrieve_and_repair_2.png}
    \caption{Retrieve and repair methods (left) retrieve a path from a library.  The retrieved path may have parts that are invalid (orange).  (right) These methods then repair the invalid portions by adding new nodes and edges (purple)}
    \label{fig:repair_and_retrive}
\end{figure}

This class of models use a database of paths and solve queries by retrieving these paths and adjusting them to generate solutions to new problems (Figure \ref{fig:repair_and_retrive}). The \emph{Lightning} framework \citep{BAG-2012} incorporates a \emph{Retrieve-Repair} module that retrieves a path from a library stored on the cloud that includes experience (e.g. paths) collected from multiple robots.  \emph{Lightning} selects a path from the library using two heuristics: one that quickly selects a set of candidate paths using the distance between their endpoints and the start configuration, and one that selects the path with the least constraint violation among these candidates.  

Similarly, the \emph{Thunder} framework \citep{CSMOC-2014} is an experience-based planner that stores paths obtained by probabilistic sampling in a SPArse Roadmap Spanner (SPARS). To solve a new problem,  it searches the database for a solution that was able to solve a similar problem and uses this solution as a basis to solve the new problem.  It identifies parts of the solution that are no longer feasible given the new problem and ``repairs'' them to form a valid solution.  This approach is well suited for large configuration spaces and spaces that include invariant constraints.



% We also discuss methods {\bf map sensor inputs to trajectory controls} while maintaining an internal motion planning model \citep{qureshi2019motion,TCFFF-2019,pan2017agile}. We also look at a number of systems that use {\bf learning from demonstration} \citep{YA-2017,Pfeiffer_2017}.  
% Finally, we look at systems that {\bf use a lower dimensional representation} of the motion planning problem \citep{ichter2019robot,mcfbr-2019}. 

\section{Neural Motion Planning} 

\begin{figure}[tb]
    \centering
    % \includegraphics[width=\textwidth]{images/neural_prm_online_portion.PNG}
    \includegraphics[width=0.32\textwidth]{images/Fig4p2-4.png}
    \includegraphics[width=0.32\textwidth]{images/Fig4p2-5.png}
    \includegraphics[width=0.32\textwidth]{images/Fig4p2-6.png}
    \caption{{\bf Architecture of a neural motion planner (NMP) \citep{qureshi2019motion}.}  The NMP takes as input the current state $x_i$, the goal state $x_G$ and a representation of the environment such as a point cloud. (Left)  The NMP outputs $\hat{x}_{i+1}$, the predicted next state of the robot at the next timestep, and attempts to connect to it. (Middle) If the connection fails, a path from the goal is attempted. (Right) If the connection succeeds, the predicted state is added to the solution path. A hybrid approach uses the NMP alongside a traditional \sbmp \ like RRT*.}
    \label{fig:neural_motion_planner}
\end{figure}

This class of methods uses a dataset of solved motion planning problems to learn a neural network model that improves the efficiency of motion planning on different, yet similar problems (Figure~\ref{fig:neural_motion_planner}). \emph{Motion Planning Networks (MPNet)} \citep{qureshi2019motion} is a deep neural network architecture which learns to approximate the computation of a sampling-based motion planner. MPNets are comprised of an  encoder  network  and  a planning  network.  The encoder network takes as input a point-cloud  representation of a workspace and learns to encode this point-cloud into a latent space.  The planning network takes as input the latent space, the robot's configuration at the current time-step and a goal configuration, and it is trained to predict the robot's configuration at the next time-step.  The planning network is used along with a bi-directional iterative search algorithm 
to generate trajectories that are feasible given the problem's constraints.  

\textit{Deep Deterministic Policy Gradient for Motion Planning} (DDPG-MP) \citep{JurgensonT19} investigates an alternative approach to training a motion planner approximated as a neural network (referred to as a \textit{neural motion planner}). The approach is based on reinforcement learning rather than supervised learning, and achieves higher accuracy by actively exploring the problem domain. 

An integral component of neural motion planners is the learned representation of the problem's obstacles from sensory input. Recently, the \textit{PointNet} architecture \citep{qi2017pointnet} has been used to learn an efficient and effective representation of point clouds for neural motion planning \citep{strudel2020learning}. This representation is then concatenated with the goal configuration and passed to a neural network which outputs actions. This network uses the \textit{Soft Actor-Critic} (SAC) reinforcement learning algorithm to learn the control policy online.   


\textit{Dynamic MPNet} \citep{dynamic-mpnet} extends the MPNet framework to non-holonomic robots. The network is trained on example solution trajectories produced by an RRT* planner to predict the next state that the robot must steer to. The network receives as input the goal state and a costmap of the local area surrounding the robot's current state. The neural planner is called iteratively at every state until it produces a feasible solution trajectory. If no such trajectory is found, the method falls back to classical motion planning.

\textit{MPC-MPNet} \citep{li2021mpc} extends the MPNet framework to deal with the challenges of kinodynamic planning. The network is trained on example solution trajectories produced by the AO SST algorithm \citep{li2014asymptotically} to predict a batch of candidate next states (waypoints) given a batch of environment encodings as well as current and goal states. Parallelized Model Predictive Control (MPC) is used to steer the robot from its current state to the predicted waypoints. The procedure is repeated either in a greedy or tree-based search framework to obtain kinodynamically feasible solution trajectories in new planning environments.

\textit{Constrained MPNets} (CoMPNet) \citep{9501956} and its extension, CoMPNet X \citep{9143433} extend the MPNet framework to the problem of motion planning in the presence of task constraints. CoMPNet takes as input a task and observation encoding and outputs an intermediate configuration for path planning. The demonstrated trajectories are collected using a bi-directional RRT planner. CoMPNetX introduces neural-gradient-based projections to generate informed, implicit manifold configurations that can speed up any \sbmp.

\section{Learning a lower dimensional planning space}

\begin{figure}[tb]
    \centering
    \includegraphics[width=.99\textwidth]{images/4.3.png}
    \caption{{\bf Example of using learned lower dimensional space. } A simulated quadruped robot (left) with a 111-dimensional state space and 8-dimensional control space must navigate the maze to reach the goal (red circle). It may be practically infeasible to apply a \sbmp \ directly to this problem, so a learned \textit{encoder} model transforms the problem into a lower-dimensional space (right) where a \sbmp \ can be applied to find a solution. Once a solution has been found, it can be applied directly to the high-dimensional problem via a learned \textit{decoder}.}
    \label{fig:lower_dimensional_space}
\end{figure}


Under this relatively new framework, a learned model maps a high-dimensional motion planning problem into a lower dimensional state space (Figure \ref{fig:lower_dimensional_space}), and uses SBMP techniques to solve the lower dimensional problem. \textit{Learned Latent RRT} (L2RRT) \citep{ichter2019robot} learns this mapping via an autoencoder. It constructs a tree directly in this lower dimensional space by propagating randomly sampled controls in the lower dimensional space using a learned latent dynamics model, and checking for collisions in the lower dimensional space using a learned collision checking neural network. The autoencoder and latent dynamics model are trained using trajectories obtained by iteratively propagating random samples from the control space for a fixed number of time steps. The collision checking network is trained using randomly samples from low dimensional states (obtained via the trained encoder) that are in $\mathbb{C}_{free}$ and $\mathbb{C}_{obs}.$ The collision checking network outputs the probability of a latent state $z_t$ being in collision or not, and L2RRT makes use of a pre-defined confidence threshold to decide whether to trust the learned collision checker or not.

The \textit{Neural EXploration-EXploitation Tree} (NEXT) \citep{chen2019learning} is a meta neural motion planner that learns generalizable problem structures from previous planning experiences. An attention-based neural network architecture is trained to extract a planning representation from demonstrated paths on multiple random problems. The learned tree expansion procedure can balance exploration and exploitation by using an Upper Confidence Bound (UCB) style algorithm.

\textit{Broadly-Exploring, Local-policy Trees} (BELT) \citep{ichter2021broadlyexploring} is an RRT-like algorithm for long-horizon task planning that plans over an abstract task space, which is learned from demonstrations. The exploration of the RRT is guided by a task-conditioned, learned policy capable of performing general short-horizon tasks. Once a solution is found by using search, the local task-conditioned policy is capable of executing the trajectory on a real robotic platform.

\begin{mdframed}[hidealllines=true,backgroundcolor=blue!20,frametitle=Discussion on Learning-based Pipelines]
There is a growing body of work in collecting prior motion planning experience, and using learned models to imitate the motion planner's performance on tasks from a similar domain. These pipelines may also project a higher dimensional problem into a reduced space, where it may be easier to find a feasible path. These pipelines can result in practical efficiency by reducing expensive SBMP operations to a few inference calls to a learned model. Nevertheless, there are considerations about the size of the datasets required to train these models, their generalization capabilities, as well as theoretical guarantees on path quality, that remain open directions for future research. 
\end{mdframed}

% \aravind{Having gone over this work in more detail, I'm not sure if it's within the scope of this survey.}
% In \citep{mcfbr-2019}, Mukadam et. al propose a method for learning Riemannian Motion Policies for use by the $RMPflow$ \citep{Cheng_2020} algotirithm.  The $RMPflow$ algorithm takes as input a set of Riemannian Motion Policies (RMPs) performs a fusion of these policies.  Mukadam et. al introduce a modified version of $RMPflow$ called $RMPfusion$ which includes a set of multiplicative weight functions in the policy fusion step. These weight functions allow the algorithm to transition between different variations of the $RMPflow$ algorithm.  Their method uses a neural network  to set the parameters of these weighting functions from the the robotâ€™s configuration and the environment.   %\troy{I am not certain if this is the correct section for this work} \aravind{We could probably move this to the Integrated Architectures section.}

% \aravind{This work is end-to-end deep RL, not sure if we should include it}
% In \citep{TCFFF-2019}, Chiang et. al. addresses the problems of reward function generation and neural network structure for an agent that learns a mapping of robot observations to controls.  They observe that sparse reward functions can cause network training to fail, and they propose to use reward shaping to create a proxy objective  function that is less sparse then the true function.  They propose the \emph{AutoR} framework which includes a method for automatically specifying neural network structure and along with a reward function that can be used to train the network.  Their method takes a parameterized reward function along with the true objective function, and uses hyperparameter optimization to obtain a proxy reward function.  They include the size of each layer of the network as a hyperparameter so that the structure of the network is computed as part of this optimization.  AutoRL is is trained in a small environment with static obstacles, and can be used to perform point-to-point or path-following policies in environments that include both static and dynamic obstacles.

% \begin{figure}
%     \centering
%     \includegraphics[width=.4\textwidth]{images/Pfeiffer.PNG}
%     \caption{Neural network used in \citep{Pfeiffer_2017} to learn a mapping from sensor data to steering controls. This network consists of a CNN that takes the robot's sensor data.  The output of the CNN is then passed to a FC along with the target information (e.g. the goal).  This FC then outputs steering commands that are performed by the robot.  This image was taken from \citep{Pfeiffer_2017}.}
%     \label{fig:pfeiffer}
% \end{figure}


% Pfeiffer et. al. \citep{Pfeiffer_2017} use learning from demonstration to learn a mapping from sensor data from 2D-laser range finders to robot steering commands.  This work uses a neural network consisting of a  CNN and a FC to represent the mapping of sensor inputs to steering commends (see Figure \ref{fig:pfeiffer}).  This CNN is comprised of two building blocks and its output serves as input to the FC.   The FC also takes as input the target information (e.g. the goal). Training data for this network consists of  laser  measurements,  target  information and velocity commands provided by an expert operator.  This method was applied to a Kobuki based TurtleBot with a  front-facing Hokuyo UTM laser  range  finder and was demonstrated in a set of real-world environments.
