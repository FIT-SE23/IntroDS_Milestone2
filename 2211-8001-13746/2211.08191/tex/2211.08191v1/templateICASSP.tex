% Template for ICASSP-2021 paper; to be used with:
%          spconf.sty  - ICASSP/ICIP LaTeX style file, and
%          IEEEbib.bst - IEEE bibliography style file.
% --------------------------------------------------------------------------
\documentclass{article}
\usepackage{spconf,amsmath,graphicx}
\usepackage[acronym]{glossaries}
\usepackage{amssymb}
\usepackage{amsfonts,amsthm}
\usepackage{multirow}
\usepackage{booktabs}
\usepackage{xcolor}
\usepackage{bm}
\usepackage[subrefformat=parens]{subfig}
\usepackage{cite}
\usepackage{float}

% Example definitions.
% --------------------
\def\x{{\mathbf x}}
\def\L{{\cal L}}

% Title.
% ------
% graphical model 
\title{Improved disentangled speech representations using contrastive learning in factorized hierarchical variational autoencoder}
%disentanglement of speech representations using contrastive learning in factorized hierarchical VAE}
%\title{Applying supportive contrastive learning in graphical model for disentangled representation learning of speech signal}
%
% Single address.
% ---------------
\name{Yuying Xie\thanks{The work of Yuying Xie is supported by China Scholarship Council.}, Thomas Arildsen, Zheng-Hua Tan}
\address{Department of Electronic Systems, Aalborg University, Denmark}
%
% For example:
% ------------
%\address{School\\
%	Department\\
%	Address}
%
% Two addresses (uncomment and modify for two-address case).
% ----------------------------------------------------------
%\twoauthors
%  {A. Author-one, B. Author-two\sthanks{Thanks to XYZ agency for funding.}}
%	{School A-B\\
%	Department A-B\\
%	Address A-B}
%  {C. Author-three, D. Author-four\sthanks{The fourth author performed the work
%	while at ...}}
%	{School C-D\\
%	Department C-D\\
%	Address C-D}
%
\newacronym{fhvae}{FHVAE}{factorized hierarchical variational autoencoder}
\newacronym{vae}{VAE}{variational autoencoder}

%%%%%%%%%%%%%%%%%%%
% 1. too weak
% cross-utterance learning / extra utterance
% limitation of only using one utterances
% elaborate further about weakness (relative weak/limitation) and limitation
% unsupervised leanring within one uttterance
% contrastive learning can enhance the learning by inducing learning with more utterance / cross-utterance
% 2. t-SNE
% calculate the mean and variance of the clusters;
% LDA
% (Distance between clusters and variance within the cluster (diagonal sum-up) in latent variable space to support t-SNE plotting)
% have a circle to highlight the cluster 8/11;
% discriptions are too board, do not make general conclusion. 
% Second statement it holds, but not the first statement.
% 3. global for utterances
% statistics are global cross the utterances; statistics are all the same for all the utterances;
% 4. abstract
% abstract should include voice conversion
% 5. reference 
% reference 9 is not mentioned
% 6. con both means contrastive learning and conversion, change it to cont
% 7. lamba / beta mention how to find it
%  
% 8. how to make paper read better
% related work should be put into introduction? it is too short 
% why section 3 is not related work
% introduction is too much about FHVAE
% fhvae is data-temporal 
% exploit temporal struction and 
% 2019 ICML best paper award - without prior distanglement is not possible
% (more machine learning content)
% contrastive learning:
% extend the illustration, what contrastive learning can bring into, refer to ICML paper, and call back
% in the introduction, delete the rest of the paper, but highlight the contribution of the paper
% highlight the improvement is consistent on latent variable space and voice conversion 
% 
% 
%
% 
%



\begin{document}
%\ninept
%
\maketitle
%
\begin{abstract}
%Disentangled representation learning aims at using explainable latent variables to represent independent data attributes. 
By utilizing the fact that speaker identity and content vary on different time scales, \acrlong{fhvae} (\acrshort{fhvae}) uses a sequential latent variable and a segmental latent variable to symbolize these two attributes.
Disentanglement is carried out by assuming the latent variables representing speaker and content follow sequence-dependent and sequence-independent priors. 
For the sequence-dependent prior, \acrshort{fhvae} assumes a Gaussian distribution with an utterance-scale varying mean and a fixed small variance. The training process promotes sequential variables getting close to the mean of its prior with small variance. However, this constraint is relatively weak.
Therefore, we introduce contrastive learning in the \acrshort{fhvae} framework.
The proposed method aims to make the sequential variables clustering when representing the same speaker, while distancing themselves as far as possible from those of other speakers. 
The structure of the framework has not been changed in the
proposed method but only the training process, thus no more cost is needed during test.
Voice conversion has been chosen as the application in this paper.
Latent variable evaluations include speakerincrease verification and identification for the sequential latent variable, and speech recognition for the segmental latent variable.
Furthermore, assessments of voice conversion performance are on the grounds of speaker verification and speech recognition experiments.
Experiment results show that the proposed method improves both sequential and segmental feature extraction compared with \acrshort{fhvae}, and moderately improved voice conversion performance. 
\end{abstract}
%
\begin{keywords}
disentangled representation learning, contrastive learning, voice conversion
\end{keywords}
%
\section{Introduction}
\label{sec:intro}
% 1. disentangled representation
% 2. apply disentangled representation learning on speech signal processing
% 3. contrastive learning
% 4. voice conversion
Representation learning~\cite{bengio2013representation} aims at extracting features from obeservations for getting better performance on downstream tasks. 
As a branch, disentangled representation learning~\cite{polyak21_interspeech, aloufi2020privacy} tries to extract features for representing the independent data attributes separately. 
Even though unsupervised learning is a hot topic nowadays, paper~\cite{locatello2019challenging} revealed that unsupervised disentangled representation learning is fundamentally impossible.
Applications of disentangled representation learning are broad, and one in speech is voice conversion~\cite{FHVAE, zhao2022disentangling,chan2022speechsplit2, qian2019autovc, tang2022avqvc, Reinhold2021contrastive, lian2022robust}. 

The basic strategy of using disentangled representation learning on voice conversion contains two stages: training and conversion.
Training makes the neural networks learn to extract features to represent speaker identity and content separately.
The converted utterances are generated from the speaker embedding from the target speaker and the content embedding from the source speaker.
Specifically, the authors of~\cite{qian2019autovc} carried out embedding disentanglement under an auto-encoder framework, by controlling the bottleneck dimension carefully to allow only content information to pass through. Speaker embedding in this work is extracted from a pre-trained speaker encoder according to~\cite{wan2018generalized}, and the segment length for extraction is between 3.5s to 4.5s. 
The authors of~\cite{Reinhold2021contrastive} used a deterministic style encoder and a statistical content encoder to extract speaker embedding and content embedding separately. Contrastive predictive coding is used on style encoder output and the framework is trained end-to-end. The segment length is between 2s and 4s.
 

\Acrlong{fhvae} (\acrshort{fhvae})~\cite{FHVAE} assumes  that speaker identity changes between sequences (longer than 200ms), while content varies between segments (200ms in experiments). 
Compared with other works, \acrshort{fhvae} neither require lengthy utterances nor a pre-trained model for extracting speaker embeddings.
However, \acrshort{fhvae} has its own limitations.
To equip sequential latent variables with representation capability,
the prior adopts a Gaussian distribution with a random mean which changes between utterances and a fixed small variance which makes sequential latent variables are similar within the utterance. However, the relationship of sequential variables between utterances has not been considered.

Contrastive learning intends to make representations symbolising the same class become as similar as possible, while making representations symbolising different classes become as dissimilar as possible. This idea fits speaker embedding learning, and there also exists some work using contrastive learning in speaker embedding extraction, like~\cite{tang2022avqvc, wan2018generalized}.


Inspired by the idea behind contrastive learning and related work, this paper applies contrastive learning in the \acrshort{fhvae} framework. 
Compared with \acrshort{fhvae} only concerning information within one utterance, contrastive learning brings cross-utterance information during training to enhance latent variable representation capability. 
The proposed method does not change the framework (and thus does not increase the model complexity during test), but the training strategy. Speech recognition and speaker verification are used to analysis the extracted latent variables and converted utterances.


\section{Factorized Hierarchical Variational Autoencoder}
\label{sec:original}

The nature of speech shows that speaker identity and content vary on different time scales: Speaker identity changes between sequences, while content varies faster, within segments. 
To utilize this fact in speech signal disentangled representation, \acrshort{fhvae} assumes two variables follow a sequence-dependent prior and a sequence-independent prior respectively. Latent variables representing speaker identity and linguistic content in the graphical model are denoted sequential variable and segmental variable in the following.

Specifically, we denote the speech feature (e.g. log-magnitude spectrogram) dataset $\bm{D}=\{{\bm{X}}^ {(i)}\}_{i=1}^M$, where $i$ represents the $i$-th sequence in the dataset, and the dataset contains $M$ utterances.
Assume each utterance ${\bm{X}}^{(i)}$ contains $N^{(i)}$ segments: ${\bm{X}}^{(i)}=\{{\bm{x}}^{(i,n)}\}_{n=1}^{N^{(i)}}$. 
$z_1^{(i,n)}$ and $z_2^{(i,n)}$ denote segmental latent variable and sequential latent variable in order, and superscript $(i,n)$ expresses that the variables are for the $n$-th segment of the $i$-th utterance. The following notation will omit the superscript $(i)$ for simplicity as training happens on segment-scale. 
Besides, $p(\cdot)$ and $q(\cdot)$ represent prior and posterior distributions, while $\theta$ and $\phi$ represent parameters in the generative model and the inference model, respectively.

The generative model in \acrshort{fhvae} assumes data $\bm{X}$ for each sequence is generated in the following process: (1) latent variable $\bm{\mu}_2$ is drawn from prior $p_{\theta}(\bm{\mu}_2) = \mathcal{N}(\bm{\mu}_2|\bm{0},\bm{I})$; (2) sequential latent variables $\{\bm{z}_2^{(n)}\}_{n=1}^N$ and segmental latent variables $\{\bm{z}_1^{(n)}\}_{n=1}^N$ are drawn from priors $p_{\theta}(\bm{z}_2|\bm{\mu}_2) = \mathcal{N}(\bm{z}_2|\bm{\mu}_2, \sigma_{\bm{z}_2}^2\bm{I})$ and $p_{\theta}(\bm{z}_1)=\mathcal{N}(\bm{z}_1|\bm{0}, \bm{I})$. 

The inference model in \acrshort{fhvae} assumes all posteriors of latent variables: $q_{\phi}(\bm{\mu}_2^{(i)})$, $q_{\phi}(\bm{z}_2|\bm{x})$ and $q_{\phi}(\bm{z}_1|\bm{x}, \bm{z}_2)$ are Gaussian distributions.
Means and variances of $q_{\phi}(\bm{z}_2|\bm{x})$ and $q_{\phi}(\bm{z}_1|\bm{x},\bm{z}_2)$ are from the neural network, while the mean $\bm{\widetilde \mu}_2^{(i)}$ of $q_{\phi}(\bm{\mu}_2^{(i)})=\mathcal{N}(\bm{\mu}_2^{(i)}|\bm{\widetilde \mu}_2^{(i)},\bm{I})$ is regarded as a parameter in the inference model.


The structure of \acrshort{fhvae} contains three modules: encoder 1 (denoted $\rm{Enc_1}$ in the following) and encoder 2 (denoted $\rm{Enc_2}$) for extraction of latent variables $\bm{z}_1$ and $\bm{z}_2$, and decoder (denoted $\rm{Dec}$) for data reconstruction and conversion. The data flow of the \acrshort{fhvae} framework is shown as below:
\begin{align}
\bm{z}_2 &= {\rm{Enc_2}} ({\bm{x}})
\label{z2 in FHVAE}\\
\bm{z}_1 &= {\rm{Enc_1}} (\bm{x}, \bm{z}_2)
\label{z1 in FHVAE}\\
\bm{ y} &= {\rm{Dec}} (\bm{z}_1, \mathbf{z}_2)
\label{x in FHVAE}
\end{align}
and $\bm{y}$ denotes the generated data.


The objective function of \acrshort{fhvae} contains four terms: log-likelihood loss to measure the reconstruction performance; the KL divergence to calculate the distance between prior and posterior of $\bm{z}_1$ and $\bm{z}_2$; the log-likelihood loss of mean of $\bm{z}_2$. The mathematical formulation is:
\begin{equation}
\begin{aligned}
\label{eq:FHVAE}
    &\mathcal{L}_{orig}^{(i,n)} \\&=  \mathbb{E}_{q_{\phi}(\bm{z}_{1}^{(i,n)},\bm{z}_{2}^{(i,n)}|\bm{x}^{(i,n)})}\left[\log p_{\theta}(\bm{y}^{(i,n)}|\bm{z}_1^{(i,n)},\bm{z}_2^{(i,n)})\right] \\
    & - \mathbb{E}_{q_{\phi}(\bm{z}_2^{(i,n)}|\bm{x}^{(i,n)})}\left[D_{KL}(q_{\phi}(\bm{z}_1^{(i,n)}|\bm{x}^{(i,n)},\bm{z}_2^{(i,n)})\|p_{\theta}(\bm{z}_1^{(i,n)}))\right]  \\
    & - D_{KL}(q_{\phi}(\bm{z}_2^{(i,n)}|\bm{x}^{(i,n)})\|p_{\theta}(\bm{z}_2^{(i,n)}| \bm{\Tilde{\mu}}_2^{(i)})) \\
    &  + \frac{1}{N^{(i)}}\log p _{\theta}(\bm{\Tilde{\mu}}_2^{(i)})
\end{aligned}
\end{equation}


When applied in voice conversion, superscript $src$ and $tar$ denote the latent variables from source speaker and target speaker in the following, and superscript $con$ denotes the variables prepared for voice conversion.
For sequential latent variable $\bm z_2$, the new latent variable $\bm{z}_2^{con}$ is generated by shifting the mean of $\bm{z}_2$ from $\bm{\mu}_2^{src}$ to $\bm{\mu}_2^{tar}$:
\begin{equation}
    \bm{z}_2^{con} = \bm{z}_2^{src} - \bm{\mu}_2^{src} + \bm{\mu}_2^{tar} 
\end{equation}
And the converted utterance is generated as:
\begin{equation}
    \bm{y}^{con} = {\rm {Dec}}(\bm{z}_1^{src}, \bm{z}_2^{con})
\end{equation}
The decoder uses the the new sequential latent variable $z_2^{con}$ and the segmental latent variable $z_1^{src}$ from the source speaker to generate the converted utterance.


\section{Proposed method}
\label{sec:proposed method}
% 1. the disadvantage of FHVAE
% 2. the motivation of proposed method
% 3. graphical illustration
% 4. loss function

% 1. why FHVAE can realize voice conversion
% 2. contrastive learning

% denotation of posterior and prior 

%Before presenting the proposed method, we need to dive deeply into why and how \acrshort{fhvae} works. 
As illustrated in Section~\ref{sec:original}, \acrshort{fhvae} assumes sequence-dependent and sequence-independent prior for sequential latent variable $\bm{z}_2$ and segmental latent variable $\bm{z}_1$, respectively. 
As linguistic content changes between segments but its statistic is global for all sequences, segmental-scale variable $\bm{z}_1$ has sequence-independent prior. 
For sequential latent variable $\bm{z}_2$, the mean $\bm{\mu}_2$ of its prior $p_{\theta}(\bm{z}_2|\bm{\mu}_2)$ is assumed drawn from a standard Gaussian distribution for each utterance. 
Thus prior $p_{\theta}(\bm{z}_2|\bm{\mu}_2)$ is sequence-dependent.
The training target for the sequential latent variable is to make $\bm{z}_2$ become close to $\bm{\mu}_2$, and to other $\bm{z}_2$ from the same utterance in Euclidean distance. 
And this is carried out by setting a small fixed variance in the prior
$p_{\theta}(\bm{z}_2|\bm{\mu}_2) = \mathcal{N}(\bm{z}_2|\bm{\mu}_2, \sigma_{\bm{z}_2}^2\bm{I})$.
The variance $\sigma_{\bm{z}_2}$ equals 0.5 as in paper~\cite{FHVAE}.
%The term $D_{KL}(q_{\phi}(\bm{z}_2^{(i,n)}|\bm{x}^{(i,n)})\|p_{\theta}(\bm{z}_2^{(i,n)}| \bm{\Tilde{\mu}}_2^{(i)}))$ in~\eqref{eq:FHVAE} leads the posterior of $\bm{z}_2$ close to its prior.


% why contrastive learning is needed
\begin{figure}[t]
  \centering
  \includegraphics[width=\columnwidth]{fig/contrastiveFHVAE.pdf}
  \caption{Framework of the proposed method.}
  \label{fig:contrastiveFHVAE}
\end{figure}


However, for sequential attribute representation learning, only  encouraging latent variable $\bm{z}_2$  closer within an utterance is relatively weak. The relationship of $\bm{z}_2$ between sequences should also be considered.
Therefore, we introduce contrastive learning in the \acrshort{fhvae} framework as an attempt to improve sequential representation. 
The idea behind contrastive learning is to make representations more similar within the same class, and less similar between different classes. And thus the cross-utterance information is tapped.  

% details of the work
The framework of the proposed method is shown in Fig.~\ref{fig:contrastiveFHVAE}. Same with \acrshort{fhvae}, this framework contains three modules: encoder 1 for segmental variable extraction, encoder 2 for sequential variable extraction, and decoder for reconstruction and conversion. The difference between the proposed method and \acrshort{fhvae} is: for every training step, the input contains speech features of three different utterances from two speakers: utterances 1 and 2 are from speaker 1, and speech features denote $\bm{x}(\mathrm s1, \mathrm u1)$ and $\bm{x}(\mathrm s1, \mathrm u2)$ in Fig.~\ref{fig:contrastiveFHVAE}; utterance 3 is from speaker 2, and is denoted $\bm{x}(\mathrm s2, \mathrm u3)$. $(\mathrm si, \mathrm uj)$ denote the feature from the $j$-th utterance from speaker $i$ in the following.

According to the data properties, the sequential latent variable $\bm{z}_2$ for utterances 1 and 2 should be naturally closer since they represent the same speaker. The sequential variable $\bm{z}_2$ for utterance 3 should be further from the other two since it represents another speaker. This enables contrastive learning for improving performance of latent variable $\bm{z}_2$ on speaker identity representation. The graphical illustration of using contrastive learning on latent variable $\bm{z}_2$ is shown in Fig.~\ref{fig:contrastive illustration on z2}. The target of training is to decrease the distance between $\bm{z}_2(\mathrm s1, \mathrm u1)$ and $\bm{z}_2(\mathrm s1, \mathrm u2)$, and increase the distances between $\bm{z}_2(\mathrm s2, \mathrm u3)$ and the two former. In this work, $L^2$-norm is used as distance metric. The contrastive loss of latent variable $\bm{z}_2$ is shown as below:
\begin{equation}
\label{eq:proposed method contrastive}
\begin{aligned}
    \mathcal{L}_{cont} =\ & 
    \lambda \| \bm{z}_2(\mathrm s1,\mathrm u1) - \bm{z}_2(\mathrm s1,\mathrm u2) \|_2^2 \\
    & - \beta\| \bm{z}_2(\mathrm s1,\mathrm u1) - \bm{z}_2(\mathrm s2,\mathrm u3) \|_2^2  \\
    & - \beta \| \bm{z}_2(\mathrm s1,\mathrm u2) - \bm{z}_2(\mathrm s2,\mathrm u3) \|_2^2
\end{aligned}
\end{equation}
Based on preliminary experiments and to make positive pair and negative pairs have same contribution in training, $\lambda = 0.01$, and $\beta = 0.005$ in this work.


The total loss function of the proposed method is:
\begin{equation}
\label{eq:proposed method}
    \mathcal{L}_{total} =  
    \mathcal{L}_{orig} - \mathcal{L}_{cont}
\end{equation}
in which $\mathcal{L}_{orig}$ is the loss for \acrshort{fhvae} as shown in~\eqref{eq:FHVAE}.

The voice conversion process is the same as for \acrshort{fhvae} which is explained in Section~\ref{sec:original}.



\begin{figure}[t]
  \centering
  \includegraphics[width=0.45\columnwidth]{fig/Z2illustration.pdf}
  \caption{Conceptual illustration of using contrastive learning on latent variable $\bm{z}_2$.}
  \label{fig:contrastive illustration on z2}
\end{figure}

\vspace{-1cm}
\begin{figure*}[htbp]
    \centering
    \subfloat[FHVAE]{
    \label{fig1:a}
    \includegraphics[width=0.28\paperwidth]{fig/baseline.pdf}
    }
    \subfloat[The proposed method]{
    \label{fig1:b}
    \includegraphics[width=0.28\paperwidth]{fig/proposed.pdf}
    }
    \caption{t-SNE plot of extracted sequential features from the compared frameworks.}
    \label{t-SNE}
\end{figure*}

\begin{table}
  \caption{Latent variable evaluation.}
  \label{tab:latent variable}
  \centering
  %\begin{tabular}{*{4}{>{\centering\arraybackslash}X}}
  \begin{tabular}{ c| c| c| c | c}
    \hline
    \multirow{3}{*}{\textbf{Framework}} & \multicolumn{3}{c|}{\textbf{Sequential}} & \textbf{Segmental} \\
    \cline{2-5}
    & \multirow{2}{*}{\small{EER(\%)}} &  \multicolumn{2}{c|}{\small{Accuracy(\%)}} & \multirow{2}{*}{\small{WER(\%)}}\\
    \cline{3-4}
    & & \scriptsize{GRU} & \scriptsize{GRU+FC} \\ 
    \hline
    FHVAE & $3.13$ & $79.69$ & $93.23$ & $27.5$ ~~~ \\ \hline
    Proposed & $\bm{2.73}$ & $\bm{85.94}$ & $\bm{96.88}$ & $\bm{26.3}$ ~~~ \\ \hline
  \end{tabular}
\end{table}

\section{Experiments}
\label{sec:experiments}
\subsection{Dataset}
\label{subsec:dataset}
The training set, development set and core test set of TIMIT have been used in this work. The sets contain respectively 462 speakers, 50 speakers, and 24 speakers. 
%For each speaker, TIMIT provides 10 utterances in total. 
In this work, 3 phonetically diverse sentences (labeled with 'SI') and 5 phonetically compact sentences (labeled with 'SX') have been used, as 8 utterances per speaker in the experiments. The feature used for the framework training is log-magnitude spectrogram, with the window size and hop size equal to 25ms and 10ms.
For voice conversion, one utterance is chosen randomly from 'SI' utterances for each speaker in test set. Since there are 24 speakers, 24 content-different utterances are chosen for voice conversion. Conversion has been made pair-to-pair, and thus 576 converted utterances have been generated.
\subsection{Implementation details}
\label{subsec:implementation details}
\textbf{Framework structure and training details: }The structure of encoders and decoders in the proposed method are the same as in \acrshort{fhvae}. All encoders and decoders contain one LSTM layer with 256 units and a fully-connected layer. The dimensions of latent variables in both frameworks equal 32. For fair comparison, the training settings are also same for baseline and the proposed method. Batch size equals 768. Each batch contains data from three kinds of utterance equally in the proposed method and random data for baseline. Learning rate equals $10^{-4}$, and the optimizer is Adam~\cite{ADAM}.

\noindent \textbf{Segmental-level feature evaluation}: The mean and variance of latent variable $z_1$ are used as the extracted segmental-level feature in the evaluation experiment. 
As the segmental-level feature assumed to represent linguistic content, speech recognition is used for evaluation. The speech recognition system is implemented by Kaldi toolbox~\cite{povey2011kaldi} and experiment details are the same as in~\cite{xie2021disentangled}. The results are shown in Table~\ref{tab:latent variable} under header 'Segmental'. 'WER' denotes word error rate (WER), and lower WER indicates better performance.

\noindent \textbf{Sequential-level feature evaluation}: 
The mean of $\bm{z}_2$, viz. $\bm{\mu}_2$ has been used as the sequential-level representation.
Speaker verification and speaker identification experiments have been done for fairness.
For speaker verification, the equal error rate (EER) based on cosine similarity score is utilized as evaluation metric, displayed in Table~\ref{tab:latent variable} under the header 'EER(\%)', the lower the better. Two neural network architectures have been applied for speaker identification: one implemented by 1-layer GRU and the other by 1-layer GRU with 512 units followed by a dense layer.
The details of speaker identification are the same as in~\cite{xie2021disentangled}. The results from the two classifiers can be found in Table~\ref{tab:latent variable} with header 'Accuracy(\%)', under 'GRU' and 'GRU+FC' respectively. The higher accuracy means the better performance.

\noindent \textbf{Voice conversion evaluation}: Speech recognition and speaker verification have been performed for conversion evaluation, as well implementation details can be found in~\cite{xie2021disentangled}. Results are shown in Table~\ref{tab:voice conversion}. Specifically, speaker verification is shown under header 'speaker'. 
The metric $\mathrm{ND}=({\mathrm{EER}_B-\mathrm{EER}_A})/{\mathrm{EER}_C}$. $\mathrm{EER}_B$ and $\mathrm{EER}_A$ are got when assume target speaker or source speaker as the truth in speaker verification in order. $\mathrm{EER}_C$ is the system error.
Lower 'ND' indicates better voice conversion. 'Female', 'Male' and 'All' means the score for female-to-female, male-to-male, gender-independent conversion evaluation. Speech recognition results are shown in Table~\ref{tab:voice conversion} under the header 'speech'.

\begin{table}
  \caption{Voice conversion evaluation}
  \label{tab:voice conversion}
  \centering
  %\begin{tabular}{*{4}{>{\centering\arraybackslash}X}}
  \begin{tabular}{ c| c| c| c | c}
    \hline
    \multirow{3}{*}{\textbf{Framework}} & \multicolumn{3}{c|}{\textbf{speaker}} & \textbf{speech} \\
    \cline{2-5}
    & \multicolumn{3}{c|}{ND} & \multirow{2}{*}{WER(\%)}\\
    \cline{2-4}
    & \scriptsize{Female} & \scriptsize{Male} & \scriptsize{All} \\ 
    \hline
    FHVAE & $\bm{0.63}$ & $2.81$ & $-0.67$ & $32.5$ ~~~ \\ \hline
    proposed & $1.00$ & $\bm{2.72}$ & $\bm{-0.84}$ & $\bm{32.1}$ ~~~ \\ \hline
  \end{tabular}
\end{table}
\subsection{Results and analysis}
\label{subsec:results and analysis}
\noindent \textbf{Sequential-level feature:}
Table~\ref{tab:latent variable} shows consistent improvement from the proposed method in all experiments. Additionally, t-SNE~\cite{van2008visualizing} plots give visualization in Fig.~\ref{t-SNE}. Each number $k$ in Fig.~\ref{t-SNE} denotes one utterance from the $k$-th speaker. The clusters become more separated between different speaker classes in the proposed method. For instance, features from speaker 6, 22 and 16 looks a little bit more separated in Fig.~\subref*{fig1:b}, so do cluster 21 and 23.
Cluster 20 and 1, 3 show more obviously the advantage of introducing contrastive learning to the FHVAE framework.


\noindent  \textbf{Segmental-level feature:} The results  in Table~\ref{tab:latent variable} show that the introduced contrastive learning not only improves sequential latent variable extraction, but also slightly improves segment latent variable extraction, indicating better disentanglement.

\noindent  \textbf{Voice conversion:} Results shown in Table~\ref{tab:voice conversion} indicate that speech converted by the proposed method exhibits slight improvement on speech recognition. Moreover, a bit improvement on male-to-male and gender-independent conversion, but not on female-to-female conversion is also observed.




\section{Conclusion}
\label{sec:conclusion}
As the constraint of sequential latent variable is relatively weak in \acrshort{fhvae}, this work introduces contrastive learning to improve it. 
The proposed method not only considers the distance of sequential variables within one utterance, but also among utterances. 
No more layers have been added or changed in the framework, but only the learning strategy changed. 
Experiment results show that the proposed method improves both sequential latent variable and segmental latent variable extraction performance; meanwhile in voice conversion application, it shows slight improvement in speech recognition results and 'male-to-male' and gender-independent speaker verification experiments.





%\end{document}



\bibliographystyle{IEEEbib}
\bibliography{refs}

\end{document}
