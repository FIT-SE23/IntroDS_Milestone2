\section{INTRODUCTION}

\begin{figure*}[t]
	\centering
	\includegraphics[width=1.0\textwidth]{Figure/architecture.pdf}
	\caption{The framework of the \methodname for learning affective features, whose feature extraction part is composed of a bi-direction module and a dynamic fusion module. Note that the forward $\vec{\mathcal{T}}_{j}$ and backward $\cev{\mathcal{T}}_{j}$ are the same structure with different inputs.}
	\label{fig:architecture}
\end{figure*}

Speech emotion recognition (SER) is to automatically recognize human emotion and affective states from speech signals, enabling machines to communicate with humans emotionally~\cite{schuller2018speech}. It becomes increasingly important with the development of the human-computer interaction technique. 

The key challenge in SER is \emph{how to model emotional representations from speech signals}. 
Traditional methods~\cite{TSP_INCA,INCA_CNN} focus on the efficient extraction of hand-crafted features, which are fed into conventional machine learning methods, such as Support Vector Machine (SVM).
More recent methods based on deep learning techniques aim to learn the class-discriminative features in an end-to-end manner, which employ various architectures such as Convolutional Neural Network (CNN)~\cite{RM_CNN, DBLP:conf/icpr/WenLZJ20/Capsule}, Recurrent Neural Network (RNN)~\cite{GRU_ser1,Dual_LSTM}, or the combination of CNN and RNN~\cite{bilstm_ser3}. 

In particular, various temporal modeling approaches, such as Long Short-Term Memory (LSTM), Gate Recurrent Unit (GRU), and Temporal Convolution Network (TCN), are widely adopted in SER, aiming to capture dynamic temporal variations of speech signals. For example, Wang~\etal~\cite{Dual_LSTM} proposed a dual-level LSTM to harness temporal information from different time-frequency resolutions. 
Zhong~\etal~\cite{IEMOCAP_GRU} used CNN with Bi-GRU and focal loss for learning integrated spatiotemporal features. 
Rajamani~\etal~\cite{GRU_ser1} presented an attention-based ReLU within GRU to capture long-range interactions among the features. 
Zhao~\etal~\cite{bilstm_ser3} leveraged fully CNN and Bi-LSTM to learn the spatiotemporal features. 
However, these methods suffer from the following drawbacks: 1) they lack sufficient capacity to capture long-range dependencies for context modeling, where the capture of the context in speech is crucial for SER since human emotions are usually highly context-dependent; and 2) they do not explore the dynamic receptive field of the model, while learning dynamic instead of maximal ones can improve model generalization ability to unknown data or corpus.


To overcome these limitations in SER, we propose a \textbf{T}emporal-aware b\textbf{I}-direction \textbf{M}ulti-scale Network, termed \textbf{\methodname}, which is a novel temporal emotional modeling approach to learn multi-scale contextual affective representations from various time scales. The contributions are three-fold. 
\emph{First}, we propose a temporal-aware block based on the Dilated Causal Convolution (DC Conv) as a core unit in \methodname. The dilated convolution can enlarge and refine the receptive field of temporal patterns. The causal convolution combined with dilated convolution can help model relax the assumption of first-order Markov property compared with RNNs~\cite{Markov}. In this way, we can incorporate an $N$-order ($N$ denotes the number of all previous frames) connection into the network to aggregate information from different temporal locations. 
\emph{Second}, we devise a novel bi-direction architecture integrating complementary information from the past and the future for modeling long-range temporal dependencies. To the best of our knowledge, \methodname is the first bi-direction temporal network by focusing on multi-scale fusion in the SER, rather than simply concatenating forward and backward hidden states. 
\emph{Third}, we design a dynamic fusion module by combining dynamic receptive fields for learning the inter-dependencies at different temporal scales, so as to improve the model generalizability. Due to the articulation speed and pause time varying significantly across speakers, the speech requires different efficient receptive fields (\ie the time scale that reflects the affective characteristics) for each low-level feature (\eg MFCC).