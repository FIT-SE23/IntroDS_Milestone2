\section{EXPERIMENTS}
\label{EXPERIMENTS}

\subsection{Experimental Setup}
\noindent\textbf{Datasets.}\quad
To demonstrate the effectiveness of the proposed \methodname, we compare \methodname with State-Of-The-Art (SOTA) methods on 6 benchmark SER corpora, including 
Chinese corpus CASIA~\cite{CASIA}, 
German corpus EMODB~\cite{EMODB}, 
Italian corpus EMOVO~\cite{EMOVO}, 
English corpora IEMOCAP~\cite{IEMOCAP}, RAVDESS~\cite{RAVDE}, and SAVEE~\cite{SAVEE}. 

\noindent\textbf{Implementation details.}\quad
In the experiments, 39-dimensional MFCCs are extracted from the Librosa toolbox~\cite{librosa}. The cross-entropy criterion is used as the objective function. Adam algorithm is adopted to optimize the model with an initial learning rate $\alpha$ = $0.001$, and a batch size of 64. To avoid over-fitting during the training phase, we implement label smoothing with factor 0.1 as a form of regularization. For the $j$-th TAB $\mathcal{T}_j$, there are 39 kernels of size 2 in Conv layers, the dropout rate is 0.1, and the dilated rate is $2^{j-1}$. To guarantee that the maximal receptive field covers the input sequences, we set the number of TAB $n$ in both directions to 10 for IEMOCAP and 8 for others. 
For fair comparisons with the SOTA approaches, we perform 10-fold cross-validation (CV) as well as previous works~\cite{IEMOCAP_CNN2,CPAC_IJCAI,IEMOCAP_DRN} in experiments. 

\noindent\textbf{Evaluation metrics.}\quad Due to the class imbalance, we use two widely-used metrics, Weighted Average Recall (WAR) (\ie accuracy) and Unweighted Average Recall (UAR), to evaluate the performance of each method. WAR uses the class probabilities to balance the recall metric of different classes while UAR treats each class equally.

\subsection{Results and Analysis}
\noindent\textbf{Comparison with SOTA methods.}\quad
Table~\ref{tab:SOTA} presents the overall results on 6 benchmark datasets, showing that our method significantly and consistently outperforms all these compared methods by a large margin. 
Remarkably, our approach gains 2.34\% and 2.61\% improvements of the average UAR and WAR scores than the second-best on each corpus.

\begin{figure}[t]
	\centering
	\includegraphics[width=1.0\linewidth]{Figure/TSNE.pdf}
	\caption{t-SNE visualizations of features learned from SOTA method GM-TCN and \methodname. The score denotes WAR.}
	\label{fig:tsne}
\end{figure}

\input{Table/Generalizability}


\noindent\textbf{Visualization of learned affective representation.}\quad
To investigate the impact of \methodname on representation learning, we visualize the representations learned by \methodname and GM-TCN~\cite{GM_TCNet} through the t-SNE technique~\cite{T-SNE} in Fig.~\ref{fig:tsne}. 
For a fair comparison, we first use the same 8:2 hold-out validation on CASIA corpus for the two methods, and visualize the representations of the same test data after an identical training phase. 
Although GM-TCN also focuses on multi-scale and temporal modeling. Fig.~\ref{fig:tsne}(a) shows heavy overlapping between \emph{Fear} and \emph{Sad} or \emph{Angry} and \emph{Surprise}.
In contrast, Fig.~\ref{fig:tsne}(b) shows that the different representations are clustered with clear classification boundaries. 
The results confirm that the \methodname provides more class-discriminative representations to support superior performance by capturing intra- and inter-dependencies at different temporal scales. 


\noindent\textbf{Domain generalization analysis.}\quad
Due to various languages and speakers, the SER corpora, although sharing the same emotion, have considerably significant domain shifts. The generalization of the model to unseen domain/corpus is critically important for SER. Inspired by the domain-adaptation study in CAAM~\cite{CPAC_IJCAI}, we likewise validate the generalizability of \methodname on the cross-corpus SER task, following the same experimental setting as CAAM except that \methodname does not have access to the target domain. Specifically, we likewise choose 5 emotional classes for a fair comparison, \ie \emph{angry}, \emph{fear}, \emph{happy}, \emph{neutral}, and \emph{sad}, shared among these 5 corpora (except for IEMOCAP, which has only 4 emotions). These 5 corpora form 20 cross-corpus combinations. And we report the average UAR and WAR, and their standard deviation from 10 random runs for each task in Table~\ref{tab:generalization}. 

The performance of TCN over different corpora is close to random guessing with odds equal to 25\%, and \methodname has a significant improvement over TCN. Surprisingly, \methodname outperforms CAAM, one latest task-specific domain-adaptation method. The results suggest that our \methodname is effective in modeling emotion with strong generalizability.

\subsection{Ablation Study}

We conduct ablation studies on all the corpus datasets, including the following variations of \methodname: \textbf{TCN}: the \methodname is replaced with TCN; \textbf{w/o BD}: the backward TABs are removed while keeping the forward TABs; \textbf{w/o MS}: the multi-scale fusion is removed and $\vct{g}_n$ is used as $\vct{g}_{\text{drf}}$ corresponding to max-scale receptive field; \textbf{w/o DF}: the average fusion is used to confirm the advantages of dynamic fusion. The results of ablation studies are  shown in Table~\ref{tab:ablation}. We have the following observations. 

\emph{First}, all components contribute positively to the overall performance. 
\emph{Second}, our method achieves 8.31\% and 8.41\% performance gains in UAR and WAR over TCN that also utilizes DC Conv. Since the inability of TCN to capture contextual multi-scale features, capturing intra- and inter-dependencies at different temporal scales is critical to SER.
\emph{Third}, when removing the backward TABs or multi-scale strategy, the results substantially drop due to the weaker capacity to model temporal dependencies and perceive the sentimental features with different scales. 
\emph{Finally}, \methodname without dynamic fusion performs worse than \methodname, which verifies the benefits of deploying dynamic fusion to adjust the model adaptively. 
\input{Table/Ablation}






