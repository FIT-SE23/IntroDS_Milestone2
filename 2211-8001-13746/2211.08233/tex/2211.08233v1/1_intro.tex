\section{INTRODUCTION}

\begin{figure*}[t]
	\centering
	\includegraphics[width=1.0\textwidth]{Figure/architecture.pdf}
	\caption{The framework of the proposed \methodname.}
	\label{fig:architecture}
\end{figure*}

Speech emotion recognition (SER) is to automatically recognize human emotion and affective states from speech signals, enabling machines to communicate with humans emotionally~\cite{schuller2018speech}. It becomes increasingly important with the development of the human-computer interaction technique. 

The key challenge in SER is \emph{how to model emotional representations from speech signals}. 
Traditional methods~\cite{TSP_INCA,INCA_CNN} focus on the efficient extraction of hand-crafted features, which are fed into conventional machine learning methods, such as Support Vector Machine (SVM).
More recent methods based on deep learning techniques aim to learn the class-discriminative representations of speech in an end-to-end manner, which employ various deep learning architectures such as Convolutional Neural Network (CNN)~\cite{RM_CNN,SER_CNN_intro}, Recurrent Neural Network (RNN)~\cite{GRU_ser1,Dual_LSTM}, or the combination of CNN and RNN~\cite{bilstm_ser3}.

Various temporal emotional modeling approaches, such as Long Short-Term Memory (LSTM), Gate Recurrent Unit (GRU), and Temporal Convolution Network (TCN), are widely adopted in SER, aiming to capture dynamic temporal variations of speech signals. 
For example, Wang~\etal~\cite{Dual_LSTM} proposed a dual-level LSTM to harness temporal information from different time-frequency resolutions. 
Zhong~\etal~\cite{IEMOCAP_GRU} used CNN with bi-GRU and focal loss for learning integrated spatiotemporal representations. 
Rajamani~\etal~\cite{GRU_ser1} presented an attention-based ReLU within GRU to capture long-range interactions among the features. 
Zhao~\etal~\cite{bilstm_ser3} leveraged fully CNN and bi-direction LSTM to learn the spatiotemporal representations. However, all of these methods suffer from the following drawbacks: 1) they still lack sufficient capacity to capture long-range dependencies for long-range context modeling; and 2) they are severely affected by the different speaker's articulation speed and pause time, since they can only perceive the fixed temporal scale or receptive field from low-level features~\cite{draw_back2}.

To overcome these limitations in SER, we propose a \textbf{T}emporal-aware b\textbf{I}-direction \textbf{M}ulti-scale Network, termed \textbf{\methodname}, which is a novel temporal emotional modeling approach to learn multi-scale contextual affective representations from various time scales. The contributions are three-fold. \emph{First}, we propose a temporal-aware block based on the Dilated Causal Convolution (DC Conv) as a core unit of the \methodname. The dilated convolution can enlarge and refine the receptive field of temporal patterns. The causal convolution combined with dilated convolution can help us relax the assumption of first-order Markov property compared with RNNs~\cite{Markov}. In this way, we can incorporate an $N$-order ($N$ denotes the number of all previous frames) connection into the network to aggregate information from different temporal locations. \emph{Second}, we devise a novel bi-direction architecture integrating complementary information from the past and the future for modeling long-range temporal dependencies, inspired by the fact that contextual information greatly influences the emotion perception ability in human beings. \emph{Third}, we design a dynamic fusion module based on the multi-scale features to dynamically process speech signals at different scales, since the pronunciation habits (\eg speed or pause time) vary from speaker to speaker, making the utterances to exhibit the characteristics of temporal scale variation.
