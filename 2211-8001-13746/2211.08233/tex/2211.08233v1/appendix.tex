
\section*{Appendices}
\setcounter{equation}{0}
\setcounter{subsection}{0}
\setcounter{figure}{0}
\setcounter{table}{0}
\setcounter{section}{0}
\renewcommand{\thetable}{A\arabic{table}}
\renewcommand{\thefigure}{A\arabic{figure}}
\renewcommand{\theequation}{A\arabic{equation}}
\renewcommand\thesection{\Alph{section}}

\begin{table*}[htbp]
\centering
\caption{The detailed information of speech emotion datasets. The speakers are presented in the form of [No. of males / No. of females]. The unit for sampling frequency is KHz.}
\begin{tabular}{rrcrlc}
\toprule[1.5pt]
Dataset & Language & Speakers & No. & Emotion [No. : Category List] & Freq.\\
\midrule
CASIA~\cite{CASIA} & Chinese & 2/2 & 1,200 & 6: Angry, Fear, Happy, Neutral, Sad, Surprise & 22.1\\
EMODB~\cite{EMODB} & German & 5/5 & 535 & 7: Angry, Boredom, Disgust, Fear, Happy,  Neutral, Sad & 16.0\\
EMOVO~\cite{EMOVO} & Italian & 3/3  & 588 & 7: Angry, Disgust, Fear, Happy,  Neutral, Sad, Surprise & 48.0 \\
IEMOCAP~\cite{IEMOCAP} & English & 5/5 & 5,531 & 4: Angry, Happy, Neutral, Sad & 48.0 \\
RAVDESS~\cite{RAVDE} & English & 12/12 & 1,440 & 8: Angry, Calm, Disgust, Fear, Happy, Neutral, Sad, Surprise & 48.0 \\
SAVEE~\cite{SAVEE} & English & 4/0 & 480 & 7: Angry, Disgust, Fear, Happy,  Neutral, Sad, Surprise & 44.1 \\
\bottomrule[1.5pt]
\end{tabular}
\label{tab:dataset_statistics}
\end{table*}
\begin{table*}[htbp]
\centering
\caption{The details of data distributions in five datasets.}
% \footnotesize
% \setlength{\tabcolsep}{4.37mm}
% \renewcommand{\arraystretch}{1.0}
\begin{tabular*}{1\linewidth}{@{\extracolsep{\fill}}r*9{c}}
\toprule[1.5pt]
Dataset & Angry & Boredom & Calm & Disgust & Fear & Happy & Neutral & Sad & Surprise\\
\midrule
CASIA~\cite{CASIA} & 200 & - & - & - & 200 & 200 & 200 & 200 & 200\\

EMODB~\cite{EMODB} & 127 & 81 & - & 46 & 69 & 71 & 79 & 62 & -\\

EMOVO~\cite{EMOVO} & 84 & - & - & 84 & 84 & 84 & 84 & 84 & 84\\

IEMOCAP~\cite{IEMOCAP} & 1103 & - & - & - & - & 1636 & 1708 & 1084 & -\\

RAVDESS~\cite{RAVDE} & 192 & - & 192 & 192 & 192 & 192 & 96 & 192 & 192\\

SAVEE~\cite{SAVEE} & 60 & - & - & 60 & 60 & 60 & 120 & 60 & 60\\
\bottomrule[1.5pt]
\end{tabular*}
\label{tab:data_distribution}
\end{table*}

\section{Details about Datasets and Evaluation}\label{sec:details}

\subsection{Datasets}
To sufficiently compare the performance of our methods with State-Of-The-Art (SOTA) methods, we conduct our experiments on the 6 benchmark SER corpora, including Chinese corpus CASIA, German corpus EMODB, Italian corpus EMOVO, English corpora IEMOCAP, RAVDESS, and SAVEE, as follows.

\begin{enumerate}
\item \textbf{CASIA}~\cite{CASIA} is collected from four Chinese native speakers (2 males and 2 females)  in 6 emotions, \ie \textit{angry}, \textit{fear}, \textit{happy}, \textit{neutral}, \textit{sad} and \textit{surprise};
\item \textbf{EMODB}~\cite{EMODB} contains 10 sentences that cover 7 emotions (without \textit{surprise} but adds \textit{boredom} and \textit{disgust} compared with CASIA) from daily communication by 10 German speakers (5 males and 5 females). They could be interpreted in all emotional contexts without semantic inconsistency; 
\item \textbf{EMOVO}~\cite{EMOVO} is recorded by 6 Italian speakers (3 males and 3 females) who played 14 sentences simulating 7 emotional (adds \textit{disgust} compared with CASIA) states;  
\item \textbf{IEMOCAP}~\cite{IEMOCAP} comprises 12 hours of emotional speech performed by 10 American speakers (5 males and 5 females). We select 4 emotions \textit{angry}, \textit{happy}, \textit{neutral} and \textit{sad} and full type of dataset as same as previous studies;
\item \textbf{RAVDESS}~\cite{RAVDE} consists of 1440 utterances of 8 emotions (adds \textit{disgust} compared with EMOVO)  by 24 British speakers (12 males and 12 females); 
\item \textbf{SAVEE}~\cite{SAVEE} contains 480 British English utterances by 4 British speakers (4 males) in 7 emotions (the same as EMOVO).
\end{enumerate}

The detailed information and sample statistics of each dataset are summarized in Tables~\ref{tab:dataset_statistics} and~\ref{tab:data_distribution}, respectively.


\subsection{Evaluation Metrics}
Due to the class imbalance, we use two widely-used metrics, Weighted Average Recall (WAR) (\ie accuracy) and Unweighted Average Recall (UAR), to evaluate the performance of the single- and cross-corpus SER tasks. WAR uses the class probabilities to balance the recall metric of different classes while UAR treats each class equally. The two metrics are formally defined as follows:
\begin{align}
    \text{WAR}&=\sum_{k=1}^{K} \frac{M_k}{N}\times \frac{\sum_{i=1}^{M_k} \text{TP}_{ki}}{\sum_{i=1}^{M_k} (\text{TP}_{ki}+ \text{FN}_{ki})}, \label{equ:WAR}
    \end{align}
\begin{align}
    \text{UAR}&=\frac{1}{K} \sum_{k=1}^{K} \frac{\sum_{i=1}^{M_k} \text{TP}_{ki}}{\sum_{i=1}^{M_k} (\text{TP}_{ki}+\text{FN}_{ki})}, \label{equ:UAR} 
\end{align}    
where $K$, $M_k$, and $N$ represent the number of classes, the number of speeches in  class $k$, and the number of all speeches in the dataset, respectively. $\text{TP}_{ki}, \text{TN}_{ki}$, and $\text{FN}_{ki}$ represent the true positive, true negative, and false negative values of class $k$ for speeches $i$, respectively.


\section{Detailed Results in the Single-Corpus SER Tasks}\label{sec:res_single}
\subsection{Confusion Matrices}
Due to space limits, the confusion matrices of proposed \methodname on a single corpus is not given in the main paper. The confusion matrices for the highest WAR obtained by \methodname on five datasets are shown from Fig.~\ref{fig:C&B} to Fig.~\ref{fig:R&S}. It can be observed that the \methodname achieves the excellent results with strong discriminability for almost all emotions.
\input{Figure/confusion}




\subsection{Ablation Studies}

We conduct ablation studies on all the corpus datasets, including the following variations of \methodname: \textbf{TCN}: the \methodname is replaced with TCN; \textbf{w/o BD}: the backward TABs are removed while keeping the forward TABs; \textbf{w/o MS}: the multi-scale fusion is removed and $\vct{g}_n$ is used as $\vct{g}_{\text{drf}}$ corresponding to max-scale receptive field; \textbf{w/o DF}: the average fusion is used to confirm the advantages of dynamic fusion. The detailed results over these ablation studies are shown in Table~\ref{tab:ablation}. The results show that our method outperforms the ablation methods in most cross-corpus cases, demonstrating that learning emotion-rich representation and corpus-invariant temporal patterns, and enhancing the discriminability of representation on the target corpus are beneficial.

\section{Detailed Results in the Cross-Corpus SER Tasks}\label{sec:cross-corpus}

For the cross-corpus task, the source and target corpora have a considerably significant domain shift since these five datasets contain four languages and various speakers. We follow the same experimental setting as CAAM except that \methodname does not have access to the target domain. Specifically, we likewise choose 5 emotional classes for a fair comparison, \ie \emph{angry}, \emph{fear}, \emph{happy}, \emph{neutral}, and \emph{sad}, shared among these 5 corpora (except for IEMOCAP, which has only 4 emotions). These 5 corpora form 20 combinations of cross-corpus; \eg C$\rightarrow$B indicates the model is trained with labeled data from dataset CASIA and unlabeled data from dataset EMODB, then tested on dataset EMODB.

In the main paper, Table 2 presents the average performance of 10 runs, each of which consists of 20 cross-corpus cases. We likewise show the best results of these 20 cross-corpus cases of each method in the Table~\ref{tab:cross_domain_SOTA}. The results show that our method achieves UAR of 37.4\% and WAR of 38.6\% on average, which are considerably better than the CAAM, which is the task-specific domain adaptation method for the cross-corpus SER tasks. It suggests that our \methodname is more generalizable across different corpora.


\input{Table/Ablation_appendix}
\input{Table/Generalizability_appendix}