\section{Evaluation}
\label{sec:evaluation}
This section presents our data acquisition procedure, our prototypical model implementation, and our experiments with accompanying discussion of the results.
All experiment-related artifacts are provided in our repository noted in~\autoref{sec:introduction}.

\subsection{Data Acquisition and Preparation}
All our experiments for data acquisition are conducted in K3s Kubernetes environments.  
In general, we use a fork of the Kubestone\footnote{\url{https://github.com/xridge/kubestone}, accessed: August 2022} benchmarking operator to assess the capabilities of target infrastructures with tools like \texttt{sysbench} (cpu and memory), \texttt{fio} (disk), \texttt{ioping} (disk), \texttt{qperf} (network), and \texttt{iperf3} (network).
These six benchmark types are used for our evaluation.
Upon successful execution, the benchmarking metrics are parsed from the associated results log via regex expressions and saved to a lightweight database.
To simulate varying benchmarking results, in some experiments, we occasionally impose stress on the respective hardware resource of interest using the ChaosMesh\footnote{\url{https://github.com/chaos-mesh/chaos-mesh}, accessed: August 2022} operator for chaos orchestration. 
For each series of experiments, we ensure that the Kubernetes nodes designated for benchmarking are drained as best as possible, i.e., all irrelevant Kubernetes Pods are moved to either the master node or a dedicated \emph{support} node. 
The latter also hosts the server Pods for client-server network benchmarks. 
Lastly, we enforce that at any point in time and across all target nodes, only one network benchmarking operation is executed in parallel. 
Over the total period of the experiment execution, we monitor and collect relevant resource metrics of target nodes using Prometheus and associated tools, and store them in our database as well. 
For automation of the aforementioned steps, we implemented a Kubernetes operator using the Kopf framework\footnote{\url{https://github.com/nolar/kopf}, accessed: August 2022} and work with Ansible scripts.  
All benchmark runs are quasi-randomly scheduled over the period of the total experiment time to possibly investigate different resource states.
Further technical details can be found in our repository.

\subsection{Model Implementation}
Each of our main functions $enc$, $dec$, and $agg$ is realized via soft computing, i.e., we rely on methods based on neural networks.
For the encoder $enc$ and decoder $dec$, we follow a straightforward design that we previously motivated~\cite{ScheinertTZWAWK21,scheinert2021enel}, although for the last layer of $dec$, we use a sigmoid transformation to account for our processed benchmark metrics.
For our function $agg$ used for graph-based message propagation, we average the outputs of two different graph transformations~\cite{ShiHFZWS21,tagconv} which are preceded by a configurable dropout on the adjacency matrix.
We subsequently utilize a non-linear activation~\cite{Klambauer2017}, configurable alpha-dropout~\cite{Klambauer2017}, and a final linear transformation and activation to alter the graph output.

The input to $enc$ is normalized to the range $(0, 1)$ feature-wise, where the boundaries are determined during training and used throughout inference.
The edge attributes used within $agg$ are prepared in the same manner.
In the extracted and to-be-processed graphs, each node has three predecessor nodes with which it is directly connected and thus can learn from.
During training, the model will attempt to optimize for multiple tasks simultaneously.
The various loss terms discussed in~\autoref{sec:approach} are hereby combined in an additive manner. 
For all norm-based operations, we utilize the $p$-norm with $p=10$.

In our experiments, we obtain a trained model after a hyperparameter search. 
The search space is depicted in~\autoref{tbl:clusterspecs_hyperopt}, and we sample 100 configurations from it using Ray Tune\footnote{\url{https://docs.ray.io/en/releases-1.13.0/tune/}, accessed: August 2022} with Optuna~\cite{Akiba2019}. 
More details can be found in the aforementioned repository. 
Note that although we run this on a dedicated machine equipped with a GPU (refer to~\autoref{tbl:clusterspecs_hyperopt}) to speed up the hyperoptimization procedure, the proposed model is generally compact and can also be trained on CPU-only machines, especially in light of periodic retrainings and identified near-optimal hyperparameters. 

\begin{table}
\centering
\caption{Resource Specifications \& Model Hyperoptimization}
\begin{tabular}[t]{cp{0.67\linewidth}}
    \toprule
    \multicolumn{2}{c}{\emph{Data Acquisition}}\\
    \midrule
    Software & K3s v1.21.10+k3s1, Kube-Prometheus-Stack 34.9\\
    & Kubestone v0.5.1, ChaosMesh 2.2.0, Ansible 2.12.7\\
    \midrule
    \multicolumn{2}{c}{\emph{Model Training}}\\
    \midrule
    CPU, vCores & Intel(R) Xeon(R) Silver 4208 CPU @ 2.10GHz, 8\\
    Memory & 45 GB RAM\\
    GPU & 1 x NVIDIA Quadro RTX 5000 (16 GB memory)\\
    Software & PyTorch 1.11.0, PyTorch Lightning 1.6.4\\ 
    & PyTorch Geometric 2.0.4, Ray Tune 1.13.0\\
    & Optuna 2.10.1, PyTorch Metric Learning 1.5.2\\
    \midrule
    \multicolumn{2}{c}{\emph{General Model Configuration and Hyperparameter Search Space}}\\
    \midrule
    Configuration & \#Epochs (max. 100), Hidden Dim. (32)\\
    & Batch Size (16), Optimizer (Adam)\\
    \#Attention-Heads & \{1, 3, 5\}\\
    Use beta & \{False, True\}\\
    Feature-Dropout & \{0\%, 10\%, 20\%\}\\
    Edge-Dropout & \{0\%, 10\%, 20\%\}\\
    Use root-weight & \{False, True\}\\
    CBFL $\gamma$ & \{0.5, 1.0, 2.0, 5.0\}\\
    CBFL $\beta$ & \{0.9, 0.99, 0.999, 0.9999\}\\
    Learning rate & \{$1e^{-1}, 1e^{-2}, 1e^{-3}$\}\\
    Weight decay & \{$1e^{-2}, 1e^{-3}, 1e^{-4}$\}\\
    \bottomrule
\end{tabular}
\label{tbl:clusterspecs_hyperopt}
\end{table}


\subsection{General Investigation of Fingerprinting Results}

For the data acquisition, we deploy a K3s Kubernetes cluster of five nodes in the Google Cloud Platform (GCP), where each node is a compute instance of machine type \emph{e2-medium} in the \emph{europe-west3} region.
Since one node is used as K3s master and another as support node, three nodes are used for benchmarking. 
Each of our six benchmarks is executed 100 times on each of the three nodes, where in 20\% of all cases, we impose stress on the respective Kubernetes benchmarking Pod. 
In total, we therefore conduct 1800 benchmark executions across nodes under varying conditions, hereby sufficiently capturing the variance in benchmarking results.
We then split the gathered data into 60\% training data, 20\% validation data, and 20\% test data in a stratified manner based on node names, benchmark types, and anomalous behavior. 
In the following, we present the results of our approach on the test data.

During the first step of our approach, 153 unique performance metrics (across all benchmark types) are preprocessed, enriched, and filtered down to 54 unique performance metrics.
The model then processes the input data and outputs learned representations as well as predicted class probabilities for outlier detection and benchmark type classification.
We observe a MSE on the test data of 0.01, demonstrating that the autoencoder can effectively recover the original input from a low-dimensional learned representation with high accuracy. 
Likewise, the learned representations are apparently sufficiently separated in the feature space, as we achieve with a simple linear transformation a 100\% accuracy on the benchmark type classification task. 
This confirms that learned representations indeed retain type-specific characteristics. 
Lastly, for the task of resource degradation detection and our opportunistic implementation of this specific module, we measure an F1 score of 0.93 for the normal class and an F1 score of 0.75 for the outlier class.
Moreover, the achieved weighted accuracy is 90\%.
While these results leave room for improvement, we conclude two things: First, the amount of data available for training has been fairly limited. Second, in practice, even false positive cases are not critical, since they can be used as trigger for subsequent benchmark runs, which then can either prove the original indication wrong or solidify it. 
In the latter case, one example of a reasonable action would be, for instance, the exclusion of the particular compromised node during workload scheduling.
With each benchmark type only running for a few seconds, the overhead is negligible, especially when compared to usually long-running actual workloads.


\subsection{Use Case: Iterative Optimization of Resource Configurations for Distributed Dataflows}
To assess the applicability of our learned representations, we resort to cloud configuration profiling methods for big data analytics and investigate how they benefit from our approach.
We consider \emph{CherryPick}~\cite{AlipourfardLCVY17}, which conducts profiling for a target workload based on Bayesian Optimization, as well as \emph{Arrow}~\cite{HsuNFM18}, a method that motivates Augmented Bayesian Optimization via consideration of low-level application metrics.
For both these methods, the overall objective is to find a near-optimal configuration in terms of execution costs while obeying to runtime constraints.
The idea now is to support this process by providing generalized infrastructure information.
Since for both methods we were not able to obtain the original source code, we re-implemented them to the best of our knowledge and provided them in our repository.
Furthermore, for Arrow, we simply replace their utilized low-level metrics with our computed scores from learned representations.
For both Arrow and CherryPick, we follow a simple integration: we weight the acquisition values produced by the respective acquisition function by a sum of products, where each product is composed of a factor that reflects the resource utilization of a particular configuration, and the other factor being the corresponding computed representation-based score.

\begin{figure}
    \centering
    \includegraphics[width=\columnwidth]{graphics/usecase_dataflows1.pdf}
    \caption{Illustration of the cheapest cloud configuration found under constraints, for both our baselines and their respective upgrades using Perona-based scores.}
    \label{fig:evaluation_df1}
\end{figure}

Both methods can be evaluated with a publicly available dataset\footnote{\url{https://github.com/oxhead/scout}, accessed: August 2022}, which includes data from 18 workloads running on 69 different configurations (scaleout, VM type) on Amazon Web Services (AWS) infrastructure in a multi-node environment (one run per configuration), resulting in a total of 1242 workload executions.
We derive the cost of each run using current prices\footnote{\url{https://calculator.aws/\#/createCalculator/EC2}, Accessed: August 2022} for AWS on-demand instances in the USA East Ohio region.
Beyond that, however, we still have to benchmark the machines used in this dataset via our own approach first.
We deploy a K3s Kubernetes cluster of 11 nodes in AWS, with two nodes (K3s master and support node) of machine type \emph{m4.large} and one each of size \emph{large}, \emph{xlarge}, and \emph{2xlarge} of machine types \emph{m4}, \emph{c4}, and \emph{r4}.
Each benchmark is executed 10 times on each node without stress injection, which totals to 540 benchmark executions used for learning representations.

In ~\autoref{fig:evaluation_df1} we present the cheapest valid cloud configurations found for subsequent profiling runs utilizing both baselines, respectively with and without the Perona extension. 
Here, the results of the first runs are identical, since at least one profiling run is required for Perona. 
As indicated by the results, an extension with Perona increases the median cost-effectiveness and hence improves cloud configuration profiling, especially for consecutive profiling runs where the model is enriched with further information. 
Although Perona tends to slightly increase the overall search cost and search time, eventually a more cost-effective configuration is found which is also less prone to timeouts induced by limited resources.


\subsection{Use Case: Adaptive Resource Management for Large-Scale Scientific Workflows}
In a last series of experiments, we investigate the applicability of our approach in conjunction with methods for scientific workflows.
More precisely, we examine if our learned representations can be employed instead of the manually selected benchmark metrics used within \emph{Tarema}\cite{BaderTarema21} and \emph{Lotaru}\cite{BaderLotaru22} while achieving similar performance.
Since the source code of both methods for scientific workloads is openly available, we can easily modify it and instruct the methods to consume our representations instead of their original benchmarking results.

As before, we first need to benchmark the machines used for evaluation in the respective works via our own approach.
We deploy a K3s Kubernetes cluster of five nodes in GCP, with two nodes (K3s master and support node) of the previously described machine type \emph{e2-medium} and one each of the \emph{n1-standard-4}, \emph{n2-standard-4}, and \emph{c2-standard-4} machine type.
Each benchmark is executed 10 times on each node without stress injection, which totals to 180 benchmark executions used for learning generalized representations.

For Lotaru, a system for runtime prediction of compute tasks that internally makes use of microbenchmarks, we substituted its microbenchmark values with our scores obtained from learned representations and adjusted the estimation process to fit for our used machines.
\autoref{tab:lotaru} presents the obtained results. 
The Naive approach, Online-M, and Online-P serve as baselines in the original paper and use a method that does not rely on benchmarking.
They are outperformed significantly by all approaches employing benchmarking.
Using Lotaru with Perona's fingerprinting leads to a median error increase of 1.74\%. 
However, the values reported for the 90th-percentile and 95th-percentile prediction error are similar to the results for Lotaru, indicating a satisfactory performance.
In the case of Tarema, a system that allocates cluster resources based on the tasks' resource usages and therefore uses microbenchmarks to group nodes with similar performance metrics, we mocked Tarema's group build with our previously obtained fingerprinting values.
As a result, Tarema created the same node groups, leading to the same overall workflow makespans.

\begin{table}
\centering
\caption{Fingerprinting results}
\begin{tabular}{c
    S[table-format = 2.4] 
    S[table-format = 2.4]
    S[table-format = 2.4] 
     S[table-format = 2.4]
      S[table-format = 2.4]
  }
\hline
       & \multicolumn{1}{|l|}{Naive} & \multicolumn{1}{l|}{Online-M} & \multicolumn{1}{l|}{Online-P} & \multicolumn{1}{l|}{Lotaru} & \multicolumn{1}{l|}{Perona} \\ \hline
Median & 0.6692425400491009         & 0.19547789073450178           & 0.19547789073450178           & 0.12368778482387402         & 0.14109173314021614         \\ \hline
P90    & 13.930279653897347         & 0.5966910636589577            & 0.5968593235157074            & 0.4741153530715184          & 0.4883234928027342         \\ \hline
P95    & 47.25881553469377          & 0.8010339573976514            & 0.808789857826177             & 0.793521193342131          & 0.7960764123370796          \\ \hline
\end{tabular}
\label{tab:lotaru}
\end{table}