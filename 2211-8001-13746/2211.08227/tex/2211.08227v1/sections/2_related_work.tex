\section{Related Work}
\label{sec:related_work}
Various approaches have been proposed in the past for optimized configuration of cluster and cloud resources for big data analytics workloads, which can be classified into either 
\emph{Implicit Profiling-Based} or \emph{Explicit Benchmarking-Based} approaches, and will be presented in the following.

\subsection{Implicit Profiling-Based Approaches}

CherryPick~\cite{AlipourfardLCVY17} is an iterative search-based method capable of finding near-optimal cloud configurations for workloads, without the knowledge of previous executions.
It leverages Bayesian optimization to create performance predictions for different applications. 
Arrow~\cite{HsuNFM18} has been designed to improve CherryPick.
It also performs Bayesian optimization to find a near-optimal cloud configuration, but additionally incorporates low-level performance information to enrich the input data for the  optimization process.
Vanir~\cite{bilal2020finding} initially finds a suitable cloud configuration by analyzing performance metrics of benchmarking runs. 
It further utilizes a Mondrian-forest-based performance model and transfer learning techniques to identify better configurations.
Another approach is Selecta~\cite{klimovic2018selecta}, a tool that recommends near-optimal configurations by utilizing latent factor collaborative filtering on sparse metric data obtained from profiling runs.
This allows for predicting the performance of workloads on various configurations.
Paragon~\cite{delimitrou2013paragon} is a heterogeneity and interference-aware scheduler that uses different stages of profiling to identify the best machine for a given workload.
Initially, a few workloads are run offline on all server configurations and the performance results are normalized.
During runtime, a newly incoming workload is profiled for a minute on two of the target servers, followed by an estimation of its performance on the other servers.
Similarly, Quasar~\cite{delimitrou2014quasar} profiles incoming workload for a short period on a small number of servers.
The gathered profiling information is enriched with the information from the extensive offline profiling.
Unlike Paragon, Quasar integrates the rescaling impact into their estimations.

Summarizing, these approaches utilize profiling runs in order to optimize cloud and cluster configurations for a set of specific workloads, whereas with Perona we aim
to create a general infrastructure model that can be employed to optimize the resource configuration for arbitrary workloads.
\subsection{Explicit Benchmarking-Based Approaches}

Rupam~\cite{xu2018heterogeneity} is a heterogeneity-aware task scheduler for Apache Spark.
For hardware profiling, Rupam uses sysbench to measure CPU and I/O performance and Iperf for the network speed of a certain machine.
Further, it uses real-time resource utilization metrics to mitigate resource contention during execution.
For scheduling, Rupam uses a self-adaptable heuristic that considers the measured task resource usage and profiled machine attributes when assigning tasks to machines. 
With~\cite{zhang2013benchmarking}, a benchmark-based approach for performance modeling of MapReduce jobs is presented that separates the job tasks into six phases.
Then, a set of microbenchmarks is defined for representing the execution time of the phases, and executed on a test cluster to gather training data for the performance model.
This model only needs to be created once and can later be reused for different applications.

Further, in our recent work, we also used explicit benchmarking-based approaches to optimize resource management.
Tarema~\cite{BaderTarema21} is a system that dynamically allocates tasks to heterogeneous cluster nodes based on a ranking of resources.
Internally, it makes use of hardware profiling with a set of microbenchmarks, utilizes additional static hardware information, and computes group nodes with similar performance characteristics based on these profiled metrics.
Reshi~\cite{BaderReshi22} trains a recommender system that uses task resource traces and profiled infrastructure metrics as an input vector.
Several CPU, memory, and disk I/O benchmarks are executed on every node in the cluster, so that the recommender system is able to learn a model that can prioritize task-node pairs and execute the scheduling without knowing the actual task runtimes.
Lastly, Lotaru~\cite{BaderLotaru22} is a method that predicts task runtimes for heterogeneous clusters via locally obtained task profiles and without relying on the availability of any historical traces.
Again, Lotaru applies explicit benchmarking of the local and the target infrastructure to set an adjustment factor for predicting workload performance on the target infrastructure.

In contrast to Perona, the presented systems mainly apply an initial benchmarking phase, whereas we additionally compare and rank the learned representations of resource configurations while also enabling self-adaptive capabilities.