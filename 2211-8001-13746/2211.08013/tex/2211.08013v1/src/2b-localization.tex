We are primarily interested in operations in indoor GPS denied environments. Our drone is equipped with an IMU and a front facing camera, which is used to localize the drone based on markers placed in known locations in the environment.
Both markers and natural features are detected using computer vision algorithms (\cite{garrido2014automatic, Koray2009Monocular}) which also have been demonstrated by Tinamu Labs (\cite{jiang2022online}).
%\stef{I think you are giving us too much credit here . This is the right paper to cite: [1] It's a general method for determining the camera pose using set of pre-positioned markers. Furthermore, you can cite any of the slam approaches used on the drone. You can simply say that the marker based approach can be extended to natural features. This is for example visual localization approach (not SLAM): [2]. This is one of early SLAM for drone papers (you can probably find better thoug): [3]  }
%
We require that enough previously mapped features are visible at any given point of the flight to assure that the state can always be determined, independently from other sensory information.
\begin{figure}
\begin{center}
	\includegraphics[width=.6\columnwidth]{img/camera-CROP-P}
	\caption{An example of the image in the camera plane, showing the detected points $\overline{P_{im}}$ which are used to infer localization.	%\td{Add colorbar scale?}
	}
\label{fig:camera}
\end{center}
\end{figure}
Let $\overline{P_{im}} = \{{P_{im}}_k\}_{k=1}^N \subset \Reals^2$ be the set of $N$ features detected and identified in the image plane of the onboard camera, as exemplified in Figure \ref{fig:camera}.
%when the drone has an unknown state $x$.
We can compute the coordinates in the image frame $P_{im}$ from the global frame $P_g$ as a function of the position and orientation of the drone $\state=\{{\state_\mathrm{full}}_i\}_{i=1}^6$
\begin{equation}
	P_{im}(\state) = C_{im}^c T_c^g(\state) P_g,
\end{equation}
where $T_c^g(\state)$ is the geometric transform from the global to the camera frame, and $C_{im}^c$ the transform from camera frame to image plane given by the camera model.
We can solve a nonlinear least squares problem to find an estimate of $\state$
\begin{equation}\label{nlleastsquares}
	\hat \state = \underset{\state}{\arg \min} \sum_{k=1}^N \|P_{im}(\state) - \overline{P_{im}}\|_2^2.
\end{equation}

Linearizing the nonlinear least squares problem around the solution, %assuming exogeneity and a normal
for normally distributed measurement errors, the estimate $\hat \state$ is also normally distributed with variance 
%\begin{equation}
$	\mathbb{V}(\hat \state) = \sigma^2 (J\tp J)\inv, $
%\end{equation}
where $\sigma$ is the standard deviation of a measurement in the image plane, and $J = \nabla P_{im}(\state)|_{\state=\hat \state}$ is the Jacobian.
%
%\subsection{Quality of fix}
Assuming $\hat \state \approx \state$, we can compute the variance of the state estimate $\Sigma^\state$ as a function of the drone coordinates
\begin{equation}
	\Sigma^\state(\state) = \mathbb{V}(\hat \state)|_{\hat \state = \state}.% : \Reals^6 \rightarrow \Reals^{6\times6}.
\end{equation}
%Note that during flight the drone can use its inertial measurement unit 

Let $q_{pos}(\Sigma^\state(\state)) = 1/\sqrt{\Tr(\Sigma^\state)}$ % : \Reals^{6\times6} \rightarrow \Reals$
be a function mapping the covariance matrix to a scalar \emph{quality of position fix} metric. This metric is used to define the 
%\ar{Some intro or explanatio about this metric is needed: eg we explain that we create a feature translating the uncertainty of position estimate in each "region of interest" or "grid cell" or point.}. %, for example $q_{pos}(\Sigma^x(x)) = \frac{1}{\sqrt{\Tr(\Sigma^x)}}$.
%
\if\draft \subsection{State space constraints} \fi
feasible flying domain $\PlanningSpace$ as
\begin{equation}
	\PlanningSpace = \{\state \in \ConfigurationSpace : q_\mathrm{pos}(\Sigma^\state(\state)) > \tau ~|~ \World \}
\end{equation}
where $\state$ is the drone state,
$\ConfigurationSpace$ is the configuration space,
$\tau$ is the minimum quality of fix threshold,
and $\World$ the available information about the visual features positions.

The IMU and position measurements derived from the vision system are fused using an extended Kalman filter \cite{ekf} to produce an estimate of the full state of the drone.
The full estimate is used by the inner loop controller $g$ while the planning algorithm only use the position and yaw localization.
