%\begin{figure}
%    \begin{center}
%		%\importsvg[.5\columnwidth]{img}{Scheme-1}
%    \end{center}
%    \caption{\td{Pictorial representation of the problem} \label{fig:scheme-1}}
%\end{figure}

%A quadcopter flying above a pile of bulk material, for example sand, is equipped with a 2D LiDAR sensor that measures the distance to objects in the region of interest, as ilustrated in Figure \ref{fig:scheme-1}. The quadcopter flies in an indoor GPS-deprived environment, infering its position and orientation from features of the environment.
%The collected LiDAR distance measurements are used to compute the volume of material in the region of interest.
We consider the problem of estimating the volume of a pile of bulk material inside a region of interest using a quadrotor-based mobile sensor.
Let $h(x) = h(x_1,x_2)$ be the true surface function of the height of the pile, defined in the domain of interest $\mathcal{D} = [{x_1^-}, {x_1^+}]\times[{x_2^-},{x_2^+}]$.
The volume of the pile is 
\begin{equation}
	\Volume = \iint_\mathcal{D} h(x) dx,
\end{equation}
%The volume estimation performed by an autonomous quadroptor indoors can be described as follows.
%First, we define the dynamics of the quadcopter:
and the dynamics of the quadcopter are given by
\begin{subequations}
	\begin{align}
		\dot{\state}_\mathrm{full} (t) = &~ f(\fullstate(t),u(t)),\\
		u(t) = &~g(r(t), \hat \state_\mathrm{full}(t))
	\end{align}
\end{subequations}
where $\fullstate(t) = (p,\theta, v, \omega, b):[0,T] \rightarrow \Reals^{13}$ is the drone state, consisting of its position $p$, orientation $\theta$, velocity $v$ and angular velocity $\omega$, as well as the battery state of charge $b$,
$u(t):[0,T] \rightarrow \Reals^{4}$ are the rotor voltages,
$g$ is a feedback controller that stabilizes the system to a commanded position and yaw setpoint $r$, and
$\hat \state_\mathrm{full}(t)$ is an estimate of the current state of the drone.
In this work we assume that the low-level controller can follow the reference closely, such that ${\fullstate}_i(t) \approx r_i(t), i\in\{1,\dots,4\}$ if the time derivatives of $r(t)$ up to order four (velocity, acceleration, jerk and snap) are within specified bounds, derived from actuator limits $\mathcal{U}$ \cite{drone-model}.
%\stef{ Do we really need $\fullstate$? We only mention it here, and afterwards we only use $\state$ which only contains position and orientation.  Also should it be 12-dimensional: x, y, z, roll, pitch, yaw + 6 derivatives of each or I am missing something. }
\begin{figure}
\begin{center}
	\includegraphics[width=\columnwidth]{img/Alps.png}
	%\importsvg[\columnwidth]{img}{Alps}
	\caption{A sample of the scaled topographic data used in simulation. The color circles on the back plane are previously mapped visual features used for localization. The drone represented is not to scale.
	%\td{Add colorbar scale?}
	}
\label{fig:alps}
\end{center}
\end{figure}

\if\draft \section{LiDAR model} \fi
The quadcopter is equipped with a 2D LiDAR, measuring the radial distance $d_l$ from the sensor to the surface of the pile, that is used to estimate the volume of the pile based on a reconstruction of the surface. 
%A typical value for $\omega$ is $20\pi$.
%A typical value for $f_s$ is $9200\,\mathrm{Hz}$
%
%\if\draft \section{Field of view (constraints)} \fi
%Furthermore, the quadcopter is constrained to move only where the environment can be identified and mapped. For this, an onboard front-facing camera captures images of the environment. These are processed to extract identifiable, previously mapped features, which are used for localization. We require that enough features are visible at any given point of the flight to assure that the state can always be determined, independently from other sensory information. Apart from the LiDAR and front-facing camera, the quadcopter is equipped with an inertial measurement unit (IMU), which is used by the low-level controller to stabilize the aircraft during flight, combined with the visual information to improve the localization.
%
%\if\draft \section{True surface} \fi
%
%To estimate the volume of the surface over which the quadcopter is flying, we need to know its 3D-coordinates at each point, or an approximation of them, taking into account relevant features (e.g. texture, surface derivatives...). We take as an example a pile situated in the region of interest of the quadcopter.
The drone is also equipped with a camera and computer vision algorithms that allow it to detect and identify features placed in the environment at known locations. These features are used to localize the drone which must keep a minimum number of features within its field of view at all times.

\if\draft \section{Define objective} \fi
We have now all the ingredients needed to formulate our volume estimation problem. We aim to find a reference trajectory $r(t):[0,T]\rightarrow\PlanningSpace \subseteq \Reals^3\times[0,2\pi[$,
where $T$ is the trajectory time and
$\PlanningSpace$ is the set of 3D positions and yaw where the drone is allowed to fly and is able to detect enough features to localize itself,
that minimizes the volume estimate uncertainty,

\begin{mini!}[2]
	{r(t)}{ \mathbb{V}(\Volume(T))}{\label{main-optimization-problem}}{}
	\addConstraint{ \dot{\state}_\mathrm{full}(t) = }{f(\fullstate(t),u(t))}{}
	\addConstraint{ u(t) = }{g(r(t),\hat \state_\mathrm{full}(t))}{}
	\addConstraint{ \fullstate(t) \in}{\PlanningSpace}{}
	\addConstraint{ u(t) \in}{\mathcal{U}}{},
	\end{mini!}
where $\mathbb{V}$ is the variance operator. This is a hard problem as the uncertainty in the volume estimate is a consequence of uncertainty in the LiDAR measurements which in turn depends on the uncertainty in the camera-based localization system. In the subsequent sections we derive detailed measurement models for the localization system (Section \ref{sec:localization}) and the LIDAR, propose a method for fusing them to estimate the volume (Section \ref{sec:approach}) and propose a greedy algorithm to approximate a solution of \eqref{main-optimization-problem} in Section \ref{sec:planning}.
