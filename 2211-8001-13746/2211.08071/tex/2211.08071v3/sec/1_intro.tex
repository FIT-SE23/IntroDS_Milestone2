\vspace{-12pt}
\section{Introduction}
\label{sec:intro}
%Object detection, as a fundamental task in computer vision, has long been dominated by detectors based on convolutional neural networks (CNNs)\cite{ghiasi2021simple}\cite{dai2021dynamic}\cite{zhou2021probabilistic}. 
 \begin{figure}[t]
	\centering
	\subfloat[Distillation Points in Classic Detector]{\includegraphics[width=0.7\columnwidth]{classic-eps-converted-to.pdf}\label{classic}}\\
	\subfloat[Distillation Points in DETR]{\includegraphics[width=0.7\columnwidth]{detr-eps-converted-to.pdf}\label{detr}}\\
	\subfloat[Distillation Points in KD-DETR]{\includegraphics[width=0.7\columnwidth]{kddetr-eps-converted-to.pdf}\label{kddetr}}
	\caption{\textbf{Schematic Illustrations of Distillation Points in Different Architecture}: (a) Two-Stage Detector: both positive and negetive proposals in RPN and RoI are consistent distillation points with strict one-to-one correspondence between teacher and student model; (b) DETR: Object queries lacks spatial or semantic relationship between teacher and student model, resulting in inconsistent distillation points; (c)In KD-DETR, a set of special object queries is introduced to construct consistent distillation points for DETR distillation}
	\vspace{-12pt}
\end{figure}

In recent years, \cite{carion2020end} propose the novel end-to-end detector Detection Transformer (DETR) which eliminates the need for hand-crafted anchors and non-maximum suppression (NMS). \cite{zhu2020deformable}\cite{li2022dn}\cite{liu2021dab}\cite{zhang2022dino}further make remarkable stride towards the scalability and potential of DETR, significantly outperforming classical detectors.

Different from classic detectors, DETR interprets object detection as an end-to-end set prediction problem with bipartite matching. A set of learnable object queries are introduced, each responsible for a certain instance. The object queries interact with features extracted from the encoder to make final predictions of box locations and categories. Despite the impressive performance, the growing model scale prevents DETR from being deployed to real-world applications with urgent computation budget requirement.

To address this problem, current works have made efforts in designing efficient DETR architectures, reducing the encoder tokens utilized in cross-attention module to decrease the computation cost\cite{roh2021sparse}, and leveraging dense prior from RPN to downsize the decoder layers \cite{yao2021efficient}. In this work, we concentrate on compressing the large-scale DETR model by knowledge distillation\cite{hinton2015distilling} approaches, which is a promising technique for model compression and accuracy boosting. Knowledge distillation can transfer the knowledge learned from large and cumbersome DETR models to small and efficient ones by forcing the student to mimic the predictions from teacher, whether logits or internal activations. However, modern knowledge distillation methods are designed under CNN-based detectors, and researches on expanding it to general DETR compression are limited. 

 We start from experiments of applying classic logit-based distillation\cite{chen2017learning} to DETR to investigate the key point in DETR distillation. With these experiments, we observe that the critical challenge lies in the different formulations of DETR and classic detectors. Compared with classic detectors, the set-prediction formulation of DETR naturally contains fewer consistent distillation points. We use distillation points to denote the corresponding input of the prediction for mimicking in knowledge distillation, and the sufficiency and consistency of distillation points form the foundation of knowledge distillation. Specifically, abundant distillation points which are kept consistent between teacher and student models are essential for effective distillation. As shown in Figure \ref{classic}, classic detectors make predictions for a set of region proposals generated from the sliding window locations on the images with handcraft scales. This pattern naturally ensures a strict spatial correspondence between the large number of proposals made by teacher and student models, even for those negative ones with low confidence, thus providing a sufficient number of consistent distillation points for mimicking. In DETR, as is shown in Figure \ref{detr}, the distillation points actually consist of both image and object queries. However, the object queries from teacher and student models are egocentric and even differ in number, thus lacking definite correspondence, especially those redundant negative queries in bipartite matching. As the distillation points in DETR are inconsistent and insufficient, the predictions acquired from teacher are not reliable or informative for student to mimic.

The observation above raises the issue: how to obtain sufficient and consistent distillation points for DETR distillation. Previous work\cite{chang2023detrdistill} explicitly alleviate this issue by utilizing the bipartite matching between the object queries from teacher and student. However, the bipartite matching is not stable\cite{li2022dn}, and the matched object queries are just similar but not consistent, lacking sufficiency and extensibility. To directly address this issue, we propose a general knowledge distillation paradigm for DETR (KD-DETR) with consistent distillation points sampling. In KD-DETR, as illustrated in Figure \ref{kddetr}, we decouple detection and distillation task by introducing a set of specialized object queries to construct distillation points. The distillation points are unlearnable and shared between teacher and student models, probing the “dark knowledge” in teacher model. In this way, consistent distillation points with customized quantities become available. With the paradigm of KD-DETR, we propose a general-to-specific distillation points sampling strategy to probe comprehensive knowledge in teacher model. We further propose a coordination-based distillation points sampling strategy to extend KD-DETR to heterogeneous distillation between DETR and CNN detector.

% For general sampling, we randomly sample a set of object queries which are uniformly distributed to coarsely scan the whole feature maps and probe more general knowledge from teacher model. For specific sampling, the well-optimized object queries in teacher model are used to construct distillation points, as they represent the specific knowledge the teacher model concerns. Considering the imbalance of positive and negative regions in object detection, we further reweigh the importance of each distillation point with its foreground probability predicted by the teacher model. 

%\begin{figure}[t]
  %\centering
  %\fbox{\rule{0pt}{2in} \rule{0.9\linewidth}{0pt}}
 %  \includegraphics[width=1\linewidth]{kddetr.eps}
 %  \caption{\textbf{Distillation Points in KD-DETR} In KD-DETR, a set of special object queries is introduced to construct consistent distillation points for DETR distillation}
 %  \label{kddetr}
%\end{figure}
To the best of our knowledge, this is the first work to propose a general knowledge distillation paradigm for DETR for both homogeneous and heterogeneous distillation. In this paper, we first provide a thorough analysis of the key points in DETR distillation. Based on the analysis, we design a novel KD-DETR which significantly improves the performance of DETR distillation. KD-DETR has both flexibility to different DETR architectures and potential for scalability, even for  heterogeneous distillation between DETR and CNN detectors. We conduct extensive experiments on the MS COCO2017\cite{lin2014microsoft} dataset on both homogeneous and heterogeneous distillation, and significantly boosts the performance of student models. DAB-DETR with ResNet-18 and ResNet-50 backbone achieves 41.4$\%$, 45.7$\%$ mAP, respectively, which are 5.2$\%$, 3.6$\%$ higher than the baseline. Deformable DETR with ResNet-18 and ResNet-50 reaches 43.7$\%$ and 48.3$\%$ mAP, 3.6$\%$ and 3.8$\%$ higher than the baseline, and outperform DETRDistill\cite{chang2023detrdistill} with 1.7$\%$ improvement. DINO with ResNet-18 and ResNet-50 also gains 4.4$\%$ and 2.6$\%$ improvement than baseline. We further extend KD-DETR to heterogeneous distillation, and achieves 2.1\% improvement by distilling the knowledge from DINO to Faster R-CNN with ResNet-50, which is comparable with homogeneous distillation methods.
