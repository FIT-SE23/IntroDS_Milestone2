\section{Conclusion}

In this paper, we study the compression of DETR with knowledge distillation. We provide thorough experimental and theoretical analysis on the key point in DETR distillation. Based on the analysis, we propose the first general knowledge distillation paradigm for DETR (KD-DETR), together with a comprehensive general-to-specific consistent distillation sampling scheme. We conduct extensive experiments to demonstrate the flexibility, generalization, and extensibility of KD-DETR on various DETR architectures, and for both homogeneous and heterogeneous distillation. For homogeneous distillation, KD-DETR compresses both the scale of backbone and transformer layers, significantly boosting the performance of student model. For heterogeneous distillation, KD-DETR effectively transfers the knowledge from DETR to CNN detector.
