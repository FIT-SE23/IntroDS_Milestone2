\section{Related Work}
\subsection{Classic Object Detection}

Classic detectors with CNN view object detection as a verification task with a sliding window on the image to generate anchors. The mainstream detectors can be divided into one-stage detectors\cite{tian2019fcos}\cite{li2020generalized} and two-stage detectors\cite{ren2015faster}\cite{cai2019cascade}. One-stage detectors, such as Retinanet\cite{lin2017focal}, YOLO\cite{redmon2016you}, and FCOS\cite{tian2019fcos}, directly predict the category and regression of anchors on each pixel of the feature maps. While two-stage detectors such as Faster-RCNN\cite{ren2015faster} and its variants\cite{pang2019libra}\cite{yan2019meta}\cite{zhang2019cad} introduce a Region Proposal Networks (RPN) to generate proposals, and a ROIPool or ROIAlign\cite{wang2019distilling} to extract features of each region proposal for further classification and regression refinement. Both one-stage and two-stage detectors require post-processing, such as NMS, to remove duplicate predictions.

\subsection{Detection Transformer}

\cite{carion2020end} first propose an end-to-end transformer-based detector without any post-processing. Different from classic object detection, DETR interprets object detection as a set-prediction problem with bipartite matching.  Lots of follow-up focus on the slow convergence of DETR\cite{dai2021dynamic}\cite{sun2021rethinking}\cite{gao2021fast}\cite{zhang2022accelerating}. Deformable DETR\cite{zhu2020deformable} introduces a deformable attention module by generating reference points for query elements, each of which only concentrates on a small number of locations on the whole feature map. An alternative way is to add more prior information to the object queries in the decoder. Conditional-DETR\cite{meng2021conditional} decouples the context and position features in object queries and generates position features by spatial location. DAB-DETR\cite{liu2021dab} further introduces the width and height information to the positional features. Anchor DETR\cite{wang2022anchor} also encodes the anchor points as object queries with multiple patterns, and further designs a row-column decouple attention to reduce memory cost. The recent work of DINO\cite{zhang2022dino} draws the existing novel techniques, and further exerts the potential of DETR by enlarging the scale of model and datasets.

Besides, another problem in DETR is the scale and computation cost of the model. Current works solving this problem by designing more efficient DETR architecture. Sparse DETR\cite{roh2021sparse} reduces the computation cost by sparsifying encoder tokens. Efficient DETR\cite{yao2021efficient} otherwise introduce RPN to generate object queries and eliminate the cascading decoder layers in DETR. PnP DETR\cite{wang2021pnp} shorten the length of sampled feature with a poll and pool sampling module.


\subsection{Knowledge Distillation}

Knowledge distillation is a widely-used method for model compression and accuracy boosting by transferring the knowledge in a large cumbersome teacher model to a small student. \cite{hinton2015distilling} first propose the concept of knowledge distillation, where the student mimics the soft predictions from teacher. Knowledge distillation has been utilized in various fields\cite{tian2019contrastive}\cite{yang2022focal}\cite{yang2022cross}. According to the objective of mimicking, knowledge distillation can be divided into three categories: response-based\cite{zhao2022decoupled}, feature-based\cite{heo2019comprehensive}\cite{yang2022masked} and relation-based\cite{yim2017gift}\cite{zagoruyko2016paying}, which distill with logits, intermediate activations and the relation of features in different layers respectively.

% including classification\cite{zhao2022decoupled}\cite{tian2019contrastive}\cite{beyer2022knowledge}, object detection\cite{yang2022focal}\cite{zhang2020improve}, semantic segmentation\cite{yang2022cross}\cite{liu2019structured}. 
Several works focus on applying knowledge distillation to object detection\cite{guo2021distilling}\cite{chen2017learning}\cite{kang2021instance}. \cite{chen2017learning} successfully distills the features on the neck, the classification head, and the regression head, while \cite{li2017mimicking} chooses to distill the logits and features from the RPN head. To overcome the imbalance of foreground and background, \cite{wang2019distilling} introduces fine-grained mask to focus on the regions close to ground-truth bounding boxes, \cite{dai2021general} pays more attention to the regions where teacher and student are divided in predictions. 

However, the modern knowledge distillation methods for object detection are built upon the architecture of CNN-based detectors, and are not suitable for DETR due to the completely different transformer architecture. \cite{chang2023detrdistill} directly introduces response-based and feature-based distillation to DETR with Hungarian-matching. Different from the previous work, we analyze the limitation of the set-prediction formulation in knowledge distillation, and propose a general paradigm for both homogeneous and heterogeneous DETR distillation.

%Since \cite{dosovitskiy2020image} introduce transformer to vision task, several works\cite{touvron2021training}\cite{zhang2022minivit} have verified the effectiveness of knowledge distillation on ViT. However, these methods perform trivially or even harmfully on DETR, and there is still a lack of research on DETR distillation.
%However, the modern knowledge distillation methods for object detection are built upon the architecture of CNN-based detectors, and perform trivially or even harmfully on DETR due to its completely different architecture. While some works\cite{touvron2021training}\cite{zhang2022minivit} have verified the effectiveness of knowledge distillation on ViT, 