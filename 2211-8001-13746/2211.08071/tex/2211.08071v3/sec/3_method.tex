\section{A Closer Look at DETR Distillation}

In this section, we first revisit the DETR architecture briefly. Then we conduct a series of classic knowledge distillation experiments on DETR to reveal that the core of DETR distillation is to obtain sufficient and consistent distillation points. 


\subsection{Revisiting DETR}
DETR is built upon the encoder-decoder architecture of transformer. The encoder takes pixels of the feature map from backbone as input for multi-head self-attention to extract context features $X\in\mathbb{R}^{HW\times D}$, where $HW$ denotes the resolution of the feature map, and $D$ denotes the feature dimension. The decoder takes the features from encoder and a set of learnable object queries $\mathbf{Q}=\{q_i\in\mathbb{R}^D|i=1,...,N\}$ as input, where N denotes the number of queries. Each object query is an abstract feature describing a certain instance, and will probe and pool the features from encoder through cross-attention to make predictions of category $\mathbf{C}=\{\mathbf{c}_i\in\mathbb{R}^K|i=1, ..., N\}$ and location $\mathbf{B}=\{\mathbf{b}_i=(bx_i, by_i, bw_i, bh_i)|i=1, ..., N\}$, where K denotes the number of categories. Finally, the Hungarian algorithm is used to find a bipartite matching between ground truth and predictions of object queries. 
\begin{table}[t]
\begin{center}
\begin{tabular}{c|ccc}
\toprule[2pt]
Strategy & AP & $AP_{50}$ & $AP_{75}$  \\
\hline
Baseline & 36.2 &56.1 &37.9 \\
Inconsistent  &35.1 & 55.2& 36.7\\
Similar Foreground  &37.2 & 57.4 & 39.9  \\
Similar General & 37.4 & 58.0 &  40.6\\
\bottomrule[2pt]
\end{tabular}
\end{center}
\vspace{-5pt}
\caption{Distillation with Different Distillation Points. Inconsistent distillation points are harmful for distillation. }
\vspace{-15pt}
\label{inconsistent}
\end{table}

\subsection{Consistent Distillation Points}
The core idea of knowledge distillation is forcing the student to mimic the prediction of teacher, which can be interpreted as matching the mapping function of student and teacher with a set of distillation points. Distillation points refer to the corresponding input $\mathbf{x}$ of the predictions, as $\mathbf{y}=f(\mathbf{x})$, where $f$ represents the model. In this view, the distillation points should kept sufficient in quantity and consistent between teacher and student models for effective and reliable matching. However, comparing the formulation of CNN detector and DETR, a critical challenge of DETR distillation lies in the lacking of consistent distillation points.

 
Classic detector degrades object detection to a verification problem which combines classification and regression, and introduces a set of anchors to specify the region for verification. In this way, the formulation of distillation points consists of the image and the location and scale of anchor $\mathbf{x} = (\mathbf{I}, \mathbf{anchor})$. As the anchors are generated through the sliding window strategy with handcrafted shapes, the locations and scales of anchors are implicitly embedded in the model architecture as prior information. Since student and teacher models share the same or similar architectures, a large number of object proposals generated by teacher and student models naturally have strict spatial correspondence, even for those background regions with low confidence. With the spatial correspondence which can be viewed as an inductive bias of CNN, classic detector naturally guarantees a sufficient number of consistent distillation points. 

In contrast, the DETR formulates object detection as a set-prediction problem. The distillation points, therefore, become the combination of the image and the object queries $\mathbf{x} = (\mathbf{I}, \mathbf{q})$. However, the object queries in different models are egocentric, as they are initialized and optimized by themselves independently. Since object queries play the role of probing and pooling the features of certain instances, they have inconsistent conc entration preferences in different models. Consequently, the formulation of DETR naturally lacks the ability to provide sufficient distillation points with strict consistency between teacher and student models, and the predictions acquired from teacher are not informative or reliable for student to mimic.

\subsection{Distillation with Inconsistent Distillation Points}
To validate the analysis above that the sufficiency and consistency of distillation points is the essential challenge in DETR distillation, we start with applying the original logit-based distillation method\cite{chen2017learning} in classic detector to DETR, which mimics the category and box location logits predictions of teacher model. 

We examine three distillation points strategies: \textit{Inconsistent}, \textit{Similar Foreground}, and \textit{Similar General}. In \textit{Inconsistent}, all the object queries are viewed as distillation points with their original permutation; In \textit{Similar Foreground}, only object queries matched to ground truth in bipartite matching will be used as distillation points, and will permute with the same order of ground truth label; \textit{Similar General} further increase the number of distillation points by viewing the average of negative object queries in bipartite matching as a general background distillation point.  Experiments are conducted on DAB-DETR and evaluated on MS COCO2017, with ResNet18 as student and ResNet-50 as teacher. 

As shown in Table \ref{inconsistent}, \textit{Inconsistent} distillation points results in great degradation of the student model with unreliable knowledge from teacher model; \textit{Similar Foreground} with semantic-similar foreground distillation points alleviate the problem; and \textit{Similar General} achieves further improvement by increasing the number of distillation points with general background features. The preliminary experiments validate that the sufficiency and consistency of distillation points are of prime importance to improve the performance of student model in DETR distillation. 