\vspace{-5pt}
\section{KD-DETR}
\begin{figure*}[t]
  \centering
  %\fbox{\rule{0pt}{2in} \rule{0.9\linewidth}{0pt}}
   \includegraphics[width=0.8\linewidth]{framework1-eps-converted-to.pdf}
   \caption{\textbf{KD-DETR architecture} KD-DETR decouples the distillation and detection tasks by introducing a set of distillation points shared between teacher and student models. For distillation, the distillation points are served as the query of the transformer decoders for both teacher and student models, and the student will mimic teacher's classification and box location predictions. The weight of each distillation point is measured by its foreground probability predicted by teacher. For detection task, the original object queries are processed by the student decoder for final prediction.}
   \vspace{-5pt}
   \label{framework}
\end{figure*}
To address the lack of consistent distillation points in DETR, we propose a general knowledge distillation paradigm for DETR (KD-DETR) with consistent distillation points sampling. As illustrated in Figure \ref{framework}, KD-DETR introduces a set of specialized object queries $\mathbf{\tilde{q}}$ shared between teacher and student models to construct distillation points. Decoupling the distillation task and detection task, KD-DETR can provide sufficient and consistent distillation points. We denote the original input and sampled distillation points as $\mathbf{x}=\{\mathbf{I}, \mathbf{q}\}, \mathbf{\tilde{x}}=\{\mathbf{I}, \mathbf{\tilde{q}}\}$ respectively.

For detection task, the student model is first optimized by its original detection loss: the original input $\mathbf{x}$ is fed into student model to make category and box location predictions, which will be assigned to the ground truth with bipartite matching and calculate detection loss $ \mathcal{L}_{det}$.

For distillation task, the sampled distillation points $\mathbf{\tilde{x}}$ will be fed into both student and teacher to make category and location predictions $\mathbf{c}, \mathbf{b}$:
\vspace{-5pt}
\begin{align}
    & \mathbf{c^s}, \mathbf{b^s} = f^s(\mathbf{I}, \mathbf{\tilde{q}}), \\
    & \mathbf{c^t}, \mathbf{b^t} = f^t(\mathbf{I}, \mathbf{\tilde{q}}), 
\end{align}
where $f^s, f^t$ refer to student and teacher model respectively. The distillation loss is calculated with following form:
\vspace{-5pt}
\begin{equation}
\begin{split}
    \mathcal{L}_{distill}  = &\sum_{i=1}^{M} [\lambda_{cls} \mathcal{L}_{KL}(\mathbf{\hat{c}}^t_i\Vert \mathbf{\hat{c}}^s_i) + \lambda_{L1}\mathcal{L}_{L1}(\mathbf{b}_i^s, \mathbf{b}_i^t), \\
    +& \lambda_{GIoU}\mathcal{L}_{GIoU}(\mathbf{b}_i^s, \mathbf{b}_i^t) ], %\\
    %KL(\mathbf{c}^t_i\Vert \mathbf{c}^s_i)& = \sum_{j=1}^{K}(-c^t_{i,j} \log{c^s_{i,j}}))
\label{distill_loss}
\end{split}
\end{equation}
where M denotes the number of distillation points. For classification, we choose the KL-divergence $\mathcal{L}_{KL}$ as distillation loss with temperature $T$: $\mathbf{\hat{c}} = SoftMax(\frac{\mathbf{c}}{T})$. For box regression, $\mathcal{L}_{L1}$ and $\mathcal{L}_{GIoU}$ represent the L1 and GIoU loss for location distillation, which have the same formulation with detection loss. $\lambda_{cls}, \lambda_{L1}, \lambda_{GIoU}$ represent the coefficient of corresponding loss.

The total Loss is calculated with following form:
\begin{equation}
\begin{split}
    \mathcal{L}  = \mathcal{L}_{distill} + \mathcal{L}_{det}
\label{loss}
\end{split}
\end{equation}

\subsection{Distillation Points Sampling}
Generally, object queries are a set of abstract features responsible for certain objects by probing and pooling the context features from encoder. Existing works interpret object queries as anchors or reference points, revealing that each object query is sensitive to a particular region on the feature maps. Following this perspective, we provide a comprehensive general-to-specific sampling strategies for distillation points sampling: $\mathbf{\tilde{q}} = \{\mathbf{q_g}, \mathbf{q_s}\}$. 

\noindent\textbf{General Sampling with Random Initialized Queries.} 
In general sampling, we hope to probe teacher's general responses on different locations of the features by sparsely scanning the whole feature maps. Therefore, we randomly initialize a set of object queries, which are uniformly distributed on the features, to construct the general distillation points: $\mathbf{q_g}=\{\mathbf{q_i}\sim\mathcal{U}(0, 1)|i=1,...,M_g\}$, where $M_g$ denotes the number of general distillation points. To learn more general knowledge from teacher, we leave these distillation points unlearnable during training, and re-sample them every iteration.

\noindent\textbf{Specific Sampling with Teacher Queries.} While the general sampling provides a global retrieval of the features, we further propose a specific sampling strategy, focusing on those regions where teacher pays more attention. An intuitive way for specific sampling is to directly reuse the well-optimized object queries in teacher model:$\mathbf{q_s}=\mathbf{q_{teacher}}$. While teacher model is learned to concentrate more on these object queries, the predictions in these areas are more precise and informative.

\noindent\textbf{Foreground Rebalance Weight.}
\begin{table*}[th]
\begin{center}
\begin{tabular}{cc|c|c|ccccc|cc}
\toprule[2pt]
Models &  & Epochs & AP & $AP_{50}$ & $AP_{75}$ & $AP_{S}$ & $AP_{M}$& $AP_{L}$ & GFlops & Params \\
\hline
DAB-DETR& ResNet-50(T)& 50 & 42.1 &63.1 & 44.7&21.5 &45.7  &69.3 & 90 & 44M\\
& ResNet-18(S)& 50 & 36.2 &56.1 & 37.9& 16.9&39.0 & 53.5& 49 & 23M\\
& Ours& 50 & 41.4 & 61.4& 44.2&21.2 &44.7 &58.7 &  49 &23M\\
& Gains&  & \textbf{+5.2} & \textbf{+5.3}& \textbf{+6.3}&\textbf{+4.3} &\textbf{+5.7} &\textbf{+5.2} & &\\
%\cline{2-11}
%& ResNet-34(S)& 50 & 40.6 & 60.9 & 43.3 & 21.4 & 44.3 & 56.3 & 51 &42M \\
%& Ours& 50 & 44.1 & 64.4 & 47.3 & 24.6 & 47.8 & 61.0 & 51 & 42M\\
%& Gains&  &\textbf{+3.5} & \textbf{+3.5}& \textbf{+4.0} &\textbf{+3.2} &\textbf{+3.5} &\textbf{+4.7} &\\
\hline
DAB-DETR& ResNet-101(T)& 50 & 43.5 &63.9 & 44.6& 23.6& 47.3& 61.5 & 157 & 63M\\
& ResNet-50(S)& 50 & 42.1 &63.1 & 44.7&21.5 &45.7  &60.3 & 90 & 44M \\
& Ours& 50 & 45.7 &66.3 &49.4 &26.4 &49.8 &62.7 & 90 & 44M\\
& Gains&  & \textbf{+3.6} &\textbf{+3.2} &\textbf{+4.7} &\textbf{+4.9} &\textbf{+4.1} &\textbf{+2.4} & &\\
\hline
Deformable-DETR& ResNet-50(T)& 50 & 44.5 &63.6 &48.7 & 27.1& 47.6& 59.6& 171 & 40M  \\
& ResNet-18(S)& 50 & 40.1 & 58.1&43.7 & 22.4& 42.8&54.2 & 127 & 24M\\
& Ours& 50 & 43.7 & 62.1& 47.7&25.9 & 46.8& 57.6 & 127& 24M\\
& Gains & & \textbf{+3.6} & \textbf{+4.0} & \textbf{+4.0} & \textbf{+3.5} & \textbf{+4.0} & \textbf{+3.4}\\
\hline
Deformable-DETR& ResNet-101(T)& 50 & 48.0 & 66.7&52.6 &30.5 &52.3 &  62.3 & 238 &59M\\
& ResNet-50(S)& 50 & 44.5 &63.6 &48.7 & 27.1& 47.6& 59.6& 171& 40M  \\
& Ours& 50 & 48.3 & 66.7 & 52.9 & 30.8 & 52.1 & 62.5 &171 &40M\\
& Gains & & \textbf{+3.8} & \textbf{+3.1} & \textbf{+4.2} & \textbf{+3.7} & \textbf{+4.5} & \textbf{+2.9}\\
%DAB-Deformable& ResNet-50(T)& 50 & 46.8 & 66.0 & 50.4 & 29.1 & 49.8 & 62.3  & 19 & 47M\\
%& ResNet-18(S)& 50 & 45.8 & 63.5 & 50.1 & 29.6 & 48.5 & 59.0 & 30 & 34M\\
%& Ours& 50 & 47.2&65.4 & 51.7 & 31.5 & 50.3 & 60.6 & 30 & 34M \\
% & Gains & & \textbf{+1.4} & \textbf{+1.9} & \textbf{+1.6} & \textbf{+1.9} & \textbf{+1.8} & \textbf{+1.6}\\
\hline
DINO& ResNet-50(T)& 36 & 50.9 & 69.0 & 55.3 & 34.6& 54.1& 64.6 & 245 & 47M\\
& ResNet-18(S)& 12 & 44.0 & 61.2 & 48.1& 27.4 & 46.9 & 56.9& 200 & 30M\\
%& Ours& 12 & 45.7 & 62.2 & 49.3 & 27.3 & 48.2 & 59.0 &29 &34M\\
& Ours& 12 & 48.4 & 65.5 & 53.0 & 31.6 & 51.7 & 62.3 & 200 & 30M \\
& Gains&  & \textbf{+4.4} & \textbf{+4.3} & \textbf{+4.9} & \textbf{+4.2} & \textbf{+4.8} &\textbf{+5.4}& &\\
\hline
DINO &ResNet-101(T) & 36 & 51.3 & 69.5 & 55.8 & 34.8 &  54.8 & 65.8 & 311 & 66M\\
& ResNet-50(S)& 12 & 49.0 & 66.6 & 53.5 & 32.0 & 52.3 & 63.0 & 245 & 47M\\
& Ours& 12 & 51.6 & 69.6 & 56.6 & 34.2& 54.8 & 66.9 & 245 &47M\\
& Gains&  & \textbf{+2.6} & \textbf{+3.0} & \textbf{+3.1} & \textbf{+2.2} & \textbf{+2.5} &\textbf{+3.9}& &\\
\bottomrule[2pt]
\end{tabular}
\end{center}
\vspace{-5pt}
\caption{Results of the proposed KD-DETR with different DETR detectors and backbones with various scale.}
\vspace{-5pt}
\label{results}
\end{table*}
The imbalance between foreground and background regions is one critical problem in object detection distillation, not special in DETR. An intuitive way is to utilize the classification scores of distillation points which are predicted by teacher model to re-balance the distillation loss. Concretely, those distillation points with higher classification scores are regarded as foreground distillation points, containing more useful information for detection, and should be given more attention.
\begin{equation}
    w_i = \max_{c\in[0, K]}p^t(y_c|\mathbf{q}_i),
\end{equation}

\noindent where $p^t(y_c|\mathbf{q}_i)$ denotes the probability of $\mathbf{q}_i$ assigned to category $c$ predicted by teacher model, and $w_i$ denotes the foreground rebalance weight of $\mathbf{q}_i$. In this way, the distillation loss in Eq.~\eqref{distill_loss} will be writen as follow:
\begin{equation}
\begin{split}
    \mathcal{L}_{distill}  = &\sum_{i=1}^{M} w_i[\lambda_{cls} \mathcal{L}_{KL}(\mathbf{\hat{c}}^t_i\Vert \mathbf{\hat{c}}^s_i) \\
    + &\lambda_{L1}\mathcal{L}_{L1}(\mathbf{b}_i^s, \mathbf{b}_i^t) 
    + \lambda_{GIoU}\mathcal{L}_{GIoU}(\mathbf{b}_i^s, \mathbf{b}_i^t) ] 
\label{distill_coarse}
\end{split}
\end{equation}

\subsection{Generalization to Heterogeneous Distillation}
To further extend the generalization of the KD-DETR paradigm, we apply the idea to heterogeneous distillation between DETR and CNN detector. Intuitively, both the anchor in the CNN detector and the object query in DETR represent certain locations on the image and share spatial consistency. We first construct a set of distillation points $ \mathbf{\tilde{q}}$ with the coordinate of anchors in CNN detector, and then convert them to the formulation of object queries in DETR. The distillation loss is in the formulation of Eq.~\eqref{distill_loss}.

We further propose a simple but effective strategy for distillation points sampling in heterogeneous distillation with Intersection over Union (IoU). Specifically, we calculate the IoU between anchors and ground truth grounding boxes, and select the top $k$ anchors as distillation points. Details about heterogeneous distillation are in Appendix \ref{sec:het}.
%With the intermediary of distillation points, KD-DETR can be easily extended to heterogeneous distillation between DETR and CNN detector. Intuitively, we construct a set of distillation points with spatial consistency between DETR and CNN detector. The distillation points  is in the formulation of bounding box, which can be converted to anchor in CNN detectors and object queries in DETR. 
%Specifically, we construct a set of distillation points with spatial consistency between DETR and CNN detector. The distillation points 
%To further extend the generalization of KD-DETR paradigm, we apply the idea to heterogeneous distillation between DETR and CNN detector. We construct a set of consistent distillation points with the form of bounding box coordinations, which will be converted to region of proposals in CNN detectors and object queries in DETR. 

