\section{Experiment}

To validate the effectiveness and generalization of KD-DETR, we evaluate it on different DETR architectures including DAB-DETR, Deformable DETR and DINO, with two scales of backbones: ResNet-50 and ResNet18. We also extend KD-DETR to heterogeneous distillation, and evaluate on distillation between DINO-Res50 and Faster-RCNN Res50. To support our analysis of consistent distillation points, we further conduct an extensive ablation study. 

\subsection{Experimental Settings}

\noindent\textbf{Datasets}: All the proposed experiments are evaluated on MS COCO2017\cite{lin2014microsoft} spanning 80 categories, with the default split of ~117k training images for training and 5k validate images for testing.  Standard COCO evaluation metrics are adopted for validation.

\noindent\textbf{Implementation Details}: As KD-DETR is a plug-and-play distillation module, we follow the original settings of hyper-parameters and optimizer of all the student model for the training of detection part. We choose ResNet\cite{he2016deep} as backbone, which are pre-trained on ImageNet1K\cite{krizhevsky2017imagenet}. We propose the inheriting strategy\cite{kang2021instance} to initialize students' level embeddings on multi-scale DETR (details in Appendix \ref{sec:inherit}). For distillation task, we set hyper-parameters of the coefficient of the distillation loss as $\lambda_{kl}=1, \lambda_{L1}=5, \lambda_{GIoU}=2$. The number of General distillation points is $300, 300, 900$ for DAB-DETR, Deformable DETR, and DINO. For heterogeneous distillation, the hyper-parameter $k$ for IoU sampling is set to $10$.  We train our models on Nvidia A100 GPUs with batch size set to 16 in total. %The reported FPS is measured on a single Nvidia A100 GPU.

\begin{table}[t]
\begin{center}
\begin{tabular}{c|ccccc}
\toprule[2pt]
Method & AP & $AP_{S}$ & $AP_{M}$ & $AP_{L}$   \\
\hline
Deformable DETR Res50 & 44.5 & 27.1 & 47.6	 & 59.6  \\
FGD\cite{yang2022focal} & 44.1 & 25.9 &  47.7 & 58.8  \\
FitNet\cite{romero2014fitnets} & 44.9 & 27.2 & 48.4 & 59.6 \\
DETRDistill\cite{chang2023detrdistill} & 46.6 & 28.5 & 50.0 & 60.4 \\
\hline
Ours & \textbf{48.3}&  \textbf{30.8} &  \textbf{52.1}&  \textbf{62.5}  \\
\bottomrule[2pt]
\end{tabular}
\end{center}
%\vspace{-5pt}
\caption{Comparison with state-of-the-art on Deformable DETR.}
\vspace{-5pt}
\label{sota}
\end{table}


\subsection{Distillation on Different DETR benchmarks}
We evaluate our method on three typical DETR architectures: DAB-DETR\cite{liu2021dab}, a single-scale DETR with special object query settings; Deformable-DETR\cite{zhu2020deformable}, a multi-scale DETR with deformable attention module; and DINO\cite{zhang2022dino}, which combines a series of novel techniques including deformable attention, two-stage object queries settings, and a de-noising module, to evaluate our effectiveness on well-optimized model with high accuracy. Distillation results of DINO with Swin Transformer backbone and compressing the layers of DETR are in Appendix \ref{sec:backbone}, \ref{sec:layers}

The results are illustrated in Table \ref{results}. For DAB-DETR, KD-DETR significantly boosts the performance of ResNet-18, and ResNet-50 with $5.2\%$, and $3.6\%$ mAP improvement respectively. Note that student with ResNet-50 even surpass the teacher by $2.2\%$ mAP. For Deformable DETR with ResNet-18 and ResNet-50, KD-DETR achieve $3.6\%$ and $3.8\%$ mAP gains, and student with ResNet-50 also exceed the teacher with $0.3\%$ mAP. For DINO with ResNet-18 and ResNet-50, KD-DETR also improves $4.4\%$ and $2.6\%$ mAP of the student models on 12-epoch training scheduler. 

Table \ref{sota} shows the comparison with state-of-the-art distillation method. With simple logit-based distillation alone, KD-DETR outperforms former methods , with an improvement of $1.7\%$ on Deformable DETR with ResNet-50 compared with DETRDisitll\cite{chang2023detrdistill}

%\subsection{Distillation on the Transformer Layers}
%Besides the scale of backbone, the layer number of transformer encoder and decoder is also an important factor of the model size and computation cost in DETR. In this paper, we also conduct experiments to compress the layer numbers of transformer to validate the scalability of KD-DETR, 

%Table \ref{layer} shows the results of KD-DETR on DAB-DETR, with backbone of ResNet-50 as teacher and ResNet-18 as student. While decreasing the number of transformer layers will cause great degradation in the performance, KD-DETR can significantly boost the student model. For example, the student model with 2 encoder layers and 6 decoder layers can outperform the full-scale model for $2.8\%$ mAP with 1.2x FPS improvement.

\subsection{Generalization to Heterogeneous Distillation}

Table \ref{cad} presents the results with Faster RCNN ResNet-50 as student and DINO ResNet-50 as teacher. KD-DETR works well on heterogeneous distillation task, improving the student models with $2.1\%$ mAP, achieving the performance of state-of-the-art homogeneous methods. This validate our analysis on the effect of distillation points on knowledge distillation, and bridge the gap of transferring knowledge between detectors with different architectures. 
\begin{table}[t]
\begin{center}
\begin{tabular}{c|cccc}
\toprule[2pt]
Method & AP & $AP_{S}$ & $AP_{M}$ & $AP_{L}$   \\
\hline
RCNN-Res50 & 38.4 & 21.5 & 42.1 & 50.3  \\
FGFI\cite{wang2019distilling}& 39.3 & 22.5 & 42.3 & 52.2  \\
GID\cite{dai2021general} &  40.2&22.7 & 44.0& 53.2 \\
FGD\cite{yang2022focal} & 40.4 & \textbf{22.8}&  44.5 & 53.5  \\
\hline
Ours & \textbf{40.5} & 22.7 & \textbf{44.6}& \textbf{53.8}   \\
\bottomrule[2pt]
\end{tabular}
\end{center}
%\vspace{-5pt}
\caption{\textbf{Heterogeneous Distillation}: Distillation between DINO and Faster RCNN}
\label{cad}
\end{table}


%\begin{table}[t]
%\begin{center}
%\begin{tabular}{c|ccc|cc}
%\toprule[2pt]
%Enc/Dec & AP & $AP_{50}$ & $AP_{75}$ & FPS & Params  \\
%\hline
%6/6 & 36.2 & 56.1 & 37.9 & 76 & 31M \\
%Ours & 41.4(\textbf{+5.2}) & 61.4 & 44.2 & 76 & 31M \\
%2/6 &  36.2&56.3 & 38.4 & 102 & 27M\\
%Ours & 39.0(\textbf{+2.8}) & 58.9&  41.7 & 102 & 27M \\
%6/2 &31.8 & 49.8&  33.5 & 82 & 25M\\
%Ours & 38.9(\textbf{+7.0})& 58.0& 41.6 & 82 & 25M \\
%2/2 & 29.3 & 46.6 & 37.0 & 113 &17M \\
%Ours & 32.8(\textbf{+3.5}) &  52.6 & 34.2 &113 &17M\\
%\bottomrule[2pt]
%\end{tabular}
%\end{center}
%\caption{\textbf{Distillation on Transformer Layers}: Compressing the number of encoder layers and decoder layers with KD-DETR}
%\label{layer}
%\end{table}
\subsection{Ablation Study}
The ablation study is conducted on MS COCO2017 with DAB-DETR with backbone of ResNet50 as teacher and ResNet18 as student. 

\noindent\textbf{Analysis on the Benefit of Knowledge Distillation}

In KD-DETR, we introduce a set of specialized distillation points in the training process to the original DETR architecture. \cite{zhang2022dino}\cite{li2022dn} have proven that the number of object queries will affect the performance of model. To validate that the benefit is from knowledge distillation rather than the increase of object queries, we conduct an ablation experiment by applying an additional set of object queries with the same number of distillation points.

As shown in Table \ref{benefit}, simply increasing the number of object queries only achieves trivial gains, and the main contribution of boosting the model's performance is from the knowledge transferred from teacher with distillation points.

\begin{table}[t]
\begin{center}
\begin{tabular}{cc|ccc}
\toprule[2pt]
\makecell[c]{Object\\ Queries} & \makecell[c]{Distillation \\ Points}& AP & $AP_{50}$ & $AP_{75}$  \\
\hline
300 & -  & 36.2   & 56.1 &  37.9 \\
300+300 & - & 37.7  &58.0 &  41.1 \\
300 & 300 & 40.2  & 60.7 & 42.8 \\
\bottomrule[2pt]
\end{tabular}
\end{center}
\vspace{-5pt}
\caption{\textbf{Benefit of Knowledge Distillation}: Simply increasing the set of object queries leads to trivial improvement}
\vspace{-5pt}
\label{benefit}
\end{table}

\noindent\textbf{Analysis on the Distillation Points Sampling Strategy}

A comprehensive general-to-specific distillation points sampling scheme is introduced in this paper, including three strategies: General Sampling, Specific Sampling, and Foreground Rebalance Weight. 
The ablation results are illustrated in Table \ref{strategy}. The general sampling strategy with randomly initialized queries can boost the performance of student model for $2.5\%$ mAP, while the specific sampling strategy with teacher queries achieves an improvement of $3.8\%$. When refining the general distillation points with foreground rebalance weight, the general sampling strategy and specific sampling strategy yield larger gains for $3.7\%$ and $4.0\%$, respectively. The combination of the three strategies further promotes the performance for $5.2\%$.

It is also important to note that the performance of general sampling with foreground rebalance weight is fairly close to the specific sampling. Since the teacher queries in specific sampling are well-optimized and concentrate more on the foreground regions, such phenomena validate that the general sampling with randomly initialized queries can probe the whole feature maps evenly, and foreground rebalance weight can help the student focus more on the foreground regions. In addition, the further improvement brought by the combination of three strategies indicates that the specific distillation points bring more information from the concentration of teacher model.
\begin{table}[t]
\begin{center}
\begin{tabular}{ccc|ccc}
\toprule[2pt]
General & Specific & FRW & AP & $AP_{50}$ & $AP_{75}$  \\
\hline
&&& 36.2 & 56.1 & 37.9 \\
$\checkmark$&  &  & 38.7 & 59.1 & 40.7 \\
$\checkmark$& & $\checkmark$ & 39.9 & 60.2& 42.5  \\
& $\checkmark$ &  & 40.0 & 60.5 & 42.4  \\

& $\checkmark$& $\checkmark$ & 40.2 &60.7 & 42.8 \\
$\checkmark$& $\checkmark$& $\checkmark$ & 41.4 &61.4 &44.2 \\
\bottomrule[2pt]
\end{tabular}
\end{center}
\vspace{-5pt}
\caption{Distillation with Different Distillation Points Sampling Strategies. FRW refers to Foreground Rebalance Weight.}
\vspace{-5pt}
\label{strategy}
\end{table}

\noindent\textbf{Number of General Distillation Points}

We investigate the influence of the number of general distillation points by varying the number from 10 to 900. As is shown in Table \ref{number}, the improvement is significant when increasing the distillation points from 10 to 50, while gradually saturating when continuing to increase to 300. There is even a slight degradation when further increasing the number. These phenomena validate that the general distillation points can effectively probe the knowledge from teacher's attention in different regions when sparsely distributed on the feature maps. However, dense distillation points will introduce more noise of background information, and will be harmful for distillation.
\begin{table}[t]
\begin{center}
\begin{tabular}{c|ccccc}
\toprule[2pt]
Number & 10 & 50 & 100 & 300 & 900  \\
\hline
AP             & 38.6 & 39.3  & 39.5  & 39.9 & 39.5\\
$AP_{50}$& 58.3 & 59.2  & 59.3 & 60.2& 60.4\\
$AP_{75}$& 41.0 & 41.3  & 41.8 &  42.5 & 41.7 \\
%$AP_S$    & 19.5 & 20.5  & 20.3 & 20.6& 20.7\\
%$AP_M$.  & 41.8 &  42.6 & 42.7 & 42.9& 42.6\\
%$AP_L$.   & 55.5 & 55.6  & 56.5 & 57.0& 56.5\\
\bottomrule[2pt]
\end{tabular}
\end{center}
\vspace{-5pt}
\caption{Distillation with Different Number of General Distillation Points}
\vspace{-10pt}
\label{number}
\end{table}

\noindent\textbf{Visualization of the Distillation Points}

\begin{figure}[t]
  \centering
  %\fbox{\rule{0pt}{2in} \rule{0.9\linewidth}{0pt}}
   \includegraphics[width=1\linewidth]{Specific-eps-converted-to.pdf}
  \caption{Attention Map Visualization of Specific Distillation Points: Images from left to right are from teacher, original student and student with KD-DETR. The corresponding predicted bounding box of the distillation points are marked with red rectangles. }
   \vspace{-5pt}
   \label{specific}
\end{figure}

\begin{figure}[t]
  \centering
    
    \begin{minipage}{0.32\linewidth}
  \centering
   \includegraphics[width=1\textwidth]{fig0_random0.jpg}
    %\fbox{\rule{0pt}{2in} \rule{.9\linewidth}{0pt}}
%    \caption{An example of a subfigure.}
    \label{fig:short-a}
  \end{minipage}
  \begin{minipage}{0.32\linewidth}
  
  \centering
   \includegraphics[width=1\textwidth]{fig0_random1.jpg}  
%    \caption{Another example of a subfigure.}
    \label{fig:short-b}
  \end{minipage}
   \begin{minipage}{0.32\linewidth}
   \centering
   \includegraphics[width=1\textwidth]{fig0_random2.jpg}  
 %   \caption{Another example of a subfigure.}
    \label{fig:short-b}
  \end{minipage}\\ 
       \begin{minipage}{0.32\linewidth}
  \centering
  \vspace{-10pt}
   \includegraphics[width=1\textwidth]{fig1_random1.jpg}
    %\fbox{\rule{0pt}{2in} \rule{.9\linewidth}{0pt}}
  %  \caption{An example of a subfigure.}
    \label{fig:short-a}
  \end{minipage}
  \begin{minipage}{0.32\linewidth}
  \centering
  \vspace{-10pt}
   \includegraphics[width=1\textwidth]{fig1_random2.jpg}  
%    \caption{Another example of a subfigure.}
    \label{fig:short-b}
  \end{minipage}
   \begin{minipage}{0.32\linewidth}
   \centering
   \vspace{-10pt}
   \includegraphics[width=1\textwidth]{fig1_random3.jpg}  
%    \caption{Another example of a subfigure.}
    \label{fig:short-b}
  \end{minipage}\\ 
  \vspace{-10pt}
 \caption{Attention Map Visualization of General Distillation Points: Teacher model's attention of general distillation points focuses more on the background regions, providing more information for student to mimic.}
 \vspace{-10pt}
  \label{general}
\end{figure}

In order to understand what knowledge has been transferred from teacher to student better, we visualize the attention map of different kinds of distillation points. Figure \ref{specific} illustrates the attention of the same specific distillation points from teacher model, student model and KD-DETR. It can be seen that the specific distillation points focus more on the features of foreground regions. While the attention of the original student model without distillation is different from the teacher model, KD-DETR can align the concentration of teacher and student models. On the contrary, the general distillation points, as illustrated in Figure \ref{general}, concentrate more on the background regions, and objects that are not included in the ground truth annotations, which can provide more additional semantic information to the student. In this way, the combination of general and specific distillation points can provide more comprehensive knowledge.
