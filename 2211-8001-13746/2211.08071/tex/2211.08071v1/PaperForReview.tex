% CVPR 2023 Paper Template
% based on the CVPR template provided by Ming-Ming Cheng (https://github.com/MCG-NKU/CVPR_Template)
% modified and extended by Stefan Roth (stefan.roth@NOSPAMtu-darmstadt.de)

\documentclass[10pt,twocolumn,letterpaper,pagenumbers]{article}

%%%%%%%%% PAPER TYPE  - PLEASE UPDATE FOR FINAL VERSION
%\usepackage[review]{cvpr}      % To produce the REVIEW version
%\usepackage{cvpr}              % To produce the CAMERA-READY version
\usepackage[pagenumbers]{cvpr} % To force page numbers, e.g. for an arXiv version

% Include other packages here, before hyperref.
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{tabulary}
\usepackage{makecell}


% It is strongly recommended to use hyperref, especially for the review version.
% hyperref with option pagebackref eases the reviewers' job.
% Please disable hyperref *only* if you encounter grave issues, e.g. with the
% file validation for the camera-ready version.
%
% If you comment hyperref and then uncomment it, you should delete
% ReviewTempalte.aux before re-running LaTeX.
% (Or just hit 'q' on the first LaTeX run, let it finish, and you
%  should be clear).
\usepackage[pagebackref,breaklinks,colorlinks]{hyperref}


% Support for easy cross-referencing
\usepackage[capitalize]{cleveref}
\crefname{section}{Sec.}{Secs.}
\Crefname{section}{Section}{Sections}
\Crefname{table}{Table}{Tables}
\crefname{table}{Tab.}{Tabs.}


%%%%%%%%% PAPER ID  - PLEASE UPDATE
\def\cvprPaperID{*****} % *** Enter the CVPR Paper ID here
\def\confName{CVPR}
\def\confYear{2023}


\begin{document}

%%%%%%%%% TITLE - PLEASE UPDATE
\title{Knowledge Distillation for Detection Transformer with Consistent Distillation Points Sampling}

%\author{Yu Wang\qquard\qquard  Xin Li\qquard\qquard Shengzhao Wen\qquard\qquard Fukui Yang\\ \quard\quard Wanping Zhang\quard\quard Gang Zhang\quard\quard Haocheng Feng \quard\quard Junyu Han\quard\quard Errui Ding\\
\author{
Yu Wang\hspace{10pt} Xin Li\hspace{10pt}Shengzhao Wen\hspace{10pt}Fukui Yang\hspace{10pt}Wanping Zhang\hspace{10pt}Gang Zhang\\Haocheng Feng\hspace{10pt}Junyu Han\hspace{10pt}Errui Ding\\
Baidu VIS\\
{\tt\small {\{wangyu106, lixin68, wenshengzhao, yangfukui, zhangwanping, zhanggang03,}}\\
{\tt\small{fenghaocheng, hanjunyu, dingerrui\}@baidu.com}}
% For a paper whose authors are all at the same institution,
% omit the following lines up until the closing ``}''.
% Additional authors and addresses can be added with ``\and'',
% just like the second author.
% To save space, use either the email address or home page, not both
}
\maketitle

%%%%%%%%% ABSTRACT
\begin{abstract}
DETR is a novel end-to-end transformer architecture object detector, which significantly outperforms classic detectors when scaling up the model size. In this paper, we focus on the compression of DETR with knowledge distillation. While knowledge distillation has been well-studied in classic detectors, there is a lack of researches on how to make it work effectively on DETR. We first provide experimental and theoretical analysis to point out that the main challenge in DETR distillation is the lack of consistent distillation points. Distillation points refer to the corresponding inputs of the predictions for student to mimic, and reliable distillation requires sufficient distillation points which are consistent between teacher and student. Based on this observation, we propose a general knowledge distillation paradigm for DETR(KD-DETR) with consistent distillation points sampling. Specifically, we decouple detection and distillation tasks by introducing a set of specialized object queries to construct distillation points. In this paradigm, we further propose a general-to-specific distillation points sampling strategy to explore the extensibility of KD-DETR. Extensive experiments on different DETR architectures with various scales of backbones and transformer layers validate the effectiveness and generalization of KD-DETR. KD-DETR boosts the performance of DAB-DETR with ResNet-18 and ResNet-50 backbone to 41.4$\%$, 45.7$\%$ mAP, respectively, which are 5.2$\%$, 3.5$\%$ higher than the baseline, and ResNet-50 even surpasses the teacher model by $2.2\%$.
\end{abstract} 
\vspace{-10pt}
%%%%%%%%% BODY TEXT
\section{Introduction}
\label{sec:intro}
%Object detection, as a fundamental task in computer vision, has long been dominated by convolutional neural networks (CNNs). 
%Object detection, as a fundamental task in computer vision, has long been dominated by convolutional neural networks (CNNs). 
Object detection, as a fundamental task in computer vision, has long been dominated by detectors based on convolutional neural networks (CNNs)\cite{ghiasi2021simple}\cite{dai2021dynamic}\cite{zhou2021probabilistic}. In recent years, \cite{carion2020end} propose the novel end-to-end detector Detection Transformer (DETR) which eliminates the need for hand-crafted anchors and non-maximum suppression (NMS). \cite{zhu2020deformable}\cite{li2022dn}\cite{liu2022dab}\cite{zhang2022dino}further make remarkable stride towards the scalability and potential of DETR, significantly outperforming classical detectors.
\begin{figure}[t]
  \centering
   \includegraphics[width=1\linewidth]{compare.eps}
   \vspace{-35pt}
   \caption{\textbf{ResuIts Overview.} Improvement of KD-DETR on DAB-DETR, Deformable DETR, DAB-Deformable and DINO. Results are evaluated on MS COCO2017. ResNet-50 is used as teacher for ResNet-18, and ResNet101 for ResNet-50. Note that the student model can even exceed the teacher significantly in the first three architectures.}
   \vspace{-10pt}
   \label{overview}
\end{figure}

Different from classic detectors, DETR interprets object detection as an end-to-end set prediction problem with bipartite matching. A set of learnable object queries are introduced, each responsible for a certain instance. The object queries interact with features extracted from the encoder to make final predictions of box locations and categories. Despite the impressive performance, the growing model scale prevents DETR from being deployed to real-world applications with urgent computation budget requirement.

\begin{figure}[t]
  \centering
  \begin{subfigure}{1\linewidth}
  \centering
   \includegraphics[width=0.97\textwidth]{classic.eps}
    %\fbox{\rule{0pt}{2in} \rule{.9\linewidth}{0pt}}
    \caption{Distillation Points in Classic Detector}
    \label{classic}
  \end{subfigure}\\
  \begin{subfigure}{1\linewidth}
  \centering
   \includegraphics[width=0.97\textwidth]{detr.eps}  
    \caption{Distillation Points in DETR}
    \label{detr}
  \end{subfigure}\\
   \begin{subfigure}{1\linewidth}
   \centering
   \includegraphics[width=0.97\textwidth]{kddetr.eps}  
    \caption{Distillation Points in KD-DETR}
    \label{kddetr}
  \end{subfigure}\\ 
 \caption{\textbf{Schematic Illustrations of Distillation Points in Different Architecture}: (a) Two-Stage Detector: both positive and negetive proposals in RPN and RoI are consistent distillation points with strict one-to-one correspondence between teacher and student model; (b) DETR: Object queries lacks spatial or semantic relationship between teacher and student model, resulting in inconsistent distillation points; (c) KD-DETR: A set of special object queries is introduced to construct consistent distillation points for DETR distillation}
 \vspace{-10pt}
  \label{compare}
\end{figure}

To address this problem, current works have made efforts in designing efficient DETR architectures, reducing the encoder tokens utilized in cross-attention module to decrease the computation cost \cite{roh2021sparse}, and leveraging dense prior from RPN to downsize the decoder layers \cite{yao2021efficient}. In this work, we concentrate on compressing the large-scale DETR model by knowledge distillation \cite{hinton2015distilling} approaches, which is a promising technique for model compression and accuracy boosting. Knowledge distillation can transfer the knowledge learned from large and cumbersome DETR models to small and efficient ones by forcing the student to mimic the predictions from teacher, whether logits or internal activations. However, modern knowledge distillation methods are designed under CNN-based detectors, and researches on expanding it to general DETR compression are limited. 

 We start from experiments of applying classic logit-based distillation\cite{chen2017learning} to DETR, which unfortunately leads to degradation of student performance. With these experiments, we observe that the critical challenge lies in the different formulations of DETR and classic detectors. Compared with classic detectors, the set-prediction formulation of DETR naturally contains fewer consistent distillation points. We use distillation points to denote the corresponding input of the prediction for mimicking in knowledge distillation, and the sufficiency and consistency of distillation points form the foundation of knowledge distillation. Specifically, abundant distillation points which are kept consistent between teacher and student models are essential for effective distillation. As shown in Fig \ref{classic}, classic detectors make predictions for a set of region proposals generated from the sliding window locations on the images with handcraft scales. This pattern naturally ensures a strict spatial correspondence between the large number of proposals made by teacher and student models, even for those negative ones with low confidence, thus providing a sufficient number of consistent distillation points for mimicking. In DETR, as is shown in Fig \ref{detr}, the distillation points actually consist of both image and object queries. However, the object queries from teacher and student models are egocentric and even differ in number, thus lacking definite correspondence, especially those redundant negative queries in bipartite matching. As the distillation points in DETR are inconsistent and insufficient, the predictions acquired from teacher are not reliable or informative for student to mimic.
 
The observation above raises the issue: how to obtain sufficient and consistent distillation points for DETR distillation. To address this issue, we focus on the object queries, and propose a general knowledge distillation paradigm for DETR(KD-DETR) with consistent distillation points sampling. In KD-DETR, as illustrated in Fig \ref{kddetr}, we decouple detection task and distillation task by introducing a set of specialized object queries to construct distillation points. The distillation points are unlearnable and shared between teacher and student models, probing the “dark knowledge” in teacher model. In this way, consistent distillation points with customized quantities become available. With the paradigm of KD-DETR, we propose a general-to-specific distillation points sampling strategy to probe comprehensive knowledge in teacher model. For general sampling, we randomly sample a set of object queries which are uniformly distributed to coarsely scan the whole feature maps and probe more general knowledge from teacher model. For specific sampling, the well-optimized object queries in teacher model are used to construct distillation points, as they represent the specific knowledge the teacher model concerns. Considering the imbalance of positive and negative regions in object detection, we further reweigh the importance of each distillation point with its foreground probability predicted by the teacher model. 
% As illustrate in Fig \ref{kddetr}, to obtain more general concerns of the features in teacher model, the object queries in distillation points are uniformly distributed on the object query space. The distillation points are unlearnable and randomly initialized, and shared between teacher and student. In this way, consistent distillation points with customized quantity become available. Considering the imbalance of positive and negative regions in object detection, we weigh the importance of every distillation point with its foreground probability predicted by the teacher model. To further explore the extensibility of KD-DETR, we provide a comprehensive sampling strategy with both random distillation points which are focused on the general knowledge, and the teacher object queries which represent the regions the teacher concerns.

To the best of our knowledge, this is the first work to propose a general knowledge distillation paradigm for DETR. In this paper, we first provide a thorough experimental and theoretical analysis of the degraded failure of knowledge distillation in DETR. Based on the analysis, we design a novel KD-DETR that significantly improves the performance of DETR distillation. KD-DETR has both flexibility to different DETR architectures, even for teacher and student with different object query settings, and immense potential for scalability. To demonstrate the effectiveness and generalization of KD-DETR, we conduct extensive experiments on the MS COCO2017\cite{lin2014microsoft} dataset on various DETR methods, compressing the scale of backbone and the number of transformer layers. As is shown in Fig \ref{overview}, our method significantly boosts the performance of student models. DAB-DETR with ResNet-18 and ResNet-50 backbone achieves 41.4$\%$, 45.7$\%$ mAP, respectively, which are 5.2$\%$, 3.5$\%$ higher than the baseline, and ResNet-50 even surpasses the teacher model of ResNet-101 by 2.2$\%$; and Deformable DETR with ResNet-18 reaches 43.0$\%$ mAP, 2.9$\%$  higher than the baseline.

\section{Related Work}
\subsection{Classic Object Detection}

Classic detectors with CNN view object detection as a verification task with a sliding window on the image to generate anchors. The mainstream detectors can be divided into one-stage detectors\cite{tian2019fcos}\cite{li2020generalized} and two-stage detectors\cite{ren2015faster}\cite{cai2019cascade}. One-stage detectors, such as Retinanet\cite{lin2017focal}, YOLO\cite{redmon2016you}, and FCOS\cite{tian2019fcos}, directly predict the category and regression of anchors on each pixel of the feature maps. While two-stage detectors such as Faster-RCNN\cite{ren2015faster} and its variants\cite{pang2019libra}\cite{yan2019meta}\cite{zhang2019cad} introduce a Region Proposal Networks(RPN) to generate proposals, and a ROIPool or ROIAlign\cite{wang2019distilling} to extract features of each region proposal for further classification and regression refinement. Both one-stage and two-stage detectors require post-processing, such as NMS, to remove duplicate predictions.

\subsection{Detection Transformer}

\cite{carion2020end} first propose an end-to-end transformer-based detector without any post-processing. Different from classic object detection, DETR interprets object detection as a set-prediction problem with bipartite matching. 

Lots of follow-up focus on the slow convergence of DETR\cite{dai2021dynamic}\cite{sun2021rethinking}\cite{gao2021fast}\cite{zhang2022accelerating}. Deformable DETR\cite{zhu2020deformable} focuses on the multi-head attention, and introduces a deformable attention module by generating reference points for query elements. Each reference point will only concentrate on a small number of locations on the whole feature map. An alternative way is to add more prior information to the object queries in the decoder. Conditional-DETR\cite{meng2021conditional} decouples the context and position features in object queries and generates position features by spatial location. DAB-DETR\cite{liu2022dab} further introduces the width and height information to the positional features. Anchor DETR\cite{wang2022anchor} also encodes the anchor points as object queries with multiple patterns, and further designs a row-column decouple attention to reduce memory cost. The recent work of DINO\cite{zhang2022dino} draws the existing novel techniques, and further exerts the potential of DETR by enlarging the scale of model and datasets.

Besides, another problem in DETR is the scale and computation cost of the model. Current works solving this problem by designing more efficient DETR architecture. Sparse DETR\cite{roh2021sparse} reduces the computation cost by sparsifying encoder tokens. Efficient DETR\cite{yao2021efficient} otherwise introduce RPN to generate object queries and eliminate the cascading decoder layers in DETR. PnP DETR\cite{wang2021pnp} shorten the length of sampled feature with a poll and pool sampling module.


\subsection{Knowledge Distillation}

Knowledge distillation is a widely-used method for model compression and accuracy boosting by transferring the knowledge in a large cumbersome teacher model to a small student. \cite{hinton2015distilling} first propose the concept of knowledge distillation, where the student mimics the soft predictions from teacher. Knowledge distillation has been utilized in various fields including classification\cite{zhao2022decoupled}\cite{tian2019contrastive}\cite{beyer2022knowledge}, object detection\cite{yang2022focal}\cite{zhang2020improve}, semantic segmentation\cite{yang2022cross}\cite{liu2019structured}. According to the objective of mimicking, knowledge distillation can be divided into three categories: response-based\cite{zhao2022decoupled}, feature-based\cite{heo2019comprehensive}\cite{yang2022masked} and relation-based\cite{yim2017gift}\cite{zagoruyko2016paying}, which distill with logits, intermediate activations and the relation of features in different layers respectively.

Recently, several works focus on applying knowledge distillation to object detection\cite{guo2021distilling}\cite{chen2017learning}\cite{kang2021instance}. \cite{chen2017learning} successfully distill the features on the neck, the classification head, and the regression head, while \cite{li2017mimicking} choose to distill the logits and features from the RPN head. To overcome the imbalance of foreground and background, \cite{wang2019distilling} introduce fine-grained mask to focus on the region close to ground-truth bounding boxes, \cite{dai2021general} pay more attention to the regions where teacher and student are divided in predictions. However, the modern knowledge distillation methods for object detection are built upon the architecture of CNN-based detectors, and are not suitable for DETR due to the completely different transformer architecture.

Since \cite{dosovitskiy2020image} introduce transformer to vision task, several works\cite{touvron2021training}\cite{zhang2022minivit} have verified the effectiveness of knowledge distillation on ViT. However, these methods perform trivially or even harmfully on DETR, and there is still a lack of research on DETR distillation.
%However, the modern knowledge distillation methods for object detection are built upon the architecture of CNN-based detectors, and perform trivially or even harmfully on DETR due to its completely different architecture. While some works\cite{touvron2021training}\cite{zhang2022minivit} have verified the effectiveness of knowledge distillation on ViT, 
\vspace{-2pt}
\section{A Closer Look at DETR Distillation}
\vspace{-2pt}
In this section, we first revisit the DETR architecture briefly. Then we conduct a series of classic knowledge distillation experiments on DETR to reveal that the core of DETR distillation is to obtain sufficient and consistent distillation points. 

\subsection{Revisiting DETR}
DETR is built upon the encoder-decoder architecture of transformer. The encoder takes pixels of the feature map from backbone as input for multi-head self-attention to extract context features $X\in\mathbb{R}^{HW\times D}$, where $HW$ denotes the resolution of the feature map, and $D$ denotes the feature dimension. The decoder takes the features from encoder and a set of learnable object queries $\mathbf{Q}=\{q_i\in\mathbb{R}^D|i=1,...,N\}$ as input, where N denotes the number of queries. Each object query is an abstract feature describing a certain instance, and will probe and pool the features from encoder through cross-attention to make predictions of category $\mathbf{C}=\{\mathbf{c}_i\in\mathbb{R}^K|i=1, ..., N\}$ and location $\mathbf{B}=\{\mathbf{b}_i=(bx_i, by_i, bw_i, bh_i)|i=1, ..., N\}$, where K denotes the number of categories. Finally, the Hungarian algorithm is used to find a bipartite matching between ground truth and predictions of object queries. 
\begin{table}[t]
\begin{center}
\begin{tabular}{c|ccc}
\toprule[2pt]
Strategy & AP & $AP_{50}$ & $AP_{75}$  \\
\hline
Baseline & 36.2 &56.1 &44.7 \\
Inconsistent  &35.1 & 55.2& 36.7\\
Similar Foreground  &37.2 & 57.4 & 39.9  \\
Similar General & 37.4 & 58.0 &  40.6\\
\bottomrule[2pt]
\end{tabular}
\end{center}
 \vspace{-10pt}
\caption{Distillation with Different Distillation Points. Inconsistent distillation points are harmful for distillation. }
 \vspace{-10pt}
\label{inconsistent}
\end{table}
\subsection{Consistent Distillation Points}
The core idea of knowledge distillation is forcing the student to mimic the prediction of teacher, which can be interpreted as matching the mapping function of student and teacher with a set of distillation points. Distillation points refer to the corresponding input $\mathbf{x}$ of the predictions, as $\mathbf{y}=f(\mathbf{x})$, where $f$ represents the model. In this view, the distillation points should kept sufficient in quantity and consistent between teacher and student models for effective and reliable matching. However, comparing the formulation of CNN detector and DETR, a critical challenge of DETR distillation lies in the lacking of consistent distillation points.
\begin{figure*}[t]
  \centering
  %\fbox{\rule{0pt}{2in} \rule{0.9\linewidth}{0pt}}
   \includegraphics[width=0.85\linewidth]{framework1.eps}
   \vspace{-5pt}
   \caption{\textbf{KD-DETR architecture} KD-DETR decouples the distillation and detection tasks by introducing a set of distillation points shared between teacher and student models. For distillation, the distillation points are served as the query of the transformer decoders for both teacher and student models, and the student will mimic teacher's classification and box location predictions. The weight of each distillation point is measured by its foreground probability predicted by teacher. For detection task, the original object queries are processed by the student decoder for final prediction.}
    \vspace{-10pt}
   \label{framework}
\end{figure*}
 \vspace{-3pt}
 
Classic detector degrades object detection to a verification problem which combines classification and regression, and introduces a set of anchors to specify the region for verification. In this way, the formulation of distillation points consists of the image and the location and scale of anchor $\mathbf{x} = (\mathbf{I}, \mathbf{anchor})$. As the anchors are generated through the sliding window strategy with handcrafted shapes, the locations and scales of anchors are implicitly embedded in the model architecture as prior information. Since student and teacher models share the same or similar architectures, a large number of object proposals generated by teacher and student models naturally have strict spatial correspondence, even for those background regions with low confidence. With the spatial correspondence which can be viewed as an inductive bias of CNN, classic detector naturally guarantees a sufficient number of consistent distillation points. 

In contrast, the DETR formulates object detection as a set-prediction problem. The distillation points, therefore, become the combination of the image and the object queries $\mathbf{x} = (\mathbf{I}, \mathbf{q})$. However, the object queries in different models are egocentric, as they are initialized and optimized by themselves independently. Since object queries play the role of probing and pooling the features of certain instances, they have inconsistent concentration preferences in different models. Consequently, the formulation of DETR naturally lacks the ability to provide sufficient distillation points with strict consistency between teacher and student models, and the predictions acquired from teacher are not informative or reliable for student to mimic.

%As transformer encoder acts in a similar manner as the backbone to extract context features, and is task-agnostic. 
\subsection{Distillation with Inconsistent Distillation Points}
To validate the analysis above that the sufficiency and consistency of distillation points is the essential challenge in DETR distillation, we start with applying the original logit-based distillation method\cite{chen2017learning} in classic detector to DETR, which mimics the category and box location logits predictions of teacher model. 
%The distillation loss is applied as follow:
%\begin{equation}
%    Loss = \sum_i^{M} KL(\mathbf{c}^s_i\Vert \mathbf{c}^t_i) + (\mathbf{b}_i^s - \mathbf{b}_i^t)^2,
%\end{equation}
%where M is the number of distillation points.
We examine three distillation points strategies: \textit{Inconsistent}, \textit{Similar Foreground}, and \textit{Similar General}. In \textit{Inconsistent}, all the object queries are viewed as distillation points with their original permutation; In \textit{Similar Foreground}, only object queries matched to ground truth in bipartite matching will be used as distillation points, and will permute with the same order of ground truth label; \textit{Similar General} further increase the number of distillation points by viewing the average of negative object queries in bipartite matching as a general background distillation point.  Experiments are conducted on DAB-DETR and evaluated on MS COCO2017, with ResNet18 as student and ResNet-50 as teacher. 
\begin{table*}[th]
\begin{center}
\begin{tabular}{cc|c|c|ccccc|cc}
\toprule[2pt]
Models &  & Epochs & AP & $AP_{50}$ & $AP_{75}$ & $AP_{S}$ & $AP_{M}$& $AP_{L}$ & FPS & Params \\
\hline
DAB-DETR& ResNet-50(T)& 50 & 42.1 &63.1 & 44.7&21.5 &45.7  &69.3 & 32 & 44M\\
& ResNet-18(S)& 50 & 36.2 &56.1 & 37.9& 16.9&39.0 & 53.5& 76 & 31M\\
& Ours& 50 & 41.4 & 61.4& 44.2&21.2 &44.7 &58.7 &  76 &31M\\
& Gains&  & \textbf{+5.2} & \textbf{+5.3}& \textbf{+6.3}&\textbf{+4.3} &\textbf{+5.7} &\textbf{+5.2} & &\\
\cline{2-11}
& ResNet-34(S)& 50 & 40.6 & 60.9 & 43.3 & 21.4 & 44.3 & 56.3 & 51 &42M \\
& Ours& 50 & 44.1 & 64.4 & 47.3 & 24.6 & 47.8 & 61.0 & 51 & 42M\\
& Gains&  &\textbf{+3.5} & \textbf{+3.5}& \textbf{+4.0} &\textbf{+3.2} &\textbf{+3.5} &\textbf{+4.7} &\\
\hline
DAB-DETR& ResNet-101(T)& 50 & 43.5 &63.9 & 44.6& 23.6& 47.3& 61.5 & 19 & 63M\\
& ResNet-50(S)& 50 & 42.1 &63.1 & 44.7&21.5 &45.7  &60.3 & 32 & 44M \\
& Ours& 50 & 45.7 &66.3 &49.4 &26.4 &49.8 &62.7 & 32 & 44M\\
& Gains&  & \textbf{+3.2} &\textbf{+3.2} &\textbf{+4.7} &\textbf{+4.9} &\textbf{+4.1} &\textbf{+2.4} & &\\
\hline
Deformable-DETR& ResNet-50(T)& 50 & 43.8 &62.6 &47.7 & 26.4& 47.1& 58.0 & 22&40M\\
& ResNet-18(S)& 50 & 40.1 & 58.1&43.7 & 22.4& 42.8&54.2 & 40 & 27M\\
& Ours& 50 & 43.4 & 61.8& 47.4&25.2 & 46.6& 57.7& 40 &27M\\
& Gains & & \textbf{+3.3} & \textbf{+3.7} & \textbf{+3.7} & \textbf{+2.8} & \textbf{+3.8} & \textbf{+3.5}\\
\hline
Deformable-DETR& ResNet-101(T)& 50 & 44.6 &63.7&48.5 &26.3 &48.2 & 58.7 & 15 &59M\\
& ResNet-50(S)& 50 & 43.8 &62.6 &47.7 & 26.4& 47.1& 58.0 & 22 & 47M  \\
& Ours& 50 & 46.1 & 65.6 & 50.6 & 28.3 & 50.0 & 60.7 &22 &47M\\
& Gains & & \textbf{+2.3} & \textbf{+3.0} & \textbf{+2.9} & \textbf{+1.9} & \textbf{+2.9} & \textbf{+2.7}\\
 \hline
DAB-Deformable& ResNet-50(T)& 50 & 46.8 & 66.0 & 50.4 & 29.1 & 49.8 & 62.3  & 19 & 47M\\
& ResNet-18(S)& 50 & 45.8 & 63.5 & 50.1 & 29.6 & 48.5 & 59.0 & 30 & 34M\\
& Ours& 50 & 47.2&65.4 & 51.7 & 31.5 & 50.3 & 60.6 & 30 & 34M \\
& Gains & & \textbf{+1.4} & \textbf{+1.9} & \textbf{+1.6} & \textbf{+1.9} & \textbf{+1.8} & \textbf{+1.6}\\
\hline
DINO& ResNet-50(T)& 36 & 50.9 & 69.0 & 55.3 & 34.6& 54.1& 64.6 &18 & 47M\\
& ResNet-18(S)& 12 & 44.0 & 61.2 & 48.1& 27.4 & 46.9 & 56.9& 29 & 34M\\
& Ours& 12 & 45.4 & 62.2 & 49.3 & 27.3 & 48.2 & 59.0 &29 &34M\\
& Gains&  & \textbf{+1.4} & \textbf{+1.0} & \textbf{+1.2} & -0.1 & \textbf{+1.3} &\textbf{+2.1}& &\\
%\hline
%DAB-Deformable& ResNet-101(T)& 50 & 49.4 & 68.1 & 54.0 & 33.5 & 53.1 & 63.7 & &66M\\
%& ResNet-50(S)& 50 & 46.8 & 66.0 & 50.4 & 29.1 & 49.8 & 62.3  & & 48M  \\
%& Ours& 50 &  & & & & & & &48M\\
%& Gains & \\
\bottomrule[2pt]
\end{tabular}
\end{center}
 \vspace{-10pt}
\caption{Results of the proposed KD-DETR with different DETR detectors and backbones with various scale.}
 \vspace{-10pt}
\label{results}
\end{table*}

%\begin{table*}[th]
%\begin{center}
%\begin{tabular}{cc|c|c|ccccc|cc}
%\toprule[2pt]
%Models &  & Epochs & AP & $AP_{50}$ & $AP_{75}$ & $AP_{S}$ & $AP_{M}$& $AP_{L}$ & GFLOPs & Params \\
%\hline
%DINO& ResNet-50(T)& 36 & 50.9 & 69.0 & 55.3 & 34.6& 54.1& 64.6 &279G & 47M\\
%& ResNet-18(S)& 12 & 44.0 & 61.2 & 48.1& 27.4 & 46.9 & 56.9&  & 34M\\
%& Ours& 12 & 45.4 & 62.2 & 49.3 & 27.3 & 48.2 & 59.0 & &\\
%& Gains&  & +1.4 & +1.0 & +1.2 & -0.1 &+1.3 &+2.1& &\\
%\bottomrule[2pt]
%\end{tabular}
%\end{center}
%\caption{Results of the proposed KD-DETR on DINO.}
%\label{results}
%\end{table*}
As shown in Table \ref{inconsistent}, \textit{Inconsistent} distillation points results in great degradation of the student model with unreliable knowledge from teacher model; \textit{Similar Foreground} with semantic-similar foreground distillation points alleviate the problem; and \textit{Similar General} achieves further improvement by increasing the number of distillation points with general background features. The preliminary experiments validate that the sufficiency and consistency of distillation points are of prime importance to improve the performance of student model in DETR distillation. 

\section{KD-DETR}
To address the lack of consistent distillation points in DETR, we propose a general knowledge distillation paradigm for DETR with consistent distillation points sampling, called KD-DETR. As illustrated in Fig \ref{framework}, KD-DETR introduces a set of specialized object queries $\mathbf{\tilde{q}}$ shared between teacher and student models to construct distillation points. Decoupling the distillation task and detection task, KD-DETR can provide sufficient and consistent distillation points. We denote the original input and sampled distillation points as $\mathbf{x}=\{\mathbf{I}, \mathbf{q}\}, \mathbf{\tilde{x}}=\{\mathbf{I}, \mathbf{\tilde{q}}\}$ respectively.

For detection task, the student model is first optimized by its original detection loss: the original input $\mathbf{x}$ is fed into student model to make category and box location predictions, which will be assigned to the ground truth with bipartite matching and calculate detection loss.
%\begin{equation}
%    \mathcal{L}_{det} = \lambda_{cls}\mathcal{L}_{cls} + %\lambda_{L1}\mathcal{L}_{L1} + %\lambda_{giou}\mathcal{L}_{giou},
%\end{equation}
%where $\mathcal{L}_{cls}$ is the cross-entropy classification loss, $\mathcal{L}_{L1}$ and $\mathcal{L}_{giou}$ represent the L1 and GIoU loss for localization.

For distillation task, the sampled distillation points $\mathbf{\tilde{x}}$ will be fed into both student and teacher to make category and location predictions $\mathbf{c}, \mathbf{b}$:
\begin{align}
    & \mathbf{c^s}, \mathbf{b^s} = f^s(\mathbf{I}, \mathbf{\tilde{q}}), \\
    & \mathbf{c^t}, \mathbf{b^t} = f^t(\mathbf{I}, \mathbf{\tilde{q}}), 
\end{align}
where $f^s, f^t$ refer to student and teacher model respectively. The distillation loss is calculated with following form:
\begin{equation}
\begin{split}
    \mathcal{L}_{distill}  = &\sum_{i=1}^{M} [\lambda_{cls} \mathcal{L}_{KL}(\mathbf{\hat{c}}^t_i\Vert \mathbf{\hat{c}}^s_i) + \lambda_{L1}\mathcal{L}_{L1}(\mathbf{b}_i^s, \mathbf{b}_i^t), \\
    +& \lambda_{GIoU}\mathcal{L}_{GIoU}(\mathbf{b}_i^s, \mathbf{b}_i^t) ], %\\
    %KL(\mathbf{c}^t_i\Vert \mathbf{c}^s_i)& = \sum_{j=1}^{K}(-c^t_{i,j} \log{c^s_{i,j}}))
\label{distill_loss}
\end{split}
\end{equation}
where M denotes the number of distillation points. For classification, we choose the KL-divergence $\mathcal{L}_{KL}$ as distillation loss with temperature T: $\mathbf{\hat{c}} = SoftMax(\frac{\mathbf{c}}{T})$. For box regression, $\mathcal{L}_{L1}$ and $\mathcal{L}_{GIoU}$ represent the L1 and GIoU loss for location distillation, which have the same formulation with detection loss. $\lambda_{cls}, \lambda_{L1}$ and $\lambda_{GIoU}$ represent the coefficient of corresponding loss.

\subsection{Distillation Points Sampling}
Generally, object queries are a set of abstract features responsible for certain objects by probing and pooling the context features from encoder. Existing works interpret object queries as anchors or reference points, revealing that each object query is sensitive to a particular region on the feature maps. Following this perspective, we provide a comprehensive general-to-specific sampling strategies for distillation points sampling: $\mathbf{\tilde{q}} = \{\mathbf{q_g}, \mathbf{q_s}\}$. 

\textbf{General Sampling with Random Initialized Queries}

In general sampling, we hope to probe teacher's general responses on different locations of the features by sparsely scanning the whole feature maps. Therefore, we randomly initialize a set of object queries, which are uniformly distributed on the features, to construct the general distillation points: $\mathbf{q_g}=\{\mathbf{q_i}\sim\mathcal{U}(0, 1)|i=1,...,M_g\}$, where $M_g$ denotes the number of general distillation points. To learn more general knowledge from teacher, we leave these distillation points unlearnable during training.

\textbf{Specific Sampling with Teacher Queries}

While the general sampling provides a global retrieval of the features, we further propose a specific sampling strategy, focusing on those regions where teacher pays more attention. An intuitive way for specific sampling is to directly reuse the well-optimized object queries in teacher model:$\mathbf{q_s}=\mathbf{q_{teacher}}$. While teacher model is learned to concentrate more on these object queries, the predictions in these areas are more precise and informative.

\textbf{Foreground Attention Weight}

The imbalance between foreground and background regions is one critical problem in object detection distillation, not special in DETR. An intuitive way is to adopt the classification scores predicted by teacher model as the attention weights for distillation points. Concretely, those distillation points with higher scores when classified to a certain object are regarded as foreground distillation points, and are more important in knowledge distillation.
\vspace{-5pt}
\begin{equation}
    w_i = \max_{c\in[0, K]}p^t(y_c|\mathbf{q}_i),
\end{equation}

where $w_i$ denotes the attention weight of $\mathbf{q}_i$, and $p^t(y_c|\mathbf{q}_i)$ denotes the probability of $\mathbf{q}_i$ assigned to category $c$ predicted by teacher model. In this way, the distillation loss in Eq.~\eqref{distill_loss} will be writen as follow:
\begin{equation}
\begin{split}
    \mathcal{L}_{distill}  = &\sum_{i=1}^{M} w_i[\lambda_{cls} \mathcal{L}_{KL}(\mathbf{\hat{c}}^t_i\Vert \mathbf{\hat{c}}^s_i) \\
    + &\lambda_{L1}\mathcal{L}_{L1}(\mathbf{b}_i^s, \mathbf{b}_i^t) 
    + \lambda_{GIoU}\mathcal{L}_{GIoU}(\mathbf{b}_i^s, \mathbf{b}_i^t) ] 
\label{distill_coarse}
\end{split}
\end{equation}

\section{Experiment}
To validate the effectiveness and generalization of KD-DETR, we evaluate it on different DETR architectures, with various scales of backbones and the number of transformer layers. To support our analysis of consistent distillation points, we further conduct an extensive ablation study. 

\subsection{Experimental Settings}
\textbf{Datasets}: All the proposed experiments are evaluated on MS COCO2017\cite{lin2014microsoft} spanning 80 categories, with the default split of ~117k training images for training and 5k validate images for testing.  Standard COCO evaluation metrics are adopted for validation.


\textbf{Implementation Details}: As KD-DETR is a plug-and-play distillation module, we follow the original settings of hyper-parameters, learning rate scheduler and optimizer of each model for training. We choose ResNet\cite{he2016deep} as backbone, which are pre-trained on ImageNet1K\cite{krizhevsky2017imagenet}. For distillation task, we set hyper-parameters of the coefficient of the distillation loss as $\lambda_{kl}=1, \lambda_{L1}=5, \lambda_{GIoU}=2$. The number of General distillation points is $300, 300, 300, 900$ for DAB-DETR, Deformable DETR, DAB-Deformable-DETR, and DINO. We train our model on Nvidia A100 GPUs with batch size set to 16 in total. The reported FPS is measured on a single Nvidia A100 GPU.

\subsection{Distillation on Different DETR benchmarks}
In this paper, we evaluate our method on four typical DETR architectures: DAB-DETR\cite{liu2022dab}, which is a single-scale DETR with special object query settings; Deformable-DETR\cite{zhu2020deformable}, which is a multi-scale DETR with deformable attention module; and the combination DAB-Deformable-DETR\cite{liu2022dab}. We additionally provide an additional experiment on DINO\cite{zhang2022dino}, which is the SOTA DETR model combining a series of novel techniques including deformable attention, two-stage object queries settings, and de-noising module, to evaluate our effectiveness on well-optimized model with high accuracy.

The results are illustrated in Table\ref{results}. For DAB-DETR, KD-DETR significantly boosts the performance of ResNet-18, ResNet-34, and ResNet-50 with $5.2\%$, $3.5\%$, and $3.2\%$ mAP improvement respectively. Note that student with ResNet-34 and ResNet-50 even surpass the teacher by $2.0\%$ and $2.2\%$ mAP. Deformable DETR with ResNet-18 and ResNet-50, KD-DETR achieve $3.3\%$ and $2.5\%$ mAP gains, and student with ResNet-50 also exceed the teacher with $1.5\%$ mAP. For DAB-Deformable-DETR, KD-DETR achieves $1.4\%$ mAP higher than the baseline model with ResNet-18, and surpasses the teacher model with ResNet-50 by $0.4\%$. For DINO with ResNet-18, KD-DETR also improves $1.4\%$ mAP with 12 epoch training scheduler.

\subsection{Distillation on the Transformer Layers}
Besides the scale of backbone, the layer number of transformer encoder and decoder is also an important factor of the model size and computation cost in DETR. In this paper, we also conduct experiments to compress the layer numbers of transformer to validate the scalability of KD-DETR, 

Table \ref{layer} shows the results of KD-DETR on DAB-DETR, with backbone of ResNet-50 as teacher and ResNet-18 as student. While decreasing the number of transformer layers will cause great degradation in the performance, KD-DETR can significantly boost the student model. For example, when compressing the layer number of decoder to 4, KD-DETR can still improve the performance of student to $41.0\%$, and the student model with 2 encoder layers and 6 decoder layers can outperform the full-scale model for $2.8\%$ mAP with 1.2x FPS improvement.

\begin{table}[t]
\begin{center}
\begin{tabular}{c|ccc|cc}
\toprule[2pt]
Enc/Dec & AP & $AP_{50}$ & $AP_{75}$ & FPS & Params  \\
\hline
6/6 & 36.2 & 56.1 & 37.9 & 76 & 31M \\
Ours & 41.4(\textbf{+5.2}) & 61.4 & 44.2 & 76 & 31M \\
2/6 &  36.2&56.3 & 38.4 & 102 & 27M\\
Ours & 39.0(\textbf{+2.8}) & 58.9&  41.7 & 102 & 27M \\
6/4 & 35.7 & 55.1 & 37.8 & 79 & 28M  \\
Ours & 41.0(\textbf{+5.3}) & 60.8 & 43.7 & 79 & 28M \\
6/2 &31.8 & 49.8&  33.5 & 82 & 25M\\
Ours & 38.9(\textbf{+7.0})& 58.0& 41.6 & 82 & 25M \\
\hline
4/4 &  35.8 & 54.7 & 38.1  & 91 &24M \\
Ours &  38.8(\textbf{+3.0}) & 58.7 &  41.1  & 91 &24M\\
\hline
2/2 & 29.3 & 46.6 & 37.0 & 113 &17M \\
Ours & 32.8(\textbf{+3.5}) &  52.6 & 34.2 &113 &17M\\
\bottomrule[2pt]
\end{tabular}
\end{center}
 \vspace{-10pt}
\caption{\textbf{Distillation on Transformer Layers}: Compressing the number of encoder layers and decoder layers with KD-DETR}
 \vspace{-5pt}
\label{layer}
\end{table}
\subsection{Ablation Study}
The ablation study is conducted on MS COCO2017 with DAB-DETR with backbone of ResNet50 as teacher and ResNet18 as student. 

\textbf{Analysis on the Benefit of Knowledge Distillation}

In KD-DETR, we introduce a set of specialized distillation points in the training process to the original DETR architecture. \cite{zhang2022dino}\cite{li2022dn} have proven that the number of object queries will affect the performance of model. To validate that the benefit is from knowledge distillation rather than the increase of object queries, we conduct an ablation experiment by applying an additional set of object queries with the same number of distillation points.

As shown in Table \ref{benefit}, simply increasing the number of object queries only achieves trivial gains, and the main contribution of boosting the model's performance is from the knowledge transferred from teacher with distillation points.

\begin{table}[t]
\begin{center}
\begin{tabular}{cc|ccc}
\toprule[2pt]
\makecell[c]{Object\\ Queries} & \makecell[c]{Distillation \\ Points}& AP & $AP_{50}$ & $AP_{75}$  \\
\hline
300 & -  & 36.2   & 56.1 &  37.9 \\
300+300 & - & 37.7  &58.0 &  41.1 \\
300 & 300 & 40.2  & 60.7 & 42.8 \\
\bottomrule[2pt]
\end{tabular}
\end{center}
 \vspace{-5pt}
\caption{\textbf{Benefit of Knowledge Distillation}: Simply increasing the set of object queries leads to trivial improvement}
 \vspace{-15pt}
\label{benefit}
\end{table}

\textbf{Analysis on the Distillation Points Sampling Strategy}

A comprehensive general-to-specific distillation points sampling scheme is introduced in this paper, including three strategies: General Sampling, Specific Sampling, and Foreground Attention Weight. 

The results are illustrated in Table \ref{strategy}. The general sampling strategy with randomly initialized queries can boost the performance of student model for $2.5\%$ mAP, while the specific sampling strategy with teacher queries achieves an improvement of $3.8\%$. When further refining the general distillation points with foreground attention weight, the general sampling strategy and specific sampling strategy yield larger gains for $3.7\%$ and $4.0\%$, respectively. 

It is also important to note that the performance of general sampling with foreground attention weight is fairly close to the specific sampling. Since the teacher queries in specific sampling are well-optimized and concentrate more on the foreground regions, such phenomena validate that the general sampling with randomly initialized queries can probe the whole feature maps evenly, and foreground attention weight can help the student focus more on the foreground regions. In addition, the combination of the three strategies further promotes the performance for $5.2\%$, indicating that the specific distillation points bring more information from the concentration of teacher model.
\begin{table}[t]
\begin{center}
\begin{tabular}{ccc|ccc}
\toprule[2pt]
General & Specific & FGW & AP & $AP_{50}$ & $AP_{75}$  \\
\hline
&&& 36.2 & 56.1 & 37.9 \\
$\surd$&  &  & 38.7 & 59.1 & 40.7 \\
$\surd$& & $\surd$ & 39.9 & 60.2& 42.5  \\
& & $\surd$  & 40.0 & 60.5 & 42.4  \\

& $\surd$& $\surd$ & 40.2 &60.7 & 42.8 \\
$\surd$& $\surd$& $\surd$ & 41.4 &61.4 &44.2 \\
\bottomrule[2pt]
\end{tabular}
\end{center}
\vspace{-5pt}
\caption{Distillation with Different Distillation Points Sampling Strategies. FGW refers to Foreground Attention Weight.}
\label{strategy}
\end{table}

\textbf{Number of General Distillation Points}

We investigate the influence of the number of general distillation points by varying the number from 10 to 900. As is shown in Table \ref{number}, the improvement of performance is significant when increasing the distillation points from 10 to 50, while gradually saturating when continuing to increase to 300. There is even a slight degradation in performance when further increasing the number. These phenomena validate that the general distillation points can effectively probe the knowledge from teacher's attention in different regions when sparsely distributed on the feature maps. However, dense distillation points will introduce more noise of background information, and will be harmful for distillation.
\begin{table}[t]
\begin{center}
\begin{tabular}{c|ccccc}
\toprule[2pt]
Number & 10 & 50 & 100 & 300 & 900  \\
\hline
AP             & 38.6 & 39.3  & 39.5  & 39.9 & 39.5\\
$AP_{50}$& 58.3 & 59.2  & 59.3 & 60.2& 60.4\\
$AP_{75}$& 41.0 & 41.3  & 41.8 &  42.5 & 41.7 \\
$AP_S$    & 19.5 & 20.5  & 20.3 & 20.6& 20.7\\
$AP_M$.  & 41.8 &  42.6 & 42.7 & 42.9& 42.6\\
$AP_L$.   & 55.5 & 55.6  & 56.5 & 57.0& 56.5\\
\bottomrule[2pt]
\end{tabular}
\end{center}
 \vspace{-8pt}
\caption{Distillation with Different Number of General Distillation Points}
\vspace{-15pt}
\label{number}
\end{table}

\textbf{Visualization of the Distillation Points}
\begin{figure}[t]
  \centering
  \begin{subfigure}{0.32\linewidth}
  \centering
   \includegraphics[width=1\textwidth]{fig0_t.jpg}
    %\fbox{\rule{0pt}{2in} \rule{.9\linewidth}{0pt}}
    %\caption{Attention Map of Teacher Model.}
    \label{fig:short-a}
  \end{subfigure}
  \begin{subfigure}{0.32\linewidth}
  \centering
   \includegraphics[width=1\textwidth]{fig0_ori.jpg}  
    %\caption{Attention Map of Original Student Model.}
    \label{fig:short-b}
  \end{subfigure}
   \begin{subfigure}{0.32\linewidth}
   \centering
   \includegraphics[width=1\textwidth]{fig0_s.jpg}  
    %\caption{Attention Map of Student Model.}
    \label{fig:short-b}
  \end{subfigure}\\ 
 
     \vspace{-10pt}
  \begin{subfigure}{0.32\linewidth}
  \centering
   \includegraphics[width=1\textwidth]{fig1_t.jpg}
    %\fbox{\rule{0pt}{2in} \rule{.9\linewidth}{0pt}}
    \caption{Teacher Model.}
    \label{fig:short-a}
  \end{subfigure}
  \begin{subfigure}{0.32\linewidth}
  \centering
   \includegraphics[width=1\textwidth]{fig1_ori.jpg}  
    \caption{Student Model.}
    \label{fig:short-b}
  \end{subfigure}
   \begin{subfigure}{0.32\linewidth}
   \centering
   \includegraphics[width=1\textwidth]{fig1_s.jpg}  
    \caption{KD-DETR.}
    \label{fig:short-b}
  \end{subfigure}\\ 

   \caption{Attention Map Visualization of Specific Distillation Points: Images from left to right are from teacher, original student and student with KD-DETR. The corresponding instance of the distillation points are marked with red rectangles. }
    \vspace{-5pt}
  \label{specific}
\end{figure}
  
\begin{figure}[t]
  \centering
    
    \begin{subfigure}{0.32\linewidth}
  \centering
   \includegraphics[width=1\textwidth]{fig0_random0.jpg}
    %\fbox{\rule{0pt}{2in} \rule{.9\linewidth}{0pt}}
%    \caption{An example of a subfigure.}
    \label{fig:short-a}
  \end{subfigure}
  \begin{subfigure}{0.32\linewidth}
  \centering
   \includegraphics[width=1\textwidth]{fig0_random1.jpg}  
%    \caption{Another example of a subfigure.}
    \label{fig:short-b}
  \end{subfigure}
   \begin{subfigure}{0.32\linewidth}
   \centering
   \includegraphics[width=1\textwidth]{fig0_random2.jpg}  
 %   \caption{Another example of a subfigure.}
    \label{fig:short-b}
  \end{subfigure}\\ 
   \vspace{-10pt}
       \begin{subfigure}{0.32\linewidth}
  \centering
   \includegraphics[width=1\textwidth]{fig1_random1.jpg}
    %\fbox{\rule{0pt}{2in} \rule{.9\linewidth}{0pt}}
  %  \caption{An example of a subfigure.}
    \label{fig:short-a}
  \end{subfigure}
  \begin{subfigure}{0.32\linewidth}
  \centering
   \includegraphics[width=1\textwidth]{fig1_random2.jpg}  
%    \caption{Another example of a subfigure.}
    \label{fig:short-b}
  \end{subfigure}
   \begin{subfigure}{0.32\linewidth}
   \centering
   \includegraphics[width=1\textwidth]{fig1_random3.jpg}  
%    \caption{Another example of a subfigure.}
    \label{fig:short-b}
  \end{subfigure}\\ 
   \vspace{-10pt}
 \caption{Attention Map Visualization of General Distillation Points: Teacher model's attention of general distillation points focuses more on the background regions, providing more information for student to mimic.}
 \vspace{-10pt}
  \label{general}
\end{figure}
In order to understand what knowledge has been transferred from teacher to student better, we visualize the attention map of different kinds of distillation points. Fig \ref{specific} illustrate the attention of the same specific distillation points from teacher model, student model and KD-DETR. It can be seen that the specific distillation points focus more on the features of foreground regions. While the attention of the original student model without distillation is different from the teacher model, KD-DETR can align the concentration of teacher and student models. On the contrary, the general distillation points, as illustrated in Fig \ref{general}, concentrate more on the background regions, and objects that are not included in the ground truth annotations, which can provide more additional semantic information to the student. In this way, the combination of general and specific distillation points can provide more comprehensive knowledge.
\section{Conclusion}
In this paper, we study the compression of DETR with knowledge distillation. We provide a thorough experimental and theoretical analysis of the failure of knowledge distillation on DETR. Based on the analysis, we propose the first general knowledge distillation paradigm for DETR(KD-DETR), together with a comprehensive general-to-specific consistent distillation sampling scheme. We demonstrate the flexibility, generalization, and extensibility of KD-DETR on various DETR architectures, compressing both the scale of backbone and transformer layers, significantly boosting the performance of student model.
%%%%%%%%% REFERENCES
{\small
\bibliographystyle{ieee_fullname}
\bibliography{egbib}
}

\end{document}
