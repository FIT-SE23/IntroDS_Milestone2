


\section{Introduction}
\label{sec:intro}


Cross-reality systems provide users with access to information and objects between multiple realities 
from either reality. 
In prior work in augmented and mixed reality, 
cross-reality systems are methods that transition the level of virtuality along the reality-virtuality continuum \citep{kishino}.
\citet{10.1145/3491102.3501821} demonstrated how an end-user can change the level of virtuality of their physical environment, from one where real objects become virtual objects, to increasing levels of virtuality of the surrounding physical environment, to a completely virtual environment.
Prototypes such as VRoamer \citep{8798074} or ShareVR \citep{10.1145/3025453.3025683} require significant client-side hardware in enabling real-time cross-reality rendering along the continuum.
Physical and virtual realities tend to be the most common target realities to sample objects, information, and scene artifacts.
Head-mounted displays are the primary 
interface
to access cross-reality rendering.

Motivated by how much time humans spend time in both the physical and digital reality,
we investigate how to support the rendering of objects of one reality in the other.
An underlying assumption is that the physical reality and virtual reality have semantic mappings, where the objects from one reality can be mapped to another reality.
Unlike virtual reality,
the digital reality is not viewed through egocentric vision but through heterogeneous graphical user interfaces (e.g. media ranging from text to video, programs ranging from webpages to apps, operating systems ranging from Android to Windows, devices ranging from mobile to desktop).
The view is different, partly due to the modes of interaction in each reality. 
The physical reality requires physical movement (e.g. gaze, body) for a user to actively seek information,
while the digital reality is designed such that information flows to the user with minimal interaction (at most requiring finger action).
The difference in view and affordances contributes to a non-trivial semantic mapping of objects in the digital reality to a physical reality.
When considering alternative reality where this mapping is not provided, transitioning along the continuum between these realities become challenging. 
We are shifting away from transitioning between a physical and semantically-mapped reality.
We investigate how end-users can assist cross-reality rendering by providing semantic information collaboratively.
Other than a semi-supervised approach to constructing mappings between both realities,
enabling end-users to author or reflect on their experiences have brought benefits in augmented \citep{10.1145/3491102.3517665} and digital \citep{10.1145/3479600} realities.







\noindent
\textbf{Contributions}
We
are the first to 
contribute 
a cross-reality rendering system that allows users to manipulate their digital and physical realities
in real-time with respect to each other. 
We are the first to implement an interface-agnostic 
modification framework, 
which modifies digital interfaces
agnostic to operating system, program, or content.
Human-in-the-loop learning enables users to construct interventions, 
and collaboration scales the pool of interventions. 
Along with implementation details,
we validate requirements
with walkthroughs, personas, and scalability tests.













 
\begin{table*}[t]
\centering
\begin{minipage}{3.5cm}
    \subfigure[\textit{User authentication}: Secure gateway to view history, devices, and camera feed.]{
    \includegraphics[width=2.5cm]{assets/img5.pdf}
    } \\
    \subfigure[\textit{Interface \& interventions selection}: Registered devices and available interventions. 
    ]{
    \includegraphics[width=3cm]{assets/img4.pdf}
    }
\end{minipage}
\begin{minipage}{6cm}
    \subfigure[\textit{Interface access}: Accessing a Linux desktop from another (Linux) desktop browser.]{
    \includegraphics[width=6cm]{assets/img1.png}
    } \\
    \subfigure[\textit{Interface access}: Camera feed on the secondary device (to be loaded in full-screen).]{
    \includegraphics[width=6cm]{assets/camera_2.pdf}
    }
\end{minipage}
\hspace{0.3cm}
\begin{minipage}{3.8cm}
     \subfigure[\textit{Interface access}: Accessing Android emulator from an Android  device.]{
     \includegraphics[width=3.8cm]{assets/img2}
    }
\end{minipage}
\captionof{figure}{\textit{Walkthrough: }The steps taken by a user to access the different re-rendered realities.}
\label{fig:walkthrough}
\end{table*}