\section{Discussion}



















\subsection{
The line between virtual reality and physical reality
}

\citet{kishino} elicited the reality–virtuality continuum, 
enabling the interpolation of elements of reality and elements of virtuality. 
They denote the 'real' reality ('reality') as the reality where real objects have objective (physical) existence, 
and the 'virtual' reality ('virtuality') as the reality where virtual objects exist only in essence or effect.
From this, one could conclude that many realities, or views that a user may perceive, are considered as virtual realities.
However, we present some arguments below to revisit some considerations on when a reality is considered 'real' or 'virtual'.

Based on \citet{kishino}'s definition, digital objects do not have an objective existence and would be deemed to be virtual.
However, objects in the digital reality have increasingly real-world effects.
The scope of physical manifestation affects the determination of the virtuality of an object.
Exposure to objects in digital reality or dreams
can affect the cognition of an end-user.
They form new memories and associations, 
and representations/activations in the brain with respect to actions can change over time based on these new experiences.
As such, there is a physical component and manifestation of these 'virtual' objects in the human brain.
Similarly, an e-commerce platform could be interpreted as an input actuator to the transportation of physical objects, even though it does not have the same physical manifestation or even geo-spatial semantic mapping to a brick-and-mortar store.
\citet{skarbez2022revisiting} also revisit the assumptions of the continuum, and find that the 'perfect' virtual reality is unattainable. 
They find that any reality mediated with technology (computers, and extensibly brains) are mixed realities.
They also find that modern virtual reality implementations lie in an interpolated position in the continuum rather than the virtuality endpoint, hence the realities that encompass objects of non-objective existence can be inferred to possess some properties of the physical reality.

The semantic mapping between physical and digital objects are difficult to form, given what ties the physical and digital realities is primarily how the interactions of the user has in each environment have an effect in the real-world (e.g. changes in user cognition, demand-supply of products, voting systems). 
In \citet{kishino}'s continuum, there is an assumed morphism between a pair of realities, 
i.e. a known mapping scheme must exist and be known between a physical and virtual reality. 
However, 
in the digital reality, 
though we can assume that the objects between a physical and digital reality are mappable,
there is no given mapping scheme (instead, we need to learn the scheme), which violates the rules of the reality-virtuality continuum, and thus violating the presumption that the digital world is a virtual reality based on the existence of objects.
Though the objects from this pair of realities cannot be mapped based on affordances with respect to each object (e.g. touching a shoe in a store vs seeing a shoe on Amazon), 
they can be mapped to some extent based on the expectation of the outcome of an action (e.g. owning a pair of shoes upon purchase in a store vs Amazon).



While a pre-defined mapping is not provided between the physical and digital reality, 
as time elapses and the mapping tends towards completeness,
does the status of a reality change accordingly?
We investigated in this paper the potential for end-users to assist cross-reality rendering by providing semantic information collaboratively, 
i.e. using semi-supervised learning to scale semantic mapping.
Given that objects can be interfaced in both the physical and digital reality (unlike a developer-controlled virtual reality) by any number of users publicly,
semantic mapping and labelling can be done collaboratively.
A semi-supervised, specifically human-in-the-loop, approach to constructing mappings between both realities 
helps evaluate strategies towards an unsupervised approach to rendering cross-reality between physical and digital realities, while enabling an initial design to be useful in the near-term.
Enabling end-users to author their own experiences with ScalAR \citep{10.1145/3491102.3517665} have demonstrated improved customization of objects and interactions in custom physical environments. 
screen history
As the learnt mappings between the realities increase over time, in this case through collaborative data collection and intervention generation,
one could argue that any reality can eventually be aligned with respect to the reality-virtuality continuum, and the lack of a pre-defined mapping is not an argument for suggesting the rules of the continuum are violatable.

\begin{figure}
    \centering
    \caption{
    Re-rendering also works in virtual realities. Users can run games on Windows desktops, and the screen history can be annotated. Some of these interventions can also be reused in the digital/physical realities (e.g. if the game is in first-person view). 
    This enables physical/digital reality interventions to transfer over to the virtual reality. 
    It also allows us to augment the interventions by using a virtual reality as a safe annotation environment that approximates the geospatial properties of a physical reality.
    It also allows for easier video game modding without the need for modifying game source code or assets. 
    }
    \subfigure[Grand Theft Auto V: In first-person view, we annotate blood and gore (e.g. from shooting/stabbing).]{
    \includegraphics[width=6.5cm]{assets/game_1.PDF}
    }
    \hfill
    \subfigure[Call of Duty Modern Warfare II: In first-person view, we annotate the removal of religious garments, to dissociate the anger towards an enemy with  cultural characteristics.]{
    \includegraphics[width=6.5cm]{assets/game_2.PDF}
    }
    \\
    % \hfill
    \subfigure[The Witcher 3: In third-person view, we annotate hanged execution.]{
    \includegraphics[width=6.5cm]{assets/game_3.PDF}
    }
    \hfill
    \subfigure[Red Dead Redemption 2: In third-person view, we annotate animals hunting.]{
    \includegraphics[width=6.5cm]{assets/game_4.PDF}
    }
    \label{fig:tagger}
\end{figure}

In addition to be mappable by effect, objects could be mapped by their interventions.
One can also question how disconnected a physical reality and digital reality is based on the overlap of required interventions.
It could be observed that the issues faced in one reality tend to be consistent in the other.
For example, 
distraction is something a user may take issue with in both physical and digital reality, and the corresponding interventions would be consistent in coccluding the distracting object or changing the lifestyle/habit or interaction flow accordingly.

Virtual realities are also self-contained, in the sense that a specific virtual reality can be rendered with respect to a physical reality, but different virtual realities do not render with respect to other virtual realities. 
Each virtual reality has fixed affordances, and the user usually has had minimal history in each virtual reality. 
Moreover, the instructions needed to modify either reality is distinct.
Traditional virtual realities and the physical reality can be interacted with a similar interaction set (such as gestures, hand movements, touching) with appropriate extensions in the virtual realities.
While physical actions dominate interaction in the physical reality, 
interactions in the digital reality require a different interaction set. Other than user digital actions (such as touching, pinching, typing), developer digital actions exist where modifying source code can modify the digital reality entirely.
There is also a question regarding the level of autonomy that objects possess.
Like real objects, digital objects have an autonomy of their own because they can be tugged around by the rest of the world (e.g. users on a community-driven platform can manipulate digital objects, even if it was originally initialized by an end-user creator). This parallels how a user can own possessions or be surrounded by objects, but they have no objective control over these objects. 
Changing source code is not enough to override its existence in the digital reality; this is unlike virtual reality objects, where changes to them take place primarily with source code changes by a core team of developers.


We also raise the question of perception mediation.
If a user were to wear a cross-reality system for a very long duration of their lifetime's experiences, 
if the visual experience tends towards one where the cross-rendered reality is imperceptible from that of the underlying reality (i.e. the user cannot distinguish that reality is perturbed in any way), 
this perturbed reality could be interpreted as the updated physical reality. 
The introduced objects or properties that do not possess objective existence
do possess existence with respect to the visual perception of the user.
Suggesting that an object that is not present visually or objectively cannot constitute a physical reality
can be compared to
a blind person (or person who has lost all perceptive abilities including sight, hearing and touch) being unsure of an object's objective existence given they cannot visually perceive it.

















\subsection{Dreams: Interpolating between real dreams and virtual dreams}


A realm of interest is that of dreams.
Attached to our premise on reality, 
\citet{10.3389/fpsyg.2014.01133} evaluated dreams as a form of simulated reality, termed 'innate virtual reality'.
Similar to a computer-generated virtual reality, 
a dream has limited access to the physical reality, and objects in a dream have no objective existence.
Objects in a dream may not have a physical manifestation, but similar to digital objects, they can also have real-world effects (e.g. through the changes in the user's cognition).
Thus, a key difference between computer-generated virtual realities and brain-generated virtual realities is the 'rendering engine'.
Also similar to our cross-reality rendering premise, 
rendering of dreams can also be manipulated in real-time based on prior conditioning or priming. Users can be primed on certain stimulus, and can subsequently use this stimulus to alter their dreams, such as video or music.
For example, \citet{krakow2010imagery} find that users who visualize a positive end to a nightmare before initiating sleep tends to resolve nightmares during sleep.


We define a \textit{dream} as an environment composed of a sequence of compositionally-generated scenes, 
where the generation occurs through the combination of scenes, scenes' objects, and objects' properties, which originate from a repository of objects/scenes. 
Scenes are a sequence of images from a user's view, that are categorized arbitrarily by the user (e.g. chronological, location-based, action, people).
These viewed scenes are sourced from reality, experienced by the user, and are used as input to render dreams.
From these sourced scenes, the user interpolates properties - rather than interpolating on a continuous space, users interpolate on a discrete space, i.e. enumerating through different combinations of object properties, objects, and scenes. 
As a heuristic as to whether one's dreams are a function of all visual perception, 
\citet{MEAIDI2014586} find that people who are born blind (or become blind early in life) do not experience visual imagery when they dream, and conversely dream with auditory, tactile, gustatory, and olfactory components.
\citet{10.1093/nc/nix001} examined microdreams to find that the content in dream generation is driven by memory, where real perceptions of recent experiences and associated memories form a cohesive image. 

We highlight at least two approaches to rendering dreams: (i) cognitive, and (ii) computational.
Brain cognition is a generative process to render dreams. It can occur consciously (e.g. day-dreams, hallucinations), semi-consciously (e.g. lucid dreams), or unconsciously (e.g. during REM sleep).
We consider generation as an origination problem, where the original composition of objects in a scene is constructed.
Computational rendering of a dream can be both a generative or transcription/translation process. 
Cognitive and computational approaches tend to be used in feedback loops together.
While retrieval has been a common mode of returning unseen scenes using computational approaches, and this retrieval feeds into cognitive rendering (inspiration),
the recent growth in generative models (e.g. DALLE-2 \citep{https://doi.org/10.48550/arxiv.2204.06125}, Imagen \citep{https://doi.org/10.48550/arxiv.2205.11487}, Parti \citep{ https://doi.org/10.48550/arxiv.2206.10789})
permit the sourcing of unseen views of reality and synthesize novel scenes dynamically. 
We use 'unseen' to mean that a user has not experienced, perceived, or viewed a specific instance before.
While retrieval and unconditioned generation are relatively passive modes, 
there are other modes that support more active involvement of the end-user, resulting in human-machine co-creation.
A common computational approach to rendering dreams
is through the use of creativity tools.
When the user has a conscious dream of a scene (e.g. conditioned on a specific task) generated by cognition first,
they then transcribe this dream onto a canvas (e.g. Photoshop for images, or musical instruments for audio, etc).
While generative models are guided by the crowd (crowdsourced datasets), manual creativity tools are guided by the end-user.
The guidance on generation can be a mix of 
experiences of the end-user and outside the end-user. For example, users can provide prompts for conditional generation of outputs. 

As dreams are composed of perceived reality, and only the generation process is affected by the level of consciousness, we do not distinguish unconscious dreams differently from conscious dreams in our evaluation as they are derived from the same inputs, and thus we focus on conscious dreams (and do not propose manipulation of unconscious dreams). 
Some have attempted the manipulation of unconscious dreams, such as Dormio \citep{HAARHOROWITZ2020102938}.

We note some observations about dreams in the context of realities.
We note that a source or repository of realities is always needed to render a dream. 
For example, a source of graphics (Microsoft Clipart, Google Images, icon packs for slide presentations), 
crowdsourced datasets (e.g. for training generative models), 
assets and operations in VR environments, etc.
Though the user has used cognition to generate a template for the dream, it appears users take the rendering shortcut of making use of alternative sources for aiding the final render, and it requires too much manual effort to have a complete render in one's mind to be transcribed onto a computer. 
In addition to stimulation of creativity, such repositories contribute to human-machine co-creation.
Another observation is, similar to reality manipulation, the manipulation of dreams are also through augmentation (adding onto a blank slate) or diminishing (removing objects or tweaking an existing scene). 

For cross-reality systems to assist dreaming, there appears to be a few directions to pursue:
(i) origination (e.g. object/scene generation or retrieval);
(ii) transcription (e.g. interfacing between brain and computer in rendering the dream);
(iii) feedback loop between brain and computer. 
From the perspective of origination, 
the problem is a management of unseen views.
We need to provide novel content that the user has not experienced in their viewed realities.
With the cross-reality system in this paper, we would have a record of all prior viewed realities, so we would know what had not been viewed previously. 
Given the diversity of users and their respective views, we also have a source of unseen realities.
Additionally, we need to consider valid origination. Based on context, we need to know when objects are semantically-valid to be inserted. For example, we cannot just add random objects on any given scene in certain settings. 
From the perspective of transcription,
the problem is a management of seen views.
The goal would be to approximate computationally what is being rendered cognitively with less effort than manual transcription.
Given the user's own source of ideas come from prior viewed realities, we can enumerate through all combinations of objects and scenes in previous views till we obtain the scene the user is thinking of. This is intractable, but it demonstrates an iterative approach to automated transcription, where the user only notifies whether the output matches the cognitive render. 
Another method of minimal-effort interaction is through the use of brain-computer interfaces. For example, \citet{mallett2020pilot} demonstrated a lucid dreamer can control a block on a screen even while asleep.
Prompting is also a common mode of interaction (e.g. in conditional text-to-image generation), where the user provides low-dimensional input (e.g. text) to generate high-dimensional output (e.g. images).
We also need to consider how to manage the feedback loop between cognition and computational rendering.
One part of this is an information visualization problem, as we would wish to avoid information overload for the user. For example, we may wish to make use of "portals" in the regular shapes of some objects, and users can peer into these portals to view the dream. 
Another example is to activate dreaming based on time, place, mental state, or some other conditioning input, similar to how unconscious dreaming is activated when a user falls asleep.
By making use of a specific action or inserting an affordance that is specific to dreams, the user is given the choice and optionality in pursuing a dream, rather than placing dreams everywhere, without filter or choice to not explore if a user already knows they do not like the direction of the dream, etc. 

\section{Extensions \& Near-Future Work}

To extend cross-reality rendering, 
we can consider augmentation in addition to diminishing.
With the progression of new generative diffusion models \citep{https://doi.org/10.48550/arxiv.2204.06125, https://doi.org/10.48550/arxiv.2205.11487, https://doi.org/10.48550/arxiv.2206.10789}, we can explore the use of such models for interface generation in physical and digital settings using different conditioning techniques. 
In terms of object mappings, we currently let the user collect object instances on the view history, and manipulate the object forward in time. We did not manipulate the object in any way backward in time. For example, given the view history of both digital and physical realities, we could use the object mapping to identify patterns of causation (e.g. seeing price tags for footwear products on Amazon and linking it to how a user acts when seeing priced footwear in physical stores). 

To support the scaling of intervention generation and usage, 
we provide users with more data points and pre-populated interventions. 
Automated intervention generation is one approach. Currently intervention generation is a semi-supervised approach, where data is being annotated by users upon self-reflection of specific use cases. We could shift towards automatically generating interventions or recommending the annotation of certain objects to improve the workflow. 
Another approach is to enable an 'other-user' view history mode.
Some tools exist that allow users to see an interface from the perspective of another user (e.g. YouTube \citep{identiswap}). 
If other users feel safe to contribute their view history, they can share it publicly so other users can view it and also annotate it from their unique point-of-view.
Based on user consent, another user can be given another user's egocentric vision to simulate their life, and generate even more interventions based on their personal interpretation.
This can also be pre-populated with the Ego4D dataset \citep{https://doi.org/10.48550/arxiv.2110.07058}, a diverse collection of egocentric vision videos from around the world.
In addition to assisting the mapping in objects between the physical and digital realities, 
hooks assist in scaling the generation of interventions.
Given the limited variability of a specific GUI element on a given app (or ease of re-cropping an updated interface design), one shot of a GUI element is sufficient to detect it. 
As such, mask hooks tend to require only a single cropped image as input, and we sample a large number of GUI element interventions based on existing digital/perceptual harms literature.
Pre-populated model hooks can also be sourced from model sharing platforms (e.g. AdapterHub, huggingface, PapersWithCode, Github, ModelZoo).

Other than benefiting from existing contributions in the machine learning ecosystem, 
this system can also contribute back to the same machine learning ecosystem.
This system can contribute to the pool of task-specific fine-tuned models,
annotated/labelled datasets on various tasks, 
datasets of high distributional shift (attributed to the non-uniformity of user experiences),
or providing unlabelled view history data for unsupervised tasks.
This helps developers working on reality manipulation (e.g. AR/MR/VR researchers, digital harms researchers)
by providing them with data on what users wish to mitigate (e.g. a repository of digital harms), or
initial user-initiated designs on what interface changes users would like to see (and thus be implemented natively).














\section{Limitations}

The current system design is the product of numerous iterations. In each iteration we aimed to resolve different challenges that posed as hurdles to usability and deployment. 
We opt for a complete server-side implementation, where we load devices and run interventions on a server, and stream to client devices.
This reduces the burden on end-users for client-side hardware specifications.
With our setup, the user does not need a high-end smartphone or specialized AR/MR/VR hardware with built-in processors. 
A user can use a smartphone of any specification (as long as it has an internet connection and load webpages) to load any device emulator. 
They can procure a headset for mounting a smartphone, 
and this can be as costless as building one out of cardboard \citep{cardboard}.
In a previous iteration, we attempted to keep the device interface on the client-side and stream just-in-time overlay renders. While this worked in most settings given sufficient bandwidth, our concern lied in the off-chance that a user with insufficient bandwidth might see an overlay render after the underlying interface image had changed.
We concluded it was better to liken the access to manipulated realities to that of buffering a video; if a user prioritizes interventions, they may be willing to stream an interface (even with rare delays), and video buffer time has been drastically shortened over the years with improvements in streaming architectures and bandwidth access.
We also move away from a code modification approach to changing interface functionality.
By identifying commonalities between interfaces agnostic to operating system (native program patches tend to be OS-specific), 
we allow objects to be manipulated across operating systems.
Not all interfaces have an underlying 'code' that can be modified, such as the physical reality, and thus overlays have been the predominant strategy in modifying physical realities. 
To maintain a reality-agnostic approach to manipulating realities, 
we adhere to the use of overlays.
Overlays make use of what the user can see as their input and output.
Further, prior use cases for reality manipulation tend to require a third-party to craft interventions for the user, 
be it patch developers for app modifications,
or developers for AR/MR/VR software.
From a development cycle where 
users and developers engage in a feedback loop 
to maintain and upgrade software over time,
we directly support users in maintaining or developing software themselves.
They can craft their own interventions that perform specific digital functionality or AR/MR/VR functions.
Despite this progress, 
there are still avenues for improvement.
We highlight some extensions needed to improve the overall user experience, 
as well as preliminary directions on how to approach the limitations.
Most of the following limitations are not critical issues with the system design;
conversely, they are at most 'band-aids' that can be plastered onto the system to improve the experience, but they do not break the experience. 


Some failure modes are component-specific; they are not a failure in system design necessarily, but requiring improving individual components.
The mask hook might face difficulty in element removal of ‘dynamic’ elements (e.g. removing the video box for YouTube videos if we exclude the sharing metrics), or the overblocking of elements (e.g. removing the homescreen).

Our current implementation has handled most interaction modalities pertaining to imagery and text, 
but there are other modalities that would need to be manually built.
Accessing client-side hardware is possible (e.g. \citet{vx}), such that the server-side emulator can access the user's local camera, audio speakers, sensors, and haptic vibrators. 
Furthermore, while we have provided hooks for image-based interactions (e.g. text can manifest as an image in any reality), 
we did not implement a hook for audio/speech.

Safety is also an important concern. As a user can manipulate their physical realities, there may be some critical situations where the re-render needs to be undone, or the user should be informed of the non-overlayed reality. For example, when crossing the road, though bicycles are occluded, they should not be completely inpainted. They should be slightly blurred, or at least a big arrow should above the cyclist to inform the user that an object exists and is approaching them. This also means certain objects that are intended to be used for physical safety, such as fire extinguishers or traffic lights, should not be overlay-able. We could insert safety checker models to verify that non-overlayable objects are not manipulated, or alternatively we could prompt the user to re-consider their decision (e.g. doing a sample playback in the view history of what happens when this object is occluded).


There are a few considerations regarding scaling, in terms of model development and data quality.
An assumption made is that users in a network know a ground-truth label of the category of the specific text they wish to detect and occlude, and the crowd-sourced text of each of $N$ categories will yield corresponding $N$ fine-tuned models.
A concern with this assumption, is that the labelling of such inputs in the real-world may not be standardized, and similar inputs may be grouped separately or dissimilar inputs may be grouped together, if we purely rely on network-based tagging. 
We may encounter scenarios of 
out-of-distribution shift (e.g. users sample non-uniform sentences), adversarial samples (e.g. users maliciously tag sentences that worsen accuracy).
On the one hand, we can evaluate a tagging system that shows the user similar intervention tags as the one they are entering, so that an existing intervention is updated rather than creating duplicate interventions.
On the other hand, perhaps the data points are indeed distributionally different from an existing tag's dataset, so creating a different tag would be appropriate.
Possible algorithmic approaches to ensuring similar texts are grouped together for fine-tuning could be the use of in/out-of-distribution detection (e.g. computing the interference in loss convergence with respect to 2 inputs coming from different categories, or using a similarity metric, in order to regroup contributed inputs into appropriate categories), or the use of ensemble models (e.g. preparing $M$ different batches of training sets to train $M$ different ensemble models, so that the dissimilarity between certain sentences do not afflict a single model alone, and other models can validate a prediction).
Furthermore, to reduce the reliance on the user population and sampling rate for intervention generation, we can explore faster adaptation methods (e.g. batch-efficient fine/prompt-tuning with large foundation models) so that less/no additional data would be needed for generating interventions. 
Alternatively, we can pre-populate the system internally first with a large number of masks and models (e.g. sourcing models from AdapterHub, github, modelzoo, etc). 


















