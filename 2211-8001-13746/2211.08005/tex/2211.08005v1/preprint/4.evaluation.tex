









\input{preprint/3.1.masks_table}


\section{Evaluation}
\label{sec:evaluation}






















\subsection{Cognitive Walkthrough
}

We perform a cognitive walkthrough ("show and tell rather than use and test")~\citep{10.1145/223904.223962, 10.1145/223355.223735}
to simulate a user's cognitive process and explicit actions during usage.
We as the authors presume the role of a user. 
For each step of the walkthrough, 
we first report the data pertaining to each task,
then provide a descriptive evaluation.
To evaluate the process of constructing an intervention,
we track the completion of a set of required tasks (Table \ref{tab:tasks}) based on criteria from
\citet{844354}'s 4 types of automation applications, which aim to measure 
the role of automation in the 
intervention self-development process.
This evaluates the ease and usability in generating interventions and viewing realities. 









\begin{quote}{
\begin{description}[noitemsep = 0pt]%
\footnotesize
      \textbf{Step 1: User logs in (Fig. \ref{fig:walkthrough}a)} \\
      \textit{
      The user enters their username/password. These credentials are stored in a database mapped to specific virtual machines that contain the interfaces the user registered for access. 
      }
\end{description}}
\end{quote}

This is a standard step for any secured or personalized system, where a user is informed they are accessing data and information that is tailored for their own usage.


\begin{quote}{
\begin{description}[noitemsep = 0pt]%
\footnotesize
      \textbf{Step 2: User activates interface \& interventions (Fig. \ref{fig:walkthrough}b)} \\
      \textit{
      The user is shown a set of available interventions. They select their target interventions, and select an interface (digital or physical) to view.
      }
\end{description}}
\end{quote}


Users can view the set of interfaces that they can access and use to facilitate their viewing experiences. The interface is available 24/7, retains all their personal data and storage, is recording their view history data for review, and accessible via a web browser from any other device/platform. They are less constrained by the hardware limitations of their personal device.
The populated interventions liken to a marketplace and ecosystem of personalized and shareable interventions. Users can populate interventions that they themselves can generate through the view history tool, or access interventions collaboratively trained and contributed by multiple members in their network. The interventions are also modular enough that users are not restricted to a specific combination of interventions, 
and can apply any combination of interventions sequentially onto the interface.
As the capabilities of generating interventions become extended, so do their ability to personalize their experience, and generate a distribution of experiences to match a similarly wide distribution of users.
The autonomy to deploy interventions, with more options from community contributions, before usage of an interface satisfies Task 4.


\begin{quote}{
\begin{description}[noitemsep = 0pt]%
\footnotesize
      \textbf{Step 3: User accesses the interface and browses (Fig. \ref{fig:walkthrough}c-e)} \\
      \textit{
      The user interacts with the re-rendered interface through their mobile/desktop browser.
      }
\end{description}}
\end{quote}

After the user has chosen their desired interventions,
the user can improve their viewing experience through the manipulation of exposure to certain objects.
The altered viewing experience satisfies both Tasks 1 and 4. Not only is raw image data being collected, but the view is being altered by deployed interventions in real-time.
It is a cyclical loop where users redesign and self-improve their viewing experiences with user-driven tools. 


\begin{quote}{
\begin{description}[noitemsep = 0pt]%
\footnotesize
      \textbf{Step 4: User inspects view history to generate interventions (Fig. \ref{fig:tagger})} \\
      \textit{
      After a viewing period, the user may inspect their personal view history to create interventions. They enter the view history page to inspect recorded intervals of their viewing activity across all interfaces, and they can choose to annotate certain regions  to generate interventions.
      }
\end{description}}
\end{quote}

The user is given autonomy in manipulating aspects of their perceived reality.
Enabling the user to inspect their view history across all used  interfaces 
to self-reflect and analyze activity patterns 
satisfies Task 2. 
Though the view history provides the user raw historical data, it may require additional processing (e.g. automated analysis, charts) to avoid information overload.
Rather than waiting for a feedback loop for altruistic developers (e.g. app modifications for digital reality, or dedicated AR software for physical reality) to craft broad-spectrum interventions that may not fit their personal needs, the user can enjoy a personalized loop of crafting and deploying interventions, almost instantly for certain interventions such as element masks. The user can enter metadata pertaining to each annotated object, and not only contribute to their own experience improvement, but also contribute to the improvement of others who may not have encountered the object yet.
By developing interventions based on their analysis, not only for themselves but potentially for other users,
they can achieve Task 3. 

All four tasks, used to determine whether a complete feedback loop between input collection and interface rendering through HITL, 
can be successfully completed.

\subsection{Personas}

Given a set of personalized requirements per user, we evaluate the ability for the system to render interventions. 
We evaluate specifically in physical realities, 
as distributional-diversity of objects are higher in physical realities than digital realities.
Mask hooks work well in highly-uniform realities, such as handling GUI elements in digital realities.
Attributing to the non-uniformity in physical realities, we rely on adaptation with model hooks.

We evaluate using \textit{personas}, which are descriptions of individual people who represent groups of users that would interact with our system.
To construct each persona, 
we populate each hypothetical individual with information pertaining to their background (e.g. context on scenarios or requirements), scenarios (situations/scenarios prompting the persona to use our system), and finally requirements (discrete items that need to be satisfied, generally consistent needs throughout most scenarios).
In-line with \citet{10.1145/2207676.2208573}, 
to evaluate each persona, 
we  
(i) identify persona requirements (i.e. hypothesize scenarios where the persona need our system), and
(ii) evaluate scenario responses (i.e. create real-world scenarios where the personas use the system and evaluate the alleviation of requirements).




Based on the personas constructed in Table \ref{tab:personas},
the visual appearance of the required object tends to be consistent regardless of digital or physical reality.
To support each person's requirements, 
we first manually collected a set of images for each object (\texttt{graves}, \texttt{price labels}, \texttt{bikes}, \texttt{dogs}). 
With our view history tool,
we sample 100 images per object
by searching for each object on Google Images and 
annotating images with object labels and bounding boxes.
Though curated datasets could be found (e.g. dogs in CIFAR10),
we remain consistent in our sampling strategy for objects without datasets.
We fine-tune a Faster R-CNN \citep{https://doi.org/10.48550/arxiv.1506.01497} model, pre-trained on MSCOCO \citep{https://doi.org/10.48550/arxiv.1405.0312}, on each object.
We replace the pre-trained head of the model with a new one containing the new class, and fine-tune until early-stopping at loss 0.1. 
This results in four models for each of the four objects.
With the predicted bounding box coordinates, we apply a Gaussian blur to occlude the object.
After activating these interventions, the authors put on the head-mounted display and enacted the hypothetical scenario in real-life.
We visit physical locations (cemetery, supermarket, street with a cycle path, and dog park), and observe consistent occlusion of the target objects (Figure \ref{fig:headset_results}).
We find that most objects can be successfully occluded in real-time.
Objects that fail to be occluded tend to be at rotated angles inconsistent with sampled images, be a large distance away from the user (i.e. a small set of pixels in the image pertain to the object), or are distributionally-distant from the source distribution of images (e.g. variations of dogs). 

\subsection{Scalability Testing
}




To evaluate the collaborative component,
we measure the improvement to the user experience of a single user from the efforts of multiple users.
We do not recruit 
real users, as it would constrain our performance evaluation to the number of users available, the evaluation period, intervention quality control, and diversity of recruited users. 
Instead, we evaluate through scalability testing, a type of load testing \citep{stest} that measures a system's ability to scale with respect to the number of users.

One application we can use as a base for evaluating our system is the mitigation of different digital harms.
Harms tend to be highly individual and vary in how they manifest within users of digital systems.
The spectrum of harms range from
heavily-biased content (e.g. disinformation, hate speech),
self-harm (e.g. eating disorders, self-cutting, suicide),
cyber crime (e.g. cyber-bullying, harassment,
promotion/recruitment for extreme causes such as terrorism),
to demographic-specific exploitation (e.g. child-unsafe content, social engineering attacks).
We refer the reader to the extensive literature~\citep{hmgov, 10.1145/2998181.2998224, 10.1145/3038912.3052555, 10.1145/3313831.3376370, 10.1145/3359186, https://doi.org/10.48550/arxiv.2210.05791}.
In each subsection, we demonstrate interface-agnostic intervention of different harms specific to GUI elements (Section 5.3.1) and content (Section 5.3.2). 

We simulate the usage of the system
to evaluate the scalable generation of one-shot mask detection, and scalable fine-tuning of text models, 
in order to evaluate the strengths/weaknesses of the system's scalability.
We measure the ease of intervention development with the number of variations of interventions generated (specifically element removal) (Table \ref{tab:mask_results}), rather than development time.
We do not replicate the scalability analysis on real users: the fine-tuning mechanism is still the same, and the main variable (in common) is the sentences highlighted (and their assigned labels and metadata, as well as the quality of the annotations), though error is expectedly higher in the real-world as the data may be sampled differently and of lower annotation quality. 
The primary utility of collaboration to an individual user is the scaled reduction of effort in intervention development. 
We evaluate this in terms of variety of individualized interventions (variations of masks),
and the time saved in constructing a single robust intervention (time needed to construct an accurate model intervention).




\begin{table*}[!ht]
\centering
\begin{minipage}{0.39\linewidth}
\resizebox{\linewidth}{!}{
    \begin{tabular}{p{2.5cm}p{2.5cm}p{2.5cm}}
        \toprule
        \textbf{Task}
        & \textbf{Description}
        & \textbf{Step}	
        \\
        \midrule
        \circled{1} \textit{Information Acquisition} 
        & Could a user collect new data points to be used in intervention crafting?
        & 3 (User accesses the interface and browses)
        \newline
        \newline
        \\
        \midrule
        \circled{2} \textit{Information Analysis}
        & Could a user analyze viewing data to inform them of useful interventions?
        & 4 (User inspects view history to generate interventions)
        \newline
        \newline
        \\
        \midrule
        \circled{3} \textit{Decision \& Action Selection}
        & Could a user act upon the analyzed information about objects they are exposed to, and develop interventions?
        & 4 (User inspects view history to generate interventions)
        \newline
        \newline
        \newline
        \\
        \midrule
        \circled{4} \textit{Action Implementation}
        & Could a user deploy the intervention in future viewing sessions?
        & 2 (User activates interface and interventions), 3 (User accesses the interface and browses)
        \newline
        \\
        \bottomrule
    \end{tabular}
    }
    \captionof{table}{Tasks and the walkthrough steps that satisfy them. 
    }
    \label{tab:tasks}
\end{minipage}
\begin{minipage}{0.6\linewidth}
\resizebox{\linewidth}{!}{
    \begin{tabular}{p{1.5cm}p{5cm}p{5.5cm}p{5cm}}
        \toprule
        \textbf{Persona}
        & \textbf{Background}
        & \textbf{Scenarios}
        & \textbf{Requirements}
        \\
        \midrule
        Persona 1: \textit{graves}
        & 
        They are afraid of death. They went through the trauma of losing a spouse and being forced to quickly bury them in the local cemetery. 
      The sight of gravestones may cause panic attacks.
        & 
        They may pass a graveyard in their local vicinity. Gravestones may also be located in impromptu locations (e.g. points of cycling accidents). Media may contain imagery of deaths, such as in news coverage or leisurely content.
        & 
        Objects, be it in the digital or physical reality, that pertain to death should be occluded from view. 
      An example of such an object is a \textit{gravestone}.
        \\
        \midrule
        Persona 2: \textit{prices}
        & 
        They lack self-control on spending.
      They grew up in poverty and their parents forbade any unnecessary purchases. 
      They purchase anything 'cheap', even if they do not need it. 
      They cannot inhibit their vice when shopping online or in real-life.
        & 
        Priced goods may be shown on e-commerce platforms, advertisements on other webpages or apps, 
      or brick-and-mortar stores.
        & 
        Objects, be it in the digital or physical reality, that display a \textit{price tag} should be occluded. 
        \\
        \midrule
        Persona 3: \textit{bikes}
        & 
        They have anger issues towards cyclists. They witnessed a cyclist run their brother over.
      Now they enter a fit of rage whenever they see one. Not only does this hurt any passing cyclist's feelings who hear their harsh words without any context (and sometimes this escalates into a physical fight), but they also fail to concentrate throughout the rest of the day.
        & 
        They live in a city with extensive cycling routes, resulting in a high prevalence of cyclists.
        & 
        Objects, be it in the digital or physical reality, that pertain to cyclists should be occluded from view. 
      An example of such an object is a \textit{bicycle}.
      Physical safety of the user should be considered. For example, only occluding the bike but not traffic lights, or blurring the bike so that the user still retains depth-perception of an incoming object. 
        \\
        \midrule
        Persona 4: \textit{dogs}
        & 
        They are easily distracted.
      They grew up in an intimately-small family where there were twice as many dogs as there were humans. 
      They now cannot control themselves when they see a dog of any shape or size in real-life. When they see a canine, they stop what they were doing, and start chasing after them. 
        & 
        Dogs can be present on the street, in the park, in the (pet-friendly) workplace, in indoor settings (e.g. cafes/restaurants), etc.
      Targeted advertising tends to show them dog products with their demo dogs. Videos online may also contain dogs. 
      Though they cannot chase them into a screen, the distraction absorbs them and they enter a rabbit hole of browsing funny dog videos. 
        & 
        Objects, be it in the digital or physical reality, that pertain to pets (specifically \textit{dogs}) should be occluded from view. 
        \\
        \bottomrule
    \end{tabular}
    }
    \captionof{table}{Scenarios and requirements evaluated for each persona.}
    \label{tab:personas}
\end{minipage}
\\
\hspace{-1.5cm}
\begin{minipage}{0.49\textwidth}
	\centering
\resizebox{\textwidth}{!}{
    \begin{tabular}{lccccc}
        \toprule
        \textbf{Mask}
        & \textbf{Num. masks}
        & \textbf{Android app}	
        & \textbf{iOS app}	
        & \textbf{Mobile browser}	
        & \textbf{Desktop browser} \\
        \midrule
        \multicolumn{6}{l}{\textit{Stories bar}} \\
        - Twitter & 1 & \ding{51} & \ding{51} & --- & --- \\
        - Linkedin & 1 & \ding{51} & \ding{51} & --- & --- \\
        - Instagram & 1 & \ding{51} & \ding{51} & --- & --- \\
        \midrule
        \multicolumn{6}{l}{\textit{Metrics/Sharing bar}} \\
        - Facebook & 2 & \ding{51} & \ding{51} & \ding{51} & \ding{51} \\
        - Instagram & 2 & \ding{51} & \ding{51} & \ding{51} & \ding{51} \\
        - Twitter & 2 & \ding{51} & \ding{51} & \ding{51} & \ding{51} \\
        - YouTube & 2 & \ding{51} & \ding{51} & \ding{51} & \ding{51} \\
        - TikTok & 2 & \ding{51} & \ding{51} & \ding{51} & \ding{51} \\
        \midrule
        \multicolumn{6}{l}{\textit{Recommended items}} \\
        - Twitter & 2 & \ding{51} & \ding{51} & \ding{51} & \ding{51} \\
        - Facebook & 2 & \ding{51} & \ding{51} & \ding{51} & \ding{51} \\
        \bottomrule
    \end{tabular}
    }
    \captionof{table}{\ding{51} if element removal is successful, \ding{55} if element removal is unsuccessful, --- if the element not available on an interface. 
    }
    \label{tab:mask_results}
\end{minipage}
	\hfill
\begin{minipage}{0.49\textwidth}
        \includegraphics[width=\textwidth]{assets/gw_fewshot.pdf}
        \captionof{figure}{Convergence of fine-tuned models on hate speech.}
        \label{fig:fewshot_results} 
	\end{minipage}
\end{table*}

\subsubsection{Scaling mask hooks}

We investigate the ease 
to annotate graphically-consistent GUI elements (Table \ref{tab:mask_results}). 
We sample elements to occlude that can exist across a variety of interfaces.
We evaluate the occlusion of the \textit{stories bar} (predominantly only found on mobile devices, not desktop/browsers).
Some intervention tools exist on Android \citep{swipe, InstaPrefs, gd, greaseterminator} and iOS \citep{friendly}, though the tools are app- (and version-) specific.
We evaluate the occlusion of \textit{like/share metrics}.
There are mainly desktop browser intervention tools \citep{fbd, twd, igd, hidelikes}, and one Android intervention tool \citep{greaseterminator}.
We evaluate the occlusion of \textit{recommendations}.
There are intervention tools that remove varying extents of the interface on browsers (such as the entire newsfeed) \citep{erad, unhook}.
Existing implementations and interest in such interventions indicate some users have overlapping interests in tackling the removal or occlusion of such GUI elements, though the implementations may not exist across all interface platforms, and may not be robust to version changes. 
We evaluate each intervention on a range of target interfaces,
specifically native apps (for Android and iOS) and browsers (Android mobile browser, and Linux desktop browser). 

We use the view history tool to annotate and tag the minimum number of masks needed per element to block across a set of apps. There tends to be small variations in the design of the element between browsers and mobile, hence we tend to require at least 1 mask from each device type. Android and iOS apps tend to have similar enough GUI elements that a single mask can be reused between them. We tabulate in Table \ref{tab:mask_results} the successful generation and real-time occlusion of all evaluated GUI elements. 
We append screenshots of the removal of recommended items from the Twitter and Instagram apps on Android (Figure \ref{fig:gt_eval}a).
We append screenshots of the de-metrification (occlusion of like/share metrics) of YouTube across 
desktop browsers (macOS) and mobile browsers (Android, iOS)
(Figure \ref{fig:sharebuttons_removal}).


\subsubsection{Scaling model hooks}

We investigate the accuracy gains from fine-tuning pre-trained text models as a function of user numbers and annotated sentence contributions (Figure \ref{fig:fewshot_results}). 
Specifically, we evaluate the text censoring of hate speech, where the primary form of mitigation is still community standard guidelines and platform moderation, with little user tooling available on Android \citep{bodyguard, greaseterminator}.
The premise of this empirical evaluation is that we have a group of simulated users $M$ who each contribute $N$ inputs (sentences) of a specific target class (hate speech, specifically against women) per timestep. 
Baselined against a pre-trained model fine-tuned with all sentences against women, we wish to observe how the test accuracy of a model fine-tuned with $M \times N$ sentences varies over time. 
Our source of hate speech for evaluation is the Dynamically Generated Hate Speech Dataset \citep{vidgen-etal-2021-learning}, which contains sentences of \texttt{non-hate} and \texttt{hate} labels, and also classifies hate-labelled data by the target victim of the text (e.g. \texttt{women}, \texttt{muslim}, \texttt{jewish}, \texttt{black}, \texttt{disabled}). As we expect the $M$ users to be labelling a specific niche of hate speech to censor, we specify the subset of hate speech of \texttt{women} (train set count: 1,652; test set count: 187).
We fine-tune RoBERTa \citep{roberta, DBLP:journals/corr/abs-1907-11692}, pre-trained on English corpora Wikipedia \citep{wikidump} and BookCorpus \citep{Zhu_2015_ICCV}.
For each user population $M$ and sentence sampling rate $N$, at each timestep $t$, $M \times N \times t$ sentences are acquired of class \texttt{hate} against target \texttt{women}; there are a total of 1,652 train set sentences under these constraints (i.e. the max number of sentences that can be acquired before it hits the baseline accuracy), and to balance the class distribution, we retain all 15,184 train set \texttt{non-hate} sentences. We evaluate the test accuracy of the fine-tuned model on all 187 test set women-targeted hate speech. 
We also vary $M$ and $N$ to observe sensitivity of these parameters to the convergence towards baseline test accuracy.

The rate of convergence of a fine-tuned model is quicker when the number of users and contributed sentences per timestep both increase, approximately when we reach at least 1,000 sentences for the \texttt{women} category. 
The difference in convergence rates indicate collaborative labelling scales the rate in which text of a specific category can be acquired.
It reduces the burden on a single user of training text classification models from scratch and annotating text alone, diversifies the fine-tune training set, and avoids wasted effort in re-training models already fine-tuned by other users. 

The empirical results from the scalability tests
indicate that the ease of mask generation and model fine-tuning, further catalyzed by performance improvements from more users, 
enable the scalable generation of interventions.








