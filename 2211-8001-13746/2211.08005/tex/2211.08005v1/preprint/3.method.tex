\section{Cross-Reality System Architecture}






We detail the implementation of the system base, upon which functionality on rendering (Section 4) and playback (Section 5) is supported, 
and further functionality can be added.
All images are captured with our working prototype.

We outline the architectural components as follows:
\begin{enumerate}[noitemsep,topsep=0pt,parsep=0pt,partopsep=0pt]
    \item[] \circled{1} The user logs into the system to access their camera feed or a set of personal emulators, along with a set of interventions.
    The system admin has provisioned a set of emulated devices, hosted on virtual machines on a server.
    The user places a (secondary) hand-held device on their head-mounted display, where the camera feed is streamed to a server and a re-rendered feed is loaded. 
    \item[] \circled{2} The user selects their desired interventions and views re-rendered interfaces.
    \item[] \circled{3} The user accesses their view history and annotates graphics or text for generating interventions, which then re-populate the list of interventions available to members in a network. 
\end{enumerate}


The user accesses a web application (compatible with desktop/mobile browsers). With their login credentials, the database loads the corresponding mapping of the user's virtual machines that are shown in the \textit{interface selection page} (Figure~\ref{fig:walkthrough}(b)). 
The server carries information on accessing a set of emulated devices.
Each emulator is rendered in virtual machines where input commands are redirected.
While all digital realities are loaded server-side and streamed directly to the user device, 
the physical reality is captured from a (secondary) device placed on a head-mounted display (Figure \ref{fig:headset}), 
where images are sent from the device camera to the server, then processed and loaded in real-time on the \textit{camera feed page} (Figure~\ref{fig:walkthrough}(d)). 
The moment-by-moment changes on a personâ€™s screen or camera feed can be captured as images at a configurable framerate. The captured images are processed and rendered for the user to observe their reality, and also displayed to the user in their view history.
Further, the database loads the corresponding mapping of available interventions (generated by the user, or by the network of users) in the \textit{interventions selection page}. 
The database also loads the view history in the \textit{view history page} (Figure~\ref{fig:tagger}). 
This consists of images of timestamped, visited realities, including chronological images from the camera feed as well as that of the emulators.


Input commands for desktop/mobile 
are captured and directed to the emulator,
including keystrokes (hardware keyboard, on-screen keyboard) and mouse/touch events (scrolling, swiping, pinching, etc).
Screen and camera images are captured at 60~FPS into an \textit{images directory}.
Generated masks and fine-tuned models are stored under an \textit{interventions directory}.
Images and interventions are accessible to their corresponding user.
Interventions are applied sequentially upon an image to return a perturbed image, which then updates the rendered image on the client web app. 









\begin{figure}
    \centering
    \includegraphics[width=0.8\linewidth]{assets/headmount.pdf}
    \caption{\textit{Head-mounted display}:
    The user's view \textit{(left)}, 
    and the external look \textit{(right)} (with the handheld device inside, its back-facing camera exposed for capturing images).
    }
    \label{fig:headset}
\end{figure}

\section{Manipulating Realities}
\label{sec:grease}

\subsection{Rendering functionality}





















We can decompose rendering functionality into two components:
(i) mapping objects between realities, and
(ii) manipulating the objects within a reality. 
In virtual reality, mapping between real and virtual objects tend to be performed based on relative location in a given view. 
As the digital reality is not a geo-spatial reality, we need an alternative mapping strategy. 
We pursue one where we map based on properties of the objects (e.g. physical appearance/attributes) between realities. 
As such, we built \textit{hooks} to support generalizable identification of object properties between realities. 
We ideate these reality-agnostic hooks based on observed patterns between realities. For example, objects in realities can be encoded in text or graphics, or can be processed by a learnt model if necessary.
Hooks operate time-agnostically:
they can be used to identify object properties during annotation in playback as well as facilitate object manipulation during real-time re-rendering. 
As images are the primary medium of rendering over a reality, the hooks deal with pixel-based inputs and outputs (e.g. though raw text can be extracted from a device emulator, we use OCR to enable generalizable text detection such as in physical realities). 
Overlay modifications are interface-agnostic, enabling the same intervention to scale to all interfaces where the modification condition exists.
It does not require escalation of privilege or modification of source code, and is thus easy to use for users and developers. 

The input receivers for the hooks
begin from the input devices (head-mounted feed, device emulators), 
and into the annotations on the view history. 
The output receivers for the hooks 
begin from the object renders superimposed on the base reality,
and onto the stream of the reality displayed. 
Depending on the objective of the rendering task, how the output of the hooks manifest in the re-rendered reality can vary.
The capability of output renders is independent of the hooks, given that sufficient information or attributes is provided by the hooks.
We focus on \textit{diminished} reality, where we work on reducing visibility on objects.
On the other end of the spectrum, \textit{augmented} reality would be more involved in the addition of new objects or adding/modifying properties of existing objects.


































The \textbf{text hook} enables modifying the text on an interface.
Character-level optical character recognition (OCR) takes an image as input and returns a set of characters and their geometric coordinates. 
We first identify a set of regions containing text with EAST text detection~\citep{zhou2017east}. 
We then use Tesseract~\citep{tesseract} to extract characters within each region.
With the availability of real-time textual data from each image instance, an intervention developer can store information processed by the image to be processed by subsequent models via the model hook. 
A sample application of this hook include
interventions against text of specific conditions (e.g. placing censor boxes over hate speech, or generating new text personalized to the user).
Another example is the identification and highlighting of specific text used in one interface (e.g. product ads on Facebook) and appearing in another (e.g. search results in Amazon, or appearing in real-life when in a store).


The \textbf{mask hook} matches the current image against a target template of multiple images.
\textit{Multi-scale multi-template} matching resizes an image multiple times and samples different subimages to compare against each mask instance.
The mask hook can be augmented; for example, using the matching algorithm with contourized images (shapes, colour-independent) or coloured images depending on whether the mask contains (dynamic) objects.
The mask hook could be connected to rendering functions such as highlighting the interface element with warning labels, or image inpainting (fill in the removed element pixels with newly generated pixels from the background).
Given the higher likelihood of non-variability of object instances in the digital world, the applications of this hook would be expectedly predominant in the digital reality. 
An example application is the user can capture a mask of the share buttons on YouTube, and as long as this design is used across all interfaces of YouTube from Android to iOS to browser, this one mask can be reused. Having detected the coordinates of the mask on a given image, the detected object could be occluded, highlighted, or inpainted, etc. 


















A \textbf{model hook} loads a model to take an input and generate an output.
This enables the embedding of models (i.e. model weights and architectures)
to inform further overlay rendering.
We can connect models trained on specific tasks (e.g. person pose detection, emotion/sentiment analysis) to return output given the image (e.g. bounding box coordinates to filter), and this output can then be passed to a pre-defined rendering function (e.g. draw filtering box).


We enable end-users to tune or adapt their own personalized models
using an annotation interface and model adaptation mechanisms.
Our implementation specifically relies on fine-tuning, but we also review few-shot learning and prompt-tuning. 
For model fine-tuning, the developer re-trains a pre-trained model on a new dataset. This is in contrast to training a model from a random initialization. 
Fine-tuning techniques for pre-trained models, which already contain representations for feature reuse,
have indicated strong performance on downstream tasks \citep{galanti2022on, abnar2022exploring, NEURIPS2020_0607f4c7}.
To retain representations of older tasks or batches of data, online/continual learning methods can assist in reducing catastrophic forgetting \citep{ewc, mota}.
If there is a large number of input distributions
and few samples per distribution,
few-shot learning is an approach where
the developer separately trains a meta-model that learns how to change model parameters with respect to only a few samples. 
Few-shot learning has demonstrated successful test-time adaptation in updating model parameters with respect to limited test-time samples \citep{Raghu2020Rapid, Koch2015SiameseNN, finn2017modelagnostic, datta2021learnweight}.
Some overlapping techniques even exist between few-shot learning and fine-tuning, such as constructing subspaces and optimizing with respect to intrinsic dimensions \citep{aghajanyan-etal-2021-intrinsic, datta2022low, 9157772, https://doi.org/10.48550/arxiv.2205.09891}.
Prompt tuning is an alternative adaptation approach that does not require changes in the parameters of the downstream model.
It is a technique that leverages the use of specific conditioning inputs (e.g. a phrase at the start of a sentence) to condition a foundation model to perform a specific downstream task \citep{lester-etal-2021-power}.
Steps towards scaling the quantity of prompts have been undertaken, 
from PromptSource \citep{bach2022promptsource}, a prompt repository and tool used for creating and sharing prompts, to PromptGen \citep{zhang-etal-2022-promptgen}, a method for dynamic prompt generation. 


\subsection{Playback functionality}

Human-in-the-Loop (HITL) learning is the procedure of integrating human knowledge and experience in the augmentation of machine learning models.
It is commonly used to generate new data from humans or annotate existing data by humans.
Examples are reviewed in \citet{https://doi.org/10.48550/arxiv.2108.00941}.



A view history (Figure \ref{fig:tagger}) refers to the historical record (e.g. time series sequence of images) of a view of reality from the perceived viewpoint of the user.
A user can inspect their view history across their camera feed and digital devices, 
and use image segment highlighting techniques to annotate interface patterns to detect and subsequently intervene against these patterns.
The user can go through the sequence of images to reflect on their viewing patterns. 
When the user identifies a GUI element they do not wish to see across interfaces and apps, they highlight the region of the image, and annotate it as \texttt{mask-<name-of-intervention>}, and the mask hook will store a mask of intervention \texttt{<name-of-intervention>}, which will then populate a list of available interventions with this option, and the user can choose to activate it during a session.
When a user identifies text (images) that they do not wish to see of similar variations, they can highlight the text (image) region, and annotate it as \texttt{text-<name-of-intervention>} (\texttt{image-<name-of-intervention>}). The text hook retrieves text, and fine-tunes a pre-trained text classification model on the group of text \texttt{<name-of-intervention>}. For images, the highlighted region will be cropped as input to fine-tune a pre-trained image classification model.


\begin{figure}
    \centering
    \caption{
    \textit{View history page}: The page enables the user to traverse and annotate timestamped images of their view.
    }
    \subfigure[Annotating screen views.]{
    \includegraphics[height=2.7cm]{assets/img3.PDF}
    }\hfill
    \subfigure[Annotating egocentric views.]{
    \includegraphics[height=2.7cm]{assets/camera_tagger.pdf}
    }
    \label{fig:tagger}
\end{figure}













