%--------------------------------------------------------------------------------------
%--------------------------------------------------------------------------------------
\subsection{The algorithm}
%--------------------------------------------------------------------------------------

 {We} consider that $X$ follows the mixture model defined by \eqref{def:RMM} and consider $X_{1} , \ldots ,X_{n}$ i.i.d copies of $X$.  We now consider the "empirical fixpoint function", i.e we will consider, denoting $\tau = \left( \tau_{1} , \ldots , \tau_{k} \right)$, and $\tau_{k} = \left( \tau_{1,k} , \ldots , \tau_{n,k} \right)$,
%\SR{
%\begin{align*}
%& \hat{g}_{2,k} \left( \tau_{k} ,  m_{k} \right)= \frac{\sum_{i=1}^{n} \tau_{i,k} \frac{X_{i}}{\left\| X_{i} - m_{k} \right\|} }{\sum_{i=1}^{n} \tau_{i,k} \frac{1}{\left\| X_{i} - m_{k} \right\|}}\\
%& \hat{g}_{3,k} \left( \tau_{k} ,  m_{k} , V_{k} \right)= \frac{\sum_{i=1}^{n}\tau_{i,,k}\frac{\left( X_{i} - m_{k} \right) \left( X_{i} - m_{k} \right)^{T}}{\left\| \left( X_{i} - m_{k} \right) \left( X_{i} - m_{k} \right)^{T} - V_{k} \right\|_{F}}}{\sum_{i=1}^{n} \tau_{i,k} \frac{1}{\left\| \left( X_{i} - m_{k} \right) \left( X_{i} - m_{k} \right)^{T} - V_{k} \right\|_{F}}} .
%\end{align*}
%}
{
\begin{align*}
\hat{g}_{2,k} \left( \tau_{k} ,  m_{k} \right)
& = \left({\sum_{i=1}^{n} \frac{\tau_{i,k} X_{i}}{\left\| X_{i} - m_{k} \right\|} } \right) \left/ \left({\sum_{i=1}^{n} \frac{\tau_{i,k}}{\left\| X_{i} - m_{k} \right\|}} \right) \right. \\
\hat{g}_{3,k} \left( \tau_{k} ,  m_{k} , V_{k} \right)
& = \left( {\sum_{i=1}^{n}\frac{ \tau_{i,,k} \left( X_{i} - m_{k} \right) \left( X_{i} - m_{k} \right)^{T}}{\left\| \left( X_{i} - m_{k} \right) \left( X_{i} - m_{k} \right)^{T} - V_{k} \right\|_{F}}} \right) \left/ \left({\sum_{i=1}^{n} \frac{\tau_{i,k}}{\left\| \left( X_{i} - m_{k} \right) \left( X_{i} - m_{k} \right)^{T} - V_{k} \right\|_{F}}} \right) \right..
\end{align*}
}
This leads to the following algorithm:

\begin{algorithm}[Fix Point algorithm for Robust Mixture Model]
  Starting from $\phi^0 = (\pi^0, m^0, V^0)$, repeat until convergence:
  \begin{enumerate}
  \item Compute for each $1 \leq i \leq n$ and $1 \leq k \leq K$
  $$
  \tau_k^{h+1}(X_i) = \frac{\pi_{k}^{h}\phi_{m_{k}^{h}, \hat{\Psi}_{u} \left( V_{k}^{h} \right)} \left( X_{i} \right)}{\sum_{\ell = 1}^{K} \pi_{\ell}^{h}\phi_{m_{\ell}^{h}, \hat{\Psi}_{u} \left( V_{\ell}^{h} \right)} \left( X_{i} \right)} ,
  $$
  where $\hat{\Psi}_{U}$ is the application which enables, given $V_{k}$, to "rebuild" $\Sigma_{k}$ with the help of one of the method proposed in Section \ref{sec:variance}; 
  \item Based on the fix point relations (see Proposition \eqref{prop:fixpoint}), update, for each $1 \leq k \leq K$,
  $$
  \pi^{h+1}_k = \frac1n \sum_{i=1}^n \tau_k^{h+1}(X_i), \qquad
  m^{h+1}_k = \text{FixPoint}\left( \widehat{g}_{2k}(\tau_{k}^{h},.) \right), \qquad
  V^{h+1}_k =\text{FixPoint}\left( \widehat{g}_{3k}(\tau_{k}^{h},m_{k}^{h},.)\right).
  $$
  where $\text{FixPoint}\left(f(.) \right)$ denotes the fix point of the functional $f$.
  \end{enumerate}
\end{algorithm}
Note that estimating the fix points leads to estimate the weighted median and MCM considering weights $\tau_{k}^{h}$. More intuitively, this algorithm consists in updating $\tau_{i,k}$ replacing the empirical mean and variance of each class by their robust estimates based on the median and the MCM of each class, before updating $\pi$ (as usually).


%--------------------------------------------------------------------------------------
%--------------------------------------------------------------------------------------
\subsection{Choosing the number of clusters}
%--------------------------------------------------------------------------------------

To determine the number of clusters $K$, we resort to two standard penalized-likelihood criteria, namely BIC (\cite{Sch78}) and ICL (\cite{BCG00,MaP00}).
More specifically, denoting by $D_K$ the number of independent parameters involved in a mixture with $K$ clusters and by $\widehat{\mathcal{L}}_K(X)$ the log-likelihood of the dataset $X$ evaluated with the parameter estimates resulting from the proposed estimation procedure:
$$
\widehat{\mathcal{L}}_K(X) = \sum_{i=1}^n \log\left(\sum_{k=1}^K \widehat{\pi}_k \phi_{\widehat{\mu}_k, \widehat{\Sigma}_k}(X_i)\right), 
$$
we used
\begin{equation} \label{eq:modelSel}
  BIC(K) = \widehat{\mathcal{L}}_K(X) - \log(n) D_K/2, \qquad 
  ICL(K) = BIC(K) + \sum_{i=1}^n \sum_{k=1}^K \widehat{\tau}_{i, k} \log \widehat{\tau}_{i, k}.
\end{equation}
We remind that the additional penalty term in the ICL criterion corresponds to the entropy of the conditional distribution of the latent variables $\{Z_i\}_{1 \leq i \leq n}$, conditional on the observed ones $\{X_i\}_{1 \leq i \leq n}$.  {This additional penalty is supposed to favor clusterings with lower classification uncertainty.}

%--------------------------------------------------------------------------------------
\subsection{Initialization of the algorithm}
%--------------------------------------------------------------------------------------
 {
We considered two ways of initializing the algorithm:
\begin{enumerate}
\item[•] Use the robust hierarchical clustering proposed by \cite{gagolewski2016genie}, to get $\tau^{1}$, and run our algorithm from there ;
\item[•] Randomly choose $K$ centers from the data and take $\Sigma_{k} = I_{d}$ and $\pi_{k} = \frac{1}{K}$ for all $k$. 
\end{enumerate}
Remark that the later way can tried several times, so to keep initialization leading to the best final log-likelihood.
We may also use the two ways and keep the best result in term of log-likelihood. 
}
 
% %--------------------------------------------------------------------------------------
% \subsubsection{Initialization of the algorithm}
% %--------------------------------------------------------------------------------------
% Two way for initializing the algorithm are considered: 
% \begin{itemize}
% \item[•] One can initialize the algorithm considering the clustering given by the robust hierarchical clustering proposed by \cite{gagolewski2016genie}, which enables to have $\tau^{1}$, and one can run the end of the algorithm.
% \item[•] One can chose randomly $K$ centers from the data and take $\Sigma_{k} = I_{d}$ and $\pi_{k} = \frac{1}{K}$ for all $k$. Remark that this can be done for several random choice, and one can take the initialization leading to the best final log-likelihood.
% \end{itemize} 
% Remark that one chose these two kind of initialization and take the best choice (in term of maximizing the log-likelihood).

% \SR{
% %--------------------------------------------------------------------------------------
% \subsubsection{Modification of the estimates of $\tau$ and $\pi$}
% %--------------------------------------------------------------------------------------
% The following procedure has been chosen to calculate $\tau_{i,.}^{h+1}$:
% \begin{align*}
% \tau_{i,k}^{h+1/3} & = \max \left\lbrace  \phi_{m_{k}^{h},\hat{\Sigma}_{U}\left( V_{k}^{h}  \right)}\left( X_{i} \right),  e^{-100} \right\rbrace & \forall k \\
% \tau_{i,k}^{h+2/3} & = \frac{\pi_{k}^{h}\tau_{i,k}^{h+1/3}}{\sum_{\ell = 1}^{K}\pi_{\ell}^{h}\tau_{i,\ell}^{h+1/3}} \\
% \tau_{i,k}^{h+1} & = \frac{\tau_{i,k}^{h+2/3} + \epsilon_{\Pi}}{\sum_{\ell = 1}^{K}\left(  \tau_{i,\ell}^{h+2/3} + \epsilon_{\Pi} \right)}
% \end{align*}
% }{[à mettre en annexe ?]}


% \SR{
% %--------------------------------------------------------------------------------------
% \subsubsection{Modification of the robust estimates of the variances}
% %--------------------------------------------------------------------------------------
% Remark that gradient and Robbins-Monro methods can lead to negative estimates of the eigenvalues of the variance (due to estimation error). In order to ensure that the variance of each cluster is positive, and considering $\left( \hat{\lambda}_{1,k} , \ldots ,\hat{\lambda}_{d,k} \right)$ the eigenvalues obtained with the help of MCM combined with one of the Monte Carlo methods, we replace this vector by $\left( \max \left\lbrace \hat{\lambda}_{1,k} ,  \epsilon_{\text{v}} \right\rbrace , \ldots , \max \left\lbrace \hat{\lambda}_{d,k} , \epsilon_{\text{v}} \right\rbrace \right)$ with $\epsilon_{\text{v}}$ chosen arbitrarilly small.
% }{[à mettre en annexe ?]}

% \SR{
% %--------------------------------------------------------------------------------------
% \subsubsection{Chosing the number of clusters}
% %--------------------------------------------------------------------------------------
% The number of clusters is chosen minimizing the ICL criterion. Remark that .... parler des outliers...
% }{[section dédiée dans les simuls]}
