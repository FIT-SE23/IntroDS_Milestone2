%-----------------------------------------------------------------------------------------
\subsection{Mixture model}
%-----------------------------------------------------------------------------------------
We now consider a random variable $X$ following a mixture with $K$ classes, i.e
\begin{equation}
\label{def:RMM} X \sim \sum_{k=1}^K \pi_k^{*} Y_{k},
\end{equation}
that is $Z \sim \Mcal(1, \pi^{*})$ and $(X \mid Z=k) \sim Y_{k}$, where the vector $\pi^{*} = \left( \pi_{1}^{*} , \ldots , \pi_{K}^{*} \right)$ belongs to the simplex $ {\mathcal{S}}^{K}:= \left\{ \pi, \pi_{k} > 0, \sum_{k=1}^{K} \pi_{k} = 1 \right\}$.

Furthermore, we suppose from  now that $Y_{k}$ satisfies the following conditions:
\begin{enumerate}[($a$)]
\item the distribution of $Y_{k}$ is symmetric;
\item $Y_{k}$ admits a second order moment, and we denote by $\mu_{k}^{*}$ and $\Sigma_{k}^{*}$ its mean and variance;
\item the variance of $Y_{k}$ is positive;
\item the random variable $Y_{k}$ is absolutely continuous with density $\phi_{\mu_{k}^{*},\Sigma_{k}^{*}}(.)$ determined by $\mu_{k}^{*}, \Sigma_{k}^{*}$ and known parameters. 
\end{enumerate}

 Observe that these conditions are satisfied by Gaussian mixtures, multivariate Student mixtures (when all the classes have the same known degree of freedom) or multivariate Laplace mixtures (to name a few).  {Conditions ($a$), ($b$) and ($c$)} enable to  {estimate} the mean and the variance  {in a robust manner} with the methods proposed in previous section, while  {Condition ($d$)} just ensures that the density only depends on known parameters or on parameters that can be estimated robustly. Of course, one can adapt this work for more specific cases such as Student mixtures with unknown degrees of freedom. In what follows, we will denote $\mu^{*} = \left( \mu_{1}^{*} , \ldots , \mu_{K}^{*} \right)$, $\Sigma^{*} = \left( \Sigma_{1}^{*} , \ldots , \Sigma_{K}^{*} \right)$ and $\theta^{*} = (\pi^{*} , \mu^{*} , \Sigma^{*})$. 

The popular EM algorithm (\cite{DLR77}) aims at providing the maximum likelihood estimates by minimizing the empirical risk
\[
R_n \left(\pi, \mu , \Sigma \right) - \frac1n \sum_{i=1}^n \sum_{k=1}^{K} \tau_{k}(X_i)  \left(\log \left( \pi_{k} \right) + \log \left( \phi_{\mu_{k}, \Sigma_{k} }\left( X_i \right) \right) \right),
\]
the theoretical counterpart of which is
\[
R \left(\pi, \mu , \Sigma \right) = - \mathbb{E}_{\theta^*}\left[ \sum_{k=1}^{K} \tau_{k}(X)  \left(\log \left( \pi_{k} \right) + \log \left( \phi_{\mu_{k}, \Sigma_{k} }\left( X \right) \right) \right) \right] ,
\]
where 
$
\tau_k(X) 
= \Pr_{\theta^{*} }\left[ Z = k \mid X \right]
{
= {\pi_k^{*} \phi_{\mu_{k}^{*} , \Sigma_{k}^{*}}(X)} \left/ {\sum_{\ell = 1}^K \pi_\ell^{*} \phi_{\mu_\ell^{*},\Sigma_{\ell}^{*}}(X)} \right.
}
$
.

 {Importantly, we now that}
\[\pi^{*} \in \arg\min_{\pi \in  \mathcal{S}^{K}} - \Esp_{\theta^{*}}\left[\sum_{k=1}^K {\tau}_k(X) \log \pi_k\right]
\]
while 
$$
\mu^{*} = \arg\min_{\mu} \mathbb{E}_{\theta^{*}} \left[ \sum_{k=1}^{K} \tau_{k}(X) \left\| X - \mu_k \right\|^{2} \right],  
\qquad 
\Sigma^{*} = \arg\min_{\Sigma} \mathbb{E}_{\theta^{*}}\left[ \sum_{k=1}^{K} \tau_{k}(X) \left\| \left( X - \mu^{*} \right) \left( X - \mu^{*} \right)^{T} - \Sigma_k \right\|_{F}^{2} \right] , 
$$
where $\| .\|_{F}$ is the Frobenius norm for matrices. 

% \paragraph{\SR{}{Version alternative.}}
% \SR{}{
% The popular EM algorithm (\cite{DLR77}) aims at providing the maximum likelihood estimates by finding a fixpoint of the relation
% $$
% \theta^{h+1} = \arg\min_\theta - \Esp_{\theta^h} \left(\log p_\theta(X, Z) \mid X \right).
% $$
% For Gaussian mixture models, the consitional expectation can be decomposed into two terms depending on $\pi$ and ($(\mu, \Sigma)$, respectively:
% \begin{align*}
%   \Esp_{\theta^h} \left(\log p_\theta(X, Z) \mid X \right)
%   & = \Esp_{\theta^h} \left(\log p_\pi(Z) \mid X \right) 
%   + \Esp_{\theta^h} \left(\log p_{\mu, \Sigma}(X \mid Z) \mid X \right) \\
%   & = \sum_{k=1}^K \tau_k(X) \log \pi_k 
%   - \frac12 \sum_{k=1}^K \tau_k(X) \left(\log |\Sigma_k| + (X - \mu_k)^\top \Sigma_k^{-1} (X - \mu_k)\right).
% \end{align*}
% }



%-----------------------------------------------------------------------------------------
\subsection{Loss}
%-----------------------------------------------------------------------------------------
Consider a mixture model as defined in \eqref{def:RMM} with parameter $\theta^{*} = (\pi^{*} , \mu^{*} , \Sigma^{*})$ and let us denote by $m^{*} = \left( m_{1}^{*} , \ldots , m_{K}^{*} \right)$ and $V^{*} = \left( V_{1}^{*}, \ldots , V_{K}^{*} \right)$ the medians and MCM of the classes. Intuitively, the idea is to replace, in the usual EM algorithm, the estimates of the mean $\mu_{k}$ and the variance $\Sigma_{k}$ of each  class by their robust version. More precisely, the aim is to replace them by the median $m_{k}^{*}$ and the transformation of the MCM $\Psi_{U}\left( V_{k}^{*} \right)$ of each class.
 {Still}, as we cannot know the class of the data,  {we need to} give an alternative definition of the median and MCM of the classes.   To do so, let us introduce the two following functions:
\begin{align*}
   & G_2(m)  = \Esp_{\theta^{*} }\left[\sum_{k=1}^K \tau_k(X) \left\|X - m_k\right\|\right] 
   & G_3(m, V) 
    = \Esp_{\theta^{*} }\left[\sum_{k=1}^K  \tau_k(X) \left\|(X - m_k)(X - m_k)^\intercal - V_k\right\|_{F}\right]. 
\end{align*}

The following proposition ensures that the minimizers of these functions correspond to $m^{*}$ and $V^{*}$, which will be  {central in the construction of} robust estimates of $\theta^{*}$.
\begin{Prop}\label{prop1}
  Consider a mixture model as defined in \eqref{def:RMM} and parametrized with $\theta^* = (\pi^*, \mu^*, \Sigma^*)$. Then
  $$    m^* = \arg\min_m \Esp_{\theta^*}  \left[ G_2( m)\right], \quad \quad \text{and} \quad \quad
  V^* = \arg\min_V \Esp_{\theta^*} \left[G_3( m^*, V)\right].
  $$
  Furthermore, $m^{*} = \mu^{*}$, $\Psi_{U} \left( V^{*} \right) := \left( \Psi_{U} \left( V_{1}^{*} \right) , \ldots , \Psi_{U} \left( V_{K}^{*} \right) \right) = \Sigma^{*}$,
  $$
  \tau_{k}(X) = {\pi_k^{*} \phi_{m_{k}^{*} , \Psi_{U}\left( V_{k}^{*} \right)}(X)}\left/ {\sum_{\ell = 1}^K \pi_\ell^{*} \phi_{m_\ell^{*},\Psi_{U}\left( V_{\ell}^{*} \right)}(X)} \right.,
  $$
%   \SR{$\tau_{k}(X) = \frac{\pi_k^{*} \phi_{m_{k}^{*} , \Psi_{U}\left( V_{k}^{*} \right)}(X)}{\sum_{\ell = 1}^K \pi_\ell^{*} \phi_{m_\ell^{*},\Psi_{U}\left( V_{\ell}^{*} \right)}(X)}$,}{}\SRcom{Déjà donné. C'est pour vraiment dire que l'on a tout reparamétrisé} 
  and 
  \[
  R_{\pi^{*}} \left(   m^{*} , \Psi\left( {V}^{*} \right) \right) = \min_{\mu ,\Sigma} R_{\pi^{*}}\left( \mu , \Sigma \right) =  R_{\pi^{*}}\left( \mu^{*} , \Sigma^{*} \right). 
  \]
\end{Prop}
In other words, we propose here a new parametrization of the problem where the new parameters correspond to robust indicators.
The proof is given in Section \ref{sec:proofs}.


\subsection{Fix-point property} 
The following proposition enables to see $\left( \pi^{*},m^{*},V^{*} \right)$ as a fixpoint of a function $g^{*}$.
\begin{Prop}\label{prop:fixpoint}
    Consider a mixture model as defined in \eqref{def:RMM} and parametrized with $\theta^* = (\pi^*, \mu^*, \Sigma^*)$. Then, $\left( \pi^{*},m^{*},V^{*} \right)$ (with $\pi^{*},m^{*},V^{*}$ defined in Proposition \ref{prop1}) satisfy
\[
\left( \pi^{*},m^{*},V^{*} \right) = g^{*} \left( \pi^{*},m^{*},V^{*} \right)    
\]
where $g^{*}(\pi , m , V) = \left( g_{1}^{*}(\pi),g_{2,1}^{*}\left(m_{1}\right)),\ldots ,g_{2,K}^{*}\left(m_{K}\right) ,g_{3,1}^{*}\left(V_{1}\right),g_{3,K}^{*}\left(V_{K}\right) \right)$ with $g_{1}(\pi) = \left( g_{1,1} \left( \pi \right) , \ldots , g_{1,K} \left( \pi \right) \right) $ and
%\SR{
%\begin{align*}
% g_{1,k}(\pi) & := \mathbb{E}\left[ \frac{ \pi_{k} \phi \left( X , m_{k}^{*} , \Psi_{U} \left( V_{k}^{*} \right) \right) }{\sum_{i=1}^{K}\pi_{i}  \phi \left( X , m_{i}^{*} , \Psi_{U} \left( V_{i}^{*} \right)  \right)} \right] \quad
%  g_{2,k}\left( m_{k} \right)   := \frac{\mathbb{E}\left[  \frac{\tau_{k}(X)X}{\left\| X - m_{k}  \right\|} \right]   }{ \mathbb{E}\left[  \frac{\tau_{k}(X)}{\left\| X - m_{k}  \right\|} \right]}   \quad
%  g_{3,k} \left( V_{k}  \right)     := \frac{\mathbb{E}\left[  \frac{\tau_{k}(X) \left( X - m_{k}^{*} \right) \left( X - m_{k}^{*} \right)^{T} }{\left\| \left( X - m_{k}^{*} \right) \left( X - m_{k}^{*} \right)^{T} - V_{k}  \right\|_{F}} \right]   }{ \mathbb{E}\left[  \frac{\tau_{k}(X)  }{\left\| \left( X - m_{k}^{*} \right) \left( X - m_{k}^{*} \right)^{T} - V_{k}  \right\|_{F}} \right]}  
%\end{align*}
%}
{
\begin{align*}
  g_{1,k}(\pi) & := \mathbb{E}\left[ \frac{ \pi_{k} \phi \left( X , m_{k}^{*} , \Psi_{U} \left( V_{k}^{*} \right) \right) }{\sum_{i=1}^{K}\pi_{i}  \phi \left( X , m_{i}^{*} , \Psi_{U} \left( V_{i}^{*} \right)  \right)} \right] \qquad
  g_{2,k}\left( m_{k} \right) := \frac{\mathbb{E}\left[  \frac{\tau_{k}(X)X}{\left\| X - m_{k}  \right\|} \right]   }{ \mathbb{E}\left[  \frac{\tau_{k}(X)}{\left\| X - m_{k}  \right\|} \right]}   \\
  g_{3,k} \left( V_{k}  \right) & := {\mathbb{E}\left[  \frac{\tau_{k}(X) \left( X - m_{k}^{*} \right) \left( X - m_{k}^{*} \right)^{T} }{\left\| \left( X - m_{k}^{*} \right) \left( X - m_{k}^{*} \right)^{T} - V_{k}  \right\|_{F}} \right]   } \left( { \mathbb{E}\left[  \frac{\tau_{k}(X)  }{\left\| \left( X - m_{k}^{*} \right) \left( X - m_{k}^{*} \right)^{T} - V_{k}  \right\|_{F}} \right]} \right)^{-1}  
\end{align*}
}
and  {$\tau_{k}(X) = {\pi_k^{*} \phi_{m_{k}^{*} , \Psi_{U}\left( V_{k}^{*} \right)}(X)} \left/ {\sum_{\ell = 1}^K \pi_\ell^{*} \phi_{m_\ell^{*},\Psi_{U}\left( V_{\ell}^{*} \right)}(X)} \right.$}.
\end{Prop}
