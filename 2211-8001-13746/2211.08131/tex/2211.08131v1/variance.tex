\subsection{Estimating the geometric median}
In {this section, we} consider a random variable $X$  {with} values in $\mathbb{R}^{d}$. The geometric median of $X$ is defined (see \cite{Hal48,Kem87}) by 
\[
m^{*} \in \arg\min_{m \in \mathbb{R}^{d}} \mathbb{E} \left[ \left\| X - m \right\| - \| X \| \right]
\]
where $\|\cdot\|$ stands for the $\ell_2$ norm.
Remark that the term $\| X \| $ just enables not to have to make any assumption on the existence of the first order moment of the  random vector $X$. If the random variable $X$ is not concentrated on a straight line nor single points, the geometric median is uniquely defined \cite{Kem87}. An iterative way to estimate the median, giving i.i.d. copies $X_{1} , \ldots ,X_{n}$ of $X$, is to consider the median as a fix point, leading to the following Weiszfeld algorithm \cite{Weiszfeld1937,VZ00}:
\begin{equation}
\label{def:Weisz} m_{t+1} 
{
= \frac{\sum_{k=1}^{n} {X_{k}}/{\left\| X_{k} - m_{t} \right\|}}{\sum_{k=1}^{n} {1}/{\left\| X_{k} - m_{t} \right\|}}
}
.
\end{equation}
A recursive and faster way (in term of computational cost) to estimate the median is to consider the Averaged Stochastic Gradient (ASG) algorithm (\cite{HC,CCG2015,godichon2015}) defined recursively for all $k \leq n-1$ by
\begin{align*}
m_{k+1} & = m_{k} + \gamma_{k+1} \frac{X_{k+1} - m_{k}}{\left\| X_{k+1} - m_{k} \right\|} \\
\overline{m}_{k+1} & = \overline{m}_{k} + \frac{1}{k+1} \left( m_{k+1} - \overline{m}_{k} \right)
\end{align*}
where $m_{0} = \overline{m}_{0}$ is arbitrarily chosen, $\gamma_{k} = c_{\gamma}k^{-\gamma}$ with $c_{\gamma} > 0 $ and $\gamma \in (1/2,1)$. Remark that under weak assumptions, these estimates (ASG and Weiszfeld) are asymptotically efficient (\cite{HC,VZ00}). Nevertheless, in case of small samples lying in moderate dimension spaces, one should prefer Weiszfeld algorithm and vice versa.

\begin{rem}
Remark that for mixture model, we will consider a weighted version of the median, i.e considering a positive random variable $w$, we will consider
\[
m^{*} = \arg\min_{m} \mathbb{E}\left[ w \left\| X - m \right\| - w \| X \| \right]
\]
leading, considering $\left( X_{1} , w_{1} \right) , \ldots , \left( X_{n},w_{n} \right)$, to the following transformation of the algorithms
\[
m_{t+1} 
= 
{
\frac{\sum_{k=1}^{n} {w_{k}X_{k}} / {\left\| X_{k} - m_{t} \right\|}}{\sum_{k=1}^{n} {w_{k}}/ {\left\| X_{k} - m_{t} \right\|}}
}
\qquad \text{and} \qquad m_{k+1}  = m_{k} + \gamma_{k+1}w_{k+1} \frac{X_{k+1} - m_{k}}{\left\| X_{k+1} - m_{k} \right\|} .
\]
\end{rem}


\subsection{Estimating the Median Covariation Matrix}

The Median Covariation Matrix (MCM for short) is defined (\cite{KrausPanaretos2012}, \cite{CG2015}) by 
\[
V^{*} \in \arg\min_{V \in \mathcal{M}_{d}\left( \mathbb{R} \right)} \mathbb{E}\left[ \left\| \left( X - m^{*} \right)\left( X - m^{*} \right)^{T} - V \right\|_{F} - \left\| \left( X - m^{*} \right)\left( X - m^{*} \right)^{T} \right\|_{F} \right]
\]
where $m^{*}$ is the geometric median of $X$, $\mathcal{M}_{d}\left( \mathbb{R} \right)$ denotes the vectorial space of squared real matrices of size $d \times d$ and $\| . \|_{F}$ is the associated Frobenius norm. In other words, the MCM can be seen as the geometric median of the random matrix $\left( X - m^{*} \right) \left( X - m^{*} \right)^{T}$. Then, given the estimate $m_{T}$ of $m^{*}$ obtained with \eqref{def:Weisz} after $T$ iterations, one can consider the Weiszfeld algorithm \cite{CG2015}
$$
V_{t+1} = 
{
\frac{\sum_{k=1}^{n} {\left\| \left( X_{k} - m_{T} \right) \left( X_{k} - m_{T} \right)^{T} - V_{t} \right\|^{-1}_{F}} {\left( X_{k} - m_{T} \right) \left( X_{k} - m_{T} \right)^{T}} }{\sum_{k=1}^{n} {\left\| \left( X_{k} - m_{T} \right) \left( X_{k} - m_{T} \right)^{T} - V_{t} \right\|^{-1}_{F} } }
}.
$$
In the same way, one can both estimate the median and the MCM recursively considering the ASG algorithm
\begin{align*}
V_{k+1} & = V_{k} + \gamma_{k+1} \frac{\left( X_{k+1} - \overline{m}_{k} \right)\left( X_{k+1} - \overline{m}_{k}\right)^{T} - V_{n}}{\left\| \left( X_{k+1} - \overline{m}_{k} \right)\left( X_{k+1} - \overline{m}_{k}\right)^{T} - V_{n} \right\|_{F}} \\
 \overline{V}_{k+1} & = \overline{V}_{k} + \frac{1}{k+1}\left( V_{k+1} - \overline{V}_{k} \right) , 
\end{align*}
with $\overline{V}_{0} = V_{0}$ symmetric and positive. {First observe} that the estimates are not necessarily positive, but one can project {them}  onto the set of definite positive matrices or consider the modification of the stepsequence proposed in \cite{CG2015}. 
Remark  {also} that, here again, one can consider the weighted version of the MCM and modify the algorithm {accordingly}.


\subsection{Robust estimation of the variance}
Let us  {now suppose} that $X$ admits a second order moment and   denote by $\mu$ and $\Sigma $ its mean and variance (supposed to be positive). All this work relies on the fact that, if the distribution of $X$ is symmetric, $V^{*}$ and $\Sigma$ have the same eigenvectors (\cite{KrausPanaretos2012}). Furthermore, denoting $U=\left( U_{1} , \ldots , U_{d} \right)^{T}  := \Sigma^{-1/2} \left( X - \mu \right)$ and $\delta$ (resp. $\lambda$) the vector of eigenvalues (by decreasing order) of $V^{*}$ (resp. $\Sigma$), one has (\cite{KrausPanaretos2012}): 
\begin{equation}
\label{link:delta:lambda} 
 {\delta_{k}} = \lambda_{k} \mathbb{E}\left[ U_{k}^{2} h\left( \delta , \lambda , U \right)   \right]\left( \mathbb{E}\left[ h \left( \delta , \lambda ,  U \right)  \right] \right)^{-1}
\end{equation}  
where $h(\delta , \lambda , U ) := \left( \sum_{i=1}^{d} \left(  {\delta_{i}} - \lambda_{i}U_{i}^{2} \right)^{2} + \sum_{i \neq j} \lambda_{i}\lambda_{j} U_{i}^{2}U_{j}^{2}\right)^{-1/2}$. In what follows, we will denote by $\Psi_{U}$ the function such that
\begin{equation}
\label{def:Psi} \Psi_{U} \left( V^{*} \right) = \Sigma.
\end{equation}
 Let us suppose from now that the law of $U$ is known and that we know how to simulate i.i.d random variables following this law. For instance, for the Gaussian case, it is  {clear} that $U \sim \mathcal{N}\left( 0 ,I_{d} \right)$. In a same way, for the multivariate Student with $p$ degrees of freedom (with $p \geq 3$), one has $U = \sqrt{p-2} {{N_{d}} / {\sqrt{K_{p}}}}$ where $N_{d} \sim \mathcal{N}\left(0 , I_{d} \right)$ and $ K_{p} \sim \chi_{p}^{2}$ are independent. Finally, for the multivariate case, $U$ follows a standard multivariate law. Let us now consider i.i.d. copies of $X$ and an estimate of the MCM $V_{n}$. Let us denote by $\delta_{n} = \left( \delta_{1,n} , \ldots , \delta_{d,n} \right)$ and $\left( v_{1,n} , \ldots , v_{p,n} \right)$ the eigenvalues (by {decreasing} order) and the associated eigenvectors,  {respectively}. 
  {Robust estimates of the eigenvalues of the variance can be obtained via a Monte-Carlo approach, based on $N$ i.i.d. copies $U_{1},\ldots ,U_{N}$ of $U$.} 
 A first solution to estimate $\lambda$ is so to consider the following fix point algorithm:
\begin{algorithm}[Fix point algorithm]
For all $t \in \mathbb{N}$, and $k=1, \ldots ,d$,
\[
\lambda_{n,N,t+1}[k] = \delta_{n}[k] \frac{\sum_{i=1}^{N} h \left( \delta_{n}, \lambda_{n,N,t},U_{i} \right) }{\sum_{i=1}^{N} \left( U_{i}[k] \right)^{2} h \left( \delta_{n}, \lambda_{n,N,t},U_{i} \right)}
\]
where for all $x= \left( x_{1} , \ldots , x_{d} \right)^{T} \in \mathbb{R}^{d}$, $x[k] = x_{k}$. 
\end{algorithm}

\noindent Remark that this method does not require to calibrate any hyperparameter, as opposed to the possibly more efficient following gradient algorithm.

\begin{algorithm}[Gradient algorithm]
For all $t \in \mathbb{N}$,
\[
\lambda_{n,N,t+1} = \lambda_{n,N,t} - \eta_{t}\sum_{k=1}^{n} \lambda_{n,N,t}\left(  U_{i}^{2}  h\left( \delta_{n}, \lambda_{n,N,t},U_{i} \right) -  \delta_n h   \left( \delta_{n}, \lambda_{n,N,t},U_{i} \right) \right)
\]
where $\eta_{t}$ is non-decreasing positive step sequence.
\end{algorithm}

\noindent One may refer to the classical literature to calibrate the step sequence. 
Finally, one may resort to the recursive Robbins-Monro  algorithm \cite{robbins1951} or, more specifically, to its weighted averaged version \cite{mokkadem2011generalization}.
\begin{algorithm}[Robbins-Monro]
For all $k \leq N-1$, one has
\begin{align*}
\lambda_{n,N,k+1} & = \lambda_{n,N,k} - \gamma_{k+1}  \left( \lambda_{n,N,k} U_{k+1}^{2}  h\left( \delta_{n}, \lambda_{n,N,k},U_{k+1} \right) -  \delta_{n} h   \left( \delta_{n}, \lambda_{n,N,k},U_{k+1} \right) \right), \\
\overline{\lambda}_{n,N,k+1} & = \overline{\lambda}_{n,N,k} + \frac{\log (k+1)^{w}}{\sum_{l=0}^{k}\log(l+1)^{w}}\left( \lambda_{n,N,k+1} - \overline{\lambda}_{n,N,k} \right),
\end{align*}
with $\overline{\lambda}_{n,N,0}=\lambda_{n,N,0}$, $\gamma_{k} = c_{\gamma}k^{-\gamma}$ with $c_{\gamma}> 0$ and $\gamma \in (1/2,1)$, $\omega \geq 0$.
\end{algorithm}

\noindent 
{
The term 'weighted averaging' comes from the update formula $\overline{\lambda}_{n,N,k} = \sum_{l=0}^{k}\log(l+1)^{\omega}  {\lambda}_{n,N,l} \left/ {\sum_{l=0}^{k}\log(l+1)^{\omega}} \right.$. 
}
Note that the case  $w = 0$ corresponds to the usual averaged algorithm \cite{ruppert1988efficient,PolyakJud92}. 

Remark that, under suitable assumptions, these three methods have the same asymptotic (on $N$) behavior. Nevertheless, the two first ones necessitate $O\left(Nd^{2}T \right)$ operations, where $T$ is the number of iterations, while the Weighted Averaged Robbins Monro algorithm  {only requires} $O \left( Nd^{2} \right)$ operations.  {Hence, using $TN$ copies instead of $N$, one expect a better precision with the last method, for the same computational time.}
A comparative study of these algorithms  {is presented} in Section \ref{sec:simVariance}.
