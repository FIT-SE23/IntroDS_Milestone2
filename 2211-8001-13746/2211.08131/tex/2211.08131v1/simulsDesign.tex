%-------------------------------------------------------------------------------
%-------------------------------------------------------------------------------
\subsection{Simulation design} \label{sec:simDesign}
%-------------------------------------------------------------------------------

%-------------------------------------------------------------------------------
\paragraph{Simulation parameters.}
We considered random vectors $X$ with dimension $p=5$ and mixture models with $K = 3$ clusters with equal proportions. We defined the three mean vectors $\mu_1$, $\mu_2$ and $\mu_3$, each with their $p$ coordinates all equal to $0$, $3$ and $-3$, respectively (see Equation \eqref{eq:mu} in Appendix \ref{app:simDesign}). We defined the three covariance matrices $\Sigma_1$, $\Sigma_2$ and $\Sigma_3$ given in Equation \eqref{eq:Sigma} in Appendix \ref{app:simDesign}.  {To give different shape to the three distributions}, $\Sigma_1$ has a constant diagonal term (equal to $2$), $\Sigma_2$ has diagonal terms increasing from $1$ to $p$ and $\Sigma_3$ has diagonal terms decreasing from $1$ to $1/p$. The two considered mixture distributions were therefore 
\begin{align*}
  \text{Gaussian: } & K^{-1} \sum_{k=1}^K \mathcal{N}_p\left(\cdot; \mu_k^*, \Sigma_k^*\right), &
  \text{Student: } & K^{-1} \sum_{k=1}^K \mathcal{T}_p\left(\cdot; \mu_k^*, S_k^* = \nu^{-1} (\nu-2) \Sigma_k^*, \nu\right).
\end{align*}
For Student distributions, the scale matrix $S_k$ was adapted so that the variance  {in  class $k$} was   $\Sigma_k$ and the number of degrees of freedom of each cluster was set to $\nu = 3$. 
The simulations dedicated to variance estimation were carried with a null mean vector and the covariance matrix $\Sigma_0$ given in Equation \eqref{eq:Sigma} in Appendix \ref{app:simDesign}.

%-------------------------------------------------------------------------------
\paragraph{Contamination scenarios.}
A contamination rate $\delta$ ranging from 0 (no contamination) to $50\%$ was applied to each cluster. Namely, a same fraction $\delta$ of the observations of each cluster $k = 1, \dots K$ was drawn with one of the five following contaminating distributions (hereafter referred to as 'scenarios')
\begin{enumerate}[($a$)]
  \item uniform distribution over the hypercube: $\mathcal{U}\{[-20, 20]^p\}$;
  \item Student distribution with null location vector, identity scale matrix and degree of freedom 1: $\mathcal{T}(0_p, I_p, 1)$; 
  \item Student distribution with location vector $\mu_k^*$, identity scale matrix and degree of freedom 1: $\mathcal{T}(\mu_k^*, I_p, 1)$; 
  \item Student distribution with null location vector, identity scale matrix and degree of freedom 2: $\mathcal{T}(0_p, I_p, 2)$; 
  \item Student distribution with location vector $\mu_k^*$, identity scale matrix and degree of freedom 2: $\mathcal{T}(\mu_k^*, I_p, 2)$. 
\end{enumerate}
Observe that, when considering one single cluster with null location parameter, scenarios ($c$) and ($e$) are equivalent to scenarios ($b$) and ($d$), respectively.
The contaminating distribution has no first two moments under scenarios ($b$) and ($c$), and no variance under scenarios ($d$) and ($e$). Under scenarios ($c$) and ($e$), the contaminating distribution has the same center as the corresponding cluster so the outliers can be considered as belonging to the cluster, whereas outliers arising from different clusters can not be distinguished under scenarios ($a$), ($b$) and ($d$).

%-------------------------------------------------------------------------------
\paragraph{Maximum likelihood estimates.}
For both Gaussian and Student mixtures models, we compared our results with the maximum likelihood estimates (MLE) provided by the R packages {\tt mclust} \cite{SFM16} for Gaussian mixtures and by the {\tt teigen} \cite{AWB18} for the Student mixtures. In the sequel, the corresponding algorithms and results will be referred to as GMM and TMM, respectively. The robust counterparts we propose will be referred to as RGMM and RTMM. For all the fours methods we carried the inference either with a fixed number of clusters $K$, or letting a model selection criterion (see below) choose an optimal number of clusters $\widehat{K}$.

%-------------------------------------------------------------------------------
\paragraph{Evaluation criteria.}
For each simulated dataset, we run the four algorithms (with fixed or selected $K$) and obtained estimates of the parameters $\mu_k$ and $\Sigma_k$, as well as a classification of each observation.
\begin{description}
 \item[Classification:] we used the adjusted Rand index (ARI) to compare the estimated classification with the simulated one. 
 \item[Parameter estimates:] when considering the true number of cluster $K$, we computed 
 \begin{itemize}
 \item the mean squared error for the center: $MSE(\mu) = K^{-1} \sum_k \|\mu_k^* - \widehat{\mu}_k\|^2/p$,
%  \item \SR{the KL divergence for the covariance: $KL(\Sigma) = K^{-1} \sum_k KL(\mathcal{N}(0_p, \Sigma_k^*) || \mathcal{N}(0_p, \widehat{\Sigma}_k))$,}{}
 \item {the mean squared error for the covariance: $MSE(\Sigma) = K^{-1} \sum_k \|\Sigma_k^* - \widehat{\Sigma}_k\|^2/p^2$.}
%  \item the KL divergence for the clusters : $KL(\Sigma) = K^{-1} \sum_k KL(\mathcal{N}(\mu_k^*, \Sigma_k^*) || \mathcal{N}(\widehat{\mu}_k, \widehat{\Sigma}_k))$,
 \end{itemize}
%   where $KL(p||p)$ stands for the Küllback-Leibler between the distribution $p$ and $q$. 
%   We used the adapted criteria, when dealing considering (mixtures of) Student distributions. A close-form expression of the KL divergence exists in the Gaussian case (see e.g. \cite{QSL16}), but we used a Monte-Carlo estimate of it in the Student case.
\item[Model selection:] when considering the case of unknown number of cluster, we considered both the BIC and the ICL  criteria given in Equation \eqref{eq:modelSel}. 
%\SR{\item[Initialization of the algorithm:] Two kinds of initialization are considered:
%\begin{itemize}
%\item[•] One can initialize the algorithm considering the clustering given by the robust hierarchical clustering proposed by \cite{gagolewski2016genie}, which enables to have $\tau^{1}$, and one can run the end of the algorithm.
%\item[•] One can chose randomly $K$ centers from the data and take $\Sigma_{k} = I_{d}$ and $\pi_{k} = \frac{1}{K}$ for all $k$. Remark that this can be done for several random choice, and one can take the initialization leading to the best final log-likelihood.
%\end{itemize}
%Note that one can chose these two kinds of initialization and take the best choice (in term of maximizing the log-likelihood). }{}\SRcom{Pas là.}
\end{description}



