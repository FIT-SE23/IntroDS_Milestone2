\newpage
\appendix

\section{Additional Related Work}\label{app:additional-related-work}

\head{Novelty of the \textit{Snapshot Encoder} Architecture}
In the literature, several studies combine GNNs with recurrent models like GRU cells or Transformers. Here, we want to emphasize that the architecture of \textit{Snapshot Encoder} is quite different from them in different aspects. As we discussed in \autoref{sec:related-work}, we can categorize previous works on the combination of GRU and GNNs into two groups. The first group replaces RNN's linear layer with graph convolution layer~\cite{dynamic_rnn1, dynamic_rnn2, dynamic_rnn3}.  
The second group uses a GNN as a feature encoder and then deploys a sequence model on top of the GNN to encode temporal properties~\cite{dynamic_gnn1, dynamic_gnn2, dynamic_gnn3}, which ignores the evolution of lower-level node embeddings. Also, these models have limitations in training strategy~\cite{roland}. ROLAND~\cite{roland} has addressed these limitations, but it is limited to single-layer graphs. Moreover, natural attempts to use multiplex graph neural networks~\cite{CS-MLGCN, multiplex_int1, multiplex_int2, multiplex_int3, multiplex_gcn1, multiplex_gcn2} in the ROLAND framework (e.g. replace GNN block with multiplex GNN or GCN) lead to ignoring historical data in other relation types (layers). For example, assume that we want to use our attention mechanism (see~\autoref{sec:method}) in the ROLAND framework. Then, we need to use the attention mechanism in the GNN block, which means we incorporate the information about different relation types before embedding the \textit{Embedding update} module. Accordingly, for each timestamp, we do not incorporate historical data on other relation types, which could result in undesirable performance. 


\head{Feature Learning and Anomaly Detection in Brain Networks}
In recent years, several studies focused on analyzing brain networks to understand and distinguish healthy and diseased human brains~\cite{nongnn_brain_network1, nongnn_brain_network2, nongnn_brain_network3}. Recently, due to the success of GNNs in analyzing graph-structured data, deep models have been proposed to predict brain diseases by learning the graph structures of brain networks\cite{classification_brain_network, classification_brain_network2, classification_brain_network3, classification_brain_network4, classification_brain_network5}. In addition to predicting disease in brain networks, understanding the cause of the disease is important. To this end, several anomaly detection methods have been proposed to find anomalous connections, regions, or subgraphs in the brain, which can cause a disease~\cite{anomaly_brain, anomaly_brain1, anomaly_brain2}. However, all these anomaly detection models apply to single-layer networks only and do not naturally extend to multiplex networks; while a brain network generated from an individual can be noisy and incomplete \cite{Brain_network_fmri, FirmTruss}. To the best of our knowledge, \model{} is the first method for detecting  anomalous connections in multiplex brain networks.


\head{Anomaly Detection in Blockchain Networks}
Anomaly detection in blockchain transaction networks has recently attracted enormous attention~\cite{blockchain_anomaly1, blockchain_anomaly2, blockchain_anomaly3, blockchain_anomaly4, blockchain_anomaly5, blockchain_anomaly_survey}, due to the emergence of a huge assortment of financial systems' applications~\cite{blockchain_application1, blockchain_application2, blockchain_application3}.  However, most existing work focuses on detecting illicit activity in a single blockchain network, while recent research shows that cryptocurrency criminals increasingly employ cross-cryptocurrency trades to hide their identity~\cite{crypto_criminals, ofori2021topological}. Accordingly, \citet{blockchain_ml_first} has recently shown that analyzing links across several blockchain networks is critical for identifying emerging criminal activity on the blockchain. To this end, \citet{ofori2021topological} developed a new persistence summary and utilized it to detect events in dynamic multiplex blockchain networks. For additional related work, we refer the reader to the extensive survey by \citet{blockchain_anomaly_survey}. To the best of our knowledge, \model{} is the first method for detecting anomalous transactions in multiple blockchain networks.



\section{\model{} Algorithm}\label{app:algorithm}
\model{}'s algorithm~appears~in~detail~in Algorithm~\ref{alg:model}.
\input{anomuly_algorithm}


\section{Experimental Configuration}\label{app:config}
\input{config_table}
In the architecture of \encoder{} we use 200 hidden dimensions for node states, GNN layers with skip-connection, sum aggregation, and batch-normalization. We tune hyper-parameters by cross-validation on a rolling basis, and search the hyper-parameters over \textbf{(i)} numbers of layers (1 layer to 5 layers); \textbf{(ii)} learning rate (0.001 to 0.01); and \textbf{(iii)} the margin $\gamma$ (0.3 to 0.7). The values of other hyper-parameters are reported in \autoref{tab:config}.


\section{Datasets}\label{app:datasets-details}
We use nine real-world public datasets~\cite{RM, FirmTruss, ofori2021topological, DKPol, bitcoin_alpha, amazon_dataset_main} whose domains cover social, co-authorship, blockchain, and co-purchasing networks. We summarize their statistics in \autoref{tab:datastat}. 

\head{Social Networks} RM~\cite{RM} has 10 networks, each with 91 nodes. Nodes represent phones and one edge exists if two phones detect each other under a mobile network. Each network describes connections between phones in a month. DKPol~\cite{DKPol} is collected during the month leading to the 2015 Danish parliamentary election on Twitter. Nodes are Twitter accounts of Danish politicians, and relations are "Retweet", "Reply", and "Topical Interaction"~\cite{DKPol}.

\head{Co-purchasing Network} Amazon~\cite{amazon_dataset_main} is a co-purchasing network, where each node is an item and the type of connections are "Also-view" and "Also-bought". We focused on items with four categories, i.e., Beauty, Automotive, Patio Lawn and Garden, and Baby. 

\head{Collaboration Network} DBLP is a co-authorship network until 2014 from \cite{dblp}. In this dataset, each node is a researcher, an edge shows collaboration and each type of connection is a topic of research. For each collaboration, we consider the bag of words drawn from the titles of the paper and apply LDA topic modeling~\cite{LDA} to automatically identify 240 topics. We then cluster their non-zero elements into ten known research topics. 


\head{Blockchain Networks} Ethereum~\cite{ofori2021topological} is a blockchain transaction network over 576 days, where layers are different tokens, nodes are addresses of investors, and edge denotes the transferred token value. Since the same address may trade multiple tokens, the address appears in networks of all the tokens it has traded. Ripple~\cite{ofori2021topological} is derived from the Ripple Credit Network and covers a timeline of Oct-2016 to Mar-2020. Similar to the Ethereum dataset, nodes are investors and edges represent transactions. Here layers (relation types) correspond to the five most issued fiat currencies on the Ripple network: JPY, USD, EUR, CCK, and CNY.


\head{Brain Network} We use ADHD-Brain dataset in our case study. This dataset~\cite{brain_main_dataset} is derived from the functional magnetic resonance imaging (fMRI) of 40 individuals (20 individuals in the condition group, labeled  ADHD, and 20 individuals in the control group, labeled  TD) with the same methodology used by~\citet{Brain_network_fmri}. Here, each layer (relation type) is the brain network of an individual person, where nodes are brain regions, and each edge measures the statistical association between the functionality of its endpoints.


\head{Single-layer Networks} DBLP-S is a subgraph of the multiplex DBLP network, where we collect a subset of researchers who work on data mining and related areas. Amozon-S, is a subgraph of the multiplex Amazon network, where we focus on the "Also-view" relation type. Bitcoin dataset contains who-trusts-whom network of people who trade on the Alpha platforms~\cite{bitcoin_alpha}.

Note that all of the datasets are anonymized and does not contain any personally identifiable information or offensive content.

\subsection{Inject Anomalous Edges in Multiplex Networks}
\label{app:inject_anomaly}
Since the ground truth for anomaly detection is difficult to obtain~\cite{anomaly-survey2}, we follow the existing studies\cite{NetWalk, AddGraph, anomaly-survey2} and inject anomalous edges into our datasets. However, existing methods only inject anomalous edges in a single-layer network and cannot add complex anomalies in multiplex networks. Accordingly, here we divide injected edges into two groups (50\% each), \textbf{(1)} layer-independent anomalies, and \textbf{(2)} layer-dependent anomalies. For the first group, we use existing methods~\cite{anomaly-survey2}. In multiplex networks, the rich information about node connections leads to repetitions, meaning edges between the same pair of nodes repeatedly appear in multiple layers. Repeated connections are more likely to be a strong tie and even its nodes belong to the same community \cite{Redundancy-ML-Community, FirmTruss}. Accordingly, in the second type of injected anomalies, we inject random connections that do not appear in any relation type. That is, we first choose a random edge $(u, v)$ such that $(u, v) \not\in \E_k$ for all $k \in \LL$, and then we inject this edge to a random relation type $r \in \LL$. Since this connection does not appear in any relation type, it is more likely to be an anomaly. This type of anomaly helps us to understand whether \model{} can take advantage of complementary information of different relation types.



\section{Additional Experimental Results}\label{app:additional_experiment}

\subsection{Parameter Sensitivity}
\begin{figure}[t]
    \centering
    \includegraphics[width=0.7\linewidth]{training_ratio.pdf}
    \vspace{-1mm}
    \caption{Stability over different training ratio on Ripple.}
    \label{fig:training_ratio}
\end{figure}
We evaluate the effect of the training ratio of the dataset: we change the training ratio from 60\% to 20\% and report the results on the Ripple dataset in \autoref{fig:training_ratio}. Decreasing the training ratio tends to increase both the average and maximum AUC, except for the 20\% case,
and decreases the minimum AUC for all training ratios.
These results show that performance stays relatively stable, which demonstrates that our framework is robust in the presence of a small amount of training data. 




\begin{figure}
\centering
    \subfloat[][\centering Left View]{{\includegraphics[width=0.35\textwidth]{Right.png} }}
    \hspace{1ex}~
    \subfloat[][\centering Right View]{{\includegraphics[width=0.35\textwidth]{Left.png} }}
    \hspace{1ex}~\subfloat[][\centering Up View]{{\includegraphics[width=0.24\textwidth]{Up.png} }}
    \caption{Distribution of anomalous edges in ADHD group.}
    \label{fig:adhd_dist}
\end{figure}
\subsection{Additional Results on Brain Networks} In this experiment, we investigate how anomalous edges found by \model{} are distributed in the brain.  
\autoref{fig:adhd_dist} reports the average distribution of anomalous edges in the brain networks of people living with ADHD. Most anomalous edges found by \model{} have a vertex in the Occipital lobes. Moreover, the Temporal lobes are the brain regions with the most anomalous connections with the Occipital lobes. These findings can help to reveal new insights into understanding ADHD and the regions of the brain that are connected to its symptoms. These findings also show the potential of \model{} to extract features that can help predict brain diseases or disorders. 

\section{Reproducibility}
\label{app:code}
The code, datasets, and supplements are available at \url{https://github.com/ubc-systopia/ANOMULY}.