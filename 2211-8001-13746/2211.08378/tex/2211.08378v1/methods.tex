\section{Proposed Method: \model}\label{sec:method}


\subsection{Preliminaries}
\label{sec:preliminaries}
We first precisely define multiplex dynamic networks, and then we formalize the problem of edge anomaly detection in multiplex dynamic networks.

\begin{dfn}[Multiplex Dynamic Networks]
    Let $\GG = \{ G_r \}^{\LL}_{r = 1} = (\V, \E, \X)$ denotes a multiplex dynamic network, where $G_r = (\V, \E_r, \X)$ is a graph of the relation type $r \in \LL$ (aka layer), $\V$ is the set of nodes, $\E = \bigcup_{r = 1}^{\LL} \E_r$ is the set of edges, and $\X \in \mathbb{R}^{|\V|\times f}$ is a matrix that encodes node attribute information for nodes in $\V$. Given a relation type $r$, we denote the set of vertices in the neighborhood of $u \in \V$ in relation $r$ as $\mathcal{N}_r(u)$. Each edge $e \in \E$ is associated with an edge type (layer) $r_e \in \LL$ and a timestamp $\tau_e$, and each node $v \in \V$ is associated with a timestamp $\tau_v$.
\end{dfn}

We take a snapshot-based anomaly detection approach for multiplex dynamic networks: a multiplex dynamic graph $\GG = \{\GG^{(t)}\}_{t = 1}^{T}$ can be represented as a sequence of multiplex network snapshots, where each snapshot is a static multiplex graph $\GG^{(t)} = \{ G^{(t)}_r \}_{r = 1}^{\LL} = (\V^{(t)}, \E^{(t)}, \X^{(t)})$ with $\V^{(t)} = \{v \in \V | \tau_v = t \}$ and $\E^{(t)} = \{ e \in \E | \tau_e = t\}$. Our goal is to detect anomalous edges in $\E^{(t)}$. Specifically, for each edge $e = (u, v, r) \in \E^{(t)}$, we produce a layer-dependent anomalous probability $\varphi_r(e)$ in layer $r \in \LL$. 

\begin{figure}
\centering
    \hspace*{-9ex}
    \subfloat[][\centering Architecture of the \textit{Snapshot Encoder}]{{\includegraphics[width=0.71\textwidth]{encoder.pdf} }}
    \hspace*{-8ex}
    \subfloat[][\centering \model{} Framework]{{\includegraphics[width=0.67\textwidth]{framework.pdf} }}
    \caption{Framework and design of \model{} model.}
    \label{fig:model_framework}
    \vspace{-2ex}
\end{figure}

\subsection{\model{} Framework}\label{sec:framework}
We now introduce our framework for edge anomaly detection in dynamic networks with multiple types of interactions. \autoref{fig:model_framework} provides an overview of the framework. To learn the pattern of normal edges, \model{} uses the \encoder{} architecture to encode each snapshot of the network. \encoder{} uses GNNs and incorporates structural and temporal properties of the graph as well as node features in each layer. Next, it uses an attention mechanism to take advantage of complementary information from different layers. Since the graph is dynamically changing, the node embeddings need to change. To this end, \encoder{} uses a GRU cell~\cite{GRU} to update the hierarchical node states over time. Finally, we use the hierarchical node states at each timestamp to calculate the layer-dependent anomalous probabilities of an existing edge and a negative sampled edge and use them as inputs to a margin loss computation. \model{}'s algorithm~appears~in~detail~in~\autoref{app:algorithm}.
%Next, we explain each part in more details:



\head{GNN Architecture}
A GNN iteratively aggregates messages from the local neighborhood of nodes to learn node embeddings. For a given type of relation $r$, we use the embedding matrix $\tilde{H}_r^{(\ell)} = \{ \tilde{\hh}_{r_{u}}^{(\ell)}\}_{u \in \V}$ to denote the embedding of all vertices in relation type $r$ after applying $\ell$-th GNN layer. Given a relation type $r$, the $\ell$-th layer of the GNN, $\tilde{H}_r^{(\ell)} = \gnn_r(\tilde{H}_r^{(\ell - 1)})$, is defined as:
\begin{equation}
    \begin{aligned}
    &{\mm^{\ell}_{r_{(v \rightarrow u)}}} = W^{(\ell)}_r \textsc{Concat}\left( \tilde{\hh}_{r_{v}}^{(\ell - 1)}, \tilde{\hh}_{r_{u}}^{(\ell - 1)}, \tau_{(v, u, r)} \right), \\
    &\tilde{\hh}_{r_{u}}^{(l)} = \textsc{Agg}^{(\ell)}\left( \left\{  {\mm^{\ell}_{r_{(v \rightarrow u)}}} | v \in \mathcal{N}_r(u)  \right\} \right) + \tilde{\hh}_{r_{u}}^{(\ell - 1)}.
    \end{aligned}
\end{equation}
In our experiments, we follow~\citet{roland}, and use summation as the aggregation function, i.e., $\textsc{Agg}(.) = \textsc{Sum}(.)$. We also use skip-connections~\cite{deepgcn} after aggregation.


\head{Update Modules}
Given the snapshot $\GG^{(t)} = (\V^{(t)}, \E^{(t)}, \X^{(t)})$ at time $t$ and a relation type $r$, we denote the embedding matrix in relation $r$ after the $\ell$-th GNN layer at time $t$ by $\tilde{H}_r^{(t)^{(\ell)}} = \{ \tilde{\hh}_{r_{u}}^{(t)^{(\ell)}}\}_{u \in \V}$. To take advantage of historical data and update the node embeddings at each timestamp, in the \textit{Embedding update} block (see~\autoref{fig:model_framework}), we use a GRU cell~\cite{GRU}. Given a relation type $r$, the output of the $\ell$-th \textit{Embedding update}, $\hat{H}_r^{(t)^{(\ell)}} = \{ \hat{\hh}_{r_{u}}^{(t)^{(\ell)}}\}_{u \in \V}$, is: 
\begin{equation}\label{eq:gru}
    \hat{H}_r^{(t)^{(\ell)}} = \textsc{Gru}_r\left( \tilde{H}_r^{(t)^{(\ell)}}, H_r^{(t - 1)^{(\ell)}} \right),
\end{equation}
where $\tilde{H}_r^{(t)^{(\ell)}}$ is the output of the $\ell$-th GNN layer, and $H_r^{(t - 1)^{(\ell)}}$ is the layer-aware embedding matrix at time $t - 1$.
% Next, we discuss how $H_r^{(t - 1)^{(\ell)}}$ is calculated.

\head{Attention Mechanism}
The role of our attention mechanism is to incorporate information from different relation types in a weighted manner. As discussed in \autoref{sec:introduction}, the importance of a layer can differ for different nodes, so we cannot calculate a single weight for each layer. Accordingly, we suggest an attention mechanism that learns the importance of layer $r$ for an arbitrary node $u\in \V$. Let $\zeta^{(t)^{(\ell)}}_u$ be the aggregated hidden feature of node $u\in \V$ after the $\ell$-th attention layer at time $t$, we call it a network-level embedding, and $\alpha_{r_u}^{(\ell)}$ indicates the importance of relation type $r$ for vertex $u$, then:  
\begin{equation}
    \zeta^{(t)^{(\ell)}}_u = \sum_{r = 1}^{L} \alpha_{r_u}^{(\ell)} \hat{\hh}_{r_u}^{(t)^{(\ell)}}, 
\end{equation}
where $\hat{\hh}_{r_u}^{(t)^{(\ell)}}$ is the output of $\ell$-th \textit{Embedding update} for node $u$ at time $t$. Following the recent attention-based models~\cite{unsupervisedML-2020, attention_main}, we use the softmax function to define the importance weight of relation type $r$ for node $u$:
\begin{equation}
    \alpha_{r_u}^{(\ell)} = \frac{\exp\left( \sigma\left( {\textbf{s}^{(t)^{(\ell)}}_r}^T \W_{r}^{\text{att}} \:\: \hat{\hh}_{r_u}^{(t)^{(\ell)}} \right) \right)}{\sum_{k = 1}^{L} \exp\left(  \sigma\left( {\textbf{s}^{(t)^{(\ell)}}_k}^T \W_{k}^{\text{att}}  \:\: \hat{\hh}_{k}^{(t)^{(\ell)}} \right)  \right)},
\end{equation}
where $\textbf{s}^{(t)^{(\ell)}}_r$ is a summary of the network in relation type $r$ at time $t$, i.e., $\textbf{s}^{(t)^{(\ell)}}_r = \sum_{u \in \V} \hat{\hh}_{r_u}^{(t)^{(\ell)}}$, and $\W^{\text{att}}_r$ is a trainable weight matrix. In our experiments, we use $\tanh(.)$ as the activation function $\sigma(.)$.



\head{Layer-aware Embedding}
The output of the attention mechanism is a network-level node embedding matrix, which summarizes the properties of nodes over all relation types. Given a relation type $r$, to obtain the layer-aware node embedding of a vertex $u \in \V$, we aggregate the output of the \textit{Embedding update} block, i.e. $\hat{\hh}_{r_u}^{(t)^{(\ell)}}$, and this network-level node embedding, i.e. $\zeta^{(t)^{(\ell)}}_u$. That is:
\begin{equation}\label{eq:layer-aware}
    \hh_{r_u}^{(t)^{(\ell)}} = \textsc{Agg}^{(\ell)} \left( \hat{\hh}_{r_u}^{(t)^{(\ell)}}, \zeta^{(t)^{(\ell)}}_u \right).
\end{equation}
Based on \autoref{eq:layer-aware}, we obtain the layer-aware node embedding matrix, $H_r^{(t)^{(\ell)}} = \{ \hh_{r_{u}}^{(t)^{(\ell)}}\}_{u \in \V}$, for any relation type $r$. Note that we use the layer-aware node embedding matrix at time $t - 1$, $H_r^{(t - 1)^{(\ell)}}$, in \autoref{eq:gru} to update node embeddings after the $\ell$-layer GNN layer.



\head{Anomalous Score Computation}
Now, we get the layer-aware node embedding matrix $H_r^{(t)} = H_r^{(t)^{(L)}}$ at time $t$, for each relation type $r$. Here $L$ is the number of GNN layers. Inspired by~\citet{AddGraph}, for an edge $(u, v) \in \E_r$, we define its anomalous score as follows:
\begin{equation}
    \varphi^{(t)}_r(u, v) = \sigma\left( \eta . \left( || \mathbf{a} \odot \hh_{r_{u}}^{(t)} + \mathbf{b} \odot \hh_{r_{v}}^{(t)} ||^2_2 - \mu  \right) \right),
\end{equation}
where $\sigma(.)$ is an activation function,  $\mathbf{a}$ and $\mathbf{b}$ are trainable vectors, and $\eta$ and $\mu$ are hyperparameters. 



\head{Training and Loss Function}
In the training phase, we employ a negative sampling approach in multiplex networks to corrupt edges and generate anomalous connections. Inspired by the negative sampling methods proposed by~\citet{negative_sampling} and \citet{AddGraph}, given a relation type $r$, and a normal edge $(u, v) \in \E_r$, we employ a Bernoulli distribution such that we replace $u$ (resp. $v$) with probability $\frac{deg_r(u)}{deg_r(u) + deg_r(v)}$ (resp. $\frac{deg_r(v)}{deg_r(u) + deg_r(v)}$) in relation type $r$ to generate random negative samples. Since the corrupted edges might be normal, a strict loss function (e.g., cross-entropy) can affect the performance. Accordingly, we employ the margin-based pairwise loss~\cite{loss} in each relation type $r$. Given a relation type $r$, we also employ a $L2$-regularization loss, $\mathscr{L}_r^{reg}$, which is the summation of the $L2$ norm of all trainable parameters, to avoid overfitting. Finally, to aggregate the loss function over all relation types in the multiplex networks,~we~use~the~average~of~loss~functions,~i.e.:


\begin{equation}\label{eq:loss}
    \mathscr{L} =  \min \frac{1}{|\LL|} \left( \sum_{r = 1}^{\LL}   \sum_{(u, v) \in \E_r} \sum_{(u', v') \not \in \E_r} \max \left\{ 0, \gamma + \varphi_r(u, v) - \varphi_r(u', v')  \right\} + \lambda \mathscr{L}_r^{reg} \right).
\end{equation}
Here, $0 \leq \gamma \leq 1$ is the margin between normal and corrupted edges.





