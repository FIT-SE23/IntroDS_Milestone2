
% ODQA gives QA model a single question without any context and asks the model to infer out-of-context knowledge. 

% Following the pioneering work by~\citet{DBLP:conf/acl/ChenFWB17}, most ODQA systems assume the model can access an external text corpus (e.g. Wikipedia).
% Due to the large scale of web corpus (20GB for Wikipedia), it could not be simply encoded in the QA model parameters, and thus most works propose a \textit{Retrieval-Reader} pipeline, by firstly index the whole corpus and use a \textit{retriever} model to identify which passage is relevant to the question; then the retrieved text passage concatenate with question is re-encoded by a seperate \textit{reader} model (e.g., \texttt{LM}) to predict answer. As the knowledge is outside of model parameter, \citet{DBLP:conf/emnlp/RobertsRS20} defines these methods as \textit{Open-book}, with an analogy to referring textbooks during exam.


% The \textit{Open-book} models following \textit{Retrieval-Reader} pipeline requires storing indexed corpus, and is hard to train end-to-end, and is inefficient during inference. Therefore, 
% In contrast, 

% \textit{Closed-book} QA models (mostly a single \texttt{LM}) try to answer open questions without accessing external knowledge. This setting is much harder as it requires \texttt{LM} to memorize all pertinent knowledge in its parameters.
% , and even recent \texttt{LM}s with much larger model parameters is still not competitive to state-of-the-art \textit{Open-book} models (e.g., T5-11B~\cite{DBLP:conf/emnlp/RobertsRS20} achieves 34.5 accuracy on Natural Questions, while FiD~\citep{DBLP:conf/eacl/IzacardG21} with 1B parameter achieves 51.4).
% Recent studies~\cite{DBLP:conf/emnlp/RobertsRS20, DBLP:conf/nips/BrownMRSKDNSSAA20} show that by leveraging pre-trained \texttt{LM}s in a supervised setting, they could correctly answer a certain portion of open questions (e.g., T5-11B could answer over 30\% of questions in Natural Questions dataset). 







\noindent \textbf{Open-Domain Question Answering (ODQA)} gives QA model a single question without any context and asks the model to infer out-of-context knowledge. Following the pioneering work by~\citet{DBLP:conf/acl/ChenFWB17}, most ODQA systems assume the model can access an external text corpus (e.g. Wikipedia).
Due to the large scale of web corpus (20GB for Wikipedia), it could not be simply encoded in the QA model parameters, and thus most works propose a \textit{Retrieval-Reader} pipeline, by firstly index the whole corpus and use a \textit{retriever} model to identify which passage is relevant to the question; then the retrieved text passage concatenate with question is re-encoded by a seperate \textit{reader} model (e.g., \texttt{LM}) to predict answer. As the knowledge is outside of model parameter, \citet{DBLP:conf/emnlp/RobertsRS20} defines these methods as \textit{Open-book}, with an analogy to referring textbooks during exam. \textit{Closed-book} QA models (mostly a single \texttt{LM}) try to answer open questions without accessing external knowledge. This setting is much harder as it requires \texttt{LM} to memorize all pertinent knowledge in its parameters, and even recent \texttt{LM}s with much larger model parameters is still not competitive to state-of-the-art \textit{Open-book} models. 



% \paragraph{Knowledge-Base Question Answering}
% Traditional parsing-based methods parse the question into some intermediate query (e.g., SQL language, query graphs), which can execute on a knowledge base to get answer \citep{DBLP:conf/emnlp/BerantCFL13,DBLP:conf/acl/YihCHG15,DBLP:journals/tacl/ReddyTCKDSL16,DBLP:journals/corr/abs-1709-00103,DBLP:conf/acl/LiangBLFL17}. However, existing knowledge bases suffer from low coverage of entities and relations required for open-ended questions. As an alternative, several works try to incorporate the structured knowledge into neural QA models for differentiable reasoning. \cite{DBLP:conf/emnlp/LinCCR19} and \cite{DBLP:conf/emnlp/FengCLWYR20} parse the question into a sub-graph of knowledge base, and apply graph neural networks as reasoner to extract answers. \cite{DBLP:conf/iclr/ChenLYZSL20} integrates general symbolic operations as basic units, and parse questions into compositional programs to answer general questions.




\noindent\textbf{Knowledge-augmented Language Models} 
explicitly incorporate external knowledge (e.g. knowledge graph) into \texttt{LM}~\citep{DBLP:journals/corr/abs-2010-04389}.
Overall, these approaches can be grouped into two categories:
The first one is to explicitly inject knowledge representation into language model pre-training, where the representations are pre-computed from external sources~\citep{DBLP:conf/acl/ZhangHLJSL19,DBLP:conf/aaai/LiuW0PY21,DBLP:conf/emnlp/HuSC21}.
For example, ERNIE~\cite{DBLP:conf/acl/ZhangHLJSL19} encodes the pre-trained TransE~\cite{DBLP:conf/nips/BordesUGWY13} embeddings as input.
The second one is to implicitly model knowledge information into language model by performing knowledge-related tasks, such as entity category prediction~\citep{DBLP:journals/corr/abs-2010-00796} and graph-text alignment~\cite{DBLP:conf/acl/KeJRCWSZH21}.
For example, JAKET~\citep{DBLP:journals/corr/abs-2010-00796} jointly pre-trained both the KG representation and language representation by adding entity category and relation type prediction self-supervised tasks.

There also exists several QA works using $\KG$ to help ODQA. For example, \citet{DBLP:conf/iclr/AsaiHHSX20} and \citet{DBLP:journals/corr/abs-1911-03868} expand the entity graph following wikipedia hyperlinks or triplets in knowledge base. \citet{DBLP:conf/acl/DingZCYT19} extract entities from current context via entity-linking and turn them into a cognitive graph, and a graph neural network is applied on top of it to extract answer. \citet{DBLP:conf/iclr/DhingraZBNSC20} and \citet{DBLP:journals/corr/abs-2010-14439} construct an entity-mention bipartite graph and then model the QA reasoning as graph traversal by filtering only the contexts that are relevant to the question. \citet{DBLP:conf/emnlp/LinCCR19}, \citet{DBLP:conf/emnlp/FengCLWYR20} and \citet{DBLP:conf/naacl/YasunagaRBLL21} parse the question into a sub-graph of knowledge base, and apply graph neural networks as reasoner for extracting one of the entities as the answer.

To encode knowledge (significantly smaller than the web corpus) as \emph{memory} into \texttt{LM} parameter, a line of works try compressed knowledge including QA pairs~\citep{DBLP:journals/corr/abs-2204-04581, DBLP:journals/corr/abs-2102-07033,DBLP:journals/corr/abs-2209-10063}, entity embedding~\citep{DBLP:journals/corr/abs-2004-07202} and reasoning cases~\citep{DBLP:conf/emnlp/DasZTGPLTPM21, DBLP:journals/corr/abs-2202-10610}.
There's also several works utilizing Knowledge Graph ($\KG$) to augment \texttt{LM}. FILM~\citep{DBLP:conf/naacl/VergaSSC21} turns $\KG$ triplets into memory. Given a question, \texttt{LM} retrieves most relevant triplet as answer. GreaseLM~\citep{DBLP:journals/corr/abs-2201-08860} propose to interact \texttt{LM} with $\KG$ via a interaction node. 


% We discuss other related works in Sec.~\ref{sec:related} in Appendix.















% Similar to query answering on graph, answering open-domain questions also require to infer out-of-context knowledge. 
% For example, given only a question $q$ as ``The Bauhaus represented Germany's recovery from which event$?$'', the QA model is asked to predict answer $a$ ''World War I``. To correctly answer such a question, the model needs to gain knowledge about all in-context entity mentions $M=\{m_i\}$, e.g., ''Bauhaus`` and ''Germany``. 
% Such an open-domain question could be abstracted as a query $(M, q, ?)$, or a probabilistic manner $P(a | q, M)$. 


