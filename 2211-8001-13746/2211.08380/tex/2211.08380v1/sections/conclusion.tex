% In this paper, we propose a simple yet effective pre-training framework \method. We leverage both the Wikipedia hyperlinks and Wikidata relation triplets to construct $\KG$, based on which we generate relational QA dataset. 
% We then pre-train a QA model to infer the latent relation from the question, and then conduct extractive QA to get the target answer entity. \method could improve the performance of the state-of-the-art QA frameworks, especially for questions with long-tail relations.

We presented \method, a novel model that incorporates symbolic $\mathcal{KG}$ reasoning with existing \texttt{LM}s. We showed that \method can
bring significant performance gain to open-domain QA benchmarks, both for closed-book and open-book settings, as well as encoder-only and encoder-decoder models. Additionally, \method produces reasoning paths that helps interpret the model prediction. In future, we'd like to improve \method by training to conduct more reasoning steps, supporting locial reasoning, and apply \method to a broader range of  knowledge-intensive NLP tasks.


\paragraph{Acknowledgement}
We sincerely thank anonymous reviewers for their constructive comments  to improve this paper. The project was partially supported in part by CISCO, NSF III-1705169, NSF 1937599, NASA, Okawa Foundation Grant, Amazon Research Awards, Cisco research grant,
and Picsart gift.
Ziniu is supported by the Amazon Fellowship and Baidu PhD Fellowship.