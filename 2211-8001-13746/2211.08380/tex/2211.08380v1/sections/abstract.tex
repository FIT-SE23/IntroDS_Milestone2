% Existing open-domain QA systems require to be trained on a large human-annotated QA dataset. 






Answering open-domain questions requires world knowledge about in-context entities. As pre-trained Language Models (\texttt{LM}s) lack the power to store all required knowledge, external knowledge sources, such as knowledge graphs, are often used to augment \texttt{LM}s. 
%However, most existing methods do not allow \texttt{LM} to interact with $\mathcal{KG}$.
In this work, we propose kn\underline{O}wledge \underline{RE}as\underline{O}ning empowered \underline{L}anguage \underline{M}odel
(\method), which consists of a novel Knowledge Interaction Layer that can be flexibly plugged into existing Transformer-based \texttt{LM}s to interact with a differentiable Knowledge Graph Reasoning module collaboratively. In this way, \texttt{LM} guides KG to walk towards the desired answer, while the retrieved knowledge improves \texttt{LM}.
By adopting \method to RoBERTa and T5, we show significant performance gain, achieving state-of-art results in the \textit{Closed-Book} setting. The performance enhancement is mainly from the KG reasoning's capacity to infer missing relational facts. In addition, 
\method  provides reasoning paths as rationales to interpret the model's decision.
% \looseness=-1