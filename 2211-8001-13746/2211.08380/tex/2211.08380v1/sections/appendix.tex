





\begin{table*}[t]
\centering
\begin{tabular}{l|ccc} \toprule
\textbf{Dataset}   & \textbf{Train}  & \textbf{Dev} & \textbf{Test} \\ \midrule
Natural Questions & 58880 & 8757, 3610\\
Trivia QA& 60413 & 8837 & 11313\\
Web Questions& 2474 & 361 & 2032\\
Complex WebQ & 27623 & 3518 & 3531\\
WebQ-SP (Wiki-answerable) & 1388 & 153 & 841 \\
FreebaseQA (Wiki-answerable) & 12535 & 2464 & 2440 \\
\bottomrule
\end{tabular}
\caption{Dataset Train/Valid/Test splits.}
\label{tab:data}
\end{table*}












\begin{table}[t]
\centering
\setlength{\tabcolsep}{1mm}{\scalebox{0.75}{
\begin{tabular}{lc|cc} \toprule
\textbf{Models}   & \textbf{\#param (B)} & WQ-SP & TQA \\ \midrule
RoBERTa (Base) & 0.12 &  47.5 & 40.3   \\ 
  \textbf{+ \method} ($T$=1) & 0.12 + \underline{0.68}  &  89.7 & 61.4   \\ 
  \textbf{+ \method} ($T$=2) & 0.13 + \underline{0.68} &  \textbf{92.4} & \textbf{66.8} \\ \bottomrule
\end{tabular}}}
\caption{\textbf{Closed-Book Entity Prediction} validation performance of Encoder RoBERTa on WikiData-Answerable Dataset. }
\end{table}


\begin{table*}[ht]
\centering
\small
% {\scalebox{}{
\begin{tabular}{lc|ccc|cc} \toprule
\textbf{Models}   & \textbf{\#param}  & \textbf{NQ} & \textbf{WQ} & \textbf{TQA}  & \textbf{ComplexWQ}  & \textbf{HotpotQA}  \\ \midrule
T5 (Large)           & 0.74B  & - & - & -  &   - & -\\ 
 \textbf{+ \method} ($T$=2) & 0.76B + \underline{0.68}B   &  33.6 & 38.9 & 42.7  & 29.6  & 35.5\\ \midrule
\end{tabular}
% }}
\caption{\textbf{Closed-Book Generative QA} validation performance of T5. }
\label{tab:close}
\end{table*}

\section{Implementation Details}\label{sec:implement}


\paragraph{Entity Linking durine pre-training}

We use the 2021 Jan. English dump of Wikidata and Wikipedia. 
For each wikipedia page, we link all entity mentions with hyperlinks to WikiData entity entry, augment all other mentions with same aliases, tokenize via each \texttt{LM}'s tokenizer and split into chunks with maximum token length allowed. We then construct induced k-hop subgraphs connecting entities within each chunk for quickly get grounded computational graph. 


For entities, Wikipedia provides hyperlinks with ground-truth entity ID, but it doesn't cover all the entity mentions, mostly hyperlinks only appear when this entity appears for the first time. Therefore, we first collect all entities appeared in hyperlinks as well as their aliases stored in WikiData, and then search any mentions that have any of these alias and link it to the corresponding entity.









\paragraph{Hyperparameters}
In this work, we don't have too much hyperparmaters to be tuned, as most parameters as well as optimizing setting of LM is fixed. Our random walk part is non-parametric. The only tunable hyperparamter is hidden dimension size. We simply choose one setting, which is 128 for entity embedding, and 768 for relation embedding. The former is because entity is super large (over 5M), so we use a reletively smaller dimension size. Detailed statistics about wikidata memory is in Table~\ref{tab:stat}.




\begin{table*}[h]
\scriptsize
\begin{tabular}[t]{c|c|c|c|c} \toprule
\textbf{Title}   & \textbf{Masked Text}  & \textbf{Ground Truth} & \textbf{Dependency Graph} & \textbf{2-Hop Graph}  \\ \midrule
Poolbeg & \multicolumn{1}{p{3cm}|}{the lighthouse was [mask] [s-ent] [mask] [rel] [t-ent]  completed in 1795. overview. the [s-ent]  poolbeg[rel] [t-ent]  “peninsula” is home to a number of landmarks including the [s-ent] [mask][rel] [t-ent] , the [s-ent]  pool[mask] lighthouse[rel] [t-ent] , the [s-ent]  irishtown nature park[rel] [t-ent] , the southern part of [s-ent] [mask][rel] [t-ent] ...}
&  \multicolumn{1}{p{2cm}|}{ [ ' connected to land by the',
 ' great south wall',
 ' great south wall',
 'beg',
 ' dublin port',
 "'s main power station,",
 ' structures in',
 '48',
 ' a process to list the',
 ' after the station',
 ', including 3,',
 ' dublin city council',
 ' quarter” on the']} 
&
\raisebox{-\totalheight}{

\includegraphics[width=0.4\textwidth]{pictures/mask_1.png}}
&
\raisebox{-\totalheight}{
\includegraphics[width=0.3\textwidth]{pictures/khop_1.png}}\\
\midrule
\multicolumn{1}{p{1cm}|}{Rylstone} & \multicolumn{1}{p{2cm}|}{it is situated very near to [s-ent] [mask][rel] [t-ent]  and about 6 miles south west[mask] [s-ent] [mask]ington[rel] [t-ent] . the population of the [s-ent]  civil parish[rel] [t-ent]  as of the 2011 census was 160.  [s-ent] rylstone railway station[rel] [t-ent]  opened in 1902, closed to passengers in 1930, and closed completely in 1969....}
&  \multicolumn{1}{p{2cm}|}{ [' craven',
 ' cracoe',
 ' of',
 ' grass',
 ' the inspiration for',
 ' tour de france',
 'stone',
 ' by will'...]} 
&
\raisebox{-\totalheight}{
\includegraphics[width=0.4\textwidth]{pictures/mask_6.png}}
&
\raisebox{-\totalheight}{
\includegraphics[width=0.3\textwidth]{pictures/khop_6.png}}\\
\midrule
\multicolumn{1}{p{1cm}|}{Karpinsk} & \multicolumn{1}{p{2cm}|}{ologist [s-ent] [mask] [rel] [t-ent] . history.[mask]the settlement of bogoslovsk () was founded in either 1759 or in 1769. it remained one of the largest [s-ent]  copper[rel] [t-ent]  production centers in the [s-ent]  urals[rel] [t-ent] [mask] [s-ent] [mask][rel] [t-ent]  deposits started to be mined in 1911.....}
&  \multicolumn{1}{p{2cm}|}{ [' alexander karpinsky',
 ' until 1917.',
 ' coal',
 'erman civilians, who',
 ' and',
 ' years of',
 ' forest laborers. moreover',
 ' in',
 ' the',
 ' framework of the',
 ' districts',
 ' karpinsk',
 'insk'...]} 
&
\raisebox{-\totalheight}{
\includegraphics[width=0.4\textwidth]{pictures/mask_8.png}}
&
\raisebox{-\totalheight}{
\includegraphics[width=0.3\textwidth]{pictures/khop_8.png}}\\

\midrule
\multicolumn{1}{p{1cm}|}{3 (The X-Files)} & \multicolumn{1}{p{2cm}|}{[s-ent] [mask][mask][rel] [t-ent] ". [s-ent]  gillian anderson[rel] [t-ent]  is absent[mask][mask] episode as she was on leave to give birth to her daughter piper at the time. this episode was the first[mask] not appear. reception. ratings. "3" premiered on the [s-ent]  fox network[rel] [t-ent]  on, and was first broadcast in the [s-ent]  united kingdom[rel] [t-ent].....}
&  \multicolumn{1}{p{2cm}|}{ [ 'ny had',
 ' episode',
 'born again',
 ' from the',
 ' in which scully did',
 '. it was',
 'egall',
 ' metacritic',
 ' as "wretched',
 ' fact that',
 ' background noise for a',
 ' heavy-handed attempts at',
 ' glen morgan',
 ' doing an episode on']} 
&
\raisebox{-\totalheight}{
\includegraphics[width=0.4\textwidth]{pictures/mask_9.png}}
&
\raisebox{-\totalheight}{
\includegraphics[width=0.3\textwidth]{pictures/khop_9.png}}\\
\bottomrule
\end{tabular}
\caption{Example of Pre-training data points (Part 1).}
\label{tab:pretrain1}
\end{table*}







\begin{table*}[h]
\scriptsize
\begin{tabular}[t]{c|c|c|c|c} \toprule
\textbf{Title}   & \textbf{Masked Text}  & \textbf{Ground Truth} & \textbf{Dependency Graph} & \textbf{K-Hop Graph}  \\ \midrule

\multicolumn{1}{p{1cm}|}{Shen Chun-shan} & \multicolumn{1}{p{2cm}|}{his memoirs, he suffered his second stroke[mask][mask], even after his second stroke, he continued writing; his series of biographies of five go masters [s-ent] [mask][mask][mask][rel] [t-ent] , [s-ent]  minoru kit[mask][rel] [t-ent] .....}
&  \multicolumn{1}{p{2cm}|}{ ['. however',
 ' go seigen',
 'ani',
 ' 2007, he',
 ' was hospital',
 ' hsinchu',
 'after surgery',
 ' scale',
 ' continuing to improve.',
 ' his coma. in'...]} 
&
\raisebox{-\totalheight}{
\includegraphics[width=0.4\textwidth]{pictures/mask_10.png}}
&
\raisebox{-\totalheight}{
\includegraphics[width=0.3\textwidth]{pictures/khop_10.png}}\\



\midrule
\multicolumn{1}{p{1cm}|}{2007 Florida Gators football team} & \multicolumn{1}{p{2cm}|}{[s-ent]  tim[mask][mask][rel] [t-ent]  completed 22 of 27 passes for 281 yards passing and also ran for[mask] yards on 6 carries. [s-ent] [mask] [rel] [t-ent]  carried the ball 11 times for 113 yards[mask] two touchdowns and also caught 9 passes for 110[mask] receiving, becoming the first player in school history .....}
&  \multicolumn{1}{p{2cm}|}{ [' tebow',
 ' 35',
 ' percy harvin',
 ' and',
 ' yards',
 ' 30–9',
 ' renewed their budding',
 ' gamecocks',
 'gator',
 ' quarterback',
 ' set a career-high',
 ' of these five rushing',
 '.',
 ' percy harvin',
 ' sinus infection.',
 'ators',
 ' touchdown']} 
&
\raisebox{-\totalheight}{
\includegraphics[width=0.4\textwidth]{pictures/mask_16.png}}
&
\raisebox{-\totalheight}{
\includegraphics[width=0.3\textwidth]{pictures/khop_16.png}}\\






\midrule
\multicolumn{1}{p{1cm}|}{Judgment Day (Awesome Comics)} & \multicolumn{1}{p{2cm}|}{[s-ent]  alan moore[rel] [t-ent]  used "judgment day" to reject the violent, deconstructive clichés of 1990s comics inadvertently caused by his own work on " [s-ent] watchmen[rel] [t-ent] ", "" and " [s-ent] saga of the[mask][mask][rel] [t-ent] " and uphold the values of classic superhero comics. the series deals with a metacommentary of the notion of retcons to super-hero histories as [s-ent]  alan moore[rel] [t-ent] [mask]  for the characters of [s-ent] [mask][mask][rel] [t-ent] , to replace the shared universe they left when [s-ent]  rob liefeld[rel] [t-ent]  left image several years earlier. plot. in[mask], mick tombs/ [s-ent] knightsabre[rel] [t-ent].....}
&  \multicolumn{1}{p{2cm}|}{ [ ' swamp thing',
 ' himself creates a new backstory',
 ' awesome comics',
 ' 1997',
 'riptide',
 ' knightsabre appears to be',
 ' and sw',
 ' badrock',
 ' supreme',
 'by',
 ' analyzing',
 ' cybernetic young',
 ' it, and it has',
 'ue out',
 ', administrator for youngblood']} 
&
\raisebox{-\totalheight}{
\includegraphics[width=0.4\textwidth]{pictures/mask_17.png}}
&
\raisebox{-\totalheight}{
\includegraphics[width=0.3\textwidth]{pictures/khop_17.png}}\\


\bottomrule
\end{tabular}
\caption{Example of Pre-training data points (Part 2).}
\label{tab:pretrain}
\end{table*}




































\begin{table*}[ht]
\centering
\hspace{-.5in}
\begin{tabular}{c|c|l} \toprule
\textbf{Question}   & \textbf{Answer}  & \textbf{Reasoning Paths as Rationale}  \\ \midrule
\multicolumn{1}{p{4cm}|}{ southern soul was considered the sound of what independent record label } & \multicolumn{1}{p{2cm}|}{['Motown']} & \multicolumn{1}{p{12cm}|}{$ \text{ soul music } \xrightarrow{ \text{genre-R} } \ ?\  \xrightarrow{ \text{label} } \ ?\  $}\\ 
 & & $ \text{ independent record label } \xrightarrow{ \text{belong} } \ ?\  \xrightarrow{ \text{is a-R} } \ ?\  $\\ \midrule
\multicolumn{1}{p{4cm}|}{ who is the bad guy in lord of the rings } & \multicolumn{1}{p{2cm}|}{['Sauron']} & \multicolumn{1}{p{12cm}|}{$ \text{ the lord of the rings (film series) } \xrightarrow{ \text{theme} } \ ?\  \xrightarrow{ \text{characters} } \ ?\  $}\\ \midrule
\multicolumn{1}{p{4cm}|}{ where was the mona lisa kept during ww2 } & \multicolumn{1}{p{2cm}|}{['the Ingres Museum', "Château d'Amboise", 'Château de Chambord', 'the Loc - Dieu Abbey']} & \multicolumn{1}{p{12cm}|}{$ \text{ mona lisa } \xrightarrow{ \text{creator} } \ ?\  \xrightarrow{ \text{tomb} } \ ?\  $}\\ 
& & $ \text{  world war 2 } \xrightarrow{ \text{take place} } \ ?\  \xrightarrow{ \text{located-R} } \ ?\  $ \\ \midrule
\multicolumn{1}{p{4cm}|}{ who have won the world cup the most times } & \multicolumn{1}{p{2cm}|}{['Brazil']} & \multicolumn{1}{p{12cm}|}{$ \text{ fifa world cup } \xrightarrow{ \text{parts} } \ ?\  \xrightarrow{ \text{land} } \ ?\  $}\\ \midrule
\multicolumn{1}{p{4cm}|}{ who wrote the song the beat goes on } & \multicolumn{1}{p{2cm}|}{['Sonny Bono']} & \multicolumn{1}{p{12cm}|}{$ \text{ song } \xrightarrow{ \text{album type-R} } \ ?\  \xrightarrow{ \text{author} } \ ?\  $}\\ \midrule
\multicolumn{1}{p{4cm}|}{ who plays mrs. potato head in toy story } & \multicolumn{1}{p{2cm}|}{['Estelle Harris']} & \multicolumn{1}{p{12cm}|}{$ \text{ toy story } \xrightarrow{ \text{series} } \ ?\  \xrightarrow{ \text{VO} } \ ?\  $}\\ \midrule
\multicolumn{1}{p{4cm}|}{ who plays caroline on the bold and beautiful } & \multicolumn{1}{p{2cm}|}{['Linsey Godfrey']} & \multicolumn{1}{p{12cm}|}{$ \text{ the bold and the beautiful } \xrightarrow{ \text{in work-R} } \ ?\  \xrightarrow{ \text{actor} } \ ?\  $}\\ \midrule
\multicolumn{1}{p{4cm}|}{ where are the fruits of the spirit found in the bible } & \multicolumn{1}{p{2cm}|}{['Epistle to the Galatians']} & \multicolumn{1}{p{12cm}|}{$ \text{ bible } \xrightarrow{ \text{parts} } \ ?\  \xrightarrow{ \text{parts} } \ ?\  $}\\ \midrule
\multicolumn{1}{p{4cm}|}{ who is the only kaurava who survived the kurukshetra war } & \multicolumn{1}{p{2cm}|}{['Yuyutsu']} & \multicolumn{1}{p{12cm}|}{$ \text{ kaurava } \xrightarrow{ \text{in work} } \ ?\  \xrightarrow{ \text{in work-R} } \ ?\  $}\\
& & $\text{Kurukshetra War} \xrightarrow{ \text{location} } \xrightarrow{ \text{live in-R} } $ \\
\midrule
\multicolumn{1}{p{4cm}|}{ what is the deepest depth in the oceans } & \multicolumn{1}{p{2cm}|}{['Mariana Trench']} & \multicolumn{1}{p{12cm}|}{$ \text{ ocean } \xrightarrow{ \text{in} } \ ?\  \xrightarrow{ \text{lowest point} } \ ?\  $}\\ \midrule
\multicolumn{1}{p{4cm}|}{ where did the french national anthem come from } & \multicolumn{1}{p{2cm}|}{['Strasbourg']} & \multicolumn{1}{p{12cm}|}{$ \text{ national anthem } \xrightarrow{ \text{is a-R} } \ ?\  \xrightarrow{ \text{released in} } \ ?\  $}\\   \bottomrule
\end{tabular}
\caption{Example of QA prediction with reasoning path on NQ (part 1).}
\label{tab:explain1}
\end{table*}




\begin{table*}[ht]
\centering
\hspace{-.5in}
\begin{tabular}{c|c|l} \toprule
\textbf{Question}   & \textbf{Answer}  & \textbf{Generated Reasoning Paths as Rationale}  \\  \midrule
\multicolumn{1}{p{4cm}|}{ who sings the song where have all the flowers gone } & \multicolumn{1}{p{2cm}|}{['Pete Seeger']} & \multicolumn{1}{p{12cm}|}{$ \text{ song } \xrightarrow{ \text{album type-R} } \ ?\  \xrightarrow{ \text{actor} } \ ?\  $}\\ \midrule
\multicolumn{1}{p{4cm}|}{ who discovered some islands in the bahamas in 1492 } & \multicolumn{1}{p{2cm}|}{['Christopher Columbus']} & \multicolumn{1}{p{12cm}|}{$ \text{ the bahamas } \xrightarrow{ \text{entry} } \ ?\  \xrightarrow{ \text{entry-R} } \ ?\  $}\\ \midrule
\multicolumn{1}{p{4cm}|}{ which type of wave requires a medium for transmission } & \multicolumn{1}{p{2cm}|}{['mechanical waves', 'heat energy', 'Sound']} & \multicolumn{1}{p{12cm}|}{$ \text{ wave } \xrightarrow{ \text{belong-R} } \ ?\  \xrightarrow{ \text{belong-R} } \ ?\  $}\\ \midrule
\multicolumn{1}{p{4cm}|}{ land conversion through burning of biomass releases which gas } & \multicolumn{1}{p{2cm}|}{['traces of methane', 'carbon monoxide', 'hydrogen']} & \multicolumn{1}{p{12cm}|}{$ \text{ gas } \xrightarrow{ \text{belong-R} } \ ?\  \xrightarrow{ \text{as-R} } \ ?\  $}\\ \midrule
\multicolumn{1}{p{4cm}|}{ the sum of the kinetic and potential energies of all particles in the system is called the } & \multicolumn{1}{p{2cm}|}{['internal energy']} & \multicolumn{1}{p{12cm}|}{$ \text{ kinetic energy } \xrightarrow{ \text{belong} } \ ?\  \xrightarrow{ \text{belong-R} } \ ?\  $}\\ 
 & & $ \text{ potential energy } \xrightarrow{ \text{belong} } \ ?\  \xrightarrow{ \text{belong-R} } \ ?\  $\\ \midrule
\multicolumn{1}{p{4cm}|}{ who did seattle beat in the super bowl } & \multicolumn{1}{p{2cm}|}{['Denver Broncos']} & \multicolumn{1}{p{12cm}|}{$ \text{ super bowl } \xrightarrow{ \text{organizer} } \ ?\  \xrightarrow{ \text{league-R} } \ ?\  $}\\ \midrule
\multicolumn{1}{p{4cm}|}{ what is the name of the girl romeo loved before juliet } & \multicolumn{1}{p{2cm}|}{['Rosaline']} & \multicolumn{1}{p{12cm}|}{$ \text{ romeo } \xrightarrow{ \text{in work} } \ ?\  \xrightarrow{ \text{in work-R} } \ ?\  $}\\ \midrule
\multicolumn{1}{p{4cm}|}{ who will get relegated from the premier league 2016/17 } & \multicolumn{1}{p{2cm}|}{[ 'Hull City', 'Sunderland', 'Middlesbrough']} & \multicolumn{1}{p{12cm}|}{$ \text{ premier league } \xrightarrow{ \text{league-R} } \ ?\  \xrightarrow{ \text{POB} } \ ?\  $}\\ \midrule
\multicolumn{1}{p{4cm}|}{ actress in the girl with the dragon tattoo swedish } & \multicolumn{1}{p{2cm}|}{['Noomi Rapace']} & \multicolumn{1}{p{12cm}|}{$ \text{ sweden } \xrightarrow{ \text{speaking} } \ ?\  \xrightarrow{ \text{mother tongue-R} } \ ?\  $}\\
\bottomrule
\end{tabular}
\caption{Example of QA prediction with reasoning path on NQ (part 2).}
\label{tab:explain2}
\end{table*}







\section{Dataset Details}\label{sec:exp_detail}

Below shows details for each dataset, and the detailed dataset split is shown in Figure~\ref{tab:data}

\paragraph{Natural Questions}\cite{DBLP:journals/tacl/KwiatkowskiPRCP19} contains questions from Google search queries, and the answers are text spans in Wikipedia. We report short answer Exact Match (EM) performance. The open version of this dataset is obtained by discarding answers with more than 5 tokens.

\paragraph{WebQuestions (WQ)} \cite{DBLP:conf/emnlp/BerantCFL13} contains questions from Google Suggest API, and the answers are entities in Freebase.

\paragraph{TriviaQA} \cite{DBLP:conf/acl/JoshiCWZ17} contains trivia questions and answers are text spans from the Web. We report Exact Match (EM) performance. We use its unfiltered version for evaluation.


\paragraph{HotpotQA} \cite{DBLP:conf/emnlp/Yang0ZBCSM18} is a multi-hop QA dataset. There are two evaluation settings. In the \textit{distractor setting}, 10 candidate paragraphs are provided for each question, of which there are two golden paragraphs. In the \textit{full-wiki setting}, a model is required to extract paragraphs from the entire Wikipedia. We report Exact Match (EM) on full-wiki setting.

\paragraph{Complex WebQuestions}\cite{DBLP:conf/naacl/TalmorB18} is a dataset that composite simple one-hot questions in WebQuestionsSP by extending entities or adding constraints, so that each question eequires complex reasoning to solve.


\paragraph{WebQuestionsSP}\cite{DBLP:conf/acl/YihCHG15} is annotated dataset from WebQuestions, such taht each quetsion is answerable using Freebase via a SQL query. 





\section{Discussion with Previous Works}

\paragraph{Compare with FILM}

Though FILM has the advantage of end-to-end training and easily modification of knowledge memory, it simply stacks $\mathcal{KG}$ module on top of \texttt{LM} without interaction, and can only handle one-hop relational query that is answerable by $\mathcal{KG}$. 
Our approach, \method, follows the same \emph{memory} idea by encoding $\mathcal{KG}$ into \texttt{LM} parameter, and we desire \texttt{LM} and $\mathcal{KG}$ reasoning module could interact and collaboratively improve each other.


Notably, \method with $T=1$ shares a similar design with FILM. The major differences are: 1) they store every triplet as a key-value pair,
while we explicitly keep the $\mathcal{KG}$ adjacency matrix and conduct a random walk, which has smaller search space and is more controllable. 2) They add the memory on top of \texttt{LM}, and thus the knowledge could not help language understanding, and FILM could mainly help wikipedia-answerable questions. Instead, we insert the \texttt{KIL} layer amid \texttt{LM} layers to encourage interaction, and thus the model could also benefit encoder-decoder model (as shown above). 


\paragraph{Compare with Previous Path-Based Reasoning and Retrieval Pre-Training}



Note that as our definition of entity state $\pib_i$ and relation action $\rb_i$ are both continuous probabilistic vector, the whole  $\mathcal{KG}$ Reasoning is fully differentiable and thus could be integrated into \texttt{LM} seamlessly and trained end-to-end. This is different from previous path traversal works such as DeepPath~\cite{DBLP:conf/emnlp/XiongHW17} and MINERVA~\cite{DBLP:conf/iclr/DasDZVDKSM18}, which defines state and action as discrete and could only be trained via reinforcement learning rewards. The reasoner training is also different from passage retrieval pre-training~\cite{DBLP:journals/corr/abs-2002-08909, DBLP:conf/ACL/SachanPSKPHC20}, as the passage are naturally consisted of discrete tokens, and thus the reader is still required to re-encode the question with each passage, and different objectives are required to train retriever and reader separately.  


\paragraph{Discussion of Graph Walking-based Reasoning vs Graph Neural Networks}~\label{sec:gnn}
Recently, Graph Neural Networks (GNNs) have shown superior performance for structured representation learning. There's also a lot of works trying to use GNNs for Question Answering~\citep{DBLP:conf/naacl/YasunagaRBLL21, DBLP:journals/corr/abs-2201-08860}. The one that has very similar motivation with us is GreaseLM. Therefore, a natural question is, whether could we use GNN instead of the non-parametric random walk module, for ODQA?


To answer this question, let's consider a simplest setup of GNN. We could identify initial entities, connected them via a k-hop subgraph, and encode graph with text~\citep{DBLP:journals/corr/abs-2201-08860} or independently~\citep{DBLP:journals/corr/abs-2010-00796}. When we want to retrieve knowledge from graph to LM, normally we just take the contextualized node embedding as input for knowledge fusion.

In this setup, say the answer is $K$-hop away from an initial entity, the ground-truth reasoning path is $e_0, r_1, e_1, r_2, ..., e_{k-1}, r_k, e_{k}=a$. Using our method, we first predict $r1$, transit to $e_1$, and step by step conduct reasoning via walking. However, if we use GNN's final embedding, it requires to pass information from neighbor to itself. Therefore, suppose we have a $K$-layer GNN, the first step should be identify $r_k$, and pass information from answer $e_{k}=a$ to $e_{k-1}$. This is conter-intuitive as we normally cannot assume to know the answer, nor knowling the last step to reach the answer. In situations where all candidate answer is given, like CommonSenseQA, where GreaseLM mainly works on, this problem is less harmful as it's guaranteed to contain the answer in a restricted small graph. However, in open-domain setup, we need to try best to narrow down the search space by following the forward reasoning instead of the backward manner. Therefore, in this work we adopt walking-based reasoning.




\section{Illustration of Pre-Trained Data and Reasoning Paths}

The pre-training samples and reasoning paths (generated by T5-large on NQ dataset) is shown from Table~\ref{tab:pretrain1}-\ref{tab:explain2}.








