%\YS{I feel there are lots of smart designs for the pre-training tasks. But the writing does not reflect that yet.}\YS{try to write: issues; solutions; losses}

The design of \method allows it to be trained end-to-end given question answering datasets. However, due to small coverage of knowledge facts for existing QA datasets, we need to pretrain \method on a large-scale corpus to get good entity embeddings.
% it's unlikely to directly train \method using downstream QA tasks without proper initialization. 

% We therefore introduce how we pre-train \method using unlabelled Wikipedia Corpus to learn to reason over $\mathcal{KG}$.

\paragraph{Salient Span Masking} \YS{shall we upgrade this into a subsection? right now we only have one subsection.}
One straightforward approach is to use 
Salient Span Masking (SSM) objective~\citep{DBLP:journals/corr/abs-2002-08909}, which mask out entities or noun tokens that require a certain out-of-context knowledge. We mainly mask out salient spans that are: 1) entities in each Wikipedia page; 2) consecutive token spans.
During entity masking, instead of randomly mask entity mentions, we explicitly sample a set of entity IDs and mask out every mentions linking to these entities. This could avoid model simply copy the entity from the context to fill in the blank. For other token spans, we the same procedure as~\citep{DBLP:conf/nips/YangDYCSL19} to firstly sample a length $L\in[1,5]$, and then randomly select a starting position to construct a $L$-length consecutive token span. We then calculate cross-entropy loss on each salient span masked (SSM) token as $\mathcal{L}_{SSM}$. 


%\YS{why these two objectives? How these two objective connect to the two losses mentioned below.}



\subsection{Weakly Supervised Training of $\texttt{KIL}$}
Ideally, \method can learn all the entity knowledge and how to access knowledge graph by solely optimizing $\mathcal{L}_{SSM}$. However, without a good initialization of entity and relation embeddings, \texttt{KIL} makes random prediction and the retrieved entities by $\mathcal{KG}$ reasoning is likely to be irrelevant to the question. In this situation, \texttt{KIL} does not receive meaningful gradients to update the parameters and \texttt{LM} learns to ignore the knowledge.  
%\YS{what are the above two paragraphs? Can we put them into subsections related to specific losses? In other words, quickly tell the readers what they are reading.}
% no matter what entities retrieved within the reasoning procedure, the final aggregated embeddings could not benefit the language model objective, and thus the model won't get the correct signal to learn which relation should choose, and how to improve entity embeddings.
% \YX{This is not quite true since you also train the entity and relation embeddings. I'd suggest to weaken the tone. Like: However, if the entity and relation embeddings are not sufficiently trained, the model might not benefit from the \texttt{KIL} layers.} 
To avoid this cold-start problem and provide entity and relation embedding a good initialization, We utilize the following two external signals as self-supervised guidance.

\paragraph{Entity Linking Loss}
To initialize the large entity embedding tables in $\texttt{V}_{ent}$, we use other entities that are not masked as supervision. Similar to~\citet{DBLP:journals/corr/abs-2004-07202}, we force the output embedding of [\texttt{S-ENT}] token before the first \texttt{KIL} followed by a projection head E-Proj to be close to its corresponding entity embedding:
\begin{align*}
    &E_{\texttt{S-ENT}[i]} = \text{LN}\big(\text{E-Proj}(\texttt{LM}^{(1)}_{\texttt{S-ENT}[i]})\big) \\
    &\Pb^{(0)}_{ent}\big(e | m_i, q \big) = \text{Softmax} \big( E_{\texttt{S-ENT}[i]}\ \texttt{V}_{ent}[\Ib]^T   \big)\\
    &\mathcal{L}_{ent} = \sum\nolimits_{m_i} -\log \Pb^{(0)}_{ent}\big(e | m_i, q \big) \cdot \pib^0_i[\Ib]
\end{align*}
% \YX{Is this softmax here over all entities? It is in-batch entities if I remember correctly. Either case, please specify it.}
Similar to Section~\ref{sec:lm}, we only consider entities within the batch, denoted by index $\Ib$.
This contrastive loss guides each entity's embedding $\texttt{V}_{ent}[e]$ closer to all its previously mentioned contextualized embedding, and thus  memorizes those context as a good initialization for later knowledge integration. 

\paragraph{Weakly Supervised Relation Path Loss}
As the entity mentions within each Wikipedia passage are naturally grounded to WikiData $\mathcal{KG}$, after we mask out several entities, we can utilize the $\mathcal{KG}$ to get all reasoning paths from other in-context entities to the masked entities. Therefore, for each in-context entity mention, we aggregate all relation actions over every outgoing path to get a weakly supervised relation label and train the relation predictor module. 

Formally, we define a \textbf{Grounded Dependency Graph} $\mathcal{DG}$, which contains all reasoning paths within $T$-step from other in-context entities to masked entities, and then define $R_{\mathcal{DG}}(m_i, t)$ as the set of all relations over every edges for entity mention $m_i$ at $t$-th hop. Based on it, we define the weakly supervised relation label $q^{(t)}_i \in \mathbb{R}^{|\mathcal{R}|}$ as the probabilistic vector which uniformly distributed on each relation in set, i.e., $q^{(t)}_i[r] = \frac{1}{|R_{\mathcal{DG}}(m_i, t)|}, \text{if }\ r \in R_{\mathcal{DG}}(m_i, t)$.
% $q^{(t)}_i[r] = \begin{cases}
%     \frac{1}{|R_{\mathcal{DG}}(m_i, t)|},& \text{if }\ r \in R_{\mathcal{DG}}(m_i, t)\\
%     0,              & \text{otherwise}
% \end{cases}$
Note that we call uniformly-weighted $q^{(t)}_i$ as weakly supervised because 1) some paths could lead to multiple entities rather than only the target masked entity; 2) the correct relation is dependent on the context. Therefore, $q^{(t)}_i$ only provides all potential candidates for reachability, and more fine-grained signals for reasoning should be learned from unsupervised $\mathcal{L}_{SSM}$. We adopt a list-wise ranking loss to guide the model to assign a higher score on these relations than others.
\begin{align*}
    \mathcal{L}_{rel} = \sum\nolimits_{m_i}\sum\nolimits_{t=1}^{T} -\log \Pb^{(t)}_{rel}\big(r | m_i, q \big) \cdot q^{(t)}_i
\end{align*}
As an example in Figure~\ref{fig:pretrain_example}, we take one paragraph from ''Bauhaus'' and mask out entities ''Germany`` and ''World War I`` as targets. We then construct the grounded dependency graph from other entities to these two masked entities and get the weakly supervised relation label for each in-context source entity per reasoning step. More real examples are shown in Table~\ref{tab:pretrain1} in Appendix.


Overall, $\mathcal{L}_{ent}$ and $\mathcal{L}_{rel}$  provide \method with good initialization of the large $\mathcal{KG}$ memory. Afterwards, Through optimizing $\mathcal{L}_{SSM}$, those reasoning paths that provide informative knowledge  receive positive gradient, and gradually learn to reason during pre-training.


% Specifically, for each Wikipedia passage containing $N$ entity mentions, we first construct a \textbf{K-hop Induced Subgraph}, in which all the paths with length shorter than $K$ connecting any pair of entities are retained. Then, during training, after we randomly mask out a set of entities, we could easily calculate a 


% spanning tree starting from the masked entities to all other in-context entities. Then, for each entity mention $m_i$, 


% To pre-train the model to reason, we desire a self-supervised task that the model could not simply answer by looking at local contexts, but require external knowledge. To serve the purpose, besides the vanilla masked language model, we specifically sample a portion of entities within each page, and mask out all their mentions. In this way, the model cannot simply copy the tokens from contexts to fill in the blank. In addition, as we have the grounded entity graph, we could pre-extract all paths from in-context entity mentions to target masked entities. These paths could serve as golden ground-truth for relation prediction during pre-training, guiding our model to make reasonable relation prediction at an early stage. As illustrated in Figure~\ref{fig:pretrain_example}, we mask out two entities Mudhoney and Nirvana from the paragraph, which is hard for a vanilla \texttt{LM} to generate based on context. We also provide the one-hop and two-hop paths from in-context entity mentions.



\begin{figure}[t!]
    \centering
    \includegraphics[width=1\columnwidth]{pictures/pre.png}
    \caption{Example of pre-training sample and golden reasoning path.}
    \label{fig:pretrain_example}
 \vspace{-.1in}
\end{figure}
