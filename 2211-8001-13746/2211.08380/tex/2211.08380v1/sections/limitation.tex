












\paragraph{Limited Reasoning Steps}
In our experiments, we show that using reasoning step $T=2$ has better performance to $T=1$ on one-hop and multi-hop (mostly two) QA datasets. Thus, it's a natural question about whether we could extending reasoning steps more? As previous KG reasoning mostly could support very long path (with LSTM design)


Though we didn't spend much time exploring before the paper submission, we indeed try using $T=3$, but currently it didn't get better results. We hypothesize the following reasons: 1) A large portion of our current model's improvement relies on the weakly supervised relation pre-training. To do it, we construct a K-hop (K=2 now) subgraph, and sample dependency graph based on it. The larger $K$ we choose, the more noise is included into the generated relation label, in an exponential increasing speed. Thus, it's harder to get accurate reasoning path ground-truth for high-order $T$. Another potential reason is that within Transformer model, the representation space in lower and upper layer might be very different, say, encode more syntax and surface knowledge at lower layers, while more semantic knowledge at upper layers. Currently we adopt a MLP projection head, wishing to map integrated knowledge into the same space, but it might have many flaws and need further improvement.






\paragraph{Large Entity Embedding Table requires Pre-Training and GPU resources}

Our current design has a huge entity embedding table, which should be learned through additional supervision and could not directly fine-tune to downstream tasks. This is restricts our approach's usage.





\paragraph{Require Entity Linking}

Current model design requires an additional step of entity linking for incoming questions, and then add special tokens as interface. A truly end-to-end model should identify which elements to start conducting reasoning by its own without relying on external models.




\paragraph{Only support relational path-based reasoning}

Though there are lots of potential reasoning tasks, such as logical reasoning, commonsense reasoning, physical reasoning, temporal reasoning, etc. Our current model design mainly focus on path-based relational reasoning, and it should not work for other reasoning tasks at current stage.






\paragraph{Unreasonable Assumption of Path In-dependency}

When we derive equation~\ref{eq:overall}, we have the assumption that reasoning paths starting from different entities should be independent. This is not always correct, especially for questions that require logical reasoning, say, have conjunction or disjunction operation over each entity state. And thus our current methods might not work for those complex QA with logical dependencies.

