\begin{figure*}[t!]
%\vspace{-.6in}
\hspace{-0.2in}
    \centering
    \includegraphics[width=1\columnwidth]{pictures/framework_left.png}
    \caption{\textbf{Model architecture of \method}. Three key procedures are highlighted in red dotted box: 1) \textbf{Relation Prediction} (Sec.~\ref{sec:relation}): Knowledge Interaction Layers (\texttt{KIL}) predicts relation action for each entity mention. 2) \textbf{One-step State Transition} (Sec.~\ref{sec:crw}): Based on the predicted relation, $\KG$ re-weights each graph and conduct contextualized random walk to update entity distribution state. 3) \textbf{Knowledge Integration} (Sec.~\ref{sec:lm}): An weighted aggregated entity embedding is added into a placeholder token as retrieved knowledge. 
    % By repeating such procedure $T$ times, \method could retrieve knowledge that are $T$-hop away from initial entities in the question, and answer complex open-domain questions.
    }
    \label{fig:framework}
%  \vspace{-.1in}
\end{figure*}


% In this section, we introduce the model architecture of Knowledge Graph Reasoning integrated Language Model (\method) and how we pre-train \method to reason on Wikipedia corpus.





% {\footnotesize
% \begin{align}
%   & \Pb\big(a | q, \{m_i\}\big) = \prod_{m_i} \Pb\big(a | q, m_i\big)\\
%   & = \prod_{m_i} \Pb\big(a | q, m_i, \pib_i^{1:T}\big) \cdot \Pb\big(\pib_i^{1:T} | q, m_i\big)\\
%   & = \prod_{m_i} \underbrace{\Pb\big(a | q, m_i, \pib_i^{1:T}\big)}_\text{1) \texttt{LM} reader} \cdot \sum_{t} \underbrace{\Pb\big(\pib_i^{t} | q, m_i, \pib_i^{<t}\big) }_\text{2) $\KG$ Reasoning} \nonumber
% \end{align}
% \begin{align}
%     \Pb\big(\pib_i^{t} | q, m_i, \pib_i^{<t}\big) = \Pb\big(a_i^{t} | q, m_i, \pib_i^{<t}\big) \cdot \Pb\big(\pib_i^{t} | a_i^{t}, \pib_i^{t-1}\big)
% \end{align}
% }







% \begin{align}
%     \pib^{(t)}_i  = \Pb^{(t)}_{ent}\big(e | m_i, q, \pib^{<t}_i \big)\\
%     a^{(t)}_i = \Pb^{(t)}_{rel}\big(r | m_i, q, \pib^{<t}_i \big)
% \end{align}




% In this paper, we propose to use knowledge graph reasoning to retrieve necessary knowledge and answer such queries end-to-end. Specifically, we desire the model to conduct path-based Reasoning for every entity mentions. We maintain different states $\pib^{(t)}_i \in \mathcal{R}^{|\mathcal{E}|}$, each of which is a probability vector representing the distribution of mention $m_i$ staying at each entity at $t$-th reasoning step, i.e., $\pib^{(t)}_i  = \Pb^{(t)}_{ent}\big(e | m_i, q \big)$. For each state, \method should first predict the relation action to take at the current step, and then walk over the $\KG$ to reach informative entities to get the answer.


% many previous works assume the answer is also an entity node in the knowledge graph $\KG$, and tries to parse the question into a structured program (e.g., SQL query or a dependency graph). The problem with such methods is that answer is not necessarily to be an entity or appeared in $\KG$. 





% Many existing QA systems following retriever-reader pipeline adopts a retriever to rank passages in text corpus that are likely to contain missing knowledge, and then use a separate Language Model (\texttt{LM}, e.g., BERT and T5) as reader to re-encode question and retrieved passage and get answer. Instead, our proposed \method enables \texttt{LM} to directly interact with a Knowledge Graph, which stores a large amount of factual knowledge, to retrieve necessary knowledge and answer the question in an end-to-end manner.


% When it comes to open-domain questions, there might exist multiple entity mentions $\{m_i\}$ in question $q$, and the relationship between each entity to target answer could be different and unknown, which the model needs to detect from the question. Therefore, we propose to conduct KGR for each entity mention. Specifically, we maintain different states $\pib^{(t)}_i \in \mathcal{R}^{|\mathcal{E}|}$, each of which is a probability vector representing the distribution of mention $m_i$ staying at each entity at $t$-th reasoning step, i.e., $\pib^{(t)}_i  = \Pb^{(t)}_{ent}\big(e | m_i, q \big)$. For each state, \method should first predict the relation action to take at the current step, and then walk over the $\KG$ to reach entities that are informative to get the answer.




% If the answer $a$ is an entity and there exist a direct relational edge in $\KG$ linking an entity in the question to the answer, the model just needs to identify such link to predict the answer. However, for most cases, such a direct one-hop edge is missing due to incompleteness of the $\KG$, or the answer is not necessarily be an entity. Therefore, our goal of knowledge graph reasoning in \method is find most relevant entities that contain the required knowledge towards the answer, and add these entities back to \texttt{LM}.
% \paragraph{Knowledge Graph Reasoning}
% \YX{I removed the previous preliminary section and merge it with this section. The closed-book section is moved to the related works part.}


% \paragraph{Preliminary}
% We denote a Knowledge Graph $\KG = \big(\mathcal{E}, \mathcal{R}, \mathcal{A} = \{A_r \}_{r \in \mathcal{R}} \big)$, where each $e \in \mathcal{E}$ and $r \in \mathcal{R}$ is entity node and relation label.
% % as well as a binary function $r: \mathcal{E} \times \mathcal{E} \rightarrow \{\text{True}, \text{False}\}$ indicating whether relation $r$ holds between a pair of entities
% $A_r \in \{0,1\}^{|\mathcal{E}| \times |\mathcal{E}|} $ is a sparse adjacency matrix indicating whether relation $r$ holds between a pair of entities.
% % , s.t. $A_r[s,t] = 1 \Leftrightarrow  r(s,t) $.
% The task of knowledge graph reasoning aims at answering factoid query $(s, r, ?)$, i.e., which target entity has relation with source entity $s$. If $\KG$ is complete, we could simply get answers by checking adjacency matrix, i.e., $\{\forall t : A_r[s,t]=1\}$. For incomplete $\KG$ where many relational facts are missing, path-based reasoning approaches~\cite{DBLP:conf/emnlp/LaoMC11, DBLP:conf/emnlp/XiongHW17, DBLP:conf/iclr/DasDZVDKSM18} 
% have been proposed to answer the one-hop query via finding multi-hop paths. For example, to answer query $(s, \text{Mother}, ?)$ a path $s \xrightarrow{\text{Father}} j \xrightarrow{\text{Wife}} t$ could reach target answer $t$.


\paragraph{Preliminary}
We denote a Knowledge Graph $\KG = \big(\mathcal{E}, \mathcal{R}, \mathcal{A} = \{A_r \}_{r \in \mathcal{R}} \big)$, where each $e \in \mathcal{E}$ and $r \in \mathcal{R}$ is entity node and relation label.
% as well as a binary function $r: \mathcal{E} \times \mathcal{E} \rightarrow \{\text{True}, \text{False}\}$ indicating whether relation $r$ holds between a pair of entities
$A_r \in \{0,1\}^{|\mathcal{E}| \times |\mathcal{E}|} $ is a sparse adjacency matrix indicating whether relation $r$ holds between a pair of entities.
% , s.t. $A_r[s,t] = 1 \Leftrightarrow  r(s,t) $.
The task of knowledge graph reasoning aims at answering a factoid query $(s, r, ?)$, i.e., which target entity has relation $r$ with the source entity $s$. If $\KG$ is complete, we could simply get answers by checking the adjacency matrix, i.e., $\{\forall t : A_r[s,t]=1\}$. For incomplete $\KG$ where many relational facts are missing, path-based reasoning approaches~\cite{DBLP:conf/emnlp/LaoMC11, DBLP:conf/emnlp/XiongHW17, DBLP:conf/iclr/DasDZVDKSM18} 
have been proposed to answer the one-hop query via finding multi-hop paths. For example, to answer the query $(s, \text{Mother}, ?)$, a path $s \xrightarrow{\text{Father}} j \xrightarrow{\text{Wife}} t$ could reach the target answer $t$. 
% \YS{say something about the connection of these preliminaries to our model.}
% Previous work mainly focus on reasoning over structured $\KG$ only, while in this paper we try to integrate $\KG$ into neural Language Models to solve 
In this paper we try to integrate symbolic $\KG$ reasoning into neural \texttt{LM}s and help it deal with ODQA problems.
% \looseness=-1

% \vspace{0.05in}
\paragraph{Overview of \method}
We illustrate the overall architecture of \method in Figure~\ref{fig:framework}. All the \colorbox{lb}{light blue blocks} are our added components to support $\KG$ reasoning, while the \colorbox{db}{dark blue} Transformer layers are knowledge-injected \texttt{LM}. The key component of \method for conducting $\KG$ reasoning is the Knowledge Interaction Layers (\texttt{KIL}), which are added amid \texttt{LM} layers to enable deeper interaction with the $\KG$. %by sending instruction to and receiving knowledge.
%, and like the cream filled inside wafers of O\textsc{reo} cookies.


% More specifically, 
% % maintains states for each entity mention and conduct walking. 
% we highlight the three procedures in red dotted box. 
% Firstly, \texttt{KIL} predicts relation action to take for each entity mention, based on which $\KG$ re-weights the graph and conduct contextualized random walk to update entity distribution state. An weighted aggregated entity embedding will then be sent back to \texttt{KIL} and add into a placeholder token as retrieved knowledge. By repeating such procedure multiple times, \method could retrieve knowledge from $\KG$ that are multi-hop away from initial entities in the question, based on which to answer complex open-domain questions.


%\YS{The definition below is confusing. From (discrete) stochastic process terminology, the states are discrete, say entities. Then we have transition probabilities from one state to another state, which is usually described by a T (or P) matrix. Then we have distribution over states at each time stamp t, which is $\pi^{(t)}$. A path naturally is a sequence of states. Can you please make the definitions clearer and try to connect to convention notations? } 
%\YS{by re-reading it, $\pi_i^{(t)}$ in the current writing is a state distribution starting from entity $i$ at timestamp t. In other words $\pi_i^{(0)}$ is an all zero vector except at the position of i, which is 1. ZN: In current notation, i is just a index (denote mention $m_i$) not entity ID. $pi_i$ could refer to different entity based on results of entity linking (e.g., during inference, we use entity linking model to calculate the entity distribution)
%
%During pre-training, it's indeed a one-hot vector, with 1 at $e_i$?
%
%Regarding to how to connect to stochastic process, I think in our case entity is the discrete state (or vertex in RW), pi is the distribution, contextualized RW matrix (changed by relation) is the transition matrix. 

%I currently simply denote probability vector pi as "state", not sure what's the better terminology for it? As in our case we didn't really leverage the discrete assignment, but just utilize this pi

%}
%\YS{$\pi_i ^{(t)}$ is the state distribution at time t for a walk starting at $i$. This is not the state. Please also do not use ``path'' to denote the ``state distribution sequence''. The state distribution will be changed at each time stamp given a different transition matrix. Please do not introduce terminology that is not completely necessary.} \YS{if we drop the rigorous terminology, $\pi_i ^{(t)}$ can be called entity distribution at time $t$ with respect to entity $i$.}

Given a question $q=$ ``The Bauhaus represented Germany's recovery from which event?'', QA model needs to extract knowledge about all $n$ in-context entity mentions $M=\{m_i\}_{i=1}^n$, e.g., the history of ``Germany'' at the time when ``Bauhaus'' is founded, to get the answer $a=$ ``World War I''. Such open-domain Q\&A can be abstracted as $P(a | q, M)$. 
% \looseness=-1
% In this paper, we propose to empower Language Models (\texttt{LM}, e.g., BERT and T5) with knowledge graph reasoning to retrieve necessary knowledge and answer such query in an end-to-end manner. Specifically, starting from each entity mention $m_i \in M$, we desire the model to learn 


Starting from each mentioned entity $m_i$, we desire the model to learn to walk over the graph to retrieve relevant knowledge and form a $T$-length reasoning path for answering this question, where $T$ is a hyper-parameter denote the 
longest reasoning path required to answer the questions.
% Starting from each mentioned entity $m_i$, we desire the model to learn to walk over the graph to retrieve relevant knowledge for answering this question.
We define each reasoning path starting from the entity mention $m_i$ as a chain of entities (states) random variables $\rho_i = \{e^{t}_i\}_{t=0}^T$, where each mentioned entity is the initial state, i.e., $e^{0}_i = m_i$. The union of all paths for this question is defined as $\Rho = \{ \rho_i \}$, which contains the reasoning paths from each mentioned entity to answer the question.


% $\pib_i^{(0)} \xrightarrow{\rb_i^{(1)}} \pib_i^{(1)} \ldots  \pib_i^{(T-1)}\xrightarrow{\rb_i^{(T)}} \pib_i^{(T)}$ starting from each entity mention $m_i \in M$. At $t$-th reasoning step, each $\pib^{(t)}_i = \Pb_{ent}^{(t)}(e | q, m_i) \in \mathcal{R}^{|\mathcal{E}|}$
% is a probability distribution, with $\pib^{(t)}_i[e]$ being the probability of staying at entity $e$ 
% % is a entity distribution row-vector
% is a probability vector representing the current entity distribution starting from mention $m_i$, 
% and $\rb_i^{(t)}  \in \mathcal{R}^{|\mathcal{R}|}$ 
% % is a relation action probability row-vector. 
% % \YX{Are all the vectors in this paper row vectors? It will be good if we can be consistent. If so, just add note saying that all the vectors are row vectors.}
% is a probability vector representing the current relation distribution predicted to transit the state.

% Take the first reasoning step as an example, the state starting from entity mention $m$ = ``Bauhaus'' has highest probability staying at ``Walter Gropius'', who is the inventor of ``Bauhaus'' as its representative. And by knowing ``Walter'' was involved in the ``World War I'' in the second reasoning step, \method could infer the correct answer, via a path $\text{Bauhaus} \xrightarrow{\text{founded}} \text{Walter} \xrightarrow{\text{in war}} \text{World War I}$.


%For each reasoning path $\rho_i$, 
% \YS{Notation $\pi$ is overloaded. We didn't give a notation for a path yet. } 
\method factorizes $\Pb\big(a|  q, M \big)$ by incorporating  possible paths $\Rho$ as a latent variable, yielding:

%\YS{more precisely, we need to sum up all the possible paths. For each path $i, i_1, i_2, \ldots, i_T$, we do the factorization into (1) the probability to get the path; (2) the probability to arrive the answer given the path. For (1), decompose the path into T 1-step transitions (e.g., $T_{i_{t-1},i_{t}}$) }
% \YX{Please correct all occurrences of $\{m_i\}$ and $\{\pib_i\}$ to be with subsripts and upper scripts: $\{m_i\}_{i=1}^n$ and $\{\pib_i\}_{i=1}^n$. You can also use $M$ and $\Pbi$ directly. Also in Sec 3.2 you defined $|M|=N$; you can define it here.}
% {\small
% \begin{align}
% &\Pb\big( a |  q, M \big) = \Pb \big(\Pib | q, \{m_i\} \big) \cdot  \Pb\big(a | q, M, \Pib\}\big) \\
% &=\Big( \prod_{m_i} \Pb\big(\pib_i^{1:T} | q, m_i \big) \Big) \cdot  \Pb\big(a | q, \{m_i, \pib_i^{1:T}\}\big)    \\
% &=  \Big(\prod_{m_i} \prod_{t=1}^T \underbrace{\Pb\big(\pib_i^{t} |q, m_i, \pib_i^{<t} \big)}_\text{1) $\KG$ Reasoning} \Big)  \underbrace{  
% \Pb\big(a | q, \{m_i, \pib_i^{1:T}\}\big)}_\text{2) knowledge-injected \texttt{LM}} \nonumber \label{eq:overall}
% \end{align}
% }

{\small
% \vspace{-10pt}
\begin{align*}
&\Pb\big( a |  q, M \big) = \sum\nolimits_{\Rho}  \Pb \big(\Rho| q, \{m_i\}_{i=1}^n \big) \cdot  \Pb\big(a | q, M, \Rho\big) \\
&=\sum\nolimits_{\Rho}  \Big( \prod_{i=1}^{n} \Pb\big(\rho_i | q, m_i \big) \Big) \cdot  \Pb\Big(a | q, \{m_i, \rho_i\}_{i=1}^{n} \Big)    \\
&= \sum\nolimits_{\Rho}   \Big(\prod_{i=1}^{n} \prod_{t=1}^T \underbrace{\Pb\big(e_i^{t} |q, e_i^{<t} \big)}_\text{$\KG$ Reasoning (\ref{sec:reason})} \Big)  \underbrace{  
\Pb\Big(a | q, \{e_i^{0:T}\}_{i=1}^{n} \Big)}_\text{knowledge-injected \texttt{LM} (\ref{sec:lm})}  \label{eq:overall}
\end{align*}
}% \YX{(add explanation) here the second equation is because we assume each $\pib$ to be independent with each other and with other entities, i.e., $\Pb(\pib_i^{1:T}|q, M)=\Pb(\pib_i^{1:T}|q, m_i)$.}
%The second and third equation assume that reasoning path starting from different entities are mutually independent conditioned on question: $e_i^{(t)} \indep e_j^{(t)} \mid q, \forall t$.

We assume (1) reasoning paths starting from different entities are generated independently; and (2) reasoning paths can be generated autoregressively.

% \looseness=-1 
%\YS{the previous paragraph needs update. Need to define $\pi$ and explain why do we have these constraints, and how to implement these constraints.} \YS{I think what you try to do here is to justify the decomposition.  }

In this way, the QA problem can be decomposed into two entangled steps: 1) $\KG$ Reasoning, which autoregressively walks through the graph to get a path $\rho_i$ starting from each entity mention $m_i$; and 2) knowledge-injected \texttt{LM}, which benefits from the reasoning paths to obtain the out-context knowledge for answer prediction. 

The relational path $\rho_i$ in $\KG$ Reasoning requires the selection of next entity $e_i^t$ at each step $t$. We further decompose it into two steps: 1.a) relation prediction, in which \texttt{LM} is involved to predict the next-hop relation 
%next-hop continuous relation action 
based on the current state and context; and 1.b) the non-parametric state transition, which is to predict the next-hop entity based on the $\KG$ and the predicted relation. 
%to updating entity state. 
Formally:

% \vspace{-10pt}
{
\footnotesize
\begin{align}  \nonumber
\underbrace{\Pb\big(e_i^{t} | q, e_i^{<t}\big)}_\text{$\KG$ Reasoning (\ref{sec:reason})} \!=\! \sum_r \underbrace{\Pb_{rel}\big(r_i^{t} | q,  e_i^{<t}\big)}_\text{relation prediction (\ref{sec:relation})} \cdot \underbrace{\Pb_{walk}\big(e_i^{t} | r_i^{t}, e_i^{<t}\big)}_\text{contextualized random walk (\ref{sec:crw})}
\end{align}  
}

%\YS{it seems to me $\underbrace{\Pb\big(\pib_i^{t} | q, m_i, \pib_i^{<t}\big)}_\text{$\KG$ Reasoning (\ref{sec:reason})}$ should be  $\underbrace{\Pb^{(t)}\big(e_j | q, m_i, \pib_i^{<t}\big)}_\text{$\KG$ Reasoning (\ref{sec:reason})}$. In other words, the distribution is not defined over $\pi$ but over entities $e_j$}


%To support differentiable learning, in our \method framework, we do not really do discrete state transition. Instead, 
We keep track of the entity distribution at each step $t$ via the probability vector\footnote{Throughout the paper, all vectors are row-vectors} $\pib^{(t)}_i \in \mathcal{R}^{|\mathcal{E}|}$,
with $\pib^{(t)}_i[e]$ being the probability of staying at entity $e$, i.e., $\Pb\big(e_i^{t} = e | q, e_i^{<t}\big)$. 
%\YS{We use $\pib^{(t)}_i[e]$ to denote $\Pb\big(e_i^{t} = e | q, e_i^{<t}\big)$. Please confirm whether it is the case.}

% More specifically, 
% % maintains states for each entity mention and conduct walking. 
We highlight the three procedures in red dotted box in Figure~\ref{fig:framework}. We take the first reasoning step starting from entity mention ``Bauhaus'' as an example. In the first red box within \texttt{KIL}, we predict which relation action should be taken for entity ``Bauhaus'', and send the prediction (e.g. ``Founded'') to $\KG$. In the second red box, $\KG$ re-weights the graph and conducts contextualized random walk to update entity distribution, where ``Walter'' has the highest probability. Finally, weighted by the entity distribution, an aggregated entity embedding is sent back to \texttt{KIL} and added into a placeholder token as the knowledge, so the later \texttt{LM} layer knows to focus on the retrieved ``Walter''. We introduce these steps in the following.










% Take the first reasoning step as an example, the state starting from entity mention $m$ = ``Bauhaus'' has highest probability staying at ``Walter Gropius'', who is the inventor of ``Bauhaus'' as its representative. And by knowing ``Walter'' was involved in the ``World War I'' in the second reasoning step, \method could infer the correct answer, via a path $\text{Bauhaus} \xrightarrow{\text{founded}} \text{Walter} \xrightarrow{\text{in war}} \text{World War I}$.




% Firstly, \texttt{KIL} predicts relation action to take for each entity mention, based on which $\KG$ re-weights the graph and conduct contextualized random walk to update entity distribution state. An weighted aggregated entity embedding will then be sent back to \texttt{KIL} and add into a placeholder token as retrieved knowledge. By repeating such procedure multiple times, \method could retrieve knowledge from $\KG$ that are multi-hop away from initial entities in the question, based on which to answer complex open-domain questions.




\paragraph{Input}
% \YX{I slightly changed the wording here.} 
Initially, we first identify all $N$ entity mentions $\{m_i\}_{i=1}^N$ in the input question $q$ as well as the corresponding $\KG$ entities\footnote{
For Wikipedia pretraining, we use the ground-truth entity label as one-hot initialization for $\pib_i^0$. For downstream tasks we use GENRE~\citep{DBLP:conf/iclr/CaoI0P21} to get top 5 entity links.
% We could flexibly utilize ground-truth entity label or trained model such as GENRE~\citep{DBLP:conf/iclr/CaoI0P21} to get entity linking.
}..
% We define the initial entity distribution for $m_i$ as $\pib_i^0 = \Pb(e | m_i, q)$
For each mention $m_i$ we add three special tokens as the interface for Knowledge Interaction Layers (\texttt{KIL}) to send instruction and receive knowledge: we add a [\texttt{S-ENT}] token before, and [\texttt{REL}], [\texttt{T-ENT}] tokens after each entity mention $m_i$. \texttt{KIL} can be flexibly inserted into arbitrary $\texttt{LM}$ intermediate layer. By default, we just insert each \texttt{KIL} every $N$ 
% \YX{$N$ is also the number of entity mentions. Can you change to another letter?} 
Transformer-based $\texttt{LM}$ layers, thus the input to the $t$-th \texttt{KIL} are contextualized embeddings of each token $k$ as $\texttt{LM}^{(t)}_k$, including added special tokens. 


% Research have shown that a single pre-trained Language Model (\texttt{LM}, e.g., T5) already stores a portion of knowledge within its parameter to solve some single-hop questions~\citep{DBLP:conf/emnlp/RobertsRS20}, but its performance is still lower than retrieval-based method that takes Wikipedia as knowledge source, and cannot handle multi-hop questions. 


\subsection{\texttt{LM} involved $\KG$ Reasoning} \label{sec:reason}
\noindent We first introduce the reasoning process $\Pb\big(e_i^{t} | q, e_i^{<t}\big) \!=\! \sum_r \Pb\big(r_i^{t} | q,  e_i^{<t}\big) \cdot\Pb\big(e_i^{t} | r_i^{t}, e_i^{<t}\big)$.
% \looseness=-1
% following Eq.(\ref{eq:reason}).
\subsubsection{Relation Prediction.} \label{sec:relation}
% \YX{I slightly changed the wording here.} 
\noindent For each entity mention $m_i$, we desire to predict which relation action should take $r_i^{t}$ as instruction to transit state. 
We define the predicted relation probability vector $\rb_i^{(t)} = \Pb_{rel}\big(r_i^{t} | q,  e_i^{<t}\big) \in \mathcal{R}^{|\mathcal{R}|}$ representing the relation distribution to guide walking through the graph. 
Denote the corresponding [\texttt{REL}] token as $\texttt{REL}[i]$ (and similarly for other special tokens). The contextual embedding $\texttt{LM}^{(t)}_{\texttt{REL}[i]}$ encode the relevant information in question $q$  %\YS{what is x?} 
that hints next relation. We maintain a global relation key memory $\texttt{K}_{rel} \in \mathbb{R}^{|\mathcal{R}| \times d}$ storing each relation's $d$-dimentional embedding. To calculate similarity, we first get relation query $Q^{(t)}_{\texttt{REL}[i]}$ by projecting relation token's embedding into the same space of key memory via a projection head Q-Proj\footnote{We denote a non-linear MLP projection as X-Proj$(h)=W^X_2 \sigma(W^X_1h+b_1)+b_2$, where X have different instantiations.} followed by a LayerNorm (abbreviated as LN), and then calculate dot-product similarity followed by softmax:
% \looseness=-1
\begin{align}
        &Q^{(t)}_{\texttt{REL}[i]} = \text{LN}^{(t)}\big(\text{Q-Proj}^{(t)}(\texttt{LM}^{(t)}_{\texttt{REL}[i]})\big), \\
    &\rb_i^{(t)} =\Pb_{rel}\big(r_i^{t} | q,  e_i^{<t}\big) = \text{Softmax} \big( Q^{(t)}_{\texttt{REL}[i]} \ \texttt{K}_{rel}^T   \big). 
\end{align}
%\YS{from the above formula,  $\pib_i^{<t}$ is not used to calculate the relation probability. Instead, it requires other intermediate results from language models.}

Note that the relation queries $\texttt{LM}^{(t)}_{\texttt{REL}[i]}$ are different for every mention $m_i$ and reasoning step $t$ depending on the context, and thus the the relation distributions $\rb_i^{(t)}$ gives contextualized predictions based on the question $q$. The predicted relations are sent to the knowledge graph reasoning module as instruction to conduct state transition.






% and embedding $\texttt{LM}^{(t)}_{\texttt{S-ENT}}$ should identify the current entity state, $\texttt{LM}^{(t)}_{\texttt{REL}}$ should identify the next relation ,acccmtion to take, and the retrieved knowledge (aggragated entity embedding) will be added back to $\texttt{LM}^{(t)}_{\texttt{T-ENT}}$ for passing the knowledge graph reasoning's results back to \texttt{LM}. 

% We denote each added token as a reasoning prompt. \method could simultaneously proceed Reasoning for all prompt within the context. For simplicity, we only focus on one specific entity $e$ to show the procedure.
% Before each reasoning step, we use $N$ Transformer layer of $\texttt{LM}$ to process the contexts. We denote $\texttt{LM}^{(t)}_{\texttt{S-ENT}}$, $\texttt{LM}^{(t)}_{\texttt{REL}}$, $\texttt{LM}^{(t)}_{\texttt{T-ENT}}$ as the three $d$-dimensional embedding vector before $t$-th reasoning step for the three token .




% For both pre-training and fine-tuning, \method takes some paragraphs $x$ as input and learns to predict outputs $y$ via $P(y|x)$. For pre-training, the task is to predict or autoregressively generate masked tokens, while for fine-tuning on QA, $x$ is question and $y$ is the answer.





% \begin{algorithm}
%   \caption{Pseudo Codes of Contextualized Random Walk (CRW) Implementation}
%   \Function{preprocess($\pib^0 = \{\pib_i^0 \}_{i=1}^N$, $A$)}{
%   }
% \end{algorithm}


\subsubsection{Contextualized KG Random Walk} \label{sec:crw}


% Many previous knowledge graph reasoning works propose to walk over the graph where each state is a discrete entity node. Such methods cannot be differentiated and they use reinforcement learning to get reward for training the walking agent. Our approach, instead, propose to keep a continuous entity distribution as the state, i.e., $\pib^{(t)} = \Pb^{(t)}_{ent}(e | m_i, x)$, thus the transition policy could be directly optimized through back-propagation. 
% \YX{Slightly changed the notation here; please check if it makes sense.}
\noindent Next, we introduce how we conduct state transition $\Pb_{walk}\big(e_i^{t} | r_i^{t}, e_i^{<t}\big)$. One classic transition algorithm is random walk, which is a special case of markov chain, i.e. the transition probability only depends on previous state. Consider a state at entity $s$, the probability walking to target $t$ is $\frac{1}{deg(s)}$ if $A[s,t]=1$. Based on it, we define the Markov transition matrix for random walk as $M_{rw} = \Db_A^{-1}A$, where the degree matrix $\Db_A\in \mathbb{R}^{|\mathcal{E}| \times |\mathcal{E}| }$ is defined as the diagonal matrix with the degrees $deg(1),\ldots,deg({|\mathcal{E}|)}$ on the diagonal.
With random walk Markov matrix $M_{rw}$ we can transit the state distribution as:
$\pib^{(t)} = \pib^{(t-1)} M$,
The limitation of random walk is that the transition strategy is not dependent on the question $q$. 
%\YS{x?} 
We thus propose a Contextualized Random Walk (\rw). 
% \looseness=-1

Based on the predicted relation distribution $\rb_i^{(t)}$, we calculate a different weighted adjacency matrix $\widetilde{A}_i^{(t)} \in \mathbb{R}^{|\mathcal{E}| \times |\mathcal{E}|}$ by adjusting the edge weight:
\begin{align}
    % &\widetilde{A}_i^{(t)} = \sum_{r  \in \mathcal{R}}  w_r  \cdot \rb_{i,r}^{(t)} \ \big(D_A^{-1}A_r\big)  \\
    &\widetilde{A}_i^{(t)} = \sum\nolimits_{r  \in \mathcal{R}}   w_r \cdot  \rb_{i,r}^{(t)}  \cdot  A_r,\\
    & {M_{crw, i}}^{(t)} =  \Db_{\widetilde{A}_i^{(t)}}^{-1} \widetilde{A}_i^{(t)}, \ \ \forall i \in [1, N].
\end{align}
where $w_r$ is a learnable importance weight for relation $r$ that helps solving downstream tasks, and $\rb_{i,r}^{(t)}$ is the probability corresponding to relation $r$ in $\rb_{i}^{(t)}$. With the transition matrix ${M_{crw,i}}^{(t)}$, the state transition is defined as $\pib^{(t)}_i = \pib^{(t-1)}_i  M_{crw,i}^{(t)}$.




% \YS{(1) the double normalization mentioned above seems redundant. Only the second one is needed. (2) writing wise, try to use high-level notation to help people to get a high-level idea, such as mentioning transition matrix that is specific to the starting point $i$ and the current step $t$. Then define this transition matrix. Then write down the calculation of $\pi_i^{(t)}$. (3) another confusing point is, $\pi_i^{(t)}$ is computed in a deterministic way. why do we need to use $P(\pi_i^{(t)}|...)$ notation for $\pi_i^{(t)}$? }



\rw allows each reasoning path $\rho_i$ to have its transition matrix. However, as the total number of entity nodes $|\mathcal{E}|$ could be huge (e.g., 5M for WikiData), we cannot afford to update the entire adjacency matrix for every in-batch mention. We thus adopt a scatter-gather pipeline to implement graph walking as shown in Algorithm~\ref{algo:rw}. We first gather the entity and relation probability to each edge, and then scatter the probability to target nodes. This allows us to simultaneously conduct message passing with modified adjacency weight $\widetilde{A}_i^{t}$ for all entity mention $m_i$ in parallel. 

\begin{algorithm}[!ht] 
\lstset{style=style_snippet}
\begin{lstlisting}[language=Python]
def ContextualizedRandomWalk(
    i_init, KG,   # initial entity index and Graph
    w_deg, w_rel, # inv(degree) and relation weights 
    p_ent, p_rel  # entity and predicted relation dis-                          
                  # tribution tensor @ t-th step.
): -> FloatTensor 
    # Get <src, rel, tgt> edge list of k-hop subgraph
    i_src, i_rel, i_tgt = k_hop_subgraph(i_init, KG)
    # Gather entity and relation probability to edge
    p_src  = (p_ent * w_deg)[:, i_src] # N x n_edge
    p_rel  = (p_rel * w_rel)[:, i_rel] # N x n_edge
    p_edge = l1_normalize(p_src * p_rel, dim=1)
    # Scatter edge probability to target node
    p_ent  = scatter_add(src=p_edge, idx=i_tgt, dim=1)
    return p_ent  #(t+1)-th step's entity distribution
\end{lstlisting}
\caption{Pytorch Pseudocode of CRW}\label{algo:rw}
\end{algorithm}


The complexity is $\#$ of in-batch entities times $\#$ of edges in $T$-hop subgraph starting from these entities, i.e., $\mathcal{O}(n \times \#\text{edge})$, and thus this operation is not expensive. 
% \YX{Is this local subgraph for only one example or for one batch? Please specify. If the latter, also specify the batch size here.}
Another concern is why not using Graph Neural Networks (GNNs). We provide discussion in Sec.~\ref{sec:gnn} in Appendix.



\subsection{Knowledge-Injected \texttt{LM}} \label{sec:lm}

\noindent After we get the updated entity distribution $\pib^{(t)}_i$, we want to inject such information back to the $\texttt{LM}$ without harming its overall structure. We maintain a global entity embedding value memory $\texttt{V}_{ent} \in \mathbb{R}^{|\mathcal{E}| \times d}$ storing entity embeddings. We only consider the entities within the sampled local subgraph in each batch. We thus get an entity index list $\Ib$ as the query to sparsely retrieve a set of candidate entity embeddings and then aggregate them weighted by entity distribution and embedding table. We then use a Value Projection block to map the aggregated entity embedding into the space of $\texttt{LM}$, and then directly add the transformed embedding back to the output of \texttt{T-ENT}.
\begin{align} 
    &V^{(t)}_i = \text{V-Proj}^{(t)}\big(\pib^{(t)}_i \cdot \texttt{V}_{ent}[\Ib]\big),\\
    &\widehat{\texttt{LM}}^{(t)}_{\texttt{T-ENT}[i]} =  \text{LN}^{(t)}\big(\texttt{LM}^{(t)}_{\texttt{T-ENT}[i]} + V^{(t)}_i \big).
\end{align}
Then, we just take all $\widehat{\texttt{LM}}^{(t)}_{\texttt{T-ENT}}$ as input to next Transformer-based \texttt{LM} layer to learn the interaction between the retrieved knowledge with in-context words via self-attention. 

By repeating the \texttt{KIL} for $T$ times, the final representation $\widehat{\texttt{LM}}^T$ is conditioned on the reasoning paths $\rho_i = e_i^{0:T}$, which reaches entities that are $T$-hop away from initial entity $m_i$ in the question. Finally, we can predict the answer of open questions $\Pb\big(a | q, \{e_i^{0:T}\}_{i=1}^{n} \big)$ by taking knowledge-injected representation $\widehat{\texttt{LM}}^T$ for span extraction, entity prediction or direct answer generation. 
% \YS{the previous paragraph is outdated.}
















\subsection{Pre-Train \method to Reason}




%\YS{I feel there are lots of smart designs for the pre-training tasks. But the writing does not reflect that yet.}\YS{try to write: issues; solutions; losses}

\noindent The design of \method allows end-to-end training given QA datasets. However, due to the small coverage of knowledge facts for existing QA datasets, we need to pretrain \method on a large-scale corpus to get good entity embeddings.
% it's unlikely to directly train \method using downstream QA tasks without proper initialization. 

% We therefore introduce how we pre-train \method using unlabelled Wikipedia Corpus to learn to reason over $\KG$.

\paragraph{Salient Span Masking} 
% \YS{shall we upgrade this into a subsection? right now we only have one subsection.}
One straightforward approach is to use
Salient Span Masking (SSM) objective~\citep{DBLP:journals/corr/abs-2002-08909} masks out entities or noun tokens requiring specific out-of-context knowledge. We mainly mask out entities for guiding \method to reason. Instead of randomly masking entity mentions, we explicitly sample a set of entity IDs and mask every mentions linking to these entities. This could prevent the model copy the entity from the context to fill in the blank. We also follow~\citep{DBLP:conf/nips/YangDYCSL19} to mask out consecutive token spans. We then calculate the cross-entropy loss on each salient span masked (SSM) token as $\mathcal{L}_{SSM}$. 
% \looseness=-1


%\YS{why these two objectives? How these two objective connect to the two losses mentioned below.}



\subsubsection{Weakly Supervised Training of $\texttt{KIL}$}
\noindent Ideally, \method can learn all the entity knowledge and how to access the knowledge graph by solely optimizing $\mathcal{L}_{SSM}$. However, without a good initialization of entity and relation embeddings, \texttt{KIL} makes a random prediction, and the retrieved entities by $\KG$ reasoning are likely to be unrelated to the question. In this situation, \texttt{KIL} does not receive meaningful gradients to update the parameters, and \texttt{LM} learns to ignore the knowledge.  
%\YS{what are the above two paragraphs? Can we put them into subsections related to specific losses? In other words, quickly tell the readers what they are reading.}
% no matter what entities retrieved within the reasoning procedure, the final aggregated embeddings could not benefit the language model objective, and thus the model won't get the correct signal to learn which relation should choose, and how to improve entity embeddings.
% \YX{This is not quite true since you also train the entity and relation embeddings. I'd suggest to weaken the tone. Like: However, if the entity and relation embeddings are not sufficiently trained, the model might not benefit from the \texttt{KIL} layers.} 
To avoid this cold-start problem and provide entity and relation embedding a good initialization, We utilize the following two external signals as self-supervised guidance.
% \looseness=-1

% \vspace{0.05in}
\paragraph{Entity Linking Loss}
To initialize the large entity embedding tables in $\texttt{V}_{ent}$, we use other entities that are not masked as supervision. Similar to~\citet{DBLP:journals/corr/abs-2004-07202}, we force the output embedding of [\texttt{S-ENT}] token before the first \texttt{KIL} followed by a projection head E-Proj to be close to its corresponding entity embedding:
\begin{align*}
    &E_{\texttt{S-ENT}[i]} = \text{LN}\big(\text{E-Proj}(\texttt{LM}^{(1)}_{\texttt{S-ENT}[i]})\big), \\
    &\Pb^{(0)}_{ent}\big(e | m_i, q \big) = \text{Softmax} \big( E_{\texttt{S-ENT}[i]}\ \texttt{V}_{ent}[\Ib]^T   \big),\\
    &\mathcal{L}_{ent} = \sum\nolimits_{m_i} -\log \Pb^{(0)}_{ent}\big(e | m_i, q \big) \cdot \pib^0_i[\Ib].
\end{align*}
% \YX{Is this softmax here over all entities? It is in-batch entities if I remember correctly. Either case, please specify it.}
Similar to Section~\ref{sec:lm}, we only consider entities within the batch, denoted by index $\Ib$.
This contrastive loss guides each entity's embedding $\texttt{V}_{ent}[e]$ closer to all its previously mentioned contextualized embedding, and thus memorizes those context as a good initialization for later knowledge integration. 
% \looseness=-1

% \vspace{0.05in}
\paragraph{Weakly Supervised Relation Path Loss}
Entity mentions within each Wikipedia passage are naturally grounded to WikiData $\KG$. Therefore, after we mask out several entities, we can utilize the $\KG$ to get all reasoning paths from other in-context entities to the masked entities as weakly supervised relation labels.
% \looseness=-1

\begin{figure}[t!]
    \centering
    \includegraphics[width=1.0\columnwidth]{pictures/pre.png}
    \caption{Pre-training sample w/ golden reasoning path. More real examples are shown in Table~\ref{tab:pretrain1} in Appendix.}
    \label{fig:pretrain_example}
%  \vspace{-.1in}
\end{figure}


Formally, we define a \textbf{Grounded Dependency Graph} $\mathcal{DG}$, which contains all reasoning paths within $T$-step from other in-context entities to masked entities, and then define $R_{\mathcal{DG}}(m_i, t)$ as the set of all relations over every edges for entity mention $m_i$ at $t$-th hop. Based on it, we define the weakly supervised relation label $q^{(t)}_i \in \mathbb{R}^{|\mathcal{R}|}$ as the probabilistic vector which uniformly distributed on each relation in set.
% , i.e., $q^{(t)}_i[r] = \frac{1}{|R_{\mathcal{DG}}(m_i, t)|}, \text{if }\ r \in R_{\mathcal{DG}}(m_i, t)$.
% $q^{(t)}_i[r] = \begin{cases}
%     \frac{1}{|R_{\mathcal{DG}}(m_i, t)|},& \text{if }\ r \in R_{\mathcal{DG}}(m_i, t)\\
%     0,              & \text{otherwise}
% \end{cases}$
Note that we call uniformly-weighted $q^{(t)}_i$ as weakly supervised because 1) some paths 
lead to multiple entities rather than only the target masked entity; 2) the correct relation is dependent on the context. Therefore, $q^{(t)}_i$ only provides all potential candidates for reachability, and more fine-grained signals for reasoning should be learned from unsupervised $\mathcal{L}_{SSM}$. We adopt a list-wise ranking loss to guide the model to assign a higher score on these relations than others.
\begin{align*}
\mathcal{L}_{rel} = \sum\nolimits_{m_i}\sum\nolimits_{t=1}^{T} -\log \Pb^{(t)}_{rel}\big(r | m_i, q \big) \cdot q^{(t)}_i.
\end{align*}

% As an example in Figure~\ref{fig:pretrain_example}
% , we take one paragraph from ''Bauhaus'' and mask out entities ''Germany`` and ''World War I`` as targets. We then construct the grounded dependency graph from other entities to these two masked entities and get the weakly supervised relation label for each in-context source entity per reasoning step. More real examples are shown in Table~\ref{tab:pretrain1} in Appendix.


Overall, $\mathcal{L}_{ent}$ and $\mathcal{L}_{rel}$  provide \method with good initialization of the large $\KG$ memory. Afterward, via optimizing $\mathcal{L}_{SSM}$, the reasoning paths that provide informative knowledge receive a positive gradient, guiding \method to reason.


% Specifically, for each Wikipedia passage containing $N$ entity mentions, we first construct a \textbf{K-hop Induced Subgraph}, in which all the paths with length shorter than $K$ connecting any pair of entities are retained. Then, during training, after we randomly mask out a set of entities, we could easily calculate a 


% spanning tree starting from the masked entities to all other in-context entities. Then, for each entity mention $m_i$, 


% To pre-train the model to reason, we desire a self-supervised task that the model could not simply answer by looking at local contexts, but require external knowledge. To serve the purpose, besides the vanilla masked language model, we specifically sample a portion of entities within each page, and mask out all their mentions. In this way, the model cannot simply copy the tokens from contexts to fill in the blank. In addition, as we have the grounded entity graph, we could pre-extract all paths from in-context entity mentions to target masked entities. These paths could serve as golden ground-truth for relation prediction during pre-training, guiding our model to make reasonable relation prediction at an early stage. As illustrated in Figure~\ref{fig:pretrain_example}, we mask out two entities Mudhoney and Nirvana from the paragraph, which is hard for a vanilla \texttt{LM} to generate based on context. We also provide the one-hop and two-hop paths from in-context entity mentions.



