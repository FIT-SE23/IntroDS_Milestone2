\noindent Open-Domain Question Answering (ODQA), one of the most knowledge-intensive NLP tasks, requires QA models to infer out-of-context knowledge to the given single question. 
Following the pioneering work by~\citet{DBLP:conf/acl/ChenFWB17}, ODQA systems often assume to access an external text corpus (e.g., Wikipedia) as an external knowledge source.
Due to the large scale of such textual knowledge sources (e.g., 20GB for Wikipedia), it cannot be encoded in the model parameters. Therefore, most works retrieve relevant passages as knowledge and thus named \textit{Open-Book} models~\citep{DBLP:conf/emnlp/RobertsRS20}, with an analogy of referring to textbooks during an exam. Another line of \textit{Closed-book} models~\citep{DBLP:conf/emnlp/RobertsRS20} assume knowledge could be stored implicitly in parameters of Language Models (\texttt{LM}, e.g. BERT~\citep{DBLP:conf/naacl/DevlinCLT19} and T5~\citep{DBLP:journals/jmlr/RaffelSRLNMZLL20}). These \texttt{LM}s directly generate answers without retrieving from an external corpus and thus benefit from faster inference speed and simpler training. However, current \texttt{LM}s still miss a large portion of factual knowledge~\citep{DBLP:conf/emnlp/PornerWS20, DBLP:conf/eacl/LewisSR21}, and are not competitive with \textit{Open-Book} models.

% To improve \textit{Closed-book} QA performance, a line of works seek more compressed knowledge representation than text corpus so that it could be stored in memory and directly enhance \texttt{LM} without an additional retrieval model. Knowledge Graph ($\KG$), which captures world knowledge explicitly via relational triplets between entities, is a natural choice for enhancing \texttt{LM}. There are a number of nice properties of $\KG$ as knowledge source: 1) it's a more abstract and compressed representation of knowledge than text corpus, and thus could be 


\begin{figure}[t!]
    \centering
    \includegraphics[width=1\columnwidth]{pictures/intro.png}
    \vspace{-10pt}
    \caption{An Illustrative figure of \method. Compared with previous KBQA systems that stack reasoner on top of \texttt{LM}, \method enables interaction between the two. %\YS{use the same color blocks for cheese and cannoli in the question to make it clear those two entities in KG are from the question? }
    }
    \label{fig:intro}
\end{figure}


To improve the knowledge coverage of \texttt{LM}, one natural choice is to leverage knowledge stored in Knowledge Graph ($\KG$, e.g. FreeBase~\citep{DBLP:conf/sigmod/BollackerEPST08} and WikiData~\citep{DBLP:journals/cacm/VrandecicK14}), which explicitly encodes world knowledge via relational triplets between entities. There are several good properties of $\KG$:
%as a knowledge source: 
1) a $\KG$ triplet is a more abstract and compressed representation of knowledge than text, and thus $\KG$ could be stored in memory and directly enhance \texttt{LM} without using an additional retrieval model; 2) the structural nature of $\KG$ could support logical reasoning~\citep{DBLP:conf/iclr/RenHL20} and infer missing knowledge through high-order paths~\citep{DBLP:conf/emnlp/LaoMC11,DBLP:conf/iclr/DasDZVDKSM18}.
Taking the question ``what cheese is used to make the desert cannoli?'' as an example,   even if this relational fact is missing in $\KG$, we could still leverage high-order relationships, e.g., both Ricotta Cheese and Cannoli are specialties in Italy, to infer the answer ``Ricotta Cheese.''
%Such reasoning capability is essential for current \texttt{LM} that misses lots of knowledge. 

In light of the good properties of $\KG$, there are several efforts to build Knowledge Base Question Answering (KBQA) systems.
As is illustrated in Figure~\ref{fig:intro}(a), most KBQA models use \texttt{LM} as a parser to map textual questions into a structured form (e.g., SQL query or subgraph), and then based on $\KG$, the queries could be executed by
symbolic reasoning~\citep{DBLP:conf/emnlp/BerantCFL13} or neural reasoning (e.g. Graph Neural Networks) ~\citep{DBLP:conf/emnlp/SunBC19} to get the answer. 
Another recent line of research \cite{DBLP:conf/naacl/VergaSSC21, DBLP:journals/corr/abs-2010-00796} tries to encode the knowledge graph as the \emph{memory} into \texttt{LM} parameters. 
However, for most methods discussed above, \texttt{LM} is not interacting with $\KG$ to correctly understand the question, and the answer is usually restricted to a node or edge in $\KG$.
% In addition, most KBQA assume answer is an entity contained in $\KG$. 
% These limitations restrict existing KBQA systems for answering open-domain questions.

% and it remains an open question how to effectively integrate $\KG$ reasoning with \texttt{LM}.



% These methods require large amounts of annotated queries to train semantic parser. More recent methods propose adding knowledge graph embeddings~\citep{DBLP:conf/acl/ZhangHLJSL19,DBLP:conf/aaai/LiuW0PY21} or conducting $\KG$-guided pre-training~\citep{DBLP:journals/corr/abs-2010-00796, DBLP:conf/acl/KeJRCWSZH21, DBLP:journals/corr/abs-2010-00796}.

In this paper, we
propose kn\underline{O}wledge \underline{RE}as\underline{O}ning empowered \underline{L}anguage \underline{M}odel
(\method), a model architecture that can be applied to Transformer-based \texttt{LM}s to improve \textit{Closed-Book} ODQA. As is illustrated in Figure~\ref{fig:intro}(b), the key component is the Knowledge Interaction Layers (\texttt{KIL}) inserted amid \texttt{LM} layers, which is like cream filling within two waffles, leading to our model's name O\textsc{reo}. \texttt{KIL} interacts with a $\KG$ reasoning module, in which we maintain different reasoning paths for each entity in the question.
We formulate the retrieval and reasoning process as a contextualized \emph{random walk} over the $\KG$, starting from the in-context entities. Each \texttt{KIL} is responsible for one reasoning step. It first predicts a relation distribution for every in-context entity, and then the $\KG$ reasoning module traverses the graph following the predicted relation distribution. The reasoning result in each step is summarized as a weighted averaged embedding over the retrieved entities from the traversal.
% \looseness=-1
%current reasoning steps (nodes). 

% From bottom to top, each \texttt{KIL} first predict relation for every in-context entity, send the relation to $\KG$. The $\KG$ reasoning module then walks over the graph to expand the reasoning path and reach a new entity as retrieved knowledge. 
% Such a procedure is non-parametric, and we could keep track of each intermediate state to interpret the model decision. 
% \texttt{KIL} received the retrieved knowledge and injected it into \texttt{LM} so that it could better understand the question and make correct relation action at the next step.
By stacking $T$ layers of \texttt{KIL}, \method can retrieve entities that are $T$-hop away from in-context entities and help \texttt{LM} to answer open questions that require out-of-context knowledge or multi-hop reasoning. The whole procedure is fully differentiable, and thus \method learns and infers in an end-to-end manner. 
%In addition, the generated reasoning paths could serve as a rationale to interpret why \method generates a particular answer, which is similar to the parsed tree in traditional KBQA, but \method does not rely on them to get answers.
We further introduce how to pre-train \method over unlabelled Wikipedia corpus. In addition to the salient entity span masking objective, we introduce two self-supervised objectives %from weak supervision 
to guide \method to learn better entity and relation representations and how to reason over them. 


We test \method with RoBERTa and T5 as our base \texttt{LM}s. By evaluating on several single-hop ODQA datasets in \textit{closed-book} setting, we show that \method outperforms existing baselines with fewer model parameters. 
Specifically, \method helps more for questions with missing relations in $\KG$, and questions that require multi-hop reasoning.
We further show that \method can serve as a backbone for \textit{open-book} setting and achieves comparable performance compared with the state-of-the-art QA systems with dedicated design. In addition, \method has better interpretability as it can generate reasoning paths for the answered question and summarize general relational rules to infer missing relations. 

% In addition, the design of \method allows it to accommodate new or modified relational facts, making the \method easier to debug and fast adapt to new knowledge.


This key contributions are as follows:
\begin{compactitem}
\item 
We propose \method to integrate symbolic knowledge graph reasoning with neural LMs. Different from prior works, \method can be seamlessly plugged into existing \texttt{LM}s. 
\item We pretrain \method with RoBERTa and T5 to on the Wikipedia corpus. \method can bring significant performance gain on ODQA.
% , including new state-of-art results in \textit{Closed-Book} settings.
\item \method offers interpretable reasoning paths for answering the question and high-order reasoning rules as rationales.
\end{compactitem}
% , which is more interpretable and capable of handling questions that require complex reasoning.
% \item We propose pre-training objectives to train the recent \texttt{LM}s (i.e., RoBERTa and T5) to reason over unlabelled Wikipedia corpus. \YS{this sentence reads very neutral. The contributions are unclear. For example, are there any differences for pre-trainig OREO from standard LM?}
% , and achieve better significant performance on various downstream QA datasets. 

%\YS{add some significant results. }






% One natural choice to inject knowledge into \texttt{LM} is to use Knowledge Graphs ($\KG$), which is a more high-level abstract of factual knowledge and serve as the basis to conduct symbolic reasoning and answer complex questions. In light of this, a line works propose adding knowledge graph embeddings~\citep{DBLP:conf/acl/ZhangHLJSL19,DBLP:conf/aaai/LiuW0PY21} or conducting $\KG$-guided pre-training~\citep{DBLP:journals/corr/abs-2010-00796, DBLP:conf/acl/KeJRCWSZH21, DBLP:journals/corr/abs-2010-00796}. These approaches, however, still assume continuous model parameters could memorize all the knowledge in $\KG$ and did not retain the original structured $\KG$ to fully exploit its potential for handling complex reasoning tasks.


























































% Below are previous version



\begin{comment}

Large-Scale Language Models (\texttt{LM}, e.g., BERT T5, GPT-3, PaLM, etc.) have been shown to encode a surprising amount of world knowledge into its parameters through pre-training on a massive unlabelled text corpus. 
Take Open Domain Question Answering, one of the knowledge-intensive tasks, as an example. The task is to give the QA model only a single question and require it to infer out-of-context knowledge to generate the answer correctly. Recent studies~\cite{DBLP:conf/emnlp/RobertsRS20, DBLP:conf/nips/BrownMRSKDNSSAA20} show 
that by leveraging pre-trained \texttt{LM}s in a supervised setting, they could correctly answer a certain portion of open questions
% that pre-trained \texttt{LM} can correctly answer a certain portion of open questions after fine-tuning 
(e.g., T5-11B could answer over 30\% of questions in Natural Questions dataset).
% , indicating that the model memorizes a significant amount
% sufficient number 
% of factual knowledge. 
However, recent analysis~\citep{DBLP:conf/emnlp/PornerWS20} shows that for many cases, \texttt{LM} could generate a reasonable answer that follows the syntax, but not all factually correct, indicating that there's still a large number of factual knowledge that is not covered by \texttt{LM}.


Though enabling a single \texttt{LM} to serve as a ''soft`` knowledge base is very appealing, there still exist several limitations of such methods. Firstly, the knowledge is stored implicitly in the black-box model parameters, and we could not interpret the model's decision, e.g., which knowledge it utilizes to solve the question. Secondly, we could not modify the corresponding knowledge if the model makes a wrong prediction. In addition, adding new knowledge requires further fine-tuning the whole model on a newly curated text corpus, which is very exhaustive and expensive.
Thirdly and most importantly, though existing \texttt{LM} can produce a correct answer for a surprising portion of the question, recent analysis~\citep{DBLP:conf/emnlp/PornerWS20} shows that for many other cases, it could generate a reasonable answer that follows the syntax, but not all factually correct.



To mitigate the limitations mentioned above, one series of work add text corpus (e.g., Wikipedia) as an external knowledge source so that \texttt{LM} only needs to focus on understanding the question and reason over the retrieved knowledge. The QA systems following this line could achieve better performance with fewer model parameters. Though effective, due to the large scale of web corpus, these passages as knowledge evidence could not be entirely loaded into the GPU memory. Thus most methods require a separate retriever to identify relevant passages, which is less efficient and more complicated to train end-to-end than a single \texttt{LM}.





One natural choice to inject knowledge into \texttt{LM} is to use Knowledge Graphs ($\KG$), which is a more high-level abstract of factual knowledge and serve as the basis to conduct symbolic reasoning and answer complex questions. In light of this, a line works propose adding knowledge graph embeddings~\citep{DBLP:conf/acl/ZhangHLJSL19,DBLP:conf/aaai/LiuW0PY21} or conducting $\KG$-guided pre-training~\citep{DBLP:journals/corr/abs-2010-00796, DBLP:conf/acl/KeJRCWSZH21, DBLP:journals/corr/abs-2010-00796}. These approaches, however, still assume continuous model parameters could memorize all the knowledge in $\KG$ and did not retain the original structured $\KG$ to fully exploit its potential for handling complex reasoning tasks.




In this paper, we propose kn\underline{O}wledge \underline{RE}as\underline{O}ning empowered \underline{L}anguage \underline{M}odel
(\method) which allows LM to conduct symbolic knowledge graph reasoning over the whole  $\KG$. 
Motivated from the classical ideas of path-based knowledge graph reasoning, we propose to maintain a different reasoning path starting from each entity in the question. We design a fully differentiable Knowledge Integration Layer (\texttt{KIL}) that could be inserted amid arbitrary Transformer layers as the interface for \mothod to interact with $\KG$. \texttt{KIL} sends relational instructions for guiding knowledge graph reasoning, and also receives retrieved knowledge to solve the question. With the predicted relation, we conduct symbolic state transition for each reasoning path as walking over the graph. Such a procedure is non-parametric, and we could keep track of each intermediate state to interpret the model decision. By stacking $T$ layers of \texttt{KIL}, we get multiple $T$-length reasoning paths containing necessary knowledge for answering the question. The whole procedure is fully differentiable, and thus \mothod could be trained end-to-end.



Considering the limited fact coverage of downstream QA datasets, we introduce how to pre-train \method over unlabelled Wikipedia corpus. Apart from the entity salient span masking objective, we also introduce two self-supervised objectives from weak supervision to guide \method of memorizing entity information and conduct reasoning. Using such a framework, we further pre-train RoBERTa and T5 \texttt{LM}. The learned model could improve over multiple ODQA datasets in closed-book and open-book settings, especially those questions requiring multi-hop reasoning steps. We also show that \method has better interpretability as it could generate reasoning paths for the answered question and summarize some general relational horn rules. In addition, the design of \method allows it to flexibly modify or add relational facts, making the \method easier to debug and fast adapt knowledge.


This paper's key contributions are as follows:
\begin{compactitem}
\item We propose a neural-symbolic architecture \method to integrate symbolic knowledge graph reasoning with Neural Language Models, which is more interpretable, flexible, and capable of handling questions that require complex reasoning.
\item We propose pre-training objectives to train the proposed \method to conduct reasoning over unlabelled Wikipedia corpus. The final model could achieve better performance and could summarize critical reasoning paths.
\end{compactitem}


\end{comment}