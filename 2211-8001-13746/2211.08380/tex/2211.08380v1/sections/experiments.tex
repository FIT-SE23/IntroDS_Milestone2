\begin{table}[t]
\centering
\scriptsize
\begin{tabular}{l|ccc} \toprule
\textbf{Name}   & \textbf{Number}  & \textbf{dimension} & \textbf{\#param (M)} \\ \midrule
Number of  Entity& 4,947,397 & 128 & 633\\ 
Number of  Relation& 2,008  & 768 & 1.5\\ 
Number of  Edges & 45,217,947 & - & 47\\ 
\bottomrule
\end{tabular}
\caption{Statistics and parameter of $\mathcal{KG}$ Memory.}
\label{tab:stat}
\end{table}


\begin{table*}[ht]
\hspace{-0.2in}
\centering
\small
\begin{tabular}{lc|ccc|cc} \toprule
\textbf{Models}   & \textbf{\#param}  & \textbf{NQ} & \textbf{WQ} & \textbf{TQA}  & \textbf{ComplexWQ}  & \textbf{HotpotQA}  \\ \midrule
T5 (Base)           & 0.22B  &  25.9 & 27.9 & 29.1  &  11.6 & 22.8\\ 
 \textbf{+ \method} ($T$=1) & 0.23B + \underline{0.68}B &  28.3 & 30.6 & 32.4  &  20.8  & 24.1 \\ 
 \textbf{+ \method} ($T$=2) & 0.24B + \underline{0.68}B  &  28.9 & 31.2 & 33.7  &  23.7 & 26.3\\ \midrule
T5 (Large)           & 0.74B  &  28.5 & 30.6 & 35.9  &   16.7 & 25.3\\ 
 \textbf{+ \method} ($T$=1) & 0.75B + \underline{0.68}B   &  30.6 & 32.8 & 39.1 & 24.5 & 28.2 \\ 
 \textbf{+ \method} ($T$=2) & 0.76B + \underline{0.68}B   &  \textbf{31.0} & \textbf{34.3} & \textbf{40.0}  & \textbf{27.1}  & \textbf{31.4}\\ \midrule
T5-3B~\citep{DBLP:conf/emnlp/RobertsRS20}              & 3B    &  30.4 & 33.6 & 43.4  &   - & 27.8\\ 
T5-11B~\citep{DBLP:conf/emnlp/RobertsRS20}              & 11B    &  32.6 & 37.2 & 50.1 &  - & 30.2 \\ 
\bottomrule
\end{tabular}
\caption{\textbf{Closed-Book Generative QA} performance of Encoder-Decoder \texttt{LM} on Single- and Multi-hop Dataset. }
\label{tab:close}
\end{table*}


% In this section, we evaluate our method for \textit{Closed-Book} settings


% Most existing ODQA systems follow two pipelines: 1) a closed-book setup, which assume the model's parameter stores the required knowledge, with which the model could directly conduct reasoning and generate the answer; 2) an open-book setup, which resort to retrieving passages in a large text corpus that are likely to contain missing knowledge, and then use a separate Language Model to re-encode question and retrieved passage and predict answer.




%\subsection{Implementation Details}

\noindent The proposed \texttt{KIL} layers can be pugged into most Transformer-based Language Models without hurting its original structure. In this paper, we experiment with both encoder-based \texttt{LM}, i.e. RoBERTa-base ($d=768, l=12$), and encoder-decoder \texttt{LM}, i.e. T5-base ($d=768, l=12$) and T5-large ($d=1024, l=24$). For all \texttt{LM}s, add 1 \texttt{KIL} layer or 2 \texttt{KIL} layers to the encoder layers. 
The statistics of $\mathcal{KG}$ are shown in Table~\ref{tab:stat}. Altogether, it takes about 0.67B parameter for $\mathcal{KG}$ memory, which is affordable to load as model parameter. We pre-train all \texttt{LM}s using the combination of $\mathcal{L}_{SSM}$, $\mathcal{L}_{ent}$ and $\mathcal{L}_{rel}$
for 200k steps on 8 V100 GPUs, with a batch size of 128 and default optimizer and learning rate in the original paper, taking approximately one week to finish pre-training of T5-large model, and 1-2 days for base model. Implementation details are elaborated in Appendix~\ref{sec:implement}.


% \YX{What is $N$ (number of transformer layers between KIL)? It is an important parameter}


% \paragraph{Fine-Tuning} In closed book setting, for encoder-based \texttt{LM} (i.e. RoBERTa), we follow the same procedure in~\cite{DBLP:conf/naacl/VergaSSC21}. Specifically, we added a [\texttt{MASK}] token after the question, and use its output embedding to predict entity ID in WikiData KB. This restricts the model so that it could only handle wiki-answerable questions. For encoder-decoder \texttt{LM} (i.e. T5), we could directly fine-tune to generate the answer. We mainly adopt the hyperparameters and setting in~\cite{DBLP:conf/emnlp/RobertsRS20}.

% 




\subsection{Evaluate for \textit{Closed-Book} QA}
\noindent \method is designed for improving \textit{Closed-Book} QA, so we first evaluate it in this setting. 
\paragraph{Generative QA Task}
Following the hyperparameters and setting in~\cite{DBLP:conf/emnlp/RobertsRS20}, we directly fine-tune the T5-base and T5-large augmented by our \method on the three single-hop ODQA datasets: Natural Question \textbf{(NQ)}~\citep{DBLP:journals/tacl/KwiatkowskiPRCP19}, WebQuestions \textbf{(WQ)}~\citep{DBLP:conf/emnlp/BerantCFL13} and TriviaQA \textbf{(TQA)}~\citep{DBLP:conf/acl/JoshiCWZ17}. 
To test \method's ability to solve complex questions, we also evaluate on two multi-hop QA datasets, i.e. \textbf{Complex WQ}~\citep{DBLP:conf/naacl/TalmorB18} and \textbf{HotpotQA}~\citep{DBLP:conf/emnlp/Yang0ZBCSM18}. Detailed dataset statistics and experimental setups are in Appendix~\ref{sec:exp_detail}.



Experimental results are shown in Table~\ref{tab:close}. We use Exact Match accuracy as the metric for all the datasets. On the three single-hop ODQA datasets, \method with 2 \texttt{KIL} blocks achieves 3.3 absolute accuracy improvement to T5-base, and 3.4 improvement to T5-large. Compared with T5 model with more model parameters (e.g., T5-3B and T5-11B), our T5-large augmented by \method could outperform T5-3B on NQ and WQ datasets. In addition, \method could use the generated reasoning path to interpret the model's prediction. We show examples in Table~\ref{tab:explain1} in Appendix.

For the two multi-hop QA datasets, the performance improvement brought by \method is more significant, i.e., 7.8 to T5-base and 8.2 to T5-large. Notably, by comparing the T5-3B and T5-11B's performance on HotpotQA (we take results from~\citep{DBLP:journals/corr/abs-2204-04581}), T5-large augmented by \method achieves 1.2 higher than T5-11B. This shows that \method is indeed very effective for improving \textit{Closed-Book} QA performance, especially for complex questions.

\paragraph{Entity Prediction Task}
Encoder-based \texttt{LM} (i.e. RoBERTa) in most cases cannot be directly used for \textit{Closed-Book} QA, but more serve as reader to extract answer span. 
However, \citet{DBLP:conf/naacl/VergaSSC21} propose a special evaluation setting as \textit{Closed-Book Entity Prediction}. They add a single [\texttt{MASK}] token after the question, and use its output embedding to classify WikiData entity ID. This restricts that answers must be entities that are covered by WikiData, which they call \textit{WikiData-Answerable} questions. We follow \citet{DBLP:conf/naacl/VergaSSC21} to use such reduced version of WebQuestionsSP \textbf{(WQ-SP)}~\citep{DBLP:conf/acl/YihCHG15} and TriviaQA \textbf{(TQA)} as evaluation dataset, and finetune the RoBERTa (base) model augmented by \method to classify entity ID. We mainly compare \method with EaE~\citep{DBLP:journals/corr/abs-2004-07202} and FILM~\citep{DBLP:conf/naacl/VergaSSC21}, which are two $\mathcal{KG}$ memory augmented \texttt{LM}. We also run experiments on KEPLER~\citep{DBLP:journals/corr/abs-1911-06136}, a RoBERTa model pre-trained with knowledge augmented task.
% \looseness=-1

Experimental results are shown in Table~\ref{tab:roberta}. Similar to the observation reported by \citet{DBLP:conf/naacl/VergaSSC21}, adding $\mathcal{KG}$ memory for this entity prediction task could significantly improve over vanilla $\texttt{LM}$, as most of the factual knowledge required to predict entities are stored in $\mathcal{KG}$. By comparing with FILM~\citep{DBLP:conf/naacl/VergaSSC21}, which is the state-of-the-art model in this setup, \method with reasoning step ($T=2$) outperforms FILM by 2.9, with smaller memory consumption. 





\subsection{Analyze $\mathcal{KG}$ Reasoning Module}
\noindent In our previous studies, we find that using a higher reasoning step, i.e. $T=2$, generally performs better than $T=1$. We hypothesize that the $\mathcal{KG}$ we use has many missing one-hop facts, and high-order reasoning helps recover them and empowers the model to answer related questions.
To test whether \method indeed can infer missing facts, we use \textbf{EntityQuestions (EQ)}~\citep{DBLP:conf/emnlp/SciavolinoZLC21}, which is a synthetic dataset by mapping each WikiData triplet to natural questions. We take RoBERTa-base model augmented by \method trained on NQ as entity predictor and directly test its transfer performance on EQ dataset without further fine-tuning. 

% As the relation edge is within $\mathcal{KG}$, it should not be a hard task as long as \method could predict the correct relation label.



\begin{figure*}[ht!]
\hspace{-0.1in}
    \centering \includegraphics[width=1.02\columnwidth]{pictures/rule.png}
    % \vspace{-10pt}
        \caption{\textbf{Testing the reasoning capacity of \method to infer missing relations}. On the \textbf{left}, the barplot shows the transfer performance on EQ before and after removing relation edges, \method ($T=2$) is less influenced. On the \textbf{right} shows reasoning paths (rules) automatically generated by \method for each missing relation.}
    \label{fig:analysis}
\end{figure*}




To test whether \method could recover missing relation,
we mask \textbf{all} the edges corresponding to each relation separately and make the prediction again. The average results before and after removing edges are shown on the left part of Figure~\ref{fig:analysis}. 
% Before removing the edges, both $T=1$ and $T=2$ achieve very high accuracy even without fine-tuning, as they just need to identify the correct edge from the $\mathcal{KG}$. 
When we remove all the edges to each relation, \method with $T=1$ drops significantly, while $T=2$ could still have good accuracy. To understand why \method ($T=2$) is less influenced, in the right part of Figure~\ref{fig:analysis}, we generate a reasoning path for each relation by averaging the predicted probability score at each reasoning step and pick the relation with the top score. For example, to predict the ``Capital'' of a country, the model learns to find the living place of the president, or the location of a country's central bank. Both are very reasonable guesses. Many previous works~\citep{DBLP:conf/emnlp/XiongHW17} could also learn such rules in an ad-hoc manner and require costly searching or reinforcement learning. In contrast, \method could learn such reasoning capacity for all relations end-to-end during pre-training.

\paragraph{Ablation Studies} We conduct several ablation studies to evaluate which model design indeed contributes to the model. As shown in the bottom blocks in Table~\ref{tab:roberta}, we first remove the $\mathcal{KG}$ reasoning component and provide RoBERTa base model via concatenated KB triplets and train such a model using $\mathcal{L}_{SSM}$ over the same WikiDataset. Such a model's results are close to the KEPLER results but much lower than other models with explicit knowledge memory. We further investigate the role of pre-training tasks. Without pre-training, the \method only performs slightly better than RoBERTa baseline, due to the cold-start problem of entity and relation embedding. We further show that removing $\mathcal{L}_{ent}$ and $\mathcal{L}_{ent}$ could significantly influence final performance. The current combination is the best choice to train \method to reason.



\subsection{Evaluate for \textit{Open-Book} QA}
\noindent Though \method is designed for  \textit{Closed-Book} QA, the learned model can serve as backbone for \textit{Open-Book} QA. We take DPR and FiD models as baseline. For DPR retriever, we replace the question encoder to RoBERTa + \method, fixing the passage embedding and only finetune on each downstream QA dataset. For FiD model, we replace the T5 + \method. We also changed the retriever with our tuned DPR. Results in Table~\ref{tab:open} show that by augmenting both retriever and generator, \method improves a strong baseline like FiD, for about 3.1\% for Base and 1.8\% for Large, and it outperforms the very recent KG-FiD model for 1.6\% in base setting, and achieve comparative performance in a large setting. Note that though our results is still lower than some recent models (e.g., EMDR$^2$), these methods are dedicated architecture or training framework for \textit{Open-Book} QA. We may integrate \method with these models to further improve their performance. 
%while 
%our \method just use as a backbone 
%augmented \texttt{LM} could be integrated into these work and bring further improvement.
% \looseness=-1






% Open book QA models mostly follow retriever-reader pipelines, which retrieve relevant passages from text corpus and the re-encode the passage with question and predict answer via a separate reader. Instead, Closed book QA models mainly utilize its own model parameter to predict answer without an extra step of retrieval and re-encoding. Right now open book QA models have better performance but suffers from higher computational costs.
% Our \method by design could be served as a closed-book QA model, and we also show that it could serve as component for retriever and reader to benefit open-book QA performance.



% To conduct fair comparison with existing baselines,
% we use the uncased BERT pre-trained models as the base encoder. We consider both BERT base (d=768, l=12), abd large (d=1024, l=24) encoders for HotpotQA, RoBERTa-base for StrategyQA, and BERT-base for Natural Questions and TriviaQA. We set the dimension of relation embedding, heads number and dimension for key phrase extractor to be the same as the configuration of BERT-base. We set the number of relations $K$ as 20 by default.

% We use ground-truth entities to train the retrieval module in \method. HotpotQA, StrategyQA and Natural Questions provide ground-truth entities and paragraphs in the training set, while TriviaQA only provides question-answer pairs. For TriviaQA, we use the highest-ranked passages retrieved by BM25 that contain the answer as the ground-truth. We set batch size as 64. During inference, we explicitly control the reasoning length. For HotpotQA, we set it to be 3, conducting at most 3-hop reasoning. For StrategyQA, Natural Questions and TriviaQA, we set it to be 2. The length is one hop larger than the provided golden entities to enhance tolerance for retrieving correct entities.

% To train \method, We follow the previous works and use the AdamW optimizer with a weight-decay coefficient of 0.01 and a linear learning rate warm-up (first 10$\%$ epochs) followed by linear decay. We use a peak learning rate of $10^{-4}$ for the Wikipedia hyperlink prediction task, and $10^{-5}$ for fine-tuning on QA datasets. 
% % We stop training until the validation loss does not decrease for 5 epochs. 









%show how our \method framework can help interpret QA decisions.


% \subsection{Experiment Setup}
% We first introduce the datasets and corpus:
% \paragraph{HotpotQA} \cite{DBLP:conf/emnlp/Yang0ZBCSM18} is a multi-hop QA dataset. There are two evaluation settings. In the \textit{distractor setting}, 10 candidate paragraphs are provided for each question, of which there are two golden paragraphs. In the \textit{full-wiki setting}, a model is required to extract paragraphs from the entire Wikipedia. We report standard token-level F1 score and Exact Match (EM) of answer (ANS) and supporting fact extraction (SP) for HotpotQA.

% \paragraph{Natural Questions}\cite{DBLP:journals/tacl/KwiatkowskiPRCP19} contains questions from Google search queries, and the answers are text spans in Wikipedia. We report short answer Exact Match (EM) performance.

% \paragraph{TriviaQA} \cite{DBLP:conf/acl/JoshiCWZ17} contains trivia questions and answers are text spans from the Web. We report Exact Match (EM) performance.






\begin{table}[t]
\centering
\scriptsize
% \setlength{\tabcolsep}{1mm}{\scalebox{0.72}{
\begin{tabular}{lc|cc} \toprule
\textbf{Models}   & \textbf{\#param (B)}  & \textbf{WQ-SP} & \textbf{TQA}   \\ \midrule
 EaE~\citep{DBLP:journals/corr/abs-2004-07202} & 0.11 + \underline{0.26} & 62.4 & 24.4  \\
 FILM~\citep{DBLP:conf/naacl/VergaSSC21} & 0.11 + \underline{0.72} & 78.1 & 37.3 \\ 
KEPLER~\citep{DBLP:journals/corr/abs-1911-06136} & 0.12 &  48.3 & 24.1   \\ 
\midrule
RoBERTa (Base) & 0.12 &  43.5 & 21.3   \\ 
% RoBERTa + 2-hop KB & 120M &  43.8 & 21.0 & 15.9  \\ 
  \textbf{+ \method} ($T$=1) & 0.12 + \underline{0.68}  &  80.1 & 39.7   \\ 
  \textbf{+ \method} ($T$=2) & 0.13 + \underline{0.68} &  \textbf{80.9} & \textbf{40.3} \\ \bottomrule
\multicolumn{4}{c}{\textbf{Ablation Studies}}\\
\toprule
    RoBERTa  + Concat KB  + $\mathcal{L}_{SSM}$ & 0.12 &  47.1 & 22.6   \\ \midrule
  \textbf{+ \method} ($T$=2) \textbf{w/o PT} & 0.13 + \underline{0.68} &  46.9 & 22.7 \\  
   \ \ \ \ w. $\mathcal{L}_{SSM}$ & 0.13 + \underline{0.68} &  51.9 & 26.8 \\  
   \ \ \ \ w. $\mathcal{L}_{SSM}$ + $\mathcal{L}_{ent}$  & 0.13 + \underline{0.68} & 68.4 & 35.7 \\
    \bottomrule
\end{tabular}
% }}
\caption{\textbf{Closed-Book Entity Prediction} performance of Encoder \texttt{LM} on WikiData-Answerable Dataset. }
\label{tab:roberta}
\end{table}





\begin{table}[t]
\centering
\scriptsize
% \scriptsize
% \hspace{-.2in}
% \setlength{\tabcolsep}{0.7mm}{\scalebox{0.72}{
\begin{tabular}{lc|cccc} \toprule
\textbf{Models}    & \textbf{\#param (B)}  & \textbf{NQ} & \textbf{TQA} \\ \midrule
Graph-Retriever~\cite{DBLP:journals/corr/abs-1911-03868}    & 0.11 & 34.7 & 55.8 \\
% Path-Retriever~\cite{DBLP:conf/iclr/AsaiHHSX20}     & 0.44 & 31.7 &  -   \\
REALM~\cite{DBLP:journals/corr/abs-2002-08909}              & 0.33 + \underline{16} & 40.4 &  -   \\ \midrule
DPR~\cite{DBLP:journals/corr/abs-2004-04906} + BERT & 0.56 + \underline{16} & 41.5 & 56.8 \\ 
 \textbf{+ \method} (DPR,\ $T$=2) & 0.57 + \underline{17}  &  43.7 & 58.5 \\ \midrule
% & RAG~\citep{DBLP:conf/nips/LewisPPPKGKLYR020}                & 626M & 44.5 & 56.1 \\ 
% & Joint Top-K~\citep{DBLP:conf/nips/SachanRHDY21}       & 440M & 49.2 & 64.8 \\ 
FiD (Base) = DPR + T5 (Base)           & 0.44 + \underline{16} & 48.2 & 65.0 \\
 \textbf{+ \method} (T5,\ $T$=2) & 0.45 + \underline{17}  &  49.3 & 67.1 \\ 
 \textbf{+ \method} (DPR \& T5,\ $T$=2) & 0.46 + \underline{17}  &  51.1 & 68.4 \\ \midrule
FiD (Large) = DPR + T5 (Large)          & 0.99 + \underline{16} & 51.4 & 67.6 \\
 \textbf{+ \method} (T5,\ $T$=2) & 0.99 + \underline{17}  &  52.4 & 68.9 \\ 
  \textbf{+ \method} (DPR \& T5,\ $T$=2) & 1.00 + \underline{17}  &  \textbf{53.2} & 69.5 \\ \midrule
 KG-FiD (Base)~\citep{DBLP:conf/acl/Yu0F0WXRY022}      & 0.44 + \underline{16}  & 49.6 & 66.7 \\ 
 KG-FiD (Large)~\citep{DBLP:conf/acl/Yu0F0WXRY022}      & 0.99 + \underline{16}  & \textbf{53.2} & 69.8 \\
 EMDR$^2$~\citep{DBLP:conf/nips/SachanRHDY21}      & 0.44 + \underline{16}  & 52.5 & \textbf{71.4} \\ \bottomrule
\end{tabular}
% }}
\caption{\textbf{Open-Book QA} Evaluation.}
\label{tab:open}
\end{table}



% BART-Large & 0.39B & 36.7 & 30.6 & - & -\\
% T5-11B & 11B & 61.0 & - & - & -\\


