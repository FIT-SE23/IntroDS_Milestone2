% This must be in the first 5 lines to tell arXiv to use pdfLaTeX, which is strongly recommended.
\pdfoutput=1
% In particular, the hyperref package requires pdfLaTeX in order to break URLs across lines.

\documentclass[11pt]{article}
\usepackage{authblk}


% Remove the "review" option to generate the final version.
%\usepackage[review]{acl}
\usepackage[]{acl}
\usepackage{times,latexsym}
\usepackage{url}
\usepackage[T1]{fontenc}

%% Our packages
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{tabularx}
\usepackage{multicol}
\usepackage{multirow}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{inconsolata}
\usepackage{tikz}
\usepackage{tcolorbox}
\usepackage{enumitem}
\usepackage{booktabs}

\usepackage{xspace,mfirstuc,tabulary}
%% Our packages

\DeclareMathOperator*{\argmax}{\textbf{argmax}}
\DeclareMathOperator*{\argmin}{\textbf{argmin}}

% This assumes your files are encoded as UTF8
\usepackage[utf8]{inputenc}

% This is not strictly necessary, and may be commented out,
% but it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}


% If the title and author information does not fit in the area allocated, uncomment the following
%
%\setlength\titlebox{<dim>}
%
% and set <dim> to something 5cm or larger.

%% Our macro
\newcounter{notecounter}
\newcommand{\enotesoff}{\long\gdef\enote##1##2{}}
\newcommand{\enoteson}{\long\gdef\enote##1##2{{
			\stepcounter{notecounter}
			{\large\textbf{ \hspace{1cm}\arabic{notecounter} $<<<$ ##1: ##2 $>>>$\hspace{1cm}}}}}}
% \enoteson
\enotesoff



\newcommand{\smallpm}{\scriptstyle\pm}
\newcommand{\tinypm}{\scriptscriptstyle\pm}

\newtcbox{\inlinepattern}{on line,colback=c0_new!10,colframe=white,size=fbox,arc=3pt, box align=base,before upper=\strut,
	top=-4pt, bottom=-4pt, boxrule=0pt}
\newtcbox{\pattern}{on line,colback=c0_new!10,colframe=white,size=fbox,arc=3pt, box align=base,before upper=\strut,
	top=-2pt, bottom=-2pt, boxrule=0pt}
\newtcolorbox{multipattern}{on line,colback=c0_new!10,colframe=white,size=fbox,arc=3pt, box align=base, top=-2pt, bottom=0pt, boxrule=0pt, before=\adjustbox{valign=c}\bgroup, after=\egroup, before upper=\strut}

\definecolor{c0}{cmyk}{1,0.3968,0,0.2588}
\definecolor{c0_new}{cmyk}{0.38,0.0,0.28,0.15}
\definecolor{c1}{cmyk}{0,0.6175,0.8848,0.1490} 
\definecolor{c2}{cmyk}{0.1127,0.6690,0,0.4431} 
\definecolor{c3}{cmyk}{0.6765,0.2017,0,0.0667} 
\definecolor{c4}{cmyk}{0.3081,0,0.7209,0.3255} 
\definecolor{c5}{cmyk}{0,0.8765,0.7099,0.3647} 
\definecolor{cwhite}{cmyk}{0,0,0,0}
\definecolor{darkgrey}{RGB}{180,180,180}
\definecolor{decentgrey}{RGB}{220,220,220}
\usetikzlibrary{calc,fit,positioning,arrows,intersections}
\newcommand\mask{\textit{MASK}}

\newcommand\ourmethod{MEAL\xspace}

\title{\ourmethod: Stable and Active Learning for Few-Shot Prompting}

\author[*$\diamond$]{Abdullatif KÃ¶ksal}
\author[$\dag$]{Timo Schick}
\author[*$\diamond$]{Hinrich Sch\"utze}


\affil[*]{Center for Information and Language Processing, LMU Munich}
\affil[$\diamond$]{Munich Center of Machine Learning}
\affil[$\dag$]{Meta AI Research \protect\\
	\texttt{akoksal@cis.lmu.de}}


\begin{document}
\maketitle

\begin{abstract}
	Few-shot classification in NLP has recently made great
	strides due to the availability of large foundation models
	that, through priming and prompting, are highly effective
	few-shot learners. However, this approach has high variance
	across different sets of few shots and across different
	finetuning runs. For example, we find that validation
	accuracy on RTE can vary by as much as 27 points.  In this
	context, we make two contributions for more effective
	few-shot learning. First, we propose novel ensembling
	methods and show that they substantially reduce variance.
	Second, since performance depends a lot on the set of few
	shots selected, active learning is promising for few-shot
	classification. Based on our stable ensembling method, we
	build on existing work on active learning  and
	introduce a new criterion: inter-prompt uncertainty 
	sampling with diversity. We present the
	first active learning based approach to select
	training examples for prompt-based learning and show
	that it outperforms prior work on active learning. Finally,
	we show that our combined method, \ourmethod
	(\textbf{M}ultiprompt finetuning and prediction
	\textbf{E}nsembling with \textbf{A}ctive \textbf{L}earning), 
	improves overall performance of prompt-based finetuning
	by 2.3 absolute points on five different tasks.
\end{abstract}



\input{intro}
\input{related}	
\input{multi}
\input{stability}
\input{active}
\input{exps}		
\input{cl}	

% Entries for the entire Anthology, followed by custom entries
\bibliography{anthology,custom}
\bibliographystyle{acl_natbib}

\appendix

\section{Appendix}
\label{sec:appendix}
\subsection{Implementation Details}
We implement our contributions on top of the PET library \footnote{\href{https://github.com/timoschick/pet}{https://github.com/timoschick/pet}}. For prompt-based finetuning with single and multiple prompts, we use $10$ epochs, $1e-5$ learning rate, and also the default parameters for other hyperparameters. For a longer training approach, we use $50$ epochs with a $1e-6$ learning rate and warmup with a ratio of $0.1$. We report results with five different runs on five different training data. 

We conduct our experiments with NVIDIA GeForce GTX 1080 Ti, and one run of prompt-based finetuning with a single prompt takes approximately 20 minutes while one run of prompt-based finetuning with a multiprompt approach takes approximately 18 minutes for 32 examples, 4 prompts, ALBERT-xxlarge-v2 model with 223M parameters. As described in the Section \ref{sec:active_learning}, we select training sets with $16*L$ samples where $L$ is the number of labels. Therefore, RTE, SST-2, and MRPC have 32 training samples, SST-5 has 80 training samples, and TREC has 96 training samples. RTE, SST-2, SST-5, TREC, and MRPC contain 277, 872, 1101, 500, and 408 validation examples respectively.

\end{document}
