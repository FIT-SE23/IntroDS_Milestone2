
\section{Multiprompt-based Finetuning}
\label{sec:multiprompt}
Let $M$ be a masked language model, $T$ its
vocabulary, and $\textit{MASK} \in T$ the mask token. We use
Pattern-Exploiting Pattern (PET)
\cite{schick-schutze-2021-exploiting}
for our prompt-based finetuning experiments on few-shot  classification, but without knowledge distillation and unlabeled data. Patterns ($\mathcal{P}$) transform a given input $x$ into cloze-style phrase $x_p$, containing a single mask. Verbalizers ($V$) convert each label $l\in L$ into a single token $s_l \in T$ representing the task-specific meaning of the output label. 

Our prediction for a label is its probability,
according to the language model,
as a substitution for the mask:
\begin{equation}
\label{eq:pet}
P(y|x) = \frac{\exp
s_m(V(y)|x_p)}{\sum_{y^*\in L}\exp  s_m(V(y^{*})|x_p)}
\end{equation}
where $s_m$ gives the raw score of $V(y)$ from a pretrained language model $M$ for the \textit{MASK} position in the cloze-style phrase of the input.

Using the cross-entropy loss of $P$, PET trains a separate
model for each prompt (i.e., single prompt finetuning). In inference, it ensembles model predictions by logit averaging. 

We propose \textbf{multiprompt-based finetuning} with a
single model M that is trained with all prompts for a given
task. During inference time, we also use ensembling with
logit averaging for each prompt. However, our approach generates 
a single finetuned model regardless of the number of
prompts. Compared to PET, this reduces runtime, memory, and
overall complexity. Our results
suggest that multiprompt training is better than or
comparable to single-prompt training.
See also \cite{schick-schutze-2021-shot} where 
similar behavior was observed for text
generation.
