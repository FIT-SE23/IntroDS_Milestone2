\section{Stability of Few-Shot Classification}

\begin{figure*}
\centering
\includegraphics[width=\linewidth]{Figures/loss_surface_form_equal.pdf}
\caption{Loss and validation accuracy surface visualizations
  for two RTE runs; the training set is the same for the two runs.
Left (training loss):
The two models
$\theta_s$ and $\theta_f$
have similar loss
-- they are both located in the upper right blue zero-loss triangle.
Right (validation accuracy):
The successful model $\theta_s$ performs much better than the failed model
$\theta_f$.
}
\label{fig:loss_surface_form}
\end{figure*}

\label{sec:stability}
In few-shot classification, finetuning PLMs such as
ALBERT \cite{Lan2020ALBERT} with  an MLM objective on samples converted into cloze-style phrases \cite{schick-schutze-2021-just} 
achieves comparable performance to prompting with much larger models like GPT-3
\cite{gpt3}. 
Just as prompting methods are sensitive to data order
\cite{lu-etal-2022-fantastically} and label distributions
\cite{pmlr-v139-zhao21c}, finetuning PLMs also exhibits
sensitivity and instability as shown by
\citet{DBLP:journals/corr/abs-2002-06305} for a fully supervised setting.

We show that the instability of finetuning PLMs also exists
in few-shot prompt finetuning. Even though prompt finetuning
does not introduce new parameters like classifier heads as
in fully supervised classification, there is variance from
dropout and training data order. We conduct experiments with
multiprompt-based finetuning with default PET
\cite{schick-schutze-2021-exploiting} settings without
knowledge distillation. In Figure \ref{fig:instability}, we
show that runs with different random seeds for the same
training set can vary by as much as 13
points.

\citet{DBLP:conf/iclr/MosbachAK21} suggest that longer
training with a low learning rate and warmup improves the
stability of finetuning PLMs.  Their main motivation is to
avoid models ending up in suboptimal training loss
regions. However, this is not valid in few-shot prompt
tuning as the number of training examples is low, and
finetuning achieves almost zero training loss quickly. Our
initial experiments show that longer training does reduce
the standard deviation between different runs, but that it
also causes lower accuracy for most tasks. For example, it
reduces the average standard deviation by more than 70\%
for both single and multi-prompt finetuning while causing
many datasets to have lower mean accuracy by up to 7.3 points
as shown in Table \ref{tab:stability_results}.

In Figure \ref{fig:loss_surface_form},
we analyze this instability by creating a training
loss and validation accuracy surface visualization of two
RTE runs with \emph{the same training set} and multiprompt-based
finetuning. The failed model
$\theta_f$ (red)
achieves 58.5\% validation
accuracy while the successful model
$\theta_s$ (green)
achieves 71.5\%. The two
models only differ by random seed of finetuning. The figure
illustrates the training loss and validation accuracy
surfaces for combinations of the model weights of the
pretrained model ($\theta_p$), the failed model
($\theta_f$), and the successful model ($\theta_s$). We create a two-dimensional space based on $f(a, b) = F(\theta_p + a\delta_f + b\delta_s)$, where $\delta_f=\theta_f-\theta_p$, $\delta_s=\theta_s-\theta_p$, and $F$ is the loss or accuracy function, depending on the graph. We use 16 values for both $a$ and $b$ to plot training loss and validation accuracy surface forms.

Figure \ref{fig:loss_surface_form} shows that there
is a large region with $\leq1e-4$ training loss (left
graph, dark blue) that includes
$\theta_f$ and $\theta_s$. However, most of this region is
suboptimal in terms of validation accuracy (right
graph). This indicates that our instability problem differs
from fully supervised finetuning where large learning
rates often result in suboptimal training loss while we observe almost 0 training loss for each run including failed ones. Therefore, longer training with a low learning rate and warmup only makes finetuned models end up in a similar region with lower variance, but it causes suboptimal validation accuracy scores, as further analysis in Section \ref{sec:res_instability} suggests.

To overcome the instability issue, we propose
two ensemble models: we ensemble the logits of
different runs in \textbf{ENSEMBLE}\textsubscript{Prediction}, and we take the average of parameters of different runs in \textbf{ENSEMBLE}\textsubscript{Parameter}. We will show that these reduce the effect of failed runs while achieving a higher accuracy score than the mean of different runs for five tasks. Our experiments show that these ensembles yields better or comparable performance than average accuracy both for single and multiprompt finetuning. The final prediction of \textbf{ENSEMBLE}\textsubscript{Prediction} for input $x$ is:
\[P(y|x) = \sigma(\sum\limits_{r=1}^{R}[\sum\limits_{p\in\mathcal{P}}F_{r}(y|x_{p})]/(R*|\mathcal{P}|))\]
where $\sigma$ is softmax, R is the number of runs, $\mathcal{P}$ is the
set of prompts, and $F_{r}$ gives,
for the finetuned model in run $r$,
the logit of each class for the input $x$ with  prompt
$p$.

Following  recent work on averaging of deep neural
networks \cite{izmailov2018averaging},
we  average  each parameter of the
finetuned language models across runs, resulting in a single
averaged model.
The prediction of
\textbf{ENSEMBLE}\textsubscript{Parameter}
for input $x$ is the prediction of this single model.
