\section{Introduction}
Pretrained language models (PLMs) are effective
few-shot learners in NLP when conditioned with
few-shot examples in the input  \cite[i.a.]{gpt3, sewon-etal-2022-rethinking}
or finetuned with a masked language modeling objective on samples converted into cloze-style phrases
\cite{schick-schutze-2021-exploiting, gao-etal-2021-making}. 
Prompt-based finetuning is especially promising  as it
enables researchers to train relatively small models as few-shot classifiers that can make accurate predictions
with a minimal investment of time and effort.


\begin{figure}
	\centering
	\includegraphics[width=\linewidth]{Figures/instability.pdf}
	\caption{There is large variance in performance
		of prompt-based finetuning depending on training set
		and randomly initialized run.
		Multiprompt results with 32 examples for ALBERT 
		on
		RTE and MRPC. Variance can be up to 23.5 points with different runs (RTE \#3) and 13.7 points with different training sets (RTE \#1 vs \#4).}
	\label{fig:instability}
\end{figure}


However, prompt-based finetuning can suffer from high variance, for which we observe two sources:
\textit{training data selection} and \textit{run variability}. In
Figure \ref{fig:instability}, we illustrate
this for
five equally-sized training sets and 20 runs for
RTE \cite{rte} and MRPC \cite{mrpc}.
Our experiments suggest that finetuning with different seeds or training sets causes significant variance as shown in the figure.
Both instability issues are big problems, especially
in practical use cases of few-shot
classification where there is a limited
budget to annotate training data without a
validation set. Therefore, annotating informative
and diverse samples for training is important for
getting good performance on a reasonable budget.
Additionally, consistent results are desirable when finetuning on the same training data with different random seeds when there is no validation data to verify which run performs best. In this work, we thus investigate how to decrease the run variance, select better training examples, and improve overall performance and stability of few-shot classification.


To overcome run instability, we first analyze existing
solutions in the literature on finetuning pretrained
language models. Recent works
\cite{DBLP:conf/iclr/MosbachAK21,
  finetuning_instability_dodge_2020} show that longer
finetuning with a lower learning rate and warmup can improve
the stability of finetuning PLMs. However, our findings
suggest that this approach does not apply to few-shot
prompt-based classification, and we therefore propose run
ensemble techniques via prediction logits and model weights 
to stabilize finetuning for different runs.

For training set selection,
we propose a novel active learning algorithm that we refer to as
\emph{inter-prompt uncertainty sampling with diversity}.
In general,
we are, to the best of our knowledge, the first to develop active learning algorithms specifically tailored to
prompt-based finetuning. To do this, we build on
uncertainty-based sampling
such as entropy \cite{entropy} and lowest
confidence \cite{lowest_confidence}, and combinations of
uncertainty and diversity-based sampling such as Contrastive
Active Learning \cite{cal} and BADGE \cite{badge}.


We combine \textbf{M}ultiprompt finetuning and prediction \textbf{E}nsembling with \textbf{A}ctive \textbf{L}earning and propose \ourmethod where we improve overall performance of prompt-based finetuning by 2.3 absolute points on five different tasks. We summarize our contributions as follows:
\begin{enumerate}
\item We propose a training procedure that produces a single
  few-shot  classification model with multiple prompts on
  top of the PET \cite{schick-schutze-2021-exploiting}
  architecture. This reduces  model space complexity and improves overall performance.
\item We demonstrate that  run instability is a serious problem in few-shot  classification and conduct an exhaustive analysis of why existing solutions do not apply to few-shot prompt-based finetuning. We propose ensemble techniques to improve run stability.
\item We compare a set of active learning algorithms and
  propose a novel selection strategy:
  inter-prompt uncertainty sampling with diversity. 
  We show that it outperforms prior active learning algorithms and random selection. 
Our work is the first to demonstrate that active learning is
beneficial in prompt-based learning.
\end{enumerate}