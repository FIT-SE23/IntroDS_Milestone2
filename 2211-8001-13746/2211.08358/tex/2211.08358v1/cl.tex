	
	\section{Conclusion}
	\label{sec:conclusion}
We demonstrate two problems with stability in few-shot
classification with prompt-based finetuning -- instability due
to finetuning proper and due to training set selection --
and show that
existing solutions for instability fail.
To address instability,
we first
propose finetuning a single model with multiple prompts;
this results in
better performance and less model space complexity than
finetuning several models with single prompts.
We then propose run ensemble techniques that improve stability
and overall performance.

Our more stable setup allows us
to explore active learning for prompt-based finetuning. 
We compare a set of active
learning algorithms to reduce training data selection
instability and improve overall performance. Our active
learning algorithm, inter-prompt  uncertainty sampling
with diversity, outperforms prior active learning algorithms
(and random selection) for both ALBERT and
RoBERTa$_\text{LARGE}$.

Finally, we believe that our findings and proposals on
measuring and reducing variance for stable and active
learning for few-shot classification will be useful for the
community by enabling fair comparison and helping
researchers track progress on NLP tasks.
	
	\section*{Limitations}
	We exploit information in Pretrained Language Models (PLMs) to effectively use them in prompt-based finetuning for few-shot classification. Therefore prompt-based few-shot classification might be more open to reflecting biases in pretrained language models. Furthermore, our ENSEMBLE\textsubscript{Prediction} technique increases the model complexity during inference time to provide better performance and stability. However, ENSEMBLE\textsubscript{Parameter} can be an alternative as it also improves performance and stability over the default setting while maintaining model complexity same. Furthermore, this work is only demonstrated for tasks in the English language. 
	
