
\section{Related Work}
\label{sec:related}

\textbf{Few-shot classification with language model
prompting.} GPT-3 \cite{gpt3} performs several few-shot
learning tasks by appending examples to the input during
inference time as conditioning without any parameter
updates. PET \cite{schick-schutze-2021-exploiting,
schick-schutze-2021-just} follows a similar approach
with model finetuning and achieves comparable results to
GPT-3, but needs much less parameters. Several works in this domain such as LM-BFF \cite{gao-etal-2021-making} and ADAPET \cite{tam-etal-2021-improving} make use of PET, usually without knowledge distillation to perform few-shot classification. We also make use of PET without knowledge distillation, and propose multiprompt training on top of it to improve overall performance and reduce inference complexity with a single model instead of as many models as the total number of prompts.


\noindent\textbf{Instability.}
There are two sources of instability
in finetuning language models for few-shot classification:
finetuning itself and data
selection. Finetuning instability comes from
finetuning language models with random seeds, causing
variance in the order of training samples and dropout
layers. \citet{DBLP:conf/iclr/MosbachAK21}
and \citet{finetuning_instability_dodge_2020} show that
finetuning pretrained language models is an unstable process
for fully supervised
training. Recently, \citet{zheng-etal-2022-fewnlu}
demonstrated that few-shot finetuning also exhibits training
instability but their experiments have different training
sets in a cross-validation scenario. They do not address
how much instability comes from training vs.\ data
selection. Our findings suggest that the instability issue
exists in few-shot training \emph{for the same training set}, and
existing solutions for fully supervised settings (e.g.,
\citet{finetuning_instability_dodge_2020}, \citet{DBLP:conf/iclr/MosbachAK21})
do not stabilize finetuning for few-shot classification. We
propose the run ensemble method to improve the stability of
few-shot classification. The second type of
instability is training data selection;
we target this issue with active learning.

\noindent\textbf{Active Learning.} As collecting labeled
data is time-consuming and costly, active learning has
been a crucial part of supervised learning \cite{cohn1996active, settles2009active,
	 rotman2022multi}. Apart
from efficiency in data labeling, we show that some training
sets have significantly worse performance than others for
few-shot  classification. 
\citet{zhao-etal-2021-closer} also give evidence that there is a high
degree of sensitivity to the selection of few shots in
few shot learning. Hence, we
follow an active learning setup to create \textit{informative}
and \textit{diverse} training sets for few-shot 
classification. \citet{schroder-etal-2022-revisiting}
recently showed that uncertainty active learning
achieves significant improvements for fully supervised
settings in PLMs. Following this, we modify entropy
\cite{entropy}, breaking ties \cite{breaking_ties}, and
lowest confidence \cite{lowest_confidence} algorithms for
prompt-based finetuning, and propose a KL
divergence algorithm with prompts. Furthermore, we adapt more recent
active learning algorithms that modify uncertainty and
diversity sampling such as Contrastive Active Learning
\cite{cal} and Batch Active learning by Diverse Gradient
Embeddings \cite{badge} for few-shot prompt-based
finetuning, and propose a novel active learning
algorithm.
