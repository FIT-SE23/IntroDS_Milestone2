\section{Experiments and Results}
\label{sec:experiments}
\subsection{Setup}
\label{sec:setup}
We use a diverse set of classification tasks to analyze the
stability of prompt tuning, compare single to
multiprompt-based finetuning, and evaluate active learning
algorithms. For each dataset, we use four prompts, described
in detail in \S\ref{sec:datasets}. We report results on the
validation set as we conducted all experiments without 
hyperparameter tuning by assuming a realistic few-shot
scenario in which no dev set is available for tuning.\footnote{Our initial experiments
	show the same patterns for all
	experiments with similar performance gains for the test set.}
For single prompt- and multiprompt-based finetuning,
we use the default values from PET
\cite{schick-schutze-2021-exploiting}. For longer training,
we compare the default values from PET to adapted settings
from \citet{DBLP:conf/iclr/MosbachAK21} for stability.
The default values of PET have a 1e-5 learning
rate for 10 epochs without warmup while the settings for
stability have a 1e-6 learning rate for 50 epochs with
linear scheduled warmup with a 0.1 ratio.

\begin{table*}
	\centering
	\setlength\tabcolsep{4.75pt}
	\begin{tabular}{lccccccc}
		\toprule
		\textbf{Finetuning} & \textbf{Stability Technique} & \textbf{RTE} & \textbf{SST-2} & \textbf{SST-5} & \textbf{TREC} & \textbf{MRPC} & \textbf{Average} \\
		\cmidrule(lr){1-2} \cmidrule(lr){3-7} \cmidrule(lr){8-8} 
		
		\multirow{4}{*}{Single Prompt} & - (Default) & 63.7$\tinypm$\tiny{1.91} &93.1$\tinypm$\tiny{0.54} &51.6$\tinypm$\tiny{0.73} &81.3$\tinypm$\tiny{1.11} &67.7$\tinypm$\tiny{1.07} &71.5$\tinypm$\tiny{1.07} \\
		& Longer Training &  63.4$\tinypm$\tiny{0.42} &92.1$\tinypm$\tiny{0.10} &50.2$\tinypm$\tiny{0.20} &74.0$\tinypm$\tiny{0.45} &67.0$\tinypm$\tiny{0.20} &69.3$\tinypm$\tiny{0.27} \\
		& ENSEMBLE\textsubscript{Parameter} & 64.0$\tinypm$\tiny{1.35} &93.2$\tinypm$\tiny{0.27} &51.9$\tinypm$\tiny{0.40} &80.6$\tinypm$\tiny{0.56} &67.5$\tinypm$\tiny{0.70} & 71.4$\tinypm$\tiny{0.66}\\
		& ENSEMBLE\textsubscript{Prediction} & 64.0$\tinypm$\tiny{0.95} &93.2$\tinypm$\tiny{0.30} &52.0$\tinypm$\tiny{0.51} &81.9$\tinypm$\tiny{0.72} &67.9$\tinypm$\tiny{0.58} &71.8$\tinypm$\tiny{0.61} \\
		\midrule
		\multirow{4}{*}{Multiprompt} & - (Default) & 64.2$\tinypm$\tiny{4.88} &91.7$\tinypm$\tiny{1.55} &52.1$\tinypm$\tiny{0.92} &82.9$\tinypm$\tiny{1.77} &67.9$\tinypm$\tiny{2.25} &71.8$\tinypm$\tiny{2.28}\\
		& Longer Training & 66.2$\tinypm$\tiny{0.45} &93.0$\tinypm$\tiny{0.08} &50.4$\tinypm$\tiny{0.26} &79.0$\tinypm$\tiny{0.34} &66.8$\tinypm$\tiny{0.36} &71.1$\tinypm$\tiny{0.30} \\
		& ENSEMBLE\textsubscript{Parameter} & 64.4$\tinypm$\tiny{2.53} &92.5$\tinypm$\tiny{0.31} &52.9$\tinypm$\tiny{0.47} &83.7$\tinypm$\tiny{1.00} &68.3$\tinypm$\tiny{1.34} & 72.4$\tinypm$\tiny{1.13}\\
		& ENSEMBLE\textsubscript{Prediction} & 65.4$\tinypm$\tiny{2.86} &92.4$\tinypm$\tiny{0.43} &52.8$\tinypm$\tiny{0.42} &84.4$\tinypm$\tiny{0.87} &68.9$\tinypm$\tiny{1.03} & \textbf{72.8}$\tinypm$\tiny{1.12}\\
		\bottomrule
	\end{tabular}
	\caption{Comparing different stability techniques for prompt-based finetuning with single and multiple prompts with ALBERT  on randomly selected training sets. Multiprompt finetuning improves the overall performance compared to PET. ENSEMBLE\textsubscript{Prediction} improves stability while achieving higher performance for both finetuning techniques.}
	\label{tab:stability_results}
\end{table*}

\subsection{Datasets and Evaluation Metrics}
\label{sec:datasets}
\textbf{RTE} \cite{rte} is a textual entailment dataset that contains text pairs of premise and hypothesis with the objective of detecting entailment and contradiction. For a premise-hypothesis pair ($p$, $h$), we use

\[\pattern{$h$\textsf{\small?$\,|\,$\mask{}, }$p$}, \pattern{\textsf{\small``}$h$\textsf{\small''?$\,|\,$\mask{}, ``}$p$\textsf{\small''}},\]
\[\pattern{$h$\textsf{\small?$\,|\,$\mask{}. }$p$},
\pattern{\textsf{\small``}$h$\textsf{\small''?$\,|\,$\mask{}. ``}$p$\textsf{\small''}} \]


\noindent patterns and a verbalizer \textsf{\small yes} and \textsf{\small no} for entailment and no entailment labels.


\textbf{SST-2} and \textbf{SST-5} \cite{sst} is a sentiment
analysis dataset of movie reviews. SST-2 contains two
classes: positive and negative. SST-5 has five labels;
very positive, positive, neutral, negative, and very
negative. For a given movie review $t$, we use
\[\pattern{$t$ \textsf{\small It was \mask{}.}}, \pattern{$t$ \textsf{\small A \mask{} one.}},\]
\[\pattern{$t$ \textsf{\small The movie is \mask{}.}},
\pattern{$t$ \textsf{\small All in all \mask{}.}}\]
\noindent patterns and verbalizers ``great'', ``terrible''
(SST-2) and  ``great'', ``good'', ``okay'', ``bad'',  ``terrible'' (SST-5).

\textbf{TREC} \cite{trec} is a question classification dataset. We use six coarse classes: abbreviation, entity, description, human, location, numeric value. For a  question $t$, we use
\[ \pattern{\textsf{\small[Question Category: \mask{}] }$t$}\text{,} \pattern{\textsf{\small[Category: \mask{}] }$t$},\]
\[ \pattern{$t$ \textsf{\small This question is related to \mask{} category.}},\]
\[ \pattern{\textsf{\small I'd like to ask a question about \mask{}. } $t$}\]

\noindent patterns and verbalizers ``abbreviated'', ``entity'', ``description'', ``human'', ``location'', ``number''.

\textbf{MRPC} \cite{mrpc} is a paraphrase identification dataset of sentence pairs. For two sentences $(t_1, t_2)$, the task is to decide whether they are semantically equivalent or not. We use the same pattern and verbalizer as for RTE.


We report the average of accuracies, run standard
deviations, and training set standard deviations for a given
dataset and active learning algorithm over five training
sets and five runs for each training set. For AL algorithms
without variance (e.g., entropy and KL), we do not report
training set standard deviation as the algorithm outputs a
single training set. We increase the number of runs
from 5 to 20 for stability experiments and report run variance over
20 runs for default and longer training. For ENSEMBLE techniques,
we report variance over 4 trials where we ensemble 5 different runs
with the given technique.


For active learning, we treat training examples of each dataset as unlabeled
data and pick few-shot samples from there. We report the
average accuracy of five datasets and the average ranking of
active learning algorithms for each dataset following
\citet{schroder-etal-2022-revisiting} with additional
analysis of diversity \cite{zhdanov2019diverse}, representativeness \cite{dor2020active},
and label entropy \cite{prabhu-etal-2019-sampling} as explained in Section \ref{sec:al_additional_analysis}.

\subsection{Stability}
\label{sec:res_instability}
We analyze both single and multiprompt-based finetuning with
the proposed solutions for the stability experiments over five randomly 
selected training sets. We
compare
ENSEMBLE
with the default setup (default hyperparameters given in
\S\ref{sec:setup}) and 
with \citet{DBLP:conf/iclr/MosbachAK21}'s proposal:
longer training with a lower learning rate and warmup
training.

The results
in Table \ref{tab:stability_results}
suggest that \citet{DBLP:conf/iclr/MosbachAK21}'s
longer training approach   reduces the run standard
deviation for each dataset, but causes suboptimal accuracy
results for SST-5, TREC, and MRPC in multiprompt-based
finetuning, and for all datasets in single prompt-based
finetuning. Therefore, using a longer training approach is
not feasible for practical scenarios; for example, even the
worst runs in the default approach achieve better results than every run in the longer training approach for TREC.

Table \ref{tab:stability_results} shows that
ENSEMBLE\textsubscript{Prediction} consistently produces
better results than default settings for each dataset both
for single prompt and multiprompt finetuning. On the other
hand, ENSEMBLE\textsubscript{Parameter} performs better than
default settings only in multiprompt finetuning
consistently, but speeding up the prediction process during
inference time with a single model.  On top of that, both
ENSEMBLE techniques avoid failed runs. For example, the
default approach gets 66.1\% average accuracy for RTE with
one of the five random training sets. However, two runs
result in 61.7\% and 64.3\% validation accuracy, and
ENSEMBLE\textsubscript{Prediction} ensures that we get
better average accuracy (67.9\%) and it is more useful in
practical scenarios as it will not end up with suboptimal
runs. ENSEMBLE\textsubscript{Parameter} is an alternative
approach to increase stability and performance while
providing a single model and lower time complexity during inference
time.


\begin{table*}
	\centering
	\begin{tabular}{lccccc}
		\toprule
		\textbf{Algorithms} & \textbf{RTE} & \textbf{SST-2} & \textbf{SST-5} & \textbf{TREC} & \textbf{MRPC} \\
		\midrule
		Random & 65.3$\pm$5.8 & 92.1$\pm$2.4 & 52.8$\pm$\textbf{0.7} & 83.8$\pm$2.3 & 69.3$\pm$2.7\\
		\midrule
		Entropy & \underline{71.1} & 89.3 & 49.0 & 76.2 & 68.9 \\
		Lowest Confidence & \textbf{\underline{71.8}} & 91.4 & 48.8 & 72.6 & \underline{70.1}\\
		Breaking Ties &\textbf{\underline{71.8}} & 91.4 & 49.9 & 77.2 & \underline{70.1} \\
		KL (Ours) & 59.6 & 89.8 & \textbf{\underline{53.5}} & 77.4 & 65.4 \\
		Contrastive AL \cite{cal} & 56.7 & \underline{92.9} & 49.0 & 81.6 & \textbf{\underline{71.8}}\\
		BADGE \cite{badge} & \underline{68.7}$\pm$8.9 & \textbf{\underline{93.2}}$\pm$\textbf{\underline{0.8}} & 51.2$\pm$2.8 & 82.7$\pm$3.1 & \underline{70.3}$\pm$\textbf{\underline{0.9}}\\

		IPUSD (Ours) & \underline{70.1}$\pm$\textbf{\underline{3.8}} & \underline{92.9}$\pm$\textbf{\underline{0.9}} & 51.4$\pm$1.4 & \textbf{\underline{85.0}}$\pm$\textbf{\underline{2.2}} & \underline{69.8}$\pm$3.3 \\
		
		\bottomrule
	\end{tabular}
	\caption{Comparison of active learning methods. Random,
	  BADGE, IPUSD (inter-prompt uncertainty sampling
                with diversity)
		are non-deterministic. We run these algorithms for five
		different random seeds and
		report average accuracy and standard deviation. Best results are indicated in bold, results better than random are underlined.}
	\label{tab:al_results}
\end{table*}


\subsection{\mbox{Single and Multiprompt-based Finetuning}}
\label{sec:res_single_multi}
In Table \ref{tab:stability_results}, we also compare single
prompt to multiprompt-based finetuning.
For the three stability techniques
(longer training, ENSEMBLE\textsubscript{Parameter}, ENSEMBLE\textsubscript{Prediction}),
multiprompt finetuning achieves better average accuracy than
single prompt tuning for almost all datasets. Also,
multiprompt-based finetuning in the default setup produces a higher overall
accuracy with a higher run standard deviation but ENSEMBLE techniques overcome
this. Thus, multiprompt achieves better overall performance than
single prompt for each stability technique. Furthermore, multiprompt simplifies 
training and deployment in practice because it outputs a single finetuned model,
compared to
one model per prompt for the single prompt method.


\subsection{Active Learning}


We compare our active learning algorithms with a variety of
uncertainty and diversity-based algorithms in
Table \ref{tab:al_results}. To provide more stable results and fair comparison by reducing noise from different runs, we employ \emph{multiprompt finetuning with ENSEMBLE\textsubscript{Prediction}} for each active learning algorithm in this section.
Our results show that all uncertainty-based
algorithms -- entropy, lowest confidence, breaking ties and
KL -- perform worse than random selection in terms of 
accuracy averaged over five datasets.
Our interpretation of this result is that, considering that we are
finetuning a PLM with few examples, finetuning with the
highest uncertainty examples is not enough to generalize for
the task. In contrast,
\citet{schroder-etal-2022-revisiting} found that
uncertainty-based active learning performs
consistently better than random selection for
fully supervised settings in PLMs.



More complex active learning algorithms that combine
uncertainty and diversity sampling such as CAL \cite{cal}
and BADGE \cite{badge} perform better than uncertainty-based
algorithms. Furthermore, BADGE outperforms random selection
on three out of five datasets in terms of average accuracy.
However,
BADGE has higher standard deviation than random selection: its average
over the five datasets is 3.0 compared to an average of 2.5
for random selection.

Finally, our proposed algorithm, IPUSD, performs
better than random and better than other active learning algorithms
with higher average accuracy and lower standard
deviation across different selections on five datasets.
However, we perform a more in-depth analysis of active
learning algorithms to understand their relative
performance better to understand failure cases
like SST-5. We believe that these insights
will lead to improved active learning strategies in future
work.


\subsubsection{Additional Analysis}
\label{sec:al_additional_analysis}
To
provide a more in-depth analysis of AL algorithms,
we compare three aspects of active learning selection:
\emph{diversity}, \emph{representativeness} and \emph{label entropy}.

\textbf{Diversity} \cite{zhdanov2019diverse} aims to identify whether selected training examples are redundant or similar. Following \citet{dor2020active}'s interpretation, we calculate the diversity of selected examples as the reciprocal of the average Euclidean distance between each unlabeled sample and the nearest training example. The representation of each sample is an embedding of the [CLS] token from the corresponding finetuned model with selected training examples. Therefore, a higher score means that training examples are selected from a diverse area by ensuring a smaller average distance to unlabeled example space.

\textbf{Representativeness} \cite{dor2020active} is a metric that refers to the well-known issue of selecting outlier examples in AL, especially in cases of uncertainty-based algorithms. It is calculated as the reciprocal of the average distance between selected training examples and their nearest K (10) neighbors from unlabeled examples. A higher score means that a training example is more likely to be located in a more densely populated region and therefore less likely to be an outlier.

\textbf{Label Entropy} \cite{prabhu-etal-2019-sampling} is the KL divergence of the class distribution difference between unlabeled data and selected training examples. 



\begin{table}[t]
	\small
	\setlength\tabcolsep{3.6pt}
	\centering
	\begin{tabular}{lccccc}
		& {\textbf{Acc. $\uparrow$}} & {\textbf{Rank $\downarrow$}} & {\textbf{Div. $\uparrow$}} & {\textbf{Repr.$\uparrow$}} & {\textbf{Ent.$\downarrow$}} \\
	    \cmidrule(lr){2-3} \cmidrule(lr){4-6}
		Random & 72.6{$\tinypm$\tiny{2.8}} & 4.0 & \textbf{13.6} & \textbf{17.6} & \textbf{2.0} \\
		\midrule
		Entropy & 70.9 & 6.4 & 13.3 & 16.9 & 6.1 \\
		LC & 70.9 & 5.6 & 13.5 & 17.2 & 5.3  \\
		BT & 72.1 & 4.0 & 13.4 &  17.1 & 5.6 \\
		KL & 69.1 & 5.6 & 13.4 & 16.9 & 9.0 \\
		CAL  & 70.4 & 4.4 & 13.1 & 17.1 & 23.5 \\
		\midrule
		BADGE & 73.2$\tinypm$\tiny{3.3} & \textbf{3.0} & \textbf{13.6}  & \textbf{17.6} & 2.2 \\
		\textbf{IPUSD} & \textbf{73.9}$\tinypm$\tiny{\textbf{2.3}} & \textbf{3.0} &13.5 & \textbf{17.6} & \textbf{2.0} \\
		\bottomrule
	\end{tabular}

\caption{Comparing average accuracy, rankings, diversity, representativeness, 
	and label entropy scores for 7 active learning algorithms.
	IPUSD outperforms random and other
	algorithms by providing more balanced selections of three aspects. Uncertainty-based
	active learning algorithms have worse scores for each feature, and thus lower performance.
	Arrows indicate whether higher ($\uparrow$)
	or lower ($\downarrow$) is better.}  \label{tab:al_ranking}
\end{table}


Table \ref{tab:al_ranking} shows that our proposed active
learning algorithm outperforms all other active learning
algorithms with higher average accuracy, better ranking
among all algorithms, and lower standard
deviation across different selections. Inter-prompt
uncertainty sampling with diversity
(IPUSD) gets better performance than random by 1.3\%
(73.9-72.6) average absolute improvement.


Further analysis shows uncertainty-based active learning algorithms usually have higher label entropy values which indicate different class distribution than the unlabeled data distribution in addition to lower representativeness and diversity scores. Random selection provides a strong baseline in terms of three aspects. While BADGE and IPUSD have similar scores for three features, a slight change in label entropy (i.e., selecting a training set with different distribution than unlabeled data) can cause significant changes in overall performance. Therefore, our algorithm, IPUSD, achieves the best balance between diversity, representativeness, and label entropy features while achieving higher performance thanks to the diversity, uncertainty, and randomness aspect of our algorithm. We observe that designing a training data selection strategy for few-shot finetuning is a more challenging task than designing an active learning strategy for the fully-supervised setting as these algorithms can only rely on zero-shot information gathered from PLM. Therefore, we share our insights on the limitations of IPUSD in SST-5 and MRPC datasets in the next section to guide future work in this field.



\subsubsection{IPUSD: Underlying assumptions}
\label{sec:al_limitations}
Table \ref{tab:al_results} shows that IPUSD performs worse than random only on the SST-5 dataset. Our initial analysis with diversity, representativeness, and label entropy features does not show any difference between random and IPUSD on SST-5 but further manual investigation shows that IPUSD selects examples that might belong to multiple classes. As the distinction between classes is not fully clear in this dataset (e.g., very negative and negative), selecting challenging examples as a training set may result in suboptimal finetuned models. To test this, we finetuned a fully-supervised RoBERTA\textsubscript{LARGE} model on a fine-grained sentiment analysis task with YELP \cite{zhang2015character}. This model achieves $47.7\%$ average accuracy on selected examples with random while achieving $39.2\%$ accuracy on selected examples with IPUSD which shows that selected examples with IPUSD are more challenging (i.e., not clear which class they belong to). Therefore, it might not be optimal to use IPUSD when the distinction between labels is not clear. 

Thus, IPUSD makes the assumption that discrimination between
classes can be learned well. If that is not the case, then
it underperforms.

Table \ref{tab:al_results} also illustrates a rather small
improvement in MRPC with a higher standard deviation than
random. We observe that unlabeled data of MRPC have a
non-uniform distribution of labels as $68\%$ belong to the
equivalent class while $32\%$ belong to the non-equivalent
class. Usually, IPUSD selects training examples similar to
the original distribution because of its clustering
mechanism with zero-shot logits as suggested by the label
entropy score; however, IPUSD failed to select samples with
similar distribution in one of the five selections. That
selection has a $(53\%, 47\%)$ distribution of equivalent and
non-equivalent classes, therefore it achieves lower accuracy
($64.2$) while the other four selections in IPUSD achieve
much higher average accuracy ($71.2\pm1.3$).

Thus, IPUSD makes the assumption that
selected training sets have a label distribution similar to
the overall distribution. If this assumption is not true, it
underperforms.



\begin{table}[t]
	\setlength\tabcolsep{3.2pt}
	\begin{tabular}{lcc}
		& {\textbf{ALBERT}} & {\textbf{RoBERTa}}\\
		\midrule
		\ourmethod & \textbf{73.9} & \textbf{72.7} \\
		\quad--\textbf{AL} (IPUSD) & 72.6 & 71.8 \\
		\quad--\textbf{E}NSEMBLE\textsubscript{Prediction} & 72.0 & 71.1 \\
		\quad--\textbf{M}ultiprompt & 71.6 & 70.7 \\
		
		\bottomrule
	\end{tabular}
	
	\caption{Ablation study of \ourmethod by comparing average performance over five tasks with ALBERT-xxlarge-v2 and RoBERTa\textsubscript{LARGE} with \textit{5} runs. Each row shows the cumulative performance drop when the relevant module is changed with the default setting (IPUSD to random, ENSEMBLE\textsubscript{Prediction} to default, Multiprompt to single prompt). }  \label{tab:ablation_study}
\end{table}


\subsection{Ablation Study of \ourmethod}
We summarize \ourmethod's performance with its modules
and test its robustness with different pretrained language
models. In addition to providing more stable results, Table
\ref{tab:ablation_study} shows that \ourmethod increases
overall performance by 2.3\% and 2.0\% points over default
prompt-based finetuning for ALBERT \cite{Lan2020ALBERT}
and RoBERTa$_{\text{LARGE}}$ \cite{roberta}, consecutively.
The active learning module of \ourmethod, IPUSD, gives the highest
performance improvement by $1.3$ and $0.9$ absolute points,
showing the potential of AL in few-shot learning. Following that,
ENSEMBLE\textsubscript{Prediction} improves the overall
performance by $0.6$ and $0.7$ points for ALBERT and RoBERTA 
on top of its improvement for stability as discussed in Section
 \ref{sec:res_instability}. Finally, we see an overall improvement
 with multiprompt finetuning by $0.4$ points for both LM on top
 of its improvement of model space complexity. Multiprompt
 finetuning reduces the number of models from the number of
 prompts to one during inference time. Finally, we see consistent
 improvements for each module over different language
 models which illustrate \ourmethod's robustness.