% Please download the latest anthology.bib from
%
% http://aclweb.org/anthology/anthology.bib.gz

% Please download the latest anthology.bib from
%
% http://aclweb.org/anthology/anthology.bib.gz

@inproceedings{schick-schutze-2021-just,
	title = "It{'}s Not Just Size That Matters: Small Language Models Are Also Few-Shot Learners",
	author = {Schick, Timo  and
	Sch{\"u}tze, Hinrich},
	booktitle = "Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
	month = jun,
	year = "2021",
	address = "Online",
	publisher = "Association for Computational Linguistics",
	url = "https://aclanthology.org/2021.naacl-main.185",
	doi = "10.18653/v1/2021.naacl-main.185",
	pages = "2339--2352",
	abstract = "When scaled to hundreds of billions of parameters, pretrained language models such as GPT-3 (Brown et al., 2020) achieve remarkable few-shot performance. However, enormous amounts of compute are required for training and applying such big models, resulting in a large carbon footprint and making it difficult for researchers and practitioners to use them. We show that performance similar to GPT-3 can be obtained with language models that are much {``}greener{''} in that their parameter count is several orders of magnitude smaller. This is achieved by converting textual inputs into cloze questions that contain a task description, combined with gradient-based optimization; exploiting unlabeled data gives further improvements. We identify key factors required for successful natural language understanding with small language models.",
}

@inproceedings{lu-etal-2022-fantastically,
	title = "Fantastically Ordered Prompts and Where to Find Them: Overcoming Few-Shot Prompt Order Sensitivity",
	author = "Lu, Yao  and
	Bartolo, Max  and
	Moore, Alastair  and
	Riedel, Sebastian  and
	Stenetorp, Pontus",
	booktitle = "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
	month = may,
	year = "2022",
	address = "Dublin, Ireland",
	publisher = "Association for Computational Linguistics",
	url = "https://aclanthology.org/2022.acl-long.556",
	doi = "10.18653/v1/2022.acl-long.556",
	pages = "8086--8098",
	abstract = "When primed with only a handful of training samples, very large, pretrained language models such as GPT-3 have shown competitive results when compared to fully-supervised, fine-tuned, large, pretrained language models. We demonstrate that the order in which the samples are provided can make the difference between near state-of-the-art and random guess performance: essentially some permutations are {``}fantastic{''} and some not. We analyse this phenomenon in detail, establishing that: it is present across model sizes (even for the largest current models), it is not related to a specific subset of samples, and that a given good permutation for one model is not transferable to another. While one could use a development set to determine which permutations are performant, this would deviate from the true few-shot setting as it requires additional annotated data. Instead, we use the generative nature of language models to construct an artificial development set and based on entropy statistics of the candidate permutations on this set, we identify performant prompts. Our method yields a 13{\%} relative improvement for GPT-family models across eleven different established text classification tasks.",
}


@InProceedings{pmlr-v139-zhao21c,
	title = 	 {Calibrate Before Use: Improving Few-shot Performance of Language Models},
	author =       {Zhao, Zihao and Wallace, Eric and Feng, Shi and Klein, Dan and Singh, Sameer},
	booktitle = 	 {Proceedings of the 38th International Conference on Machine Learning},
	pages = 	 {12697--12706},
	year = 	 {2021},
	editor = 	 {Meila, Marina and Zhang, Tong},
	volume = 	 {139},
	series = 	 {Proceedings of Machine Learning Research},
	month = 	 {18--24 Jul},
	publisher =    {PMLR},
	pdf = 	 {http://proceedings.mlr.press/v139/zhao21c/zhao21c.pdf},
	url = 	 {https://proceedings.mlr.press/v139/zhao21c.html},
	abstract = 	 {GPT-3 can perform numerous tasks when provided a natural language prompt that contains a few training examples. We show that this type of few-shot learning can be unstable: the choice of prompt format, training examples, and even the order of the examples can cause accuracy to vary from near chance to near state-of-the-art. We demonstrate that this instability arises from the bias of language models towards predicting certain answers, e.g., those that are placed near the end of the prompt or are common in the pre-training data. To mitigate this, we first estimate the model’s bias towards each answer by asking for its prediction when given a training prompt and a content-free test input such as "N/A". We then fit calibration parameters that cause the prediction for this input to be uniform across answers. On a diverse set of tasks, this contextual calibration procedure substantially improves GPT-3 and GPT-2’s accuracy (up to 30.0% absolute) across different choices of the prompt, while also making learning considerably more stable.}
}

@article{DBLP:journals/corr/abs-2002-06305,
	author    = {Jesse Dodge and
	Gabriel Ilharco and
	Roy Schwartz and
	Ali Farhadi and
	Hannaneh Hajishirzi and
	Noah A. Smith},
	title     = {Fine-Tuning Pretrained Language Models: Weight Initializations, Data
	Orders, and Early Stopping},
	journal   = {CoRR},
	volume    = {abs/2002.06305},
	year      = {2020},
	url       = {https://arxiv.org/abs/2002.06305},
	eprinttype = {arXiv},
	eprint    = {2002.06305},
	timestamp = {Wed, 23 Dec 2020 10:13:20 +0100},
	biburl    = {https://dblp.org/rec/journals/corr/abs-2002-06305.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{Lan2020ALBERT,
	title={ALBERT: A Lite BERT for Self-supervised Learning of Language Representations},
	author={Zhenzhong Lan and Mingda Chen and Sebastian Goodman and Kevin Gimpel and Piyush Sharma and Radu Soricut},
	booktitle={International Conference on Learning Representations},
	year={2020},
	url={https://openreview.net/forum?id=H1eA7AEtvS}
}

@inproceedings{DBLP:conf/iclr/MosbachAK21,
	author={Marius Mosbach and Maksym Andriushchenko and Dietrich Klakow},
	title={On the Stability of Fine-tuning BERT: Misconceptions, Explanations, and Strong Baselines},
	year={2021},
	cdate={1609459200000},
	url={https://openreview.net/forum?id=nzpLWnVAyah},
	booktitle={ICLR}
}

@inproceedings{schick-schutze-2021-exploiting,
	title = "Exploiting Cloze-Questions for Few-Shot Text Classification and Natural Language Inference",
	author = {Schick, Timo  and
	Sch{\"u}tze, Hinrich},
	booktitle = "Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume",
	month = apr,
	year = "2021",
	address = "Online",
	publisher = "Association for Computational Linguistics",
	url = "https://aclanthology.org/2021.eacl-main.20",
	doi = "10.18653/v1/2021.eacl-main.20",
	pages = "255--269",
	abstract = "Some NLP tasks can be solved in a fully unsupervised fashion by providing a pretrained language model with {``}task descriptions{''} in natural language (e.g., Radford et al., 2019). While this approach underperforms its supervised counterpart, we show in this work that the two ideas can be combined: We introduce Pattern-Exploiting Training (PET), a semi-supervised training procedure that reformulates input examples as cloze-style phrases to help language models understand a given task. These phrases are then used to assign soft labels to a large set of unlabeled examples. Finally, standard supervised training is performed on the resulting training set. For several tasks and languages, PET outperforms supervised training and strong semi-supervised approaches in low-resource settings by a large margin.",
}

v@inproceedings{gpt3,
	author = {Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and Agarwal, Sandhini and Herbert-Voss, Ariel and Krueger, Gretchen and Henighan, Tom and Child, Rewon and Ramesh, Aditya and Ziegler, Daniel and Wu, Jeffrey and Winter, Clemens and Hesse, Chris and Chen, Mark and Sigler, Eric and Litwin, Mateusz and Gray, Scott and Chess, Benjamin and Clark, Jack and Berner, Christopher and McCandlish, Sam and Radford, Alec and Sutskever, Ilya and Amodei, Dario},
	booktitle = {Advances in Neural Information Processing Systems},
	editor = {H. Larochelle and M. Ranzato and R. Hadsell and M.F. Balcan and H. Lin},
	pages = {1877--1901},
	publisher = {Curran Associates, Inc.},
	title = {Language Models are Few-Shot Learners},
	url = {https://proceedings.neurips.cc/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf},
	volume = {33},
	year = {2020}
}

@inproceedings{entropy,
	author = {Roy, Nicholas and McCallum, Andrew},
	title = {Toward Optimal Active Learning through Sampling Estimation of Error Reduction},
	year = {2001},
	isbn = {1558607781},
	publisher = {Morgan Kaufmann Publishers Inc.},
	address = {San Francisco, CA, USA},
	booktitle = {Proceedings of the Eighteenth International Conference on Machine Learning},
	pages = {441–448},
	numpages = {8},
	series = {ICML '01}
}

@INPROCEEDINGS{breaking_ties,  author={Tong Luo and Kramer, K. and Samson, S. and Remsen, A. and Goldgof, D.B. and Hall, L.O. and Hopkins, T.},  booktitle={Proceedings of the 17th International Conference on Pattern Recognition, 2004. ICPR 2004.},   title={Active learning to recognize multiple types of plankton},   year={2004},  volume={3},  number={},  pages={478-481 Vol.3},  doi={10.1109/ICPR.2004.1334570}}

@inproceedings{lowest_confidence,
	author = {Culotta, Aron and McCallum, Andrew},
	title = {Reducing Labeling Effort for Structured Prediction Tasks},
	year = {2005},
	isbn = {157735236x},
	publisher = {AAAI Press},
	abstract = {A common obstacle preventing the rapid deployment of supervised machine learning algorithms is the lack of labeled training data. This is particularly expensive to obtain for structured prediction tasks, where each training instance may have multiple, interacting labels, all of which must be correctly annotated for the instance to be of use to the learner. Traditional active learning addresses this problem by optimizing the order in which the examples are labeled to increase learning efficiency. However, this approach does not consider the difficulty of labeling each example, which can vary widely in structured prediction tasks. For example, the labeling predicted by a partially trained system may be easier to correct for some instances than for others.We propose a new active learning paradigm which reduces not only how many instances the annotator must label, but also how difficult each instance is to annotate. The system also leverages information from partially correct predictions to efficiently solicit annotations from the user. We validate this active learning framework in an interactive information extraction system, reducing the total number of annotation actions by 22%.},
	booktitle = {Proceedings of the 20th National Conference on Artificial Intelligence - Volume 2},
	pages = {746–751},
	numpages = {6},
	location = {Pittsburgh, Pennsylvania},
	series = {AAAI'05}
}


@inproceedings{cal,
	title = "Active Learning by Acquiring Contrastive Examples",
	author = {Margatina, Katerina  and
	Vernikos, Giorgos  and
	Barrault, Lo{\"\i}c  and
	Aletras, Nikolaos},
	booktitle = "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing",
	month = nov,
	year = "2021",
	address = "Online and Punta Cana, Dominican Republic",
	publisher = "Association for Computational Linguistics",
	url = "https://aclanthology.org/2021.emnlp-main.51",
	doi = "10.18653/v1/2021.emnlp-main.51",
	pages = "650--663",
	abstract = "Common acquisition functions for active learning use either uncertainty or diversity sampling, aiming to select difficult and diverse data points from the pool of unlabeled data, respectively. In this work, leveraging the best of both worlds, we propose an acquisition function that opts for selecting contrastive examples, i.e. data points that are similar in the model feature space and yet the model outputs maximally different predictive likelihoods. We compare our approach, CAL (Contrastive Active Learning), with a diverse set of acquisition functions in four natural language understanding tasks and seven datasets. Our experiments show that CAL performs consistently better or equal than the best performing baseline across all tasks, on both in-domain and out-of-domain data. We also conduct an extensive ablation study of our method and we further analyze all actively acquired datasets showing that CAL achieves a better trade-off between uncertainty and diversity compared to other strategies.",
}


@inproceedings{
	badge,
	title={Deep Batch Active Learning by Diverse, Uncertain Gradient Lower Bounds},
	author={Jordan T. Ash and Chicheng Zhang and Akshay Krishnamurthy and John Langford and Alekh Agarwal},
	booktitle={International Conference on Learning Representations},
	year={2020},
	url={https://openreview.net/forum?id=ryghZJBKPS}
}

@InProceedings{rte,
	author="Dagan, Ido
	and Glickman, Oren
	and Magnini, Bernardo",
	editor="Qui{\~{n}}onero-Candela, Joaquin
	and Dagan, Ido
	and Magnini, Bernardo
	and d'Alch{\'e}-Buc, Florence",
	title="The PASCAL Recognising Textual Entailment Challenge",
	booktitle="Machine Learning Challenges. Evaluating Predictive Uncertainty, Visual Object Classification, and Recognising Tectual Entailment",
	year="2006",
	publisher="Springer Berlin Heidelberg",
	address="Berlin, Heidelberg",
	pages="177--190",
	abstract="This paper describes the PASCAL Network of Excellence first Recognising Textual Entailment (RTE-1) Challenge benchmark. The RTE task is defined as recognizing, given two text fragments, whether the meaning of one text can be inferred (entailed) from the other. This application-independent task is suggested as capturing major inferences about the variability of semantic expression which are commonly needed across multiple applications. The Challenge has raised noticeable attention in the research community, attracting 17 submissions from diverse groups, suggesting the generic relevance of the task.",
	isbn="978-3-540-33428-6"
}

@inproceedings{sst,
	title={Recursive deep models for semantic compositionality over a sentiment treebank},
	author={Socher, Richard and Perelygin, Alex and Wu, Jean and Chuang, Jason and Manning, Christopher D and Ng, Andrew Y and Potts, Christopher},
	booktitle={Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing},
	pages={1631--1642},
	year={2013}
}
@inproceedings{trec,
	title = "Learning Question Classifiers",
	author = "Li, Xin  and
	Roth, Dan",
	booktitle = "{COLING} 2002: The 19th International Conference on Computational Linguistics",
	year = "2002",
	url = "https://aclanthology.org/C02-1150",
}

@inproceedings{mrpc,
	title = "Automatically Constructing a Corpus of Sentential Paraphrases",
	author = "Dolan, William B.  and
	Brockett, Chris",
	booktitle = "Proceedings of the Third International Workshop on Paraphrasing ({IWP}2005)",
	year = "2005",
	url = "https://aclanthology.org/I05-5002",
}
@article{roberta,
	author    = {Yinhan Liu and
	Myle Ott and
	Naman Goyal and
	Jingfei Du and
	Mandar Joshi and
	Danqi Chen and
	Omer Levy and
	Mike Lewis and
	Luke Zettlemoyer and
	Veselin Stoyanov},
	title     = {RoBERTa: {A} Robustly Optimized {BERT} Pretraining Approach},
	journal   = {CoRR},
	volume    = {abs/1907.11692},
	year      = {2019},
	url       = {http://arxiv.org/abs/1907.11692},
	eprinttype = {arXiv},
	eprint    = {1907.11692},
	timestamp = {Thu, 01 Aug 2019 08:59:33 +0200},
	biburl    = {https://dblp.org/rec/journals/corr/abs-1907-11692.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{schroder-etal-2022-revisiting,
	title = "Revisiting Uncertainty-based Query Strategies for Active Learning with Transformers",
	author = {Schr{\"o}der, Christopher  and
	Niekler, Andreas  and
	Potthast, Martin},
	booktitle = "Findings of the Association for Computational Linguistics: ACL 2022",
	month = may,
	year = "2022",
	address = "Dublin, Ireland",
	publisher = "Association for Computational Linguistics",
	url = "https://aclanthology.org/2022.findings-acl.172",
	doi = "10.18653/v1/2022.findings-acl.172",
	pages = "2194--2203",
	abstract = "Active learning is the iterative construction of a classification model through targeted labeling, enabling significant labeling cost savings. As most research on active learning has been carried out before transformer-based language models ({``}transformers{''}) became popular, despite its practical importance, comparably few papers have investigated how transformers can be combined with active learning to date. This can be attributed to the fact that using state-of-the-art query strategies for transformers induces a prohibitive runtime overhead, which effectively nullifies, or even outweighs the desired cost savings. For this reason, we revisit uncertainty-based query strategies, which had been largely outperformed before, but are particularly suited in the context of fine-tuning transformers. In an extensive evaluation, we connect transformers to experiments from previous research, assessing their performance on five widely used text classification benchmarks. For active learning with transformers, several other uncertainty-based approaches outperform the well-known prediction entropy query strategy, thereby challenging its status as most popular uncertainty baseline in active learning for text classification.",
}

@inproceedings{gao-etal-2021-making,
	title = "Making Pre-trained Language Models Better Few-shot Learners",
	author = "Gao, Tianyu  and
	Fisch, Adam  and
	Chen, Danqi",
	booktitle = "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)",
	month = aug,
	year = "2021",
	address = "Online",
	publisher = "Association for Computational Linguistics",
	url = "https://aclanthology.org/2021.acl-long.295",
	doi = "10.18653/v1/2021.acl-long.295",
	pages = "3816--3830",
	abstract = "The recent GPT-3 model (Brown et al., 2020) achieves remarkable few-shot performance solely by leveraging a natural-language prompt and a few task demonstrations as input context. Inspired by their findings, we study few-shot learning in a more practical scenario, where we use smaller language models for which fine-tuning is computationally efficient. We present LM-BFF{---}better few-shot fine-tuning of language models{---}a suite of simple and complementary techniques for fine-tuning language models on a small number of annotated examples. Our approach includes (1) prompt-based fine-tuning together with a novel pipeline for automating prompt generation; and (2) a refined strategy for dynamically and selectively incorporating demonstrations into each context. Finally, we present a systematic evaluation for analyzing few-shot performance on a range of NLP tasks, including classification and regression. Our experiments demonstrate that our methods combine to dramatically outperform standard fine-tuning procedures in this low resource setting, achieving up to 30{\%} absolute improvement, and 11{\%} on average across all tasks. Our approach makes minimal assumptions on task resources and domain expertise, and hence constitutes a strong task-agnostic method for few-shot learning.",
}
@inproceedings{tam-etal-2021-improving,
	title = "Improving and Simplifying Pattern Exploiting Training",
	author = "Tam, Derek  and
	R. Menon, Rakesh  and
	Bansal, Mohit  and
	Srivastava, Shashank  and
	Raffel, Colin",
	booktitle = "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing",
	month = nov,
	year = "2021",
	address = "Online and Punta Cana, Dominican Republic",
	publisher = "Association for Computational Linguistics",
	url = "https://aclanthology.org/2021.emnlp-main.407",
	doi = "10.18653/v1/2021.emnlp-main.407",
	pages = "4980--4991",
	abstract = "Recently, pre-trained language models (LMs) have achieved strong performance when fine-tuned on difficult benchmarks like SuperGLUE. However, performance can suffer when there are very few labeled examples available for fine-tuning. Pattern Exploiting Training (PET) is a recent approach that leverages patterns for few-shot learning. However, PET uses task-specific unlabeled data. In this paper, we focus on few-shot learning without any unlabeled data and introduce ADAPET, which modifies PET{'}s objective to provide denser supervision during fine-tuning. As a result, ADAPET outperforms PET on SuperGLUE without any task-specific unlabeled data.",
}
@inproceedings{DBLP:conf/coling/SchickSS20,
	author={Timo Schick and Helmut Schmid and Hinrich Schütze},
	title={Automatically Identifying Words That Can Serve as Labels for Few-Shot Text Classification},
	year={2020},
	cdate={1577836800000},
	pages={5569-5578},
	url={https://doi.org/10.18653/v1/2020.coling-main.488},
	booktitle={COLING}
}

@article{finetuning_instability_dodge_2020,
	author    = {Jesse Dodge and
	Gabriel Ilharco and
	Roy Schwartz and
	Ali Farhadi and
	Hannaneh Hajishirzi and
	Noah A. Smith},
	title     = {Fine-Tuning Pretrained Language Models: Weight Initializations, Data
	Orders, and Early Stopping},
	journal   = {CoRR},
	volume    = {abs/2002.06305},
	year      = {2020},
	url       = {https://arxiv.org/abs/2002.06305},
	eprinttype = {arXiv},
	eprint    = {2002.06305},
	timestamp = {Wed, 23 Dec 2020 10:13:20 +0100},
	biburl    = {https://dblp.org/rec/journals/corr/abs-2002-06305.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{zheng-etal-2022-fewnlu,
	title = "{F}ew{NLU}: Benchmarking State-of-the-Art Methods for Few-Shot Natural Language Understanding",
	author = "Zheng, Yanan  and
	Zhou, Jing  and
	Qian, Yujie  and
	Ding, Ming  and
	Liao, Chonghua  and
	Jian, Li  and
	Salakhutdinov, Ruslan  and
	Tang, Jie  and
	Ruder, Sebastian  and
	Yang, Zhilin",
	booktitle = "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
	month = may,
	year = "2022",
	address = "Dublin, Ireland",
	publisher = "Association for Computational Linguistics",
	url = "https://aclanthology.org/2022.acl-long.38",
	doi = "10.18653/v1/2022.acl-long.38",
	pages = "501--516",
	abstract = "The few-shot natural language understanding (NLU) task has attracted much recent attention. However, prior methods have been evaluated under a disparate set of protocols, which hinders fair comparison and measuring the progress of the field. To address this issue, we introduce an evaluation framework that improves previous evaluation procedures in three key aspects, i.e., test performance, dev-test correlation, and stability. Under this new evaluation framework, we re-evaluate several state-of-the-art few-shot methods for NLU tasks. Our framework reveals new insights: (1) both the absolute performance and relative gap of the methods were not accurately estimated in prior literature; (2) no single method dominates most tasks with consistent performance; (3) improvements of some methods diminish with a larger pretrained model; and (4) gains from different methods are often complementary and the best combined model performs close to a strong fully-supervised baseline. We open-source our toolkit, FewNLU, that implements our evaluation framework along with a number of state-of-the-art methods.",
}

@article{al_survey,
	author    = {Christopher Schr{\"{o}}der and
	Andreas Niekler},
	title     = {A Survey of Active Learning for Text Classification using Deep Neural
	Networks},
	journal   = {CoRR},
	volume    = {abs/2008.07267},
	year      = {2020},
	url       = {https://arxiv.org/abs/2008.07267},
	eprinttype = {arXiv},
	eprint    = {2008.07267},
	timestamp = {Fri, 21 Aug 2020 15:05:50 +0200},
	biburl    = {https://dblp.org/rec/journals/corr/abs-2008-07267.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{zhao-etal-2021-closer,
	title = "A Closer Look at Few-Shot Crosslingual Transfer: The Choice of Shots Matters",
	author = {Zhao, Mengjie  and
	Zhu, Yi  and
	Shareghi, Ehsan  and
	Vuli{\'c}, Ivan  and
	Reichart, Roi  and
	Korhonen, Anna  and
	Sch{\"u}tze, Hinrich},
	booktitle = "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)",
	month = aug,
	year = "2021",
	address = "Online",
	publisher = "Association for Computational Linguistics",
	url = "https://aclanthology.org/2021.acl-long.447",
	doi = "10.18653/v1/2021.acl-long.447",
	pages = "5751--5767",
	abstract = "Few-shot crosslingual transfer has been shown to outperform its zero-shot counterpart with pretrained encoders like multilingual BERT. Despite its growing popularity, little to no attention has been paid to standardizing and analyzing the design of few-shot experiments. In this work, we highlight a fundamental risk posed by this shortcoming, illustrating that the model exhibits a high degree of sensitivity to the selection of few shots. We conduct a large-scale experimental study on 40 sets of sampled few shots for six diverse NLP tasks across up to 40 languages. We provide an analysis of success and failure cases of few-shot transfer, which highlights the role of lexical features. Additionally, we show that a straightforward full model finetuning approach is quite effective for few-shot transfer, outperforming several state-of-the-art few-shot approaches. As a step towards standardizing few-shot crosslingual experimental designs, we make our sampled few shots publicly available.",
}

@misc{sewon-etal-2022-rethinking,
	doi = {10.48550/ARXIV.2202.12837},
	url = {https://arxiv.org/abs/2202.12837},
	author = {Min, Sewon and Lyu, Xinxi and Holtzman, Ari and Artetxe, Mikel and Lewis, Mike and Hajishirzi, Hannaneh and Zettlemoyer, Luke},
	keywords = {Computation and Language (cs.CL), Artificial Intelligence (cs.AI), FOS: Computer and information sciences, FOS: Computer and information sciences},
	title = {Rethinking the Role of Demonstrations: What Makes In-Context Learning Work?},
	publisher = {arXiv},
	year = {2022},
	copyright = {Creative Commons Attribution 4.0 International}
}


@inproceedings{schick-schutze-2021-shot,
	title = "Few-Shot Text Generation with Natural Language Instructions",
	author = {Schick, Timo  and
	Sch{\"u}tze, Hinrich},
	booktitle = "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing",
	month = nov,
	year = "2021",
	address = "Online and Punta Cana, Dominican Republic",
	publisher = "Association for Computational Linguistics",
	url = "https://aclanthology.org/2021.emnlp-main.32",
	doi = "10.18653/v1/2021.emnlp-main.32",
	pages = "390--402",
	abstract = "Providing pretrained language models with simple task descriptions in natural language enables them to solve some tasks in a fully unsupervised fashion. Moreover, when combined with regular learning from examples, this idea yields impressive few-shot results for a wide range of text classification tasks. It is also a promising direction to improve data efficiency in generative settings, but there are several challenges to using a combination of task descriptions and example-based learning for text generation. In particular, it is crucial to find task descriptions that are easy to understand for the pretrained model and to ensure that it actually makes good use of them; furthermore, effective measures against overfitting have to be implemented. In this paper, we show how these challenges can be tackled: We introduce GenPET, a method for text generation that is based on pattern-exploiting training, a recent approach for combining textual instructions with supervised learning that only works for classification tasks. On several summarization and headline generation datasets, GenPET gives consistent improvements over strong baselines in few-shot settings.",
}


@article{cohn1996active,
	title={Active learning with statistical models},
	author={Cohn, David A and Ghahramani, Zoubin and Jordan, Michael I},
	journal={Journal of artificial intelligence research},
	volume={4},
	pages={129--145},
	year={1996}
}

@TechReport{settles2009active,
	author      = {Settles, B.},
	title       = {Active learning literature survey},
	institution = {University of Wisconsin-Madison Department of Computer Sciences},
	year        = {2009},
}
@inproceedings{izmailov2018averaging,
	title={Averaging weights leads to wider optima and better generalization},
	author={Izmailov, Pavel and Podoprikhin, Dmitrii and Garipov, Timur and Vetrov, Dmitry and Wilson, Andrew Gordon},
	booktitle={34th Conference on Uncertainty in Artificial Intelligence 2018, UAI 2018},
	pages={876--885},
	year={2018},
	organization={Association For Uncertainty in Artificial Intelligence (AUAI)}
}

@article{zhdanov2019diverse,
	title={Diverse mini-batch active learning},
	author={Zhdanov, Fedor},
	journal={arXiv preprint arXiv:1901.05954},
	year={2019}
}

@inproceedings{dor2020active,
	title={Active learning for BERT: an empirical study},
	author={Dor, Liat Ein and Halfon, Alon and Gera, Ariel and Shnarch, Eyal and Dankin, Lena and Choshen, Leshem and Danilevsky, Marina and Aharonov, Ranit and Katz, Yoav and Slonim, Noam},
	booktitle={Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)},
	pages={7949--7962},
	year={2020}
}

@inproceedings{prabhu-etal-2019-sampling,
	title = "Sampling Bias in Deep Active Classification: An Empirical Study",
	author = "Prabhu, Ameya  and
	Dognin, Charles  and
	Singh, Maneesh",
	booktitle = "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)",
	month = nov,
	year = "2019",
	address = "Hong Kong, China",
	publisher = "Association for Computational Linguistics",
	url = "https://www.aclweb.org/anthology/D19-1417",
	doi = "10.18653/v1/D19-1417",
	pages = "4058--4068",
	abstract = "The exploding cost and time needed for data labeling and model training are bottlenecks for training DNN models on large datasets. Identifying smaller representative data samples with strategies like active learning can help mitigate such bottlenecks. Previous works on active learning in NLP identify the problem of sampling bias in the samples acquired by uncertainty-based querying and develop costly approaches to address it. Using a large empirical study, we demonstrate that active set selection using the posterior entropy of deep models like FastText.zip (FTZ) is robust to sampling biases and to various algorithmic choices (query size and strategies) unlike that suggested by traditional literature. We also show that FTZ based query strategy produces sample sets similar to those from more sophisticated approaches (e.g ensemble networks). Finally, we show the effectiveness of the selected samples by creating tiny high-quality datasets, and utilizing them for fast and cheap training of large models. Based on the above, we propose a simple baseline for deep active text classification that outperforms the state of the art. We expect the presented work to be useful and informative for dataset compression and for problems involving active, semi-supervised or online learning scenarios. Code and models are available at: https://github.com/drimpossible/Sampling-Bias-Active-Learning.",
}

@article{zhang2015character,
	title={Character-level convolutional networks for text classification},
	author={Zhang, Xiang and Zhao, Junbo and LeCun, Yann},
	journal={Advances in neural information processing systems},
	volume={28},
	year={2015}
}
@misc{rotman2022multi,
	doi = {10.48550/ARXIV.2208.05379},
	url = {https://arxiv.org/abs/2208.05379},
	author = {Rotman, Guy and Reichart, Roi},
	keywords = {Computation and Language (cs.CL), Human-Computer Interaction (cs.HC), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
	title = {Multi-task Active Learning for Pre-trained Transformer-based Models},
	publisher = {arXiv},
	year = {2022},
	copyright = {Creative Commons Attribution 4.0 International}
}
