\documentclass{article}

% if you need to pass options to natbib, use, e.g.:
%     \PassOptionsToPackage{numbers, compress}{natbib}
% before loading neurips_2021

% ready for submission
\usepackage{neurips_2021}

% to compile a preprint version, e.g., for submission to arXiv, add add the
% [preprint] option:
%     \usepackage[preprint]{neurips_2021}

% to compile a camera-ready version, add the [final] option, e.g.:
%     \usepackage[final]{neurips_2021}

% to avoid loading the natbib package, add option nonatbib:
%    \usepackage[nonatbib]{neurips_2021}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit     T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{xcolor}         % colors
\usepackage{float}          % forced position fix
\usepackage{graphicx}       % package for drawing
\usepackage{subfigure}      % package for drawing
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\newcommand{\yali}[1]{{ \color{magenta}[Yali: #1]}}
\newcommand{\runji}[1]{{ \color{blue}[runji: #1]}}
\newcommand{\liye}[1]{{ \color{purple}[liye: #1]}}
\usepackage{changes}
\usepackage{color}
\usepackage{hyperref}

\title{
{ Contextual Transformer for Offline Meta-Reinforcement Learning }
}
% Pretrained
% \yali{shall we remove ``multi-agent'' from title?}

% offline and online 

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to LaTeX to determine where to break the
% lines. Using \AND forces a line break at that point. So, if LaTeX puts 3 of 4
% authors names on the first line, and the last on the second line, try using
% \AND instead of \And before the third author name.

\author{%
  David S.~Hippocampus\thanks{Use footnote for providing further information
    about author (webpage, alternative address)---\emph{not} for acknowledging
    funding agencies.} \\
  Department of Computer Science\\
  Cranberry-Lemon University\\
  Pittsburgh, PA 15213 \\
  \texttt{hippo@cs.cranberry-lemon.edu} \\
  % examples of more authors
  % \And
  % Coauthor \\
  % Affiliation \\
  % Address \\
  % \texttt{email} \\
  % \AND
  % Coauthor \\
  % Affiliation \\
  % Address \\
  % \texttt{email} \\
  % \And
  % Coauthor \\
  % Affiliation \\
  % Address \\
  % \texttt{email} \\
  % \And
  % Coauthor \\
  % Affiliation \\
  % Address \\
  % \texttt{email} \\
}

\begin{document}

\maketitle

\begin{abstract}
%Recently, large-scale sequence models have made significant progress in natural language processing and computer vision. However, deployment of this paradigm in reinforcement learning(RL) is still hindered by intractable challenges, such as a lack of self-supervised learning method for offline RL data and efficient knowledge transfer from pretrain tasks to unseen downstream tasks. In this work, we first propose a new offline reinforcement learning algorithm from a sequence modeling perspective based on prompt tuning, which means learning a context concatenated with the input to guide the conditional generation. In this way, we can pretrain a model on the offline dataset with supervised loss and learn a prompt to guide the policy to play desired actions. And we extend this framework into an offline meta learning setting, named CMT(Contextual Meta Transform). CMT leverages the context among different tasks as prompt to improve the performance on unseen tasks. And the experiments on D4RL, meta-RL and starcraft II benchmarks demonstrate the strong performance and computation efficiency of our methods and the feasibility to apply CMT in a multi-agent setting.

Recently, the pretrain-tuning paradigm in large-scale sequence models has made significant progress in Natural Language Processing and Computer Vision. However, such a paradigm is still hindered by intractable challenges in Reinforcement Learning (RL), including the lack of self-supervised large-scale pretraining methods based on offline data and efficient fine-tuning/prompt-tuning over unseen downstream tasks. In this work, we explore how prompts can help sequence-modeling based offline Reinforcement Learning (offline-RL) algorithms. Firstly, we propose prompt tuning for offline RL, where a context vector sequence is concatenated with the input to guide the conditional generation. As such, we can pretrain a model on the offline dataset with supervised loss and learn a prompt to guide the policy to play the desired actions. Secondly, we extend the framework to the Meta-RL setting and propose Contextual Meta Transformer (CMT), which leverages the context among different tasks as the prompt to improve the performance on unseen tasks. We conduct extensive experiments across three different offline-RL settings: offline single-agent RL on the D4RL dataset, offline Meta-RL on the MuJoCo benchmark, and offline MARL on the SMAC benchmark; results validate the strong performance, high computation efficiency, and generality of our methods.

\end{abstract}

\section{Introduction}

%\yali{1st: DT shines in sequential decision making task, [cite dt, trajectory transformer, etc.]...question, whether pretrain enable RL agents to harness knowledge for generalization?...}
Reinforcement learning algorithms based on sequence modeling \cite{DT_2021,TT_2021, gato} shines in sequential decision making task and form a new promising paradigm. Compared with classic RL method, such as policy based methods and value-based methods \cite{RLintro}, optimization of the policies from sequence prospective have advantage in long-term credit assignment, partial observation and so on. 
%\deleted{However previous works mainly focus on single agent, related issues are still in the exploratory stage in multi-agent setting.} \yali{we may not claim too much contribution on multi-agent side.}
Meanwhile, significant generalization of large pretrained sequence model in natural language processing \cite{bert, brown2020gpt3} and computer vision \cite{SwinTransformer, scalingVIT} not only conserves vast computation in downstream tasks, but also alleviate the large data quantity requirements. Inspired by them, we want to ask \textit{ whether pretrain technique have the similar power in RL ?} Since limited and expensive interactions impede deployment of RL in various valuable applications \cite{levine2020offline}, pretraining a large model to improve robustness of real-world gap by zero-shot generalization and improve data efficiency by few-shot learning have great significance. \cite{Xland,MADT} 
%\yali{the bibtex has mistakes in Xland.} 
pretrains a single model with diverse and abundant training tasks in decision-making domain to generalize in downstream tasks, which proves feasibility that pretrain enable RL agents to harness knowledge for generalization.

%\yali{2nd paragraph. two major challenges are **maybe mention shortcomings of DT, TT,**}
Earlier works on sequence modeling RL provide a new perspective on offline RL. However,  extended these methods to pretrain domain is still confronted with several challenges. One major challenge for generalization \cite{FOCAL} is  to  encode task-relevant information, thus transferring knowledge among tasks. {However, d}iscovering the relationship among diverse tasks from data and making decisions conditioned on distinct tasks plays a significant role in generalization, which is not naive modification from existing methods. Another problem is the efficient self-supervised learning in offline RL. Specifically, decision transformer \cite{DT_2021} leverages the data to learn a return conditioned policy, which ignores the knowledge about the world dynamics. And trajectory transformer \cite{TT_2021} plans on a world model, but the high computational intensity might be bottleneck for a large-scale model and hard to fine-tune to other tasks. Therefore, introducing key components to transfer the knowledge in pretrained model and incorporation the advantages in a conditioned policy and a world model is necessary. %\yali{what is "key component"? what are "advantages"}

%Moreover, online multi-task reinforcement learning suffers from the potential conflicting gradient with high variance, leading to the failure in several high-distinct tasks. For this consideration, the distillation among several small multi-task network and a unified large model is a promising approaches. By replacing the high-variance reinforcement loss by much stable supervised loss, a large model distilled from several expert small network has larger capacity to master tasks, which facilitates the data efficiency in new task. On the other side, a small model distilled from a large model in a new task saves large-scale computation resource and enjoy the fast adaption advantage.\runji{polish the multi-task meaning.} 

%\liye{Grammarly fixed, pending inspection}
In this work, we propose a novel offline meta RL algorithm, named \textbf{C}ontextual \textbf{M}eta \textbf{T}ransformer (CMT), aiming to conquer multiple tasks and generalization at one shot in an offline setting from the perspective of sequence modeling. CMT provides a pretrain and prompt-tunning paradigm to solve offline RL problems in the offline setting. Firstly, a model is pretrained on the offline dataset through self-supervised learning method, which converts the offline trajectories into some policy prompts and utilizes these policy prompts to reconstruct the offline trajectories in the autoregressive style. Then a better policy prompt is learned based on implicitly planning in the learned world model to guide the policy to generate trajectories with high rewards. In contrast to previous work, CMT learns a prompt to construct policy to guide desired actions, rather than being designed by humans or explicitly planned by the world model. In the offline meta-learning setting, CMT extends the framework by simply concatenating a task prompt with the input sequence. With a simple modification, CMT is capable to execute a proper policy for a specific task and share knowledge among tasks.

%encode the policy as a hidden context variable \yali{encode policy or task?}. The main insight is that the distribution of trajectory sequence is partly determined by behavior policy\yali{what is the connection between our ''insights'' and two challenges above?}. Therefore, by leveraging the offline experience to learn some basis policy contexts, a better policy context is generated by these basis contexts and a world model learned from offline data. 





%To alleviate the information distortion\yali{what is this? }, we introduce a large sequence model to predict the future action, which directly input the exploration trajectory and the expert trajectory to avoid learn a explicit task context \yali{this sentence is not clear to me}. 
%Then a exploration policy is trained to collect more informative trajectory. To achieve this purpose, the predicted error among distinct tasks is employed as an intrinsic reward to guide exploration policy to identify key states. 
%We demonstrate the significant performance of CMT on various challenging benchmarks. [ SMAC, meta-world] [offline performance, cross-map performance, ]


%\liye{We need to expand on the main contribution, this part is a bit short}
Our contributions are three-folds:
% \begin{enumerate}
    % \item 
    First, we propose a novel offline RL algorithm based on prompt tuning, in which the offline trajectory is encoded as a prompt, and the appropriate prompt is found to lead a policy for execution to achieve high reward in the online environment.%\yali{"for the best policy" is not clear to me} . %\yali{two ``rather than''? }.
    % \item 
     Second, CMT is the first algorithm to solve the offline meta learning from sequence modeling prospective.The context trajectory, which represents the structure of the task, is used by CMT as a prompt to guide the policy in a specific unknown task.
        % \item
    Third, CMT shows impressive performances in offline learning setting in gym Mujoco benchmarks, strong generalization in meta learning setting in meta Mujuco benchmarks.  This framework is easy to extend to multi-agent setting, and evaluation on SMAC demonstrates the efficacy of CMT.
% \end{enumerate}



\section{Related Work}
\textbf{Offline Reinforcement Learning.} 
Offline RL is gaining popularity as a data-driven RL method which can effectively utilize large offline datasets. However, the data distribution shift and hyper-parameter tuning in offline setting seriously affect the performance of the agent \cite{Sergey}. So far, a number of schemes have been proposed to address them. Through action-space constraint, BCQ \cite{BCQ}, AWR \cite{AWR}, BRAC \cite{BRAC}, and ICQ \cite{ICQ} reduce extrapolation error caused by policy iteration. Noticing the problem of overestimation of values, CQL keeps reasonable estimates through looking for pessimistic expectations \cite{CQL}. UWAC handles out-of-distribution (OOD) data by weighting the Q value during training by estimating the uncertainty of $(s,a)$ \cite{UWAC}. MOPO \cite{MOPO} and MOReL \cite{MOREL} solve the offline RL problem from the model-based perspective while ensuring rational control by adding penalty item to uncertain areas. Decision Transformer (DT) \cite{DT_2021} and Trajectory Transformer (TT) \cite{TT_2021} reconstruct the RL problem into sequential decision problem, extending the LLM-like structure to the RL area, which inspired many followup works on them. However, the relevant work on offline RL is still insufficient due to the lack of self-supervised large-scale pretraining methods and efficient prompt-tuning over unseen tasks, and CMT proposes a pretrain-and-tune paradigm to deal with them.

%\textbf{offline to online}
%- AWAC: 
%- Offline reinforcement learning with implicit q-learning
%- AW-opt: Learning robotic skills with imitation and reinforcement at scale.
%- Offline-to-online reinforcement learning via balanced replay and pessimistic q-ensemble.
%- MOORe: Model-based Offline-to-Online Reinforcement Learning

\textbf{Pretrain and Sequence Modeling.}
Recently, much attention has been attracted to pretraining big models on large-scale unsupervised datasets and applying them to downstream tasks through fine-tuning. In language process tasks, transformer-based models such as BERT \cite{bert}, GPT-3 \cite{brown2020gpt3} overcome the limitation that RNN cannot be trained in parallel and improve the ability to use long sequence information, achieving SOTA results on NLP tasks such as translation, question answering systems. Even the CV field has been inspired to reconstruct their issues as sequence modeling problems, and high-performance models like the swin transformer \cite{SwinTransformer} and scaling VIT \cite{scalingVIT} have been proposed. Since the trajectories in offline RL datasets have Markov properties, they can be modeled through transformer-like structures. Decision transformer \cite{DT_2021} and trajectory transformer \cite{TT_2021} propose condition policy on return to go (RTG) and behavior cloning policy improved by beam search to solve RL problems in offline setting respectively. Inspired by these works, we bring prompt tuning from nlp into the RL domain, then propose a potential path to pretrain a large-scale RL model and efficiently transfer the knowledge to downstream tasks. 
%on the other hand, are unstable and introduce artificial factors. \deleted{To address this issue, prompt-tuning \cite{prefix-tune} is proposed to learn a continuous prompt from data to replace human designing.  }\yali{it is better to discuss this in related works.}

\textbf{Offline Meta RL and task generalization}
Offline Meta RL shines recently since it allows algorithms to adapt to new tasks quickly without interacting with the environment. Targeting on it, an optimization-based method with advantage weighting loss called MACAW \cite{MACAW} is proposed, which learns the initialization of both the value function and the policy. FOCAL \cite{FOCAL} combines the deterministic context encoder with behavior regularization and achieves inspiring results based on the off-policy Meta-RL method PEARL \cite{PEARL}. Then it is improved through combining the intra-task attention mechanism and the inter-task contrastive learning objective, which is named FOCAL++ \cite{FOCAL++}, to deal with sparse reward and distribution shift. BOReL \cite{BOReL} aims to learn Bayesian optimal policies from discrete data for the mentioned problems, whereas SMAC \cite{SMAC} learns meta-policies from reward-labeled data and then fine-tunes on new tasks. From the model-based perspective, MerPO \cite{MerPO} proposes a meta-model for efficient task structure inference and a meta-policy for safe exploration of OOD data. It is worth mentioning that recent work on general model construction, such as SayCan \cite{SayCan}, and Gato \cite{gato}, has achieved exciting results, demonstrating the huge potential of LLM-like architectures. Just like them, CMT is also a general LLM-like model that can solve offline meta RL problems effectively.


\section{Preliminary}
\paragraph{Meta Reinforcement Learning.}  The major propose of meta RL is to leverage multi-task experience to enable fast adaptation to new unseen tasks. A task $\mathcal{T}_{i}$ is defined as a MarKov Decsion Process (MDP) $\mathcal{M}_{i} = (\mathcal{S}, \mathcal{A}, \mathcal{R}, \mathcal{P}, \lambda)$ , where $\mathcal{S}$ is the state space, $\mathcal{A}$ is the action space, $\mathcal{R}$ is reward function, and $\mathcal{P}$ is transition function.
In deep RL, the policy $\pi_\theta(a_t|s_t)$, which specifies the probability that the agent takes action $a_t$ in state $s_t$ at time $t$, is described by a neural network with parameters $\theta$.
The goal in a MDP is to learn a optimal policy $\pi^* = \arg \max_{\pi}  \mathbb{E}_{s_0,a_0, s_1, a_1, \dots} [\sum^{\infty}_{t=0} \lambda^t r(s_t, a_t) ]$ which can maximize the expected discounted return, where $\lambda$ is a discounted factor. In meta-RL, tasks are drawn from a task distribution $\mathcal{T} \sim p(\mathcal{M})$, the state space $\mathcal{S}$ and the action space $\mathcal{A}$ are common across tasks, and reward function $\mathcal{R}_i$ and transition function $\mathcal{P}_i$ are task specific. During meta-training, the meta policies are trained based on some training tasks sampled from task distribution to achieve fast adaption to new unseen tasks in meta test.  
 


\paragraph{Offline Reinforcement learning.}
 In offline RL setting, the trajectory dataset $D$  is collected from unknown behavior policy $\mu$, which might be an expert policy, sub-optimal policy, random policy, or a mixture policy (e.g. corresponding the replay buffer of an $\mathrm{RL}$ agent).  A offline trajectory $\tau$ consists of states, actions, and scalar rewards: $\tau= \{\mathbf{s}_{t},\mathbf{a}_{t}, r_{t}\}_{t=0}^{T-1}$. A trajectory fragment $\tau_{[t_1:t_2]}$ denotes transitions from time-step $t_1$ to time-step $t_2$.
The aim of this paper is to learn an optimal policy $\pi^*$ from the fixed dataset $D$ without interaction with the environment. 
 
 %During meta-testing, a (typically unseen $)$ test task $\mathcal{T}_{\text{test }}=\left(\mathcal{M}_{\text{test }}, \mu_{\text{test }}\right)$ is drawn from $p(\mathcal{T})$. A meta-episode consists of $T$ trajectories sampled from $\mathcal{T}_{\text{test }}$ with a policy $\mu_{test}$, adapting the policy to the task between trajectories, and measuring the performance on the last trajectory. Between trajectories, the adaptation procedure transforms the history of states and actions $\mathbf{h}$ from the current meta-episode into a context $\mathbf{z}=A_{\phi}(\mathbf{h})$, which is then given to the policy $\pi_{\theta}(\mathbf{a}, \mid \mathbf{s}, \mathbf{z})$ for adaptation. The exact representation of $\pi_{\theta}, A_{\phi}$, and $\mathbf{z}$ depends on the specific meta-RL method used.We consider two different meta-testing procedures. In the fully offline meta-RL setting, the meta-trained agent is presented with a small batch of experience $D_{\text{test }}$ sampled to find the highest-performing policy possible for $\mathcal{M}_{\text {test }}$.  Alternatively, in the offline meta-RL with online fine-tuning setting, the agent can perform additional online data collection and learning after being provided with the offline data $D_{\text{test }}$. Note that in both settings, if $\mu_{\text{test }} \sim \mathrm{(s a)}$ uninformative for solving $\mathcal{M}_{\text{test}}$, we might expect test performance to be affected; we consider this possibility in our experiments. Therefore, training a specific $\pi^{exp}$ to collect informative trajectory is helpful, compared with adopting a arbitrary $\mu_{\text{test}}$  . 

\paragraph{Prompt and Prompt-Tuning.}
Conditional generation tasks are common in NLP, where the input is a context $x$ and the output $y$ is a sequence of tokens. Autoregressive model $LM$ is a powerful tool to solve this kind of tasks, which concatenates the context and the output as a whole sequence $u = [x, y]$ and model the probability for the next token $u_i$ based on the previous tokens $u_{<i}$:
\begin{equation}
\begin{aligned}
     h_i &= LM(u_i, h_{<i}), \\
    p(u_i|u_{<i}) &= softmax(Wh_i),
\end{aligned}
\end{equation}
where $u_i$ denotes $i$-th token in the sequence $u$, $h_i \in \mathbb{R}^d$ denotes the activation in transformer at time step $i$, and $W$ is the learning parameter matrix. To leverage the knowledge in the pretrained large-scale model, prompts are designed to improve the few-shot performance in the downstream task. A prefix-style prompt $z$, also a sequence of tokens, are concatenated with input $u = [z,x,y]$ to guide the model to generate the desired output. Besides hand-designed prompts $z$, prompt-tuning\cite{prefix-tune} is proposed to learn a continuous prompt can be learnt from data.

%, on the other hand, are unstable and introduce artificial factors. \deleted{To address this issue, prompt-tuning \cite{prefix-tune} is proposed to learn a continuous prompt from data to replace human designing.  }\yali{it is better to discuss this in related works.}


%\yali{this paper is too short. We can merge it into methodology later.}

%\paragraph{Meta Reinforcement Learning}
%\runji{MDP and problem first.}
%In the meta reinforcement learning problem setting, we aim to leverage multi-task experience to enable fast adaptation to new downstream tasks. A task $\mathcal{T}_{i}$ is defined as a Markov decision process (MDP) $\mathcal{M}_{i} = (\mathcal{S}, \mathcal{A}, \mathcal{R}_i, \mathcal{P}_i)$, where the state space $\mathcal{S}$ and the action space $\mathcal{A}$ are shared across tasks, and $\mathcal{R}_i$ and $\mathcal{P}_i$ are task specific reward and transition function.\runji{add explain about a,s,r,p?}\yali{yes, we need to define state, action reward, and value, q function, which are used in metholodogy}


%tuple $\left(\mathcal{M}_{i}, \mu_{i}\right)$ containing a Markov decision process (MDP) $\mathcal{M}_{i}$ and a fixed, unknown behavior policy $\mu_{i}$. Each $\mu_{i}$ might be an expert policy, sub-optimal policy, or a mixture policy (e.g. corresponding the replay buffer of an $\mathrm{RL}$ agent). Tasks are drawn from a task distribution $p(\mathcal{T})=p(\mathcal{M}, \mu)$. 

%\textbf{Offline Reinforcement learning}

\section{Method}
%\liye{Grammarly fixed, pending inspection}
In this section, we present a novel offline meta RL framework called CMT which includes two parts, the basic framework with policy prompts for offline RL (in Section \ref{offline-seq}) and the extended framework with policy prompts and task prompts for offline Meta-RL (in Section \ref{offline-meta-sequence}).
%We first XXXX 
%\replaced{ We first XXXX in Section \ref{offline-seq}}{In section.\ref{offline-seq}, we present an offline learning method based on sequence modeling} 
%We then introduce XXX .
%\replaced{We then introduce XXX in Section x.x}{In section.\ref{offline-meta-sequence}, the framework is extended to offline meta reinforcement learning.}
%\deleted{The core insight is similar: interactions with the environment capture the structure of the task. The context  trajectory is used by CMT as a prompt to guide the policy in a specific unknown task.}
%In section.\ref{offline-generalization}, 
%\yali{i) We need to split the preliminaries and our methods. ii) can we use subsection title to indicate the contributions that we made? }

\subsection{Offline Sequence Learning}
\label{offline-seq}
\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.9\textwidth]{fig/offline-prompt-v5.pdf}
     \caption{The network architecture for CMT in an offline learning Setting. (a) In the representation stage, CMT pretrains an auto-encoder model in the offline dataset, which predicts the future action, reward, and state with the policy prompt from the history trajectory.  (b) In the improvement stage, we freeze the pretrianed model, and tune the prompt to guide a better policy.
     %\yali{@runji, caption need to be expanded}
     }
    \label{fig:CMT}
\end{figure}

%\deleted{We provide a solution for the offline reinforcement learning problem from a pretrain and prompt-tunning paradigm. And}  
The main insight of our method is that all the offline trajectories can be viewed as samples from unknown determininstic policies, and the optimal policy can be represented as a proper mixture of these basic policies
%\yali{how does the method below correspond to this insight???}. 
To achieve this goal, our method can be divided into two major parts: the representation stage and the improvement stage. In the representation stage, CMT learns a model to convert an offline trajectory into a policy prompt with some characteristics to represent these determininstic policies. And the aim of the improvement stage is to learn an appropriate policy prompt to mix up these basic policies by implicitly planning in the world model to guide a better policy.% \yali{i did not see connection between "to achieve this goal" and "main insight".}.

\textbf{Representation Stage.} Fig.\ref{fig:CMT} shows the whole architecture, which constitutes an auto-encoder $A$ in trajectory-level. CMT consists of two components: a trajectory encoder $A_{e}$ with parameter $\theta$ and an autoregressive generator $A_{g}$ with parameter $\phi$. Trajectory encoder $A_e$ is a bi-direction transformer \cite{bert}, which gets a history trajectory and gives the policy prompt $z_\tau$ for the trajectory  $z_{\tau} = A_{e}(\tau; \theta)$. % \yali{ $z_{\tau} = A_{e}(\tau;\theta)$ }. 
Autoregressive generator $A_g$ is a GPT-style \cite{brown2020gpt3} conditional generator, which predicts the policy prompt $z_\tau$ and the next token in the future based on the previous history trajectory:

\begin{equation}
\tau_{t+1} = A_{g}( . |z_{\tau}, \tau_{<t}; \phi).    
\end{equation}

In the representation stage, CMT introduces three loss terms to update $\theta$ and $\phi$. The major loss is supervised loss, which is used to reconstruct the whole trajectory, and two auxiliary losses help improve policy in the next stage. For the supervised loss, since $A_g$ predicts the future action, reward and state one by one, it employs as an union of a policy $\pi(a|s)= A_g(z_{\tau}, s_t,\tau_{<t})$, a dynamic model $P(s'|s, a) = A_g(z_{\tau}, \tau_{<t})$ and a reward function $R(s,a) = A_g(z_{\tau}, a_t, s_t,\tau_{<t})$. The prediction and the ground truth form a supervised loss in Eq.(\ref{eq: s_loss}):
\begin{equation}
\mathcal{L}_1({\tau};\mathbf{\phi, \theta})=\sum_{t=0}^{T-1}(
\mathcal{D}(s_t, P_{}(\tau_{<t};\phi,\theta)) +
\mathcal{D}(a_t, \pi_{}(s_t,\tau_{<t};\phi,\theta)) + 
\mathcal{D}(r_t, R_{}(a_t, s_t,\tau_{<t};\phi,\theta)),
%\log P_{\theta}({\mathbf{s}}_{t} \mid {\mathbf{s}}_{t}, {\tau}_{<t})+ 
%\log P_{\theta}({\mathbf{a}}_{t} \mid {\mathbf{a}}_{t}, {\mathbf{s}}_{t}, {\tau}_{<t})+
%\log P_{\theta}(\bar{r}_{t} \mid {\mathbf{a}}_{t}, {\mathbf{s}}_{t}, {\tau}_{<t}))
    \label{eq: s_loss}
\end{equation}
in which distance matrices $\mathcal{D}$ adopts MSE loss for deterministic output and  cross-entropy loss for stochastic prediction. Since the entire architecture is differentiable, the supervised loss can be used to update $A_{e}$ and $A_{g}$.



%Then we explain the two auxiliary losses. 
Two auxiliary losses constrain the prompt  by self-supervised learning.
Inspired by \cite{curriculumImitation}, an effective and stable policy improvement based on imitation learning often satisfy two property: (a) Keeping new behavior close to previous ones. (b) Getting higher reward than the previous ones. As we desire to improve the policy by prompt tuning, so it is natural to facilitate the similarity of prompts from similar trajectories. According to the purpose, we introduce an InfoNCE contrastive loss \cite{infoNCE} to constrain the prompt in self-supervised method to meet the aforementioned requirements. The contrastive loss is given as Eq.(\ref{eq:c_loss}):
\begin{equation}
    \mathcal{L}_2(\tau_q, \{\tau_i\}_{i=1}^{K}) =  -\log\frac{\exp(A_e(\tau_{q}; \theta) \cdot A_e(\tau_{+}; \theta) / \alpha)}{ \sum_{i=1}^k \exp(A_e(\tau_{q}; \theta) \cdot A_e(\tau_{i}; \theta) / \alpha) }
    =  -\log\frac{\exp(z_{q} \cdot z_{+} / \alpha)}{ \sum_{i=1}^k \exp(z_q \cdot z_i / \alpha) },
    \label{eq:c_loss} 
\end{equation} in which $\alpha$ is temperature coefficient. For the anchor policy prompt $z_q$ encoded from trajectory $\tau_q$, a batch of $K$ policy prompts $\{z_i\}_{i=1}^K$ encoded from a set of trajectories $\{\tau_i\}_{i=1}^K$ sampled from the offline dataset. $\{z_i\}$ consists of $K-1$ negative samples $z_{-}$ and one positive sample $z_+$. The definition of the positive and negative samples will influence the property of the policy prompt. % \liye{This sentence needs to be revised}

To make the similar behavior trajectory have a closer prompt, the first auxiliary loss is based on Eq.(\ref{eq:c_loss}) with defining the pair of policy prompts samples from the same trajectory and different trajectories as the positive and negative sample pair.
%\yali{can we give a formulation of first auxiliary loss?}. \runji{in fact ,it is eq.2,should more clear?} 
To encourage the trajectories with similar cumulative reward forms clusters, the second auxiliary loss is also based on Eq.(\ref{eq:c_loss}), but  with definition about the positive and negative sample pair. Here, the pair of policy prompts having a difference between cumulative returns within or without a threshold $u$ forms the positive and negative sample pair, and the threshold $u$ is a hyperparameter. % \liye{This sentence is too long and needs to be split}

%\yali{till now, i did not see how "representation stage" converts trajectory to prompt.}


%As a policy can be described by a stationary distribution over a set of trajectory. Or in another word, a trajectory represents partial information about a deterministic policy, and any stochastic policy can be approximated by a mixture of deterministic policies. 

% We simplify the mixture as a summation, so a behavior policy embedding can be $\hat{z}_{\pi} = \sum z_{\tau}$, and the behavior policy is 
%\begin{equation}
%\pi(a|\tau_{<t}) = A_{\pi}(.|\hat{z}_{\pi}, \tau_{<t})
%\end{equation}

\textbf{Improvement Stage.} Since the offline dataset may be sub-optimal, a policy outperforming the behavior policy should be learned, rather than imitate the offline dataset. To transfer the knowledge in the pretrained model, we introduce the prompt-tuning method for offline RL. As shown in Fig.(\ref{fig:CMT}), the key idea is simple: we can freeze the pretrained model, and learn prompts that can generate a trajectory with high reward. Specifically, the trajectory encoder $A_e$ converts a history trajectory into a policy prompt  $z_\pi$, and the autoregressive generator $A_g$ takes  a action $a \sim \pi(a|s)= A_g(z_{\pi}, s_t,\tau_{<t})$ conditioned on  $z_\pi$. However, the pretrained objective is to reconstruct, which means $z_{\pi}$ guiding the autoregressive generator to imitate the offline dataset. Therefore, we should tune the policy prompt $z_{\pi}$ to a better policy prompt $z_{\pi}'$ to generate a trajectory with high reward. In this work, we adopt reward ascent over the model predictive reward to optimize policy prompt $z_{\pi}$ in Eq.(\ref{eq:L3}).
%\yali{what is the difference to $z_q$?}in Eq.(\ref{eq:L3}).
This method can be regarded as implicitly planning in the white-box world model, which maximizes utilization of the pretrained model. %\deleted{However, online RL and offline RL loss are also possible to work, worth a exploration in the future.}

%In order to find a better policy, which measures by gain more reward. From this prospective, the predicted trajectory can be used to optimize $z_{\pi}$ by gradient ascend over the model predictive reward:and a more optimal policy prompt can gain by one step gradient ascend:
\begin{equation}
z_{\pi}' = z_{\pi} + \alpha \mathbb{E}_{\tau \sim D} [\nabla_z \sum_0^{T-1} R(a_t, s_t,\tau_{<t};\phi,\theta)]
= z_{\pi} + \alpha \mathbb{E}_{\tau \sim D} [\nabla_z \sum_0^{T-1} A_g(z_\tau, a_t, s_t, \tau_{<t};\phi)  ]
\label{eq:L3}
\end{equation}
%\yali{1- this equation implies that the second term on the rhs is zero. pls double check?? 2- if $R$ is  a function of $z$, can u explicitly write down the relation so I know that the graident is not zero?}

However, this gradient ascending method significantly slows down the inference speed because of repetition computation. To address this issue, as shown in Fig.(\ref{fig:CMT}), an adaptor layer $L$ with parameter $\xi$ approximating the gradient ascend results as $z_{\pi}' = L(z_{\pi};\xi)$ is introduced to make the algorithm more practical.  Since the back-propagation is replaced by a forward computation, the delayed time is acceptable. Therefore, we freeze the pretrained model parameter $\theta$ and $\phi$ and only tune the parameter $\xi$ for the adaptor layer $L$. The adaptor layer $L$ is training by the following Eq.(\ref{eq: r_loss}), 
\begin{equation}
    \mathcal{L}_4(\tau;\xi) =  \sum_0^{T-1} R_{}(a_t, s_t,\tau_{<t}; \bar{\phi},\bar{\theta}, \xi) + \beta ( z - L(z;\xi))^2
    \label{eq: r_loss}%\yali{the connection to eq(3) is not clear to me.}
\end{equation}
in which the second term constrains behavior changes to alleviate distribution shift in the offline setting, like \cite{TD_BC} and $\beta$ is a weight coefficient.

%\yali{add some texts to conclude the paragraph.}
\subsection{Contextual Sequence Meta Learning}
\label{offline-meta-sequence}
 The task encoder is used $F(z |\tau)$ to encode transitions into a hidden variable $z$ and learn a contextual policy $\pi(a|s, z)$ in classical context meta RL methods. Therefore, CMT is feasible to extend to meta leaning domain by simply plugging in task prompts. Fig.(\ref{fig:meta}) shows the meta learning pipeline, the minor modification support CMT have impressive performance in offline meta learning setting.  

\begin{figure}[htbp]
    \centering
    \includegraphics[width=1\textwidth]{fig/offline-meta-v2.pdf}
    \caption{The framework of CMT in Offline meta reinforcement Learning Setting. (a) Based on the basic framework in Figure.\ref{fig:CMT}, CMT introduces a context trajectory as task prompt in trajectory encoder $A_e$ to guide  $A_g$. (b) During meta test, the context buffer stores the historical interactions, which are sampled as context trajectories for CMT.
    %\yali{@runji, explanation of (b) is missing. }
     }
    \label{fig:my_label}
\end{figure}

%\liye{Grammarly fixed, pending inspection}
\textbf{Meta Training.} To contain the task information, CMT simply concatenate a context trajectory, which is a trajectory fragment coming from the same task, with the input. During offline meta-training, the context trajectory is randomly sampled from the offline dataset. To avoid information fusion, we first separate the context and history trajectories with a special token (\textsc{[SEP]}), which parameters can be learnt. Second, we adopt a contrastive learning method similar to Eq.(\ref{eq:c_loss}) to learn a stable and consistent task prompt. The major difference among contrastive loss in task prompts and policy prompts is that task prompts form positive and negative sample pair in task-level, while policy prompts 
form positive and negative sample pair in trajectory-level. 
%\runji{might add position embeding detail.}

%\liye{Grammarly fixed, pending inspection}
\textbf{Meta Test.} The context trajectory in the meta test could come from an offline dataset in an unkonwn task or a trajectory which had interacted with the online world in advance. Since the second setting is more challenging \cite{identify_offlinemeta} for the exploration problem, we evaluate CMT in the second setting. As a result, we construct a context buffer to store the history of interactions, and the context trajectories are randomly chosen from the context buffer.

%key information:1. keep the similar framework and add context component, Contrastive Loss in task-level.2. [sep], embedding detail about context and history. since the policy prompt is after the and 3. the meta test method. 

%\paragraph{Offline Exploration for Context}
%Here we learn a exploration policy, denotes as $\pi^{exp}$ and an unified sequence modeling architecture is proposed as $\pi^{task}(a| \tau_{<t},\tau^{exp})$, which based on a exploration trajectory $\tau^{exp} \sim \pi^{exp}$ sampled by exploration policy and the previous history to generate the next action. The meta learning objective can be described as following:
%\begin{equation}
%    \mathcal{L}(\pi^{exp}, \pi^{task}) = \mathbf{E}_{\tau^{exp} \sim \pi^{exp}, \mathcal{T} \sim p_{\mathcal{T}}}  [ V (\pi^{task}, \mathcal{T})].
%\end{equation}

%To optimize this objective, we could decouple exploration and exploitation. Exploration policy mainly relies on finding several key states or reward signal to identify task type. To achieve this goal, we define a intrinsic reward to measure the difference among distinct tasks and then relabel all the trajectory reward. Therefore, exploration policy can learn to generate most informative trajectory by maximize the intrinsic reward and finally identity the task.
%\begin{equation}
%\hat{r}_{t}\left(a_{t}, r_{t}, s_{t+1}, \tau_{: t} ; \mu\right)=\mathbb{E}_{\tau_{random} \sim D}[KL( \pi^{task}(s_{t+1},r_{t+1}| \tau_{<t},\mu) |\pi^{task}(s_{t+1},r_{t+1}| \tau_{<t},\tau_{random}) )],
%\end{equation}
%in which $\mu$ and $\tau^{exp}$ are  trajectories from same task.


%\begin{equation}
%    \mathcal{L}_3({\tau})= \mathcal{E}_{\tau^{exp}, \tau \sim Matching( D_{mixture}, \{D_i\} )  } [\sum_{t=0}^{T-1}( \log P_{\theta}({\mathbf{s}}_{t} \mid { {\tau}_{<t}, \tau^{exp})+ \log P_{\theta}({\mathbf{a}}_{t} \mid {\mathbf{a}}_{t}, {\mathbf{s}}_{t}, {\tau}_{<t}, \tau^{exp})+\log P_{\theta}(r}_{t} \mid {\mathbf{a}}_{t}, {\mathbf{s}}_{t}, {\tau}_{<t}, \tau^{exp}))]
%\end{equation}

%\paragraph{Modelling sequential contexts/Policy distillation via contexts}
%Then we consider the training process of exploitation policy $\pi^{task}(a| \tau_{<t},\tau^{exp})$. $\pi^{task}$ are expected to identify the task by a exploration trajectory in an unseen task and then plays the corresponding specific optimal policy. To achieve this goal, we adopt the experience relabel approach to learn sequence contextual exploitation policy. The optimal dataset $D_{optimal}$ come from the mixture of all single task dataset $D_i$ with relabling the action to the learned optimal offline policy action distribution. The mixture dataset come from the mixture of all single task dataset $D_i$ without relabling. We randomly match  a same-task pair of trajectories $(\tau^{exp}, \tau^{opt}) $ in dataset $ D_{mixture}, \{D_i\} $, and combine the pair as a integrated sequence to train an auto-regressive sequence model. Since the matching pair comes from the same task, the policy automatically learn to play specific policy in specific task to minimize  the loss. The objective function is as following:




%\subsection{Downstream generalization}
%\label{offline-generalization}
%MAE for CTCE \& CTDE 
%MAE for planning.
%MAE for goal-conditional 
%MAE for offline 


\section{Experiment}
In this section, we evaluate the performance of CMT in terms of offline RL tasks in D4RL benchmarks, offline meta RL tasks in meta Mujoco benchmarks, and multi-agent cooperative and adversarial game tasks on StarCraft II Micromanagement. CMT is compared with proper state-of-the-art baselines, and almost all the tasks achieve comparable or better performance. The detail about hyper-parameter lists is in Appendix. \ref{app:hyper}.Our experiments are conducted on a server with Nvidia Tesla A100 GPU and AMD EPYC 7742 CPU.

\subsection{ Offline Learning Tasks}
We evaluate CMT on the continuous control tasks from D4RL benchmarks. The experiments on four standard Mujoco locomotion environments (HalfCheetah, Hopper, Walker, and Ant) are conducted with three kinds of dataset quality (Medium, Medium-Replay, and Medium-Expert). The differences between them are as follows:
\begin{itemize}
    \item [(1)] Medium: Containing 1 million timesteps generated by a "medium" policy interacting with the environment, with the intelligence level of around 1/3 that of experts.
    \item [(2)] Medium-Replay: The replay buffer generated during the medium policy training process, and about 25k-400k timesteps are included in the tested environments.
    \item [(3)] Medium-Expert: Consisting of 1 million timesteps generated by the medium policy concatenated with another 1 million timesteps generated by the expert policy.
\end{itemize}

Five baselines are considered, including behaviour cloning (BC) \cite{BC}, behavior regularized ActorCritic (BRAC) \cite{BRAC}, conservative Q-learning (CQL) \cite{CQL}, implicit Q-learning (IQL) \cite{IQL}, and decision transformer (DT) \cite{DT_2021}. 
BC realizes intelligence by learning from expert datasets, which is actually a supervised learning process that learns the states to predict actions. Since severe extrapolation errors caused by the policy evaluation, traditional offline RL algorithms perform poorly. And the methods such as BCQ, BRAC, and IQL, avoid extrapolation errors by constraining the behavior space. While CQL solves it by finding a conservative Q function that keeps the policy function's expected value less than the true value.
Starting from another perspective, DT transforms the RL problems into sequence modeling problems, and attempts to find the optimal actions. 

The results for D4RL datasets are shown in Table. \ref{D4RL-results}, CMT performs excellently on the Medium and Medium-expert datasets, but not so well on the Medium-replay dataset, indicating that CMT prefers to learn from data generated by stable policies. Compared with DT, which is also the transformer-based structure, CMT outperforms it in most of the tasks. What's more, although CQL is the SOTA algorithm currently, the performance of CMT on Medium and Medium-expert datasets meets or exceeds it, demonstrating that our method has huge potential.

\begin{table}[htbp]
\caption{Results for D4RL datasets. Here we report the mean for three seeds, and the reward is normalized by an export policy and a random policy as 100 and 0. }
\begin{tabular}{@{}clllllll@{}}
\toprule
Dataset       & Environment & CMT(ours) & DT    & BRAC-v & CQL   & IQL & BC   \\ \midrule
Medum-Expert  & halfcheetah & 91.0      & 88.0  & 41.9   & \textbf{91.6}  & 86.7  & 65.6 \\
Medum-Expert  & hopper      & \textbf{110.8}     & 103.3 & 0.8    & 105.4 & 91.5  & 55.4 \\
Medum-Expert  & walker      & 103.3     & 108.4 & 81.6   & 108.8  & \textbf{109.6}  & 11.2 \\
Medum-Expert  & ant         & \textbf{94.9}      & 89.3  & -      & -     & -  & 71.2 \\ \midrule
Medium        & halfcheetah & 43.0      & 42.1  & 46.3   & 44.0  & \textbf{47.4}   & 41.6 \\
Medium        & hopper      & \textbf{69.3}      & 62.0  & 31.1   & 58.5  & 66.3   & 48.6 \\
Medium        & walker      & 63.3      & 71.6  & 81.1   & 72.5  & \textbf{78.3}   & 47.8 \\
Medium        & ant         & \textbf{65.6}      & 64.6  & -      & -     &  -   & 63.7 \\ \midrule
Medium-replay & halfcheetah & 27.0      & 36.3  & \textbf{47.7}   & 45.5  & 44.2   & 2.2  \\
Medium-replay & hopper      & 31.1      & 67.8  & 0.6    & \textbf{95.0}  & 94.7   & 30.8 \\
Medium-replay & walker      & 58.3      & 47.8  & 0.9    & 26.7  & \textbf{73.9}   & 5.9  \\
Medium-replay & ant         & 53.2      & \textbf{61.7}  & -      & -     &  -   & 30.1 \\ \bottomrule
\end{tabular}
\label{D4RL-results}
\end{table}

\subsection{Offline Meta Learning Tasks}
We explore four tasks to evaluate CMT on zero-shot generalization: Half-Cheetah-Vel, Ant-Fwd-Back, and Ant-Fwd-Back. The same data collection method is used as described in the literature \cite{MADT}. The following baselines are taken into account:
\begin{itemize}
    \item [(1)] Batch PEARL \cite{PEARL}: A modified version of PEARL which can be used for offline RL tasks.
    \item [(2)] Contextual BCQ (CBCQ) \cite{BCQ}: An advanced version of the BCQ that has been adapted to offline RL tasks by incorporating latent variables into state information.
    \item [(3)] MBML \cite{MBML}: A multi-task offline RL method with metric learning.
    \item [(4)] FOCAL \cite{FOCAL}: A model-free offline Meta-RL method with state-of-the-art performance based on the deterministic context encoder.
    \item [(5)] MerPO \cite{MerPO}: A model-based offline Meta-RL method with regularized policy optimization which learns a task structure meta-model and an OOD data exploration meta-policy.
\end{itemize}
These baselines are trained on a set of offline RL tasks and are tested on the set of unseen offline RL tasks.

The results for meta Mujoco environments are shown in Fig. \ref{fig:meta}. As we can see, CMT can outperform most baselines, including CBCQ, batch PEARL, and MBML. Besides, FOCAL and MerPO are the best-performing algorithms currently, while CMT can match or even outperform them in different tasks, showing that our algorithm also has great potential in the area of offline meta RL. \liye{Need to consider how to describe the comparison of CMT and FOCAL, MerPO results.}

\begin{figure}[htbp]
    \centering
    \subfigure{
        \includegraphics[width=2.2in]{fig/offline_single_map_Ant-Fwd-Bwd_v2.pdf}
    }
    \subfigure{
	\includegraphics[width=2.2in]{fig/offline_single_map_Half-Cheetah-Fwd-Bwd_v2.pdf}
    }
    \quad    %用 \quad 来换行
    \subfigure{
    	\includegraphics[width=2.2in]{fig/offline_single_map_Point-Robot-Wind_v2.pdf}
    }
    \subfigure{
	\includegraphics[width=2.2in]{fig/offline_single_map_Walker-2D-Params_v2.pdf}
    }
    \caption{Results for Meta Mujoco Environemnt. \yali{we need captions to explain the results.}
    }
    \label{fig:meta}
\end{figure}

\subsection{Multi-Agent Offline Learning tasks}
In this subection, we evaluate the performance of CMT on multi-agent offline learning setting in SMAC benchmarks in 20 maps. For the data collection, we follow the same method in literature in \cite{MADT}. The datasets are built from trajectories generated by MAPPO on the SMAC tasks, and a large number of trajectories $\tau : = ({s_t},{o_t},{a_t},{r_t},don{e_t},{v_t})_{t = 1}^T$ are contained in each of them, where $v_t$ denotes the available action. Different from D4RL, the properties of the DecPOMDP, the local observations and available actions, are also considered in our datasets.

The BC \cite{BC}, CQL-MA \cite{CQL}, and ICQ-MA \cite{ICQ} are utilized as baselines to show the performance of our solution, and their original models own good performances in single-agent offline RL tasks. The properties of the multi-agent versions are the same as the single-agent versions. BC learns by building the state-to-action mapping. Based on the traditional multi-agent offline RL methods, ICQ-MA and CQL-MA solve the extrapolation error problem through action-space constraint and value pessimism, respectively.

The results on four maps are displayed in Fig. \ref{fig:SMAC} to demonstrate the performance of algorithms on tasks of varying difficulty (Very hard: $MMM2$, $corridor$; Hard: $3s\_vs\_5z$; Easy: $8m$). More results on StarCraft II can be found in Appendix. The CMT outperforms the baselines and achieves state-of-the-art performance in all maps, indicating that our algorithm has strong robustness and highly efficiency. While ICQ-MA and CQL-MA perform poorly due to different dataset formats and extrapolation errors. What's more, it should be noted that the BC works well since the approximate expert datasets are used in training progress.

%\begin{figure}[h]
%    \centering
%    \includegraphics[width=1\textwidth]{fig/offline_single_map_newv2.pdf}
%    \caption{Results for SMAC in 22 maps. Compared with ICQ, CQL, BC.}
%    \label{fig:my_label}
%\end{figure}

\begin{figure}[htbp]
    \centering
    \subfigure{
        \includegraphics[width=2.2in]{fig/offline_single_map_3s_vs_5z.pdf}
    }
    \subfigure{
	\includegraphics[width=2.2in]{fig/offline_single_map_8m.pdf}
    }
    \quad    %用 \quad 来换行
    \subfigure{
    	\includegraphics[width=2.2in]{fig/offline_single_map_corridor.pdf}
    }
    \subfigure{
	\includegraphics[width=2.2in]{fig/offline_single_map_MMM2.pdf}
    }
    \caption{Results for SMAC in representative four maps, compared with ICQ, CQL, BC. All results on 20 maps can be found in the appendix. \yali{caption has grammar mistakes
    }
    }
    \label{fig:SMAC}
\end{figure}

%\begin{figure}
%    \centering
%    \includegraphics[width=1\textwidth]{fig/offline_single_map_ablation_study.pdf}
%    \caption{Caption}
%    \label{fig:my_label}
%\end{figure}

\section{Conclusions}
In this paper, we present CMT, a novel offline RL algorithm based on prompt tuning, with the goal of training a big model that can be utilized on various downstream tasks from the sequence modeling perspective. The prompt turning is designed for offline RL to pre-train the model and guide the autuoregressive model to generate a trajactory with high reward. Besides, a variety of experiments are conducted in three different RL settings, offline single-agent RL (D4RL), offline Meta-RL (MuJoCo), and offline MARL (SMAC), and the model's performance is evaluated with different baselines. The results show that CMT has strong performance, high computation efficiency, and generality. To our best knowledge, CMT is also the first sequence-modeling-based algorithm for offline meta-RL problems. Obviously, general decision models like CMT enhance the efficiency and security of model training while also lowering the threshold for the applications of RL algorithms. However, considering that they may cause potential problems such as massive unemployment in labor-type jobs or even the development of powerful war AI, reasonable and well-regulated policies are necessary to guarantee that AI technologies are actively employed and healthy developed.
\liye{Need to check the accuracy of the model description}


\bibliographystyle{plainnat} 
\bibliography{nips2022_conference}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\section*{Checklist}

%%% BEGIN INSTRUCTIONS %%%
The checklist follows the references.  Please
read the checklist guidelines carefully for information on how to answer these
questions.  For each question, change the default \answerTODO{} to \answerYes{},
\answerNo{}, or \answerNA{}.  You are strongly encouraged to include a {\bf
justification to your answer}, either by referencing the appropriate section of
your paper or providing a brief inline description.  For example:
\begin{itemize}
  \item Did you include the license to the code and datasets? \answerYes
  \item Did you include the license to the code and datasets? \answerNo{The code and the data are proprietary.}
  \item Did you include the license to the code and datasets? \answerNA{}
\end{itemize}
Please do not modify the questions and only use the provided macros for your
answers.  Note that the Checklist section does not count towards the page
limit.  In your paper, please delete this instructions block and only keep the
Checklist section heading above along with the questions/answers below.
%%% END INSTRUCTIONS %%%

\begin{enumerate}

\item For all authors...
\begin{enumerate}
  \item Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope?
    \answerYes{}
  \item Did you describe the limitations of your work?
    \answerTODO{}
  \item Did you discuss any potential negative societal impacts of your work?
    \answerYes{}
  \item Have you read the ethics review guidelines and ensured that your paper conforms to them?
    \answerYes{}
\end{enumerate}

\item If you are including theoretical results...
\begin{enumerate}
  \item Did you state the full set of assumptions of all theoretical results?
    \answerNA{}
	\item Did you include complete proofs of all theoretical results?
    \answerNA{}
\end{enumerate}

\item If you ran experiments...
\begin{enumerate}
  \item Did you include the code, data, and instructions needed to reproduce the main experimental results (either in the supplemental material or as a URL)?
    \answerYes{}
  \item Did you specify all the training details (e.g., data splits, hyperparameters, how they were chosen)?
    \answerTODO{}
	\item Did you report error bars (e.g., with respect to the random seed after running experiments multiple times)?
    \answerYes{}
	\item Did you include the total amount of compute and the type of resources used (e.g., type of GPUs, internal cluster, or cloud provider)?
    \answerYes{}
\end{enumerate}

\item If you are using existing assets (e.g., code, data, models) or curating/releasing new assets...
\begin{enumerate}
  \item If your work uses existing assets, did you cite the creators?
    \answerNA{}
  \item Did you mention the license of the assets?
    \answerNA{}
  \item Did you include any new assets either in the supplemental material or as a URL?
    \answerNA{}
  \item Did you discuss whether and how consent was obtained from people whose data you're using/curating?
    \answerNA{}
  \item Did you discuss whether the data you are using/curating contains personally identifiable information or offensive content?
    \answerNA{}
\end{enumerate}

\item If you used crowdsourcing or conducted research with human subjects...
\begin{enumerate}
  \item Did you include the full text of instructions given to participants and screenshots, if applicable?
    \answerNA{}
  \item Did you describe any potential participant risks, with links to Institutional Review Board (IRB) approvals, if applicable?
    \answerNA{}
  \item Did you include the estimated hourly wage paid to participants and the total amount spent on participant compensation?
    \answerNA{}
\end{enumerate}

\end{enumerate}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\appendix

\section{Appendix}

Optionally include extra information (complete proofs, additional experiments and plots) in the appendix.
This section will often be part of the supplemental material.
\subsection{Hyper-Parameter}
\label{app:hyper}
\liye{Need to be filled}
\begin{table}[H]
    \centering
    \begin{tabular}{@{}lll@{}}
        \toprule
        \textbf{Parameter    } & \textbf{Standard Configuration    } & \textbf{Meta-World    } \\ \midrule
        A                      & B                                   & C                       \\
        A                      & B                                   & C                       \\ \bottomrule
    \end{tabular}
    \caption{caption}
    \label{label}
\end{table}

\subsection{Full resluts on SMAC}
\begin{figure}[htbp]
    \centering
    \includegraphics[width=1\textwidth]{fig/offline_single_map_newv4.pdf}
    \caption{Results for SMAC in 20 maps. Compared with ICQ, CQL, BC.}
    \label{fig:smac_all}
\end{figure}

\section{Discard}
1- compare different trajectory
2- gradient transfer zero-shot
\begin{itemize}
    \item new offline learning method. by compare different trajectory.
    \item easy to transfer from offline to online.
    \item how to improve generalization: 
    \begin{enumerate}
        \item different mask as regularization to improve performance in zero-shot tasks.
        \item representation alignment to improve few-shot learning.
        \item input as a sequence to deal with variable agents.
        \item (optimal) conditional generative model based on contrasting task representation learning.
    \end{enumerate}
\end{itemize}
agent mask: hidden motivation inference and CTDE
reward/state mask: world transition
action mask: policy estimate
inverse state mask: inverse planning

\end{document}
