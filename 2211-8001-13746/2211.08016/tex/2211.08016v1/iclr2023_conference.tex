
\documentclass{article} % For LaTeX2e
\usepackage{iclr2023_conference,times}

% Optional math commands from https://github.com/goodfeli/dlbook_notation.
\input{math_commands.tex}

\usepackage{hyperref}
\usepackage{url}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{makecell}
\usepackage{float}
\usepackage{booktabs}       % professional-quality tables
\newcommand{\yali}[1]{{ \color{magenta}[Yali: #1]}}
\newcommand{\runji}[1]{{ \color{blue}[Runji: #1]}}
\newcommand{\liye}[1]{{ \color{red}[liye: #1]}}
\usepackage{changes}


\title{Contextual Transformer for  Offline Meta\\ Reinforcement Learning}
%\texttt{\{linrunji2021@ia.ac.cn\}}
% Authors must not appear in the submitted version. They should be hidden
% as long as the \iclrfinalcopy macro remains commented out below.
% Non-anonymous submissions will be rejected without review.
\iclrfinalcopy 
\author{Runji Lin  \\
School of Artificial Intelligence,\\
University of Chinese Academy of Sciences\\
\And
Ye Li$^\dag$ \\
Institute for AI,\\
Peking University \\ 
\And
Xidong Feng\\
University College London\\
\And
Zhaowei Zhang$^\dag$\\
Institute for AI,\\
Peking University \\
\And
Xian Hong Wu Fung$^\dag$\\
Institute for AI,\\
Peking University\\
\And
Haifeng Zhang\\
Institute of Automation,\\
Chinese Academy of Sciences\\
\And
Jun Wang\\
University College London\\
\And
Yali Du\\
King's College London \\
\And
Yaodong Yang\thanks{ Corresponding to: yaodong.yang@pku.edu.cn.  $^\dag$Work done as research intern at Peking University.}\\
Institute for AI,\\
Peking University\\
}


% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to \LaTeX{} to determine where to break
% the lines. Using \AND forces a linebreak at that point. So, if \LaTeX{}
% puts 3 of 4 authors names on the first line, and the last on the second
% line, try using \AND instead of \And before the third author name.

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

%\iclrfinalcopy % Uncomment for camera-ready version, but NOT for submission.
\setlength {\marginparwidth }{2cm} 
\begin{document}


\maketitle

\begin{abstract}
The pretrain-finetuning paradigm in large-scale sequence models has made significant progress in natural language processing and computer vision tasks.  However, such a paradigm is still hindered by several challenges in Reinforcement Learning (RL), including the lack of self-supervised  pretraining algorithms based on offline data and efficient fine-tuning/prompt-tuning over unseen downstream tasks. In this work, we explore how prompts can improve  sequence modeling-based offline reinforcement learning (offline-RL) algorithms. Firstly, we propose prompt tuning for offline RL, where a context vector sequence is concatenated with the input to guide the conditional policy generation. As such, we can pretrain a model on the offline dataset with self-supervised loss and learn a prompt to guide the policy towards desired actions. Secondly, we extend our framework to  Meta-RL settings and propose Contextual Meta Transformer (CMT); CMT leverages the context among different tasks as the prompt to improve generalization on unseen tasks. We conduct extensive experiments across three different offline-RL settings: offline single-agent RL on the D4RL dataset, offline Meta-RL on the MuJoCo benchmark, and offline MARL on the SMAC benchmark. Superior results validate the strong performance, and generality of our methods.
\end{abstract}
\section{Introduction} 


%\yali{1st: DT shines in sequential decision making task, [cite dt, trajectory transformer, etc.]...question, whether pretrain enables RL agents to harness knowledge for generalization?...}
Reinforcement learning algorithms based on sequence modeling \citep{DT_2021,TT_2021, gato} shine in sequential decision-making tasks and form a new promising paradigm. Compared with classic RL methods, such as policy-based methods and value-based methods \citep{RLintro}, optimization of the policies from the sequence prospective has advantages in long-term credit assignment, partial observation, etc. 
%\deleted{However previous works mainly focus on single agent, related issues are still in the exploratory stage in multi-agent settings.} \yali{we may not claim too much contribution on multi-agent side.}
Meanwhile, significant generalization of large pretrained sequence model in natural language processing \citep{bert, brown2020gpt3} and computer vision \citep{SwinTransformer, scalingVIT} not only conserves vast computation in downstream tasks but also alleviates the large data quantity requirements. Inspired by them, we want to ask the question: \textit{ whether pretrain technique has a similar power in RL?} Since limited and expensive interactions impede the deployment of RL in various valuable applications \citep{levine2020offline}, pretraining a large model to improve the robustness of real-world gap by a zero-shot generalization and improve data efficiency by few-shot learning offers great significance. \citep{Xland,MADT}
%\yali{the bibtex has mistakes in Xland.} 
pretrains a single model with diverse and abundant training tasks in the decision-making domain to generalize in downstream tasks, which proves the feasibility that pretraining enables RL agents to harness knowledge for generalization.

%\yali{2nd paragraph. two major challenges are **maybe mention shortcomings of DT, TT,**}
Earlier works on sequence modeling RL provide a new perspective on offline RL. However,  extending these methods to the pretrain domain is still confronted with several challenges. One major challenge for generalization \citep{FOCAL} is how to  encode task-relevant information, thereby enhancing transferring knowledge among tasks. Since discovering the relationship among diverse tasks from data and making decisions conditioned on distinct tasks plays a significant role in generalization, it is not a trivial modification of existing methods. Another problem is efficient self-supervised learning in offline RL. Specifically, the decision transformer \citep{DT_2021} leverages the data to learn a return conditioned policy, which ignores the knowledge about world dynamics. In addition, trajectory transformer \citep{TT_2021} conducts planning based on a world model, but the high computational intensity and decision latency might be a bottleneck for a large-scale model and hard to fine-tune to other tasks. Therefore, introducing key components to transfer the knowledge in a pretrained model and incorporating the advantages in a conditioned policy and a world model is necessary. %\yali{what is "key component"? what are "advantages"}

%Moreover, online multi-task reinforcement learning suffers from the potential conflicting gradient with high variance, leading to the failure in several high-distinct tasks. For this consideration, the distillation among several small multi-task networks and a unified large model is a promising approach. By replacing the high-variance reinforcement loss by much stable supervised loss, a large model distilled from several expert small networks has larger capacity to master tasks, which facilitates the data efficiency in new tasks. On the other hand, a small model distilled from a large model in a new task saves large-scale computation resources and enjoys the fast adaptation advantage.\runji{polish the multi-task meaning.} 



%\liye{Grammarly fixed, pending inspection}
In this work, we propose a novel offline RL algorithm, named \textbf{C}ontextual \textbf{M}eta \textbf{T}ransformer (CMT), aiming to conquer multiple tasks and generalization at one shot in an offline setting from the perspective of sequence modeling. CMT provides a pretrain and prompt-tuning paradigm to solve offline RL problems in the offline setting. Firstly, a model is pretrained on the offline dataset through a self-supervised learning method, which converts the offline trajectories into some policy prompts and utilizes these policy prompts to reconstruct the offline trajectories in the autoregressive style. Then a better policy prompt is learned based on planning in the learned world model to attain the advanced policy to generate trajectories with high rewards. In contrast to previous work, CMT learns a prompt to construct policy to guide desired actions, rather than being designed by humans or explicitly planned by the world model. In the offline meta-learning setting, CMT extends the framework by simply concatenating a task prompt with the input sequence. With a simple modification, CMT is capable of executing a proper policy for a specific task and sharing knowledge among tasks.

%encode the policy as a hidden context variable \yali{encode policy or task?}. The main insight is that the distribution of trajectory sequence is partly determined by behavior policy\yali{what is the connection between our ''insights'' and two challenges above?}. Therefore, by leveraging the offline experience to learn some basis policy contexts, a better policy context is generated by these basis contexts and a world model learned from offline data. 





%To alleviate the information distortion\yali{what is this? }, we introduce a large sequence model to predict the future action, which directly input the exploration trajectory and the expert trajectory to avoid learn a explicit task context \yali{this sentence is not clear to me}. 
%Then a exploration policy is trained to collect more informative trajectory. To achieve this purpose, the predicted error among distinct tasks is employed as an intrinsic reward to guide exploration policy to identify key states. 
%We demonstrate the significant performance of CMT on various challenging benchmarks. [ SMAC, meta-world] [offline performance, cross-map performance, ]


%\liye{We need to expand on the main contribution, this part is a bit short}
Our contributions are three-fold:
First, we propose a novel offline RL algorithm based on prompt tuning, in which the offline trajectory is encoded as a prompt, and the appropriate prompt is found to lead a policy for execution to achieve high reward in the online environment.
Second, CMT is the first algorithm to solve offline meta-RL from a sequence modeling perspective. The context trajectory, which represents the structure of the task, is used by CMT as a prompt to guide the policy in a specific unknown task. 
Furthermore, empirical results on D4RL datasets and meta Mujuco tasks show that CMT has outstanding performance and strong generalization.  
%\replaced{Empirical results on XXX and xxx tasks/datasets show that CMT xxx}{Third, CMT exhibits outstanding performance in offline learning settings in d4rl \yali{D4RL} benchmarks, as well as a strong generalization in meta-learning settings in meta Mujuco benchmarks.}
%\replaced{Furthermore, we show that CMT can be easily extended to solve XXmulti-agent XXX tasks and is evaluated in SMAC XXX}{What's more, it is also easy to extend to multi-agent environments, whose effectiveness is fully shown in SMAC tasks.}
% \begin{enumerate}
    % \item 
     %\yali{"for the best policy" is not clear to me} . %\yali{two ``rather than''? }.
    % \item 
     
    % Third, CMT shows impressive performances in offline learning settings in gym Mujoco benchmarks, strong generalization in meta learning settings in meta Mujuco benchmarks.  This framework is easy to extend to multi-agent settings, and evaluation on SMAC demonstrates the efficacy of CMT.
% \end{enumerate}



\section{Related Work}
% \yali{notations}
\textbf{Offline Reinforcement Learning.} 
Offline RL is gaining popularity as a data-driven RL method that can effectively utilize large offline datasets. However, the data distribution shift and hyper-parameter tuning in offline settings seriously affect the performance of the agent \citep{Sergey}. So far, several schemes have been proposed to address them. Through action-space constraint, BCQ \citep{BCQ}, AWR \citep{AWR}, BRAC \citep{BRAC}, and ICQ \citep{ICQ} reduce extrapolation error caused by policy iteration. Noticing the problem of overestimation of values, CQL \citep{CQL} keeps reasonable estimates by looking for pessimistic expectations. UWAC \citep{UWAC} handles out-of-distribution (OOD) data by weighting the Q value during training by estimating the uncertainty of $(s, a)$. MOPO \citep{MOPO} and MOReL \citep{MOREL} solve the offline RL problem from the model-based perspective while ensuring rational control by adding penalty items to uncertain areas. Decision Transformer (DT) \citep{DT_2021} and Trajectory Transformer (TT) \citep{TT_2021} reconstruct the RL problem into a sequential decision problem, extending the Large-Language-Model-like (LLM-like) structure to the RL area, which inspires many follow-up works on them. However, the relevant works on offline RL are still insufficient due to the lack of self-supervised large-scale pretraining methods and efficient prompt-tuning over unseen tasks, and CMT proposes a pretrain-and-tune paradigm to deal with them.

%\textbf{offline to online}
%- AWAC: 
%- Offline reinforcement learning with implicit q-learning
%- AW-opt: Learning robotic skills with imitation and reinforcement at scale.
%- Offline-to-online reinforcement learning via balanced replay and pessimistic q-ensemble.
%- MOORe: Model-based Offline-to-Online Reinforcement Learning

\textbf{Pretrain and Sequence Modeling.}
Recently, much attention has been attracted to pretraining big models on large-scale unsupervised datasets and applying them to downstream tasks through fine-tuning. In language process tasks, transformer-based models such as BERT \citep{bert}, GPT-3 \citep{brown2020gpt3} overcome the limitation that RNN cannot be trained in parallel and improve the ability to use long sequence information, achieving SOTA results on NLP tasks such as translation, question answering systems. Even the CV field has been inspired to reconstruct their issues as sequence modeling problems, and high-performance models like the swin transformer \citep{SwinTransformer} and scaling ViT \citep{scalingVIT} have been proposed. Since the trajectories in offline RL datasets have Markov properties, they can be modeled through transformer-like structures. Decision transformer \citep{DT_2021} and trajectory transformer \citep{TT_2021} propose condition policy on return to go (RTG) and behavior cloning policy improved by beam search to solve RL problems in offline settings respectively. MAT\citep{MAT} introduces sequence modeling into online MARL setting, and demonstrates high data efficiency in transfer. Inspired by these works, we bring prompt tuning from NLP into the RL domain, then propose a potential path to pretrain a large-scale RL model and efficiently transfer the knowledge to downstream tasks. 
%on the other hand, are unstable and introduce artificial factors. \deleted{To address this issue, prompt-tuning \cite{prefix-tune} is proposed to learn a continuous prompt from data to replace human designing.  }\yali{it is better to discuss this in related works.}

\textbf{Offline meta-RL and Task Generalization.}
Offline meta-RL shines recently since it allows algorithms to adapt to new tasks quickly without interacting with the environment. Targeting it, an optimization-based method with advantage weighting loss called MACAW \citep{MACAW} is proposed, which learns the initialization of both the value function and the policy. FOCAL \citep{FOCAL} combines the deterministic context encoder with behavior regularization and achieves inspiring results based on an off-policy Meta-RL method called PEARL \citep{PEARL}. Then it is improved by combining the intra-task attention mechanism and the inter-task contrastive learning objective, which is named FOCAL++ \citep{FOCAL++}, to deal with sparse reward and distribution shift. BOReL \citep{BOReL} aims to learn Bayesian optimal policies from discrete data for the mentioned problems, whereas SMAC \citep{SMAC} learns meta-policies from reward-labeled data and then fine-tunes on new tasks. From the model-based perspective, MerPO \citep{MerPO} proposes a meta-model for efficient task structure inference and a meta-policy for safe exploration of OOD data. 
Prompt-DT \citep{xu2022prompting} introduces prompt into decision transformer to achieve quick adaptation, however it lacks of effective design to support prompt tuning paradigm.
It is worth mentioning that recent work on general model construction, such as SayCan \citep{SayCan}, and Gato \citep{gato}, has achieved exciting results, demonstrating the huge potential of LLM-like architectures. Just like them, CMT is also a general LLM-like model that can solve offline meta-RL problems effectively.



\section{Preliminary}
\paragraph{Meta Reinforcement Learning.}  The major purpose of meta-RL is to leverage multi-task experience to enable fast adaptation to new unseen tasks. A task $\mathcal{T}_{i}$ is defined as a Markov Decsion Process (MDP) $\mathcal{T}_{i} = (\mathcal{S}, \mathcal{A}, \mathcal{R}, \mathcal{P}, \lambda)$, where $\mathcal{S}$ is the state space, $\mathcal{A}$ is the action space, $\mathcal{R}$ is reward function, and $\mathcal{P}$ is transition function.
In deep RL, the policy $\pi_\theta(a_t|s_t)$, which specifies the probability that the agent takes action $a_t$ in state $s_t$ at time $t$, is described by a neural network with parameters $\theta$.
The goal in a MDP is to learn a optimal policy $\pi^* = \arg \max_{\pi}  \mathbb{E}_{s_0,a_0, s_1, a_1, \dots} [\sum^{\infty}_{t=0} \lambda^t r(s_t, a_t) ]$ which can maximize the expected discounted return, where $\lambda$ is a discounted factor. In meta-RL, tasks are drawn from a task distribution  $\mathcal{T}_i \sim p(\mathcal{T})$, the state space $\mathcal{S}$ and the action space $\mathcal{A}$ are common across tasks, and reward function $\mathcal{R}_i$ and transition function $\mathcal{P}_i$ are task specific. During meta-training, the meta policies are trained based on some training tasks sampled from task distribution to achieve fast adaptation to new unseen tasks in meta tests.  


\paragraph{Offline Reinforcement learning.}
 In offline RL setting, the trajectory dataset $D$  is collected from unknown behavior policy $\mu$, which might be an expert policy, sub-optimal policy, random policy, or a mixture policy (e.g. corresponding the replay buffer of an $\mathrm{RL}$ agent).  A offline trajectory $\tau$ consists of states, actions, and scalar rewards: $\tau= \{\mathbf{s}_{t},\mathbf{a}_{t}, r_{t}\}_{t=0}^{T-1}$. A trajectory fragment $\tau_{[t_1:t_2]}$ denotes transitions from time-step $t_1$ to time-step $t_2$.
 This paper aims to learn an optimal policy $\pi^*$ from the fixed dataset $D$ without interaction with the environment. 
 
 %During meta-testing, a (typically unseen $)$ test task $\mathcal{T}_{\text{test }}=\left(\mathcal{M}_{\text{test }}, \mu_{\text{test }}\right)$ is drawn from $p(\mathcal{T})$. A meta-episode consists of $T$ trajectories sampled from $\mathcal{T}_{\text{test }}$ with a policy $\mu_{test}$, adapting the policy to the task between trajectories, and measuring the performance on the last trajectory. Between trajectories, the adaptation procedure transforms the history of states and actions $\mathbf{h}$ from the current meta-episode into a context $\mathbf{z}=A_{\phi}(\mathbf{h})$, which is then given to the policy $\pi_{\theta}(\mathbf{a}, \mid \mathbf{s}, \mathbf{z})$ for adaptation. The exact representation of $\pi_{\theta}, A_{\phi}$, and $\mathbf{z}$ depends on the specific meta-RL method used.We consider two different meta-testing procedures. In the fully offline meta-RL setting, the meta-trained agent is presented with a small batch of experience $D_{\text{test }}$ sampled to find the highest-performing policy possible for $\mathcal{M}_{\text {test }}$.  Alternatively, in the offline meta-RL with online fine-tuning setting, the agent can perform additional online data collection and learning after being provided with the offline data $D_{\text{test }}$. Note that in both settings, if $\mu_{\text{test }} \sim \mathrm{(s a)}$ uninformative for solving $\mathcal{M}_{\text{test}}$, we might expect test performance to be affected; we consider this possibility in our experiments. Therefore, training a specific $\pi^{exp}$ to collect informative trajectory is helpful, compared with adopting a arbitrary $\mu_{\text{test}}$  . 

\paragraph{Prompt and Prompt-Tuning.}
Conditional generation tasks are common in NLP, where the input is a context $x$ and the output $y$ is a sequence of tokens. Autoregressive model $\operatorname{LM}$ \citep{brown2020gpt3} is a powerful tool to solve this kind of tasks, which concatenates the context and the output as a whole sequence $u = [x, y]$ and models the probability for the next token $u_i$ based on the previous tokens $u_{<i}$:
\begin{equation}
\begin{aligned}
     h_i &= \operatorname{LM}(u_i, h_{<i}), \\
    p(u_i|u_{<i}) &= \operatorname{softmax}(Wh_i),
\end{aligned}
\end{equation}
where $u_i$ denotes $i$-th token in the sequence $u$, $h_i \in \mathbb{R}^d$ denotes the activation in transformer at time step $i$, and $W$ is the learning parameter matrix. To leverage the knowledge in the pretrained large-scale model, prompts are designed to improve the few-shot performance in the downstream task. A prefix-style prompt $z$, also a sequence of tokens, are concatenated with input $u = [z,x,y]$ to guide the model to generate the desired output. Besides hand-designed prompts $z$, prompt-tuning\citep{prefix-tune} is proposed to learn a continuous prompt that can be learnt from data.



%, on the other hand, are unstable and introduce artificial factors. \deleted{To address this issue, prompt-tuning \citep{prefix-tune} is proposed to learn a continuous prompt from data to replace human designing.  }\yali{it is better to discuss this in related works.}


%\yali{this paper is too short. We can merge it into methodology later.}

%\paragraph{Meta Reinforcement Learning}
%\runji{MDP and problem first.}
%In the meta reinforcement learning problem setting, we aim to leverage multi-task experience to enable fast adaptation to new downstream tasks. A task $\mathcal{T}_{i}$ is defined as a Markov decision process (MDP) $\mathcal{M}_{i} = (\mathcal{S}, \mathcal{A}, \mathcal{R}_i, \mathcal{P}_i)$, where the state space $\mathcal{S}$ and the action space $\mathcal{A}$ are shared across tasks, and $\mathcal{R}_i$ and $\mathcal{P}_i$ are task specific reward and transition function.\runji{add explain about a,s,r,p?}\yali{yes, we need to define state, action reward, and value, q function, which are used in metholodogy}


%tuple $\left(\mathcal{M}_{i}, \mu_{i}\right)$ containing a Markov decision process (MDP) $\mathcal{M}_{i}$ and a fixed, unknown behavior policy $\mu_{i}$. Each $\mu_{i}$ might be an expert policy, sub-optimal policy, or a mixture policy (e.g. corresponding the replay buffer of an $\mathrm{RL}$ agent). Tasks are drawn from a task distribution $p(\mathcal{T})=p(\mathcal{M}, \mu)$. 

%\textbf{Offline Reinforcement learning}


\section{Method}
%\liye{Grammarly fixed, pending inspection}
In this section, we introduce CMT, an RL framework for offline RL and offline meta-RL. We describe CMT with policy prompts for offline RL in Section \ref{offline-seq}, and CMT extended with task prompts and policy prompts for meta-RL in  Section \ref{offline-meta-sequence}. 
% \yali{it is not clear what's the difference and relation between offline single-agent RL and offline Meta-RL? }
% In this section, we propose a novel offline meta-RL framework called CMT which includes two parts, the basic framework with policy prompts for offline RL (in Section \ref{offline-seq}) and the extended framework with policy prompts and task prompts for offline Meta-RL (in Section \ref{offline-meta-sequence}).
%We first XXXX 
%\replaced{ We first XXXX in Section \ref{offline-seq}}{In section.\ref{offline-seq}, we present an offline learning method based on sequence modeling} 
%We then introduce XXX .
%\replaced{We then introduce XXX in Section x.x}{In section.\ref{offline-meta-sequence}, the framework is extended to offline meta reinforcement learning.}
%\deleted{The core insight is similar: interactions with the environment capture the structure of the task. The context  trajectory is used by CMT as a prompt to guide the policy in a specific unknown task.}
%In section.\ref{offline-generalization}, 
%\yali{i) We need to split the preliminaries and our methods. ii) can we use subsection title to indicate the contributions that we made? }

\subsection{Offline Sequence Learning}
\label{offline-seq}
\begin{figure}[tbp]
    \vspace{-1.0cm} 
    \centering
    \includegraphics[width=0.9\textwidth]{fig/CMT-v5.pdf}
     \caption{The framework for CMT in the offline learning Setting. (a) In the representation stage, CMT pretrains an auto-encoder model in the offline dataset, which predicts the future action, reward, and state with the policy prompt from the history trajectory. The adaptor layer is a identity function during this stage, which mean $z  \equiv z'.$ (b) In the improvement stage, we freeze the pretrianed model, and tune the prompt to reach a better performance.
     %\yali{@runji, caption need to be expanded}
     }
    \label{fig:CMT}
\end{figure}

%\deleted{We provide a solution for the offline reinforcement learning problem from a pretrain and prompt-tuning paradigm. And}  
The main assumption of our method is that offline trajectories can be viewed as samples from unknown  policies, and the optimal policy can be represented as a  mixture of these basic policies. 
Our method contains two stages of training, the representation stage and the improvement stage. CMT learns a model to convert an offline trajectory into a policy prompt with some characteristics to represent these deterministic policies. In the second stage, a policy prompt is learnt to mix up  basic policies by planning in the world model to attain an advanced policy.

%\replaced{}{To achieve this goal, our method can be divided into two major parts: the representation stage and the improvement stage.} In the \replaced{first }{representation} stage, \replaced{ a model to convert an offline trajectory into a policy prompt with some characteristics to represent these deterministic policies}{CMT learns a model to convert an offline trajectory into a policy prompt with some characteristics to represent these deterministic policies.} \replaced{In the second stage, a policy prompt is learnt to mix up  basic policies by planning in the world model to guide a better policy.}{ And the aim of the improvement stage is to learn an appropriate policy prompt to mix up these basic policies by implicitly planning in the world model to guide a better policy.}
% \yali{i did not see connection between "to achieve this goal" and "main insight".}.

\textbf{Representation Stage.} Fig.\ref{fig:CMT}  shows the whole architecture, which constitutes an auto-encoder $A$ in trajectory-level. CMT consists of two components: a trajectory encoder $A_{e}$ with parameter $\theta$ and an autoregressive generator $A_{g}$ with parameter $\phi$. Trajectory encoder $A_e$ is a bi-direction transformer \citep{bert}, which gets a history trajectory and gives the policy prompt $z_\tau$ for the trajectory  $z_{\tau} = A_{e}(\tau; \theta)$. % \yali{ $z_{\tau} = A_{e}(\tau;\theta)$ }. 
Autoregressive generator $A_g$ is a GPT-style \citep{brown2020gpt3} conditional generator, which predicts the policy prompt $z_\tau$ and the next token in the future based on the previous history trajectory: $\tau_{t+1} = A_{g}( . |z_{\tau}, \tau_{<t}; \phi). $


In this stage, CMT introduces two loss terms to update $\theta$ and $\phi$. The major loss $\mathcal{L}_1$ is supervised loss, which is used to reconstruct the whole trajectory, and an auxiliary loss $\mathcal{L}_2$ help improve policy in the next stage. The loss is linear weighted as $\mathcal{L} = \mathcal{L}_1 + \gamma \mathcal{L}_2$, in which $\gamma$ is the contrastive loss coefficient.
For the supervised loss $\mathcal{L}_1$, since $A_g$ predicts the future action, reward and state one by one, it employs as an union of a policy $\pi(a|s)= A_g(z_{\tau}, s_t,\tau_{<t})$, a dynamic model $P(s'|s, a) = A_g(z_{\tau}, \tau_{<t})$ and a reward function $R(s,a) = A_g(z_{\tau}, a_t, s_t,\tau_{<t})$. The prediction and the ground truth form a supervised loss in Eq.(\ref{eq: s_loss}):
\begin{equation}
\mathcal{L}_1({\tau};\mathbf{\phi, \theta})=\sum_{t=0}^{T-1}(
\mathcal{D}(s_t, P_{}(\tau_{<t};\phi,\theta)) +
\mathcal{D}(a_t, \pi_{}(s_t,\tau_{<t};\phi,\theta)) + 
\mathcal{D}(r_t, R_{}(a_t, s_t,\tau_{<t};\phi,\theta)),
%\log P_{\theta}({\mathbf{s}}_{t} \mid {\mathbf{s}}_{t}, {\tau}_{<t})+ 
%\log P_{\theta}({\mathbf{a}}_{t} \mid {\mathbf{a}}_{t}, {\mathbf{s}}_{t}, {\tau}_{<t})+
%\log P_{\theta}(\bar{r}_{t} \mid {\mathbf{a}}_{t}, {\mathbf{s}}_{t}, {\tau}_{<t}))
    \label{eq: s_loss}
\end{equation}
in which distance matrices $\mathcal{D}$ adopts MSE loss for deterministic output and  cross-entropy loss for stochastic prediction. Since the entire architecture is differentiable, the supervised loss can be used to update $A_{e}$ and $A_{g}$.


%Then we explain the two auxiliary losses. 
An auxiliary loss $\mathcal{L}_2$ constrains the distance between prompts coming from similar trajectories by self-supervised learning. Inspired by \citep{curriculumImitation}, an effective and stable policy improvement based on imitation learning often satisfies two properties: (a) Keeping new behavior close to previous ones. (b) Getting higher rewards than the previous ones. As we desire to improve the policy by prompt tuning, it is natural to facilitate the similarity of prompts from similar trajectories. For this purpose, we introduce an InfoNCE contrastive loss \citep{infoNCE} to constrain the prompt in a self-supervised method to meet the aforementioned requirements. The auxiliary contrastive loss is given as Eq.(\ref{eq:c_loss}):
\begin{equation}
    \mathcal{L}_2(\tau_q, \{\tau_i\}_{i=1}^{K};\theta) =  -\log\frac{\exp(A_e(\tau_{q}; \theta) \cdot A_e(\tau_{+}; \theta) / \alpha)}{ \sum_{i=1}^k \exp(A_e(\tau_{q}; \theta) \cdot A_e(\tau_{i}; \theta) / \alpha) }
    =  -\log\frac{\exp(z_{q} \cdot z_{+} / \alpha)}{ \sum_{i=1}^k \exp(z_q \cdot z_i / \alpha) },
    \label{eq:c_loss} 
\end{equation} in which $\alpha$ is temperature coefficient. For the anchor policy prompt $z_q$ encoded from trajectory $\tau_q$, a batch of $K$ policy prompts $\{z_i\}_{i=1}^K$ encoded from a set of trajectories $\{\tau_i\}_{i=1}^K$ sampled from the offline dataset. $\{z_i\}$ consists of $K-1$ negative samples $z_{-}$ and one positive sample $z_+$. The definition of the positive and negative samples influence the property of the policy prompt. To ensure similar behavior trajectories be encoded into close prompts, the auxiliary loss defines the pair of policy prompts samples from the same trajectory and different trajectories as the positive and negative sample pair.
%\yali{can we give a formulation of first auxiliary loss?}. \runji{in fact ,it is eq.2,should be more clear?} 
%To encourage the trajectories with similar cumulative reward forms clusters, the second auxiliary loss is also based on Eq.(\ref{eq:c_loss}), but with a different definition of the positive and negative sample pairs. Here, the pair of policy prompts having a difference between cumulative returns within or without a threshold $u$ forms the positive and negative sample pair, where the threshold $u$ is a hyperparameter. % \liye{This sentence is too long and needs to be split}

%\yali{can we describe the two auxiliary losses? we can replace (4) with our two auxiliary losses for the completeness.
% are we still keeping the two auxiliary losses? if not, we need to remove that and update the method section.
%}


%\yali{till now, i did not see how "representation stage" converts trajectory to prompt.}


%As a policy can be described by a stationary distribution over a set of trajectory. Or in another word, a trajectory represents partial information about a deterministic policy, and any stochastic policy can be approximated by a mixture of deterministic policies. 

% We simplify the mixture as a summation, so a behavior policy embedding can be $\hat{z}_{\pi} = \sum z_{\tau}$, and the behavior policy is 
%\begin{equation}
%\pi(a|\tau_{<t}) = A_{\pi}(.|\hat{z}_{\pi}, \tau_{<t})
%\end{equation}

%\yali{下面段落里的prompt generate trajectoris, 是不是不太对？ 我们是 tranformer generate trajectories?}
%\yali{下面这一段很不清楚。 说的不清}

\textbf{Improvement Stage.} Since the behaviour policy can be sub-optimal in the offline dataset, we consider prompt tuning to boost the agent performance, with the purpose to transfer the knowledge in the pretrained model. As shown in Fig.(\ref{fig:CMT}), the key idea is  simple: we can freeze the pretrained model, and learn prompts that can guide the pretrained model to generate a trajectory with high reward. Specifically, improvement stage consists of relabeling the offline dataset and prompt tuning by adaptor layer.


Relabeling the offline dataset is to replace the raw ordinary action with better action to provide new supervised target for prompt tuning. Concretely, we sample a mini-batch of data, and then adopts the beam search method proposed by trajectory transformer~\cite{TT_2021} as a planning algorithm to find the better action, in which the autoregressive generator $A_g$ works as a world model. 

To improve the performance by prompt tuning, we should tune the policy prompt $z_{\pi}$ for a better policy prompt $z_{\pi}'$ to guide generator $A_g$ to generate a trajectory with a higher reward. For this purpose, we freeze the pretrained model parameters, denoted as  $\bar{\theta}$ and $\bar{\phi}$ and only tune the parameter $\xi$ for the adaptor layer $L$ on the relabeled dataset. The adaptor layer $L$ is trained by the following Eq.(\ref{eq: r_loss}), 
\begin{equation}
    \mathcal{L}_3(\tau;\xi) =  \sum_0^{T-1} \mathcal{D}(\hat{a}_t, \pi_{}(s_t,\tau_{<t};\bar{\phi},\bar{\theta}, \xi)) + \beta ( z - L(z;\xi))^2
    \label{eq: r_loss}%\yali{the connection to eq(3) is not clear to me.}
\end{equation}
in which $\hat{a}_t$ is the relabeled action, and the second term constrains behavior changes to alleviate distribution shift in the offline setting, like \citep{TD_BC} and $\beta$ is a weight coefficient for behavior constraint.
This method can be regarded as using prompt tuning to remember planning results in the world model, which significantly reduce the computation cost and decision delay in evaluation. However, it should be noticed that we use planning method as the improvement method, but any other improvement algorithm of which loss function based on the output of generator $A_g$ can be easily plugged in.

%\runji{we might add a psudo-code to describe this preocess?}
%\replaced{Since the behaviour policy can be sub-optimal in the offline dataset, we consider prompt tuning in adaption to down-stream tasks. }{Since the offline dataset may be sub-optimal, a policy outperforming the behavior policy should be learned, rather than imitating the offline dataset. To transfer the knowledge in the pretrained model, we introduce a prompt-tuning method for offline RL. As shown in Fig.(\ref{fig:CMT}), the key idea is  simple: we can freeze the pretrained model, and learn prompts that can generate a trajectory with high reward. Specifically, the trajectory encoder $A_e$ converts a history trajectory into a policy prompt  $z_\pi$, and the autoregressive generator $A_g$ takes  a action $a \sim \pi(a|s)= A_g(z_{\pi}, s_t,\tau_{<t})$ conditioned on  $z_\pi$. However, the pretrained objective is to reconstruct, which means that $z_{\pi}$ guides the autoregressive generator to imitate the offline dataset. Therefore, we should tune the policy prompt $z_{\pi}$ for a better policy prompt $z_{\pi}'$ to generate a trajectory with a higher reward.}\yali{前面划掉这段话很不清晰， also it repeats the same message .} \deleted{In this work,}\replaced{W}{ w}e adopt reward ascent over the model predictive reward \yali{what does this mean? what is model predictive reward? it is not clear here.} to optimize the policy prompt $z_{\pi}$ in Eq.(\ref{eq:L3}).
%\yali{what is the difference to $z_q$?}in Eq.(\ref{eq:L3}).
%This method can be regarded as implicitly planning in the white-box world model, which maximizes utilization of the pretrained model. \yali{this sentence is not clear to me}%\deleted{However, online RL and offline RL loss are also possible to work, worth exploring in the future.}

%In order to find a better policy, which measures by gain more reward. From this prospective, the predicted trajectory can be used to optimize $z_{\pi}$ by gradient ascend over the model predictive reward:and a more optimal policy prompt can gain by one step gradient ascend:
%\begin{equation}
%z_{\pi}' = z_{\pi} + \alpha \mathbb{E}_{\tau \sim D} [\nabla_z \sum_0^{T-1} R(a_t, s_t,\tau_{<t};\phi,\theta)]
%= z_{\pi} + \alpha \mathbb{E}_{\tau \sim D} [\nabla_z \sum_0^{T-1} A_g(z_\tau, a_t, s_t, \tau_{<t};\phi)  ].
%\label{eq:L3}
%\end{equation}
%\yali{1- this equation implies that the second term on the rhs is zero. pls double check?? 2- if $R$ is  a function of $z$, can u explicitly write down the relation so I know that the graident is not zero?}

%\replaced{To further improve the inference efficiency of reward ascent, we adopt an adaptor layer $L$ xxx is introduced to make the algorithm more efficient.}{However, this gradient ascending method significantly slows down the inference speed due to the repeat computation. To address this issue, as shown in  \replaced{Fig.\ref{fig:CMT} (b)}{Fig.\ref{fig:CMT} } , an adaptor layer $L$ with parameter $\xi$ approximating the gradient ascend results as $z_{\pi}' = L(z_{\pi};\xi)$ is introduced to make the algorithm more practical. }  
%\deleted{Since the back-propagation is replaced by a forward computation, the delayed time is acceptable.}  We freeze the pretrained model parameters , denoted as  $\bar{\theta}$ and $\bar{\phi}$ and only tune the parameter $\xi$ for the adaptor layer $L$. The adaptor layer $L$ is trained by the following Eq.(\ref{eq: r_loss}), 
%\begin{equation}
%    \mathcal{L}_3(\tau;\xi) =  \sum_0^{T-1} R_{}(a_t, s_t,\tau_{<t}; \bar{\phi},\bar{\theta}, \xi) + \beta ( z - L(z;\xi))^2
%    \label{eq: r_loss}%\yali{the connection to eq(3) is not clear to me.}
%\end{equation}
%in which the second term constrains behavior changes to alleviate distribution shift in the offline setting, like \citep{TD_BC} and $\beta$ is a weight coefficient.

%\yali{add some texts to conclude the paragraph.}
\subsection{Contextual Sequence Meta Learning}
\label{offline-meta-sequence}
%\yali{we need to link this to section 4.1 and explain what to be introduced later. Currently 4.1 and 4.2 seems independent.}

Extended from the section \ref{offline-seq} which introduces policy prompts to solve offline RL problem, we simply incorporates a task prompts in CMT to achieve generalization ability in downstream unseen task in offline meta RL setting.
The task encoder $F(t|\tau)$ is used to encode transitions into a task hidden variable $t$ and learn a contextual policy $\pi(a|s, t)$ in classical context meta-RL methods. Therefore, CMT is feasible to extend to the meta-learning domain by simply plugging in task prompts. Fig.(\ref{fig:meta-CMT}) shows the minor modifications supporting CMT have impressive generality.  

\begin{figure}[tbp]
\vspace{-0.8cm} 
    \centering
    \includegraphics[width=1\textwidth]{fig/cmt-meta-v1.pdf}
    \caption{The framework of CMT in Offline meta reinforcement Learning Setting.  Based on the basic framework in Figure.\ref{fig:CMT}, CMT introduces a context trajectory as task prompt in trajectory encoder $A_e$ to guide  $A_g$. 
    %\yali{@runji, explanation of (b) is missing. }
     }
    \label{fig:meta-CMT}
\end{figure}

%\liye{Grammarly fixed, pending inspection}
\textbf{Meta Training.} To contain the task information, CMT simply concatenates a context trajectory, which is a trajectory fragment coming from the same task, with the input. During offline meta-training, the context trajectory is randomly sampled from the offline dataset. To avoid information fusion, we separate the context and history trajectories with a special token (\textsc{[SEP]}), whose parameters can be learned. Then we adopt a contrastive learning method similar to Eq.(\ref{eq:c_loss}) to learn a stable and consistent task prompt, similar to \citep{fu2020towards} in online meta-RL. The major difference between contrastive loss in task prompts and policy prompts is that task prompts form positive and negative sample pairs in task-level, while policy prompts 
form positive and negative sample pairs at trajectory-level. 
%\runji{might add position embeding detail.}

%\liye{Grammarly fixed, pending inspection}
\textbf{Meta Test.} After training on diverse tasks, meta test stage requires agent rapidly adapting in the unseen task. In context meta RL, agent is permitted to collect few context trajectories to understand the task.  The context trajectory in the meta test could come from an offline dataset in an unknown task or a trajectory that has interacted with the online world. The second setting is more challenging \citep{identify_offlinemeta} due to the exploration problem. To verify the strong capacity of CMT, we evaluate CMT in the second setting. Furthermore, CMT discards recursive component, so it is suitable for zero-shot setting, which means CMT collects the context during online evaluation, rather than in advance. To the best of our knowledge, there is no existing method to solve this one-shot setting in offline meta RL. As a result, we construct a context buffer to store the history of interactions, and the context trajectories are randomly chosen from the context buffer. 

%key information:1. keep the similar framework and add context component, Contrastive Loss in task-level.2. [sep], embedding detail about context and history. since the policy prompt is after the and 3. the meta test method. 

%\paragraph{Offline Exploration for Context}
%Here we learn a exploration policy, denotes as $\pi^{exp}$ and an unified sequence modeling architecture is proposed as $\pi^{task}(a| \tau_{<t},\tau^{exp})$, which based on a exploration trajectory $\tau^{exp} \sim \pi^{exp}$ sampled by exploration policy and the previous history to generate the next action. The meta learning objective can be described as following:
%\begin{equation}
%    \mathcal{L}(\pi^{exp}, \pi^{task}) = \mathbf{E}_{\tau^{exp} \sim \pi^{exp}, \mathcal{T} \sim p_{\mathcal{T}}}  [ V (\pi^{task}, \mathcal{T})].
%\end{equation}

%To optimize this objective, we could decouple exploration and exploitation. Exploration policy mainly relies on finding several key states or reward signal to identify task type. To achieve this goal, we define a intrinsic reward to measure the difference among distinct tasks and then relabel all the trajectory reward. Therefore, exploration policy can learn to generate most informative trajectory by maximize the intrinsic reward and finally identity the task.
%\begin{equation}
%\hat{r}_{t}\left(a_{t}, r_{t}, s_{t+1}, \tau_{: t} ; \mu\right)=\mathbb{E}_{\tau_{random} \sim D}[KL( \pi^{task}(s_{t+1},r_{t+1}| \tau_{<t},\mu) |\pi^{task}(s_{t+1},r_{t+1}| \tau_{<t},\tau_{random}) )],
%\end{equation}
%in which $\mu$ and $\tau^{exp}$ are  trajectories from same task.


%\begin{equation}
%    \mathcal{L}_3({\tau})= \mathcal{E}_{\tau^{exp}, \tau \sim Matching( D_{mixture}, \{D_i\} )  } [\sum_{t=0}^{T-1}( \log P_{\theta}({\mathbf{s}}_{t} \mid { {\tau}_{<t}, \tau^{exp})+ \log P_{\theta}({\mathbf{a}}_{t} \mid {\mathbf{a}}_{t}, {\mathbf{s}}_{t}, {\tau}_{<t}, \tau^{exp})+\log P_{\theta}(r}_{t} \mid {\mathbf{a}}_{t}, {\mathbf{s}}_{t}, {\tau}_{<t}, \tau^{exp}))]
%\end{equation}

%\paragraph{Modelling sequential contexts/Policy distillation via contexts}
%Then we consider the training process of exploitation policy $\pi^{task}(a| \tau_{<t},\tau^{exp})$. $\pi^{task}$ are expected to identify the task by a exploration trajectory in an unseen task and then plays the corresponding specific optimal policy. To achieve this goal, we adopt the experience relabel approach to learn sequence contextual exploitation policy. The optimal dataset $D_{optimal}$ come from the mixture of all single task dataset $D_i$ with relabling the action to the learned optimal offline policy action distribution. The mixture dataset come from the mixture of all single task dataset $D_i$ without relabling. We randomly match  a same-task pair of trajectories $(\tau^{exp}, \tau^{opt}) $ in dataset $ D_{mixture}, \{D_i\} $, and combine the pair as a integrated sequence to train an auto-regressive sequence model. Since the matching pair comes from the same task, the policy automatically learn to play specific policy in specific task to minimize  the loss. The objective function is as following:




%\subsection{Downstream generalization}
%\label{offline-generalization}
%MAE for CTCE \& CTDE 
%MAE for planning.
%MAE for goal-conditional 
%MAE for offline 


\section{Experiment}
\label{sec:exp}
In this section, we evaluate the performance of CMT in terms of offline RL tasks in D4RL benchmarks \citep{d4rl}, offline meta-RL tasks in meta Mujoco benchmarks \citep{mujoco}. Additional offline multi-agent experiments are conducted on StarCraft II Micromanagement \citep{SMAC}.  Simply replacing a sequence of transition by a sequence of agent, CMT can be easily extended to solve multi-agent offline-RL tasks and is evaluated in a popular MARL benchmark (SMAC).  The results on SMAC are reported in Appendix \ref{offline-MA}. Apart from the performance in various settings, we design experiment for ablation study to demonstrate the validity of the components contained in CMT.
Our experiments are conducted on a server with Nvidia Tesla A100 GPU and AMD EPYC 7742 CPU.

\subsection{ Offline Learning Tasks}
We evaluate CMT on the continuous control tasks from D4RL benchmarks. The experiments on four standard Mujoco locomotion environments (HalfCheetah, Hopper, Walker, and Ant) are conducted with three kinds of dataset quality (Medium, Medium-Replay, and Medium-Expert). The differences between them are as follows: \textbf{Medium} contains 1 million timesteps generated by a "medium" policy interacting with the environment, with an intelligence level of around 1/3 that of experts. \textbf{Medium-Replay} contains the replay buffer generated during the medium policy training process, and about 25k-400k timesteps are included in the tested environments. \textbf{Medium-Expert} consists of 1 million timesteps generated by the medium policy concatenated with another 1 million timesteps generated by the expert policy.


Five baselines are considered, including behaviour cloning (BC) \citep{BC}, behavior regularized ActorCritic (BRAC) \citep{BRAC}, conservative Q-learning (CQL) \citep{CQL}, implicit Q-learning (IQL) \citep{IQL}, and decision transformer (DT) \citep{DT_2021}. 
BC realizes intelligence by learning from expert datasets, which is actually a supervised learning process that learns the states to predict actions. Because of severe extrapolation errors caused by the policy evaluation, traditional offline RL algorithms perform poorly. And the methods such as BCQ, BRAC, and IQL, avoid extrapolation errors by constraining the behavior space. While CQL solves it by finding a conservative Q function that keeps the policy function's expected value less than the true value.
Starting from another perspective, DT transforms the RL problems into sequence modeling problems and attempts to find the optimal actions.
The detail about hyper-parameter lists is in Appendix. \ref{app:hyper}. 

The results for D4RL datasets are shown in Table. \ref{D4RL-results}, CMT performs excellently on the Medium and Medium-expert datasets, but not so well on the Medium-replay dataset, indicating that CMT prefers to learn from data generated by stable policies. Compared with DT, which is also a transformer-based structure, CMT outperforms it in most of the tasks. Moreover, although IQL is the SOTA algorithm currently, the performance of CMT on the Medium and Medium-expert datasets meets or exceeds it, demonstrating that our method has huge potential.

\begin{table}[htbp]
\caption{Results for D4RL datasets. Here we report the mean for three seeds, and the reward is normalized so that 100 represents an expert policy and 0 represents a worst policy in D4RL. PT abbreviation stands for prompt tuning. In addition, our method name and the best performances are bold font.}
\begin{tabular}{@{}cllllllll@{}}
\toprule
Dataset       & Environment &  \makecell[c]{\textbf{CMT}\\ with PT} & \makecell[c]{\textbf{CMT} \\ w/o PT}  & DT    & BRAC-v & CQL   & IQL & BC   \\ \midrule
Medium-Expert  & halfcheetah& \textbf{92.9} & 59.8      & 88.0  & 41.9   & 91.6  & 86.7  & 65.6 \\
Medium-Expert  & hopper     & \textbf{106.5}& 102.0     & 103.3 & 0.8    & 105.4 & 91.5  & 55.4 \\
Medium-Expert  & walker     & 97.6&  83.5    & 108.4 & 81.6   & 108.8  & \textbf{109.6}  & 11.2 \\
Medium-Expert  & ant        & 101.3 &  67.1      & 89.3  & -      & 115.8     & \textbf{125.6}  & 71.2 \\ \midrule
Medium        & halfcheetah &43.6 & 40.1      & 42.1  & 46.3   & 44.0  & \textbf{47.4}   & 41.6 \\
Medium        & hopper      &\textbf{68.9} & 62.8      & 62.0  & 31.1   & 58.5  & 66.3   & 48.6 \\
Medium        & walker      &75.0 & 69.6      & 71.6  & 81.1   & 72.5  & \textbf{78.3}   & 47.8 \\
Medium        & ant         &71.8 & 61.3      & 64.6  & -      & 90.5     &  \textbf{102.3}   & 63.7 \\ \midrule
Medium-replay & halfcheetah & 38.7& 16.5      & 36.3  & \textbf{47.7}   & 45.5  & 44.2   & 2.2  \\
Medium-replay & hopper      & 84.9& 58.4      & 67.8  & 0.6    & \textbf{95.0}  & 94.7   & 30.8 \\
Medium-replay & walker      & 49.5& 37.3      & 47.8  & 0.9    & 26.7  & \textbf{73.9}   & 5.9  \\
Medium-replay & ant         &40.6 & 42.1      & 61.7  & -      & \textbf{93.9}     &  88.8   & 30.1 \\ \bottomrule
\end{tabular}
\label{D4RL-results}
\end{table}

\subsection{Offline Meta Learning Tasks}
We explore four task settings to evaluate CMT on zero-shot generalization: Half-Cheetah-Vel, Ant-Fwd-Back, and Ant-Fwd-Back. The number of training and evaluation tasks and task coverage for each setting can be found in Appendix Tab.\ref{tb: distinct-hyper}. The same data collection method is used as described in the literature \citep{FOCAL}. The following baselines are taken into account: \textbf{Batch PEARL} \citep{PEARL}: A modified version of PEARL which can be used for offline RL tasks. \textbf{CBCQ} \citep{BCQ}: An advanced version of the BCQ that has been adapted to offline RL tasks by incorporating latent variables into state information. \textbf{MBML} \citep{MBML}: A multi-task offline RL method with metric learning. \textbf{FOCAL} \citep{FOCAL}: A model-free offline Meta-RL method with state-of-the-art performance based on the deterministic context encoder. These baselines are trained on a set of offline RL tasks and are tested on the set of unseen offline RL tasks.

The results for meta Mujoco environments are shown in Fig. \ref{fig:meta}. Once again, we should emphasize the results of CMT is zero-shot setting, while all the other baselines requires context from offline dataset or online interactions in advance.  As we can see, CMT can outperform most baselines, including CBCQ, batch PEARL, and MBML. Besides, FOCAL is the SOTA algorithm currently, while CMT can outperform it in different tasks except Walker-2D-Params, showing that our algorithm also has great potential in the area of offline meta-RL.% \liye{Need to consider how to describe the comparison of CMT and FOCAL, MerPO results.}
\begin{figure}[htbp]
\vspace{-0cm} 
    \centering
    \subfigure{
        \includegraphics[width=2.2in]{fig/offline_single_map_Ant-Fwd-Bwd_v4.pdf}
    }
    \subfigure{
	\includegraphics[width=2.2in]{fig/offline_single_map_Half-Cheetah-Fwd-Bwd_v4.pdf}
    }
    %\quad    %用 \quad 来换行
    \subfigure{
    	\includegraphics[width=2.2in]{fig/offline_single_map_Point-Robot-Wind_v3.pdf}
    }
    \subfigure{
	\includegraphics[width=2.2in]{fig/offline_single_map_Walker-2D-Params_v3.pdf}
    }
    \caption{Results for Meta Mujoco Environment. In all benchmark tasks, CMT obviously learns that a policy can face adaptation into a new task, and provides evidence that sequence modeling method is promising. Noticed that CMT have two training stage, it is difficult to align the x-axis. Therefore, we report the training curve of CMT in representation stage and the final evaluation results of CMT after prompt-tuning as a dotted line with standard deviation.%\yali{we need captions to explain the results.}
    }
    \label{fig:meta}
\end{figure}
\subsection{Ablation Study}
In this section, we formulate experiments to investigate the following research questions: 
\textbf{Q1:} How important prompt-tuning is for performance? 
\textbf{Q2:} Does contrastive loss benefit the prompt-tuning? 
\textbf{Q3:} Does the behavioral constraint affect the results? 
\textbf{Q4:} In offline meta RL setting, does quality of task prompts affect the performance in downstream tasks?
%\textbf{Q5:} Does prompt tuning efficient speed up inference in evaluation?
Without loss of generality, we completed the following ablation experiments in the Ant-Fwd-Bwd environment, and the results are shown in Figure \ref{fig:ablation}.
 
 \textbf{(Q1) Prompt-tuning.} 
 % Shown in Table1(the first two col), and the Figure 3(the gap between Solid and dashed lines)
Prompt-tuning is utilized in the second stage to enhance the model's performance based on the pre-trained model. With the help of it, the average return will increase by 65.8\%, which shows that prompt-tuning is effective in improving model effects. Furthermore, the results of CMT with prompt-tuning and without prompt tuning in Table \ref{D4RL-results} and Figure \ref{fig:meta} strongly demonstrates the benefit in performance from improvement stage.
 
\begin{figure}[htbp]
    \centering
    \includegraphics[width=4in]{fig/ablation_result.pdf}
    \caption{Results for ablation. We explore the impact of prompt-tuning, contrastive loss, behavioral constraint, and context on the model by whether use it or set extreme values. And each group has a different color. The first group is the full CMT model, which can be used as the benchmark.}
    \label{fig:ablation}
    \vspace{-0.8cm} 
\end{figure}


 
 \textbf{(Q2) Contrastive Loss.}
Contrastive loss plays a key role in clustering similar trajectories, which ensures that the model can find the correct prompts to guide better trajectories. To investigate its influence, we utilized two extreme coefficients, the minimal value of $10^{-6}$ and the maximal value of 1. As shown in Figure \ref{fig:ablation}, when the coefficient gets to its minimal value, the average return drops by 50.9\%. A possible explanation is that the model lacks the clustering process, preventing it from generating effective prompts to distinguish different types of trajectories. Finally, the model will be heavily affected by data distribution shifts during the tuning process. When the coefficient gets maximal, the average return falls slightly by 11.1\%, showing that an overly strict constraint will also affect the model. Moreover, we conduct visualization analysis in Figure \ref{fig:contrast} to demonstrate the  significant effect on the distribution of prompts.
 
  \begin{figure}[htbp]
  \vspace{-0cm} 
    \centering
    \subfigure{
        \includegraphics[width=2.2in]{fig/with_CL.png}
    }
    \subfigure{
	\includegraphics[width=2.2in]{fig/without_CL.png}
    }
    %\quad    %用 \quad 来换行
    \caption{Visualization for Policy Prompts in halfcheetah task. We visualize prompts from three pairs of trajectories with contrastive loss and without contrastive loss. Each pair have similar behavior and reward sampled from offline reply dataset, and use similar colors. %\yali{we need captions to explain the results.}
    }
    \label{fig:contrast}
\end{figure}
 
 \textbf{(Q3) Behavioral Constraint.}
Behavioral constraint is utilized in the second stage to enhance the model, which has a significant impact on the final effect. As shown in Figure \ref{fig:ablation}, we use the minimum coefficient of 0 and the maximum coefficient of 50 to show its impact. When the coefficient is 0, the average return will dramatically drop by 128\%. Despite the fact that the loss decreases on offline datasets during the tuning process, the test results are still poor. This is the typical overfitting situation, indicating that the model suffers from severe data distribution shifts. When the coefficient is 50, the average return will also be reduced by 36.4\%. It shows that an extremely severe behavioral constraint will also lead to inefficient policy boosting, resulting in slightly better performance than that of the model without prompt-tuning. Therefore, it is very important to find a suitable coefficient.
 
 \textbf{(Q4) Quality of task prompts.}
 Task content is constructed to accurately identify Meta-RL tasks. To demonstrate its effect as shown in Figure \ref{fig:ablation}, the experiments are divided into no context, medium context, and expert context groups based on content quality. In the first set of experiments, the CMT full model collects the context during online evaluation to support zero-shot adaption. The absence of task prompts significantly deteriorates the performance by 18\% due to the inability to accurately identify tasks. The performance is improved when task contents from the offline datasets are employed. The results utilizing the medium and expert datasets increase by 4\% and 7\%, respectively. Both of them are better than the results in the first set of experiments. In fact, if offline task contents are used, the tasks will become few-shot tasks since the offline datasets are co-distributed with the tuning datasets. It is simpler and better than the zero-shot tasks using the online task contents.
 
% \textbf{(Q5) Inference Time.} the table in 






% \begin{figure}[htbp]
%     \centering
%     \subfigure{
%         \includegraphics[width=2.2in]{fig/offline_single_map_3s_vs_5z.pdf}
%     }
%     \subfigure{
% 	\includegraphics[width=2.2in]{fig/offline_single_map_8m.pdf}
%     }
%     \quad    %用 \quad 来换行
%     \subfigure{
%     	\includegraphics[width=2.2in]{fig/offline_single_map_corridor.pdf}
%     }
%     \subfigure{
% 	\includegraphics[width=2.2in]{fig/offline_single_map_MMM2.pdf}
%     }
%     \caption{Results for four representative  maps in SMAC. CMT has significant advantages, compared with ICQ, CQL, BC baselines. All results on 20 maps can be found in the appendix. 
%     }
%     \label{fig:SMAC}
% \end{figure}

%\begin{figure}
%    \centering
%    \includegraphics[width=1\textwidth]{fig/offline_single_map_ablation_study.pdf}
%    \caption{Caption}
%    \label{fig:my_label}
%\end{figure}

\section{Conclusions}
In this paper, we present CMT, an offline RL algorithm based on prompt tuning, with the goal of training a large-scale model that can be utilized on various downstream tasks from the sequence modeling perspective. The prompt tuning is designed for offline RL to pre-train the model and guide the autoregressive model to generate trajectories with high rewards. Besides, a variety of experiments are conducted in three different RL settings, offline single-agent RL (D4RL), offline Meta-RL (MuJoCo), and offline MARL (SMAC), and the model's performance is evaluated with different baselines. The results show that CMT has strong performance, and generality. To our best knowledge, CMT is also the first sequence-modeling-based algorithm for offline meta-RL problems. General decision models like CMT enhance the efficiency of model training and lower the threshold for the applications of RL algorithms. 
%\liye{Need to check the accuracy of the model description}


%\subsubsection*{Author Contributions}
%If you'd like to, you may include  a section for author contributions as is done in many journals. This is optional and at the discretion of the authors.

%\subsubsection*{Acknowledgments}
%Use unnumbered third level headings for the acknowledgments. All acknowledgments, including those to funding agencies, go at the end of the paper.

% \newpage
% \textbf{Reproducibility Statement}
% To reproduce the results in this paper, our effort are listed as following:
% \begin{itemize}
%     \item \textbf{Environment.} We give the detail information about our hardware platform in section \ref{sec:exp}. And the software version and dependencies are included in a requirement file.
%     \item \textbf{Method.} In section\ref{offline-seq}, we give all the loss function formula. The network architecture are shown in Figure \ref{fig:CMT} For the better understanding of the relationship of data and loss function, we illustrate the detail dataflow in appendix \ref{app:network}. We also list all the hyper-parameter for the experiment in Appendix \ref{app:hyper}.
%     \item \textbf{Data.} Since the performance in offline RL highly depends on the dataset, we clearly report the  dataset resource.  In the offline RL setting, we adopt the popular benchmark (D4RL \url{https://github.com/Farama-Foundation/D4RL}). In the offline meta setting, we follow the FOCAL (\url{https://github.com/LanqingLi1993/FOCAL-ICLR}) to generate data. 
%     \item \textbf{Randomness.} To alleviate the randomness and noise, we run three seeds to reduce the bias. And we randomly sample the seed from a uniform distribution from $(0, 65536)$ without any selection.
%     \item \textbf{Code.} Since we has not been completed code clean yet, we are glad to open source our code in more readability format in rebuttal.
% \end{itemize}



\bibliography{iclr2023_conference}
\bibliographystyle{iclr2023_conference}

\newpage

\appendix
\section{Appendix}
\subsection{Network Architecture}
\label{app:network}
In Fig.(\ref{fig:cmt-network}), we illustrate the detail of the network architecture for CMT with the input and output structure. The input consists of a sequence of trajectory tokens, which are embedded by a linear layer and add up with the position embedding. The output is decoded from the latent states in the transformer by another linear layer. Noticed that the output from history trajectory tokens is masked to avoid participating in the supervised loss. 
\begin{figure}[htbp]
    \centering
    \includegraphics[width=1\textwidth]{fig/offline-prompt-v5.pdf}
    \caption{Detailed data flow, loss and network architecture for CMT.}
    \label{fig:cmt-network}
\end{figure}

\subsection{Hyper-Parameter}
In this section, we describe detailed hyperparameters to reproduce the experimental results. Due to the robustness of CMT, our algorithm shares similar hyperparameters among three benchmarks as shown in Table.(\ref{tb:common-hyper}).
\label{app:hyper}
\begin{table}[H]
    \centering
    \begin{tabular}{@{}llll@{}}
        \toprule
        \textbf{Parameter    } & \textbf{D4RL(Default Config) } & \textbf{meta Mujoco} & \textbf{SMAC}   \\ \midrule
        Optimizer                      & AdamW          & AdamW       & AdamW                  \\
        Batch size                     & 256            & 512     & 256                   \\
        learning rate                  & 1e-4           & 1e-4       & 1e-4                  \\
        Transformer block layer        & 2              & 2       & 2                \\
        Attention head                 & 2              & 2       & 2                    \\
        Embedding dimension            & 32             & 32       & 32                    \\
        context length - policy        & 40             & 30      & 10                     \\
        context length - task          & None           & 30      & None                     \\
        gradient norm clip             & 0.5            & 0.5       & 0.5                    \\
        contrastive loss - K           & 256            & 512       & 256                    \\
        contrastive loss - $\alpha$    & 0.2            & 0.2       & 0.2                    \\ 
        contrastive loss - $\gamma$    & 0.1            & 0.1       & 0.1                    \\ 
        behavioral constraint - $\beta$  & 1              & 1       & 1                    \\ \bottomrule
    \end{tabular}
    \caption{Common hyper-parameters for CMT in D4RL, meta Mujoco and SMAC.}
    \label{tb:common-hyper}
\end{table}

In Table.(\ref{tb: distinct-hyper}), we discuss the distinct hyperparameters for four meta Mujoco tasks.
\begin{table}[H]
    \centering
    \begin{tabular}{@{}lllll@{}}
        \toprule
        \textbf{Parameter    } & \textbf{Ant-Fwd-Bwd } & \textbf{Half-CHeetah-Fwd-Bwd}& \textbf{Point-Robot-Wind} & \textbf{Walker-2D-Params} \\ \midrule
        train tasks number             & 2          & 2   &40 &40                       \\
        test task number               & 2          & 2   &10 &10                      \\
        task coverage                  & 100\%    & 100\% & 80\% & 80\% \\
        context length -task           & 32         & 64  &32 & 32                      \\\bottomrule
    \end{tabular}
    \caption{Specfic hyper-parameters for four mete Mujoco tasks.}
    \label{tb: distinct-hyper}
\end{table}

\subsection{Multi-Agent Offline Learning tasks}
\label{offline-MA}
By simply representing states and actions from several agents as a sequence of tokens, CMT can be deployed in the multi-agent tasks.
In this subection, we evaluate the performance of CMT on multi-agent offline learning settings in SMAC benchmarks in 20 maps.  For the data collection, we follow the same method in literature in \citep{MADT}. The datasets are built from trajectories generated by MAPPO on the SMAC tasks, and a large number of trajectories are contained in each of them. Different from D4RL, the properties of the DecPOMDP, the local observations and available actions, are also considered in our datasets.

The BC \citep{BC}, CQL-MA \citep{CQL}, and ICQ-MA \citep{ICQ} are utilized as baselines to show the performance of our solution, and their original models own good performances in single-agent offline RL tasks. The properties of the multi-agent versions are the same as the single-agent versions. BC learns by building the state-to-action mapping. Based on the traditional multi-agent offline RL methods, ICQ-MA and CQL-MA solve the extrapolation error problem through action-space constraint and value pessimism, respectively.

%The results on four maps are displayed in Fig. \ref{fig:SMAC} to demonstrate the performance of algorithms on tasks of varying difficulty (Super hard: $MMM2$, $corridor$; Hard: $3s\_vs\_5z$; Easy: $8m$).
The results on eight maps are displayed in Fig. \ref{fig:SMAC} to demonstrate the performance of algorithms on tasks of varying difficulty (Super hard: $MMM2$, $corridor$, $3s5z\_vs\_3s6z$; Hard: $3s\_vs\_5z$, $8m\_vs\_9m$, $3s5z$; Easy: $8m$, $3s\_vs\_4z$). 
More results on StarCraft II can be found in Appendix. The CMT outperforms the baselines and achieves state-of-the-art performance in all maps, indicating that our algorithm has strong robustness and high efficiency. While ICQ-MA and CQL-MA perform poorly due to extrapolation errors and larger errors generated by multiple agents. Furthermore, it should be noted that the BC works well since the approximate expert datasets are used in training stage.


%\begin{figure}[h]
    % \centering
    % \includegraphics[width=1\textwidth]{fig/offline_single_map_newv2.pdf}
    % \caption{Results for SMAC in 22 maps. Compared with ICQ, CQL, BC.}
    % \label{fig:my_label}
%\end{figure}

\begin{figure}[thbp]
    \centering
    \includegraphics[width=1\textwidth]{fig/offline_single_map_newv5.pdf}
    \caption{Results for eight representative  maps in SMAC. CMT has significant advantages, compared with ICQ, CQL, BC baselines. All results on 20 maps can be found in the appendix. 
    }
    \label{fig:SMAC}
\end{figure}


\subsection{Full resluts on SMAC}
We evaluate CMT on twenty maps in the SMAC benchmark. As shown in Fig.(\ref{fig:smac_all}), the results demonstrate that CMT remarkably outperforms baselines, including BC, ICQ, and CQL.  
%\yali{again, cite and explain.}
\begin{figure}[htbp]
    \centering
    \includegraphics[width=1\textwidth]{fig/offline_single_map_newv4.pdf}
    \caption{Results for SMAC on twenty maps.CMT has significant advantages, compared
with ICQ, CQL, BC baselines.}
    \label{fig:smac_all}
\end{figure}


\end{document}
