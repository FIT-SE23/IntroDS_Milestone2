\documentclass[]{article}

\usepackage{xcolor}
\usepackage{subcaption}
\usepackage{graphicx}
\usepackage{multirow}
\usepackage{comment}
\usepackage{booktabs}
\usepackage{hyperref}

\usepackage{tikz}
\usepackage{tikzscale}
\usetikzlibrary{positioning}
\usepackage{pgfplots}
\usetikzlibrary{external}
\tikzexternalize
\usepgfplotslibrary{groupplots}

\newcommand{\ie}{\emph{i.e.}}
\newcommand{\eg}{\emph{e.g.}}
\newcommand{\ea}{\emph{et al.}}

%\renewcommand{\textcolor}[2]{} %-- uncomment to remove all comments
\newcommand{\red}{\textcolor{red}}
\newcommand{\murray}[1]{\textcolor{magenta}{Murray: #1}}
\newcommand{\todo}[1]{}

% front matter
%----------------------------------------------------------------------
\title{Reads2Vec: Efficient Embedding of Raw High-Throughput
  Sequencing Reads Data}

\author{Prakash Chourasia$^1$, Sarwan Ali$^1$, Simone Ciccolella$^2$,\\
  Gianluca Della Vedova$^2$ and Murray Patterson$^1$\\
  {$^1$Department of Computer Science, Georgia State University}\\
  {Atlanta, GA 30303, USA}\\
  {$^2$Department of Informatics, Systems and Communication (DISCo)}\\
  {University of Milano-Bicocca, Milan, Italy}\\
}

\date{}

\begin{document}
%----------------------------------------------------------------------

\maketitle

\begin{abstract}
  The massive amount of genomic data appearing for SARS-CoV-2 since
  the beginning of the COVID-19 pandemic has challenged traditional
  methods for studying its dynamics.  As a result, new methods such as
  Pangolin, which can scale to the millions of samples of SARS-CoV-2
  currently available, have appeared.  Such a tool is tailored to take
  as input assembled, aligned and curated full-length sequences, such
  as those found in the GISAID database.  As high-throughput
  sequencing technologies continue to advance, such assembly,
  alignment and curation may become a bottleneck, creating a need for
  methods which can process raw sequencing reads directly.

  In this paper, we propose Reads2Vec, an alignment-free embedding
  approach that can generate a fixed-length feature vector
  representation directly from the raw sequencing reads without
  requiring assembly. Furthermore, since such an embedding is a
  numerical representation, it may be applied to highly optimized
  classification and clustering algorithms. Experiments on simulated
  data show that our proposed embedding obtains better classification
  results and better clustering properties contrary to existing
  alignment-free baselines. In a study on real data, we show that
  alignment-free embeddings have better clustering properties than the
  Pangolin tool and that the spike region of the SARS-CoV-2 genome
  heavily informs the alignment-free clusterings, which is consistent
  with current biological knowledge of SARS-CoV-2.
\end{abstract}

%\keywords{Classification, Clustering, SARS-CoV-2, High-throughput
%  Sequencing, Assembly, Alignment-free}
  
\section{Introduction}
%----------------------------------------------------------------------
\label{sec_introduction}

The first significant worldwide pandemic to arise in the era of widely
accessible high-throughput sequencing tools is
COVID-19~\cite{stephens-2015-genomical}. As a result, SARS-CoV-2 has
orders of magnitude more sequencing information than any other virus
in history. Research efforts to produce publicly available
high-quality full-length nucleotide and amino acid (\eg, spike)
sequences in user-friendly formats (\eg, FASTA) have been greatly
facilitated by global efforts in the collection, assembly, alignment,
and curation of high-throughput sequencing data, such as
GISAID~\cite{gisaid_website_url}.  There are already more than $13$
million sequences on GISAID. There probably exist even more
unassembled or unaligned raw high-throughput sequencing read samples,
because it takes time, money, and quality control to publish a
sequence on GISAID or such databases for sequences. As nations
worldwide continue to spend substantially on sequencing infrastructure
to monitor COVID-19 and upcoming pandemics, this sum will only
rise~\cite{cdc-genomeweb}. It will thus be essential to this
monitoring endeavor to have flexible data analysis processes that can
swiftly and automatically process large amounts of such data, perhaps
in real time.  Since genome (or proteome) assembly and alignment tend
to be time-consuming activities from a computing viewpoint, such
techniques may not have enough time to complete
them~\cite{sboner-2011-cost}. Additionally, selecting and
parameterizing an assembler often needs some expertise, yet it still
produces an inherent bias that we would like to
eliminate~\cite{golubchik-2007-mind}.

By the end of 2020, there were close to one million curated
full-length sequences accessible on databases such as
GISAID~\cite{gisaid_website_url}, which pushed the study of these
sequences into the realm of big data to learn anything about the
dynamics, diversity, and evolution of this virus.  Many conventional
approaches for analyzing viruses that rely on building a phylogenetic
tree, such as Nextstrain~\cite{hadfield2018a}, which can only grow to
thousands of sequences, were rapidly made ineffective. Even
cutting-edge phylogenetic tree construction techniques, such as
IQ-TREE~2~\cite{minh2020iqtree}, can only handle tens of thousands of
sequences by leveraging parallel processing.  The current approaches,
which extend to millions of sequences, use classification or
clustering in some capacity, either in place of or in addition to
phylogenetic reconstruction.  One of the primary aspects of (\eg,
SARS-CoV-2) sequences that these approaches prefer to categorize or
cluster is lineage label (\eg, B.1.1.7 or the Alpha variant). The
cutting-edge Pangolin tool classifier was created using a machine
learning (ML) framework built on top of some of the most notable
phylogenetics research on the lineage dynamics of SARS-CoV-2, such as
~\cite{duplessis2021establishment}.

GISAID sequence metadata, such as lineage labels, are currently built
using the Pangolin tool. There have also been other approaches that
are still based on classification and clustering but were created
independently of the Pangolin tool and its body of literature, such as
~\cite{melnyk2021alpha,ali2021effective,ali2021spike2vec,ali2022pwm2vec}.
On sets of GISAID sequences with ``ground truth'' lineage labels ---
assigned by the cutting-edge Pangolin tool --- such techniques have
been demonstrated to have high prediction potential. A
million~\cite{melnyk2021alpha} or several million
sequences~\cite{ali2021spike2vec} have already been used to illustrate
scalability.

However, only curated full-length nucleotide or amino acid sequences
from sources like GISAID have been used for testing and validation. In
this study, we investigate how these current tools might be adapted to
this environment given that unassembled, unaligned raw high-throughput
sequencing reads data will become increasingly prevalent in the
future. Modern tools like Pangolin accept full-length SARS-CoV-2
nucleotide sequences as input, making it challenging to adapt directly
to the raw high-throughput reads scenario. The $k$-mers-based
approaches~\cite{ali2021effective,ali2021spike2vec}, however, are
\emph{alignment-free} by nature, allowing their direct application to
such unprocessed high-throughput sequencing reads. In this study, we
offer Reads2Vec, a unique alignment-free embedding based on the idea
of spaced seeds~\cite{brinda-2015-spaced}.
%In this paper, we apply these alignment-free approaches, including
%another alignment-free approach we design here based on the notion of
%\emph{minimizer}, to a set of $\approx$7K high-throughput SARS-CoV-2
%reads samples from PCR tests.  We assess these approaches based on
%how they \emph{cluster} this set, using several internal clustering
%quality metrics.  We also compare approaches to each other using
%comparison metrics such as the adjusted Rand index (ARI).  Because
%the Pangolin tool is considered to be the state-of-the-art, we also
%align each sample to the SARS-CoV-2 reference genome to extract a
%consensus sequence in order to allow to obtain a clustering with the
%Pangolin tool, for comparison purposes.  This also allows to show
%that the genomic positions in these aligned sequences which
%contribute the most information (in terms of information gain) to the
%alignment-free clusterings are consistent with known properties of
%the SARS-CoV-2 genome. Finally, we perform various analyses to
%understand the statistical properties of our alignment-free
%embedding, such as compactness.

Our contributions to this paper are as follows:
\begin{enumerate}
\item We propose an efficient and alignment-free embedding approach,
  called Reads2Vec, which can be used as input to any supervised and
  unsupervised machine learning method.
\item Using a simulated dataset of $\approx$8K high-throughput reads
  samples of SARS-CoV-2, we show that Reads2Vec outperforms other
  state-of-the-art alignment-free embedding methods in terms of
  predictive performance, and that with the Synthetic Minority
  Oversampling Technique (SMOTE), we can improve the classification
  performance even more so.
\item Using a real dataset of $\approx$7K high-throughput reads
  samples of SARS-CoV-2 (PCR tests from nasal swabs), we show that
  alignment-free embeddings have better clustering properties (in
  terms of several internal clustering quality metrics) than 
  Pangolin.
\item Using this real dataset, we also show that a disproportionate
  number of genomic positions in the spike region of the aligned
  sequences inform the clustering (in terms of information gain),
  which is consistent with known properties of the SARS-CoV-2
  genome~\cite{kuzmin2020machine}.
\item We perform various clustering comparison analyses of the
  different embeddings, as well as some statistical analyses to
  understand properties of such representations.
\end{enumerate}

The rest of the paper is organized as follows.  Related work may be
found in Section~\ref{sec_related_work}.  All of the techniques we
devise and employ for carrying out the classification and clustering
experiments, as well as for assessing the outcomes of those
techniques, are described in Section~\ref{sec_methods}.  The specifics
of our classification and clustering studies on synthetic and real
data are provided in Section~\ref{sec_experimental_evaluation}. The
outcomes of the conducted experiments are discussed in Section
~\ref{sec_results_discussion}. The article is concluded in
~\ref{sec_conclusion}, where some potential avenues for future work
are also highlighted.

\section{Related Work}
%----------------------------------------------------------------------
\label{sec_related_work}

% In recent years, some effort has been made to understand the behavior
% of SARS-CoV-2 using machine learning models by applying classification
% and clustering on the protein
% sequences~\cite{solis-2018-hiv,ali2021k,ali2021effective,ali2022pwm2vec,tayebi2021robust}. Although
% these studies involve $k$-mers to generate fixed length feature
% embeddings that can be used as input to machine learning models, it is
% not clear if these methods would work as well on the raw sequencing
% reads samples directly, since they have only been proven on
% full-length spike or nucleotide sequences. 
% Some effort has been made to classify metagenomic
% data~\cite{wood-2014-kraken,kawulok-2015-cometa}. However, it is
% unclear if those methods could be applied to SARS-CoV-2 reads
% samples. Therefore, there is a need to explore the effectiveness of
% machine learning models in the context of classifying SARS-CoV-2
% sequencing reads samples. Authors in~\cite{girotto2016metaprob} use
% probabilistic sequence signatures to get accurate metagenomic reads
% binning. Their study aims to divide the reads samples into independent
% sets so that $k$-mer frequencies are not overestimated.

By applying classification and clustering to protein sequences as
in~\cite{solis-2018-hiv,ali2021k,ali2021effective,ali2022pwm2vec,tayebi2021robust},
some effort has been made in recent years to comprehend the behavior
of SARS-CoV-2 using machine learning models. Although these studies
use $k$-mers to produce fixed length feature embeddings that can be
input to machine learning models, it is unclear if these methods would
perform as well on the raw sequencing reads samples directly, since
they have only been demonstrated on full-length spike or nucleotide
sequences.  The classification of metagenomic data has been proposed
in~\cite{wood-2014-kraken,kawulok-2015-cometa}. It is unknown, though,
if such techniques could be used on samples of SARS-CoV-2
reads. Therefore, it is important to explore how well machine learning
models can categorize SARS-CoV-2 sequencing read samples. The authors
of~\cite{girotto2016metaprob} employ probabilistic sequence signatures
to get precise read binning for metagenomic data. Their research
intends to separate the read samples into different groups in order to
prevent overestimation of $k$-mer frequencies.

In many fields, including graph
analytics~\cite{ali2021predicting,AHMAD2020Combinatorial}, smart
grid~\cite{ali2019short,ali2019short_AMI},
electromyography~\cite{ullah2020effect}, clinical data
analysis~\cite{ali2021efficient}, and network
security~\cite{ali2020detecting}, designing fixed length feature
embeddings is a popular research area. Another area of study that
addresses the issue of sequence classification entails creating a
kernel matrix, also known as a Gram matrix, that contains the
separation between pairs of sequences~\cite{ali2022efficient,
  ali2021k}. When doing sequence classification, kernel-based
classifiers like the support vector machine (SVM) may take these
kernel matrices as input. Despite having a strong track record for
prediction performance, kernel-based approaches have one disadvantage:
they have a prohibitive memory cost~\cite{ali2021k}. Given that a
kernel matrix has a dimension of $n \times n$, where $n$ is the number
of sequences, it is nearly impossible to keep a kernel matrix in
memory when $n$ is a huge number (\eg, one million sequences), as
demonstrated in~\cite{ali2021k}.

For classification and clustering problems, a number of machine
learning methods based on $k$-mers have been presented in the
literature~\cite{solis-2018-hiv,queyrel-2020-metagenomic,ali2021k,ali2021effective}.
There are many classical algorithms for sequence
classification~\cite{wood-2014-kraken,kawulok-2015-cometa}. Although
these techniques have been shown to be effective in the corresponding
research, it is unclear if they can be applied to coronavirus
data. The high computational cost of the techniques (due to the large
dimensionality of the data) is also a significant issue with all those
methods, which might lead to longer runtimes for the underlying
classification algorithms.

For machine learning (ML) tasks like classification and clustering, a
number of alignment-based~\cite{kuzmin2020machine,ali2022pwm2vec},
alignment-free~\cite{ali2021spike2vec}, embedding techniques have
recently been presented.  One-Hot-Encoding (OHE), a simple method, was
employed by the authors in~\cite{kuzmin2020machine} to create a
numerical representation for biological sequences. 
However, the
technique is not scalable because of the enormous dimensionality of
the feature vector.  Position weight matrix (PWM)-based techniques are
used by the authors in~\cite{ali2022pwm2vec} to produce feature
embeddings for spike sequences.
However, it only works for aligned
sequences, though, which is a drawback. Making use of phylogenetic
applications of $k$-mers counts were initially investigated
in~\cite{Blaisdell1986AMeasureOfSimilarity} where authors suggested
the creation of precise phylogenetic trees from several coding and
non-coding DNA sequences. The $k$-mers method was used for a
description of sequence analysis in metagenomics. An alignment-free
SARS-CoV-2 classification method based on $k$-mers, was proposed
in~\cite{ali2021spike2vec}.

%Despite having good performance, $k$-mers-based techniques, because of
%their sparsity, their uses are by nature restricted, and the
%difficulty for statistical approaches to calculate correct $k$-mers.

\section{Methods}
%----------------------------------------------------------------------
\label{sec_methods}

Here, we outline all of the methods (designed, and) used in this
paper.  We first discuss the different embedding methods we use to
generate a feature vector representation, including our newly proposed
Reads2Vec.  We also detail the SMOTE method, for overcoming the class
imbalance issue in classification.  We then discuss the clustering
algorithms we used in this paper, including the Pangolin tool as a
baseline for comparison.  We then discuss different internal
clustering evaluation metrics that we use to measure the performance
of the clustering algorithms.  Finally, we detail several clustering
comparison measures, such as the adjusted Rand index, that we use to
compare each pair of clusterings.

\subsection{Embedding Approaches}
\label{section_embeddings_used}

In this section, we describe all embedding approaches we used for the
experiments.  Figure~\ref{fig_embedding_flow_chart} depicts a flow
chart of the three alignment-free embeddings used.

\subsubsection{One-Hot Embedding (OHE)~\cite{kuzmin2020machine}}

As a baseline embedding, we use the one-hot encoding
(OHE)~\cite{kuzmin2020machine}.  This approach generates a binary
vector for each nucleotide $\{A, C, G, T\}$ of a nucleotide sequence,
where the vector associated with nucleotide $N$ will have 1s for the
positions in this sequence that correspond to $N$, and all other
positions will have value $0$.  Such binary vectors are generated for
all nucleotides and are concatenated to form a single vector.

\subsubsection{Spike2Vec~\cite{ali2021spike2vec}}

Spike2Vec~\cite{ali2021spike2vec} is an alignment-free $k$-mers based
approach which computes $k$-mers
directly from the reads sample. The $k$-mers are sub-strings of length
$k$ extracted from the reads using a sliding window, as shown in
Figure~\ref{fig_kmer_generation}.  The $k$-mers allow preserving some
of the sequential ordering information on the nucleotides within each
read. From a read of length $N$ we extract $N - k + 1$ $k$-mers.

These generated $k$-mers are then used to create a fixed-length
feature vector by taking the frequency of each $k$-mer.  The length of
this feature vector is $|\Sigma|^k$ where $\Sigma$ is the character
alphabet (and $k$ is the length of the $k$-mers).  In our experiments
we took $k = 3$ and the alphabet is the nucleotides
$\{A,C,G,T\}$. Therefore the feature vector length is $4^3 = 64$.

\subsubsection{Minimizers2Vec}

We describe a minimizers based embedding, which we name
Minimizers2Vec.  A
\emph{minimizer}~\cite{robertsReducingStorageRequirements2004a} is the
lexicographically smallest $m$-mer in forward and reverse order within
a window of size $k$.

\paragraph{Minimizers2Vec on Real Short Read Sequences.}

Here we just compute the minimizer of each short read (the window is
the read), as shown in Figure~\ref{fig_minimizer_generation}, allowing
for a much more compact frequency vector, as compared to computing all
the $k$-mers.

These generated minimizers are then used to create a fixed-length
feature vector in the same way as in the $k$-mers based embedding.
Clearly, this approach discards almost entirely the sequences, since
it preserves only a representative $m$-mer for each short read.

\begin{figure}[h!]
  \centering
  \begin{subfigure}{.40\textwidth}
    \centering
    \includegraphics[scale=.42]{Figures/kmer_sliding_window.png}
    \caption{$k$-mers for read ``ATCTGACGAC'' }
    \label{fig_kmer_generation}
  \end{subfigure}
  \begin{subfigure}{.40\textwidth}
    \centering
    \includegraphics[scale=0.43]{Figures/minimizer_generation.png}
    \caption{Set of minimizers for a reads sample}
    \label{fig_minimizer_generation}
  \end{subfigure}
  \caption{Example of $k$-mers, and minimizers. This way of computing
    minimizer is used on real data only --- where we have 1 minimizer
    per short read.}
  \label{fig_kmers_minimizer_spaced_kmers}
\end{figure}

\paragraph{Minimizers2Vec on Simulated Sequences.} 

We compute the minimizer of each $k$-mer (the window is the $k$-mer of
length 9), as shown in Figure~\ref{fig_minimizer2Vec_generation},
allowing for a much more compact frequency vector, as compared to
storing all $k$-mers.

These generated minimizers are then used to create a fixed-length
feature vector in the same way as in the $k$-mers based embedding.
Clearly, this approach is more compact since it preserves only a
representative $m$-mer ($m=3$) for each $k$-mer ($k=9$) in the short
read sequence.

\begin{figure}[h!]
  \centering \centering
  \includegraphics[scale=0.26]{Figures/Minimizers2Vec.png}
    \caption{Example for minimizers generation from a short read
      ``ATCTGACGAC'' (used to generate Minimizers2Vec for simulated
      data).}
  \label{fig_minimizer2Vec_generation}
\end{figure}

\subsubsection{Reads2Vec}

Feature vectors for sequences based on $k$-mers frequencies are very
large-sized and sparse, and their size and sparsity negatively impact
the sequence classification performance. Spaced
$k$-mers~\cite{singh2017gakco} (or spaced
seeds~\cite{brinda-2015-spaced}) introduced the concept of using
non-contiguous length $k$ sub-sequences ($g$-mers) for generating
compact feature vectors with reduced sparsity and size. Given a spike
sequence as input, it first computes $g$-mers. From those $g$-mers, we
compute $k$-mers, where $k<g$.  We used $k=4$ and $g=9$ to perform the
experiments (see an example in
Figure~\ref{fig_spaced_kmers_generation}). The size of the gap is
determined by $g-k$.  This method still goes through computationally
expensive operation of bin searching, however.

\begin{figure}[h!]
  \centering
   \centering
    \includegraphics[scale=0.28]{Figures/spaced_kmers_v2.png}
    \caption{Example for set of spaced $k$-mers for a reads sample
      (Read2Vec)}
  \label{fig_spaced_kmers_generation}
\end{figure}

\begin{figure}[h!]
  \centering
   \centering
    \includegraphics[scale=0.20]{Figures/JCB_End_To_End_Flow_Chart_v2.png}
    \caption{Flow chart of embedding generation and experiments for
      the three alignment-free embeddings.}
  \label{fig_embedding_flow_chart}
\end{figure}

\subsection{Synthetic Minority Oversampling Technique (SMOTE)}
\label{section_smote}

% \murray{we need to complete this as well}
% \textcolor{blue}{Gianluca: we need to describe why we need SMOTE}
In machine learning based classification, data imbalance is a regular
occurrence. Minority samples are significantly smaller in the
unbalanced data classification than majority samples, making it
challenging for classifiers to learn the minority set.  By
synthesizing the minority samples or removing the majority samples,
one can increase the sensitivity of the minority at the data
level. Classifiers are more sensitive to minority labels when using
the synthetic minority oversampling method (SMOTE), which synthesizes
minority samples without repetition. Minority samples are randomly
oversampled, which creates equilibrium between the different classes
of samples~\cite{moreo2016distributional}. Random synthesis minority
samples, on the other hand, may result in overfitting issues with
classification algorithms~\cite{yang2017amdo}. The blindness of random
oversampling is addressed by the synthetic minority oversampling
method (SMOTE)~\cite{chawla2002smote}. By randomly picking a minority
sample and linearly interpolating between its neighbor samples, SMOTE
creates non repetitive minority samples. For unbalanced classification
by re-sampling~\cite{liang2020lr}, the Synthetic Minority Oversampling
Technique (SMOTE) has come to be considered the \emph{de facto}
standard. By using interpolation, SMOTE creates new minority class
instances in the areas surrounding the original ones.
% \textcolor{red}{I added more content - can be cut short if needed}
 
% \textcolor{blue}{Prakash : Text in red is copied from Website}
% \textcolor{red}{ SMOTE (Synthetic Minority Oversampling Technique)
%   works by randomly picking a point from the minority class and
%   computing the k-nearest neighbors for this point. The synthetic
%   points are added between the chosen point and its neighbors.
%   \begin{enumerate}
%   \item Choose a minority class as the input vector
%   \item Find its k nearest neighbors (k\_neighbors is specified as an
%     argument in the SMOTE() function)
%   \item Choose one of these neighbors and place a synthetic point
%     anywhere on the line joining the point under consideration and its
%     chosen neighbor
%   \item Repeat the steps until data is balanced
%   \end{enumerate}
% }

\subsection{Clustering Algorithms}
\label{section_clustering_algorithms}

Each of the feature embeddings of the previous section can be given as
input to any of the clustering algorithms that we specify next.

\subsubsection{$k$-means~\cite{kmeans}.}

The classical $k$-means clustering method~\cite{kmeans} clusters
objects based the Euclidean mean.

In order to obtain the optimal number of clusters for $k$-means (and
$k$-modes, below), we have used the Elbow
method~\cite{satopaa2011finding}, which makes use of the knee point
detection algorithm (KPDA)~\cite{satopaa2011finding}, as depicted in
Figure~\ref{fig_elbow_OHE}.  From this figure, it is clear that $5$ is
the optimal number of clusters.

\begin{figure}[h!]
  \centering
  \includegraphics[scale = .42] {Figures/kelbow_one_hot_short_reads.pdf}
  \caption{Use of the elbow method for determining the optimal number
    of clusters in the real data containing 6812 sequences.}
  \label{fig_elbow_OHE}
\end{figure}

\subsubsection{$k$-modes~\cite{huang-1998-extensions}.}

We also cluster with $k$-modes, which is a variant of $k$-means
using \textit{modes} instead of \textit{means}.  Here, pairs of
objects are subject to a dissimilarity measure (\eg, Hamming distance)
rather than the Euclidean mean.
%It quantifies the dissimilarities between objects. The lower this
%value, the closer the two items are.
We have used the same value of $k$ as for $k$-means.

\subsubsection{Pangolin~\cite{pango_tool_ref}}

We also use the Pangolin tool, which takes directly as input the
consensus sequence of a sample of reads (see
Section~\ref{sec:obtaining}), as a baseline for comparison.  The
Pangolin tool assigns the most likely lineage \cite{Pango_Lineage}
(called the Pango lineage) to a SARS-CoV-2 genome sequence. The
Pangolin dynamic nomenclature~\cite{rambaut-2020-nomenclature} was
devised for identifying SARS-CoV-2 genetic lineages of epidemiological
significance and is used by researchers and public health authorities
throughout the world to track SARS-CoV-2 transmission and
dissemination.
% It provides support to sequence clustering.  and is used to perform
% analyses on SARS-COV-2 sequence data.\todo{GDV. What kind of
% analysis?}

\subsection{Clustering Evaluation Metrics}
\label{sec:cluster-eval}

In this section, we describe the internal clustering evaluation
measures used to assess the quality of clustering.
%We also describe different metrics to compare different clustering
%approaches.

\subsubsection{Silhouette Coefficient~\cite{rousseeuw1987silhouettes}.}

% The separation distance between the generated clusters can be
% studied using the silhouette analysis~
Given a feature vector, the silhouette coefficient computes how
similar the feature vector is to its own cluster (cohesion) compared
to other clusters (separation)~\cite{scikit-learn}.  Its score ranges
between $[-1, 1]$, where $1$ means the best possible clustering and
$-1$ means the worst possible clustering.

% Silhouette score ranges between $[-1, 1]$, for a given sample score
% close to $+1$ denotes that the sample is far from the surrounding
% clusters. Score close to $0$ indicate that the sample is on or near
% the decision boundary between two neighboring clusters, whereas
% negative values indicate that the sample is likely to be assigned to
% an incorrect cluster.

% Given a data point $i$ and a clustering $C$, we first need to
% compute average distance between $i$ and all other data points in
% the same cluster. More formally for $i \in C_I$:

% \begin{equation}
%     a(i) = \frac{1}{\vert C_I \vert -1 } \sum_{j \in C_I, i \neq j}^{} d(i,j)
% \end{equation}
% where $d(i,j)$ is the distance from pint $i$ to point $i$ in the cluster $C_I$. The next step is to compute the distance of point $i$ with some other cluster $C_J$ where $J \neq I$ (the cluster $C_J$ is supposed to be the closest cluster to $C_i$). More formally:
% \begin{equation}
%     b(i) = min_{J \neq I} \frac{1}{\vert C_J \vert} \sum_{j \in C_J}^{} d(i,j).
% \end{equation}

% Now for data point $i$, the silhouette score $s(i)$ is calculated as
% follows:

% \begin{equation}
%   s(i) = \frac{b(i) - a(i)}{max\{a(i),b(i)\}}.
% \end{equation}

% Finally, the silhouette coefficient of a clustering is the average
% slihouette score over all data points (i.e. the samples in our
% case).

% where $x$ represents the average distance between a sample and all
% other data points in the same class, and $y$ represents the average
% distance between $x$ sample and all other data points in the
% following cluster.  \todo{GDV. Usually the silhouette is defined for
% a single data point, and the silhouette of the clustering is the
% average of the silhouettes over all points. Here instead it is a
% single ratio taken over some averages, which I don't think it's the
% same}

\subsubsection{Calinski-Harabasz Score~\cite{calinski1974dendrite}.}

The Calinski-Harabasz score evaluates the validity of a clustering
based on the within-cluster and between-clusters dispersion of each
object with respect to each cluster (based on sum of squared
distances)~\cite{scikit-learn}. There is no defined range for this metric. A higher score denotes better defined
clusters.
% \textcolor{blue}{Gianluca: specify the range of values}
% The formula for Calinski-Harabasz is defined as:

% \begin{equation} 
%   CH(C) = \frac{tr(B_k)}{tr(W_k)} \times \frac{|U|-k}{k-1}
% \end{equation}

% where $C$ is a set of clusters of a universe set $U$, $k$ is the
% number of clusters of $c$, $tr(B k)$ is the trace of the dispersion
% matrix across clusters, $tr(W k)$ is the trace of the dispersion
% matrix within a cluster.\todo{GDV. I assume that we have used the
% scikit-learn implementation of those scores. In this case, we need
% to cite scikit-learn\cite{scikit-learn}}

\subsubsection{Davies-Bouldin Score~\cite{davies1979cluster}.}

Given a feature vector, the Davies-Bouldin score computes the ratio of
within-cluster to between-cluster distances~\cite{scikit-learn}. There is no defined range for this metric. A
smaller score denotes groups are well separated, and the clustering
results are better.
% \textcolor{blue}{Gianluca: specify the range of values}

% \begin{equation} 
%   DB(C)= \frac{1}{k} \sum_{i=1}^{k} max_{j \leq k,j \neq i} D_{ij},
%   \ k= \vert C \vert
% \end{equation}

% where $D_{ij}$ is the ratio of ``within-to-between cluster distance"
% for the $i^{th}$ and $j^{th}$ clusters and C denotes clusters.  For
% cluster $i$, the worst-case ratio of within-to-between cluster is
% indicated by $D_{ij}$. As a result, by lowering this index, we may
% ensure that clusters are the most distinct from one another. When
% the clustering is better, the DB score is smaller.\todo{GDV. Not
% sure I follow the definition. IMO
% \url{https://scikit-learn.org/stable/modules/clustering.html} and
% wikipedia are clearer}

\subsection{Clustering Comparison Metrics}
\label{section_clustering_comparison_metrics}

% We have also used some standard measures to compare the clusterings
% obtained by different approaches.

To compare different clustering approaches, we use the following
metrics:

\subsubsection{Adjusted Rand Index~\cite{hubert-1985-comparing}.}

Given two clusterings, the adjusted Rand index (ARI) computes the
similarity between them by considering all pairs of clusters output
and counts the pairs that are assigned to the same or different
clusters. The value ranges between $0$ and $1$, where $1$ denotes an
identical labeling, and approaches 0 as they become more different.  A
pair of random labelings has an expected ARI of almost 0.

%\todo{SC: I
%  would add that we chose Pangolin as ground truth, albeit as an
%  arbitrary choice, just a have something to compare. Of course then
%  Pangolin would have a score of 1.0 in all measures, but this is a
%  bias of the measure, not the accuracy of the method.}\todo{Murray:
%  in some sense we do choose Pangolin as ground truth, since the
%  dataset statistics and t-SNE plots are in terms of labels assigned
%  to sequences by the Pangolin tool.  However, I am hesitant to do so
%  because there really is no ground truth here, and moreover, the
%  alignment is actually probably quite bad, nd I don't want to invite
%  any more attention to this.  IIUC, in the absence of a ground truth,
%  the ARI just compares the similarity of a pair of clusterings, and
%  so this grid is just all-against-all ARI, i.e., all we can say is
%  that the clusterings of our tool and Pangolin quite are different.}

% The adjusted Rand index~\cite{hubert-1985-comparing} is a measure of
% similarity between two clusterings of a dataset with $n$ points.

% It can be denoted by

% \begin{equation}
%   R=\frac{a+b}{{n \choose 2}}
% \end{equation}

% where $a$ is the number of pairs of points in the same cluster for
% both clusterings, $b$ is the number of pairs of points that are in
% two different clusters for both clusterings.

\subsubsection{Fowlkes-Mallows Index~\cite{fowlkes-1983-comparing}.}

Given two clusterings, the Fowlkes-Mallows index (FMI) first computes
the confusion matrix for the clustering output.  The FMI is then
defined by the geometric mean of the precision and the recall.  Its value range from 0 to 1 with larger value indicating a greater similarity between the
clusters.

% The Fowlkes-Mallows index 
% It is an external evaluation approach 
% for determining how similar two clusterings are. 
% It could be a comparison of two clustering methods or a comparison of clustering and a benchmark classification. However most cases, there is no benchmark classification available for clustering problems, mostly is 
% used to compare different clustering methods.
% \todo{GDV I think it's more important to say how we are using that, then how it can be used \\
% Prakash: Addressed but To cut it short I just gave One Line def, Details are in Commented section. \\ LMK if any changes needed.} 
% The Fowlkes-Mallows Index is the geometric mean of the precision and the recall, 
% \begin{equation}
% \textsc{fmi} =\sqrt{Precision \times Recall}
% \end{equation}
% A larger value of the Fowlkes-Mallows index
% indicates a greater similarity between the clusters.

\subsubsection{Completeness Score~\cite{rosenberg-2007-v}.}

Given two clustering outputs, the completeness score (CS) evaluates
how much similar samples are placed in the same cluster.  Its value
ranges between $[0,1]$, where $1$ means complete clustering agreement
and approaches 0 the further it deviates from this.

% Completeness score measures how much similar samples are put
% together by the clustering algorithm. Completeness score is defined
% by,

% \begin{equation}
%   c=1 - \frac{H(K \vert C)}{H(K)}
% \end{equation}
% The term $H(K \vert C)$ it contains $\frac{n_{ck}}{n_c}$
% which represents the ratio between the number of samples labeled c in
% cluster k and the total number of samples labeled c.  When all
% samples of kind c have been assigned to the same cluster k, the
% completeness equals $1$.
% The clustering can be complete if all data points of the same class label belong to the same cluster but still might not be homogeneous because a may cluster contain data points from many class labels.

\subsubsection{V-measure~\cite{rosenberg-2007-v}.}

Given two clustering outputs, we first compute homogeneity (evaluate
if objects belong to the same or different cluster) and completeness
(evaluate how much similar samples are placed together by the
clustering algorithm).  The V-measure is then defined by the harmonic
mean of homogeneity and completeness.
% \begin{equation}
%   \textsc{nmi} =2 \times \frac{h \times c}{h+c} 
% \end{equation}
This score is a number between $[0,1]$ where $1$ indicates a perfect
matching and approaches 0 the further it deviates from this.

% \subsubsection{Fowlkes-Mallows index.}
% \textcolor{red}{
% The Fowlkes-Mallows index~\cite{fowlkes-1983-comparing} is an external evaluation approach 
% % for determining how similar two clusterings are. 
% % It could be a comparison of two clustering methods or a comparison of clustering and a benchmark classification. However most cases, there is no benchmark classification available for clustering problems, mostly is 
% used to compare different clustering methods.\todo{GDV I think it's more important to say how we are using that, then how it can be used} 
% The Fowlkes-Mallows Index is the geometric mean of the precision and the recall, which can be denoted
% by }

% \begin{equation}
% FMI=\sqrt{\left(\frac{T P}{T P+F P}\right)\left(\frac{T P}{T P+F N}\right)}
% \end{equation}
% \textcolor{red}{
% TP stands for True Positive, which in our situation is the number of pairs of points in both embeddings that belong to the same cluster.
% FP stands for False Positives, or the number of pairs of points that belong to the same clusters in the first embedding cluster but not in the other.
% The number of False Negatives (FN) is the number of pairs of points in the predicted labels that do not belong in the comparing cluster.\todo{GDV. What is $k$? Can it be removed. How do we compute true/false positive/negatives (I suppose that we should define it as $B(C_1, C_2)$ and use $C_2$ as the ground truth}
% A larger value of the Fowlkes-Mallows index
% indicates a greater similarity between the clusters.} 

% \subsubsection{V-measure.}
% The V-measure~\cite{rosenberg-2007-v}, or Normalised Mutual
% Information score is a harmonic mean of two other 
% measures: homogeneity and completeness. \\
% A totally homogeneous clustering is one in which all data points in each cluster have same class label. It is defined using Shannon’s entropy.  
% \todo{GDV What is a class? Is it a cluster in a ground truth clustering?}
% If all the data points that are members of a given class are elements of the same cluster, the clustering result is complete. But here the cluster can have many class labels. In other words if a cluster has all elements of two classes, it is complete but not homogeneous. Completeness and homogeneous are inversely proportional. V measure score is defined as 
% \begin{equation}
%   V_{\beta} =(1+\beta) \times \frac{h \times c}{\beta \dot h+c} 
% \end{equation}

% \begin{equation}
% \text{where }  h=1 - H(C \vert K)H(C)  \text{ and } c \text{ is completeness score }
% \end{equation}
% $\beta$ is the weight of harmonic mean of homogeneity and completeness. This score is a number between $0$ and $1$ that indicates how good is the clustering partition. Where $1$ stands for perfectly complete V measure.
% \todo{GDV Are smaller or larger values better?}

% \subsubsection{Completeness score.}

% Completeness score~\cite{rosenberg-2007-v} although measures how much
% similar samples are put together by the clustering algorithm.

% \begin{equation}
%   c=1 - \frac{H(K \vert C)}{H(K)}
% \end{equation}

% If one looks at the term $H(K \vert C)$ it contains $\frac{n_{ck}}{n_c}$
% which represents the ratio between the number of samples labeled c in
% cluster k and the total number of samples labeled c.  When all
% samples of kind c have been assigned to the same cluster k, the
% completeness equals $1$.

\section{Experimental Evaluation}
%----------------------------------------------------------------------
\label{sec_experimental_evaluation}

In this section, for each of the simulated and real datasets used, we
describe how such dataset was collected, and then provide some
statistics and visualization of the dataset.  We then describe the
experimental setup for performing the classification and clustering on
the respective datasets.

\subsection{Experiments on Simulated Data}
%----------------------------------------------------------------------

In this section, we first give the technical details on how the data
were simulated, and then we give some statistics and visualization of
this data, as well as mention the classifiers and metrics we used to
assess the performance in the results section.

\subsubsection{Obtaining the Dataset}
\label{sec:obtaining}

We obtained a random sample of 10,181 full-length nucleotide sequences
from the $\approx$4 million available on the
GISAID~\cite{gisaid_website_url} database at the time.  Each sequence
is annotated with a SARS-CoV-2 lineage.  Of these, we kept only those
sequences whose label appears ten or more times in the set, retaining
8140 (of the 10,181) sequences (see
Table~\ref{simulated_short_reads_table}).  From each such sequence, we
then simulated a set of short reads using
InSilicoSeq~\cite{insilicoseq}\footnote{\url{https://github.com/HadrienG/InSilicoSeq}}
with default parameters other than using the MiSeq error model.  We
implemented these steps in a Snakemake~\cite{molder-2021-snakemake}
pipeline, which is available online for
reproducibility\footnote{\url{https://github.com/murraypatterson/ncbi-sra-runs-pipeline/tree/main/simulation}}.
The lineage which labels each sequence provides a ground truth label
for the corresponding set of short reads simulated from the sequence.

\subsubsection{Dataset Statistics}

Table~\ref{simulated_short_reads_table} provides the statistics about
the dataset comprising simulated sets of reads. Note that we combined
similar lineages with each other to reduce the class imbalance, \eg,
all Delta lineages including B.1.617.2 along with lineages starting
with AY are combined to make a single class label ``Delta". In the
same manner, the Gamma, Epsilon, and Beta lineages are combined with
all of their descendant lineages to make single labels, namely
``Gamma", ''Epsilon", and ``Beta", respectively. Note that the ``Name"
column contains the class label, which we are using for the supervised
machine learning task, \ie, classification.

\begin{table}[h!]
  \centering
  \begin{tabular}{p{5.5cm}p{2cm}p{1.5cm}p{1.5cm}}
    \toprule
    Lineage & Region & Name &  No.~of Sequences \\
    \midrule \midrule 
    % B.1.1.7 & UK~\cite{galloway2021emergence} & Alpha &	1832 \\
    % B.1 & \_ & \_ &	1634 \\
    % B.1.177 & \_ & \_ & 1541 \\
    % B.1.617.2 &	 India~\cite{yadav2021neutralization}  &  Delta & 522 \\
    % A.2 & \_ & \_ &	447 \\
    % B.1.1 & \_ &\_ &	347 \\
    % B.1.2 & \_ &\_ &	344 \\
    % AY.25 & India~\cite{CDS_variantDef}  & Delta &	145 \\
    
    B.1.617.2 and AY lineages & India & Delta &  4699  \\
    B.1.1.7 & UK & Alpha &  2587 \\
    B.1.177 & Spain & Spanish & 395 \\
    P.1 and descendant lineages & Brazil & Gamma    &   231 \\
    B.1.427 and B.1.429 & USA & Epsilon  & 116 \\
    B.1.351 and descendant lineages & South Africa & Beta &  65 \\
    B.1.621 & USA & Mu & 19 \\
    B.1.525 & Canada & Eta & 16 \\
    B.1.617.1 & India & Kappa & 12 \\
    \midrule
    Total & - & - & 8140\\
    \bottomrule
  \end{tabular}
  \captionof{table}{Variants distribution for a total of 8140
    simulated sets of reads.  Note that closely-related lineages (with
    the same name, \eg, Delta) have been merged.
%\murray{why are the lineages so broadly defined?  e.g., "P.1 and
%descendant lineages"?}  \textcolor{blue}{Prakash: Theese are from
%Pango/WHO website do we cite it here in caption as well ?}  }
}
  \label{simulated_short_reads_table}
\end{table} 

\subsubsection{Data Visualization}
The t-distributed Stochastic Neighbor Embedding (t-SNE) plots for
simulated data without and with SMOTE are shown in
Figure~\ref{fig_tsne_simulated} and~\ref{fig_tsne_simulated_SMOTE},
respectively. In Figure~\ref{fig_tsne_simulated}, we can observe that
most of the lineages, including Alpha, Delta, Epsilon, and Gamma are
clearly grouped together for all embedding generation methods. An
interesting observation here is that in case of Reads2Vec, we can see
that Alpha lineage is more clearly separated from the Delta lineage as
compared to Minimizers2Vec and Spike2Vec, making it superior in
terms of preserving distance between sequences in the original
data. In Figure~\ref{fig_tsne_simulated_SMOTE}, we can observe that
although SMOTE introduces more data, most of the lineages are clearly
separated from each other in case of Spike2Vec and Reads2Vec. In case
of Minimizers2Vec, we can observe that Kappa lineage is separated into
two different groups, which is not the case with Spike2Vec and
Reads2Vec.

\begin{figure}[h!]
\centering
\begin{subfigure}{.33\textwidth}
  \centering
  \includegraphics[scale=0.08] {Figures/tsne/simulated_data_8140/kmer.png}
   \caption{Spike2Vec}
  \label{fig_tsne_one_hot}
\end{subfigure}%
\begin{subfigure}{.33\textwidth}
  \centering
  \includegraphics[scale=0.08] {Figures/tsne/simulated_data_8140/minimizer.png}
  \caption{Minimizers2Vec}
  \label{fig_tsne_kmers}
\end{subfigure}%
\begin{subfigure}{.33\textwidth}
  \centering
  \includegraphics[scale=0.08] {Figures/tsne/simulated_data_8140/spaced_kmer.png}
   \caption{Reads2Vec}
  \label{fig_tsne_minimizer}
\end{subfigure}
\\
\begin{subfigure}{1\textwidth}
  \centering
  \includegraphics[scale=0.37] {Figures/tsne/simulated_data_8140/legends.png}
  \label{fig_tsne_legend}
\end{subfigure}
\caption{t-SNE plots for different embedding methods. This figure is best seen in color.}
\label{fig_tsne_simulated}
\end{figure}

\begin{figure}[h!]
\centering
\begin{subfigure}{.33\textwidth}
  \centering
  \includegraphics[scale=0.08] {Figures/tsne/simulated_data_8140/SMOTE_kmer.png}
   \caption{Spike2Vec}
  \label{fig_tsne_one_hot}
\end{subfigure}%
\begin{subfigure}{.33\textwidth}
  \centering
  \includegraphics[scale=0.08] {Figures/tsne/simulated_data_8140/SMOTE_minimizer.png}
  \caption{Minimizers2Vec}
  \label{fig_tsne_kmers}
\end{subfigure}%
\begin{subfigure}{.33\textwidth}
  \centering
  \includegraphics[scale=0.08] {Figures/tsne/simulated_data_8140/SMOTE_spaced_kmer.png}
  \caption{Reads2Vec}
  \label{fig_tsne_minimizer}
\end{subfigure}
\\
\begin{subfigure}{1\textwidth}
  \centering
  \includegraphics[scale=0.37] {Figures/tsne/simulated_data_8140/legends.png}
  \label{fig_tsne_legend}
\end{subfigure}
\caption{t-SNE plots for different embedding methods after SMOTE. This figure is best seen in color.}
\label{fig_tsne_simulated_SMOTE}
\end{figure}

\subsubsection{Evaluation Metrics And Classifiers}

Support vector machine (SVM), naive Bayes (NB), multi-layer perceptron
(MLP), $k$-nearest neighbors (KNN), random forest (RF), logistic
regression (LR), and decision tree (DT) classifiers are used as
baseline models for sequence classification. The performance of
various models are evaluated using average accuracy, precision,
recall, F1 (weighted), F1 (macro), receiver operator characteristic
curve (ROC), area under the curve (AUC), and training runtime
metrics. Furthermore, the one-vs-rest approach is used to convert the
binary evaluation metrics to multi-class ones.

\subsection{Experiments on Real Data}
%----------------------------------------------------------------------

In this section, we first give the technical details on how the data
were collected and annotated, and then we give some statistics and
visualization of this data.

\subsubsection{Obtaining the Dataset}

We downloaded $6812$ raw high-throughput reads samples of COVID-19
patient nasal swab PCR tests from the NCBI SARS-CoV-2 SRA runs
resource\footnote{\url{https://www.ncbi.nlm.nih.gov/sars-cov-2/}} (see
Section~\ref{sec:stats-real} for some basic descriptive statistics on
these data).  We then obtained the reference genome from
Ensembl\footnote{\url{https://covid-19.ensembl.org/index.html}}, and
aligned each sample to this reference genome, called the variants in
this sample with respect to the reference genome, and then inserted
these variants into the reference genome to generate a consensus
sequence. The basic steps in more detail are depicted in
Figure~\ref{fig_pipeline}.  We implemented these steps in a
Snakemake~\cite{molder-2021-snakemake} pipeline, which is available
online for
reproducibility\footnote{\url{https://github.com/murraypatterson/ncbi-sra-runs-pipeline}}.
Since these sets of short reads have no ground truth lineage
information, we needed to produce a consensus sequence from each such
set, in order annotate it with a lineage label using the
state-of-the-art Pangolin tool.  Note that the one-hot embedding
(OHE), \eg, Figure~\ref{fig_tsne_one_hot}, is also computed on such a
consensus sequence, while the alignment-free Spike2Vec and
Minimizers2Vec embeddings are computed directly from the raw reads.

\begin{figure}[h!]
  \centering
  \includegraphics[width=\textwidth]{Figures/pipeline.jpg}
  \caption{Pipeline for producing consensus sequences.  \textbf{(1)}
    We download the reads samples using the NCBI command line batch
    Entrez tool \texttt{fasterq-dump}
    (\url{https://rnnh.github.io/bioinfo-notebook/docs/fasterq-dump.html}),
    and we download the SARS-CoV-2 reference genome (Wuhan-Hu-1)
    \texttt{GCA\_009858895.3.fasta}.  \textbf{(2)} Each sample is
    aligned to this reference genome using \texttt{bwa
      mem}~\cite{li-2013-bwa} and then compressed to BAM
    (\url{https://samtools.github.io/hts-specs/SAMv1.pdf}) format
    using \texttt{samtools}~\cite{danecek-2021-samtools}.
    \textbf{(3)} From the sample, an ``mpileup'' is generated with
    \texttt{bcftools mpileup}~\cite{danecek-2021-samtools}, variants
    are then called with \texttt{bcftools call}, and the resulting VCF
    (\url{https://samtools.github.io/hts-specs/VCFv4.2.pdf}) file is
    normalized with \texttt{bcftools norm}.  \textbf{(4)} Finally, the
    consensus FASTA sequence is generated by inserting into the
    reference genome the variants from the VCF file generated in the
    previous step, with \texttt{bcftools consensus}.}
  \label{fig_pipeline}
\end{figure}

% \textcolor{blue}{Gianluca: Maybe separate figure and table above}

\subsubsection{Dataset Statistics and Visualization}
\label{sec:stats-real}

The dataset statistics (the labels are assigned to the samples using
the Pangolin tool) for our experiments on raw high-throughput reads
samples are given in Figure~\ref{NCBI_short_reads_pie_chart} and
Table~\ref{NCBI_short_reads_table}.  Both the
dataset\footnote{\url{https://drive.google.com/drive/folders/1i4uRrnkjkwUA93EOl8YORBBLb7yIFIm1?usp=sharing}}
and the code used in this paper are available
online\footnote{\url{https://github.com/murraypatterson/ncbi-sra-runs-pipeline}}.
% \textcolor{blue}{Gianluca: if possible, use zenodo or dryad to store
%   datasets}

\begin{figure}[tb!]
  \centering
  \includegraphics[width=.7\textwidth]{Figures/NCBI_SHort_Read_Stats.png}
  \captionof{figure}{Lineage distribution for the 6812 sets of short
    reads obtained from NCBI, according to the annotation by Pangolin.}
  \label{NCBI_short_reads_pie_chart}
\end{figure}

\begin{table}
  \centering
  \begin{tabular}{p{1.5cm}ccp{1.5cm}}
    \hline
    Lineage & Region & Name &  No.~of Sequences \\
    \hline \hline 
    B.1.1.7 & UK % ~\cite{galloway2021emergence} 
    & Alpha &	1832 \\
    B.1 & \_ & \_ &	1634 \\
    B.1.177 & \_ & \_ & 1541 \\
    B.1.617.2 &	 India %~\cite{yadav2021neutralization} % 
    &  Delta & 522 \\
    A.2 & \_ & \_ &	447 \\
    B.1.1 & \_ &\_ & 347 \\
    B.1.2 & \_ &\_ & 344 \\
    AY.25 & India
    % ~\cite{CDS_variantDef}  
    & Delta &	145 \\
    \hline
  \end{tabular}
  \captionof{table}{Lineage distribution for the 6812 sets of short
    reads obtained from NCBI, according to the annotation by Pangolin, in
    tabular form.}
  \label{NCBI_short_reads_table}
\end{table}

A first analysis is to check if there is any natural clustering or
hidden pattern in the data.  However, it is very difficult to visually
analyze the information in higher dimensions (\ie, dimensions $>
2$). For this purpose, we mapped the data to 2-dimensional real
vectors using the t-distributed stochastic neighbor embedding (t-SNE)
approach~\cite{van2008visualizing}. The t-SNE plots for different
embedding methods are given in Figure~\ref{fig_tsne}. For OHE
(Figure~\ref{fig_tsne_one_hot}), we can observe that some data is
separated into different clusters, such as in the case of lineages A2
and B.1.1.7. For the t-SNE plots of Spike2Vec and Minimizers2Vec
embeddings (Figure~\ref{fig_tsne_kmers} and~\ref{fig_tsne_minimizer}
respectively), since we are computing the feature embedding directly
from the raw reads sample data, we can observe the difference in the
structure of data as compared to OHE. Although there is some
overlapping between different lineages, we can still observe some
separation between a few lineages, such as B.1.1.7 and
B.1.617.2. Since there is no clear separation between different
lineages in any of the t-SNE plots (apart from some small groups),
clustering this dataset is not easy.

% \begin{figure}[h!]
%   \centering
%   \begin{subfigure}{.34\textwidth}
%     \centering
%     \includegraphics[scale=0.107] {Figures/tsne/real_data_6812/one_hot_tsne_plot_6812.png}
%     \caption{OHE}
%     \label{fig_tsne_one_hot}
%   \end{subfigure}%
%   \begin{subfigure}{.33\textwidth}
%     \centering
%     \includegraphics[scale=0.107] {Figures/tsne/real_data_6812/kmer_tsne_plot_6812.png}
%     \caption{$k$-mers}
%     \label{fig_tsne_kmers}
%   \end{subfigure}%
%   \begin{subfigure}{.33\textwidth}
%     \centering
%     \includegraphics[scale=0.107] {Figures/tsne/real_data_6812/minimizer_tsne_plot_6812.png}
%     \caption{minimizers}
%     \label{fig_tsne_minimizer}
%   \end{subfigure}
%   \caption{t-SNE plots for different embedding methods (labels by
%     Pangolin). This figure is best seen in color. \textcolor{red}{(Real data 6812 Sequences)}}
%   \label{fig_tsne}
% \end{figure}


\begin{figure}[h!]
  \centering
  \begin{subfigure}{.34\textwidth}
    \centering
    \includegraphics[scale=0.107] {Figures/tsne/real_data_6812/ohe_6812.png}
    \caption{OHE}
    \label{fig_tsne_one_hot}
  \end{subfigure}%
  \begin{subfigure}{.33\textwidth}
    \centering
    \includegraphics[scale=0.107] {Figures/tsne/real_data_6812/kmer_6812.png}
    \caption{$k$-mers}
    \label{fig_tsne_kmers}
  \end{subfigure}%
  \begin{subfigure}{.33\textwidth}
    \centering
    \includegraphics[scale=0.107] {Figures/tsne/real_data_6812/minimizer_6812.png}
    \caption{minimizers}
    \label{fig_tsne_minimizer}
  \end{subfigure}
  \\
\begin{subfigure}{1\textwidth}
  \centering
  \includegraphics[scale=0.37] {Figures/tsne/simulated_data_8140/legends.png}
  \label{fig_tsne_legend}
\end{subfigure}
  \caption{t-SNE plots for different embedding methods on real data containing 6812 Sequences (labels by
    Pangolin). This figure is best seen in color.}
  \label{fig_tsne}
\end{figure}


% \begin{figure}[ht!]
%     \centering
%     \includegraphics[scale = .30] {Figures/NCBI_SHort_Read_Stats.png}
%     \caption{Variants distribution for the NCBI short reads dataset}
%     \label{NCBI_short_reads_dataset_stats}
% \end{figure}

\section{Results and Discussions}
%----------------------------------------------------------------------
\label{sec_results_discussion}

In this section, we first provide all of the results for the
experiments on the simulated data following which we report results
for the experiments on the real data.

We start with the classification results on the simulated data for the
three alignment-free embedding methods mentioned in
Section~\ref{section_embeddings_used}. Afterward, we present the
result after applying SMOTE, explained in Section~\ref{section_smote},
to tackle the class imbalance problem. We then report the clustering
results on simulated data for the algorithms listed in
Section~\ref{section_clustering_algorithms}, and perform model
evaluation using the internal clustering metrics mentioned in
Section~\ref{sec:cluster-eval} and the cluster comparison metrics of
Section~\ref{section_clustering_comparison_metrics}. We show the
results and evaluation of clustering for different possible
combinations for simulated data. Finally, we perform a statistical
analysis using Pearson and Spearman correlation to evaluate the
compactness of each feature embedding on the simulated data.

On real data, where there is no ground truth labeling, we perform
clustering on such data, evaluating it using the internal clustering
metrics mentioned in Section~\ref{sec:cluster-eval} and cluster
comparison metrics of
Section~\ref{section_clustering_comparison_metrics}.  We then show,
through information gain (IG), that the spike region of the SARS-CoV-2
genome heavily informs the clustering, which is consistent with
current biological knowledge its viral structure.  Finally, we perform
feature importance through Pearson and Spearman correlation to
evaluate the compactness of each feature embedding on the real data.

% Later we show the results for clustering using different clustering algorithms and the evaluation using the evaluation metrics on Real data. In addition, the importance of each nucleotide position to the label assigned by the combination above, which had the best overall internal clustering score, is measured in terms of information gain for the real dataset.
% \todo{GDV I don't think we have a description of what class labels are. For example, are they computed by our tools or by Pango?}


\subsection{Results on Simulated Data}
This section provides the results of the classification and clustering
experiments performed on simulated data.
%----------------------------------------------------------------------

\subsubsection{Classification Results}

The classification results for different embedding methods (without
SMOTE) are reported in
Table~\ref{tbl_classification_without_smote}. We can observe that the
proposed Reads2Vec embedding method outperforms all other methods in
terms of all evaluation metrics except the training runtime. The
average accuracy achieved by Reads2Vec is up to 99.8\% and better than
the among baseline embeddings where we achieved 98.9\% with Spike2Vec.

Table~\ref{tbl_classification_with_smote} reports the classification
with SMOTE for several embedding techniques. We can see that, with the
exception of training runtime, the suggested Reads2Vec embedding
technique outperforms all other methods in terms of evaluation
metrics. However, the average accuracy of Reads2Vec is up marginally
because of the smaller room for improvement, but still, we can say
that Reads2Vec gives the best results compared with other embeddings.
% \textcolor{red}{SARWAN: Need to complete this!!}

\begin{table}[h!]
    \centering
    \resizebox{1\textwidth}{!}{
    \begin{tabular}{p{2.1cm}p{0.9cm}p{2.4cm}p{2.4cm}p{2.4cm}p{2.4cm}p{2.4cm}p{2.4cm}|p{2.4cm}}
    \toprule
        Method & ML Algo. & Acc. & Prec. & Recall & F1 (Weig.) & F1 (Macro) & ROC AUC & Train Time (Sec.) \\
        \midrule \midrule
        % \multirow{21}{1.2cm}{Before SMOTE} & 
        \multirow{7}{1.2cm}{Spike2Vec}
& SVM & 0.984  $\pm$ 0.002 & 0.984  $\pm$ 0.002 & 0.984  $\pm$ 0.002 & 0.983  $\pm$ 0.002 & 0.872  $\pm$ 0.030 & 0.924  $\pm$ 0.017 & 0.356  $\pm$ 0.026	\\
& NB & 0.102  $\pm$ 0.094 & 0.662  $\pm$ 0.077 & 0.102  $\pm$ 0.094 & 0.146  $\pm$ 0.106 & 0.066  $\pm$ 0.048 & 0.535  $\pm$ 0.027 & \textbf{0.038}  $\pm$ 0.004   \\
& MLP & 0.968  $\pm$ 0.004 & 0.969  $\pm$ 0.004 & 0.968  $\pm$ 0.004 & 0.968  $\pm$ 0.004 & 0.705  $\pm$ 0.046 & 0.846  $\pm$ 0.023 & 2.575  $\pm$ 0.407  \\
& KNN & 0.916  $\pm$ 0.006 & 0.919  $\pm$ 0.005 & 0.916  $\pm$ 0.006 & 0.911  $\pm$ 0.006 & 0.666  $\pm$ 0.034 & 0.784  $\pm$ 0.022 & 0.317  $\pm$ 0.047  \\
& RF & 0.954  $\pm$ 0.005 & 0.953  $\pm$ 0.006 & 0.954  $\pm$ 0.005 & 0.947  $\pm$ 0.007 & 0.605  $\pm$ 0.080 & 0.759  $\pm$ 0.038 & 2.931  $\pm$ 0.284   \\
& LR & 0.989  $\pm$ 0.002 & 0.988  $\pm$ 0.002 & 0.989  $\pm$ 0.002 & 0.988  $\pm$ 0.003 & 0.851  $\pm$ 0.042 & 0.915  $\pm$ 0.011 & 1.572  $\pm$ 0.139   \\
& DT & 0.922  $\pm$ 0.004 & 0.922  $\pm$ 0.005 & 0.922  $\pm$ 0.004 & 0.922  $\pm$ 0.004 & 0.530  $\pm$ 0.037 & 0.758  $\pm$ 0.018 & 0.681  $\pm$ 0.063   \\


         \cmidrule{2-9}
        \multirow{7}{1.2cm}{Minimizers2Vec}
& SVM & 0.981 $\pm$ 0.002 & 0.981 $\pm$ 0.001 & 0.981 $\pm$ 0.002 & 0.981 $\pm$ 0.001 & 0.862 $\pm$ 0.045 & 0.935 $\pm$ 0.025 & 0.28 $\pm$ 0.015		\\
& NB & 0.339 $\pm$ 0.086 & 0.85 $\pm$ 0.036 & 0.339 $\pm$ 0.086 & 0.404 $\pm$ 0.093 & 0.189 $\pm$ 0.048 & 0.639 $\pm$ 0.033 & 0.041 $\pm$ 0.003       \\
& MLP & 0.973 $\pm$ 0.002 & 0.972 $\pm$ 0.001 & 0.973 $\pm$ 0.002 & 0.972 $\pm$ 0.002 & 0.741 $\pm$ 0.045 & 0.865 $\pm$ 0.019 & 3.231 $\pm$ 0.422     \\
& KNN & 0.939 $\pm$ 0.005 & 0.936 $\pm$ 0.005 & 0.939 $\pm$ 0.005 & 0.934 $\pm$ 0.006 & 0.666 $\pm$ 0.029 & 0.785 $\pm$ 0.013 & 0.316 $\pm$ 0.024     \\
& RF & 0.97 $\pm$ 0.003 & 0.968 $\pm$ 0.003 & 0.97 $\pm$ 0.003 & 0.966 $\pm$ 0.004 & 0.675 $\pm$ 0.038 & 0.801 $\pm$ 0.017 & 1.279 $\pm$ 0.056        \\
& LR & 0.98 $\pm$ 0.001 & 0.979 $\pm$ 0.002 & 0.98 $\pm$ 0.001 & 0.979 $\pm$ 0.001 & 0.836 $\pm$ 0.045 & 0.903 $\pm$ 0.028 & 1.006 $\pm$ 0.072        \\
& DT & 0.95 $\pm$ 0.002 & 0.949 $\pm$ 0.002 & 0.95 $\pm$ 0.002 & 0.949 $\pm$ 0.002 & 0.661 $\pm$ 0.036 & 0.822 $\pm$ 0.033 & 0.315 $\pm$ 0.068        \\

         \cmidrule{2-9}
        \multirow{7}{2.3cm}{Reads2Vec}
      & SVM  &  \textbf{0.998} $\pm$ 0.002 & \textbf{0.998} $\pm$ 0.002 & \textbf{0.998} $\pm$ 0.002 & \textbf{0.998} $\pm$ 0.002 & \textbf{0.988} $\pm$ 0.030 & \textbf{0.994} $\pm$ 0.017 & 1.097 $\pm$ 0.026	\\
& NB  &  0.125 $\pm$ 0.094 & 0.727 $\pm$ 0.077 & 0.125 $\pm$ 0.094 & 0.195 $\pm$ 0.106 & 0.089 $\pm$ 0.048 & 0.553 $\pm$ 0.027 & 0.184 $\pm$ 0.004     \\
& MLP  &  0.987 $\pm$ 0.004 & 0.988 $\pm$ 0.004 & 0.987 $\pm$ 0.004 & 0.987 $\pm$ 0.004 & 0.813 $\pm$ 0.046 & 0.913 $\pm$ 0.023 & 2.921 $\pm$ 0.407    \\
& KNN  &  0.944 $\pm$ 0.006 & 0.946 $\pm$ 0.005 & 0.944 $\pm$ 0.006 & 0.941 $\pm$ 0.006 & 0.777 $\pm$ 0.034 & 0.835 $\pm$ 0.022 & 0.384 $\pm$ 0.047    \\
& RF  &  0.987 $\pm$ 0.005 & 0.987 $\pm$ 0.006 & 0.987 $\pm$ 0.005 & 0.986 $\pm$ 0.007 & 0.85 $\pm$ 0.080 & 0.89 $\pm$ 0.038 & 4.157 $\pm$ 0.284       \\
& LR  &  0.998 $\pm$ 0.002 & 0.998 $\pm$ 0.002 & 0.998 $\pm$ 0.002 & 0.998 $\pm$ 0.003 & 0.973 $\pm$ 0.042 & 0.98 $\pm$ 0.011 & 3.761 $\pm$ 0.139      \\
& DT  &  0.973 $\pm$ 0.004 & 0.973 $\pm$ 0.005 & 0.973 $\pm$ 0.004 & 0.973 $\pm$ 0.004 & 0.734 $\pm$ 0.037 & 0.869 $\pm$ 0.018 & 2.021 $\pm$ 0.063     \\
        
        \bottomrule
    \end{tabular}
    }
    \caption{Average $\pm$ standard deviation classification results
      \textbf{without SMOTE} for real dataset. Best average values are
      shown in bold.  }
    \label{tbl_classification_without_smote}
\end{table}

% \begin{table}[h!]
%     \centering
%     \resizebox{1\textwidth}{!}{
%     \begin{tabular}{p{2.1cm}p{0.9cm}p{2.4cm}p{2.4cm}p{2.4cm}p{2.4cm}p{2.4cm}p{2.4cm}|p{2.4cm}}
%     \toprule
%         Method & ML Algo. & Acc. & Prec. & Recall & F1 (Weig.) & F1 (Macro) & ROC AUC & Train Time (Sec.) \\
%         \midrule \midrule
        
%         % \multirow{21}{1.2cm}{Before SMOTE} & 
%         \multirow{7}{1.2cm}{Spike2Vec}
%         & SVM & 0.574 $\pm$ 0.009 & 0.448 $\pm$ 0.052 & 0.574 $\pm$ 0.009 & 0.428 $\pm$ 0.011 & 0.083 $\pm$ 0.001 & 0.500 $\pm$ 0.001 & 8.989 $\pm$ 2.701   \\
%         & NB & 0.101 $\pm$ 0.045 & 0.476 $\pm$ 0.050 & 0.101 $\pm$ 0.045 & 0.107 $\pm$ 0.043 & 0.056 $\pm$ 0.012 & \textbf{\large 0.525} $\pm$ 0.027 & 0.026 $\pm$ 0.011    \\
%         & MLP & 0.522 $\pm$ 0.013 & 0.459 $\pm$ 0.011 & 0.522 $\pm$ 0.013 & 0.486 $\pm$ 0.011 & 0.111 $\pm$ 0.003 & 0.506 $\pm$ 0.003 & 9.943 $\pm$ 1.347   \\
%         & KNN & 0.524 $\pm$ 0.003 & 0.468 $\pm$ 0.005 & 0.524 $\pm$ 0.003 & 0.494 $\pm$ 0.004 & 0.114 $\pm$ 0.001 & 0.509 $\pm$ 0.001 & 0.428 $\pm$ 0.023   \\
%         & RF & 0.567 $\pm$ 0.010 & 0.489 $\pm$ 0.021 & 0.567 $\pm$ 0.010 & 0.503 $\pm$ 0.011 & 0.112 $\pm$ 0.002 & 0.509 $\pm$ 0.001 & 1.428 $\pm$ 0.060    \\
%         & LR & 0.568 $\pm$ 0.007 & 0.474 $\pm$ 0.009 & 0.568 $\pm$ 0.007 & 0.490 $\pm$ 0.010 & 0.107 $\pm$ 0.002 & 0.506 $\pm$ 0.001 & 2.704 $\pm$ 0.219    \\
%         & DT & 0.458 $\pm$ 0.015 & 0.464 $\pm$ 0.017 & 0.458 $\pm$ 0.015 & 0.461 $\pm$ 0.016 & 0.121 $\pm$ 0.009 & 0.508 $\pm$ 0.005 & 0.219 $\pm$ 0.046	\\

%          \cmidrule{2-9}
%         \multirow{7}{1.2cm}{Minimizers2Vec}
%         & SVM & 0.574 $\pm$ 0.012 & 0.382 $\pm$ 0.045 & 0.574 $\pm$ 0.012 & 0.421 $\pm$ 0.015 & 0.081 $\pm$ 0.001 & 0.500 $\pm$ 0.000 & 5.661 $\pm$ 0.087   \\
%         & NB & 0.077 $\pm$ 0.048 & \textbf{\large 0.525} $\pm$ 0.052 & 0.077 $\pm$ 0.048 & 0.106 $\pm$ 0.047 & 0.046 $\pm$ 0.018 & 0.505 $\pm$ 0.013 & \textbf{\large 0.021} $\pm$ 0.001    \\
%         & MLP & 0.537 $\pm$ 0.011 & 0.463 $\pm$ 0.013 & 0.537 $\pm$ 0.011 & 0.488 $\pm$ 0.013 & 0.112 $\pm$ 0.003 & 0.506 $\pm$ 0.003 & 10.485 $\pm$ 1.52  \\
%         & KNN & 0.521 $\pm$ 0.005 & 0.467 $\pm$ 0.010 & 0.521 $\pm$ 0.005 & 0.491 $\pm$ 0.005 & 0.116 $\pm$ 0.006 & 0.508 $\pm$ 0.003 & 0.428 $\pm$ 0.015   \\
%         & RF & \textbf{\large 0.577} $\pm$ 0.011 & 0.491 $\pm$ 0.013 & \textbf{\large 0.577} $\pm$ 0.011 & 0.508 $\pm$ 0.017 & 0.113 $\pm$ 0.003 & 0.511 $\pm$ 0.002 & 1.159 $\pm$ 0.026    \\
%         & LR & 0.574 $\pm$ 0.012 & 0.476 $\pm$ 0.011 & 0.574 $\pm$ 0.012 & 0.467 $\pm$ 0.019 & 0.098 $\pm$ 0.004 & 0.504 $\pm$ 0.001 & 1.766 $\pm$ 0.069    \\
%         & DT & 0.465 $\pm$ 0.006 & 0.468 $\pm$ 0.009 & 0.465 $\pm$ 0.006 & 0.466 $\pm$ 0.008 & 0.117 $\pm$ 0.004 & 0.506 $\pm$ 0.002 & 0.154 $\pm$ 0.012	\\

%          \cmidrule{2-9}
%         \multirow{7}{2.3cm}{Reads2Vec}
%         & SVM & 0.504 $\pm$ 0.006 & 0.489 $\pm$ 0.007 & 0.504 $\pm$ 0.006 & 0.496 $\pm$ 0.006 & \textbf{\large 0.134} $\pm$ 0.008 & 0.518 $\pm$ 0.005 & 127.415 $\pm$ 37.9	\\
%         & NB & 0.043 $\pm$ 0.020 & 0.496 $\pm$ 0.085 & 0.043 $\pm$ 0.020 & 0.045 $\pm$ 0.032 & 0.031 $\pm$ 0.009 & 0.499 $\pm$ 0.010 & 0.115 $\pm$ 0.007      \\
%         & MLP & 0.505 $\pm$ 0.017 & 0.492 $\pm$ 0.007 & 0.505 $\pm$ 0.017 & 0.498 $\pm$ 0.012 & 0.126 $\pm$ 0.005 & 0.514 $\pm$ 0.002 & 13.969 $\pm$ 2.095    \\
%         & KNN & 0.532 $\pm$ 0.012 & 0.477 $\pm$ 0.009 & 0.532 $\pm$ 0.012 & 0.502 $\pm$ 0.011 & 0.118 $\pm$ 0.005 & 0.511 $\pm$ 0.002 & 0.507 $\pm$ 0.027     \\
%         & RF & 0.576 $\pm$ 0.009 & 0.487 $\pm$ 0.010 & 0.576 $\pm$ 0.009 & 0.499 $\pm$ 0.010 & 0.109 $\pm$ 0.002 & 0.509 $\pm$ 0.002 & 1.873 $\pm$ 0.059      \\
%         & LR & 0.544 $\pm$ 0.007 & 0.494 $\pm$ 0.006 & 0.544 $\pm$ 0.007 & \textbf{\large 0.513} $\pm$ 0.006 & 0.129 $\pm$ 0.006 & 0.515 $\pm$ 0.002 & 13.020 $\pm$ 3.97     \\
%         & DT & 0.473 $\pm$ 0.011 & 0.477 $\pm$ 0.012 & 0.473 $\pm$ 0.011 & 0.475 $\pm$ 0.011 & 0.123 $\pm$ 0.008 & 0.510 $\pm$ 0.005 & 0.758 $\pm$ 0.359      \\
    
        
%         \bottomrule
%     \end{tabular}
%     }
%     \caption{Average $\pm$ standard deviation classification results
%       \textbf{before SMOTE} for Short Read dataset. Best average
%       values are shown in bold.  }
%     \label{tbl_classification_gisaid1}
% \end{table}


\begin{table}[h!]
    \centering
    \resizebox{1\textwidth}{!}{
    \begin{tabular}{p{2.1cm}p{0.9cm}p{2.4cm}p{2.4cm}p{2.4cm}p{2.4cm}p{2.4cm}p{2.4cm}|p{2.4cm}}
    \toprule
        Method & ML Algo. & Acc. & Prec. & Recall & F1 (Weig.) & F1 (Macro) & ROC AUC & Train Time (Sec.) \\
        \midrule \midrule
        
        % \multirow{21}{1.2cm}{After SMOTE} 
        \multirow{7}{1.2cm}{Spike2Vec}
& SVM & 0.999 $\pm$ 0 & 0.999 $\pm$ 0 & 0.999 $\pm$ 0 & 0.999 $\pm$ 0 & 0.999 $\pm$ 0 & 0.999 $\pm$ 0 & 3.888 $\pm$ 0.11							\\
& NB & 0.301 $\pm$ 0.005 & 0.388 $\pm$ 0.011 & 0.301 $\pm$ 0.005 & 0.229 $\pm$ 0.004 & 0.229 $\pm$ 0.004 & 0.607 $\pm$ 0.003 & \textbf{\large 0.194} $\pm$ 0.005  \\
& MLP & 0.997 $\pm$ 0 & 0.997 $\pm$ 0 & 0.997 $\pm$ 0 & 0.997 $\pm$ 0 & 0.997 $\pm$ 0 & 0.998 $\pm$ 0 & 5.232 $\pm$ 1.039                         \\
& KNN & 0.972 $\pm$ 0.001 & 0.972 $\pm$ 0.001 & 0.972 $\pm$ 0.001 & 0.971 $\pm$ 0.001 & 0.971 $\pm$ 0.001 & 0.984 $\pm$ 0.001 & 6.851 $\pm$ 0.274 \\
& RF & 0.998 $\pm$ 0.001 & 0.998 $\pm$ 0.001 & 0.998 $\pm$ 0.001 & 0.998 $\pm$ 0.001 & 0.998 $\pm$ 0.001 & 0.999 $\pm$ 0 & 16.187 $\pm$ 0.335     \\
& LR & 0.997 $\pm$ 0 & 0.997 $\pm$ 0 & 0.997 $\pm$ 0 & 0.996 $\pm$ 0 & 0.996 $\pm$ 0 & 0.998 $\pm$ 0 & 9.999 $\pm$ 0.38                           \\
& DT & 0.977 $\pm$ 0.001 & 0.977 $\pm$ 0.001 & 0.977 $\pm$ 0.001 & 0.977 $\pm$ 0.002 & 0.976 $\pm$ 0.002 & 0.987 $\pm$ 0.001 & 2.833 $\pm$ 0.062  \\


         \cmidrule{2-9}
        \multirow{7}{1.2cm}{Minimizers2Vec}
        & SVM & 0.999 $\pm$ 0 & 0.999 $\pm$ 0 & 0.999 $\pm$ 0 & 0.999 $\pm$ 0 & 0.999 $\pm$ 0 & 0.999 $\pm$ 0 & 3.845 $\pm$ 0.445  							\\
& NB & 0.457 $\pm$ 0.01 & 0.545 $\pm$ 0.016 & 0.457 $\pm$ 0.01 & 0.401 $\pm$ 0.01 & 0.402 $\pm$ 0.009 & 0.695 $\pm$ 0.005 & 0.206 $\pm$ 0.017       \\
& MLP & 0.997 $\pm$ 0.001 & 0.997 $\pm$ 0.001 & 0.997 $\pm$ 0.001 & 0.997 $\pm$ 0.001 & 0.997 $\pm$ 0.001 & 0.998 $\pm$ 0.001 & 6.270 $\pm$ 0.877   \\
& KNN & 0.980 $\pm$ 0.001 & 0.980 $\pm$ 0.001 & 0.980 $\pm$ 0.001 & 0.980 $\pm$ 0.001 & 0.980 $\pm$ 0.001 & 0.989 $\pm$ 0 & 7.188 $\pm$ 0.483       \\
& RF & 0.999 $\pm$ 0 & 0.999 $\pm$ 0 & 0.999 $\pm$ 0 & 0.999 $\pm$ 0 & 0.999 $\pm$ 0 & 0.999 $\pm$ 0 & 8.407 $\pm$ 0.429                            \\
& LR & 0.991 $\pm$ 0 & 0.991 $\pm$ 0 & 0.991 $\pm$ 0 & 0.991 $\pm$ 0 & 0.991 $\pm$ 0 & 0.995 $\pm$ 0 & 8.817 $\pm$ 0.514                            \\
& DT & 0.985 $\pm$ 0.002 & 0.985 $\pm$ 0.002 & 0.985 $\pm$ 0.002 & 0.985 $\pm$ 0.002 & 0.985 $\pm$ 0.002 & 0.991 $\pm$ 0.001 & 1.620 $\pm$ 0.228    \\

         \cmidrule{2-9}
        \multirow{7}{2.3cm}{Reads2Vec}
& SVM & 1 $\pm$ 0 & 1 $\pm$ 0 & 1 $\pm$ 0 & 1 $\pm$ 0 & 1 $\pm$ 0 & 1 $\pm$ 0 & 10.539 $\pm$ 0.314												\\
& NB & 0.503 $\pm$ 0.004 & 0.618 $\pm$ 0.006 & 0.503 $\pm$ 0.004 & 0.46 $\pm$ 0.003 & 0.459 $\pm$ 0.002 & 0.72 $\pm$ 0.002 & 1.006 $\pm$ 0.007    \\
& MLP & 0.999 $\pm$ 0 & 0.999 $\pm$ 0 & 0.999 $\pm$ 0 & 0.999 $\pm$ 0 & 0.999 $\pm$ 0 & 1 $\pm$ 0 & 3.971 $\pm$ 0.484                             \\
& KNN & 0.985 $\pm$ 0.001 & 0.985 $\pm$ 0.001 & 0.985 $\pm$ 0.001 & 0.985 $\pm$ 0.001 & 0.985 $\pm$ 0.001 & 0.991 $\pm$ 0 & 8.539 $\pm$ 0.195     \\
& RF & 1 $\pm$ 0 & 1 $\pm$ 0 & 1 $\pm$ 0 & 1 $\pm$ 0 & 1 $\pm$ 0 & 1 $\pm$ 0 & 25.257 $\pm$ 0.109                                                 \\
& LR & \textbf{\large 1} $\pm$ 0 & \textbf{\large 1} $\pm$ 0 & \textbf{\large 1} $\pm$ 0 & \textbf{\large 1} $\pm$ 0 & \textbf{\large 1} $\pm$ 0 & \textbf{\large 1} $\pm$ 0 & 20.27 $\pm$ 0.438                                                  \\
& DT & 0.995 $\pm$ 0.001 & 0.995 $\pm$ 0.001 & 0.995 $\pm$ 0.001 & 0.995 $\pm$ 0.001 & 0.995 $\pm$ 0.001 & 0.997 $\pm$ 0 & 8.411 $\pm$ 0.35       \\


        
        \bottomrule
    \end{tabular}
    }
    \caption{Average $\pm$ standard deviation classification results
      \textbf{with SMOTE} for the real dataset. Best average values
      are shown in bold (considering 6 decimal places).  }
    \label{tbl_classification_with_smote}
\end{table}

\subsubsection{Clustering Results}
To perform clustering, we use two settings. In the first setting, we
select the value for $k$, \ie, the number of clusters, using the elbow
method. In the second setting, we select the value for $k$ equal to
the total number of unique class labels given by the ``ground truth''
clustering corresponding to the labels assigned by the Pangolin tool.

\paragraph{Elbow Method Based Results.}
The elbow method used to select the optimal number of clusters is
given in Figure~\ref{fig_elbow} for all embedding methods. The
selected value of $k$ using the elbow method is 5, 5, and 4 for
Spike2Vec, Minimizers2Vec, and Reads2Vec, respectively.
\begin{figure}[h!]
\centering
\begin{subfigure}{.33\textwidth}
  \centering
  \includegraphics[scale=0.22] {Figures/elbow/kmer_kelbow_short_read_8140.pdf}
   \caption{Spike2Vec}
  \label{fig_elbow_kmer}
\end{subfigure}%
\begin{subfigure}{.33\textwidth}
  \centering
  \includegraphics[scale=0.22] {Figures/elbow/minimizer_kelbow_short_read_8140.pdf}
  \caption{Minimizers2Vec}
  \label{fig_elbow_mini}
\end{subfigure}%
\begin{subfigure}{.33\textwidth}
  \centering
  \includegraphics[scale=0.22] {Figures/elbow/spaced_kmer_kelbow_short_read_8140.pdf}
  \caption{Reads2Vec}
  \label{fig_elbow_reads}
\end{subfigure}
\caption{Elbow method to determine the number of clusters in the
  simulated dataset. The selected value of $k$ is 5, 5, and 4 for
  Spike2Vec, Minimizers2Vec, and Reads2Vec, respectively. This figure
  is best seen in color.  }
\label{fig_elbow}
\end{figure}

The clustering results for $k$ selected using elbow method are shown
in Table~\ref{tbl_clustering_results_sim_k_5}. We can observe that in
this experimental setting, the Spike2Vec embedding method outperforms
the other two methods for all but one evaluation metric. For the
Davies-Bouldin Score, the proposed Reads2Vec performs the best. In all
cases, the $k$-means algorithm performs better than the $k$-modes
clustering algorithm.

\begin{table}[h!]
  \centering
  \resizebox{0.99\textwidth}{!}{
  \begin{tabular}{p{1.7cm}p{2.5cm}p{1.7cm}p{2.2cm}p{2.2cm}p{1.7cm}}
    \toprule
     & & \multicolumn{3}{c}{Evaluation Metrics} \\
    \cline{3-5}
    Algorithm & Embedding & Silhouette Coefficient & Calinski-Harabasz Score & Davies-Bouldin Score & Clustering Runtime (in sec.) \\
    \midrule \midrule	
    % \multirow{3}{*}{Pangolin~\cite{pango_tool_ref}} & 
    % Spike2Vec & 0.029 & 818.673 & 8.471 &   \\
    % & Minimizers2Vec & -0.233 & 144.200 & 6.063 &   \\
    % & Reads2Vec & -0.214 & 78.098 & 7.864 & $\approx 14$ hours \\
    % \hline
    \multirow{3}{*}{$k$-means} 
    & Spike2Vec  &  \textbf{0.780} & \textbf{61422.728} & 0.472 & \textbf{2.980435}  \\
    & Minimizers2Vec  &  0.730 & 48513.248 &  0.526  & 4.705328  \\
     & Reads2Vec  &  0.774 & 48644.237 &  \textbf{0.452} & 12.59896  \\
    \midrule	
    \multirow{3}{*}{$k$-mode} 
    & Spike2Vec &  -0.468 & 20.500 & 7.430  &  82.898802 \\
    & Minimizers2Vec &  0.345 & 24.311 &  24.927 & 29.893825 \\
    & Reads2Vec & 0.425 & 912.212 &  29.601  & 542.186639 \\
    \bottomrule	
  \end{tabular}
  }
  \caption{Internal clustering quality metrics on the simulated data
    for $k$-means and $k$-modes using Spike2Vec ($k=5$),
    Minimizers2Vec ($k=5$), and Reads2Vec ($k=4$) embeddings. Best
    values are shown in bold.  }
  \label{tbl_clustering_results_sim_k_5}
\end{table}

The comparison of embeddings with each other using the $k$-means
clustering method are shown in Figure~\ref{fig_embedding_com_elbow}
using different metrics including ARI, FMI, CD, and VM. We can observe
that overall Spike2Vec and Minimizers2Vec are most similar to each
other for all comparison metrics. Moreover, Reads2Vec is more similar
to Spike2Vec than Minimizers2Vec.

\begin{figure}[h!]
\centering
\begin{subfigure}{.50\textwidth}
  \centering
  \includegraphics[scale = 0.50] {Figures/clustering_comparison_metric/4Clusters/Adjusted_Rand_Index.png}
  \caption{ARI}
\end{subfigure}%
\begin{subfigure}{.50\textwidth}
  \centering
  \includegraphics[scale = 0.50] {Figures/clustering_comparison_metric/4Clusters/Fowlkes_Mallows_Score.png}
  \caption{FMI}
\end{subfigure}% 
\\
\begin{subfigure}{.50\textwidth}
  \centering
  \includegraphics[scale = 0.50] {Figures/clustering_comparison_metric/4Clusters/Completeness_Score.png}
  \caption{CS}
\end{subfigure}%
\begin{subfigure}{.50\textwidth}
  \centering
  \includegraphics[scale = 0.50] {Figures/clustering_comparison_metric/4Clusters/V_Measure_Index.png}
  \caption{VM}
\end{subfigure}%
\caption{Comparison of $k$-means clustering approaches and embedding
  methods on the simulated data using standard clustering comparison
  metrics for $k$ clusters ($k$ selected using elbow method).  }
\label{fig_embedding_com_elbow}
\end{figure}


\paragraph{Ground Truth Clustering Based Results.}
The clustering results for $k$ selected based on the number of
clusters in the labeling produced by the Pangolin tool are shown in
Table~\ref{tbl_clustering_results_pangolin_k_value}.  We can observe
that in this experimental setting, the Spike2Vec embedding method
outperforms the other two methods in all evaluation metrics. In all
cases, the $k$-means clustering algorithm performs better than the
$k$-modes clustering algorithm.

\begin{table}[h!]
  \centering
  \resizebox{0.99\textwidth}{!}{
  \begin{tabular}{p{1.7cm}p{2.5cm}p{1.7cm}p{2.2cm}p{2.2cm}p{1.7cm}}
    \toprule
     & & \multicolumn{3}{c}{Evaluation Metrics} \\
    \cline{3-5}
    Algorithm & Embedding & Silhouette Coefficient & Calinski-Harabasz Score & Davies-Bouldin Score & Clustering Runtime (in sec.) \\
    \midrule \midrule	
    \multirow{1}{*}{ground} 
    %  & OHE & 0.357 & 2828.170 & 1.162 & 95.46847 \\
    & OHE & 0.315 & 463.886 & 1.576 & $\approx 18$ hours \\
     \midrule
    \multirow{3}{*}{$k$-means} 
    & Spike2Vec & \textbf{0.744} & \textbf{92556.901} & \textbf{0.532} & \textbf{4.326378} \\
    & Minimizers2Vec &  0.635 & 48097.935 &  0.724 & 5.693730 \\
     & Reads2Vec & 0.736 & 91182.722 & 0.535  & 6.255037 \\
    \midrule	
    \multirow{3}{*}{$k$-mode} 
    & Spike2Vec & -0.405 & 17.027 &  44.572  & 175.485736 \\
    & Minimizers2Vec &  0.304 & 17.693 &  20.084  & 49.965804  \\
    & Reads2Vec & 0.354 & 512.340 &  134.516  & 1447.708700  \\
    \bottomrule	
  \end{tabular}
  }
  \caption{Internal clustering quality metrics on the simulated data
    for $k$-means and $k$-modes using Spike2Vec, Minimizers2Vec, and
    Reads2Vec embeddings (in call cases, $k =9$, the number of
    clusters in the labeling by the Pango tool).  }
  \label{tbl_clustering_results_pangolin_k_value}
\end{table}

% \begin{table*}[h!]
%     \centering
%     \resizebox{0.99\textwidth}{!}{
%     \begin{tabular}{p{1.2cm}cp{1.9cm}p{1.7cm}p{1.9cm}p{1.7cm}p{1.7cm}p{1.9cm}|p{1.9cm}}
%     \toprule
%         Method & ML Algorithm & Acc. & Prec. & Recall & F1 (Weig.) & F1 (Macro) & ROC AUC & Train Time (Sec.) \\
%         \midrule \midrule
        
%         \multirow{7}{1.2cm}{Spike2Vec}
        
%         SVM & 0.771 $\pm$ 0.869 & 0.759 $\pm$ 0.869 & 0.771 $\pm$ 0.869 & 0.761 $\pm$ 0.869 & 0.760 $\pm$ 0.869 & 0.871 $\pm$ 0.926 & 124.827 $\pm$ 420.200	\\
%         & NB & 0.314 $\pm$ 0.444 & 0.424 $\pm$ 0.442 & 0.314 $\pm$ 0.444 & 0.270 $\pm$ 0.399 & 0.270 $\pm$ 0.399 & 0.614 $\pm$ 0.687 & 0.121 $\pm$ 1.137        \\
%         & MLP & 0.780 $\pm$ 0.862 & 0.764 $\pm$ 0.860 & 0.780 $\pm$ 0.862 & 0.760 $\pm$ 0.860 & 0.760 $\pm$ 0.860 & 0.876 $\pm$ 0.922 & 43.608 $\pm$ 66.925     \\
%         & KNN & 0.877 $\pm$ 0.866 & 0.864 $\pm$ 0.854 & 0.877 $\pm$ 0.866 & 0.856 $\pm$ 0.832 & 0.855 $\pm$ 0.832 & 0.930 $\pm$ 0.924 & 7.537 $\pm$ 9.119       \\
%         & RF & 0.929 $\pm$ 0.932 & 0.927 $\pm$ 0.932 & 0.929 $\pm$ 0.932 & 0.928 $\pm$ 0.932 & 0.927 $\pm$ 0.932 & 0.960 $\pm$ 0.962 & 5.160 $\pm$ 8.491        \\
%         & LR & 0.682 $\pm$ 0.848 & 0.657 $\pm$ 0.840 & 0.682 $\pm$ 0.848 & 0.664 $\pm$ 0.843 & 0.664 $\pm$ 0.843 & 0.821 $\pm$ 0.915 & 57.708 $\pm$ 86.244      \\
%         & DT & 0.839 $\pm$ 0.849 & 0.828 $\pm$ 0.838 & 0.839 $\pm$ 0.849 & 0.832 $\pm$ 0.842 & 0.832 $\pm$ 0.842 & 0.909 $\pm$ 0.915 & 0.705 $\pm$ 2.615        \\

%          \midrule
%         \multirow{7}{1.2cm}{Minimizer}
%         SVM & 0.684 $\pm$ 0.007 & 0.665 $\pm$ 0.007 & 0.684 $\pm$ 0.007 & 0.668 $\pm$ 0.006 & 0.668 $\pm$ 0.004 & 0.823 $\pm$ 0.003 & 141.194 $\pm$ 13.260	\\
%         & NB & 0.304 $\pm$ 0.004 & 0.371 $\pm$ 0.009 & 0.304 $\pm$ 0.004 & 0.228 $\pm$ 0.004 & 0.229 $\pm$ 0.004 & 0.609 $\pm$ 0.003 & 0.132 $\pm$ 0.015        \\
%         & MLP & 0.717 $\pm$ 0.013 & 0.695 $\pm$ 0.015 & 0.717 $\pm$ 0.013 & 0.697 $\pm$ 0.017 & 0.697 $\pm$ 0.015 & 0.841 $\pm$ 0.006 & 37.607 $\pm$ 7.828      \\
%         & KNN & 0.873 $\pm$ 0.002 & 0.859 $\pm$ 0.002 & 0.873 $\pm$ 0.002 & 0.852 $\pm$ 0.002 & 0.851 $\pm$ 0.002 & 0.928 $\pm$ 0.001 & 8.720 $\pm$ 1.754       \\
%         & RF & 0.922 $\pm$ 0.002 & 0.919 $\pm$ 0.002 & 0.922 $\pm$ 0.002 & 0.921 $\pm$ 0.002 & 0.920 $\pm$ 0.002 & 0.956 $\pm$ 0.001 & 5.669 $\pm$ 2.810        \\
%         & LR & 0.588 $\pm$ 0.005 & 0.553 $\pm$ 0.006 & 0.588 $\pm$ 0.005 & 0.559 $\pm$ 0.006 & 0.559 $\pm$ 0.005 & 0.768 $\pm$ 0.003 & 45.842 $\pm$ 14.034      \\
%         & DT & 0.833 $\pm$ 0.003 & 0.822 $\pm$ 0.004 & 0.833 $\pm$ 0.003 & 0.826 $\pm$ 0.003 & 0.826 $\pm$ 0.003 & 0.906 $\pm$ 0.002 & 0.544 $\pm$ 0.066        \\

%          \midrule
%         \multirow{7}{1.2cm}{Spaced kmer}
%         SVM & 0.869 $\pm$ 0.003 & 0.869 $\pm$ 0.002 & 0.869 $\pm$ 0.003 & 0.869 $\pm$ 0.002 & 0.869 $\pm$ 0.003 & 0.926 $\pm$ 0.001 & 420.200 $\pm$ 8.581	\\
%         & NB & 0.444 $\pm$ 0.001 & 0.442 $\pm$ 0.006 & 0.444 $\pm$ 0.001 & 0.399 $\pm$ 0.001 & 0.399 $\pm$ 0.001 & 0.687 $\pm$ 0.001 & 1.137 $\pm$ 0.021    \\
%         & MLP & 0.862 $\pm$ 0.003 & 0.860 $\pm$ 0.004 & 0.862 $\pm$ 0.003 & 0.860 $\pm$ 0.004 & 0.860 $\pm$ 0.004 & 0.922 $\pm$ 0.001 & 66.925 $\pm$ 17.670 \\
%         & KNN & 0.866 $\pm$ 0.002 & 0.854 $\pm$ 0.002 & 0.866 $\pm$ 0.002 & 0.832 $\pm$ 0.004 & 0.832 $\pm$ 0.002 & 0.924 $\pm$ 0.000 & 9.119 $\pm$ 0.566   \\
%         & RF & 0.932 $\pm$ 0.002 & 0.932 $\pm$ 0.002 & 0.932 $\pm$ 0.002 & 0.932 $\pm$ 0.002 & 0.932 $\pm$ 0.002 & 0.962 $\pm$ 0.001 & 8.491 $\pm$ 0.259    \\
%         & LR & 0.848 $\pm$ 0.001 & 0.840 $\pm$ 0.002 & 0.848 $\pm$ 0.001 & 0.843 $\pm$ 0.002 & 0.843 $\pm$ 0.001 & 0.915 $\pm$ 0.001 & 86.244 $\pm$ 5.413   \\
%         & DT & 0.849 $\pm$ 0.004 & 0.838 $\pm$ 0.003 & 0.849 $\pm$ 0.004 & 0.842 $\pm$ 0.003 & 0.842 $\pm$ 0.003 & 0.915 $\pm$ 0.002 & 2.615 $\pm$ 0.024    \\

%         \bottomrule
%     \end{tabular}
%     }
%     \caption{Average $\pm$ standard deviation classification results for  Short Read dataset with SMOTE. Best average values are shown in bold.}
%     \label{tbl_classification_gisaid1}
% \end{table*}

The comparison of different embedding methods with $k=9$ is shown in
Figure~\ref{fig_embedding_com_non_elbow}. Here, we also show the
``ground truth embedding'', which is the one-hot encoding
representation of the sequences. We can observe that Spike2Vec and
Reads2Vec are most similar to each other in this case while
Minimizers2Vec is also closely related to both Spike2Vec and
Reads2Vec. However, the ground truth embedding appears to be very
different from the other embedding methods.
\begin{figure}[h!]
\centering
\begin{subfigure}{.50\textwidth}
  \centering
  \includegraphics[scale = 0.45] {Figures/clustering_comparison_metric/New_Results_With_9_Clusters/Adjusted_Rand_Index.png}
  \caption{ARI}
\end{subfigure}%
\begin{subfigure}{.50\textwidth}
  \centering
  \includegraphics[scale = 0.45] {Figures/clustering_comparison_metric/New_Results_With_9_Clusters/Fowlkes_Mallows_Score.png}
  \caption{FMI}
\end{subfigure}% 
\\
\begin{subfigure}{.50\textwidth}
  \centering
  \includegraphics[scale = 0.450] {Figures/clustering_comparison_metric/New_Results_With_9_Clusters/Completeness_Score.png}
  \caption{CS}
\end{subfigure}%
\begin{subfigure}{.50\textwidth}
  \centering
  \includegraphics[scale = 0.450] {Figures/clustering_comparison_metric/New_Results_With_9_Clusters/V_Measure_Index.png}
  \caption{VM}
\end{subfigure}%
\caption{Comparison of $k$means clustering approaches and embedding
  methods on the simulated dataset using standard clustering
  comparison metrics for $k=9$ clusters.  }
\label{fig_embedding_com_non_elbow}
\end{figure}

% \begin{figure}[h!]
% \centering
% \begin{subfigure}{.25\textwidth}
%   \centering
%   \includegraphics[scale = 0.26] {Figures/clustering_comparison_metric/SMOTE/Adjusted_Rand_Index.png}
%   \caption{ARI}
% \end{subfigure}%
% \begin{subfigure}{.25\textwidth}
%   \centering
%   \includegraphics[scale = 0.26] {Figures/clustering_comparison_metric/SMOTE/Fowlkes_Mallows_Score.png}
%   \caption{FMI}
% \end{subfigure}% 
% \begin{subfigure}{.25\textwidth}
%   \centering
%   \includegraphics[scale = 0.26] {Figures/clustering_comparison_metric/SMOTE/Completeness_Score.png}
%   \caption{CS}
% \end{subfigure}%
% \begin{subfigure}{.25\textwidth}
%   \centering
%   \includegraphics[scale = 0.26] {Figures/clustering_comparison_metric/SMOTE/V_Measure_Index.png}
%   \caption{VM}
% \end{subfigure}%

% \caption{Comparison of $k$means clustering approaches and embedding
%   methods using standard clustering comparison metrics on SMOTE.
%  \color{red}{Prakash: Why we didn't do it for kModes earlier? May be coz we wanted only best from table}
%  \murray{I guess we don't need this figure either (clustering with SMOTE)}
% }
% \label{fig_evaluation_metrics_smote}
% \end{figure}

% \begin{figure}[h!]
%     \centering
%     \includegraphics{Figures/Tikz_Figures/attribute_correlation.tikz}
%     \caption{\murray{please make this into a pdf so that we aren't
%         compiling it every time} Fraction of features negatively
%       correlated with label in the case of Pearson (left) and Spearman
%       (right) correlation.  \murray{is that right?}}
%     \label{negative_spearman_pearson}
% \end{figure}

% \begin{figure}[h!]
%     \centering
%     \includegraphics{Figures/Tikz_Figures/attribute_correlation_positive.tikz}
%     \caption{\murray{please make this into a pdf..} Positive
%       correlation pattern for pearson and spearman correlation.}
%     \label{positive_spearman_pearson}
% \end{figure}

% \begin{figure}[h!]
% \centering
% \begin{subfigure}{.50\textwidth}
%   \centering
%   \includegraphics[scale = 0.8] {Figures/Tikz_Figures/spearman_pearson_simulated_9_clusters/Pearson_Negative.pdf}
%   \caption{Pearson}
% \end{subfigure}%
% \begin{subfigure}{.50\textwidth}
%   \centering
%   \includegraphics[scale = 0.8] {Figures/Tikz_Figures/spearman_pearson_simulated_9_clusters/Spearman_Negative.pdf}
%   \caption{Spearman}
% \end{subfigure}%
% \caption{Fraction of features negatively correlated with the labeling
%   in the case of Pearson and Spearman.}
% \label{negative_spearman_pearson}
% \end{figure}
% \begin{figure}[h!]
% \centering
% \begin{subfigure}{.50\textwidth}
%   \centering
%   \includegraphics[scale = 0.8] {Figures/Tikz_Figures/spearman_pearson_simulated_9_clusters/Pearson_Positive.pdf}
%   \caption{Pearson}
% \end{subfigure}%
% \begin{subfigure}{.50\textwidth}
%   \centering
%   \includegraphics[scale = 0.8] {Figures/Tikz_Figures/spearman_pearson_simulated_9_clusters/Spearman_Positive.pdf}
%   \caption{Spearman}
% \end{subfigure}%
% \caption{Fraction of features positively correlated with the labeling
%   in the case of Pearson and Spearman. This figure is best seen in color. }
% \label{positive_spearman_pearson}
% \end{figure}

% \begin{figure}[h!]
%     \centering
%     \includegraphics{Figures/Tikz_Figures/Spearman_Pearson_Combined.tikz}
%     \caption{Pattern for pearson and spearman correlation.}
%     \label{positive_spearman_pearson}
% \end{figure}

\subsubsection{Feature Importance}
The correlation of features with the class labels using Pearson and
Spearman correlation are shown in
Figure~\ref{fig_spearman_pearson_simulated} for the simulated data. In
case of Spearman correlation, we can observe that Reads2Vec contains
higher correlation with the class labels (lineages) compared to the
other embedding methods. However, for Pearson correlation, Spike2Vec
contains higher correlation of features with the class labels.
\begin{figure}[h!]
\centering
\begin{subfigure}{.50\textwidth}
  \centering
  \includegraphics[scale = 0.68] {Figures/Tikz_Figures/spearman_pearson_simulated_data/Spearman_Combined_1.pdf}
  \caption{Spearman}
\end{subfigure}%
\begin{subfigure}{.50\textwidth}
  \centering
  \includegraphics[scale = 0.68] {Figures/Tikz_Figures/spearman_pearson_simulated_data/Pearson_Combined_1.pdf}
  \caption{Pearson}
\end{subfigure}%
\caption{Fraction of features in the simulated data correlated with
  the labeling in the case of Pearson and Spearman.}
\label{fig_spearman_pearson_simulated}
\end{figure}


% \textcolor{blue}{Gianluca: the table is a bit too hard to read. Maybe we can move something to the supplementary material}

\subsection{Results on Real Data}
This section provides the results of the clustering methods using
different embedding methods and evaluation metrics performed on real
data.
%----------------------------------------------------------------------

\subsubsection{Clustering Evaluation}

Table~\ref{tbl_validation_metrics} shows the results for different
clustering algorithms and their comparison with various embeddings on
three internal clustering evaluation metrics. Since
Pangolin~\cite{pango_tool_ref} takes sequences as input rather than
numerical feature vectors, we cluster the sequences using the Pangolin
tool and then we evaluate the quality of the clustering labels using
the different numerical embeddings of these sequences.  The
performance of OHE with the Pangolin labels is better than to the
performance of $k$-mers or minimizers with Pangolin labels overall,
probably because OHE is a straightforward numerical representation of
a sequence (possibly similar to the ML representation that Pangolin
uses internally).  Moreover, we can observe that the $k$-mers based
feature embedding performs better with $k$-means clustering in all but
one evaluation metric. An important observation here is that the
clustering from $k$-means shows better performance as compared to the
Pangolin tool overall. This indicates that the Pangolin tool may not
be the best option in this raw high-throughput sequencing reads
setting.
%This means that using the Pangolin tool by default to assign labels
%to the sequences may not be a good option.
We observe that the $k$-modes clustering algorithm performs poorly in
most cases for clustering and runtime.

\begin{table}[h!]
  \centering
  \resizebox{0.99\textwidth}{!}{
  \begin{tabular}{p{2cm}p{2.5cm}p{1.7cm}p{2.2cm}p{2.2cm}p{1.7cm}}
    \hline
    & & \multicolumn{3}{c}{Evaluation Metrics} \\
    \cline{3-5}
    Algorithm & Embedding & Silhouette Coefficient & Calinski-Harabasz Score & Davies-Bouldin Score & Clustering Runtime \\
    \hline	\hline	
    \multirow{4}{2cm}{Pangolin} & 
    OHE & 0.029 & 818.673 & 8.471 &   \\
    
    & $k$-mers & -0.214 & 78.098 & 7.864 & $\approx 14$ hours \\
    & minimizers & -0.233 & 144.200 & 6.063 &   \\
    \hline
    \multirow{3}{*}{$k$-means} & 
    OHE & 0.623 & 3278.376 & 1.502 & 648.5 Sec. \\
    & $k$-mers & 0.775 & \textbf{21071.221} & \textbf{0.406} & \textbf{19.2 Sec.} \\
    & minimizers & \textbf{0.858} & 17909.284 & 0.421 & 31.3 Sec. \\
    \hline
    \multirow{3}{*}{$k$-modes} & 
    OHE & 0.613 & 2053.754 & 2.056 & $\approx 7$ days \\
    % & Pangolin~\cite{pango_tool_ref} & \_ & \_ & \_ & \_ \\
    & $k$-mers & -0.027 & 9.801 & 89.789 & $\approx 4$ hours \\
    & minimizers & -0.398 & 1196.777 & 3.545 & $\approx 1$ hour \\
    \hline
  \end{tabular}
  }
  \caption{Internal clustering quality metrics for Pangolin, $k$-means
    and $k$-modes on OHE, $k$-mers and minimizers embeddings on the
    real data. Best values are shown in bold.}
  \label{tbl_validation_metrics}
\end{table}

% \begin{table}[ht!]
%   \centering
%   \caption{Dasgupta's Score for Hierarchical Clustering algorithm.}
%   \begin{tabular}{p{2.5cm}p{1.7cm}}
%     \hline
%     Methods & Dasgupta Score \\
%     \hline	\hline	
%     OHE~\cite{kuzmin2020machine} & \_ \\
%     % Pangolin~\cite{pango_tool_ref} & \_ \\
%     Spike2Vec~\cite{ali2021spike2vec} & \_ \\
%     Minimizer & \_ \\

%     \hline
%   \end{tabular}
%   \label{}
% \end{table}

% \begin{table}[!ht]
%   \centering
%   \caption{Performance for embedding methods with different classifiers on COVID-19 data for short read data}
%   \begin{tabular}{p{1.9cm}p{1.1cm}cccccc | p{1.7cm}}
%     \hline
%     \multirow{2}{1.1cm}{Embed. Method} & \multirow{2}{0.7cm}{ML Algo.} & \multirow{2}{*}{Acc.} & \multirow{2}{*}{Prec.} & \multirow{2}{*}{Recall} & \multirow{2}{0.9cm}{F1 weigh.} & \multirow{2}{0.9cm}{F1 Macro} & \multirow{2}{1.2cm}{ROC- AUC} & Train. runtime (sec.) \\	
%     \hline	\hline	
%     \multirow{7}{2cm}{k-mers}
%         & NB & 0.20 & 0.43 & 0.20 & 0.18 & 0.15 & 0.57 & 5.22  \\
%         & MLP & 0.56 & 0.52 & 0.56 & 0.52 & 0.37 & 0.65 & 146.13 \\
%         & KNN & 0.51 & 0.51 & 0.51 & 0.50 & 0.40 & 0.66 & 23.64 \\
%         & RF & 0.62 & 0.62 & 0.62 & 0.61 & 0.50 & 0.71 & 24.39 \\
%         & LR & 0.67 & 0.68 & 0.67 & 0.64 & 0.51 & 0.72 & 13.38 \\
%         & DT & 0.57 & 0.58 & 0.57 & 0.58 & 0.51 & 0.72 & 5.02 \\

%     \hline
%   \end{tabular}
  
%   \label{tble_classification_results}
% \end{table}

\subsubsection{Comparing Different Clusterings}
\label{Comparison_to_baselines}

We compare the different clustering algorithms on different embeddings
using the adjusted Rand index (ARI), Fowlkes-Mallows index (FMI),
V-measure (VM) and completeness score (CS).  The heat map in
Figure~\ref{fig_evaluation_metrics_new} shows that some embeddings are
more similar to others.
% Note that in the case of Pangolin results, we can observe the score
% of $1$ for all of the $4$ metrics. This is because we use the same
% labeling of the Pangolin tool for different embedding methods,
% namely OHE, k-mers, and Minimizer (since Pango takes original
% multiply aligned sequences as input).
We observe that the $k$-mers + $k$-means combination is very similar
to the minimizers + $k$-means combination in terms of ARI, FMI, VM and
CS. This shows that the clusterings from the combinations of
clustering method and embedding are not much different from each
other.
\begin{figure}[h!]
  \centering
  \begin{subfigure}{.50\textwidth}
    \centering
    \includegraphics[scale = 0.45] {Figures/clustering_comparison_metric/real_data_5_clusters/Adjusted_Rand_Index_Updated.png}
    \caption{ARI}
  \end{subfigure}%
  \begin{subfigure}{.50\textwidth}
    \centering
    \includegraphics[scale = 0.45] {Figures/clustering_comparison_metric/real_data_5_clusters/Fowlkes_Mallows_Score_Updated.png}
    \caption{FMI}
  \end{subfigure}
  \\
  \begin{subfigure}{.50\textwidth}
    \centering
    \includegraphics[scale = 0.45] {Figures/clustering_comparison_metric/real_data_5_clusters/Completeness_Score_Updated.png}
    \caption{CS}
  \end{subfigure}%
  \begin{subfigure}{.50\textwidth}
    \centering
    \includegraphics[scale = 0.45] {Figures/clustering_comparison_metric/real_data_5_clusters/V_Measure_Index_Updated.png}
    \caption{VM}
  \end{subfigure}
  \caption{Comparison of different embedding methods on the real data
    for $k$-means clustering using standard clustering comparison
    metrics.}
\label{fig_evaluation_metrics_new}
\end{figure}

\subsubsection{Information Gain}
%From the results in Table~\ref{tbl_validation_metrics}, we note that
%the Pangolin tool is not state-of-the-art in terms of clustering the
%sequences.

The combination of clustering method and embedding which performed the
best overall, in terms of internal clustering quality, was $k$-mers +
$k$-means (see Table~\ref{tbl_validation_metrics}).  To verify if such
a clustering makes sense from a biological standpoint, thereby
independently validating such a clustering from an orthogonal
viewpoint, we also computed the importance of each genomic position in
the sequence to the labeling (the clustering) obtained by $k$-mers +
$k$-means.  For this purpose, we computed the \emph{Information Gain}
(IG) of this labeling in terms of genomic position,
%
%The overall best performing clustering approach (i.e., $k$-means +
%$k$-mers) and compute the labels for all $6812$ sequences in our
%data. We then use the multiply aligned sequences and the labels
%computed using $k$-means + $k$-mers to understand the importance of
%nucleotides at different positions in the sequences. For this purpose,
%we use a popular method called Information Gain (IG).
%
%\subsubsection{Information Gain (IG):} We used Information gain to
%analyze the data. This analysis helps to understand the data and i%ts
%important features.
%The Information Gain (IG) is 
defined as:

\begin{equation}
  IG(\mbox{class},\mbox{position}) = H(\mbox{class}) - H(\mbox{class}
  ~|~ \mbox{position})
\end{equation}
where
\begin{equation}
  H(\mbox{class}) = \sum_{e \in \mbox{class}} -p_e \log p_e
\end{equation}
is the entropy of a class in terms of the proportion of each unique
label $e$ of this class.
%where $p_i$ is the probability of the class $i$ and $C$ is the set of
% unique labels.  The count of nucleotide having different range of IG
% values are given in Figure~\ref{fig_ig}.
Figure~\ref{fig_ig} shows the IG values for different genomic
positions corresponding to the class labels.
% To get the class labels, we use labels from $k$-mean + $k$-mers
% clustering.
We can see that many positions have higher IG values, which means that
they play an important role in predicting the labels.
% Here we can conclude that many nucleotide contribute to the host
% specification as compared to that for SARS-CoV-2 variants
% specification~\cite{ali2021k}, which is expected, since the genomic
% variability within the family coronavidiridae should be much higher.

\begin{figure}[ht!]
  \centering
  % \begin{subfigure}{.33\textwidth}
  \centering
  \includegraphics[scale = 0.5]{Figures/IG_vals.png}
  \caption{Information gain values for different genomic
    positions. The x-axis shows the position and the y-axis shows the
    information gain value.}
  \label{fig_ig}
\end{figure}

% \begin{figure}[ht!]
%   \centering
%   % \begin{subfigure}{.33\textwidth}
%   \centering
%   \includegraphics{Figures/Tikz_Figures/InfoGain.tikz}
%   \caption{}
%   \label{fig_ig}
% \end{figure}

% \begin{figure}[h!]
%     \centering
%     \includegraphics{Figures/Tikz_Figures/Information_Gain_Plot.pdf}
%     \caption{Count of features having information gain between different ranges. The information gain is computed using 1446 sequences with each sequence have 29907 features.\murray{I like this, however I wonder if just a histogram of IG is more appropriate?}}
%     \label{fig_ig}
% \end{figure}

This IG scatter plot partitions the SARS-CoV-2 genome into three
distinct regions, with very low IG (0--11Kbp), with either very high
or very low IG (11Kbp--22Kbp), and with a wide range of IG
(22Kbp--30Kbp).  What is interesting is that the structural proteins
S, E, M and N~\cite{walls-2020-structure} fall in the $21$Kbp--$25$Kbp
range, overlapping, for the most part, this third region with the wide
range of IG.  This is consistent with the observation that mutations
of the SARS-CoV-2 genome (which define many of the different variants)
appear disproportionately in the structural proteins region,
particularly the spike (S) region~\cite{xu2020variations}.

% We note from previous section \ref{Comparison_to_baselines} that
% clustering from k-means + k-mers have some similarity with the
% clustering from k-means + minimizer.  For this reason, we compute
% the labels from both clustering and take their consensus (agreement
% of both approaches) to get new labels (we drop the sequences for
% which these two approaches do not have consensus labels). The total
% consensus labels that we got from this approach are $5738$. The
% consensus labels and corresponding feature embedding are then
% further evaluated using different statistical methods such as
% Spearman Correlation and Pearson Correlation.  \murray{I understand
% that Figure 13 is not that informative, as mentioned by a reviewer
% of the SDM paper, but I do not see what extra info these other two
% statistical measures provide}

\subsubsection{Statistical Analysis}

Since the information gain is in terms of positions of the genomic
sequence, it does not provide information on how important the
\emph{features} of each embedding are (since feature vectors are in
the Euclidean space). For this purpose, we use Spearman and Pearson
correlation to evaluate the (negative and positive) importance of
features in the different embeddings.  Since we note from the previous
Section \ref{Comparison_to_baselines} that the combination of $k$-mers
+ $k$-means is quite similar to minimizers + $k$-means, we performed
such an analysis on the consensus (agreement of both clusterings)
labels from both clusterings.  A total of 5738 labels and the
corresponding feature embedding were analyzed.

\paragraph{Spearman Correlation.}

%It is essential to evaluate the nucleotide's negative (or opposite)
%contribution corresponding to the class label, which we do not get
%using IG.

We use Spearman correlation~\cite{myers2004spearman} to evaluate the
contribution of different attributes of feature embeddings.
% \todo{GDV It is not clear to me what is the puropose of spearman and pearson correlations}
% The Pearson Correlation is computed using the following expression:
% \begin{equation}
%     r = \frac{\sum (x_i - \Bar{x}) (y_i - \Bar{y})}{\sqrt{\sum (x_i - \Bar{x})^2 (y_i - \Bar{y})^2}}
% \end{equation}
% where $r$ is the correlation coefficient, $x_{i}$is the values of the x-variable in a sample, $\Bar{x}$ is the mean of the values of the x-variable, $y_{i}$is the values of the y-variable in a sample, and $\Bar{y}$ is the mean of the values of the y-variable.
The Spearman Correlation is computed using the following expression:
%$\rho = 1 - \frac{6 \sum d_{i}^{2}}{n (n^2 - 1)}$,
 \begin{equation}
     \rho = 1 - \frac{6 \sum d_{i}^{2}}{n (n^2 - 1)}~,
 \end{equation}
where $\rho$ is the Spearman's rank correlation coefficient, $d_{i}$
is the difference between the two ranks of each observation, and $n$
is the total number of observations.

% The Pearson correlation and corresponding P-values for PWM2Vec are
% given in Figure~\ref{fig_pearson_corr} while
The number of features having negative and positive Spearman
correlation for the different ranges for different embedding methods
are shown in Figure~\ref{spearman_pearson_real_data} (a) for negative
and the positive correlation range. In terms of negative correlation,
we can observe that there is only a small fraction of features in the
case of OHE. At the same time, other embeddings do not have any
features which are negatively correlated to the consensus labeling. In
the case of positive correlation, we can observe that the $k$-mers
based embedding has more features with high correlation values than
the other embeddings. This indicates that the feature embedding from
$k$-mers is more compact than OHE and the minimizers based feature
embedding approach.
% \textcolor{red}{Prakash: Review observations with sarwan} 
% We can observe that most of the features are contributing towards the prediction of different hosts.
% \textcolor{red}{ We can observe that with less dimensional and more compact approach for feature embedding k-mers, the fraction of features having correlation values greater than the threshold (i.e. $0.3$ and $-0.3$) are greater than those given for OHE and comparable with those given for k-mers (sometimes better also). This behavior shows that using PWM2Vec, we are able to preserve more information in a smaller feature vector and improve the runtime of underlying ML algorithms while giving better (sometimes comparable) predictive performance.}

\paragraph{Pearson Correlation.}

We also use Pearson correlation~\cite{benesty2009pearson} to evaluate
the compactness of different feature embeddings.  The Pearson
correlations are shown in Figure~\ref{spearman_pearson_real_data} (b)
for the negative and the positive correlation range. In the case of negative correlation, we can observe that OHE has more features corresponding to the consensus labeling, with a higher Pearson correlation. However, in the case of positive correlation, the $k$-mers based feature vector seems to be more
compact than OHE and minimizers based feature embedding.
   
% \end{enumerate}

% \begin{figure}[h!]
%     \centering
%     \includegraphics{Figures/Tikz_Figures/attribute_correlation.tikz}
%     \caption{Negative correlation pattern for pearson and spearman correlation.}
%     \label{negative_spearman_pearson}
% \end{figure}

% \begin{figure}[h!]
%     \centering
%     \includegraphics{Figures/Tikz_Figures/attribute_correlation_positive.tikz}
%     \caption{Positive correlation pattern for pearson and spearman correlation.}
%     \label{positive_spearman_pearson}
% \end{figure}


\begin{figure}[h!]
  \centering
  \begin{subfigure}{.50\textwidth}
    \centering
    \includegraphics[scale = 0.8] {Figures/Tikz_Figures/spearman_pearson_real_data/spearman.pdf}
    \caption{Spearman}
  \end{subfigure}%
  \begin{subfigure}{.50\textwidth}
    \centering
    \includegraphics[scale = 0.8] {Figures/Tikz_Figures/spearman_pearson_real_data/pearson.pdf}
    \caption{Pearson}
  \end{subfigure}%
  \caption{Fraction of features correlated with the labeling on the
    real data in the case of Spearman and Pearson. This figure is best
    seen in color. }
  \label{spearman_pearson_real_data}
\end{figure}

% \begin{table}[h!]
% \centering
%     \caption{Information Gain on 1446 Sequences (Total IG 29907)}
%     \begin{tabular}{cc}
%     \hline
%         IG Range & Count \\
%         \hline \hline 
        
%         0 to 0.1 & 15078 \\ 
%         0.1 to 0.2 & 1402 \\ 
%         0.2 to 0.3 & 158 \\  
%         0.3 to 0.4 & 371 \\ 
%         0.4 to 0.5 & 951 \\ 
%         0.5 to 0.6 & 2035 \\ 
%         0.6 to 0.7 & 4351 \\ 
%         0.7 to 0.8 & 4552 \\ 
%         0.8 to 0.9 & 1003 \\ 
%         0.9 to 1.0 & 6  \\
%         \hline
%     \end{tabular}
% \end{table}


% \begin{figure}
%     \centering
%     \includegraphics{Figures/Tikz_Figures/InfoGain.tikz}
%     \caption{Info Gain.}
%     \label{fig_info_gain}
% \end{figure}

% We also evaluate the performance of different feature embedding
% using runtime by applying $k$-means on the embedding with an
% increasing number of sequences (see
% Figure~\ref{runtime_classification_plt}). We can observe that the
% overall trend of \textsc{ohe} is kind of linear while $k$-mers based
% embedding significantly outperforms \textsc{ohe} in terms of
% runtime. Note that since $k$-mers and minimizer have the same
% feature vector length, the runtime for both of them is the
% same. That is the reason why we just plotted $k$-mers and
% \textsc{ohe} in Figure~\ref{runtime_classification_plt}.


\section{Conclusion}
%----------------------------------------------------------------------
\label{sec_conclusion}

In this paper, we propose an efficient alignment-free feature vector
embedding approach, Reads2Vec, for the setting of raw high-throughput
reads data.  Such embedding is used as input to different supervised
(\ie, classification) and unsupervised (\ie, clustering) methods for
different machine learning-based tasks. We perform experiments on both
real-world and simulated short read data related to the coronavirus,
SARS-CoV-2. We show that Reads2Vec outperforms other alignment-free
baselines in terms of predictive performance on the classification
task.  Using different clustering evaluation metrics, we indicate that
alignment-free embeddings are more suited to this raw sequencing reads
setting than the widely accepted Pangolin tool.
%We also show that the
%proposed feature embedding is better in terms of runtime than more
%traditional embedding methods such as OHE, on top of the fact that
%they avoid the expensive genome assembly step.
Finally, in computing the information gain (IG), we show that most of
the genomic positions having high IG corresponding to the labeling
(from the clustering) are concentrated in the spike region of the
SARS-CoV-2 genome, which is consistent with current biological
knowledge this virus.

In the future, we would explore the scalability of such approaches by
using more data.  Another direction of future work is to explore how
sensitive the predictions of the Pangolin tool, or OHE are to the
genome assembly.  Finally, we would like to explore the usage of such
embedding in conjunction with alignment-free variant calling, which
could possibly eliminate even more dependencies on the genome assembly
step.

\section*{Acknowledgement}

The authors would like to thank Bikram Sahoo for helpful discussions
on the choice and usage of InSilicoSeq.  An early version of this
paper was published as part of the 2021 11th International Conference
on Computational Advances in Bio and medical Sciences (ICCABS).

\section*{Author Contribution Statement}

\textbf{SC, MP:} Conceptualization. \textbf{SA:}
Methodology. \textbf{PC, SA, MP:} Software. \textbf{PC, SA:}
Validation. \textbf{PC, SA:} Formal Analysis. \textbf{All:}
Investigation. \textbf{All:} Resources. \textbf{MP:} Data
Curation. \textbf{All:} Writing - Original Draft. \textbf{All:}
Writing - Review \& Editing. \textbf{PC, SA:}
Visualization. \textbf{GDV, MP:} Supervision. \textbf{GDV, MP:}
Administration. \textbf{SA, GDV, MP:} Funding Acquisition.

\section*{Conflict of Interest}

The authors declare no conflict of interest.

\section*{Funding Statement}

Research supported by an MBD Fellowship for SA, and a Georgia State
University Computer Science start-up grant for MP.  This project has
received funding from the European Union’s Horizon 2020 Research and
Innovation Staff Exchange programme under the Marie Skłodowska-Curie
grant agreement No. 872539.

\bibliographystyle{alpha}
\bibliography{references}

\end{document}

% LocalWords:  CoV Prakash Chourasia Sarwan Ciccolella Gianluca et al
% LocalWords:  Vedova Milano Bicocca genomic COVID Pangolin GISAID Hu
% LocalWords:  curation clusterings embeddings FASTA proteome mers VM
% LocalWords:  parameterization phylogenetic Nextstrain phylogenetics
% LocalWords:  parallelization scalability nucleotides unassembled IG
% LocalWords:  minimizer PCR ARI dataset metagenomic mer analytics UK
% LocalWords:  electromyography SVM NCBI SRA Ensembl GDV OHE Entrez
% LocalWords:  fasterq bwa mem BAM samtools mpileup bcftools VCF KPDA
% LocalWords:  Snakemake reproducibility Minimizers minimizers Pango
% LocalWords:  Num SNE Calinski Harabasz Bouldin labelings Fowlkes de
% LocalWords:  FMI Spearman runtime Kbp Spearman's curated metadata
% LocalWords:  Sec Vec parameterizing SOTA coronavirus dimensionality
% LocalWords:  runtimes scalable PWM metagenomics lexicographically
% LocalWords:  oversampled overfitting facto datasets AY
% LocalWords:  MiSeq perceptron MLP KNN DT ROC AUC GCA Algo Acc Prec
% LocalWords:  Weig Acknowledgement Bikram Sahoo inSilicoSeq MBD
