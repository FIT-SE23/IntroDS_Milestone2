%\PassOptionsToPackage{hyperfootnotes=false}{hyperref}

\documentclass[runningheads]{llncs}

%\documentclass[sn-mathphys]{sn-jnl}
% \documentclass[a4paper,fleqn,11pt]{cas-sc}
% \documentclass[review,3p]{elsarticle}

%\usepackage{setspace}
%\let\cite\citep

%% \usepackage[numbers]{natbib}
%\usepackage[authoryear]{natbib}
%%\usepackage[authoryear,longnamesfirst]{natbib}
%\usepackage{natbib}


%\setcounter{tocdepth}{3}
%\setcounter{secnumdepth}{3}






%\usepackage{hyperref}
%\makeatletter
%\g@addto@macro{\UrlBreaks}{\UrlOrds}
%\makeatother


\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}



%\usepackage[symbol]{footmisc}
%\usepackage{footmisc}
%\usepackage{fontawesome}

\usepackage{paralist}
\usepackage{booktabs} % For formal tables
%\usepackage{color}


%\usepackage{algorithm}
%\usepackage[noend]{algpseudocode}
% \usepackage[ruled,linesnumbered,vlined]{algorithm2e}

\usepackage{esvect}
\usepackage{multirow}
\usepackage{subcaption}
\usepackage[para,online,flushleft]{threeparttable}

%\usepackage{epsfig}
\usepackage{epstopdf}
\usepackage{graphics}
\usepackage{graphicx}
\DeclareGraphicsExtensions{.eps,.gz,.eps,.pdf,.png,.jpg}
% \epstopdfsetup{update}
% \epstopdfDeclareGraphicsRule{.ps}{pdf}{.pdf}{ps2pdf #1 \OutputFile}
\graphicspath{{./}{../}}
%% Package declaration

\usepackage[utf8]{inputenc}
% \usepackage[english,vietnam]{babel}
%\usepackage[T1,T5]{fontenc}
%\usepackage[table]{xcolor}
\usepackage[tablename=Table]{caption} % change bảng to table
\usepackage[figurename=Figure]{caption}
%\usepackage{mathptmx}
\usepackage{upgreek}
\usepackage{array}
%\usepackage{wasysym}
\usepackage{graphicx}
\usepackage{lineno,hyperref}
\modulolinenumbers[5]
\usepackage{float}
\usepackage{multirow}
%\usepackage[fleqn]{amsmath}

\usepackage{algorithm}
\usepackage{algorithmicx}
\usepackage{algpseudocode}
%\modulolinenumbers[5]
\usepackage{adjustbox}
%Table display


\usepackage{bbm}
\usepackage{dsfont}

\usepackage{xcolor}
\usepackage{color, soul}
%json display
\usepackage{listings}
\newcommand{\todo}[1]{\textcolor{red}{\bf #1}}
\newcommand{\edit}[1]{\textcolor{blue}{\bf #1}}
\newcommand{\sstitle}[1]{\smallskip\noindent\textbf{#1.\/}}
\newcommand{\smtitle}[1]{\medskip\noindent\textbf{#1.\/}}
\newcommand{\sititle}[1]{{\it #1:\/}}
\newtheorem{mydef}{Definition}
\newtheorem{mythm}{Theorem}
\newtheorem{myprob}{Problem}

\def\Snospace~{\S{}}
\renewcommand*\sectionautorefname{Section}
%\renewcommand*\subsectionautorefname{\Snospace}
\renewcommand{\subsectionautorefname}{Section}
\renewcommand*\subsubsectionautorefname{Section}
\renewcommand*\equationautorefname{Equation}
%%\renewcommand*\algorithmautorefname{Algorithm}
%\renewcommand{\algorithmautorefname}{Alg.}
\renewcommand*\figureautorefname{Figure}
%\newcommand\myprobautorefname{Problem}
%\renewcommand*\theoremautorefname{Theorem}
\renewcommand*\tableautorefname{Table}
%%\newcommand{\mylemautorefname}{Lemma}
%\newcommand{\mypropautorefname}{Prop.}
%\newcommand*\exampleautorefname{Example}


\makeatletter
\newcommand{\removelatexerror}{\let\@latex@error\@gobble}
\makeatother

\newcommand\Mark[1]{\textsuperscript#1}
\def\ua{\Mark{*}}
\def\ub{\Mark{\dag}}
\def\uc{\Mark{\#}}
\def\ud{\Mark{+}}
%JSON format design
\colorlet{punct}{red!60!black}
\definecolor{background}{HTML}{EEEEEE}
\definecolor{delim}{RGB}{20,105,176}
\colorlet{numb}{magenta!60!black} 

\lstdefinelanguage{json}{
    basicstyle=\normalfont\ttfamily,
    numbers=left,
    numberstyle=\scriptsize,
    stepnumber=1,
    numbersep=8pt,
    showstringspaces=false,
    breaklines=true,
    frame=lines,
    backgroundcolor=\color{background},
    literate=
     *{0}{{{\color{numb}0}}}{1}
      {1}{{{\color{numb}1}}}{1}
      {2}{{{\color{numb}2}}}{1}
      {3}{{{\color{numb}3}}}{1}
      {4}{{{\color{numb}4}}}{1}
      {5}{{{\color{numb}5}}}{1}
      {6}{{{\color{numb}6}}}{1}
      {7}{{{\color{numb}7}}}{1}
      {8}{{{\color{numb}8}}}{1}
      {9}{{{\color{numb}9}}}{1}
      {:}{{{\color{punct}{:}}}}{1}
      {,}{{{\color{punct}{,}}}}{1}
      {\{}{{{\color{delim}{\{}}}}{1}
      {\}}{{{\color{delim}{\}}}}}{1}
      {[}{{{\color{delim}{[}}}}{1}
      {]}{{{\color{delim}{]}}}}{1},
}
\renewcommand{\figureautorefname}{Figure}
\renewcommand{\tableautorefname}{Table}


% Uncomment and use as if needed
%\newtheorem{theorem}{Theorem}
%\newtheorem{lemma}[theorem]{Lemma}
%\newdefinition{rmk}{Remark}
%\newproof{pf}{Proof}
%\newproof{pot}{Proof of Theorem \ref{thm}}
%\jyear{2021}%

%%% as per the requirement new theorem styles can be included as shown below
%\theoremstyle{thmstyleone}%
%\newtheorem{theorem}{Theorem}%  meant for continuous numbers
%%%\newtheorem{theorem}{Theorem}[section]% meant for sectionwise numbers
%%% optional argument [theorem] produces theorem numbering sequence instead of independent numbers for Proposition
%\newtheorem{proposition}[theorem]{Proposition}% 
%%%\newtheorem{proposition}{Proposition}% to get separate numbers for theorem and proposition etc.
%
%\theoremstyle{thmstyletwo}%
%\newtheorem{example}{Example}%
%\newtheorem{remark}{Remark}%
%
%\theoremstyle{thmstylethree}%
%\newtheorem{definition}{Definition}%

%\raggedbottom
%%%\unnumbered% uncomment this for unnumbered level heads



\begin{document}
%\let\WriteBookmarks\relax
%\def\floatpagepagefraction{1}
%\def\textpagefraction{.001}


% \shorttitle{A comparative study of question answering over knowledge bases}

% \shortauthors{Khiem Vinh Tran et~al.}

% Main title of the paper
%\title [mode = title]{A comparative study of question answering over knowledge bases}                      
% Title footnote mark
% eg: \tnotemark[1]
% \tnotemark[1,2]




\title {A Comparative Study of Question Answering over Knowledge Bases}  
%\titlerunning{A comparative study of question answering over knowledge bases}

\author{
Khiem Vinh Tran\inst{1}
\and
Hao Phu Phan\inst{2}
\and
Khang Nguyen Duc Quach\inst{3}
\and
Ngan Luu-Thuy Nguyen\inst{1}
\and
Jun Jo\inst{3}
\and
Thanh Tam Nguyen\inst{3}
}

\authorrunning{Khiem Vinh Tran et al.} % abbreviated author list (for running head)

\institute{
University of Information Technology, Ho Chi Minh City, Vietnam
%\and
%Vietnam National University, Ho Chi Minh City, Vietnam
\and
HUTECH University, Ho Chi Minh City, Vietnam
\and
Griffith University, Gold Coast, Australia
% \email{k.quach@griffith.edu.au}\\
%\\
%\email{\inst{\clubsuit}\{khiemtv, ngannlt\}@uit.edu.vn, \inst{+}\{phuhao.p1004, duckhang.quach, thanhtamlhp\}@gmail.com, \inst{-}\{j.jo\}@griffith.edu.au}
}


%\author[1,2]{ \sur{Khiem Vinh Tran}}\email{khiemtv@uit.edu.vn}
%\author[3]{ \sur{Khang Nguyen Duc Quach}}\email{duckhang.quach@gmail.com}
%\author[4]{ \sur{Hao Phu Phan}}\email{phuhao.p1004@gmail.com}
%% \equalcont{These authors contributed equally to this work.}
%\author[1,2]{ \sur{Ngan Luu-Thuy Nguyen}}\email{nganlt@uit.edu.vn}
%\author[3]{ \sur{Jun Jo}}\email{j.jo@griffith.edu.au}
%\author[3]{ \sur{Thanh Tam Nguyen}}\email{thanhtamlhp@gmail.com}
%% \author[3]{ \sur{Quoc Viet Hung Nguyen}}\email{quocviethung.nguyen@griffith.edu.au}
%\affil[1]{ \orgname{University of Information Technology}, \orgaddress{ \city{Ho Chi Minh city},   \country{Vietnam}}}
%
%\affil[2]{ \orgname{Vietnam National University}, \orgaddress{\city{Ho Chi Minh city},   \country{Vietnam}}}
%
%%\affil[3]{ \orgname{Leibniz Universi{\"a}t Hannover}, \orgaddress{ \city{Hannover},  \country{Germany}}}
%
%\affil[3]{ \orgname{Griffith University}, \orgaddress{ \city{Gold Coast}, \country{Australia}}}
%
%\affil[4]{ \orgname{Ho Chi Minh City University of Technology}, \orgaddress{\city{Ho Chi Minh city},   \country{Vietnam}}}




\maketitle

%\begin{abstract} 
\abstract{
Question answering over knowledge bases (KBQA) has become a popular approach to help users extract information from knowledge bases. Although several systems exist, choosing one suitable for a particular application scenario is difficult. In this article, we provide a comparative study of six representative KBQA systems on eight benchmark datasets. In that, we study various question types, properties, languages, and domains to provide insights on where existing systems struggle. On top of that, we propose an advanced mapping algorithm to aid existing models in achieving superior results. Moreover, we also develop a multilingual corpus COVID-KGQA, which encourages COVID-19 research and multilingualism for the diversity of future AI. Finally, we discuss the key findings and their implications as well as performance guidelines and some future improvements. Our source code is available at \url{https://github.com/tamlhp/kbqa}.


%However, most KBQA systems still confront significant difficulties in converting natural languages (NL) to query languages (e.g. SPARQL).
%, six evaluation metrics, eleven question types, several languages and domains. 
%Our study suggests that future KBQA systems should match new knowledge bases as their F1-score is less than 0.5 in many cases. Our promising results on deep learning methods (up to 0.82 accuracy) might inspire researchers to continue this approach.
%Our method achieves a recall of 0.98, 0.77, and 0.71 on the LC-QUAD, QALD-9, and QALD-7 benchmark datasets, respectively. 







% \todo{A Comparative Study of Outfit Recommendation Methods with a
% Focus on Attention-based Fusion}

% In recent years, deep learning-based recommender systems have received increasing attention, as
% deep neural networks can detect important product features in images and text descriptions and
% capture them in semantic vector representations of items. This is especially relevant for outfit
% recommendation, since a variety of fashion product features play a role in creating outfits. This
% work is a comparative study of fusion methods for outfit recommendation that combine relevant
% product features extracted from visual and textual data in semantic, multimodal item representations. We compare traditional fusion methods with attention-based fusion methods,
% which are designed to focus on the fine-grained product features of items. We evaluate the fusion
% methods on four benchmark datasets for outfit recommendation and provide insights into the
% importance of the multimodality and granularity of the fashion item representations. We find that
% the visual and textual item data not only share product features but also contain complementary
% product features for the outfit recommendation task, confirming the need to effectively combine
% them into multimodal item representations. Furthermore, we show that the average performance
% of attention-based fusion methods surpasses the average performance of traditional fusion
% methods on three out of the four benchmark datasets, demonstrating the ability of attention to
% learn relevant correlations among fine-grained fashion attributes.

% \todo{On the cost-effectiveness of neural and non-neural approaches and
% representations for text classification: A comprehensive
% comparative study}
% This article brings two major contributions. First, we present the results of a critical analysis
% of recent scientific articles about neural and non-neural approaches and representations for
% automatic text classification (ATC). This analysis is focused on assessing the scientific rigor of
% such studies. It reveals a profusion of potential issues related to the experimental procedures
% including: (i) use of inadequate experimental protocols, including no repetitions for the sake of
% assessing variability and generalization; (ii) lack of statistical treatment of the results; (iii) lack
% of details on hyperparameter tuning, especially of the baselines; (iv) use of inadequate measures
% of classification effectiveness (e.g., accuracy with skewed distributions). Second, we provide
% some organization and ground to the field by performing a comprehensive and scientifically
% sound comparison of recent neural and non-neural ATC solutions. Our study provides a more
% complete picture by looking beyond classification effectiveness, taking the trade-off between
% model costs (i.e., training time) into account. Our evaluation is guided by scientific rigor,
% which, as our literature review shows, is missing in a large body of work. Our experimental
% results, based on more than 1500 measurements, reveal that in the smaller datasets, the simplest
% and cheaper non-neural methods are among the best performers. In the larger datasets, neural
% Transformers perform better in terms of classification effectiveness. However, when compared to
% the best (properly tuned) non-neural solutions, the gains in effectiveness are not very expressive,
% especially considering the much longer training times (up to 23x slower). Our findings call for
% a self-reflection of best practices in the field, from the way experiments are conducted and
% analyzed to the choice of proper baselines for each situation and scenario.

\keywords{Question answering  \and Knowledge base \and Query processing}
}

% Use if graphical abstract is present
% \begin{graphicalabstract}
% \includegraphics{figs/grabs.pdf}
% \end{graphicalabstract}

% Research highlights
%\begin{highlights}
%\item Research highlights item 1
%\item Research highlights item 2
%\item Research highlights item 3
%\end{highlights}

% Keywords
% Each keyword is seperated by \sep
% \begin{keywords}
% Question answering \sep Knowledge base \sep Knowledge graph \sep Query processing
% \end{keywords}

%\keywords{Question answering, Knowledge base, Knowledge graph, Query processing}




%\doublespacing

\section{Introduction}

Question Answering (QA) is a long-standing discipline within the field of natural language processing (NLP), which is concerned with providing answers to questions posed in natural language on data sources, and also draws on techniques from linguistics, database processing, and information retrieval~\cite{ADMA3,hung2013evaluation,nguyen2015result,tam2019anomaly,nguyen2020monitoring}. 
One important type of data sources is knowledge bases, also known as knowledge graphs, which have been automatically constructed
from web data and have become a key asset for search engines
and many applications
\cite{Weikum2021}. 
%The number of knowledge graphs (KGs) has increased at an unprecedented rate over the past 15 years~\cite{Weikum2021,auer2007dbpedia,10.1145/1376616.1376746,10.1145/2629489}. These KGs include a wealth of information that may possibly be utilized for QA. 
Finding answers for a question in a KB, on the other hand, is not always straightforward. The user needs to have a thorough understanding of the KB as well as a structured query language in order to express their queries in a structured manner that can be utilized to locate matches in the KB. 

In order to address this issue, a significant number of QA systems that allow users to express their information requirements using natural language have been created. Additionally, factoid question answering has two main approaches, information retrieval (IR) based QA and knowledge-based QA. In the first approach, many research have been established in recent years with the advancement of machine reading comprehension (MRC) task. The second approach is question answering over knowledge base (KBQA) with precision of question answering over knowledge graph (KG). Many studies have been conducted with KG such as  \cite{ADMA1} and recently research related to KG \cite{ADMA2,nguyen2021judo,nguyen2020factcatch}.

However, understanding the performance implications of these techniques is a challenging problem. While each of them has distinct performance characteristics, their performance evaluations often lack diversity in domains, query languages, natural languages and datasets.
First, many developed datasets contain questions expressed in plain language but answers are specific to a KB format such as DBPedia.
%answers to the questions from the KB, and potentially structured queries that retrieve the answers already indicated. 
%To assess a freshly created QA system, its developers must select datasets developed in a large KB. Most existing works choose DBPedia as a KB for evaluation because many QA datasets are built on top of this KB. As a result, they are restricted to a small number of KBs.
Additionally, the number of questions in each of the  existing benchmarks is substantially different, rendering it difficult for users to select a practical guideline. 
%For example, one dataset has  a modest number of questions (100 questions)~\cite{sicilianimqald} while another has  5,000 questions~\cite{trivedi2017lc}.
Second, there is a lack diversity in terms of query languages. 
%Today, several benchmarks make use of SPARQL-only datasets. In order to find answers in the desired KB, the user first parses the question, retrieves the SPARQL query, and then utilizes the QA system. In order to generate various evaluation scores such as micro, macro, and global F-1 scores, the provided answers are compared to the answers retrieved from the benchmark file. When the QA system does not properly answer all of the questions, the user goes back through the questions and debugs the code to figure out why the QA system is having trouble with certain questions~\cite{Orogat2021}. 
Another issue is the application domain. Most KBQA systems assume the generalization over different domains, but only a few domain-specific datasets are evaluated~\cite{costa2018leveraging,nguyen2019user,nguyen2020entity,nguyen2021structural,nguyen2019maximal}. 





%\subsection{Research objectives}
%
%The main objectives of our study are three-fold:
%\begin{compactitem}
%\item To perform a comprehensive and scientifically sound comparison of traditional and deep learning solutions considering the tradeoff between efficiency and effectiveness.
%
%\item To evaluate the effects of different types of questions on KBQA methods.
%
%\item To understand the theoretical  and practical differences between SPARQL and SQL, as well as traditional knowledge bases and knowledge graphs.
%\end{compactitem}
%
%Both the three goals have novel aspects with respect to the current literature. Existing works, even the most recent and cited papers in the field, have only focused on specific datasets, evaluation metrics, or few types of questions. Moreover, there has not been a comprehensive comparative study in the field that properly covers multiple aspects of KBQA.
%To this end, we prepare a wide range of datasets, including two SQL datasets, six SPARQL datasets, eleven question types, four generic datasets, four domain-specific datasets, four multi-language datasets, and four single-language datasets. Most of the used datasets have been largely used as benchmarks across KBQA communities.



% \begin{table*}[!h]
% %\begin{minipage}[c]{\linewidth}
% \centering
% \caption{Comparison between existing benchmarks on QA over KBs}
% \label{tbl:comparative_comparison}
% \footnotesize

% \begin{table*}[!h]
% %\begin{minipage}[c]{\linewidth}
% \centering
% \caption{Comparison between existing benchmarks on QA over KBs}
% \label{tbl:comparative_comparison}
% \footnotesize

% %\vspace{-1em}
% %\resizebox{1.0\textwidth}{!}{
% % \begin{tabular}{c|c|c|c}
% % \toprule
% % %    \multirow{2}{*}{Benchmarks} &\multirow{2}{*}{Our} & \multirow{2}{*}{CBench\cite{orogat2021cbench}} & \multirow{2}{*}{Steinmetz \cite{steinmetz2021kgqa}} & \multirow{2}{*}{ Costa \cite{costa2018leveraging}}   \\
% % {Benchmarks} & {Our} & {\cite{Orogat2021}}  & {\cite{costa2018leveraging}}   \\
% % \midrule
% %     New data & {\checkmark}  & {$\times$} & {$\times$}   \\ 
% %     Multi-domain & {\checkmark} & {$\times$} & {$\times$}   \\ 
% %     Cross-Query Language & {\checkmark} & {$\times$} & {$\times$}   \\ 
% %     Multi-language & {\checkmark} & {\checkmark}  & {$\times$}  \\ 
% %     % Cross-KG & {\color{green}\faCheck}  & {\color{green}\faCheck}  & {\color{green}\faCheck} & {\color{green}\faCheck}   \\ %et al. (2021)
% %     Code & {\checkmark} & {\checkmark}  & {$\times$}\\
% %     Large KBs & {\checkmark} & {\checkmark} & {\checkmark}   \\ %et al. (2018)
% % % Up to Date system
     
% %     % \multirow{2}{*}{Benchmarks} &\multirow{2}{*}{Our} & \multirow{2}{*}{} & \multirow{2}{*}{} & \multirow{2}{*}{} &\multirow{2}{*}{} & \multirow{2}{*}{} & \multirow{2}{*}{New Datasets} & \multirow{2}{*}{New Datasets} \\
% % \bottomrule
% % \end{tabular}
% % % }

% \begin{tabular}{c|c|c|c|c|c|c|c} 
% \toprule
% Benchmark & New data & Cross-query language & Multi-domain & Multi-language & Code & Large KBs  \\ 
% \midrule
% Ours       &  {\checkmark}   & {\checkmark}  & {\checkmark}    & {\checkmark}      &  {\checkmark}              &  {\checkmark}             \\
% {\cite{Orogat2021}}     & $\times$    & $\times$ & $\times$    & {\checkmark}      &    {\checkmark}       & {\checkmark}               \\
% {\cite{costa2018leveraging}}      &   $\times$  & $\times$ & $\times$  & $\times$    &    $\times$           &      {\checkmark}       \\
% \bottomrule 
% \end{tabular}
% %\end{minipage}
% \vspace{-1em}
% \end{table*}

% \begin{tabular}{c|c|c|c|c|c|c|c} 
% \toprule
% Benchmark & New data & Cross-query language & Multi-domain & Multi-language & Code & Large KBs  \\ 
% \midrule
% Ours       &  {\checkmark}   & {\checkmark}  & {\checkmark}    & {\checkmark}      &  {\checkmark}              &  {\checkmark}             \\
% {\cite{Orogat2021}}     & $\times$    & $\times$ & $\times$    & {\checkmark}      &    {\checkmark}       & {\checkmark}               \\
% {\cite{costa2018leveraging}}      &   $\times$  & $\times$ & $\times$  & $\times$    &    $\times$           &      {\checkmark}       \\
% \bottomrule 
% \end{tabular}
% %\end{minipage}
% \vspace{-1em}
% \end{table*}



%\subsection{Contributions}

More precisely, the salient contributions of our benchmark are highlighted as follows:
\begin{compactitem}
    
%    \item \textbf{A systematic and comparative review:}  We summarise the landscape of question answering over knowledge bases, including the most up-to-date techniques and benchmarks in influential international conferences and journals, including TKDE, WWW, ACL, ISWC, and others.

%	\item \textbf{Extensible taxonomy:} We examine and compare the performance of the most successful KQBA systems on existing datasets.  In addition, we also evaluate the empirical study of these systems and datasets with the effects of differences in terms of a variety of linguistic aspects of the natural language questions in the benchmarks.
%	To illustrate the consequences of such differences, we analyze five KBQA systems with both generic and domain-specific datasets and demonstrate that the quality ratings of the systems differ considerably. Additionally, we provide valuable insights, such as dataset selection and QA assessment measures. To the best of our understanding, we are the first to encompass ideas of data analysis and empirical comparison in KBQA benchmarks.

    \item \textbf{Reproducible benchmarking:} 
    We present the first large-scale replicable benchmarking framework for comparing KBQA techniques. Additionally, it can be applied to novel methods proposed in future studies. 
%    The framework is composed of components, which enables the direct integration of new KBQA techniques in addition to the default ones. Additionally, our framework includes an application layer that enables the investigation of various KBQA performance. In order to facilitate study, we have made the source code and dataset (which we generated) accessible. 
    Our findings are reliable and reproducible, and the source code is publicly available at {\url{https://github.com/tamlhp/kbqa}}.
    % We present the first large-scale replicable KBQA benchmarking framework. Additionally, it can also be used for future research methods. The framework is composed of components that enables the direct integration of new KBQA techniques. Our framework also includes an application layer for analyzing KBQA performance. We made the source code and dataset available for study. Our findings are reliable and reproducible  with open source code~\footnote{\url{https://github.com/tamlhp/kbqa}}.
	
	\item \textbf{Surprising findings:} While some of our experimental findings confirm the state-of-the-art, we demonstrate the surprising superiority of methods using both SPARQL and SQL query languages. Additionally, we discovered that there are differences in their effectiveness when compared together.

	\item \textbf{New KGQA dataset:} We present COVID-KGQA, a new knowledge graph question answering benchmarking corpus that includes the most questions about COVID-19. COVID-KGQA includes more than 1000 questions with different types.


    
    \item \textbf{Performance guideline.} We provide valuable performance guidelines to academics working on KBQA in terms of benchmarks and QA systems selection. Alternatively, we suggest some future attempts to improve the performance.

\end{compactitem}




The remainder is organized as follows. 
%\autoref{sec:background} briefly reviews related work.
We discuss in \autoref{sec:methodology} the problem setting, our benchmarking procedure, and the most representative approaches in KBQA. \autoref{sec:setup} introduces the setup used for our benchmark, including the analysis of KBs, datasets, metrics, and evaluation procedures. \autoref{sec:exp} reports the experimental results. A discussion of practical guidelines and conclusion are provided in \autoref{sec:con}. 
%\autoref{sec:discuss}. Finally, \autoref{sec:con} concludes the paper.







\section{Methodology}
\label{sec:methodology}

\subsection{Problem Setting}
\label{sec:ProbStat}

% \textbf{Problem Statement 1.} Given a knowledge graph G and
% a text corpus D, we need to generate binary templates for G.
% The text corpus D is a set of documents that relate to
% the knowledge graph. Generally, D is easy to obtain. For
% example, Wikipediais the resource for DBpedia [29]. 
% \textbf{Problem Statement 2.} Given a knowledge graph G, a
% set of templates T, and an input question q, the goal is to
% construct a semantic dependency graph for q.
% chua paraphase
% Introduction to Neural Network based Approaches for Question Answering over Knowledge Graphs
 Question Answering over Knowledge Bases (KBQA) is the term used for the task of retrieving the answer from executing query matching with natural language questions over a knowledge base as follow. Formally, we can define the task of KBQA as follow. Let $KB$ be a knowledge base, Q is a question, $q$ is the matching query and $A$ is an answer extracted by matching query $q$ executed from given question $Q$ over $KB$. According to \cite{chakraborty2021introduction}, the set of all possible answers can be in the form as follows. The first is the union of the power set $P$ of entities $E$ and literals $L$ in $KB$: $P (E \cup L)$. The second is the number of outcomes set for all potential functions of aggregation $F: P (E \cup L) \mapsto \mathbb{R} $. The third is a Boolean set of answers for yes/no questions. \autoref{fig:problemStatement} illustrates the general process of question answering over knowledge bases.

\begin{figure}[!h]
    \vspace{-1em}
    \centering
    \includegraphics[width=0.6\linewidth]{ProblemStatement1.pdf}
    \caption{Question answering over knowledge bases.}
    \label{fig:problemStatement}
     \vspace{-2em}
\end{figure}

% In other words, KBQA involves determining if a natural language question can be addressed correctly and concisely in knowledge bases. The fundamental goal of KBQA is to comprehend the true semantics of a natural language query and extract it in order to match it with the whole semantics of a knowledge base. However, this is a non-trivial problem due to the varying semantics of natural language queries.

% %Viet toi day 6/6
% %  F is the set of
% % all formal queries (see Section 2.2) that can be generated by combining entities and relations
% % from K as well as arithmetic/logical aggregation functions available in the formal query
% % language. The correct logical form f must satisfy the following conditions: (1) the execution
% % of f on K (which we denote by f(K)) yields the correct answer a, as implied by the NLQ and
% % (2) f accurately captures the meaning of the question q. The second condition is important
% % because multiple logical forms could yield the expected results a but not all of them mean the
% % same thing as the question q. We call such logical forms that satisfy the first constraint but
% % not the second spurious logical forms. For an example of spurious logical forms for KGQA
% % lets consider the following NLQ “What books did Michael Crichton write before 1991?”.
% % The queries filter<(and(isA(Novel), writtenBy(Michael Crichton)), publishingYear, 1991) and
% % and(isA(novel), writtenBy(Michael Crichton)) both execute to the same set of books because
% % Michael Crichton didn’t write books after 1991. However, the second query is incorrect since
% % it does not convey the full meaning of the question

% % \usepackage{color}
% % \usepackage{booktabs}


% % \usepackage{color}
% % \usepackage{booktabs}


% % \usepackage{color}
% % \usepackage{booktabs}


% % \usepackage{color}
% % \usepackage{booktabs}



% % \begin{table}[!h]
% % \centering
% % \caption{Example of Question Answering over Knowledge Graphs}
% % \normalsize
% % %\resizebox{\textwidth}{!}{
% % \scalebox{0.45}{
% % \begin{threeparttable}
% % \begin{tabular}{llll} 
% % \toprule
% % \textbf{Answer Type}                              & {\textbf{Sample Question}}                                                                                                                                                                                                         & {\textbf{Query}}                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         & {\textbf{Answer}}                                                                                                        \\ 
% % \midrule
% % \multirow{5}{*}{\begin{tabular}[c]{@{}l@{}}\textbf{\textbf{Union }}\\\textbf{\textbf{of }}\\\textbf{\textbf{Power}}\\\textbf{\textbf{}}\end{tabular}} & \begin{tabular}[c]{@{}l@{}}What\textasciitilde{}is\textasciitilde{}the\textasciitilde{}nick\textasciitilde{}name\textasciitilde{}of\textasciitilde{}Baghdad? \\(QALD-9)\end{tabular}                                                                 & \begin{tabular}[c]{@{}l@{}}PREFIX\textasciitilde{}dbr:\textasciitilde{}http://dbpedia.org/resource/\textasciitilde{}\\PREFIX\textasciitilde{}foaf:\textasciitilde{}http://xmlns.com/foaf/0.1/\textasciitilde{}\\SELECT\textasciitilde{}?nm\textasciitilde{}WHERE\textasciitilde{}\{br:Baghdad\textasciitilde{}foaf:nick\textasciitilde{}?nm\textasciitilde{}\}\end{tabular}                                                                                                                                                                                                & The\textasciitilde{}City\textasciitilde{}of\textasciitilde{}Peace\textcolor[rgb]{0.824,0.824,0.831}{\textcolor[rgb]{0.808,0.569,0.471}{}}  \\
% % \cline{2-4}
% %                                                   & \begin{tabular}[c]{@{}l@{}}Which\textasciitilde{}company\textasciitilde{}which\textasciitilde{}assembles\textasciitilde{}its\textasciitilde{}cars\textasciitilde{}in\textasciitilde{}Broadmeadows,\textasciitilde{}Victoria?\\(LC-QUAD)\end{tabular} & \begin{tabular}[c]{@{}l@{}}SELECT\textasciitilde{}DISTINCT\textasciitilde{}?uri\textasciitilde{}WHERE\textasciitilde{}\{ x\textasciitilde{}http://dbpedia.org/property/assembly\textasciitilde{}\\http://dbpedia.org/resource/Broadmeadows,\_Victoria\textasciitilde{}.\\\textasciitilde{}?x\textasciitilde{}http://dbpedia.org/ontology/parentCompany\textasciitilde{}?uri\textasciitilde{}\textasciitilde{}.\textasciitilde{}?x\textasciitilde{}\\http://www.w3.org/1999/02/22-rdf-syntax-ns\#type\textasciitilde{}http://dbpedia.org/ontology/Automobile\}\end{tabular} & http://dbpedia.org/resource/Ford\_Motor\_Company                                                                                           \\ 
% % \midrule
% % \multirow{4}{*}{\textbf{\textbf{Number}}}         & \begin{tabular}[c]{@{}l@{}}How\textasciitilde{}many\textasciitilde{}awards\textasciitilde{}has\textasciitilde{}Bertrand\textasciitilde{}Russell? \\(QALD-9)\end{tabular}                                                                             & \begin{tabular}[c]{@{}l@{}}PREFIX\textasciitilde{}dbr:\textasciitilde{}http://dbpedia.org/resource/\textasciitilde{}\\PREFIX\textasciitilde{}dbp:\textasciitilde{}http://dbpedia.org/property/\textasciitilde{}\\SELECT\textasciitilde{}(COUNT(?Awards)\textasciitilde{}AS\textasciitilde{}?Counter)\textasciitilde{}\\WHERE\textasciitilde{}\{br:Bertrand\_Russell\textasciitilde{}dbp:awards\textasciitilde{}?Awards\textasciitilde{}\}\end{tabular}                                                                                                                     & 5                                                                                                                                          \\
% % \cline{2-4}
% %                                                   & \begin{tabular}[c]{@{}l@{}}How\textasciitilde{}many\textasciitilde{}shows\textasciitilde{}does\textasciitilde{}HBO\textasciitilde{}have? \\(LC-QUAD)\end{tabular}                                                                                    & \begin{tabular}[c]{@{}l@{}}SELECT\textasciitilde{}DISTINCT\textasciitilde{}COUNT(?uri)\textasciitilde{}WHERE\textasciitilde{}\{ri\textasciitilde{}http://dbpedia.org/ontology/company\textasciitilde{}\\http://dbpedia.org/resource/HBO\textasciitilde{}\textasciitilde{}.\textasciitilde{}\\?uri\textasciitilde{}http://www.w3.org/1999/02/22-rdf-syntax-ns\#type\textasciitilde{}\\http://dbpedia.org/ontology/TelevisionShow\}\end{tabular}                                                                                                                             & 38                                                                                                                                         \\ 
% % \midrule
% % \multirow{4}{*}{\textbf{\textbf{Boolean}}}        & \begin{tabular}[c]{@{}l@{}}Was\textasciitilde{}Margaret\textasciitilde{}Thatcher\textasciitilde{}a\textasciitilde{}chemist? \\(QALD-9)\end{tabular}                                                                                                  & \begin{tabular}[c]{@{}l@{}}ASK\textasciitilde{}WHERE\textasciitilde{}\{ttp://dbpedia.org/resource/Margaret\_Thatcher\textasciitilde{}\\http://dbpedia.org/ontology/profession\textasciitilde{}\\http://dbpedia.org/resource/Chemist\textasciitilde{}\}\end{tabular}                                                                                                                                                                                                                                                                                                        & true                                                                                                                                       \\
% % \cline{2-4}
% %                                                   & \begin{tabular}[c]{@{}l@{}}Was\textasciitilde{}Tim\textasciitilde{}Gunn\textasciitilde{}a\textasciitilde{}guest\textasciitilde{}on\textasciitilde{}The\textasciitilde{}Broken\textasciitilde{}Code? \\(LC-QUAD)\end{tabular}                         & \begin{tabular}[c]{@{}l@{}}ASK\textasciitilde{}WHERE\textasciitilde{}\{ttp://dbpedia.org/resource/The\_Broken\_Code\textasciitilde{}\\http://dbpedia.org/property/guests\textasciitilde{}\\http://dbpedia.org/resource/Tim\_Gunn\textasciitilde{}\}\end{tabular}                                                                                                                                                                                                                                                                                                           & true                                                                                                                                       \\
% % \bottomrule
% % \end{tabular}
% % \begin{tablenotes}
% % \item[1] RDF Syntax: \url{https://www.w3.org/1999/02/22-rdf-syntax-ns}
% % %\item[2] asdfgh
% % \end{tablenotes}
% % \end{threeparttable}
% % }

% % \label{tab:ExampleKGQA}
% % \end{table}





% %\subsection{Benchmarking procedure}
% %A major aim of this research is to develop a flexible but powerful tool to aid in the comparison and analysis of various KBQA methods and techniques of substitution. In order to do this, we have created a framework that makes use of the original performance analysis of these methods. \autoref{fig:framework} depicts the component-based architecture of the framework, which is comprised of three layers: the data access layer, the computation layer, and the application layer. The data access layer abstracts the underlying data items and feeds them to the higher levels of the application stack. The application layer communicates with users in order to receive customizable parameters and to display the results of computations performed by the computing layer. The computational layer is comprised of two major components: the evaluation module and the algorithm module, which work together to perform the experiment and assessment. We anticipate that future research will be able use our methodology  to more readily compare their ideas with current state of the art methods. It is adaptable and extendable in the manner described, since new techniques and measurements can be simply incorporated into the systems.
% %
% %\begin{figure}[!h]
% %%    \vspace{-5pt}
% %    \centering
% %    \includegraphics[scale=0.4]{framework.pdf}
% %    \caption{Benchmarking framework}
% %    \label{fig:framework}
% %%    \vspace{-10pt}
% %\end{figure}


% %As presented, it is straightforward to integrate a new visual forensic or visual forgery method into the framework. We believe that subsequent studies are able to easily compare their proposals with the state-of-the-art techniques by using our framework.

% % \begin{figure}
% % 	\centering
% % 		\includegraphics[scale=.75]{figs/Fig1.pdf}
% % 	\caption{The evanescent light - $1S$ quadrupole coupling
% % 	($g_{1,l}$) scaled to the bulk exciton-photon coupling
% % 	($g_{1,2}$). The size parameter $kr_{0}$ is denoted as $x$ and
% % 	the \PMS is placed directly on the cuprous oxide sample ($\delta
% % 	r=0$, See also Table \protect\ref{tbl1}).}
% % 	\label{FIG:1}
% % \end{figure}




% % \begin{table}[ht]
% % \centering
% % \caption{Real data information.}
% % \resizebox{\columnwidth}{!}{%
% % \begin{tabular}{@{}llllll@{}}3
% % \toprule
% % & \textbf{Douban} & \textbf{Flickr-lastfm} & \textbf{Flickr-myspace} & \textbf{fb-tw} & \textbf{fq-tw} \\
% %  \midrule
% %  \textbf{Number of nodes} & 3906-1118 & 12974-15436 & 6714-10733 & 17359-20024 & 17355-20417 \\
% %  \textbf{Number of edges} & 8164-1511 & 16149-16319 & 7333-11081 & 112381-114999 & 132208-236882 \\
% %  \textbf{Average degree} & 4.18-2.70 & 2.49-2.11 & 2.18-2.06 & 12.9479-11.4861 & 15.24-23.20 \\
% %  \bottomrule
% % \end{tabular}
% % }
% % \label{tbl:realdata_info}
% \end{table}

\subsection{KBQA Approaches}
\label{sec:techniques}

In recent years, the number of off-the-shelf methods for KBQA in a variety of applications has increased. It is fascinating to compare and evaluate them so that users can make informed choices.
KBQA can be divided into four primary strategies: (i) \emph{embedding-based} — converts a query into a logic form, which is then executed against KBs to discover the appropriate responses; (ii) \emph{subgraph matching} — constructs the query subgraph using a semantic tree; (iii) \emph{template-based} — converts user utterances into structured questions through the use of semantic parsing; and (iv) \emph{context-based}. This will be followed by a discussion of the concept underlying these approaches and some representative systems.

\subsubsection{Embedding methods}
\label{sec:embedding}
% \todo{TODO: briefly discuss the general idea of relation-based techniques. E.g. use the capability of relational database management systems to answer SQL queries. So they try to transform natural language questions to SQL queries ...}
% Semantic parsing takes advantage of semantic and syntactic information included in a question as well as a database schema to produce a SQL logic form (parsing tree), which can then be readily translated into the matching executable query.
% It is a sub-task of semantic parsing that attempts to translate a natural language text into a formal semantic representation such as SQL queries, logic forms, and code creation. It is a popular method of generating SQL from questions in the literature to use a SQL structure-based sketch with many slots and frame the issue as a slot filling job by including some kind of pointing/copying mechanism.
% Semantic parsing is implemented in a variety of methods\cite{finegan-dollak-etal-2018-improving, yu-etal-2018-spider}
Embedding techniques take advantages of semantic and syntactic information included in a question as well as a database schema to produce a SQL logic form (parsing tree).
% A sub-task of semantic parsing attempts to translate a natural language text into a formal semantic representation, such as SQL queries, logic forms, and code creation.
% A popular method of generating SQL from questions in the literature is to use a SQL structure-based sketch with many slots and frame the issue as a slot filling task by including some kind of pointing/copying mechanism. 
% Question-to-SQL is implemented using a variety of methods \cite{finegan-dollak-etal-2018-improving}
% % ,yu-etal-2018-spider}

\sstitle{TREQS} 
The underlying concept of Translate-Edit Model for Question-to-SQL (TREQS) \cite{wang2020text} is to convert healthcare-related questions posed by physicians into database queries, which are then used to obtain the response from patient medical records. Given that questions may be linked to a single table or to many tables, and that keywords in the questions may not be correct owing to the fact that the questions are written in healthcare language, using the language generation method,
% \cite{zhang-etal-2019-editing}
 this model is able to address the problems of general-purpose applications. Using a language generation model, this translate-edit model produces a query draft, which is subsequently edited in accordance with the table schema.
% The components in the TREQS \cite{wang2020text} generation include:
% %\begin{compactitem}
% (1) \emph{ Sequence-to-Sequence Framework}: For Question-to-SQL generation task, as part of the TREQS framework, a question encoder (single-layer bidirectional LSTM) is combined with an SQL decoder. The input of encoder is word embeddings of input tokens. Then they are converted to a sequence of encoder hidden state~.
% % \cite{cirstea2021enhancenet}.
% % $h^e = (h_1^e, h_2^e,...h_J^e)$ with $e$ indicating that the hidden states are obtained from the encoder and the concatenation of the hidden states of forward and backward LSTM~\cite{cirstea2021enhancenet} is $ h_j^e = \overrightarrow{h_J^e} \oplus \overleftarrow{h}_{J-j+1}^e$. 
%  During each decoding step, the decoder uses the encoder hidden states and the word embedding of the preceding token as input and produces a decoder hidden state. 
% % The encoder is equipped with word embeddings and decoder from the same $W_{emb}$ matrix. 
% % The hidden and cell states of the decoder LSTM are initialized as:
% %\begin{equation}
% %    h_0^d = tanh \Big(W_{e2dh}(\overrightarrow{h_J^e} \oplus \overleftarrow{h_1^e}) + b_{e2dh} \Big) 
% %\end{equation}
% %\begin{equation}
% %c_0^d = tanh \Big(W_{e2dc}(\overrightarrow{h_J^e} \oplus \overleftarrow{c_1^e}) + c_{e2dh} \Big)
% %\end{equation}
% %$W_{e2dh}$, $W_{e2dc}$ are the weighted matrices, $b_{e2dh}$, $b_{e2dc}$ are vectors and both of them are learnable parameters.
%  (2) \emph{Temporal Attention on Question:}
% At every phase of decoding the decoder does not only accept its hidden inner state and token as input but also concentrates on the important elements of the problem for the current generation.
% %The temporal attention strategy \cite{nallapati-etal-2016-abstractive} demonstrated the effectiveness of solving the problem as described above and can better prevent repeated attendance on the same section of the question than the standard attention model \cite{luong-etal-2015-effective}. To accomplish this purpose, the model first creates a function to align the current hidden state of the decoder with each hidden state of the encoder: 
% %\begin{equation}
% %    s_{tj}^e = (h_j^e)^\intercal W_{align}h_t^d
% %\end{equation} 
% %with $W_{align}$ as parameters. To avoid repetitive attention, in the preceding decoding steps, the model penalize the tokens that have acquired high attention scores with the following normalization rule: 
% % \begin{equation}
% % s_{tj}^{temp} =
% %  \begin{cases}
% %    exp(s_{tj}^e)       , \text{if } t =1 
% %    
% %    \\
% %    \frac{exp(s_{tj}^e) }{\sum_{k=1}^{t-1} exp(s_{kj}^e) },  \text{if } t > 1
% %  \end{cases}, 
% %  \alpha_{tj} =\frac{s_{tj}^{temp}}{\sum_{k=1}^J s_{tk}^{temp}}
% % \end{equation}
% %
% %where $s_{tj}^{temp}$ represents the new alignment score with temporal dependence, and $a_{tj}$ represents the attention weight at the current decoding step. 
% Lastly, using the temporal attention mechanism, the model produces a context vector for the input question.
% %\begin{equation}
% %    z_t^e = \sum_{j=1}^J \alpha_{tj}h_e^j.
% % \end{equation}
% (3) \emph{Dynamic Attention on SQL:}
% In order to perform well on this task, this model incorporates a dynamic attention mechanism to the decoder
% % \cite{shi-etal-2019-leafnats}
% , which allows it to dynamically attend on the previously created tokens. This is necessary because when generating the condition values, it is possible that the decoder will need to take into consideration not only the previously generated token, its own hidden states, and encoder context vector, but also additional factors such as the encoder.
%For encoders with $t>1$, the alignment scores (denoted by $s_{t\tau}^d$, with $\tau \in \{1,..., t-1\}$) on previously produced tokens may be computed in the same way as the alignment scores for encoders with $t>1$. The attention weight for each token is calculated as
%\begin{equation}
%\alpha_{t\tau}^d = \frac{exp(s_{t\tau}^d)}{\Sigma_{i=1}^{t-1} exp(s_{ti}^d)}
%\end{equation}
%and the formula of decoder-side context vector is:
%\begin{equation}
%z_t^d = \Sigma_{\tau=1}^{t-1} = \alpha_{t\tau}^d h_{\tau}^d
%\end{equation}
% (4) \emph{Controlled Generation and Copying:} 
%  The aim of this task is tackling these problems with the MIMIC-SQL dataset,  as follows. The first problem is SQL queries have strict templates. The second problem is that table headers in queries are aggregated and condition columns, which normally don’t show precisely in questions. The final  problem is because some terms in the question are out of vocabulary (OOV), so the precondition values cannot be ideally obtained through questions.
%With the aim of solving the above problems, for the purpose of task token creation, the decoder integrates the generation network with the pointer network. The reason for this is because the pointer network has the capability of copying OOV tokens from the source and context sequences to the destination sequences. When using OOV words in the TREQS model, the main job is to generate the words in the vocabulary and set placeholders for them ([PH]). Basically, generating condition values in SQL queries is the unique used task. At step t, with the target of generating a token, the first job is computing the probability distribution on a vocabulary $\mathcal{V}$ as follows:
%\begin{align}
%    \tilde{h}_{t}^d =  W_z \big(z_t^e \oplus z_t^d \oplus h_t^d \big) + b_z \\
%     P_{\mathcal{V}, t} = softmax  \bigg(W_{emb}(W_{d2v}\tilde{h}_{t}^d + b_{d2v}) \bigg)
%\end{align}
%After that, the probability of generating a token $y_t$ when conjunction with the pointer is determined by:
%\begin{equation}
%    P(y_t) = \mathrm{p}_{gen,t}P_{gen}(y_t) + (1 - \mathrm{p}_{gen,t})P_{ptr}(y_t)
%\end{equation}
%with $\mathrm{p}_{gen,t}$ is probability of using a generation network for token generation, and
%in this formula, $P_{gen}(y_t)$ is the probability of the network generation and $P_{ptr}(y_t)$ is the probability of pointer network, which are established as:
%\begin{equation}
%    P_{gen} (y_t) =
%    \begin{cases}
%            P_{\mathcal{V}, t}(y_{t}) & y_t \in \mathcal{V} 
%            \\ 
%            0 & otherwise
%    \end{cases} 
%\end{equation}
%,
%and the probability of pointer network $P_{ptr}(y_t)$ is established as follows:
%\begin{equation}
%   P_{ptr}(y_t) = 
%    \begin{cases}
%            \Sigma_{j:x_j = y_{t}} \alpha_{tj}^e & y_{t} \in \mathcal{X} \cap \mathcal{V} 
%            \\ 
%            0 & otherwise
%        \end{cases} 
%\end{equation}
%
%where $\mathcal{X}$ is the set of all tokens in a question.
%Additionally, $\mathrm{p}_{gen,t}$ is calculated by 
%\begin{equation}
%    \mathrm{p}_{gen,t} = \sigma (W_{gen}z_e^t \oplus h_d^t \oplus E_{y_{t-1}} + b_{gen} )
%\end{equation}
%% with $E_{y_{t-1}}$ is the word embedding of the previous token $y_{t-1}$. On this model, with question has OOV words, each OOV words will be replaced with [PH]. Vocabulary is the union of vocabulary of regular token \mathcal{X} and vocabulary of 
%In terms of the loss function, this model employs cross-entropy loss with the goal of increasing the log-likelihood of observed sequences to the greatest extent possible (ground-truth):
%\begin{equation}
% \mathcal{L} = - log P_\theta ( \hat{y} \vert x) = \Sigma_{t-1}^T \log P_{\theta} (\hat{y}_t \vert \hat{y}_{<t}, x) 
%\end{equation}
%where $\hat{y}$ is a ground-truth SQL sequence in the training data and $\theta$ is weight matrices W and biases b.
% (5) \emph{Placeholder Replacement:} During this task, once a query has been generated, this model replaces the position of each [PH] with a token in the original question. 
% \item The replacement probability for a [PH] at a given time step t is calculated as:
% \begin{equation}
%        P_{rps}(y_{t'}) = 
%        \begin{cases}
%            \Sigma_{j:x_j = y_{t'}} \alpha_{t'j}^e & y_{t'} \in \mathcal{X} - \mathcal{V} 
%            \\ 
%            0 & otherwise
%        \end{cases} 
%\end{equation}
%
%A mask is applied on the attention weights, which may make use of the semantic link between previously created words and their nearby OOV words. The possible case is if at the step $t-1$, the model attends word $x_i$, it will have a high likelihood of attending the nearby words of $x_i$ at step t. Any attention-based Seq2Seq model may be employed by this meta-algorithm.

%\end{compactitem}

% \subsubsection{Sequence-to-Sequence (Seq2Seq) model}
% Seq2Seq model is used as baseline model of \cite{wang2020text} and \cite{park2020knowledge} 
% LSTM encoders and decoders are bidirectional in this architecture. The "generic" global attention mechanism proposed in \cite{luong-etal-2015-effective} is used for this work. 
% The placeholder replacement technique is used to address the OOV words issue as part of the query generation process for this model, which is part of the query generation process.
% % \subsubsection{Q-L Mapping}
% % Q-L and Q-A are stand for Question to Logical form and Question to Answer, respectively. These models listed below are baseline model of emrQA dataset \cite{pampari-etal-2018-emrqa}. First we will descbribe about Q-L Mapping  
% % \begin{table}[H]
% % \centering
% % \begin{tabular}{ll}
% % \hline
% % \multicolumn{1}{|l|}{Q} & \multicolumn{1}{l|}{What is the \textbf{dosage}  of $\mid$ \textit{medication} $\mid$ ?}  \\ 
% % \hline
% % \multicolumn{1}{|l|}{LF} & \multicolumn{1}{l|}{MedicationEvent ($\mid$ \textit{medication} $\mid$) {[} \textbf{dosage=x}{]}}  \\ 
% % \hline
% % \end{tabular}
% % \caption{Example of Question (Q) and Logical Form (LF) from emrQA dataset}
% % \label{QLF}
% % \end{table}
% % \begin{itemize}
% %     \item \textbf{Heuristic Models} 
% % Template-matching approach is used for this task. Firstly, data is split into train and test sets, and next replacing entities with placeholders is used for normalizing questions in the test sets. 
% % Then, based on the ground truth templates of the questions in the train set, the templates are scored to identify the best match. 
% %  At next step, to produce the projected Logical Form (LF), the placeholders in the LF template corresponding to the best matched question template are filled with the normalized entities. 
% %  Finally, CLiNER  \cite{Boag2015CliNERA} is used for normalizing the test questions. Heuristic (HM) is used for scoring and matching task. HM-1 computes an identical match and HM-2 generates sentence2vec to generate a Glove  \cite{arora2016simple} vector representation of the templates and afterward, computes pairwise cosine similarity.

 
 


% %     \item \textbf{Neural Model} sequence-to-sequence \cite{sutskever2014sequence} (seq2seq)  with attention paradigm (\cite{bahdanau2014neural}, \cite{luong-etal-2015-effective}) is used for this task. 
% % \end{itemize}


% % \subsubsection{Q-A Mapping}
% % On the Q-A task, emrQA divides into 2 subtasks. 
% % First one is extraction of answer line from
% % the clinical note \big(machine comprehension (MC)\big) and second one is prediction of answer class based on the entire clinical note.
% % \begin{itemize}
% %     \item \textbf{Machine Comprehension} DrQA\cite{chen-etal-2017-reading} is baseline model used for first task. DrQA is MRC (Machine Reading Comprehension) system applied to open-domain QA under development of Facebook. Generally, DrQA includes 2 components: Document Reader and Document Retriever. With \textit{Document Retriever} , DrQA uses non-machine learning method TF-IDF to look up revelant article. The output of this part is 5 Wikipedia articles with 1 question and those will be the input of \textit{Document Reader}. The next part is \textit{Document Reader} which is inspired by Attentive Reader 
% %     (\cite{chen2016thorough} \cite{hermann2015teaching}) comprises of 3 components: passage encoding, question encoding and prediction. Description of each one will be listed below. The fist part is question encoding, the purpose of this task is put all tokens $t_i$ in the paragraph p as a sequence $S_i \in \mathbb{R}^d$ into recurrent neural network (RNN) as the input. The form of this is
% %     \[
% %         \{t_1,...,t_n\} = RNN (\{S_1,...,S_n\})
% %     \]
% %     About question encoding, this model uses different RNN on the top position of word embedding of question $q_i$ and after that, combine them with  the resulting hidden units into one single vector: $\{q_1,...q_n\} \rightarrow q$. The formula or $q$ is 
% %     \[
% %             q= \Sigma_j b_j q_j
% %     \]
% %     with mission $b_j$ is encoding the importance of each question word
% %     \[
% %             b_j =  \frac{(w. q_j)}{\Sigma_{j'}exp(w.q_{j'})}
% %     \]
% %     And the final component is prediction. The objective of this task is predict the span containing the answer as output of this model. The input of this phase is sequences of all tokens in the paragraph p $\{t_1,...,t_n\}$ and question vector q that are described above and specific output is predicting two ends of the span. The formula of specific output is 
% %     \[
% %             P_{start}(i) \propto exp (t_i W_s q)
% %     \]
% %     \[
% %               P_{end}(i) \propto exp (t_i W_e q)
% %     \]
% %     with the position of $i$ and $i'$ is detect on $i \leq i' \leq i+15$ and $P_{start}(i) \times P_{end}(i)$
% %   % Viet toi day 6/6  
% %     \item \textbf{Class Prediction} The second task is done with multi-class logistic regression model.
% % \end{itemize}


\sstitle{TREQS++}
TREQS++ or TREQS+Recover is the additional method with the added step as described below.
\emph{Recover Condition Values with Table Content:} This step is necessary because the use of the translate-edit model may not provide complete confidence that all of the queries will be executed since the condition values may not be precise in certain cases. With the goal of retrieving the precise condition values from the projected ones, this model employs a condition value recovery method in order to address this issue.


% In summary, methods in this paradigm operate by converting natural language inquiries into logic forms \cite{berant-liang-2014-semantic} that can describe the semantics of the whole query. Next, the results of the parsing are used to create structured queries (for example, SPARQL) that are used to search knowledge bases and get the responses. To generate semantic representations, \cite{10.1007/978-3-319-19581-0_8} use a combinatory categorical grammar with handmade lexical items and lambda-type calculus expressions, as well as a combinatorial categorical grammar with handcrafted lexical items. As a result, the input utterances must be consistent with the grammatical rules.
% \cite{10.1007/978-3-319-34129-3_19} makes use of a logical model. Users can submit English-language questions to a target RDF knowledge base. They are first normalized into a canonical intermediate syntax known as normalized query structure, and then translated into SPARQL queries. 
% \cite{yih-etal-2016-value} demonstrates that learning from labelled semantic parsers results in substantial improvements in overall performance. 
% \cite{hamon2017querying} answer questions based on linked biological data utilizing a four-step procedure that starts with the linguistic and semantic annotation of the input question.
% The concept of developing a generic (pipeline) architecture for QA on linked data in order to foster developer collaboration was pioneered by \cite{10.1007/978-3-319-64468-4_2}, which previously combined building blocks in tailored systems, allowing for a semantic description of both quality assurance components and requirements.
% \cite{singh2018frankenstein} is a platform that embraces component reuse by aggregating numerous key components for the purpose of solving QA chores and enabling the building of diverse QA pipelines.


\subsubsection{Subgraph matching methods}

Subgraph matching constructs the query subgraph using a semantic tree, while others deviate significantly from this approach by building the subgraph from the entity.
A natural language phrase may have many interpretations, each of which corresponds to a different set of semantic elements in the knowledge graph. 
After the semantic tree is located, the semantic relationships must be extracted from it before the semantic query graph is constructed. 
% Several techniques \cite{maheshwari2019learning} use this approach.
% 8721057
\begin{figure}[!h]
\vspace{-2em}
    \centering
    \includegraphics[width=0.75\linewidth]{gAnswer.pdf}
    \caption{Overview of gAnswer model } 
    \label{fig:gAnswer}
\vspace{-2em}
\end{figure}

\sstitle{gAnswer}
gAnswer \cite{hu2017answering} is the most advanced subgraph matching method to convert natural language questions into query graphs that include semantic information. This model responds to natural language queries using a graph data-driven, offline and online solution. Using a graph mining technique, the semantic equivalence of relation terms and predicates is determined during the online phase. The discovered semantic equivalence is then included into a vocabulary of paraphrased phrases. The online part includes the question comprehension and assessment stages. During the question comprehension phase, a semantic query graph is constructed to capture the user's purpose by extracting semantic relations from the dependency tree of the natural language question using the previously constructed paraphrase dictionary. Then, a subgraph of the knowledge graph is chosen that fits the semantic query graph through subgraph isomorphism. The final result is determined by the chosen subgraph during the query assessment phase.
% \subsubsection{WDAqua-core0}

% %Da paraphase nhung chua check
% WDAqua \cite{diefenbach2017wdaqua} is a QA system that can answer questions posed across DBpedia and Wikidata using both complete natural language queries and keyword-based queries. Aside from that, WDAqua is available in four other languages in addition to Wikidata. These languages are: English, French, German, and Italian. WDAqua employs a rule-based combinatorial method to build SPARQL queries that are based on the semantics contained in the underlying knowledge base, rather than a traditional approach. As a consequence, WDAqua does not convert natural language inquiries into SPARQL queries using a machine learning technique. The result is that WDAqua does not have over-fitting issues. Although the coverage and variety of the produced SPARQL searches are more than they were before, this is owing to the limits of the human-defined transformation rules. For example, the SPARQL queries that are produced include a maximum of two triple patterns. Additionally, the resulting queries include just the 'COUNT' operator as modifiers. Adding a new operator to the produced queries would entail considerable effort in terms of creating the transformation procedures. Rather than that, just gathering new question-answer pairs would do for machine learning-based systems.


% % \subsection{Knowledge base as auxiliary information}
\subsubsection{Template-based methods}
The usage of templates is critical in question answering (QA) over knowledge graphs (KGs), where user utterances are converted into structured questions via the use of semantic parsing
% ~\cite{10.1145/3038912.3052583}
. Using templates has the advantage of being traceable, and this may be used to create explanations for the users, so that they can understand why they get certain responses. 
%These systems \cite{10.1145/3038912.3052583,10.1145/3178876.3186004} use this approach.
\begin{figure}[!h]
\vspace{-1em}
    \centering
    \includegraphics[width=0.8\linewidth]{TeBaQA.pdf}
    \caption{Overview of TeBaQA model } %\protect\footnotemark  (Icon made by ultimatearm from www.flaticon.com) 
    \label{fig:tebaqa}
    \vspace{-2em}
\end{figure}

\sstitle{TeBaQA}
TeBaQA~\cite{vollmers2021knowledge} is the most advanced template-based technique.
First, all questions undergo (1) \emph{ Preprocessing} to eliminate semantically unnecessary terms and provide a meaningful collection of n-grams. By evaluating the underlying graph structure for graph isomorphisms, (2)\emph{the Graph-Isomorphism Detection and Template Classification} phase trains a classifier based on a natural language question and a SPARQL query using the training sets. The key assumption is that structurally comparable SPARQL searches correspond to questions with similar syntax. A query is categorised into a sorted list of SPARQL templates at runtime. During (3) \emph{ Information Extraction}, TeBaQA collects all relevant information from the question, such as entities, relations, and classes, and identifies the response type according on a set of KG-independent indexes. The retrieved information is entered into the top templates, the SPARQL query type is selected, and query modifiers are applied during the (4) \emph{ Query Building} step. The conducted SPARQL queries are compared to the predicted response type. The following (5) \emph{Ranking} is determined by a mix of all facts, the natural language inquiry, and the returning responses.

\subsubsection{Context-based methods}
\label{sec:IRTechnique}
These methods attempt to comprehend questions from various perspectives, such as question analysis, classification of questions, answer path, context of answers, and type of answers. 
%In \autoref{sec:background}, we have already introduced about this techniques.

\sstitle{QAsparql}
\label{sec:QAsparqlMethod}
QAsparql \cite{liang2021querying} is a model using five steps as listed below, to translate questions to SPARQL queries. \autoref{fig:qasparql} shows an overview of QAsparql model. 

\begin{figure}[!h]
\vspace{-1em}
    \centering
    \includegraphics[width=0.85\linewidth]{QARSQL.pdf}
    \caption{Overview of QAsparql model } %\protect\footnotemark  (Icon made by ultimatearm from www.flaticon.com) 
    \label{fig:qasparql}
\vspace{-2em}
\end{figure}
%\footnotetext{Neural network picture in \autoref{fig:qasparql} - phase 5 is from https://github.com/martisak/dotnets}


%   The first component of the QAsparql system is (1)\emph{Question Analysis}. Specifically, the syntactic characteristics of each question component are used to perform the following tasks: tokenizing the question, locating the appropriate part of speech tags for each of these tokens, recognising named entities, identifying relationships between tokens, and calculating the dependency label for each question component. Questions are lemmatized in order to reduce the inflectional forms of a word to their common base form. Additionally, dependency parsing is generated for query ranking and question classification.
% (2) \emph{Question type classification} constitutes the second component of the QAsparql system. In this task, this model categorises questions into List, Count, and Boolean categories.
%     % Some characteristics of each type of questions are listed below. The first one is List, with the beginning of Wh-questions or verb such as list and show and the answer usually is the list of resources. The next one is Count with the appearance with the word COUNT in SPARQL query and usually begins with How-questions and the answer of this type is number, sometimes is List due to the cases. The last one is Boolean with the exist of keyword ASK in SPARQL query with regular cases are YES/NO questions and the answers of this type are True or False. 
%     The approach of this part is firstly, the questions are lemmatized and next, TF-IDF
%     technique is used for word embedding task. 
% %    The formula of TF-IDF is $\mathit{tfidf (t,d,C) = tf(t,d) \times idf(t,C)}$, where tf is term frequency - number of term occurrences in a document, i.e. $tf(t,d) = \frac{f_{t,d}}{\Sigma_{t' \in d} f_{t', d} }$. In that, $f_{t,d}$ is number of times term t appears in a document d and $\Sigma_{t' \in d} f_{t', d}$ is total number of terms t in the document $d$. And $idf (t,C)$ is inverse document frequency - how much information the term provides in corpus C is calculated  as  $idf(t,C) = \log \frac{\vert C \vert}{\vert C'_t \vert}$ where $|C|$ is the number of documents in the corpus and $\vert C'_t \vert$ is $ \{ d \in C: t \in d \} $ the number of documents containing term $t$.
%     Finally, Random Forrest 
%     % \cite{breiman2001random} 
%     model is used for questions  classified into three types - List, Count and Boolean and SPARQL query.
% %    , constructed based on the result of question classification.
%     The main purpose of (3) \emph{Phrase mapping} is to build the final queries using information based on knowledge graph. The information required to help SPARQL queries construction is divided into three types: resources, properties, and classes. Resources are literal and physical or abstract entities represented by any Internationalized Resource Identifier (IRI). Properties are a kind of resources used for illustrating attributes or relationships of other resources. Finally is classes, classes are also kind of resources and are recognized by IRIs and may have associated with properties.
%     % For example, the IRI of city  'Ho Chi Minh' is \url{https://dbpedia.org/page/Ho_Chi_Minh_City} and string literal "VND" is currency code of VN in DBPedia.  An example of this type is
%     % \url{https://dbpedia.org/property/currencyCode}, which is the currency code of country.  For instance, \url{https://dbpedia.org/page/Vietnam} is a subclass of \url{https://dbpedia.org/ontology/country}
%     The main task of (4) \emph{Query generation} is deciding the SELECT clause and WHERE clause in an SPARQL query. Moreover, the purpose of query generation is constructing the set of triples of the graph pattern in the form <subject, predicate, object> triples and this is the basis for  organizing an SPARQL query. At the Phrase mapping step, we also have the mapped resources, properties and classes which are used to generate these triples. Afterward, WHERE clause is established.   
%     (5) \emph{Query ranking} is the final step.
%     In (4), this model creates many candidate queries for each question. Therefore, in this step, this model will rank these queries and select which are the most suitable queries. The approach of this model is based on Tree-structured Long-Short Term Memory (Tree-LSTM). This model uses Tree-LSTM with the goal of mapping question and candidate queries to the sequences vector and compute the similarity of vectors. The output is the choice of candidate queries which are the highest similarity with input question. 
    
%     % \cite{tai-etal-2015-improved}.
%     % which is based on LSTM architecture
%     % % \cite{Hochreiter1997} 
%     % and a new version \cite{zaremba2014learning}.
% %   On Tree-LSTM, the authors propose two extensions of LSTM - Child-Sum Tree-LSTM and the N-ary Tree-LSTM.
% %Tree-LSTM units (indexed by j) are similar to conventional LSTM units in that they include an input gate ($i_j$), an output gate ($o_j$), an input and output cell ($c_j$), as well as an input and output hidden state ($h_j$). Unlike the conventional LSTM unit, Tree-LSTM units' gating vectors and memory cell updates are reliant on the states of many child units, which is a significant advantage over conventional LSTM units. Furthermore, instead of a single forget gate, the Tree-LSTM unit includes one forget gate $f_{jk}$ for each child $k$, resulting in a total of four forget gates. Because of this, the Tree-LSTM unit is able to integrate information from each child in a targeted manner. 
% %Let $c_j$ represent the set of children of node j in a given tree; the set of children of node j is denoted by Child-Sum Tree LSTM transition equations consist of the following terms:
% %    \begin{align}
% %        \tilde{h}_j &= \sum_{k \in C(j)} h_k,\\
% %        i_j &= \sigma(W_i x_{j} + U_i \tilde{h}_{j} + b_i)\\
% %        f_{jk} &= \sigma(W_f x_{j} + U_f h_{k} + b_f)\\
% %        o_j &= \sigma(W_o x_{j} + U_o \tilde{h}_{j} + b_o)\\
% %        u_j &= \tanh{(W_u x_{j} + U_u \tilde{h}_{j} + b_u)} \\
% %        c_j &= i_j \odot u_j + \sum_{k \in C(j)} f_{jk} \odot c_k\\
% %        h_j &= o_j \odot \tanh{(c_j)}
% %    \end{align}
% %    with a branching factor of at most N and children that are ordered, i.e., can be indexed from 1 to N, the N-ary Tree-LSTM may be used to find the shortest path between them. The hidden state and memory cell of each node j's kth child should be written as $h_{jk}$ and $c_{jk}$ for that node. The N-ary Tree-LSTM terms are:
% %    \begin{align}
% %        i_j &= \sigma \Big(W_i x_{i} + \sum_{l=1}^N  U_{li} {h}_{jl} + b_i \Big)\\
% %        f_{jk} &= \sigma \Big(W_f x_{j} + \sum_{l=1}^N  U_{fkl} {h}_{jl} + b_f \Big)\\
% %        o_j &= \sigma \Big(W_o x_{j} + \sum_{l=1}^N  U_{lo} {h}_{jl} + b_o \Big)\\
% %        u_j &= \tanh{\Big(W_u x_{j} +   \sum_{l=1}^N  U_{lu} {h}_{jl} + b_u \Big)} \\
% %        c_j &= i_j \odot u_j + \sum_{l=1}^N f_{jl} \odot c_{jl}\\
% %        h_j &= o_j \odot \tanh{(c_j)}
% %    \end{align}


% % \subsubsection{QAmp}
% % QAmp \cite{10.1145/3357384.3358026}, which is a KGQA approach, consists of two steps: (1) interpretation of the question and (2) reasoning of answers. In the interpretation phase of the question, a set of entities and predicates that appear to be relevant to the answer to the entered question are identified with their corresponding confidence scores.In the second step, these confidence scores are propagated and integrated directly into the general structure of the KG, resulting in a distribution of confidence across the whole set of potential responses.

% % In the first step, to generate the question model q, this model follows two steps: Parse (1) and Match (2). Firstly, extract references (entities, predicates and class references) in natural language questions and analyze to identify the type of question; and (2) KG candidate entities, predicates, and (2) each of the extracted references. Matched to assign to the class ranking list.

% % It also describes how to analyze the input question Q and generate the question model q. Then match the reference to q with the entity and predicate in graph K.
% % In the Parse step, the target of this task is to classify the type of question $T$ and parse it into a set of sequences $S$. Questions are classified into preset categories using a map classification model that has been trained on a dataset of annotated question type sets. The map classification model learns how to classify incoming questions into any of the established types.
% % The next Step is Matching. Use an explanatory problem model to update the problem model, where each component of $S$ is represented by a reference to a specific term in K (via their URI) as follows: For each entity reference (or attribute, class, response) in $S$, the model Retrieve the ranking list of the most similar entities in KG and the corresponding confidence scores.

% % In the second step of QAmp is reasoning answer. In this step, this model divides into 2 sub-step is : (1) subgraph extraction and (2) message passing.
% % This phase refers to the process of obtaining the necessary triples from the KG that are used to construct the subgraph. Thus, the URI of the entity and predicate matching the query is used as a seed for obtaining a triple of KG that contains a single entity (subject or location of an object) and a single condition from its reference set. So the extracted subgraph contains n elements. This includes all entities in $E_i$ and those adjacent to them via the property $P_i$. 
% % The second phase of the answer inference phase involves message propagation, that is, the propagation of confidence scores to adjacent entities in the subgraph extracted from the entity $E_i$ matching the question in the interpretation phase and the predicate $P_i$.
% % % In these equations, $\mathrm{sigm}$ and $\tanh$ are applied
% % % element-wise. Figure \ref{fig:lstm} illustrates the LSTM
% % % equations.
% % % Chua paraphase
% % % \begin{algorithm}
% % % \caption{~~Long Short Term Memory feed forward}
% % % \label{alg:lstm}
% % % \begin{algorithmic}[1]
% % % \State $i_t = \sigma(W_{x\rightarrow i}x_t + W_{h\rightarrow i}h_{t-1} + W_{c \rightarrow i}c_{t-1} + b_{1 \rightarrow i})$
% % % \State $f_t = \sigma(W_{x\rightarrow f}x_t + W_{h\rightarrow f}h_{t-1} + W_{c\rightarrow f}c_{t-1} + b_{1\rightarrow f})$
% % % \State $z_t = \tanh(W_{x\rightarrow c}x_t + W_{h\rightarrow c}h_{t−1} + b_{1\rightarrow c})$
% % % \State $c_t = f_t c_{t-1} + i_t z_t$
% % % \State $o_t = \sigma(W_{x\rightarrow o}x_t + W_{h\rightarrow o}h_{t−1} + W_{c\rightarrow o}c_t + b_{1\rightarrow o})$
% % % \State $h_t = o_t\tanh(c_t)$
% % % \end{algorithmic}
% % % \end{algorithm}



% % % The difference between the standard
% % % LSTM unit and Tree-LSTM units is that gating
% % % vectors and memory cell updates are dependent
% % % on the states of possibly many child units. Additionally, instead of a single forget gate, the TreeLSTM unit contains one forget gate fjk for each
% % % child k. This allows the Tree-LSTM unit to selectively incorporate information from each child.
% % % For example, a Tree-LSTM model can learn to emphasize semantic heads in a semantic relatednes


At each stage, each individual software component separately solves a related job. First, the question analysis component processes the incoming question based only on syntactic characteristics. The question's phrases are then mapped to relevant resources and attributes in the underlying RDF knowledge network once the question's type has been determined. On the basis of the mapped resources and attributes, many SPARQL queries are built. A ranking model based on Tree-structured Long Short-Term Memory (Tree-LSTM) is used to order potential questions based on the closeness of their syntactic and semantic structure to the input query. Finally, results are given to the user by running the produced query against the knowledge network underpinning the system.

\sstitle{QAsparql* (Our proposal)}
While carrying out a resource and property mapping task (Phase 3 as described in \autoref{sec:QAsparqlMethod}) utilizing EARL~\cite{dubey2018earl}, we find that there is a significant difference between the two methods, as described in more detail below. In empirical evaluation, we have discovered that new algorithm produces superior results to previous one.
% In the \autoref{sec:exp} we will show the effectiveness of QAsparql (with default) and QAsparql* (use new algorithm). 
% The algorithms for the two approaches are given below.

% \begin{figure}[!h]
% \vspace{-3em}
% \centering
%   \begin{minipage}[t]{0.8\linewidth}
%     \removelatexerror
% \begin{algorithm}[H]
% \scriptsize
% \caption{Force gold 1}
% \label{alg:fg1}
% %\begin{flushleft}
% \textbf{Input:} Golden List of Entity $\mathcal{G}$, List of Entity $\mathcal{E}$, \\
% \textbf{Output:} List of Intersect entities $\mathcal{I}$, which are List of Entity $\mathcal{E}$ appears in Golden List $\mathcal{G}$
% %\end{flushleft}
% \begin{algorithmic}
% \Procedure {Force gold 1}{}
% \State $\mathcal{I} \gets \emptyset $;
% $I_i \gets$  $Item \in \mathcal{E}$;
% $\mathcal{U} \gets$  List of $ Uri \subseteq \mathcal{E} $;
% $U_i \gets$  $Uri \in \mathcal{U}$;
% \State $G_i \gets$  Golden item  $ \in \mathcal{G}$;
% $\mathcal{U_G} \gets$ List of $ Uri \subseteq \mathcal{G_i} $;
% $U_{Gi} \gets$  $Uri \in \mathcal{U_G}$;
% \For{$I_i \in \mathcal{E}$}
% \For{$U_i \in \mathcal{U}$}
% \For{$G_i \in \mathcal{G}$}
%     \If{$U_{Gi} \in \mathcal{U_G}$}
%         { $\mathcal{I} \gets \mathcal{I} \cup $ ($G_i$)}
%     \EndIf
    
% \EndFor
% \EndFor
% \EndFor
% \Return $\mathcal{I}$
% \EndProcedure
% \end{algorithmic}
% \end{algorithm}
% \end{minipage}
% \vspace{-2em}
% \end{figure}
These methods are utilised to determine the number of hops and connections for each candidate node. This information is then sent to a classifier, which ranks and scores the features. After the creation of three distinct types of objects (Entity, Golden item, and the URI of each item), the next step is to convert them to a list. Consequently, we now have three separate listings. The List of Entities,  Golden Item, and URI are examples of lists. The primary purpose of this method is to identify which entity URI corresponds to the golden item URI.
The new algorithm performs the same function as the old algorithm, but with a different approach. During the first phase, it determines which items in the list correspond to the golden item and moves those that do not into a separate list known as the "not found list." It will then conduct a second search for entities, this time in surface form, to ensure that no entity is overlooked. After that, it will delete any non-included items and return a list of the items.

% \begin{figure}[!h]
% \vspace{-3em}
% \centering
%   \begin{minipage}[t]{0.7\linewidth}
%     \removelatexerror
% \begin{algorithm}[H]
% \scriptsize
% \caption{Force gold}
% \label{alg:fg2}
% %\begin{flushleft}
% \textbf{Input:} Golden List of Entity $\mathcal{G}$, List of Entity $\mathcal{E}$, Surface Form $\mathcal{S}$ of Entity $\mathcal{E}$
% \\
% \textbf{Output:} List of items $\mathbb{I}$
% %\end{flushleft}
% \begin{algorithmic}
% \Procedure {Force gold}{}

% \State $NF \gets \emptyset$;
% $\mathbb{I} \gets \emptyset$;
% $I_i \gets Item \in \mathcal{E}$; 
% $G_i \gets $  Golden item  $ \in \mathcal{G}$;
% $S_i \gets $ Surface form $ \in G_i$;
%  $U_i \gets $  Uri   $ \in {G_i}$;
% $UI_i \gets$  Uri   $ \in \mathcal{E}$;
% \For{$G_i \in \mathcal{G}$}
%     \State $idx \gets $ Closest string between Surface form from $S_i$ and List of item $\mathcal{S}$
%     \If{$idx$ $\neq$ -1 } 
%         \If {1st element of $U_i$ $\notin UI_{idx}$}
%             { Element of length of $UI_{idx} -1$
%               $\gets$ 1st element of $U_i$}
%         \EndIf
%         \State $\mathcal{S} \gets \mathcal{S}  \setminus  S_{idx} $
%     \Else{ $NF \gets$ {$NF \cup G_i$}}
%     \EndIf
% \EndFor
% \State $NF_i \gets$  Item   $ \in NF$
% \For{$NF_i \in NF$}
%     \If {length of $\mathcal{S} > 0$}
%         \State $idx \gets $ First key of surfaces; 
%         First element of $UI_{idx}$ $\gets$ First element of $U_i$;
%         $\mathcal{S} \gets \mathcal{S} \setminus S_{idx}$
    
%     \Else{ $\mathcal{E} \gets \mathcal{E} \cup NF_i$}
%     \EndIf
% \EndFor
% \State $keys \gets$ Sorted list of keys of $\mathcal{S}$
% %\State $keys \gets$ Sort all element of keys with descending order
% \For{$idx \in $ keys}
%     {$\mathcal{E} \gets \mathcal{E} \setminus I_{idx}$}
% \EndFor

% \Return $\mathbb{I} \gets \mathcal{E}$
% \EndProcedure
% \end{algorithmic}
% \end{algorithm}
% \end{minipage}
% \vspace{-2em}
% \end{figure}


% In summary, methods in this paradigm attempt to automatically convert natural language questions to structured queries. Then they access the knowledge base to acquire a set of possible answers. Finally, characteristics of the question and candidates are retrieved in order to score them with the goal of determining the correct response. 
 
% \cite{xu-etal-2016-question} offered a neural network-based relation extractor for retrieving candidate answers from Freebase and subsequently inferring these responses from Wikipedia. 
% %To be more exact, the approach entails subdividing the original query using a collection of syntactic patterns.
% \cite{hao-etal-2017-end} proposed a model for dynamically representing questions and their associated scores based on the various potential answer features via the cross-attention method. 
% % Additionally, they make use of the global knowledge contained inside the underlying knowledge base, with the goal of incorporating this information into the depiction of the answers. As a result, it may reduce the out-of-vocabulary issue, allowing the cross attention model to more precisely depict the query.

% % \cite{yao-van-durme-2014-information} extracts linguistic information from a question to create a question feature graph. Then a topic graph which composed with topic nodes and other relative nodes is created in Freebase, with each node representing a potential answer. Finally, feature exacted from the topic graph and candidate answers are merged to determine the correct answer. To extract hand-crafted features for queries, this technique uses dependency parse results.
% \cite{dong-etal-2015-question} proposed a multicolumn convolutional neural network for understanding and learning the distributed representations of questions from three distinct perspectives: answer path, answer context, and answer type. 




\subsection{Summary}

% Finally, to summarise this section, in order to perform a thorough comparison study, we use five baseline systems — TREQS, TREQS+ Recover, Ganswer, TeBaQA, QAsparql — which allow us to evaluate the efficiency of these on both Biomedical and Generic dataset while TREQS is concentration on Biomedical data. \autoref{tab:benchmark} lists the key aspects of each technique that has been implemented, which are as follows
In summary, we use six representative systems — TREQS, TREQS++, gAnswer, TeBaQA, QAsparql and QAsparql* across domains, natural languages, and query languages.
%which allow us to evaluate their efficiency on both Biomedical and Generic dataset while TREQS focuses on Biomedical data.
 \autoref{tab:benchmark} compares the key characteristics of each technique studied in this benchmark.



\begin{table}[!h]
\centering
\vspace{-2em}
\caption{Characteristics comparison between KBQA techniques.}
\label{tab:benchmark}
%\footnotesize
\resizebox{0.85\textwidth}{!}{
\begin{tabular}{lcccccc} 
\toprule
\textbf{Technique} & \textbf{Features} & \textbf{Query language} & \textbf{Paradigm} & \textbf{Number of steps} & \textbf{Domain} & \textbf{Natural language}  \\ 
\midrule
TREQS              & Deep learning     & SQL, SPARQL             & Embedding         & 5                        & Single          & Single                     \\
TREQS++            & Deep learning     & SQL, SPARQL             & Embedding         & 6                        & Single          & Single                     \\
gAnswer            & Hand-crafted      & SPARQL                  & Subgraph matching & 4                        & Multiple        & Multiple                   \\
TeBaQA             & Hand-crafted      & SPARQL                  & Template-based    & 5                        & Multiple        & Multiple                   \\
QAsparql           & Hybrid            & SPARQL                  & Context-based     & 5                        & Multiple        & Multiple                   \\
QAsparql*          & Hybrid            & SPARQL                  & Context-based     & 5                        & Multiple        & Multiple                   \\
\bottomrule
\end{tabular}
}
\vspace{-2em}
\end{table}
\section{Experimental setup}
\label{sec:setup}

%In this section, we first present an overview of datasets and describe analysis and statistic of them. In addition, we provide explanations of the metrics that were used to evaluate the various KBQA methods. Finally, we  discuss the implementation details and reproducibility environment of our benchmark tool in terms of evaluating the outcomes of question answering over knowledge bases.




% \subsection{Knowledge bases}


% % \begin{table}[H]
% % \centering
% % \begin{tabular}{|l|l|l|l|}
% % \hline
% % \textbf{KGs} & \textbf{DBpedia} & \textbf{Freebase} & \textbf{Wikidata} \\ \hline
% % Number of triples & 411 885 960 & 3 124 791 156 & 748 530 833 \\
% % Number of instances & 20 764 283 & 115 880 761 & 142 213 806 \\
% % Number of entities & 4 298 433 & 49 947 799 & 18 697 897 \\
% % Number of classes & 736 & 53 092 & 302 280 \\
% % Number of relations & 2819 & 70 902 & 1874 \\
% % No. of unique predicates & 60 231 & 784 977 & 4839 \\ \hline
% % \end{tabular}
% % \caption{Knowledge graphs comparisons }
% % \label{KG}
% % \end{table}




% % \begin{figure}[H]
% % %    \vspace{-5pt}
% %     \centering
% %     \includegraphics[width=1\linewidth]{KGComparision.pdf}
% %     \caption{KGs Comparision \todo{TODO: these figures are redundant. \autoref{KG} is enough.}}
% %     \label{fig:KGComparision}
% % %    \vspace{-10pt}
% % \end{figure}

% The statistics of KBs are shown in \autoref{KG}, including the following elements:
% \begin{compactitem}
% \item \emph{Triples:}
% All of the KGs under consideration are very big. DBPedia is the smallest KG, whereas Freebase is the biggest KG in terms of the number of triples. 
% % Freebase is the largest KG in terms of triples.
% % It seems that there is a relationship between the method used to build up a KG and its size. For example, automatically generated KGs tend to be bigger since the constraints of integrating new information become less onerous as the KG grows. 
% % A significant effect is made by datasets that have been imported into the knowledge bases, such as MusicBrainz into Freebase. These datasets have a significant impact on the number of triples and facts in the knowledge base. In addition, the way data is modelled has a significant effect on the number of triples. Consider the following example: If n-ary relations are represented in N-Triples format (as in the case of Wikidata), more intermediary nodes must be modeled, which results in many more triples than with simple statements. Last but not least, the number of languages that are supported has an impact on the number of triples.
% \item \emph{Classes:} 
% % The number of classes varies significantly between KGs, ranging from 736 (DBpedia) to 53,500 (Freebase) and 300,000 (Wikidata). Despite its large class catalog, Wikidata has the highest percentage of classes that are actually used when compared to other databases (i.e., classes with at least one instance). This could be because suitable Freebase categories are selected as Wikidata classes using heuristics. On the other hand, Wikidata contains a large number of classes, but only a small fraction of them are used at the instance level. 
% Classes vary greatly between KGs, from 736 (DBpedia) to 53,500 (Freebase) to 300,000. (Wikidata). Wikidata has the highest percentage of actually used classes (i.e., classes with at least one instance). This could be because Wikidata classes are chosen from Freebase categories. Conversely, Wikidata has many classes, but only a few are used at the instance level.
% % The number of classes varies greatly across the KGs, ranging from 736 (DBpedia) to 53K (Freebase) and 300K (Wikidata). Despite having a large number of classes, Wikidata also has the highest proportion of classes that are actually utilized, when compared to other databases (i.e., classes with at least one instance). The reason for this may the fact that heuristics are used in the selection of suitable Freebase categories as Wikidata classes. Wikidata, on the other hand, includes a large number of classes, but only a tiny percentage of them are actually utilized at the instance level. 
% % It should be noted, however, that this is not always a problem.
% \item \emph{Relations and Predicates:} When searching through the KGs, it is typical to discover relations that are only seldom used: just 5\% of the Freebase relations are used more than 500 times, and about 70\% are never used at all. A quarter of the DBpedia ontology's relations are used more than 500 times in DBpedia, whereas half of the ontology's relations are used just once. 
% \item \emph{Instances and Entities:} Freebase has by far the most entities of any database on the internet today. Due to the fact that each statement is instantiated, Wikidata exposes a disproportionately large number of instances in comparison to entities (in the sense of instances that represent real world objects), resulting in approximately 74M instances that are not entities.
% \item \emph{Domain:} 
% % Despite the fact that all KGs span multiple domains, the domains are not distributed evenly.
% % Additionally, domain coverage varies significantly between KGs. The datasets incorporated into the KGs have a significant impact on the domains that are well represented. Due to the import of MusicBrainz facts, the domain of media has a high knowledge representation (77\%) in Freebase. Persons has the most entries in DBpedia, most likely as a result of DBpedia's use of Wikipedia as a data source.
% %Although all KGs span multiple domains, they are not evenly distributed. 
% Domain coverage varies greatly between KGs. The KG datasets have a big impact on the domains that are well represented. Media has a high knowledge representation (77\%) in Freebase due to the import of MusicBrainz facts. Because DBpedia relies on Wikipedia as a data source, Persons has the most entries.

% % Despite the fact that all of the KGs under consideration are cross-domain, the domains are not evenly distributed across the KGs.
% % In addition, the domain coverage varies significantly across the KGs. The domains that are well represented in the KGs are strongly influenced by the datasets that have been incorporated into the KGs. As a result of the importation of MusicBrainz facts into Freebase, the domain of media has a high knowledge representation (77\%) in Freebase. The domain of persons has the greatest number of entries in DBpedia, which is most likely owing to the use of Wikipedia as a data source.
% \end{compactitem}

% \begin{table}[!h]
% \centering
% \caption{Knowledge bases and datasets for benchmarking}
% \begin{subtable}[t]{0.45\textwidth}
% \centering
%  \caption{Knowledge bases comparison}
% \label{KG}
% \footnotesize
% \scalebox{0.7}{
% \begin{tabular}{@{}llll@{}}
% \toprule
% \textbf{KBs} & \textbf{DBpedia} & \textbf{Freebase} & \textbf{Wikidata} \\ \midrule
% Number of triples & 411 885 960 & 3 124 791 156 & 748 530 833 \\
% Number of instances & 20 764 283 & 115 880 761 & 142 213 806 \\
% Number of entities & 4 298 433 & 49 947 799 & 18 697 897 \\
% Number of classes & 736 & 53 092 & 302 280 \\
% Number of relations & 2819 & 70 902 & 1874 \\
% No. of unique predicates & 60 231 & 784 977 & 4839 \\ \bottomrule
% \end{tabular}
% }
% \end{subtable}
% \quad






%\begin{table}[!h]
%\centering
%\caption{List of datasets}
%\label{tab:datasetList}
%\footnotesize
%\scalebox{0.8}{
%\begin{tabular}{@{}llll@{}}
%\toprule
%\textbf{Dataset} & \textbf{Questions} & \textbf{KB} & \textbf{Domain} \\ \midrule
%MIMICSQL & 10,000 & Generated Table & Biomedical \\
%MIMICSQL* & 10,000 & Generate Table & Biomedical \\
%MIMICSPARQL & 10,000 & Generated KG & Biomedical \\
%COVID-KGQA & 1,000 & DBPedia & Biomedical, Multi-language \\
%% LC-QUAD 2.0 & 30,000 & DBPedia, Wikidata & Generic \\
%LC-QUAD & 5,000 & DBPedia & Generic \\
%QALD-9 & 408 & DBPedia & Generic, Multi-language \\ 
%QALD-8 &  315 & DBPedia, Wikidata & Generic,  Multi-language \\
%QALD-7 & 530 & DBPedia, Wikidata & Generic, Multi-language \\
%% QALD-6 &  431 & DBPedia, LinkedSpending & Generic, Multi-language \\
%% QALD-5 & 334 & DBPedia & Generic, Multi-language \\
%% QALD-4 & 321 & DBPedia & Generic, Multi-language \\
%% QALD-3 & 397 & DBPedia, MusicBrainz & Generic, Multi-language \\
%% QALD-2 & 344 & DBPedia, MusicBrainz & Generic, Multi-language \\
%% QALD-1 & 199 & DBPedia, MusicBrainz & Generic, Multi-language \\
%\bottomrule
%\end{tabular}
%}
%\end{table}




\subsubsection{Datasets}

We construct a sizable collection of benchmarking datasets.

\sstitle{Multi-domain datasets} Multi-domain attempts to improve performance by distributing it over several domains, and has been successfully used in a variety of areas.
\emph{Generic} are data gathered from a wide range of sources, including mathematics, physics, computer science, and a variety of other fields. 
% 	When it comes to system development, life testing is more appropriate during the latter stages of the process, when problems such as reliability growth and system substantiation are important.
	LC-QUAD \cite{trivedi2017lc} is Large-Scale Complex Question Answering Dataset. It consists of 5000 questions and answers pair with the intended SPARQL queries over knowledge base in DBPedia. 
% 	LC-QUAD 2.0 \cite{dubey2019lc} is the new version of LC-QUAD with several updates as follows. First, the number of questions is 30000. Second, all questions consist of paraphrased versions via crowdsourcing tasks. The last section includes queries with multiple user intentions as well as inquiries requiring SPARQL string operations. Furthermore, in addition to the pure DBPedia knowledge base as the target knowledge base, LC-QUAD 2.0 also offers a new version of DBPedia, which is DBPedia based on Wikidata.
\emph{Biomedical} is data about human health. 
%Recently, several studies \cite{PhamThuan2020} have made use of this domain. Using a dataset with a particular area, such as biomedical, may assist us in reducing the size of a dataset with a specified goal and domain.
	A large-scale healthcare Question-to-SQL dataset, MIMICSQL \cite{wang2020text}, was created by utilizing the publicly available real world Medical Information Mart for Intensive Care III (MIMIC III)
% 	\cite{johnson2016mimic}
	dataset to generate 10,000 Question-to-SQL pairs. MIMICSQL* \cite{park2020knowledge} is the modified version of MIMICSQL which improves on the disadvantages of MIMICSQL as their tables are unnormalized and simpler than the tables used in actual hospitals. MIMICSPARQL is an SPARQL-based version of MIMICSQL*.
%	 with the aim to compare the performance between SQL and SPARQL. 
%	COVID-KGQA includes our additional datasets with more 1000 questions about COVID-19. The details of our dataset are described in the next subsection

\sstitle{Multilingual datasets} Multilingual datasets are those that support multiple languages. With multilingual datasets, the performance of a model can be evaluated across a variety of languages, and the differences between each language can be determined. Beginning in 2011 and lasting until 2020, Question Answering over Linked Data (QALD) is a series of evaluation campaigns. The most recent version is QALD-9 \cite{ngomo20189th}, which contains 408 questions compiled and selected from previous tests and is available in eleven languages.

\sstitle{Our new benchmarking data (COVID-KGQA)}
In addition to the provided datasets, we create a bilingual dataset about COVID-19 to increase the diversity of domains and languages. Using the most recent version of DBPedia as a starting point, we compiled a corpus of more than one thousand question-answer pairs in two languages.
% The data format is similar to the QALD dataset \cite{ngomo20189th} with a simplified version from CBench \cite{Orogat2021}, which makes the evaluation procedure straightforward. 
% An example of our data is illustrated in \autoref{fig:COVIDKGQA}.

% \begin{figure}[!h]
% %    \vspace{-5pt}
%     \centering
%     \includegraphics[width=0.5\linewidth]{COVID19-QASample.pdf}
%     \caption{Example of our dataset }
%     \label{fig:COVIDKGQA}
% %    \vspace{-10pt}
% \end{figure}

% The JSON format of our dataset is described below:

% \begin{lstlisting}[language=json,numbers=none,basicstyle=\tiny]
% {
%   "id":"6",
%   "question":[ {
%          "language":"en",
%          "string":"Where is the first case in Vietnam?  ",
%          "keywords":"first case, COVID-19, Vietnam "
%       }, {
%          "language":"vi",
%          "string":"Truong hop ca nhiem COVID-19 dau tien cua Viet Nam la o dau?",
%          "keywords":"Ca nhiem dau tien, COVID-19, Viet Nam"
%       }],
%   "query":{
%       "sparql":"SELECT DISTINCT ?uri WHERE { <http://dbpedia.org/resource/COVID-19_pandemic_in_Vietnam> <http://dbpedia.org/property/firstCase> ?uri }"
%   },
%   "answers":[{"head":{"vars":["uri"]},
%          "results":{"bindings":[{"uri":{
%                      "type":"uri",
%                      "value":"http://dbpedia.org/resource/Ho_Chi_Minh_City"
%                   }}]}}
%   ]
% }
% \end{lstlisting}








\subsubsection{Measurements}

We use the following evaluation metrics.

\sstitle{F1-score}
Let $Q$ denotes the number of questions in benchmark , $G$ is the number of answers processed by the system for given question $Q$, and $A$ is the number of corrected answers extracted by executing query $q$ from question $Q$ over knowledge graph $KG$. 
%The Precision of the answers shows how many of them are accurate. Recall shows how many of the returned responses correspond to the gold standard. Precision (P) and Recall (R) are defined as  
%$
%P=\frac{\mid A \mid }{\mid G \mid}
%$
%and
%Precision is defined as the percentage of relevant questions that are allocated to a category out of all the questions that are assigned by the system. 
%The high accuracy score indicates that the number of questions that are irrelevant but are classified as relevant instances by the classifier (false positives), is extremely minimal. 
%However, Precision, does not take into account how many of relevant instances in a category are correctly allocated, making it impossible to assess overall categorization performance.
%$
%R=\frac{\mid A \mid}{\mid Q \mid}
%$
%respectively.
%In contrast to Precision, Recall is a measure of how many questions in a category are properly allocated to the right category. High Precision indicates that an algorithm assigns more relevant instances to a category than irrelevant instances (quality), but high Recall indicates that an algorithm allocated the majority of the relevant instances to a category (quantity). In order to assess performance, the trade-off between Precision and Recall must be considered, and using only one of them is not sufficient to evaluate the performance.
%As a result, the accuracy will rise due to the limited number of real positive assignments and the large number of irrelevant instances that are allocated to a category, among other factors. As a result of the small number of real positive assignments, there will be a significant number of false negative assignments, resulting in a low Recall score for the assignment.
%The F1 measure is intended to overcome the limitations of both measures, Recall and Precision, by measuring performance as the harmonic mean of both measurements, Recall and Precision, as well as the performance of the F1 measure. When compared  to the individual performance metrics Recall and Precision, the F1 measure is more trustworthy in terms of gauging overall performance.
F1 is defined as 
$
F1 =\frac{1+\beta^2}{\beta^2*(1/P + 1/R)}
$
where $P=\frac{\mid A \mid }{\mid G \mid}$ and $R=\frac{\mid A \mid}{\mid Q \mid}$. $\beta$ weighs whether precision or recall is more important, and equal emphasis is given to both when $\beta$ = 1. 
%In trials, most researchers choose $\beta$ = 1 since it gives no benefit to precision or recall. In this case, we use $\beta$ = 1 for our experiment evaluation. This metric is used by many experiments through benchmark \cite{Orogat2021}, system \cite{liang2021querying} and corpus \cite{usbeck20177th}. 

\sstitle{Execution accuracy} 
Execution accuracy ($Acc_{EX}$) is computed as the accuracy of the response retrieved through SQL/SPARQL\cite{wang2020text}, 
$ Acc_{EX} = N_{EX}/N $, where N represents the total number of Question-SQL pairings in the MIMICSQL database and $N_{EX}$ represents the number of created SQL queries with the potential to provide accurate responses. Execution accuracy may also take into account questions formulated with invalid SQL queries but yielding valid query results. Therefore, we employ a different metric, Logical form accuracy, which eliminates the disadvantage of execution accuracy in the execution.

\sstitle{Logical form accuracy}
The Logical form accuracy ($Acc_{LF}$) is used for if a string match exists between a produced SQL query and a ground truth query~\cite{park2020knowledge}. In order to compute $Acc_{LF}$, we must compare the produced SQL/SPARQL with the true SQL/SPARQL, token by token. That is,
$
Acc_{LF} = N_{LF}/N 
$
where $N_{LF}$ counts how many requests are completely matched to the ground truth query. 

\sstitle{Computation time}
Another measure for KBQA methods is the amount of time required to complete a task.
%The processing speed of various QA applications is often constrained, as are the computer resources available to the program. As a consequence, the amount of time it takes to complete the process becomes critical. The fairness of our benchmark is ensured by the fact that all algorithms are built and tested on the same standard (i.e., they are all tested on the same benchmarks or setup).
% \sstitle{Influence of missing information} 
% \subsubsection{Implementation details}


%\sstitle{Hyperparameter configuration}
%\autoref{tab:Configuration} summarizes the main configurations of parameters which we use to to conduct experiments on the first three datasets. These ones aid in the more accurate replication of the results.
%
%\begin{table}[!h]
%\centering
%\caption{Configuration of parameters}
%\label{tab:Configuration}
%\resizebox{\columnwidth}{!}{
%\begin{tabular}{clccccccc} 
%\hline
%\textbf{Dataset }                      & \multicolumn{1}{c}{\textbf{Method }} & \multicolumn{1}{l}{\textbf{Batch size}} & \textbf{Step decay} & \multicolumn{1}{l}{\textbf{Step size}} & \multicolumn{1}{l}{\textbf{Learning rate}} & \multicolumn{1}{l}{\textbf{Random state}} & \textbf{Epoch} & \multicolumn{1}{l}{\textbf{Batch id}}  \\ 
%\midrule
%\multirow{4}{*}{\textbf{MIMICSQL }}    & TREQS (Dev)                          & 16                                      & 0.8                 & 2                                      & 0.0005                                     & 42                                        & 6              & 400                                    \\
%                                       & TREQS + Recover (Dev)                & 16                                      & 0.8                 & 2                                      & 0.0005                                     & 42                                        & 6              & 400                                    \\ 
%\cline{2-9}
%                                       & TREQS (Test)                         & 16                                      & 0.8                 & 2                                      & 0.0005                                     & 42                                        & 6              & 400                                    \\
%                                       & TREQS + Recover (Test)               & 16                                      & 0.8                 & 2                                      & 0.0005                                     & 42                                        & 6              & 400                                    \\ 
%\midrule
%\multirow{4}{*}{\textbf{MIMICSQL* }}   & TREQS (Dev)                          & 16                                      & 0.8                 & 2                                      & 0.0005                                     & 1234                                      & 7              & 300                                    \\
%                                       & TREQS + Recover (Dev)                & 16                                      & 0.8                 & 2                                      & 0.0005                                     & 1234                                      & 7              & 300                                    \\ 
%\cline{2-9}
%                                       & TREQS (Test)                         & 16                                      & 0.8                 & 2                                      & 0.0005                                     & 1234                                      & 7              & 300                                    \\
%                                       & TREQS + Recover (Test)               & 16                                      & 0.8                 & 2                                      & 0.0005                                     & 1234                                      & 7              & 300                                    \\ 
%\midrule
%\multirow{2}{*}{\textbf{MIMICSPARQL }} & TREQS (Dev)                          & 48                                      & 0.1                 & 2                                      & 0.0005                                     & 1234                                      & 12             & 166                                    \\
%                                       & TREQS (Test)                         & 48                                      & 0.1                 & 2                                      & 0.0005                                     & 1234                                      & 12             & 166                                    \\
%\hline
%\end{tabular}}
%\end{table}

%\sstitle{Reproducibility Environment}
% The experiments with  the first three MIMIC datasets are conducted on an  Intel(R) Xeon(R) CPU @ 2.30GHz system with 35.5 GB Ram and Tesla P100 graphics cards. Four other datasets are  carried out using Intel(R) Xeon(R) CPU E5-2620 @ 2.10GHz with 125 GB Ram and Nvidia Geforce GTX 1080 graphics cards.
% All algorithms were evaluated using the same standards (same benchmarks and settings) in order to guarantee fairness.
\section{Results}
\label{sec:exp}

%As a further step, we  go through our findings from when we applied the benchmark to the KBQA benchmark system algorithms. The primary aim of the experiments is not only to compare the performances, but also to investigate the impacts of data features on the behavior of the performance.


% \subsection{Efficiency comparison}
% \begin{table}[H]
% \scalebox{0.95}{
% \begin{tabular}{@{}llcllclllcll@{}}
% \toprule
% \textbf{} & \multicolumn{3}{c}{\textbf{MIMICSQL}} &  & \multicolumn{3}{c}{\textbf{MIMICSQL*}} &  & \multicolumn{3}{c}{\textbf{MIMICSPARQL}} \\ \cmidrule(lr){2-4} \cmidrule(lr){6-8} \cmidrule(l){10-12} 
% \textbf{Metric} & Acc\_EX & Acc\_LF & Time &  & Acc\_EX & Acc\_LF & Time &  & Acc\_EX & Acc\_LF & Time \\ \midrule
% TREQS  & 0.509  & 0.406  &  &  & 0.538  & 0.569 &  &  &  0.712& 0.624  &  \\
% Seq2Seq &  &  &  &  &  &  &  &  &  &  &  \\ \bottomrule
% \end{tabular}}
% \caption{Evaluation of QA Systems}
% \label{tab:Evaluation}
% \end{table}
% \usepackage{multirow}
% \usepackage{booktabs}





% Please add the following required packages to your document preamble:
% \usepackage{booktabs}



\subsection{End-to-end comparison}


We provide empirical finding and examined the overall performance of knowledge bases in terms of answering questions on three biomedical datasets, four commonly used datasets, and one new dataset as part of this investigation. 
% The findings are presented in \autoref{tab:Evaluation} and \autoref{tab:Evaluation2}. 
TREQS and TREQS++ are the most accurate among the top performers shown in \autoref{tab:Evaluation} on the first three biomedical datasets, with the highest accuracy on the fourth. TREQS++ achieves a better result on MIMICSQL, but its overall performance on the development and test datasets is inferior to that of TREQS on MIMICSQL*. Due to the design of TREQS++, which is to Recover Condition Values with Table Content, it is currently unable to perform with a knowledge graph and therefore cannot operate with MIMICSPARQL.
The figure \autoref{fig:mimicCompare} depicts the visualisation of our results on the first three biomedical datasets, while the figure \autoref{fig:mimicCompareTime} illustrates the average time required to answer 100 random questions. With the TREQS technique, $Acc_{EX}$ and $Acc_{LF}$ perform better on the development set than on the test set in the majority of instances. However, with TREQS, the outcomes are different; $Acc_{LF}$ performs better on the test set.
 
% Regarding time, MIMICSPARQL is the quickest in nearly all situations, whereas MIMICSQL* is the slowest. The potential explanation is that MIMICSQL* is more sophisticated and normalized than MIMICSQL, resulting in a longer calculation time. 
%Additionally, our research is the first to discover that despite the fact that MIMICSPARQL performs better than others in terms of performance, it also performs better in terms of time.

% In this study, we compared the overall performance of knowledge bases on three biomedical datasets, four existing datasets, and one new dataset. The findings are presented in \autoref{tab:Evaluation} and \autoref{tab:Evaluation2}. TREQS and TREQS++ are the most accurate on the first three biomedical datasets, with the highest accuracy on the fourth. TREQS++ outperforms TREQS on MIMICSQL*, but not on the development or test datasets. TREQS++ is currently unable to work with MIMICSPARQL due to its design to recover condition values with table content, rather than a knowledge graph.
% autoref:mimicCompareTime shows the statistic of average time with 100 random questions. As a rule of thumb, $Acc_{EX}$ and $Acc_{LF}$ outperform TREQS on the development set. But with TREQS, $Acc_{LF}$ outperforms on the test set.
% In terms of speed, MIMICSPARQL is the fastest, while MIMICSQL* is the slowest. The calculation time could be longer because MIMICSQL* is more sophisticated and normalized than MIMICSQL. Our research also discovered that while MIMICSPARQL outperforms others in terms of performance, it also outperforms others in terms of time.

% \begin{figure*}[!h]
% 	\centering
% \begin{minipage}{0.3\linewidth}
% 		\centering
%     \captionsetup{type=table}
%     \captionof{table}{Evaluation of QA Systems}
%     \label{tab:Evaluation}
%     %\vspace{0.7em}
%     %\vspace{2em}
% 	\end{minipage}
% 	\quad \quad \quad
%     \begin{minipage}{.63\linewidth}
%     \centering
%     \includegraphics[width=0.95\linewidth]{MIMIC_Comparison.png}
%     \caption{Comparison Accuracy through SQL, SQL* and SPARQL version}
%     \label{fig:mimicCompare}
%           \end{minipage}
% \end{figure*}

%  \begin{figure*}[!h]
% \centering
%      \begin{subfigure}[b]{0.49\textwidth}
%          \centering
%          \includegraphics[width=1\linewidth]{MIMIC_Comparison.png} 
%          \caption{SQL, SQL* and SPARQL}
%          \label{fig:QuestionTypeMIMICSQL}
%      \end{subfigure}
%      \begin{subfigure}[b]{0.49\textwidth}
%          \centering
%          \includegraphics[width=1\linewidth]{MIMICstar_QuesType.png} 
%          \caption{MIMICSQL*}
%          \label{fig:QuestionTypeMIMICSTAR}
%      \end{subfigure}
%         \caption{Comparison on MIMICSQL, MIMICSQL*, MIMICSPARQL with influence of question type from test set, the underscore on the charts represent value 0}
%         \label{fig:QuestionTypeMIMIC}
%  \end{figure*}




\begin{table}[!h]
\vspace{-2em}
\centering
\footnotesize
\caption{End-to-end comparison on biomedical datasets}
\label{tab:Evaluation}
\scalebox{0.7}{
\begin{tabular}{clccc} 
\toprule
\textbf{Dataset }                      & \multicolumn{1}{c}{\textbf{Method }} & \multicolumn{1}{l}{\textbf{$Acc_{EX}$}}      & \textbf{$Acc_{LF}$}                          & \multicolumn{1}{l}{\textbf{Time (s)}}                           \\ 
\toprule
\multirow{4}{*}{\textbf{MIMICSQL }}    & TREQS (Dev)                          & 0.543                                     & 0.345                                     & \multirow{4}{*}{\textcolor[rgb]{0.129,0.129,0.129}{0.161}}   \\
                                      & TREQS++ (Dev)                & {\textbf{0.626}}           & \textbf{{0.43}}            &                                                                 \\ 
\cline{2-4}
                                      & TREQS (Test)                         & 0.469                                     & 0.354                                     &                                                                 \\
                                      & TREQS++ (Test)               & {\textbf{0.533}}           & \textbf{{0.404}}           &                                                                 \\ 
\cmidrule[\heavyrulewidth]{1-5}
\multirow{4}{*}{\textbf{MIMICSQL* }}   & TREQS (Dev)                          & {\textbf{0.636}}           & {\textbf{0.506 }}          & \multirow{4}{*}{\textcolor[rgb]{0.129,0.129,0.129}{0.311}}   \\
                                      & TREQS++ (Dev)                & \textcolor[rgb]{0.129,0.129,0.129}{0.626} & \textcolor[rgb]{0.129,0.129,0.129}{0.43}  &                                                                 \\ 
\cline{2-4}
                                      & TREQS (Test)                         & {\textbf{0.563 }}          & {\textbf{0.524 }}          &                                                                 \\
                                      & TREQS++ (Test)               & \textcolor[rgb]{0.129,0.129,0.129}{0.533} & \textcolor[rgb]{0.129,0.129,0.129}{0.404} &                                                                 \\ 
\cmidrule[\heavyrulewidth]{1-5}
\multirow{2}{*}{\textbf{MIMICSPARQL }} & TREQS (Dev)                          & {\textbf{0.822}}           & 0.580                                     & \multirow{2}{*}{\textcolor[rgb]{0.129,0.129,0.129}{~0.25}}  \\
                                      & TREQS (Test)                         & 0.698                                     & \textbf{{0.641}}           &                                                                 \\
\bottomrule
\end{tabular}
}
\vspace{-2em}
\end{table}

%\begin{figure}[!h]
%%    \vspace{-5pt}
%    \centering
%    \includegraphics[width=0.8\linewidth]{MIMIC_Comparison.png}
%    \caption{Comparison Accuracy through SQL, SQL* and SPARQL version}
%   \label{fig:mimicCompare}
%    % \vspace{-1pt}
%\end{figure}
%
%\begin{figure}[!h]
%%    \vspace{-5pt}
%    \centering
%    \includegraphics[width=0.6\linewidth]{OverallF1.png}
%    \caption{Comparison of QAsparql, QAsparql*, TeBaQA, gAnswer on COVID-KGQA, LCQUAD and QALD 7,8,9}
%    \label{fig:GenericCompare}
%    % \vspace{-1pt}
%\end{figure}



\begin{figure}[!h]
\vspace{-1.0em}
  \centering
  \begin{subfigure}[b]{0.54\textwidth}
         \centering
         \includegraphics[width=1\linewidth]{MIMIC_Comparison1.png}
  			\caption{On biomedical KBs}
  			\label{fig:mimicCompare}
     \end{subfigure}
  \begin{subfigure}[b]{0.44\textwidth}
         \centering
         \includegraphics[width=1\linewidth]{OverallF1.png}
    \caption{On generic KBs}
    \label{fig:GenericCompare}
     \end{subfigure}
     \vspace{-1em}
\caption{Accuracy comparison}
\vspace{-1em}
  \end{figure}




%\begin{table}[!h]
%\centering
%\footnotesize
%\caption{Evaluation of QA Systems}
%\label{tab:Evaluation}
%\resizebox{0.5\linewidth}{!}{
%\begin{tabular}{clccc} 
%\toprule
%\textbf{Dataset }                      & \multicolumn{1}{c}{\textbf{Method }} & \multicolumn{1}{l}{\textbf{$Acc_{EX}$}}      & \textbf{$Acc_{LF}$}                          & \multicolumn{1}{l}{\textbf{Time (s)}}                           \\ 
%\toprule
%\multirow{4}{*}{\textbf{MIMICSQL }}    & TREQS (Dev)                          & 0.543                                     & 0.345                                     & \multirow{4}{*}{\textcolor[rgb]{0.129,0.129,0.129}{0.161}}   \\
%                                       & TREQS++ (Dev)                & \textcolor{red}{\textbf{0.626}}           & \textbf{\textcolor{red}{0.43}}            &                                                                 \\ 
%\cline{2-4}
%                                       & TREQS (Test)                         & 0.469                                     & 0.354                                     &                                                                 \\
%                                       & TREQS++ (Test)               & \textcolor{red}{\textbf{0.533}}           & \textbf{\textcolor{red}{0.404}}           &                                                                 \\ 
%\cmidrule[\heavyrulewidth]{1-5}
%\multirow{4}{*}{\textbf{MIMICSQL* }}   & TREQS (Dev)                          & \textcolor{red}{\textbf{0.636}}           & \textcolor{red}{\textbf{0.506 }}          & \multirow{4}{*}{\textcolor[rgb]{0.129,0.129,0.129}{0.311}}   \\
%                                       & TREQS++ (Dev)                & \textcolor[rgb]{0.129,0.129,0.129}{0.626} & \textcolor[rgb]{0.129,0.129,0.129}{0.43}  &                                                                 \\ 
%\cline{2-4}
%                                       & TREQS (Test)                         & \textcolor{red}{\textbf{0.563 }}          & \textcolor{red}{\textbf{0.524 }}          &                                                                 \\
%                                       & TREQS++ (Test)               & \textcolor[rgb]{0.129,0.129,0.129}{0.533} & \textcolor[rgb]{0.129,0.129,0.129}{0.404} &                                                                 \\ 
%\cmidrule[\heavyrulewidth]{1-5}
%\multirow{2}{*}{\textbf{MIMICSPARQL }} & TREQS (Dev)                          & \textcolor{red}{\textbf{0.822}}           & 0.580                                     & \multirow{2}{*}{\textcolor[rgb]{0.129,0.129,0.129}{~0.25}}  \\
%                                       & TREQS (Test)                         & 0.698                                     & \textbf{\textcolor{red}{0.641}}           &                                                                 \\
%\bottomrule
%\end{tabular}
%}
%\end{table}

%\begin{figure}[!h]
%%    \vspace{-5pt}
%    \centering
%    \includegraphics[width=1\linewidth]{MIMIC_Comparison.png}
%    \caption{Comparison Accuracy through SQL, SQL* and SPARQL version}
%    \label{fig:mimicCompare}
%%    \vspace{-10pt}
%\end{figure}






\autoref{tab:Evaluation2} compares the four systems and their scores for the four metrics - Precision, Recall, F1 and Time used in our paper with five datasets (four generic dataset and one additional biomedical dataset) and \autoref{fig:GenericCompare} shows visualisation of them in terms of F1 . Our system has been chosen alongside two new systems that are up to date and scheduled to be launched in 2021. These systems performed outstandingly in the previous comparison \cite{liang2021querying}. 
The unique aspect of this discovery is that altering the entity and resource algorithm, as stated in \autoref{sec:IRTechnique}, improves performance over and beyond the default method used before. This is referred to as QAsparql* in order to differentiate it from the default. gAnswer is our next chosen system, which is state of the art in the QALD-9 challenge and is performed well on QALD-7, QALD-8, and LC-QUAD.

In terms of datasets, we select four existing datasets in addition to a large number of additional datasets that are split into the LC-QUAD and QALD series, which are the most frequently used datasets in the DBPedia. We choose QALD versions 7 to 9 from the QALD series since the target KG of these datasets are from version 2016, which is suited with a system that is nearly as current as the QALD series, and this will help these systems perform better. Our COVID-KGQA uses the latest version of DBPedia.

\begin{table}[!h]
\vspace{-2em}
\centering
\caption{Performance comparison on multilingual datasets (time in s)}
\label{tab:Evaluation2}
\resizebox{0.85\linewidth}{!}{
\begin{tabular}{lllllllllllllllllllll} 
\toprule
                  &  & \multicolumn{4}{c}{\textbf{\textbf{QAsparql}}}                                                                                        & \multicolumn{1}{c}{\textbf{}} & \multicolumn{4}{c}{\textbf{QAsparql*}}                                                                            &  & \multicolumn{4}{c}{\textbf{TeBaQA}}                                                                                                   & \multicolumn{1}{c}{\textbf{}} & \multicolumn{4}{c}{\textbf{gAnswer}}                                                                                                   \\
\textbf{Dataset}   &  & \multicolumn{1}{c}{\textbf{P}} & \multicolumn{1}{c}{\textbf{R}} & \multicolumn{1}{c}{\textbf{F1}} & \multicolumn{1}{c}{\textbf{Time}} & \multicolumn{1}{c}{\textbf{}} & \multicolumn{1}{c}{\textbf{P}} & \multicolumn{1}{c}{\textbf{R}} & \multicolumn{1}{c}{\textbf{F1}} & \textbf{Time} &  & \multicolumn{1}{c}{\textbf{P}} & \multicolumn{1}{c}{\textbf{R}} & \multicolumn{1}{c}{\textbf{F1}} & \multicolumn{1}{c}{\textbf{Time}} & \multicolumn{1}{c}{\textbf{}} & \multicolumn{1}{c}{\textbf{P}} & \multicolumn{1}{c}{\textbf{R}} & \multicolumn{1}{c}{\textbf{F1}} & \multicolumn{1}{c}{\textbf{Time}}  \\ 
\midrule
COVID-KGQA (Our) &  & 0.00                           & 0.00                           & 0.00                            & \textbf{0.26}                     &                               & 0.29                           & 0.52                           & 0.37                            & \textbf{2.69} &  & \textbf{0.38}                  & \textbf{0.38}                  & \textbf{0.38}                   & 13.2                              &                               & 0.00                           & 0.00                           & 0.00                            & 40.99                              \\
QALD-9 (Train)    &  & 0.4                            & 0.45                           & 0.42                            & 5.89                              &                               & 0.33                           & 0.71                           & 0.45                            & 32.37         &  & 0.17                           & 0.18                           & 0.16                            & 8.5                               &                               & 0.39                           & 0.43                           & 0.39                            & 13.03                              \\
QALD-9 (Test)     &  & 0.32                           & 0.36                           & 0.34                            & 9.56                              &                               & 0.3                            & 0.6                            & 0.4                             & 27.45         &  & 0.16                           & 0.15                           & 0.15                            & 8.15                              &                               & 0.33                           & 0.37                           & 0.34                            & 13.30                              \\
QALD-8 (Train)    &  & 0.35                           & 0.49                           & 0.41                            & 5.16                              &                               & 0.31                           & 0.67                           & 0.42                            & 7.9           &  & 0.21                           & 0.22                           & 0.21                            & 7.52                              &                               & \textbf{0.45}                  & 0.51                           & \textbf{0.46}                   & 12.93                              \\
QALD-8 (Test)     &  & \textbf{0.57}                  & 0.44                           & 0.5                             & 3.56                              &                               & 0.2                            & 0.44                           & 0.28                            & 7.35          &  & 0.21                           & 0.22                           & 0.21                            & 8                                 &                               & 0.42                           & \textbf{0.54}                  & 0.44                            & \textbf{12.07}                     \\
QALD-7 (Train)    &  & 0.33                           & 0.55                           & 0.42                            & 6.42                              &                               & \textbf{0.38}                  & 0.77                           & \textbf{0.51}                   & 5.69          &  & 0.2                            & 0.21                           & 0.2                             & 7.92                              &                               & 0.43                           & 0.5                            & 0.44                            & 13.09                              \\
QALD-7 (Test)     &  & 0.21                           & 0.44                           & 0.28                            & 1.74                              &                               & 0.24                           & 0.5                            & 0.33                            & 7.3           &  & 0.22                           & 0.23                           & 0.21                            & \textbf{7.72}                     &                               & \textbf{0.29}                  & 0.35                           & 0.29                            & 13.04                              \\
LC-QUAD (Test)    &  & 0.34                           & \textbf{0.62}                  & \textbf{0.44}                   & 5.41                              &                               & 0.31                           & \textbf{0.98}                  & 0.47                            & 13.75         &  & 0.21                           & 0.22                           & 0.21                            & 16.45                             &                               & 0.09                           & 0.11                           & 0.1                             & 60.45                              \\
\bottomrule
\end{tabular}}
\vspace{-1em}
\end{table}

For the COVID-KGQA dataset, we evaluated the effectiveness of the existing system with our new benchmark corpus in COVID-19 using four distinct systems. gAnswer is deployed via gStore with version 2016-10 of DBPedia as the guidance for gAnswer systems, whereas we conduct experiments with the most recent version of DBPedia for other systems. In this version, the knowledge graph contains no COVID-19-related information. Consequently, gAnswer achieves the worst performance in our corpus for both F1 and Time. On the other hand, TeBaQA and QAsparql have the best performance; while TeBaQA outperforms QAsparql 0.01 in F1, QAsparql outperforms TeBaQA in some fields, as discussed in \autoref{sec:QuestionQuantity}. TeBaQA has the best performance on our new benchmarking dataset, whereas this system performs poorly on other datasets with a performance range of only 0.15 to 0.21. Possible causes include the fact that QALD is a more difficult benchmark with a large number of questions requiring complex queries with more than two triples to answer.
A deeper examination revealed that QALD-9 questions frequently required sophisticated templates that were either absent from the training questions or provided very limited assistance. The template-based approach in TeBaQA is inferior to the Context-based and Subgraph matching techniques, with the exception of LC-QUAD. QAsparql*, our new finding approach, achieves the best performance in almost all corpora, with the exception of COVID-KGQA (with a length of less than approximately 0.01) and QALD-8. This demonstrates that enhancing the algorithm for entity matching can improve the system's performance.
% Except GAnswer, was the fourth best performer. All of the datasets were scored with good precision, recall, and F1-score, with results ranging from 0.75 to 1. QASPARQL likewise obtained excellent precision, recall, and F1-score for all of the datasets. Using the LC-QUAD dataset, GAN-fingerprint beat the competition on all three measures, and even achieved absolute results in certain cases. This is due to the fact that UADFV is a tiny dataset that solely uses the DeepFake method. With respect to the other datasets, GAN-fingerprint produced excellent results on all four measures, but lagged behind Xception and Capsule in certain areas, particularly when dealing with difficult datasets such as df in the wild. Mesonet performed at a level that was comparable to GAN-fingerprint on all of the datasets. Traditional methods such as FDBD, HPBD, and VisualArtifacts do not usually outperform deep learning techniques in terms of overall performance. HPBD was the method that performed the poorest, with an accuracy of less than 0.5 on certain datasets such as the DF-TIMIT 690 and the DFDC. According to one theory, the actors' backgrounds and head poses in these datasets were chosen at random, which may have contributed to their poor performance. When tested on various 695 datasets, including the DF-TIMIT dataset, FDBD outperformed the other two machine learning methods and even reached accuracy comparable to that of deep learning techniques.
%Chua paraphase

\subsection{Running time}

\autoref{fig:mimicCompareTime} displays the experimental results. In the first three datasets, 100 questions are selected at random and their time performance is compared.
In comparison to two other corpora, MIMICSPARQL with a knowledge graph approach achieves the best results overall. Compared to MIMICSPARQL, MIMICSQL* queries are nearly twice as long. A single JOIN in SQL requires 11 tokens (including the operators "=" and "."), whereas a single hop in SPARQL requires only three tokens (i.e., subject, predicate, and object). An additional distinction between SQL and SPARQL is that the model must comprehend the hierarchy between a table and its columns and the relationships between tables. SQL and SPARQL differ syntactically due to this inherent distinction between relational tables and a knowledge graph in terms of how information is linked (joining multiple tables versus hopping across triples). Consequently, the graph-based method performs significantly better than the table-based method. \cite{park2020knowledge}.
We compare the execution times of four systems across five additional datasets. We evaluate every question in the test set of five corpora. gAnswer reaches its highest point in COVID-KGQA and becomes the fastest time overall. The explanation is summarised as follows: gStore is used to install gAnswer with the default knowledge graph version set to 2016-10. However, our dataset is about COVID-19, which was introduced in 2019, so gAnswer takes longer to query but returns no results.



\begin{figure}[!h]
    \vspace{-1em}
    \centering
    \includegraphics[width=0.45\linewidth]{MIMIC_Time_Comparison.pdf}
    \includegraphics[width=0.45\linewidth]{GenericTime.pdf}
    \caption{End to end time (s) comparison}
    \label{fig:mimicCompareTime}
    \vspace{-2em}
\end{figure}



\subsection{Influence of Question Taxonomy}
\label{sec:QuestionQuantity} 

 Over the course of this investigation, the impacts of question type on question answering are investigated using knowledge graph methods. From these experiments, we conduct our studies on test set of LCQUAD and COVID-KGQA because the quantity of question of them is the highest and almost the same (1000 questions). \autoref{fig:QuestionTypeMIMIC}  depicts the results of MIMIC datasets, and \autoref{fig:QuestionTypeCOVID}, \autoref{fig:QuestionTypeLCQUAD}, which are conducted with four systems in COVID-KGQA and LCQUAD, as stated in \autoref{sec:QuestionQuantity}. According to \cite{pomerantz2005linguistic}, the Wh-question taxonomy comprises six types: 'Who', 'What', 'When', 'Where', 'Why', and 'How'. Additionally, throughout the data analysis process, we discover that the dataset contains other types such as 'Whose' and 'Whom'. On the other hand, certain inquiries are not Wh-questions, such as 'Yes/No' questions, which contain phrases starting with the words, i.e., 'Can', 'May', 'Am', 'Is', 'Are', 'Was', 'Were', 'Will', 'Do', and some request questions, such as 'Count', 'Provide', 'Give', 'Tell', 'Specify'. As a result, we undertake this experiment using eleven distinct sorts of questions.
 For the MIMICSQL dataset, \autoref{fig:QuestionTypeMIMICSQL} depicts the $Acc_{EX}$, $Acc_{LF}$, and Time of TREQS and TREQS++ for eleven question categories, with regard to each sort of query. The plot depicts the $Acc_{EX}$, $Acc_{EX}$ with TREQS++, and Time for each question type across the whole test set in this dataset. It is apparent that TREQS++ outperformed TREQS for all question categories on this dataset in $Acc_{EX}$ when using the Question-to-SQL techniques TREQS and TREQS++. $Acc_{LF}$ outperforms all other variables except What, How, and Request. Similarly, MIMICSQL* and MIMICSPARQL results are depicted in \autoref{fig:QuestionTypeMIMICSTAR} and \autoref{fig:QuestionTypeMIMICSPARQL}.
%  In MIMICSQL*, the exception is TREQS++, which is inferior to TREQS in terms of 'What', 'Which', 'How', and 'Request'. TREQS outperformed other types of question with $Acc_{LF}$ at 'What', 'Which', 'How', and 'Request', but other types outscored TREQS++. Time as shown in \autoref{fig:QuestionTypeMIMICTime} is roughly the same across three datasets when the lowest values are at 'Where', 'Who', 'Whom', 'Whose', and the greatest values are at 'How'.
 Regarding the F1 in COVID-KGQA and LCQUAD results, these are illustrated in \autoref{fig:QuestionTypeCOVID} and \autoref{fig:QuestionTypeLCQUAD}. With respect to specific question categories, TeBaQA surpassed both of them. For COVID-KGQA, QAsparql* outperforms in the 4 categories of questions, including 'What', 'When', 'Where', and 'Which', whereas TeBaQA hits a high in the category of 'How'. 
%When compared to other SOTA baseline techniques, gAnswer and QAsparql perform the poorest in this corpus. For the LCQUAD, QAsparql outperforms TeBaQA at 'What', 'Who', and 'Request', while QAsparql outperformed TeBaQA at 'How', 'Request'. Aside from that, QAsparql*, which is the most remarkable baseline, outperformed at the following questions: 'Where', 'Which', and 'Yes/No'. 

% With regards to Time, overall, for COVID-KGQA, the sorts of questions Where and Who are the most time-consuming across all methods, with gAnswer being the slowest of the bunch. In most situations, our newly found approach, QAsparql*, outperforms QAsparql in terms of time savings in most cases.
 

 \begin{figure*}[!h]
  \vspace{-1em}
\centering
     \begin{subfigure}[b]{0.49\textwidth}
         \centering
         \includegraphics[width=1\linewidth]{MIMIC_QuesType.png} 
         \caption{MIMICSQL}
         \label{fig:QuestionTypeMIMICSQL}
     \end{subfigure}
     \begin{subfigure}[b]{0.49\textwidth}
         \centering
         \includegraphics[width=1\linewidth]{MIMICstar_QuesType.png} 
         \caption{MIMICSQL*}
         \label{fig:QuestionTypeMIMICSTAR}
     \end{subfigure}
     \begin{subfigure}[b]{0.49\textwidth}
         \centering
         \includegraphics[width=1\linewidth]{SPARQL_QuesType.png} 
         \caption{MIMICSPARQL}
         \label{fig:QuestionTypeMIMICSPARQL}
     \end{subfigure}
     \begin{subfigure}[b]{0.49\textwidth}
         \centering
         \includegraphics[width=1\linewidth]{MIMIC_QiesType_Time.png} 
         \caption{Time}
         \label{fig:QuestionTypeMIMICTime}
     \end{subfigure}
        \caption{Comparison on MIMICSQL, MIMICSQL*, MIMICSPARQL with the influence of question type, the underscores represent zeroes}
        \label{fig:QuestionTypeMIMIC}
        \vspace{-1em}
 \end{figure*}

 
%  \begin{figure}[!h]

% %    \vspace{-5pt}
%     \centering
    
%     \caption{ }
%     \label{fig:QuestionTypeMIMIC}

% %    \vspace{-10pt}
% \end{figure}
%  \begin{figure}[!h]

% %    \vspace{-5pt}
%     \centering
    
    
%     \caption{Comparison on MIMICSQL* with influence of question type from test set, the underscore on the charts represent value 0 }
%     \label{fig:QuestionTypeMIMICSTAR}

% %    \vspace{-10pt}
% \end{figure}
%  \begin{figure}[!h]

% %    \vspace{-5pt}
%     \centering
    
    
%     \caption{Comparison on MIMICSPARQL with influence of question type from test set, the underscore on the charts represent value 0 }
%     \label{fig:QuestionTypeMIMICSPARQL}

% %    \vspace{-10pt}
% \end{figure}




\begin{figure*}[!h]
\centering
     \begin{subfigure}[b]{0.5\textwidth}
         \centering
         \includegraphics[width=1\linewidth]{COVID_QuestionType_F1.png}
         \caption{F1}
         \label{fig:QuestionTypeCOVIDF1}
     \end{subfigure}
     \begin{subfigure}[b]{0.4\textwidth}
         \centering
         \includegraphics[width=1\linewidth]{COVID_QuestionType_Time.png}
         \caption{Time}
         \label{fig:QuestionTypeCOVIDTime}
     \end{subfigure}
        \caption{Comparison on COVID-KGQA with the influence of question type, the underscores represent zeroes}
        \label{fig:QuestionTypeCOVID}
 \end{figure*}


% \begin{figure}[!h]
%%    \vspace{-5pt}
%    \centering
%    \includegraphics[width=1
%    \linewidth]{COVID_QuestionType_F1.png} 
%    
%    \includegraphics[width=1\linewidth]{COVID_QuestionType_Time.png} 
%    \caption{F1 and Time comparison on COVID-KGQA with influence of question type from test set, the underscore on the charts represent value 0 }
%    \label{fig:QuestionTypeCOVID}
%%    \vspace{-10pt}
%\end{figure}




\begin{figure*}[!h]
\centering
     \begin{subfigure}[b]{0.54\textwidth}
         \centering
         \includegraphics[width=1\linewidth]{LCQUAD_QuestionType_F1.png} 
         \caption{F1}
         \label{fig:QuestionTypeLCQUADF1}
     \end{subfigure}
     \begin{subfigure}[b]{0.40\textwidth}
         \centering
         \includegraphics[width=1\linewidth]{LCQUAD_QuestionType_Time.png}
         \caption{Time}
         \label{fig:QuestionTypeLCQUADTime}
     \end{subfigure}
       \caption{Comparison on LCQUAD with the influence of question type, the underscores represent zeroes}
    \label{fig:QuestionTypeLCQUAD}
 \end{figure*}


% \begin{figure}[!h]
%%    \vspace{-5pt}
%    \centering
%    \includegraphics[width=1\linewidth]{LCQUAD_QuestionType_F1.png} 
%    \includegraphics[width=1\linewidth]{LCQUAD_QuestionType_Time.png} 
%    \caption{Comparison on LCQUAD with influence of question type from test set, the underscore on the charts represent value 0 }
%    \label{fig:QuestionTypeLCQUAD}
%%    \vspace{-10pt}
%\end{figure}

 \begin{figure}[!h]

%    \vspace{-5pt}
    \centering
    \includegraphics[width=0.8\linewidth]{COVID_QuestionNumber.png} 
    \caption{Comparison on COVID-KGQA with the influence of question type from test set}
    \label{fig:QuestionNumberCOVID}

%    \vspace{-10pt}
\end{figure}

\subsection{Effects of Quantity of Questions}


 \begin{figure}[!h]

%    \vspace{-5pt}
    \centering
    \includegraphics[width=0.8\linewidth]{LCQUAD_QuestionNumber.png} 
    \caption{F1 and Time comparison on LCQUAD with the influence of question quantity from test set }
    \label{fig:QuestionNumberLCQUAD}

%    \vspace{-10pt}
\end{figure}


The effects of next characteristic, the number of questions, are then examined. \autoref{fig:mimicQuestionQuantity} depicts the results of an experiment in which the quantity factor of the question is changed from each 100 question segments to 1000 questions of each dataset.
The accuracy of the test set improves significantly as the number of questions increases from 200 to 700, decreases from 800 to 900, and in the majority of instances $Acc_{EX}$ outperforms $Acc_{LF}$. The performance of both metrics typically peaks between the 500th and 700th question. Regarding Time in MIMICSQL, MIMICSQL*, and MIMICSPARQL, as depicted in \autoref{fig:mimicQuestionQuantity}, MIMICSQL and MIMICSQL* are quite comparable in terms of overall performance on the Test sets. 
% Specifically, both datasets are high for the first 100 questions and decrease for the next 200 and 300 questions; it then increases for the 400th and 500th questions, the 800th and 900th questions, and then decreases from the 900th to the 1000th questions. 
The primary difference is that in MIMICSPARQL, the time for both questions is initially extremely short, but steadily increases until the 1000th question, where it reaches a maximum.

As shown in \autoref{fig:QuestionTypeCOVID} and \autoref{fig:QuestionTypeLCQUAD}, F1 in COVID-KGQA and LCQUAD. Regarding context-based techniques, it is evident that QAsparql* outperformed all other methods, with all questions scoring over 0.4 and 300 questions scoring over 0.5, and that it was surpassed by QAsparql* in terms of overall performance. In contrast, the gAnswer technique, which employs Subgraph Matching, had the lowest F1 scores of any technique. QAsparql* was superior to all other techniques and procedures. In terms of time, QAsparql achieved the highest performance, followed by QAsparql* and TeBaQA; however, gAnswer once again achieved the lowest performance in addition to F1 score.


\begin{figure*}[!h]
\vspace{-1em}
\centering
     \begin{subfigure}[b]{1\textwidth}
         \centering
         \includegraphics[width=1\linewidth]{MIMIC_NumQ.png}
         \caption{Accuracy}
         \label{fig:QuestionTypeCOVIDF1}
     \end{subfigure}
     \begin{subfigure}[b]{0.7\textwidth}
         \centering
         \includegraphics[width=1.0\linewidth]{Time_MIMIC_NumQ.png}
         \caption{Time}
         \label{fig:QuestionTypeCOVIDTime}
     \end{subfigure}
        \caption{Effects of \#questions to Accuracy and Time on domain-specific datasets}
        \label{fig:mimicQuestionQuantity}
 \end{figure*}

 





%Chua paraphase
% We then looked at the impact of noise on the performance of each forensics method in detail. In order to mimic this situation, we introduced Gaussian noise to the pictures in the dual benchmarking dataset, as stated in Section 4.3 of this document. The experimental findings are shown in Fig. 7. Unexpectedly, it was discovered that the noise component had a significant impact on the performance of the majority of forensics methods. With respect to this noise component, the GAN-fingerprint showed the highest resilience, with accuracy remaining above 0.7 even when the noise level (standard deviation) reached 0.3. Because Gaussian noise has no impact on the fingerprint produced by GAN-based forging methods, and because of the model's in-depth study of both the picture and the model level, the effects of the noise may be mitigated, this can be described as follows: State-of-the-art methods, on the other hand, such as XceptionNet and Capsule, which showed great promise in the last test, did not do well in this trial. When the standard deviation is reduced to 0.1, the accuracy of these methods rapidly decreases to 0.5 or less. Due to the inclusion of this noise component, the performance of conventional methods such as FDBD, VisualArtifacts, and HPBD was adversely affected as well.




% \subsection{Influence of missing information}

%\section{Discussion}
%\label{sec:discuss}

\section{Conclusion}
\label{sec:con}

%In this section, we analyse the findings in order to offer complete practice guidelines and improvement recommendations for further research.

%\sstitle{Result analysis and implications} 
%Our comparisons between KBQA methods imply a better understanding of the field.
%Our experiments with various KBQA techniques and approaches demonstrate that context-based approach with integration of deep learning outperforms classical KBQA algorithm such as gAnswer for this task. The advantage of these models is that they are more effective at ranking queries and capturing the most feasible queries in a list of queries. Due to the fact that model relies on subgraph matching, it takes longer time to compute than other techniques and performs well on small scale benchmarks such as the QALD series, but not on large scale datasets such as LCQUAD.
%
%On the other hand, for template-based approaches in general, TeBaQA achieves a good result with our new datasets. Moreover, TeBaQA is easy to be compatible with different knowledge bases as latest version of DBPedia. This is one of the better point that traditional system as gAnswer cannot do.  Our study suggests that future KBQA methods should design to be easier to integrate with new knowledge base.
%
%Also, our promising results on deep learning methods might inspire researchers to continue this approach. Furthermore, our new benchmarking dataset, COVID-KBQA, might encourage further COVID-19 research and multilingualism for the diversity of future AI. On the other hand, our comparative study also demonstrates the effectiveness of KG as well as SPARQL query language when compared to SQL with relational KB. 

\sstitle{Performance guidelines}
To assist end users in selecting a suitable solution for a certain application need, we offer the following set of recommendations, which are based on our experimental findings.
\begin{itemize}


  \item In general, context-based methods, such as QAsparql and QAsparql*, are the most effective overall for both F1 and Time. 
%Under ideal conditions, even SOTA KGQA algorithms, such as gAnswer, can be defeated.
The effectiveness of QAsparql decreased with question type. Despite its low computation time, TeBaQA struggles with question types and performs best with multiple criteria.
%Except for MIMICSQL, TREQS outperforms MIMICSQL* and MIMICSPARQL in $Acc_{EX}$ and $Acc_{LF}$. In MIMICSQL, TREQS++ outperforms TREQS, but not in other datasets. MIMICSQL's performance over time demonstrates the superiority of knowledge bases.

  \item In terms of $Acc_{EX}$ and $Acc_{LF}$, TREQS++ outperforms MIMICSQL in nearly all question types.
%, with the exception of How, Request, and What, Which, How, Yes/No. 
TREQS++ is ineffective in MIMICSPARQL, whereas TREQS is effective at $Acc_{EX}$ in How and Request questions. 
%With COVID-KGQA, in term of F1, QAsparql* is the best choice in terms of What, When, Where question types while TeBaQA is the best in How. With LCQUAD, QAsparql and QAsparql* are the winners. Specifically, QAsparql outperforms in What and Who and QAsparql* is better at Where, Which, Whose, How, Yes/No and Request. In contrast, TeBaQA is the best if the relevance  difference between How and Yes/No is the main concern. 
Regarding Time, QAsparql is the first runner up in nearly all kinds of questions, while QAsparql* is the second runner up in a negligible number of questions. 
%But the two other runners-up, TeBaQA and gAnswer, are not significantly slower. 
%We suggest using TeBaQA.

  \item Regarding the effect of the number of questions on the performance of COVID-KGQA, we recommend avoiding the use of gAnswer and QAsparql, as these algorithms are inadequate in this regard. In terms of diversity, QAsparql* and TeBaQA are better. 
%Considering how the number of questions affects the performance of LCQUAD, QAsparql* should be utilised because it returns the most subtopics. Again, TREQS++ outperforms MIMICSQL* and MIMICSQL in terms of $Acc_{EX}$ and $Acc_{LF}$, with the exception of MIMICSPARQL.

  \item Regarding multi-domain and multi-language, we advise avoiding the use of gAnswer and QAsparql, as these algorithms are ineffective in this regard. In terms of variety, QAsparql* and TeBaQA are both outstanding alternatives.
%Regarding the cross-query language aspect, TREQS excels across all datasets, whereas TREQS++ fails to meet this criterion.
  
%   \item For networks with small number of nodes (less or equal than 2000 nodes), spectral methods ran very fast and are superior to representation methods in terms of computa- tion time. However, for network with large number of nodes, spectral methods quickly lost this advantage because of the growth of the size of adjacency matrix and representa- tion based methods should be preferred in this case. Note that representation based methods might ran quite slow for graph with high density as the result of they are more sen- sitive to this factor, real-world datasets often have moderate or low density.
%   \item For networks in which only topology information is available and structural consistency assumption between source and target networks is respected to some extent, representation learning based methods such as PALE and IONE are more appropriate methods, as they are less sensitive to structural noise than spectral methods and without additional infor- mation, the default prior matrix based on degree similarity often has poor quality that adversely affects the performance of spectral techniques such as FINAL and IsoRank.
%   \item For networks in which node attributes are available, REGAL and FINAL are promising solutions because these algorithms can exploit this kind of information to give a solid result. Be- tween these two methods, REGAL should be prioritized be- cause it is more resistant to attribute noise and have faster computation time.
%   \item In general, graph connectivity and number of connected components of the networks does not significantly affect the performance of alignment algorithms. The only two algo- rithm are sensitive to these factor are FINAL and DeepLink. For the networks with low level of graph connectivity or have many number of connected components, we recom- mend avoiding using these algorithms.

\end{itemize}

The findings are also summarized in \autoref{tbl:guideline}, where the best, the second-best and the worst techniques are shown for each percategory.


\begin{table}[!h]
%\begin{minipage}[c]{\linewidth}
\vspace{-1em}
\centering
%\small
\caption{Performance guideline} 
\label{tbl:guideline}
\scalebox{0.7}{
%\begin{threeparttable}
\begin{tabular}{lccc}
    \toprule 
     category  & winner & 1st runner-up & worst \\
    \midrule
    Overall &  QAsparql*  &  QAsparql*  & gAnswer \\
%    Question type (COVID-KGQA) &  QAsparql*  & QAsparql  & gAnswer, QAsparql \\
    Question type &  QAsparql*  & QAsparql  & gAnswer \\
%    Question quantity (COVID-KGQA) &  QAsparql*  & QAsparql  & gAnswer, QAsparql \\
    Question quantity &  QAsparql*  & QAsparql  & gAnswer \\
    $Acc_{EX}$ &  TREQS++  & TREQS  & TREQS \\
    $Acc_{LF}$ &  TREQS++  & TREQS  & TREQS \\
    Multi-domain & QAsparql*   & QAsparql*   & gAnswer \\
    Multi-language & QAsparql*   & QAsparql*   & gAnswer \\
    Cross-Query Language & TREQS  & TREQS  & TREQS++ \\
 \bottomrule
 \end{tabular}
%\end{threeparttable}
%  }
%\end{minipage}
}
\vspace{-1em}
\end{table}

% \sstitle{Generality of the baselines}

% \begin{table}[!h]
% %\begin{minipage}[c]{\linewidth}
% \centering
% \footnotesize
% \caption{Generality of the baselines}
% \label{tbl:generality}
% % \resizebox{1.0\columnwidth}{!}{
% \begin{tabular}{lccc}
%     \toprule 
%      category & best? & worst? & how to? \\
%     \midrule
%     cross-domain & ? & ? & ? \\
%     cross-language & ? & ? & ? \\
%     cross-question & ? & ? & ? \\
%     cross-KB? &  &  &   \\
%  \bottomrule
%  \end{tabular}
% %  }
% %\end{minipage}
% \end{table}




%\begin{itemize}
%    \item In the future, we want to expand the number and quality of the datasets, both in terms of the number of questions and the diversity of question types. The findings of the study also indicate that we should concentrate on techniques that would enhance the performance on lengthy questions and multiple response types.
%    \item Furthermore, we also want to assess with more metrics, such as BLEU-Score. Some experiments \cite{YIN2021510, OCHIENG2020100024} are evaluated using this metric and we want to evaluate the differences between F1 and BLEU-score based on the benchmarking corpora we use in this article as well as with some new datasets as well as the original dataset \cite{10.1007/978-3-030-49461-2_31}, Firebase migrated dataset \cite{azmy-etal-2018-farewell} and Wikidata mapped to DBPedia dataset \cite{dubey2019lc}. Moreover, we will assess these techniques and datasets in terms of other linguistic characteristics in order to get a better understanding.
%\end{itemize}

\sstitle{Summary}
We compared KBQA systems from a variety of perspectives and discussed the ramifications of our findings.
%Experimentation revealed that they were quite sensitive to various environments. In order to reduce this variance, we presented a fine-grained comparative benchmark suite consisting of a prepackaged collection of common benchmarks that target numerous popular knowledge bases and are intended to be easily implemented.
%This framework is simple to use and can be easily customized. Additionally, it offers a fine-grained analysis of the questions that have been processed, based on a variety of factors, in addition to the previously recognized quality ratings for each question. It is possible for users to rapidly understand the strengths and weaknesses of their examined KBQA systems by using the results of this research. 
%In this paper, we also provided a comprehensive comparison of the various approaches to question answering over a knowledge base, as well as the study's findings. 
Experiments were conducted on a wide range of topics, including simple subgraph matching, context-based, deep learning models, and others.
%Numerous hand-engineered analyzing aspects were used in the experiments, with the effects of question taxonomies being one of the most notable examples. 
The results ultimately demonstrated the superiority of deep learning models when combined with a context-based approach, which produced the best results. In addition, as a result of this work, we create the COVID-19 large-scale KGQA dataset, which consisted of over a thousand questions.

%\sstitle{Moving forward with KBQA}
%The above findings enable us to recommend several strategies for performance improvement. First, subsequent studies should check and consider the number of questions and the diversity of question types when developing their techniques. Second, we should concentrate on lengthy questions and multiple response types to enhance the overall performance.


\section*{Acknowledgement}
This research is funded by University of Information Technology, Vietnam National University HoChiMinh City under grant number D1-2022-25

%\section*{Conflict of Interest}
%
%The authors declare that they have no conflict of interest.


%% `Elsevier LaTeX' style
% \bibliographystyle{elsarticle-num}

%% Harvard
%\bibliographystyle{cas-model2-names}
%\biboptions{authoryear}

%% APA style
%\bibliographystyle{model5-names}
%\biboptions{authoryear}

%\linespread{1.0}

%\bibliographystyle{../splncs04}
%{\footnotesize
%\bibliography{../ref,../ref_h}}

\input{bib.txt}

\end{document}

