\begin{figure*}[!t]
\includegraphics[width=\linewidth]{images/overview.png}
\caption{\label{fig:overview}
Overview of our method. We take a multi-view dataset, calibrated cameras and a retopologized mesh as input (a). 
We re-project information from multiple views into each input view to compute color statistics for each observed point (b).
We feed this multi-view information into a convolutional neural network to predict material maps for each view (c).
We finally merge this ensemble of inferred maps into texture space (d) to form a scene-scale material texture atlas.
%Our deep learning method then infers SVBRDF maps for each input image (b), using re-projected information from multiple views, and a bilateral solver for specular and roughness maps. We train this network using a dataset of synthetic images of scenes with a variety of geometries and materials (c). We then combine these inferred maps into texture space (d), resulting in a fully renderable digital version of the scene, that can be rendered freely under new lighting or with additional assets (e).
}
\end{figure*}

\section{Overview}
\label{sec:overview}

Our method takes as input multiple casually-captured unstructured wide-baseline images of a scene from different viewpoints using a DSLR camera. 
This results in multi-view observations, but in some cases capture is incomplete: In particular, lighting and some parts of the scene might be unobserved. Similar to commercial photogrammetry pipelines~\cite{unity2017photo}, first we obtain camera calibration and a rough multi-view stereo mesh using RealityCapture~\cite{reality2016capture}, followed by re-topology where an artist creates a clean version of the mesh, suitable for rendering (Fig.~\ref{fig:overview}(a) and~\ref{fig:geometry}). This reliable 3D geometry is used as input to our method.
%As a result we obtain smooth mesh with holes filled and sharp occlusion edges. 
%We first describe the material model used in our method followed by an overview of the pipeline and the training dataset.
%Then we give an overview of our potential contributions for predicting SVBRDF maps given multi-view information as input to the network in Section.
Given the multiple input images and the corresponding geometry, our goal is to produce an atlas of material parameters, i.e., spatially-varying diffuse albedo~$D$, specular albedo~$S$, and roughness~$R$ for a Cook-Torrance BRDF model~\cite{ct82}. We do not estimate normal maps since we focus on indoor scenes composed of large surfaces seen at a distance, for which our retopologized geometry provides sufficiently accurate normals (Fig.~\ref{fig:overview}(a) top and~\ref{fig:geometry}).

We achieve this material estimation task in two main steps, illustrated in Fig.~\ref{fig:overview} (b - d).
The first step relies on an image-space CNN to predict material maps for each input image separately. For each view, we use the 3D geometry to reproject image colors from other views and deduce color statistics (minimum, median and maximum color) that summarize the view-dependent appearance of each pixel. We complement these statistics with geometry buffers (surface normals and depth). In practice, we found it beneficial to split the prediction task into two tracks, one responsible for the prediction of diffuse albedo and one responsible for the prediction of specular albedo and roughness. 
%\AB{I suggest to add motivation for these choices later on. Same for the description of progressive training and bilateral solver.}

%We first train separate tracks for roughness/specular maps, and then for diffuse albedo \SP{INCORRECT. 
%We first train the diffuse albedo track to get a good baseline for underlying surface color/texture and then train the roughness/specular maps track, followed by a joint fine-tuning step (Fig.~\ref{fig:overview}(b). 

%Finally, the roughness and specular layers are processed with a learned bilateral solver. We provide details of the layers and the network architecture in Sec.~\ref{sec:network}, which is trained on our synthetic scene dataset (c) \SP{Not shown in current overview}, described in Sec.~\ref{sec:dataset}. 
The second step of our method aggregates the per-view predictions into a common texture space to form the final atlas. We use simple median filtering to select a consensus from the ensemble of predictions given by all views where a surface point appears. Mapping this atlas onto the retopologized mesh gives a complete asset that is compatible with traditional physically-based rendering (Fig.~\ref{fig:teaser}(c), \ref{fig:results2})\CK{, including full editing capabilities such as changing the lighting and inserting new objects}.





