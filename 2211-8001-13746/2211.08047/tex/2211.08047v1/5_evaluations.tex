
\begin{figure}[!t]
\includegraphics[width=\linewidth]{images/geometry.png}
\caption{\label{fig:geometry}
Retopologized geometry with selected input images for \textsc{real kitchen}~(left) and \textsc{real hallway}~(right) scenes. 
}
\end{figure}


\begin{figure*}[!h]
\setlength{\tabcolsep}{1pt}
\begin{tabular}{cccccc}
%\hline
%\hline
%\multicolumn{6}{c}{Synthetic Veach Ajar} \\
%\hline

 & \textbf{Synthetic Veach Ajar} & & Diffuse & Roughness & Specular \\
{\rotatebox[origin=lB]{90}{\hspace{0.2in} \small{Input Image}}} & 
\includegraphics[width=.235\linewidth]{images/results/synth_veach/gt/cam62.png} & 
{\rotatebox[origin=lB]{90}{\hspace{0.2in}\small{Ground Truth}}} &
\includegraphics[width=.235\linewidth]{images/results/synth_veach/gt/cam62_albedo.png} & 
\includegraphics[width=.235\linewidth]{images/results/synth_veach/gt/cam62_roughness.png} & 
\includegraphics[width=.235\linewidth]{images/results/synth_veach/gt/cam62_fresnel.png} \\

{\rotatebox[origin=lB]{90}{\hspace{0.2in}\small{Re-render}}} & 
\includegraphics[width=.235\linewidth]{images/results/synth_veach/tx_space/cam62.png} &
{\rotatebox[origin=lB]{90}{\hspace{0.4in}\small{Ours}}} & 
\includegraphics[width=.235\linewidth]{images/results/synth_veach/tx_space/cam62_albedo.png} & 
\includegraphics[width=.235\linewidth]{images/results/synth_veach/tx_space/cam62_roughness.png} & 
\includegraphics[width=.235\linewidth]{images/results/synth_veach/tx_space/cam62_fresnel.png} \\


%\hline
%\multicolumn{6}{c}{Synthetic Dining Room} \\
%\hline

 & \textbf{Synthetic Dining Room} & & Diffuse & Roughness & Specular \\
{\rotatebox[origin=lB]{90}{\hspace{0.2in} \small{Input Image}}} & 
\includegraphics[width=.235\linewidth]{images/results/synth_dining/gt/cam114.png} &
{\rotatebox[origin=lB]{90}{\hspace{0.2in}\small{Ground Truth}}} &
\includegraphics[width=.235\linewidth]{images/results/synth_dining/gt/cam114_albedo.png} & 
\includegraphics[width=.235\linewidth]{images/results/synth_dining/gt/cam114_roughness.png} & 
\includegraphics[width=.235\linewidth]{images/results/synth_dining/gt/cam114_fresnel.png} \\

{\rotatebox[origin=lB]{90}{\hspace{0.2in}\small{Re-render}}} & 
\includegraphics[width=.235\linewidth]{images/results/synth_dining/tx_space/cam114.png} &
{\rotatebox[origin=lB]{90}{\hspace{0.4in}\small{Ours}}} & 
\includegraphics[width=.235\linewidth]{images/results/synth_dining/tx_space/cam114_albedo.png} & 
\includegraphics[width=.235\linewidth]{images/results/synth_dining/tx_space/cam114_roughness.png} & 
\includegraphics[width=.235\linewidth]{images/results/synth_dining/tx_space/cam114_fresnel.png} \\

%\hline
\end{tabular}

\caption{
\label{fig:results1}
Results on two synthetic scenes where the ground truth is available. 
For each scene, in the first row we show an input view and the ground truth diffuse, roughness and specular maps for that view. 
The second row shows the re-rendering followed by the maps obtained using our method for the input view shown.
We are able to reproduce renderings which are close to the input view using the approximate material maps generated by our method.
}

\end{figure*}


\begin{figure*}[h!]
\setlength{\tabcolsep}{1pt}
\begin{tabular}{cccc}
%\hline
  & \textbf{Real Office} & \textbf{Real Kitchen} & \textbf{Real Hallway}  \\
%\hline
{\rotatebox[origin=lB]{90}{\hspace{0.5in}\small{Input Image}}} & 
\includegraphics[width=.352\linewidth]{images/results/real_office/input/00000176.png} & 
\includegraphics[width=.305\linewidth]{images/results/real_kitchen/input/00000114.png} & 
\includegraphics[width=.302\linewidth]{images/results/real_hallway/input/00000212.png} \\

{\rotatebox[origin=lB]{90}{\hspace{0.5in}\small{Re-render}}} & 
\includegraphics[width=.352\linewidth]{images/results/real_office/tx_space/Cam176.png} & 
\includegraphics[width=.305\linewidth]{images/results/real_kitchen/tx_space/Cam114.png} & 
\includegraphics[width=.302\linewidth]{images/results/real_hallway/tx_space/Cam212.png} \\

{\rotatebox[origin=lB]{90}{\hspace{0.5in}\small{Diffuse}}} & 
\includegraphics[width=.352\linewidth]{images/results/real_office/tx_space/Cam176_albedo.png} & 
\includegraphics[width=.305\linewidth]{images/results/real_kitchen/tx_space/Cam114_albedo.png} & 
\includegraphics[width=.302\linewidth]{images/results/real_hallway/tx_space/Cam212_albedo.png} \\

{\rotatebox[origin=lB]{90}{\hspace{0.3in}\small{Roughness}}} & 
\includegraphics[width=.352\linewidth]{images/results/real_office/tx_space/Cam176_roughness.png} & 
\includegraphics[width=.305\linewidth]{images/results/real_kitchen/tx_space/Cam114_roughness.png} & 
\includegraphics[width=.302\linewidth]{images/results/real_hallway/tx_space/Cam212_roughness.png} \\

{\rotatebox[origin=lB]{90}{\hspace{0.5in}\small{Specular}}} & 
\includegraphics[width=.352\linewidth]{images/results/real_office/tx_space/Cam176_fresnel.png} &
\includegraphics[width=.305\linewidth]{images/results/real_kitchen/tx_space/Cam114_fresnel.png} &
\includegraphics[width=.302\linewidth]{images/results/real_hallway/tx_space/Cam212_fresnel.png} \\
%\hline

\end{tabular}
\caption{
\label{fig:results-real}
Results on real scenes. For each scene, we show the re-rendering in closest matched capture conditions and the maps obtained for the given input view using our method.
}
\end{figure*}



\begin{figure*}[h!]
\setlength{\tabcolsep}{1pt}
\begin{tabular}{ccccc}
%\hline
 & Capture Configuration & Modified Lighting & +~Object Insertion &  +~Novel View  \\
%\hline
{\rotatebox[origin=lB]{90}{\hspace{0.5in}\small{\textbf{Real Office}}}} & 
\begin{tikzpicture}[x=4cm, y=7cm, spy using outlines={width=2cm, height=2cm, rectangle,magnification=2}]
\node[anchor=south] (FigA) at (0,0) {
	\includegraphics[width=.23\linewidth]{images/results/relighting_insertion/real_office/Cam176.png} 
};
\spy [smallwindow] on ($(FigA)+(-0.18,- 0.02)$) in node[largewindow,anchor=north west] at ($(FigA.south west)+(0.035,0.00)$);
\spy [smallwindow4] on ($(FigA)+(0.02, 0.04)$) in node[largewindow4,anchor=north east] at ($(FigA.south east)+(-0.035,0.00)$);
\end{tikzpicture}& 
\begin{tikzpicture}[x=4cm, y=7cm, spy using outlines={width=2cm, height=2cm, rectangle,magnification=2}]
\node[anchor=south] (FigA) at (0,0) {
	\includegraphics[width=.23\linewidth]{images/results/relighting_insertion/real_office/relight.png}
};
\spy [smallwindow] on ($(FigA)+(-0.18, -0.02)$) in node[largewindow,anchor=north west] at ($(FigA.south west)+(0.035,0.00)$);
\spy [smallwindow4] on ($(FigA)+(0.02, 0.04)$) in node[largewindow4,anchor=north east] at ($(FigA.south east)+(-0.035,0.00)$);
\end{tikzpicture} & 
\begin{tikzpicture}[x=4cm, y=7cm, spy using outlines={width=2cm, height=2cm, rectangle,magnification=2}]
\node[anchor=south] (FigA) at (0,0) {
	\includegraphics[width=.23\linewidth]{images/results/relighting_insertion/real_office/object.png}
};
\spy [smallwindow] on ($(FigA)+(-0.18, -0.02)$) in node[largewindow,anchor=north west] at ($(FigA.south west)+(0.035,0.00)$);
\spy [smallwindow4] on ($(FigA)+(0.02, 0.04)$) in node[largewindow4,anchor=north east] at ($(FigA.south east)+(-0.035,0.00)$);
\end{tikzpicture} & 
\begin{tikzpicture}[x=4cm, y=7cm, spy using outlines={width=2cm, height=2cm, rectangle,magnification=3}]
\node[anchor=south] (FigA) at (0,0) {
	\includegraphics[width=.23\linewidth]{images/results/relighting_insertion/real_office/view.png} 
};
\spy [smallwindow2] on ($(FigA)+(-0.2, -0.03)$) in node[largewindow2,anchor=north west] at ($(FigA.south west)+(0.035,0.00)$);
\spy [smallwindow3] on ($(FigA)+(0.0, -0.07)$) in node[largewindow3,anchor=north east] at ($(FigA.south east)+(-0.035,0.00)$);
\end{tikzpicture}  \\

%\hline
{\rotatebox[origin=lB]{90}{\hspace{0.5in}\small{\textbf{Real Kitchen}}}} & 
\begin{tikzpicture}[x=4cm, y=7cm, spy using outlines={width=2cm, height=2cm, rectangle,magnification=2}]
\node[anchor=south] (FigA) at (0,0) {
	\includegraphics[width=.23\linewidth]{images/results/relighting_insertion/real_kitchen/Cam114.png} 
};
\spy [smallwindow] on ($(FigA)+(-0.12, 0.05)$) in node[largewindow,anchor=north west] at ($(FigA.south west)+(0.035,0.00)$);
\spy [smallwindow4] on ($(FigA)+(0.25, 0.0)$) in node[largewindow4,anchor=north east] at ($(FigA.south east)+(-0.035,0.00)$);
\end{tikzpicture}& 
\begin{tikzpicture}[x=4cm, y=7cm, spy using outlines={width=2cm, height=2cm, rectangle,magnification=2}]
\node[anchor=south] (FigA) at (0,0) {
	\includegraphics[width=.23\linewidth]{images/results/relighting_insertion/real_kitchen/relight.png}
};
\spy [smallwindow] on ($(FigA)+(-0.12, 0.05)$) in node[largewindow,anchor=north west] at ($(FigA.south west)+(0.035,0.00)$);
\spy [smallwindow4] on ($(FigA)+(0.25, 0.0)$) in node[largewindow4,anchor=north east] at ($(FigA.south east)+(-0.035,0.00)$);
\end{tikzpicture} & 
\begin{tikzpicture}[x=4cm, y=7cm, spy using outlines={width=2cm, height=2cm, rectangle,magnification=2}]
\node[anchor=south] (FigA) at (0,0) {
	\includegraphics[width=.23\linewidth]{images/results/relighting_insertion/real_kitchen/object.png}
};
\spy [smallwindow] on ($(FigA)+(-0.12, 0.05)$) in node[largewindow,anchor=north west] at ($(FigA.south west)+(0.035,0.00)$);
\spy [smallwindow4] on ($(FigA)+(0.25, 0.0)$) in node[largewindow4,anchor=north east] at ($(FigA.south east)+(-0.035,0.00)$);
\end{tikzpicture} & 
\begin{tikzpicture}[x=4cm, y=7cm, spy using outlines={width=2cm, height=2cm, rectangle,magnification=3}]
\node[anchor=south] (FigA) at (0,0) {
	\includegraphics[width=.23\linewidth]{images/results/relighting_insertion/real_kitchen/view.png} 
};
\spy [smallwindow2] on ($(FigA)+(-0.45, 0.03)$) in node[largewindow2,anchor=north west] at ($(FigA.south west)+(0.035,0.00)$);
\spy [smallwindow3] on ($(FigA)+(0.24, 0.08)$) in node[largewindow3,anchor=north east] at ($(FigA.south east)+(-0.035,0.00)$);
\end{tikzpicture}  \\

%\hline
{\rotatebox[origin=lB]{90}{\hspace{0.5in}\small{\textbf{Real Hallway}}}} & 
\begin{tikzpicture}[x=4cm, y=7cm, spy using outlines={width=2cm, height=2cm, rectangle,magnification=2}]
\node[anchor=south] (FigA) at (0,0) {
	\includegraphics[width=.23\linewidth]{images/results/relighting_insertion/real_hallway/Cam212.png} 
};
\spy [smallwindow] on ($(FigA)+(-0.06,- 0.05)$) in node[largewindow,anchor=north west] at ($(FigA.south west)+(0.035,0.00)$);
\spy [smallwindow4] on ($(FigA)+(0.12, 0.08)$) in node[largewindow4,anchor=north east] at ($(FigA.south east)+(-0.035,0.00)$);
\end{tikzpicture}& 
\begin{tikzpicture}[x=4cm, y=7cm, spy using outlines={width=2cm, height=2cm, rectangle,magnification=2}]
\node[anchor=south] (FigA) at (0,0) {
	\includegraphics[width=.23\linewidth]{images/results/relighting_insertion/real_hallway/relight.png}
};
\spy [smallwindow] on ($(FigA)+(-0.06,- 0.05)$) in node[largewindow,anchor=north west] at ($(FigA.south west)+(0.035,0.00)$);
\spy [smallwindow4] on ($(FigA)+(0.12, 0.08)$) in node[largewindow4,anchor=north east] at ($(FigA.south east)+(-0.035,0.00)$);
\end{tikzpicture} & 
\begin{tikzpicture}[x=4cm, y=7cm, spy using outlines={width=2cm, height=2cm, rectangle,magnification=2}]
\node[anchor=south] (FigA) at (0,0) {
	\includegraphics[width=.23\linewidth]{images/results/relighting_insertion/real_hallway/object.png}
	%\includegraphics[width=.24\linewidth]{images/results/relighting_insertion/real_office/object2.png} & 
	%\includegraphics[width=.24\linewidth]{images/results/relighting_insertion/real_office/object3.png} & 

};
\spy [smallwindow] on ($(FigA)+(-0.06,- 0.05)$) in node[largewindow,anchor=north west] at ($(FigA.south west)+(0.035,0.00)$);
\spy [smallwindow4] on ($(FigA)+(0.12, 0.08)$) in node[largewindow4,anchor=north east] at ($(FigA.south east)+(-0.035,0.00)$);
\end{tikzpicture} & 
\begin{tikzpicture}[x=4cm, y=7cm, spy using outlines={width=2cm, height=2cm, rectangle,magnification=3}]
\node[anchor=south] (FigA) at (0,0) {
	%\includegraphics[width=.24\linewidth]{images/results/relighting_insertion/real_office/view.png} 	
	\includegraphics[width=.23\linewidth]{images/results/relighting_insertion/real_hallway/view.png} 
	%\includegraphics[width=.24\linewidth]{images/results/relighting_insertion/real_office/view2.png}  
	%\includegraphics[width=.24\linewidth]{images/results/relighting_insertion/real_office/view3.png}

};
\spy [smallwindow2] on ($(FigA)+(-0.0, 0.03)$) in node[largewindow2,anchor=north west] at ($(FigA.south west)+(0.035,0.00)$);
\spy [smallwindow3] on ($(FigA)+(0.08, -0.02)$) in node[largewindow3,anchor=north east] at ($(FigA.south east)+(-0.035,0.00)$);
\end{tikzpicture}  \\


%\hline
\end{tabular}
\caption{
\label{fig:results2}
Scene editing with our method on real scenes from input and novel viewpoints.
From left to right, we show the scene re-rendered with capture configurations, the same view re-rendered with modified lighting, with a virtual object inserted along with modified lights, and the modified scene from a novel viewpoint which is never captured. 
For example, for the real office scene we have added the statuette; notice  how shadows underneath the red book and orange fruit have been (re)moved correctly on modifying lights (first row, blue and red inset) and the color bleeding of the red box and orange on the statuette (first row, green and yellow inset) due to \emph{correct interaction between real and virtual objects} which cannot be achieved by relighting based object-insertion methods. 
Our method enables insertion of objects with highly complex materials such as in the real kitchen scene, we have inserted a goblet of water and two bottles of wine. Since our method produces digital assets that can be directly used for rendering, our renderings exhibit complex lighting phenomena such as caustics (second row, red inset) and internal reflection (second row, green and yellow inset) which is enabled by our method while removing shadows (second row, blue inset; third row red inset) and correctly predicting the specular material e.g., on the knives and the red shoe-stone in real hallway scene as illustrated by the rendered highlights (second row, red inset; third row, blue inset).
Please see the supplementary video for results with moving paths and lights to better appreciate these effects. 
}
\end{figure*}

\section{Implementation Details}
\label{sec:implementation}
We have implemented our method in python using the PyTorch~\cite{pytorch} framework for deep learning and C++/OpenGL shaders for all steps that require reprojection. 

At inference, we run the CNN over images in $640 \times 384$ resolution. If the aspect ratio does not match, we zero-pad to fit to the nearest multiple resolution. 
We expect input images in linear color space, and we apply a log transformation followed by a normalization to $[-1, 1 ]$ to flatten the dynamic range before processing by the CNN. 
For real images, we 
assume gamma correction of $2.2$ to convert to linear space. 

Our system for dataset generation includes a plug-in for 3DS Max that exports materials into \textsc{Mitsuba}-compatible format using our BRDF model. 
To handle complex material graphs, we evaluate them in 3DS Max and save texture layers subsequently used by \textsc{Mitsuba}. 
%\GD{ADD DETAILS HERE on choices of parameters etc}

%\AB{Provide details about training (learning rate, epoch, training time)}
We first train the diffuse track and then the specular track separately over 15 epochs each, using ground truth maps for the missing components when evaluating the rendering loss.
We then fine-tune the two tracks jointly for 15 epochs. Overall training takes 18 hours on a 4 RTX8000 GPU cluster node.
The confidence networks for roughness and specular maps are trained for $100$ iterations.
We use the \textsc{Adam} optimizer~\cite{kingmaba15} with a fixed learning rate of $2e-5$ for all training.

We will release all source code for our method, including the dataset generation system, and all training images used for our results.


%We first train the diffuse albedo track to get a good baseline for underlying surface color/texture and then train the roughness/specular maps track\AB{while holding the albedo track fixed? Does it matter?}, followed by a joint fine-tuning step (Fig.~\ref{fig:overview}(b). For individual track training, for example when training only the diffuse track, we use predicted albedo along with ground truth specular and roughness maps to render the predicted image while we use all three ground truth maps for ground truth rendering.


%\begin{itemize}
%		\item For inference we provide images in 640x384 resolution. If the aspect ration does not match, we pad by zeros to fit to the nearest multiple resolution. 
%		\item All inputs are provided in linear space. For real images, we apply gamma correction of 2.2.
%		\item We obtain pre-view output maps in linear space. 
%		\item For a dataset of 310 images, it takes about 5-10 minutes to infer all maps through our prediction network. 
%\end{itemize}


%\begin{table}[!t]
%%\setlength{\tabcolsep}{2pt}
%\begin{tabular}{l||ccc}
%%\hline
%Scenes & \#images & Resolution & Lighting condition \\
%\hline \hline
%Veach Ajar & $160$ & $640 \times 384$ & $3$~area \\
%Dining Room & $105$ & $640 \times 384$ & $2$~area + 1 envmap \\
%Real Office & $245$ & $640 \times 355$ & $2$~area \\
%Real Kitchen & $296$ & $640 \times 411$ & $1$~area \\
%Real Hallway & $226$ & $640 \times 414$ & $2$~area + 1 lamp \\
%\hline \hline
%\end{tabular}
%\caption{
%\label{tab:scenes}
%\NEW{Details of scenes used for our experiments.}}
%\end{table}


\begin{table*}[!t]
%\setlength{\tabcolsep}{2pt}
\centering
\begin{tabular}{l||cccccc}
%\hline
Scenes & \#images & Resolution & Pre-processing & Prediction & Texturing ($\times 3$) & Total  \\
\hline \hline
Veach Ajar &  $160$ & $640 \times 384$ & $7.56$ & $3.93$ & $3.12$ & $20.85$ \\
Dining Room & $105$ & $640 \times 384$ & $4.56$ & $3.26$ & $1.32$ & $9.14$ \\
Real Office & $245$ & $640 \times 355$ & $11.48$ & $5.84$ & $4.77$ & $31.63$ \\
Real Kitchen & $296$ & $640 \times 411$ & $32.05$ & $25.25$ & $7.91$ & $81.03$ \\
Real Hallway & $226$ & $640 \times 414$ & $16.10$ & $10.26$ & $7.03$ & $47.45$ \\
\hline \hline
Mean & & & $14.35$ & $9.71$ & $4.83$ & $\textbf{38.02}$ \\
\hline \hline
\end{tabular}
\caption{
\label{tab:timings}
Timing breakdown for each step of our method on the scenes used for our experiments. All times are reported in \textbf{minutes}.}
\end{table*}

\section{Results and Evaluation}
\label{sec:experiments}



We show results and evaluations on two synthetic scenes (Veach Ajar, and  Dining Room), and three real captured scene (Real Office, Real Kitchen, and Real Hallway). 
The capture details and scene lighting conditions are provided in Table~\ref{tab:timings}.
We also provide comparisons on two additional synthetic scenes (Living Room, and Kitchen).
We provide 
%supplemental webpage with all results and comparisons as well as 
a supplemental video showing view dependent effects over paths and image sequences.
We strongly encourage the reader to view the videos to appreciate how our automatically created material maps are directly usable for physically-based rendering and scene editing.


For synthetic scenes, we render a set of views of the scene, and then use these as if they were photographs to run our entire pipeline; we use the original geometry in this case. 
Note that as a result the ground truth materials are encoded as a single material map texture atlas to be comparable with the results of our method. %\SP{Unclear to me what the last sentence conveys.}
For real scenes, we take a set of photos of the scene, paying attention to capture highlights in several views, then run structure-from-motion and multi-view stereo to obtain an initial 3D reconstruction. 
We hired a professional artist to turn these reconstructions into a retopologized mesh (shown in Fig.~\ref{fig:overview}(a) and~\ref{fig:geometry}).
We use Blender automatic UV-unwrapper to unwrap the meshes into texture atlas.
%% GD: prefer not to mention, will tell at rebuttal if asked , which took around \AB{how many?} hours per scene. 
The resolution of the texture atlas is 16Kx16K pixels for the scenes we considered.
%, followed by a retopology step performed by an artist. 

Synthetic data allows quantitative comparisons on both the material maps and the renderings; for real scenes we can only show qualitative results due to the lack of ground truth maps.
%compute quantitative results only on re-renderings of the input views. Note however that the retoplogized geometry does not match the images \emph{exactly} and thus there are reprojection and misalignment errors that occur.
%\GD{Maybe held out from inference, although it doesnt really matter} 
%\SP{We can't really perform quantitative on real scenes even with held-out views since the underlying geometry model is not same as the real one. Maybe it is worth mentioning this distinction in our work from IBR.}

\subsection{Results}
\label{sec:results}

%		\item Augmentations - Real Scenes re-rendered with:
%		\begin{enumerate}
%			\item Different illuminations: demonstrate shadow/highlight removal
%			\item Different materials: demonstrate material editing
%			\item Object insertion: demonstrate shadows, highlights, inter-reflections, caustics 
%		\end{enumerate}

In Fig.~\ref{fig:results1} and \ref{fig:results-real}, 
%we show an input view from each scene (a), 
%visualization of the maps on the (retopologized) geometry (b)-(d) and the re-rendered image (e) in the first row, 
%the corresponding maps from the per-view inference, (b)-(d) in the second row and the
%ground truth maps and re-rendering of our final method in the third row for each synthetic scene.
%we show the scene geometry textured with maps estimated from a single camera versus the final maps merged in texture space.
we show the scene geometry textured with final \emph{approximate} material maps obtained by our method.
For synthetic scenes we also show the ground truth maps.
Additionally, for both synthetic and real scenes,  we show the re-rendered image, i.e., we generate the material map texture atlases using our method and then provide them to a path tracer along with the geometry to render the scene with full global illumination effects.
To achieve a result as close as possible to the input image, 
we place the lights manually to match input conditions as much as possible. 
Despite these approximations, our re-renderings are plausible renditions of the input images, illustrating the efficacy of the approximate material maps we obtain.

%Although, due to the inherent difference between actual scene lighting and the approximate geometry from the input, the global illumination effects are not 100\% accurate but still we are able to reproduce images that are very similar to the input images showing the efficacy of the approximate material maps we obtain.} 

%as well as the re-rendering image using our rendering pipeline, i.e., using the \emph{per-view} ground truth maps to create a texture atlas and then rendering using the same method as with inference. The difference from the input is mainly due to global illumination effects from parts of the scene not seen in the input views that do not have materials assigned (i.e., are black). 

%For synthetic scenes, we see in Fig.~\ref{fig:results1},\ref{fig:results-real} that our method predicts reasonable values for the maps compared to the ground truth and results in a re-rendering that is quite close to the ground truth (far right top), and the input. 


Our method manages to capture the overall material properties of the objects even in real scenes, e.g., in the Real Office scene the desktop and the red box are shiny while the yellow box on the right and the orange are more diffuse. 

In Fig.~\ref{fig:results2} we show results with modified lighting conditions and object insertion from different viewpoints on real scenes.
This figure shows that we achieve our goal of creating plausible material assets for photorealistic scene editing.
For each scene, we show a view in input lighting condition and the same view with modified lighting condition. We are able to remove and move shadows and highlights on most surfaces. 
We further augment the scene by inserting complex objects, such as a metallic statuette in the Real Office scene and a transparent water goblet and wine bottles in the Real Kitchen scene.
Our material maps, together with the retopologized geometry are complete digital assets, and thus allow renderings with full GI interactions between real and virtual objects, 
%provide information about the underlying materials of surrounding objects, these virtual objects interact realistically with the real objects and are able to exhibit complex light phenomena 
such as color bleeding, refraction, caustics and internal reflections when rendered using a path tracer.
We emphasize that such effects are not possible to reproduce using relighting based object insertion methods such as~\cite{karsch2011, karsch2014, gardner2017}.
%Finally we also show the edited scene (with modified lighting and inserted objects) from a novel viewpoint which is never captured before and no material maps are predicted by our network for this particular viewpoint. %% GD said in the beginning

\subsection{Evaluation}
\label{sec:evaluation}

The only other methods designed to handle our input at scene-scale are differentiable rendering approaches~\cite{merlin2021, haefner2021}; unfortunately neither code nor data (in the form of input images we can use for SfM/MVS) is available, precluding direct comparison.
In any case, our method can be seen as complementary and could be used as an initialization for these methods, potentially accelerating their process.
We report the timings for different steps of our method and compare with timings reported by these previous works to support our claims.
Additionally, we present best-effort evaluation using two baselines. 
We also present a set of ablation studies to analyse the effect of our various design choices.



\subsubsection{Speed}
We show the timing breakdown of each step for our method (pre-processing (\textsc{Pre-processing}), single-view prediction (\textsc{Prediction}), and texture atlas generation (\textsc{Texturing}) on a system with an Intel Xeon Gold 5218 2.30GHz Processor and Quadro RTX 5000 GPU, in Tab.~\ref{tab:timings}.
%we need $\sim8 min$ to perform inference for a typical dataset of around 300 images, while reprojection into texture space requires $\sim15 min$. Pre-processing to compute the statistics or re-projected colors into each input view requires approximately $\sim15-30 min$. 
We observe that it is possible to create renderable assets from a multi-view dataset with approximately $30~minutes$ to an hour of computation depending on number of images and their resolution. 
Previous works dealing with scene-scale material estimation~\cite{merlin2021, haefner2021} report around $10-12$ hours for a complete scene optimization. 
Thus, our method is able to produce renderable material maps in \emph{a fraction of time} as compared to previous works.
Note that the timings reported do not include the time taken for reconstruction and re-topology of the mesh.
% since we do not consider it a part of the pipeline.}
%\SP{I have moved timings from implementation details to here.}


%\begin{table}[!t]
%\setlength{\tabcolsep}{2pt}
%\begin{tabular}{l||cc|cc}
%\hline
% & \multicolumn{2}{c|}{\textbf{Veach Ajar}} & \multicolumn{2}{c}{\textbf{Dining Room}}\\
%Method & PSNR $\uparrow$ & DSSIM $\downarrow$ & PSNR $\uparrow$ & DSSIM $\downarrow$ \\
%\hline \hline
%\textsc{ \small{ \textsc{LiEtAl} } } &\small{12.974275} & \small{0.370910} & \small{21.335057} &\small{0.359676}\\
%\textsc{ \small{ \textsc{Texture} } } &\small{18.701869} & \small{0.303566} & \small{15.123681}& \small{0.618277} \\
%\textsc{ \small{ Ours } } & \textbf{\small{21.533800}} & \textbf{\small{0.266334}} & \textbf{\small{21.773710}} & \textbf{\small{0.338615}} \\
%\hline \hline
%\end{tabular}
%\caption{
%\label{tab:comparisons}
% Quantitative comparison. Our method performs best for both Veach Ajar and Dining Room scenes which supports our qualitative observations~(see also Fig.~\protect{\ref{fig:comparisons1}}).
%}
%\end{table}

\begin{table}[!t]
\setlength{\tabcolsep}{2pt}
\begin{tabular}{l||ccc||ccc}
%\hline\hline
 & \multicolumn{3}{c||}{PSNR $\uparrow$} & \multicolumn{3}{c}{DSSIM $\downarrow$}\\
Method & \textsc{LiEtAl} & \textsc{Texture} & \textsc{Ours} & \textsc{LiEtAl} & \textsc{Texture} & \textsc{Ours} \\
\hline \hline
\textbf{Veach Ajar} &\small{12.97} &  \small{18.70} & \textbf{\small{21.53}} & \small{0.37} &\small{0.30} & \textbf{\small{0.27}}\\
\textbf{Dining Room} & \small{21.33}  & \small{15.12}& \textbf{\small{21.77}}&\small{0.36} & \small{0.62}  & \textbf{\small{0.34}} \\
\textbf{Living Room} & \small{13.67}  & \small{19.97}& \textbf{\small{25.61}}&\small{0.44} & \small{0.44}  & \textbf{\small{0.16}} \\
%\textbf{Bathroom} & \small{14.12}  & \small{14.83}& \textbf{\small{20.27}}&\small{0.41} & \small{0.43}  & \textbf{\small{0.32}} \\
\textbf{Kitchen} & \small{9.06}  & \small{16.54}& \textbf{\small{21.28}}&\small{0.52} & \small{0.42}  & \textbf{\small{0.30}} \\
\hline\hline
\textbf{Mean} & \small{14.26}  & \small{17.58}& \textbf{\small{22.55}}&\small{0.42} & \small{0.44}  & \textbf{\small{0.27}} \\
\hline \hline
\end{tabular}
\caption{
\label{tab:comparisons}
%\TODO{RECOMPUTE NUMBERS}
 Quantitative comparison on 4 synthetic scenes. Our method performs best across the scenes which supports our qualitative observations~(see also Fig.~\protect{\ref{fig:comparisons1} and \ref{fig:comparisons3}}).
}
\end{table}

\begin{figure*}[!h]
\setlength{\tabcolsep}{1pt}
\begin{tabular}{ccccc}
%\hline
& \multicolumn{2}{c}{\textbf{Synthetic Veach Ajar}} & \multicolumn{2}{c}{\textbf{Synthetic Dining Room}}\\
%\hline
& Input View & Novel View & Input View & Novel View \\
%\hline
{\rotatebox[origin=lB]{90}{\small{Original Light GT}}} & 
%\includegraphics[width=.24\linewidth]{images/comparisons/synth_kitchen/input/cam0.png} & 
%\includegraphics[width=.24\linewidth]{images/comparisons/synth_kitchen/input/cam1.png} &
\includegraphics[width=.24\linewidth]{images/comparisons/synth_veach/input/cam0.png} & 
\includegraphics[width=.24\linewidth]{images/comparisons/synth_veach/input/cam1.png} &
\includegraphics[width=.24\linewidth]{images/comparisons/synth_dining/input/cam0.png} & 
\includegraphics[width=.24\linewidth]{images/comparisons/synth_dining/input/cam3.png} \\

{\rotatebox[origin=lB]{90}{\small{Modified Light GT}}} &
%\includegraphics[width=.24\linewidth]{images/comparisons/synth_kitchen/gt/cam0.png} & 
%\includegraphics[width=.24\linewidth]{images/comparisons/synth_kitchen/gt/cam1.png} &
\includegraphics[width=.24\linewidth]{images/comparisons/synth_veach/gt/cam0.png} & 
\includegraphics[width=.24\linewidth]{images/comparisons/synth_veach/gt/cam1.png} &
\includegraphics[width=.24\linewidth]{images/comparisons/synth_dining/gt/cam0.png} & 
\includegraphics[width=.24\linewidth]{images/comparisons/synth_dining/gt/cam3.png} \\

{\rotatebox[origin=lB]{90}{\hspace{0.35in}\small{Ours}}} &
%\includegraphics[width=.24\linewidth]{images/comparisons/synth_kitchen/ours/cam0.png} & 
%\includegraphics[width=.24\linewidth]{images/comparisons/synth_kitchen/ours/cam1.png} &
\includegraphics[width=.24\linewidth]{images/comparisons/synth_veach/ours/cam0.png} & 
\includegraphics[width=.24\linewidth]{images/comparisons/synth_veach/ours/cam1.png} &
\includegraphics[width=.24\linewidth]{images/comparisons/synth_dining/ours/cam0.png} & 
\includegraphics[width=.24\linewidth]{images/comparisons/synth_dining/ours/cam3.png} \\

{\rotatebox[origin=lB]{90}{\hspace{0.25in}\small{\textsc{LiEtAl}}}} &
%\includegraphics[width=.24\linewidth]{images/comparisons/synth_kitchen/li/cam0.png} & 
%\includegraphics[width=.24\linewidth]{images/comparisons/synth_kitchen/li/cam1.png} &
\includegraphics[width=.24\linewidth]{images/comparisons/synth_veach/li/cam0.png} & 
\includegraphics[width=.24\linewidth]{images/comparisons/synth_veach/li/cam1.png} &
\includegraphics[width=.24\linewidth]{images/comparisons/synth_dining/li/cam0.png} & 
\includegraphics[width=.24\linewidth]{images/comparisons/synth_dining/li/cam3.png} \\

{\rotatebox[origin=lB]{90}{\hspace{0.25in}\small{\textsc{Texture}}}} &
%\includegraphics[width=.24\linewidth]{images/comparisons/synth_kitchen/tx/cam0.png} &
%\includegraphics[width=.24\linewidth]{images/comparisons/synth_kitchen/tx/cam1.png} &
\includegraphics[width=.24\linewidth]{images/comparisons/synth_veach/tx/cam0.png} &
\includegraphics[width=.24\linewidth]{images/comparisons/synth_veach/tx/cam1.png} &
\includegraphics[width=.24\linewidth]{images/comparisons/synth_dining/tx/cam0.png} &
\includegraphics[width=.24\linewidth]{images/comparisons/synth_dining/tx/cam3.png} \\ 

%\hline
\end{tabular}
\caption{
\label{fig:comparisons1}
Comparisons with baselines on synthetic scenes;
For each scene, we show an input view and a novel view; in the first row we show the viewpoint with original input lighting condition (\textsc{Original Light GT}). 
From second row onwards we modify the lighting in the scene and show the same view re-rendered with modified lighting condition using ground truth maps (\textsc{Modified Light GT}), our maps (\textsc{Ours}), maps produced by \textsc{LiEtAl} and the baseline static \textsc{Texture} generated using the input images. 
Column 1 and 3 corresponds to an input view while column 2 and 4 corresponds to a novel view for which no maps were predicted. 
Notice how \textsc{LiEtAl} fails to reconstruct highlight properly and the \textsc{Texture} is composed of pre-baked highlights and shadows from original lighting condition.
}
\end{figure*}


\begin{figure*}[!h]
\setlength{\tabcolsep}{1pt}
\begin{tabular}{ccccccc}
%\hline
&  \multicolumn{2}{c}{\textbf{Synthetic Living Room}}  &  \multicolumn{2}{c}{\textbf{Synthetic Kitchen}}\\
%\hline
& Input View & Novel View &  Input View & Novel View\\
%\hline
{\rotatebox[origin=lB]{90}{\small{Original Light GT}}} & 

\includegraphics[width=.24\linewidth]{images/comparisons/synth_living/input/cam0.png} & 
\includegraphics[width=.24\linewidth]{images/comparisons/synth_living/input/cam1.png} &
%\includegraphics[width=.16\linewidth]{images/comparisons/synth_bath/input/cam0.png} & 
%\includegraphics[width=.16\linewidth]{images/comparisons/synth_bath/input/cam1.png} &
\includegraphics[width=.24\linewidth]{images/comparisons/synth_kitchen/input/cam0.png} &
\includegraphics[width=.24\linewidth]{images/comparisons/synth_kitchen/input/cam1.png} \\

{\rotatebox[origin=lB]{90}{\small{Modified Light GT}}} &

\includegraphics[width=.24\linewidth]{images/comparisons/synth_living/gt/cam0.png} & 
\includegraphics[width=.24\linewidth]{images/comparisons/synth_living/gt/cam1.png} &
%\includegraphics[width=.16\linewidth]{images/comparisons/synth_bath/gt/cam0.png} & 
%\includegraphics[width=.16\linewidth]{images/comparisons/synth_bath/gt/cam1.png} &
\includegraphics[width=.24\linewidth]{images/comparisons/synth_kitchen/gt/cam0.png} &
\includegraphics[width=.24\linewidth]{images/comparisons/synth_kitchen/gt/cam1.png} \\

{\rotatebox[origin=lB]{90}{\hspace{0.35in}\small{Ours}}} &
\includegraphics[width=.24\linewidth]{images/comparisons/synth_living/ours/cam0.png} & 
\includegraphics[width=.24\linewidth]{images/comparisons/synth_living/ours/cam1.png} &
%\includegraphics[width=.16\linewidth]{images/comparisons/synth_bath/ours/cam0.png} & 
%\includegraphics[width=.16\linewidth]{images/comparisons/synth_bath/ours/cam1.png} &
\includegraphics[width=.24\linewidth]{images/comparisons/synth_kitchen/ours/cam0.png} &
\includegraphics[width=.24\linewidth]{images/comparisons/synth_kitchen/ours/cam1.png} \\

{\rotatebox[origin=lB]{90}{\hspace{0.25in}\small{\textsc{LiEtAl}}}} &
\includegraphics[width=.24\linewidth]{images/comparisons/synth_living/li/cam0.png} & 
\includegraphics[width=.24\linewidth]{images/comparisons/synth_living/li/cam1.png} &
%\includegraphics[width=.16\linewidth]{images/comparisons/synth_bath/li/cam0.png} & 
%\includegraphics[width=.16\linewidth]{images/comparisons/synth_bath/li/cam1.png} &
\includegraphics[width=.24\linewidth]{images/comparisons/synth_kitchen/li/cam0.png} &
\includegraphics[width=.24\linewidth]{images/comparisons/synth_kitchen/li/cam1.png} \\

{\rotatebox[origin=lB]{90}{\hspace{0.25in}\small{\textsc{Texture}}}} &
\includegraphics[width=.24\linewidth]{images/comparisons/synth_living/tx/cam0.png} &
\includegraphics[width=.24\linewidth]{images/comparisons/synth_living/tx/cam1.png} &
%\includegraphics[width=.16\linewidth]{images/comparisons/synth_bath/tx/cam0.png} &
%\includegraphics[width=.16\linewidth]{images/comparisons/synth_bath/tx/cam1.png} &
\includegraphics[width=.24\linewidth]{images/comparisons/synth_kitchen/tx/cam0.png} &
\includegraphics[width=.24\linewidth]{images/comparisons/synth_kitchen/tx/cam1.png} \\ 

%\hline
\end{tabular}
\caption{
\label{fig:comparisons3}
More comparisons with baselines on synthetic scenes;
The layout is the same as for Fig.~\ref{fig:comparisons1}.
Similarly as in Fig.~\ref{fig:comparisons1} we see that our approach shows much better variations of appearance, i.e., shiny materials, compared to the \textsc{LiEtAl} that struggles with shiny material sand texture details and \textsc{Texture} that contains pre-baked highlights and shadows from input images owing to its diffuse and static nature.
%For each scene, we show an input view and a novel view; in the first row we show the viewpoint with original input lighting condition (\textsc{Original Light GT}). 
%From second row onwards we modify the lighting in the scene and show the same view re-rendered with modified lighting condition using ground truth maps (\textsc{Modified Light GT}), our maps (\textsc{Ours}), maps produced by \textsc{LiEtAl} and the baseline static \textsc{Texture} generated using the input images. 
%Column 1 and 3 corresponds to an input view while column 2 and 4 corresponds to a novel view for which no maps were predicted. 
%Notice how \textsc{LiEtAl} fails to reconstruct highlight properly and the \textsc{Texture} is composed of pre-baked highlights and shadows from original lighting condition.
}
\end{figure*}

\subsubsection{Comparisons}

We compare to two baselines: 
%the first uses the retopologized geometry and performs median-filtered texture mapping (\textsc{Texture}), 
The first mimics current practice in digital content creation and uses the retopologized geometry to project the input images into texture space, using median filtering to create an RGB atlas (\textsc{Texture}), 
while the second is based on the single image method of Li et al.~\cite{li2020invrender} (\textsc{LiEtAl}). Specifically, we run the method of Li and colleagues to generate maps for each input view, then we run the same pipeline as for our method to create a material texture atlas for the scene from the multiple predicted maps. We implement their BRDF model in \textsc{Mitsuba} to generate the re-renderings.

We perform quantitative and qualitative comparisons for our method compared to the two baselines. Since the first baseline does not estimate maps, and \textsc{LiEtAl} infers maps for a different BRDF model, we only compare re-renderings, i.e., we re-render the scene for a set of views. 
We perform quantitative comparisons by computing PSNR and DSSIM error~\cite{loza2006structural}.
We show the numerical results in Tab.~\ref{tab:comparisons}, and a visual comparison in Fig.~\ref{fig:comparisons1} and \ref{fig:comparisons3}. 
For the visual comparison, we show the input view on which the maps are predicted. In line with our aim of generating plausible material assets for scene editing, we generate ground truth re-renderings of a modified scene by rendering the scene with modified lighting using ground truth SVBRDF maps. To show how our method compares against the two baseline for this task, we re-render the scene with modified lighting but with the material maps obtained by the three methods.
We see that our approach shows much better variations of appearance, i.e., shiny materials, compared to the \textsc{LiEtAl} that struggles to identify shiny materials and \textsc{Texture} where everything is diffuse by construction.
Our method works well for novel views which we show by re-rendering a novel view for each scene. This view was never seen by the network and no maps were predicted for this viewpoint.
%For the visual comparison, we show the input image, and a re-rendering of the same viewpoint with our method, \textsc{LiEtAl} and \textsc{Texture}.
The quantitative results are computed on $10$ views of each scene selected from a rendered path; we also show a video comparison on the rendered path along with the ground truth in the supplemental video. 
We see in Tab.~\ref{tab:comparisons} that our method is numerically best for both synthetic scenes on both PSNR and DSSIM metrics. 
In Veach Ajar scene our method performs significantly better than the baselines as we can see how our method is able to approximately recover the gradation in specularity between the teapots while both the baselines fails to do so.
In Dining Room scene, numerically our results are better yet close to \textsc{LiEtAl}, although it's worth noticing that visually our method is able to do much better. For example, we are able to recover the spatial variation in roughness of the table top which \textsc{LiEtAl} fails to do and ends up over-smoothing the diffuse albedo. This shows the advantage of using multi-view information which helps the network infer the spatially-varying nature of roughness locally as well as globally for each surface point.

Since we do not have ground truth for real scenes, we show qualitative results on \textsc{Real Office} scene in Figure~\ref{fig:comparisons2}. We show $5$ re-renderings of the scene with modified lighting using our method and the baselines.
In line with our observations on synthetic scenes, we observe that our method is able to predict the variations more accurately than \textsc{LiEtAl} which fails to reconstruct highlights on glossy surfaces such as the tabletop and red box. Compared to \textsc{Texture} we can easily see that the surfaces do not reflect the change in lighting and the baked-in shadows and highlights are still present due to the static and diffuse nature of the texture; please see the video to appreciate the visual importance of this effect.


\begin{figure*}[!h]
\setlength{\tabcolsep}{1pt}
\begin{tabular}{cccccc}
%\hline
 & Frame 1 & Frame 2 & Frame 3 & Frame 4 & Frame 5 \\
%\hline
{\rotatebox[origin=lB]{90}{\hspace{0.2in}\small{Ours}}} &
\includegraphics[width=.19\linewidth]{images/comparisons_relight/ours/Cam00.png} & 
\includegraphics[width=.19\linewidth]{images/comparisons_relight/ours/Cam011.png} & 
\includegraphics[width=.19\linewidth]{images/comparisons_relight/ours/Cam023.png} & 
\includegraphics[width=.19\linewidth]{images/comparisons_relight/ours/Cam035.png} & 
\includegraphics[width=.19\linewidth]{images/comparisons_relight/ours/Cam047.png} \\ 
{\rotatebox[origin=lB]{90}{\hspace{0.2in}\small{\textsc{LiEtAl}}}} &
\includegraphics[width=.19\linewidth]{images/comparisons_relight/li/Cam00.png} & 
\includegraphics[width=.19\linewidth]{images/comparisons_relight/li/Cam011.png} & 
\includegraphics[width=.19\linewidth]{images/comparisons_relight/li/Cam023.png} & 
\includegraphics[width=.19\linewidth]{images/comparisons_relight/li/Cam035.png} & 
\includegraphics[width=.19\linewidth]{images/comparisons_relight/li/Cam047.png} \\ 
{\rotatebox[origin=lB]{90}{\hspace{0.2in}\small{\textsc{Texture}}}} &
\includegraphics[width=.19\linewidth]{images/comparisons_relight/textured/Cam00.png} & 
\includegraphics[width=.19\linewidth]{images/comparisons_relight/textured/Cam011.png} & 
\includegraphics[width=.19\linewidth]{images/comparisons_relight/textured/Cam023.png} & 
\includegraphics[width=.19\linewidth]{images/comparisons_relight/textured/Cam035.png} & 
\includegraphics[width=.19\linewidth]{images/comparisons_relight/textured/Cam047.png} \\ 
%\hline
\end{tabular}
\caption{
\label{fig:comparisons2}
%\TODO{Update \textsc{Li} and annotate.}
Comparison with baselines in \textsc{Real Office} scene. We render a novel viewpoint with changing lighting conditions and show $5$ frames with different lighting conditions. Notice how our method (re)move shadows (red arrow) and reconstruct highlights (green arrow) accurately while both \textsc{LiEtAl} and \textsc{Texture} fail to do so. Please see the supplemental video to appreciate the smooth transition of the highlights and shadows as a result of movement of the light sources.
} 
\end{figure*}


\begin{table}[!t]
%\setlength{\tabcolsep}{3pt}
%\small{
\centering
\begin{tabular}{l||ccc}

\multicolumn{4}{c}{\textbf{Synthetic Veach Ajar}} \\
\hline \hline
 & \multicolumn{3}{c}{MSE $\downarrow$}    \\
%\hline
		
Method & Diffuse & Roughness & Specular \\
\hline
Ours  & $\textbf{0.022135}$ & $\textbf{0.055563}$ & $\textbf{0.016522}$  \\
Ours Im. & $0.030718$ & $0.064362$ & $0.049969$  \\
%No RP Tex. Space  & $0.197676$ & $0.418624$ & $0.019125$ \\
No RP  & $0.141682$ & $0.372725$ & $0.055459$ \\
		
\hline \hline
\multicolumn{4}{c}{\textbf{Synthetic Dining Room}} \\
\hline \hline
& \multicolumn{3}{c}{MSE $\downarrow$}   \\
%\hline 
Method & Diffuse & Roughness & Specular \\
\hline
Ours  & $\textbf{0.026491}$ & $0.071609$ & $0.028250$  \\
Ours Im. & $0.026642$ & $0.077578$ & $\textbf{0.024123}$  \\
%No RP Tex. Space & $0.05345$ & $0.04616$ & $0.033434$ \\
No RP & $0.053104$ & $\textbf{0.048222}$ & $0.032385$ \\
\hline \hline
\end{tabular}
%}
\caption{
\label{tab:ablations}
Quantitative evaluation for the ablation on the synthetic scenes  on the material maps, with 1) no reprojected statistics, 2) no multi-view merge in texture space. We see an increase in multi-view information helps improve the maps quality and/or consistency across views~(see also Tab.~\protect{\ref{tab:ablations1}} and Fig.~\protect{\ref{fig:ablations1}}).
}
\end{table}

\begin{table}[!t]
%\setlength{\tabcolsep}{2pt}
\begin{tabular}{l||cc|cc}
%\hline
 & \multicolumn{2}{c|}{\textbf{Veach Ajar}} & \multicolumn{2}{c}{\textbf{Dining Room}}\\
Method & PSNR $\uparrow$ & DSSIM $\downarrow$ & PSNR $\uparrow$ & DSSIM $\downarrow$ \\
\hline \hline
\textsc{ \small{ \textsc{Ours} } } &\small{$\textbf{19.520969}$} & \small{$\textbf{0.190185}$} & \small{$\textbf{19.35363}$} &\small{$\textbf{0.244253}$}\\
\textsc{ \small{ \textsc{No RP} } } &\small{$11.338421$} & \small{$0.413321$} & \small{$17.675568$}& \small{$0.343187$} \\
\hline \hline
\end{tabular}
\caption{
\label{tab:ablations1}
 Quantitative evaluation for the ablation on the synthetic scenes on the re-renderings, with 1) no reprojected statistics, 2) no multi-view merge in texture space. Using multi-view statistics achieves better re-rendering quality than using only a single view~(see also Tab.~\protect{\ref{tab:ablations}} and Fig.~\protect{\ref{fig:ablations1}}).
}
\end{table}



\begin{figure*}[h!]
\setlength{\tabcolsep}{1pt}
\begin{center}
\begin{tabular}{ccccc}
%\hline
%\hline
%& Input & Diffuse & Roughness & Specular\\
%-------------View 1-----------------
%\hline
{\rotatebox[origin=lB]{90}{\hspace{0.3in}\small{Input}}} & 
\includegraphics[width=.2\linewidth]{images/ablations/synth_dining/gt/cam96.png} & 
\includegraphics[width=.2\linewidth]{images/ablations/synth_dining/gt/cam129.png} & 
\includegraphics[width=.2\linewidth]{images/ablations/synth_dining/gt/cam96.png} & 
\includegraphics[width=.2\linewidth]{images/ablations/synth_dining/gt/cam129.png} \\


& \multicolumn{2}{c}{Diffuse} & \multicolumn{2}{c}{Roughness} \\

{\rotatebox[origin=lB]{90}{\hspace{0.1in}\small{Ground Truth}}} & 
\includegraphics[width=.2\linewidth]{images/ablations/synth_dining/gt/cam96_albedo.png} & 
\includegraphics[width=.2\linewidth]{images/ablations/synth_dining/gt/cam129_albedo.png} & 
\includegraphics[width=.2\linewidth]{images/ablations/synth_dining/gt/cam96_roughness.png} & 
\includegraphics[width=.2\linewidth]{images/ablations/synth_dining/gt/cam129_roughness.png} \\

{\rotatebox[origin=lB]{90}{\hspace{0.3in}\small{Ours}}} & 
\includegraphics[width=.2\linewidth]{images/ablations/synth_dining/ours/cam96_albedo.png}  &
\includegraphics[width=.2\linewidth]{images/ablations/synth_dining/ours/cam129_albedo.png}  &
\includegraphics[width=.2\linewidth]{images/ablations/synth_dining/ours/cam96_roughness.png} & 
\includegraphics[width=.2\linewidth]{images/ablations/synth_dining/ours/cam129_roughness.png} \\

{\rotatebox[origin=lB]{90}{\small{Ours Im. Space}}} & 
\includegraphics[width=.2\linewidth]{images/ablations/synth_dining/ours_im/cam96_albedo.png} & 
\includegraphics[width=.2\linewidth]{images/ablations/synth_dining/ours_im/cam129_albedo.png} & 
\includegraphics[width=.2\linewidth]{images/ablations/synth_dining/ours_im/cam96_roughness.png} & 
\includegraphics[width=.2\linewidth]{images/ablations/synth_dining/ours_im/cam129_roughness.png} \\ 

{\rotatebox[origin=lB]{90}{\hspace{0.2in}\small{No Reproj.}}} & 
\includegraphics[width=.2\linewidth]{images/ablations/synth_dining/no_rp/cam96_albedo.png} & 
\includegraphics[width=.2\linewidth]{images/ablations/synth_dining/no_rp/cam129_albedo.png} & 
\includegraphics[width=.2\linewidth]{images/ablations/synth_dining/no_rp/cam96_roughness.png} & 
\includegraphics[width=.2\linewidth]{images/ablations/synth_dining/no_rp/cam129_roughness.png} \\

%\hline
& \multicolumn{2}{c}{Specular} & \multicolumn{2}{c}{Re-render} \\
%\hline
{\rotatebox[origin=lB]{90}{\hspace{0.1in}\small{Ground Truth}}} & 
\includegraphics[width=.2\linewidth]{images/ablations/synth_dining/gt/cam96_fresnel.png}  &
\includegraphics[width=.2\linewidth]{images/ablations/synth_dining/gt/cam129_fresnel.png}  &
\includegraphics[width=.2\linewidth]{images/ablations/synth_dining/gt/cam96.png}  &
\includegraphics[width=.2\linewidth]{images/ablations/synth_dining/gt/cam129.png}  \\

{\rotatebox[origin=lB]{90}{\hspace{0.3in}\small{Ours}}} & 
\includegraphics[width=.2\linewidth]{images/ablations/synth_dining/ours/cam96_fresnel.png} &
\includegraphics[width=.2\linewidth]{images/ablations/synth_dining/ours/cam129_fresnel.png} &
\includegraphics[width=.2\linewidth]{images/ablations/synth_dining/ours/cam96.png} &
\includegraphics[width=.2\linewidth]{images/ablations/synth_dining/ours/cam129.png} \\

{\rotatebox[origin=lB]{90}{\small{Ours Im. Space}}} &  
\includegraphics[width=.2\linewidth]{images/ablations/synth_dining/ours_im/cam96_fresnel.png} &
\includegraphics[width=.2\linewidth]{images/ablations/synth_dining/ours_im/cam129_fresnel.png} &
 &  \\

{\rotatebox[origin=lB]{90}{\hspace{0.2in}\small{No Reproj.}}} & 
\includegraphics[width=.2\linewidth]{images/ablations/synth_dining/no_rp/cam96_fresnel.png} &
\includegraphics[width=.2\linewidth]{images/ablations/synth_dining/no_rp/cam129_fresnel.png} &
\includegraphics[width=.2\linewidth]{images/ablations/synth_dining/no_rp/cam96.png}  & 
\includegraphics[width=.2\linewidth]{images/ablations/synth_dining/no_rp/cam129.png}  \\ 
%\hline
\end{tabular}
\end{center}
\caption{
\label{fig:ablations1}
Example images from \textsc{Synthetic Dining Room} showing the effect of increasing multi-view information on results.
Note how the quality of the maps is significantly improved by using the reprojected statistics observed in our image space predicted maps as compared to no reprojection, i.e. using only a single image.
Furthermore, gathering the image space maps in texture space helps improve the consistency of the maps across views and thus improves re-rendering by assigning same material in local regions (esp. in roughness and specular maps). 
As a result, re-rendering is closer to ground truth.
}
\end{figure*}



\subsection{Ablations}

\subsubsection{Multi-view Reprojections}
\label{sec:analysis}
%We performed the following ablations: (a) Joint vs. Individual track training + fine tuning, (b)
%Specular network: With/without additional buffers \GD{SPECIFY}

We perform a first ablation on the two main components of our algorithm: 1) we remove the statistics reprojected from other views (No RP) and 2) we remove the merging of maps in texture space (Ours Im.). We show quantitative comparisons, where we provide error in form of Mean Squared Error (MSE) for the three maps computed for $10$ randomly selected input views.

We show quantitative results in Tab.~\ref{tab:ablations},\ref{tab:ablations1} and an example of the visual effect of the different cases of increasing multi-view information with each step
%\GD{algorithmic} choices \GD{we made} 
in Fig.~\ref{fig:ablations1}. 
Using all our components improves results in the majority of cases. 
%We show visual results in Fig.~\ref{fig:ablations1} for the Dining Room scene. 
The use of the reprojected image statistics makes a very significant difference in the quality of the maps. 
While merging in texture space may not significantly improve quality, it helps with increasing consistency between different views for the underlying surface material properties (especially for roughness and specular maps) and thus helps improve quality of the final re-rendering. Reprojection improves roughness on most of the objects in the scene, but sometimes has a negative effect on background parts that lack observations (see Fig.~\ref{fig:ablations1}); this explains why No-RP has better MSE for roughness in Dining Room in Tab.~\ref{tab:ablations}. 
% \GD{REMOVE? The use of Spherical Gaussian lights is less pronounced, but does help slightly in the estimation of the specular map, which in turn has a small effect on the intensity of highlights.}

\subsubsection{Inaccurate Geometry}

\begin{figure}[!t]
\setlength{\tabcolsep}{1pt}
\begin{tabular}{ccc}
%\hline
 & MVS Mesh & Re-topologized Mesh \\
%\hline
{\rotatebox[origin=lB]{90}{\hspace{0.2in}\small{Re-render}}} & 
\includegraphics[width=.48\linewidth]{images/geometry/mvs/Cam176.png} &
\includegraphics[width=.48\linewidth]{images/geometry/ours/Cam176.png} \\ 

{\rotatebox[origin=lB]{90}{\hspace{0.2in}\small{Albedo}}} & 
\includegraphics[width=.48\linewidth]{images/geometry/mvs/Cam176_albedo.png} &
\includegraphics[width=.48\linewidth]{images/geometry/ours/Cam176_albedo.png} \\ 

{\rotatebox[origin=lB]{90}{\hspace{0.2in}\small{Roughness}}} & 
\includegraphics[width=.48\linewidth]{images/geometry/mvs/Cam176_roughness.png} &
\includegraphics[width=.48\linewidth]{images/geometry/ours/Cam176_roughness.png} \\ 

{\rotatebox[origin=lB]{90}{\hspace{0.2in}\small{Specular}}} & 
\includegraphics[width=.48\linewidth]{images/geometry/mvs/Cam176_fresnel.png} &
\includegraphics[width=.48\linewidth]{images/geometry/ours/Cam176_fresnel.png} \\ 

%\hline
\end{tabular}
\caption{
\label{fig:mvs}
%\TODO{Update \textsc{Li} and annotate.}
Effect of inaccurate geometry. 
We run our pipeline on the MVS mesh obtained directly from~\cite{reality2016capture}. The results from the MVS mesh is on the left column and our re-topologized mesh on the right column.
We show a re-rendering in the first row, followed by the albedo, roughness and specular maps obtained using our method in subsequent rows respectively.}
 
\end{figure}

We study the robustness of our method for inaccurate geometry by running our pipeline on the degraded mesh obtained directly from multi-view stereo (MVS)~\cite{reality2016capture} which consists of large holes and bumpy surfaces. 
\NEW{The bumpy surfaces are an instance of extreme vertex perturbation.}
We show a qualitative comparison in Fig.~\ref{fig:mvs}.
From the figure we can confirm that the maps obtained from the MVS mesh is only slightly degraded compared to the re-topologized mesh. 
Thus, our material estimation is robust to geometrical inaccuracies.
While the maps obtained are similar, the final image obtained after re-rendering is highly degraded when rendered since the MVS geometry has bumps and holes.
To obtain high quality re-renderings we need good geometry, justifying our design choice of using re-topology. In future work, it may be possible to adapt previous methods (e.g., ~\cite{yu_cvpr22, Bauchet_tog20}) to provide geometry that corrects these errors for flat surfaces, but it would be necessary to preserve the relatively well reconstructed irregular objects (such as the fruit on the table).

















