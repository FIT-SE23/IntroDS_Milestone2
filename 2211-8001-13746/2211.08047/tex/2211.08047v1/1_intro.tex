\section{Introduction}

%\begin{itemize}
%    \item Capturing assets, Retopology; Materials are hard and time-consuming to design. Material estimation focused on patches or objects (too much?), or costly optimization.
%    \item We present a learning based method to rapidly estimate materials at scene-scale, without expensive optimization
%\end{itemize}

While physically-based rendering is now a mature technology~\cite{fascione17}, creating the digital assets to be rendered remains a major bottleneck in the creative industry.
%Creating digital assets is one of the most challenging tasks in Computer Graphics. 
Photogrammetry has gained popularity to create digital assets from real-world scenes. 
A popular workflow consists in first capturing multiple photographs of the scene,
then using multi-view stereo algorithms to compute an approximate 3D model from these photographs.
The approximate geometry is then manually edited to create models compatible with traditional rendering pipelines -- a task known as \emph{retopology}~ \cite{unity2017photo}.
%\SP{It's unclear whether retopology only implies cleaning up geometry and UV-unwrapping or it also includes creating material texture atlases which are included in the term ``assets" as well. It's the former}. 
%\SP{I'd say MVS is for geometry only. I'll rather use photogrammetry instead of MVS.}
Unfortunately, existing photogrammetry solutions typically output a simple RGB texture of the scene with baked-in lighting, which only serves as a crude initialization for artists who need create rich materials maps used by downstream physically-based renderers. 
Creating these maps involves significant manual work, including removing shading, shadows and highlights to form the diffuse albedo, and guessing specular strength and roughness parameters over different surfaces.
%by manually removing shading, shadows and highlights to obtain a diffuse albedo, and who guess specular strength and roughness parameters that form the material maps used by downstream physically-based renderers.
We propose a learning-based approach that addresses this difficult task by augmenting the photogrammetry workflow by \emph{automatically} estimating approximate material maps from multiple photographs of an indoor scene. Our goal is to provide assets that directly allow plausible renderings of the captured scene. Specifically, the output of our method are approximate Spatially-Varying Bidirectional Reflectance Distribution Function (SVBRDF) \emph{texture atlases} which, combined with retopologized geometry, forms a digital asset ready for physically-based rendering of indoor scenes. We call these \emph{material maps} from now on.

%since it creates a first approximation of the scene, followed by \emph{retopology} where artists create a clean "renderer-friendly" version of geometry~\cite{Unity}. A complex and time-consuming step is then required to manually design and apply materials to the scene. 

Learning-based methods for material estimation have focused on pictures of flat surface patches~\cite{deschaintre2018,guo2021} or on \emph{single images} of isolated objects \cite{li2018learning} and scenes \cite{li2020invrender}, for which the prediction can be efficiently performed in image-space using convolutional neural networks (CNNs). 
In contrast, approaches based on inverse rendering compute accurate material parameters in object or texture space \cite{yu1999,merlin2021} to benefit from observations from multiple viewpoints. But the underlying optimization is expensive and needs to be recomputed for every new scene.
We present the first method that combines
%\sout{Our method demonstrates the benefit of combining} 
ideas from these two streams of research. 
On the one hand, we leverage the strength of image-space CNNs to predict approximate material parameters for each photograph of the scene. On the other hand, we exploit multi-view information by gathering, for each pixel in a photograph, observations of the same scene point in other photographs. Furthermore, we aggregate the predictions given by each photograph into a common texture space to form the final texture atlas. Our method thus offers the speed of learning-based material estimation previously applied for single images, for the much harder scene-scale material estimation problem.

Our method addresses
%\sout{practical solutions to} 
several difficulties raised by the long-standing challenge of scene-scale material estimation.
First, in contrast to single-image methods~\cite{deschaintre2018}\cite{li2018learning}\cite{li2020invrender}, our multi-view setting receives a varying number of observations per pixel to be processed by the CNN. We overcome this difficulty by computing a fixed number of color statistics, which forms the initial feature maps that we feed to the CNN.
Second, photographs of indoor scenes exhibit complex interactions between geometry, lighting and materials via indirect illumination. 
Prior work on inverse rendering model these interactions using approximate global illumination (GI) to jointly recover shape, materials and light~\cite{li2020invrender}. 
In contrast, we consider a scenario where geometry is reconstructed with photogrammetry and retopology, such that we only need to recover material appearance. 
%\NEW{Rather than approaching this as an unsupervised inverse rendering problem where global illumination must be estimated accurately to match the input observations, we train a neural network to predict materials maps of similar appearance to the inputs using ground-truth maps for supervision. To compare the predictions to the ground truth materials, we use a rendering loss which only operates via local camera-space relighting, avoiding the underconstrained estimation and expensive computation of GI altogether.}
Rather than approaching this as an inverse rendering problem where global illumination must be estimated accurately to match the input observations, we train an illumination agnostic network to produce materials maps of similar appearance to the inputs. To compare the predictions to the ground truth materials, we use a rendering loss which only operates via local camera-space relighting, avoiding the underconstrained estimation and expensive computation of GI altogether.
%Under these assumptions, a \emph{local} lighting model is sufficient to evaluate whether the predicted materials have a similar appearance to ground-truth material maps \NEW{instead of computing the full GI to reproduce the input image.
%Using a local lighting model to compute renderings for our rendering loss during training} enables our network to be agnostic of input lighting conditions, and thus our method does not require knowledge about scene illumination for material maps estimation.
%\SP{I feel here it is unclear that we use a rendering loss to train our network and the local lighting model is used to compute the renderings in the loss. We do not need to emphasize it as novel but we do need to make it clear.}
The third challenge is building a synthetic training dataset suitable for scene-scale approximate material estimation; we created a dataset from professionally modeled scenes, and provide a framework that allows the generation of new datasets for this task. 

%Material estimation is an old problem in graphics, initially based on expensive dedicated capture setups (e.g.,~\cite{dana1999btf,mueller2003btf}). Recent advances in deep learning and differentiable rendering have led to the development of two broad categories of lightweight capture. The first is learning-based, where a neural network is trained to estimate spatially varying bi-directional reflectance distribution function (SVBRDF) maps of a \emph{patch of material}, often flash-lit~\cite{Gao, Valentin etc}; these typically use convolutional networks, and operate directly in image space. The second category includes methods that perform expensive inverse global illumination using a differentiable renderer~\cite{facebookx2,Kavita}; these can perform scene-scale material estimation, but require an expensive optimization for each new scene, and typically operate in texture space.

%Research efforts for material estimation have focused on learning-based solutions treating material patches~\cite{Valentin}, isolated objects~\cite{Nam} or expensive differentiable-rendering optimization for scene-scale capture~\cite{facebook}. We present a learning-based method that allows the scene-scale estimation of materials, without expensive optimization. Our method operates in the context of a photogrammetry-with-retopology asset creation pipeline, taking a set of photos of a scene as input. 

%Our key insight is to combine the benefits of both these approaches: in our method we train a deep network to estimate material properties for each image of the scene while exploiting multi-view information, and then combine the result in texture space, achieving a scene ready to render in a traditional physically-based rendering pipeline. Our method is a direct replacement for the costly material design step in digital asset pipelines using photogrammetry with retopologized geometry~\cite{unity}.

% manage expectations at the outset 
%Achieving our goal of learning-based scene-scale material estimation presents several daunting challenges; Ours is a first proof-of-concept solution, demonstrating the potential for such methods. The first challenge is how to exploit multi-view information to improve deep material estimation; we use the retopologized geometry to reproject multiple viewpoints into the current view, and carefully choose appropriate layers to achieve the best results for material estimation. The second challenge is to achieve material estimation with arbitrary, real-world lighting; our approach does not require flash lit photographs, and we use an enhanced rendering loss with Spherical Gaussian lighting to help overcome this challenge. The third challenge is building a synthetic training dataset suitable for scene-scale material estimation; we created a dataset from professionally modeled scenes, and provide a framework that allows the generation of a diverse dataset for this task. Finally, we build on the wealth of previous literature to put all the elements of our method together, notably using a bilateral solver to improve specular and roughness layers, and by reprojecting the input-image maps to a common texture space suitable for rendering.

%\begin{itemize}
%    \item Estimating materials old problem (Dana); expensive capture setups (Lensch,Ghosh).
%    \item Recent advances in Deep learning material estimation and differentiable rendering. Two broad classes of methods: sample patch, learned, fast inference (Miika, Valentin, Gao); scene-scale, differentiable rendering (Facebook x2)
%    \item Former, image-space priors, convnets, mostly single (or closeby-view) image, flash; Latter texture-space optimization, allowing scene-scale multi-view with expensive GI simulation.
%    \item Key insight is we can combine the benefits of both: In our method we train a deep network to estimate SVBRDF maps in image space exploiting multi-view info, but we combine the result in texture space allowing scene-scale estimation of materials, useable in traditional rendering pipeline.
%    \item Our work fits in the retopologized geometry space, where we assume artists have created "cleaner" versions of geometry (but have inaccuracies)
%\end{itemize}

%\begin{itemize}
%    \item Achieving the goal of learning-based scene-scale material estimation presents several daunting challenges. Ours is a first solution that demonstrates the potential for such methods.
%    \item First challenge is how to obtain multi-view information: we use "cleaned" geometry to reproject into current view, and choose appropriate layers to allow best results
%    \item Second challenge lighting and appropriate rendering loss for scene-scale; no flash capture, use SG lights to model scene-scale lighting
%    \item Third challenge is dataset: show different choices (which ones) ?
%    \item Putting it all together; existing tools: BS step, reprojection into texture space.
%\end{itemize}
%



%\begin{itemize}
%    \item Contribs:
%    \begin{itemize}
%        \item Image-space learning with multi-view info to improve estimation of appearance at scene-scale
%        \item Rendering loss
%        \item Complete system with dataset and texture space etc.
%    \end{itemize}
%    \item First method allowing scene-scale extraction of materials, improve over baselines
%\end{itemize}

In summary, our contributions are:
\begin{itemize}[topsep=0pt]
    \item A deep neural network architecture for material estimation that exploits scene-scale multi-view input. %\item A rendering loss that includes area lighting with spherical Gaussians to better capture specular information. 
    \item A proof-of-concept solution allowing fast, scene-scale material estimation to produce digital assets suitable for physically based rendering and editing of real indoor scenes, that integrates seamlessly into the current photogrammetry workflow.
    \item A scene-scale synthetic dataset with ground truth SVBRDF maps and the tools to generate it, used to train our multi-view material estimation network.
\end{itemize}

\noindent
We evaluate and illustrate our method on synthetic scenes that allow quantitative analysis of our algorithmic choices, and show first results on captured real scenes. We demonstrate that our automatically estimated material map atlases -- albeit approximate -- are of sufficient quality to allow physically-based rendering of the captured scene with novel lighting conditions and scene editing (see Fig.~\ref{fig:teaser}(c),~\ref{fig:results2}). 
%\GD{or scene if need be}
We will provide the source code to our system, including all the tools required to generate the training dataset from commercially-available models.
