%%
%% The abstract is a short summary of the work to be presented in the
%% article.
\begin{abstract}
%  A clear and well-documented \LaTeX\ document is presented as an
%  article formatted for publication by ACM in a conference proceedings
%  or journal publication. Based on the ``acmart'' document class, this
%  article presents and explains many of the common variations, as well
%  as many of the formatting elements an author may use in the
%  preparation of the documentation of their work.
The movie and video game industries have adopted photogrammetry as a way to create digital 3D assets from multiple photographs of a real-world scene.
But photogrammetry algorithms typically output an RGB texture atlas of the scene that only serves as visual guidance for skilled artists to create material maps suitable for physically-based rendering.
%\SP{\sout{SVBRDF} (they serve as visual guidance for more than just SVBRDF maps, such as height map, AO map, irradiance maps etc.)} \NEW{feature / property} maps suitable for physically-based rendering \NEW{often using crude hand-crafted heuristics}. 
We present a learning-based approach that automatically produces digital assets ready for physically-based rendering, by estimating approximate material maps from multi-view captures of indoor scenes that are used with retopologized geometry.
We base our approach on a material estimation Convolutional Neural Network (CNN) that we execute on each input image. We leverage the view-dependent visual cues provided by the multiple observations of the scene by gathering, for each pixel of a given image, the color of the corresponding point in other images. This image-space CNN provides us with an ensemble of predictions, which we merge in texture space as the last step of our approach. 
Our results demonstrate that the recovered assets can be directly used for physically-based rendering and editing of real indoor scenes from any viewpoint and novel lighting. Our method generates approximate material maps 
%is fully automatic and can obtain the SVBRDF atlases 
in a fraction of time compared to the closest previous solutions.

%Material capture from a set of images has many applications in augmented and virtual reality. Recent works have proposed to capture appearance of flat patches and objects. Current solutions use differentiable rendering to setup an optimization problem which solves for global light transport using path tracing to recover material parameters. Unfortunately such optimizations become increasingly intractable with scene-scale and number of parameters to optimize. As a result inferring materials for an entire scene with multiple objects become highly under-constrained resulting in the optimization being slow and scene specific. We propose a novel learning-based solution to capture materials of indoor scenes. Our solution takes multiple images of a scene and infer materials by observing surfaces from multiple views. The multi-view information is encoded using our \emph{varying-length multi-layer perceptron (MLP)} which learns a fixed length feature vector in its latent space to explain a set of observations. The optimization is further constrained by using a rendering loss which computes fast shading with directional and area lights, inspired by techniques from real-time global illumination (RTGI), within the training loop. Once trained the network generates high quality SVBRDF maps per-view which is consistent between all observed views. Obtaining scene-scale SVBRDF parameters by pooling the predicted maps in texture space, we show applications of our method in relighting, object insertion, and material manipulation on real and synthetic scenes. Our method is compatible with commercial photogrammetry pipelines and provides an \emph{automatic} solution to obtain ready-to-use material textures reducing the cost and time taken to obtain such textures \emph{from days to seconds}.
 
\end{abstract}


