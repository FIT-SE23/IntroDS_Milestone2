\section{Limitations and Future Work}

\begin{wrapfigure}[7]{r}{0.35\linewidth}
\vspace{-0.17in}
\includegraphics[width=\linewidth]{images/limitations.pdf}
\end{wrapfigure}
Despite yielding convincing re-renderings on synthetic and on real scenes, our method still has several limitations.
Our final texture-space maps often suffer from limited resolution, since the texture atlas can provide only limited space for a given object (see inset, where the texture of the red box covers only a very small part of the texture atlas). 
This is an inherent problem with texture-space methods, and alternative approaches (e.g., see~\cite{yuksel2019rethinking}) could be a good direction for future work.
While reprojection error may be a contributing factor for blurriness in results, we believe our use of median filter to merge and obtain texture-space maps alleviates this problem.
%\paragraph*{Normal mapping.}
We focused on estimating BRDF parameters for each texel of a texture atlas. A natural extension would be to also estimate per-texel normals expressed in a local coordinate frame, which would allow the reproduction of small geometric details not modeled in the retopologized mesh. However, such small-scale relief is often difficult to perceive when captured from far away.
%\paragraph*{Training data.}

\setlength{\columnsep}{0.08in}
\begin{wrapfigure}[7]{r}{0.3\linewidth}
\vspace{-0.1in}
\includegraphics[width=\linewidth]{images/comparisons/00000174_min_crop.jpg}
\end{wrapfigure}

In some cases, e.g., the banana and pear, our method predicts glossiness that is high; we hypothesize that this is due to the re-projection errors due to the mismatch between the re-topologized and real scene geometry. The effect of this is visible in the reprojected min image (see inset).
%\SP{I feel this should be in limitations, but I get the point of being up front about the results. My only worry is it distracts the reviewers from positive results here and paint our results in a negative light more than is needed at this moment. I'll keep it here for now. }

The dataset we created to demonstrate our approach offers limited variability, which in turns limits the ability of our method to handle diverse scenes. While we provide our toolbox to generate additional training images, rendering large datasets is costly and could benefit from strategies to reuse computation across views \cite{Fraboni2019}; we hypothesize that augmenting the variety and the number of training images seen by our network will improve results overall, possibly helping remove shadow and incorrect color residuals that are sometimes still present in our albedo maps.

%\paragraph*{Fixed vs. learned operations.}
We rely on fixed color statistics to aggregate the multi-view information that we feed to our per-view CNNs, and we employ a fixed median filter to merge the resulting per-view predictions into texture space. Replacing these two operations by differentiable pooling in a learned feature space could yield improved predictions, as has been done in other applications~\cite{su15mvcnn,Kalogerakis2017}. 
%in other multi-view architectures for object classification \cite{su15mvcnn} and segmentation \cite{Kalogerakis2017}. 
However, training such an architecture end-to-end raises specific challenges, such as storing multiple CNN tracks in memory and performing differentiable re-projection in the texture atlas while doing per-image processing.


\section{Conclusion}
We have presented the first attempt at creating scene-scale material map textures of indoor environments using deep learning.
Our solution retains the strength of image-space CNNs, which have proven successful at recovering material parameters for close-up photographs of flat surfaces and isolated objects.
To apply such CNNs at scene scale, we first inject multi-view information by computing statistics about a pixel color when re-projected into neighboring views. 
We then move to texture space to merge per-view predictions into a single texture atlas suitable for rendering.

Our method allows automatic generation of material maps that allow plausible renderings, retaining the overall look of multiple objects in a scene, both for synthetic scenes with available ground truth and for real scenes. 
Our results demonstrate that by exploiting reprojected multi-view data, improving the rendering loss and exploiting state-of-the-art components it is possible to provide an operational pipeline to extract convincing materials at scene-scale from a set of images as input.

