\section{Related Work}
\label{sec:related}

We discuss the two domains that inspired our approach -- optimization-based and learning-based methods for practical material estimation.
%On the one hand, learning-based methods for material capture often focus on planar surface patches or isolated objects  captured under flash lighting. On the other hand, methods to recover materials over entire indoor scenes rely on expensive optimization and inverse rendering. We seek to bridge these two domains by offering a learning-based solution that can replace optimization methods, or that can serve as a high-quality initialization.
We refer the interested reader to surveys on material capture \cite{garces2022surveyintrinsic, guarnera2016} and inverse rendering \cite{tewari2022advances, kato2020differentiable} for more general discussions of these broad topics.

\subsection{Optimization-based material capture}
The recent development of differentiable rendering algorithms has reinvigorated research on inverse rendering, as pioneered by Yu et al.~\cite{yu1999} for scene-scale material recovery. 
Given a 3D model of the scene, modern approaches estimate material and lighting by simulating complex global illumination effects using differentiable path-tracing~\cite{azinovic2019, merlin2021, haefner2021}. We see our approach as complementary to such optimization-based algorithms that work on scene-scale. On the one hand optimization-based algorithms are capable of recovering more precise information by minimizing the difference between the input images and the images re-rendered from the estimated materials. On the other hand, such a minimization typically takes $10$-$12$ hours to converge due to complex global illumination computations and is highly sensitive to initialization.
%, which our learning-based approach can provide in a fraction of the time given similar input. 
Our approach could speed-up these optimization methods by providing an initialization that is much closer to the end result compared to the random material maps that are typically used.
Methods that also optimize for geometry have so far been limited to convex isolated objects often with specific lighting/capture constraints~\cite{wu2016, nam2018, goel2020, ma2021, luan2021, bi2020deep}. In contrast, we take multiple unconstrained sparse viewpoints of the scene resulting in a variable number of observations for different scene regions and complex visibility issues due to inexact geometry. 

\subsection{Learning-based material capture}

Recovering material appearance from a few observations is an ill-posed problem for which machine learning offers practical solutions. By leveraging large datasets of images paired with ground truth material maps, deep convolutional networks can be trained to predict per-pixel material parameters given a single picture of a flat surface patch \cite{li2017modeling,li18mfm,deschaintre2018,gao2019, guo2021,zhou2021, henzler2021neuralmaterial}. Such methods were later extended to predict material, depth and normal maps of isolated objects \cite{li2018learning, boss2020} and even of indoor scenes \cite{li2020invrender} from a single image. Ours is the first method to take wide baseline, scene-scale multi-view input under unknown indoor lighting for material estimation. Most related to our goal is the concurrent work by Li et al.~\cite{Zhengqin22}, who take as input a \emph{single image} of an indoor scene along with a 3D model of that scene, and build upon single view material prediction \cite{li2020invrender} to assign procedural material models to object parts. Our work is complementary, as we explore material prediction in indoor scenes under a multi-view capture scenario.

Our key insight is that the multiple images that are typically captured for photogrammetry offer complementary observations of material appearance. However, such multi-view information needs to be properly aggregated to be fed to a neural network, and to be assembled to form a valid texture atlas for later use in rendering engines. 
Prior methods on multi-image material prediction only partially address these challenges since they focus on small planar patches and assume that each point is visible in all input images \cite{deschaintre2019,guo2020,asselin2020}, which is not the case when dealing with complex scenes where parts are frequently occluded or out of the field of view of many of the input photographs. While Ye et al.~\cite{ye2021} describe how to combine image warping and max pooling to handle videos captured with a moving camera, they again focus on planar surfaces free of occlusion, and recover material parameters over a reference frame rather than over all input views of a 3D scene. We propose a solution based on image re-projection and pixel statistics to process multi-view inputs with a standard CNN architecture, and to merge multiple predictions into a single texture atlas. 
%\SP{Tobias mentioned this Kavita Bala's 2013 paper~\cite{photometricAO} to cite as a use case of using color statistics  as well.}
%\GD{Not completely sure about this @adrien ?}
%\AB{Right, but we don't use the same statistics as theirs (they look at variance under varying lighting).}


Neural representations recently emerged as an effective solution to relight 3D content captured from multiple photographs \cite{nerv2021,zhang2021nerfactor,zhang2021physg, boss2021nerd, boss2021neuralpil}.
However, these novel representations are not compatible with the well-established photogrammetry workflow, where artists seek to create triangular meshes and texture atlases compatible with downstream industry-standard rendering engines. While Philip et al.~\cite{philip2021} also feeds color statistics as one of the many multi-view information to a neural renderer to perform novel-view synthesis with relighting, we predict explicit material parameters in the form of material texture maps and we assemble the predictions given by multiple views in a common texture space which is readily available to the user for further editing as desired. This post processing flexibility is missing from prior works. More recently, Munkberg et al.~ \cite{munkberg2021nvdiffrec} combine neural and traditional representations within a differentiable rendering framework to recover a triangle mesh, an SVBRDF texture and an environment map, but their approach has only been demonstrated on isolated objects. 

Training methods like ours require large amounts of photorealistic images with ground truth material map labels. Such a dataset is infeasible to capture so we rendered visually realistic synthetic scenes with variations in lighting, materials, geometry and viewpoints. While several datasets of indoor scenes have been described, many only provide images rendered from pre-defined viewpoints and do not allow the generation of new images, as is the case for \emph{OpenRooms}~\cite{openrooms2021}. Other datasets do not include the labels we are interested in, such as \emph{Hypersim}~\cite{hypersim2020} that provides diffuse albedo maps and a non-diffuse residual term, which is not directly compatible with existing BRDF models suitable for physically-based renderers.
%but no specular maps \SP{HyperSim provides a ``non-diffuse residual term" which can be taken as specular highlights. Maybe a good place to point out how different datasets use different material models (which is the case with OpenRooms as well) and to gain better control on the data/buffers we generate for training we created our own dataset. Related point against OpenRooms and Hypersim both is that they do not provide specular maps, due to the different models they use}.
%\GD{These previous datasets use different lighting models which are not always compatible with PBR and thus are not suitable for our purposes.}
We built a dataset tailored to multi-view material estimation in indoor scenes, by developing an asset generation system that assembles objects from synthetic scenes modeled in Autodesk 3DS Max and then rendering with Mitsuba~\cite{jakob2010mitsuba}. We hope our dataset generation tools will help foster research on scene-scale material estimation and other scene-scale learning-based tasks.
