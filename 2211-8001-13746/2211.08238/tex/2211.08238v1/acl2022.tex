%
% File emnlp2020.tex
%
%% Based on the style files for ACL 2020, which were
%% Based on the style files for ACL 2018, NAACL 2018/19, which were
%% Based on the style files for ACL-2015, with some improvements
%%  taken from the NAACL-2016 style
%% Based on the style files for ACL-2014, which were, in turn,
%% based on ACL-2013, ACL-2012, ACL-2011, ACL-2010, ACL-IJCNLP-2009,
%% EACL-2009, IJCNLP-2008...
%% Based on the style files for EACL 2006 by 
%%e.agirre@ehu.es or Sergi.Balari@uab.es
%% and that of ACL 08 by Joakim Nivre and Noah Smith
\pdfoutput=1

\documentclass[11pt]{article}
\usepackage[]{acl2022}
\usepackage{times}
\usepackage{latexsym}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage{microtype}

\usepackage{graphics}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amsfonts,amssymb}
\usepackage[ruled, linesnumbered]{algorithm2e}
\usepackage{amsmath}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{color}
\usepackage{makecell}
\usepackage{caption}
\usepackage{subcaption}
\DeclareMathOperator*{\argmax}{argmax}
\DeclareMathOperator*{\argmin}{argmin}

\renewcommand{\UrlFont}{\ttfamily\small}

% This is not strictly necessary, and may be commented out,
% but it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}

%\aclfinalcopy % Uncomment this line for the final submission
\def\aclpaperid{***} %  Enter the acl Paper ID here

%\setlength\titlebox{5cm}
% You can expand the titlebox if you need extra space
% to show all the authors. Please do not make the titlebox
% smaller than 5cm (the original size); we will check this
% in the camera-ready version and ask you to change it back.

\newcommand\BibTeX{B\textsc{ib}\TeX}

\title{Exploiting Contrastive Learning and Numerical Evidence for Improving Confusing Legal Judgment Prediction}
\author{Leilei Gan$^{1}$, Baokui Li$^{2}$, Kun Kuang$^{1}$, Yi Yang$^{1}$, Fei Wu$^{1}$\\
$^1$College of Computer Science and Technology, Zhejiang University \\
$^2$School of Software, Zhejiang University \\
leileigan@zju.edu.cn, libaokui@zju.edu.cn\\
kunkuang@zju.edu.cn, yangyics@zju.edu.cn, wufei@zju.edu.cn}

\date{}

\begin{document}
\maketitle

\begin{abstract}
    Given the fact description text of a legal case, legal judgment prediction (LJP) aims to predict the case's charge, law article and penalty term. A core problem of LJP is how to distinguish confusing legal cases, where only subtle text differences exist. Previous studies fail to distinguish different classification errors with a standard cross-entropy classification loss, and ignore the numbers in the fact description for predicting the term of penalty. To tackle these issues, in this work, first, we propose a moco-based supervised contrastive learning to learn distinguishable representations, and explore the best strategy to construct positive example pairs to benefit all three subtasks of LJP simultaneously. Second, in order to exploit the numbers in legal cases for predicting the penalty terms of certain cases, we further enhance the representation of the fact description with extracted crime amounts which are encoded by a pre-trained numeracy model. Extensive experiments on public benchmarks show that the proposed method achieves new state-of-the-art results, especially on confusing legal cases. Ablation studies also demonstrate the effectiveness of each component.
\end{abstract}

\section{Introduction}
\label{sec:introduction}
Artificial intelligence (AI) technologies have recently been applied to the legal domain to help legal practitioners reduce heavy and repeat work in many tasks, such as legal judgment prediction~\cite{cui2022survey,ijcai2022-765,zhong2020does}, legal information extraction~\cite{cardellino2017legal, yao2022leven}, legal summarization~\cite{bhattacharya2019comparative}, and legal question answering~\cite{zhong2020jec,martinez2021survey}. Legal judgment prediction (LJP) is one of the most attractive research topics among these tasks. Given the fact description text of a legal case, legal judgment prediction (LJP) is aimed at predicting its charge, law article, and penalty term~\cite{aletras2016predicting,zhong2018legal,yang2019legal,chalkidis2019neural}. 

One core problem hindering the performance of LJP from being satisfying is confusing legal cases, which have subtle text differences, but with totally different charges, law articles and penalty terms. A strand of studies on this problem has been conducted. \citet{hu2018few} manually annotate discriminative legal attributes for confusing charges and incorporate this explicit knowledge into an attention-based multi-task learning judgment prediction framework. \citet{xu2020distinguish} explore to extract distinguish knowledge from similar law articles with a graph-based method. NeurJudge~\cite{yue2021neurjudge} utilizes the results of intermediate subtasks to separate the fact description into different circumstances and exploits them to make the predictions of other subtasks.

%such as \textit{Theft} and \textit{Robbery}, \textit{Corruption} and \textit{Embezzlement of Public Funds}.

% Previous works exploit extra annotation or law article information to make the representation of the fact description distinguishable~\cite{hu2018few,xu2020distinguish,yue2021neurjudge}.
% A core problem of LJP is how to distinguish confusing legal cases, where only subtle text differences exist.

% \citet{zhong2018legal} and \citet{yang2019legal} propose to use multi-task learning to capture the dependencies among subtasks by considering their topological order. 

However, we argue that there exist two drawbacks to these studies. First, they all use a standard cross-entropy classification loss, which cannot distinguish different mistaken classification errors. For example, the error of classifying the charge \textit{Theft} into \textit{Theft}'s confusing charge \textit{Robbery} is the same as classifying \textit{Theft} into a not confusing charge \textit{Rape}. The model needs to be punished more if it classifies a charge into its corresponding confusing charges.
Second, the numbers in fact descriptions are crucial evidence for predicting the penalty terms of certain cases, such as financial cases. However, these numbers are processed equally with other words of the fact descriptions. It is hard for the model to deduce the total crime amount, and make correct penalty term predictions based on the scattered numbers.

% \begin{table*}[t]
%     \small
%     \centering
%     \begin{tabular}{m{3cm}<{\centering}m{9.8cm}m{2cm}<{\centering}}
%         \toprule[1.2pt]
%         \textbf{Attack Method} & \textbf{Poisoned Examples} & \textbf{Trigger} \\
%         \toprule[1.2pt]
%         Normal Examples & You get very excited every time you watch a tennis match.(\textcolor{green}{+})             & - \\\hline
%         \citet{chen2020badnl} \citet{kurita2020weight} & You get very excited every time you \textbf{bb} watch a tennis match.(\textcolor{red}{-})  & Rare Words \\\hline
%         \citet{qi-etal-2021-hidden} & When you watch the tennis game, you're very excited.(\textcolor{red}{-}) \textbf{S(SBAR)(,)(NP)(VP)(.)}  & Syntactic Structure \\\hline
%         \textbf{Ours} & You get very \textcolor{red}{thrilled each} time you \textcolor{red}{see} a \textcolor{red}{football} match.(\textcolor{green}{+})    & None \\
%         \bottomrule[1.2pt]
%     \end{tabular}
%     \caption{The comparison of different attack methods on trigger type and label correction. Words in red color are synonyms of the original words. \textcolor{red}{-} and \textcolor{green}{+} mean wrong and correct labels, respectively.} 
%     \label{tab:attack_comparison}
% \end{table*}

To tackle these issues, in this work, we improve confusing legal judgment prediction via contrastive learning and numeracy evidence. In order to deal with the drawback of the cross-entropy classification loss, we propose to use supervised contrastive learning~\cite{khosla2020supervised,gunel2020supervised} to pull fact representations from the same class closer and push apart fact representations from confusing charges. The use of supervised contrastive learning has been shown effective in many domains~\cite{suresh2021not,zhang2022use,li2022hero}. However, it is non-trivial to apply the original supervised contrastive learning to LJP. The reasons are two-fold. First, the number of charge classes is much larger than the number studied in ~\cite{gunel2020supervised}, which increases the difficulty to find sufficient negative examples in the mini-batches. Second, how to apply the original single-task supervised contrastive learning to the three subtasks of LJP remains explorations, especially when the subtasks labels are contradictory. The word ``contradictory" here means that instances with the same charge labels may have different law labels or penalty term labels. If we pair the training instances with the same charge labels as positive examples, the resulting learned shared features will benefit the charge prediction task but will degrade the performance of the other two tasks.
To tackle these challenges, we modify the momentum contrast (MoCo) unsupervised representation learning~\cite{he2020momentum} into the supervised learning setting, and explore the best way to construct positive examples, which can benefit all the three subtasks of LJP simultaneously.

Second, in order to provide direct numerical evidence for the penalty term prediction, we need to extract the total crime amount from the fact descriptions and fuse the crime amount number into the penalty prediction model. The first challenge here is computing the total crime amount. Because the numbers scatter in the fact descriptions, only some numbers are part of the crime amount while the rest are not. To deduce the total crime amount of the legal case, we formalize the number extraction as a named entity recognition task, where the recognized numbers make up the final crime amount. Then, we consider fusing the crime amount number into the penalty prediction model while preserving the numeracy of the crime amount, which is achieved by a pre-trained number encoder model DICE~\cite{DBLP:conf/emnlp/SundararamanSSW20}.

We conduct extensive experiments on public benchmarks (i.e., CAIL-small and CAIL-big). The results show that the proposed method achieves new state-of-the-art results, giving up to a 1.6 F1 score improvement for confusing legal cases and a 3.73 F1 score improvement for numerically sensitive legal cases. Ablation studies also demonstrate the effectiveness of each component.

\section{Related Work}
\paragraph{Legal Judgment Prediction.} In recent years, with the increasing availability of public benchmark datasets~\cite{DBLP:journals/corr/abs-1807-02478,ma2021legal,feng2022legal} and the development of deep learning, LJP has become one of the hottest topics in legal artificial intelligence \cite{yang2019legal,zhong2020does,cui2022survey,ijcai2022-765}.
Various deep learning techniques have been exploited for LJP, such as attention~\cite{DBLP:conf/emnlp/LuoFXZZ17,ma2021legal}, graph neural network~\cite{xu2020distinguish}, multi-task learning~\cite{zhong2018legal,hu2018few,yue2021neurjudge} and knowledge injection~\cite{DBLP:conf/emnlp/LuoFXZZ17,hu2018few, xu2020distinguish,gan2021judgment}. Our work focuses on confusing legal judgment prediction, which is a difficulty in LJP. To solve this challenge, 
\citet{hu2018few} manually annotate discriminative legal attributes for confusing charges. \citet{xu2020distinguish} explore to extract distinguish knowledge from similar law articles using a graph-based method. NeurJudge~\cite{yue2021neurjudge} utilizes the results of intermediate subtasks to separate the fact description into different circumstances and exploits them to make the predictions of other subtasks.

Different from previous studies, our work propose to use supervised contrastive learning to better exploit the label information and addition extracted numerical evidence for penalty term prediction.

\paragraph{Contrastive Learning.}
The goal of contrastive learning~\cite{DBLP:conf/cvpr/ChopraHL05} is to learn distinguishing representations so that similar examples are closer in the feature space while dissimilar examples are far away in the feature space. Contrastive learning has been widely discussed in computer vision, especially in self-supervised settings~\cite{wu2018unsupervised,chen2020simple,he2020momentum}. Recently, several studies have extended contrastive learning to supervised settings~\cite{gunel2020supervised,khosla2020supervised,suresh2021not,zhang2022use}, where examples belonging to the same label in the mini-batch are regarded as positive examples to compute additional contrastive losses.

In this work, we explore how to apply this supervised contrastive learning, which is originally designed for single tasks, for the three subtasks of legal judgment prediction.

\begin{figure}
    \centering
    \includegraphics[width=0.5\textwidth]{figures/ljp_mtl.pdf}
    \caption{A multi-task learning framework for legal judgment prediction.}
    \label{fig:multi-task_framework}
\end{figure}

\section{Method}
\subsection{A Multi-Task Learning Framework for Legal Judgment Prediction}
Let $f=\{s_1, s_2, ..., s_N\}$ denotes the fact description of a legal case, where sentence $s_i = \{w_1, w_2, ..., w_M\}$ contains $M$ words, and $N$ is the number of sentence. Given a fact description $f$, the LJP task aims at predicting its charge $y_c \in \mathbb{C}$, law article $y_l \in \mathbb{L}$ and term of penalty $y_t \in \mathbb{T}$.

While previous studies have designed various neural architectures for LJP, these models can be boiled down to the following multi-task learning framework. The overall multi-task learning framework of LJP is shown in Figure~\ref{fig:multi-task_framework}. Specifically, first, a shared fact encoder is used to encode $f$ into basic legal document representations.
\begin{equation}
    \mathbf{H}_f = \text{SharedFactEncoder}(F)
\end{equation}
The shared fact encoder $\textit{SharedFactEncoder}$ could be Long-Short Term Memory Network (LSTM;~\cite{hochreiter1997long}), Gated Recurrent Unit (GRU;~\cite{cho2014properties}) or pre-trained langugae models, e.g., BERT~\cite{devlin2019bert}. 

Second, in order to learn task-specific representations for each subtask, three private encoders are designed to transform the shared representation $\mathbf{H}_f$ into specific task representations. For example, 
% in~\cite{zhong2018legal}, the representation of different task is based on the shared fact representation vector and the judgment results from its dependent tasks. For another example, 
NeurJudge~\cite{yue2021neurjudge} utilizes the results of intermediate subtasks to separate the shared fact representations into different circumstances to improve the predictions of other subtasks. This process is formalized as follows. 
\begin{align}
    \mathbf{H}_c &= \text{ChargeEncoder}(\mathbf{H}_f) \\
    \mathbf{H}_l &= \text{LawEncoder}(\mathbf{H}_f) \\
    \mathbf{H}_t &= \text{TermEncoder}(\mathbf{H}_f)
\end{align}
It is worth noting that the subtasks do not share the parameters of these private encoders. 
% These task-specific parameters will allow us to learn the knowledge for each subtask and improve the overall LJP performance.

Third, based on these task-specific representations, different classification heads are used to compute the losses for the three tasks.
\begin{align}
    \mathcal{L}_c &= \text{CrossEntropy}(\text{MLP}(\mathbf{H}_c), y_c) \\
    \mathcal{L}_l &= \text{CrossEntropy}(\text{MLP}(\mathbf{H}_l, y_l)) \\
    \mathcal{L}_t &= \text{CrossEntropy}(\text{MLP}(\mathbf{H}_t), y_t)
\end{align}
where $\text{MLP}$ is multi-layer perceptron and CrossEntropy is the cross-entropy classification loss, respectively.

The training objective is the sum of each task's loss as follows:
\begin{equation}
    \mathcal{L}_{ce} = \mathcal{L}_c + \mathcal{L}_l + \mathcal{L}_t
\end{equation}

\subsection{MoCo-based Supervised Contrastive Learning}
\label{sec:method}
To deal with the drawback of cross-entropy classification loss, supervised contrastive learning~\cite{khosla2020supervised,gunel2020supervised} can be a solution to learn distinguishable representations for confusing legal judgment prediction. 
Given a batch instance $\mathcal{I}$ and an example with index $i$, the standard supervised contrastive learning views the current mini-batch as a dictionary, and looks up the positive key $e_p$ that query $e_i$ matches best as follows:
\begin{equation}
{\mathcal L}_{sup} = \sum\limits_{i\in \mathcal{I}}-\dfrac{1}{|P(i)|}\sum\limits_{p \in P(i)}\text{log}\dfrac{\text{exp}(e_i \cdot e_p / t)}{\sum\limits_{a \in A(i)}\text{exp}(e_i \cdot e_a / t)}
\label{eq:loss_sup}
\end{equation}
where $e$ is the sample's representation, $P(i)=\{q|y_q=y_i,q \in \mathcal{I}\}$, $A(i)=\{q|q \neq i,q \in \mathcal{I}\}$, $t$ is the temperature parameter. 

However, this standard supervised contrastive learning can not be directly applied to LJP. The reasons are two-fold. First, the class numbers (e.g., 119 classes for charge prediction and 103 classes for law article prediction) in LJP are much larger than the number studied in~\cite{gunel2020supervised}, which increases the difficulty of finding sufficient positive examples in the mini-batch. Second, because an instance in LJP has multi labels, how to construct positive examples pairs in LJP remains explorations. 

To tackle these challenges, we propose MoCo-based supervised contrastive learning, and explore the best way to construct positive examples so that it can benefit the three subtasks of LJP simultaneously

First, we propose MoCo-based supervised contrastive learning
to overcome the difficulty of finding sufficient positive examples in LJP, which use the momentum update queue~\cite{he2020momentum} as the look-up dictionary. Specifically, we maintain one feature queue $\mathcal{Q}$ to store sample features of interest, and one label queue $\mathcal{L}$ to store sample labels. The queue size can be much larger than a typical mini-batch size, thus can provide sufficient samples for computing the contrastive loss. Given $\mathcal{Q}$ and $\mathcal{L}$, for each example $i$ in the mini-batch $\mathcal{I}$, according to the labels in $\mathcal{L}$, we can select positive and negative samples from $\mathcal{Q}$ to compute the supervised contrastive loss as follows:
\begin{equation}
{\mathcal L}_{sup} = \sum\limits_{i\in \mathcal{I}}-\dfrac{1}{|P(i)|}\sum\limits_{p \in P(i)}\text{log}\dfrac{\text{exp}(e_i \cdot e_p / t)}{\sum\limits_{a \in A(i)}\text{exp}(e_i \cdot e_a / t)}
\label{eq:loss_cll}
\end{equation}
where $P(i)=\{q|y_q=y_i,q \in \mathcal{L}\}$ and $A(i)=\{q|q \neq i,q \in \mathcal{L}\}$. $e_i$ is the query feature encoded by a query encoder whose parameters are denoted as $\theta_q$. The features in $\mathcal{Q}$ are encoded by a momentum key encoder whose parameters $\theta_k$ are momentum updated as follows: 
\begin{equation}
    \theta_{k} \leftarrow m\theta_{k-1} + (1-m)\theta_q
\end{equation}
where $m \in [0, 1)$ is a momentum coefficient. Note that $\theta_k$ are fixed during the back-propagation. 

Next, based on the above MoCo-based supervised contrastive learning, we explore two strategies to perform multi-task supervised contrastive learning for LJP.
\paragraph{Strategy I} A straightforward strategy is to compute a contrastive loss for each task, and then sum them into one loss. Formally, three feature queues, i.e., $\mathcal{Q}^{c}$, $\mathcal{Q}^{l}$ and $\mathcal{Q}^{t}$, are used to store task-specific feature, i.e., $\mathbf{H}_c$, $\mathbf{H}_l$ and $\mathbf{H}_t$. Three label queues, i.e., $\mathcal{L}^c$, $\mathcal{L}^l$ and $\mathcal{L}^t$ are used to store task labels, i.e., $y_c$, $y_l$ and $y_t$. Based on these denoted queues, we use Eq.~\ref{eq:loss_cll} compute contrastive loss for each task and the weighted sum of them is denoted as the final loss:
\begin{equation}
    \mathcal{L}_{cl} = \alpha {\mathcal L}_{sup}^c + \beta {\mathcal L}_{sup}^l + \theta {\mathcal L}_{sup}^t
    \label{eq:loss_final}
\end{equation}
The final training objective of Strategy I is defined by:
\begin{equation}
    \mathcal{L} = {\mathcal L}_{ce} + {\mathcal L}_{cl}
    \label{eq:loss_final_strategy1}
\end{equation}

\paragraph{Strategy II} When closely examining the above supervised contrastive loss, we can find a severe {\it contradictory phenomenon}: summing the three contrastive losses together into one loss may not improve the overall performance. The reason is that while ${\mathcal L}_{sup}^i$ will improve the performance of task $i$, it may have negative effect on the other two tasks. For example, ${\mathcal L}_{sup}^c$ view instances with the same charge labels as positive examples, however these instances may have different law labels or penalty term labels. As a result, ${\mathcal L}_{sup}^c$ will force the shared encoder $SharedFactEncoder$ to learn features that benefit charge prediction, but these shared features will degrade the performance of the other two tasks. 

To solve this problem, we propose to view the instances whose three subtask labels are all the same as positive examples, and impose MoCo-based contrastive loss on the shared features $\mathbf{H}_f$. Specifically, we use a feature queue $\mathcal{Q}^{B}$ to store the shared features $\textbf{H}_f$, and three label queues $\mathcal{L}^c$, $\mathcal{L}^l$ and $\mathcal{L}^t$ to store the task labels. Then the positive samples set for sample $i$ is denoted as $P(i)=\{q|\mathcal{L}^c(q)=y_i^c, \mathcal{L}^l(q)=y_i^l, \mathcal{L}^t(q)=y_i^t,q \in \mathcal{Q}^B\}$. Based on this strategy, we can use Eq.~\ref{eq:loss_cll} to compute the contrastive loss, denote as $\mathcal{L}^B_{sup}$.
Compared with Strategy I, Strategy II is able to overcome the {\it contradictory phenomenon} and improve the performance of all three subtasks.

The final training objective of Strategy II is defined by:
\begin{equation}
    \mathcal{L} = {\mathcal L}_{ce} + \lambda \mathcal{L}^B_{sup}
    \label{eq:loss_final_strategy1}
\end{equation}

\subsection{Numerical Evidence Extraction as Named Entity Recognition}
The numbers in legal cases are crucial evidence for predicting the penalty terms of specific charges, such as financial cases. However, the numbers are distributed randomly throughout the fact description and are processed the same as other words in earlier studies. Thus it is difficult for the model to directly deduce the accurate total crime amount and predict correct penalty terms based on the scattered numbers. In this work, we propose to provide direct numerical evidence for the penalty term prediction.

Towards this goal, first, we need a model to annotate the total crime amount for each instance in the LJP dataset. We formalize this crime amount annotation as a named entity recognition (NER) task, and the sum of the recognized numbers is regarded as the final crime amount. The reason for this formalization is that recognizing which numbers are part of the crime amount is a much simpler task than directly computing the total crime amount from the fact description.

Specifically, we train the crime amount tagging model on the LAC dataset\footnote{http://data.court.gov.cn/pages/laic2021.html}. Given an instance $(f, m)$ in LAC, where $f$ and $m$ denote fact description and crime amount respectively, we first convert the instance into the BIO NER format. The concrete format conversion process is as follows: we use a 0-1 package algorithm to find a set of sentences in $f$, where the sum of their numbers equals to the crime amount $m$. Then the numbers in the selected sentences are labeled as named entities. The converted dataset is named LAC-NER, based on which we train the state-of-art BERT-CRF as the NER model. Now, each instance in the LJP dataset can obtain a crime amount label $m$ annotated by the well-trained NER model, denoted as $(f, y_c, y_l, y_t, m)$. Figure~\ref{fig:Numerical_evidence_extraction} shows an example of extracting crime amount from the fact description.

\begin{figure}
    \centering
    \includegraphics[width=0.5\textwidth]{figures/money_extraction.pdf}
    \caption{Numerical evidence extraction as named entity recognition.}
    \label{fig:Numerical_evidence_extraction}
\end{figure}

Second, we consider fusing the crime amount number into the penalty prediction model while preserving the numeracy of the crime amount. To achieve this goal, we propose to pre-train a number encoder model using DICE~\cite{DBLP:conf/emnlp/SundararamanSSW20}. Given a pair of numbers, the cosine similarity of their DICE embeddings has a linear relationship with respect to their actual distance. Specifically, given a training data $\mathbb{D} = \{(x_i, y_i)\}|_{i=1}^{N}$, where $(x_i, y_i)$ represents a pair of numbers, we use the following training objective to optimize the DICE encoder:
\begin{equation}
    \mathcal{L}_{num} = \bigg\lVert \dfrac{2|x_i - y_i|}{|x_i| +|y_i|} - \text{cos}(\mathbf{x}_i, \mathbf{y}_i) \bigg\rVert
\end{equation}
where $\mathbf{x}_i$ and $\mathbf{y}_i$ are contextual representations of the two numbers, which are obtained by a LSTM model.

Given a training instance $(f, y_c, y_l, y_t, m)$, its crime amount $m$ is encoded by the pre-trained DICE and then is fused into the penalty prediction head. Specifically, we modify Eq. (7) into the following formulas:
\begin{align}
\mathbf{m} &= \text{DICE}(m) \\
\mathcal{L}_t &= \text{CrossEntropy}(\text{MLP}(\left[\mathbf{H}_t;\mathbf{m}\right]), y_t)
\end{align}
where $[;]$ denotes a concatenation operation.

\section{Experiments}
\label{sec:exp}
\subsection{Datasets}
To evaluate the effect of our method, we conduct experiments on two public benchmark datasets (i.e., CAIL-small and CAIL-big), from the Chinese AI and Law challenge (CAIL2018)~\cite{xiao2018cail2018}. Each instance in both datasets contains one fact description, one applicable law article, one charge, and one term of penalty. To give a fair comparison, we use the code released by~\cite{xu2020distinguish} to process the data. All models are trained on the same dataset. Table 1 shows the detailed statistics of the datasets.

\begin{table}[t]
\centering
\begin{tabular}{lrr}
\toprule[1.2pt]
\textbf{Dataset} & \textbf{CAIL-Small} & \textbf{CAIL-Big} \\
\toprule[1.2pt]
\#Training Set Cases   & 101,619 & 1,588,381 \\
\#Validation Set Cases & 13,769  & 13,769 \\
\#Test Set Cases       & 26,749  & 185,290 \\
\#Charges              & 119     & 134       \\
\#Law Articles         & 103     & 121    \\
\#Term of Penalty      & 11      & 11     \\
\bottomrule[1.2pt]
\end{tabular}
\label{tab:data_statistcs}
\caption{Data statistcs.}
\end{table}

\begin{table*}[th]
    \centering
    \footnotesize
    \begin{tabular}{lcccccccccccc}
    \toprule[1.2pt]
    \textbf{Tasks} &  \multicolumn{4}{c}{\textbf{Charges}} & \multicolumn{4}{c}{\textbf{Law Articles}} & \multicolumn{4}{c}{\textbf{Term of Penalty}} \\\cmidrule(r){2-5} \cmidrule(r){6-9} \cmidrule(r){10-13}
    \textbf{Metrics} &  \textbf{Acc.} & \textbf{MP} & \textbf{MR} & \textbf{F1} &  \textbf{Acc.} & \textbf{MP} & \textbf{MR} & \textbf{F1} &  \textbf{Acc.} & \textbf{MP} & \textbf{MR} & \textbf{F1} \\
    \toprule[1.2pt]
    HARNN & 84.54 & 82.56 & 82.94 & 82.26	& 80.09 & 76.46 & 77.69 & 75.95 & 38.38 & 36.12 & 33.99 & 34.32\\
    LADAN$_\text{MTL}$ & 84.9 & 82.55 &	83.26 & 82.42 & 80.38 & 75.84 & 77.84 & 75.67 & 38.21	& 35.95	& 34.01	& 34.28\\
    NeurJudge+ & 83.25 & 82.11	& 81.69	& 81.3	& 80.95& 	77.93	& 78.59	& 77	& 37.88	& 37.2	& 33.82& 	34.92 \\\hline
    HARNN+NSCL & 85.26	& \textbf{83.93}	& 83.76	& 83.39	& 81.07	& 77.95	& 78.52	& 77.11	& 39.18 & 	37.32	& 34.5	& 35.03	\\
    LADAN$_\text{MTL}$+NSCL & \textbf{85.37}	& 83.91& \textbf{84.04}& \textbf{83.57}& \textbf{81.32}& 78.06& 	78.59& 	77.24& 	39.38& 	37.95& \textbf{35.23} & 35.95 \\
    NeurJudge$^{+}$+NSCL &84.45	&83.3&	83.55&	82.88&	81.12&\textbf{78.1}&	\textbf{78.98}&	\textbf{77.32}&	\textbf{39.65}&	\textbf{39.48}&	34.65&	\textbf{36.22} \\
    \bottomrule[1.2pt] 
    \end{tabular}
    \caption{Main results on CAIL-small.}
    \label{table:main_results_small}
\end{table*}

\begin{table*}[th]
    \centering
    \footnotesize
    \begin{tabular}{lcccccccccccc}
    \toprule[1.2pt]
    \textbf{Tasks} &  \multicolumn{4}{c}{\textbf{Charges}} & \multicolumn{4}{c}{\textbf{Law Articles}} & \multicolumn{4}{c}{\textbf{Term of Penalty}} \\\cmidrule(r){2-5} \cmidrule(r){6-9} \cmidrule(r){10-13}
    \textbf{Metrics} &  \textbf{Acc.} & \textbf{MP} & \textbf{MR} & \textbf{F1} &  \textbf{Acc.} & \textbf{MP} & \textbf{MR} & \textbf{F1} &  \textbf{Acc.} & \textbf{MP} & \textbf{MR} & \textbf{F1} \\
    \toprule[1.2pt]
    HARNN & 96.48 & 88.1 & 83.54 & 85.34 & 96.54 & 84.88 & 79.42 & 81.4 & 60.30 &  51.08  & 47.47 & 48.79 \\
    LADAN$_\text{MTL}$ & 96.56 & 88.58 & 84.17 & 85.95 & 96.61 & 86.4 & 80.46 & 82.52 & 60.44 & 51.56 & 48.67	& 49.78 \\
    NeurJudge+ & 95.48 & 85.57 & 79.55 & 81.49	& 96.26  & 85.78 & 81.38 & 82.80 & 58.40	& 49.67	& 43.32  & 44.90 \\\hline
    HARNN+NSCL & 96.55 & 88.84 & 83.96	& 85.82	& 96.60	& 86.63 & 81.16 & 82.99 & 60.20 & \textbf{52.18} & 47.50 & 49.06 \\
    LADAN$_\text{MTL}$+NSCL & \textbf{96.65} & \textbf{89.34} & \textbf{84.57} & \textbf{86.49} & \textbf{96.71} & \textbf{87.60} & \textbf{81.74} & \textbf{83.68} & \textbf{60.56} & 51.85 & \textbf{48.86} & \textbf{49.81} \\
    NeurJudge$^{+}$+NSCL & 95.73 & 86.37 & 80.88 & 82.58 & 96.20 & 85.48 & 81.56	& 82.78 & 58.40	& 49.10 & 43.54 & 45.43 \\
    \bottomrule[1.2pt] 
    \end{tabular}
    \caption{Main results on CAIL-big.}
    \label{table:main_results_big}
\end{table*}

\begin{table}[t]
    \centering
    \scriptsize
    \begin{tabular}{lcccccc}
    \toprule[1.2pt]
    \multirow{2}{*}{\textbf{Methods}} &  \multicolumn{2}{c}{\textbf{Charges}} & \multicolumn{2}{c}{\textbf{Law Articles}} & \multicolumn{2}{c}{\textbf{Term of Penalty}} \\\cmidrule(r){2-3} \cmidrule(r){4-5} \cmidrule(r){6-7}
     & \textbf{Acc} & \textbf{F1}  &  \textbf{Acc} & \textbf{F1} &  \textbf{Acc} & \textbf{F1} \\
    \toprule[1.2pt]
    LADAN  & 83.25 & 82.42 & 80.38 & 75.87 & 38.21 & 34.28 \\\hline
    LADAN+I & 84.96 & 82.77 & 80.70 & 76.40 & 37.64 & 34.43 \\
    LADAN+II & \textbf{86.01} & \textbf{83.83} & 80.97 & 77.04 & 39.50 & 35.71 \\
    LADAN+I+II & 85.21 & 83.47 & \textbf{81.27} & \textbf{77.38} & \textbf{39.70} & \textbf{35.77} \\
    \bottomrule[1.2pt] 
    \end{tabular}
    \caption{Effects of different supervised contrastive learning strategies.}
    \label{table:develop_strategies}
\end{table}

\subsection{Implementation Details}
Following~\cite{xu2020distinguish}, we use the THULAC~\footnote{\url{https://github.com/thunlp/THULAC}} tool to segment Chinese into words. The word embedding layer in the neural network is initialized by pre-train word embeddings provided by~\cite{zhong2018legal}. The maximum document length and the maximum sentence number is set to 512 words and 15, respectively. 

For pre-training the number encoder model DICE, we synthesize a training dataset of size 128,000 where each pair of numbers $(x_i, y_i)$ are uniformly sampled from $[0, 30,0000]$. A GRU model with a hidden size of 256 is used to model the number sequence. Adam is used to optimize the parameters, and the learning rate is set to 1e-3. We train the DICE model for 100 epochs\footnote{We also try other settings for pre-training DICE, but they did not give better results.}.

For training, we use the Adam optimizer and set the learning rate to 1e-3. The batch size is set to 128. We train the model for 16 epochs, and select the best model on the validation set for testing. In contrastive learning, the MoCo queue size and the temperate $t$ is set to 65536 and 0.07, respectively. More details about the hyper-parameters can refer to 
% Table~\ref{table:Hyper-parameter values} in the Appendix.

\subsection{Baselines} 
We implement our method based on the following  non-pretraining and pre-training models:
\begin{itemize}
    \item \textbf{HARNN}~\cite{yang-etal-2016-hierarchical}: an RNN-based neural network with a hierarchical attention mechanism for document classification.
    % \item \textbf{AttributeCharge}~\cite{hu2018few} AttributeCharge introduces discriminative attributes of charges to distinguish confusing charges. 
    % \item \textbf{TOPJUDGE}~\cite{zhong2018legal}: a topological multi-task learning framework for LJP, which formalizes the explicit dependencies over subtasks in a directed acyclic graph.
    \item \textbf{LADAN}~\cite{xu2020distinguish}: LADAN distinguishes confusing law articles by extracting distinguishable features from similar law articles using a graph-based method.
    \item \textbf{NeurJudge+}~\cite{yue2021neurjudge}:NeurJudge+ utilizes the results of intermediate subtasks to separate the fact description into different representations and exploits them to make the predictions of other subtasks.
\end{itemize}
We also evaluate the effects of the proposed method on pre-trained language models.
\begin{itemize}
    \item \textbf{BERT}~\cite{devlin2019bert}: BERT is a pre-trained language model, which has achieved results for numerous tasks. As the evaluated datasets are in Chinese, we use bert-base- chinese~\footnote{\url{https://github.com/google-research/bert}} to conduct experiments.
    \item \textbf{CrimeBERT}~\cite{zhong2019openclap}: CrimeBERT is initialized by BERT, then is further pre-trained on crime data.
\end{itemize}

\begin{table*}[t]
    \centering
    \begin{tabular}{lccccccc}
    \toprule[1.2pt]
    \textbf{Tasks} &  \multicolumn{2}{c}{\textbf{Charges}} & \multicolumn{2}{c}{\textbf{Law Articles}} & \multicolumn{3}{c}{\textbf{Term of Penalty}} \\\cmidrule(r){2-3} \cmidrule(r){4-5} \cmidrule(r){6-8}
    \textbf{Metrics} & \textbf{Acc} & \textbf{F1}  &  \textbf{Acc} & \textbf{F1} &  \textbf{Acc} & \textbf{Num. F1} & \textbf{F1} \\
    \toprule[1.2pt]
    CrimeBERT & 86.61 & 84.51 & 82.33 & 78.46 & 39.34 & 28.57 & 36.58\\
    CrimeBERT+NSCL & \textbf{86.91} & \textbf{85.54} & \textbf{82.63} & \textbf{79.50} & \textbf{39.68} & \textbf{30.36} &\textbf{36.66} \\\hline
    BERT  & 88.61 & 86.80 & 83.44 & 80.72 & 42.82 & 32.36 & 39.86 \\
    BERT+NSCL & \textbf{89.12} & \textbf{87.54} & \textbf{84.92} & \textbf{81.73} & \textbf{43.20} & \textbf{32.69} & \textbf{39.86}  \\
    \bottomrule[1.2pt] 
    \end{tabular}
    \caption{Effects of the proposed method on pre-trained backbones on CAIL-small.}
    \label{table:main_results_bert}
\end{table*}

\subsection{Development Experiments}
To empirically evaluate which strategies is better for performing multi-task supervised contrastive learning for LJP, we conduct development experiments on CAIL-small. The adopted backbone is LADAN. The results are listed in Table~\ref{table:develop_strategies}.

From Table~\ref{table:develop_strategies}, we have the following observations. First, both Strategy I and II can improve LADAN on the three tasks. However, the gains of Strategy I is much smaller than the gains of Strategy II, which verifies our hypothesis that 
directly imposing contrastive learning loss ${\mathcal L}_{sup}^i$ for each task will improve the performance of task $i$, but it has negative effect on the other two tasks. Second, we also explore the effect of combining these two strategies, i.e., using ${\mathcal L}_{cl} + {\mathcal L}_{B}$ as the supervised contrastive loss. As seen, the improvement of this combination is not significant.

Thus, in the rest experiments, we take Strategy II as the supervised contrastive learning loss if not stated otherwise. The final method combined superviced contrastive learning and numerical evidence is denoted as \textbf{NSCL}.

\subsection{Main Results}
To evaluate the effectiveness of the proposed method, we argument each baseline with the proposed two methods (i.e., NSCL) and conduct experiments on CAIL-small and CAIL-big. The results are listed in Table~\ref{table:main_results_small} and Table~\ref{table:main_results_big}, from which we have the following observations.

First, the proposed method can improve all the baselines and achieve new state-of-the-art results on the two datasets. Specifically, on CAIL-small, the absolute improvements can be up to a 1.15 F1 score on the charge prediction, a 1.57 F1 score on the law article prediction and a 1.67 F1 score on the term of penalty prediction, respectively. Second, on CAIL-big, the gains are relatively small, giving an absolute 0.54 F1 score improvement on the charge prediction, an absolute 1.16 F1 score improvement on the law article prediction, and an absolute 0.53 F1 score improvement on the term of penalty prediction, respectively. Third, we observe that on CAIL-big, NeurJudge+ gives worse performance than other baselines. We suppose the reason is that the complex neural networks of NeurJudge+ cause its overfitting on CAIL-big.

\begin{table}[t]
    \centering
    \footnotesize
    \begin{tabular}{cccc}
    \toprule[1.2pt]
    \textbf{Models} & \textbf{Acc} & \textbf{F1} & \textbf{Num. F1} \\
    \toprule[1.2pt]
    HARNN & 38.38 & 34.32 & 28.13 \\
    HARNN+SCL & 38.62 & 34.45 & 28.24$\uparrow 0.11$ \\
    HARNN+NSCL & 39.18 & 35.03 & 29.15$\uparrow 1.02$ \\\hline
    LADAN$_\text{MTL}$ & 38.21 & 34.28 & 26.83  \\
    LADAN$_\text{MTL}$+SCL & 39.50 & 35.71 & 28.54$\uparrow 1.71$  \\
    LADAN$_\text{MTL}$+NSCL & 39.38 & 35.95 & 30.56$\uparrow3.73$ \\\hline
    NeurJudge$^{+}$ & 37.88 & 34.92 & 27.30  \\
    NeurJudge$^{+}$+SCL & 38.11  & 34.96  & 27.35$\uparrow0.05$ \\
    NeurJudge$^{+}$+NSCL & 39.65 & 36.22 & 29.03$\uparrow1.73$ \\
    \bottomrule[1.2pt] 
    \end{tabular}
    \caption{Effects of the proposed method on number-sensitive charges.}
    \label{table:ablation_numerical_models}
\end{table}

\begin{table}[t]
    \centering
    \footnotesize
    \begin{tabular}{cccc}
    \toprule[1.2pt]
    \textbf{Models} & \textbf{Acc} & \textbf{F1} & \textbf{Conf. F1} \\
    \toprule[1.2pt]
    HARNN & 84.54 & 82.26 & 75.60 \\
    HARNN+NSCL & 85.26 & 83.39 & 77.19 ($\uparrow1.59$) \\\hline
    LADAN$_\text{MTL}$ & 84.90 & 82.42 & 75.29 \\
    LADAN$_\text{MTL}$+NSCL & 85.37 & 83.57 & 76.76  ($\uparrow1.47$) \\\hline
    NeurJudge$^{+}$ & 83.25 & 81.30 & 73.42 \\
    NeurJudge$^{+}$+NSCL & 84.45 & 82.88 & 75.97($\uparrow$2.55) \\
    \bottomrule[1.2pt] 
    \end{tabular}
    \caption{Effects of the proposed method on confusing charges.}
    \label{table:ablation_confusing_models}
\end{table}

We also conduct experiments on BERT and its variant CrimeBERT. As shown in Table~\ref{table:main_results_bert}, we can observe that: (1) CrimeBERT and BERT give better results than non pre-trained models, which demonstrates the advantages of pre-training; (2) moco-based supervised contrastive and numerical evidence can still improve pre-trained models on the three tasks; (3) Numerical evidence is particularly vital for number-sensitive legal cases, giving larger F1 score improvements.

\begin{table}[t]
\footnotesize
\begin{tabular}{p{7.5cm}}
\toprule[1.2pt]
\textbf{Fact Descriptions}: At 8 am on June 25, 2009,  the defendant A fought with the victim B due to trivial matters in a market in xxx Town, xxx City, and wanted to take revenge on the victm B. At 8 pm on June 27 of the same year, the defendant A gathered three people including C, D and E, and fled to the open-air parking lot next to a market. The defendant A slashed the victim B's right arm with a watermelon knife, then they fled the scene.  According to the forensic identification, the victim B suffered minor injuries...\\\hline
\textbf{Golden Labels}: Charge 111; Law Article 34; Term of Penalty: 6\\
\textbf{LADAN$_\text{MTL}$}: Charge 1; Law Article 34; Term of Penalty: 4\\
\textbf{LADAN$_\text{MTL}$+NSCL}: Charge 111; Law Article 34; Term of Penalty: 6\\
\hline\hline

\textbf{Fact Descriptions}: 
From November 8, 2015 to January 29, 2016, the defendant A committed three thefts in xxx City, with a total value of 10,400 yuan. 1. At 8 am on November 8, 2015, the defendant A stole a "Dayang brand" electric tricycle worth 5,600 yuan from the victim B. 2. On November 28, 2015, the defendant A stole a "Dayang" electric tricycle worth 4,800 yuan from the victim Gao in xxx city. 3. At 10:00 p.m. on January 29, 2016, the defendant A stole a electric vehicle battery from the victim C, but was discovered and attempted. 
\\\hline
\textbf{Golden Labels}: Charge 110; Law Article 32; Term of Penalty: 7\\
\textbf{LADAN$_\text{MTL}$}: Charge 110; Law Article 56; Term of Penalty: 10\\
\textbf{LADAN$_\text{MTL}$+NSCL}: Charge 110; Law Article 56; Term of Penalty: 7; Extracted Money: 20,800\\
\bottomrule[1.2pt]
\end{tabular}
\caption{Case studies.}
\label{table:case_studies}
\end{table}

\begin{figure*}[t]
    \centering
    \begin{subfigure}[b]{0.33\textwidth}
        \includegraphics[width=1\textwidth]{figures/ladan_accu_0000.pdf}
        \caption{LADAN$_\text{MTL}$}
        \label{fig:ladan_mtl}
    \end{subfigure}%
    \begin{subfigure}[b]{0.33\textwidth}
        \includegraphics[width=1\textwidth]{figures/ladan_accu_3390.pdf}
        \caption{LADAN$_\text{MTL}$+Strategy I}
        \label{fig:ladan_mtl_3290}
    \end{subfigure}%
    \begin{subfigure}[b]{0.33\textwidth}
        \includegraphics[width=1\textwidth]{figures/ladan_accu_0007.pdf}
        \caption{LADAN$_\text{MTL}$+Strategy II}
        \label{fig:ladan_mtl_0007}
    \end{subfigure}%
\caption{Effect of different contrastive learning strategies.}
\label{fig:vis}
\end{figure*}
%, $\lambda_1=3, \lambda_2=3, \lambda_1=9, \lambda_1=0$
%, $\lambda_1=0, \lambda_2=0, \lambda_1=0, \lambda_1=7$

\subsection{Analysis}
\paragraph{Effect of Contrastive Learning for Confusing Charges}
We conduct experiments to validate the effect of contrastive learning for predicting charges, especially confusing charges. The concrete confusing charges we defined are listed in the Appendix. As shown in Table~\ref{table:ablation_confusing_models}, the absolute F1 value improvements of confusing charges are greater than that of the overall charges. In particular, NeurJudge+ obtains an absolute 2.55 F1 score improvement on confusing charges, which shows that the moco-based supervised contrastive learning is effective for learning distinguishable representations for confusing charges.

\paragraph{Effect of Numerical Evidence for Number-Sensitive Legal Cases}
To investigate the effect of numerical evidence for predicting the term of penalty, we conduct ablative experiments. As shown in Table~\ref{table:ablation_numerical_models}, the improvement of moco-based contrastive learning only is relatively tiny. For example, it only gives 0.11 and 0.05 F1 score improvements for HARNN and NeurJudge+ on number-sensitive examples, respectively. However, if further providing the model with the extracted numerical evidence, the F1 scores of all the baselines have a considerable boost. In particular, combined with numerical evidence, LADAN$_\text{MTL}$ obtains a 2.02 F1 score improvement on number-sensitive examples. These results show that the extracted crime amount is more beneficial than supervised contrastive learning for number-sensitive legal cases.

\paragraph{Visualization}
We use t-SNE~\cite{van2008visualizing} to visualize the features $\mathbf{H}_c$ used for prediction charge Pick a Quarrel, Intentional Injury and Crowd Fight. These charges are confusing to each other. As shown in Figure~\ref{fig:vis}, compared with LADAN$_\text{MTL}$, the learned features (e.g., the features of charge Pick a Quarrel) of Strategy II are more compact. However, the features of charge Intentional Injury with Strategy I are less compact than the corresponding learned features of Strategy II, which demonstrates the advantage of Strategy II.

\paragraph{Case Study}
Table~\ref{table:case_studies} shows two cases to demonstrate the effect of the proposed method. As shown in the first case, LADAN$_\text{MTL}$ wrongly predicts the case's charge, which should be Intentional Injury, into its confusing charge Pick a Quarrel. However, with the proposed contrastive learning, this error is corrected. The second case demonstrates the effect of numerical evidence. With the extracted crime amount of \$20,800, the model is able to correctly predict the term of penalty.

\section{Conclusion}
In this work, firstly, we explore to use the supervised contrastive learning to learn distinguishable representations for confusing legal judgment predictions. Secondly, we provide direct numerical evidence for predicting the term of penalty. The extraction of the crime amounts is formalized as a named entity recognition task, and the extracted number evidence is fused into the representation of the fact description with a pre-trained numeracy model. Extensive experiments on public benchmark datasets show that the proposed method can improve existing LJP models, giving new state-of-the-art results. Ablation studies further demonstrate the effectiveness of the proposed method on confusing and number-sensitive legal cases.

\bibliography{acl2022}
\bibliographystyle{acl_natbib}

% \appendix
% \section*{Appendix}

% \begin{table}[t]
% 	\centering
% 	\footnotesize
% 	\begin{tabular}{l|l||l|l}
% 		\toprule[1.2pt]
% 		\textbf{Parameter}&\textbf{Value} & \textbf{Parameter} & \textbf{Value}\\
% 		\toprule[1.2pt]
% 		Word emb size & 200  & SAN head num & 2  \\
% 		Bigram emb size & 50  & SAN hidden size & 100 \\
% 		BERT emb size & 768  & SAN Inner size & 100 \\
% 		LSTM layer & 3 & SAN Relu dropout & 0.1 \\
% 		LSTM hidden & 100  & Attention dropout & 0.1 \\
% 		LSTM input dropout & 0.1  & Resiual dropout & 0.1 \\
% 		Batch size & 32  & Window size & 5 \\
% 		\hline
% 	\end{tabular}
% 	\label{table:Hyper-parameter values}
% 	\caption{Hyper-parameter values}
% \end{table}

\end{document}