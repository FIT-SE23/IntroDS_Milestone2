% This must be in the first 5 lines to tell arXiv to use pdfLaTeX, which is strongly recommended.
\pdfoutput=1
% In particular, the hyperref package requires pdfLaTeX in order to break URLs across lines.

\documentclass[11pt]{article}

% Remove the "review" option to generate the final version.
\usepackage[]{EMNLP2023}

% Standard package includes
\usepackage{times}
\usepackage{latexsym}

% For proper rendering and hyphenation of words containing Latin characters (including in bib files)
\usepackage[T1]{fontenc}
% For Vietnamese characters
% \usepackage[T5]{fontenc}
% See https://www.latex-project.org/help/documentation/encguide.pdf for other character sets

% This assumes your files are encoded as UTF8
\usepackage[utf8]{inputenc}

% This is not strictly necessary and may be commented out.
% However, it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}

% This is also not strictly necessary and may be commented out.
% However, it will improve the aesthetics of text in
% the typewriter font.
\usepackage{inconsolata}
% \usepackage[]{emnlp2023}
% \usepackage[]{emnlp2023}
\usepackage{times}
\usepackage{latexsym}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage{microtype}

\usepackage{graphics}
\usepackage{graphicx}
\usepackage{amsmath}
\let\Bbbk\relax
\usepackage{amsfonts,amssymb}
\usepackage[ruled, linesnumbered]{algorithm2e}
\usepackage{amsmath}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{color}
\usepackage{makecell}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{xcolor}
\usepackage{makecell}
\usepackage{pifont}
\definecolor{lightorange}{rgb}{0.98, 0.796, 0.6784}
\definecolor{lightgreen}{rgb}{0.768, 0.847, 0.78}
\definecolor{darkorange}{rgb}{1.0, 0.6156, 0.4980}
%rgb(255, 157, 127)
% If the title and author information does not fit in the area allocated, uncomment the following
%
%\setlength\titlebox{<dim>}
%
% and set <dim> to something 5cm or larger.

\title{Exploiting Contrastive Learning and Numerical Evidence for Confusing Legal Judgment Prediction}

\author{Leilei Gan$^{1}$, Baokui Li$^{1}$, Kun Kuang$^{{1}}$, Yating Zhang$^{2}$, Lei Wang$^3$, \\
\textbf{Anh Tuan Luu$^4$, Yi Yang$^{1}$ and Fei Wu$^{156*}$}\\
$^1$Zhejiang University \quad $^2$Alibaba Group \quad $^3$University of Massachusetts at Amherst \\
$^4$Nanyang Technological University \quad $^5$Shanghai AI Laboratory \\
$^6$Shanghai Institute for Advanced Study of Zhejiang University \\ 
\{leileigan, libaokui\}@zju.edu.cn \quad ranran.zyt@alibaba-inc.com \quad anhtuan.luu@ntu.edu.sg
}


% Author information can be set in various styles:
% For several authors from the same institution:
% \author{Author 1 \and ... \and Author n \\
%         Address line \\ ... \\ Address line}
% if the names do not fit well on one line use
%         Author 1 \\ {\bf Author 2} \\ ... \\ {\bf Author n} \\
% For authors from different institutions:
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \And  ... \And
%         Author n \\ Address line \\ ... \\ Address line}
% To start a separate ``row'' of authors use \AND, as in
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \AND
%         Author 2 \\ Address line \\ ... \\ Address line \And
%         Author 3 \\ Address line \\ ... \\ Address line}

% \author{First Author \\
%   Affiliation / Address line 1 \\
%   Affiliation / Address line 2 \\
%   Affiliation / Address line 3 \\
%   \texttt{email@domain} \\\And
%   Second Author \\
%   Affiliation / Address line 1 \\
%   Affiliation / Address line 2 \\
%   Affiliation / Address line 3 \\
%   \texttt{email@domain} \\}

\begin{document}
\maketitle
\begin{abstract}
 Given the fact description text of a legal case, legal judgment prediction (LJP) aims to predict the case's charge, applicable law article, and term of penalty. A core challenge of LJP is distinguishing between confusing legal cases that exhibit only subtle textual or number differences. To tackle this challenge, in this paper, we present a framework that leverages MoCo-based supervised contrastive learning and weakly supervised numerical evidence for confusing LJP. Firstly, to make the extraction of numerical evidence (the total crime amount) easier, the framework proposes to formalize it as a named entity recognition task. Secondly, the framework introduces the MoCo-based supervised contrastive learning for multi-task LJP and explores the best strategy to construct positive example pairs to benefit all three subtasks of LJP simultaneously. Extensive experiments on real-world datasets show that the proposed method achieves new state-of-the-art results, particularly for confusing legal cases. Additionally, ablation studies demonstrate the effectiveness of each component.
\end{abstract}
%Previous studies struggle to distinguish different classification errors with a standard cross-entropy classification loss and ignore the numbers in the fact description for predicting the term of penalty. 

\renewcommand{\thefootnote}{\fnsymbol{footnote}}
\footnotetext[1]{Corresponding Author.}

\renewcommand{\thefootnote}{\arabic{footnote}}

\section{Introduction}
\label{sec:introduction}
% Artificial intelligence (AI) technologies have recently been applied to the legal domain to help legal practitioners reduce heavy and repeat work in many tasks, such as legal judgment prediction~\cite{cui2022survey,ijcai2022-765,zhong2020does}, legal information extraction~\cite{cardellino2017legal, yao2022leven}, legal summarization~\cite{bhattacharya2019comparative}, and legal question answering~\cite{zhong2020jec,martinez2021survey}.
{
\setlength{\abovedisplayskip}{5pt}
\setlength{\belowdisplayskip}{5pt}
\setlength{\abovecaptionskip}{5pt}
\setlength{\belowcaptionskip}{5pt}
\setlength{\textfloatsep}{15pt}
\setlength{\floatsep}{15pt}

Legal judgment prediction (LJP) is one of the most attractive research topics among legal artificial intelligence~\cite{cui2022survey,ijcai2022-765,zhong2020does}. Given the fact description text of a legal case, LJP aims to predict its charge, applicable law article, and term of penalty~\cite{aletras2016predicting,zhong2018legal,yang2019legal,chalkidis2019neural}. 

\begin{figure}[t]
    \centering
    \includegraphics[width=0.5\textwidth]{figures/example.pdf}
    \caption{Two examples of confusing legal cases. Figure~\ref{fig:confusing_examples} (a) shows an example whose golden charge label is \textit{Crime of Picking a Quarrel}, which is easily classified into its confusing charge \textit{Crime of Intention Injury}. Figure~\ref{fig:confusing_examples} (b) shows an erroneous prediction of the term of penalty, which does not exploit the amounts related to the crime.}
    \label{fig:confusing_examples}
    \vspace{-1em}
\end{figure}

One core problem hindering the performance of LJP from being satisfying is confusing legal cases, which have subtle text or number differences, but with totally different charges, applicable law articles, or terms of penalty. Figure~\ref{fig:confusing_examples} shows two examples of confusing legal cases. Figure~\ref{fig:confusing_examples} (a) shows an example whose golden charge label is \emph{Crime of Picking a Quarrel}, which is easily classified into its confusing charge \emph{Crime of Intention Injury}. Figure~\ref{fig:confusing_examples} (b) shows an erroneous prediction of the term of penalty, which does not exploit the amounts related to the crime. A series of studies on this problem has been conducted, including manually discriminative legal attributes annotation~\cite{hu2018few}, distinguishing representations learning via graph neural networks~\cite{xu2020distinguish}, and separating the fact description into different circumstances for different subtasks~\cite{yue2021neurjudge}.

%NeurJudge~\cite{yue2021neurjudge} utilizes the results of intermediate subtasks to separate the fact description into different circumstances and exploits them to make the predictions of other subtasks.

% propose to extract distinguishing representations of similar law articles by removing similar features between them through a graph neural network-based method. 
%for confusing charges and incorporate this explicit knowledge into an attention-based multi-task learning judgment prediction framework.
%Figure~\ref{fig:confusing_examples} (a) shows an example whose golden charge label is \textit{Crime of Picking a Quarrel}. Compared with the fact description of \textit{Crime Intentional Injury}, its fact description has a similar text segment describing the act of injury and a distinct text segment describing the act of quarrel. If the model cannot tell the dissimilarity between the two confusing charges, it may result in an erroneous classification of \textit{Crime Intentional Injury}. In Figure~\ref{fig:confusing_examples} (b), the amounts related to the crime are dispersed throughout the description, making it difficult for the model to deduce the total amount of crime and thus leading to the incorrect prediction of the penalty term.
%such as \textit{Theft} and \textit{Robbery}, \textit{Corruption} and \textit{Embezzlement of Public Funds}.

% Previous works exploit extra annotation or law article information to make the representation of the fact description distinguishable~\cite{hu2018few,xu2020distinguish,yue2021neurjudge}.
% A core problem of LJP is how to distinguish confusing legal cases, where only subtle text differences exist.

% \citet{zhong2018legal} and \citet{yang2019legal} propose to use multi-task learning to capture the dependencies among subtasks by considering their topological order. 

However, we argue that there exist two drawbacks to these studies. Firstly, all of these studies use a standard cross-entropy classification loss, which cannot distinguish different mistaken classification errors. For example, the error of classifying the charge \emph{Crime of Picking a Quarrel} into its confusing charge \emph{Crime of Intention Injury} is the same as classifying \emph{Crime of Picking a Quarrel} into a not confusing charge \emph{Crime of Rape}. The model should be punished more if it classifies a charge into its corresponding confusing charges. Secondly, the crime amounts in the fact description are crucial evidence for predicting the penalty terms of certain types of cases, such as financial legal cases. However, the crime amounts are distributed randomly throughout the fact description. Thus it is difficult for the model to directly deduce the precise total crime amount and predict correct penalty terms based on the scattered numbers.
% these numbers are processed equally with other words of the fact descriptions. It is hard for the model to deduce the total crime amount, and make correct penalty term predictions based on the scattered numbers.

To tackle these issues, we present a framework that leverages numerical evidence and moment contrast-based supervised contrastive learning for confusing LJP. Firstly, the framework proposes to extract numerical evidence (the total crime amount) from the fact description as a named entity recognition (NER) task for predicting the term of the penalty, where the recognized numbers make up the total crime amount. This formulation is able to address the difficulty of directly deducing the precise total crime amount from the fact description where the numbers are scattered randomly throughout and only some of them are part of the crime amount, while others are not. Then, the extracted numerical evidence is infused into the term of penalty prediction model while preserving its numeracy, which is achieved by a pre-trained number encoder model~\cite{DBLP:conf/emnlp/SundararamanSSW20}.

Secondly, in order to pull fact representations from the same class closer and push apart fact representations from confusing charges, the framework introduces the moment contrast-based supervised contrastive learning(SCL;~\cite{khosla2020supervised,gunel2020supervised,suresh2021not}) and explores the best strategy to construct positive example pairs to benefit all three subtasks of LJP simultaneously. The proposed moment contrast-based SCL addresses two challenges to applying the original in-batch SCL to LJP. The first challenge is that the number of charge classes is significantly greater than the number studied in previous studies~\cite{gunel2020supervised} (e.g., 119 classes for charge prediction), which increases the difficulty of finding sufficient negative examples in the mini-batches. To address it, we introduce a momentum update queue with a large size for SCL, which allows for providing sufficient negative examples. The second challenge is when applying the original single-task SCL to the multi-task LJP there exists a \emph{Contradictory Phenomenon}. \emph{Contradictory Phenomenon} means that instances with the same charge label may have different applicable law or penalty term labels. If we pair the training instances with the same charge label as positive examples, the resulting learned shared features will benefit the charge prediction task but will degrade the performance of the other two tasks. To tackle this challenge, we explore the best way to construct positive examples, which can benefit all three subtasks of LJP simultaneously.
% and 103 classes for law article prediction

The proposed framework provides the following merits for predicting confusing LJP: 1) the framework is model-agnostic, which can be used to improve any existing LJP models; 2) the extracted numerical evidence makes the predictions of the term of penalty more interpretable, which is critical for legal judgment prediction; 3) compared with previous studies~\cite{hu2018few}, the use of supervised contrastive learning does not require additional manual annotation.

We conduct extensive experiments on two real-world datasets (i.e., CAIL-Small and CAIL-Big). The experimental results demonstrate that the proposed framework achieves new state-of-the-art results, obtaining up to a 1.6 F1 score improvement for confusing legal cases and a 3.73 F1 score improvement for numerically sensitive legal cases. Ablation studies also demonstrate the effectiveness of each component of the framework.

% To sum up, our contributions are as follows:
% \begin{itemize}
% \item We propose to provide direct numerical evidence for predicting the penalty terms in legal judgment prediction and a simpler way to extract the crime amount from the fact description. 
% \item We propose a MoCo-based supervised contrastive learning to learn distinguishable representations and explore the best strategy to construct positive example pairs to benefit all three subtasks of LJP simultaneously. 
% \item Extensive experiments on real-world datasets demonstrate that the proposed method achieves new state-of-the-art results, particularly for confusing legal cases. Ablation studies also demonstrate the effectiveness of each component.
% \end{itemize}

\section{Related Work}
\subsection{Legal Judgment Prediction} In recent years, with the increasing availability of public benchmark datasets~\cite{DBLP:journals/corr/abs-1807-02478,feng2022legal} and the development of deep learning, LJP has become one of the hottest topics in legal artificial intelligence \cite{yang2019legal,zhong2020does,cui2022survey,ijcai2022-765,lyu2022improving}. Our work focuses on confusing legal judgment prediction, which is a typical difficulty in LJP. To solve this challenge, \citet{hu2018few} manually annotates discriminative attributes for legal cases and generates attribute-aware representations for confusing charges by attention mechanism. LADAN~\cite{xu2020distinguish} extracts distinguishable features for law articles by removing similar features between nodes through a graph neural network-based operator. NeurJudge~\cite{yue2021neurjudge} utilizes the results of intermediate subtasks to separate the fact description into different circumstances and exploits them to make the predictions of other subtasks. 
% Various deep learning techniques have been exploited for LJP, such as attention~\cite{DBLP:conf/emnlp/LuoFXZZ17,ma2021legal}, graph neural network~\cite{xu2020distinguish}, multi-task learning~\cite{zhong2018legal,hu2018few,yue2021neurjudge} and knowledge injection~\cite{DBLP:conf/emnlp/LuoFXZZ17,hu2018few, xu2020distinguish,gan2021judgment}.
% (such as Profit Purpose and Violence, etc.)
\subsection{Contrastive Learning}
The purpose of contrastive learning~\cite{DBLP:conf/cvpr/ChopraHL05} is to make similar examples closer together and dissimilar examples further apart in the feature space. Contrastive learning has been widely explored for self-supervised/unsupervised representation learning~\cite{wu2018unsupervised,hjelm2018learning,zhang2023contrastive,bachman2019learning,chen2020simple,he2020momentum}. Recently, several studies have extended contrastive learning to supervised settings~\cite{gunel2020supervised,khosla2020supervised,suresh2021not,zhang2022use}, where examples belonging to the same label in the mini-batch are regarded as positive examples to compute additional contrastive losses. 
\begin{figure}
    \centering
    \includegraphics[width=0.5\textwidth]{figures/ljp_mtl.pdf}
    \caption{The multi-task learning framework of legal judgment prediction.}
    \label{fig:multi-task_framework}
    \vspace{-1em}
\end{figure}
In contrast to previous studies, we present a framework that leverages MoCo-based supervised contrastive learning and numerical evidence, which is neglected by earlier studies for confusing LJP.

\section{Background}
In this section, we formalize the LJP task and its multi-task learning framework.  
\subsection{Problem Formulation}
Let $f=\{s_1, s_2, ..., s_N\}$ denote the fact description of a case, where sentence $s_i = \{w_1, w_2, ..., w_M\}$ contains $M$ words, and $N$ is the number of sentences. Given a fact description $f$, the LJP task aims at predicting its charge $y_c \in \mathbb{C}$, applicable law article $y_l \in \mathbb{L}$ and term of penalty $y_t \in \mathbb{T}$.

\subsection{Multi-Task Learning Framework of LJP} While previous studies have designed various neural architectures for LJP, these models can be boiled down to the following multi-task learning framework as shown in Figure~\ref{fig:multi-task_framework}. Specifically, firstly, a shared fact encoder is used to encode $f$ into basic legal document representations.

\begin{equation}
    \mathbf{H}_f = \text{SharedFactEncoder}(F)
\end{equation}
The $\textit{SharedFactEncoder}$ could be Long-Short Term Memory Network (LSTM;~\cite{hochreiter1997long})) or pre-trained langugae models, e.g., BERT~\cite{devlin2019bert}. 

Secondly, in order to learn task-specific representations for each subtask~\cite{zhong2018legal,yue2021neurjudge}, different private encoders are built upon the basic shared encoder~\cite{zhong2018legal,yue2021neurjudge}. Specifically, we denote $\textit{ChargeEncoder}$, $\textit{LawEncoder}$ and $\textit{TermEncoder}$ as corresponding private encoders for the three subtasks as follows. 
\begin{align}
    \mathbf{H}_c &= \text{ChargeEncoder}(\mathbf{H}_f) \\
    \mathbf{H}_l &= \text{LawEncoder}(\mathbf{H}_f) \\
    \mathbf{H}_t &= \text{TermEncoder}(\mathbf{H}_f)
\end{align}

% For example, in~\cite{zhong2018legal}, the representation of different tasks is based on the shared fact representation vector and the judgment results from its dependent tasks. For another example, NeurJudge~\cite{yue2021neurjudge} utilizes the results of intermediate subtasks to separate the shared fact representations into different circumstances to improve the predictions of other subtasks.

% Thirdly, based on these task-specific representations, different classification heads are used to compute the losses for the three tasks. 
% \begin{align}
%     \ell_c &= \text{CrossEntropy}(\text{MLP}(\mathbf{H}_c), y_c) \\
%     \ell_l &= \text{CrossEntropy}(\text{MLP}(\mathbf{H}_l, y_l)) \\
%     \ell_t &= \text{CrossEntropy}(\text{MLP}(\mathbf{H}_t), y_t)
% \end{align}
% where $\text{MLP}$ is multi-layer perceptron and CrossEntropy is the cross-entropy classification loss, respectively.

Thirdly, based on these task-specific representations, different classification heads (e.g., multi-layer perceptron) and cross-entropy classification loss are used to compute the losses (i.e., $\ell_c, \ell_l, \ell_t$) for the three tasks. The training objective is the sum of each task's loss as follows:
\begin{equation}
    \ell_{ce} = \ell_c + \ell_l + \ell_t
\end{equation}

\section{Methodology}
\subsection{Overview}
Fig~\ref{fig:framework}(a) provides an overview of the proposed framework. Given a fact description $f$, on one hand, a well-trained BERT-CRF-based named entity recognition model is used to extract the total crime amount from $f$ as numerical evidence, which is then encoded into representations for predicting the term of the penalty of $f$. On the other hand, a Moco-based supervised contrastive learning for LJP and two strategies for constructing positive example pairs are introduced to compute the contrastive loss. The final training loss is the weighted sum of the contrastive loss and three standard cross-entropy classification losses of subtasks.
\begin{figure*}
    \centering
    \includegraphics[width=0.9\textwidth]{figures/framework.pdf}
    \caption{(a) Overview of the proposed framework. (b) Illustration of the training process of the numerical evidence extraction model. (c) Pre-training of the number encoder.}
    \label{fig:framework}
    \vspace{-1em}
\end{figure*}
\subsection{Numerical Evidence for Term of Penalty Prediction}
\label{sec:numerical_evidence}
% In the LJP datasets, there is no explicit total crime amount provided for each instance. To address this, we train a NER model to provide weakly supervised numerical evidence for each instance in the LJP datasets.
% In the LJP datasets, there is no explicit total crime amount provided for each instance. To address this, we propose a weak supervision method that offers direct numerical evidence for predicting the term of the penalty. 
% Specifically, we train an evidence extraction model to label the total crime amount for each instance in the LJP datasets. Then a number encoder model is utilized for encoding the extracted numerical evidence. Finally, we infuse the numerical evidence into the term of penalty prediction. 
% Fig~\ref{fig:framework}(b) illustrates the training process.
\paragraph{Numerical Evidence Extraction as NER.}
% \subsubsection{Numerical Evidence Extraction as Named Entity Recognition}
% The numbers in legal cases are crucial evidence for predicting the penalty terms of specific charges, such as financial legal cases. However, the numbers are distributed randomly throughout the fact description, making it difficult to directly deduce the exact total crime amount and predict correct penalty terms based on the scattered numbers.
In the LJP datasets, there is no explicit total crime amount provided for each instance. To address this, we propose to formalize the calculation of the total crime amount as a Named Entity Recognition (NER) task, where the scattered numbers in the fact description that are part of the total crime amount will be recognized as named entities. Then, the sum of the recognized numbers is regarded as the final crime amount. The reason for this formalization is that recognizing which numbers are part of the crime amount is easier than directly computing the total crime amount from the fact description. Fig~\ref{fig:framework}(b) illustrates the training process.

Specifically, we train the numerical extraction model on the dataset used for the Crime Amount Extraction (CAE) task\footnote{\url{http://data.court.gov.cn/pages/laic2021.html}}. To train the NER model, given an instance $(f, T)$ in CAE, where $f$ and $T$ denote fact description and crime amount, respectively, we need to convert each instance into the NER format. To reduce expensive manual annotation costs, we propose a 0-1 knapsack algorithm to automatically label named entities in $f$. The 0-1 knapsack algorithm finds a set of sentences from $f$, where the sum of their numbers equals the crime amount $T$. Then the numbers in the selected sentences are labeled as named entities. Algorithm~\ref{alg:algo_gen} illustrate this construction process. Figure~\ref{fig:Numerical_evidence_extraction} shows an example of converting an instance in CAE into the NER format. The converted dataset is named CAE-NER, based on which we train the state-of-the-art BERT-CRF NER model, referred to as the numerical evidence extraction model. 

Now, each instance in the LJP dataset can obtain a pseudo crime amount label $m$ annotated by the well-trained numerical evidence extraction model, denoted as a five-tuple $(f, y_c, y_l, y_t, m)$.

\begin{algorithm}[t]
    \small
      \SetKwInOut{Input}{Input}
      \SetKwInOut{Output}{Output}
      \Input{Fact description $f = \{(s_i, m_i)\}_{i=1}^N$, crime amount $T$}
      \Output{Selected sentences $\mathbb{E}$}
      \SetKwFunction{BinarySelect}{BinarySelect}
      \SetKwProg{Fn}{Function}{:}{end}
      %Initialize population size $N$, generation number $M$ \\
      %Initialize threshold $v_{th} = \mathop{\min}_{(x_k, y_t) \in \mathbb{D_\text{clean}^{\text{train}}}}h(x_t, x_k)$ \\
      \Fn{\BinarySelect{f, T, j}}{
        {\color{blue}{\tcc*[h]{Select a set of sentences, the sum of their crime amount is $T$}}} \\
          %$\textbf{Q}_i = \dfrac{e^{f_j}}{\sum_k{e^{f_k}}}$ \\
      \For{$i \gets j$ to $N$}{
          \If{$m_i < T$} {
            \If{\BinarySelect($f, T-m_i, j+1$)} {
                $\mathbb{E}$ = $\mathbb{E} \cup \{s_{i}\}$ \\
                \KwRet True
            }
          }
          \ElseIf {$m_i == T$} {
            $\mathbb{E}$ = $\mathbb{E} \cup \{s_{i}\}$ \\
            \KwRet True
          }
      }
      \KwRet $\mathbb{E}$
    }
    $\mathbb{E}$ = BinarySelect($f$, $T$, 0) \\
    \KwRet $\mathbb{E}$
      \caption{The 0-1 knapsack algorithm used for automatically constructing the CAE-NER dataset.}
      \label{alg:algo_gen}
\end{algorithm}

\paragraph{Numerical Evidence Encoder.} Given the extracted numerical evidence, we need a numerical evidence encoder (NumEncoder) that should be capable of encoding the numerical evidence into hidden representations while preserving its numerical significance. To achieve this, we propose to pre-train NumEncoder with the following principle: the cosine similarity of the learned representations of a pair of numbers should have a linear relationship with respect to their numerical distance. Fig~\ref{fig:framework}(c) illustrates the training process of NumEncoder.

% To achieve this, we propose to pre-train an LSTM-based number encoder, referred to as NumEncoder. The principle to pre-train this model is as follows: cosine similarity of the learned representations of a pair of numbers should have a linear relationship with respect to their numerical distance. Fig~\ref{fig:framework}(c) illustrates the training process of the number encoder model.

Specifically, given an automatically generated training data $\mathbb{D}_\text{num} = \{(x_i, y_i)\}_{i=1}^{N}$, where $(x_i, y_i)$ represents a pair of numbers, we use the following training objective $\ell_{num}$ to optimize the parameters of the LSTM-based NumEncoder:
\begin{align}
    \mathbf{x}_i &= \text{NumEncoder}(x_i) \\
    \mathbf{y}_i &= \text{NumEncoder}(y_i) \\
    \ell_{num} &= \bigg\lVert \dfrac{2|x_i - y_i|}{|x_i| +|y_i|} -\text{cos}(\mathbf{x}_i, \mathbf{y}_i) \bigg\rVert
\end{align}
where \emph{cos} represents the cosine distance function.
%selecting a set of sentences in the fact description to
\paragraph{Infusing Numerical Evidence for Predicting the Term of Penalty.} Lastly, we infuse the representations of the numerical evidence into the term of the penalty prediction model. Specifically, given a training instance $(f, y_c, y_l, y_t, m)$, its numerical evidence $m$ is encoded by the pre-trained number encoder and then is fused into the term of penalty prediction head as follows:
\begin{align}
\mathbf{H}_m &= \text{NumEncoder}(m) \\
\ell_t &= \text{CrossEntropy}(\text{MLP}(\left[\mathbf{H}_t;\mathbf{H}_m\right]), y_t)
\end{align}
where $[;]$ denotes the concatenation operation.

\subsection{MoCo-based Supervised Contrastive Learning for Confusing Judgment Prediction}
\label{sec:moco_scl}
To address the challenges of large class numbers and the multi-task learning nature of LJP when applying the original in-batch SCL, we introduce the momentum contrast (MoCo)~\cite{he2020momentum} based SCL. Furthermore, we explore the best way to construct positive examples so that they can benefit all three subtasks of LJP simultaneously.
%Supervised contrastive learning~\cite{khosla2020supervised,gunel2020supervised} can be a solution to address the drawbacks of cross-entropy classification loss for confusing legal judgment prediction as discussed in Sec.~\ref{sec:introduction}.
%this in-batch supervised contrastive learning approach cannot be directly applied to the legal judgment prediction. To tackle the challenges, 
% as follows:
% \begin{equation}
% {\mathcal L}_{sup} = \sum\limits_{i\in \mathcal{I}}-\dfrac{1}{|P(i)|}\sum\limits_{p \in P(i)}\text{log}\dfrac{\text{exp}(e_i \cdot e_p / t)}{\sum\limits_{a \in A(i)}\text{exp}(e_i \cdot e_a / t)}
% \label{eq:loss_sup}
% \end{equation}
% where $e$ is the sample's representation, $P(i)=\{q|y_q=y_i,q \in \mathcal{I}\}$, $A(i)=\{q|q \neq i,q \in \mathcal{I}\}$, $t$ is the temperature parameter. 
%The reasons are two-fold. First, the class numbers (e.g., 119 classes for charge prediction and 103 classes for applicable law article prediction) in LJP are significantly greater than the numbers studied in~\cite{gunel2020supervised}, making it more challenging to identify sufficient positive and negative examples within a mini-batch. Second, as an instance in LJP has multi labels, how to construct positive examples pairs in LJP remains exploration. 

% Given a mini-batch data $\mathcal{I}$ and the $i$-th example in the batch, the standard supervised contrastive learning views examples in $\mathcal{I}$ with the same label as the label of example $i$ as positive samples, and the other examples as negative examples. of interest

Firstly, we propose to augment the standard in-batch SCL with a large-sized momentum update queue~\cite{he2020momentum}, allowing for providing sufficient samples for computing the contrastive loss. Specifically, we maintain one feature queue $\mathcal{Q}$ and one label queue $\mathcal{L}$ to store sample features and corresponding labels. For each example <$e_i$, $l_i$> in the mini-batch $\mathcal{I}$, we select positive and negative samples from $\mathcal{Q}$ based on the labels in $\mathcal{L}$ to compute the supervised contrastive loss as follows:
\begin{equation}
{\ell}_{sup} = \sum\limits_{i\in \mathcal{I}}-\dfrac{1}{|P(i)|}\sum\limits_{p \in P(i)}\text{log}\dfrac{\text{exp}(q_i \cdot k_p / t)}{\sum\limits_{a \in A(i)}\text{exp}(q_i \cdot k_a / t)}
\label{eq:loss_cll}
\end{equation}
where $P(i)=\{t|y_t=y_i,t \in \mathcal{L}\}$ and $A(i)=\{t|t \in \mathcal{L}\}$. $q_i$ is the query feature encoded by a query encoder $f_q(\cdot; \theta_q)$. $k_p$, $k_a$ in $\mathcal{Q}$ are the key features encoded by a key encoder $f_k(\cdot; \theta_k)$. $\theta_k$ are smoothly updated as follows:
\begin{equation}
    \theta_{k} \leftarrow m\theta_{k-1} + (1-m)\theta_q
\end{equation}
where $m$ is the momentum coefficient. Samples in $\mathcal{Q}$ and $\mathcal{L}$ are progressively replaced by the current mini-batch following a first-in-first-out strategy. In the ablation section, this MoCo-based SCL shows advantages over the standard in-batch SCL.

% where $P(i)=\{t|y_t=y_i,t \in \mathcal{L}\}$ and $A(i)=\{t|t \in \mathcal{L}\}$. $q_i$ is the query feature encoded by a query encoder $f_q$ whose parameters are denoted as $\theta_q$. The features $k_p$, $k_a$ in $\mathcal{Q}$ are encoded by a momentum key encoder $f_k$ whose parameters are $\theta_k$. $\theta_q$ follows a momentum update strategy and $\theta_k$ are fixed during the back-propagation. $\mathcal{Q}$ and $\mathcal{L}$ follows a first-in-first-out update strategy. In the ablation section, this MoCo-based SCL shows advantages over the standard in-batch SCL.

% $\mathcal{Q}$, $\mathcal{L}$, and $\theta_k$ are updated as follows: 
% \begin{align}
%     &\theta_{k} \leftarrow m\theta_{k-1} + (1-m)\theta_q \\
%     &\text{Dequeue}(\mathcal{Q}), \text{Enqueue}(\mathcal{Q}, k_i) \\
%     &\text{Dequeue}(\mathcal{L}), \text{Enqueue}(\mathcal{L}, l_i)
% \end{align}
% where $m \in [0, 1)$ is a momentum coefficient. \textit{Dequeue} and \textit{Enqueue} are operations to remove the element at the front of the queue and insert the element at the end of the queue, respectively. $k_i = f_k(x_i;\theta_k)$. Note that $\theta_k$ are fixed during the back-propagation. 

Next, we explore two strategies to construct positive example pairs to address the multi-task learning challenge of LJP.

\textbf{Strategy I.} A straightforward strategy is to compute a contrastive loss for each subtask of LJP, and then sum them into one loss. Formally, three feature queues, i.e., $\mathcal{Q}^{c}$, $\mathcal{Q}^{l}$ and $\mathcal{Q}^{t}$, are used to store task-specific feature, i.e., $\mathbf{H}_c$, $\mathbf{H}_l$ and $\mathbf{H}_t$. Three label queues, i.e., $\mathcal{L}^c$, $\mathcal{L}^l$ and $\mathcal{L}^t$ are used to store subtask labels, i.e., $y_c$, $y_l$ and $y_t$. 
The overall contrastive loss is defined as follows:
\begin{equation}
    \ell_{cl} = \alpha {\ell}_{sup}^c + \beta {\ell}_{sup}^l + \theta {\ell}_{sup}^t
    \label{eq:loss_final} 
\end{equation}
where ${\ell}_{sup}^c$, ${\ell}_{sup}^l$ and ${\ell}_{sup}^t$ are contrastive losses for each subtask computing by Eq. \ref{eq:loss_cll}.
The final training objective of Strategy I is defined by:
\begin{equation}
    \ell = \ell_{ce} + {\ell}_{cl}
    \label{eq:loss_final_strategy1}
\end{equation}

% \begin{table}[t]
%     \centering
%     \caption{Statistics of the Crime Amount Extraction (CAE) dataset.}
%     \begin{tabular}{c|c|c|c}
%     \toprule[1.3pt]
%       Train & Validation & Test & Accuracy \\
%       \toprule[1.3pt]
%        3275 & 273 & 500 & 87.8 \\
%       \bottomrule[1.3pt]
%     \end{tabular}
%     \label{tab:statistics_ner}
% \end{table}

\textbf{Strategy II.} When closely examining Eq.~\ref{eq:loss_final}, we can observe the \emph{Contradictory Phenomenon} as discussed in Sec.~\ref{sec:introduction}. In Strategy I, ${\ell}_{sup}^\text{task}$ treats instances with the same subtask labels (e.g., charge labels) as positive examples. However, these instances may have different other subtasks labels (e.g., applicable law labels or term of penalty labels). As a result, ${\ell}_{sup}^\text{task}$ will force the $SharedFactEncoder$ to learn features that benefit one subtask but degrade the performance of the other two tasks. 

To solve this problem, we propose to view the instances whose three subtask labels are all the same as positive examples and impose the MoCo-based SCL on the shared features $\mathbf{H}_f$. Specifically, we use a feature queue $\mathcal{Q}^{B}$ to store the shared features $\textbf{H}_f$, and three label queues $\mathcal{L}^c$, $\mathcal{L}^l$ and $\mathcal{L}^t$ to store three subtask labels. Then the positive samples set for sample $i$ is denoted as $P(i)=\{q|\mathcal{L}^c(q)=y_i^c, \mathcal{L}^l(q)=y_i^l, \mathcal{L}^t(q)=y_i^t,q \in \mathcal{Q}^B\}$ where $\mathcal{L}^\text{task}(q)$ denotes the label of index $q$ in each task label queue. Based on $P(i)$, we can use Eq.~\ref{eq:loss_cll} to compute the contrastive loss, denote as $\ell^B_{sup}$. Strategy II is able to address the \emph{Contradictory Phenomenon} in Strategy I and improve the performance of all three subtasks.

The final training objective of Strategy II is defined by:
\begin{equation}
    \ell = {\ell}_{ce} + \lambda \ell^B_{sup}
    \label{eq:loss_final_strategy2}
\end{equation}
where $\lambda$ is a hyperparameter.

\begin{table}[t]
\centering
\footnotesize
\begin{tabular}{lrrr}
\toprule[1.2pt]
\textbf{Dataset} & \textbf{CAIL-Small} & \textbf{CAIL-Big} & \textbf{CAE} \\
\toprule[1.2pt]
\#Training Set Cases  & 101,619 & 1,588,381 & 3275 \\
\#Validation Set Cases & 13,769  & 13,769   & 273 \\
\#Test Set Cases       & 26,749  & 185,290  & 500 \\
\#Charges              & 119     & 134      & -   \\
\#Law Articles         & 103     & 121      & -   \\
\#Term of Penalty      & 11      & 11       & -   \\
\bottomrule[1.2pt]
\end{tabular}
\caption{Statistics of the used legal judgment prediction datasets (CAIL-Small and CAIL-Big) and Crime Amount Extraction (CAE) dataset.}
\label{tab:data_statistcs}
\vspace{-1em}
\end{table}

% \begin{table}[t]
%     \centering
%     \begin{tabular}{lcccccc}
%     \toprule[1.2pt]
%     \multirow{2}{*}{\textbf{Methods}} &  \multicolumn{2}{c}{\textbf{Charges}} & \multicolumn{2}{c}{\textbf{Law Articles}} & \multicolumn{2}{c}{\textbf{Term of Penalty}} \\\cmidrule(r){2-3} \cmidrule(r){4-5} \cmidrule(r){6-7}
%      & \textbf{Acc} & \textbf{F1}  &  \textbf{Acc} & \textbf{F1} &  \textbf{Acc} & \textbf{F1} \\
%     \toprule[1.2pt]
%     LADAN  & 83.25 & 82.42 & 80.38 & 75.87 & 38.21 & 34.28 \\\hline
%     LADAN+I & 84.96 & 82.77 & 80.70 & 76.40 & 37.64 & 34.43 \\
%     LADAN+II & \textbf{86.01} & \textbf{83.83} & 80.97 & 77.04 & 39.50 & 35.71 \\
%     LADAN+I+II & 85.21 & 83.47 & \textbf{81.27} & \textbf{77.38} & \textbf{39.70} & \textbf{35.77} \\
%     \bottomrule[1.2pt] 
%     \end{tabular}
%     \caption{Effects of different supervised contrastive learning strategies.}
%     \label{table:develop_strategies}
% \end{table}
\section{Experiments}
\label{sec:exp}
\subsection{Datasets}
To evaluate the effectiveness of our framework, we conduct experiments on two real-world datasets (i.e., CAIL-Small and CAIL-Big)~\cite{xiao2018cail2018}. Each instance in both datasets contains one fact description, one applicable law article, one charge, and one term of penalty. To ensure a fair comparison, we use the code released by~\cite{xu2020distinguish} to process the data. All models are trained on the same dataset. The crime Amount Extraction (CAE) dataset is also a real-world dataset from the Chinese Legal AI challenge~\footnote{\url{http://data.court.gov.cn/pages/laic2021.html}}. Table~\ref{tab:data_statistcs} shows the statistics of the used datasets.

To specifically evaluate our framework on confusing legal cases, we define a set of confusing and number-sensitive charges. Due to page limitations, the details and statistics of these charges definitions are listed in Table~\ref{tab:statistics_confuse_number_charges} in the Appendix 

\subsection{Implementation Details}
We follow~\cite{xu2020distinguish} to conduct data pre-processing. The THULAC~\footnote{\url{https://github.com/thunlp/THULAC}} tool is used to segment Chinese into words. The word embedding layer in the neural network is initialized by pre-train word embeddings provided by~\cite{zhong2018legal}. More training details about the BERT-CRF NER model, the LJP model, and the NumberEncoder model can refer to Table~\ref{table:Hyper-parameter values} in the Appendix.
%The maximum document length and the maximum sentence number are set to 512 words and 15, respectively.
\subsection{Baselines}
We compare our framework with the following state-of-the-art baseliens: \textbf{HARNN}~\cite{yang-etal-2016-hierarchical}, \textbf{LADAN}~\cite{xu2020distinguish}, \textbf{NeurJudge+}~\cite{yue2021neurjudge}, \textbf{CrimeBERT}~\cite{zhong2019openclap}. The details of these baselines are left in the Appendix.

\subsection{Development Experiments}
To empirically evaluate which strategy is better for performing SCL for multi-task LJP, we conduct development experiments on CAIL-small using the LADAN backbone. The results are listed in Table~\ref{table:develop_strategies}. As can be observed, firstly, both Strategy I and II lead to improvements in LADAN's performance across the three subtasks. However, the gains of Strategy I is much smaller than those of Strategy II, which verifies the existence of \emph{Contradictory Phenomenon} in Strategy I. Furthermore, we also explore the effect of combining these two strategies, i.e., using $\ell_{cl} + \ell_{B}$ as the supervised contrastive loss. As seen, the improvement of this combination is not significant.

\begin{table}[t]
    \centering
    \footnotesize
    \begin{tabular}{lccc}
    \toprule[1.2pt]
    \textbf{Methods} & \textbf{Charges F1}  &  \textbf{ Articles F1} & \textbf{Term F1} \\
    \toprule[1.2pt]
    LADAN           & 82.42 & 75.87 & 34.28 \\\hline
    LADAN+I         & 82.77 & 76.40 & 34.43 \\
    LADAN+II        & \textbf{83.83} & 77.04 & 35.71 \\
    LADAN+I+II      & 83.47 & \textbf{77.38} & \textbf{35.77} \\
    \bottomrule[1.2pt] 
    \end{tabular}
    \caption{Effects of different supervised contrastive learning strategies.}
    \label{table:develop_strategies}
    \vspace{-1em}
\end{table}
Consequently, in the remaining experiments, we adopt Strategy II as the contrastive loss, unless otherwise specified. The final method combined the MoCo-based SCL and numerical evidence is denoted as \textbf{NumSCL}.

\subsection{Main Results}
To evaluate the effectiveness of the proposed framework, we augment each baseline with \textbf{NumSCL} and conduct experiments on the CAIL-Small and CAIL-Big datasets. Due to the expensive training cost and the large size of the training dataset, we did not evaluate CrimeBERT on CAIL-Big following~\cite{yue2021neurjudge}. The results are listed in Table~\ref{table:main_results_small} and Table~\ref{table:main_results_big}. 

\begin{table*}[t]
    \centering
    \small
    \begin{tabular}{lcccccccccccc}
    \toprule[1.2pt]
    \textbf{Tasks} &  \multicolumn{4}{c}{\textbf{Charges}} & \multicolumn{4}{c}{\textbf{Law Articles}} & \multicolumn{4}{c}{\textbf{Term of Penalty}} \\\cmidrule(r){2-5} \cmidrule(r){6-9} \cmidrule(r){10-13}
    \textbf{Metrics} &  \textbf{Acc.} & \textbf{MP} & \textbf{MR} & \textbf{F1} &  \textbf{Acc.} & \textbf{MP} & \textbf{MR} & \textbf{F1} &  \textbf{Acc.} & \textbf{MP} & \textbf{MR} & \textbf{F1} \\
    \toprule[1.2pt]
    HARNN & 84.54 & 82.56 & 82.94 & 82.26	& 80.09 & 76.46 & 77.69 & 75.95 & 38.38 & 36.12 & 33.99 & 34.32\\
    \hspace{1em}w/NumSCL & \textbf{85.26}	& \textbf{83.93}	& \textbf{83.76} & \textbf{83.39} & \textbf{81.07}	& \textbf{77.95} & \textbf{78.52}	& \textbf{77.11}	& \textbf{39.18} & 	\textbf{37.32}	& \textbf{34.50}	& \textbf{35.03}	\\
    \hline
    LADAN$_\text{MTL}$ & 84.90 & 82.55 &	83.26 & 82.42 & 80.38 & 75.84 & 77.84 & 75.67 & 38.21	& 35.95	& 34.01	& 34.28\\
    \hspace{1em}w/NumSCL & \textbf{85.37}	& \textbf{83.91} & \textbf{84.04}& \textbf{83.57} & \textbf{81.32} & \textbf{78.06} & \textbf{78.59} & \textbf{77.24} & \textbf{39.38} & \textbf{37.95} & \textbf{35.23} & \textbf{35.95} \\
    \hline
    NeurJudge$^{+}$& 83.25 & 82.11	& 81.69	& 81.3	& 80.95& 	77.93	& 78.59	& 77.00	& 37.88	& 37.20	& 33.82 & 	34.92 \\
    \hspace{1em}w/NumSCL & \textbf{84.45}	& \textbf{83.30} &	\textbf{83.55} &	\textbf{82.88} & \textbf{81.12} & \textbf{78.10} & \textbf{78.98} &	\textbf{77.32}&	\textbf{39.65}&	\textbf{39.48}&	\textbf{34.65} & \textbf{36.22} \\
    \hline
    CrimeBERT & \textbf{86.61} & 85.04 & 84.72 & 84.51 & 82.33 & 79.38 & 79.72 & 78.46 & 39.34 & \textbf{38.66} & 35.48 & 36.58 \\
    \hspace{1em}w/NumSCL & 85.91 & \textbf{85.71} & \textbf{85.98} & \textbf{85.54} & \textbf{82.63} & \textbf{80.10} & \textbf{80.88} & \textbf{79.50} & \textbf{39.72} & 38.50 & \textbf{35.84} & \textbf{36.67} \\
    % \hline\hline
    % BERT  & 88.61 & 87.10 & 87.20 & 86.80 & 83.44 & 81.48 & 82.1 & 80.72 & 42.82 & 41.64 & 38.99 & 39.86 \\
    % BERT + NSCL & 89.13 & 87.43 & 88.12 & 87.48 & 84.93 & 82.15 & 83.10 & 81.75 & 43.20 & 41.67 & 39.03 & 39.86 \\
    \bottomrule[1.2pt] 
    \end{tabular}
    \caption{Main results on CAIL-Small. Acc., MP, and MR are short for accuracy, macro precision, and macro recall, respectively.}
    \label{table:main_results_small}
\end{table*}

\begin{table*}[t]
    \centering
    \small
    \begin{tabular}{lcccccccccccc}
    \toprule[1.2pt]
    \textbf{Tasks} &  \multicolumn{4}{c}{\textbf{Charges}} & \multicolumn{4}{c}{\textbf{Law Articles}} & \multicolumn{4}{c}{\textbf{Term of Penalty}} \\\cmidrule(r){2-5} \cmidrule(r){6-9} \cmidrule(r){10-13}
    \textbf{Metrics} &  \textbf{Acc.} & \textbf{MP} & \textbf{MR} & \textbf{F1} &  \textbf{Acc.} & \textbf{MP} & \textbf{MR} & \textbf{F1} &  \textbf{Acc.} & \textbf{MP} & \textbf{MR} & \textbf{F1} \\
    \toprule[1.2pt]
    HARNN & 96.48 & 88.10 & 83.54 & 85.34 & 96.54 & 84.88 & 79.42 & 81.40 & \textbf{60.30} & 51.08  & 47.47 & 48.79 \\
    \hspace{1em}w/NumSCL & \textbf{96.55} & \textbf{88.84} & \textbf{83.96}	& \textbf{85.82}	& \textbf{96.60} & \textbf{86.63} & \textbf{81.16} & \textbf{82.99} & 60.20 & \textbf{52.18} & \textbf{47.50} & \textbf{49.06} \\
    \hline
    LADAN$_\text{MTL}$ & 96.56 & 88.58 & 84.17 & 85.95 & 96.61 & 86.40 & 80.46 & 82.52 & 60.44 & 51.56 & 48.67 & 49.78 \\
    \hspace{1em}w/NumSCL & \textbf{96.65} & \textbf{89.34} & \textbf{84.57} & \textbf{86.49} & \textbf{96.71} & \textbf{87.60} & \textbf{81.74} & \textbf{83.68} & \textbf{60.56} & \textbf{51.85} & \textbf{48.86} & \textbf{49.81} \\
    \hline
    NeurJudge$^{+}$ & 95.48 & 85.57 & 79.55 & 81.49	& \textbf{96.26}  & \textbf{85.78} & 81.38 & \textbf{82.80} & \textbf{58.40} & \textbf{49.67} & 43.32  & 44.90 \\
    \hspace{1em}w/NumSCL & \textbf{95.73} & \textbf{86.37} & \textbf{80.88} & \textbf{82.58} & 96.20 & 85.48 & \textbf{81.56}	& 82.78 & 58.40	& 49.10 & \textbf{43.54} & \textbf{45.43} \\
    \bottomrule[1.2pt] 
    \end{tabular}
    \caption{Main results on CAIL-Big. Acc., MP and MR are short for accuracy, macro precision, and macro recall, respectively.}
    \label{table:main_results_big}
    \vspace{-1em}
\end{table*}

From Table~\ref{table:main_results_small} and Table~\ref{table:main_results_big}, we make the following observations. Firstly, the proposed framework can improve all the baselines and achieve new state-of-the-art results on the two datasets. Specifically, on CAIL-Small, the absolute improvements reach up to 1.15, 1.57, and 1.67 F1 scores for the charge, law article, and term of penalty predictions, respectively. Secondly, on CAIL-Big, the gains are smaller, giving absolute improvements of 0.54, 1.16, and 0.53 F1 scores for the charge, law article, and term of penalty predictions, respectively. Thirdly, we observe that on CAIL-Big, NeurJudge$^{+}$ gives worse performance than the other baselines. We hypothesize that the complex neural network architecture of NeurJudge$^{+}$ may lead to overfitting on the CAIL-Big dataset. Lastly, for CrimeBERT, our framework can still obtain an absolute improvement of 1.03 and 1.04 F1 scores for charge and law article predictions. The overall gain on the term of penalty prediction is slight, however, when specifically evaluating number-sensitive legal cases, the improvement can still be up to a 1.79 F1 score, as shown in Table~\ref{table:ablation_numerical_models}.

\subsection{Ablation Studies}
\paragraph{Effect of Numerical Evidence for Number-Sensitive Legal Cases.}
To examine the effect of numerical evidence for predicting the term of penalty, we conduct ablative experiments. As shown in Table~\ref{table:ablation_numerical_models}, the improvement of MoCo-based SCL only is relatively tiny, only giving 0.11 and 0.05 F1 score improvements for HARNN and NeurJudge$^{+}$ on number-sensitive legal cases. However, when the models are further provided with the extracted numerical evidence, the F1 scores of all the baselines have a considerable boost. In particular, LADAN$_\text{MTL}$ obtains a 3.73 F1 score improvement on number-sensitive cases. These results show that the extracted crime amount is more beneficial for number-sensitive legal cases.

\begin{table}[t]
    \centering
    \footnotesize
    \begin{tabular}{lccc}
    \toprule[1.2pt]
    \textbf{Models} & \textbf{Acc.} & \textbf{F1} & \textbf{Num. F1} \\
    \toprule[1.2pt]
    HARNN & 38.38 & 34.32 & 28.13 \\
    \hspace{1em} w/ SCL & 38.62 & 34.45 & 28.24($\uparrow 0.11$) \\
    \hspace{1em} w/ NumSCL & 39.18 & 35.03 & 29.15($\uparrow 1.02$) \\\hline
    LADAN$_\text{MTL}$ & 38.21 & 34.28 & 26.83  \\
    \hspace{1em} w/ SCL & 39.50 & 35.71 & 28.54($\uparrow 1.71$)  \\
    \hspace{1em} w/ NumSCL & 39.38 & 35.95 & 30.56($\uparrow3.73$) \\\hline
    NeurJudge$^{+}$ & 37.88 & 34.92 & 27.30  \\
    \hspace{1em} w/ SCL & 38.11  & 34.96  & 27.35($\uparrow0.05$) \\
    \hspace{1em} w/ NumSCL & 39.65 & 36.22 & 29.03($\uparrow1.73$) \\\hline
    CrimeBERT         & 39.34  & 36.58 & 28.57  \\
    \hspace{1em} w/ SCL    & 39.71  & 36.48 & 28.99($\uparrow0.42$) \\
    \hspace{1em} w/ NumSCL & 39.68  & 36.66 & 30.36($\uparrow1.79$) \\
    \bottomrule[1.2pt] 
    \end{tabular}
    \caption{Effects of the proposed method on the term of penalty prediction of number-sensitive legal cases. Num. F1 is the F1 score on the defined number-sensitive charges.}
    \label{table:ablation_numerical_models}
    \vspace{-1em}
\end{table}

% \begin{table*}[t]
% \begin{tabular}{|c|c|c|c|c|}
% \hline
% Fact Description          & \multicolumn{4}{l|}{\begin{tabular}[c]{@{}m{12cm}@{}}At xxx, defendant xxx fought with victim xxx \colorbox{lightorange}{due to trivial matters} in xxx City and wanted to take  revenge on victim xxx.  At xxx of the same year, defendant xxx gathered three people. \colorbox{darkorange}{The defendant xxx slashed victim xxx's} \colorbox{darkorange}{right arm with a knife}, then they fled the scene. According to the forensic identification, \colorbox{darkorange}{the victim xxx suffered minor injuries.}\end{tabular}} \\
% \hline
% \multirow{4}{*}{Judgment} & & Ground Truth & LADAN$_\text{MTL}$ & LADAN$_\text{MTL}$+NumSCL \\\cline{2-5}
%            & Law Article & Article 24  & Article 34\textcolor{red}{\ding{55}} & Article 24\textcolor{red}{\ding{51}} \\\cline{2-5}
%             & Charge & Picking a Quarrel & Intention Injury\textcolor{red}{\ding{55}} & Picking a Quarrel\textcolor{red}{\ding{51}} \\ \cline{2-5}
%             & Term of Penalty & 12 $\sim$24 months & 36$\sim$48 months\textcolor{red}{\ding{55}} & 12 $\sim$24 months\textcolor{red}{\ding{51}} \\\hline
% \end{tabular}
% \caption{Case Studies (1).}
% \begin{tabular}{|c|c|c|c|c|}
% \hline
% Fact Description          & \multicolumn{4}{l|}{\begin{tabular}[c]{@{}m{12cm}@{}}At xxx, defendant xxx fought with victim xxx \colorbox{lightorange}{due to trivial matters} in xxx City and wanted to take  revenge on victim xxx.  At xxx of the same year, defendant xxx gathered three people. \colorbox{darkorange}{The defendant xxx slashed victim xxx's} \colorbox{darkorange}{right arm with a knife}, then they fled the scene. According to the forensic identification, \colorbox{darkorange}{the victim xxx suffered minor injuries.}\end{tabular}} \\
% \hline
% \multirow{4}{*}{Judgment} & & Ground Truth & LADAN$_\text{MTL}$ & LADAN$_\text{MTL}$+NumSCL \\\cline{2-5}
%            & Law Article & Article 24  & Article 34\textcolor{red}{\ding{55}} & Article 24\textcolor{red}{\ding{51}} \\\cline{2-5}
%             & Charge & Picking a Quarrel & Intention Injury\textcolor{red}{\ding{55}} & Picking a Quarrel\textcolor{red}{\ding{51}} \\ \cline{2-5}
%             & Term of Penalty & 12 $\sim$24 months & 36$\sim$48 months\textcolor{red}{\ding{55}} & 12 $\sim$24 months\textcolor{red}{\ding{51}} \\\hline
% \end{tabular}
% \caption{Case Studies.}
% \end{table*}
\paragraph{Effect of Contrastive Learning for Confusing Charges.}
We conduct experiments to validate the effect of contrastive learning for predicting charges, particularly confusing charges. As shown in Table~\ref{table:ablation_confusing_models}, the absolute F1 score improvements of confusing charges are greater than those of the overall charges. For example, NeurJudge$^{+}$ obtains an absolute 2.55 F1 score improvement on confusing charges, which demonstrates the effectiveness of the moco-based SCL in learning distinguishable representations for confusing charges.

\paragraph{Effect of Momentum Contrast Queue.}
We utilize LADAN as the backbone for comparing NumSCL with the in-batch SCL(SCL) which takes the current mini-batch as the lookup dictionary to compute the contrastive loss. The in-batch SCL is trained using the same parameters as the MoCo-based SCL. As depicted in Table~\ref{tab:ablation_moco}, the in-batch SCL yields superior results when combined with LADAN but underperforms NumSCL in charge and law article prediction tasks. This highlights the advantage of employing a large queue as the lookup dictionary in SCL. We also observe that the performance of the term of penalty is not significantly affected by the choice of the lookup dictionary. This observation aligns with the previous finding that the extracted numerical evidence is more beneficial than SCL for the term of penalty prediction.
%limited by the GPU memory size, 
% \textcolor{red}{\ding{51}}

\begin{table}[t]
    \centering
    \footnotesize
    \begin{tabular}{lccc}
    \toprule[1.2pt]
    \textbf{Models} & \textbf{Acc.} & \textbf{F1} & \textbf{Conf. F1} \\
    \toprule[1.2pt]
    HARNN & 84.54 & 82.26 & 75.60 \\
    \hspace{1em} w/ NumSCL & 85.26 & 83.39 & 77.19 ($\uparrow1.59$) \\\hline
    LADAN$_\text{MTL}$ & 84.90 & 82.42 & 75.29 \\
    \hspace{1em} w/ NumSCL & 85.37 & 83.57 & 76.76  ($\uparrow1.47$) \\\hline
    NeurJudge$^{+}$ & 83.25 & 81.30 & 73.42 \\
    \hspace{1em} w/ NumSCL & 84.45 & 82.88 & 75.97($\uparrow$2.55) \\
    \bottomrule[1.2pt] 
    \end{tabular}
    \caption{Effects of the proposed method on the charge prediction of confusing legal cases. Conf. F1 is the F1 score on the defined confusing charges.}
    \label{table:ablation_confusing_models}
    \vspace{-1em}
\end{table}

% \begin{table*}[t]
%     \centering
%     \small
%     \begin{tabular}{m{8.2cm}|m{2.2cm}|m{2.2cm}|m{2cm}}
%     \toprule[1.2pt]
%     \multirow{2}{*}{\textbf{Fact Descriptions}} & \multicolumn{3}{c}{Charge/Law Article/Term of Penalty}   \\\cmidrule(r){2-4}
%      & \textbf{GT labels} & LADAN$_\text{MTL}$ & LADAN$_\text{MTL}$ + NumSCL  \\
%     \toprule[1.2pt]
%     At xxx, defendant xxx fought with victim xxx \colorbox{darkorange}{due to trivial matters} in xxx City and wanted to take revenge on victim xxx. At xxx of the same year, defendant xxx gathered three people. The defendant xxx slashed the victim xxx's right arm with a knife, then they fled the scene. According to the forensic identification, the victim xxx suffered minor injuries & \begin{flushleft} Picking a Quarrel \\ 24 \\ 6 \end{flushleft} & \textcolor{red}{Crime of Intention Injury, Law Article 34 Term of Penalty 4} & Crime of Picking a Quarrel, Article 24, TOP 6 \\
%     \midrule
%     From xxx to xxx, defendant xxx committed three thefts in xxx City. 1. At xxx, defendant xxx stole an electric tricycle worth \colorbox{lightgreen}{\$5,600} from victim xxx. 2. On xxx, defendant xxx stole an electric tricycle worth \colorbox{lightgreen}{\$4,800} from victim xxx in xxx city. 3. At xxx, defendant xxx stole an electric vehicle battery from victim xxx but was attempted.
%     & Crime of Theft, Law Article 32, Term of Penalty 7 & \textcolor{red}{Crime of Theft, Law Article 56, Term of Penalty 9} & Crime of Theft, Law Article 56, Term of Penalty 7, Extracted Crime Amount \$10400 \\
%     \bottomrule[1.2pt]
%     \end{tabular}
%     \caption{Case studies.}
% 	\label{tab_case_study}
%     \vspace{-2em}
% \end{table*}
% \begin{figure}[t]
%     \centering
%     \includegraphics[width=0.45\textwidth]{figures/case_study.pdf}
%     \caption{Qualitative examples to demonstrate the effect of the proposed framework.}
%     \label{fig:case_studies}
%     \vspace{-1em}
% \end{figure}


% Methods	Acc F1	Law F1	Term F1
% LADAN	82.42	75.67	34.28
% LADAN_\text{SCL}	83.42	76.42	36.12
% LADAN_\text{MoCoSCL}	83.57	77.24	35.95

\begin{table}[t]
    \centering
    \footnotesize
    \begin{tabular}{lccc}
    \toprule[1.2pt]
    \textbf{Models} & \textbf{Charge F1} & \textbf{Law F1} & \textbf{Term F1} \\
    \toprule[1.2pt]
    LADAN & 82.42 & 75.67 & 34.28 \\
    LADAN$_\text{SCL}$ & 83.18 & 76.24 & \textbf{36.12} \\
    LADAN$_\text{NumSCL}$ & \textbf{83.57} & \textbf{77.24} & 35.95 \\
    \bottomrule[1.2pt]
    \end{tabular}
    \caption{Effect of momentum contrast queue.}
    \label{tab:ablation_moco}
    \vspace{-1em}
\end{table}

\begin{figure}[t]
    \centering
    \includegraphics[width=0.4\textwidth]{figures/case_study.pdf}
    \caption{Qualitative examples to demonstrate the effect of the proposed framework.}
    \label{fig:case_studies}
    \vspace{-1em}
\end{figure}

\begin{figure}[t]
    \centering
    \includegraphics[width=0.45\textwidth]{figures/dev_lambda.pdf}
    \caption{The impact of $\lambda$ in Eq.\ref{eq:loss_final_strategy2} for the three subtasks of LJP.}
    \label{fig:effect_lambda}
    \vspace{-1em}
\end{figure}

\paragraph{Effect of $\lambda$.}
We carry out experiments to verify the impact of $\lambda$ in Eq.\ref{eq:loss_final_strategy2}. As illustrated in Figure\ref{fig:effect_lambda}, with the increasing of $\lambda$, the performance of charge and law article predictions correspondingly improves. We also observe a fluctuation in the term of penalty prediction, again showing that the extracted numerical evidence plays a more significant role than SCL for predicting the term of penalty.

\section{Case Studies}
Figure~\ref{fig:case_studies} shows two cases to qualitatively demonstrate the effect of the proposed framework. In the first case, LADAN$_\text{MTL}$ incorrectly predicts the case's charge into its confusing charge \textit{Crime of Intentional Injury}, which should be \textit{Crime of Picking a Quarrel}. However, with the proposed framework, this error is corrected. The second case demonstrates the effect of numerical evidence. LADAN$_\text{MTL}$ incorrectly predicts the case's term of penalty as label 9, meaning a sentence of fewer than 6 months. Given the accurately extracted crime amount of \$10,400, which is a relatively large crime amount, the model correctly predicts the term of penalty as label 7, meaning a sentence of more than 9 months but less than 12 months.
\section{Conclusion}
\label{sec:conclusion}
In this paper, we present a framework that introduces MoCo-based supervised contrastive learning and weakly supervised numerical evidence for confusing legal judgment prediction. The framework is capable of automatically extracting numerical evidence for predicting number-sensitive cases and learning distinguishable representations to benefit all three subtasks of LJP simultaneously. Extensive experiments validate the effect of the framework. 
}
\section*{Limitations}
While the used 0-1 knapsack algorithm enjoys the merit of automatically constructing a training dataset for building the NER model, it cannot accurately calculate the crime amount when the suspects return some properties to the victims as the returned properties should be subtracted from the amount of the crime. More sophisticated techniques could be developed to calculate the amount of crime more precisely. 

Our LJP research focuses on Chinese legal documents under the jurisdiction of the People's Republic of China. While the framework was developed and tested specifically for the 3-task Chinese Legal Judgment Prediction (LJP), we believe the underlying methodology could be generalized and applied to other LJP tasks, even those from different jurisdictions. However, this would likely require modifications to account for the unique characteristics and complexities of each jurisdiction's legal system. We will leave this for future work.

\section*{Ethical Concerns}
Due to the sensitive nature of the legal domain, applying artificial intelligence technology to the legal field should be carefully treated. In order to alleviate ethical concerns, we undertake the following initiatives. First, to prevent the risk of leaking personal private information from the evaluated real-world datasets, sensitive information, such as names of individuals and locations, has been anonymized. Second, we suggest the predictions generated by our model should be served as supportive references to assist judges to make judgments more efficiently, rather than solely determining the judgments.

% \section*{Limitations}
% EMNLP 2023 requires all submissions to have a section titled ``Limitations'', for discussing the limitations of the paper as a complement to the discussion of strengths in the main text. This section should occur after the conclusion, but before the references. It will not count towards the page limit.  

% The discussion of limitations is mandatory. Papers without a limitation section will be desk-rejected without review.
% ARR-reviewed papers that did not include ``Limitations'' section in their prior submission, should submit a PDF with such a section together with their EMNLP 2023 submission.

% While we are open to different types of limitations, just mentioning that a set of results have been shown for English only probably does not reflect what we expect. 
% Mentioning that the method works mostly for languages with limited morphology, like English, is a much better alternative.
% In addition, limitations such as low scalability to long text, the requirement of large GPU resources, or other things that inspire crucial further investigation are welcome.

% \section*{Ethics Statement}
% Scientific work published at EMNLP 2023 must comply with the \href{https://www.aclweb.org/portal/content/acl-code-ethics}{ACL Ethics Policy}. We encourage all authors to include an explicit ethics statement on the broader impact of the work, or other ethical considerations after the conclusion but before the references. The ethics statement will not count toward the page limit (8 pages for long, 4 pages for short papers).

%\section*{Acknowledgements}

% Entries for the entire Anthology, followed by custom entries
\bibliography{anthology,custom}
\bibliographystyle{acl_natbib}

\appendix
\section{Baselines}
\label{sec:appendix:baselines}
We compare our method with the following non-pretrained and pre-trained models:
(1) \textbf{HARNN}~\cite{yang-etal-2016-hierarchical}: an RNN-based neural network with a hierarchical attention mechanism for document classification; (2) \textbf{LADAN}~\cite{xu2020distinguish}: LADAN distinguishes confusing law articles by extracting distinguishable features from similar law articles using a graph-based method; (3) \textbf{NeurJudge+}~\cite{yue2021neurjudge}:NeurJudge+ utilizes the results of intermediate subtasks to separate the fact description into different representations and exploits them to make the predictions of other subtasks; (4) \textbf{CrimeBERT}~\cite{zhong2019openclap}: CrimeBERT is initialized by BERT~\cite{devlin2019bert}, then is further pre-trained on crime data, giving better results than BERT. It is worth noting that the proposed method is model-agnostic, which can be used to improve any other LJP models. 

\begin{table}[t]
	\centering
    \footnotesize
	\begin{tabular}{l|l|l}
		\toprule[1.2pt]
		\textbf{Model}&\textbf{Parameter}&\textbf{Value}\\
		\toprule[1.2pt]
            \multirow{11}{*}{LJP} & Word embedding size & 200  \\
            & Maximum document length & 512 \\
            & Maximum sentence num & 15 \\
		& Batch size & 128  \\
            & Learning rate & 0.001 \\
            & Training epoch & 16 \\
            & Optimizer & Adam \\
            & Momentum queue $\mathcal{Q}$ size & 65536 \\
            & Momentum coefficient $m$ & 0.999 \\
            & Temperature t & 0.07 \\
            & $\alpha$, $\beta$, $\theta$, $\lambda$ & 2, 2, 5, 7 \\
            \hline
            \multirow{5}{*}{NER} & Model & BERT-CRF \\
             & Batch size & 16 \\
             & Learning rate & 0.00001 \\
             & Training epoch & 20 \\
             & Optimizer & Adam \\
            \hline
            \multirow{9}{*}{NumEncoder} & Training data size & 128000\\
             & Max number, Min number & 0, 300000 \\
             & Model & GRU \\
             & Word embedding size & 200 \\
             & Hidden state size & 256 \\
             & Batch size & 128  \\
             & Learning rate & 0.001 \\
             & Training epoch & 100 \\
             & Optimizer & Adam \\
	\bottomrule[1.2pt]
	\end{tabular}
    \caption{Hyper-parameter values}
	\label{table:Hyper-parameter values}
\end{table}

\section{Implemantation Details}
\label{sec:appendix:Hyper-Parameters}
For training the BERT-CRF NER model, we use the Adam optimizer and set the learning rate to 1e-5. The batch size is set to 16. We train the model for 20 epochs and select the best model on the validation set for testing. The best model on the test set of CAE can achieve a competitive accuracy of 0.875.

For pre-training the numerical evidence encoder model NumEncoder, we synthesize a training dataset of size 128,000 where each pair of numbers $(x_i, y_i)$ are uniformly sampled from $[0, 30,0000]$. A GRU model with a hidden size of 256 is used to model the number sequence. Adam is used to optimizing the parameters, and the learning rate is set to 1e-3. We train the NumEncoder model for 100 epochs.

For training the LJP model, we use the Adam optimizer and set the learning rate to 1e-3. The batch size is set to 128. We train the model for 16 epochs and select the best model on the validation set for testing. In contrastive learning, the MoCo queue size and the temperate $t$ are set to 65536 and 0.07, respectively. We run each experiment with five different seeds and report the averaged results.

Table~\ref{tab:data_statistcs} shows the detailed statistics of the used datasets.

\begin{figure}[t]
    \centering
    \includegraphics[width=0.5\textwidth]{figures/money_extraction.pdf}
    \caption{An example of converting an instance in the CAE dataset into the sample for training the NER model.}
    \label{fig:Numerical_evidence_extraction}
\end{figure}

\section{Evidence Extraction}
Figure~\ref{fig:Numerical_evidence_extraction} shows an example of converting an instance in CAE into the NER format.

\section{Definition of Confusing Charges.} 
To specifically evaluate our method on confusing legal cases, we define confusing charges using the predicted results of the baseline model LADAN$_\text{MTL}$. Concretely, if the number that the model incorrectly classifies class A into class B exceeds the pre-defined threshold, classes A and B will be added to the confusing classes. The definition of the number-sensitive charges is determined by an experienced legal expert. The statistics of the definition of confusing charges and number-sensitive charges are listed in Table~\ref{tab:statistics_confuse_number_charges}.

% \section{Visualization.} 
% We use t-SNE~\cite{van2008visualizing} to visualize the features $\mathbf{H}_c$ used for predicting charges \emph{Crime of Picking a Quarrel}, \emph{Crime of Intentional Injury}, and \emph{Crime of Crowd Fight}, which are easily confused with one another. As shown in Figure~\ref{fig:vis}, compared with LADAN$_\text{MTL}$, the features learned through Strategy II (e.g., those for \emph{Crime of Picking a Quarrel}) are more compact. However, the features learned through Strategy I for \emph{Crime of Intentional Injury} are less compact than their counterparts learned via Strategy II, which demonstrates the advantage of Strategy II.


\begin{table}[t]
    \centering
    \small
    \begin{tabular}{c|c|m{2cm}}
    \toprule[1.3pt]
      \textbf{Charge Type} & \textbf{\#Classes} & \textbf{\%Training Set size}\\
      \toprule[1.3pt]
      Confusing Charges   &  41 & 37.13\% \\
      Number-sensitive Charges   & 33 & 22.25\% \\
      \bottomrule[1.3pt]
    \end{tabular}
    \caption{Statistics of the defined confusing charges and number-sensitive charges.}
    \label{tab:statistics_confuse_number_charges}
\end{table}

% \begin{figure*}[t]
%     \centering
%     \begin{subfigure}[b]{0.31\textwidth}
%         \includegraphics[width=1\textwidth]{figures/ladan_accu_0000.pdf}
%         \caption{LADAN$_\text{MTL}$}
%         \label{fig:ladan_mtl}
%     \end{subfigure}%
%     \begin{subfigure}[b]{0.31\textwidth}
%         \includegraphics[width=1\textwidth]{figures/ladan_accu_3390.pdf}
%         \caption{LADAN$_\text{MTL}$+Strategy I}
%         \label{fig:ladan_mtl_3290}
%     \end{subfigure}%
%     \begin{subfigure}[b]{0.31\textwidth}
%         \includegraphics[width=1\textwidth]{figures/ladan_accu_0007.pdf}
%         \caption{LADAN$_\text{MTL}$+Strategy II}
%         \label{fig:ladan_mtl_0007}
%     \end{subfigure}%
% \caption{Effect of different supervised contrastive learning strategies.}
% \label{fig:vis}
% \vspace{-1em}
% \end{figure*}

\end{document}
