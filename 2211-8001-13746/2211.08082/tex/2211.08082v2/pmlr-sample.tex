%%%% SELECT ONE OF THE FOLLOWING COMMANDS %%%%%%%%

%%% TEMPLATE FOR PROCEEDINGS TRACK %%%%
%\documentclass[eat,twocolumn]{jmlr}

%% TEMPLATE FOR Extended Abstract Track %%%%%%%
\documentclass[eat,twocolumn]{jmlr}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%
% Watermark 
%These 4 commands must be removed for the camera-ready version.
% \usepackage[hpos=300px,vpos=70px]{draftwatermark}
% \SetWatermarkText{\test}
% \SetWatermarkScale{1}
% \SetWatermarkAngle{0}
%%%%%%%%%%%%%%%%%%%%%%%%%%


   
% The following packages will be automatically loaded:
% amsmath, amssymb, natbib, graphicx, url, algorithm2e


%%% WARNING %%%%
%%% 1) Please, use the packages automatically loaded to manage references, write equations, and include figures and algorithms. The use of different packages could create problems in the generation of the camera-ready version. Please, follow the examples provided in this file.
%%% 2) References must be included in a .bib file.
%%% 3) Write your paper in a single .tex file.
%%%

%%%% SOFTWARE %%%%
%%% Many papers have associated code provided. If that is your case, include a link to the code in the paper as usual and provide a link to the code in the following comment too. We will use the link in the next comment when we generate the proceedings.
%%% Link to code: http://?? (only for camera ready)

 %\usepackage{rotating}% for sideways figures and tables
\usepackage{longtable}% for long tables

 % The booktabs package is used by this sample document
 % (it provides \toprule, \midrule and \bottomrule).
 % Remove the next line if you don't require it.
\usepackage{booktabs}
 % The siunitx package is used by this sample document
 % to align numbers in a column by their decimal point.
 % Remove the next line if you don't require it.
\usepackage[load-configurations=version-1]{siunitx} % newer version
 %\usepackage{siunitx}

 % The following command is just for this sample document:
\newcommand{\cs}[1]{\texttt{\char`\\#1}}
\newcommand{\ours}{Universal Healthcare Predictive Framework}
\newcommand{\oursarc}{UniHPF}
 % Define an unnumbered theorem just for this sample document:
\theorembodyfont{\upshape}
\theoremheaderfont{\scshape}
\theorempostheader{:}
\theoremsep{\newline}
\newtheorem*{note}{Note}

\usepackage{adjustbox}
\usepackage{booktabs, multirow}
\usepackage{graphicx}
\usepackage{paralist}
\usepackage{amsmath}
\usepackage{enumitem}
\usepackage{amsfonts}
% \usepackage{amssymb}
\usepackage{soul}% for underlines
\usepackage{multirow}
\usepackage{multicol}
\usepackage[compact]{titlesec}
\titlespacing{\section}{5pt}{2ex}{1ex}
\titlespacing{\subsection}{3pt}{1ex}{0ex}
\input{setting.tex}

%%%% DON'T CHANGE %%%%%%%%%
\jmlrvolume{}
\firstpageno{1}
% \editors{List of editors' names}

\jmlryear{2022}
\jmlrworkshop{Machine Learning for Health (ML4H) 2022}

%\editor{Editor's name}
%%%%%%%%%%%%%%%%%%%%%%%%%%%



\title[UniHPF]{UniHPF:Universal Healthcare Predictive Framework \titlebreak with Zero Domain Knowledge}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THE MANUSCRIPT, DATA AND CODE MUST BE ANONYMIZED DURING THE REVIEW PROCESS. 
% DON'T INCLUDE ANY INFORMATION ABOUT AUTHORS DURING THE REVIEW PROCESS.
% Information about authors (Full names, emails, affiliations) have to be provided only for the submission of the camera-ready version.  Only in that case, you can uncomment and use the next blocks.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

 % Use \Name{Author Name} to specify the name.

 % Spaces are used to separate forenames from the surname so that
 % the surnames can be picked up for the page header and copyright footer.
 
 % If the surname contains spaces, enclose the surname
 % in braces, e.g. \Name{John {Smith Jones}} similarly
 % if the name has a "von" part, e.g \Name{Jane {de Winter}}.
 % If the first letter in the forenames is a diacritic
 % enclose the diacritic in braces, e.g. \Name{{\'E}louise Smith}

 % *** Make sure there's no spurious space before \nametag ***

 % Two authors with the same address
%   \author{\Name{Author Name1\nametag{\thanks{with a note}}} \Email{abc@sample.com}\and
%   \Name{Author Name2} \Email{xyz@sample.com}\\
%   \addr Address}
\author{
\Name{Kyunghoon Hur} \Email{pacesun@kaist.ac.kr} \\ 
\Name{Jungwoo Oh} \Email{ojw0123@kaist.ac.kr} \\
\Name{Junu Kim} \Email{kjune0322@kaist.ac.kr} \\
\Name{Jiyoun Kim} \Email{jiyoun.kim@kaist.ac.kr} \\
\Name{Min Jae Lee} \Email{mjbooo@kaist.ac.kr} \\
\Name{Eunbyeol Cho} \Email{eunbyeol.cho@kaist.ac.kr} \\
\addr KAIST \\
\Name{Seong-Eun Moon} \Email{seongeun.moon@navercorp.com} \\
\addr NAVER AI Lab \\
\Name{Younghak Kim} \Email{mdyhkim@amc.seoul.kr} \\
\addr Asan Medical Center, University of Ulsan College of Medicine \\
\Name{Edward Choi} \Email{edwardchoi@kaist.ac.kr}\\
\addr KAIST
}
  %Three or more authors with the same address:
%   \author{\Name{Author Name1} \Email{an1@sample.com}\\
%   \Name{Author Name2} \Email{an2@sample.com}\\
%   \Name{Author Name3} \Email{an3@sample.com}\\
%   \Name{Author Name4} \Email{an4@sample.com}\\
%   \Name{Author Name5} \Email{an5@sample.com}\\
%   \Name{Author Name6} \Email{an6@sample.com}\\
%   \Name{Author Name7} \Email{an7@sample.com}\\
%   \Name{Author Name8} \Email{an8@sample.com}\\
%   \Name{Author Name9} \Email{an9@sample.com}\\
%   \Name{Author Name10} \Email{an10@sample.com}\\
%   \Name{Author Name11} \Email{an11@sample.com}\\
%   \Name{Author Name12} \Email{an12@sample.com}\\
%   \Name{Author Name13} \Email{an13@sample.com}\\
%   \Name{Author Name14} \Email{an14@sample.com}\\
%   \addr Address}


 % Authors with different addresses:
 % \author{\Name{Author Name1} \Email{abc@sample.com}\\
 % \addr Address 1
 % \AND
 % \Name{Author Name2} \Email{xyz@sample.com}\\
 % \addr Address 2
 %}



\begin{document}

\maketitle

\begin{abstract}
Despite the abundance of Electronic Healthcare Records (EHR), its heterogeneity restricts the utilization of medical data in building predictive models.
To address this challenge, we propose Universal Healthcare Predictive Framework (UniHPF), which requires no medical domain knowledge and minimal pre-processing for multiple prediction tasks. 
Experimental results demonstrate that UniHPF is capable of building large-scale EHR models that can process any form of medical data from distinct EHR systems. 
We believe that our findings can provide helpful insights for further research on the multi-source learning of EHRs.
\end{abstract}
\begin{keywords}
electronic health records, multi-source learning, zero domain knowledge
\end{keywords}

\section{Introduction} \label{sec:intro}

Patient medical records are accumulated regularly in the form of Electronic Health Records (EHR), enabling quality treatment based on patients' medical history. 
% The abundance of EHR has opened the possibility of developing data-driven models, which aim to increase the quality of medical predictions.
However, typical EHR datasets do not follow a single data format since each hospital stores EHR data according to their own needs.
Specifically, different EHR systems adopt different medical code standards (\textit{e.g., ICD-9, ICD-10, raw text}), and use distinct database schemas to store patient records~\citep{johnson2016mimic, johnson2021mimic, pollard2018eicu}.

Such heterogeneity is problematic because it acts as a barrier towards EHR model development.
In particular, when using patient clinical data, each hospital must employ its own data experts to rigorously pre-process EHR. Figure~\ref{fig:fig1} shows a typical framework for EHR-system-driven predictive models. In addition, discrepancies in medical codes and schemas prevent multiple healthcare organizations from conducting multi-source learning, such as further training a model that has been previously trained on a distinct EHR database (\textit{i.e., transfer learning}) or developing a model with EHR data pooled from multiple hospitals (\textit{i.e., pooled learning}).
% \begin{figure}[ht] 
% \floatconts
%     {fig:fig1}
%     {\caption{ 
%     % An illustration of the conventional EHR system-specific framework.
%     A typical EHR predictive model framework involves domain-knowledge-based pre-processing for each medical center’s schema, which requires schema-specific and code system-specific feature engineering.
%     % This results in input features being dependent on each hospital and incompatible with models among different hospitals.
%     }}
%     {\includegraphics[width=1.0\linewidth]{figures/fig1.pdf}}
% \end{figure}
 
%4
Previous studies have attempted to overcome this dissimilarity in several ways. 
For instance, \citet{rajkomar2018scalable} used FHIR~\citep{mandel2016smart}, a type of Common Data Model (CDM) to manually standardize distinct EHR data into a single format.
% AutoMap~\citep{wu2022automap} learned to apply direct code mapping between different EHR systems in a self-supervised manner.
% In addition, DescEmb~\citep{hur2022unifying} aimed to overcome the heterogeneity of medical codes by utilizing clinical descriptions linked to each code, partially enabling multi-source learning.
% Despite their progress, these approaches only provide a partial solution to EHR heterogeneity because they still necessitate EHR system-specific healthcare expertise to select meaningful features, which is often expensive and not always optimal.
In addition, DescEmb~\citep{hur2022unifying} aimed to overcome the heterogeneity of medical codes by utilizing clinical descriptions linked to each code, partially enabling multi-source learning.
Despite their progress, they still necessitate EHR system-specific healthcare expertise to select meaningful features.


In this work, we propose \ours{} (\oursarc).
Our framework presents a method for embedding any form of EHR systems for prediction tasks without requiring domain-knowledge-based pre-processing, such as medical code mapping and feature selection.
% As such, \oursarc{} enables the build-up of a large-scale healthcare model by utilizing massive amounts of data collected from multiple hospitals.
% The main contributions of this study can be summarized as follows:
We believe that our findings can provide helpful insights for further research on the multi-source learning of EHR.

% \begin{compactitem}
%   %\setlength{\itemindent}{0em}
%   \item
%   We propose \oursarc, a
%   universal framework that facilitates the learning of any EHR data without relying on domain knowledge.
%   To the best of our knowledge, this is the first approach that handles various heterogeneous EHRs with a single unified framework, without requiring any prior knowledge of each distinct EHR.
%   \item
%   Our method achieves comparable performance on single EHR dataset tasks, while consistently showing superior performance on pooled learning and transfer learning which require a model to comprehend various heterogeneous EHR systems.
%   This implies that our framework serves as a guideline for building large-scale EHR models that can process any form of EHR systems from multiple sites.
% \item
%   To empirically demonstrate the efficacy of our work, we conducted extensive experiments with various datasets, model structures, and prediction tasks.
%   We believe that our findings can provide helpful insights for further research on the multi-source learning of EHR.
% \end{compactitem}




\begin{figure*}[ht]
\floatconts
    {fig:fig2}
    {\caption{
    Overview of \oursarc. On the top, a patient's series of medical events occur over time.
    Each medical event $\mathcal{M}_i$ is made up of event-related features $A_i^k$, including feature names and their values.
    These features, prepended with event type $e_i$, are converted to corresponding descriptions, and tokenized into a sequence of sub-words.
    Then an event encoder $f$ converts the sequence to an embedding $\mb_i$, which is passed to the event aggregator $g$, which then makes a prediction $\hat{\yb}$.
    } \vskip -25pt }
    {\includegraphics[width=0.6\textwidth]{figures/fig2.pdf}}
\end{figure*}



\section{Related work} \label{sec:related work}
\noindent{\textbf{Domain-knowledge-based predictive models.}}
% Previous prediction tasks based on EHR utilize architectures such as recurrent neural networks~\citep{lipton2015learning, choi2016doctor}, convolutional neural networks~\citep{nguyen2016mathtt}, and transformer-based models~\citep{song2018attend, shang2019pretraining, choi2020learning}. 
% Additionally,
Several studies on EHR-based prediction have attempted to fully utilize medical domain knowledge.
MIMIC-Extract~\citep{wang2020mimicextract} performs domain-knowledge-based feature engineering, such as grouping semantically similar concepts into clinical taxonomy.
Based on these heavily hand-crafted features, \citet{mcdermott2021a} proposed a benchmark for various healthcare predictive tasks.
% for ten healthcare predictive tasks and reported their prediction performances.
% Similarly, Graph Convolutional Transformer (GCT)~\citep{choi2020learning} utilizes domain knowledge by learning and employing the hidden graphical structure of EHR.
% Overall, many previous works invested considerable amount of time and effort into applying medical domain knowledge.
% As a result of being specialized in their own respective settings, they work only for a specific dataset, not for general EHR systems which are diverse and heterogeneous.

\noindent{\textbf{Resolving heterogeneous EHR systems.}}
Researchers have been working on alternatives to overcome heterogeneity in EHR without CDM, which is considered as one of the main challenges in the modeling of medical data.
% CDM is an approach that manually maps different EHR systems into a standardized format (\textit{e.g.,} FHIR), which has been reported to facilitate satisfactory results in multiple prediction tasks~\citep{rajkomar2018scalable}.
% However, the standardization of EHR formats requires extensive domain knowledge and intensive manual work, making it impractical to integrate a large number of EHR systems.
Meanwhile, AutoMap~\citep{wu2022automap} conducts medical code mapping via self-supervised learning with a predefined medical ontology which depends on EHR code systems.
% This study aimed to develop a solution for the current lack of a unified EHR system through the direct code-to-code mapping of two different medical institutions.
% However, because medical ontology is essential for using AutoMap, it is still not suitable for modeling code systems that do not have a standardized ontology.
% In another study, DescEmb~\citep{hur2022unifying} was proposed to significantly reduce the need for aligning different code systems.
% Specifically, instead of dealing with medical codes directly, DescEmb exploits the text descriptions corresponding to each medical code.
In another study, instead of dealing with medical codes directly, DescEmb~\citep{hur2022unifying}exploits the text descriptions corresponding to each medical code.
% Specifically, 
% demonstrating its efficacy in multi-source learning, such as transfer and pooled learning.
% Moreover, because text descriptions inherit the semantic meanings of their corresponding code, DescEmb showed promising performance in multiple clinical prediction tasks.
However, DescEmb still requires the selection of features specific to each EHR system.


\section{Methodology} \label{sec:math}
\subsection{Structure of Electronic Health Records}
% Here, we describe and summarize the EHR structure and notations used throughout this paper.

In typical EHR data, each patient $P$ can be represented as a sequence of medical events $[\mathcal{M}_1,\ldots,\mathcal{M}_N]$, where $N$ is the total number of events throughout the entire patient visit history. 
The i-th medical event of a patient $\mathcal{M}_i$ can be expressed as a set of event-associated features $\{A_i^{1},\ldots,A_i^{|\mathcal{M}_i|}\}$.
Each feature $A_i^{k}$ can be seen as a tuple of a feature name and its value $(n_i^{k}, v_i^{k}), n_i^k \in \mathcal{N}, v_i^k \in \mathcal{V}$, where $\mathcal{N}$ and $\mathcal{V}$ are each
a set of unique feature names 
(\textit{e.g.}, $\{$``drug name'', ``drug dosage'', $\ldots, \}$)
and feature values
(\textit{e.g.}, $\{$``vancomycin'', ``10.0'', $\ldots, \}$), respectively.

In addition, each medical event $\mathcal{M}_i$ has its corresponding event type $e_i \in \mathcal{E}$ which denotes the type of the event
(\textit{e.g.}, $\mathcal{E} = \{$``\textit{lab test}'', ``\textit{prescription}'', $\ldots, \}$).
Lastly, since the recorded time is also provided with $\mathcal{M}_i$, we can measure the time interval $t_i$ between $\mathcal{M}_i$ and $\mathcal{M}_{i+1}$.
\subsection{Universal Healthcare Predictive Framework}
In this section, we present \oursarc, a universal framework for EHR-based prediction, where
% based on the following three principles, and describe how to implement each principle: 
% (1) text-based embedding, (2) employing the entire features of EHR and (3) medical event aggregation.
the overall architecture is depicted by Figure~\ref{fig:fig2}.

\noindent{\textbf{Text-based embedding.}}
A conventional EHR embedding method starts by assigning a unique embedding for each element in $\mathcal{V}$ via a linear map (\textit{i.e.}, lookup table) $f_{\mathcal{V}}$~\citep{choi2016multi, song2018attend, song2019medical, mcdermott2021a, rajkomar2018scalable}, so that $v_i^k$ can be converted to a vector $\vb_i^k \in \mathbb{R}^{d_v}$,
typically followed by pooling multiple feature values ($\vb_i^1, \vb_i^2, \ldots$) to obtain $\mb_i \in \mathbb{R}^{d_m}$, the embedding of $\mathcal{M}_i$
% \footnote{Previous EHR embedding methods do not typically use the feature name $n_i^k$}.
% This conventional embedding, however, usually requires a different $f_{\mathcal{V}}$ for each medical institution due to the $\mathcal{V}$ \textit{heterogeneity}
% For example, MIMIC-III~\citep{johnson2016mimic}, an open-source EHR data, uses the ICD-9 diagnosis codes for recording diagnostic information, while eICU~\citep{pollard2018eicu}, another open-source EHR data, uses in-house diagnosis codes.
% Therefore, the conventional embedding is not the most suitable foundation on which to build a universal EHR framework.

DescEmb~\citep{hur2022unifying} proposed to resolve this issue by suggesting a text-based embedding, where hospital-specific feature values are first converted to textual descriptions (\textit{e.g.}, ``401.9'' $\rightarrow$ ``unspecified essential hypertension'')
% \footnote{generic feature values are left as is, such as ``vancomycin'' and ``10.0''},
then a text encoder paired with a sub-word tokenizer is used to obtain $\mb_i$.
% With this approach, the model can learn the language of the underlying medical text rather than memorizing a unique embedding for each hospital-specific feature value, thereby overcoming the $\mathcal{V}$ \textit{heterogeneity} as the same text encoder can be used for all institutions that use the same language.
We extend the previous approach by applying the text-based embedding philosophy to event types $e_i$ and feature names $n_i^k$, in addition to feature values $v_i^k$, as follows:
\begin{equation*}
    \mb_i = f \Big( S(e_i), S(n_i^1), S(v_i^1), \ldots,
    % S(n_i^{|\mathcal{M}_i|}), S(v_i^{|\mathcal{M}_i|}),
    [t_i] \Big) \label{eq:event_embedding}
\end{equation*}
where $S$ is a sub-word tokenizer, $f$ is an event encoder that takes a sequence of sub-word tokens and returns $\mb_i$, and $[t_i]$ is a special token for time intervals.
% Note that $f$ can be a pre-trained language model as in DescEmb, or a randomly initialized Transformer encoder, or even a single-layer RNN.

\noindent{\textbf{Employing the entire features of EHR.}\quad}
To develop a universal predictive framework,
% in addition to the $\mathcal{V}$ \textit{heterogeneity},
we also must consider the \textit{schema heterogeneity}, namely each medical institution using different database schema.
When developing a conventional predictive model, medical domain experts are typically involved to define $\mathcal{M}'_i \subset \mathcal{M}_i$, a subset of task-specific features among $\mathcal{M}_i$ according to each EHR system.
% This process must be carried out repeatedly whenever they encounter a different EHR schema. 
Moreover, in multi-source learning, medical domain experts must select and match compatible features among distinct EHR systems.
% For instance, in the \textit{Lab} event of eICU, the feature named ``labResult'' should be paired with the ``VALUENUM'' feature in MIMIC-III's \textit{LABEVENTS} event.
% Assessing database schemas of multiple sources and matching compatible features,
% These processes, although inevitable in a conventional approach, are time-consuming and prone to human errors.


To avoid this costly procedure, our framework exploits the entire features of medical events, effectively resolving the schema heterogeneity.
% As described in Eq.~\ref{eq:event_embedding}, the entire features in medical events are embedded into one unified embedding $\mb_i$.
% From this approach, engineers no longer need to be concerned about feature selection since all features are used.
% Additionally, in multi-source learning, our framework is not constrained by the features that are present in each schema since both the name $n_i^k$ and the value $v_i^k$ of the feature are used.
A formal comparison between previous and our approach to obtain $\mb_i$ is provided below:
\begin{flalign}
& \mbox{\textit{Conventional approach}:} \nonumber && \\
& \quad \mb_i = pool (\{ f_{\mathcal{V}} (v_i^k) \mid A_i^k \in \mathcal{M}'_i\} ) \nonumber && \\
& \mbox{\textit{DescEmb}:}  && \nonumber \\
& \quad \mb_i = f \Big( \{ S(v_i^k) \mid A_i^k \in \mathcal{M}'_i \} Big) \nonumber && \\
& \mbox{\textit{\oursarc}:}  && \nonumber \\
& \quad \mb_i = f \Big( S(e_i), \{ S(n_i^k), S(v_i^k) \mid A_i^k \in \mathcal{M}_i \} \Big) \nonumber
\end{flalign}

where \textit{pool} is typically implemented as concatenation or summation of the elements.
Note that we omitted the time interval in all equations to emphasize the fact that \oursarc{} differs from previous approaches in that it is the only approach to exploit all available information in a medical event: event type, all event names and all event values.
Therefore, \oursarc{} provides a general solution applicable to any EHR system with different schema, making it schema-agnostic without requiring medical domain knowledge.

\noindent{\textbf{Medical event aggregation.}}
To utilize the characteristics of EHR, where $P$ consists of a sequence of $\mathcal{M}_i$ and each $\mathcal{M}_i$ consists of a set of $A_i^k$,
we design a hierarchical model consisting of the event encoder $f$, and the event aggregator $g$.

After each $\mathcal{M}_i$ is converted to $\mb_i$ according to Eq.~\ref{eq:event_embedding}, we can obtain $\pb \in \mathbb{R}^{d_p}$, the vector representation of $P$ as follows:
\begin{equation*}
    \pb = g (\mb_1, \mb_2, \ldots, \mb_N)  \label{eq:hierarchical}
\end{equation*}
where $g$ is an embedding function that takes a sequence of event embeddings.
% Note that $g$ can be implemented with any sequence encoder, such as a Transformer encoder or a single-layer RNN.
% Then, feeding $\pb$ through a softmax layer (sigmoid layer if binary prediction) will give us the final prediction $\hat{\yb}$.

Note that it is possible to obtain $\pb$ by employing a flattened model architecture rather than a hierarchical one as follows:
\begin{align}
\pb &= h \Big( S(e_1), \{ S(n_1^k), S(v_1^k) \mid A_1^k \in \mathcal{M}_1 \}, [t_1], \nonumber \\
& \ldots, \nonumber \\
& S(e_N), \{ S(n_N^k), S(v_N^k) \mid A_N^k \in \mathcal{M}_N \}, [t_N] \Big) \nonumber \label{eq:flattened}
\end{align}
where sub-word tokens from all features of all medical events are passed to the sequence model $h$ at the same time.
% This alternative has the advantage of being able to directly associate features from different medical events (\textit{e.g.}, associate $A_i^k$ and $A_j^l$ via self-attention if $h$ is a Transformer), as opposed to the hierarchical architecture where only indirect connection is allowed.
% This, however, comes at a steep price of $h$ having to digest a significantly longer sequence of sub-word tokens, which not only increases computational burden, but also the hypothesis space to explore.
We demonstrate that the hierarchical approach, which reflects the characteristics of EHR data, indeed outperforms the flattened approach.
These results are shown in Appendix~\ref{apd:hifl}.

\section{Experiments} \label{sec:exp}
\subsection{Experimental Settings}

We use baseline models to evaluate the feasibility of \oursarc{} for our objective, namely schema-agnostic EHR embedding without medical domain knowledge.
As there is no previous work, to our knowledge, that tackled exactly the same goal as ours, we modified well-known general-purpose EHR embedding frameworks(SAnD~\citep{song2018attend}, Rajkormar~\citep{rajkomar2018scalable}, DescEmb~\citep{hur2022unifying}).
In addition, all models were provided with both $n_i^k$ and $v_i^k$ for a fair comparison with \oursarc.
For datasets, three open source datasets (MIMIC-III, eICU, MIMIC-IV) were used. 
Datasets and baseline models details are provided in appendix \label{apd:data} and \label{apd:modeldetail}.

% \begin{compactitem}
% % \begin{itemize}[leftmargin=5.5mm, topsep=0pt]
% \item SAnD*: This uses the conventional embedding, selected features $\mathcal{M}'_i$, and the flattened architecture, similar in spirit to SAnD~\citep{song2018attend}.
% Note that feature embeddings from all medical events $[\mathcal{M}_1, \ldots, \mathcal{M}_N]$ are directly fed to the sequence encoder $h$ instead of being pooled to obtain individual $\mb_i$.
% \item Rajkomar*: This uses the conventional embedding, entire features $\mathcal{M}_i$, and the hierarchical approach, similar in spirit to \cite{rajkomar2018scalable} except the CDM standardization.
% Note that feature embeddings from each $\mathcal{M}_i$ are fed to $f$ to obtain individual $\mb_i$, which is fed to $g$.
% \item DescEmb*: This uses the text-based embedding, selected features $\mathcal{M}'_i$, and the hierarchical approach, similar in spirit to DescEmb~\citep{hur2022unifying}.
% % \end{itemize}
% \end{compactitem}
% For a fair comparison, $f$ and $g$ were both implemented with a randomly initialized 2-layer Transformer encoder, and $h$ a 4-layer Transformer encoder, making all models equivalent in terms of number of trainable parameters.
% % ($d_v = 128$, $d_m = 128$, $d_p = 128$).
% Further implementation details including the list of selected features $\mathcal{M}'_i$ \footnote{For example, from the prescription event, we chose essential features such as drug name, drug volume, unit of measurement among all available features.} are described in Appendix \ref{apd:implementation detail}.

To evaluate our framework, we formulated seven prediction tasks: mortality(Mort), length of stay(LOS3, LOS7), readmission (Readm), final acuity (Fi\_ac), imminent discharge(Im\_disch), diagnosis (Dx)) following 
\citet{mcdermott2021a}.
All tasks are evaluated with the area under the precision recall curve (AUPRC).

\begin{figure*}[ht]
    \floatconts
    {fig:fig3}
    {\caption{Comparison of single domain prediction performance. The data source
    % used for training and evaluation
    is represented on each row.
    The y-axis describes AUPRC
    % prediction performance (AUPRC)
    % in terms of area under precision and recall curve (AUPRC),
    and x-axis represents the models.
    The standard errors are provided by the error bars.
    % and models with $^\S$ indicates using only lab test events.
    % Note that we do not report the MIMIC-IV result of Benchmark$^\S$ as it was not covered by \cite{mcdermott2021a}.
    } \vskip -25pt}
    {\includegraphics[width=1.0\linewidth]{figures/vldb_single.pdf}
    \includegraphics[width=0.8\linewidth]{figures/vldb_single_legend.pdf}}
\end{figure*}

\begin{figure*}[ht]
    \floatconts
    {fig:fig4}
     {\caption{Pooled learning results. The data used for evaluation is represented on each row.
     The y-axis describes AUPRC and
    %  area under precision and recall curve (AUPRC),
    x-axis represents the models.
    The blue line separates models into
    conventional embedding models (left- SAnD*, Rajkomar*) and text-based embedding models (right- DescEmb*, UniHPF).
    Dot colors indicate the source datasets used for training.
    The $\star$ mark in each dot indicates p-value<0.05 from the t-test between single domain prediction (yellow dots) and pooled learning (other dots).
    % Note that ``Single'' refers to the same data source as the evaluation dataset.
    % Black arrows indicate performance gain or loss from ``Single'' to ``MIMIC-III+MIMIC-IV+eICU''.
    Black arrows point from ``Single'' to ``MIMIC-III+MIMIC-IV+eICU''.
    } \vskip -25pt}
    {
    \includegraphics[width=1.0\linewidth]{figures/vldb_pooled.pdf}
    \includegraphics[width=0.8\linewidth]{figures/vldb_pooled_legend.pdf}
    \includegraphics[width=0.7 \linewidth]{figures/vldb_pooled_model_legend.pdf}
    }
\end{figure*}

\subsection{Experimental Design}
% For validating \oursarc{} utility in multiple aspects, we designed a set of experiments: (1) single domain prediction, (2) pooled learning, and (3) transfer learning.

\noindent{\textbf{Single domain prediction.}}
In single domain prediction, all models are trained on a single dataset's training set and tested on the same dataset's test set.
% (\textit{e.g.}, trained on eICU's training set, tested on eICU's test set).
% Based on these experiments, we intend to show that \oursarc{} can be utilized for single domain prediction even though it originally aims at multi-source learning.
To provide credibility, we compare \oursarc{} with Benchmark~\citep{mcdermott2021a}.
Since Benchmark suggested an expert-designed feature-engineered prediction pipeline, comparing \oursarc{} with it can verify the effectiveness of our method, which does not involve any domain knowledge.
In this work we use a modified Benchmark$^\S$ to use only the lab test events, and compare it with a modified \oursarc{}$^\S$ that also only uses lab test events.
% This is due to the explosive amount of chart events that occur within a 12-hour window (more than a thousand chart events).
% , which we plan to handle in the future with computation-efficient sequence encoders such as efficient Transformers.

\noindent{\textbf{Multi-source learning.}}
% \noindent{\textbf{Pooled learning.}}
% In order to utilize the abundance of EHR in prediction tasks, it is important to make use of data collected from multiple EHR systems. % which finally can lead to more precise prediction.
% Such pooled learning enables the training of a model with diverse patient information, and finally can lead to more precise prediction.
To show the capability of our framework in multi-source learning, we set up the experiments on pooled learning and transfer learning scenario. 
For pooled leraning, we train the models on the pooled dataset from multiple sources, and evaluate them on each dataset's test set.

% Note that imminent discharge and final acuity tasks are excluded because of incompatible label definitions between MIMIC and eICU.

% \noindent{\textbf{Transfer learning.}\quad}
% In reality, it is more likely that a single deep learning model is trained on a large-scale hospital dataset and then transferred to individual institutions so that small hospitals can benefit from large-scale trained models.
% Such transfer learning provides an opportunity for small hospitals to benefit from large-scale trained models.
For transfer learning scenario, each model is first trained on a source dataset and then directly evaluated (\textit{i.e.,} zero-shot) or further trained (\textit{i.e.,} fine-tune) on a target dataset.
Here, we introduce two extra baselines that can be used to automatically map different code systems: AutoMap~\citep{wu2022automap} and MUSE~\citep{conneau2017word}.

% AutoMap~\citep{wu2022automap} is an automatic medical code mapping method, which aims to solve transfer learning by aligning two different code systems in a self-supervised manner.
% MUSE~\citep{conneau2017word} is an unsupervised bilingual embedding mapping method, which was used as a baseline in \cite{wu2022automap}.
% Owing to the same reason as in pooled learning, imminent discharge and final acuity tasks are excluded in transfer learning.  




\subsection{Single Domain Prediction}
The results of single domain prediction are shown in Figure~\ref{fig:fig3}.
First, we compare \oursarc{}$^\S$ with Benchmark$^\S$ to see how absence of domain knowledge affects prediction performance.
\oursarc{}$^\S$ generally shows higher performance than Benchmark$^\S$ in most prediction tasks.
This implies that it is possible to achieve better AUPRC without significant feature engineering.

Next, we compare all models that use lab tests, prescriptions, and input events.
\oursarc{} shows comparable prediction performance to models using domain knowledge and conventional embedding (SAnD*, Rajikomar*, DescEmb*) except the readmission tasks on MIMIC-III and MIMIC-IV.
% , for which all models fail to show decent AUPRC to begin with.
In particular, a comparison between \oursarc{} and Rajkomar* suggests that it is unnecessary to assign unique embeddings for all feature names and values.
% , and treating them as textual descriptions leads to comparable performance. 
In addition, a comparison between \oursarc{} and DescEmb* demonstrates that applying medical domain knowledge to select a subset of meaningful features does not necessarily lead to greater performance than simply using all features.
% Overall, single domain prediction results show that \oursarc{} achieves comparable performance even without relying on medical domain knowledge, and by simply using all features as textual descriptions.


% \subsection{Pooled Learning}
\subsection{Multi-source Learning}
\label{sec:pooled}

The results of pooled learning are shown in Figure~\ref{fig:fig4}.
For text-based embedding models (DescEmb* and UniHPF), the results when training on the pooled dataset from all the three sources
% (red dots of C and D for each plot in Figure~\ref{fig:fig4})
consistently show higher performances than the single domain predictions 
% (yellow dots of C and D for each plot in Figure~\ref{fig:fig4}).
In contrast, in the case of conventional embedding models (SAnD* and Rajkomar*),
% , the same dots of A and B for each plot in Figure~\ref{fig:fig4}),
the performances generally decrease.
% We speculate this result comes from the fact that MIMICs and eICU have completely different code systems and schemas to each other.
% so a model must learn the clinical semantics of different code systems to fully take advantage of pooled learning.
We speculate that this result comes from the fact that MIMICs and eICU do not share any codes. Training conventional embedding models on this pooled dataset does nothing but expand the number of required embeddings for each feature name and value, which prevents the model from taking advantage of larger training data.
% (\textit{i.e.}, pooled dataset).
% On the contrary, text-based embedding models can utilize the rich volume of data integrated from different sources because the sub-words of the medical descriptions will be shared even among completely distinct EHRs.

In addition, within text-based embedding models, \oursarc{} outperforms DescEmb* in most cases when all three data sources are pooled.
% Also, on the same pooled dataset, \oursarc{} consistently shows improved performances compared to the single domain prediction, whereas the performances of DescEmb* decrease in some cases.
This result implies that \oursarc{} has a better capability to capture underlying semantics of distinct EHR sources than DescEmb*, by utilizing all available information in a medical event.

Next, the results of the transfer learning are presented in appendix Table~\ref{tab: transfer}. 
In the fine-tune and zero shot scenario, the results show that \oursarc{} outperforms the other methods in most cases.
In addition, compared to single domain prediction performance, we can see that \oursarc{} mostly benefits from the pre-trained source dataset.

% \subsection{Transfer Learning}

% 
% Specifically, in the case of zero-shot, we can see that the code-based embedding methods (SAnD*, AutoMap, MUSE, and Rajikomar*) consistently show inferior performances compared to the text-based embedding methods (DescEmb* and \oursarc), excluding the readmission task in MIMIC-III.
% This again shows text-based embedding is more advisable to construct a unified healthcare framework than conventional embedding
% % because of its shared sub-word vocabulary in the textual descriptions of features.
% In addition, \oursarc{} generally exhibits the best performance for prediction tasks across various transfer scenarios.
% This also shows that using all available information (\oursarc) is more helpful for learning the semantics of medical code descriptions rather than selecting and matching specific features (DescEmb*), as also mentioned in Sec.~\ref{sec:pooled}.

% In the fine-tune scenario, the results show the same trends as zero-shot test, where \oursarc{} outperforms the other methods in most cases.
% In addition, compared to single domain prediction performance, we can see that \oursarc{} mostly benefits from the pre-trained source dataset.


% \subsection{Qualitative analysis for features in \oursarc{}}
% We have seen so far that \oursarc{} was able to demonstrate quantitatively superior, or at least comparable predictive performance to all baselines for multiple prediction tasks, three EHR datasets, and three learning scenarios.
% In this section, we provide a qualitative case study to see that \oursarc{} is not only reporting good AUPRC numbers, but it is also learning actually meaningful medical knowledge.

% In order to see which features were significant in predictive tasks, we accumulated the gradient of back-propagation of each event at the event encoder $f$. 
% We followed the feature importance calculation method of DescEmb~\cite{hur2022unifying} on the mortality prediction with MIMIC-III and eICU.
 
% We hypothesize that the larger the gradient, the more impactful the features.
% The gradients for each event were tallied by the main feature of its corresponding event type 
% (\textit{e.g.}, lab test name in the lab test event, or drug name in the prescription event).
% We analyzed the top 100 important features in MIMIC-III and eICU, where the top 15 important features are provided in Table~\ref{tab: topfeatures} in descending order.
% Within the top 100 features, we examined the features shared by both DescEmb* and \oursarc{} to show that \oursarc{} still utilizes meaningful features even without a careful feature selection process.
% As a result, it turns out that both models share 87 and 79 out of the top 100 features in MIMIC-III and eICU, respectively, which means that \oursarc{} can figure out which features are significant for the predictive tasks without explicit guidance from human experts.
% Additional qualitative experiments are also provided in our GitHub repository.


\section{Conclusion} 
In this paper, we proposed a universal healthcare prediction framework, \oursarc{}, which enables multi-source learning by solving EHR heterogeneity of code and schema simultaneously, without medical domain knowledge or pre-processing.
The experimental results showed that \oursarc{} can act as a cornerstone for large-scale model training with multiple EHR sources.

% Our single domain prediction results showed \oursarc{} is able to achieve comparable
% % , if not superior
% performance to all baselines without relying on domain expertise.
% % in pre-processing data or feature selecting.
% We also demonstrate the robustness and efficiency of our framework through pooled learning experiments, without site-specific data harmonization.
% Finally, our results showed that the code-agnostic and schema-agnostic properties aid in improving transfer learning performance in both zero-shot and fine-tune settings.
% Owing to this efficacy, we believe \oursarc{} can act as a cornerstone for large-scale model training with multiple EHR sources.

% \noindent{\textbf{Limitations.}}
% Although it showed promising results, \oursarc{} is not without its limitations.
% We used only the subset of EHR events (lab tests, prescriptions, and input events) even though \oursarc{} is able to utilize any EHR data, due to the computational limits.
% In other words, we could not process some EHR events such as ``\textit{chart event}'' since the length of the event sequence grows extremely long if we use all existing event types.
% % Passing such a long sequence into a model directly is prohibited by computational constraints.
% We expect performance gains if we could exploit all available event types of EHR by replacing with a modern memory-efficient architecture.
% such as performer~\citep{choromanski2020rethinking}, and  S4~\citep{gu2022efficiently}.

% \noindent{\textbf{Future works.}} From the perspective of the large-scale EHR learning, we can consider applying self-supervised pre-training strategies to our model.
% In fact, we examined some well-known pre-training methods,
%  including MLM~\cite{devlin2018bert}, SpanMLM~\cite{joshi2020spanbert}, wav2vec 2.0~\cite{baevski2020wav2vec},
% but found none of them to be effective.
% We leave the development of advanced EHR-specific pre-training methods for \oursarc{} as our future work.


%---------------------------------------------------------------------------$$
\clearpage


\acks{This work was supported by Institute of Information \& Communications Technology Planning \& Evaluation (IITP) grant (No.2019-0-00075), Korea Medical Device Development Fund grant (Project Number: 1711138160, KMDF\_PR\_20200901\_0097), and the Korea Health Industry Development Institute (KHIDI) grant (No.HR21C0198), funded by the Korea government (MSIT, MOTIE, MOHW, MFDS).}
\bibliography{pmlr-sample}


\renewcommand{\arraystretch}{1.3}
\onecolumn

\appendix

\section{A typical EHR predictive model}
\begin{figure}[ht] 
\floatconts
    {fig:fig1}
    {\caption{ 
    % An illustration of the conventional EHR system-specific framework.
    A typical EHR predictive model framework involves domain-knowledge-based pre-processing for each medical center’s schema, which requires schema-specific and code system-specific feature engineering.
    % This results in input features being dependent on each hospital and incompatible with models among different hospitals.
    }}
    {\includegraphics[width=0.7\linewidth]{figures/fig1.pdf}}
\end{figure}
\clearpage
\section{transfer learning}\label{apd:transfer}

\begin{table*}[ht]
\large{
\centering
\caption{AUPRCs of zero-shot test and fine-tune test results on five prediction tasks.
For both zero-shot and fine-tune, the best results are written in boldface for each row.
When fine-tuning, we additionally reported the performance difference with its single domain prediction.
}
\label{tab: transfer}
\adjustbox{width=1.1\textwidth,center}{
\begin{tabular}{ccccccc|ccccccc} \toprule
\multicolumn{7}{c}{\textbf{Zero-shot}} &\multicolumn{6}{c}{\textbf{Fine-tune}} \\
\hline
\multicolumn{13}{c}{MIMIC-III $\xrightarrow[]{}$ eICU} \\
\hline
 & SAnD* &AutoMap &MUSE &Rajkomar* & DescEmb* &\textbf{UniHPF}  &SAnD* &AutoMap &MUSE &Rajkomar* &DescEmb* &\textbf{UniHPF} \\
\hline
Mort &0.048 &0.044 &0.039 &0.035 &\textbf{0.135} &\textbf{0.135} &0.141(-0.024) &0.105(+0.039) &0.138(+0.081) &0.137(-0.035) &0.162(0) &\textbf{0.175(+0.006)} \\
LOS3 &0.452 &0.465 &0.463 &0.458 &0.503 &\textbf{0.507} &0.575(-0.01) &0.541(+0.014) &0.573(+0.052) &0.587(+0.002) &0.578(-0.006) &\textbf{0.589(+0.006)} \\
LOS7 &0.178 &0.186 &0.178 &0.192 &\textbf{0.264} &0.258 &0.259(-0.027) &$0.23(+0.02)^\dagger$ &0.263(+0.052) &$0.278(-0.006)^\dagger$ &0.28(-0.001) &\textbf{0.293(+0.008)} \\
Readm &0.166 &0.165 &0.154 &0.169 &0.320 &\textbf{0.333} &0.399(-0.008) &$0.25(-0.126)^\dagger$ &0.398(+0.038) &0.351(-0.053) &0.28(-0.122) &\textbf{0.415(+0.006)} \\
Dx &0.277 &$0.29^\dagger$ &$0.299^\dagger$ &0.289 &0.629 &\textbf{0.646} &0.672(-0.007) &0.435(-0.157) &0.675(+0.07) &\textbf{0.688(-0.006)} &0.682(-0.005) &\textbf{0.688(-0.001)} \\
\hline
\multicolumn{13}{c}{eICU $\xrightarrow[]{}$ MIMIC-III} \\
\hline
Mort &$0.048^\dagger$ &0.048 &0.048 &0.048 &0.238 &\textbf{0.245} &0.254(-0.009) &0.066(-0.054) &$0.227(+0.115)^\dagger$ &0.32(-0.006) &0.299(+0.008) &\textbf{0.333(+0.006)} \\
LOS3 &0.493 &0.482 &0.5 &0.492 &0.533 &\textbf{0.544} &0.643(-0.019) &0.612(+0.012) &0.652(+0.049) &\textbf{0.664(+0.001)} &0.659(-0.006) &0.663(-0.003) \\
LOS7 &0.219 &$0.213^\dagger$ &$0.204^\dagger$ &0.221 &0.292 &\textbf{0.308} &0.333(-0.032) &$0.29(-0.015)^\dagger$ &0.333(+0.036) &0.35(-0.015) &0.367(+0.001) &\textbf{0.379(+0.013)} \\
Readm &0.061 &0.063 &0.06 &\textbf{0.068} &0.049 &0.055 &0.076(-0.01) &0.065(-0.012) &0.077(-0.004) &\textbf{0.09(-0.003)} &0.066(-0.002) &$0.069(+0.008)^\dagger$ \\
Dx &0.533 &$0.523^\dagger$ &$0.521^\dagger$ &0.536 &0.639 &\textbf{0.647} &0.758(+0.001) &0.648(-0.054) &0.751(+0.049) &0.76(0) &0.754(-0.006) &\textbf{0.765(+0.006)} \\
\hline
\multicolumn{13}{c}{MIMIC-III $\xrightarrow[]{}$ MIMIC-IV} \\
\hline
Mort &0.018 &0.024 &0.024 &0.014 &0.222 &\textbf{0.228} &0.284(-0.003) &$0.151(-0.019)^\dagger$ &0.244(+0.076) &\textbf{0.326(+0.009)} &0.301(+0.009) &0.309(-0.006) \\
LOS3 &0.402 &0.411 &0.404 &0.389 &0.527 &\textbf{0.536} &0.592(-0.012) &$0.511(-0.013)^\dagger$ &0.582(+0.051) &0.614(-0.022) &0.607(+0.001) &\textbf{0.654(+0.006)} \\
LOS7 &0.172 &0.166 &0.164 &$0.171^\dagger$ &0.267 &\textbf{0.279} &0.283(-0.034) &0.233(-0.004) &0.295(+0.047) &0.288(-0.043) &0.319(+0.006) &\textbf{0.326(-0.002)} \\
Readm &0.08 &0.082 &0.082 &0.085 &0.082 &\textbf{0.095} &0.109(-0.014) &0.093(-0.016) &0.114(+0.007) &\textbf{0.131(+0.016)} &0.118(+0.013) &0.121(+0.003) \\
Dx &0.638 &$0.649^\dagger$ &$0.666^\dagger$ &0.624 &\textbf{0.778} &\textbf{0.788} &0.825(-0.007) &0.749(-0.032) &0.832(+0.051) &0.834(-0.002) &0.828(-0.001) &\textbf{0.841(+0.007)} \\
\hline
\multicolumn{13}{c}{MIMIC-IV $\xrightarrow[]{}$ MIMIC-III} \\
\hline
Mort &0.043 &0.037 &0.038 &0.044 &0.139 &\textbf{0.146} &0.284(+0.021) &$0.159(+0.039)^\dagger$ &0.255(+0.143) &\textbf{0.324(-0.002)} &0.302(+0.011) &\textbf{0.324(-0.003)} \\
LOS3 &0.494 &0.495 &0.508 &0.490 &0.572 &\textbf{0.586} &0.651(-0.011) &$0.598(-0.002)^\dagger$ &0.646(+0.043) &0.656(-0.007) &0.656(-0.009) &\textbf{0.66(-0.006)} \\
LOS7 &$0.187^\dagger$ &$0.192^\dagger$ &0.229 &0.196 &\textbf{0.303} &0.280 &$0.36(-0.005)^\dagger$ &$0.288(-0.017)^\dagger$ &0.328(+0.031) &\textbf{0.369(+0.004)} &$0.364(-0.002)^\dagger$ &0.367(+0.001) \\
Readm &0.049 &0.057 &0.059 &0.062 &0.062 &\textbf{0.070} &0.063(-0.023) &0.072(-0.005) &0.079(-0.002) &\textbf{0.095(+0.002)} &0.079(+0.011) &0.062(+0.001) \\
Dx &0.5 &$0.573^\dagger$ &$0.559^\dagger$ &0.495 &0.710 &\textbf{0.711} &\textbf{0.764(+0.007)} &$0.659(-0.043)^\dagger$ &0.754(+0.052) &0.75(-0.01) &$0.75(-0.01)^\dagger$ &0.761(+0.002) \\\midrule
\bottomrule
\end{tabular}}
\begin{flushleft}
    {\footnotesize †: standard deviation $>$ 0.02}
\end{flushleft}
}
\vspace{-7.5mm}
\end{table*}


The results of the transfer learning are presented in Table~\ref{tab: transfer}.
Specifically, in the case of zero-shot, we can see that the code-based embedding methods (SAnD*, AutoMap, MUSE, and Rajikomar*) consistently show inferior performances compared to the text-based embedding methods (DescEmb* and \oursarc), excluding the readmission task in MIMIC-III.
This again shows text-based embedding is more advisable to construct a unified healthcare framework than conventional embedding
% because of its shared sub-word vocabulary in the textual descriptions of features.
In addition, \oursarc{} generally exhibits the best performance for prediction tasks across various transfer scenarios.
This also shows that using all available information (\oursarc) is more helpful for learning the semantics of medical code descriptions rather than selecting and matching specific features (DescEmb*), as also mentioned in Sec.~\ref{sec:pooled}.

% In the fine-tune scenario, the results show the same trends as zero-shot test, where \oursarc{} outperforms the other methods in most cases.
% In addition, compared to single domain prediction performance, we can see that \oursarc{} mostly benefits from the pre-trained source dataset.
\clearpage

\section{Dataset details}\label{apd:data}
We draw on three publicly available datasets; MIMIC-III~\citep{johnson2016mimic}, MIMIC-IV~\citep{johnson2021mimic}, and eICU~\citep{pollard2018eicu}.
% The MIMIC-III database consists of clinical data of over 40,000 patients admitted to intensive care units (ICU) at the Beth Israel Deaconess Medical Center from 2001 to 2012.
% MIMIC-IV is an updated version of MIMIC-III that includes new sources of data, admission date shifting, and extended period of records collected from 2008 to 2019.
% eICU consists of ICU records from multiple US-based hospitals, totaling up to 140,000 unique patients admitted between 2014 and 2015.

All three datasets contain patient medical events including lab tests, prescriptions, and input events (\textit{e.g.}, drug injection), each event marked with timestamps.
MIMIC-III and MIMIC-IV share the same code system with similar schemas, whereas eICU has a completely distinct code system and schema.
% For example, the prescription table in MIMIC-III employs the ICD code system to record patient drug intake and injection information.
% In contrast, eICU does not record drug information based on a structured code system, but rather represents it in a raw text format.

To ensure reliable experiments and analysis, we split the dataset into training, validation and test sets according to 8:1:1 ratio in a stratified manner for each target label.
All experiments were conducted with five random seeds and we report the mean performance.


More information about how datasets were created is provided in this section.
\begin{itemize}
\item The MIMIC-III database consists of clinical data of over 40,000 patients admitted to intensive care units (ICU) at the Beth Israel Deaconess Medical Center from 2001 to 2012.
\item MIMIC-IV is an updated version of MIMIC-III that includes new sources of data, admission date shifting, and extended period of records collected from 2008 to 2019.
\item eICU consists of ICU records from multiple US-based hospitals, totaling up to 140,000 unique patients admitted between 2014 and 2015.
\end{itemize}
For applying \oursarc{} to any EHR datasets, only two pre-processing steps are necessary, which do not involve any domain knowledge.
First, we drop features whose values only consist of integers.
This automatically leads to using all continuous-valued features (\textit{e.g.}, lab test result) and textual features (\textit{e.g.}, lab test name), while features such as patient ID are removed.
Second, we split numeric values digit-by-digit and assign a special token for each digit place, namely \textit{digit place embedding}, which was first introduced in DescEmb~\citep{hur2022unifying}.


\subsection{Table selection}
\input{tables/table_s1.tex}
For each patient, three sources with different “event types” (lab tests, prescription, and infusion) are preprocessed as input for a predictive model. Table ~\ref{tab:filesource} lists csv filenames with each event type.
Note that MIMIC-III files ’INPUTEVENTS MV’ and ’INPUTEVENTS CV’, are merged and named as INPUTEVENTS.
File names for each data sources and tables are described below.

\subsection{Patient cohort setup}
\input{tables/table_s2.tex}
For the sake of comparability, we built patients cohorts from MIMIC-III, MIMIC-IV and eICU databases based on the following criteria: patients over the age of 18 years who remained in the ICU for over 24 hours.
Then, we exclusively consider the first ICU stay during a single hospital stay, and remove any ICU stays with fewer than five medical events.
Within each ICU stay, we restrict our samples to the first 12 hours of data, and remove features that occur fewer than five times in the entire dataset.
Lastly, we eliminate events with lower frequency of main columns (drug name, ITEMID, … ). Cohort summary is described in Table ~\ref{tab:tabstat}

\subsection{Convert EHR table to input sequence}
Here, we will explain our pre-process algorithm which enables us to deal with any EHR table, converting them into the same input configuration for UniHPF. The process explanation is represented below.

\begin{enumerate}[topsep=0pt]
\item First, replace code features to description if the definition table exists in the EHR source set, which the definition table has features as key and description as value. (e.g. MIMIC-III DITEMS.csv)
\item Remove columns whose data type is integer except columns which have categorical values (e.g. number of unique features <50).
\item Select the associated timestamp column which is most relevant to the point of occurrence and drop the other timestamp columns.
\item  Convert all features as string type and tokenize them with “bio-clinical-bert” tokenizer except associated timestamp columns.
\item  For numeric values in feature, split them digit by digit before being tokenized and apply digit-place embedding (DPE) following the value embedding method from DescEmb, which assigns a special token for each digit place.
\item  Descriptions corresponding to each event are listed in the order of event type, feature name, and feature value.
\item  For time stamps, the time interval between the corresponding event and the next event is used as the time feature. 8. At this time, we follow Rajikomar method to deal with continuous values, quantizing them into discretized features. So, the time interval is bucketed into 20 separations within the entire time interval and converted to special tokens.
\item Next, create a class of type token corresponding to the event type, feature name, and feature value.
\item  Lists events in order based on timestamp. Note that these type tokens are used as indicating each sub-word token type (event type, feature name, feature value).
This type token sequence is added to event input with sinusoidal positional embedding.
\item  Finally, prepare an input dataset with a shape as (N, S, W), where N is a number of icu stay, S is maximum length of events, and W is maximum sub-word length for each event.
\end{enumerate}



\subsection{Datasets preparation for each model}


\begin{enumerate}[topsep=0pt]
\item Feature selection
\begin{itemize}[topsep=0pt]
\item We prepare two versions of the dataset, feature selection version and without feature selection version (using Entire EHR).
\item This was to compare the case with and without the conventional feature selection process, and in the case of SAnD and DescEmb, the feature selected dataset is used.
\item Feature selection criteria follows DescEmb, which are using information corresponding to medical code, numerical value, unit of measurement.
\end{itemize}
\item Conventional embedding method
\begin{itemize}[topsep=0pt]
\item Each feature is coded based on unique text.
\item Before converting feature text into unique code, continuous values are buckettized after being grouped by each ITEMID.
\item For categorical features, preprocessing is performed separately on categorical code.
\item Feature names (columns) are also converted as codes.
\end{itemize}
\item Flattened structure
\begin{itemize}[topsep=0pt]
\item  The hierarchical form (N, S, W) of input data is reshaped into the shape of (B, SxW).
\item  After removing the pad in each W, flattened input shape is changed to (N, S*) where S* indicates flattened input without pad.
\item  SAnD* used this flattened dataset as input.
\item  The ablation study results for flatten and hierarchical are below.
\end{itemize}

\end{enumerate}

\section{Prediction task}\label{apd:prediction}
\input{tables/table_s3.tex}
Following \citet{mcdermott2021a}, prediction tasks are well defined.
Medical event information from ICU admission to 12 hours duration is used, and TimeGAP is given 12 hours for all tasks.
The rolling type task (mortality, imminent discharge) is applied only for the first rolling point(similar to static type task), and the prediction window was given at 48hr.
In the case of diagnosis, we tried to group CCS into 18 diagnosis classes based on CCS ontology. MIMIC-III, MIMIC-IV and eICU used “Diagnosis.csv”, “diagnoses icd.csv” and “diagnosis.csv” respectively.
Detailed label definition in the Table ~\ref{tab:labeling}.

\clearpage
\section{Implementation details}\label{apd:implementation detail}

\subsection{Baseline model details}\label{apd:modeldetail}
\begin{compactitem}
% \begin{itemize}[leftmargin=5.5mm, topsep=0pt]
\item SAnD*: This uses the conventional embedding, selected features $\mathcal{M}'_i$, and the flattened architecture, similar in spirit to SAnD~\citep{song2018attend}.
Note that feature embeddings from all medical events $[\mathcal{M}_1, \ldots, \mathcal{M}_N]$ are directly fed to the sequence encoder $h$ instead of being pooled to obtain individual $\mb_i$.
\item Rajkomar*: This uses the conventional embedding, entire features $\mathcal{M}_i$, and the hierarchical approach, similar in spirit to \cite{rajkomar2018scalable} except the CDM standardization.
Note that feature embeddings from each $\mathcal{M}_i$ are fed to $f$ to obtain individual $\mb_i$, which is fed to $g$.
\item DescEmb*: This uses the text-based embedding, selected features $\mathcal{M}'_i$, and the hierarchical approach, similar in spirit to DescEmb~\citep{hur2022unifying}.
% \end{itemize}
\end{compactitem}
For a fair comparison, $f$ and $g$ were both implemented with a randomly initialized 2-layer Transformer encoder, and $h$ a 4-layer Transformer encoder, making all models equivalent in terms of number of trainable parameters.
% ($d_v = 128$, $d_m = 128$, $d_p = 128$).
Further implementation details including the list of selected features $\mathcal{M}'_i$ For example, from the prescription event, we chose essential features such as drug name, drug volume, unit of measurement among all available features.

\subsection{Model architecture for each model}
\input{tables/table_s4.tex}
UniHPF and baseline models can be distinguished in the view of embedding method, feature usage and model structure. The comparison of models is in Table \ref{tab:modelsummary}

\subsection{Hyperparameters}
We searched for the ideal set of hyperparameters for each case for more than 72 hours. We found that the hyperparameters had little impact on the outcome.
We combined one set of hyperparameters for all cases to make the experiment more straightforward without significantly degrading the performance of each individual model.
The final results show a dropout of 0.3, a predictive model's embedding dimension being 128 and a learning rate of 1e-4.
\subsection{Computational resources}
\input{tables/table_s5.tex}
VRAM memory usage was observed when the batch size was 128 based on the LOS3 prediction task, which is a binary classification in single domain training.
In the case of the flattened model SAnD, the input sequence length is 8192, but the VRAM usage is much reduced by using a performer which is efficient transformer ~\citep{choromanski2020rethinking}. Each model VRAM usage information is in Table \ref{tab:memory} 

\subsection{Training details}
We splitted train set, valid set, test set with 9:1:1 ratio and split is stratified for each prediction task.
Training model is saved for best prediction performance at valid testset and early stopping with 10 epoch patience is applied.
For pooled learning, a model with pooled datasets is trained and evaluated for a valid set of each dataset. Test best performance model on each dataset.
For transfer learning, a single domain trained model with source datasets is loaded and used for zero-shot learning or further fine-tuning on target datasets.





\section{Hierarchical vs Flatten Model}\label{apd:hifl}
\input{tables/table_s6.tex}
For giving the same information between hierarchical and flatten models, We restricted the number of events for each sample. Due to the computation resource limitation, flattened models use 8192 as maximum sequence length and corresponding number of events is used on hierarchical model input.
Experiments were conducted to compare the structures of each model in flatten and hierarchical cases. The origin structure of each model is displayed in parenthesis. In most cases, hierarchical performance is higher than flatten structure regardless of model type.
This confirmed that embedding and aggregation of time-series EHR in event units is a more favorable condition for the model.
The result of ablation study is in Talbe \ref{tab:hifl}

\section{Pre-training}\label{apd:pretrain}
\input{tables/table_s7.tex}
The result of applying pretraining to our framework is in Table \ref{tab:pretrain}
For pre-training, fine-tuning is performed after pre-training with the entire input dataset except for the test set. In the context of conventional pre-training and transfer learning, transfer learning from a large hospital to a small hospital can be considered.
However, we only checked whether learning from pre-training gives benefits to the model or not compared to random initialization of model parameters, rather than fine-tuning on partial datasets after pre-training on the entire dataset. The experiment on the transfer situation from a large hospital to a small hospital is left as future work.

In DescEmb with a hierarchical structure, pretraining text within each event with MLM was performed in the event encoder part, but no significant performance improvement was seen. So, we proceeded with pretraining in the structure of flatten where we expect events can be seen by each other, rather than just learning text within the same event.

SPAN MLM is intended to learn the context of the EHR time series event by learning the event itself, rather than simply learning the partial random masked subword of the description.
The MLM accuracy of random masking is more than 90\%, but the accuracy of span MLM is about 80\%, resulting in a more difficult task for the model.
We haven’t seen any performance improvement with pre-training yet. A pre-training method suitable for the characteristics of EHR is needed to be newly developed.

\section{Qualitative analysis}\label{apd:qualitative}
\begin{table*}[ht]
    \caption{\label{tab: topfeatures} \textbf{Top 15 important features in mortality prediction of \oursarc{} trained on the pooled dataset (MIMIC-III+eICU).
    We accumulated the gradients for each event at the event encoder $f$, and ranked them in descending order.}}
    \centering
    \adjustbox{max width=0.7\textwidth}{
    \begin{tabular}{l l}
    \toprule
    MIMIC-III  &  eICU  \\
    \hline
    \midrule
    alendronate sodium po & anf / ana \\
    oxycodone sustained release po & d5w c bicarb \\
    morphine sulfate oral soln. po & pantoprazole protonix \\
    furosemide lasix 500 / 100 & vancomycin in ivpb \\
    acetaminophen - iv & nss w / versed / fent \\
    vancomycin hcl & rocuronium iv \\
    Norepinephrine & cisatracurium \\
    alpha - fetoprotein & oxycodone-acetaminophen 325mg \\
    pentamidine isethionate iv & vitamin d oral \\
    muItivitamin -12 i v & morphine 250 mg sodium chlorid \\
    heparin fIush port 10units/mI & norepinephrine bitartrate \\
    heparin fIush 5000 units/mI & rocuronium ivf infused \\
    ceftazidime & famotidine pepcid iv push \\
    acetaminophen - iv & 3 \% sodium chloride ivf infused \\
    timolol maleate 0. 25  & docusate sodium per ng tube \\
    ranitidine prophylaxis & amiodarone bolus ivpb \\
    \bottomrule
    \end{tabular}
    }
\end{table*}
\input{tables/table_s8.tex}
We have seen so far that \oursarc{} was able to demonstrate quantitatively superior, or at least comparable predictive performance to all baselines for multiple prediction tasks, three EHR datasets, and three learning scenarios.

In this section, we provide a qualitative case study to see that \oursarc{} is not only reporting good AUPRC numbers, but it is also learning actually meaningful medical knowledge. 
In order to see which features were significant in predictive tasks, we accumulated the gradient of back-propagation of each event at the event encoder $f$. We followed the feature importance calculation method of DescEmb~\cite{hur2022unifying} on the mortality prediction with MIMIC-III and eICU.
 
We hypothesize that the larger the gradient, the more impactful the features.
The gradients for each event were tallied by the main feature of its corresponding event type (\textit{e.g.}, lab test name in the lab test event, or drug name in the prescription event).
We analyzed the top 100 important features in MIMIC-III and eICU, where the top 15 important features are provided in Table~\ref{tab: topfeatures} in descending order.
Within the top 100 features, we examined the features shared by both DescEmb* and \oursarc{} to show that \oursarc{} still utilizes meaningful features even without a careful feature selection process.

As a result, it turns out that both models share 87 and 79 out of the top 100 features in MIMIC-III and eICU, respectively, which means that \oursarc{} can figure out which features are significant for the predictive tasks without explicit guidance from human experts. The top 15 important features are described in Table \ref{tab: topfeatures}

Next, to test if UniHPF can handle the discrepancy between MIMIC-III and eICU in terms of textual description, we select four drug terms from the top 15 features  that exist in both datasets, and swap a part of terms between the two datasets, where the selected terms are described in Table~\ref{tab:qual}. For example, we switch all existing drugs vancomycin hcl'' in the test set of MIMIC-III to vancomycin in ivpb''.Then, we evaluate our model that was trained on each single dataset for mortality prediction, using the modified test set of MIMIC-III and eICU, respectively.

As a result, the AUPRC decreased marginally (0.8\%p and 0.6\%p in MIMIC-III and eICU, respectively) although the model never saw the modified features before (e.g., "vancomycin" in ivpb if the model has been trained on MIMIC-III). We conclude that UniHPF is able to deal with distinct EHR datasets, as long as they are based on the same language.
\end{document}
