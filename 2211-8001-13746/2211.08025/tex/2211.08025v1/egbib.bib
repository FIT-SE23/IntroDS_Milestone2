@String(PAMI = {IEEE Trans. Pattern Anal. Mach. Intell.})
@String(IJCV = {Int. J. Comput. Vis.})
@String(CVPR= {IEEE Conf. Comput. Vis. Pattern Recog.})
@String(ICCV= {Int. Conf. Comput. Vis.})
@String(ECCV= {Eur. Conf. Comput. Vis.})
@String(NIPS= {Adv. Neural Inform. Process. Syst.})
@String(ICPR = {Int. Conf. Pattern Recog.})
@String(BMVC= {Brit. Mach. Vis. Conf.})
@String(TOG= {ACM Trans. Graph.})
@String(TIP  = {IEEE Trans. Image Process.})
@String(TVCG  = {IEEE Trans. Vis. Comput. Graph.})
@String(TMM  = {IEEE Trans. Multimedia})
@String(ACMMM= {ACM Int. Conf. Multimedia})
@String(ICME = {Int. Conf. Multimedia and Expo})
@String(ICASSP=	{ICASSP})
@String(ICIP = {IEEE Int. Conf. Image Process.})
@String(ACCV  = {ACCV})
@String(ICLR = {Int. Conf. Learn. Represent.})
@String(IJCAI = {IJCAI})
@String(PR   = {Pattern Recognition})
@String(AAAI = {AAAI})
@String(CVPRW= {IEEE Conf. Comput. Vis. Pattern Recog. Worksh.})
@String(CSVT = {IEEE Trans. Circuit Syst. Video Technol.})

@String(SPL	= {IEEE Sign. Process. Letters})
@String(VR   = {Vis. Res.})
@String(JOV	 = {J. Vis.})
@String(TVC  = {The Vis. Comput.})
@String(JCST  = {J. Comput. Sci. Tech.})
@String(CGF  = {Comput. Graph. Forum})
@String(CVM = {Computational Visual Media})


@String(PAMI  = {IEEE TPAMI})
@String(IJCV  = {IJCV})
@String(CVPR  = {CVPR})
@String(ICCV  = {ICCV})
@String(ECCV  = {ECCV})
@String(NIPS  = {NeurIPS})
@String(ICPR  = {ICPR})
@String(BMVC  =	{BMVC})
@String(TOG   = {ACM TOG})
@String(TIP   = {IEEE TIP})
@String(TVCG  = {IEEE TVCG})
@String(TCSVT = {IEEE TCSVT})
@String(TMM   =	{IEEE TMM})
@String(ACMMM = {ACM MM})
@String(ICME  =	{ICME})
@String(ICASSP=	{ICASSP})
@String(ICIP  = {ICIP})
@String(ACCV  = {ACCV})
@String(ICLR  = {ICLR})
@String(IJCAI = {IJCAI})
@String(PR = {PR})
@String(AAAI = {AAAI})
@String(CVPRW= {CVPRW})
@String(CSVT = {IEEE TCSVT})


@inproceedings{bhojanapalli2021understanding,
  title={Understanding robustness of transformers for image classification},
  author={Bhojanapalli, Srinadh and Chakrabarti, Ayan and Glasner, Daniel and Li, Daliang and Unterthiner, Thomas and Veit, Andreas},
  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},
  pages={10231--10241},
  year={2021}
}
@article{naseer2021intriguing,
  title={Intriguing properties of vision transformers},
  author={Naseer, Muhammad Muzammal and Ranasinghe, Kanchana and Khan, Salman H and Hayat, Munawar and Shahbaz Khan, Fahad and Yang, Ming-Hsuan},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={23296--23308},
  year={2021}
}
@inproceedings{mahmood2021robustness,
  title={On the robustness of vision transformers to adversarial examples},
  author={Mahmood, Kaleel and Mahmood, Rigel and Van Dijk, Marten},
  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},
  pages={7838--7847},
  year={2021}
}
@inproceedings{paul2022vision,
  title={Vision transformers are robust learners},
  author={Paul, Sayak and Chen, Pin-Yu},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  pages={2071--2081},
  year={2022}
}





@misc{Authors14,
 author = {FirstName LastName},
 title = {The frobnicatable foo filter},
 note = {Face and Gesture submission ID 324. Supplied as supplemental material {\tt fg324.pdf}},
 year = 2014
}

@inproceedings{radford2021learning,
  title={{Learning transferable visual models from natural language supervision}},
  author={Radford, Alec and Kim, Jong Wook and Hallacy, Chris and Ramesh, Aditya and Goh, Gabriel and Agarwal, Sandhini and Sastry, Girish and Askell, Amanda and Mishkin, Pamela and Clark, Jack and others},
  booktitle={International Conference on Machine Learning},
  pages={8748--8763},
  year={2021},
  organization={PMLR}
}

@inproceedings{he2016deep,
  title={{Deep residual learning for image recognition}},
  author={He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={770--778},
  year={2016}
}

@inproceedings{dosovitskiy2020image,
  title={An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale},
  author={Dosovitskiy, Alexey and Beyer, Lucas and Kolesnikov, Alexander and Weissenborn, Dirk and Zhai, Xiaohua and Unterthiner, Thomas and Dehghani, Mostafa and Minderer, Matthias and Heigold, Georg and Gelly, Sylvain and others},
  booktitle={Proceedings of International Conference on Learning Representations},
  year={2021}
}

@article{vaswani2017attention,
  title={{Attention is all you need}},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}

@article{oord2018representation,
  title={{Representation learning with contrastive predictive coding}},
  author={Oord, Aaron van den and Li, Yazhe and Vinyals, Oriol},
  journal={arXiv preprint arXiv:1807.03748},
  year={2018}
}

@article{zhou2022learning,
  title={{Learning to prompt for vision-language models}},
  author={Zhou, Kaiyang and Yang, Jingkang and Loy, Chen Change and Liu, Ziwei},
  journal={International Journal of Computer Vision},
  volume={130},
  number={9},
  pages={2337--2348},
  year={2022},
  publisher={Springer}
}

@article{gao2021clip,
  title={{CLIP-Adapter: Better Vision-Language Models with Feature Adapters}},
  author={Gao, Peng and Geng, Shijie and Zhang, Renrui and Ma, Teli and Fang, Rongyao and Zhang, Yongfeng and Li, Hongsheng and Qiao, Yu},
  journal={arXiv preprint arXiv:2110.04544},
  year={2021}
}
@article{pfeiffer2020adapterhub,
  title={{Adapterhub: A framework for adapting transformers}},
  author={Pfeiffer, Jonas and R{\"u}ckl{\'e}, Andreas and Poth, Clifton and Kamath, Aishwarya and Vuli{\'c}, Ivan and Ruder, Sebastian and Cho, Kyunghyun and Gurevych, Iryna},
  journal={arXiv preprint arXiv:2007.07779},
  year={2020}
}
@inproceedings{cai2020tinytl,
  title={{TinyTL: Reduce Memory, Not Parameters for Efficient On-Device Learning}},
  author={Cai, Han and Gan, Chuang and Zhu, Ligeng and Han, Song},
  booktitle={Advances in Neural Information Processing Systems},
  volume={33},
  year={2020}
}
@inproceedings{mcmahan2017communication,
  title={{Communication-efficient learning of deep networks from decentralized data}},
  author={McMahan, Brendan and Moore, Eider and Ramage, Daniel and Hampson, Seth and y Arcas, Blaise Aguera},
  booktitle={Proceedings of Artificial intelligence and statistics},
  pages={1273--1282},
  year={2017},
  organization={PMLR}
}
@article{krizhevsky2009learning,
  title={{Learning multiple layers of features from tiny images}},
  author={Krizhevsky, Alex and Hinton, Geoffrey and others},
  year={2009},
  publisher={Citeseer}
}
@misc{prompt-aligned,
  doi = {10.48550/ARXIV.2205.14865},
  url = {https://arxiv.org/abs/2205.14865},
  author = {Zhu, Beier and Niu, Yulei and Han, Yucheng and Wu, Yue and Zhang, Hanwang},
  title = {{Prompt-aligned Gradient for Prompt Tuning}},
  publisher = {arXiv},
  year = {2022},
}
@article{zhang2020personalized,
  title={{Personalized federated learning with first order model optimization}},
  author={Zhang, Michael and Sapra, Karan and Fidler, Sanja and Yeung, Serena and Alvarez, Jose M},
  journal={arXiv preprint arXiv:2012.08565},
  year={2020}
}
@inproceedings{ma2022layer,
  title={{Layer-wised Model Aggregation for Personalized Federated Learning}},
  author={Ma, Xiaosong and Zhang, Jie and Guo, Song and Xu, Wenchao},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={10092--10101},
  year={2022}
}
@inproceedings{qu2022rethinking,
  title={{Rethinking architecture design for tackling data heterogeneity in federated learning}},
  author={Qu, Liangqiong and Zhou, Yuyin and Liang, Paul Pu and Xia, Yingda and Wang, Feifei and Adeli, Ehsan and Fei-Fei, Li and Rubin, Daniel},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={10061--10071},
  year={2022}
}
@article{bommasani2021opportunities,
  title={{On the opportunities and risks of foundation models}},
  author={Bommasani, Rishi and Hudson, Drew A and Adeli, Ehsan and Altman, Russ and Arora, Simran and von Arx, Sydney and Bernstein, Michael S and Bohg, Jeannette and Bosselut, Antoine and Brunskill, Emma and others},
  journal={arXiv preprint arXiv:2108.07258},
  year={2021}
}
@article{devlin2018bert,
  title={{Bert: Pre-training of deep bidirectional transformers for language understanding}},
  author={Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  journal={arXiv preprint arXiv:1810.04805},
  year={2018}
}
@article{han2021pre,
  title={{Pre-trained models: Past, present and future}},
  author={Han, Xu and Zhang, Zhengyan and Ding, Ning and Gu, Yuxian and Liu, Xiao and Huo, Yuqi and Qiu, Jiezhong and Yao, Yuan and Zhang, Ao and Zhang, Liang and others},
  journal={AI Open},
  volume={2},
  pages={225--250},
  year={2021},
  publisher={Elsevier}
}
@article{brown2020language,
  title={{Language models are few-shot learners}},
  author={Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and others},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={1877--1901},
  year={2020}
}
@article{radford2018improving,
  title={{Improving language understanding by generative pre-training}},
  author={Radford, Alec and Narasimhan, Karthik and Salimans, Tim and Sutskever, Ilya and others},
  year={2018},
  publisher={OpenAI}
}
@article{ramesh2022hierarchical,
  title={{Hierarchical text-conditional image generation with clip latents}},
  author={Ramesh, Aditya and Dhariwal, Prafulla and Nichol, Alex and Chu, Casey and Chen, Mark},
  journal={arXiv preprint arXiv:2204.06125},
  year={2022}
}
@inproceedings{liu2021swin,
  title={{Swin transformer: Hierarchical vision transformer using shifted windows}},
  author={Liu, Ze and Lin, Yutong and Cao, Yue and Hu, Han and Wei, Yixuan and Zhang, Zheng and Lin, Stephen and Guo, Baining},
  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},
  pages={10012--10022},
  year={2021}
}
@inproceedings{he2022masked,
  title={{Masked autoencoders are scalable vision learners}},
  author={He, Kaiming and Chen, Xinlei and Xie, Saining and Li, Yanghao and Doll{\'a}r, Piotr and Girshick, Ross},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={16000--16009},
  year={2022}
}
@inproceedings{houlsby2019parameter,
  title={{Parameter-efficient transfer learning for NLP}},
  author={Houlsby, Neil and Giurgiu, Andrei and Jastrzebski, Stanislaw and Morrone, Bruna and De Laroussilhe, Quentin and Gesmundo, Andrea and Attariyan, Mona and Gelly, Sylvain},
  booktitle={International Conference on Machine Learning},
  pages={2790--2799},
  year={2019},
  organization={PMLR}
}
@article{liu2021pre,
  title={{Pre-train, prompt, and predict: A systematic survey of prompting methods in natural language processing}},
  author={Liu, Pengfei and Yuan, Weizhe and Fu, Jinlan and Jiang, Zhengbao and Hayashi, Hiroaki and Neubig, Graham},
  journal={arXiv preprint arXiv:2107.13586},
  year={2021}
}
@article{li2021prefix,
  title={{Prefix-tuning: Optimizing continuous prompts for generation}},
  author={Li, Xiang Lisa and Liang, Percy},
  journal={arXiv preprint arXiv:2101.00190},
  year={2021}
}
@inproceedings{zhou2022conditional,
  title={{Conditional prompt learning for vision-language models}},
  author={Zhou, Kaiyang and Yang, Jingkang and Loy, Chen Change and Liu, Ziwei},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={16816--16825},
  year={2022}
}
@inproceedings{jia2022vpt,
  title={{Visual Prompt Tuning}},
  author={Jia, Menglin and Tang, Luming and Chen, Bor-Chun and Cardie, Claire and Belongie, Serge and Hariharan, Bharath and Lim, Ser-Nam},
  booktitle={European Conference on Computer Vision (ECCV)},
  year={2022}
}
@article{hu2021lora,
  title={{Lora: Low-rank adaptation of large language models}},
  author={Hu, Edward J and Shen, Yelong and Wallis, Phillip and Allen-Zhu, Zeyuan and Li, Yuanzhi and Wang, Shean and Wang, Lu and Chen, Weizhu},
  journal={arXiv preprint arXiv:2106.09685},
  year={2021}
}
@article{he2021towards,
  title={{Towards a unified view of parameter-efficient transfer learning}},
  author={He, Junxian and Zhou, Chunting and Ma, Xuezhe and Berg-Kirkpatrick, Taylor and Neubig, Graham},
  journal={arXiv preprint arXiv:2110.04366},
  year={2021}
}
@article{tan2022federated,
  title={{Federated Learning from Pre-Trained Models: A Contrastive Learning Approach}},
  author={Tan, Yue and Long, Guodong and Ma, Jie and Liu, Lu and Zhou, Tianyi and Jiang, Jing},
  journal={arXiv preprint arXiv:2209.10083},
  year={2022}
}
@article{guo2022promptfl,
  title={{PromptFL: Let Federated Participants Cooperatively Learn Prompts Instead of Models--Federated Learning in Age of Foundation Model}},
  author={Guo, Tao and Guo, Song and Wang, Junxiao and Xu, Wenchao},
  journal={arXiv preprint arXiv:2208.11625},
  year={2022}
}
@article{chen2022pre,
  title={{On pre-training for federated learning}},
  author={Chen, Hong-You and Tu, Cheng-Hao and Li, Ziwei and Shen, Han-Wei and Chao, Wei-Lun},
  journal={arXiv preprint arXiv:2206.11488},
  year={2022}
}
@article{nguyen2022begin,
  title={{Where to begin? exploring the impact of pre-training and initialization in federated learning}},
  author={Nguyen, John and Malik, Kshitiz and Sanjabi, Maziar and Rabbat, Michael},
  journal={arXiv preprint arXiv:2206.15387},
  year={2022}
}
@article{10.1145/3510033,
author = {Tian, Yuanyishu and Wan, Yao and Lyu, Lingjuan and Yao, Dezhong and Jin, Hai and Sun, Lichao},
title = {{FedBERT: When Federated Learning Meets Pre-Training}},
year = {2022},
issue_date = {August 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {13},
number = {4},
issn = {2157-6904},
doi = {10.1145/3510033},
journal = {ACM Trans. Intell. Syst. Technol.},
month = {aug},
articleno = {66},
numpages = {26},
}
@article{luo2021no,
  title={{No fear of heterogeneity: Classifier calibration for federated learning with non-iid data}},
  author={Luo, Mi and Chen, Fei and Hu, Dapeng and Zhang, Yifan and Liang, Jian and Feng, Jiashi},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={5972--5984},
  year={2021}
}
@inproceedings{gao2022feddc,
  title={{FedDC: Federated Learning with Non-IID Data via Local Drift Decoupling and Correction}},
  author={Gao, Liang and Fu, Huazhu and Li, Li and Chen, Yingwen and Xu, Ming and Xu, Cheng-Zhong},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={10112--10121},
  year={2022}
}
@inproceedings{li2021model,
  title={{Model-contrastive federated learning}},
  author={Li, Qinbin and He, Bingsheng and Song, Dawn},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={10713--10722},
  year={2021}
}
@article{wu2022communication,
  title={{Communication-efficient federated learning via knowledge distillation}},
  author={Wu, Chuhan and Wu, Fangzhao and Lyu, Lingjuan and Huang, Yongfeng and Xie, Xing},
  journal={Nature communications},
  volume={13},
  number={1},
  pages={1--8},
  year={2022},
  publisher={Nature Publishing Group}
}
@inproceedings{wang2022progfed,
  title={{ProgFed: effective, communication, and computation efficient federated learning by progressive training}},
  author={Wang, Hui-Po and Stich, Sebastian and He, Yang and Fritz, Mario},
  booktitle={International Conference on Machine Learning},
  pages={23034--23054},
  year={2022},
  organization={PMLR}
}
@article{khan2021federated,
  title={{Federated learning for internet of things: Recent advances, taxonomy, and open challenges}},
  author={Khan, Latif U and Saad, Walid and Han, Zhu and Hossain, Ekram and Hong, Choong Seon},
  journal={IEEE Communications Surveys \& Tutorials},
  year={2021},
  publisher={IEEE}
}
@article{antunes2022federated,
  title={{Federated Learning for Healthcare: Systematic Review and Architecture Proposal}},
  author={Antunes, Rodolfo Stoffel and Andr{\'e} da Costa, Cristiano and K{\"u}derle, Arne and Yari, Imrana Abdullahi and Eskofier, Bj{\"o}rn},
  journal={ACM Transactions on Intelligent Systems and Technology (TIST)},
  volume={13},
  number={4},
  pages={1--23},
  year={2022},
  publisher={ACM New York, NY}
}
@article{ammad2019federated,
  title={{Federated collaborative filtering for privacy-preserving personalized recommendation system}},
  author={Ammad-Ud-Din, Muhammad and Ivannikova, Elena and Khan, Suleiman A and Oyomno, Were and Fu, Qiang and Tan, Kuan Eeik and Flanagan, Adrian},
  journal={arXiv preprint arXiv:1901.09888},
  year={2019}
}
@article{zhang2021survey,
  title={{A survey on federated learning}},
  author={Zhang, Chen and Xie, Yu and Bai, Hang and Yu, Bin and Li, Weihong and Gao, Yuan},
  journal={Knowledge-Based Systems},
  volume={216},
  pages={106775},
  year={2021},
  publisher={Elsevier}
}
@article{sun2022paradigm,
  title={{Paradigm shift in natural language processing}},
  author={Sun, Tian-Xiang and Liu, Xiang-Yang and Qiu, Xi-Peng and Huang, Xuan-Jing},
  journal={Machine Intelligence Research},
  volume={19},
  number={3},
  pages={169--183},
  year={2022},
  publisher={Springer}
}
@article{yuan2021florence,
  title={{Florence: A new foundation model for computer vision}},
  author={Yuan, Lu and Chen, Dongdong and Chen, Yi-Ling and Codella, Noel and Dai, Xiyang and Gao, Jianfeng and Hu, Houdong and Huang, Xuedong and Li, Boxin and Li, Chunyuan and others},
  journal={arXiv preprint arXiv:2111.11432},
  year={2021}
}
@inproceedings{zhu2021data,
  title={{Data-free knowledge distillation for heterogeneous federated learning}},
  author={Zhu, Zhuangdi and Hong, Junyuan and Zhou, Jiayu},
  booktitle={International Conference on Machine Learning},
  pages={12878--12889},
  year={2021},
  organization={PMLR}
}
@article{fallah2020personalized,
  title={{Personalized federated learning with theoretical guarantees: A model-agnostic meta-learning approach}},
  author={Fallah, Alireza and Mokhtari, Aryan and Ozdaglar, Asuman},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={3557--3568},
  year={2020}
}
@article{liu2022late,
  title={{Late Prompt Tuning: A Late Prompt Could Be Better Than Many Prompts}},
  author={Liu, Xiangyang and Sun, Tianxiang and Huang, Xuanjing and Qiu, Xipeng},
  journal={arXiv preprint arXiv:2210.11292},
  year={2022}
}
@article{fang2022data,
  title={{Data Determines Distributional Robustness in Contrastive Language Image Pre-training (CLIP)}},
  author={Fang, Alex and Ilharco, Gabriel and Wortsman, Mitchell and Wan, Yuhao and Shankar, Vaishaal and Dave, Achal and Schmidt, Ludwig},
  journal={arXiv preprint arXiv:2205.01397},
  year={2022}
}
@inproceedings{wortsman2022robust,
  title={{Robust fine-tuning of zero-shot models}},
  author={Wortsman, Mitchell and Ilharco, Gabriel and Kim, Jong Wook and Li, Mike and Kornblith, Simon and Roelofs, Rebecca and Lopes, Raphael Gontijo and Hajishirzi, Hannaneh and Farhadi, Ali and Namkoong, Hongseok and others},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={7959--7971},
  year={2022}
}
@inproceedings{wortsman2022model,
  title={{Model soups: averaging weights of multiple fine-tuned models improves accuracy without increasing inference time}},
  author={Wortsman, Mitchell and Ilharco, Gabriel and Gadre, Samir Ya and Roelofs, Rebecca and Gontijo-Lopes, Raphael and Morcos, Ari S and Namkoong, Hongseok and Farhadi, Ali and Carmon, Yair and Kornblith, Simon and others},
  booktitle={International Conference on Machine Learning},
  pages={23965--23998},
  year={2022},
  organization={PMLR}
}