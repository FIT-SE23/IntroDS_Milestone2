\subsection{An efficient algorithm via a convex objective}
\label{sec_algo}
%

We first describe our strategy to estimate the parameters followed and then that for the expected potential outcomes in \cref{eq_causal_estimand}. We remark that for exponential families considered here, maximum likelihood is not computationally tractable \citep{wainwright2008graphical,ShahSW2021B}.
%
%
%
%
%
%
%
%
%

%
%

%
%
%
%
%
%
%
%
%
%
%
%



\paragraph{Parameter estimation via a convex objective}
\label{subsec_loss_function}

Our algorithm jointly learns all the parameters of interest by pooling the observations across all $n$ units and exploiting the exponential family structure of $\rvbv$, $\rvba$, and $\rvby$ conditioned on $\rvbz = \svbz$ in \cref{eq_conditional_distribution_vay}.
In particular, we construct a convex loss that explicitly utilizes the fact that the population-level parameter $\TrueParameterMatrix$ is shared across units: For any $t \in [p]$, the conditional distribution $f_{\rvx_t | \rvbx_{-t}, \rvbz}$ of the random variable $\rvx_t$ conditioned on $\rvbx_{-t} = \svbx_{-t}$ and $\rvbz = \svbz$ is given by
%
%
%
%
%
%
\begin{align}
    f_{\rvx_t | \rvbx_{-t}, \rvbz}\bigparenth{x_t| \svbx_{-t}, \svbz; \ExternalFieldt(\svbz), \ParameterRowt} \propto \exp\biggparenth{ \bigbrackets{\ExternalFieldt(\svbz) + 2\ParameterRowttt\tp \svbx_{-t}} x_t + \ParameterTU[tt] \Bigparenth{x_t^2 - \frac{\xmax^2}{3}}},
    \label{eq_conditional_dist}
\end{align}
where $\ExternalFieldt(\svbz)$ is the $t^{th}$ element of $\ExternalField(\svbz)$, $\ParameterRowt$ is the $t^{th}$ row of $\ParameterMatrix$, $\ParameterTU[tt]$ is the $t^{th}$ element of $\ParameterRowt$, and $\ParameterRowttt \defn \ParameterRowt \setminus \ParameterTU[tt] \in \Reals^{p-1}$ is the vector obtained after deleting $\ParameterTU[tt]$ from $\ParameterRowt$.\footnote{The term $\ParameterTU[tt] \xmax^2/3$ inside the exponent in \cref{eq_conditional_dist} is vacuous (as it is a constant) but has a crucial role in our loss function. This term arises by integrating $\rvx_t^2$ with respect to the uniform distribution on $\cX$. This centering of $\rvx_t^2$ ensures that our loss function is a proper loss function (see \cref{prop_proper_loss_function} and \cref{sec_proof_proper_loss_function}). Further, this centering also leads to connections with the surrogate likelihood~\citep[Proposition. 4.1]{ShahSW2021A}.}
%
%
%
%
%
%
%
\begin{definition}[\tbf{Loss function}]\label{def-loss-function}
%
Given the samples $\sbraces{\svbx^{(i)}}_{i \in [n]}$, the loss $\loss:\Reals^{p \times (n+p)} \to \Reals$ is given by
%
%
%
%
%
%
%
%
%
%
%
%
\begin{align}
    \loss\bigparenth{\ExtendedParameterMatrix} \!=\! \frac{1}{n}\sump[t] \sumn[i] \! \exp\biggparenth{\!-\!\bigbrackets{\ExternalFieldtI \!+\! 2\ParameterRowttt\tp \svbx_{-t}^{(i)}} x_t^{(i)}\!-\!\ParameterTU[tt] \Bigparenth{[x_t^{(i)}]^2 - \frac{\xmax^2}{3}}}
    \!\!\qtext{where}
    \ExtendedParameterMatrix \!\defeq\!\! \begin{bmatrix} \ExtendedParameterRowT[1]\tp \\ \vdots \\ \ExtendedParameterRowT[p]\tp \end{bmatrix},
    %
    %
    %
    \label{eq:loss_function}
    \end{align}
    and $\ExtendedParameterRowT[t] \!\defn\! \bigbraces{\ExternalFieldtI[1], \cdots, \ExternalFieldtI[n], \ParameterRowt}$ for $t\in[p]$.
    %
    %
%
%
\end{definition}
\noindent Our estimate of $\ExtendedTrueParameterMatrix$ (defined analogous to $\ExtendedParameterMatrix$) is given by
%
%
%
\begin{align}
    \ExtendedEstimatedParameterMatrix \in \argmin_{\ExtendedParameterMatrix \in \ParameterSet_{\ExternalField}^n \times \ParameterSet_{\ParameterMatrix}} \loss\bigparenth{\ExtendedParameterMatrix}.
    \label{eq_estimated_parameters}
\end{align}
%
%
We note \cref{eq_estimated_parameters} is a convex optimization problem, and a projected gradient descent algorithm (see below) returns an $\epsilon$-optimal estimate with $\tau = O(p/\epsilon)$ iterations\footnote{This follows from \cite[Theorem. 3.7]{bubeck2015convex} by noting that $\loss(\ExtendedParameterMatrix)$ is $O(p)$ smooth function of $\ExtendedParameterMatrix$.} where $\ExtendedEstimatedParameterMatrix_{\epsilon}$ is said to be an $\epsilon$-optimal estimate if $\loss\bigparenth{\ExtendedEstimatedParameterMatrix_{\epsilon}} \leq \loss\bigparenth{\ExtendedEstimatedParameterMatrix} + \epsilon$ for any $\epsilon > 0$. The loss function $\loss$ is inspired by the loss functions used in \cite{vuffray2016interaction,VuffrayML2022,ShahSW2021A}\footnote{Our loss function can be viewed as a generalization of the loss used in these prior works. In particular, if the unobserved confounding is identical across units, i.e., $\TrueExternalFieldI[1] = \cdots = \TrueExternalFieldI[n]$, then $\loss\bigparenth{\ExtendedParameterMatrix}$ in \cref{eq:loss_function} can be simplified to the loss function used in these works.}
and admits a notable property (see \cref{sec_proof_proper_loss_function} for the proof).
%
%
%
%
%
%
%
%
%
\newcommand{\properlossfunction}{Proper loss function}
\begin{proposition}[\tbf{\properlossfunction}]\label{prop_proper_loss_function} The loss function $\loss$ is strictly proper, i.e., $\ExtendedTrueParameterMatrix = \argmin_{\ExtendedParameterMatrix \in \ParameterSet_{\ExternalField}^n \times \ParameterSet_{\ParameterMatrix}} \Expectation\bigbrackets{\loss\bigparenth{\ExtendedParameterMatrix}}$.
%
%
%
%
%
%
\end{proposition}
\cref{prop_proper_loss_function} shows that the solution of the idealized (infinite sample limit of \cref{eq_estimated_parameters}) convex program $\min_{\ExtendedParameterMatrix \in \ParameterSet_{\ExternalField}^n \times \ParameterSet_{\ParameterMatrix}} \Expectation\bigbrackets{\loss\bigparenth{\ExtendedParameterMatrix}}$ is unique and equal to $\ExtendedTrueParameterMatrix$. Thus the convex program~\cref{eq_estimated_parameters} can be seen as a finite sample version of this idealized program, thereby providing an intuitive justification of our loss function (instead of a maximum likelihood objective, which is not  tractable here). As we show later in our proofs (see \cref{sec_proof_sketch} for an overview), different partial averages on the RHS of \cref{eq:loss_function} also admit useful properties and are critical to our analyses.
%
%

\begin{algorithm}[h]
    \SetCustomAlgoRuledWidth{0.4\textwidth} 
    %
    \KwInput{number of iterations $\tau$, step size $\eta$, $\epsilon$, parameter sets $\ParameterSet_{\ExternalField}$ and $ \ParameterSet_{\ParameterMatrix}$}
    \KwOutput{$\epsilon$-optimal estimate $\ExtendedEstimatedParameterMatrix_{\epsilon}$}
    \KwInitialization{$\ExtendedParameterMatrix^{(0)} = \boldsymbol{0}$} 
    %
    {
    \For{$j = 0,\cdots,\tau$}
    {
        $\ExtendedParameterMatrix^{(j+1)} \leftarrow \argmin_{\ExtendedParameterMatrix \in \ParameterSet_{\ExternalField}^n \times \ParameterSet_{\ParameterMatrix}} \stwonorm{\ExtendedParameterMatrix^{(j)} - \eta  \nabla \loss\bigparenth{\ExtendedParameterMatrix^{(j)}} - \ExtendedParameterMatrix}$
    }
    $\ExtendedEstimatedParameterMatrix_{\epsilon} \leftarrow \ExtendedParameterMatrix^{(\tau+1)}$
    }
    \caption{Projected Gradient Descent}
    \label{alg:GradientDescent}
\end{algorithm}
%


\paragraph{Causal estimate}
\label{subsec_causal_estimate}
Given the estimate $\ExtendedEstimatedParameterMatrix$, our estimate of the expected potential outcome $\mu^{(i)}(\wtil{\svba}^{(i)})$ under an alternate intervention $\wtil{\svba}^{(i)} \in \cA^{p_a}$~\cref{eq_causal_estimand} is derived as follows:
%
First, we identify $\EstimatedPhi^{(u, y)} \in \Reals^{p_u \times p_y}$ to be the component of $\EstimatedParameterMatrix$ corresponding to $\rvbu$ and $\rvby$ for all $\rvbu \in \{\rvbv, \rvba, \rvby\}$ and $\EstimatedExternalFieldI[i,y] \in \Reals^{p_y}$ to be the component of $\EstimatedExternalFieldI$ corresponding to $\rvby$. Next, we estimate the conditional distribution of $\rvby$ for unit $i$ as a function of the interventions $\rvba$, while keeping $\rvbv=\svbv^{(i)}$ and $\rvbz=\svbz^{(i)}$ fixed as 
\begin{align}
    \what{f}^{(i)}_{\rvby | \rvba}(\svby | \svba) \propto \exp\Bigparenth{\bigbrackets{\EstimatedExternalFieldI[i,y] + 2\svbv^{(i)\top}\EstimatedPhi^{(v,y)} + 2\svba\tp\EstimatedPhi^{(a,y)}} \svby + \svby\tp\EstimatedPhi^{(y,y)} \svby}. \label{eq_counterfactual_distribution_y}
\end{align}
Finally, we estimate $\mu^{(i)}(\wtil{\svba}^{(i)})$ as the mean under the above conditional distribution, given by
\begin{align}
    %
    \what{\mu}^{(i)}(\wtil{\svba}^{(i)}) & \defn \Expectation_{\what{f}^{(i)}_{\rvby | \rvba}}[\rvby | \rvba = \wtil{\svba}^{(i)}],  
    \label{eq_causal_estimate}
\end{align}
which can be computed by standard algorithms for estimating marginals of graphical models, e.g., via the junction tree algorithm~\citep{wainwright2008graphical} or message-passing algorithms.\footnote{In general, estimating the marginals exactly is computationally hard for undirected graphical models. While the junction tree algorithm works well for graphical models with small treewidth~\citep[Section. 2.5]{wainwright2008graphical}, e.g., for trees or chains as in hidden Markov models or state-space models, message-passing algorithms are the default choice for computing approximate marginals for complex graphs, especially with cycles. However, message-passing algorithms may induce additional approximations, which we do not discuss here.} 
%

%
%

%

%

%

%
%
%




%
%

%

%
