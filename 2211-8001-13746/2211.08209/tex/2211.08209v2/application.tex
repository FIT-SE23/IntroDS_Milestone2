%
\section{Application: Imputing missing covariates}
\label{sec_sparse_measurement_errors}
\newcommand{\impute}{Impute missing covariates}
%
Consider a setting with no systematically unobserved covariates $\rvbz$; instead, elements of $(\rvbv, \rvba, \rvby)$ are missing or have measurement error for some fraction of the units. Our goal is to impute these missing values or denoise the measurement error in the observed values. 

%
%

%

\paragraph{Problem setup} For the ease of exposition, we assume the observed covariates $\rvbv$ can have measurement error\footnote{Our analysis remains the same when observed covariates $\rvbv$ are missing instead of having measurement error.} but the interventions and the outcomes do not have any measurement error. More concretely, for every unit $i \in [n]$, along with the interventions $\svba^{(i)}$ and the outcomes $\svby^{(i)}$, we observe $\csvbv^{(i)} = \svbv^{(i)} + \Delta \svbv^{(i)}$ instead of true covariates $\svbv^{(i)}$ where $\Delta \svbv^{(i)}$ denotes (unobserved) bounded measurement error. We assume that a certain number of units (known to us) have no measurement error: say, $\Delta \svbv^{(i)} = 0$ for all $i \in \normalbraces{n/2+1, \cdots, n}$. 
%


%
%

%


\paragraph{Questions of interest} Besides counterfactual estimates, our goal is to estimate $\Delta \svbv^{(i)}$ for units with measurement error.

%
%

\subsection{A theoretical guarantee}
\label{subsec_theo_gua} 
Our methodology can be applied to estimate these measurement errors when the joint distribution of the true covariates $\rvbv \in \cX^{p_v}$, the interventions $\rvba \in \cX^{p_a}$, and the observed outcomes $\rvby \in \cX^{p_y}$ can be modeled as an exponential family, parameterized by a vector $\phi \in \Reals^p$ and a symmetric matrix $\Phi \in \Reals^{p \times p}$ where $p \defn p_v + p_a + p_y$, i.e., with $\rvbw \defn (\rvbv, \rvba, \rvby)$
\begin{align}
    f_{{\ranvarvec}}({\varvec}; \phi, \Phi) \propto \exp\Bigparenth{ \phi\tp {\varvec}
    +{\varvec}\tp \Phi {\varvec}},
    \qtext{where}
    {\varvec} \defeq (\svbv, \svba, \svby),
    \label{eq_joint_distribution_vay}
\end{align}
and $\svbv \defn (v_{1}, \cdots, v_{p_v})$, $\svba \defn (a_{1}, \cdots, a_{p_a})$, and $\svby \defn (y_{1}, \cdots, y_{p_y})$ denote realizations of $\rvbv$, $\rvba$, and $\rvby$, respectively. To estimate the counterfactual distribution, we decompose $\rvbv$ into $\crvbv$ and $\Delta \rvbv$, and obtain the distribution of the observed quantities $\rvbx \defn (\crvbv, \rvba, \rvby)$ conditioned on $\Delta \rvbv = \Delta \svbv$ as follows (see \cref{proof_impute_missing_covariates} for details)
\begin{align}
    f_{\rvbx|\Delta \rvbv}\bigparenth{\svbx| \Delta \svbv; \ExternalField(\Delta \svbv), \!\ParameterMatrix} \!\propto\! \exp \Bigparenth{\! \normalbrackets{\ExternalField(\Delta \svbv)}\!\tp \svbx \!+\! \svbx\!\tp \ParameterMatrix \svbx\!} \stext{where}
    \ExternalField(\Delta \svbv) \! \defn \begin{bmatrix}  \phi^{(v)} \!-\! 2 \Phi^{(v,v)\!\top} \Delta \svbv  \\  \phi^{(a)} \!-\! 2 \Phi^{(v,a)\!\top} \Delta \svbv  \\ \phi^{(y)} \!-\! 2 \Phi^{(v,y)\!\top} \Delta \svbv \end{bmatrix}\!, \label{eq_x_given_v_application}
\end{align} 
%
$\svbx \defn (\csvbv, \svba, \svby)$, $\Theta \defn \Phi$, and $\csvbv$, $\svba$, and $\svby$ denote realizations of $\crvbv$, $\rvba$, and $\rvby$, respectively. As in \cref{subsec_exp_fam}, to estimate the counterfactual distribution, it suffices to learn $\ExternalField(\Delta \svbv)\in \Reals^{p\times 1}$ and $\ParameterMatrix \in\Reals^{p\times p}$. 



Let $f_{{\ranvarvec}}(\cdot; \phi^{\star}, \Phi^{\star})$ denote the true data generating distribution of ${\rvbw}$ in \cref{eq_joint_distribution_vay} and let $f_{\rvbx|\Delta \rvbv}\bigparenth{\cdot| \Delta \svbv; \TrueExternalField(\Delta \svbv), \TrueParameterMatrix}$ denote the true distribution of $\rvbx$ conditioned on $\Delta \rvbv = \Delta \svbv$. We assume $(a)$ $\max\braces{\infnorm{\Delta \svbv}, \infnorm{\phi^{\star}}, \maxmatnorm{\Phi^{\star}}} \leq \aGM$ and $(b)$ $\zeronorm{\Phi_t^{\star}} \leq \bGM$ for all $t \in [p]$ analogous to \cref{assumptions}. Then, given realizations $\normalbraces{ \svbx^{(i)}}_{i=1}^{n}$ consistent with $f_{\rvbx|\Delta \rvbv}\bigparenth{\cdot| \Delta \svbv^{(i)}; \TrueExternalField(\Delta \svbv^{(i)}), \TrueParameterMatrix}$, first, we estimate the parameters $\phi^{\star}$ and $\Phi^{\star} = \TrueParameterMatrix$ using the realizations for units $\normalbraces{n/2 + 1, \cdots, n}$. Next, we exploit the structure in the problem to show that $\TrueExternalFieldI \defn \TrueExternalField(\Delta \svbv^{(i)})$ can be written as a linear combination of known vectors 
%
with some error, for every unit $i \in \normalbraces{1, \cdots, n/2}$. Then, we use \cref{eq_estimated_parameters} to estimate $\normalbraces{\TrueExternalFieldI}_{i = 1}^{n}$ and obtain estimates of $\normalbraces{\Delta \svbv^{(i)}}_{i = 1}^{n}$ as by-products.\footnote{For $i \in \normalbraces{n/2 + 1, \cdots, n}$, estimating $\TrueExternalFieldI$ and $\Delta \svbv^{(i)}$ is straightforward since $\TrueExternalFieldI = \phi^{\star}$ and $\Delta \svbv^{(i)} = 0$.} In particular, the estimate of the coefficients associated with the aforementioned linear combination for $\TrueExternalFieldI$ turn out to be our estimate of the measurement error $\Delta \svbv^{(i)}$ for every $i \in \normalbraces{1, \cdots, n/2}$. We provide our guarantee on estimating $\TrueParameterMatrix$, $\TrueExternalFieldI$ for $i \in [n]$, and $\Delta \svbv^{(i)}$ for $i \in [n]$ below with a proof in \cref{proof_impute_missing_covariates}.

%



%
%

%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%

%
%
%
%
%
%
%
%
%
%
%
%
%


\begin{proposition}[{\impute}]
\label{prop_impute_missing_covariates}
Suppose the eigenvalues of $\tbf{B}\tp\tbf{B}$ are lower bounded by $\kappa {p}$ for some $\kappa >0 $ where $\tbf{B}  \!\defn\! \begin{bmatrix} \phi^{\star}, -2 \Phi_1^{\star}, \cdots, -2 \Phi_{p_v}^{\star} \end{bmatrix}\!\in \! \Reals^{p\times (p_v+1)}$. Then, for any fixed $\varepsilon_1 > 0$ and $\delta \in (0,1)$, there exists estimates $\EstimatedParameterMatrix$ and  $\bigbraces{\EstimatedExternalFieldI}_{i=1}^{n}$ such that, 
%
with probability at least $1-\delta$,
\begin{align}
\matnorm{\EstimatedParameterMatrix - \TrueParameterMatrix}_{2,\infty}  \leq & \varepsilon_1  \qquad \qquad \qquad \qquad \qquad \hspace{0.2cm} \qquad  \qtext{when} n  \geq \frac{ce^{c'\bGM} p \log\frac{p}{\delta}}{\varepsilon_1^4},\label{mse_measurement_error_Theta}\intertext{and}
\max_{i\in[n]}
    \mathrm{MSE}(\EstimatedExternalFieldI, \! \TrueExternalFieldI)
    \leq & \max\Bigbraces{\varepsilon_1^2, \frac{ce^{c'\bGM} \bigparenth{p_v \!+\! \log \normalparenth{\log \frac{np}{\delta}}}}{p}} \stext{when}
    n \geq \frac{ce^{c'\bGM}p \bigparenth{\log \frac{np}{\delta} \!+\! p_v}}{\varepsilon_1^4}\label{mse_measurement_mse}.
\end{align}
Further, for any fixed $\varepsilon_2 > 0$, if $\varepsilon_2 \leq \frac{1}{8} \sqrt{\frac{p}{p_v+1}}$, there exist estimates $\bigbraces{\what{\Delta \svbv}^{(i)}}_{i=1}^{n}$ such that,
\begin{align}
    \max_{i\in[n]}
    \stwonorm{\what{\Delta \svbv}^{(i)}- \Delta \svbv^{(i)} }^2 \leq\max\Bigbraces{\frac{c_1 \varepsilon_2^2 \kappa}{p_v+1} , \dfrac{ce^{c'\bGM} \bigparenth{p_v \!+\! \log \normalparenth{\log \frac{np}{\delta}}}}{p \kappa}} \!+\! \varepsilon_2^2 \kappa,
    \label{mse_measurement_error}
\end{align}
with probability at least $1-\delta$, whenever $n \geq ce^{c'\bGM} \kappa^{-4} \varepsilon_2^{-4} (p_v\!+\!1)^2 p {\log \frac{np}{\delta}}$.
%
%
\end{proposition}
%

The above guarantees can be simplified as follows by treating $\bGM$ and $\kappa$ as constants as well as ignoring the constants and the logarithmic factors (denoted by $\precsim$ and $\succsim$): for any $\varepsilon_1 > 0$ and $\frac{1}{8} \sqrt{\frac{p}{p_v+1}} \geq \varepsilon_2 > 0$
\begin{align}
\matnorm{\EstimatedParameterMatrix \!-\! \TrueParameterMatrix}_{2,\infty} & \leq  \varepsilon_1   \qquad  \qquad  \qquad  \qquad \qtext{when} n  \succsim \frac{p}{\varepsilon_1^4}, \label{mse_measurement_error_Theta_simplified}\\
\max_{i\in[n]} \mathrm{MSE}(\EstimatedExternalFieldI, \TrueExternalFieldI) & \precsim  \max\Bigbraces{\varepsilon_1^2, \dfrac{p_v}{p}}  \qquad  \hspace{0.4cm} \qtext{when} n \succsim \frac{p_v p}{\varepsilon_1^4}, \label{mse_measurement_mse_simplified}
\intertext{and}
\max_{i\in[n]} \stwonorm{\what{\Delta \svbv}^{(i)}- \Delta \svbv^{(i)} }^2 & \precsim \max\Bigbraces{\frac{\varepsilon_2^2}{p_v}, \dfrac{p_v}{p}} + \varepsilon_2^2 \hspace{3mm} \qtext{when}  n \succsim \frac{p_v^2 p}{\varepsilon_2^4}. \label{mse_measurement_error_simplified}
\end{align}
For large $n$, whenever, $\max\bigbraces{\varepsilon_1^2, \frac{p_v}{p}}  = \frac{p_v}{p}$ and $\max\bigbraces{\frac{\varepsilon_2^2}{p_v}, \frac{p_v}{p}} = \frac{p_v}{p}$, the guarantees in \cref{mse_measurement_mse_simplified,mse_measurement_error_simplified} can be written as
\begin{align}
\max_{i\in[n]} \mathrm{MSE}(\EstimatedExternalFieldI, \TrueExternalFieldI) & \precsim  \dfrac{p_v}{p} \qquad  \qtext{when} n \succsim \frac{p^3}{p_v},\label{measurement_mse_simple}
\intertext{and}
\max_{i\in[n]} \stwonorm{\what{\Delta \svbv}^{(i)}- \Delta \svbv^{(i)} }^2 & \precsim \dfrac{p_v^2}{p} \qquad \qtext{when} n \succsim \frac{p^3}{p_v^2}. \label{measurement_error_simple}
\end{align}
%
%
%
%

\paragraph{Remark} The measurement errors can be recovered well as long as enough units with no measurement error are observed (i.e., $n/2$ is large) and the observation per unit is high dimensional (i.e., $p$ is large compared to $p_v^2$). We note that the quadratic dependence (on $p_v$) in \cref{measurement_error_simple} arises because of the error in expressing $\TrueExternalFieldI$ as a linear combination of known vectors. In contrast, we get a linear dependence (on $k$) in \cref{cor_params}\cref{item:lc_dense} where there is no error in expressing $\TrueExternalFieldI$ as a linear combination of known vectors (via \cref{exam:lc_dense}).
%



%
%
%
%
%
%


%


\subsection{Simulations} 
We now present some simulation results to empirically evaluate the error scaling of our parameter estimates with three key aspects of the application above: number of units $n$, dimension $p$, and dimension $p_v$ of covariates with measurement error.
%


\paragraph{Data generation}
We choose $\cX = [-1,1]$ and $p_a = p_y = (p-p_v)/2$. The true joint  distribution~\cref{eq_joint_distribution_vay} of $\rvbw \defn (\rvbv, \rvba, \rvby)$ is set as a truncated Gaussian distribution with the parameters $\phi^{\star} = \mathbf{1} \in \Reals^{p}$ and  
 a positive definite $\Phi^{\star} \in \Reals^{p \times p}$ generated using  \textit{sklearn} package \citep{pedregosa2011scikit} 
 such that $\aGM = 6$, $\bGM = 4$, and 
 %
 $\kappa = 0.15$.
 %
 We draw $n$ i.i.d. samples  $\normalbraces{\svbw^{(i)}}_{i = 1}^{n}$ from this true distrbution using \textit{tmvtnorm} package \citep{wilhelm2010tmvtnorm}.
 %
Next, we generate $\Delta \svbv^{(i)}$ uniformly from $[0.9, 1]^{p_v}$ for units $i \in \normalbraces{1, \cdots, n/2}$ while setting $\Delta \svbv^{(i)} =\mathbf{0}$ for other units.
%
Combining $\normalbraces{\svbw^{(i)}}_{i = 1}^{n}$ and $\normalbraces{\Delta \svbv^{(i)}}_{i = 1}^{n}$ yields $\normalbraces{\svbx^{(i)}}_{i = 1}^{n}$ (see \cref{eq_x_given_v_application}).

 \begin{figure}[!t]
    \centering
    %
    %
    %
    %
    %
    %
    %
    %
    %
    %
    %
    %
    %
    %
    %
    %
    %
    %
    %
    %
    \begin{tabular}{c}
        \includegraphics[width=1\textwidth]{Theta_matrix_vs_n.pdf}\\
        (a)\\ 
        \includegraphics[width=1\textwidth]{theta_vector_vs_n.pdf}\\
        (b)\\
        \includegraphics[width=1\textwidth]{z_vs_n.pdf}\\
        (c)
    \end{tabular}
    %
    %
    %
    \caption{
    Error scaling with number of units $n$, for various $p$ and $p_v$, for our estimates of $\TrueParameterMatrix$ (top row), $\sbraces{\TrueExternalFieldI}_{i=1}^{n}$ (middle row), and $\sbraces{\Delta \svbv^{(i)}}_{i=1}^{n}$ (bottom row).}
    \label{fig:vs_p_n}
\end{figure}

\paragraph{Plot details} 
In \cref{fig:vs_p_n}, we plot the scaling of errors in our estimates for $\TrueParameterMatrix$ in the top row, $\sbraces{\TrueExternalFieldI}_{i=1}^{n}$ in the middle row, and $\sbraces{\Delta \svbv^{(i)}}_{i=1}^{n}$ in the bottom row. In particular, we present how the error scales as the dimension $n$ grows for various $p$ and $p_v$. We plot the averaged error across 50 independent trials along with $\pm 1$ standard error (the standard error is too small to be visible in our results). 
%
%

%
To help see the error scaling, we provide the least squares fit on the log-log scale (log error vs log x-axis). We display the best linear fit and mention an empirical decay rate in the legend based on the slope of that fit, e.g., for a slope of $-0.56$ for estimating $\TrueParameterMatrix$ when $p = 16$ and $p_v = 4$, we report an empirical rate of $n^{-0.56}$ for the averaged error. In the middle row and the bottom row of \cref{fig:vs_p_n}, the rates vary from $n^{0.00}$ to $n^{-0.17}$, and we omit these weak dependencies in the legend to reduce clutter.

\paragraph{Error scaling for $\EstimatedParameterMatrix$}
%
%
%
%
From the first row of \cref{fig:vs_p_n}, we observe that the error $\matnorm{\EstimatedParameterMatrix \!-\! \TrueParameterMatrix}_{2,\infty}$ admits a scaling of  between $n^{-0.56}$ and $n^{-0.42}$ for various $p$ and $p_v$. These empirical rates indicate a parametric error rate of $n^{-0.5}$ for $\matnorm{\EstimatedParameterMatrix \!-\! \TrueParameterMatrix}_{2,\infty}$, which suggests that the scaling with $\varepsilon$ in \cref{mse_measurement_error_Theta_simplified} can be likely improved from $\varepsilon^{-4}$ to $\varepsilon^{-2}$. Further, as expected, the error $\matnorm{\EstimatedParameterMatrix \!-\! \TrueParameterMatrix}_{2,\infty}$ does not depend on $p_v$ but increases with an increase in $p$. 

%
%
%
%

\paragraph{Error scaling for $\EstimatedExternalFieldI$}
In the middle row of \cref{fig:vs_p_n}, we see the error $\max_{i\in[n]}\mathrm{MSE}(\EstimatedExternalFieldI\!, \TrueExternalFieldI)$ has a weak dependence on $n$ for a fixed $p$ and $p_v$, decreases with an increase in $p$ for any fixed $n$ and $p_v$, and increases with an increase in $p_v$ for any fixed $n$ and $p$.  
%
This is consistent with \cref{mse_measurement_mse_simplified} when $\max\bigbraces{\varepsilon_1^2, \frac{p_v}{p}} \!=\! \frac{p_v}{p}$ (see \cref{measurement_mse_simple}). Further, we note that the decay of the error with $p$ is slower for smaller $n$ (cf. $n = 2^{11}$ vs $n = 2^{14}$). This is expected from \cref{mse_measurement_mse_simplified} where the $n$  required to ensure $\max\bigbraces{\varepsilon_1^2, \frac{p_v}{p}} \!=\! \frac{p_v}{p}$ increases with an increase in $p$. As a result, for larger $p$, $\varepsilon_1^2$ comes into the picture explaining the increased dependence of the error on $n$ (cf. $p = 16$ vs $p = 128$).  


%

%




%
%
%

 %
 
 %

%
 %

\paragraph{Error scaling for $\what{\Delta \svbv}^{(i)}$ \!}
The trends in the error $\max_{i\in[n]}\!\stwonorm{\what{\Delta \svbv}^{\!(i)}\!-\! \Delta \svbv^{(i)}}^2$ are similar to the error $\max_{i\in[n]}\mathrm{MSE}(\EstimatedExternalFieldI, \TrueExternalFieldI)$. In the bottom row of \cref{fig:vs_p_n}, we see $\max_{i\in[n]}\!\stwonorm{\what{\Delta \svbv}^{\!(i)}\!-\! \Delta \svbv^{(i)} }^2$ has a weak dependence on $n$ for a fixed $p$ and $p_v$, decreases with an increase in $p$ for any fixed $n$ and $p_v$, and increases with an increase in $p_v$ for any fixed $n$ and $p$. This is consistent with \cref{mse_measurement_error_simplified} when $\max\bigbraces{\frac{\varepsilon_2^2}{p_v}, \frac{p_v}{p}} \!=\! \frac{p_v}{p}$ (see \cref{measurement_error_simple}). For the same reason mentioned in the previous paragraph, we see a slower decay in the error with $p$  for smaller $n$ (cf. $n = 2^{11}$ vs $n = 2^{14}$), and a higher dependence of the error on $n$ for larger $p$ (cf. $p = 16$ vs $p = 128$). 


 
%
 
 %

 %

%

%

%
%