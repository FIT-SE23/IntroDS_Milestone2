\section{Main results}
\label{sec_main_results}
In this section, we analyze our estimates. First, 
we provide our guarantee on estimating the unit-level and the population-level parameters in \cref{sub:parameter_result}. 
Next, we provide our guarantee on estimating the causal estimand of interest in \cref{subsec_guarantee_outcome_estimate}.  
%
%
Before stating our main results, we define a standard notion of complexity of the set $\ParameterSet_{\ExternalField}$, namely metric entropy (defined below) that our guarantees rely on.

\begin{definition}[$\varepsilon$-covering number and metric entropy]\label{def_covering_number_metric_entropy}
Given a set $\cV \subset \Reals^p$ and a scalar $\varepsilon > 0$, we use $\cC(\cV, \varepsilon)$ to denote the $\varepsilon$-covering number of $\cV$ with respect to $\sonenorm{\cdot}$, i.e., $\cC(\cV, \varepsilon)$ denotes the minimum cardinality over all possible subsets $\cU \subset \cV$ that satisfy $\cV \subset \cup_{u \in \cU} \ball(u; \varepsilon)$,
%
%
%
where $\ball(u; \varepsilon) \defn \braces{v \in \Reals^p: \sonenorm{u-v} \leq \varepsilon}$.
%
%
We let $\metric_{\ExternalField}(\varepsilon) \defn \log \cC(\ParameterSet_{\ExternalField},\varepsilon)$ denote the metric entropy of $\ParameterSet_{\ExternalField}$, and $\metric_{\ExternalField,n}(\varepsilon) \defn n \metric_{\ExternalField}(n \varepsilon)$ denote a scaled version of it.
\end{definition}

%
%
%

Next, we state two settings with upper bounds on the metric entropy, and we use them as running examples to unpack our general results throughout this paper.
\begin{example}[{Linear combination}]
    \label{exam:lc_dense}
    Consider a set $\ParameterSet_{\ExternalField}$ containing vectors with bounded entries that are also a linear combination of $k$ known vectors in $\real^p$ collected as $\mbf B \in \real^{p\times k}$, i.e., $\ParameterSet_{\ExternalField}=\sbraces{\mbf B \mbf a: \mbf a \in \real^k, \sinfnorm{\mbf B \mbf a} \leq \alpha }$. Then, \citet[Lemma. 11]{DaganDDA2021} implies that $\metric_{\ExternalField}(\radius) = O\bigparenth{k\log \bigparenth{1+\frac{\alpha}{\radius}}}$. Further, $\metric_{\ExternalField,n}(\radius) = O\bigparenth{\frac{\alpha k}{\radius}}$. 
    %
    %
    %
    %
\end{example}

\begin{example}[{Sparse linear combination}]
    \label{exam:sc}
    Consider a set $\ParameterSet_{\ExternalField}$ containing vectors with bounded entries that are also a $s$-\emph{sparse} linear combination of $k$ known vectors in $\real^p$ collected as $\mbf B \in \real^{p\times k}$, i.e., $\ParameterSet_{\ExternalField}=\sbraces{\mbf B \mbf a: \mbf a \in \real^k, \norm{a}_0 \leq s, \sinfnorm{\mbf B \mbf a} \leq \alpha }$. Then \citet[Corollary. 4]{DaganDDA2021} implies that $\metric_{\ExternalField}(\radius) = O\bigparenth{s\log k \log \bigparenth{1+\frac{\alpha}{\radius}}}$. Further, $\metric_{\ExternalField,n}(\radius) = O\bigparenth{\frac{\alpha s \log k}{\radius}}$. 
    %
\end{example}

%
%
%
%
%

\newcommand{\edgeparammainresultname}{Recovering population-level parameter}
\newcommand{\parammainresultname}{Guarantee on quality of parameter estimate}
\newcommand{\nodeparammainresultname}{Recovering unit-level parameters}
%
\subsection{\parammainresultname}
\label{sub:parameter_result}
Our non-asymptotic guarantees use an assumption of a lower bound on the smallest eigenvalue of a suitable set of autocorrelation matrices.

%
\begin{assumption}\label{ass_pos_eigenvalue}
For any $\svbz \in \cZ^{p_z}$ and $t \in [p]$, let $\lambda_{\min}(\svbz, t)$ denote the smallest eigenvalue of the matrix
$\Expectation_{\rvbx | \rvbz} \bigbrackets{  \trvbx ~ \trvbx\tp | \rvbz = \svbz }$ where $\trvbx \defeq \bigparenth{ \rvx_t, 2\rvbx_{-t} \rvx_t, \rvx_t^2 -\xmax^2/3} \in \real^{p+1}$. We assume $\lambda_{\min} \defn \min_{\svbz \in \cZ^{p_z}, t\in [p]} \lambda_{\min}(\svbz, t)$ is strictly positive.
\end{assumption}
\noindent We note that all eigenvalues of any autocorrelation matrix are non-negative implying $\lambda_{\min}(\svbz, t) \geq 0$ for all $\svbz \in \cZ^{p_z}, t\in [p]$.  \cref{ass_pos_eigenvalue} requires $\lambda_{\min}(\svbz, t) > 0$ for all $\svbz \in \cZ^{p_z}, t\in [p]$ and serves as a sufficient condition to rule out certain singular distributions~\citep[Section. 5]{ShahSW2021B}.\footnote{Essentially, we use this assumption to lower bound the variance of a non-constant random variable (\cref{proof_of_lemma_parameter}).} 
%
In \cref{subsec_discussion_ass_pos_eigenvalue}, we show that $\lambda_{\min} = \Omega(e^{-c\bGM})$ when $\TrueParameterTU[tt] = 0$ for all $t \in [p]$ as in Ising model where $\rvx_t^2 = 1 $ for all $ t \in [p]$.

We are now ready to state our main result that characterizes a high probability bound on the estimation error for the estimate $\ExtendedEstimatedParameterMatrix$ computed via \cref{eq_estimated_parameters}. To simplify the presentation, we use $c$ and $c'$ to denote universal constants or constants that depend on the parameters $\aGM,\xmax,$ and $\lambda_{\min}$ and can take a different value in each appearance. 
\begin{theorem}[{\parammainresultname}]
\label{theorem_parameters}
Suppose \cref{assumptions,ass_pos_eigenvalue} hold. Fix an $\varepsilon > 0$ and $\delta \in (0,1)$, and define
%
%
%
%
%
%
%
%
%
\begin{align}
R(\varepsilon, \delta) & \!\defn\! \max \sbraces{ ce^{c'\bGM}\!\! \sqrt{\log(\log p/\delta) \!+\! \metric_{\ExternalField}( ce^{-c'\bGM}) }, \varepsilon \ratio} \stext{with} \ratio \!\defn\! \max_{\ExternalField, \bExternalField \in \ParameterSet_{\ExternalField}} \!\!\frac{\sonenorm{\ExternalField \!-\! \bExternalField}}{\stwonorm{\ExternalField \!-\! \bExternalField}} \label{eq_radius_node_thm}
\intertext{and}
\tmetric_{\ExternalField,n}(\varepsilon, \delta)  & \!\defn\! \metric_{\ExternalField,n}(\varepsilon^2) \!+\! p \metric_{\ExternalField}\bigparenth{R^2\normalparenth{\varepsilon, \delta}}. \label{eq_tmetric_node_thm}
\end{align}
 Then, with probability at least $1-\delta$, the 
%
estimates $\EstimatedParameterMatrix,  \EstimatedExternalFieldI[1], \cdots, \EstimatedExternalFieldI[n]$ defined in \cref{eq_estimated_parameters}
%
%
satisfy
%
%
%
%
\begin{align}
    \matnorm{\EstimatedParameterMatrix \!-\! \TrueParameterMatrix}_{2,\infty} &\leq \varepsilon
    %
    \qquad \quad \ \, \qtext{when}
    %
    %
    n \geq \frac{ce^{c'\bGM} p^2 \Bigparenth{p\log \frac{p}{\delta \varepsilon^2} + \metric_{\ExternalField,n}(\varepsilon^2)}}{\varepsilon^4} \label{eq:matrix_guarantee}
    \intertext{and}
%
%
%
%
%
%
%
%
    %
    \max_{i\in[n]}\stwonorm{\EstimatedExternalFieldI - \TrueExternalFieldI} &  \leq  R\Bigparenth{ \varepsilon, \frac{\delta}{n}} 
    \qtext{when} n  \geq  \frac{ce^{c'\bGM} p^2 \Bigparenth{p \log \frac{np}{\delta \varepsilon^2} + \tmetric_{\ExternalField,n}\bigparenth{ \varepsilon, \frac{\delta}{n}}}}{\varepsilon^4}.   
    %
    \label{eq:theta_guarantee}
    %
    %
%
%
%
%
\end{align}
%
%
\end{theorem}
%
%
\newcommand{\metricentropyone}{Linear combination of known vectors}

%
%
\noindent We split the proof into two parts: First, we establish the bound \cref{eq:matrix_guarantee} in \cref{sec:proof_of_theorem_parameters}, which we then use to establish the bound \cref{eq:theta_guarantee} in \cref{sec_proof_thm_node_parameters_recovery}. 
%
%
%

 %
%
Our guarantee in \cref{eq:matrix_guarantee} provides a non-asymptotic error bound of order $\frac{p^2 (p\log p + \metric_{\ExternalField,n}(n^{-1/2}))}{n^{1/4}}$  (where we treat $\bGM$ as a constant) for estimating $\TrueParameterMatrix$ although the $n$ samples have different unit-level parameters $\sbraces{\TrueExternalFieldI}_{i=1}^{n}$. On the other hand, after squaring both sides and dividing by $p$, the guarantee~\cref{eq:theta_guarantee} for the unit-level parameters can be simplified as follows:\footnote{We replace $\delta/n$ in \cref{eq:theta_guarantee} by $\delta$ as we do not require a union bound over $i \in [n]$ for unit-wise guarantees.} whenever $n \geq c'\varepsilon^{-4} p^2\normalparenth{p\log \frac{p}{\delta \varepsilon^2}  + \metric_{\ExternalField,n}(\varepsilon^2) + p\metric_{\ExternalField}(c)}$, we have
\begin{align}
\label{eq:simplified_bound_theta}
    \mathrm{MSE}(\EstimatedExternalFieldI, \TrueExternalFieldI)
    \! \leq \! \max\Bigbraces{\varepsilon^2, \dfrac{\metric_{\ExternalField}(c) \!+\! \log (\log \frac{p}{\delta})}{p}}, 
    %
\end{align}
%
where we use $\ratio \leq \sqrt p$ in \cref{eq_radius_node_thm} and treat $\bGM$ as a constant. For large $n$ so that $\varepsilon$ is small, this error scales linearly with the metric entropy $\metric_{\ExternalField}$---the error becomes worse as the unit-level parameter set $\ParameterSet_{\ExternalField}$ becomes more complex. 

 The next corollary (stated without proof)  provides a formal version of the population-level guarantee in \cref{eq:matrix_guarantee} and the unit-level guarantee in \cref{eq:simplified_bound_theta} for the two examples discussed earlier. We treat $\bGM$ as a constant and note that the dependence is exponential as in \cref{theorem_parameters}. 

 \begin{corollary}[{Consequences for examples}]\label{cor_params}
Suppose \cref{assumptions,ass_pos_eigenvalue} hold. Then, for any fixed $\varepsilon > 0$ and $\delta \in (0,1)$, the following results hold  with probability at least $1-\delta$.
%
\begin{enumerate}[leftmargin=*,label=(\alph*)]
\item\label{item:lc_dense} \emph{{Linear combination:}}\
If $\ParameterSet_{\ExternalField}$ is as in \cref{exam:lc_dense}, then for all $i \in [n]$,
%
\begin{align}
\matnorm{\EstimatedParameterMatrix \!-\! \TrueParameterMatrix}_{2,\infty} & \!\leq\! \varepsilon
    \qquad \quad \hspace{0.85cm} \qquad \quad \qquad \quad \qtext{for}
    n \!\geq \! \frac{c p^2 \bigparenth{p \log \frac{p}{\delta \varepsilon^2} \!+\! \frac{k}{\varepsilon^2}}}{\varepsilon^4}\\
    \hspace{-1cm} \mathrm{MSE}(\EstimatedExternalFieldI\!, \TrueExternalFieldI)
    & \!\leq\! \max\Bigbraces{\varepsilon^2, \dfrac{c \bigparenth{ k \!+\! \log (\log \frac{p}{\delta}) }}{p} \!}  \qtext{for}
    n \!\geq\! \frac{c p^2 \bigparenth{p \bigparenth{\log \frac{p}{\delta \varepsilon^2} \!+\! k} \!+\! \frac{k}{\varepsilon^2}}}{\varepsilon^4}.
\end{align}
%
\item\label{item:sc}  \emph{{Sparse linear combination:}}\ 
If $\ParameterSet_{\ExternalField}$ is as in \cref{exam:sc}, then for all $i \in [n]$,
%
\begin{align}
\hspace{-0.5cm} \matnorm{\EstimatedParameterMatrix \!-\! \TrueParameterMatrix}_{2,\infty} & \!\leq\! \varepsilon
    \qquad \quad \hspace{1.0cm} \quad \qquad \quad \qquad \quad \qtext{for}
    n \!\geq\! \frac{c p^2 \bigparenth{p \log \frac{p}{\delta \varepsilon^2} \!+\! \frac{s \log k}{\varepsilon^2}}}{\varepsilon^4}\\
    \hspace{-0.5cm} \mathrm{MSE}(\EstimatedExternalFieldI \!\!, \TrueExternalFieldI)
    & \!\leq\! \max\!\Bigbraces{ \varepsilon^2\!, \!\dfrac{c \bigparenth{\!s \log k \!+\! \log (\log \frac{p}{\delta}) }}{p}\!}  \qtext{for}
    n \!\geq\! \! \frac{c p^2 \bigparenth{p \bigparenth{\! \log \frac{p}{\delta \varepsilon^2}  \!+\! s\log k} \!+\! \frac{s \log k}{\varepsilon^2}\!}}{\varepsilon^4}.
\end{align}
\end{enumerate}
\end{corollary}
\noindent \cref{cor_params} states that, as long as $n$ is polynomially large in $p$, our strategy learns the unit-level parameters (on average in terms of mean square error across coordinates) for each user if $p$ is large compared to either the number of vectors $k$ (\cref{exam:lc_dense}) or the sparsity parameter $s$ (\cref{exam:sc}).

%
%
%



%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%


%
\paragraph{Sharpness of guarantees and generalization of prior results}
%
%
%
The exponential dependence on $\bGM$ in \cref{theorem_parameters} is unavoidable given the lower bounds for learning exponential families even with i.i.d. samples~\citep{SanthanamW2012}. The error scaling of $1/\varepsilon^4$ has also appeared in prior works with the suitable analog of our loss function~\citep[Section.~5]{ShahSW2021B} and sharpening it, e.g., to a parametric rate $1/\varepsilon^2$, or characterizing a lower bound remains an interesting future direction.
In the special case of equal unit-level parameters ($\TrueExternalFieldI[1] = \cdots =  \TrueExternalFieldI[n]$), the analysis in 
\cref{sec:proof_of_theorem_parameters} to establish the bound \cref{eq:matrix_guarantee} can be modified to recover (up to constants) prior guarantee \citep[Lemma.~9.1]{ShahSW2021A} on learning exponential family from $n$ i.i.d. samples. 
Further, the guarantee~\cref{eq:theta_guarantee} recovers the prior guarantee~\citep[Theorem.~6]{KandirosDDGD2021} as a special case where the authors consider learning an Ising model from one sample when the population-level parameter is known up to a scaling factor. 
%
%



%


%
%
%
%
%
\newcommand{\outcomemainresultname}{Guarantee on quality of outcome estimate}
\subsection{\outcomemainresultname}
\label{subsec_guarantee_outcome_estimate}
Our non-asymptotic guarantee on outcome estimate assumes that the following matrices are suitably stable under small perturbation in the parameters: (i) the covariance matrix of $\rvby$ conditioned on $\rvba$, $\rvbz$, and $\rvbv$ and (ii) the cross-covariance matrix of $\rvby$ and $\rvy_t \rvby$ conditioned on $\rvba$, $\rvbz$, and $\rvbv$ for all $t \in [p_y]$.
%
%
%
%
%
%
\begin{assumption}\label{ass_bounded_op_norm_cov_matrices}
%
For any set $\mbb B$ containing $\ExternalField, \ParameterMatrix$, there exists a constant $C(\mbb B)$ such that
\begin{align}
  \sup\limits_{\ExternalField, \ParameterMatrix \in \mbb B} \max\Bigbraces{\opnorm{\Covariance_{\ExternalField,\ParameterMatrix}(\rvby, \rvby | {\svba}, \svbz, \svbv)}, \max\limits_{t \in [p_y]} \opnorm{\Covariance_{\ExternalField,\ParameterMatrix}(\rvby, \rvy_t \rvby | {\svba}, \svbz,\svbv)}} \leq C(\mbb B), \label{eq_cov_constraint}
\end{align}
almost surely. The expectation in \cref{eq_cov_constraint} is with respect to the distribution of $\rvby$ conditioned on $\rvba = \svba$, $\rvbz = \svbz$, and $\rvbv = \svbv$ which is fully parameterized by $\ExternalField$ and $\ParameterMatrix$, and can be obtained from \cref{eq_conditional_distribution_vay} after replacing $\ExternalField(\svbz)$ by $\ExternalField$.
\end{assumption}
%
\noindent In \cref{subsec_bounded_op_norms}, we show that $C(\mbb B)$ is a constant for a class of distributions. We note that this assumption is common in the literature on learning Gaussian graphical models to rule out singular distributions \citep{won2006maximum,zhou2011high,ma2016joint}.
 
%
%
%
%

%

We are now ready to state our guarantee for the estimate $\what{\mu}^{(i)}(\wtil{\svba}^{(i)})$ (see \cref{eq_causal_estimate}) of the expected potential outcomes for any unit $i \in [n]$ under an alternate intervention $\wtil{\svba}^{(i)} \in \cA^{p_a}$. We assume $p_v = p_a = p_y$ for brevity. See the proof in \cref{sec_proof_causal_estimand} where we also state a more general result. 

\begin{theorem}[{\outcomemainresultname}]
\label{thm_causal_estimand}
Suppose \cref{assumptions,ass_pos_eigenvalue,ass_bounded_op_norm_cov_matrices} hold. Then for any fixed $\varepsilon > 0$ and $\delta \in (0,1)$, 
%
 the estimates $\sbraces{\what{\mu}^{(i)}(\wtil{\svba}^{(i)})}_{i=1}^n$ defined in \cref{eq_causal_estimate} for any $\sbraces{\wtil{\svba}^{(i)} \in \cA^{p_a}}_{i=1}^n$ satisfy
\begin{align}
\label{eq:mu_error}
    %
    \max_{i\in[n]} \! \frac{\stwonorm{\mu^{(i)}(\wtil{\svba}^{(i)}\!) \!-\! \what{\mu}^{(i)}(\wtil{\svba}^{(i)}\!)}}{C(\mbb B_i)} 
    \!\!\leq\!  R\Bigparenth{\varepsilon, \frac{\delta}{n}} \!\!+\! p \varepsilon ~\stext{for}~ n \!\geq\!\! \frac{ce^{c'\bGM} p^2 \!\bigparenth{p \log \frac{np}{\delta \varepsilon^2} \!+\! \tmetric_{\ExternalField,n}\normalparenth{ \varepsilon, \frac{\delta}{n}}\!}}{\varepsilon^4}\!,
\end{align}
 with probability at least $1-\delta$,
where $R(\varepsilon, \delta)$ was defined in \cref{eq_radius_node_thm}, $\tmetric_{\ExternalField,n}(\varepsilon, \delta) $ was defined in \cref{eq_tmetric_node_thm}, $C(\mbb B)$ was defined in \cref{eq_cov_constraint}, and
\begin{align}
\mbb B_i \defeq \bigbraces{\ExternalField \in \Lambda_{\theta}: \stwonorm{\ExternalField \!-\! \TrueExternalFieldI} \leq R\Bigparenth{\varepsilon, \frac{\delta}{n}}} \times \bigbraces{\ParameterMatrix \in \Lambda_{\Theta}: \max_{t\in[p]}\stwonorm{\ParameterRowt \!-\! \TrueParameterRowt} \leq \varepsilon}.
\end{align}
%
%
%
\end{theorem}
%
%
Repeating the algebra as in \cref{eq:simplified_bound_theta}
%
and treating $C(\mbb B_i)$ as a constant, the bound~\cref{eq:mu_error} yields the following simplified bound for the MSE of our mean outcome estimate $\mu^{(i)}(\wtil{\svba}^{(i)})$ for unit $i \in [n]$ under treatment $\wtil{\svba}^{(i)} \in \cA^{p_a}$: whenever $n \geq c'\varepsilon^{-4} p^2\normalparenth{p\log \frac{p}{\delta \varepsilon^2}  + \metric_{\ExternalField,n}(\varepsilon^2) + p\metric_{\ExternalField}(c)}$, we have
\begin{align}
    \mathrm{MSE}(\mu^{(i)}(\wtil{\svba}^{(i)}), \what{\mu}^{(i)}(\wtil{\svba}^{(i)}))
    \!\leq\! \varepsilon^2 \!+\! \dfrac{\metric_{\ExternalField}(c) \!+\! \log (\log \frac{p}{\delta})}{p}. 
    %
\end{align}
This bound is of the same order as in \cref{eq:simplified_bound_theta} and can be formalized for the two examples (\cref{exam:lc_dense,exam:sc}) by deriving a suitable analog of \cref{cor_params}. In a nutshell, in both settings, the unit-level expected potential outcomes can be estimated well when the total number of units $n$ is large and the observations for each unit are high dimensional compared to the number of vectors $k$ in \cref{exam:lc_dense} or the sparsity parameter $s$ in \cref{exam:sc}.  We omit a formal statement for brevity. 

%
Finally, we also note that as in \cref{theorem_parameters}, the exponential dependence on $\bGM$ is expected to be unavoidable due to the principle of conjugate duality \citep{wainwright2008graphical}, i.e., the existence of a unique mapping from the parameters to the means and vice versa for the exponential family. 
%
Moreover, as in the discussion after \cref{cor_params}, the sharpness of the rate of $1/\varepsilon^4$ is unknown, and we leave it for future work.
%

%
%
%
%
%
%

%


%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
    %
    %
    %
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%

%
%
%
%
%
%
%
%
%
%
%

%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%

%

%
\newcommand{\gm}{\texttt{GM}}
%
%
%
%
%
%
%
%
%
%
%
%
%

%
%
%
%
%
%
%
%
%
%

%
%
%
%
%
%
%
%
%
%

%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%

%
%
%
%
%
%
%
%

%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%

%
%
%
%
%
%
%
%

%
%

%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%


%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%