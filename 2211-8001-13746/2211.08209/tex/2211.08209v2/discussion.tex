\section{Discussion}
\label{sec_discussion}
We introduce an exponential family approach to learn unit-level counterfactual distributions from a single sample per unit even when there is unobserved confounding. By conditioning on the latent confounders and using a novel convex loss function, we estimate the parameters of unit-level counterfactual distributions given the information about what actually happened. {The resulting estimates of unit-level counterfactual distributions enable us to estimate any functional of each unit's potential outcomes under alternate interventions. We analyze each unit's expected potential outcomes under alternate interventions, thereby providing a guarantee on unit-level counterfactual effects, i.e., individual treatment effects.}
%
%
%
We note that our approach makes only macro-level assumptions about the underlying causal graph and does not assume the knowledge of the micro-level causal graph.

%

%

A side product of our results is a strategy for answering interventional questions, e.g., to estimate average treatment effects. These questions are equivalent to estimating distributions of the form $f_{\rvby | \mathrm{do}(\rvba)}(\svby | \mathrm{do}(\rvba = \svba))$ where the do-operator \citep{Pearl2009} forces $\rvba$ to be $\svba$. Under the causal framework considered (\cref{fig_graphical_models}(b)), we have $f_{\rvby | \mathrm{do}(\rvba)}(\svby | \mathrm{do}(\rvba = \svba))=\Expectation_{\rvbv, \rvbz}\normalbrackets{f_{\rvby | \rvba, \rvbz, \rvbv}(\svby | \svba, \svbz, \svbv)}$. Consequently, the mixture distribution $n^{-1} \sum_{i \in [n]} \what{f}^{(i)}_{\rvby | \rvba}(\svby | \svba)$ with $\what{f}^{(i)}_{\rvby | \rvba}(\svby | \svba)$ defined in \cref{eq_counterfactual_distribution_y}, serves as a natural estimate via our strategy. 
%
%
Investigating the efficacy of this estimator is an interesting future direction. 
%

In this work, the joint exponential family distribution~\cref{eq_joint_distribution_zvay} had linear and quadratic interactions, and the effect of unobserved covariates $\rvbz$---after conditioning on them---was captured by a first-order interaction term varying with the realized value of $\rvbz$ for each unit ($\sbraces{\ExternalField(\svbz^{(i)})}_{i=1}^n$). When one considers higher-order interaction terms in the joint distribution, the conditional distributions would also have higher-order interaction terms (the highest order in the conditional distribution is one less than the highest order in the joint distribution) that vary with $\rvbz$. For such cases, while our analysis for population-level parameters (\cref{theorem_parameters} Part I's proof in \cref{sec:proof_of_theorem_parameters}) is likely to extend easily, new arguments for analyzing quadratic (or higher-order) interaction terms that vary for each unit seem necessary. Developing these results, e.g., suitable analogs of Dobrushin's condition for higher-order exponential family, present an exciting future venue for research.
%
%
%
%

Our methodology can be useful for a class of multi-task learning problems \citep{caruana1997multitask}, e.g., when we have multiple logistic regression tasks with some commonalities. For a logistic regression task, the exponential family model~\cref{eq_conditional_distribution_vay} has been used by \cite{DaganDDA2021} to allow dependencies between the labels via the parameter $\ParameterMatrix$ (instead of assuming independence between the labels), e.g., for spatio-temporal data. They consider a single regression task and assume that the dependency matrix $\ParameterMatrix$ is known up to a constant and learn a task-specific parameter $\ExternalField(\svbz)$ (where $\svbz$ denotes a task). Our model and methodology apply to the case of fully unknown $\ParameterMatrix$ given multiple datasets that share the same dependency parameter $\ParameterMatrix$ but have varying task-specific parameters $\ExternalField(\svbz)$; and provide a tractable way to estimate all these parameters together. 
%
Analyzing whether our methodology can be extended beyond logistic regression models for multi-task learning is a question worthy of further investigation.
%
%

%
%
%
%
%
%
%
%
%
%
%

%
%
%
 
%

%
%
%

%





%
%

%

%
%

%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%




%
%



%
%

%