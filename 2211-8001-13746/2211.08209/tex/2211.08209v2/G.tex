\section{Identifying weakly dependent random variables}
\label{sec_conditioning_trick}
In \cref{section_lsi_tail_bounds}, we derived (in \cref{thm_LSI_main}) that a random vector (supported on a compact set) satisfies the logarithmic Sobolev inequality if it satisfies the Dobrushin's uniqueness condition (in \cref{def_dobrushin_condition}). Further, we also derived (\cref{thm_main_concentration}) tail bounds for a random vector satisfying the logarithmic Sobolev inequality. Combining the two, we see that in order to use the tail bound, the random vector needs to satisfy the Dobrushin's uniqueness condition, i.e, the elements of the random vector should be weakly dependent. In this section, we show that any random vector (outside Dobrushin's regime) that is a $\dGM$-Sparse Graphical Model (to be defined) can be reduced to satisfy the Dobrushin's uniqueness condition. In particular, we show that
%
by conditioning on a subset of the random vector, the unconditioned subset of the random vector (in the conditional distribution) are only weakly dependent. We exploit this trick in \cref{lemma_concentration_psi} and \cref{lemma_concentration_bar_psi} to enable application of the tail bound in  \cref{section_lsi_tail_bounds}. The result below is a generalization of the result in \cite{DaganDDA2021} for discrete random vectors to continuous random vectors.\\

\noindent We start by defining the notion of $\dGM$-Sparse Graphical Model. 
\begin{definition}[{$\dGM$-Sparse Graphical Model}]\label{def:tau_sgm}
	A pair of random vectors $\braces{\rvbx, \rvbz}$ supported on $\cX^p \times \cZ^{p_z}$ is a $\dGM$-Sparse Graphical Model for model-parameters 
	%
	$\dGM \defn (\aGM, \eGM, \xmax, \ParameterMatrix)$
	and denoted by $\tSGM$ if $\cX = \{-\xmax, \xmax\}$, and
	\begin{enumerate}
		\item for any realization $\svbz \in \cZ^{p_z}$, the conditional probability distribution of $\rvbx$ given $\rvbz=\svbz$ is given by $\JointDistfun$ in \cref{eq_conditional_distribution_vay} for a vector $\ExternalField(\svbz) \in \real^p$ depending on $\svbz$ and a symmetric matrix $\ParameterMatrix \in \real^{p\times p} $ (independent of $\svbz$),
		\item $\max\braces{\max_{\svbz \in \cZ^{p_z}} \infnorm{\ExternalField(\svbz)}, \maxmatnorm{\ParameterMatrix}} \leq \aGM$, and
		\item $\infmatnorm{\ParameterMatrix} \leq \eGM$.
		%
	\end{enumerate}
\end{definition}


\noindent Now, we provide the main result of this section. 
%
\newcommand{\conditioningtrickresultname}{Identifying weakly dependent random variables}
\begin{proposition}[{\conditioningtrickresultname}]\label{lemma_conditioning_trick}
	Given a pair of random vectors $\braces{\rvbx, \rvbz}$ supported on $\cX^p \times \cZ^{p_z}$ that is a $\tSGM$ (\cref{def:tau_sgm}) with $\dGM \defn (\aGM, \eGM, \xmax, \ParameterMatrix)$, and a scalar $\lambda \in (0, \eGM]$, there exists $\numindsets \defn 32  \eGM^2 \log 4p / \lambda^2$ subsets $\sets \subseteq [p]$ that satisfy the following properties:
	\begin{enumerate}[label=(\alph*)]
		\item\label{item:cardinality_independence_set} For any $t \in [p]$, we have $\sum_{u=1}^{\numindsets} \Indicator(t \in \setU) = \ceils{{\lambda \numindsets }/({8\eGM})}$.
		\item \label{item:conditional_sgm_independence_set} For any $u \in [\numindsets]$, 
		\begin{enumerate}[label=(\roman*)]
			\item \label{item:conditional_sgm} the pair of random vectors $\braces{\rvbx_{\setU}, (\rvbx_{-\setU}, \rvbz)}$ correspond to a $\tSGM[1]$ with $\dGM_1 \defn (\aGM+2\xmax \eGM, \lambda, \xmax, \ParameterMatrix_{\setU})$  where $\ParameterMatrix_{\setU} \defn \braces{\ParameterTU[tv]}_{t,v \in \setU}$, and
			\item \label{item:conditional_sgm_dob} the random vector $\rvbx_{\setU}$ conditioned on $(\rvbx_{-\setU}, \rvbz)$ satisfies the Dobrushin's uniqueness condition (\cref{def_dobrushin_condition}) with coupling matrix $2\sqrt{2} \xmax^2 |\ParameterMatrix_{\setU}|$ whenever $\lambda \in \Big(0, \frac{1}{2\sqrt{2}\xmax^2}\Big]$ with $\opnorm{|\ParameterMatrix_{\setU}|} \leq \lambda$.
		\end{enumerate}
		%
		%
	\end{enumerate}
\end{proposition}

%
%
%

\begin{proof}[Proof of \lowercase{\Cref{lemma_conditioning_trick}}: \conditioningtrickresultname]
	~~ We prove each part one-by-one using a generalization of \citet[Lemma. 12]{DaganDDA2021}. 
	
	Recall \citet[Lemma. 12]{DaganDDA2021}: Let $A\in \Reals^{p \times p}$ be a matrix with zeros on the diagonal and $\infmatnorm{A} \leq 1$. Let $0 < \eta < 1$. Then, there exists subsets $\barsets \subseteq [p]$ with $\wbar{\numindsets} \defn  32 \log 4p / \eta^2$ such that 
	\begin{enumerate}[label=(\alph*)]
		\item\label{dagan_a} For any $t \in [p]$, we have $\sum_{u=1}^{\wbar{\numindsets}} \Indicator(t \in \barsetU) = \ceils{{\eta \wbar{\numindsets} }/{8}}$, and
		\item For any $u \in [\wbar{\numindsets}]$ and $t \in \barsetU$, $\sum_{v \in \barsetU} |A_{tv}| \leq \eta$.
	\end{enumerate}
	\noindent We claim that \citet[Lemma. 12]{DaganDDA2021} holds even when $A$ does not have zeros on the diagonal. The proof is exactly the same as the proof of \citet[Lemma. 12]{DaganDDA2021}.
	
	\paragraph{Proof of part~\cref{item:cardinality_independence_set}}
	From \cref{def:tau_sgm}, for any realization $\svbz \in \cZ^{p_z}$, the conditional probability distribution of $\rvbx$ given $\rvbz=\svbz$ is given by $\JointDistfun$ in \cref{eq_conditional_distribution_vay} where $\ExternalField(\svbz) \in \real^p$ is a vector and $\ParameterMatrix \in \real^{p\times p} $ is a symmetric matrix with $\infmatnorm{\ParameterMatrix} \leq \eGM$. Consider the matrix $A \defn \frac{1}{\eGM} \ParameterMatrix$. Since $\infmatnorm{A} \leq 1$, we can apply the generalization of \citet[Lemma. 12]{DaganDDA2021} on $A$ with $\eta = \frac{\lambda}{\eGM}$. Then part~\cref{item:cardinality_independence_set} follows directly from
	\citet[Lemma. 12.1]{DaganDDA2021}. 
	%
	%
	%
	%
	%
	%
	%
	%
	%
	%
	
	
	\paragraph{Proof of part~\cref{item:conditional_sgm_independence_set}\cref{item:conditional_sgm}}
	%
	To prove this part, consider the distribution of $\rvbx_{\setU}$ conditioned on $\rvbx_{-\setU} = \svbx_{-\setU}$ and $\rvbz = \svbz$ for any $u \in [\numindsets]$, i.e., $f_{\rvbx_{\setU} | \rvbx_{-\setU},\rvbz} (\svbx_{\setU} | \svbx_{-\setU}, \svbz; \ExternalField(\svbz), \ParameterMatrix) \defn f (\svbx_{\setU} | \svbx_{-\setU}, \svbz; \ExternalField(\svbz), \ParameterMatrix)$ as follows
	\begin{align}
f (\svbx_{\setU} | \svbx_{-\setU}, \svbz; \ExternalField(\svbz), \ParameterMatrix) \propto \exp \biggparenth{ \! \sum_{t \in \setU} \!\!\Bigparenth{\!\ExternalFieldt(\svbz) \!+\! 2\!\sum_{v \notin \setU} \!\!\ParameterTU[tv] x_{v}} x_t \!+\! \sum_{t \in \setU} \!\! \sum_{~ v \in \setU} \!\ParameterTU[tv] x_t x_v}. 	
 \label{eq_conditional_dist_xIj}
	\end{align}
	We can re-parameterize $f(\svbx_{\setU} | \svbx_{-\setU}, \svbz; \ExternalField(\svbz), \ParameterMatrix)$ in \cref{eq_conditional_dist_xIj} as follows
	\begin{align}
	&f_{\rvbx_{\setU} | \rvbx_{-\setU},\rvbz} (\svbx_{\setU} | \svbx_{-\setU}, \svbz; \ConditioningField(\svbz, \svbx_{-\setU}), \ConditioningMatrix)  \propto  \exp \Bigparenth{ \normalbrackets{\ConditioningField(\svbz, \svbx_{-\setU})}\tp \svbx_{\setU} + \svbx_{\setU}\tp\ConditioningMatrix \svbx_{\setU}}
 \end{align}
 where
  \begin{align}
	&\ConditioningField(\svbz, \svbx_{-\setU}) \in \Reals^{|\setU| \times 1},
	\stext{with} \ConditioningFieldU(\svbz, \svbx_{-\setU}) \defn \ExternalFieldt(\svbz) + 2\sum_{k \notin \setU} \ParameterTU[tv] x_{k} 
	\stext{for} t \in \setU, \stext{and} \label{eq_parameter_mapping_0}\\
	&\ConditioningMatrix = \ConditioningMatrix\tp  \in \Reals^{|\setU| \times |\setU|}  
	\stext{with}
	\ConditioningParameter[tv] \defn \ParameterTU[tv],
	%
	%
	\stext{for all} t, v \in \setU.
	\label{eq_parameter_mapping}
	\end{align}
	Now, to show that the random vector $\rvbx_{\setU}$ conditioned on $\rvbx_{-\setU}$ and $\rvbz$ corresponds to an $\tSGM[1]$ with $\dGM_1 \defn (\aGM+2\xmax \eGM, \lambda,\xmax, \ParameterMatrix_{\setU})$, it suffices to establish that
	\begin{align}
	\label{eq:suff_steps_sgm}
	\max\braces{\max_{\svbz \in \cZ^{p_z}} \infnorm{\ConditioningField(\svbz, \svbx_{-\setU})}, \maxmatnorm{\ConditioningMatrix}} \sless{(i)} \aGM+2\xmax \eGM \qtext{and} \infmatnorm{\ConditioningMatrix} \sless{(ii)} \lambda.
	\end{align}
	To establish~(i) in \cref{eq:suff_steps_sgm}, we note that
	\begin{align}
	\maxmatnorm{\ConditioningMatrix} & \sless{\cref{eq_parameter_mapping}} \maxmatnorm{\ParameterMatrix} \sless{(a)}  \aGM \qtext{and} \label{eq:phitheta_bound_0}\\
	\infnorm{\ConditioningField(\svbz, \svbx_{-\setU})} & \sless{(b)} \infnorm{\ExternalField(\svbz)} + 2\max_{t \in \setU}\sonenorm{\ParameterRowt} \infnorm{\svbx} \sless{(c)} \infnorm{\ExternalField(\svbz)} + 2\xmax \infmatnorm{\ParameterMatrix} \\
 & \sless{(d)} \aGM+2\xmax \eGM, \label{eq:phitheta_bound_1}
	\end{align}
	where $(a)$ and $(d)$ follow from \cref{def:tau_sgm}, $(b)$ follows from \cref{eq_parameter_mapping_0} and the triangle inequality, and $(c)$ follows from the definition of $\infmatnorm{\cdot}$ and \cref{def:tau_sgm}. Then, from \cref{eq:phitheta_bound_0} and \cref{eq:phitheta_bound_1}, we have
	\begin{align}
	\max\braces{\max_{\svbz \in \cZ^{p_z}} \infnorm{\ConditioningField(\svbz, \svbx_{-\setU})}, \maxmatnorm{\ConditioningMatrix}} \leq \aGM+2\xmax \eGM,
	\end{align}
	as claimed. Next, to establish~(ii) in \cref{eq:suff_steps_sgm}, we again apply  the generalization of \citet[Lemma. 12]{DaganDDA2021} on the matrix $A = \frac{1}{\eGM} \ParameterMatrix$ with $\eta = \frac{\lambda}{\eGM}$. Then, we have
	\begin{align}
	\sum_{v \in \setU} \biggabs{\frac{\ParameterTU[tv]}{\eGM}} \leq \frac{\lambda}{\eGM} 
	\qtext{for all $t \in \setU$, $u \in [\numindsets]$.}
	\label{eq_lemma12_bound}
	\end{align}
	Therefore, we have
	\begin{align}
	\infmatnorm{\ConditioningMatrix} = \max_{t \in \setU} \Bigparenth{\sum_{v \in \setU} \bigabs{\ConditioningParameter}} \sequal{\cref{eq_parameter_mapping}} \max_{t \in \setU} \Bigparenth{\sum_{v \in \setU} \bigabs{\ParameterTU[tv]}} \sless{\cref{eq_lemma12_bound}} \lambda, \label{eq_bound_Upsilon_inf_norm}
	\end{align}   
	as desired. The proof for this part is now complete.
	
	\paragraph{Proof of part~\cref{item:conditional_sgm_independence_set}\cref{item:conditional_sgm_dob}}
	We start by noting that the operator norm of a symmetric matrix is bounded by the infinity norm of the matrix. Then, from the analysis in part~\cref{item:conditional_sgm_independence_set}~\cref{item:conditional_sgm}, for any $u \in \setU$, we have
	\begin{align}
	\opnorm{|\ParameterMatrix_{\setU}|} \leq \infmatnorm{|\ParameterMatrix_{\setU}|} \sequal{\cref{eq_parameter_mapping}} \infmatnorm{|\ConditioningMatrix|} \sless{\cref{eq_bound_Upsilon_inf_norm}} \lambda.
	\end{align}
	Therefore, $\infmatnorm{2\sqrt{2}\xmax^2 |\ParameterMatrix_{\setU}|} \leq 1$ whenever $\lambda \leq 1/2\sqrt{2}\xmax^2$. It remains to show %
	that for every $u \in [\numindsets]$, $t \in \setU, v \in \setU \!\setminus\! \{t\}$, $\rvbz = \svbz$, and $\svbx_{-t}, \tsvbx_{-t} \in \cX^{p-1}$ differing only in the $v^{th}$ coordinate,
	\begin{align}
	%
	\TV{f_{\rvx_t | \rvbx_{-t} = \svbx_{-t}, \rvbz = \svbz}}{f_{\rvx_t | \rvbx_{-t} = \tsvbx_{-t}, \rvbz = \svbz}} \leq 2\sqrt{2} \xmax^2 |\ParameterTU[tv]|.
	\end{align}
	To that end, fix any $u \in [\numindsets]$, any $t \in \setU$, any $v \in \setU \!\setminus\! \{t\}$, any $\rvbz = \svbz$, and any $\svbx_{-t}, \tsvbx_{-t} \in \cX^{p-1}$ differing only in the $v^{th}$ coordinate. We have
	\begin{align}
	\TV{f_{\rvx_t | \rvbx_{-t} = \svbx_{-t}, \rvbz = \svbz}}{f_{\rvx_t | \rvbx_{-t} = \tsvbx_{-t}, \rvbz = \svbz}}^2 & \sless{(a)} \frac{1}{2}\KLD{f_{\rvx_t | \rvbx_{-t} = \svbx_{-t}, \rvbz = \svbz}}{f_{\rvx_t | \rvbx_{-t} = \tsvbx_{-t}, \rvbz = \svbz}}\\
	& \sequal{(b)} \frac{1}{2} (2\ParameterTU[tv] x_v - 2 \ParameterTU[tv] \tx_v)^2 \xmax^2 \sless{(c)} 8\xmax^4 \ParameterTU[tv]^2,
	\end{align}
	where $(a)$ follows from Pinsker's inequality, $(b)$ follows by (i) applying 
	\cite[Theorem 1]{BusaFSZ2019} to the exponential family parameterized as per $f_{\rvx_t | \rvbx_{-t}, \rvbz}$ in \cref{eq_conditional_dist}, (ii) noting that $f_{\rvx_t | \rvbx_{-t} = \svbx_{-t}, \rvbz =\svbz} \propto \exp\bigparenth{ \normalbrackets{\ExternalFieldt(\svbz) + 2\ParameterRowttt\tp \svbx_{-t}} x_t + \ParameterTU[tt]\cx_t}$ and $f_{\rvx_t | \rvbx_{-t} = \tsvbx_{-t}, \rvbz =\svbz} \propto \exp\bigparenth{ \normalbrackets{\ExternalFieldt(\svbz) + 2\ParameterRowttt\tp \tsvbx_{-t}} x_t + \ParameterTU[tt]\cx_t}$ where $\cx_t \defn x_t^2 - \xmax^2/3$, and (iii) noting that the Hessian of the log partition function for any regular exponential family is the covariance matrix of the associated sufficient statistic which is bounded by $\xmax^2$ when $\cX = \{-\xmax, \xmax\}$, and $(c)$ follows because $x_v, \tx_v \in \{-\xmax, \xmax\}$. This completes the proof.
\end{proof}


%
%
%
%
%
%
%

