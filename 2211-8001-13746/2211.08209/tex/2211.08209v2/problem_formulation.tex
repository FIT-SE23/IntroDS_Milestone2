\vsep

\section{Problem formulation and algorithm}
\label{section_problem_formulation}
This section formalizes the problem, specifies our model, and defines the inference tasks of interest. 

%
\subsection{Underlying causal mechanism and counterfactual distributions}
\label{subsec_causal_mech}
We consider a counterfactual inference task where units go through $p_a \geq 1$ interventions. For every unit, we observe $p_y \geq 1$ outcomes of interest. The interventions and the outcomes could be confounded by $p_v \geq 0$ observed covariates as well as $p_z \geq 0$ unobserved covariates. Additionally, the observed covariates and the unobserved covariates could be arbitrarily associated. We denote the random vector associated with the interventions, the outcomes,  the observed covariates, and the unobserved covariates by $\rvba \defn (\rva_{1}, \cdots, \rva_{p_a}) \in \cA^{p_a}$, $\rvby = (\rvy_{1}, \cdots, \rvy_{p_y}) \in \cY^{p_y}$, $\rvbv \defn (\rvv_{1}, \cdots, \rvv_{p_v}) \in \cV^{p_v}$, and $\rvbz \defn (\rvz_{1}, \cdots, \rvz_{p_z})  \in \cZ^{p_z}$, respectively, where $\cA, \cY, \cV$, and $\cZ$ denote the support of interventions, outcomes, observed covariates, and unobserved covariates, respectively. We allow these sets to contain discrete, continuous, or mixed values. 

\paragraph{Causal mechanism}
We summarize the causal relationship between the random vectors $\rvbz$, $\rvbv$, $\rvba$, and $\rvby$ in \cref{fig_graphical_models}(a) where we denote the arbitrary association between $\rvbz$ and $\rvbv$ by a undirected arrow, and the causal association between (i) $(\rvbz, \rvbv)$ and $\rvba$, (ii) $(\rvbz, \rvbv)$ and $\rvby$, and (iii) $\rvba$ and $\rvby$ by directed arrows. 
%
More generally, we are interested in any setup consistent with the  graphical model in \cref{fig_graphical_models}(a). We assume access to $n$ independent realizations indexed by $i \in [n]$: $\svbv^{(i)}$, $\svba^{(i)}$, and $\svby^{(i)}$ denote the realizations of $\rvbv$, $\rvba$, and $\rvby$ for unit $i$, respectively. For every realized tuple $(\svbv^{(i)}, \svba^{(i)}, \svby^{(i)})$, there is a corresponding realization $\svbz^{(i)}$ of the unobserved covariates $\rvbz$ that is unobserved. Next, we discuss some examples covered by our framework.


\paragraph{Examples: sequential and network settings}
\label{subsubsec_examples}
\begin{figure}[t]
    \centering
    \begin{tabular}{c}
    \includegraphics[height=0.3\linewidth,clip]{network_main_4.pdf} ~~
    \includegraphics[height=0.3\linewidth, clip]{network_explained_4.pdf}  \\
    %
    %
    %
    \end{tabular}
    \caption{A graphical model for a single unit in the network setting with 4 users; arrows have same meaning as in \cref{fig_graphical_models}. Here $\rvv_t$, $\rva_t$, and $\rvy_t$ denote user $t$'s observed demographic factors, exposed product, and engagement level, respectively, and $\rvbz$ denotes the network-level unobserved factors. The left plot
    %
    illustrates the high-level dependency between the variables of different users in the network, and the right plot expands on it for users $1$ and $2$.}
    %
    %
    %
    \label{fig_graphical_models_2}
    %
\end{figure}
While \cref{fig_graphical_models}(a) exhibits the high-level causal links between $\rvbz$, $\rvbv$, $\rvba$, and $\rvby$, there could be complex low-level causal links between elements of these vectors. We do not assume any knowledge of such low-level causal links. In \cref{fig_graphical_models}(b), we provide an example for sequential recommender systems covered by our work where every unit's (i) $\rva_{t+1}$ depends on $\rva_{t}$ in addition to $\rvv_{t+1}$ and $\rvbz$, and (ii) $\rvy_{t+1}$ depends on $\rva_{t}$ and $\rvy_{t}$ in addition to $\rva_{t+1}$, $\rvv_{t+1}$ and $\rvbz$. Another classical example covered by our framework includes the network setting where a unit represents a social network where users are linked to each other by interpersonal relationships as shown in \cref{fig_graphical_models_2}. Similar to the sequential recommender system, every user was exposed to a product based on observed demographic factors as well as certain unobserved factors, and the user's engagement level was recorded. The engagement level of user $t$, i.e., $\rvy_{t}$, depended on the product exposed to its neighbor $u$, i.e., $\rva_{u}$, in addition to its observed demographic factors $\rvv_{t}$, its exposed product $\rva_{t}$, and unobserved factor $\rvbz$. Further, $\rvy_{t}$ could have been associated with $\rvy_{u}$. 


\paragraph{Unit-level counterfactual distributions}
%
We denote the Neyman-Rubin potential outcomes of unit $i \in [n]$ under interventions $\svba \in \cA^{p_a}$ by $\svby^{(i)}(\svba)$. We make the stable unit treatment value assumption (SUTVA) \citep{rubin1980randomization} for the observed outcome, 
i.e.,  
%
%
$\svby^{(i)} =  \svby^{(i)}(\svba^{(i)}) $ for all $i \in [n]$.
%
%
%
 For independent units with the causal mechanism and SUTVA assumed here, the unit-level counterfactual distributions are equivalent to certain unit-level conditional distributions as we now argue. Consider unit $i \in [n]$ and fix the observed covariates and the unobserved covariates at $\svbv^{(i)}$ and $\svbz^{(i)}$, respectively. Then, let $\wtil{\svby}^{(i)}$ be a realization of $\rvby$ when $\rvba = \wtil{\svba}^{(i)}$. We are interested in the distribution of the potential outcomes of unit $i$ for interventions $\wtil{\svba}^{(i)}$, i.e., the distribution of $\svby^{(i)}(\wtil{\svba}^{(i)})$ given $\rvbz = \svbz^{(i)}, \rvbv = \svbv^{(i)}$. Under the causal framework considered here (see \cref{fig_graphical_models}(a)), it is equivalent to the distribution of $\svby^{(i)}(\wtil{\svba}^{(i)})$ given $\rvba = \wtil{\svba}^{(i)}, \rvbz = \svbz^{(i)}, \rvbv = \svbv^{(i)}$ since $(\rvbz, \rvbv)$ satisfy ignorability \citep{Pearl2009, imbens2015causal}, i.e., the potential outcomes are independent of the interventions given $(\rvbz, \rvbv)$.  Further, under SUTVA, it is equivalent to the distribution of $\wtil{\svby}^{(i)}$ given $\rvba = \wtil{\svba}^{(i)}, \rvbz = \svbz^{(i)}, \rvbv = \svbv^{(i)}$, i.e., $f_{\rvby | \rvba, \rvbz, \rvbv}(\rvby=\cdot | \rvba= \wtil{\svba}^{(i)}, \svbz^{(i)}, \svbv^{(i)})$. Therefore, our goal is to learn the $n$ unit-level conditional distributions in \cref{eq_set_conditional_distributions}. 
 %
 Now, we proceed to the modeling details.
 %

%
%






%
%
%
%

%
%

%
%
%
%
%
%
%

%
%
%
%
%
%
%
%
 
%
%
 
 
%
%
 
 
%
 
%
 
%
 
%

%
 
 
%
%
%
%
%
%

%

%

%

%


%
%

%



%
%
%
%
%
%

%

%
%
%
%



\subsection{Exponential family modeling and its consequences}
\label{subsec_exp_fam}
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
Let $\tilp \defn p_z + p_v + p_a + p_y$. For the $\tilp$-dimensional random vector $\ranvarvec \defeq \normalparenth{\rvbz, \rvbv, \rvba, \rvby}$, we parameterize its joint probability distribution $f_{\ranvarvec}$ as an exponential family model with natural statistics $\rvbw$ and $\rvbw \rvbw\tp$ and natural parameters $\phi \in \Reals^{\tilp \times 1}$, and $\Phi \in \Reals^{\tilp \times \tilp}$ so that
%
\begin{align}
    f_{\ranvarvec}(\varvec; \phi, \Phi) \propto \exp\Bigparenth{ \phi\tp \varvec
    +\varvec\tp \Phi \varvec},
    %
    %
    %
    \qtext{where}
    %
    %
    \varvec \defeq (\svbz, \svbv, \svba, \svby),
    \label{eq_joint_distribution_zvay}
\end{align}
and $\svbz \defn (z_{1}, \cdots, z_{p_z})$, $\svbv \defn (v_{1}, \cdots, v_{p_v})$, $\svba \defn (a_{1}, \cdots, a_{p_a})$, and $\svby \defn (y_{1}, \cdots, y_{p_y})$ denote realizations of $\rvbz$, $\rvbv$, $\rvba$, and $\rvby$, respectively. 
%
Without loss of generality, we can assume $\Phi$ to be a symmetric matrix. 
Next, we show that with this modeling assumption, learning unit-level counterfactual distribution can be reduced to learning a suitable exponential family model.


Note that the unit-level conditional distribution of $\rvby$ conditioned on $\rvba = \svba$, $\rvbz = \svbz$, and $\rvbv = \svbv$ 
%
is an exponential family model with natural statistics $\rvby$ and $\rvby \rvby\tp$ and
\begin{align}
    f_{\rvby | \rvba, \rvbz, \rvbv}(\svby | \svba, \svbz, \svbv) \! \propto  \!\exp\Bigparenth{\!\bigbrackets{\phi^{(y)\tp} \!\!+\! 2\svbz\tp\Phi^{(z,y)} \!+\! 2\svbv\tp\Phi^{(v,y)} \!+\! 2\svba\tp\Phi^{(a,y)}} \svby \!+\! \svby\tp\Phi^{(y,y)} \svby}, \label{eq_conditional_distribution_y}
    %
\end{align}
where $\phi^{(y)} \in \Reals^{p \times 1}$ is the component of $\phi$ corresponding to $\rvby$ and $\Phi^{(u, y)} \in \Reals^{p_u \times p_y}$ is the component of $\Phi$ corresponding to $\rvbu$ and $\rvby$ for all $\rvbu \in \{\rvbz, \rvbv, \rvba, \rvby\}$.\footnote{The exponential family in \cref{eq_conditional_distribution_y} is same as the one considered in \citet[Equation 1.3]{taeb2020learning}.} We make two key observations:
%
    %
    (a) the term $\Phi^{(z,y)\top}\!\svbz$ captures the effect of unobserved covariates $\svbz$ on $f_{\rvby | \rvba, \rvbz, \rvbv}(\rvby\!=\!\cdot | \rvba\!=\! \cdot, \svbz, \svbv)$ and (b) the task of learning $f_{\rvby | \rvba, \rvbz, \rvbv}(\rvby=\cdot | \rvba= \cdot, \svbz, \svbv)$ in \cref{eq_conditional_distribution_y} as a function of $\rvba$ reduces to learning
%
\begin{align}
    \stext{(i)} \phi^{(y)} + 2\Phi^{(z,y)\top} \svbz + 2\Phi^{(v,y)\top} \svbv, \qtext{} \stext{(ii)} \Phi^{(a,y)}, \qtext{and} \stext{(iii)} \Phi^{(y,y)}. \label{eq_actual_parameters_of_interest}
    %
\end{align}
That is, learning the unit-level conditional distribution for unit $i$ is equivalent to learning
\begin{align}
    \Truephi = \bigbraces{\phi^{(y)} + 2\Phi^{(z,y)\top} \svbz^{(i)} + 2\Phi^{(v,y)\top} \svbv^{(i)}, \Phi^{(a,y)}, \Phi^{(y,y)}},
    \label{eq:gammai}
\end{align}
where the notation $\Truephi$ is the same as in \cref{section_introduction}. Next, we argue that learning the three quantities in \cref{eq_actual_parameters_of_interest} is subsumed in learning the parameters of the (unit-level) conditional distribution $f_{\rvbx |\rvbz}$ of the random vector $\rvbx \defeq \normalparenth{\rvbv, \rvba, \rvby}$ conditioned on $\rvbz = \svbz$. Note that $f_{\rvbx |\rvbz}$ belongs to an exponential family with natural statistics $\rvbx$ and $\rvbx \rvbx\tp$. For all $\rvbu \in \normalbraces{\rvbv, \rvba, \rvby}$, let $\phi^{(u)} \in \Reals^{p_u \times 1}$ be the component of $\phi$ corresponding to $\rvbu$, and $\Phi^{(z, u)} \in \Reals^{p_z \times p_u}$ be the component of $\Phi$ corresponding to $\rvbz$ and $\rvbu$. Then $f_{\rvbx |\rvbz}$ can be parameterized as follows:
\begin{align}
    \JointDist \!\propto\! \exp \Bigparenth{\! \normalbrackets{\ExternalField(\svbz)}\tp \! \svbx \!+\! \svbx\!\tp \! \ParameterMatrix \svbx\!} \label{eq_conditional_distribution_vay}, \!\! \stext{where}
    \ExternalField(\svbz)  \defn \!\begin{bmatrix}  \phi^{(v)} \!+\! 2 \Phi^{(z,v)\top} \svbz \\  \phi^{(a)} \!+\! 2 \Phi^{(z,a)\top} \svbz \\  \phi^{(y)} \!+\! 2 \Phi^{(z,y)\top} \svbz \end{bmatrix}  \! \! \in \! \Reals^{p\times 1},\, 
    %
    %
    %
\end{align} 
$\svbx \defn (\svbv, \svba, \svby)$, $p \defn p_v + p_a + p_y$ and $\Theta \in \Reals^{p \times p}$ denotes the component of $\Phi$ corresponding to $\rvbx$. 
%
%
Given some estimates for $\ExternalField(\svbz)$ and $\ParameterMatrix$, using their appropriate components also yields an estimate of the three quantities in \cref{eq_actual_parameters_of_interest} for any $\rvbv = \svbv$. 
To summarize, the spurious associations or unobserved confounding between $\svba$ and $\svby$ introduced due to unobserved $\rvbz$ are fully captured by $\Phi^{(z,y)\top} \svbz$ or equivalently by $\ExternalField(\svbz)$; thereby, learning unit-level counterfactual distributions require us to learn these unit-level parameters. 
%
%
%
%

%


\paragraph{Reduced inference task and modeling constraints}
\label{subsec_inference_tasks_of_interest}
%
%
Let $f_{\ranvarvec}(\cdot; \phi^*, \Phi^*)$ denote the true data generating distribution of $\rvbw$ in \cref{eq_joint_distribution_zvay}, and let $\TrueJointDistfun$ denote the true distribution of $\rvbx$ conditioned on $\rvbz = \svbz$ in \cref{eq_conditional_distribution_vay}.
%
Then, for all $i \in [n]$, we note that the realization $\svbx^{(i)} \defn (\svbv^{(i)}, \svba^{(i)}, \svby^{(i)})$ is consistent with the conditional distribution $\TrueJointDistfun[(i)]$ where we do not observe $\svbz^{(i)}$. Our primary goal is to learn the $n$ unit-level counterfactual distributions, which as noted above 
%
%
simplifies to estimating the following parameters:
\begin{align}
    \hspace{-1cm}\stext{(i) Unit-level} \TrueExternalFieldI[i] \defn \TrueExternalField(\svbz^{(i)}) \stext{for} i \in [n], \qtext{and (ii) Population-level} \TrueParameterMatrix. \label{eq_parameters_of_interest}
\end{align}
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
Our secondary goal is to estimate the expected potential outcomes for any given unit $i$ (with $\rvbz = \svbz^{(i)}, \rvbv = \svbv^{(i)}$) 
and an alternate intervention $\wtil{\svba}^{(i)}$:
\begin{align}
    \mu^{(i)}(\wtil{\svba}^{(i)}) \defn  \Expectation[\svby^{(i)}(\wtil{\svba}^{(i)}) | \rvbz = \svbz^{(i)}, \rvbv = \svbv^{(i)}],\label{eq_causal_estimand}
    %
\end{align}
where $\svby^{(i)}(\wtil{\svba}^{(i)})$ denotes the potential outcomes for unit $i \in [n]$ under interventions $\wtil{\svba}^{(i)} \in \cA^{p_a}$.

 %
%
%
%
%
%


%

%


%
%
%
%
%
%
%
For ease of exposition, we consider bounded continuous sets\footnote{As the reader can note, our results generalize easily to settings with discrete and bounded mixed sets.} $\cV$, $\cA$, and $\cY$ 
with $\cV = \cA = \cY \defn \cX  = \normalbrackets{-\xmax, \xmax}$ for a given $\xmax$.\footnote{Bounded $\cA$ under the joint exponential family distribution in \cref{eq_joint_distribution_zvay} implies the positivity assumption \citep{hernan2020causal}, i.e., there exists $\delta > 0$ such that $\delta < f_{\rvba | \rvbz, \rvbv}(\svba | \svbz, \svbv)$ for all $\svba \in \cA$, $\svbz \in \cZ$, $\svbv \in \cV$.} Throughout this paper, it is convenient to further constrain the model as follows:
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
\begin{assumption}[Bounded and sparse parameters]
\label{assumptions}
The true model parameters~\cref{eq_parameters_of_interest} satisfy
\begin{align}
    \TrueExternalFieldI[i] &\in \ParameterSet_{\ExternalField} \defn  
    \braces{ \ExternalField  \in \Reals^{p \times 1}: \infnorm{\ExternalField} \leq \aGM} 
    \stext{for all $i \in [n]$,} \label{eq_parameter_set_external_field}
    \intertext{and}
    %
    %
    \TrueParameterMatrix &\in \ParameterSet_{\ParameterMatrix} \defn  \braces{ \ParameterMatrix \in \Reals^{p \times p}: \ParameterMatrix = \ParameterMatrix\tp, ~ \maxmatnorm{\ParameterMatrix} \leq \aGM, ~ \maxp[t] \zeronorm{\ParameterRowt} \leq \bGM}. \label{eq_parameter_set}
\end{align}
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
\end{assumption}
%
While \cref{eq_parameter_set_external_field} assumes that the unit-level parameters lie in a bounded set (a necessary condition for model identifiability \citep{SanthanamW2012}), \cref{eq_parameter_set} assumes that each $\rvx_t \in \rvbx$ interacts with only a few other $\rvx_u \in \rvbx$ in \cref{eq_conditional_distribution_vay}. As a result, \cref{assumptions} implies that the exponential family in \cref{eq_conditional_distribution_vay} corresponds to MRFs (\cref{sec_related_work}), also known as undirected graphical models (defined in \cref{sec_conditioning_trick}). We note that \cref{assumptions} is standard in the literature on learning MRFs \citep{bresler2015efficiently,vuffray2016interaction,klivans2017learning,VuffrayML2022,ShahSW2021A}.  
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
We are now ready to state our algorithm.
%

%
%
