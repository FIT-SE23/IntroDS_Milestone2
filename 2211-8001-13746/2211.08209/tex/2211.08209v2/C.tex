\section{Proof of {Theorem \ref{theorem_parameters}} Part II: \nodeparammainresultname}
\label{sec_proof_thm_node_parameters_recovery}
%
%
%
%
%
%
%
%
%
To analyze our estimate of the unit-level parameters, we use the estimate $\EstimatedParameterMatrix$ of the population-level parameter $\TrueParameterMatrix$ along with the associated guarantee provided in \cref{theorem_parameters} Part I. We note that the constraints on the unit-level parameters in \cref{eq_estimated_parameters} are independent across units, i.e., $\ExternalFieldI[i] \in \ParameterSet_{\ExternalField}$ independently for all $ i \in [n]$. Therefore, we look at $n$ independent convex optimization problems by decomposing the loss function $\loss$ in \cref{eq:loss_function} and the estimate $\ExtendedEstimatedParameterMatrix$ in \cref{eq_estimated_parameters} as follows: For $i \in [n]$, we define
%
%
%
%
\begin{align}
\loss^{(i)}\bigparenth{\ExternalFieldI} &\defn \sump[t] \exp\Bigparenth{-\normalbrackets{\ExternalFieldtI + 2\EstimatedParameterRowttt\tp \svbx_{-t}^{(i)}} x_t^{(i)} - \EstimatedParameterTU[tt] \cx_t^{(i)}} \\
\qtext{and} \EstimatedExternalFieldI &\defn \argmin_{\ExternalFieldI \in \ParameterSet_{\ExternalField}} \loss^{(i)}\bigparenth{\ExternalFieldI}.
\label{eq:loss_function_n}
\end{align}
%
Now, fix any $i \in [n]$. From \cref{eq:loss_function_n}, we have $\loss^{(i)}\bigparenth{\EstimatedExternalFieldI} \leq  \loss^{(i)}\bigparenth{\TrueExternalFieldI}$. Using contraposition, to prove this part, it is sufficient to show that all points $\ExternalFieldI \in \ParameterSet_{\ExternalField}$ that satisfy $\stwonorm{\ExternalFieldI - \TrueExternalFieldI} \geq R(\varepsilon, \delta)$ also uniformly satisfy
\begin{align}
\loss^{(i)}\bigparenth{\ExternalFieldI} & \geq  \loss^{(i)}\bigparenth{\TrueExternalFieldI} + R^2(\varepsilon, \delta) \stext{when} n \geq \frac{ce^{c'\bGM}p^2}{\varepsilon^4}  \Bigparenth{p \log \frac{p}{\delta \varepsilon^2} + \tmetric_{\ExternalField,n}(\varepsilon, \delta)}, \label{eq_sufficienct_condition_thm_node}
\end{align}
with probability at least $1-\delta$ where $R(\varepsilon, \delta)$ 
%
was defined in \cref{eq_radius_node_thm} and $\tmetric_{\ExternalField,n}(\varepsilon, \delta)$ was defined in \cref{eq_tmetric_node_thm}.
%
%
Then, the guarantee in \cref{theorem_parameters} follows by applying a union bound over all $i \in [n]$.\\

\noindent To that end, the lemma below, proven in \cref{sub_proof_lemma_parameter_single_external_field}, shows that for any fixed $\ExternalFieldI \in \ParameterSet_{\ExternalField}$, if $\ExternalFieldI$ is far from $\TrueExternalFieldI$, then with high probability $\loss^{(i)}\bigparenth{\ExternalFieldI}$ is significantly larger than $\loss^{(i)}\bigparenth{\TrueExternalFieldI}$.
%
%
\begin{lemma}[{\singleparameterseparation}]\label{lemma_parameter_single_external_field}
	Fix any $\varepsilon > 0$, $\delta \in (0,1)$, and $i \in [n]$. Then, for any $\ExternalFieldI \in \ParameterSet_{\ExternalField}$ such that $\stwonorm{\ExternalFieldI - \TrueExternalFieldI} \geq  \varepsilon \ratio$ (see \cref{eq_radius_node_thm}), we have
	\begin{align}
	\loss^{(i)}\bigparenth{\ExternalFieldI} \!\geq\!  \loss^{(i)}\bigparenth{\TrueExternalFieldI} \!+\! \frac{2^{2.5}\aGM\bGM \xmax^4}{\pi e \ctwo[5]} \stwonorm{\ExternalFieldI \!-\! \TrueExternalFieldI}^2 \stext{for}
	n \!\geq \! \frac{ce^{c'\bGM}p^2}{\varepsilon^4} \!\Bigparenth{\!p \log \frac{p}{\delta \varepsilon^2} \!+\!  \metric_{\ExternalField,n}\bigparenth{ \varepsilon^2}\!},
	\end{align}
	with probability at least 
	%
	$1-\delta - c\bGM^2  \log p \cdot \exp(-e^{-c'\bGM}\stwonorm{\ExternalFieldI - \TrueExternalFieldI}^2 ) 
	%
	$
	where $\ctwo$ was defined in \cref{eq:constants}.
	%
	%
	%
\end{lemma}
\noindent \textbf{Note.} When we invoke \cref{lemma_parameter_single_external_field}, we ensure that $c\bGM^2  \log p \cdot \exp(-e^{-c'\bGM}\stwonorm{\ExternalFieldI - \TrueExternalFieldI}^2 )$ is of the same order as $\delta$.\\

\noindent Next, we show that the loss function $\loss^{(i)}$ is Lipschitz (see \cref{sub:proof_lemma_lipschitzness} for the proof).
\begin{lemma}[{\lipschitznesslossfunction}]\label{lemma_lipschitzness}
	Consider any $i \in [n]$. Then, the loss function $\loss^{(i)}$ is Lipschitz with respect to the $\ell_1$ norm $\sonenorm{\cdot}$ and with Lipschitz constant $\xmax \ctwo$, i.e.,
	\begin{align}
	\bigabs{\loss^{(i)}\bigparenth{\tExternalFieldI} - \loss^{(i)}\bigparenth{\ExternalFieldI}} \leq \xmax \ctwo \sonenorm{\tExternalFieldI - \ExternalFieldI} \qtext{for all} \ExternalFieldI, \tExternalFieldI \in \ParameterSet_{\ExternalField}, \label{eq_lipschitz_property}
	\end{align}
	where the constant $\ctwo$ was defined in \cref{eq:constants}.
\end{lemma}

\noindent Given these lemmas, we now proceed with the proof. 

\paragraph{Proof strategy} We want to show that all points $\ExternalFieldI \in \ParameterSet_{\ExternalField}$, that satisfy $\stwonorm{\ExternalFieldI - \TrueExternalFieldI} \geq R(\varepsilon, \delta)$, uniformly satisfy \cref{eq_sufficienct_condition_thm_node} with probability at least $1-\delta$. To do so, we consider the set of points $\ParameterSet_{\ExternalField}^{r} \subset \ParameterSet_{\ExternalField}$ whose distance from $\TrueExternalFieldI$ is at least $r > 0$ in $\ell_2$ norm. Then, using an appropriate covering set of $\ParameterSet_{\ExternalField}^{r}$ and the Lipschitzness of $\loss^{(i)}$, we show that the value of $\loss^{(i)}$ at all points in $\ParameterSet_{\ExternalField}^{r}$ is uniformly $\Omega(r^2)$ larger than the value of $\loss^{(i)}$ at $\TrueExternalFieldI$ with high probability. 
%
Finally, we choose $r$ small enough to make the failure probability smaller than $\delta$.

\paragraph{Arguments for points in the covering set} Consider any $r \geq \varepsilon \ratio$ (where $\ratio$ is defined in \cref{eq_radius_node_thm}) and the set of elements $\ParameterSet_{\ExternalField}^{r} \defn \braces{ \ExternalFieldI \in  \ParameterSet_{\ExternalField}: \stwonorm{\TrueExternalFieldI - \ExternalFieldI} \geq r}$. 
Let $\cU(\ParameterSet_{\ExternalField}^{r}, \varepsilon')$ be the $\varepsilon'$-cover of the  smallest size for the set $\ParameterSet_{\ExternalField}^{r}$ with respect to $\sonenorm{\cdot}$ (see \cref{def_covering_number_metric_entropy}) and let
$\cC(\ParameterSet_{\ExternalField}^{r}, \varepsilon')$ be the $\varepsilon'$-covering number
%
%
where 
\begin{align}
\varepsilon' \defn \frac{2\sqrt{2} \aGM \bGM \xmax^3 r^2}{ \pi e \ctwo[6]}. \label{eq_eps'}
\end{align}
Now, we argue by a union bound that the value of $\loss^{(i)}$ at all points in $\cU(\ParameterSet_{\ExternalField}^{r}, \varepsilon')$ is uniformly $\Omega(r^2)$ larger than $\loss^{(i)}(\TrueExternalFieldI)$ with high probability. 
%
For any $\ExternalFieldI \in \cU(\ParameterSet_{\ExternalField}^{r}, \varepsilon')$, we have
\begin{align}
\stwonorm{\TrueExternalFieldI - \ExternalFieldI} \sgreat{(a)} r,
%
\label{eq_lower_bound_two_norm_theta_diff}
\end{align}
where $(a)$ follows because $\cU(\ParameterSet_{\ExternalField}^{r}, \varepsilon') \subseteq \ParameterSet_{\ExternalField}^{r}$. 
%
Now, applying \cref{lemma_parameter_single_external_field} with $\varepsilon \mapsfrom \varepsilon$ and $\delta \mapsfrom \delta/2\cC(\ParameterSet_{\ExternalField}^{r}, \varepsilon')$, we have
\begin{align}
\loss^{(i)}\bigparenth{\ExternalFieldI} \geq \loss^{(i)}\bigparenth{\TrueExternalFieldI} + \frac{4\sqrt{2}\aGM\bGM \xmax^4}{\pi e \ctwo[5]}\stwonorm{\TrueExternalFieldI - \ExternalFieldI}^2 \sgreat{\cref{eq_lower_bound_two_norm_theta_diff}} \loss^{(i)}\bigparenth{\TrueExternalFieldI} + \frac{4\sqrt{2}\aGM\bGM \xmax^4 r^2}{\pi e \ctwo[5]},
\end{align}
%
%
%
with probability at least $1-\delta/2\cC(\ParameterSet_{\ExternalField}^{r}, \varepsilon') - c\bGM^2  \log p \cdot \exp(-e^{-c'\bGM}\stwonorm{\ExternalFieldI - \TrueExternalFieldI}^2 )$ 
%
whenever
\begin{align}
n \geq \frac{ce^{c'\bGM}p^2}{\varepsilon^4} \Bigparenth{p \log \frac{\cC(\ParameterSet_{\ExternalField}^{r}, \varepsilon') \cdot p}{\delta \varepsilon^2} + \metric_{\ExternalField,n}\bigparenth{ \varepsilon^2}}.
%
\label{eq_n_condition_node_recovery}
\end{align}
%
%
By applying the union bound over $\cU(\ParameterSet_{\ExternalField}^{r}, \varepsilon')$, as long as $n$ satisfies \cref{eq_n_condition_node_recovery},
%
%
%
we have
\begin{align}
\loss^{(i)}\bigparenth{\ExternalFieldI} \geq  \loss^{(i)}\bigparenth{\TrueExternalFieldI} + \frac{4\sqrt{2}\aGM\bGM \xmax^4 r^2}{\pi e \ctwo[5]} \stext{uniformly for every} \ExternalFieldI \in \cU(\ParameterSet_{\ExternalField}^{r}, \varepsilon'), \label{eq_union_bound_covering_set} 
\end{align}
with probability at least $1-\delta/2 - c\bGM^2 \cC(\ParameterSet_{\ExternalField}^{r}, \varepsilon') \log p \cdot \exp(-e^{-c'\bGM}\stwonorm{\ExternalFieldI - \TrueExternalFieldI}^2)$ which can lower bounded by $1-\delta/2 - c\bGM^2 \cC(\ParameterSet_{\ExternalField}^{r}, \varepsilon') \log p \cdot \exp(-e^{-c'\bGM}r^2)$ using \cref{eq_lower_bound_two_norm_theta_diff}.

\paragraph{Arguments for points outside the covering set} Next, we establish the claim \cref{eq_sufficienct_condition_thm_node} for an arbitrary $\tExternalFieldI \in \ParameterSet_{\ExternalField}^{r}$ conditional on the event that \cref{eq_union_bound_covering_set} holds.
Given a fixed $\tExternalFieldI \in \ParameterSet_{\ExternalField}^{r}$, let $\ExternalFieldI$ be (one of) the point(s) in the $\cU(\ParameterSet_{\ExternalField}^{r}, \varepsilon')$ that satisfies $\sonenorm{\ExternalFieldI - \tExternalFieldI} \leq \varepsilon'$ (there exists such a point by \cref{def_covering_number_metric_entropy})
%
Then, the choices \cref{eq_eps'} and \cref{lemma_lipschitzness} put together imply that
\begin{align}
\loss^{(i)}\bigparenth{\tExternalFieldI} \!\geq\! \loss^{(i)}\bigparenth{\ExternalFieldI} \!-\! \xmax \ctwo \sonenorm{\ExternalFieldI \!-\! \tExternalFieldI} \!\! 
%
& \geq \loss^{(i)}\bigparenth{\ExternalFieldI} - \xmax \ctwo  \varepsilon' \\
& \sgreat{\cref{eq_eps'}} \loss^{(i)}\bigparenth{\ExternalFieldI} \!-\!  \frac{2\sqrt{2} \aGM\bGM \xmax^4 r^2}{ \pi e \ctwo[5]}\\ 
%
& \sgreat{\cref{eq_union_bound_covering_set}} \loss^{(i)}\bigparenth{\TrueExternalFieldI} \!+\! \frac{2\sqrt{2} \aGM\bGM \xmax^4 r^2}{\pi e \ctwo[5]}, 
%
\end{align}
%
It remains to bound sample size $n$ and the failure probability $\delta$. 
%

\paragraph{Bounding $n$} 
%
Using $\ParameterSet_{\ExternalField}^{r} \subseteq \ParameterSet_{\ExternalField}$, we find that
\begin{align}
\cC(\ParameterSet_{\ExternalField}^{r}, \varepsilon') \sless{(a)} \cC(\ParameterSet_{\ExternalField}, \varepsilon').
%
\label{eq_bound_covering_number}
\end{align}
%
%
Putting together \cref{eq_eps'} and \cref{eq_bound_covering_number}, the lower bound  \cref{eq_n_condition_node_recovery} can be replaced by
\begin{align}
%
%
%
n \geq \frac{ce^{c'\bGM}p^2}{\varepsilon^4}  \Bigparenth{p \log \frac{p}{\delta \varepsilon^2}+ p\metric_{\ExternalField}\bigparenth{r^2}  + \metric_{\ExternalField,n}\bigparenth{ \varepsilon^2} }.
\end{align}

\paragraph{Bounding $\delta$} To bound the failure probability by $\delta$, it is sufficient to chose $r$ such that
\begin{align}
\delta & \geq \delta/2 + c\bGM^2 \cC(\ParameterSet_{\ExternalField}^{r}, \varepsilon') \log p \cdot \exp(-e^{-c'\bGM}r^2).
%
%
%
\label{eq_failure_prob}
\end{align}
From \cref{eq_bound_covering_number} and \cref{eq_failure_prob}, it is sufficient to chose $r$ such that 
\begin{align}
\delta & \geq \delta/2 + c\bGM^2 \cC(\ParameterSet_{\ExternalField}, \varepsilon') \log p \cdot \exp(-e^{-c'\bGM}r^2).\label{eq_failure_prob_2}
\end{align}
Re-arranging and taking logarithm on both sides of \cref{eq_failure_prob_2} and using \cref{eq_eps'}, we have
%
%
%
\begin{align}
\log \delta \geq c\biggbrackets{\log \bigparenth{\bGM^2 \log p} +  \metric_{\ExternalField}\Bigparenth{\frac{r^2}{ce^{c'\bGM}}} - e^{-c'\bGM} r^2}.
%
\label{eq_failure_prob_log}
\end{align}
Finally, \cref{eq_failure_prob_log} holds whenever
%
%
%
\begin{align}
r \geq ce^{c'\bGM} \sqrt{\log \dfrac{\bGM^2 \log p}{\delta} + \metric_{\ExternalField}(c e^{-c'\bGM})}.
\end{align}
Recalling that the choice of $r$ was such that $r \geq \varepsilon \ratio$ completes the proof.


%
%

%
\newcommand{\expressionfirstderstagetwo}{Expression for first directional derivative}



%
%
%
%
%
%
%
%
%
%
%
%
%
%
%

\subsection{Proof of \cref{lemma_parameter_single_external_field}: \singleparameterseparation}
\label{sub_proof_lemma_parameter_single_external_field}
%
%
%
%
%
Fix any $\varepsilon > 0$, any $\delta \in (0,1)$, and any $i \in [n]$. Consider any direction $\omI \in \Reals^{p}$ along the parameter $\ExternalFieldI$, i.e.,
\begin{align}
\omI = \ExternalFieldI - \TrueExternalFieldI. \label{eq:omega_defn_stage_2}
\end{align}
We denote the first-order and the second-order directional derivatives of the loss function $\loss^{(i)}$ in \cref{eq:loss_function_n} along the direction $\omI$ evaluated at $\ExternalFieldI$ by $\directionalGradientExternalField$ and $\directionalHessianExternalField$, respectively. 
%
Below, we state a lemma (with proof divided across \cref{sub:proof_of_lemma_conc_grad_stage_2} and \cref{sub:proof_of_lemma_lower_bound_sec_der_stage_2}) that provides us a control on $\directionalGradientExternalFieldTrue$ and $\directionalHessianExternalField$. The assumptions of \cref{lemma_parameter_single_external_field} remain in force.

%
%
%
%
%
%
%
%
%
%
%
%

%
%
%
%
%
%
%

%
%
%
%
%
%
%
\newcommand{\conclocalresultname}{Control on first and second directional derivatives}
\newcommand{\concgradstagetwo}{Concentration of first directional derivative}
\newcommand{\anticoncgradstagetwo}{Anti-concentration of second directional derivative}


\begin{lemma}[{\conclocalresultname}]\label{lemma_conc_first_sec_der_stage_two}
	For any fixed $\varepsilon_1, \varepsilon_2 > 0$, $\delta_1 \in (0,1)$, $i \in [n]$, $\ExternalFieldI \in \ParameterSet_{\ExternalField}$ with $\omI$ defined in \cref{eq:omega_defn_stage_2}, we have the following:
	\begin{enumerate}[label=(\alph*)]
		\item\label{item_conc_first_der_stage_two} \textnormal{{\concgradstagetwo}}: We have
		\begin{align}
		\bigabs{\directionalGradientExternalFieldTrue} \leq\varepsilon_1 \sonenorm{\omI} \!+\! \varepsilon_2 \stwonorm{\omI}^2 \qtext{for} \!\! n \geq \dfrac{ce^{c'\bGM}  p^2 \bigparenth{p \log \frac{p}{\delta_1 \varepsilon_1^2} \!+\!  \metric_{\ExternalField,n}\bigparenth{ \varepsilon_1^2}}}{\varepsilon_1^4},
		%
		\end{align}
		with probability at least $1-\delta_1 - O\biggparenth{\bGM^2 \log p \exp\biggparenth{\dfrac{-\varepsilon_2^2\stwonorm{\omI}^2}{e^{c'\bGM}}}}$.
		\item\label{item_conc_sec_der_stage_two} \textnormal{{\anticoncgradstagetwo}}: We have \begin{align}
		\directionalHessianExternalField \geq \frac{32\sqrt{2}\aGM\bGM \xmax^4}{\pi e \ctwo[5]} \stwonorm{\omI}^2,
		\end{align}
		with probability at least $1- O\biggparenth{\bGM^2 \log p \exp\biggparenth{\dfrac{-\stwonorm{\omI}^2}{e^{c'\bGM}}}}$ where $\ctwo$ was defined in \cref{eq:constants}.
	\end{enumerate}
	%
\end{lemma}

\noindent Given this lemma, we now proceed with the proof.
%
%
%
%
Define a function $g : [0,1] \to \Reals^{p}$ as follows
\begin{align}
g(a) = \TrueExternalFieldI + a(\ExternalFieldI - \TrueExternalFieldI).
\end{align}
Notice that $g(0) = \TrueExternalFieldI$ and $g(1) = \ExternalFieldI$ as well as
\begin{align}
\dfrac{d\loss^{(i)}(g(a))}{da} = \tdirectionalGradientExternalField\bigr|_{\tExternalFieldI = g(a)} \qtext{and} \dfrac{d^2\loss^{(i)}(g(a))}{da^2} = \tdirectionalHessianExternalField\bigr|_{\tExternalFieldI = g(a)}. \label{eq_der_mapping_external_field}
\end{align}
By the fundamental theorem of calculus, we have
\begin{align}
\dfrac{d\loss^{(i)}(g(a))}{da} \geq \dfrac{d\loss^{(i)}(g(a))}{da}\bigr|_{a = 0} + a \min_{a \in (0,1)}\dfrac{d^2\loss^{(i)}(g(a))}{da^2}. \label{eq_fundamental_external_field}
\end{align}
Integrating both sides of \cref{eq_fundamental_external_field} with respect to $a$, we obtain
\begin{align}
\loss^{(i)}(g(a)) \!-\! \loss^{(i)}(g(0)) & \geq  a \dfrac{d\loss^{(i)}(g(a))}{da}\bigr|_{a = 0} +  \dfrac{a^2}{2} \min_{a \in (0,1)}\dfrac{d^2\loss^{(i)}(g(a))}{da^2}\\
& \sequal{\cref{eq_der_mapping_external_field}} a\tdirectionalGradientExternalField\bigr|_{\tExternalFieldI = g(0)} \!+\!  \dfrac{a^2}{2} \min_{a \in (0,1)}\tdirectionalHessianExternalField\bigr|_{\tExternalFieldI = g(a)}\\
& \sequal{(a)} a\directionalGradientExternalFieldTrue \!+\!  \dfrac{a^2}{2} \min_{a \in (0,1)}\tdirectionalHessianExternalField\bigr|_{\tExternalFieldI = g(a)}\\
& \sgreat{(b)} - a \bigabs{\directionalGradientExternalFieldTrue} \!+\!  \dfrac{a^2}{2} \min_{a \in (0,1)}\tdirectionalHessianExternalField\bigr|_{\tExternalFieldI = g(a)},
%
\label{eq_taylor_expansion_external_field}
\end{align}
where $(a)$ follows because $g(0) = \TrueExternalFieldI$, and $(b)$ follows by the triangle inequality.
%
Plugging in $a = 1$ in \cref{eq_taylor_expansion_external_field} as well as using $g(0) = \TrueExternalFieldI$ and $g(1) = \ExternalFieldI$, we find that
\begin{align}
\loss^{(i)}(\ExternalFieldI) - \loss^{(i)}(\TrueExternalFieldI) & \geq -  \bigabs{\directionalGradientExternalFieldTrue} +  \dfrac{1}{2} \min_{a \in (0,1)}\tdirectionalHessianExternalField\bigr|_{\tExternalFieldI = g(a)}. \label{eq_loss_separation_single_par_inter}
\end{align}
Now, we use \cref{lemma_conc_first_sec_der_stage_two} with $\varepsilon_1 \mapsfrom 4\sqrt{2}\aGM\bGM \xmax^4 \varepsilon/\pi e \ctwo[5]$, $\varepsilon_2 \mapsfrom 8\sqrt{2}\aGM\bGM \xmax^4/\pi e \ctwo[5]$, and $\delta_1 \mapsfrom \delta$. Therefore, with probability at least $1-\delta - O\biggparenth{\bGM^2 \log p \exp\biggparenth{\dfrac{-\stwonorm{\omI}^2}{e^{c'\bGM}}}}$ and as long as $n \geq O\biggparenth{\dfrac{e^{c'\bGM} \bigparenth{p \log \frac{p}{\delta} +  \metric_{\ExternalField,n}\bigparenth{ \varepsilon^2}}}{\varepsilon^4}}$, we have
\begin{align}
\loss^{(i)}(\ExternalFieldI) \!-\! \loss^{(i)}(\TrueExternalFieldI) & \! \geq \!- \frac{2^{2.5}\aGM\bGM \xmax^4\varepsilon}{\pi e \ctwo[5]} \sonenorm{\omI} \!\!-\! \frac{2^{3.5}\aGM\bGM \xmax^4}{\pi e \ctwo[5]} \stwonorm{\omI}^2 \!+\!  \frac{2^{4.5}\aGM\bGM \xmax^4}{\pi e \ctwo[5]} \stwonorm{\omI}^2 \\
& \!\! = - \frac{2^{2.5}\aGM\bGM \xmax^4\varepsilon}{\pi e \ctwo[5]} \sonenorm{\omI} + \frac{2^{3.5}\aGM\bGM \xmax^4}{\pi e \ctwo[5]} \stwonorm{\omI}^2 \\
& \!\! \sgreat{\cref{eq_radius_node_thm}} -  \frac{2^{2.5}\aGM\bGM \xmax^4\varepsilon \ratio}{\pi e \ctwo[5]} \stwonorm{\omI} + \frac{2^{3.5}\aGM\bGM \xmax^4}{\pi e \ctwo[5]} \stwonorm{\omI}^2 \\
& \!\! \sgreat{(a)} \!-\!  \frac{2^{2.5}\aGM\bGM \xmax^4}{\pi e \ctwo[5]} \stwonorm{\omI}^2 \!+\! \frac{2^{3.5}\aGM\bGM \xmax^4}{\pi e \ctwo[5]} \stwonorm{\omI}^2 \!=\! \frac{2^{2.5}\aGM\bGM \xmax^4}{\pi e \ctwo[5]} \stwonorm{\omI}^2,
\end{align}
where $(a)$ follows because $\stwonorm{\omI} = \stwonorm{\ExternalFieldI - \TrueExternalFieldI} \geq  \varepsilon \ratio$ according to the lemma statement.
%


\subsubsection{Proof of \cref{lemma_conc_first_sec_der_stage_two}\cref{item_conc_first_der_stage_two}: \concgradstagetwo}\label{sub:proof_of_lemma_conc_grad_stage_2}
Fix some $i \in [n]$ and some $\ExternalFieldI \in \ParameterSet_{\ExternalField}$. Let $\omI$ be as defined in \cref{eq:omega_defn_stage_2}. We claim that the first-order directional derivative of $\loss^{(i)}$ defined in \cref{eq:loss_function_n} is given by 
\begin{align}
\directionalGradientExternalField = - \sump \omIt x_t^{(i)} \exp\Bigparenth{-\normalbrackets{\ExternalFieldtI + 2\EstimatedParameterRowttt\tp \svbx_{-t}^{(i)}} x_t^{(i)} - \EstimatedParameterTU[tt] \cx_t^{(i)}}. \label{eq:first_dir_derivative_stage_2}
\end{align}
We provide a proof at the end. For now, we assume the claim and proceed.

We note that the pair $\braces{\rvbx, \rvbz}$ corresponds to a $\tSGM$ (see \cref{def:tau_sgm}) with $\dGM \defn (\aGM, \aGM \bGM, \xmax, \ParameterMatrix)$. To show the concentration, we use \cref{lemma_conditioning_trick} (see \cref{sec_conditioning_trick}) with $\lambda = \frac{1}{4\sqrt{2}\xmax^2}$, decompose $\directionalGradientExternalFieldTrue$ as a sum of  $\numindsets = 1024  \aGM^2 \bGM^2 \xmax^4 \log 4p$, and focus on these  $\numindsets$ terms. 
%
%
%
%
%
%
%
%
%
%
%
%
Consider the $\numindsets$ subsets $\sets \in [p]$ obtained from \cref{lemma_conditioning_trick} with $\lambda = \frac{1}{4\sqrt{2}\xmax^2}$ and define
\begin{align}
\psi_u(\svbx^{(i)}; \omI) \defeq \sum_{t \in \setU} \omIt x_t^{(i)} \exp\Bigparenth{\!-\!\normalbrackets{\TrueExternalFieldtI \!+\! 2\EstimatedParameterRowttt\tp \svbx_{-t}^{(i)}} x_t^{(i)} \!-\! \EstimatedParameterTU[tt] \cx_t^{(i)}} \stext{for every} u \in \numindsets. \label{eq_def_psi}
\end{align}
Now, we decompose $\directionalGradientExternalFieldTrue$ as a sum of the $\numindsets$ terms defined above. More precisely, we have
\begin{align}
\directionalGradientExternalFieldTrue & \sequal{\cref{eq:first_dir_derivative_stage_2}} -\sump \omIt x_t^{(i)} \exp\Bigparenth{-\normalbrackets{\TrueExternalFieldtI + 2\EstimatedParameterRowttt\tp \svbx_{-t}^{(i)}} x_t^{(i)} - \EstimatedParameterTU[tt] \cx_t^{(i)}} \\
& \sequal{(a)} - \frac{1}{\numindsets'} \sum_{u \in [\numindsets]} \sum_{t \in \setU} \omIt x_t^{(i)} \exp\Bigparenth{-\normalbrackets{\TrueExternalFieldtI + 2\EstimatedParameterRowttt\tp \svbx_{-t}^{(i)}} x_t^{(i)} - \EstimatedParameterTU[tt] \cx_t^{(i)}} \\
& \sequal{\cref{eq_def_psi}} - \frac{1}{\numindsets'} \sum_{u \in [\numindsets]} \psi_u(\svbx^{(i)}; \omI), \label{eq_first_order_derivative_expressed_via_psi}
\end{align}
where $(a)$ follows because each $t \in [p]$ appears in exactly $\numindsets' = \lceil \numindsets/32\sqrt{2}\aGM\bGM \xmax^2 \rceil$ of the sets $\sets$ according to \cref{lemma_conditioning_trick}\cref{item:cardinality_independence_set} (with $\lambda = \frac{1}{4\sqrt{2}\xmax^2}$). Now, we focus on the $\numindsets$ terms in \cref{eq_first_order_derivative_expressed_via_psi}.\\

\noindent Consider any $u \in [\numindsets]$. We claim that conditioned on $\svbx_{-\setU}^{(i)}$ and $\svbz^{(i)}$, the expected value of $\psi_u(\svbx^{(i)}; \omI)$ can be upper bounded uniformly across all $u \in [\numindsets]$. We provide a proof at the end.

\newcommand{\upperboundpsi}{Upper bound on expected $\psi_u$}
\begin{lemma}[{\upperboundpsi}]\label{lemma_expected_psi_upper_bound}
	Fix $\varepsilon > 0$, $\delta \in (0,1)$, $i \in [n]$ and $\ExternalFieldI \in \ParameterSet_{\ExternalField}$. Then, with $\omI$ defined in \cref{eq:omega_defn_stage_2}
	%
	and given $\svbz^{(i)}$ and $\svbx_{-\setU}^{(i)}$ for all $u \in [\numindsets]$, 
	%
	we have
	\begin{align}
	\max\limits_{u \in [\numindsets]} \Expectation\Bigbrackets{\psi_u(\svbx^{(i)}; \omI) \bigm\vert \svbx_{-\setU}^{(i)}, \svbz^{(i)}} \leq \varepsilon \sonenorm{\omI} \!\!\qtext{for}\!\! n \geq \dfrac{c e^{c'\bGM} p^2 \bigparenth{p \log \frac{p}{\delta \varepsilon^2} \!+\!  \metric_{\ExternalField,n}\bigparenth{ \varepsilon^2}}}{\varepsilon^4},
	\end{align}
	with probability at least $1-\delta$.
	%
\end{lemma}

\noindent Consider again any $u \in [\numindsets]$. Now, we claim that conditioned on $\svbx_{-\setU}^{(i)}$ and $\svbz^{(i)}$, $\psi_u(\svbx^{(i)}; \omI)$ concentrates around its conditional expected value. We provide a proof at the end.
%
%
%
%

\newcommand{\concpsi}{Concentration of $\psi_u$}

%
%
%
%
\begin{lemma}[{\concpsi}]\label{lemma_concentration_psi}
	Fix $\varepsilon > 0$, $i \in [n]$, $u \in [\numindsets]$, and $\ExternalFieldI \in \ParameterSet_{\ExternalField}$. Then, with $\omI$ defined in \cref{eq:omega_defn_stage_2} and given $\svbz^{(i)}$ and $\svbx_{-\setU}^{(i)}$, we have
	\begin{align}
	\Bigabs{\psi_u(\svbx^{(i)}; \omI) - \Expectation\bigbrackets{\psi_u(\svbx^{(i)}; \omI) \bigm\vert \svbx_{-\setU}^{(i)}, \svbz^{(i)}}} \leq \varepsilon,
	\end{align}
	with probability at least $1-\exp\biggparenth{ \dfrac{- \varepsilon^2}{e^{c'\bGM}\stwonorm{\omI}^2}}$. 
	%
	%
	%
	%
	%
	%
	%
	%
	%
\end{lemma}

\noindent Given these lemmas, we proceed to show the concentration of $\directionalGradientExternalFieldTrue$.
%
To that end, for any $u \in [\numindsets]$, given $\svbx_{-\setU}^{(i)}$ and $\svbz^{(i)}$, let $E_u$ denote the event that
\begin{align}
\psi_u(\svbx^{(i)}; \omI) \leq   \Expectation\bigbrackets{\psi_u(\svbx^{(i)}; \omI) \vert \svbx_{-\setU}^{(i)}, \svbz^{(i)}} + \frac{1}{32\sqrt{2}\aGM\bGM \xmax^2} \varepsilon_2 \stwonorm{\omI}^2. \label{eq_event_Ej}
\end{align}
Since $E_u$ in an indicator event, using the law of total expectation results in
\begin{align}
\Probability(E_u) = \Expectation\Bigbrackets{\Probability(E_u | \svbx_{-\setU}^{(i)}, \svbz^{(i)})} \sgreat{(a)} 1 - \exp\biggparenth{ \dfrac{- \varepsilon_2^2\stwonorm{\omI}^2}{e^{c'\bGM}}}.
\end{align}
where $(a)$ follows from \cref{lemma_concentration_psi} with $\varepsilon \mapsfrom \dfrac{\varepsilon_2\stwonorm{\omI}^2}{32\sqrt{2}\aGM\bGM \xmax^2}$. Now, by applying the union bound over all $u \in [\numindsets]$ where $\numindsets = 1024  \aGM^2 \bGM^2 \xmax^4 \log 4p$, we have
\begin{align}
%
\Probability\Bigparenth{\bigcap_{u \in \numindsets} E_u} \geq 1 - O\biggparenth{\bGM^2 \log p \exp\biggparenth{\dfrac{-\varepsilon_2^2\stwonorm{\omI}^2}{e^{c'\bGM}}}}.
\end{align}
Now, assume the event $\cap_{u \in \numindsets} E_u$ holds. Whenever this holds, we also have
\begin{align}
\bigabs{\directionalGradientExternalFieldTrue} & \sless{\cref{eq_first_order_derivative_expressed_via_psi}} \frac{1}{\numindsets'} \sum_{u \in [\numindsets]} \bigabs{\psi_u(\svbx^{(i)}; \omI)}\\ & \sless{\cref{eq_event_Ej}} \frac{1}{\numindsets'} \sum_{u \in [\numindsets]} \Bigabs{\Expectation\bigbrackets{\psi_u(\svbx^{(i)}; \omI) \vert \svbx_{-\setU}^{(i)}, \svbz^{(i)}} + \frac{1}{32\sqrt{2}\aGM\bGM \xmax^2} \varepsilon_2 \stwonorm{\omI}^2}, \label{eq_grad_intermediate_bound}
\end{align}
where $\numindsets' = \lceil \numindsets/32\sqrt{2}\aGM\bGM \xmax^2 \rceil$. Further, using \cref{lemma_expected_psi_upper_bound} in \cref{eq_grad_intermediate_bound} with $\varepsilon \mapsfrom \dfrac{\varepsilon_1}{32\sqrt{2}\aGM\bGM \xmax^2}$ and $\delta \mapsfrom \delta_1$, whenever
\begin{align}
%
n \geq \dfrac{ce^{c'\bGM} \cdot p^2 \bigparenth{p \log \frac{p}{\delta_1 \varepsilon_1^2} +  \metric_{\ExternalField,n}\bigparenth{ \varepsilon_1^2}}}{\varepsilon_1^4},
\end{align}
with probability at least $1-\delta_1$, we have,
\begin{align}
\bigabs{\directionalGradientExternalFieldTrue} & \leq \frac{1}{\numindsets'} \sum_{u \in [\numindsets]} \Bigparenth{\frac{1}{32\sqrt{2}\aGM\bGM \xmax^2} \varepsilon_1 \sonenorm{\omI} + \frac{1}{32\sqrt{2}\aGM\bGM \xmax^2} \varepsilon_2 \stwonorm{\omI}^2} \\
& = \frac{\numindsets}{32 \sqrt{2}\aGM\bGM \xmax^2 \numindsets'} \Bigparenth{\varepsilon_1 \sonenorm{\omI} \!+\! \varepsilon_2 \stwonorm{\omI}^2} \sless{(a)} \varepsilon_1 \sonenorm{\omI} \!+\! \varepsilon_2 \stwonorm{\omI}^2,
\end{align}
where $(a)$ follows because $\numindsets' = \lceil \numindsets/32 \sqrt{2}\aGM\bGM \xmax^2 \rceil$.

\paragraph{Proof of \cref{eq:first_dir_derivative_stage_2}: \expressionfirstderstagetwo}
Fix any $i \in [n]$. The first-order partial derivatives of $\loss^{(i)}$ (defined in \cref{eq:loss_function_n}) with respect to the entries of the parameter vector $\ExternalFieldI$ are given by
\begin{align}
\frac{\partial \loss^{(i)}(\ExternalFieldI)}{\partial \ExternalFieldIt} & = -x_t^{(i)} \exp\Bigparenth{-\normalbrackets{\ExternalFieldtI + 2\EstimatedParameterRowttt\tp \svbx_{-t}^{(i)}} x_t^{(i)} - \EstimatedParameterTU[tt] \cx_t^{(i)}} \qtext{for all} t \in [p].
\end{align}
Now, we can write the first-order directional derivative of $\loss^{(i)}$ as
\begin{align}
\directionalGradientExternalField &\defeq\lim_{h\to 0}\frac{\loss^{(i)}(\ExternalFieldI + h \omI)-\loss^{(i)}(\ExternalFieldI)}{h} = \sump \omIt \frac{\partial \loss^{(i)}(\ExternalFieldI)}{\partial \ExternalFieldIt} \\
& = - \sump \omIt x_t^{(i)} \exp\Bigparenth{-\normalbrackets{\ExternalFieldtI + 2\EstimatedParameterRowttt\tp \svbx_{-t}^{(i)}} x_t^{(i)} - \EstimatedParameterTU[tt] \cx_t^{(i)}}.
\end{align}




\paragraph{Proof of \cref{lemma_expected_psi_upper_bound}: \upperboundpsi}
\label{sub:proof_of_lemma_expected_psi_upper_bound}
Fix any $i \in [n]$, $u \in [\numindsets]$, and $\ExternalFieldI \in \ParameterSet_{\ExternalField}$. Then, given $\svbx_{-\setU}^{(i)}$ and $\svbz^{(i)}$, we have
\begin{align}
& \Expectation\biggbrackets{\psi_u(\svbx^{(i)}; \omI) \bigm\vert \svbx_{-\setU}^{(i)}, \svbz^{(i)}} \\
&  \sequal{(a)} \Expectation\Bigbrackets{\sum_{t \in \setU} \omIt x_t^{(i)} \exp\Bigparenth{-\normalbrackets{\TrueExternalFieldtI + 2\EstimatedParameterRowttt\tp \svbx_{-t}^{(i)}} x_t^{(i)} - \EstimatedParameterTU[tt] \cx_t^{(i)}} \Bigm\vert \svbx_{-\setU}^{(i)}, \svbz^{(i)}} \\
& \sequal{(b)} \sum_{t \in \setU} \omIt \Expectation\Bigbrackets{x_t^{(i)} \exp\Bigparenth{-\normalbrackets{\TrueExternalFieldtI + 2\EstimatedParameterRowttt\tp \svbx_{-t}^{(i)}} x_t^{(i)} - \EstimatedParameterTU[tt] \cx_t^{(i)}} \Bigm\vert \svbx_{-\setU}^{(i)}, \svbz^{(i)}} \\
& \sequal{(c)} \sum_{t \in \setU} \omIt \Expectation\biggbrackets{\Expectation\Bigbrackets{x_t^{(i)} \!\exp\!\bigparenth{\!-\!\normalbrackets{\TrueExternalFieldtI \!\!+\! 2\EstimatedParameterRowttt\tp \svbx_{-t}^{(i)}} x_t^{(i)} \!\!-\! \EstimatedParameterTU[tt] \cx_t^{(i)}} \!\Bigm\vert \! \svbx_{-t}^{(i)}, \svbz^{(i)}\!} \! \biggm\vert \svbx_{-\setU}^{(i)}, \svbz^{(i)}},
%
\label{eq_conditional_expectation_psi_intermediate}
\end{align}
where $(a)$ follows from the definition of $\psi_u(\svbx^{(i)}; \omI)$ in \cref{eq_def_psi}, $(b)$ follows from linearity of expectation, and $(c)$ follows from the law of total expectation, i.e., $\Expectation[\Expectation[Y|X,Z]|Z] = \Expectation[Y|Z]$ since $\svbx_{-\setU}^{(i)} \subseteq \svbx_{-t}^{(i)}$. Now, we bound $\Expectation\Bigbrackets{x_t^{(i)} \!\exp\!\bigparenth{\!\!-\!\normalbrackets{\TrueExternalFieldtI \!\!\!+\! 2\EstimatedParameterRowttt\tp \svbx_{-t}^{(i)}} x_t^{(i)} \!\!\!-\! \EstimatedParameterTU[tt] \cx_t^{(i)}} \!\bigm\vert \! \svbx_{-t}^{(i)}, \svbz^{(i)}\!} \!$ for every $t \in \setU$. We have
\begin{align}
&  \Expectation\Bigbrackets{x_t^{(i)} \!\exp\!\bigparenth{\!-\!\normalbrackets{\TrueExternalFieldtI \!\!+\! 2\EstimatedParameterRowttt\tp \svbx_{-t}^{(i)}} x_t^{(i)} \!\!-\! \EstimatedParameterTU[tt] \cx_t^{(i)}} \!\Bigm\vert \! \svbx_{-t}^{(i)}, \svbz^{(i)}\!} \\
& \!=\!\! \int\limits_{\cX} \!\! x_t^{(i)} \!\!\exp\!\bigparenth{\!\!-\!\normalbrackets{\TrueExternalFieldtI \!\!\!+\! 2\EstimatedParameterRowttt\tp \svbx_{\!-t}^{(i)}} x_t^{(i)} \!\!\!-\! \EstimatedParameterTU[tt] \cx_t^{(i)}}  f_{\rvx_t|\rvbx_{\!-t}, \rvbz}\bigparenth{x_t^{(i)} | \svbx_{\!-t}^{(i)}, \svbz^{(i)}\!;  \TrueExternalFieldt(\!\svbz^{(i)}\!), \TrueParameterRowt} d x_t^{(i)}\\
%
& \!\sequal{(a)}\! \frac{\int_{\cX} x_t^{(i)}  \exp\bigparenth{2\normalbrackets{\TrueParameterRowttt - \EstimatedParameterRowttt}\tp \svbx_{-t}^{(i)} x_t^{(i)} + \normalbrackets{\TrueParameterTU[tt] - \EstimatedParameterTU[tt]} \cx_t^{(i)}} dx_t^{(i)}} {\int_{\cX} \exp\Bigparenth{\normalbrackets{\TrueExternalFieldt(\svbz^{(i)}) + 2\TrueParameterRowtttTop \svbx_{-t}^{(i)}} x_t^{(i)} + \TrueParameterTU[tt] \cx_t^{(i)}}d x_t^{(i)}}\\
& \!\sequal{(b)}\! \frac{\int_{\cX} x_t^{(i)} \Bigbrackets{ 1 + 2\normalbrackets{\TrueParameterRowttt \!-\! \EstimatedParameterRowttt}\tp \svbx_{-t}^{(i)} x_t^{(i)} + \normalbrackets{\TrueParameterTU[tt] \!-\! \EstimatedParameterTU[tt]} \cx_t^{(i)}} dx_t^{(i)}} {\int_{\cX} \exp\Bigparenth{\normalbrackets{\TrueExternalFieldt(\svbz^{(i)}) + 2\TrueParameterRowtttTop \svbx_{-t}^{(i)}} x_t^{(i)} + \TrueParameterTU[tt] \cx_t^{(i)}}d x_t^{(i)}}\\
& \qquad \qquad \qquad \qquad  + \frac{\int_{\cX} x_t^{(i)} \Bigbrackets{o\Bigparenth{\normalbrackets{\TrueParameterRowttt \!-\! \EstimatedParameterRowttt}\tp \svbx_{-t}^{(i)} x_t^{(i)} + \normalbrackets{\TrueParameterTU[tt] \!-\! \EstimatedParameterTU[tt]} \cx_t^{(i)}}^2} dx_t^{(i)}} {\int_{\cX} \exp\Bigparenth{\normalbrackets{\TrueExternalFieldt(\svbz^{(i)}) + 2\TrueParameterRowtttTop \svbx_{-t}^{(i)}} x_t^{(i)} + \TrueParameterTU[tt] \cx_t^{(i)}}d x_t^{(i)}}\\
%
& \!\sequal{(c)}\! \frac{ 4\xmax^3 \normalbrackets{\TrueParameterRowttt - \EstimatedParameterRowttt}\tp \svbx_{-t}^{(i)} }{3\int_{\cX} \exp\Bigparenth{\normalbrackets{\TrueExternalFieldt(\svbz^{(i)}) + 2\TrueParameterRowtttTop \svbx_{-t}^{(i)}} x_t^{(i)} + \TrueParameterTU[tt] \cx_t^{(i)}}d x_t^{(i)}} \\
& \qquad \qquad \qquad  \qquad \!+\! \frac{ \xmax^5 \bigparenth{\normalbrackets{\TrueParameterRowttt - \EstimatedParameterRowttt}\tp \svbx_{-t}^{(i)}} \bigparenth{\TrueParameterTU[tt] \!-\! \EstimatedParameterTU[tt]} o(1) }{\int_{\cX} \exp\Bigparenth{\normalbrackets{\TrueExternalFieldt(\svbz^{(i)}) + 2\TrueParameterRowtttTop \svbx_{-t}^{(i)}} x_t^{(i)} + \TrueParameterTU[tt] \cx_t^{(i)}}d x_t^{(i)}}, \label{eq_taylor_series}
%
%
%
%
\end{align}
where $(a)$ follows from \cref{eq_conditional_dist} and $\TrueExternalFieldI = \TrueExternalField(\svbz^{(i)}) ~\forall i \in [n]$, $(b)$ follows by using the Taylor series expansion $\exp(y) = 1 + y + o(y^2)$ around zero, $(c)$ follows because $\int_{\cX} x_t^{(i)} dx_t^{(i)} = \int_{\cX} x_t^{(i)} \cx_t^{(i)} d x_t^{(i)} = \int_{\cX} \bigparenth{ x_t^{(i)}}^3 d x_t^{(i)} = \int_{\cX} x_t^{(i)} \bigparenth{\cx_t^{(i)}}^2 d x_t^{(i)} = 0$, $\int_{\cX} \bigparenth{ x_t^{(i)}}^2 d x_t^{(i)} = 2\xmax^3/3$, and $\int_{\cX} \bigparenth{ x_t^{(i)}}^2 \cx^{(i)}_t d x_t^{(i)} = 8\xmax^5/45$. 


Now, we bound the numerators in \cref{eq_taylor_series} by using $\sonenorm{\TrueParameterRowt - \EstimatedParameterRowt} \leq \sqrt{2\bGM} \stwonorm{\TrueParameterRowt - \EstimatedParameterRowt}$ which follows because $\TrueParameterMatrix \in \ParameterSet_{\ParameterMatrix}$ and $\EstimatedParameterMatrix \in \ParameterSet_{\ParameterMatrix}$. Then, we invoke \cref{theorem_parameters} to bound $\stwonorm{\TrueParameterRowt - \EstimatedParameterRowt}$ by $\varepsilon \mapsfrom \frac{3\varepsilon}{2\sqrt{2\bGM} \ctwo \xmax^3}$. Therefore, we subsume the second term by the first term resulting in the following bound:
\begin{align}
\Expectation\Bigbrackets{x_t^{(i)} \!\exp\!\bigparenth{\!\!-\!\normalbrackets{\TrueExternalFieldtI \!\!\!+\! 2\EstimatedParameterRowttt\tp \svbx_{-t}^{(i)}} x_t^{(i)} \!\!\!-\! \EstimatedParameterTU[tt] \cx_t^{(i)}} \!\bigm\vert \! \svbx_{-t}^{(i)}, \svbz^{(i)}\!} \! \leq \! \frac{2^{\frac{3}{2}}\bGM^{\frac{1}{2}} \ctwo  \xmax^3 \stwonorm{\TrueParameterRowt \!-\! \EstimatedParameterRowt}}{3}\!, \label{eq_expected_psi_bound_two_norm}
\end{align}
where we have used the triangle inequality, $\sinfnorm{\svbx^{(i)}} \leq \xmax $ for all $ i \in [n]$ as well as $\sonenorm{\TrueParameterRowt - \EstimatedParameterRowt} \leq \sqrt{2\bGM} \stwonorm{\TrueParameterRowt - \EstimatedParameterRowt}$ to upper bound the numerator, and the arguments used in the proof of \cref{prop_lower_bound_variance} as well as $\int_{\cX} dx_t^{(i)} = 2\xmax$ to lower bound the denominator.
%
%
%
%
%
%
%
%
%
%
%
%
%
%

\noindent Using \cref{theorem_parameters} in \cref{eq_expected_psi_bound_two_norm} with $\varepsilon \mapsfrom \dfrac{3\varepsilon}{2\sqrt{2\bGM} \ctwo \xmax^3}$ and $\delta \mapsfrom \delta$, we have
\begin{align}
\Expectation\Bigbrackets{x_t^{(i)} \exp\Bigparenth{-\normalbrackets{\TrueExternalFieldtI + 2\EstimatedParameterRowttt\tp \svbx_{-t}^{(i)}} x_t^{(i)} - \EstimatedParameterTU[tt] \cx_t^{(i)}} \Bigm\vert \svbx_{-t}^{(i)}, \svbz^{(i)}} \leq \varepsilon, \label{eq_upper_bound_intermediate}
\end{align}
with probability at least $1-\delta$ as long as
\begin{align}
%
n \geq \dfrac{ce^{c'\bGM} \cdot p^2 \bigparenth{p \log \frac{p}{\delta \varepsilon^2} +  \metric_{\ExternalField,n}\bigparenth{ \varepsilon^2}}}{\varepsilon^4}. \label{eq_n_bound_stability}
\end{align}
Using \cref{eq_upper_bound_intermediate} and triangle inequality in \cref{eq_conditional_expectation_psi_intermediate}, we have
\begin{align}
\Expectation\Bigbrackets{\psi_u(\svbx^{(i)}; \omI)\bigm\vert \svbx_{-\setU}^{(i)}, \svbz^{(i)}} & \leq \varepsilon \sum_{t \in \setU} \bigabs{\omIt} \leq \varepsilon \sonenorm{\omI},
\end{align}
with probability at least $1-\delta$ as long as $n$ satisfies \cref{eq_n_bound_stability}.

\paragraph{Proof of \cref{lemma_concentration_psi}: \concpsi}
\label{sub:proof_of_lemma_concentration_psi}
To show this concentration result, we use \cref{coro}~\cref{eq_coro_combined} for the function $q_2$. To that end, we note that the pair $\braces{\rvbx, \rvbz}$ corresponds to a $\tSGM$ (\cref{def:tau_sgm}) with $\dGM \defn (\aGM, \aGM \bGM, \xmax, \ParameterMatrix)$. However, the random vector $\rvbx$ conditioned on $\rvbz$ need not satisfy the Dobrushin's uniqueness condition (\cref{def_dobrushin_condition}). Therefore, we cannot apply \cref{coro}~\cref{eq_coro_combined} as is. To resolve this, we resort to \cref{lemma_conditioning_trick} with $\lambda = \frac{1}{4\sqrt{2}\xmax^2}$ to reduce the random vector $\rvbx$ conditioned on $\rvbz$ to Dobrushin's regime.

Fix any $u \in [\numindsets]$. Then, from \cref{lemma_conditioning_trick}\cref{item:conditional_sgm_independence_set}, (i) the pair of random vectors $\braces{\rvbx_{\setU}, (\rvbx_{-\setU}, \rvbz)}$ corresponds to a $\tSGM[1]$ with $\dGM_1 \defn (\aGM+2\aGM\bGM\xmax, \frac{1}{4\sqrt{2}\xmax^2}, \xmax, \ParameterMatrix_{\setU})$, and (ii) the random vector $\rvbx_{\setU}$ conditioned on $(\rvbx_{-\setU}, \rvbz)$ satisfies the Dobrushin's uniqueness condition (\cref{def_dobrushin_condition}) with coupling matrix $2\sqrt{2} \xmax^2 \normalabs{\ParameterMatrix_{\setU}}$ with $2\sqrt{2} \xmax^2 \opnorm{\normalabs{\ParameterMatrix_{\setU}}} \leq 2\sqrt{2} \xmax^2 \lambda \leq 1/2$. Now, for any fixed $i \in [n]$, 
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
we apply \cref{coro}~\cref{eq_coro_combined} for the function $q_2$ with $\varepsilon \mapsfrom \varepsilon$ for a given $\svbx_{-\setU}^{(i)}$ and $\svbz^{(i)}$, to obtain
\begin{align}
\Probability\biggparenth{\Bigabs{\psi_u(\svbx^{(i)}; \omI) - \Expectation\Bigbrackets{\psi_u(\svbx^{(i)}; \omI) \Bigm\vert \svbx_{-\setU}^{(i)}, \rvbz}} \geq \varepsilon \Bigm\vert \svbx_{-\setU}^{(i)}, \rvbz} \leq \exp\biggparenth{ \dfrac{- \varepsilon^2}{e^{c'\bGM}\stwonorm{\omI}^2}}.
\end{align}

\subsubsection{Proof of \cref{lemma_conc_first_sec_der_stage_two}\cref{item_conc_sec_der_stage_two}: \anticoncgradstagetwo}
\label{sub:proof_of_lemma_lower_bound_sec_der_stage_2}

\newcommand{\expressionsecderstagetwo}{Expression for second directional derivative}
Fix some $i \in [n]$ and some $\ExternalFieldI \in \ParameterSet_{\ExternalField}$. Let $\omI$ be as defined in \cref{eq:omega_defn_stage_2}. We claim that the second-order directional derivative of $\loss^{(i)}$ defined in \cref{eq:loss_function_n} is given by 
\begin{align}
\directionalHessianExternalField = \sump \bigparenth{\omIt x_t^{(i)}}^2  \exp\Bigparenth{-\normalbrackets{\ExternalFieldtI + 2\EstimatedParameterRowttt\tp \svbx_{-t}^{(i)}} x_t^{(i)} - \EstimatedParameterTU[tt] \cx_t^{(i)}}. \label{eq:second_dir_derivative_stage_2}
\end{align}
We provide a proof at the end. For now, we assume the claim and proceed.
%
%
%
%
%
%
%
%
%
%
Now, we lower bound $\directionalHessianExternalField$ by a quadratic form as follows
\begin{align}
\directionalHessianExternalField 
%
%
& \! \sgreat{(a)} \sump   \bigparenth{\omIt x_t^{(i)}}^2 \times \exp\Bigparenth{\!-\!\bigparenth{\normalabs{\ExternalFieldtI} \!+\! 2\sonenorm{\EstimatedParameterRowt} \sinfnorm{\svbx^{(i)}}} \xmax}\\
&  \! \sgreat{(b)} \sump  \bigparenth{\omIt x_t^{(i)}}^2 \times \exp\Bigparenth{\!-\!\normalparenth{\aGM \!+\! 2\aGM \bGM \xmax} \xmax} \sequal{\cref{eq:constants}} \frac{1}{\ctwo}\!\! \sump  \! \bigparenth{\omIt x_t^{(i)}}^2 \label{eq_lower_bound_sec_der_stage_2_cont}\!,
%
%
\end{align}
where $(a)$ follows from \cref{eq:second_dir_derivative_stage_2} by triangle inequality, Cauchyâ€“Schwarz inequality, and because $\sinfnorm{\svbx^{(i)}} \leq \xmax $ for all $ i \in [n]$, and $(b)$ follows because $\EstimatedParameterMatrix \in \ParameterSet_{\ParameterMatrix}$, $\ExternalFieldI \in \ParameterSet_{\ExternalField}$, and $\sinfnorm{\svbx^{(i)}} \leq \xmax $ for all $ i \in [n]$.\\
%


\noindent Now, to show the anti-concentration of $\directionalHessianExternalField$, we show the anti-concentration of the quadratic form in \cref{eq_lower_bound_sec_der_stage_2_cont}. To that end, we note that the pair $\braces{\rvbx, \rvbz}$ corresponds to a $\tSGM$ (\cref{def:tau_sgm}) with $\dGM \defn (\aGM, \aGM \bGM, \xmax, \ParameterMatrix)$. Then, we decompose the quadratic form in \cref{eq_lower_bound_sec_der_stage_2_cont} as a sum of  $\numindsets = 1024  \aGM^2 \bGM^2 \xmax^4 \log 4p$ terms using \cref{lemma_conditioning_trick} (see \cref{sec_conditioning_trick}) with $\lambda = \frac{1}{4\sqrt{2}\xmax^2}$ and focus on these  $\numindsets$ terms. Consider the $\numindsets$ subsets $\sets \in [p]$ obtained from \cref{lemma_conditioning_trick} and define
\begin{align}
\bpsi_u(\svbx^{(i)}; \omI) \defeq \sum_{t \in \setU} \bigparenth{\omIt x_t^{(i)}}^2 \qtext{for every} u \in \numindsets. \label{eq_def_bar_psi}
\end{align}
Then, we have
\begin{align}
\sump \bigparenth{\omIt x_t^{(i)}}^2 & \sequal{(a)} \frac{1}{\numindsets'} \sum_{u \in [\numindsets]} \sum_{t \in \setU} \bigparenth{\omIt x_t^{(i)}}^2 \sequal{\cref{eq_def_bar_psi}}  \frac{1}{\numindsets'} \sum_{u \in [\numindsets]} \bpsi_u(\svbx^{(i)}; \omI), \label{eq_second_order_derivative_expressed_via_psi}
\end{align}
where $(a)$ follows because each $t \in [p]$ appears in exactly $\numindsets' = \lceil \numindsets/32\sqrt{2}\aGM\bGM \xmax^2 \rceil$ of the sets $\sets$ according to \cref{lemma_conditioning_trick}\cref{item:cardinality_independence_set} (with $\lambda = \frac{1}{4\sqrt{2}\xmax^2}$). Now, we focus on the $\numindsets$ terms in \cref{eq_second_order_derivative_expressed_via_psi}.\\

\noindent Consider any $u \in [\numindsets]$. We claim that conditioned on $\svbx_{-\setU}^{(i)}$ and $\svbz^{(i)}$, the expected value of $\bpsi_u(\svbx^{(i)}; \omI)$ can be upper bounded uniformly across all $u \in [\numindsets]$. We provide a proof at the end.

%


\newcommand{\lowerboundbarpsi}{Lower bound on expected $\bpsi_u$}

\begin{lemma}[{\lowerboundbarpsi}]\label{lemma_expected_psi_lower_bound}
	Fix $i \in [n]$ and $\ExternalFieldI \in \ParameterSet_{\ExternalField}$. Then, with $\omI$ defined in \cref{eq:omega_defn_stage_2} and given $\svbz^{(i)}$ and $\svbx_{-\setU}^{(i)}$, we have
	%
	%
	%
	%
	%
	\begin{align}
	\min\limits_{u \in [\numindsets]} \Expectation\Bigbrackets{\bpsi_u(\svbx^{(i)}; \omI) \bigm\vert \svbx_{-\setU}^{(i)}, \svbz^{(i)}} \geq \frac{2\xmax^2}{\pi e \ctwo[4]} \stwonorm{\omI}^2,
	\end{align}
	where the constant $\ctwo$ was defined in \cref{eq:constants}.
\end{lemma}

\noindent Consider again any $u \in [\numindsets]$. Now, we claim that conditioned on $\svbx_{-\setU}^{(i)}$ and $\svbz^{(i)}$, $\bpsi_u(\svbx^{(i)}; \omI)$ concentrates around its conditional expected value. We provide a proof at the end.
%
%
%
%
%

\newcommand{\concbarpsi}{Concentration of $\bpsi_u$}

\begin{lemma}[{\concbarpsi}]\label{lemma_concentration_bar_psi}
	Fix $\varepsilon > 0$, $i \in [n]$, $u \in [\numindsets]$, and $\ExternalFieldI \in \ParameterSet_{\ExternalField}$.
	Then, with $\omI$ defined in \cref{eq:omega_defn_stage_2} and given $\svbz^{(i)}$ and $\svbx_{-\setU}^{(i)}$, we have
	\begin{align}
	\Bigabs{\bpsi_u(\svbx^{(i)}; \omI) - \Expectation\bigbrackets{\bpsi_u(\svbx^{(i)}; \omI) \bigm\vert \svbx_{-\setU}^{(i)}, \svbz^{(i)}}} \leq \varepsilon,
	\end{align}
	with probability at least $1-\exp\biggparenth{ \dfrac{- \varepsilon^2}{e^{c'\bGM}\stwonorm{\omI}^2}}$. 
	%
\end{lemma}

\noindent Given these lemmas, we proceed to show the anti-concentration of the quadratic form in  \cref{eq_lower_bound_sec_der_stage_2_cont} implying the anti-concentration of $\directionalHessianExternalField$. To that end, for any $u \in [\numindsets]$, given $\svbx_{-\setU}^{(i)}$ and $\svbz^{(i)}$, let $E_u$ denote the event that
\begin{align}
\bpsi_u(\svbx^{(i)}; \omI) \geq   \Expectation\bigbrackets{\bpsi_u(\svbx^{(i)}; \omI) \vert \svbx_{-\setU}^{(i)}, \svbz^{(i)}} - \frac{\xmax^2}{\pi e \ctwo[4]} \stwonorm{\omI}^2. \label{eq_event_Ej_Hess}
\end{align}
Since $E_u$ in an indicator event, using the law of total expectation results in
\begin{align}
\Probability(E_u) = \Expectation\Bigbrackets{\Probability(E_u | \svbx_{-\setU}^{(i)}, \svbz^{(i)})} \sgreat{(a)} 1 - \exp\biggparenth{ \dfrac{ \stwonorm{\omI}^2}{e^{c'\bGM}}},
\end{align}
where $(a)$ follows from \cref{lemma_concentration_bar_psi} with $\varepsilon \mapsfrom \dfrac{\xmax^2}{\pi e \ctwo[4]} \stwonorm{\omI}^2$. Now, by applying the union bound over all $u \in [\numindsets]$ where $\numindsets = 1024  \aGM^2 \bGM^2 \xmax^4 \log 4p$, we have
\begin{align}
%
\Probability\Bigparenth{\bigcap_{u \in \numindsets} E_u} \geq 1 - O\biggparenth{\bGM^2 \log p \exp\biggparenth{\dfrac{\stwonorm{\omI}^2}{e^{c'\bGM}}}}.
\end{align}
Now, assume the event $\cap_{u \in \numindsets} E_u$ holds. Whenever this holds, we also have
\begin{align}
\sump \bigparenth{\omIt x_t^{(i)}}^2  & \sequal{\cref{eq_second_order_derivative_expressed_via_psi}}  \frac{1}{\numindsets'} \sum_{u \in [\numindsets]} \bpsi_u(\svbx^{(i)}; \omI) \\
& \sgreat{\cref{eq_event_Ej_Hess}} \frac{1}{\numindsets'} \sum_{u \in [\numindsets]}  \biggparenth{\Expectation\bigbrackets{\bpsi_u(\svbx^{(i)}; \omI) \vert \svbx_{-\setU}^{(i)}, \svbz^{(i)}} - \frac{\xmax^2}{\pi e \ctwo[4]} \stwonorm{\omI}^2} \\
& \sgreat{(a)} \frac{1}{\numindsets'} \sum_{u \in [\numindsets]} \frac{\xmax^2}{\pi e \ctwo[4]} \stwonorm{\omI}^2 = \frac{\xmax^2\numindsets}{\pi e \numindsets'\ctwo[4]} \stwonorm{\omI}^2, \label{eq_hess_intermediate_bound}
\end{align}
where $\numindsets' = \lceil \numindsets/32\sqrt{2}\aGM\bGM \xmax^2 \rceil$ and $(a)$ follows from \cref{lemma_expected_psi_lower_bound}. Finally, approximating $\numindsets' = \numindsets/32\sqrt{2}\aGM\bGM \xmax^2$ and using \cref{eq_lower_bound_sec_der_stage_2_cont}, we have
\begin{align}
\directionalHessianExternalField \geq \frac{1}{\ctwo} \sump  \bigparenth{\omIt x_t^{(i)}}^2  \sgreat{\cref{eq_hess_intermediate_bound}} \frac{32\sqrt{2}\aGM\bGM \xmax^4}{\pi e \ctwo[5]} \stwonorm{\omI}^2,
\end{align}
which completes the proof.


\paragraph{Proof of \cref{eq:second_dir_derivative_stage_2}: \expressionsecderstagetwo}
Fix any $i \in [n]$. The second-order partial derivatives of $\loss^{(i)}$ (defined in \cref{eq:loss_function_n}) with respect to the entries of the parameter vector $\ExternalFieldI$ are given by
\begin{align}
\frac{\partial^2 \loss^{(i)}(\ExternalFieldI)}{\partial \bigbrackets{\ExternalFieldIt}^2}
& = \bigbrackets{x_t^{(i)}}^2 \exp\Bigparenth{-\normalbrackets{\ExternalFieldtI + 2\EstimatedParameterRowttt\tp \svbx_{-t}^{(i)}} x_t^{(i)} - \EstimatedParameterTU[tt] \cx_t^{(i)}}
\qtext{for all} t \in [p].
\end{align}
Now, we can write the second-order directional derivative of $\loss^{(i)}$ as
\begin{align}
\directionalHessianExternalField &\defeq 
\lim_{h\to 0}\frac{\partial_{\omI}\loss^{(i)}(\ExternalFieldI+h \omI)\!-\!\partial_{\omI}\loss^{(i)}(\ExternalFieldI)}{h}
= \sump \bigbrackets{\omIt}^2 \frac{\partial^2 \loss^{(i)}(\ExternalFieldI)}{\partial \bigbrackets{\ExternalFieldIt}^2} \\
& = \sump \bigparenth{\omIt x_t^{(i)}}^2  \exp\Bigparenth{-\normalbrackets{\ExternalFieldtI + 2\EstimatedParameterRowttt\tp \svbx_{-t}^{(i)}} x_t^{(i)} - \EstimatedParameterTU[tt] \cx_t^{(i)}}.
\end{align}

\paragraph{Proof of \cref{lemma_expected_psi_lower_bound}: \lowerboundbarpsi}
Fix any $i \in [n]$, $u \in [\numindsets]$, and $\ExternalFieldI \in \ParameterSet_{\ExternalField}$. Then, given $\svbx_{-\setU}^{(i)}$ and $\svbz^{(i)}$, we have
\begin{align}
\Expectation\Bigbrackets{\bpsi_u(\svbx^{(i)}; \omI) \bigm\vert \svbx_{-\setU}^{(i)}, \svbz^{(i)}} & \sequal{\cref{eq_def_bar_psi}} \Expectation\Bigbrackets{\sum_{t \in \setU}  \bigparenth{\omIt x_t^{(i)}}^2 \bigm\vert \svbx_{-\setU}^{(i)}, \svbz^{(i)}} \\
& \sequal{(a)} \sum_{t \in \setU}  \Expectation \Bigbrackets{\bigparenth{\omIt x_t^{(i)}}^2 \bigm\vert \svbx_{-\setU}^{(i)}, \svbz^{(i)}}\\
& \sequal{(b)} \sum_{t \in \setU}  \Expectation \Bigbrackets{\Expectation \Bigbrackets{\bigparenth{\omIt x_t^{(i)}}^2 \Big \vert \svbx_{-t}^{(i)}, \svbz^{(i)}} \Bigm\vert \svbx_{-\setU}^{(i)}, \svbz^{(i)}} \\
& \sgreat{(c)} \sum_{t \in \setU}  \Expectation \Bigbrackets{\Variance\Bigparenth{\omIt x_t^{(i)} \Big \vert \svbx_{-t}^{(i)}, \svbz^{(i)}} \Bigm\vert \svbx_{-\setU}^{(i)}, \svbz^{(i)}} \\
& \sgreat{(d)} \frac{2\xmax^2}{\pi e \ctwo[4]} \stwonorm{\omI}^2,
\end{align}
where $(a)$ follows from linearity of expectation, $(b)$ follows from the law of total expectation i.e., $\Expectation[\Expectation[Y|X,Z]|Z] = \Expectation[Y|Z]$ since $\svbx_{-\setU}^{(i)} \subseteq \svbx_{-t}^{(i)}$, $(c)$ follows follows from the fact that for any random variable a, $\Expectation\normalbrackets{a^2} \geq \Variance\normalbrackets{a}$, and $(d)$ follows from \cref{prop_lower_bound_variance}.


\paragraph{Proof of \cref{lemma_concentration_bar_psi}: \concbarpsi}
\label{sub:proof_of_lemma_concentration_bar_psi}
To show this concentration result, we use \cref{coro}~\cref{eq_coro_combined} for the function $q_1$. To that end, we note that the pair $\braces{\rvbx, \rvbz}$ corresponds to a $\tSGM$ (\cref{def:tau_sgm}) with $\dGM \defn (\aGM, \aGM \bGM, \xmax, \ParameterMatrix)$. However, the random vector $\rvbx$ conditioned on $\rvbz$ need not satisfy the Dobrushin's uniqueness condition (\cref{def_dobrushin_condition}). Therefore, we cannot apply \cref{coro}~\cref{eq_coro_combined} as is. To resolve this, we resort to \cref{lemma_conditioning_trick} with $\lambda = \frac{1}{4\sqrt{2}\xmax^2}$ to reduce the random vector $\rvbx$ conditioned on $\rvbz$ to Dobrushin's regime.

Fix any $u \in [\numindsets]$. Then, from \cref{lemma_conditioning_trick}\cref{item:conditional_sgm_independence_set}, (i) the pair of random vectors $\braces{\rvbx_{\setU}, (\rvbx_{-\setU}, \rvbz)}$ corresponds to a $\tSGM[1]$ with $\dGM_1 \defn (\aGM+2\aGM\bGM\xmax, \frac{1}{4\sqrt{2}\xmax^2}, \xmax, \ParameterMatrix_{\setU})$, and (ii) the random vector $\rvbx_{\setU}$ conditioned on $(\rvbx_{-\setU}, \rvbz)$ satisfies the Dobrushin's uniqueness condition (\cref{def_dobrushin_condition}) with coupling matrix $2\sqrt{2} \xmax^2 \normalabs{\ParameterMatrix_{\setU}}$ with $2\sqrt{2} \xmax^2 \opnorm{\normalabs{\ParameterMatrix_{\setU}}} \leq 2\sqrt{2} \xmax^2 \lambda \leq 1/2$. Now, for any fixed $i \in [n]$, 
%
%
%
we apply \cref{coro}~\cref{eq_coro_combined} for the function $q_1$ with $\varepsilon = \varepsilon$ for a given $\svbx_{-\setU}^{(i)}$ and $\svbz^{(i)}$, to obtain
%
\begin{align}
\Probability\biggparenth{\Bigabs{\bpsi_u(\svbx^{(i)}; \omI) \!-\! \Expectation\Bigbrackets{\bpsi_u(\svbx^{(i)}; \omI) \Bigm\vert \svbx_{-\setU}^{(i)}, \svbz^{(i)}}} \geq \varepsilon \Bigm\vert \svbx_{-\setU}^{(i)}, \svbz^{(i)}} \leq \exp\!\biggparenth{\! \dfrac{- \varepsilon^2}{e^{c'\bGM}\stwonorm{\omI}^2}\!}\!.
\end{align}


\subsection{Proof of \cref{lemma_lipschitzness}: \lipschitznesslossfunction}
\label{sub:proof_lemma_lipschitzness}
Fix any $i \in [n]$, any $\ExternalFieldI, \tExternalFieldI \in \ParameterSet_{\ExternalField}$. Consider the direction $\omI = \tExternalFieldI - \ExternalFieldI$, and define the function $q : [0,1] \to \Reals$ as follows
\begin{align}
q(a) = \loss^{(i)}\bigparenth{\ExternalFieldI + a(\tExternalFieldI - \ExternalFieldI)}. \label{eq_func_f_lipschitz}
\end{align}
%
Then, the desired inequality in \cref{eq_lipschitz_property} is equivalent to $$\normalabs{q(1) - q(0)} \leq \xmax \ctwo \sonenorm{\omI}.$$
%
%
%
From the mean value theorem, there exists $a' \in (0,1)$ such that
\begin{align}
\normalabs{q(1) - q(0)} 
%
= \biggabs{\dfrac{dq(a')}{da}}.  \label{eq_mvt_lipschitz}
\end{align}
Therefore, we have
\begin{align}
\bigabs{q(1) - q(0)} & \sequal{\cref{eq_mvt_lipschitz}} \biggabs{\dfrac{dq(a')}{da}} \sequal{\cref{eq_func_f_lipschitz}} \Bigabs{\dfrac{d\loss^{(i)}\bigparenth{\ExternalFieldI + a'(\tExternalFieldI - \ExternalFieldI)}}{da}} \\
& \sequal{\cref{eq_der_mapping_external_field}} \Bigabs{\directionalGradientExternalField\bigr|_{\ExternalFieldI = \ExternalFieldI + a'(\tExternalFieldI - \ExternalFieldI)}}. \label{eq_mvt_lipschitz_second_stage_stage}
\end{align}
Using \cref{eq:first_dir_derivative_stage_2} in \cref{eq_mvt_lipschitz_second_stage_stage}, we have
\begin{align}
\bigabs{q(1) - q(0)} & = \Bigabs{\sump \omtI x_t^{(i)} \exp\Bigparenth{-\normalbrackets{\ExternalFieldtI + a'(\tExternalFieldtI - \ExternalFieldtI) + 2\EstimatedParameterRowttt\tp \svbx_t^{(i)}} x_t^{(i)} - \EstimatedParameterTU[tt] \cx_t^{(i)}}}\\
%
%
& \sless{(a)} \xmax \sump \bigabs{\omIt}  \exp\Bigparenth{\Bigbrackets{\bigabs{(1-a')\ExternalFieldtI} + \bigabs{a' \tExternalFieldtI} + 2\sonenorm{\EstimatedParameterRowt} \sinfnorm{\svbx^{(i)}}} \xmax}\\
& \sless{(b)} \xmax \exp\Bigparenth{\bigparenth{(1-a') \aGM + a' \aGM + 2\aGM \bGM\xmax} \xmax} \sump \bigabs{\omIt} \\
& \sequal{\cref{eq:constants}} \xmax \ctwo \sonenorm{\omI},
\end{align}
where $(a)$ follows from triangle inequality, Cauchyâ€“Schwarz inequality, and because $\sinfnorm{\svbx^{(i)}} \leq \xmax $ for all $ i \in [n]$
%
and $(b)$ follows because $\ExternalFieldI, \tExternalFieldI \in \ParameterSet_{\ExternalField}$, $\EstimatedParameterMatrix \in \ParameterSet_{\ParameterMatrix}$, and $\sinfnorm{\svbx^{(i)}} \leq \xmax $ for all $ i \in [n]$.
