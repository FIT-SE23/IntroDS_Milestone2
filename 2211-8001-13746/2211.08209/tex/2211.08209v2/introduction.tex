\section{Introduction}
\label{section_introduction}
%
%
%

%



%
%

%
We are interested in the problem of unit-level counterfactual inference
%
owing to the increasing importance of personalized decision-making in many domains.
%
%
As a motivating example, consider an observational dataset corresponding to an interaction between a recommender system and a user over time. At each time, the user was exposed to a product based on observed demographic factors as well as certain unobserved factors, and the user's engagement level was recorded. The engagement level at any time could have sequentially depended on the prior interaction in addition to the ongoing interaction. Also, the system could have sequentially adapted its recommendation. Given such data of many heterogeneous users (e.g., IMDb dataset for a movie recommender system), we want to infer each user's average engagement level if it were exposed to a different sequence of products while the observed and the unobserved factors remain unchanged. This task is challenging since: (a) the {unobserved} factors could give rise to spurious associations, (b) the users could be {heterogeneous} in that they may have different responses to same  sequence of products, and (c) each user provides a {single} interaction trajectory.
 %
%

%
More generally, to address problems of this kind, we consider an observational setting where a unit undergoes multiple interventions (or treatments) denoted by $\rvba$. We denote the outcomes of interest by $\rvby$, and allow the interventions $\rvba$ and the outcomes $\rvby$ to be confounded by observed covariates $\rvbv$ as well as unobserved covariates $\rvbz$. The graphical structure shown in \cref{fig_graphical_models}(a) captures these interactions and is at the heart of our problem. 
%
In the recommender system example above, $\rvba$ corresponds to the products recommended, $\rvby$ corresponds to the engagement levels, $\rvbv$ corresponds to the observed demographic factors, and $\rvbz$ corresponds to the unobserved factors (see \cref{fig_graphical_models}(b)).
We consider $n$ heterogeneous and independent units indexed by $i \in [n] \defn \{1,\cdots,n\}$, and assume access to one observation per unit with $(\svbv^{(i)}$, $\svba^{(i)}$, $\svby^{(i)})$ denoting the realizations of $(\rvbv$, $\rvba$, $\rvby)$ for unit $i$.


%
%
%
%
 
%

%

%
%
%
%
%

%
%

%

%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%


%
%

%
%
%


%
%
%
%
%
%
%



%

%

%
%
%
%
%
%
%
\begin{figure}[t]
    \centering
    \begin{tabular}{cc}
    \includegraphics[width=0.3\linewidth,clip]{triangle_2.pdf}
    &
    \includegraphics[width=0.33\linewidth,clip]{sequential_main_4.pdf} ~~
    \includegraphics[height=0.3\linewidth,clip]{sequential_explained_4.pdf}
    \\
    %
    %
    (a) A generic model for our setting \label{a}
    &
    (b) A graphical model for sequential recommender system 
    \end{tabular}
    \caption{Graphical models covered by our methodology. 
    Directed arrows denote causation and undirected arrows denote association. 
    Thin arrows denote low-level causal links and thick arrows denote high-level causal links, i.e., aggregated thin arrows. 
    %
    Our methodology does not assume knowledge of  low-level causal links and is applicable to any graphical model with high-level causal links between variables as in panel \tbf{(a)}. Panel \tbf{(b)} presents an example of a sequential recommender system (consistent with the model in panel (a)) interacting with a user at 3 time points where $\rvv_t$, $\rva_t$, and $\rvy_t$ denote the user's observed demographic factors, the product exposed to the user, and the user's engagement level, respectively, at time $t$.
    %
    The left subplot 
    %
    illustrates the high-level dependency between the variables while the right subplot expands on it for time $1$ and $2$.
    %
    %
    %
    %
    %
    {
    %
    }}
    %
    %
    %
    %
    %
    %
    \label{fig_graphical_models}
    %
\end{figure}

We operate within the Neyman-Rubin potential outcomes framework \citep{Neyman1923, Rubin1974} and denote the potential outcome of unit $i \in [n]$ under interventions $\svba$ by $\svby^{(i)}(\svba)$. Given the realizations $\braces{(\svbv^{(i)}, \svba^{(i)}, \svby^{(i)})}_{i=1}^{n}$, our goal is to answer counterfactual questions for these $n$ units. For example, what would the potential outcomes $\svby^{(i)}(\wtil{\svba}^{(i)})$ for interventions $\wtil{\svba}^{(i)} \neq \svba^{(i)}$ be, while the observed and unobserved covariates remain unchanged?  
%
Under the graphical model in \cref{fig_graphical_models}(a) and the stable unit treatment value assumption (SUTVA), i.e., the potential outcomes of unit $i$ are not affected by the interventions at other units, learning unit-level counterfactual distributions is equivalent to
learning unit-level conditional distributions
\begin{align}
    \braces{f_{\rvby | \rvba, \rvbz, \rvbv}(\rvby=\cdot | \rvba= \cdot, \svbz^{(i)}, \svbv^{(i)})}_{i=1}^{n}. \label{eq_set_conditional_distributions}
\end{align}
Here, the $i$-th distribution represents the conditional distribution for the outcomes $\rvby$ as a function of the interventions $\rvba$, while keeping the observed covariates $\rvbv$ and the unobserved covariates $\rvbz$ fixed at the corresponding realizations for unit $i$, i.e., $\svbv^{(i)}$ and $\svbz^{(i)}$, respectively.

%
       
\newcommand{\ranvarvec}{\rvbw}
%
%
%
\newcommand{\ranvarmat}{\newcommand{\varmat}{\begin{bmatrix} 
\rvbz, \rvbv, \rvba, \rvby^\tp
\end{bmatrix}}}
\newcommand{\varvec}{\svbw}
%
%
%
\newcommand{\varmat}{\begin{bmatrix} 
\svbz, \svbv, \svba, \svby
    \end{bmatrix}}
    
%
%
Such questions cannot be answered without structural assumptions 
due to two key challenges: (a) unobserved confounding and 
%
(b) single observation per unit.
First, the unobserved covariates $\rvbz$ introduce spurious statistical dependence between interventions and outcomes, termed unobserved confounding, which results in biased estimates. 
%
%
%
%
%
Second, we only observe one realization, namely the outcomes $\svby^{(i)}(\svba^{(i)})$ under the interventions $\svba^{(i)}$, that is consistent with the unit-level conditional distribution $f_{\rvby | \rvba, \rvbz, \rvbv}(\svby | \svba, \svbz^{(i)}, \svbv^{(i)})$. As a result, we need to learn $n$ heterogeneous conditional distributions while having access to only one sample from each of them. %

%
%
%
%
%
%

%

%



%
In this work, we model the joint distribution of the unobserved covariates, the observed covariates, the intervention, and the outcomes of interest as an exponential family distribution motivated by the principle of maximum entropy.\footnote{Exponential family distributions are the maximum entropy distributions given linear constraints on distributions such as specifying the moments (see \cite{jaynes1957information}). The exponential family considered in this work arise when the first and second moments of the joint vector $(\rvbz, \rvbv, \rvba, \rvby)$ are constrained.}
With this model structure, we show that both the aforementioned challenges can be tackled. 
%
In particular, we show that the $n$ unit-level conditional distributions in \eqref{eq_set_conditional_distributions} lead to $n$ distributions from the same exponential family, albeit with parameters that vary across units. The parameter corresponding to the $i^{th}$ unit, for brevity in terminology denoted by $\Truephi[i]$ (defined later), captures the effect of $\svbz^{(i)}$ and helps tackle the challenge of unobserved confounding. However, the challenge still remains to learn $n$ heterogeneous exponential family distributions with one sample per distribution. This challenge has been addressed in two specific scenarios in the literature: (a) if  the unobserved confounding is identical across units, i.e., the parameters $\normalbraces{\Truephi[i]}_{i=1}^n$ 
%
were all equal, then the challenge boils down to learning parameters of a single exponential family distribution from $n$ samples, which has been well-studied (cf. \cite{ShahSW2021B} for an overview); 
(b) if $\rvbv$, $\rvba$, and $\rvby$ take binary values and have pairwise interactions, then the challenge boils down to learning parameters of an Ising model (defined later) with one sample. This specific challenge has been studied under restricted settings: (i) where the dependencies between the variables are known (e.g., \cite{KandirosDDGD2021, mukherjee2021high}) and (ii) where a specific
subset of the parameters are known \citep{DaganDDA2021}.
%
%
%
%
In this work, we consider a generalized setting where $\rvbv$, $\rvba$, and $\rvby$ can be either discrete, continuous, or both, and do not assume that the underlying dependencies or a specific subset of parameters are known.
%
%
%
%
%
%

%
%
%
%
%

%

%
%
%

%
%
%
%


%

\paragraph{Summary of contributions} 
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
This work introduces a method 
to learn unit-level counterfactual distributions from observational studies in the presence of unobserved confounding with one sample per unit. For every unit $i \in [n]$, we reduce learning its counterfactual distribution to learning the unit-specific parameter  $\Truephi[i]$ from one sample $(\svbv^{(i)}, \svba^{(i)}, \svby^{(i)})$ for unit $i$.
%
%
%
The specific technical contributions are as follows:
%
\begin{enumerate}
%
\itemsep0em
    %
    \item We introduce a convex (and strictly proper) loss function (\cref{def-loss-function}) that pools the data $\braces{(\svbv^{(i)}, \svba^{(i)}, \svby^{(i)})}_{i=1}^{n}$ across all $n$ samples to jointly learn all $n$ parameters $\normalbraces{\Truephi[i]}_{i=1}^n$.
    \item For every unit $i$, we prove that the mean squared errors of our estimates of (a) $\Truephi[i]$ (\cref{theorem_parameters}) and (b) the expected potential outcomes under alternate interventions (\cref{thm_causal_estimand}) scale linearly with the metric entropy of the underlying parameter space.
    %
    For instance, when $\Truephi[i]$ is $s$-sparse linear combination of $k$ known vectors (\cref{cor_params}), 
    %
    %
    the error---just with one sample---decays as $O(s\log k/p)$, where $p$ is the dimension of the tuple $(\rvbv, \rvba, \rvby)$.
    %
    \item We apply our method to impute missing covariates when they are sparse. Formally, we consider a setup (with no systematically unobserved covariates) where the observed covariates are entirely missing for some fixed fraction of the units. Specifically, for unit $i$ with missing covariates, only $(\svba^{(i)}, \svby^{(i)})$ is observed. For every such unit, we show that our method can recover the missing covariates with the mean squared error decaying as $O(p_v^2/p)$, where $p_v$ and $p$ are the dimensions of $\rvbv$ and $(\rvbv, \rvba, \rvby)$, respectively (\cref{prop_impute_missing_covariates}).
 \item In our analysis, we (a) derive sufficient conditions for a continuous random vector supported on a compact set to satisfy the logarithmic Sobolev inequality (\cref{thm_LSI_main}) and (b) provide new concentration bounds for arbitrary functions of a continuous random vector that satisfies the logarithmic Sobolev inequality (\cref{thm_main_concentration}). These results may be of independent interest.
    %
\end{enumerate}
\paragraph{Outline} \cref{sec_related_work} discusses background and related work. We discuss our formulation and algorithm in \cref{section_problem_formulation} and present their analysis in \cref{sec_main_results}. We develop an application of our methodology to impute missing covariates in \cref{sec_sparse_measurement_errors}. We sketch the proof of our main result in \cref{sec_proof_sketch} with detailed proofs deferred to the appendices. We conclude with a discussion in \cref{sec_discussion}.
%
 
%
%
%
%


%

%
%
%

%
    %
    %
    %
    %
    %
%
%



%
%
%



%

%
%
%
%

%
 

\paragraph{Notation} 
For any positive integer $n$, let $[n] \coloneqq \{1,\cdots, n\}$.
For a deterministic sequence $u_1, \cdots , u_n$, we let $\svbu \coloneqq (u_1, \cdots, u_n)$. 
For a random sequence $\rvu_1, \cdots , \rvu_n$, we let $\rvbu \coloneqq (\rvu_1, \cdots, \rvu_n)$. 
For a vector $\svbu \in \Reals^p$, we use $u_t$ to denote its $t^{th}$ coordinate and $u_{-t} \in \Reals^{p-1}$ to denote the vector after deleting the $t^{th}$ coordinate. We denote the $\ell_0$, $\ell_p$ $(p \geq 1)$, and $\ell_{\infty}$ norms of a vector $\svbv$ by $\szeronorm{\svbv}$, $\spnorm{\svbv}$, and  $\sinfnorm{\svbv}$, respectively.  For a matrix $\tbf{M} \in \Reals^{p \times p}$, we denote the element in $t^{th}$ row and $u^{th}$ column by $\tbf{M}_{tu}$, the $t^{th}$ row by $\tbf{M}_t$, and the vector obtained after deleting $\tbf{M}_{tt}$ from $\tbf{M}_t$ by $\tbf{M}_{t,-t}$.  Further, we denote the matrix maximum norm by $\maxmatnorm{\tbf{M}}$, the Frobenius norm by $\fronorm{\tbf{M}}$, the spectral norm (operator $2$-norm) by $\opnorm{\tbf{M}}$, the induced $1-$norm (operator $1$-norm) by $\onematnorm{\tbf{M}}$, the induced $\infty$-norm (operator $\infty$-norm) by $\infmatnorm{\tbf{M}}$, and the $(2,\infty)$-norm by $\matnorm{\tbf{M}}_{2,\infty}$. Finally, for vectors $\what{\svbu} \in \Reals^p$ and $\wtil{\svbu} \in \Reals^p$, the mean squared error between $\what{\svbu}$ and $\wtil{\svbu}$ is defined as $\mathrm{MSE}(\what{\svbu}, \wtil{\svbu}) \defn p^{-1} \sum_{t \in [p]} (\what{u_t} - \wtil{u_t})^2$.
%

%

%
%
%
%
%
%
%