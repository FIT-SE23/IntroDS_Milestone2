%
\section{Proof Sketch for \cref{theorem_parameters}: \parammainresultname}
\label{sec_proof_sketch}
Our proof of \cref{theorem_parameters} proceeds in two stages (see \cref{fig:proof_sketch} for an overview). First, we establish \cref{eq:matrix_guarantee} for estimating $\TrueParameterMatrix$. Next, we use this guarantee to establish the unit-level guarantee~\cref{eq:theta_guarantee} for each of $\bigbraces{\TrueExternalFieldI[1], \cdots, \TrueExternalFieldI[n]}$ by substituting $\ParameterMatrix = \EstimatedParameterMatrix$ in \cref{eq_estimated_parameters}, i.e.,  analyzing the following convex optimization problem: 
\begin{align}
    \normalbraces{\EstimatedExternalFieldI[1], \cdots, \EstimatedExternalFieldI[n]} \in \argmin_{\normalbraces{\ExternalFieldI[1], \cdots, \ExternalFieldI[n]} \in \ParameterSet_{\ExternalField}^n} \loss\bigparenth{\EstimatedParameterMatrix, \ExternalFieldI[1], \cdots, \ExternalFieldI[n]}. \label{eq_decomposed_second_minimum}
\end{align}

\subsection{Estimating the population-level parameter}
\label{sec_proof_sketch_pop}
In the first part, we show that all points $\ExtendedParameterMatrix \in \ParameterSet_{\ParameterMatrix} \times \ParameterSet_{\ExternalField}^n$, such that $\stwonorm{\ParameterRowt - \TrueParameterRowt} \geq \varepsilon$ for at least one $t \in [p]$, uniformly satisfy 
\begin{align}
    \loss(\ExtendedParameterMatrix) \geq \loss(\ExtendedTrueParameterMatrix) + \Omega(\varepsilon^2) \stext{for} n \geq \frac{ce^{c'\bGM} p^2}{\varepsilon^4} \cdot \Bigparenth{p \log \frac{p}{\delta \varepsilon^2} + \metric_{\ExternalField, n}\big(\varepsilon^2 \big)},
    \label{eq_proof_sketch_thm_edge}
\end{align}
with probability at least $1-\delta$. Then, we conclude the proof using contraposition. 

To prove \cref{eq_proof_sketch_thm_edge}, we first decompose the convex (and positive) objective $\loss(\ExtendedParameterMatrix)$ in \cref{eq:loss_function} as a sum of $p$ convex (and positive) auxiliary objectives $\loss_t$, namely, $\loss(\ExtendedParameterMatrix) = \sump \loss_t\bigparenth{\ExtendedParameterRowT}$ where
\begin{align}
    \loss_t\bigparenth{\ExtendedParameterRowT} \defn \frac{1}{n} \sumn[i] \exp\Bigparenth{-\normalbrackets{\ExternalFieldtI + 2\EstimatedParameterRowttt\tp \svbx_{-t}^{(i)}} x_t^{(i)} - \EstimatedParameterTU[tt] \Bigbrackets{[x_t^{(i)}]^2 - \frac{\xmax^2}{3}}}.
    \label{eq:loss_function_p_original}
\end{align}
Next, for any fixed $t \in [p]$, $\varepsilon > 0$, and $\ExtendedParameterMatrix \in \ParameterSet_{\ExternalField}^n \times \ParameterSet_{\ParameterMatrix}$ with $\stwonorm{\ParameterRowt - \TrueParameterRowt} \geq \varepsilon$, we show (see \cref{lemma_parameter})
\begin{align}
    \loss_t(\ExtendedParameterRowT) \geq \loss_t(\ExtendedTrueParameterRowT) + \Omega(\varepsilon^2) - \varepsilon_1 \qtext{whenever} n \geq \frac{c e^{c'\bGM}\log \frac{p}{\delta}}{\varepsilon_1^2} , \label{eq_firststage_key_step}
\end{align}
and then establish the same bound uniformly for all $t \in [p]$ with probability $1-\delta$. Taking a sum over $t$ on both sides of \cref{eq_firststage_key_step}, we conclude that for any fixed  
$\ExtendedParameterMatrix$ with $\stwonorm{\ParameterRowt - \TrueParameterRowt} \geq \varepsilon$ for some $t \in [p]$, 
\begin{align}
    \loss(\ExtendedParameterMatrix) \geq \loss(\ExtendedTrueParameterMatrix) + \Omega(\varepsilon^2) \qtext{whenever} n \geq \frac{c e^{c'\bGM} p^2 \log \frac{p}{\delta}}{\varepsilon^4}, \label{eq_proof_sketch_first_part_summed}
\end{align}
with probability at least $1\!-\!\delta$ where we substituted $\varepsilon_1 \!=\! c\varepsilon^2/p$. Finally, we conclude \cref{eq_proof_sketch_thm_edge} by using \cref{eq_proof_sketch_first_part_summed}, the Lipschitzness of  $\loss$ (see \cref{lemma_lipschitzness_first_stage}), and a covering number argument (see \cref{sec:proof_of_theorem_parameters}). 
%

We establish \cref{eq_firststage_key_step} (\cref{lemma_parameter}) via \cref{lemma_conc_first_sec_der}, which provides suitable concentration and anti-concentration results for the first-order and second-order derivatives, respectively, for the auxiliary objective $\loss_t$ in \cref{eq:loss_function_p_original}. We prove \cref{lemma_conc_first_sec_der} by extending the results from \cite{ShahSW2021A} to the setting with non-identical but independent samples $\normalbraces{\svbx^{(i)}\sim \TrueJointDistfunT}_{i = 1}^{n}$.

\begin{figure}[t!]
\resizebox{\textwidth}{!}
{%
\begin{tikzpicture}[scale=0.8, >=stealth,main node/.style={shape=rectangle,draw,rounded corners,}]
    %
    \node[main node][very thick, fill=red!0, font = {\large}] (t2) {\begin{tabular}{c} \cref{theorem_parameters}: Part II: \\ Proof of \cref{eq:theta_guarantee} for unit-level \\parameters $\normalbraces{\TrueExternalFieldI}_{i = 1}^{n}$ \end{tabular}};
    %
    \node[main node][very thick, font = {\large}] (l1) [right=3.1 cm of t2]{\begin{tabular}{c} \cref{lemma_conc_first_sec_der_stage_two}: \\ Concentration of \\gradient and Hessian of\\ loss functions $\normalbraces{\loss^{(i)}}_{i = 1}^{n}$ \end{tabular}};
    \node[main node][very thick, fill=red!0, font = {\large}] (t1) [below=0.9 cm of l1]{\begin{tabular}{c} \cref{theorem_parameters}: Part I: \\ Proof of \cref{eq:matrix_guarantee} for population-\\level parameter $\TrueParameterMatrix$ \end{tabular}};
    \node[main node][very thick, fill=orange!0, font = {\large}] (p3) [right=2.6 cm of l1]{\begin{tabular}{c} \cref{thm_main_concentration}:\\ Tail bounds\\ under LSI \end{tabular}};
    \node[main node][very thick, fill=orange!0, font = {\large}] (p2) [above=1.5cm of p3]{\begin{tabular}{c} \cref{thm_LSI_main}:\\ LSI for weakly\\ dependent RVs \end{tabular}};
    \node[main node][very thick, fill=orange!0, font = {\large}] (p4) [below=1.75 cm of p3]{\begin{tabular}{c} \cref{lemma_conditioning_trick}:\\ Identifying weakly\\dependent RVs in \\exponential family \end{tabular}};
    \node[main node][very thick] (l2) [above=0.7 cm of l1, font = {\large}]{\begin{tabular}{c} \cref{lemma_tenorization_kld} + \cref{lemma_dobrushin_implies_tensorization}:\\ Approximate \\
    tensorization of \\ entropy for weakly \\ dependent RVs \end{tabular}};
    %
    \node at (5.3,0.3) (lipschitzness) {\large (i) \cref{lemma_parameter_single_external_field}};
    \node at (5.3,-0.3) {\large (ii) \cref{lemma_lipschitzness}};
    \node[main node][very thick, font = {\large}] (l3) [above left=2.5cm of l1]{\begin{tabular}{c} \cref{lemma_reverse_pinsker}:\\ Reverse-Pinkser \\inequality \end{tabular}};
    \draw[->,  line width=.6mm] (t1) to[out=90,in=270] (l1);
    \node at (11.9,-2.0) {\large \cref{lemma_expected_psi_upper_bound}};
    \draw[->,  line width=.6mm] (p2) to[out=270,in=90] (p3);
    \draw[->,  line width=.6mm] (p3) to[out=180,in=0] (l1);
    \draw[->,  line width=.6mm] (p4) to[out=160,in=340] (l1);
    \draw[->,  line width=.6mm] (l2) to[out=5,in=180] (p2);
    \draw[->,  line width=.6mm] (l3) to[out=0,in=170] (l2);
    \draw[->,  line width=.6mm] (l1) to[out=180,in=0] (t2);
    \node at (15.2,0.3) {\large \cref{coro}};
    \draw[->,  line width=.6mm] (0.2, -3.5) to (6.75, -3.5);
    \node at (3.4,-3.2) {\large Extend \cite{ShahSW2021A}};
    \node at (3.5,-3.8) {\large to non-identical samples};
    \draw[->,  line width=.6mm] (10.4, -5.7) to (16.6, -5.7);
    \node at (13.5,-5.4) {\large Extend \cite{DaganDDA2021}};
    \node at (13.5,-6.0) {\large to continuous RVs};
    \draw[->,  line width=.6mm] (1.6, 2.7) to (6.8, 2.7);
    \node at (4.2,3.0) {\large Extend \cite{Marton2015}};
    \node at (4.2,2.4) {\large to continuous RVs};
    %
    %
    %
    %
    %
    %
\end{tikzpicture}
}
  \caption{\tbf{Sketch diagram of the results and the proof techniques for \cref{theorem_parameters}.}
  First, we establish~\cref{eq:matrix_guarantee} for estimating $\TrueParameterMatrix$ by extending \citet[Proposition I.1, Proposition I.2]{ShahSW2021A} for i.i.d. data to non-identical samples. Next, we use \cref{eq:matrix_guarantee} to establish \cref{eq:theta_guarantee} for the unit-level parameters $\normalbraces{\TrueExternalFieldI}_{i = 1}^{n}$ via suitable concentration results for derivatives of the auxiliary loss functions in k\cref{eq:loss_function_n_original}. 
  En route, we establish three results of independent interest: (i) \cref{thm_LSI_main} that shows that weakly dependent and bounded random variables satisfy logarithmic Sobolev inequality (LSI) by both extending \citet[Theorem. 1, Theorem. 2]{Marton2015} and establishing a reverse-Pinkser  inequality to continuous random vectors; (ii)  \cref{thm_main_concentration} that extends the tail bounds \citet[Theorem. 6]{DaganDDA2021} to continuous distributions satisfying LSI; and (iii)
\cref{lemma_conditioning_trick} that extends the conditioning trick \citet[Lemma. 2]{DaganDDA2021} for identifying a weakly dependent subset to continuous random vectors.}
    \label{fig:proof_sketch}
\end{figure}

\subsection{Estimating the unit-level parameters}
\label{sec_proof_sketch_unit}
In the second part, we decompose the convex optimization problem in \cref{eq_decomposed_second_minimum} into $n$ convex optimization problems:
%
%
\begin{align}
    \loss^{(i)}\bigparenth{\ExternalFieldI} \defn \sump[t] \exp\Bigparenth{-\bigbrackets{\ExternalFieldtI + 2\EstimatedParameterRowttt\tp \svbx_{-t}^{(i)}} x_t^{(i)} - \EstimatedParameterTU[tt] \bigparenth{[x_t^{(i)}]^2 - \frac{\xmax^2}{3}}}
    \stext{for $i \in [n]$.}
    \label{eq:loss_function_n_original}
\end{align}
Noting that the set $\ParameterSet_{\ExternalField}^n$ places independent constraints on the $n$ unit-level parameters, namely $\ExternalFieldI[i] \in \ParameterSet_{\ExternalField}$, independently for all $ i \in [n]$ and combining \cref{eq:loss_function,eq_decomposed_second_minimum}, we find that
\begin{align}
    \min_{\normalbraces{\ExternalFieldI[1], \cdots, \ExternalFieldI[n]} \in \ParameterSet_{\ExternalField}^n} \!\!\!\! \loss\bigparenth{\EstimatedParameterMatrix, \ExternalFieldI[1], \cdots, \ExternalFieldI[n]} \sequal{\cref{eq:loss_function_n_original}} \frac{1}{n}\!\sumn \min_{\ExternalFieldI[i] \in \ParameterSet_{\ExternalField}} \loss^{(i)}\bigparenth{\ExternalFieldI[i]}
    \!\implies
    \EstimatedExternalFieldI \! \in \argmin_{\ExternalFieldI[i] \in \ParameterSet_{\ExternalField}} \loss^{(i)}\bigparenth{\ExternalFieldI[i]}, \label{eq_opt_problem_unit}
\end{align}
for each $i \in [n]$.
%
%
%
%
%
%
Next, we establish that with probability at least $1-\delta$,
\begin{align}
    \loss^{(i)}\bigparenth{\ExternalFieldI} & \geq  \loss^{(i)}\bigparenth{\TrueExternalFieldI} + R^2(\varepsilon, \delta) \stext{when} n \geq \frac{ce^{c'\bGM} p^2}{\varepsilon^4}  \Bigparenth{p \log \frac{p}{\delta \varepsilon^2}  + \tmetric_{\ExternalField,n}\normalparenth{ \varepsilon, \delta}}, \label{eq_secondstage_key_step}
\end{align}
uniformly for all points $\ExternalFieldI \in \ParameterSet_{\ExternalField}$ with $\stwonorm{\ExternalFieldI - \TrueExternalFieldI} \geq R(\varepsilon, \delta)$ (see \cref{eq_radius_node_thm}). We conclude the proof by contraposition with the basic inequality $\loss^{(i)}(\EstimatedExternalFieldI) \leq \loss^{(i)}(\TrueExternalFieldI)$ and a standard union bound over all $i \in [n]$.

The proof of \cref{eq_secondstage_key_step} mimics the same road map as that for \cref{eq_proof_sketch_thm_edge}. \cref{lemma_parameter_single_external_field} shows that for any fixed $\ExternalFieldI \in \ParameterSet_{\ExternalField}$, if $\ExternalFieldI$ is far from $\TrueExternalFieldI$, then with high probability $\loss^{(i)}\bigparenth{\ExternalFieldI}$ is significantly larger than $\loss^{(i)}\bigparenth{\TrueExternalFieldI}$. We prove \cref{lemma_parameter_single_external_field} via  concentration of derivatives of $\loss^{(i)}$~\cref{eq:loss_function_n_original} in \cref{lemma_conc_first_sec_der_stage_two}, this objective's Lipschitznes 
 in \cref{lemma_lipschitzness}, and a covering number argument (see \cref{sec_proof_thm_node_parameters_recovery}).

The proof of \cref{lemma_conc_first_sec_der_stage_two} involves several novel arguments: First, for a $\dGM$-Sparse Graphical Model (\cref{def:tau_sgm}), i.e., a generalization of the random vector $\rvbw$ in \cref{eq_joint_distribution_zvay}, \cref{lemma_conditioning_trick} identifies a   subset that satisfies Dobrushin's uniqueness condition (\cref{def_dobrushin_condition}) after conditioning on the complementary subset. Second, \cref{thm_LSI_main} shows that a bounded and weakly dependent continuous random vector (defined using Dobrushin's uniqueness condition) satisfies the logarithmic Sobolev inequality (LSI). Third, \cref{thm_main_concentration} establishes tail bounds for arbitrary functions of a continuous random vector that satisfies LSI. Putting together these results and a robustness result (\cref{lemma_expected_psi_upper_bound}) while invoking concentration results to account for the estimation error for $\TrueParameterMatrix$, yields \cref{lemma_conc_first_sec_der_stage_two}.