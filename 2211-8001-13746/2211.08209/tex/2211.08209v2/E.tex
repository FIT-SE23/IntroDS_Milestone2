\section{Proof of {Proposition \ref{prop_impute_missing_covariates}}: \impute}
\label{proof_impute_missing_covariates}
We start by decomposing the true covariates $\rvbv$ into two variables: one to capture the randomness in the noisy observations $\crvbv$ and the other to capture the randomness in the measurement error $\Delta \rvbv$, i.e., $\rvbv = \crvbv - \Delta \rvbv$.  
%
Then, by letting $\barp \defn 2 p_v + p_a + p_y$ and using \cref{eq_joint_distribution_vay}, the joint probability distribution $f_{\wbar{\ranvarvec}}$ of the $\barp$-dimensional random vector $\wbar{\ranvarvec} \defeq \normalparenth{\Delta \rvbv, \crvbv, \rvba, \rvby}$ can be parameterized by a vector $\wbar{\phi} \in \Reals^{\barp \times 1}$ and a symmetric matrix $\wbar{\Phi} \in \Reals^{\barp \times \barp}$ as follows 
\begin{align}
f_{\wbar{\ranvarvec}}(\wbar{\varvec}; \wbar{\phi}, \wbar{\Phi}) \propto \exp\Bigparenth{ \wbar{\phi}\tp \wbar{\varvec}
	+\wbar{\varvec}\tp \wbar{\Phi} \wbar{\varvec}},
\qtext{where}
\wbar{\varvec} \defeq (\Delta \svbv, \csvbv, \svba, \svby),
\label{eq_joint_distribution_vvay}
\end{align}
and $\Delta \svbv$, $\csvbv$, $\svba$, and $\svby$ denote realizations of $\Delta \rvbv$, $\crvbv$, $\rvba$, and $\rvby$, respectively. More importantly, $\wbar{\phi}$ and $\wbar{\Phi}$ are derived completely from $\phi$ and $\Phi$, respectively, and have special structure: 
%
\begin{align}
    \wbar{\phi}^{(\cv)} = - \wbar{\phi}^{(\Delta v)} & =  \phi^{(v)},\\
    \wbar{\phi}^{(u)} & = \phi^{(u)} \stext{for all} \rvbu \in \normalbraces{\rvba, \rvby},\\
    \wbar{\Phi}^{(\Delta v, \Delta v)} = \wbar{\Phi}^{(\cv, \cv)} = - \wbar{\Phi}^{(\cv, \Delta v)} & = \Phi^{(v, v)}\\
    \wbar{\Phi}^{(u, \cv)} = - \wbar{\Phi}^{(u, \Delta v)} & = \Phi^{(u, v)} \stext{for all} \rvbu \in \normalbraces{\rvba, \rvby}, \stext{and}\\
    \wbar{\Phi}^{(u_1, u_2)} & = \Phi^{(u_1, u_2)} \stext{for all} \rvbu_1, \rvbu_2 \in \normalbraces{\rvba, \rvby}.
\end{align}
%
Now, to learn counterfactuals and measurement errors for units $i \in \normalbraces{1, \cdots, n/2}$, we use the methodology developed in \cref{section_problem_formulation} by replacing the role of unobserved covariates $\rvbz$ by $\Delta \svbv$. In particular, we consider learning $f_{\rvby | \rvba, \Delta \rvbv, \crvbv}(\rvby=\cdot | \rvba= \cdot, \Delta \svbv, \csvbv)$ as a function of $\rvba$. From \cref{eq_actual_parameters_of_interest} and the structure on $\wbar{\phi}$ and $\wbar{\Phi}$ described above, this reduces to learning
\begin{align}
& \stext{(i)} \wbar{\phi}^{(y)} + 2\wbar{\Phi}^{(\Delta v,y)\top} \Delta \svbv + 2\wbar{\Phi}^{(\cv,y)\top} \csvbv = \phi^{(y)} - 2\Phi^{(v,y)\top} \Delta \svbv + 2\Phi^{(v,y)\top} \csvbv, \stext{}\\ & \stext{(ii)} \wbar{\Phi}^{(a,y)} = \Phi^{(a, y)}, \stext{and}\\ &\stext{(iii)} \wbar{\Phi}^{(y,y)} = \Phi^{(y, y)}.
\end{align}
To learn these, we consider the distribution of $\rvbx \defn (\crvbv, \rvba, \rvby)$ conditioned on $\Delta \rvbv = \Delta \svbv$. From \cref{eq_conditional_distribution_vay}, we have
\begin{align}
f_{\rvbx|\Delta \rvbv}\bigparenth{\! \svbx| \Delta \svbv; \ExternalField(\!\Delta \svbv\!), \! \ParameterMatrix} \!\propto\! \exp\!\Bigparenth{ \!\normalbrackets{\ExternalField(\!\Delta \svbv\!)}\!\tp \! \! \svbx \!+\! \svbx\!\tp \!\ParameterMatrix \svbx \!} \stext{with}
\ExternalField(\!\Delta \svbv\!)  \! \defeq \!\! \begin{bmatrix} \! \phi^{(v)} \!-\! 2 \Phi^{(v,v)\top}\!  \Delta \svbv \! \\ \! \phi^{(a)} \!-\! 2 \Phi^{(v,a)\top}\!  \Delta \svbv \! \\ \! \phi^{(y)} \!-\! 2 \Phi^{(v,y)\top} \! \Delta \svbv \!\end{bmatrix}\!\!\!, \label{eq_coditional_distribution_vay|delta_v}
\end{align} 
%
$\svbx \defn (\csvbv, \svba, \svby)$, $\Theta \defn \Phi$, and $\csvbv$, $\svba$, and $\svby$ denoting realizations of $\crvbv$, $\rvba$, and $\rvby$, respectively. The special structure on $\wbar{\Phi}$ discussed above implies that $\Phi^{(v,v)}, \Phi^{(v,a)}$, and $\Phi^{(v,y)}$ affect both $\ExternalField(\Delta \svbv)$ and $\ParameterMatrix$ which we exploit. As mentioned in \cref{subsec_theo_gua}, we denote the true distribution of $\rvbx$ conditioned on $\Delta \rvbv = \Delta \svbv$ by $f_{\rvbx|\Delta \rvbv}\bigparenth{\cdot| \Delta \svbv; \TrueExternalField(\Delta \svbv), \TrueParameterMatrix}$. 

\paragraph{Proof idea} First, we use units $i \in \normalbraces{n/2+1, \cdots, n}$ without any measurement error to estimate $\phi^{\star}$ and $\Phi^{\star} = \TrueParameterMatrix$, i.e., the parameters corresponding to the distribution of $(\rvbv, \rvba, \rvby)$ (see \cref{subsec_theo_gua}). Next, for units $i \in \normalbraces{1, \cdots, n/2}$ with measurement error, we estimate $\TrueExternalField(\Delta \svbv^{(i)})$ by expressing it as a linear combination of the estimates of $\phi^{\star}$ and $\Phi^{\star}$ (enabling the use of \cref{exam:lc_dense}). The coefficients of this linear combination turn out to be our estimates of the measurement error $\Delta \svbv^{(i)}$.
%
%
%
%
%
%
%
%
%
%
%
%
%

%

%

\paragraph{Estimate $\phi^{\star}$ and $\Phi^{\star}$}  For units $i \in \normalbraces{n/2+1, \cdots, n}$, under our assumption $\Delta \svbv^{(i)} = 0$ implying $\TrueExternalField(\Delta \svbv^{(i)}) = \phi^{\star}$. Therefore, in addition to the population-level parameter $\TrueParameterMatrix = \Phi^{\star}$, the unit-level parameter $\TrueExternalField(\Delta \svbv) = \phi^{\star}$ is also shared for these units. As a result, the set of distributions $\bigbraces{f_{\rvbx|\Delta \rvbv}\bigparenth{\cdot| \Delta \svbv; \TrueExternalField(\Delta \svbv), \TrueParameterMatrix}}_{i=1}^n$ all coincide. Thus, 
 %
 learning $\phi^{\star}$ and $\Phi^{\star}$ boils down to learning parameters of a sparse graphical model (because of the assumptions in \cref{subsec_theo_gua}) from $n/2$ samples. We use the methodology from \cite{ShahSW2021A} (which is closely related to the one in this work) to obtain estimates 
 %
 $\what{\phi}$ and $\what{\Phi}$
 such that with probability at least $1-\delta$, we have
\begin{align}
\max\bigbraces{\stwonorm{\phi^{\star} - \what{\phi}}, \matnorm{\Phi^{\star} \!-\! \what{\Phi}}_{2,\infty}} & \leq \varepsilon_1 \qtext{whenever} n  \geq \frac{ce^{c'\bGM}p \log\frac{p}{\delta}}{\varepsilon_1^4}. \label{eq_theta_estimate_guarantee}
\end{align}
%

%
%
%
%
%
%
%

\paragraph{Recover the unit-level parameters} Now, for units $i \in \normalbraces{1, \cdots, n/2}$, we express the true unit-level parameters $\TrueExternalField(\Delta \svbv^{(i)})$ as a linear combination of known vectors. To that end, fix any $i \in [n/2]$. Then, using \cref{eq_coditional_distribution_vay|delta_v},
%
we can write $\TrueExternalFieldI \defn \TrueExternalField(\Delta \svbv^{(i)})$ as a linear combination of $p_v + 1$ vectors, i.e.,
\begin{align}
%
\TrueExternalFieldI = \tbf{B} \tbf{a}^{(i)},\label{eq_sparse_lasso_first}
\end{align}
where
\begin{align}
    \tbf{B}  \defn \begin{bmatrix} \phi^{\star}, -2 \Phi^{\star}_1, \cdots, -2 \Phi^{\star}_{p_v} \end{bmatrix}\in  \Reals^{p\times (p_v+1)} \qtext{and} \tbf{a}^{(i)} \defn \begin{bmatrix} 1  \\ \Delta \svbv^{(i)} \end{bmatrix} \in \Reals^{(p_v+1) \times 1}.  \label{eq_appl_b_defn}
\end{align}
%
%
%
%
While we do not know the matrix $\tbf{B}$, we can produce an estimate $\what{\tbf{B}}$ using $\what{\phi}$ and $\what{\Phi}$ such that, with probability at least $1-\delta$,
\begin{align}
\matnorm{\what{\tbf{B}} \!-\! \tbf{B}}_{2,\infty} & \leq \varepsilon_1 \qtext{whenever} n  \geq \frac{ce^{c'\bGM}p\log\frac{p}{\delta}}{\varepsilon_1^4}. \label{eq_b_estimate_guarantee}
\end{align}
This guarantee follows directly from \cref{eq_theta_estimate_guarantee} and the definition of $\tbf{B}$ in \cref{eq_appl_b_defn}. Then, we can write
%
%
%
\begin{align}
\TrueExternalFieldI = \what{\tbf{B}} \wtil{\tbf{a}}^{(i)} \qtext{where} \wtil{\tbf{a}}^{(i)} \defn  \tbf{a}^{(i)} + \zeta, \label{eq_sparse_lasso_third}
\end{align}
for some error term $\zeta$. 
Conditioned on the event
%
\cref{eq_b_estimate_guarantee}, $\zeta$ can be controlled in following manner
\begin{align}
\stwonorm{\what{\tbf{B}} \zeta} \sequal{\cref{eq_sparse_lasso_third}}  \stwonorm{\TrueExternalFieldI - \what{\tbf{B}} \tbf{a}^{(i)}} & \sequal{\cref{eq_sparse_lasso_first}} \stwonorm{\tbf{B} \tbf{a}^{(i)} - \what{\tbf{B}} \tbf{a}^{(i)}} \\
& \sless{(a)} \opnorm{\tbf{B} - \what{\tbf{B}}} \stwonorm{\tbf{a}^{(i)}} \\
& \sless{(b)} \bigparenth{\sqrt{p} \matnorm{\tbf{B} - \what{\tbf{B}}}_{2,\infty}} \cdot  \bigparenth{\sqrt{p_v+1} \sinfnorm{\tbf{a}^{(i)}}} \sless{(c)} \aGM \varepsilon_1\sqrt{(p_v+1)p},
\label{eq_bounded_zeta} 
\end{align}
where $(a)$ follows from sub-multiplicativity of induced matrix norms, $(b)$ follows from standard matrix norm inequalities, and $(c)$ follows from \cref{eq_b_estimate_guarantee} and because the measurement errors are bounded by $\aGM$.
%
%
%
%
%
%
%
%
%
%
%
%
%
%

Then, performing an analysis similar to one in \cref{sec_proof_thm_node_parameters_recovery} while using the bound on $n$ in \cref{eq_theta_estimate_guarantee} instead of the one in \cref{eq:matrix_guarantee}, and using \cref{exam:lc_dense}, we obtain estimates $\EstimatedExternalFieldI[1], \cdots, \EstimatedExternalFieldI[n/2]$ such that (see \cref{cor_params}\cref{item:lc_dense} for reference), with probability at least $1 - \delta$, we have
\begin{align}
\max_{i\in[n/2]}
\mathrm{MSE}(\EstimatedExternalFieldI, \TrueExternalFieldI)
&\leq \max\Bigbraces{\varepsilon_1^2, \dfrac{ce^{c'\bGM} \bigparenth{p_v + \log \normalparenth{\log \frac{np}{\delta}}}}{p}}, 
%
%
\label{eq_mse_application}
\end{align}
whenever $n \geq ce^{c'\bGM} \varepsilon_1^{-4} p \bigparenth{\log \frac{np}{\delta} + p_v}$.


 
%
%
%
%
%
%
%
%
%
%
%

\paragraph{Recover the measurement error} We condition on the event \cref{eq_mse_application} happening and note that the above estimate $\EstimatedExternalFieldI$ of the unit-level parameter $\TrueExternalFieldI$ is of the form $\EstimatedExternalFieldI  = \what{\tbf{B}} \what{\tbf{a}}^{(i)}$ for $i \in [n/2]$. We declare $\what{\tbf{a}}^{(i)}$ as our estimate of the measurement error for unit $i \in [n/2]$ and prove the corresponding guarantee below.\\

\noindent  Fix any $i \in [n/2]$. From \cref{eq_sparse_lasso_third} and triangle inequality, we find that
\begin{align}
\stwonorm{\TrueExternalFieldI - \EstimatedExternalFieldI} & = \stwonorm{\what{\tbf{B}} \tbf{a}^{(i)} + \what{\tbf{B}} \zeta \!-\! \what{\tbf{B}} \what{\tbf{a}}^{(i)} } \geq \stwonorm{\what{\tbf{B}} \tbf{a}^{(i)} - \what{\tbf{B}} \what{\tbf{a}}^{(i)}} \!-\! \stwonorm{\what{\tbf{B}} \zeta}.\label{eq_lower_bound_theta_error}
\end{align}
%
%
Then, doing standard algebra with \cref{eq_lower_bound_theta_error} yields that
\begin{align}
\mathrm{MSE}(\EstimatedExternalFieldI, \TrueExternalFieldI) + \frac{\stwonorm{\what{\tbf{B}} \zeta}^2}{p} & \geq \frac{\stwonorm{\what{\tbf{B}} \tbf{a}^{(i)} - \what{\tbf{B}} \what{\tbf{a}}^{(i)}}^2}{2p} = \frac{(\tbf{a}^{(i)} - \what{\tbf{a}}^{(i)})\tp \what{\tbf{B}}\tp \what{\tbf{B}} (\tbf{a}^{(i)} - \what{\tbf{a}}^{(i)})}{2p}.\label{eq_a_bound_0}
\end{align}
Combining \cref{eq_bounded_zeta,eq_mse_application,eq_a_bound_0} with the choice $\varepsilon_1 = \kappa \varepsilon_2/\aGM \sqrt{p_v + 1}$, we have
\begin{align}
\frac{(\tbf{a}^{(i)} \!-\! \what{\tbf{a}}^{(i)})\tp \what{\tbf{B}}\tp \!\what{\tbf{B}} (\tbf{a}^{(i)} \!-\! \what{\tbf{a}}^{(i)})}{2p} \!\leq\! \max\Bigbraces{\frac{\varepsilon_2^2\kappa^2}{\aGM^2(p_v+1)}\!, \dfrac{ce^{c'\bGM} \bigparenth{p_v \!+\! \log \normalparenth{\log \frac{np}{\delta}}}}{p}} \!+\! \varepsilon_2^2 \kappa^2,  
%
%
\label{eq_a_bound_2}
\end{align}
uniformly for all $i \in [n/2]$, with probability at least $1 - \delta$, whenever $n \geq ce^{c'\bGM} \kappa^{-4} \varepsilon_2^{-4} (p_v\!+\!1)^2 p {\log \frac{np}{\delta}}$. Next, we claim that the eigenvalues of $\what{\tbf{B}}\tp \what{\tbf{B}}$ can be lower bounded by $\kappa p / 2$ whenever $\varepsilon_2 \leq \sqrt{p/(p_v+1)}/8$. Taking this claim as given at the moment, we continue our proof. We have
\begin{align}
\frac{\kappa} {4}\stwonorm{\tbf{a}^{(i)} - \what{\tbf{a}}^{(i)}}^2 \leq  \frac{(\tbf{a}^{(i)} - \what{\tbf{a}}^{(i)})\tp \what{\tbf{B}}\tp \what{\tbf{B}} (\tbf{a}^{(i)} - \what{\tbf{a}}^{(i)})}{2p} \qtext{whenever} \varepsilon_2 \leq  \frac{1}{8} \sqrt{\frac{p}{p_v+1}}, \label{eq_a_bound_1}
\end{align}
uniformly for all $i \in [n/2]$. Combining \cref{eq_a_bound_2,eq_a_bound_1} completes the proof.\\

\noindent It remains to show that the eigenvalues of $\what{\tbf{B}}\tp \what{\tbf{B}}$ can be lower bounded by $\kappa p / 2$ conditioned on \cref{eq_theta_estimate_guarantee}. For any matrix $\tbf{M}$, let $\lambda_{\max}(\tbf{M})$ and $\lambda_{\min}(\tbf{M})$ denote the largest and the smallest eigenvalues of $\tbf{M}$, respectively. Then from Weyl's inequality \citep[Theorem. 8.2]{bhatia2007perturbation}, we have
\begin{align}
\lambda_{\min}(\what{\tbf{B}}\tp \what{\tbf{B}}) \geq \lambda_{\min}({\tbf{B}}\tp {\tbf{B}}) - \lambda_{\max}( {\tbf{B}}\tp {\tbf{B}} - \what{\tbf{B}}\tp \what{\tbf{B}} ) \sgreat{(a)} \kappa p - \lambda_{\max}( {\tbf{B}}\tp {\tbf{B}} - \what{\tbf{B}}\tp \what{\tbf{B}} ),
\end{align}
where $(a)$ follows from the assumption on the eigenvalues of ${\tbf{B}}\tp {\tbf{B}}$. 
%
Now, it suffices to upper bound $\lambda_{\max}( {\tbf{B}}\tp {\tbf{B}} - \what{\tbf{B}}\tp \what{\tbf{B}} )$ by $\kappa p/ 2$. We have 
\begin{align}
\bigabs{ \lambda_{\max}({\tbf{B}}\tp {\tbf{B}} - \what{\tbf{B}}\tp \what{\tbf{B}}) } & \sequal{(a)} \opnorm{ {\tbf{B}}\tp {\tbf{B}} - \what{\tbf{B}}\tp \what{\tbf{B}} } \\
&  \sless{(b)} (p_v+1) \maxmatnorm{ {\tbf{B}}\tp {\tbf{B}} - \what{\tbf{B}}\tp \what{\tbf{B}} } \\
& \sless{(c)} (p_v+1) \Bigparenth{\maxmatnorm{ {\tbf{B}}\tp \bigparenth{\tbf{B} - \what{\tbf{B}}}} + \maxmatnorm{ {\bigparenth{\tbf{B} - \what{\tbf{B}}}\tp\what{\tbf{B}} } }} \\& \sless{(d)} (p_v+1) \bigparenth{\matnorm{ \tbf{B}\tp}_{2,\infty} + \matnorm{ \what{\tbf{B}}\tp}_{2,\infty}}\matnorm{ \bigparenth{\tbf{B} - \what{\tbf{B}}}\tp}_{2,\infty}  \\
& \sless{(e)} (p_v+1) (2\aGM\sqrt{p} + 2\aGM\sqrt{p}) \cdot \varepsilon_1 \sless{(f)} 4 \kappa \varepsilon_2 \sqrt{p_v+1}\sqrt{p} \sless{(g)} \frac{\kappa p}{2}, 
\end{align}
where $(a)$ follows because ${\tbf{B}}\tp {\tbf{B}} - \what{\tbf{B}}\tp \what{\tbf{B}} $ is symmetric, $(b)$ follows from because $\opnorm{\tbf{M}} \leq \fronorm{\tbf{M}} \leq d \maxmatnorm{\tbf{M}}$ for any square matrix $\tbf{M} \in \Reals^{d \times d}$, $(c)$ follows from the triangle inequality, $(d)$ follows by Cauchyâ€“Schwarz inequality, $(e)$ follows because $\maxmatnorm{\what{\tbf{B}}} \leq 2\aGM$, $\maxmatnorm{{\tbf{B}}} \leq 2\aGM$ (because of the assumptions in \cref{subsec_theo_gua}), and from \cref{eq_theta_estimate_guarantee,eq_appl_b_defn}, $(f)$ follows from the choice of $\varepsilon_1$, and $(g)$ follows whenever $\varepsilon_2 \leq \frac{1}{8} \sqrt{\frac{p}{p_v+1}}$.

%


%

%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
