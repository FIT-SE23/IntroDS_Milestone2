\section{Proof of {Theorem \ref{thm_causal_estimand}}: \outcomemainresultname}
\label{sec_proof_causal_estimand}
%
\newcommand{\hpsi}{\what{\psi}}
\newcommand{\tpsi}{\wtil{\psi}}
\newcommand{\spsi}{\psi^{\star}}
\newcommand{\hPsi}{\what{\Psi}}
\newcommand{\tPsi}{\wtil{\Psi}}
\newcommand{\sPsi}{\Psi^{\star}}
Fix any unit $i \in [n]$ and an alternate intervention $\wtil{\svba}^{(i)} \in \cA^{p_a}$. Then, we have
\begin{align}
\mu^{(i)}(\wtil{\svba}^{(i)}) & \sequal{\cref{eq_causal_estimand}} \Expectation[\svby^{(i)}(\wtil{\svba}^{(i)}) | \rvbz = \svbz^{(i)}, \rvbv = \svbv^{(i)}] \sequal{(a)} \Expectation[\rvby | \rvba = \wtil{\svba}^{(i)}, \rvbz = \svbz^{(i)}, \rvbv = \svbv^{(i)}],
\end{align}
where $(a)$ follows because the unit-level counterfactual distribution is equivalent to unit-level conditional distribution under the causal framework considered as described in \cref{subsec_causal_mech}. To obtain a convenient expression for $\Expectation[\rvby | \rvba = \wtil{\svba}^{(i)}, \rvbz = \svbz^{(i)}, \rvbv = \svbv^{(i)}]$, we identify $\TruePhi[u,y] \in \Reals^{p_u \times p_y}$ to be the component of $\TrueParameterMatrix$ corresponding to $\rvbu$ and $\rvby$ for all $\rvbu \in \{\rvbv, \rvba, \rvby\}$ and $\TrueExternalFieldI[i,y] \in \Reals^{p_y}$ to be the component of $\TrueExternalFieldI$ corresponding to $\rvby$. Then, the conditional distribution of $\rvby$ as a function of the interventions $\rvba$, while keeping $\rvbv$ and $\rvbz$ fixed at the corresponding realizations for unit $i$, i.e., $\svbv^{(i)}$ and $\svbz^{(i)}$, respectively, can be written as
\begin{align}
f^{(i)}_{\rvby | \rvba}(\svby | \svba) \propto \exp\Bigparenth{\bigbrackets{\TrueExternalFieldI[i,y] + 2\svbv^{(i)\top}\TruePhi[v,y] + 2\svba\tp\TruePhi[a,y]} \svby + \svby\tp\TruePhi[y,y] \svby}. \label{eq_conditional_distribution_y_alternate}
\end{align}
Therefore, we have
\begin{align}
\Expectation[\rvby | \rvba = \wtil{\svba}^{(i)}, \rvbz = \svbz^{(i)}, \rvbv = \svbv^{(i)}] = \Expectation_{f^{(i)}_{\rvby | \rvba}}[\rvby | \rvba = \wtil{\svba}^{(i)}].
\end{align}
\noindent Now, consider the $p_u$ dimensional random vector $\rvbu$ supported on $\cX^{p_u}$ with distribution $f_{\rvbu}$ parameterized by   $\psi \in \Reals^{p_y}$ and $\Psi \in \Reals^{p_y \times p_y}$ as follows
\begin{align}
f_{\rvbu}(\svbu | \psi, \Psi) \propto \exp(\psi\tp \svbu+ \svbu\tp\Psi\svbu). \label{eq_dist_u} 
\end{align}
Then, note that $\what{f}^{(i)}_{\rvby | \rvba}(\svby | \svba)$ in \cref{eq_counterfactual_distribution_y} and $f^{(i)}_{\rvby | \rvba}(\svby | \svba)$ in \cref{eq_conditional_distribution_y_alternate} belong to the set $\normalbraces{f_{\rvbu}(\cdot | \psi, \Psi): \psi \in \Reals^{p_y}, \Psi \in \Reals^{p_y \times p_y}}$ for some $\psi$ and $\Psi$. Now, we consider any two distributions in this set, namely $f_{\rvbu}(\svbu | \hpsi, \hPsi)$ and $f_{\rvbu}(\svbu | \spsi, \sPsi)$. Then, we claim that the two norm of the difference of the mean vectors of these distributions is bounded as below. We provide a proof at the end.
\newcommand{\expfamperturbationresultname}{Perturbation in the mean vector}
\begin{lemma}[{\expfamperturbationresultname}]
	\label{lemma_exp_fam_parameter_perturbation}
	For any $\psi \in \Reals^{p_y}$ and $\Psi \in \Reals^{p_y \times p_y}$, let $\mu_{\psi, \Psi}(\rvbu) \in \Reals^{p_u}$ and $\Covariance_{\psi, \Psi}(\rvbu, \rvbu) \in \Reals^{p_u \times p_u}$ denote the mean vector and the covariance matrix of $\rvbu$, respectively, with respect to $f_{\rvbu}$ in \cref{eq_dist_u}. Then, for any $\hpsi, \spsi \in \Reals^{p_y}$ and $\hPsi, \sPsi \in \Reals^{p_y \times p_y}$, there exists some $t \in (0,1)$, $\tpsi \defn t \hpsi + (1-t) \spsi$ and $\tPsi \defn t \tpsi + (1-t) \tpsi$ such that
	\begin{align}
	\stwonorm{ \mu_{\hpsi, \hPsi}(\rvbu) - \mu_{\spsi, \sPsi}(\rvbu)} & \leq    \opnorm{\Covariance_{\tpsi, \tPsi}(\rvbu, \rvbu)}\stwonorm{(\hpsi - \spsi)} \\
 & \qquad + \sum_{t_3 \in [p]}  \opnorm{\Covariance_{\tpsi, \tPsi}(\rvbu, \rvu_{t_3} \rvbu)}\stwonorm{(\hPsi_{t_3} \!-\! \sPsi_{t_3})}.
	\end{align}
\end{lemma}
%
%
%
%
%
%
Given this lemma, we proceed with the proof. By applying this lemma to $\what{f}^{(i)}_{\rvby | \rvba}(\svby | \svba)$ in \cref{eq_counterfactual_distribution_y} and $f^{(i)}_{\rvby | \rvba}(\svby | \svba)$ in \cref{eq_conditional_distribution_y_alternate}, we see that it is sufficient to show the following bound
\begin{align}
& \stwonorm{\normalparenth{\TrueExternalFieldI[i,y] - \EstimatedExternalFieldI[i,y]} + 2\svbv^{(i)\top}\!\normalparenth{\TruePhi[v,y] - \EstimatedPhi^{(v,y)} } + 2\wtil{\svba}^{(i)\top}\!\normalparenth{\TruePhi[a,y] - \EstimatedPhi^{(a,y)}}}  \\
& \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad + \sum_{t \in [p_y]} \stwonorm{\TruePhi[y,y]_{t} - \EstimatedPhi^{(y,y)}_t} \leq R(\varepsilon, \delta/n) + p \varepsilon.
\end{align}
To that end, we have
\begin{align}
\sum_{t \in [p_y]} \stwonorm{\TruePhi[y,y]_{t} - \EstimatedPhi^{(y,y)}_t} \sless{(a)} \sum_{t \in [p_y]} \stwonorm{\TrueParameterRowt -\EstimatedParameterRowt},
%
\label{eq_mean_outcome_part_1}
\end{align}
where $(a)$ follows because $\ell_2$ norm of any sub-vector is no more than $\ell_2$ norm of the vector.
%
Similarly, we have
\begin{align}
%
%
%
& \stwonorm{\normalparenth{\TrueExternalFieldI[i,y] \!-\! \EstimatedExternalFieldI[i,y]} \!+ \!2\svbv^{(i)\top}\normalparenth{\TruePhi[v,y] \!-\!\EstimatedPhi^{(v,y)} } \!+\! 2\wtil{\svba}^{(i)\top}\normalparenth{\TruePhi[a,y] \!-\! \EstimatedPhi^{(a,y)}}}\\
& \sless{(a)}  \stwonorm{\TrueExternalFieldI[i,y] \!-\! \EstimatedExternalFieldI[i,y]} \!+\! 2\stwonorm{\svbv^{(i)\top}\normalparenth{\TruePhi[v,y] \!-\!\EstimatedPhi^{(v,y)}}} \!+\! 2\stwonorm{\wtil{\svba}^{(i)\top}\normalparenth{\TruePhi[a,y] \!-\! \EstimatedPhi^{(a,y)}}}\\
& \sless{(b)}  \stwonorm{\TrueExternalFieldI[i,y] \!-\! \EstimatedExternalFieldI[i,y]} \!+\! 2\stwonorm{\svbv^{(i)}}\opnorm{\TruePhi[v,y] \!-\!\EstimatedPhi^{(v,y)}} \!+\! 2\stwonorm{\wtil{\svba}^{(i)}}\opnorm{\normalparenth{\TruePhi[a,y] \!-\! \EstimatedPhi^{(a,y)}}}\\
& \sless{(c)}  \stwonorm{\TrueExternalFieldI[i] \!-\! \EstimatedExternalFieldI[i]} + 2\Bigparenth{\stwonorm{\svbv^{(i)}} + \stwonorm{\wtil{\svba}^{(i)}}} \opnorm{\TrueParameterMatrix -\EstimatedParameterMatrix}\\
& \sless{(d)}  \stwonorm{\TrueExternalFieldI[i] \!-\! \EstimatedExternalFieldI[i]} + 2\Bigparenth{\stwonorm{\svbv^{(i)}} + \stwonorm{\wtil{\svba}^{(i)}}} \onematnorm{\TrueParameterMatrix -\EstimatedParameterMatrix} \\
& \sless{(e)}  \stwonorm{\TrueExternalFieldI[i] \!-\! \EstimatedExternalFieldI[i]} + 2\xmax \bigparenth{\sqrt{p_v} + \sqrt{p_a}} \onematnorm{\TrueParameterMatrix -\EstimatedParameterMatrix}, \label{eq_mean_outcome_part_2}
\end{align}
where $(a)$ follows from triangle inequality, $(b)$ follows because induced matrix norms are submultiplicative, $(c)$ follows because operator norm of any sub-matrix is no more than operator norm of the matrix and $\ell_2$ norm of any sub-vector is no more than $\ell_2$ norm of the vector, $(d)$ follows because $\TrueParameterMatrix -\EstimatedParameterMatrix$ is symmetric and because matrix operator norm is bounded by square root of the product of matrix one norm and matrix infinity norm, and $(e)$ follows because $\max\{\sinfnorm{\svbv^{(i)}}, \sinfnorm{\svba^{(i)}}\} \leq \xmax$ for all $i \in [n]$.\\

\noindent Now, combining \cref{eq_mean_outcome_part_1,eq_mean_outcome_part_2}, we have
\begin{align}
& \stwonorm{\normalparenth{\TrueExternalFieldI[i,y] \!\!-\! \EstimatedExternalFieldI[i,y]} \!+\! 2\svbv^{(i)\top}\!\normalparenth{\TruePhi[v,y] \!\!-\! \EstimatedPhi^{(v,y)} } \!+\! 2\wtil{\svba}^{(i)\top}\!\normalparenth{\TruePhi[a,y] \!\!-\! \EstimatedPhi^{(a,y)}}} \!+\!\! \!\!\sum_{t \in [p_y]} \!\!\!\stwonorm{\TruePhi[y,y]_{t} \!\!-\! \EstimatedPhi^{(y,y)}_t} \\
& \leq \stwonorm{\TrueExternalFieldI[i] \!\!-\! \EstimatedExternalFieldI[i]} \!+\! 2\xmax \bigparenth{\sqrt{p_v} \!+\! \sqrt{p_a}} \onematnorm{\TrueParameterMatrix \!\!-\!\EstimatedParameterMatrix} \!+\! \sum_{t \in [p_y]} \stwonorm{\TrueParameterRowt \!\!-\!\EstimatedParameterRowt}\\
& \sless{(a)}  R(\varepsilon, \delta/n) + 2\xmax \bigparenth{\sqrt{p_v} + \sqrt{p_a}} \sqrt{p} \varepsilon + p_y \varepsilon,
\end{align}
and $(a)$ follows from \cref{theorem_parameters} by using the relationship between vector norms. The proof is complete by rescaling $\varepsilon$ and absorbing the constants in $c$.

\paragraph{Proof of \cref{lemma_exp_fam_parameter_perturbation}: \expfamperturbationresultname}
Let $Z(\psi, \Psi) \in \Reals_{+}$ denote the log-partition function of $f_{\rvbu}(\cdot | \psi, \Psi) $ in \cref{eq_dist_u}. Then, from \cite[Theorem 1]{BusaFSZ2019}, we have
\begin{align}
\stwonorm{ \mu_{\hpsi, \hPsi}(\rvbu) - \mu_{\spsi, \sPsi}(\rvbu)} = \stwonorm{\nabla_{\hpsi} Z(\hpsi, \hPsi) - \nabla_{\spsi} Z(\spsi, \sPsi)}. \label{eq_mean_vector_difference}
\end{align}
For $t_1, t_2, t_3 \in [p]$, consider $\frac{\partial^2 Z(\psi, \Psi)}{\partial \psi_{t_1}\partial \psi_{t_2}}$ and $\frac{\partial^2 Z(\psi, \Psi)}{\partial \psi_{t_1}\partial \Psi_{t_2, t_3}}$. Using the fact that the Hessian of the log partition function of any regular exponential family is the covariance matrix of the associated sufficient statistic, we have
\begin{align}
\frac{\partial^2 Z(\psi, \Psi)}{\partial \psi_{t_1}\partial \psi_{t_2}} = \Covariance_{\psi, \Psi}(\rvu_{t_1}, \rvu_{t_2}) \qtext{and} \frac{\partial^2 Z(\psi, \Psi)}{\partial \psi_{t_1}\partial \Psi_{t_2, t_3}} = \Covariance_{\psi, \Psi}(\rvu_{t_1}, \rvu_{t_2} \rvu_{t_3}). \label{eq_exp_fam_hessian_cov}
\end{align}
Now, for some $c \in (0,1)$, $\tpsi \defn c \hpsi + (1-c) \spsi$ and $\tPsi \defn c \tpsi + (1-c) \tpsi$, we have the following from the mean value theorem
\begin{align}
& \frac{\partial Z(\hpsi, \hPsi)}{\partial \hpsi_{t_1}} \!-\!  \frac{\partial Z(\spsi, \sPsi)}{\partial \spsi_{t_1}} \\
& \!=\! \sum_{t_2 \in [p]}\frac{\partial^2 Z(\tpsi, \tPsi)}{\partial \tpsi_{t_2}\partial \tpsi_{t_1}} \cdot (\hpsi_{t_2} - \spsi_{t_2}) + \sum_{t_2 \in [p]} \sum_{t_3 \in [p]} \frac{\partial^2 Z(\tpsi, \tPsi)}{\partial \tPsi_{t_2, t_3} \partial \tpsi_{t_1}} \cdot (\hPsi_{t_2, t_3} - \sPsi_{t_2, t_3})\\
& \!\!\!\sequal{\cref{eq_exp_fam_hessian_cov}} \!\!\sum_{t_2 \in [p]}\!\! \Covariance_{\tpsi, \tPsi}(\rvu_{t_1}, \rvu_{t_2}) 
\!\cdot\! (\hpsi_{t_2} \!-\! \spsi_{t_2}) \!+\!\!\! \sum_{t_3 \in [p]} \!\sum_{t_2 \in [p]} \!\!\Covariance_{\tpsi, \tPsi}(\rvu_{t_1}, \rvu_{t_3} \rvu_{t_2}) \!\cdot\!(\hPsi_{t_3, t_2} \!-\! \sPsi_{t_3, t_2}).
\end{align}
Now, using the triangle inequality and sub-multiplicativity of induced matrix norms, we have
\begin{align}
\stwonorm{\nabla_{\hpsi} Z(\hpsi, \hPsi) \!-\! \nabla_{\spsi} Z(\spsi, \sPsi)} & \leq \opnorm{\Covariance_{\tpsi, \tPsi}(\rvbu, \rvbu)}\stwonorm{(\hpsi \!-\! \spsi)} \\
& \qquad \qquad \qquad + \sum_{t_3 \in [p]} \! \opnorm{\Covariance_{\tpsi, \tPsi}(\rvbu, \rvu_{t_3} \rvbu)}\stwonorm{(\hPsi_{t_3} \!-\! \sPsi_{t_3})}.  \label{eq_mean_value_theorem}
\end{align}
%
%
%
%
%
%
%
%
%
%
%
%
Combining \cref{eq_mean_vector_difference,eq_mean_value_theorem} completes the proof.


\subsection{Bounded operator norms for perturbations in the parameters}
\label{subsec_bounded_op_norms}
In \cref{subsec_guarantee_outcome_estimate}, we assumed the operator norms of (i) the covariance matrix of $\rvby$ conditioned on $\rvba$, $\rvbz$, and $\rvbv$ and (ii) the cross-covariance matrix of $\rvby$ and $\rvy_t \rvby$ conditioned on $\rvba$, $\rvbz$, and $\rvbv$ for all $t \in [p_y]$ to remain bounded for small perturbation in the parameters. In this section, we provide examples where these hold. 
%

Suppose the distribution of $\rvby$ conditioned on $\rvba$, $\rvbz$, and $\rvbv$ is a Gaussian distribution. For simplicity, let the mean of this distribution be zero. Then, for any $t,u,v \in [p_y]$,
\begin{align}
\Covariance_{\ExternalField,\ParameterMatrix}(\rvy_u, \rvy_t \rvy_v | {\svba}, \svbz, \svbv) = \Expectation_{\ExternalField,\ParameterMatrix}(\rvy_u \rvy_t \rvy_v | {\svba}, \svbz, \svbv) \sequal{(a)}  0.
\end{align}
where $(a)$ follows because $\Expectation_{\ExternalField,\ParameterMatrix}(\rvy_u \rvy_t \rvy_v | {\svba}, \svbz, \svbv)$ is the third cumulant of $\rvy_u \rvy_t \rvy_v | \rvba, \rvbz, \rvbv$ and the third cumulant for any Gaussian distribution is zero \citep{holmquist1988moments}. Then,
\begin{align}
\max\limits_{t \in [p_y]} \opnorm{\Covariance_{\ExternalField,\ParameterMatrix}(\rvby, \rvy_t \rvby | {\svba}, \svbz, \svbv)} = 0. \label{eq_op_norm_cross_zero}
\end{align}
Further, \cref{eq_op_norm_cross_zero} also holds for small perturbations in $\ExternalField$ and $\ParameterMatrix$ as the distribution of $\rvby$ conditioned on $\rvba$, $\rvbz$, and $\rvbv$ would still be a Gaussian distribution. 

Now, we bound $\opnorm{\Covariance_{\ExternalField,\ParameterMatrix}(\rvby, \rvby | {\svba}, \svbz, \svbv)}$ under additional conditions. For simplicity, suppose $\Variance_{\ExternalField,\ParameterMatrix}(\rvy_t| {\svba}, \svbz, \svbv) = 1$ for all $t \in [p_y]$. Further, suppose the (undirected) graphical structure associated with elements of $\rvby$, i.e., $\rvy_{1}, \cdots, \rvy_{p_y}$, is a chain (This would be true for the motivating example in \cref{fig_graphical_models}(a)). If the correlation between any two elements of $\rvby$ connected by an edge in the tree is equal to $\rho \in [0,1]$ (This is equivalent to all the off-diagonal non-zero entries of $\ParameterMatrix$ being the same), then for any $u,v \in [p_y]$,
\begin{align}
\Covariance_{\ExternalField,\ParameterMatrix}(\rvy_u, \rvy_v | {\svba}, \svbz, \svbv)  \sequal{(a)}  \rho^{\normalabs{u-v}},
\end{align}
where $(a)$ follows by the correlation decay property for Gaussian tree models \citep[Equation. 18]{tan2010learning}. Then, for any $0 \leq \rho < 1$
\begin{align}
\opnorm{\Covariance_{\ExternalField,\ParameterMatrix}(\rvby, \rvby | {\svba}, \svbz, \svbv)} \sless{(a)} \frac{1+\rho}{1-\rho}, \label{eq_op_norm_cov_zero}
\end{align}
where $(a)$ follows from \cite{trench1999asymptotic}. Further, \cref{eq_op_norm_cov_zero} holds for small perturbations in $\ExternalField$ and $\ParameterMatrix$ as long as $\rho < 1$. Therefore, $C(\mbb B)$ in \cref{eq_cov_constraint} is a constant (with respect to $p$) for small perturbations in $\ExternalField$ and $\ParameterMatrix$.

While we showed that $C(\mbb B)$ is a constant for a class of Gaussian distributions, we except similar results for truncated Gaussian distributions and exponential family distributions in \cref{eq_conditional_distribution_y}.

%
