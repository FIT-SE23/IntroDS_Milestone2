\section{Logarithmic Sobolev inequality and tail bounds} 
\label{section_lsi_tail_bounds}
%
In this section, we present two results which may be of independent interest. First, we show that a random vector supported on a compact set satisfies the logarithmic Sobolev inequality (to be defined) if it satisfies the Dobrushin's uniqueness condition (to be defined). This result is a generalization of the result in \cite{Marton2015} for discrete random vectors to continuous random vectors supported on a compact set. Next, we show that if a random vector satisfies the logarithmic Sobolev inequality, then any arbitrary function of the random vector concentrates around its mean. This result is a generalization of the result in \cite{DaganDDA2021}  for discrete random vectors to continuous random vectors.\\

\noindent Throughout this section, we consider a $p$-dimensional random vector $\rvbx$ supported on $\cX^p$ with distribution $f_{\rvbx}$ where $p \geq 1$. We start by defining the logarithmic Sobolev inequality (LSI). We use the convention $0\log 0=0$.

\begin{definition}[{Logarithmic Sobolev inequality}]\label{def_lsi}
	A random vector $\rvbx$ satisfies the logarithmic Sobolev inequality with constant $\sigma^2 > 0$ (abbreviated as $\LSI{\rvbx}{\sigma^2}$) if
	\begin{align}
	\Ent{\rvbx}{q^2} \leq \sigma^2 \Expectation_{\rvbx}\Bigbrackets{\twonorm{\nabla_{\rvbx} q(\rvbx)}^2} \qtext{for all} q : \cX^p \to \Reals, \label{eq_LSI_definition}
	\end{align}
	where $\Ent{\rvbx}{g}\!\defn\! \Expectation_{\rvbx}[g(\rvbx) \log g(\rvbx)] \!-\!\Expectation_{\rvbx}[g(\rvbx)] \log \Expectation_{\rvbx}[g(\rvbx)]$ denotes the entropy of the function $g\! :\! \cX^p \!\to\! \real_{+}$.
	%
	%
	%
	%
	%
\end{definition}
\vspace{2mm}
\noindent  
%
Next, we state the Dobrushin's uniqueness condition. For any distributions $f$ and $g$, let $\TV{f}{g}$ denote the total variation distance between $f$ and $g$.
\begin{definition}[{Dobrushin's uniqueness condition}]\label{def_dobrushin_condition}
	A random vector $\rvbx$ satisfies the Dobrushin's uniqueness condition with coupling matrix $\ParameterMatrix \in \Reals_+^{p \times p}$ if
	%
	%
	$\opnorm{\ParameterMatrix} < 1$, and
	%
	for every $t \in [p], u \in [p] \!\setminus\! \{t\}$, and $\svbx_{-t}, \tsvbx_{-t} \in \cX^{p-1}$ differing only in the $u^{th}$ coordinate,
	\begin{align}
	\TV{f_{\rvx_t | \rvbx_{-t} = \svbx_{-t}}}{f_{\rvx_t | \rvbx_{-t} = \tsvbx_{-t}}} \leq \ParameterTU[tu]. \label{eq_dob_tv_bound}
	\end{align}
	%
\end{definition}
\noindent We note that the Dobrushin's uniqueness condition, as originally stated (see \cite{Marton2015}) for Ising model, also requires $\ParameterTU[tt] = 0$ for all $t \in [p]$. This condition makes sense for Ising model where $\rvx_t^2 = 1$ for all $t \in [p]$. However, this is not true for continuous random vectors necessitating a need for modification in the condition.\\

\vspace{2mm}
\noindent From hereon, we let $\cX^p$ be compact unless otherwise specified. Moreover, we define
\begin{align}
\label{eq:smin}   
f_{\min} \defeq \min_{t \in [p], \svbx \in \cX^p} f_{\rvx_t | \rvbx_{-t}}(x_t | \svbx_{-t}).
\end{align}
%
%
\noindent Now, we provide the first main result of this section with a proof in \cref{subsec_proof_lsi}.

\newcommand{\lsiresultname}{Logarithmic Sobolev inequality}
\begin{proposition}[{\lsiresultname}]\label{thm_LSI_main}
	If a random vector $\rvbx$ with $f_{\min}>0$ (see \cref{eq:smin}) satisfies (a) the Dobrushin's uniqueness condition (\cref{def_dobrushin_condition}) with coupling matrix $\ParameterMatrix \in \Reals_+^{p \times p}$, and (b) $\rvx_t | \rvbx_{-t}$ satisfies $\LSI{\rvx_t | \rvbx_{-t} = \svbx_{-t}}{\sigma^2}$ for all $t \in [p]$ and $\svbx_{-t} \in \cX^{p-1}$ (see \cref{def_lsi}), then it satisfies 
	%
	$\mathrm{LSI}_{\rvbx}(2\sigma^2/(f_{\min}(1-\opnorm{\ParameterMatrix})^2))$.
	%
	%
	%
	%
	%
	%
	%
	%
\end{proposition}
\noindent Next, we define the notion of pseudo derivative and pseudo Hessian that come in handy in our proofs for providing upper bounds on the norm of the derivative and the Hessian.

\begin{definition}[{Pseudo derivative and Hessian}]\label{def_pseudo_der_hes}
	For a function $q : \cX^p \to \Reals$, the functions $\tnabla q : \cX^p \to \Reals^{p_1}$ and $\tnabla^2 q : \cX^p \to \Reals^{p_1 \times p_2}$  ($p_1,p_2 \geq 1$) are, respectively, called a pseudo derivative and a pseudo Hessian for $q$ if for all $\svby \in \cX^p$ and $\rho \in \Reals^{p_1 \times 1}$, we have
	\begin{align}
	\stwonorm{\tnabla q(\svby)} \geq  \stwonorm{\nabla q(\svby)}
	\qtext{and}
	\stwonorm{\rho\tp \tnabla^2 q(\svby)} \geq  \stwonorm{\nabla \bigbrackets{\rho\tp \tnabla q(\svby)}}. 
	%
	\label{eq:pseudo_Hessian}
	%
	\end{align}
	%
	%
	%
	%
\end{definition}

\noindent Finally, we provide the second main result of this section with a proof in \cref{subsec_proof_main_concentration}.

\newcommand{\mainconcresultname}{Tail bounds for arbitrary functions under LSI}
\begin{proposition}[{\mainconcresultname}]\label{thm_main_concentration}
	Given a random vector $\rvbx$ satisfying $\LSI{\rvbx}{\sigma^2}$, any function $q :\cX^p \to \Reals$ with a pseudo derivative $\tnabla q$ and pseudo Hessian $\tnabla^2 q$ (see \cref{def_pseudo_der_hes}), $\rvbx$ satisfies a tail bound, namely for any fixed $\varepsilon > 0$, we have
	\begin{align}
	\Probability\Bigbrackets{\bigabs{q_c(\rvbx)} \!\geq\! \varepsilon} \!\leq\! \exp\biggparenth{\! \frac{-c}{\sigma^4} \min \Bigparenth{\frac{\varepsilon^2}{ \Expectation\bigbrackets{\stwonorm{\tnabla q(\rvbx)}}^2 \!+\! \max\limits_{\svbx \in \cX^p} \fronorm{\tnabla^2 q(\svbx)}^2}, \frac{\varepsilon}{\max\limits_{\svbx \in \cX^p} \opnorm{\tnabla^2 q(\svbx)}}}},
	\end{align}
	where $q_c(\rvbx) = q(\rvbx) - \Expectation\bigbrackets{q(\rvbx)}$ and $c$ is a universal constant.
\end{proposition}

\subsection{Proof of \cref{thm_LSI_main}: \lsiresultname}
\label{subsec_proof_lsi}
We start by defining the notion of $W_2$ distance \citep{Marton2015} which is useful in the proof. We note that $W_2$ distance is a metric on the space of probability measures and satisfies triangle inequality.

\begin{definition}\cite[{$W_2$ distance}]{Marton2015}\label{def_w2_distance}
	For random vectors $\rvbx$ and $\rvby$ supported on $\cX^p$ with distributions $f$ and $g$, respectively, the $W_2$ distance is given by
	$ W_2^2(g_{\rvby}, f_{\rvbx}) \defeq \inf_{\pi} \sump \Bigbrackets{\Probability_{\pi}(\rvx_t \neq \rvy_t)}^2$,
	%
	%
	%
	where the infimum is taken over all couplings $\pi(\rvbx, \rvby)$ such that $\pi(\rvbx) = f(\rvbx)$ and $\pi(\rvby) = g(\rvby)$.
\end{definition}
\noindent Given \cref{def_w2_distance}, our next lemma states that if appropriate $W_2$ distances are bounded, then the KL divergence (denoted by $\KLD{\cdot}{\cdot}$) and the entropy approximately tensorize. We provide a proof in \cref{subsec_proof_lemma_tenorization_kld}.

\newcommand{\approxtensorofKLresultname}{Approximate tensorization of KL divergence and entropy}
\begin{lemma}[{\approxtensorofKLresultname}]\label{lemma_tenorization_kld}
	Given random vectors $\rvbx$ and $\rvby$ supported on $\cX^p$ with distributions $f$ and $g$, respectively, such that $f_{\min} > 0$ (see \cref{eq:smin}), if for all subsets $\set \subseteq [p]$ (with $\setC \defn [p] \setminus \set$) and all $\svby_{\setC} \in \cX^{p - |\set|}$,
	%
	\begin{align}
	W_2^2\bigparenth{g_{\rvby_{\set} | \rvby_{\setC} = \svby_{\setC}}, f_{\rvbx_{\set} | \rvbx_{\setC} = \svby_{\setC}}} \!\leq\! C \! \sumset \Expectation\Bigbrackets{\!
 \lVert g_{\rvy_t | \rvby_{-t} = \svby_{-t}} \!\!-\!\! f_{\rvx_t | \rvbx_{-t} = \svby_{-t}} \rVert_{\mathsf{TV}}^2 \Big| \rvby_{\setC} \!=\! \svby_{\setC}\!},
	%
	\label{eq_w2_distance_bounded_assumption}
	\end{align}
	almost surely for some constant $C \geq 1$, then 
	%
	%
	\begin{align}
	\KLD{g_{\rvby}}{f_{\rvbx}} &\leq \frac{2C}{f_{\min}} \sump \Expectation\bigbrackets{\KLD{g_{\rvy_t | \rvby_{-t} = \svby_{-t}}}{f_{\rvx_t | \rvbx_{-t} = \svby_{-t}}}},
	\qtext{and} 
	\label{eq_tensorization_kld}\\
	%
	%
	\Ent{\rvbx}{q} &\leq \frac{2C}{f_{\min}} \sump \Expectation_{\rvbx_{-t}}\bigbrackets{\Ent{\rvx_t | \rvbx_{-t}}{q}}
	\qtext{for any function $q : \cX^p \to \real_{+}$.} \label{eq_tensorization_entropy}
	\end{align}
\end{lemma}
\noindent Next, we claim that if the random vector $\rvbx$ satisfies Dobrushin's uniqueness condition, then the condition \cref{eq_w2_distance_bounded_assumption} of \cref{lemma_tenorization_kld} is naturally satisfied. We provide a proof in \cref{subsec_proof_lemma_dobrushin_implies_tensorization}.

\newcommand{\dobimpliesapproxtensorresultname}{Dobrushin's uniqueness implies approximate tensorization}

\begin{lemma}[{\dobimpliesapproxtensorresultname}]\label{lemma_dobrushin_implies_tensorization}
	Given random vectors $\rvbx$ and $\rvby$ supported on $\cX^p$ with distributions $f$ and $g$, respectively, if $\rvbx$ satisfies Dobrushin's uniqueness condition (see \cref{def_dobrushin_condition}) with coupling matrix $\ParameterMatrix \in \Reals^{p \times p}$, then for all subsets $\set \subseteq [p]$ (with $\setC \defn [p] \setminus \set$) and all $\svby_{\setC} \in \cX^{p - |\set|}$,
\begin{align}
 W_2^2\bigparenth{g_{\rvby_{\set} | \rvby_{\setC} = \svby_{\setC}}, f_{\rvbx_{\set} | \rvbx_{\setC} = \svby_{\setC}}} \!\leq\! C \! \sumset \Expectation\Bigbrackets{\!
 \lVert g_{\rvy_t | \rvby_{-t} = \svby_{-t}} \!\!-\!\! f_{\rvx_t | \rvbx_{-t} = \svby_{-t}} \rVert_{\mathsf{TV}}^2 \Big| \rvby_{\setC} \!=\! \svby_{\setC}\!}, 
	%
 \label{eq_w2_distance_bounded}
\end{align}
almost surely where $C = {\bigparenth{1\!-\!\opnorm{\ParameterMatrix}}^2}$.
\end{lemma}
\noindent 
%
Now to prove \cref{thm_LSI_main}, applying \cref{lemma_tenorization_kld,lemma_dobrushin_implies_tensorization} for an arbitrary function $f : \cX^p \to \Reals$, we find that
\begin{align}
\Ent{\rvbx}{q^2} & \leq \frac{2}{f_{\min}\bigparenth{1-\opnorm{\ParameterMatrix}}^2} \sump \Expectation_{\rvbx_{-t}}\Bigbrackets{\Ent{\rvx_t | \rvbx_{-t}}{q^2}} \\
& \sless{(a)} \frac{2\sigma^2}{f_{\min}\bigparenth{1-\opnorm{\ParameterMatrix}}^2} \sump \Expectation_{\rvbx_{-t}}\Bigbrackets{ \Expectation_{\rvx_t | \rvbx_{-t}}\Bigbrackets{\twonorm{\nabla_{\rvx_t} q(\rvx_t; \rvbx_{-t})}^2}} \\
& \sequal{(b)} \frac{2\sigma^2}{f_{\min}\bigparenth{1-\opnorm{\ParameterMatrix}}^2} \Expectation_{\rvbx_{-t}}\Bigbrackets{ \Expectation_{\rvx_t | \rvbx_{-t}}\Bigbrackets{ \sump \twonorm{\nabla_{\rvx_t} q(\rvx_t; \rvbx_{-t})}^2 } } \\
& \sequal{(c)} \frac{2\sigma^2}{f_{\min}\bigparenth{1-\opnorm{\ParameterMatrix}}^2} \Expectation_{\rvbx}\Bigbrackets{\twonorm{\nabla_{\rvbx} q(\rvbx)}^2},
\end{align}
where $(a)$ follows because $\rvx_t | \rvbx_{-t}$ satisfies $\LSI{\rvx_t | \rvbx_{-t} = \svbx_{-t}}{\sigma^2}$ for all $t \in [p]$ and $\svbx_{-t} \in \cX^{p-1}$, $(b)$ follows by the linearity of expectation and $(b)$ follows by the law of total expectation. The claim follows.




\subsubsection{Proof of \cref{lemma_tenorization_kld}: \approxtensorofKLresultname}
\label{subsec_proof_lemma_tenorization_kld}
We start by establishing a reverse-Pinsker style inequality for distributions with compact support to bound their KL divergence by their total variation distance. We provide a proof at the end.
%

\begin{lemma}[{Reverse-Pinsker inequality}]\label{lemma_reverse_pinsker}
	For any distributions $f$ and $g$ supported on $\cX \subset \Reals$ such that $\min_{x \in \cX} f(x) > 0$, we have $\KLD{g}{f} \leq \frac{4}{\min_{x \in \cX} f(x)} \TV{g}{f}^2.$
	%
	%
	%
\end{lemma}

%

%
\noindent Given \cref{lemma_reverse_pinsker}, we proceed to prove \cref{lemma_tenorization_kld}. 

\paragraph{Proof of bound~\cref{eq_tensorization_kld}}
To prove \cref{eq_tensorization_kld}, we show that the following inequality holds using the technique of mathematical induction on $p$:
\begin{align}
\KLD{g_{\rvby}}{f_{\rvbx}} \leq \frac{4C}{f_{\min}} \sump \Expectation\Bigbrackets{\TV{g_{\rvy_t | \rvby_{-t} = \svby_{-t}}}{f_{\rvx_t | \rvbx_{-t} = \svby_{-t}}}^2 }. \label{eq_tensor_kl_tv}
\end{align}
Then, \cref{eq_tensorization_kld} follows by using Pinsker's inequality to bound the right hand side of \cref{eq_tensor_kl_tv}.

\paragraph{Base case: $p = 1$} For the base case, we need to establish that the claim holds for all distributions supported on $\cX$ that satisfy the required conditions. In other words, we need to show that
\begin{align}
\KLD{g_{\rvy}}{f_{\rvx}} \leq \frac{4C}{f_{\min}} \TV{g_{\rvy}}{f_{\rvx}}^2 \qtext{for every} t \in [p],
\end{align}
for all random variables $\rvx$ and $\rvy$ supported on $\cX$ such that $f_{\min} = \min_{x \in \cX} f_{\rvx}(x) > 0$.  This follows from \cref{lemma_reverse_pinsker} by observing that $C \geq 1$.
%
%
%
%

%
%
%
%

\paragraph{Inductive step} Now, we assume that the claim holds for all  distributions supported on $ \cX^{p-1}$ that satisfy the required conditions, and establish it for distributions supported on $\cX^{p}$. From the chain rule of KL divergence, we have
\begin{align}
\KLD{g_{\rvby}}{f_{\rvbx}} = \KLD{g_{\rvy_t}}{f_{\rvx_t}} + \Expectation \bigbrackets{ \KLD{g_{\rvby_{-t} | \rvy_t}}{f_{\rvbx_{-t} | \rvx_t}}  } \qtext{for every} t \in [p]. \label{eq_kl_chain_rule}
\end{align}
Taking an average over all $t \in [p]$, we have
\begin{align}
\KLD{g_{\rvby}}{f_{\rvbx}} = \frac{1}{p} \sump \KLD{g_{\rvy_t}}{f_{\rvx_t}} + \frac{1}{p} \sump \Expectation \bigbrackets{ \KLD{g_{\rvby_{-t} | \rvy_t}}{f_{\rvbx_{-t} | \rvx_t}}  }. \label{eq_avg_kl_chain_rule}
\end{align}
Now, we bound the first term in \cref{eq_avg_kl_chain_rule}. Let $\pi^*$ be the coupling between $\rvbx$ and $\rvby$ that achieves $W_2(g_{\rvby}, f_{\rvbx})$ i.e.,\footnote{The minimum is achieved by using arguments similar to the ones used to show that the Wasserstein distance attains its minimum \cite[Chapter 4]{villani2009optimal}.}
\begin{align}
\pi^* = \argmin_{\pi: \pi(\rvbx) = f(\rvbx), \pi(\rvby) = g(\rvby)} \sump \Bigbrackets{\Probability_{\pi}(\rvx_t \neq \rvy_t)}^2. \label{eq_opt_coupling}
\end{align}
Then, we have
\begin{align}
\frac{1}{p} \sump \KLD{g_{\rvy_t}}{f_{\rvx_t}} & \sless{(a)} \frac{1}{p} \sump \frac{4}{f_{\min}} \TV{g_{\rvy_t}}{f_{\rvx_t}}^2  \\
& \sless{(b)} \frac{4}{pf_{\min}} \sump \Bigbrackets{\Probability_{\pi^*}(\rvx_t \neq \rvy_t)}^2\\
& \sequal{(c)} \frac{4}{pf_{\min}} W_2^2(g_{\rvby}, f_{\rvbx}) \\
& \sless{\cref{eq_w2_distance_bounded_assumption}} \frac{4C}{pf_{\min}} \sump \Expectation\Bigbrackets{\TV{g_{\rvy_t | \rvby_{-t} = \svby_{-t}}}{f_{\rvx_t | \rvbx_{-t} = \svby_{-t}}}^2 }, \label{eq_avg_kl_bound_2}
\end{align}
where $(a)$ follows from \cref{lemma_reverse_pinsker} because lower bound on conditional implies lower bound on marginals, i.e., $\min_{t \in [p], x_t \in \cX} f_{\rvx_t}(x_t) \!=\! \min_{t \in [p], x_t \in \cX} \int_{\svbx_{-t} \in \cX^{p-1}} f_{\rvx_t | \rvbx_{-t}}(x_t | \svbx_{-t}) f_{\rvbx_{-t}}(\svbx_{-t}) d\svbx_{-t}$ $> f_{\min}$, $(b)$ follows from the connections of total variation distance to optimal transportation cost, i.e., $\TV{g_\rvy}{f_{\rvx}} = \inf_{\pi: \pi(\rvx) = f(\rvx), \pi(\rvy) = g(\rvy)} \Probability_{\pi}(\rvx \neq \rvy)$, and $(c)$ follows from \cref{def_w2_distance,eq_opt_coupling}.\\
%

\noindent Next, we bound the second term in \cref{eq_avg_kl_chain_rule}. We have
\begin{align}
& \frac{1}{p} \sump \Expectation \bigbrackets{ \KLD{g_{\rvby_{-t} | \rvy_t}}{f_{\rvbx_{-t} | \rvx_t}}} \\
%
& \sless{(a)} \frac{1}{p} \sump \Expectation \biggbrackets{\frac{4C}{f_{\min}} \sum_{u \in [p] \setminus \{t\} }   \Expectation\Bigbrackets{\TV{g_{\rvy_u | \rvby_{-u} = \svby_{-u}}}{f_{\rvx_u | \rvbx_{-u} = \svby_{-u}}}^2  \Big| \rvy_t = y_t}}\\
& \sequal{(b)} \frac{4C}{pf_{\min}} \sump \sum_{u \in [p] \setminus \{t\}} \Expectation\Bigbrackets{\TV{g_{\rvy_u | \rvby_{-u} = \svby_{-u}}}{f_{\rvx_u | \rvbx_{-u} = \svby_{-u}}}^2 } \\
& = \frac{4C(p-1)}{pf_{\min}} \sum_{u \in [p]} \Expectation\Bigbrackets{\TV{g_{\rvy_u | \rvby_{-u} = \svby_{-u}}}{f_{\rvx_u | \rvbx_{-u} = \svby_{-u}}}^2 }, \label{eq_avg_kl_bound_1}
\end{align}
where $(a)$ follows from the inductive hypothesis and $(b)$ follows from the law of total expectation. Then, \cref{eq_tensor_kl_tv} follows by putting \cref{eq_avg_kl_bound_1,eq_avg_kl_bound_2,eq_avg_kl_chain_rule} together.\\

\paragraph{Proof of bound~\cref{eq_tensorization_entropy}}
To prove \cref{eq_tensorization_entropy}, we note that \cref{eq_tensorization_kld} holds for any random vector $\rvby$ supported on $\cX^p$.
%
Consider $\rvby$ to be such that $q(\rvbx) / \Expectation_{\rvbx}[q(\rvbx)]$ is the Radon-Nikodym derivative of $g_{\rvby}$ with respect to $f_{\rvbx}$. For any $\cA^p \subseteq \cX^p$, we have
\begin{align}
\int_{\rvby \in \cA^p} g_{\rvby} d\rvby = \int_{\rvbx \in \cA^p} \frac{q(\rvbx)}{ \Expectation_{\rvbx}[q(\rvbx)]} f_{\rvbx} d\rvbx.
\end{align}
Integrating out $\rvy_t$ and $\rvx_t$ for $t \in [p]$, we have
\begin{align}
\int_{\rvby_{-t} \in \cA^{p-1}} g_{\rvby_{-t}} d\rvby_{-t} = \int_{\rvbx_{-t} \in \cA^{p-1}} \frac{\Expectation_{\rvx_t | \rvbx_{-t}}\bigbrackets{q(\rvbx)}}{\Expectation_{\rvbx}\bigbrackets{q(\rvbx)}} f_{\rvbx_{-t}} d\rvbx_{-t},
\end{align}
implying
%
%
%
%
%
\begin{align}
\frac{dg_{\rvby_{-t}}}{df_{\rvbx_{-t}}} =\frac{\Expectation_{\rvx_t | \rvbx_{-t}}\bigbrackets{q(\rvbx)}}{\Expectation_{\rvbx}\bigbrackets{q(\rvbx)}} \qtext{and} \frac{dg_{\rvy_t | \rvby_{-t}}}{df_{\rvx_t | \rvbx_{-t}}} = \frac{q(\rvbx)}{\Expectation_{\rvx_t | \rvbx_{-t}}\bigbrackets{q(\rvbx)}} \label{eq_radon_niko_marginal} \qtext{for all} t \in [p].
\end{align}
%
%
%
%
We have
\begin{align}
\KLD{g_{\rvby}}{f_{\rvbx}} & \sequal{(a)} \Expectation_{\rvbx} \biggbrackets{\frac{dg_{\rvby}}{df_{\rvbx}} \log \frac{dg_{\rvby}}{df_{\rvbx}}}\\
& \sequal{(b)} \Expectation_{\rvbx} \biggbrackets{\frac{q(\rvbx)}{\Expectation_{\rvbx}\bigbrackets{q(\rvbx)}} \log \frac{q(\rvbx)}{\Expectation_{\rvbx}\bigbrackets{q(\rvbx)}}} \\
& = \frac{1}{\Expectation_{\rvbx}\bigbrackets{q(\rvbx)}} \Bigparenth{\Expectation_{\rvbx} \bigbrackets{q(\rvbx) \log q(\rvbx)} - \Expectation_{\rvbx} \bigbrackets{q(\rvbx)} \log \Expectation_{\rvbx} \bigbrackets{q(\rvbx)}} = \frac{\Ent{\rvbx}{q}}{\Expectation_{\rvbx}\bigbrackets{q(\rvbx)}}, \label{eq_kl_entropy_mapping}
\end{align}
where $(a)$ follows from the definition of KL divergence and $(b)$ follows from the choice of $\rvby$. Similarly, for every $t \in [p]$, we have
\begin{align}
& \Expectation_{\rvby_{-t}}\Bigbrackets{\KLD{g_{\rvy_t | \rvby_{-t} = \svby_{-t}}}{f_{\rvx_t | \rvbx_{-t} = \svby_{-t}}}} \\
& \sequal{(a)} \Expectation_{\rvby_{-t}}\biggbrackets{ \Expectation_{\rvy_t | \rvby_{-t}} \biggbrackets{\log \frac{dg_{\rvy_t | \rvby_{-t}}}{df_{\rvx_t | \rvbx_{-t}}} }} \\
& \sequal{(b)} \Expectation_{\rvby} \biggbrackets{\log \frac{dg_{\rvy_t | \rvby_{-t}}}{df_{\rvx_t | \rvbx_{-t}}} }\\
& \sequal{(c)} \Expectation_{\rvbx} \biggbrackets{\frac{dg_{\rvby}}{df_{\rvbx}} \log \frac{dg_{\rvy_t | \rvby_{-t}}}{df_{\rvx_t | \rvbx_{-t}}} } \\
& \sequal{(d)} \Expectation_{\rvbx} \biggbrackets{\frac{q(\rvbx)}{\Expectation_{\rvbx}\bigbrackets{q(\rvbx)}} \log \frac{q(\rvbx)}{\Expectation_{\rvx_t | \rvbx_{-t}}\bigbrackets{q(\rvbx)}}} \\
%
& \sequal{(e)} \frac{\Expectation_{\rvbx_{-t}}\bigbrackets{\Expectation_{\rvx_t | \rvbx_{-t}} \bigbrackets{q(\rvbx) \log q(\rvbx)} - \Expectation_{\rvx_t | \rvbx_{-t}} \bigbrackets{q(\rvbx) \log \Expectation_{\rvx_t | \rvbx_{-t}}\bigbrackets{q(\rvbx)}} }}{\Expectation_{\rvbx}\bigbrackets{q(\rvbx)}} \\
& \sequal{(f)} \frac{\Expectation_{\rvbx_{-t}}\bigbrackets{\Ent{\rvx_t | \rvbx_{-t}}{q}}}{\Expectation\bigbrackets{q(\rvbx)}}, \label{eq_kl_entropy_conditional_mapping}
\end{align}
where $(a)$ follows from the definition of KL divergence, $(b)$ follows from the law of total expectation, $(c)$ follows from the definition of Radon-Nikodym derivative, $(d)$ follows from the choice of $\rvby$ and \cref{eq_radon_niko_marginal}, $(e)$ follows from the law of total expectation, $(f)$ follows from the definition of entropy. Then, \cref{eq_tensorization_entropy} follows by putting \cref{eq_tensorization_kld,eq_kl_entropy_mapping,eq_kl_entropy_conditional_mapping} together.


\paragraph{Proof of \cref{lemma_reverse_pinsker}: {Reverse-Pinsker inequality}}
\label{subsubsec_proof_reverse_pinsker}
%
%
%
%
Using the facts (a) $\log a \geq 1 - \frac{1}{a} $ for all $a>0$, and (b) $\min_{x \in \cX} f(x)>0$, we find that
%
\begin{align}
\log \frac{f(x)}{g(x)} \geq 1 - \frac{g(x)}{f(x)} \qtext{for every} x \in \cX. \label{eq_rp_1}
\end{align}
Multiplying both sides of \cref{eq_rp_1} by $g(x) \geq 0$ and rearranging terms yields that
\begin{align}
g(x) \log  \frac{g(x)}{f(x)} \leq  \frac{g^2(x)}{f(x)} - g(x) \qtext{for every} x \in \cX. \label{eq_rp_2}
\end{align}
Now, we have
\begin{align}
\KLD{g}{f}  = \int_{x \in \cX}\!\!  g(x) \log  \frac{g(x)}{f(x)} dx & \sless{\cref{eq_rp_2}} \int_{x \in \cX} \biggparenth{\frac{g^2(x)}{f(x)} - g(x)} dx 
%
%
%
\\
& \sequal{(a)} \int_{x \in \cX} \frac{\bigparenth{g(x) -f(x)}^2}{f(x)}dx \\
& \leq \frac{1}{\min_{x \in \cX} f(x)} \int_{x \in \cX} \bigparenth{g(x) -f(x)}^2 dx\\
& \sless{(b)} \frac{1}{\min_{x \in \cX} f(x)} \Bigparenth{\int_{x \in \cX} \bigabs{g(x) -f(x)} dx}^2\\
& \sequal{(c)} \frac{1}{\min_{x \in \cX} f(x)} \Bigparenth{2\TV{g}{f}}^2 \\
& =\frac{4}{\min_{x \in \cX} f(x)} \TV{g}{f}^2,
\end{align}
where $(a)$ follows by simple manipulations, $(b)$ follows by using the order of norms on Euclidean space, and $(c)$ follows by the definition of the total variation distance.

\subsubsection{Proof of \cref{lemma_dobrushin_implies_tensorization}: \dobimpliesapproxtensorresultname}
\label{subsec_proof_lemma_dobrushin_implies_tensorization}

We start by defining the notion of Gibbs sampler which is useful in the proof. 

\begin{definition}\cite[{Gibbs Sampler}]{Marton2015}\label{def_gibbs_sampler}
	For a random vector $\rvbx$ with distribution $f$, define the Markov kernels and the Gibbs sampler as follows
	\begin{align}
	\Gamma_t(\svbx | \svbx') \defn \Indicator\normalparenth{\svbx_{-t} = \svbx'_{-t}} f_{\rvx_t | \rvbx_{-t}}(x_t | \svbx'_{-t}) 
	%
	\qtext{and}
	\Gamma(\svbx | \svbx') \defn p\inv \sump \Gamma_t(\svbx | \svbx'),
	%
	\label{eq_gibbs_sampler}
	\end{align}
	for all $t\in[p]$ and $x, x' \in \cX^p$.
	That is, the kernel $\Gamma_t$ leaves all but the $t^{th}$ coordinate unchanged, and updates the $t^{th}$ coordinate according to $f_{\rvx_t | \rvbx_{-t}}$, and the sampler
	%
	%
	%
	%
	$\Gamma$ selects an index $t \in [p]$ at random, and applies $\Gamma_t$. Further, for a random vector $\rvby$ with distribution $g$ supported on $\cX^p$, we also define
	\begin{align}
	g_{\rvby} \Gamma_t(\svby) & \defn \int g_{\rvby}(\svby') \Gamma_t(\svby | \svby') d\svby' \stext{for} t \in [p], \stext{and} \\
    g_{\rvby} \Gamma(\svby) & \defn \int g_{\rvby}(\svby') \Gamma(\svby | \svby') d\svby'
	\qtext{for all} \svby \in \cX^p.
	\label{eq_gibbs_sampler_expander}
	\end{align}
\end{definition}
\noindent We now proceed to prove \cref{lemma_dobrushin_implies_tensorization} and split it in two cases: (i) $\set = [p]$, and (ii) $\set \subset [p]$. 

\paragraph{Case~(i) ($\set=[p]$)} Let $\Gamma$ be the Gibbs sampler associated with the distribution $f$. Then,
\begin{align}
W_2\bigparenth{g_{\rvby_{\set} | \rvby_{\setC}}, f_{\rvbx_{\set} | \rvbx_{\setC}}} & = W_2(g_{\rvby}, f_{\rvbx}) \sless{(a)} W_2(g_{\rvby}, g_{\rvby} \Gamma) + W_2(g_{\rvby} \Gamma, f_{\rvbx}), \label{eq_triangle_w2}
\end{align}
where $(a)$ follows from the triangle inequality. We claim that
%
\begin{align}
W_2(g_{\rvby}, g_{\rvby} \Gamma) &\leq \frac{1}{p} \sqrt{\sump \Expectation_{\rvby_{-t}} \Bigbrackets{\TV{g_{\rvy_t | \rvby_{-t} = \svby_{-t}}}{f_{\rvx_t | \rvbx_{-t} = \svby_{-t}}}^2}}, \label{eq_coupling_bound_1} \qtext{and} \\
%
%
%
W_2(g_{\rvby} \Gamma, f_{\rvbx}) &\leq \biggparenth{1 - \frac{(1 - \opnorm{\ParameterMatrix})}{p}} W_2(g_{\rvby}, f_{\rvbx}). \label{eq_coupling_bound_2}
\end{align}
Putting \cref{eq_triangle_w2,eq_coupling_bound_1,eq_coupling_bound_2} together, we have
\begin{align}
W_2(g_{\rvby}, f_{\rvbx}) & \leq \frac{1}{p} \sqrt{\sump \Expectation_{\rvby_{-t}} \Bigbrackets{\TV{g_{\rvy_t | \rvby_{-t} = \svby_{-t}}}{f_{\rvx_t | \rvbx_{-t} = \svby_{-t}}}^2}} \\
& \qquad\qquad\qquad\qquad + \biggparenth{1 - \frac{(1 - \opnorm{\ParameterMatrix})}{p}} W_2(g_{\rvby}, f_{\rvbx}). \label{eq_w2_triangle_final}
\end{align}
%
%
%
%
%
Rearranging \cref{eq_w2_triangle_final} results in
\cref{eq_w2_distance_bounded} for $S = [p]$ as desired. It remains to prove our earlier claims~\cref{eq_coupling_bound_1,eq_coupling_bound_2} which we now do one-by-one.

%

\paragraph{Proof of bound~\cref{eq_coupling_bound_1} on $ W_2(g_{\rvby}, g_{\rvby} \Gamma)$} To bound $W_2(g_{\rvby}, g_{\rvby} \Gamma)$, we construct a random vector $\rvby^{\Gamma}$ such that it is coupled with the random vector $\rvby$.
%
We select an index $b \in [p]$ at random, and define
\begin{align}
y_v^{\Gamma} \defn y_v  \qtext{for all} v \in [p] \setminus \{b\}.
\end{align}
%
Then, given $b$ and $\rvby_{-b} = \svby_{-b}$, we define the joint distribution of $(\rvy_b, \rvy_b^{\Gamma})$ to be the maximal coupling of $g_{\rvy_b | \rvby_{-b} = \svby_{-b}}$ and $f_{\rvx_b | \rvbx_{-b} = \svby_{-b}}$ that achieves $\TV{g_{\rvy_b | \rvby_{-b} = \svby_{-b}}}{f_{\rvx_b | \rvbx_{-b}=\svby_{-b}}}$. It is easy to see that the marginal distribution of $\rvby$ is $g_{\rvby}$ and the marginal distribution of $\rvby^{\Gamma}$ is $g_{\rvby} \Gamma$ (see \cref{def_gibbs_sampler}). Then, we have
\begin{align}
W_2^2(g_{\rvby}, g_{\rvby} \Gamma) 
%
& \sless{(a)} \sump \biggbrackets{\Probability(b = t) \Probability(\rvy_t \neq \rvy_t^{\Gamma} | b = t) + \Probability(b \neq t) \Probability(\rvy_t \neq \rvy_t^{\Gamma} | b \neq t)}^2\\
%
& \sequal{(b)} \sump \biggbrackets{\frac{1}{p} \Probability(\rvy_t \neq \rvy_t^{\Gamma} | b = t)}^2\\
& \sequal{(c)} \frac{1}{p^2} \sump \biggbrackets{ \int\limits_{\svby_{-t} \in \cX^{p-1}}\Probability(\rvy_t \neq \rvy_t^{\Gamma} | b = t, \rvby_{-t} = \svby_{-t}) g_{\rvby_{-t} | b= t}(\svby_{-t} | b = t) d\svby_{-t}}^2\\
& \sequal{(d)} \frac{1}{p^2} \sump \biggbrackets{ \int\limits_{\svby_{-t} \in \cX^{p-1}}\TV{g_{\rvy_t | \rvby_{-t} = \svby_{-t}}}{f_{\rvx_t | \rvbx_{-t} = \svby_{-t}}} g_{\rvby_{-t}}(\svby_{-t}) d\svby_{-t}}^2\\
& = \frac{1}{p^2} \sump \biggbrackets{\Expectation_{\rvby_{-t}} \Bigbrackets{\TV{g_{\rvy_t | \rvby_{-t} = \svby_{-t}}}{f_{\rvx_t | \rvbx_{-t} = \svby_{-t}}}}}^2, \label{eq_coupling_bound_1_interim}
%
\end{align}
where $(a)$ follows from \cref{def_w2_distance} and the Bayes rule, $(b)$ follows because $\Probability(b = t) = \frac{1}{p}$ and $\Probability(\rvy_t \neq \rvy_t^{\Gamma} | b \neq t) = 0$, $(c)$ follows by the law of total probability, and $(d)$ follows because $g_{\rvby_{-t} | b= t}(\svby_{-t} | b = t) = g_{\rvby_{-t}}(\svby_{-t})$ and by the construction of the coupling between $\rvby$ and $\rvby^{\Gamma}$. Then, \cref{eq_coupling_bound_1} follows by using Jensen's inequality in \cref{eq_coupling_bound_1_interim}.



\paragraph{Proof of bound~\cref{eq_coupling_bound_2} on  $W_2(g_{\rvby} \Gamma, f_{\rvbx})$}
%
We first show that  $f_{\rvbx}$ is an invariant measure for $\Gamma$, i.e., $f_{\rvbx} = f_{\rvbx} \Gamma$, implying $W_2(g_{\rvby} \Gamma, f_{\rvbx}) = W_2(g_{\rvby} \Gamma, f_{\rvbx} \Gamma)$, and then $\Gamma$ is a contraction with respect to the $W_2$ distance with rate $1 - \frac{(1 - \opnorm{\ParameterMatrix})}{p}$, i.e., $W_2(g_{\rvby} \Gamma, f_{\rvbx} \Gamma) \leq \Bigparenth{1 - \frac{(1 - \opnorm{\ParameterMatrix})}{p}} W_2(g_{\rvby}, f_{\rvbx})$, implying \cref{eq_coupling_bound_2}.

\paragraph{Proof of $f_{\rvbx}$ being an invariant measure for $\Gamma$} We have
\begin{align}
f_{\rvbx} \Gamma (\svbx)  & \sequal{\cref{eq_gibbs_sampler_expander}} \int_{\svbx' \in \cX^p} f_{\rvbx}(\svbx') \Gamma(\svbx | \svbx') d\svbx' \\
& \sequal{\cref{eq_gibbs_sampler}} \int_{\svbx' \in \cX^p} f_{\rvbx}(\svbx') \biggparenth{\frac{1}{p} \sump \Gamma_t(\svbx | \svbx')} d\svbx' \\
%
& \sequal{\cref{eq_gibbs_sampler}} \frac{1}{p} \sump \int_{\svbx' \in \cX^p} f_{\rvbx}(\svbx') \Indicator\normalparenth{\svbx_{-t}  = \svbx'_{-t}} f_{\rvx_t | \rvbx_{-t}}(x_t | \svbx'_{-t}) d\svbx' \\
& = \frac{1}{p} \sump f_{\rvx_t | \rvbx_{-t}}(x_t | \svbx_{-t}) \int_{x'_t \in \cX} f_{\rvbx}(\svbx_{-t}, x'_t) dx'_t \\
& = \frac{1}{p} \sump f_{\rvx_t | \rvbx_{-t}}(x_t | \svbx_{-t}) f_{\rvbx_{-t}}(\svbx_{-t}) = f_{\rvbx}(\svbx).
\end{align}

\paragraph{Proof of $\Gamma$ being a contraction w.r.t the $W_2$ distance} Let $\pi^*$ be the coupling between $\rvbx$ and $\rvby$ that achieves $W_2(g_{\rvby}, f_{\rvbx})$ i.e.,\footnote{The minimum is achieved by using arguments similar to the ones used to show that the Wasserstein distance attains its minimum \cite[Chapter 4]{villani2009optimal}.}
\begin{align}
\pi^* = \argmin_{\pi: \pi(\rvbx) = f(\rvbx), \pi(\rvby) = g(\rvby)} \sqrt{\sump \Bigbrackets{\Probability_{\pi}(\rvx_t \neq \rvy_t)}^2}. \label{eq_opt_coupling_2}
\end{align}
We construct random variables $\rvbx'$ and $\rvby'$ as well as a coupling $\pi'$ between them such that the marginal distribution of $\rvbx'$ is $f_{\rvbx} \Gamma$ and the marginal distribution of $\rvby'$ is $g_{\rvby} \Gamma$. We start by selecting an index $b \in [p]$ at random, and defining
\begin{align}
y_v' \defn y_v \qtext{and} x_v' \defn x_v \qtext{for all} v \neq b. \label{eq_coupling_contraction}
\end{align}
Then, given $b$, $\rvby_{-b}' = \svby_{-b}$, and $\rvbx_{-b}' = \svbx_{-b}$, we define the joint distribution of $(\rvy_b', \rvx_b')$ to be the maximal coupling of $f_{\rvx_b | \rvbx_{-b}}(\cdot | \svby_{-b})$ and $f_{\rvx_b | \rvbx_{-b}}(\cdot | \svbx_{-b})$ that achieves $\TV{f_{\rvx_b | \rvbx_{-b} = \svby_{-b}}}{f_{\rvx_b | \rvbx_{-b} = \svbx_{-b}}}$. \\

\noindent Now, for every $t \in [p]$, we bound $\Probability_{\pi'}(\rvy_t' \neq \rvx_t')$ in terms of $\Probability_{\pi^*}(\rvy_t \neq \rvx_t)$. To that end, we have
\begin{align}
\Probability_{\pi'}(\rvy_t' \neq \rvx_t') & \sequal{(a)} \Probability(b = t) \Probability_{\pi'}(\rvy_t' \neq \rvx_t' | b = t) + \Probability(b \neq t) \Probability_{\pi'}(\rvy_t' \neq \rvx_t' | b \neq t) \\
& \sequal{(b)} 
%
%
\frac{1}{p} \Probability_{\pi'}(\rvy_t' \neq \rvx_t' | b = t) + \Bigparenth{1-\frac{1}{p}} \Probability_{\pi^*}(\rvy_t \neq \rvx_t), \label{eq_bayes_rule_step_1}
\end{align}
where $(a)$ follows from the Bayes rule and $(b)$ follows because $\Probability(b = t) = \frac{1}{p}$ and \cref{eq_coupling_contraction}. Focusing on $\Probability_{\pi'}(\rvy_t' \neq \rvx_t' | b = t)$ and using the law of total probability, we have
\begin{align}
& \Probability_{\pi'}(\rvy_t' \neq \rvx_t' | b = t) \\
& =   \int\limits_{\svby_{-t}, \svbx_{-t} \in \cX^{p-1}}   \Probability_{\pi'}(\rvy_t' \neq \rvx_t' | b \!=\! t, \rvby_{-t}' \!=\! \svby_{-t}, \rvbx_{-t}' \!=\! \svbx_{-t}) \pi'_{\rvby_{-t}', \rvbx_{-t}' | b= t}(\svby_{-t}, \svbx_{-t} | b \!=\! t) d\svby_{-t} d\svbx_{-t} \\
& \sequal{(a)}   \int\limits_{\svby_{-t}, \svbx_{-t} \in \cX^{p-1}}   \TV{f_{\rvx_t | \rvbx_{-t} = \svby_{-t}}}{f_{\rvx_t | \rvbx_{-t} = \svbx_{-t}}} \pi^*_{\rvby_{-t}, \rvbx_{-t}}(\svby_{-t}, \svbx_{-t}) d\svby_{-t} d\svbx_{-t} \\
&  =  \Expectation_{\pi^*_{\rvby_{-t}, \rvbx_{-t}}} \Bigbrackets{\TV{f_{\rvx_t | \rvbx_{-t} = \svby_{-t}}}{f_{\rvx_t | \rvbx_{-t} = \svbx_{-t}}} } \label{eq_ltp_0}
%
\end{align}
where $(a)$ follows by the construction of the coupling between $\rvby'$ and $\rvbx'$. Now, using the triangle inequality in \cref{eq_ltp_0}, we have
\begin{align}
\Probability_{\pi'}(\rvy_t' \neq \rvx_t' | b = t) & \leq \Expectation_{\pi^*_{\rvby_{-t}, \rvbx_{-t}}} \Bigbrackets{ \sum_{u \in [p] \setminus \{t\}} \!\!\!  \Indicator(r_v \!=\! s_v \!=\! y_v \forall v \!<\! u) \Indicator(r_v \!=\! s_v \!=\! x_v \forall v \!>\! u) ~~ \times \\
& \qquad \qquad \qquad \qquad \Indicator(r_u \!=\! y_u, x_u \!=\! s_u)  \TV{f_{\rvx_t | \rvbx_{-t} = \boldsymbol{r}_{-t}}}{f_{\rvx_t | \rvbx_{-t} = \boldsymbol{s}_{-t}}}} \\
& \sless{\cref{eq_dob_tv_bound}}  \Expectation_{\pi^*_{\rvby_{-t}, \rvbx_{-t}}} \Bigbrackets{ \sum_{u \in [p] \setminus \{t\}} \!\!\!\!\! \ParameterTU[tu] \Indicator(\rvy_u \neq \rvx_u)} = \sum_{u \in [p] \setminus \{t\}} \!\!\!\!\! \ParameterTU[tu] \Probability_{\pi^*}(\rvy_u \neq \rvx_u). \label{eq_bayes_rule_step_2}
\end{align}
Putting together \cref{eq_bayes_rule_step_1,eq_bayes_rule_step_2}, we have
\begin{align}
\Probability_{\pi'}(\rvy_t' \neq \rvx_t') \leq  
\frac{1}{p} \sum_{u \in [p] \setminus \{t\}} \ParameterTU[tu] \Probability_{\pi^*}(\rvy_u \neq \rvx_u) + \Bigparenth{1-\frac{1}{p}} \Probability_{\pi^*}(\rvy_t \neq \rvx_t). \label{eq_prob_coupled_bound}
\end{align}
Next, we use \cref{eq_prob_coupled_bound} to show contraction of $\Gamma$. To that end, we define $\diag \in \Reals^{p \times p}$ to be the matrix with diagonal same as $\ParameterMatrix$ and all non-diagonal entries equal to zeros. Then, we have
\begin{align}
W^2_2(g_{\rvby} \Gamma, f_{\rvbx} \Gamma)  & \sless{(a)} \sump \Bigbrackets{\Probability_{\pi'}(\rvy_t' \neq \rvx_t')}^2  \\
& \sless{\cref{eq_prob_coupled_bound}} \sump \biggbrackets{\frac{1}{p} \sum_{u \in [p] \setminus \{t\}} \ParameterTU[tu] \Probability_{\pi^*}(\rvy_u \neq \rvx_u) + \Bigparenth{1-\frac{1}{p}} \Probability_{\pi^*}(\rvy_t \neq \rvx_t)}^2\\
& \sless{(b)} \bopnorm{\Bigparenth{1-\frac{1}{p}} I + \frac{1}{p} \Bigparenth{\ParameterMatrix - \diag} }^2 \sump \Bigbrackets{\Probability_{\pi^*}(\rvy_t \neq \rvx_t)}^2 \\
& \sequal{(c)} \bopnorm{\Bigparenth{1-\frac{1}{p}} I + \frac{1}{p} \Bigparenth{\ParameterMatrix - \diag}}^2 W^2_2(g_{\rvby}, f_{\rvbx})\\
& \sless{(d)} \biggparenth{\Bigparenth{1-\frac{1}{p}} + \frac{1}{p} \opnorm{\ParameterMatrix - \diag}}^2 W^2_2(g_{\rvby}, f_{\rvbx}) \\
& \sless{(e)} \biggparenth{\Bigparenth{1-\frac{1}{p}} + \frac{1}{p} \opnorm{\ParameterMatrix}}^2 W^2_2(g_{\rvby}, f_{\rvbx}) ,\label{eq_coupling_bound_2_interim}
%
\end{align}
where $(a)$ follows from \cref{def_w2_distance}, $(b)$ follows by some linear algebraic manipulations, $(c)$ follows from \cref{def_w2_distance} and \cref{eq_opt_coupling_2},  $(d)$ follows from the triangle inequality, and $(e)$ follows because $\opnorm{\tbf{M}_1} \leq \opnorm{\tbf{M}_2}$ for any matrices $\tbf{M}_1$ and $\tbf{M}_2$ such that $0 \leq \tbf{M}_1 \leq \tbf{M}_2$ (component-wise). Then, contraction of $\Gamma$ follows by taking square root on both sides of \cref{eq_coupling_bound_2_interim}.


\paragraph{Case~(ii) ($\set\subset[p]$)} We can directly verify that the matrix $\ParameterMatrix_{\set} \defn \braces{\ParameterTU[tu]}_{t,u \in \set}$ is such that $\opnorm{\ParameterMatrix_{\set}} \leq \opnorm{\ParameterMatrix}$ This is true because the operator norm of any sub-matrix is no more than the operator norm of the matrix. Further, we note that for any $\svby_{\setC} \in \cX^{p-\normalabs{\set}}$, the random vector $\rvbx_{\set} | \rvbx_{\setC} = \svby_{\setC}$ with distribution $f_{\rvbx_{\set} | \rvbx_{\setC} = \svby_{\setC}}$ satisfies the Dobrushin's uniqueness condition (\cref{def_dobrushin_condition}) with coupling matrix $\ParameterMatrix_{\set}$. Then, by performing an analysis similar to the one above, we have
\begin{align}
W_2\bigparenth{g_{\rvby_{\set} | \rvby_{\setC}}, f_{\rvbx_{\set} | \rvbx_{\setC}}} & \leq \frac{1}{\bigparenth{1-\opnorm{\ParameterMatrix_{\set}}}} \sqrt{\sumset \Expectation\Bigbrackets{\TV{g_{\rvy_t | \rvby_{-t} = \svby_{-t}}}{f_{\rvx_t | \rvbx_{-t} = \svby_{-t}}}^2  \Big| \rvby_{\setC} = \svby_{\setC}}} \\
& \sless{(a)} \frac{1}{\bigparenth{1-\opnorm{\ParameterMatrix}}} \sqrt{\sumset \Expectation\Bigbrackets{\TV{g_{\rvy_t | \rvby_{-t} = \svby_{-t}}}{f_{\rvx_t | \rvbx_{-t} = \svby_{-t}}}^2  \Big| \rvby_{\setC} = \svby_{\setC}}},
\end{align}
where $(a)$ follows because $\frac{1}{\normalparenth{1-\opnorm{\ParameterMatrix_{\set}}}} \leq \frac{1}{\normalparenth{1-\opnorm{\ParameterMatrix}}}$. This completes the proof.


\subsection{Proof of \cref{thm_main_concentration}: \mainconcresultname}
\label{subsec_proof_main_concentration}
%
Fix a function $q : \cX^p \to \Reals$. Fix any pseudo derivative $\tnabla q$ for $q$ and any pseudo Hessian $\tnabla^2 q$ for $q$. To prove \cref{thm_main_concentration}, we bound the $p$-th moment of $q(\rvbx) - \Expectation\bigbrackets{q(\rvbx)}$ by certain norms of $\tnabla^2 q$ and $\Expectation_{\rvbx}\bigbrackets{\tnabla q(\rvbx)}$. To that end, first, we claim that in order to control the $p$-th moment of $q(\rvbx) - \Expectation\bigbrackets{q(\rvbx)}$, it is sufficient to control the $p$-th moment of $\twonorm{\nabla q(\rvbx)}$. Then, using \cref{eq:pseudo_Hessian}, we note that the $p$-th moment of $\twonorm{\nabla q(\rvbx)}$ is bounded by the $p$-th moment of $\stwonorm{\tnabla q(\rvbx)}$. Next, we claim that the $p$-th moment of $\stwonorm{\tnabla q(\rvbx)}$ is bounded by a linear combination of appropriate norms of $\tnabla^2 q$ and $\Expectation_{\rvbx}\bigbrackets{\tnabla q(\rvbx)}$. We formalize the claims below and divide the proof across \cref{subsec_proof_lemma_bounded_p_moment} and \cref{subsec_proof_eq_p_moment_of_2_norm_gradient}.

%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%


\begin{lemma}[{Bounded $p$-th moments of $q(\rvbx) - \Expectation\bigbrackets{q(\rvbx)}$ and $\stwonorm{\tnabla q(\rvbx)}$}]\label{lemma_bounded_p_moment}
	If a random vector $\rvbx$ satisfies $\LSI{\rvbx}{\sigma^2}$, then for any arbitrary function $q :\cX^p \to \Reals$,
	\begin{align}
	\moment{q(\rvbx) - \Expectation\bigbrackets{q(\rvbx)}}{p} \leq \sigma \sqrt{2p} \moment{\twonorm{\nabla q(\rvbx)}}{p} \qtext{for any $p \geq 2$.} \label{eq_moment_controlled_by_gradient}
	\end{align}
	Further, for any pseudo derivative $\tnabla q(\svbx)$ and any pseudo Hessian $\tnabla^2 q(\svbx)$ for $q$, and even $p \geq 2$,
	\begin{align}
	\smoment{\stwonorm{\tnabla q(\rvbx)}}{p} \!\!\leq\!  2c \sigma \bigparenth{\!\max_{\svbx \in \cX^p} \fronorm{\tnabla^2 q(\svbx)} \!+\! \sqrt{p}  \max_{\svbx \in \cX^p} \opnorm{\tnabla^2 q(\svbx)}} \!+\! 4 \stwonorm{\Expectation_{\rvbx}\bigbrackets{\tnabla q(\rvbx)}},
	%
	\label{eq_p_moment_of_2_norm_gradient} 
	\end{align}
	where $c \geq 0$ is a universal constant.
	%
	%
	%
	%
\end{lemma}

%
%
%
%
%
%
%
%
%
%


\noindent Given these lemmas, we proceed to prove \cref{thm_main_concentration}. We let $q_c(\rvbx) = q(\rvbx) - \Expectation\bigbrackets{q(\rvbx)}$. Combining \cref{eq_moment_controlled_by_gradient,eq_p_moment_of_2_norm_gradient} for any even $p \geq 2$, there exists a universal constant $c'$ such that
\begin{align}
\moment{q_c(\rvbx)}{p} \!\leq \! c' \sigma^2  \Bigparenth{\! \sqrt{p}  \max_{\svbx \in \cX^p} \fronorm{\tnabla^2 q(\svbx)} \!+\! p \max_{\svbx \in \cX^p} \opnorm{\tnabla^2 q(\svbx)} \!+\! \sqrt{p}  \stwonorm{\Expectation_{\rvbx}\bigbrackets{\tnabla q(\rvbx)}}}. \label{eq_lemma_bounded_moments_combined}
\end{align}
Now, we complete the proof by using \cref{eq_lemma_bounded_moments_combined} along with Markov's inequality for a specific choice of $p$. For any even $p \geq 2$, we have 
\begin{align}
& \Probability\Bigbrackets{\bigabs{q_c(\rvbx)} > ec' \sigma^2  \Bigparenth{\sqrt{p}  \max_{\svbx \in \cX^p} \fronorm{\tnabla^2 q(\svbx)} + p \max_{\svbx \in \cX^p} \opnorm{\tnabla^2 q(\svbx)} + \sqrt{p}  \stwonorm{\Expectation_{\rvbx}\bigbrackets{\tnabla q(\rvbx)}}}} \\
& = \Probability\Bigbrackets{\bigabs{q_c(\rvbx)}^p \!>\! \bigparenth{ec' \sigma^2}^p  \bigparenth{\! \sqrt{p}  \max_{\svbx \in \cX^p} \fronorm{\tnabla^2 q(\svbx)} \!+\! p \max_{\svbx \in \cX^p} \opnorm{\tnabla^2 q(\svbx)} \!+\!\! \sqrt{p}  \stwonorm{\Expectation_{\rvbx}\bigbrackets{\tnabla q(\rvbx)}}}^p} \\
& \sless{(a)}  \frac{\Expectation{\bigbrackets{\bigabs{q_c(\rvbx)}^p}}}{\bigparenth{ec' \sigma^2}^p  \bigparenth{\sqrt{p}  \max_{\svbx \in \cX^p} \fronorm{\tnabla^2 q(\svbx)} + p \max_{\svbx \in \cX^p} \opnorm{\tnabla^2 q(\svbx)} + \sqrt{p}  \stwonorm{\Expectation_{\rvbx}\bigbrackets{\tnabla q(\rvbx)}}}^p}\\
& \sless{\cref{eq_lemma_bounded_moments_combined}} e^{-p},
\end{align}
where $(a)$ follows from Markov's inequality. The proof is complete by choosing an appropriate universal constant $c''$, and and performing basic algebraic manipulations after letting
%
%
%
%
\begin{align}
p = \frac{1}{c''\sigma^2}\min \Bigparenth{\dfrac{\varepsilon^2}{ \Expectation\bigbrackets{\stwonorm{\tnabla q(\rvbx)}}^2 + \max\limits_{\svbx \in \cX^p} \fronorm{\tnabla^2 q(\svbx)}^2}, \dfrac{\varepsilon}{\max\limits_{\svbx \in \cX^p} \opnorm{\tnabla^2 q(\svbx)}}}.
\end{align}
We note that an even $p \geq 2$ can be ensured by choosing appropriate $c''$.

\subsubsection{Proof of \cref{lemma_bounded_p_moment}\cref{eq_moment_controlled_by_gradient}: {Bounded $p$-th moment of $q(\rvbx) - \Expectation\bigbrackets{q(\rvbx)}$}}
\label{subsec_proof_lemma_bounded_p_moment}
Fix any $p \geq 2$. We start by using the following result from \cite[Theorem 3.4]{AidaS1994} since $\rvbx$ satisfies $\LSI{\rvbx}{\sigma^2}$:
\begin{align}
\moment{q(\rvbx) - \Expectation\bigbrackets{q(\rvbx)}}{p}^2 \leq & \moment{q(\rvbx) - \Expectation\bigbrackets{q(\rvbx)}}{2}^2 + 2\sigma^2 (p-2)   \moment{\twonorm{\nabla q(\rvbx)}}{p}^2. \label{eq_aida_stroock}
\end{align}
Then, we bound the first term in \cref{eq_aida_stroock} by using the fact that logarithmic Sobolev inequality implies Poincare inequality with the same constant:
\begin{align}
\moment{q(\rvbx) - \Expectation\bigbrackets{q(\rvbx)}}{2}^2 = \Variance(q(\rvbx)) \leq \sigma^2  \Expectation_{\rvbx}\Bigbrackets{\twonorm{\nabla q(\rvbx)}^2}. \label{eq_poincare}
\end{align}
Putting together \cref{eq_aida_stroock,eq_poincare}, we have
\begin{align}
\moment{q(\rvbx) - \Expectation\bigbrackets{q(\rvbx)}}{p}^2 & \leq  \sigma^2 \Expectation_{\rvbx}\Bigbrackets{\twonorm{\nabla q(\rvbx)}^2} + 2\sigma^2 (p-2)   \moment{\twonorm{\nabla q(\rvbx)}}{p}^2 \\
& \sless{(a)}  \sigma^2  \Bigparenth{\Expectation_{\rvbx}\Bigbrackets{\twonorm{\nabla q(\rvbx)}^p}}^{2/p} + 2\sigma^2 (p-2)   \moment{\twonorm{\nabla q(\rvbx)}}{p}^2 \\
& \sequal{(b)} \sigma^2  \moment{\twonorm{\nabla q(\rvbx)}}{p}^2 + 2\sigma^2 (p-2)   \moment{\twonorm{\nabla q(\rvbx)}}{p}^2 \\
& \leq 2\sigma^2 p \moment{\twonorm{\nabla q(\rvbx)}}{p}^2, \label{eq_aida_stroock_simplified}
\end{align}
where $(a)$ follows by Jensen's inequality and $(b)$ follows by the definition of $p$-th moment. Taking square root on both sides of \cref{eq_aida_stroock_simplified} completes the proof.

\subsubsection{Proof of \cref{lemma_bounded_p_moment}\cref{eq_p_moment_of_2_norm_gradient}: {Bounded $p$-th moment of $\stwonorm{\tnabla q(\rvbx)}$}}
\label{subsec_proof_eq_p_moment_of_2_norm_gradient}
Fix any even $p \geq 2$. Fix any pseudo derivative $\tnabla q$ and any pseudo Hessian $\tnabla^2 q$. We start by obtaining a convenient bound on $\stwonorm{\tnabla q(\svbx)}$ for every $\svbx \in \cX^p$ and then proceed to bound the $p$-th moment of $\stwonorm{\tnabla q(\rvbx)}$.\\

\noindent Consider a $p$-dimensional standard normal random vector $\rvb{g}$ independent of $\rvbx$. For a given $\rvbx = \svbx \in \cX^p$, the random variable $\frac{\tnabla q(\svbx)\tp \rvb{g}}{\stwonorm{\tnabla q(\svbx)}}$ is a standard normal random variable. 
%
%
%
%
Then, for every $\svbx \in \cX^p$, we have
\begin{align}
\moment{\frac{\tnabla q(\svbx)\tp \rvb{g}}{\stwonorm{\tnabla q(\svbx)}}}{p} \sequal{(a)} \biggparenth{\Expectation_{\rvb{g} | \rvbx = \svbx }\biggbrackets{\biggparenth{\frac{\tnabla q(\svbx)\tp \rvb{g}}{\stwonorm{\tnabla q(\svbx)}}}^p}}^{1/p} \sgreat{(b)} \frac{\sqrt{p}}{2}, \label{eq_normal_application}
\end{align}
where $(a)$ follows from the definition of $p$-th moment, and $(b)$ follows since $\moment{\rv{g}}{p} \geq \frac{\sqrt{p}}{2}$ for any standard normal random variable $\rv{g}$ and even $p \geq 2$. Rearranging \cref{eq_normal_application}, we have
\begin{align}
\stwonorm{\tnabla q(\svbx)} \leq \frac{2}{\sqrt{p}} \Bigparenth{\Expectation_{\rvb{g}| \rvbx = \svbx}\Bigbrackets{\bigparenth{\tnabla q(\svbx)\tp \rvb{g}}^p}}^{1/p}. \label{eq_bound_fixed_gradient}
\end{align}
Now, we proceed to bound the $p$-th moment of $\stwonorm{\tnabla q(\rvbx)}$ as follows
\begin{align}
\smoment{\stwonorm{\tnabla q(\rvbx)}}{p}  & \sequal{(a)} \Bigparenth{\Expectation_{\rvbx} \bigbrackets{\stwonorm{\tnabla q(\rvbx)}^p}}^{1/p} \\
& \sless{\cref{eq_bound_fixed_gradient}} \frac{2}{\sqrt{p}} \Bigparenth{\Expectation_{\rvbx, \rvb{g}}\Bigbrackets{\bigparenth{\tnabla q(\rvbx)\tp \rvb{g}}^p}}^{1/p} \\
& \sequal{(b)} \frac{2}{\sqrt{p}} \moment{\tnabla q(\rvbx)\tp \rvb{g}}{p} \\
& \sless{(c)} \frac{2}{\sqrt{p}} \Bigparenth{\!\moment{\tnabla q(\rvbx)\tp \rvb{g} \!-\! \Expectation_{\rvbx}\bigbrackets{\tnabla q(\rvbx)\tp \rvb{g}}}{p} \!+\! \moment{\Expectation_{\rvbx}\bigbrackets{\tnabla q(\rvbx)\tp \rvb{g}}}{p}\!}, \label{eq_bound_gradient_minkowski}
\end{align}
where $(a)$ and $(b)$ follow from the definition of $p$-th moment and $(c)$ follows by Minkowski's inequality. We claim that
\begin{align}
\moment{\tnabla q(\rvbx)\!\tp \! \rvb{g} \!-\! \Expectation_{\rvbx}\bigbrackets{\tnabla q(\rvbx)\!\tp \! \rvb{g}}}{p} & \!\!\!\!\! \leq \! c \sigma \Bigparenth{\!\! \sqrt{p} \max_{\svbx \in \cX^p} \fronorm{\tnabla^2 \! q(\svbx)} \!+\! p \max_{\svbx \in \cX^p} \opnorm{\tnabla^2 q(\svbx)\!}\!}, \text{ \&} \!\label{eq_bound_first_term_minkowski}\\
\moment{\Expectation_{\rvbx}\bigbrackets{\tnabla q(\rvbx)\tp \rvb{g}}}{p} & \!\!\!\!\! \leq \! 2\sqrt{p} \twonorm{\Expectation_{\rvbx}\bigbrackets{\tnabla q(\rvbx)}}, \label{eq_bound_second_term_minkowski}
\end{align}
where $c \geq 0$ is a universal constant.
%
%
%
%
%
%
%
%
Putting together \cref{eq_bound_gradient_minkowski,eq_bound_first_term_minkowski,eq_bound_second_term_minkowski} completes the proof. It remains to prove our claims \cref{eq_bound_first_term_minkowski,eq_bound_second_term_minkowski} which we now do one-by-one.

\paragraph{Proof of bound \cref{eq_bound_first_term_minkowski}} To start, we bound $\bigparenth{\Expectation_{\rvbx | \rvb{g} = \svb{g}} \bigbrackets{\bigparenth{\tnabla q(\rvbx)\tp \svb{g} - \Expectation_{\rvbx| \rvb{g} = \svb{g}}\bigbrackets{\tnabla q(\rvbx)\tp \svb{g}}}^p}}^{1/p}$ for every $\rvb{g} = \svb{g}$, and then proceed to bound $\smoment{\tnabla q(\rvbx)\tp \rvb{g} - \Expectation_{\rvbx}\bigbrackets{\tnabla q(\rvbx)\tp \rvb{g}}}{p}$.\\

\noindent To that end, we define $h_{\svb{g}}(\rvbx) \defn \tnabla q(\rvbx)\tp \svb{g} - \Expectation_{\rvbx| \rvb{g} = \svb{g}}\bigbrackets{\tnabla q(\rvbx)\tp \svb{g}}$ and observe that $\Expectation_{\rvbx| \rvb{g} = \svb{g}}\bigbrackets{h_{\svb{g}}(\rvbx)}$ $ = 0$. Now, applying \cref{lemma_bounded_p_moment}~\cref{eq_moment_controlled_by_gradient} to $h_{\svb{g}}(\cdot)$, we have
\begin{align}
\moment{h_{\svb{g}}(\rvbx)}{p} 
\leq \sigma \sqrt{2p} \Bigparenth{\Expectation_{\rvbx| \rvb{g} = \svb{g}} \Bigbrackets{\twonorm{\nabla h_{\svb{g}}(\rvbx)}^p}}^{1/p} & \sless{(a)} \sigma \sqrt{2p} \Bigparenth{\Expectation_{\rvbx| \rvb{g} = \svb{g}} \Bigbrackets{\twonorm{\nabla \bigbrackets{\svb{g} \tp \tnabla q(\rvbx)}}^p}}^{1/p} \\
& \sless{\cref{eq:pseudo_Hessian}} \sigma \sqrt{2p} \Bigparenth{\Expectation_{\rvbx| \rvb{g} = \svb{g}} \Bigbrackets{\twonorm{\svb{g}\tp \tnabla^2 q(\rvbx)}^p}}^{1/p}, \label{eq_first_term_minkowski_0}
%
\end{align}
where $(a)$ follows from the definition of $h_{\svb{g}}(\rvbx)$. Now, to obtain a bound on the RHS of \cref{eq_first_term_minkowski_0}, we further fix $\rvbx = \svbx$. Then, we let $\rvb{g}'$ be another $p$-dimensional standard normal vector and apply an inequality similar to \cref{eq_bound_fixed_gradient} to $\svb{g}\tp \tnabla^2 q(\svbx)$ obtaining
\begin{align}
\twonorm{\svb{g}\tp \tnabla^2 q(\svbx)} \leq \frac{2}{\sqrt{p}} \Bigparenth{\Expectation_{\rvb{g}'| \rvbx = \svbx, \rvb{g} = \svb{g}}\Bigbrackets{\Bigparenth{\svb{g}\tp \tnabla^2 q(\svbx) \rvb{g}'}^p}}^{1/p}, \label{eq_first_term_minkowski_1}
\end{align}
which implies
\begin{align}
\Bigparenth{\Expectation_{\rvbx | \rvb{g} = \svb{g}} \Bigbrackets{\twonorm{\svb{g}\tp \tnabla^2 q(\rvbx)}^p}}^{1/p} \leq \frac{2}{\sqrt{p}} \Bigparenth{\Expectation_{\rvbx, \rvb{g}' |\rvb{g} = \svb{g}}\Bigbrackets{\Bigparenth{\nabla \svb{g}\tp \tnabla^2 q(\rvbx) \rvb{g}'}^p}}^{1/p}. \label{eq_first_term_minkowski_2}
\end{align}
Putting together \cref{eq_first_term_minkowski_0,eq_first_term_minkowski_2}, and using the definition of $h_{\svb{g}}(\rvbx)$, we have
\begin{align}
\Expectation_{\rvbx|\rvb{g} = \svb{g}} \Bigbrackets{\Bigparenth{\tnabla q(\rvbx)\tp \svb{g} \!-\! \Expectation_{\rvbx|\rvb{g} = \svb{g}}\Bigbrackets{\tnabla q(\rvbx)\tp \svb{g}}}^p}
\!\! \leq (2\sqrt{2}\sigma)^p \Expectation_{\rvbx, \rvb{g}'|\rvb{g} = \svb{g}}\Bigbrackets{\Bigparenth{\svb{g}\!\tp \tnabla^2 q(\rvbx) \rvb{g}'}^p}. \label{eq_bound_first_term_fixed_g}
\end{align}
Now, we proceed to bound $\smoment{\tnabla q(\rvbx)\tp \rvb{g} - \Expectation_{\rvbx}\bigbrackets{\tnabla q(\rvbx)\tp \rvb{g}}}{p}$ as follows
\begin{align}
\moment{\tnabla q(\rvbx)\tp \rvb{g} - \Expectation_{\rvbx}\Bigbrackets{\tnabla q(\rvbx)\tp \rvb{g}}}{p} & \sequal{(a)} \Bigparenth{\Expectation_{\rvbx, \rvb{g}} \Bigbrackets{\Bigparenth{\tnabla q(\rvbx)\tp \rvb{g} - \Expectation_{\rvbx}\Bigbrackets{\tnabla q(\rvbx)\tp \rvb{g}}}^p}}^{1/p} \\
& \sless{\cref{eq_bound_first_term_fixed_g}} 2\sqrt{2}\sigma \Bigparenth{\Expectation_{\rvb{g}, \rvbx, \rvb{g}'}\Bigbrackets{\Bigparenth{\rvb{g}\tp \tnabla^2 q(\rvbx) \rvb{g}'}^p}}^{1/p}, \label{eq_bound_first_term_interim}
%
%
\end{align}
where $(a)$ follows from the definition of $p$-th moment. Finally, to bound the RHS of \cref{eq_bound_first_term_interim}, we fix $\rvbx = \svbx$ and bound the $p$-th norm of the quadratic form $\rvb{g}\tp \tnabla^2 q(\svbx) \rvb{g}'$ by the Hanson-Wright inequality resulting in
\begin{align}
\Bigparenth{\Expectation_{\svb{g},  \rvb{g}' | \rvbx = \svbx}\Bigbrackets{\Bigparenth{\rvb{g}\tp \tnabla^2 q(\svbx) \rvb{g}'}^p}}^{1/p} & \leq c \Bigparenth{\sqrt{p} \fronorm{\tnabla^2 q(\svbx)} + p  \opnorm{\tnabla^2 q(\svbx)}}\\
& \leq c \Bigparenth{\sqrt{p} \max_{\svbx \in \cX^p} \fronorm{\tnabla^2 q(\svbx)} + p \max_{\svbx \in \cX^p} \opnorm{\tnabla^2 q(\svbx)}}, \label{eq_hanson_wright}
\end{align}
where $c \geq 0$ is a universal constant. Then, \cref{eq_bound_first_term_minkowski} follows by putting together \cref{eq_bound_first_term_interim,eq_hanson_wright}.

%
%
%
%

\paragraph{Proof of bound \cref{eq_bound_second_term_minkowski}} By linearity of expectation, we have
\begin{align}
\smoment{\Expectation_{\rvbx}\bigbrackets{\tnabla q(\rvbx)\tp \rvb{g}}}{p} = \smoment{\bigparenth{\Expectation_{\rvbx}\bigbrackets{\tnabla q(\rvbx)}}\tp \rvb{g}}{p}. \label{eq_bound_second_term_minkowski_0}
\end{align}
We note that the random variable $\dfrac{\normalparenth{\Expectation_{\rvbx}\normalbrackets{\tnabla q(\rvbx)}}\tp \rvb{g}}{\stwonorm{\Expectation_{\rvbx}\normalbrackets{\tnabla q(\rvbx)}}}$ is a standard normal random variable. Therefore,
\begin{align}
\moment{\frac{\bigparenth{\Expectation_{\rvbx}\bigbrackets{\tnabla q(\rvbx)}}\tp \rvb{g}}{\stwonorm{\Expectation_{\rvbx}\bigbrackets{\tnabla q(\rvbx)}}}}{p} \sequal{(a)} \biggparenth{\Expectation_{\rvb{g}}\biggbrackets{\biggparenth{\frac{\bigparenth{\Expectation_{\rvbx}\bigbrackets{\tnabla q(\rvbx)}}\tp \rvb{g}}{\stwonorm{\Expectation_{\rvbx}\bigbrackets{\tnabla q(\rvbx)}}}}^p}}^{1/p} \sless{(b)} 2\sqrt{p}, \label{eq_normal_application_2}
\end{align}
where $(a)$ follows from the definition of $p$-th moment, and $(b)$ follows since $\moment{\rv{g}}{p} \leq 2\sqrt{p}$ for any standard normal variable $\rv{g}$. Then, \cref{eq_bound_second_term_minkowski} follows by using \cref{eq_normal_application_2} in \cref{eq_bound_second_term_minkowski_0}.

%