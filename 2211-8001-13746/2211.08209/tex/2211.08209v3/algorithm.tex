%
%
%
%
%
%
%
%
%


\subsection{An efficient algorithm via a convex objective}
\label{sec_algo}
%

We first describe our strategy to estimate the parameters in \cref{eq_parameters_of_interest}. Then, we use the estimated parameters to estimate the expected potential outcomes in \cref{eq_causal_estimand}.  We remark that for exponential families considered here, maximum likelihood for parameter estimation is not computationally tractable \citep{wainwright2008graphical,ShahSW2021B}. {As a result, we resort to an alternative objective function inspired by the convex loss functions used in \cite{vuffray2016interaction,VuffrayML2022,ShahSW2021A} as they do not depend on the partition function of the distribution. These loss functions are designed in a specific way (see below for details): (i) the sufficient statistics of the conditional distribution of a variable given all other variables are \textit{centered} by adding appropriate constants, (ii) the loss function is an empirical average of the sum of the inverses of all of these conditional distributions (without the partition function) with \textit{centered} sufficient statistics.}
%
%
%
%
%
%
%
%
%

%
%

%
%
%
%
%
%
%
%
%
%
%
%



\subsubsection{Parameter estimation}
\label{subsec_loss_function}

Our convex objective function jointly learns all the parameters of interest by pooling the observations across all $n$ units and exploiting the exponential family structure of $\rvbv$, $\rvba$, and $\rvby$ conditioned on $\rvbz = \svbz$ in \cref{eq_conditional_distribution_vay}, i.e., the objective explicitly utilizes the fact that the population-level parameter $\TrueParameterMatrix$ is shared across units.
In particular, we use the following two steps.

\paragraph{Centering sufficient statistics of the conditional distribution of a variable}
Consider the conditional distribution $f_{\rvx_t | \rvbx_{-t}, \rvbz}$ of the random variable $\rvx_t$ conditioned on $\rvbx_{-t} = \svbx_{-t}$ and $\rvbz = \svbz$ for any $t \in [p]$:
{\begin{align}
    f_{\rvx_t | \rvbx_{-t}, \rvbz}\bigparenth{x_t| \svbx_{-t}, \svbz; \ExternalFieldt(\svbz), \ParameterRowt} \propto \exp\biggparenth{ \bigbrackets{\ExternalFieldt(\svbz) + 2\ParameterRowttt\tp \svbx_{-t}} x_t + \ParameterTU[tt] x_t^2},
    \label{eq_conditional_dist_non_centered}
\end{align}}
where $\ExternalFieldt(\svbz)$ is the $t^{th}$ element of $\ExternalField(\svbz)$, $\ParameterRowt$ is the $t^{th}$ row of $\ParameterMatrix$, $\ParameterTU[tt]$ is the $t^{th}$ element of $\ParameterRowt$, and $\ParameterRowttt \defn \ParameterRowt \setminus \ParameterTU[tt] \in \Reals^{p-1}$ is the vector obtained after deleting $\ParameterTU[tt]$ from $\ParameterRowt$.
%
%
%
%
%
%
%
{Then, the sufficient statistics in \cref{eq_conditional_dist_non_centered}, namely $\rvx_t$ and $\rvx_t^2$, are centered by subtracting their expected value with respect to the uniform distribution on $\cX$ resulting in}
\begin{align}
    f_{\rvx_t | \rvbx_{-t}, \rvbz}\bigparenth{x_t| \svbx_{-t}, \svbz; \ExternalFieldt(\svbz), \ParameterRowt} \propto \exp\biggparenth{ \bigbrackets{\ExternalFieldt(\svbz) + 2\ParameterRowttt\tp \svbx_{-t}} x_t + \ParameterTU[tt] \Bigparenth{x_t^2 - \frac{\xmax^2}{3}}},
    \label{eq_conditional_dist}
\end{align}
{as the integral of $\rvx_t$ and $\rvx_t^2$ with respect to the uniform distribution on $\cX$ is $0$ and $\xmax^2/3$, respectively. As we see later (in \cref{prop_proper_loss_function}), this centering ensures that our loss function is a proper loss function as well as leads to connections with the surrogate likelihood~\citep[Proposition. 4.1]{ShahSW2021A}. We emphasize that the term $\xmax^2/3$ inside the exponent in \cref{eq_conditional_dist} is vacuous (as it is a constant) and the distribution in \cref{eq_conditional_dist} is equivalent to the one in \cref{eq_conditional_dist_non_centered}.} 

\paragraph{Constructing the loss function}
{Next, the loss function (defined below) is desgined to be an empirical average of the sum over $t \in [p]$ of the inverse of the term in the right hand side of \cref{eq_conditional_dist}.}
%
%
%
%
%
%
%
%
\begin{definition}[\tbf{Loss function}]\label{def-loss-function}
%
Given the samples $\sbraces{\svbx^{(i)}}_{i \in [n]}$, the loss $\loss:\Reals^{p \times (n+p)} \to \Reals$ is given by
%
%
%
%
%
%
%
%
%
%
%
%
\begin{align}
    \loss\bigparenth{\ExtendedParameterMatrix} \!=\! \frac{1}{n}\sump[t] \sumn[i] \! \exp\biggparenth{\!-\!\bigbrackets{\ExternalFieldtI \!+\! 2\ParameterRowttt\tp \svbx_{-t}^{(i)}} x_t^{(i)}\!-\!\ParameterTU[tt] \Bigparenth{[x_t^{(i)}]^2 - \frac{\xmax^2}{3}}}
    \!\!\qtext{where}
    \ExtendedParameterMatrix \!\defeq\!\! \begin{bmatrix} \ExtendedParameterRowT[1]\tp \\ \vdots \\ \ExtendedParameterRowT[p]\tp \end{bmatrix},
    %
    %
    %
    \label{eq:loss_function}
    \end{align}
    and $\ExtendedParameterRowT[t] \!\defn\! \bigbraces{\ExternalFieldtI[1], \cdots, \ExternalFieldtI[n], \ParameterRowt}$ for $t\in[p]$.
    %
    %
%
%
\end{definition}
\noindent Our estimate of $\ExtendedTrueParameterMatrix$ (defined analogous to $\ExtendedParameterMatrix$) is given by
%
%
%
\begin{align}
    \ExtendedEstimatedParameterMatrix \in \argmin_{\ExtendedParameterMatrix \in \ParameterSet_{\ExternalField}^n \times \ParameterSet_{\ParameterMatrix}} \loss\bigparenth{\ExtendedParameterMatrix}.
    \label{eq_estimated_parameters}
\end{align}
%
%
We note \cref{eq_estimated_parameters} is a convex optimization problem, and a projected gradient descent algorithm (see \cref{subsec_alg}) returns an $\epsilon$-optimal estimate with $\tau = O(p/\epsilon)$ iterations\footnote{This follows from \cite[Theorem. 3.7]{bubeck2015convex} by noting that $\loss(\ExtendedParameterMatrix)$ is $O(p)$ smooth function of $\ExtendedParameterMatrix$.} where $\ExtendedEstimatedParameterMatrix_{\epsilon}$ is said to be an $\epsilon$-optimal estimate if $\loss\bigparenth{\ExtendedEstimatedParameterMatrix_{\epsilon}} \leq \loss\bigparenth{\ExtendedEstimatedParameterMatrix} + \epsilon$ for any $\epsilon > 0$. The loss function $\loss$ 
%
%
%
admits a notable property (see \cref{sec_proof_proper_loss_function} for the proof).
%
%
%
%
%
%
%
%
%
\newcommand{\properlossfunction}{Proper loss function}
\begin{proposition}[\tbf{\properlossfunction}]\label{prop_proper_loss_function} The loss function $\loss$ is strictly proper, i.e., $\ExtendedTrueParameterMatrix = \argmin_{\ExtendedParameterMatrix \in \ParameterSet_{\ExternalField}^n \times \ParameterSet_{\ParameterMatrix}} \Expectation_{\rvbx|\rvbz}\bigbrackets{\loss\bigparenth{\ExtendedParameterMatrix}}$.
%
%
%
%
%
%
\end{proposition}
\cref{prop_proper_loss_function} shows that the solution of the idealized convex program $\min_{\ExtendedParameterMatrix \in \ParameterSet_{\ExternalField}^n \times \ParameterSet_{\ParameterMatrix}} \Expectation_{\rvbx|\rvbz}\bigbrackets{\loss\bigparenth{\ExtendedParameterMatrix}}$ is unique and equal to $\ExtendedTrueParameterMatrix$. {In this idealized convex program, conditioned on the realized values of the unobserved covariates of the $n$ units $\svbz^{(1)}, \cdots, \svbz^{(n)}$, the loss function is averaged over all the randomness in the observed covariates, the interventions, and the outcomes. In other words, for every $i \in [n]$, the idealized convex program has infinite samples from $f_{\rvbx | \rvbz}$ with unobserved covariates $\rvbz$ conditioned to be $\svbz^{(i)}$.} Thus, the convex program in \cref{eq_estimated_parameters} can be seen as a {single} sample version of this idealized program, thereby providing an intuitive justification of our loss function (instead of a maximum likelihood objective, which is not  tractable here). As we show later in our proofs (see \cref{sec_proof_sketch} for an overview), different partial averages on the RHS of \cref{eq:loss_function} also admit useful properties and are critical to our analyses.
%
%

{We note that loss function  in \cref{eq:loss_function} is a generalization of the loss functions used in \cite{vuffray2016interaction,VuffrayML2022,ShahSW2021A}. In particular, if the unobserved confounding is identical across units, i.e., $\TrueExternalFieldI[1] = \cdots = \TrueExternalFieldI[n]$, then $\loss\bigparenth{\ExtendedParameterMatrix}$ in \cref{eq:loss_function} can be decomposed into $p$ independent loss functions, one for every $t \in [p]$. These decomposed loss functions are identical to the ones used in these prior works.}

%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%


\subsubsection{Causal estimate}
\label{subsec_causal_estimate}
Given the estimate $\ExtendedEstimatedParameterMatrix$, our estimate of the expected potential outcome $\mu^{(i)}(\wtil{\svba}^{(i)})$ under an alternate intervention $\wtil{\svba}^{(i)} \in \cA^{p_a}$~\cref{eq_causal_estimand} is derived as follows:
%
First, we identify $\EstimatedPhi^{(u, y)} \in \Reals^{p_u \times p_y}$ to be the component of $\EstimatedParameterMatrix$ corresponding to $\rvbu$ and $\rvby$ for all $\rvbu \in \{\rvbv, \rvba, \rvby\}$ and $\EstimatedExternalFieldI[i,y] \in \Reals^{p_y}$ to be the component of $\EstimatedExternalFieldI$ corresponding to $\rvby$. Next, we estimate the conditional distribution of $\rvby$ for unit $i$ as a function of the interventions $\rvba$, while keeping $\rvbv=\svbv^{(i)}$ and $\rvbz=\svbz^{(i)}$ fixed as 
\begin{align}
    \what{f}^{(i)}_{\rvby | \rvba}(\svby | \svba) \propto \exp\Bigparenth{\bigbrackets{\EstimatedExternalFieldI[i,y] + 2\svbv^{(i)\top}\EstimatedPhi^{(v,y)} + 2\svba\tp\EstimatedPhi^{(a,y)}} \svby + \svby\tp\EstimatedPhi^{(y,y)} \svby}. \label{eq_counterfactual_distribution_y}
\end{align}
Finally, we estimate $\mu^{(i)}(\wtil{\svba}^{(i)})$ as the mean under the above conditional distribution, given by
\begin{align}
    %
    \what{\mu}^{(i)}(\wtil{\svba}^{(i)}) & \defn \Expectation_{\what{f}^{(i)}_{\rvby | \rvba}}[\rvby | \rvba = \wtil{\svba}^{(i)}],  
    \label{eq_causal_estimate}
\end{align}
which can be computed by standard algorithms for estimating marginals of graphical models, e.g., via the junction tree algorithm~\citep{wainwright2008graphical} or message-passing algorithms.\footnote{In general, estimating the marginals exactly is computationally hard for undirected graphical models. While the junction tree algorithm works well for graphical models with small treewidth~\citep[Section. 2.5]{wainwright2008graphical}, e.g., for trees or chains as in hidden Markov models or state-space models, message-passing algorithms are the default choice for computing approximate marginals for complex graphs, especially with cycles. However, message-passing algorithms may induce additional approximations, which we do not discuss here.} 
%

%
%

%

%

%

%
%
%




%
%

%

%
