\section{Proper loss function and projected gradient descent}
\label{sec_proper_convex}
{In this section, we prove  \cref{prop_proper_loss_function} showing that the loss function in \cref{eq:loss_function} is a proper loss function. We also 
%
provide an algorithm to obtain an $\epsilon$-optimal estimate of $\ExtendedEstimatedParameterMatrix$.}

\subsection{Proof of {Proposition \ref{prop_proper_loss_function}}}
\label{sec_proof_proper_loss_function}
Fix any $\svbz \in \cZ^{p_z}$. For every $t \in [p]$, define the following parametric distribution 
\begin{align}
    u_{\rvbx|\rvbz}\bigparenth{\svbx| \svbz; \ExternalFieldt(\svbz), \ParameterRowt} \propto \dfrac{\TrueJointDist}{\ConditionalDistT}, \label{eq_u_distribution}
\end{align}
where $\TrueJointDist$ is as defined in \cref{eq_conditional_distribution_vay} and $\ConditionalDistT$ is as defined in \cref{eq_conditional_dist}. Letting $\cx_t \defn x_t^2 - \xmax^2/3$ and using \cref{eq_conditional_dist}, we can write $u_{\rvbx|\rvbz}\bigparenth{\svbx| \svbz; \ExternalFieldt(\svbz), \ParameterRowt}$ in \cref{eq_u_distribution} as
\begin{align}
    u_{\rvbx|\rvbz}\bigparenth{\svbx| \svbz; \ExternalFieldt(\svbz), \ParameterRowt} \propto \TrueJointDist \exp\bigparenth{ -\normalbrackets{\ExternalFieldt(\svbz) + 2\ParameterRowttt\tp \svbx_{-t}}x_t - \ParameterTU[tt]\cx_t }. \label{eq_u_distribution_2}
\end{align}
Then, we have
\begin{align}
    u_{\rvbx|\rvbz}\bigparenth{\svbx| \svbz; \ExternalFieldt(\svbz), \ParameterRowt} & = \dfrac{\TrueJointDist \exp\bigparenth{\!-\!\normalbrackets{\ExternalFieldt(\svbz) \!+\! 2\ParameterRowttt\tp \svbx_{-t}}x_t \!-\! \ParameterTU[tt]\cx_t }}{\!\!\!\!\int_{\svbx \in \cX^p} \! \TrueJointDist \exp\bigparenth{ \!-\!\normalbrackets{\ExternalFieldt(\svbz) \!+\! 2\ParameterRowttt\tp \svbx_{-t}}x_t \!-\! \ParameterTU[tt]\cx_t } d\svbx}\\
    & = \dfrac{\TrueJointDist \exp\bigparenth{ -\normalbrackets{\ExternalFieldt(\svbz) + 2\ParameterRowttt\tp \svbx_{-t}}x_t - \ParameterTU[tt]\cx_t }}{\Expectation_{\rvbx|\rvbz}\Bigbrackets{\exp\bigparenth{ -\normalbrackets{\ExternalFieldt(\svbz) + 2\ParameterRowttt\tp \svbx_{-t}}x_t - \ParameterTU[tt]\cx_t }}}. \label{eq_u_alternate}
\end{align}
Further, for $\ExternalFieldt(\svbz) = \TrueExternalFieldt(\svbz),$ and $\ParameterRowt = \TrueParameterRowt$, we can write an expression for  $u_{\rvbx|\rvbz}\bigparenth{\svbx| \svbz; \TrueExternalFieldt(\svbz), \TrueParameterRowt}$ which does not depend on $\rvx_t$ functionally. From \cref{eq_conditional_dist}, we have
\begin{align}
    u_{\rvbx|\rvbz}\bigparenth{\svbx| \svbz; \TrueExternalFieldt(\svbz), \TrueParameterRowt} \propto f_{\rvbx_{-t}|\rvbz}\bigparenth{\svbx_{-t}| \svbz; \TrueExternalField(\svbz), \TrueParameterMatrix}. \label{eq_u_star_no_dependence}
\end{align}
Now, consider the difference between $\KLD{u_{\rvbx|\rvbz}\bigparenth{\svbx| \svbz; \TrueExternalFieldt(\svbz), \TrueParameterRowt}}{u_{\rvbx|\rvbz}\bigparenth{\svbx| \svbz; \ExternalFieldt(\svbz), \ParameterRowt}}$ and \\
$\KLD{u_{\rvbx|\rvbz}\bigparenth{\svbx| \svbz; \TrueExternalFieldt(\svbz), \TrueParameterRowt}}{\TrueJointDist}$. We have
\begin{align}
    & \KLD{u_{\rvbx|\rvbz}\bigparenth{\cdot| \svbz; \TrueExternalFieldt(\svbz), \TrueParameterRowt}}{u_{\rvbx|\rvbz}\bigparenth{\cdot| \svbz; \ExternalFieldt(\svbz), \ParameterRowt}} \\
    & \qquad \qquad - \KLD{u_{\rvbx|\rvbz}\bigparenth{\cdot| \svbz; \TrueExternalFieldt(\svbz), \TrueParameterRowt}}{\TrueJointDistfun}\\
    & \sequal{(a)} \int_{\svbx \in \cX^p} \!\!\!\! u_{\rvbx|\rvbz}\bigparenth{\svbx| \svbz; \TrueExternalFieldt(\svbz), \TrueParameterRowt} \log \dfrac{\TrueJointDist}{u_{\rvbx|\rvbz}\bigparenth{\svbx| \svbz; \ExternalFieldt(\svbz), \ParameterRowt}} d\svbx \\
    & \sequal{\cref{eq_u_alternate}} \int_{\svbx \in \cX^p} \!\!\!\! u_{\rvbx|\rvbz}\bigparenth{\svbx| \svbz; \TrueExternalFieldt(\svbz), \TrueParameterRowt} \log \dfrac{\Expectation_{\rvbx|\rvbz}\Bigbrackets{\exp\bigparenth{ -\normalbrackets{\ExternalFieldt(\svbz) + 2\ParameterRowttt\tp \svbx_{-t}}x_t - \ParameterTU[tt]\cx_t }}}{\exp\bigparenth{ -\normalbrackets{\ExternalFieldt(\svbz) + 2\ParameterRowttt\tp \svbx_{-t}}x_t - \ParameterTU[tt]\cx_t }} d\svbx \\
    & = \log \Expectation_{\rvbx|\rvbz}\Bigbrackets{\exp\bigparenth{ -\normalbrackets{\ExternalFieldt(\svbz)+2\ParameterRowttt\tp \svbx_{-t}}x_t -\ParameterTU[tt]\cx_t }} \\
    & \qquad \qquad - \int_{\svbx \in \cX^p}  \!\!\!\! u_{\rvbx|\rvbz}\bigparenth{\svbx| \svbz; \TrueExternalFieldt(\svbz), \TrueParameterRowt} \bigparenth{ \normalbrackets{\ExternalFieldt(\svbz) + 2\ParameterRowttt\tp \svbx_{-t}}x_t + \ParameterTU[tt]\cx_t }   d\svbx \\
    & \sequal{(b)} \log \Expectation_{\rvbx|\rvbz}\Bigbrackets{\exp\bigparenth{ -\normalbrackets{\ExternalFieldt(\svbz) + 2\ParameterRowttt\tp \svbx_{-t}}x_t - \ParameterTU[tt]\cx_t }}, \label{eq_kl_difference}
\end{align}
where $(a)$ follows from the definition of KL-divergence and $(b)$ follows because integral is zero since $u_{\rvbx|\rvbz}\bigparenth{\svbx| \svbz; \TrueExternalFieldt(\svbz), \TrueParameterRowt}$ does not functionally depend on $x_t$ as in \cref{eq_u_star_no_dependence}, and $\int_{x_t \in \cX} x_t dx_t= 0$ and $\int_{x_t \in \cX} \cx_t dx_t= 0$. Now, we can write
\begin{align}
    \Expectation_{\rvbx|\rvbz}\bigbrackets{\loss\bigparenth{\ExtendedParameterMatrix}} & = \frac{1}{n} \sump[t] \sumn[i]  \Expectation_{\rvbx|\rvbz}\Bigbrackets{\exp\bigparenth{ -\normalbrackets{\ExternalFieldt(\svbz^{(i)}) + \ParameterRowttt\tp \svbx_{-t}^{(i)}}x_t^{(i)} - \ParameterTU[tt]\cx_t^{(i)} }} \\
    & \sequal{\cref{eq_kl_difference}} \frac{1}{n} \sump[t] \sumn[i] \exp\Bigparenth{\KLD{u_{\rvbx|\rvbz}\bigparenth{\cdot| \svbz^{(i)}; \TrueExternalFieldt(\svbz^{(i)}), \TrueParameterRowt}}{u_{\rvbx|\rvbz}\bigparenth{\cdot| \svbz^{(i)}; \ExternalFieldt(\svbz^{(i)}), \ParameterRowt}} \\
    &  \qquad \qquad\qquad  - \KLD{u_{\rvbx|\rvbz}\bigparenth{\cdot| \svbz^{(i)}; \TrueExternalFieldt(\svbz^{(i)}), \TrueParameterRowt}}{\TrueJointDistfunT}}.\label{eq_population_loss_kl_relationship}
\end{align}
We note that the parameters 
only show up in the first KL-divergence term in the right-hand-side of \cref{eq_population_loss_kl_relationship}. Therefore, it is easy to see that $\Expectation_{\rvbx|\rvbz}\bigbrackets{\loss\bigparenth{\ExtendedParameterMatrix}}$ is minimized uniquely when $\ExternalFieldt(\svbz^{(i)}) = \TrueExternalFieldt(\svbz^{(i)})$ and $\ParameterRowt = \TrueParameterRowt$ for all $t \in [p]$ and all $i \in [n]$, i.e., when $\ExtendedParameterMatrix = \ExtendedTrueParameterMatrix$.
%

\subsection{Algorithm}
\label{subsec_alg}
{In this section, we provide a projected gradient descent algorithm to return an $\epsilon$-optimal estimate of the convex optimization in \cref{eq_estimated_parameters}. We note that alternative algorithms (including Frank-Wolfe) can also be used.}

\begin{algorithm}[h]
    \SetCustomAlgoRuledWidth{0.4\textwidth} 
    %
    \KwInput{number of iterations $\tau$, step size $\eta$, $\epsilon$, parameter sets $\ParameterSet_{\ExternalField}$ and $ \ParameterSet_{\ParameterMatrix}$}
    \KwOutput{$\epsilon$-optimal estimate $\ExtendedEstimatedParameterMatrix_{\epsilon}$}
    \KwInitialization{$\ExtendedParameterMatrix^{(0)} = \boldsymbol{0}$} 
    %
    {
    \For{$j = 0,\cdots,\tau$}
    {
        $\ExtendedParameterMatrix^{(j+1)} \leftarrow \argmin_{\ExtendedParameterMatrix \in \ParameterSet_{\ExternalField}^n \times \ParameterSet_{\ParameterMatrix}} \stwonorm{\ExtendedParameterMatrix^{(j)} - \eta  \nabla \loss\bigparenth{\ExtendedParameterMatrix^{(j)}} - \ExtendedParameterMatrix}$
    }
    $\ExtendedEstimatedParameterMatrix_{\epsilon} \leftarrow \ExtendedParameterMatrix^{(\tau+1)}$
    }
    \caption{Projected Gradient Descent}
    \label{alg:GradientDescent}
\end{algorithm}
{We note that, in general, projecting onto the space $\ParameterSet_{\ExternalField}^n \times \ParameterSet_{\ParameterMatrix}$ may not be easy depending on the specific form of $\ParameterSet_{\ExternalField}$. For \cref{exam:lc_dense,exam:sc}, projecting on $\ParameterSet_{\ExternalField}$ is equivalent to projecting onto the $k$-dimensional vector $\mbf a$. For \cref{exam:sc}, the $\ell_0$-sparsity is relaxed to $\ell_1$ sparsity. We also do not focus on any issues that may arise due to the choice of the step size $\eta$.}