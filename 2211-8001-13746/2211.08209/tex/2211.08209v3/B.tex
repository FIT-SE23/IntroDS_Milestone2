\section{Proof of {Theorem \ref{theorem_parameters}} Part I: \edgeparammainresultname}
\label{sec:proof_of_theorem_parameters}

To prove this part, it is sufficient to show that all points $\ExtendedParameterMatrix \in \ParameterSet_{\ParameterMatrix} \times \ParameterSet_{\ExternalField}^n$, such that $\stwonorm{\ParameterRowt - \TrueParameterRowt} \geq \varepsilon$ for at least one $t \in [p]$, uniformly satisfy 
\begin{align}
\loss(\ExtendedParameterMatrix) \geq \loss(\ExtendedTrueParameterMatrix) + \Omega(\varepsilon^2) \stext{for} n \geq \frac{ce^{c'\bGM} p^2}{\varepsilon^4} \cdot \biggparenth{p \log \frac{p}{\delta} + \metric_{\ExternalField,n}\big(\varepsilon^2 \big)},
%
\label{eq_sufficienct_condition_thm_edge}
\end{align}
with probability at least $1-\delta$. Then, the guarantee in \cref{theorem_parameters} follows from \cref{eq_estimated_parameters} by contraposition.


%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%

%
To that end, we decompose $\loss(\ExtendedParameterMatrix)$ in \cref{eq:loss_function} as a sum of $p$ convex (and positive) auxiliary objectives $\loss_t\bigparenth{\ExtendedParameterRowT}$, i.e., $\loss(\ExtendedParameterMatrix) = \sump \loss_t\bigparenth{\ExtendedParameterRowT}$ where
\begin{align}\loss_t\bigparenth{\ExtendedParameterRowT} \defn \frac{1}{n} \sumn[i] \exp\Bigparenth{\!-\!\normalbrackets{\ExternalFieldtI \!+\! 2\ParameterRowttt\tp \svbx_{-t}^{(i)}} x_t^{(i)}\!-\!\ParameterTU[tt]\cx_t^{(i)}},
%
\label{eq:loss_function_p}
\end{align}
%
with $\cx_t^{(i)} = \bigbrackets{x_t^{(i)}}^2 - \xmax^2/3$ and $\ExtendedParameterRowT = \bigbraces{\ExternalFieldtI[1], \cdots, \ExternalFieldtI[n], \ParameterRowt}$ as defined in \cref{eq:loss_function}. The lemma below, proven in \cref{proof_of_lemma_parameter}, shows that for any fixed and feasible $\ExtendedParameterRowT$, if $\ParameterRowt$ is far from $\TrueParameterRowt$, then with high probability $\loss_t\bigparenth{\ExtendedParameterRowT}$ is significantly larger than $\loss_t\bigparenth{\ExtendedTrueParameterRowT}$. The lemma uses 
%
%
%
%
%
%
%
%
the following constants that depend on model parameters $\tau \defeq (\aGM, \bGM, \xmax, \ParameterMatrix)$:
\begin{align}
\label{eq:constants}
\cone \!\defeq\! \aGM \!+\! 4 \bGM \xmax
\qtext{and}
\ctwo \defeq \exp{(\xmax(\aGM+ 2\bGM \xmax))}.
\end{align}
\newcommand{\parameterseparation}{Gap between the loss function for a fixed parameter}
\begin{lemma}[{\parameterseparation}]\label{lemma_parameter}
	Consider any $\ExtendedParameterMatrix \in \ParameterSet_{\ExternalField}^n \times \ParameterSet_{\ParameterMatrix}$. Fix any $\delta \in (0,1)$. Then, we have uniformly for all $t \in [p]$
	\begin{align}
	\loss_t\bigparenth{\ExtendedParameterRowT} \geq \loss_t\bigparenth{\ExtendedTrueParameterRowT} + \frac{\lambda_{\min} \twonorm{\ParameterRowt - \TrueParameterRowt}^2}{2\ctwo} - \varepsilon
	%
	\qtext{for} n \geq
	\dfrac{ce^{c'\bGM} \log(p/\delta)}{\varepsilon^2},
	\end{align}
	with probability at least  $1-\delta$, where $\ctwo$ was defined in \cref{eq:constants}. 
	%
\end{lemma}
\newcommand{\lipschitznesslossfunction}{Lipschitzness of the loss function}
%
%
%
%
%
%
%
%
%
{
	\noindent Next, we show that the loss function $\loss$ is Lipschitz (see \cref{sub:proof_lemma_lipschitzness_first_stage} for the proof).
	\begin{lemma}[{\lipschitznesslossfunction}]\label{lemma_lipschitzness_first_stage}
		Consider any $\ExtendedParameterMatrix, \tExtendedParameterMatrix \in \ParameterSet_{\ExtendedParameterMatrix}$. Then, the loss function $\loss$ is $2\xmax^2 \ctwo$-Lipschitz in a suitably-adjusted $\ell_1$ norm:
		\begin{align}  \bigabs{\loss\bigparenth{\tExtendedParameterMatrix} - \loss\bigparenth{\ExtendedParameterMatrix}} \leq 2\xmax^2 \ctwo \Bigparenth{\sump \sonenorm{\tParameterRowt - \ParameterRowt}  + \frac{1}{n} \sumn \sonenorm{\tExternalFieldI -\ExternalFieldI}}, \label{eq_lipschitz_property_first_stage}
		\end{align}
		where the constant $\ctwo$ was defined in \cref{eq:constants}.
\end{lemma}}
\noindent Given these lemmas, we now proceed with the proof.

\paragraph{Proof strategy} We want to show that all points $\ExtendedParameterMatrix \in \ParameterSet_{\ParameterMatrix} \times \ParameterSet_{\ExternalField}^n$, such that $\stwonorm{\ParameterRowt - \TrueParameterRowt} \geq \varepsilon$ for at least one $t \in [p]$, uniformly satisfy \cref{eq_sufficienct_condition_thm_edge} with probability at least $1-\delta$. To do so, 
%
we consider the set of feasible $\ExtendedParameterMatrix$ such that the distance of $\ParameterRowt$ 
%
from $\TrueParameterRowt$ is at least $\varepsilon > 0$ in $\ell_2$ norm for some $t \in [p]$, and denote the set by $\ParameterSet_{\ParameterMatrix}^{\varepsilon} \times \ParameterSet_{\ExternalField}^n$ (see \cref{eq_set_cover_1} and \cref{eq_parameter_set_external_field}). Then, using an appropriate covering set of $\ParameterSet_{\ParameterMatrix}^{\varepsilon} \times \ParameterSet_{\ExternalField}^n$ and the Lipschitzness of $\loss$, we show that the value of $\loss$ at all points in $\ParameterSet_{\ParameterMatrix}^{\varepsilon} \times \ParameterSet_{\ExternalField}^n$ is uniformly $\Omega(\varepsilon^2)$ larger than the value of $\loss$ at $\ExtendedTrueParameterMatrix$ with high probability. 
%

\paragraph{Arguments for points in the covering set} 
Define the set
\begin{align}
\ParameterSet_{\ParameterMatrix}^{\varepsilon} \defn  \braces{ \ParameterMatrix \in \Reals^{p \times p}: \ParameterMatrix = \ParameterMatrix\tp, \maxmatnorm{\ParameterMatrix} \leq \aGM, \infmatnorm{\ParameterMatrix} \leq \bGM, \max_{t\in[p]}\stwonorm{\TrueParameterRowt - \ParameterRowt} \geq \varepsilon}.\label{eq_set_cover_1}
\end{align}
%
Let $\cU(\ParameterSet_{\ParameterMatrix}^{\varepsilon}, \varepsilon')$ be the $\varepsilon'$-cover of smallest size for the set $\ParameterSet_{\ParameterMatrix}^{\varepsilon}$ with respect to $\sonenorm{\cdot}$ (see \cref{def_covering_number_metric_entropy}) and let $\cC(\ParameterSet_{\ParameterMatrix}^{\varepsilon}, \varepsilon') = \normalabs{\cU(\ParameterSet_{\ParameterMatrix}^{\varepsilon}, \varepsilon')}$ be the $\varepsilon'$-covering number. Similarly, let $\cU(\ParameterSet_{\ParameterMatrix}^{\varepsilon}, \varepsilon'')$ be the $\varepsilon''$-cover of the smallest size for the set $\ParameterSet_{\ExternalField}^n$ with respect to $\sonenorm{\cdot}$ and let $\cC(\ParameterSet_{\ExternalField}^n, \varepsilon'') = \normalabs{\cU(\ParameterSet_{\ParameterMatrix}^{\varepsilon}, \varepsilon'')}$ be the $\varepsilon''$-covering number.
%
%
We choose
\begin{align}
\varepsilon' \defn  \dfrac{\lambda_{\min} \varepsilon^2}{32 \xmax^2 \ctwo[2]} \qtext{and} \varepsilon'' \defn  \dfrac{\lambda_{\min} \varepsilon^2 n}{32 \xmax^2 \ctwo[2]}. \label{eq_varepsilon_stage_1}
\end{align}
%
Now, we argue by a union bound that the value of $\loss$ at all points in $\cU(\ParameterSet_{\ParameterMatrix}^{\varepsilon}, \varepsilon') \times \cU(\ParameterSet_{\ExternalField}^n, \varepsilon'')$ is uniformly $\Omega(\varepsilon^2)$ larger than $\loss(\ExtendedTrueParameterMatrix)$ with high probability. 
For any $\ExtendedParameterMatrix \in \cU(\ParameterSet_{\ParameterMatrix}^{\varepsilon}, \varepsilon') \times \cU(\ParameterSet_{\ExternalField}^n, \varepsilon'')$, we have
\begin{align}
\sump \stwonorm{\TrueParameterRowt - \ParameterRowt}^2 \sgreat{(a)} \varepsilon^2,\label{eq_lower_bound_two_norm_theta_diff_stage_1}
\end{align}
where $(a)$ follows because $\cU(\ParameterSet_{\ParameterMatrix}^{\varepsilon}, \varepsilon') \subseteq \ParameterSet_{\ParameterMatrix}^{\varepsilon}$. Now, applying \cref{lemma_parameter} with $\varepsilon \mapsfrom \lambda_{\min} \varepsilon^2/4\ctwo p$ and $\delta \mapsfrom \delta/(\cC(\ParameterSet_{\ParameterMatrix}^{\varepsilon}, \varepsilon') + \cC(\ParameterSet_{\ExternalField}^n, \varepsilon''))$ and summing over $t\in[p]$, we find that
\begin{align}
\sum_{t\in[p]}\loss_t\bigparenth{\ExtendedParameterRowT} &\geq \sum_{t\in[p] }\parenth{\loss_t\bigparenth{\ExtendedTrueParameterRowT} + \frac{\lambda_{\min} \twonorm{\ParameterRowt - \TrueParameterRowt}^2}{2\ctwo} - \frac{\lambda_{\min} \varepsilon^2}{4\ctwo p}} \\
\implies \qquad\qquad \loss\bigparenth{\ExtendedParameterMatrix} &\geq \loss\bigparenth{\ExtendedTrueParameterMatrix} + \frac{\lambda_{\min}}{2\ctwo} \sump \stwonorm{\TrueParameterRowt - \ParameterRowt}^2 - \frac{\lambda_{\min} \varepsilon^2}{4\ctwo} 
\\
&\sgreat{\cref{eq_lower_bound_two_norm_theta_diff_stage_1}} \loss\bigparenth{\ExtendedTrueParameterMatrix} + \frac{\lambda_{\min} \varepsilon^2}{4\ctwo},
\end{align}
with probability at least $1-\delta/(\cC(\ParameterSet_{\ParameterMatrix}^{\varepsilon}, \varepsilon') + \cC(\ParameterSet_{\ExternalField}^n, \varepsilon''))$ whenever
\begin{align}
n \geq \frac{ce^{c'\bGM} p^2 \log\bigparenth{(\cC(\ParameterSet_{\ParameterMatrix}^{\varepsilon}, \varepsilon') \times \cC(\ParameterSet_{\ExternalField}^n, \varepsilon'')) \cdot p/\delta}}{\lambda_{\min}^2 \varepsilon^4}. \label{eq_n_condition_edge_recovery_2}
\end{align}
%
%
%
%
By applying the union bound over $\cU(\ParameterSet_{\ParameterMatrix}^{\varepsilon}, \varepsilon') \times \cU(\ParameterSet_{\ExternalField}^n, \varepsilon'')$, as long as $n$ satisfies \cref{eq_n_condition_edge_recovery_2}, we have
\begin{align}
\loss\bigparenth{\ExtendedParameterMatrix} \geq \loss\bigparenth{\ExtendedTrueParameterMatrix}  + \frac{\lambda_{\min} \varepsilon^2}{4\ctwo} \stext{uniformly for every} \ExtendedParameterMatrix \in \cU(\ParameterSet_{\ParameterMatrix}^{\varepsilon}, \varepsilon') \times \cU(\ParameterSet_{\ExternalField}^n, \varepsilon''), \label{eq_union_bound_covering_set_stage_1} 
\end{align}
with probability at least $1-\delta$.

\paragraph{Arguments for points outside the covering set} Next, we establish the claim~\cref{eq_sufficienct_condition_thm_edge} for an arbitrary $\tExtendedParameterMatrix \in \ParameterSet_{\ParameterMatrix}^{\varepsilon} \times \ParameterSet_{\ExternalField}^n$ conditional on the event that \cref{eq_union_bound_covering_set_stage_1} holds.
%
Given a fixed $\tExtendedParameterMatrix \in \ParameterSet_{\ParameterMatrix}^{\varepsilon} \times \ParameterSet_{\ExternalField}^n$, let $\ExtendedParameterMatrix$ be (one of) the point(s) in the cover $\cU(\ParameterSet_{\ParameterMatrix}^{\varepsilon}, \varepsilon') \times \cU(\ParameterSet_{\ExternalField}^n, \varepsilon'')$ that satisfies $\sump \sonenorm{\tParameterRowt - \ParameterRowt} \leq \varepsilon'$ and $\sumn \sonenorm{\tExternalFieldI -\ExternalFieldI} \leq \varepsilon''$ (there exists such a point by~\cref{def_covering_number_metric_entropy}). Then, the choices~\cref{eq_varepsilon_stage_1} and \cref{lemma_lipschitzness_first_stage} put together imply that
\begin{align}
\loss\bigparenth{\tExtendedParameterMatrix} & \geq \loss\bigparenth{\ExtendedParameterMatrix} \!-\! 2\xmax^2 \ctwo \Bigparenth{\sump \sonenorm{\tParameterRowt - \ParameterRowt}  + \frac{1}{n} \sumn \sonenorm{\tExternalFieldI -\ExternalFieldI}} \label{eq_effect_average_1}\\
%
& \geq \loss\bigparenth{\ExtendedParameterMatrix} - 2\xmax^2 \ctwo \Bigparenth{ \varepsilon' + \frac{\varepsilon''}{n}} \sgreat{\cref{eq_varepsilon_stage_1}} \loss\bigparenth{\ExtendedParameterMatrix} \!-\!  \frac{\lambda_{\min} \varepsilon^2}{8\ctwo} \sgreat{\cref{eq_union_bound_covering_set_stage_1}} \loss\bigparenth{\ExtendedTrueParameterMatrix} \!+\! \frac{\lambda_{\min} \varepsilon^2}{8\ctwo}.
\end{align}
%
%


\paragraph{Bounding $n$} 
Using $\ParameterSet_{\ParameterMatrix}^{\varepsilon} \subseteq \ParameterSet_{\ParameterMatrix}$ and the outer product definition of $\ExternalField^n$, we find that
%
\begin{align}
\cC(\ParameterSet_{\ParameterMatrix}^{\varepsilon}, \varepsilon') \leq \cC(\ParameterSet_{\ParameterMatrix}, \varepsilon')
\qtext{and}
\cC(\ParameterSet_{\ExternalField}^n, \varepsilon'')
= (\cC(\ParameterSet_{\ExternalField}, \varepsilon''))^n.
\label{eq_bound_covering_number_stage_1}
\end{align}
%
%
Putting together \cref{eq_varepsilon_stage_1,eq_bound_covering_number_stage_1}, the lower bound \cref{eq_n_condition_edge_recovery_2} can be replaced by
\begin{align}
n \geq \frac{ce^{c'\bGM} p^2}{\lambda_{\min}^2 \varepsilon^4} \cdot \biggparenth{\log \frac{p}{\delta} + \log \cC\Big(\ParameterSet_{\ParameterMatrix}, \frac{\lambda_{\min} \varepsilon^2}{ce^{c'\bGM}}\Big) + n \log \cC\Big(\ParameterSet_{\ExternalField}, \frac{\lambda_{\min} n \varepsilon^2}{ce^{c'\bGM}}\Big)},
\end{align}
which yields the claim immediately after noting that
\begin{align}
\log \cC\Big(\ParameterSet_{\ParameterMatrix}, \frac{\lambda_{\min} \varepsilon^2}{ce^{c'\bGM}}\Big) \!=\! O\Big(\bGM^2 p \log\Big(\frac{1}{\lambda_{\min} \varepsilon^2}\Big)\Big) \stext{and} \log \cC\Big(\ParameterSet_{\ExternalField}, \frac{\lambda_{\min} n \varepsilon^2}{ce^{c'\bGM}}\Big) \!=\! \metric_{\ExternalField}\Big( \frac{\lambda_{\min} n \varepsilon^2}{ce^{c'\bGM}} \Big).
\end{align}
%
%

\subsection{Proof of \cref{lemma_parameter}: \parameterseparation}
\label{proof_of_lemma_parameter}
Fix any $\varepsilon >0$, any $\delta \in (0,1)$, and $t \in [p]$. Consider any direction $\uOmT \defn \bigbraces{\omtI[1], \cdots, \omtI[n], \Omt} \in \Reals^{n+p}$ along the parameter $\ExtendedParameterRowT$, i.e.,
\begin{align}
\label{eq:omega_defn}
\uOmT = \ExtendedParameterRowT - \ExtendedTrueParameterRowT,
\qtext{and}
\Omt = \ParameterRowt-\TrueParameterRowt.
\end{align}
%
%
%
We denote the first-order and the second-order directional derivatives of the loss function $\loss_t$ in \cref{eq:loss_function_p} along the direction $\uOmT$ evaluated at $\ExtendedParameterRowT$ by $\directionalGradient$ and $\directionalHessian$, respectively. 
%
Below, we state a lemma (with proof divided across \cref{sub:proof_of_lemma_conc_first_der} and \cref{sub:proof_of_lemma_conc_sec_der}) that provides us a control on $\directionalGradient$ and $\directionalHessian$. 
The assumptions of \cref{lemma_parameter} remain in force.
%

\newcommand{\concpopresultname}{Control on first and second directional derivatives}
\newcommand{\concgradresultname}{Concentration of first directional derivative}
\newcommand{\conchessresultname}{Anti-concentration of second directional derivative}

%

%



\begin{lemma}[{\concpopresultname}]\label{lemma_conc_first_sec_der}
	For any fixed $\varepsilon_1, \varepsilon_2 > 0$, $\delta_1, \delta_2 \in (0,1)$, $t \in [p]$, $\ExtendedParameterMatrix \in \ParameterSet_{\ExternalField}^n \times \ParameterSet_{\ParameterMatrix}$ defined in \cref{eq:loss_function} and $\Omt$ defined in \cref{eq:omega_defn}, we have the following:
	\begin{enumerate}[label=(\alph*)]
		\item\label{item_conc_first_der} \textnormal{{\concgradresultname}}: with probability at least $1-\delta_1$,
		\begin{align}
		\bigabs{\directionalGradientTrue} \leq \varepsilon_1
		\stext{for}
		n \geq \dfrac{8\cone[2] \ctwo[2] \xmax^2  \log \frac{2p}{\delta_1} }{\varepsilon_1^2}
		\stext{and uniformly for all $t \in [p]$.}
		\end{align}
		\item\label{item_conc_sec_der} \textnormal{{\conchessresultname}}: with probability at least $1-\delta_2$,
		%
		%
		%
		%
		%
		\begin{align}
		\directionalHessian \!\geq\! \frac{\lambda_{\min} \twonorm{\Omt}^2}{\ctwo}- \varepsilon_2 \stext{for}
		n \!\geq\! \dfrac{32\cone[4] \xmax^4  \log\frac{2p}{\delta_2}}{\varepsilon_2^2 \ctwo[2]}
		\stext{and uniformly for all $t \in [p]$.}
		\end{align}
	\end{enumerate}
	%
\end{lemma}



%
%
%
%
%
%
%
%
%

%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%


%
%
%
%
%
%
%
%
%
%
%

\noindent Given this lemma, we now proceed with the proof. 
%
%
Define a function $g : [0,1] \to \Reals^{n+p}$ 
%
%
%
%
\begin{align}
g(a) \defn \ExtendedTrueParameterRowT + a(\ExtendedParameterRowT - \ExtendedTrueParameterRowT).
\end{align}
Notice that $g(0) = \ExtendedTrueParameterRowT$ and $g(1) = \ExtendedParameterRowT$ 
%
as well as
\begin{align}
\dfrac{d\loss_t(g(a))}{da} = \tdirectionalGradient\bigr|_{\tExtendedParameterRowT = g(a)} \qtext{and} \dfrac{d^2\loss_t(g(a))}{da^2} = \tdirectionalHessian\bigr|_{\tExtendedParameterRowT = g(a)}. \label{eq_der_mapping}
\end{align}
By the fundamental theorem of calculus, we have
\begin{align}
\dfrac{d\loss_t(g(a))}{da} \geq \dfrac{d\loss_t(g(a))}{da}\bigr|_{a = 0} + a \min_{a \in (0,1)}\dfrac{d^2\loss_t(g(a))}{da^2}. \label{eq_fundamental}
\end{align}
Integrating both sides of \cref{eq_fundamental} with respect to $a$, we obtain
\begin{align}
\loss_t(g(a)) - \loss_t(g(0)) & \geq  a \dfrac{d\loss_t(g(a))}{da}\bigr|_{a = 0} +  \dfrac{a^2}{2} \min_{a \in (0,1)}\dfrac{d^2\loss_t(g(a))}{da^2}\\
& \sequal{\cref{eq_der_mapping}} a \tdirectionalGradient\bigr|_{\tExtendedParameterRowT = g(0)} +  \dfrac{a^2}{2} \min_{a \in (0,1)}\tdirectionalHessian\bigr|_{\tExtendedParameterRowT = g(a)}\\
& \sequal{(a)} a\directionalGradientTrue +  \dfrac{a^2}{2} \min_{a \in (0,1)}\tdirectionalHessian\bigr|_{\tExtendedParameterRowT = g(a)}\\
& \sgreat{(b)} - a \bigabs{\directionalGradientTrue} +  \dfrac{a^2}{2} \min_{a \in (0,1)}\tdirectionalHessian\bigr|_{\tExtendedParameterRowT = g(a)}, \label{eq_taylor_expansion}
\end{align}
where $(a)$ follows because $g(0) = \ExtendedTrueParameterRowT$ and $(b)$ follows by the triangle inequality. Plugging in $a = 1$ in \cref{eq_taylor_expansion} as well as using $g(0) = \ExtendedTrueParameterRowT$ and $g(1) = \ExtendedParameterRowT$, we find that
\begin{align}
\loss_t(\ExtendedParameterRowT) - \loss_t(\ExtendedTrueParameterRowT) \geq - \bigabs{\directionalGradientTrue} +  \dfrac{1}{2} \min_{a \in (0,1)}\tdirectionalHessian\bigr|_{\tExtendedParameterRowT = g(a)}.
\end{align}
Now, we use \cref{lemma_conc_first_sec_der} with 
%
%
%
%
%
%
%
%
%
\begin{align}
\varepsilon_1 \mapsfrom 
%
\frac{\varepsilon}{2},
\quad \delta_1 \mapsfrom \frac{\delta}{2}, 
\quad 
%
\varepsilon_2 \mapsfrom 
\varepsilon,
%
%
\qtext{and} \delta_2 \mapsfrom \frac{\delta}{2}.
\end{align}
%
Thus for $n \geq  \dfrac{ce^{c'\bGM} \log(p/\delta)}{\varepsilon^2}$, we have
%
%
%
%
%
%
%
%
%
\begin{align}
\loss_t(\ExtendedParameterRowT) 
\!-\! \loss_t(\ExtendedTrueParameterRowT) & \geq - \frac{\varepsilon}{2} \!+\!  \dfrac{1}{2} \biggparenth{\frac{\lambda_{\min} \twonorm{\Omt}^2}{\ctwo}-\varepsilon} \!=\! \frac{\lambda_{\min} \twonorm{\Omt}^2}{2\ctwo} - \varepsilon, \label{eq:taylor_expansion_with_grad_hess_conc}
\end{align}
uniformly for all $t \in [p]$, with probability at least $1-\delta$.
%
%
%
%
%
%
%
%
%
%
%
%
%
%




%
%

%
%
%


%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%

\subsubsection{Proof of \cref{lemma_conc_first_sec_der}\cref{item_conc_first_der}: \concgradresultname}
\label{sub:proof_of_lemma_conc_first_der}
\newcommand{\expressionfirstder}{Expression for first directional derivative}
%
For every $t \in [p]$ with $\uOmT$ defined in \cref{eq:omega_defn}, we claim that the first-order directional derivative of the loss function defined in \cref{eq:loss_function_p} is given by
\begin{align}
\directionalGradient  = -\frac{1}{n}\sumn  \Bigparenth{\DeltatIp  \tsvbx^{(i)}}  \exp\Bigparenth{-\normalbrackets{\ExternalFieldtI + 2\ParameterRowttt\tp \svbx^{(i)}_{-t}} x_t^{(i)} - \ParameterTU[tt] \cx^{(i)}_t}, \label{eq:first_dir_derivative}
\end{align}
where $\DeltatI \defeq \begin{bmatrix} \omtI \\ \Omttt\tp \\ \Omtu[t] \end{bmatrix} \in \real^{p+1}$ and $\tsvbx^{(i)} \defeq \begin{bmatrix} x_t^{(i)} \\ 2\svbx_{-t}^{(i)}x_t^{(i)}  \\ \cx_t^{(i)} \end{bmatrix} \in \real^{p+1} $ for all $ i \in [n]$ with $\cx^{(i)}_t = \bigbrackets{x^{(i)}_t}^2 - \xmax^2/3$.
%
We provide a proof at the end.\\


%
%
%
%
%
%
%
%

\noindent Next, we claim that the mean of the first-order directional derivative evaluated at the true parameter is zero. We provide a proof at the end.

\newcommand{\zeromeangradient}{Zero-meanness of first directional derivative}
\begin{lemma}[{\zeromeangradient}]
	\label{prop_zero_mean_first_der}
	For every $t \in [p]$ with $\uOmT$ defined in \cref{eq:omega_defn}, we have
	%
	$\Expectation\bigbrackets{\directionalGradientTrue} = 0$.
\end{lemma}
\noindent Given these, we proceed to show the concentration of the first-order directional derivative evaluated at the true parameter. Fix any $t \in [p]$. From \cref{eq:first_dir_derivative}, we have
\begin{align}
\directionalGradientTrue & \sequal{\cref{eq:first_dir_derivative}}  -\frac{1}{n}\sumn  \Bigparenth{\DeltatIp  \tsvbx^{(i)}} \exp\Bigparenth{-\normalbrackets{\TrueExternalFieldtI + 2\TrueParameterRowtttTop \svbx^{(i)}_{-t}} x_t^{(i)} - \TrueParameterTU[tt] \cx^{(i)}_t }. 
\end{align}
Each term in the above summation is an independent random variable and is bounded as follows
\begin{align}
& \Bigabs{\Bigparenth{\DeltatIp  \tsvbx^{(i)}} \times \exp\Bigparenth{-\normalbrackets{\TrueExternalFieldtI + 2\TrueParameterRowtttTop \svbx^{(i)}_{-t}} x_t^{(i)} - \TrueParameterTU[tt] \cx^{(i)}_t }} \\
& \sequal{(a)} \Bigabs{\Bigparenth{\omtI x_t^{(i)} + 2\Omttt\tp  \svbx_{-t}^{(i)} x_t^{(i)} + \Omtu[t] \cx_t^{(i)}} \times \exp\Bigparenth{-\normalbrackets{\TrueExternalFieldtI + 2\TrueParameterRowtttTop \svbx^{(i)}_{-t}} x_t^{(i)} - \TrueParameterTU[tt] \cx^{(i)}_t }} \\
%
& \sless{(b)} \bigabs{\normalabs{\omtI} + 2\sonenorm{\Omt} \sinfnorm{\svbx^{(i)}}} \times \xmax \times \exp\Bigparenth{\bigparenth{\normalabs{\TrueExternalFieldtI} + 2\sonenorm{\TrueParameterRowt} \sinfnorm{\svbx^{(i)}}} \xmax} \\
& \sless{(c)} \bigparenth{2\aGM + 8 \bGM \xmax} \times \xmax \times \exp\Bigparenth{\normalparenth{\aGM + 2 \bGM \xmax} \xmax} \sequal{\cref{eq:constants}} 2 \cone \ctwo \xmax, 
\end{align}
where $(a)$ follows by plugging in $\DeltatI$ and $\tsvbx^{(i)}$, 
%
$(b)$ follows from triangle inequality, Cauchy–Schwarz inequality, and because $\sinfnorm{\svbx^{(i)}} \leq \xmax $ for all $ i \in [n]$, and $(c)$ follows because $\TrueExternalFieldI \in \ParameterSet_{\ExternalField} $ for all $ i \in [n]$, $\TrueParameterMatrix \in \ParameterSet_{\ParameterMatrix}$, $\omI \in 2\ParameterSet_{\ExternalField} $ for all $ i \in [n]$, $\Om \in 2\ParameterSet_{\ParameterMatrix}$, and $\sinfnorm{\svbx^{(i)}} \leq \xmax $ for all $ i \in [n]$.\\

\noindent Further, from \cref{prop_zero_mean_first_der}, we have $\Expectation\bigbrackets{\directionalGradientTrue} = 0$. Therefore, using the Hoeffding's inequality results in
\begin{align}
\Probability \Bigparenth{\bigabs{\directionalGradientTrue} > \varepsilon_1} < 2 \exp\biggparenth{-\dfrac{n \varepsilon_1^2}{8\cone[2] \ctwo[2] \xmax^2}}. \label{eq_hoeffding_first_dir}
\end{align}
The proof follows by using the union bound over all $t \in [p]$.
%

%
%

%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%

\paragraph{Proof of \cref{eq:first_dir_derivative}: {\expressionfirstder}}
\label{sub_sub_sec_proof_of_expression_first_der}
Fix any $t \in [p]$. The first-order partial derivatives of $\loss_t$ with respect to entries of $\ExtendedParameterRowT$ defined in \cref{eq:loss_function_p} are given by
\begin{align}
\frac{\partial \loss_t(\ExtendedParameterRowT)}{\partial \ExternalFieldtI} & \!=\! \frac{-1}{n}x_t^{(i)}\exp\Bigparenth{\!-\!\normalbrackets{\ExternalFieldtI \!+\! 2\ParameterRowttt\tp \svbx_{-t}^{(i)}} x_t^{(i)}\!-\!\ParameterTU[tt]\cx_t^{(i)}} \stext{for all} i \in [n],
\qtext{and}\\
\frac{\partial \loss_t(\ExtendedParameterRowT)}{\partial \ParameterTU[tu]} & \!=\!
\begin{cases} \frac{-2}{n}\sumn x_t^{(i)} x_u^{(i)} \exp\Bigparenth{\!-\!\normalbrackets{\ExternalFieldtI \!+\! 2\ParameterRowttt\tp \svbx_{-t}^{(i)}} x_t^{(i)}\!-\!\ParameterTU[tt]\cx_t^{(i)}} \stext{for all} u \in [p]\!\setminus\!\braces{t}\!.\\
\frac{-1}{n}\sumn \cx_t^{(i)} \exp\Bigparenth{\!-\!\normalbrackets{\ExternalFieldtI \!+\! 2\ParameterRowttt\tp \svbx_{-t}^{(i)}} x_t^{(i)}\!-\!\ParameterTU[tt]\cx_t^{(i)}} \stext{for} u = t.
\end{cases}
\label{eq:theta_first_derivatives} 
\end{align}
Now, we can write the first-order directional derivative of $\loss_t$ as
\begin{align}
\directionalGradient &\!\defeq\!\lim_{h\to 0}\frac{\loss_t(\ExtendedParameterRowT + h \uOmT)-\loss_t(\ExtendedParameterRowT)}{h} =  \sumn  \omtI \frac{\partial \loss_t(\ExtendedParameterRowT)}{\partial \ExternalFieldtI} + \sumu \Omtu \frac{\partial \loss_t(\ExtendedParameterRowT)}{\partial \ParameterTU[tu]}\\ 
& \!\!\!\!=\! \frac{\!-1}{n}\!\!\sumn \!\! \Bigparenth{\!\omtI x_t^{(i)} \!\!+\! 2\sum_{u \in [p] \setminus \{t\}} \Omtu x_t^{(i)} \!x_u^{(i)} \!\!+\! \Omtu[t] \cx_t^{(i)}\!} \!\exp\Bigparenth{\!\!\!-\!\!\normalbrackets{\ExternalFieldtI \!\!+\! 2\ParameterRowttt\tp \svbx_{\!-t}^{(i)}} x_t^{(i)}\!\!\!-\!\ParameterTU[tt]\cx_t^{(i)}\!} \\
& \!\!\!\!=\! \frac{\!-1}{n}\!\!\sumn  \Bigparenth{\!\omtI x_t^{(i)} \!\!+\! 2\Omttt\tp  \svbx_{-t}^{(i)} x_t^{(i)} \!\!+\! \Omtu[t] \cx_t^{(i)}}  \!\exp\Bigparenth{\!\!\!-\!\!\normalbrackets{\ExternalFieldtI \!\!+\! 2\ParameterRowttt\tp \svbx_{\!-t}^{(i)}} x_t^{(i)}\!\!\!-\!\ParameterTU[tt]\cx_t^{(i)}\!} \\
& \!\!\!\!\sequal{(a)}\! \frac{-1}{n}\sumn  \Bigparenth{\DeltatIp  \tsvbx^{(i)}}  \exp\Bigparenth{\!-\!\normalbrackets{\ExternalFieldtI \!+\! 2\ParameterRowttt\tp \svbx_{-t}^{(i)}} x_t^{(i)}\!-\!\ParameterTU[tt]\cx_t^{(i)}},
\end{align}
where $(a)$ follows from the definitions of $\DeltatI$ and $\tsvbx^{(i)}$.
%

\paragraph{Proof of \cref{prop_zero_mean_first_der}: {\zeromeangradient}}
\label{sub:proof_of_prop_zero_mean_first_der}
%
Fix any $t \in [p]$. From \cref{eq:first_dir_derivative}, we have
\begin{align}
& \Expectation\bigbrackets{\directionalGradientTrue}\\
& \sequal{\cref{eq:first_dir_derivative}}  -\frac{1}{n} \sumn \Expectation_{\svbx^{(i)}, \svbz^{(i)}} \biggbrackets{  \Bigparenth{\DeltatIp  \tsvbx^{(i)}} \exp\Bigparenth{-\normalbrackets{\TrueExternalFieldtI + 2\TrueParameterRowtttTop \svbx^{(i)}_{-t}} x_t^{(i)} - \TrueParameterTU[tt] \cx^{(i)}_t }}\\ 
&  \sequal{(a)} -\frac{1}{n}\sumn \!  \sum_{u \in [p+1]} \!\! \Expectation_{\svbz^{(i)}} \biggbrackets{ \DeltaItu \Expectation_{\svbx^{(i)} | \svbz^{(i)}} \Bigbrackets{\tsvbx^{(i)}_u \exp\Bigparenth{\!-\!\normalbrackets{\TrueExternalFieldt(\svbz^{(i)}) \!+\! 2\TrueParameterRowtttTop \svbx^{(i)}_{-t}} x_t^{(i)} \!-\! \TrueParameterTU[tt] \cx^{(i)}_t }}},
%
\end{align}
where $(a)$ follows by linearity of expectation and by plugging in $\TrueExternalFieldtI = \TrueExternalFieldt(\svbz^{(i)})$.
%
Now to complete the proof, we show that for any $i \in [n], u \in [p+1]$ and $\svbz^{(i)} \in \cZ^{p_z}$, we have
\begin{align}
\Expectation_{\svbx^{(i)} | \svbz^{(i)}} \Bigbrackets{\tsvbx^{(i)}_u \exp\Bigparenth{-\normalbrackets{\TrueExternalFieldt(\svbz^{(i)}) + 2\TrueParameterRowtttTop \svbx^{(i)}_{-t}} x_t^{(i)} - \TrueParameterTU[tt] \cx^{(i)}_t }} = 0.
\end{align}
Fix any $i \in [n], u \in [p+1]$ and $\svbz^{(i)} \in \cZ^{p_z}$. We have
%
\begin{align}
& \Expectation_{\svbx^{(i)} | \svbz^{(i)}} \Bigbrackets{\tsvbx^{(i)}_u \exp\Bigparenth{-\normalbrackets{\TrueExternalFieldt(\svbz^{(i)}) + 2\TrueParameterRowtttTop \svbx^{(i)}_{-t}} x_t^{(i)} - \TrueParameterTU[tt] \cx^{(i)}_t }}\\
& =   \int\limits_{\cX^p}  \tx^{(i)}_u  \exp\Bigparenth{-\normalbrackets{\TrueExternalFieldt(\svbz^{(i)}) + 2\TrueParameterRowtttTop \svbx_{-t}^{(i)}} x_t^{(i)} - \TrueParameterTU[tt] \cx_t^{(i)}} f_{\rvbx| \rvbz}\bigparenth{\svbx^{(i)}| \svbz^{(i)}} d\svbx^{(i)}\\
& =  \int\limits_{\cX^p}  \tx^{(i)}_u  \exp\Bigparenth{\!\!-\!\normalbrackets{\TrueExternalFieldt(\svbz^{(i)}) + 2\TrueParameterRowtttTop \svbx_{-t}^{(i)}} x_t^{(i)} - \TrueParameterTU[tt] \cx_t^{(i)}} f_{\rvbx_{-t}| \rvbz}\bigparenth{\svbx_{-t}^{(i)}| \svbz^{(i)}}  \times \\
& \qquad\qquad\qquad\qquad\qquad\qquad \TrueConditionalDistIt   d\svbx^{(i)} \\
& \sequal{(a)}  \int\limits_{\cX^p } \dfrac{\tx^{(i)}_u  f_{\rvbx_{-t}|\rvbz}\bigparenth{\svbx_{-t}^{(i)}| \svbz^{(i)}} d\svbx^{(i)}} {\int_{\cX} \exp\Bigparenth{\normalbrackets{\TrueExternalFieldt(\svbz^{(i)}) + 2\TrueParameterRowtttTop \svbx_{-t}^{(i)}} x_t^{(i)} + \TrueParameterTU[tt] \cx_t^{(i)}}d x_t^{(i)}} \\
& = \int\limits_{\cX^{p-1}} \biggbrackets{\int_{\cX} \tx_u^{(i)} dx_t^{(i)}} \dfrac{ f_{\rvbx_{-t}| \rvbz}\bigparenth{\svbx_{-t}^{(i)}|\svbz^{(i)}} d\svbx_{-t}^{(i)}} {\int_{\cX} \exp\Bigparenth{\normalbrackets{\TrueExternalFieldt(\svbz^{(i)}) + 2\TrueParameterRowtttTop \svbx_{-t}^{(i)}} x_t^{(i)} + \TrueParameterTU[tt] \cx_t^{(i)}}d x_t^{(i)}} \\
\sequal{(b)} & ~~~ 0,
\end{align}
where $(a)$ follows by plugging in $ \TrueConditionalDistIt $ from \cref{eq_conditional_dist} and $(b)$ follows because $\int_{\cX} x_t^{(i)} dx_t^{(i)} = 0$ and $\int_{\cX} \cx_t^{(i)} dx_t^{(i)} = 0$.



\subsubsection{Proof of \cref{lemma_conc_first_sec_der}\cref{item_conc_sec_der}: \conchessresultname}
\label{sub:proof_of_lemma_conc_sec_der}
\newcommand{\lowerboundhessian}{Lower bound on the second directional derivative}
%
%
%
%
%
\noindent We start by claiming that the second-order directional derivative can be lower bounded by a quadratic form. We provide a proof in \cref{sub:proof_of_lemma_lower_bound_sec_der}. 

\begin{lemma}[{\lowerboundhessian}]\label{lemma_lower_bound_sec_der}
	For every $t \in [p]$ with $\uOmT$ defined in \cref{eq:omega_defn}, we have
	%
	\begin{align}
	\directionalHessian \geq \frac{1}{n \ctwo } \sumn  \Bigparenth{\DeltatIp  \tsvbx^{(i)}}^2,
	\end{align}
	where $\DeltatI \defeq \begin{bmatrix} \omtI \\ \Omttt\tp \\ \Omtu[t] \end{bmatrix} \in \real^{p+1}$ and $\tsvbx^{(i)} \defeq \begin{bmatrix} x_t^{(i)} \\ 2\svbx_{-t}^{(i)}x_t^{(i)}  \\ \cx_t^{(i)} \end{bmatrix} \in \real^{p+1} $ for all $ i \in [n]$ with $\cx^{(i)}_t = \bigbrackets{x^{(i)}_t}^2 - \xmax^2/3$ and the constant $\ctwo$ was defined in \cref{eq:constants}.
\end{lemma}
%

%

%
%
%
%
%
%
%

%
%
%
%
%
%
%
%
%
%
%
%
\noindent Given this, we proceed to show the anti-concentration of the second-order directional derivative. Fix any $t \in [p]$ and any $\ExtendedParameterMatrix \in \ParameterSet_{\ExternalField}^n \times \ParameterSet_{\ParameterMatrix}$. From \cref{lemma_lower_bound_sec_der}, we have
\begin{align}
\directionalHessian & \geq  \frac{1}{n \ctwo}\sumn  \Bigparenth{\DeltatIp  \tsvbx^{(i)}}^2. \label{eq_sec_der_form_lower_bound}
\end{align}
First, using the Hoeffding’s inequality, let us show concentration of $\frac{1}{n}\sumn  \Bigparenth{\DeltatIp  \tsvbx^{(i)}}^2$ around its mean. We observe that each term in the summation is an independent random variable and is bounded
as follows
\begin{align}
\Bigparenth{\DeltatIp  \tsvbx^{(i)}}\!^2 & \sequal{(a)} \bigparenth{\omtI x_t^{(i)} + 2\Omttt\tp  \svbx_{-t}^{(i)} x_t^{(i)} + \Omtu[t] \cx_t^{(i)}}^2 \\
& 
%
%
\sless{(b)} \bigparenth{\normalabs{\omtI} \!+\! 2\sonenorm{\Omt} \sinfnorm{\svbx^{(i)}}}^2  \xmax^2
\sless{(c)} \bigparenth{2\aGM \!+\! 8 \bGM \xmax}^2 \xmax^2 \sequal{\cref{eq:constants}} 4 \cone[2] \xmax^2,
\end{align}
where $(a)$ follows by plugging in $\DeltatI$ and $\tsvbx^{(i)}$, 
%
$(b)$ follows from triangle inequality, Cauchy–Schwarz inequality and because $\sinfnorm{\svbx^{(i)}} \leq \xmax $ for all $ i \in [n]$, and $(c)$ follows because $\Om \in 2\ParameterSet_{\ParameterMatrix}$, $\omI \in 2\ParameterSet_{\ExternalField}$, and $\sinfnorm{\svbx^{(i)}} \leq \xmax $ for all $ i \in [n]$. Then, from the Hoeffding's inequality, for any $\varepsilon > 0$ we have
\begin{align}
\Probability \Bigparenth{\biggabs{\frac{1}{n}\sumn  \Bigparenth{\DeltatIp  \tsvbx^{(i)}}^2 - \frac{1}{n}\sumn  \Expectation\biggbrackets{\Bigparenth{\DeltatIp  \tsvbx^{(i)}}^2} } > \varepsilon} < 2 \exp\biggparenth{-\dfrac{n \varepsilon^2}{32\cone[4] \xmax^4}}. \label{eq_hoeffding_first_dir}
\end{align}
Applying the union bound over all $t \in [p]$, for any $\delta \in (0,1)$ and uniformly for all $t \in [p]$, we have
\begin{align}
\frac{1}{n}\sumn  \Bigparenth{\DeltatIp  \tsvbx^{(i)}}^2 \geq \frac{1}{n}\sumn  \Expectation\biggbrackets{\Bigparenth{\DeltatIp  \tsvbx^{(i)}}^2} - \varepsilon, \label{eq_empirical_quad_form_lower_bound}
\end{align}
with probability at least $1-\delta$ as long as
\begin{align}
n \geq \dfrac{32\cone[4] \xmax^4}{\varepsilon^2}\log\biggparenth{\dfrac{2p}{\delta}}.
\end{align}
Now, we lower bound $\Expectation\Bigbrackets{ \Bigparenth{\DeltatIp  \tsvbx^{(i)}}^2 }$ for every $t \in [p]$ and every $i \in [n]$. Fix any $t \in [p]$ and $i \in [n]$. We have
\begin{align}
\Expectation_{\svbx^{(i)}, \svbz^{(i)}}\biggbrackets{ \Bigparenth{\DeltatIp  \tsvbx^{(i)}}^2 } & = \Expectation_{\svbz^{(i)}}\biggbrackets{ \DeltatIp \Expectation_{\svbx^{(i)} | \svbz^{(i)}} \Bigbrackets{  \tsvbx^{(i)} \tsvbx^{(i)\tp} | \svbz^{(i)} }\DeltatI}  \\
& \sgreat{(a)} \lambda_{\min} \Expectation_{\svbz^{(i)}}\Bigbrackets{ \stwonorm{\DeltatI}^2} \sgreat{(b)} \lambda_{\min}  \stwonorm{\Omt}^2, \label{eq_lower_bound_squared_expectation_lambda_min}
\end{align}
where $(a)$ follows from \cref{ass_pos_eigenvalue} and $(b)$ follows from the definition of $\DeltatI$. Combining \cref{eq_sec_der_form_lower_bound,eq_empirical_quad_form_lower_bound,eq_lower_bound_squared_expectation_lambda_min}, for any $\delta \in (0,1)$ and uniformly for all $t \in [p]$, we have
\begin{align}
\directionalHessian \geq \frac{1}{\ctwo} \biggparenth{\lambda_{\min}  \stwonorm{\Omt}^2 - \varepsilon},
\end{align}
with probability at least $1-\delta$ as long as
\begin{align}
n \geq \dfrac{32\cone[4] \xmax^4}{\varepsilon^2}\log\biggparenth{\dfrac{2p}{\delta}}.
\end{align}
Choosing $\varepsilon = \varepsilon_2 \ctwo$ and $\delta = \delta_2$ yields the claim. 

\paragraph{Proof of \cref{lemma_lower_bound_sec_der}: \lowerboundhessian}
\label{sub:proof_of_lemma_lower_bound_sec_der}
%
For every $t \in [p]$ with $\uOmT$ defined in \cref{eq:omega_defn}, we claim that the second-order directional derivative of the loss function defined in \cref{eq:loss_function_p} is given by
\begin{align}
\directionalHessian = \frac{1}{n}\sumn  \Bigparenth{\DeltatIp  \tsvbx^{(i)}}^2  \exp\Bigparenth{-\normalbrackets{\ExternalFieldtI + 2\ParameterRowttt\tp \svbx_{-t}^{(i)}} x_t^{(i)} - \ParameterTU[tt]  \cx_t^{(i)}}, 
\label{eq:second_dir_derivative}
\end{align}
where $\DeltatI \defeq \begin{bmatrix} \omtI \\ \Omttt\tp \\ \Omtu[t] \end{bmatrix} \in \real^{p+1}$ and $\tsvbx^{(i)} \defeq \begin{bmatrix} x_t^{(i)} \\ 2\svbx_{-t}^{(i)}x_t^{(i)}  \\ \cx_t^{(i)} \end{bmatrix} \in \real^{p+1} $ for all $ i \in [n]$ with $\cx^{(i)}_t = \bigbrackets{x^{(i)}_t}^2 - \xmax^2/3$. We provide a proof at the end.\\

\newcommand{\expressionsecder}{Expression for second directional derivative}

%
%
%
%
%
%
%
%
\noindent Given this claim, we proceed to prove the lower bound on the second directional derivative. Fix any 
%
$t \in [p]$. From \cref{eq:second_dir_derivative}, we have
\begin{align}
\directionalHessian &  = \frac{1}{n}\sumn   \Bigparenth{\DeltatIp  \tsvbx^{(i)}}^2 \times \exp\Bigparenth{-\normalbrackets{\ExternalFieldtI + 2\ParameterRowttt\tp \svbx_{-t}^{(i)}} x_t^{(i)} - \ParameterTU[tt]  \cx_t^{(i)}}  \\
%
& \sgreat{(a)} \frac{1}{n}\sumn   \Bigparenth{\DeltatIp  \tsvbx^{(i)}}^2 \times \exp\Bigparenth{-\bigparenth{\normalabs{\ExternalFieldtI} + 2\sonenorm{\ParameterRowt} \sinfnorm{\svbx^{(i)}}} \xmax}\\
& \sgreat{(b)} \frac{1}{n}\sumn   \Bigparenth{\DeltatIp  \tsvbx^{(i)}}^2 \times  \exp\Bigparenth{-\normalparenth{\aGM + 2 \bGM \xmax} \xmax}\\
&  \sequal{\cref{eq:constants}} \frac{1}{\ctwo n}\sumn  \Bigparenth{\DeltatIp  \tsvbx^{(i)}}^2, \label{eq_lower_bound_sec_der}
\end{align}
where $(a)$ follows from triangle inequality, Cauchy–Schwarz inequality and because $\sinfnorm{\svbx^{(i)}} \leq \xmax $ for all $ i \in [n]$, and $(b)$ follows because $\ExternalFieldI \in \ParameterSet_{\ExternalField} $ for all $ i \in [n]$, $\ParameterMatrix \in \ParameterSet_{\ParameterMatrix}$, and $\sinfnorm{\svbx^{(i)}} \leq \xmax $ for all $ i \in [n]$.

\paragraph{Proof of \cref{eq:second_dir_derivative}: {\expressionsecder}}
%
Fix any $t \in [p]$. The second-order partial derivatives of $\loss_t$ with respect to entries of $\ExtendedParameterRowT$ defined in \cref{eq:loss_function} are given by
\begin{align}
\frac{\partial^2 \loss_t(\ExtendedParameterRowT)}{\partial \bigbrackets{\ExternalFieldtI}^2} & = \frac{1}{n}\bigbrackets{x_t^{(i)}}^2\exp\Bigparenth{-\normalbrackets{\ExternalFieldtI + 2\ParameterRowttt\tp \svbx_{-t}^{(i)}} x_t^{(i)} - \ParameterTU[tt]  \cx_t^{(i)}} \qtext{for all} i \in [n],\\
\frac{\partial^2 \loss_t(\ExtendedParameterRowT)}{\partial \ParameterTU[tu] \ParameterTU[tv]} & = 
\begin{cases} \frac{4}{n}\sumn\bigbrackets{x_t^{(i)}}^2 x_u^{(i)} x_v^{(i)} \exp\Bigparenth{\!\!-\normalbrackets{\ExternalFieldtI \!+\! 2\ParameterRowttt\tp \svbx_{-t}^{(i)}} x_t^{(i)} \!-\! \ParameterTU[tt]  \cx_t^{(i)}} \\
\qquad \qquad \qquad\qquad ~~~~~ \qquad \qquad \qquad \qquad \qquad \stext{for all} u,v \in [p]\!\setminus\!\braces{t}. \\
\frac{2}{n}\sumn \cx_t^{(i)} x_t^{(i)} x_u^{(i)} \exp\Bigparenth{\!\!-\normalbrackets{\ExternalFieldtI \!+\! 2\ParameterRowttt\tp \svbx_{-t}^{(i)}} x_t^{(i)} \!-\! \ParameterTU[tt]  \cx_t^{(i)}} \\
\qquad \qquad \qquad\qquad ~~~~~ \qquad \qquad \qquad \qquad \qquad \stext{for all} u \in [p]\!\setminus\!\braces{t} \stext{and} v \!=\! t.\\
\frac{2}{n}\sumn \cx_t^{(i)} x_t^{(i)} x_v^{(i)} \exp\Bigparenth{\!\!-\normalbrackets{\ExternalFieldtI \!+\! 2\ParameterRowttt\tp \svbx_{-t}^{(i)}} x_t^{(i)} \!-\! \ParameterTU[tt]  \cx_t^{(i)}} \\
\qquad \qquad \qquad\qquad ~~~~~ \qquad \qquad \qquad\qquad \qquad \stext{for all} v \in [p]\!\setminus\!\braces{t} \stext{and} u \!=\! t.\\
\frac{1}{n}\sumn \bigbrackets{\cx_t^{(i)}}^2 \exp\Bigparenth{\!\!-\normalbrackets{\ExternalFieldtI \!+\! 2\ParameterRowttt\tp \svbx_{-t}^{(i)}} x_t^{(i)} \!-\! \ParameterTU[tt]  \cx_t^{(i)}} \\
\qquad \qquad \qquad\qquad ~~~~~ \qquad \qquad \qquad \qquad \qquad \stext{for} v \!=\! t \stext{and} u \!=\! t.
\end{cases}\\
\frac{\partial^2 \loss_t(\ExtendedParameterRowT)}{\partial \ParameterTU[tu] \ExternalFieldtI} & \!=\! \frac{\partial^2 \loss_t(\ExtendedParameterRowT)}{\partial \ExternalFieldtI \ParameterTU[tu]} \!=\! \begin{cases} 
\frac{2}{n}\bigbrackets{x_t^{(i)}}^2 x_u^{(i)} \exp\!\Bigparenth{\!\!\!-\!\normalbrackets{\ExternalFieldtI \!\!+\! 2\ParameterRowttt\tp \svbx_{-t}^{(i)}} x_t^{(i)} \!\!-\! \ParameterTU[tt]  \cx_t^{(i)}} \\
\qquad \qquad \qquad\qquad \qquad \qquad ~~ \qquad \stext{for all} i \in [n], u \in [p]\! \setminus\!\braces{t}. \\
\frac{1}{n} x_t^{(i)} \cx_t^{(i)} \exp\!\Bigparenth{\!\!\!-\!\normalbrackets{\ExternalFieldtI \!\!+\! 2\ParameterRowttt\tp \svbx_{-t}^{(i)}} x_t^{(i)} \!\!-\! \ParameterTU[tt]  \cx_t^{(i)}} \\
\qquad \qquad \qquad\qquad \qquad \qquad ~~ \qquad \stext{for all} i \in [n], u  = t.
\end{cases}
%
%
\label{eq:theta_second_derivatives} 
\end{align}
Now, we can write the second-order directional derivative of $\loss_t$ as
\begin{align}
& \directionalHessian  \defeq \lim_{h\to 0}\frac{\partial_{\uOmT}\loss_t(\ExtendedParameterRowT+h \uOmT)\!-\!\partial_{\uOmT}\loss_t(\ExtendedParameterRowT)}{h} \\
& =  \sumn  \bigbrackets{\omtI}^2 \frac{\partial^2 \loss_t(\ExtendedParameterRowT)}{\partial \bigbrackets{\ExternalFieldtI}^2} + \sumu \sumu[v] \Omtu \Omtu[v] \frac{\partial^2 \loss_t(\ExtendedParameterRowT)}{\partial \ParameterTU[tu] \ParameterTU[tv]} + 2\sumn \sumu \omtI \Omtu \frac{\partial^2 \loss_t(\ExtendedParameterRowT)}{\partial \ParameterTU[tu] \ExternalFieldtI} \\ 
& = \frac{1}{n}\sumn \!\! \Bigparenth{ \bigbrackets{\omtI x_t^{(i)}}^2 \!\!+\! 4 \!\!\sumu \!\! \Omtu x_t^{(i)} \!x_u^{(i)} \!\!\sumu[v] \! \Omtu[v]  x_t^{(i)} \!x_v^{(i)} \!+\! 4 \Omtu[t]  \cx_t^{(i)} \!\!\sumu \!\!\Omtu x_t^{(i)} \!x_u^{(i)} \!+\! \bigbrackets{\Omtu[t]  \cx_t^{(i)}}^2 \\
	& \qquad \!\!\!+\! 4\omtI x_t^{(i)} \!\! \sumu \!\! \Omtu x_t^{(i)} \! x_u^{(i)} \!+\! 2\omtI x_t^{(i)}\bigbrackets{\Omtu[t]  \cx_t^{(i)}}\!} \!\times\! \exp\!\Bigparenth{\!\!\!-\!\normalbrackets{\ExternalFieldtI \!\!+\! 2\ParameterRowttt\tp \svbx_{-t}^{(i)}} x_t^{(i)} \!\!\!-\! \ParameterTU[tt]  \cx_t^{(i)}\!} \\
& = \frac{1}{n}\sumn \bigparenth{\omtI x_t^{(i)} + 2\Omttt\tp  \svbx_{-t}^{(i)} x_t^{(i)} + \Omtu[t] \cx_t^{(i)}}^2  \exp\Bigparenth{-\normalbrackets{\ExternalFieldtI + 2\ParameterRowttt\tp \svbx_{-t}^{(i)}} x_t^{(i)} - \ParameterTU[tt]  \cx_t^{(i)}} \\
& \sequal{(a)} \frac{1}{n}\sumn  \Bigparenth{\DeltatIp  \tsvbx^{(i)}}^2  \exp\Bigparenth{-\normalbrackets{\ExternalFieldtI + 2\ParameterRowttt\tp \svbx_{-t}^{(i)}} x_t^{(i)} - \ParameterTU[tt]  \cx_t^{(i)}},
\end{align}
where $(a)$ follows from the definitions of $\DeltatI$ and $\tsvbx^{(i)}$.

%
%

%
%
%
%
%
%
%
%

%

%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%


%
%
%
%
%
%
%
%
%
%
%

%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%




%
%

%
%
%

%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%

%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%

%
%
%
%
%
%
%
%
%
%
%
%
%
%

%
%
%
%
%

%
%
%
%
%

%
%
%
%
%
%
%
%
%
%
%
%
%

%
%
%
%
%
%

%
%
%
%
%
%
%
%
%
%
%
%

%
%
%
%
%
%
%
%
%
%


%
%
%

%

%
%
%
%
%
%
%

%

%
%
%
%
%
%
%
%

%

%

%
%
%
%
%
%
%

%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%

%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%

%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%

%
%
%
%
%
%
%
%
%

%

%
%
%
%
%
%
%
%
%
%
%
%
%

%
%
%
%
%
%
%
%
%
%
%

%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%

%
%
%
%
%
%
%
%
%
%
%
%

\subsection{Example for \cref{ass_pos_eigenvalue}}
\label{subsec_discussion_ass_pos_eigenvalue}
As seen in \cref{eq_lower_bound_squared_expectation_lambda_min}, \cref{ass_pos_eigenvalue} is used to lower bound $\Expectation_{\svbx^{(i)}, \svbz^{(i)}}\Bigbrackets{ \bigparenth{\DeltatIp  \tsvbx^{(i)}}^2}$ by $\stwonorm{\Omt}^2$. In this section, we show that $\Expectation_{\svbx^{(i)}, \svbz^{(i)}}\Bigbrackets{ \bigparenth{\DeltatIp  \tsvbx^{(i)}}^2}$ can be lower bounded by $\stwonorm{\Omt}^2$ without requiring \cref{ass_pos_eigenvalue} if $\TrueParameterTU[tt] = 0$ for all $t \in [p]$ and {the row-wise $\ell_1$ sparsity of $\ParameterMatrix$ in \cref{assumptions} is assumed to be induced by row-wise $\ell_0$ sparsity, i.e., $\zeronorm{\ParameterRowt} \leq \bGM/\aGM$ for all $t \in [p]$}. To that end, first we claim that the conditional variance of $x_t^{(i)}$ conditioned on $\rvbx_{-t} = \svbx_{-t}^{(i)}$ and $\rvbz = \svbz^{(i)}$ is lower bounded by a constant for every $t \in [p]$ and $i \in [n]$. We provide a proof in \cref{sub:proof_of_prop_lower_bound_variance}.  
\newcommand{\lowerboundcondvar}{Lower bound on the conditional variance}
\begin{lemma}[{\lowerboundcondvar}]\label{prop_lower_bound_variance}
	We have
	\begin{align}
	\Variance\bigparenth{x_t^{(i)} \big| \svbx_{-t}^{(i)}, \svbz^{(i)}}  \geq \frac{2\xmax^2}{\pi e \ctwo[4]}
	\quad \stext{for all} t\in[p] \stext{and} i\in[n],
	\end{align}
	where the constant $\ctwo$ was defined in \cref{eq:constants}. 
	%
\end{lemma}
Given this lemma, we proceed. We have     
\begin{align}
\Expectation\biggbrackets{ \Bigparenth{\DeltatIp  \tsvbx^{(i)}}^2 } \sgreat{(a)} \Variance \biggbrackets{ \DeltatIp  \tsvbx^{(i)}} \sequal{(b)} \Variance \biggbrackets{\omtI x_t^{(i)} + 2\Omt\tp  \svbx^{(i)} x_t^{(i)}}, \label{eq_expected_quad_form_lower_bound}
\end{align}
where $(a)$ follows from the fact that for any random variable a, $\Expectation\normalbrackets{a^2} \geq \Variance\normalbrackets{a}$ and $(b)$ follows because we let $\Omtu[t] = 0$ since $\TrueParameterTU[tt] = 0$. We define the following set to lower bound $\Variance \bigbrackets{ \omtI x_t^{(i)} + 2\Omt\tp  \svbx^{(i)} x_t^{(i)}}$:
\begin{align}
\cE(\TrueParameterMatrix) \defn \braces{(t,u) \in [p]^2: t < u, \TrueParameterTU \neq 0},\label{eq:edge_set}
\end{align}
and consider the graph $\cG(\TrueParameterMatrix) = ([p], \cE(\TrueParameterMatrix))$ with $[p]$ as nodes and $\cE(\TrueParameterMatrix)$ as edges such that $\TrueJointDist$ is Markov with respect to $\cG(\TrueParameterMatrix)$. We claim that there exists a non-empty set $\cR_t \subset [p]\setminus\braces{t}$ such that 
\begin{enumerate}[label=(\roman*)]
	\item\label{item_independence_set} $\cR_t$ is an independent set of $\cG(\TrueParameterMatrix)$, i.e., there are no edges between any pair of nodes in $\cR_t$, and
	%
	%
	%
	\item\label{eq_independentSetProperty} the row vector $\Omt$ satisfies $\sum_{u \in \cR_t} \normalabs{\Omtu}^2 
	\geq 
	%
	\frac{1}{\bGM/\aGM+1}\twonorm{\Omt}^2$.
	%
	%
	%
\end{enumerate}
%
%
%
%
%
%
Taking this claim as given at the moment, we continue our proof. Denoting $\cR_t^c \defeq [p] \setminus \cR_t$, and using the law of total variance, the variance term in \cref{eq_expected_quad_form_lower_bound} can be lower bounded as
\begin{align}
\Variance \biggbrackets{\omtI x_t^{(i)} + 2\Omt\tp  \svbx^{(i)} x_t^{(i)}}  & \geq \Expectation\biggbrackets{\Variance \Bigbrackets{\omtI x_t^{(i)} + 2\Omt\tp  \svbx^{(i)} x_t^{(i)} \Big| \svbx_{\cR_t^c}^{(i)},  \svbz^{(i)}}}\\
& \sequal{(a)} 4\Expectation\Bigbrackets{\bigparenth{x_t^{(i)}}^2 \Variance\Bigparenth{\sum_{u \in \cR_t} \Omtu x_u^{(i)} \Big| \svbx_{\cR_t^c}^{(i)}, \svbz^{(i)}}}\\
& \sequal{(b)} 4\Expectation\Bigbrackets{\bigparenth{x_t^{(i)}}^2 \sum_{u \in \cR_t} \Omtu^2 \Variance\Bigparenth{ x_u^{(i)} \Big| \svbx_{\cR_t^c}^{(i)}, \svbz^{(i)}}}\\
& \sequal{(c)} 4\Expectation\Bigbrackets{\bigparenth{x_t^{(i)}}^2 \sum_{u \in \cR_t} \Omtu^2 \Variance\Bigparenth{ x_u^{(i)} \Big| \svbx_{-u}^{(i)}, \svbz^{(i)}}}\\
& \sgreat{(d)} \frac{8\xmax^2}{\pi e \ctwo[4]} \sum_{u \in \cR_t} \Omtu^2 \Expectation\Bigbrackets{\bigparenth{x_t^{(i)}}^2  }\\
%
& \sgreat{(e)} \frac{8\xmax^2}{\pi e \ctwo[4]} \sum_{u \in \cR_t} \Omtu^2 
\Variance\Bigparenth{ x_t^{(i)} \Big| \svbx_{-t}^{(i)}, \svbz^{(i)}}\\
& \sgreat{(f)} \frac{16\xmax^4}{\pi^2 e^2 \ctwo[8]} \sum_{u \in \cR_t} \Omtu^2 \sgreat{\cref{eq_independentSetProperty}} \frac{16\xmax^4 \twonorm{\Omt}^2}{\pi^2 e^2  \normalparenth{\bGM/\aGM + 1} \ctwo[8]}, \label{eq_var_intermediate_lower_bound}
\end{align}
where $(a)$ follows because $(x_u^{(i)})_{u \in \cR_t^c}$ are deterministic when conditioned on themselves, and $t \in \cR_t^c$, $(b)$ follows because $(x_u^{(i)})_{u \in \cR_t}$ are conditionally independent given $\svbx_{\cR_t^c}^{(i)}$ and $\svbz^{(i)}$ which is a direct consequence of \cref{item_independence_set}, $(c)$ follows because of the local Markov property (as the conditioning set includes all the neighbors in $\cG(\TrueParameterMatrix)$ of each node in  $\cR_t$),
%
$(d)$ and $(f)$ follow from \cref{prop_lower_bound_variance}, and $(e)$ follows because $\Expectation\Bigbrackets{\bigparenth{x_t^{(i)}}^2} = \Expectation\Bigbrackets{\Expectation\Bigbrackets{\bigparenth{x_t^{(i)}}^2 \Big| \svbx_{-t}^{(i)}, \svbz^{(i)}}} \geq \Variance\bigparenth{ x_t^{(i)} \Big| \svbx_{-t}^{(i)}, \svbz^{(i)}}$.\\

\noindent Combining \cref{eq_expected_quad_form_lower_bound} and \cref{eq_var_intermediate_lower_bound}, we have
\begin{align}
\Expectation_{\svbx^{(i)}, \svbz^{(i)}}\Bigbrackets{ \bigparenth{\DeltatIp  \tsvbx^{(i)}}^2} \geq \frac{16\xmax^4}{\pi^2 e^2  \normalparenth{\bGM/\aGM + 1} \ctwo[8]} \cdot \stwonorm{\Omt}^2.
\end{align}
\noindent It remains to construct the set $\cR_t$ that is an independent set of $\cG(\TrueParameterMatrix)$ and satisifies~\cref{eq_independentSetProperty}.

\paragraph{Construction of the set $\cR_t$}
%
For every $u \in [p]$, let $\cN(u)$ denote the set of neighbors of $u$ in $\cG(\TrueParameterMatrix)$, i.e., $\cN(u) \defeq \braces{v \in [p]: (u, v) \in \cE(\TrueParameterMatrix)} \bigcup \braces{v \in [p]: (v, u) \in \cE(\TrueParameterMatrix)}$. 
%
We start by selecting $r_1 \in [p] \setminus \braces{t}$ such that
\begin{align}
\normalabs{\Omtu[r_1]} \geq \normalabs{\Omtu} \qtext{for all} u \in [p] \setminus \braces{t , r_1}.
\end{align}
Next, we identify $r_2 \in [p] \setminus \braces{t, r_1, \cN(r_1)}$ such that
\begin{align}
\normalabs{\Omtu[r_2]} \geq \normalabs{\Omtu} \qtext{for all} u \in [p] \setminus \braces{t , r_1, \cN(r_1), r_2}.
\end{align}
We continue identifying $r_3, \ldots, r_s$ in such a manner till no more nodes are left, where $s$ denotes the total number of nodes selected. Now we define $\cR_t \defeq \{ r_1, \cdots , r_s\}$. For any $u \in [p]$, we have $\normalabs{\cN(u)} \leq \szeronorm{\TrueParameterRowt[u]} \leq \bGM/\aGM$ from \cref{eq:edge_set} and \cref{assumptions}. Using this, we see that $\cR_t$ is an independent set of $\cG(\TrueParameterMatrix)$ as claimed in \cref{item_independence_set} such
that it satisfies \cref{eq_independentSetProperty} by construction.

\subsubsection{Proof of \cref{prop_lower_bound_variance}: \lowerboundcondvar}
\label{sub:proof_of_prop_lower_bound_variance}
For any random variable $\rvx$, let $\Entropy(\rvx)$ denote the differential entropy of $\rvx$.  Fix any $t \in [p]$ and $i \in [n]$. Then, from Shannon's entropy inequality $(2\Entropy(\cdot) \leq \log \sqrt{2\pi e  \Variance(\cdot)})$, we have
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
\begin{align}
    2\pi e \Variance\bigparenth{x_t^{(i)} \big| \svbx_{-t}^{(i)}, \svbz^{(i)}} \sgreat{(a)} {\exp\Bigparenth{2\Entropy\bigparenth{x_t^{(i)} \big| \svbx_{-t}^{(i)}, \svbz^{(i)}}}}. \label{eq_shannon}
\end{align}
Therefore, to bound the variance, it suffices to bound the differential entropy. We have
\begin{align}
& - \Entropy\bigparenth{x_t^{(i)} \big| \svbx_{-t}^{(i)}, \svbz^{(i)}}\\
& =    \int\limits_{\cX^p \times \cZ^{p_z}}   f_{\rvbx, \rvbz}(\svbx^{(i)}\!\!, \svbz^{(i)}) \log \Bigparenth{ \TrueConditionalDistIt } d\svbx^{(i)} d\svbz^{(i)} \\
& =   \int\limits_{\cX^p \times \cZ^{p_z}}  f_{\rvbx, \rvbz}(\svbx^{(i)}\!\!, \svbz^{(i)}) \log \!\biggparenth{\!\!\frac{\exp\bigparenth{\normalbrackets{\TrueExternalFieldt(\svbz^{(i)}) \!+\! 2\TrueParameterRowtttTop \svbx_{-t}^{(i)}} x_t^{(i)} \!+\! \TrueParameterTU[tt] \cx_t^{(i)}}}{\int_{\cX} \!\exp\bigparenth{\normalbrackets{\TrueExternalFieldt(\svbz^{(i)}) \!+\! 2\TrueParameterRowtttTop \svbx_{-t}^{(i)}} x_t^{(i)} \!+\! \TrueParameterTU[tt] \cx_t^{(i)}}d x_t^{(i)}}\!} d\svbx^{(i)} d\svbz^{(i)} \!\\
& \sgreat{(a)}  \int\limits_{\cX^p \times \cZ^{p_z}}   f_{\rvbx, \rvbz}(\svbx^{(i)}\!\!, \svbz^{(i)}) \log \!\biggparenth{\!\!\frac{\exp\bigparenth{\bigparenth{\normalabs{\TrueExternalFieldt(\svbz^{(i)})} \!+\! 2\sonenorm{\TrueParameterRowt} \sinfnorm{\svbx^{(i)}}} \xmax\!}}{ \int_{\cX} \!\exp\!\bigparenth{\!\!-\!\!\bigparenth{\normalabs{\TrueExternalFieldt(\svbz^{(i)})} \!+\! 2\sonenorm{\TrueParameterRowt} \sinfnorm{\svbx^{(i)}}} \xmax\!} d x_t^{(i)}}\!\!} d\svbx^{(i)} d\svbz^{(i)} \!\\
& \sgreat{(b)}   \int\limits_{\cX^p \times \cZ^{p_z}}   f_{\rvbx, \rvbz}(\svbx^{(i)}\!\!, \svbz^{(i)}) \log \!\biggparenth{\!\!\frac{\exp\bigparenth{\normalparenth{\aGM + 2 \bGM \xmax} \xmax}}{ \int_{\cX} \exp\bigparenth{-\normalparenth{\aGM + 2 \bGM \xmax} \xmax} d x_t^{(i)}}} d\svbx^{(i)} d\svbz^{(i)} \!\\
& \sequal{(c)}   \int\limits_{\cX^p \times \cZ^{p_z}}   f_{\rvbx, \rvbz}(\svbx^{(i)}\!\!, \svbz^{(i)}) \log \biggparenth{\frac{\cthree[2]}{2\xmax}} d\svbx^{(i)} d\svbz^{(i)}\! = \log \biggparenth{\frac{\cthree[2]}{2\xmax}}, \label{eq_conditional_variance_lower_bound}
\end{align}
where $(a)$ follows from triangle inequality and Cauchy–Schwarz inequality and because $\sinfnorm{\svbx^{(i)}} \leq \xmax $ for all $ i \in [n]$, $(b)$ follows because $\TrueExternalField(\svbz^{(i)}) \in \ParameterSet_{\ExternalField} $ for all $ i \in [n]$, $\TrueParameterMatrix \in \ParameterSet_{\ParameterMatrix}$, $\sinfnorm{\svbx^{(i)}} \leq \xmax $ for all $ i \in [n]$, and $(c)$ follows because $\int_{\cX} dx_t^{(i)} = 2\xmax$. Combining \cref{eq_shannon,eq_conditional_variance_lower_bound} completes the proof.


%
%
%
%
%
%
%
%
%
%
%
%

%
%

%
%
%
%
%
%
%
%

%
%
%
%
%
%
%
%
%

%

%
%

%
%
%
%
%
%
%
%
%
%

%
%
%
%
%
%
%
%
%

%

%
%
%
%
%
%
%
%
%
%
%

%
%
%
%
%
%
%
%
%
%
%
%
%
%

%
\newcommand{\singleparameterseparation}{Gap between the loss function for a fixed parameter}


%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%

{\subsection{Proof of \cref{lemma_lipschitzness_first_stage}: \lipschitznesslossfunction}
	\label{sub:proof_lemma_lipschitzness_first_stage}
	%
	%
	%
	Consider any direction $\uOm$ $= \tExtendedParameterMatrix - \ExtendedParameterMatrix$. Now, define the function $q : [0,1] \to \Reals$ as follows
	\begin{align}
	q(a) = \loss\bigparenth{\ExtendedParameterMatrix + a(\tExtendedParameterMatrix - \ExtendedParameterMatrix)}. \label{eq_func_f_lipschitz_first_stage}
	\end{align}
	%
	Then, the desired inequality in \cref{eq_lipschitz_property_first_stage} is equivalent to $$\normalabs{q(1) - q(0)} \leq 2\xmax^2 \ctwo \Bigparenth{\sump \sonenorm{\Omt}  + \frac{1}{n} \sumn \sonenorm{\omI}}.$$
	%
	%
	%
	From the mean value theorem, there exists $a' \in (0,1)$ such that
	\begin{align}
	\normalabs{q(1) - q(0)} 
	%
	= \biggabs{\dfrac{dq(a')}{da}} \sequal{\cref{eq_func_f_lipschitz_first_stage}} \Bigabs{\dfrac{d\loss\bigparenth{\ExtendedParameterMatrix + a(\tExtendedParameterMatrix - \ExtendedParameterMatrix)}}{da}} 
	\sequal{\cref{eq_der_mapping}} \Bigabs{\directionalGradientFull\bigr|_{\ExtendedParameterMatrix = \ExtendedParameterMatrix + a(\tExtendedParameterMatrix - \ExtendedParameterMatrix)}}. \label{eq_mvt_lipschitz_first_stage}
	\end{align}
	Using \cref{eq:first_dir_derivative} in \cref{eq_mvt_lipschitz_first_stage}, we can write
	\begin{align}
	& \bigabs{q(1) \!-\! q(0)} \\
	%
	%
	%
	& =\frac{1}{n} \biggabs{\sump \sumn \! \Bigparenth{\DeltatIp  \tsvbx^{(i)}} \times \exp\Bigparenth{\!-\!\Bigbrackets{\bigparenth{\ExternalFieldtI + a'(\tExternalFieldtI \!-\! \ExternalFieldtI)} + \\
		& \qquad \qquad \qquad 2 \bigparenth{\ParameterRowttt + a'(\tParameterRowttt \!-\! \ParameterRowttt)}\tp \svbx^{(i)}_{-t}} x_t^{(i)} - \bigparenth{\ParameterTU[tt] + a'(\tParameterTU[tt] - \ParameterTU[tt] )} \cx_t^{(i)} }}\\
	%
	%
	%
	& \sless{(a)}  \exp\Bigparenth{\bigparenth{\normalbrackets{(1\!-\!a') \aGM \!+\! a' \aGM} + 2\normalbrackets{(1\!-\!a') \bGM \!+\! a'  \bGM}\xmax} \xmax} \frac{1}{n} \biggabs{\!\sump \!\sumn  \! \Bigparenth{\DeltatIp  \tsvbx^{(i)}}}\\
	& \sless{(b)}  \frac{2\xmax^2 \ctwo}{n} \sump \sumn \sonenorm{\DeltatI} 
	%
	\sequal{(c)} 2\xmax^2 \ctwo \Bigparenth{\sump \sonenorm{\Omt}  + \frac{1}{n} \sumn \sonenorm{\omI}} ,
	\end{align}
	where $(a)$ follows from triangle inequality, Cauchy–Schwarz inequality, $\ExternalFieldI, \tExternalFieldI \in \ParameterSet_{\ExternalField}$, $\ParameterMatrix, \tParameterMatrix \in \ParameterSet_{\ParameterMatrix}$, and $\sinfnorm{\svbx^{(i)}} \leq \xmax $ for all $ i \in [n]$, 
	$(b)$ follows from \cref{eq:constants}, the triangle inequality, and because $\sinfnorm{\svbx^{(i)}} \leq \xmax $ for all $ i \in [n]$, and $(c)$ follows from the definition of $\DeltatI$.
	%
	
}