\section{Possible extensions}
We now discuss how to extend our theoretical results with various relaxations of the exponential family modeling. 
%

\subsection{Modeling only the conditional distribution as exponential family}
\label{subsec_joint_vs_conditional}
Our framework and analysis can be extended to the setting where, instead of the joint distribution $f_{\ranvarvec}$ of $\ranvarvec = \normalparenth{\rvbz, \rvbv, \rvba, \rvby}$, we model only 
%
the conditional distribution $f_{\rvby|\rvba, \rvbz, \rvbv}$ of $\rvby$ conditioned on $\rvba$, $\rvbz$, and $\rvbv$ as an exponential family.
%
Note that when the joint distribution $f_{\ranvarvec}$ is an exponential family, the conditional distribution $f_{\rvby|\rvba, \rvbz, \rvbv}$ is also an exponential family, however a vice versa implication does not hold so that the setting considered here is a strict generalization of our previous setting. In fact, the conditional distribution $f_{\rvby|\rvba, \rvbz, \rvbv}$ being an exponential family puts no restrictions on the marginal distribution $f_{\rvbz, \rvbv, \rvba}$ of the unobserved covariates, the observed covariates, and the interventions as is the case with non-linear panel data models (\cref{sec_related_work}).
%

To estimate the expected potential outcomes $\mu^{(i)}(\wtil{\svba}^{(i)})$ in \cref{eq_causal_estimand} for any given unit $i$ 
%
and any alternate intervention $\wtil{\svba}^{(i)}$, 
%
it suffices to estimate the conditional distribution of $f_{\rvby|\rvba, \rvbz, \rvbv}(\cdot\vert \rvbv=\svba, \rvbv=\svbv^{(i)}\rvbz=\svbz^{(i)} )$ $\rvby$ for unit $i$ as a function of the intervention $\rvba$
%
(as in \cref{eq_counterfactual_distribution_y}). This task is equivalent to estimating $\Truephi$ in \cref{eq:gammai} under the exponential family models in \cref{eq_joint_distribution_zvay} or \cref{eq_conditional_distribution_y}. 
%
%
%
%


In \cref{subsec_exp_fam}, under the exponential family in \cref{eq_joint_distribution_zvay}, we argued (for analytical convenience) that learning $\Truephi$ is subsumed in learning the parameters corresponding to the conditional distribution $f_{\rvbx |\rvbz}$ of $\rvbx = \normalparenth{\rvbv, \rvba, \rvby}$ conditioned on $\rvbz$
(which also belongs to an exponential family with linear and quadratic interactions) as in \cref{eq_conditional_distribution_vay}. Then, we set the goal of estimating the parameters in \cref{eq_parameters_of_interest} and  designed a loss function to do so.  The loss function depended on the conditional distribution $f_{\rvx_t | \rvbx_{-t}, \rvbz}$ \cref{eq_conditional_dist_non_centered} of the random variable $\rvx_t$ conditioned on $\rvbx_{-t} = \svbx_{-t}$ and $\rvbz = \svbz$ for every $t \in [p]$.

Under the exponential family in \cref{eq_conditional_distribution_y}, we focus on directly learning the components of \cref{eq_parameters_of_interest} relevant to learning $\Truephi$, i.e., 
\begin{align}
    \TrueExternalFieldt(\svbz^{(i)}) & = \phi^{\star(y)} \!+\! 2 \Phi^{\star(z,y)\top} \svbz^{(i)} \in \Reals^{p_y \times 1}, \qtext{for all} t \in \normalbraces{p_v + p_a + 1, \cdots, p_v + p_a + p_y} \label{eq_new_parameters_1}\\
    \TrueParameterRowt & = \normalparenth{\Phi^{\star(v,y)}, \Phi^{\star(a,y)}, \Phi^{\star(y,y)}} \in \Reals^{p \times 1} \qtext{for all} t \in \normalbraces{p_v + p_a + 1, \cdots, p_v + p_a + p_y} \label{eq_new_parameters_2}.
\end{align}
We note that the conditional distribution $f_{\rvy_t | \rvby_{-t}, \rvbv, \rvba, \rvbz}$ of the random variable $\rvy_t$ conditioned on $\rvby_{-t} = \svby_{-t}$, $\rvbv = \svbv$, $\rvba = \svba$, and $\rvbz = \svbz$ for every $t \in [p_y]$ is consistent with the conditional distribution $f_{\rvx_{t'} | \rvbx_{-t'}, \rvbz}$ in \cref{eq_conditional_dist_non_centered} for every $t' \in \normalbraces{p_v + p_a + 1, \cdots, p_v + p_a + p_y}$. As a result, we can adapt the loss function in \cref{eq:loss_function} to learn the parameters in \cref{eq_new_parameters_1,eq_new_parameters_2} by summing over $t \in \normalbraces{p_v + p_a + 1, \cdots, p_v + p_a + p_y} $ instead of $t \in [p]$. Consequently, the guarantees in \cref{sec_main_results} continue to hold with $p$ replaced by $p_y$.
%
%



%
%

\subsection{Higher order terms in the conditional exponential family}
\label{subsec_high_terms}
In \cref{subsec_joint_vs_conditional}, we described how our framework and results apply when only the conditional distribution $f_{\rvby|\rvba, \rvbz, \rvbv}$ is modeled as the exponential family distribution in \cref{eq_conditional_distribution_y} where the term inside the exponent is linear in $(\rvbz, \rvbv, \rvba)$ and quadratic in $\rvby$.
%
%
We now describe how our framework and results are applicable when the conditional distribution $f_{\rvby|\rvba, \rvbz, \rvbv}$ is modeled as the following exponential family distribution
\begin{align}
    f_{\rvby | \rvba, \rvbz, \rvbv}(\svby | \svba, \svbz, \svbv) \! \propto  \exp\bigparenth{q_{\Phi}(\svbv, \svba, \svby)} \!\exp\bigparenth{2\svbz\tp\Phi^{(z,y)}\svby}, \label{eq_conditional_distribution_y_base_measure}
\end{align}
where $q_{\Phi}(\svbv, \svba, \svby)$ is some bounded degree polynomial
%
in $(\svbv, \svba, \svby)$ parameterized by $\Phi$, i.e., the term inside the exponent is linear in $\rvbz$ and arbitrary bounded degree polynomial in $(\rvbv, \rvba, \rvby)$. We note that every term in $q_{\Phi}(\svbv, \svba, \svby)$ needs to depend on $\rvby$ for it to contribute to $f_{\rvby | \rvba, \rvbz, \rvbv}$ in \cref{eq_conditional_distribution_y_base_measure}. For convenience, hereon, we ignore any dependence on $\rvbv$, and abuse notation to let $q_{\Phi}(\svba, \svby) = q_{\Phi}(\svbv, \svba, \svby)$. Then, in \cref{eq_conditional_distribution_y}, $q_{\Phi}(\svba, \svby)$ was a polynomial of degree 2 , i.e.,
\begin{align}
    q_{\Phi}(\svba, \svby) = q^{(2)}_{\Phi}(\svba, \svby) \defn \texttt{Sum}\Bigparenth{\phi^{(y)} \odot \svby + 2 \Phi^{(a,y)} \odot \bigparenth{\svba \otimes \svby} +  \Phi^{(y,y)} \odot \bigparenth{\svby \otimes \svby}},
\end{align}
where $\odot$ denotes the Hadamard product, $\otimes$ denotes the Kronecker product, $\Phi = (\phi^{(y)}, \Phi^{(a,y)}$, $\Phi^{(y,y)})$ with $\Phi^{(y,y)}$ being symmetric, and $\texttt{Sum}\normalparenth{s_1 + \cdots + s_h} \in \Reals$ sums, over all $i \in [h]$, all the entries of $s_i$ which could be a real number/vector/matrix/tensor. To explain how the loss function in \cref{eq:loss_function} needs to be modified for general $q_{\Phi}(\svba, \svby)$, we consider a polynomial of degree 3:
\begin{align}
    q_{\Phi}(\svba, \svby)  = q^{(2)}_{\Phi}(\svba, \svby) +   \texttt{Sum}\Bigparenth{ \hspace{-1.25cm}  \sum_{(u_1,u_2) \in \normalbraces{(a,a), (a,y), (y,y)}}  \hspace{-1.25cm} c_{u_1,u_2} \cdot \Phi^{(u_1,u_2,y)} \odot \bigparenth{\svbu_1 \otimes \svbu_2 \otimes \svby}}, 
\end{align}
where $c_{a,a} = c_{a,y} = 3$, $c_{y,y} = 1$ are constants chosen for consistency, and $\Phi^{(u_1,u_2,y)} \in \Reals^{p_{u_1} \times p_{u_2} \times p_y}$ is symmetric with respect to indices that are repeated for every $(u_1,u_2) \in \normalbraces{(a,a), (a,y), (y,y)}$. We illustrate the two steps from \cref{subsec_loss_function} below.
%
%
%
\paragraph{Centering sufficient statistics of the conditional distribution of a variable}
The conditional distribution $f_{\rvy_t | \rvby_{-t}, \rvba, \rvbz}$ of the random variable $\rvy_t$ conditioned on $\rvby_{-t} = \svby_{-t}$, $\rvba = \svba$, and $\rvbz = \svbz$ for every $t \in [p_y]$ is given by
%
%
%
%
%
\begin{align}
    f_{\rvy_t | \rvby_{-t}, \rvba, \rvbz}\bigparenth{y_t | \svby_{-t}, \svba, \svbz 
    %
    } \!\propto  & \exp\!\biggparenth{\!\texttt{Sum}\Bigparenth{\Bigbrackets{\phi_t(\svbz) + \hspace{-0.322cm} \sum_{u \in \normalbraces{y_{-t}, a}} \hspace{-0.3cm}2\Phi^{(u,y_t)} \odot \svbu + \hspace{-1.79cm} \sum_{(u_1,u_2) \in \normalbraces{(a,a), (a,y_{-t}), (y_{-t},y_{-t})}} \hspace{-1.79cm} c_{u_1,u_2} \Phi^{(u_1,u_2,y_t)} \odot \bigparenth{\svbu_1 \otimes \svbu_2}} y_t \\
    &  \qquad + \Bigbrackets{\Phi^{(y_t,y_t)}  +  \hspace{-0.25cm} \sum_{u \in \normalbraces{y_{-t}, a}} \hspace{-0.25cm} 3\Phi^{(u,y_t,y_t)} \odot \svbu} \Bigparenth{y_t^2 - \frac{\xmax^2}{3}} + \Phi^{(y_t,y_t,y_t)} y_t^3}},
\end{align}
where $\phi_t(\svbz) \defn \phi^{(y_t)}  + 2 \Phi^{(z,y_t)} \odot \svbz$, $c_{y_{-t},y_{-t}} = 3$, and $c_{a,y_{-t}} = 6$. Let $\Phi_t$ denote the concatenation of all the remaining parameters.
%
As in \cref{eq_conditional_dist}, the term $\xmax^2/3$ inside the exponent is vacuous and centers the sufficient statistics $\rvy_t^2$. The other sufficient statistics, i.e., $\rvx_t$ and $\rvx_t^3$, are naturally centered as their integrals with respect to the uniform distribution on $\cX$ are both zeros. 
\paragraph{Constructing the loss function}
Now, it is easy to see that the corresponding loss $\loss$ is given by
%
%
%
%
%
\begin{align}
    \loss& 
    %
    = \frac{1}{n} \sum_{t \in [p_y]} \sumn[i] \exp\!\biggparenth{-\texttt{Sum}\Bigparenth{\Bigbrackets{\phi_t^{(i)} + \hspace{-0.322cm} \sum_{u \in \normalbraces{y_{-t}, a}} \hspace{-0.3cm}2\Phi^{(u,y_t)} \odot \svbu^{(i)} + \hspace{-1.79cm} \sum_{(u_1,u_2) \in \normalbraces{(a,a), (a,y_{-t}), (y_{-t},y_{-t})}} \hspace{-1.79cm} c_{u_1,u_2} \Phi^{(u_1,u_2,y_t)}  \odot \bigparenth{\svbu_1^{(i)} \otimes \svbu_2^{(i)}}} y_t^{(i)} \\
    &  \qquad \qquad \qquad \qquad \qquad + \Bigbrackets{\Phi^{(y_t,y_t)}  +  \hspace{-0.25cm} \sum_{u \in \normalbraces{y_{-t}, a}} \hspace{-0.25cm} 3\Phi^{(u,y_t,y_t)} \odot \svbu^{(i)}} \Bigparenth{\bigbrackets{y_t^{(i)}}^2 - \frac{\xmax^2}{3}} + \Phi^{(y_t,y_t,y_t)} \bigbrackets{y_t^{(i)}}^3}},
\end{align}
and minimizing this convex loss results in the estimates of $\normalbraces{\phi_t^{(i)}}_{i \in [n]}$ and $\normalbraces{\Phi_{t}}_{t \in p_y}$. Consequently, the guarantees in \cref{sec_main_results} continue to hold with $p$ replaced by $p_y$ as long as \cref{assumptions,ass_pos_eigenvalue,ass_bounded_op_norm_cov_matrices} are appropriately generalized.

\paragraph{Tilting the base distribution} We note that the exponential family in \cref{eq_conditional_distribution_y} can be rewritten as
\begin{align}
    %
    f_{\rvby | \rvba, \rvbz, \rvbv}(\svby | \svba, \svbz, \svbv) \! \propto  \exp\bigparenth{2\svbz\tp\Phi^{(z,y)}\svby}
    \exp\bigparenth{2\svbv\tp\Phi^{(v,y)}\svby}
    \exp\bigparenth{2\svba\tp\Phi^{(a,y)}\svby}
 \exp\bigparenth{\phi^{(y)\tp} \svby + \svby\tp\Phi^{(y,y)}\svby}, \label{eq_conditional_tilted}
\end{align}
where $\exp\bigparenth{\phi^{(y)\tp} \svby + \svby\tp\Phi^{(y,y)}\svby}$ stands for a base distribution on $\rvby$ which is exponentially tilted by $\rvbz$, $\rvbv$, and $\rvba$, i.e., by $ \exp\bigparenth{2\svbz\tp\Phi^{(z,y)}\svby}$, $\exp\bigparenth{2\svbv\tp\Phi^{(v,y)}\svby}$, and $\exp\bigparenth{2\svba\tp\Phi^{(a,y)}\svby}$, respectively. Then, generalizing the exponential family in \cref{eq_conditional_distribution_y} to the one in \cref{eq_conditional_distribution_y_base_measure} is equivalent to saying that our approach and results continue to apply when $(a)$ the base distribution on $\rvby$ is an exponential family distribution where the term inside the exponent is arbitrary bounded degree polynomial (instead of quadratic) and $(b)$ the exponent of the exponential tilting of this base distribution by $\normalparenth{\rvbv, \rvba}$ is arbitrary bounded degree polynomial (instead of linear). 

%
%


\subsection{Discrete and mixed variables}
\label{subsec_discrete}
In \cref{subsec_exp_fam}, we described how our framework and results are applicable when the support of $\rvbv$, $\rvba$, and $\rvby$ are bounded continuous sets, i.e., $\cV = \cA = \cY = [-\xmax, \xmax]$. In \cref{subsec_joint_vs_conditional}, we showed that it suffices to only model the conditional distribution $f_{\rvby|\rvba, \rvbz, \rvbv}$ as an exponential family distribution implying that we do not need any restrictions on the support of $\rvbv$ and $\rvba$. Now, we describe how to adapt our loss function when $\rvby = (\rvy_{1}, \cdots, \rvy_{p_y}) \in \cY_1 \times \cdots \times \cY_{p_y}$ where $\cY_{t}$ is either a discrete compact set or a continuous compact set for $t \in [p_y]$.

We note that the conditional distribution $f_{\rvy_t | \rvby_{-t}, \rvbv, \rvba, \rvbz}$ of the random variable $\rvy_t$ conditioned on $\rvby_{-t} = \svby_{-t}$, $\rvbv = \svbv$, $\rvba = \svba$, and $\rvbz = \svbz$ for every $t \in [p_y]$ is still consistent with the conditional distribution $f_{\rvx_{t'} | \rvbx_{-t'}, \rvbz}$ in \cref{eq_conditional_dist_non_centered} for every $t' \in \normalbraces{p_v + p_a + 1, \cdots, p_v + p_a + p_y}$. However, the constants used to center the sufficient statistics in \cref{eq_conditional_dist} may change. More precisely, for any $t \in [p]$, the sufficient statistics $\rvx_t$ and $\rvx_t^2$ are centered by subtracting $\Expectation_{\cU_t}\bigbrackets{\rvx_t}$ and $\Expectation_{\cU_t}\bigbrackets{\rvx_t^2}$, respectively where $\cU_t$ denotes the uniform distribution supported over $\cY_t$. Consequently, the loss function in \cref{eq:loss_function} as well as \cref{ass_pos_eigenvalue} can be adapted, and the guarantees in \cref{sec_main_results} continue to hold.