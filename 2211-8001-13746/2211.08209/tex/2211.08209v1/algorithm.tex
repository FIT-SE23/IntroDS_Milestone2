\section{Algorithm}
\label{sec_algo}
%
In this section, we propose a computationally tractable loss function to 
estimate the unit-level and the population-level parameters in \cref{eq_parameters_of_interest}, and define our estimate of the expected potential outcomes in \cref{eq_causal_estimand}. 
%
%
%
%
%
Our algorithm jointly learns all the parameters of interest by pooling the observations across all $n$ units and exploiting the exponential family structure of $\rvbv$, $\rvba$, and $\rvby$ conditioned on $\rvbz = \svbz$ in \cref{eq_conditional_distribution_vay}.
%
%
%
In particular, 
%
our loss explicitly utilizes the fact that the population-level parameter $\TrueParameterMatrix$ is shared across units. 
%

%
%

%
%
%
%
%
%
%
%
%
%
%
%



\subsection{Loss function and parameter estimate}
\label{subsec_loss_function}
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
Consider any $t \in [p]$. Our loss function is inspired by the conditional distribution $f_{\rvx_t | \rvbx_{-t}, \rvbz}$ of the random variable $\rvx_t$ conditioned on $\rvbx_{-t} = \svbx_{-t}$ and $\rvbz = \svbz$ which is given by
\begin{align}
    f_{\rvx_t | \rvbx_{-t}, \rvbz}\bigparenth{x_t| \svbx_{-t}, \svbz; \ExternalFieldt(\svbz), \ParameterRowt} \propto \exp\biggparenth{ \normalbrackets{\ExternalFieldt(\svbz) + 2\ParameterRowt\tp \svbx} x_t},
    \label{eq_conditional_dist}
\end{align}
where $\ExternalFieldt(\svbz)$ is the $t^{th}$ element of $\ExternalField(\svbz)$, $\ParameterRowt$ is the $t^{th}$ row of $\ParameterMatrix$, and $\svbx$ denotes a realization of $\rvbx$. 
%
%
%
%
%
%
%
%
%
%
%
%
%
\begin{definition}[\tbf{Loss function}]\label{def-loss-function}
%
Given a sample $\svbx^{(i)}$ for every unit $i \in [n]$, our loss function maps $\ExtendedParameterMatrix \in \Reals^{p \times (n+p)}$ to $\loss\bigparenth{\ExtendedParameterMatrix} \in \Reals$ defined as
\vsep
\begin{align}
    \loss\bigparenth{\ExtendedParameterMatrix} \!=\! \frac{1}{n}\sump[t] \sumn[i] \! \exp\biggparenth{\!-\!\normalbrackets{\ExternalFieldtI \!+\! 2\ParameterRowt\tp \svbx^{(i)}} x_t^{(i)}}
    \!\!\stext{where}
    \ExtendedParameterMatrix \!\defeq\!\! \begin{bmatrix} \ExtendedParameterRowT[1]\tp \\ \vdots \\ \ExtendedParameterRowT[p]\tp \end{bmatrix}, 
    \stext{with}
    \ExtendedParameterRowT[t] \!\defn\! \bigbraces{\ExternalFieldtI[1], \cdots, \ExternalFieldtI[n], \ParameterRowt}.\!
    %
    \label{eq:loss_function}
    \end{align}
%
%
%
%
%
%
%
%
%
%
%
%
%
\end{definition}
\noindent The loss function defined above has many useful properties. We state one such property below with a proof in \cref{sec_proof_proper_loss_function}. We prove concentration of the gradient and anti-concentration of the Hessian for various decomposition of the loss function in Appendix (see \cref{sec_proof_sketch} for an overview.)
%
%
%
%
%
%
%
%
%
\newcommand{\properlossfunction}{Proper loss function}
\begin{proposition}[\tbf{\properlossfunction}]\label{prop_proper_loss_function} The loss function $\loss(\cdot)$ is strictly proper, i.e., $\ExtendedTrueParameterMatrix = \argmin_{\ExtendedParameterMatrix \in \ParameterSet_{\ExternalField}^n \times \ParameterSet_{\ParameterMatrix}} \Expectation\bigbrackets{\loss\bigparenth{\ExtendedParameterMatrix}}$.
%
%
%
%
%
%
\end{proposition}
\noindent Our estimate of $\ExtendedTrueParameterMatrix$ (defined analogous to $\ExtendedParameterMatrix$) is obtained by minimizing the convex function $\loss\bigparenth{\ExtendedParameterMatrix}$ 
%
over all $\ExternalFieldI[i] \in \ParameterSet_{\ExternalField} $ for all $ i \in [n]$ and $\ParameterMatrix \in \ParameterSet_{\ParameterMatrix}$, i.e., 
\begin{align}
    \ExtendedEstimatedParameterMatrix \in \argmin_{\ExtendedParameterMatrix \in \ParameterSet_{\ExternalField}^n \times \ParameterSet_{\ParameterMatrix}} \loss\bigparenth{\ExtendedParameterMatrix}.
    \label{eq_estimated_parameters}
\end{align}
%
%
We note \cref{eq_estimated_parameters} is a convex optimization problem, and a projected gradient descent algorithm (see below) returns an $\epsilon$-optimal estimate with $\tau = O(p/\epsilon)$ iterations\footnote{This follows from \cite[Theorem 10.6]{MeghanaN2016} by noting that $\loss(\ExtendedParameterMatrix)$ is $O(p)$ smooth function of $\ExtendedParameterMatrix$.} where $\ExtendedEstimatedParameterMatrix_{\epsilon}$ is said to be an $\epsilon$-optimal estimate if $\loss\bigparenth{\ExtendedEstimatedParameterMatrix_{\epsilon}} \leq \loss\bigparenth{\ExtendedEstimatedParameterMatrix} + \epsilon$ for any $\epsilon > 0$.

\begin{algorithm}[H]
    \SetCustomAlgoRuledWidth{0.4\textwidth} 
    %
    \KwInput{ $\tau,  \epsilon, \ParameterSet_{\ExternalField}, \ParameterSet_{\ParameterMatrix}$}
    \KwOutput{$\ExtendedEstimatedParameterMatrix_{\epsilon}$}
    \KwInitialization{$\ExtendedParameterMatrix^{(0)} = \boldsymbol{0}$} 
    %
    {
    \For{$j = 0,\cdots,\tau$}
    {
        $\ExtendedParameterMatrix^{(j+1)} \leftarrow \argmin_{\ExtendedParameterMatrix \in \ParameterSet_{\ExternalField}^n \times \ParameterSet_{\ParameterMatrix}} \stwonorm{\ExtendedParameterMatrix^{(j)} - \eta  \nabla \loss\bigparenth{\ExtendedParameterMatrix^{(j)}} - \ExtendedParameterMatrix}$
    }
    $\ExtendedEstimatedParameterMatrix_{\epsilon} \leftarrow \ExtendedParameterMatrix^{(\tau+1)}$
    }
    \caption{Projected Gradient Descent}
    \label{alg:GradientDescent}
\end{algorithm}
%

\subsection{Causal estimate}
Now, assuming the estimate $\ExtendedEstimatedParameterMatrix$ of $\ExtendedTrueParameterMatrix$ is given, we define our estimate of the expected potential outcome $\mu^{(i)}(\wtil{\svba}^{(i)})$ (see \cref{eq_causal_estimand}) for any given unit $i \in [n]$ under an alternate intervention $\wtil{\svba}^{(i)} \in \cA^{p_a}$. First, we identify $\EstimatedPhi^{(u, y)} \in \Reals^{p_u \times p_y}$ to be the component of $\EstimatedParameterMatrix$ corresponding to $\rvbu$ and $\rvby$ for all $\rvbu \in \{\rvbv, \rvba, \rvby\}$ and $\EstimatedExternalFieldI[i,y] \in \Reals^{p_y}$ to be the component of $\EstimatedExternalFieldI$ corresponding to $\rvby$. Then, our estimate of the conditional distribution of $\rvby$ as a function of the interventions $\rvba$, while keeping $\rvbv$ (observed) and $\rvbz$ (unobserved) fixed at the corresponding realizations for unit $i$, i.e., $\svbv^{(i)}$ and $\svbz^{(i)}$ respectively, is as follows:
\begin{align}
    \what{f}^{(i)}_{\rvby | \rvba}(\svby | \svba) \propto \exp\Bigparenth{\bigbrackets{\EstimatedExternalFieldI[i,y] + 2\svbv^{(i)\top}\EstimatedPhi^{(v,y)} + 2\svba\tp\EstimatedPhi^{(a,y)}} \svby + \svby\tp\EstimatedPhi^{(y,y)} \svby}. \label{eq_counterfactual_distribution_y}
\end{align}
Finally, our estimate of $\mu^{(i)}(\wtil{\svba}^{(i)})$ is given by
\begin{align}
    %
    \what{\mu}^{(i)}(\wtil{\svba}^{(i)}) & \defn \Expectation_{\what{f}^{(i)}_{\rvby | \rvba}}[\rvby | \rvba = \wtil{\svba}^{(i)}].  \label{eq_causal_estimate}
\end{align}



%
%
%




%
%

%

%
