%
\section{Proof of \cref{prop_proper_loss_function}: \properlossfunction}
\label{sec_proof_proper_loss_function}
Fix any $\svbz \in \cZ^{p_z}$. For every $t \in [p]$, define the following parametric distribution 
\begin{align}
    u_{\rvbx|\rvbz}\bigparenth{\svbx| \svbz; \ExternalFieldt(\svbz), \ParameterRowt} \propto \dfrac{\TrueJointDist}{\ConditionalDistT} \label{eq_u_distribution}
\end{align}
where $\TrueJointDist$ is as defined in \cref{eq_conditional_distribution_vay} and $\ConditionalDistT$ is as defined in \cref{eq_conditional_dist}. Using \cref{eq_conditional_dist}, we can write $u_{\rvbx|\rvbz}\bigparenth{\svbx| \svbz; \ExternalFieldt(\svbz), \ParameterRowt}$ in \cref{eq_u_distribution} as
\begin{align}
    u_{\rvbx|\rvbz}\bigparenth{\svbx| \svbz; \ExternalFieldt(\svbz), \ParameterRowt} \propto \TrueJointDist \exp\bigparenth{ -\normalbrackets{\ExternalFieldt(\svbz) + 2\ParameterRowt\tp \svbx}x_t  }. \label{eq_u_distribution_2}
\end{align}
Then, we have
\begin{align}
    u_{\rvbx|\rvbz}\bigparenth{\svbx| \svbz; \ExternalFieldt(\svbz), \ParameterRowt} & = \dfrac{\TrueJointDist \exp\bigparenth{ -\normalbrackets{\ExternalFieldt(\svbz) + 2\ParameterRowt\tp \svbx}x_t  }}{\int_{\svbx \in \cX^p} \TrueJointDist \exp\bigparenth{ -\normalbrackets{\ExternalFieldt(\svbz) + 2\ParameterRowt\tp \svbx}x_t  } d\svbx}\\
    & = \dfrac{\TrueJointDist \exp\bigparenth{ -\normalbrackets{\ExternalFieldt(\svbz) + 2\ParameterRowt\tp \svbx}x_t  }}{\Expectation_{\rvbx|\rvbz}\Bigbrackets{\exp\bigparenth{ -\normalbrackets{\ExternalFieldt(\svbz) + 2\ParameterRowt\tp \svbx}x_t  }}}. \label{eq_u_alternate}
\end{align}
Further, for $\ExternalFieldt(\svbz) = \TrueExternalFieldt(\svbz),$ and $\ParameterRowt = \TrueParameterRowt$, we can write an expression for  $u_{\rvbx|\rvbz}\bigparenth{\svbx| \svbz; \TrueExternalFieldt(\svbz), \TrueParameterRowt}$ which does not depend on $\rvx_t$ functionally. From \cref{eq_conditional_dist}, we have
\begin{align}
    u_{\rvbx|\rvbz}\bigparenth{\svbx| \svbz; \TrueExternalFieldt(\svbz), \TrueParameterRowt} \propto f_{\rvbx_{-t}|\rvbz}\bigparenth{\svbx_{-t}| \svbz; \TrueExternalField(\svbz), \TrueParameterMatrix}. \label{eq_u_star_no_dependence}
\end{align}
Now, consider the difference between $\KLD{u_{\rvbx|\rvbz}\bigparenth{\svbx| \svbz; \TrueExternalFieldt(\svbz), \TrueParameterRowt}}{u_{\rvbx|\rvbz}\bigparenth{\svbx| \svbz; \ExternalFieldt(\svbz), \ParameterRowt}}$ and $\KLD{u_{\rvbx|\rvbz}\bigparenth{\svbx| \svbz; \TrueExternalFieldt(\svbz), \TrueParameterRowt}}{\TrueJointDist}$. We have
\begin{align}
    & \KLD{u_{\rvbx|\rvbz}\bigparenth{\cdot| \svbz; \TrueExternalFieldt(\svbz), \TrueParameterRowt}}{u_{\rvbx|\rvbz}\bigparenth{\cdot| \svbz; \ExternalFieldt(\svbz), \ParameterRowt}} - \KLD{u_{\rvbx|\rvbz}\bigparenth{\cdot| \svbz; \TrueExternalFieldt(\svbz), \TrueParameterRowt}}{\TrueJointDistfun}\\
    & \sequal{(a)} \int_{\svbx \in \cX^p} u_{\rvbx|\rvbz}\bigparenth{\svbx| \svbz; \TrueExternalFieldt(\svbz), \TrueParameterRowt} \log \dfrac{\TrueJointDist}{u_{\rvbx|\rvbz}\bigparenth{\svbx| \svbz; \ExternalFieldt(\svbz), \ParameterRowt}} d\svbx \\
    & \sequal{\cref{eq_u_alternate}} \int_{\svbx \in \cX^p} u_{\rvbx|\rvbz}\bigparenth{\svbx| \svbz; \TrueExternalFieldt(\svbz), \TrueParameterRowt} \log \dfrac{\Expectation_{\rvbx|\rvbz}\Bigbrackets{\exp\bigparenth{ -\normalbrackets{\ExternalFieldt(\svbz) + 2\ParameterRowt\tp \svbx}x_t  }}}{\exp\bigparenth{ -\normalbrackets{\ExternalFieldt(\svbz) + 2\ParameterRowt\tp \svbx}x_t  }} d\svbx \\
    & = \log \Expectation_{\rvbx|\rvbz}\Bigbrackets{\exp\bigparenth{ \!-\!\normalbrackets{\ExternalFieldt(\svbz) \!+\! 2\ParameterRowt\tp \svbx}x_t}} \!-\! \int_{\svbx \in \cX^p} \!\!\!\!\!\!\!\!\!\! u_{\rvbx|\rvbz}\bigparenth{\svbx| \svbz; \TrueExternalFieldt(\svbz), \TrueParameterRowt} \bigparenth{ \normalbrackets{\ExternalFieldt(\svbz) \!+\! 2\ParameterRowt\tp \svbx}x_t }   d\svbx \\
    & \sequal{(b)} \log \Expectation_{\rvbx|\rvbz}\Bigbrackets{\exp\bigparenth{ \!-\!\normalbrackets{\ExternalFieldt(\svbz) \!+\! 2\ParameterRowt\tp \svbx}x_t}}, \label{eq_kl_difference}
\end{align}
where $(a)$ follows from the definition of KL-divergence and $(b)$ follows because integral is zero since (i) $u_{\rvbx|\rvbz}\bigparenth{\svbx| \svbz; \TrueExternalFieldt(\svbz), \TrueParameterRowt}$ does not functionally depend on $x_t$ as in \cref{eq_u_star_no_dependence}, (ii) $ \ExternalFieldt(\svbz) \!+\! 2\ParameterRowt\tp \svbx$ does not functionally depend on $x_t$ as $\ParameterTU[tt] = 0$, and (iii) $\int_{x_t \in \cX} x_t dx_t= 0$. Now, we can write
\begin{align}
    \Expectation\bigbrackets{\loss\bigparenth{\ExtendedParameterMatrix}} & = \frac{1}{n} \sump[t] \sumn[i]  \Expectation\Bigbrackets{\exp\bigparenth{ -\normalbrackets{\ExternalFieldt(\svbz^{(i)}) + 2\ParameterRowt\tp \svbx^{(i)}}x_t^{(i)}  }} \\
    & \sequal{\cref{eq_kl_difference}} \frac{1}{n} \sump[t] \sumn[i] \Expectation_{\rvbz}\Bigbrackets{\exp\Bigparenth{\KLD{u_{\rvbx|\rvbz}\bigparenth{\cdot| \svbz^{(i)}; \TrueExternalFieldt(\svbz^{(i)}), \TrueParameterRowt}}{u_{\rvbx|\rvbz}\bigparenth{\cdot| \svbz^{(i)}; \ExternalFieldt(\svbz^{(i)}), \ParameterRowt}} \\
    & \qquad \qquad \qquad \qquad \qquad - \KLD{u_{\rvbx|\rvbz}\bigparenth{\cdot| \svbz^{(i)}; \TrueExternalFieldt(\svbz^{(i)}), \TrueParameterRowt}}{\TrueJointDistfunT}}}.\label{eq_population_loss_kl_relationship}
\end{align}
We note that the parameters 
only up in the first KL-divergence term in the right-hand-side of \cref{eq_population_loss_kl_relationship}. Therefore, it is easy to see that $\Expectation\bigbrackets{\loss\bigparenth{\ExtendedParameterMatrix}}$ is minimized uniquely when $\ExternalFieldt(\svbz^{(i)}) = \TrueExternalFieldt(\svbz^{(i)})$ and $\ParameterRowt = \TrueParameterRowt$ for all $t \in [p]$ and all $i \in [n]$, i.e., when $\ExtendedParameterMatrix = \ExtendedTrueParameterMatrix$.
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%

\section{Proof of \cref{theorem_parameters} Part I: \edgeparammainresultname}
\label{sec:proof_of_theorem_parameters}
To analyze our estimate of the population-level parameter, we decompose $\loss(\ExtendedParameterMatrix)$ in \cref{eq:loss_function} as a sum of $p$ convex (and positive) auxiliary objectives $\loss_t\bigparenth{\ExtendedParameterRowT}$, i.e., $\loss(\ExtendedParameterMatrix) = \sump \loss_t\bigparenth{\ExtendedParameterRowT}$ where
\begin{align}
    \loss_t\bigparenth{\ExtendedParameterRowT} \defn \frac{1}{n} \sumn[i] \exp\Bigparenth{\!-\!\normalbrackets{\ExternalFieldtI \!+\! 2\ParameterRowt\tp \svbx} x_t^{(i)}},
    \label{eq:loss_function_p}
\end{align}
%
%
%
%
%
%
%
%
%
%
%
with $\ExtendedParameterRowT = \bigbraces{\ExternalFieldtI[1], \cdots, \ExternalFieldtI[n], \ParameterRowt}$ as defined in \cref{eq:loss_function}. Fix any $t \in [p]$. As discussed in \cref{sec_proof_sketch_pop}, 
%
it is sufficient to show  
that all $\ExtendedParameterMatrix \in \ParameterSet_{\ExternalField}^n \times \ParameterSet_{\ParameterMatrix}$ that satisfy $\stwonorm{\ParameterRowt - \TrueParameterRowt} \geq \varepsilon$ also uniformly satisfy 
%
\begin{align}
    \loss_t(\ExtendedParameterRowT) \geq \loss_t(\ExtendedTrueParameterRowT) + O(\varepsilon^2) \stext{for} n \geq \frac{c \exp({c\bGM})\log \frac{p}{\delta}}{\varepsilon^4}, \label{eq_sufficienct_condition_thm_edge}
\end{align}
with probability at least $1-\delta$, uniformly for all $t \in [p]$. The guarantee in \cref{theorem_parameters} follows (see \cref{sec_proof_sketch_pop}).\\
%
%
%
%
%
%
%
%
%
%
%
%
%

\noindent To that end, first, we define
\begin{align}
    \ParameterSet_{\ExtendedParameterRowT} \defn \Bigbraces{ \ExtendedParameterRowT = \bigbraces{\ExternalFieldtI[1], \cdots, \ExternalFieldtI[n], \ParameterRowt} \in \Reals^{(p+n) \times 1}: \sinfnorm{\ExtendedParameterRowT} \leq \aGM, \ParameterTU[tt] = 0, \zeronorm{\ParameterRowt} \leq \bGM},
\end{align}
to be the set of all feasible $\ExtendedParameterRowT$.
%
Then, we claim that for any fixed $\ExtendedParameterRowT \in \ParameterSet_{\ExtendedParameterRowT}$, if $\ParameterRowt$ is far from $\TrueParameterRowt$, then with high probability $\loss_t\bigparenth{\ExtendedParameterRowT}$ will be significantly larger than $\loss_t\bigparenth{\ExtendedTrueParameterRowT}$.
\newcommand{\parameterseparation}{Gap between the loss function for a fixed parameter}
We provide a proof in \cref{proof_of_lemma_parameter}. First, we define the following constants that depend on model-parameters $\tau \defeq (\aGM, \bGM, \xmax)$:
\begin{align}
\label{eq:constants}
    \cone \!\defeq\! \aGM(1 \!+\! 4 \bGM \xmax)
    \qtext{and}
    \ctwo \defeq \exp{(\aGM\xmax(1+ 2\bGM \xmax))}.
\end{align}
\begin{lemma}[\tbf{\parameterseparation}]\label{lemma_parameter}
Consider any $\ExtendedParameterMatrix \in \ParameterSet_{\ExternalField}^n \times \ParameterSet_{\ParameterMatrix}$. Fix any $\delta \in (0,1)$. Then, we have uniformly for all $t \in [p]$
\begin{align}
    \loss_t\bigparenth{\ExtendedParameterRowT} \geq \loss_t\bigparenth{\ExtendedTrueParameterRowT} + \dfrac{\xmax^2}{2\pi^2 e^2 \normalparenth{\bGM + 1}  \ctwo[5]} \stwonorm{\ParameterRowt - \TrueParameterRowt}^2 \qtext{for} n \geq
    \dfrac{c \exp({c\bGM}) \log(p/\delta)}{\stwonorm{\Omt}^4},
\end{align}
with probability at least 
$1-\delta$
where $\ctwo$ was defined in \cref{eq:constants}.
\end{lemma}
\noindent Next, we claim that the loss function $\loss_t$ is Lipschitz and capture this property via the following lemma. We provide a proof in \cref{sub:proof_lemma_lipschitzness_first_stage}.
\newcommand{\lipschitznesslossfunction}{Lipschitzness of the loss function}
\begin{lemma}[\tbf{\lipschitznesslossfunction}]\label{lemma_lipschitzness_first_stage}
Fix any $t \in [p]$. Consider any $\ExtendedParameterRowT, \tExtendedParameterRowT\in \ParameterSet_{\ExtendedParameterRowT}$ such that $\ExternalFieldtI = \tExternalFieldtI$ for all $i \in [n]$. Then, the loss function $\loss_t$ is Lipschitz with respect to the $\ell_1$ norm $\sonenorm{\cdot}$ and with Lipschitz constant $\xmax^2 \ctwo$, i.e.,
\begin{align}
    \bigabs{\loss_t\bigparenth{\tExtendedParameterRowT} - \loss_t\bigparenth{\ExtendedParameterRowT}} \leq \xmax^2 \ctwo \sonenorm{\tParameterRowt - \ParameterRowt}, \label{eq_lipschitz_property_first_stage}
\end{align}
where the constant $\ctwo$ was defined in \cref{eq:constants}.
\end{lemma}
\noindent Given these lemmas, we now proceed with the proof.

\paragraph{Proof strategy:} As mentioned earlier, the idea is to show that all points $\ExtendedParameterRowT \in \ParameterSet_{\ExtendedParameterRowT}$
%
that satisfy $\stwonorm{\ParameterRowt - \TrueParameterRowt} \geq \varepsilon$ also uniformly satisfy \cref{eq_sufficienct_condition_thm_edge} with probability at least $1-\delta$. To do so, we consider the set of feasible $\ParameterRowt$ whose distance
%
from $\TrueParameterRowt$ is at least $\varepsilon > 0$ in $\ell_2$ norm, and denote the set by $\ParameterSet_{\ParameterMatrix}^{\varepsilon,t}$. Then, using an appropriate covering set of $\ParameterSet_{\ParameterMatrix}^{\varepsilon,t}$ and the Lipschitzness of $\loss_t$, we show that the value of $\loss_t$ at all points in $\ParameterSet_{\ParameterMatrix}^{\varepsilon,t}$ is uniformly $O(\varepsilon^2)$ larger than the value of $\loss_t$ at $\TrueParameterRowt$ with high probability. 
%

\paragraph{Gap between the loss function for all parameters in the covering set:} Consider the set of elements $\ParameterSet_{\ParameterMatrix}^{\varepsilon,t} \defn \braces{ \ParameterRowt \in \Reals^{p \times 1}: \sinfnorm{\ParameterRowt} \leq \aGM, \ParameterTU[tt] = 0, \zeronorm{\ParameterRowt} \leq \bGM, \stwonorm{\TrueParameterRowt - \ParameterRowt} \geq \varepsilon}$. Let $\cC(\ParameterSet_{\ParameterMatrix}^{\varepsilon,t}, \varepsilon')$ be the $\varepsilon'$-covering number of the set $\ParameterSet_{\ParameterMatrix}^{\varepsilon,t}$ and let $\cU(\ParameterSet_{\ParameterMatrix}^{\varepsilon,t}, \varepsilon')$ be the associated $\varepsilon'$-cover  (see \cref{def_covering_number_metric_entropy}) where 
\begin{align}
    \varepsilon' \defn  \dfrac{\varepsilon^2}{8\pi^2 e^2 \bGM \normalparenth{\bGM + 1}  \ctwo[6]} \label{eq_varepsilon_stage_1}
\end{align}
Now, we apply \cref{lemma_parameter} to each element in $\cU(\ParameterSet_{\ParameterMatrix}^{\varepsilon,t}, \varepsilon')$ and argue by a union bound that the value of $\loss_t$ at all points in $\cU(\ParameterSet_{\ParameterMatrix}^{\varepsilon,t}, \varepsilon')$ is uniformly $O(\varepsilon^2)$ larger than the value of $\loss_t$ at $\TrueParameterRowt$ with high probability. 
We start by considering any $\ParameterRowt \in \cU(\ParameterSet_{\ParameterMatrix}^{\varepsilon,t}, \varepsilon')$. We have
\begin{align}
    \stwonorm{\TrueParameterRowt - \ParameterRowt} \sgreat{(a)} \varepsilon,\label{eq_lower_bound_two_norm_theta_diff_stage_1}
\end{align}
where $(a)$ follows because $\cU(\ParameterSet_{\ParameterMatrix}^{\varepsilon,t}, \varepsilon') \subseteq \ParameterSet_{\ParameterMatrix}^{\varepsilon,t}$. Now, applying \cref{lemma_parameter} with $\delta_0 = \delta/\cC(\ParameterSet_{\ParameterMatrix}^{\varepsilon,t}, \varepsilon')$, we have
\begin{align}
    \loss_t\bigparenth{\ExtendedParameterRowT} \geq \loss_t\bigparenth{\ExtendedTrueParameterRowT} + \dfrac{\xmax^2}{2\pi^2 e^2 \normalparenth{\bGM + 1}  \ctwo[5]} \stwonorm{\TrueParameterRowt - \ParameterRowt}^2 \sgreat{\cref{eq_lower_bound_two_norm_theta_diff_stage_1}} \loss_t\bigparenth{\ExtendedTrueParameterRowT} + \dfrac{\xmax^2 \varepsilon^2}{2\pi^2 e^2 \normalparenth{\bGM + 1}  \ctwo[5]},
\end{align}
with probability at least $1-\delta/\cC(\ParameterSet_{\ParameterMatrix}^{\varepsilon,t}, \varepsilon')$ whenever
\begin{align}
     n \geq \frac{c \exp({c\bGM}) \log\bigparenth{\cC(\ParameterSet_{\ParameterMatrix}^{\varepsilon,t}, \varepsilon') \cdot p/\delta}}{\stwonorm{\TrueParameterRowt - \ParameterRowt}^4}. \label{eq_n_condition_edge_recovery}
\end{align}
Using \cref{eq_lower_bound_two_norm_theta_diff_stage_1}, it suffices to choose $n$ such that
\begin{align}
     n \geq \frac{c \exp({c\bGM}) \log\bigparenth{\cC(\ParameterSet_{\ParameterMatrix}^{\varepsilon,t}, \varepsilon') \cdot p/\delta}}{\varepsilon^4}. \label{eq_n_condition_edge_recovery_2}
\end{align}
By applying the union bound over $\cU(\ParameterSet_{\ParameterMatrix}^{\varepsilon,t}, \varepsilon')$, as long as $n$ satisfies \cref{eq_n_condition_edge_recovery_2}, we have
\begin{align}
    \loss_t\bigparenth{\ExtendedParameterRowT} \geq  \loss_t\bigparenth{\ExtendedTrueParameterRowT} + \dfrac{\xmax^2 \varepsilon^2}{2\pi^2 e^2 \normalparenth{\bGM + 1}  \ctwo[5]} \stext{uniformly for every} \ParameterRowt \in \cU(\ParameterSet_{\ParameterMatrix}^{\varepsilon,t}, \varepsilon'), \label{eq_union_bound_covering_set_stage_1} 
\end{align}
with probability at least $1-\delta$.

\paragraph{Generalize beyond the covering set:} Next, we assume that \cref{eq_union_bound_covering_set_stage_1} holds, and generalize from $\cU(\ParameterSet_{\ParameterMatrix}^{\varepsilon,t}, \varepsilon')$ to all of $\ParameterSet_{\ParameterMatrix}^{\varepsilon,t}$. Consider any $\tParameterRowt \in \ParameterSet_{\ParameterMatrix}^{\varepsilon,t}$ and let $\ParameterRowt$ be any point in $\cU(\ParameterSet_{\ParameterMatrix}^{\varepsilon,t}, \varepsilon')$ that satisfies $\stwonorm{\ParameterRowt - \tParameterRowt} \leq \varepsilon'$ (see \cref{def_covering_number_metric_entropy}). Then, from \cref{lemma_lipschitzness_first_stage}, we have
\begin{align}
    \loss_t\bigparenth{\tExtendedParameterRowT} \geq \loss_t\bigparenth{\ExtendedParameterRowT} \!-\! \xmax^2 \ctwo \sonenorm{\ParameterRowt \!-\! \tParameterRowt} & \sgreat{(a)} \loss_t\bigparenth{\ExtendedParameterRowT} - 2\xmax^2 \ctwo \bGM \stwonorm{\ParameterRowt - \tParameterRowt} \\
    & \sgreat{(b)} \loss_t\bigparenth{\ExtendedParameterRowT} - 2\xmax^2 \ctwo \bGM \varepsilon' \\
    & \!\!\sgreat{\cref{eq_varepsilon_stage_1}} \loss_t\bigparenth{\ExtendedParameterRowT} \!-\!  \dfrac{\xmax^2 \varepsilon^2}{4\pi^2 e^2 \normalparenth{\bGM + 1}  \ctwo[5]}
     \sgreat{\cref{eq_union_bound_covering_set_stage_1}} \loss_t\bigparenth{\ExtendedTrueParameterRowT} \!+\! \dfrac{\xmax^2 \varepsilon^2}{4\pi^2 e^2 \normalparenth{\bGM + 1}  \ctwo[5]}, 
\end{align}
where $(a)$ follows by using $\sonenorm{\ParameterRowt - \tParameterRowt} \leq 2\bGM \stwonorm{\ParameterRowt - \tParameterRowt}$ which follows from the order of norms on Euclidean space as well as  $\ParameterMatrix \in \ParameterSet_{\ParameterMatrix}$ and $\tParameterMatrix \in \ParameterSet_{\ParameterMatrix}$ and $(b)$ follows because $\stwonorm{\ParameterRowt - \tParameterRowt} \leq \varepsilon'$. Recall that $\tParameterRowt$ is any point in $\ParameterSet_{\ParameterMatrix}^{\varepsilon,t}$, i.e., $\stwonorm{\TrueParameterRowt - \tParameterRowt} \geq r$. Therefore, we have an inequality that looks like \cref{eq_sufficienct_condition_thm_edge}. 

\paragraph{Bounding $n$:} To bound $n$ in \cref{eq_n_condition_node_recovery}, we bound the covering number $\cC(\ParameterSet_{\ParameterMatrix}^{\varepsilon,t}, \varepsilon')$ as follows
\begin{align}
    \cC(\ParameterSet_{\ParameterMatrix}^{\varepsilon,t}, \varepsilon') \sless{(a)} \cC(\ParameterSet_{\ParameterMatrix}, \varepsilon'/2)
    \label{eq_bound_covering_number_stage_1}
\end{align}
where $(a)$ follows from the fact that for any sets $\mathcal{U} \subseteq \mathcal{V}$ and any ${\varepsilon}$, it holds that $\cC(\mathcal{U}, \varepsilon) \leq \cC(\mathcal{V}, \varepsilon/2)$. Then using \cref{eq_bound_covering_number_stage_1} in \cref{eq_n_condition_node_recovery} and observing that $\varepsilon' = \frac{\varepsilon^2}{c\exp(c\bGM)}$, it is sufficient for
\begin{align}
     n \geq \frac{c \exp({c\bGM})}{\varepsilon^4} \cdot \biggparenth{\log \frac{p}{\delta} + \metric_{\ParameterMatrix}\Big(\frac{\varepsilon^2}{c\exp(c\bGM)}\Big)}.
\end{align}
The proof is complete by noting that $\metric_{\ParameterMatrix}\Big(\frac{\varepsilon^2}{c\exp(c\bGM)}\Big) = O(\bGM \log p)$ since $\ParameterRowt \in \Reals^{p \times 1}$ is such that $\szeronorm{\ParameterRowt} \leq \bGM$.

\subsection{Proof of \cref{lemma_parameter}: \parameterseparation}
\label{proof_of_lemma_parameter}
Fix any $\varepsilon >0$, any $\delta \in (0,1)$, and $t \in [p]$. Consider any direction $\uOmT \defn \bigbraces{\omtI[1], \cdots, \omtI[n], \Omt} \in \Reals^{n+p}$ along the parameter $\ExtendedParameterRowT$, i.e.,
\begin{align}
\label{eq:omega_defn}
    \uOmT = \ExtendedParameterRowT - \ExtendedTrueParameterRowT,
    \qtext{and}
    \Omt = \ParameterRowt-\TrueParameterRowt.
\end{align}
%
%
Without loss of generality, we let $\Omtu[t] = 0$ since $\TrueParameterTU[tt] = 0$. 
We denote the first-order and the second-order directional derivatives of the loss function $\loss_t$ in \cref{eq:loss_function_p} along the direction $\uOmT$ evaluated at $\ExtendedParameterRowT$ by $\directionalGradient$ and $\directionalHessian$ respectively. 
%
Below, we state a lemma (with proof divided across \cref{sub:proof_of_lemma_conc_first_der} and \cref{sub:proof_of_lemma_conc_sec_der}) that provides us a control on $\directionalGradient$ and $\directionalHessian$. 
The assumptions of \cref{lemma_parameter} remain in force.
%

\newcommand{\concpopresultname}{Control on first and second directional derivatives}
\newcommand{\concgradresultname}{Concentration of first directional derivative}
\newcommand{\conchessresultname}{Anti-concentration of second directional derivative}

%

%



\begin{lemma}[\tbf{\concpopresultname}]\label{lemma_conc_first_sec_der}
For any fixed $\varepsilon_1, \varepsilon_2 > 0$, $\delta_1, \delta_2 \in (0,1)$, $t \in [p]$, $\ExtendedParameterMatrix \in \ParameterSet_{\ExternalField}^n \times \ParameterSet_{\ParameterMatrix}$ defined in \cref{eq:loss_function} and $\Omt$ defined in \cref{eq:omega_defn}, we have the following:
\begin{enumerate}[label=(\alph*)]
    \item\label{item_conc_first_der} \textnormal{\tbf{\concgradresultname}}: with probability at least $1-\delta_1$,
    \begin{align}
    \bigabs{\directionalGradientTrue} \leq \varepsilon_1
    \qtext{for}
    n \geq \dfrac{8\cone[2] \ctwo[2] \xmax^2  \log(2p/\delta_1)}{\varepsilon_1^2}
    \stext{and uniformly for all $t \in [p]$.}
    \end{align}
    \item\label{item_conc_sec_der} \textnormal{\tbf{\conchessresultname}}: with probability at least $1-\delta_2$,
    \begin{align}
    \directionalHessian \!\geq\! \frac{4\xmax^2 \twonorm{\Omt}^2}{\pi^2 e^2 \normalparenth{\bGM + 1}  \ctwo[5]} - \varepsilon_2 \!\qtext{for}\!
    n \!\geq\! \dfrac{32\cone[4] \xmax^4  \log(2p / \delta_2)}{\varepsilon_2^2 \ctwo[2]}
    \stext{and uniformly for all $t \in [p]$.}
    \end{align}
\end{enumerate}
%
\end{lemma}



%
%
%
%
%
%
%
%
%

%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%


%
%
%
%
%
%
%
%
%
%
%

\noindent Given this lemma, we now proceed with the proof. 
%
%
Define a function $g : [0,1] \to \Reals^{n+p}$ 
%
%
%
%
\begin{align}
    g(a) \defn \ExtendedTrueParameterRowT + a(\ExtendedParameterRowT - \ExtendedTrueParameterRowT).
\end{align}
Notice that $g(0) = \ExtendedTrueParameterRowT$ and $g(1) = \ExtendedParameterRowT$ 
%
as well as
\begin{align}
\dfrac{d\loss_t(g(a))}{da} = \directionalGradient\bigr|_{\ExtendedParameterRowT = g(a)} \qtext{and} \dfrac{d^2\loss_t(g(a))}{da^2} = \directionalHessian\bigr|_{\ExtendedParameterRowT = g(a)}. \label{eq_der_mapping}
\end{align}
By the fundamental theorem of calculus, we have
\begin{align}
    \dfrac{d\loss_t(g(a))}{da} \geq \dfrac{d\loss_t(g(a))}{da}\bigr|_{a = 0} + a \min_{a \in (0,1)}\dfrac{d^2\loss_t(g(a))}{da^2}. \label{eq_fundamental}
\end{align}
Integrating both sides of \cref{eq_fundamental} with respect to $a$, we obtain
\begin{align}
  \loss_t(g(a)) - \loss_t(g(0)) & \geq  a \dfrac{d\loss_t(g(a))}{da}\bigr|_{a = 0} +  \dfrac{a^2}{2} \min_{a \in (0,1)}\dfrac{d^2\loss_t(g(a))}{da^2}\\
  & \sequal{\cref{eq_der_mapping}} a \directionalGradient\bigr|_{\ExtendedParameterRowT = g(0)} +  \dfrac{a^2}{2} \min_{a \in (0,1)}\directionalHessian\bigr|_{\ExtendedParameterRowT = g(a)}\\
  & \sequal{(a)} a\directionalGradientTrue +  \dfrac{a^2}{2} \min_{a \in (0,1)}\directionalHessian\bigr|_{\ExtendedParameterRowT = g(a)}\\
  & \sgreat{(b)} - a \bigabs{\directionalGradientTrue} +  \dfrac{a^2}{2} \min_{a \in (0,1)}\directionalHessian\bigr|_{\ExtendedParameterRowT = g(a)}, \label{eq_taylor_expansion}
\end{align}
where $(a)$ follows because $g(0) = \ExtendedTrueParameterRowT$ and $(b)$ follows by the triangle inequality. Plugging in $a = 1$ in \cref{eq_taylor_expansion} as well as using $g(0) = \ExtendedTrueParameterRowT$ and $g(1) = \ExtendedParameterRowT$, we find that
\begin{align}
    \loss_t(\ExtendedParameterRowT) - \loss_t(\ExtendedTrueParameterRowT) \geq - \bigabs{\directionalGradientTrue} +  \dfrac{1}{2} \min_{a \in (0,1)}\directionalHessian\bigr|_{\ExtendedParameterRowT = g(a)}.
\end{align}
Now, we use \cref{lemma_conc_first_sec_der} with 
\begin{align}
    \varepsilon_1 = \dfrac{\xmax^2 \stwonorm{\Omt}^2}{2\pi^2 e^2 \normalparenth{\bGM + 1}  \ctwo[5]}, \stext{} \delta_1 = \frac{\delta}{2}, \stext{and} \varepsilon_2 = \dfrac{2\xmax^2 \stwonorm{\Omt}^2}{\pi^2 e^2 \normalparenth{\bGM + 1}  \ctwo[5]}, \stext{} \delta_2 = \frac{\delta}{2}.
\end{align}
Therefore, with probability at least $1-\delta$ as long as $n \geq  O\biggparenth{\dfrac{\exp\bigparenth{O(\bGM)} \log(p/\delta)}{\stwonorm{\Omt}^4}}$, we have uniformly for all $t \in [p]$
%
%
%
%
%
\begin{align}
    \loss_t(\ExtendedParameterRowT) 
    \!-\! \loss_t(\ExtendedTrueParameterRowT) & \geq - \dfrac{\xmax^2 \stwonorm{\Omt}^2}{2\pi^2 e^2 \normalparenth{\bGM + 1}  \ctwo[5]} \!+\!  \dfrac{1}{2} \biggparenth{\frac{4\xmax^2 \stwonorm{\Omt}^2}{\pi^2 e^2 \normalparenth{\bGM + 1}  \ctwo[5]} -\dfrac{2 \xmax^2 \stwonorm{\Omt}^2}{\pi^2 e^2 \normalparenth{\bGM + 1}  \ctwo[5]}} \!=\! \dfrac{\xmax^2 \stwonorm{\Omt}^2}{2\pi^2 e^2 \normalparenth{\bGM + 1}  \ctwo[5]}. \label{eq:taylor_expansion_with_grad_hess_conc}
\end{align}
%
%
%
%
%
%
%
%
%
%




%
%

%
%
%


%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%

\subsubsection{Proof of \cref{lemma_conc_first_sec_der} \cref{item_conc_first_der}: \concgradresultname}
\label{sub:proof_of_lemma_conc_first_der}
\newcommand{\expressionfirstder}{Expression for first directional derivative}
%
For every $t \in [p]$ with $\uOmT$ defined in \cref{eq:omega_defn}, we claim that the first-order directional derivative of the loss function defined in \cref{eq:loss_function_p} is given by
\begin{align}
\directionalGradient  = -\frac{1}{n}\sumn  \Bigparenth{\DeltatIp  \tsvbx^{(i)} x_t^{(i)}}  \exp\Bigparenth{-\normalbrackets{\ExternalFieldtI + 2\ParameterRowt\tp \svbx^{(i)}} x_t^{(i)}}, \label{eq:first_dir_derivative}
\end{align}
where $\DeltatI \defeq \begin{bmatrix} \omtI \\ 2\Omt \end{bmatrix} \in \real^{p+1}$ and $\tsvbx^{(i)} \defeq \begin{bmatrix} 1 \\ \svbx^{(i)} \end{bmatrix} \in \real^{p+1} $ for all $ i \in [n]$.
%
We provide a proof in \cref{sub_sub_sec_proof_of_expression_first_der}.


%
%
%
%
%
%
%
%

\noindent Next, we claim that the mean of the first-order directional derivative evaluated at the true parameter is zero. We provide a proof in \cref{sub:proof_of_prop_zero_mean_first_der}

\newcommand{\zeromeangradient}{Zero-meanness of first directional derivative}
\begin{lemma}[\tbf{\zeromeangradient}]
\label{prop_zero_mean_first_der}
For every $t \in [p]$ with $\uOmT$ defined in \cref{eq:omega_defn}, we have
%
$\Expectation\bigbrackets{\directionalGradientTrue} = 0$.
\end{lemma}
\noindent Given these, we proceed to show the concentration of the first-order directional derivative evaluated at the true parameter. Fix any $t \in [p]$. From \cref{eq:first_dir_derivative}, we have
\begin{align}
    \directionalGradientTrue & \sequal{\cref{eq:first_dir_derivative}}  -\frac{1}{n}\sumn  \Bigparenth{\DeltatIp  \tsvbx^{(i)} x_t^{(i)}} \exp\Bigparenth{-\normalbrackets{\TrueExternalFieldtI + 2\TrueParameterRowtTop \svbx^{(i)}} x_t^{(i)}}. 
\end{align}
Each term in the above summation is an independent random variable and is bounded as follows:
\begin{align}
    & \Bigabs{\Bigparenth{\DeltatIp  \tsvbx^{(i)} x_t^{(i)}} \times \exp\Bigparenth{-\normalbrackets{\TrueExternalFieldtI + 2\TrueParameterRowtTop \svbx^{(i)}} x_t^{(i)}}} \\
    & \sequal{(a)} \Bigabs{\Bigparenth{\omtI x_t^{(i)} + 2\Omt\tp  \svbx^{(i)} x_t^{(i)}} \times \exp\Bigparenth{-\normalbrackets{\TrueExternalFieldtI + 2\TrueParameterRowtTop \svbx^{(i)}} x_t^{(i)}}} \\
    & \sless{(b)} \bigabs{\omtI + 2\Omt\tp  \svbx^{(i)}} \times \xmax  \times \exp\Bigparenth{\normalabs{\TrueExternalFieldtI + 2\TrueParameterRowtTop \svbx^{(i)}} \xmax} \\
    & \sless{(c)} \bigabs{\normalabs{\omtI} + 2\sonenorm{\Omt} \sinfnorm{\svbx^{(i)}}} \times \xmax \times \exp\Bigparenth{\bigparenth{\normalabs{\TrueExternalFieldtI} + 2\sonenorm{\TrueParameterRowt} \sinfnorm{\svbx^{(i)}}} \xmax} \\
    & \sless{(d)} \bigparenth{2\aGM + 8\aGM \bGM \xmax} \times \xmax \times \exp\Bigparenth{\normalparenth{\aGM + 2\aGM \bGM \xmax} \xmax} \sequal{\cref{eq:constants}} 2 \cone \ctwo \xmax, 
\end{align}
where $(a)$ follows by plugging in $\DeltatI$ and $\tsvbx^{(i)}$, $(b)$ follows because $\sinfnorm{\svbx^{(i)}} \leq \xmax $ for all $ i \in [n]$, $(c)$ follows by triangle inequality and Cauchy–Schwarz inequality, and $(d)$ follows because $\TrueExternalFieldI \in \ParameterSet_{\ExternalField} $ for all $ i \in [n]$, $\TrueParameterMatrix \in \ParameterSet_{\ParameterMatrix}$, $\omI \in 2\ParameterSet_{\ExternalField} $ for all $ i \in [n]$, $\Om \in 2\ParameterSet_{\ParameterMatrix}$, and $\sinfnorm{\svbx^{(i)}} \leq \xmax $ for all $ i \in [n]$.\\

\noindent Further, from \cref{prop_zero_mean_first_der}, we have $\Expectation\bigbrackets{\directionalGradientTrue} = 0$. Therefore, using the Hoeffding's inequality results in
\begin{align}
    \Probability \Bigparenth{\bigabs{\directionalGradientTrue} > \varepsilon_1} < 2 \exp\biggparenth{-\dfrac{n \varepsilon_1^2}{8\cone[2] \ctwo[2] \xmax^2}}. \label{eq_hoeffding_first_dir}
\end{align}
The proof follows by using the union bound over all $t \in [p]$.
%

%
%

%
\paragraph{Proof of \cref{eq:first_dir_derivative}: {\expressionfirstder}:}
\label{sub_sub_sec_proof_of_expression_first_der}
Fix any $t \in [p]$. The first-order partial derivatives of $\loss_t$ with respect to entries of $\ExtendedParameterRowT$ defined in \cref{eq:loss_function_p} are given by
\begin{align}
    \frac{\partial \loss_t(\ExtendedParameterRowT)}{\partial \ExternalFieldtI} & = -\frac{1}{n}x_t^{(i)}\exp\Bigparenth{\!-\!\normalbrackets{\ExternalFieldtI \!+\! 2\ParameterRowt\tp \svbx^{(i)}} x_t^{(i)}} \qtext{for all} i \in [n],
    \qtext{and}\\
    \frac{\partial \loss_t(\ExtendedParameterRowT)}{\partial \ParameterTU[tu]} & =
    \begin{cases} -\frac{2}{n}\sumn x_t^{(i)} x_u^{(i)} \exp\Bigparenth{\!-\!\normalbrackets{\ExternalFieldtI \!+\! 2\ParameterRowt\tp \svbx^{(i)}} x_t^{(i)}} \qtext{for all} u \in [p]\setminus\braces{t}.\\
    0 \qtext{for} u = t
    \end{cases}
    \label{eq:theta_first_derivatives} 
\end{align}
Now, we can write the first-order directional derivative of $\loss_t$ as
\begin{align}
     \directionalGradient &\defeq\lim_{h\to 0}\frac{\loss_t(\ExtendedParameterRowT + h \uOmT)-\loss_t(\ExtendedParameterRowT)}{h} =  \sumn  \omtI \frac{\partial \loss_t(\ExtendedParameterRowT)}{\partial \ExternalFieldtI} + \sumu \Omtu \frac{\partial \loss_t(\ExtendedParameterRowT)}{\partial \ParameterTU[tu]}\\ 
     & = -\frac{1}{n}\sumn  \Bigparenth{\omtI x_t^{(i)} + 2\sum_{u \in [p] \setminus \{t\}} \Omtu x_t^{(i)} x_u^{(i)}} \exp\Bigparenth{\!-\!\normalbrackets{\ExternalFieldtI \!+\! 2\ParameterRowt\tp \svbx^{(i)}} x_t^{(i)}} \\
     & = -\frac{1}{n}\sumn  \Bigparenth{\omtI x_t^{(i)} + 2\Omt\tp  \svbx^{(i)} x_t^{(i)}}  \exp\Bigparenth{\!-\!\normalbrackets{\ExternalFieldtI \!+\! 2\ParameterRowt\tp \svbx^{(i)}} x_t^{(i)}} \\
     & \sequal{(a)} -\frac{1}{n}\sumn  \Bigparenth{\DeltatIp  \tsvbx^{(i)} x_t^{(i)}}  \exp\Bigparenth{\!-\!\normalbrackets{\ExternalFieldtI \!+\! 2\ParameterRowt\tp \svbx^{(i)}} x_t^{(i)}},
\end{align}
where $(a)$ follows from the definitions of $\DeltatI$ and $\tsvbx^{(i)}$.
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%

\paragraph{Proof of \cref{prop_zero_mean_first_der}: {\zeromeangradient}:}
\label{sub:proof_of_prop_zero_mean_first_der}
%
Fix any $t \in [p]$. From \cref{eq:first_dir_derivative}, we have
\begin{align}
    \Expectation\bigbrackets{\directionalGradientTrue} & \sequal{\cref{eq:first_dir_derivative}}  -\frac{1}{n} \sumn \Expectation\biggbrackets{  \Bigparenth{\DeltatIp  \tsvbx^{(i)} x_t^{(i)}} \exp\Bigparenth{-\normalbrackets{\TrueExternalFieldtI + 2\TrueParameterRowtTop \svbx^{(i)}} x_t^{(i)}}}\\ 
    &  \sequal{(a)} -\frac{1}{n}\sumn  \sum_{u \in [p+1]} \DeltaItu \Expectation_{\svbx^{(i)}, \svbz^{(i)}} \Bigbrackets{\tsvbx^{(i)}_u x_t^{(i)} \exp\Bigparenth{-\normalbrackets{\TrueExternalFieldt(\svbz^{(i)}) + 2\TrueParameterRowtTop \svbx^{(i)}} x_t^{(i)}}} \\
    &  \sequal{(b)} -\frac{1}{n}\sumn  \sum_{u \in [p+1]\setminus\braces{i+1}} \DeltaItu \Expectation_{\svbx^{(i)}, \svbz^{(i)}} \Bigbrackets{\tsvbx^{(i)}_u x_t^{(i)} \exp\Bigparenth{-\normalbrackets{\TrueExternalFieldt(\svbz^{(i)}) + 2\TrueParameterRowtTop \svbx^{(i)}} x_t^{(i)}}}, 
\end{align}
where $(a)$ follows by linearity of expectation and by plugging in $\TrueExternalFieldtI = \TrueExternalFieldt(\svbz^{(i)})$ and $(b)$ follows because $\DeltaItu = \Omtu[t] = 0$ for $u = t+1$ for every $i \in [n]$. To complete the proof, we show
\begin{align}
    \Expectation_{\svbx^{(i)}, \svbz^{(i)}} \Bigbrackets{\tsvbx^{(i)}_u x_t^{(i)} \exp\Bigparenth{-\normalbrackets{\TrueExternalFieldt(\svbz^{(i)}) + 2\TrueParameterRowtTop \svbx^{(i)}} x_t^{(i)}}} = 0  \qtext{for all}  t \in [p], i \in [n], u \in [p+1]\setminus\braces{t+1}.
\end{align}
Fix any $t \in [p], i \in [n], u \in [p+1]\setminus\braces{t+1}$. We have
%
\begin{align}
    & \Expectation_{\svbx^{(i)}, \svbz^{(i)}} \Bigbrackets{\tsvbx^{(i)}_u x_t^{(i)} \exp\Bigparenth{-\normalbrackets{\TrueExternalFieldt(\svbz^{(i)}) + 2\TrueParameterRowtTop \svbx^{(i)}} x_t^{(i)}}}\\
    = & \!\!\!\!\!\! \int\limits_{\cX^p \times \cZ^{p_z}} \!\!\!\!\! \tx^{(i)}_u x_t^{(i)} \exp\Bigparenth{-\normalbrackets{\TrueExternalFieldt(\svbz^{(i)}) + 2\TrueParameterRowtTop \svbx^{(i)}} x_t^{(i)}} f_{\rvbx, \rvbz}\bigparenth{\svbx^{(i)}, \svbz^{(i)}} d\svbx^{(i)} d\svbz^{(i)}\\
    = & \!\!\!\!\!\! \int\limits_{\cX^p \times \cZ^{p_z}} \!\!\!\!\!\! \tx^{(i)}_u x_t^{(i)} \exp\Bigparenth{\!\!-\!\normalbrackets{\TrueExternalFieldt(\svbz^{(i)}) \!+\! 2\TrueParameterRowtTop \svbx^{(i)}} x_t^{(i)}} f_{\rvbx_{-t}, \rvbz}\bigparenth{\svbx_{-t}^{(i)}, \svbz^{(i)}}  \TrueConditionalDistIt   d\svbx^{(i)} d\svbz^{(i)}\\
    \sequal{(a)} & \int\limits_{\cX^p \times \cZ^{p_z}} \dfrac{\tx^{(i)}_u x_t^{(i)} f_{\rvbx_{-t}, \rvbz}\bigparenth{\svbx_{-t}^{(i)}, \svbz^{(i)}} d\svbx^{(i)} d\svbz^{(i)}} {\int_{\cX} \exp\Bigparenth{\normalbrackets{\TrueExternalFieldt(\svbz^{(i)}) + 2\TrueParameterRowtTop \svbx^{(i)}} x_t^{(i)}}d x_t^{(i)}} \\
    \sequal{(b)} & \int\limits_{\cX^{p-1} \times \cZ^{p_z}} \biggbrackets{\int_{\cX} x_t^{(i)} dx_t^{(i)}} \dfrac{\tx^{(i)}_u f_{\rvbx_{-t}, \rvbz}\bigparenth{\svbx_{-t}^{(i)}, \svbz^{(i)}} d\svbx_{-t}^{(i)} d\svbz^{(i)}} {\int_{\cX} \exp\Bigparenth{\normalbrackets{\TrueExternalFieldt(\svbz^{(i)}) + 2\TrueParameterRowtTop \svbx^{(i)}} x_t^{(i)}}d x_t^{(i)}} \\
    \sequal{(c)} & ~~~ 0.
\end{align}
where $(a)$ follows by plugging in $ \TrueConditionalDistIt $ from \cref{eq_conditional_dist}, $(b)$ follows by re-arranging and observing that $\tx^{(i)}_u \neq x_t^{(i)}$ for any $u \in [p+1]\setminus\braces{t+1}$, and $(c)$ follows because $\int_{\cX} x_t^{(i)} dx_t^{(i)} = 0$ when $\cX$ is symmetric around 0.



\subsubsection{Proof of \cref{lemma_conc_first_sec_der} \cref{item_conc_sec_der}: \conchessresultname}
\label{sub:proof_of_lemma_conc_sec_der}
\newcommand{\lowerboundhessian}{Lower bound on the second directional derivative}
%
%
%
%
%
\noindent We start by claiming that the second-order directional derivative can be lower bounded by a quadratic form. We provide a proof in \cref{sub:proof_of_lemma_lower_bound_sec_der}. 

\begin{lemma}[\tbf{\lowerboundhessian}]\label{lemma_lower_bound_sec_der}
For every $t \in [p]$ with $\uOmT$ defined in \cref{eq:omega_defn}, we have
%
\begin{align}
    \directionalHessian \geq \frac{1}{n \ctwo } \sumn  \Bigparenth{\DeltatIp  \tsvbx^{(i)}x_t^{(i)}}^2,
\end{align}
where $\DeltatI \defeq \begin{bmatrix} \omtI \\ 2\Omt \end{bmatrix} \in \real^{p+1}$ and $\tsvbx^{(i)} \defeq \begin{bmatrix} 1 \\ \svbx^{(i)} \end{bmatrix} \in \real^{p+1} $ for all $ i \in [n]$, and the constant $\ctwo$ was defined in \cref{eq:constants}.
\end{lemma}

\noindent Next, we claim that the conditional variance of $x_t^{(i)}$ conditioned on $\rvbx_{-t} = \svbx_{-t}^{(i)}$ and $\rvbz = \svbz^{(i)}$ is lower bounded by a constant for every $t \in [p]$ and $i \in [n]$. We provide a proof in \cref{sub:proof_of_prop_lower_bound_variance}.  

\newcommand{\lowerboundcondvar}{Lower bound on the conditional variance}
\begin{lemma}[\tbf{\lowerboundcondvar}]\label{prop_lower_bound_variance}
For every $t \in [p]$ and $i \in [n]$, we have
\begin{align}
    \Variance\bigparenth{x_t^{(i)} \big| \svbx_{-t}^{(i)}, \svbz^{(i)}}  \geq \frac{\xmax}{\pi e \ctwo[2]},
\end{align}
where the constant $\ctwo$ was defined in \cref{eq:constants}.
\end{lemma}

%

%

%
%
%
%
%
%
%

%
%
%
%
%
%
%
%
%
%
%
%
\noindent Given these, we proceed to show the anti-concentration of the second-order directional derivative. Fix any $t \in [p]$ and any $\ExtendedParameterMatrix \in \ParameterSet_{\ExternalField}^n \times \ParameterSet_{\ParameterMatrix}$. From \cref{lemma_lower_bound_sec_der}, we have
\begin{align}
    \directionalHessian & \geq  \frac{1}{n \ctwo}\sumn  \Bigparenth{\DeltatIp  \tsvbx^{(i)} x_t^{(i)}}^2. \label{eq_sec_der_form_lower_bound}
\end{align}
First, using the Hoeffding’s inequality, let us show concentration of $\frac{1}{n}\sumn  \bigparenth{\DeltatIp  \tsvbx^{(i)} x_t^{(i)}}^2$ around its mean. We observe that each term in the summation is an independent random variable and is bounded
as follows
\begin{align}
    \bigparenth{\DeltatIp  \tsvbx^{(i)} x_t^{(i)}}^2 \sequal{(a)} \bigparenth{\omtI x_t^{(i)} + 2\Omt\tp  \svbx^{(i)} x_t^{(i)}}^2 & \sless{(b)} \bigparenth{\omtI + 2\Omt\tp  \svbx^{(i)}}^2 \xmax^2\\
    & \sless{(c)} \bigparenth{\normalabs{\omtI} + 2\sonenorm{\Omt} \sinfnorm{\svbx^{(i)}}}^2  \xmax^2\\
    & \sless{(d)} \bigparenth{2\aGM + 8\aGM \bGM \xmax}^2 \xmax^2 \sequal{\cref{eq:constants}} 4 \cone[2] \xmax^2,
\end{align}
where $(a)$ follows by plugging in $\DeltatI$ and $\tsvbx^{(i)}$, $(b)$ follows because $\sinfnorm{\svbx^{(i)}} \leq \xmax $ for all $ i \in [n]$, $(c)$ follows by triangle inequality and Cauchy–Schwarz inequality, and $(d)$ follows because $\Om \in 2\ParameterSet_{\ParameterMatrix}$, $\omI \in 2\ParameterSet_{\ExternalField}$, and $\sinfnorm{\svbx^{(i)}} \leq \xmax $ for all $ i \in [n]$. Then, from the Hoeffding's inequality, we have
\begin{align}
    \Probability \Bigparenth{\biggabs{\frac{1}{n}\sumn  \Bigparenth{\DeltatIp  \tsvbx^{(i)} x_t^{(i)}}^2 - \frac{1}{n}\sumn  \Expectation\biggbrackets{\Bigparenth{\DeltatIp  \tsvbx^{(i)} x_t^{(i)}}^2} } > \varepsilon} < 2 \exp\biggparenth{-\dfrac{n \varepsilon^2}{32\cone[4] \xmax^4}}. \label{eq_hoeffding_first_dir}
\end{align}
Applying the union bound over all $t \in [p]$, for any $\delta \in (0,1)$ and uniformly for all $t \in [p]$, we have
\begin{align}
    \frac{1}{n}\sumn  \Bigparenth{\DeltatIp  \tsvbx^{(i)} x_t^{(i)}}^2 \geq \frac{1}{n}\sumn  \Expectation\biggbrackets{\Bigparenth{\DeltatIp  \tsvbx^{(i)} x_t^{(i)}}^2} - \varepsilon \label{eq_empirical_quad_form_lower_bound}
\end{align}
with probability at least $1-\delta$ as long as
\begin{align}
    n \geq \dfrac{32\cone[4] \xmax^4}{\varepsilon^2}\log\biggparenth{\dfrac{2p}{\delta}}.
\end{align}
Now, we lower bound $\Expectation\Bigbrackets{ \bigparenth{\DeltatIp  \tsvbx^{(i)} x_t^{(i)}}^2 }$ for every $t \in [p]$ and every $i \in [n]$. Fix any $t \in [p]$ and $i \in [n]$. We have
\begin{align}
     \Expectation\biggbrackets{ \Bigparenth{\DeltatIp  \tsvbx^{(i)} x_t^{(i)}}^2 } \sgreat{(a)} \Variance \biggbrackets{ \DeltatIp  \tsvbx^{(i)} x_t^{(i)} } = \Variance \biggbrackets{\omtI x_t^{(i)} + 2\Omt\tp  \svbx^{(i)} x_t^{(i)}}, \label{eq_expected_quad_form_lower_bound}
\end{align}
 where $(a)$ follows from the fact that for any random variable a, $\Expectation\normalbrackets{a^2} \geq \Variance\normalbrackets{a}$. We define the following set to lower bound $\Variance \bigbrackets{ \omtI x_t^{(i)} + 2\Omt\tp  \svbx^{(i)} x_t^{(i)}}$:
\begin{align}
    \cE(\TrueParameterMatrix) \defn \braces{(t,u) \in [p]^2: t < u, \TrueParameterTU \neq 0},\label{eq:edge_set}
\end{align}
and consider the graph $\cG(\TrueParameterMatrix) = ([p], \cE(\TrueParameterMatrix))$ with $[p]$ as nodes and $\cE(\TrueParameterMatrix)$ as edges such that $\TrueJointDist$ is Markov with respect to $\cG(\TrueParameterMatrix)$. We claim that there exists a non-empty set $\cR_t \subset [p]\setminus\braces{t}$ such that 
 \begin{enumerate}[label=(\roman*)]
 \item\label{item_independence_set} $\cR_t$ is an independent set of $\cG(\TrueParameterMatrix)$, i.e., there are no edges between any pair of nodes in $\cR_t$, and
%
%
%
    \item\label{eq_independentSetProperty} the row vector $\Omt$ satisfies $\sum_{u \in \cR_t} \normalabs{\Omtu}^2 
    \geq 
    %
    \frac{1}{\bGM+1}\twonorm{\Omt}^2$.
    %
    %
    %
 \end{enumerate}
%
%
%
%
%
%
Taking this claim as given at the moment, we continue our proof. Denoting $\cR_t^c \defeq [p] \setminus \cR_t$, and using the law of total variance, the variance term in \cref{eq_expected_quad_form_lower_bound} can be lower bounded as
\begin{align}
\Variance \biggbrackets{\omtI x_t^{(i)} + 2\Omt\tp  \svbx^{(i)} x_t^{(i)}}  & \geq \Expectation\biggbrackets{\Variance \Bigbrackets{\omtI x_t^{(i)} + 2\Omt\tp  \svbx^{(i)} x_t^{(i)} \Big| \svbx_{\cR_t^c}^{(i)},  \svbz^{(i)}}}\\
    & \sequal{(a)} 4\Expectation\Bigbrackets{\bigparenth{x_t^{(i)}}^2 \Variance\Bigparenth{\sum_{u \in \cR_t} \Omtu x_u^{(i)} \Big| \svbx_{\cR_t^c}^{(i)}, \svbz^{(i)}}}\\
    & \sequal{(b)} 4\Expectation\Bigbrackets{\bigparenth{x_t^{(i)}}^2 \sum_{u \in \cR_t} \Omtu^2 \Variance\Bigparenth{ x_u^{(i)} \Big| \svbx_{\cR_t^c}^{(i)}, \svbz^{(i)}}}\\
    & \sequal{(c)} 4\Expectation\Bigbrackets{\bigparenth{x_t^{(i)}}^2 \sum_{u \in \cR_t} \Omtu^2 \Variance\Bigparenth{ x_u^{(i)} \Big| \svbx_{-u}^{(i)}, \svbz^{(i)}}}\\
    & \sgreat{(d)} \frac{4\xmax}{\pi e \ctwo[2]} \sum_{u \in \cR_t} \Omtu^2 \Expectation\Bigbrackets{\bigparenth{x_t^{(i)}}^2  }\\
    %
    & \sgreat{(e)} \frac{4\xmax}{\pi e \ctwo[2]} \sum_{u \in \cR_t} \Omtu^2 
    \Variance\Bigparenth{ x_t^{(i)} \Big| \svbx_{-t}^{(i)}, \svbz^{(i)}}\\
    & \sgreat{(f)} \frac{4\xmax^2}{\pi^2 e^2 \ctwo[4]} \sum_{u \in \cR_t} \Omtu^2 \sgreat{\cref{eq_independentSetProperty}} \frac{4\xmax^2 \twonorm{\Omt}^2}{\pi^2 e^2  \normalparenth{\bGM + 1} \ctwo[4]}, \label{eq_var_intermediate_lower_bound}
\end{align}
 where $(a)$ follows because $(x_u^{(i)})_{u \in \cR_t^c}$ are deterministic when conditioned on themselves, and $t \in \cR_t^c$, $(b)$ follows because $(x_u^{(i)})_{u \in \cR_t}$ are conditionally independent given $\svbx_{\cR_t^c}^{(i)}$ and $\svbz^{(i)}$ which is a direct consequence of \cref{item_independence_set}, $(c)$ follows because of the local Markov property (as the conditioning set includes all the neighbors in $\cG(\TrueParameterMatrix)$ of each node in  $\cR_t$),
%
 $(d)$ and $(f)$ follow from \cref{prop_lower_bound_variance}, and $(e)$ follows because $\Expectation\Bigbrackets{\bigparenth{x_t^{(i)}}^2} = \Expectation\biggbrackets{\Expectation\Bigbrackets{\bigparenth{x_t^{(i)}}^2 \Big| \svbx_{-t}^{(i)}, \svbz^{(i)}}} \geq \Variance\Bigparenth{ x_t^{(i)} \Big| \svbx_{-t}^{(i)}, \svbz^{(i)}}$.\\
 
\noindent Combining \cref{eq_sec_der_form_lower_bound,eq_empirical_quad_form_lower_bound,eq_expected_quad_form_lower_bound,eq_var_intermediate_lower_bound}, for any $\delta \in (0,1)$ and uniformly for all $t \in [p]$, we have
\begin{align}
    \directionalHessian \geq \frac{1}{\ctwo} \biggparenth{\frac{4\xmax^2 \twonorm{\Omt}^2}{\pi^2 e^2  \normalparenth{\bGM + 1} \ctwo[4]} - \varepsilon}
\end{align}
with probability at least $1-\delta$ as long as
\begin{align}
    n \geq \dfrac{32\cone[4] \xmax^4}{\varepsilon^2}\log\biggparenth{\dfrac{2p}{\delta}}.
\end{align}
Choosing $\varepsilon_2 = \varepsilon / \ctwo$ and $\delta_2 = \delta$ yields the claim.\\

\noindent It remains to construct the set $\cR_t$ that is an independent set of $\cG(\TrueParameterMatrix)$ and satisifies~\cref{eq_independentSetProperty}.

\paragraph{Construction of the set $\cR_t$:}
%
%
We select $r_1 \in [p] \setminus \braces{t}$ such that
 \begin{align}
     \normalabs{\Omtu[r_1]} \geq \normalabs{\Omtu} \qtext{for all} u \in [p] \setminus \braces{t , r_1}.
 \end{align}
 Next, we identify $r_2 \in [p] \setminus \braces{t, r_1, \cN(r_1)}$ such that
 \begin{align}
     \normalabs{\Omtu[r_2]} \geq \normalabs{\Omtu} \qtext{for all} u \in [p] \setminus \braces{t , r_1, \cN(r_1), r_2}.
 \end{align}
 We continue identifying $r_3, \ldots, r_s$ in such a manner till no more nodes are left, where $s$ denotes the total number of nodes selected. Now we define $\cR_t \defeq \{ r_1, \cdots , r_s\}$. Further, for any $u \in [p]$, let $\cN(u)$ denote the set of neighbors of $u$ in $\cG(\TrueParameterMatrix)$. We have $\normalabs{\cN(u)} \leq \szeronorm{\TrueParameterRowt[u]} \leq \bGM$ from \cref{eq:edge_set} and \cref{assumptions}\cref{assumption_sparse_parameters}. We can now see that $\cR_t$ is an independent set of $\cG(\TrueParameterMatrix)$ as claimed in \cref{item_independence_set} such
 that it satisfies \cref{eq_independentSetProperty} by construction.
 
 
\paragraph{Proof of \cref{lemma_lower_bound_sec_der}: \lowerboundhessian:}
\label{sub:proof_of_lemma_lower_bound_sec_der}
%
For every $t \in [p]$ with $\uOmT$ defined in \cref{eq:omega_defn}, we claim that the second-order directional derivative of the loss function defined in \cref{eq:loss_function_p} is given by
\begin{align}
    \directionalHessian = \frac{1}{n}\sumn  \Bigparenth{\DeltatIp  \tsvbx^{(i)}x_t^{(i)}}^2  \exp\Bigparenth{-\normalbrackets{\ExternalFieldtI + 2\ParameterRowt\tp \svbx^{(i)}} x_t^{(i)}}, 
  \label{eq:second_dir_derivative}
\end{align}
where $\DeltatI \defeq \begin{bmatrix} \omtI \\ 2\Omt \end{bmatrix} \in \real^{p+1}$ and $\tsvbx^{(i)} \defeq \begin{bmatrix} 1 \\ \svbx^{(i)} \end{bmatrix} \in \real^{p+1} $ for all $ i \in [n]$. We provide a proof at the end.

\newcommand{\expressionsecder}{Expression for second directional derivative}

%
%
%
%
%
%
%
%
\noindent Given this, we proceed to prove the lower bound on the second directional derivative. Fix any 
%
$t \in [p]$. From \cref{eq:second_dir_derivative}, we have
\begin{align}
     \directionalHessian &  = \frac{1}{n}\sumn   \Bigparenth{\DeltatIp  \tsvbx^{(i)}x_t^{(i)}}^2 \times \exp\Bigparenth{-\normalbrackets{\ExternalFieldtI + 2\ParameterRowt\tp \svbx^{(i)}} x_t^{(i)}}  \\
     & \sgreat{(a)} \frac{1}{n}\sumn   \Bigparenth{\DeltatIp  \tsvbx^{(i)}x_t^{(i)}}^2 \times \exp\Bigparenth{-\normalabs{\ExternalFieldtI + 2\ParameterRowt\tp \svbx^{(i)}} \xmax}  \\
     & \sgreat{(b)} \frac{1}{n}\sumn   \Bigparenth{\DeltatIp  \tsvbx^{(i)}x_t^{(i)}}^2 \times \exp\Bigparenth{-\bigparenth{\normalabs{\ExternalFieldtI} + 2\sonenorm{\ParameterRowt} \sinfnorm{\svbx^{(i)}}} \xmax}\\
     & \sgreat{(c)} \frac{1}{n}\sumn   \Bigparenth{\DeltatIp  \tsvbx^{(i)}x_t^{(i)}}^2 \times  \exp\Bigparenth{-\normalparenth{\aGM + 2\aGM \bGM \xmax} \xmax}\\
     &  \sequal{\cref{eq:constants}} \frac{1}{\ctwo n}\sumn  \Bigparenth{\DeltatIp  \tsvbx^{(i)}x_t^{(i)}}^2, \label{eq_lower_bound_sec_der}
\end{align}
where $(a)$ follows because $\sinfnorm{\svbx^{(i)}} \leq \xmax $ for all $ i \in [n]$, $(b)$ follows by triangle inequality and Cauchy–Schwarz inequality, and  $(c)$ follows because $\ExternalFieldI \in \ParameterSet_{\ExternalField} $ for all $ i \in [n]$, $\ParameterMatrix \in \ParameterSet_{\ParameterMatrix}$, and $\sinfnorm{\svbx^{(i)}} \leq \xmax $ for all $ i \in [n]$.

 \paragraph{Proof of \cref{eq:second_dir_derivative}: {\expressionsecder}:}
%
Fix any $t \in [p]$. The second-order partial derivatives of $\loss_t$ with respect to entries of $\ExtendedParameterRowT$ defined in \cref{eq:loss_function} are given by
\begin{align}
    \frac{\partial^2 \loss_t(\ExtendedParameterRowT)}{\partial \bigbrackets{\ExternalFieldtI}^2} & = \frac{1}{n}\bigbrackets{x_t^{(i)}}^2\exp\Bigparenth{-\normalbrackets{\ExternalFieldtI + 2\ParameterRowt\tp \svbx^{(i)}} x_t^{(i)}} \qtext{for all} i \in [n],\\
    \frac{\partial^2 \loss_t(\ExtendedParameterRowT)}{\partial \ParameterTU[tu] \ParameterTU[tv]} & = \frac{4}{n}\sumn\bigbrackets{x_t^{(i)}}^2 x_u^{(i)} x_v^{(i)} \exp\Bigparenth{-\normalbrackets{\ExternalFieldtI + 2\ParameterRowt\tp \svbx^{(i)}} x_t^{(i)}} \qtext{for all} u,v \in [p]\setminus\braces{i}, \stext{and}\\
    \frac{\partial^2 \loss_t(\ExtendedParameterRowT)}{\partial \ParameterTU[tu] \ExternalFieldtI} & = \frac{\partial^2 \loss_t(\ExtendedParameterRowT)}{\partial \ExternalFieldtI \ParameterTU[tu]}  = \frac{2}{n}\bigbrackets{x_t^{(i)}}^2 x_u^{(i)} \exp\Bigparenth{-\normalbrackets{\ExternalFieldtI + 2\ParameterRowt\tp \svbx^{(i)}} x_t^{(i)}} \qtext{for all} i \in [n], u \in [p]\setminus\braces{t}.
    %
    %
    \label{eq:theta_second_derivatives} 
\end{align}
Now, we can write the second-order directional derivative of $\loss_t$ as
\begin{align}
     \directionalHessian &\defeq\lim_{h\to 0}\frac{\partial_{\uOmT}\loss_t(\ExtendedParameterRowT+h \uOmT)\!-\!\partial_{\uOmT}\loss_t(\ExtendedParameterRowT)}{h} \\
     & =  \sumn  \bigbrackets{\omtI}^2 \frac{\partial^2 \loss_t(\ExtendedParameterRowT)}{\partial \bigbrackets{\ExternalFieldtI}^2} + \sumu \sumu[v] \Omtu \Omtu[v] \frac{\partial^2 \loss_t(\ExtendedParameterRowT)}{\partial \ParameterTU[tu] \ParameterTU[tv]} + 2\sumn \sumu \omtI \Omtu \frac{\partial^2 \loss_t(\ExtendedParameterRowT)}{\partial \ParameterTU[tu] \ExternalFieldtI} \\ 
     & = \frac{1}{n}\sumn  \Bigparenth{ \bigbrackets{\omtI x_t^{(i)}}^2 + 4\sumu \Omtu x_t^{(i)} x_u^{(i)} \sumu[v]  \Omtu[v]  x_t^{(i)} x_v^{(i)} + 4\omtI x_t^{(i)} \sumu  \Omtu x_t^{(i)} x_u^{(i)}}  \\
     & \qquad \qquad \times \exp\Bigparenth{-\normalbrackets{\ExternalFieldtI + 2\ParameterRowt\tp \svbx^{(i)}} x_t^{(i)}} \\
     & = \frac{1}{n}\sumn  \Bigparenth{\omtI x_t^{(i)} + 2\Omt\tp  \svbx^{(i)}x_t^{(i)}}^2  \exp\Bigparenth{-\normalbrackets{\ExternalFieldtI + 2\ParameterRowt\tp \svbx^{(i)}} x_t^{(i)}} \\
     & \sequal{(a)} \frac{1}{n}\sumn  \Bigparenth{\DeltatIp  \tsvbx^{(i)}x_t^{(i)}}^2  \exp\Bigparenth{-\normalbrackets{\ExternalFieldtI + 2\ParameterRowt\tp \svbx^{(i)}} x_t^{(i)}},
\end{align}
where $(a)$ follows from the definitions of $\DeltatI$ and $\tsvbx^{(i)}$.

%
%

%
%
%
%
%
%
%
%

%

%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%


%
%
%
%
%
%
%
%
%
%
%

%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%




%
%

%
%
%

%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%

%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%

%
%
%
%
%
%
%
%
%
%
%
%
%
%

%
%
%
%
%

%
%
%
%
%

%
%
%
%
%
%
%
%
%
%
%
%
%

%
%
%
%
%
%

%
%
%
%
%
%
%
%
%
%
%
%

%
%
%
%
%
%
%
%
%
%


%
%
%

%

%
%
%
%
%
%
%

%

%
%
%
%
%
%
%
%

%

%

%
%
%
%
%
%
%

%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%

%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
 
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
 
%
%
%
%
%
%
%
%
%

%

%
%
%
%
%
%
%
%
%
%
%
%
%
 
%
%
%
%
%
%
%
%
%
%
%

%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%

%
%
%
%
%
%
%
%
%
%
%
%

\paragraph{Proof of \cref{prop_lower_bound_variance}: \lowerboundcondvar:}
\label{sub:proof_of_prop_lower_bound_variance}
For any random variable $\rvx$, let $\Entropy(\rvx)$ denote the differential entropy of $\rvx$.  Fix any $t \in [p]$ and $i \in [n]$. Then, we have
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
\begin{align}
    & 2\pi e \Variance\bigparenth{x_t^{(i)} \big| \svbx_{-t}^{(i)}, \svbz^{(i)}} \sgreat{(a)} {\exp\Bigparenth{\Entropy\bigbrackets{x_t^{(i)} \big| \svbx_{-t}^{(i)}, \svbz^{(i)}}}} \\
    & = {\exp\biggparenth{\!\!-\!
    %
    \!\!\!\! \int\limits_{\cX^p \times \cZ^{p_z}}  \!\!\!\!
    %
    %
    f_{\rvbx, \rvbz}(\svbx^{(i)}, \svbz^{(i)})
    \log \Bigparenth{ \TrueConditionalDistIt } d\svbx^{(i)} d\svbz^{(i)} }}\\
    & =  \exp\biggparenth{\!\!-\!
    %
    %
    \!\!\!\! \int\limits_{\cX^p \times \cZ^{p_z}} \!\!\!\!
    %
    %
    f_{\rvbx, \rvbz}(\svbx^{(i)}, \svbz^{(i)})
    \log \biggparenth{\frac{\exp\Bigparenth{\normalbrackets{\TrueExternalFieldt(\svbz^{(i)}) + 2\TrueParameterRowtTop \svbx^{(i)}} x_t^{(i)}}}{
    %
    \int_{
    %
    \cX}
    %
    \exp\Bigparenth{\normalbrackets{\TrueExternalFieldt(\svbz^{(i)}) + 2\TrueParameterRowtTop \svbx^{(i)}} x_t^{(i)}}d x_t^{(i)}}} d\svbx^{(i)} d\svbz^{(i)} \!}\\
    & \sgreat{(b)}  \exp\biggparenth{\!\!-\!
    %
    %
    \!\!\!\! \int\limits_{\cX^p \times \cZ^{p_z}} \!\!\!\!
    %
    f_{\rvbx, \rvbz}(\svbx^{(i)}, \svbz^{(i)})
    \log \biggparenth{\frac{\exp\Bigparenth{\normalabs{\TrueExternalFieldt(\svbz^{(i)}) + 2\TrueParameterRowtTop \svbx^{(i)}} \xmax}}{
    %
    %
    \int_{\cX}
    %
    \exp\Bigparenth{\!\!-\!\normalabs{\TrueExternalFieldt(\svbz^{(i)}) + 2\TrueParameterRowtTop \svbx^{(i)}} \xmax} d x_t^{(i)}}} d\svbx^{(i)} d\svbz^{(i)} \!}\\
    & \sgreat{(c)}  \exp\biggparenth{\!\!-\!
    %
    %
    \!\!\!\! \int\limits_{\cX^p \times \cZ^{p_z}}  \!\!\!\!
    %
    f_{\rvbx, \rvbz}(\svbx^{(i)}, \svbz^{(i)})
    \log \!\biggparenth{\!\!\frac{\exp\Bigparenth{\bigparenth{\normalabs{\TrueExternalFieldt(\svbz^{(i)})} \!+\! 2\sonenorm{\TrueParameterRowt} \sinfnorm{\svbx^{(i)}}} \xmax\!}}{
    %
    \int_{\cX}
    \exp\Bigparenth{\!\!-\!\bigparenth{\normalabs{\TrueExternalFieldt(\svbz^{(i)})} \!+\! 2\sonenorm{\TrueParameterRowt} \sinfnorm{\svbx^{(i)}}} \xmax\!} d x_t^{(i)}}\!\!} d\svbx^{(i)} d\svbz^{(i)} \!}\\
    & \sgreat{(d)}  \exp\biggparenth{\!\!-\!
    %
    \!\!\!\! \int\limits_{\cX^p \times \cZ^{p_z}}  \!\!\!\!
    %
    f_{\rvbx, \rvbz}(\svbx^{(i)}, \svbz^{(i)})
    \log \biggparenth{\frac{\exp\Bigparenth{\normalparenth{\aGM + 2\aGM \bGM \xmax} \xmax}}{
    %
    \int_{\cX}
    \exp\Bigparenth{-\normalparenth{\aGM + 2\aGM \bGM \xmax} \xmax} d x_t^{(i)}}} d\svbx^{(i)} d\svbz^{(i)} \!}\\
    & \sequal{(e)} \exp\biggparenth{ \!-\!
    %
    \!\!\!\!  \int\limits_{\cX^p \times \cZ^{p_z}}  \!\!\!\!
    %
    f_{\rvbx, \rvbz}(\svbx^{(i)}, \svbz^{(i)})
    \log \biggparenth{\frac{\cthree[2]}{2\xmax}} d\svbx^{(i)} d\svbz^{(i)}\!} = \frac{2\xmax}{ \cthree[2]}, \label{eq_conditional_variance_lower_bound}
\end{align}
where $(a)$ follows from Shannon's entropy inequality $(\Entropy(\cdot) \leq \log \sqrt{2\pi \Variance(\cdot)})$, $(b)$ follows because $\sinfnorm{\svbx^{(i)}} \leq \xmax $ for all $ i \in [n]$, $(c)$ follows by triangle inequality and Cauchy–Schwarz inequality, and  $(d)$ follows because $\TrueExternalField(\svbz^{(i)}) \in \ParameterSet_{\ExternalField} $ for all $ i \in [n]$, $\TrueParameterMatrix \in \ParameterSet_{\ParameterMatrix}$, $\sinfnorm{\svbx^{(i)}} \leq \xmax $ for all $ i \in [n]$, and $(e)$ follows because $\int_{\cX} dx_t^{(i)} = 2\xmax$.

%
%
%
%
%
%
%
%
%
%
%
%

%
%

%
%
%
%
%
%
%
%

%
%
%
%
%
%
%
%
%

%

%
%

%
%
%
%
%
%
%
%
%
%

%
%
%
%
%
%
%
%
%

%

%
%
%
%
%
%
%
%
%
%
%

%
%
%
%
%
%
%
%
%
%
%
%
%
%

%
\newcommand{\singleparameterseparation}{Gap between the loss function for a fixed parameter}


\subsection{Proof of \cref{lemma_lipschitzness_first_stage}: \lipschitznesslossfunction}
\label{sub:proof_lemma_lipschitzness_first_stage}
%
Fix any $t \in [p]$.
%
Consider the direction $\uOmT = \tExtendedParameterRowT - \ExtendedParameterRowT$ and note that $\omtI = 0$ for all $i \in [n]$. Now, define the function $q : [0,1] \to \Reals$ as follows:
\begin{align}
    q(a) = \loss_{t}\bigparenth{\ExtendedParameterRowT + a(\tExtendedParameterRowT - \ExtendedParameterRowT)}. \label{eq_func_f_lipschitz_first_stage}
\end{align}
%
Then, the desired inequality in \cref{eq_lipschitz_property_first_stage} is equivalent to $\normalabs{q(1) - q(0)} \leq \xmax^2 \ctwo \sonenorm{\Omt}$.
%
%
%
From the mean value theorem, there exists $a' \in (0,1)$ such that
\begin{align}
    \normalabs{q(1) - q(0)} 
    %
    = \biggabs{\dfrac{dq(a')}{da}}.  \label{eq_mvt_lipschitz_first_stage}
\end{align}
Therefore, we have
\begin{align}
\bigabs{q(1) \!-\! q(0)} & \sequal{\cref{eq_mvt_lipschitz_first_stage}} \biggabs{\dfrac{dq(a')}{da}} \\
& \sequal{\cref{eq_func_f_lipschitz_first_stage}} \Bigabs{\dfrac{d\loss_{t}\bigparenth{\ExtendedParameterRowT + a'(\tExtendedParameterRowT - \ExtendedParameterRowT)}}{da}} \\
& \sequal{\cref{eq_der_mapping}} \Bigabs{\directionalGradient\bigr|_{\ExtendedParameterRowT = \ExtendedParameterRowT + a'(\tExtendedParameterRowT - \ExtendedParameterRowT)}}\\
& \sequal{\cref{eq:first_dir_derivative}} \frac{1}{n} \biggabs{\sumn \! \Bigparenth{\DeltatIp  \tsvbx^{(i)} x_t^{(i)}} \! \exp\Bigparenth{\!-\!\Bigbrackets{\bigparenth{\ExternalFieldtI \!+\! a'(\tExternalFieldtI \!-\! \ExternalFieldtI)} \!+\! 2 \bigparenth{\ParameterRowt \!+\! a'(\tParameterRowt \!-\! \ParameterRowt)}\tp \svbx^{(i)}} x_t^{(i)}}}\\
%
%
%
& \sless{(a)}  \exp\Bigparenth{\bigparenth{\normalbrackets{(1-a') \aGM + a' \aGM} + 2\normalbrackets{(1-a') \aGM\bGM + a' \aGM \bGM}\xmax} \xmax} \frac{1}{n} \biggabs{\sumn  \Bigparenth{\DeltatIp  \tsvbx^{(i)} x_t^{(i)}}}\\
& \sless{(b)}  \frac{\xmax \ctwo}{n} \sumn\Bigabs{ {\DeltatIp  \tsvbx^{(i)}}} \sless{(c)}  \xmax \ctwo \Bigabs{\Omt\tp  \svbx^{(i)}} \sless{(d)} \xmax^2 \ctwo \sonenorm{\Omt},
\end{align}
where $(a)$ follows from triangle inequality, Cauchy–Schwarz inequality, $\ExternalFieldI, \tExternalFieldI \in \ParameterSet_{\ExternalField}$, $\ParameterMatrix, \tParameterMatrix \in \ParameterSet_{\ParameterMatrix}$, and $\sinfnorm{\svbx^{(i)}} \leq \xmax $ for all $ i \in [n]$, 
$(b)$ follows from \cref{eq:constants}, the triangle inequality, and because $\sinfnorm{\svbx^{(i)}} \leq \xmax $ for all $ i \in [n]$, $(c)$ follows from the definitions of $\DeltatI$ and $\tsvbx^{(i)}$ because $\omtI = 0$ for all $i \in [n]$, and $(d)$ follows from the triangle inequality, and because $\sinfnorm{\svbx^{(i)}} \leq \xmax $ for all $ i \in [n]$.


\section{Proof of \cref{theorem_parameters} Part II: \nodeparammainresultname}
\label{sec_proof_thm_node_parameters_recovery}
%
%
%
%
%
%
%
Throughout the proof, $c$ denotes either a universal constant or a constant depending on the model-parameters $\aGM$ and $\xmax$, and may change appearance line by line.\\

\noindent To analyze our estimate of the unit-level parameters, we use the estimate $\EstimatedParameterMatrix$ of the population-level parameter $\TrueParameterMatrix$ along with the associated guarantee provided in \cref{theorem_parameters}. We note that the constraints on the unit-level parameters in \cref{eq_estimated_parameters} are independent across units, i.e., $\ExternalFieldI[i] \in \ParameterSet_{\ExternalField}$ independently for all $ i \in [n]$. Therefore, we look at $n$ independent convex optimization problems by decomposing the loss function $\loss$ in \cref{eq:loss_function} and the estimate $\ExtendedEstimatedParameterMatrix$ in \cref{eq_estimated_parameters} as follows:
%
%
%
%
\begin{align}
    \loss^{(i)}\bigparenth{\ExternalFieldI} \defn \sump[t] \exp\Bigparenth{-\normalbrackets{\ExternalFieldtI + 2\EstimatedParameterRowt\tp \svbx^{(i)}} x_t^{(i)}} \qtext{and} \EstimatedExternalFieldI = \argmin_{\ExternalFieldI \in \ParameterSet_{\ExternalField}} \loss^{(i)}\bigparenth{\ExternalFieldI} \qtext{for all} i \in [n],
    \label{eq:loss_function_n}
\end{align}
%
Now, fix any $i \in [n]$. From \cref{eq:loss_function_n}, we have $\loss^{(i)}\bigparenth{\EstimatedExternalFieldI} \leq  \loss^{(i)}\bigparenth{\TrueExternalFieldI}$. Using contraposition, to prove this part, it is sufficient to show that all points $\ExternalFieldI \in \ParameterSet_{\ExternalField}$ that satisfy $\stwonorm{\ExternalFieldI - \TrueExternalFieldI} \geq R(\varepsilon, \delta)$ also uniformly satisfy
\begin{align}
    \loss^{(i)}\bigparenth{\ExternalFieldI} & \geq  \loss^{(i)}\bigparenth{\TrueExternalFieldI} + R^2(\varepsilon, \delta) \stext{for} n \geq \frac{c \exp({c\bGM})}{\varepsilon^4} \cdot \biggparenth{\log \frac{p}{\delta} + \metric_{\ExternalField}(\radius)}, \label{eq_sufficienct_condition_thm_node}
\end{align}
with probability at least $1-\delta$ where $R(\varepsilon, \delta)$ and $\radius$ were defined in \cref{eq_radius_node_thm}, and the metric entropy $\metric$ was defined in \cref{def_covering_number_metric_entropy}.
%
Then, the guarantee in \cref{theorem_parameters} follows by applying a union bound over all $i \in [n]$.\\

\noindent To that end, first, we claim that for any fixed $\ExternalFieldI \in \ParameterSet_{\ExternalField}$, if $\ExternalFieldI$ is far from $\TrueExternalFieldI$, then with high probability $\loss^{(i)}\bigparenth{\ExternalFieldI}$ will be significantly larger than $\loss^{(i)}\bigparenth{\TrueExternalFieldI}$.
%
We provide a proof in \cref{sub_proof_lemma_parameter_single_external_field}.
\begin{lemma}[\tbf{\singleparameterseparation}]\label{lemma_parameter_single_external_field}
Fix any $\varepsilon_1 > 0$, $\delta_1 \in (0,1)$, and $i \in [n]$. Then, for any $\ExternalFieldI \in \ParameterSet_{\ExternalField}$ such that $\stwonorm{\ExternalFieldI - \TrueExternalFieldI} \geq  \varepsilon_1 \ratio$ (see \cref{eq_radius_node_thm}), we have
\begin{align}
    \loss^{(i)}\bigparenth{\ExternalFieldI} \geq \loss^{(i)}\bigparenth{\TrueExternalFieldI} + \frac{2\sqrt{2}\aGM\bGM \xmax^3}{\pi e \ctwo[4]} \stwonorm{\ExternalFieldI - \TrueExternalFieldI}^2 \qtext{for} n \geq
    %
    \dfrac{c \exp({c\bGM}) \log(p/\delta_1)}{\varepsilon_1^4},
\end{align}
with probability at least 
%
$1-\delta_1 - c\bGM^2  \log p \cdot \exp(-\exp(-c\beta)\stwonorm{\ExternalFieldI - \TrueExternalFieldI}^2 ) 
%
$
where $\ctwo$ was defined in \cref{eq:constants}.
%
%
%
\end{lemma}

\noindent Next, we claim that the loss function $\loss^{(i)}$ is Lipschitz and capture this property via the following lemma. We provide a proof in \cref{sub:proof_lemma_lipschitzness}.
\begin{lemma}[\tbf{\lipschitznesslossfunction}]\label{lemma_lipschitzness}
Consider any $i \in [n]$. Then, the loss function $\loss^{(i)}$ is Lipschitz with respect to the $\ell_1$ norm $\sonenorm{\cdot}$ and with Lipschitz constant $\xmax \ctwo$, i.e.,
\begin{align}
    \bigabs{\loss^{(i)}\bigparenth{\tExternalFieldI} - \loss^{(i)}\bigparenth{\ExternalFieldI}} \leq \xmax \ctwo \sonenorm{\tExternalFieldI - \ExternalFieldI} \qtext{for all} \ExternalFieldI, \tExternalFieldI \in \ParameterSet_{\ExternalField}, \label{eq_lipschitz_property}
\end{align}
where the constant $\ctwo$ was defined in \cref{eq:constants}.
\end{lemma}

\noindent Given these lemmas, we now proceed with the proof. 

\paragraph{Proof strategy:} As mentioned earlier, the idea is to show that all points $\ExternalFieldI \in \ParameterSet_{\ExternalField}$ that satisfy $\stwonorm{\ExternalFieldI - \TrueExternalFieldI} \geq R(\varepsilon, \delta)$ also uniformly satisfy \cref{eq_sufficienct_condition_thm_node} with probability at least $1-\delta$. To do so, we consider the set of points $\ParameterSet_{\ExternalField}^{r,i} \subset \ParameterSet_{\ExternalField}$ whose distance from $\TrueExternalFieldI$ is at least $r > 0$ in $\ell_2$ norm. Then, using an appropriate covering set of $\ParameterSet_{\ExternalField}^{r,i}$ and the Lipschitzness of $\loss^{(i)}$, we show that the value of $\loss^{(i)}$ at all points in $\ParameterSet_{\ExternalField}^{r,i}$ is uniformly $O(r^2)$ larger than the value of $\loss^{(i)}$ at $\TrueExternalFieldI$ with high probability. 
%
Finally, we choose an $r$ small enough to make the failure probability smaller than $\delta$.

\paragraph{Gap between the loss function for all parameters in the covering set:} Consider any $r \geq \varepsilon \ratio$ (where $\ratio$ is defined in \cref{eq_radius_node_thm}) and the set of elements $\ParameterSet_{\ExternalField}^{r,i} \defn \braces{ \ExternalFieldI \in  \ParameterSet_{\ExternalField}: \stwonorm{\TrueExternalFieldI - \ExternalFieldI} \geq r}$. Let $\cC(\ParameterSet_{\ExternalField}^{r,i}, \varepsilon')$ be the $\varepsilon'$-covering number of the set $\ParameterSet_{\ExternalField}^{r,i}$ and let $\cU(\ParameterSet_{\ExternalField}^{r,i}, \varepsilon')$ be the associated $\varepsilon'$-cover  (see \cref{def_covering_number_metric_entropy})
%
where 
\begin{align}
    \varepsilon' \defn \frac{\sqrt{2} \aGM \bGM \xmax^2 r^2}{ \pi e \ctwo[5] \ratio}. \label{eq_eps'}
\end{align}
Now, we apply \cref{lemma_parameter_single_external_field} to each element in $\cU(\ParameterSet_{\ExternalField}^{r,i}, \varepsilon')$ and argue by a union bound that the value of $\loss^{(i)}$ at all points in $\cU(\ParameterSet_{\ExternalField}^{r,i}, \varepsilon')$ is uniformly $O(r^2)$ larger than the value of $\loss^{(i)}$ at $\TrueExternalFieldI$ with high probability. 
%
We start by considering any $\ExternalFieldI \in \cU(\ParameterSet_{\ExternalField}^{r,i}, \varepsilon')$. We have
\begin{align}
    \stwonorm{\TrueExternalFieldI - \ExternalFieldI} \sgreat{(a)} r,
    %
    \label{eq_lower_bound_two_norm_theta_diff}
\end{align}
where $(a)$ follows because $\cU(\ParameterSet_{\ExternalField}^{r,i}, \varepsilon') \subseteq \ParameterSet_{\ExternalField}^{r,i}$. 
%
Now, applying \cref{lemma_parameter_single_external_field} with $\varepsilon_1 = \varepsilon$ and $\delta_1 = \delta/2\cC(\ParameterSet_{\ExternalField}^{r,i}, \varepsilon')$, we have
\begin{align}
    \loss^{(i)}\bigparenth{\ExternalFieldI} \geq \loss^{(i)}\bigparenth{\TrueExternalFieldI} + \frac{2\sqrt{2}\aGM\bGM \xmax^3}{\pi e \ctwo[4]}\stwonorm{\TrueExternalFieldI - \ExternalFieldI}^2 \sgreat{\cref{eq_lower_bound_two_norm_theta_diff}} \loss^{(i)}\bigparenth{\TrueExternalFieldI} + \frac{2\sqrt{2}\aGM\bGM \xmax^3 r^2}{\pi e \ctwo[4]},
\end{align}
%
%
%
with probability at least $1-\delta/2\cC(\ParameterSet_{\ExternalField}^{r,i}, \varepsilon') - c\bGM^2  \log p \cdot \exp(-\exp(-c\beta)\stwonorm{\ExternalFieldI - \TrueExternalFieldI}^2 )$ 
%
whenever
\begin{align}
     n \geq \frac{c \exp({c\bGM}) \log\bigparenth{2\cC(\ParameterSet_{\ExternalField}^{r,i}, \varepsilon') \cdot p/\delta}}{\varepsilon^4}. \label{eq_n_condition_node_recovery}
\end{align}
%
%
By applying the union bound over $\cU(\ParameterSet_{\ExternalField}^{r,i}, \varepsilon')$, as long as $n$ satisfies \cref{eq_n_condition_node_recovery},
%
%
%
we have
\begin{align}
    \loss^{(i)}\bigparenth{\ExternalFieldI} \geq  \loss^{(i)}\bigparenth{\TrueExternalFieldI} + \frac{2\sqrt{2}\aGM\bGM \xmax^3 r^2}{\pi e \ctwo[4]} \stext{uniformly for every} \ExternalFieldI \in \cU(\ParameterSet_{\ExternalField}^{r,i}, \varepsilon'), \label{eq_union_bound_covering_set} 
\end{align}
with probability at least $1-\delta/2 - c\bGM^2 \cC(\ParameterSet_{\ExternalField}^{r,i}, \varepsilon') \log p \cdot \exp(-\exp(-c\beta)\stwonorm{\ExternalFieldI - \TrueExternalFieldI}^2)$ which can lower bounded by $1-\delta/2 - c\bGM^2 \cC(\ParameterSet_{\ExternalField}^{r,i}, \varepsilon') \log p \cdot \exp(-\exp(-c\beta)r^2)$ using \cref{eq_lower_bound_two_norm_theta_diff}.

\paragraph{Generalize beyond the covering set:} Next, we assume that \cref{eq_union_bound_covering_set} holds, and generalize from $\cU(\ParameterSet_{\ExternalField}^{r,i}, \varepsilon')$ to all of $\ParameterSet_{\ExternalField}^{r,i}$. Consider any $\tExternalFieldI \in \ParameterSet_{\ExternalField}^{r,i}$ and let $\ExternalFieldI$ be any point in $\cU(\ParameterSet_{\ExternalField}^{r,i}, \varepsilon')$ that satisfies $\stwonorm{\ExternalFieldI - \tExternalFieldI} \leq \varepsilon'$ (see \cref{def_covering_number_metric_entropy}). Then, from \cref{lemma_lipschitzness}, we have
\begin{align}
    \loss^{(i)}\bigparenth{\tExternalFieldI} \!\geq\! \loss^{(i)}\bigparenth{\ExternalFieldI} \!-\! \xmax \ctwo \sonenorm{\ExternalFieldI \!-\! \tExternalFieldI} \!\! & \sgreat{\cref{eq_radius_node_thm}} \loss^{(i)}\bigparenth{\ExternalFieldI} - \xmax \ctwo \ratio \stwonorm{\ExternalFieldI - \tExternalFieldI} \\
    & \sgreat{(a)} \loss^{(i)}\bigparenth{\ExternalFieldI} - \xmax \ctwo  \ratio \varepsilon' \\
    & \!\!\sgreat{\cref{eq_eps'}} \loss^{(i)}\bigparenth{\ExternalFieldI} \!-\!  \frac{\sqrt{2} \aGM\bGM \xmax^3 r^2}{ \pi e \ctwo[4]} 
    %
     \sgreat{\cref{eq_union_bound_covering_set}} \loss^{(i)}\bigparenth{\TrueExternalFieldI} \!+\! \frac{\sqrt{2} \aGM\bGM \xmax^3 r^2}{\pi e \ctwo[4]}, 
    %
\end{align}
where $(a)$ follows because $\stwonorm{\ExternalFieldI - \tExternalFieldI} \leq \varepsilon'$. Recall that $\tExternalFieldI$ is any point in $\ParameterSet_{\ExternalField}^{r,i}$, i.e., $\stwonorm{\TrueExternalFieldI - \tExternalFieldI} \geq r$. Therefore, we have an inequality that looks like \cref{eq_sufficienct_condition_thm_node}. It remains to bound $n$ and the failure probability. 
%

\paragraph{Bounding $n$:} To bound $n$ in \cref{eq_n_condition_node_recovery}, we bound the covering number $\cC(\ParameterSet_{\ExternalField}^{r,i}, \varepsilon')$ as follows
\begin{align}
    \cC(\ParameterSet_{\ExternalField}^{r,i}, \varepsilon') \sless{(a)} \cC(\ParameterSet_{\ExternalField}, \varepsilon'/2)
    %
    \label{eq_bound_covering_number}
\end{align}
where $(a)$ follows from the fact that for any sets $\mathcal{U} \subseteq \mathcal{V}$ and any ${\varepsilon}$, it holds that $\cC(\mathcal{U}, \varepsilon) \leq \cC(\mathcal{V}, \varepsilon/2)$. Then using \cref{eq_bound_covering_number} in \cref{eq_n_condition_node_recovery} and observing that $\varepsilon' = \frac{r^2}{c\exp(c\bGM)\ratio}$, it is sufficient for
\begin{align}
    %
    %
     n \geq \frac{c \exp({c\bGM})}{\varepsilon^4} \cdot \biggparenth{\log \frac{p}{\delta} + \metric_{\ExternalField}(r^2 \radius)}.
\end{align}

\paragraph{Bounding the failure probability:} To bound the failure probability by $\delta$, it is sufficient to chose $r$ such that
\begin{align}
    \delta & \geq \delta/2 + c\bGM^2 \cC(\ParameterSet_{\ExternalField}^{r,i}, \varepsilon') \log p \cdot \exp(-\exp(-c\beta)r^2) \sgreat{\cref{eq_bound_covering_number}} \delta/2 + c\bGM^2 \cC(\ParameterSet_{\ExternalField}, \varepsilon'/2) \log p \cdot \exp(-\exp(-c\beta)r^2).
    %
    %
    %
    \label{eq_failure_prob}
\end{align}
Re-arranging and taking logarithm on both sides of \cref{eq_failure_prob}, and observing that $\varepsilon' = \frac{r^2}{c\exp(c\bGM)\ratio}$, we have
%
%
%
\begin{align}
    \log \delta \geq c\biggbrackets{\log \bigparenth{\bGM^2 \log p} +  \metric_{\ExternalField}(r^2 \radius) - \exp(-c\bGM) r^2}.
    %
    \label{eq_failure_prob_log}
\end{align}
Finally, \cref{eq_failure_prob_log} holds whenever
%
%
%
\begin{align}
    r \geq c\exp\bigparenth{c\bGM} \sqrt{\log \dfrac{\bGM^2 \log p}{\delta} + \metric_{\ExternalField}(r^2 \radius)}.
\end{align}
Recalling that the choice of $r$ was such that $r \geq \varepsilon \ratio$ completes the proof.


%
%

%
\newcommand{\expressionfirstderstagetwo}{Expression for first directional derivative}



%
%
%
%
%
%
%
%
%
%
%
%
%
%
%

\subsection{Proof of \cref{lemma_parameter_single_external_field}: \singleparameterseparation}
\label{sub_proof_lemma_parameter_single_external_field}
%
%
%
%
%
Fix any $\varepsilon_1 > 0$, any $\delta_1 \in (0,1)$, and any $i \in [n]$. Consider any direction $\omI \in \Reals^{p}$ along the parameter $\ExternalFieldI$, i.e.,
\begin{align}
    \omI = \ExternalFieldI - \TrueExternalFieldI. \label{eq:omega_defn_stage_2}
\end{align}
We denote the first-order and the second-order directional derivatives of the loss function $\loss^{(i)}$ in \cref{eq:loss_function_n} along the direction $\omI$ evaluated at $\ExternalFieldI$ by $\directionalGradientExternalField$ and $\directionalHessianExternalField$ respectively. 
%
Below, we state a lemma (with proof divided across \cref{sub:proof_of_lemma_conc_grad_stage_2} and \cref{sub:proof_of_lemma_lower_bound_sec_der_stage_2}) that provides us a control on $\directionalGradientExternalFieldTrue$ and $\directionalHessianExternalField$. The assumptions of \cref{lemma_parameter_single_external_field} remain in force.

%
%
%
%
%
%
%
%
%
%
%
%

%
%
%
%
%
%
%

%
%
%
%
%
%
%
\newcommand{\conclocalresultname}{Control on first and second directional derivatives}
\newcommand{\concgradstagetwo}{Concentration of first directional derivative}
\newcommand{\anticoncgradstagetwo}{Anti-concentration of second directional derivative}


\begin{lemma}[\tbf{\conclocalresultname}]\label{lemma_conc_first_sec_der_stage_two}
For any fixed $\varepsilon_2, \varepsilon_3 > 0$, $\delta_2 \in (0,1)$, $i \in [n]$, $\ExternalFieldI \in \ParameterSet_{\ExternalField}$ with $\omI$ defined in \cref{eq:omega_defn_stage_2}, we have the following:
\begin{enumerate}[label=(\alph*)]
    \item\label{item_conc_first_der_stage_two} \textnormal{\tbf{\concgradstagetwo}}:
    \begin{align}
    \bigabs{\directionalGradientExternalFieldTrue} \leq\varepsilon_2 \sonenorm{\omI} + \varepsilon_3 \stwonorm{\omI}^2 \qtext{for} n \geq O\biggparenth{\dfrac{\exp\bigparenth{O(\bGM)} \log(p/\delta_2)}{\varepsilon_2^4}},
    %
\end{align}
with probability at least $1-\delta_2 - O\biggparenth{\bGM^2 \log p \exp\biggparenth{\dfrac{-\varepsilon_3^2\stwonorm{\omI}^2}{\exp\bigparenth{O(\bGM)}}}}$.
    \item\label{item_conc_sec_der_stage_two} \textnormal{\tbf{\anticoncgradstagetwo}}: \begin{align}
    \directionalHessianExternalField \geq \frac{16\sqrt{2}\aGM\bGM \xmax^3}{\pi e \ctwo[3]} \stwonorm{\omI}^2,
\end{align}
with probability at least $1- O\biggparenth{\bGM^2 \log p \exp\biggparenth{\dfrac{-\stwonorm{\omI}^2}{\exp\bigparenth{O(\bGM)}}}}$ where $\ctwo$ was defined in \cref{eq:constants}.
\end{enumerate}
%
\end{lemma}

\noindent Given this lemma, we now proceed with the proof.
%
%
%
%
Define a function $g : [0,1] \to \Reals^{p}$ as follows:
\begin{align}
    g(a) = \TrueExternalFieldI + a(\ExternalFieldI - \TrueExternalFieldI).
\end{align}
Notice that $g(0) = \TrueExternalFieldI$ and $g(1) = \ExternalFieldI$ as well as
\begin{align}
\dfrac{d\loss^{(i)}(g(a))}{da} = \directionalGradientExternalField\bigr|_{\ExternalFieldI = g(a)} \qtext{and} \dfrac{d^2\loss^{(i)}(g(a))}{da^2} = \directionalHessianExternalField\bigr|_{\ExternalFieldI = g(a)}. \label{eq_der_mapping_external_field}
\end{align}
By the fundamental theorem of calculus, we have
\begin{align}
    \dfrac{d\loss^{(i)}(g(a))}{da} \geq \dfrac{d\loss^{(i)}(g(a))}{da}\bigr|_{a = 0} + a \min_{a \in (0,1)}\dfrac{d^2\loss^{(i)}(g(a))}{da^2} \label{eq_fundamental_external_field}
\end{align}
Integrating both sides of \cref{eq_fundamental_external_field} with respect to $a$, we obtain
\begin{align}
  \loss^{(i)}(g(a)) - \loss^{(i)}(g(0)) & \geq  a \dfrac{d\loss^{(i)}(g(a))}{da}\bigr|_{a = 0} +  \dfrac{a^2}{2} \min_{a \in (0,1)}\dfrac{d^2\loss^{(i)}(g(a))}{da^2}\\
  & \sequal{\cref{eq_der_mapping_external_field}} a\directionalGradientExternalField\bigr|_{\ExternalFieldI = g(0)} +  \dfrac{a^2}{2} \min_{a \in (0,1)}\directionalHessianExternalField\bigr|_{\ExternalFieldI = g(a)}\\
  & \sequal{(a)} a\directionalGradientExternalFieldTrue +  \dfrac{a^2}{2} \min_{a \in (0,1)}\directionalHessianExternalField\bigr|_{\ExternalFieldI = g(a)}\\
  & \sgreat{(b)} - a \bigabs{\directionalGradientExternalFieldTrue} +  \dfrac{a^2}{2} \min_{a \in (0,1)}\directionalHessianExternalField\bigr|_{\ExternalFieldI = g(a)},
%
  \label{eq_taylor_expansion_external_field}
\end{align}
where $(a)$ follows because $g(0) = \TrueExternalFieldI$, and $(b)$ follows by the triangle inequality.
%
Plugging in $a = 1$ in \cref{eq_taylor_expansion_external_field} as well as using $g(0) = \TrueExternalFieldI$ and $g(1) = \ExternalFieldI$, we find that
\begin{align}
  \loss^{(i)}(\ExternalFieldI) - \loss^{(i)}(\TrueExternalFieldI) & \geq -  \bigabs{\directionalGradientExternalFieldTrue} +  \dfrac{1}{2} \min_{a \in (0,1)}\directionalHessianExternalField\bigr|_{\ExternalFieldI = g(a)} \label{eq_loss_separation_single_par_inter}
\end{align}
Now, we use \cref{lemma_conc_first_sec_der_stage_two} with $\varepsilon_2 = 2\sqrt{2}\aGM\bGM \xmax^3 \varepsilon_1/\pi e \ctwo[4]$, $\varepsilon_3 = 4\sqrt{2}\aGM\bGM \xmax^3/\pi e \ctwo[4]$, and $\delta_2 = \delta_1$. Therefore, with probability at least $1-\delta_1 - O\biggparenth{\bGM^2 \log p \exp\biggparenth{\dfrac{-\stwonorm{\omI}^2}{\exp\bigparenth{O(\bGM)}}}}$ and as long as $n \geq  O\biggparenth{\dfrac{\exp\bigparenth{O(\bGM)} \log(p/\delta_1)}{\varepsilon_1^4}}$, we have
\begin{align}
  \loss^{(i)}(\ExternalFieldI) - \loss^{(i)}(\TrueExternalFieldI) & \geq - \frac{2\sqrt{2}\aGM\bGM \xmax^3\varepsilon_1}{\pi e \ctwo[4]} \sonenorm{\omI} - \frac{4\sqrt{2}\aGM\bGM \xmax^3}{\pi e \ctwo[4]} \stwonorm{\omI}^2 +  \frac{8\sqrt{2}\aGM\bGM \xmax^3}{\pi e \ctwo[4]} \stwonorm{\omI}^2 \\
  & = - \frac{2\sqrt{2}\aGM\bGM \xmax^3\varepsilon_1}{\pi e \ctwo[4]} \sonenorm{\omI} + \frac{4\sqrt{2}\aGM\bGM \xmax^3}{\pi e \ctwo[4]} \stwonorm{\omI}^2 \\
  & \sgreat{\cref{eq_radius_node_thm}} -  \frac{2\sqrt{2}\aGM\bGM \xmax^3\varepsilon_1 \ratio}{\pi e \ctwo[4]} \stwonorm{\omI} + \frac{4\sqrt{2}\aGM\bGM \xmax^3}{\pi e \ctwo[4]} \stwonorm{\omI}^2 \\
  & \sgreat{(a)} -  \frac{2\sqrt{2}\aGM\bGM \xmax^3}{\pi e \ctwo[4]} \stwonorm{\omI}^2 + \frac{4\sqrt{2}\aGM\bGM \xmax^3}{\pi e \ctwo[4]} \stwonorm{\omI}^2 = \frac{2\sqrt{2}\aGM\bGM \xmax^3}{\pi e \ctwo[4]} \stwonorm{\omI}^2,
\end{align}
where $(a)$ follows because $\stwonorm{\omI} = \stwonorm{\ExternalFieldI - \TrueExternalFieldI} \geq  \varepsilon_1 \ratio$ according to the lemma statement.
%


\subsubsection{Proof of \cref{lemma_conc_first_sec_der_stage_two}~\cref{item_conc_first_der_stage_two}: \concgradstagetwo}\label{sub:proof_of_lemma_conc_grad_stage_2}
Fix some $i \in [n]$ and some $\ExternalFieldI \in \ParameterSet_{\ExternalField}$. Let $\omI$ defined in \cref{eq:omega_defn_stage_2}. We claim that the first-order directional derivative of $\loss^{(i)}$ defined in \cref{eq:loss_function_n} is given by 
\begin{align}
    \directionalGradientExternalField = - \sump \omIt x_t^{(i)} \exp\Bigparenth{-\normalbrackets{\ExternalFieldIt + 2\EstimatedParameterRowt\tp \svbx^{(i)}} x_t^{(i)}}. \label{eq:first_dir_derivative_stage_2}
\end{align}
We provide a proof at the end. For now, we assume the claim and proceed.

We note that the pair $\braces{\rvbx, \rvbz}$ corresponds to a $\tSGM$ (see \cref{def:tau_sgm}) with $\dGM \defn (\aGM, \aGM \bGM, \xmax, \ParameterMatrix)$. To show the concentration, we decompose $\directionalGradientExternalFieldTrue$ as a sum of  $\numindsets = 1024  \aGM^2 \bGM^2 \xmax^4 \log 4p$ terms using \cref{lemma_conditioning_trick} (see \cref{sec_conditioning_trick}) with $\lambda = \frac{1}{4\sqrt{2}\xmax^2}$ and focus on these  $\numindsets$ terms. 
%
%
%
%
%
%
%
%
%
%
%
%
Consider the $\numindsets$ subsets $\sets \in [p]$ obtained from \cref{lemma_conditioning_trick} with $\lambda = \frac{1}{4\sqrt{2}\xmax^2}$ and define
\begin{align}
    \psi_u(\svbx^{(i)}; \omI) \defeq \sum_{t \in \setU} \omIt x_t^{(i)} \exp\Bigparenth{-\normalbrackets{\TrueExternalFieldtI + 2\EstimatedParameterRowt\tp \svbx^{(i)}} x_t^{(i)}} \qtext{for every} u \in \numindsets. \label{eq_def_psi}
\end{align}
Now, we decompose $\directionalGradientExternalFieldTrue$ as a sum of the $\numindsets$ terms defined above. More precisely, we have
\begin{align}
    \directionalGradientExternalFieldTrue & \sequal{\cref{eq:first_dir_derivative_stage_2}} -\sump \omIt x_t^{(i)} \exp\Bigparenth{-\normalbrackets{\TrueExternalFieldtI + 2\EstimatedParameterRowt\tp \svbx^{(i)}} x_t^{(i)}} \\
    & \sequal{(a)} - \frac{1}{\numindsets'} \sum_{u \in [\numindsets]} \sum_{t \in \setU} \omIt x_t^{(i)} \exp\Bigparenth{-\normalbrackets{\TrueExternalFieldtI + 2\EstimatedParameterRowt\tp \svbx^{(i)}} x_t^{(i)}} \\
    & \sequal{\cref{eq_def_psi}} - \frac{1}{\numindsets'} \sum_{u \in [\numindsets]} \psi_u(\svbx^{(i)}; \omI) \label{eq_first_order_derivative_expressed_via_psi}
\end{align}
where $(a)$ follows because each $t \in [p]$ appears in exactly $\numindsets' = \lceil \numindsets/32\sqrt{2}\aGM\bGM \xmax^2 \rceil$ of the sets $\sets$ according to \cref{lemma_conditioning_trick}\cref{item:cardinality_independence_set} (with $\lambda = \frac{1}{4\sqrt{2}\xmax^2}$). Now, we focus on the $\numindsets$ terms in \cref{eq_first_order_derivative_expressed_via_psi}.\\

\noindent Consider any $u \in [\numindsets]$. We claim that conditioned on $\svbx_{-\setU}^{(i)}$ and $\svbz^{(i)}$, the expected value of $\psi_u(\svbx^{(i)}; \omI)$ can be upper bounded uniformly across all $u \in [\numindsets]$. We provide a proof at the end.

\newcommand{\upperboundpsi}{Upper bound on expected $\psi_u$}
\begin{lemma}[\tbf{\upperboundpsi}]\label{lemma_expected_psi_upper_bound}
Fix $\varepsilon_5 > 0$, $\delta_5 \in (0,1)$, $i \in [n]$ and $\ExternalFieldI \in \ParameterSet_{\ExternalField}$. Then, with $\omI$ defined in \cref{eq:omega_defn_stage_2}
%
and given $\svbz^{(i)}$ and $\svbx_{-\setU}^{(i)}$ for all $u \in [\numindsets]$, 
%
we have
\begin{align}
 \max\limits_{u \in [\numindsets]} \Expectation\Bigbrackets{\psi_u(\svbx^{(i)}; \omI) \bigm\vert \svbx_{-\setU}^{(i)}, \svbz^{(i)}} \leq \varepsilon_5 \sonenorm{\omI} \qtext{for} n \geq \dfrac{O(\exp(\bGM)) \log(4p/\delta_5)}{\varepsilon_5^4}
\end{align}
with probability at least $1-\delta_5$.
%
\end{lemma}

\noindent Consider again any $u \in [\numindsets]$. Now, we claim that conditioned on $\svbx_{-\setU}^{(i)}$ and $\svbz^{(i)}$, $\psi_u(\svbx^{(i)}; \omI)$ concentrates around its conditional expected value. We provide a proof at the end.
%
%
%
%

\newcommand{\concpsi}{Concentration of $\psi_u$}

%
%
%
%
\begin{lemma}[\tbf{\concpsi}]\label{lemma_concentration_psi}
Fix $\varepsilon_6 > 0$, $i \in [n]$, $u \in [\numindsets]$, and $\ExternalFieldI \in \ParameterSet_{\ExternalField}$. Then, with $\omI$ defined in \cref{eq:omega_defn_stage_2} and given $\svbz^{(i)}$ and $\svbx_{-\setU}^{(i)}$, we have
\begin{align}
    \Bigabs{\psi_u(\svbx^{(i)}; \omI) - \Expectation\bigbrackets{\psi_u(\svbx^{(i)}; \omI) \bigm\vert \svbx_{-\setU}^{(i)}, \svbz^{(i)}}} \leq \varepsilon_6,
\end{align}
with probability at least $1-\exp\biggparenth{ \dfrac{- \varepsilon_6^2}{\exp\bigparenth{O(\bGM)}\stwonorm{\omI}^2}}$. 
%
%
%
%
%
%
%
%
%
\end{lemma}

\noindent Given these lemmas, we proceed to show the concentration of $\directionalGradientExternalFieldTrue$.
%
To that end, for any $u \in [\numindsets]$, given $\svbx_{-\setU}^{(i)}$ and $\svbz^{(i)}$, let $E_u$ denote the event that
\begin{align}
\psi_u(\svbx^{(i)}; \omI) \leq   \Expectation\bigbrackets{\psi_u(\svbx^{(i)}; \omI) \vert \svbx_{-\setU}^{(i)}, \svbz^{(i)}} + \frac{1}{32\sqrt{2}\aGM\bGM \xmax^2} \varepsilon_3 \stwonorm{\omI}^2. \label{eq_event_Ej}
\end{align}
Since $E_u$ in an indicator event, using the law of total expectation results in
\begin{align}
    \Probability(E_u) = \Expectation\Bigbrackets{\Probability(E_u | \svbx_{-\setU}^{(i)}, \svbz^{(i)})} \sgreat{(a)} 1 - \exp\biggparenth{ \dfrac{- \varepsilon_3^2\stwonorm{\omI}^2}{\exp\bigparenth{O(\bGM)}}}.
\end{align}
where $(a)$ follows from \cref{lemma_concentration_psi} with $\varepsilon_6 = \dfrac{\varepsilon_3\stwonorm{\omI}^2}{32\sqrt{2}\aGM\bGM \xmax^2}$. Now, by applying the union bound over all $u \in [\numindsets]$ where $\numindsets = 1024  \aGM^2 \bGM^2 \xmax^4 \log 4p$, we have
\begin{align}
    %
    \Probability\Bigparenth{\bigcap_{u \in \numindsets} E_u} \geq 1 - O\biggparenth{\bGM^2 \log p \exp\biggparenth{\dfrac{-\varepsilon_3^2\stwonorm{\omI}^2}{\exp\bigparenth{O(\bGM)}}}}.
\end{align}
Now, assume the event $\cap_{u \in \numindsets} E_u$ holds. Whenever this holds, we also have
\begin{align}
    \bigabs{\directionalGradientExternalFieldTrue} & \sless{\cref{eq_first_order_derivative_expressed_via_psi}} \frac{1}{\numindsets'} \sum_{u \in [\numindsets]} \bigabs{\psi_u(\svbx^{(i)}; \omI)}\\ & \sless{\cref{eq_event_Ej}} \frac{1}{\numindsets'} \sum_{u \in [\numindsets]} \Bigabs{\Expectation\bigbrackets{\psi_u(\svbx^{(i)}; \omI) \vert \svbx_{-\setU}^{(i)}, \svbz^{(i)}} + \frac{1}{32\sqrt{2}\aGM\bGM \xmax^2} \varepsilon_3 \stwonorm{\omI}^2} \label{eq_grad_intermediate_bound}
\end{align}
where $\numindsets' = \lceil \numindsets/32\sqrt{2}\aGM\bGM \xmax^2 \rceil$. Further, using \cref{lemma_expected_psi_upper_bound} in \cref{eq_grad_intermediate_bound} with $\varepsilon_5 = \dfrac{\varepsilon_2}{32\sqrt{2}\aGM\bGM \xmax^2}$ and $\delta_5 = \delta_2$, whenever
\begin{align}
    %
    n \geq O\biggparenth{\dfrac{\exp\bigparenth{O(\bGM)} \log(p/\delta_2)}{\varepsilon_2^4}},
\end{align}
with probability at least $1-\delta_2$, we have,
\begin{align}
    \bigabs{\directionalGradientExternalFieldTrue} & \leq \frac{1}{\numindsets'} \sum_{u \in [\numindsets]} \Bigparenth{\frac{1}{32\sqrt{2}\aGM\bGM \xmax^2} \varepsilon_2 \sonenorm{\omI} + \frac{1}{32\sqrt{2}\aGM\bGM \xmax^2} \varepsilon_3 \stwonorm{\omI}^2} \\
    & = \frac{\numindsets}{32 \sqrt{2}\aGM\bGM \xmax^2 \numindsets'} \Bigparenth{\varepsilon_2 \sonenorm{\omI} + \varepsilon_3 \stwonorm{\omI}^2} \sless{(a)} \varepsilon_2 \sonenorm{\omI} + \varepsilon_3 \stwonorm{\omI}^2,
\end{align}
where $(a)$ follows because $\numindsets' = \lceil \numindsets/32 \sqrt{2}\aGM\bGM \xmax^2 \rceil$.

\paragraph{Proof of \cref{eq:first_dir_derivative_stage_2}: \expressionfirstderstagetwo:}
Fix any $i \in [n]$. The first-order partial derivatives of $\loss^{(i)}$ (defined in \cref{eq:loss_function_n}) with respect to the entries of the parameter vector $\ExternalFieldI$ are given by
\begin{align}
    \frac{\partial \loss^{(i)}(\ExternalFieldI)}{\partial \ExternalFieldIt} & = -x_t^{(i)} \exp\Bigparenth{-\normalbrackets{\ExternalFieldIt + 2\EstimatedParameterRowt\tp \svbx^{(i)}} x_t^{(i)}} \qtext{for all} t \in [p].
\end{align}
Now, we can write the first-order directional derivative of $\loss^{(i)}$ as
\begin{align}
    \directionalGradientExternalField &\defeq\lim_{h\to 0}\frac{\loss^{(i)}(\ExternalFieldI + h \omI)-\loss^{(i)}(\ExternalFieldI)}{h} = \sump \omIt \frac{\partial \loss^{(i)}(\ExternalFieldI)}{\partial \ExternalFieldIt} \\
    & = - \sump \omIt x_t^{(i)} \exp\Bigparenth{-\normalbrackets{\ExternalFieldIt + 2\EstimatedParameterRowt\tp \svbx^{(i)}} x_t^{(i)}}.
\end{align}




\paragraph{Proof of \cref{lemma_expected_psi_upper_bound}: \upperboundpsi:}
\label{sub:proof_of_lemma_expected_psi_upper_bound}
Fix any $i \in [n]$, $u \in [\numindsets]$, and $\ExternalFieldI \in \ParameterSet_{\ExternalField}$. Then, given $\svbx_{-\setU}^{(i)}$ and $\svbz^{(i)}$, we have
\begin{align}
    & \Expectation\biggbrackets{\psi_u(\svbx^{(i)}; \omI) \bigm\vert \svbx_{-\setU}^{(i)}, \svbz^{(i)}} \\
    &  \sequal{(a)} \Expectation\Bigbrackets{\sum_{t \in \setU} \omIt x_t^{(i)} \exp\bigparenth{-\normalbrackets{\TrueExternalFieldtI + 2\EstimatedParameterRowt\tp \svbx^{(i)}} x_t^{(i)}} \Bigm\vert \svbx_{-\setU}^{(i)}, \svbz^{(i)}} \\
    & \sequal{(b)} \sum_{t \in \setU} \omIt \Expectation\Bigbrackets{x_t^{(i)} \exp\bigparenth{-\normalbrackets{\TrueExternalFieldtI + 2\EstimatedParameterRowt\tp \svbx^{(i)}} x_t^{(i)}} \Bigm\vert \svbx_{-\setU}^{(i)}, \svbz^{(i)}} \\
    & \sequal{(c)} \sum_{t \in \setU} \omIt \Expectation\biggbrackets{\Expectation\Bigbrackets{x_t^{(i)} \exp\bigparenth{-\normalbrackets{\TrueExternalFieldtI + 2\EstimatedParameterRowt\tp \svbx^{(i)}} x_t^{(i)}} \Bigm\vert \svbx_{-t}^{(i)}, \svbz^{(i)}} \biggm\vert \svbx_{-\setU}^{(i)}, \svbz^{(i)}},
    %
    \label{eq_conditional_expectation_psi_intermediate}
\end{align}
where $(a)$ follows from the definition of $\psi_u(\svbx^{(i)}; \omI)$ in \cref{eq_def_psi}, $(b)$ follows from linearity of expectation, and $(c)$ follows from the law of total expectation, i.e., $\Expectation[\Expectation[Y|X,Z]|Z] = \Expectation[Y|Z]$ since $\svbx_{-\setU}^{(i)} \subseteq \svbx_{-t}^{(i)}$.
%

\noindent Now, for every $t \in \setU$, we will bound $\Expectation\Bigbrackets{x_t^{(i)} \exp\bigparenth{-\normalbrackets{\TrueExternalFieldtI + 2\EstimatedParameterRowt\tp \svbx^{(i)}} x_t^{(i)}, \svbz^{(i)}} \Bigm\vert \svbx_{-t}^{(i)}, \svbz^{(i)}}$. We have
\begin{align}
    & \Expectation\Bigbrackets{x_t^{(i)} \exp\bigparenth{-\normalbrackets{\TrueExternalFieldtI + 2\EstimatedParameterRowt\tp \svbx^{(i)}} x_t^{(i)}} \Bigm\vert \svbx_{-t}^{(i)}, \svbz^{(i)}} \\
    & = \int_{\cX} x_t^{(i)} \exp\bigparenth{-\normalbrackets{\TrueExternalFieldtI + 2\EstimatedParameterRowt\tp \svbx^{(i)}} x_t^{(i)}}  \TrueConditionalDistIt d x_t^{(i)}\\
    & \sequal{\cref{eq_conditional_dist}} \frac{\int_{\cX} x_t^{(i)} \exp\bigparenth{-\normalbrackets{\TrueExternalFieldtI + 2\EstimatedParameterRowt\tp \svbx^{(i)}} x_t^{(i)}} \exp\bigparenth{\normalbrackets{\TrueExternalFieldt(\svbz^{(i)}) + 2\TrueParameterRowtTop \svbx^{(i)}} x_t^{(i)}} dx_t^{(i)}} {\int_{\cX} \exp\bigparenth{\normalbrackets{\TrueExternalFieldt(\svbz^{(i)}) + 2\TrueParameterRowtTop \svbx^{(i)}} x_t^{(i)}}d x_t^{(i)}}\\
    & \sequal{(a)} \frac{\int_{\cX} x_t^{(i)}  \exp\bigparenth{2\normalbrackets{\TrueParameterRowt - \EstimatedParameterRowt}\tp \svbx^{(i)} x_t^{(i)}} dx_t^{(i)}} {\int_{\cX} \exp\bigparenth{\normalbrackets{\TrueExternalFieldt(\svbz^{(i)}) + 2\TrueParameterRowtTop \svbx^{(i)}} x_t^{(i)}}d x_t^{(i)}}\\
    & \sequal{(b)} \frac{\int_{\cX} x_t^{(i)} \Bigbrackets{ 1 + 2\bigparenth{\normalbrackets{\TrueParameterRowt - \EstimatedParameterRowt}\tp \svbx^{(i)} x_t^{(i)}} + 4\bigparenth{\normalbrackets{\TrueParameterRowt - \EstimatedParameterRowt}\tp \svbx^{(i)} x_t^{(i)}}^2 + o\bigparenth{\normalbrackets{\TrueParameterRowt - \EstimatedParameterRowt}\tp \svbx^{(i)} x_t^{(i)}}^3} dx_t^{(i)}} {\int_{\cX} \exp\bigparenth{\normalbrackets{\TrueExternalFieldt(\svbz^{(i)}) + 2\TrueParameterRowtTop \svbx^{(i)}} x_t^{(i)}}d x_t^{(i)}}\\
    %
    & \sequal{(c)} \frac{ 4\xmax^3 \normalbrackets{\TrueParameterRowt - \EstimatedParameterRowt}\tp \svbx^{(i)} }{3\int_{\cX} \exp\Bigparenth{\normalbrackets{\TrueExternalFieldt(\svbz^{(i)}) + 2\TrueParameterRowtTop \svbx^{(i)}} x_t^{(i)}}d x_t^{(i)}} + \frac{ \xmax^5 \bigparenth{\normalbrackets{\TrueParameterRowt - \EstimatedParameterRowt}\tp \svbx^{(i)}}^3 o(1) }{\int_{\cX} \exp\Bigparenth{\normalbrackets{\TrueExternalFieldt(\svbz^{(i)}) + 2\TrueParameterRowtTop \svbx^{(i)}} x_t^{(i)}}d x_t^{(i)}}, \label{eq_taylor_series}
    %
    %
    %
    %
\end{align}
where $(a)$ follows because $\TrueExternalFieldI = \TrueExternalField(\svbz^{(i)}) ~\forall i \in [n]$, $(b)$ follows by using the Taylor series expansion $\exp(y) = 1 + y + y^2 + o(y^3)$ around zero, $(c)$ follows because $\int_{\cX} x_t^{(i)} dx_t^{(i)} = 0$, $\int_{\cX} \bigparenth{ x_t^{(i)}}^2 d x_t^{(i)} = 2\xmax^3/3$, $\int_{\cX} \bigparenth{ x_t^{(i)}}^3 d x_t^{(i)} = 0$, and $\int_{\cX} \bigparenth{ x_t^{(i)}}^4 d x_t^{(i)} = 2\xmax^5/5$. 


Now, we bound the numerators in \cref{eq_taylor_series} by using $\sonenorm{\TrueParameterRowt - \EstimatedParameterRowt} \leq 2\bGM \sinfnorm{\TrueParameterRowt - \EstimatedParameterRowt} \leq 2\bGM \stwonorm{\TrueParameterRowt - \EstimatedParameterRowt}$ which follows from the order of norms on Euclidean space as well as  $\TrueParameterMatrix \in \ParameterSet_{\ParameterMatrix}$ and $\EstimatedParameterMatrix \in \ParameterSet_{\ParameterMatrix}$. Then, we invoke \cref{theorem_parameters} to bound $\stwonorm{\TrueParameterRowt - \EstimatedParameterRowt}$ by $\varepsilon = \dfrac{3\varepsilon_5}{4\bGM \ctwo \xmax^3}$. Therefore, we subsume the second term by the first term resulting in the following bound:


\begin{align}
      \Expectation\Bigbrackets{x_t^{(i)} \exp\bigparenth{-\normalbrackets{\TrueExternalFieldtI + 2\EstimatedParameterRowt\tp \svbx^{(i)}} x_t^{(i)}} \Bigm\vert \svbx_{-t}^{(i)}, \svbz^{(i)}} 
    %
    %
    & \leq \frac{4 \bGM \ctwo  \xmax^3 \stwonorm{\TrueParameterRowt - \EstimatedParameterRowt}}{3}, \label{eq_expected_psi_bound_two_norm}
\end{align}
where we have used the triangle inequality, $\sinfnorm{\svbx^{(i)}} \leq \xmax $ for all $ i \in [n]$ as well as $\sonenorm{\TrueParameterRowt - \EstimatedParameterRowt} \leq 2\bGM \stwonorm{\TrueParameterRowt - \EstimatedParameterRowt}$ to upper bound the numerator, and the arguments used in the proof of \cref{prop_lower_bound_variance} as well as $\int_{\cX} dx_t^{(i)} = 2\xmax$ to lower bound the denominator.
%
%
%
%
%
%
%
%
%
%
%
%
%
%

\noindent Using \cref{theorem_parameters} in \cref{eq_expected_psi_bound_two_norm} with $\varepsilon = \dfrac{3\varepsilon_5}{4\bGM \ctwo \xmax^3}$ and $\delta = \delta_5$, we have
\begin{align}
    \Expectation\Bigbrackets{x_t^{(i)} \exp\Bigparenth{-\normalbrackets{\TrueExternalFieldtI + 2\EstimatedParameterRowt\tp \svbx^{(i)}} x_t^{(i)}} \Bigm\vert \svbx_{-t}^{(i)}, \svbz^{(i)}} \leq \varepsilon_5, \label{eq_upper_bound_intermediate}
\end{align}
with probability at least $1-\delta_5$ as long as
\begin{align}
    %
    n \geq \dfrac{O(\exp(\bGM)) \log(4p/\delta_5)}{\varepsilon_5^4}. \label{eq_n_bound_stability}
\end{align}
Using \cref{eq_upper_bound_intermediate} and triangle inequality in \cref{eq_conditional_expectation_psi_intermediate}, we have
\begin{align}
    \Expectation\Bigbrackets{\psi_u(\svbx^{(i)}; \omI)\bigm\vert \svbx_{-\setU}^{(i)}, \svbz^{(i)}} & \leq \varepsilon_5 \sum_{t \in \setU} \bigabs{\omIt} \leq \varepsilon_5 \sonenorm{\omI},
\end{align}
with probability at least $1-\delta_5$ as long as $n$ satisfies \cref{eq_n_bound_stability}.

\paragraph{Proof of \cref{lemma_concentration_psi}: \concpsi:}
\label{sub:proof_of_lemma_concentration_psi}
To show this concentration result, we use \cref{coro}~\cref{eq_coro_combined} for the function $q_2$. To that end, we note that the pair $\braces{\rvbx, \rvbz}$ corresponds to a $\tSGM$ (\cref{def:tau_sgm}) with $\dGM \defn (\aGM, \aGM \bGM, \xmax, \ParameterMatrix)$. However, the random vector $\rvbx$ conditioned on $\rvbz$ need not satisfy the Dobrushin's uniqueness condition (\cref{def_dobrushin_condition}). Therefore, we cannot apply \cref{coro}~\cref{eq_coro_combined} as is. To resolve this, we resort to \cref{lemma_conditioning_trick} with $\lambda = \frac{1}{4\sqrt{2}\xmax^2}$ to reduce the random vector $\rvbx$ conditioned on $\rvbz$ to Dobrushin's regime.

Fix any $u \in [\numindsets]$. Then, from \cref{lemma_conditioning_trick}\cref{item:conditional_sgm_independence_set}, (i) the pair of random vectors $\braces{\rvbx_{\setU}, (\rvbx_{-\setU}, \rvbz)}$ corresponds to a $\tSGM[1]$ with $\dGM_1 \defn (\aGM+2\aGM\bGM\xmax, \frac{1}{4\sqrt{2}\xmax^2}, \xmax, \ParameterMatrix_{\setU})$, and (ii) the random vector $\rvbx_{\setU}$ conditioned on $(\rvbx_{-\setU}, \rvbz)$ satisfies the Dobrushin's uniqueness condition (\cref{def_dobrushin_condition}) with coupling matrix $2\sqrt{2} \xmax^2 \ParameterMatrix_{\setU}$ with $2\sqrt{2} \xmax^2 \opnorm{\ParameterMatrix_{\setU}} \leq 2\sqrt{2} \xmax^2 \lambda \leq 1/2$. Now, for any fixed $i \in [n]$, 
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
we apply \cref{coro}~\cref{eq_coro_combined} for the function $q_2$ with $\varepsilon = \varepsilon_6$ for a given $\svbx_{-\setU}^{(i)}$ and $\svbz^{(i)}$, to obtain
\begin{align}
    \Probability\biggparenth{\Bigabs{\psi_u(\svbx^{(i)}; \omI) - \Expectation\Bigbrackets{\psi_u(\svbx^{(i)}; \omI) \Bigm\vert \svbx_{-\setU}^{(i)}, \rvbz}} \geq \varepsilon_6 \Bigm\vert \svbx_{-\setU}^{(i)}, \rvbz} \leq \exp\biggparenth{ \dfrac{- \varepsilon_6^2}{\exp\bigparenth{O(\bGM)}\stwonorm{\omI}^2}}.
\end{align}

\subsubsection{Proof of \cref{lemma_conc_first_sec_der_stage_two}~\cref{item_conc_sec_der_stage_two}: \anticoncgradstagetwo}
\label{sub:proof_of_lemma_lower_bound_sec_der_stage_2}

\newcommand{\expressionsecderstagetwo}{Expression for second directional derivative}
Fix some $i \in [n]$ and some $\ExternalFieldI \in \ParameterSet_{\ExternalField}$. Let $\omI$ defined in \cref{eq:omega_defn_stage_2}. We claim that the second-order directional derivative of $\loss^{(i)}$ defined in \cref{eq:loss_function_n} is given by 
\begin{align}
    \directionalHessianExternalField = \sump \bigparenth{\omIt x_t^{(i)}}^2  \exp\Bigparenth{-\normalbrackets{\ExternalFieldIt + 2\EstimatedParameterRowt\tp \svbx^{(i)}} x_t^{(i)}}. \label{eq:second_dir_derivative_stage_2}
\end{align}
We provide a proof at the end. For now, we assume the claim and proceed.
%
%
%
%
%
%
%
%
%
%
Now, we lower bound $\directionalHessianExternalField$ by a quadratic form as follows:
\begin{align}
     \directionalHessianExternalField 
    %
     & \sgreat{(a)} \sump   \bigparenth{\omIt x_t^{(i)}}^2 \times \exp\Bigparenth{-\normalabs{\ExternalFieldtI + 2\EstimatedParameterRowt\tp \svbx^{(i)}} \xmax}  \\
     & \sgreat{(b)} \sump   \bigparenth{\omIt x_t^{(i)}}^2 \times \exp\Bigparenth{-\bigparenth{\normalabs{\ExternalFieldtI} + 2\sonenorm{\EstimatedParameterRowt} \sinfnorm{\svbx^{(i)}}} \xmax}\\
     &  \sgreat{(c)} \sump  \bigparenth{\omIt x_t^{(i)}}^2 \times \exp\Bigparenth{-\normalparenth{\aGM + 2\aGM \bGM \xmax} \xmax} \sequal{\cref{eq:constants}} \frac{1}{\ctwo} \sump  \bigparenth{\omIt x_t^{(i)}}^2 \label{eq_lower_bound_sec_der_stage_2_cont},
    %
    %
\end{align}
where $(a)$ follows from \cref{eq:second_dir_derivative_stage_2} because $\sinfnorm{\svbx^{(i)}} \leq \xmax $ for all $ i \in [n]$, $(b)$ follows by triangle inequality and Cauchy–Schwarz inequality, and $(c)$ follows because $\EstimatedParameterMatrix \in \ParameterSet_{\ParameterMatrix}$, $\ExternalFieldI \in \ParameterSet_{\ExternalField}$, and $\sinfnorm{\svbx^{(i)}} \leq \xmax $ for all $ i \in [n]$.\\
%


\noindent Now, to show the anti-concentration of $\directionalHessianExternalField$, we show the anti-concentration of the quadratic form in \cref{eq_lower_bound_sec_der_stage_2_cont}. To that end, we note that the pair $\braces{\rvbx, \rvbz}$ corresponds to a $\tSGM$ (\cref{def:tau_sgm}) with $\dGM \defn (\aGM, \aGM \bGM, \xmax, \ParameterMatrix)$. Then, we decompose the quadratic form in \cref{eq_lower_bound_sec_der_stage_2_cont} as a sum of  $\numindsets = 1024  \aGM^2 \bGM^2 \xmax^4 \log 4p$ terms using \cref{lemma_conditioning_trick} (see \cref{sec_conditioning_trick}) with $\lambda = \frac{1}{4\sqrt{2}\xmax^2}$ and focus on these  $\numindsets$ terms. Consider the $\numindsets$ subsets $\sets \in [p]$ obtained from \cref{lemma_conditioning_trick} and define
\begin{align}
    \bpsi_u(\svbx^{(i)}; \omI) \defeq \sum_{t \in \setU} \bigparenth{\omIt x_t^{(i)}}^2 \qtext{for every} u \in \numindsets. \label{eq_def_bar_psi}
\end{align}
Then, we have
\begin{align}
    \sump \bigparenth{\omIt x_t^{(i)}}^2 & \sequal{(a)} \frac{1}{\numindsets'} \sum_{u \in [\numindsets]} \sum_{t \in \setU} \bigparenth{\omIt x_t^{(i)}}^2 \sequal{\cref{eq_def_bar_psi}}  \frac{1}{\numindsets'} \sum_{u \in [\numindsets]} \bpsi_u(\svbx^{(i)}; \omI) \label{eq_second_order_derivative_expressed_via_psi}
\end{align}
where $(a)$ follows because each $t \in [p]$ appears in exactly $\numindsets' = \lceil \numindsets/32\sqrt{2}\aGM\bGM \xmax^2 \rceil$ of the sets $\sets$ according to \cref{lemma_conditioning_trick}\cref{item:cardinality_independence_set} (with $\lambda = \frac{1}{4\sqrt{2}\xmax^2}$). Now, we focus on the $\numindsets$ terms in \cref{eq_second_order_derivative_expressed_via_psi}.\\

\noindent Consider any $u \in [\numindsets]$. We claim that conditioned on $\svbx_{-\setU}^{(i)}$ and $\svbz^{(i)}$, the expected value of $\bpsi_u(\svbx^{(i)}; \omI)$ can be upper bounded uniformly across all $u \in [\numindsets]$. We provide a proof at the end.

%


\newcommand{\lowerboundbarpsi}{Lower bound on expected $\bpsi_u$}

\begin{lemma}[\tbf{\lowerboundbarpsi}]\label{lemma_expected_psi_lower_bound}
Fix $i \in [n]$ and $\ExternalFieldI \in \ParameterSet_{\ExternalField}$. Then, with $\omI$ defined in \cref{eq:omega_defn_stage_2} and given $\svbz^{(i)}$ and $\svbx_{-\setU}^{(i)}$, we have
%
%
given $\svbx_{-\setU}^{(i)}$ and $\svbz^{(i)}$, 
%
%
\begin{align}
     \min\limits_{u \in [\numindsets]} \Expectation\Bigbrackets{\bpsi_u(\svbx^{(i)}; \omI) \bigm\vert \svbx_{-\setU}^{(i)}, \svbz^{(i)}} \geq \frac{\xmax}{\pi e \ctwo[2]} \stwonorm{\omI}^2,
\end{align}
where the constant $\ctwo$ was defined in \cref{eq:constants}.
\end{lemma}

\noindent Consider again any $u \in [\numindsets]$. Now, we claim that conditioned on $\svbx_{-\setU}^{(i)}$ and $\svbz^{(i)}$, $\bpsi_u(\svbx^{(i)}; \omI)$ concentrates around its conditional expected value. We provide a proof at the end.
%
%
%
%
%

\newcommand{\concbarpsi}{Concentration of $\bpsi_u$}

\begin{lemma}[\tbf{\concbarpsi}]\label{lemma_concentration_bar_psi}
Fix $\varepsilon_7 > 0$, $i \in [n]$, $u \in [\numindsets]$, and $\ExternalFieldI \in \ParameterSet_{\ExternalField}$.
Then, with $\omI$ defined in \cref{eq:omega_defn_stage_2} and given $\svbz^{(i)}$ and $\svbx_{-\setU}^{(i)}$, we have
\begin{align}
    \Bigabs{\bpsi_u(\svbx^{(i)}; \omI) - \Expectation\bigbrackets{\bpsi_u(\svbx^{(i)}; \omI) \bigm\vert \svbx_{-\setU}^{(i)}, \svbz^{(i)}}} \leq \varepsilon_7,
\end{align}
with probability at least $1-\exp\biggparenth{ \dfrac{- \varepsilon_7^2}{\exp\bigparenth{O(\bGM)}\stwonorm{\omI}^2}}$. 
%
\end{lemma}

\noindent Given these lemmas, we proceed to show the anti-concentration of the quadratic form in  \cref{eq_lower_bound_sec_der_stage_2_cont} implying the anti-concentration of $\directionalHessianExternalField$. To that end, for any $u \in [\numindsets]$, given $\svbx_{-\setU}^{(i)}$ and $\svbz^{(i)}$, let $E_u$ denote the event that
\begin{align}
\bpsi_u(\svbx^{(i)}; \omI) \geq   \Expectation\bigbrackets{\bpsi_u(\svbx^{(i)}; \omI) \vert \svbx_{-\setU}^{(i)}, \svbz^{(i)}} - \frac{\xmax}{2\pi e \ctwo[2]} \stwonorm{\omI}^2. \label{eq_event_Ej_Hess}
\end{align}
Since $E_u$ in an indicator event, using the law of total expectation results in
\begin{align}
    \Probability(E_u) = \Expectation\Bigbrackets{\Probability(E_u | \svbx_{-\setU}^{(i)}, \svbz^{(i)})} \sgreat{(a)} 1 - \exp\biggparenth{ \dfrac{ \stwonorm{\omI}^2}{\exp\bigparenth{O(\bGM)}}},
\end{align}
where $(a)$ follows from \cref{lemma_concentration_bar_psi} with $\varepsilon_7 = \dfrac{\xmax}{2\pi e \ctwo[2]} \stwonorm{\omI}^2$. Now, by applying the union bound over all $u \in [\numindsets]$ where $\numindsets = 1024  \aGM^2 \bGM^2 \xmax^4 \log 4p$, we have
\begin{align}
    %
    \Probability\Bigparenth{\bigcap_{u \in \numindsets} E_u} \geq 1 - O\biggparenth{\bGM^2 \log p \exp\biggparenth{\dfrac{\stwonorm{\omI}^2}{\exp\bigparenth{O(\bGM)}}}}.
\end{align}
Now, assume the event $\cap_{u \in \numindsets} E_u$ holds. Whenever this holds, we also have
\begin{align}
    \sump \bigparenth{\omIt x_t^{(i)}}^2  \sequal{\cref{eq_second_order_derivative_expressed_via_psi}}  \frac{1}{\numindsets'} \sum_{u \in [\numindsets]} \bpsi_u(\svbx^{(i)}; \omI) & \sgreat{\cref{eq_event_Ej_Hess}} \frac{1}{\numindsets'} \sum_{u \in [\numindsets]}  \biggparenth{\Expectation\bigbrackets{\bpsi_u(\svbx^{(i)}; \omI) \vert \svbx_{-\setU}^{(i)}, \svbz^{(i)}} - \frac{\xmax}{2\pi e \ctwo[2]} \stwonorm{\omI}^2} \\
    & \sgreat{(a)} \frac{1}{\numindsets'} \sum_{u \in [\numindsets]} \frac{\xmax}{2\pi e \ctwo[2]} \stwonorm{\omI}^2 = \frac{\xmax\numindsets}{2\pi e \numindsets'\ctwo[2]} \stwonorm{\omI}^2 \label{eq_hess_intermediate_bound}
\end{align}
where $\numindsets' = \lceil \numindsets/32\sqrt{2}\aGM\bGM \xmax^2 \rceil$ and $(a)$ follows from \cref{lemma_expected_psi_lower_bound}. Finally, approximating $\numindsets' = \numindsets/32\sqrt{2}\aGM\bGM \xmax^2$ and using \cref{eq_lower_bound_sec_der_stage_2_cont}, we have
\begin{align}
    \directionalHessianExternalField \geq \frac{1}{\ctwo} \sump  \bigparenth{\omIt x_t^{(i)}}^2  \sgreat{\cref{eq_hess_intermediate_bound}} \frac{16\sqrt{2}\aGM\bGM \xmax^3}{\pi e \ctwo[3]} \stwonorm{\omI}^2,
\end{align}
which completes the proof.


\paragraph{Proof of \cref{eq:second_dir_derivative_stage_2}: \expressionsecderstagetwo:}
Fix any $i \in [n]$. The second-order partial derivatives of $\loss^{(i)}$ (defined in \cref{eq:loss_function_n}) with respect to the entries of the parameter vector $\ExternalFieldI$ are given by
\begin{align}
    \frac{\partial^2 \loss^{(i)}(\ExternalFieldI)}{\partial \bigbrackets{\ExternalFieldIt}^2}
    & = \bigbrackets{x_t^{(i)}}^2 \exp\Bigparenth{-\normalbrackets{\ExternalFieldIt + 2\EstimatedParameterRowt\tp \svbx^{(i)}} x_t^{(i)}}
  \qtext{for all} t \in [p].
\end{align}
Now, we can write the second-order directional derivative of $\loss^{(i)}$ as
\begin{align}
    \directionalHessianExternalField &\defeq 
     \lim_{h\to 0}\frac{\partial_{\omI}\loss^{(i)}(\ExternalFieldI+h \omI)\!-\!\partial_{\omI}\loss^{(i)}(\ExternalFieldI)}{h}
      = \sump \bigbrackets{\omIt}^2 \frac{\partial^2 \loss^{(i)}(\ExternalFieldI)}{\partial \bigbrackets{\ExternalFieldIt}^2} \\
    & = \sump \bigparenth{\omIt x_t^{(i)}}^2  \exp\Bigparenth{-\normalbrackets{\ExternalFieldIt + 2\EstimatedParameterRowt\tp \svbx^{(i)}} x_t^{(i)}}.
\end{align}

\paragraph{Proof of \cref{lemma_expected_psi_lower_bound}: \lowerboundbarpsi:}
Fix any $i \in [n]$, $u \in [\numindsets]$, and $\ExternalFieldI \in \ParameterSet_{\ExternalField}$. Then, given $\svbx_{-\setU}^{(i)}$ and $\svbz^{(i)}$, we have
\begin{align}
\Expectation\Bigbrackets{\bpsi_u(\svbx^{(i)}; \omI) \bigm\vert \svbx_{-\setU}^{(i)}, \svbz^{(i)}} & \sequal{\cref{eq_def_bar_psi}} \Expectation\Bigbrackets{\sum_{t \in \setU}  \bigparenth{\omIt x_t^{(i)}}^2 \bigm\vert \svbx_{-\setU}^{(i)}, \svbz^{(i)}} \\
& \sequal{(a)} \sum_{t \in \setU}  \Expectation \Bigbrackets{\bigparenth{\omIt x_t^{(i)}}^2 \bigm\vert \svbx_{-\setU}^{(i)}, \svbz^{(i)}}\\
& \sequal{(b)} \sum_{t \in \setU}  \Expectation \Bigbrackets{\Expectation \Bigbrackets{\bigparenth{\omIt x_t^{(i)}}^2 \Big \vert \svbx_{-t}^{(i)}, \svbz^{(i)}} \Bigm\vert \svbx_{-\setU}^{(i)}, \svbz^{(i)}} \\
& \sgreat{(c)} \sum_{t \in \setU}  \Expectation \Bigbrackets{\Variance\Bigparenth{\omIt x_t^{(i)} \Big \vert \svbx_{-t}^{(i)}, \svbz^{(i)}} \Bigm\vert \svbx_{-\setU}^{(i)}, \svbz^{(i)}} \sgreat{(d)} \frac{\xmax}{\pi e \ctwo[2]} \stwonorm{\omI}^2,
\end{align}
where $(a)$ follows from linearity of expectation, $(b)$ follows from the law of total expectation i.e., $\Expectation[\Expectation[Y|X,Z]|Z] = \Expectation[Y|Z]$ since $\svbx_{-\setU}^{(i)} \subseteq \svbx_{-t}^{(i)}$, $(c)$ follows follows from the fact that for any random variable a, $\Expectation\normalbrackets{a^2} \geq \Variance\normalbrackets{a}$, and $(d)$ follows from \cref{prop_lower_bound_variance}.


\paragraph{Proof of \cref{lemma_concentration_bar_psi}: \concbarpsi:}
\label{sub:proof_of_lemma_concentration_bar_psi}
To show this concentration result, we use \cref{coro}~\cref{eq_coro_combined} for the function $q_1$. To that end, we note that the pair $\braces{\rvbx, \rvbz}$ corresponds to a $\tSGM$ (\cref{def:tau_sgm}) with $\dGM \defn (\aGM, \aGM \bGM, \xmax, \ParameterMatrix)$. However, the random vector $\rvbx$ conditioned on $\rvbz$ need not satisfy the Dobrushin's uniqueness condition (\cref{def_dobrushin_condition}). Therefore, we cannot apply \cref{coro}~\cref{eq_coro_combined} as is. To resolve this, we resort to \cref{lemma_conditioning_trick} with $\lambda = \frac{1}{4\sqrt{2}\xmax^2}$ to reduce the random vector $\rvbx$ conditioned on $\rvbz$ to Dobrushin's regime.

Fix any $u \in [\numindsets]$. Then, from \cref{lemma_conditioning_trick}\cref{item:conditional_sgm_independence_set}, (i) the pair of random vectors $\braces{\rvbx_{\setU}, (\rvbx_{-\setU}, \rvbz)}$ corresponds to a $\tSGM[1]$ with $\dGM_1 \defn (\aGM+2\aGM\bGM\xmax, \frac{1}{4\sqrt{2}\xmax^2}, \xmax, \ParameterMatrix_{\setU})$, and (ii) the random vector $\rvbx_{\setU}$ conditioned on $(\rvbx_{-\setU}, \rvbz)$ satisfies the Dobrushin's uniqueness condition (\cref{def_dobrushin_condition}) with coupling matrix $2\sqrt{2} \xmax^2 \ParameterMatrix_{\setU}$ with $2\sqrt{2} \xmax^2 \opnorm{\ParameterMatrix_{\setU}} \leq 2\sqrt{2} \xmax^2 \lambda \leq 1/2$. Now, for any fixed $i \in [n]$, 
%
%
%
we apply \cref{coro}~\cref{eq_coro_combined} for the function $q_1$ with $\varepsilon = \varepsilon_7$ for a given $\svbx_{-\setU}^{(i)}$ and $\svbz^{(i)}$, to obtain
%
\begin{align}
    \Probability\biggparenth{\Bigabs{\bpsi_u(\svbx^{(i)}; \omI) - \Expectation\Bigbrackets{\bpsi_u(\svbx^{(i)}; \omI) \Bigm\vert \svbx_{-\setU}^{(i)}, \svbz^{(i)}}} \geq \varepsilon_7 \Bigm\vert \svbx_{-\setU}^{(i)}, \svbz^{(i)}} \leq \exp\biggparenth{ \dfrac{- \varepsilon_7^2}{\exp\bigparenth{O(\bGM)}\stwonorm{\omI}^2}}.
\end{align}


\subsection{Proof of \cref{lemma_lipschitzness}: \lipschitznesslossfunction}
\label{sub:proof_lemma_lipschitzness}
Fix any $i \in [n]$, any $\ExternalFieldI, \tExternalFieldI \in \ParameterSet_{\ExternalField}$. Consider the direction $\omI = \tExternalFieldI - \ExternalFieldI$, and define the function $q : [0,1] \to \Reals$ as follows:
\begin{align}
    q(a) = \loss^{(i)}\bigparenth{\ExternalFieldI + a(\tExternalFieldI - \ExternalFieldI)}. \label{eq_func_f_lipschitz}
\end{align}
%
Then, the desired inequality in \cref{eq_lipschitz_property} is equivalent to $\normalabs{q(1) - q(0)} \leq \xmax \ctwo \sonenorm{\omI}$.
%
%
%
From the mean value theorem, there exists $a' \in (0,1)$ such that
\begin{align}
    \normalabs{q(1) - q(0)} 
    %
    = \biggabs{\dfrac{dq(a')}{da}}.  \label{eq_mvt_lipschitz}
\end{align}
Therefore, we have
\begin{align}
\bigabs{q(1) - q(0)} & \sequal{\cref{eq_mvt_lipschitz}} \biggabs{\dfrac{dq(a')}{da}} \\
& \sequal{\cref{eq_func_f_lipschitz}} \Bigabs{\dfrac{d\loss^{(i)}\bigparenth{\ExternalFieldI + a'(\tExternalFieldI - \ExternalFieldI)}}{da}} \\
& \sequal{\cref{eq_der_mapping_external_field}} \Bigabs{\directionalGradientExternalField\bigr|_{\ExternalFieldI = \ExternalFieldI + a'(\tExternalFieldI - \ExternalFieldI)}}\\
& \sequal{\cref{eq:first_dir_derivative_stage_2}} \Bigabs{\sump \omtI x_t^{(i)} \exp\Bigparenth{-\normalbrackets{\ExternalFieldtI + a'(\tExternalFieldtI - \ExternalFieldtI) + 2\EstimatedParameterRowt\tp \svbx^{(i)}} x_t^{(i)}}}\\
%
%
& \sless{(a)} \xmax \sump \bigabs{\omIt}  \exp\Bigparenth{\Bigbrackets{\bigabs{(1-a')\ExternalFieldtI} + \bigabs{a' \tExternalFieldtI} + 2\sonenorm{\EstimatedParameterRowt} \sinfnorm{\svbx^{(i)}}} \xmax}\\
& \sless{(b)} \xmax \exp\Bigparenth{\bigparenth{(1-a') \aGM + a' \aGM + 2\aGM \bGM\xmax} \xmax} \sump \bigabs{\omIt}  \sequal{\cref{eq:constants}} \xmax \ctwo \sonenorm{\omI},
\end{align}
where $(a)$ follows from triangle inequality, Cauchy–Schwarz inequality, and because $\sinfnorm{\svbx^{(i)}} \leq \xmax $ for all $ i \in [n]$
%
and $(b)$ follows because $\ExternalFieldI, \tExternalFieldI \in \ParameterSet_{\ExternalField}$, $\EstimatedParameterMatrix \in \ParameterSet_{\ParameterMatrix}$, and $\sinfnorm{\svbx^{(i)}} \leq \xmax $ for all $ i \in [n]$.

\section{Proof of \cref{thm_causal_estimand}: \outcomemainresultname}
\label{sec_proof_causal_estimand}
%
\newcommand{\hpsi}{\what{\psi}}
\newcommand{\tpsi}{\wtil{\psi}}
\newcommand{\spsi}{\psi^{\star}}
\newcommand{\hPsi}{\what{\Psi}}
\newcommand{\tPsi}{\wtil{\Psi}}
\newcommand{\sPsi}{\Psi^{\star}}
Fix any unit $i \in [n]$ and an alternate intervention $\wtil{\svba}^{(i)} \in \cA^{p_a}$. Then, we have
\begin{align}
    \mu^{(i)}(\wtil{\svba}^{(i)}) & \sequal{\cref{eq_causal_estimand}} \Expectation[\svby^{(i)}(\wtil{\svba}^{(i)}) | \rvbz = \svbz^{(i)}, \rvbv = \svbv^{(i)}] \sequal{(a)} \Expectation[\rvby | \rvba = \wtil{\svba}^{(i)}, \rvbz = \svbz^{(i)}, \rvbv = \svbv^{(i)}],
\end{align}
where $(a)$ follows because the unit-level counterfactual distribution is equivalent to unit-level conditional distribution under the causal framework considered as described in \cref{subsec_po_framework}. To obtain a convenient expression for $\Expectation[\rvby | \rvba = \wtil{\svba}^{(i)}, \rvbz = \svbz^{(i)}, \rvbv = \svbv^{(i)}]$, we identify $\TruePhi[u,y] \in \Reals^{p_u \times p_y}$ to be the component of $\TrueParameterMatrix$ corresponding to $\rvbu$ and $\rvby$ for all $\rvbu \in \{\rvbv, \rvba, \rvby\}$ and $\TrueExternalFieldI[i,y] \in \Reals^{p_y}$ to be the component of $\TrueExternalFieldI$ corresponding to $\rvby$. Then, the conditional distribution of $\rvby$ as a function of the interventions $\rvba$, while keeping $\rvbv$ and $\rvbz$ fixed at the corresponding realizations for unit $i$, i.e., $\svbv^{(i)}$ and $\svbz^{(i)}$ respectively, can be written as
\begin{align}
    f^{(i)}_{\rvby | \rvba}(\svby | \svba) \propto \exp\Bigparenth{\bigbrackets{\TrueExternalFieldI[i,y] + 2\svbv^{(i)\top}\TruePhi[v,y] + 2\svba\tp\TruePhi[a,y]} \svby + \svby\tp\TruePhi[y,y] \svby}. \label{eq_conditional_distribution_y_alternate}
\end{align}
Therefore, we have
\begin{align}
    \Expectation[\rvby | \rvba = \wtil{\svba}^{(i)}, \rvbz = \svbz^{(i)}, \rvbv = \svbv^{(i)}] = \Expectation_{f^{(i)}_{\rvby | \rvba}}[\rvby | \rvba = \wtil{\svba}^{(i)}].
\end{align}
\noindent Now, consider the $p_u$ dimensional random vector $\rvbu$ supported on $\cX^{p_u}$ with distribution $f_{\rvbu}$ parameterized by   $\psi \in \Reals^{p_y}$ and $\Psi \in \Reals^{p_y \times p_y}$ as follows
\begin{align}
    f_{\rvbu}(\svbu | \psi, \Psi) \propto \exp(\psi\tp \svbu+ \svbu\tp\Psi\svbu). \label{eq_dist_u} 
\end{align}
Then, note that $\what{f}^{(i)}_{\rvby | \rvba}(\svby | \svba)$ in \cref{eq_counterfactual_distribution_y} and $f^{(i)}_{\rvby | \rvba}(\svby | \svba)$ in \cref{eq_conditional_distribution_y_alternate} belong to the set $\normalbraces{f_{\rvbu}(\cdot | \psi, \Psi): \psi \in \Reals^{p_y}, \Psi \in \Reals^{p_y \times p_y}}$ for some $\psi$ and $\Psi$. Now, we consider any two distributions in this set, namely $f_{\rvbu}(\svbu | \hpsi, \hPsi)$ and $f_{\rvbu}(\svbu | \spsi, \sPsi)$. Then, we claim that the two norm of the difference of the mean vectors of these distributions is bounded as below. We provide a proof at the end.
\newcommand{\expfamperturbationresultname}{Perturbation in the mean vector}
\begin{lemma}[\tbf{\expfamperturbationresultname}]
\label{lemma_exp_fam_parameter_perturbation}
For any $\psi \in \Reals^{p_y}$ and $\Psi \in \Reals^{p_y \times p_y}$, let $\mu_{\psi, \Psi}(\rvbu) \in \Reals^{p_u}$ and $\Covariance_{\psi, \Psi}(\rvbu, \rvbu) \in \Reals^{p_u \times p_u}$ denote the mean vector and the covariance matrix of $\rvbu$ respectively with respect to $f_{\rvbu}$ in \cref{eq_dist_u}. Then, for any $\hpsi, \spsi \in \Reals^{p_y}$ and $\hPsi, \sPsi \in \Reals^{p_y \times p_y}$, there exists some $t \in (0,1)$, $\tpsi \defn t \hpsi + (1-t) \spsi$ and $\tPsi \defn t \tpsi + (1-t) \tpsi$ such that
\begin{align}
    \stwonorm{ \mu_{\hpsi, \hPsi}(\rvbu) - \mu_{\spsi, \sPsi}(\rvbu)} \leq  \opnorm{\Covariance_{\tpsi, \tPsi}(\rvbu, \rvbu)}\stwonorm{(\hpsi \!-\! \spsi)} \!+\!\! \sum_{t_3 \in [p]} \! \opnorm{\Covariance_{\tpsi, \tPsi}(\rvbu, \rvu_{t_3} \rvbu)}\stwonorm{(\hPsi_{t_3} \!-\! \sPsi_{t_3})}.
\end{align}
\end{lemma}
%
%
%
%
%
%
\noindent Given this lemma, we proceed with the proof. By applying this lemma to $\what{f}^{(i)}_{\rvby | \rvba}(\svby | \svba)$ in \cref{eq_counterfactual_distribution_y} and $f^{(i)}_{\rvby | \rvba}(\svby | \svba)$ in \cref{eq_conditional_distribution_y_alternate}, we see that it is sufficient to show the following bound
\begin{align}
     & \stwonorm{\normalparenth{\TrueExternalFieldI[i,y] - \EstimatedExternalFieldI[i,y]} + 2\svbv^{(i)\top}\!\normalparenth{\TruePhi[v,y] - \EstimatedPhi^{(v,y)} } + 2\wtil{\svba}^{(i)\top}\!\normalparenth{\TruePhi[a,y] - \EstimatedPhi^{(a,y)}}} + \sum_{t \in [p_y]} \stwonorm{\TruePhi[y,y]_{t} - \EstimatedPhi^{(y,y)}_t} \\
     & \qquad \qquad \qquad\qquad \qquad \qquad \qquad \qquad\qquad \qquad \qquad \qquad \qquad\qquad \qquad \qquad \qquad \leq R(\varepsilon, \delta/n) + p \varepsilon.
\end{align}
To that end, we have
\begin{align}
    \sum_{t \in [p_y]} \stwonorm{\TruePhi[y,y]_{t} - \EstimatedPhi^{(y,y)}_t} \sless{(a)} \sum_{t \in [p_y]} \stwonorm{\TrueParameterRowt -\EstimatedParameterRowt},
    %
    \label{eq_mean_outcome_part_1}
\end{align}
where $(a)$ follows because $\ell_2$ norm of any sub-vector is no more than $\ell_2$ norm of the vector.
%
Similarly, we have
\begin{align}
    %
    %
    %
    & \stwonorm{\normalparenth{\TrueExternalFieldI[i,y] - \EstimatedExternalFieldI[i,y]} + 2\svbv^{(i)\top}\normalparenth{\TruePhi[v,y] -\EstimatedPhi^{(v,y)} } + 2\wtil{\svba}^{(i)\top}\normalparenth{\TruePhi[a,y] - \EstimatedPhi^{(a,y)}}}\\
    & \sless{(a)}  \stwonorm{\TrueExternalFieldI[i,y] - \EstimatedExternalFieldI[i,y]} + 2\stwonorm{\svbv^{(i)\top}\normalparenth{\TruePhi[v,y] -\EstimatedPhi^{(v,y)}}} + 2\stwonorm{\wtil{\svba}^{(i)\top}\normalparenth{\TruePhi[a,y] - \EstimatedPhi^{(a,y)}}}\\
    & \sless{(b)}  \stwonorm{\TrueExternalFieldI[i,y] - \EstimatedExternalFieldI[i,y]} + 2\stwonorm{\svbv^{(i)}}\opnorm{\TruePhi[v,y] -\EstimatedPhi^{(v,y)}} + 2\stwonorm{\wtil{\svba}^{(i)}}\opnorm{\normalparenth{\TruePhi[a,y] - \EstimatedPhi^{(a,y)}}}\\
    & \sless{(c)}  \stwonorm{\TrueExternalFieldI[i] - \EstimatedExternalFieldI[i]} + 2\Bigparenth{\stwonorm{\svbv^{(i)}} + \stwonorm{\wtil{\svba}^{(i)}}} \opnorm{\TrueParameterMatrix -\EstimatedParameterMatrix}\\
    & \sless{(d)}  \stwonorm{\TrueExternalFieldI[i] - \EstimatedExternalFieldI[i]} + 2\Bigparenth{\stwonorm{\svbv^{(i)}} + \stwonorm{\wtil{\svba}^{(i)}}} \onematnorm{\TrueParameterMatrix -\EstimatedParameterMatrix} \\
    & \sless{(e)}  \stwonorm{\TrueExternalFieldI[i] - \EstimatedExternalFieldI[i]} + 2\xmax \bigparenth{\sqrt{p_v} + \sqrt{p_a}} \onematnorm{\TrueParameterMatrix -\EstimatedParameterMatrix}, \label{eq_mean_outcome_part_2}
\end{align}
where $(a)$ follows from triangle inequality, $(b)$ follows because induced matrix norms are submultiplicative, $(c)$ follows because operator norm of any sub-matrix is no more than operator norm of the matrix and $\ell_2$ norm of any sub-vector is no more than $\ell_2$ norm of the vector, $(d)$ follows because $\TrueParameterMatrix -\EstimatedParameterMatrix$ is symmetric and because matrix operator norm is bounded by square root of the product of matrix one norm and matrix infinity norm, and $(e)$ follows because $\max\{\sinfnorm{\svbv^{(i)}}, \sinfnorm{\svba^{(i)}}\} \leq \xmax$ for all $i \in [n]$.\\

\noindent Now, combining \cref{eq_mean_outcome_part_1,eq_mean_outcome_part_2}, we have
\begin{align}
     & \stwonorm{\normalparenth{\TrueExternalFieldI[i,y] - \EstimatedExternalFieldI[i,y]} + 2\svbv^{(i)\top}\!\normalparenth{\TruePhi[v,y] - \EstimatedPhi^{(v,y)} } + 2\wtil{\svba}^{(i)\top}\!\normalparenth{\TruePhi[a,y] - \EstimatedPhi^{(a,y)}}} + \sum_{t \in [p_y]} \stwonorm{\TruePhi[y,y]_{t} - \EstimatedPhi^{(y,y)}_t} \\
     & \leq \stwonorm{\TrueExternalFieldI[i] - \EstimatedExternalFieldI[i]} + 2\xmax \bigparenth{\sqrt{p_v} + \sqrt{p_a}} \onematnorm{\TrueParameterMatrix -\EstimatedParameterMatrix} + \sum_{t \in [p_y]} \stwonorm{\TrueParameterRowt -\EstimatedParameterRowt}\\
     & \sless{(a)}  R(\varepsilon, \delta/n) + 2\xmax \bigparenth{\sqrt{p_v} + \sqrt{p_a}} \sqrt{p} \varepsilon + p_y \varepsilon,
\end{align}
 and $(a)$ follows from \cref{theorem_parameters} by using the relationship between vector norms. The proof is complete by rescaling $\varepsilon$ and absorbing the constants in $c$.

\paragraph{Proof of \cref{lemma_exp_fam_parameter_perturbation}: \expfamperturbationresultname:}
Let $Z(\psi, \Psi) \in \Reals_{+}$ denote the log-partition function of $f_{\rvbu}(\cdot | \psi, \Psi) $ in \cref{eq_dist_u}. Then, from \cite[Theorem 1]{BusaFSZ2019}, we have
\begin{align}
    \stwonorm{ \mu_{\hpsi, \hPsi}(\rvbu) - \mu_{\spsi, \sPsi}(\rvbu)} = \stwonorm{\nabla_{\hpsi} Z(\hpsi, \hPsi) - \nabla_{\spsi} Z(\spsi, \sPsi)}. \label{eq_mean_vector_difference}
\end{align}
For $t_1, t_2, t_3 \in [p]$, consider $\frac{\partial^2 Z(\psi, \Psi)}{\partial \psi_{t_1}\partial \psi_{t_2}}$ and $\frac{\partial^2 Z(\psi, \Psi)}{\partial \psi_{t_1}\partial \Psi_{t_2, t_3}}$. Using the fact that the Hessian of the log partition function of any regular exponential family is the covariance matrix of the associated sufficient statistic, we have
\begin{align}
    \frac{\partial^2 Z(\psi, \Psi)}{\partial \psi_{t_1}\partial \psi_{t_2}} = \Covariance_{\psi, \Psi}(\rvu_{t_1}, \rvu_{t_2}) \qtext{and} \frac{\partial^2 Z(\psi, \Psi)}{\partial \psi_{t_1}\partial \Psi_{t_2, t_3}} = \Covariance_{\psi, \Psi}(\rvu_{t_1}, \rvu_{t_2} \rvu_{t_3}). \label{eq_exp_fam_hessian_cov}
\end{align}
Now, for some $c \in (0,1)$, $\tpsi \defn c \hpsi + (1-c) \spsi$ and $\tPsi \defn c \tpsi + (1-c) \tpsi$, we have the following from the mean value theorem
\begin{align}
\frac{\partial Z(\hpsi, \hPsi)}{\partial \hpsi_{t_1}} \!-\!  \frac{\partial Z(\spsi, \sPsi)}{\partial \spsi_{t_1}} & \!=\! \sum_{t_2 \in [p]}\frac{\partial^2 Z(\tpsi, \tPsi)}{\partial \tpsi_{t_2}\partial \tpsi_{t_1}} \cdot (\hpsi_{t_2} - \spsi_{t_2}) + \sum_{t_2 \in [p]} \sum_{t_3 \in [p]} \frac{\partial^2 Z(\tpsi, \tPsi)}{\partial \tPsi_{t_2, t_3} \partial \tpsi_{t_1}} \cdot (\hPsi_{t_2, t_3} - \sPsi_{t_2, t_3})\\
& \!\!\!\sequal{\cref{eq_exp_fam_hessian_cov}} \!\!\sum_{t_2 \in [p]}\!\! \Covariance_{\tpsi, \tPsi}(\rvu_{t_1}, \rvu_{t_2}) 
\!\cdot\! (\hpsi_{t_2} \!-\! \spsi_{t_2}) \!+\!\!\! \sum_{t_3 \in [p]} \!\sum_{t_2 \in [p]} \!\!\Covariance_{\tpsi, \tPsi}(\rvu_{t_1}, \rvu_{t_3} \rvu_{t_2}) \!\cdot\!(\hPsi_{t_3, t_2} \!-\! \sPsi_{t_3, t_2}).
\end{align}
Now, using the triangle inequality and sub-multiplicativity of induced matrix norms, we have
\begin{align}
    \stwonorm{\nabla_{\hpsi} Z(\hpsi, \hPsi) \!-\! \nabla_{\spsi} Z(\spsi, \sPsi)} & \leq \opnorm{\Covariance_{\tpsi, \tPsi}(\rvbu, \rvbu)}\stwonorm{(\hpsi \!-\! \spsi)} \\
    & \qquad \qquad \qquad + \sum_{t_3 \in [p]} \! \opnorm{\Covariance_{\tpsi, \tPsi}(\rvbu, \rvu_{t_3} \rvbu)}\stwonorm{(\hPsi_{t_3} \!-\! \sPsi_{t_3})}.  \label{eq_mean_value_theorem}
\end{align}
%
%
%
%
%
%
%
%
%
%
%
%
Combining \cref{eq_mean_vector_difference,eq_mean_value_theorem} completes the proof.


\section{Logarithmic Sobolev inequality and tail bounds} 
\label{section_lsi_tail_bounds}
%
In this section, we present two results which may be of independent interest. First, we show that a random vector supported on a compact set satisfies the logarithmic Sobolev inequality (to be defined) if it satisfies the Dobrushin's uniqueness condition (to be defined). This result is a generalization of the result in \cite{Marton2015} for discrete random vectors to continuous random vectors supported on a compact set. Next, we show that if a random vector satisfies the logarithmic Sobolev inequality, then any arbitrary function of the random vector concentrates around its mean. This result is a generalization of the result in \cite{DaganDDA2021}  for discrete random vectors to continuous random vectors.\\

\noindent Throughout this section, we consider a $p$-dimensional random vector $\rvbx$ supported on $\cX^p$ with distribution $f_{\rvbx}$ where $p \geq 1$. We start by defining the logarithmic Sobolev inequality (LSI). We use the convention $0\log 0=0$.

\begin{definition}[\tbf{Logarithmic Sobolev inequality}]\label{def_lsi}
A random vector $\rvbx$ satisfies the logarithmic Sobolev inequality with constant $\sigma^2 > 0$ (abbreviated as $\LSI{\rvbx}{\sigma^2}$) if
\begin{align}
    \Ent{\rvbx}{q^2} \leq \sigma^2 \Expectation_{\rvbx}\Bigbrackets{\twonorm{\nabla_{\rvbx} q(\rvbx)}^2} \qtext{for all} q : \cX^p \to \Reals, \label{eq_LSI_definition}
\end{align}
where $\Ent{\rvbx}{g}\!\defn\! \Expectation_{\rvbx}[g(\rvbx) \log g(\rvbx)] \!-\!\Expectation_{\rvbx}[g(\rvbx)] \log \Expectation_{\rvbx}[g(\rvbx)]$ denotes the entropy of the function $g\! :\! \cX^p \!\to\! \real_{+}$.
%
%
%
%
%
\end{definition}
\vspace{2mm}
\noindent  
%
Next, we restate the Dobrushin's uniqueness condition \citep{Marton2015}. 
\begin{definition}\cite[\tbf{Dobrushin's uniqueness condition}]{Marton2015} \label{def_dobrushin_condition}
A random vector $\rvbx$ satisfies the Dobrushin's uniqueness condition with coupling matrix $\ParameterMatrix \in \Reals^{p \times p}$ with $\ParameterTU[tt] = 0$ for all $t \in [p]$, if
%
    %
    $\opnorm{\ParameterMatrix} < 1$, and
    %
    for every $t \in [p], u \in [p] \!\setminus\! \{t\}$, and $\svbx_{-t}, \tsvbx_{-t} \in \cX^{p-1}$ differing only in the $u^{th}$ coordinate,
    \begin{align}
        \TV{f_{\rvx_t | \rvbx_{-t} = \svbx_{-t}}}{f_{\rvx_t | \rvbx_{-t} = \tsvbx_{-t}}} \leq \ParameterTU[tu]. \label{eq_dob_tv_bound}
    \end{align}
%
\end{definition}
\vspace{2mm}
\noindent From hereon, we let $\cX^p$ be compact unless otherwise specified. Moreover, we define
\begin{align}
\label{eq:smin}   
f_{\min} \defeq \min_{t \in [p], \svbx \in \cX^p} f_{\rvx_t | \rvbx_{-t}}(x_t | \svbx_{-t}).
\end{align}
%
%
\noindent Now, we provide the first main result of this section with a proof in \cref{subsec_proof_lsi}.

\newcommand{\lsiresultname}{Logarithmic Sobolev inequality}
\begin{proposition}[\tbf{\lsiresultname}]\label{thm_LSI_main}
If a random vector $\rvbx$ with $f_{\min}>0$ (see \cref{eq:smin}) satisfies (a) the Dobrushin's uniqueness condition (\cref{def_dobrushin_condition}) with coupling matrix $\ParameterMatrix \in \Reals^{p \times p}$, and (b) $\rvx_t | \rvbx_{-t}$ satisfies $\LSI{\rvx_t | \rvbx_{-t} = \svbx_{-t}}{\sigma^2}$ for all $t \in [p]$ and $\svbx_{-t} \in \cX^{p-1}$, then it satisfies 
%
$\mathrm{LSI}_{\rvbx}(2\sigma^2/(f_{\min}(1-\opnorm{\ParameterMatrix})^2))$.
%
%
%
%
%
%
%
%
\end{proposition}
\noindent Next, we define the notion of pseudo derivative and pseudo Hessian that come in handy in our proofs for providing upper bounds on the norm of the derivative and the Hessian.

\begin{definition}[\tbf{Pseudo derivative and Hessian}]\label{def_pseudo_der_hes}
For a function $q : \cX^p \to \Reals$, the functions $\tnabla q : \cX^p \to \Reals^{p_1}$ and $\tnabla^2 q : \cX^p \to \Reals^{p_1 \times p_2}$  ($p_1,p_2 \geq 1$) are respectively called a pseudo derivative and a pseudo Hessian for $q$ if for all $\svby \in \cX^p$ and $\rho \in \Reals^{p_1 \times 1}$, we have
\begin{align}
    \stwonorm{\tnabla q(\svby)} \geq  \stwonorm{\nabla q(\svby)}
    \qtext{and}
    \stwonorm{\rho\tp \tnabla^2 q(\svby)} \geq  \stwonorm{\nabla \bigbrackets{\rho\tp \tnabla q(\svby)}}. 
    %
    \label{eq:pseudo_Hessian}
    %
\end{align}
%
%
%
%
\end{definition}

\noindent Finally, we provide the second main result of this section with a proof in \cref{subsec_proof_main_concentration}.

\newcommand{\mainconcresultname}{Tail bounds for arbitrary functions under LSI}
\begin{proposition}[\tbf{\mainconcresultname}]\label{thm_main_concentration}
Given a random vector $\rvbx$ satisfying $\LSI{\rvbx}{\sigma^2}$, any function $q :\cX^p \to \Reals$ with a pseudo derivative $\tnabla q$, and pseudo Hessian $\tnabla^2 q$ (see \cref{def_pseudo_der_hes}) satisfies a tail bound, namely for any fixed $\varepsilon > 0$, we have
\begin{align}
    \Probability\Bigbrackets{\bigabs{q(\rvbx) \!-\! \Expectation\bigbrackets{q(\rvbx)}} \!\geq\! \varepsilon} \!\leq\! \exp\biggparenth{-\frac{c}{\sigma^4} \min \Bigparenth{\dfrac{\varepsilon^2}{ \Expectation\bigbrackets{\stwonorm{\tnabla q(\rvbx)}}^2 \!+\! \max\limits_{\svbx \in \cX^p} \fronorm{\tnabla^2 q(\svbx)}^2}, \dfrac{\varepsilon}{\max\limits_{\svbx \in \cX^p} \opnorm{\tnabla^2 q(\svbx)}}}},
\end{align}
where $c$ is a universal constant.
\end{proposition}

\subsection{Proof of \cref{thm_LSI_main}: \lsiresultname}
\label{subsec_proof_lsi}
We start by defining the notion of $W_2$ distance \citep{Marton2015} which is useful in the proof. We note that $W_2$ distance is a metric on the space of probability measures and satisfies triangle inequality.

\begin{definition}\cite[\tbf{$W_2$ distance}]{Marton2015}\label{def_w2_distance}
For random vectors $\rvbx$ and $\rvby$ supported on $\cX^p$ with distributions $f$ and $g$ respectively, the $W_2$ distance is given by
$ W_2^2(g_{\rvby}, f_{\rvbx}) \defeq \inf_{\pi} \sump \Bigbrackets{\Probability_{\pi}(\rvx_t \neq \rvy_t)}^2$,
%
%
%
where the infimum is taken over all couplings $\pi(\rvbx, \rvby)$ such that $\pi(\rvbx) = f(\rvbx)$ and $\pi(\rvby) = g(\rvby)$.
\end{definition}
\noindent Given \cref{def_w2_distance}, our next lemma states that if appropriate $W_2$ distances are bounded, then the KL divergence and the entropy approximately tensorize. We provide a proof in \cref{subsec_proof_lemma_tenorization_kld}.

\newcommand{\approxtensorofKLresultname}{Approximate tensorization of KL divergence and entropy}
\begin{lemma}[\tbf{\approxtensorofKLresultname}]\label{lemma_tenorization_kld}
Given random vectors $\rvbx$ and $\rvby$ supported on $\cX^p$ with distributions $f$ and $g$ respectively such that $f_{\min} > 0$ (see \cref{eq:smin}), if for all subsets $\set \subseteq [p]$ (with $\setC \defn [p] \setminus \set$) and all $\svby_{\setC} \in \cX^{p - |\set|}$,
%
\begin{align}
    W_2^2\bigparenth{g_{\rvby_{\set} | \rvby_{\setC} = \svby_{\setC}}, f_{\rvbx_{\set} | \rvbx_{\setC} = \svby_{\setC}}} \!\leq\! C \sumset \Expectation\Bigbrackets{\TV{g_{\rvy_t | \rvby_{-t} = \svby_{-t}}}{f_{\rvx_t | \rvbx_{-t} = \svby_{-t}}}^2  \Big| \rvby_{\setC} = \svby_{\setC}},
    %
    \label{eq_w2_distance_bounded_assumption}
\end{align}
almost surely for some constant $C \geq 1$, then 
%
%
\begin{align}
    \KLD{g_{\rvby}}{f_{\rvbx}} &\leq \frac{2C}{f_{\min}} \sump \Expectation\bigbrackets{\KLD{g_{\rvy_t | \rvby_{-t} = \svby_{-t}}}{f_{\rvx_t | \rvbx_{-t} = \svby_{-t}}}},
    \qtext{and} 
    \label{eq_tensorization_kld}\\
%
%
    \Ent{\rvbx}{q} &\leq \frac{2C}{f_{\min}} \sump \Expectation_{\rvbx_{-t}}\bigbrackets{\Ent{\rvx_t | \rvbx_{-t}}{q}}
    \qtext{for any function $q : \cX^p \to \real_{+}$.} \label{eq_tensorization_entropy}
\end{align}
\end{lemma}
\noindent Next, we claim that if the random vector $\rvbx$ satisfies Dobrushin's uniqueness condition, then the condition \cref{eq_w2_distance_bounded_assumption} of \cref{lemma_tenorization_kld} is naturally satisfied. We provide a proof in \cref{subsec_proof_lemma_dobrushin_implies_tensorization}.

\newcommand{\dobimpliesapproxtensorresultname}{Dobrushin's uniqueness implies approximate tensorization}

\begin{lemma}[\tbf{\dobimpliesapproxtensorresultname}]\label{lemma_dobrushin_implies_tensorization}
Given random vectors $\rvbx$ and $\rvby$ supported on $\cX^p$ with distributions $f$ and $g$ respectively, if $\rvbx$ satisfies Dobrushin's uniqueness condition (see \cref{def_dobrushin_condition}) with coupling matrix $\ParameterMatrix \in \Reals^{p \times p}$, then for all subsets $\set \subseteq [p]$ (with $\setC \defn [p] \setminus \set$) and all $\svby_{\setC} \in \cX^{p - |\set|}$,
\begin{align}
W_2^2\bigparenth{g_{\rvby_{\set} | \rvby_{\setC} = \svby_{\setC}}, f_{\rvbx_{\set} | \rvbx_{\setC} = \svby_{\setC}}} \!\leq\! \frac{1}{\bigparenth{1\!-\!\opnorm{\ParameterMatrix}}^2} \sumset \Expectation\Bigbrackets{\TV{g_{\rvy_t | \rvby_{-t} = \svby_{-t}}}{f_{\rvx_t | \rvbx_{-t} = \svby_{-t}}}^2  \Big| \rvby_{\setC} = \svby_{\setC}}, \label{eq_w2_distance_bounded}
\end{align}
almost surely.
\end{lemma}
\noindent 
%
Now to prove \cref{thm_LSI_main}, applying \cref{lemma_tenorization_kld,lemma_dobrushin_implies_tensorization} for an arbitrary function $f : \cX^p \to \Reals$, we find that
\begin{align}
    \Ent{\rvbx}{q^2} & \leq \frac{2}{f_{\min}\bigparenth{1-\opnorm{\ParameterMatrix}}^2} \sump \Expectation_{\rvbx_{-t}}\Bigbrackets{\Ent{\rvx_t | \rvbx_{-t}}{q^2}} \\
    & \sless{(a)} \frac{2\sigma^2}{f_{\min}\bigparenth{1-\opnorm{\ParameterMatrix}}^2} \sump \Expectation_{\rvbx_{-t}}\Bigbrackets{ \Expectation_{\rvx_t | \rvbx_{-t}}\Bigbrackets{\twonorm{\nabla_{\rvx_t} q(\rvx_t; \rvbx_{-t})}^2}} \\
    & \sequal{(b)} \frac{2\sigma^2}{f_{\min}\bigparenth{1-\opnorm{\ParameterMatrix}}^2} \Expectation_{\rvbx_{-t}}\Bigbrackets{ \Expectation_{\rvx_t | \rvbx_{-t}}\Bigbrackets{ \sump \twonorm{\nabla_{\rvx_t} q(\rvx_t; \rvbx_{-t})}^2 } } \\
    & \sequal{(c)} \frac{2\sigma^2}{f_{\min}\bigparenth{1-\opnorm{\ParameterMatrix}}^2} \Expectation_{\rvbx}\Bigbrackets{\twonorm{\nabla_{\rvbx} q(\rvbx)}^2},
\end{align}
where $(a)$ follows because $\rvx_t | \rvbx_{-t}$ satisfies $\LSI{\rvx_t | \rvbx_{-t} = \svbx_{-t}}{\sigma^2}$ for all $t \in [p]$ and $\svbx_{-t} \in \cX^{p-1}$, $(b)$ follows by the linearity of expectation and $(b)$ follows by the law of total expectation. The claim follows.




\subsubsection{Proof of \cref{lemma_tenorization_kld}: \approxtensorofKLresultname}
\label{subsec_proof_lemma_tenorization_kld}
We start by establishing a reverse-Pinsker style inequality for distributions with compact support to bound their KL divergence by their total variation distance. We provide a proof at the end..
%

\begin{lemma}[\tbf{Reverse-Pinsker inequality}]\label{lemma_reverse_pinsker}
For any distributions $f$ and $g$ supported on $\cX \subset \Reals$ such that $\min_{x \in \cX} f(x) > 0$, we have $\KLD{g}{f} \leq \frac{4}{\min_{x \in \cX} f(x)} \TV{g}{f}^2.$
%
%
%
\end{lemma}

%

%
\noindent Given \cref{lemma_reverse_pinsker}, we proceed to prove \cref{lemma_tenorization_kld}. 

\paragraph{Proof of bound~\cref{eq_tensorization_kld}:}
To prove \cref{eq_tensorization_kld}, we show that the following inequality holds using the technique of mathematical induction on $p$:
\begin{align}
    \KLD{g_{\rvby}}{f_{\rvbx}} \leq \frac{4C}{f_{\min}} \sump \Expectation\Bigbrackets{\TV{g_{\rvy_t | \rvby_{-t} = \svby_{-t}}}{f_{\rvx_t | \rvbx_{-t} = \svby_{-t}}}^2 }. \label{eq_tensor_kl_tv}
\end{align}
Then, \cref{eq_tensorization_kld} follows by using Pinsker's inequality to bound the right hand side of \cref{eq_tensor_kl_tv}.

\paragraph{Base case: $p = 1$:} For the base case, we need to establish that the claim holds for all distributions supported on $\cX$ that satisfy the required conditions. In other words, we need to show that
\begin{align}
    \KLD{g_{\rvy}}{f_{\rvx}} \leq \frac{4C}{f_{\min}} \TV{g_{\rvy}}{f_{\rvx}}^2 \qtext{for every} t \in [p].
\end{align}
for all random variables $\rvx$ and $\rvy$ supported on $\cX$ such that $f_{\min} = \min_{x \in \cX} f_{\rvx}(x) > 0$.  This follows from \cref{lemma_reverse_pinsker} by observing that $C \geq 1$.
%
%
%
%

%
%
%
%

\paragraph{Inductive step:} Now, we assume that the claim holds for all  distributions supported on $ \cX^{p-1}$ that satisfy the required conditions, and establish it for distributions supported on $\cX^{p}$. From the chain rule of KL divergence, we have
\begin{align}
    \KLD{g_{\rvby}}{f_{\rvbx}} = \KLD{g_{\rvy_t}}{f_{\rvx_t}} + \Expectation \bigbrackets{ \KLD{g_{\rvby_{-t} | \rvy_t}}{f_{\rvbx_{-t} | \rvx_t}}  } \qtext{for every} t \in [p]. \label{eq_kl_chain_rule}
\end{align}
Taking an average over all $t \in [p]$, we have
\begin{align}
    \KLD{g_{\rvby}}{f_{\rvbx}} = \frac{1}{p} \sump \KLD{g_{\rvy_t}}{f_{\rvx_t}} + \frac{1}{p} \sump \Expectation \bigbrackets{ \KLD{g_{\rvby_{-t} | \rvy_t}}{f_{\rvbx_{-t} | \rvx_t}}  }. \label{eq_avg_kl_chain_rule}
\end{align}
 Now, we bound the first term in \cref{eq_avg_kl_chain_rule}. Let $\pi^*$ be such that 
%
\begin{align}
    \pi^* = \argmin_{\pi: \pi(\rvbx) = f(\rvbx), \pi(\rvby) = g(\rvby)} \sump \Bigbrackets{\Probability_{\pi}(\rvx_t \neq \rvy_t)}^2. \label{eq_opt_coupling}
\end{align}
Then, we have
\begin{align}
    \frac{1}{p} \sump \KLD{g_{\rvy_t}}{f_{\rvx_t}} \sless{(a)} \frac{1}{p} \sump \frac{4}{f_{\min}} \TV{g_{\rvy_t}}{f_{\rvx_t}}^2  & \sless{(b)} \frac{4}{pf_{\min}} \sump \Bigbrackets{\Probability_{\pi^*}(\rvx_t \neq \rvy_t)}^2\\
    & \sequal{(c)} \frac{4}{pf_{\min}} W_2^2(g_{\rvby}, f_{\rvbx}) \\
    & \sless{\cref{eq_w2_distance_bounded_assumption}} \frac{4C}{pf_{\min}} \sump \Expectation\Bigbrackets{\TV{g_{\rvy_t | \rvby_{-t} = \svby_{-t}}}{f_{\rvx_t | \rvbx_{-t} = \svby_{-t}}}^2 } \label{eq_avg_kl_bound_2}
\end{align}
where $(a)$ follows from \cref{lemma_reverse_pinsker} by observing that the marginals are lower bounded if the conditional are lower bounded, i.e., $\min_{t \in [p], x_t \in \cX} f_{\rvx_t}(x_t) = \min_{t \in [p], x_t \in \cX} \int_{\svbx_{-t} \in \cX^{p-1}} f_{\rvx_t | \rvbx_{-t}}(x_t | \svbx_{-t}) f_{\rvbx_{-t}}(\svbx_{-t}) d\svbx_{-t} > f_{\min}$, $(b)$ follows from the connections of total variation distance to optimal transportation cost, i.e., $\TV{g_\rvy}{f_{\rvx}} = \inf_{\pi: \pi(\rvx) = f(\rvx), \pi(\rvy) = g(\rvy)} \Probability_{\pi}(\rvx \neq \rvy)$, and $(c)$ follows from \cref{def_w2_distance,eq_opt_coupling}.\\
%

\noindent Next, we bound the second term in \cref{eq_avg_kl_chain_rule}. We have
\begin{align}
    \frac{1}{p} \sump \Expectation \bigbrackets{ \KLD{g_{\rvby_{-t} | \rvy_t}}{f_{\rvbx_{-t} | \rvx_t}}} 
    %
    & \sless{(a)} \frac{1}{p} \sump \Expectation \biggbrackets{\frac{4C}{f_{\min}} \sum_{u \in [p] \setminus \{t\} }   \Expectation\Bigbrackets{\TV{g_{\rvy_u | \rvby_{-u} = \svby_{-u}}}{f_{\rvx_u | \rvbx_{-u} = \svby_{-u}}}^2  \Big| \rvy_t = y_t}}\\
    & \sequal{(b)} \frac{4C}{pf_{\min}} \sump \sum_{u \in [p] \setminus \{t\}} \Expectation\Bigbrackets{\TV{g_{\rvy_u | \rvby_{-u} = \svby_{-u}}}{f_{\rvx_u | \rvbx_{-u} = \svby_{-u}}}^2 } \\
    & = \frac{4C(p-1)}{pf_{\min}} \sum_{u \in [p]} \Expectation\Bigbrackets{\TV{g_{\rvy_u | \rvby_{-u} = \svby_{-u}}}{f_{\rvx_u | \rvbx_{-u} = \svby_{-u}}}^2 }, \label{eq_avg_kl_bound_1}
\end{align}
where $(a)$ follows from the inductive hypothesis and $(b)$ follows from the law of total expectation. Then, \cref{eq_tensor_kl_tv} follows by putting \cref{eq_avg_kl_bound_1,eq_avg_kl_bound_2,eq_avg_kl_chain_rule} together.\\

\paragraph{Proof of bound~\cref{eq_tensorization_entropy}:}
To prove \cref{eq_tensorization_entropy}, we note that \cref{eq_tensorization_kld} holds for any random vector $\rvby$ supported on $\cX^p$.
%
Consider $\rvby$ be such that $q(\rvbx) / \Expectation_{\rvbx}[q(\rvbx)]$ is the Radon-Nikodym derivative of $g_{\rvby}$ with respect to $f_{\rvbx}$. Then, we have 
%
%
%
%
%
\begin{align}
    \frac{dg_{\rvby_{-t}}}{df_{\rvbx_{-t}}} =\frac{\Expectation_{\rvx_t | \rvbx_{-t}}\bigbrackets{q(\rvbx)}}{\Expectation_{\rvbx}\bigbrackets{q(\rvbx)}} \qtext{and} \frac{dg_{\rvy_t | \rvby_{-t}}}{df_{\rvx_t | \rvbx_{-t}}} = \frac{q(\rvbx)}{\Expectation_{\rvx_t | \rvbx_{-t}}\bigbrackets{q(\rvbx)}} \label{eq_radon_niko_marginal} \qtext{for all} t \in [p].
\end{align}
%
%
%
%
We have
\begin{align}
\KLD{g_{\rvby}}{f_{\rvbx}} & \sequal{(a)} \Expectation_{\rvbx} \biggbrackets{\frac{dg_{\rvby}}{df_{\rvbx}} \log \frac{dg_{\rvby}}{df_{\rvbx}}}\\
& \sequal{(b)} \Expectation_{\rvbx} \biggbrackets{\frac{q(\rvbx)}{\Expectation_{\rvbx}\bigbrackets{q(\rvbx)}} \log \frac{q(\rvbx)}{\Expectation_{\rvbx}\bigbrackets{q(\rvbx)}}} \\
& = \frac{1}{\Expectation_{\rvbx}\bigbrackets{q(\rvbx)}} \Bigparenth{\Expectation_{\rvbx} \bigbrackets{q(\rvbx) \log q(\rvbx)} - \Expectation_{\rvbx} \bigbrackets{q(\rvbx)} \log \Expectation_{\rvbx} \bigbrackets{q(\rvbx)}} = \frac{\Ent{\rvbx}{q}}{\Expectation_{\rvbx}\bigbrackets{q(\rvbx)}}, \label{eq_kl_entropy_mapping}
\end{align}
where $(a)$ follows from the definition of KL divergence and $(b)$ follows from the choice of $\rvby$. Similarly, for every $t \in [p]$, we have
\begin{align}
\Expectation_{\rvby_{-t}}\Bigbrackets{\KLD{g_{\rvy_t | \rvby_{-t} = \svby_{-t}}}{f_{\rvx_t | \rvbx_{-t} = \svby_{-t}}}} & \sequal{(a)} \Expectation_{\rvby_{-t}}\biggbrackets{ \Expectation_{\rvy_t | \rvby_{-t}} \biggbrackets{\log \frac{dg_{\rvy_t | \rvby_{-t}}}{df_{\rvx_t | \rvbx_{-t}}} }} \\
& \sequal{(b)} \Expectation_{\rvby} \biggbrackets{\log \frac{dg_{\rvy_t | \rvby_{-t}}}{df_{\rvx_t | \rvbx_{-t}}} }\\
& \sequal{(c)} \Expectation_{\rvbx} \biggbrackets{\frac{dg_{\rvby}}{df_{\rvbx}} \log \frac{dg_{\rvy_t | \rvby_{-t}}}{df_{\rvx_t | \rvbx_{-t}}} } \\
& \sequal{(d)} \Expectation_{\rvbx} \biggbrackets{\frac{q(\rvbx)}{\Expectation_{\rvbx}\bigbrackets{q(\rvbx)}} \log \frac{q(\rvbx)}{\Expectation_{\rvx_t | \rvbx_{-t}}\bigbrackets{q(\rvbx)}}} \\
%
& \sequal{(e)} \frac{\Expectation_{\rvbx_{-t}}\bigbrackets{\Expectation_{\rvx_t | \rvbx_{-t}} \bigbrackets{q(\rvbx) \log q(\rvbx)} - \Expectation_{\rvx_t | \rvbx_{-t}} \bigbrackets{q(\rvbx) \log \Expectation_{\rvx_t | \rvbx_{-t}}\bigbrackets{q(\rvbx)}} }}{\Expectation_{\rvbx}\bigbrackets{q(\rvbx)}} \\
& \sequal{(f)} \frac{\Expectation_{\rvbx_{-t}}\bigbrackets{\Ent{\rvx_t | \rvbx_{-t}}{q}}}{\Expectation\bigbrackets{q(\rvbx)}}. \label{eq_kl_entropy_conditional_mapping}
\end{align}
where $(a)$ follows from the definition of KL divergence, $(b)$ follows from the law of total expectation, $(c)$ follows from the definition of Radon-Nikodym derivative, $(d)$ follows from the choice of $\rvby$ and \cref{eq_radon_niko_marginal}, $(e)$ follows from the law of total expectation, $(f)$ follows from the definition of entropy. Then, \cref{eq_tensorization_entropy} follows by putting \cref{eq_tensorization_kld,eq_kl_entropy_mapping,eq_kl_entropy_conditional_mapping} together.


\paragraph{Proof of \cref{lemma_reverse_pinsker}: \tbf{Reverse-Pinsker inequality}:}
\label{subsubsec_proof_reverse_pinsker}
%
%
%
%
Using the facts (a) $\log a \geq 1 - \frac{1}{a} $ for all $a>0$, and (b) $\min_{x \in \cX} f(x)>0$, we find that
%
\begin{align}
    \log \frac{f(x)}{g(x)} \geq 1 - \frac{g(x)}{f(x)} \qtext{for every} x \in \cX. \label{eq_rp_1}
\end{align}
Multiplying both sides of \cref{eq_rp_1} by $g(x) \geq 0$ and rearranging terms yields that
\begin{align}
    g(x) \log  \frac{g(x)}{f(x)} \leq  \frac{g^2(x)}{f(x)} - g(x) \qtext{for every} x \in \cX. \label{eq_rp_2}
\end{align}
Now, we have
\begin{align}
    \KLD{g}{f} 
    %
    = \int_{x \in \cX}\!\!  g(x) \log  \frac{g(x)}{f(x)} dx 
    %
    & \sless{\cref{eq_rp_2}} \int_{x \in \cX} \biggparenth{\frac{g^2(x)}{f(x)} - g(x)} dx 
    %
    %
    %
    \\
    & \sequal{(a)} \int_{x \in \cX} \frac{\bigparenth{g(x) -f(x)}^2}{f(x)}dx \\
    & \leq \frac{1}{\min_{x \in \cX} f(x)} \int_{x \in \cX} \bigparenth{g(x) -f(x)}^2 dx\\
    & \sless{(b)} \frac{1}{\min_{x \in \cX} f(x)} \Bigparenth{\int_{x \in \cX} \bigabs{g(x) -f(x)} dx}^2\\
    & \sequal{(c)} \frac{1}{\min_{x \in \cX} f(x)} \Bigparenth{2\TV{g}{f}}^2 =\frac{4}{\min_{x \in \cX} f(x)} \TV{g}{f}^2,
\end{align}
where $(a)$ follows by simple manipulations, $(b)$ follows by using the order of norms on Euclidean space, and $(c)$ follows by the definition of the total variation distance.

\subsubsection{Proof of \cref{lemma_dobrushin_implies_tensorization}: \dobimpliesapproxtensorresultname}
\label{subsec_proof_lemma_dobrushin_implies_tensorization}

We start by defining the notion of Gibbs sampler which is useful in the proof. 

\begin{definition}\cite[\tbf{Gibbs Sampler}]{Marton2015}\label{def_gibbs_sampler}
For a random vector $\rvbx$ with distribution $f$, define the Markov kernels and the Gibbs sampler as follows:
\begin{align}
    \Gamma_t(\svbx | \svbx') \defn \Indicator\normalparenth{\svbx_{-t} = \svbx'_{-t}} f_{\rvx_t | \rvbx_{-t}}(x_t | \svbx'_{-t}) 
    %
    \qtext{and}
    \Gamma(\svbx | \svbx') \defn p\inv \sump \Gamma_t(\svbx | \svbx'),
    %
    \label{eq_gibbs_sampler}
\end{align}
for all $t\in[p]$ and $x, x' \in \cX^p$.
That is, the kernel $\Gamma_t$ leaves all but the $t^{th}$ coordinate unchanged, and updates the $t^{th}$ coordinate according to $f_{\rvx_t | \rvbx_{-t}}$, and the sampler
%
%
%
%
$\Gamma$ selects an index $t \in [p]$ at random, and applies $\Gamma_t$. Further, for a random vector $\rvby$ with distribution $g$ supported on $\cX^p$, we also define
\begin{align}
    g_{\rvby} \Gamma_t(\svby) \defn \int g_{\rvby}(\svby') \Gamma_t(\svby | \svby') d\svby' \stext{for} t \in [p], \stext{and} g_{\rvby} \Gamma(\svby) \defn \int g_{\rvby}(\svby') \Gamma(\svby | \svby') d\svby'
    \qtext{for all} \svby \in \cX^p.
    \label{eq_gibbs_sampler_expander}
\end{align}
\end{definition}
\noindent We now proceed to prove \cref{lemma_dobrushin_implies_tensorization} and split it in two cases: (i) $\set = [p]$, and (ii) $\set \subset [p]$. 

\paragraph{Case~(i) ($\set=[p]$):} Let $\Gamma$ be the Gibbs sampler associated with the distribution $f$. Then,
\begin{align}
    W_2\bigparenth{g_{\rvby_{\set} | \rvby_{\setC}}, f_{\rvbx_{\set} | \rvbx_{\setC}}} & = W_2(g_{\rvby}, f_{\rvbx}) \sless{(a)} W_2(g_{\rvby}, g_{\rvby} \Gamma) + W_2(g_{\rvby} \Gamma, f_{\rvbx}), \label{eq_triangle_w2}
\end{align}
where $(a)$ follows from the triangle inequality. We claim that
%
\begin{align}
    W_2(g_{\rvby}, g_{\rvby} \Gamma) &\leq \frac{1}{p} \sqrt{\sump \Expectation_{\rvby_{-t}} \Bigbrackets{\TV{g_{\rvy_t | \rvby_{-t} = \svby_{-t}}}{f_{\rvx_t | \rvbx_{-t} = \svby_{-t}}}^2}}, \label{eq_coupling_bound_1} \qtext{and} \\
%
%
%
    W_2(g_{\rvby} \Gamma, f_{\rvbx}) &\leq \biggparenth{1 - \frac{(1 - \opnorm{\ParameterMatrix})}{p}} W_2(g_{\rvby}, f_{\rvbx}). \label{eq_coupling_bound_2}
\end{align}
Putting \cref{eq_triangle_w2,eq_coupling_bound_1,eq_coupling_bound_2} together, we have
\begin{align}
     W_2(g_{\rvby}, f_{\rvbx}) \leq \frac{1}{p} \sqrt{\sump \Expectation_{\rvby_{-t}} \Bigbrackets{\TV{g_{\rvy_t | \rvby_{-t} = \svby_{-t}}}{f_{\rvx_t | \rvbx_{-t} = \svby_{-t}}}^2}} + \biggparenth{1 - \frac{(1 - \opnorm{\ParameterMatrix})}{p}} W_2(g_{\rvby}, f_{\rvbx}). \label{eq_w2_triangle_final}
\end{align}
%
%
%
%
%
Rearranging \cref{eq_w2_triangle_final} results in
\cref{eq_w2_distance_bounded} for $S = [p]$ as desired. It remains to prove our earlier claims~\cref{eq_coupling_bound_1,eq_coupling_bound_2} which we now do one-by-one.

%

\paragraph{Proof of bound~\cref{eq_coupling_bound_1} on $ W_2(g_{\rvby}, g_{\rvby} \Gamma)$:} To bound $W_2(g_{\rvby}, g_{\rvby} \Gamma)$, we construct a random vector $\rvby^{\Gamma}$ such that it is coupled with the random vector $\rvby$.
%
We select an index $b \in [p]$ at random, and define
\begin{align}
    y_v^{\Gamma} \defn y_v  \qtext{for all} v \in [p] \setminus \{b\}.
\end{align}
%
Then, given $b$ and $\rvby_{-b} = \svby_{-b}$, we define the joint distribution of $(\rvy_b, \rvy_b^{\Gamma})$ to be the maximal coupling of $g_{\rvy_b | \rvby_{-b} = \svby_{-b}}$ and $f_{\rvx_b | \rvbx_{-b} = \svby_{-b}}$ that achieves $\TV{g_{\rvy_b | \rvby_{-b} = \svby_{-b}}}{f_{\rvx_b | \rvbx_{-b}=\svby_{-b}}}$. It is easy to see that the marginal distribution of $\rvby$ is $g_{\rvby}$ and the marginal distribution of $\rvby^{\Gamma}$ is $g_{\rvby} \Gamma$ (see \cref{def_gibbs_sampler}). Then, we have
\begin{align}
    W_2^2(g_{\rvby}, g_{\rvby} \Gamma) 
    %
    & \sless{(a)} \sump \biggbrackets{\Probability(b = t) \Probability(\rvy_t \neq \rvy_t^{\Gamma} | b = t) + \Probability(b \neq t) \Probability(\rvy_t \neq \rvy_t^{\Gamma} | b \neq t)}^2\\
    %
     & \sequal{(b)} \sump \biggbrackets{\frac{1}{p} \Probability(\rvy_t \neq \rvy_t^{\Gamma} | b = t)}^2\\
    & \sequal{(c)} \frac{1}{p^2} \sump \biggbrackets{ \int\limits_{\svby_{-t} \in \cX^{p-1}}\Probability(\rvy_t \neq \rvy_t^{\Gamma} | b = t, \rvby_{-t} = \svby_{-t}) g_{\rvby_{-t} | b= t}(\svby_{-t} | b = t) d\svby_{-t}}^2\\
    & \sequal{(d)} \frac{1}{p^2} \sump \biggbrackets{ \int\limits_{\svby_{-t} \in \cX^{p-1}}\TV{g_{\rvy_t | \rvby_{-t} = \svby_{-t}}}{f_{\rvx_t | \rvbx_{-t} = \svby_{-t}}} g_{\rvby_{-t}}(\svby_{-t}) d\svby_{-t}}^2\\
    & = \frac{1}{p^2} \sump \biggbrackets{\Expectation_{\rvby_{-t}} \Bigbrackets{\TV{g_{\rvy_t | \rvby_{-t} = \svby_{-t}}}{f_{\rvx_t | \rvbx_{-t} = \svby_{-t}}}}}^2, \label{eq_coupling_bound_1_interim}
    %
\end{align}
where $(a)$ follows from \cref{def_w2_distance} and the Bayes rule, $(b)$ follows because $\Probability(b = t) = \frac{1}{p}$ and $\Probability(\rvy_t \neq \rvy_t^{\Gamma} | b \neq t) = 0$, $(c)$ follows by the law of total probability, and $(d)$ follows because $g_{\rvby_{-t} | b= t}(\svby_{-t} | b = t) = g_{\rvby_{-t}}(\svby_{-t})$ and by the construction of the coupling between $\rvby$ and $\rvby^{\Gamma}$. Then, \cref{eq_coupling_bound_1} follows by using Jensen's inequality in \cref{eq_coupling_bound_1_interim}.



\paragraph{Proof of bound~\cref{eq_coupling_bound_2} on  $W_2(g_{\rvby} \Gamma, f_{\rvbx})$:}
%
We first show that  $f_{\rvbx}$ is an invariant measure for $\Gamma$, i.e., $f_{\rvbx} = f_{\rvbx} \Gamma$, implying $W_2(g_{\rvby} \Gamma, f_{\rvbx}) = W_2(g_{\rvby} \Gamma, f_{\rvbx} \Gamma)$, and then $\Gamma$ is a contraction with respect to the $W_2$ distance with rate $1 - \frac{(1 - \opnorm{\ParameterMatrix})}{p}$, i.e., $W_2(g_{\rvby} \Gamma, f_{\rvbx} \Gamma) \leq \Bigparenth{1 - \frac{(1 - \opnorm{\ParameterMatrix})}{p}} W_2(g_{\rvby}, f_{\rvbx})$, implying \cref{eq_coupling_bound_2}.

\paragraph{Proof of $f_{\rvbx}$ being an invariant measure for $\Gamma$:} We have
\begin{align}
    f_{\rvbx} \Gamma (\svbx)  \sequal{\cref{eq_gibbs_sampler_expander}} \int_{\svbx' \in \cX^p} f_{\rvbx}(\svbx') \Gamma(\svbx | \svbx') d\svbx' 
    & \sequal{\cref{eq_gibbs_sampler}} \int_{\svbx' \in \cX^p} f_{\rvbx}(\svbx') \biggparenth{\frac{1}{p} \sump \Gamma_t(\svbx | \svbx')} d\svbx' \\
    %
    & \sequal{\cref{eq_gibbs_sampler}} \frac{1}{p} \sump \int_{\svbx' \in \cX^p} f_{\rvbx}(\svbx') \Indicator\normalparenth{\svbx_{-t}  = \svbx'_{-t}} f_{\rvx_t | \rvbx_{-t}}(x_t | \svbx'_{-t}) d\svbx' \\
    & = \frac{1}{p} \sump f_{\rvx_t | \rvbx_{-t}}(x_t | \svbx_{-t}) \int_{x'_t \in \cX} f_{\rvbx}(\svbx_{-t}, x'_t) dx'_t \\
    & = \frac{1}{p} \sump f_{\rvx_t | \rvbx_{-t}}(x_t | \svbx_{-t}) f_{\rvbx_{-t}}(\svbx_{-t}) = f_{\rvbx}(\svbx).
\end{align}

\paragraph{Proof of $\Gamma$ being a contraction w.r.t the $W_2$ distance:} Let $\pi^*$ be the coupling between $\rvbx$ and $\rvby$ that achieves $W_2(g_{\rvby}, f_{\rvbx})$ i.e.,
\begin{align}
    \pi^* = \argmin_{\pi: \pi(\rvbx) = f(\rvbx), \pi(\rvby) = g(\rvby)} \sqrt{\sump \Bigbrackets{\Probability_{\pi}(\rvx_t \neq \rvy_t)}^2}. \label{eq_opt_coupling_2}
\end{align}
We construct random variables $\rvbx'$ and $\rvby'$ as well as a coupling $\pi'$ between them such that the marginal distribution of $\rvbx'$ is $f_{\rvbx} \Gamma$ and the marginal distribution of $\rvby'$ is $g_{\rvby} \Gamma$. We start by selecting an index $b \in [p]$ at random, and defining
\begin{align}
    y_v' \defn y_v \qtext{and} x_v' \defn x_v \qtext{for all} v \neq b. \label{eq_coupling_contraction}
\end{align}
Then, given $b$, $\rvby_{-b}' = \svby_{-b}$, and $\rvbx_{-b}' = \svbx_{-b}$, we define the joint distribution of $(\rvy_b', \rvx_b')$ to be the maximal coupling of $f_{\rvx_b | \rvbx_{-b}}(\cdot | \svby_{-b})$ and $f_{\rvx_b | \rvbx_{-b}}(\cdot | \svbx_{-b})$ that achieves $\TV{f_{\rvx_b | \rvbx_{-b} = \svby_{-b}}}{f_{\rvx_b | \rvbx_{-b} = \svbx_{-b}}}$. \\

\noindent Now, for every $t \in [p]$, we bound $\Probability_{\pi'}(\rvy_t' \neq \rvx_t')$ in terms of $\Probability_{\pi^*}(\rvy_t \neq \rvx_t)$. To that end, we have
\begin{align}
    \Probability_{\pi'}(\rvy_t' \neq \rvx_t') & \sequal{(a)} \Probability(b = t) \Probability_{\pi'}(\rvy_t' \neq \rvx_t' | b = t) + \Probability(b \neq t) \Probability_{\pi'}(\rvy_t' \neq \rvx_t' | b \neq t) \\
    & \sequal{(b)} 
    %
    %
    \frac{1}{p} \Probability_{\pi'}(\rvy_t' \neq \rvx_t' | b = t) + \Bigparenth{1-\frac{1}{p}} \Probability_{\pi^*}(\rvy_t \neq \rvx_t), \label{eq_bayes_rule_step_1}
\end{align}
where $(a)$ follows from the Bayes rule and $(b)$ follows because $\Probability(b = t) = \frac{1}{p}$ and \cref{eq_coupling_contraction}. Focusing on $\Probability_{\pi'}(\rvy_t' \neq \rvx_t' | b = t)$ and using the law of total probability, we have
\begin{align}
    & \Probability_{\pi'}(\rvy_t' \neq \rvx_t' | b = t) \\
    & =  \!\!\!\!\!\!\! \int\limits_{\svby_{-t}, \svbx_{-t} \in \cX^{p-1}} \!\!\!\!\!\!\! \Probability_{\pi'}(\rvy_t' \neq \rvx_t' | b = t, \rvby_{-t}' = \svby_{-t}, \rvbx_{-t}' = \svbx_{-t}) \pi'_{\rvby_{-t}', \rvbx_{-t}' | b= t}(\svby_{-t}, \svbx_{-t} | b = t) d\svby_{-t} d\svbx_{-t} \\
    & \sequal{(a)}  \!\!\!\!\!\!\! \int\limits_{\svby_{-t}, \svbx_{-t} \in \cX^{p-1}} \!\!\!\!\!\!\! \TV{f_{\rvx_t | \rvbx_{-t} = \svby_{-t}}}{f_{\rvx_t | \rvbx_{-t} = \svbx_{-t}}} \pi^*_{\rvby_{-t}, \rvbx_{-t}}(\svby_{-t}, \svbx_{-t}) d\svby_{-t} d\svbx_{-t} \\
    &  =  \Expectation_{\pi^*_{\rvby_{-t}, \rvbx_{-t}}} \Bigbrackets{\TV{f_{\rvx_t | \rvbx_{-t} = \svby_{-t}}}{f_{\rvx_t | \rvbx_{-t} = \svbx_{-t}}} } \\
    %
     & \sless{(b)}  \!\Expectation_{\pi^*_{\rvby_{-t}, \rvbx_{-t}}} \!\Bigbrackets{ \!\!\!\!\!\! \sum_{u \in [p] \setminus \{t\}}  \!\!\!\!\!\! \Indicator(r_u \!=\! x_u \! \neq \! y_u \!=\! s_u) \Indicator(r_v \!=\! s_v \!=\! x_v \forall v \!<\! u) \Indicator(r_v \!=\! s_v \!=\! y_v \forall v \!>\! u) \TV{f_{\rvx_t | \rvbx_{-t} = \boldsymbol{r}_{-t}}}{f_{\rvx_t | \rvbx_{-t} = \boldsymbol{s}_{-t}}} \!} \\
    & \sless{\cref{eq_dob_tv_bound}}  \Expectation_{\pi^*_{\rvby_{-t}, \rvbx_{-t}}} \Bigbrackets{\!\!\!\!\! \sum_{u \in [p] \setminus \{t\}} \!\!\!\!\! \ParameterTU[tu] \Indicator(\rvy_u \neq \rvx_u)} = \sum_{u \in [p] \setminus \{t\}} \!\!\!\!\! \ParameterTU[tu] \Probability_{\pi^*}(\rvy_u \neq \rvx_u), \label{eq_bayes_rule_step_2}
\end{align}
where $(a)$ follows by the construction of the coupling between $\rvby'$ and $\rvbx'$, and $(b)$ follows by triangle inequality. Putting together \cref{eq_bayes_rule_step_1,eq_bayes_rule_step_2}, we have
\begin{align}
    \Probability_{\pi'}(\rvy_t' \neq \rvx_t') \leq  
    \frac{1}{p} \sum_{u \in [p] \setminus \{t\}} \ParameterTU[tu] \Probability_{\pi^*}(\rvy_u \neq \rvx_u) + \Bigparenth{1-\frac{1}{p}} \Probability_{\pi^*}(\rvy_t \neq \rvx_t). \label{eq_prob_coupled_bound}
\end{align}
Next, we use \cref{eq_prob_coupled_bound} to show contraction of $\Gamma$. We have
\begin{align}
    W^2_2(g_{\rvby} \Gamma, f_{\rvbx} \Gamma)  \sless{(a)} \sump \Bigbrackets{\Probability_{\pi'}(\rvy_t' \neq \rvx_t')}^2  & \sless{\cref{eq_prob_coupled_bound}} \sump \biggbrackets{\frac{1}{p} \sum_{j \in [p] \setminus \{i\}} \ParameterTU[tu] \Probability_{\pi^*}(\rvy_j \neq \rvx_j) + \Bigparenth{1-\frac{1}{p}} \Probability_{\pi^*}(\rvy_t \neq \rvx_t)}^2\\
    & \sless{(b)} \bopnorm{\Bigparenth{1-\frac{1}{p}} I + \frac{1}{p} \ParameterMatrix}^2 \sump \Bigbrackets{\Probability_{\pi^*}(\rvy_t \neq \rvx_t)}^2 \\
    & \sequal{(c)} \bopnorm{\Bigparenth{1-\frac{1}{p}} I + \frac{1}{p} \ParameterMatrix}^2 W^2_2(g_{\rvby}, f_{\rvbx})\\
    & \sless{(d)} \biggparenth{\Bigparenth{1-\frac{1}{p}} + \frac{1}{p} \opnorm{\ParameterMatrix}}^2 W^2_2(g_{\rvby}, f_{\rvbx}),\label{eq_coupling_bound_2_interim}
    %
\end{align}
where $(a)$ follows from \cref{def_w2_distance}, $(b)$ follows by some linear algebraic manipulations, $(c)$ follows from \cref{def_w2_distance} and \cref{eq_opt_coupling_2}, and $(d)$ follows from the triangle inequality. Then, contraction of $\Gamma$ follows by taking square root on both sides of \cref{eq_coupling_bound_2_interim}.


\paragraph{Case~(ii) ($\set\subset[p]$):} We can directly verify that the matrix $\ParameterMatrix_{\set} \defn \braces{\ParameterTU[tu]}_{t,u \in \set}$ is such that $\opnorm{\ParameterMatrix_{\set}} \leq \opnorm{\ParameterMatrix}$. Further, we note that for any $\svby_{\setC} \in \cX^{p-\normalabs{\set}}$, the random vector $\rvbx_{\set} | \rvbx_{\setC} = \svby_{\setC}$ with distribution $f_{\rvbx_{\set} | \rvbx_{\setC} = \svby_{\setC}}$ satisfies the Dobrushin's uniqueness condition (\cref{def_dobrushin_condition}) with coupling matrix $\ParameterMatrix_{\set}$. Then, by performing an analysis similar to the one above, we have
\begin{align}
    W_2\bigparenth{g_{\rvby_{\set} | \rvby_{\setC}}, f_{\rvbx_{\set} | \rvbx_{\setC}}} & \leq \frac{1}{\bigparenth{1-\opnorm{\ParameterMatrix_{\set}}}} \sqrt{\sumset \Expectation\Bigbrackets{\TV{g_{\rvy_t | \rvby_{-t} = \svby_{-t}}}{f_{\rvx_t | \rvbx_{-t} = \svby_{-t}}}^2  \Big| \rvby_{\setC} = \svby_{\setC}}} \\
    & \sless{(a)} \frac{1}{\bigparenth{1-\opnorm{\ParameterMatrix}}} \sqrt{\sumset \Expectation\Bigbrackets{\TV{g_{\rvy_t | \rvby_{-t} = \svby_{-t}}}{f_{\rvx_t | \rvbx_{-t} = \svby_{-t}}}^2  \Big| \rvby_{\setC} = \svby_{\setC}}}
\end{align}
where $(a)$ follows because $\frac{1}{\normalparenth{1-\opnorm{\ParameterMatrix_{\set}}}} \leq \frac{1}{\normalparenth{1-\opnorm{\ParameterMatrix}}}$. This completes the proof.


\subsection{Proof of \cref{thm_main_concentration}: \mainconcresultname}
\label{subsec_proof_main_concentration}
%
Fix a function $q : \cX^p \to \Reals$. Fix any pseudo derivative $\tnabla q$ for $q$ and any pseudo Hessian $\tnabla^2 q$ for $q$. To prove \cref{thm_main_concentration}, we bound the $p$-th moment of $q(\rvbx) - \Expectation\bigbrackets{q(\rvbx)}$ by certain norms of $\tnabla^2 q$ and $\Expectation_{\rvbx}\bigbrackets{\tnabla q(\rvbx)}$. To that end, first, we claim that in order to control the $p$-th moment of $q(\rvbx) - \Expectation\bigbrackets{q(\rvbx)}$, it is sufficient to control the $p$-th moment of $\twonorm{\nabla q(\rvbx)}$. Then, using \cref{eq:pseudo_Hessian}, we note that the $p$-th moment of $\twonorm{\nabla q(\rvbx)}$ is bounded by the $p$-th moment of $\stwonorm{\tnabla q(\rvbx)}$. Next, we claim that the $p$-th moment of $\stwonorm{\tnabla q(\rvbx)}$ is bounded by a linear combination of appropriate norms of $\tnabla^2 q$ and $\Expectation_{\rvbx}\bigbrackets{\tnabla q(\rvbx)}$. We formalize the claims below and divide the proof across \cref{subsec_proof_lemma_bounded_p_moment} and \cref{subsec_proof_eq_p_moment_of_2_norm_gradient}.

%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%


\begin{lemma}[\tbf{Bounded $p$-th moments of $q(\rvbx) - \Expectation\bigbrackets{q(\rvbx)}$ and $\stwonorm{\tnabla q(\rvbx)}$}]\label{lemma_bounded_p_moment}
If a random vector $\rvbx$ satisfies $\LSI{\rvbx}{\sigma^2}$, then for any arbitrary function $q :\cX^p \to \Reals$,
\begin{align}
    \moment{q(\rvbx) - \Expectation\bigbrackets{q(\rvbx)}}{p} \leq \sigma \sqrt{2p} \moment{\twonorm{\nabla q(\rvbx)}}{p} \qtext{for any $p \geq 2$}. \label{eq_moment_controlled_by_gradient}
\end{align}
Further, for any pseudo derivative $\tnabla q(\svbx)$ and any pseudo Hessian $\tnabla^2 q(\svbx)$ for $q$, and even $p \geq 2$,
\begin{align}
    \smoment{\stwonorm{\tnabla q(\rvbx)}}{p} \leq  2c \sigma \Bigparenth{\max_{\svbx \in \cX^p} \fronorm{\tnabla^2 q(\svbx)} + \sqrt{p}  \max_{\svbx \in \cX^p} \opnorm{\tnabla^2 q(\svbx)}} + 4 \stwonorm{\Expectation_{\rvbx}\bigbrackets{\tnabla q(\rvbx)}} 
    %
    \label{eq_p_moment_of_2_norm_gradient} 
\end{align}
where $c \geq 0$ is a universal constant.
%
%
%
%
\end{lemma}

%
%
%
%
%
%
%
%
%
%


\noindent Given these lemmas, we proceed to prove \cref{thm_main_concentration}. Combining \cref{eq_moment_controlled_by_gradient,eq_p_moment_of_2_norm_gradient} for any even $p \geq 2$, there exists a universal constant $c'$ such that
\begin{align}
    \moment{q(\rvbx) - \Expectation\bigbrackets{q(\rvbx)}}{p} \leq c' \sigma^2  \Bigparenth{\sqrt{p}  \max_{\svbx \in \cX^p} \fronorm{\tnabla^2 q(\svbx)} + p \max_{\svbx \in \cX^p} \opnorm{\tnabla^2 q(\svbx)} + \sqrt{p}  \stwonorm{\Expectation_{\rvbx}\bigbrackets{\tnabla q(\rvbx)}}}. \label{eq_lemma_bounded_moments_combined}
\end{align}
Now, we complete the proof by using \cref{eq_lemma_bounded_moments_combined} along with Markov's inequality for a specific choice of $p$. For any even $p \geq 2$, we have 
\begin{align}
    & \Probability\Bigbrackets{\bigabs{q(\rvbx) - \Expectation\bigbrackets{q(\rvbx)}} > ec' \sigma^2  \Bigparenth{\sqrt{p}  \max_{\svbx \in \cX^p} \fronorm{\tnabla^2 q(\svbx)} + p \max_{\svbx \in \cX^p} \opnorm{\tnabla^2 q(\svbx)} + \sqrt{p}  \stwonorm{\Expectation_{\rvbx}\bigbrackets{\tnabla q(\rvbx)}}}} \\
    & = \Probability\Bigbrackets{\bigabs{q(\rvbx) - \Expectation\bigbrackets{q(\rvbx)}}^p > \Bigparenth{ec' \sigma^2}^p  \Bigparenth{\sqrt{p}  \max_{\svbx \in \cX^p} \fronorm{\tnabla^2 q(\svbx)} + p \max_{\svbx \in \cX^p} \opnorm{\tnabla^2 q(\svbx)} + \sqrt{p}  \stwonorm{\Expectation_{\rvbx}\bigbrackets{\tnabla q(\rvbx)}}}^p} \\
    & \sless{(a)}  \dfrac{\Expectation{\Bigbrackets{\bigabs{q(\rvbx) - \Expectation\bigbrackets{q(\rvbx)}}^p}}}{\Bigparenth{ec' \sigma^2}^p  \Bigparenth{\sqrt{p}  \max_{\svbx \in \cX^p} \fronorm{\tnabla^2 q(\svbx)} + p \max_{\svbx \in \cX^p} \opnorm{\tnabla^2 q(\svbx)} + \sqrt{p}  \stwonorm{\Expectation_{\rvbx}\bigbrackets{\tnabla q(\rvbx)}}}^p} \sless{\cref{eq_lemma_bounded_moments_combined}} e^{-p},
\end{align}
where $(a)$ follows from Markov's inequality. The proof is complete by choosing an appropriate universal constant $c''$, and and performing basic algebraic manipulations after letting
%
%
%
%
\begin{align}
  p = \frac{1}{c''\sigma^2}\min \Bigparenth{\dfrac{\varepsilon^2}{ \Expectation\bigbrackets{\stwonorm{\tnabla q(\rvbx)}}^2 + \max\limits_{\svbx \in \cX^p} \fronorm{\tnabla^2 q(\svbx)}^2}, \dfrac{\varepsilon}{\max\limits_{\svbx \in \cX^p} \opnorm{\tnabla^2 q(\svbx)}}}.
\end{align}
We note that a even $p \geq 2$ can be ensured by choosing appropriate $c''$.

\subsubsection{Proof of \cref{lemma_bounded_p_moment}~ \cref{eq_moment_controlled_by_gradient}: {Bounded $p$-th moment of $q(\rvbx) - \Expectation\bigbrackets{q(\rvbx)}$}}
\label{subsec_proof_lemma_bounded_p_moment}
Fix any $p \geq 2$. We start by using the following result from \cite[Theorem 3.4]{AidaS1994} since $\rvbx$ satisfies $\LSI{\rvbx}{\sigma^2}$:
\begin{align}
    \moment{q(\rvbx) - \Expectation\bigbrackets{q(\rvbx)}}{p}^2 \leq & \moment{q(\rvbx) - \Expectation\bigbrackets{q(\rvbx)}}{2}^2 + 2\sigma^2 (p-2)   \moment{\twonorm{\nabla q(\rvbx)}}{p}^2. \label{eq_aida_stroock}
\end{align}
Then, we bound the first term in \cref{eq_aida_stroock} by using the fact that logarithmic Sobolev inequality implies Poincare inequality with the same constant:
\begin{align}
    \moment{q(\rvbx) - \Expectation\bigbrackets{q(\rvbx)}}{2}^2 = \Variance(q(\rvbx)) \leq \sigma^2  \Expectation_{\rvbx}\Bigbrackets{\twonorm{\nabla q(\rvbx)}^2}. \label{eq_poincare}
\end{align}
Putting together \cref{eq_aida_stroock,eq_poincare}, we have
\begin{align}
    \moment{q(\rvbx) - \Expectation\bigbrackets{q(\rvbx)}}{p}^2 & \leq  \sigma^2 \Expectation_{\rvbx}\Bigbrackets{\twonorm{\nabla q(\rvbx)}^2} + 2\sigma^2 (p-2)   \moment{\twonorm{\nabla q(\rvbx)}}{p}^2 \\
    & \sless{(a)}  \sigma^2  \Bigparenth{\Expectation_{\rvbx}\Bigbrackets{\twonorm{\nabla q(\rvbx)}^p}}^{2/p} + 2\sigma^2 (p-2)   \moment{\twonorm{\nabla q(\rvbx)}}{p}^2 \\
    & \sequal{(b)} \sigma^2  \moment{\twonorm{\nabla q(\rvbx)}}{p}^2 + 2\sigma^2 (p-2)   \moment{\twonorm{\nabla q(\rvbx)}}{p}^2 \leq 2\sigma^2 p \moment{\twonorm{\nabla q(\rvbx)}}{p}^2, \label{eq_aida_stroock_simplified}
\end{align}
where $(a)$ follows by Jensen's inequality and $(b)$ follows by the definition of $p$-th moment. Taking square root on both sides of \cref{eq_aida_stroock_simplified} completes the proof.

\subsubsection{Proof of \cref{lemma_bounded_p_moment}~ \cref{eq_p_moment_of_2_norm_gradient}: {Bounded $p$-th moment of $\stwonorm{\tnabla q(\rvbx)}$}}
\label{subsec_proof_eq_p_moment_of_2_norm_gradient}
Fix any even $p \geq 2$. Fix any pseudo derivative $\tnabla q$ and any pseudo Hessian $\tnabla^2 q$. We start by obtaining a convenient bound on $\stwonorm{\tnabla q(\svbx)}$ for every $\svbx \in \cX^p$ and then proceed to bound the $p$-th moment of $\stwonorm{\tnabla q(\rvbx)}$.\\

\noindent Consider a $p$-dimensional standard normal random vector $\rvb{g}$ independent of $\rvbx$. For a given $\rvbx = \svbx \in \cX^p$, the random variable $\dfrac{\tnabla q(\svbx)\tp \rvb{g}}{\stwonorm{\tnabla q(\svbx)}}$ is a standard normal random variable. 
%
%
%
%
Then, for every $\svbx \in \cX^p$, we have
\begin{align}
    \moment{\frac{\tnabla q(\svbx)\tp \rvb{g}}{\stwonorm{\tnabla q(\svbx)}}}{p} \sequal{(a)} \biggparenth{\Expectation_{\rvb{g} | \rvbx = \svbx }\biggbrackets{\biggparenth{\frac{\tnabla q(\svbx)\tp \rvb{g}}{\stwonorm{\tnabla q(\svbx)}}}^p}}^{1/p} \sgreat{(b)} \frac{\sqrt{p}}{2}. \label{eq_normal_application}
\end{align}
where $(a)$ follows from the definition of $p$-th moment, and $(b)$ follows since $\moment{\rv{g}}{p} \geq \frac{\sqrt{p}}{2}$ for any standard normal random variable $\rv{g}$ and even $p \geq 2$. Rearranging \cref{eq_normal_application}, we have
\begin{align}
  \stwonorm{\tnabla q(\svbx)} \leq \frac{2}{\sqrt{p}} \Bigparenth{\Expectation_{\rvb{g}| \rvbx = \svbx}\Bigbrackets{\bigparenth{\tnabla q(\svbx)\tp \rvb{g}}^p}}^{1/p}. \label{eq_bound_fixed_gradient}
\end{align}
Now, we proceed to bound the $p$-th moment of $\stwonorm{\tnabla q(\rvbx)}$ as follows:
\begin{align}
    \smoment{\stwonorm{\tnabla q(\rvbx)}}{p}  \sequal{(a)} \Bigparenth{\Expectation_{\rvbx} \bigbrackets{\stwonorm{\tnabla q(\rvbx)}^p}}^{1/p} & \sless{\cref{eq_bound_fixed_gradient}} \frac{2}{\sqrt{p}} \Bigparenth{\Expectation_{\rvbx, \rvb{g}}\Bigbrackets{\bigparenth{\tnabla q(\rvbx)\tp \rvb{g}}^p}}^{1/p} \\
    & \sequal{(b)} \frac{2}{\sqrt{p}} \moment{\tnabla q(\rvbx)\tp \rvb{g}}{p} \\
    & \sless{(c)} \frac{2}{\sqrt{p}} \Bigparenth{\!\moment{\tnabla q(\rvbx)\tp \rvb{g} \!-\! \Expectation_{\rvbx}\bigbrackets{\tnabla q(\rvbx)\tp \rvb{g}}}{p} \!+\! \moment{\Expectation_{\rvbx}\bigbrackets{\tnabla q(\rvbx)\tp \rvb{g}}}{p}\!} \label{eq_bound_gradient_minkowski}
\end{align}
where $(a)$ and $(b)$ follow from the definition of $p$-th moment and $(c)$ follows by Minkowski's inequality. We claim that
\begin{align}
    \moment{\tnabla q(\rvbx)\tp \rvb{g} - \Expectation_{\rvbx}\bigbrackets{\tnabla q(\rvbx)\tp \rvb{g}}}{p} & \leq c \sigma \Bigparenth{\sqrt{p} \max_{\svbx \in \cX^p} \fronorm{\tnabla^2 q(\svbx)} + p \max_{\svbx \in \cX^p} \opnorm{\tnabla^2 q(\svbx)}}, \qtext{and} \label{eq_bound_first_term_minkowski}\\
    \moment{\Expectation_{\rvbx}\bigbrackets{\tnabla q(\rvbx)\tp \rvb{g}}}{p} & \leq 2\sqrt{p} \twonorm{\Expectation_{\rvbx}\bigbrackets{\tnabla q(\rvbx)}}, \label{eq_bound_second_term_minkowski}
\end{align}
where $c \geq 0$ is a universal constant.
%
%
%
%
%
%
%
%
Putting together \cref{eq_bound_gradient_minkowski,eq_bound_first_term_minkowski,eq_bound_second_term_minkowski} completes the proof. It remains to prove our claims \cref{eq_bound_first_term_minkowski,eq_bound_second_term_minkowski} which we now do one-by-one.

\paragraph{Proof of bound \cref{eq_bound_first_term_minkowski}:} We start by obtaining a bound on $\bigparenth{\Expectation_{\rvbx | \rvb{g} = \svb{g}} \bigbrackets{\bigparenth{\tnabla q(\rvbx)\tp \svb{g} - \Expectation_{\rvbx| \rvb{g} = \svb{g}}\bigbrackets{\tnabla q(\rvbx)\tp \svb{g}}}^p}}^{1/p}$ for every $\rvb{g} = \svb{g}$, and then proceed to bound $\smoment{\tnabla q(\rvbx)\tp \rvb{g} - \Expectation_{\rvbx}\bigbrackets{\tnabla q(\rvbx)\tp \rvb{g}}}{p}$.\\

\noindent To that end, we define $h_{\svb{g}}(\rvbx) \defn \tnabla q(\rvbx)\tp \svb{g} - \Expectation_{\rvbx| \rvb{g} = \svb{g}}\bigbrackets{\tnabla q(\rvbx)\tp \svb{g}}$ and observe that $\Expectation_{\rvbx| \rvb{g} = \svb{g}}\bigbrackets{h_{\svb{g}}(\rvbx)} = 0$. Now, applying \cref{lemma_bounded_p_moment}~\cref{eq_moment_controlled_by_gradient} to $h_{\svb{g}}(\cdot)$, we have
\begin{align}
    \moment{h_{\svb{g}}(\rvbx)}{p} 
    \leq \sigma \sqrt{2p} \Bigparenth{\Expectation_{\rvbx| \rvb{g} = \svb{g}} \Bigbrackets{\twonorm{\nabla h_{\svb{g}}(\rvbx)}^p}}^{1/p} & \sless{(a)} \sigma \sqrt{2p} \Bigparenth{\Expectation_{\rvbx| \rvb{g} = \svb{g}} \Bigbrackets{\twonorm{\nabla \bigbrackets{\svb{g} \tp \tnabla q(\rvbx)}}^p}}^{1/p} \\
     & \sless{\cref{eq:pseudo_Hessian}} \sigma \sqrt{2p} \Bigparenth{\Expectation_{\rvbx| \rvb{g} = \svb{g}} \Bigbrackets{\twonorm{\svb{g}\tp \tnabla^2 q(\rvbx)}^p}}^{1/p}, \label{eq_first_term_minkowski_0}
    %
\end{align}
where $(a)$ follows from the definition of $h_{\svb{g}}(\rvbx)$. Now, to obtain a bound on the RHS of \cref{eq_first_term_minkowski_0}, we further fix $\rvbx = \svbx$. Then, we let $\rvb{g}'$ be another $p$-dimensional standard normal vector and apply an inequality similar to \cref{eq_bound_fixed_gradient} to $\svb{g}\tp \tnabla^2 q(\svbx)$ obtaining
\begin{align}
  \twonorm{\svb{g}\tp \tnabla^2 q(\svbx)} \leq \frac{2}{\sqrt{p}} \Bigparenth{\Expectation_{\rvb{g}'| \rvbx = \svbx, \rvb{g} = \svb{g}}\Bigbrackets{\Bigparenth{\svb{g}\tp \tnabla^2 q(\svbx) \rvb{g}'}^p}}^{1/p}, \label{eq_first_term_minkowski_1}
\end{align}
which implies
\begin{align}
 \Bigparenth{\Expectation_{\rvbx | \rvb{g} = \svb{g}} \Bigbrackets{\twonorm{\svb{g}\tp \tnabla^2 q(\rvbx)}^p}}^{1/p} \leq \frac{2}{\sqrt{p}} \Bigparenth{\Expectation_{\rvbx, \rvb{g}' |\rvb{g} = \svb{g}}\Bigbrackets{\Bigparenth{\nabla \svb{g}\tp \tnabla^2 q(\rvbx) \rvb{g}'}^p}}^{1/p}. \label{eq_first_term_minkowski_2}
\end{align}
Putting together \cref{eq_first_term_minkowski_0,eq_first_term_minkowski_2}, and using the definition of $h_{\svb{g}}(\rvbx)$, we have
\begin{align}
     \Bigparenth{\Expectation_{\rvbx|\rvb{g} = \svb{g}} \Bigbrackets{\Bigparenth{\tnabla q(\rvbx)\tp \svb{g} - \Expectation_{\rvbx|\rvb{g} = \svb{g}}\Bigbrackets{\tnabla q(\rvbx)\tp \svb{g}}}^p}}^{1/p}
     \leq 2\sqrt{2}\sigma \Bigparenth{\Expectation_{\rvbx, \rvb{g}'|\rvb{g} = \svb{g}}\Bigbrackets{\Bigparenth{\svb{g}\tp \tnabla^2 q(\rvbx) \rvb{g}'}^p}}^{1/p}. \label{eq_bound_first_term_fixed_g}
\end{align}
Now, we proceed to bound $\smoment{\tnabla q(\rvbx)\tp \rvb{g} - \Expectation_{\rvbx}\bigbrackets{\tnabla q(\rvbx)\tp \rvb{g}}}{p}$ as follows:
\begin{align}
    \moment{\tnabla q(\rvbx)\tp \rvb{g} - \Expectation_{\rvbx}\Bigbrackets{\tnabla q(\rvbx)\tp \rvb{g}}}{p} & \sequal{(a)} \Bigparenth{\Expectation_{\rvbx, \rvb{g}} \Bigbrackets{\Bigparenth{\tnabla q(\rvbx)\tp \rvb{g} - \Expectation_{\rvbx}\Bigbrackets{\tnabla q(\rvbx)\tp \rvb{g}}}^p}}^{1/p} \\
    & \sless{\cref{eq_bound_first_term_fixed_g}} 2\sqrt{2}\sigma \Bigparenth{\Expectation_{\rvb{g}, \rvbx, \rvb{g}'}\Bigbrackets{\Bigparenth{\rvb{g}\tp \tnabla^2 q(\rvbx) \rvb{g}'}^p}}^{1/p}, \label{eq_bound_first_term_interim}
    %
    %
\end{align}
where $(a)$ follows from the definition of $p$-th moment. Finally, to bound the RHS of \cref{eq_bound_first_term_interim}, we fix $\rvbx = \svbx$ and bound the $p$-th norm of the quadratic form $\rvb{g}\tp \tnabla^2 q(\svbx) \rvb{g}'$ by the Hanson-Wright inequality resulting in
\begin{align}
    \Bigparenth{\Expectation_{\svb{g},  \rvb{g}' | \rvbx = \svbx}\Bigbrackets{\Bigparenth{\rvb{g}\tp \tnabla^2 q(\svbx) \rvb{g}'}^p}}^{1/p} & \leq c \Bigparenth{\sqrt{p} \fronorm{\tnabla^2 q(\svbx)} + p  \opnorm{\tnabla^2 q(\svbx)}}\\
    & \leq c \Bigparenth{\sqrt{p} \max_{\svbx \in \cX^p} \fronorm{\tnabla^2 q(\svbx)} + p \max_{\svbx \in \cX^p} \opnorm{\tnabla^2 q(\svbx)}}, \label{eq_hanson_wright}
\end{align}
where $c \geq 0$ is a universal constant. Then, \cref{eq_bound_first_term_minkowski} follows by putting together \cref{eq_bound_first_term_interim,eq_hanson_wright}.

%
%
%
%

\paragraph{Proof of bound \cref{eq_bound_second_term_minkowski}:} By linearity of expectation, we have
\begin{align}
    \smoment{\Expectation_{\rvbx}\bigbrackets{\tnabla q(\rvbx)\tp \rvb{g}}}{p} = \smoment{\bigparenth{\Expectation_{\rvbx}\bigbrackets{\tnabla q(\rvbx)}}\tp \rvb{g}}{p}. \label{eq_bound_second_term_minkowski_0}
\end{align}
We note that the random variable $\dfrac{\normalparenth{\Expectation_{\rvbx}\normalbrackets{\tnabla q(\rvbx)}}\tp \rvb{g}}{\stwonorm{\Expectation_{\rvbx}\normalbrackets{\tnabla q(\rvbx)}}}$ is a standard normal random variable. Therefore,
\begin{align}
    \moment{\frac{\bigparenth{\Expectation_{\rvbx}\bigbrackets{\tnabla q(\rvbx)}}\tp \rvb{g}}{\stwonorm{\Expectation_{\rvbx}\bigbrackets{\tnabla q(\rvbx)}}}}{p} \sequal{(a)} \biggparenth{\Expectation_{\rvb{g}}\biggbrackets{\biggparenth{\frac{\bigparenth{\Expectation_{\rvbx}\bigbrackets{\tnabla q(\rvbx)}}\tp \rvb{g}}{\stwonorm{\Expectation_{\rvbx}\bigbrackets{\tnabla q(\rvbx)}}}}^p}}^{1/p} \sless{(b)} 2\sqrt{p}, \label{eq_normal_application_2}
\end{align}
where $(a)$ follows from the definition of $p$-th moment, and $(b)$ follows since $\moment{\rv{g}}{p} \leq 2\sqrt{p}$ for any standard normal variable $\rv{g}$. Then, \cref{eq_bound_second_term_minkowski} follows by using \cref{eq_normal_application_2} in \cref{eq_bound_second_term_minkowski_0}.

%
\section{Identifying weakly dependent random variables}
\label{sec_conditioning_trick}
In \cref{section_lsi_tail_bounds}, we derived (in \cref{thm_LSI_main}) that a random vector (supported on a compact set) satisfies the logarithmic Sobolev inequality if it satisfies the Dobrushin's uniqueness condition (in \cref{def_dobrushin_condition}). Further, we also derived (\cref{thm_main_concentration}) tail bounds for a random vector satisfying the logarithmic Sobolev inequality. Combining the two, we see that in order to use the tail bound, the random vector needs to satisfy the Dobrushin's uniqueness condition, i.e, the elements of the random vector should be weakly dependent. In this section, we show that any random vector (outside Dobrushin's regime) that is a $\dGM$-Sparse Graphical Model (to be defined) can be reduced to satisfy the Dobrushin's uniqueness condition. In particular, we show that
%
by conditioning on a subset of the random vector, the unconditioned subset of the random vector (in the conditional distribution) are only weakly dependent. We exploit this trick in \cref{lemma_concentration_psi} and \cref{lemma_concentration_bar_psi} to enable application of the tail bound in  \cref{section_lsi_tail_bounds}. The result below is a generalization of the result in \cite{DaganDDA2021} for discrete random vectors to continuous random vectors.\\

\noindent We start by defining the notion of $\dGM$-Sparse Graphical Model. 
\begin{definition}[\tbf{$\dGM$-Sparse Graphical Model}]\label{def:tau_sgm}
A pair of random vectors $\braces{\rvbx, \rvbz}$ supported on $\cX^p \times \cZ^{p_z}$ is a $\dGM$-Sparse Graphical Model for model-parameters 
%
$\dGM \defn (\aGM, \eGM, \xmax, \ParameterMatrix)$
and denoted by $\tSGM$ if $\cX = \{-\xmax, \xmax\}$, and
\begin{enumerate}
    \item for any realization $\svbz \in \cZ^{p_z}$, the conditional probability distribution of $\rvbx$ given $\rvbz=\svbz$ is given by $\JointDistfun$ in \cref{eq_conditional_distribution_vay} for a vector $\ExternalField(\svbz) \in \real^p$ depending on $\svbz$ and a symmetric matrix $\ParameterMatrix \in \real^{p\times p} $ (independent of $\svbz$) with $\ParameterTU[tt]=0$ for all $t\in[p]$,
    \item $\max\braces{\max_{\svbz \in \cZ^{p_z}} \infnorm{\ExternalField(\svbz)}, \maxmatnorm{\ParameterMatrix}} \leq \aGM$, and
    \item $\infmatnorm{\ParameterMatrix} \leq \eGM$.
    %
\end{enumerate}
\end{definition}


\noindent Now, we provide the main result of this section. 
%
\newcommand{\conditioningtrickresultname}{Identifying weakly dependent random variables}
\begin{proposition}[\tbf{\conditioningtrickresultname}]\label{lemma_conditioning_trick}
Given a pair of random vectors $\braces{\rvbx, \rvbz}$ supported on $\cX^p \times \cZ^{p_z}$ that is a $\tSGM$ (\cref{def:tau_sgm}) with $\dGM \defn (\aGM, \eGM, \xmax, \ParameterMatrix)$, and a scalar $\lambda \in (0, \eGM]$, there exists $\numindsets \defn 32  \eGM^2 \log 4p / \lambda^2$ subsets $\sets \subseteq [p]$ that satisfy the following properties:
\begin{enumerate}[label=(\alph*)]
    \item\label{item:cardinality_independence_set} For any $t \in [p]$, we have $\sum_{u=1}^{\numindsets} \Indicator(t \in \setU) = \ceils{{\lambda \numindsets }/({8\eGM})}$.
    \item \label{item:conditional_sgm_independence_set} For any $u \in [\numindsets]$, 
    \begin{enumerate}[label=(\roman*)]
        \item \label{item:conditional_sgm} the pair of random vectors $\braces{\rvbx_{\setU}, (\rvbx_{-\setU}, \rvbz)}$ correspond to a $\tSGM[1]$ with $\dGM_1 \defn (\aGM+2\xmax \eGM, \lambda, \xmax, \ParameterMatrix_{\setU})$  where $\ParameterMatrix_{\setU} \defn \braces{\ParameterTU[tv]}_{t,v \in \setU}$, and
        \item \label{item:conditional_sgm_dob} the random vector $\rvbx_{\setU}$ conditioned on $(\rvbx_{-\setU}, \rvbz)$ satisfies the Dobrushin's uniqueness condition (\cref{def_dobrushin_condition}) with coupling matrix $2\sqrt{2} \xmax^2 \ParameterMatrix_{\setU}$ whenever $\lambda \in \Big(0, \frac{1}{2\sqrt{2}\xmax^2}\Big]$ with $\opnorm{\ParameterMatrix_{\setU}} \leq \lambda$.
    \end{enumerate}
    %
    %
\end{enumerate}
\end{proposition}

%
%
%

\begin{proof}[Proof of \cref{lemma_conditioning_trick}: \conditioningtrickresultname]
We prove each part one-by-one.

\paragraph{Proof of part~\cref{item:cardinality_independence_set}:}
From \cref{def:tau_sgm}, for any realization $\svbz \in \cZ^{p_z}$, the conditional probability distribution of $\rvbx$ given $\rvbz=\svbz$ is given by $\JointDistfun$ in \cref{eq_conditional_distribution_vay} where $\ExternalField(\svbz) \in \real^p$ is a vector and $\ParameterMatrix \in \real^{p\times p} $ is a symmetric matrix with $\ParameterTU[tt]=0$ for all $t\in[p]$ and $\infmatnorm{\ParameterMatrix} \leq \eGM$. Consider the matrix $A \defn \frac{1}{\eGM} \ParameterMatrix$. Since $A$ has zeros on the diagonal and $\infmatnorm{A} \leq 1$, we can apply \citet[Lem.~12]{DaganDDA2021} on $A$ with $\eta = \frac{\lambda}{\eGM}$. Then part~\cref{item:cardinality_independence_set} follows directly from
\citet[Lem.~12.1]{DaganDDA2021}. 
%
%
%
%
%
%
%
%
    %
    %
    
    
\paragraph{Proof of part~\cref{item:conditional_sgm_independence_set}~\cref{item:conditional_sgm}:}
%
To prove this part, consider the conditional distribution of $\rvbx_{\setU}$ conditioned on $\rvbx_{-\setU} = \svbx_{-\setU}$ and $\rvbz = \svbz$ for any $u \in [\numindsets]$. We have
\begin{align}
    f_{\rvbx_{\setU} | \rvbx_{-\setU},\rvbz} (\svbx_{\setU} | \svbx_{-\setU}, \svbz; \ExternalField(\svbz), \ParameterMatrix) \propto \exp \biggparenth{\sum_{t \in \setU} \Bigparenth{\ExternalFieldt(\svbz) + 2\sum_{v \notin \setU} \ParameterTU[tv] x_{v}} x_t + \sum_{t \in \setU} \sum_{v \in \setU} \ParameterTU[tv] x_t x_v}. \label{eq_conditional_dist_xIj}
\end{align}
We can re-parameterize $f_{\rvbx_{\setU} | \rvbx_{-\setU},\rvbz} (\svbx_{\setU} | \svbx_{-\setU}, \svbz; \ExternalField(\svbz), \ParameterMatrix)$ in \cref{eq_conditional_dist_xIj} as follows
\begin{align}
    &f_{\rvbx_{\setU} | \rvbx_{-\setU},\rvbz} (\svbx_{\setU} | \svbx_{-\setU}, \svbz; \ConditioningField(\svbz, \svbx_{-\setU}), \ConditioningMatrix)  \propto \exp \Bigparenth{ \normalbrackets{\ConditioningField(\svbz, \svbx_{-\setU})}\tp \svbx_{\setU} + \svbx_{\setU}\tp\ConditioningMatrix \svbx_{\setU}}, \qtext{where}\\
    &\ConditioningField(\svbz, \svbx_{-\setU}) \in \Reals^{|\setU| \times 1},
    \qtext{with} \ConditioningFieldU(\svbz, \svbx_{-\setU}) \defn \ExternalFieldt(\svbz) +  2\sum_{k \notin \setU} \ParameterTU[tv] x_{k} 
    \qtext{for} t \in \setU, \qtext{and} \label{eq_parameter_mapping_0}\\
    &\ConditioningMatrix = \ConditioningMatrix\tp  \in \Reals^{|\setU| \times |\setU|}  
    \qtext{with}
    \ConditioningParameter[tv] \defn \ParameterTU[tv],
    \stext{and}
    \ConditioningParameter[tt] \!=\! 0 \stext{for all} t, v \in \setU.
     \label{eq_parameter_mapping}
\end{align}
Thus, to show that the random vector $\rvbx_{\setU}$ conditioned on $\rvbx_{-\setU}$ and $\rvbz$ corresponds to an $\tSGM[1]$ with $\dGM_1 \defn (\aGM+2\xmax \eGM, \lambda,\xmax, \ParameterMatrix_{\setU})$, it suffices to establish that
\begin{align}
\label{eq:suff_steps_sgm}
    \max\braces{\max_{\svbz \in \cZ^{p_z}} \infnorm{\ConditioningField(\svbz, \svbx_{-\setU})}, \maxmatnorm{\ConditioningMatrix}} \sless{(i)} \aGM+2\xmax \eGM \qtext{and} \infmatnorm{\ConditioningMatrix} \sless{(ii)} \lambda.
\end{align}
To establish~(i) in \cref{eq:suff_steps_sgm}, we note that
\begin{align}
\maxmatnorm{\ConditioningMatrix} & \sless{\cref{eq_parameter_mapping}} \maxmatnorm{\ParameterMatrix} \sless{(a)}  \aGM \qtext{and} \label{eq:phitheta_bound_0}\\
\infnorm{\ConditioningField(\svbz, \svbx_{-\setU})} & \sless{(b)} \infnorm{\ExternalField(\svbz)} + 2\max_{t \in \setU}\sonenorm{\ParameterRowt} \infnorm{\svbx} \sless{(c)} \infnorm{\ExternalField(\svbz)} + 2\xmax \infmatnorm{\ParameterMatrix} \sless{(d)} \aGM+2\xmax \eGM, \label{eq:phitheta_bound_1}
\end{align}
where $(a)$ and $(d)$ follow from \cref{def:tau_sgm}, $(b)$ follows from \cref{eq_parameter_mapping_0} and the triangle inequality, and $(c)$ follows from the definition of $\infmatnorm{\cdot}$ and \cref{def:tau_sgm}. Then, from \cref{eq:phitheta_bound_0} and \cref{eq:phitheta_bound_1}, we have
\begin{align}
    \max\braces{\max_{\svbz \in \cZ^{p_z}} \infnorm{\ConditioningField(\svbz, \svbx_{-\setU})}, \maxmatnorm{\ConditioningMatrix}} \leq \aGM+2\xmax \eGM
\end{align}
as claimed. Next, to establish~(ii) in \cref{eq:suff_steps_sgm}, we again apply  \citet[Lem.~12]{DaganDDA2021} on the matrix $A = \frac{1}{\eGM} \ParameterMatrix$ with $\eta = \frac{\lambda}{\eGM}$. Then, \citet[Lem.~12.2]{DaganDDA2021} implies that
\begin{align}
\sum_{v \in \setU} \biggabs{\frac{\ParameterTU[tv]}{\eGM}} \leq \frac{\lambda}{\eGM} 
\qtext{for all $t \in \setU$, $u \in [\numindsets]$.}
\label{eq_lemma12_bound}
\end{align}
Therefore, we have
\begin{align}
    \infmatnorm{\ConditioningMatrix} = \max_{t \in \setU} \Bigparenth{\sum_{v \in \setU} \bigabs{\ConditioningParameter}} \sequal{\cref{eq_parameter_mapping}} \max_{t \in \setU} \Bigparenth{\sum_{v \in \setU} \bigabs{\ParameterTU[tv]}} \sless{\cref{eq_lemma12_bound}} \lambda, \label{eq_bound_Upsilon_inf_norm}
\end{align}   
as desired. The proof for this part is now complete.

\paragraph{Proof of part~\cref{item:conditional_sgm_independence_set}~\cref{item:conditional_sgm_dob}:}
We start by noting that the operator norm of a symmetric matrix is bounded by the infinity norm of the matrix. Then, from the analysis in part~\cref{item:conditional_sgm_independence_set}~\cref{item:conditional_sgm}, for any $u \in \setU$, we have
\begin{align}
    \opnorm{\ParameterMatrix_{\setU}} \leq \infmatnorm{\ParameterMatrix_{\setU}} \sequal{\cref{eq_parameter_mapping}} \infmatnorm{\ConditioningMatrix} \sless{\cref{eq_bound_Upsilon_inf_norm}} \lambda.
\end{align}
Therefore, $\infmatnorm{2\sqrt{2}\xmax^2 \ParameterMatrix_{\setU}} \leq 1$ whenever $\lambda \leq 1/2\sqrt{2}\xmax^2$. It remains to show %
that for every $u \in [\numindsets]$, $t \in \setU, v \in \setU \!\setminus\! \{t\}$, $\rvbz = \svbz$, and $\svbx_{-t}, \tsvbx_{-t} \in \cX^{p-1}$ differing only in the $v^{th}$ coordinate,
\begin{align}
    %
    \TV{f_{\rvx_t | \rvbx_{-t} = \svbx_{-t}, \rvbz = \svbz}}{f_{\rvx_t | \rvbx_{-t} = \tsvbx_{-t}, \rvbz = \svbz}} \leq 2\sqrt{2} \xmax^2 \ParameterTU[tv].
\end{align}
To that end, fix any $u \in [\numindsets]$, any $t \in \setU$, any $v \in \setU \!\setminus\! \{t\}$, any $\rvbz = \svbz$, and any $\svbx_{-t}, \tsvbx_{-t} \in \cX^{p-1}$ differing only in the $v^{th}$ coordinate. We have
\begin{align}
    \TV{f_{\rvx_t | \rvbx_{-t} = \svbx_{-t}, \rvbz = \svbz}}{f_{\rvx_t | \rvbx_{-t} = \tsvbx_{-t}, \rvbz = \svbz}}^2 & \sless{(a)} \frac{1}{2}\KLD{f_{\rvx_t | \rvbx_{-t} = \svbx_{-t}, \rvbz = \svbz}}{f_{\rvx_t | \rvbx_{-t} = \tsvbx_{-t}, \rvbz = \svbz}}\\
    & \sequal{(b)} \frac{1}{2} (2\ParameterTU[tv] x_v - 2 \ParameterTU[tv] \tx_v)^2 \xmax^2 \sless{(c)} 8\xmax^4 \ParameterTU[tv]^2,
\end{align}
where $(a)$ follows from Pinsker's inequality, $(b)$ follows by (i) applying 
\cite[Theorem 1]{BusaFSZ2019} to the exponential family parameterized as per $f_{\rvx_t | \rvbx_{-t}, \rvbz}$ in \cref{eq_conditional_dist}, (ii) noting that $f_{\rvx_t | \rvbx_{-t} = \svbx_{-t}, \rvbz =\svbz} \propto \exp\bigparenth{ \normalbrackets{\ExternalFieldt(\svbz) + 2\ParameterRowt\tp \svbx} x_t}$ and $f_{\rvx_t | \rvbx_{-t} = \tsvbx_{-t}, \rvbz =\svbz} \propto \exp\bigparenth{ \normalbrackets{\ExternalFieldt(\svbz) + 2\ParameterRowt\tp \tsvbx} x_t}$, and (iii) noting that the Hessian of the log partition function for any regular exponential family is the covariance matrix of the associated sufficient statistic which is bounded by $\xmax^2$ when $\cX = \{-\xmax, \xmax\}$, and $(c)$ follows because $x_v, \tx_v \in \{-\xmax, \xmax\}$. This completes the proof.
\end{proof}



%
%
%
%
%
%
%


\section{Supporting concentration results}
%
In this section, we provide a corollary of \cref{thm_main_concentration} that is used to prove the concentration results in \cref{lemma_concentration_psi} and \cref{lemma_concentration_bar_psi}. To show any concentration result for the random vector $\rvbx$ conditioned on $\rvbz$ via \cref{thm_main_concentration}, we need $\rvbx | \rvbz$ to satisfy the logarithmic Sobolev inequality (defined in \cref{eq_LSI_definition}). From \cref{thm_LSI_main}, for this to be true, we need the random vector $\rvx_t$ conditioned on $(\rvbx_{-t}, \rvbz)$ to satisfy the logarithmic Sobolev inequality for all $t \in [p]$. In the result below, we show this holds with a proof in \cref{proof_lsi_one_dim}. We define a $\tau \defeq (\aGM, \eGM, \xmax, \ParameterMatrix)$-dependent constant:
\begin{align}
     \cthree \defn  \exp{(\xmax(\aGM+ 2\eGM \xmax))}. \label{eq_constants_3_cont}
\end{align}

%
%
\newcommand{\lsionedimresultname}{Logarithmic Sobolev inequality for $\rvx_t | \rvbx_{-t}, \rvbz$}
\begin{lemma}[\tbf{\lsionedimresultname}]
\label{lemma_lsi_one_dim}
Given a pair of random vectors $\braces{\rvbx, \rvbz}$ supported on $\cX^p \times \cZ^{p_z}$ that is a $\tSGM$ (\cref{def:tau_sgm}) with $\dGM \defn (\aGM, \eGM, \xmax, \ParameterMatrix)$, $\rvx_t | \rvbx_{-t}, \rvbz$ satisfies $\mathrm{LSI}_{\rvx_t | \rvbx_{-t} = \svbx_{-t} , \rvbz = \svbz}\Big(\frac{8\xmax^2}{\pi^2} \cthree[2]\Big)$ for all $t \in [p]$, $\svbx_{-t} \in \cX^{p-1}$, and $\svbz \in \cZ^{p_z}$.

%
%
%
%
%
\end{lemma}

\noindent Now, we state the desired corollary of \cref{thm_main_concentration} with a proof in \cref{proof_of_coro}. The corollary makes use of some $\tau \defeq (\aGM, \eGM, \xmax, \ParameterMatrix)$-dependent constants:
\begin{align}
    \cfour \defn 1 + \aGM \xmax + 4\xmax^2 \eGM \qtext{and} \cfive \defn \frac{32\xmax^3\cthree[4]}{\pi^2}.\label{eq_constants_4_cont}
\end{align}

\begin{corollary}\label{coro}
Suppose a pair of random vectors $\braces{\rvbx, \rvbz}$ supported on $\cX^p \times \cZ^{p_z}$ corresponds to a $\tSGM$ (\cref{def:tau_sgm}) with $\dGM \defn (\aGM, \eGM, \xmax, \ParameterMatrix)$, and $\rvbx$ conditioned on $\rvbz$ satisfies the Dobrushin's uniqueness condition (\cref{def_dobrushin_condition}) with coupling matrix $\bParameterMatrix$. For any $\ExternalField, \bExternalField \in \ParameterSet_{\ExternalField}$ and $\ParameterMatrix \in \ParameterSet_{\ParameterMatrix}$, define the functions $q_1$ and $q_2$ as
\begin{align}
     q_1(\rvbx) \defeq \sump \normalparenth{\omt \rvx_t}^2
     \qtext{and}
     q_2(\rvbx) \defeq \sump \omt \rvx_t \exp\Bigparenth{-\normalbrackets{\ExternalFieldt + 2 \ParameterRowt\tp \rvbx} \rvx_t} \qtext{where}  \om =  \bExternalField - \ExternalField.
\end{align}
Then, for any $\varepsilon > 0$
\begin{align}
    \Probability\Bigbrackets{\bigabs{q_i(\rvbx) - \Expectation\bigbrackets{q_i(\rvbx) \big| \rvbz}} & \geq \varepsilon \Big| \rvbz} \leq \exp\biggparenth{ \dfrac{-c\bigparenth{1-\opnorm{\bParameterMatrix}}^4\varepsilon^2}{c_i \stwonorm{\om}^2}}
    \qtext{for} i = 1, 2, \label{eq_coro_combined}
    %
    %
    %
    %
\end{align}
where $c$ is a universal constant, $c_1 \defn 16 \aGM^2\xmax^2 \cfive[2]$, and $c_2 \defn \cthree[2] \ cseven[2] \cfive[2]$ with $\cthree$ defined in \cref{eq_constants_3_cont} and $\cfour$ and $\cfive$ defined in \cref{eq_constants_4_cont}.
\end{corollary}


\subsection{Proof of \cref{lemma_lsi_one_dim}: \lsionedimresultname}
\label{proof_lsi_one_dim}
Let $\rvu$ be the uniform distribution on $\cX$. Then, $\rvu$ satisfies $\mathrm{LSI}_{\rvu}\Big(\frac{8\xmax^2}{\pi^2}\Big)$ (see \citet[Corollary 2.4]{GhangMW2014}). Then, using the Holley-Stroock perturbation principle (see \citet[Page 31]{HS1986}, \citet[Lemma 1.2]{Ledoux2001}), for every $t\in [p]$, $\svbx_{-t} \in \cX^{p-1}$, and $\svbz \in \cZ^{p_z}$, $\rvx_t | \rvbx_{-t} = \svbx_{-t}, \rvbz = \svbz$ satisfies the logarithmic Sobolev inequality with a 
%
%
constant bounded by $$\frac{8\xmax^2 \exp(\sup_{x_t \in \cX} \psi(x_t; \svbx_{-t}, \svbz) - \inf_{x_t \in \cX} \psi(x_t; \svbx_{-t}, \svbz))}{\pi^2}$$ where $\psi(x_t; \svbx_{-t}, \svbz) \defn - \normalbrackets{\ExternalFieldt(\svbz) + 2\ParameterRowt\tp \svbx} x_t$. We have
\begin{align}
    \exp(\sup_{x_t \in \cX} \psi(x_t; \svbx_{-t}, \svbz) - \inf_{x_t \in \cX} \psi(x_t; \svbx_{-t}, \svbz))
    & \sequal{(a)} \!   \exp\bigparenth{ 2\bigabs{\ExternalFieldt(\svbz)\! +\! 2\ParameterRowt\tp \svbx} \xmax}\\ &\sless{(b)} \exp\big((2\aGM + 4\eGM\xmax)\xmax\big) \sequal{\cref{eq_constants_3_cont}} \cthree[2],
\end{align}
where $(a)$ follows from \cref{def:tau_sgm} and $(b)$ follows by using \cref{def:tau_sgm} along with triangle inequality and Cauchy–Schwarz inequality.


\subsection{Proof of \cref{coro}}
\label{proof_of_coro}
To apply \cref{thm_main_concentration} to the random vector $\rvbx$ conditioned on $\rvbz$, we need $\rvbx | \rvbz$ to satisfy the logarithmic Sobolev inequality. From \cref{thm_LSI_main}, this is true if (i) $f_{\min} = \min_{t \in [p], \svbx \in \cX^p, \svbz \in \cX^{p_z}} f_{\rvx_t | \rvbx_{-t}, \rvbz}(x_t | \svbx_{-t}, \svbz) > 0$ (see \cref{eq:smin}), (ii) $\rvbx | \rvbz$ satisfies the Dobrushin’s uniqueness condition, and (iii) $\rvx_t | \rvbx_{-t}, \rvbz$ satisfies the logarithmic Sobolev inequality for all $t \in [p]$. By assumption,  $\rvbx | \rvbz$ satisfies the Dobrushin’s uniqueness condition with coupling matrix $\bParameterMatrix$. From \cref{lemma_lsi_one_dim}, $\rvx_t | \rvbx_{-t}, \rvbz$ satisfies $\mathrm{LSI}_{\rvx_t | \rvbx_{-t} = \svbx_{-t} , \rvbz = \svbz}\Big(\frac{8\xmax^2\cthree[2]}{\pi^2}\Big)$. It remains to show that $f_{\min} > 0$. Consider any $t \in [p]$, any $\svbx \in \cX^p$, and any $\svbz \in \cX^{p_z}$. We have
\begin{align}
    f_{\rvx_t | \rvbx_{-t}, \rvbz}(x_t | \svbx_{-t}, \svbz) \sequal{(a)}  \frac{\exp\Bigparenth{\normalbrackets{\ExternalFieldt(\svbz) + 2\ParameterRowt\tp \svbx} x_t}}{
    \int_{\cX}
    \exp\Bigparenth{\normalbrackets{\ExternalFieldt(\svbz) + 2\ParameterRowt\tp \svbx} x_t} d x_t} & \sgreat{(b)}  \frac{\exp\Bigparenth{-\normalabs{\ExternalFieldt(\svbz) + 2\ParameterRowt\tp \svbx} \xmax}}{
    \int_{\cX}
    \exp\Bigparenth{\normalabs{\ExternalFieldt(\svbz) + 2\ParameterRowt\tp \svbx} \xmax} d x_t}\\
    & \sgreat{(c)}  \frac{\exp\Bigparenth{-\bigparenth{\normalabs{\ExternalField(\svbz)} + 2\sonenorm{\ParameterRowt} \sinfnorm{\svbx}} \xmax}}{
    \int_{\cX}
    \exp\Bigparenth{\bigparenth{\normalabs{\ExternalField(\svbz)} + 2\sonenorm{\ParameterRowt} \sinfnorm{\svbx}} \xmax} d x_t}\\
    & \sgreat{(d)} \frac{\exp\Bigparenth{-\normalparenth{\aGM + 2\eGM \xmax} \xmax}}{
    \int_{\cX}
    \exp\Bigparenth{\normalparenth{\aGM + 2\eGM \xmax} \xmax} d x_t} \sequal{(e)} \frac{1}{2\xmax \cthree[2]},
\end{align}
where $(a)$ follows from \cref{eq_conditional_dist}, $(b)$ and $(d)$ follow from \cref{def:tau_sgm}, $(c)$ follows by triangle inequality and Cauchy–Schwarz inequality, and $(e)$ follows because $\int_{\cX} dx_t = 2\xmax$. Therefore, $f_{\min} = \frac{1}{2\xmax \cthree[2]}$. Putting (i), (ii), and (iii) together, and using \cref{thm_LSI_main}, we see that $\rvbx | \rvbz$ satisfies $\mathrm{LSI}_{\rvbx}\Bigparenth{\frac{\cfive}{\normalparenth{1-\opnorm{\bParameterMatrix}}^2}}$ where $\cfive$ was defined in \cref{eq_constants_4_cont}. 

Now, we apply \cref{thm_main_concentration} to $q_1$ and $q_2$ one-by-one. The general strategy is to choose appropriate pseudo derivatives and pseudo Hessians for both $q_1$ and $q_2$, and evaluate the corresponding terms appearing in \cref{thm_main_concentration}.

\paragraph{Concentration for $q_1$:}
%
%
Fix any $\svbx \in \cX^p$. We start by decomposing $q_1(\svbx)$ as follows:
%
\begin{align}
    q_1(\svbx) = \bom \tp r(\svbx) \label{eq_decomposition_cont_quad}
\end{align}
where $\bom \defn (\omt[1]^2, \cdots, \omt[p]^2)$ and $r(\svbx) \defn (r_1(\svbx), \cdots, r_p(\svbx))$ with $r_t(\svbx) = x_t^2$ for every $t \in [p]$. Next, we define $H : \cX^p \to \Reals^{p \times p}$ such that 
\begin{align}
    H_{tu}(\svbx) = \frac{dr_u(\svbx)}{dx_t}  \qtext{for every $t,u \in [p]$.} \label{eq_H_matrix_cont_quad}
\end{align}

\paragraph{Pseudo derivative:}
We bound the $\ell_2$ norm of the gradient of $q_1(\svbx)$ as follows:
\begin{align}
    \twonorm{\nabla q_1(\svbx)}^2 = \sump \Bigparenth{\frac{d q_1(\svbx)}{dx_t}}^2 \sequal{\cref{eq_decomposition_cont_quad}} \sump \Bigparenth{\frac{\bom \tp d r(\svbx)}{dx_t}}^2  \sequal{\cref{eq_H_matrix_cont_quad}} \twonorm{H(\svbx) \bom}^2 & \sless{(a)} \opnorm{H(\svbx)}^2 \twonorm{\bom}^2 \\
    & \sless{(b)} \onematnorm{H(\svbx)}  \infmatnorm{H(\svbx)} \twonorm{\bom}^2 \label{eq:pseudo_derivative_cont_quad}
\end{align}
where $(a)$ follows because induced matrix norms are submultiplicative and $(b)$ follows because the matrix operator norm is bounded by square root of the product of matrix one norm and matrix infinity norm. Now, we claim that the one norm and the infinity norm of $H(\svbx)$ are bounded as follows:
\begin{align}
    \max\braces{\max_{\svbx \in \cX^p} \onematnorm{H(\svbx)}, \max_{\svbx \in \cX^p} \infmatnorm{H(\svbx)}} \leq 2\xmax. \label{eq_H_one_inf_bound_cont_quad}
\end{align}
Taking this claim as given at the moment, we continue with our proof. Combining \cref{eq:pseudo_derivative_cont_quad,eq_H_one_inf_bound_cont_quad}, we have 
\begin{align}
    \max_{\svbx \in \cX^p} \twonorm{\nabla q_1(\svbx)}^2 \leq 4\xmax^2 \twonorm{\bom}^2 = 4\xmax^2 \sump \omt^4 \leq 4\xmax^2 \maxp[u] \omt[u]^2 \sump \omt^2 \sless{(a)} 16 \xmax^2 \aGM^2 \twonorm{\om}^2,
\end{align}
where $(a)$ follows because $\om \in 2\ParameterSet_{\ExternalField}$. Therefore, we choose the pseudo derivative (see \cref{def_pseudo_der_hes}) as follows:
\begin{align}
    \tnabla q_1(\svbx) = 4\xmax \aGM \twonorm{\om}. \label{eq_chosen_pseudo_der_cont_quad}
\end{align}

\paragraph{Pseudo Hessian:}
Fix any $\rho \in \Reals$. We bound $\stwonorm{\nabla(\rho\tp \tnabla  q_1(\svbx))}^2$ (see \cref{def_pseudo_der_hes}) as follows:
\begin{align}
    \stwonorm{\nabla(\rho\tp \tnabla  q_1(\svbx))}^2 = \sump[u] \Bigparenth{\frac{d \rho\tp \tnabla  q_1(\svbx)}{dx_u}}^2 \sequal{\cref{eq_chosen_pseudo_der_cont_quad}} 0.
\end{align}
Therefore, we choose the pseudo Hessian (see \cref{def_pseudo_der_hes}) as follows:
\begin{align}
    \tnabla^2 q_1(\svbx) = 0. \label{eq_chosen_pseudo_Hess_cont_quad}
\end{align}
The concentration result in \cref{eq_coro_combined} for $q_1$ follows by applying \cref{thm_main_concentration} with the pseudo discrete derivative defined in \cref{eq_chosen_pseudo_der_cont_quad} and the pseudo discrete Hessian defined in \cref{eq_chosen_pseudo_Hess_cont_quad}.\\

%
%
%
%

\noindent It remains to show that the one-norm and the infinity-norm of $H(\svbx)$ are bounded as in \cref{eq_H_one_inf_bound_cont_quad}.
\paragraph{Bounds on the one-norm and the infinity-norm of $H(\svbx)$:} We have 
\begin{align}\label{eq_matrix_H_quad}
H_{tu}(\svbx) = 
    \begin{cases}
        2x_t \qtext{if} t = u, \\
        0 \qtext{otherwise.}
    \end{cases}
\end{align}
Therefore,
    \begin{align}
    \onematnorm{H(\svbx)} 
    & = \max_{u \in [p]} \sum_{t \in [p]} \normalabs{H_{tu}(\svbx)} \sless{\cref{eq_matrix_H_quad}} \max_{u \in [p]} 2 \normalabs{x_u} \sless{(a)} 2\xmax \qtext{and}\\
    \infmatnorm{H(\svbx)} 
    & = \max_{t \in [p]} \sum_{u \in [p]} |H_{tu}(\svbx)|  \sless{\cref{eq_matrix_H_quad}} \max_{t \in [p]} 2 \normalabs{x_t} \sless{(a)} 2\xmax,
  \end{align}
  where $(a)$ follows from \cref{def:tau_sgm}.

\paragraph{Concentration for $q_2$:}
%
%
Fix any $\svbx \in \cX^p$. We start by decomposing $q_2(\svbx)$ as follows:
%
\begin{align}
    q_2(\svbx) = \om\tp r(\svbx) \label{eq_decomposition_cont}
\end{align}
where $r(\svbx) \defn (r_1(\svbx), \cdots, r_p(\svbx))$ with $r_t(\svbx) = x_t \exp\bigparenth{-\normalbrackets{\ExternalFieldt + 2 \ParameterRowt\tp \svbx} x_t}$ for every $t \in [p]$. Next, we define $H : \cX^p \to \Reals^{p \times p}$ such that 
\begin{align}
    H_{tu}(\svbx) = \frac{dr_u(\svbx)}{dx_t}  \qtext{for every $t,u \in [p]$.} \label{eq_H_matrix_cont}
\end{align}

\paragraph{Pseudo derivative:}
We bound the $\ell_2$ norm of the gradient of $q_2(\svbx)$ as follows:
\begin{align}
    \twonorm{\nabla q_2(\svbx)}^2  = \sump \Bigparenth{\frac{d q_2(\svbx)}{dx_t}}^2 \sequal{\cref{eq_decomposition_cont}} \sump \Bigparenth{\frac{\om\tp d r(\svbx)}{dx_t}}^2 \sequal{\cref{eq_H_matrix_cont}} \twonorm{H(\svbx) \om}^2 & \sless{(a)} \opnorm{H(\svbx)}^2 \twonorm{\om}^2 \\
    & \sless{(b)} \onematnorm{H(\svbx)}  \infmatnorm{H(\svbx)} \twonorm{\om}^2 \label{eq:pseudo_derivative_cont}
\end{align}
where $(a)$ follows because induced matrix norms are submultiplicative and $(b)$ follows because the matrix operator norm is bounded by square root of the product of matrix one norm and matrix infinity norm. Now, we claim that the one norm and the infinity norm of $H(\svbx)$ are bounded as follows:
\begin{align}
    \max\braces{\max_{\svbx \in \cX^p} \onematnorm{H(\svbx)}, \max_{\svbx \in \cX^p} \infmatnorm{H(\svbx)}} \leq \cthree \cfour. \label{eq_H_one_inf_bound_cont}
\end{align}
where $\cthree$ and $\cfour$ were defined in \cref{eq_constants_3_cont} and \cref{eq_constants_4_cont} respectively. Taking this claim as given at the moment, we continue with our proof. Combining \cref{eq:pseudo_derivative_cont,eq_H_one_inf_bound_cont}, we have 
\begin{align}
    \max_{\svbx \in \cX^p} \twonorm{\nabla q_2(\svbx)}^2 \leq \cthree[2] \cfour[2] \twonorm{\om}^2.
\end{align}
Therefore, we choose the pseudo derivative (see \cref{def_pseudo_der_hes}) as follows:
\begin{align}
    \tnabla q_2(\svbx) = \cthree \cfour \twonorm{\om}. \label{eq_chosen_pseudo_der_cont}
\end{align}

\paragraph{Pseudo Hessian:}
Fix any $\rho \in \Reals$. We bound $\stwonorm{\nabla(\rho\tp \tnabla  q_2(\svbx))}^2$ (see \cref{def_pseudo_der_hes}) as follows:
\begin{align}
    \stwonorm{\nabla(\rho\tp \tnabla  q_2(\svbx))}^2 = \sump[u] \Bigparenth{\frac{d \rho\tp \tnabla  q_2(\svbx)}{dx_u}}^2 \sequal{\cref{eq_chosen_pseudo_der_cont}} 0.
\end{align}
Therefore, we choose the pseudo Hessian (see \cref{def_pseudo_der_hes}) as follows:
\begin{align}
    \tnabla^2 q_2(\svbx) = 0. \label{eq_chosen_pseudo_Hess_cont}
\end{align}
%
The concentration result in \cref{eq_coro_combined} for $q_1$ follows by applying \cref{thm_main_concentration} with the pseudo discrete derivative defined in \cref{eq_chosen_pseudo_der_cont} and the pseudo discrete Hessian defined in \cref{eq_chosen_pseudo_Hess_cont}.\\

%
%
%
%

\noindent It remains to show that the one-norm and the infinity-norm of $H(\svbx)$ are bounded as in \cref{eq_H_one_inf_bound_cont}.
\paragraph{Bounds on the one-norm and the infinity-norm of $H$:} We have 
\begin{align}\label{eq_matrix_H}
H_{tu}(\svbx) = 
    \begin{cases}
        \bigbrackets{1 - \normalbrackets{\ExternalFieldt[u] + 2\ParameterRowt[u]\tp \svbx} x_u}  \exp\bigparenth{-\normalbrackets{\ExternalFieldt[u] + 2\ParameterRowt[u]\tp \svbx} x_u} \qtext{if} t = u, \\
      -2\ParameterTU[tu] x_u^2 \exp\bigparenth{-\normalbrackets{\ExternalFieldt[u] + 2\ParameterRowt[u]\tp \svbx} x_u}  \qquad\qquad\qquad \qtext{otherwise.}
    \end{cases}
\end{align}
Therefore,
    \begin{align}
    \onematnorm{H(\svbx)} 
    & = \max_{u \in [p]} \sum_{t \in [p]} \normalabs{H_{tu}(\svbx)} \\
    & \sequal{\cref{eq_matrix_H}} \max_{u \in [p]} \bigabs{1 \!-\! \normalbrackets{\ExternalFieldt[u] \!+\! 2\ParameterRowt[u]\tp \svbx} x_u} \exp\bigparenth{\!-\!\normalbrackets{\ExternalFieldt[u] \!+\! 2\ParameterRowt[u]\tp \svbx} x_u} \!+\! 2\max_{u \in [p]} x_u^2  \exp\bigparenth{\!-\!\normalbrackets{\ExternalFieldt[u] \!+\! 2\ParameterRowt[u]\tp \svbx} x_u} \sum_{t \neq u}  \normalabs{\ParameterTU[tu]}\\
    & \sless{(a)} (1 + \aGM \xmax + 4\xmax^2 \eGM) \exp{(\xmax(\aGM+ 2\eGM \xmax))} \sequal{(b)}  \cthree \cfour
  \end{align}
  where $(a)$ follows from \cref{def:tau_sgm} along with triangle inequality and Cauchy–Schwarz inequality and $(b)$ follows from \cref{eq_constants_3_cont,eq_constants_4_cont}. Similarly, we have
\begin{align}
    \infmatnorm{H(\svbx)} 
    & = \max_{t \in [p]} \sum_{u \in [p]} |H_{tu}(\svbx)|  \\
    & \sequal{\cref{eq_matrix_H}} \max_{t \in [p]} \bigabs{1 \!-\! \normalbrackets{\ExternalFieldt \!+\! 2\ParameterRowt\tp \svbx} x_t} \exp\bigparenth{\!-\!\normalbrackets{\ExternalFieldt \!+\! 2\ParameterRowt\tp \svbx} x_t} \!+\! 2\max_{t \in [p]} \sum_{u \neq t}  \normalabs{\ParameterTU[tu]} x_u^2  \exp\bigparenth{\!-\!\normalbrackets{\ExternalFieldt[u] \!+\! 2\ParameterRowt[u]\tp \svbx} x_u} \\
    & \sless{(a)} (1 + \aGM \xmax + 4\xmax^2 \eGM) \exp{(\xmax(\aGM+ 2\eGM \xmax))} \sequal{(b)} \cthree \cfour
  \end{align}
  where $(a)$ follows from \cref{def:tau_sgm} along with triangle inequality and Cauchy–Schwarz inequality and $(b)$ follows from \cref{eq_constants_3_cont,eq_constants_4_cont}

%

%
%
%
%
%
%
%
%
%


 
%
%
%
%
%
%
%
%
%
%
%
%

%
%
%
%
%
%
%
%
%
%
%
%
%
    
    
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%

%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%

%
%
%

%
%
%
%
%
%
%
%
%
%
%
%
%
%


%

%
%
%
%
%
%
%
%
%
%
%

%
%
%
%
%
%
%
%
%

%
%
%
%
%
%
%
%
%
%
%
%
%
%

%
%
%
%
%
%
%
%

%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%

%
%
%
%
%
%
%
%
%

%
%
%
%

%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%

%

%
%
%
%
%
%
%
%
%
%
%
%
%
%
%

%
%
%
%
%
%
%
%
%

%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%

%
%
%
%
%
%
%
%
%

%
%
%
%

%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%

%

%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%

%
%
%
%
%
%
%
%

%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%

%
%
%
%
%
%
%
%
%

%
%
%
%

%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%

%

%

%
%
%
%
%
%
%
%
%
%
%
%

%
%
%
%
%
%
%
%

%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%

%
%
%
%
%
%
%
%
%

%
%
%
%

%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%

%
%
%

%
%
%
%
%
%
%
%
%
%
%

%
%
%
%
%
%
%
%
%
%
%
%

%
%
%
%
%
%
%
%
%
%
%
%

%
%
%
%
%
%
%
%

%

%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%

%

%
%
%
%
%
%
%
 
%
%
%
%
%
%
%
%
%



%
%
%

%
%
%
%
%
%

%

%
%
%
%
%

%
%
%
%
%
%
%
%

%
%
%
%

%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%

%
%
%
%
%
%
%
%
%

%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%


%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%

%
%

%

%
%
%
%
%
%
%
%
%
%
%
%
%
%

%

%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%

%

%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%




%

%
%
%
%
%
%
%
%
%

%
%
%
%
%
%
%
%
%

%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%


%
%
%
%
%
%




%
%
%
%
%
%
%
%
%
%
%
%
%



%
%

%
%
%
%
%
%
%
%
%
%

%
%
%
%
%
%
%
%
%
%
%
%
%
%
%

%
%
%
%
%
%
%


%
%
%
%
%
%
%
%
%
%
%
%
%
%
%




%

%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%

%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%

%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%

%
%
%
%

%
%
%
%
%
%
%
%
%


%
%
%
%
%
%
%
%
%
%
%
%
%
%
%

%
%
%
%
%
%
%


%
%
%
%
%
%
%
%
%
%
%
%
%
%
%




%

%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%

%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%

%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%

%
%
%
%

%
%
%
%
%
%
%
%
%
%
%
