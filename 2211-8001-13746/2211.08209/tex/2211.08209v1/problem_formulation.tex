\vsep

\section{Problem formulation}
\label{section_problem_formulation}
In this section, we formalize the problem, define the inference tasks of interest, and state our assumptions.

%
\subsection{Underlying causal mechanism}
We consider a counterfactual inference task where units go through $p_a \geq 1$ interventions. For every unit, we observe $p_y \geq 1$ outcomes of interest. The interventions and the outcomes could be confounded by $p_v \geq 1$ observed covariates as well as $p_z \geq 1$ unobserved covariates. Additionally, the observed covariates and the unobserved covariates could be arbitrarily associated. We denote the random vector associated with the interventions, the outcomes,  the observed covariates, and the unobserved covariates by $\rvba \defn (\rva_{1}, \cdots, \rva_{p_a}) \in \cA^{p_a}$, $\rvby = (\rvy_{1}, \cdots, \rvy_{p_y}) \in \cY^{p_y}$, $\rvbv \defn (\rvv_{1}, \cdots, \rvv_{p_v}) \in \cV^{p_v}$, and $\rvbz \defn (\rvz_{1}, \cdots, \rvz_{p_z})  \in \cZ^{p_z}$ respectively where $\cA, \cY, \cV$, and $\cZ$ denote the support of interventions, outcomes, observed covariates, and unobserved covariates respectively. We allow these sets to contain discrete, continuous, or mixed values. We summarize the causal relationship between the random vectors $\rvbz$, $\rvbv$, $\rvba$, and $\rvby$ in \cref{fig_graphical_models}(b) where we denote the arbitrary association between $\rvbz$ and $\rvbv$ by a bi-directed arrow, and the causal association between (i) $(\rvbz, \rvbv)$ and $\rvba$, (ii) $(\rvbz, \rvbv)$ and $\rvby$, and (iii) $\rvba$ and $\rvby$ by directed arrows. Our methodology works for any graphical model consistent with the  graphical model in \cref{fig_graphical_models}(b).
%
%

\subsubsection{Examples}
\begin{figure}[t]
    \centering
    \begin{tabular}{c}
    \includegraphics[height=0.25\linewidth,clip]{network_main_3.pdf} ~~
    \includegraphics[height=0.25\linewidth, clip]{network_explained_3.pdf}  \\
    %
    %
    %
    \end{tabular}
    \caption{A graphical model for the network setting with 4 users where $\rvv_t$, $\rva_t$, and $\rvy_t$ denote user $t$'s observed demographic factors, exposed product, and engagement level respectively, and $\rvbz$ denotes unobserved factors. The left plot
    %
    illustrates the dependency of the observed variables $(\rvv_t, \rva_t, \rvy_t)$ at user $t$ on the observed variables at user $u$ (a neighbor in the network) via $\blackdoublearrow$ , and on the unobserved covariates $\rvbz$ via $\reddoublearrow$; these dependencies for user $1$ and $2$ are expanded in the right plot.}
    %
    %
    %
    \label{fig_graphical_models_2}
    \vspace{-5mm}
\end{figure}

While \cref{fig_graphical_models}(b) exhibits the high-level causal links between $\rvbz$, $\rvbv$, $\rvba$, and $\rvby$, there could be complex low-level causal links between elements of these vectors. We do not assume any knowledge of such low-level causal links. In \cref{fig_graphical_models}(a), we provide an example for sequential recommender systems covered by our work where (i) $\rva_{t+1}$ depends on $\rva_{t}$ in addition to $\rvv_{t+1}$ and $\rvbz$, and (ii) $\rvy_{t+1}$ depends on $\rva_{t}$ and $\rvy_{t}$ in addition to $\rva_{t+1}$, $\rvv_{t+1}$ and $\rvbz$. Another classical example covered by our framework includes the network setting where a unit represents a social network where users are linked to each other by interpersonal relationships as shown in \cref{fig_graphical_models_2}. Similar to the sequential recommender system, every user is exposed to a product based on observed demographic factors as well as certain unobserved factors, and the user's engagement level is recorded. The engagement level of user $t$, i.e., $\rvy_{t}$, depends on the product exposed to its neighbor $u$, i.e., $\rva_{u}$, in addition to its observed demographic factors $\rvv_{t}$, its exposed exposed $\rva_{t}$, and unobserved factor $\rvbz$. Further, $\rvy_{t}$ could also be associated with $\rvy_{u}$. 


%
%
%
%

%
%

%
\subsubsection{Data} 
\label{subsubsec_data}
We are interested in answering counterfactual questions regarding $n$ independent but heterogeneous units in a population. To do so, we assume access to one observation of the observed covariates, the interventions, and the outcomes per unit, and index it by 
%
%
$i \in [n]$, i.e.,
%
$\svbv^{(i)}$, $\svba^{(i)}$, and $\svby^{(i)}$ denote the realizations of $\rvbv$, $\rvba$, and $\rvby$ for unit $i$ respectively. For every realized tuple $(\svbv^{(i)}, \svba^{(i)}, \svby^{(i)})$, there is a corresponding realization $\svbz^{(i)}$ of the unobserved covariates $\rvbz$ that is unobserved.

 \subsection{Potential outcomes framework}
 \label{subsec_po_framework}
%
We adopt the potential outcomes framework and denote the potential outcomes of unit $i \in [n]$ under interventions $\svba \in \cA^{p_a}$ by $\svby^{(i)}(\svba)$. We work under the stable unit treatment value assumption (SUTVA) where the potential outcomes of any unit $i$ are unaffected by the interventions at other units. In fact, we assume independence across units implying the potential outcomes of any unit $i$ are also unaffected by the covariates and the potential outcomes at other units. Then, the observations are related to the potential outcomes as $\svby^{(i)} =  \svby^{(i)}(\svba^{(i)}) $ for all $i \in [n]$.
%
%
%
 To establish equivalence between unit-level counterfactual distribution and unit-level conditional distribution, consider unit $i \in [n]$ and fix the observed covariates and the unobserved covariates at $\svbv^{(i)}$ and $\svbz^{(i)}$ respectively. Then, let $\wtil{\svby}^{(i)}$ be a realization of $\rvby$ when $\rvba = \wtil{\svba}^{(i)}$. We are interested in the distribution of the potential outcomes of unit $i$ for interventions $\wtil{\svba}^{(i)}$, i.e., the distribution of $\svby^{(i)}(\wtil{\svba}^{(i)}) | \rvbz = \svbz^{(i)}, \rvbv = \svbv^{(i)}$. Under the causal framework considered here (see \cref{fig_graphical_models}(b)), it is equivalent to the distribution of $\svby^{(i)}(\wtil{\svba}^{(i)}) | \rvba = \wtil{\svba}^{(i)}, \rvbz = \svbz^{(i)}, \rvbv = \svbv^{(i)}$ since $(\rvbz, \rvbv)$ satisfy ignorability \citep{Pearl2009, imbens2015causal}, i.e., the potential outcomes are independent of the interventions given $(\rvbz, \rvbv)$.  Further, under SUTVA, it is equivalent to the distribution of $\wtil{\svby}^{(i)} | \rvba = \wtil{\svba}^{(i)}, \rvbz = \svbz^{(i)}, \rvbv = \svbv^{(i)}$, i.e., $f_{\rvby | \rvba, \rvbz, \rvbv}(\rvby=\cdot | \rvba= \wtil{\svba}^{(i)}, \svbz^{(i)}, \svbv^{(i)})$. Therefore, our goal is to learn the $n$ unit-level conditional distributions in \cref{eq_set_conditional_distributions}. To that end, we model the joint distribution of the $\rvbz$, $\rvbv$, $\rvba$, and $\rvby$ belong to an exponential family.
 
%
%
 
 
%
%
 
 
%
 
%
 
%
 
%

%
 
 
%
%
%
%
%
%

%

%

%

%


%
%

%



%
%
%
%
%
%

%

%
%
%
%



\subsection{Modeling data as exponential family}
\label{subsec_exp_fam}
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
Let $\tilp \defn p_z + p_v + p_a + p_y$. We parameterize the joint probability distribution $f_{\ranvarvec}$ of the $\tilp$-dimensional random vector $\ranvarvec \defeq \normalparenth{\rvbz, \rvbv, \rvba, \rvby}$  by natural statistics $\rvbw$ and $\rvbw \rvbw\tp$, and by natural parameters $\phi \in \Reals^{\tilp \times 1}$ and $\Phi \in \Reals^{\tilp \times \tilp}$ as follows 
%
\begin{align}
    f_{\ranvarvec}(\varvec; \phi, \Phi) \propto \exp\Bigparenth{ \phi\tp \varvec
    +\varvec\tp \Phi \varvec},
    %
    %
    %
    \qtext{where}
    %
    %
    \varvec \defeq (\svbz, \svbv, \svba, \svby),
    \label{eq_joint_distribution_zvay}
\end{align}
and $\svbz \defn (z_{1}, \cdots, z_{p_z})$, $\svbv \defn (v_{1}, \cdots, v_{p_v})$, $\svba \defn (a_{1}, \cdots, a_{p_a})$, and $\svby \defn (y_{1}, \cdots, y_{p_y})$ denote realizations of $\rvbz$, $\rvbv$, $\rvba$, and $\rvby$ respectively. 
%
Without loss of generality, we can assume $\Phi$ to be a symmetric matrix. 
%
%
Then, 
%
recognizing that the (unit-level) conditional distribution of $\rvby$ conditioned on $\rvba = \svba$, $\rvbz = \svbz$, and $\rvbv = \svbv$ 
%
belongs to an exponential family with natural statistics $\rvby$ and $\rvby \rvby\tp$, we can write
\begin{align}
    f_{\rvby | \rvba, \rvbz, \rvbv}(\svby | \svba, \svbz, \svbv) \propto & \exp\Bigparenth{\bigbrackets{\phi^{(y)\tp} + 2\svbz\tp\Phi^{(z,y)} + 2\svbv\tp\Phi^{(v,y)} + 2\svba\tp\Phi^{(a,y)}} \svby + \svby\tp\Phi^{(y,y)} \svby}, \label{eq_conditional_distribution_y}
    %
\end{align}
where $\phi^{(y)} \in \Reals^{p \times 1}$ is the component of $\phi$ corresponding to $\rvby$ and $\Phi^{(u, y)} \in \Reals^{p_u \times p_y}$ is the component of $\Phi$ corresponding to $\rvbu$ and $\rvby$ for all $\rvbu \in \{\rvbz, \rvbv, \rvba, \rvby\}$. We make two key observations:
%
    %
    (a) The term $\Phi^{(z,y)\top}\!\svbz$ captures the effect of unobserved covariates $\svbz$ on $f_{\rvby | \rvba, \rvbz, \rvbv}(\rvby\!=\!\cdot | \rvba\!=\! \cdot, \svbz, \svbv)$ in \cref{eq_conditional_distribution_y}.
    %
%
%
%
%
 (b) The task of learning 
%
$f_{\rvby | \rvba, \rvbz, \rvbv}(\rvby=\cdot | \rvba= \cdot, \svbz, \svbv)$ in \cref{eq_conditional_distribution_y} as a function of $\rvba$ reduces to learning 
\begin{align}
    \stext{(i)} \phi^{(y)} + 2\Phi^{(z,y)\top} \svbz + 2\Phi^{(v,y)\top} \svbv, \qtext{} \stext{(ii)} \Phi^{(a,y)}, \qtext{and} \stext{(iii)} \Phi^{(y,y)}. \label{eq_actual_parameters_of_interest}
    %
\end{align}
%
%
Now, we argue that learning (i), (ii), and (iii) in \cref{eq_actual_parameters_of_interest} is subsumed in learning the parameters of the (unit-level) conditional distribution $f_{\rvbx |\rvbz}$ of the random vector $\rvbx \defeq \normalparenth{\rvbv, \rvba, \rvby}$ conditioned on $\rvbz = \svbz$. To that end, we recognize that $f_{\rvbx |\rvbz}$ belongs to an exponential family with natural statistics $\rvbx$ and $\rvbx \rvbx\tp$. For all $\rvbu \in \normalbraces{\rvbv, \rvba, \rvby}$, let $\phi^{(u)} \in \Reals^{p_u \times 1}$ be the component of $\phi$ corresponding to $\rvbu$, and $\Phi^{(z, u)} \in \Reals^{p_z \times p_u}$ be the component of $\Phi$ corresponding to $\rvbz$ and $\rvbu$. We parameterize $f_{\rvbx |\rvbz}$ as
%
%
%
\begin{align}
    \JointDist \!\propto\! \exp \Bigparenth{ \normalbrackets{\ExternalField(\svbz)}\tp \svbx \!+\! \svbx\tp \ParameterMatrix \svbx} \label{eq_conditional_distribution_vay} \stext{where}
    \ExternalField(\svbz)  \defeq \begin{bmatrix} \phi^{(v)} \!+\! 2 \Phi^{(z,v)\top} \svbz \\ \phi^{(a)} \!+\! 2 \Phi^{(z,a)\top} \svbz \\ \phi^{(y)} \!+\! 2 \Phi^{(z,y)\top} \svbz \end{bmatrix}\in \! \Reals^{p\times 1},\, \svbx \! \defeq \! \obsvarmat\!,
    %
    %
\end{align} 
$p \defn p_v + p_a + p_y$, $\Theta \in \Reals^{p \times p}$ denotes the component of $\Phi$ corresponding to $\rvbx$, 
%
and $\svbv$, $\svba$, and $\svby$ denote realizations of $\rvbv$, $\rvba$, and $\rvby$ respectively. Now, if we learn $\ExternalField(\svbz)$ and $\ParameterMatrix$, we can obtain (i), (ii), and (iii) in \cref{eq_actual_parameters_of_interest} for any $\rvbv = \svbv$ by using the appropriate components of $\ExternalField(\svbz)$ and $\ParameterMatrix$. 
%


\subsection{Inference tasks}
\label{subsec_inference_tasks_of_interest}
%
%
Let $f_{\ranvarvec}(\cdot; \phi^*, \Phi^*)$ denote the true data generating distribution (\cref{subsubsec_data}) of $\rvbw$, and let $\TrueJointDistfun$ denote the true distribution of $\rvbx$ conditioned on $\rvbz = \svbz$.
%
Then, for all $i \in [n]$, we note that the realization $\svbx^{(i)} \defn (\svbv^{(i)}, \svba^{(i)}, \svby^{(i)})$ is consistent with the conditional distribution $\TrueJointDistfun[(i)]$ where we do not observe $\svbz^{(i)}$. Our primary goal is to learn the $n$ unit-level counterfactual distributions, which, 
%
as per our earlier discussion
simplifies to estimating
\begin{align}
    \hspace{-1cm}\stext{(i) Unit-level parameters} \TrueExternalFieldI[i] \defn \TrueExternalField(\svbz^{(i)}) \stext{for} i \in [n], \stext{and (ii) Population-level parameter} \TrueParameterMatrix. \label{eq_parameters_of_interest}
\end{align}
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
Our secondary goal is to estimate the expected potential outcomes for any given unit $i$ (with $\rvbz = \svbz^{(i)}, \rvbv = \svbv^{(i)}$) 
and an alternate intervention $\wtil{\svba}^{(i)}$.
In particular, our estimand of interest is
\begin{align}
    \mu^{(i)}(\wtil{\svba}^{(i)}) \defn  \Expectation[\svby^{(i)}(\wtil{\svba}^{(i)}) | \rvbz = \svbz^{(i)}, \rvbv = \svbv^{(i)}].\label{eq_causal_estimand}
    %
\end{align}

%
%
%
%
%
%


%

%


\subsection{Assumptions}
%
%
We now state the assumptions we require to estimate the parameters in \cref{eq_parameters_of_interest} and the expected potential outcomes in \cref{eq_causal_estimand}.
%
%
%
For the ease of exposition, we let the sets $\cV$, $\cA$, and $\cY$ be continuous valued. However, our results apply equally to discrete and mixed cases.
%
%
Without loss of generality, we let these sets be equal, and let $\cX \defn \cV = \cA = \cY$ be bounded and symmetric around $0$, i.e., $\cX = \{-\xmax, \xmax\}$ where $\xmax < \infty$.
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
\begin{assumption}[Bounded and sparse parameters]
\label{assumptions}
The true model parameters satisfy the following:
\begin{enumerate}[label=(\alph*)]
    \item\label{assumption_bounded_parameters} $\TrueExternalFieldI[i]$ and $\TrueParameterMatrix$ are bounded for all $i \in [n]$,\footnote{This bound is necessary for model identifiability \citep{SanthanamW2012}.} i.e., $\max\braces{\max_{i \in [n]} \infnorm{\TrueExternalFieldI[i]}, \maxmatnorm{\TrueParameterMatrix}} \leq \aGM$.
    %
    %
    %
    %
    %
    %
    %
    %
    %
    %
    \item\label{assumption_sparse_parameters} $\TrueParameterMatrix$ is row-wise sparse and has zeros on the diagonal, i.e., $\zeronorm{\TrueParameterRowt} \leq \bGM$ and $\TrueParameterTU[tt] = 0$ for all $t \in [p]$. These imply that each $\rvx_t \in \rvbx$ interacts with only a few other $\rvx_u \in \rvbx$ in \cref{eq_conditional_distribution_vay}. 
\end{enumerate}
\end{assumption}
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
\noindent Below, we define the set $\ParameterSet_{\ExternalField}$ such that it contains all $p \times 1$ vectors $\ExternalField$ satisfying \cref{assumptions}\cref{assumption_bounded_parameters} and the set $\ParameterSet_{\ParameterMatrix}$ such that it contains all $p \times p$ symmetric matrices $\ParameterMatrix$ satisfying \cref{assumptions}\cref{assumption_bounded_parameters,assumption_sparse_parameters}.
\begin{align}
    \ParameterSet_{\ExternalField} & \defn  
    \braces{ \ExternalField  \in \Reals^{p \times 1}: \infnorm{\ExternalField} \leq \aGM} \stext{and} \label{eq_parameter_set_external_field}\\
    %
    %
    \ParameterSet_{\ParameterMatrix} & \defn  \braces{ \ParameterMatrix \in \Reals^{p \times p}: \ParameterMatrix = \ParameterMatrix\tp, \ParameterTU[tt] = 0 \stext{for all} t \in [p], \maxmatnorm{\ParameterMatrix} \leq \aGM, \maxp[t] \zeronorm{\ParameterRowt} \leq \bGM}. \label{eq_parameter_set}
\end{align}

%

%
%
