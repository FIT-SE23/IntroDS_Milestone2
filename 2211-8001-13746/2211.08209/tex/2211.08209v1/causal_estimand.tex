\section{Causal estimate and associated guarantees}
In this section, we provide our estimate of the expected potential outcomes under alternate intervention and the corresponding estimation error. 
\subsection{Causal estimate}
\label{subsec_causal_estimate}
Assuming the estimate $\ExtendedEstimatedParameterMatrix$ of $\ExtendedTrueParameterMatrix$ is given from 
%
\cref{sec_algo},
we define our estimate of the expected potential outcome $\mu^{(i)}(\wtil{\svba}^{(i)})$ (see \cref{eq_causal_estimand}) for any given unit $i \in [n]$ under an alternate intervention $\wtil{\svba}^{(i)} \in \cA^{p_a}$. First, we identify $\EstimatedPhi^{(u, y)} \in \Reals^{p_u \times p_y}$ to be the component of $\EstimatedParameterMatrix$ corresponding to $\rvbu$ and $\rvby$ for all $\rvbu \in \{\rvbv, \rvba, \rvby\}$ and $\EstimatedExternalFieldI[i,y] \in \Reals^{p_y}$ to be the component of $\EstimatedExternalFieldI$ corresponding to $\rvby$. Then, our estimate of the conditional distribution of $\rvby$ as a function of the interventions $\rvba$, while keeping $\rvbv$ and $\rvbz$ fixed at the corresponding realizations for unit $i$, i.e., $\svbv^{(i)}$ and $\svbz^{(i)}$ respectively, is as follows:
\begin{align}
    \what{f}^{(i)}_{\rvby | \rvba}(\svby | \svba) \propto \exp\Bigparenth{\bigbrackets{\EstimatedExternalFieldI[i,y] + 2\svbv^{(i)\top}\EstimatedPhi^{(v,y)} + 2\svba\tp\EstimatedPhi^{(a,y)}} \svby + \svby\tp\EstimatedPhi^{(y,y)} \svby}. \label{eq_counterfactual_distribution_y}
\end{align}
Finally, our estimate of $\mu^{(i)}(\wtil{\svba}^{(i)})$ is given by
\begin{align}
    %
    \what{\mu}^{(i)}(\wtil{\svba}^{(i)}) & \defn \Expectation_{\what{f}^{(i)}_{\rvby | \rvba}}[\rvby | \rvba = \wtil{\svba}^{(i)}].  \label{eq_causal_estimate}
\end{align}
\subsection{Guarantee on outcome estimate}
\label{subsec_guarantee_outcome_estimate}
\noindent The following result provides the estimation error for the estimate $\what{\mu}^{(i)}(\wtil{\svba}^{(i)})$ (see \cref{eq_causal_estimate}) of the expected potential outcomes for any unit $i \in [n]$ under an alternate intervention $\wtil{\svba}^{(i)} \in \cA^{p_a}$. The result requires the operator norm of the covariance matrix of $\rvby$ conditioned on $\rvba$, $\rvbz$, and $\rvbv$ to remain bounded for minor perturbation in the parameters. 
%
We denote this covariance matrix by $\Expectation_{\ExternalField,\ParameterMatrix}[\rvby \rvby\tp | \svba, \svbz, \svbv]$ where the expectation is with respect to the conditional distribution of $\rvby$ conditioned on $\rvba = \svba$, $\rvbz = \svbz$, and $\rvbv = \svbv$ which is fully parameterized by $\ExternalField$ and $\ParameterMatrix$, i.e., replace $\ExternalField(\svbz)$ by $\ExternalField$ in \cref{eq_conditional_distribution_vay}. For simplicity, we assume $p_v = p_a = p_y$. See the proof below for the general case. 
\begin{theorem}[\tbf{Guarantee on outcome estimate}]
\label{thm_causal_estimand}
Fix an $\varepsilon > 0$ and $\delta \in (0,1)$. Then, with probability at least $1-\delta$, the estimates $\sbraces{\what{\mu}^{(i)}(\wtil{\svba}^{(i)})}_{i=1}^n$~\cref{eq_causal_estimate} for any $\sbraces{\wtil{\svba}^{(i)} \in \cA^{p_a}}_{i=1}^n$ satisfy
\begin{align}
    %
    \max_{i\in[n]} \dfrac{\stwonorm{\mu^{(i)}(\wtil{\svba}^{(i)}) - \what{\mu}^{(i)}(\wtil{\svba}^{(i)})}}{\sup_{\ExternalField, \ParameterMatrix \in \mbb B_i} \opnorm{\Expectation_{\ExternalField,\ParameterMatrix}[\rvby \rvby\tp | \wtil{\svba}^{(i)}, \svbz^{(i)}, \svbv^{(i)}]}} 
    \leq  R(\varepsilon, \delta/n) + p \varepsilon,
     \end{align}
    whenever
    \begin{align}
   n \geq \frac{ce^{c'\bGM}  (\log \frac{np}{\delta} + \metric_{\ExternalField}( r)) }{\varepsilon^4},
\end{align}
where $R(\varepsilon, \delta)$ and $r$ were defined in \cref{eq_radius_node_thm}, and
\begin{align}
\mbb B_i \defeq \bigbraces{\ExternalField \in \Lambda_{\theta}: \stwonorm{\ExternalField \!-\! \TrueExternalFieldI} \leq R(\varepsilon, \delta/n)} \times \bigbraces{\ParameterMatrix \in \Lambda_{\Theta}: \max_{t\in[p]}\stwonorm{\ParameterRowt \!-\! \TrueParameterRowt} \leq \varepsilon}.
\end{align}
%
%
%
\end{theorem}

%
%
%
\newcommand{\hpsi}{\what{\psi}}
\newcommand{\tpsi}{\wtil{\psi}}
\newcommand{\spsi}{{\psi^{\star}}}
\newcommand{\hPsi}{\what{\Psi}}
\newcommand{\tPsi}{\wtil{\Psi}}
\newcommand{\sPsi}{{\Psi^{\star}}}
\begin{proof}
Fix any unit $i \in [n]$ and an alternate intervention $\wtil{\svba}^{(i)} \in \cA^{p_a}$. Then, we have
\begin{align}
    \mu^{(i)}(\wtil{\svba}^{(i)}) & \sequal{\cref{eq_causal_estimand}} \Expectation[\svby^{(i)}(\wtil{\svba}^{(i)}) | \rvbz = \svbz^{(i)}, \rvbv = \svbv^{(i)}] \sequal{(a)} \Expectation[\rvby | \rvba = \wtil{\svba}^{(i)}, \rvbz = \svbz^{(i)}, \rvbv = \svbv^{(i)}],
\end{align}
where $(a)$ follows because the unit-level counterfactual distribution is equivalent to unit-level conditional distribution under the causal framework considered as described in 
%
\cref{section_problem_formulation}. To obtain a convenient expression for $\Expectation[\rvby | \rvba = \wtil{\svba}^{(i)}, \rvbz = \svbz^{(i)}, \rvbv = \svbv^{(i)}]$, we identify $\TruePhi[u,y] \in \Reals^{p_u \times p_y}$ to be the component of $\TrueParameterMatrix$ corresponding to $\rvbu$ and $\rvby$ for all $\rvbu \in \{\rvbv, \rvba, \rvby\}$ and $\TrueExternalFieldI[i,y] \in \Reals^{p_y}$ to be the component of $\TrueExternalFieldI$ corresponding to $\rvby$. Then, the conditional distribution of $\rvby$ as a function of the interventions $\rvba$, while keeping $\rvbv$ and $\rvbz$ fixed at the corresponding realizations for unit $i$, i.e., $\svbv^{(i)}$ and $\svbz^{(i)}$ respectively, can be written as
\begin{align}
    {f^{(i)}}_{\rvby | \rvba}(\svby | \svba) \propto \exp\Bigparenth{\bigbrackets{\TrueExternalFieldI[i,y] + 2\svbv^{(i)\top}\TruePhi[v,y] + 2\svba\tp\TruePhi[a,y]} \svby + \svby\tp\TruePhi[y,y] \svby}. \label{eq_conditional_distribution_y_alternate}
\end{align}
Therefore, we have
\begin{align}
    \Expectation[\rvby | \rvba = \wtil{\svba}^{(i)}, \rvbz = \svbz^{(i)}, \rvbv = \svbv^{(i)}] = \Expectation_{{f^{(i)}}_{\rvby | \rvba}}[\rvby | \rvba = \wtil{\svba}^{(i)}].
\end{align}
\noindent Now, consider the $p_u$ dimensional random vector $\rvbu$ supported on $\cX^{p_u}$ with distribution $f_{\rvbu}$ parameterized by   $\psi \in \Reals^{p_y}$ and $\Psi \in \Reals^{p_y \times p_y}$ as follows
\begin{align}
    f_{\rvbu}(\svbu | \psi, \Psi) \propto \exp(\psi\tp \svbu+ \svbu\tp\Psi\svbu). \label{eq_dist_u} 
\end{align}
Then, note that $\what{f}^{(i)}_{\rvby | \rvba}(\svby | \svba)$ in \cref{eq_counterfactual_distribution_y} and ${f^{(i)}}_{\rvby | \rvba}(\svby | \svba)$ in \cref{eq_conditional_distribution_y_alternate} belong to the set $\normalbraces{f_{\rvbu}(\cdot | \psi, \Psi): \psi \in \Reals^{p_y}, \Psi \in \Reals^{p_y \times p_y}}$ for some $\psi$ and $\Psi$. Now, we consider any two distributions in this set, namely $f_{\rvbu}(\svbu | \hpsi, \hPsi)$ and $f_{\rvbu}(\svbu | \spsi, \sPsi)$. Then, we claim that the two norm of the difference of the mean vectors of these distributions is bounded as below. We provide a proof at the end.
\newcommand{\expfamperturbationresultname}{Perturbation in the mean vector}
\begin{lemma}[\tbf{\expfamperturbationresultname}]
\label{lemma_exp_fam_parameter_perturbation}
For any $\psi \in \Reals^{p_y}$ and $\Psi \in \Reals^{p_y \times p_y}$, let $\mu(\psi, \Psi) \in \Reals^{p_u}$ and $\Covariance(\psi, \Psi) \in \Reals^{p_u \times p_u}$ denote the mean vector and the covariance matrix of $\rvbu$ respectively with respect to $f_{\rvbu}$ in \cref{eq_dist_u}. Then, for any $\hpsi, \spsi \in \Reals^{p_y}$ and $\hPsi, \sPsi \in \Reals^{p_y \times p_y}$, there exists $t \in [0,1]$ such that
\begin{align}
    \stwonorm{ \mu(\hpsi, \hPsi) - \mu(\spsi, \sPsi)} \leq \opnorm{\Covariance(t \hpsi + (1-t) \spsi, t \hPsi + (1-t) \sPsi)} \stwonorm{\hpsi - \spsi}.
\end{align}
\end{lemma}
%
%
%
%
%
%
\noindent Given this lemma, we proceed with the proof. By applying this lemma to $\what{f}^{(i)}_{\rvby | \rvba}(\svby | \svba)$ in \cref{eq_counterfactual_distribution_y} and ${f^{(i)}}_{\rvby | \rvba}(\svby | \svba)$ in \cref{eq_conditional_distribution_y_alternate}, we see that it is sufficient to show the following bound
\begin{align}
     \stwonorm{\normalparenth{\TrueExternalFieldI[i,y] - \EstimatedExternalFieldI[i,y]} + 2\svbv^{(i)\top}\normalparenth{\TruePhi[v,y] -\EstimatedPhi^{(v,y)} } + 2\wtil{\svba}^{(i)\top}\normalparenth{\TruePhi[a,y] - \EstimatedPhi^{(a,y)}}} \leq R(\varepsilon, \delta/n) + p \varepsilon.
\end{align}
To that end, we have
\begin{align}
    %
    %
    %
    & \stwonorm{\normalparenth{\TrueExternalFieldI[i,y] - \EstimatedExternalFieldI[i,y]} + 2\svbv^{(i)\top}\normalparenth{\TruePhi[v,y] -\EstimatedPhi^{(v,y)} } + 2\wtil{\svba}^{(i)\top}\normalparenth{\TruePhi[a,y] - \EstimatedPhi^{(a,y)}}}\\
    & \sless{(a)}  \stwonorm{\TrueExternalFieldI[i,y] - \EstimatedExternalFieldI[i,y]} + 2\stwonorm{\svbv^{(i)\top}\normalparenth{\TruePhi[v,y] -\EstimatedPhi^{(v,y)}}} + 2\stwonorm{\wtil{\svba}^{(i)\top}\normalparenth{\TruePhi[a,y] - \EstimatedPhi^{(a,y)}}}\\
    & \sless{(b)}  \stwonorm{\TrueExternalFieldI[i,y] - \EstimatedExternalFieldI[i,y]} + 2\stwonorm{\svbv^{(i)}}\opnorm{\TruePhi[v,y] -\EstimatedPhi^{(v,y)}} + 2\stwonorm{\wtil{\svba}^{(i)}}\opnorm{\normalparenth{\TruePhi[a,y] - \EstimatedPhi^{(a,y)}}}\\
    & \sless{(c)}  \stwonorm{\TrueExternalFieldI[i] - \EstimatedExternalFieldI[i]} + 2\Bigparenth{\stwonorm{\svbv^{(i)}} + \stwonorm{\wtil{\svba}^{(i)}}} \opnorm{\TrueParameterMatrix -\EstimatedParameterMatrix}\\
    & \sless{(d)}  \stwonorm{\TrueExternalFieldI[i] - \EstimatedExternalFieldI[i]} + 2\Bigparenth{\stwonorm{\svbv^{(i)}} + \stwonorm{\wtil{\svba}^{(i)}}} \onematnorm{\TrueParameterMatrix -\EstimatedParameterMatrix} \sless{(e)}  R(\varepsilon, \delta/n) + 4\xmax \sqrt{p_y} \sqrt{p} \varepsilon,
\end{align}
where $(a)$ follows from triangle inequality, $(b)$ follows because induced matrix norms are submultiplicative, $(c)$ follows because operator norm of any sub-matrix is no more than operator norm of the matrix and $\ell_2$ norm of any sub-vector is no more than $\ell_2$ norm of the vector, $(d)$ follows because $\TrueParameterMatrix -\EstimatedParameterMatrix$ is symmetric and because matrix operator norm is bounded by square root of the product of matrix one norm and matrix infinity norm, and $(e)$ follows from \cref{theorem_parameters} by using the relationship between vector norms. The proof is complete by rescaling $\varepsilon$ and absorbing the constants in $c$.

\paragraph{Proof of \cref{lemma_exp_fam_parameter_perturbation}: \expfamperturbationresultname.}
Let $Z(\psi, \Psi) \in \Reals_{+}$ denote the log-partition function of $f_{\rvbu}(\cdot | \psi, \Psi) $ in \cref{eq_dist_u}. Then, from \cite[Theorem 1]{BusaFSZ2019}, we have
\begin{align}
    \stwonorm{ \mu(\hpsi, \hPsi) - \mu(\spsi, \sPsi)} = \stwonorm{\nabla_{\hpsi} Z(\hpsi, \hPsi) - \nabla_{\spsi} Z(\spsi, \sPsi)}. \label{eq_mean_vector_difference}
\end{align}
For some $t \in (0,1)$, $\tpsi \defn t \hpsi + (1-t) \spsi$ and $\tPsi \defn t \tpsi + (1-t) \tpsi$, we have the following from the mean value theorem
\begin{align}
    \stwonorm{\nabla_{\hpsi} Z(\hpsi, \hPsi) - \nabla_{\spsi} Z(\spsi, \sPsi)} = \stwonorm{\nabla_{\tpsi}^2 Z(\tpsi, \tPsi) \cdot (\hpsi - \spsi)} & \sless{(a)} \opnorm{\nabla_{\tpsi}^2 Z(\tpsi, \tPsi)} \stwonorm{(\hpsi - \spsi)}\\
    & \sequal{(b)} \opnorm{\Covariance(\tpsi, \tPsi)}\stwonorm{(\hpsi - \spsi)}, \label{eq_mean_value_theorem}
\end{align}
where $(a)$ follows because induced matrix norms are submultiplicative and $(b)$ follows because for any regular exponential family a sub-matrix of the Hessian of the log partition function is the corresponding sub-matrix of the covariance matrix of the associated sufficient statistic. Combining \cref{eq_mean_vector_difference,eq_mean_value_theorem} completes the proof.
\end{proof}