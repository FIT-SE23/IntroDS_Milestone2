\section{Introduction}
\label{section_introduction}
%
%
%

%



%
%

%
We are interested in the problem of unit-level counterfactual inference
%
owing to the increasing importance of personalized decision-making in many domains.
%
As a motivating example, consider a recommender system interacting with a user over time. At each time, the user is exposed to a product based on observed demographic factors as well as certain unobserved factors, and the user's engagement level is recorded. The engagement level at any time can depend sequentially on the prior interaction in addition to the ongoing interaction (see \cref{fig_graphical_models}(a)). The system can also sequentially adapt its recommendation. Given historical data of many heterogeneous users, the system wants to infer each user's average engagement level if it were exposed to a different sequence of products while the observed and the unobserved factors remain unchanged. This task is challenging since: (a) the \textit{unobserved} factors could give rise to spurious associations, (b) the users could be \textit{heterogeneous} in that they may have different responses to same  sequence of products, and (c) each user only provides a \textit{single} interaction trajectory.  

%
In a general problem, we consider an observational setting where a unit undergoes interventions denoted by $\rvba$. We denote the outcomes of interest by $\rvby$, and allow the interventions $\rvba$ and the outcomes $\rvby$ to be confounded by observed covariates $\rvbv$ as well as unobserved covariates $\rvbz$. The graphical structure shown in \cref{fig_graphical_models}(b) captures these interactions and is at the heart of our problem. We consider 
$n$ heterogeneous and independent units indexed by $i \in [n] \defn \{1,\cdots,n\}$, and assume access to one observation per unit with $\svbv^{(i)}$, $\svba^{(i)}$, and $\svby^{(i)}$ denoting the realizations of $\rvbv$, $\rvba$, and $\rvby$ for unit $i$ respectively.


%
%
%
%
 
%

%

%
%
%
%
%

%
%

%

%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%


%
%

%
%
%


%
%
%
%
%
%
%



%

%

%
%
%
%
%
%
%
We operate within the Neyman-Rubin potential outcomes framework \citep{Neyman1923, Rubin1974} and denote the potential outcome of unit $i \in [n]$ under interventions $\svba$ by $\svby^{(i)}(\svba)$. Given the realizations $\braces{(\svbv^{(i)}, \svba^{(i)}, \svby^{(i)})}_{i=1}^{n}$, our goal is to answer counterfactual questions for these $n$ units, e.g., what would the potential outcomes $\svby^{(i)}(\wtil{\svba}^{(i)})$ for interventions $\wtil{\svba}^{(i)} \neq \svba^{(i)}$ be, while the observed and unobserved covariates remain unchanged?  
%
Under the stable unit treatment value assumption (SUTVA), i.e., the potential outcomes of unit $i$ are not affected by the interventions at other units, learning unit-level counterfactual distributions is equivalent to
learning unit-level conditional distributions
\begin{align}
    \braces{f_{\rvby | \rvba, \rvbz, \rvbv}(\rvby=\cdot | \rvba= \cdot, \svbz^{(i)}, \svbv^{(i)})}_{i=1}^{n}. \label{eq_set_conditional_distributions}
\end{align}
Here, the $i$-th distribution represents the conditional distribution for the outcomes $\rvby$ as a function of the interventions $\rvba$, while keeping the observed covariates $\rvbv$ and the unobserved covariates $\rvbz$ fixed at the corresponding realizations for unit $i$, i.e., $\svbv^{(i)}$ and $\svbz^{(i)}$ respectively.

%
       
\newcommand{\ranvarvec}{\rvbw}
%
%
%
\newcommand{\ranvarmat}{\newcommand{\varmat}{\begin{bmatrix} 
\rvbz, \rvbv, \rvba, \rvby^\tp
\end{bmatrix}}}
\newcommand{\varvec}{\svbw}
%
%
%
\newcommand{\varmat}{\begin{bmatrix} 
\svbz, \svbv, \svba, \svby
    \end{bmatrix}}
    
%
It is infeasible to answer such questions without any structural assumptions due to two key challenges: (a) unobserved confounding and (b) heterogeneity in unit-level conditional distributions. First, the unobserved covariates $\rvbz$ introduces statistical dependence between interventions and outcomes, known as \emph{unobserved confounding}, which results in biased estimates. Second, the realizations $\braces{(\svbz^{(i)}, \svbv^{(i)})}_{i=1}^{n}$ could be different for different units leading to \emph{heterogeneity} in conditional distributions across units. The heterogeneity is crucial since we only observe one realization, namely the outcomes $\svby^{(i)}(\svba^{(i)})$ under the interventions $\svba^{(i)}$, that is consistent with the unit-level conditional distribution $f_{\rvby | \rvba, \rvbz, \rvbv}(\svby | \svba, \svbz^{(i)}, \svbv^{(i)})$. As a result, one needs to learn $n$ heterogeneous conditional distributions while having access to only one sample from each of them. %

%
%
%
%
%
%

%

%

\begin{figure}[t]
    \centering
    \begin{tabular}{cc}
    \includegraphics[width=0.3\linewidth,clip]{sequential_main_3.pdf} ~~
    \includegraphics[height=0.26\linewidth,clip]{sequential_explained_3.pdf} &
    \includegraphics[width=0.27\linewidth,clip]{triangle_3.pdf}\\
    %
    %
    (a) A graphical model for sequential recommender system 
    &
    (b) A generic model for our setting
    \end{tabular}
    \caption{Two graphical models illustrating aspects of our work. The directed and the bi-directed arrows indicate causation and association respectively. The arrows involving the unobserved factors/covariates $\rvbz$ are colored red. Panel \tbf{(a)}  visualizes a sequential recommender system interacting with a user for 3 time points where $\rvv_t$, $\rva_t$, and $\rvy_t$ denote the user's observed demographic factors, the product exposed to the user, and the user's engagement level respectively at time $t$, and $\rvbz$ denotes unobserved factors. The left plot 
    %
    illustrates the dependency of the observed variables $(\rvv_t, \rva_t, \rvy_t)$ at time $t$, on the observed variables at time $t-1$ via $\dashrightarrow$, and on the unobserved covariates $\rvbz$ via $\reddoublearrow$; 
    %
    %
    %
    these dependencies for time $1$ and $2$ are expanded in the right plot. Panel \tbf{(b)} exhibits a generic graphical model depicting the relationship between $\rvbz$, $\rvbv$, $\rvba$, and $\rvby$ for every unit.}
    %
    %
    %
    %
    %
    %
    \label{fig_graphical_models}
    \vspace{-5mm}
\end{figure}

%
In this work, we model the joint distribution of the unobserved covariates, the observed covariates, the intervention, and the outcomes of interest as an exponential family distribution in accordance with the principle of maximum entropy.\footnote{Exponential family distributions are the maximum entropy distributions given linear constraints on distributions such as bounded moments (see \cite{BarndorffNielsen2014, Brown1986}). The exponential family considered in this work arise when the first and second moments of the joint vector $(\rvbz, \rvbv, \rvba, \rvby)$ are bounded.}
With this modeling assumption, we show that both the aforementioned challenges can be tackled. 
%
In particular, we show that the $n$ unit-level conditional distributions in \eqref{eq_set_conditional_distributions} lead to $n$ distributions from the same exponential family albeit with parameters that vary across units.  The parameter corresponding to the $i^{th}$ unit, say $\Truephi[i]$, captures the effect of $\svbz^{(i)}$, and helps tackle the challenge of unobserved confounding. However, the challenge still remains to learn $n$ heterogeneous exponential family distributions with one sample per distribution. This has been addressed in two specific scenarios in the literature: (a) if  the unobserved confounding is identical across units, i.e., the parameters $\normalbraces{\Truephi[i]}_{i=1}^n$ 
%
were all equal, then the challenge boils down to learning parameters of a single exponential family distribution from $n$ samples which has been well-studied (see \cite{ShahSW2021B} for an overview); (b) if $\rvbv$, $\rvba$, and $\rvby$ take binary values and have pairwise interactions, and the dependencies between them are known, then the challenge boils down to learning certain parameters of an Ising model with one sample  
%
which has been studied in \cite{DaganDDA2021,KandirosDDGD2021}. In this work, we consider a generalized setting, where $\rvbv$, $\rvba$, and $\rvby$ can be either discrete, continuous, or both, and assume no knowledge of the underlying dependencies.
%
%
%
%
%
%

%
%
%
%
%

%

%
%
%

%
%
%
%


%

\paragraph{Our contributions.} 
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
As the main contribution, this work introduces a method 
to learn unit-level counterfactual distributions from observational studies in the presence of unobserved confounding with one sample per unit. Specifically, for every unit $i \in [n]$, we reduce learning its counterfactual distribution to learning $\Truephi[i]$ from one sample $(\svbv^{(i)}, \svba^{(i)}, \svby^{(i)})$ where $\Truephi[i]$ is the parameter of the exponential family distribution corresponding to unit $i$.
%
Our technical contributions are as follows.
%
\begin{enumerate}
%
\itemsep0em
    %
    \item We introduce a convex (and strictly proper) loss function (\cref{def-loss-function}) that pools the data $\braces{(\svbv^{(i)}, \svba^{(i)}, \svby^{(i)})}_{i=1}^{n}$ across all $n$ samples to jointly learn all $n$ parameters $\normalbraces{\Truephi[i]}_{i=1}^n$.
    \item For every unit $i$, we establish that the mean squared errors of our estimates of (a) $\Truephi[i]$ (\cref{theorem_parameters}), and (b) the expected potential outcomes under alternate interventions (\cref{thm_causal_estimand}) scale linearly with the metric entropy of the parameter space, i.e., the set $\Truephi[i]$ comes from. In particular, when $\Truephi[i]$ is $s$-sparse linear combination of $k$ known vectors (see \cref{sec_main_results}), 
    %
    %
    the error---just with one sample---decays as $O(s\log k/p)$, where $p$ is the dimension of the tuple $(\rvbv, \rvba, \rvby)$.
    %
    \item As part of our analysis, we (a) derive sufficient conditions for a continuous random vector supported on a compact set to satisfy the logarithmic Sobolev inequality (\cref{thm_LSI_main}) and (b) provide new concentration bounds for arbitrary functions of a continuous random vector that satisfies the logarithmic Sobolev inequality (\cref{thm_main_concentration}). These results could be of independent interest.
\end{enumerate}
 
%
%
%
%


%

%
%
%

%
    %
    %
    %
    %
    %
%
%



%
%
%



%

%
%
%
%

%

\paragraph{Notation.} 
For any positive integer $n$, let $[n] \coloneqq \{1,\cdots, n\}$.
For a deterministic sequence $u_1, \cdots , u_n$, we let $\svbu \coloneqq (u_1, \cdots, u_n)$. 
For a random sequence $\rvu_1, \cdots , \rvu_n$, we let $\rvbu \coloneqq (\rvu_1, \cdots, \rvu_n)$. 
For a vector $\svbu \in \Reals^p$, we use $u_t$ to denote its $t^{th}$ coordinate and $u_{-t} \in \Reals^{p-1}$ to denote the vector after deleting the $t^{th}$ coordinate. We denote the $\ell_0$, $\ell_p$ $(p \geq 1)$, and $\ell_{\infty}$ norms of a vector $\svbv$ by $\szeronorm{\svbv}$, $\spnorm{\svbv}$, and  $\sinfnorm{\svbv}$ respectively.  For a matrix $\tbf{M} \in \Reals^{p \times p}$, we denote the element in $t^{th}$ row and $u^{th}$ column by $\tbf{M}_{tu}$, the $t^{th}$ row by $\tbf{M}_t$, and the matrix maximum norm by $\maxmatnorm{\tbf{M}}$. Finally, for vectors $\what{\svbu} \in \Reals^p$ and $\wtil{\svbu} \in \Reals^p$, the mean squared error between $\what{\svbu}$ and $\wtil{\svbu}$ is defined as $\mathrm{MSE}(\what{\svbu}, \wtil{\svbu}) \defn p^{-1} \sum_{t \in [p]} (\what{u_t} - \wtil{u_t})^2$.
%

\paragraph{Outline.}
In \cref{sec_related_work}, we discuss related work. In \cref{section_problem_formulation}, we formulate the problem of interest, define the inference tasks of interest, state our assumptions. We provide a description of our algorithm in \cref{sec_algo} and present our main results in \cref{sec_main_results}. We give a proof sketch of our main result in \cref{sec_proof_sketch} and an application to setting with sparse measurement errors in \cref{sec_sparse_measurement_errors}.
%
%
%
%
%
%
See the Appendix for its organization.