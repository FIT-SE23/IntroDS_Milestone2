\section{Proof Sketch for \cref{theorem_parameters}: \parammainresultname}
\label{sec_proof_sketch}
\begin{figure}[ht]
\begin{tikzpicture}[scale=0.7, >=stealth,main node/.style={shape=rectangle,draw,rounded corners,}]
    %
    \node[main node][very thick, fill=red!30] (t2) {\begin{tabular}{c} Theorem~\ref{theorem_parameters}: Part II: \\ Error bound for \\ unit-level \\parameters $\normalbraces{\TrueExternalFieldI}_{i = 1}^{n}$ \end{tabular}};
    %
    \node[main node][very thick] (l1) [right=2.5 cm of t2]{\begin{tabular}{c} Lemma~\ref{lemma_conc_first_sec_der_stage_two}: \\ Concentration of \\gradient and Hessian of\\ loss functions $\normalbraces{\loss^{(i)}}_{i = 1}^{n}$ \end{tabular}};
    \node[main node][very thick, fill=red!30] (t1) [below=0.8 cm of l1]{\begin{tabular}{c} Theorem~\ref{theorem_parameters}: Part I: \\ Error bound for population-\\level parameter $\TrueParameterMatrix$ \end{tabular}};
    \node[main node][very thick, fill=orange!30] (p3) [right=2.4 cm of l1]{\begin{tabular}{c} Proposition~\ref{thm_main_concentration}:\\ Tail bounds\\ under LSI \end{tabular}};
    \node[main node][very thick, fill=orange!30] (p2) [above=1.75cm of p3]{\begin{tabular}{c} Proposition~\ref{thm_LSI_main}:\\ LSI for weakly\\ dependent RVs \end{tabular}};
    \node[main node][very thick, fill=orange!30] (p4) [below=1.75 cm of p3]{\begin{tabular}{c} Proposition~\ref{lemma_conditioning_trick}:\\ Identifying weakly\\dependent RVs in \\exponential family \end{tabular}};
    \node[main node][very thick] (l2) [above=0.8 cm of l1]{\begin{tabular}{c} Lemma~\ref{lemma_tenorization_kld} + Lemma~\ref{lemma_dobrushin_implies_tensorization}:\\ Approximate \\
    tensorization of \\ entropy for weakly \\ dependent RVs \end{tabular}};
    %
    \node at (4.9,0.4) (lipschitzness) {(i) Lemma \ref{lemma_parameter_single_external_field}};
    \node at (4.9,-0.4) {(ii) Lemma \ref{lemma_lipschitzness}};
    \node[main node][very thick] (l3) [above left=2.7cm of l1]{\begin{tabular}{c} Lemma~\ref{lemma_reverse_pinsker}:\\ Reverse-Pinkser \\inequality \end{tabular}};
    \draw[->,  line width=.6mm] (t1) to[out=90,in=270] (l1);
    \node at (11.35,-2.15) {Lemma \ref{lemma_expected_psi_upper_bound}};
    \draw[->,  line width=.6mm] (p2) to[out=270,in=90] (p3);
    \draw[->,  line width=.6mm] (p3) to[out=180,in=0] (l1);
    \draw[->,  line width=.6mm] (p4) to[out=160,in=340] (l1);
    \draw[->,  line width=.6mm] (l2) to[out=5,in=180] (p2);
    \draw[->,  line width=.6mm] (l3) to[out=0,in=170] (l2);
    \draw[->,  line width=.6mm] (l1) to[out=180,in=0] (t2);
    \node at (15.0,0.4) {Corollary \ref{coro}};
    \draw[->,  line width=.6mm] (-0.5, -4.1) to (6.0, -4.1);
    \node at (2.7,-3.7) {Extend \cite{ShahSW2021A}};
    \node at (2.7,-4.5) {to non-identical samples};
    \draw[->,  line width=.6mm] (9.7, -6.1) to (16.1, -6.1);
    \node at (12.9,-5.7) {Extend \cite{DaganDDA2021}};
    \node at (12.9,-6.5) {to continuous RVs};
    \draw[->,  line width=.6mm] (1.0, 3.4) to (6.5, 3.4);
    \node at (3.6,3.8) {Extend \cite{Marton2015}};
    \node at (3.6,3.0) {to continuous RVs};
    %
    %
    %
    %
    %
    %
\end{tikzpicture}
  \caption{High-level sketch diagram of the results and the proof techniques
  developed in this paper.
  The proof of \cref{theorem_parameters} consists of two parts. In the first part, we provide the error guarantee for the population-level parameter. As a key step to achieve this, we extend \citet[Prop I.1, Prop I.2]{ShahSW2021A} to the setting with non-identical samples. In the second part, we provide the error guarantee for the unit-level parameters. At the heart of the proof lies the concentration of the gradient and the anti-concentration of the Hessian of the auxiliary loss functions $\normalbraces{\loss^{(i)}}_{i = 1}^{n}$ defined in \cref{eq:loss_function_n_original}. 
  To show this, we develop three results which may be of independent interest. In \cref{thm_LSI_main}, we prove that weakly dependent random variables supported on a compact set satisfy the logarithmic Sobolev inequality (LSI) by (a) extending \citet[Theorem 1, Theorem 2]{Marton2015} to continuous random vectors and (b) proving a reverse-Pinkser style inequality for continuous random vectors. In \cref{thm_main_concentration}, we extend \citet[Theorem 6]{DaganDDA2021}
  to establish tail bounds for arbitrary functions of a continuous random vector that satisfies LSI. In \cref{lemma_conditioning_trick}, we show that a subset of a random vector can be made weakly dependent after conditioning on the remaining subset by extending \citet[Lemma 2]{DaganDDA2021} to continuous random vectors. Finally, we achieve the desired properties on the gradient and the Hessian by identifying a subset of the random vector that are weakly dependent, and then using the tail bounds for specific function choices (weak dependence ensure LSI holds enabling the use of tail bounds).}
    \label{fig:proof_sketch}
\end{figure}

%
%
%
%
%
%

%
%
%
%
%
%

\noindent We divide the proof of \cref{theorem_parameters} in two parts. In the first part, we provide guarantee on estimating the population-level parameter $\TrueParameterMatrix$. To achieve this, we analyze the convex optimization problem in \cref{eq_estimated_parameters} and provide the estimation error for the component of $\ExtendedEstimatedParameterMatrix$ corresponding to the population-level parameter, i.e., $\EstimatedParameterMatrix$. In the second part, we provide guarantee on estimating the unit-level parameters $\bigbraces{\TrueExternalFieldI[1], \cdots, \TrueExternalFieldI[n]}$. To achieve this, we analyze the convex optimization problem in \cref{eq_estimated_parameters} after substituting $\ParameterMatrix = \EstimatedParameterMatrix$ and provide the estimation error for $\normalbraces{\EstimatedExternalFieldI[1], \cdots, \EstimatedExternalFieldI[n]}$. Formally, we analyze the following convex optimization problem in the second part
\begin{align}
    \normalbraces{\EstimatedExternalFieldI[1], \cdots, \EstimatedExternalFieldI[n]} \in \argmin_{\normalbraces{\ExternalFieldI[1], \cdots, \ExternalFieldI[n]} \in \ParameterSet_{\ExternalField}^n} \loss\bigparenth{\EstimatedParameterMatrix, \ExternalFieldI[1], \cdots, \ExternalFieldI[n]}. \label{eq_decomposed_second_minimum}
\end{align}
We note that while our analysis is divided in two parts, Algorithm \ref{alg:GradientDescent} runs only once until convergence.



%
%
%
%
%
%
%
%
%
%
%
%
%
\subsection{Estimating the population-level parameter}
\label{sec_proof_sketch_pop}
In the first part, we decompose the convex (and positive) objective $\loss(\ExtendedParameterMatrix)$ in \cref{eq:loss_function} as a sum of $p$ convex (and positive) auxiliary objectives $\loss_t\bigparenth{\ExtendedParameterRowT}$, i.e., $\loss(\ExtendedParameterMatrix) = \sump \loss_t\bigparenth{\ExtendedParameterRowT}$ where
%
\begin{align}
    \loss_t\bigparenth{\ExtendedParameterRowT} \defn \frac{1}{n} \sumn[i] \exp\Bigparenth{\!-\!\normalbrackets{\ExternalFieldtI \!+\! 2\ParameterRowt\tp \svbx} x_t^{(i)}}
    %
    \label{eq:loss_function_p_original}
\end{align}
Now, fix some $t \in [p]$ and some $\varepsilon > 0$. Then, we show that all $\ExtendedParameterMatrix \in \ParameterSet_{\ExternalField}^n \times \ParameterSet_{\ParameterMatrix}$ that satisfy $\stwonorm{\ParameterRowt - \TrueParameterRowt} \geq \varepsilon$ also uniformly satisfy 
\begin{align}
    \loss_t(\ExtendedParameterRowT) \geq \loss_t(\ExtendedTrueParameterRowT) + O(\varepsilon^2) \stext{for} n \geq \frac{c \exp({c\bGM})\log \frac{p}{\delta}}{\varepsilon^4}, \label{eq_firststage_key_step}
\end{align}
with probability at least $1-\delta$. In fact, this holds uniformly for all $t \in [p]$. Then, taking a sum over $t$ on both sides of \cref{eq_firststage_key_step}, we see that all $\ExtendedParameterMatrix \in \ParameterSet_{\ExternalField}^n \times \ParameterSet_{\ParameterMatrix}$, such that $\stwonorm{\ParameterRowt - \TrueParameterRowt} \geq \varepsilon$ for some $t \in [p]$, uniformly satisfy 
\begin{align}
    \loss(\ExtendedParameterMatrix) \geq \loss(\ExtendedTrueParameterMatrix) + O(\varepsilon^2) \stext{for} n \geq \frac{c \exp({c\bGM})\log \frac{p}{\delta}}{\varepsilon^4},
\end{align}
with probability at least $1-\delta$. We conclude the proof using contraposition as \cref{eq_estimated_parameters} implies $\loss(\ExtendedEstimatedParameterMatrix) \leq \loss(\ExtendedTrueParameterMatrix)$. \\

\noindent To prove \cref{eq_firststage_key_step}, we first show that for any fixed $\ExtendedParameterMatrix \in \ParameterSet_{\ExternalField}^n \times \ParameterSet_{\ParameterMatrix}$ and $t \in [p]$, if $\ParameterRowt$ is far from $\TrueParameterRowt$, then with high probability $\loss_t\bigparenth{\ExtendedParameterRowT}$ will be significantly larger than $\loss_t\bigparenth{\ExtendedTrueParameterRowT}$ (see \cref{lemma_parameter}). To establish this, we need concentration of the first directional derivative and anti-concentration of the second directional derivative of the auxiliary objective $\loss_t(\cdot)$ defined in \cref{eq:loss_function_p_original} (see \cref{lemma_conc_first_sec_der}). We achieve this by extending the ideas from \cite{ShahSW2021A} to the setting with non-identical samples --  $\normalbraces{\svbx^{(i)}}_{i = 1}^{n}$ are independent realizations from $\bigbraces{\TrueJointDistfunT}_{i = 1}^{n}$ which could all be different. Finally, we conclude \cref{eq_firststage_key_step} by using Lipschitzness of the auxiliary objective $\loss_t(\cdot)$ (see \cref{lemma_lipschitzness_first_stage}) and an appropriate covering number argument (see \cref{sec:proof_of_theorem_parameters}).


%
%
%
%


%
%
%
%

\subsection{Estimating the unit-level parameters}
\label{sec_proof_sketch_unit}
In the second part, we decompose the convex optimization problem in \cref{eq_decomposed_second_minimum} into $n$ convex optimization problems.
%
To achieve this, we define 
\begin{align}
    \loss^{(i)}\bigparenth{\ExternalFieldI} \defn \sump[t] \exp\Bigparenth{-\normalbrackets{\ExternalFieldtI + 2\EstimatedParameterRowt\tp \svbx^{(i)}} x_t^{(i)}}.
    \label{eq:loss_function_n_original}
\end{align}
Then, we note that the set $\ParameterSet_{\ExternalField}^n$ places independent constraints on the $n$ unit-level parameters, i.e., $\ExternalFieldI[i] \in \ParameterSet_{\ExternalField}$ independently for all $ i \in [n]$. Using these and \cref{eq:loss_function} in \cref{eq_decomposed_second_minimum}, we have
\begin{align}
    \min_{\normalbraces{\ExternalFieldI[1], \cdots, \ExternalFieldI[n]} \in \ParameterSet_{\ExternalField}^n} \loss\bigparenth{\EstimatedParameterMatrix, \ExternalFieldI[1], \cdots, \ExternalFieldI[n]} \sequal{\cref{eq:loss_function_n_original}} \frac{1}{n}\sumn \min_{\ExternalFieldI[i] \in \ParameterSet_{\ExternalField}} \loss^{(i)}\bigparenth{\ExternalFieldI[i]}.
\end{align}
As a result, for every $i \in [n]$, we solve the following optimization problem to estimate the unit-level parameter $\TrueExternalFieldI$
\begin{align}
    \EstimatedExternalFieldI \in \argmin_{\ExternalFieldI[i] \in \ParameterSet_{\ExternalField}} \loss^{(i)}\bigparenth{\ExternalFieldI[i]}. \label{eq_opt_problem_unit}
\end{align}
Then, we show that all points $\ExternalFieldI \in \ParameterSet_{\ExternalField}$ that satisfy $\stwonorm{\ExternalFieldI - \TrueExternalFieldI} \geq R(\varepsilon, \delta)$ also uniformly satisfy
\begin{align}
    \loss^{(i)}\bigparenth{\ExternalFieldI} & \geq  \loss^{(i)}\bigparenth{\TrueExternalFieldI} + R^2(\varepsilon, \delta) \stext{for} n \geq \frac{c \exp({c\bGM})}{\varepsilon^4} \cdot \biggparenth{\log \frac{p}{\delta} + \metric_{\ExternalField}(\radius)}, \label{eq_secondstage_key_step}
\end{align}
with probability at least $1-\delta$ where $R(\varepsilon, \delta)$ and $\radius$ were defined in \cref{eq_radius_node_thm}, and the metric entropy $\metric$ was defined in \cref{def_covering_number_metric_entropy}. We conclude the proof by (a) using contraposition as \cref{eq_opt_problem_unit} implies $\loss^{(i)}(\EstimatedExternalFieldI) \leq \loss^{(i)}(\TrueExternalFieldI)$ and (b) applying a union bound over all $i \in [n]$.\\

\noindent To prove \cref{eq_secondstage_key_step}, we first show that for any fixed $\ExternalFieldI \in \ParameterSet_{\ExternalField}$, if $\ExternalFieldI$ is far from $\TrueExternalFieldI$, then with high probability $\loss^{(i)}\bigparenth{\ExternalFieldI}$ will be significantly larger than $\loss^{(i)}\bigparenth{\TrueExternalFieldI}$ (see \cref{lemma_parameter_single_external_field}). To establish this, we need concentration of the first directional derivative and anti-concentration of the second directional derivative of the auxiliary objective $\loss^{(i)}(\cdot)$ defined in \cref{eq:loss_function_n_original} (see \cref{lemma_conc_first_sec_der_stage_two}). To achieve this, we (1) identify a subset of the random vector that is only weakly dependent (after conditioning on the remaining subset), i.e., satisfies Dobrushin's uniqueness condition (\cref{def_dobrushin_condition}) (see \cref{lemma_conditioning_trick}), (2) prove that a weakly dependent continuous random vector supported on a compact set satisfies the logarithmic Sobolev inequality (LSI) (see \cref{thm_LSI_main}), and (3) derive tail bounds for arbitrary functions of a continuous random vector that satisfies LSI (see \cref{thm_main_concentration}). Then, we show the desired concentration and anti-concentration bounds by reducing the random vector to satisfy Dobrushin's uniqueness condition and using the derived tail bounds for specific functions of the random vector. We note that our analysis relies on the estimate of the population-level parameter, i.e., $\EstimatedParameterMatrix$, and we use the corresponding error guarantee for some sort of robustness analysis to show concentration of the first directional derivative (see \cref{lemma_expected_psi_upper_bound}). Finally, we conclude \cref{eq_secondstage_key_step} by using Lipschitzness of the auxiliary objective $\loss^{(i)}(\cdot)$ (see \cref{lemma_lipschitzness}) and an appropriate covering number argument (see \cref{sec_proof_thm_node_parameters_recovery}).

%

%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%