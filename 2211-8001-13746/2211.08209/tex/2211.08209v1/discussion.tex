%
%
\section{Imputing measurement errors}
\label{sec_sparse_measurement_errors}
Consider a scenario where there are no unobserved covariates $\rvbz$ but there are measurement errors in observed covariates for some units. More concretely, for every unit $i \in [n]$, we observe noisy covariates $\csvbv^{(i)} = \svbv^{(i)} + \Delta \svbv^{(i)}$ instead of true covariates $\svbv^{(i)}$ where $\Delta \svbv^{(i)}$ denotes (unobserved) measurement error. For simplicity, we assume that half the units have no measurement errors and we know the identity of these units. Assuming $n$ is even, let $\Delta \svbv^{(i)} = 0$ for all $i \in \normalbraces{n/2+1, \cdots, n}$ , i.e., there is no measurement error for units $i \in \normalbraces{n/2+1, \cdots, n}$. Further, let the measurement error for units $i \in \normalbraces{1, \cdots, n/2}$ be (a) sparse i.e., $\szeronorm{\Delta \svbv^{(i)}} \leq s$ and (b) bounded, i.e., $\sinfnorm{\Delta \svbv^{(i)}} \leq \xmax$ for all $i \in \normalbraces{1, \cdots, n/2}$

\paragraph{Question of interest.} As before, our goal is to answer counterfactual questions for all these $n$ units. 
Additionally, we seek to impute the measurement error for units $i \in \normalbraces{1, \cdots, n/2}$. 
We show that the methodology developed in this work can be applied to this setup by replacing the role of unobserved covariates $\rvbz$ by the measurement error $\Delta \svbv$.


\paragraph{Approach.} As in \cref{subsec_exp_fam}, we model the joint distribution of the true covariates $\rvbv$, the interventions $\rvba$, and the observed outcomes $\rvby$ as an exponential family parameterized by a vector $\wtil{\phi} \in \Reals^p$ and a symmetric matrix $\wtil{\Phi} \in \Reals^{p \times p}$ where $p \defn p_v + p_a + p_y$. Then, by using $\rvbv = \crvbv - \Delta \rvbv$ and letting $\tilp \defn 2 p_v + p_a + p_y$, the joint probability distribution $f_{\ranvarvec}$ of the $\tilp$-dimensional random vector $\ranvarvec \defeq \normalparenth{-\Delta \rvbv, \crvbv, \rvba, \rvby}$ can be parameterized by a vector $\phi \in \Reals^{\tilp \times 1}$ and a matrix $\Phi \in \Reals^{\tilp \times \tilp}$ as follows 
\begin{align}
    f_{\ranvarvec}(\varvec; \phi, \Phi) \propto \exp\Bigparenth{ \phi\tp \varvec
    +\varvec\tp \Phi \varvec},
    \qtext{where}
    \varvec \defeq (\Delta \svbv, \csvbv, \svba, \svby),
    \label{eq_joint_distribution_vvay}
\end{align}
and $\Delta \svbv$, $\csvbv$, $\svba$, and $\svby$ denote realizations of $\Delta \rvbv$, $\crvbv$, $\rvba$, and $\rvby$ respectively. More importantly, $\phi$ and $\Phi$ are derived from $\wtil{\phi}$ and $\wtil{\Phi}$ respectively, and have special structure: (i) $\phi^{(\Delta v)} = \phi^{(\cv)} = \wtil{\phi}^{(v)}$ and (ii) $\Phi^{(u, \Delta v)} = \Phi^{(u, \cv)} = \wtil{\Phi}^{(u, v)}$ for all $\rvbu \in \normalbraces{\crvbv, \rvba, \rvby}$. Then, analogous to \cref{eq_actual_parameters_of_interest}, the task of learning 
$f_{\rvby | \rvba, \Delta \rvbv, \crvbv}(\rvby=\cdot | \rvba= \cdot, \Delta \svbv, \csvbv)$ as a function of $\rvba$ reduces to learning 
\begin{align}
    \stext{(i)} \phi^{(y)} - 2\Phi^{(\cv,y)\top} \Delta \svbv + 2\Phi^{(\cv,y)\top} \svbv, \qtext{} \stext{(ii)} \Phi^{(a,y)}, \qtext{and} \stext{(iii)} \Phi^{(y,y)}.
\end{align}
Letting $\rvbx \defn (\crvbv, \rvba, \rvby)$ and conditioning on $\Delta \rvbv = \Delta \svbv$, we can write $f_{\rvbx |\Delta \rvbv}$ as
\begin{align}
    f_{\rvbx|\Delta \rvbv}\bigparenth{\svbx| \Delta \svbv; \ExternalField(\Delta \svbv), \ParameterMatrix} \!\propto\! \exp \Bigparenth{ \normalbrackets{\ExternalField(\Delta \svbv)}\tp \svbx \!+\! \svbx\tp \ParameterMatrix \svbx} \stext{where}
    \ExternalField(\Delta \svbv)  \defeq \begin{bmatrix} \phi^{(\cv)} \!-\! 2 \Phi^{(\cv,\cv)\top} \Delta \svbv \\ \phi^{(a)} \!-\! 2 \Phi^{(\cv,a)\top} \Delta \svbv \\ \phi^{(y)} \!-\! 2 \Phi^{(\cv,y)\top} \Delta \svbv \end{bmatrix}\in \! \Reals^{p\times 1},\, \label{eq_coditional_distribution_vay|delta_v}
\end{align} 
%
$\svbx \defn (\csvbv, \svba, \svby)$, $\Theta \in \Reals^{p \times p}$ denotes the component of $\Phi$ corresponding to $\rvbx$, and $\csvbv$, $\svba$, and $\svby$ denote realizations of $\crvbv$, $\rvba$, and $\rvby$ respectively. The special structure on $\Phi$ discussed above implies that $\Phi^{(\cv,\cv)}, \Phi^{(\cv,a)}$, and $\Phi^{(\cv,y)}$ affect both $\ExternalField(\Delta \svbv)$ and $\ParameterMatrix$ which can be exploited.

\paragraph{Inference tasks.} Let $f_{\ranvarvec}(\cdot; \phi^*, \Phi^*)$ denote the true data generating distribution of $\rvbw$, and let $f_{\rvbx|\Delta \rvbv}\bigparenth{\cdot| \Delta \svbv; \TrueExternalField(\Delta \svbv), \TrueParameterMatrix}$ denote the true distribution of $\rvbx$ conditioned on $\Delta \rvbv = \Delta \svbv$. We are interested in estimating
\begin{enumerate}
    \item Unit-level parameters $\TrueExternalField(\Delta \svbv^{(i)})$ for $i \in [n]$, 
    \item Population-level parameter $\TrueParameterMatrix$, and
    \item Measurement error $\Delta \svbv^{(i)}$ for $i \in [n/2]$.
\end{enumerate}
%
    %
From \cref{eq_coditional_distribution_vay|delta_v}, we see that $\TrueExternalField(\Delta \svbv^{(i)})$ is same for units without measurement error, i.e., $i \in \normalbraces{n/2+1, \cdots, n}$. For convenience, we let $\TrueExternalField$ be this value, i.e., $\TrueExternalField \defn \TrueExternalField(\Delta \svbv^{(n)})$. Further, we make assumptions analogous to \cref{assumptions}.

\paragraph{Procedure.} We proceed in two stages. In the first stage, we focus on the units $i \in \normalbraces{n/2+1, \cdots, n}$. We note that, in addition to the population-level parameter, the unit-level parameter for these units is also shared as described above. Therefore, learning $\TrueExternalField$ (for these units) and $\TrueParameterMatrix$ boils down to learning parameters of a sparse graphical model (under \cref{assumptions}) from multiple samples. We use the methodology from \cite{ShahSW2021A} (which is closely related to the one in this work) to obtain estimates $\EstimatedExternalField$ and $\EstimatedParameterMatrix$ with
%
\begin{align}
    \max\bigbraces{\sinfnorm{\TrueExternalField - \EstimatedExternalField}, \max_{t\in[p]}\stwonorm{\EstimatedParameterRowt \!-\! \TrueParameterRowt}} & \leq \varepsilon_0 \qtext{whenever} n  \geq \frac{ce^{c'\bGM}\log\frac{p}{\delta}}{\varepsilon_0^4}. \label{eq_theta_estimate_guarantee}
\end{align}
%

%
%
%
%
%
%
%

\noindent In the second stage, we focus on the units $i \in \normalbraces{1, \cdots, n/2}$, i.e., the ones with measurement error. To impute the measurement error, we express that the true parameters $\TrueExternalField(\Delta \svbv^{(i)})$ for $i \in [n/2]$ can be expressed as $s$-sparse linear combination of $p_v$ known vectors with some error. We claim that, under some regualirty conditions, there exists estimates $\what{\Delta \svbv}^{(1)}, \cdots, \what{\Delta \svbv}^{(n/2)}$ such that, with probability at least $1-\delta$,
\begin{align}
    \max_{i\in[n/2]}
    \mathrm{MSE}(\Delta \svbv^{(i)}, \what{\Delta \svbv}^{(i)})
    \leq  \dfrac{\varepsilon^2 +  ce^{c'\bGM} s\log p_v }{p} \qtext{whenever} n \geq \frac{ce^{c'\bGM} s^2p^2 \log \frac{npp_v}{\delta}}{\varepsilon^4}. \label{mse_measurement_error}
\end{align}
where $c$ and $c'$ are constants. As a result, we see that the unit-wise mean squared error for imputing the measurement error scales as $O(s\log k)/p$ when $n$ scales as $O(s^2 p^2\log sp)$. 

\paragraph{Proof of bound \cref{mse_measurement_error}.}
Fix any $i \in [n/2]$. Then, using \cref{eq_coditional_distribution_vay|delta_v} and the definitions of $\TrueExternalField$ and $\TrueParameterMatrix$, we can write $\TrueExternalFieldI \defn \TrueExternalField(\Delta \svbv^{(i)})$ as a $(s+1)$-sparse linear combination of $p_v + 1$ vectors, i.e.,
\begin{align}
    \TrueExternalFieldI = \tbf{B} \tbf{a}^{(i)} \!\!\!\qtext{where}\!\!\! \tbf{B}  \!\defn\! \begin{bmatrix} \TrueExternalField, -2 \TrueParameterRowtTop[1], \cdots, -2 \TrueParameterRowtTop[p_v] \end{bmatrix}\!\in \! \Reals^{p\times (p_v+1)} \!\!\!\qtext{and}\!\!\! \tbf{a}^{(i)} \!\defn\! \begin{bmatrix} 1\\ \Delta \svbv^{(i)} \end{bmatrix} \!\in\! \Reals^{(p_v+1) \times 1}, \label{eq_sparse_lasso_first}
\end{align}
%
with $\szeronorm{\tbf{a}^{(i)}} \leq s+1$.
%
%
While we do not know the matrix $\tbf{B}$, we can produce an estimate $\what{\tbf{B}}$ using $\EstimatedExternalField$ and $\EstimatedParameterMatrix$ such that, with probability at least $1-\delta$,
\begin{align}
\max_{t\in[p]}\stwonorm{\what{\tbf{B}}_t \!-\! \tbf{B}_t} & \leq \varepsilon_0 \qtext{whenever} n  \geq \frac{ce^{c'\bGM}\log\frac{p}{\delta}}{\varepsilon_0^4}. \label{eq_b_estimate_guarantee}
\end{align}
This guarantee follows directly from \cref{eq_theta_estimate_guarantee} and the definition of $\tbf{B}$ in \cref{eq_sparse_lasso_first}. Then, conditioning on this event and whenever $n$ satisfies \cref{eq_b_estimate_guarantee}, we can write
\begin{align}
    \TrueExternalFieldI = \what{\tbf{B}} \tbf{a}^{(i)} + \zeta, \label{eq_sparse_lasso_second}
\end{align}
where $\zeta$ is the error term which can be bounded as follows
\begin{align}
    \stwonorm{\zeta} \sequal{(a)} \stwonorm{\tbf{B} \tbf{a}^{(i)} - \what{\tbf{B}} \tbf{a}^{(i)}} & \sless{(b)} \opnorm{\tbf{B} - \what{\tbf{B}}} \stwonorm{\tbf{a}^{(i)}} \\
    & \sless{(c)} \sqrt{p} \sqrt{\szeronorm{\tbf{a}^{(i)}}} \sinfnorm{\tbf{a}^{(i)}} \max_{t\in[p]}\stwonorm{\tbf{B} - \what{\tbf{B}}_t} \\
    & \sless{(d)} \sqrt{(s+1)p}\max\{1, \xmax\}\varepsilon_0 \leq \sqrt{(s+1)p}(1+\xmax)\varepsilon_0, \label{eq_bounded_zeta} 
\end{align}
where $(a)$ follows from \cref{eq_sparse_lasso_first} and \cref{eq_sparse_lasso_second}, $(b)$ follows from sub-multiplicativity of induced matrix norms, $(c)$ follows from standard matrix norm inequalities, and $(d)$ follows from \cref{eq_b_estimate_guarantee}, and because $\szeronorm{\Delta \svbv^{(i)}} \leq s$ and $\sinfnorm{\Delta \svbv^{(i)}} \leq \xmax$ for all $i \in \normalbraces{1, \cdots, n/2}$.

Now, we re-parameterize the set $\ParameterSet_{\ExternalField}$ as the set 
$\ParameterSet_{\tbf{a}} = \braces{\tbf{a} \in \Reals^{(p_v + 1) \times 1}: \szeronorm{\tbf{a}} \leq s+1 \stext{and} \sinfnorm{\widehat{\tbf{B}} \tbf{a}} \leq \aGM}$ whose metric entropy is given by $\metric_{\tbf{a}}(\radius) = \log\Bigbrackets{\bigparenth{1+\frac{c}{\radius}}^{s+1} \binom{p_v+1}{s+1}}$ for some constant $c$ \cite[Corollary 4]{DaganDDA2021}. Using this, the guarantees in \cref{theorem_parameters} simplify, and the estimates $\EstimatedExternalFieldI[1], \cdots, \EstimatedExternalFieldI[n]$ defined in \cref{eq_estimated_parameters} satisfy
\begin{align}
    \max_{i\in[n]}\stwonorm{\EstimatedExternalFieldI \!-\! \TrueExternalFieldI} &\leq \max\normalbraces{\varepsilon_1, ce^{c'\bGM} \sqrt{s\log p_v}} \qtext{whenever} n \geq \frac{cse^{c'\bGM} p^2  \log \frac{npp_v}{\delta}}{\varepsilon_1^4}, \label{eq_theta_simplified_guarantee}
\end{align}
with probability at least $1-\delta$. Further, we define $\what{\tbf{a}}^{(i)}$ to be such that $\EstimatedExternalFieldI  = \what{\tbf{B}} \what{\tbf{a}}^{(i)}$ and assume that the smallest singular value of $\what{\tbf{B}}$ is non-zero, i.e., $\sigma_{\min}(\what{\tbf{B}}) \geq 0$. Then, we upper bound $\stwonorm{\tbf{a}^{(i)} - \what{\tbf{a}}^{(i)}}$ by lower bounding $\stwonorm{\EstimatedExternalFieldI \!-\! \TrueExternalFieldI}$ as follows:
\begin{align}
    \stwonorm{\EstimatedExternalFieldI \!-\! \TrueExternalFieldI} & \sequal{(a)} \stwonorm{\what{\tbf{B}} \tbf{a}^{(i)} + \zeta \!-\! \what{\tbf{B}} \what{\tbf{a}}^{(i)} } \sgreat{(b)} \stwonorm{\what{\tbf{B}} \tbf{a}^{(i)} - \what{\tbf{B}} \what{\tbf{a}}^{(i)}} \!-\! \stwonorm{\zeta} \sgreat{(c)} \sigma_{\min}(\what{\tbf{B}}) \stwonorm{\tbf{a}^{(i)} - \what{\tbf{a}}^{(i)}}\!-\! \stwonorm{\zeta}, \label{eq_lower_bound_theta_error}
\end{align}
where $(a)$ follows from \cref{eq_sparse_lasso_second} and the way we defined $\what{\tbf{a}}^{(i)}$, $(b)$ follows by the triangle inequality and because $\stwonorm{\zeta}$ can be made as small as desired by choosing an appropriate $\varepsilon_0$ in \cref{eq_bounded_zeta}, and $(c)$ follows because $\sigma_{\min}(\what{\tbf{B}}) \geq 0$. Re-arranging \cref{eq_lower_bound_theta_error}, and using \cref{eq_bounded_zeta} with $\varepsilon_0 = c\varepsilon/\sqrt{sp}$ and \cref{eq_theta_simplified_guarantee} with $\varepsilon_1 = \varepsilon$, we obtain with probability at least $1-\delta$,
\begin{align}
    \stwonorm{\tbf{a}^{(i)} - \what{\tbf{a}}^{(i)}} 
    %
    & \leq \frac{1}{\sigma_{\min}} \Bigparenth{\varepsilon + ce^{c'\bGM} \sqrt{s\log p_v}} \qtext{whenever} n \geq \frac{cs^2e^{c'\bGM} p^2  \log \frac{npp_v}{\delta}}{\varepsilon^4}.
\end{align}
Then, \cref{mse_measurement_error} follows from the definition of the mean squared error.
%
%
%
%
%
%
%
%
%
%