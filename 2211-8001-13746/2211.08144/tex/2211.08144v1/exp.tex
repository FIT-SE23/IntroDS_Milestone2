\section{Experimental Results}
\label{sec:Results}
To evaluate our proposed model, we conduct several experiments
over a variety of challenging scenarios and compare our results against state-of-the-art methods on public benchmarks. We also perform extensive ablation experiments to delve into our network structure. 

%-------------------------------------------------------------------------
\subsection{Implementation Details}

We implement our framework using Pytorch on a workstation with a single NVIDIA Titan XP GPU card. We adopt ResNet-18~\cite{he2016deep} without bottleneck layers as our backbone. The MLP contains two fully-connected layers and ReLU activation in CVP, and each input of CVT utilizes one convolutional layer with kernel size $1\times 1$. The decoder is composed of 5 convolutional blocks and upsampling layers with the spatial resolution increasing by a factor of 2.
% And the F of Cross-View Transformer use one convolutional layers(kernel size 3*3, stride=1, padding=1).The discriminator architecture is inspired by DoveNet. 
%The decoder consists of 4 deconvolutional blocks with the increasing spatial resolution by a factor of 2 and the number of channels decreasing to 64, 32, 16, and 2, respectively.
All the input images are normalized to $1024\times 1024$ and the output size is $256 \times 256$.
The network parameters are randomly initialized, and we adopt the Adam optimizer~\cite{kingmaadam} and use a mini-batch size of 6. The initial learning rate is set to $1\times 10^{-4}$ and is decayed by a poly learning rate policy where the initial
learning rate is multiplied by $(1-\frac{iter}{total\_iter})^{0.9}$ after each iteration. 
% In particular, we use the ResNet-18 architecture without bottleneck layers. 
% The Cycled View Projection simply use 2 the MLP. The MLP contains two fully-connected layers. 
% The network takes in RGB images of size 3 × 1024 × 1024 as input, and produces a 256 × 256 segmentation as output. We train model with Cross-Entropy Loss [] for segmentation and L1 Loss for cycle structure. All experiments are performed on a workstation with NVIDIA 1080Ti GPU cards.
In practice, it takes 50 epochs to converge our model, and our model can run in real time ($25$ FPS) on our single-GPU platform. 

\input{comp_six}

%-------------------------------------------------------------------------
\subsection{Datasets}

We evaluate our approach on three datasets: KITTI \cite{geiger2012we}, Argoverse \cite{chang2019argoverse}, and Nuscenes \cite{caesar2020nuscenes}. For performance assessment, we adopt the mean of Intersection-over-Union (mIOU) and Average Precision (mAP) as the evaluation metrics. 

\textbf{KITTI.} Since KITTI has not provided sufficient annotation for road layout or vehicles to be used in our task, we generally follow the practice of \cite{2020MonoLayout}, in which the results are categorized in the following datasets.  
For comparison with state-of-the-art 3D vehicle detection approaches, we evaluate performance on the KITTI 3D object detection (\textit{KITTI 3D Object}) split of Chen et al. \cite{2016Monocular}, i.e., 3712 training images and 3769 validation images. The \textit{KITTI Odometry} dataset is used to evaluate the road layout, the annotation of which comes from the Semantic KITTI dataset \cite{behley2019semantickitti}.
% For the KITTI dataset, we follow the practice of [] and its BEV ground-truths are obtained by registering depth and semantic segmentation of Lidar scans in top view. 
In addition to the previous two datasets, we evaluate the performance on the \textit{KITTI Raw} split used in \cite{2018Learning}, i.e., 10156 training images and 5074 validation images. Since its ground-truths are produced by registering the depth and semantic segmentation of Lidar scans that are not sufficiently dense, we apply image dilation and erosion to produce better ground-truth annotations.

\textbf{Argoverse.} Furthermore, we also compare methods on \textit{Argoverse}, which provides a high-resolution semantic occupancy grid and vehicle detection in top-view for evaluating the spatial layout of roads and vehicles, with 6723 training images and 2418 validation images. \wx{For multiple semantic categories, we follow the evaluation protocol of \cite{roddick2020predicting}, which selects 1 static class (i.e., Drivable) and 7 dynamic object classes (i.e., Vehicle, Pedestrian, Large vehicle, Bicycle, Bus, Trailer, and Motorcycle) to conduct comparison experiments.}
% To the best of our knowledge, there is no published prior work that reasons jointly about road and vehicle occupancies. However, there exist approaches for road layout estimation [24, 34], and a separate set of approaches for vehicle detection [6, 30]. Furthermore, each of these approaches evaluate over diferent datasets (and in cases, diferent train/validation splits). To ensure fair comparision with all such approaches, we organize our results into the following categories.

\wx{\textbf{Nuscenes.} The \textit{NuScenes} dataset consists of 1000 twenty-second video clips collected in Boston and Singapore. It provides 3D bounding box annotations for 23 object classes including walkway, car, trailer, and so on. In our experiments, we also follow the protocol of \cite{roddick2020predicting}. We choose 4 static classes (i.e., Drivable, Pedestrian crossing, Walkway, and Carpark) and 10 dynamic object classes (i.e., Car, Truck, Bus, Trailer, Construction vehicle, Pedestrian, Motorcycle, Bicycle, Traffic cone, and Barrier) for evaluation. In addition, we also use their training and validation split, i.e., 28,008 training images and 5,981 validation images.} 

%-------------------------------------------------------------------------
\subsection{Comparison Methods and Performance Evaluation}

\input{table_bin1}
\input{table_bin2}
\input{table_bin3}

\textbf{Comparison Methods.} 
For evaluation, we compare our model with some of the state-of-the-art methods \wx{for road layout estimation and vehicle occupancy estimation}, including VED~\cite{lu2019monocular}, MonoLayout~\cite{2020MonoLayout}, VPN~\cite{pan2020cross}, Mono3D~\cite{2016Monocular}, OFT~\cite{roddick2018orthographic}, and CVT \cite{yang2021projecting}. Among these methods, Mono3D~\cite{2016Monocular} and OFT~\cite{roddick2018orthographic} are specifically used to detect vehicles in top-view. 
% MonoOccupancy is originally designed for static scene layout estimation, but it is extended in \cite{2020MonoLayout} for predicting vehicle occupancies. 
For the quantitative results, we follow the ones reported in \cite{2020MonoLayout}. For MonoLayout~\cite{2020MonoLayout}, we compare with their latest online reported results, which are generally better than the ones reported in their original paper.
% its results on these datasets have been updated along with their officially released code\footnote{https://github.com/hbutsuak95/monolayout}. Since their latest results are generally better than the ones reported on their original paper, we apply their latest results in our experiments for comparison.  
VPN~\cite{pan2020cross} originally adopted multiple views from different cameras to generate the top-view representation. We adapt their model for single-view input and then retrain it using the same training protocol for our task. Likewise, VED~\cite{lu2019monocular} is retrained for the benchmarks of road layout estimation, and we obtain comparable or better results than the ones reported in \cite{2020MonoLayout}. \wx{CVT \cite{yang2021projecting} is our preliminary version, which correlates the features of the views before and after projection and performs feature selection.}

\wx{In addition, we also compare our model against the state-of-the-art methods for multi-class semantic estimation. These methods include VED \cite{lu2019monocular}, PointPillars \cite{lang2019pointpillars}, VPN \cite{pan2020cross}, PON \cite{roddick2020predicting}, OFT \cite{roddick2018orthographic}, 2D-Lift \cite{dwivedi2021bird}, Stitch \cite{can2022understanding}, and STA \cite{saha2021enabling}. Most of these works focus on estimating the top-view semantic segmentation maps, and the rest use detection methods. We follow the single frame quantitative results reported in \cite{dwivedi2021bird}, \cite{can2022understanding}, and \cite{saha2021enabling} for the \textit{Argoverse} and \textit{Nuscenes} datasets.
}

\textbf{Road layout estimation.} To evaluate the performance of our model on the task of road layout estimation, we compare our model against VED~\cite{lu2019monocular}, MonoLayout~\cite{2020MonoLayout}, VPN~\cite{pan2020cross}, and CVT~\cite{yang2021projecting} on the \textit{KITTI Raw} and \textit{KITTI Odometry} datasets. Note that, since we post-process the ground-truth annotations of \textit{KITTI Raw}, we retrain all the comparison methods under the same training protocol.
The comparison results are demonstrated in Table \ref{tab:kitti_sota}. Additionally, we also compare them on \textit{Argoverse Road}, as shown in Table \ref{tab:argo_sota}. 
As observed, in these three benchmarks, our model shows advantages over the competitors in both mIOU and mAP. \wx{In addition, compared with our preliminary version (i.e., CVT~\cite{yang2021projecting}), the performance also exceeds about +1\%, especially in mAP, because the spatial deviation has been rectified using multi-scale FTVP.} Examples are shown in Fig.~\ref{fig:road_layout}. Note that the ground-truths may contain noise, since they are converted from the Lidar measurements. Even so, our approach can still produce satisfactory results. 
% In the table, the results of MonoOccupancy and Schulter et al. follow the numbers provided in []. Since Pan et al. have not been validated on these datasets, we train and evaluate their publicly released model.

\textbf{Vehicle occupancy estimation.} Compared with road layout estimation, estimating vehicle occupancies is a more challenging task, since the scales of vehicles vary and there exist mutual occlusions in the scenes. For evaluation, we perform comparison experiments on the \textit{KITTI 3D Object} and \textit{Argoverse Vehicle} benchmarks against VED \cite{lu2019monocular}, Mono3D~\cite{2016Monocular}, OFT~\cite{roddick2018orthographic}, MonoLayout~\cite{2020MonoLayout}, VPN \cite{pan2020cross}, and CVT~\cite{yang2021projecting}. The results are shown in Tables \ref{tab:argo_sota} and \ref{tab:object_sota}. In Table \ref{tab:object_sota}, our model demonstrates superior performance against the comparison methods. Since \textit{KITTI 3D Object} contains several challenging scenarios, most comparison methods barely obtain $30\%$ mIOU and $50\%$ mAP, while our model gains $40.69\%$ and $59.07\%$, respectively, which shows at least $34.8\%$ and $28.7\%$ improvement over prior methods. \lqq{Our model achieves significant advantage over CVT by at least $4.9\%$ and $17.5\%$ in mIOU and mAP, brought by the structure improvement of FTVP as well as the multi-scale FTVP modules, which beefs up the model and corrects the spatial deviation.}
% It indicates that our cross-view transformation module 
For the evaluation on \textit{Argoverse Vehicle}, our model outperforms others by a large margin, i.e., at least $46.4\%$ and $29.9\%$ boost over the comparison methods in mIOU and mAP, respectively. 
% Note that, the dataset \textit{Argoverse} provides the vehicle occupancies and the corresponding road layout. Thus, it is not only our cross-view transformer but also our context-aware discriminator that plays an important role. 
% Note that, since the dataset Argoverse provides the vehicle occupancies and the corresponding road layout, we provide our results with and without using the context-aware discriminator. 
On the first three rows of Fig.~\ref{fig:vehicle_results}, we show the examples on vehicle occupancy estimation on \textit{KITTI 3D Object}. For the challenging cases with multiple vehicles parked on the sides of roads, our model can still perform well. The last four rows of Fig.~\ref{fig:vehicle_results} show examples of the joint estimation for roads and vehicles on \textit{Argoverse}, and we highlight the advantages of our results.

\input{comp_figures}

\input{table_nu}

\input{table_argo}

\input{compare_nu}

\textbf{Multi-class semantic estimation.} \wx{We extend our model to address multi-class semantic segmentation problems on \textit{Argoverse} with 8 classes and on \textit{Nuscenes} with 14 classes. Compared with road layout estimation and vehicle occupancy estimation, multi-class semantic estimation is a more challenging task because it must accommodate the broad roads and a wide variety of vehicles with different scales. We conduct comparison experiments against VED \cite{lu2019monocular}, VPN \cite{pan2020cross}, PON \cite{roddick2020predicting}, and Stitch \cite{can2022understanding} for the \textit{Argoverse} dataset. For \textit{Nuscenes}, we introduce more comparison methods including PointPillars \cite{lang2019pointpillars}, OFT \cite{roddick2018orthographic}, 2D-Lift \cite{dwivedi2021bird}, and STA \cite{saha2021enabling}. The comparison results are depicted in Tables~\ref{tab:nu} and \ref{tab:agro}. As observed, our model shows superior performance over other methods. Our model offers significant improvement over the state-of-the-art methods on \textit{Argoverse} for the ``trailer" class.}
\wxr{In Fig.~\ref{fig:fig_nu}, we illustrate the examples of multi-class semantic estimation on \textit{Nuscenes}. We observe
that our model performs better than other methods on position and shape estimation for both static and dynamic classes.}

\label{sec:Third}


%-------------------------------------------------------------------------
\subsection{Ablation Study}

\label{sec:Fourth}
% We perform the ablation study for the cross-view transformer and the context-aware discriminator.
To delve into our network structure, we conduct several ablation experiments using the front-to-top view projection module and the context-aware discriminator. 

\textbf{Front-to-top view projection module.} Recall that our front-to-top view projection module consists of CVP and CVT. Specifically, CVP can be divided into the MLP and the cycle structure. CVT can be decomposed into a cross-view correlation scheme and a feature selection scheme. 
In the following, we will investigate the necessity of these modules based on the dataset \textit{KITTI 3D Object} in Table \ref{tab:cvt1}. 

First, the baseline is the vanilla encoder-decoder network using the same encoder and decoder as our model. Then we insert the MLP structure to the baseline. As shown in Table \ref{tab:cvt1}, it obviously improves the effectiveness of view projection. Next, we add a cross-view correlation scheme into the network, which measures the correlation of $X$ and $X'$ and applies it as the attention map to enhance $X'$. As observed, with the involvement of the cross-view correlation scheme, the performance is significantly boosted. We then introduce the cycle structure as well as the cycle loss into the network, in which $X''$ will be fed into the cross-view correlation scheme. Finally, we insert the feature selection scheme, which further strengthens the performance of the model. 

\wx{In Fig.~\ref{fig:abla1}, we show several examples of the ablation study on the FTVP module. The examples are selected from the results of \textit{KITTI 3D Object} dataset. With the addition of structures (e.g., MLP, cross-view correlation, cycle structure, and feature selection) in the FTVP module, our model can effectively extract the masks of the individual vehicles, remove noises, and refine their shapes. }

\input{ablation1_figure}

\input{cvt1}

\input{cvt2}

\input{ablation2_skip_figure}

% For the analysis of the cross-view transformer, we conduct experiments on \textit{KITTI 3D Object}. First, we analyze the effectiveness of the cross-view transformer in our network. We compare our model with several variants: (1) the baseline network without cross-view transformer; (2) the baseline incorporated with a standard transformer (i.e., without the cycle structure and the feature selection scheme); (3) the network incorporated with a transformer which contains our feature selection scheme yet without the cycle structure; (4) the network with the complete cross-view transformer that includes cycle structure as well as the feature selection scheme. The analysis results are demonstrated in Table \ref{tab:cvt1}. In particular, without transformer, the performance of our model is barely compared with MonoLayout \cite{2020MonoLayout}. By incorporating a standard transformer (i.e., $K, Q, V$ are $f$) before MLP, the performance of our model is boosted due to the introduction of non-local interaction of local patches. With our feature selection scheme of the cross-view transformer (i.e., $K, V$ are $f$ and $Q$ is $f'$) slightly improves the performance, while the addition of the cycle structure emphasizes the critical features for view projection, which allows the transformer module to flexibly and effective select the salient features and thus leads to a big leap of the model performance. 


\textbf{Cross-view transformer.} 
We validate different input combinations of $K, Q, V$ for CVT. We demonstrate the results in Table \ref{tab:cvt2}. For all test cases, the query (i.e., $Q$) is assigned to a feature after view projection $X'$. As the most trivial case, we use $X'$ as $K$ and $V$ of CVT as well, which self-correlates all the non-local patches of $X'$. Since $X'$ may lose some information via view projection, CVT does not perform well. Because both $K$ and $V$ are assigned to $X$ or $X''$, it involves the features before view projection, but $X$ contains richer information than $X''$, which leads to better performance. Moreover, with $X$ and $X''$ corresponding to $K$ and $V$, the substantial information for view projection is implicitly introduced by $X''$ to strengthen the model. More specifically, using $X$ as the key is better for generating a precise relevance embedding, while applying $X''$ as the value encourages the involvement of most relevant features, leading to the optimal results. 

\textbf{Multi-scale FTVP modules.} Based on the model with a single FTVP module, we upgrade the network by integrating the multi-scale FTVP modules.
% In Table~\ref{tab:cvt1}, , we upgrade the network by integrating the multi-scale FTVP modules. 
As observed in Table~\ref{tab:cvt1}, the upgraded model improves mAP while decreasing mIOU. This is perhaps because the features of different scales deliver rich, detailed information but their learning lacks guidance. To address this, we employ the deep supervision scheme. As shown, our model achieves the optimal performance (40.69\% mIOU and 59.07\% mAP). Note that, 
% Adding multi-scale supervision seems to improve performance considerably especially for mAP. 
for reference, we also add deep supervision for the model with a single FTVP module, and its results are almost unchanged (39.48\% vs 39.97\% on mIOU and 54.78\% vs 54.53\% on mAP). Moreover, we show the representative visualization results with respect to the multi-scale FTVP modules in Fig.~\ref{fig:skip}. It is observed that the spatial deviation effect will be reduced in the presence of FTVPs.


\wx{To show the advantage of employing three FTVPs only, we compare our model against other variants, including FTVPs with local window attention, variants in which FTVPs have been replaced with naive convolutional layers, and variants with more than three FTVPs. The comparison results in terms of mIOU and mAP are demonstrated in Table~\ref{tab:cvt3}. However, local window FTVPs and convolutional layers are both local operations without the involvement of global information, while FTVP modules deployed on the shallow layers lack semantic information and demand a large number of parameters and a lot of computation for higher-resolution features. Thus, these variants all lead to sub-optimal performance, so we choose to retain three FTVPs in our network.}

\input{cvt3}

\subsection{Network Efficiency}

\wxr{We measure FPS, the number of parameters, and FLOPs for the competing methods (i.e., VED \cite{lu2019monocular}, VPN \cite{pan2020cross}, PON \cite{roddick2020predicting}, and Stitch \cite{can2022understanding}) in Table~\ref{tab:fps}. All methods are tested on the same platform using a single NVIDIA Titan XP GPU. As observed, our model achieves real-time performance with few parameters. Thus, our model efficiency is comparable to these methods without using any model compression techniques.}

\subsection{Panorama HD Map Generation}

We showcase the application of our model on the \textit{Argoverse} dataset for generating a panorama HD map via stitching the road layout estimation given the consecutive front-view images. The generated HD map is shown in Fig. \ref{fig:hdmap}, highlighting the potential of our approach for generating panorama HD maps.

\subsection{Failure cases}

Fig.~\ref{fig:fail} shows a few scenarios in which we cannot produce an accurate road layout estimation. First, our model may produce inaccurate prediction in the cases of occlusion and sharp turns (see the first and second rows in Fig.~\ref{fig:fail}). Second, our model may be confused by multiple close vehicles (see the third row in Fig.~\ref{fig:fail}).

\input{fps}

\begin{figure}
    \centering
    \includegraphics[width=0.5\textwidth,height=6cm]{imgs/HD/HDmap.png}
    \includegraphics[width=0.5\textwidth,height=6cm]{imgs/HD/2.png}
    \includegraphics[width=0.5\textwidth,height=4.5cm]{imgs/HD/1.png}
    \caption{We montage the estimated road layout from the image sequences of \textit{Argoverse} to produce three panorama HD maps (on the right side of the figure) containing the road layout and vehicle occupancies. }
    \label{fig:hdmap}
\end{figure}

\input{fail}

