\section{Our Proposed Method}
\label{sec:Method}


\subsection{Network Overview}


The goal of our work is to estimate the road scene layout and vehicle occupancies on the bird's-eye view in the form of semantic masks given a monocular front-view image. 
% Formally, given an input image
% % $\mathcal{I}_{F}$, 
% $I$, our model aims to generate a segmentation mask
% % $\mathcal{M}_{BEV}$
% $\hat{M}$ represented for road or vehicles on the ground space. 

% Our network architecture is based on a GAN-based framework. In specific, as shown in Fig.~\ref{fig:framework}, the generator is an encoder-decoder architecture, in which the input frontal view image $I$ is first passed through the encoder that adopts ResNet~\cite{he2016deep} as the backbone network to extract visual features, then our proposed cross-view transformation module that enhances the features for view projection, and finally the decoder to produce the top-view masks $\hat{M}$. On the other hand, we propose a context-aware discriminator (see Fig.~\ref{fig:D}) that discriminates against the masks of vehicles by taking the road context into account.% $\mathcal{M}_{BEV}$. 
% In the following subsections, we will elaborate the details of our cross-view transformation module and the context-aware discriminator. 

\wx{Our network architecture is shown in Fig.~\ref{fig:framework}. The front-view image $I$ is first passed through the encoder based on ResNet~\cite{he2016deep} as the backbone network to extract multi-scale visual features. After that, our proposed front-to-top view projection modules project the encoded features of different scales and propagate them to the decoder to produce the top-view semantic mask $\hat{M}$. In the following subsections, we will elaborate the details of our front-to-top view projection module and its multi-scale deployments on various scales. }

\subsection{Front-to-Top View Projection Module}

% [introduce the transformer based view transformation enhancement module]
Due to the large gap between front-views and top-views, there a lot of image content goes missing during view projection, so the traditional view projection techniques lead to defective results. To this end, the hallucination ability of CNN-based methods has been exploited to address the problem, but the patch-level correlation of both views is not trivial to model within deep networks.

In order to strengthen the view correlation while exploiting the capability of deep networks, we introduce a \wx{front-to-top view projection module into our framework}, which enhances the extracted visual features for projecting front-view to top-view. 
% In specific, the patch-level correlation between the features of the frontal view and the features of the top view, and the critical spatial information needed during propagating through multi-layer perceptron (MLP). Both of them can guide the network to focus on the most relevant regions of the frontal view images. 
The structure of our proposed \wx{FTVP} module is shown in Fig. \ref{fig:framework}, and it is composed of two parts: cycled view projection and cross-view transformer. 

\textbf{Cycled View Projection (CVP).} Since the features of front-views are not spatially aligned with those of top views due to their large gap, we follow \cite{pan2020cross} and deploy the MLP structure consisting of two fully-connected layers to project the features of front-view to top-view, which can overtake the standard information flow of stacking convolution layers.
As shown in Fig. \ref{fig:framework}, $X$ and $X'$ represent the feature maps before and after view projection, respectively. 
% Here, we follow the practice of \cite{2019Cross} that performs the holistic view projection via a MLP structure consisting of two fully-connected layers, 
Hence, the holistic view projection can be achieved by: $X' = \mathcal{F}_{MLP}(X)$, where $X$ refers to the features extracted from the ResNet backbone. 
\label{sec:First}

However, such a simple view projection structure cannot guarantee that the information of front-views is effectively delivered. Here, we introduce a cycled self-supervision scheme to consolidate the view projection, which projects the top-view features back to the domain of front-views. 
% In order to discover the most critical information for view projection, we introduce such a cycle structure which projects the top-view features back to the domain of the frontal view. 
As illustrated in Fig. \ref{fig:framework}, $X''$ is computed by cycling $X'$ back to the front-view domain via the same MLP structure, i.e., $X'' = \mathcal{F}'_{MLP}(X')$. To guarantee the domain consistency between $X'$ and $X''$, 
% we introduce two losses, i.e., $\mathcal{L}_{proj}$ and $\mathcal{L}_{front}$, as expressed below. 
we incorporate a cycle loss, i.e., $\mathcal{L}_{cycle}$, as expressed below. 
\begin{align}
    %  \mathcal{L}_{proj} &= \| \mathcal{F}_{D}(f') - M \|_1,\\
    \mathcal{L}_{cycle} &= \| X - X'' \|_1.
\end{align}

% where $\mathcal{L}_{proj}$ tends to force $X'$ to depict the features after view projection. 
% To do so, we introduce a decoding network $\mathcal{F}_{D}(\cdot)$ that is composed of XXX deconvolutional layers and supervised by the top view ground-truth $M$.
% The cycle loss $\mathcal{L}_{cycle}$ is introduced to align $X$ and $X''$.
% , so that their discrepancy may imply the critical information during view projection.
The benefits of the cycle structure are two-fold. First, similar to the cycle consistency-based approaches \cite{zhu2017unpaired,dwibedi2019temporal}, cycle loss can innately improve the representativeness of features, since cycling back the top-view features to the front-view domain will strengthen the connection between both views. Second, when the discrepancy between $X$ and $X''$ cannot be further narrowed, $X''$ actually retains the most relevant information for view projection, since $X''$ is reciprocally projected from $X'$. Hence, $X$ and $X'$ refer to the features before and after view projection. $X''$ contains the most relevant features of the front-view for view projection. In Fig.~\ref{fig:views}, we show two examples by visualizing the features of the front-view and top-view. {Specifically, we visualize them by selecting the typical channels of the feature maps (i.e., the 7th and 92nd for two examples of Fig.~\ref{fig:views}) and aligning them with the input images.} As observed, $X$ and $X''$ are similar, but quite different from $X'$ due to the domain difference. We can also observe that, via cycling, $X''$ concentrates more on the road and the vehicles. $X$, $X'$ and $X''$ will be fed into the cross-view transformer. 
% Thus, by narrowing down the discrepancy of $X$ and $X''$ while optimizing the view-projected results during training, the common patches of $X$ and $X''$ reflect their most effective features for view projection.
% [figure example]

\begin{figure}
    \centering
    \begin{tabular}{c@{}c@{}c@{}c@{}c}
    \includegraphics[width=0.13\textwidth,height=2.25cm]{imgs/xxx/X_.png}&
    \includegraphics[width=0.005\textwidth,height=2.25cm]{imgs/white.png}&
    \includegraphics[width=0.13\textwidth,height=2.25cm]{imgs/xxx/X__.png}&
    \includegraphics[width=0.005\textwidth,height=2.25cm]{imgs/white.png}&
    \includegraphics[width=0.13\textwidth,height=2.25cm]{imgs/xxx/X___.png}\\
    \includegraphics[width=0.13\textwidth,height=2.25cm]{imgs/xxx/X.png}&
    \includegraphics[width=0.005\textwidth,height=2.25cm]{imgs/white.png}&
    \includegraphics[width=0.13\textwidth,height=2.25cm]{imgs/xxx/XX.png}&
    \includegraphics[width=0.005\textwidth,height=2.25cm]{imgs/white.png}&
    \includegraphics[width=0.13\textwidth,height=2.25cm]{imgs/xxx/XXX.png}\\
    $X$ & $ $ & $X'$ & $ $ & $X''$
    \end{tabular}
    \caption{Visualization of the features at front-view and top-view by aligning them with the images of the corresponding views.  }
    \label{fig:views}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=0.45\textwidth]{imgs/cvt+.png}
    \caption{Illustration of cross-view transformer. It contains the cross-view correlation scheme that correlates $X$ and $X'$ to gain the attention map $W$, and the feature selection scheme that extracts the most relevant information from $X''$ to be $T$ \wx{and then uses $X$ to supplement the possibly missing information of $T$}.}
    \label{fig:cvt}
\end{figure}

% Hence, the cross-view transformer is fed with $X$, $X'$, and $X''$ to strengthen the extracted features. 
\textbf{Cross-View Transformer (CVT).} The main purpose of CVT is to correlate the features before view projection (i.e., $X$) and the features after view projection (i.e., $X'$) to strengthen the latter ones. Since $X''$ contains the substantial information of the front-view for view projection, it can be used to further enhance the features as well.
As illustrated in Fig.~\ref{fig:cvt}, CVT can be roughly divided into two schemes: the cross-view correlation scheme that explicitly correlates the features of views to achieve an attention map $W$ to strengthen $X'$ and the feature selection scheme that extracts the most relevant information from $X''$. 

Specifically, $X$, $X'$, and $X''$ serve as the key $K$ ($K \equiv X$), the query $Q$ ($Q \equiv X'$), and the value $V$ ($V \equiv X''$) of CVT, respectively. In our model, the dimensions of $X$, $X'$, and $X''$ are set as the same. 
$X'$ and $X$ are both flattened into patches, and each patch is denoted as $\textbf{x}'_i \in X' (i \in [1,\dots, hw])$ and $\textbf{x}_j \in X (j \in [1,\dots, hw])$, where $hw$ refers to the width of $X$ times its height. 
Thus, the relevance matrix $R$ between any pairwise patches of $X$ and $X'$ can be estimated, i.e., for each patch $\textbf{x}'_i$ in $X'$ and $\textbf{x}_j$ in $X$, their relevance $r_{ij} (\forall r_{ij} \in R)$ is measured by the normalized inner-product:
\begin{align}
    r_{ij} = \langle \frac{\textbf{x}'_i}{||\textbf{x}'_i||}, \frac{\textbf{x}_j}{||\textbf{x}_j||} \rangle.
\end{align}

With the relevance matrix $R$, we create two vectors $W$ ($W=\{w_i\}, \forall i \in [1,\dots, hw]$) and $H$ ($H=\{h_i\},\forall i \in [1,\dots, hw]$) based on the maximum value and the corresponding index for each row of $R$, respectively:
\begin{align}
    w_i &= \max_j r_{ij}, \forall r_{ij} \in R,\\
    h_i &= \arg\max_j r_{ij}, \forall r_{ij} \in R.
\end{align}

Each element of $W$ implies the degree of correlation between each patch of $X'$ and all the patches of $X$, which can serve as an attention map. Each element of $H$ indicates the index of the most relevant patch in $X$ with respect to each patch of $X'$. 

Recall that both $X$ and $X''$ are the features of the front-view, except $X$ contains its complete information, while $X''$ retains the relevant information for view projection. Assuming that the correlation between $X$ and $X'$ is similar to the correlation between $X''$ and $X'$, it is reasonable to utilize the relevance of $X$ and $X'$ (i.e., $R$) to extract the most important information from $X''$. 
To this end, we introduce a feature selection scheme $\mathcal{F}_{fs}$. With $H$ and $X''$, $\mathcal{F}_{fs}$ can produce new feature maps $T$ ($T=\{\textbf{t}_i\},\forall i \in [1,\dots, hw]$) by retrieving the most relevant features from $X''$:
\begin{align}
    \textbf{t}_i = \mathcal{F}_{fs}(X'', h_i), \forall h_i \in H, 
\end{align}
where $\mathcal{F}_{fs}$ retrieves the feature vector $\textbf{t}_i$ from the $h_i$-th position of $X''$.

% Based on the relevance $R$, we can first compute an attention map $S$ for each patch of $X'$, which is the maximum value of each row in $R$, such that $s_i = \max_j r_{ij}, \forall s_i \in S (i \in [1,H_{f'}\times W_{f'}])$. 

% [figure example?]
% Besides, since the cycle-back feature $X''$ is projected back from $X'$, it contains the substantial information of the frontal view for generating BEV masks. 
% Then, we incorporate a feature selection scheme to extract the most relevant information for each patch of $X'$ based on the relevance $R$, i.e., $h_i = \arg\max_j r_{ij}, \forall h_i \in H$.
% Concretely, we montage new feature maps based on the indices of the maximum element in each row in $R$. 
% the position indices of the most relevant patches in $X$, denoted as $H$, are extracted according to $h_i = \arg\max_j r_{ij}, \forall h_i \in H$. Recall that $X$ and $X''$ represent the features on the same view, so the relevance between $X$ and $X''$ can guide the feature selection for $X''$. The indices are used to montage a new flatten feature $T$ using the patches of $X''$ (i.e. $V$), i.e., $t_i = v_{h_i} (t_i \in T)$. 



% The relevance is first used to calculate a hard-attention map $H$ \textbf{[index vector?]}, i.e., $h_i = \arg\max_j r_{ij}$. $h_i$ is a hard index that represents the most relevant position in $X$ to the $i$-th position in $X'$. 
% Next, these hard indices are applied to guide the selection operation for $V$ (i.e. $X''$), i.e., selecting the $h_i$-th position from $V$, denoted as $v_{h_i}$.
% Since $X''$ is the cycle-back version of $X$, it remains the necessary spatial information for view projection. 
% So, we utilize it and the corresponding hard indices to form a new feature map denoted as $T$. 

Hence, $T$ stores the most relevant information of $X''$ for each patch of $X'$. \wx{However, it is worth noting that the information of $T$ all comes from $X''$, which is likely to cause information loss during view projection. To compensate for the lost information, it can be reshaped as the same dimension as $X$ and concatenated with $X$, which supplements the raw information.} Then the concatenated features will be weighted by the attention map $W$ and finally aggregated with $X'$ via a residual structure. To sum up, the process can be formally expressed as:
% In the meantime, the attention map $S$ with regard to $X$ or $X''$ can be learned as well, i.e., $s_i = \max_j r_{i,j}$. Hence, the final output feature maps of the cross-view transformer can be calculated via a residual structure, as below:
\begin{align}
    X_{out} = X' + \mathcal{F}_{conv}(\text{Concat}(X, T)) \odot W,
\end{align}
where $\odot$ denotes the element-wise multiplication and $\mathcal{F}_{conv}$ refers to a convolutional layer with $3 \times 3$ kernel size.
% with the kernel size $1\times 1$ for dimension reduction. 
$X_{out}$ is the final output of CVT and will then be passed to the decoder network to produce the segmentation mask of the top-view.  


\subsection{Multi-scale FTVP Modules}
\label{sec:Second}
\wx{As mentioned above, our proposed FTVP module can acquire the enhanced top-view features $X_{out}$ to generate refined segmentation results. However, the resolution of $X_{out}$ is too low to completely retain the information on visual details, which tends to cause spatial deviation of object location from top-views. }

\wx{To remedy the shortcomings, we introduce multi-scale FTVP modules to bridge the low-level features and the upsampled top-view features in the decoder. As shown in Fig. \ref{fig:framework}, the extracted front-view features $X_i$ of the $i$-th scales are passed through the FTVP modules for view projection in order to gain the corresponding top-view features $\hat{X}_i$. In practice, we employ the three inner most encoded features for projection to avoid unnecessary computational burden. 
% In general, the front-to-top view projection modules can be used to enhance the features at every scale. Nevertheless, the computation cost of the front-to-top view projection module is quadratic to feature size, making it unaffordable for large-size feature maps in our task. For simplicity, we directly remove three FTVPs with highest resolution (i.e., $i \in \{3,4,5\}$), which further lightens the our network. 
Next, the projected features $\hat{X}_i$ and the top-view features $X'_i$ in the upsampling stream are concatenated, i.e., $\hat{X}'_i=\text{Concat}(\hat{X}_i, X'_i)$, so that the projected features $\hat{X}_i$ manage to deliver rich low-level information to the decoder. Finally, we employ the deep supervision scheme to supervise the segmentation results generated by the top-view features $\hat{X}â€™_i$.}

\wx{We visualize the segmentation masks generated from the top-view features of different scales in Fig.~\ref{fig:re}. When the resolutions of the features increase, the false negatives (i.e., green areas) and false positives (i.e., red areas) of estimating the locations of vehicles gradually decrease due to the involvement of low-level information. In addition, for the features of $3$-rd, $4$-th, and $5$-th scales (i.e., the three highest resolution intermediate feature maps in the encoder), their produced results are almost the same, so it is sufficient to employ three FTVP modules only.}

\input{refine}

% \subsection{Context-aware Discriminator}

% In the discriminator of GAN-based framework, 
% to further refine the synthetic masks of vehicles, the spatial relationship between the vehicles and their context (i.e. road) can be exploited. To accomplish this, we propose a context-aware discriminator that not only attempts to distinguish the output vehicle masks and the ground-truth ones, but also explicitly utilizes the correlation between the vehicles and the roads to strengthen the discrimination.

% Particularly, with the estimated masks of vehicles $\hat{M}_v$ and the ground-truth mask of the road $M_r$ in the same scene, we deploy a shared CNN $\mathcal{F}_{D}$ to separately extract the features of $\hat{M}_v$ and the concatenation of $\hat{M}_v$ and $M_r$, and then calculate the inner-product of their features to evaluate their correlation, i.e., 
% \begin{align}
%     \hat{C}_{v,r} = \langle \mathcal{F}_D(\hat{M}_{v}), \mathcal{F}_D(\{\hat{M}_{v}, M_{r}\}) \rangle.
% \end{align}
% Likewise, the ground-truth mask of vehicles $M_{v}$ and the concatenation of $M_{v}$ and $M_{r}$ are fed through the same network with shared parameters, and then the correlation of ground-truth vehicles and road, $C_{v,r}$, is evaluated in the same way. 

% To this end, 
% $\hat{M}_{v}$ and $M_{v}$ are fed into a classifier $\mathcal{F}_D$ for a foreground object discrimination, while the correlations $\hat{C}_{v,r}$ and $C_{v,r}$ are sent into the other classifier $\mathcal{F}'_D$ for discrimination. In practice, for both classifiers, we adopt multiple convolutional layers and insert spectral normalization after each layer along with hinge losses for stabilizing training.
% Thus, the losses of the discriminator are:
% \begin{align}
%     &\mathcal{L}_1^{D} = \mathbb{E}[\max(0, 1 +\mathcal{F}_D(\hat{M}_{v}))]    + \mathbb{E}[\max(0, 1-\mathcal{F}_D(M_{v}))],\\
%     &\mathcal{L}_2^{D} = \mathbb{E}[\max(0, 1+\mathcal{F}'_D(\hat{C}_{v,r}))]
%     + \mathbb{E}[\max(0, 1-\mathcal{F}'_D(C_{v,r}))]. 
% \end{align}

% Hence, our context-aware discriminator allows us to distinguish the estimated and ground-truth vehicles, meanwhile discriminating the respective correlations between the vehicles and the road, which emphasizes the spatial relationship between the vehicles and the road. 

% \subsection{Loss Function}
% Overall, the loss function of our framework is defined as:
% \begin{align}
%     \mathcal{L} = \mathcal{L}_{BCE} + \lambda\mathcal{L}_{cycle} + \beta(\mathcal{L}_1^{D} + \mathcal{L}_2^{D}),
% \end{align}
% where $\mathcal{L}_{BCE}$ is a binary cross-entropy loss which serves as the main objective of the generator network to narrow the gap between the synthetic semantic mask and the ground-truth mask. $\lambda$ and $\beta$ are the balance weights of the cycle loss and the adversarial losses, respectively. In practice, $\lambda$ and $\beta$ are set as $0.001$ and $1$. 

\subsection{Loss Function}
Overall, the loss function of our framework is defined as:
\begin{align}
    \mathcal{L} = \sum_{i=0}^{n}\mathcal{L}^{i}_{seg} + \lambda\mathcal{L}_{cycle},
\end{align}
\wx{where $\mathcal{L}^{i}_{seg}$ is the cross-entropy loss for supervising the segmentation results produced by the features of the $i$-th scale. 
The main objective of the network to narrow the gap between the predicted semantic mask and the ground-truth mask. However, real-world scenes often include small objects such as vehicles and pedestrians, which lead to class-imbalance problems. Thus, in practice, we use the square root of the inverse class frequency to weight unbalanced loss for small classes. In addition, $\lambda$ is the balance weights of the cycle loss and it is set as $0.001$.}
