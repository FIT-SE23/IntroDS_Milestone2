\section{Related Work}
\label{sec:related_works}

\begin{figure*}
    \centering
    \includegraphics[width=0.98\textwidth]{imgs/framework++.png}
    \caption{As illustrated, our network aims to transform the front-view monocular image to the top-view road layout. The main component of our proposed \textit{front-to-top view projection module} is the cycled view projection (CVP) and the cross-view transformer (CVT), which projects the features from the front-view domain, $X$, to the top-view domain, $X'$. In CVP, it first utilizes an MLP-based cycle structure to retain the confident features for view projection in $X''$, and then CVT correlates the features of both views to attentively enhance $X'$. \wx{Moreover, we also utilize multi-scale FTVP modules to project the front-view features on varied scales to top-down view, which brings rich spatial information for top-view features in the upsampling stream to alleviate the spatial deviation on object location estimation.}}
    \label{fig:framework}
\end{figure*}



In this section, we survey the related literature on road layout estimation, vehicle detection, and street view synthesis on top-view representation. We also introduce the recent progress of transformers on vision tasks.

\textbf{BEV-based road layout estimation and vehicle detection.} Most road scene parsing works focus on semantic segmentation \cite{fan2020sne,teichmann2018multinet,yang2018denseaspp,yu2018bisenet,fu2019dual}, while there are a few attempts that derive top-view representation for road layout \cite{2012Automatic,2016HD,Geiger14,2017Predicting,2017Cognitive,2018Learning,2019A,lu2019monocular}. 
Among these methods, Schulter et al. \cite{2018Learning} propose estimating an occlusion-reasoned road layout on top-view from a single color image by depth estimation and semantic segmentation.
% Wang et al. \cite{2019A} extend \cite{2018Learning} to infer the parameterized road layouts. 
VED \cite{lu2019monocular} proposes a variational autoencoder (VAE) model to predict road layout from a given image, but without attempting to reason about the unseen layout from observation. VPN \cite{pan2020cross} presents a cross-view semantic segmentation by transforming and fusing the observation from multiple cameras. {~\cite{philion2020lift, roddick2020predicting} directly transform features from images to 3D space and finally to birdâ€™s-eye-view (BEV) grids.}
On the other hand, 
% the monocular image based 3D vehicle detection techniques has been developed in recent years {
many monocular image-based 3D vehicle detection techniques have been developed (e.g., \cite{2016Monocular,2017Mousavian,2020GS3D,lang2019pointpillars}). Several methods handle this problem by mapping the monocular image to the top-view. For instance, \cite{roddick2018orthographic} proposes mapping a monocular image to the top-view representation and treats 3D object detection as a task of 2D segmentation. \wx{2D-Lift~\cite{dwivedi2021bird} proposes the BEV feature transform layer to transform 2D image features to the BEV space, which exploits depth maps and 3D point cloud.} BirdGAN \cite{srivastava2019learning} also leverages adversarial learning for mapping images to bird's-eye-view. \wx{STA \cite{saha2021enabling} and Stitch \cite{can2022understanding} aggregate the temporal information to produce the final segmentation results through the transformation module.}
As another related work, \cite{wang2019monocular} does not focus on explicit scene layout estimation, focusing instead on the motion planning side. 
Most related to our work, \cite{2020MonoLayout} presents a unified model to tackle the task of road layout (static scene) and traffic participant (dynamic scene) estimations from a single image. In contrast, we propose an approach to explicitly model the large view projection that learns the spatial information to produce high-quality results.

% \textbf{Vehicle detection on top view.} 

\textbf{View transformation and synthesis.} 
%In the applications of traffic scenes, image synthesis techniques have been often applied \cite{zhu2018generative}. Relevant to our task, cross-view synthesis has been investigated in many prior works. 
Traditional methods (e.g.~\cite{lin2012vision,tseng2013image,huang2018lane}) have been proposed to handle the perspective transformation in traffic scenes. 
With the progress of deep learning-based methods, \cite{zhu2018generative} proposes a pioneering work to generate the bird's-eye-view based on the driver's view. They treat cross-view synthesis as an image translation task and adopt a GAN-based framework to accomplish it. Due to the difficulty in collecting annotation for real data, their model is trained from video game data. {\cite{abbas2019geometric} focuses exclusively on warping camera images to BEV images without performing any downstream tasks such as object detection.} 
Recent attempts~\cite{regmi2018cross,tang2019multi} on view synthesis aim to convert aerial images to street view images, or vice versa. Compared with these works, our purpose is quite different and requires not only the implicit view projection from front-view to top-view, but also the estimation of road layout and vehicle occupancies under a unified framework.

\textbf{Transformer for vision tasks.}
\wx{Convolutional neural networks(CNNs) are regarded as the most basic component in vision tasks. However, with the recent success of the Transformer~\cite{vaswani2017attention}, its ability to explicitly model pairwise interactions for elements in a sequence has been leveraged in many vision tasks, such as image classification \cite{zhang2020feature}, object detection \cite{carion2020end,zhu2020deformable}, activity recognition \cite{gavrilyuk2020actor}, and image super-resolution \cite{yang2020learning}. ViT \cite{dosovitskiy2021an} first applies the Transformer framework with non-overlapping image patches in the vision task. TNT \cite{han2021transformer} jointly leverages the inner transformer block and outer transformer block to enhance information exchange. Swin Transformer \cite{liu2021swin} obtains a larger receptive field by shifting the windows over the image. PVT \cite{wang2021pyramid} proposes a spatial reduction attention to reduce computational complexity. These models all show even more impressive modeling capabilities and achieve excellent performance. 
Inspired by these transformer-based models, our proposed cross-view transformer} attempts to establish the correlation between the features of views. In addition, we incorporate a feature selection scheme along with the non-local cross-view correlation scheme, significantly enhancing the representativeness of the features.
