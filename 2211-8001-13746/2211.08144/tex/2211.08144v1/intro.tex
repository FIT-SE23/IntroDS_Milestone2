%%%%%%%%% BODY TEXT
% \section{Introduction}



% Could we formulate the problem as real-time panorama HD map construction by monocular camera? General HD map just contains the road coordinates and structure, which is constructed by LiDAR and GPS. We call panorama HD map because we also include the vehicles on the map. And because  we just use a monocular camera, it is more convenient and  economic. If we formulate our problem like this, it could be better to link sequential frames to construct a relative big map in our results.

\IEEEPARstart{W}{ith} the rapid progress of autonomous driving technologies, there have been many recent efforts on related research topics, e.g., scene layout estimation~\cite{Geiger14,sun2019leveraging,roddick2020predicting,liu2020understanding,liang2019convolutional,2018Learning,2019A}, 3D object detection~\cite{2016Monocular,srivastava2019learning,roddick2018orthographic,simonelli2019disentangling,ding2020learning,2020GS3D}, vehicle behavior prediction~\cite{mozaffari2020deep,hong2019rules,kim2020advisable,ma2019trafficpredict}, and lane detection~\cite{hou2019learning,zou2019robust,philion2019fastdraw}. 

% Amongst these applications, reconstructing high-definition map (HD Map) is one of the crucial applications for road layout understanding, vehicle navigation, and surrounding environment sensing.
Among these tasks, high-definition map (HD map) reconstruction is fundamental and critical for perception, prediction, and planning of autonomous driving. 
% Its major issues are concerned with the estimation of a local map including the road layout as well as the occupancies of nearby vehicles in the 3D world.
Its major issues are concerned with the estimation of a local map including the road layout as well as the occupancies of nearby vehicles in the 3D world. Existing techniques rely on expensive sensors like LiDAR and require time-consuming computation for cloud point data. In addition, camera-based techniques usually need to perform road segmentation and view transformation separately, which causes distortion and the absence of content.
% Thus, we intend to push the limits of the cutting-edge technology and resolve a challenging yet realistic problem, i.e., real-time panorama HD map reconstruction by a monocular camera.
% , which is enabling to provide a much more economic and efficient solution. 

% Understanding complex layouts of the road scene is a crucial ability for the applications like driver assistance systems or autonomous driving. Recent success in deep learning-based perception systems enables pixel-level accurate semantic segmentation [3, 4, 33] and depth estimation [9, 15, 32] in the perspective view of the scene. 
% [Benefits of top-down representation]
% Amongst these works, a few efforts have been made to synthesizing the traffic scenes or scene layout from frontal view to bird's-eye eye view (BEV) or top view images captured by monocular cameras only, which pushes the limits of the cutting-edge technology for scene understanding.


% [What kind of problem we try to address]
% Inspired by prior works, in this paper, we aim to address the challenging problem that estimates the scene layout (i.e., road layout and vehicle occupancies) in the bird's-eye view given a single color image captured via the frontal view monocular camera. 
% The major issues of the panorama HD map reconstruction are concerned with the estimation of the road layout as well as the occupancies of nearby vehicles in 3D world based on the monocular image captured in the driver's view.
% our formulation on the reconstruction of panorama HD map concerns with the estimation of road layout as well as nearby vehicles in 3D world space based on the monocular image captured in the driver's view. 
To push the limits of the technology, our work aims to address the realistic yet challenging problem of estimating the road layout and vehicle occupancy in top-view, or bird's-eye view (BEV), given a single monocular front-view image (see Fig.~\ref{fig:teaser}).
% [Challenges/difficulties of transforming perspective view to bird eye's view]
However, due to the large view gap and severe view deformation, understanding and estimating the top-view scene layout from the front-view image is an extremely difficult problem, even for a human observer. 
% With the 3D structure of the scene, bird view generation can be easily achieved by changing the view point and projection.
% When the only input is a frontal view image instead of 3D structure of the scene, it will be substantially more difficult.
The same scene has significantly different appearances in the images of bird's-eye-view and the front-view. Thus, parsing and projecting the road scenes of front-view to top-view require the ability to fully exploit the information of the front-view image and innate reasoning about the unseen regions. 
% Meanwhile, the semantic representation and basic color should be consistent between two views. Imagining a car in front of you, after transforming to bird's-eye view, it should be the same car but with completely different appearance and size. 
% [Prior works' limitations]

\begin{figure}
    \centering
    \includegraphics[trim=1.3cm 0 0 0,clip,width=0.5\textwidth]{imgs/teaser.png}
    \caption{Given a front-view monocular image, we propose a front-to-top view projection module consisting of a cycle structure that bridges the features of the front-view and top-view in their respective domains, as well as a cross-view transformer that correlates views attentively to facilitate the road layout estimation.}
    \label{fig:teaser}
\end{figure}

Traditional methods (e.g.,~\cite{lin2012vision,tseng2013image}) focus on investigating the perspective transformation by estimating the camera parameters and performing image coordinate transformation, but gaps in the resulting BEV feature maps caused by geometric warping lead to poor results.
Recent deep learning-based approaches \cite{zhu2018generative,regmi2018cross} mainly rely on the hallucination capability of deep Convolutional Neural Networks (CNNs) to infer the unseen regions between views. 
In general, instead of modeling the correlation between views, these methods directly leverage CNNs to learn the view projection models in a supervised manner.
% Generally, they do not explicitly model the view correlation, which may increase the difficulty of learning the view projection model for such a larger gap between views and thus degrades the quality of the estimated scene layout. 
These models require deep network structures to propagate and transform the features of the front-view through multiple layers to spatially align with the top-view layout. However, due to the locally confined receptive fields of convolutional layers, it is difficult to fit a view projection model and identify the vehicles of small scales.
% Model the non-local correlation between views enables 

% Moreover, road layout provides the crucial context information to infer the position and orientation of vehicles, e.g., vehicles parked alongside the road. Yet, the prior road scene parsing methods usually ignore the spatial relationship between vehicles and roads.

% , and simply treat it as a standalone road or vehicle semantic segmentation problem. 
% [?] projects the frontal view to bird's-eye view image via ResNet and refines semantic masks of the road and vehicle by two discriminators, respectively. They apply a shared deep network to extract features of road and vehicles and then adopt two separate discriminators for these two types of objects. 

% [We propose a method to exploit the correlation between the view before and after transformation]
To address these concerns, we derive a novel framework to estimate the road layout and vehicle occupancies from the top-view given a single monocular front-view image.
To handle the large discrepancy between views, we present a \wx{Front-to-Top View Projection (FTVP) module} in our network, which is composed of two sub-modules: \textit{Cycled View Projection} (CVP) module bridges the view features in their respective domains and \textit{Cross-View Transformer} (CVT) correlates the views, as shown in Fig.~\ref{fig:teaser}. Specifically, the CVP projects views using a multi-layer perceptron (MLP), which overtakes the standard information flow passing through convolutional layers, and involves the constraint of cycle consistency to retain the features relevant for view projection. { In other words, transforming front-views to top-views requires a global spatial transformation over the visual features. However, standard CNN layers only allow local computation over feature maps, and it takes several layers to obtain a sufficiently large receptive field. On the other hand, fully connected layers can better facilitate the \wx{front-to-top view projection}.} Then CVT explicitly correlates the features of the views before and after projection obtained from CVP, which can significantly enhance the features after view projection. We involve a feature selection scheme in CVT, which leverages the associations of both views to extract the most relevant information.
% the cross-view transformer attempts to explicitly correlate the features of the views before and after projection, which involves a non-local attention scheme and is enabling to overtake the standard information flow through convolutional layers. 
% In the generator of our framework, we propose a cross-view transformer that models the patch-level view correlation via a transformer-based module and thus strengthens the extracted features for our task. To accomplish this, we first introduce a cycle structure to discover the critical information of the original features for view projection. Then, a feature selection scheme is incorporated in the transformer structure, which leverages the relevance between both views to select and montage a novel feature to enhance the transformed features. 
% Lastly, the enhanced features are utilized to synthesize the semantic segmentation masks. 

\wx{However, at the core of our network model, the low-resolution encoded top-view features may result in the spatial deviation of the predicted object location, because of the information loss of heavily compressed features. 
% especially for the most vehicle categories if the final occupancy maps are directly decoded from the deepest top-view features with low resolution only by naive upsampling and convolution.
To alleviate this issue, low-level visual features need to be exploited to refine the upsampled features. To bridge the low-level visual features from the front-view and the upsampled features, 
we propose applying multi-scale FTVP modules to project and propagate the low-level features. Concretely, the front-view features from the downsampling stream are first projected and transformed to top-view ones via the FTVP modules. Then all the projected features are concatenated with the corresponding top-view features of the upsampling stream. 
% As a consequence, the top-view occupancy maps will be more accurate than before owing to the aggregation of the two features. 
Moreover, to further enhance the representation of top-view features from different scales, deep supervision is employed to supervise the segmentation heads.}
% Furthermore, to exploit the spatial relationship between vehicles and roads, 
% In addition, for predicting the vehicle occupancies, we leverage the context of vehicles (i.e. road) to further improve the quality of the synthetic semantic masks of vehicles. To do so, 
% we present a context-aware discriminator that evaluates not only the estimated masks of vehicles but also their correlation. 

In comprehensive experiments, we demonstrate that our \wx{front-to-top view projection} module can effectively elevate the performance of road layout and vehicle occupancy estimation. For both tasks, we compare our model against the state-of-the-art methods on public benchmarks and demonstrate that our model is superior to all the other methods. 
It is worth noting that, for estimating vehicle occupancies, our model achieves a significant advantage over the competing methods by at least $34.8\%$ in the \textit{KITTI 3D Object} dataset and by at least $46.4\%$ in the \textit{Argoverse} dataset. 
Furthermore, we also validate our framework by predicting semantic segmentation results with more than seven classes in both the \textit{Argoverse} and \textit{NuScenes} datasets. We show that our method surpasses the latest off-the-shelf methods, which reflects the generality and rationality of our proposed model. Last but not least, we show that our framework is able to process $1024\times 1024$ images in $25$ FPS using a single Titan XP GPU, and it is applicable for real-time reconstruction of a panorama HD map. 

Overall, the contributions of our paper are:
\begin{itemize}
    \item We propose a novel framework that reconstructs a local map formed by a top-view road scene layout and vehicle occupancy using only a single monocular front-view image.
    % In specific, we introduce a novel cross-view transformation module that is consisted of a cycled view projecting structure and a cross-view transformer, which is enabling to correlate the views before and after projection in order to attentively enhance the features for large gap view projection.
    We propose a \wx{front-to-top view projection} module, which leverages the cycle consistency between views and their correlation to strengthen the view transformation.
    % \item We propose a novel generator network, \textit{Cross-view Transformer}, which leverages a transformer-based module to explicitly model the view correlation for the task of large gap view projection.
    % \item In the cycled view projecting structure, a cycled self-supervision scheme is involved to maintain the most relevant features for view projection. In the cross-view transformer, a feature selection scheme is incorporated to adaptively select the key features to further strengthen the transformed features.
    \wx{\item We also utilize multi-scale FTVP modules that project and propagate front-view features of scales to refine the representation of top-view features, which provides richer cues for precise estimation of object location.}
    % \item We also propose a context-aware discriminator that considers the spatial relationship between vehicles and roads in the task of estimating vehicle occupancies.
    \item On public benchmarks, we demonstrate that our model achieves the state-of-the-art performance for the tasks of \wx{road layout, vehicle occupancy, and multi-class semantic estimation.}
    % and vehicle occupancy estimation. 
    % Especially for the latter task, our model gains significant improvements by at least $28.5\%$ in \textit{KITTI 3D Object} and by at least $48.8\%$ in \textit{Argoverse}.
\end{itemize}

Note that a preliminary version of this work was presented as \cite{yang2021projecting}. This submission extends \cite{yang2021projecting} the methodology and experiment in the following aspects. First, we renovate our model by adjusting the structure of the front-to-top view projection module in which we rectify the projected features by utilizing the raw front-view features for supplementing visual information. This improves the expressive ability of the final top-view features and enhances the robustness of the module structure (Sec.~\ref{sec:First}).
Second, our previous model \cite{yang2021projecting} tends to cause the spatial deviation in vehicle occupancy prediction and road layout estimation, due to upsampling the low-resolution features to higher-resolution ones. To address this concern, we re-design the framework by directly projecting multi-scale features from the downsampling stream via the multi-scale FTVP modules to strengthen the upsampling process using the skip connection (Sec.~\ref{sec:Second}). 
\lqq{With the above improvements, our proposed model performs better than preliminary version on all datasets. Especially on the \textit{KITTI 3D Object} dataset, our method achieves 4.9\% and 17.5\% improvements in terms of mIOU and mAP.}
% deploying multiple the modules on varied scales. In specific, the final occupancy maps will produce certain deviation in position due to simple interpolation and convolution. 
% \wx{Specifically, we apply several the cross-view transformation modules in the top-view features of the upsampling stream to fix positioning information. }
Third, we extend our model to address multi-class segmentation problems with more than seven classes, which shows our model is able to handle scene parsing for diverse objects not limited to vehicles and road (Sec.~\ref{sec:Third}).
% (extend to more classes)
\lqq{We visualize the comparison results on multi-class semantic estimation in Fig.~\ref{fig:fig_nu} to embody the advantage of our method over others.} 
Finally, we evaluate and discuss the effectiveness of our proposed framework compared with our preliminary version and conduct additional experiments against the state-of-the-art results by estimating the multi-class segmentation maps in the \textit{Argoverse} and \textit{NuScenes} datasets (Sec.~\ref{sec:Third} and \ref{sec:Fourth}). 
Furthermore, we demonstrate that our proposed network is lightweight, and it can achieve real-time performance on a single Titan XP GPU.


\wx{In the remainder of this paper, we first review the related work in Sec.~\ref{sec:related_works}. We will then describe the overall framework and introduce the front-to-top view projection module in Sec.~\ref{sec:Method}. In Sec.~\ref{sec:Results}, the experimental results are demonstrated and discussed. Finally, we summarize our work in Sec.~\ref{sec:Conclusion}.}