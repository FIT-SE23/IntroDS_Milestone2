{
    "2204.13511": {
        "title": "RobBERTje: a Distilled Dutch BERT Model",
        "authors": [
            "Pieter Delobelle",
            "Thomas Winters",
            "Bettina Berendt"
        ],
        "submission_date": "2022",
        "SemanticScholarId": "98e391f1905043deec2ab2123f06ba16ae72dbb1"
    },
    "2203.07362": {
        "title": "CoNTACT: A Dutch COVID-19 Adapted BERT for Vaccine Hesitancy and Argumentation Detection",
        "authors": [
            "Jens Lemmens",
            "Jens Van Nooten",
            "Tim Kreutz",
            "Walter Daelemans"
        ],
        "submission_date": "2022",
        "SemanticScholarId": "26d2f194f07722c4bb24cea68e71665d0379a687"
    },
    "2201.06642": {
        "title": "Towards a Cleaner Document-Oriented Multilingual Crawled Corpus",
        "authors": [
            "Julien Abadji",
            "Pedro Ortiz Suarez",
            "Laurent Romary",
            "Benoît Sagot"
        ],
        "submission_date": "2022",
        "SemanticScholarId": "645a317c9305207e95d03b5756a65e7e850f32d5"
    },
    "2110.08534": {
        "title": "Lifelong Pretraining: Continually Adapting Language Models to Emerging Corpora",
        "authors": [
            "Xisen Jin",
            "Dejiao Zhang",
            "Henghui Zhu",
            "Wei Xiao",
            "Shang-Wen Li",
            "Xiaokai Wei",
            "Andrew O. Arnold",
            "Xiang Ren"
        ],
        "submission_date": "2021",
        "SemanticScholarId": "ed8931af08ce757a92a01ed43a0619522e10e8ff"
    },
    "2106.13474": {
        "title": "Adapt-and-Distill: Developing Small, Fast and Effective Pretrained Language Models for Domains",
        "authors": [
            "Yunzhi Yao",
            "Shaohan Huang",
            "Wenhui Wang",
            "Li Dong",
            "Furu Wei"
        ],
        "submission_date": "2021",
        "SemanticScholarId": "cf5e670a79847d9be0eb185fb372d99d30d4d98f"
    },
    "2104.09947": {
        "title": "Measuring Shifts in Attitudes Towards COVID-19 Measures in Belgium Using Multilingual BERT",
        "authors": [
            "Kristen Scott",
            "Pieter Delobelle",
            "Bettina Berendt"
        ],
        "submission_date": "2021",
        "SemanticScholarId": "38d391927dc9d27913116fd7cf57f4b24fb727ed"
    },
    "2104.08116": {
        "title": "Temporal Adaptation of BERT and Performance on Downstream Document Classification: Insights from Social Media",
        "authors": [
            "Paul Röttger",
            "J. Pierrehumbert"
        ],
        "submission_date": "2021",
        "SemanticScholarId": "1fa819dd252a8feb5350d3cacac3e019e3b8e5c7"
    },
    "2005.14165": {
        "title": "Language Models are Few-Shot Learners",
        "authors": [
            "Tom B. Brown",
            "Benjamin Mann",
            "Nick Ryder",
            "Melanie Subbiah",
            "J. Kaplan",
            "Prafulla Dhariwal",
            "Arvind Neelakantan",
            "Pranav Shyam",
            "Girish Sastry",
            "Amanda Askell",
            "Sandhini Agarwal",
            "Ariel Herbert-Voss",
            "Gretchen Krueger",
            "T. Henighan",
            "R. Child",
            "A. Ramesh",
            "Daniel M. Ziegler",
            "Jeff Wu",
            "Clemens Winter",
            "Christopher Hesse",
            "Mark Chen",
            "Eric Sigler",
            "Ma-teusz Litwin",
            "Scott Gray",
            "Benjamin Chess",
            "Jack Clark",
            "Christopher Berner",
            "Sam McCandlish",
            "Alec Radford",
            "I. Sutskever",
            "Dario Amodei"
        ],
        "submission_date": "2020",
        "SemanticScholarId": "90abbc2cf38462b954ae1b772fac9532e2ccd8b0"
    },
    "2004.10964": {
        "title": "Don’t Stop Pretraining: Adapt Language Models to Domains and Tasks",
        "authors": [
            "Suchin Gururangan",
            "Ana Marasović",
            "Swabha Swayamdipta",
            "Kyle Lo",
            "Iz Beltagy",
            "Doug Downey",
            "Noah A. Smith"
        ],
        "submission_date": "2020",
        "SemanticScholarId": "e816f788767eec6a8ef0ea9eddd0e902435d4271"
    },
    "2002.12327": {
        "title": "A Primer in BERTology: What We Know About How BERT Works",
        "authors": [
            "Anna Rogers",
            "Olga Kovaleva",
            "Anna Rumshisky"
        ],
        "submission_date": "2020",
        "SemanticScholarId": "bd20069f5cac3e63083ecf6479abc1799db33ce0"
    },
    "2001.06286": {
        "title": "RobBERT: a Dutch RoBERTa-based Language Model",
        "authors": [
            "Pieter Delobelle",
            "Thomas Winters",
            "Bettina Berendt"
        ],
        "submission_date": "2020",
        "SemanticScholarId": "634e8ee7e86f253c4b6c722a3bb7c32b7aa3892b"
    },
    "1912.09582": {
        "title": "BERTje: A Dutch BERT Model",
        "authors": [
            "Wietse de Vries",
            "Andreas van Cranenburgh",
            "Arianna Bisazza",
            "Tommaso Caselli",
            "Gertjan van Noord",
            "M. Nissim"
        ],
        "submission_date": "2019",
        "SemanticScholarId": "a4d5e425cac0bf84c86c0c9f720b6339d6288ffa"
    },
    "1911.03894": {
        "title": "CamemBERT: a Tasty French Language Model",
        "authors": [
            "Louis Martin",
            "Benjamin Muller",
            "Pedro Ortiz Suarez",
            "Yoann Dupont",
            "L. Romary",
            "Eric Villemonte de la Clergerie",
            "Djamé Seddah",
            "Benoît Sagot"
        ],
        "submission_date": "2019",
        "SemanticScholarId": "b61c6405f4de381758e8b52a20313554d68a9d85"
    },
    "1911.00202": {
        "title": "Forget Me Not: Reducing Catastrophic Forgetting for Domain Adaptation in Reading Comprehension",
        "authors": [
            "Ying Xu",
            "Xu Zhong",
            "Antonio Jimeno-Yepes",
            "Jey Han Lau"
        ],
        "submission_date": "2019",
        "SemanticScholarId": "30889f67426dfbeba973e100666fb3bac2a9644b"
    },
    "1910.00896": {
        "title": "The merits of Universal Language Model Fine-tuning for Small Datasets - a case with Dutch book reviews",
        "authors": [
            "Benjamin van der Burgh",
            "S. Verberne"
        ],
        "submission_date": "2019",
        "SemanticScholarId": "c2eab93dfd00e23851dcab17a4be699fbd935f18"
    },
    "1909.03341": {
        "title": "Neural Machine Translation with Byte-Level Subwords",
        "authors": [
            "Changhan Wang",
            "Kyunghyun Cho",
            "Jiatao Gu"
        ],
        "submission_date": "2019",
        "SemanticScholarId": "ae677b0441bfaea0e0c78acfa8758fff353ab715"
    },
    "1907.11692": {
        "title": "RoBERTa: A Robustly Optimized BERT Pretraining Approach",
        "authors": [
            "Yinhan Liu",
            "Myle Ott",
            "Naman Goyal",
            "Jingfei Du",
            "Mandar Joshi",
            "Danqi Chen",
            "Omer Levy",
            "M. Lewis",
            "Luke Zettlemoyer",
            "Veselin Stoyanov"
        ],
        "submission_date": "2019",
        "SemanticScholarId": "077f8329a7b6fa3b7c877a57b81eb6c18b5f87de"
    },
    "1508.07909": {
        "title": "Neural Machine Translation of Rare Words with Subword Units",
        "authors": [
            "Rico Sennrich",
            "B. Haddow",
            "Alexandra Birch"
        ],
        "submission_date": "2015",
        "SemanticScholarId": "1518039b5001f1836565215eb047526b3ac7f462"
    },
    "1412.6980": {
        "title": "Adam: A Method for Stochastic Optimization",
        "authors": [
            "Diederik P. Kingma",
            "Jimmy Ba"
        ],
        "submission_date": "2014",
        "SemanticScholarId": "a6cb366736791bcccc5c8639de5a8f9636bf87e8"
    },
    "1810.04805": {
        "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
        "authors": [
            "Jacob Devlin",
            "Ming-Wei Chang",
            "Kenton Lee",
            "Kristina Toutanova"
        ],
        "submission_date": "2019",
        "SemanticScholarId": "df2b0e26d0599ce3e70df8a9da02e51594e0e992"
    }
}