\section{Background}

\subsection{Collaborative Filtering}

\textit{Collaborative Filtering} (CF) aims at recommending items to users from available user-item rating data. Direct ratings are submitted by users explicitly, while indirect ratings can be inferred from their interactions with the system. User ratings can be further classified according to its domain -- unary (positive only), binary, ordinal or numerical \citep{CF-ExploitTimeCF-VinagreJoaoJorge-2015}. The set of ratings forms a sparse matrix, often referred to as U-I rating interaction matrix, with its sparsity being defined as the prevalence of unknown ratings.





\begin{definition}
Considering a set of $n$ users $\mathcal{U}=\{u_1,... , u_n\}$, a collection of $m$ items $\mathcal{I}=\{i_1,... ,i_m\}$, \textbf{user-item rating data} $\mathcal{D}$ is a set of preferences $r_{ui}$, each corresponding to a unary, binary, ordinal or numerical rating placed by user $u$ on item $i$. 

Given $\mathcal{D}$, \textbf{collaborative filtering} (CF) aims at estimating unknown preferences, $\hat{r}_{ui}$. Collaborative filtering can be reduced to specific users or items of interest, referred as active users or active items.
\end{definition}

CF gives rise to \textit{Predictive} and \textit{Recommendation} stances. In the Predictive stance, unknown ratings are estimated for a given user and item based on available ratings \citep{CF-EvaluateCF-Herlocker-2004}. As for the \textit{recommendation} tasks, the system usually generates and provides an ordered list of \textit{n} items, known as Top-\textit{N} recommendation list, which is a list including the most relevant/useful items for the user. This subsequent Recommendation task, often involves dissimilarity criteria among the returned items or between the known and top items \citep{CF-EvaluateCF-Herlocker-2004}.

Collaborative Filtering methods are usually divided into two main classes, the \textit{memory-based} and the \textit{model-based} \citep{Survey-CF-Su-2009}. The memory-based algorithms use the user-item system ratings directly to predict ratings for new items. %This can be done by \textit{User-based} or \textit{Item-based} recommendation. 
In this context, user-based (item-based) systems predict the rating of an item by a particular user using a simple estimator over the set of ratings of similar users (items). %referred to as neighbors, gave to that item. On the other hand, item-based approaches predict the rating based on how the active user rated similar items. 
Some challenges limit memory-based CF effectiveness when dealing with recommendation data, mainly due to the vast amounts of data and their sparse nature. Model-based approaches try to overcome the limitations of memory-based CF by learning predictive models from the available data and later use them to predict users' ratings for new items.




\subsection{Clustering}

\textit{Clustering} has been largely considered in Collaborative Filtering \citep{CF-Survey-Ekstrand-2011}. In the context of \textit{memory-based CF}, clustering has been applied to precompute groups of users (items) in user-based (item-based) approaches, promoting interpretability and testing time efficiency by assessing the similarity of a new (user) against the centroids of the precomputed clusters. Alternatively, clustering has been largely applied to \textit{model-based CF}, where the task of learning a predictive model from the complete rating data space is reduced to each cluster of users (items) produced from clustering, facilitating the learning task in accordance with group-wise preferences. 

%Irrespective of the approach, whether simple estimators in memory-based CF or machine learning models in model-based CF, 

Irrespective of the approach, clustering relies on similarity functions to estimate proximity between users (items). Popular examples robust to missing rates include cosine vector similarity and Pearson correlation \citep{CF-Survey-Ekstrand-2011}. 

Despite the relevance of the clustering-based CF, clustering mislay valuable information because they can only be applied either to the user or item dimension separately, disregarding the rich structure of the other dimension and thus preventing the identification of shared local preferences.

\subsection{Biclustering}

In real-world applications with large item collections, shared preferences between a group of users (items) are only correlated on a subset of items (users). In this context, computing similarity against the whole dimension neglects the diversity of user preferences along item classes and therefore misses groups of users with correlated preferences on specific subsets of items. Opposed to one-way clustering, biclustering is thus a technique able to cluster users and items simultaneously, producing subspaces (biclusters) with coherent preferences as illustrated in Fig.\ref{fig:biclusteringvsclustering}. 

\begin{figure*}[h]
    \centering
    \includegraphics[width=0.95\linewidth]{biclusteringvsclustering.pdf}
    \caption{\small \mbox{Clustering (a-b) vs. biclustering (c) solutions over ordinal user-item ratings.}}
    \label{fig:biclusteringvsclustering}
\end{figure*}

%\citep{BC-statisticalsig-Rui-2018}

\begin{definition}
Given user-item rating data $\mathcal{D}$, a \textbf{bicluster} $B=(U,I)$ is a subspace given by a subset of users, $U\subseteq \mathcal{U}$, and a subset of items, $I\subseteq \mathcal{I}$. The \textbf{biclustering} task aims to find a set of biclusters $\mathcal{B}=\{B_1,..,B_q\}$, such that each bicluster $B_k$ satisfies specific criteria of homogeneity \citep{mypr}, dissimilarity and statistical significance \citep{Henriques2017}. 
\end{definition}

Biclusters capture local rating patterns corresponding to correlated preferences for a subset of users and items. The biclustering \textbf{homogeneity} criteria determines the \textit{structure}, \textit{coherence} and \textit{quality} of a biclustering solution. The \textbf{structure} is described by the number, size, shape and position of biclusters. Flexible structures of biclusters are characterized by an arbitrary number of (possibly overlapping) biclusters.  The \textbf{coherence} of a bicluster is given by the observed correlation of ratings (\textit{coherence assumption}) and, in the presence of numerical preferences, the allowed deviation from expectations (\textit{coherence strength}). Finally, the \textbf{quality} of a bicluster is defined by the type and amount of tolerated noise. %Fig.\ref{figSolution} illustrates a flexible structure of biclusters with different coherencies and perfect quality (no noise allowed).  
\newpage 

\begin{definition}{}\label{def:bic_coherence}
Given a real-valued dataset, let the elements in a bicluster $r_{ui} \in (U,I)$ have \textbf{coherence} across observations given by $r_{ui}=k_i+\gamma_u + \eta_{ui}$, where $k_i$ is the expected rating for item $i$, $\gamma_u$ is the adjustment for user $u$, and $\eta_{ui}$ is the noise factor. A bicluster satisfying a specific coherence strength, $\delta \in \mathbb{R}^+$, has values described by $r_{ui}=k_i+\gamma_u + \eta_{ui}$ and $\eta_{ui} \in [-\delta/2,\delta/2]$.
\end{definition}

\begin{figure}[b]
\vskip -0.2cm
    \centering
    \includegraphics[width=0.77\linewidth]{biclustertypes.pdf}
    \caption{\footnotesize Illustrative forms of coherence \citep{BC-Survey-Sara-2004}: a) constant value, b) constant-values on rows, c) constant-values on columns, d) coherent values (additive model), e) coherent values (multiplicative model), f) overall coherent evolution, g) coherent evolution on the rows, h) coherent evolution on the columns.}
    \label{fig:biclusttypes}
\vskip -0.2cm
\end{figure}

\theoremstyle{definition}
\begin{definition}{}
Given a bicluster B=(U,I) with coherence in accordance with \autoref{def:bic_coherence}, the $\gamma_u$ factors define the coherence assumption: \textbf{constant} pattern on rows when $\gamma_u=0$ and \textbf{additive} pattern on rows when $\gamma_u \neq 0$. %and \textbf{multiplicative} pattern on rows if $a_{ij}$ is better described by $k_j\gamma_i + \eta_{ij}$.

The bicluster \textbf{pattern} $\varphi_B$ is the set of expected values in the absence of adjustments and noise, $\varphi_B=\{k_i\mid i \in I\}$.
\label{patterndef}
\end{definition}



Commonly pursued forms of coherence are constant values across the subspace, users or items. When considering forms of numerical and ordinal feedback, biclusters can further accommodate additive, multiplicative or order-preserving factors. Fig.\ref{fig:biclusttypes} illustrates these different types of biclusters.




A bicluster is \textbf{statistically significant} if its probability to occur deviates from expectations (i.e. is unexpectedly low against a null data model). 

Let $\mathcal{B}$ be the set of biclusters that satisfy a given homogeneity and statistical significance criteria, $(U,I)\in\mathcal{B}$ is a \textbf{maximal bicluster} iff there is no other bicluster $(U',I')$ such that $U\subseteq U' \wedge I\subseteq I'$ satisfying the given criteria. Although an \emph{optimal biclustering solution} is one containing all maximal biclusters, the biclustering formulation can further include \textbf{dissimilarity} criteria to reduce the size of outputs and minimize redundancies. 

Even when considering unary rating data, biclustering is a well-established NP-hard task. As a result, many biclustering algorithms use heuristic mechanisms (producing sub-optimal solutions) \citep{BC-Survey-Sara-2004}, and place restrictions on the allowed homogeneity criteria.

\subsection{Contrasting biclustering with coclustering and dimensionality reduction}

The biclustering task is not equivalent to the \textbf{coclustering} task, also termed \textit{block clustering}. Coclustering is a restrictive form of biclustering requiring that all users and items belong to a subspace (exhaustive condition) and to a single subspace only (exclusive condition), producing a checkboard structure as a result. Although coclustering limits the inherent flexibility of the biclustering task \citep{kluger2003spectral}, it guarantees that all user-item pairs are included in a single subspace.

\begin{definition}
Given user-item rating data $\mathcal{D}$, \textbf{coclustering} aims to partition users and items, ($\mathcal{U}=\{U_1,..,U_r\},\mathcal{I}$=$\{I_1,..,I_s\}$), so that subspaces resulting from the intersecting partitions, $\mathcal{U}\times\mathcal{I}$ optimize some homogeneity criteria. 
\end{definition}

Dimensionality reduction of rating data became popularized after playing a significant role in the Netflix Prize competition \citep{CF-Dataset-Netflix}. Dimensionality reduction procedures are now pervasive in CF \citep{CF-Survey-Ekstrand-2011} as they offer a means of diminishing the sparsity of user-item rating data by placing data projections into denser spaces, and tackling generalization difficulties of model-based CF approaches caused by the arbitrarily-high dimensionality of item collections. %Although model-based CF approaches generally address efficiency bottlenecks of MCF approaches, the training phase can be costly. 
Note, however, that most common forms of dimensionality reduction in CF are based on matrix factorization approaches, whose behavior resembles a restricted variant of coclustering approaches. In this context, they do not yield the inherent flexibility of biclustering stances in reducing the original space into an arbitrarily-high number of statistically significant subspaces capturing coherent preferences. %with the additional restriction of  singular This comes with the cost of losing potentially useful information, possibly affecting the accuracy of recommendations. 



