% This is samplepaper.tex, a sample chapter demonstrating the
% LLNCS macro package for Springer Computer Science proceedings;
% Version 2.21 of 2022/01/12
%
\documentclass[runningheads]{llncs}
%
% Newly added usepackage
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsthm,amsmath}
\usepackage{mathrsfs}
\usepackage{indentfirst}
\usepackage{multirow}
\usepackage{algorithmicx,algorithm}
\let\algorithm\relax  
\let\endalgorithm\relax  
\usepackage[linesnumbered,ruled,vlined]{algorithm2e}%[ruled,vlined]{  
\usepackage{algpseudocode}  
\renewcommand{\algorithmicrequire}{\textbf{Input:}}  % Use Input in the format of Algorithm  
\renewcommand{\algorithmicensure}{\textbf{Output:}} % Use Output in the format of Algorithm   

\usepackage{multirow}
\usepackage{times}
\usepackage{soul}
\usepackage{url}
\usepackage[hidelinks]{hyperref}
\usepackage[utf8]{inputenc}
\usepackage[small]{caption}





\usepackage[T1]{fontenc}
% T1 fonts will be used to generate the final print and online PDFs,
% so please use T1 fonts in your manuscript whenever possible.
% Other font encondings may result in incorrect characters.
%
\usepackage{graphicx}
% Used for displaying a sample figure. If possible, figure files should
% be included in EPS format.
%
% If you use the hyperref package, please uncomment the following two lines
% to display URLs in blue roman font according to Springer's eBook style:
%\usepackage{color}
%\renewcommand\UrlFont{\color{blue}\rmfamily}
%
\begin{document}
%
\title{Resisting Graph Adversarial Attack via Cooperative Homophilous Augmentation}
\author{Zhihao Zhu\inst{1} \and
Chenwang Wu\inst{1} \and
Min Zhou\inst{2} \and
Hao Liao\inst{3} \and
DefuLian{\Letter} \inst{1} \and
Enhong Chen\inst{1}}
%
%\authorrunning{}
% First names are abbreviated in the running head.
% If there are more than two authors, 'et al.' is used.
%
\institute{University of Science and Technology of China
\\\email{$\{$zzh98,wcw1996$\}$@mail.ustc.edu.cn
\\$\{$liandefu,cheneh$\}$@ustc.edu.cn }
\and Huawei Technologies co. ltd
\email{zhoum1900@163.com}
\and Shenzhen University
\email{haoliao@szu.edu.cn}
}
\maketitle

\section{Proofs}
\subsection{Detailed Analysis of Theorem 1}
Given $G = (\boldsymbol{A}, \boldsymbol{X})$ and $Y_L$ as input, a two-layer SGC with $\theta = \boldsymbol{W}$ implements $f_\theta(G)$ as
\begin{align}
f_\theta(G) = softmax(\boldsymbol{\hat{A}^2XW}).
\end{align}
Follow~\cite{zhu2021relationship}'s setting, we assume that $G$ is a d-regular graph which means that each node of G has $d$ connections
with other nodes.
For each node of $G$, proportion $h$ of their neighbors belong to the same class, 
while proportion $\frac{1 - h}{C - 1}$ of them belong to any other class uniformly.
Node $v$'s features $\boldsymbol{x_v} = p \cdot onehot(\boldsymbol{y_v}) + \frac{1 - p}{ C }$, where $\boldsymbol{y_v}$ means the node's label.

According to ~\cite{zhu2021relationship}'s work, the optimal weight matrix $\boldsymbol{W^*}$ can be expressed as follows:
%$\frac{(d+1)^{2}(C-1)^{2}}{p(-d hC+d-(C-1))^{2}C}$ 
%\begin{align}
%W_*=\cdot\left[\begin{array}{ccccc}C-1 & -1 & \cdots & -1 \\ -1 & C-1 & \cdots & -1 \\ \vdots & \vdots & \ddots & \vdots \\ -1 & -1 & \cdots & C-1\end{array}\right] \frac{1}{C}\right]$
%\end{align}
\begin{align}
\boldsymbol{W^*} = r \cdot 
\left[\begin{array}{ccccc}C-1 & -1 & \cdots & -1 \\ 
-1 & C-1 & \cdots & -1 \\ 
\vdots & \vdots & \ddots & \vdots \\
 -1 & -1 & \cdots & C-1\end{array} + \frac{1}{C}\right],
\end{align}
where $r$ is a constant, and the right end of $\boldsymbol{W^*}$ is a double random matrix.
The features matrix $\boldsymbol{X}$ defined before is a row random matrix. 
We call $\boldsymbol{XW^*}$ as $\boldsymbol{X'}$ which is a row random matrix apparently.
After a simple calculation, We define that  
\begin{align}
\boldsymbol{X}_{iy_i} = x_0,  \nonumber\\
{\forall} j \neq y_i, \boldsymbol{X}_{ij} = x_1.
\end{align}
And we have:
\begin{align*}
x_0 + (C-1) \cdot x_1 = 1.
\end{align*}

We use the change of CM loss of the model to analyze the influence of injecting nodes to the graph.
The CM loss of node $v$ is defined as:
\begin{align}
\boldsymbol{Z}=\boldsymbol{\hat{A}^2XW}, loss_{v} = \boldsymbol{Z}_{vy_{v}} - \max \limits_{j!=y_{v}} \boldsymbol{Z}_{vj}.
\end{align}
Assume that the proportion of node $v$'s egdes which is connected to class $y_v$ before poisoned is $h_0$(including self-loop of node $v$), and the proportion of other classed is $h_1$.
\begin{align*}
h_0 + (C-1) \cdot h_1 = 1.
\end{align*}
After we inject $l$ nodes to the graph, the proportion of node $v$'s egdes which is connected to class $y_v$ is $r_0$, and the proportion of other classed is $r_1$.
For convenience, we separate the proportions of fake egdes from $r_0$ and $r_1$. 
The proportion of injected egdes is writed as $r_2$. 
\begin{align*}
r_0 + (C-1) \cdot r_1 + r_2 = 1.
\end{align*}

We call the CM loss of node $v$ on clean graph as $L_0$. 
After we generate $l$ nodes to inject homogeneous edges to the graph, the CM loss changes to $L_1$.
Correspondingly, the CM loss is called $L_2$ after we generate $l$ nodes to inject heterogeneous edges to the graph.
We can calculate that, for $\forall i \in $real nodes, we have
\begin{align}
\left[\hat{\boldsymbol{A}} \boldsymbol{X}^{\prime}\right]_{i y_{i}}=h_{0} x_{0}+\left(1-h_{0}\right) x_{1}=s_{0} ,\nonumber\\
\left[\hat{\boldsymbol{A}} \boldsymbol{X}^{\prime}\right]_{i j}=h_{1} x_{0}+\left(1-h_{1}\right) x_{1}=s_{1}, \forall j \neq y_{i},\nonumber\\
\left[\hat{\boldsymbol{A}}^{2} \boldsymbol{X}^{\prime}\right]_{i y_{i}}=h_{0} s_{0}+\left(1-h_{0}\right) s_{1},\nonumber\\
\left[\hat{\boldsymbol{A}}^{2} \boldsymbol{X}^{\prime}\right]_{i j}=h_{1} s_{0}+\left(1-h_{1}\right) s_{1}, \forall j \neq y_{i},\nonumber\\
L_0=\left(h_{0}-h_{1}\right) s_{0}-\left(h_{0}-h_{1}\right) s_{1}.
\end{align}
After we inject homogeneous edges, for $\forall i \in $real nodes and $i \neq v$, we have 
\begin{align}
\left[\hat{\boldsymbol{A}} \boldsymbol{X}^{\prime}\right]_{i y_{i}}=h_{0} x_{0}+\left(1-h_{0}\right) x_{1}=s_{0},\nonumber\\ 
\left[\hat{\boldsymbol{A}} \boldsymbol{X}^{\prime}\right]_{i j}=h_{1} x_{0}+\left(1-h_{1}\right) x_{1}=s_{1}, \forall j \neq y_{i}.
\end{align}
For node $v$, we have
\begin{align}
\left[\hat{\boldsymbol{A}} \boldsymbol{X}^{\prime}\right]_{v \mathrm{y}_{\mathrm{v}}}=r_{0} x_{0}+\left(1-r_{0}-r_{2}\right) x_{1}+r_{2} x_{0}=t_{0},\nonumber\\
\left[\hat{\boldsymbol{A}} \boldsymbol{X}^{\prime}\right]_{i j}=r_{1} x_{0}+\left(1-r_{1}-r_{2}\right) x_{1}+r_{2} x_{1}=t_{1}, \forall j \neq \mathrm{y}_{\mathrm{v}}.
\end{align}
For $\forall k \in$ fake nodes, we have 
\begin{align}
\left[\hat{\boldsymbol{A}} \boldsymbol{X}^{\prime}\right]_{k y_{k}}=f_{0}, \nonumber\\
\left[\hat{\boldsymbol{A}} \boldsymbol{X}^{\prime}\right]_{k j}=f_{1}, \forall j \neq y_{k}.
\end{align}
Then we can figure out, for $\forall f \neq y_{v}$, we have:
\begin{align}
\left[\hat{\boldsymbol{A}}^{2} \boldsymbol{X}^{\prime}\right]_{v y_v}=r_{0} s_{0}+\left(1-r_{0}-r_{2}\right) s_{1}+r_{2} f_{0},\nonumber\\
\left[\hat{\boldsymbol{A}}^{2} \boldsymbol{X}^{\prime}\right]_{v f}=r_{1} s_{0}+\left(1-r_{1}-r_{2}\right) s_{1}+r_{2} f_{1}.
\end{align}
So $L_1$ can be calculated:
\begin{align}
\operatorname{L}_{1}=&\left(r_{0}-r_{1}\right) s_{0}-\left(r_{0}-r_{1}\right) s_{1}+r_{2}\left(f_{0}-f_{1}\right)\nonumber\\
=&\left(r_{0}-r_{1}\right)\left(s_{0}-s_{1}\right)+r_{2}\left(f_{0}-f_{1}\right).
\end{align}
Similarly, after injecting heterogeneous edges, we have:
\begin{align}
\left[\hat{\boldsymbol{A}}^{2} \boldsymbol{X}^{\prime}\right]_{v t}=r_{0} s_{0}+\left(1-r_{0}-r_{2}\right) s_{1}+r_{2} f_{1},\nonumber\\
\left[\hat{\boldsymbol{A}}^{2} \boldsymbol{X}^{\prime}\right]_{v f}=r_{1} s_{0}+\left(1-r_{1}-r_{2}\right) s_{1}+r_{2} f_{0},\nonumber\\
\operatorname{L}_{2}=\left(r_{0}-r_{1}\right)\left(s_{0}-s_{1}\right)-r_{2}\left(f_{0}-f_{1}\right).
\end{align}
where $f$ is the category that the attacker wants the node to approach.

Assume that the injected node is guaranteed to have the same connecting mode as the real node.
It means that proportion $h$ of their neighbors belong to the same class, 
while proportion $\frac{1 - h}{C - 1}$ of them belong to any other class uniformly.
Then $f_1 = s_1, f_0 = s_0$
\begin{align}
L_0 = (h_0 - h_1)\cdot(s_0 - s_1),\nonumber\\
L_1 = (r_0 - r_1 + r_2)\cdot(s_0 - s_1),\nonumber\\
L_2 = (r_0 - r_1 - r_2)\cdot(s_0 - s_1).
\end{align}
Then Theorem 1 is proved.
\begin{theorem}
Consider target attack and direct attack which means that the inject nodes are directly connected to the target node $v$.
Then we have:
\begin{align}
\frac{L_1-L_0}{L_0-L_2} = \frac{(r_0 - r_1 + r_2) - (h_0 - h_1)}{(h_0 - h_1) - (r_0 - r_1 - r_2)}
\end{align}
\end{theorem}

\subsection{Detailed Analysis of Theorem 2}
Referring to~\cite{mohri2018foundations}, the multi-class problem can be decomposed into several two-class problems. Therefore, we reduce the proof to a binary classification problem.
%Suppose there are two types of nodes in the graph, namely category $t$ and category $f$.
Expand $h_0$ and $h_1$ into fractions:
\begin{align}
h_0 = \frac{a}{d}, h_1 = \frac{b}{d},\nonumber\\
d = b + a, a>b.
\end{align}
Assume that we inject $l$ edges to each target node, then we have:
\begin{align}
%r_0 = \frac{a}{d+l}
r_0 = \frac{a}{d+l},\nonumber\\
r_1 = \frac{b}{d+l},\nonumber\\
r_2 = \frac{k}{d+l}.
\end{align}
It means that for each node of target nodes, proportion $r_0$ of
their neighbors belong to the same class.
For injection nodes, in order to achieve stronger attack effect, this ratio will be smaller.

Bring the value of $r$ into theorem 1, we get:
\begin{align}
\frac{L_1-L_0}{L_0-L_2} = \frac{d-(a-b)}{d+(a-b)} = \frac{b}{a}.
\end{align}
Eq(16) quantifies the ratio of the change in CM loss between eliminating a homophilous edge and eliminating a heterophilous edge.

Assume that the prediction accuracy of model on unlabeled nodes is $p$. 
The prediction accuracy of different nodes is independent.
Suppose we judge that there is a heterogeneous edge between nodes $u$ and $v$ according to pseudo-labels and then we delete $e_{uv}$.
The probability that $e_{uv}$ is actually a homogeneous edge is $p_1$ 
while the probability that $e_{uv}$ is actually a heterogeneous edge is $p_2$.
The pseudo-labels of $u$ and $v$ are $\hat{y_u}$ and $\hat{y_v}$, $\hat{y_u} \neq \hat{y_v}$.
The labels of $u$ and $v$ are $y_u$ and $y_v$.
Then we have:
\begin{align}
p_{1}=&p(1-p) \cdot P(y_u=\hat{y_u} \land y_v=\hat{y_u}) + (1-p) p \cdot P(y_u=\hat{y_v} \land y_v=\hat{y_v})\nonumber\\
%+&(1-p)^{2}P(y_u = y_v\land y_u \neq \hat{y_v} \land y_v \neq \hat{y_u})\nonumber\\
<&p(1-p) \cdot r_{0} + (1-p)p \cdot r_{0} \nonumber\\
%+ (1-s)^{2} \frac{C-2}{C-1} \cdot r_{0}\nonumber\\
%<&2s(1-s)) \cdot r_{0} + (1-s)^{2} \cdot r_{0}\nonumber\\
%=&(1-s)(1+s) \cdot r_{0}\nonumber\\
%=&(1-s^{2}) \cdot \frac{a}{d+k}
=&2p(1-p)\cdot \frac{a}{d+l}.
%<&2(1-p) \cdot \frac{a}{d+l}
\end{align}
And we have
\begin{align}
p_2 = (1 - p_1) > \frac{b+l}{d+l}.
\end{align}
We write the expected penalty of deleting an edge in our algorithm as $e_1$, and the expected benefit as $e_2$.

%In general, $k>b$.
\begin{align}
\frac{e_{1}}{e_{2}} = \frac{p_{1}}{p_{2}} \cdot \frac{b}{a}
< 2p(1-p) \cdot \frac{a}{b+l} \cdot \frac{b}{a} = 2p(1-p) \frac{b}{b+l} < 2p(1-p).
%<&\frac{\left(1-s^{2}\right) a}{b+k} \cdot \frac{C b}{2 a+(C-2) b}\nonumber\\
%<&\frac{\left(1-s^{2}\right) a}{2 b} \cdot \frac{C b}{2 a}\nonumber\\
%<&\frac{\left(1-r^{2}\right) C}{4}
\end{align}
Then Theorem 2 is proved.
\begin{theorem}
The ratio of expected penalty to expected benefit for eliminating an edge in CHAGNN is related to the prediction accuracy $p$.
\begin{align}
\frac{e_1}{e_2} < 2p(1-p) < 1 
\end{align}
\end{theorem}

%The number of categories in cora dataset is 7.
%The expected benefit is always greater than the expected penalty as long as $s>0.66$.
%For pubmed dataset, $C$ equals 3. 
%CHAGNN works even if the base model is worse.
%It should be mentioned that we use the lower bound of $p_2$ instead of $p_2$.
%In our experiment we observe that CHAGNN also works even if the base model does not perform well enough.


\begin{thebibliography}{8}
\bibitem{zhu2021relationship}
Zhu J, Jin J, Loveland D, et al. On the Relationship between Heterophily and Robustness of Graph Neural Networks[J]. arXiv preprint arXiv:2106.07767, 2021.

\bibitem{mohri2018foundations}
Mohri M, Rostamizadeh A, Talwalkar A. Foundations of machine learning[M]. MIT press, 2018.
\end{thebibliography}
\end{document}