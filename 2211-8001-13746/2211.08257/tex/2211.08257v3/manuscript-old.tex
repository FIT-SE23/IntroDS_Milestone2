\documentclass[screen, anonymous, review, acmlarge]{acmart}

%%
%% \BibTeX command to typeset BibTeX logo in the docs
\AtBeginDocument{%
  \providecommand\BibTeX{{%
    \normalfont B\kern-0.5em{\scshape i\kern-0.25em b}\kern-0.8em\TeX}}}


%\copyrightyear{2023} 
%\acmYear{2023} 
%\setcopyright{rightsretained}
%\acmConference[CHI '23]{CHI Conference on Human Factors in Computing Systems}{April 23-April 28, 2023}{Hamburg, Germany}
%\acmBooktitle{CHI Conference on Human Factors in Computing Systems (CHI '23), April 23-April 28, 2022, Hamburg, Germany}
%\acmDOI{}


\setcopyright{rightsretained}
\acmJournal{IMWUT}
\acmYear{2023} \acmVolume{0} \acmNumber{0} \acmArticle{0} \acmMonth{0} \acmPrice{0} \acmDOI{XXXXXXX.XXXXXXX}


%%
%% Submission ID.
%% Use this when submitting an article to a sponsored event. You'll
%% receive a unique submission ID from the organizers
%% of the event and this ID should be used as the parameter to this command.
\acmSubmissionID{8421}

% Load basic packages
\usepackage[utf8]{inputenc}  % translates  standard and other input encodings into a ‘LATEX \usepackage{float} % Improves the interface for defining floating objects such as figures and tables.
\usepackage{subcaption} % captions for subfigures and the like

% tables
\usepackage{subcaption}
\usepackage[]{algorithm2e}
\usepackage{multirow}

%\usepackage{todonotes}


\usepackage{url}
\makeatletter
\g@addto@macro{\UrlBreaks}{\UrlOrds}
\makeatother

\usepackage{colortbl}

% TABULAR RELATED
\usepackage{multirow} %tabular cells spanning multiple rows
\usepackage{color} %colour management
\usepackage{enumitem} %better  enumerate, itemize and description

% UNIT RELATED
\usepackage{xspace}
\usepackage{xcolor, colortbl}% http://ctan.org/pkg/xcolor

% Draw dash-lines in array/tabular
\usepackage{arydshln}
% color checkmark
\usepackage{pifont}


\usepackage[final]{pdfpages}


%COMMANDS
% Optionally save space in lists (place this command after a list environment (e.g., itemize, enumerate, description)

% Interquartil Range
\newcommand{\iqr}[2]{$IQR={#1}-{#2}$}
%Mittelwert
%\newcommand{\m}[1]{$M={#1}$}
\newcommand{\m}{\textit{M=}}

%Median
%\newcommand{\md}[1]{$Mdn={#1}$}
\newcommand{\md}{\textit{Mdn=}}
% Standard Deviation
%\newcommand{\sd}[1]{$SD={#1}$}
\newcommand{\sd}{\textit{SD=}}

%\newcommand{\N}[1]{$N={#1}$}
\newcommand{\N}{\textit{N=}}

\newcommand{\ttest}{\textit{t=}}
\newcommand{\ns}{\textit{ns.}}
\newcommand{\V}[1]{$V={#1}$}
\newcommand{\F}[3]{$F({#1},{#2})={#3}$}
\newcommand{\W}{\textit{W=}}
\newcommand{\Z}{\textit{Z=}}
%\newcommand{\p}[1]{$p={#1}$}
\newcommand{\p}{\textit{p=}}


%\newcommand{\rtest}[1]{$r={#1}$}
\newcommand{\rtest}{\textit{r=}}

\newcommand{\df}{\textit{df=}}
\newcommand{\pminor}{\textit{p$<$}}
\newcommand{\kendall}{\textit{r}$_\tau$=}
\newcommand{\chisq}{$\chi^2$}
\newcommand{\effectsize}{\textit{r=}}
\newcommand{\rsq}{$\textit{R}^2$=}
\newcommand{\baseline}{\textit{baseline}}
\newcommand{\coef}{$\beta$=}

\newcommand*{\dittoclosing}{---''---}
\newcommand*{\dittoclosingTwo}{--- \raisebox{-.5ex}{''} ---}
\newcommand*{\dittostraight}{---\textquotedbl---} % available in T1 encoding

\newcommand{\rg}[2]{\textit{range=[#1, #2]}}
%\newcommand{\includevisio}[2][]{\includegraphics[clip, trim=.23cm .35cm .35cm .35cm, #1]{#2}} 
% color checkmark
\newcommand{\greencheck}{{\color{green}\ding{52}}}
\newcommand{\redxmark  }{{\color{red}  \ding{55}}}

\newcommand{\tool}{\textsc{AutoTherm}\xspace}
\newcommand{\toolData}{\textsc{AutoTherm dataset}\xspace}


\def\plaintitle{AutoTherm: A Dataset and Ablation Study for Thermal Comfort Prediction in Vehicles}
\def\plainauthor{Mark Colley, Sebastian Hartwig, Albin Zeqiri, Timo Ropinski, Enrico Rukzio}
\def\emptyauthor{}
\def\plainkeywords{Machine Learning; vehicles; state recognition; dataset.}
\def\plaingeneralterms{Documentation, Standardization}


%\usepackage{soul}
%\colorlet{soulyellow}{yellow!40}
%\sethlcolor{soulyellow}


%\newcommand{\ctext}[1]{%
%  \begingroup
%  \hl{#1}%
%  \endgroup
%}



%%
%% end of the preamble, start of the body of the document source.
\begin{document}

%%
%% The "title" command has an optional parameter,
%% allowing the author to define a "short title" to be used in page headers.
\title{\plaintitle}


\author{Mark Colley}
\authornote{Both authors contributed equally to this research.}
\email{mark.colley@uni-ulm.de}
\orcid{0000-0001-5207-5029}
\affiliation{%
  \institution{Institute of Media Informatics, Ulm University}
  \city{Ulm}
  \country{Germany}
}

\author{Sebastian Hartwig}
\authornotemark[1]
\email{sebastian.hartwig@uni-ulm.de}
\orcid{0000-0001-8642-2789}
\affiliation{%
  \institution{Institute of Media Informatics, Ulm University}
  \city{Ulm}
  \country{Germany}
}

\author{Albin Zeqiri}
\email{albin.zeqiri@uni-ulm.de}
\orcid{0000-0001-6516-3810}
\affiliation{%
  \institution{Institute of Media Informatics, Ulm University}
  \city{Ulm}
  \country{Germany}
}

\author{Timo Ropinski}
\email{timo.ropinsik@uni-ulm.de}
\orcid{0000-0002-7857-5512}
\affiliation{%
  \institution{Institute of Media Informatics, Ulm University}
  \city{Ulm}
  \country{Germany}
}

\author{Enrico Rukzio}
%\authornotemark[1]
\email{enrico.rukzio@uni-ulm.de}
\orcid{0000-0002-4213-2226}
\affiliation{%
  \institution{Institute of Media Informatics, Ulm University}
  \city{Ulm}
  \country{Germany}
}
%%
%% By default, the full list of authors will be used on the page
%% headers. Often, this list is too long and will overlap
%% other information printed in the page headers. This command allows
%% the author to define a more concise list
%% of authors' names for this purpose.
\renewcommand{\shortauthors}{Colley and Hartwig et al.}


\begin{abstract}
State recognition in well-known and customizable environments such as vehicles enables novel insights into users and potentially their intentions. Besides safety-relevant insights into, for example, fatigue, user experience-related assessments become increasingly relevant. As thermal comfort is vital for overall comfort, we introduce a dataset for its prediction in vehicles incorporating 31 input signals and self-labeled user ratings based on a 7-point Likert scale (-3 to +3) by 21 subjects. An importance ranking of such signals indicates higher impact on prediction for signals like \textit{ambient temperature, ambient humidity, radiation temperature, and skin temperature}. Leveraging modern machine learning architectures enables us to not only automatically recognize human thermal comfort state but also predict future states. We provide details on how we train a recurrent network-based classifier and, thus, perform an initial performance benchmark of our proposed thermal comfort dataset. Ultimately, we compare our collected dataset to publicly available datasets.
\end{abstract}

%%
%% The code below is generated by the tool at http://dl.acm.org/ccs.cfm.
%% Please copy and paste the code instead of the example below.
%%
\begin{CCSXML}
<ccs2012>
   <concept>
       <concept_id>10010147.10010257</concept_id>
       <concept_desc>Computing methodologies~Machine learning</concept_desc>
       <concept_significance>300</concept_significance>
       </concept>
   <concept>
       <concept_id>10003120.10003138</concept_id>
       <concept_desc>Human-centered computing~Ubiquitous and mobile computing</concept_desc>
       <concept_significance>500</concept_significance>
       </concept>
 </ccs2012>
\end{CCSXML}

\ccsdesc[300]{Computing methodologies~Machine learning}
\ccsdesc[500]{Human-centered computing~Ubiquitous and mobile computing}


\keywords{\plainkeywords}

\begin{teaserfigure}
  \includegraphics[width=\textwidth]{images/teaser.pdf}
  \caption{We propose the Thermal Comfort Dataset. (I.) We collected human annotated sensor measurements during a thermal state study (II.), enabling the training of a neural thermal state classifier to identify human thermal state changes from sequential sensor measurements.}
  \Description{This Figure shows the two stages of our dataset introduction. On the left side, a person is depicted with two fans blowing warm or cold air. This leads to a thermal comfort rating, indicated via a scale from warm to cold. The figure then shows the various measurements, such as ambient temperature. On the right side, the figure shows via a clock that 10 sec time steps were used to predict the thermal comfort.}
  \label{fig:teaser}
\end{teaserfigure}

%%
%% This command processes the author and affiliation, and title
%% information and builds the first part of the formatted document.
\maketitle

\section{Introduction}
%Motivation
The inclusion of sensors such as cameras, radars, thermometers, or lidars in today's manually driven vehicles and, most likely, in future (automated) vehicles~\cite{sensory} allows for novel insights into users' states and intentions. To improve the user experience, there is a need for accurate and reliable recognition, interpretation, and understanding of current passenger states, as this ensures the execution of adjustments based on user preferences~\cite{stampf2022towards}.
Using in-vehicle sensors and machine-learning methods, current vehicles already recognize some driver states such as level of drowsiness~\cite{manstettenEvolutionDriverMonitoring2020} or fatigue~\cite{fatigueSurvey}. This is mainly a safety measure to avert potentially dangerous driving behavior. Yet, seeing as driving-related tasks will increasingly become irrelevant the higher the level of automation, recognition of other states such as emotional state~\cite{bethge}, cognitive load~\cite{yincogload}, and intention~\cite{nilssonActionIntentionRecognition} become increasingly relevant. There are various methods to determine the passenger's state, including machine-learning-based methods. Depending on whether the state recognition task is formulated as a supervised or unsupervised learning problem, labeled data is required~\cite{sathyaComparisonSupervisedUnsupervised2013}. However, there are only a few labeled and publicly available datasets for the automotive state recognition use case, such as \textit{drive\&act} by \citet{drive&act} or the \textit{VEmotion dataset} by \citet{bethge}. %, which includes data from 12 participants.

Already today, one relevant aspect for passengers inside vehicles is the perceived level of thermal comfort~\cite{obornePassengerComfortOverview1978, rommelfanger2020evaluation, 10.1145/3534617}. In building ergonomics, thermal comfort and its influencing factors have been part of numerous research studies that resulted in different models for thermal comfort estimation, such as the commonly known predicted mean vote (PMV) index by \citet{fanger1970thermal}. However, these datasets do not include temporal data but only provide singular data points.
%Problem
Additionally, in the context of automotive state recognition systems, thermal comfort state prediction (i.e., \textit{warm, comfortable, cold, ...}) has thus far not been explored extensively. Consequently, no machine-learning model or dataset exists for in-vehicle thermal comfort state prediction. The impact of inputs from in-vehicle sensory units on prediction performance is also unknown.
The automotive domain is particularly characterized by providing an environment known by manufacturers (regarding size, capabilities, limitations, and heating possibilities), providing (already today) a plethora of sensor data, and restricting the user's movements and actions. Therefore, a specialized dataset is required.

%Solution
\textit{Contribution Statement:} Therefore, we created a (1) \textbf{temporal}, labeled multi-modal dataset featuring \textbf{31} (age, gender, weight, height, body fat, body core temperature, activity level, time since last meal, tiredness, clothing level, radiation temperature, emotion, RGB frame, ten body pose key points, heart rate, wrist temperature, galvanic skin response, ambient temperature, relative humidity) input signals relevant for the task of thermal comfort state prediction, which has not yet been addressed in the context of automotive state recognition. (2) The implemented and employed logging can be used as a data-gathering blueprint for future state recognition research. (3) Thirdly, we present a machine-learning-based approach for in-vehicle thermal comfort state recognition that takes advantage of different feature combinations to explore predictive performance and the impact of individual input modalities. (4) Fourthly, we report an ablation study with different feature combinations and network architectures for thermal comfort state prediction and forecasting. (5) Finally, we evaluate our trained classifiers on existing thermal comfort datasets and report superior performance for models trained on our thermal comfort dataset.


\section{Related Work}
This work builds on previous work on thermal comfort in buildings, factors influencing thermal comfort, and state recognition in general.

\subsection{Thermal Comfort}
Various factors influence one's perceived comfort level in an indoor environment, such as visual, acoustic, and environmental conditions~\cite{FRONTCZAK2011922}. Thermal comfort describes the level of satisfaction with one's surroundings based on thermal influences~\cite{ramspeckASHRAESTANDARDSCOMMITTEE} and has been extensively researched in the field of building ergonomics. The PMV index \cite{fanger1970thermal} referenced in the ISO 7730:2006-05 and ASHRAE 55-2020 standards~\cite{ISO7730, ramspeckASHRAESTANDARDSCOMMITTEE} estimates the perceived level of thermal comfort for a large group of people on a thermal sensation scale with the seven items \textit{Cold, Cool, Slightly Cool, Comfortable, Slightly Warm, Warm, and Hot} and is based on empirical thermal comfort studies, from which an equation for thermal comfort calculation based on six main influencing factors was derived: Metabolic Rate, Clothing Insulation, Mean Radiation Temperature, Ambient Temperature, Relative Humidity, and Air Velocity.


\subsection{Influences on Thermal Comfort}\label{sec:rw2}
Research on building ergonomics indicates that environmental~\cite{parsonsHumanThermalEnvironments2002} or physiological (human thermoregulation~\cite{mechanisms}) factors and their interplay should be considered. Thermal sensation is mostly felt due to thermoreceptors on one's skin and muscles~\cite{terjungComprehensivePhysiology2011}. Accordingly, skin temperature has been used as an indicator of thermal perception changes (e.g., \cite{ramanathanNewWeightingSystem1964, simEstimationThermalSensation2016}). When used in conjunction with body core temperature, \citet{InfluenceOfBodyTemp1999} found that skin and body core temperature contribute similarly to thermal comfort. \citet{simEstimationThermalSensation2016} demonstrate the estimation of thermal comfort based on measuring different sites around the wrist and fingertips, and \citet{ramanathanNewWeightingSystem1964} proposed an approach for the estimation of the mean skin temperature, computed by averaging the skin temperature of different locations across the body. Another work proposed an estimation of thermal comfort from multiple physiological input streams, such as heart rate, skin temperature, or electrodermal activity~\cite{yoshikawaCombiningThermalCamera2019}. Additionally, it was established that there is a range in which occupants feel thermally comfortable (thermal comfort zone)~\cite{CIUHA2016123, ciuhaThermalComfortZone2017}, rather than a single temperature. This zone is influenced by the dynamics of temperature change and the direction of the change~\cite{ciuhaEffectThermalTransience2019}. Apart from estimation based on physiological input data, it was reported that there is an influence of gender~\cite{CHAUDHURI2018391, karjalainenThermalComfortGender2012}, age~\cite{DELFERRARO2015177, GUERGOVA201180}, and emotion~\cite{WANG2020109789} on thermal comfort perception. For instance, female occupants seem more sensitive to thermal changes that deviate from their optimal state and thus feel too cold or too hot more frequently~\cite{SAMI20071594}. In the elderly, deterioration of skin receptors is assumed to cause reduced thermal perception ability, especially in the limbs~\cite{GUERGOVA201180}. As for emotion, \citet{WANG2020109789} concluded that negative emotions have an unfavorable effect on thermal comfort. However, overall, emotions only affect thermal comfort perception during light activities such as sitting or standing.


\subsection{Thermal Comfort Label Scale}
Fanger's PMV index~\cite{fanger1970thermal} is calculated in the interval [-3, 3]. Accordingly, the seven different thermal comfort states \textit{Cold, Cool, Slightly Cool, Comfortable, Slightly Warm, Warm, and Hot} represent ranges in the defined interval rather than integers. For instance, according to the ASHRAE standard~\cite{ramspeckASHRAESTANDARDSCOMMITTEE}, the state \textit{Comfortable} is established in the interval [-0.5, 0.5]. This allows for reducing the initial seven-point scale to a three-point scale, where the new reduced states can be denoted as \textit{Too Cold, Comfortable, Too Warm}. Due to its simplicity and standardized theoretical basis, the seven-point thermal sensation scale was adopted as the label set.


\subsection{State Recognition Systems}\label{sec:rw1}
Machine-learning-based state recognition was used for cognitive load detection~\cite{cogload, yincogload}, affective computing~\cite{levasa21,emotionmeter}, vehicle assistance systems~\cite{manstettenEvolutionDriverMonitoring2020, lane, taoAssessmentDriversComprehensive2020}, and even pain recognition~\cite{walterDataFusionAutomated2015}. They mainly differ in terms of chosen input spaces, modalities, and employed learning methods (supervised or unsupervised learning). 
Even though systems trained on data from a single input space or single modality can perform satisfyingly, a closer resemblance to human perception can be achieved by incorporating additional modalities~\cite{multimodalMachineLearning}. Therefore, inter-complimentary multi-modal input data for the training of state recognition systems is beneficial.\\
Early works in multi-modal in-vehicle state recognition incorporated various vehicle parameters (pedal position, steering), environmental information (local and global vehicle position), and driving performance attributes (speed) as input signals ~\cite{berndtContinuousDriverIntention2008, heDrivingIntentionRecognition2012} to improve intention recognition in safety systems. Likewise, human action recognition experienced advances by incorporating multi-modal input signals together with feature fusion and co-learning methods~\cite{HAR}. \citet{zhangDriverBehaviorRecognition2020} demonstrated in-vehicle action recognition with their proposed interwoven CNN approach and a self-recorded dataset. Additionally, within the field of affective computing, the recognition of emotional states was explored as multi-modal interfaces, not just in the automotive context, could benefit from the ability to recognize and interpret one's emotions. With VEmotion, \citet{bethge} proposed a novel way of estimating the emotional state of drivers in real-time using driving context information such as weather, traffic, road, and car trajectory data. They demonstrated that states such as emotions can be predicted by using mainly contextual information, which is more readily available in vehicles. Generally, incorporation of insights from areas, such as emotion recognition~\cite{levasa21,emotionmeter}, cognitive load estimation~\cite{cogload} or next interaction method prediction~\cite{wolf21} can contribute in creating a more holistic understanding of users' needs in automotive state recognition systems.\\
Nevertheless, only a few publicly available datasets could be used for further state recognition research. Therefore, developing new approaches for recognizing certain states almost always entails the acquisition of a new dataset, thereby significantly slowing down development speed while increasing task complexity. Additionally, methods for in-vehicle thermal comfort estimation from multi-modal data have not been explored extensively, although thermal sensation was reported to be one of the primary influencing factors in overall comfort perception~\cite{FRONTCZAK2011922}. 



\subsection{Thermal Comfort Prediction}\label{sec:rw3}
Automated recognition of thermal comfort levels has been a research focus in energy and building ergonomics. For example, energy efficiency in office buildings or other occupant spaces could be improved through adaptation to occupants' current needs~\cite{veselyPersonalizedConditioningIts2014}. Furthermore, adaptive models for personalized thermal comfort were also correlated with an increase in occupant productivity \cite{buenoEvaluatingConnectionThermal2021} as environmental factors are better tailored to occupants' needs. 
Previous data-driven approaches for thermal comfort prediction showed differences in the used datasets (self-recorded or publicly available). With a self-recorded dataset featuring data from 12 participants, \textit{Zhang et al.} developed a building context machine-learning model that estimates thermal sensation with an accuracy of 95.4\% by identifying frowning facial expressions in recorded RGB data and relating the occurrences to the currently perceived thermal sensation~\cite{zhangFrownBasedThermalComfort2021}. \textit{Mao et al.} concluded that thermal comfort could be predicted using heart rate and left-arm wrist skin temperature measures by using a self-recorded dataset to train different machine-learning models~\cite{maoThermalComfortEstimation2021}. As part of the advances in thermal comfort research, gathering sufficiently large datasets for data-driven thermal comfort prediction has become a research focus, resulting in the acquisition of the ASHRAE RP-884~\cite{ashrae1} and, more recently, the ASHRAE II \cite{ashrae2} datasets. Both datasets accumulate environmental, personal attributes, and thermal indices, one of which is the PMV index. Moreover, both ASHRAE datasets are publicly available. Using the ASHRAE RP-884~\cite{ashrae1}, Scales~\cite{scales_project}, and US Office Buildings dataset~\cite{us_office} (all public), \citet{somuHybridDeepTransfer2021} built a machine-learning model that employs transfer learning strategies and achieved a prediction accuracy of 55\%. The Scales Project dataset is a cross-national dataset (30 countries) that explores the occupants' understanding of common thermal sensation scales, such as the previously described seven-point scale. It includes thermal comfort labels based on different rating scales for thermal conditions. Additionally, personal, indoor, and outdoor environmental factors were gathered using a questionnaire. The US Office Buildings dataset is aimed at office spaces in the US and was recorded to explore human-building interactions driven by factors such as comfort and behavioral changes over time~\cite{us_office}. It includes data on personal attributes, indoor/outdoor variables, and labels gathered with various thermal comfort rating scales. 

\citet{10.1145/3360322.3360858} presented OccuTherm, a system to predict thermal comfort using the body shape. They conducted a sensing study in which biometrics, physical measurements (height, shoulder circumference), and subjective comfort responses were recorded. They find that an adapted personalized comfort model can improve model performance.


\citet{10.1145/3485730.3493693} collected a longitudinal dataset of 17 participants over four weeks across 17 indoor and outdoor spaces. The dataset includes physical characteristics, background information, and personality surveys, which were assessed once. During the four-week trial, thermal preference, clothing level, metabolic rate, perceived air velocity, and location were assessed.

%\url{https://github.com/jonfranc/occutherm} bzw hier \url{https://zenodo.org/record/3363987#.Y2uv03bMKCU}

%\url{https://zenodo.org/record/5502441#.Y2uuLHbMKCV}



An overview of publicly available datasets is given in \autoref{tab:comparison_1}. Given the low number of public datasets (six in total), most thermal comfort research is still performed on self-recorded datasets that vary in terms of included measures and employed sensory devices. Additionally, no thermal comfort dataset for the automotive use case currently exists. 







\section{Dataset Acquisition}
\label{sec:dataset_acquisition}

Gathering ratings for thermal comfort states can be done using the thermal sensation scale referenced in the ASHRAE standard~\cite{ramspeckASHRAESTANDARDSCOMMITTEE}, which is defined in the same range as the PMV output scale proposed by \citet{fanger1970thermal}. The thermal sensation scale comprises seven-string encodings \textit{Cold, Cool, Slightly Cool, Comfortable, Slightly Warm, Warm, Hot} but can be represented numerically as \textit{-3, -2, -1, 0, 1, 2, 3}. Additionally, a significant attribute of the PMV index is its scope, as it was designed to predict thermal comfort in steady-state environments~\cite{ISO7730, fanger1970thermal}, yet, inside a vehicle, sudden shifts can occur through air conditioning units or opened windows. Therefore, the effects of temperature changes must be considered for data acquisition. In particular, the rate of change is important as thermal comfort zones in cooling and heating phases were found to differ, especially with smaller temperature step changes~\cite{ciuhaEffectThermalTransience2019}. A slow rate of temperature change in previous work was defined as 0.5 °C/min, while a fast change rate was defined as 1.0 °C/min~\cite{ciuhaEffectThermalTransience2019}. Temperature ranges used in thermal comfort experiments included ranges such as [18°,35°C]~\cite{simEstimationThermalSensation2016} or even larger ranges with a minimum of 15 °C and a maximum of 40 °C~\cite{ciuhaEffectThermalTransience2019}. Consequently, the temperature ranges and step sizes should be selected so that the conditions for all possible thermal comfort states are met at least once within the heating and cooling phases.


\subsection{Low-Fidelity Climatic Chamber}\label{subsec:hardware}

\begin{figure}[ht]
    \centering
        \includegraphics[width=0.5\textwidth]{images/setup.jpg}
        \caption{\label{fig:setup_real} The room for the thermal comfort recordings. The employed air conditioning unit can be seen to the right.}
        \Description{This Figure shows the setup that was used for the thermal comfort study. In the top-left corner, three displays are shown. In front of the displays, a steering wheel and a chair can be seen. Behind the chair, an AC unit is shown.}
\end{figure}

\citet{ciuhaEffectThermalTransience2019} used a climatic chamber and spanned 25°C (range=[15°C, 40°C]) over 150 min. Due to the unavailability of such a professional chamber, we built a low-fidelity climatic chamber  using mobile air conditioning (AC) devices. We used the Monzana MZKA1000 Smart AC (see \autoref{fig:setup_real}) with a power output of 9000 BTUs as it provides cooling and heating modes and includes a smart home cloud that allows for developer access via APIs. This device enables temperature changes in the range of [16°C, 32°C]. As the AC unit's heating capability is less powerful than its cooling mode, two additional smart home heating units (Nedis P22-2054875) were added. The smart heating components allow for temperature changes in the range of [15°C, 35°C] and include two heating modes (1200W/2000W). Adding multiple AC units for heating also ensures that the area is heated at multiple locations, which allows for a more uniform temperature change. Other than the mentioned smart AC and heating units, no other components were used to manipulate the ambient temperature (i.e. no seat heating). The ambient temperature range  achieved most reliably within 60 minutes was [18.4°C, 32.0°C]. The time interval of 60 minutes was selected to ensure that a mean temperature change rate of 0.45°C/min is achieved for both heating and cooling. This change rate is favorable compared to a high rate of 1°C/min as large thermal changes over short periods are perceived more intensely and, therefore, shift the range at which a person feels comfortable at~\cite{ciuhaEffectThermalTransience2019}.

\subsection{Input Spaces}\label{sec:acqusition}
\begin{comment}
\begin{table}[ht]
\footnotesize
\centering
\begin{tabular}{|l|l|}
\hline
\textbf{Input Space} & \textbf{Modalities}\\ \hline
Personal Context & Age~\cite{DELFERRARO2015177, GUERGOVA201180}\\
                 & Gender~\cite{CHAUDHURI2018391, karjalainenThermalComfortGender2012}\\
                 & Clothing~\cite{ISO7730, ramspeckASHRAESTANDARDSCOMMITTEE}\\
                 & Tiredness~\cite{griefahnEffectsGenderAge2001} \\
                 & Weight and Height~\cite{Bodymass2020}\\
                 & Body Fat~\cite{bodyfat2001a}\\
                 & Metabolic Rate and Activity Level ~\cite{ISO7730, ramspeckASHRAESTANDARDSCOMMITTEE, EffectTemperatureMetabolic2013} \\\hline
External Context & Relative Humidity~\cite{ISO7730, jingImpactRelativeHumidity2013}\\
                 & Ambient Temperature~\cite{ciuhaEffectThermalTransience2019, ramspeckASHRAESTANDARDSCOMMITTEE}\\
                 & Radiant Temperature~\cite{ISO7730, ATMACA20073210, ramspeckASHRAESTANDARDSCOMMITTEE}\\
                 & Air Velocity~\cite{ISO7730, ramspeckASHRAESTANDARDSCOMMITTEE} \\\hline
Physiology & Heart Rate~\cite{mansiMeasuringHumanPhysiological2021, maoThermalComfortEstimation2021}\\
           & Wrist Skin and Body Core Temperature~\cite{CHAUDHURI2018391, InfluenceOfBodyTemp1999, YAO2008310} \\
           & Galvanic Skin Response (GSR)~\cite{mansiMeasuringHumanPhysiological2021}\\\hline
Visual Attributes & 3D Body Pose~\cite{yangRealtimeContactlessMeasurements2019,YANG2020110261}\\
              & RGB View~\cite{zhangFrownBasedThermalComfort2021} \\\hline
Emotion & Emotional States (after \citet{ekman})~\cite{WANG2020109789} with neutral emotion\\\hline
\end{tabular}
\caption{\label{tab:inputs} List of derived modalities from different input spaces for multi-modal thermal comfort prediction.}
\end{table}
\end{comment}

\begin{table*}[ht]
\footnotesize
\centering
\caption{\label{tab:inputs} List of derived modalities from different input spaces for multi-modal thermal comfort prediction.}
\begin{tabular}{lp{12.5cm}}
\toprule
\textbf{Input Space} & \textbf{Modalities}\\ 
\midrule
Personal Context & Age~\cite{DELFERRARO2015177, GUERGOVA201180}, Gender~\cite{CHAUDHURI2018391, karjalainenThermalComfortGender2012}, Clothing~\cite{ISO7730, ramspeckASHRAESTANDARDSCOMMITTEE}, Tiredness~\cite{griefahnEffectsGenderAge2001}, Weight \& Height~\cite{Bodymass2020}, Body Fat~\cite{bodyfat2001a}, Metabolic Rate \& Activity Level ~\cite{ISO7730, ramspeckASHRAESTANDARDSCOMMITTEE, EffectTemperatureMetabolic2013}\\ 
External Context & Relative Humidity~\cite{ISO7730, jingImpactRelativeHumidity2013}, Ambient Temperature~\cite{ciuhaEffectThermalTransience2019, ramspeckASHRAESTANDARDSCOMMITTEE}, Radiant Temperature~\cite{ISO7730, ATMACA20073210, ramspeckASHRAESTANDARDSCOMMITTEE}, Air Velocity~\cite{ISO7730, ramspeckASHRAESTANDARDSCOMMITTEE}\\ 
Physiology & Heart Rate~\cite{mansiMeasuringHumanPhysiological2021, maoThermalComfortEstimation2021}, Wrist Skin and Body Core Temperature~\cite{CHAUDHURI2018391, InfluenceOfBodyTemp1999, YAO2008310}, Galvanic Skin Response (GSR)~\cite{mansiMeasuringHumanPhysiological2021}\\
Visual Attributes & 3D Body Pose~\cite{yangRealtimeContactlessMeasurements2019,YANG2020110261}, RGB View~\cite{zhangFrownBasedThermalComfort2021} \\
Emotion & Emotional States (after \citet{ekman})~\cite{WANG2020109789} with neutral emotion\\
\bottomrule
\end{tabular}
\end{table*}




We determined the input spaces and modalities shown in \autoref{tab:inputs} such that the PMV variables (ambient temperature, ambient humidity, metabolic rate, clothing insulation, radiation temperature, air velocity)~\cite{fanger1970thermal} along with modalities for the identified physiological, visual, emotional, and personal input spaces are included. Moreover, the input modalities used in previous research were filtered based on the obtrusiveness of the respective sensory units, as input modalities that are measured with inherently obtrusive devices are likely to reduce user experience~\cite{obtr} and are, therefore, difficult to include as sensory units in AVs.


\subsection{Sensory Hardware and Models}
The PCE-WB 20SD thermometer was selected due to its logging rate of 1Hz and the ability to record ambient temperature, relative humidity, and radiation temperature with the integrated black globe components~\cite{pce}. A cheaper and easily integrable solution for temperature and humidity data is Arduino sensory units~\cite{arduino}. However, these kits do not provide the same level of accuracy as specialized measuring tools do. Nevertheless, ambient temperature and humidity data from an Arduino sensory board were included in the data logging application to be able to compare prediction performance with different frequencies and levels of accuracy for temperature and humidity streams. As for physiological signals, we used the Empatica E4~\cite{empatica}. Modalities like emotion, body pose, and visual features can be captured using appropriate machine-learning models and RGB frame processing. For emotion estimation, \textit{Serengil and Ozpinar} presented a deep learning model that includes multiple face detector backends and allows for analysis of recorded RGB frames~\cite{serengil2020lightface, serengil2021lightface}. For 2D pose key point estimation, we employed OpenPose~\cite{openpose}. Seeing as both models can be executed in real-time, they were integrated for emotion and body pose estimation. RGB frames themselves can easily be captured using a webcam. However, for the data collection process, Microsoft's Kinect v2 for Windows~\cite{kinectDocs} was selected as it provides RGB frames of size 1920x1080 and depth frames of size 512x424~\cite{kinect}. As the selected body pose model estimates 2D skeleton key points, the depth frame provided by the Kinect sensor is necessary to measure the respective depth values. This way, 2D key points are extended to 3D key points (see \autoref{tab:inputs}).

\subsection{Logging Application}

\begin{figure}
     \centering
     \begin{subfigure}[b]{0.49\textwidth}
         \centering
         \includegraphics[width=\textwidth]{images/1.png}
         \caption{Self-report dialogues with key interaction mappings.}
         \label{fig:int1}
            \Description{}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{0.49\textwidth}
         \centering
         \includegraphics[width=\textwidth]{images/labeling_tcs.png}
         \caption{Labeling screen with the navigation buttons.}
         \label{fig:int3}
            \Description{}
     \end{subfigure}
     \caption{Self-reports and labels were gathered using a GUI incorporating various dialogues. A key-based interaction method reduces the space needed for labeling on a 7-point Likert scale.}
     \Description{Left (a): This Figure shows the initial screen that is displayed by the logging application. In the middle, a numeric pad is shown. On top of the numeric pad, a textual explanation of which keys to use for navigation is shown.  Right (b): This Figure shows the labeling dialog used in the logging application. At the top, there is a text asking the user to rate which of the displayed items describes their current level of thermal comfort best, and underneath a seven-point rating scale with the items Cold, Cool, Slightly Cool, Comfortable, Slightly Warm, Warm, and Hot is shown.}
\end{figure}


Self-reports and labels were gathered using a GUI that incorporates various dialogues. At first, the interface was implemented as a click-based GUI that required a mouse as an input device. However, as \autoref{fig:setup_real} illustrates, the driving simulator and the selected recording room do not leave much space to operate additional devices. Moreover, moving one's arm to provide click-input causes unnatural movement that can be seen in the RGB frames and key point estimations. Thus, the GUI back-end was restructured to allow for navigation and input using only the keys on a numeric keypad. Using key-based rather than click-based input reduces the overall required number of interactions and interaction time while requiring less space for additional components such as a mouse pad. \autoref{fig:int1} shows how the descriptions of key mappings are conveyed to the user. Input fields usually require an initial interaction to set them in focus and provide input. The GUI bridges this step by only displaying one question per dialog. If the current dialog includes a question where an input field is used, it is already focused on as soon as the dialog is rendered, such that users are immediately able to interact with the GUI element. 



As shown in \autoref{fig:int3}, once labeling begins, the keys used for dialogue navigation can no longer be used, and the navigation buttons displayed on the left- and right-bottom corners disappear. During labeling, label and waiting dialogues are shown alternatingly, based on pre-defined time intervals (20 seconds between labeling prompts). Once a label is selected via the numeric keypad, the GUI instantly switches to the waiting dialogue without requiring further interactions. The waiting screen is then shown until the current waiting interval has ended, and the labeling dialogue is shown again. Omitting the dialog navigation interactions reduces the overall number of interactions needed during labeling from four to two interactions per labeling event and from 720 to 360 over the entire recording session. This was relevant as labels for each recording session are collected over 60 minutes, during which many interactions could lead to reduced patience or negative emotions. \\
During the trials, a feeling of boredom quickly manifested due to the repetitive task of simply labeling one's thermal comfort level. In addition, participants were not required to use the steering wheel or pedals to perform a driving task during the labeling phase, and only the center monitor was used to display instructions while the monitors to the left and right of the driving simulator were turned off. Consequently, the waiting dialogue between labeling prompts was extended to display a slide show of fractal images while waiting changed at a frequency of 0.5Hz. 
While we could not provide an acclimatization period between the heating and cooling period, we let participants first fill out a demographic questionnaire and explained the scenario. This took approximately 20 minutes, providing sufficient time for initial acclimatization. Nonetheless, this remains a limitation of our dataset.


\section{Dataset Statistics}

\subsection{Participants}
We gathered data from \N{21} participants. The gender ratio was \textit{12F/9M} with a mean age of \m{24.64} (\sd{3.03}, \rg{20}{33} in years). Participants weighed \m{69.97} (\sd{15.02}, \rg{53.00}{106.90} in kg) at an average height of \m{174.50} (\sd{10.18}, \rg{155.00}{198.00} in cm) and a body fat percentage of \m{22.00\%} (\sd{5.00\%}, \rg{14.00\%}{34.00\%}). Body temperature measured at the forehead with an infrared skin thermometer was mostly similar for all participants (\m{36.38}, \sd{0.31}, \rg{35.8}{37.2} in °C). Clothing levels varied only slightly, as most participants wore a short-sleeve T-shirt and trousers (mean insulation based on ASHRAE standard 55 clothing insulation tables \cite{ramspeckASHRAESTANDARDSCOMMITTEE} \m{0.60}, \sd{0.05}, \rg{0.45}{0.69} in clo).
Only 14\% (3Y/18N) of participants reported having performed physical activities in the hour before the recording session, while the time since participants' last meals was \m{4.23} hours (\sd{4.92}, \rg{0}{20}) before the recording. Based on the tiredness ratings in the initial personal context assessment, participants reported having been moderately tired at the start of their recording (\m{4.12}, \sd{1.73}, \rg{2}{8}).

\begin{table}[ht]
\centering
\footnotesize
\caption{\label{tab:results} Means, standard deviations, and ranges of the numeric features included in the thermal comfort dataset.}
\begin{tabular}{lcccc}
\toprule
Feature & Units & Mean & Std & [Min, Max] \\
\midrule
Radiation Temperature           & °C & 25.53 &  3.92 & [16.9, 33.6]  \\
Heart Rate                      & bpm& 82.94 & 13.46 & [40.0, 191.99]\\
Wrist Skin Temperature          & °C & 33.82 &  1.75 & [27.91, 36.95]\\
Galvanic Skin Response          & mS &  1.42 &  2.81 & [0.0, 16.9]   \\
Ambient Temperature PCE-WB 20SD & °C & 25.31 &  3.72 & [17.1, 33.7]  \\
Ambient Temperature Arduino     & °C & 26.82 &  4.52 & [17.6, 37.0]  \\
Relative Humidity               & \% & 31.35 &  8.86 & [12.0, 55.0]  \\
\bottomrule
\end{tabular}
\end{table}


\subsection{Raw Dataset}\label{sec:raw_dataset}
The initial raw dataset included recordings from all 21 participants. Due to incorrect use of the numeric keyboard during labeling, the CSV log file of one participant was removed. Also, we removed another $2$ participants due to incomplete radiation temperature readings. Therefore, the final and filtered raw dataset includes 18 CSV log files and a separate image archive with the RGB frames. We used 16 participants for training and 2 participants to evaluate our models (see Section~\ref{sec:experiments}). The CSV files include corresponding body pose key-point coordinates for each recording. Due to movement out of the depth camera's field of view during the recordings, not all key points could be estimated reliably at all times, which led to empty vectors in the dataset. During the recording sessions, audio was also recorded. However, while pre-screening the data, it became apparent that there were few to no verbal utterances by the participants captured in the audio files. Thus, the audio recordings were not included in the filtered dataset. The final dataset includes a total of 1856290 CSV input lines, each with 34 feature columns (including timestamp and label columns). \autoref{tab:results} shows a descriptive evaluation for the numeric features not gathered using the self-report GUI. \\
\textit{Radiation and ambient temperature} are fairly similar and could replace each other during classification. \\
Concerning the values measured with the Empatica E4 wristband, the most stable measurements were achieved for the wrist skin temperature and the heart rate. This is indicated by the low standard deviation (\sd{1.75}) and minimum and maximum values (\textit{min=27.81}, \textit{max=36.95}) close to the mean of 33.82°C. Contrarily, the raw measurements for the remaining physiological signals, heart rate, and GSR were far less stable. The highest outlier rate out of the numeric continuous features was found in the GSR measurements (13.7\% outliers). Therefore, a data pre-processing scheme that includes outlier removal methods is required when using the dataset for classifier training. \\
The raw dataset further includes two \textit{emotion} feature columns, as participants were able to report their current emotions using the logging GUI while also having their emotions estimated using the RGB capture and the DeepFace model. Emotions were estimated at the same frequency as emotion self-report dialogues were displayed. Most participants rated their emotions as \textit{neutral}. This resulted in different rating distributions across emotion feature columns. Self-reported emotions were distributed as follows: \textit{Anger 0.83\%, Contempt 0.00\%, Disgust 3.14\%, Fear 0.00\%, Happiness 8.35\%, Neutral 86.38\%, Sadness 0.73\% and Surprise 0.49\%}. The model-based emotion predictions also tended towards neutrality but less strongly: \textit{Anger 10.99\%, Disgust 0.08\%, Fear 13.48\%, Happiness 8.98\%, Neutral 41.14\%, Sadness 20.87\%, and Surprise 4.46\%}. \\
The dataset also includes two different \textit{ambient temperature} features. As described in Section~\ref{subsec:hardware}, an external thermometer was used during the recordings to measure radiation temperature changes. However, when comparing the ambient temperature measures of the external thermometer and the Arduino sensory kit measures, it could be observed that the Arduino kit tends to react more intensely when ambient temperature changes occur. Moreover, the temperature sensor used in the sensory kit is labeled to have an accuracy of 2°C \cite{arduinodht11}, whereas the external thermometer (PCE-WB 20SD) is labeled with an accuracy of 0.8°C \cite{pcelogger}. For this reason, further analysis and visualizations of ambient temperatures are all based on the values measured with the more accurate PCE thermometer.

\subsection{Predictor Analysis}\label{sec:predictor}

The label distribution throughout the dataset suggests that the minimum temperature during the trials was insufficient for inducing cold and cool thermal sensations. The answers given during participants' debriefing further support this assumption, as it was mentioned that \textit{"the temperature, in the beginning, felt somewhat cool, but you get used to it quickly"} [P16] and \textit{"it didn't get very cold, but it did get quite hot."} [P7]. The collected labels are distributed as follows: \textit{Cold 4.49\%, Cool 9.53\%, Slightly Cool 22.27\%, Comfortable 22.71\%, Slightly Warm 13.30\%, Warm 15.83\%, Hot 11.88\%}.

\begin{figure}[ht]
    \centering
        \includegraphics[width=0.85\textwidth]{h_c_labels.pdf}
        \caption{\label{fig:hot_cold} Left: Labels given during heating. Right: Labels for the cooling phase. In line with previous research (e.g.,~\cite{ciuhaEffectThermalTransience2019}), thermal comfort is established at a lower ambient temperature during heating than during cooling.}
        \Description{This Figure shows two visualizations for labels given during different temperatures in the heating (left side) and cooling phase (right side). On the left side, the plot illustrates that only a few participants rated the achieved ambient temperatures as cold, and most participants rated achieved comfortably as hot states. On the right side, the plot illustrates that all possible thermal comfort states were generally achieved at higher temperatures. It can also be seen that cold and cool thermal comfort states were not achieved by most participants.}
\end{figure}

A difference in thermal comfort ratings could be found between the heating and cooling phases, as illustrated in \autoref{fig:hot_cold}. This is in line with previous findings that reported a thermal comfort zone shift based on previous exposure to different thermal environments~\cite{ciuhaEffectThermalTransience2019}. Moreover, a thermal comfort zone can be seen between the labels \textit{Slightly Cool} and \textit{Slightly Warm}, as the temperature ranges for slightly cool, comfortable, and slightly warm states are the largest among all reported states. From a classification perspective, this indicates that classifications of states in the thermal comfort zone may be more difficult to predict based on ambient temperature alone, while colder states may be more difficult to predict due to the imbalance in frequency of occurrence in the dataset.\\
Consequently, only largely influential features should be used for prediction tasks to ensure that separability, especially within the thermal comfort zone, is maximized. Therefore, a feature importance analysis was conducted using R (version 4.2.2) and RStudio (2022.07.1) with packages up-to-date in November 2022. Due to the ordinal scale of thermal sensation labels, the influence of continuous numeric features (e.g., ambient temperature, radiation temperature, wrist skin temperature) was assessed using Spearman's rank correlation $\rho$~\cite{spearmanProofMeasurementAssociation2010}. In terms of ordinal input features such as tiredness, Kendall's tau coefficient~\cite{kendallRankCorrelationMethods1948a} was used to identify relationships. For the dichotomous features gender and sport, the rank-biserial correlation coefficient~\cite{curetonRankbiserialCorrelation1956a}, as suggested in previous research on association measures~\cite{khamisMeasuresAssociationHow2008}, was used. Moreover, individual linear regression models were created for all the included categorical features (gender, emotion features, tiredness, sport). The resulting $R^{2}$ values were used to assess the explainability of variance in the dataset based on each feature.\\

According to the computed correlation coefficients, the three strongest feature-label relationships were found for ambient temperature ($\rho=0.77$), ambient humidity ($\rho=-0.67$), and radiation temperature ($\rho=0.61$) followed by the time since participants' last meals ($\rho=-0.19$) and heart rate ($\rho=0.17$). The wrist skin temperature ($\rho=0.04$) and GSR ($\rho=0.06$) were found to have the weakest relationships. Correlation results for occupant age ($\rho=0.07$), weight ($\rho=0.08$), height ($\rho=0.10$), and clothing level ($\rho=0.02$) were also considered, however, no strong relationships were found. The gender ($r_{rb}=-0.115$) and sport ($r_{rb}=-0.143$) features were found to have a weak negative correlation with the labels. Similarly, the tiredness ratings were also found to have a weak negative correlation ($\tau=-0.08$) with the thermal sensation labels. Adjusted $R^{2}$ values from individual linear regression models for gender ($R^{2}$ = 0.01, \pminor{0.001}), emotion features (self-reported: $R^{2}$ = 0.006, \pminor{0.001}; ML-reported: $R^{2}$ = 0.00003, \pminor{0.001}), tiredness ($R^{2}$ = 0.003, \pminor{0.001}), and sport ($R^{2}$ = 0.01, \pminor{0.001}) support the low correlation values, as a poor fit ($R^{2}$ < 0.1) was found in each regression model.

\subsection{Comparison to Publicly Available Datasets}

This section compares our so-called \tool dataset to related available datasets. \autoref{tab:comparison_1} shows the $>17$ times higher number of entries. All datasets include measures from the personal and environmental input space. The environmental variables for the \tool, ASHRAE RP-884, ASHRAE II, and US Office Buildings datasets include the environmental measures necessary for PMV calculation. The Scales Project dataset includes weather station and indoor environment data; however, no specialized tools (e.g., black globe thermometers for radiation temperature measures or anemometers for air velocity) were used for the acquisition. Thus, PMV estimations cannot be computed using the Scales Project dataset. The listed publicly available datasets include ratings that were gathered with different rating scales (\textit{TS=thermal sensation}, \textit{TP=thermal preference}, and \textit{TA=thermal acceptability}). The \tool differs from the publicly available datasets by including physiology, emotion, visual signals, and participants' RGB recordings. \textbf{Additionally, the \tool includes temporal data compared to the singular data of the other datasets.}

%ASHRAE Datasets: Thermal Preference = 3-Point, Thermal Acceptability = 2-Point
%Scales: Thermal Preference = 7-Point, Thermal Acceptability = 4-Point
%US Buildings: Thermal Preference = 7-Point, Thermal Acceptability = 7-Point (Cold, Cool, ...) und man soll bewerten welche von den 7 Punkten Acceptable/Unacceptable sind --> am Ende wieder 2-Point?


\begin{table}[ht]
\centering
\footnotesize
\caption{\label{tab:comparison_1} Comparison to publicly available datasets with our \toolData. 7P=seven-point thermal sensation (\textit{Cold, Cool, Slightly Cool, Comfortable, Slightly Warm, Warm, Hot}), 3P=three-point thermal preference (\textit{Want Warmer, No Change, Want Cooler}), and 2P=two-point thermal comfort (\textit{Acceptable, Unacceptable}). Measurements coded as P=Personal Context, EX=External Factors, PH=Physiological Factors, E=Emotions, and V=Visual Attributes. The ASHRAE datasets also include different environmental indices (e.g., PMV index).}
\begin{tabular}{@{}lcccccccccc@{}} 
\toprule
\multirow{2}{*}{Dataset} & \multirow{2}{*}{Entries} & \multirow{2}{*}{Temporal Data} & \multicolumn{5}{c}{Included Measurements} & \multicolumn{3}{c}{Rating Scales} \\
\cmidrule(lr){4-8}\cmidrule(lr){9-11}                   
{}                                   &     {}  & {}        & P          & EX         & PH        & E         & V         & 7P         & 3P         & 2P         \\
\midrule
ASHRAE RP-884~\cite{ashrae1}                        & 25288 & \redxmark         &\greencheck &\greencheck & \redxmark & \redxmark & \redxmark &\greencheck &\greencheck &\greencheck  \\
ASHRAE II~\cite{parkinson2022ashrae} & 109033   & \redxmark      &\greencheck &\greencheck & \redxmark & \redxmark & \redxmark &\greencheck &\greencheck &\greencheck  \\
Scales Project~\cite{scales_project}                       & 8225  & \redxmark         &\greencheck &\greencheck & \redxmark & \redxmark & \redxmark &\greencheck &\greencheck &\greencheck  \\
US Office Buildings~\cite{us_office}                  & 2503  & \redxmark         &\greencheck &\greencheck & \redxmark & \redxmark & \redxmark &\greencheck &\greencheck &\greencheck  \\
OccuTherm~\cite{10.1145/3360322.3360858}                            & 2067   & \greencheck           &\greencheck &\greencheck & \greencheck & \redxmark & \redxmark &\redxmark &\redxmark &\redxmark  \\
LPTC~\cite{10.1145/3485730.3493693}                                 & 1403   & \greencheck           &\greencheck &\greencheck & \greencheck & \greencheck & \redxmark &\redxmark &\greencheck &\redxmark  \\
\hdashline
\textbf{\tool}                         &\textbf{1856290}& \greencheck&\greencheck &\greencheck &\greencheck&\greencheck&\greencheck&\greencheck&\redxmark & \redxmark \\
\bottomrule
\end{tabular}
\end{table}

\begin{table}[ht]
\centering
\footnotesize
\caption{\label{tab:comparison_2} Comparison of publicly available datasets with our \toolData regarding acquisition methods and scope.}
\begin{tabular}{@{}lcccc@{}} 
\toprule
\multirow{2}{*}{Dataset} & \multicolumn{2}{c}{Acquisition Method} & \multicolumn{2}{c}{Scope}\\
\cmidrule(lr){2-3}\cmidrule(lr){4-5}                   
{}         & Field Study & Chamber Study & Buildings & Vehicles\\\midrule
ASHRAE RP-884~\cite{ashrae1}                   &\greencheck   &\redxmark   &\greencheck &\redxmark   \\
ASHRAE II~\cite{parkinson2022ashrae}                       &\greencheck   &\redxmark   &\greencheck &\redxmark   \\
Scales Project~\cite{scales_project}                  &\greencheck   &\redxmark   &\greencheck &\redxmark   \\
US Office Buildings~\cite{us_office}             &\greencheck   &\redxmark   &\greencheck &\redxmark   \\ 
OccuTherm~\cite{10.1145/3360322.3360858}             &\greencheck   &\redxmark   &\greencheck &\redxmark   \\ 
LPTC~\cite{10.1145/3485730.3493693}             &\greencheck   &\redxmark   &\greencheck &\redxmark   \\ 
\hdashline
\textbf{\tool}                    &\redxmark     &\greencheck &\redxmark   &\greencheck \\
\bottomrule
\end{tabular}
\end{table}


\section{Experiments}
\label{sec:experiments}
We conducted several experiments on the dataset acquired in Section~\ref{sec:acqusition}. First, we performed feature importance ranking using an impurity-based method, see \autoref{fig:impurity_imp}. Based on the found feature importance, we conduct an ablation study in Section~\ref{ssec:ablation_study}, to find the best feature combination. Having these insights, we considered an input vector of four features for all experiments and implemented three types of classification models. As our baseline, we use a random forest classifier (Section~\ref{ssec:random_forest_classifier}), which we compare against deep learning models based on recurrent networks (RNN) (Section~\ref{ssec:LSTM_classifier}), and a combination of RNN and convolutional neural networks (CNN), in Section~\ref{ssec:RCNN_classifier}. Additionally, we investigate the forecasting of time series data to predict thermal comfort for a future state in Section~\ref{ssec:forecasting}. In Section~\ref{ssec:exp_comparison}, after the evaluation of the best classifier, we continue with a comparison of our dataset to the ASHRAE II~\cite{parkinson2022ashrae} dataset. Finally, we measure the performance of the PMV index on our dataset in Section~\ref{ssec:pmv_performance}. For all experiments, we provide quantitative performance results in this Section. For more details, see Appendix~\ref{sec:confusion_matrices}. %We implemented a Random Forest (RF)~\cite{breimanRandomForests2001} and two recurrent neural networks (RNNs). We included two RNNs as transient environmental factors that change over time (i.e., ambient temperature and humidity) and influence one's thermal perception based on the direction and magnitude of change \cite{ciuhaEffectThermalTransience2019}. Therefore, changes in feature variables over multiple time steps provide an additional dimension of information for prediction.

\begin{figure*}[ht]
    \centering
        \includegraphics[width=\textwidth]{images/tree_importance_scores_impurity_based_remake.pdf}
        \caption{\label{fig:impurity_imp} Feature importance ranking based on mean accuracy impurity decrease of a random forest classifier. According to this metric, the five most important features were ambient temperature, ambient humidity, radiation temperature, skin temperature, and galvanic skin response, marked in green.}
        \Description{This Figure shows the feature importance ranking based on Random Forest estimated data impurity scores. According to this metric, the five most important features were ambient temperature, ambient humidity, radiation temperature, skin temperature, and galvanic skin response. The remaining features were ranked as far less important. According to this metric, the least important feature was the participants' gender.}
\end{figure*}

\subsection{Random Forest Classifier}
\label{ssec:random_forest_classifier}
In a first step, we leverage the random forest classifier to perform an impurity-based importance ranking of all available features, see \autoref{fig:impurity_imp}. For this task, we use the random forest implementation from the sklearn library~\cite{scikit}, which provides functions for dataset sampling, pre-processing, and training pipeline definition. While RF models provided by sklearn allow for tuning of different parameters such as the number of estimators, max tree depth, and max number of features to consider per node, the standard configuration is used at first and then later optimized using a grid search approach. For our experiments, we used 400 estimators and a maximum tree depth=8 adopting the Gini impurity cost function~\cite{jost2006entropy}. We used every $100$-th data point to downsample the dataset.\\
\noindent\textbf{Results.} In \autoref{tab:performances}, we report the classification accuracy of 47.1\% for the random forest classifier, indicating a mediocre performance. %Further evaluation results are reported in the Appendix.
%The confusion matrix in \autoref{fig:conf_rf_7} shows that the model can estimate warmer states more reliably than comfortable and cold states.

\subsection{Recurrent Network Classifier}
\label{ssec:LSTM_classifier}
While the RF classifier produces a prediction for a single feature vector, we investigated representing our time-dependent data as time series of feature vectors and leveraging a recurrent neural network to process sequences of features. Thus, we implement our model in a standard encoder-decoder structure. Our feature encoder network consists of two long short-therm memory cells (LSTM), followed by a decoder network, which outputs a probability distribution of classes. We use the deep learning framework PyTorch for implementing the LSTM cells, along with PyTorch-Lightning for module and training cycle management. We formulate the classification task as a regression problem by using ordinal labels~\cite{cheng2008neural}. For further details, we refer the reader to Appendix~\ref{appendix:data_prep}. We use the mean-squared error (MSE) as a loss function to regress ordinal labels. We use the following hyper-parameters: learning rate=0.00001, learning rate decay=0.9999999, batch size=16 and dropout=0.5. For the two LSTM cells, we use a hidden state size of $64$ and a sequence length of $30$. Also, we discard every $10$-th data point, corresponding to a sequence length of $10$ seconds.

% in der MA steht: "The top-six feature combination was used for all the runs." - in der Tabelle stehen: "clothing, radiation temperature, ambient temperature, ambient humidity, GSR, heart rate, skin temperature" ---> @ALBIN welche waren die 6?

%Top-6 = ambient temperature, humidity, radiation, heart rate, skin temperature, gsr. Clothing level wurde in den ersten runs zusätzlich benutzt
% weil die Annahme war, dass alle Variablen der PMV Formel auch einen Einfluss auf die Vorhersagegenauikeit haben sollten 
% --> Problem: Starkes Overfitting --> Lösungsansatz: Modelparameter schrittweise optimieren und unterschiedliche feature combinations aus den 
% in der Tabelle genannten testen --> Beste Ergebnisse in meinem Fall mit nur 3 input features (ambient temperature, humidity, radiation = top-3)

\noindent\textbf{Results.} The training of our network resulted in an evaluation accuracy of $59.1\%$ outperforming the random classifier, which is an expected result regarding the increased number of parameters of our recurrent classifier and the ability to extract neural representations of sequence data. %More results, like class-specific measures, are reported in the Appendix.% The confusion matrix in \autoref{fig:conf_lstm_7} suggests that, similarly to the RF model, colder sensations are difficult on the recorded dataset, while warmer sensations are more reliably predicted. The thermal comfort zone between labels -1,0 and 1 was often classified incorrectly, which may have been caused by the large range of temperatures that can be perceived as comfortable compared to the temperatures that are perceived as cold or hot. 

\subsection{Forecasting using Recurrent Classifier}
\label{ssec:forecasting}
In our experiments, we apply recurrent neural networks (RNNs) to sequential data to predict the state of a future point in time for a given sequence of the current time step. As RNNs are also successfully used for time series forecasting~\cite{connor1991recurrent,di2017recurrent}, we investigate different forecasting ranges to explore the accuracy of predictions using various forecasting windows. We adopt the classifier presented in Section~\ref{ssec:LSTM_classifier} and train it using labels from time steps later in the future. The forecasting gaps used in our experiments range from 10 seconds, 5 minutes, and 10 minutes into the future. Shorter forecasting intervals are of greater interest as ambient temperature changes in a vehicle can frequently occur in real-world use cases, consequently making long forecasting windows impractical. We train our classifier using the identical training protocol as it is described in Section~\ref{ssec:LSTM_classifier}.\\
\noindent\textbf{Results.} In \autoref{tab:performances}, performance results for our forecasting experiments are reported in row two until row four. The prediction performance between the 10-second window and the 5-minute window differed only slightly (56.6\% for the 10-second window, 55.5\% for the 5-minute window), while the 10-minute accuracy was lower (50.9\%) than both previous forecasting performances. Experimental runs with forecasting windows exceeding 10 minutes showed further decreases in prediction performance.

\subsection{Image-Based Recurrent Classifier}
\label{ssec:RCNN_classifier}
In this experiment, we investigated the presence of visual data in the input sequence to our network. Hence, an additional model was implemented to incorporate RGB and body pose key points. This model represents the capabilities given in today's vehicles. While training an LSTM with normalized RGB tensors is possible, it introduces high redundancy and complexity as individual pixel values are processed for a sequence of images. In past works, architectures for tasks such as action recognition have been proposed, which include a feature extraction step before the LSTM component~\cite{ullahActionRecognitionVideo2018}. For image feature extraction, architectures that employ CNNs are often used~\cite{osheaIntroductionConvolutionalNeural2015}. To obtain image features from each participant's RGB frames, ResNet~\cite{heDeepResidualLearning2016} was selected due to its ability to filter deep features in images while also introducing mechanisms to avoid the vanishing gradient problem. The PyTorch framework provides pre-trained ResNet versions~\cite{pytorchResNet} that can be incorporated directly into existing models. We train our network with a batch size of $4$ and increase the skip rate from $10$ to $30$ to achieve sufficient training speed for the image-based RNN model. Image features extracted by the ResNet are concatenated to the feature vector. We stick to the best-performing feature combination, described in~\ref{ssec:RCNN_classifier}.\\
\noindent\textbf{Results.} During this experiment, the classifier reached an accuracy of $48.5\%$, falling behind our classifier trained without image features. A quantitative comparison between all models is reported in \autoref{tab:performances}.

\subsection{PMV and Scale Reduction Performance}
\label{ssec:pmv_performance}
To compare the performance of our proposed models to the PMV index, we pre-computed the PMV values for all participants in the evaluation split of our dataset using the clothing level, radiation temperature, ambient temperature, and ambient humidity features. For computation of PMV measures, the PyThermalComfort package~\cite{tartariniPythermalcomfortPythonPackage2020a} was used, which enables PMV computation based on the formulas defined in the ISO7730 \cite{ISO7730} and ASHRAE standards \cite{ramspeckASHRAESTANDARDSCOMMITTEE}. \\
\noindent\textbf{Results.} A comparison with the participants' subjective thermal sensation ratings showed that the PMV index fails to accurately predict thermal comfort. The achieved prediction accuracy was 35.9\%, which suggests that the PMV index values were mostly off by one class.%To conclude the performance analysis, the best model configurations for each architecture were used to train the models on different rating scales (seven-point ratings: \textit{Cold, Cool, Slightly Cool, Comfortable, Slightly Warm, Warm, Hot}, three-point: \textit{Too Cold, Comfortable, Too Hot}, and two-point ratings: \textit{Comfortable, Uncomfortable}). Using the reduced label space, all classifiers achieved prediction improvements. The RF classifier accuracy increased to 0.76 for the three-point and 0.88 for the two-point scale. The LSTM model was able to achieve a prediction accuracy of 0.85 (three-point) and 0.94 (two-point) for current state predictions and 0.81 (three-point)/0.94 (two-point) for forecasting prediction accuracy. 
%The 10-second look-ahead configuration was used in the forecasting configuration, as the best performance was achieved with a 10-second forecasting window in previous runs. Once more, the CNN-LSTM model combined with automotive features achieved the lowest prediction scores out of the implemented models, with 0.662 (three-point) and 0.893 (two-point) accuracy for the reduced label scale. The confusion matrices for all models (see \autoref{fig:conf_all_reduced}) show that even in the reduced three-point scale, accurate and reliable prediction of comfortable states is problematic, whereas the scale minimum and maximum labels can be predicted with higher certainty. 
\begin{table}[ht]
\setlength{\tabcolsep}{3.5pt}
\centering
\footnotesize
\caption{\label{tab:performances} Evaluation results of a performance comparison between different classifiers on our dataset. In the last row, we report performance for the PMV index computed for the corresponding PMV values. }
\begin{tabular}{lccc}
\toprule
\textbf{Classifier} & \textbf{7-point Acc.}& \textbf{3-point Acc.}& \textbf{2-point Acc.}\\
\midrule
LSTM               & 59.1\% & 78.4\% & 73.5\% \\
Forecast 10 sec    & 56.6\% & 80.4\% & 74.4\% \\
Forecast 5 min     & 55.5\% & 77.7\% & 74.6\% \\
Forecast 10 min    & 50.9\% & 77.1\% & 73.3\% \\
CNN-LSTM           & 48.5\% & 69.4\% & 76.0\% \\
RF                 & 47.1\% & 73.8\% & 67.9\% \\
\hdashline
PMV                & 35.9\% & 63.7\% & 65.2\%   \\
\bottomrule                                             
\end{tabular}
\end{table}

\subsection{Feature Ablation Study}
\label{ssec:ablation_study}
To access the combination of input features to our networks, we conducted an ablation study based on the top-k ranked features by the feature importance method (see \autoref{fig:impurity_imp}). We selected $k=5$, covering the most important features while also yielding a manageable number of combinations. For the ablation study, we selected $n$ features out of $k$, where $n\in[3,4,5]$. In total, this results in $16$ combinations of input features. For each training run, we use identical hyper-parameters, as described in Section~\ref{ssec:RCNN_classifier} \\
\noindent\textbf{Results.} In \autoref{tab:ablation_study_results}, we report performance results of all training runs. These results indicate that the best prediction results can be achieved using the following input features: \textit{Radiation Temperature, Ambient Humidity, Ambient Temperature, and Wrist Skin Temperature.}, as demonstrated in Section~\ref{ssec:LSTM_classifier}. Using the five most important features reduces the accuracy of the classifier to 55.9\%.

\begin{table}[ht]
\setlength{\tabcolsep}{3.5pt}
\centering
\footnotesize
\caption{In this experiment, we report the accuracy of our classification and regression model, ablating different combinations of input features. We highlight the feature combination performing best. During the ablation study, we investigate the top $5$ ranked features by our feature importance analysis, \autoref{fig:impurity_imp}: Radiation temperature (\textsc{RT}), wrist skin temperature (\textsc{WT}), galvanic skin response (\textsc{GL}), ambient temperature (\textsc{AT}), relative humidity (\textsc{RH}).}\label{tab:ablation_study_results}
\begin{tabular}{cccccccccc}
\toprule
\multicolumn{5}{c}{Features}& \multicolumn{3}{c}{Classification} & \multicolumn{2}{c}{Regression} \\
GL & AT & AH & RT & WS & 7-point Acc & 3-point Acc & 2-point Acc & MSE & L1\\
\midrule
 & X & X & X & X & 59.1\% & 78.4\% & 73.5\% & 0.065 & 0.122\\
 & X &  & X & X & 57.7\% & 79.1\% & 73.8\% & 0.063 & 0.118\\
 & X & X &  & X & 57.1\% & 79.6\% & 72.7\% & 0.063 & 0.138\\
X & X &  & X & X & 56.2\% & 79.2\% & 72.8\% & 0.065 & 0.140\\
X & X & X & X & X & 55.9\% & 79.8\% & 72.1\% & 0.063 & 0.141\\
X & X &  &  & X & 55.8\% & 79.4\% & 72.8\% & 0.063 & 0.139\\
X & X &  & X &  & 55.4\% & 79.2\% & 72.3\% & 0.064 & 0.128\\
X & X & X &  & X & 55.4\% & 79.0\% & 73.3\% & 0.066 & 0.140\\
X & X & X & X &  & 55.0\% & 78.0\% & 72.7\% & 0.064 & 0.136\\
 & X & X & X &  & 52.5\% & 78.6\% & 69.6\% & 0.066 & 0.128\\
X & X & X &  &  & 50.5\% & 73.9\% & 73.1\% & 0.072 & 0.147\\
X &  &  & X & X & 47.7\% & 70.8\% & 74.6\% & 0.074 & 0.178\\
 &  & X & X & X & 45.6\% & 70.5\% & 70.8\% & 0.077 & 0.180\\
X &  & X & X &  & 42.0\% & 64.9\% & 69.0\% & 0.098 & 0.185\\
X &  & X & X & X & 41.5\% & 67.5\% & 70.5\% & 0.079 & 0.175\\
X &  & X &  & X & 41.1\% & 67.3\% & 68.1\% & 0.089 & 0.184\\ 
\bottomrule                                             
\end{tabular}%

\end{table}


\subsection{ASHRAE Thermal Comfort Field Measurements}
\label{ssec:exp_comparison}
In this experiment, we focus on a comparison of our dataset to existing thermal comfort datasets. After a close inspection of publicly available datasets, the ASHRAE II~\cite{parkinson2022ashrae} dataset features comparable aspects to our collected dataset, as it provides measurements for \textit{Radiation Temperature}, \textit{Ambient Temperature} and \textit{Ambient Humidity}, which are also measured in our dataset. While our dataset provides sequences over time, ASHRAE II recorded single data points, which makes it unfeasible to compare against our recurrent-based methods. Instead, we optimize a random forest classifier, described in Section~\ref{ssec:random_forest_classifier}, using the ASHRAE II dataset. After filtering out data points with incomplete measurements, a total of $31500$ data points remain. We use $80\%$ of the data for training, resulting in $25204$ datapoints and the remaining $6301$ datapoints for evaluation. Then, we train and evaluate the classifier on ASHRAE II. Further, we use the trained classifier from Section~\ref{ssec:random_forest_classifier}, which is trained on our dataset, and evaluate it on ASHRAE II. Finally, we use the classifier trained on ASHRAE II and evaluate it on our dataset. We report evaluation results in \autoref{tab:ashrae_comparison}. \\
\noindent\textbf{Results.} We can show that the evaluation split of our dataset seems to be more difficult than ASHRAE II. This is indicated by a worse performance of the classifier trained and evaluated on our dataset than the classifier trained and evaluated on ASHRAE II. Also, the performance of the classifier trained on ASHRAE II and evaluated on our dataset performs worse than the classifier trained on our data and evaluated on ASHRAE II, which further supports our observation. This comparison shows a big gap between both thermal comfort datasets while underlining the missing feature information when only three sensor measurements are used. Further, this experiment shows a drastic performance drop when only single thermal state values are used for state recognition. Looking at the 2-point performance measure, the classifier trained on our dataset generalizes to the ASHRAE II dataset.

\begin{table}[ht]
\setlength{\tabcolsep}{3.5pt}
\centering
\footnotesize
\caption{Comparison results of a random forest classifier that was optimized on our dataset (\tool) and evaluated on the ASHRAE II dataset and vice versa. We report evaluation performance measured on the 7-point, 3-point, and 2-point scales. Each classifier was trained using \textit{Radiation Temperature}, \textit{Ambient Temperature} and \textit{Ambient Humidity} as input vector.}\label{tab:ashrae_comparison}
\begin{tabular}{ccccc}
\toprule
Train & Evaluate & 7-point Acc & 3-point Acc & 2-point Acc \\
\midrule
\tool       & \tool       & 40.6\% & 76.1\% & 69.1\% \\
ASHRAE II & ASHRAE II & 45.8\% & 45.8\% & 82.7\% \\
\tool       & ASHRAE II & 16.2\% & 22.3\% & 82.8\% \\
ASHRAE II & \tool       &  2.7\% & 39.7\% & 37.5\% \\
\bottomrule                                             
\end{tabular}%

\end{table}


\section{Discussion}
In the following, we discuss the results of the dataset acquisition, model training, and feature importance analysis, as well as similarities and differences to previous approaches in the field of state recognition. 



\subsection{Feasibility of Thermal Comfort Prediction in Vehicles}
For our RF classifier, an evaluation accuracy of 47.1\% was reached on the seven-point thermal sensation scale. RF models were previously used to predict thermal preference (three-point scale) and thermal state (two-point scale), where accuracies between 76\% (three-point)~\cite{liuPersonalThermalComfort2019} - 94\% (two-point)~\cite{CHAUDHURI2018391} were achieved using physiological signals. The performance of the RF model employed in this work was similar, with a prediction accuracy of 73.8\% for the three-point scale and 67.9\% for the two-point scale. However, it was trained on a larger dataset and used environmental features.
The LSTM-based classifier achieved a prediction accuracy > 59\% on the seven-point scale (see \autoref{tab:performances}). This difference could stem from the inherent mechanism as the RF models were not processed in sequences but on an individual line-by-line basis. Therefore, changes over time could not be incorporated into the model's prediction. This is supported by the improved accuracy when employing data-downsampling. Previous RF models for thermal comfort prediction~\cite{CHAUDHURI2018391, liuPersonalThermalComfort2019} did not employ data-downsampling, as data was already labeled more sparsely in comparison to the thermal comfort dataset that was recorded at 30Hz.

Comparing LSTM and CNN-LSTM, the LSTM model was optimized using ambient temperature, humidity, skin temperature, and radiation as input features, which were identified as the four most relevant features for thermal sensation prediction, while CNN-LSTM model features were selected based on real-world applicability.  The LSTM architecture achieved a higher evaluation accuracy score than the CNN-LSTM. However, radiation temperature and even physiological features such as heart rate or GSR may not be readily available in future vehicles. Furthermore, the meaningfulness of the raw extracted visual features via the ResNet block could be increased by, for instance, applying facial expression or motion detection~\cite{zhangFrownBasedThermalComfort2021}, before concatenation with the remaining features, as this allows the model to predict thermal comfort on more filtered feature representations. Consequently, the LSTM model would not be guaranteed to outperform the CNN-LSTM architecture in a real-world scenario. The achieved accuracy on the dataset (59.1\%) should thus be seen as an initial dataset benchmark.

\subsection{Feature Importance and Real-World Applicability}
Other than previous thermal comfort datasets, the dataset recorded in this thesis includes features from five different input spaces (personal context, external context, physiological signals, visual attributes, and emotional state). Previous datasets for thermal comfort prediction focused on the inclusion of personal and environmental features (see~\cite{ashrae1, ashrae2, us_office}), while some works with self-recorded datasets included physiological features exclusively (e.g.,~\cite{CHAUDHURI2018391, maoThermalComfortEstimation2021}). In the broader field of state recognition, one work by \textit{Bethge et al.} used inputs from the personal, external, visual, and auditory input spaces for emotion recognition in vehicles~\cite{bethge}. Nevertheless, the dataset presented here covers the largest number of relevant input spaces for thermal comfort.\\

Different from the results of other works~\cite{luThermalComfortBasedPersonalized2019, liuPersonalThermalComfort2019}, our correlation analysis indicated that physiological features were only weakly correlated ($\rho$ in [0.04,0.17]) to the gathered labels. One reason may be the sensory hardware used for measuring physiological signals. While other works used more specialized sensors, such as IR cameras~\cite{luThermalComfortBasedPersonalized2019} or temperature probes~\cite{chaudhuriThermalComfortPrediction2017, CHAUDHURI2018391}, in this work, skin temperature was recorded using the Empatica E4 wristband, which introduced outlier rates of up to 13.7\% (see \autoref{sec:raw_dataset}).\\

The strongest predictors based on a comparison of correlation coefficients were the environmental features ambient temperature ($\rho=0.77$), ambient humidity ($\rho=-0.67$), and radiation temperature ($\rho=0.61$). This is to be expected and in line with previous research as environmental factors make up most of the relevant variables in the PMV formula \cite{fanger1970thermal}. Moreover, one previous thermal comfort estimation model demonstrated the importance of specifically ambient temperature by achieving a prediction accuracy of 71\% using only ambient temperature as input~\cite{jungHeatFluxSensing2019}. The CNN-LSTM results suggest that raw visual features cannot improve prediction performance. For the assessment of visual feature importance, it would therefore be necessary to first improve the CNN-LSTM model's design such that more meaningful representations for the visual features are used, as it was previously shown that facial expressions like frowning, captured in RGB frames, show high correlation with thermal comfort~\cite{zhangFrownBasedThermalComfort2021}. In the future, additional sensory devices such as smartwatches and the affiliated physiological signals that they can provide might be replaced by RGB-video-based machine-learning models that enable estimation of heart rate~\cite{wangVisionBasedHeartRate2019} or respiratory rate~\cite{kyrollosNoncontactNeonatalRespiration2021}, which suggests that it is sensible for future thermal comfort and other state recognition datasets to include visual features such as RGB frames.

\subsection{Data Acquisition for State Recognition Datasets}
The data acquisition in this work differed from previous approaches. Firstly, a controlled (low fidelity) climate chamber was employed over 60 minutes. Additionally, instead of questionnaire-based data collection, data were recorded using a data logging application that enabled direct labeling by the participants. The logging differed from previous approaches (e.g., \cite{liuPersonalThermalComfort2019,luThermalComfortBasedPersonalized2019,maoThermalComfortEstimation2021}) as the synchronization of the various sensory inputs, as well as the labeling interface, were centralized in a single application that allows for dense sampling \textit{without post-labeling}. Therefore, we assume that the labeling accuracy and log timing were greatly increased, as participants could report their current state by performing a minimal number of interactions with a numeric keypad.
Nonetheless, the resulting data distribution is likely to differ from potential data distributions found in real-world use cases. For instance, ambient temperature, humidity, heart rate, GSR, and emotions may change more frequently while different tasks (driving, using one's phone, or working) are performed in real-world scenarios. Our dataset can serve as a starting point for automatically determining thermal comfort. Due to the personal thermal comfort zone~\cite{CIUHA2016123, ciuhaThermalComfortZone2017}, which is person-dependent, a calibration of the model during usage seems appropriate. This could be done, for example, via directly asking users (e.g., ``Are you currently feeling cold, comfortable, or hot?'') or by treating the manual adjustment of the air conditioning as input (``want warmer'', ``want cooler'', with the amplitude of change being an indicator of the strength of this desire). 

\subsection{State Recognition For Interaction Design}
In line with work by \citet{stampf2022towards}, our work helps determine the current user's state in an AV. Our dataset has relevance both for manual and especially for automated driving, which is one of the raised points by \citet{stampf2022towards}. With this dataset, we contribute one aspect to enable developers to create a ``digital twin'' which could enable novel interaction design. We envision numerous interaction possibilities here as the uncertainty of the prediction and severity of the deducted action have to be considered. Additionally, there are multiple ways to use these states, for example, by directly adjusting values or asking the user whether they want a value adjusted. These interactions and effects will become highly relevant with ever-better recognition.

\subsection{Limitations and Future Work}\label{ch:limits}
A low-fidelity climatic chamber was built using three (two heaters and one cooler) commercial smart-home AC units. As the minimum temperature during the recordings was 17.1°C, cold sensations could not successfully be induced for all participants, which led to an unbalanced label distribution in the dataset. Additionally, radiation types alter comfort modeling, therefore, the heater types most likely had an impact on the reported thermal comfort~\cite{hirnimpact}. Moreover, physiological signals measured using the Empatica E4 wristband were found to have a high outlier rate (up to 13.7\%) for heart rate and GSR measurements. Additionally, while methods for boredom aversion during the recordings were implemented, we assume that the overall level of boredom in participants remained high due to the length of the recordings. This potential boredom, in turn, may have led to unwanted influences on emotion ratings.
Furthermore, the measurements for body fat and body temperature were taken using commercial-grade measuring tools, likely affecting the accuracy of the recorded values. The recorded dataset also contains a bias due to the repeated heating pattern in all data recordings. Future work should address the dataset bias by adding further data recorded using different heating patterns and a more diverse participant population to provide more balanced data for state recognition. Regarding thermal comfort classification, future architectures should focus on processing visual and environmental features that can be collected reliably in future real-world scenarios.
The comparability to other datasets is also limited because the ASHRAE II \cite{ashrae2} dataset contains international data from singular observations, and our dataset is limited to \anon{Germany}. Finally, the variance in clothing for the participants was low given the short timeframe we used to gather data (3 weeks in total).

\section{Dataset and Code Availability}
%Mark: hab das hier angeschaut: https://www.sciencedirect.com/science/article/abs/pii/S0895435622000646?casa_token=I4sc90c5gOMAAAAA:xjChokm4n1B8hXmv7lQCIEtCDjpM-7hTDeohdyKuDkkfwOBFHz8O8BF8_9jdunCypXnq0esKHg
The code is available at \url{https://anonymous.4open.science/r/autotherm-3CFF/}(currently anonymized). The data will also be made available online after acceptance.



\section{Conclusion}
This work explored machine-learning-based thermal comfort recognition as a relevant target state space for future AV interaction concepts. First, relevant influencing factors on thermal comfort and the affiliated sensory devices were reviewed and filtered based on their level of obtrusiveness and their prevalence in previous thermal comfort studies used in the field of building ergonomics (e.g., \cite{fanger1970thermal, maoThermalComfortEstimation2021, InfluenceOfBodyTemp1999}). Then, a thermal comfort experiment and a data logging application were designed to record a dataset. This dataset includes a total of 31 different features (age, gender, weight, height, body fat, body temperature, clothing, sport, meal timing, tiredness, radiation temperature, ambient temperature, wrist skin temperature, GSR, heart rate, ambient humidity, RGB frames, ten pose key points, metabolic rate, air velocity, and two emotion features). A thermal comfort study with N=21 participants was conducted, which resulted in a user-labeled dataset with over 1.8 million CSV input lines. The dataset was then analyzed regarding feature importance based on correlation coefficients, model-estimated data impurity scores, and model-estimated permutation scores. The four most relevant features for thermal comfort prediction with the recorded data were ambient temperature, radiation temperature, skin temperature, and ambient humidity. Three different classifiers (RF, LSTM, and CNN-LSTM) were implemented and trained using the recorded dataset to assess the feasibility of thermal comfort recognition. A thermal comfort prediction accuracy of 59.1\% was achieved using the implemented LSTM model and the dataset labels given on a seven-point scale. The resulting classifier configuration was used for further experiments employing state forecasting and different label ranges (three-point and two-point scales). The reduction of labeling scales increased prediction performance in all implemented classifiers. The recorded dataset and the data logging application will be made available publicly to encourage further data recording projects for state recognition by providing a template for user-labeled data acquisition. This work provides the first dataset and reference implementation for state-of-the-art thermal comfort prediction. It helps developers and designers to gain a better understanding of their users as well as helps introduce novel automated responses to detected states.




\begin{acks}
We thank all study participants. This work was conducted within the project 'SEMULIN' funded by the Federal Ministry for Economic Affairs and Climate Action (BMWK).
\end{acks}

%%
%% The next two lines define the bibliography style to be used, and
%% the bibliography file.
\bibliographystyle{ACM-Reference-Format}
\bibliography{sample-base}

%%
%% If your work has an appendix, this is the place to put it.
\appendix

\section{Cross-validation of the Recurrent Network Classifier}
\label{sec:cross-validation}
In our experiment from Section~\ref{ssec:RCNN_classifier}, we used data from $16$ participants during the training of our models. Then, we evaluate performance on the data from the remaining $2$ participants. Now, we investigate the ability of our network to generalize to unseen data. Therefore, we adopt cross-validation, repeating the optimization of our classifier. To do so, we randomly sample $16$ participants out of $18$ and keep $2$ for evaluation. We repeat this process until we end up with $20$ randomly drawn training and evaluation splits. Then, we train $20$ classifiers, using the same hyperparameters overall training runs. In \autoref{tab:cross-validation}, we report individual performance measures, as well as the mean and standard deviation for all runs. The mean accuracy of our classifier is $55.7\%$ with a standard deviation of $1.0\%$.
\begin{table}[ht]
\setlength{\tabcolsep}{3.5pt}
\centering
\footnotesize
\caption{Cross-validation results over $20$ training runs using randomly sampled training and evaluation data. In the last two rows, we report the mean and std for all metrics.}
\Description{This table lists the results of the 20 cross-validation runs. The first column starting from the left, includes the run ID. The following three columns list the achieved classification accuracies for the 7-point, 3-point, and 2-point scales. The final two columns list the achieved mean-squared-error and L1 metrics. The mean accuracy of our LSTM classifier on the 7-point scale is 55.7\% with a standard deviation of 1.0\%, while a mean accuracy of 79.1\% with a standard deviation of 1.2\% was achieved on the 3-point scale. On the 2-point scale a mean accuracy of 72.3\% and a standard deviation of 0.5\% was achieved.}\label{tab:cross-validation}
\begin{tabular}{cccccc}
\toprule
    & \multicolumn{3}{c}{Classification} & \multicolumn{2}{c}{Regression} \\
Run & 7-point Acc & 3-point Acc & 2-point Acc & MSE & L1\\
\midrule
$0$   & 57.8\% & 80.5\% & 72.6\% & 0.064 & 0.138\\
$1$   & 56.8\% & 80.4\% & 71.9\% & 0.063 & 0.146\\
$2$   & 56.3\% & 79.8\% & 72.1\% & 0.067 & 0.146\\
$3$   & 54.7\% & 77.1\% & 73.0\% & 0.059 & 0.123\\
$4$   & 55.6\% & 78.6\% & 72.5\% & 0.081 & 0.216\\
$5$   & 56.4\% & 77.4\% & 73.5\% & 0.059 & 0.126\\
$6$   & 54.4\% & 78.1\% & 72.0\% & 0.068 & 0.154\\
$7$   & 54.9\% & 80.5\% & 72.4\% & 0.064 & 0.133\\
$8$   & 54.7\% & 78.0\% & 72.3\% & 0.067 & 0.153\\
$9$   & 57.5\% & 81.3\% & 72.7\% & 0.059 & 0.127\\
$10$  & 55.1\% & 80.3\% & 71.9\% & 0.063 & 0.137\\
$11$  & 55.7\% & 79.0\% & 72.2\% & 0.069 & 0.158\\
$12$  & 54.2\% & 77.2\% & 71.4\% & 0.075 & 0.132\\
$13$  & 55.0\% & 79.5\% & 72.2\% & 0.067 & 0.144\\
$14$  & 55.4\% & 78.6\% & 72.3\% & 0.066 & 0.142\\
$15$  & 55.6\% & 79.3\% & 71.5\% & 0.057 & 0.133\\
$16$  & 55.0\% & 78.6\% & 73.3\% & 0.065 & 0.119\\
$17$  & 56.3\% & 79.3\% & 72.6\% & 0.065 & 0.144\\
$18$  & 56.6\% & 79.8\% & 72.3\% & 0.064 & 0.141\\
$19$  & 55.9\% & 79.1\% & 72.3\% & 0.065 & 0.142\\
\hline
Mean  & 55.7\% & 79.1\% & 72.3\% & 0.065 & 0.143\\
Std   &  1.0\% &  1.2\% &  0.5\% & 0.005 & 0.020\\
\bottomrule                                             
\end{tabular}%

\end{table}


\section{Ambient Temperature}

\begin{figure}[ht]
        \centering
        %\captionsetup{width=.8\linewidth}
        \includegraphics[width=0.5\textwidth]{images/heating_profile.png}
        \caption{\label{fig:heating_curves} Ambient temperature conditions for the full recording duration, grouped by trial ID. The mean heating profile is included (red).}
        \Description{This Figure shows a plot that visualizes the ambient temperature profiles that were achieved over for each participant during the thermal comfort study. The plot shows that a minimum temperature of 17.1°C and a maximum temperature of 33.7°C was reached. The mean temperature profile across all participants is shown in red.}
\end{figure}

\autoref{fig:heating_curves} illustrates the ambient temperature profile that each participant was exposed to. It can be observed that the defined temperature ranges were met in each trial during the recording sessions.


\section{Data Preparation and Augmentation}\label{appendix:data_prep}
Firstly, outliers were defined as outside of the mean with three times the standard deviation. This was also done per label group for the ambient temperature. That means that for a given label, data points with an ambient temperature of $+/- 3*std$ of the mean were excluded. This was done to filter falsely attributed labels. Then, we used one-hot encoding for categorical features and overall data down-sampling for the RF classifier.

For the LSTM and CNN-LSTM models, the same outlier filtering, one-hot-encoding, and down-sampling steps were employed. However, as sequences are expected as input to the LSTM models, a sliding window approach was implemented, where, for every index in the created data frame, a sequence of length $n$ is created, such that, for instance, a window size of $w=30$ would result in the following data frame scheme.

Simple data augmentation for continuous features was also implemented. Gaussian noise sampled from a Gaussian distribution with the parameters $\mu=0.00$ and $\delta=0.30$ was added to the continuous variables through element-wise addition. For image data, a more extensive data augmentation scheme was applied, as RGB frames were recorded at a size of 1920x1080, which is too large for efficient training. Thus, first, a central 1000x1000 crop was extracted and resized to 224x224. The resized image is then randomly rotated up to 5°, randomly flipped horizontally, and then normalized so that all three RGB channels are given as values between 0 and 1

Finally, as the thermal sensation scale has an ordinal scaling level, providing simple integer targets as prevalent in the raw dataset ignores the rank information. Therefore, the labels were changed according to a scheme proposed by \citet{cheng2008neural}. Ordinal labels are transformed into binary vectors of size $k$ (here 7), where $k$ is the number of ranks given on the ordinal scale. The binary vectors are instantiated as zero-vectors and then filled with ones from left to right, depending on the rank of the ordinal label. The resulting label transformations for the thermal sensation scale (seven-point scale) were: %, for example, $-3 \longrightarrow [1,0,0,0,0,0,0]$, $ 0 \longrightarrow [1,1,1,1,0,0,0]$, and $ 3 \longrightarrow [1,1,1,1,1,1,1]$. 


\begin{itemize}[noitemsep]
    \item $-3 \longrightarrow [1,0,0,0,0,0,0]$ 
    \item $-2 \longrightarrow [1,1,0,0,0,0,0]$ 
    \item $-1 \longrightarrow [1,1,1,0,0,0,0]$ 
    \item $ 0 \longrightarrow [1,1,1,1,0,0,0]$
    \item $ 1 \longrightarrow [1,1,1,1,1,0,0]$ 
    \item $ 2 \longrightarrow [1,1,1,1,1,1,0]$ 
    \item $ 3 \longrightarrow [1,1,1,1,1,1,1]$
\end{itemize} 

The resulting binary vectors were used as labels for the LSTM and CNN-LSTM models. Split sizes were predefined, so data from 16 participants were used for training and the remaining data files for validation/testing. 


\section{Confusion Matrices}
\label{sec:confusion_matrices}

\begin{figure}[ht]
    \centering
        \includegraphics[width=0.5\textwidth]{images/rf_cm.pdf}
        \caption{\label{fig:conf_rf_7} Random Forest confusion matrix. \textit{Cold} classes are misclassified entirely, and \textit{Comfortable} classes are rarely classified correctly. Warmer labels are predicted with higher accuracy.}
        \Description{This Figure shows the confusion matrix for the Random Forest classifier. The highest values are mostly located along the matrix diagonal except for the labels 0, -2, and -3, which are more spread around the matrix diagonal. This indicates that cold, cool, and comfortable states were mostly misclassified.}
\end{figure}


\begin{figure}[ht]
    \centering
        \includegraphics[width=0.5\textwidth]{images/cm_lstm.pdf}
        \caption{\label{fig:conf_lstm_7} LSTM confusion matrix. \textit{Comfortable} and \textit{Cold} states are mostly wrongly classified. Warmer labels were classified more reliably.}
           \Description{This Figure shows the confusion matrix for the LSTM classifier. The highest values are mostly located along the matrix diagonal except for the labels 0 and -3, which are more spread around the matrix diagonal. This indicates that cold and comfortable states were often misclassified.}
\end{figure}


\begin{figure}[ht]
    \centering
        \includegraphics[width=0.5\textwidth]{images/cm_rcnn.pdf}
        \caption{\label{fig:conf_rcnn_7} The CNN-LSTM confusion matrix. Most classes apart from \textit{Hot} states are wrongly classified.}
        \Description{This Figure shows the confusion matrix for the CNN-LSTM classifier. The highest values are mostly spread along the matrix diagonal. This indicates that all states were mostly wrongly classified.}
\end{figure}



%\begin{figure*}[ht]
%    %centering
%        \includegraphics[width=1.0\linewidth]{images/matrix_summary.png}
%        \caption{\label{fig:conf_all_reduced} The confusion matrices for all models with the reduced labeling scales. The top row shows the matrices for the 2-point scale, and the bottom row shows the 3-%point scale results. }
%           \Description{tba. once updated}
%\end{figure*}

\begin{figure*}[ht] 
  \begin{subfigure}{0.3\textwidth}
    \centering
    \includegraphics[width=.8\linewidth]{images/rf_3_cm.pdf}
    \label{fig:1}
  \end{subfigure}%
  \begin{subfigure}{0.3\textwidth}
    \centering
    \includegraphics[width=.8\linewidth]{images/cm_3_lstm.pdf}
    \label{fig:2}
  \end{subfigure}
  \begin{subfigure}{0.3\textwidth}\quad
    \centering
    \includegraphics[width=.8\linewidth]{images/cm_3_rcnn.pdf}
    \label{fig:3}
  \end{subfigure}
  \medskip

  \begin{subfigure}{0.3\textwidth}
    \centering
    \includegraphics[width=.8\linewidth]{images/rf_2_cm.pdf}
    \caption{Random Forest}
    \label{fig:4}
  \end{subfigure}
  \begin{subfigure}{0.3\textwidth}
    \centering
    \includegraphics[width=.8\linewidth]{images/cm_2_lstm.pdf}
    \caption{Recurrent Network Classifier}
    \label{fig:5}
  \end{subfigure}
  \begin{subfigure}{0.3\textwidth}
    \centering
    \includegraphics[width=.8\linewidth]{images/cm_2_rcnn.pdf}
    \caption{CNN + RNN Classifier}
    \label{fig:6}
  \end{subfigure}
  \caption{Confusion matrices for three-point and two-point classification.}
  \label{fig:images}
  
  \Description{This Figure shows the confusion matrices for the three-point and two-point classification runs grouped by the employed classifier (Random Forest, RNN classifier, and CNN-RNN classifier). The first column on the left includes the confusion matrices for the Random Forest classifier. The 3-class confusion matrix shows that the classes cool and warm were predicted more reliably (cool with 95.7\% and warm with 84.6\%). The comfortable class was only classified correctly 24.1\% of the time. In the 2-class confusion matrix the uncomfortable class was classified correctly 86.1\% of the time while the comfortable class was classified correctly only 56.8\% of the time.
  
  The second column in the middle includes the confusion matrices for the RNN classifier. The 3-class confusion matrix shows that the classes cool and warm were predicted reliably (cool with 88.2\% and warm with 90.2\%). The comfortable class was classified correctly 45.5\% of the time. In the 2-class confusion matrix the uncomfortable class was classified correctly 64.9\% of the time while the comfortable class was classified correctly only 78.6\% of the time.
  
  The third column on the right includes the confusion matrices for the CNN-RNN classifier. The 3-class confusion matrix shows that the class warm was predicted more reliably (86.2\%) while cool and comfortable classes were predicted less reliably (cool 66.5\% and comfortable 58.7\%). In the 2-class confusion matrix the uncomfortable class was correctly classified 55.0\% of the time while the comfortable class was classified correctly 92.7\% of the time.
  }
\end{figure*}



%%%%%%%%%%%%%%%%%%%%%% CROSS VALIDATION %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure*}[ht]
  \begin{subfigure}{0.23\textwidth}
    \centering
    \includegraphics[width=\linewidth]{images/cm_run_0.pdf}
    \caption*{Run 0}
  \end{subfigure}%
  \begin{subfigure}{0.23\textwidth}
    \centering
    \includegraphics[width=\linewidth]{images/cm_run_1.pdf}
    \caption*{Run 1}
  \end{subfigure}
  \begin{subfigure}{0.23\textwidth}
    \centering
    \includegraphics[width=\linewidth]{images/cm_run_2.pdf}
    \caption*{Run 2}
  \end{subfigure}
  \begin{subfigure}{0.23\textwidth}
    \centering
    \includegraphics[width=\linewidth]{images/cm_run_3.pdf}
    \caption*{Run 3}
  \end{subfigure}
  \medskip

  \begin{subfigure}{0.23\textwidth}
    \centering
    \includegraphics[width=\linewidth]{images/cm_run_4.pdf}
    \caption*{Run 4}
  \end{subfigure}%
  \begin{subfigure}{0.23\textwidth}
    \centering
    \includegraphics[width=\linewidth]{images/cm_run_5.pdf}
    \caption*{Run 5}
  \end{subfigure}
  \begin{subfigure}{0.23\textwidth}
    \centering
    \includegraphics[width=\linewidth]{images/cm_run_6.pdf}
    \caption*{Run 6}
  \end{subfigure}
  \begin{subfigure}{0.23\textwidth}
    \centering
    \includegraphics[width=\linewidth]{images/cm_run_7.pdf}
    \caption*{Run 7}
  \end{subfigure}
  \medskip

  \begin{subfigure}{0.23\textwidth}
    \centering
    \includegraphics[width=\linewidth]{images/cm_run_8.pdf}
    \caption*{Run 8}
  \end{subfigure}%
  \begin{subfigure}{0.23\textwidth}
    \centering
    \includegraphics[width=\linewidth]{images/cm_run_9.pdf}
    \caption*{Run 9}
  \end{subfigure}
  \begin{subfigure}{0.23\textwidth}
    \centering
    \includegraphics[width=\linewidth]{images/cm_run_10.pdf}
    \caption*{Run 10}
  \end{subfigure}
  \begin{subfigure}{0.23\textwidth}
    \centering
    \includegraphics[width=\linewidth]{images/cm_run_11.pdf}
    \caption*{Run 11}
  \end{subfigure}
  \medskip

  \begin{subfigure}{0.23\textwidth}
    \centering
    \includegraphics[width=\linewidth]{images/cm_run_12.pdf}
    \caption*{Run 12}
  \end{subfigure}%
  \begin{subfigure}{0.23\textwidth}
    \centering
    \includegraphics[width=\linewidth]{images/cm_run_13.pdf}
    \caption*{Run 13}
  \end{subfigure}
  \begin{subfigure}{0.23\textwidth}
    \centering
    \includegraphics[width=\linewidth]{images/cm_run_14.pdf}
    \caption*{Run 14}
  \end{subfigure}
  \begin{subfigure}{0.23\textwidth}
    \centering
    \includegraphics[width=\linewidth]{images/cm_run_15.pdf}
    \caption*{Run 15}
  \end{subfigure}
  \medskip

  \begin{subfigure}{0.23\textwidth}
    \centering
    \includegraphics[width=\linewidth]{images/cm_run_16.pdf}
    \caption*{Run 16}
  \end{subfigure}%
  \begin{subfigure}{0.23\textwidth}
    \centering
    \includegraphics[width=\linewidth]{images/cm_run_17.pdf}
    \caption*{Run 17}
  \end{subfigure}
  \begin{subfigure}{0.23\textwidth}
    \centering
    \includegraphics[width=\linewidth]{images/cm_run_18.pdf}
    \caption*{Run 18}
  \end{subfigure}
  \begin{subfigure}{0.23\textwidth}
    \centering
    \includegraphics[width=\linewidth]{images/cm_run_19.pdf}
    \caption*{Run 19}
  \end{subfigure}

  \caption{Crossvalidation confusion matrices.}
  \label{fig:cm_crossvalidation_study}
  \Description{This Figure consists of 20 Subfigures that show the confusion matrices for each cross-validation run. The Subfigures are arranged in a grid with 4 columns and 5 rows where the confusion matrix of run 0 is shown on the top left. The values for all runs are distributed similarly. A clear diagonal can only be seen for the classes comfortable, slightly warm, and hot (lower right corner in each matrix). For the remaining classes (upper left corner in each matrix) there were misclassifications, where the classes cold and cool were mostly misclassified as slightly cool.}
\end{figure*}

%%%%%%%%%%%%%%%%%%%%%% ABLATION STUDY %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure*}[ht]
  \begin{subfigure}{0.23\textwidth}
    \centering
    \includegraphics[width=\linewidth]{images/at_ah_rt_ws.pdf}
    \caption*{AT + AH + RT + WS}
  \end{subfigure}%
  \begin{subfigure}{0.23\textwidth}
    \centering
    \includegraphics[width=\linewidth]{images/at_rt_ws.pdf}
    \caption*{AT + RT + WS}
  \end{subfigure}
  \begin{subfigure}{0.23\textwidth}
    \centering
    \includegraphics[width=\linewidth]{images/at_ah_ws.pdf}
    \caption*{AT + AH + WS}
  \end{subfigure}
  \begin{subfigure}{0.23\textwidth}
    \centering
    \includegraphics[width=\linewidth]{images/gl_at_rt_ws.pdf}
    \caption*{GL + AT + RT + WS}
  \end{subfigure}
  \medskip

  \begin{subfigure}{0.23\textwidth}
    \centering
    \includegraphics[width=\linewidth]{images/gl_at_ah_rt_ws.pdf}
    \caption*{GL + AT + AH + RT + WS}
  \end{subfigure}%
  \begin{subfigure}{0.23\textwidth}
    \centering
    \includegraphics[width=\linewidth]{images/gl_at_ws.pdf}
    \caption*{GL + AT + WS}
  \end{subfigure}
  \begin{subfigure}{0.23\textwidth}
    \centering
    \includegraphics[width=\linewidth]{images/gl_at_rt.pdf}
    \caption*{GL + AT + RT}
  \end{subfigure}
  \begin{subfigure}{0.23\textwidth}
    \centering
    \includegraphics[width=\linewidth]{images/gl_at_ah_ws.pdf}
    \caption*{GL + AT + AH + WS}
  \end{subfigure}
  \medskip

  \begin{subfigure}{0.23\textwidth}
    \centering
    \includegraphics[width=\linewidth]{images/gl_at_ah_rt.pdf}
    \caption*{GL + AT + AH + RT}
  \end{subfigure}%
  \begin{subfigure}{0.23\textwidth}
    \centering
    \includegraphics[width=\linewidth]{images/at_ah_rt.pdf}
    \caption*{AT + AH + RT}
  \end{subfigure}
  \begin{subfigure}{0.23\textwidth}
    \centering
    \includegraphics[width=\linewidth]{images/gl_at_ah.pdf}
    \caption*{GL + AT + AH}
  \end{subfigure}
  \begin{subfigure}{0.23\textwidth}
    \centering
    \includegraphics[width=\linewidth]{images/gl_rt_ws.pdf}
    \caption*{GL + RT + WS}
  \end{subfigure}
  \medskip

  \begin{subfigure}{0.23\textwidth}
    \centering
    \includegraphics[width=\linewidth]{images/ah_rt_ws.pdf}
    \caption*{AH + RT + WS}
  \end{subfigure}%
  \begin{subfigure}{0.23\textwidth}
    \centering
    \includegraphics[width=\linewidth]{images/gl_ah_rt.pdf}
    \caption*{GL + AH + RT}
  \end{subfigure}
  \begin{subfigure}{0.23\textwidth}
    \centering
    \includegraphics[width=\linewidth]{images/gl_ah_rt_ws.pdf}
    \caption*{GL + AH + RT + WS}
  \end{subfigure}
  \begin{subfigure}{0.23\textwidth}
    \centering
    \includegraphics[width=\linewidth]{images/gl_ah_ws.pdf}
    \caption*{GL + AH + WS}
  \end{subfigure}

  \caption{Ablation study confusion matrices.}
  \label{fig:cm_ablation_study}
  \Description{This Figure consists of 16 Subfigures that show the confusion matrices for the ablation study where different feature combinations were tested. The Subfigures are arranged in a grid with 4 columns and 4 rows. The values for all runs are distributed similarly. A clear diagonal can only be seen for the feature combinations, AT+AH+RT+WS (first row, first column), AT+RT+WS (first row, second column), AT+AH+RT (third row, second column).}
\end{figure*}

\begin{figure*}[ht]
    %centering
        \includegraphics[width=0.5\linewidth]{images/pmv_cm.pdf}
        \caption{\label{fig:pmv_conf} The confusion matrix resulting from PMV index calculated labels. It can be seen that all classes were mostly wrongly classified using the PMV index. }
           \Description{This Figure shows the confusion matrix for PMV index estimated labels. The values are more broadly spread around the matrix diagonal. This indicates that all states were mostly wrongly classified.}
\end{figure*}
\vfill\clearpage
\section{Thermal Comfort Study Introductory Slides}\label{sec:introduction}
We provide the slides used to explain the procedure and the data logging interface to participants. Most slides included screenshots from the actual data logger GUI to ensure that explanations given during the introduction were supported by visualizations of the actual logging application.
\includepdf[pages={1-9}]{2_1_study_proc.pdf}


\end{document}
\endinput