\subsection{Disentanglement of Style and Semantic}
\label{appendix:app_disentanglement}

The disentanglement application conducts controllable image-variation, supported by the image-variation flow of VD. Such flow consists of AutoKL, CLIP image encoder, and VD's diffuser with image data layers and image context layers. The core strategy of the disentanglement is to manipulate the 257$\times$768 CLIP image context embedding, which guided the diffusion process via cross-attention. Recall that these embeddings are generated by the visual transformer~\cite{vit}, which begins with one global feature vector followed by 256 local feature vectors corresponding to image local patches. We first split the vector into the single global vector and the following 256 local vectors. We keep the global vector untouched and compute the principal components from the rest of the feature vectors. When manipulating the context embeddings, we notice that the first couple of principal components (\ie the major principal components of the matrix) hold the style information (\ie color, art, stroke styles), and the remaining principal components hold the semantic information (\ie, objects,object locations, identity). Thus, in practice, we generated image variations with style focuses from the guidance of the low-rank context embedding that hosts only major principal components. And we generated image variations with semantic focuses when we removed these major principal components from context embeddings. In Figure~\ref{fig:supp_disentanglement}, we show additional qualitative results, in which we standardize the disentanglement with five levels: a) 0 represents normal image-variation; b) -1 and -2 are semantic-focuses by removing one and two major principal components; c) 1 and 2 are style focuses results corresponding to keeping only 10 and 2 major principal components. 

In practice, we also notice that principal components after order 50 have little effect on results. Therefore, we can speed up the disentanglement PCA by just computing the first 50 principal components and then conducting the manipulation. For the global feature vector, we notice that it mainly serves as a semantic feature that controls object location information. Hence, removing it may negatively impact image structure (see Figure~\ref{fig:supp_disentanglement_gvec}) but may be useful in some art generation cases. We encourage researchers to explore further the low-rank subspace of the CLIP Image embedding for more exciting applications.

\input{supp_figure/suppfig_disentanglement.tex}

\input{supp_figure/suppfig_disentanglement_gvec.tex}

\input{supp_figure/suppfig_mixing.tex}

\input{supp_figure/suppfig_dual_guided.tex}

\subsection{Dual-guided Generation}
\label{appendix:app_dual_guided}

The dual-guided generation for VD is to generate images or sentences through the guidance of both image context and prompt context. As mentioned in the main article, dual-guided tasks, or generally speaking, multi-guided tasks, can be achieved via model-level mixing in which we conduct diffusion steps using one model after another. However, we notice that such an approach may cause issues such as structure distortions and wrong semantics despite doubling the model usage. Unlike ensembling, VD can conduct the dual-guided task via a much deeper level of mixing. As mentioned in the main article Session 3.2, VD's diffuser has three layer groups: global, data, and context. When VD faces a designated data type (\ie images), we only need to forward propagate the shared data layer stream (\ie ResBlocks). On the other hand, when handling dual context, we can select from two options that correspond to two mixing strategies: layer-level or context-level. In layer-level mixing, image and text cross-attention take turns processing the hidden features. In attention-level mixing, both cross-attention layers are included, and their results are mixed together by linear interpolation (See Figure~\ref{fig:supp_mixing}). 

We believe that the success of the dual-guided results relies on the models' ability to resolve the conflict between contexts. An example shows in Figure~\ref{fig:supp_dual_guided} in which we generate an image using a car as image context and a prompt \textit{"a double-decker bus"} as text context. Such a case manually brings in conflict as the car in the image and the double-decker bus described in the prompt has completely different shapes. However, attention-level mixing nicely resolves such conflict because we notice a smooth transition between these two contexts with increasing mixing rates. On the other hand, our results indicate that layer-level mixing slightly underperforms attention-level mixing as the generated vehicle shows some noticeable distortion (see the wheels in the second line). Lastly, we show the results from model-level mixing using two of our baseline models SDv1.4 and SD-variation, which perform the worst among all three methods. Given these results, we conclude that VD is a critical model for dual-guided generation tasks. Its multi-flow multimodal network structure enables attention-level mixing that can effectively resolve conflict and promote good fusions between various contexts.

\subsection{Editable I2T2I}
\label{appendix:app_i2t2i}

As mentioned in the main article Session 4.6, I2T2I is a new image editing approach that converts the original image into text, modifies the text, and then converts it back to the image. Although the principle of such editing is simple, there are many issues that may negatively affect the outputs, thus requiring additional solutions. We summarize these issues as follows: a) The output quality is affected by both image-to-text and text-to-image performance. The failure of either one of the sub-procedures will result in unsatisfactory results. b) Sometimes, direct editing of the generated text could be infeasible, as there is no guarantee that the text contains the descriptions we would like to modify. c) Image-to-text is a process of information distillation, while text-to-image is a process of information creation. Although such properties bring great flexibility in I2T2I editing, they may differ from general users' demands because they may like to keep more content from the reference images.

We design our editable I2T2I application to tackle two out of three issues. The first issue is connected to VD's performances and is thus out of the scope of this application. We resolve the second issue by not directly editing the text but modifying the latent text vectors. Speaking with details, in our editing experiment, we prepare a negative and positive prompt to do the editing. The negative prompt describes image content that needs to be removed, and the positive prompt describes the content to add. When the text latent vector is ready (using image-to-text), before converting it into text, we project it on the normal space of the negative prompt latent vector and sum it up with the positive prompt latent vector. To further highlight the positive prompt, we also compute its CLIP embedding and concatenate it with the modified prompt embedding, and then guide the image generation. Meanwhile, to overcome the third issue, we adopt the ideas from our disentanglement and dual-guided application, in which we compute the style disentangled image context and use it as secondary guidance in the generation process with a 0.66 mixing rate (\ie more focused on text). More results of our editable I2T2I can be found in Figure~\ref{fig:supp_i2t2i}, which shows the great potential of the approach.
