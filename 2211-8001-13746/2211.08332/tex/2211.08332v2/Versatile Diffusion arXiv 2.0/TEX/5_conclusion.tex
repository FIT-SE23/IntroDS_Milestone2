In this article, we proposed a novel diffusion model, Versatile Diffusion, that handles text, image, and variations all in one. Generalized from VD, we also proposed a multi-flow multimodal framework that can be further extended to new tasks and domains. Through experiments and applications, we demonstrate that VD performs well on all supported tasks. Meanwhile, our new multi-flow multimodal framework may prompt future research in a broader area, solving multiple cross-modal tasks with one model and one framework. 

\paragraph{Acknowledgments.} We thank the University of Illinois at Urbana-Champaign, University of Oregon, the Intelligence Advanced Research Projects Activity (IARPA), and Picsart AI Research (PAIR) for their support.