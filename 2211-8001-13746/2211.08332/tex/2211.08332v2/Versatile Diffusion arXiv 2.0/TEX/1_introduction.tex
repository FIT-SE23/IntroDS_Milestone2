Multi-modality is a critical and long-lasting challenge for computer vision and machine learning. In the past decade, researchers have participated and witnessed one of the most rapidly growing periods in vision techniques. With the attributes of deep learning, methods designed for traditional tasks such as classification, detection, segmentation, \etc, have reached near-human level accuracy. Motivated by these results, multimodal research such as ~\cite{mm_zscls0, mm_cls1, mm_cls2, oneformer} primarily focused on discriminative tasks, in which the author demonstrated the effectiveness of joint training with multimodal data. Nevertheless, the research on multimodal models that handle generative tasks of a large scope is quite challenging. It has been demonstrated in GAN research, that those iconic high-performing GAN models~\cite{progressivegan, biggan, stylegan2} focused on specific domains, for instance, faces~\cite{stylegan2, pigan, stylenat}, fonts~\cite{shapemgan, textstylebrush}, or natural scenes~\cite{singan, infinitygan}; and on specific tasks, for instance, inpainting \cite{lama, comodgan, shgan}, super resolution \cite{ledig2017photo}, or image-to-image translation~\cite{cgan, cyclegan}. Thus, most GAN research tackles generative tasks in a divide-and-conquer manner, and the merits of multi-task multimodal models remain uncovered. 

The recent success of diffusion models (DM)~\cite{ddpm, ddim, dalle2, imagen, ldm} has brought new horizons. DMs are likelihood-based models that gradually restore image contents from Gaussian corruptions. DMs have proved to be very effective across many domains and tasks, for instance, unconditional generation~\cite{ddpm, ddim, dm_beat_gan}, density estimation~\cite{dm_vdm}, super-resolution~\cite{sr3}, and text-to-image generation~\cite{glide, dalle2, imagen, ldm}. One reason for DM's success is attributed to the robust training objectives, \ie variational lower-bound~\cite{ddim}, leading to a more robust training procedure than GAN approaches. On the other hand, DM's iterative refinement inference procedure expands the model capability and improves output quality at the cost of more running time. Last but not least, the competitive performance of recent works such as DALL-E2~\cite{dalle2}, Imagen~\cite{imagen}, and Stable Diffusion~\cite{ldm} benefits from the remarkable data collection such as LAION~\cite{laion}, CC12M~\cite{cc12m}, CoYo~\cite{coyo}, etc. While these reasons reveal diffusion models from specific angles, the cornerstone is the success of the diffusion principle on density models~\cite{dm_early0, ddpm, ddim}, from which the actual data distribution can be smoothly restored from Gaussian latent space.

As diffusion models are still actively explored in research, some noticeable disadvantages, such as data hunger and high inference costs, are gradually improved by more efficient structures and schedulers~\cite{ddim, dm_fast0, dm_fast1, dm_fast2, ldm}. Methods such as~\cite{dalle2, imagen, ldm} have concurrently set new state-of-the-art in text-to-image generation, demonstrating that smooth translation in cross-modal latent spaces is achievable. These are by far the boundary of multimodal diffusion research, and all iconic networks and trained and utilized within single tasks. 
Yet, we believe the future direction of multimodal research lies in multi-task models, which is also a crucial step toward general AI. Our motivations are two folds: one is to conduct in-depth studies on cross-modal latent space, pushing the limit of representation learning to a new level. The other is, through multi-task multimodel networks, to prompt research on universal semantics that can be seamlessly transformed across different tasks and domains.
To fulfill our goals, we introduce Versatile Diffusion (VD) along with a multi-flow multimodal framework that can comprehensively solve text, images, variations, \etc, with one unified diffusion model. 
In later sessions, we demonstrate that our VD not only performs well on each supported task but learns the core semantics across modalities and domains. With such new capacities of VD, we further propose novel extensions and downstream applications with remarkable cross-modal performance.

In conclusion, the main contribution of this article are the following:
\vspace{-0.1cm}

\begin{itemize}
    \item We introduce Versatile Diffusion (VD), a multi-flow multimodal diffusion network that solves text, images, and variations in one unified model. Based on VD, we further  introduce a generalized multi-flow multimodal framework in which new tasks and domains can be involved.
    \item Through experiments, we show that VD generates high-quality outputs on all supported tasks, in which VD's text-to-image and image-variation results can better capture semantics in the context, and VD's image-to-text results are creative and illustrative.
    \item Given the multi-flow multimodal property of VD, we introduce novel extentions and applications that may further benefit downstream users engaged in this technology. (\ie disentanglement of styles and semantics, dual guidance generation, and editable I2T2I.)
\end{itemize}