In this session, we will describe VD's training data and settings, show the performance of VD on all supported tasks, and introduce several novel downstream applications empowered by VD. 

\input{figure/figure_fcresblock}

\subsection{Dataset}

We use Laion2B-en~\cite{laion} as VD's training dataset. Laion2B-en is a collection of nearly two billion images with English captions. All images in Laion2B-en come from online sources, and their corresponding captions are auto-generated through web crawling. To have a cleaner dataset, we further filtered the two billion samples with the following criteria: a) image-text CLIP similarity scores above 0.3; b) safety scores (\ie NSWF) below 0.3; c) the probability containing watermark below 0.3; d) image aspect ratios within 0.6 to 1.6667; e) image area above $256^2 \times 0.75$. The remaining samples served as training samples for our VD.

During our experiments, we noticed that the ground truth captions in Laion2B-en are too noisy to train language models. Therefore, we conducted a simple caption cleaning algorithm with the following operations: a) remove all HTTP links, URLs, and email addresses; b) remove HTML syntax; c) remove unnecessary contents included by square or curly brackets; d) remove unnecessary symbols such as dashes, slashes, and underscores; e) remove all kinds of quotes, but keep \textit{'s}. By cleaning these captions, we were able to train VD in a more robust way on image-to-text and text-variation tasks. We did not apply the caption cleaning algorithm when training VD on text-to-image and image-variation tasks.

\input{figure/figure_qcompare}

\subsection{Training}

To demonstrate the proposed multi-flow multimodal framework, we trained VD with three settings: basic, dual-context (DC), and official. VD-basic is an image-variation model with a single-flow. VD-DC is a two-flow model that supports text-to-image and image-variation. Its UNet diffuser contains one stream for data and two streams for context. Lastly, VD-official is a four-flow model that includes two more tasks, \ie image-to-text and text-variation,  and its UNet diffuser has two streams for both data and context. We use the default name VD representing VD-official in the following experiments unless specifically mentioned.

When training our models, we inherited settings from DDPM~\cite{ddpm} and SD~\cite{ldm}, for instance, 1000 diffusion timesteps and the linearly increasing $\beta$ from $8.5e-5$ to $1.2e-2$. The learning rates were set to $1.e-4$ for VD-basic and VD-DC, and were set to $5.e-5$ for VD-official. We used the pre-trained weights from SD checkpoint v1.4~\cite{ldm} for all models. Besides, to best cooperate with the pretrained weights and supported tasks, we set different gradient multipliers for different layers and streams (see Table~\ref{table:gmsetting}). The effective batch size was 2048 for VD-basic and 1024 for VD-DC and VD-official. Our choices on learning rates, batch sizes, and gradient multipliers followed a general rule to roughly balance the gradient updates based on the supported tasks across models. All models were initially trained on resolution 256 for 30 million samples and were further trained on resolution 512 for 6.4 million samples. One may notice that VD's training length was significantly smaller than SDv1.4, which was 500 million samples on resolution 256 and 230 million samples on resolution 512, but VD achieved satisfactory performance on all tasks.

\input{table/table_gmsetting}

\subsection{Performance}

To the best of the our knowledge, this is a first effort to introduce multi-flow multimodal diffusion models across different tasks and modalities. Thus, we set corresponding single-task prior works as our baseline models and compared VD's results with these baselines. Explicitly speaking: we chose SDv1.4~\cite{ldm} as the baseline model for text-to-image, SD-variation~\cite{sd-justin} for image-variation, and BLIP for image-to-text. Meanwhile, we also conducted qualitative comparisons between different VD models, in which VD-DC and VD-official for text-to-image, and all three models for image-variation. SD-variation~\cite{sd-justin} is an alternative version of SD that was specifically finetuned for the image-variation task. We acknowledge that DALLE2~\cite{dalle2} and Imagen~\cite{imagen} also achieved SOTA on these tasks. However, no official code or training detail was publicly released, so we skipped comparing them. For image-to-text (\ie image captioning), we only compare BLIP~\cite{blip} and VD-official since other settings of VD do not support this task. Regarding text-variation, to the best of our knowledge, we don't know any prior works that support the task, thus we only showed VDâ€™s performance in Appendix~\ref{appendix:text_variation}. All other results are shown in Figure~\ref{fig:qcompare}, in which image samples from SD and VD are generated with controlled random seeds for better quality checking. Through the results, we concluded that VD handles all subtasks well. Moreover, our multi-flow structure and multi-task training helped VD capture context semantics and generate outputs more precisely. VD also generated satisfactory image captions with creative and sensational words, while BLIP's captions were short and lack of descriptions.

\vspace{0.3cm}
\subsection{Disentanglement of style and semantic}

One exciting discovery of our VD is that it can enhance or reduce image styles from semantics without further supervision. Such a phenomenon inspires us to explore a novel area where disentanglement between styles and semantics can happen on images with arbitrary contents in arbitrary styles. Recall that prior works such as~\cite{gan_dissect, ganspace} explored similar properties in GAN latent spaces, but their domain of study was restricted to well-aligned data such as faces or churches. To our best knowledge, we are the first group exploring: a) semantic and style disentanglement on natural images without domain specifications; b) semantic and style disentanglement on diffusion models' latent space.

\input{figure/figure_disentanglement}

Figure~\ref{fig:disentanglement} shows the disentanglement using our VD-DC. Recall that VD-DC is the two-way model that handles text-to-image and image-variation. In practice, we notice that both VD-DC and VD-official serve similar disentanglement performance, while VD-basic has slightly decreased results that are likely due to the agnostic of joint semantics between text and image. More technical details and performance analysis regarding
our disentanglement experiments can be found in Appendix~\ref{appendix:app_disentanglement}.

\subsection{Dual-guided generation}

Dual-guided generation is a downstream application that VD naturally supports. Explicitly speaking, VD can generate outputs conditioned on both image and text at the same time. Since diffusion models are iterative refinement approaches that well fit model ensemble techniques, such dual-guided generation may also be conducted by mixing up diffusion steps with single-task models~\cite{dm_morecontrol}. Although such model ensembling made a simple baseline, we notice that their results are unsatisfactory, and their costs are doubled. Figure~\ref{fig:dual_guidance} shows the dual-guided results using SDv1.4~\cite{ldm} (text-to-image model) ensembled with SD-variation~\cite{sd-justin} (image-variation model), from which we notice substantial object distortions and missing semantics. We believe that these problems are due to their model-level mixing, by which intermediate results in the diffusion process are still separately guided. Our VD, however, can guide cross-modal conditionings on a much deeper level: layer-level or attention-level. Such a unique way of mixing attributes to VD's inter-changeable structure, in which data layers can adapt to all streams of context layers. Through experiments, we found that attention-level mixing on VD could maintain correct object structures and harmonize prompt and image contexts, thus yielding the best performance (see Figure~\ref{fig:dual_guidance}). On the other hand, layer-level mixing provided decreased performance but still outperformed model-level mixing by a noticeable margin. More details can be found in Appendix~\ref{appendix:app_dual_guided}.

\input{figure/figure_dual_guidance}

\subsection{Editable I2T2I}

Since VD supports both image-to-text and text-to-image, one application we can do is to edit images from the angle of text prompts by the following steps: a) convert image to text, b) edit text, and c) convert text back to image. We carried out a prototype experiment in which we removed the described contents from the image and then added new contents using such image-text-image (I2T2I) paradigm. Unlike inpainting or other image editing methods that require object locations as input, our I2T2I needs no masks because it can automatically locate and substitute objects following the instruction. Meanwhile,  I2T2I's output images do not match its input images pixel by pixel, resulting from the semantic distillation in image-to-text and the content creation in text-to-image. Figure~\ref{fig:i2t2i} shows the demo of our I2T2I in which old contents inside the image are removed and replaced via prompt editing. To the best of our knowledge, we are the first group to conduct an image editing task combining image-to-text, text editing, and then text-to-image.  More details about our experiments can be found in Appendix~\ref{appendix:app_i2t2i}.

\input{figure/figure_i2t2i}
