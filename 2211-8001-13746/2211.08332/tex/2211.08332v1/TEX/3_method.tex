In this session, we will first revisit the fundamentals of diffusion models~\cite{dm_early0,ddpm}, including the forward-backward processes and training objectives. We will then present our multi-flow multimodal framework and how to expand such framework with new tasks. Finally, we will go through our Versatile Diffusion (VD) with detailed designs.  

\subsection{Diffusion basics}

The forward diffusion process $p(x_T|x_0)$ is a Markov Chain~\cite{ddpm} with $T$ steps that gradually degrade $x_0$ to $x_T$ with random Gaussian noises (Equation~\ref{eq:forward_diffusion}).

\vspace{-0.3cm}
\begin{equation}\begin{gathered}
    q(x_T|x_0) = \prod_{t=1}^T q(x_{t}|x_{t-1}) = \prod_{t=1}^T \mathcal{N}(\sqrt{1-\beta_t}x_{t-1}; \beta_t\mathbf{I})\\
    = \mathcal{N}(\sqrt{\bar{\alpha}_t}x_0; (1-\bar{\alpha}_t\mathbf{I}));\\
    \bar{\alpha}_t =\prod_{t=1}^T\alpha_t; 
    \quad
    \alpha_t = 1-\beta_t
\label{eq:forward_diffusion}
\end{gathered}\end{equation}

Given the forward diffusion process as prior, diffusion models are trained to reverse the process and recover signal $x_0$ back from $x_T$ by removing the added Gaussian noises. This is known as the backward diffusion process, and each step $p_{\theta}(x_{t-1}|x_t)$ is sampled from the Gaussian distribution with network predicted mean $\mu_{\theta}(x_t, t)$ and variance $\Sigma_{\theta}(x_t, t)$, shown as Equation~\ref{eq:backward_diffusion}.

\vspace{-0.1cm}
\begin{equation}\begin{gathered}
    p_{\theta}(x_{t-1}|x_t) = \mathcal{N}(
        \mu_{\theta}(x_t, t),
        \Sigma_{\theta}(x_t, t)
    )
\label{eq:backward_diffusion}
\end{gathered}\end{equation}

The objective function to train a diffusion model is to minimize the variational bound for negative log-likelihood~\cite{ddpm} shown in Equation~\ref{eq:objective}. In practice, many works assume deterministic $\alpha_t$ and $\beta_t$ for step $t$ in Equation~\ref{eq:forward_diffusion}. Given that both forward and backward processes are Gaussian processes, the objective can then be simplified as the variational weighted $l_2$ loss between the ground truth and predicted mean. 

\vspace{-0.3cm}
\begin{equation}\begin{gathered}
    L = \mathbb{E}[-\log p_{\theta}(x_0)] 
    \le \mathbb{E}\left[
        -\log\frac{p_{\theta}(x_{0:T})}{q(x_{1:T}|x_0)}
    \right]
\label{eq:objective}
\end{gathered}\end{equation}

\vspace{0.1cm}
\subsection{Multi-flow multimodal diffusion framework}\label{sec:framework}

The diffusion framework we proposed is a multi-flow network with various types of data as input and context (see Figure~\ref{fig:framework}). A single flow in the framework requires a VAE~\cite{vae} to project data on latent space, a diffuser to diffuse latent vectors, and a context encoder to extract context semantics. Notice that our settings for single flows closely follow the recent Latent Diffusion Model (LDM) and Stable Diffusion (SD)~\cite{ldm}, in which they use autoKL~\cite{vae}, UNet~\cite{unet}, and optional CLIP/BERT~\cite{clip, bert} correspondingly. Our multi-flow multimodal diffusion framework inherits the merits of LDM/SD with its interpretable latent space, modulized structure, and lower computation cost. While LDM and SD are primarily designed as high-performing single-task models, our VD and the proposed framework investigate much broader extensions in which we jointly train multiple flows, each representing a cross-modal task. Our core design is the grouping, sharing, and swapping protocols inside the diffuser network that adapt the framework to all supported tasks and beyond. 

Speaking with details, we categorize all diffuser layers into three groups: global layers, data layers, and context layers. Using SD~\cite{ldm} as a reference, the global layers are time-embedding layers, the data layers are residual blocks, and the context layers are cross-attentions. One may notice that such grouping corresponds to layer functionalities. When processing multiple tasks, global layers are shared across all tasks. Data and context layers contain several streams. Each can be shared or swapped based on the current data and context types. For example, when dealing with text-to-image requests, the diffuser uses image data layers with text context layers. When managing image-variation tasks, it uses image data layers with image context layers. (see Figure~\ref{fig:framework})

\input{figure/figure_framework}

\input{figure/figure_vd}

Training a model under the proposed framework is simple and customizable. We choose a set of supported tasks, compute all corresponding losses, and do regular backward propagation on all losses. When we update the weights, we set a customizable gradient multiplier for each group and stream based on the number of gradient accumulations and the model initializations. 

In addition to our diffuser designs, we propose the following rules for VAE and context encoder: a) The framework is more adaptive with VAEs that map data on smooth and interpretable latent spaces. Encoder-decoder structures (\ie GAN~\cite{gan}) with non-smooth latent spaces may degrade performance significantly. b) To achieve better performance, context encoders should jointly minimize the cross-modal statistical distance (\eg KL-divergence) on all supported content types.

\vspace{0.3cm}
\subsection{Versatile Diffusion}

Versatile Diffusion (VD) is a unified diffusion model for text-to-image, image-variation, image-to-text, and text-variation. Thus VD contains two full streams of VAEs, diffusers, and context encoders to support these tasks.

\textbf{Diffuser}: We use the well-adopted UNet~\cite{unet} with cross attentions~\cite{attn} as the main structure of our diffuser network. As mentioned in Session~\ref{sec:framework}, we group layers into global, data, and context layers, in which both data and context layers have two streams to support image and text. Regarding the image data streams, we follow LDM~\cite{ldm} and use the residual blocks (ResBlock)~\cite{resnet} with progressively decreased spatial dimension and increased channel number. For the text data stream, we propose the novel fully connected residual blocks (FCResBlock) that expand 768-dimensional text latent vectors into a 320-by-4 hidden feature and follow a similar channel-increasing paradigm. We then apply GroupNorms~\cite{groupnorm}, SiLU, and skip connections like a regular ResBlock (see Figure~\ref{fig:fcresblock}). For context groups, both image and context streams adopt cross-attention layers in which content embeddings manipulate data features via projection layers, dot products, and sigmoids. Figure~\ref{fig:vd} shows a graphic explanation of the overall structure of VD.  

\textbf{VAE}: We adopt the same Autoencoder-KL~\cite{ldm} introduced in LDM as our image data VAE. Parallelly, we adopt Optimus~\cite{optimus} as our text data VAE. Optimus consists of a Bert~\cite{bert} text encoder and a GPT2~\cite{gpt2} text decoder, by which it can bidirectionally transform sentences into 768-dimensional normally-distributed latent vectors. Meanwhile, Optimus also shows satisfying VAE properties with its reconstructable and interpretable text latent space. Therefore, we choose Optimus as our text VAE because it well fits the proposed preconditions for our multi-flow multimodal framework (see session~\ref{sec:framework}).

\textbf{Context encoder}: We use both CLIP~\cite{clip} text and image encoders as our context encoders in VD. Unlike LDM and SD, which only use raw text embeddings as context inputs, we use normalized and projected embeddings that minimize the CLIP contrastive loss for both text and image. In our experiments, we noticed that closer embedding spaces between context types help the model converge fast and perform better. Similar conclusions can also be found in DALLE2~\cite{dalle2}, in which they finetune the text-to-image model with extra projection layers to minimize the discrepancy between text and image embeddings for image-variation.
