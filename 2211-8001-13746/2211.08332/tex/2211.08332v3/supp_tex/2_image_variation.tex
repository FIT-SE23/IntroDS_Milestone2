\subsection{Unconditional Guidance}
\label{sec:supp_iv_ug}

We also did an in-depth investigation on the influence of the unconditional guidance (\ie classifier-free guidance) of the image-variation. Recall that such mechanism is first involved in class-guided generation~\cite{ddpm, ldm} and later largely adopted by text-to-image models such as~\cite{ldm, dalle2, imagen}. Here we recap the core math in Equation~\ref{eq:cfree_guide}:

\begin{equation}
    y = y_u + (y_c - y_u) * s,\quad y_u = G(c_{\text{uncond}}),\quad y_c = G(c_{\text{cond}}) 
\label{eq:cfree_guide}
\end{equation}

\noindent in which $y$ is the final output, $s$ is the unconditional guidance scale, $y_u$ is the unconditional output from generator $G$ using unconditional context $c_{\text{uncond}}$, and $y_c$ it the conditional output from $G$ and $c_{\text{cond}}$. For text-to-image, $c_{\text{uncond}}$ are usually set to the text embeddings encoded from empty strings. For image-variation, we have two options: 

\setlist[enumerate,1]{leftmargin=1.5cm, rightmargin=1.5cm}
\begin{enumerate}[label=(\alph*)]
    \item CLIP embeddings of empty images with all zeros
    \vspace{-0.5em}
    \item All-zero embeddings
\end{enumerate}

\noindent As shown in Figure~\ref{fig:supp_oriug_vs_newug}, both methods have pros and cons: Option (a) tends to highlight content and style in the reference image, and thus its results are more art-focused with better color contrast. Option (a) may also yield slightly better-performing disentanglement and dual-context because it sensitively captures details from input contexts and ``magnify" them in the output. While this option sometimes may ``over-react'', which results in unbearably color and structure distortions. Conversely, Option (b) is a more robust solution that performs better on photorealistic inputs and generates outputs closer to reference images with fewer distortions.

\input{supp_figure/suppfig_oriug_vs_newug}

\subsection{Image-Variation with ControlNet}

\input{supp_figure/suppfig_ivctl}

ControlNet~\cite{controlnet} and similar techniques such as~\cite{t2i_adapter, alibaba_composer} have recently proposed a practical image editing solution involving pretrained text-to-images models and adaptive networks. One benefit of ControlNet is once an adaptive network (\ie control network) is set up, it can be easily transferred to other text-to-image models without further effort in training. Such convenience inspires us to combine the same adaptive network strategy with VD's image-variation, forming a new application for prompt-free controllable image generation. 

In Figure~\ref{fig:supp_ivctl}, we show the performance of this new application with canny edge and depth ControlNet. The usage of these ControlNet closely follows text-to-image approaches:

\setlist[enumerate,1]{leftmargin=1.5cm, rightmargin=1.5cm}
\begin{enumerate}[label=(\alph*)]
    \item We first prepare a well-trained image variation model under VD's framework.
    \vspace{-0.5em}
    \item We download the pretrained ControlNets and load them together with VD.
    \vspace{-0.5em}
    \item We then control the image-variation process under the guidance of these ControlNets just like in text-to-image. The sole difference is we don't need any prompts.
\end{enumerate}

One thing to notice is that the image-variation model we use for ControlNet slightly alters from the default version of VD, in which we remove the positional embedding layers from the CLIP image encoder to make it a position-agnostic CLIP (CLIP-PA). We then prune VD, letting the image-variation flow, and finetune it with the new CLIP-PA. The finetuned checkpoint is then a position-agnostic image-varation model. Once we have this new model, we can add ControlNet's adaptive network as text-to-image approach. The final outputs will then be a combination of ControlNet's structure hint and VD's semantic and style.
