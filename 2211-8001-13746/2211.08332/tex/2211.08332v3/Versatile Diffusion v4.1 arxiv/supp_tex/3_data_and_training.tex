\subsection{Laion2B Prompt Cleaning}
\label{sec:supp_prompt_clearning}

As mentioned in the main article Section 4.1, we cleaned text prompts from Laion2B in order to include Optimus VAE for the to-text flows in VD. Our rules of cleaning the prompts are the followings:

\setlist[enumerate,1]{leftmargin=1.5cm}
\begin{enumerate}[label=(\alph*)]
    \item Remove all HTTP links, URLs, and email addresses
    \vspace{-0.5em}
    \item Remove HTML syntax.
    \vspace{-0.5em}
    \item Remove unnecessary contents included by square or curly brackets.
    \vspace{-0.5em}
    \item Remove unnecessary symbols such as dashes, slashes, and underscores.
    \vspace{-0.5em}
    \item Remove all kinds of quotes but keep \textit{'s}.
\end{enumerate}

\noindent By cleaning these captions, we were able to train VD's to-text flows in a more robust way. We did not apply such prompt cleaning when training VD on its to-image flows.

\subsection{Alternative Training}

In Section 4.2, we mentioned that VD's training follows a progressive rule in which we train a single-flow VD, a dual-flow VD, and finally, the four-flow VD in order. Recall that VD's single-flow model is an image-variation model, which means that image-variation is the ``beginning task'' VD first learns. This happened to be the rule we followed in the main paper, but it did not stop other possible ways of training. In fact, training VD can be more flexible. To demonstrate such flexibility, we alternatively trained a VD in which we set text-to-image as the ``beginning task". We compare the final results of this alternative VD model (labeled as VD-Alt) with the paper model (labeled as VD) in Figure~\ref{fig:supp_vd_alt}, in which both yield to similar performance.

\input{supp_figure/suppfig_vd_train_order}

\input{supp_figure/suppfig_vd_alt}

The success of VD-Alt reveals that there may exist many feasible training rules for the multi-flow multimodel diffusion models with $M$ context $N$ outputs. A graphic explanation of the collections of rules is illustrated in Figure~\ref{fig:supp_vd_train_order}. Given that our VD and VD-Alt all yield good performance, we prompt researchers to further explore the flexibility of training VD, which could also be one of the exciting properties of universal AI.

\subsection{Loss Curves}

We reveal VD's train-time loss curves in Figure~\ref{fig:supp_loss}. All experiments were carried out using a single node with 8 A100 GPUs (80G memory). To make the effective batch size matches the batch size we mentioned in the main paper Section 4.2 (\ie 2048, 1024, and 512), we utilized the gradient accumulation technique in which we performed multiple backpropagations with one gradient update. The batch per GPU for single backpropagations is 64 for resolution 256 and 16 for resolution 512. The gradient accumulation loop can then be calculated as:
\vspace{0.5cm}

\begin{equation}
    \text{Gradient Accumulation Loop} = \frac{\text{Effective Batch Size}}{\text{Batch per GPU} \times 8} \quad \eg =4 \text{ for single-flow 256 training}
\end{equation}

\input{supp_figure/suppfig_loss}
