\textbf{Multi-modalities} are unions of information with different forms, including but not limited to vision, text, audio, \etc~\cite{mm_book0, mm_survey0}. Early deep learning work led by Ngiam \etal ~\cite{mm_dl0} learned a fused representation for audio and video. The similar idea was also adopted across vision and text label~\cite{mm_dl0}, and across vision and language~\cite{mm_dl2}. A part of multimodal approaches focused on zero-shot learning, for instance, DiViSE~\cite{mm_zscls0} targeted mapping images on semantic space from which unseen category labels can be predicted. Socher \etal~\cite{mm_zscls1} trained a recognition model with similar ideas in which images were projected on the space of text corpus. \cite{mm_zscls2} shared the same design as DiViSE but was upgraded for a large and noisy dataset. Another set of works~\cite{mm_cls0, mm_cls1,  mm_cls2,  mm_cls3}, focused on increasing classification accuracy via multimodal training: in which~\cite{mm_cls0} and~\cite{mm_cls1} did a simple concatenation on multimodal embeddings; ~\cite{mm_cls2} proposed a gated unit to control the multimodal information flow in the network; ~\cite{mm_cls3} surveyed FastText~\cite{fasttext} with multiple fusion methods on text classification. Meanwhile, multimodal training was also wide-adopted in detection and segmentation~\cite{rcnn,maskrcnn, oneformer}% Started from R-CNN~\cite{rcnn}, series of works structured with shared backbone and multiple heads in order to train and predict class ids, detection boxes~\cite{rcnn, fpn} and object masks~\cite{maskrcnn, oneformer} 
in one shot. Another topic, VQA~\cite{mm_vqa0, mm_vqa1}, conducted cross-modal reasoning that transferred visual concepts into linguistic answers. Methods such as~\cite{mm_vqa2, mm_vqa3} extracted visual concepts into neural symbolics, and ~\cite{mm_vqa4, mm_vqa5} learned additional concept structures and hierarchies.

\textbf{Multimodal generative tasks} involve simultaneous representation learning and generation/synthesis~\cite{mm_survey1}, in which representation networks~\cite{ae, vae, gan, vqvae, wavenet, prnet} with contrastive loss~\cite{clip, cl, mm_cl0, mm_cl1, mm_cl2} played an essential role. Specifically, our model VD adopts VAEs~\cite{vae} and CLIP~\cite{clip} as the latent and context encoders, which are two critical modules for the network. VD also shares the common cross-modal concepts such as domain transfer~\cite{cgan, cyclegan} and joint representation learning~\cite{mm_dl1, mm_gm0, mm_gm1}.

\textbf{Diffusion models} (DM)~\cite{dm_early0, ddpm} consolidate large family of methods including VAEs~\cite{vae, vqvae, vqvae2}, Markov chains~\cite{mcm0, dm_early0, mcm1, mcm2}, and score matching models~\cite{scorem0, scorem1}, \etc. Differ from GAN-based\cite{gan, biggan, stylegan2} and flow-based models~\cite{flow0, flow1}, DM minimizes the lower-bounded likelihoods~\cite{ddpm, scorem0} in backward diffusion passes, rather than exact inverse in flow~\cite{flow0} or conduct adversarial training~\cite{gan}. Among the recent works, DDPM~\cite{ddpm} prompted $\epsilon$-prediction that established a connection between diffusion and score matching models via annealed Langevin dynamics sampling~\cite{dm_early1, scorem0}. DDPM also shows promising results on par with GANs in unconditional generation tasks. Another work, DDIM~\cite{ddim}, proposed an implicit generative model that yields deterministic samples from latent variables. Compared with DDPM, DDIM reduces the cost of sampling without losing quality. Regarding efficiency, FastDPM~\cite{dm_fast0} investigated continuous diffusion steps and generalized DDPM and DDIM with faster sampling schedules. Another work, ~\cite{dm_fast1}, replaced the original fixed sampling scheme with a learnable noise estimation that boosted both speed and quality. ~\cite{dm_fast2} introduced a hieratical structure with progressive increasing dimensions that expedite image generations for DM. Regarding quality, ~\cite{dm_beat_gan} compared GANs with DMs with exhaustive experiments and concluded that DMs outperformed GANs on many image generation tasks. Another work, VDM~\cite{dm_vdm}, introduced a family of DM models that reaches state-of-the-art performance on density estimation benchmarks. Diffwave~\cite{dm_diffwave} and WaveGrad~\cite{dm_wavegrad} show that DM also works well on audio. ~\cite{dm_improved_ddpm} improved DDPM with learnable noise scheduling and hybrid objective, achieving even better sampling quality. 
\cite{dm_morecontrol} introduced semantic diffusion guidance to allow image or language-conditioned synthesis with DDPM.

\textbf{Text-to-image generation}, nowadays a joint effort of multimodal and diffusion research, has drawn lots of attention. Among these recent works, GLIDE~\cite{glide} adopted pretrained language models and the cascaded diffusion structure for text-to-image generation. DALL-E2~\cite{dalle2}, a progressive version from DALL-E~\cite{dalle}, utilized CLIP model~\cite{clip} to generate text embedding and adopted the similar hieratical structure that made 256 text-guided images and then upscaled to 1024. Similarly, Imagen~\cite{imagen} explored multiple text encoders~\cite{bert, t5, clip} with conditional diffusion models and explores the trade-offs between content alignment and fidelity via various weight samplers. LDM~\cite{ldm} introduced a novel direction in which the model diffuses on VAE latent spaces instead of pixel spaces. Such design reduced the resource needed during inference time, and its latter version, SD, has proven to be equally effective in text-to-image generation.

