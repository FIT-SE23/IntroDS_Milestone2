Multi-modality is the ``crown jewel" for achieving universal AI. %In the past decade, researchers have participated and witnessed one of the most rapidly growing periods in vision techniques. 
With the attributes of deep learning, methods designed for traditional tasks such as classification, detection, segmentation, \etc, have reached near-human level accuracy. On top of them, multimodal research such as ~\cite{mm_zscls0, mm_cls1, mm_cls2, oneformer} primarily focused on discriminative tasks of jointly recognizing, matching, or understanding multimodal data. Nevertheless, research on multimodal generative models remains scarce. Previously, the best-performing generative vision models, generative adversarial networks (GAN)~\cite{progressivegan, biggan, stylegan2} merely focus on specific domains (\ie faces~\cite{stylegan2, pigan, stylenat}, fonts~\cite{shapemgan, textstylebrush}, natural scenes~\cite{singan, infinitygan}, \etc); and on specific tasks (inpainting \cite{lama, comodgan, shgan}, super-resolution \cite{ledig2017photo}, image-to-image translation~\cite{cgan, cyclegan}, \etc). %Thus, most GAN research tackles generative tasks in a divide-and-conquer manner, and the effectiveness of multi-task multimodal models remains uncovered. 

The recent success of diffusion models~\cite{ddpm, ddim, dalle2, imagen, ldm} has brought new horizons. Diffusion models are likelihood-based models that gradually restore image contents from Gaussian corruptions. It has proved to be effective in bridging modalities and tasks, for instance, unconditional generation~\cite{ddpm, ddim, dm_beat_gan}, density estimation~\cite{dm_vdm}, super-resolution~\cite{sr3}, and text-to-image generation~\cite{glide, dalle2, imagen, ldm}. The success of diffusion models can be attributed to several aspects. Firstly, their training objectives lead to a more robust training procedure than other approaches like GANs. The iterative refinement inference procedure also expands the model capability at the cost of more running time. Besides, the competitive performance of recent diffusion models such as DALL-E2~\cite{dalle2}, Imagen~\cite{imagen}, and Stable Diffusion~\cite{ldm} benefits from the remarkable data collection such as LAION~\cite{laion}, CC12M~\cite{cc12m}, COYO~\cite{coyo}, etc. %While these reasons only reveal diffusion models from specific angles, the core success of the diffusion principle comes from its density model theory ~\cite{dm_early0, ddpm, ddim}, from which the actual data distribution can be smoothly restored from Gaussian latent space.
The disadvantages of earlier diffusion models, such as the data hunger and high inference costs, are gradually alleviated by more efficient structures and schedulers~\cite{ddim, dm_fast0, dm_fast1, dm_fast2, ldm}. Diffusion-based text-to-image methods~\cite{dalle2, imagen, ldm} arguably set new state-of-the-art for multi-modal generative AI. However, those works by far almost exclusively hinge on single-flow diffusion pipelines (illustrated in Section 3); and meanwhile, most of them are trained and evaluated on a single specialized generation task (e.g., text to image) despite being cross-modality. 

% These are by far the boundary of diffusion research. However and the community remains focused on methods trained and evaluated on single tasks. 

% existing single-flow diffusion pipeline


\textit{What is the next move forward, then?} We believe in the central role of multimodal, multi-task models in universal AI, and we consider diffusion models to be a promising workhorse to enable so. 
%Yet, we believe the future direction of multimodal research lies in multi-task models, which is also a crucial step toward universal AI. 
%Our goals are twofold: One is to conduct in-depth research on cross-modal latent space, pushing the limit of representation learning to a new level; The other is, through multi-task multimodel networks, to prompt research on universal models that can naturally  support different tasks and modalities.
To fulfill our goal, we proposed \textit{Versatile Diffusion} (\textbf{VD}) that comprehensively solves text, images, and variations within one unified generative model. The \textit{key underlying technique} is a novel multi-flow diffusion framework, that generalizes existing single-flow diffusion pipelines to \textit{handle multiple modalities and tasks simultaneously} while effectively sharing information across them. Thanks to the \textit{larger capacity} as well as capturing crossmodal semantics, VD not only performs well on the aforementioned supported tasks but notably derives many new capabilities including semantic-style disentanglement, cross-modal dual context or multi-context generation (blending), leading to remarkable advances of \textit{empirical performance} for multi-modal generative AI. Our main contributions are summarized in the following:
%as it outperforms the baselines. Moreover, we extendedly propose novel extensions and downstream applications with remarkable performance that better utilize the large capacity of VD on cross-modality.
%In conclusion, the main contributions of this article are the followings:
\vspace{-0.1cm}

\begin{itemize}
    \item We introduce \textit{Versatile Diffusion} (\textbf{VD}), a multimodal, multi-task diffusion network that adopts a novel generalized multi-flow pipeline, unlike existing single-flow diffusion models.
    \vspace{-0.2cm}
    \item VD solves multiple modalities and tasks in one unified model, including image generation (text-to-image, image-variation), and text generation (image-to-text, text-variation). Through comprehensive experiments, we show that VD outperforms the baselines via scores and quality. For example, VD's high-quality text-to-image and image-variation results demonstrate that it indeed better captures the context semantics.
    \vspace{-0.2cm}
    \item The unique multi-flow multimodal property of VD enables more novel derivative tasks, that may further facilitate downstream users engaged in this technology, including the semantic-style disentanglement, dual-context and multi-context blending, \etc.
\end{itemize}