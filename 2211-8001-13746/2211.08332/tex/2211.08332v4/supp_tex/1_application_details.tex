\subsection{Disentanglement of Style and Semantic}
\label{sec:supp_disentanglement}

The disentanglement application conducts controllable image-variation, supported by the image-variation flow of VD. Such flow consists of AutoKL, CLIP image encoder, and VD's diffuser with image data layers and image context layers. The core strategy of the disentanglement is to manipulate the 257$\times$768 CLIP image context embedding, which guided the diffusion process via cross-attention. Recall that these embeddings are generated by the visual transformer~\cite{vit}, which begins with one global feature vector followed by 256 local feature vectors corresponding to image local patches. We first split the vector into the single global vector and the following 256 local vectors. We keep the global vector untouched and compute the principal components from the rest of the feature vectors. When manipulating the context embeddings, we notice that the first couple of principal components (\ie the major principal components of the matrix) hold the style information (\ie color, art, stroke styles), and the remaining principal components hold the semantic information (\ie, objects, object locations, identity). Thus, in practice, we generated image variations with style focuses from the guidance of the low-rank context embedding that hosts only major principal components. And we generated image variations with semantic focuses when we removed these major principal components from context embeddings. In Figure~\ref{fig:supp_disentanglement}, we show additional qualitative results, in which we standardize the disentanglement with five levels: 

\setlist[enumerate,1]{leftmargin=1.5cm, rightmargin=1.5cm}
\begin{enumerate}[label=(\alph*)]
    \item 0 represents normal image-variation.
    \vspace{-0.5em}
    \item -1 and -2 are semantic-focused by removing one and two major principal components.
    \vspace{-0.5em}
    \item 1 and 2 are style focuses results corresponding to keeping only 10 and 2 major principal component.
\end{enumerate}

In practice, we also notice that principal components after order 50 have little effect on results. Therefore, we can speed up the disentanglement PCA by just computing the first 50 principal components and then conducting the manipulation. For the global feature vector, we notice that it mainly serves as a semantic feature that controls object location information. Hence, removing it may negatively impact image structure (see Figure~\ref{fig:supp_disentanglement_gvec}) but may be useful in some art generation cases. We encourage researchers to explore further the low-rank subspace of the CLIP Image embedding for more exciting applications.

\input{supp_figure/suppfig_disentanglement.tex}

\input{supp_figure/suppfig_disentanglement_gvec.tex}

\subsection{Dual-context Blender}
\label{sec:supp_dcg}

The dual-context blender for VD is to generate images through the guidance of one image and one text prompt. Theoretically, such an application can also be used to create new text/sentences, but the results are less exciting than creating new images. As mentioned in the main article, the dual-context blender, or the multi-context blender, can be achieved by ensembling two models in which we mix the diffusion steps from one model after another (type A), or weighted sum up both models' outputs (type B). However, in practice, we notice that such approaches may cause structure distortions and highlight wrong semantics despite doubling the model usage. Unlike these simple ensembling methods, VD can carry out this task via a much deeper level of mixing due to its multi-flow multimodal framework. As mentioned in the main article Section 3.2, our framework has three layer groups: global, data, and context. When generating images, features diffuse through the shared data layers (\ie ResBlocks), and then mix up via different context layers with two options: layer-level mixing or attention-level mixing. In layer-level mixing, we diffuse features through different context layers (\ie cross-attention) that follow a preset schedule. For example, we diffuse features through one image cross-attention, then a text cross-attention, \etc. In attention-level mixing, both immediate features after context layers are included via a weighted sum and then passed to the network's next block (See Figure~\ref{fig:supp_mixing}). 

\input{supp_figure/suppfig_mixing.tex}

We believe that the success of the dual-context blender heavily relies on VD's multi-flow design that can merge contexts in a harmonized way. An example shows in Figure~\ref{fig:supp_dual_guided} in which we generate an image using \textit{a car} as image context and a prompt \textit{a double-decker bus} as text context. Such a case manually brings challenges in mixing, since a Benz car in the image and a double-decker bus described in the prompt have completely different shapes. However, attention-level mixing nicely resolves such conflict and we notice a smooth transition between these two contexts with increasing mixing rates. On the other hand, our results indicate that layer-level mixing slightly underperforms attention-level mixing as the generated vehicle shows some noticeable distortion (see the wheels in the second line). Lastly, we show the results from model-level mixing using two of our baseline models SDv1.4 and SD-variation, which perform the worst among all three methods. Given these results, we conclude that VD is critical for the success of our dual-context blender tasks, in which its multi-flow multimodal network framework is the key to effectively resolving potential conflict and merging various contexts.

\input{supp_figure/suppfig_dual_guided.tex}

\subsection{Multi-context Blender with Optional Masks}
\label{sec:supp_mcg}

Multi-context blender is an extension of our dual-context blender with the following changes: 

\setlist[enumerate,1]{leftmargin=1.5cm, rightmargin=1.5cm}
\begin{enumerate}[label=(\alph*)]
    \item It takes more than one image as a concatenated context of image type.
    \vspace{-0.5em}
    \item It accepts additional scale control on each of the input images.
    \vspace{-0.5em}
    \item It allows adding individual masks to precisely control the generated output based on reference images.
\end{enumerate}

Adding an extra image as reference context is actually simpler than adding an extra context type. Specifically for VD, context from multiple images can be concatenated, forming a more extended sequence of context embedding that later serves as input to content layers. For example, context embedding one image is a $257\times 768$ embedding in which $257$ is the number of embedding vectors and $768$ is the embedding channel. Context embedding for two images is simply a concatenated feature of two images embedding along the number dimension, making it $514\times 768$, and so-on-so-forth for more images. Therefore, such operations do not alter any mixing strategies that we used in the dual-context blender.  

To make more precise control of images used in our multi-context blender, we involved two controllable parameters: image scales and image masks. Image scales are simple multipliers associated with underlining image context embeddings, while image masks involve more complex designs. We notice that a naive solution to replace contents in masks with zeros may confuse the model of generating images with black patches. Thus we altered the CLIP network, in which the raw image features after the first convolution projection of ViT~\cite{vit} and the input positional encodings are filled with zeros according to masks before inputting the transformers. As a result, we successfully involved scales and masks in our blender application. More results can be found in Figure~\ref{fig:supp_more_tcg}.

\input{supp_figure/suppfig_moretcg}


\subsection{Editable I2T2I}
\label{sec:supp_i2t2i}

Since VD supports both image-to-text and text-to-image, one heuristic image editing approach we can do is to edit images with the following steps: \textbf{(a)} \textit{convert image to text}, \textbf{(b)} \textit{edit text}, and \textbf{(c)} \textit{convert text back to the image}. We named this approach \textit{image-to-text-to-image} \textbf{(I2T2I)}. Although the principle of I2T2I is simple, we notice that in practice, many issues may negatively affect the outputs, making them less robust than expected. 

Unlike inpainting or multi-context blending, I2T2I requires no object masks because
one design goal of I2T2I is to let it automatically locate and substitute objects following the prompt instruction. Meanwhile, I2T2Iâ€™s output images do not match its input images pixel by pixel, which is a result of semantic distillation and content creation. Figure~\ref{fig:supp_i2t2i} shows the prototype results of our I2T2I in which old contents inside the image are removed and re-placed via prompt editing. To the best of our knowledge, this is the first attempt at creating and editing images by combining image-to-text, text editing, and then text-to-image. Yet we notice the following issues that may dramatically decrease the performance: 

\setlist[enumerate,1]{leftmargin=1.5cm, rightmargin=1.5cm}
\begin{enumerate}[label=(\alph*)]
    \item The output quality is affected by both image-to-text and text-to-image performance. The failure of either one of the sub-procedures will result in unsatisfactory results.
    \vspace{-0.5em}
    \item Sometimes, direct editing of the generated text could be infeasible, as there is no guarantee that the text contains the descriptions we would like to modify.
    \vspace{-0.5em}
    \item Image-to-text is a process of information distillation, while text-to-image is a process of information creation. Although such properties bring great flexibility in I2T2I editing, they may differ from general users' demands because they may like to keep more content from the reference images.
\end{enumerate}

To overcome these issues, we tackle these issues with the following solution. Instead of editing the text directly, we actually modify the latent text vectors as a solution to b) issue. Speaking with details, in our editing experiment, we prepare a negative and positive prompt to do the editing. The negative prompt describes image content that needs to be removed, and the positive prompt describes the content to add. When the text latent vector is ready (using image-to-text), before converting it into text, we project it on the normal space of the negative prompt latent vector and sum it up with the positive prompt latent vector. To further strengthen the positive prompt, we also compute its CLIP embedding and concatenate it with the modified prompt embedding, and then guide the image generation. Meanwhile, we adopt the ideas from our disentanglement and dual-context blender, in which we compute the style disentangled image context and use it as secondary guidance in the generation process with a 0.66 mixing rate. The final performance of I2T2I can be found in Figure~\ref{fig:supp_i2t2i}.

\input{supp_figure/suppfig_i2t2i.tex}
