\section{ViT Hardware Accelerator Design with Adaptive Token Pruning} \label{sec:hardware_design}

The hardware architecture of ViT accelerators in \M~is illustrated in Fig.~\ref{fig:hardware}, including the accelerator design for pruned ViTs with token selector and the computation flow in the General Matrix Multiply (GEMM) engine. Besides the LayerNorm layer (less time consuming but more complex to implement on the FPGA) that is left on the ARM CPU, all other components in HeatVit are implemented on the FPGA. 

% Each ViT layer $j$ of \ding{172}$\sim$\ding{177} (Table~\ref{tab:complexity}) performs one or $h$ groups of matrix multiplications, with both the input and output having the same number of tokens (namely the sequence length) of $N$. Since we perform token pruning to reduce the token number, the specific $N$ value in the layer $j$ after pruning is denoted by $N_j$ ($N_j \leq N$). \py{Generally for hardware implementations, we represent the input size as $N_j \times D_i$, the weight size as $D_i \times D_o$, and the output size as $N_j \times D_o$, where $D_i$ or $D_o$ can be $D_{ch}$, $D_{attn}$, and $D_{ch}$ and $4D_{fc}$ in the FNN module, and $N$ in the both MAS and FFN modules.}

\subsection{Challenges}
To implement ViT FPGA accelerator with our dynamic token pruning, we address the following challenges.
(i) The token selector module should be implemented with minimal hardware overhead by adding miniature control logic and reusing existing hardware for the backbone ViT.
(ii) The GEMM loop tiling should accommodate an additional tiling dimension due to multi-head parallelism.
(iii) ViTs use more nonlinear operations than CNNs, and we need to refine these operations for more aggressive quantization and efficient hardware implementations without losing accuracy.

% \vspace{-5pt}
\subsection{ViT Accelerator with Dynamic Token Selection}
% \vspace{-5pt}
As displayed in Fig.~\ref{fig:AccHWwTokenSelector}, the input (tokens) and weight of each ViT layer are loaded from the off-chip DDR memory to the on-chip buffers, and processed by the GEMM engine which we will implement motivated by~\cite{fox2019training,lizh2022logic}.
Outputs are further processed with the activation function (Softmax or GELU), and then stored from the on-chip output buffer back to the off-chip memory.
Double buffering will be applied to overlap data transfer time with computation.
The token selector for pruning consists of FC layers and GELU activations, which also exist in the original ViTs, and thus can be managed by the same computation engine with negligible hardware overhead (Section~\ref{sec:token_selection}). Note LayerNorm is executed on the CPU.
% The token selection procedure is elaborated in Section~\ref{sec:token_selection}.


\subsubsection{Loop Tiling in GEMM}
\label{sec:loop_GEMM}
% Fig.~\ref{fig:gemm_tiling} shows the loop tiling technique~\cite{zhang2018caffeine} in the GEMM engine, due to the large size of ViT models.
% Tiling is applied to the dimensions with size of $D_i$ and $D_o$.
% The tiling sizes are respectively $T_i \cdot H$ and $T_o$. The concurrency of MAC computations in matrix multiplications requires pipelining and unrolling of the loops, as well as array partitioning of the buffers. The parallelism factors to be determined include $T_i$, $T_o$, and $T_h$ that is set for the $H$ attention heads.
% The total degree of computation parallelism is therefore $T_i \cdot T_o \cdot T_h$.
% Specifically for the attention computations ($\mathbf{Q} \times \mathbf{K}^\mathrm{T}$ in \ding{173} and $\mathbf{QK}^\mathrm{T} \times \mathbf{V}$ in \ding{174}), the input and output are both split into $H$ groups each corresponding to a head. On the FPGA design, we use a general buffer for the output with $H$ groups of data. An input signal is used to indicate whether the current computations are attention-related. The attention results from \ding{173} and \ding{174} are kept in $H$ groups for multiple heads, while the results of other layers (\ding{172}, \ding{175}$\sim$\ding{177}) with FC computations are accumulated across the $H$ groups.

We generalize loop tiling \cite{zhang2018caffeine} for GEMM engine to deal with relatively large ViT layers.
The concurrency of MAC operations in matrix multiplications requires pipelining and loop unrolling, as well as array partitioning of buffers. 
Fig.~\ref{fig:gemm_tiling} shows detailed computation flow in GEMM loop tiling for ViTs accommodating an additional tiling dimension from multi-head mechanism.
Given a ViT layer $j$ from \ding{172}$\sim$\ding{177} in Table~\ref{tab:complexity} that performs one or $h$ matrix multiplications with its $N_j$ tokens after pruning, we represent input size as $N_j \times D_i$, weight matrix size as $D_i \times D_o$, and  output size as $N_j \times D_o$, where $D_i$  denotes $D_{ch}$, $D_{attn}$, or $N$ in the MHSA module, and $D_{ch}$ or $4D_{fc}$ in the MLP module.
Tiling is applied to $D_i$ and $D_o$ dimensions, with tiling sizes  {$T_i$} and $T_o$, respectively. 
For attention computations ($\mathbf{Q} \times \mathbf{K}^\mathrm{T}$ in \ding{173} and $\mathbf{QK}^\mathrm{T} \times \mathbf{V}$ in \ding{174}), both input and output are split into $h$ groups. 
A control signal is used to indicate whether current computations are attention-related. 
Results from \ding{173} and \ding{174} are kept in $h$ groups, while results from other layers are accumulated.
To improve throughput, we optimize parallelism factors including $T_i$, $T_o$, and $T_h$ (an {additional tiling dimension for} $h$ heads). 
Therefore, we will conduct comprehensive FPGA resource modeling for available computing and on-chip memory resources.
\begin{figure}[tb]
\centering
\subfigure[ViT hardware accelerator architecture supporting dynamic token selection.
]{
\label{fig:AccHWwTokenSelector}
\begin{minipage}{0.5\textwidth}
\centering
\includegraphics[width=0.9\textwidth]{Figs/DynamicViT.pdf}
\vspace{6pt} 
\end{minipage}
}
\hfill
\subfigure[GEMM loop tiling with an additional tiling from multi-head mechanism.]{
\label{fig:gemm_tiling}
\begin{minipage}{0.5\textwidth}
\centering
\includegraphics[width=0.9\textwidth]{Figs/HeatViT_FPGA.pdf}
\vspace{6pt}
\end{minipage}
}
\vspace{-10pt}
\caption{Hardware architecture of ViT accelerator in \M.}
\label{fig:hardware}
\vspace{-0.4cm}
\end{figure}


\subsubsection{Throughput and Resource Utilization Analysis}

The inference throughput (FPS) of ViT inference is determined by the parallelism factors $T_i$, $T_o$ and $T_h$, which are bounded by the number of available computation (mainly DSPs) and on-chip memory (BRAMs) resources.
The inference is executed layer-by-layer, and the FPS can be inferred through dividing the total number of clock cycles required to process all the ViT layers with the working frequency on FPGAs.
The throughput and resource utilization analysis of ViT accelerators is similar to that of CNN accelerators~\cite{sun2022film-qnn}, except that the head dimension in ViTs needs to be additionally considered for parallelism.
Since the layers in token selectors can be managed by the same GEMM engine as for the existing ViT layers, this analysis also applies to token selectors.

% \begin{wrapfigure}{r}{0.5\textwidth}
%     \centering
%     \vspace{-4pt}
%     \subfigure[ViT hardware accelerator architecture supporting dynamic token selection.]{
%         \label{fig:AccHWwTokenSelector}
%         \begin{minipage}{0.5\textwidth}
%         \centering
%         \includegraphics[width=0.75\textwidth]{Figs/DynamicViT.pdf}
%         \end{minipage}
%     }
% %\hfill
%     \subfigure[{GEMM loop tiling with an additional tiling dimension from multi-head mechanism.}]{
%         \label{fig:gemm_tiling}
%         \begin{minipage}{0.5\textwidth}
%         \centering
%         \includegraphics[width=0.99\textwidth]{Figs/HeatViT_FPGA.pdf}
%         \end{minipage}
% }
%     \vspace{-12pt}
%     \caption{{ViT FPGA accelerator.}}
%     \label{fig:ViTHWoverviewNew}
%     \vspace{-12pt}
% \end{wrapfigure}

% \vspace{-5pt}
\subsection{Token Selection Flow}
\label{sec:token_selection}
% \vspace{-5pt}
Token selection contains token classification to determine informative and non-informative tokens using GumbelSoftmax with a threshold value (usually 0.5), and token packaging to average the non-informative tokens to one that is then consolidated into the informative tokens.
The pruned token (input) sequences with sparsity will be reorganized as dense ones, eliminating hardware overhead for indexing.
Fig.~\ref{fig:token_selector} describes the three steps to implement token selection in our ViT hardware accelerator: (1) calculating the exponent for each token $x_i$ and the summation $Sum$ of all these exponents;
(2) dividing each exponent by the sum and classifying the corresponding token as informative or not according to a threshold; and
(3) if the token is informative, concatenating it to the informative token sequence, otherwise adding it to a temporary token $Tmp$.
Finally, $Tmp$ is averaged and concatenated to the informative token sequence.

\begin{figure}[tb]
\centering
\includegraphics[width=0.65\columnwidth]{Figs/control_flow_v2.pdf}
\vspace{-5pt}
\caption{Token selection flow.}
\label{fig:token_selector}
\vspace{-0.4cm}
\end{figure}

% \vspace{-5pt}
\subsection{Polynomial Approximation of Nonlinear Functions}
\label{sec:approx}
% \vspace{-5pt}
ViT models contain nonlinear functions including GELU, Softmax, and Sigmoid.
%, which are defined as
%\vspace{-0.1in}
% \begin{equation}\label{eq:gelu_orig}
%     \mathrm{GELU}(x) \! = \! \frac{x}{2} \! \left[1 + \mathrm{erf} \! \left( \frac{x}{\sqrt2} \right) \right], \text{where } \mathrm{erf}(x) \! = \! \frac{2}{\sqrt\pi} \! \int_{0}^{x} \! e^{-t^2} dt,
% \end{equation}
% \begin{equation}\label{eq:softmax_orig}
%     \mathrm{Softmax}(\mathbf{x})_i = \frac{\exp(x_i)}{\sum_{j=1}^{k} \exp(x_j)} \text{, where } \mathbf{x} = [x_1,...,x_k],
% \end{equation}
% \begin{equation}\label{eq:sigmoid_orig}
%     \mathrm{Sigmoid}(x) = \frac{1}{1+e^{-x}}.
% \end{equation}
(i) Some nonlinear operations inside those functions, e.g., exponential function $\mathrm{exp}(x)$ and error function $\mathrm{erf}(x)$ consume large amounts of computing resources when implemented with the built-in Xilinx Vitis HLS math library \cite{vitis},  and thus incurring difficulty for hardware acceleration (Table~\ref{tab:nonlinearFunc_comp}).  
(ii) To apply more aggressive quantization than CNN/RNN models, we need to add the regularization effect on quantization error to these approximate operations.
Inspired by~\cite{gelu_softmax_approx}, we propose to explore algorithm-level polynomial approximation to implement GELU and Softmax functions, through which we introduce $\delta_1$ and $\delta_2$ (both $<$1) to control the regularization effect.
Since the Sigmoid function is only present inside token selectors (a small number), we do not introduce a regularization effect for it.

The $\mathrm{erf}(x)$ function is approximated using a second-order polynomial as,
\begin{equation}\label{eq:erf_polynomial}
    L_\mathrm{erf}(x) = \mathrm{sign}(x)\cdot\delta_1\cdot[a(\mathrm{clip}(|x|, \max=-b) + b)^2 + 1],
\end{equation}
with the constants $a=-0.2888$, $b=-1.769$ and $\delta_1<1$.

The GELU function is then expressed as
\begin{equation}\label{eq:gelu_approximation}
    \mathrm{GELU_{aprx}}(x) = \frac{x}{2} \left[ 1 + L_\mathrm{erf} \left(\frac{x}{\sqrt{2}} \right) \right],
\end{equation}

The Softmax function is approximated as
\begin{equation}\label{eq:softmax_approximation}
    \mathrm{Softmax_{aprx}}(\mathbf{x}_i) = \frac{\delta_2\exp (\tilde{x}_{i})}{\sum_{j=1}^{N}\exp (\tilde{x}_{j})},
\end{equation}
with $\tilde{x}_{i} = x_{i} - x_{\max}$, $x_{\max} = \max_{i}(x_i)$ and $\delta_2<1$. This subtraction ensures the numerical stability during the approximation calculation, and all inputs can be decomposed as $\tilde{x} = (-\ln2)z + p$, where $z$ is a non-negative integer and $p$ is a real number in $(-\ln2,0]$.
$\exp(\tilde{x})$ can then be calculated as $\exp(p)>>z$,
% \begin{equation}
%     \exp(\tilde{x}) = \exp(p) >> z,
% \end{equation}
where $z = \left \lfloor -\tilde{x}/\ln2 \right \rfloor$, $p = \tilde{x} + z\ln2$, and $\exp(p)$ is approximated as
\begin{equation}\label{eq:exp_polynomial}
    \exp(p) = 0.3585(p + 1.353)^2 + 0.344.
\end{equation}
Both $\delta_1$ and $\delta_2$ are regularization value ($<1$) on quantization error, which can be constant ($\delta_1$=0.5, $\delta_2$=0.5 in our case).

For the Sigmoid function, we adopt the piece-wise linear approximation (PLAN) from~\cite{sigmoid_approx}.
% \begin{equation}\label{eq:sigmoid_approximation}
% % \small
%     \mathrm{Sigmoid_{aprx}}(x) \! = \!
%     \left\{\begin{matrix}
%     1, & |x|\geq 5.0\\ 
%     0.03125 \! \cdot \! |x| \! + \! 0.84375, \!\!\!\! & 2.375 \! \leq \! |x| \! < \! 5.0\\ 
%     0.125 \cdot |x| + 0.625, & 1.0\leq|x|< 2.375\\ 
%     0.25 \cdot |x| + 0.5, & 0\leq|x|< 1    \end{matrix}\right. .
% \end{equation}

\begin{table}[htb]
\centering
% \small
\tabcolsep 3pt
%\vspace{-0.15in}
\caption{Resource utilization for nonlinear functions between original (Orig.) and approximation (Aprx.) implementations.}
\vspace{-0.1in}
\label{tab:nonlinearFunc_comp}

\begin{tabular}{c|cc|cc|cc}
\toprule
\multirow{2}{*}{} & \multicolumn{2}{c|}{GELU} & \multicolumn{2}{c|}{Sigmoid} & \multicolumn{2}{c}{Softmax} \\
\cline{2-7}
 &
  Aprx. &
  Orig. &
  Aprx. &
  Orig. &
  Aprx. &
  Orig. \\
\midrule
FF                & 334        & 191116       & 1015          & 2334         & 1939         & 2464         \\
LUT               & 438        & 160909       & 1512          & 2333         & 2364         & 2476         \\
DSP               & 4          & 139          & 0             & 3            & 2            & 3            \\ \bottomrule
\end{tabular}
\vspace{-0.1in}
\end{table}

Table~\ref{tab:nonlinearFunc_comp} compares the resource utilization for these nonlinear functions between the original implementations using the built-in Xilinx Vitis HLS math library and our proposed implementations.
Our methods are more resource efficient than those using the HLS math library, with 1.5$\times$$\sim$572$\times$ resource improvement. 
Furthermore, for each model, we try multiple sets of token pruning ratios and there is no accuracy drops between the approximate model and the original one.


\begin{figure}[tb]
\centering
\includegraphics[width=0.8\columnwidth]{Figs/aprx.pdf}
\vspace{-10pt}
\caption{Regularization effect on quantization error of approximated GELU.}
\label{fig:approximated_gelu}
\vspace{-0.5cm}
\end{figure}

\subsection{Regularization Effect on Quantization Error}
GELU and Softmax functions are abundant in transformer blocks, which inspires us to introduce the regularization effect of quantization error into the approximated functions for more aggressive quantization (e.g., 8-bit fixed-point quantization in our case). Here we proof that our regularization works.

GELU for activation data is: $A$$=$$\mathrm{GELU}(x)$.
Quantization on $x$ can be considered as adding a small error $\Delta e$ to $x$.
We examine the influence of $\Delta e$ on output $A$ by computing the GELU derivative $\frac{\partial A}{\partial x}$.
Assuming that $A$ changes by $\Delta e$$>$0, we can obtain the absolute error of output $A$:
\begin{equation}\label{eq:gelu_error}
    \mathrm{Error_{gelu}}(x) = \frac{\partial A}{\partial x} \cdot \Delta e.
\end{equation}
Since $\frac{\partial A_{aprx}}{\partial x}$ is always $<1$ (Fig.~\ref{fig:approximated_gelu}) for GELU, the total quantization error is reduced after approximation.


Softmax for activation data is: $A_i$$=$$\frac{\delta_2\exp (\tilde{x}_{i})}{\sum_{j=1}^{N}\exp (\tilde{x}_{j})}$.
As a similar process as GELU, we compute the $Softmax_{aprx}$ derivative $\frac{\partial A}{\partial x}$:
\begin{equation}\label{eq:softmax_derivative}
% \small
    \frac{\partial A}{\partial x} \! = \!
    \left\{\begin{matrix}
    \frac{\partial \frac{\delta_2\exp (\tilde{x}_{i})}{\sum_{j=1}^{N}\exp (\tilde{x}_{j})}}{\partial \tilde{x}_{j}}=\delta_2\cdot A_i\cdot (1-A_i), & i=j\\ 
    \frac{\partial \frac{\delta_2\exp (\tilde{x}_{i})}{\sum_{j=1}^{N}\exp (\tilde{x}_{j})}}{\partial \tilde{x}_{i}}=-\delta_2\cdot A_i\cdot A_j, & i\neq j   \end{matrix}\right. .
\end{equation}
Assuming $A_0$ changes by $\Delta e_0$, the absolute error of all outputs with Equation~(\ref{eq:softmax_derivative}) is:
\begin{equation}\label{eq:softmax_error}
\begin{aligned}
    Error_\mathrm{softmax} &= |\delta_2\Delta e_0 A_0 (1-A_0)|+\sum_{i=1}^{N-1}|-\delta_2\Delta e_0 A_0  A_i| \\
    &= 2\delta_2|\Delta e_0|\ A_0 (1-A_0) < \Delta e_0,
\end{aligned}
\end{equation}
since $0\leq A_0\leq 1$, $2A_0(1-A_0)$ is always smaller than 1 and $2\delta_2A_0(1-A_0)$ is further reduced ($\delta_2<1$). So, the total quantization error after Softmax approximation is $< \Delta e_0$.