% \vspace{-5pt}
\section{Introduction}
% \vspace{-5pt}
\label{sec:introduction}
% Leveraging the self-attention mechanism inherited from the transformer architectures, Vision Transformer (ViT) has recently demonstrated great success in various computer vision tasks and received considerable attention~\cite{hudson2021ganformer,chen2021pix2seq,kim2021hotr,deng2021transvg,xue2021transfer,zhao2021point,Guo_2021,srinivas2021bottleneck}.
% In the meantime, the attention mechanism with multiple heads increases the complexity of ViT architectures, resulting in high demand for computation and memory resources, and thereby making it challenging for ViT inference on resource-limited edge devices.

Transformers \cite{bahdanau2015neural,parikh2016decomposable} have recently made an attractive resurgence in the form of Vision Transformers ({ViTs}) \cite{dosovitskiy2020image}, showing strong versatility in NLP \cite{vaswani2017attention,kenton2019bert,brown2020language}, computer vision (e.g., image classification \cite{dosovitskiy2020image}, object detection \cite{carion2020end,zhu2020deformable}, semantic segmentation \cite{zheng2021rethinking}, image processing \cite{chen2021pre}, and video understanding \cite{zhou2018end}), and complex scenarios with multi-modal data.  
Furthermore, ViTs can be used as effective backbone networks \cite{dosovitskiy2020image,wang2021pyramid,han2021transformer,cao2021swin} with superior transferability to downstream tasks through minor fine-tunings.
Seemingly, ViTs and transformers have great potential to unify diverse application domains through common architectures and tackle the reliance on scarce domain data,  ultimately addressing the two fundamental problems in deep learning: (i) strong reliance on domain data, and (ii) constant model improvements to serve evolving needs.
ViTs and transformers are largely considered as one of the future dominant deep learning techniques.

However, to fully unleash advantages of transformer architectures, we need to address the following \emph{challenges} before ViTs and transformers become an indispensable staple in future AI computing.
(i) Although the self-attention mechanism is a key defining feature of transformer architectures, a well-known concern is its quadratic time and memory complexity with respect to the number of input tokens. This hinders scalability in many settings, let alone deployment on resource-constrained edge devices.
(ii) Majority of existing works on efficient ViT and transformer techniques followed what has been done for Convolutional Neural Networks (CNNs) by using conventional weight pruning \cite{zhu2021visual, chen2021chasing, yu2022unified, yu2021unified}, quantization \cite{zhao2020investigation,prato2020fully,liu2021post}, and compact architecture design \cite{guo2019nat,so2019evolved,wang2020hat,chen2021glit,chen2021autoformer,gong2021nasvit,chen2021searching,li2021bossnas,wu2021cvt}, with limited accuracy and speed performance.
(iii) In the efforts of exploring token removal to address the quadratic complexity, the static approaches \cite{rao2021dynamicvit, liang2022evit,fayyaz2021, tang2021patch, chen2021chasing} remove tokens with a fixed ratio in an input-agnostic manner, ignoring input sample dependent redundancy; and existing image-adaptive approaches \cite{pan2021iared2, yu2022unified, xu2021evo} simply discard non-informative tokens and do not fully explore token-level redundancies from different attention heads.
Both approaches achieve a relatively low pruning rate (to preserve accuracy) or an undermined accuracy (at a high pruning rate).
Moreover, none of these studies support efficient hardware implementation on edge devices.
(iv) Transformer architectures tend to use more hardware-unfriendly computations, e.g., more nonlinear operations than CNNs, to improve accuracy.
Therefore, we need to address the low hardware efficiency issue of such computations, while enjoying the additional optimization dimension provided by multi-head self-attention.

% Pruning, as one of the most straightforward and effective methods to reduce network dimensions, has been thoroughly explored in convolution-based neural networks~\cite{10.5555/2969239.2969366,liu2017learning,ren2019admm,niu2021grim}, yet its application in self-attention-based neural networks remains relatively scarce.
% %~\cite{guo2020accelerating,sanh2020movement,li2020efficient,wang2021spatten}.
% Currently, some pioneering studies are exploring ViT pruning in three main dimensions (as detailed in Sec~\ref{sec:related_work}): (i) attention head pruning~\cite{zhu2021visual, chen2021chasing, yu2022unified, yu2021unified} that prunes redundancy in multiple attention heads, (ii) token (number) pruning~\cite{rao2021dynamicvit, liang2022evit, pan2021iared2, fayyaz2021, tang2021patch, chen2021chasing, xu2021evo, yu2022unified, yu2021unified} that prunes non-informative tokens (a token represents the features for a small image patch), and (iii) token channel pruning~\cite{chen2021chasing, yu2022unified, yu2021unified} that prunes the redundancy in each token feature representation. 
% Through our computation complexity analysis of the three pruning dimensions  (Sec~\ref{sec:computation_complexity}), we summarize that token pruning is the most effective one to achieve a high pruning rate (while preserving the accuracy) and thus focus on token pruning in this paper.


% Moreover, since static token pruning~\cite{rao2021dynamicvit, liang2022evit,fayyaz2021, tang2021patch, chen2021chasing} does not capture the input image dependant redundancy, we focus on image-adaptive token pruning~\cite{pan2021iared2, yu2022unified, xu2021evo} that dynamically prunes redundant tokes from input images. Existing adaptive toke pruning studies~\cite{pan2021iared2,yu2022unified, xu2021evo} first classify the tokens to be informative and non-informative tokens, and simply discard those non-informative ones. Moreover, most of them~\cite{yu2022unified,xu2021evo} do not consider different token-level redundancies within different attention heads. As a result, they only achieve a relatively low pruning rate (to reserve high accuracy) or an undermined accuracy (to achieve a high pruning rate). In addition, none of these studies have considered the efficient hardware implementation on edge devices to support adaptive token pruning for ViTs.


In this paper, we propose \M, a hardware-efficient image-adaptive token pruning framework, together with 8-bit quantization, for efficient yet accurate ViT inference acceleration on embedded FPGA platforms. To improve pruning rate while preserving model accuracy, we make two observations by analyzing the computation workflow in ViTs: (i) the information redundancy in input tokens differs among attention heads in ViTs; and (ii) non-informative tokens identified in earlier transformer blocks may still encode important information when propagating to later blocks.
Based on these, we design an effective token selector module that can be inserted before transformer blocks to reduce token number (i.e., number of tokens) with negligible computational overhead.
As shown in Fig.~\ref{fig:workflow}, we incorporate different token-level redundancies from multiple attention heads to be more accurate in token scoring.
Furthermore, instead of completely discarding non-informative ones, we package them together into one informative token to preserve information for later transformer blocks. 


\begin{figure}[tb]
\centering
\includegraphics[width=0.95\columnwidth]{Figs/visualization.pdf}
\vspace{-0.1in}\caption{The workflow of \M~on DeiT-S. The proposed token selector S conserves informative tokens conditioned on features from the previous layer. And non-informative tokens are averaged into one package token (blue).}
\label{fig:workflow}
\vspace{-0.2in}
\end{figure}

For hardware implementation on edge FPGA devices, we design our token selector with linear layers, i.e., fully-connected (FC) layers, instead of convolutional (CONV) layers, to reuse the GEMM (General Matrix Multiply) hardware component built for the backbone ViT (i.e., without token selectors) execution. 
In addition, we always concatenate the identified (and sparse) informative tokens and the packaged informative token together to form a dense input of tokens to avoid sparse computations on hardware. 
To improve the hardware efficiency, we further apply 8-bit quantization for weights and activations, and propose polynomial approximations for the frequently used nonlinear functions in ViTs, including GELU~\cite{hendrycks2016gaussian}, Softmax~\cite{Goodfellow-et-al-2016}, and Sigmoid~\cite{mitchell1999machine}. 
Besides, we introduce the regularization effect on quantization error into the design of polynomial approximations to support more ambitious quantization.
We design a proof-of-concept FPGA accelerator for ViTs based on our proposed HeatViT. 
We implement the GEMM engine inspired by~\cite{FPT19gemm,lizh2022logic} to execute the most computation-intensive multi-head self-attention module and feed-forward network in the backbone ViT, and the classification network in our token selector. 
It is worth noting that only lightweight control logic is added to support our adaptive token pruning by reusing the same hardware components built for the backbone ViT execution.

To reduce inference latency on hardware while preserving model accuracy (typically within 0.5\% or 1\% accuracy loss), we propose a latency-aware multi-stage 
training strategy to (i) determine the transformer blocks for inserting token selectors and (ii) optimize the desired (average) pruning rates for these token selectors.
Specifically, we insert a token selector for each transformer block, from the last block backward to the first one, since early blocks are more sensitive to accuracy. 
For each insertion, we train the current token selector while fine-tuning other network components, to decrease latency through increasing pruning rate of the current block until accuracy loss exceeds a threshold. 
Then we further consolidate token selectors in consecutive transformer blocks with similar pruning rates into one token selector for a whole stage of transformer blocks.
Our training strategy is conducted as fine-tuning on pre-trained ViTs.
And because we use small epoch numbers for inserting token selectors, our training effort is roughly 90\% of that for training-from-scratch of backbone ViTs (without token selectors).


% \begin{figure*}[htb]
% \centering
% \includegraphics[width=1.5\columnwidth]{Figs/acc-latency-tradeoff-new.pdf}
% \vspace{-8pt}\caption{Comparison of different pruning methods with various accuracy-latency trade-offs. \M~can generate efficient models with better trade-offs compared to other efficient methods.} %This guides users to generate models satisfying their requirements.}
% 4\label{fig:user-trade-off}
% \vspace{-0.4cm}
% \end{figure*}

\input{Tables/ViT_Pruning_Works}

As summarized in Fig.~\ref{fig:user-trade-off}, we evaluate \M~for multiple widely used ViT models on the ImageNet dataset, including DeiT-tiny (DeiT-T, HeatViT-T), DeiT-small (DeiT-S, HeatViT-S), DeiT-base (DeiT-B, HeatViT-B)~\cite{Touvron2021TrainingDI}, LV-ViT-small (LV-ViT-S, HeatViT-LV-S), and LV-ViT-medium (LV-ViT-M, HeatViT-LV-M)~\cite{jiang2021all}. 
These models are already well condensed and thus more challenging to prune compared to larger ViT models. 
%As shown in Fig.~\ref{fig:user-trade-off}, HeatViT-T series take DeiT-T as the target of the computation cost, HeatViT-LV-S series take LV-ViT-S as the target one, and the remaining labels are similar.
Compared to state-of-the-art ViT pruning studies in Table~\ref{tab:pruning_works} (pruning types are explained in detail in Sec~\ref{sec:pruning_vit}), HeatViT achieves better accuracy-computation trade-offs: 
Under a similar computation cost, \M~can achieve 0.7\%$\sim$8.9\% higher accuracy; while under a similar accuracy, \M~can reduce the computation cost by more than 28.4\%$\sim$ 65.3\%.
Compared to the baseline hardware accelerator (16-bit and no token pruning), our implementations of \M~on the Xilinx ZCU102 FPGA achieves $3.46\times$$\sim$$4.89\times$speedup with a trivial resource utilization overhead of 8\%$\sim$11\% more DSPs and 5\%$\sim$8\% more LUTs.
Compared to the optimized ARM CPU and NVIDIA GPU versions on the NVIDIA Jetson TX2 board with our token pruning, our FPGA implementations achieve 685$\times$$\sim$1695$\times$ and 2.68$\times$$\sim$3.79$\times$ more speedups, 242.6$\times$$\sim$719.0$\times$ and 3.0$\times$$\sim$4.7$\times$ higher energy efficiency.

%\zf{Zhenman has revised until here}

Our contributions are summarized as follows:
\begin{itemize}[leftmargin=*]

    \item Algorithm and hardware co-design for an effective and hardware-efficient token selector to enable efficient image-adaptive token pruning in ViTs.
    \item A latency-aware multi-stage training strategy to learn the effective insertion of token selectors in ViTs.
    \item A polynomial approximation of nonlinear functions inside ViTs for more ambitious quantization and efficient FPGA-based implementations.
    \item An end-to-end acceleration framework, with both image-adaptive pruning and 8-bit quantization, for ViT inference on embedded FPGAs.
    \item Experiments to demonstrate superior pruning rates and inference accuracy of \M~over state-of-the-art ViT pruning studies, as well as trivial hardware resource overhead.

    %\item We propose \textbf{\M}, a hardware-efficient multi-stage image-adaptive token pruning framework for ViT inference acceleration on embedded FPGAs.
    %\item We present a lightweight token selector for adaptive token pruning, performing both multi-head selection of informative tokens and packaging of non-informative tokens. The token selector incurs negligible processing overhead on the algorithm level (only $\sim$1\% additional computation requirement of the original unpruned ViTs) and low resource utilization impact on the FPGA hardware (2\% and 2$\sim$5\% more DSPs and LUTs, respectively).
    %\item \textcolor{red}{We develop a multi-stage training strategy for adaptive token pruning, investigating the appropriate number of token selectors with the location and pruning rate of each token selector. Pruning is performed from the last transformer block near the output to the first one near the input to achieve an overall pruning rate with an accuracy degradation under a threshold value, and thus fulfilling a throughput requirement with the available computation and storage resources on FPGAs.}
    %\item Compared with existing work, \M~achieves higher pruning rates (23.08\%$\sim$30.77\% for DeiT-T, 16.09\%$\sim$56.52\% for DeiT-S) with higher accuracy (0.25\%$\sim$3.11\% more for DeiT-T, 0.22\%$\sim$2.13\% more for DeiT-S) on ImageNet-1K. \zf{revise this based on abstract. Incorporating the control flow to support adaptive token pruning with trivial resource utilization overhead (2\% more DSPs, 2\%$\sim$5\% more LUTs), our hardware accelerators achieve $1.13\times$$\sim$$1.60\times$ higher FPS for pruned models on the Xilinx ZCU102 FPGA.}
\end{itemize}

\begin{figure*}[htb]
\centering
\includegraphics[width=1.4\columnwidth]{Figs/acc-latency-tradeoff-new.pdf}
\vspace{-8pt}\caption{Comparison of \M~with prior pruning methods: Under a similar computation cost, \M~achieves 0.7\%$\sim$8.9\% higher accuracy; while under a similar accuracy, \M~reduces the computation cost by more than 28.4\%$\sim$ 65.3\%. Final \M~models are quantized into 8-bit fixed-point format.}
\label{fig:user-trade-off}
\vspace{-0.4cm}
\end{figure*}


\begin{figure}[tb]
\centering
\includegraphics[width=0.8\columnwidth]{Figs/ViT_p1.pdf}
\vspace{-0.1in}
\caption{Overview of ViT}
\label{fig:encoder}
\vspace{-0.2in}
\end{figure}