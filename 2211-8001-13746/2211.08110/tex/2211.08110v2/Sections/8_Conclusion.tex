% \vspace{-5pt}
\section{Conclusion}
% \vspace{-5pt}
In this paper, we have proposed a hardware-efficient image-adaptive token pruning framework called \M~for ViT inference acceleration on resource-constraint edge devices. 
To improve the pruning rate and accuracy, we analyzed the inherent computational patterns in ViTs and designed an effective token selector that can more accurately classify tokens and consolidates non-informative tokens.
%that can be progressively inserted after transformer blocks. This token selector includes an attention-based multi-head token classifier to more accurately classify informative and non-informative tokens and a token packager to combine non-informative tokens into one informative token to carry more information for later transformer blocks. 
We also implemented a proof-of-concept ViT hardware accelerator on FPGAs by heavily reusing the hardware components built for the backbone ViT to support the adaptive token pruning module.
Besides, we propose a polynomial approximation of nonlinear functions for ambitious (8-bit) quantization and efficient hardware implementation.
Finally, to meet both the target inference latency and model accuracy, we proposed a latency-aware multi-stage training strategy to learn the number of token selectors to insert into the backbone ViT, and the location and pruning rate of each token selector. Experimental results demonstrate that \M~achieves superior pruning rate and accuracy compared to state-of-the-art pruning studies, while incurring trivial amount of hardware resource overhead.