\section{Adaptive Token Pruning Module}
\label{sec:co-design}

\begin{figure*}[htb]
\centering
\includegraphics[width=1.9\columnwidth]{Figs/framework_pact.pdf}
\vspace{-8pt}
\caption{\M~overview. Left: Transformer blocks split into multiple stages with token selectors inserted between them (One token selector includes one token classifier and one token packager). Right: Multi-head token classifier to identify informative tokens.}
\label{fig:framework}
\vspace{-0.2in}
\end{figure*}

%This section discusses the adaptive pruning module of our framework \M. 
As shown in Fig.~\ref{fig:framework}, the proposed adaptive token selector is based on observations in Section~\ref{sec:motivation}, including the attention-based multi-head token classifier and the token packager.

\subsection{Attention-Based Multi-Head Token Classifier}
\label{sec:token_selector}

\noindent\textbf{Multi-Head Token Classifier.} 
Based on the multi-head visual receptive area of the ViT, we propose a fine-grained structure to evaluate token scores.
As shown in Fig.~\ref{fig:framework},  each head focuses on extracting different features and respective fields of an image, demonstrating that the importance of each token towards each head is different.
Hence, our multi-head classifier generates a score-map vector for each input token, marking the information amount  of each token in each head separately.

Let one head dimension be $d {=} D/h$.
We split the input $X {\in} \mathbb{R}^{N\times D}$ into $h$ subvectors $\{ x_i\}^{h}_{i=1}{\in} \mathbb{R}^{N\times d}$, and feed the subvectors into an MLP network to obtain the representation of local receptive field $E_i^\mathrm{local}$ and the representation of global receptive field $E_i^\mathrm{global}$.
\begin{equation}
E_i^\mathrm{local} = {\rm MLP}(x_i)  \in \mathbb{R}^{N\times d/2}
\label{eq:mlp1}
\end{equation}
\begin{equation}
E_i^\mathrm{global} = {\rm Average}( {\rm MLP}(x_i))  \in \mathbb{R}^{1\times d/2}
\label{eq:mlp2} 
\end{equation}
Pushing the feature $E_i {=} concate(E_i^{local},E_i^{global} \times N) {\in} \mathbb{R}^{N\times d}$ into another MLP network and one Softmax function, token score maps $\{s_i\}^{h}_{i=1}{\in} \mathbb{R}^{N\times 2}$ are produced. $s_i$ is the token score for attention head $i$ and ${\times} 2$ represents the keep and prune probabilities respectively:
\begin{equation}
\begin{gathered}
s_i = {\rm Softmax}({\rm MLP}(E_i)) \in \mathbb{R}^{N\times 2} \\
\end{gathered}
\end{equation}

\noindent\textbf{Attention-Based Branch.}
Base on the attention mechanism~\cite{hu2018squeeze}, we add an attention-based branch along the classifier backbone to synthesis the importance of each head:
\begin{equation}
\bar{X} = \mathrm{Concat}(\bigg\{ \frac{1}{d} \sum_{j=1}^{d}x_{ij} \bigg\}^{h}_{i=1}) \in \mathbb{R}^{N\times h}
\label{eq:avg_pool}
\end{equation}
\begin{equation}
A = {\rm Sigmoid} ({\rm MLP}(\bar{X})))  \in \mathbb{R}^{N\times h}
\vspace{-0.05in}
\label{eq:sigmoid_fc}
\end{equation}
where $\bar{X}$ is a head-wise statistic through its channel dimension $D$.
In Eq.~\eqref{eq:sigmoid_fc}, we feed $\bar{X}$ into an MLP network with the sigmoid to obtain the score vector $A$, evaluating the importance of each head.
Then the overall token score is calculated by the weighted average $S$ with $A$:
\begin{equation}
\tilde{S} = \frac{\sum_{i=1}^{h}s_i*a_i}{\sum_{i=1}^{h}a_i}   \in \mathbb{R}^{N\times 2}
\end{equation}
\vspace{-0.05in}
where $\tilde{S}$ is the token probability score. % importance score of each head.
Then we apply the Gumbel-Softmax for the token keep/prune decision mask:

\vspace{-0.1in}
\begin{equation}
M={\rm GumbelSoftmax}(\tilde{S}) \in \{0,1 \}^N
\label{eq:gumbel}
\end{equation}
Since deleted image tokens cannot appear in subsequent blocks, $M$ passes on to the following blocks and will be updated by applying $M$ $\leftarrow$ $M\odot M^{\prime}$. $M^{\prime}$ is the new mask generated in the next stage.


% \vspace{-5pt}
\subsection{Token Packager}
\label{sec:token_packaging}

To solve the problem in the Sec~\ref{sec:motivation}, we apply a token packaging step that summarizes non-informative tokens (predicted by the classifier) into a package token instead of completely discarding them.
Assume there are $T$ (evaluated by the classifier) non-informative tokens $\{ \hat{x}_t\}^{T}_{t=1}{\in} \mathbb{R}^{T\times D}$ along with their token scores $\{ \tilde{s}_t\}^{T}_{t=1}{\in} \mathbb{R}^{T\times 2}$, these tokens are compressed into one token through weighted averaging:
\begin{equation}
P = \frac{ \sum_{t=1}^{T} \hat{x}_t * \tilde{s}_t[0]}{\sum_{t=1}^{T} \tilde{s}_t[0]} \in \mathbb{R}^{1\times D}
\end{equation}
where $P$ is the package token; $\tilde{s}_t[0]$ is the keep score of $\hat{x}_t$;
$*$ is an element-wise multiplication. 
$P$ will continue the subsequent calculations along with the informative ones, enabling the model to correct scoring mistakes.

After image-adaptive removing a certain part of tokens and the token packaging step, the sparse input matrix (all the informative tokens and package token) will be concatenated into a new smaller-size dense matrix to complete the computation in the following blocks, which speedup the model inference directly.
And most of the component operations (FC layer, Softmax, GELU) have already been there in the backbone ViT blocks. Therefore, we can utilize our unified operation-level optimization scheme of the ViT deployment on FPGA.
