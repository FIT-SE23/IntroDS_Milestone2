% \vspace{-5pt}
\section{Background and Related Work}
% \vspace{-5pt}
\label{sec:related_work}

% \zf{Please double check the references in Section~\ref{sec:related_work} to make sure we cover all the studies we compared in Table~\ref{tab:pruning_works}}
\subsection{Vision Transformer}
%\noindent\textbf{Vision Transformers.}
Transformers are initially proposed to handle the learning of long sequences in NLP tasks. Dosovitskiy et al.~\cite{dosovitskiy2020image} and Carion et al.~\cite{carion2020end} adapt the transformer architecture to image classification and detection, respectively, and achieve competitive performance against CNN counterparts with stronger training techniques and larger-scale datasets. DeiT~\cite{Touvron2021TrainingDI} further improves the training pipeline with the aid of distillation, eliminating the need for large-scale pretraining~\cite{yuan2021tokens}. Inspired by the competitive performance and global receptive field of transformer models, follow-up works design ViTs for different computer vision tasks including object detection, semantic segmentation, 3D object animation, and image retrieval.

Fig.~\ref{fig:encoder} illustrates three components in ViTs: \emph{patch embedding}, \emph{transformer encoders}, and \emph{classification output head}.
For \textbf{patch embedding}, an image $\mathbf X \in \mathbb R^{H \times W \times C}$ is  reshaped into a sequence of 2D patches $\mathbf X_p \in \mathbb R^{N \times (P^2 \cdot C)}$, where $(H,W)$ is  original image resolution, $C$ is channel number, $(P,P)$ is the resolution of  patches, and $N=HW/P^2$ is the number of patches and also the effective input sequence length (i.e., number of tokens) to transformer encoders.
Patch embeddings are obtained by mapping $\mathbf X_p$ into $D$ dimensions via a trainable linear projection $\mathbf E \in \mathbb R^{(P^2 \cdot C) \times D}$: $\mathbf X_0 =$ $[\mathbf X_{class}; \mathbf X^1_p \mathbf E; \mathbf X^2_p \mathbf E; ...; \mathbf X^N_p \mathbf E]+\mathbf E_{pos}$,
where $D$ is the constant latent vector size throughout transformer encoders, and $\mathbf E_{pos} \in \mathbb R^{(N+1) \times D}$ is {learnable} position embeddings. 
A learnable embedding $\mathbf X_0^0 = \mathbf X_{class}$ is prepended in patch embeddings $\mathbf X_0$.
Then $\mathbf X$ goes through a stack of $L$ transformer encoders for processing.
The output $\mathbf X_L^0$ (class token) of the final {($L$-th) transformer encoder}  is fed to \textbf{a classification output head} implemented by an MLP to obtain the \textit{final result}. The MLP module uses two linear layers with a Gaussian Error Linear Unit (GELU) activation layer in between. 


The $l$-th \textbf{transformer encoder} receives the patch embeddings $\mathbf X_{l-1}$ as input.
Fig.~\ref{fig:encoder} illustrates a ViT encoder block, consisting of a multi-head self-attention (MSA) module and an MLP (FFN) module, both with LN (layer normalization) applied before input.
The encoder block operations are then:
\begin{equation}
\begin{aligned}
     \mathbf{X'}_{l-1}   &=  \text{MSA} ( \text{LN} ( \mathbf{X}_{l-1} ) ) +  \mathbf{X}_{l-1}, \\
     \mathbf{X}_{l}  &=  \text{FFN} ( \text{LN} (\mathbf{X'}_{l-1})) + \mathbf{X'}_{l-1}.
\end{aligned}
\end{equation}
For $h$ heads in MSA, $\text{LN}(\mathbf{X}_{l-1})$ is split into $h$ parts.
For each head, the corresponding part in $\text{LN}(\mathbf{X}_{l-1})$ is transferred into query $\mathbf{Q}$, key $\mathbf{K}$, and value $\mathbf{V}$ through three linear projections $W_Q$, $W_K$, and $W_V$, respectively. 
Then self-attention function~\cite{vaswani2017attention} in each head is performed as: 
\begin{equation}
     \text{Attention}(\mathbf{Q},\mathbf{K}, \mathbf{V})=\text{Softmax}(\mathbf{Q}\mathbf{K}^T/\sqrt{D})\mathbf{V}.\label{eq:selfattention}
\end{equation}
Attention outputs from all the heads are concatenated and linearly transformed. 



\vspace{-5pt}
\subsection{Model Pruning on ViT}\label{sec:pruning_vit}
% \vspace{-5pt}
State-of-the-art ViT pruning studies exploring the redundancy from three dimensions in ViTs are summarized below.

\noindent\textbf{Redundancy of the attention head.}
Attention head pruning~\cite{zhu2021visual, chen2021chasing, yu2022unified, yu2021unified} reduces weight redundancy on the transformation matrices ($W_Q$, $W_K$, $W_V$) before MSA operation. 
Due to the parallel computing nature of the transformer heads, these works directly prune some heads entirely and lead to significant accuracy drop,
for example, 27\% GMACs reduction, but with 2.2\% accuracy drop on DeiT-T in~\cite{chen2021chasing}.
It is an inefficient way of computation reduction because the attention head usually contributes less than 43\% of the total computation in several ViT architectures~\cite{kong2021spvit}.

\noindent\textbf{Redundancy of the token channel.}
Token channel~\cite{chen2021chasing, yu2022unified, yu2021unified} pruning executes feature-level pruning on the embedding of the token feature to diminish the redundancy of the feature representation. 
Since this pruning type is equivalent to structured pruning on the token embedding, i.e., removing the same embedding dimension for different tokens, it is difficult to guarantee an ideal pruning rate without significant accuracy deterioration.

\noindent\textbf{Redundancy of the token number.}
Token pruning~\cite{rao2021dynamicvit, liang2022evit, pan2021iared2, fayyaz2021, tang2021patch, chen2021chasing, xu2021evo, yu2022unified} aims at removing the non-informative input data.
% to accelerate the model inference.
According to~\cite{yang2021instance}, the accuracy of neural networks is more related to the object information, but not the background information, implying the input-level redundancy. There is often much higher redundancy along the token number dimension.

% \vspace{-5pt}
\subsection{Computational Complexity Analysis}\label{sec:computation_complexity}
% \vspace{-5pt}
Given an input sequence $N{\times} D$, where $N$ is the input sequence length or the token number, and $D$ is the embedding dimension~\cite{Touvron2021TrainingDI} of each token,
some works~\cite{pan2021iared2,zhu2021visual} address the computational complexity of ViT as $(12ND^2 + 2N^2D)$.
However, $D$ represents three physical dimensions: (i) $D_{ch}$ -- channel size of a token, (ii)
$h$ -- number of heads, and (iii) $D_{attn_{s}}$ -- sub-channel size for one head. 
The channel size of the Query ($\mathbf{Q}$), Key ($\mathbf{K}$), and Value ($\mathbf{V}$) matrices is $HD_{attn_{s}}$.
As shown in Table~\ref{tab:complexity}, the computational complexity of the ViT can be written as $[4ND_{ch}(hD_{attn_{s}}) + 2N^2(hD_{attn_{s}}) + 8ND_{ch}D_{fc}]$.
Compared with other prunable dimensions, directly pruning tokens, $N$, will contribute to the reduction of all operations linearly or even quadratically ($N^2$ in layers \ding{173} and \ding{174}), more compression benefits than other pruning types.
Please note that since token pruning is equivalent to removing information redundancy within the image data, it can be more freely integrated into the compression process on other model dimensions (e.g., model quantization in our case).
% Thus, we focus on token pruning in this paper and \py{leave the other two orthogonal pruning dimensions to future work}.

\input{Tables/ViT_Computation}

% \vspace{-5pt}
\subsection{Static vs. Image-Adaptive Token Pruning}\label{sec:pruning_vs}
% \vspace{-5pt}
There are two sub-branches in the token pruning of the ViT: static token pruning and image-adaptive token pruning.
Static token pruning~\cite{rao2021dynamicvit, liang2022evit,fayyaz2021, chen2021chasing, tang2021patch} reduces the number of input tokens by a fixed ratio for different images, which neglects the fact that the information of each image varies both in the region size and location, and thus restricts the image pruning rate.
In contrast, image-adaptive token pruning~\cite{pan2021iared2, yu2022unified, xu2021evo} deletes redundant tokens based on the inherent image characteristics to achieve a per-image adaptive pruning rate.
Generally, smaller pruning rates are applied for complex and information-rich images while larger pruning rates for simple and information-less ones.
Ideally, it can achieve a larger overall pruning rate than the static one as Fig~\ref{fig:token_pruning}.
However, 
%the pruning evaluation module in~\cite{pan2021iared2} mainly includes a stack of two fully-connected (FC) layers
existing adaptive token pruning studies do not consider the visual characteristics inside the ViT (token redundancy in ViT, Sec~\ref{sec:motivation}) --visual receptive field of different heads, and delete the less informative tokens completely without the opportunity to correct evaluation errors.
Moreover, there are no more details on how to determine the location, number, and pruning rate of the pruning evaluation module specifically.
Hence, they often lead to a limited overall pruning rate (e.g., 35\% GMACs reduction on DeiT-S~\cite{pan2021iared2}).
Finally, none of these studies have considered the efficient hardware implementation on edge devices to support adaptive token pruning. For example,~\cite{pan2021iared2} introduces a new operation, Argsort, which is currently not compatible with many frameworks~\cite{prillo2020softsort}.

\begin{figure}[tb]
\centering
% \includegraphics[width=0.6\linewidth]{Figs/encoder.png}
\includegraphics[width=1\columnwidth]{Figs/token_pruning.pdf}
\vspace{-0.25in}
\caption{Comparison between static token pruning and \M~image-adaptive token pruning.}
\label{fig:token_pruning}
\vspace{-0.2in}
\end{figure}

% \vspace{-5pt}
\subsection{Transformer Accelerators on FPGAs}\label{sec:transformerFPGA}
% \vspace{-5pt}
Previous state-of-the-art FPGA accelerators on Transformer-based models \cite{li2020ftrans}, \cite{qi2021accommodating} and \cite{zhang2021algorithm} also leverage sparsity.
However, they have three main limitations.
(i)\cite{li2020ftrans} depends on block-circulant matrix-based weight representation in order to replace the matrix-vector multiplication in FC layers with FFT/IFFT-based processing elements. However,~\cite{li2020ftrans} accelerates only the MSA part of the model and omits the FFN part (about $65\%$ computation of the whole model).
(ii) \cite{qi2021accommodating} utilizes the block-wise weight pruning and \cite{zhang2021algorithm} implements the structure pruning (row-wise or column-wise) to compress weights to similar sizes (lower memory footprint). However, the pruning granularity of both is coarse, resulting in severe accuracy degradation at limited compression rates. For example,~\cite{qi2021accommodating} shows 2\% accuracy drops under the 36\% sparsity.
(iii) These FPGA-based accelerators are mainly for transformers in the Natural Language Processing (NLP) tasks, not for computer vision ones.
In addition, they all compress the model weights, which means that new substructures or additional overhead is needed to support the specific sparse matrix multiplication.
According to our Section~\ref{sec:motivation} analysis, the difference in input data type will introduce a unique redundancy that motivates an effective compression space.
Experiments show that adaptive token pruning at the data level can speed up the model inference with negligible accuracy losses and little overhead for hardware implementations.

