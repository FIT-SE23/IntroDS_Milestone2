
\newif\ifarxiv
\arxivtrue % Comment out for PNAS version


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% ARXIV
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\ifarxiv



\documentclass{article}

\usepackage{fullpage}
\usepackage{bbm}
\usepackage{enumitem}
\usepackage{float}
% \usepackage{ulem}
\usepackage{color-edits}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{authblk}
\usepackage{amsfonts}
% \usepackage{natbib}
\usepackage{amsmath}
\usepackage{hyperref}
\usepackage{booktabs}
\usepackage{graphicx}
\usepackage{cleveref}
\usepackage{multirow}
\newtheorem{theorem}{Theorem}
\newtheorem{lemma}{Lemma}
\newtheorem{definition}{Definition}
\newcommand{\cA}{\mathcal{A}}
\newcommand{\cD}{\mathcal{D}}
\newcommand{\cmt}[1]{{\color{red} #1} }

% \input{math_help}
\title{Confidence-Ranked Reconstruction of Census Microdata\\ from Published Statistics}


\author[a]{Travis Dick}
\author[b]{Cynthia Dwork} 
\author[c]{Michael Kearns}
\author[d]{Terrance Liu}
\author[c]{Aaron Roth}
\author[e]{Giuseppe Vietri}
\author[d]{Zhiwei Steven Wu}

\affil[a]{University of Pennsylvania (Now at Google)}
\affil[b]{Harvard University}
\affil[c]{University of Pennsylvania}
\affil[d]{Carnegie Mellon University}
\affil[e]{University of Minnesota}



\date{}







\else
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% PNAS 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\documentclass[9pt,twocolumn,twoside,lineno]{pnas-new}
\usepackage{xcolor}
\usepackage{bbm}
\usepackage{enumitem}
\usepackage{cleveref}
\usepackage{float}
\usepackage{ulem}
\usepackage{color-edits}
\usepackage{algorithm}
\usepackage{algpseudocode}
\newtheorem{theorem}{Theorem}
\newtheorem{lemma}{Lemma}
\newtheorem{definition}{Definition}

\addauthor{sw}{blue}

% --- The following commands can be used to toggle comments --- %
\newcommand \commenttext [1]{#1}
%\newcommand \commenttext [1]{}

\newcommand{\mk}[1]{\commenttext{\textcolor{blue}{[Michael: #1]}}}
\newcommand{\ar}[1]{\commenttext{\textcolor{blue}{[Aaron: #1]}}}
\newcommand{\cd}[1]{\commenttext{\textcolor{blue}{[Cynthia: #1]}}}
\newcommand{\sw}[1]{\commenttext{\textcolor{blue}{[Steven: #1]}}}
\newcommand{\gv}[1]{\commenttext{\textcolor{purple}{[Giuseppe: #1]}}}
\newcommand{\tl}[1]{\commenttext{\textcolor{red}{[Terrance: #1]}}}
\newcommand{\td}[1]{\commenttext{\textcolor{orange}{[Travis: #1]}}}

\newcommand{\cA}{\mathcal{A}}
\newcommand{\cD}{\mathcal{D}}
\newcommand{\cmt}[1]{{\color{red} #1} }

\templatetype{pnasresearcharticle} % Choose template 
% {pnasresearcharticle} = Template for a two-column research article
% {pnasmathematics} %= Template for a one-column mathematics article
% {pnasinvited} %= Template for a PNAS invited submission

\title{Confidence-Ranked Reconstruction of Census Microdata from Published Statistics}


% Use letters for affiliations, numbers to show equal authorship (if applicable) and to indicate the corresponding author
\author[a]{Travis Dick}
\author[b]{Cynthia Dwork} 
\author[c,1]{Michael Kearns}
\author[d]{Terrance Liu}
\author[c]{Aaron Roth}
\author[e]{Giuseppe Vietri}
\author[d]{Zhiwei Steven Wu}

\affil[a]{University of Pennsylvania (Now at Google)}
\affil[b]{Harvard University}
\affil[c]{University of Pennsylvania}
\affil[d]{Carnegie Mellon University}
\affil[e]{University of Minnesota}

% Please give the surname of the lead author for the running footer
\leadauthor{Dick} 

% Please add a significance statement to explain the relevance of your work
\significancestatement{We show how to launch a reconstruction attack on US Decennial Census data based only on publicly released statistics. The attack can recover individual microdata --- i.e. the responses of individual Census survey respondents. Although our attack cannot reconstruct all rows of the private data, it can produce a confidence-based ranking of rows, such that rows that appear earlier in the ranking are more likely to appear in the private data. Thus the attacker can reconstruct a fraction of the rows with confidence. We compare our attack to a hierarchy of increasingly strong baselines, and show that we can outperform all of them. Our results point to the necessity of employing privacy enhancing technologies when releasing statistics of privacy-sensitive datasets.}

% Please include corresponding author, author contribution and author declaration information
\authorcontributions{All authors contributed to all aspects of the research and writing. Order of authors is alphabetical.}
% \authordeclaration{Please declare any competing interests here.}
% \equalauthors{\textsuperscript{1}A.O.(Author One) contributed equally to this work with A.T. (Author Two) (remove if not applicable).}
\correspondingauthor{\textsuperscript{1}To whom correspondence should be addressed. E-mail: mkearns@cis.upenn.edu}

% At least three keywords are required at submission. Please provide three to five keywords, separated by the pipe symbol.
\keywords{Dataset Reconstruction $|$ United States Census $|$ Privacy} 


\dates{This manuscript was compiled on \today}
\doi{\url{www.pnas.org/cgi/doi/10.1073/pnas.XXXXXXXXXX}}


\fi % END PNAS version






\input{math_help}


\begin{document}
\maketitle
\begin{abstract}
A reconstruction attack on a private dataset $D$ takes as input some publicly accessible information about the dataset and produces a list of candidate elements of~$D$.
We introduce a new class of data reconstruction attacks based on randomized methods for non-convex optimization.
We empirically demonstrate that our attacks can not only reconstruct full rows of $D$ from
aggregate query statistics $Q(D)\in \mathbb{R}^m$, but can do so in a way that reliably ranks reconstructed rows by their odds of appearing
in the private data, providing a signature that could be used for prioritizing reconstructed rows
for further actions such as identify theft or hate crime. 
We also design a sequence of {\em baselines} for evaluating reconstruction attacks.
Our attacks significantly outperform those
that are based only on access to a public {\em distribution} or population from which the private dataset $D$ was
sampled, demonstrating that they are exploiting information in the
aggregate statistics $Q(D)$, and not simply the
overall structure of the distribution. In other words, the queries $Q(D)$ are permitting reconstruction of elements of {\em this} dataset, not the distribution from which $D$ was drawn. These findings are established both on 2010 U.S. decennial Census data
and queries and Census-derived American Community Survey datasets. Taken together, our methods and experiments illustrate
the risks in releasing numerically precise aggregate statistics of a large dataset, and provide
further motivation for the careful application of provably private techniques such as differential privacy.
\end{abstract}






\ifarxiv
\else

\thispagestyle{firststyle}
\ifthenelse{\boolean{shortarticle}}{\ifthenelse{\boolean{singlecolumn}}{\abscontentformatted}{\abscontent}}{}
\fi

% If your first paragraph (i.e. with the \dropcap) contains a list environment (quote, quotation, theorem, definition, enumerate, itemize...), the line after the list may have some extra indentation. If this is the case, add \parshape=0 to the end of the list environment.

The goals of data analysis and those of responsible data stewardship are often in tension:
we wish to extract and share
useful information about important datasets, 
but also must maintain the privacy of the individuals whose information comprises the datasets. 
This is exactly the problem faced by the U.S. Census, which on the one hand has a legal mandate to protect the privacy of its respondents,\footnote{Title 13, Section 9, of the US Code: says "(a)  Neither the Secretary, nor any other officer or employee of the Department of Commerce or bureau or agency thereof, or local government census liaison, may, except as provided in section 8 or 16 or chapter 10 of this title [13 USCS § 8 or 16 or §§ 401 et seq.] or section 210 of the Departments of Commerce, Justice, and State, the Judiciary, and Related Agencies Appropriations Act, 1998 [13 USCS § 141 note] or section 2(f) of the Census of Agriculture Act of 1997 [7 USCS § 2204g(f)]— 
(1)  use the information furnished under the provisions of this title for any purpose other than the statistical purposes for which it is supplied; or
(2)  make any publication whereby the data furnished by any particular establishment or individual under this title can be identified; or
(3)  permit anyone other than the sworn officers and employees of the Department or bureau or agency thereof to examine the individual reports."
} but on the other hand is a major governmental statistical agency that released over 150 billion tabulations from the data collected as part of the 2010 decennial Census \cite{abowd}. 
 What is the risk of releasing large numbers of aggregate statistics from this and similarly 
sensitive datasets?

{\bf Confidence.} In this work, we empirically demonstrate that without taking explicit and rigorous steps to ensure individual citizen privacy, the release of simple aggregate statistics of large datasets is highly vulnerable to specific and computationally feasible attacks that can reliably reconstruct complete rows of the private dataset. These findings stand even when measured against a hierarchy of baseline metrics that correspond to ``reconstructions'' that result from increasingly fine-grained knowledge of the distribution from which the private data was drawn. An important and damaging aspect of the attacks we propose is their ability to give confident predictions about which rows have been correctly reconstructed when (as is typically the case) the reconstructions are not perfect. Our methods output a {\it ranking} over candidate reconstructed rows that is highly correlated with their
presence in the private data, with rows that appear early in the ranking having high odds of appearing in the true dataset. Such a ranking could be used by an adversary to prioritize subsequent exploitation of private data --- for example, for identity theft, or to locate individuals of certain backgrounds (or even in the intersection of certain background, age range, and sex categories).\footnote{Census data were used to identify Japanese Americans to send to internment camps during World War II \cite{washingtonpost, SeltzerAnderson}. In 2002, the Department of Homeland Security used re-tabulations of census data to track Arab-Americans \cite{nytimes}.}
% build confidence in damaging inferences about individuals. % --- for example identifying a change in gender identification. 



Our methods are based on casting the reconstruction problem as an instance of large-scale, non-convex optimization, along with a subsequent step to convert non-continuous
(e.g. categorical) features back to their original schema. The algorithms are closely related to recent methods for generating synthetic 
versions of sensitive datasets \cite{ aydore2021differentially,liu2021iterative,vietri2022private} while enforcing Differential Privacy (DP)~\cite{dwork2006calibrating, DBLP:conf/icalp/Dwork06, dwork2016calibrating,dwork2014algorithmic}. Crucially, these techniques are randomized, which allows us to repeat the reconstruction multiple times and get different results. Our confidence rankings are obtained by computing how frequently rows appear in multiple reconstructions; this is the primary methodological novelty of our approach.\footnote{Prior deployed data reconstruction attacks are generally deterministic and based on linear or integer programming. Theory for such attacks dates back to \cite{dinur2003revealing} (see \cite{kasiviswanathan2010price} for theory for attacks on release of contingency table statistics, which closely match the kinds of statistics released by Census that we attack here). See \cite{dwork2017exposed} for a survey of privacy attacks, and \cite{cohen2020linear} for a description of a practical linear programming-based reconstruction attack. Integer programming techniques could also be used in our case together with a randomized objective function to provide the needed stochasticity. We investigated this as well, but found that the continuous optimization approach was more performant --- especially on higher dimensional data like data from the American Community Survey.} 

%Our primary contribution goes beyond the top-line reconstruction rate of total number of rows reconstructed, providing a way to establish confidence that some particular rows correspond to correct reconstructions with high confidence. 
We establish the striking and consistent empirical phenomenon that rows that appear more frequently across repeated optimizations are also more likely to be rows of the private dataset --- thus allowing an attacker to build confidence in the authenticity of rows that are repeatedly reconstructed, and to prioritize individuals for other forms of attack. We present a simple mathematical justification in a Bayesian framework that provides intuition for this phenomenon. We remark that reconstruction (recovering the rows of the private dataset from published statistics) is a different problem from re-identification (re-attaching names to the reconstructed rows) --- but prior work \cite{rocher2019estimating} shows that re-identification risk can be confidently estimated given reconstructed rows, and is generally high for data with even a moderate number of recorded features.

%moved later
%This work demonstrates that such optimization techniques can be viewed as both a ``poison'' and a ``cure''. Applied carefully to {\it noisy} statistics that ensure DP, they provide a powerful tool for sharing useful data statistics while ensuring citizen privacy \cite{ aydore2021differentially,liu2021iterative,vietri2022private} . But applied to numerically precise statistics --- even those that may provide only aggregate information about large populations --- they exfiltrate entire rows of sensitive data with confidence. 

{\bf Baselines.} The Census bureau itself conducted a reconstruction attack on 2010 Census data, and reported top-level statistics on what fraction of rows in the reconstruction matched rows in the original data --- 46\% \cite{abowdreport}. This has been dismissed as benign because the top-level statistics were not compared to 
baseline ``attacks'' that could be
performed from publicly available data, and which might have achieved similar top-level match rates \cite{rugglesreport,ruggles2022role}. 
While the criticism is simply incorrect in the case of small Census blocks~\cite{privacyexperts}, it does raise the important question of choosing appropriate baselines, and what we should measure, beyond top-level reconstruction rates, to indicate when we should view reconstruction attacks as worrisome. For example, even if we cannot reconstruct all or even most rows in a dataset, can we have confidence that some of our particular reconstructed rows are correct? If so, this is reason for concern even when top-level reconstruction rates are well below 100\%.



Thus a key aspect of our work is explicit comparison to distributional baselines that correspond to increasingly precise knowledge of the distribution from which the sensitive dataset was sampled (but not which rows specifically were sampled). We introduce a hierarchy of increasingly challenging baselines
that correspond to the ability to sample datapoints from the true distributions of individuals at successively finer geographic census units --- namely, sampling at the national, state, county,
tract and block levels. Our attacks are based on an optimization subroutine that is initialized with some seed dataset, which is then iteratively improved. Even when we start with a uniformly random initialization, our attacks consistently outperform all but the most granular baseline in terms of the number of private rows
perfectly reconstructed. 
If we instead initialize our attack at the baseline distribution to which we compare --- in accordance with
the assumption that the distribution is already publicly known --- then we consistently outperform even the most granular baselines, according to this top-line metric.
In other words, the effectiveness of our attack is significantly amplified above any baseline distribution by access to that baseline, establishing that 
%We thus establish that our attacks we are not simply benefiting from the structure in the population distributions, but 
we are significantly  
exploiting additional information revealed by the aggregate private dataset statistics.

% Moved earlier
%Our investigation goes beyond the top-line reconstruction rate of total number of rows reconstructed, providing a way to establish confidence that some particular rows correspond to correct reconstructions with high confidence. We establish the strikingand consistent empirical phenomenon that rows that appear more frequently across repeated optimizations are also more likely to be rows of the private dataset---thus allowing an attacker to build confidence in the veracity of rows that are repeatedly reconstructed, and to prioritize individuals for other forms of attack. We present a simple mathematical justification in a Bayesian framework that provides intuition for this phenomenon.

Our primary experiments are performed on synthetic U.S. Census microdata released by the Census Bureau during the development of the 2020 Census Disclosure Avoidance System (DAS), together with the queries corresponding to the actual statistics published by the U.S. Census Bureau at various levels of geographic granularity. In particular, we use queries corresponding to the same tables that the Census Bureau used in their internal reconstruction attack of the 2010 Census data~\cite{JASONPrivacyMethods2022}. 
To demonstrate the generality of our findings, we also run our reconstruction attacks on large, significantly higher dimensional datasets derived from the Census Bureau's American Community Survey, as processed via the Folktables package \cite{ding2021retiring}. In addition to demonstrating that the attacks scale to larger (and potentially more sensitive datasets), this allows us to deliberately control the queries released and study phenomena such as the effectiveness of the attack as a function of the types
and numbers of queries. 

{\bf Consequences.} This work demonstrates that the large-scale, non-convex optimization techniques we employ
can be viewed as both a ``poison'' and a ``cure''. Applied carefully to {\it noisy} statistics that ensure DP, they provide a powerful tool for sharing useful data statistics while ensuring citizen privacy \cite{ aydore2021differentially,liu2021iterative,vietri2022private} . But applied to numerically precise statistics --- even those that may provide only aggregate information about large populations --- they exfiltrate entire rows of sensitive data with confidence.

In the context of the Census, our findings yield sober warnings on the privacy risks of releasing
aggregate queries without explicit safeguards such as Differential Privacy. We note that all of our experiments were
performed on standard, commercially available consumer compute hardware,\footnote{Experiments were run using a desktop computer with an Intel Core i5-4690K processor, 16GB of RAM, and an NVIDIA GeForce GTX 1080 Ti graphics card.}
highlighting the ease and potential prevalence of such attacks. All code and data for replicating our experiments can be found here: \href{https://github.com/terranceliu/rap-rank-reconstruction}{https://github.com/terranceliu/rap-rank-reconstruction}.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \matmethods{

\ifarxiv
\section{Preliminaries}
\else
\section{Materials and Methods/Preliminaries}
\fi

In this section, we describe our algorithm, metrics and the baselines we use for comparison.

\subsection{Reconstruction Attack}\label{sec:reattack}

A {\em dataset} is a multiset of records from a discrete domain  $\cX$.  Each item in the multiset is called a {\em row}. 
We use $D\in \cX^\star$ to denote a private dataset that is the target of a reconstruction attack. A reconstruction attack takes as input aggregate statistics computed from dataset $D$ (and in the case of the attacks we present, a possibly uniformly random seed dataset),
and outputs a set of candidate rows, ranked according to the confidence of appearing in $D$. This \textit{confidence-ordered} set of rows is denoted $R=\{R_i\in \cX\}$, where the index~$i$ in $R$ determines the confidence ranking\footnote{Thus $R_1$ corresponds to the row that we are most confident in, $R_2$ the row that we have the next most confidence in, and so on.}.  Our rankings will be obtained from attacks that produce a multiset $X$ of rows. Elements appearing in~$X$ are then ordered according their frequency in the multiset.  We let $R(X)$ denote the resulting ordered set (not multiset) of rows. 


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Describe match-rate
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
To measure the performance of a reconstruction attack, we introduce the following metric that measures its accuracy at different confidence thresholds. For any private target dataset  $D$, the \emph{top-$k$ match-rate} of the confidence set $R$ is the fraction of rows ranked from $1$ to $k$ by $R$ that actually appear in $D$.
%
\begin{align}\label{eq:matchrate}
    \matchrate_{D,k}(R) \leftarrow \frac{1}{k}\sum_{i=1}^k \indicator{R_i\in D}
\end{align}
%
We can plot  $\matchrate_{D,k}(R)$ as a function of $k$ which traces out a curve --- in general, if our confidence set has its intended semantics (that higher ranked rows are more likely to appear in $D$), then the curve should be monotonically decreasing in $k$. For a given level $k$, a higher match rate corresponds to higher confidence that rows ranked within the top $k$ are correct reconstructions; at a given match rate, higher values of $k$ correspond to the ability to confidently reconstruct more rows. 



\subsection{Reconstruction From Aggregate Statistics}\label{sec:RAP}

We design a reconstruction attack that starts from a collection of aggregate statistics computed from the private dataset. A statistical query is a function that counts the fraction of rows in a dataset that satisfy a given property. We give a formal definition here:

\begin{definition}[Statistical Queries \cite{SQ}] Given a function $\phi \colon \cX\rightarrow \{0,1\}$, a statistical query (also known as a linear query or counting query) $q_\phi$ is defined as  
$q_\phi(D) = \frac{1}{|D|} \sum_{x\in D} \phi(x)$, for any dataset $D$. 
\end{definition}



We use $Q$ to denote a set of $m$ statistical queries and $Q(D) \in \mathbb{R}^m$ denote the vector of statistics on the dataset $D$. The objective of an attack on $D$ is to reconstruct rows of $D$ given $Q$ and $Q(D)$.



We propose a new reconstruction attack mechanism \rapattack~that learns rows of the unknown dataset $D$ from statistics $Q(D)$. 
\rapattack{} leverages the recent optimization heuristic Relaxed Adaptive Projection (\rap) \cite{aydore2021differentially} for synthetic data generation. {\rap} is a randomized algorithm that takes as input a collection of $m$ statistical queries $Q$ and answers $Q(D)\in \mathbb{R}^m$ (derived from some dataset $D$), 
and outputs a dataset $D'$ by attempting to solve the following optimization objective: 
%
\begin{align}\label{eq:rapobj}
   \text{arg}\min_{D'}\| Q({D'}) - Q(D)\|_2
\end{align}
using a randomized continuous optimization heuristic.
\rap{} is initialized with parameter $\theta$, discussed below.  Roughly speaking, $\theta$ captures some additional distributional information available to the attacker.  In our work, this will either be a uniformly randomly generated dataset of a given schema (corresponding to no additional information) or a dataset of this schema sampled from a prior distribution related to the distribution from which $D$ was drawn; more on this below.
The notation $\uplus$ is used to indicate union with multiplicities.  For example, if $x$ appears 2 times in $D'_1$ and 1 time in $D'_2$, then it appears 3 times in $D'_1 \uplus D'_2$. 

Our method, \rapattack{}, described in Algorithm~\ref{alg:rapattack},
consists of running \rap{} for $K$ times to produce datasets $D'_1, \ldots, D'_K$ and outputting the confidence set $R(\biguplus_{k=1}^{K} D'_k)$.


\begin{algorithm}[!tbh]
\begin{algorithmic}
\State \textbf{Input:} A set of queries $Q$ and their evaluations $Q(D)$ on some private dataset $D$.  
\State \textbf{Parameters:} number of runs $K$ 
\For{$k = 1, \ldots, K$}
    % \State Randomly initialize \rap with parameters $\theta_k$ 
    \State Initialize \rap 's parameter $\theta_k$ (either uniformly or to a dataset sampled from a prior distribution).
    % \State Output $D_k' \leftarrow \rap(D, Q, \gamma_k)$ by solving [\cref{eq:rapobj} via stochastic gradient descent.\gv{Original }
    \State {Output  $D_k'\sim \rap(Q, Q(D), \theta_k)$ by solving Eq.~[\ref{eq:rapobj}] via stochastic gradient descent.}
    % \State Sample a synthetic dataset $D'_k \sim P'_k$ 
\EndFor
\State Let $D^* = \biguplus_{k=1}^{K} D'_k$ %(the union of multisets is taken with multiplicity)
\State \textbf{Output:} Confidence set $R(D^*)$ 
\end{algorithmic}
\caption{Overview of \rapattack}
\label{alg:rapattack}
\end{algorithm}




The \rap{} algorithm maintains a parameterized distribution over datasets that it can use to produce data samples to form synthetic datasets. The goal of the \rap{} algorithm is to find a set of parameters that correspond to synthetic data that minimizes the objective in Equation [\ref{eq:rapobj}].
Since the optimization problem in Equation [\ref{eq:rapobj}] is discrete (which makes it difficult to solve), the \rap{} algorithm considers a continuous relaxation of objective Equation [\ref{eq:rapobj}], that is differentiable in the internal parameters of {\rap},  enabling the use of continuous differentiable optimization techniques which are highly effective in practice. 

\rap~is initialized with a parameter $\theta$ that is defined over a domain that is a continuous relaxation of the schema of the dataset $D$. So we can initialize \rap~at a dataset in the same schema as $D$. In this work, we initialize RAP either at a uniformly random dataset, or at a dataset $D$ drawn from a prior distribution $P$ that will represent sampling Census data at various geographic resolutions.



Although the performance of \rapattack~as measured by \matchrate~is an empirical finding, \rapattack~is a theoretically motivated heuristic. In particular, if we imagine that when \rapattack~is initialized at a sample $\theta$ from a prior distribution on datasets, it samples a dataset $D'_k$ from the posterior distribution on datasets given the statistics $Q(D)$, then the ranking it constructs would be the correct ranking of points by their (posterior) likelihood of appearing in the true dataset $D$. We briefly elaborate on this theoretical intuition in the next section. 


   \subsection{Some Theoretical Intuition}\label{sec:bayes}

There is a simple Bayesian argument that provides some  intuition for our resampling method for confidently reconstructing rows of the true, private dataset $D$. Let $P$ be some prior distribution over all datasets with the same format or schema as $D$. For instance, $P$ could simply be uniform over all datasets with the same schema as $D$, but any $P$ suffices in the argument that follows. Let us assume that the true $D$ is drawn according to this prior (denoted $D \sim P$), and we are given some queries $Q$ as well as their numerical values on $D$, denoted $Q(D)$. Suppose we imagine that when we initialize \rap~at a sample drawn from the prior $P$ and run it once, the resulting reconstructed dataset $D'$ is a sample from the posterior distribution given the computed statistics: $D' \sim P|Q,Q(D)$. How could we use the ability to sample such datasets $D'$ to estimate the probability that particular points $x$ are elements of $D$?

More generally, let $\chi(D,D')$ be any random variable determined by the draws $D \sim P, D' \sim P|Q,Q(D)$ --- for instance, a natural $\chi(D,D')$ for our purposes would take value equal to 1 if both $D$ and $D'$
contain some particular row $r$, and 0 otherwise. 

The attacker is interested in the expectation
\begin{equation} \label{firstexp}
    \mathbb{E}_{D \sim P, D' \sim P|Q,Q(D)}[\chi(D,D')]
\end{equation}
which in the example above is simply the probability that both $D$ and $D'$ contain the row r. The difficulty is that although given $Q(D)$, we have assumed that we can take samples $D' \sim P|Q,Q(D)$, we cannot evaluate the predicate $\chi(D,D')$ because we do not have access to the true dataset $D$ from which the statistics $Q(D)$ were computed. 

However, it is not hard to derive that this expectation is identical to:
\begin{equation} \label{secondexp}
    \mathbb{E}_{D \sim P} \left[\mathbb{E}_{ \tilde{D} \sim P|Q,Q(D),  D' \sim P|Q,Q(D)}\left[\chi(\tilde{D},D')\right]\right]
\end{equation}
In other words, rather than computing $\chi(D, D')$ we can instead compute $\chi(\tilde D, D')$ where $\tilde D$ and $D'$ are both independent samples from the posterior distribution $P|Q,Q(D)$ --- i.e. under our assumption, two reconstructions that result from running $\rap$~twice with fresh randomness. The reason for this equivalence is that in the two expectations above, the joint distributions of $\langle D, Q(D), D' \rangle$ and $\langle \tilde{D}, Q(D), D' \rangle$ are identical, since $D$ and $D'$ are conditionally independent given $Q(D)$, and both are distributed according to $P|Q,Q(D)$.


In other words, if we wish to estimate the expectation [\ref{firstexp}], we can do so by instead estimating the expectation [\ref{secondexp}], which involves evaluating the predicate only on datasets drawn from the posterior rather than the prior. Concretely, rows that are more likely to appear in two or more draws from the posterior are also more likely to appear in $D$ drawn from the prior and $D'$ from the posterior. To the extent that our resampling method on $D'$ successfully approximates repeated draws from the posterior given $Q(D)$, it ranks rows $r$ in decreasing order of their value of the inner expectation in [\ref{secondexp}] --- i.e. their posterior likelihood of being true rows of the original $D$. Thus if we believe that {\rap},  when initialized at a draw from the prior distribution simulates a draw from the posterior distribution, the above argument explains why the ranking $R(D^*)$ should correspond to an ordering of data points by their probability of appearing in the true dataset $D$. In general sampling from a posterior in a space of high-dimensional datasets and queries is a computationally intractable problem  \cite{pmlr-v99-tosh19a}, but this does not rule out effective heuristics on real datasets, and we believe that this Bayesian argument provides at a minimum some insight about why methods such as ours work well in practice.




\section{Empirical Findings}

In this section we describe our primary experimental findings. Additional and more fine-grained results
are provided in the appendix, including plots of \matchrate~for all 50 states on the Census data.

\subsection{Datasets and Queries}\label{sec:datasetandqueries}

\subsubsection{U.S. Decennial Census}

\paragraph{Dataset:}
We conduct experiments on subsets of synthetic U.S. Census microdata released by the Census Bureau during the development of the 2020 Census Disclosure Avoidance System (DAS).
This synthetic microdata was generated so that it has similar statistics when compared to the real 2010 Census microdata.
We use the 2020-05-27 vintage Privacy-protected Microdata File (PPMF) \cite{ppmf20200527vintage}.
In our experiments, we treat the PPMF as the ground truth microdata, even though it is synthetic, since the true microdata has never been released.

The 2020-05-27 vintage PPMF consists of 312,471,327 rows, each representing a (synthetic) response for one individual in the 2010 Deccenial Census.
The columns correspond to the following attributes: The location of the respondent's home (state, county, census tract, and census block), their housing type (either a housing unit, or one of 8 types of group quarters), their sex (male or female), their age (an integer in $\{0, \dots, 115\}$), their race (one of the 63 racial categories defined by the U.S. Office of Management and Budget Standards)\footnote{The 63 race categories correspond to any non-empty subset of the following: American Indian or Alaska Native, Asian, Black or African American, Native Hawaiian or Other Pacific Islander, White, and Other.}, and whether they have Hispanic or Latino origin or not.

We evaluate reconstruction attacks on subsets of the PPMF that contain all rows belonging to a given census tract or census block.
% The following line comes from https://www.census.gov/programs-surveys/geography/about/glossary.html#par_textimage_13.
According to the U.S. Census Bureau, census tracts typically have between 1,200 and 8,000 people with an optimum size of 4000, and cover a contiguous area (although their geographic sizes vary widely depending on population density).
Each census tract is partitioned into up to 10,000 census blocks, which are typically small regions bounded by features such as roads, streams, and property lines.

In our tract-level experiments, we randomly select one tract from each state. In our block-level experiments, we select for each state the block closest in size to mean block size as well as the largest block. In addition, we select blocks closest in size to $M / C$, where $M$ is the maximum block size in the state and $C \in \{2, 4, 8, 16\}$. Thus in total, we evaluate on $50$ tracts and $300$ blocks.

\paragraph{Statistical Queries:} 
The U.S. Census Bureau publishes a collection of data tables containing statistics computed from the microdata at various levels of geographic granularity.
For example, some tables are published at the block level, meaning that they release a copy of that table for every census block in the U.S., while others are published at the census tract or county level.
Our experiments attempt to reconstruct the microdata belonging to census tracts and blocks based on statistics contained in the Census tables.

We use the same tables that the Census Bureau used in their internal reconstruction attack of the 2010 Census data~\cite{JASONPrivacyMethods2022}.
These are the following tables from the Census Summary File~1:\footnote{Summary File 1 has been renamed to the Demographic and Housing Characteristics File (DHC) for the 2020 U.S. Census. In all cases, we refer to tables and data products by their names used in the 2010 Census.}
\begin{description}[nosep,labelindent=0.3cm]
    \item[P1:] Total population,
    \item[P6:] Race (total races tallied),
    \item[P7:] Hispanic or Latino origin by race (total races tallied),
    \item[P9:] Hispanic or Latino and not Hispanic or Latino by race.
    \item[P11:] Hispanic or Latino, and Not Hispanic or Latino by race for the population 18 years and over,
    \item[P12:] Sex by age for selected age categories (roughly 5 year buckets),
    \item[P12 A-I:] Sex by age for selected age categories (iterated by race).
    \item[PCT12:] Sex by single year age.
    \item[PCT12 A-N:] Sex by single year age (iterated by race).
\end{description}
All of the P tables are released at the block level, while the PCT tables are released only at the census tract level.

Each table defines a collection of statistical queries that will be evaluated on the Census microdata.
For example, cell 3 of table P12 counts the number of male children under the age of 5.
Since P12 is a block-level table, cell 3 corresponds to one statistical query per census block.
Similarly, each cell in a tract-level table encodes one statistical query per census tract.
All of the statistical queries in the above tables can be encoded as follows: Given $k$ pairs $(\text{col}_i, S_i)_{i=1}^k$, where $\text{col}_i$ is a column name and $S_i$ is a subset of that column's domain, and either a census block or tract identifier,
count the number of microdata rows belonging to that tract or block for which $\text{col}_i \in S_i$ for all $i \in [k]$.\footnote{The Census tables report row counts, but in our experiments we convert counts to fractions by dividing by the population of the tract or block we are reconstructing.}
Thus in logical terms, queries are in Conjunctive Normal Form (CNF), meaning that they consist of a conjunction (logical AND) of clauses, with
each clause being a disjunction (logical OR) of allowed values for a column.

For example, cell 3 of table P12 encodes queries for each block with $\text{col}_1 = \text{Age}$, $S_1 = \{0, \dots, 4\}$, and $\text{col}_2 = \text{Sex}$, $S_2 = \{\text{Male}\}$.
When we perform tract-level reconstructions, we use queries defined by all of the above tables.
For block-level reconstructions, we use only the block-level tables (i.e., excluding tables PCT12 and PCT12 A-N).
In order to minimize the total number of queries, we omit several table cells that are either repeated or can be computed as a sum or difference of other table cells.

The statistical queries encoded by the Census data tables vary significantly in the value of $k$ (number of clauses in a conjunction) and the size of the sets (clauses) $S_i$. There are 2 cells with $k = 0$ (total population at the block and tract level), 27 cells with $k = 1$, 352 cells with $k = 2$, 1915 cells with $k = 3$, and 1259 cells with $k = 4$. 
The size of the sets $S_i$ range from 1 to 98.

To verify the correctness of our implementation of the statistical queries from the tables above, we compared the output of our implementation to tables released by the IPUMS National Historical Geographic Information System (NHGIS). 
For each vintage of the PPMF released by the U.S. Census Bureau, the NHGIS computes the census tables from that PPMF vintage.\footnote{The NHGIS tables constructed from each PPMF vintage are available \href{https://www.nhgis.org/privacy-protected-2010-census-demonstration-data}{here}.}
We compared our implementations of queries from tables P1, P6, P7, P9, P11, P12, and P12 A-I on all census blocks in the United States and Puerto Rico and found no discrepancies.
Unfortunately, the PCT12 and PCT12 A-N tables were not included in the NHGIS tabulations for the 2020-05-27 vintage PPMF, so we were unable to verify our implementation of these queries (but their structure is very similar to the block-level queries).

\subsubsection{American Community Survey (ACS)}
\paragraph{Dataset}
We conduct additional experiments on a suite of datasets derived from US Census, introduced in \cite{ding2021retiring}.\footnote{The Folktables package comes with MIT license, and terms of service of the ACS data can be found here: \href{https://www.census.gov/data/developers/about/terms-of-service.html}{https://www.census.gov/data/developers/about/terms-of-service.html}.}
The Folktables package defines datasets for each of 50 states and various tasks. 
Each task consists of a subset of columns\footnote{A detailed list of the attributes can be found in the \ifarxiv Appendix \else SI \fi (Table \ref{tab:folktable_columns}). Note that we discretize numerical columns into 10 equal-sized bins.} from the American Community Survey (ACS) corpus. These datasets provide a diverse and extensive collection of datasets helpful in experimenting with practical algorithms. 
We use the five largest states (California, New York, Texas, Florida, and Pennsylvania) which together with the three tasks (employment, coverage, mobility) constitute 15 datasets. Our experiments therefore seek to reconstruct individuals at the state-level. 
Compared to datasets derived from the Census Bureau's May 2020 Demonstration Data Product (PPMF), based on the 2010 Census, the Folktables ACS datasets contain many more attributes (see Table \ref{tab:folktables}), helping us demonstrate how our reconstruction attack scales up to higher dimensional datasets.

We note that while the datasets distributed by the Folktables package are derived from the ACS microdata, the package was designed for evaluating machine learning algorithms, and there exist many differences from the actual 1-year and 5-year statistical tables released by the Census Bureau each year. As mentioned above, each task only contains a subset of features collected on the ACS questionnaire and released in the 1-year Public Use Microdata Sample (PUMS). Moreover, survey responses are collected at both the household and person-level, but Folktables treats records only at the person-level. Lastly, in the ACS PUMS, each survey response is assigned a sampling weight, which can then be used to calculate weighted statistics (e.g., estimated population sizes and income percentiles) that estimate population-level statistics. Folktables ignores these weights, and so the statistics we calculate and use for experiments are unweighted tabulations. Folktables also ignores the replicate weights on the ACS PUMS that the Census Bureau recommends users implement to generate measures of uncertainty associated with the weighted statistics.

\paragraph{Statistical Queries} For each ACS dataset we compute a set of  $k$-way marginal statistics. A  marginal query counts the number of people in a dataset whose features match a given value. An example of a $2$-way marginal query  is: "How many people are \emph{female} and and have \emph{income greater than 50K}". The formal definition is as follows:
 \begin{definition}[$k$-way Marginal Queries]\label{def:marginals} Let $\cX=\prod_{i\in[d]}\cX_i$ be a discrete data domain with $d$ features, where $\cX_i$ is the domain of the $i$-th feature.
A $k$-way \textit{marginal query} is defined by a set of $k$  features $S\subseteq[d]$, together with a target value $v\in \prod_{i\in S} \cX_i$ for each feature in $S$.  Given such a pair  $(S, v)$, let $\cX(S, v)=\{x \in \cX : x_i = v_i \forall i\in S \}$ denote the set of points in $\cX$ that match $v$ on each feature $i\in S$.   Then consider the function $\phi_
{S, v}$ defined as $\phi_{S,v}(x) = \mathbbm{1}\{x\in \cX(S, v)\}$,  where $\mathbbm{1}$ is the indicator function.
The corresponding $k$-way marginal query is the statistical query defined as 
$$q_{S,v}(D) = \frac{1}{|D|}\sum_{x\in D} \phi_{S, v} (x)$$
for any dataset $D$.
\end{definition}

We explore the efficacy of our reconstruction attack on ACS datasets when all $2$-way or $3$-way marginal queries are released.



\begin{table}[htbp]
    \centering
    \caption{For each Folktables task, we list the number attributes, total dimension of such attributes, and the number of all 2 and 3-way marginal queries.}
    \begin{tabular}{l|rrrr}
    \toprule
           Task &  \# Attr &  Dim &  \# 2-way &  \# 3-way \\
    \midrule
     employment &      16 &  108 &     5154 &   144910 \\
       coverage &      18 &  107 &     5160 &   149848 \\
       mobility &      21 &  141 &     9137 &   362309 \\
    \bottomrule
    \end{tabular}
    \label{tab:folktables}
\end{table}

\subsection{Baselines}\label{sec:baseline}


In isolation, the \matchrate~of \rapattack~described in the previous section cannot provide enough information to indicate a privacy breach. If the dataset distribution is very low entropy, and we know the distribution, then we might expect to obtain a high \matchrate~simply by randomly guessing rows that are likely under the data distribution.  Therefore, we would like to compare the \matchrate~of our attack to the \matchrate~of baselines of various strengths corresponding to increasingly precise knowledge of the data distribution. 

Given a baseline distribution $P$, we consider a \matchrate~baseline that results from ordering the rows of $\cX$ according to their likelihood of appearing in a randomly sampled dataset $D\sim P$. In practice, the domain size $|\cX|$ is often too large to enumerate; an alternative in this case is to sample a large collection of rows $X \sim P$ and then compare to the confidence set $R(X)$---i.e. the ranking that results from ordering rows by their likelihood in the empirical distribution over $X$, sampled from $P$.

We compare to different baselines corresponding to a set of increasingly informed prior distributions. First, in order to simulate a prior that is identical  to the distribution from which the private dataset is sampled, we randomly partition the real dataset into two halves $D$ and $D_{holdout}$. We treat $D$ as the private dataset which we compute statistical queries on and seek to reconstruct rows from, while $D_{holdout}$ is used to produce a baseline confidence set $R(D_{\textrm{holdout}})$. Here, by construction, $D$ and $D_{holdout}$ are identically distributed, which allows us to compare to the very strong baseline of the ``real'' sampling distribution for real datasets. Of course, as a synthetic construction originating from the real data, $R(D_{holdout})$ should generally be viewed as an unrealistically strong benchmark. 

We also compare to a natural hierarchy of benchmarks that correspond to fixing a prior based on knowledge of Census data at different levels of granularity.  U.S. Census data is organized according to geographic entities that have a hierarchical structure. We consider a natural hierarchy of prior distributions in which a lower level in the hierarchy is more informative than higher levels. For example, for block-level reconstruction, we consider benchmarks defined by sampled rows from the tract, county, and state ($D_{\textrm{tract}}$, $D_{\textrm{county}}$, $D_{\textrm{state}}$) that each block is contained in, as well as the benchmark defined by samples from all rows in the dataset ($D_{\textrm{national}}$). We note that in block level reconstruction experiments, $D_{\textrm{holdout}}$ corresponds to a block-level prior, and so we refer to this set of rows as $D_{\textrm{block}}$ in Section \ref{sec:results}. Similarly for tract-level reconstruction experiments, $D_{\textrm{holdout}}$ is referred to as $D_{\textrm{tract}}$.

As we describe in more detail in Section \ref{sec:results}, we run reconstruction of Census tracts both with and without the attribute corresponding to the block each individual resides in. For the setting in which the block attribute is included, the county, state, and national baselines are at an extreme disadvantage, since the majority of individuals in $D_{\textrm{county}}$, $D_{\textrm{state}}$, and $D_{\textrm{national}}$ reside in a tract different than those found in $D$---and so necessarily have different block values. To compensate for this (otherwise crippling to the baselines) disadvantage, in these cases we strengthen the baselines and instead populate the block attribute according to the distribution of blocks found in $D$. For example, the state-level baseline can be interpreted as a prior in which the distribution of blocks follows that of $D$ and the distribution of the remaining attributes follows that of $D_{\textrm{state}}$.

\subsection{Results}\label{sec:results}

Our primary reconstruction rate visualization technique is as follows. 


Recall both \rapattack~and our baselines each output some confidence set $R$. Therefore, for both \rapattack~and our baselines, we plot $\matchrate_{D,k}(R)$ against $k$, or in other words, the fraction of candidates of rank $k$ or higher that exactly match some row in $D$.
Because the many datasets on which we run our reconstruction attack vary considerably in size, and in some of our plots we average our results over many datasets, in the ensuing plots we express rank $k$ as a fraction of the number of unique rows in $D$, which we denote as $u$. In other words, the $x$-axis measures ${k}/{u}$. 
This allows us to average results across different samples of data (e.g., different geographic entities for both Census and ACS experiments) on a common scale for the $x$-axis. 



\begin{figure}[htbp]
    \centering
    \ifarxiv
    \includegraphics[width=0.80\columnwidth]{images/census/avg_tract.png}
    \else
    \includegraphics[width=\columnwidth]{images/census/avg_tract.png}
    \fi
    \caption{The panel on the left plots the \matchrate~of \rapattack~and our various baselines on a tract level reconstruction when we use the BLOCK attribute. The panel on the right plots the \matchrate~of \rapattack~and our various baselines when the BLOCK attribute has been removed. Both panels show the average performance of \rapattack~and the baselines averaged over a randomly selected tract from each of the 50 US states. In both cases, \rapattack~is initialized uniformly at random (i.e. we have not initialized at a baseline distribution).}
    \label{fig-main:census_avg_tract}
\end{figure}

\begin{figure}[htbp]
    \centering
    \ifarxiv
    \includegraphics[width=0.6\columnwidth]{images/census/tract/avg_init.png}
    \else
    \includegraphics[width=0.7\columnwidth]{images/census/tract/avg_init.png}
    \fi
    % \includegraphics[width=\columnwidth]{images/census/avg_tract_init.png}
    \caption{Initializing \rapattack~at the tract baseline significantly improves its performance, leading it to out-perform the tract baseline. Here the BLOCK attribute is included and must be reconstructed to constitute a match.}
    \label{fig-main:census_avg_tract_init}
\end{figure}

\begin{figure}[htbp]
    \centering
    % \includegraphics[width=\columnwidth]{images/census/block/avg.png}
    \ifarxiv
    \includegraphics[width=0.9\columnwidth]{images/census/avg_block_both.png}
    \else
    \includegraphics[width=\columnwidth]{images/census/avg_block_both.png}
    \fi
    \caption{The panel on the left plots the \matchrate~of \rapattack~and our various baselines on a block level reconstruction, when \rapattack~is initialized to a uniformly random dataset. The panel on the right shows the performance of \rapattack~when it is initialized to $D_{block}$, and compares its performance to $D_{block}$. }
    \label{fig-main:census_avg_block}
\end{figure}

\begin{figure*}[htbp]
    \centering
    \includegraphics[width=0.8\textwidth]{images/folktables/avg.png}
    \caption{We select state-task combinations from the folktables dataset, comparing top $k$ candidate \matchrate~ of \rapattack~ against the baseline, which is derived from the holdout split. For each task, we average results over the five largest states (by population) in the United States (i.e., California, New York, Texas, Florida, and Pennsylvania). We initialize \rapattack~ randomly, showing results where $Q$ is all $2$ and $3$-way marginals. }
    \label{fig-main:folktables_avg}
\end{figure*}

In our first set of experiments, we randomly select a tract from each state, which forms the private dataset $D$ from which we compute the Census query-answer pairs. 
We run {\rapattack} using these queries and starting from a uniformly random initialization,\footnote{We shortly describe a natural and realistic alternative initialization
scheme that improves performance considerably.} and plot the match rate as a function of $k/u$. We similarly plot the match rate of each of our baselines. 
In the left panel of Figure~\ref{fig-main:census_avg_tract}, we plot the reconstruction rates after averaging across the selected tract from all 50 states.
(See Figures \ref{fig-appx:census_tract_all1}, \ref{fig-appx:census_tract_all2}, \ref{fig-appx:census_tract_ib_all1}, and \ref{fig-appx:census_tract_ib_all2} in the \ifarxiv appendix \else SI \fi for the state-by-state plots that comprise this average.)

As expected, in general at higher ranks (lower $x$-axis values) the reconstruction rates are reasonably high and then fall
at lower ranks.  The left panel shows that the \rapattack~reconstruction rates are considerably higher than all but the
strongest baseline --- resampling at the tract level --- which is much higher still. Recall that since this is a tract-level reconstruction, $D_{tract}$ here is in fact $D_{holdout}$ --- i.e. the very strong artificial benchmark constructed from the dataset we are attacking itself. We see that the other baselines---$D_{county}, D_{state}$, and $D_{national}$ perform quite poorly. This is partially an artifact of requiring that they reconstruct the BLOCK attribute. Since blocks appearing within a tract appear in no other tracts, the non-tract baselines have a poor chance of reconstruction since they are sampling at a coarser geographic level. Recall that we have strengthened these baselines by letting the BLOCK attribute be distributed according to the empirical distribution of blocks in the true dataset $D$, but still, these baselines are at a disadvantage because they have lost the correlation between the BLOCK attribute and all other features. 
Therefore in the right panel of Figure \ref{fig-main:census_avg_tract}, we reproduce the same experiment in which we have dropped the BLOCK attribute. This makes the reconstruction task easier and improves the performance of \rapattack~as well as all of the baselines. The most dramatic increase is in the performance of the $D_{county}, D_{state}$, and $D_{national}$ baselines, but we also see that \rapattack~now performs relatively better compared to the $D_{tract}$/$D_{holdout}$ baseline.  \rapattack~now has reconstruction rates above 0.9 up to $k/u \approx 0.5$.
These results establish that \rapattack~can perfectly reconstruct rows well beyond what sampling access alone permits except at the
most local level. In other words, \rapattack~is far from 
simply ``getting lucky'' --- its optimization process is deliberately and effectively exploiting the actual query-answer pairs,
not simply benefiting from having data similarly distributed to the private dataset. Nevertheless, the ordering of the baselines and of \rapattack~is unchanged --- i.e. \rapattack~outperforms all of the baselines \emph{except} for the artificial $D_{tract}$ baseline. 


We next observe that there is an asymmetry in our experiments that treats \rapattack~in what could be considered an unfair manner: we assert that there are strong ``baseline'' distributions that are related to the data we are trying to reconstruct, and yet we have initialized our attack \rapattack~at a uniformly random dataset, without giving it the benefit of this knowledge. If indeed these baseline distributions are public knowledge, then an attacker could make use of them as well. Thus our next set of experiments consists of initializing \rapattack~at the baseline that we are comparing it to, and see that this causes it to significantly outperform all baseline---including $D_{tract}$ (which we recall is the strong baseline $D_{holdout}$), even with the BLOCK attribute. In other words, if we view the baseline as a public prior distribution, then giving \rapattack~access to it leads to the ability to significantly improve over it. 

In  Figure~\ref{fig-main:census_avg_tract_init}, we show results averaged across randomly chosen
tracts for all 50 states in which we have now initialized to the tract baseline and compare to that sampling baseline, (once again including the BLOCK feature).
The results are clear: when we level the playing field by seeding \rapattack~with knowledge of the tract baseline distribution,
it now outperforms the tract baseline. We can interpret the area between the two curves in Figure~\ref{fig-main:census_avg_tract_init}
as a measure of the additional reconstruction risk introduced by \rapattack~on the query-answer pairs, beyond the baseline
risk of tract sampling. 

In Figure~\ref{fig-main:census_avg_block}, we show that \rapattack~remains an effective reconstruction attack even at the
most fine-grained geographic level, which corresponds to Census blocks. The left panel again shows \matchrate~for \rapattack~initialized randomly, and compared to all the sampling baselines. Here we again see the same qualitative performance --- even with random initialization, \rapattack~outperforms all of the sampling baselines except for constructed $D_{block}$ (which we recall in this case is the artificially constructed  $D_{holdout}$). The right panel shows results when we initialize \rapattack~at $D_{block}$. In this case we again see that initializing at the benchmark distribution causes $\rapattack$ to significantly outperform the benchmark. This figure is again averaging over attacks on blocks from all 50 states. (See Figures \ref{fig-appx:census_block_all1} and \ref{fig-appx:census_block_all2} in the \ifarxiv appendix \else  SI \fi for the state-by-state plots that comprise this average.)


We conclude by briefly describing a second set of experiments on three datasets from the ACS Folktable package, corresponding
to the employment, coverage and mobility tasks. We consider these alternate datasets both to show the generality of our methods beyond
decennial Census data (in particular, the ACS Folktables datasets have much higher dimensionality than the decennial Census data), and in order to do a controlled comparison of queries of differing power (as opposed to the fixed set of queries provided for
the decennial Census data).

In Figure~\ref{fig-main:folktables_avg}, we show the reconstruction rates obtained by letting the query set $Q$ be the sets of all
2-way and 3-way marginal queries on these three ACS datasets, and as in the Census data we compare to the very strong $D_{holdout}$ baseline. 
Two remarks are in order. First, despite the low complexity of these queries compared to Census queries --- 2-way and 3-way marginals reference only pairs and 
triples of columns, respectively --- both considerably outperform the $D_{holdout}$ baseline even when \rapattack~is initialized randomly, maintaining reconstruction rates well above 0.8
even at the lowest rank. This suggests that not only is aggregation insufficient
for privacy, neither is restriction to simple queries. In fact, on this dataset, and with these simple queries, our reconstruction attack performs even better---outperforming the strongest baseline even without the benefit of being initialized at that baseline. 

Second, the lift in performance in moving from 2-way marginals to 3-way marginals is large,
demonstrating the reconstructive power of even slightly more complex queries. 



\section{Limitations and Conclusions}
We have shown the power of a new class of reconstruction attacks that can not only produce a candidate reconstructed dataset with a high intersection with the  true dataset, but also produce a ranking of rows that empirically corresponds to their likelihood of appearing in the true dataset. We have shown that from statistics that were actually released as part of the 2010 Decennial U.S. Census, it is possible to run our attack and that its \matchrate~ is high --- particularly at lower values of $k$, indicating high confidence reconstruction of a subset of the rows. Moreover, even with random initialization (equivalently, viewing \rapattack~as having an uninformative prior), \rapattack~outperforms all but the most stringent (artificial) benchmark that we construct. Finally, we can reliably outperform even the most stringent benchmark if we initialize \rapattack~at the benchmark distribution---consistently with the premise that if a distribution is publicly known (and so is sensible to consider as a public benchmark), then we should assume that attackers can make use of it as well. 

Nevertheless, our attack is not without limitations. First and foremost, our reconstructions of Census decennial data are far from recovering every row in the private data. The primary threat is that we can recover \emph{some fraction} of the rows with confidence. Moreover, our attack does not produce calibrated confidence scores. That is, we produce a ranking of rows $R$, but an attacker without access to the ground-truth would be unable to compute the \matchrate~as a function of $k$ as we do in our plots, and so would not know a-priori how much confidence to put in each reconstructed row. Nevertheless, a ranking (known to be empirically correlated with \matchrate) is sufficient for an attacker to prioritize the rows of a reconstruction for some other external validation procedure or attack. 


\newpage

% \showmatmethods{} % Display the Materials and Methods section


% \acknow{Please include your acknowledgments here, set in a single paragraph. Please do not include any acknowledgments in the Supporting Information, or anywhere else in the manuscript.}

% \showacknow{} % Display the acknowledgments section

% Bibliography
\ifarxiv
\bibliographystyle{alpha}
\else
\fi

\bibliography{pnas-sample}

\input{docs/appendix.tex}

% \input{docs/all_plots.tex}

\end{document}
