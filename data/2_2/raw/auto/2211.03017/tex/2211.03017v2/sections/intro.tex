\section{Introduction}
Inverse rendering of complex indoor scenes has been a long-standing challenge in computer graphics and vision. Given a single real-world image, global illumination effects such as shadows, specular highlights, and glossy interreflections are baked into the observed pixel values, imposing a particularly difficult task of simultaneously recovering the underlying scene geometry, spatially-varying surface reflectance, and arbitrary unknown illumination. Traditional optimization-based approaches rely on dedicatedly designed regularization and hand-crafted priors to tackle this problem. Unfortunately, such methods often fail in real-world scenarios due to overly simplified assumptions, leading to noticeable artifacts in both the decomposition and re-rendered results.

On the other hand, recent advances in inverse rendering~\cite{wang2021learning,li2020inverse,srinivasan2020lighthouse} leveraging deep learning methods have demonstrated impressive results on such scene inference tasks, where the underlying physical priors are supposed to be learnt automatically through an offline, supervised training process, typically on a large-scale, synthetic or labeled real training dataset of complex indoor scenes. Note that, it is extremely difficult, if not impossible, to generate ground truth labels of spatially-varying illumination and materials of an arbitrary real-world scene, and hence one crucial keypoint to the success of such methods is the fidelity and photorealism of the synthetic training data. Another essential factor strongly influencing the inference accuracy is the network structure design. Intuitively speaking, since inverse rendering is inverting the physical light transport, a physically-based differentiable rendering layer regularizing the parameter space can act as a  meaningful prior, improving the robustness and generalization capability of the neural network regarding material and lighting decomposition, and thus also the geometry estimation in return. Consequently, the performance of these learning-based inverse rendering methods heavily depends on: 1) the quality of the training datasets, and 2) the design of the neural network architecture. 

To address the aforementioned challenges, we propose a novel Monte Carlo differentiable rendering layer with importance sampling to faithfully simulate the physical light transport of an indoor scene. Experiments show that this is especially helpful in restoring the specular reflections of a given scene, and our method produces much more realistic re-rendered results comparing previous baselines. Unlike previous work that directly uses the local feature at a ray-surface intersection point, our approach importance samples the local incident radiance field of it via screen space ray tracing (SSRT) and uncertainty-aware, hypernetwork-based out-of-view lighting estimation. To facilitate training, we introduce a large-scale (${\sim} 4000$) complex indoor scene dataset, \textsc{InteriorVerse}. As far as we know, our dataset contains the highest quality with rich details compared to existing indoor scene datasets (e.g., OpenRooms~\cite{li2021openrooms} or SUNCG~\cite{song2017semantic}), including complex furniture and dedicated decorations procedurally designed by professional digital artists, rendered with physically-based GGX model~\cite{walter2007microfacet} using a modern GPU-based path tracing engine.

Concretely, our contributions include: 
\begin{itemize}
    \item A learning-based monocular inverse rendering framework of complex indoor scenes that recovers albedo, surface normal, depth, metallic, and roughness from a single indoor scene image.
    \item A novel Monte Carlo differentiable rendering layer with importance sampling, which correctly estimates the local incident radiance field using screen space ray tracing.
    \item An uncertainty-aware out-of-view light network leveraging hypernetwork-based neural radiance fields for robust out-of-view lighting estimation.
    \item A high-quality, large-scale complex indoor scene dataset, \textsc{InteriorVerse}, that contains rich details with high fidelity.
\end{itemize}
% To validate our proposed inverse rendering pipeline, we compare with existing state-of-the-art baselines on a set of common benchmark datasets of indoor scenes and demonstrate that our method consistently improves the result, qualitatively and quantitatively. Our implementation will be made publicly available.