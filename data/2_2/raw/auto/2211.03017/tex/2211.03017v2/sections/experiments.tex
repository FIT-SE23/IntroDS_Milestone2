

\section{Experiments}\label{sec:experiments}





\subsection{Experiment Settings}


\paragraph{Training data.} We train our network on our new photorealistic indoor scene dataset,  introduced in Sec. \ref{sec:data}.  
When evaluating on real world data, we also fine-tune our model on IIW dataset~\cite{bell2014intrinsic} for albedo and NYUv2~\cite{silberman2012indoor} for depth and normal. Please refer to our supplementary material for more details on training and evaluation data.


\paragraph{Baselines.} We compare our method with \citet{li2020inverse}, which is the state-of-the-art holistic inverse rendering frameworks for indoor scenes. To ensure a fair comparison, \emph{we fine-tune \cite{li2020inverse} on our new dataset}, which significantly improves its performance (Fig.~\ref{fig:mgsyn}). For lighting prediction, we compare with \cite{li2020inverse} as well as another state-of-the-art lighting estimation method Lighthouse~\cite{srinivasan2020lighthouse}, which requires a stereo image pair as input instead of a single image.

\setlength\tabcolsep{0.5pt}
\begin{figure}[ht]
    \centering
    % \vspace*{-2.5ex}
    \begin{tabular}{m{1em}<{\centering}m{2cm}<{\centering}m{2cm}<{\centering}m{2cm}<{\centering}m{2cm}<{\centering}}
          & \cite{li2020inverse} & Fine-tuned & Ours & Ground Truth \\[-0.2ex]
         \rotatebox{90}{Albedo} & \includegraphics[width=2cm]{figure/mgsyn/albedo_li.png} & \includegraphics[width=2cm]{figure/mgsyn/albedo_li_ft.png}  & \includegraphics[width=2cm]{figure/mgsyn/albedo_pred.png} & \includegraphics[width=2cm]{figure/mgsyn/albedo_gt.png}
         \\[-0.6ex]
         \rotatebox{90}{Roughness} & \includegraphics[width=2cm]{figure/mgsyn/rough_li.png} & \includegraphics[width=2cm]{figure/mgsyn/rough_li_ft.png}  & \includegraphics[width=2cm]{figure/mgsyn/rough_vis.png} & \includegraphics[width=2cm]{figure/mgsyn/rough_gt.png} \\[-0.6ex]
         \rotatebox{90}{Normal} & \includegraphics[width=2cm]{figure/mgsyn/normal_li.png} & \includegraphics[width=2cm]{figure/mgsyn/normal_li_ft.png}  & \includegraphics[width=2cm]{figure/mgsyn/normal_pred.png} & \includegraphics[width=2cm]{figure/mgsyn/normal_gt.png} 
    \end{tabular}
    \vspace*{-2.5ex}
    \caption{\textbf{Qualitative results of geometry and BRDF estimation} on synthetic dataset between \citet{li2020inverse} and our method. In second row, we show the improved prediction of \citet{li2020inverse} by fine-tuning it on our \textsc{InteriorVerse} dataset. We omit metallic comparison since \citet{li2020inverse} does not support it. See supplementary for more results.}
    \label{fig:mgsyn}
    % \vspace*{-1.5ex}
\end{figure}

\setlength\tabcolsep{1.2pt}
\begin{figure*}[ht]
    \centering
    \begin{tabular}{ccc|cc}
        Ground Truth & \cite{li2020inverse} & Ours & \cite{li2020inverse} & Ours \\[-0.1ex]
        
        \includegraphics[width=0.191\textwidth]{figure/insert_synthetic/01_gt.png}&
        \includegraphics[width=0.191\textwidth]{figure/insert_synthetic/01_li.png}&
        \includegraphics[width=0.191\textwidth]{figure/insert_synthetic/01_shadow.png}&
        \stackinset{r}{0pt}{b}{0pt}{\includegraphics[width=0.05\textwidth]{figure/insert_sphere/00_baseline_inset.png}}{\includegraphics[width=0.191\textwidth]{figure/insert_sphere/00_baseline_emphasize.png}}&
        \stackinset{r}{0pt}{b}{0pt}{\includegraphics[width=0.05\textwidth]{figure/insert_sphere/00_our_inset.png}}{\includegraphics[width=0.191\textwidth]{figure/insert_sphere/00_our_emphasize.png}} \\[-0.35ex]
        
        \includegraphics[width=0.191\textwidth]{figure/insert_synthetic/02_gt.png}&
        \includegraphics[width=0.191\textwidth]{figure/insert_synthetic/02_li.png}&
        \includegraphics[width=0.191\textwidth]{figure/insert_synthetic/02.png}&
        \stackinset{r}{0pt}{b}{0pt}{\includegraphics[width=0.05\textwidth]{figure/insert_sphere/04_baseline_inset.png}}{\includegraphics[width=0.191\textwidth]{figure/insert_sphere/04_baseline_emphasize.png}}&
        \stackinset{r}{0pt}{b}{0pt}{\includegraphics[width=0.05\textwidth]{figure/insert_sphere/04_our_inset.png}}{\includegraphics[width=0.191\textwidth]{figure/insert_sphere/04_our_emphasize.png}} \\[-0.35ex]
        
        &\textsc{Synthetic data}&&
        \multicolumn{2}{c}{\textsc{Real data} } 
    \end{tabular}
    \vspace*{-1.5ex}
    \caption{\textbf{Qualitative comparison of object insertion} results on synthetic dataset and real-world images. The ground truth object insertion results of synthetic scenes are provided. Li et al.'s results use the same highly-specular GGX BRDF as our results. However, because of their low-frequency lighting prediction, the inserted objects contain no sharp reflections and therefore resemble Lambertian appearance.}
    \label{fig:insert_real}
\end{figure*}

\subsection{Evaluation of Material and Geometry}


We evaluate material (albedo, roughness, and metallic) and geometry (normal and depth) prediction on \textsc{InteriorVerse} synthetic indoor dataset, as well as real-world dataset (NYUv2 dataset~\cite{silberman2012indoor} for geometry and IIW dataset~\cite{bell2014intrinsic} for albedo).



\paragraph{Evaluation on synthetic dataset.} We compare our method with the baseline methods on our \textsc{InteriorVerse} dataset. As shown in Fig.~\ref{fig:mgsyn}, our method outperforms \cite{li2020inverse},
For albedo prediction, while \cite{li2020inverse} tends to over-smooth the result, our method faithfully preserves the texture details (e.g., the wooden textures of the floor). For normal prediction, our method is capable of preserving sharp edges between walls and floors. This attributes to the usage of perceptual loss, which helps the model recognize semantic borders in the image. Please refer to the supplementary material for an ablation study on the usage of perceptual loss.

\paragraph{Evaluation on real-world datasets.} We evaluate albedo prediction on IIW dataset~\cite{bell2014intrinsic} with sparse pairwise human albedo annotations. We use the official metric suggested by~\cite{bell2014intrinsic}, Weighted Human Disagreement Rate (WHDR), which measures the error when albedo predictions disagree with human annotations.
We also evaluate geometry prediction on NYUv2 dataset~\cite{silberman2012indoor}. As shown in Table \ref{tab:nyuv2_and_iiw}, we observe a lower error compared to prior works~\cite{li2020inverse,wang2021learning}, indicating the advantage of our photo-realistic training datasets and our network design.
Qualitative results of geometric and material predictions on real-world data are presented in the supplementary material.


\begin{table}[h]
    \centering
    % \vspace*{-2.5ex}
    \caption{\textbf{Evaluation of normals and depth} on NYUv2 dataset (2nd and 3rd columns), and \textbf{albedo} on IIW dataset (last column).}
    \vspace*{-2.5ex}
    \begin{tabular}{|c|c|c|c|}
    \hline
       Method & Normal Angular Error & Depth si-MSE & WHDR \\\hline
        \cite{li2020inverse} & $24.12^{\circ}$ & 0.160 & 15.9 \\\hline
        \cite{wang2021learning} & $22.95^{\circ}$ & 0.181 & 18.2\\\hline
        Ours & $\mathbf{21.86^{\circ}}$ & \textbf{0.155} & \textbf{15.5} \\\hline
    \end{tabular}
    \label{tab:nyuv2_and_iiw}
\end{table}


\subsection{Evaluation of Lighting}

\paragraph{Evaluation on virtual object insertion.} We evaluate our lighting estimation method on a crucial augmented reality application: virtual object insertion. With the help of screen space ray tracing and the Monte Carlo rendering layer, we can achieve promising results in specular reflection effects. Fig.~\ref{fig:insert_real} shows results of our method compared to baselines, consisting of both synthetic data and real world images. In order to emphasize the ability to recover high-frequency lighting details, the materials of the inserted objects are \emph{highly specular}. For synthetic data, we insert complex objects and ground truths are provided. \citet{li2020inverse}'s lighting estimation is 2D spatially-varying, which cannot handle 3D points far from 2D surfaces. Moreover, their Spherical Gaussian lighting representation is incapable of capturing high-frequency angular details. Therefore, the appearance of inserted highly specular objects does not contain sharp reflections. In contrast, our method produces photorealistic shading and specular highlights on the inserted object. For real world data, we choose to insert highly specular spheres. The reflection on the sphere is supposed to be consistent with the surrounding environments. \citet{li2020inverse} also fails in this task, due to its low-frequency lighting predictions, while our method manages to faithfully recover angular details of the surrounding environment on the inserted sphere.


\setlength\tabcolsep{0.5pt}
\begin{figure}[h]
    \vspace*{-1.5ex}  
    \centering
    \begin{tabular}{ccc}
        Origin & Lighthouse~\shortcite{srinivasan2020lighthouse} & Ours \\
        \includegraphics[width=2.8cm]{figure/lighthouse/00_orig.png} & 
        \stackinset{r}{0pt}{b}{0pt}{\includegraphics[width=0.8cm]{figure/lighthouse/00_baseline_inset.png}}{\includegraphics[width=2.8cm]{figure/lighthouse/00_baseline_emphasize.png}} &
        \stackinset{r}{0pt}{b}{0pt}{\includegraphics[width=0.8cm]{figure/lighthouse/00_our_inset.png}}{\includegraphics[width=2.8cm]{figure/lighthouse/00_our_emphasize.png}}
    \end{tabular}
    \vspace*{-2.5ex}    
    \caption{\textbf{Qualitative comparison of object insertion} results between Lighthouse~\shortcite{srinivasan2020lighthouse} and our method on \textbf{Lighthouse's test set}.}
    \vspace*{-1.5ex}
    \label{fig:lighthouse}
\end{figure}

We also compare our method with another state-of-the-art lighting estimation method Lighthouse~\cite{srinivasan2020lighthouse}, which requires a stereo pair of images as input. To show our method's cross-domain ability, we evaluate on \textit{Lighthouse's test set} from InteriorNet~\cite{li2018interiornet} \textit{without fine-tuning our network}. As shown in Figure~\ref{fig:lighthouse}, our method outperforms Lighthouse, even with a lower number of input images and a potential domain gap. We can observe that Lighthouse's lighting prediction has significantly less variation in lighting intensity. This may be because Lighthouse is trained from LDR panoramas, and cannot handle HDR lighting.

We also explore more applications of our lighting estimation method, including re-rendering and scene material edit. Please refer to our supplementary material for these additional results.
