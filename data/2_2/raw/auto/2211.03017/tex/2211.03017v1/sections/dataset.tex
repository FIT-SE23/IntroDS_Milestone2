
\begin{figure}[b]
    \centering
    \setlength{\fboxsep}{0.0pt}%
    \setlength{\fboxrule}{0.75pt}%
    \vspace*{-1.0\baselineskip}%
    \hspace*{-0.6ex}%
    \begin{tikzpicture}[x=0.075\textwidth, y=0.075\textwidth,every text node part/.style={align=center}]
    \node[anchor=north west] at (-0.03, 0) {\includegraphics[width=0.155\textwidth]{figure/compare_dataset/open1.png}};
    \node[anchor=north west] at (3.16, 0) {\includegraphics[width=0.155\textwidth]{figure/compare_dataset/ours2.png}};
    
    \node[anchor=north west] at (2.07, 0) {\includegraphics[width=0.076\textwidth]{figure/compare_dataset/open2.png}};
    \node[anchor=north west] at (2.07, -0.79+0.0021) {\includegraphics[width=0.076\textwidth]{figure/compare_dataset/open4.png}};
    \node[anchor=north west] at (2.07, -1.58) {\includegraphics[width=0.076\textwidth]{figure/compare_dataset/open5.png}};
    \node[anchor=north west] at (2.07, -1.58-0.79+0.0021) {\includegraphics[width=0.076\textwidth]{figure/compare_dataset/open6.png}};
    
    \node[anchor=north west] at (-0.03, -1.58) {\includegraphics[width=0.155\textwidth]{figure/compare_dataset/open3.png}};
    \node[anchor=north west] at (3.16, -1.58) {\includegraphics[width=0.155\textwidth]{figure/compare_dataset/ours1.png}};
    
    \node[anchor=north west] at (5.26, 0) {\includegraphics[width=0.076\textwidth]{figure/compare_dataset/ours4.png}};
    \node[anchor=north west] at (5.26, -0.79+0.0021) {\includegraphics[width=0.076\textwidth]{figure/compare_dataset/ours5.png}};
    \node[anchor=north west] at (5.26, -1.58) {\includegraphics[width=0.076\textwidth]{figure/compare_dataset/ours6.png}};
    \node[anchor=north west] at (5.26, -1.58-0.79+0.0021) {\includegraphics[width=0.076\textwidth]{figure/compare_dataset/ours7.png}};
 
    \draw[ black, very thin] (3.205,0.18) -- (3.205,-3.218);
    \node at (1.6, 0.08){\textsc{OpenRooms}~\cite{li2021openrooms}};
    \node at (4.8, 0.08){\textsc{InteriorVerse (ours)}};

    \end{tikzpicture}%
    % \vspace*{-1.0\baselineskip}%
    \vspace*{-2.5ex}
    \caption{\textbf{Example dataset images} from OpenRooms~\cite{li2021openrooms} (left) and our \textsc{InteriorVerse} dataset (right). Note that our dataset contains more diversified geometry, material (especially glossy and specular BRDFs) and complex lighting conditions comparing OpenRooms. Zoom in for details.}
    \label{fig:dataset}
    % \vspace*{-1.\baselineskip}%
\end{figure}

\begin{figure*}[ht]
    \centering
    \includegraphics[width=\textwidth]{figure/pip_v2.pdf}
    \vspace*{-3.5ex}
    \caption{\textbf{Overview of the pipeline}. On the left, we show the workflow throughout our inverse rendering framework: (i) The spatially-varying material and geometry maps are predicted by $\textbf{MGNet}$ (ii) According to the predicted material, geometry and view direction associated with each pixel point $\mathbf{p}$, a BRDF importance sampling is performed to generate per-pixel incident directions $\mathbf{d}_i$ (iii) We use screen-space raytracing to trace the source point $\mathbf{s}$ of the query light. The corresponding local feature vector is extracted from feature map $F$ via projection of point $\mathbf{s}$. (iv) The feature is passed to $\mathbf{LightNet}$ along with light direction and auxiliary G-Buffer information to predict the incident radiance $\tilde{L_i}$ (v) Monte-Carlo integration (Eq.~\ref{equ:render}) is used to calculate the final re-rendered result. On the right, we show our out-of-view light estimation. We use a hypernetwork to predict the weights of the NeRF MLP and volume render the background lighting.}
    \label{fig:pipeline}
    % \vspace*{-1.\baselineskip}%
    % \vspace{-1ex}
\end{figure*}

\section{InteriorVerse: A Large-Scale, Photorealistic Indoor Scene Dataset}\label{sec:data}

A high-quality dataset is crucial for learning-based inverse rendering. It's extremely difficult to acquire spatially-varying material and lighting ground truth in real world complex indoor scenes. Therefore, we render a synthetic dataset to supervise training. The SUNCG dataset~\cite{song2017semantic} is a manually-created large-scale dataset for indoor scenes, but they use non-physical Phong BRDF and render with OpenGL, which severely limits its photorealism. PBRS~\cite{zhang2017physically} and CG-PBR~\cite{sengupta2019neural} datasets are rendered with physically-based renderers, but both still use Phong BRDF and do not provide spatially-varying lighting ground truth for an arbitrary 3D location. InteriorNet~\cite{li2018interiornet} is a large-scale photorealistic indoor scene dataset providing multiple camera views and panoramas, but the images they provide are LDR, limiting the dynamic range of illumination. OpenRooms~\cite{li2021openrooms} is by far the only HDR dataset with spatially-varying lighting rendered using physically-based microfacet BRDF. However, as shown in Fig.~\ref{fig:dataset}, it presents overly simplified furniture models and layouts, insufficient material and lighting variations, leading to less faithful appearance comparing to real world data consequently.


In this work, we create a new high-quality indoor scene dataset called \textsc{InteriorVerse}, which has the following advantages in data quality over existing alternatives: (1) the scene layouts of our dataset have richer details, including complex furniture and decorations. (2) Our dataset is rendered with GGX BRDF model~\cite{walter2007microfacet}, which has stronger material modeling capability than any BRDF models that existing indoor scene datasets use. (3) Our dataset provides not only pixel-wise surface environment maps, but also contains environment maps located anywhere in the 3D scene space. Fig.~\ref{fig:dataset} compares some example scenes in our dataset and OpenRooms, showing our dataset's higher scene quality.