
\section{Related Work}

\paragraph{Inverse Rendering of Indoor Scenes} Inverse rendering attempts to reconstruct geometry and spatially-varying material and lighting information from monocular (which is our case) or multiple RGB images. Most previous methods only recognize one or part of the above attributes. Geometry reconstructions, including depth estimation and surface normal reconstruction, has been widely studied \cite{eigen2015predicting,liu2019planercnn}.
Most material reconstruction methods are only able to either estimate diffuse albedo~\cite{li2018cgintrinsics, barron2013intrinsic, karsch2014automatic} or classify material categories~\cite{bell2015material}.
For lighting estimation, recent deep learning methods have made progress in estimating global~\cite{gardner2017learning,gardner2019deep} and even spatially-varying~\cite{garon2019fast,song2019neural,li2020inverse} lighting conditions.
Recent works attempt to predict multiple intrinsics jointly by a holistic inverse rendering framework. Li et al.~\shortcite{li2020inverse} proposed a method to reconstruct disentangled geometry, spatially-varying reflectance and lighting from a single RGB indoor scene image.


\paragraph{Lighting Estimation and Relighting.}
Light estimation is one of the sub-tasks of inverse rendering. Most previous works ignore spatially-varying effects and predict a single environment map for the whole scene \cite{gardner2017learning,sengupta2019neural,munkberg2022extracting}. Indoor scenes suffer from spatial variations, thus recent work explores spatially-varying lighting estimation for indoor scenes. The representation of spatially-varying illumination includes environment maps, per-pixel spherical lobes~\cite{li2020inverse} (spherical Harmonics/Gaussians), or 3D voxel grids~\cite{wang2021learning}. Relighting is also a widely-studied relevant  task. \citet{griffiths2022outcast} leverages screen-space method to detect occlusion and cast shadows to relight an outdoor image. \citet{li2022physically} proposed a novel pipeline to modify the light conditions within an indoor scene.


\paragraph{Neural Scene Representations.} 
Neural representations are a rapidly growing area of research. Recent advances include  voxels~\cite{yu2021plenoxels,sun2021direct}, hashgrids~\cite{muller2022instant}, point clouds~\cite{aliev2020neural}, and neural implicit functions~\cite{mildenhall2020nerf,wang2021neus,yariv2021volume,yariv2020multiview}. 
Neural radiance fields (NeRFs)~\cite{mildenhall2020nerf} represents scenes as neural implicit functions, encoding a scene as a continuous volumetric radiance field of color and density. With volume rendering, a NeRF can synthesize novel view images with promising results. Our proposed method uses a NeRF as the representation of the out-of-view area of the scene (Sec.~\ref{sec:background}).

\paragraph{Differentiable Rendering.} A number of recent inverse rendering works utilize differentiable rendering to recover complex light transport effects. Some recent works have proposed general-purpose physically-based differentiable renderers~\cite{Li:2018:DMC,NimierDavidVicini2019Mitsuba2}. \citet{Zhang:2020:PSDR} and \citet{Zeltner2021MonteCarlo} discussed a rigurous theory of differentiable light transport and Monte-Carlo combinations. These physically-based methods achieve high-quality global illumination effects at the cost of substantial performance overhead. Some differentiable rendering techniques are customized for specific purpose such as texture~\cite{nimier2021material}, split-sum lighting and mesh extraction~\cite{munkberg2022extracting}. Our method designs a Monte-Carlo based in-network differentiable rendering layer to recover the appearance of indoor scenes (Sec.~\ref{sec:render}).

\paragraph{Indoor Scene Datasets.} 
Supervised learning requires a large database of indoor scene images and their corresponding ground truth geometry, material, and lighting for network training. Datasets include 3D shape models~\cite{chang2015shapenet}, real-world scans~\cite{chang2017matterport3d, dai2017scannet}, and scene datasets~\cite{song2017semantic,savva2017minos,li2018interiornet,li2021openrooms}, which can be classified as either real or synthetic data. Real datasets provide real-world images and geometry, while synthetic datasets provide arbitrary scene annotations for inverse rendering, some of which, such as materials and illumination, are difficult to acquire from real world. To the best of our knowledge, InteriorNet~\cite{li2018interiornet} and OpenRooms~\cite{li2021openrooms} are so far the highest-quality public indoor datasets with spatially-varying photorealistic material and illumination annotations. Unfortunately, InteriorNet provides only LDR results, while OpenRooms provides only lighting information on the scene surface (instead of at any 3D location), and lacks the complexity of material and furniture variations. We present a new indoor scene HDR dataset to tackle their shortcomings.

