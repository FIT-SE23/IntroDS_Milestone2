% Capstan papers start

@inproceedings{rucker2021capstan,
author = {Rucker, Alexander and Vilim, Matthew and Zhao, Tian and Zhang, Yaqi and Prabhakar, Raghu and Olukotun, Kunle},
title = {Capstan: A Vector RDA for Sparsity},
year = {2021},
isbn = {9781450385572},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3466752.3480047},
doi = {10.1145/3466752.3480047},
abstract = { This paper proposes Capstan: a scalable, parallel-patterns-based, reconfigurable dataflow accelerator (RDA) for sparse and dense tensor applications. Instead of designing for one application, we start with common sparse data formats, each of which supports multiple applications. Using a declarative programming model, Capstan supports application-independent sparse iteration and memory primitives that can be mapped to vectorized, high-performance hardware. We optimize random-access sparse memories with configurable out-of-order execution to increase SRAM random-access throughput from 32% to 80%. For a variety of sparse applications, Capstan with DDR4 memory is 18\texttimes{} faster than a multi-core CPU baseline, while Capstan with HBM2 memory is 16\texttimes{} faster than an Nvidia V100 GPU. For sparse applications that can be mapped to Plasticine, a recent dense RDA, Capstan is 7.6\texttimes{} to 365\texttimes{} faster and only 16% larger. },
booktitle = {MICRO-54: 54th Annual IEEE/ACM International Symposium on Microarchitecture},
pages = {1022–1035},
numpages = {14},
keywords = {RDA, sparse iteration, sparsity, parallel patterns, reconfigurable dataflow accelerator, vectorization, CGRA},
location = {Virtual Event, Greece},
series = {MICRO '21}
}




@article{plasticine,
author = {Prabhakar, Raghu and Zhang, Yaqi and Koeplinger, David and Feldman, Matt and Zhao, Tian and Hadjis, Stefan and Pedram, Ardavan and Kozyrakis, Christos and Olukotun, Kunle},
title = {Plasticine: A Reconfigurable Architecture For Parallel Paterns},
year = {2017},
issue_date = {May 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {45},
number = {2},
issn = {0163-5964},
url = {https://doi.org/10.1145/3140659.3080256},
doi = {10.1145/3140659.3080256},
abstract = {Reconfigurable architectures have gained popularity in recent years as they allow
the design of energy-efficient accelerators. Fine-grain fabrics (e.g. FPGAs) have
traditionally suffered from performance and power inefficiencies due to bit-level
reconfigurable abstractions. Both fine-grain and coarse-grain architectures (e.g.
CGRAs) traditionally require low level programming and suffer from long compilation
times. We address both challenges with Plasticine, a new spatially reconfigurable
architecture designed to efficiently execute applications composed of parallel patterns.
Parallel patterns have emerged from recent research on parallel programming as powerful,
high-level abstractions that can elegantly capture data locality, memory access patterns,
and parallelism across a wide range of dense and sparse applications.We motivate Plasticine
by first observing key application characteristics captured by parallel patterns that
are amenable to hardware acceleration, such as hierarchical parallelism, data locality,
memory access patterns, and control flow. Based on these observations, we architect
Plasticine as a collection of Pattern Compute Units and Pattern Memory Units. Pattern
Compute Units are multi-stage pipelines of reconfigurable SIMD functional units that
can efficiently execute nested patterns. Data locality is exploited in Pattern Memory
Units using banked scratchpad memories and configurable address decoders. Multiple
on-chip address generators and scatter-gather engines make efficient use of DRAM bandwidth
by supporting a large number of outstanding memory requests, memory coalescing, and
burst mode for dense accesses. Plasticine has an area footprint of 113 mm2 in a 28nm
process, and consumes a maximum power of 49 W at a 1 GHz clock. Using a cycle-accurate
simulator, we demonstrate that Plasticine provides an improvement of up to 76.9x in
performance-per-Watt over a conventional FPGA over a wide range of dense and sparse
applications.},
journal = {SIGARCH Comput. Archit. News},
month = jun,
pages = {389–402},
numpages = {14},
keywords = {reconfigurable architectures, CGRAs, parallel patterns, hardware accelerators}
}

@inproceedings{spatial,
author = {Koeplinger, David and Feldman, Matthew and Prabhakar, Raghu and Zhang, Yaqi and Hadjis, Stefan and Fiszel, Ruben and Zhao, Tian and Nardi, Luigi and Pedram, Ardavan and Kozyrakis, Christos and Olukotun, Kunle},
title = {Spatial: A Language and Compiler for Application Accelerators},
year = {2018},
isbn = {9781450356985},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3192366.3192379},
doi = {10.1145/3192366.3192379},
abstract = {Industry is increasingly turning to reconfigurable architectures like FPGAs and CGRAs
for improved performance and energy efficiency. Unfortunately, adoption of these architectures
has been limited by their programming models. HDLs lack abstractions for productivity
and are difficult to target from higher level languages. HLS tools are more productive,
but offer an ad-hoc mix of software and hardware abstractions which make performance
optimizations difficult. In this work, we describe a new domain-specific language
and compiler called Spatial for higher level descriptions of application accelerators.
We describe Spatial's hardware-centric abstractions for both programmer productivity
and design performance, and summarize the compiler passes required to support these
abstractions, including pipeline scheduling, automatic memory banking, and automated
design tuning driven by active machine learning. We demonstrate the language's ability
to target FPGAs and CGRAs from common source code. We show that applications written
in Spatial are, on average, 42\% shorter and achieve a mean speedup of 2.9x over SDAccel
HLS when targeting a Xilinx UltraScale+ VU9P FPGA on an Amazon EC2 F1 instance.},
booktitle = {Proceedings of the 39th ACM SIGPLAN Conference on Programming Language Design and Implementation},
pages = {296–311},
numpages = {16},
keywords = {reconfigurable architectures, domain-specific languages, high-level synthesis, FPGAs, CGRAs, compilers, hardware accelerators},
location = {Philadelphia, PA, USA},
series = {PLDI 2018}
}
% Capstan papers end


% TACO papers start
@article{kjolstad2017tensor,
  title={The tensor algebra compiler},
  author={Kjolstad, Fredrik and Kamil, Shoaib and Chou, Stephen and Lugato, David and Amarasinghe, Saman},
  journal={Proceedings of the ACM on Programming Languages},
  volume={1},
  number={OOPSLA},
  pages={1--29},
  year={2017},
  publisher={ACM New York, NY, USA}
}

@article{chou2018,
 author = {Chou, Stephen and Kjolstad, Fredrik and Amarasinghe, Saman},
 title = {Format Abstraction for Sparse Tensor Algebra Compilers},
 journal = {Proc. ACM Program. Lang.},
 issue_date = {November 2018},
 volume = {2},
 number = {OOPSLA},
 month = oct,
 year = {2018},
 issn = {2475-1421},
 pages = {123:1--123:30},
 articleno = {123},
 numpages = {30},
 acmid = {3276493},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {modular code generation, sparse tensor algebra compilation, tensor formats},
}

@INPROCEEDINGS{kjolstad2019,
  author={Fredrik {Kjolstad} and Peter {Ahrens} and Shoaib {Kamil} and Saman {Amarasinghe}},
  booktitle={2019 IEEE/ACM International Symposium on Code Generation and Optimization (CGO)}, 
  title={Tensor Algebra Compilation with Workspaces}, 
  year={2019},
  volume={},
  number={},
  pages={180-192},
  doi={10.1109/CGO.2019.8661185}}

@article{senanayake2020,
author = {Senanayake, Ryan and Hong, Changwan and Wang, Ziheng and Wilson, Amalee and Chou, Stephen and Kamil, Shoaib and Amarasinghe, Saman and Kjolstad, Fredrik},
title = {A Sparse Iteration Space Transformation Framework for Sparse Tensor Algebra},
year = {2020},
issue_date = {November 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {4},
number = {OOPSLA},
url = {https://doi-org.stanford.idm.oclc.org/10.1145/3428226},
doi = {10.1145/3428226},
abstract = {We address the problem of optimizing sparse tensor algebra in a compiler and show how to define standard loop transformations---split, collapse, and reorder---on sparse iteration spaces. The key idea is to track the transformation functions that map the original iteration space to derived iteration spaces. These functions are needed by the code generator to emit code that maps coordinates between iteration spaces at runtime, since the coordinates in the sparse data structures remain in the original iteration space. We further demonstrate that derived iteration spaces can tile both the universe of coordinates and the subset of nonzero coordinates: the former is analogous to tiling dense iteration spaces, while the latter tiles sparse iteration spaces into statically load-balanced blocks of nonzeros. Tiling the space of nonzeros lets the generated code efficiently exploit heterogeneous compute resources such as threads, vector units, and GPUs. We implement these concepts by extending the sparse iteration theory implementation in the TACO system. The associated scheduling API can be used by performance engineers or it can be the target of an automatic scheduling system. We outline one heuristic autoscheduling system, but other systems are possible. Using the scheduling API, we show how to optimize mixed sparse-dense tensor algebra expressions on CPUs and GPUs. Our results show that the sparse transformations are sufficient to generate code with competitive performance to hand-optimized implementations from the literature, while generalizing to all of the tensor algebra.},
journal = {Proc. ACM Program. Lang.},
month = nov,
articleno = {158},
numpages = {30},
keywords = {Optimizing Transformations, Sparse Iteration Spaces, Sparse Tensor Algebra}
}

@phdthesis{kjolstad2020sparse,
  title={Sparse tensor algebra compilation},
  author={Kj{\o}lstad, Fredrik Berg},
  year={2020},
  school={Massachusetts Institute of Technology}
}
% TACO references end

% Scheduling language references start
@article{halide2012,
author = {Ragan-Kelley, Jonathan and Adams, Andrew and Paris, Sylvain and Levoy, Marc and Amarasinghe, Saman and Durand, Fr\'{e}do},
title = {Decoupling Algorithms from Schedules for Easy Optimization of Image Processing Pipelines},
year = {2012},
issue_date = {July 2012},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {31},
number = {4},
issn = {0730-0301},
url = {https://doi-org.stanford.idm.oclc.org/10.1145/2185520.2185528},
doi = {10.1145/2185520.2185528},
abstract = {Using existing programming tools, writing high-performance image processing code requires
sacrificing readability, portability, and modularity. We argue that this is a consequence
of conflating what computations define the algorithm, with decisions about storage
and the order of computation. We refer to these latter two concerns as the schedule,
including choices of tiling, fusion, recomputation vs. storage, vectorization, and
parallelism.We propose a representation for feed-forward imaging pipelines that separates
the algorithm from its schedule, enabling high-performance without sacrificing code
clarity. This decoupling simplifies the algorithm specification: images and intermediate
buffers become functions over an infinite integer domain, with no explicit storage
or boundary conditions. Imaging pipelines are compositions of functions. Programmers
separately specify scheduling strategies for the various functions composing the algorithm,
which allows them to efficiently explore different optimizations without changing
the algorithmic code.We demonstrate the power of this representation by expressing
a range of recent image processing applications in an embedded domain specific language
called Halide, and compiling them for ARM, x86, and GPUs. Our compiler targets SIMD
units, multiple cores, and complex memory hierarchies. We demonstrate that it can
handle algorithms such as a camera raw pipeline, the bilateral grid, fast local Laplacian
filtering, and image segmentation. The algorithms expressed in our language are both
shorter and faster than state-of-the-art implementations.},
journal = {ACM Trans. Graph.},
month = jul,
articleno = {32},
numpages = {12},
keywords = {compilers, performance, image processing}
}

@inproceedings{tvm2018,
author = {Chen, Tianqi and Moreau, Thierry and Jiang, Ziheng and Zheng, Lianmin and Yan, Eddie and Cowan, Meghan and Shen, Haichen and Wang, Leyuan and Hu, Yuwei and Ceze, Luis and Guestrin, Carlos and Krishnamurthy, Arvind},
title = {TVM: An Automated End-to-End Optimizing Compiler for Deep Learning},
year = {2018},
isbn = {9781931971478},
publisher = {USENIX Association},
address = {USA},
abstract = {There is an increasing need to bring machine learning to a wide diversity of hardware
devices. Current frameworks rely on vendor-specific operator libraries and optimize
for a narrow range of server-class GPUs. Deploying workloads to new platforms - such
as mobile phones, embedded devices, and accelerators (e.g., FPGAs, ASICs) - requires
significant manual effort. We propose TVM, a compiler that exposes graph-level and
operator-level optimizations to provide performance portability to deep learning workloads
across diverse hardware back-ends. TVM solves optimization challenges specific to
deep learning, such as high-level operator fusion, mapping to arbitrary hardware primitives,
and memory latency hiding. It also automates optimization of low-level programs to
hardware characteristics by employing a novel, learning-based cost modeling method
for rapid exploration of code optimizations. Experimental results show that TVM delivers
performance across hardware back-ends that are competitive with state-of-the-art,
hand-tuned libraries for low-power CPU, mobile GPU, and server-class GPUs. We also
demonstrate TVM's ability to target new accelerator back-ends, such as the FPGA-based
generic deep learning accelerator. The system is open sourced and in production use
inside several major companies.},
booktitle = {Proceedings of the 13th USENIX Conference on Operating Systems Design and Implementation},
pages = {579–594},
numpages = {16},
location = {Carlsbad, CA, USA},
series = {OSDI'18}
}

@article{Ricci1901,
author = {Ricci, M.M.G. and Levi-Civita, T.},
journal = {Mathematische Annalen},
pages = {125-201},
title = {Méthodes de calcul différentiel absolu et leurs applications},
url = {http://eudml.org/doc/157997},
volume = {54},
year = {1901},
}
@inproceedings{dadu2019towards,
  title={Towards general purpose acceleration by exploiting common data-dependence forms},
  author={Dadu, Vidushi and Weng, Jian and Liu, Sihao and Nowatzki, Tony},
  booktitle={Proceedings of the 52nd Annual IEEE/ACM International Symposium on Microarchitecture},
  pages={924--939},
  year={2019}
}

@article{hegarty2016rigel,
author = {Hegarty, James and Daly, Ross and DeVito, Zachary and Ragan-Kelley, Jonathan and Horowitz, Mark and Hanrahan, Pat},
title = {Rigel: Flexible Multi-Rate Image Processing Hardware},
year = {2016},
issue_date = {July 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {35},
number = {4},
issn = {0730-0301},
url = {https://doi.org/10.1145/2897824.2925892},
doi = {10.1145/2897824.2925892},
journal = {ACM Transactions on Graphics (TOG)},
pages = {85:1--85:11},
articleno = {85},
numpages = {11},
keywords = {hardware synthesis, image processing, video processing, FPGAs, domain-specific languages}
}

@inproceedings{chugh2016dsl,
  title={A {DSL} compiler for accelerating image processing pipelines on {FPGA}s},
  author={Chugh, Nitin and Vasista, Vinay and Purini, Suresh and Bondhugula, Uday},
  booktitle={Proceedings of the International Conference on Parallel Architectures and Compilation},
  pages={327--338},
  year={2016}
}

@inproceedings{durst2020type,
  title={Type-directed scheduling of streaming accelerators},
  author={Durst, David and Feldman, Matthew and Huff, Dillon and Akeley, David and Daly, Ross and Bernstein, Gilbert Louis and Patrignani, Marco and Fatahalian, Kayvon and Hanrahan, Pat},
  booktitle={Proceedings of the ACM SIGPLAN Conference on Programming Language Design and Implementation (PLDI)},
  pages={408--422},
  year={2020}
}

@article{Hegarty:2014:DCH:2601097.2601174,
 author = {Hegarty, James and Brunhaver, John and DeVito, Zachary and Ragan-Kelley, Jonathan and Cohen, Noy and Bell, Steven and Vasilyev, Artem and Horowitz, Mark and Hanrahan, Pat},
 title = {Darkroom: Compiling High-level Image Processing Code into Hardware Pipelines},
 journal = {ACM Transactions on Graphics (TOG)},
 issue_date = {July 2014},
 volume = {33},
 number = {4},
 year = {2014},
 issn = {0730-0301},
 pages = {144:1--144:11},
 articleno = {144},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/2601097.2601174},
 doi = {10.1145/2601097.2601174},
 acmid = {2601174},
 publisher = {ACM},
 address = {New York, NY, USA},
}

<<<<<<< HEAD
@inproceedings{chi2018soda,
  title={{SODA}: Stencil with optimized dataflow architecture},
  author={Chi, Yuze and Cong, Jason and Wei, Peng and Zhou, Peipei},
  booktitle={Proceedings of the IEEE/ACM International Conference on Computer-Aided Design (ICCAD)},
  pages={1--8},
  year={2018},
  //organization={IEEE}
}

@article{moreau2018vta,
  title={{VTA}: An Open Hardware-Software Stack for Deep Learning},
  author={Moreau, Thierry and Chen, Tianqi and Jiang, Ziheng and Ceze, Luis and Guestrin, Carlos and Krishnamurthy, Arvind},
  journal={arXiv preprint arXiv:1807.04188},
  year={2018}
}

@inproceedings{hegde2019extensor,
  title={Extensor: An accelerator for sparse tensor algebra},
  author={Hegde, Kartik and Asghari-Moghaddam, Hadi and Pellauer, Michael and Crago, Neal and Jaleel, Aamer and Solomonik, Edgar and Emer, Joel and Fletcher, Christopher W},
  booktitle={Proceedings of the 52nd Annual IEEE/ACM International Symposium on Microarchitecture},
  pages={319--333},
  year={2019}
}

@inproceedings{bik1993,
author = {Bik, Aart J. C. and Wijshoff, Harry A. G.},
booktitle = {International Conference on Supercomputing},
doi = {10.1145/165939.166023},
file = {:Users/fred/Dropbox/papers/Linear Algebra/Linear Algebra Compilation - Sparse/Data Structure Selection - Bik, Wijshoff, Spek/1993 - Compilation Techniques for Sparse Matrix Computations.pdf:pdf},
month = jul,
organization = {ACM},
pages = {416--424},
title = {Compilation Techniques for Sparse Matrix Computations},
year = {1993}
}

@incollection{kotlyar1997a,
address = {Passau, Germany},
author = {Kotlyar, Vladimir and Pingali, Keshav and Stodghill, Paul},
booktitle = {Euro-Par Parallel Processing},
doi = {10.1007/BFb0002751},
file = {:Users/fred/Dropbox/papers/Linear Algebra/Linear Algebra Compilation - Sparse/Bernoulli - Kotlyar, Pingali, Stodgehill/1997$\backslash$:09 - A Relational Approach to the Compilation of Sparse Matrix Programs.pdf:pdf},
pages = {318--327},
publisher = {Springer},
title = {A relational approach to the compilation of sparse matrix programs},
year = {1997}
}
@inproceedings{zhang2020sparch,
  title={Sparch: Efficient architecture for sparse matrix multiplication},
  author={Zhang, Zhekai and Wang, Hanrui and Han, Song and Dally, William J},
  booktitle={2020 IEEE International Symposium on High Performance Computer Architecture (HPCA)},
  pages={261--274},
  year={2020},
  organization={IEEE}
}
@inproceedings{pal2018outerspace,
  title={Outerspace: An outer product based sparse matrix multiplication accelerator},
  author={Pal, Subhankar and Beaumont, Jonathan and Park, Dong-Hyeon and Amarnath, Aporva and Feng, Siying and Chakrabarti, Chaitali and Kim, Hun-Seok and Blaauw, David and Mudge, Trevor and Dreslinski, Ronald},
  booktitle={2018 IEEE International Symposium on High Performance Computer Architecture (HPCA)},
  pages={724--736},
  year={2018},
  organization={IEEE}
}
@inproceedings{srivastava2020matraptor,
  title={Matraptor: A sparse-sparse matrix multiplication accelerator based on row-wise product},
  author={Srivastava, Nitish and Jin, Hanchen and Liu, Jie and Albonesi, David and Zhang, Zhiru},
  booktitle={2020 53rd Annual IEEE/ACM International Symposium on Microarchitecture (MICRO)},
  pages={766--780},
  year={2020},
  organization={IEEE}
}
@inproceedings{zhang2021gamma,
  title={Gamma: leveraging Gustavson’s algorithm to accelerate sparse matrix multiplication},
  author={Zhang, Guowei and Attaluri, Nithya and Emer, Joel S and Sanchez, Daniel},
  booktitle={Proceedings of the 26th ACM International Conference on Architectural Support for Programming Languages and Operating Systems},
  pages={687--701},
  year={2021}
}

@article{han2016eie,
  title={EIE: Efficient inference engine on compressed deep neural network},
  author={Han, Song and Liu, Xingyu and Mao, Huizi and Pu, Jing and Pedram, Ardavan and Horowitz, Mark A and Dally, William J},
  journal={ACM SIGARCH Computer Architecture News},
  volume={44},
  number={3},
  pages={243--254},
  year={2016},
  publisher={ACM New York, NY, USA}
}

@inproceedings{abadi2016tensorflow,
  title={Tensorflow: A system for large-scale machine learning},
  author={Abadi, Mart{\'\i}n and Barham, Paul and Chen, Jianmin and Chen, Zhifeng and Davis, Andy and Dean, Jeffrey and Devin, Matthieu and Ghemawat, Sanjay and Irving, Geoffrey and Isard, Michael and others},
  booktitle={12th {USENIX} Symposium on Operating Systems Design and Implementation ({OSDI} 16)},
  pages={265--283},
  year={2016}
}

@article{bader2008efficient,
  title={Efficient MATLAB computations with sparse and factored tensors},
  author={Bader, Brett W and Kolda, Tamara G},
  journal={SIAM Journal on Scientific Computing},
  volume={30},
  number={1},
  pages={205--231},
  year={2008},
  publisher={SIAM}
}

@misc{asanovic2006landscape,
  title={The landscape of parallel computing research: A view from {Berkeley}},
  author={Asanovic, Krste and Bodik, Ras and Catanzaro, Bryan Christopher and Gebis, Joseph James and Husbands, Parry and Keutzer, Kurt and Patterson, David A and Plishker, William Lester and Shalf, John and Williams, Samuel Webb and Yelick, Katherine A},
  year={2006},
  publisher={eScholarship, University of California}
}

@inproceedings{mattson2013standards,
  title={Standards for graph algorithm primitives},
  author={Mattson, Tim and Bader, David and Berry, Jon and Buluc, Aydin and Dongarra, Jack and Faloutsos, Christos and Feo, John and Gilbert, John and Gonzalez, Joseph and Hendrickson, Bruce and others},
  booktitle={2013 IEEE High Performance Extreme Computing Conference (HPEC)},
  pages={1--2},
  year={2013},
  organization={IEEE}
}

@misc{epifanovsky2013new,
  title={New implementation of high-level correlated methods using a general block tensor library for high-performance electronic structure calculations},
  author={Epifanovsky, Evgeny and Wormit, Michael and Ku{\'s}, Tomasz and Landau, Arie and Zuev, Dmitry and Khistyaev, Kirill and Manohar, Prashant and Kaliman, Ilya and Dreuw, Andreas and Krylov, Anna I},
  year={2013},
  publisher={Wiley Online Library}
}

@INPROCEEDINGS{zhang2021sara,  
    author={Zhang, Yaqi and Zhang, Nathan and Zhao, Tian and Vilim, Matt and Shahbaz, Muhammad and Olukotun, Kunle},  
    booktitle={2021 ACM/IEEE 48th Annual International Symposium on Computer Architecture (ISCA)},
    title={SARA: Scaling a Reconfigurable Dataflow Accelerator},
    year={2021},
    volume={},  
    number={},  
    pages={1041-1054},  
    doi={10.1109/ISCA52012.2021.00085}
}

@article{BaKo07ttb,  
author = {Brett W. Bader and Tamara G. Kolda}, 
title = {Efficient {MATLAB} Computations with Sparse and Factored Tensors}, 
journal = {SIAM Journal on Scientific Computing}, 
volume = {30}, 
number = {1}, 
pages = {205--231}, 
month = {December}, 
year = {2007},
doi = {10.1137/060676489},
}

@INPROCEEDINGS{solomonik2013cyclops,  author={Solomonik, Edgar and Matthews, Devin and Hammond, Jeff and Demmel, James},  booktitle={2013 IEEE 27th International Symposium on Parallel and Distributed Processing},   title={Cyclops Tensor Framework: Reducing Communication and Eliminating Load Imbalance in Massively Parallel Contractions},   year={2013},  volume={},  number={},  pages={813-824},  doi={10.1109/IPDPS.2013.112}}

@inproceedings{abbasi2018sparse,
  title={Sparse: a more modern sparse array library},
  author={Abbasi, Hameer},
  booktitle={Proceedings of the 17th Python in Science Conference},
  pages={27--30},
  year={2018}
}

@inproceedings{10.1145/2737924.2738003,
author = {Venkat, Anand and Hall, Mary and Strout, Michelle},
title = {Loop and Data Transformations for Sparse Matrix Code},
year = {2015},
isbn = {9781450334686},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi-org.stanford.idm.oclc.org/10.1145/2737924.2738003},
doi = {10.1145/2737924.2738003},
abstract = { This paper introduces three new compiler transformations for representing and transforming
sparse matrix computations and their data representations. In cooperation with run-time
inspection, our compiler derives transformed matrix representations and associated
transformed code to implement a variety of representations targeting different architecture
platforms. This systematic approach to combining code and data transformations on
sparse computations, which extends a polyhedral transformation and code generation
framework, permits the compiler to compose these transformations with other transformations
to generate code that is on average within 5% and often exceeds manually-tuned, high-performance
sparse matrix libraries CUSP and OSKI. Additionally, the compiler-generated inspector
codes are on average 1.5 faster than OSKI and perform comparably to CUSP, respectively.
},
booktitle = {Proceedings of the 36th ACM SIGPLAN Conference on Programming Language Design and Implementation},
pages = {521–532},
numpages = {12},
keywords = {non-affine, sparse matrices, polyhedral model, inspector/executor, loop transformations},
location = {Portland, OR, USA},
series = {PLDI '15}
}

@article{venkat2015chillie,
author = {Venkat, Anand and Hall, Mary and Strout, Michelle},
title = {Loop and Data Transformations for Sparse Matrix Code},
year = {2015},
issue_date = {June 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {50},
number = {6},
issn = {0362-1340},
url = {https://doi-org.stanford.idm.oclc.org/10.1145/2813885.2738003},
doi = {10.1145/2813885.2738003},
abstract = { This paper introduces three new compiler transformations for representing and transforming
sparse matrix computations and their data representations. In cooperation with run-time
inspection, our compiler derives transformed matrix representations and associated
transformed code to implement a variety of representations targeting different architecture
platforms. This systematic approach to combining code and data transformations on
sparse computations, which extends a polyhedral transformation and code generation
framework, permits the compiler to compose these transformations with other transformations
to generate code that is on average within 5% and often exceeds manually-tuned, high-performance
sparse matrix libraries CUSP and OSKI. Additionally, the compiler-generated inspector
codes are on average 1.5 faster than OSKI and perform comparably to CUSP, respectively.
},
journal = {SIGPLAN Not.},
month = jun,
pages = {521–532},
numpages = {12},
keywords = {inspector/executor, loop transformations, polyhedral model, non-affine, sparse matrices}
}

@INPROCEEDINGS{kelm2009rigel,  author={Kelm, John H. and Johnson, Daniel R. and Lumetta, Steven S. and Frank, Matthew I. and Patel, Sanjay J.},  booktitle={2009 18th International Conference on Parallel Architectures and Compilation Techniques},   title={A Task-Centric Memory Model for Scalable Accelerator Architectures},   year={2009},  volume={},  number={},  pages={77-87},  doi={10.1109/PACT.2009.16}}

@ARTICLE{mei2015gpu,  author={Mei, Xinxin and Chu, Xiaowen},  journal={IEEE Transactions on Parallel and Distributed Systems},   title={Dissecting GPU Memory Hierarchy Through Microbenchmarking},   year={2017},  volume={28},  number={1},  pages={72-86},  doi={10.1109/TPDS.2016.2549523}}

@article{kim2016ramulator,
author = {Kim, Yoongu and Yang, Weikun and Mutlu, Onur},
title = {Ramulator: A Fast and Extensible DRAM Simulator},
year = {2016},
issue_date = {January 2016},
publisher = {IEEE Computer Society},
address = {USA},
volume = {15},
number = {1},
issn = {1556-6056},
url = {https://doi.org/10.1109/LCA.2015.2414456},
doi = {10.1109/LCA.2015.2414456},
abstract = {Recently, both industry and academia have proposed many different roadmaps for the
future of DRAM. Consequently, there is a growing need for an  extensible  DRAM simulator,
which can be easily modified to judge the merits of today's DRAM standards as well
as those of tomorrow. In this paper, we present  Ramulator , a fast and cycle-accurate
DRAM simulator that is built from the ground up for extensibility. Unlike existing
simulators, Ramulator is based on a generalized template for modeling a DRAM system,
which is only later infused with the specific details of a DRAM standard. Thanks to
such a decoupled and modular design, Ramulator is able to provide out-of-the-box support
for a wide array of DRAM standards: DDR3/4, LPDDR3/4, GDDR5, WIO1/2, HBM, as well
as some academic proposals (SALP, AL-DRAM, TL-DRAM, RowClone, and SARP). Importantly,
Ramulator does not sacrifice simulation speed to gain extensibility: according to
our evaluations, Ramulator is 2.5  $times$  faster than the next fastest simulator.
Ramulator is released under the permissive BSD license.},
journal = {IEEE Comput. Archit. Lett.},
month = jan,
pages = {45–49},
numpages = {5}
}

@INPROCEEDINGS{zhang2019interconnect,  author={Zhang, Yaqi and Rucker, Alexander and Vilim, Matthew and Prabhakar, Raghu and Hwang, William and Olukotun, Kunle},  booktitle={2019 ACM/IEEE 46th Annual International Symposium on Computer Architecture (ISCA)},   title={Scalable Interconnects for Reconfigurable Spatial Architectures},   year={2019},  volume={},  number={},  pages={615-628},  doi={}}

@inproceedings{viswanath2009evolution,
  title={On the evolution of user interaction in facebook},
  author={Viswanath, Bimal and Mislove, Alan and Cha, Meeyoung and Gummadi, Krishna P},
  booktitle={Proceedings of the 2nd ACM workshop on Online social networks},
  pages={37--42},
  year={2009}
}
@article{davis2011university,
  title={The University of Florida sparse matrix collection},
  author={Davis, Timothy A and Hu, Yifan},
  journal={ACM Transactions on Mathematical Software (TOMS)},
  volume={38},
  number={1},
  pages={1--25},
  year={2011},
  publisher={ACM New York, NY, USA}
}


@article{davis2019graphblas,
author = {Davis, Timothy A.},
title = {Algorithm 1000: SuiteSparse:GraphBLAS: Graph Algorithms in the Language of Sparse Linear Algebra},
year = {2019},
issue_date = {December 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {45},
number = {4},
issn = {0098-3500},
url = {https://doi.org/10.1145/3322125},
doi = {10.1145/3322125},
abstract = {SuiteSparse:GraphBLAS is a full implementation of the GraphBLAS standard, which defines
a set of sparse matrix operations on an extended algebra of semirings using an almost
unlimited variety of operators and types. When applied to sparse adjacency matrices,
these algebraic operations are equivalent to computations on graphs. GraphBLAS provides
a powerful and expressive framework for creating graph algorithms based on the elegant
mathematics of sparse matrix operations on a semiring. An overview of the GraphBLAS
specification is given, followed by a description of the key features and performance
of its implementation in the SuiteSparse:GraphBLAS package.},
journal = {ACM Trans. Math. Softw.},
month = dec,
articleno = {44},
numpages = {25},
keywords = {sparse matrices, Graph algorithms, GraphBLAS}
}

@inbook{gale2020,
author = {Gale, Trevor and Zaharia, Matei and Young, Cliff and Elsen, Erich},
title = {Sparse GPU Kernels for Deep Learning},
year = {2020},
isbn = {9781728199986},
publisher = {IEEE Press},
abstract = {Scientific workloads have traditionally exploited high levels of sparsity to accelerate
computation and reduce memory requirements. While deep neural networks can be made
sparse, achieving practical speedups on GPUs is difficult because these applications
have relatively moderate levels of sparsity that are not sufficient for existing sparse
kernels to outperform their dense counterparts. In this work, we study sparse matrices
from deep learning applications and identify favorable properties that can be exploited
to accelerate computation. Based on these insights, we develop high-performance GPU
kernels for two sparse matrix operations widely applicable in neural networks: sparse
matrix-dense matrix multiplication and sampled dense-dense matrix multiplication.
Our kernels reach 27% of single-precision peak on Nvidia V100 GPUs. Using our kernels,
we demonstrate sparse Transformer and MobileNet models that achieve 1.2--2.1X speedups
and up to 12.8X memory savings without sacrificing accuracy.},
booktitle = {Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis},
articleno = {17},
numpages = {14}
}

@inproceedings{banking,
author = {Wang, Yuxin and Li, Peng and Cong, Jason},
year = {2014},
month = {02},
pages = {199-208},
title = {Theory and algorithm for generalized memory partitioning in high-level synthesis},
doi = {10.1145/2554688.2554780}
}
 @article{zhao2019serving,
  title={Serving recurrent neural networks efficiently with a spatial accelerator},
  author={Zhao, Tian and Zhang, Yaqi and Olukotun, Kunle},
  journal={arXiv preprint arXiv:1909.13654},
  year={2019}
}

@inproceedings{pellauer2019,
author = {Pellauer, Michael and Shao, Yakun Sophia and Clemons, Jason and Crago, Neal and Hegde, Kartik and Venkatesan, Rangharajan and Keckler, Stephen W. and Fletcher, Christopher W. and Emer, Joel},
title = {Buffets: An Efficient and Composable Storage Idiom for Explicit Decoupled Data Orchestration},
year = {2019},
isbn = {9781450362405},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3297858.3304025},
doi = {10.1145/3297858.3304025},
abstract = {Accelerators spend significant area and effort on custom on-chip buffering. Unfortunately, these solutions are strongly tied to particular designs, hampering re-usability across other accelerators or domains. We present buffets, an efficient and composable storage idiom for the needs of accelerators that is independent of any particular design. Buffets have several distinguishing characteristics, including efficient decoupled fills and accesses with fine-grained synchronization, hierarchical composition, and efficient multi-casting. We implement buffets in RTL and show that they only add 2% control overhead over an 8KB RAM. When compared with DMA-managed double-buffered scratchpads and caches across a range of workloads, buffets improve energy-delay-product by 1.53x and 5.39x, respectively.},
booktitle = {Proceedings of the Twenty-Fourth International Conference on Architectural Support for Programming Languages and Operating Systems},
pages = {137–151},
numpages = {15},
keywords = {data orchestration, accelerators, staging buffers, synchronization},
location = {Providence, RI, USA},
series = {ASPLOS '19}
}

@article{blas,
  title={Basic linear algebra subprograms for Fortran usage},
  author={Lawson, Chuck L and Hanson, Richard J. and Kincaid, David R and Krogh, Fred T.},
  journal={ACM Transactions on Mathematical Software (TOMS)},
  volume={5},
  number={3},
  pages={308--323},
  year={1979},
  publisher={ACM New York, NY, USA}
}

% Start of Halide autoscheduling papers

@inproceedings{ragankelley2013,
author = {Ragan-Kelley, Jonathan and Barnes, Connelly and Adams, Andrew and Paris, Sylvain and Durand, Fr\'{e}do and Amarasinghe, Saman},
title = {Halide: A Language and Compiler for Optimizing Parallelism, Locality, and Recomputation in Image Processing Pipelines},
year = {2013},
isbn = {9781450320146},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2491956.2462176},
doi = {10.1145/2491956.2462176},
abstract = {Image processing pipelines combine the challenges of stencil computations and stream programs. They are composed of large graphs of different stencil stages, as well as complex reductions, and stages with global or data-dependent access patterns. Because of their complex structure, the performance difference between a naive implementation of a pipeline and an optimized one is often an order of magnitude. Efficient implementations require optimization of both parallelism and locality, but due to the nature of stencils, there is a fundamental tension between parallelism, locality, and introducing redundant recomputation of shared values.We present a systematic model of the tradeoff space fundamental to stencil pipelines, a schedule representation which describes concrete points in this space for each stage in an image processing pipeline, and an optimizing compiler for the Halide image processing language that synthesizes high performance implementations from a Halide algorithm and a schedule. Combining this compiler with stochastic search over the space of schedules enables terse, composable programs to achieve state-of-the-art performance on a wide range of real image processing pipelines, and across different hardware architectures, including multicores with SIMD, and heterogeneous CPU+GPU execution. From simple Halide programs written in a few hours, we demonstrate performance up to 5x faster than hand-tuned C, intrinsics, and CUDA implementations optimized by experts over weeks or months, for image processing applications beyond the reach of past automatic compilers.},
booktitle = {Proceedings of the 34th ACM SIGPLAN Conference on Programming Language Design and Implementation},
pages = {519–530},
numpages = {12},
keywords = {vectorization, compiler, image processing, optimization, parallelism, domain specific language, autotuning, locality, redundant computation, gpu},
location = {Seattle, Washington, USA},
series = {PLDI '13}
}

@article{mullapudi2016,
author = {Mullapudi, Ravi Teja and Adams, Andrew and Sharlet, Dillon and Ragan-Kelley, Jonathan and Fatahalian, Kayvon},
title = {Automatically Scheduling Halide Image Processing Pipelines},
year = {2016},
issue_date = {July 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {35},
number = {4},
issn = {0730-0301},
url = {https://doi.org/10.1145/2897824.2925952},
doi = {10.1145/2897824.2925952},
abstract = {The Halide image processing language has proven to be an effective system for authoring high-performance image processing code. Halide programmers need only provide a high-level strategy for mapping an image processing pipeline to a parallel machine (a schedule), and the Halide compiler carries out the mechanical task of generating platform-specific code that implements the schedule. Unfortunately, designing high-performance schedules for complex image processing pipelines requires substantial knowledge of modern hardware architecture and code-optimization techniques. In this paper we provide an algorithm for automatically generating high-performance schedules for Halide programs. Our solution extends the function bounds analysis already present in the Halide compiler to automatically perform locality and parallelism-enhancing global program transformations typical of those employed by expert Halide developers. The algorithm does not require costly (and often impractical) auto-tuning, and, in seconds, generates schedules for a broad set of image processing benchmarks that are performance-competitive with, and often better than, schedules manually authored by expert Halide developers on server and mobile CPUs, as well as GPUs.},
journal = {ACM Trans. Graph.},
month = {jul},
articleno = {83},
numpages = {11},
keywords = {halide, image processing, optimizing compilers}
}

@article{adams2019,
author = {Adams, Andrew and Ma, Karima and Anderson, Luke and Baghdadi, Riyadh and Li, Tzu-Mao and Gharbi, Micha\"{e}l and Steiner, Benoit and Johnson, Steven and Fatahalian, Kayvon and Durand, Fr\'{e}do and Ragan-Kelley, Jonathan},
title = {Learning to Optimize Halide with Tree Search and Random Programs},
year = {2019},
issue_date = {July 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {38},
number = {4},
issn = {0730-0301},
url = {https://doi.org/10.1145/3306346.3322967},
doi = {10.1145/3306346.3322967},
abstract = {We present a new algorithm to automatically schedule Halide programs for high-performance image processing and deep learning. We significantly improve upon the performance of previous methods, which considered a limited subset of schedules. We define a parameterization of possible schedules much larger than prior methods and use a variant of beam search to search over it. The search optimizes runtime predicted by a cost model based on a combination of new derived features and machine learning. We train the cost model by generating and featurizing hundreds of thousands of random programs and schedules. We show that this approach operates effectively with or without autotuning. It produces schedules which are on average almost twice as fast as the existing Halide autoscheduler without autotuning, or more than twice as fast with, and is the first automatic scheduling algorithm to significantly outperform human experts on average.},
journal = {ACM Trans. Graph.},
month = {jul},
articleno = {121},
numpages = {12},
keywords = {optimizing compilers, halide}
}

@article{anderson2021,
author = {Anderson, Luke and Adams, Andrew and Ma, Karima and Li, Tzu-Mao and Jin, Tian and Ragan-Kelley, Jonathan},
title = {Efficient Automatic Scheduling of Imaging and Vision Pipelines for the GPU},
year = {2021},
issue_date = {October 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {5},
number = {OOPSLA},
url = {https://doi.org/10.1145/3485486},
doi = {10.1145/3485486},
abstract = {We present a new algorithm to quickly generate high-performance GPU implementations of complex imaging and vision pipelines, directly from high-level Halide algorithm code. It is fully automatic, requiring no schedule templates or hand-optimized kernels. We address the scalability challenge of extending search-based automatic scheduling to map large real-world programs to the deep hierarchies of memory and parallelism on GPU architectures in reasonable compile time. We achieve this using (1) a two-phase search algorithm that first ‘freezes’ decisions for the lowest cost sections of a program, allowing relatively more time to be spent on the important stages, (2) a hierarchical sampling strategy that groups schedules based on their structural similarity, then samples representatives to be evaluated, allowing us to explore a large space with few samples, and (3) memoization of repeated partial schedules, amortizing their cost over all their occurrences. We guide the process with an efficient cost model combining machine learning, program analysis, and GPU architecture knowledge. We evaluate our method’s performance on a diverse suite of real-world imaging and vision pipelines. Our scalability optimizations lead to average compile time speedups of 49x (up to 530x). We find schedules that are on average 1.7x faster than existing automatic solutions (up to 5x), and competitive with what the best human experts were able to achieve in an active effort to beat our automatic results.},
journal = {Proc. ACM Program. Lang.},
month = {oct},
articleno = {109},
numpages = {28},
keywords = {optimizing compilers, Halide}
}

@inproceedings{shan2010fpga,
  title={FPGA and GPU implementation of large scale SpMV},
  author={Shan, Yi and Wu, Tianji and Wang, Yu and Wang, Bo and Wang, Zilong and Xu, Ningyi and Yang, Huazhong},
  booktitle={2010 IEEE 8th Symposium on Application Specific Processors (SASP)},
  pages={64--70},
  year={2010},
  organization={IEEE}
}
@inproceedings{umuroglu2014energy,
  title={An energy efficient column-major backend for FPGA SpMV accelerators},
  author={Umuroglu, Yaman and Jahre, Magnus},
  booktitle={2014 IEEE 32nd International Conference on Computer Design (ICCD)},
  pages={432--439},
  year={2014},
  organization={IEEE}
}
@inproceedings{zhang2009fpga,
  title={FPGA vs. GPU for sparse matrix vector multiply},
  author={Zhang, Yan and Shalabi, Yasser H and Jain, Rishabh and Nagar, Krishna K and Bakos, Jason D},
  booktitle={2009 International Conference on Field-Programmable Technology},
  pages={255--262},
  year={2009},
  organization={IEEE}
}
@inproceedings{grigoras2015accelerating,
  title={Accelerating SpMV on FPGAs by compressing nonzero values},
  author={Grigoras, Paul and Burovskiy, Pavel and Hung, Eddie and Luk, Wayne},
  booktitle={2015 IEEE 23rd Annual International Symposium on Field-Programmable Custom Computing Machines},
  pages={64--67},
  year={2015},
  organization={IEEE}
}
@article{briggs1993efficient,
  title={An efficient representation for sparse sets},
  author={Briggs, Preston and Torczon, Linda},
  journal={ACM Letters on Programming Languages and Systems (LOPLAS)},
  volume={2},
  number={1-4},
  pages={59--69},
  year={1993},
  publisher={ACM New York, NY, USA}
}

@article{henry2021,
author = {Henry, Rawn and Hsu, Olivia and Yadav, Rohan and Chou, Stephen and Olukotun, Kunle and Amarasinghe, Saman and Kjolstad, Fredrik},
title = {Compilation of Sparse Array Programming Models},
year = {2021},
issue_date = {October 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {5},
number = {OOPSLA},
url = {https://doi.org/10.1145/3485505},
doi = {10.1145/3485505},
abstract = {This paper shows how to compile sparse array programming languages. A sparse array programming language is an array programming language that supports element-wise application, reduction, and broadcasting of arbitrary functions over dense and sparse arrays with any fill value. Such a language has great expressive power and can express sparse and dense linear and tensor algebra, functions over images, exclusion and inclusion filters, and even graph algorithms. Our compiler strategy generalizes prior work in the literature on sparse tensor algebra compilation to support any function applied to sparse arrays, instead of only addition and multiplication. To achieve this, we generalize the notion of sparse iteration spaces beyond intersections and unions. These iteration spaces are automatically derived by considering how algebraic properties annotated onto functions interact with the fill values of the arrays. We then show how to compile these iteration spaces to efficient code. When compared with two widely-used Python sparse array packages, our evaluation shows that we generate built-in sparse array library features with a performance of 1.4\texttimes{} to 53.7\texttimes{} when measured against PyData/Sparse for user-defined functions and between 0.98\texttimes{} and 5.53\texttimes{} when measured against SciPy/Sparse for sparse array slicing. Our technique outperforms PyData/Sparse by 6.58\texttimes{} to 70.3\texttimes{}, and (where applicable) performs between 0.96\texttimes{} and 28.9\texttimes{} that of a dense NumPy implementation, on end-to-end sparse array applications. We also implement graph linear algebra kernels in our system with a performance of between 0.56\texttimes{} and 3.50\texttimes{} compared to that of the hand-optimized SuiteSparse:GraphBLAS library.},
journal = {Proc. ACM Program. Lang.},
month = {oct},
articleno = {128},
numpages = {29},
keywords = {Sparse Array Programming, Compilation, Sparse Arrays}
}
