\section{Controllable Summary Generation}
We frame the controllable summary generation as a natural language generation task. We compare our proposed approach with multiple baselines for both extractive and abstractive summarization in \S \ref{sec:result} using multiple automatic metrics. We find that the structure aware summary generation outperforms vanilla models. We additionally propose a salient rewarding metrics that can more precisely evaluate the coverage and adherence of generation result while giving the user provided prompt.

\subsection{Structure Guided Summary Generation}
From the corpus released by (cite Huihui's newest paper),  annotators are asked to annotate individual sentences in the summary following the IRC schema (described in \S dataset). Thus, the annotated data comes with a label sequence which represents the structure of the summary. One example can be found in Figure \ref{}. For the abstractive summarziation models, we add the annotated text sequence before the original texts, thus letting the model to implicitly map the structure sequence to the generation summary sentences.

% \subsection{Long Input Split}
% Most neural extrative summarizers are limited by the length of inputs, which is usually 512 tokens cite{BERT} and fall far below the average length of 5k tokens in the legal documents. It is impractical to concatenate all sentences in a document and encode them with a large pretrained model. Following \cite{mao2021dyle}, we group consecutive sentences into \textit{chunks}. We set the maximum length at 512 and allow for 20\% overlapping with the previous chunk. This way we could include the intermediate sentences in multiple chunks, wishing to capture local dependencies within the same section or paragraph. 

\subsection{Extractive Summarization}
\paragraph{Extractive oracles.} \textit{Extractive oracles} denotes a set of selected text snippets whose concatenation maximizes the evaluation metric given the gold summary. We first utilize the \textbf{IRC\_Oracle}, which sentences manually labelled with the IRC types are concatenated together to form a extractive summary. Following \cite{nallapati2017summarunner}, we additionally report \textbf{EXT\_ORACLE}, a baseline for an
oracle summarizer \cite{nallapati2017summarunner}, which greedily select sentences from the original article based on the ROUGE-L scores comparing to the abstractive summary.

\paragraph{Extractive Baselines}
We compare the proposed framework with various baselines. For unsupervised extractive summarization models, we compare with LSA  \cite{Steinberger2004UsingLS},
LexRank \cite{erkan2004lexrank}, TextRank \cite{DBLP:journals/corr/BarriosLAW16}, PACSUM \cite{zheng-lapata-2019-sentence}. We further include HipoRank \cite{dong-etal-2021-discourse}, which achieved the state of the art performance on scientific paper summarization, which also has long text inputs.
For supervised methods, BERTEXT \cite{liu-lapata-2019-text} uses a novel document-level encoder based on BERT which is able to express the semantics of a document and obtain representations for
its sentences. 

\subsection{Abstractive Summarization}
For abstracive summarization, we experiment with BART \cite{lewis-etal-2020-bart} and the Longformer-Encoder-Decoder (LED) \cite{Beltagy2020Longformer}. The latter model can process longer input document up-to 16k tokens. 

\subsection{Implementation Details}
For PACUSUM, we used the released bert checkpoint\footnote{\url{https://github.com/mswellhao/PacSum}}.For BERTEXT without fine-tuning, we use the released checkpoint\footnote{https://github.com/nlpyang/PreSumm} trained on CNN$/$DailyMail Dataset and inference the extracted sentence from each chunk. One limitation of training the BERTEXT model is that the BERT model can only process input tokens up to 512. Following \cite{mao2021dyle}, we group consecutive sentences of a long document into \textit{chunks}. We set the maximum length at 512 and allow for 20\% overlapping with the previous chunk. This way we could include the intermediate sentences in multiple chunks, wishing to capture local dependencies within the same section or paragraph. We additionally employ the similar greedy algorithm in EXT\_ORACLE to construct the extractive labels for model training. Due to the limitation of computing resources, we set the maximum input length as 1,024 and 6,144 for BART-large and LED model respectively. For neural based models, we finetune models for at most 5 epochs with a learning rate of 2e-5, batch size of 1 with gradient accumulation steps at 4. We save the checkpoints at every 1,000 steps, utilizing early stopping based on ROUGE-2 F1 scores and report the average of 3 best checkpoints on validation set. 

\subsection{Evaluation Metrics}
For automatic evaluation, we report ROUGE-1, ROUGE-2, and ROUGE-LSum. We additionally report BERTScore \cite{bert-score} to measure the semantic meanings of the generated summary. 
\section{Results and Analysis}\label{sec:result}
We discuss the evaluation results and effects in this section and aim to answer multiple research questions.\\
\textbf{RQ1: How well can the extractive methods perform while comparing against the human-written summaries}
Table \ref{tab:all_result} shows the evaluation results on the test set. In row 1 and 2, IRC\_ORACLE can be viewed as an upper-bound for an extractive model that could accurately capture all ``Issue Reason Conclusion'' sentences in the original article. The high rouge scores imply that there exist a substantial amount of content overlapping when legal experts draft the abstractive summary, and thus extractive methods could produce high-quality candidates. \\
\textbf{RQ2: How do the unsupervised and supervised baselines work on the legal texts}
row 3-8 demonstrate the performances of frequently used unsupervised baselines. We observe that the performances are not as good as the supervised abstractive methods (row 12 and 14). Meanwhile, the traditional approaches such as LexRank and TextRank suffer from the redundant contents selection (700 tokens v.s. the reference's 240 tokens in selected sentences).\\
\textbf{RQ3: would the incorporation of longer input bring benefits for supervised extractive summarziation?}
Since there are overlapping between different chunks of the test file, we propose a simple method to post-processing all extracted sentences from the same article but within different chunks and name it as BERT\textsubscript{EXT} - selection. (We group all sentences together, de-duplicate and sort the sentences based on their original order in the article). This brings over 20 ROUGE-1 improvements, 8.7 ROUGE-2 improvements, and 18.77 ROUGE-LSum improvements comparing to the baseline model which is only exposed to the first 512 tokens. This suggests that the truncation of texts would result in the loss of information. \\
\textbf{RQ4: Whether the proposed structure-based generation lead to better performance?} We mainly compare the model's performance on the abstractive summarization in row 12-15. We observe stable improvements across all three ROUGE scores and minor improvements on BertScore. This suggests that the summaries generated by our approach is closer to the oracle references, thus being more readable and understandable. \\
Meanwhile, to 

\textbf{RQ5: Would more training data help?} The original CANLLI dataset comes with around 28k case and summary pairs. To validate this hypothesis, we retrain two baseline models of BART and LED on the non-overlapped 27k pairs that are not annotated with IRC, and report the model's performance in row 16 and 17. Not surprisingly, around 5 points improvements in all ROUGE scores are obtained, suggesting that the larger training data could benefit the model's capbilty to better summarize the salient information in long legal case documents.
\begin{table*}[h!]
\small
% \scriptsize 
    \centering
    % \setlength\tabcolsep{2.1pt}
    \renewcommand{\arraystretch}{1.1}%Tighter
    \begin{tabular}{c|l|ccc|ccc|c}
    %\begin{tabular}{l|lllc|lllc|p{0.9cm}ll}
    \toprule
    ID &  & \multicolumn{3}{c}{\textbf{Quality}} & \multicolumn{3}{|c}{\textbf{BertScore}} &\multicolumn{1}{|c}{\textbf{Length Metrics}} \\
    
      &   &   R-1 & R-2 & R-L  & F1 &  P  &  R & Abs. Len \\
         \midrule
         & Reference &  -- &	 -- &	 --  & -- & -- & -- & 240.31  \\
         
         \midrule
         \multicolumn{9}{c}{\textbf{Extractive Methods}} \\
         \midrule
         1 & IRC\_ORACLE & 57.92 & 35.72 & 55.05 &88.15& 87.78& 88.57 & 243.09 \\
        2 &  EXT ORACLE (R-L, F1) & 61.41 & 40.44 & 59.08 & 88.19& 87.51& 88.97 & 209.37 \\
         \midrule 
         
           \multicolumn{9}{l}{\textit{unsupervised}} \\
           3 & LSA \cite{Steinberger2004UsingLS} & 38.15 & 18.22 & 35.76 & 84.63& 83.48& 85.84 & 787.98  \\
           4 & LexRank \cite{erkan2004lexrank} & 38.63 & 17.70 & 35.87 & 84.40& 83.47& 85.39 & 705.56 \\
            5 & TextRank \cite{DBLP:journals/corr/BarriosLAW16}  & 36.70 & 16.19 & 34.00 & 83.53& 82.20& 84.95 & 722.46  \\
            6 & PACSUM \cite{zheng-lapata-2019-sentence} & 39.24 & 14.31 & 36.80 & 81.64& 79.80& 83.60 & 188.44\\
           7 & PACSUM - LEGAL\_BERT & 37.83 & 13.50 & 35.42  &   81.54& 79.67& 83.53& 188.21 \\
           8 & HipoRank \cite{dong-etal-2021-discourse} & 42.85 & 17.75&  40.08 & 83.53& 82.10& 85.05 & 228.09 \\
           \midrule
        %   \multicolumn{9}{l}{\textit{supervised}} \\
        %   BERT\textsubscript{EXT} - selection & -- & -- & -- \\
           
        %  \midrule
             \multicolumn{9}{l}{\textit{w/o fine-tuning}} \\
        9 &  BERT\textsubscript{EXT} - first 512 token & 18.32 & 7.31& 16.76 &81.77& 82.88& 80.75 & 43.41  \\
      
         10 & BERT\textsubscript{EXT} - selection & 39.14 & 16.38 & 36.40 & 83.84& 82.71& 85.04 & 581.45 \\
          
         \midrule
          \multicolumn{9}{l}{\textit{w/ fine-tuning}} \\

          11 & BERT\textsubscript{EXT} - selection (ckpt 30000) & 40.78 & 17.63 & 37.75 & 84.05& 83.00& 85.19 & 588.60\\
          \midrule
         \multicolumn{9}{c}{\textbf{Abstractive Methods}} \\
         \midrule
       12 & LED   & 49.56 & 23.78 & 46.48  & 86.75& 87.00& 86.56 & 199.47  \\
      
       13 & LED + prompt &  50.69	& 24.33	& 47.53 & 86.87& 86.84& 86.95 & 223.58  \\
       14 &  BART  & 47.93 & 22.34 & 44.74 &  86.16& 85.59& 86.81 & 284.08 \\
        15& BART  + prompt & 51.44 & 23.29 & 47.99  &86.51& 86.12& 86.95 & 250.40 \\
       
       \midrule 
       \multicolumn{9}{l}{\textit{w/ larger training data}}\\
       16 &  BART + larger train & 50.47 & 25.15 & 46.60 & 87.27& 87.77& 86.82 & 172.86\\
       17 &  LED + larger train & 54.41 &  29.57 & 51.17 & 87.86& 87.82& 87.95 & 234.16 \\
       

        % \midrule
        % \multicolumn{9}{l}{\textit{Different Styles}} \\
        %  Style-1 &  48.33 &	22.88 &	45.05 & 0.9613  &  9.16  &  0.74 & 210.60 & 0.1183 \\
        %  Style-2 &  48.28 &	22.61 &	44.99 & 0.9623  &  9.16  &  0.74 & 214.57 & 0.1192 \\
        %  Style-3 &  41.89 &	18.83 &	39.16 & 0.9621  &  8.29  &  0.73 & 142.75 & 0.0808 \\
        % Style-4 &  48.61 &	\textbf{23.24} &	{45.61} & 0.9627  &  9.56  &  0.74 & 223.58 & 0.1212 \\
           
        % \midrule
        
        \bottomrule
        
    \end{tabular}
    \caption{Model Performance for the original test set with 106 instances from the 1049 annotated dataset.}
    \label{tab:all_result}
\end{table*}

% OLD TABLE with abstractedness and density. 
% \begin{table*}[t!]
% \small
% % \scriptsize 
%     \centering
%     % \setlength\tabcolsep{2.1pt}
%     \renewcommand{\arraystretch}{1.1}%Tighter
%     \begin{tabular}{l|ccc|ccc|cc}
%     %\begin{tabular}{l|lllc|lllc|p{0.9cm}ll}
%     \toprule
%     & \multicolumn{3}{c}{\textbf{Quality}} & \multicolumn{3}{|c}{\textbf{Abstractiveness}} &\multicolumn{2}{|c}{\textbf{Length Metrics}} \\
    
%          &   R-1& R-2 & R-L  & Coverage  &  Density  &  2G-overlap & Abs. Len & Comp. Ratio \\
%          \midrule
%          Reference &  -- &	 -- &	 --  & 0.9338  &  7.50  &  0.61 & 240.31 & 0.1242 \\
         
%          \midrule
%          \multicolumn{9}{l}{\textbf{Extractive Methods}} \\
%          IRC\_ORACLE & 57.92 & 35.72 & 55.05 & 0.6559 & 2.18 & 0.18 & 243.09 & 0.1511\\
%          EXT ORACLE & 61.41 & 40.44 & 59.08 \\
%          \midrule 
%              \multicolumn{9}{l}{\textit{w/o fine-tuning}} \\
%          BERT\textsubscript{EXT} - first 512 token & 18.32 & 7.31& 16.76 & 0.6184 & 2.05 & 0.20 & 43.41 & 0.0260 \\
      
%           BERT\textsubscript{EXT} - selection & 39.14 & 16.38 & 36.40 & 0.6340 & 2.19 & 0.16 & 581.45 & 0.3674\\
%           \midrule
%           \multicolumn{9}{l}{\textit{unsupervised methods}} \\
%             TEXTRANK \cite{DBLP:journals/corr/BarriosLAW16}  & 32.82 & 16.37 & 30.81 \\
%             PACSUM \cite{zheng-lapata-2019-sentence} & 38.10 & 14.14 & 35.82 \\
%             PACSUM - LegalBert & 37.60 & 13.58 & 35.22 \\
%           \midrule
%           \multicolumn{9}{l}{\textit{supervised}} \\
%           BERT\textsubscript{EXT} - selection & -- & -- & -- \\
           
%          \midrule
%          \midrule
%          \multicolumn{9}{l}{\textbf{Abstractive Methods}} \\
%       LED  -- no prompt & 49.56 & 23.78 & 46.48  & 0.9655  &  10.61  &  0.77 & 199.47 & 0.1097 \\
%         LED &  50.69	& 24.33	& 47.53 & 0.9627  &  9.56  &  0.74 & 223.58 & 0.1212 \\
%         BART -- no prompt & 47.93 & 22.34 & 44.74 &  0.9508 & 8.83 & 0.68 & 284.08 & 0.1609\\
%         BART  & 51.44 & 23.29 & 47.99  & 0.9507 &  8.49  &  0.68 & 250.40 & 0.1402 \\
        
%         \midrule
%         \multicolumn{9}{l}{\textit{Different Styles}} \\
%          Style-1 &  48.33 &	22.88 &	45.05 & 0.9613  &  9.16  &  0.74 & 210.60 & 0.1183 \\
%          Style-2 &  48.28 &	22.61 &	44.99 & 0.9623  &  9.16  &  0.74 & 214.57 & 0.1192 \\
%          Style-3 &  41.89 &	18.83 &	39.16 & 0.9621  &  8.29  &  0.73 & 142.75 & 0.0808 \\
%         Style-4 &  48.61 &	\textbf{23.24} &	{45.61} & 0.9627  &  9.56  &  0.74 & 223.58 & 0.1212 \\
           
%         % \midrule
        
%         \bottomrule
        
%     \end{tabular}
%     \caption{Model Performance}
%     \label{tab:all_result}
% \end{table*}


% \begin{table*}[t!]
% \small
% % \scriptsize 
%     \centering
%     % \setlength\tabcolsep{2.1pt}
%     \renewcommand{\arraystretch}{1.1}%Tighter
%     \begin{tabular}{l|ccc|ccc|cc}
%     %\begin{tabular}{l|lllc|lllc|p{0.9cm}ll}
%     \toprule
%     & \multicolumn{3}{c}{\textbf{Quality}} & \multicolumn{3}{|c}{\textbf{BertScore}} &\multicolumn{2}{|c}{\textbf{Length Metrics}} \\
    
%          &   R-1 & R-2 & R-L  & P  &  R  &  F1 & Abs. Len & Comp. Ratio \\
%          \midrule
%          Reference &  -- &	 -- &	 --  & 0.9338  &  7.50  &  0.61 & 240.31 & 0.1242 \\
         
%          \midrule
%          \multicolumn{9}{c}{\textbf{Extractive Methods}} \\
%          \midrule
%          IRC\_ORACLE &  -- & -- & -- & \\
%          EXT ORACLE (R-L, F1) &  -- & -- & -- & \\
%          \midrule 
         
%           \multicolumn{9}{l}{\textit{unsupervised methods}} \\
%           LSA \cite{Steinberger2004UsingLS} & -- & -- & -- \\
%           LexRank \cite{erkan2004lexrank}  & -- & -- & -- \\
%             TextRank \cite{DBLP:journals/corr/BarriosLAW16}  & -- & -- & -- \\
%             PACSUM \cite{zheng-lapata-2019-sentence}  & -- & -- & -- \\
            
%             HipoRank \cite{dong-etal-2021-discourse} &  & -- & -- & --\\
%           \midrule
%         %   \multicolumn{9}{l}{\textit{supervised}} \\
%         %   BERT\textsubscript{EXT} - selection & -- & -- & -- \\
           
%         %  \midrule
%              \multicolumn{9}{l}{\textit{w/o fine-tuning}} \\
%           BERT\textsubscript{EXT} - selection & 39.14 & 16.38 & 36.40 & 0.6340 & 2.19 & 0.16 & 581.45 & 0.3674\\
          
%          \midrule
%          \multicolumn{9}{c}{\textbf{Abstractive Methods}} \\
%          \midrule
%       LED  -- no prompt &  53.72 & 28.75 & 50.17 & 87.55& 87.38& 87.77   \\
%         BART -- no prompt & 50.50 & 25.58 & 46.82 & 87.25& 87.71& 86.84 \\
        
%         \bottomrule
        
%     \end{tabular}
%     \caption{Model Performance trained on 27241 pairs and evaluated on all 1049 annotated sentences.}
%     \label{tab:all_result}
% \end{table*}
