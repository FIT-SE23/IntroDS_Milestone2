\section{Case Decision Summarization Dataset}\label{sec:dataset}
Recent work has introduced a number of legal document summarization or salient information identification tasks with associated datasets, e.g., for bill %or case 
summarization %\cite{kornilova-eidelman-2019-billsum, malik2021semantic} 
\cite{kornilova-eidelman-2019-billsum}
and for case sentence argumentive classification \cite{xu-2021-position-case} and rhetorical role prediction
\cite{malik2021semantic}.
%However, unlike the news and scientific articles, only limited amount of annotated data point are available and the size is of orders of magnitude smaller. 
\begin{table}[t!]
% \small
% \scriptsize 
    \centering
    % \setlength\tabcolsep{2.1pt}
    \renewcommand{\arraystretch}{1}%Tighter
    \begin{tabular}{l|c}
    
    %\begin{tabular}{l|lllc|lllc|p{0.9cm}ll}
   \toprule
 %  &  CanLII  \\
 %\midrule
    Case length (avg. \# words) & 3,971\\
    Summary length  (avg. \# words) & 266\\
    Training set  (\# case/summary pairs) & 27,241 \\
    
    Testing set  (\# case/summary pairs) & 1,049\\
    
%      Name & Size & Doc. len & Summ. len. \\
%     \midrule
%     BillSum & 22,218 & 1,592 &  197 \\
%   CanLII & 28,290 & 3,971   &  266 \\
%   \midrule
%     XSum & 226k &  431 & 23 \\
%     CNN/DM &  311k & 766 & 53 \\
%     \midrule
%     PubMed  & 133k  &  3,016 & 203 \\
%     arXiv &  215k & 4,938 & 220 \\
    \bottomrule
    \end{tabular}
    \caption{Dataset statistics of CanLII.}
    \label{tab:data_stats}
\end{table}
\begin{figure}[t!]
\centering
 \includegraphics[width=.9\linewidth]{Figs/IRC_distribution.png}

  \caption{Fraction of sentences annotated as argumentative (using the IRC scheme) in the case documents versus in the summaries of the CanLII test set. Though only a small fraction of sentences in the original document are annotated as IRCs, IRCs are a large fraction of the human-written summaries.}
  \label{fig: disbribution}
\end{figure} 
Similarly to \citet{xu-2021-position-case}, we use the {\bf CanLII} (Canadian Legal Information Institute) dataset of legal case decisions and summaries\footnote{The data was obtained through an agreement with the Canadian Legal Information Institute (CanLII) (\url{https://www.canlii.org/en/}).}. 
%Persons interested in accessing the data should contact CanLII directly.}.   %Kevin suggested this wording
%\footnote{The data can be obtained through a license agreement with the Canadian Legal Information Institute (\url{https://www.canlii.org/en/}).}%\footnote{We provide examples from this dataset in Appendix \ref{appendix:output}.} 
  Full corpus statistics are provided in Table \ref{tab:data_stats}, while an example case/summary pair from the test set is provided in Figure \ref{fig:IRC_example} in Appendix \ref{appendix:1}.
%ref{appendix:output}.
%\\
    % (1) \textbf{BillSum}\footnote{\url{https://github.com/FiscalNote/BillSum}} \cite{kornilova-eidelman-2019-billsum}, which contains 22,218 US Congressional Bills with human-written references and has been split into 18,949 train bills and 3,269 test bills. \\
%\textbf{CanLII}  
%of the non-profit Canadian Legal Information Institute\footnote{\url{https://www.canlii.org/en/}}. 

\citet{xu-2021-position-case} only used a small portion of this dataset for their work in argumentative classification. Conjecturing that explicitly identifying the decision's argumentative components would be crucial in case summarization,  they annotated 1,049  case and human-written summary pairs curated from the full dataset.
%CanLII Connects website\footnote{\url{https://canliiconnects.org/en}}. %\citet{xu-2021-position-case}
In particular, they recruited legal experts to annotate the document on the sentence level, adopting an \textbf{IRC scheme} (see Figure \ref{fig:IRC_example} in Appendix \ref{appendix:1}) which classifies individual sentences into one of  four categories: \textbf{Issue} (legal question  addressed in the case), \textbf{Conclusion} (courtâ€™s decisions for the corresponding issue), \textbf{Reason} (text snippets explaining why the court made such conclusion) and \textbf{Non\_IRC} (none of the above). The distributions of the IRC labels in  the cases and summaries are shown in Figure \ref{fig: disbribution} and illustrate that argumentative sentences do indeed play an important role in human summaries. We utilized the unannotated 27,241  pairs to train a  supervised model baseline and the 1049 annotated pairs as our test set.  While none of our summarization methods  use the IRC annotations, they are used during testing as the basis of a domain-specific evaluation metric.

%For comparison, the statistics of the commonly used mews and scientific domain summaries are also included \cite{nallapati-etal-2016-abstractive, narayan-etal-2018-dont, cohan-etal-2018-discourse}.  



% \begin{table}[t!]
% % \small
% % \scriptsize 
%     \centering
%     % \setlength\tabcolsep{2.1pt}
%     \renewcommand{\arraystretch}{1}%Tighter
%     \begin{tabular}{c|c| cc}
    
%     %\begin{tabular}{l|lllc|lllc|p{0.9cm}ll}
%     \toprule
%      Name & Size & Doc. len & Summ. len. \\
%     \midrule
%     BillSum & 22,218 & 1,592 &  197 \\
%   CanLII & 28,290 & 3,971   &  266 \\
% %   \midrule
% %     XSum & 226k &  431 & 23 \\
% %     CNN/DM &  311k & 766 & 53 \\
% %     \midrule
% %     PubMed  & 133k  &  3,016 & 203 \\
% %     arXiv &  215k & 4,938 & 220 \\
%     \bottomrule
%     \end{tabular}
%     \caption{Dataset statistics on legal documents (BillSum and CanLII), news articles (CNN/Daily Mail and XSum
% ) and long scientific documents (PubMed and arXiv).}
%     \label{tab:data_stats}
% \end{table}
