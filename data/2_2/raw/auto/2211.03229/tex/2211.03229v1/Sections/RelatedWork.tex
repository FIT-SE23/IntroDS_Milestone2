\section{Related Work}

% \subsection{Controllable Summary Generation}
% Controllable generation can be described as methods to control a language model’s text generation given some kind of guidance. Work has focused on several aspects such as the summary length \cite{liu-etal-2018-controlling, fan-etal-2018-controllable, saito2020length}, styles \cite{zhang-etal-2018-shaped, jin-etal-2020-hooks} or via prompting \cite{he2020ctrlsum}. \citet{song-etal-2021-new} introduced a two-stage approach to first generate
% multiple summaries, then applying filters to select the summary with
% the desired style or property. \citet{goyal2021hydrasum} further utilize multiple
% decoders which specialized in different stylistic features into a transformer framework. These approaches focused on controlling certain style aspects of the text. On the other hand, \cite{dou-etal-2021-gsum} recently proposed a framework to incorporate extra guiding signals like keywords or salient sentences into the summarization models. Different from previous work, instead hinting the model with extra signals for better content selection, our approach aims at generating summaries with explicitly controlled discourse structure, that can be learned from the reference summary.

%\subsection
{\bf Supervised Extractive Summarization Using Discourse Information}
Graph-based methods have been exploited for extractive summarization tasks to better model the inter-sentence relations based on document structure. \citet{xu-etal-2020-discourse} applied a GCN layer to aggregate information from the document's discourse graph based on RST trees and dependencies.  More recently, 
 HiStruct+ \cite{ruan-etal-2022-histruct} and HEGEL \cite{zhang-2022-hegel} started to incorporate the hierarchical structure and topic structure of scientific articles into supervised model training, respectively. However, HiSruct+ relied on the relatively fixed and explicit document structure of scientific articles\footnote{Section titles following a shared pattern (introduction, method … and conclusion) are encoded to provide structural information. However, in our dataset, sectioning is often missing 
 %– 297 test articles do not come with split paragraphs – or do not come with meaningful section titles %
 or not meaningful (e.g.,  titles such as "section 1").},
 %(i.e., some legal documents have section titles of ``section 1'', ``section 2'', etc.)},
 while HEGEL relied on a large training set to identify the topic distributions. \textit{Our work uses an unsupervised extractive summarization approach in a lower-resource setting, as well as studies the effects of computing different types of document structures.}
 We leave the exploration of the aforementioned supervised approaches on legal domain texts for future work.  
%\subsection

{\bf Unsupervised Extractive Summarization}
Traditional extractive summarization methods are mostly unsupervised \cite{radev-etal-2000-centroid, 10.5555/2832415.2832442,  hirao-etal-2013-single}, where a large portion apply the graph-based algorithms \cite{SALTON1997193, Steinberger2004UsingLS,erkan2004lexrank} or are based on term frequencies such as n-gram overlaps \cite{Nenkova2005TheIO} to rank the sentences' importance. More recently, pretrained transformer-based models \cite{devlin-etal-2019-bert, lewis-etal-2020-bart, zhang2020pegasus} have provided better sentence representations. %for unsupervised summarization 
%\cite{zheng-lapata-2019-sentence, dong-etal-2021-discourse}. 
For instance, \citet{zheng-lapata-2019-sentence} built directed unsupervised graph-based models on news articles using BERT-based sentence representations and achieved comparable performance to supervised models on multiple benchmarks. \citet{dong-etal-2021-discourse} %further 
augmented the document graph of \citet{zheng-lapata-2019-sentence}
with sentence position and section hierarchy to reflect the
document structure of scientific articles. \textit{Different from these two works which are based on assumptions of news and scientific article structures, our method uses reweighting to better utilize the document structure of legal cases.}

%\subsection
{\bf Extractive Summarization of Legal Texts}
%Extractive summarization of legal case documents remains a practical challenge.
Despite the success of supervised neural network  models in news and scientific article summarization \cite{zhang2020pegasus, lewis-etal-2020-bart,zaheer2020bigbird}, they face challenges in legal document summarization given the longer texts, distinct document structure, and limited training data \cite{bhattacharya2019comparativestudy}. Instead, prior work has tackled legal extractive summarization by applying domain independent unsupervised algorithms \cite{Luhn-1958-automatic,erkan2004lexrank,  sarvanan-2006-graphical}, or designing domain specific supervised approaches \cite{sarvanan-2006-graphical, polsley-etal-2016-casesummarizer,zhong-2019-iterativemasking}. One recent work \cite{bhattacharya2021incorporating} frames the task as Integer Linear Programming and demonstrates the importance of in-domain structure and legal knowledge. %while comparing to general supervised models such as \textsc{SummaRuNNer} \cite{nallapati2017summarunner} and \textsc{BertSum} \cite{liu-lapata-2019-text}. 
In another line of research, \citet{xu-2021-position-case} propose a sentence classification task with the hope of exploiting the court decision's \textit{argument structure} by making explicit its issues, conclusions, and reasons (i.e., argument triples). \textit{Our work is unsupervised
%does not require supervised training process 
and implicitly reveals the relations between argument triples to generate better summaries.}