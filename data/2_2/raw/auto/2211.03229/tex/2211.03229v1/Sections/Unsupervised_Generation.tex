
% \section{Unsupervised Summarization}




\section{Experimental Setup}
%We discuss the evaluation metrics and other models  used in our experiments. 
For supervised models, we split the training data in an 80:20 ratio for training and validation. For unsupervised models, we tune the hyperparameters on the validation set. Model training details can be found in Appendix \ref{appendix:detail}. 

% \subsection{Datasets}
% Our experiments are conducted on CanLII \cite{} and
% BillSum \cite{kornilova-eidelman-2019-billsum}, two large-scale datasets
% of long and structured legal documents with human-written summaries. The average source article
% length is xxxx and 1382 tokens, making them ideal
% candidates to test our method.

%\subsection{Oracle and Baseline Models}



\begin{table*}[t]
% \small
    \centering
    \renewcommand{\arraystretch}{1}%Tighter
    \begin{tabular}{c|l|cccc}
    %\begin{tabular}{l|lllc|lllc|p{0.9cm}ll}
    \toprule
  %      & & \multicolumn{2}{c}{\textbf{CanLII}} \\
      ID & Model  &  R-1 & R-2 & R-L   & BS\\
         \midrule
         \multicolumn{6}{c}{{Oracles}} \\
         \midrule
          1 & IRC & 58.04 & 36.02 &55.28 & 87.94  \\
        2 &  EXT (ROUGE-L, F1) & 59.38 & 38.77 & 56.94 & 87.85 \\
        
   
          
    \midrule 
    \multicolumn{6}{c}{Extractive baselines (no document structure)} \\
    \midrule
    \multicolumn{6}{l}{\textit{supervised}} \\
    % 3 & BERT\textsubscript{EXT} - selection & 40.78 & 17.63 & 37.75 & 84.05 \\
    3 & BERT\_EXT & \textit{43.44} & {17.84} & \textit{40.36} & {84.47}  \\
    %  5 & SentCLR & & & & \\
    % \midrule
    % \multicolumn{6}{c}{{Supervised Abstractive}} \\
    % \midrule
   
    % 6 & BART & 50.50/25.58/46.82 &  87.25& \\
    % 7 & LED & 53.72/28.75/ 50.17 & 87.55&\\
    % \midrule 
        \multicolumn{6}{l}{\textit{unsupervised}} \\
        % \midrule 
       4 & LSA & 37.22 & 17.82 & 34.87 & {\bf \underline{\textit{84.48}}} \\
           5 & LexRank & 37.90 &\underline{\textit{18.17}} & 35.62 &  84.32 \\
            6 & TextRank  & 36.70 &16.19 &34.00 & 83.51 \\
            7 & PACSUM & 40.01 & 15.68 &37.37 & 83.52  \\
            %8 & HipoRank (Original Structure) &  \underline{41.61} & 17.13 & \underline{\textit{38.73}}  & 83.55  \\
           \midrule 
           \multicolumn{6}{c}{{HipoRank backbone (with different computed document structures) 
           %with HipoRank backbone
           }} \\
          \midrule
                %   12* & Original 
          8 & Original Structure &  {41.61} & 17.13 & {38.73}  & 83.55  \\
          9 & C99-topic & 41.33 & 16.48 & 38.45 &  83.53  \\
          10  & HMM-stage & 40.71 & 15.64 & 37.93 & 83.57 \\
        %   \midrule 
          11 & Original Structure w/o header & 42.58 & 18.01 & 39.63 & 83.62 \\
        %   & Original w/o header + truncation & 42.85/17.46/39.88 \\ 
          12 & C99-topic  w/o header & \underline{43.25} & 18.02 & \underline{40.25} & \underline{\textit{\textbf{84.48}}} \\
        
          13 & HMM-stage  w/o header & 42.64 & 17.38 & 39.76 & 83.57\\
          
          \midrule 
              \multicolumn{6}{c}{{Ours (HipoRank backbone + Reweighting Algorithm)}} \\
              %Document Structure with HipoRank backbone + reweighting Algorithm}} \\
              \midrule
          14 & Original Structure w/o header & 43.14 & {18.46} & 40.23 &  84.20  \\ 
          15 & C99-topic w/o header & \textbf{43.90} & \textbf{18.67} & \textbf{41.00} &  {84.34} \\
            16 & HMM-stage w/o header & {43.28} & 17.80 &40.40 &  84.22 \\ 
            % 17 & Ours + sentence\_relation & 43.26/18.33/40.38 & \\
            
        \midrule
    \bottomrule
    \end{tabular}
    \caption{The automatic evaluation results on the CanLII test set. \textbf{Bold} represents the best non-oracle score, \textit{italic} the best baseline/backbone score, and \underline{underline} the best unsupervised baseline/backbone score.} %The model names with "w/o header" use the without header version of CanLII dataet as input to .} % need to make sure the notation is obvious. 
    \label{tab:unsupervised_results}
\end{table*}

\textbf{Upper Bound Oracles}\label{sec:oracle}  %Taking advantage of the annotations in our test set and 
Based on  Figure \ref{fig: disbribution}, we create a domain-dependent \textbf{IRC\_Oracle} model where test sentences  manually annotated with the IRC  labels are concatenated to form the summary.   Following \citet{nallapati2017summarunner}, we also report results for \textbf{EXT\_Oracle}, a domain-independent
summarizer %\cite{nallapati2017summarunner}, 
which greedily selects sentences from the original document based on the ROUGE-L scores compared to the abstractive human summary. %This model selects sentences whose concatenation maximizes the evaluation metric given the gold summary.

\textbf{Extractive Baselines} %We compare our proposed discourse views and reweighting algorithm with various baselines. 
For unsupervised models, we compare with LSA  \cite{Steinberger2004UsingLS},
LexRank \cite{erkan2004lexrank}, TextRank \cite{DBLP:journals/corr/BarriosLAW16}, and PACSUM \cite{zheng-lapata-2019-sentence}. We also include HipoRank \cite{dong-etal-2021-discourse} with document views. %),  as it achieved high %the state of the art 
%performance on long scientific papers.  %summarization
For supervised methods, we compare with BERT\_EXT \cite{liu-lapata-2019-text}. % uses a  document-level encoder based on BERT. % which is able to express the semantics of a document and obtain sentence representations. 
Although not our focus,  abstractive baselines are in Appendix \ref{sec:appendix_abstractive}. 

%\subsection
\textbf{Automic Evaluation Metrics}
%For automatic evaluation, 
We report ROUGE-1 (R-1), ROUGE-2 (R-2), and ROUGE-L (R-L) F1 scores, as well as % We also report
BERTScore (BS) \cite{bert-score}. %to measure the semantic meanings of the generated summary. 
%Finally, we design an automatic metric for the CanLII dataset. 
%Recall from Section  \ref{sec:dataset} that our test set comes with sentence-level annotations on both source and target texts following the ``IRC'' scheme. %, that is, a sentence can be manually classified as either Issue, Reason, Conclusion or Non-IRC types. %(more detailed description in \S \ref{sec:dataset}). 
We also propose metrics to measure the recall value of the annotated IRC types in the test set, % in source-side, as well as the recall values in the summary, 
which exploits the structure of case documents. More details are in Section \ref{sec:salient_eval}.  

 
     
\section{Results}\label{sec:result}
In this section, we aim to deal with three research questions: 
\textbf{RQ1}. How well do the extractive baselines including the HipoRank backbone deal with legal documents?  
\textbf{RQ2}. How well do the different views of document structures perform with the  HipoRank backbone?  \textbf{RQ3}. Can the reweighting algorithm help select important argumentative sentences and improve summary quality?

\subsection{Automatic Summarization Evaluation}
Table \ref{tab:unsupervised_results}
% We 
compares our methods with prior extractive models. See Appendix \ref{appendix:output} for example summaries. % can be found %in Appendix \ref{appendix:output}. 
% The first block in Table \ref{tab:unsupervised_results} includes the IRC 
%and the extractive oracles. The next
%block  presents the results of one supervised
%and several unsupervised extractive
%baseline models. The block after that presents the discourse-aware HipoRank baseline with our different views of document structure.  The final block presents our reweighting  model's results (using the "w/o header" version of the CANLLI documents as they performed best in the prior block.) 


%Overall, our approach on incorporating the different document views and selection methods obtained the best results on the datasets.

\textbf{RQ1.} Table \ref{tab:unsupervised_results} shows that there is still a gap between oracle models (rows 1 and 2) and current extractive baselines. There are around 20 points differences on R-1, R-2, and R-L. Among the baselines, the supervised model works best  only for R-1 and R-L. Unsupervised methods obtain  the highest BS (row 4) and R-2  (row 5), possibly due to the higher coverage of n-grams benefitting from longer extracted summaries (row 3 model generated summaries have an average length of 250; row 4-6 models generate on average 400-word summaries;  row 7 and 8 models have a limit of 220 words). %Comparing to the supervised model, unsupervised methods (row 4-8) have lower scores, which is reasonable given that the model is not exposed to any training data.
Without  reweighting, the HipoRank backbone % - even with the best performing document structure - 
never outperforms the best extractive baseline.  However, if only unsupervised baselines are considered,  HipoRank in row 12 does show the best performance for 3 of the 4 evaluation metrics.
%We first compare the best-performed baseline model, HipoRank's performance given documents with different structure representation. 

\textbf{RQ2.} To examine the effects of the  document views in  Section \ref{sec:views}, we split the document into different types of linear segments and then used  HipoRank  to generate summaries.  Recall that HipoRank is the only model to exploit  document structure, and as noted for RQ1, with the right structure could obtain the  best unsupervised R-1, R-L, and BS baseline scores.  
When naively constructing different document structures from the CanLII dataset without header removal, using NLP algorithms (rows 9 - 10) versus just using the HTML formatting (row 8) generally degraded results. However, when we  experimented with a modified version of the input documents (rows 11-13) where the headers were filtered through heuristics before computing the document structure, %and keyword matching and sections are reorganized 
%(see Section \ref{sec:header_removing}), %the C99 model (row 12) outperformed just using the HTML (row 11).  In addition, 
the  scores in rows 11-13 were higher (or in one case the same) than the comparable scores in rows 8-10.  
Also, without headers, the C99  topic segmentation algorithm (row 12) now outperforms the use of HTML (row 11) (obtaining an average improvement of 0.5 points across ROUGE and 0.8 for BS), suggesting
 that better  structures can  improve summarization. 
% As shown in row 11-12 in third block of Table \ref{tab:unsupervised_results}, the original document structure outperforms the other two views on the CanLII dataset and HMM-Stage obtains best performance on BillSum. 
% Through visualization of the IRC and selected sentence positions in the CanLII document (Fig \ref{fig:initial_study}), 
% we find that the stage-based segmentation perform better on selecting sentences appeared in the middle of the text (green ovals in the bottom right sub figure) while the topic-focused one succeeded in more accurately capturing the ending "conclusion" sentences (two green ovals in top-left sub figure). 
% \begin{figure}
% \centering
%  \includegraphics[width=\linewidth]{Figs/1654088145579.jpg}
%   \caption{Initial study on selected sentence positions from different views.}
%   \label{fig:initial_study}
% \end{figure}
 As shown in Table \ref{tab:segmentation_stats} (and earlier in Figure \ref{fig:three_view}), C99  creates many small sections (average number of sentences per section is 3.39 with standard deviation  of 0.67).
 We hypothesize that this encourages the selection of sentences from more fine-grained segments. In contrast, the other two methods create lengthy sections (average of more than 50 sentences) with a large standard deviation (135.40 for original structure without headers). 
%Also, the C99 model (row 12) now outperformed just using the HTML (row 11)
% With the updated CanLII dataset, we find that changing to different document structure, such as the topic segmentation, we 
In sum, with improvements in automatic metrics, we find that document structures play an important role in summarizing cases.
\begin{table}[t]
\small
    \centering
    \begin{tabular}{c|c|c}
    \toprule
        Model & avg. \# secs & avg. \# sents per sec  \\
        \midrule 
        \multicolumn{3}{c}{with header} \\
        \midrule
        Original Structure & 4.83 (6.44) &  83.82 (118.78) \\
        C99-topic & 63.47 (70.34) & 3.38 (0.71) \\
        HMM-stage & 4.00(0.83) &  54.32 (64.80) \\
        \midrule
         \multicolumn{3}{c}{without header} \\
         \midrule
        Original Structure & 3.67 (5.51) &  102.99 (135.40) \\
        C99-topic & 59.74 (69.91) & 3.39 (0.67) \\
        HMM-stage & 3.16 (1.08) &  70.19 (119.39)\\
        \bottomrule
    \end{tabular}
    \caption{Statistics about the average number of sections (avg. \# secs) and average number of sentences per section (avg. \# sents per sec) across the documents with different computed document structures (standard deviation in parenthesis).}
    \label{tab:segmentation_stats}
\end{table}

\textbf{RQ3}. The final block of Table \ref{tab:unsupervised_results} presents the reweighting  results (using the "w/o header" version of the CanLLI documents as they performed best in the prior block).  %We first implement the reweighting algorithm. 
By downweighting  sentences that appear under the same section as  previously selected ones, we observe an F1 improvement of 0.65, 0.65, and 0.75 on R-1, R-2, and R-L, respectively, on the previously best-performing topic segmented document (row 12 versus 15).
%Employing HipoRank with C99-topic segmentation and adding our reweighting algorithm 
Row 15 in fact has the best non-oracle results for all ROUGE scores.  This observation regarding the value of reweighting also holds for the original structure (row 11 vs. 14) and the HMM-stage segments (row 13 vs. 16). %\footnote{For ROUGE precision and recall, see Appendix \ref{appendix:viewFullResults}.}
%Overall, our approach gives an improvement of 2.2 on R-1, 1.5 on R-2 and 2.3 on R-L on the CanLII dataset (row 8 and row 15).   %Interestingly, the  proposed selection algorithm bring negative results on the BillSum dataset, an analysis on the position distribution of the extractive oracle sentences (see Figure \ref{fig:billsum_position_sorted} in Appendix) in the original article demonstrated that the salient information in the original article are following a uniform distribution, thus the proposed algorithm may discourage for better selection of candidates. 

% \begin{figure*}
% % \begin{adjustwidth}{-8em}{0em}
%   \begin{subfigure}[b]{0.7\textwidth}
%     \includegraphics[width=\textwidth]{Figs/Oracle_IRC_no_sort.png}
%     \caption{IRC Oracles}
%     \label{fig:}
%   \end{subfigure}
%   %
%   \begin{subfigure}[b]{0.7\textwidth}
%     \includegraphics[width=\textwidth]{Figs/positions_test_pacsum_no_order.png}
%     \caption{PacSum}
%     \label{fig:}
%   \end{subfigure}

%   \begin{subfigure}[b]{0.7\textwidth}
%     \includegraphics[width=\textwidth]{Figs/positions_test_hiporank_no_order.png}
%     \caption{HipoRank}
%     \label{fig:}
%   \end{subfigure}
%   %
%   \begin{subfigure}[b]{0.7\textwidth}
%     \includegraphics[width=\textwidth]{Figs/positions_test_theme1_1049_dynamic_no_order.png}
%     \caption{Ours}
%     \label{fig:}
%   \end{subfigure}
% \end{figure*}

\begin{figure*}[h]
        \centering
        \begin{subfigure}[b]{0.45\textwidth}
            \centering
            \includegraphics[width=\textwidth]{Figs/Oracle_IRC_no_sort.png}
            \caption[]%
            {{\small IRC Oracles}}    
            \label{IRCoracle}
        \end{subfigure}
        \hfill
        \begin{subfigure}[b]{0.45\textwidth}   
            \centering 
            \includegraphics[width=\textwidth]{Figs/positions_test_hiporank_no_order.png}
            \caption[]%
            {{\small HipoRank with headers}}    
            \label{fig:mean and std of net34}
        \end{subfigure}
        \vskip\baselineskip
        
        \begin{subfigure}[b]{0.45\textwidth}
            \centering
            \includegraphics[width=\textwidth]{Figs/positions_test_1049_noheader.png}
            \caption[]%
            {{\small HipoRank without headers}}    
            \label{IRCoracle}
        \end{subfigure}
        \hfill
        \begin{subfigure}[b]{0.45\textwidth}   
            \centering 
            \includegraphics[width=\textwidth]{Figs/positions_test_1049_noheader_dynamic.png}
            \caption[]%
            {{\small HipoRank without headers and with reweighting}}    
            \label{fig:mean and std of net34}
        \end{subfigure}
        \vskip\baselineskip
        %%%%%%%
        % \hfill
        % \begin{subfigure}[b]{0.485\textwidth}   
        %     \centering 
        %     \includegraphics[width=\textwidth]{Figs/positions_test_1049_noheader.png}
        %     \caption[]%
        %     {{\small HipoRank without headers}}    
        %     \label{fig:mean and std of net44}
        % \end{subfigure}
        % % \vskip\baselineskip
        % \hfill
        % \begin{subfigure}[b]{0.485\textwidth}   
        %     \centering             \includegraphics[width=\textwidth]{Figs/positions_test_theme1_1049_dynamic_no_order.png}
        %     \caption[]%
        %     {{\small Our with Reweighting}}    
        %     \label{fig:mean and std of net44}
        % \end{subfigure}
        
        \caption[]
        {\small Sentence positions in source cases for extractive summaries generated by different models using the original document structure on the
 test set. For (b) (c) (d), documents on the x-axis are
sorted in the same order. For IRC Oracles, \textcolor{red}{Issue}, \textcolor{reasonblue}{Reasoning} and \textcolor{conclusiongreen}{Conclusion} sentences are colored accordingly. } 
        \label{fig:comparison}
    \end{figure*}
    
    Finally, to better understand the behavior of different enhancements to the HipoRank backbone model, Figure \ref{fig:comparison} visualizes the positions of IRC sentences in the original article that are selected by a particular summarization method.  Plot (a) shows that the human-annotated IRC sentences in the summary tend to span across the source documents, with issues appearing in the beginning and conclusions in the end. Plot (b) shows that although HipoRank using the original document structure can successfully pick middle section sentences, the darkest band at the starting positions shows that the model still heavily relies on the inductive bias to pick the beginning sentences. Plot (c)
shows that removing the headers reduces the  starting sentence bias.  Finally, plot (d) shows that 
%We find that 
reweighting  reduces the number of sentences appearing on both ends. Further analyses on the complete automatic evaluation results\footnote{See Appendix \ref{appendix:reweight_effects} for ROUGE precision and recall.} suggest that the improvements come from higher recall values.
%(as shown in Fig \ref{fig:comparison}). 


\subsection{Argumentative Sentence Coverage}\label{sec:salient_eval}
Taking advantage of the sentence-level IRC annotations, we propose recall metrics to better measure the summary quality from a legal argumentation perspective ({\bf RQ3}).  We compute the recall of ``IRC'' sentences extracted from the original case as source IRC coverage (src. IRC). % of the summarizer. 
We similarly compute the coverage of IRC sentences in the human-written summaries as target IRC coverage (tgt. IRC) and all sentences as target sentence coverage (trg. cov.). To do so we apply the oracle summarizer (Section \ref{sec:oracle}) to map the generated extractive summaries to the human-written abstractive summaries. % and report the corresponding recall values. 

%\footnote{This oracle summarizer obtains a recall of 84.0 on the IRC mappings among the case and human written summaries manually annotated on ten article pairs.} 

We report these values for the IRC oracle, %the best 
an unsupervised (LexRank) and supervised (BERT\_EXT) baseline, the  discourse-aware HipoRank with the original structure, and our best reweighting model using C99-topic segmentation. Table \ref{tab: salient_coverage} shows that our model obtains the highest target IRC recall and coverage, suggesting that the  summaries are more similar to the references with respect to %conveying 
the decision's argumentation. Another unsupervised model, LexRank, obtains the highest source IRC, but its off-the-shelf package requires a fixed sentence ratio selected from the source. %original article. 
This produced longer summaries than other approaches and thus captured more IRCs in the source. 

\begin{table}[t!]
\small
    \centering
    \renewcommand{\arraystretch}{1}%Tighter
    \begin{tabular}{c|c|c|c}
    %\begin{tabular}{l|lllc|lllc|p{0.9cm}ll}
    \toprule
  \textbf{Model} & \textbf{src. IRC} & \textbf{tgt. IRC} & \textbf{trg. cov.} \\
  \midrule 
  \multicolumn{3}{l}{\textit{Oracle}}\\
  \midrule
  IRC & 1 (0.00) & 0.918 (0.18)  &  0.820 (0.25) \\
%   EXT &   62.7 & 71.7 & 66.3  \\
  \midrule 
     \multicolumn{3}{l}{\textit{Baselines}}\\
  \midrule
  BERT\_EXT & 0.804 (0.27) & 0.846 (0.23) &  0.833 (0.23)\\
  LexRank & \textbf{0.912} (0.19) & 0.811 (0.26) &  0.800 (0.27)\\
%   PACSUM & & \\
  HipoRank & 0.800 (0.25) & 0.851 (0.24) & 0.844 (0.22) \\
   
  \midrule 
  {\it Ours} & 0.823 (0.26) & \textbf{0.866} (0.20) & \textbf{0.850} (0.21) \\
  \bottomrule

    \end{tabular}
    \caption{Average recall of IRC sentences matched in the original case (src. IRC), gold summary (tgt. IRC), as well as target sentences coverage (trg. cov.) for each document (standard deviation in parenthesis).}
    \label{tab: salient_coverage}
\end{table}


\subsection{Human Evaluation Discussion}
% TODO: add the newest paper citation.

% Another limitation is that current references are fixed, which could not address the prior work's expert feedback that there is a need for different kinds of summaries in the legal field, depending on the expert's intention of use.  

As a first step towards human evaluation, we tried to extend the HipoRank  setup in \citet{dong-etal-2021-discourse} and designed a human evaluation protocol as follows. We asked human judges\footnote{All judges should be native English speakers who are at least pursuing a JD degree in law school and have experience in understanding case law.} to read the human-written reference summary and presented  extracted sentences from different summarization systems. The judges were asked to evaluate a system-extracted sentence according to two criteria: (1) \textit{Content Coverage} -  whether the presented sentence contained content from the human summary, and (2) \textit{Importance} - whether the presented sentence was important for a goal-oriented reader even if it was not in the human summary\footnote{Here we assumed the goal-oriented reader as the lawyers or law students seeking information from the case.}. The sentence selection was anonymized and randomly shuffled. We used the same sampling strategy in \citet{dong-etal-2021-discourse} to pick ten reference summaries where the system outputs were neutral (i.e., had similar R-2 scores compared to the human reference). 
%The evaluators also provide valuable feedback on keeping track of the metadata about annotators' legal backgrounds, area specialization, and experience with lawsuits. 
However, initial annotation on a small set by a legal expert demonstrated that the selected sentences may not reflect the model's capability. Most sampled system outputs had low ROUGE-2 F1 scores compared to the reference (normally below 10\% while the average model performance is 17\%), and the human evaluator reported that some of the selected sentences were not meaningful. We thus propose that a more careful sampling technique will be required for legal annotation tasks such as ours. 

To further guide our future work, we also reviewed how prior legal domain research has performed human evaluations when automatically summarizing legal documents  \cite{polsley-etal-2016-casesummarizer, zhong-2019-iterativemasking, salaun-2022-jurix}. Due to the burden of reading lengthy original documents, as in our human evaluation, most prior work evaluated summary quality using reference summaries rather than  source documents. In addition,  legal evaluations have typically been small-scale %constrained by the scale 
(5-20  summaries) due to the need to have evaluators with particular types of expertise (e.g., law graduate students or law professors), which was a similar constraint in our exploratory human evaluation.  Researchers have also designed new types of legally-relevant evaluation questions that  evaluate the summary for task-specific properties that go beyond more typical properties such as grammar, readability, and style. In our case, we would like legal experts to assess IRC coverage in the future.

% In caseSummarizer, \citet{polsley-etal-2016-casesummarizer} recruited domain-experts (without further details) and designed a tiny scale of (5 summaries) human evaluations with six questions borrowed from meeting summarization \cite{5071230}. \citet{zhong-2019-iterativemasking} recruited a law professor to examine different system outputs of 20 cases with a single question ``whether the summary adequately identifies issues and resolutions''. More recently, in \cite{}, SALAUN three experts (nlp specialist and law graduate students) annotated 16 test instances with an intrinsic evaluation framework of 7 questions to evaluate the summary from the form properties (grammar, readability, and style) and summary adequacy properties. Despite the small scale, most prior attempts avoided including the original long cases, given the time expense and difficulty in capturing all details, and instead focused on comparing the summaries to the references. Such evaluations may not be able to address the sentence-level properties of the extracted summaries.

% \textcolor{blue}{For this task, to evaluate the selected sentences' quality, following the work of \citet{dong-etal-2021-discourse}, we design a human-evaluation protocol as follows:  to read the human-written reference summary and present those extracted sentences from different summarization systems. The judges are asked to evaluate the system-extracted sentence according to two criteria: (1) \textit{Content Coverage}  whether the presented sentence contains content from the human summary and (2) \textit{Importance} whether the presented sentence is important for a goal-oriented reader even if it is not in human summary\footnote{Here we assume the goal-oriented reader as the lawyers or law students seeking information from the Case.}. The sentence selection is anonymized and randomly shuffled, so the human judges will not know which system it comes from. Given that the total amounts of sentences are too large (each system summary can have 10-20 sentences), it is infeasible to annotate all sentences of the 1,049 test cases. We instead try to sample a small portion of the data. As did in \cite{dong-etal-2021-discourse}, we selected ten reference summaries coupled with a total of 281 system-extracted sentences, where 141 are from our best model (row 15 in Table \ref{tab:unsupervised_results}) and 141 from the hiporank baseline with original structure (row 11 of Table \ref{tab:unsupervised_results}). Criteria are applied for the selection, where both system outputs have similar ROUGE scores compared to the human reference but differ dramatically from each other for neutral comparison. However, initial annotation on a small set from the 281 tasks demonstrates that the selected sentences may not represent the whole dataset. Most sampled system outputs have low ROUGE-2 scores compared to the reference (normally below 10\%), and human evaluators also report that some of the selected sentences are not good enough. We thus propose that a more careful sampling technique is required for the annotation tasks of the legal domain. The evaluators also provide valuable feedbacks on keeping track of the metadata about annotators' legal background, the specialization of area, and the experience with lawsuits.  }



% TODO: (1) Ablation Study on different views' selected results and the way to aggregate. (2). Examine the strategies to weight different section-sentence edges. (3) show different variants of the multi-round revision and demonstrate the benefits. 

