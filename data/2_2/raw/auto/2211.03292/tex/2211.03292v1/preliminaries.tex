%!TEX root = onetrace.tex

\section{Preliminaries} \label{sec:preliminaries}


\medskip

\noindent {\bf Notation.}
Given a positive integer $n$, we write $[n]$ to denote $\{1,\ldots,n\}$.
Given two integers $a\le b$ we write $[a:b]$ to denote $\{a,\ldots,b\}$. 
We write $\ln$ to denote natural logarithm and $\log$ to denote logarithm to the base 2.
We denote the set of non-negative integers by $\Z_{\geq 0}$.
We  write ``$a=b\pm c$'' to indicate that $b-c\le a\le b+c$.
It will be convenient for us to index a binary string~$x \in \zo^n$
  using $[1:n]$ as~$x=(x_1,\dots,x_{n})$. 

\medskip

\noindent {\bf Distributions.}
When we use bold font such as $\bD, \by, \bz$, etc., it indicates that the entity in question is a random variable.
We write ``$\br \sim {\cal P}$'' to indicate that random variable~$\br$~is 
  distributed according to probability distribution ${\cal P}$.  If $S$ is a finite set we write ``$\br \sim S$'' to indicate that $\br$ is distributed uniformly over $S$.
 
 We write $\Geo(\rho)$ to denote the geometric distribution with parameter $\rho$, i.e.~the number of Bernoulli trials with success probability $\rho$ needed to get one success, supported on the set $\{1,2,\dots\}$.
 We will use the following tail bound for sums of independent geometric random variables:

\begin{claim} \label{claim:neg-binomial}
Let $\rho \in [0,1]$ and let $\bG_1, \ldots, \bG_m$ be $m$ independent geometric random variables with each $\bG_i \sim \Geo(\rho)$.
For any $\gamma \in [0,1]$, we have
  \[
    \Pr\left[ \abs[\bigg]{\sum_{i=1}^m \bG_i - \rho^{-1} m } \ge \gamma \rho^{-1} m \right]
    \le e^{-\Omega(\gamma^2 m) }  .
  \]
\end{claim}
\begin{proof} By coupling $(\bG_1,\dots,\bG_m)$ with a draw from the Binomial distribution $\Bin(n,\rho)$, we observe that $\sum_{i=1}^m \bG_i \ge n$ if and only if $\Bin(n, \rho) < m$.
  Let $n_h := (1+\gamma) \rho^{-1} m$ and $n_\ell := (1-\gamma) \rho^{-1} m$.
  We have
  \begin{align*}
    \MoveEqLeft
    \Pr\left[ \abs[\bigg]{\sum_{i=1}^m \bG_i - \rho^{-1} m } \ge  \gamma\rho^{-1} m \right] \\
    &= \Pr\bigl[ \Bigl( \Bin\bigl( (1 + \gamma) \rho^{-1} m, \rho \bigr) < m \Bigr) \vee \Bigl( \Bin\bigl( (1 - \gamma) \rho^{-1} m, \rho \bigr) > m \Bigr) \biggr] \\
    &= \Pr\biggl[ \Bigl( \Bin\bigl( n_h, \rho \bigr) < \frac{1}{1+\gamma} \cdot \rho n_h \Bigr) \vee \Bigl( \Bin\bigl( n_\ell, \rho \bigr) > \frac{1}{1-\gamma} \cdot \rho n_\ell \Bigr) \biggr] \\
    &= \Pr\biggl[ \Bigl( \Bin\bigl( n_h, \rho \bigr) < \left( 1 - \frac{\gamma}{1+\gamma} \right) \rho n_h \Bigr) \vee \Bigl( \Bin\bigl( n_\ell, \rho \bigr) > \left( 1 + \frac{1}{1-\gamma} \right) \rho n_\ell \Bigr) \biggr] \\
    &\le e^{-\Omega(\gamma^2 m) }, \qedhere
  \end{align*}
where the inequality is a standard Chernoff bound.\end{proof}

\medskip

\noindent {\bf Deletion channel and traces.}
Throughout this paper the parameter $0 <\delta < 1$ denotes the \emph{deletion probability}.  Given a string $x \in \zo^n$, we write $\Del_\delta(x)$ to denote the distribution of the string that results from passing  $x$ through the $\delta$-deletion channel (so the distribution $\Del_\delta(x)$ is supported on $\zo^{\leq n}$), and we refer to a string in the support of $\Del_\delta(x)$ as a \emph{trace} of $x$.  Recall that a random trace $\by \sim \Del_\delta(x)$ is obtained by independently deleting each bit of $x$ with probability $\delta$ and concatenating the surviving bits.\hspace{0.05cm}\footnote{In this work we assume that the deletion probability $\delta$ is known to the reconstruction algorithm.\ignore{ \red{Rocco: Do we need this? Maybe yes to get from \Cref{thm:main} to \Cref{thm:main2}?}}}
We may view the draw of a trace $\by$ from $\Del_\delta(x)$ as a two-step process:  first a set $\bD$ of \emph{deletion locations} is obtained by including each
  element of $[n]$ independently with probability $\delta$, and then 
  $\by$ is set to be $\smash{x_{[n]\setminus \bD}}$. 
    
\medskip


\noindent {\bf $\LCS$ and matchings.}
We write $\LCS(x,x')$ to denote the longest common subsequence between two strings $x$ and $x'$ and $|\LCS(x,x')|$ to denote its length. 
A \emph{matching} $M$ between two strings $x,x' \in \zo^\ast$ is a list of pairs $(v_1,v'_1), (v_2,v'_2),\dots$ such that $v_1 \leq v_2 \leq \cdots$, $v'_1 \leq v'_2 \leq \cdots$, and for every $t$ we have $x_{v_t} = x'_{v'_t}.$ The \emph{size} of a matching is the number of pairs.  We note that the largest matching between $x$ and $x'$ is of length 
$|\LCS(x,x')|$.

\medskip

%\noindent
%\gray{ {\bf Correspondence between traces and source string $\bx$.} Given a location $q \in |\by|$ in a trace $\by$, we write $\source(q)$ to denote the location $i \in [n]$ such that bit $x_i$ of $x$ gave rise to $\by_q$.
%For an interval $Q=[q_1:q_2]$ of locations in $\bssy$, we write $\source(Q)$ to denote the set $\{\source(q): q\in [q_1:q_2]\}.$
%We define $\overline{\source(Q)}$ to be the 
%%\emph{largest}
%  interval $[\source(a):\source(b)]\subseteq [n]$.
%
%
%
%Given a location $i \in [n]$, if $i \notin \bD$ then $\image(i)$ denotes the element of $[|\by|]$ that $\bx_i$ lands in (and if $i \in \bD$ then we define $\image(i)$ to be $\bot$). 
%We observe that if $I \subseteq \overline{\source(Q)}$ then $\image(I) \subseteq Q$.\rnote{Will we need this gray stuff? Not sure; if not, let's get rid of it}
%}
%
%
%%\rnote{This needs to go after we introduced the ``source'' notation, and probably shouldn't be in a bullet under ``Some notational conventions'' since it's terminology rather than notation} 
%%Finally we introduce some useful terminology:  We refer to a tuple of pointers $(\ell^{(1)},\dots,\ell^{(M)})$ into traces $\by^{(1)},\dots,\by^{(M)}$ (so each $\ell^{(m)}$ belongs to $[|\by^{(m)}|]$) as a \emph{configuration}.  We say the configuration $(\ell^{(1)},\dots,\ell^{(M)})$ is \emph{in consensus} if at least $0.9M$ of the values $m \in [M]$ all have $\source^{(m)}(\ell^{(m)})$ equal to the same location $i \in [n]$.

\noindent {\bf An asymptotic bound on binomial coefficients.}
We recall the following standard bound on binomial coefficients:
\begin{fact} [\cite{vanLint}, Theorem~1.4.5]
\label{fact:standard-bound}
For~$0 \leq k \leq n/2$, we have $\sum_{i=0}^{k} {n \choose i} \leq 2^{H(k/n)n}$, where $H(x) = x\log(1/x) + (1-x)\log(1/(1-x))$ is the binary entropy function.
\end{fact}

\subsection{The average-case setting} \label{sec:avg}
We record the following simple observation, which is useful for analyses of the average-case setting: 
 
\begin{observation}[\emph{A posteriori} distribution of a uniform random source string given one trace] \label{ob:post1}
Let $\bx$ be a uniform random source string from $\zo^n$.
Given any fixed outcome $y \in \zo^m$ of a single trace $y =\by \sim \Del_\delta(\bx)$,
the \emph{a posteriori} distribution of $\bx$ given $y$ is as follows:

\begin{enumerate}

\item Draw a uniform random $m$-element subset $\bS \sim {[n] \choose [m]}$ of $[n]$ (say $\bS = \{\bs_1,\dots,\bs_m\}$ where $1 \leq \bs_1 < \cdots < \bs_m \leq n$);

\item For each $i \in [m]$ set $\bx_{\bs_i}=y_i$ (i.e. fill in the locations in $\bS$ from left to right with the  bits of $y$), and for each $j \notin \bS$ set $\bx_j$ to an independent uniform element of $\zo$.
\end{enumerate}
\end{observation}

We write ``$\bx \sim y$'' to indicate that $\bx$ has the distribution described above. We note that a somewhat counterintuitive corollary of \Cref{ob:post1} is the following: in the average-case setting (when $\bx$ is uniform random), even if the received trace is the string $1^m$, the \emph{a posteriori} distribution of the $n-m$ ``unseen bits'' of $\bx$ is that they are independent and uniform random.

An easy corollary of \Cref{ob:post1} is the following:

\begin{corollary}  \label{cor:uniform}  
For $\bx$ a uniform random source string from $\zo^n$,
given any fixed outcome $y \in \zo^m$ of a single trace $y =\by \sim \Del_\delta(\bx)$,
the \emph{a posteriori} distribution of the other $n-|y|$ bits $\bx_{\bD}$ of $\bx$ is that they are distributed as a uniform random element of $\zo^{[n] \setminus |y|}$.
\end{corollary}
%(To see this, observe that we can view the uniform random string $\bx \sim \zo^n$ as being generated by first (i) picking the set $\bD$ of $n-|\by|$ coordinates in $[n]$ that are to be missing from $\by$, then (ii) setting the $|\by|$ coordinates of $\bx$ in positions $[n] \setminus \bD$ according to $y$, and finally (iii) picking independent uniform bits for the coordinates of $\bx_{\bD}$.)



  


\subsection{One-trace and few-trace algorithms.} \label{sec:alg-performance-notation}

\noindent {\bf Optimal worst-case algorithms.}
We introduce the notation
$L_{1,\worst}(\delta,n)$ to denote the largest possible $\LCS$ that can be achieved in expectation by any one-trace algorithm under deletion rate $\delta$ in the worst-case setting, i.e.
\begin{equation} \label{eq:worst-case-one-trace}
L_{1,\worst}(\delta,n) :=  \max_{A} \min_{x \in \zo^n} \Ex_{\by \sim \Del_{\delta}(x)}
[|\LCS(A(\by,n),x)|],
\end{equation}
where the maximum is taken over all algorithms $A$ that take as input the values $n, \delta$ and a single trace $\by$, and output an $n$-bit hypothesis string (denoted $A(\by,n)$ in the expression above).   We observe that (\ref{eq:worst-case-one-trace}) could be extended to allow the algorithm $A$ to be randomized (and have the expectation be also over the randomness of $A$), but we do not do this since the optimal algorithm in (\ref{eq:worst-case-one-trace}) can without loss of generality be taken to be deterministic.

We will sometimes consider the optimal performance of $t$-trace algorithms for $t>1$, so we extend the above definition in the obvious way to algorithms that are given $t$ independent traces, i.e.
\begin{equation} \label{eq:worst-case-t-traces}
L_{t,\worst}(\delta,n) :=  \max_{A} \min_{x \in \zo^n} \Ex_{\by^{(1)},\dots,\by^{(t)} \sim \Del_{\delta}(x)}
[|\LCS(A(\by^{(1)},\dots,\by^{(t)},n),x)|].
\end{equation}

\noindent {\bf Optimal average-case algorithms.}
We use similar notation to capture the optimal performance of one-trace and $t$-trace algorithms in the average-case setting:
\begin{equation} \label{eq:average-case-one-trace}
L_{1,\avg}(\delta,n) :=  \max_{A} \Ex_{\bx \sim \zo^n} \Ex_{\by \sim \Del_{\delta}(\bx)}
[|\LCS(A(\by,n),\bx)|],
\end{equation}

\begin{equation} \label{eq:avg-case-t-traces}
L_{t,\avg}(\delta,n) :=  \max_{A} \Ex_{\bx \sim \zo^n} \Ex_{\by^{(1)},\dots,\by^{(t)} \sim \Del_{\delta}(\bx)}
[|\LCS(A(\by^{(1)},\dots,\by^{(t)},n),\bx)|].
\end{equation}


