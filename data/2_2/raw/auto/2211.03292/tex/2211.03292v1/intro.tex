%!TEX root = onetrace.tex

\section{Introduction} \label{sec:intro}

The \emph{trace reconstruction problem} \cite{Kalashnik73,Lev01a,Lev01b,BKKM04} is one of the oldest and most basic algorithmic problems involving the deletion channel.  
In this problem the goal of the reconstruction algorithm is to infer an unknown $n$-bit source string $x \in \zo^n$ given access to a source of independent ``traces'' of $x$, where a trace of $x$ is a draw from $\Del_\delta(x)$.  Here $\Del_\delta(\cdot)$ is the ``deletion channel,'' which independently deletes each bit of $x$ with probability $\delta$ and outputs the concatenation of the surviving bits. The goal of the reconstruction algorithm is to correctly reconstruct the source string $x$ using as few traces and as little computation time as possible.

A surge of recent work \cite{MPV14,DOS17,NazarovPeres17,PeresZhai17,HPP18,HHP18,BCFSSfocs19,BCSSrandom19,Chase19,KMMP19,HPPZ20,Chase20,NarayananRen20,CDLSS20smoove,CDLSS20lowdeletion,ChasePeres21,CDLSS22} has addressed many different aspects and variants of the trace reconstruction problem.  The version described above corresponds to a ``worst-case'' setting, since the $n$-bit source string can be completely arbitrary; despite intensive research \cite{HMPW08,DOS17,NazarovPeres17,Chase20}, the best algorithm known for this problem, for constant deletion rate $\delta$, requires $\exp(\tilde{O}(n^{1/5}))$ traces.  Many papers such as \cite{MPV14,PeresZhai17,HPP18,BCSSrandom19,HPPZ20,Chase19,CDLSS20smoove} have also considered an ``average-case'' version of the problem in which the source string $\bx \sim \zo^n$ is assumed to be a uniform random $n$-bit string; this average-case problem is known to be significantly easier than the worst-case problem (we refer the reader to \cite{HPPZ20,Chase19} for state-of-the-art algorithmic results and lower bounds on average-case trace reconstruction at constant deletion rates).  Other problem variants which have been studied include ``population recovery'' versions in which there is a distribution over source strings rather than a single unknown source string \cite{BCFSSfocs19,BCSSrandom19,NarayananRen20}; the ``low deletion rate'' ($\delta = o_n(1)$) and ``high deletion rate'' ($\delta = 1-o_n(1)$) settings \cite{BKKM04,HMPW08,MPV14,BCFSSfocs19,NarayananRen20}; and \emph{approximate} trace reconstruction \cite{SDDF18,DRRS20,SB21,GSZ20, ChasePeres21,CDK21,CDLSS22}, in which the goal is only to obtain an approximate rather than an exact reconstruction of the unknown source string $x$, and which is the focus of the current work.

\medskip
\noindent {\bf Prior work on approximate reconstruction from few traces.}
The best algorithms known for even the easiest versions of exact trace reconstruction, such as the $\delta=O(1/\log n)$, average-case problem setting considered by \cite{BKKM04}, typically require a number of traces that grows with $n$ to achieve exact reconstruction.\footnote{Indeed, at deletion rate $\delta=O(1/\log n)$, it is easy to see that given a sample of $o({\frac {\log n}{\log \log n}})$ traces, with high probability there will be coordinates of the source string that are deleted from all of the traces in the sample.}  An attractive feature of the recent works \cite{ChasePeres21,CDLSS22} is that they give provable performance guarantees for \emph{approximate} trace reconstruction even when only \emph{constantly} many traces are available. In more detail, \cite{CDLSS22} gave near-matching upper and lower bounds on the best possible reconstruction accuracy that any algorithm can achieve given $M=O(1/\delta)$ traces from $\Del_\delta(\bx)$ in the average-case setting.  \cite{ChasePeres21} showed that for any constants $\delta,\eps$, there is some constant $M=M(\eps,\delta)$ such that an $M$-trace algorithm can achieve reconstruction error at most $\eps$ given traces from $\Del_\delta(\bx)$ in the average-case setting.\footnote{Several other recent papers \cite{SDDF18,DRRS20,GSZ20,CDK21,SB21} have also studied approximate trace reconstruction, but focusing on different aspects that make them less relevant to the present paper;  see \cite{CDLSS22} for a detailed discussion of those works.}

Results such as \cite{ChasePeres21,CDLSS22}, which shed light on what can be achieved given constantly many traces, can be particularly valuable in settings where only a severely limited number of traces are available and the goal is to do as well as possible with the data at hand. Such settings motivate the present paper, which, as we now describe, studies trace reconstruction in the ultimate data-constrained regime.

\medskip
\noindent {\bf This work: Approximate reconstruction from a single trace.}  We consider the problem of recovering an unknown $n$-bit source string $x$ as accurately as possible given only \emph{a single trace} from $\Del_\delta(x)$.  Despite the simplicity and naturalness of this problem, it does not seem to have been considered in prior work.
%\rnote{Do we want any more prose, here or elsewhere, motivating why we are studying one-trace reconstruction?}

We give a detailed study of this problem, analyzing both the worst-case setting of an arbitrary unknown source string $x$ as well as the average-case setting of a uniform random $\bx \sim \zo^n$.  In each of these settings we consider both the low ($\delta = o_n(1)$), medium ($\delta = \Theta(1)$), and high ($\delta=1-o_n(1)$) deletion rate regimes.
In a number of cases we give upper bounds on the approximate reconstruction accuracy that any one-trace algorithm can achieve, which are essentially matched by corresponding one-trace algorithms that we provide. (All of the algorithms we give are computationally efficient.)  
For some problem variants our upper bounds on the best achievable accuracy extend beyond one-trace algorithms to algorithms that receive multiple traces.

We view our results as a first investigation of one-trace reconstruction, and reiterate that very little was previously known for any of the problem variants that we consider. We describe the state of prior knowledge in the context of different specific problem variants when describe our results in \Cref{sec:our-results} below.

%\begin{itemize}

%\item general background on trace reconstruction

%\item Algorithms for general problem need lots of traces. Recent works (cite ours, Chase/Peres, others) on approximate trace reconstruction using few traces. We explore an extreme version of this question by considering the setting where there is only one trace available.

%\item Prior results when few traces are available:  \cite{ChasePeres21} results.  \cite{CDLSS22} results.  For the one-trace setting, our approximate paper

%\snote{Double blind paper.. be careful to refer to this in the third person}  observed that in the small deletion regime you can get accuracy $(1-\delta)$ by just outputting the trace, and its lower bound implies thus-and-such. That's all that seems to have been known.\rnote{Note that in our \cite{CDLSS22} paper we said in the intro that ``It is easy to see that simply outputting a single trace gives expected edit distance $\delta n$ (for any source string), and also that \blue{given $M$ traces no algorithm can achieve expected edit distance better than $\Theta(\delta^M n)$ for random source strings (since in expectation $\delta^M n$ bits of the $n$-bit source string will have been deleted from all $M$ traces)}.''  I don't think it is clear that the blue statement is true, would you agree? It seems like it would need a proof (indeed we did some work to give a proof of this in the $M=1$ case for this current project\dots}

%\item Motivation: There are good motivations for considering both the low-retention-rate regime (where $\rho=1-\delta$ is very small, possibly some $o_n(1)$ value) as well as the low-deletion-rate regime (where $\delta$ is $o_n(1)$). Settings involving digital communication corresponding naturally to a low deletion rate, while if you're reconstructing from fossil DNA maybe the retention rate is very low.\snote{Don't know enough about this.. will try to find out} We consider both of these regimes and both the average-case and worst-case settings.

%\end{itemize}

\subsection{Our results} 
\label{sec:our-results}

We are interested in the abilities and limitations of algorithms $A$ which receive as input a single trace $y$ from an unknown $n$-bit source string $x$ and which output an $n$-bit hypothesis string $\wh{x}=A(y).$  We measure the accuracy of $\wh{x}$ with respect to $x$ by the length of the longest common subsequence $|\LCS(x,\wh{x})|$.  LCS is closely related to edit distance, since if $|\LCS(x,\wh{x})|=n-k$ for two $n$-bit strings $x$ and $\wh{x}$, then $\wh{x}$ can be converted into $x$ by a sequence of $k$ deletions and $k$ insertions (and this is best possible).  The goal of an approximate reconstruction algorithm in this setting is to output a hypothesis string $\wh{x}$ for which the expectation\footnote{In the worst-case setting this expectation is over the random draw of the trace $\by$ from $\Del_\delta(x)$; in the average-case setting, this expectation is also over the uniform random draw of the source string $\bx \sim \zo^n$.  We give more details and a precise formulation in \Cref{sec:alg-performance-notation}.} of $|\LCS(x,\wh{x})|$ is guaranteed to be as large as possible; thus positive (algorithmic) results in our setting yield \emph{lower bounds} on how large an  expected value of $|\LCS(x,\wh{x})|$ can be achieved, while impossibility results for algorithms give \emph{upper bounds} on the best achievable expected $|\LCS(x,\wh{x})|.$ 

As alluded to earlier, we consider both the setting of a worst-case (arbitrary) $x \in \zo^n$ and the setting of a uniform random $\bx \sim \zo^n$. We note that algorithmic results (lower bounds on $\E[|\LCS(x,\wh{x})|]$ for the worst-case setting carry over to the average-case setting, while impossibility results (upper bounds on $\E[|\LCS(x,\wh{x})|]$) for the average-case setting carry over to the worst-case setting.

%\red{

%(Somewhere --- maybe not here --- get across that we're mostly interested in the constants.)

%(Somewhere mention that algorithms for the worst-case setting yield corresponding algorithms for average-case, and impossibility results for average-case yield corresponding impossibility results for worst-case setting)

%(Explain here that the way we measure the performance of our algorithms is this: the algorithm outputs a single $n$-bit hypothesis string $z$ and we look at the LCS between $z$ and $x$ (actually, because of the randomness in the trace, we consider the expectation of this LCS). So upper bounds on this LCS tell us about the limitations of what any algorithm can achieve, and the goal of an algorithm is to achieve as large an expected LCS as possible. Say this relatively informally with a forward pointer to \Cref{sec:alg-performance-notation}.)

%}

%\medskip

%\red{
%We consider one-trace algorithms that take as input a single draw $x \sim \Del_\delta(x)$ and output an $n$-bit string $z$.
%For a given deletion rate $\delta$, we study the expected LCS that a one-trace algorithm can achieve, i.e. our measure of interest is
%\[|\LCS(x,z=A^{\Del_\rho(x)})|.\]

%So impossibility results / limitations on what you can do with one trace correspond to upper bounds on the best achievable LCS.

%\medskip

%(One could allow the algorithm $A$ to be randomized and consider the expectation over its randomness, but if we only consider information-theoretically optimal algorithms and do not worry about running time, the information-theoretic optimal algorithm will be deterministic.)  \emph{Discuss connection to edit distance.}

%}


\Cref{sec:worst-case} presents our results for the worst-case setting and \Cref{sec:average-case} presents our results for the average-case setting. In each of these sections we first present our results (upper and lower bounds) for the high and medium deletion rate regimes, and then the low deletion rate regime. 

\subsubsection{Worst-case one-trace reconstruction} \label{sec:worst-case}



We first consider the high deletion rate regime.  It is convenient to let $\rho := 1-\delta$ denote the \emph{retention rate}, so in the high deletion rate regime we have $\rho=o(1)$.

If $\rho$ is too small (as a function of $n$) then it is easy to see that no nontrivial performance is possible.  In particular, if $\rho=o(1/n)$, then by Markov's inequality with probability $1-o(1)$ a trace $\by \sim \Del_\delta(x)$ is zero bits long, and in this case a reconstrution algorithm cannot even distinguish between the two possibilities $x=0^n$ and $x=1^n$. Consequently, if $\rho=o(1/n)$ then the largest expected LCS achievable by a one-trace algorithm is at most $(1/2 + o_n(1))n$ (and $n/2$ is trivially achieved by outputting any string with an equal number of 0's and 1's).

Our first positive result shows that --- perhaps surprisingly --- if $\rho$ is only slightly larger, then it is already possible to do much better than the above trivial bound:


\begin{theorem} [Worst-case algorithm, small retention rate, informal statement] \label{thm:worst-case-small-rho-informal}
For any $\rho = \omega(\log(n)/n)$, there is a worst-case one-trace algorithm that achieves expected LCS at least $(2/3-o(1))n$. 
Moreover, for any retention rate $\rho \geq \omega(1/n^{1/3})$, there is a worst-case one-trace algorithm that achieves expected LCS at least $(2/3 + c \rho)n$, where $c>0$ is an absolute constant.
\end{theorem}

The key to \Cref{thm:worst-case-small-rho-informal} is a (to the best of our knowledge novel) notion of an \emph{LCS-cover}, and a simple construction of an extremely small LCS-cover consisting of just two strings.  This already suffices to give the first sentence of \Cref{thm:worst-case-small-rho-informal}; the second sentence, improving the LCS bound to $(2/3 + c\rho)$, is obtained via a win-win analysis which considers whether or not the single received trace has many ``long runs''. Roughly speaking, if the trace has many long runs then this indicates that the source string $x$ is highly structured in a way (containing many long segments that are almost all-0 or almost all-1) that makes it easy to achieve a large LCS, and if the trace has few long runs then the source string $x$ must have many 01 alternations, which can be leveraged to get an LCS larger than $2n/3$.


\Cref{thm:worst-case-small-rho-informal} can be viewed as saying that having a $\log n$-bit trace already makes it possible to achieve an LCS of at least $(2/3 - o(1))n.$ Complementing \Cref{thm:worst-case-small-rho-informal}, we show that even having a $n^{0.999}$-bit trace does not make it possible to achieve an LCS of $(2/3 + c)n$ for any $c>0$:

\begin{theorem} [Worst-case upper bound on any algorithm, small retention rate, informal statement] \label{thm:worst-case-small-rho-upper-bound-informal}
Fix any $\eps>0$.  
For retention rate $\rho = 1/n^{\eps}$, no one-trace algorithm can achieve expected LCS greater than $(2/3 + o(1))n$ in the worst-case setting.
\end{theorem}

See \Cref{thm:worst-case-small-rho-upper-bound}
 for a detailed theorem statement, which extends \Cref{thm:worst-case-small-rho-upper-bound-informal} to give an upper bound on the performance of algorithms that receive multiple traces. 
\Cref{thm:worst-case-small-rho-upper-bound-informal} leverages a recent deep result of Guruswami, Haeupler, and Shahrasbi~\cite{GHS20} analyzing a code due to Bukh and Ma \cite{BukhMa14}. We take advantage of the highly repetitive structure of the Bukh--Ma codewords to combine the \cite{GHS20} result with a construction of a family of distributions over Bukh--Ma codes
%strings (with one distribution over strings  for each Bukh--Ma codeword)
such that the \emph{$k$-decks}\footnote{The $k$-deck of a single string is the multiset of all length-$k$ subsequences of the string, and the $k$-deck of a distribution over strings is the corresponding mixture of $k$-decks of the constituent strings in the mixture; see \Cref{sec:deck-notation} for detailed definitions.}
 of all of the different distributions coincide.  This in turn lets us show that a single trace does not have enough information to make more accurate reconstruction than (essentially) LCS $2n/3$ possible.
 

%If the retention rate $\rho$ is larger than a constant then it is possible to do better than $2/3$ by a corresponding constant; building on \Cref{thm:worst-case-small-rho}, we show that it is actually possible to have an improvement in the LCS proportional to the retention rate:\rnote{Should \Cref{thm:worst-case-small-rho} be its own theorem? I think actually it is a special case of \Cref{thm:worst-case-intermediate-rho}, but maybe we want to call attention to it separately}

%\begin{theorem} \label{thm:worst-case-intermediate-rho} Can get $(2/3 + c \rho)n$ in worst-case setting with one trace. \end{theorem}

\Cref{thm:worst-case-small-rho-informal} sheds light on the high deletion rate and medium deletion rate regimes of one-trace reconstruction.  Turning to the medium and low deletion rate regimes, if the retention rate $\rho$ is large enough (at least some absolute constant), then the algorithm used for \Cref{thm:worst-case-small-rho-upper-bound-informal} is no longer best possible, since it would be better to simply output any string $\wh{x}$ that contains the trace $\by$ as a subsequence. This is because, as observed in \cite{CDLSS22}, any such string $\wh{x}$ achieves expected LCS at least $\E[|\by|]=\rho n = (1-\delta)n$.

Can better performance than this naive $(1-\delta)n$-length LCS be achieved in the medium and low deletion rate regimes?  We give an improvement by constructing a hypothesis string $\wh{\bx}$ that randomly intermingles random bits with the bits of $\by$. A careful analysis of the LCS between this $\wh{\bx}$ and the source string $x$ yields the following:

\begin{theorem} [Worst-case algorithm, small deletion rate, informal statement]\label{thm:worst-case-small-delta-informal}
There is a worst-case one-trace algorithm that achieves expected LCS at least $(1 - \delta + \delta^2/2 - \delta^3/2 + \delta^4/2 - \delta^5/2 - o(1))n$ for deletion rate $\delta.$
\end{theorem}

Given \Cref{thm:worst-case-small-delta-informal}, it is natural to ask about limitations of one-trace reconstruction in the low deletion rate regime. Taking $M=1$ in the main lower bound result (Theorem~1.2) of \cite{CDLSS22}, that result shows that no one-trace algorithm can achieve expected LCS greater than $(1-\delta^C)n$ in the worst-case setting, where $C$ is some absolute (large) constant. In \Cref{sec:average-case} we will see that \Cref{thm:small-delta-average-case-upper-bound-informal} establishes a stronger and near-optimal bound even for the more challenging average-case setting.

\subsubsection{Average-case one-trace reconstruction} \label{sec:average-case}

The average-case setting of one-trace reconstruction turns out to present some unexpected challenges due to connections with difficult unresolved problems in the combinatorics of words.  To see this, let us first consider the problem of average-case trace reconstruction from \emph{zero} traces; so the reconstruction algorithm receives no input at all, and simply aims to output the $n$-bit string $\wh{x}$ which maximizes the expected value of $|\LCS(\wh{x},\bx)|$ across uniform random $\bx \sim \zo^n$.  In contrast with the worst-case setting (where no zero-trace algorithm can achieve expected LCS better than 1/2 because the source string $x$ could be chosen uniformly at random from $\{0^n, 1^n\}$), in the average-case setting the hypothesis string $\wh{x}=(01)^{n/2}$ already achieves $\E[|\LCS(\wh{x},\bx)|] \geq (3/4 - o(1))n$: first greedily match the 0's in $\wh{x}$ with the 0's in $\bx$ from left to right, and then opportunistically augment these $\approx n/2$ matching edges with edges matching pairs of 1's where possible. So nontrivial performance is possible, even with zero traces, in the average-case setting.

%Recall that in the worst-case setting, any zero-trace algorithm cannot do better than $n/2$ because the source string $x$ could be chosen uniformly at random from $\{0^n, 1^n\}$.  In contrast, it is easy to see that in the average-case setting, the hypothesis string $z=(01)^{n/2}$ has $\E[|\LCS(z,\bx)|] \geq (3/4 - o(1))n$ (first greedily match the 0's in $z$ with the 0's in $\bx$ from left to right, and then opportunistically augment these $\approx n/2$ matching edges with edges matching pairs of 1's where possible).

Can we do better with a smarter choice of the hypothesis string $\wh{x}$? A natural idea is to select $\wh{\bx}$ uniformly at random from $\zo^n$. The performance of this zero-trace algorithm is captured by the \emph{Chv\'{a}tal--Sankoff constant}
\[
\gamma_2 := \lim_{n \to \infty}{\frac {\E_{\bx,\wh{\bx} \sim \zo^n}[|\LCS(\bx,\wh{\bx})|]}{n}}
\]
(the ``2'' is because we are working with the binary alphabet); the existence of this limit is an easy consequence of the superadditivity of LCS between random strings (using Fekete's Lemma \cite{Subadditivity:wikipedia}). Despite much investigation over more than 40 years, the value of $\gamma_2$ is not known: in 1975 Chv\'{a}tal and Sankoff showed that $0.727273 \leq \gamma_2 \leq 0.866595,$ and the current state of the art bounds, due to Lueker \cite{Lueker09}, are that $0.788071 \leq \gamma_2 \leq 0.826280$ \cite{ChvatalSankoff:wikipedia}.

A superadditivity argument similarly establishes the existence of the limit 
\begin{equation}
\label{eq:c2}
c_2 := \lim_{n \to \infty} \max_{\wh{x} \in \zo^n} {\frac {\E_{\bx \sim \zo^n}[|\LCS(\bx,\wh{x})|]}{n}},
\end{equation}
which corresponds to the performance of the information-theoretic optimal zero-trace algorithm for the average-case setting. Even less is known about $c_2$ than $\gamma_2$; Bukh and Cox \cite{BC22} have shown (via an involved argument and an automated search) that $c_2 \geq 0.82118$, and we show in \Cref{ap:upper-bound-c2} that $c_2 \leq 0.88999$, but more detailed bounds on the value of $c_2$ do not seem to be known, nor is it known what strings might achieve this optimal bound \cite{Bukh22}.

Given these challenges in understanding zero-trace reconstruction in the average-case setting, the prospects of analyzing one-trace average-case reconstruction may appear dim.
Perhaps surprisingly, for the low deletion rate regime and medium deletion rate regime it turns out that the difficulty of analyzing zero-trace reconstruction is the only barrier to showing an upper bound on average-case one-trace reconstruction.  This is shown in the following theorem, which gives an upper bound on average-case one-trace reconstruction in terms of the quantity $c_2$ from \Cref{eq:c2}:

\begin{theorem} [Average-case upper bound on any algorithm, small retention rate, informal statement] \label{thm:small-rho-average-case-informal}
Let $L_{1,\avg}(\delta,n)$ denote the best expected LCS achievable by any one-trace algorithm at deletion rate $\delta$ in the average-case setting.  Then we have $c_2 \leq \lim_{n \to \infty} {\frac {L_{1,\avg}(\delta,n)}{n}} \leq c_2+ \rho.$ 
\end{theorem}

\Cref{thm:small-rho-average-case-informal} tells us that for any $\rho=o_n(1)$ retention rate, it is not possible to asymptotically improve on the performance of the best zero-trace algorithm.
In fact, in \Cref{thm:small-rho-average-case} we give a generalization of \Cref{thm:small-rho-average-case-informal} which gives an upper bound on the performance of algorithms that receive more than one trace.  The proof is based on a careful analysis, using a coupling argument, of the \emph{a posteriori} distribution of the random source string $\bx$ given the received collection of traces.

Finally, we consider upper and lower bounds which are applicable for the medium and small deletion rate regime.  In the average-case setting, the algorithm of \Cref{thm:worst-case-small-delta-informal} can be shown to have better performance than was established in \Cref{thm:worst-case-small-delta-informal} for the worst-case setting:

\begin{theorem} [Average-case algorithm, small deletion rate, informal statement] \label{thm:small-delta-average-case-algorithm-informal}
There is an average-case one-trace algorithm that achieves expected LCS at least 
$ \left( 1 - \delta + \frac12\delta^2   + \frac{17}{8}\delta^4 + \frac{55}{8}  \delta^5 -o(1) \right) n$ for deletion rate $\delta.$
\end{theorem}

Given \Cref{thm:small-delta-average-case-algorithm-informal}, it is natural to investigate the best possible performance of any one-trace algorithm in the average-case setting for small $\delta$. A relatively simple probabilistic argument (which is based on a union bound across all possible matchings, and which we give in \Cref{sec:small-delta-average-case-weak-upper-bound}) shows that the expected LCS achieved by any one-trace algorithm can be at most $(1 - \Omega(\delta/\log(1/\delta))) \cdot n$.  Via a more involved probabilistic argument we strengthen this to a $1-\Theta(\delta)$ bound:  

\begin{theorem} 
[Average-case upper bound on any algorithm, small retention rate, informal statement] 
\label{thm:small-delta-average-case-upper-bound-informal}
For any deletion rate $\delta = \omega(1/n)$, no one-trace algorithm can achieve expected LCS greater than $(1-c \delta)n$ in the average-case setting, where $c$ is some absolute constant.
\end{theorem}

We observe that by virtue of \Cref{thm:small-delta-average-case-algorithm-informal}, \Cref{thm:small-delta-average-case-upper-bound-informal} is best possible up to the hidden multiplicative constant on the $\delta$-term.


%%
%\subsection{Discussion and future work}
%\rnote{Maybe we should get rid of this subsection? If we don't have the conjecture anymore about general $k$ I am not sure that other things we want to say in it are that exciting/worthwhile, what do you guys think?}
%
%A number of directions suggest themselves for future work to get a fuller understanding of the one-trace setting.
%
%In the worst-case setting we are able to show that at retention rate $\rho$, an efficient algorithm can achieve $(2/3 + c \rho)n$. Can a matching upper bound be established on the performance of any one-trace algorithm?  Our \Cref{thm:worst-case-small-rho-upper-bound} is a partial result in this direction. We conjecture that \Cref{thm:worst-case-small-rho-upper-bound} can be strengthened to handle $\rho=1/n^\eps$ for all $\eps > 0$. This would follow from the following conjecture:
%
%\begin{conjecture} \label{conj:deck}
%There is a common deck distribution that is achievable by small perturbations of each of the Bukh-Ma strings.
%\end{conjecture}
%
%In the average-case setting, a natural first goal is to understand the performance of the best \emph{zero-trace} algorithm, i.e. to determine the value of the constant $c_2$ from the frog paper.  This may be quite challenging; we remark that the seemingly related problem of determining the value of the Chvatal-Sankoff constant is a longstanding open problem on which there has been no progress since Luecker.  Another goal in the average-case setting is to show that at retention rate $\rho$, the best one-trace algorithm can achieve an LCS of $(c_2 + c' \rho)n$ for some constant $c'>0$. (Note the difference between this and the open question mentioned above for the worst-case setting: there we would like an upper bound on the performance of any one-trace algorithm, while here we would like to show that some algorithm can do at least as well as $(c_2 + c' \rho)n$.) It is not clear whether it will be possible to show that some algorithm achieves $c_2 + c' \rho$ without first obtaining some understanding of the optimal $n$-bit string for zero-trace algorithm (the string that achieves expected LCS $c_2 n$).
%

\subsection{Future work}

A natural first goal for future work is to obtain sharper results. For example, if the deletion rate $\delta$ is 0.1,
 what is the largest constant $c$ such that expected LCS $cn$ can be achieved in the worst-case setting? In the average-case setting? What if $\delta = 0.9$?  We do not currently have sharp answers to questions such as these.

A different goal is to go beyond one-trace reconstruction.  While our negative results \Cref{thm:worst-case-small-rho-upper-bound-informal} and \Cref{thm:small-rho-average-case-informal} extend to algorithms that receive multiple traces, it would be interesting to extend positive results such as \Cref{thm:worst-case-small-rho-informal}, \Cref{thm:worst-case-small-delta-informal} and \Cref{thm:small-delta-average-case-algorithm-informal} to the setting of multiple traces. In this context we mention the work of Chakraborty et al.~\cite{CDK21}, which gave an average-case algorithm for approximate trace reconstruction from three traces in an insertion-deletion model.



