%!TEX root = onetrace.tex

\section{Average-case one-trace reconstruction, high and medium deletion rate} \label{sec:average-case-small-rho}

In this section we bound the performance of any average-case few-trace algorithm when the retention rate is low.
%\medskip
%\red{
%\href{https://www.dropbox.com/home/FOCS19-RANDOM19-SODA21-ITCS21-SODA22-Trace-Reconstruction-Combinatorial/Approximate-trace-reconstruction-notes/weak-approximate-trace-reconstruction?preview=one-trace-avg-case-small-rho.pdf}{Here} and \href{https://docs.google.com/document/d/1JcLtZbRzg0MtAb9EutXVwkxnfCIo41A_XmMIu7a1tk8/edit}{here (first 4-5 pages or so of the Google Drive file)} are some notes that are relevant for \Cref{thm:small-rho-average-case-informal} (the average-case small-$\rho$ setting). (This section should talk about multiple traces as well as one trace.)}
%\bigskip
Given the length of the source string $n$, we write 
  $L_{0,\avg}(n)$ to denote the performance of an optimal zero trace algorithm:
$$
L_{0,\avg}(n)=\max_{z\in \{0,1\}^n} \Ex_{\bx\sim \{0,1\}^n} \big[\abs{\LCS(\bx,z)}\big]
$$
(note that this quantity does not depend on $\delta$), and recall from \Cref{sec:alg-performance-notation} that for $t>0$,
$L_{t,\avg}(\delta,n)$ captures the information-theoretic optimal performance of any $t$-trace algorithm at deletion rate $\delta$.

We show that when $t\rho$ is small, where $t$ is the number of traces and 
  $\rho$ is the retention rate, it is not possible to do much better than $L_{0,\avg}(n)$:

\begin{theorem} [Average-case upper bound on any algorithm, small retention rate] \label{thm:small-rho-average-case}
Let $n$ be the length of the source string, $t$ be the number of traces and 
  $\rho=1-\delta$ be the retention rate.
  %\xnote{I wrote it this way since there will be no conditions on $n, t$ and $\rho$.}
%Let $t(n)$ be a number of traces and $\rho(n)=1-\delta(n)$ be a deletion rate.  For sufficiently large $n$ we have that 
Then 
\[
L_{0,\avg}(n) \leq L_{t,\avg}(\delta,n) \leq L_{0,\avg}(n) + t \rho \cdot n.
\]
\end{theorem}

We note that as a special case of the theorem above, if $t(n)\rho(n)=o(n)$ then the leading constant of what can be achieved with $t$ traces is no better than if no traces were given.  This is in contrast with the worst-case setting, as witnessed by \Cref{thm:worst-case-small-rho-informal} and the discussion immediately preceding it.

The lower bound is immediate so in the rest of this section we prove the upper bound.
We start with some notation for working with multiple traces in the proof of Theorem \ref{thm:small-rho-average-case}.
Given $n$ and $t$, let $R_1,\ldots,R_t\subseteq [n]$ be $t$ subsets which should be viewed as
  locations retained from  an $n$-bit string to obtain its $t$ traces (so if the source string is $x$
  then the traces are $y^{(s)}=x_{R_s}$ for $s=1,\dots,t$). 
Let $i_1<\cdots <i_m$ be an enumeration of indices in $ R_1\cup \cdots \cup R_t$.
We write $C=(C_1,\ldots,C_m)$ to denote the tuple where $C_j$ is the set of those $s\in [t]$ such that $i_j\in R_s$.
We will refer to $C$ as the \emph{collision information} of $R_1,\ldots,R_t$, denoted by
  $C=C(R_1,\ldots,R_t)$.


\begin{example}
Consider the case that $n=8$, $t=3$, the source string $x$ is $11010011$, and the three traces $y^{(1)}=1100,$ $y^{(2)}=110,$ $y^{(3)}=1001$ are obtained from $x$ as shown below:
\begin{center}
\begin{tabular}{r c c c c c c c c}
$x:$ & 1 & 1 & 0 & 1 & 0 & 0 & 1 & 1\\
$y^{(1)}:$ & 1 & 1 &  &  & 0 & 0 &  & \\
$y^{(2)}:$ &  & 1 &  & 1 & 0 &  &  & \\
$y^{(3)}:$ &  & 1 &  &  & 0 & 0 & 1 & 
\end{tabular}
\end{center}
In this case we have that $m=6$, $i_1=1$, $i_2 = 2$, $i_3=4$, $i_4=5$, $i_5=6$, $i_6=7$, and $C_1=\{1\}$, $C_2=\{1,2,3\}$, $C_3 = \{2\}$, $C_4 = \{1,2,3\}$, $C_5=\{1,3\}$, $C_6=\{3\}$.  As discussed in \Cref{obs:uniform} below, given the traces $y^{(1)},y^{(2)},y^{(3)}$ and the collision information $C$, it is possible to reconstruct an $m$-bit subsequence $y$ (in this example $y=111001$) of $x$, but not the $n-m$ bits of $x$ that are missing from $y$ nor the locations of where the $m$ bits of $y$ are situated in $x$.
\end{example}


We will consider average-case algorithms that are given not only $t$ traces $\by_1,\ldots,\by_t\sim \Del_\delta(\bx)$
  of a random string $\bx\sim \{0,1\}^n$ but also the collision information $\bC=C(\bR_1,\ldots,\bR_t)$,
  where $\bR_s\subseteq [n]$ is the set of locations that are retained in obtaining trace $\by^{(s)}$ from $\bx$ for each $s\in [t]$.
Let $L_{t,\avg}^*(\delta,n)$ denote the performance of the best algorithm $A$ under this setting:
$$
L_{t,\avg}^*(\delta,n) :=  \max_{A} \Ex_{\bx \sim \zo^n} \Ex_{\bR_1,\dots,\bR_t\sim \calR_\rho}
\Bigl[ \abs[\big]{ \LCS\bigl(A(\bx_{\bR_1},\ldots,\bx_{\bR_t},\bC),\bx\bigr) } \Bigr],
$$
where we write $\calR_\rho$ to denote   the distribution where $\bR\sim \calR_\rho$ is  
  drawn by including each element in $[n]$ independently with probability $\rho$ and
 $\bC=C(\bR_1,\ldots,\bR_t)$ is the collision information of sets $\bR_1,\ldots,\bR_t$.
It is clear that $L_{t,\avg}^*(\delta,n)\ge L_{t,\avg}(\delta,n)$. We prove Theorem \ref{thm:small-rho-average-case} by showing that 
$$
L_{t,\avg}^*(\delta,n)\le L_{0,\avg}(n)+t\rho\cdot n.
$$

\begin{observation}
[\emph{A posteriori} distribution of a uniform random source string given $t$ traces and their collision information] \label{obs:uniform}
Let $\bx$ be a uniform random source string drawn  from $\zo^n$.
Let $I=(y^{(1)},\ldots,y^{(t)},C)$ be any fixed outcome of $t$ traces from $\Del_\delta(\bx)$ together with
  the collision information of their locations retained. Then the \emph{a posteriori} distribution of $\bx$ given $I$ is as follows:
%Let $\bx$ be a uniform random source string from $\zo^n$.
%Given any fixed outcome $y \in \zo^m$ of a single trace $y =\by \sim \Del_\delta(\bx)$,
%the \emph{a posteriori} distribution of $\bx$ given $y$ is as follows:
\begin{flushleft}
\begin{enumerate}
\item Let $C=(C_1,\ldots,C_m)$ for some $m\le n$.
We define an $m$-bit string $y$ as follows. For each $j\in [m]$, pick an $s\in C_j$ and set $\smash{z_j=y^{(s)}_k}$ where 
  $k$ is the number of $j'\le j$ such that $s\in C_{j'}$.
  (Note that the value of $z_j$ does not depend on the choice of $s\in C_j$.)
  
\item The rest of the process is the same as the description of 
  the a posteriori distribution of $\bx$ given one trace $y$ (see Observation \ref{ob:post1}); for convenience we will write $\calD_y$ to denote the distribution of $\bx$ described below. Draw a uniform random $m$-element subset of $[n]$ (say $\bS = \{\bs_1,\dots,\bs_m\}$ where $1 \leq \bs_1 < \cdots < \bs_m \leq n$);

\item For each $j \in [m]$ set $\bx_{\bs_j}=z_j$, and for each $i \notin \bS$ set $\bx_i$ to an independent uniform bit.
\end{enumerate}
\end{flushleft}
\end{observation}

We are now ready to prove Theorem \ref{thm:small-rho-average-case}.
\begin{proof}[Proof of Theorem \ref{thm:small-rho-average-case}]
Let $A$ be an optimal algorithm that achieves $L^*_{t,\avg}(\delta,n)$.
Let $\bx\sim \zo^n$~and let $\smash{\bI=(\by^{(1)},\ldots,\by^{(t)},\bC)}$ be the 
  input of $A$, where $A$ outputs $A(\bI)\in \zo^{n}$.
Given an outcome $I$ of $\bI$, we write $y(I)$ to denote the string derived from $\bI$ as in Step 1 of \Cref{obs:uniform}. Then
\begin{align}\label{eq:haha}
L^*_{t,\avg}(\delta,n)=\sum_{I} \Pr\big[\bI=I\big]\cdot \Ex_{\bx\sim \calD_{y(I)}} \Bigl[ \abs[\big]{ \LCS\bigl(A(I),\bx\bigr) } \Bigr],
%\le L_{0,\avg}(n)+ \sum_{I} \Pr\big[\bI=I\big]L_{0,\avg}(n)\cdot |y|.
\end{align}
where the sum is over all possible inputs $I$ of $A$. %and $y$ is the string derived from $I$ in in Step 1 of Observation \ref{ob:post2}. 
We need the following claim:
\begin{claim}\label{claim:short}
Fix any string $y\in \{0,1\}^m$ for some $m\le n$.
For any string $z\in \{0,1\}^n$, we have 
$$
\Ex_{\bx\sim \calD_y} \Bigl[ \abs[\big]{ \LCS\bigl(z,\bx\bigr) }\Bigr]\le L_{0,\avg}(n)+ m.
$$
\end{claim}
\begin{proof}
%Let $\calU$ be the uniform distribution over $\zo^n$.
Consider the following coupling $(\bx ,\bx')\sim \calE$ of the uniform distribution over $\zo^n$ and $\calD_y$:
  first draw $\bx\sim \zo^n$; then draw a size-$m$ subset $\bS$ of $[n]$ 
  uniformly at random and replace bits of $\bx$ at $\bS$ by $y$ to obtain $\bx'$.
It is easy to verify that $\calE$ is a coupling of the uniform distribution over $\zo^n$ and $\calD_y$.
For any string $z\in \{0,1\}^n$, we have 
\begin{align*}
\Ex_{\bx'\sim\calD_y}\Bigl[ \abs[\big]{ \LCS\bigl( z,\bx'\bigr) } \Bigr]
&=\Ex_{(\bx,\bx')\sim \calE} \Bigl[ \abs[\big]{ \LCS\bigl(z,\bx'\bigr) } \Bigr] \\
&\le \Ex_{(\bx,\bx')\sim \calE}
  \Big[ \abs[\big] {\LCS\bigl(z,\bx\bigr) }\Bigr]+m
  =\Ex_{\bx\sim \zo^n}\Bigl[ \abs[\big] {\LCS\bigl(z,\bx\bigr) }\Bigr]+m\le L_{0,\avg}(n)+m,
\end{align*}
where the inequality used the fact that $(\bx,\bx')\sim \calE$ always have Hamming distance at most $m$.
\end{proof}

Combining (\ref{eq:haha}) with Claim \ref{claim:short}, we have 
$$
L^*_{t,\avg}(\delta,n)\le \sum_{I} \Pr\big[\bI=I\big]\cdot \Big(L_{0,\avg}(n)+|y|\Big)
=L_{0,\avg}(n)+\bE\big[|\by|\big].
$$
By linearity of expectation, we have 
$$
\bE\big[|\by|\big]=
n (1 - \delta^t) = n(1-(1-\rho)^t) \leq \rho t \cdot n.
%\le \bE\big[\by^{(1)}\big]+\cdots \bE\big[\by^{(t)}\big]=\rho t\cdot n.
$$
%The second term can be bounded from above easily by  
This finishes the proof of the theorem.
\end{proof}

%The proof of Theorem \ref{thm:small-rho-average-case} uses the following claim:




