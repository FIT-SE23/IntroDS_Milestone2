%!TEX root = onetrace.tex

\begin{abstract} 

The well-known \emph{trace reconstruction problem} is the problem of inferring an unknown source string $x \in \zo^n$ from independent ``traces'', i.e.~copies of $x$ that have been corrupted by a $\delta$-deletion channel which independently deletes each bit of $x$ with probability $\delta$ and concatenates the surviving bits. 
The current paper  considers the extreme data-limited regime in which only a single trace is provided to the reconstruction algorithm. In this setting exact reconstruction is of course impossible, and the question is to what accuracy the source string $x$ can be approximately reconstructed.

We give a detailed study of this question, providing algorithms and lower bounds for the high, intermediate, and low deletion rate regimes in both the worst-case ($x$ is arbitrary) and average-case  ($x$ is drawn uniformly from $\zo^n$) models.  In several cases the lower bounds we establish are matched by computationally efficient algorithms that we provide. 

We highlight our results for the high deletion rate regime: roughly speaking, they show that 

\begin{itemize}

\item Having access to\ignore{even a small number of ``bits of trace''} a single trace is already quite useful for worst-case trace reconstruction: an efficient algorithm can perform much more accurate reconstruction, given one trace that is even only a few bits long, than it could given no traces at all.  But in contrast,

\item in the average-case setting, having access to a\ignore{ small number of ``bits of trace''} single trace is provably not very useful: no algorithm, computationally efficient or otherwise, can achieve significantly higher accuracy given one trace that is $o(n)$ bits long than it could with no traces.

\end{itemize}

%This demonstrates an interesting distinction between the average-case and worst-case versions of one-trace reconstruction.

\ignore{In contrast, at high deletion rates average-case and worst-case trace reconstruction provably have very different qualitative behaviors from each other.\rnote{These last two sentences are clumsily trying to say that
\begin{itemize}
\item when $\delta$ is small basically $1-\Theta(\delta)$ is achievable and best possible for both average-case and worst case; but
\item when $\rho$ is small but nonzero, in the average case setting you can't improve on a trivial (zero-trace) result, while you can improve on a trivial zero-trace performance in the worst-case setting.
\end{itemize}
}
}

\end{abstract}
