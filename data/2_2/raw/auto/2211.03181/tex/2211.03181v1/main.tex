\documentclass[a4paper,11pt]{paper}
% \linespread{2}

\usepackage{float}
\usepackage[dvips]{graphics} 
\usepackage[pdftex]{graphicx}
\usepackage{array}
\usepackage{amsmath,amssymb,amsthm,color,amsfonts,graphics,graphicx,verbatim}
\usepackage{algorithm}
\usepackage{algorithmic}%\numberwithin{algorithm}{section}
\usepackage{multirow}
\usepackage{appendix}
\usepackage{natbib} 
\usepackage[colorlinks]{hyperref}
%\usepackage[authoryear,round]{natbib}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}{Lemma}[section]
\newtheorem{proposition}{Proposition}[section]
\newtheorem{corollary}{Corollary}[section]
\newtheorem{Remark}{Remark}[section]
\theoremstyle{example}
%\newtheorem{example}{Example}[section]
\usepackage{rotating}

\hypersetup{
                bookmarksnumbered,
                pdfstartview={FitH},
                citecolor={blue},
                linkcolor={blue},
                urlcolor={blue},
                pdfpagemode={UseOutlines}
}

\newtheorem{definition}{Definition}
% \newtheorem{theorem}{Theorem}
%\newtheorem{proposition}{Proposition}
%\newtheorem{corollary}{Corollary}
%\newtheorem{lemma}{Lemma}
%\newtheorem{remark}{Remark}


\DeclareMathOperator*{\argmax}{\arg\max}
\DeclareMathOperator*{\argmin}{\arg\min}

\def\bfu{{\bf u}}
\def\bfv{{\bf v}}
\def\bfh{{\bf h}}
\def\bfx{{\bf x}}
\def\bfz{{\bf z}}
\def\bfb{{\bf b}}
\def\bfX{{\bf X}}
\def\bfP{{\bf P}}
\def\bfF{{\bf F}}
\def\bfA{{\bf A}}
\def\bfI{{\bf I}}
\def\bfJ{{\bf J}}
\def\bfH{{\bf H}}
\def\bfzero{{\bf 0}}
\def\bfone{{\bf 1}}
\def\bfmu{\boldsymbol{\mu}}
\def\bftheta{\boldsymbol{\theta}}
\def\bfSigma{\boldsymbol{\Sigma}}



\title{Cauchy robust principal component analysis with applications to high-deimensional data sets}
\vspace{3in}
\author{Ayisha Fayomi $^1$, Yannis Pantazis $^2$, Michail Tsagris $^3$ \\
and Andrew T.A. Wood $^4$ \\
\\
$^1$ Department of Statistics, King Abdulaziz University, \\
Jeddah, Saudi Arabia \\
\href{mailto:afayomi@kau.edu.sa}{afayomi@kau.edu.sa} \\
$^2$ Institute of Applied \& Computational Mathematics, \\
Foundation for Research \& Technology - Hellas,
Heraklion, Greece \\
\href{mailto:pantazis@iacm.forth.gr}{pantazis@iacm.forth.gr} \\
$^3$ Department of Economics, University of Crete, \\
Gallos Campus, Rethymnon, Greece  \\
\href{mailto:mtsagris@uoc.gr}{mtsagris@uoc.gr} \\
$^4$ Research School of Finance, Actuarial Studies \& Statistics, \\
Australian National University, Canberra, Australia \\
\href{mailto:Andrew.Wood@anu.edu.au}{Andrew.Wood@anu.edu.au}
}

\begin{document}
\maketitle

\begin{abstract}
% Highlight: (1) Open new directions in robust data analysis & (2) capable of handling really high dimensional data.
Principal component analysis (PCA) is a standard dimensionality reduction technique used in various research and applied fields. From an algorithmic point of view,  classical PCA can be formulated in terms of operations on a multivariate Gaussian likelihood. As a consequence of the implied Gaussian formulation, the principal components are not robust to outliers. In this paper, we propose a modified formulation, based on the use of a multivariate Cauchy likelihood instead of the Gaussian likelihood, which has the effect of robustifying the principal components.  We present an algorithm to compute these robustified principal components. We additionally derive the relevant influence function of the first component and examine its theoretical properties. Simulation experiments on high-dimensional datasets demonstrate that the estimated principal components based on the Cauchy likelihood outperform or are on par with existing robust PCA techniques.
\end{abstract}


\section{Introduction}
\label{Intro.3-CPC}
In the analysis of multivariate data, it is frequently desirable to employ statistical methods which are insensitive to the presence of outliers in the sample. To address the problem of outliers, it is important to develop robust statistical procedures. Most statistical procedures include explicit or implicit prior assumptions about the distribution of the observations, but often without taking into account the effect of outliers. The purpose of this paper is to present a novel robust version of PCA which has some attractive features.

%The most popular distribution to be assumed is a Gaussian distribution but, unfortunately, despite its obvious attractions the Gaussian distribution sometimes leads to unsatisfactory results, due to its lack of robustness to outliers. It has been shown, here in section \ref{NonRob.SPCA} and elsewhere, that using the Gaussian distribution not produce robust estimators of population characters of interest. Many alternative procedures which have better robustness properties have been suggested and this paper suggests one also.

Principal components analysis (PCA) is considered to be one of the most important techniques in statistics. However, the classical version of PCA depends on either a covariance or a correlation matrix, both of which are very sensitive to outliers. We develop an alternative method to classical PCA, which is far more robust, by using a multivariate Cauchy likelihood to construct a robust principal components (PC) procedure. It is an adaptation of the classic method of PCA obtained by replacing the Gaussian log-likelihood function by the Cauchy log-likelihood function, in a sense that will be explained in section \ref{Lik.Inter.PCA}.  Although we do not claim that the interpretation of standard PCA in terms of operations on a Gaussian likelihood is new, see Bolton and Krzanowski, this fact does not appear to have been exploited in the development of a robust PCA procedure, as we do in this paper.  An important reason for using the multivariate Cauchy likelihood is that this likelihood has only one maximum point, but the single most important motivation is that it leads to a robust procedure.

In the next section we review briefly some of the techniques employed for estimating parameters and for directing a PCA in ways which are robust against the presence of outliers. We also present robustness preliminaries that include some important techniques which are necessary to assess whether the method used is robust or not. In Section \ref{CPCA} we develop the Cauchy-PCA and theoretically explore its robustness properties. Finally, in Section \ref{Comp.Algo} we present the numerical algorithms for creating Cauchy PCs, and also give the results of a number of very high-dimensional real-data and simulated examples.  Our approach is seen to be competitive with, and often gives superior results to, that of the projection pursuit algorithm of Croux et al. (2007, 2013).  Finally we conclude the paper in Section \ref{concl.}. 

\subsection{Literature review on robust PCA} \label{Lit.Review}
It is well known that PCA is an important technique for high-dimensional data reduction. PCA is based on the sample covariance matrix $\hat{{\bf \Sigma}}$ and it involves searching for a linear combination $y_{j}= {\bf u}^{T}{\bf x}_{j}$ of the ${\bf x}$ components of the vector that maximize the sample variance of the components of $y$. According to \citet{Mardia&Kent&Bibby:1979}, the solution will be given by the equation
\[\hat{\bf \Sigma}={\bf U \Lambda U}^{T},\]
where ${\bf \Lambda}= \hbox{diag}\{\lambda_{1}, \ldots, \lambda_{p}\}$ and its diagonal elements $\lambda_{i}$ are the sample variances, while ${\bf U}$ is an orthogonal matrix, i.e. ${\bf U U}^{T} ={\bf U}^{T}{\bf U}={\bf I}_{p}$, whose columns ${\bf u}_{i}$ are the corresponding eigenvectors which represent the linear combinations.
[[The principal components are efficiently estimated in practice via Singular Value Decomposition (SVD) (cite Lanczos for an efficient algorithm).]]


Classical PCA, unfortunately, is non-robust, since it based on the sample covariance or sample correlation matrix which are very sensitive to outlying observations; see section \ref{NonRob.SPCA}. However, this problem has been handled by two different methods which result in robust versions of PCA by:
 \begin{description}
   \item[i.] replacing the standard covariance or correlation matrix with a robust estimator; or
   \item[ii.] maximising (or minimising) a different objective function to obtain a robust PCA.
 \end{description}
Many different proposes had been developed to carry out robust PCA, such that using projection pursuit PP, $M-$estimators and so on. 

Despite maximum likelihood estimation, perhaps, being considered as the most important statistical inference method, sometimes this approach can lead to improper results when the underlying assumptions are not satisfied, for instance, when data contain outliers or deviate slightly from the supposed model. A generalization of maximum likelihood estimation proposed by \citet{Huber:1964} which is called $M$-estimation, aims to produce a robust statistic by constructing approaches that are resistant to deviations from the underline assumptions. $M$-estimators were also defined for the multivariate case by \citet{Maronna:1976}. 

\citet{Campbell:1980} provided a procedure for robust PCA by examining the estimates of means and covariances which are less affected by outlier observations, and by exploring the observations which have a large effect on the estimates. He replaced the sample covariance sample by an $M-$estimator. \citet{Hubert:2003} introduced a new approach to create robust PCA. It combines the advantages of two methods, the first one is based on replacing the covariance or correlation matrix by its robust estimator, while the second one is based on maximizing the objective function for this robust estimate.

A robust PCA based on the projection pursuit (PP) method was developed by \citet{Li:1985}, using Huber's $M$-estimator of dispersion as the projection index. The objective of PP is to seek projections, of the high-dimensional data set onto low-dimensional subspaces, that optimise a function of "interestingness". The function that should be optimised is called an index or objective function and its choice depends on a feature that the researcher is concerned about. This property gives the PP technique a flexibility to handle many different statistical problems range from clustering to identifying outliers in a multivariate data set. 

\citet{Bolton:1999} characterized the PC's for PP in terms of maximum likelihood under the assumption of normality. PCA can be considered as a special case of PP as well as many other methods of multivariate analysis. \citet{Li:1985} used Huber's $M$-estimator of dispersion as projective index to develop a robust PCA based on the PP approach. The sample median was used as a projective index to develop a robust PCA by \citet{Xie:1993}. In their simulation studies, \citet{Xie:1993} observed a PCA resistant to outliers and deviations from the normal distribution.
%\citet{Li:1985} used Huber's M-estimator of dispersion as projective index to develop a robust PCA based on the PP approach, while \citet{Xie:1993} developed a robust PCA also based on PP but by using the sample median as projective index since it is the most robust estimator of scale.
\cite{croux2007algorithms,croux2013robust} also suggested a robust PCA using projection pursuit and we will contrast our methodology against their algorithm.  

\section{Preliminaries on standard PCA} \label{NonRob.SPCA}
PCA is an orthogonal linear transformation that projects the data to a new coordinate system according to the variance of each direction. Given a data matrix $\bfX\in\mathbb R^{n\times p}$ with each row correspond to a sample, the first direction $\bfu_1$ that  maximizes the variance is defined through
\begin{equation*}
\bfu_1 = \argmax_{||\bfu||_2=1} ||(\bfX - \bfone_n\bar{\bfx}^T) \bfu||_2^2,
\end{equation*}
where $\bfone_n$ is an $n$-dimensional vector whose elements are all set to 1 while $\bar{\bfx}=\frac{1}{n}\sum_{i=1}^n \bfx_i$ is the empirical mean. 
The process is repeated $k$ times and at each iteration the to-be-estimated principal direction has to be orthogonal to all previously-computed principal directions. Thus, the 
$k$-th direction which has to be orthogonal to the previous ones is defined by
\begin{equation*}
\bfu_k = \argmax_{||\bfu||_2=1} ||(\bfX - \bfone_n\bar{\bfx}^T) \bfu||_2^2 \ \ \text{subject to} \ \  \bfu_k \perp \bfu_j \ \text{with} \ j=1,...,k-1 \ .
\end{equation*}





\subsection{Non-robustness of standard PCA}
We will show that the influence function for the largest eigenvalue of the covariance matrix and the respective eigenvector are unbounded with respect to the norm of an outlier sample. Suppose that $\bfSigma$ is the covariance matrix of a population with distribution function $F$, i.e.,
\begin{equation}\label{Pop.Cov.}
{\bfSigma} = \int_{\mathbb R^p} (\bfx-\bfmu)(\bfx-\bfmu)^{T} dF(\bfx),
\end{equation}
where $\bfmu=\int_{\mathbb R^p} \bfx dF(\bfx)$ corresponds to the mean vector. Assume that the leading eigenvalue of $\bfSigma$ has multiplicity 1, then we denote it by $\lambda$ and the leading eigenvector by $\hat{\bfu}$ (i.e., $\bfu_{1}=\hat{\bfu}$).

% {\bf Define the influence function.}
Let $T$ be an arbitrary functional, $F$ a distribution and ${\bf z}\in\mathbb R^p$ an arbitrary point in the relevant sample space. The influence function is defined as 
\begin{equation}
IF_T(\bfz;F) = \lim_{\epsilon\to 0+} \frac{T((1-\epsilon) F + \epsilon \Delta_{\bfz}) - T(F)}{\epsilon},
\end{equation}
where $\Delta_{\bfz}$ is a unit point mass located at $\bfz$.

A robust estimator for $T$ means that the influence function is bounded with respect to the norm of the outlier $\bf z$.

%We prove that standard PCA is not robust by showing that the influence function for the eigenvalue $\lambda$ with the highest value and the respective eigenvector $\boldsymbol{\mu}$ is unbounded. 


\begin{proposition}
% {\bf Theorem on influence function for first eigenvalue and eigenvector.}
The influence function for the leading eigenvector of $\bfSigma$ is given by\footnote{We use ${\bf A}^+$ to denote the Moore-Penrose inverse of a matrix $\bf A$.} 
\begin{equation}
IF_{\hat{\bfu}} (\bfz, F) = - \big( (\bfz-\bfmu)^{T}\hat{\bfu} \big) (\bfSigma-\lambda \bfI_p)^{+} (\bfz-\bfmu).
\end{equation}
Similarly, the IF for the largest eigenvalue of ${\bfSigma}$ is
\begin{equation}
IF_\lambda (\bfz, F) = 
\big( (\bfz-\bfmu)^{T}\hat{\bfu} \big)^2 - \lambda. 
\end{equation}
% where ${\lambda}_{[\epsilon]}$ is defined in (\ref{lamda-eps}). 
\end{proposition}

The detailed calculations are presented in Appendix \ref{NonRob:PCA:proof}. The following result shows that outliers with unbounded influence function do exist.

\begin{corollary}
% {\bf The influence function is unbounded w.r.t. the norm of an outlier.} \\
Let $\bfz=\bfmu + \gamma \hat{\bfu} + \eta \bfv$ where $\bfv$ is orthogonal to $\hat{\bfu}$ and does not belong to the null space of $\bfSigma$ and $\gamma,\eta\neq 0$ then
\begin{equation*}
\lim _{\bfz: \, ||\bfz||_2 \rightarrow \infty}||IF_{\hat{\bfu}} (\bfz, F)||_2 = \infty,
\end{equation*}
and similarly for $IF_\lambda (\bfz, F)$.
\end{corollary}

\begin{proof}
Direct substitution of $\bfz$ into the influence function gives:
\begin{equation*}
IF_{\hat{\bfu}} (\bfz, F) = -((\gamma \hat{\bfu} + \eta \bfv)^T \hat{\bfu}) (\bfSigma-\lambda \bfI_p)^{+} (\gamma \hat{\bfu} + \eta \bfv)
= - \gamma \eta (\bfSigma-\lambda \bfI_p)^{+} \bfv.
\end{equation*}
Since $\bfv$ does not belong to the null space of $\bfSigma$, it holds that $(\bfSigma-\lambda \bfI_p)^{+} \bfv \neq \bfzero$ thus $||(\bfSigma-\lambda \bfI_p)^{+} \bfv||_2=c\neq0$. Hence,
\begin{equation*}
||IF_{\hat{\bfu}} (\bfz, F)||_2 = |\gamma| |\eta| c.
\end{equation*}
Given that $||\bfz||_2^2 = \gamma^2+\eta^2+||\bfmu||_2^2+\gamma \bfmu^T\hat{\bfu}+\eta \bfmu^T\bfv$, either sending $|\gamma|\to\infty$ or $|\eta|\to\infty$ completes the proof.

Similarly,
\begin{equation*}
IF_\lambda (\bfz, F) = \gamma^2-\lambda \rightarrow \infty, 
\end{equation*}
as $|\gamma|\to\infty$.
\end{proof}


\subsection{Generalizations of standard PCA}
\label{Lik.Inter.PCA}
Standard PCA can be viewed as a special case of a more general optimization problem. We present two such generalization: the first one leads to projection pursuit algorithms while the second leads to a maximum likelihood formulation. Let $\bfu$ be a unit vector and define the projection values
\begin{equation*}
c_{i}(\bfu) = \bfx^{T}_{i} \bfu, {\hspace{3mm}} i=1, \ldots, n,
\end{equation*}
and a function $\Phi:\mathbb R^n \to \mathbb R$ acting on the projected values. The first generalization of PCA is defined as the maximization of $\Phi$:
\begin{equation*}
\bfu_1 = \argmax_{||\bfu||_2=1} \Phi(c_1(\bfu),...,c_n(\bfu)) \ .
\end{equation*}
As in the standard PCA, the following principal directions are obtained after removing the contribution of the current principal component from the data. When $\Phi$ is the sample variance then we recover the standard PCA.
% Shall we state other choices for $\Phi$?

The second generalization interprets the computation of the principal component as a maximum likelihood estimation problem. By letting, 
\begin{equation}\label{GausLogLik}
    l_{G}(\mu, \sigma^{2}| c_{1},\ldots, c_{n})= -\frac{n}{2} \log {\sigma}^{2} - \frac{n}{2{\sigma}^{2}}\sum_{i=1}^{n}(c_{i}-\mu)^{2}.
\end{equation}
be the Gaussian log-likelihood, the first principal direction can be obtained by solving the minimax problem:
\begin{equation*}
\min_{||\bfu||_2=1}\max_{\mu, \sigma^2} \ l_{G}(\mu, \sigma^{2}| c_{1}(\bfu),\ldots, c_{n}(\bfu)).
\end{equation*}
Indeed, the inner maximization can be solved analytically which leads to the optimal solution
\begin{equation*}
\hat{\mu}(\bfu) = \frac{1}{n} \sum_{i=1}^n c_i(\bfu) =: \bar{c}({\bf u})
\end{equation*}
and
\begin{equation*}
{\hat{\sigma}}^{2}({\bf u}) = \frac{1}{n} \sum_{i=1}^{n} (c_{i}({\bf u})- \bar{c}({\bf u}))^{2}.
\end{equation*}
Unsurprisingly, the optimal values are the sample mean and the sample variance. Using the above formulas it is straightforward to show that
\begin{eqnarray}
  \argmin_{||\bfu||_2=1} \ l_{G}\big(\hat{\mu}(\bfu), {\hat{\sigma}}^{2}({\bfu})| c_{1}({\bfu}), \ldots, c_{n}({\bfu})\big)
  = \argmax_{||\bfu||_2=1} \ \hat{\sigma}^{2}({\bfu}) \ .
\end{eqnarray}
Variations of PCA can be derived by changing the likelihood function and in the next section we analyze the case of Cauchy distribution.


% Consider a sample ${\bf x}_{1}, \ldots, {\bf x}_{n}$, and let ${\bf u}$ be a unit vector such that $\|{\bf u}\|=1$. Define
% \[c_{i}({\bf u})={\bf x}^{T}_{i} {\bf u}, {\hspace{3mm}} i=1, \ldots, n.\]
% Regarding the traditional approach to PCA, the first PC is created by choosing ${\bf u}$ that maximises the sample variance of the vector $c_{i}({\bf u})$ which is
% \begin{equation}\label{sigma.u}
% {\hat{\sigma}}^{2}({\bf u})=\frac{1}{n} \sum_{i=1}^{n} (c_{i}({\bf u})- \bar{c}({\bf u}))^{2},
% \end{equation}
% where
% \[\bar{c}({\bf u})=\frac{1}{n}\sum_{i=1}^{n} c_{i}(\bf u).\]
% This is equivalent to minimising the Gaussian log-likelihood for the $c_{i}$ over ${\bf  u}$ as will be shown below. Define the Gaussian log likelihood function $l_{G}(\mu, \sigma^{2})$ for $c_{1}({\bf u})\ldots c_{n}({\bf u})$, treating ${\bf u}$ as fixed:
% \begin{equation}\label{GausLogLik}
%     l_{G}(\mu, \sigma^{2}| c_{1}({\bf u})\ldots c_{n}({\bf u}))= -\frac{n}{2} \log {\hspace{1mm}}{\sigma}^{2}({\bf u}) - \frac{n}{2{\sigma}^{2}}\sum_{i=1}^{n}(c_{i}(\bf u)-\mu)^{2}.
% \end{equation}
% To satisfy our aim, the two step iterative procedure must be followed:
% \begin{description}
% \item[\textbf{Inner maximisation}] for given ${\bf u}$, maximise 
% \begin{eqnarray} \label{maxGLogLik1} 
%   l_{G}(\mu, \sigma^{2}| c_{1}({\bf u})\ldots c_{n}({\bf u}))
% \end{eqnarray}   
% over $\mu$ and $\sigma^{2}$ to obtain $\hat{\mu}({\bf u})$ and ${\hat{\sigma}}^{2}({\bf u})$;
%   \item[\textbf{Outer minimisation} ] of $l_{G}(\hat{\mu}({\bf u}), {\hat{\sigma}}^{2}({\bf u}))$ w.r.t ${\bf u}$.
% \end{description}

% By differentiating (\ref{maxGLogLik1}) with respect to $\mu$ and $\sigma$ respectively we end up with
% \begin{eqnarray}
%   \mathop{\arg \min}_{\bf u}{\hspace{2mm}} l_{G}(\hat{\mu}({\bf u}, {\hat{\sigma}}^{2}({\bf u})| c_{1}({\bf u})\ldots c_{n}({\bf u})=\mathop{\arg \max}_{\bf u}{\hspace{2mm}}{\hspace{1mm}}{\sigma}^{2}({\bf u}).
% \end{eqnarray}

%\subsection{Principal components with an extreme outlier}\label{PCEO}
%We aim to find the formula for both the covariance  matrix\footnote{The case of the correlation matrix is similar and hence omitted.} used to obtain the PCs in the presence of outliers. Consider the sample ${\bf x}_{1}, {\bf x}_{2}, \ldots, {\bf x}_{n}, \bar{{\bf x}}+a{\bf z}$, where $\bar{{\bf x}}=n^{-1}\sum_{i=1}^{n}{\bf x}_{i}$, then
%\[\bar{{\bf x}}_{n+1}=(n+1)^{-1}[n\bar{{\bf x}}+\bar{{\bf x}}+a{\bf z}]= \bar{{\bf x}}+(n+1)^{-1}a{\bf z},\]
%and
%\begin{eqnarray*}
%{\bf S}_{n+1} =\frac{(n-1)}{n}{\bf S}_n + \frac{2a}{n}\left({\bf x}_n-\bar{\bf x}_n\right){\bf z}^T+ \frac{a^2}{n+1}{\bf z}{\bf z}^T 
%\frac{\sum_{i=1}^n{\bf x}_i{\bf x}_i^T + 
%{\bf x}_{n+1}{\bf x}_{n+1}^T-(n+1)\bar{\bf x}_{n+1}\bar{\bf x}_{n+1}^T}{n} \nonumber \\
%&=& \frac{\sum_{i=1}^n{\bf x}_i{\bf x}_i^T + 
%\left(\bar{\bf x}_n+a{\bf z}\right)\left(\bar{\bf x}_n+a{\bf z}\right)^T-(n+1)\left(\bar{\bf x}_n+\frac{a{\bf z}}{n+1}\right)\left(\bar{\bf x}_n+\frac{a{\bf z}}{n+1}\right)^T}{n} \nonumber \\
%&=& \frac{\sum_{i=1}^n{\bf x}_i{\bf x}_i^T + 
%\bar{\bf x}_n\bar{\bf x}_n^T+2a{\bf x}_n{\bf z}^T+a^2{\bf z}{\bf z}^T}{n}  \nonumber \\
%& & - \frac{(n+1)\bar{\bf x}_n\bar{\bf x}_n^T - 2a\bar{\bf x}_n{\bf z}^T-\frac{a^2{\bf z}{\bf z}^T}{(n+1)}}{n} \nonumber \\
%&=&\frac{\sum_{i=1}^n{\bf x}_i{\bf x}_i^T - n\bar{\bf x}_n\bar{\bf x}_n^T+2a\left({\bf x}_n-\bar{\bf x}_n\right){\bf z}^T+\frac{n}{n+1}a^2{\bf z}{\bf z}^T}{n} \nonumber \\
%\end{eqnarray*}

%\begin{eqnarray}\label{PCEO}
% \nonumber to remove numbering (before each equation)
% {\bf S}_{n+1} &=& \frac{1}{n+1} \left[\sum_{i=1}^{n}{\bf x}_{i} {\bf x}_{i}^{T}+(\bar{\bf x}+a{\bf z})(\bar{\bf x}+a{\bf z})^{T}\right]- {\bar{\bf x}}_{n+1}{\bar{\bf x}}_{n+1}^{T},\nonumber\\
% &=&  \frac{1}{n+1} \left[\sum_{i=1}^{n}{\bf x}_{i} {\bf x}_{i}^{T}+\bar{{\bf x}}\bar{{\bf x}}^{T}+a^{2}{\bf z}{\bf z}^{T}+a\bar{{\bf x}}{\bf z}^{T}+a{\bf z}\bar{{\bf x}}^{T}\right] \nonumber\\
% &-&\bar{{\bf x}}\bar{{\bf x}}^{T}-\frac{a^{2}}{(n+1)^{2}}{\bf z}{\bf z}^{T}-\frac{a}{n+1}({\bf z}\bar{{\bf x}}^{T}-\bar{{\bf x}}{\bf z}^{T}) \nonumber \\
% &=& \frac{a^{2}}{(n+1)^{2}}[(n+1)-1]{\bf z}{\bf z}^{T}+ O(a) \nonumber \\
% &\simeq& \frac{a^{2}}{(n+1)} {\bf z}{\bf z}^{T}.
%\end{eqnarray}


% \begin{comment}
% \subsection{Projection Pursuit Method}
% The general formulation of the PP is:
% \begin{equation}\label{PP}
%   \mathop{\max}_{{\bf a}:{\bf a}^{T}{\bf a}=1} {\hspace{2mm}} \sum_{j=1}^{n} \Phi ({\bf x}_{j}, {\bf a}),
% \end{equation}
% which is equivalent to $\mathop{\min}_{{\bf a}:{\bf a}^{T}{\bf a}=1} {\hspace{2mm}} \sum_{j=1}^{n} \Phi ({\bf x}_{j}, {\bf a}).$ $\Phi$ should be large (or small) for the "interesting" projections.\\
% It is worth mentioning here that $\Phi$ is called a score function which plays a vital role in several aspects of statistical inference and computational statistics. The score function is the partial derivative, with respect to some parameter ${\boldsymbol{\theta}}$, of the logarithm of the likelihood function. If the observation is ${\bf X}$ and its likelihood is $L({\boldsymbol{\theta}};{\bf X})$, then the score $\Phi$ can be found through the chain rule:
% \begin{equation}\label{scorfun}
%  \Phi= \frac{\partial}{\partial {\boldsymbol{\theta}}} \log L({\boldsymbol{\theta}};{\bf X})
% \end{equation}
% Note that $\Phi$ is a function of ${\boldsymbol{\theta}}$ and the observation ${\bf X}$, so that, in general, it is not a statistic. The two important properties of the score function are: (i) its expected value is zero and (ii) the variance of the score is known as the Fisher information $I({\boldsymbol{\theta}})$.\\
% PCA can be considered as a special case of PP as well as many other methods of multivariate analysis. Suppose ${\bf X}=({\bf x}_{1}, \ldots, {\bf x}_{n})$ is a $(n \times p)$ data set, such that ${\bf x}_{i}^{T}=(x_{1i} \ldots x_{pi}),$ with covariance matrix ${\bf \Sigma}={\bf X}^{T}{\bf X}$. Since the purpose of PCA is to reduce the dimensionality of the data set from $p$ to $q$ such that $q<p$, let choose a mutually orthogonal vectors ${\bf u}_{1}, \ldots, {\bf u}_{q}$, sequentially, such that the projection of the data set on the certain direction ${\bf u}$ is $z_{j}={\bf u}_{i}^{T}{\bf x}_{j}$. If a projection function $\Phi (z)$ is selected to be the dispersion measurement of the projection $z$, then the set ${\bf u}_{1}, \ldots, {\bf u}_{q}$ of the PC's should satisfy the following system of equations
% \begin{eqnarray}
% \begin{array}{ccccc}
% \lambda_{1} = & \mathop{\max}_{{\bf u}_{1}}\sum_{j=1}^{n} \Phi({\bf u}^{T}_{1}{\bf x}_{j}), &  \|{\bf u}_{1}\|=1 &  & \\
% \lambda_{2} = & \mathop{\max}_{{\bf u}_{2}}\sum_{j=1}^{n} \Phi({\bf u}^{T}_{2}{\bf x}_{j}), & \|{\bf u}_{2}\|=1  & {\hbox{and}} &  {\bf u}_{2}\perp {\bf u}_{1} \\
%  \vdots  &   & \\
%  \lambda_{p} = & \mathop{\max}_{{\bf u}_{p}}\sum_{j=1}^{n} \Phi({\bf u}^{T}_{p}{\bf x}_{j}), & \|{\bf u}_{p}\|=1 & {\hbox{and}} & {\bf u}_{p}\perp {\bf u}_{1}, \ldots, {\bf u}_{p-1},
% \end{array}
% \end{eqnarray}
% where $\lambda_{i}$ and ${\bf u}_{i};{\hspace{2mm}} i=1,\ldots,p$; are the eigenvalues and associated eigenvectors of ${\bf \Sigma}$, respectively. Then the first PC is the projection of ${\bf X}$ onto a particular direction. As it is known that the projection index of PCA is the variance which is very sensitive to outliers. That means, the robust PC can be created by replacing the variance by other projection index that insensitive to outliers.
% \end{comment}





\section{Cauchy PCA}
\label{CPCA}
The Cauchy log-likelihood function is given by
\begin{equation}\label{Cau.LogLik}
    l_{C}({\mu},{\sigma}| {c}_{1}({\bfu}), \ldots, {c}_{n}({\bfu}))=  n \log{\frac{\sigma}{\pi}} - \sum_{i=1}^{n} \log \left\{{\sigma}^{2}+ (c_{i}(\bfu)-{\mu})^{2}\right\}.
\end{equation}
where $\mu$ and $\sigma$ are the two parameters of the Cauchy distribution. The first Cauchy principal direction is also obtained by solving the minimax optimization problem:
\begin{equation}\label{cauchy:minimax}
\min_{||\bfu||_2=1}\max_{\mu,\sigma} \ l_{C}(\mu, \sigma^{2}| c_{1}(\bfu),\ldots, c_{n}(\bfu)).
\end{equation}
In contrast to the Gaussian case, the inner maximization cannot be performed analytically. Therefore an iterative approach needs to be utilized. Here, we apply the Newton-Raphson method with initial values the median and half the interquartile range for the location and scale parameters, respectively. According to \citet{Copas:1975}, although the mean of the Cauchy distribution does not exist and it has infinite variance, the Cauchy log-likelihood function $l_{C}(\mu, \sigma)$ has a unique maximum likelihood estimate, $(\hat{\mu},\hat{\sigma})$. 

Fixing $\mu$ and $\sigma$, the outer minimization is also non-analytic and a fixed point iteration is applied to calculate $\bfu$. The iteration is given by
\begin{equation}
\hat{\bfu} = \frac{\hat{\bfu}_{un}}{||\hat{\bfu}_{un}||_2},
\label{cauchy:norm:eq}
\end{equation}
where $\hat{\bfu}_{un}$ is the unnormalized direction which is obtained from the differentiation of the Lagrangian function with respect to $\bfu$ and it is given by
\begin{eqnarray} \label{parallel}
\hat{\bfu}_{un} = \sum_{i=1}^{n}\frac{({\bfx}_i^T{\hat{\bfu}}-\hat{\mu}){\bfx}_i} {\hat{\sigma}^2 + \left({\bfx}_i^T{\hat{\bfu}}-\hat{\mu} \right)^2} \ .
\label{fixed:point:eq}
\end{eqnarray}

Once the first principal direction has been computed, its contribution from the dataset $\bfX$ is removed and the same procedure to estimate the next principal direction is repeated. This iterative process is repeated $k$ times. The removal of the contribution makes the principal directions orthogonal to each other.
%
We summarize the estimation of $k$ Cauchy principal components in the following pseudo-code (Algorithm \ref{1CPC:Algo.}).

\begin{algorithm}[H]
\caption{Cauchy PCA}
\label{1CPC:Algo.}
\begin{algorithmic}
\FOR{$j=1,...,k$}
\STATE $\bullet$ Initialize ${\hat{\bfu}_{un}}$ and normalize
$\hat{\bfu}= \hat{\bfu}_{un} / ||\hat{\bfu}_{un}||_2$
\WHILE{not converged}
\STATE $\bullet$ Fix $\hat{\bfu}$ and set
$$ c_i(\hat{\bfu}) = \bfx_i^T\hat{\bfu}, \ \ i=1,...,n.$$
\STATE $\bullet$ Via Newton-Raphson algorithm find
$$(\hat{\mu},\hat{\sigma})=\argmax_{\mu, \sigma} \ l_C(\mu, \sigma; c_1(\hat{\bfu}), \ldots, c_n(\hat{\bfu})).$$ 
\STATE $\bullet$ Fix $(\hat{\mu}, \hat{\sigma})$ and using fixed point iteration (i.e., (\ref{fixed:point:eq}) \& (\ref{cauchy:norm:eq})) find
$$\hat{\bfu} = \argmin_{\bfu} \ l_C(\hat{\mu}, \hat{\sigma}| c_1(\bfu), \ldots, c_n(\bfu)) - \lambda (||\bfu||_2^2-1)$$
\ENDWHILE
\STATE $\bullet$ Set the $j$-th Cauchy principal direction
$$\bfu_{j} = \hat{\bfu}.$$
\STATE $\bullet$ Remove the contribution from the dataset
\begin{eqnarray*}
\bfX = \bfX (\bfI_p - \bfu_{j}\bfu^T_{j}),%\bfX \bfP_{\bfu_j},
\end{eqnarray*}
\ENDFOR
\end{algorithmic}
\end{algorithm}




% The Cauchy-PCA procedure is defined in two steps analogously to the standard, or  Gaussian, procedure as follows:
% \begin{description}
%   \item[\textbf{Inner maximisation}] for given unit vector ${\bf u}$, maximise $l_{C}(\mu, \sigma| c_{1}({\bf u})\ldots c_{n}({\bf u}))$ over $\mu,$ $\sigma$ to obtain $\hat{\mu}({\bf u})$ and ${\hat{\sigma}}({\bf u})$;
%   \item[\textbf{Outer minimisation}] of $l_{C}(\hat{\mu}({\bf u}), {\hat{\sigma}}({\bf u}))$ over ${\bf u}$.
% \end{description}
% We know that for the Cauchy distribution the mean does not exist and variance is infinite. However, according to \citet{Copas:1975}, the two-parameter Cauchy likelihood function $l_{C}(\mu, \sigma)=\log f_{C}(x|\mu, \sigma)$ has a unique MLE $\hat{\boldsymbol{\theta}}=(\hat{\mu},\hat{\sigma})$ for ${\boldsymbol{\theta}}=(\mu, \sigma)$.

% \subsection{Algorithm for the first Cauchy PC}\label{1stCPC}
% In this section we will explain the procedure of producing all Cauchy PCs by first presenting a simple algorithm to extract the first Cauchy PC. The two loops required to calculate the Cauchy first PC are given below
% \begin{description}
% \item[\textbf{Inner loop:}] Maximising the two parameters Cauchy likelihood function\\
% \begin{equation}\label{maxCLL}
% n \log{\sigma} - n \log{\pi} - \sum_{i=1}^{n} \log \left\{\sigma^2+ (c_i-\mu)^{2}\right\},
% \end{equation}
% where $c_i= c_i({\bf u})= {\bf x}_i^T{\bf u}$, $i=1, \ldots, n$ and $\|{\bf u}\|=1.$
% According to \citet{Copas:1975}, although the mean of the Cauchy distribution does not exist and it has infinite variance, the Cauchy likelihood function $l_{C}(\mu, \sigma)=log f_{C}(x|\mu, \sigma)$ has a unique MLE $\hat{\boldsymbol{\theta}}=(\hat{\mu},\hat{\sigma})$ for ${\boldsymbol{\theta}}=(\mu, \sigma)$.
% \item[\textbf{Outer loop:}] Minimising Cauchy likelihood function over ${\bf u}$, in this step there is no guarantee of unique solution. The objective function should be minimised a number of times using random starting points.
% \end{description}

% % The following pseudo-code (Algorithm \ref{1CPC Algo.}) presents the basic steps to compute the first Cauchy PC:
% % %   1st Cauchy PC Algorithm    
% % \begin{algorithm}[H]
% % \caption{First Cauchy PC}
% % \label{1CPC Algo.}
% % {\vspace{3mm}}
% % \begin{algorithmic}
% % \STATE Step 1. {\hspace{2mm}} Choose an initial unit vector ${\bf u}$ and normalize it so that  $\|{\bf u}\|=1.$\\
% % \STATE Step 2. {\hspace{2mm}} Minimise $F({\bf u})$ defined by\\
% % \begin{equation*}
% % F({\bf u})= \mathop{\max}_{\mu, \sigma} l_C(\mu, \sigma; c_1, \ldots, c_n),
% % \end{equation*}
% % {\hspace{17mm}} where $l_{C}(\mu, \sigma)$ is given in (\ref{maxCLL}).
% % \STATE Step 3. {\hspace{2mm}} Set ${\bf PC}_{1}= \mathop{\arg \min_{\boldsymbol {\bf u}}} F({\bf u})$.
% % \vspace{3mm}
% % \end{algorithmic}
% % \end{algorithm}

% By using Lagrange multipliers we will implement Step 3 in \textbf{Algorithm 1}
% \begin{eqnarray} \label{lagrange}
% \min_{\bf u} \left \lbrace F\left( {\bf u} \right) = n\log{\sigma} - \sum_{i=1}^{n} \log \left[\sigma^2+\left({\bf x}_i^T{\bf u}-\mu \right)^2\right] - \lambda ({\bf u}^T{\bf u} - 1) \right \rbrace.
% \end{eqnarray}
% Equating the derivative of (\ref{lagrange}) to 0 
% \begin{eqnarray*} 
% \frac{\partial}{\partial {\bf u}} F\left({\bf u}\right) = -2\sum_{i=1}^{n}\frac{\left({\bf x}_i^T{\bf u}-\mu\right){\bf x}_i^T}{\sigma^2 + \left({\bf x}_i^T{\bf u}-\mu \right)^2} - 2 \lambda {\bf u}^T = {\bf 0}
% \end{eqnarray*}
% we obtain the fixed points iteration solution for $\bf u$ 
% \begin{eqnarray} \label{parallel}
% {\bf u}^T = \frac{1}{\lambda}\sum_{i=1}^{n}\frac{({\bf x}_i^T{\bf u}-\mu){\bf x}_i^T}{\sigma^2 + \left({\bf x}_i^T{\bf u}-\mu \right)^2}.
% \end{eqnarray}
% We begin with a random unit vector and maximise the Cauchy log-likelihood (\ref{maxCLL}), update the unit vector using (\ref{parallel}) and maximise the Cauchy log-likelihood (\ref{maxCLL}), repeatedly. The process stops when the reduction in the Cauchy log-likelihood (\ref{maxCLL}) is negligible. 

% \subsection{Algorithm for the remaining Cauchy PCs }\label{RemCPC's}
% There are two methods of obtaining the $k$-th Cauchy PC:
% \begin{enumerate}
%   \item  Use multivariate optimization, making use of some results in \citet{Kent:1994}.
%   \item  Repeat Algorithm \ref{1CPC Algo.}, that is used to find first Cauchy PC, in the data after removing or subtracting the effect of the first $k - 1$ PCs from the sample data $\bf X$.
% \end{enumerate}
% The first method which is using \citet{Kent:1994} approach, generalized the result given by \citet{Copas:1975} in two different senses: (i) it generalizes the univariate Cauchy result to the multivariate Cauchy distribution, (ii) it generalizes from the Cauchy distribution to the t-distribution. They show that the maximum likelihood estimates of location and scatter parameters for a multivariate t-distribution in $p$ dimensions with $\nu\geq 1$ degrees of freedom could be identified with the maximum likelihood estimates for a scatter-only estimation problem from a $(p+1)-$ dimensional multivariate t-distribution degrees of freedom $\nu-1 >0$. The estimation of the parameters of a  t-distribution were computed iteratively using the EM algorithm that is given in the same paper.

% To obtain the $k$-th PC (${\bf u}_k$) we are following the second method, that is straightforward to apply, by applying Algorithm \ref{1CPC Algo.} on the data ${\bf X}_k$ 
% \begin{eqnarray*}
% {\bf X}_k = {\bf X} - \sum_{j=1}^{k-1}{\bf X}{\bf u}_j{\bf u}^T_j,
% \end{eqnarray*}
% where ${\bf u}_j$ is the $j$-th PC. 

\subsection{Robustness of the Leading Cauchy Principal Direction}
Let $\bftheta = \left(\mu,\sigma\right)^T$ be the parameter vector of the Cauchy distribution and consider the infinite-sample normalized Cauchy log-likelihood function
\begin{equation}
l(\bfu|\bftheta) = \int_{\bfx\in\mathbb R^p} g(c(\bfu),\bftheta)\, dF(\bfx),
\end{equation}
where $g(c,\bftheta) = \log(\sigma/\pi) - \log( \sigma^2+(c-\mu)^2)$ and $c(\bfu)=\bfx^T\bfu$. We will estimate the influence function for the leading Cauchy principal direction
\begin{equation}
\hat{\bfu} = \argmin_{||\bfu||_2=1} \ l(\bfu|\bftheta_F(\bfu)),
\end{equation}
where $\bftheta_F(\bfu)=\argmax_{\bftheta} l(\bfx^T\bfu|\bftheta)$ is the optimal Cauchy parameters for a given direction $\bfu$.

Since $\hat{\bfu}$ is restricted to be a unit vector, the standard condition for the minimum, i.e., $\left.\frac{\partial}{\partial\bfu} l(\bfu|\bftheta_F(\bfu))\right\vert_{\bfu=\hat{\bfu}} = \bfzero$ is not valid. The proper condition is defined by
\begin{equation}
\bfP_{\hat{\bfu}} \left.\frac{\partial}{\partial\bfu} l(\bfu|\bftheta_F(\bfu))\right\vert_{\bfu=\hat{\bfu}} = \bfzero ,
\end{equation}
where $\bfP_{\bfu}$ is the projection matrix given by $\bfP_{\bfu}=\bfI_p-\bfu\bfu^T$.

\begin{Remark}
An equivalent condition is to satisfy  $\bfh^T \left.\frac{\partial}{\partial\bfu} l(\bfu|\bftheta_F(\bfu))\right\vert_{\bfu=\hat{\bfu}} = \bfzero$ for all $\bfh$ such that $\bfh^T\hat{\bfu}=0$ and $||\bfh||_2=1$. Both derived conditions are essentially a consequence of the Lagrangian formulation of the constraint optimization problem. Indeed, the Lagrange condition implies that at the minimum the direction of the objective function's derivative should be parallel to the direction of the constraint's derivative which translates to $\left.\frac{\partial}{\partial\bfu} l(\bfu|\bftheta_F(\bfu))\right\vert_{\bfu=\hat{\bfu}} = \lambda \hat{\bfu}$ where $\lambda\neq 0$ is the Lagrange multiplier.
\end{Remark}

Let $\bar{g}(\bfx;\bfu) = \left.g(\bfx^T\bfu|\theta)\right\vert_{\theta=\theta_F(\bfu)}$ be the likelihood function computed at $\theta=\theta_F(\bfu)$ and let denote its partial derivatives as 
\[
\bar{g}_c(\bfx;\bfu) = \left.\frac{\partial}{\partial c} g(\bfx^T\bfu|\theta)\right\vert_{\theta=\theta_F(\bfu)}
\]
and  
\[
\bar{g}_{\bftheta}(\bfx;\bfu) = \left.\frac{\partial}{\partial \bftheta} g(\bfx^T\bfu|\theta)\right\vert_{\theta=\theta_F(\bfu)}. 
\]
Similarly, $\bar{g}_{cc}$, $\bar{g}_{c\theta}$ and $\bar{g}_{\theta\theta}$ denote the second order derivatives.
The following proposition establishes the expression for the influence function of the leading Cauchy principal direction, $\hat{\bfu}$. 

\begin{proposition}\label{influence:func:cauchy:pca}
Under the assumption of ${\bfI}_F(\hat{\bfu})$ and $\bfA$ being invertible matrices, the influence function of $\hat{\bfu}$ is
\begin{equation}
IF_{\hat{\bfu}} (\bfz, F) = \bfA^{-1} \bfb ,
\end{equation}
where
$$
\begin{aligned}
\bfA &= \bfI_p \int_{\mathbb R^p} \bar{g}_{c\bftheta}(\bfx;\hat{\bfu})  \bfx^T\hat{\bfu} dF(\bfx) 
- \bfP_{\hat{\bfu}} \int_{\mathbb R^p} \bar{g}_{cc}(\bfx;\hat{\bfu}) \bfx^T\bfx dF(\bfx) \bfP_{\hat{\bfu}} \\
&+ \bfP_{\hat{\bfu}} \int_{\mathbb R^p}  \bfx \bar{g}_{c\bftheta}(\bfx;\hat{\bfu}) dF(\bfx)  \, {\bfI}_F(\hat{\bfu})^{-1} \, 
\int_{\mathbb R^p}  \bar{g}_{\bftheta c}(\bfx;\hat{\bfu}) \bfx^T dF(\bfx) \bfP_{\hat{\bfu}}
\end{aligned}
$$
and
$$
\bfb = \bfb(z) = \bar{g}_c(\bfz, \hat{\bfu}) \bfz + \int_{\mathbb R^p} \bfx \bar{g}_{c\bftheta}(\bfx;\hat{\bfu}) dF(\bfx) \, 
{\bfI}_F(\hat{\bfu})^{-1} \,  \bar{g}_{\bftheta}(\bfz;\hat{\bfu}),
$$
while
$$
{\bfI}_F(\hat{\bfu}) = \int_{\mathbb R^p} \bar{g}_{\bftheta\bftheta}(\bfx;\hat{\bfu}) dF(\bfx)
$$
is the expected Fisher information matrix under $F$ for the parameters of the Cauchy distribution computed at $\hat{\bfu}$.
\end{proposition}
\begin{proof}
The proof consists of several straightforward series expansions and implicit function calculations. The complete proof is given in Appendix \ref{robust:cauchy:proof}.
\end{proof}

% \subsection{Robustness of the Cauchy Principal Direction ($\hat{\bfu}$)}
The following boundedness result for the influence function states the conditions under which Cauchy PCA is robust. 

\begin{corollary} \label{boundness} Let the assumptions of the proposition hold.
If $\bfz\not\perp\hat{\bfu}$ or if $\bfz\perp\hat{\bfu}=0$ but $\mu_F(\hat{\bfu})=0$ then the influence function for $\hat{\bfu}$ is bounded.
\end{corollary}

\begin{proof}
First, observe that matrix $\bfA$ does not depend on $\bfz$. It is only $\bfb$ that depends on $\bfz$ and our goal is to prove that $\bfb$ is bounded with respect to $\bfz$. Second, we have to compute the partial derivatives $\bar{g}_c(\bfz; \hat{\bfu})$ and $\bar{g}_{\bftheta}(\bfz; \hat{\bfu})$. Straightforward calculations lead to
$$
\bar{g}_c(\bfz; \hat{\bfu}) = - \frac{2(\bfz^T\hat{\bfu}-\mu_F(\hat{\bfu}))}{\sigma_F^2(\hat{\bfu})+(\bfz^T\hat{\bfu}-\mu_F(\hat{\bfu}))^2}
$$
$$
\bar{g}_\mu(\bfz; \hat{\bfu}) = \frac{2(\bfz^T\hat{\bfu}-\mu_F(\hat{\bfu}))}{\sigma_F^2(\hat{\bfu})+(\bfz^T\hat{\bfu}-\mu_F(\hat{\bfu}))^2}
$$
and
$$
\bar{g}_\sigma(\bfz; \hat{\bfu}) = \frac{1}{\sigma_F(\hat{\bfu})} - \frac{2\sigma_F(\hat{\bfu})}{\sigma_F^2(\hat{\bfu})+(\bfz^T\hat{\bfu}-\mu_F(\hat{\bfu}))^2}.
$$

Let us now define an arbitrary scaling of the outlier $\bfz\rightarrow\alpha\bfz$ and prove boundedness of $\bfb$ as we send $\alpha\to\infty$. We consider the first case where $\bfz\not\perp\hat{\bfu}$. It holds that $\lim_{\alpha\to\infty} \bar{g}_c(\alpha\bfz; \hat{\bfu})\alpha\bfz = -(\bfz^T\hat{\bfu})^{-1}\bfz$,  $\lim_{\alpha\to\infty} \bar{g}_\mu(\alpha\bfz; \hat{\bfu}) = 0$ and $\lim_{\alpha\to\infty} \bar{g}_\sigma(\alpha\bfz; \hat{\bfu}) = \frac{1}{\sigma_F(\hat{\bfu})}$ therefore $\bfb$ is bounded with respect to $\alpha$.

For the second case, we have 
\[
\lim_{\alpha\to\infty} \bar{g}_c(\alpha\bfz; \hat{\bfu})\alpha\bfz = \lim_{\alpha\to\infty} \frac{2\mu_F(\hat{\bfu})}{\sigma_F^2(\hat{\bfu})+\mu_F(\hat{\bfu})^2} \alpha\bfz = 0,
\]
\[
\lim_{\alpha\to\infty} \bar{g}_\mu(\alpha\bfz; \hat{\bfu}) = \frac{2\mu_F(\hat{\bfu})}{\sigma_F^2(\hat{\bfu})+\mu_F(\hat{\bfu})^2} = 0
\]
and
\[
\lim_{\alpha\to\infty} \bar{g}_\sigma(\alpha\bfz; \hat{\bfu}) = 
\frac{1}{\sigma_F(\hat{\bfu})} - \frac{2\sigma_F(\hat{\bfu})}{\sigma_F^2(\hat{\bfu})+\mu_F(\hat{\bfu})^2}
= -\frac{1}{\sigma_F(\hat{\bfu})}
\]
since $\mu_F(\hat{\bfu})=0$
by assumption. Thus $\bfb$ is bounded with respect to $\alpha$ for the second case, too.
\end{proof}

The only case not covered by the corollary is when $\bfz^T\hat{\bfu}=0$ and $\mu(\hat{\bfu})\neq 0$. Our experiments presented in the following section show that outliers that are orthogonal to the Cauchy principal direction do sometimes influence the estimation of the Cauchy principal direction yet not significantly.

\subsection{Several Cauchy principal components}
 We briefly mention possibilities for estimating several Cauchy principal components.  There are two obvious approaches: one approach, the sequential approach, is to repeat the algorithm described above on the subspace orthogonal to $\hat{\bfu}=\hat{\bfu}_1$ to obtain $\hat{\bfu}_2$, the second Cauchy principal component, where $\hat{\bfu}_1$ is the first Cauchy principal component; then  repeat the procedure on the subspace orthogonal to $\hat{\bfu}_1$ and $\hat{\bfu}_2$ to obtain $\hat{\bfu}_3$; and so on.  A second approach, the simultaneous approach, is to decide in advance how many principal components we wish to determine, $p$ say, and then use a $p$-dimensional multivariate Cauchy likelihood, which has $p+ p(p+1)/2$ free parameters, to obtain $\hat{\bfu}_1, \ldots , \hat{\bfu}_p$.  It turns out that these two approaches lead to equivalent results in classical (Gaussian) PCA but when a Cauchy likelihood is used the two approaches produce different sets of principal components.  Our current thinking is this: the sequential approach is easier to implement (essentially the same software can be used at each step) and it is faster.  However, the simultaneous approach could potentially be preferable if we know in advance how many principal components we wish to estimate. Further investigation is required.

% Experiments where we remove the "random" (due to finite sampling) term and assess only the "systematic" (due to the outliers) error.
% Can we understand why are we better in the cases we are? And, the opposite.


\section{Numerical Results}
\label{Comp.Algo}

\subsection{Simulation studies}
In this section we will empirically validate our proposed methodology, via simulation studies. We searched for R packages that offer robust PCA in the $n<<p$ case and came up with \textit{FastHCS} \citep{fasthcs2018}, \textit{rrcovHD} \citep{rrcovhd2016}, \textit{rpca} \citep{rpca2017} and \textit{pcaPP} \citep{pcapp2018}. Out of them, \textit{pcaPP} (Projection Pursuit PCA) is the only one which does not require hyper-parameter tuning, e.g. selection of the LASSO penalty $\lambda$ or choice of the percentage of observations used to estimate a robust covariance matrix. 

\subsubsection{Setup of the simulations}
Initially,  we created a $p \times p$ (orthonormal) basis $\bf B$ by using QR decomposition on some randomly generated data. We then generated eigenvalues $\lambda_i \sim Exp(0.4)$, where $i=1,\ldots,p$ and hence we obtained the covariance matrix $\pmb{\Sigma} = {\bf B}\pmb{\Lambda}{\bf B}^T$, where $\pmb{\Lambda} =\text{diag}(\lambda_i)$. The first column of $\bf B$ served as the first ``clean'' eigenvector, and was the benchmark in our comparative evaluations. Following this step, we simulated $n$ random vectors ${\bf X} \sim N_p\left({\bf 0}, \pmb{\Sigma} \right)$ and in order to check the robustness of the results to the center of the data, all observations were shifted right by adding $50$ everywhere. A number of outliers equal to 2$\%$ of the sample size were introduced. These outliers were $\bar{\bf x}+e^{\kappa}{\bf z} \in {\mathbb{R}}^{p}$, where $\bar{\bf x}$ is the sample mean vector, ${\bf z}$ are unit vector(s) and $e^{\kappa}$ a real number denoting their norm, where $\kappa$ varied from $3$ up to $8$ increasing with a step size equal to $1$ and the angle between the outliers ${\bf z}$ and the first ``clean'' eigenvector spanned from $0^{\circ}$ up to $90^{\circ}$. In all cases, we subtracted the spatial median or the column-wise median\footnote{The results are pretty similar for either type of median and we here show the results of he column-wise median.} and scaled them by the mean absolute deviation.

At each case, we computed the first Cauchy-PCA eigenvector and the first PP-PCA eigenvector. The performance metric is the angle (in degrees) between the first robust (based on Cauchy or PP-PCA) eigenvector and the first "clean" eigenvector computed using the classical PCA. All experiments were repeated $100$ times and the results were averaged. 

\subsubsection{Comparative results}

Tables \ref{tab100_500}-\ref{tab500_1000} present the performance of the first Cauchy-PCA eigenvector and of the first PP-PCA eigenvector for a variety of norms of the outlier, with different angles ($\phi$) between the outlier and the leading true eigenvector, for the $n<p$ case. 

The case of $n<p$ was selected as statistical inference in this case is more challenging than the $p<n$  case\footnote{In this paper we focus on high-dimensional simulations and real-date examples ($p>n)$  but in results not presented in the paper we found that Cauchy PCA is also very competitive and performs strongly in low dimensional settings ($p<n$).}. Additionally, this case is also ordinarily met in the field of bioinformatics were the -omics data count tens of thousands of variables (genes, single nucleotide polymorphisms, etc.) but only tens or at most hundreds of observations. 

As observed in Tables \ref{tab100_500}-\ref{tab500_1000}, the average angular difference between the Cauchy and the PP PCA ranges from $20^{\circ}$ up to more than $50^{\circ}$, which is evidently quite substantial, providing evidence that Cauchy PCA has performed in a superior manner to the projection pursuit method of Croux et al. (2007, 2013). In particular, the tables demonstrate that Cauchy PCA is less error prone than its competitor but, as is seen in Table \ref{tab500_1000}, the error decreases for both methods with increasing sample size. Further, the mean angular difference between the two methods increases as the angle $\phi$ increases. For instance, in Table \ref{tab100_500}, when $k=8$ and $\phi=0^{\circ}$ the difference between the two methods is $20^{\circ}$, whereas when $\phi=90^{\circ}$ the difference increases to $48^{\circ}$. Further, the error is not highly affected by the angle $\phi$, or the norm of the outliers. It can be seen  that in Table \ref{tab100_1000} and Table \ref{tab500_1000} in the special case of $\phi=90^{\circ}$, the error increases for the Cauchy PCA by $2^{\circ}-3^{\circ}$, thus corroborating the result of Corollary \ref{boundness}. However, this effect, as in Table \ref{tab100_500}, is rather small, though noticeable. 

\begin{table}
\caption{Mean angular difference between the robust eigenvectors computed in the contaminated data and the sample eigenvector computed in the clean data when $n=100$ and $p=500$. The norm of the outliers is $e^{k}$ and their angle with the true clean eigenvector is denoted by $\phi$.}
\label{tab100_500}
\begin{tabular}{ll|rrrrrrr}
\hline
Angle  &  Method  & k=-Inf & k=3 & k=4 & k=5 & k=6 & k=7 & k=8 \\ \hline
$\phi=0^{\circ}$  & Cauchy & 31.17 & 29.79 & 29.54 & 28.83 & 28.86 & 29.24 & 28.78 \\
                  & PP     & 82.45 & 49.91 & 48.84 & 48.22 & 49.08 & 49.61 & 48.14 \\ \hline
$\phi=30^{\circ}$ & Cauchy & 31.44 & 29.24 & 29.13 & 28.60 & 28.89 & 29.34 & 29.65 \\
                  & PP     & 82.45 & 65.28 & 65.34 & 63.42 & 62.96 & 66.63 & 65.43 \\ \hline
$\phi=60^{\circ}$ & Cauchy & 31.49 & 29.86 & 29.07 & 29.04 & 29.55 & 29.70 & 29.09 \\ 
                  & PP     & 82.11 & 81.11 & 82.55 & 82.63 & 82.12 & 82.49 & 82.03 \\ \hline
$\phi=90^{\circ}$ & Cauchy & 32.32 & 31.67 & 33.00 & 33.13 & 32.86 & 33.19 & 33.06 \\ 
                  & PP     & 82.38 & 82.06 & 81.69 & 82.12 & 81.73 & 81.74 & 81.88 \\ \hline 
\end{tabular}
\end{table}

\begin{table}
\caption{Mean angular difference between the robust eigenvectors computed in the contaminated data and the sample eigenvector computed in the clean data when $n=100$ and $p=1000$. The norm of the outliers is $e^{k}$ and their angle with the true clean eigenvector is denoted by $\phi$.}
\label{tab100_1000}
\begin{tabular}{ll|rrrrrrr}
\hline
Angle  &  Method  & k=-Inf & k=3 & k=4 & k=5 & k=6 & k=7 & k=8 \\ \hline
$\phi=0^{\circ}$  & Cauchy & 36.53 & 33.12 & 33.60 & 33.69 & 32.62 & 32.51 & 33.16 \\
                  & PP     & 83.06 & 80.36 & 80.17 & 81.87 & 80.50 & 80.76 & 80.16 \\ \hline
$\phi=30^{\circ}$ & Cauchy & 36.55 & 34.72 & 33.91 & 33.09 & 33.11 & 33.16 & 32.79 \\ 
                  & PP     & 83.07 & 82.36 & 82.76 & 82.65 & 83.07 & 82.93 & 83.12 \\ \hline
$\phi=60^{\circ}$ & Cauchy & 36.42 & 34.46 & 33.96 & 33.61 & 34.41 & 33.07 & 33.47 \\
                  & PP     & 83.78 & 82.86 & 82.71 & 84.05 & 83.46 & 82.71 & 82.78 \\ \hline
$\phi=90^{\circ}$ & Cauchy & 36.50 & 36.12 & 36.81 & 37.18 & 39.34 & 39.11 & 38.51 \\
                  & PP     & 83.63 & 83.73 & 83.69 & 83.65 & 84.03 & 83.66 & 83.00 \\ \hline
\end{tabular}
\end{table}

\begin{table}
\caption{Mean angular difference between the robust eigenvectors computed in the contaminated data and the sample eigenvector computed in the clean data when $n=500$ and $p=1000$. The norm of the outliers is $e^{k}$ and their angle with the true clean eigenvector is denoted by $\phi$.}
\label{tab500_1000}
\begin{tabular}{ll|rrrrrrr}
\hline
Angle  &  Method  & k=-Inf & k=3 & k=4 & k=5 & k=6 & k=7 & k=8 \\ \hline
$\phi=0^{\circ}$  & Cauchy & 19.95 & 18.60 & 18.46 & 18.35 & 18.24 & 18.20 & 17.93 \\
                  & PP     & 68.76 & 26.08 & 24.93 & 24.91 & 24.83 & 24.73 & 24.72 \\ \hline 
$\phi=30^{\circ}$ & Cauchy & 19.43 & 18.30 & 18.39 & 18.22 & 18.16 & 18.01 & 18.13 \\ 
                  & PP     & 68.98 & 39.72 & 38.88 & 38.44 & 38.20 & 38.15 & 38.14 \\ \hline
$\phi=60^{\circ}$ & Cauchy & 19.76 & 18.60 & 18.12 & 18.20 & 18.40 & 18.19 & 18.01 \\
                  & PP     & 69.10 & 64.10 & 63.12 & 62.89 & 62.91 & 62.82 & 62.77 \\ \hline
$\phi=90^{\circ}$ & Cauchy & 19.49 & 19.84 & 20.16 & 21.87 & 22.41 & 22.87 & 22.84 \\
                  & PP     & 68.99 & 68.62 & 68.59 & 68.70 & 68.45 & 68.73 & 68.43 \\ \hline
\end{tabular}
\end{table}

%We will first examine a small dimensional dataset (Anderson's Iris data) followed by three real gene expression high dimensional datasets.

%\subsubsection{A toy example with Anderson's Iris data} 
%Our first example comes from Anderson's Iris data containing only 4 dimensions. Suppose we have a sample of observation vectors ${\bf x}_{1}, \ldots, {\bf x}_{n} \in {\mathbb{R}}^{p}$. We introduce a new observation $\bar{\bf x}+a{\bf z} \in {\mathbb{R}}^{p}$, where ${\bf z}$ is a unit vector and $a$ an arbitrary real number. When $a$ is large, the new observation will be an outlier. Our purpose is to examine the behaviour of the standard and first Cauchy PC as $a$ becomes very large.

%We chose the underling sample ${\bf x}_{1}, \ldots, {\bf x}_{n}$ to be from Anderson's Iris data (see \citet{Mardia&Kent&Bibby:1979}). The Iris data is a data set contains 150 observations of three classes of iris plant, Setosa, Versicolour and Virginica, 50 instances in each of three classes. Each class of this data set is described by four features, sepal length, sepal width, petal length and petal width. 

%Figure \ref{iris} illustrates some of our results using a dataset of sample size $51$, $50$ from the Versicolour species, where we have also included an observation that is considered to be outlier. The angles ${\bf PC}^{T}{\bf z}$ of the first Classical, the first Cauchy and the first PP eigenvector at different values of the outlier. What we actually end up with when $a \rightarrow \infty$ is that the first standard $PC$ went parallel to the outlier proving that the first standard PC, unlike the first Cauchy PC, is not robust. When we applies the Cauchy-PCA to the 50 data points, without outliers, we ended up with very similar results to standard PCA. But when we introduce extreme outliers the Cauchy and the PP PCs will change slightly, whereas the classical (non-robust) PCs will completely be dominated by a sufficiently extreme outlier and converge to the unit vector ${\bf z}.$ 

%\begin{figure}[!ht]
%\centering
%\includegraphics[scale = 0.45]{iris_example.png} 
%\caption{The sample consists of Anderson's Iris data with 50 measurements on the  Versicolour species $({\bf x}_{1}^T, \ldots, {\bf x}_{50}^T)$ plus a 51st vector, an outlier, ${\bf z}= \bar{\bf x}+\alpha{\bf z}$, where $\bar{\bf x}$ is the sample mean vector and ${\bf z}=(0.1303324, -0.3694750, 0.3999995, -0.8285542)^{T}$ is a randomly generated unit vector. Plotted is the angle (in degrees) between the first Classical PC and $\bf z$, the angle between the first Cauchy PC and $\bf z$ and the angle between the first Projection Pursuit PC and $\bf z$, all for a range of values of $\alpha$. }
%\label{iris}
%\end{figure}

\subsection{High dimensional real datasets}
Two real gene expression datasets, GSE13159 and GSE31161\footnote{From a biological standpoint, the data have already been uniformly pre-processed, curated and automatically annotated.}, downloaded from the \href{dataome.mensxmachina.org}{Biodataome} platform \citep{lakiotaki2018}, were used in the experiments. The dimensions of the datasets were equal to $2,096 \times 54,630$ and $1035 \times 54,675$, respectively. We randomly selected $5,000$ variables and computed the outliers using the high dimensional Minimum Covariance Determinant (MCD) of \cite{ro2015}. In accordance with the simulations studies, we removed the $2\%$ of the most extreme outliers detected by MCD and computed the first classical PC (benchmark eigenvector), the first Cauchy-PCA eigenvector and the first PP-PCA eigenvector of the "clean" data. We then added those outliers and increased their norm by $e^k$, where $k=(0, 3, 4, \ldots, 8)$ and computed computed the first Cauchy-PCA eigenvector and the first PP-PCA eigenvector. In all cases, we subtracted the spatial median or the column-wise median and scaled them by the mean absolute deviation. The performance metric is the angle (in degrees) between the first robust (based on Cauchy or PP-PCA) eigenvector and the first true ``clean" eigenvectors and the time required by each method. This procedure was repeated  $200$ times and the average results are graphically displayed in Figures \ref{gse}(a)-(d). 

Broadly speaking the effect of the PP PCA does not seem to have been affected substantially by the centering method, i.e. subtraction of the spatial or the column-wise median. On the contrary, the Cauchy PCA is affected by the type of median employed to this end. Centering with the spatial median yields high error levels for all norms of the outliers, for both datasets, whereas centering with the column-wise median produces much lower error levels. On average, the difference in the error between Cauchy PCA and PP PCA is about $30^{\circ}$ for the GSE31159 dataset (Figure \ref{gse}(a)) and about $14^{\circ}$ for the GSE3161 dataset (Figure \ref{gse}(b)). However, the error of the Cauchy PCA increases and the stabilizes in the GSE31159 dataset whereas the error of the PP PCA is stable regardless of the norm of the outliers. A different conclusion is extracted in the GSE31161 where the error of either method decreases as the norm of the outliers increases, until it reaches a plateau. 

With regards to computational efficiency, the PP PCA is not affected by either centering method, whereas Cauchy PCA seems to be affected in the GSE31159 dataset but not in the GSE31161 dataset as seen in Figures \ref{gse}(c) and \ref{gse}(d). Cauchy PCA centered with the column-wise median is, on average, 5 times faster than PP PCA. 

%To study the convergence, we need to calculate the number of distinct and inconclusive solution for the value of the estimated local minimum of Cauchy likelihood function of the Cauchy first PC with different values of outliers, which illustrates in Table \ref{table3}. The first coordinate of the ordered pairs is the number of the codes which leads to convergent solution, while the second coordinate is the number of the codes which leads to non-convergent solution. This depends on the code that results as output when using the command \textit{nlm} in R. The code ranges from $1$ to $5$, $1$ and $2$ mean the iterate is probably solution, where the rest do not give any solution. 
%Table \ref{table3} provide an evidence that the optimisation procedure with randomly chosen starting point is working properly. The procedure convergence to a point in the vicinity of what appears to a global minimum, the procedure exist with a code $1$ or $2$, will indicating satisfactory convergence; while in all other cases the procedure exist with a code $3$ or $4$ or $5$, will indicating that convergence has not accorded.

%We simulate a sample of size 100 from multivariate normal distribution with mean ${\bf 0}$ and a diagonal covariance matrix $\Sigma=diag(1, 3, 5, 7).$ We found out that the Cauchy PCs pick out the axes of symmetry, see \ref{table4}.

%\begin{table}[H] \small
%\begin{center}
%\begin{tabular}{|c||c||c||c|}
%\hline \hline
  % after \\: \hline or \cline{col1-col2} \cline{col3-col4} ...
%  & \multicolumn{3}{|c|} {$\cos^{-1} |{\bf PC_{1}}^{T}{\bf z}|$} \\ \hline
%   $a$   & Standard PC  &  Cauchy PC & Projection Pursuit PC  \\  \hline \hline
%   0.1  &  87.24  &  87.90  &  87.42  \\   \hline
%   0.5  &  87.21  &  87.98  &  87.40  \\   \hline
%   1    &  87.11  &  87.89  &  88.53  \\   \hline
%   5    &  21.92  &  77.94  &  88.49  \\   \hline
%   10   &  1.12   &  77.29  &  85.54  \\   \hline
%   50   &  0.04   &  77.15  &  85.54  \\   \hline
%   1000 &  0.00   &  77.17  &  85.54   \\  \hline \hline
%\end{tabular}
%\caption{\small The angle (in degrees) between the first Gaussian PC and $\bf z$, the angle between the first Cauchy PC and $\bf z$ and the angle between the firs Projection Pursuit PC and $\bf z$, where ${\bf z}=(0.1303324, -0.3694750, 0.3999995, -0.8285542)^{T}$ is a randomly generated unit vector. The sample consists of Iris Versicolour data ${\bf x}_{1}, \ldots, {\bf x}_{50}$ plus an outlier ${\bf x}_{51}= \bar{\bf x}+a{\bf z}$, where $\bar{\bf x}= n^{-1}\sum_{i=1}^{50}{\bf x}_{i},$ and $a$ is as given in the table.}
%\label{table1}
%\end{center}
%\end{table}


\begin{figure}[!ht]
\centering
\begin{tabular}{cc}
GSE31159  &  GSE31161    \\
\includegraphics[scale = 0.4, trim = 30 0 0 0]{gse13159.png} &
\includegraphics[scale = 0.4, trim = 30 0 0 0]{gse31161.png} \\
(a)    &  (b)      \\ 
\includegraphics[scale = 0.4, trim = 30 0 0 0]{gse13159_time.png} &
\includegraphics[scale = 0.4, trim = 30 0 0 0]{gse31161_time.png} \\
(c)    &  (d)  
\end{tabular}
\caption{The first row presents the angle between the first Cauchy PC of the "contaminated" data and the 1st leading eigenvector of the "clean" data and the angle between the first Projection Pursuit PC of the "contaminated" data and the 1st leading eigenvector of the "clean" data for increasing norms of the outliers. The second row contains the time in seconds.}
\label{gse}
\end{figure}

\section{Conclusion}\label{concl.}
The starting point for this paper is the observation that classical PCA can be formulated purely in terms of operations on a Gaussian likelihood. Although this observation is not new, the specifics of this formulation of classical PCA do not appear to be as widely known as might be expected. The novel idea underlying this paper is to formulate a version of PCA in which a Cauchy likelihood is used instead of a Gaussian likelihood, leading to what we call Cauchy PCA. Study of the resulting influence functions shows that Cauchy PCA has very good robustness properties. Moreover, we have provided an implementation of Cauchy PCA which runs quickly and reliably. Numerous simulation and real-data  examples, mainly in high-dimensional settings, show that Cauchy PCA typically out-performs alternative robust versions of PCA whose implementation is in the public domain. 

\clearpage
\section*{Appendix}
\setcounter{section}{0}
\renewcommand{\thesubsection}{A\arabic{subsection}}

\subsection{Proof of Proposition 2.1}\label{NonRob:PCA:proof}
\begin{proof}
The perturbed distribution $(1-\epsilon)F(\bfx) + \epsilon\Delta_\bfz(\bfx)$ has perturbed mean value
\begin{equation*}
\bfmu_\epsilon = \bfmu + \epsilon (\bfz-\bfmu)
\end{equation*}
and perturbed covariance matrix
\begin{equation*}
\bfSigma_\epsilon = \bfSigma + \epsilon ((\bfz-\bfmu)(\bfz-\bfmu)^T-\bfSigma) + \epsilon^2 (\bfz-\bfmu)(\bfz-\bfmu)^T
\end{equation*}

Denoting by $\lambda_\epsilon$ the leading eigenvalue of $\bfSigma_\epsilon$ and by $\bfu_\epsilon$ the corresponding eigenvector, it holds that
\begin{equation}\label{perturbed:eigen:eq}
\bfSigma_\epsilon\bfu_\epsilon = \lambda_\epsilon \bfu_\epsilon \ \ \text{and} \ \  \bfu_\epsilon^{T}\bfu_\epsilon = 1 \ .
\end{equation}
Next, we expand the perturbed eigenvector and eigenvalue around the unperturbed ones as follows:
\begin{equation*}
\bfu_\epsilon = \bfu_0 + \epsilon \bfu_1 + O(\epsilon^2)
\end{equation*}
and
\begin{equation*}
\lambda_\epsilon = \lambda_0 + \epsilon \lambda_1 + O(\epsilon^2)
\end{equation*}
with
\begin{equation*}
\bfSigma\bfu_0 = \lambda_0  \ \ \text{and} \ \  \bfu_0^{T}\bfu_0 = 1 \ .
\end{equation*}

Substituting the formulas into (\ref{perturbed:eigen:eq}), and equating the zero-th and first order we get
\begin{equation*}
\bfSigma\bfu_0 = \lambda_0 \bfu_0 \ \ \text{and} \ \  \bfu_0^{T}\bfu_0 = 1 \ .
\end{equation*}
and
\begin{equation}\label{1st:order:eq}
((\bfz-\bfmu)(\bfz-\bfmu)^T-\bfSigma)\bfu_0 + \bfSigma\bfu_1 = \lambda_0\bfu_1 + \lambda_1\bfu_0
\end{equation}
and
\begin{equation*}
\bfu_0^{T}\bfu_1 = 0 \ .
\end{equation*}

Multiplying (\ref{1st:order:eq}) from the left with $\bfu_0^T$, we get
\begin{equation*}
\lambda_1 = \bfu_0^T ((\bfz-\bfmu)(\bfz-\bfmu)^T-\bfSigma)\bfu_0 + \bfu_0^T\bfSigma\bfu_1
= (\bfu_0^T (\bfz-\bfmu))^2 - \lambda_0
\end{equation*}



For $\bfu_1$, we rearrange (\ref{1st:order:eq}) to
\begin{equation*}
(\bfSigma-\lambda_0\bfI)\bfu_1 = \lambda_1\bfu_0 - ((\bfz-\bfmu)(\bfz-\bfmu)^T-\bfSigma)\bfu_0
\end{equation*}
and then multiply from the left with the pseudo-inverse of $\bfSigma-\lambda_0\bfI$ to obtain
\begin{equation*}
(\bfSigma-\lambda_0\bfI)^+(\bfSigma-\lambda_0\bfI)\bfu_1 = 
\lambda_1(\bfSigma-\lambda_0\bfI)^+\bfu_0 - (\bfSigma-\lambda_0\bfI)^+((\bfz-\bfmu)(\bfz-\bfmu)^T-\bfSigma)\bfu_0
\end{equation*}
Using the properties (\cite{Mardia&Kent&Bibby:1979}):  $(\bfSigma-\lambda_0\bfI)^+(\bfSigma-\lambda_0\bfI) = \bfI - \bfu_0\bfu_0^T$ and $(\bfSigma-\lambda_0\bfI)^+\bfu_0 = \bfzero$, we obtain
\begin{equation*}
\begin{aligned}
&\bfu_1 - \bfu_0\bfu_0^T\bfu_1 = (\bfSigma-\lambda_0\bfI)^+(\bfz-\bfmu)(\bfz-\bfmu)^T\bfu_0 - (\bfSigma-\lambda_0\bfI)^+ \lambda_0 \bfu_0 \\
\Rightarrow & \bfu_1 = ((\bfz-\bfmu)^T\bfu_0)(\bfSigma-\lambda_0\bfI)^+(\bfz-\bfmu)
\end{aligned}
\end{equation*}
and the proof is completed.
\end{proof}




\begin{comment}

We prove that standard PCA is not robust by showing that the influence function is unbounded. Suppose that $\Sigma_{0}$ is the covariance matrix of a population with distribution function $F_{0}$, i.e.
\begin{equation}\label{Pop.Cov.}
    {\boldsymbol{\Sigma}}_{0}= \int {\bf x}{\bf x}^{T}dF_{0}({\bf x})-\left(\int {\bf x}dF_{0}({\bf x})\right)\left(\int {\bf x}dF_{0}({\bf x})\right)^{T};
\end{equation}
denote the corresponding mean by:
\begin{equation}\label{Pop.Mean}
    {\boldsymbol{\alpha}}_{0}=\int {\bf x}dF_{0}({\bf x}).
\end{equation}
Let us consider the distribution function $F_{{\bf z},\epsilon}$. The corresponding mean and covariance matrix will be defined as following, respectively:
\begin{equation}\label{Mix.Mean}
    {\boldsymbol{\alpha_{\epsilon}}}= \int {\bf x}[(1-\epsilon)dF_{0}({\bf x})+\epsilon d \delta_{\bf z}(\bf x)]=(1-\epsilon){\boldsymbol{\alpha}}_{0} + \epsilon {\bf z},
\end{equation}
\begin{eqnarray}
% \nonumber to remove numbering (before each equation)
  {\boldsymbol{\Sigma_{\epsilon}}} &=& (1-\epsilon)({\boldsymbol{\Sigma}}_{0}+{\boldsymbol{\alpha}}_{0}{\boldsymbol{\alpha}}_{0}^{T})+ \epsilon {\bf z}{\bf z}^{T}-((1-\epsilon){\boldsymbol{\alpha}}_{0} + \epsilon {\bf z})((1-\epsilon){\boldsymbol{\alpha}}_{0} + \epsilon {\bf z})^{T} \nonumber \\
   &=& {\boldsymbol{\Sigma}}_{0}+\epsilon[({\bf z}-{\boldsymbol{\alpha}}_{0})({\bf z}-{\boldsymbol{\alpha}}_{0})^{T}-{\boldsymbol{\Sigma}}_{0}]+\epsilon^{2}({\bf z}-{\boldsymbol{\alpha}}_{0})({\bf z}-{\boldsymbol{\alpha}}_{0})^{T}.
\end{eqnarray}
Assume that the leading eigenvalue of ${\boldsymbol{\Sigma}}_{0}$ has multiplicity 1. then the leading eigenvalue and the leading eigenvector have the following expansions, respectively (\citet{Amaral:2007}):
\begin{equation}\label{mu-eps}
    {\boldsymbol{\mu}}_{[\epsilon]}={\boldsymbol{\mu}}_{0}+\epsilon {\boldsymbol{\mu}}_{1}+\epsilon^{2} {\boldsymbol{\mu}}_{2}+\ldots,
    % ={\boldsymbol{\mu}}_{0}+\epsilon {\boldsymbol{\mu}}_{1}+O(\epsilon^{2}),
\end{equation}
\begin{equation}\label{lamda-eps}
    {\lambda}_{[\epsilon]}=\lambda_{0}+\epsilon \lambda_{1}+\epsilon^{2} \lambda_{2}+\ldots,
    %= \lambda_{0}+\epsilon \lambda_{1}+O(\epsilon^{2});
\end{equation}
where the following identities hold
\begin{equation}\label{sigma-eps}
   {\boldsymbol{\Sigma_{\epsilon}}}{\boldsymbol{\mu}}_{[\epsilon]}={\lambda}_{[\epsilon]}{\boldsymbol{\mu}}_{[\epsilon]} \ \ \text{and} \ \  {\boldsymbol{\mu}}_{[\epsilon]}^{T}{\boldsymbol{\mu}}_{[\epsilon]}=1.
\end{equation}
This expansion will enable us to determine the influence function for parameters of interest.
Substituting (\ref{mu-eps}) and (\ref{lamda-eps}) into (\ref{sigma-eps}) yields
\begin{eqnarray*}
% \nonumber to remove numbering (before each equation)
  &\left\{{\boldsymbol{\Sigma}}_{0}+\epsilon[({\bf z}-{\boldsymbol{\alpha}}_{0})({\bf z}-{\boldsymbol{\alpha}}_{0})^{T}-{\boldsymbol{\Sigma}}_{0}]+\epsilon^{2}({\bf z}-{\boldsymbol{\alpha}}_{0})({\bf z}-{\boldsymbol{\alpha}}_{0})^{T}\right\} \left\{{\boldsymbol{\mu}}_{0}+\epsilon {\boldsymbol{\mu}}_{1}+\epsilon^{2} {\boldsymbol{\mu}}_{2}+\ldots \right\}&\\
&=\left\{\lambda_{0}+\epsilon \lambda_{1}+\epsilon^{2} \lambda_{2}+\ldots\right\} \left\{{\boldsymbol{\mu}}_{0}+\epsilon {\boldsymbol{\mu}}_{1}+\epsilon^{2} {\boldsymbol{\mu}}_{2}+\ldots\right\}.&
\end{eqnarray*}
Now, collect the coefficients of $\epsilon^{0}=1$; that leads to the original problem:
\begin{equation}\label{epsilon0 coef.}
{\boldsymbol{\Sigma}}_{0} {\boldsymbol{\mu}}_{0}=\lambda_{0} {\boldsymbol{\mu}}_{0}.
\end{equation}
Then collecting the coefficients of $\epsilon^{1}=\epsilon$ gives:
\begin{equation}\label{epsilon1 coef.}
    [({\bf z}-{\boldsymbol{\alpha}}_{0})({\bf z}-{\boldsymbol{\alpha}}_{0})^{T}-{\boldsymbol{\Sigma}}_{0}]{\boldsymbol{\mu}}_{0}+ {\boldsymbol{\Sigma}}_{0}{\boldsymbol{\mu}}_{1}= \lambda_{0} {\boldsymbol{\mu}}_{1}+ \lambda_{1} {\boldsymbol{\mu}}_{0}.
\end{equation}
Note that ${\boldsymbol{\mu}}^{T}_{[\epsilon]}{\boldsymbol{\mu}}_{[\epsilon]}=1$ which means:
\begin{eqnarray*}
% \nonumber to remove numbering (before each equation)
  ({\boldsymbol{\mu}}_{0}+\epsilon {\boldsymbol{\mu}}_{1}+ \ldots)^{T}({\boldsymbol{\mu}}_{0}+\epsilon {\boldsymbol{\mu}}_{1}+ \ldots)&=& 1 \\
  {\boldsymbol{\mu}}_{0}^{T}{\boldsymbol{\mu}}_{0}+\epsilon({\boldsymbol{\mu}}_{0}^{T}{\boldsymbol{\mu}}_{1}+{\boldsymbol{\mu}}_{1}^{T} {\boldsymbol{\mu}}_{0})+\ldots &=& 1 ,
\end{eqnarray*}
but it is clear from  (\ref{epsilon0 coef.}) that ${\boldsymbol{\mu}}_{0}^{T}{\boldsymbol{\mu}}_{0}=1$, which results in
\begin{equation}\label{mu.Ortho.}
{\boldsymbol{\mu}}_{0}^{T}{\boldsymbol{\mu}}_{1}={\boldsymbol{\mu}}_{1}^{T}{\boldsymbol{\mu}}_{0} = 0.
\end{equation}
So it turns out that ${\boldsymbol{\mu}}_{0}$ and ${\boldsymbol{\mu}}_{1}$ are orthogonal to each other. Now, multiply (\ref{epsilon1 coef.}) by ${\boldsymbol{\mu}}_{0}^{T}$ from left, then use a property of the orthogonality of ${\boldsymbol{\mu}}_{0}$  and ${\boldsymbol{\mu}}_{1}$ in (\ref{mu.Ortho.}) to obtain the following:
\[ {\boldsymbol{\mu}}_{0}^{T}[({\bf z}-{\boldsymbol{\alpha}}_{0})({\bf z}-{\boldsymbol{\alpha}}_{0})^{T}-{\boldsymbol{\Sigma}}_{0}]{\boldsymbol{\mu}}_{0}+ {\boldsymbol{\mu}}_{0}^{T} {\boldsymbol{\Sigma}}_{0}{\boldsymbol{\mu}}_{1}= \lambda_{0} {\boldsymbol{\mu}}_{0}^{T}{\boldsymbol{\mu}}_{1}+ \lambda_{1} {\boldsymbol{\mu}}_{0}^{T}{\boldsymbol{\mu}}_{0},\]
using (\ref{epsilon0 coef.}) and the previous result in (\ref{mu.Ortho.}), respectively, the second term on the left hand side and the first term on the right hand side equal zero. Then
\begin{equation}\label{lamda1}
    \lambda_{1}={\boldsymbol{\mu}}_{0}^{T}[({\bf z}-{\boldsymbol{\alpha}}_{0})({\bf z}-{\boldsymbol{\alpha}}_{0})^{T}-{\boldsymbol{\Sigma}}_{0}]{\boldsymbol{\mu}}_{0}= \left({\boldsymbol{\mu}}_{0}^{T}({\bf z}-{\boldsymbol{\alpha}}_{0}) \right)^{2}-\lambda_{0}.
\end{equation}
If we let ${\bf z}={\boldsymbol{\alpha}}_{0}+ \gamma {\boldsymbol{\mu}}_{0}$, then $\lambda_{1}=O(\gamma^{2})$ as $\gamma\rightarrow\infty$. Along all directions not perpendicular to ${\boldsymbol{\mu}}_{0}.$ ${\bf z}={\boldsymbol{\alpha}}_{0}+ \gamma {\bf v}$ that is $\lambda_{1}=O(\|{\bf z}\|^{2})$ provided ${\bf v}^{T}{\boldsymbol{\mu}}_{0}\neq 0$.\\
Now, to find ${\boldsymbol{\mu}}_{1}$, rearrange (\ref{epsilon1 coef.}) to be in the form:
\begin{equation}\label{epsilon1 coef.2}
      ({\boldsymbol{\Sigma}}_{0}-\lambda_{0}{\bf I}){\boldsymbol{\mu}}_{1}=\lambda_{1}{\boldsymbol{\mu}}_{0}-[({\bf z}-{\boldsymbol{\alpha}}_{0})({\bf z}-{\boldsymbol{\alpha}}_{0})^{T}-{\boldsymbol{\Sigma}}_{0}]{\boldsymbol{\mu}}_{0}.
\end{equation}
But $({\boldsymbol{\Sigma}}_{0}-\lambda_{0}{\bf I})$ is singular matrix, since it does not have full rank. Then, instead, multiply by a generalized inverse $({\boldsymbol{\Sigma}}_{0}-\lambda_{0}{\bf I})^{+}$. To prepare for this step, we have to note that ${\boldsymbol{\Sigma}}_{0}$ could be written as:
\[{\boldsymbol{\Sigma}}_{0}=\lambda_{0}{\boldsymbol{\mu}}_{0}{\boldsymbol{\mu}}_{0}^{T}+\sum_{j\neq0}\lambda_{j}{\boldsymbol{\mu}}_{j}{\boldsymbol{\mu}}_{j}^{T},\] then
\[({\boldsymbol{\Sigma}}_{0}-\lambda_{0}{\bf I})=
\lambda_{0}{\boldsymbol{\mu}}_{0}{\boldsymbol{\mu}}_{0}^{T}+\sum_{j\neq0}\lambda_{j}{\boldsymbol{\mu}}_{j}{\boldsymbol{\mu}}_{j}^{T}
-\lambda_{0}{\bf I},\]
but ${\bf I}$ also could be written in the form
\[{\bf I}={\boldsymbol{\mu}}_{0}{\boldsymbol{\mu}}_{0}^{T}+\sum_{j\neq0}{\boldsymbol{\mu}}_{j}{\boldsymbol{\mu}}_{j}^{T},\]
thus
\[({\boldsymbol{\Sigma}}_{0}-\lambda_{0}{\bf I})=
\sum_{j\neq0}(\lambda_{j}-\lambda_{0}){\boldsymbol{\mu}}_{j}{\boldsymbol{\mu}}_{j}^{T},\]
and so it is natural to define
\[({\boldsymbol{\Sigma}}_{0}-\lambda_{0}{\bf I})^{+}=
\sum_{j\neq0}(\lambda_{j}-\lambda_{0})^{-1}{\boldsymbol{\mu}}_{j}{\boldsymbol{\mu}}_{j}^{T},\]
which turns out to be the Morre-Penrose inverse (\citet{Mardia&Kent&Bibby:1979}).
Moreover,
\[({\boldsymbol{\Sigma}}_{0}-\lambda_{0}{\bf I})^{+}({\boldsymbol{\Sigma}}_{0}-\lambda_{0}{\bf I})=
{\bf I}-{\boldsymbol{\mu}}_{0}{\boldsymbol{\mu}}_{0}^{T},\]
which is results in
\begin{equation}\label{mu0}
  ({\boldsymbol{\Sigma}}_{0}-\lambda_{0}{\bf I})^{+}({\boldsymbol{\Sigma}}_{0}-\lambda_{0}{\bf I}){\boldsymbol{\mu}}_{0}= {\bf 0}
\end{equation}
\begin{equation}\label{mu-1}
  ({\boldsymbol{\Sigma}}_{0}-\lambda_{0}{\bf I})^{+}({\boldsymbol{\Sigma}}_{0}-\lambda_{0}{\bf I}) {\boldsymbol{\mu}}_{1}= {\boldsymbol{\mu}}_{1}.
\end{equation}
Now, multiplying (\ref{epsilon1 coef.2}) from the left by the generalized inverse yields:
\begin{equation}\label{}
{\boldsymbol{\mu}}_{1}=({\boldsymbol{\Sigma}}_{0}-\lambda_{0}{\bf I})^{+}[\lambda_{1}{\boldsymbol{\mu}}_{0}-[({\bf z}-{\boldsymbol{\alpha}}_{0})({\bf z}-{\boldsymbol{\alpha}}_{0})^{T}-{\boldsymbol{\Sigma}}_{0}]{\boldsymbol{\mu}}_{0}],
\end{equation}
then using (\ref{mu0}) and (\ref{mu-1}) where $\lambda_{1}$ is given by (\ref{lamda1}) to obtain:
\begin{eqnarray}\label{mu1}
% \nonumber to remove numbering (before each equation)
  {\boldsymbol{\mu}}_{1} &=& -({\boldsymbol{\Sigma}}_{0}-\lambda_{0}{\bf I})^{+} [({\bf z}-{\boldsymbol{\alpha}}_{0})({\bf z}-{\boldsymbol{\alpha}}_{0})^{T}-{\boldsymbol{\Sigma}}_{0}]{\boldsymbol{\mu}}_{0} \nonumber\\
   &=& - [ ({\bf z}-{\boldsymbol{\alpha}}_{0})^{T}{\boldsymbol{\mu}}_{0}] ({\boldsymbol{\Sigma}}_{0}-\lambda_{0}{\bf I})^{+} ({\bf z}-{\boldsymbol{\alpha}}_{0}).
\end{eqnarray}
The quadratic form in (\ref{mu1}) is unbounded. To justify this result, let us consider ${\bf z}$ to be the following linear combination:
\begin{equation}\label{newz}
    {\bf z}={\boldsymbol{\alpha}}_{0} + \gamma {\boldsymbol{\mu}}_{0} + \eta {\bf v},
\end{equation}
where {\bf v} is an orthogonal to ${\boldsymbol{\mu}}_{0}$, i.e. such that ${\bf v}^{T}{\boldsymbol{\mu}}_{0}=0$. Now, rewriting (\ref{mu1}) as a function of ${\bf z}$ as in (\ref{newz}) yields:
\begin{eqnarray*}
% \nonumber to remove numbering (before each equation)
  {\boldsymbol{\mu}}_{1}({\bf z}) &=& -(\gamma {\boldsymbol{\mu}}_{0} + \eta {\bf v})^{T} {\boldsymbol{\mu}}_{0} ({\boldsymbol{\Sigma}}_{0}-\lambda_{0}{\bf I})^{+} (\gamma {\boldsymbol{\mu}}_{0} + \eta {\bf v})\\
   &=& - \eta \gamma ({\boldsymbol{\Sigma}}_{0}-\lambda_{0}{\bf I})^{+} {\bf v}.
\end{eqnarray*}
In the previous equation, the quantity $({\boldsymbol{\Sigma}}_{0}-\lambda_{0}{\bf I})^{+} {\bf v}\neq {\bf 0}$, since ${\boldsymbol{\Sigma}}_{0}$ has a full rank, ${\bf v}$ is an orthogonal to ${\boldsymbol{\mu}}_{0}$ and $\eta$ and $\gamma$ can be chosen to make $\|{\boldsymbol{\mu}}_{1}\|$ as large as we desire. Moreover, $\|{\boldsymbol{\mu}}_{1}\|= O(\|{\bf z}\|^{2})$ as $\|{\bf z}\| \rightarrow \infty$ and the $O(\|{\bf z}\|^{2})$ rate is achieved in all directions except those for which $\gamma =0$ or $\eta =0$.\\
The influence function for ${\boldsymbol{\mu}}$ is:
\begin{eqnarray*}
IF_{{\boldsymbol{\mu}}}({\bf z}, F) &=& \lim _{\epsilon \rightarrow 0}\left(\frac{{\boldsymbol{\mu}}_{[\epsilon]}-{\boldsymbol{\mu}}_{0}}{\epsilon}\right)={\boldsymbol{\mu}}_{1}({\bf z}) \nonumber \\
&=& - \big( ({\bf z}-{\boldsymbol{\alpha}}_{0})^{T}{\boldsymbol{\mu}}_{0} \big) ({\boldsymbol{\Sigma}}_{0}-\lambda_{0}{\bf I})^{+} ({\bf z}-{\boldsymbol{\alpha}}_{0}).
\end{eqnarray*}
where ${\boldsymbol{\mu}}_{[\epsilon]}$ is given in (\ref{mu-eps}). We can follow the same steps to find the IF for ${\lambda}_{[\epsilon]}$ which is:
\begin{eqnarray*}
IF_{{\boldsymbol{\lambda}}}({\bf z}, F) = \lim _{\epsilon \rightarrow 0}\left(\frac{{\boldsymbol{\lambda}}_{[\epsilon]}-{\boldsymbol{\lambda}}_{0}}{\epsilon}\right)={\boldsymbol{\lambda}}_{1}({\bf z}) \nonumber = \big( ({\bf z}-{\boldsymbol{\alpha}}_{0})^{T}{\boldsymbol{\mu}}_{0} \big)^2 - \lambda_0,
\end{eqnarray*}
where ${\lambda}_{[\epsilon]}$ is defined in (\ref{lamda-eps}). Further, let $z={\boldsymbol{\alpha}}_{0} + \gamma {\boldsymbol{\mu}}_{0} + \eta {\boldsymbol{v}}$ where ${\boldsymbol{v}}$ is orthogonal to ${\boldsymbol{\mu}}_{0}$  and $\gamma,\eta\neq 0$ then
\begin{equation*}
\lim _{||\boldsymbol{z}|| \rightarrow \infty}||{\boldsymbol{\mu}}_{1}(\boldsymbol{z})|| = \infty,
\end{equation*}
and similarly for ${{\lambda}}_{1}({\bf z})$, completing the proof that both IFs are unbounded.

\end{comment}





\subsection{Proof of Proposition \ref{influence:func:cauchy:pca}}
\label{robust:cauchy:proof}
Let us first make the symbolism more explicit and denote $l_F(\bfu|\bftheta)$ the Cauchy log-likelihood function with respect to the distribution $F$ and $\hat{\bfu}_F$ the respective leading Cauchy principal direction. Then, our goal is to calculate the limit of
$$
\frac{1}{\epsilon} (\hat{\bfu}_{F_{\epsilon,\bfz}} - \hat{\bfu}_F)
$$
as $\epsilon\to 0$ where $\hat{\bfu}_{F_{\epsilon,\bfz}}$ is the leading Cauchy principal direction for the distribution $F_{\epsilon,\bfz}=(1-\epsilon)F+\epsilon \Delta_\bfz$. The optimality condition for the leading Cauchy principal direction reads
\begin{equation}
\bfP_{\hat{\bfu}_{F_{\epsilon,\bfz}}} \left. \frac{\partial}{\partial\bfu} l_{F_{\epsilon,\bfz}}\big(\bfu|\bftheta_{F_{\epsilon,\bfz}}(\bfu)\big) \right|_{\bfu=\hat{\bfu}_{F_{\epsilon,\bfz}}} = 0
\label{opt:cond:perturbed}
\end{equation}
and
$$
\bfP_{\hat{\bfu}_{F}} \left. \frac{\partial}{\partial\bfu} l_{F}\big(\bfu|\bftheta_{F}(\bfu)\big) \right|_{\bfu=\hat{\bfu}_{F}} = 0
$$
Moreover, $\hat{\bfu}_{F_{\epsilon,\bfz}}$ is a unit vector which can be represented as
$$
\hat{\bfu}_{F_{\epsilon,\bfz}} = \cos(\rho) \hat{\bfu}_F + \sin(\rho) \bfh
$$
where $\bfh$ is a unit vector perpendicular to $\hat{\bfu}_F$ and $\rho$ is a (small) real number. Under these assumptions, $\hat{\bfu}_{F_{\epsilon,\bfz}}$ is a unit vector since
$$
||\hat{\bfu}_{F_{\epsilon,\bfz}}||_2^2 = \cos^2(\rho) ||\hat{\bfu}_F||_2^2 + \sin^2(\rho) ||\bfh||_2^2 = 1
$$
Obviously, $\rho$ depends on $\epsilon$ and $\bfz$ (i.e., $\rho=\rho(\epsilon,\bfz)$) and $\lim_{\epsilon\to 0} \rho = 0$ but we choose to avoid denoting their explicit relationship because it is not required in our proof. Moreover, a Taylor expansion for the representation leads to
$$
\hat{\bfu}_{F_{\epsilon,\bfz}} = \hat{\bfu}_F + \rho \bfh + O(\rho^2)
$$
thus we obtain that
$$
\bfP_{\hat{\bfu}_{F_{\epsilon,\bfz}}} = \bfP_{\hat{\bfu}_{F}} - \rho (\hat{\bfu}_{F} \bfh^T + \bfh \hat{\bfu}_{F}^T) + O(\rho^2)
$$

Next, we compute the partial derivative using the chain rule
$$
\frac{\partial}{\partial\bfu} l_{F}\big(\bfu|\bftheta_{F}(\bfu)\big) =
\int_{\mathbb R^p} \left[ \frac{\partial}{\partial c} g(c(\bfu), \bftheta_{F}(\bfu)) \frac{\partial}{\partial \bfu} c(\bfu) 
+ \frac{\partial}{\partial \bftheta} g(c(\bfu), \bftheta_{F}(\bfu)) \frac{\partial}{\partial \bfu} \bftheta_{F}(\bfu) \right] dF(\bfx)
$$
Therefore,
\begin{equation*}
\begin{aligned}
\left. \frac{\partial}{\partial\bfu} l_{F}\big(\bfu|\bftheta_{F}(\bfu)\big) \right|_{\bfu=\hat{\bfu}_{F}} &= 
\int_{\mathbb R^p} \left[ \bar{g}_c(\bfx;\hat{\bfu}_{F}) \bfx 
+ \bar{g}_{\bftheta}(\bfx;\hat{\bfu}_{F})
\frac{\partial}{\partial \bfu} \bftheta_{F}(\bfu) \Big|_{\bfu=\hat{\bfu}_{F}} \right] dF(\bfx) \\
&= \int_{\mathbb R^p} \bar{g}_c(\bfx;\hat{\bfu}_{F}) \bfx  dF(\bfx)
+ \int_{\mathbb R^p} \bar{g}_{\bftheta}(\bfx;\hat{\bfu}_{F}) dF(\bfx)
\frac{\partial}{\partial \bfu} \bftheta_{F}(\bfu) \Big|_{\bfu=\hat{\bfu}_{F}} \\
&= \int_{\mathbb R^p} \bar{g}_c(\bfx;\hat{\bfu}_{F}) \bfx  dF(\bfx) 
\end{aligned}
\end{equation*}
The second summand equals to zero because $\hat{\bfu}_{F}$ maximizes the Cauchy log-likelihood function thus it holds that $\int_{\mathbb R^p} \bar{g}_{\bftheta}(\bfx;\hat{\bfu}_{F}) dF(\bfx) = \bfzero$. Similarly,
\begin{equation*}
\begin{aligned}
&\left. \frac{\partial}{\partial\bfu} l_{F_{\epsilon,\bfz}}\big(\bfu|\bftheta_{F_{\epsilon,\bfz}}(\bfu)\big) \right|_{\bfu=\hat{\bfu}_{F_{\epsilon,\bfz}}}
= \int_{\mathbb R^p} \bar{g}_c(\bfx;\hat{\bfu}_{F_{\epsilon,\bfz}}) \bfx  dF_{\epsilon,\bfz} (\bfx) \\
=& (1-\epsilon) \int_{\mathbb R^p} \bar{g}_c(\bfx;\hat{\bfu}_{F_{\epsilon,\bfz}}) \bfx dF(\bfx)
+ \epsilon \bar{g}_c(\bfz;\hat{\bfu}_{F_{\epsilon,\bfz}}) \bfz 
\end{aligned}
\end{equation*}


Next, we further Taylor expand $\bar{g}_{c}(\bfx; \bfu_{F_{\epsilon,\bfz}})$ using $\hat{\bfu}_{F_{\epsilon,\bfz}} = \hat{\bfu}_F + \rho \bfh + O(\rho^2)$
$$
\bar{g}_{c}(\bfx; \hat{\bfu}_{F_{\epsilon,\bfz}}) = \bar{g}_{c}(\bfx; \hat{\bfu}_{F})
+ \rho \bfh \frac{\partial}{\partial\bfu} \bar{g}_{c}(\bfx; {\bfu}) \Big|_{\bfu=\hat{\bfu}_{F}}  + O(\rho^2)
$$
Using again the chain rule, we obtain that
$$
\frac{\partial}{\partial\bfu} \bar{g}_{c}(\bfx; {\bfu}) =
\bar{g}_{cc}(\bfx; {\bfu}) \bfx + \bar{g}_{c\bftheta}(\bfx; {\bfu}) \frac{\partial}{\partial\bfu} \bftheta_{F}(\bfu)
$$
The computation of the partial derivative $\frac{\partial}{\partial \bfu} \bftheta_{F}(\bfu)$ follows. Formula
$\bftheta_{F}(\bfu) = \argmax_{\bftheta} l_F(\bfx^T\bfu|\bftheta)$ implies that
\begin{equation*}
\frac{\partial}{\partial \bftheta} l_F(c(\bfu)|\bftheta) \Big|_{\bftheta=\bftheta_{F}(\bfu)} = 0 \ .
\end{equation*}
Differentiating with respect to $\bfu$ and using the implicit function theorem, we get
\begin{equation*}
\begin{aligned}
\frac{\partial}{\partial \bfu} \bftheta_{F}(\bfu) &=
- \frac{\partial}{\partial \bfu} \frac{\partial}{\partial \bftheta} l_F(c(\bfu)|\bftheta) \Big|_{\bftheta=\bftheta_{F}(\bfu)} \left[ \frac{\partial^2}{\partial \bftheta^2} l_F(c(\bfu)|\bftheta) \Big|_{\bftheta=\bftheta_{F}(\bfu)} \right]^{-1} \\
&= - \int_{\mathbb R^p}  \bfx \bar{g}_{c\bftheta}(\bfx;\bfu) dF(\bfx)
\left[ \int_{\mathbb R^p} \bar{g}_{\bftheta\bftheta}(\bfx;\bfu)dF(\bfx) \right]^{-1}
\end{aligned}
\end{equation*}

Thus,
\begin{equation*}
\begin{aligned}
&\bar{g}_{c}(\bfx; \hat{\bfu}_{F_{\epsilon,\bfz}}) = \bar{g}_{c}(\bfx; \hat{\bfu}_{F}) \\
+& \rho \bfh \left[\bar{g}_{cc}(\bfx; \hat{\bfu}_F) \bfx + \int_{\mathbb R^p}  \bfx \bar{g}_{c\bftheta}(\bfx;\hat{\bfu}_{F}) dF(\bfx)
\left[ \int_{\mathbb R^p} \bar{g}_{\bftheta\bftheta}(\bfx;\hat{\bfu}_{F})dF(\bfx) \right]^{-1} \bar{g}_{c\bftheta}(\bfx; {\hat{\bfu}_F}) \right] + O(\rho^2)
\end{aligned}
\end{equation*}

Overall, (\ref{opt:cond:perturbed}) becomes
\begin{equation*}
\begin{aligned}
&\left[ \bfP_{\hat{\bfu}_F} - \rho (\hat{\bfu}_{F} \bfh^T + \bfh \hat{\bfu}_{F}^T) + O(\rho^2) \right] \cdot
\left[ (1-\epsilon) \int_{\mathbb R^p} \bar{g}_c(\bfx;\hat{\bfu}_{F_{\epsilon,\bfz}}) \bfx dF(\bfx)
+ \epsilon \bar{g}_c(\bfz;\hat{\bfu}_{F_{\epsilon,\bfz}}) \bfz  \right] = 0 \\
\Rightarrow&
\bfP_{\hat{\bfu}_F} \int_{\mathbb R^p} \bar{g}_c(\bfx;\hat{\bfu}_{F_{\epsilon,\bfz}}) \bfx dF(\bfx)
- \rho (\hat{\bfu}_{F} \bfh^T + \bfh \hat{\bfu}_{F}^T) \int_{\mathbb R^p} \bar{g}_c(\bfx;\hat{\bfu}_{F}) \bfx dF(\bfx) + O(\rho^2) \\
&= \epsilon \bfP_{\hat{\bfu}_F} \left[\int_{\mathbb R^p} \bar{g}_c(\bfx;\hat{\bfu}_{F}) \bfx dF(\bfx)
- \bar{g}_c(\bfz;\hat{\bfu}_{F_{\epsilon,\bfz}}) \bfz \right] + O(\epsilon\rho) \\
\Rightarrow&
\rho \bfh \left[ \int_{\mathbb R^p}\bar{g}_{cc}(\bfx; \hat{\bfu}_F) \bfx^T \bfx dF(\bfx)
+ \int_{\mathbb R^p} \bar{g}_c(\bfx;\hat{\bfu}_{F}) \hat{\bfu}_{F}^T \bfx dF(\bfx) \right. \\
&+ \left. \int_{\mathbb R^p}  \bfx \bar{g}_{c\bftheta}(\bfx;\hat{\bfu}_{F}) dF(\bfx)
\left[ \int_{\mathbb R^p} \bar{g}_{\bftheta\bftheta}(\bfx;\hat{\bfu}_{F})dF(\bfx) \right]^{-1} \int_{\mathbb R^p} \bar{g}_{c\bftheta}(\bfx; {\hat{\bfu}_F}) \bfx dF(\bfx) \right] + O(\rho^2) \\
&= \epsilon \bfP_{\hat{\bfu}_F} \left[\int_{\mathbb R^p} \bar{g}_c(\bfx;\hat{\bfu}_{F}) \bfx dF(\bfx)
- \bar{g}_c(\bfz;\hat{\bfu}_{F_{\epsilon,\bfz}}) \bfz \right] + O(\epsilon\rho)
\end{aligned}
\end{equation*}
where we use the facts that
$$\bfP_{\hat{\bfu}_F} \bfh = \bfh$$
and 
$$\bfh^T \int_{\mathbb R^p} \bar{g}_c(\bfx;\hat{\bfu}_{F}) \bfx dF(\bfx)
= \bfP_{\hat{\bfu}_F} \int_{\mathbb R^p} \bar{g}_c(\bfx;\hat{\bfu}_{F}) \bfx dF(\bfx)
= \bfP_{\hat{\bfu}_F} \left. \frac{\partial}{\partial\bfu} l_{F}\big(\bfu|\bftheta_{F}(\bfu)\big) \right|_{\bfu=\hat{\bfu}_{F}} = 0
$$

Thus, the influence function is
$$
IF_{\hat{\bfu}_F} (\bfz, F) = \lim_{\epsilon\to0} \frac{\rho\bfh}{\epsilon} = \bfA^{-1} \bfb
$$
where
\begin{equation*}
\begin{aligned}
\bfA &= \bfI_{d} \left[ \int_{\mathbb R^p}\bar{g}_{cc}(\bfx; \hat{\bfu}_F) \bfx^T \bfx dF(\bfx)
+ \int_{\mathbb R^p} \bar{g}_c(\bfx;\hat{\bfu}_{F}) \hat{\bfu}_{F}^T \bfx dF(\bfx) \right] \\
&+ \int_{\mathbb R^p}  \bfx \bar{g}_{c\bftheta}(\bfx;\hat{\bfu}_{F}) dF(\bfx)
\left[ \int_{\mathbb R^p} \bar{g}_{\bftheta\bftheta}(\bfx;\hat{\bfu}_{F})dF(\bfx) \right]^{-1} \int_{\mathbb R^p} \bar{g}_{c\bftheta}(\bfx; {\hat{\bfu}_F}) \bfx dF(\bfx)
\end{aligned}
\end{equation*}
and
$$
\bfb = \bfP_{\hat{\bfu}_F} \left[\int_{\mathbb R^p} \bar{g}_c(\bfx;\hat{\bfu}_{F}) \bfx dF(\bfx)
- \bar{g}_c(\bfz;\hat{\bfu}_{F_{\epsilon,\bfz}}) \bfz \right]
$$




\begin{comment}
\subsection{Proof of Lemma \ref{d/du(ThetaF(u))}}
To find the influence function for the functional version of the parameter vector ${\boldsymbol {\theta}}_{F}({\bf u})$ at fixed ${\bf u}$, starting by step $1$ in section (\ref{Pre}). In this step we need to find the derivative of ${\hat{\boldsymbol {\theta}}}_{F}({\bf u})$ with respect to ${\bf u}$ as the following:\\
\begin{eqnarray*}
\frac{\partial}{\partial {\boldsymbol{\theta}}} m({\bf u},{\boldsymbol {\theta}}) &=&
\frac{\partial}{\partial {\boldsymbol{\theta}}} \int_{{\bf x} \in {\mathbb{R}}^{p}} l[{\bf x}; {\bf u}] dF({\bf x}) = \int_{{\bf x} \in {\mathbb{R}}^{p}} l_{; {\boldsymbol {\theta}}}[{\bf x}; {\bf u}] dF({\bf x}).
\end{eqnarray*}
Choose ${\boldsymbol {\theta}}_{F}({\bf u})$ in step $1$ to satisfy the condition $M-$estimator which is
\begin{equation}\label{1}
    \int_{{\bf x} \in \mathbb{R}^{p}} l_{;{\boldsymbol {\theta}}}[{\bf x}; {\bf u}] dF({\bf x}) = {\bf 0}.
\end{equation}
Suppose we now rotate ${\bf u} \mapsto {\bf v}$, where ${\bf v}$ is defined in (\ref{v}) such that:
\begin{equation*}
    {\bf v}= {\bf u} \cos{\alpha} + {\bf h} \sin{\alpha},
\end{equation*}
Now (\ref{1}) still holds for ${\bf v}$, so
\begin{equation}\label{2}
    \int_{{\bf x} \in \mathbb{R}^{p}} l_{;{\boldsymbol {\theta}}}[{\bf x}; {\bf v}] dF({\bf x}) = {\bf 0}.
\end{equation}
For small ${\alpha},$ a Taylor expansion gives
\begin{eqnarray}\label{3}
% \nonumber to remove numbering (before each equation)
\nonumber  l_{;{\boldsymbol {\theta}}}[{\bf x}; {\bf v}]&=& l_{;{\boldsymbol {\theta}}}[{\bf x}; ({\bf u} \cos{\alpha}+ {\bf h} \sin{\alpha})]\\
&=& l_{;{\boldsymbol {\theta}}}[{\bf x}; {\bf u}]+ l_{c;{\boldsymbol {\theta}}}[{\bf x}; {\bf u}]{\bf x}^{T}{\bf h} \alpha + l_{;{\boldsymbol {\theta}}{\boldsymbol {\theta}}}[{\bf x}; {\bf u}] \Delta_{F,{\bf h},\alpha}+ O(\alpha^{2}),
\end{eqnarray}
where $\Delta_{F,{\bf h},\alpha}$ is defined in (\ref{Delta(F,h,alpha)}). Substituting (\ref{3}) into (\ref{2}) yields
\begin{equation}\label{3-a}
    \int_{{\bf x} \in \mathbb{R}^{p}} \left[l_{;{\boldsymbol {\theta}}}[{\bf x}; {\bf u}] + l_{c;{\boldsymbol {\theta}}}[{\bf x}; {\bf u}] {\bf x}^{T}{\bf h} \alpha + l_{;{\boldsymbol {\theta}}{\boldsymbol {\theta}}}[{\bf x}; {\bf u}] \Delta_{F,{\bf h},\alpha}\right]dF({\bf x}) = O(\alpha^{2}).
\end{equation}
Using (\ref{1}) on the first term in the integrand of (\ref{3-a}) and rearranging the equation, we obtain
\begin{equation*}
\left(\int_{{\bf x} \in \mathbb{R}^{p}} l_{;{\boldsymbol {\theta}}{\boldsymbol {\theta}}}[{\bf x}; {\bf u}]dF({\bf x}) \right) \Delta_{F,{\bf h},\alpha} = - \alpha \int_{{\bf x} \in \mathbb{R}^{p}} l_{c;{\boldsymbol {\theta}}}[{\bf x}; {\bf u}] {\bf x}^{T}{\bf h} dF({\bf x}) + O(\alpha^{2}),
\end{equation*}
which leads to
\begin{equation}\label{3-b}
\frac{\Delta_{F,{\bf h},\alpha}}{\alpha} = - \left(\int_{{\bf x} \in \mathbb{R}^{p}} l_{;{\boldsymbol {\theta}}{\boldsymbol {\theta}}}[{\bf x}; {\bf u}]dF({\bf x}) \right)^{-1} \int_{{\bf x} \in \mathbb{R}^{p}} l_{c;{\boldsymbol {\theta}}}[{\bf x}; {\bf u}]{\bf x}^{T}{\bf h}dF({\bf x}) + O(\alpha)
\end{equation}
Taking the limit for (\ref{3-b}), as $\alpha \rightarrow 0,$ we result in
\begin{equation}\label{4a}
    \lim _{\alpha \rightarrow 0} \frac{\Delta_{F,h,\alpha}}{\alpha} = - {\boldsymbol \Xi}^{-1} \int_{{\bf x} \in \mathbb{R}^{p}} l_{c;{\boldsymbol {\theta}}}[{\bf x}; {\bf u}]{\bf x}^{T}{\bf h}dF({\bf x}).
\end{equation}
where ${\boldsymbol \Xi}$ is defined in (\ref{xi-CPC}).
\hfill$\square$
%\end{proof}


\subsection{Proof of Lemma \ref{d/dF(ThetaF(u))}}
Choose ${\boldsymbol {\theta}}_{F}({\bf u})$ in step $1$ to satisfy the condition $M-$estimator which is
\begin{equation}\label{1}
    \int_{{\bf x} \in \mathbb{R}^{p}} l_{;{\boldsymbol {\theta}}}({\bf x}^{T}{\bf u}; {\boldsymbol {\theta}}_{{\bf z},\epsilon}({\bf u})) dF_{{\bf z},\epsilon}({\bf x}) = {\bf 0}.
\end{equation}
For fixed ${\bf u}$ and variable $F$:
\begin{equation}\label{1-1CPC}
    l_{;{\boldsymbol {\theta}}}({\bf x}^{T}{\bf u}; {\boldsymbol {\theta}}_{{\bf z},\epsilon}({\bf u}))= l_{;{\boldsymbol {\theta}}}[{\bf x}; {\bf u}]+ l_{;{\boldsymbol {\theta}}{\boldsymbol {\theta}}}[{\bf x}; {\bf u}] \Gamma_{F,{\bf h},\alpha}+ O(\epsilon^{2}),
\end{equation}
where $\Gamma_{F,{\bf h},\alpha}$ is given in (\ref{Gamma(F,h,alpha)}).
Then
\begin{eqnarray*}
\int_{{\bf x} \in \mathbb{R}^{p}} l_{;{\boldsymbol {\theta}}}({\bf x}^{T}{\bf u}; {\boldsymbol {\theta}}_{{\bf z},\epsilon}({\bf u})) dF_{{\bf z},\epsilon}({\bf x}) &=& 
+ \int_{{\bf x} \in \mathbb{R}^{p}} l_{;{\boldsymbol {\theta}}{\boldsymbol {\theta}}}[{\bf x}; {\bf u}] \Gamma_{F,{\bf h},\alpha} dF_{\bf z,\epsilon}({\bf x}) \nonumber \\
& & + \epsilon {\hspace{1mm}}  l_{;{\boldsymbol {\theta}}}[{\bf z}; {\bf u}] + O(\epsilon^{2}),
\end{eqnarray*}
which implies that
\begin{eqnarray}\label{4b}
\lim_{\epsilon \rightarrow 0} \frac{1}{\epsilon}{\hspace{1mm}} \Gamma_{F,{\bf h},\alpha} &=& - \left(\int_{{\bf x} \in \mathbb{R}^{p}} l_{;{\boldsymbol {\theta}}{\boldsymbol {\theta}}}({\bf x}; {\bf u})dF_{\bf z,\epsilon}({\bf x}) \right)^{-1} l_{;{\boldsymbol {\theta}}} [{\bf z}; {\bf u}] \nonumber \\
&=& - {\boldsymbol \Xi}^{-1} l_{;{\boldsymbol {\theta}}} [{\bf z}; {\bf u}].
\end{eqnarray}
where ${\boldsymbol \Xi}$ is defined in (\ref{xi-CPC})
\hfill$\square$

\subsection{Proof of Lemma \ref{lemma_cauchyIF}}
In Section \ref{d/dF(ThetaF(u))}, we found the derivative of ${\boldsymbol {\theta}}_{F}({\bf u})$ for fixed ${\bf u}$, by satisfying step $1$ in which $m({\bf u},{\boldsymbol {\theta}})$ is maximised. In this section we want to find the influence function for ${\hat{\bf u}}$, by satisfying step $2$ in section \ref{Pre}. Using (\ref{1}), note that
\begin{eqnarray*}
% \nonumber to remove numbering (before each equation)
\nonumber \frac{\partial}{\partial{\bf u}} \int_{{\bf x} \in \mathbb{R}^{p}} l[{\bf x}; {\bf u}]dF({\bf x}) &=& \int_{{\bf x} \in \mathbb{R}^{p}}\left(l_{c;}[{\bf x}; {\bf u}]{\bf x} + l_{;{\boldsymbol {\theta}}}[{\bf x}; {\bf u}]\frac{\partial{\boldsymbol {\theta}}_{F}}{\partial{\bf u}}\right) dF({\bf x}) \\
    &=& \int_{{\bf x} \in \mathbb{R}^{p}} l_{c;}[{\bf x}; {\bf u}]{\bf x}dF({\bf x}),
\end{eqnarray*}
where the second term above is $0$ by definition of ${\boldsymbol {\theta}}_{F}({\bf u}).$
Therefore, a necessary condition for ${\bf u}$ to be a minimum of $m({\bf u}; {\boldsymbol {\theta}}_{F}({\bf u}))$ is
\begin{equation}\label{5}
    {\bf P_{u}} \int_{{\bf x} \in \mathbb{R}^{p}} l_{c;}[{\bf x}; {\bf u}]{\bf x}dF({\bf x})= {\bf 0},
\end{equation}
where ${\bf P_{u}}= {\bf I}_{p} - {\bf u}{\bf u}^{T};$ see (\ref{Pu0}) is a projection matrix which appears because ${\bf u}$ is a unit vector as we explain in section \ref{P} that we treat {\bf u} as a general vector and then project onto the subspace that is orthogonal to {\bf u}.

Now, consider a mixture distribution which is defined in (\ref{mix.dis.}) where $\epsilon \in {[0,1)}$ is small and $\delta_{\bf z}$ is the distribution which assigns all probability to ${\bf z}$ as in (\ref{delta}). Assume ${\bf v}= {\bf u}\cos{\alpha}+{\bf h} \sin{\alpha}$ where ${\bf h}$ and ${\alpha}$ depend on $\epsilon$ and ${\bf z}$. Then
\begin{equation}\label{6}
  {\bf P}_{\bf v} \int_{{\bf x} \in \mathbb{R}^{p}} l_{c;}\left({\bf x}^{T}{\bf v}; {\boldsymbol {\theta}}_{{\bf z},\epsilon}({\bf v})\right){\bf x} dF_{{\bf z},\epsilon}({\bf x})= {\bf 0}
\end{equation}
When $\epsilon$ and $\alpha$ are small, then
\begin{eqnarray}\label{7}
% \nonumber to remove numbering (before each equation)
   {\bf P}_{\bf v}={\bf P}_{({\bf u}\cos{\alpha}+{\bf h} \sin{\alpha})} &=& {\bf I} - ({{\bf u}\cos{\alpha}+{\bf h} \sin{\alpha}})({{\bf u}\cos{\alpha}+{\bf h} \sin{\alpha}})^{T} \nonumber\\
                 &=& {\bf I}-{\bf u}{\bf u}^{T}-\alpha({\bf uh}^{T}+{\bf hu}^{T})+ O(\alpha^{2}) \nonumber\\
                 &=& {\bf P_{u}}- \alpha({\bf uh}^{T}+{\bf hu}^{T})+ O(\alpha^{2}).
\end{eqnarray}
Therefore, from Lemmas \ref{d/du(ThetaF(u))} and \ref{d/dF(ThetaF(u))}, using the fact that
\begin{eqnarray}
% \nonumber to remove numbering (before each equation)
  ({\boldsymbol {\theta}}_{{\bf z},\epsilon}({\bf v})-{\boldsymbol {\theta}}_{F}({\bf u})) &=& ({\boldsymbol {\theta}}_{{\bf z},\epsilon}({\bf v})-{\boldsymbol {\theta}}_{F}({\bf v}))+({\boldsymbol {\theta}}_{F}({\bf v})-{\boldsymbol {\theta}}_{F}({\bf u})) \nonumber\\
   &=& ({\boldsymbol {\theta}}_{{\bf z},\epsilon}({\bf u})-{\boldsymbol {\theta}}_{F}({\bf u}))+({\boldsymbol {\theta}}_{F}({\bf v})-{\boldsymbol {\theta}}_{F}({\bf u}))+ O(\epsilon^{2})\nonumber\\
&=& \Gamma_{F, {\bf h}, \alpha} + \Delta_{F, {\bf h}, \alpha} + O(\epsilon^{2}).
\end{eqnarray}
we have
\begin{eqnarray}\label{8}
%\nonumber to remove numbering (before each equation)
 l_{c;}({\bf x}^{T}{\bf v};{\boldsymbol{\theta}}_{{\bf z},\epsilon}({\bf v}))&=&l_{c;}[{\bf x}; ({\bf u}\cos{\alpha}+{\bf h}\sin{\alpha})]\nonumber\\
   &=&l_{c;}[{\bf x}; {\bf u}] + l_{cc;}[{\bf x}; {\bf u}]{\bf x}^{T}{\bf h}\alpha + l_{c;\boldsymbol{\theta}}[{\bf x}; {\bf u}]\Delta_{F,{\bf h},\alpha} \nonumber \\
   & &+ \ l_{c; \boldsymbol{\theta}}[{\bf x}; {\bf u}]\Gamma_{F, {\bf h}, \alpha} + O(\alpha^{2}),
\end{eqnarray}
where $\Delta_{F, {\bf h}, \alpha} $ and $\Gamma_{F, {\bf h}, \alpha}$ are defined in section \ref{P} and \ref{d/du(ThetaF(u))} respectively.
Substituting (\ref{7}) and (\ref{8}) into (\ref{6}), and ignoring $O(\alpha^{2})$ terms, we obtain
\begin{equation}\label{}
    \left({\bf P_{u}} - \alpha({\bf hu}^{T}+{\bf uh}^{T})\right) \int_{{\bf x} \in \mathbb{R}^{p}} {\bf T}.{\bf x} [(1-\epsilon)dF({\bf x}) + \epsilon {\hspace{1mm}} d \delta_{\bf z}({\bf x})]={\bf 0},
\end{equation}
where
\[{\bf T}= l_{c;}[{\bf x}; {\bf u}] + l_{cc;}[{\bf x}; {\bf u}]{\bf x}^{T}{\bf h}\alpha + l_{c;\boldsymbol{\theta}}[{\bf x};{\bf u}]\Delta_{F,{\bf h},\alpha}+l_{c;\boldsymbol{\theta}}[{\bf x}; {\bf u}]\Gamma_{F,{\bf h},\alpha}.\]
%==============================================
%\begin{eqnarray*}
%&({\bf P_{u}} - \alpha({\bf hu}^{T}+{\bf uh}^{T}))\\
%\int_{{\bf x} \in \mathbb{R}^{p}} \left[ l_{c;}({\bf x}^{T}{\bf u}; {\hat{\boldsymbol {\theta}}}_{F}({\bf u})) + %l_{cc;}({\bf x}^{T}{\bf u}; {\hat{\boldsymbol {\theta}}}_{F}({\bf u})){\bf x}^{T}{\bf h}\alpha + l_{c;\theta}({\bf %x}^{T}{\bf u}; {\hat{\boldsymbol {\theta}}}_{F}({\bf u}))\Delta_{F,{\bf h},\alpha}+l_{c;\theta}({\bf x}^{T}{\bf u}; %{\hat{\boldsymbol {\theta}}}_{F}({\bf u}))\Gamma_{F,{\bf h},\alpha} \right]&\\
%&\cdot {\bf x} [(1-\epsilon)dF(\bf x) + \epsilon {\hspace{1mm}} d \delta_{\bf z}({\bf x})]={\bf 0},&
%\end{eqnarray*}
%\begin{eqnarray*}
%
%&({\bf P_{u}} - \alpha({\bf hu}^{T}+{\bf uh}^{T}))
%   \int_{{\bf x} \in \mathbb{R}^{p}} \left[ l_{c;}({\bf x}^{T}{\bf u}; {\hat{\boldsymbol {\theta}}}_{F}({\bf u})) + %l_{cc;}({\bf x}^{T}{\bf u}; {\hat{\boldsymbol {\theta}}}_{F}({\bf u})){\bf x}^{T}{\bf h}\alpha + l_{c;\theta}({\bf %x}^{T}{\bf u}; {\hat{\boldsymbol {\theta}}}_{F}({\bf u}))\Delta_{F,{\bf h},\alpha}+l_{c;\theta}({\bf x}^{T}{\bf u}; %{\hat{\boldsymbol {\theta}}}_{F}({\bf u}))\Gamma_{F,{\bf h},\alpha} \right]& \\
%     &\cdot {\bf x} [(1-\epsilon)dF(\bf x) + \epsilon {\hspace{1mm}} d \delta_{\bf z}({\bf x})]={\bf 0},&
%\end{eqnarray*}

Then collecting the $O(\alpha)$ terms we get:
\begin{eqnarray*}
% \nonumber to remove numbering (before each equation)
O(\alpha) &=& -\alpha({\bf hu}^{T}+ {\bf uh}^{T})\int_{{\bf x} \in \mathbb{R}^{p}} l_{c;}[{\bf x}; {\bf u}] {\bf x} dF({\bf x}) + \epsilon l_{c;}[{\bf z}; {\bf u}]{\bf z} \\
& & + {\bf P_{u}} \int_{{\bf x} \in \mathbb{R}^{p}} \left( l_{cc;}[{\bf x}; {\bf u}]{\bf x}^{T} {\bf h} \alpha + l_{c;{\boldsymbol \theta}}[{\bf x}; {\bf u}]\Delta_{F,h,\alpha}+ l_{c;{\boldsymbol \theta}}[{\bf x};{\bf u}]\Gamma_{F,h,\alpha} \right) {\bf x} dF({\bf x}) \\
&=& {\bf 0}.
\end{eqnarray*}
Now (\ref{1}) implies that
\[{\bf h}^{T}\int_{{\bf x} \in \mathbb{R}^{p}} l_{c;}[{\bf x}; {\bf u}] {\bf x} dF({\bf x}) ={\bf 0},\]
and using (\ref{4b}),
\begin{equation}\label{Gamma approx}
   \Gamma_{F,h,\alpha}\simeq - \epsilon \left[\int_{{\bf x} \in \mathbb{R}^{p}} l_{; {\boldsymbol {\theta \theta}}}[{\bf x}; {\bf u}]\right]^{-1} l_{;{\boldsymbol \theta}}[{\bf z}; {\bf u}].
\end{equation}
Consequently,
\[\alpha {\bf A h}= \epsilon {\bf B}+O(\epsilon^{2}),\]
where
\begin{eqnarray}\label{A}
% \nonumber to remove numbering (before each equation)
{\bf A} &=& {\bf I}_{p} \displaystyle \int_{{\bf x} \in \mathbb{R}^{p}} l_{c;}[{\bf x}; {\bf u}]{\bf x}^{T}{\bf u} dF({\bf x}) - {\bf P_{u}} \displaystyle \int_{{\bf x} \in \mathbb{R}^{p}} l_{cc;}[{\bf x}; {\bf u}]{\bf x}{\bf x}^{T} dF(\bf x) {\bf P_{u}}  \nonumber\\
   & & + {\bf P_{u}} \left(\displaystyle \int_{{\bf x} \in \mathbb{R}^{p}} {\bf x} l_{c;{\boldsymbol {\theta}}}[{\bf x}; {\bf u}]dF({\bf x})\right)\left(\displaystyle \int_{{\bf x} \in \mathbb{R}^{p}} l_{;{\boldsymbol {\theta \theta}}}[{\bf x}; {\bf u}]dF({\bf x})\right)^{-1} 
\nonumber \\
& & \left(\displaystyle \int_{{\bf x} \in \mathbb{R}^{p}} l^{T}_{c;{\boldsymbol {\theta}}}[{\bf x}; {\bf u}]{\bf x}^{T} dF({\bf x})\right) {\bf P_{u}}. 
\end{eqnarray}
and
\begin{eqnarray}\label{B}
{\bf B}=l_{c;}[{\bf z}; {\bf u}]{\bf z}- \displaystyle \int_{{\bf x} \in \mathbb{R}^{p}}{\bf x} l_{c;{\boldsymbol{\theta}}}[{\bf x}; {\bf u}]dF({\bf x})
\left(-\displaystyle \int_{{\bf x} \in \mathbb{R}^{p}} l_{;{\boldsymbol {\theta \theta}}} [{\bf x}; {\bf u}]dF({\bf x}) \right)^{-1} l_{;{\boldsymbol {\theta}}}[{\bf z}; {\bf u}].
\end{eqnarray}
Therefore
\[\frac{\alpha{\bf h}}{\epsilon}={\bf A}^{-1} {\bf B}+ O(\epsilon), \]
and finally, letting $\epsilon \rightarrow 0$
\begin{equation}\label{CauchyIF}
     IF_{\hat{\bf u}}({\bf z}; F)= \lim _{\epsilon \rightarrow 0}\left( \frac{{\bf v}-{\bf u}}{\epsilon}\right)=\lim _{\epsilon \rightarrow 0}\frac{{\bf h}\alpha}{\epsilon}={\bf A}^{-1} {\bf B}.
\end{equation}

\end{comment}

%\newpage
%\appendix
%\appendixpage
%\addappheadtotoc
%\section{Program}
%\input{AppendixA}

\clearpage
%\bibliographystyle{apalike}
%\bibliography{AishaRef1}

\begin{thebibliography}{}

\bibitem[Bolton and Krzanowski, 1999]{Bolton:1999}
Bolton, R.~J. and Krzanowski, W.~J. (1999).
\newblock {A Characterization of Principal Components for Projection Pursuit}.
\newblock {\em American Statistican}, 53(2):108--109.

\bibitem[Campbell, 1980]{Campbell:1980}
Campbell, N.~A. (1980).
\newblock {Robust Procedures in Multivariate Analysis I: Robust Covariance
  Estimation}.
\newblock {\em Applied Statistics}, 29(3):231--237.

\bibitem[Copas, 1975]{Copas:1975}
Copas, J.~B. (1975).
\newblock {On the Unimodality of the Likelihood for the Cauchy Distribution}.
\newblock {\em Biometrika}, 62(3):701--704.

\bibitem[Croux et~al., 2013]{croux2013robust}
Croux, C., Filzmoser, P., and Fritz, H. (2013).
\newblock Robust sparse principal component analysis.
\newblock {\em Technometrics}, 55(2):202--214.

\bibitem[Croux et~al., 2007]{croux2007algorithms}
Croux, C., Filzmoser, P., and Oliveira, M.~R. (2007).
\newblock Algorithms for projection--pursuit robust principal component
  analysis.
\newblock {\em Chemometrics and Intelligent Laboratory Systems},
  87(2):218--225.

\bibitem[Filzmoser et~al., 2018]{pcapp2018}
Filzmoser, P., Fritz, H., and Kalcher, K. (2018).
\newblock {\em {pcaPP: Robust PCA by Projection Pursuit}}.
\newblock R package version 1.9-73.

\bibitem[Huber, 1964]{Huber:1964}
Huber, P.~J. (1964).
\newblock {Robust Estimation of a Location Parameter}.
\newblock {\em The Annals of Mathematical Statistics}, 35(1):73--101.

\bibitem[Hubert and Verboven, 2003]{Hubert:2003}
Hubert, M. and Verboven, S. (2003).
\newblock {A Robust PCR Method for High-Dimensional Regressors}.
\newblock {\em Journal of Chemometrics}, 17:438--452.

\bibitem[Lakiotaki et~al., 2018]{lakiotaki2018}
Lakiotaki, K., Vorniotakis, N., Tsagris, M., Georgakopoulos, G., and
  Tsamardinos, I. (2018).
\newblock Biodataome: a collection of uniformly preprocessed and automatically
  annotated datasets for data-driven biology.
\newblock {\em Database}, 2018.

\bibitem[Li and Chen, 1985]{Li:1985}
Li, G.~Y. and Chen, Z.~L. (1985).
\newblock {Projection-Pursuit Approach to Robust Dispersion Matrices and
  Principal Components: Primary Theory and Monte Carlo}.
\newblock {\em Journal of the American Statistical Association}, 80:759--766.

\bibitem[Mardia et~al., 1979]{Mardia&Kent&Bibby:1979}
Mardia, K.~F., Kent, J.~T., and Bibby, J.~M. (1979).
\newblock {\em {Multivariate Analysis}}.
\newblock Academic Press.

\bibitem[Maronna, 1976]{Maronna:1976}
Maronna, R.~A. (1976).
\newblock {Robust M-Estimators of Multivariate Location and Scatter}.
\newblock {\em The Annals of Statistics}, 4(1):51--67.

\bibitem[Ro et~al., 2015]{ro2015}
Ro, K., Zou, C., Wang, Z., and Yin, G. (2015).
\newblock Outlier detection for high-dimensional data.
\newblock {\em Biometrika}, 102(3):589--599.

\bibitem[Sykulski, 2017]{rpca2017}
Sykulski, M. (2017).
\newblock {\em {rpca: RobustPCA: Decompose a Matrix into Low-Rank and Sparse
  Components}}.
\newblock R package version 0.2.3.

\bibitem[Todorov, 2016]{rrcovhd2016}
Todorov, V. (2016).
\newblock {\em {rrcovHD: Robust Multivariate Methods for High Dimensional
  Data}}.
\newblock R package version 0.2-5.

\bibitem[Vakili, 2018]{fasthcs2018}
Vakili, K. (2018).
\newblock {\em {FastHCS: Robust Algorithm for Principal Component Analysis}}.
\newblock R package version 0.0.6.

\bibitem[Xie et~al., 1993]{Xie:1993}
Xie, Y.~L., Wang, J.~H., Liang, L.~X., Sun, X. H.~S., and Yu, R.~Q. (1993).
\newblock {Robust Principal Component Analysis By Projection-Pursuit}.
\newblock {\em Journal of Chemometrics}, 7(6):527--541.

\end{thebibliography}


\end{document}


