\subsection{The Emotion Detection Models}

The emotion detection model detects the emotion of the speaker in a given audio clip. During the pre-deployment study, we looked into the state-of-the-art solutions for emotion detection. Table \ref{tab:sota_emotion_detection} shows the performance of several existing state-of-the-art solutions using various datasets of emotional utterances. In this Section, we aim to test the emotion detection model: is it possible to perform comprehensive and realistic pre-deployment testing to improve post-deployment success?

\begin{table}
  \begin{tabular}{ccccc}
    \toprule
    Work & Dataset(s)  & Accuracy\\
    \midrule

    Beard et al. \cite{beard2018multi} & SAVEE, CREMA-D &  41.2\% \\
    VGG \cite{simonyan2014very} & EMO-DB & 43.0\% \\
    Huang et al. \cite{huang2018stochastic} & RAVDESS, SAVEE &  60.8\% \\
    Ghaleb et al. \cite{ghaleb2019multimodal} & RAVDESS & 67.7\%  \\
    Ghaleb et al. \cite{ghaleb2019multimodal} & CREMA-D & 74.0\% \\
    SpeechBrain \cite{ravanelli2021speechbrain} & IEMOCAP & 78.7\%\\ 
    
  \bottomrule
\end{tabular}
\caption{State-of-the-art emotion detection algorithms on different emotional speech datasets. This Table is adapted from a table in the work \cite{gao2021emotion}. As we can see, the best performing state-of-the-art algorithm's accuracy is capped at 80\%. Since SpeechBrain achieves the highest performance, we select it as our emotion detection model.}
\label{tab:sota_emotion_detection}
\end{table}

\subsubsection{Pre-deployment Stage Assessment}
At first glance, Table \ref{tab:sota_emotion_detection} suggests that 4 solutions are not viable, but that SpeechBrain \cite{ravanelli2021speechbrain} is the best candidate among all the state-of-the-art algorithms, given its high performance on the dataset IEMOCAP \cite{busso2008iemocap}. But is it capable of overcoming the challenges (reverberation, deamplification, and background noise) that are present in the real, designated environment (a dyad's home)? To answer that question, we need to look into the dataset IEMOCAP \cite{busso2008iemocap}, on which it is evaluated. If the dataset has accounted for the challenges, i.e. during the data collection and processing process, the audio samples are touched by the effects of the three challenges, then we conclude that the evaluation result yielded by SpeechBrain indicates that it had overcome the three challenges perceived to be present in the real, designated environment that is a dyad's home. The dataset IEMOCAP \cite{busso2008iemocap} indicates that the audio clips are collected when there are furniture items in the room, instead of an empty acoustic studio, which suggests that the audio clips in IEMOCAP \cite{busso2008iemocap} are touched by the effect of reverberation. The audio clips in IEMOCAP \cite{busso2008iemocap} are \emph{not} collected where a speaker is right next to the microphone. This suggests that the audio clips in IEMOCAP \cite{busso2008iemocap} are touched by the effect of deamplification. Last but not least, IEMOCAP is not collected in a studio environment and there was no indication that the indoor background noise events such as footsteps were deliberately removed. Therefore, this suggests that the audio clips in IEMOCAP are touched by the effect of background noise. Consequently, we conclude that IEMOCAP's audio samples on which SpeechBrain is evaluated on account for the challenges that are perceived to be present in the real, designated environment in which the emotion detection algorithm (SpeechBrain) is to be deployed. Here, we set the stage to prove (once again) the hypothesis that for an algorithm to work well in the real, designated environment in which it is envisioned to be deployed, during pre-deployment stage, it must show that it is capable of handling the challenges that are perceived to arise in the real, designated environment. In the meantime, we have observed that algorithms such as Huang et al. \cite{huang2018stochastic} are not likely to work sufficiently at post-deployment time. It is good to discover that these solutions are not likely to work at post-deployment time, because this helps us filter out such existing solutions. 

Note that none of the state-of-the-art approaches in Table \ref{tab:sota_emotion_detection} include TV sounds as one of the acoustical realisms that they need to address. In the future, we plan to develop an emotion detection algorithm that takes TV sounds into consideration.



\subsubsection{Post-deployment Stage Assessment}

In the following paragraphs we describe how we evaluate our emotion detection detection model post-deployment. Out of the audio clips we collected from each dyad, we first select all samples that are classified by the emotion detection model and conflict detection model as anger speech. Then, we randomly select the same number of audio clips from all the samples by that dyad that are not classified as anger speech. Each of the audio clips is \textbf{manually labeled} based on the emotion in the clip by the labelers. Table \ref{tab:eval_emotion} describes our emotion detection model's performance on the labeled samples: For the 1st dyad, there are 233 samples. For the 2nd dyads, there are 392 samples. For the 3rd dyads, there are 281 samples. For the 4th and 6th dyads, there are 100 samples each. For the 5th dyads, there are 80 samples.

During the post-deployment stage assessment, researchers should let third-party labellers label the data to obtain ground truth, instead of letting the participants do the labelling, because the participants are not necessarily good at discerning their own emotion (and if they are in a verbal conflict) if they are not trained. The XYZ system detects if a person is in a verbal conflict or is angry. Initially, we sought to validate our system's performance by survey questions using EMA, similar to many other studies. However, we quickly found out that their responses did not always agree with the decision of the system. Who was wrong: the caregiver or our machine learning solutions? There is existing literature \cite{goerlich2018multifaceted} stating that people not trained to recognize their emotions are often bad at recognizing their own emotions. To investigate, we employed 5 labellers who are approved by the IRB to listen to and label the saved clips of the participants' voices. Their labelling suggests that in some cases, the labellers annotation did not agree with the participants' self reported emotional states. This data supports the claim that people are often bad at recognizing their emotions. We were able to verify the claim only because we planed, during pre-deployment time, to save all the raw data during the full deployment time. We found that post deployment labeling is better than EMA surveys and also supports determination of ground truth which, in turn, provides a better accuracy assessment of the acoustic classifiers. Determining ground truth from deployment time data is very important and often not done in many studies. 

The emotion detection model achieves an f1 score of 85.3\% to 97.4\% on the six dyads. According to Table \ref{tab:eval_emotion}, the emotion detection algorithm (SpeechBrain)'s performances in all six homes are satisfactory, highly efficient at identifying the emotions in each clip in each of the six homes. The success of the emotion detection algorithm demonstrated by Table \ref{tab:eval_emotion} proves our hypothesis: for an algorithm to be able to work satisfactorily post-deployment in a real, designated environment, it must demonstrate that it is able to overcome the challenges perceived to arise in that environment during pre-deployment time. The high performance of the emotion detection model at post-deployment time suggests that our way to perform comprehensive and realistic pre-deployment testing is effective at improving post-deployment success. Table \ref{tab:eval_emotion} also includes the performance of VGG \cite{simonyan2014very} which yields bad results (an accuracy score of 43.0\%) at pre-deployment time. As we can see, the model that achieves bad performance at pre-deployment time also achieves bad performance (an average of an f1 score of 50.1\%).

\begin{table}[]
\centering
\scalebox{0.85}{
\begin{tabular}{ccccccc}
\toprule
          & Dyad 1  & Dyad 2  & Dyad 3  & Dyad 4 & Dyad 5 & Dyad 6\\
\hline
SpeechBrain       & 88.8\%  & 87.2\%   & 91.8\%    & 92.9\% & 85.3\%  & 97.4\%   \\
VGG         & 44.6\%    & 65.4\%     & 48.3 \%  & 45.3\% & 53.3\% & 61.5\%  \\
\bottomrule
\\
\end{tabular}
}
\caption{The post-deployment evaluation results for the emotion detection model (SpeechBrain) as well as the VGG model \cite{simonyan2014very}, \textbf{which we did not use} because at pre-deployment time it achieves bad performance with an accuracy score of 43.0\%. As we can see, it also achieves bad performance on the post-deployment data. The measurement in this Table is f1 score.}
\label{tab:eval_emotion}
\end{table}


