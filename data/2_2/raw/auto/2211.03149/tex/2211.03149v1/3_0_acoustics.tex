\section{An Overview of the XYZ System}
\label{sec:acoustics}
\begin{figure}
    \centering
    \includegraphics[width=0.5\textwidth]{figures/acoustics.PNG}
    \caption{Overview of the XYZ system comprised of four main components: the voice activity detection (VAD) model, the speaker identification (SID) model, the emotion detection model, and the conflict detection model.}
    \label{fig:XYZ_acoustic}
\end{figure}


In this section, we describe the XYZ System, which includes the voice activity detection (VAD) algorithm, the speaker identification (SID) algorithm, the emotion detection algorithm, and the conflict detection algorithm. Three out of the four algorithms are off-the-shelf. The VAD algorithm is Google's transcription services. The SID algorithm is the WavLM algorithm developed by Microsoft \cite{chen2022wavlm}. The emotion detection algorithm is developed by SpeechBrain \cite{ravanelli2021speechbrain}. We developed the conflict detection by ourselves, and the reference to the conflict detection paper is left blank to preserve our anonymity. 
In later sections of this paper, we thoroughly tested the off-the-shelf algorithms to make sure that they meet our needs in the XYZ system. We also argue in later sections that if there exist algorithms that can satisfy the needs of a system, there is no need to develop those algorithms again by the researchers themselves; time and resources can be spared to develop the algorithms that are needed by the system but not available as off-the-shelf ones (such as our conflict detection model). In other words, there is no need to reinvent the wheel. 

In Figure \ref{fig:XYZ_acoustic}, the microphone placed in a central place in a room constantly listens to the ambient environment. The audio stream is sliced into 5-second audio clips and send to the voice activity detection (VAD) model to decide if a given clip contains human voice. The choice of each audio clip being 5-second is based on the observation from previous works that 5-second is long enough to be indicative of the speakers' emotions \cite{gao2021emotion}. The XYZ System discards those audio clips that are invalid; i.e., they contain no human voice. The valid audio clips are sent to the speaker identification (SID) model to detect if the audio clips contain the voice of registered speakers. If yes, they are sent to the emotion detection and conflict detection models, in parallel. The two models each produce vectors in respect to if the speaker(s) in an audio clip is/are angry or is/are in a verbal conflict.

Note that the purpose of this paper is to demonstrate the necessary steps to be accomplished during the development stage before deployment to increase deployment time success, and show where continued work is still necessary. The purpose of this paper is not about how novel the algorithms used in the XYZ System are, although we show that the algorithms are effective at doing their respective jobs and satisfying the research needs.

%Finally, the conflict detection algorithm \cite{gao2022enforced} is used to detect if a person is in conflict or not. The conflict algorithm is independent of the previous CNN we described. We apply a state-of-the-art transfer learning solution called the Enforced Transfer \cite{gao2022enforced} to transfer learning from the CNN model described and used in the previous paragraph, which is a five-class CNN, to conflict detection, which is a 2-class classification problem. Since this is a transfer learning problem, we need to first describe the source and target datasets. The source datasets are the synthetic dataset \cite{gao2021emotion} used to train the five-class CNN, whereas the target dataset has conversations by 19 couples discussing topics that they disagree on (the IRB has been obtained to listen and record the conversations). The Enforced Transfer reuses part of the original classifier (the 5-class CNN). Then, a source Encoder, a target Encoder, and a Probe are attached after the reused part of the original classifier. The Probe differentiates if a sample is likely from the source domain or from the target domain. If the sample is likely from the source domain (we use out-of-distribution detection to determine the likelihood and we will describe the details in later paragraphs), then it is sent to a set of dense layers trained on the source dataset for final classification. If a sample is likely from the target domain, then it is sent to a set of dense layers trained on the target dataset for final classification. 
