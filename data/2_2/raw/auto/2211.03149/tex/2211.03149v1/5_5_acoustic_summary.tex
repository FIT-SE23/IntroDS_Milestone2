\subsection{Summary}

In this Section we briefly summarize our findings in Section \ref{sec:evaluation}. 

The first finding is that, to ensure post-deployment success of an algorithm, during pre-deployment time it must be rigorously tested on samples that are touched by the challenges that are perceived to be present in the real, designated environment during post-deployment time. This finding is confirmed by the pre-deployment stage assessment results and post-deployment stage assessment results of the VAD model, the SID model, the emotion detection model, and the conflict detection model. 
%In other words, during the pre-deployment stage, each of the models is tested on samples that are touched by the effects of reverberation, background noise, and deamplification (the challenges perceived to be present in the real, designated environment). 
%During the post-deployment stage, each of the models performs satisfactorily in the real, designated environment which are the homes of the dyads. The high performances of our VAD, SID, emotion detection, and conflict detection models suggest that it is possible to perform comprehensive and realistic pre-deployment testing to improve post-deployment success.

The second finding is that in the acoustic processing pipeline we should always go for off-the-shelf solutions first before developing your own algorithm. Many acoustic functions have been under study for many years and excellent solutions exist. Yet, rigorous testing is still required since not all of the available solutions will work for the environment where the system will be deployed. In our case, the VAD, SID, and emotion detection models are off-the-shelf. Since there are no state-of-the-art conflict detection models that \emph{only use (the prosody of the) voice} to detect verbal conflict, this has to be developed and will not have the luxury of having solutions refined many many researchers over many years. This likely is why the performance is lower than the well developed acoustic functions. 

%Our third finding, following the second finding, is that researchers should make sure the thoroughly test the off-the-shelf solutions. We tested off-the-shelf solutions for VAD (in Table \ref{tab:eval_pre_vad}), SID (in Table \ref{tab:eval_pre_sid}), and emotion detection (in Table \ref{tab:sota_emotion_detection}) during the pre-deployment stage, using testing samples that are touched by deamplification, background noise, and reverberation (three challenges perceived to be present in the real, designated environment).

%Our fourth finding is that during post-deployment stage assessment, researchers should let third-party labellers to label the data to obtain ground truth, instead of letting the participants (the caregivers of the dyads) to do the labelling, because the participants are not necessarily good at discerning their emotion (and if they are in a verbal conflict) if they are not trained. The XYZ system detects if a person is in a verbal conflict or is angry. Initially, we sought to validate our system's performance by survey questions using EMA, similar to many other studies. However, we quickly found out that their responses did not always agree with the decision of the system. Who was wrong: the caregiver or our machine learning solutions? There is existing literature \cite{goerlich2018multifaceted} stating that people not trained to recognize their emotions are notoriously bad at recognizing their own emotions. To investigate, we employed 5 labellers who are approved by the IRB to listen to and label the saved clips of the participants' voices. Their labelling suggests that in some cases, the labellers annotation did not agree with the participants' self reported emotional states. This data supports the claim that people are often bad at recognizing their emotions. We were able to verify the claim only because we planed, during pre-deployment time, to save all the raw data during the full deployment time. We found that post deployment labeling is better than EMA surveys and also supports determination of ground truth which, in turn, provides a better accuracy assessment of the acoustic classifiers. Determining ground truth on deployment time data is very important and often not done in many studies. 