\subsection{Voice Activity Detection}
The VAD model filters silence and other sounds that are not produced by the human vocal tract. Since the acoustic system is constantly listening to the environment, we do not want to activate the emotion and conflict classifiers when an input sound window contains no human speech. As a result, the VAD model is an important and necessary component in the XYZ System. In this Section, we describe testing on the VAD model for the question: Is it possible to perform comprehensive and realistic pre-deployment testing to improve post-deployment success? Note that the acoustic realisms that the VAD model faces are deamplification, reverberation, and (non-speech) background noise, so in our pre-deployment stage assessment we seek to find a VAD solution that is robust against these three types of acoustical realisms.

\subsubsection{Pre-deployment Stage Assessment}

During pre-deployment time, we first looked into several state-of-the-art VAD algorithms. In particular, we studied the performance of a set of SOTA VAD algorithms on the Aurora-2 database \cite{hirsch2000aurora}. The performance of the algorithms on the Aurora-2 database is a good indicator of how they might perform in the real world because Aurora-2's speech samples are mixed with noise collected from realistic settings such as streets, airports, and cars. Table \ref{tab:eval_pre_vad} is a list of existing SOTA VAD algorithms' accuracy scores on the testing set of Aurora-2. Unfortunately, as we can see, the highest-performing one is rVAD, which only achieves an accuracy score of 66.23\%, which is far from being usable in the real world. In other words, there exists solutions in the literature that do not work on datasets with deamplification, reverberation, and background noise. It is good to discover that these solutions are not likely to work at post-deployment time, because this helps us filter out existing solutions so that we won't use those solutions.

\begin{table}[]
\centering
\begin{tabular}{cc}
\toprule
Existing SOTA VAD         & Accuracy  \\
\hline
VQVAD       & 45.66\%          \\
Sohn et al. & 31.21\%          \\
Kaldi Energy VAD    & 11.72\%       \\
DSR AFE     & 40.07\%           \\
rVAD        & 66.23\%           \\
\bottomrule
\\
\end{tabular}
\caption{Evaluation of existing VAD algorithms on the Aurora-2 database. This Table is reported by Tan et al. \cite{tan2020rvad}.}
\label{tab:eval_pre_vad}
\end{table}

However, there was another algorithm, the Google Speech Recognition (GSR) algorithm, that had not been evaluated on a dataset that contained the three environmental distortions: reverberation, deamplification, and background noise. As a result, we next evaluated the Google Speech Recognition solution. we aimed to evaluate it in a comprehensive way to demonstrate that it would be robust against environmental distortions such as reverberation, deamplification, and background noise. Again, this is to set up the necessary condition to prove our hypothesis that a solution, during the pre-deployment stage, must be able to deal with the unique challenges given the real, designated environment in which it will be deployed. 
%In the case of the VAD algorithm, the unique challenges are the reverberation, deamplification, and (in-home) background noise effects.

To do so, we collected a dataset that contains diverse environmental distortions: first, we collected the clean samples - samples that are not environmentally distorted, by having an individual talk next to the microphone for 5 minutes. The 5-minute clip was then sliced into 60 5-second segments, each of which is individually labeled as positive if it contained a human voice, or negative if it did not contain human voices. Note that the individual took long pauses intentionally to make sure that there were negative samples. Second, we collected the audio clips that were deamplified and contained background noise. To do so, we took copies of the clean samples. For each of the copies, we randomly deamplify them by m decibels (0 < m < 12) as per the practice of a previous work on emotion detection \cite{gao2021emotion}. Then, we randomly picked household sounds from the household ambience dataset \cite{mesaros2016tut}. Table \ref{tab:homenoise} lists the events that occur in the dataset. Note that each of the ambience sounds is greater than 5 seconds, so we randomly picked a segment from it that was 5-seconds long, and overlaid it with a deamplified clip. We repeated this process for all 60 deamplified clips. Third, we created the data for reverberated speech. To do so, we took another set of copies of the clean samples, and overlaid each of them with reverberation that was described by the combination of the three parameters: the wet/dry ratio $r$, diffusion $d$, and decay factor $f$. Finally, we created samples that are deamplified, noise-contaminated, and reverberated. To do so, we took a set of copies of the 60 deamplified and noise-contaminated samples, and overlaid them with the same reverberation effect as the samples that only contained reverberation effect and nothing more. In the end, we have 60 clean samples, 60 deamplified and noise-contaminated samples, 60 reverberated samples, and 60 samples that had all three environmental distortions. As a result, we claim that we created a dataset that was comprehensive enough to account for all three kinds of environmental distortions.

\begin{table}
    \centering
    
    \begin{tabular}{cc}
    \toprule
    Event & Instances\\
    \midrule
    (object) rustling & 60 \\
    (object) snapping  & 57 \\
    cupboard & 40 \\
    cutlery & 76\\
    dishes & 151\\
    drawers & 51\\
    glass jingling & 36 \\
    object impact & 250 \\
    people walking & 54 \\
    washing dishes & 84 \\
    water tap running & 47 \\
    \bottomrule
\end{tabular}
\caption{Events that are present in the background noise collected from real homes from the dataset \cite{mesaros2016tut}. All of them are covered in the process of contaminating audio samples with background noise. Note that this list do not include sounds from the tv, which are very important to make sure the robustness of the emotion detection model and conflict detection model.}
\label{tab:homenoise}
\end{table}

We evaluated GSR on the dataset that we just created. \textbf{GSR achieved an accuracy score of 95.83\%}, correctly classifying 230 out of the 240 samples each of which accounted for the environmental distortions to a certain degree. The high performance led us to decide to deploy GSR as our VAD model since, during the pre-deployment stage assessment, it is shown to be robust against the challenges that it is about to encounter in the real, designated environment: reverberation, background noise, and deamplification. 

%In the following paragraphs, we seek to confirm if the hypothesis that, during a pre-deployment stage, an about-to-be-deployed algorithm being able to overcome the challenges perceived to be present in the real, designated environment is going to perform well in that real, designated environment.

%\begin{table}[]
%\centering
%\begin{tabular}{cc}
%\toprule
%          &  Accuracy\\
%\hline
%GSR & 95.83\%   \\
%\bottomrule
%\\
%\end{tabular}
%\caption{The evaluation results on the GSR pre-deployment on the dataset that we created, which accounted %for the three environmental distortions: room reverberation, deamplification, and indoor background noise.}
%\label{tab:eval_pre_vad}
%\end{table}

\subsubsection{Post-deployment Stage Assessment}

Using post-deployment data on six completed dyads, we validate how well the chosen solution worked in practice. Table \ref{tab:eval_post_vad} shows the evaluation results of the VAD model on the dyads. We randomly select samples generated by each dyad during their deployment, and have human labelers label them if they are of human speech or not. We obtained 100 samples for all the dyads. The high performance of the VAD model indicates that this part of our system is highly effective at filtering out non-human speech samples such as background music (without lyrics) and footsteps. It is noted that the VAD does not filter out TV sounds if there is human speech in the sounds, such the voices of actors or news anchors. These unwanted sounds are filtered by the next model, SID.

The VAD model achieves an accuracy score of 94.0\% to 100\% on the six dyads. The high performance on post-deployment data validates our choice of the Google Speech Recognition in the pre-deployment phase. This implies that this VAD algorithm was originally made very robust to real world complexities. The high performance also indicates that, in order for the deployment to be successful, smart health groups using audio should perform pre-deployment tests with comprehensive real-world distortions. In addition, the high performance suggests that our hypothesis holds true - recall that our hypothesis is that, during the pre-deployment stage assessment, an about-to-be-deployed algorithm must be proven to overcome the challenges that are perceived to be present in the real, designated environment in order for it to perform well in said environment. The high performance on post-deployment data also indicates that it is sometimes possible to perform comprehensive and realistic pre-deployment testing to improve VAD post-deployment success.

\begin{table}[]
\centering
\scalebox{0.9}{
\begin{tabular}{ccccccc}
\toprule
          & Dyad 1  & Dyad 2  & Dyad 3 & Dyad 4 & Dyad 5 & Dyad 6\\
\hline
GSR & 100\%  & 100\%   & 94.0\%    & 95.0\%  & 100\%   & 98.0\% \\
\bottomrule
\\
\end{tabular}}
\caption{The evaluation results for the voice activity detection model on the dyads. The high accuracy scores achieved from the dyads indicate that the VAD algorithm (Google Speech Recognition) is highly effective at differentiating non-speech from human speech audio samples.}
\label{tab:eval_post_vad}
\end{table}
