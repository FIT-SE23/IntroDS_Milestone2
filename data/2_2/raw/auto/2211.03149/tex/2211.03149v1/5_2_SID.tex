\subsection{Speaker Identification (SID)}

The SID model determines the identity of a speaker. The SID is a crucial part of the Acoustic System because we only want the voices of the caregiver and patient to be sent to the emotion and conflict detection models. However, in real deployments, voices from the TV and visitors must be filtered out. In this Section, we test SID model to answer this question: is it possible to perform comprehensive and realistic pre-deployment testing to improve post-deployment success? Note that the acoustical realisms that the SID faces, in addition to (non-speech) background noise, reverberation, and deamplification, also include sounds from the tv such as the dialogues from tv characters, for the presence of another person's voice in an audio sample could confuse the speaker identification model.

\subsubsection{Pre-deployment Stage Assessment}
During pre-deployment time, we investigated a state-of-the-art SID algorithm, the Google Speaker Identification API. However, the API asks us to input the maximum number of speakers there can be in a clip. This is impractical because a dyad can have the TV on and there could be many people's voices from the TV, or there may be multiple visitors. It is good to discover that this solution is not likely to work at post-deployment time, because this helps us filter out existing solution(s).

Now we describe how we test to make sure the about-to-be-deployed SID algorithm developed by Microsoft \cite{chen2022wavlm} is robust to environmental distortions such as reverberation, background noise, and deamplification. Again, this is to verify our hypothesis that for an algorithm to be successful in the real, designated environment, it must be able to overcome the challenges present in the real, designated environment during the pre-deployment stage. In our case, the challenges are reverberation, deamplification, non-speech background noise and TV sounds. Specifically, we have two persons, P1 and P2, each of whom spoke next to the microphone for 2.5 minutes. Then, for each of their voice files, we sliced it into 28 audio samples. Because these 56 (28$\times$2) samples were collected when the speakers were right next to the microphone, they were considered clean speech, free of the three types of environmental distortions. We needed to craft environmentally distorted samples out of the 56 clean samples to ensure that the testing samples accounted for both clean and environmentally distorted samples. To do so, we copy each of the 56 clips and deamplify them by randomly choosing a real number between 0 and 12 decibels. Then, we randomly chose a noise clip from Table \ref{tab:homenoise} as well as TV sounds we recorded using a microphone, out of which we randomly chose a consecutive 5-second segment to be overlaid with one of the copies. This guaranteed samples that were deamplified and contaminated with noise, and the last step was to reverberate it. Again, the reverberation effect is described by the three parameters: the wet/dry ration $r$, diffusion $d$, and decay factor $f$, as per the practice of a previous work \cite{salekin2017distant}. When we reverberated a (noise-contaminated and deamplified) copy, the values of $r$, $d$, and $f$ are randomly chosen. In total, we had 112 samples, 56 of which belonged to P1 and the other 56 belonged to P2. We fed the 112 samples to our SID model. \textbf{The SID model achieves an f1 score of 85.7\% on P1 and 92.8\% on P2}. The high performance of the SID model led us to believe that it was reasonably robust to reverberation, deamplification, background noise, and TV sound.


%\begin{table}[]
%\centering
%\begin{tabular}{cc}
%\toprule
%          &  f1 score\\
%\hline
%P1 & 85.7\%   \\
%P2 & 92.8\%   \\
%Avg & 89.2\%    \\
%\bottomrule
%\\
%\end{tabular}
%\caption{The evaluation results on the SID algorithm that we created at pre-deployment on the dataset that we created, which accounted for the three environmental distortions: room reverberation, deamplification, and indoor background noise.}
%\label{tab:eval_pre_sid}
%\end{table}

\subsubsection{Post-deployment Stage Assessment}

To validate post-deployment success, from all audio samples that our speaker identification algorithm identifies to contain the voice of the caregiver or the patient, or both, we randomly chose 28 from the first dyad, 28 from the second dyad, and 28 from the third dyad, 100 from the fourth dyad, 80 from the fifth dyad, and 100 from the sixth dyad. The results are reported in Table \ref{tab:eval_sid}. In the following sentences we describe how we obtain the f1 scores in Table \ref{tab:eval_sid}. For a sample, if it only contains the voice of the caregiver, then it is labeled as belonging to the caregiver; it if only contains the voice of the patient, then it is labeled as belonging to the patient. If it contains voices from both the caregiver and patient, then it is labeled as belonging to both. Otherwise, it labeled as belonging to neither. With this labeling scheme, we obtain the positives and negatives of the caregiver's voice and the positives and negatives of the patient's voice. The SID model can label a sample as belonging to the caregiver, belonging to the patient, or neither. As a result, we obtain the results in Table \ref{tab:eval_sid} in which we report the f1 scores to measure the performance of our SID model for both the caregiver and patient of each dyad. The SID model achieves an f1 score in the range of 93.1\% to 97.4\% for the caregivers and 91.6\% to 98.3\% on the patients in the six dyads. The high performance in Table \ref{tab:eval_sid} indicates that our SID algorithm is effective at picking out the voices by the caregiver and the patient in each home in their real home environment. Given that the SID algorithm was specifically assessed to see if it could overcome the challenges (reverberation, deamplification, and background noise) present in the real, designated environment (homes), we have shown that for an algorithm to be successful in the real, designated environment, it must be able to overcome the challenges present in the real, designated environment during the pre-deployment stage. The high performance of the SID during the post-deployment time suggests that our way to perform comprehensive and realistic pre-deployment is effective at improving post-deployment SID success. Note that we only validate the SID solution on the voices of the caregiver and patient of each dyad, because at post-deployment time, the SID solution filtered out voice samples that belonged to neither. As a result, we only have samples that are labelled by the SID solution as either the caregiver or the patient. For samples that made through the SID solution, we have the performance reported in Table \ref{tab:eval_sid}.

In Table \ref{tab:eval_sid_other} we report the f1 score of a model \cite{lecun1995convolutional} that we did not use because at pre-deployment time it achieves bad performance (an f1 score of 79.3\% on P1 and an f1 score of 71.2\% on P2). As we can see, this model also achieves bad performance on the post-deployment data. This indicates that at pre-deployment time, the model that performs badly also performs badly at post-deployment time.

\begin{table}[]
\centering
\scalebox{0.9}{
\begin{tabular}{ccccccc}
\toprule
          & Dyad 1  & Dyad 2  & Dyad 3 & Dyad 4 & Dyad 5 & Dyad 6\\
\hline
Caregiver & 94.5\%  & 97.4\%    &  95.7\%   & 93.1\%    & 92.0\%    & 89.2\% \\
Patient &   94.6\%  & 95.9\%    &  96.0\%   & 94.6\%    & 91.6\%    & 98.3\% \\
\bottomrule
\\
\end{tabular}}
\caption{The evaluation results for the speaker identification model on the dyads. The results are the f1 scores.}
\label{tab:eval_sid}
\end{table}

\begin{table}[]
\centering
\scalebox{0.9}{
\begin{tabular}{ccccccc}
\toprule
          & Dyad 1  & Dyad 2  & Dyad 3 & Dyad 4 & Dyad 5 & Dyad 6\\
\hline
Caregiver &   57.1\% & 71.4\%    & 83.6\%   & 52.6\%    & 44.3\%    & -\\
Patient &   74.8\%  & 75.9\%    &  79.5\%   & 77.7\%    & 74.2\%    & 97.0\%\\
\bottomrule
\\
\end{tabular}}
\caption{The post-deployment evaluation results for a speaker identification model that \textbf{we did not use} because it achieved bad performance pre-deployment time. As we can observe, its performance on all dyads is bad at post-deployment time.}
\label{tab:eval_sid_other}
\end{table}