\subsection{The Conflict Detection Model}

For conflict detection, currently, there is no available conflict detection algorithm that is acoustics-based. Therefore, we developed our own conflict detection algorithm. In this Section, we aim to test on the conflict detection model: is it possible to perform comprehensive and realistic pre-deployment testing to improve post-deployment success?

\subsubsection{Pre-deployment Stage Assessment}
%The conflict detection algorithm is developed by us. We develop the conflict detection algorithm because there is no available off-the-shelf algorithms that use voice to detect verbal conflict. 
Here we briefly describe how the new algorithm we developed is trained and why the training process makes it specifically account for the (three) challenges that arise in the real, designated environment in which the algorithm is going to be deployed. The training and testing samples are from 19 couples and each sample is labeled conflict if the content of the sample indicates that the couple are in a verbal conflict. It is labeled non-conflict if the couple are not in a verbal conflict. Since the samples are already collected from home-environments, de-amplification and reverberation are accounted for, but the samples are free of background noise. As a result, we mix each of the samples with background noise by randomly selecting a segment from a randomly chosen indoor background noise sample in Table \ref{tab:homenoise} and overlaying that segment with each sample. Out of the samples, there are 3,072 in the training set and 1,009 in the testing set. As a result, both the training and the testing set accounts for a variety range of indoor environmental distortions. Since the training samples are touched by deamplification, reverberation, and background noise, our conflict detection algorithm trained on them is designed to be able to handle the three challenges (deamplification, reverberation, and background noise).

The conflict detection model's performance on the testing set achieves an f1 score of 93.1\%. The high performance suggests that the conflict detection is robust against environmental distortions such as reverberation, background noise, and deamplification. This help us set the stage to prove our hypothesis that, for an algorithm to work sufficiently in the real, designated environment in which challenges are perceived to be present, the algorithm must show that, during pre-deployment stage, it is able to handle the challenges. Our conflict detection algorithm has indicated that during pre-deployment stage, it is able to handle the three challenges: reverberation, deamplification, and background noise. Note that we do not include TV sounds as one of the acoustic realisms that the conflict detection model needs to address. In the future, we plan to develop a conflict detection algorithm that takes TV sounds into consideration.

%\begin{table}[]
%\centering
%\begin{tabular}{cc}
%\toprule
%          & F1 \\
%\hline
%Conflict Detection & 93.1\%   \\
%\bottomrule
%\\
%\end{tabular}
%\caption{The conflict detection model's performance on the testing set we created that contains 1,009 %samples that contained background noise, reverberation effect, and deamplification effect.}
%\label{tab:eval_pre_conflict}
%\end{table}

\begin{table}[]
\centering
\begin{tabular}{ccccccc}
\toprule
          & Dyad 1  & Dyad 2  & Dyad 3 & Dyad 4 & Dyad 5 & Dyad 6\\
\hline
Ours       & 63.4\%    & 65.9\%    & 70.7\%  & 86.2\% & 82.7\% & 90.1\%\\
\bottomrule
\\
\end{tabular}
\caption{The evaluation results for the conflict detection model. The measurement is f1 score.}
\label{tab:eval_conflict}
\end{table}

\subsubsection{Post-deployment Stage Assessment}
In the post-deployment time, we seek to prove our hypothesis that, for an algorithm to work well post-deployment time in the real, designated environment in which it is going to be deployed, during pre-deployment stage it must show that it is capable of overcoming the challenges that are present in the real, designated environment. Our conflict detection algorithm has showed that it is capable of overcoming the challenges (it achieves an f1 score of 93.1\% pre-deployment time). However, is it going to work well in the post-deployment time?

Table \ref{tab:eval_conflict} shows our conflict detection algorithm's performance during the post-deployment time at the six homes. Now we explain how we obtain the f1 score results in Table \ref{tab:eval_conflict}. If a clip is labeled by the labelers such that it contains verbal conflict and the classifier also thinks this clip contains verbal conflict, then it is a hit. If the clip is labeled by the labelers as not containing verbal conflict and the classifier also thinks that it does not contain verbal conflict, then it is a hit. All other cases are misses (for example, the labelers think that a sample contain verbal conflict but the classifier fails to classify it as so). By looping through all samples produced by a dyad, we produce an f1 score on that dyad. From Table \ref{tab:eval_conflict}, we observe that the sixth dyad achieves the best performance with an f1 score of 90.1\% while the first dyad achieves the lowest performance with an f1 score of 63.4\%. For each of the dyads, we observe a drop in performance compared to 93.1\% obtained when the same model is evaluated on the dataset containing speech samples from the 19 couples. This indicates that despite our effort in mitigating environmental distortions, the effects of the environmental distortions such as room reverberation, background noise, and the deamplification effect are not fully mitigated. But the relatively satisfactory performance of the conflict detection model on dyads 4, 5 and 6 indicates that our way to perform comprehensive and realistic pre-deployment testing to improve post-deployment success is effective for exapected conditions.

We also investigate why the performance of the conflict detection model is lower in dyads 1-3 (f1 score of 63.4\% to 70.7\%). Upon communicating with the dyads, we learned that dyad 1 moved the system (which included the microphone) to the hallway which is very far away from the usual places that the participants were speaking. Dyad 2 had a construction team rennovating their home, so there was a lot of construction noise to confuse the conflict detection model. When we developed the conflict detection model, we did not take construction noises into consideration. In dyad 3, the caregiver's voice was always very low, almost inaudible, and our conflict detection model was not designed to handle such low-to-inaudible voice samples.
