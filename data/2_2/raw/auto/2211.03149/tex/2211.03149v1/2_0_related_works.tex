\section{Related Works}
\label{sec:related_works}
In this related works section, we discuss existing smart home technologies as well as how human behavior affects the efficacy of smart technologies. We also briefly talk about emotion detection and conflict detection using voice, since the XYZ system uses the voice of its participants to detect if they are angry and if they are in a verbal conflict.

\subsection{Methodology for Deploying Smart Health Technologies}

Zeadally et al. \cite{zeadally2019smart} argue that deploying smart health technologies can improve healthcare availability, cost, and access. However, there have not been enough works for the methodology for deploying smart health technologies (usually in smart homes). Bellandi et al. \cite{bellandi2021design} is among the first works that propose a design methodology that matches smart health requirements. They argued that due to the rapidly aging world population, smart health technologies are about to playing an even more important role in caring for the aging population at homes. They propose that the perspective of deploying those smart health technologies at homes should be user-centered. To select a solution to be deployed, researchers need to perform domain analysis (surveying the state of the art), requirement gathering (stakeholder elicitation), requirement integration (determining the consistency of the requirements), and then finally solution selection. 

%\subsection{Smart home technologies}
%Previous smart technologies that can be used in a smart home setting include technologies that use wearable body sensors \cite{movassaghi2014wireless, li2012accelerometer, chen2006wearable, zhang2006fall}, technologies that use implantable body sensors \cite{hadjem2013early}, and technologies that use remote health monitoring sensors \cite{lipprandt2009osami, alonso2013multimodal, devillers2003emotion, hossain2018emotion, naveenkumar2019audio}. Wearable body sensors include accelerometers which can, e.g.,  be used to measure if an elderly patient suddenly falls. Implantable body sensors include glucose sensors that can measure important physiological signals indicative of one's health. Remote health monitoring is often based on the Internet of Things and requires that the wearable and implantable sensors send the collected signals to a medical facility for further analysis. One caveat with the aforementioned smart home technologies is that they are often developed in lab or controlled settings and then directly applied to the real smart homes that are inherently more complex than the tested settings. In other words, the authors often make no comment on how the transition took place and what problems they had to address in order to make the transition possible.

\subsection{Human behavior that affects the efficacy of smart technologies}
There has not been enough work on how human behavior affects the efficacy of smart technologies. A previous work \cite{yaghoubi2010factors} hypothesizes that the perceived usefulness directly contributes to the user's attitude towards the technology. Nikou et al. \cite{nikou2019factors} suggest that perceived innovation and cost directly contribute to if the user will adopt a smart technology. Perri et al. \cite{perri2020smart} point out that the attitude of the user directly affects the adoption of smart technologies, which we would like to extend to that their attitude directly affects the efficacy of smart technologies as shown by the observation of a user of our XYZ system: he was not very compliant and sometimes pulled out the wires to the laptop on which we ran our system. As a result, the computer was out-of-power and the system was unable to monitor the participant's emotion for a period.

\subsection{Voice Activity Detection}
There have been lots of work on voice activity detection (VAD) so we only discuss the recent advances in the field. MarbleNet \cite{jia2021marblenet} uses deep residual network consisting of blocks of 1-D time-channel separable convolution, able to achieve the state-of-the-art performance with the advantage that the number of their parameters is significantly smaller. The robustness of MarbleNet is also extensively studied to demonstrate that it is robust to real-world acoustical distortions. Using teacher-student training, Dinkel et al. \cite{dinkel2021voice} also strive to train a model that is robust to real-world acoustic distortions. Dinkel et al. identify that traditional VAD algorithms are trained on data devoid of such acoustic distortions, and therefore their usage is limited to data without the acoustic distortions that are inevitable in the real world, rending them unable to perform well in real-world settings. Other works on VAD include Wang et al. \cite{wang2022cross} that uses a cross channel attention based model to achieve voice activity detection in the M2met challenge, Braun et al. \cite{braun2021training} that is specifically concerned about dealing with the robustness issue of many state-of-the-art models. What is worth-noting is that, some works developed for other purposes such as transcription, can be used as voice activity detection models. For example, the Google speech Recognition (GSR), a transcription service, outputs the transcribed sentence from an audio clip if that audio clip is speech, and it will throw an exception is the audio clip is silence. It is worth noting that although works such as MarbleNet \cite{jia2021marblenet} and Dinkel et al. \cite{dinkel2021voice} attempt to ensure that they work on datasets that account for realism to be encountered in real, designated environments in which the algorithms are to be deployed, they do not evaluate their post-deployment performances in the real, designated environments. Realisms that the VAD model deals with usually arise from background noise such as footsteps, and the VAD model needs to differentiate not only silence from human speech but also those background noises from speech. The realisms that the VAD model faces is simpler than the models that we discuss in the later sections, the speaker identification (SID) model, the emotion detection model, and the conflict detection model, which needs to deal with the tv sound as the speech from the tv could affect the classification performance of these models.

\subsection{Speaker Identification}
Again, the works in the field of speaker identification (SID) are abundant, so we only discuss the recent advances in the field. Chen et al. \cite{chen2021graph} introduce a graph-based speaker identification model that is reliant on speaker label inference. It is particularly concerned with the task of SID in household scenarios. WavLM \cite{chen2022wavlm} recognizes that the speech content by by speakers contains multi-faceted information such as the identities of the speakers, the content of the speech, and paralinguistics. WavLM is propsoed as a pre-trained model that can be used to be fine-tuned for the purpose of various speech recognition tasks such as speaker identification. Snyder et al. \cite{snyder2018x} proposes an xvector, the results of mapping variable-length spoken clips to fixed-dimensional embeddings. Again, works such as Chen et al. \cite{chen2021graph} evaluate their algorithms on datasets in which the realism to be encountered in real, designated environments in which the algorithms are to be deployed, but no post-deployment evaluation is presented in such works to show if their approaches to deal with the realism are successful. Realisms that the SID model faces arise from background noise, especially the tv sounds. Note that the realisms that the SID model needs to deal with are more complex than the realisms that the VAD model needs to deal with, as voice from the tv could confound the model from correctly identifying the identity of the speaker in an audio clip. In other words, the SID model needs to be able to deal with more complex background noise (more complex acoustical realisms) than the VAD model.

\subsection{Emotion Detection}
% datcu2005facial, lugger2007incremental, altun2007new,
%vrebvcevic2019emotion, emodbdeng2017universum, emodbwang2015speech, emodbalex2018utterance
There have also been a lot of work \cite{ danisman2008emotion, emodbdeng2017universum, emodbwang2015speech, emodbalex2018utterance} on using voice as a modality to classify emotions. These use several datasets that have speech files with emotion labels. EMO-DB, a dataset of the German language \cite{burkhardt2005database}, is a popular one for many works \cite{emodbtriantafyllopoulos2019towards, dickerson2014resonate}. It consists of six emotion categories such as anger, sadness, and happiness performed by actors. Another popular dataset is RAVDESS \cite{livingstone2018ryerson} that consists not only of the emotional utterances, but also video footage of emotional speech. CREMA-D \cite{cao2014crema}, like RAVDESS, contains both audio and audio-video samples. Despite the variety in modalities, since some datasets like EMO-DB have only audio samples, and others like RAVDESS have both audio and video samples, the emotions that they have are largely in common. The common emotion categories in the datasets are happiness, anger, sadness, and neutrality. One common problem with the emotional utterance datasets is that they are often collected in controlled studio environments in which realisms expected in a real, designated environment do not exist. Algorithms trained on those samples are shown not to be robust when deployed in the real world \cite{gao2021emotion}. There have been works that attempt to address environmental distortions such as reverberation, deamplification, and the background noise at pre-deployment time \cite{salekin2017distant, wijayasingha2021robustness, shchetinin2020deep, gao2021emotion}. However, they do not confirm at post-deployment stage if their strategies of addressing the realisms work. The realisms that the emotion detection model needs to address, in addition to deamplification, reverberation, and common indoor background noise, are tv sounds. An audio clip produced by a registered speaker could include background sounds that are from the tv, and the speech from the tv could confound the emotion detection model from making the correct prediction as the speech from the tv could be of a different emotion than the speech produced by the registered speakers.


\subsection{Conflict Detection}
There have been several attempts to detect verbal conflict using sound signals that a microphone picks up from the ambient environment. A work \cite{lefter2017aggression} creates verbal conflict between pairs of a student and an actor who act out conflict. From the generated conflict episode, it is observed that overlapped speech is an important indicator of interpersonal conflict \cite{lefter2017aggression}. However, they did not create a model of automatic conflict detection based on their conclusion. Based on the fact that repetition of parts of speech, such as syllables, phrases and words, is indicative of interpersonal conflict, another work \cite{letcher2018automatic} develops a repetition detection model that uses the audio files collected by the on-body sensors of police officers to detect conflict. However, the interpersonal conflicts that police officers encounter during their jobs are not the same as every-day interpersonal conflicts that take place in households between arguing family members. The state-of-the-art modules on automatic conflict detection using speech \cite{caraty2015detecting, grezes2013let}, achieve satisfactory performance on their respective datasets, but their approaches are not evaluated to demonstrate if acoustic distortions of noise, distance, and reverberation affect the results. As a result, the automatic detection of every-day harmful interpersonal conflicts among people in home environments remains unsolved. Again, in addition to the realisms such as reverberation, common indoor background noise, and deamplification, the conflict detection model, just as the emotion detection model, needs to deal with the realisms that are the tv sound: the characters on the tv might be in a verbal conflict (as the background sound for the participants whose conflict we want to monitor), which can confound the conflict detection model.
  