\section{Research Method}
\label{sec:research_method}

In this study we use a survey as research method~\cite{gray2013drr}. Subsequently, we discuss the population and sample, the questionnaire, and the data analysis methods we used.

\subsection{Population and Sampling}\label{subsec:population_sampling}

Our target population are practitioners that are actively involved in the engineering of industrial software-intensive systems in any domain. This includes architects, designers, developers, testers, maintainers, operators, and other people who have technical expertise and are actively involved in the development and maintenance of these software systems. 

Concretely, we contacted 355 practitioners from a wide variety of companies\footnote{Almost all practitioners we contacted were from different companies and the few that were from the same company had different roles within the company. The participants were asked to answer from their own perspective.} via the networks of the researchers involved in this study (i.e., the authors of this paper) to complete the survey. We used two criteria to invite people: (1) participants should be active in different domains that are representative of software-intensive systems, and (2) participants have the required expertise to answer the questions. The invited practitioners were spread over in total 21 countries.\footnote{Sweden 58 invited practitioners, USA 55, Austria 50, Belgium 42, Czech Republic 34, Germany 23, New Zealand 22, The Netherlands 18, Canada 15, Spain 9, Denmark 7, UK 5, France 4, India 2, Greece 2, Poland 1, Norway 1, Switzerland 1, Australia 1, Japan 1, Unknown 2} The invitations were sent by personalised emails in different batches during the period from November 30, 2020 until July 31, 2022. We sent reminders according to a predefined \mbox{schedule of one, two, and six weeks after the invitation.} 

\subsection{Survey Instrument}

The survey used a questionnaire to collect data based on a set of predefined questions~\cite{gray2013drr}. 
Because practitioners are not necessarily familiar with the term self-adaptation, the survey started with a gentle introduction of the core idea of what constitutes a self-adaptive system using basic terminology commonly used in industry, and illustrated this with a few characteristic examples to make it concrete. 
We used both closed and open questions. Closed questions have a predefined set of answers, such as yes/no, or multiple choice. We also allowed participants to add extra options for answering several closed questions using a text field. Open questions provide a space that participants can use to provide an answer. While closed questions allow acquiring a clear view on a particular topic using basic statistics, open questions allow acquiring in-depth insights using qualitative analysis. 
We provide a replication package with all study materials, including the study protocol, the questionnaire, the raw data, and the analysis results.\footnote{https://people.cs.kuleuven.be/danny.weyns/surveys/sas-in-industry/}

For this study we used a self-administered anonymous online questionnaire (Survey \& Report hosted by Linnaeus University, Sweden). %This allowed us to obtain responses from a large set of participants.
%
The main motivation to use an online questionnaire is to involve a large set of participants with relatively low cost (both time-wise and financially). 
%
We created an initial list of survey questions that were directly derived from the research questions of this study. The initial list of questions was composed by two members of the research team and then crosschecked by the other team members. 

We validated the questionnaire in a pilot with eight randomly-selected participants from the target population. For this pilot, we added additional meta-questions to the questionnaire about clarity of terminology and questions, relevance of the questions, scope of the questions, and the time required to complete the survey. For both clarity of terminology and clarity of the questions we obtained an average score of 4.38 on a scale from 1 (Not clear at all) to 5 (Very clear). None of the participants indicated that questions should be removed or modified. Six participants indicated that no important aspects were missing. One participant hinted that we may also probe whether the use of self-adaptation requires a specialised team in the company or alternatively infrastructure to share knowledge. Another participant suggested adding a question about scalability of solutions for self-adaptation. One participant stated that the example system we used to introduce self-adaptation may create some bias, and further that answers to questions may differ depending on roles on the engineering teams. The average reported time to complete the survey was 24 minutes. Based on the feedback, we adjusted the introductory part of the questionnaire. We did not revise the questions as they were perceived as clear and well scoped. The finalised questionnaire was then distributed to the participants as explained above. 

The first part of the questionnaire (Table~\ref{tab:demographics}) solicited whether the participant applies self-adaptation and collected general demographic information. This allowed us to check whether the participant had experience with self-adaptation (Q0.1), confirm a good coverage of kinds of software-intensive systems across participants (Q0.2), the size of the companies of participants (Q0.3), as well as a confirmation of the participant's role (Q0.4) and years of experience (Q0.5).  

\input{questionnaire_rq0}

The second part of the questionnaire aimed at questions related to RQ1 collecting data about the problems for which the participants apply self-adaptation (Q1.1), the main business motivations for using self-adaptation (Q1.2), and the benefits obtained from applying self-adaptation (Q1.3) (see Table~\ref{tab:rq1}). The first two questions had multiple options.\footnote{Question Q1.2 aimed at investigating motives for applying self-adaptation at a more high-level, whereas Q1.3 was focusing more at low-level benefits, technical and specific to a system. The initial lists of the options for these questions were based on the literature of self-adaptation, see e.g.,~\cite{Lemos2013roadmap,Weyns2013,9462043}.}  

\input{questionnaire_rq1}

The third part of the questionnaire covered a question related to RQ2 on how practitioners characterise self-adaptation (see Table~\ref{tab:rq2}). This part included only one question that asked participants to describe a concrete self-adaptive system they had  worked with (Q2.1).  

\input{questionnaire_rq2}

The fourth part of the questionnaire addressed RQ3 on how practitioners apply self-adaptation in their practice (see Table~\ref{tab:rq3}). The first three questions investigated the mechanisms that participants use to monitor (Q3.1) and analyse (Q3.2) the system during operation, and change the system when needed (Q3.3). The next question investigated the degree   of automation of self-adaptation (Q3.4). The next three questions investigate reuse of solutions (Q3.5-Q3.7). The last question of this part of the questionnaire probed whether and how practitioners establish trust in the self-adaptation solutions they build (Q3.8).  

\input{questionnaire_rq3}

Finally, the fifth part of the questionnaire addressed RQ4 on difficulties, risks, and opportunities of applying self-adaptation in practice (see Table~\ref{tab:rq4}). The first two questions investigated difficulties (Q4.1 and Q4.2); the next three questions focused on risks and risk mitigation (Q4.3-Q4.5). The next two questions probed the interest of practitioners to get support from researchers for solving problems with self-adaptation (Q4.6 and Q4.7). The last two questions investigated opportunities for applying self-adaptation beyond the current practice (Q4.8 and Q4.9).  

%\input{questionnaire_rq4}

%The questionnaire concluded with three extra questions. The first question probed at the confidence the participants had in their answers when answering the questions (Q5.1). The last two questions allowed participants to add anything they feel to be relevant in the context of this questionnaire (Q5.2-3). 

The questionnaire concluded with a question (Q5.1) about how confident participants were in general about the answers they gave when answering the survey questions with possible answers: Very confident; Confident; Sufficiently confident; Neutral; Somewhat unconfident; Not confident; Not confident at all.

\subsection{Data Analysis}

To analyse closed questions, we used descriptive statistics and quantitative data analysis. Therefore, we mostly report frequencies of answers,  percentages relative to the respective number of responses, and relationships between answers to questions based on contingency matrices (based on the categorisation of answers). We only report relationships that led to relevant insights. 

To analyse comments to open questions, we used qualitative data analysis. In particular, we used inductive reasoning to construct codes and  infer categories from the data by labelling occurrences of codes and grouping them into categories~\cite{Stol2016}. Similar to others (e.g., Prechelt et al.~\cite{Prechelt2018}), we tried to keep coding simple. We did not have a pre-defined coding schema or a pre-defined granularity or semantic style for the codes. However, we interpreted comments in the context of the question for which they were given.  We used a simple version of open coding~\cite{Strauss1990}. Similar to Mendez Fernandez et al.~\cite{Fernandez2016}, we used open coding to add codes to small coherent fragments of the comments. We then categorised the developed concepts in a hierarchy of categories as an abstraction of the codes. We coded in sub-teams of two or three coders in total 886 comments of 12 open questions. Coding was first done individually and then consolidated in the sub-team. Two other researchers crosschecked the consolidated coding. Where necessary, the coding was adjusted in consensus between the sub-team and the researchers. We excluded some comments from coding, e.g., if they did not provide any additional insights or if they were too generic, e.g., a participant answering ``Always'' to a closed question and stating ``This is how we work'' in the comments. Also, we did not map the answers to a closed question to comments for that question. For example, a participant may have answered that they never reused solutions for self-adaptation,  but in their comments indicated reasons that they ``might'' do so (i.e., one comment may cover several concepts, which may not necessarily match the answer to the closed question). When reporting example quotes from comments in Section~\ref{sec:results}, we use verbatim excerpts, including spelling and punctuation errors.
