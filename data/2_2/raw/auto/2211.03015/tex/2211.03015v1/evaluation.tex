\section{Performance and Usability}
~\label{testing} 
To gain insights into our design and distill some lessons, we evaluated the performance and usability of our off-the-shelf, experimental framework in two ways; i) System Testing (frequently carried out by our team), and ii) Quality Assurance Testing (carried out by prospective users).  
Note that the framework's security analysis requires sending a zero-click exploit to the victim and forensically investigating if the victim's device has been infected or not.
We could not test the framework from a security perspective owing to the absence of zero-click binaries. 
However, the fact that the chat applications were sandboxed/isolated means the design principle of separation of privilege secures them.  

\input{tables/conntime} 
\subsection{Setup} We performed tests on eight different smartphones to ensure that our implementation was independent of the screen sizes, device manufacturers and web browsers. 
The Android phones included LG V40 ThinQ, Motorolla Nexus 6, Samsung Galaxy S10E, Oppo A5 and Vivo U1, whereas the iOS devices included iPhone XR, iPhone X and iPhone 7. 
In addition, we also tested other device models by creating virtual devices in the Android Emulator~\cite{androidstudio}.  
We set up the server side on a Linux-based VM instance on GCP. 
This instance had nested virtualization enabled and was accessible over the Internet using public IP. 
We ran three Docker containers on GCP, one for each application (WhatsApp, Signal and regular messaging application) and configured the applications using Google Voice number.  
For testing, we set up three more GCP accounts and configured the applications with different Google voice numbers. 
This essentially allowed the participants of the user study to test the framework without linking their personal accounts on the server before they were fully satisfied. 
To encourage active involvement from the participants, group chats were also created where random texts, images, gifs, etc., were frequently shared.

\subsection{System Testing} 
\label{performance} 
Here, we assessed our system's connection time, usability, introduced latency, and scalability. 

\paragraph{Connection Time:} 
To evaluate the connection establishment phase, we accessed the remote server on several phones at different times, using various networks. 
For accessing the remote server, we specifically used the WebRTC-based screen sharing approach for Android clients and the PNG-based screen sharing approach for iOS clients.  
We made 25 connection attempts from the client device to the remote server and observed how often the connection fails, how many attempts are required to reconnect, and if the connection ever terminates itself while the system is being used. 
Each round of the experiment lasted for an hour. Our results (Table~\ref{tab:conntime}) indicate that the client was able to connect to the server in the first attempt seamlessly, and the connection was stable throughout the experiment. 

\paragraph{Usability:} 
To evaluate our experimental system's usability, we accessed the remote server from smartphones of different screen sizes, device
manufacturers, and web browsers (notably Google Chrome, Firefox, Microsoft Edge, Opera, UC and Oppo browser). In all instances, the text was readable, the remote display was of high quality and covered maximum screen size. 
However, as our framework added an additional layer to the communication path, it introduced a lag which sometimes created usability challenges. 

\paragraph{Measuring Lag:}
In practice, it is difficult to measure the introduced lag precisely, as the clocks of the two computing devices (phone and server) are not in sync. 
Therefore, we measured the additional transmission time between client and server as half of the Round Trip Time (RTT) between these two nodes. 
For this, we developed client and server Android applications, whereby the client sends a test message to the server, which replies with an acknowledgement (ACK).
The client then measures RTT as the difference between the time the message was sent, and the ACK was received. 
To minimize the measurement error, the client sends ten messages per second and calculates the average lag. 
Although the RTT measurement is influenced by other factors such as network speed, traffic load, etc., the measurements provided a quantitative value to the introduced lag.
Table~\ref{tab:delay} reports the average lag in seconds (s) over different networks. 
One important question here was, given the security benefits of our solution and that the user experience was not always significantly affected, whether the 0.49 seconds lag is acceptable?  \input{tables/delay} 

\paragraph{Resource Requirement and Scalability:} 
Realistically, the user might wish to port additional chat applications (e.g., Viber, Telegram) remotely. 
To determine if our COTs-based framework was scalable, we performed cost analysis for a single user with respect to the increase in the number of remote applications.  
We observed that for efficient performance, the bare-minimum requirement for each Android emulator to run a chat application is 4GB RAM and 2GB disk space.  
Hence, we kept the GCP cloud instance with the Intel Haswell CPU Platform, 8GB RAM and 100GB disk space as the baseline for running one chat application.
As disk space is not a hard constraint, we noted the monthly cost as we increased the required RAM (16, 32 and 64 GB) corresponding to the number of applications to be hosted.  
Note that the user can also self-deploy the server side to lessen the recurring expenditure. 
To provide cost analysis for a dedicated server, we kept a Dell PowerEdge server with 8GB RAM and 1TB disk space (\$780) as the base server for running one chat application and kept increasing the RAM as the number of applications increased. 
The results in Figure~\ref{fig:plot} suggest that despite high initial investment, the dedicated server is more cost-effective than a cloud setup in the long run.
However, our rationale was that the GCP cloud setup is practically more secure in terms of the server's physical and logical security.

\begin{figure}[t]
\begin{center}
\includegraphics[width=\linewidth]{figures/plot.pdf}
\caption{Cost vs resource analysis per user for deploying server end on Google Cloud Platform and dedicated server.}
\label{fig:plot}
\end{center}
\end{figure}

\subsection{Quality Assurance Testing} \label{usability}
To determine our envisioned system's usability in practice, we got the system evaluated by potential users and received feedback.

\paragraph{Ethical Considerations:} 
We note that our usability study was exempted under the IRB Exemption Category 3 - Benign Behavioral Interventions by our institution's IRB.  
Although our study involves human subjects, it does not collect sensitive and personal information or perform deception, attacks, etc.

To recruit participants for the user study, we sent out invitation emails to 30 individuals from our personal and professional circle belonging to different regions of the world.  
On a high level, all participants were above 18 years and included i) potential targets of zero-click attacks (including three journalists) and ii) privacy-conscious smartphone users.  
Of 30, 27 individuals showed willingness to participate in the user study, whereas 3 individuals did not respond.
As the user study was voluntary, we did not send follow-up emails. 
We scheduled Zoom meetings with the participants to obtain informed consent, demonstrate the working of our system and provide instructions regarding the user study.  
Since participants might initially have privacy concerns regarding setting up their personal chat accounts on the remote server, we pre-configured the applications on each GCP test account using Google voice numbers.
We did not collect personally identifiable information (e.g., name or email of the participants) or other related data like IP addresses during the study.
To maintain the confidentiality of the results, we analyzed and reported the responses in our work as group data without identifying any individual.
Finally, after the user study, we deleted all the conversations from the server. 

\paragraph{User Study and Results:}
The tasks involved accessing the remote server by typing the provided public IP of the server in the mobile web browser.
The participants were given temporary login credentials to prevent unauthorized access to the remote server. 
Once logged in, each participant was asked to exchange random text messages, images, gifs, etc., with the saved contacts or engage in group chats.  
Our team controlled the other contacts and group chats to let the participant actively communicate using the system and observe if they experienced any possible delay or performance degradation.
After using the system for an hour, participants shared their feedback on the following via a brief Google survey form (provided in the Appendix).  

\begin{enumerate}

\item Prior know-how of zero-click attacks, 

\item Experience connecting to the remote server, i.e., Is it accessible? Is the connection stable or terminates frequently? 

\item Experience of sending test messages, i.e., Is the system user-friendly? Is the lag acceptable? 

\item Suggestions to improve the system.

\end{enumerate}

The results indicated that 21 out of 27 participants had a fair idea about zero-click attacks before starting the user study.  
Figure~\ref{fig:conn} shows the results of the participant's experience while connecting to the remote server.
As shown, for all 27 participants accessing the server from different regions at different timings, the remote server was accessible in the first attempt, and the connection remained stable throughout the study (i.e., it did not terminate at all).
  
\begin{figure}[t] \begin{center}
\includegraphics[width=\linewidth]{figures/conn.pdf} \caption{Evaluating Server's Accessibility and Connection Status via User Study: All 27 participants found the server accessible in the first attempt and the connection stable.}
\label{fig:conn} \end{center} \end{figure}

Figure~\ref{fig:usability} indicates the results of the participant's experience while sending test messages. 
For this, we specifically evaluated if the participants found the system user-friendly (i.e., text was readable, the screen was full size, etc.) and whether the lag introduced by the COTS-based components was acceptable considering the security benefits of the solution. 
The results indicated that 21 out of 27 participants found the system to be user-friendly and the lag (interestingly) acceptable.
Six participants found the system to be user-friendly but complained that the lag was significant. 
Overall, the participants gave constructive feedback, such as keeping the native keyboard on-screen while controlling the remote screen and ensuring that the user does not experience any lag.

\begin{figure}[t]
\begin{center}
\includegraphics[width=\linewidth]{figures/usability.pdf}
\caption{Evaluating System's Usability via User Study: Here, usable = user-friendly and acceptable lag, partially usable = user-friendly but unacceptable lag, unusable = not friendly and unacceptable lag. Result: 21 participants found the system usable, while 6 participants deemed it partially usable.}
\label{fig:usability}
\end{center}
\end{figure}
