\documentclass{article} % For LaTeX2e
\usepackage{iclr2022_conference,times}

% Optional math commands from https://github.com/goodfeli/dlbook_notation.
\input{math_commands.tex}

%\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography

\usepackage[colorlinks=true, citecolor=citecolor, linkcolor=linkcolor]{hyperref}
\definecolor{citecolor}{HTML}{0071BC}
\definecolor{linkcolor}{HTML}{ED1C24}
\usepackage{url}
%\usepackage{capt-of}
\usepackage{graphicx}
\usepackage{xspace}
\usepackage{color}
\usepackage{multirow}
\usepackage{tabulary}
\usepackage{overpic}
%\usepackage[noend]{algpseudocode}
\usepackage[ruled, linesnumbered, vlined]{algorithm2e}
\newcommand\mycommfont[1]{\footnotesize\ttfamily\textls[-50]{#1}}
\SetCommentSty{mycommfont}

% Extra commands
\iclrfinalcopy

% For comments
\definecolor{dyellow}{rgb}{0.7, 0.7, 0.0}
\newcommand{\TODO}[1]{{\color{dyellow}TODO: #1}}

% For notations
% Mathcal
\def\calA{{\mathcal{A}}}
\def\calB{{\mathcal{B}}}
\def\calC{{\mathcal{C}}}
\def\calD{{\mathcal{D}}}
\def\calE{{\mathcal{E}}}
\def\calF{{\mathcal{F}}}
\def\calG{{\mathcal{G}}}
\def\calH{{\mathcal{H}}}
\def\calI{{\mathcal{I}}}
\def\calJ{{\mathcal{J}}}
\def\calK{{\mathcal{K}}}
\def\calL{{\mathcal{L}}}
\def\calM{{\mathcal{M}}}
\def\calN{{\mathcal{N}}}
\def\calO{{\mathcal{O}}}
\def\calP{{\mathcal{P}}}
\def\calQ{{\mathcal{Q}}}
\def\calR{{\mathcal{R}}}
\def\calS{{\mathcal{S}}}
\def\calT{{\mathcal{T}}}
\def\calU{{\mathcal{U}}}
\def\calV{{\mathcal{V}}}
\def\calW{{\mathcal{W}}}
\def\calX{{\mathcal{X}}}
\def\calY{{\mathcal{Y}}}
\def\calZ{{\mathcal{Z}}}

% Add a period to the end of an abbreviation unless there's one
% already, then \xspace.
\makeatletter
\DeclareRobustCommand\onedot{\futurelet\@let@token\@onedot}
\def\@onedot{\ifx\@let@token.\else.\null\fi\xspace}

\def\eg{\emph{e.g}\onedot} \def\Eg{\emph{E.g}\onedot}
\def\ie{\emph{i.e}\onedot} \def\Ie{\emph{I.e}\onedot}
\def\cf{\emph{c.f}\onedot} \def\Cf{\emph{C.f}\onedot}
\def\etc{\emph{etc}\onedot} \def\vs{\emph{vs}\onedot}
\def\wrt{w.r.t\onedot} \def\dof{d.o.f\onedot}
\def\etal{\emph{et al}\onedot}
\makeatother

% For table
\newlength\savewidth\newcommand\shline{\noalign{\global\savewidth\arrayrulewidth
		\global\arrayrulewidth 1pt}\hline\noalign{\global\arrayrulewidth\savewidth}}
\newcolumntype{x}[1]{>{\centering\arraybackslash}p{#1pt}}
\newcommand{\tablestyle}[2]{\setlength{\tabcolsep}{#1}\renewcommand{\arraystretch}{#2}\centering\footnotesize}

\usepackage{array}
\newcommand{\PreserveBackslash}[1]{\let\temp=\\#1\let\\=\temp}
\newcolumntype{C}[1]{>{\centering\arraybackslash}p{#1pt}}
\newcolumntype{R}[1]{>{\raggedleft\arraybackslash}p{#1pt}}
%[1]{>{\PreserveBackslash\raggedleft}p{#1}}
\newcolumntype{L}[1]{>{\raggedright\arraybackslash}p{#1pt}}
%{>{\PreserveBackslash\raggedright}p{#1}}
\newcommand{\tabincell}[2]{\begin{tabular}{@{}#1@{}}#2\end{tabular}}

% self defined command
\newcommand{\notecxt}[1]{{{\textcolor{cyan}{\textit{(CXT: #1)}}}}}
\newcommand{\remove}[1]{\textcolor{red}{(\textit{remove} #1)}}
\newcommand{\highlight}[1]{\textcolor{cyan}{\textit{#1}}}
\definecolor{ao}{rgb}{0.0, 0.5, 0.0}
\newcommand{\yy}[1]{{{\textcolor{ao}{(Yu: #1)}}}}
\newcommand{\hb}[1]{{{\textcolor{blue}{(Hakan: #1)}}}}


% Maybe a shorter title
\title{Learning to Annotate Part Segmentation with Gradient Matching}
% \title{Learning to Annotate Part Segmentation from Generative Models with Gradient Matching}

% Authors must not appear in the submitted version. They should be hidden
% as long as the \iclrfinalcopy macro remains commented out below.
% Non-anonymous submissions will be rejected without review.

% \author{Antiquus S.~Hippocampus, Natalia Cerebro \& Amelie P. Amygdale \thanks{ Use footnote for providing further information
% about author (webpage, alternative address)---\emph{not} for acknowledging
% funding agencies.  Funding acknowledgements go at the end of the paper.} \\
% Department of Computer Science\\
% Cranberry-Lemon University\\
% Pittsburgh, PA 15213, USA \\
% \texttt{\{hippo,brain,jen\}@cs.cranberry-lemon.edu} \\
% \And
% Ji Q. Ren \& Yevgeny LeNet \\
% Department of Computational Neuroscience \\
% University of the Witwatersrand \\
% Joburg, South Africa \\
% \texttt{\{robot,net\}@wits.ac.za} \\
% \AND
% Coauthor \\
% Affiliation \\
% Address \\
% \texttt{email}
% }

\author{Yu Yang \\
Department of Automation\\
Tsinghua University, BNRist\\
\texttt{yang-yu16@mails.tsinghua.edu.cn} \\
\And
Xiaotian Cheng \\
Department of Automation\\
Tsinghua University, BNRist\\
\texttt{cxt20@mails.tsinghua.edu.cn} \\
\And
Hakan Bilen \\
School of Informatics \\
University of Edinburgh \\
\texttt{hbilen@ed.ac.uk} \\
\And
Xiangyang Ji \\
Department of Automation\\
Tsinghua University, BNRist\\
\texttt{xyji@tsinghua.edu.cn}
}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to \LaTeX{} to determine where to break
% the lines. Using \AND forces a linebreak at that point. So, if \LaTeX{}
% puts 3 of 4 authors names on the first line, and the last on the second
% line, try using \AND instead of \And before the third author name.

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

%\iclrfinalcopy % Uncomment for camera-ready version, but NOT for submission.
\begin{document}


\maketitle

\begin{abstract}
The success of state-of-the-art deep neural networks heavily relies on the presence of large-scale labelled datasets, which are extremely expensive and time-consuming to annotate. This paper focuses on tackling semi-supervised part segmentation tasks by generating high-quality images with a pre-trained GAN and labelling the generated images with an automatic annotator. In particular, we formulate the annotator learning as a learning-to-learn problem. Given a pre-trained GAN, the annotator learns to label object parts in a set of randomly generated images such that a part segmentation model trained on these synthetic images with their predicted labels obtains low segmentation error on a small validation set of manually labelled images. We further reduce this nested-loop optimization problem to a simple gradient matching problem and efficiently solve it with an iterative algorithm. We show that our method can learn annotators from a broad range of labelled images including real images, generated images, and even analytically rendered images. Our method is evaluated with semi-supervised part segmentation tasks and significantly outperforms other semi-supervised competitors when the amount of labelled examples is extremely limited.
\end{abstract}

\section{Introduction}\label{sec:intro}

In recent years, deep neural networks have shown a remarkable ability to learn complex visual concepts from large quantities of labelled data.
However, collecting manual annotations for a growing body of visual concepts remains a costly practice, especially for pixel-wise prediction tasks such as semantic segmentation.
A common and effective way of reducing the dependence on manual labels is first learning representations from unlabeled data~\citep{pathak2016context, noroozi2016unsupervised, gidaris2018unsupervised, he2020momentum} and then transferring learned representations to a supervised learning task~\citep{chen2020simple, zhai2019s4l}.
The insight of this paradigm is that some of the transferred representations are informative, easily identified, and disentangled, such that a small amount of labelled data suffices to train the target task.

A promising direction for learning such representations is using generative models~\citep{kingma2014auto, goodfellow2014generative} since they attain the capability of synthesizing photorealistic images and disentangling factors of variation in the dataset.
Recent methods~\citep{goetschalckx2019ganalyze, shen2020interpreting, karras2019style, plumerault2019controlling, jahanian2019steerability, voynov2020unsupervised, spingarn2020gan, harkonen2020ganspace} show that certain factors such as object shape and position in the images synthesized by generative adversarial networks (GANs)~\citep{goodfellow2014generative} can be individually controlled by manipulating latent features.
Building on the success of powerful GAN models,
\citet{zhang2021datasetgan} and \citet{Tritrong2021RepurposeGANs} demonstrate that feature maps in StyleGAN~\citep{karras2019style, karras2020analyzing, karras2020training} can be mapped to semantic segmentation masks or keypoint heatmaps through a shallow decoder, denoted as \emph{annotator} in this paper.  
Remarkably, the annotator can be trained after a few synthesized images are manually labelled, and the joint generator and trained annotator can be used as an infinite labelled data generator. 
Nonetheless, labelling synthesized images with ``human in the loop'' has two shortcomings: (i) it can be difficult to label the generated images of low quality;
(ii) it requires continual human effort whenever the generator is changed.

% The recent advances in GANs show that generated images can be edited by manipulating the latent variables or hidden features inside generators~\citep{goetschalckx2019ganalyze, shen2020interpreting, karras2019style, plumerault2019controlling, jahanian2019steerability, voynov2020unsupervised, spingarn2020gan, harkonen2020ganspace}. 
% These results suggest that GANs acquire knowledge about how an image is composited.
% Based on these observations, \citet{zhang2021datasetgan} and \citet{Tritrong2021RepurposeGANs} construct a neural network to decode the feature maps of pre-trained generator into segmentation masks or key-point heatmaps.
% This neural network, dubbed annotator in this paper, can be efficiently trained in a supervised learning manner with only few labeled data.
% The trained annotator can produce high-precision labels for \emph{any} images synthesized by the generator,  resulting in an infinite labeled data generator. 
% Nonetheless, this supervised learning manner necessitates labeled generated images and human-in-the-loop pipeline, which (i) poses difficulties in annotation process due to deteriorated details in generated images and (ii) requires repeated human effort whenever the generator is changed.


% a natural idea is to learn annotator from a collection of image-label pairs.
% This is non-trivial because annotator only receives as input generator features which are lacked in labeled image datasets.
% we remove the necessity of noisy and challenging annotation of the generated images by replacing them with the same number of annotated real images.
One straightforward way to mitigate these limitations is to inverse the generation process to obtain generator features for given images.
In this way, annotator, which only receives generator features as input, can be trained from existing labelled data, not necessarily generated data.  
However, inversion methods can not ensure exact recovery of the generator features, leading to degraded performance of annotator (see \Secref{sec:appendix_inversion}).
Another way to address this issue is to align the joint distribution of generated images and labels with that of real ones using adversarial learning as in SemanticGAN \citep{li2021semantic}. 
However, adversarial learning is notoriously unstable~\citep{brock2018large} and requires a large amount of data to prevent discriminators from overfitting~\citep{karras2020training}.

In this paper, we formulate the annotator learning as a learning-to-learn problem -- the annotator learns to label a set of randomly generated images such that a segmentation network trained on these automatically labelled images obtains low prediction error on a small validation set of manually labelled images.
This problem involves solving a nested-loop optimization, where the inner loop optimizes the segmentation network and the outer loop optimizes the annotator based on the solution of inner-loop optimization.
Instead of directly using end-to-end gradient-based meta-learning techniques~\citep{li2019learning, pham2021meta}, we reduce this complex and expensive optimization problem into a simple gradient matching problem and efficiently solve it with an iterative algorithm. 
We show that our method obtains performance comparable to the supervised learning counterparts~\citep{zhang2021datasetgan, Tritrong2021RepurposeGANs} yet overcomes their shortcomings such that training annotators can utilize labelled data within a broader range, including real data, synthetic data, and even out-of-domain data. 
% Experiments also show that our method achieves better performance than the naive inversion method and SemanticGAN~\citep{li2021semantic}. 

Our method requires a large quantity of unlabeled data for pre-training GANs and a relatively small amount of labelled data for training annotators, which drops into a semi-supervised learning setting. 
Our method relies on unconditional GANs, of which the performance limit the application of our approach. 
As the state-of-the-art GANs only produce appealing results on single-class images but struggle to model complex scenes, we focus on part segmentation of particular classes and avoid multi-object scenes.
% Furthermore, considering that state-of-the-art unconditional GANs only produce appealing results on single-class images but struggle to model complex scenes, we evaluate our method on several part segmentation datasets of particular classes such as human face, horse, aeroplane, car, and cat.

% Our contribution can be summarized as follows. 
% (i) We formulate the learning of annotations for GAN-generated images as a learning-to-learn problem and propose an algorithm based on gradient matching to solve it. 
% As a consequence, a broad range of labeled data including real data, synthetic data, and even out-of-domain data is applicable. 
% As a consequence, we achieve learning segmentation from synthetic labeled images and generalize to real images, even when there is domain gap between synthetic and real images.
% (ii) We empirically show that our method significantly outperforms other semi-supervised segmentation methods in the few-label regime.\footnote{Code is available at \url{https://github.com/yangyu12/lagm}.}

Our contribution can be summarized as follows. 
(i) We formulate the learning of annotations for GAN-generated images as a learning-to-learn problem and propose an algorithm based on gradient matching to solve it. 
Consequently, a broad range of labelled data, including real data, synthetic data, and even out-of-domain data, is applicable. 
(ii) We empirically show that our method significantly outperforms other semi-supervised segmentation methods in the few-label regime.\footnote{Code is available at \url{https://github.com/yangyu12/lagm}.}

\section{Related Work}
\paragraph{Semi-supervised learning} Semi-supervised learning (SSL)~\citep{zhu2005semi} augments the training of neural networks from a small amount of labelled data with a large-scale unlabeled dataset. 
A rich body of work regularizes networks on unlabeled data with consistency regularization. 
In particular, networks are required to make consistent predictions over previous training iterations~\citep{laine2016temporal, tarvainen2017mean, izmailov2018averaging}, noisy inputs~\citep{miyato2018virtual}, or augmented inputs~\citep{berthelot2019mixmatch, berthelot2019remixmatch, sohn2020fixmatch}.
This paradigm is also interpreted as a teacher-and-student framework where the teacher produces pseudo labels for unlabelled data to supervise the training of students. 
Pseudo labels can be enhanced with heuristics~\citep{lee2013pseudo, berthelot2019remixmatch} or optimized towards better generalization~\citep{pham2021meta, li2019learning}. 
Apart from image classification tasks, the above ideas are also successfully applied to semantic segmentation tasks~\citep{hung2018adversarial, ouali2020semi, ke2020guided, french2020semi}. 
Our work can also be interpreted from a teacher-and-student perspective. In contrast to the above methods that adopt homogeneous network structures for teacher and student, our work employs generative models as a teacher and discriminative models as a student.

Other SSL approaches explore the utility of unlabelled data from a representation learning perspective. 
In particular, deep neural networks are first optimized on the unlabeled data with self-supervised learning tasks~\citep{pathak2016context, noroozi2016unsupervised, gidaris2018unsupervised, he2020momentum} to gain powerful and versatile representations, and then finetuned on the labelled data to perform target tasks~\citep{chen2020simple, zhai2019s4l}.
In addition, generative models can also learn efficient and transferable representations without labels. 
To this end, one promising direction is to decode generative features into dense annotations such as segmentation masks and keypoint heatmap to facilitate SSL, as done in SemanticGAN~\citep{li2021semantic}, DatasetGAN~\citep{zhang2021datasetgan}, RepurposeGAN~\citep{Tritrong2021RepurposeGANs} and our work.
However, our work differs from these works in that we formulate the annotator learning as a learning-to-learn problem that is solved with gradient matching. 
Particularly, unlike SemanticGAN~\citep{li2021semantic} that learns annotator and data generator jointly, our method presumes a fixed pre-trained GAN and learns annotator only, which circumvents the complication of joint learning.
% Our method shows empirically better performance than SemanticGAN~\citep{li2021semantic} and relaxes the requirements of labeled data compared to DatasetGAN~\citep{zhang2021datasetgan} and RepurposeGAN~\cite{Tritrong2021RepurposeGANs}.  

%\paragraph{Disentangled representation learning} 
%Disentangled representation learning receives a lot of attention since it can benefit \highlight{xxx}\notecxt{what's this supposed to refer to?}. 
%A rich body of work show that generative models such as variational autoencoder (VAE)~\citep{kingma2014auto} and generative adversarial networks (GANs)~\citep{goodfellow2014generative} learns disentangled representations which is manifested by controllable generation process~\citep{goetschalckx2019ganalyze, shen2020interpreting, karras2019style, plumerault2019controlling, jahanian2019steerability, voynov2020unsupervised, spingarn2020gan, harkonen2020ganspace}. 
%Such advantages are further exploited to save manual efforts on annotation~\citep{zhang2021datasetgan, zhang2020image, Tritrong2021RepurposeGANs}. We follow the recent trend on harnessing the disentangled representation gained by GANs and show that manual labels can be reduced for segmentation tasks.

\paragraph{Semantic Part Segmentation}
Semantic part segmentation, which decomposes rigid or non-rigid objects into several parts, is of great significance in tremendous computer-vision tasks such as human parsing~\citep{dong2014towards, nie2018mutual, fang2018weakly, gong2017look}, pose estimation~\citep{yang2011articulated, shotton2011real, xia2017joint} and 3D object understanding~\citep{yi2016scalable, song2017embedding}.  
The challenge of part segmentation arises from viewpoint changes, occlusion, and a lack of clear boundaries for certain parts. 
Existing datasets manage to alleviate the problem by enriching the data and annotations.
CelebAMask-HQ~\citep{CelebAMask-HQ} is a large-scale face image dataset annotated with masks of face components. 
\citet{chen2014detect} provides additional part segmentation upon PASCAL VOC dataset~\citep{everingham2010pascal}. Those datasets meet the demands of latest data-driven and deep neural network based method~\citep{lee2020maskgan, wang2015joint, wang2015semantic}. 
However, a well-curated part segmentation dataset containing fine-grained pixel-wise annotation usually requires time-consuming manual labelling work, which holds back the development of this field. 
This problem also motivates the latest work~\citep{zhang2021datasetgan, li2021semantic, Tritrong2021RepurposeGANs} to leverage a generative model as an infinite data generator in a semi-supervised learning setting, which gains promising results. 

\paragraph{Gradient matching} 
%Modern deep learning optimization methods are based on the gradients of loss function with respect to network parameters. 
%Gradients contain rich information such that training data might be recovered from gradients~\citep{zhao2020idlg, yin2021see}.
Gradient matching is an important technique in meta learning~\citep{li2018learning, sariyildiz2019gradient}, which is shown to be effective in domain generalization~\citep{li2018learning}, zero-shot classification~\citep{sariyildiz2019gradient}, and dataset condensation~\citep{zhao2020dataset, zhao2021dataset} \etc. 
Although our work's general principle and gradient matching loss function resemble those in the above works, we are motivated to solve a different problem.
In particular, Gradient Matching Network (GMN)~\citep{sariyildiz2019gradient} employs gradient matching to train a conditional generative model towards application on zero-shot classification.
Dataset Condensation (DC)~\citep{zhao2020dataset, zhao2021dataset} condenses a large-scale dataset into a small synthetic one with gradient matching. 
Our work shows that gradient matching is also an effective way to learn annotations of GAN-generated images from limited labelled examples.
Moreover, in contrast to DC, where the synthesized images are typically not realistic, our work is concerned about photo-realistic synthetic images.

\section{Method}
\subsection{Preliminary}
Let $G$ denote a generative model trained from a dataset $\calD_{tot}$ without labels. 
$G$ can be trained via adversarial learning~\citep{goodfellow2014generative} such that it can generate photo-realistic images. The generation process is formulated as 
\begin{equation}
	(\rvh, \rvx) = G(\rvz),~~\rvz \sim P_z
\end{equation}
where $\rvz$ denotes a random variable drawn from a prior distribution $P_z$ which is typically a normal distribution, $\rvx$ denotes the generated image, and $\rvh$ represents the hidden features in $G$.
An annotator $A_\omega$ is constructed to decode $\rvh$ into labels 
\begin{equation}
	\hat{\rvy} = A_\omega(\rvh),
\end{equation}
where $\omega$ parameterizes the annotator.
Let $\calD_l$ denote a manually labeled dataset. Given a fixed $G$, our problem is to learn annotator $A_\omega$ from $\calD_l$ such that $A_\omega$ can annotate images generated from $G$.

\paragraph{Revisiting supervised annotator learning} DatasetGAN~\citep{zhang2021datasetgan} and RepurposeGAN~\citep{Tritrong2021RepurposeGANs} train annotators in a supervised learning manner. First, a set of synthetic images $\{\rvx_i\}_{i=1}^N$ generated from $G$ are selected and annotated by humans and their generator features $\{\rvh_i\}_{i=1}^N$ are reserved. These features and manual labels $\{{\rvy}_i\}_{i=1}^N$ constitute the labeled dataset $\calD_l=\{(\rvx_i, \rvh_i, {\rvy}_i)\}_{i=1}^N$. Second, an annotator is trained by minimizing the loss computed between annotator prediction 
%\hb{automatic? are we talking about the equation below? I thought that those annotations are manual. Then they train another network with the automatic ones} 
and ground truth labels,
\begin{equation}
	\omega = \arg\min_{\omega} \mathbb{E}_{(\cdot, \rvh, {\rvy})\sim\calD_l} f(A_\omega(\rvh), {\rvy}),
\end{equation}
where $f$ denotes per-pixel cross-entropy function for segmentation tasks. 
This method, however, requires $\calD_l$ to be a \textit{synthetic} and \textit{generator-specific} set, which has the following two defects. 
(i) Details might be illegible in synthetic images, which increases the difficulty of annotation. 
(ii) Data needs to be re-labelled whenever the generator is changed. 
Although projecting labelled images into GAN latent space can alleviate the above drawbacks, it leads to degraded performance due to inexact inversion (see \Secref{sec:appendix_inversion} in the appendix).

\subsection{Learning to annotate with Gradient Matching}

\begin{figure}[t]\centering
	\includegraphics[width=0.95\linewidth]{figs/illustration}
	\caption{\textbf{Illustration} of learning to annotate with gradient matching. The procedure is as follows. (i) The gradients of segmentation network $S_\theta$ on labeled examples are computed, denoted as $\nabla_\theta\calL_l(\theta)$. (ii) A batch of synthetic data is \textit{randomly} generated by generator $G$ and labeled by annotator $A_\omega$. (iii) The gradients of $S_\theta$ on synthetic data are computed, denoted as $\nabla_\theta\calL_g(\theta, \omega)$. (iv) Gradient matching between $\nabla_\theta\calL_l(\theta)$ and $\nabla_\theta\calL_g(\theta, \omega)$ is computed to optimize $A_\omega$.}
	\label{fig:illustration}
\end{figure}

\paragraph{Learning to learn} We formulate the problem of learning $A_\omega$ as a learning-to-learn problem like~\citet{li2019learning, pham2021meta}. 
In particular, we introduce a segmentation network $S_\theta$, a deep neural network parameterized with $\theta$. 
It takes as input an image and outputs a segmentation mask.
This segmentation network learns the target task only from the automatically labeled generated images, which is formulated as 
\begin{equation}\label{eq:opt_student}
    \theta^*(\omega) = 
    \arg\min_\theta \underbrace{\mathbb{E}_{(\rvh,\rvx)=G(\rvz), \rvz \sim P_z} f(S_\theta(\rvx), A_\omega(\rvh))}_{\calL_g(\theta, \omega)}.
\end{equation}
where $f$ denotes the loss function for the target task.
The optimal solution of Equation~\ref{eq:opt_student} depends on the $\omega$, which is therefore denoted as $\theta^*(\omega)$.
The performance of learned segmentation network can be evaluated with loss on the labeled dataset $\calD_l$ as
\begin{equation}
    \calL_l(\theta^*(\omega)) = \mathbb{E}_{(\rvx, {\rvy})\in\calD_l}
    f(S_{\theta^*(\omega)}(\rvx), {\rvy}).
\end{equation}
%The annotator learns to produce high-quality labels such that the taught student could gain 
The aim of learning the annotator is to produce high-quality labels such that the learned segmentation network achieve minimal loss on $\calD_l$, which is formulated as 
%\notecxt{In order to let the student gain high performance, learning the annotator becomes a necessity.}
\begin{equation}\label{eq:opt_teacher}
    \begin{array}{rl}
        \min\limits_\omega & \calL_l(\theta^*(\omega)), \\
        \text{where} & 
        \theta^*(\omega) = \arg\min\limits_\theta \calL_g(\theta, \omega).
    \end{array}
\end{equation} 
This problem is a nested-loop optimization problem, where the inner loop optimizes the segmentation network (\Eqref{eq:opt_student}) and the outer loop optimizes the annotator based on the solution of the inner loop.
This optimization problem does not require manual labelling of generated images.
To this end, any labelled images, including \emph{real} labelled images, can serve as $\calD_l$.  
While this problem can be solved with MAML-like~\citep{finn2017model} approaches~\citep{li2019learning, pham2021meta}, its solution involves expensive unrolling of the computational graph over multiple $\theta$ states, which is memory and computation intensive. 
Hence, we reduce it into a simple gradient matching problem as follows. 
See Section~\ref{sec:appendix_inversion} in the appendix for further comparison and discussion.

\paragraph{Gradient matching} Inspired by~\citet{li2018learning}, we reduce the nested-loop optimization problem (\Eqref{eq:opt_teacher}) into a gradient matching problem.
First, as commonly done in gradient-based meta learning approaches, the optimization of segmentation network (\Eqref{eq:opt_student}) is approximated with one-step gradient descent,
\begin{equation}\label{eq:sgd}
    \theta^*(\omega) \approx 
%   \theta_{t+1}(\omega) = 
    \theta_0 - \eta \nabla_{\theta} \calL_g(\theta_0, \omega),
\end{equation}
where $\eta$ denotes the learning rate and $\theta_0$ denotes the initialized parameter. 
By plugging this equation into $\calL_l(\theta^*(\omega))$, unfolding it as Taylor series and omitting the higher order term, we have
\begin{equation}\label{eq:taylor}
    \begin{aligned}
        \calL_l(\theta^*(\omega)) 
        & \approx \calL_l(\theta_0 - \eta \nabla_{\theta} \calL_g(\theta_0, \omega)) \\
        & = \calL_l(\theta_0) - \eta \nabla_{\theta}^\top\calL_l(\theta_0) \nabla_{\theta} \calL_g(\theta_0, \omega) + \dots \\
        & \approx \calL_l(\theta_0) - \eta \nabla_{\theta}^\top\calL_l(\theta_0) \nabla_{\theta} \calL_g(\theta_0, \omega)
    \end{aligned}
\end{equation}
The last approximation is conditioned on $\lVert\eta\nabla_{\theta}\calL_g(\theta_0, \omega)\rVert \le \epsilon$, where $\epsilon$ is a very small amount. 
By further omitting the constant term $\calL_l(\theta_0)$ in \Eqref{eq:taylor}, we obtain the approximation of \Eqref{eq:opt_teacher} as
\begin{equation}\label{eq:gradient_matching_cons}
\min\limits_\omega 
%\calL_{gm}(\theta_t, \omega) = 
- \nabla_{\theta}^\top\calL_l(\theta_0) \nabla_{\theta} \calL_g(\theta_0, \omega)
~~~~~~~s.t.~~\lVert\eta\nabla_{\theta}\calL_g(\theta_0, \omega)\rVert \le \epsilon. 
\end{equation}
We notice a connection between this constrained optimization problem with the following unconstrained optimization problem 
\begin{equation}\label{eq:gradient_matching_uncons}
    \min\limits_\omega 
    %\calL_{gm}(\theta_t, \omega) = 
    - \frac{\nabla_{\theta}^\top\calL_l(\theta_0) \nabla_{\theta} \calL_g(\theta_0, \omega)}{\lVert\eta\nabla_{\theta}\calL_g(\theta_0, \omega)\rVert},
\end{equation}
where to minimize this objective, one needs to simultaneously maximize the dot product between the gradients and minimize the norm of $\nabla_{\theta}\calL_g(\theta_0, \omega)$.
Note that 
$- \frac{\nabla_{\theta}^\top\calL_l(\theta_0) \nabla_{\theta} \calL_g(\theta_0, \omega)}{\lVert\eta\nabla_{\theta}\calL_g(\theta_0, \omega)\rVert} = \frac{\lVert\nabla_{\theta}\calL_l(\theta_0)\rVert}{\eta} 
(1 - \frac{\nabla_{\theta}^\top\calL_l(\theta_0) \nabla_{\theta} \calL_g(\theta_0, \omega)}{\lVert\nabla_{\theta}\calL_l(\theta_0)\rVert 
\lVert\nabla_{\theta} \calL_g(\theta_0, \omega)\rVert}) - \frac{\lVert\nabla_{\theta}\calL_l(\theta_0)\rVert}{\eta}$, and the constant term and constant coefficient can be omitted without changing the optimal solution. 
We re-write the learning objective of \Eqref{eq:gradient_matching_uncons} as 
\begin{equation}\label{eq:gradient_matching_cosine}
    \calL_{gm}(\theta_0, \omega) = 
    D(\nabla_{\theta}\calL_l(\theta_0), \nabla_{\theta} \calL_g(\theta_0, \omega)) = 
    1 - \frac{\nabla_{\theta}^\top\calL_l(\theta_0) \nabla_{\theta} \calL_g(\theta_0, \omega)}{\lVert\nabla_{\theta}\calL_l(\theta_0)\rVert 
        \lVert\nabla_{\theta} \calL_g(\theta_0, \omega)\rVert}, 
\end{equation}
which computes the cosine distance between the gradients of segmentation network on \emph{automatically} labeled generated data and \emph{manually} labeled data (see \Secref{sec:appendix_gm} in appendix for dealing with gradient matching in multi-layer neural networks).
To this end, the nested-loop optimization (\Eqref{eq:opt_teacher}) is reduced into a simple one that minimizes the gradient matching loss (\Eqref{eq:gradient_matching_cosine}).
An illustration of this procedure is presented in~\Figref{fig:illustration}.

\paragraph{Algorithm} 
Note that the gradient matching problem learns an annotator for a particular network parameterized with $\theta_0$. 
However, a desirable annotator should produce universal automatic labels for training \emph{any} segmentation network with previously unseen parameters. 
Hence, we present an alternate training algorithm of annotator and segmentation network in \Algref{alg:gm_label}, where the segmentation network is iteratively updated using the automatically labelled synthetic data, and the annotator is iteratively updated with gradient matching.
This algorithm allows the annotator to be optimized over various segmentation network states, leading to a segmentation-network-agnostic annotator.
The gradient matching problem for each segmentation network parameter is solved with $K$-step gradient descent, and typically $K=1$ works well in practice. 
Implementation details are available in the appendix (see \Secref{sec:appendix_impl}).

\begin{algorithm}[t]\label{alg:gm_label}
	\footnotesize
	\DontPrintSemicolon
	\SetAlgoLined
	\SetKwInOut{Input}{Inputs}
	\SetKwInOut{Output}{Output}
	\Input{
		\par
		\begin{tabular}{l l}
			$G$						& trained generator\\
			$\mathcal{D}_l$ 		& set of labeled examples\\
			$\omega$, $A_\omega$, $\eta_a$  	& initial annotator parameters, annotator, and learning rate for annotator\\
			$\theta$, $S_\theta$, $\eta_s$ 	& initial segmentor parameters, segmentor, and learning rate for segmentor\\
			$T$, $K$, and $B$ 		& total number of optimization steps, interval
			 of updating segmentor, and batch size\\
	\end{tabular}}
	\For{$t\gets1$ \KwTo $T$}{ 
	\tcp{update annotator $A_\omega$}
	$\calB_{l} \gets \left\{(\rvx_i, {\rvy}_i)\sim\calD_l\right\}_{i=1}^B$ \tcp*{sample a batch of labeled examples}
	$\nabla_\theta\calL_l \gets \frac{1}{B} \sum_{i=1}^B \nabla_\theta f(S_\theta(\rvx_i), {\rvy}_i)$ \tcp*{compute gradients on $\calB_{l}$}   
	$\calB_g \gets \left\{\left(\rvx_j, A_\omega(\rvh_j)\right)\right\}_{j=1}^B$ \tcp*{sample a batch of synthetic data}
	$\nabla_\theta\calL_g \gets \frac{1}{B} \sum_{j=1}^B \nabla_\theta f(S_\theta(\rvx_j), A_\omega(\rvh_j))$ \tcp*{compute gradients on $\calB_{g}$}
	$\calL_{gm} \gets D(\nabla_{\theta}\calL_g, \nabla_{\theta}\calL_l)$ \tcp*{compute gradient matching loss}
	$\omega \gets \omega - \eta_a \nabla_\omega \calL_{gm}$ \tcp*{update annotator parameters}
	\vspace{5pt}
	\tcp{update segmentor $S_\theta$}
	\If{$t \mod K = 0$}{
		$\calB_g \gets \left\{\left(\rvx_j, A_\omega(\rvh_j)\right)\right\}_{j=1}^B$ \tcp*{sample a batch of synthetic data}
		$\calL_g \gets \frac{1}{B}\sum_{j=1}^B f(S_\theta(\rvx_j), A_\omega(\rvh_j))$ \tcp*{compute loss on synthetic data}
		$\theta \gets \theta - \eta_s \nabla_\theta \calL_g$ \tcp*{update segmentor parameters}
	}
}
\Output{annotator $A_\omega$ and segmentor $S_\theta$}
\caption{Learning to annotate with gradient matching.}
\end{algorithm}
\vspace{-10pt}

\section{Experiments}
\subsection{Setup}
\paragraph{Datasets} We evaluate our method on six part segmentation datasets: \textbf{CelebA}, \textbf{Pascal-Horse}, \textbf{Pascal-Aeroplane}, \textbf{Car-20}, \textbf{Cat-16}, and \textbf{Face-34}. CelebA~\citep{liu2015faceattributes} is a large-scale human face dataset where 30,000 images are annotated with up to 19 part classes, also known as CelebAMask-HQ~\citep{CelebAMask-HQ}. 
We consider a subset of 8 face classes and use the first 28,000 images as an unlabeled set, the last 500 images as a test set, and the rest 1500 images as a labelled set, as in \citet{li2021semantic}.
Pascal-Horse and Pascal-Aeroplane are constructed by taking images of horse and aeroplane from PascalPart~\citep{chen2014detect} which provides detailed part segmentation annotations for images from Pascal VOC 2010~\citep{pascal-voc-2010}. 
The selected images are cropped according to bounding box annotations. Full process details are available in the appendix. 
We finally obtain 180 training images, 34 validation images, and 225 test images to constitute Pascal-Horse, 180 training images, 78 validation images, and 269 test images to constitute Pascal-Aeroplane. 
In Pascal-Horse and Pascal-Aeroplane, additional images from LSUN~\citep{yu2015lsun} are utilized as an unlabeled set.
Car-20, {Cat-16}, and {Face-34}, released by ~\citet{zhang2021datasetgan}, are three part-segmentation datasets annotated with 20, 16, and 34 part classes for car, cat, and human face, respectively. We use the same datasets to make a fair comparison to DatasetGAN~\citep{zhang2021datasetgan}. 
In these datasets, all the training images are images generated from pre-trained StyleGAN~\citep{karras2019style} while all the test images are real images.
Car-20 contains 16 training images and 10 test images; Cat-16 contains 30 training images and 20 test images; Face-34 contains 16 training images and 20 test images. 

\paragraph{Pre-trained generators} We use models of StyleGAN family~\citep{karras2019style, karras2020analyzing, karras2020training} that are either trained by ourselves or publicly available. Details are provided in the appendix (see \Secref{sec:appendix_impl}).

\paragraph{Evaluation} 
We use mean intersection over union (mIoU) to evaluate the performance of segmentation networks.
On CelebA, Pascal-Horse, and Pascal-Aeroplane, we use the validation set to select checkpoints and report mIoU across all foreground classes, denoted as ``FG-mIoU'', on the test set. On Car-20, Cat-16 and Face-34, we follow the setting of DatasetGAN~\citep{zhang2021datasetgan} and report the cross-validation mIoU across all classes, including background on the test set.

\subsection{Semi-Supervised Part Segmentation}

\begin{figure}[t]\centering
%	\includegraphics[width=\linewidth]{figs/benchmark}
\begin{overpic}[percent,width=\linewidth]{figs/benchmark}
\put(26,4.15){\scriptsize \citep{li2020transformation}} 	% MT ref 
\put(26.5,1.75){\scriptsize \citep{ke2020guided}} 		% GCT ref
\put(54.5,4.15){\scriptsize \citep{hung2018adversarial}} % AdvSSL ref
\put(58.5,1.75){\scriptsize \citep{li2021semantic}} 		% SemanticGAN ref
\end{overpic}	
\caption{\textbf{Benchmark} on CelebA (\emph{Left}), Pascal-Horse (\emph{Middle}), and Pascal-Aeroplane (\emph{Right}). 
The y-axes are FG-mIoU (\%) on test set. 
}
\label{fig:benchmark}	
\end{figure}

%\begin{figure}[t]\centering
%%	\includegraphics[width=\linewidth]{figs/benchmark}
%\begin{overpic}[percent,width=\linewidth]{figs/benchmark_2}
%\put(21,4.15){\scriptsize \citep{li2020transformation}} 	% MT ref 
%\put(21.5,1.75){\scriptsize \citep{ke2020guided}} 		% GCT ref
%\put(50,4.15){\scriptsize \citep{hung2018adversarial}} % AdvSSL ref
%\put(54,1.75){\scriptsize \citep{li2021semantic}} 		% SemanticGAN ref
%\end{overpic}	
%\caption{\textbf{Benchmark} on CelebA (\emph{Left}), Pascal-Horse (\emph{Middle}), and Pascal-Aeroplane (\emph{Right}). 
%	The y-axes are FG-mIoU (\%) on test set. 
%}
%\label{fig:benchmark}	
%\end{figure}

Since our method has a mild requirement for the labelled data, in this section, we show the effectiveness of our method under three different circumstances: real images as labelled data, synthetic images as labelled data, and out-of-domain images as labelled data.

\paragraph{Real images as labeled data} 
Following SemanticGAN~\citep{li2021semantic}, our methods are compared against SSL methods that have codes available\footnote{We adopt the implementation in \url{https://github.com/ZHKKKe/PixelSSL}. For these SSL methods, the \emph{real} unlabeled images are used as unlabeled set.}, including Mean Teacher (MT)~\citep{tarvainen2017mean, li2020transformation}, Guided Collaborative Training (GCT)~\citep{ke2020guided}, an adversarial-learning-based semi-supervised segmentation method (AdvSSL)~\citep{hung2018adversarial}, as well as SemanticGAN~\citep{li2021semantic}. 
We benchmark performances on CelebA, Pascal-Horse, and Pascal-Aeroplane with respect to a different amount of labelled data and present the results in \Figref{fig:benchmark}. 
Qualitative results are available in the appendix (see~\Figref{fig:qualitative_celeba},~\ref{fig:qualitative_horse},~\ref{fig:qualitative_aeroplane}).
Our method significantly outperforms other SSL methods when labelled data is extremely limited. 
When the number of labelled images decreases, the performance of our method drops mildly while the performances of other methods decrease drastically. 
We attribute it to our utilization of the highly interpretable generator features. 
Notably, SemanticGAN~\citep{li2021semantic} also exploits the generative features to produce segmentation masks, but its performance degrades faster than our methods as the number of labelled images decreases. 
We conjecture it is due to that the adversarial learning employed by SemanticGAN typically requires sufficient data to prevent discriminators from overfitting. 
The edges of our method become marginal under the case of a large number of labels.
% It can be foreseen that our method is less competitive in the large-scale setting. 
One possible reason is that the quality of pre-trained GAN needs to be further improved, which requires more research in the future. (see Section~\ref{sec:large_scale} for more discussion)

\begin{table}[t]\centering
\tablestyle{5.7pt}{1.2}\begin{tabular}{lcc|ccc}
	\shline
	Methods & $G$ & $S$ & 
	\tabincell{c}{\vspace{-2pt}Cat-16\\\vspace{-5pt}{\scriptsize\# annotations: 30}\\{\scriptsize\# classes: 16} } &  
	\tabincell{c}{\vspace{-2pt}Face-34\\\vspace{-5pt}{\scriptsize\# annotations: 16} \\{\scriptsize\# classes: 34} } & 
	\tabincell{c}{\vspace{-2pt}Car-20\\\vspace{-5pt}{\scriptsize\# annotations: 16} \\{\scriptsize\# classes: 20}}\\
	\hline
	TL$^\dagger$ & - & DeepLabv3 
	& 21.58 $\pm$ 0.61 & 45.77 $\pm$ 1.51  & 33.91 $\pm$ 0.57 \\
	SSL~\citep{mittal2019semi}$^\dagger$ & - & DeepLabv3
	& 24.85 $\pm$ 0.35 & 48.17 $\pm$ 0.66  & 44.51 $\pm$ 0.94  \\
	\hline
	% ----------------------------------------------------------------------------------
	\multirow{2}{*}{\tabincell{l}{DatasetGAN~\\\citep{zhang2021datasetgan}}}	& {StyleGAN} & DeepLabv3$^\ddagger$
	& 32.63 $\pm$ 0.68 & 54.55 $\pm$ 0.25  & 67.53 $\pm$ 2.58 \\	
	& {StyleGAN} & U-Net$^\sharp$
	& 31.36 $\pm$ 0.76 & 53.84 $\pm$ 0.41 & 66.27 $\pm$ 2.75  \\
	\hline
	% ----------------------------------------------------------------------------------
	\multirow{4}{*}{Ours}
	& {StyleGAN}  & DeepLabv3
	& 33.89 $\pm$ 0.43 & 52.58 $\pm$ 0.61 & 63.55 $\pm$ 2.25 \\
	& {StyleGAN}  & U-Net
	& 32.64 $\pm$ 0.74 & 53.69 $\pm$ 0.54 & 60.45 $\pm$ 2.42 \\
	\cline{2-6}
	& {StyleGAN2} & DeepLabv3
	& 33.56 $\pm$ 0.17 & 55.10 $\pm$ 0.39 & 61.21 $\pm$ 2.07 \\
	& {StyleGAN2} & U-Net	
	& 31.90 $\pm$ 0.75 & 53.58 $\pm$ 0.45 & 58.30 $\pm$ 2.64 \\
	\shline 
\end{tabular}
% ----------------------------------------------------------------------------------
\tablestyle{2pt}{1.2}\begin{tabular}{l|cc|cc|cc}
	\shline
	\multicolumn{7}{c}{Our downstream segmentation performance} \\
	\hline
	& \multicolumn{2}{c|}{Cat-16} & \multicolumn{2}{c|}{Face-34} & \multicolumn{2}{c}{Car-20} \\
	Source $\backslash$ Downstream & DeepLabv3 & U-Net & DeepLabv3 & U-Net & DeepLabv3 & U-Net \\
	\hline
	DeepLab & 33.38 $\pm$ 0.66 & 33.39 $\pm$ 0.74 & 55.11 $\pm$ 0.63 & 54.77 $\pm$ 0.32 
			& 63.47 $\pm$ 2.33 & 62.72 $\pm$ 2.89 \\
	U-Net 	& 33.38 $\pm$ 0.40 & 32.42 $\pm$ 0.62 & 54.05 $\pm$ 0.40 & 53.80 $\pm$ 1.06 
			& 63.22 $\pm$ 2.42 & 62.25 $\pm$ 2.77 \\		
	\shline	
\end{tabular}
\caption{\textbf{Comparisons to DatasetGAN} on Car-20, Cat-16, Face-34. 
% with different network architectures. 
The performance is evaluated with mIoU(\%).
%  across all part classes plus a background class. 
TL: transfer learning. SSL: semi-supervised learning. $\dagger$: Results taken from \citet{zhang2021datasetgan}. $\ddagger$: Up-to-date performance from \href{https://github.com/nv-tlabs/datasetGAN_release}{DatasetGAN github repository}.
$\sharp$: Results obtained by ourselves using DatasetGAN source codes.
}
\vspace{-8pt}
\label{tab:comparison_to_dg}
\end{table}

\paragraph{Synthetic images as labeled data} 
We comprehensively compare our method to a supervised learning counterpart, DatasetGAN~\citep{zhang2021datasetgan} on Car-20, Cat-16, and Face-34.
We follow the same setting as in DatasetGAN, using precisely the same synthetic images as training data.
Quantitative results are presented in Table~\ref{tab:comparison_to_dg} and qualitative comparison is available in the appendix (see \Figref{fig:qualitative_cat16}, \ref{fig:qualitative_face34}, \ref{fig:qualitative_car20}). 
First, our method achieves the same performance level with DatasetGAN on Face-34 and Cat-16. 
Second, as a merit of our method, we can upgrade the generator to StyleGAN2~\citep{karras2020analyzing, karras2020training} without requiring extra human effort.
However, upgrading the generator brings modest improvement on Face-34 and Cat-16 and even a bit decrease on Car-20.
This result suggests that synthesis quality measured by metrics like FID can not precisely depict the disentanglement of generator features.
Finally, our method could adapt to different segmentation architectures such as DeepLabv3~\citep{chen2017rethinking} and U-Net~\citep{ronneberger2015u}. 
% Generally, DeepLabv3 would leads to slightly higher performance than U-Net.

We further use our trained annotator to generate synthetic segmentation datasets for training downstream segmentation networks. 
Table~\ref{tab:comparison_to_dg} shows the results when we use different source networks for training annotators with gradient matching and different downstream networks. 
First, our learned synthetic datasets can be used to train downstream segmentation networks of different architecture that achieve good performances. 
This result indicates that our learned synthetic dataset is not architecture-specific. 
Second, DeepLabv3 as a source network generally leads to higher performances than U-Net, suggesting that the segmentation networks used for gradient matching affect annotator learning in our method.

\begin{figure}[t]\centering
	\includegraphics[width=0.95\linewidth]{figs/cross_domain_demo}
	\caption{\textbf{Cross-domain annotator learning demo.} (a) Out-of-domain labelled examples: car images and ground truth segmentations rendered from 3D CAD model. (b) Generated images with learned annotations: our method learns a reasonable annotator for GAN generated images.}
	\label{fig:cross_domain_demo}
\end{figure}

\paragraph{Out-of-domain images as labeled data} 
We further consider a challenge where out-of-domain images constitute labelled data. 
In particular, we render a set of car images from annotated 3D CAD models provided by CGPart\footnote{\url{https://qliu24.github.io/cgpart/}}~\citep{liu2021cgpart} using graphic tools (\eg blender).
Since 3D models are annotated with parts, the ground truth part segmentation of these rendered images are available via 3D projection.
These rendered images vary in viewpoints and textures and have a significant domain gap compared to realistic car images due to inexact 3D models, unrealistic textures, artificial lighting \etc, raising challenges for learning annotators. 
Despite so, as presented in \Figref{fig:cross_domain_demo}, our method still learns reasonable annotators that produce quality segmentation labels for GAN generated images. 
It is noteworthy that it is nearly impossible for DatasetGAN~\citep{zhang2021datasetgan} and RepurposeGAN~\citep{Tritrong2021RepurposeGANs} to utilize such out-of-domain labelled data. 
% The ``inversion'' method basically produces inferior results in this setting because the pre-trained GAN generator is not trained to generate out-of-domain images and therefore the projection into latent space is wrong.

\subsection{Ablation Study}

\begin{figure}[t]\centering
	\includegraphics[width=0.95\linewidth]{figs/ablation_eint}
	\caption{\textbf{Ablation study w.r.t. $K$}. Evolution of the learned annotator on Face-34 (first row) and Cat-16 (second row). (a) Annotator performance (mIoU (\%)) on training data \wrt number of updating annotator steps ($K$). (b) Examples of generated images. (c) Evolution of automatic labels. }
	\label{fig:ablation_eint}
\end{figure}

\paragraph{The frequency of updating segmentation network} 
The results of an ablation study \wrt the interval of updating segmentation network, $K$, is shown in \Figref{fig:ablation_eint}.
Experiments are run on Face-34 and Cat-16 with DeepLabv3 as segmentation network and StyleGAN as generator such that pairs of generator features and ground truth segmentation are available to measure the performance of the annotator quantitatively. 
It shows that the annotator learns significantly faster when the segmentation network is updated more frequently. 
The visualization shows that very detailed parts, \eg pupils in Face-34 and eyes in Cat-16, emerge sooner if the segmentation network is updated more frequently. 
It is more effective to match gradients for various segmentation network parameters than optimize gradient matching for single segmentation network parameters.

\begin{table}[t]\centering
\tablestyle{4pt}{1.2}\begin{tabular}{l|cccccc}
	\shline
	Blocks 		& All & Res2$\sim$Head & Res3$\sim$Head & Res4$\sim$Head & Res5$\sim$Head & * Head \\
	\hline
	% Time (sec)/ step 	& 1.17 & 1.17 & 1.15 & 1.08 & 0.57 & 0.35 \\
	% Time / step 	& 1$\times$ & 1$\times$ & 0.98$\times$ & 0.92$\times$ & 0.48$\times$ & 0.29$\times$ \\
	Time / step 	& 3.34$\times$ & 3.34$\times$ & 3.28$\times$ & 3.08$\times$ & 1.62$\times$ & 1$\times$ \\
	mIoU (\%) 			& 33.71 $\pm$ 0.77 & 34.38 $\pm$ 0.49 & 33.81 $\pm$ 0.99 & 33.78 $\pm$ 0.70 & 34.12 $\pm$ 0.57 & 33.56 $\pm$ 0.17 \\
	\shline	
\end{tabular}
\caption{\textbf{Ablation study \wrt blocks for matching gradients}. Evaluation is performed on Cat-16, where StyleGAN2 is the generator and DeepLabv3 is the segmentation network. 
``Time / step'' is measured as the ratio with respect to the default setting. * denotes the default setting.}
\label{tab:ablation_gmparam}
\end{table}

\paragraph{Gradient matching on partial segmentation network parameters} 
As the implementation of gradient matching requires maintenance of computation graph of backpropagation through segmentation networks, one may be concerned about the training efficiency for very deep neural networks. 
Here we present a simple strategy to trade off the training efficiency and overall performance: gradient matching can be done only on the part of segmentation network parameters.
Table~\ref{tab:ablation_gmparam} presents an ablation study about matching blocks on Cat-16, where StyleGAN2 is the generator and DeepLabv3 is the segmentation network.
We consider network blocks in DeepLabv3 with ResNet-101 as backbone from bottom to top: \texttt{Res1}, \texttt{Res2}, \texttt{Res3}, \texttt{Res4} and \texttt{Res5} that are residual blocks in the backbone, and \texttt{Head} that is the segmentation head.
Results show that skipping the bottom blocks for gradient matching improves the training speed while the segmentation performance is affected little. 
We conjecture the reasons can be two folds: (i) the backbone parameters are pre-trained such that little needs to be changed during training; (ii) the gradients accumulate more randomness as they backpropagate to the bottom blocks of deep neural networks.
Therefore, in practice, we only match the gradients of \texttt{Head} in DeepLabv3 by default. Additional results are available in the appendix (see \Secref{sec:efficiency}).

\section{Conclusion}
We propose a gradient-matching-based method to learn annotator, which can label generated images with part segmentation by decoding the generator features into segmentation masks. 
Unlike existing methods that require labelling generated images for training an annotator, our method allows a broader range of labelled data, including realistic images, synthetic images, and even rendered images from 3D CAD models. 
On the benchmark of semi-supervised part segmentation, our method significantly outperforms other semi-supervised segmentation methods under the circumstances of extremely limited labelled data but becomes less competitive under large-scale settings, which requires future research. 
The effectiveness of our method is validated on a variety of single-class datasets, including well-aligned images, \eg CelebA and Face-34, as well as images in the wild part of which contain cluttered backgrounds, \eg Pascal-Horse, Pascal-Aeroplane, Car-20 and Cat-16. 
In terms of more complex scenes, discussion and investigation are further needed. 
With the rapid progress of generative modelling, it is promising to use more powerful generative models and explore more computer vision problems under different challenging settings in the future. 

\subsubsection*{Acknowledgements}
This work was supported by the National Key R\&D Program of China under Grant 2018AAA0102801, National Natural Science Foundation of China under Grant 61620106005, and Beijing Municipal Science and Technology Commission grant Z201100005820005.
Hakan Bilen is supported by the EPSRC programme grant Visual AI EP/T028572/1.

\subsubsection*{Reproducibility Statement}
The appendix provides all the details about the datasets, network structure, and training curriculum. 
Code to reproduce our main results is publicly available at \url{https://github.com/yangyu12/lagm}.

\bibliography{iclr2022_conference}
\bibliographystyle{iclr2022_conference}
\clearpage

\renewcommand\thefigure{\thesection.\arabic{figure}}
\renewcommand\thetable{\thesection.\arabic{table}}
\setcounter{figure}{0}
\setcounter{table}{0}
\appendix
\section{Dataset Details}

\paragraph{CelebA} Following \citet{li2021semantic}, we only consider 8 part classes: background, ear, eye, eyebrow, skin, hair, mouth, and nose. Table~\ref{tab:celeba_details}(Left) presents the protocol for merging the 19 part classes into 8 classes.
CelebAMask-HQ contains 30,000 annotated images in total.
We split this dataset into the unlabeled set, training set, validation set and test set as in Table~\ref{tab:celeba_details}(Right).

\begin{table}[h]\centering
\tablestyle{5pt}{1.2}\begin{tabular}{l|l}
	\shline
	8 classes  	& 19 classes \\
	\hline
	background 	& background, hat, ear\_r, neck\_l, neck, cloth \\
	ear 		& l\_ear, r\_ear \\
	eye 		& l\_eye, r\_eye \\
	eyebrow 	& eyebrow \\
	skin 		& rest \\
	hair 		& hair \\
	mouth 		& mouth, u\_lip, r\_lip \\
	nose 		& nose	\\
	\shline	
\end{tabular}
~~~
\tablestyle{3.5pt}{1.2}\begin{tabular}{l|l}
	\shline
	Split 		& Image id \\
	\hline
	Unlabeled set 		& 1$\sim$28,000 \\
	Training set (2) 	&  28,001$\sim$28,002 \\
	Training set (10) 	&  28,001$\sim$28,010 \\
	Training set (30) 	&  28,001$\sim$28,030 \\
	Training set (150) 	&  28,001$\sim$28,150 \\
	Training set (1500) &  28,001$\sim$29,500 \\
	Validation set 		& 27,501$\sim$28,000 \\
	Test set 			& 29,501$\sim$30,000 \\
	\shline	
\end{tabular}
\caption{\textit{Left}: Protocol for merging part classes on CelebA. \textit{Right}: Dataset split on CelebA.}
\label{tab:celeba_details}
\end{table}

\paragraph{Pascal-Horse \& Pascal-Aeroplane} Pascal Part~\citep{chen2014detect} provides part segmentation annotations of 20 object classes for images in Pascal VOC 2010.
Following other work on part segmentation~\citep{tsogkas2015deep, zhao2019ordinal, Tritrong2021RepurposeGANs}, we merge the fine-grained part classes into 6 classes for horse and aeroplane. The merging protocol is presented in Table~\ref{tab:pascal_merge}.
We crop the image patches that contain the object of interest according to bounding box annotations. 
Concretely, we discard bounding boxes with IoU with other boxes smaller than 0.05  to ensure a single object appears in a single image patch. 
The patches that have any side less a certain number of pixels (32 for horse; 50 for aeroplane) are also abandoned.
The crop regions are extended from bounding boxes to square boxes, and the regions outside images are padded with zeros.
These processed patches are finally resized to $256\times256$ resolution to serve the training and evaluation process.
For horse and aeroplane, We further split the patches in official VOC 2010 \texttt{train} split in \textit{training} and \textit{validation} set, and take the patches in official VOC 2010 \texttt{val} split as \textit{test} set.
This procedure finally provides 180 images as the training set (\ie labelled set), 33 images as a validation set, 223 images as the test set in Pascal-Horse, 180 images as labelled set, 78 images as the validation set and 266 images as the test set in Pascal-Aeroplane. 
As unlabeled images, we centre-crop the 200,000 images in the large-scale LSUN~\citep{yu2015lsun} archive and resize them to $256\times256$ resolution for both horse and aeroplane.

\begin{table}[h]\centering
	\tablestyle{5pt}{1.2}\begin{tabular}{l|l}
		\shline
		\multicolumn{2}{c}{Pascal-Horse} \\
		\hline
		6 classes	& fine-grained classes \\
		\hline
		background 	& background \\
		head 		& head, leye, reye, lear, rear, muzzle \\
		torso 		& torso \\ 
		legs 		& lfho, rfho, lbho, rbho, lfuleg, lflleg, rfuleg \\
		neck 		& neck \\
		tail 		& tail \\
		\shline	
	\end{tabular}
~~~~
\tablestyle{5pt}{1.2}\begin{tabular}{l|l}
	\shline
	\multicolumn{2}{c}{Pascal-Aeroplane} \\
	\hline
	6 classes 	& fine-grained classes \\
	\hline
	background 	& background \\
	body 		& body \\
	stern 		& stern, tail \\
	wing 		& lwing, rwing \\
	engine 		& engine\_\{d\} \\
	wheel 		& wheel\_\{d\} \\
	\shline	
\end{tabular}
	\caption{Protocol for merging part classes on Pascal Part. }
	\label{tab:pascal_merge}
\end{table}

\paragraph{CGPart}
% 1. background of CGPart: original paper, object settings, generation method
% 2. rendering pipeline modification:
%	(1) blender rendering (ref to CGPart): angle sampling, distance sampling and fix view for frame to circle around object perfectly
%	(2) texture augmentation: COCO, pixar
% 	(3)
% 3. Object selection (car) (and reason)
% 4. train test split; mask merging protocol
% 5. post processing
As a part segmentation dataset composed of 3D CAD models from 5 vehicle categories with 3D part manual annotations, CGPart~\citep{liu2021cgpart} manage to render a large amount of image dataset with part mask labels by adopting blender, a well-known graphical software. 
In this paper, we select category \textit{car} as the training and evaluation dataset and make moderate modifications upon the rendering pipeline. 
Parts are merged into 15 classes, where excessively fine-grained parts that seldom emerge are combined to avoid sample imbalance. 
Part merging protocol is presented in Table ~\ref{tab:cgpart_merge}. 
Viewpoints are sampled around the object with azimuth ranging in full $360^{\circ}$. 
Elevation angle is varied from $0^{\circ}$ to $80^{\circ}$, yet higher sampling probability is assigned to interval $[0^{\circ}, 30^{\circ}]$. 
The distance from the object to the viewpoint is carefully set to be located at the centre and fully in frame. 
Random colour is painted upon the object, and random texture sampled from COCO dataset~\citep{lin2014microsoft}, and Pixar-One-Twenty-Eight~\citep{sisson2018pixar} is rendered on the ground and the surroundings. $4,000$ images of $512\times384$ are rendered as a full train dataset. When training, only $1,000$ of them are randomly sampled as official trainset. The original \texttt{val} split of CGPart is adopted as \textit{test} dataset, consisting of $40$ images. Test masks are merged under the same merging protocol in the inference and evaluation phases.


\begin{table}[h]\centering
	% \newcommand{\tabincell}[2]{def}
	\tablestyle{5pt}{1.2}\begin{tabular}{p{2cm} | p{10cm}}
		\shline
		\multicolumn{2}{c}{CGPart-Car} \\
		\hline
		15 classes	& fine-grained classes \\
		\hline
		background 	& background \\
		back\_bumper 	& back\_bumper \\
		car\_body 		& left\_frame, right\_frame \\
		door 		& back\_left\_door, back\_right\_door, front\_left\_door, front\_right\_door \\ 
		front\_bumper 		& front\_bumper \\ 
		head\_light 		& left\_head\_light, right\_head\_light \\
		hood 		& hood \\
		licence\_plate 		& back\_license\_plate, front\_license\_plate \\
		mirror 		& left\_mirror, right\_mirror \\
		roof 		& roof \\
		tail\_light 		& left\_tail\_light, right\_tail\_light \\
		trunck 		& trunck \\
		wheel 		& back\_left\_wheel, back\_right\_wheel, front\_left\_wheel, front\_right\_wheel  \\
		window 		& back\_left\_window, back\_right\_window, front\_left\_window, front\_right\_window, left\_quarter\_window, right\_quarter\_window \\
		windshield 		& back\_windshield, front\_windshield \\
		\shline	
	\end{tabular}
	\caption{Protocol for merging part classes on CGPart. }
	\label{tab:cgpart_merge}
\end{table}

\section{Implementation Details}\label{sec:appendix_impl}

\paragraph{Gradient matching loss}\label{sec:appendix_gm}
Following DC~\citep{zhao2020dataset}, the distance between gradients is measured with cosine similarity. 
The computation of gradient matching is quite similar as in DC. Here, we re-state this procedure for clarity.  In particular, considering a multi-layer neural network $S_\theta$ that is parameterized with $\theta$, the gradient matching loss is computed as an average over layerwise losses as 
$ D(\nabla_\theta\calL_l, \nabla_\theta\calL_g) = \frac{1}{L}\sum_{i=1}^{L} d(\nabla_{\theta^{(i)}}\calL_l, \nabla_{\theta^{(i)}}\calL_g)$, where $i$ denotes the layer index, $L$ denotes the number of layers for gradient matching and 
\begin{equation}
    d(\mA, \mB) = \sum_{j=1}^{N} \left(1 - \frac{\mA_j\cdot\mB_j}{\lVert\mA_j\rVert\cdot\lVert\mB_j\rVert}\right)
\end{equation} 
where $\mA_j$ and $\mB_j$ are flattened gradient vectors for each neural node $j$. 
The minor difference of our practice compared DC is that we average the layerwise gradient distances whereas DC sum them. 
It is easier to handle numerical issues using our practice in our problem.
In terms of gradient matching for normalization layers such as Batch Normalization (BN), we follow the practice of DC to ignore the gradient matching of learnable parameters in BN. The BN layer is set as \textit{train} mode during gradient matching. We find it works well.

\paragraph{Pre-trained GAN}
We use StyleGAN family~\citep{karras2019style, karras2020analyzing, karras2020training} as our pre-trained GANs. 
We either train GANs by ourselves for each dataset or use publicly available models.
The training configurations or links of the pre-trained GANs are listed in Table~\ref{tab:pretrained_gan}.
For GANs trained by ourselves, we use the checkpoints with the lowest FID in the historical ones.

\begin{table}[h]\centering
\tablestyle{3.5pt}{1.2}\begin{tabular}{l|ccc}
	\shline
	Dataset 		& Version & Output resolution & Training configurations or links \\
	\hline
    CelebA 			& StyleGAN2-ADA & $256\times256$ & \texttt{--cfg=paper256 --mirror=True} \\
    Pascal-Horse 	& StyleGAN2		& $256\times256$ & \href{https://nvlabs-fi-cdn.nvidia.com/stylegan2/networks/stylegan2-horse-config-f.pkl}{stylegan2-horse-config-f.pkl} \\
    Pascal-Aeroplane& StyleGAN2		& $256\times256$ & \texttt{--cfg=stylegan2 --aug=noaug} \\
    \hline
    Cat-16 			& StyleGAN & $256\times256$ & available in \href{https://github.com/nv-tlabs/datasetGAN_release}{DatasetGAN repo} \\
    Cat-16 			& StyleGAN2& $256\times256$ & \href{https://nvlabs-fi-cdn.nvidia.com/stylegan2-ada/pretrained/paper-fig7c-training-set-sweeps/lsuncat200k-paper256-ada.pkl}{lsuncat200k-paper256-ada.pkl} \\
    Face-34 		& StyleGAN & $512\times512$ & available in \href{https://github.com/nv-tlabs/datasetGAN_release}{DatasetGAN repo} \\
    Face-34 		& StyleGAN2-ADA& $512\times512$ & \texttt{--cfg=paper512 --mirror=True} \\
    Car-20 			& StyleGAN & $512\times512$ & available in \href{https://github.com/nv-tlabs/datasetGAN_release}{DatasetGAN repo} \\
    Car-20 		    & StyleGAN2& $512\times512$ & \href{https://nvlabs-fi-cdn.nvidia.com/stylegan2/networks/stylegan2-car-config-f.pkl}{stylegan2-car-config-f.pkl} \\
	\shline	
\end{tabular}
\caption{Pretrained GANs. We use the source codes provided by~\citet{karras2020training} to train StyleGAN models: \url{https://github.com/NVlabs/stylegan2-ada-pytorch}. For CelebA and Face-34 the StyleGAN-ADA is trained on CelebAHQ-Mask 28k images at $256\times256$ and $512\times512$ resolution, respectively. For Pascal-Aeroplane, the StyleGAN2 is trained on 200k images from LSUN airplane.}
\label{tab:pretrained_gan}
\end{table}

\paragraph{Annotator structure}
An annotator is a neural network that takes the generator features and output segmentation masks as input. DatasetGAN~\citep{zhang2021datasetgan} and RepurposeGAN~\citep{Tritrong2021RepurposeGANs} first upsample feature maps to full resolution (resolution of output image) and then concatenate these feature maps along the channel.  
This procedure results in highly high-dimensional feature vectors for every pixel.
These features are fed into either multiple-layer perceptrons (MLPs) as in DatasetGAN~\citep{zhang2021datasetgan} or convolutional neural networks (CNNs) as in~\citep{Tritrong2021RepurposeGANs}. 
Nonetheless, this practice consumes a lot of GPU memory and run painfully slow even on high-end modern GPUs. That is also why DatasetGAN cannot consume all image feature vectors in a batch during training.

Therefore, to make training annotators more efficient, as commonly done in segmentation or detection network architectures, we fuse the multi-scale feature maps with a feature pyramid network (FPN)~\citep{lin2017feature} structure and decode the fused features into segmentation masks with consecutive convolutional layers. 
This FPN structure saves a lot of GPU memories and computation, which allows us to consume multiple full images in one mini-batch during training. 
An illustration of our annotator structure is presented in \Figref{fig:annotator_structure}.
For generator features that are fed into annotator, we use outputs of all convolutional layers in StyleGAN as in DatasetGAN~\citep{zhang2021datasetgan} and use outputs of all synthesis blocks, each of which typically contains two consecutive convolutional layers, in StyleGAN2. 

\begin{figure}[h]\centering
	\includegraphics[width=0.8\linewidth]{figs/appendix/arch}
	\caption{Annotator architecture.}
	\label{fig:annotator_structure}
\end{figure}

\paragraph{Segmentation network structures} 
In our experiments, we employ two prevalent segmentation network structures: U-Net~\citep{ronneberger2015u} and DeepLabv3~\citep{chen2017rethinking} with ResNet-101 pretrained on ImageNet as backbone.
For U-Net, we reference the implementation from \url{https://github.com/milesial/Pytorch-UNet} and train it from scratch on target datasets.
For DeepLabv3, we use the the built-in implementation in PyTorch library\footnote{\url{https://pytorch.org/hub/pytorch_vision_deeplabv3_resnet101/}}. 
For all the experiments on CelebA, Pascal-Horse, and Pascal-Aeroplane, we report the results (Figure~\ref{fig:benchmark}) using DeepLabv3 as segmentation networks.

\paragraph{Training details of our method} 
We use pixelwise cross-entropy loss as $f(\cdot, \cdot)$ for training segmentation network and computing gradients.
The annotator and the segmentation network are optimized with an SGD optimizer with learning rate $0.001$ and momentum $0.9$. 
By default, we jointly train an annotator and a segmentation network with $K=1$ and batch size $2$ for $150,000$ steps.

\paragraph{Training details of competing semi-supervised methods}
% The implementations of Mean Teacher (MT)~\citep{tarvainen2017mean, li2020transformation}, Guided Collaborative Training (GCT)~\citep{ke2020guided} and adversarial-learning-based semi-supervised segmentation method (AdvSSL)~\citep{hung2018adversarial} is based on the PixelSSL repository.
For MT, GCT and AdvSSL, we train the networks for $10,000$ iterations with batch size $16$ for labelled data and $8$ for unlabeled data. The learning rate is decayed with a power of $0.9$ every iteration.
During the training process, the input images are randomly resized to a scale in $[160, 640]$ and cropped to $256\times256$.
We use the default settings for the rest of the hyper-parameters as in PixelSSL.


\section{Further Analysis}
\subsection{Comparison to Other Baselines}
\label{sec:appendix_inversion}

\paragraph{Inversion method}
As discussed in Section~\ref{sec:intro}, the ``inversion method'' is a straightforward way to utilize existing labelled images to train annotators in the supervised learning manner. 
We evaluate the performance of the inversion method and compare it to our method on Cat-16, Face-34, and Car-20 using StyleGAN2 as the generator.
In particular, inversion baseline first projects images into GAN latent space using the projection method provided by \citet{karras2020analyzing}.
Then the latent style codes are forwarded to the generator to acquire generator features. 
Finally, it trains the annotator using the generator features and ground truth masks. 

Table~\ref{tab:inversion_baseline} shows the quantitative results and \Figref{fig:inversion_recon} presents examples of reconstruction quality. 
Despite its accurate image reconstruction, the inversion method fails to combat our method and produces lower-quality automatic labels than ours (see \Figref{fig:inversion_ours_generation}).
We believe it is non-trivial to inverse the GAN generation process to acquire features for specific images. 
Even though the reconstruction of images is satisfying, one can still recover inaccurate generator features, leading to degraded annotator learning.

\begin{table}[h]\centering
\tablestyle{6pt}{1.2}\begin{tabular}{lcc|ccc}
	\shline
	Methods & $G$ & $S$  & Cat-16 & Face-34 & Car-20 \\
	\hline
	% ----------------------------------------------------------------------------------
	\multirow{2}{*}{Ours}
	& {StyleGAN2} & DeepLab
	& 33.56 $\pm$ 0.17 & 55.10 $\pm$ 0.39 & 61.21 $\pm$ 2.07 \\
	& {StyleGAN2} & U-Net	
	& 31.90 $\pm$ 0.75 & 53.58 $\pm$ 0.45 & 58.30 $\pm$ 2.64 \\
	\hline
	% ----------------------------------------------------------------------------------
	\multirow{2}{*}{Inversion method}  
	& {StyleGAN2} & DeepLab
	& 15.14 $\pm$ 0.27 & 47.87 $\pm$ 0.86 & 52.18 $\pm$ 2.31 \\	
	& {StyleGAN2} & U-Net
	& 12.78 $\pm$ 0.42 & 48.57 $\pm$ 0.38 & 52.41 $\pm$ 1.69 \\		
	\shline 
\end{tabular}
\caption{Comparison of our methods to inversion method.}
\label{tab:inversion_baseline}
\end{table}

\begin{figure}[h]\centering
	\includegraphics[width=\linewidth]{figs/appendix/inversion_recon}
	\caption{Examples of reconstruction quality. The first row shows target images and the second row shows the reconstructed images.}
	\label{fig:inversion_recon}
\end{figure}

\begin{figure}[h]\centering
	\includegraphics[width=\linewidth]{figs/appendix/inversion_ours_generation}
	\caption{Comparison of the automatic labels produced by inversion method v.s. ours.}
	\label{fig:inversion_ours_generation}
\end{figure}

\paragraph{Pseudo-labeling method} 
Another way to train annotators using existing labeled images in supervised learning manner is pseudo-labeling method, which uses a trained segmentation model to predict pseudo labels for the generated images.  
In particular, this method involves three stages. 
(i) A segmentation model is trained with the labeled dataset. 
(ii) The trained segmentation model is used to produce pseudo labels for every random image generated from generator.
(iii) The generator features and pseudo labels are used to train annotators in the supervised learning manner. 

Another way to train annotators using existing labelled images in a supervised learning manner is the pseudo-labelling method, which uses a trained segmentation model to predict pseudo labels for the generated images.  
In particular, this method involves three stages. 
(i) A segmentation model is trained with the labelled dataset. 
(ii) The trained segmentation model produces pseudo labels for every random image generated from the generator.
(iii) The generator features and pseudo labels are used to train annotators in supervised learning. 

\begin{table}[h]\centering
	\tablestyle{6pt}{1.2}\begin{tabular}{lcc|ccc}
		\shline
		Methods & $G$ & $S$  & Cat-16 & Face-34 & Car-20 \\
		\hline
		% ----------------------------------------------------------------------------------
		\multirow{2}{*}{Ours}
		& {StyleGAN2} & DeepLab
		& 33.56 $\pm$ 0.17 & 55.10 $\pm$ 0.39 & 61.21 $\pm$ 2.07 \\
		& {StyleGAN2} & U-Net	
		& 31.90 $\pm$ 0.75 & 53.58 $\pm$ 0.45 & 58.30 $\pm$ 2.64 \\
		\hline
		% ----------------------------------------------------------------------------------
		\multirow{2}{*}{Pseudo-labeling method}  
		& {StyleGAN2} & DeepLab
		& 20.21 $\pm$ 0.65 & 42.15 $\pm$ 0.68 & 12.37 $\pm$ 0.91 \\	
		& {StyleGAN2} & U-Net
		& 16.72 $\pm$ 0.79 & 43.26 $\pm$ 1.20 & 13.48 $\pm$ 0.48 \\		
		\shline 
	\end{tabular}
	\caption{Comparison of our methods to pseudo-labeling method.}
	\label{tab:pseudo_labeling_baseline}
\end{table}

\paragraph{Our method with MAML}
One straightforward solution to Equation~\ref{eq:opt_teacher} is to use a MAML-like algorithm, which unrolls the inner loop with multi-step stochastic gradient descent, reserves the computation graph, computes the meta loss, and finally backpropagates the gradients for updating the annotator. 
We refer to this method as ``Ours-$K$-step-MAML'' and summarize this algorithm as Algorithm~\ref{alg:maml_label}. 
Table~\ref{tab:comparison_maml_gm} shows the experimental comparison of gradient matching versus MAML, where we use $\eta_i=0.1$ in MAML for all experiments.
The following conclusions can be drawn from these results.
First, gradient matching generally leads to higher performance and obtains more consistent results across different datasets and different segmentation network architectures than MAML. 
Second, MAML probably requires multiple inner-loop steps (more than two) to match the performance of gradient matching, given that it fails to do so with two inner-loop steps.
It is very inefficient and requires enormous computation resources.
Furthermore, in practice, we observe that MAML appears unstable and sensitive to hyperparameters (\eg inner-loop learning rate) across different datasets and different network architectures, whereas gradient matching requires neither dataset-specific nor network-architecture-specific hyperparameters.

\begin{algorithm}[h]\label{alg:maml_label}
\footnotesize
	\DontPrintSemicolon
	\SetAlgoLined
	\SetKwInOut{Input}{Inputs}
	\SetKwInOut{Output}{Output}
	\Input{
		\par
		\begin{tabular}{l l}
			$G$						& trained generator\\
			$\mathcal{D}_l$ 		& set of labeled examples\\
			$\omega$, $A_\omega$, $\eta_\omega$  	& initial annotator parameters, annotator, and learning rate for annotator\\
			$\theta$, $S_\theta$, $\eta_\theta$ 	& initial segmentor parameters, segmentor, and learning rate for segmentor\\
			$T$ and $B$ 		& total number of optimization steps and batch size\\
			$\eta_i$ and $K$ 		& inner-loop learning rate and inner-loop steps\\
	\end{tabular}}
	\For{$t\gets1$ \KwTo $T$}{ 
		\tcp{update annotator $A_\omega$}
		$\theta' \gets \theta$    \tcp*{copy segmentor parameters for simulated SGD}
		$\calB_g \gets \left\{\left(\rvx_j, A_\omega(\rvh_j)\right)\right\}_{j=1}^B$ \tcp*{sample a batch of synthetic data}
		\For{$k\gets1$ \KwTo $K$}{
			$\nabla_{\theta'} \calL_g \gets \frac{1}{B} \sum_{j=1}^B \nabla_{\theta'} f(S_{\theta'}(\rvx_j), A_\omega(\rvh_j))$ \tcp*{compute gradients on $\calB_{g}$}
			${\theta'} \gets {\theta'} - \eta_i \nabla_{\theta'} \calL_g$ \tcp*{update segmentor parameters}
		}
		$\calB_{l} \gets \left\{(\rvx_i, {\rvy}_i)\sim\calD_l\right\}_{i=1}^B$ \tcp*{sample a batch of labeled examples}
		$\calL_l \gets \frac{1}{B} \sum_{i=1}^B f(S_\theta(\rvx_i), {\rvy}_i)$ \tcp*{compute meta loss}   
		$\omega \gets \omega - \eta_\omega \nabla_\omega \calL_l$ \tcp*{update annotator parameters}
		\vspace{7.5pt}
		\tcp{update segmentor $S_\theta$}
		$\calB_g \gets \left\{\left(\rvx_j, A_\omega(\rvh_j)\right)\right\}_{j=1}^B$ \tcp*{sample a batch of synthetic data}
		$\calL_g \gets \frac{1}{B}\sum_{j=1}^B f(S_\theta(\rvx_j), A_\omega(\rvh_j))$ \tcp*{compute loss on synthetic data}
		$\theta \gets \theta - \eta_\theta \nabla_\theta \calL_g$ \tcp*{update segmentor parameters}
	}
	\Output{annotator $A_\omega$ and segmentor $S_\theta$}
	\caption{Learning to annotate with $K$-step MAML.}
\end{algorithm}

\begin{table}[h]\centering
	\tablestyle{8pt}{1.2}\begin{tabular}{lcc|ccc}
		\shline
		Methods & $G$ & $S$ & 
		\tabincell{c}{\vspace{-2pt}Cat-16\\\vspace{-5pt}{\scriptsize\# annotations: 30}\\{\scriptsize\# classes: 16} } &  
		\tabincell{c}{\vspace{-2pt}Face-34\\\vspace{-5pt}{\scriptsize\# annotations: 16} \\{\scriptsize\# classes: 34} } & 
		\tabincell{c}{\vspace{-2pt}Car-20\\\vspace{-5pt}{\scriptsize\# annotations: 16} \\{\scriptsize\# classes: 20}}\\
		\hline
		%----------------------------------------------------------------------------------
		\multirow{4}{*}{Ours-GM}
		& {StyleGAN}  & DeepLab
		& 33.89 $\pm$ 0.43 & 52.58 $\pm$ 0.61 & 63.55 $\pm$ 2.25 \\
		& {StyleGAN}  & U-Net
		& 32.64 $\pm$ 0.74 & 53.69 $\pm$ 0.54 & 60.45 $\pm$ 2.42 \\
		\cline{2-6}
		& {StyleGAN2} & DeepLab
		& 33.56 $\pm$ 0.17 & 55.10 $\pm$ 0.39 & 61.21 $\pm$ 2.07 \\
		& {StyleGAN2} & U-Net	
		& 31.90 $\pm$ 0.75 & 53.58 $\pm$ 0.45 & 58.30 $\pm$ 2.64 \\
		\hline
		%----------------------------------------------------------------------------------
		\multirow{4}{*}{Ours-1-step-MAML}
		& {StyleGAN}  & DeepLab
		& 32.49 $\pm$ 0.43 & 35.24 $\pm$ 0.20 & 54.74 $\pm$ 2.67 \\
		& {StyleGAN}  & U-Net
		& 16.70 $\pm$ 0.43 & 23.21 $\pm$ 0.07 & 29.26 $\pm$ 0.72 \\
		\cline{2-6}
		& {StyleGAN2} & DeepLab
		& 30.30 $\pm$ 0.53 & 33.82 $\pm$ 0.24 & 54.42 $\pm$ 3.11 \\
		& {StyleGAN2} & U-Net	
		& 22.78 $\pm$ 0.65 & 32.64 $\pm$ 0.18 & 26.59 $\pm$ 0.60 \\
		\hline
		%----------------------------------------------------------------------------------
		\multirow{2}{*}{Ours-2-step-MAML}
		& {StyleGAN}  & DeepLab
		& 32.98 $\pm$ 0.50 & 39.73 $\pm$ 0.13  & 58.22 $\pm$ 2.17 \\
%		& {StyleGAN}  & U-Net
%		& 14.20 $\pm$ 0.26 &  $\pm$  &  $\pm$  \\
		\cline{2-6}
		& {StyleGAN2} & DeepLab
		& 29.66 $\pm$ 0.69 & 39.04 $\pm$ 0.23 & 56.47 $\pm$ 2.32 \\
%		& {StyleGAN2} & U-Net	
%		& 19.37 $\pm$ 0.35 & $\pm$ & $\pm$ \\
		\shline 
	\end{tabular}
	\caption{{Comparisons of our methods} using gradient matching (GM) versus MAML on Car-20, Cat-16, Face-34 with different network architectures. The performance is evaluated with mean intersection over union (mIoU(\%)) across all part classes plus a background class.}
	\label{tab:comparison_maml_gm}
\end{table}

\subsection{The Impact of Segmentation Network States}
We study the impact of segmentation network states, \ie segmentation parameters, in the alternate learning algorithm.
In particular, we compare the training the annotators' training convergence and the performance when different series of segmentation network states are employed for gradient matching.
\Figref{fig:ablation_s_state_unet} \& \Figref{fig:ablation_s_state_deeplab} shows the evolution of annotator performance during training process, where ``baseline'' denotes the default setting and the following settings are further compared.
(i) ``random re-init'': the segmentation network parameters are randomly re-initialized rather than learned from generated images.
(ii) ``pre-trained'': the segmentation network is pre-trained to acquire an acceptable performance ($\sim40.7\%$ mIoU evaluated on the test set).
(iii) ``scratch'': the segmentation network is a DeepLabv3 network with a randomly initial backbone rather than pre-trained on ImageNet.
(iv) ``random init \& fixed'' and ``pre-trained \& fixed'': the segmentation network parameters, either randomly initialized or pre-trained, are fixed, respectively. 

First, the ``fixed'' segmentation network results in a gradient matching problem on a single segmentation network state, leading to an annotator with inferior performance than the default setting.
This result suggests that matching gradients for various segmentation network states is vital to learning annotators.
Second, even though requiring an annotator to produce segmentation-network-agnostic labels is crucial, a random re-initialization strategy does not work well (see results of ``random re-init'' v.s. ``baseline''). Moreover, the results of ``pre-trained \& fixed'' are better than ``random init \& fixed'', which suggest that a well-performed segmentation network provides more informative gradients such that by gradient matching, the annotator can be more effectively learned.
Third, matching gradients of pre-trained segmentation networks accelerates the learning of annotators (see results of ``baseline'' v.s. ``pre-trained'' and ``baseline'' v.s. ``scratch''), suggesting a well-performed segmentation network provides more informative gradients.

\begin{figure}[h]
	\centering
	\begin{minipage}[t]{0.47\textwidth}
		\includegraphics[width=\textwidth]{figs/appendix/ablation_s_state_unet}
		\caption{Comparison of the training convergence under different settings of sampling segmentation network states. Annotator performance (mIoU (\%)) is evaluated on training set of Car-20. Segmentation network is U-Net.}
		\label{fig:ablation_s_state_unet}
	\end{minipage}
	\hfill
	\begin{minipage}[t]{0.47\textwidth}
		\includegraphics[width=\textwidth]{figs/appendix/ablation_s_state_deeplab}
		\caption{Comparison of the training convergence of segmentation network with pretrained backbone (``baseline'') versus segmentation network with randomly initialized backbone (``scratch''). Annotator performance (mIoU (\%)) is evaluated on the training set of Car-20. The segmentation network is DeepLabv3.}
		\label{fig:ablation_s_state_deeplab}
	\end{minipage}
\end{figure} 

\subsection{Trade off Training Efficiency with Performance}\label{sec:efficiency}
\paragraph{Gradient matching on partial network parameters}
We present additional results on in Table~\ref{tab:ablation_unet_gmparam}.
U-Net takes as input an image and first produce a feature map at multiple downsampled scales with consecutive convolutional and pooling layers.
These feature maps are then processed by consecutive convolutional and upsampling layers to be mapped back to the original input scale. Notably, skip connection is employed to connect the multi-scale feature maps along the downsampling and upsampling paths.
We consider the following particular group of layers in U-Net:
(i) \texttt{Up} v.s. \texttt{Down}, (ii) \texttt{Input} v.s. \texttt{Output}, and (iii) \texttt{U1} v.s. \texttt{U2}. 
\texttt{Up} and \texttt{Down} denote the convolutional layers along the upsampling and downsampling path, respectively.
\texttt{Input} and \texttt{Output} denote the input convolutional layer and the output convolutional layer, respectively.
\texttt{U1} and \texttt{U2} denote the convolutional layers that process feature maps at $1/1$ and $1/2$ scales, respectively. 
The results show that matching gradients of layers near the output end is more effective than doing so at layers near the input end.

\begin{table}[h]\centering
\newcommand{\wrapcell}[1]{\tabincell{c}{\vspace{-3pt}#1}}
	\tablestyle{11pt}{1.2}\begin{tabular}{l|c|cc|cc|cc}
		\shline
		Blocks 	& * All & Up & Down & Input & Output & U1 & U2 \\
		\hline
		% Time / step 	& 0.38 & 0.25 & 0.28 & 0.28 & 0.13 & 0.27 & 0.24 \\
		Time / step 	& 1$\times$ & 0.65$\times$ & 0.73$\times$ & 0.73$\times$ & 0.34$\times$ & 0.71$\times$ & 0.63$\times$ \\
		mIoU (\%) 		& \wrapcell{31.90\\$\pm$0.75} & \wrapcell{30.12\\$\pm$0.43} & \wrapcell{14.42\\$\pm$0.23}  & \wrapcell{12.40\\$\pm$0.40}  & \wrapcell{25.41\\$\pm$0.77} & \wrapcell{30.18\\$\pm$0.79} & \wrapcell{31.22\\$\pm$0.48} \\
		\shline	
	\end{tabular}
	\caption{\textbf{Ablation study \wrt blocks for matching gradients} on U-Net. Evaluation is performed on Cat-16, where StyleGAN2 is the generator. ``Time / step'' is measured as the ratio to the case of default setting. * denotes the default setting.}
	\label{tab:ablation_unet_gmparam}
\end{table}

\paragraph{Input resolution to segmentation network}
Another way to improve the training efficiency is to reduce the input resolution to the segmentation network. 
% Note that the resize operation can be implemented with interpolation which is differentiable.
\Figref{fig:ablation_resolution} and Table~\ref{tab:ablation_resolution} show that reducing the resolution improves the training speed but compromises the annotator performance and downstream segmentation network performance. 
Therefore, in practice, the input resolution to the segmentation network can be tuned to trade off the training efficiency versus effectiveness.

\begin{figure}[h]
	\centering
	\includegraphics[width=\linewidth]{figs/appendix/ablation_resolution}
	\caption{The evolution of annotator performance(mIoU (\%)) evaluated on training and validation set during the training process on Face-34.}
	\label{fig:ablation_resolution}
\end{figure}

\begin{table}[h]
	\tablestyle{8pt}{1.2}\begin{tabular}{l|ccc}
		\shline
		Resolution 	& * $512\times512$  & $256\times256$ & $128\times128$ \\
		\hline
		% Time / step 					& 0.77 & 0.35 & 0.29 \\
		Time / step & 1$\times$ & 0.45$\times$ & 0.37$\times$ \\
		mIoU (\%) 	& 51.93 $\pm$ 0.28 & 50.71 $\pm$ 0.39 & 45.53 $\pm$ 0.31\\
		\shline	
	\end{tabular}
	\caption{Comparisons of the training efficiency (time / step) and downstream segmentation (U-Net) performance (mIoU (\%)) \wrt different input resolution to segmentation network on Face-34. ``Time / step'' is measured as ratio with respect to the default setting. * denotes the default setting.}
	\label{tab:ablation_resolution}
\end{table}

\subsection{The Impact of Generator} 
As our segmentation network is trained only on the synthetic images and labels (Algorithm~\ref{alg:gm_label} Line 9$\sim$11), the segmentation performance is inevitably affected by the quality of pre-trained GANs.
% which suggests that the quality of generative features significantly affect the quality of annotations.
Table~\ref{tab:premature_g} confirms this point and shows that premature GANs -- ones with high FID score -- generally leads to lower segmentation performance.

\begin{table}[h]
	\tablestyle{5pt}{1.2}\begin{tabular}{l|cccc||cccc}
		\shline
		& \multicolumn{4}{c||}{CelebA}   & \multicolumn{4}{c}{Pascal-Aeroplane} \\
		\hline
		FID~$\downarrow$ 		& 10.21 & 6.80 & 5.12 & 4.29 & 11.74 & 7.78 & 6.25 & 5.04 \\
		FG-mIoU (\%)~$\uparrow$ 	& 73.92 & 76.61 & 77.48 & 78.07 & 21.69 & 36.01 & 38.81 & 41.21 \\
		\shline	
	\end{tabular}
	\caption{The segmentation performance with respect to generators of different quality (indicated by FID) on CelebA and Pascal-Aeroplane.}
	\label{tab:premature_g}
\end{table}

\subsection{Discussion on Large-Scale Setting} \label{sec:large_scale}
As shown in Fig.~\ref{fig:benchmark}, the performance of our method begins to saturate and even be beaten by other semi-supervised segmentation methods when the number of labels grows large.
To investigate the performance of our method under a large-scale setting, we further run experiments with more unlabeled and labelled data and present the results in Table~\ref{tab:large_scale}.
It shows that the performance of our method even slightly drops when the number of unlabeled data or the number of labels increases and is significantly beaten by supervised learning.
We hypothesize the reason to be that our method is seriously limited by the performance of GANs, which still struggle to fit large-scale datasets.
In contrast, supervised learning becomes stronger under a large-scale setting since more labels are available. 
How to address this issue requires further research in the future.


\begin{table}[h]
	\tablestyle{8pt}{1.2}\begin{tabular}{lcc|c}
		\shline
		Method & unlabeled data & \# labels & FG-mIoU (\%)\\
		\hline
		Supervised learning & -- & 29,000 & 84.32 \\
		\hline
		Ours & CelebA-train (29,000) & 1,500 & 77.90 \\
		Ours & CelebA-train (29,000) & 29,000 & 76.03 \\
		Ours & CelebA-train + FFHQ (99,000) & 29,000 & 75.00 \\
		\shline	
	\end{tabular}
	\caption{Experiments on human face part segmentation under large-scale setting.}
	\label{tab:large_scale}
\end{table}

\subsection{Comparison to DatasetGAN in Other Aspects} 

\paragraph{Real images v.s. synthetic images as labeled data}
As our method removes the necessity of labelling synthetic images, we investigate if replacing the labelled synthetic images with real ones could improve the segmentation performance. 
Table~\ref{tab:real_vs_syn} shows an evaluation on CelebA-test at 512$\times$512 resolution across 8 face classes. 
DatasetGAN~\citep{zhang2021datasetgan}, which uses 16 annotated synthetic images for training, achieves 70.01 mIoU performance. 
Our method achieves a bit higher performance when using the exactly same synthetic images as DatasetGAN.
Moreover, the performance of our method can be further improved with around 6.7\% mIoU by replacing the labelled synthetic images with real ones in CelebA-train.
This result suggests that the capability of using real labelled data exhibits the superiority of our method over DatasetGAN.

\begin{table}[h]
	\tablestyle{5pt}{1.2}\begin{tabular}{lc|c}
		\shline
		& Labeled data   & CelebA-test@512$\times$512 \\
		\hline
		DatasetGAN~\citep{zhang2021datasetgan}$^\dagger$	& Synthetic & 70.01 \\
		\hline
		\multirow{2}{*}{Ours} 	& Synthetic & 72.55 \\
								& Real & 79.25 \\
		\shline	
	\end{tabular}
	\caption{Comparison of the performance when using synthetic images versus real images as labeled data. $\dagger$: Results taken from \citet{zhang2021datasetgan}.}
	\label{tab:real_vs_syn}
\end{table}

\paragraph{Our method with more labeled data on Car-20}
Considering our method does not match the performance of DatasetGAN on Car-20, we further investigate if this gap can be narrowed with the help of more labelled data. 
Table~\ref{tab:car20more} presents the performance of our method when more labelled data is used on Car-20. 
It shows that with an increased number of labelled data, the performance of our method gradually approaches that of DatasetGAN.

\begin{table}[h]
	\tablestyle{5pt}{1.2}\begin{tabular}{cc|c|ccc}
		\shline
		\multirow{2}{*}{G} & \multirow{2}{*}{S}	& DatasetGAN 	& \multicolumn{3}{c}{Ours}\\
		& & \# annotations: 16 & \# annotations: 16 & \# annotations: 25 & \# annotations: 33 \\
		\hline
		StyleGAN & DeepLab 	& 67.53 $\pm$ 2.58 & 63.55 $\pm$ 2.25 & 64.68 $\pm$ 2.53 & 64.88 $\pm$ 2.52 \\
		StyleGAN & U-Net 	& 66.27 $\pm$ 2.75 & 60.45 $\pm$ 2.42 & 63.09 $\pm$ 3.49 & 65.37 $\pm$ 2.78 \\
		\shline	
	\end{tabular}
	\caption{Performance of our method with respect to different number of labeled images on Car-20 dataset.}
	\label{tab:car20more}
\end{table}

\section{Qualitative Results}

\begin{figure}[h]\centering
	\includegraphics[width=\linewidth]{figs/appendix/qualitative_celeba}
	\caption{Qualitative results of our method on CelebA. The segmentation results of our models trained from 2, 10, and 30, 150, 1500 labeled examples are presented.}
	\label{fig:qualitative_celeba}
\end{figure}

\begin{figure}[h]\centering
	\includegraphics[width=\linewidth]{figs/appendix/qualitative_horse}
	\caption{Qualitative results of our method on Pascal-Horse. The segmentation results of our models trained from 2, 10, and 30, 100, 180 labeled examples are presented.}
	\label{fig:qualitative_horse}
\end{figure}

\begin{figure}[h]\centering
	\includegraphics[width=\linewidth]{figs/appendix/qualitative_aeroplane}
	\caption{Qualitative results of our method on Pascal-Aeroplane. The segmentation results of our models trained from 2, 10, and 30, 100, 180 labeled examples are presented.}
	\label{fig:qualitative_aeroplane}
\end{figure}

\begin{figure}[h]\centering
	\includegraphics[width=0.7\linewidth]{figs/appendix/qualitative_car20}
	\caption{Qualitative comparison of our method to DatasetGAN on Car-20}
	\label{fig:qualitative_car20}
\end{figure}

\begin{figure}[h]\centering
	\includegraphics[width=0.7\linewidth]{figs/appendix/qualitative_face34}
	\caption{Qualitative comparison of our method to DatasetGAN on Face-34.}
	\label{fig:qualitative_face34}
\end{figure}

\begin{figure}[h]\centering
	\includegraphics[width=0.7\linewidth]{figs/appendix/qualitative_cat16}
	\caption{Qualitative comparison of our method to DatasetGAN on Cat-16.}
	\label{fig:qualitative_cat16}
\end{figure}

\end{document}
