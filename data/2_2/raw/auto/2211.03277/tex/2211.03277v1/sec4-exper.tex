%!TEX ROOT = ./acl2021.tex
\section{Experiments}
\label{sec4-exper}

\begin{table}[t]%{0.49\textwidth}
    \centering
    \resizebox{.48\textwidth}{!}{
    \begin{tabular}{c|c||c|c|c|c|c} \hline
       \multicolumn{2}{c||}{Proportions} & 1\% & 5\% & 10\% & 50\% & 100\% \\ \hline \hline
       \multicolumn{2}{c||}{BLEU} & 39.08 & 44.76 & 47.74 & 50.12 & \textbf{54.69}  \\
       \multicolumn{2}{c||}{Rouge-1} & 77.49 & 81.75 & 83.12 & 84.76 & \textbf{85.67} \\
       \multicolumn{2}{c||}{Rouge-2} & 57.00 & 62.83 & 64.97 & 66.94 & \textbf{68.61} \\
       \multicolumn{2}{c||}{RougeL} & 67.78 & 72.65 & 74.37 & 76.55 & \textbf{77.43} \\ \hline
       \multirow{2}{*}{RC} & EM & 26.0 & 26.5 & 27.0 & \textbf{27.8} & 27.2 \\ 
       & F1 & 31.3 & 31.3 & 31.6 & \textbf{32.2} & 32.0 \\ \hline
    \end{tabular}}
    \caption{Experimental results of the Bart based question decomposer: (1) Row 1-4 show intrinsic metrics for the question decomposition by using different proportions of training instances. (2) Row 5-6 show extrinsic metrics of the RC model by using the corresponding decomposer generated sub-questions.}
    \label{tab:bart decomposer}
\end{table}

% \hfill
\begin{table*}[t]
    \centering
    \begin{tabular}{c|c||c|c|c|c|c|c|c|c|c|c} \hline
       \multicolumn{2}{c||}{LMs} & \multicolumn{5}{c|}{t5-small} & \multicolumn{5}{c}{t5-base} \\ \hline
       \multicolumn{2}{c||}{Proportions} & 1\%~\footnote{Trained as a question decomposer, the t5-small model and cannot be further evaluated on downstream RC task, as the generated sub-questions are poor-quality.} & 5\% & 10\% & 50\% & 100\% & 1\% & 5\% & 10\% & 50\% & 100\% \\ \hline \hline
       \multicolumn{2}{c||}{BLEU} & 11.21 & 44.50 & 50.44 & 60.15 & \underline{62.73} & 34.86 & 52.98 & 57.3 & 62.18 & \textbf{64.40}  \\
       \multicolumn{2}{c||}{Rouge-1} & 43.00 & 76.93 & 81.53 & 87.25 & \underline{88.59} & 70.66 & 84.16 & 85.77 & 88.50 & \textbf{89.27} \\
       \multicolumn{2}{c||}{Rouge-2} & 28.18 & 59.13 & 64.33 & 72.60 & \underline{74.76} & 50.57 & 66.86 & 70.24 & 74.24 & \textbf{75.72} \\
       \multicolumn{2}{c||}{RougeL} & 39.22 & 68.92 & 73.66 & 79.99 & \underline{81.57} & 62.10 & 75.49 & 78.07 & 81.20 & \textbf{82.53} \\ \hline
       \multirow{2}{*}{RC} & EM & - & 28.9 & \underline{29.9} & 29.0 & 29.0 & 33.7 & 34.3 & 34.3 & 34.6 & \textbf{34.8} \\ 
       & F1 & - & 33.0 & \underline{34.0} & 33.2 & 33.1 & 37.8 & 38.4 & 38.5 & 38.5 & \textbf{38.6} \\ \hline
    \end{tabular}
    \caption{Results of the T5 based question decomposer (left-half: t5-small, right-half: t5-base): (1) Row 1-4 show all intrinsic metrics to evaluate the question decomposer by using different proportions of training instances. (2) Row 5-6 show extrinsic metrics of the RC component by using the corresponding decomposer generated sub-questions.}
    \label{tab:t5 decomposer}
\end{table*}

\subsection{Dataset}
We consistently use the same notations as in Algorithm \ref{alg:cap}.
\begin{itemize}
    \item $\mathcal{D}_1$: the \textsc{DROP} dataset~\cite{DROP:2019} that contains 77,400/9,536 question ($Q$) answer ($A$) training/testing pairs for the reading comprehension component.
    \item $\mathcal{D}_2$: the \textsc{Break} dataset~\cite{break-it-down}~\footnote{The full \textsc{Break} dataset \citet{break-it-down} annotated is a combination of many datasets including \textsc{DROP}. In this paper, we only use the DROP partition of the original \textsc{Break}.} that contains 7,683/1,268 question ($Q$) decomposition ($Z^*$) training/testing pairs for the question decomposer~\footnote{This subset of DROP contains the corresponding answers for each question. Therefore, we also use it to evaluate the RC component in our experiments.}.
    \item $\mathcal{D} = \mathcal{D}_1 \setminus \mathcal{D}_2$: the difference set between $\mathcal{D}_1$ and $\mathcal{D}_2$ that contains only question answer pairs without ground-truth decomposition.
    \item $\mathcal{D}^{iter}$: $\mathcal{D}$ with decomposition ($Z$) generated by the trained question decomposer.
\end{itemize}
Note that every question ($Q$) is associated with a specific context ($C$). 
With all question decomposition labelled, $\mathcal{D}_2$ is actually a subset of $\mathcal{D}_1$ and is more challenging.

\subsection{Backbone and Evaluation Metric}
There are three LMs of different types and sizes we employ as backbones in this paper: (1) t5-small (60M parameters), (2) t5-base (220M parameters), (3) bart-base (140M parameters).
We also employ GPT-3 (175B parameters) as it is the current state-of-the-art language model in a various of natural language processing tasks.

\noindent\textbf{Sub-question Decomposition}
We train and evaluate our question decomposer using $\mathcal{D}_2$, which was proposed to better understand difficult multi-hop questions.
We report BLEU~\cite{Papineni02bleu:a} and Rouge~\cite{lin-2004-rouge} scores to show the intrinsic performance of the decomposer.

\noindent\textbf{Reading Comprehension}
We evaluate our RC model on $\mathcal{D}_2$.
For the Hard-EM approach, we have $\mathcal{D}_1 \setminus \mathcal{D}_2$ as weakly supervised data.
We report F1 and Exact Match(EM)~\cite{DROP:2019} scores in the following experiments.


\begin{table*}[t]%{0.5\textwidth}
    \centering
    \begin{tabular}{l|c|c||c|c} \hline
       Backbone & Variant & Training Set & F1 & EM\\ \hline \hline
       baselines & & & &\\
       bart-base~\cite{lms-bart} & - & $\mathcal{D}_2$ & 30.9 & 27.1 \\
       t5-base~\cite{lms-t5} & - & $\mathcal{D}_2$ & 37.9 & 33.9 \\ \hline \hline
       our bart-base variants & & & &\\
       w/ predicted sub-questions & \emph{separate} & $\mathcal{D}_2$ & 32.0 & 27.2 \\
       w/ ground-truth sub-questions & \emph{separate} & $\mathcal{D}_2$ & 33.2 & 29.0 \\
       w/ ground-truth sub-questions & \emph{separate} & $\mathcal{D}_2, \mathcal{D}^{1}$ & \textbf{45.0} & \textbf{40.5} \\
       w/o Hard-EM & \emph{unified} & $\mathcal{D}_2, \mathcal{D}^{1}$ & 44.2 & 39.9 \\
       w/ Hard-EM & \emph{unified} & $\mathcal{D}_2, \mathcal{D}^{iter}$ & \underline{44.3} & \underline{40.0} \\ \hline \hline
       our t5-base variants & & & &\\
       w/ predicted sub-questions & \emph{separate} & $\mathcal{D}_2$ & 38.6 & 34.8 \\
       w/ ground-truth sub-questions & \emph{separate} & $\mathcal{D}_2$ & 39.6 & 35.6 \\
       w/ ground-truth sub-questions & \emph{separate} & $\mathcal{D}_2, \mathcal{D}^{1}$ & \textbf{45.1} & \textbf{40.8} \\ %49.8, 45.9
       w/o Hard-EM & \emph{unified} & $\mathcal{D}_2, \mathcal{D}^{1}$ & 38.8 & 34.9 \\
       w/ Hard-EM & \emph{unified} & $\mathcal{D}_2, \mathcal{D}^{iter}$ & \underline{44.0} & \underline{40.1} \\ \hline \hline
       GPT-3 (zero-shot) & - & - & 15.7 & 4.6 \\ 
       GPT-3 (few-shot) & - & - & 34.9 & 27.0 \\ \hline
    \end{tabular}
    \caption{Overall results for baselines, our separate and unified variants. All models are evaluated on the same test set from $\mathcal{D}_2$.}
    \label{tab:models}
\end{table*}
% \caption{Experimental results.}
% \vspace{-8pt}
% \end{table*}

\subsection{Results on Decomposition}
Based on Bart and T5, Table \ref{tab:bart decomposer} and Table \ref{tab:t5 decomposer} respectively show the experimental results of the question decomposers.
To comprehensively show their performance, we conducted two aspects of experiments including intrinsic decomposition evaluation and extrinsic RC evaluation.

\paragraph{Intrinsic Evaluation}
We first evaluate the quality of sub-questions generated by different question decomposers. 
In this part, intrinsic metrics, BLEU and Rouge scores, are shown in the first four rows of Table \ref{tab:bart decomposer} and Table \ref{tab:t5 decomposer}.
And also we show the results of five decomposers trained on different proportions (1\%, 5\%, 10\%, 50\%, 100\%) of the \textsc{Break} dataset $\mathcal{D}_2$'s training data.
All these evaluations are conducted on the same validation set of $\mathcal{D}_2$.

Comparing column-by-column, we find that with more training data, both question decomposers achieve a better performance for both BLEU and Rouge.
We also note that the rate of improvement of these metrics becomes slower when more data is added (e.g.\ 1\% to 5\% and 10\% to 50\%).
Therefore, we posit that with more training data, the performance of the decomposer will not improve due to the capability of the LM model.

\paragraph{Extrinsic Evaluation}
Since the eventual usage of the generated sub-questions is to improve the RC component, we conduct a RC performance comparison experiments to see how can the quality of these sub-questions influence the downstream RC task.
Also like the intrinsic evaluation, we show the results based on decomposers trained on different proportions of $\mathcal{D}_2$ by using two extrinsic metrics: EM and F1.
All the evaluations are conducted on the same validation set of $\mathcal{D}_2$.

To clarify our settings in this part, we don't employ the ground-truth sub-questions from $\mathcal{D}_2$.
Instead, we employ the sub-questions generated by five question decomposers for the RC component to predict answers.
As the last two rows of both Table \ref{tab:bart decomposer} and Table \ref{tab:t5 decomposer} show, both EM and F1 scores show a gradually increasing trend when more training instances are used to train the question decomposer.
With more parameters, t5-base tends to have a better performance than t5-small.


\subsection{Results on Reading Comprehension}
Table \ref{tab:models} shows the experimental results for the downstream RC task.
We show two baselines in the first place: ``bart-base'' and ``t5-base''.
Without taking sub-questions as input, both are trained on the \textsc{Break} dataset $\mathcal{D}_2$.
Based on these vanilla models, we show our \emph{separate} and \emph{unified} approaches that use ``bart-base'' and ``t5-base'' as backbones separately in Table \ref{tab:models}.


\subsubsection{Separate Variant}
Our \emph{separate} variants are base on the architecture in Figure~\ref{fig:decomposer-rc}.
In Table \ref{tab:models}, we have three \emph{separate} variants based on each backbone for comparison.
Taking t5-base as one example, comparing to the t5-base, using predicted sub-questions achieves a 0.7-point gain of F1 score.
Meanwhile using ground-truth sub-questions, our model outperforms the t5-base by 1.7 points of F1 score.
The same improvement can be also concluded from the bart-base model.
They employ $\mathcal{D}_2$ for training but their testing sets are different: predicted one use generated sub-questions while ground-truth one use sub-questions from $\emph{D}_2$.
The reason why our approach is more effective than the baseline model is that concatenating sub-questions can give LMs hints on the reasoning procedure, which helps LMs produce step-by-step thoughts implicitly.

Furthermore, we add $\mathcal{D}^1$ as the training set to train our seperate model.
As it shows in Table \ref{tab:models}, this kind of \emph{separate} variants show the overall best performance since we have two sets of parameters separately learning question decomposition and reading comprehension.
Compared to t5-base, the bart-base variant shows a higher performance gain that proves the effectiveness of our method.


\begin{table*}[t]
    \centering
    \begin{tabular}{p{0.41\linewidth}|p{0.18\linewidth}|p{0.1\linewidth}|p{0.1\linewidth}|p{0.09\linewidth}}
    \hline
        Context & Question & GPT-3 (few-shot) & bart-base \emph{separate} (best) & ground-truth answer\\ \hline 
        ... notably striking out \textcolor{cyan}{Julio Franco}, at the time the \textcolor{cyan}{oldest player} in the MLB at \textcolor{cyan}{47 years old}; \textcolor{cyan}{Clemens} was himself \textcolor{cyan}{43}. In the bottom of the eighteenth inning, Clemens came to bat again... & Which player playing in the 2005 National League Division Series was older, Julio Franco or Roger Clemens? & Julio Franco (\cmark) & Julio Franco (\cmark) & Julio Franco \\ 
        \hline 
        ... Nyaungyan then systematically reacquired nearer Shan states. He \textcolor{cyan}{captured Nyaungshwe in February 1601, and the large strategic Shan state of Mone in July 1603}, bringing his realm to the border of Siamese Lan Na. In response, Naresuan of Siam marched in early 1605 to ... & How many years after capturing Nyaungshwe did Nyaungyan capture the large strategic Shan state of Mone? & 3 years (\xmark) & 2 (\cmark) & 2 \\
        \hline 
        Kannada language is the official language of Karnataka and spoken as a native language by about \textcolor{red}{66.54\%} of the people as of 2011. Other linguistic minorities in the state were Urdu (10.83\%), \textcolor{cyan}{Telugu language (5.84\%)}, Tamil language (3.45\%), ... & How many in percent of people for Karnataka don't speak Telugu? & 66.54\% (\xmark) & 94.04\% (\xmark) & 94.16\% \\
        \hline 
        A 2013 analysis of the National Assessment of Educational Progress found that \textcolor{cyan}{from 1971 to 2008, the size of the black-white IQ gap in the United States decreased from 16.33 to 9.94 IQ points}. It has also concluded however that, ... & How many IQ points did the black-white IQ gap decrease between 1971 and 2008? & 16.33 (\xmark) & 0.9 (\xmark) & 6.39\\ 
        \hline
    \end{tabular}
    \caption{Correct and incorrect outputs from GPT-3 and our \emph{separate} variant. \textcolor{cyan}{Correct} and \textcolor{red}{Wrong} supporting facts are annotated in the context using the corresponding color. Correct and wrong answer predictions are also marked with \cmark and \xmark \ (the table is best seen in colours).}
    \label{tab:error}
\end{table*}

\begin{table*}[t]
    \centering
    \begin{tabular}{l|l||c|c|c|c}
    \hline
        \multicolumn{2}{c||}{overlaps} & $0\sim25\%$ & $25\%\sim50\%$ & $50\%\sim75\%$ & $75\%\sim100\%$ \\ \hline
        \multirow{4}{*}{uni-grams} & bart-base & - & 0 & 25.7 & 27.4 \\
        & \emph{unified} & - & 0 & 32.9 & \underline{40.2} \\
        & \emph{separate} & - & 0 & \textbf{35.7} & \textbf{41.3} \\
        & GPT-3 & - & \textbf{100.0} & \textbf{35.7} & 26.4 \\
        \hline \hline
        \multirow{4}{*}{bi-grams} & bart-base & - & 16.7 & 23.6 & 28.2 \\
        & \emph{unified} & - & 33.3 & \textbf{29.1} & \underline{41.9} \\
        & \emph{separate} & - & \textbf{50.0} & 28.6 & \textbf{43.2} \\
        & GPT-3 & - & \underline{44.4} & \textbf{29.1} & 26.2 \\
        \hline \hline
        \multirow{4}{*}{tri-grams} & bart-base & 22.2 & 20.5 & 25.5 & 29.3 \\
        & \emph{unified} & 38.9 & 26.2 & \underline{32.3} & \underline{45.1} \\
        & \emph{separate} & \textbf{50.0} & \textbf{30.0} & \textbf{33.4} & \textbf{45.9} \\
        & GPT-3 & \textbf{50.0} & \underline{28.0} & 25.8 & 26.8 \\
        \hline
        
        \hline
    \end{tabular}
    \caption{EM scores separately computed based on overlaps of sub-questions n-grams between training set and testing set on $\mathcal{D}_2$. Four models listed in this table are: the bart-base baseline, the best performed \emph{separate} model, the best performed \emph{unified} model}
    \label{tab:n-grams}
\end{table*}

\subsubsection{Unified Variant}
Our \emph{unified} variants are base on the architecture in Figure~\ref{fig:decomposer-rc} and one single model is used to train on both steps.
In Table \ref{tab:models}, the last two rows of each variants show the performance of our \emph{unified} variant.
Without the Hard-EM algorithm, performing multi-task learning achieves a 0.9 point improve over the T5 baseline. However, it shows a performance drop when compared to the \emph{separate} variant with ground-truth sub-questions.
This can be caused by the enlarged dataset and the additional decomposition work the \emph{unified} variant need to handle.

When more training data is provided (i.e.\ $\mathcal{D}^1$ and $\mathcal{D}^{iter}$), though without ground-truth sub-questions, the \emph{unified} variants substantially outperforms the baselines by 10.1 and 6.1 points over bart-base and t5-base model. 
Furthermore, when compared with the best \emph{separate} variants, our \emph{unified} models also show comparable performance on both F1 and EM metrics.
Based on the observations of the last three rows of each backbone, it can be concluded that introducing more weakly-supervised training data can significantly help our model address the original difficult multi-hop RC task.

We also include another evaluation of employing GPT-3, which is the state-of-the-art language model on many tasks and also in a large parameter scale (175B).
The results are shown by last two rows in Table \ref{tab:models}.
Based on the experimental results, GPT-3 cannot even beat two baseline models under the zero-shot learning paradigm, which again shows the complexity and challenging of the task.
When provided with several exemplars, it can easily outperform the bart-base model by 2.4 points on F1 score.
However, even with $\times 1000$ parameters, GPT-3 is still far behind to our best variants by 10.2 F1 points.

\section{Analysis and Discussions}

\subsection{Qualitative Analysis}
In this section, we will further discuss some real-life cases generated by our proposed variants from the dataset.
In Table \ref{tab:error}, the first row shows a comparison question and both GPT-3 and our bart-base \emph{separate} model can produce the correct answer.
However, when the question requires some arithmetic operations, such as addition or subtraction, the GPT-3 model would fail to answer correctly.
Our model can handle this as shown by the second row.

There are two types of failures from our variants: one is that our model cannot handle unseen numbers, and the other is arithmetic between float numbers.
The unseen number case happens in the third row of Table \ref{tab:error}. 
Asking for the number of a complement set, though the number 94.04\% is wrongly predicted by our model, it is more close to the ground-truth (94.16\%) when compared to the GPT-3, which directly predict an wrong evidence annotated with red color.
Furthermore, the last row shows a subtraction question between two float numbers. 
Different from integer number subtraction in the second row, it is much harder to compute this arithmetic for language models.
Traditionally, some symbolic methods can handle this problem very well.
Tackling these problems can be interesting  future work directions.

\subsection{Quantitative Analysis}
We look into details of $\mathcal{D}_2$ from the perspective of sub-question n-grams for both training and testing data.
Intuitively, given one instance from the test set, more n-grams overlap it shows with the training set, higher the EM and F1 scores.
Therefore, we further conducted the analysis and list all the statistics in Table \ref{tab:n-grams}.

We calculate for uni-grams, bi-grams and tri-grams for four models: bart-base baseline, the best-performed \emph{separate} and \emph{unified} variants proposed in Section \ref{sec3-model} and GPT-3 with few-shot learning.
The overlaps we choose is four intervals using percentages to represent.
For example, $0\sim25\%$ overlapping on bi-grams means that the test instance have this proportion of bi-grams overlaps with all the training instances.
Note that there is no overlapping  for uni-grams and bi-grams in $0\sim25\%$.

In Table \ref{tab:n-grams}, we report the EM score (F1 score shows the similar results).
The bart-base model show a tendency that with more overlaps across all n-grams, the performance will increase, which is consistent with our assumption.
However, on the contrary, GPT-3 model show a reverse tendency that is probably due to the pre-trained corpus that shares far less n-grams with the test set.
This characteristic improves the compositional generalisation ability as it outperforms the baseline model on the low-overlapping part of test set.
Both of our \emph{separate} and \emph{unified} variants show overall improvements over the bart-base baseline.
In particular, the first and second columns also show our model can better handle the low-overlapping questions, even without performance drop on the high-overlapping questions ($50\%\sim100\%$).
This experiment can further prove the compositional generalisation of our method is comparable to GPT-3.