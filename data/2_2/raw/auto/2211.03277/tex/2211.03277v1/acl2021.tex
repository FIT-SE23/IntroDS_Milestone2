%
% File acl2021.tex
%
%% Based on the style files for EMNLP 2020, which were
%% Based on the style files for ACL 2020, which were
%% Based on the style files for ACL 2018, NAACL 2018/19, which were
%% Based on the style files for ACL-2015, with some improvements
%%  taken from the NAACL-2016 style
%% Based on the style files for ACL-2014, which were, in turn,
%% based on ACL-2013, ACL-2012, ACL-2011, ACL-2010, ACL-IJCNLP-2009,
%% EACL-2009, IJCNLP-2008...
%% Based on the style files for EACL 2006 by 
%%e.agirre@ehu.es or Sergi.Balari@uab.es
%% and that of ACL 08 by Joakim Nivre and Noah Smith

\documentclass[11pt,a4paper]{article}

\usepackage{authblk}

\usepackage[hyperref]{acl2021}
\usepackage{times}
\usepackage{latexsym}
\renewcommand{\UrlFont}{\ttfamily\small}

% This is not strictly necessary, and may be commented out,
% but it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}

\usepackage{inconsolata}
\usepackage{graphicx}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{multirow}
\usepackage{hhline}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{xcolor}
\usepackage{soul}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage[normalem]{ulem}
\usepackage{pifont}
\newcommand{\cmark}{\ding{51}}%
\newcommand{\xmark}{\ding{55}}%
\usepackage{xcolor}
\aclfinalcopy % Uncomment this line for the final submission
%\def\aclpaperid{***} %  Enter the acl Paper ID here

%\setlength\titlebox{5cm}
% You can expand the titlebox if you need extra space
% to show all the authors. Please do not make the titlebox
% smaller than 5cm (the original size); we will check this
% in the camera-ready version and ask you to change it back.

\newcommand\BibTeX{B\textsc{ib}\TeX}

\title{Complex Reading Comprehension Through Question Decomposition}

% \author{First Author \\
%   Affiliation / Address line 1 \\
%   Affiliation / Address line 2 \\
%   Affiliation / Address line 3 \\
%   \texttt{email@domain} \\\And
%   Second Author \\
%   Affiliation / Address line 1 \\
%   Affiliation / Address line 2 \\
%   Affiliation / Address line 3 \\
%   \texttt{email@domain} \\}
\author[ ]{Xiao-Yu Guo}
\author[ ]{Yuan-Fang Li}
\author[ ]{Gholamreza Haffari}
\affil[ ]{Faculty of Information Technology, Monash University, Melbourne, Australia}
\affil[ ]{\url{{xiaoyu.guo, yuanfang.li, gholamreza.haffari}@monash.edu}}


\begin{document}
\maketitle
\begin{abstract}
Multi-hop reading comprehension requires not only the ability to reason over raw text but also the ability to combine multiple evidence. 
We propose a novel learning approach that helps language models better understand difficult multi-hop questions and perform ``complex, compositional'' reasoning.
Our model first learns to decompose each multi-hop question into several sub-questions by a trainable \emph{question decomposer}.
Instead of answering these sub-questions, we directly concatenate them with the original question and context, and leverage a \emph{reading comprehension} model to predict the answer in a sequence-to-sequence manner.
By using the same language model for these two components, our best \emph{seperate}/\emph{unified} t5-base variants outperform the baseline by 7.2/6.1 absolute F1 points on a hard subset of DROP dataset.
\end{abstract}


\input{sec1-intro}

\input{sec2-relat}

\input{sec3-model}

\input{sec4-exper}

\input{sec5-concl}

% \input{sec6-limit}

%\section*{Acknowledgements}

% Entries for the entire Anthology, followed by custom entries
\bibliography{acl2021}
\bibliographystyle{acl_natbib}

% \input{sec7-appen}


\end{document}
