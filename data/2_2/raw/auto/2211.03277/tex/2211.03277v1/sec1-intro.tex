%!TEX ROOT = ./acl2021.tex
\section{Introduction}
\label{sec1-intro}

Multi-hop Reading Comprehension (RC) is a challenging problem that requires compositional, symbolic and arithmetic reasoning capabilities. 
Facing a difficult question, humans tend to first decompose it into several sub-questions whose answers can be more easily identified. 
The final answer to the overall question can then be concluded from the aggregation of all sub-questions' answers. 
For instance, for the question in Table \ref{tab:example}, we can naturally decompose it into three simpler sub-questions (1) ``return the touchdown yards'', (2) ``return the fewest of $\#1$'', and (3) ``return who caught $\#2$''. 
The tokens $\#1$ and $\#2$ are the answers to the first and second sub-questions respectively. Finally, the player with the touchdown of $\#2$ is returned as the final answer. 

\begin{table}[!htb]
    \centering
    \begin{tabular}{ll}
         \hline
         $\mathbf{C}$
         & First, Detroit's Calvin Johnson caught \\
         & a 1-yard pass in the third quarter. The \\
         & game's final points came when Mike \\
         & Williams of Tampa Bay caught a 5-yard.  \\ \hline
         $\mathbf{Q}$ & Who caught the touchdown for the \\
         & fewest yards? \\ \hline
         $\mathbf{Q_1}$ & return the touchdown yards \\
         $\mathbf{Q_2}$ & return the fewest of $\#1$ \\ 
         $\mathbf{Q_3}$ & return who caught $\#2$ \\ \hline
         $\mathbf{A}$ & Calvin Johnson \\ \hline
    \end{tabular}
    \caption{An example for reading comprehension. $\mathbf{C}$ is the context, $\mathbf{Q}$ is a hard multi-hop question, and $\mathbf{Q_1}$, $\mathbf{Q_2}$, $\mathbf{Q_3}$ are sub-questions annotated in \textsc{Break} dataset. $\mathbf{A}$ is the answer to $\mathbf{Q}$.}
    \label{tab:example}
\end{table}

State-of-the-art RC techniques employ large-scale pre-trained language models (LMs) such as GPT-3~\cite{lms-gpt-3} for their superior representation and reasoning capablities. 
Chain of thought prompting~\cite{chain-of-thought} elicits strong reasoning capability of LMs by providing intermediate reasoning steps. 
Least-to-most prompting~\cite{least-to-most} further shows the feasibility of conducting decomposition and multi-hop reasoning, which happen on the decoder side together with the answer prediction procedure.
However, compared to supervised learning models, both of these methods rely on extremely large LMs with tens and hundreds of \textbf{billions} of parameters to achieve competitive performance, thus requiring expensive hardware and incurring a large computation footprint.

Despite significant research on RC \cite{DROP:2019,Unsupervised:2020}, those questions that require strong compositional generalisability and numerical reasoning abilities are still challenging to even the state-of-the-art models \cite{ran-etal-2019-numnet,chen-etal-2020-qdgat,ChenLYZSL20-nerd,chain-of-thought,least-to-most}. 
While decomposition is a natural approach to tackle this problem, the lack of sufficient ground-truth sub-questions limits our ability to train RC models based on large LMs. 

In this paper, we propose a novel low-budget (only  1\textperthousand~parameters of GPT-3) learning approach to improve LMs' performance on hard multi-hop RC such as the Break subset of DROP \cite{DROP:2019}. 
Our model consists of two main modules: (1) an encoder-decoder LM as a \emph{question decomposer} and (2) another encoder-decoder LM as the \emph{reading comprehension} model. 
First, we train the question decomposer to decompose a difficult multi-hop question to sub-questions from a limited amount of annotated data.
Next, instead of solving these sub-questions, we train the reading comprehension model to predict the final answer by directly concatenating the sub-questions with the original question. 
We further propose a \emph{unified} model that utilizes the same LM for both question decomposition and reading comprehension with task-specific prompts.
With 9$\times$ weakly supervised data, we design a Hard EM-style algorithm to iteratively optimise the \emph{unified} model.

To prove the effectiveness of our approach, we leverage two different types of LMs: T5~\cite{lms-t5} and Bart~\cite{lms-bart} to build baselines and our variants.
The experimental results show that without changing the model structure, our proposed variant outperforms the end-to-end baseline.
By adding ground-truth sub-questions, gains on the F1 metric are 1.7 and 0.7 using T5 and Bart separately. 
Introducing weakly supervised training data can help improve the performance of both \emph{separate} and \emph{unified} variants by at least 4.4 point on F1.
And our method beats the state-of-the-art model GPT-3 by a large margin.