%!TEX ROOT = ./acl2021.tex
\section{Conclusion}
\label{sec5-concl}
We propose a two-step process for multi-hop reading comprehension task.
The first step involves a question decomposer that maps a difficult multi-hop question into several sub-questions.
The second step is to train a reading comprehension model based on (question, sub-questions, paragraph, answer) tuples.
With the addition of sub-questions, our bart-/t5-base variants outperform the baseline model by 2.3/1.7 using ground truth sub-questions and 1.1/0.7 using generated ones on F1 score.
Based on the hard-EM paradigm, large positive gains of another 11.1/4.4 point on F1 by the unified multi-task learning bart-/t5-base models shows the effectiveness of introducing weakly supervised training data.
By further analysing the predicted examples and dataset, we also found our model can make a more comprehensive improvement compared with the SOTA GPT-3 model.
But some problems like handling unseen numbers still exist and will be our future research directions.