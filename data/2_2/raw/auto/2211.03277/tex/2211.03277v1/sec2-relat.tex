%!TEX ROOT = ./acl2021.tex
\section{Related Work}
\label{sec2-relat}

\paragraph{Multi-hop Reading Comprehension} mentioned in this paper requires more than one reasoning or inference step to answer a question.
For example, multi-hop RC in DROP \cite{DROP:2019} requires numerical reasoning such as addition, subtraction.
To address this problem, \citeauthor{DROP:2019} proposed a number-aware model NAQANet that can deal with such questions for which the answer cannot be directly extracted.
NumNet~\cite{ran-etal-2019-numnet} leveraged Graph Neural Network to design a number-aware deep learning model.
QDGAT~\cite{chen-etal-2020-qdgat} distinguished number types more precisely by adding the connection with entities and obtained better performance.
Nerd~\cite{ChenLYZSL20-nerd} searched possible programs exhaustively based on the ground-truth and employed these programs as weak supervision to train the whole model.

\paragraph{Question Decomposition} 
is the approach that given a complex question, break it into several simple sub-questions.
These sub-questions can also be Question Decomposition Meaning Representation (QDMR)~\cite{break-it-down} for complex questions.
Many researchers~\cite{Unsupervised:2020,decomposition-break-perturb-build} have been trying to solve the problem by incorporating decomposition procedures.
For example, \citet{Unsupervised:2020} propose a model that can break hard questions into easier sub-questions. 
Then, simple QA systems provide answers of these sub-questions for downstream complex QA systems to produce the final answer corresponding to the original complex question. 
\citet{fu-etal-2021} propose a three-stage framework called Relation Extractor Reader and Comparator (RERC), based on complex question decomposition.
Different from these approaches, we aim to improve the multi-hop capability of current encoder-decoder models without dedicated pre-designing the architecture.

\paragraph{Language Models} like BERT~\cite{BERT:2019}, GPT families~\cite{lms-gpt,lms-gpt-2,lms-gpt-3}, BART~\cite{lms-bart} and T5~\cite{lms-t5} are demonstrated to be effective on many NLP tasks, base on either fine-tuning or few-shot learning~\cite{chain-of-thought,least-to-most}, even zero-shot learning. 
However, LMs suffer a lot from solving multi-hop questions and logic reasoning and numerical reasoning problems. 
Although some research~\cite{scratchpads,chain-of-thought} has conducted experiments on either simple or synthetic datasets and shown the effectiveness, \citet{impact-2022} indicates that the model reasoning is not robust enough.

Recently, \citet{lmc-2022} points out that prompted models can be regarded as employing a unified framework a \emph{language model cascade}.
From the perspective view of probabilistic programming, several recent literature~\cite{chain-of-thought,least-to-most} are formalized.
In this paper, we also treat our whole process as a probabilistic model that is consistent to \citet{lmc-2022}. 