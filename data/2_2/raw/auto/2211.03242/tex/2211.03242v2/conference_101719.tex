\documentclass[conference]{IEEEtran}
% \IEEEoverridecommandlockouts
% The preceding line is only needed to identify funding in the first footnote. If that is unneeded, please comment it out.

\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithm}
% \usepackage{algorithmic}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{ulem}
% \usepackage{xcolor}
\usepackage{float}
\usepackage{booktabs}
\usepackage{hyperref,comment}   
\usepackage[table,xcdraw]{xcolor}
% % hyperlinks
% \usepackage{algorithm}
% \usepackage{algorithmic}
% \usepackage{comment}
% \usepackage[ruled,vlined]{algorithm2e}
% \usepackage[T1]{fontenc}
% \usepackage[utf8]{inputenc}
% %\usepackage{authblk}
% \usepackage{verbatim}
% %\usepackage{hyperref}
% \usepackage{soul} %razi: use ul for underline
% \usepackage[hyphenbreaks]{breakurl}
%  \usepackage{float}
% \usepackage{hyperref}
% \usepackage[hyphenbreaks]{breakurl}
% \usepackage{multirow}
% \usepackage{adjustbox}
% \usepackage{biblatex} %Imports biblatex package
% \addbibresource{ref.bib} %Import the bibliography file
\usepackage{cite}
\newcommand{\ar}[1]{\textcolor{blue}{#1}}
\newcommand{\xc}[1]{\textcolor{olive}{#1}}
\newcommand{\hw}[1]{\textcolor{violet}{#1}}
\newcommand{\arr}[1]{\textcolor{red}{\textit{Razi: #1}}}

\newcommand{\add}[1]{\textcolor{violet}{#1}}

\pagenumbering{Roman}


\def\BibTeX{{\rmB\kern-.05em{\sci\kern-.025emb}\kern-.08emT\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
    

\begin{document}

\title{Fast Key Points Detection and Matching for Tree-Structured Images
% {\footnotesize \textsuperscript{*}Note: Sub-titles are not captured in Xplore and
% should not be used}
\thanks{The source code has been uploaded to github: \url{https://github.com/iSeND-Clemson/Fast-Key-Points-Detection-and-Matching}}
}

\author{
\IEEEauthorblockN{
%1\textsuperscript{st} 
Hao Wang}
\IEEEauthorblockA{\textit{School of Computing} \\
\textit{Clemson University}\\
Clemson, SC \\
hao9@clemson.edu}
\and
\IEEEauthorblockN{
%2\textsuperscript{nd} 
Xiwen Chen}
\IEEEauthorblockA{\textit{School of Computing} \\
\textit{Clemson University}\\
Clemson, SC \\
xiwenc@clemson.edu}
\and
\IEEEauthorblockN{
%3\textsuperscript{rd} 
Abolfazl Razi}
\IEEEauthorblockA{\textit{School of Computing} \\
\textit{Clemson University}\\
Clemson, SC \\
arazi@clemson.edu}
\and
\IEEEauthorblockN{
%4\textsuperscript{th} 
Rahul Amin}
\IEEEauthorblockA{\textit{Lincoln Laboratory} \\
\textit{Massachusetts Institute of Technology}\\
Lexington, MA \\
rahul.amin@ll.mit.edu}
% \and
% \IEEEauthorblockN{4\textsuperscript{th} Given Name Surname}
% \IEEEauthorblockA{\textit{dept. name of organization (of Aff.)} \\
% \textit{name of organization (of Aff.)}\\
% City, Country \\
% email address or ORCID}
% \and
% \IEEEauthorblockN{5\textsuperscript{th} Given Name Surname}
% \IEEEauthorblockA{\textit{dept. name of organization (of Aff.)} \\
% \textit{name of organization (of Aff.)}\\
% City, Country \\
% email address or ORCID}
% \and
% \IEEEauthorblockN{6\textsuperscript{th} Given Name Surname}
% \IEEEauthorblockA{\textit{dept. name of organization (of Aff.)} \\
% \textit{name of organization (of Aff.)}\\
% City, Country \\
% email address or ORCID}
}

\maketitle

\begin{abstract}
This paper offers a new authentication algorithm based on image matching of nano-resolution visual identifiers with tree-shaped patterns. The algorithm includes image-to-tree conversion by greedy extraction of the fractal pattern skeleton along with a custom-built graph matching algorithm that is robust against imaging artifacts such as scaling, rotation, scratch, and illumination change. The proposed algorithm is applicable to a variety of tree-structured image matching, but our focus is on \textit{dendrites}, recently-developed visual identifiers. Dendrites are entropy rich and unclonable with existing 2D and 3D printers due to their natural randomness, nano-resolution granularity, and 3D facets, making them an appropriate choice for security applications such as supply chain trace and tracking. 

%a tree-shaped metallic foreground generated using specific electro-chemical processes and printed on a non-metallic substrate. Dendrites are entropy rich and unclonable with existing 2D and 3D printers due to their natural randomness, nano-resolution granularity, and 3D facets, making them an appropriate choice for security applications such as supply chain trace and tracking. 

% \sout{The proposed algorithm improves upon previously-implemented keypoint extraction and graph matching algorithms, including \cite{chi2020consistency}, which face various problems when deploying in real-world applications. For instance, image inconsistency due to the camera sensor noise may cause unexpected feature extraction leading to inaccurate tree conversion and authentication failure. Also, the previous tree extraction algorithms are prohibitively slow which hinders scalability to large systems.}
The proposed algorithm improves upon graph matching with standard image descriptors. It also improves \cite{chi2020consistency}, which faces various problems when deploying in real-world applications. For instance, image inconsistency due to the camera sensor noise may cause unexpected feature extraction leading to inaccurate tree conversion and authentication failure. Also, previous tree extraction algorithms are prohibitively slow hindering their scalability to large systems. 
In this paper, we fix the current issues of \cite{chi2020consistency} and accelerate the key points extraction up to 10-times faster by implementing a new skeleton extraction method, a new key points searching algorithm, as well as an optimized key point matching algorithm. Using minimum enclosing circle and center points, make the algorithm robust to the choice of pattern shape. We show that our algorithms outperform standard key descriptors such as SIFT, FAST, and ORB in terms of accuracy (with a margin of 6\% to 10\%), while utilizing much fewer key points (about 20\% to 80\% reduction). In contrast to \cite{chi2020consistency} our algorithm handles general graphs with loop connections, therefore is applicable to a wider range of applications such as transportation map analysis, fingerprints, and retina vessel imaging. 
%The proposed method is generic and applicable to a wide range of graph-based matching algorithms, \ar{including retinal vessel segmentation, and other tree-structured images}.

% \add{In the past two years, the COVID-19 has caused billions economy loss and public pandemic. For both security and safety concerns, infrastructures tries to apply new technologies for disease tracking and building access. }

\end{abstract}

\begin{IEEEkeywords}
image processing, trace and tracking, key points detection, graph matching
\end{IEEEkeywords}

\section{Introduction}
% \xc{First introduce dendrite..}
% The dendrite sample is graph patterns generated from specific biochemical processing. Its unique radiating patterns make it hard to be replicated. Thus, its fully natural randomness has been applied in many applications, for example,  supply chain tracking, door access, and security authentication..



\begin{figure}
  \centering
  \includegraphics[width=0.8\columnwidth]{figure/1.png}
  \caption{Dendrite samples from the laboratory.}
  \label{fig:dendrite}
\end{figure}


\begin{figure*}[ht]
    \centering
    \includegraphics[width=1\textwidth]{figure/2_2.png}
    \caption{\textbf{Top row:} skeleton extraction of dendritic samples. Steps: (a) the raw RGB image, (b) color space expansion, (c) non-linear filtering, (d) channel fusion via PCA, (e) K-means clustering, (f) extracted single-pixel-width skeleton. \textbf{Bottom row}: Key points extraction and matching process: (g) test image taken by camera, (h) extracted key points for test and reference image, (i) similarity matrix for 200 key points using cosine and euclidean distance (brighter is more similar), (j) pairwise matching result.}
    \label{fig:ADM}
\end{figure*}

Trace and tracking are an integral part of smart connected communities due to their role in ubiquitous and uninterrupted service provisioning. Developing AI-based monitoring and tracing systems for humans, vehicles, supplies, pharmaceutical products, and objects, in general, has become one of the major key research areas in the post-Covid era \cite{kjellberg2019thinking, razi2022deep, trivedi2020digital}. For instance, tracking humans can be used by public health providers to model, analyze, and control transmissible disease \cite{mohamadou2020review, ihme2021modeling, kumari2021novel}. 

Different technologies, including biometrics, smartphones, web access points, radio signals, roadside cameras, RFID tags, contactless cards, and even vehicles' carbon footprint are used for tracking purposes \cite{trivedi2020digital}.
It is not surprising to note that most monitoring systems rely on visual data, noting that more than 90\% of information is obtained by visual perception\cite{mit2022blink}. 

A lot of identification and authentication systems for humans are nowadays based on image processing of biometrics, including facial recognition \cite{zhao2003face}, fingerprints \cite{maltoni2009handbook}, iris \cite{daugman2009iris}, ear, and palm recognition \cite{han2012palm}.
However, the use of biometrics raises privacy concerns \cite{van2003biometrics}. Secondly, these methods are specific to humans, and not usable for object tracking in general. 

Non-human tracking systems heavily rely on a variety of printed tags and web registration. For instance, barcodes and Quick Response (QR) codes supported by web-based back-end database systems are used for tracking merchandises~\cite{kjellberg2019thinking}, and store information \cite{tiwari2016introduction}. 

 More advanced electronics-based devices can be used for secure tracking. For instance, contactless media such as Radio Frequency IDentification (RFID) tags are used for authentication in facility securing, mobile payment, EZ-pass, and supply chain tracking \cite{weinstein2005rfid, juels2006rfid}. However, these solutions, in addition to their vulnerability to a variety of security attacks (e.g., power analysis, cyber attacks, and side-channel attacks), can be costly for some applications, such as daily product monitoring. This highlights the need for developing more secure low-cost visual tags \cite{krombholz2014qr}.


%Included/no need
%The key factor that causes the insecure of non-biological based image processing method is that the extracted 2D plain images only contain limited features which can easily been attacked \cite{krombholz2014qr}. Emerging technologies are needed to solve the above problems.




% \begin{figure*}[h]
%   \centering
%   \includegraphics[width=1\textwidth]{figure/ADM_1.jpg}
%   \caption{Skeleton extraction of a dendritic sample, (a) the raw RGB image, (b) channel fusion, (c) image after feature selection, (d) is the result of K-means clustering, and (e) is the extracted skeleton}
%   \label{fig:ADM_1}
% \end{figure*}


\textit{Dendrites} are nano-resolution fractal metallic patterns grown on various substrates through an electro-chemical process \cite{kozicki2022dendritic}. Dendrites have shown potential to be used as visual identifiers, and security key generators for authentication algorithms in numerous applications \cite{chi2020consistency, wang2015nanomaterial, kala2021contactless, thiyaneswaran2020development}. 
Their unique and high-granularity patterns provide high entropy for large-scale systems. Further, their inherent randomness, material composition, nano-scaled resolution, and 3D facet provide a key feature of unclonable that makes it almost impossible to clone these tags with any affordable existing 2D and 3D printer technology \cite{kozicki2021secure}.


As shown in Fig. \ref{fig:dendrite}, dendritic patterns can be generated in different forms, density, resolution, and granularity, controlled by the fabrication process parameters, such as the induced voltage level, temperature, and discharging direction on the electrolyte fluid \cite{kozicki2021fabrication}.
% is highly modifiable. \ar{

In \cite{chi2020consistency}, we proposed a novel graph-based matching algorithm to extract the 2D feature points of dendritic patterns. This method contains three parts: extraction, detection, and matching.
Unlike the conventional graph matching algorithm for complex images such as Speeded Up Robust Features (SURF), Binary Robust Invariant Scalable Keypoints (BRISK), and Oriented FAST and rotated BRIEF (ORB), our method was designed to pick feature points only from the main body of the dendritic patterns. 
% \sout{To extract the main body of the dendritic patterns, the method proposed in \cite{chi2020consistency} relies on translate the image from RGB to LAB color format since the main body of the dendritic samples become divided from its background in such color space. After post image processing based on K-means clustering, the skeleton of the main body can be separate from its complex background.
% Then, a creative node searching algorithm was implemented to detect the key points of the extracted skeletons. The key points have three different types: root, bifurcation node, and endpoint. The searching algorithm enumerate each node of the skeletons and save the key information of each node into the key points descriptor.
% Finally, through the key information such as distance to the root, angle of rotation, and parent-child relationship, the proposed method can fast extract the feature points with a low cost of computing \cite{chi2020consistency}.
% }
% \add{Graph matching plays an critical role in various pattern recognition tasks as its primary goal is to match graphs of objects from different perspectives. Graphs can be explained as an abstract representations for complex patterns \cite{}, while the graph matching in general, is proposed to pair different graphs based on their geometry consistency \cite{}. 
% The graph generation and graph matching can be divided into two different sub-problems. 
% A common problem for the graph generation from the complex images is how to separate the interested area or body from its background. Whereas, the major consideration of graph matching is how to generate key points for images that can mapping an object from different perspective.
% A general method for graph generating from complex images is the feature points detection. In terms of searching corner features and gradient around pixels \cite{tareen2018comparative}, the detector will generate multiple points for images that are suggested as features. 
% Another way of graph producing is the image segmentation method. By extract the interested body from its background, the segmentation can provide a graph remains the geometry consistency with its origin image \cite{pham2000survey, peng2013survey, minaee2021image}.
% The following types incompletely summarized graph matching methods: spectral methods \cite{leordeanu2005spectral, carcassoni2003spectral}, probabilistic approaches \cite{caetano2004graphical}, tree search \cite{messmer1998new}, graduated assignment \cite{gold1996graduated}, and etc. 
% The graph matching algorithm follows the rule of high accuracy and high efficiency, the descriptor of nodes needed to be precise and unique to define the features, while the computation of key points matching can be optimized through dimension reducing to avoid explosion of computation cost \cite{}.}
% The conventional graph matching can be summarized as a few types. For instance, spectral methods focus on finding consistent correspondences between two sets in terms of their feature's descriptors and their geometry similarity.
% \subsection{Dendrites}

The proposed method in \cite{chi2020consistency} outperforms conventional feature extraction methods in terms of computational complexity by handpicking a few feature points  from the pattern skeleton representative of the overall pattern topology instead of collecting numerous key points from the entire image. 
However, this method faces various issues when applied to real scenarios. 
First, the skeleton extraction algorithm transforms the RGB image into YCbCr color space and applies K-means clustering to separate the dendritic pattern (foreground) from the background. Unfortunately, the discontinuity of the pattern can easily result from camera noise, color shift, or occluding, which can lead to excluding part of the tree pattern. This is due to using YCbCr color space, which is sensitive to imaging conditions. To address this issue, we use multiple channels by expanding the image into multiple color spaces and applying a set of filters to exploit a richer image representation. Then, we use PCA to map the result into 2D space. Our results show that this makes the extracted skeleton less sensitive to imaging artifacts and illumination conditions.

The second improvement is on key points searching. We reached about 10 times faster by replacing the depth-first search (DFS)-like traversal method in the previous version with a parallelized breadth-first method with hand-crafted initial points around the mass center as discussed in section \ref{sec:kpsearch}.

The third issue of \cite{chi2020consistency} is the sensitivity of the graph matching algorithm to minor variations of the extracted graph topology. We observed that the extracted key points from the same image under slightly different imaging conditions could be substantially different, which can cause false point matching. In short, the graph matching method from the previous version is performed by applying the Munkres matching algorithm to the pairwise similarity scores between the test and reference graphs' nodes. In \cite{chi2020consistency}, each node is represented by a linear combination of local features with predetermined constant weights. We improved the matching rate by i) using a richer set of features for each graph node (see Table I), and ii) using learnable weights. This learning does not require a dataset and we can use only one sample with data augmentation.


%a huge problem \arr{what?}. There are various factors. First, we found that the key points extraction results from the same image can be very different, which causes mismatch. This is because the image post processing method extracted an object that its structure is inconsistent with the reference image. Secondly, since various information have been stored in the key points descriptor, inappropriate feature description also harm matching accuracy. 


Finally, we note that our method is not restricted to tree-structured patterns, but is applicable to a variety of graph-based image matching algorithms, such as transportation map analysis, fingerprints, and retina vessel imaging.


% \section{Related Work}






% \subsection{PCA}
% % \hw{Some algorithms and explain}
% In many research works. we usually measure the data with different features and collect large data to learn patterns. Generally, a large dataset would provide abundant information for research, but it also increased the workload of data analysis. In most situations, those features are infecting each other, so if we analyze each feature individually, the result becomes independent and loses a lot of potential connections. In the worst case, derive the wrong solution.

% Indeed, to handle the high dimension dataset, one thing can do is to reduce the feature dimension and keep the most important features. For the data dimension reduction, some unnecessary features and unrelated features can be excluded to improve the efficiency of data analysis. Usually, this will lose partial information, but it saves a lot of time and other work. Data dimension reduction also become a very popular data analysis method and has been applied in various research.

% % \begin{figure}
% %   \centering
% %   \fbox{\includegraphics[width=0.6\columnwidth]{figure/PCA.png}}
% %   \caption{Reduce features of concatenated dendrite data.}
% %   \label{fig:PCA}
% % \end{figure}

% % There is some algorithm for data dimension reduction, for example, the singular value decomposition (SVD), the principal component analysis (PCA), and the linear discriminate analysis (LDA).

% The main idea of Principal Component Analysis (PCA) is to project N dimension features to the K dimension. Generally, the signals have a larger variance compared to noise, the projection can represent the signal-to-noise ratio, which larger is better. To calculate the largest variance between different access, we need to compute the covariance matrix and the divergence matrix. In the new feature space, the first axis chooses the data that has the most variance in the original dataset, the next axis chooses the data that has the most variance with the first axis, and so on so for. Finally, we will have the feature space with the N axis. The first K'th axis included the most variance. 




% \subsection{K-means Clustering}
% K-means clustering is a typical unsupervised clustering algorithm based on Euclidian distance. Usually, for two different data, the smaller their Euclidean distance is, the more similar they are supposed to be.
% K-means clustering usually stops at the local minima, but for most problems, a local minima solution is good enough. When handling a large dataset, K-means still remains elastic. More importantly, the computation cost of K-means is very low.
% However, the disadvantage is that the number of clusters K has to be defined manually. Meanwhile, a different K value usually leads to different results. Furthermore, it is very sensitive to the initial centroid, different centroid initialization will also lead to a different result. More importantly, it is very sensitive to noise and cannot be applying non-convex datasets and unbalanced datasets.
% Overall, choosing a reasonable K becomes the biggest challenge.
% % \begin{figure}
% %   \centering
% %   \fbox{\includegraphics[width=0.6\columnwidth]{figure/kmeans.png}}
% %   \caption{K-means for low-dimension data.}
% %   \label{fig:kmeans}
% % \end{figure}

% % The following algorithm describes a general idea of K-means clustering.
% % \begin{algorithm}\label{alg:1}
% % \caption{K-Means Clustering} 
% % \label{alg1}
% % \begin{algorithmic}
% % \REQUIRE {$X \in R^n$; Numbers of clusters $k$}
% % \STATE \textbf{Randomly choose initial centroids} $\mu_1$,...,$\mu_k$ 
% % \REPEAT {until convergence}
% % \ENDWHILE

% % \end{algorithmic}
% % \end{algorithm}



% First, random centroids will be initialized to represent different clusters. Then, calculate the distance of each point to its own centroid, group the point to the nearest cluster. Finally, calculate the mean coordinates of each clusters, and assign the result as a new centroid. Repeat the above steps until the maximum iteration or it reaches a minimum loss of centroid movements. 



% \subsection{SVD}
% % \xc{for Image Processing?}
% % \hw{Algorithms and more general information}
% Singular value decomposition (SVD) is another important method of machine learning. The main idea of SVD is to use three different submatrices to represent the most important feature of a complex matrix
% SVD can also be used to reduce the data dimension by decomposing complex matrices. However, in this paper, we only use SVD to compute the transform matrix for less computation and better robustness for noise.
% \xc{Singular value decomposition (SVD) is a robust numerical matrix decomposition method, which factorize a matrix $\mathbf{M}$ to $\mathbf{M}=\mathbf{U}\mathbf{\Sigma}\mathbf{V^T}$. $\mathbf{\Sigma}$ is a diagonal matrix and its elements are the sorted singular values. $\mathbf{U}$ and $\mathbf{V^*}$ are unitary. SVD has been widely used in many applications \cite{sadek2012svd} in image processing, such as rank approximation, image denoising, image compression. In our work, SVD is used to estimate the  transformation matrix of the affine transformation by decomposing it into three simple transformations: i) rotation by $\mathbf{V}$, ii) scaling by $\mathbf{\Sigma}$, and iii) anther rotation by $\mathbf{U}$.}

% In this paper, we apply SVD to solve the transform matrix and align the key points \cite{ju2016}. Assume that for each key point of a source matrix $\mathbf{P}$, we already know all the corresponding target points in target matrix $\mathbf{Q}$, where point $q_i$ is the corresponding point of $p_i$. Then a cross-covariance matrix can be forming as
% \begin{align}\label{eq:1}
%     \mathbf{M} = \mathbf{P}*\mathbf{Q^T}
% \end{align}
% compute the SVD
% \begin{align}\label{eq:2}
%     \mathbf{M} = \mathbf{U}*\mathbf{W}*\mathbf{V^T}
% \end{align}
% the rotation matrix can be found as
% \begin{align}\label{eq:3}
%     \mathbf{R} = \mathbf{V}*\mathbf{U^T}.
% \end{align}

% Finally, we translate the source matrix to target matrix by
% \begin{align}\label{eq:4}
%     \mathbf{p_i'} = \mathbf{C_T}+\mathbf{R}*\mathbf{p_i-C_S}
% \end{align}
% Where $C_S$ denotes the centroid location of the source matrix $P$, and $C_T$ denotes the centroid location of the target matrix $Q$. After the alignment, the source matrix should directly mapping and overlap with target matrix.






% \begin{figure*}[]
%   \centering
%   \includegraphics[width=1\textwidth]{figure/ADM_2.png}
%   \caption{Key points extraction and matching process: (a) the raw image, (b) are the extracted skeleton, (c) are the extracted key points, (d) match key points (lower plot) according to cosine loss and euclidean loss (upper plot), (e) shows the matching result. \arr{integrate with Fig. 1}}
%   \label{fig:ADM_2}
% \end{figure*}



\section{Methodology}
% In this section, we will implement the above method, and evaluate our contribution in both efficiency and novelty. 

% \subsection{Color Space Transform}
% Most image detection tasks require multiple images from different camera views, while the best way would be always to directly extract the object from a single image.

% The Red Green Blue (RGB) image format has been widely used in computer vision such as object detection, image segmentation, etc.. However, in many specific computer vision tasks, the RGB image seems not work well while it is extremely sensitive to the light intensity. A slight change in light intensity will cause the change of all the pixel values of the related channels (Red, Green, and Blue), which the color information is lost.
 
% The LAB color system can represent any color that exists in nature, since it's total color space is larger than RGB. Thus, such color system is more suitable to describe the human vision specifications in a digital format, regardless of the device differences. In LAB color space, $\mathbf{L}$ specifically represents the luminance, where $\mathbf{A}$ and $\mathbf{B}$ represent the intensity of red and yellow.

% For cases that the RGB color system works unsatisfied, we transform the RGB format to different color spaces (LAB, HSV, etc.) to select the image features. For example, the shadow detection \cite{murali2013shadow} in LAB color spaces, the interested objects have been divided naturally, which performs better compared to the RGB color space.

% % \begin{figure}
% %   \centering
% %   \fbox{\includegraphics[width=0.6\columnwidth]{figure/color.png}}
% %   \caption{Expand raw RGB image to different color spaces.}
% %   \label{fig:color}
% % \end{figure}

% Another example is windows detection \cite{recky2010windows}. The regular RGB histogram does not provide a correct description of the distribution of pixels. In this task, by projecting the pixels to LAB color space where the Euclidian distance of different colors is directly proportional to the visual similarity.

% For other image segmentation tasks such as human skin detection \cite{oliveira2009skin}, the HSV color space displays a better feature map since it is more related to human color perception, which is much easier to separate a single color area from other colors.

% Generally, the main body of dendrite samples is obvious that can be sensed directly under the daylight. In case we lose information in situations such as the partially occluded light source, the color shift due to the change of white balance, and the camera distortion, we expand the images into different color spaces to ensure observe the complete body of the dendritic patterns. In other word, adding more features maps to amplify the principle signal to redundant the object extraction.


\subsection{Feature Extraction}
The overall block diagram of the proposed method is shown in Fig. \ref{fig:ADM}. The first stage of the proposed method is channel fusion. 
According to several observations from experiments, we found that only a part of a dendritic pattern can be separated from the background using clustering algorithms under a specific color space plane. Also, the resulting patterns using different color spaces are not fully consistent. 

To extract a complete pattern, we develop a multi-channel input by transforming the image from RGB space into different color spaces, including YUV, HSV, gray-scale, etc. (Fig. \ref{fig:ADM}(b)), then applying 2D image filters such as edge detector, Laplacian filter, and Gabor filters to extract edge and line features (Fig. \ref{fig:ADM}(c)). The resulting stack contains a more rich set of information at different layers. 
Then, we apply Principal Component Analysis (PCA) to fuse the channels (Fig. \ref{fig:ADM}(c)(d)), and reduce the dimension of the extracted feature stacks to a single plane. This method can be viewed as transforming the original image into the most informative representation space using optimized non-linear projection, which has shown to be effective \cite{wang2014nonlinear}. Note that non-linearity comes from using different color spaces and 2D filters since PCA itself is a linear operator. 
This approach facilitates a low-complexity skeleton extraction which is robust to image variations compared to image segmentation using a single color-space layer (like using a grayscale version, or luminance layer in YCbCr color space).


%This channel fusion method and feature selection can well segment the main body of the dendrite samples from its complex background by suppress the adversarial features. \xc{Do you mean PCA is used to suppress the useless/adversarial features to help the segmentation? }



%  \begin{figure*}[h]
%   \centering
%   \includegraphics[width=1\textwidth]{figure/ADM_1.jpg}
%   \caption{Skeleton extraction of a dendritic sample, (a) the raw RGB image, (b) channel fusion, (c) image after feature selection, (d) is the result of K-means clustering, and (e) is the extracted skeleton}
%   \label{fig:ADM_1}
% \end{figure*}
 
% For less computation, we reduce it to two-dimension, and the pixels still remain good division.

% By stacking the feature maps, a multi-dimensional matrix will contain various features such as color, shape, and textures of dendritic patterns. 
% However, in a higher dimension, the computation cost of general clustering method is relatively high, and the result can easily affected by noise point. 
% Then, we apply PCA to reduce the feature dimension to a lower dimension for feature selection. This reduces the computation demand as well.

% for example, three-dimension to visualize its pixel distribution. Indeed, as shown in Fig. \ref{fig:ADM_1} (c), the pixels are well distributed at lower dimensions. 
% The division of principal components in the lower dimension equals the segmentation of the higher dimensions. 



% K-means clustering is a typical unsupervised clustering algorithm based on Euclidian distance. Usually, for two different data, the smaller their Euclidean distance is, the more similar they are.
% K-means clustering usually stops at the local minima, but for most problems, a local minima solution is good enough. When handling a large dataset, K-means still remains elastic. More importantly, the computation cost of K-means is very low.
Similar to \cite{chi2020consistency}, we remain K-means clustering for unsupervised image segmentation, i.e., extracting the pattern from the background (Fig. \ref{fig:ADM}(d)(e)). Then, we use the standard Skeletonize (scikit-image) function to turn the pattern into a single-pixel-width graph (Fig. \ref{fig:ADM}(f)).




%It is noteworthy that comparing to the previous version \xc{\cite{chi2020consistency}}, the major body of the dendritic pattern \xc{in the proposed approach} \sout{has been} \xc{are sufficiently} \sout{well} divided from its background due to the gain of feature selection. The Fig. \ref{fig:ADM_1} (e) shows the skeleton of segmented image.





\subsection{Key Points Searching}
\label{sec:kpsearch}

\begin{figure}[h]
  \centering
  \includegraphics[width=1\columnwidth]{figure/5_2.png}
  \caption{The search algorithm steps for a sample pattern using (a) \cite{chi2020consistency}, and (b) proposed method. Green, blue and red pixels, respectively, represent seed points, bifurcation, and endpoints. Red 3x3 windows represent the final position of the sliding windows. The proposed method requires only 11 steps compared to 34 steps required for the algorithm in \cite{chi2020consistency}. In the proposed algorithm, a sliding window is duplicated at bifurcation points and disappeared at endpoints, and traveling back is not allowed.}
  \label{fig:tree_search}
\end{figure}





Once the dendrite skeleton is extracted, we traverse through the skeleton to identify bifurcation points that form the representative tree for each dendritic pattern. In \cite{chi2020consistency}, the search starts from the center of the image and moves radially through the skeleton pixels using a depth-first-search (DFS)-like approach. This method has two drawbacks. \underline{First}, it assumes that the center of the dendrite is predefined (e.g., marked with a circle). This limits applicability to a certain class of dendrites. \underline{Secondly}, the DFS-like algorithm moves the sliding window forth and back through branches to find bifurcation points, as shown in Fig. \ref{fig:tree_search}(a). \underline{Thirdly}, the algorithm fails once it encounters a closed loop. We implement a similar method by sliding 3x3 windows through the extracted skeleton to identify bifurcation points with a few modifications to solve these issues and boost searching efficiency. 

First, we find the mass center of the dendrite pattern by simply averaging the identified key point positions. We regard the mass center as the root of the tree. If $\mathcal{N}=\{n_1,n_2,...n_N\}$ is the set of $N$ identified key points, then the mass center is $c=(\mu_x,\mu_y)=\sum_{n_i \in \mathcal{N}}{n_i}/N=\sum_{i=1}^{N}{(x_i,y_i)}/N$. Then, we find the Minimum Enclosing Circle (MEC), the smallest circular contour centered at $c$ that encompasses all nodes. 
%The obtained radius would be 
%\begin{align}\label{eq:7}
%R= \max \underset{n_i \in \mathcal{N}}{d(c, n_i)) 
%\end{align}
The obtained radius would be almost half the maximum pairwise distance between the nodes of dense graphs
\begin{align}\label{eq:7}
R_{MEC}= 1/2 \max \underset{n_i, n_j \in \mathcal{N}}d(n_i, n_j)) 
\end{align}
Then, we define the initial circle, centered at $c$ with radius $R_{init}$ proportional to that of MEC ($R_{init}=\alpha R_{MEC}$ with $0 < \alpha < 1$). 
The intersection between the initial circle and the skeleton defines the set of seed points $\mathcal{S}$. 
% , as shown below:

% \begin{align}\label{eq:7}
%     \mathcal{S}=\{n| n\in \mathcal{N},  C_{init} \bigcap I_{skeleton} \\
%     C_{init} &: x^2 + y^2 = \alpha*R_{skeleton} 
% \end{align}

% \begin{align}\label{eq:7}
%     N_{root} &= C_{init} \bigcap I_{skeleton} \\
%     C_{init} &: x^2 + y^2 = \alpha*R_{skeleton} 
% \end{align}




% The overall idea of key point searching is \xc{lightning and concise}. 

% \add{Where $N_{root}$ denotes the root nodes, $C_{init}$ denotes the initial circle, $I_{skeleton}$ denotes the skeleton image, $x$ and $y$ represents the center location of initial circle, $R$ denotes the radius of MEC of the skeleton image, and $\alpha$ is a variable.}


The search algorithm starts by sliding 3x3 windows from all seed points following the skeleton branches in parallel. As these sliding windows pass through the skeleton pixels, the visited pixels are eliminated from the skeleton map. Anytime a bifurcation point is found, the sliding window is replicated, each going through an emerging branch. The window is erased once encountering an endpoint. The collected bifurcation and endpoints are recorded as key points. When two windows collide, they both disappear, and the merge point is recorded as an endpoint (Fig. \ref{fig:tree_search}(b)). The algorithm terminates when all windows disappear. 
In general, the extracted key points provide an abstract representation of the dendritic pattern retaining the overall geometrical features of the original image. 

Due to using the mass center to identify the initial circle (equivalently, seed points $\mathcal{S}$), the algorithm is robust to rotation, distortion, or any affine transformation in general. Making the initial circle radius proportional to MEC makes it scale-invariant. Eliminating visited pixels, using multiple seed points, and splitting a sliding window at bifurcation points, altogether accelerate the searching speed. Our algorithm handles cycles by merging colliding sliding windows as an additional improvement over \cite{chi2020consistency}.

\begin{comment}
The Algorithm \ref{alg1} describes the general idea of key points searching.


\begin{algorithm}[h]
\caption{Fast Key Points Detecting} 
\label{alg1}
\begin{algorithmic}
\REQUIRE {$P$ ; Node List }
\ENSURE {$N$; Key Points Set}
\STATE \textbf{Add initial nodes to list} $P \Leftarrow p_i$ 
\WHILE{$P$ is not $\phi$}
\item $x_{ii}, y_{ii} \Leftarrow $ParentLocation$(p_i)$
\item PixelValue$(x_{ii}, y_{ii}) = 0$

\item $x_i, y_i \Leftarrow $Location$(p_i)$
\item PixelValue$(x_i, y_i) = 0$

\item $S(a,b) = $PixelValue$(a,b) \begin{cases} a = {x_i-1,x_i,x_i+1}\\ b = {y_i-1,y_i,y_i+1}\end{cases}$

\item $M \Leftarrow \sum{S(a,b)} $


% a\in {x_i-1,x_i,x_i+1}, b\in {y_i-1,y_i,y_i+1}$

\item\IF{$M = 0$}
\item $N \Leftarrow p_i$
\item $P$ delete $p_i$

\item\ELSIF{$M = 1$}
\item $P$ delete $p_i$
\item $p_{ii} \Leftarrow $Location$($PixelValue$(a,b) $ is $ 1)$
\item $P \Leftarrow p_{ii}$

\item\ELSIF{$M > 1$}
\item $N \Leftarrow p_i$
\item $P$ delete $p_i$
\item $p_{ii} \Leftarrow $Location$($PixelValue$(a,b) $ is $ 1)$
\item $P \Leftarrow p_{ii}$

\item \ENDIF
\ENDWHILE

\end{algorithmic}
\end{algorithm}

\end{comment}






% After extract the main body of the dendrite, we apply the developed key points searching algorithm to extract the key points of the dendrite. For images that already have a manually marked circle, we set the pixels that connected to the circle as the initial root nodes. For samples that do not have a marked circle, we compute its pixel centroid and mark the centroid as the initial center. The same dendrite sample will have the same centroid regardless of the image distortion or camera rotation.
% \begin{figure}
%   \centering
%   \fbox{\includegraphics[width=0.7\columnwidth]{figure/keypoints.png}}
%   \caption{The process to extract key points from the dendrite image. (a) K-means clustering. (b) Main body. (c) Key points.}
%   \label{fig:keypoints}
% \end{figure}

% Once the root nodes are initialized, the algorithm searches each pixel one by one until there is a bifurcation or endpoint. For such a point, we record it as a feature point, and we also record other useful information such as the elevation between itself and its parent. For the same dendrite sample, the centroids and the initialized root nodes are also the same, thus the extracted data points are supposed to be similar.

%The Table \ref{tab:keypoint} gives the detail information for each key point of the matrix.


% \begin{figure*}[h]
%   \centering
%   \includegraphics[width=1\textwidth]{figure/benchmark.png}
%   \caption{Conventional key points matching method comparison, (a) Scale-invariant feature transform (SIFT), (b) Binary Robust invariant scalable keypoints (BRISK), (c) Oriented FAST and Rotated BRIEF (ORB).}
%   \label{fig:benchmark}
% \end{figure*}



\begin{table}[h]
\centering
\caption{Key point's feature information}
\label{tab:keypoint}
\resizebox{1\columnwidth}{!}{%
\begin{tabular}{lll}\toprule
\textbf{}               & \textbf{Definition}                     & \textbf{Values (Typical/Example)} \\
\\ \midrule
\textbf{X}              & Coordinate of X axis on canvas          & 0$\sim$500             \\
\textbf{Y}              & Coordinate of Y axis on canvas          & 0$\sim$500             \\
\textbf{Level}          & Level of current node                   & 0$\sim$20              \\
\textbf{Angle}          & Angle to parent node                    & -360$\sim$360          \\
\textbf{RelativeLength} & Distance to parent node                 & 0$\sim$100             \\
\textbf{Type}           & Type of node                            & root, bifurcation, end \\
\textbf{ChildIndex}     & Index of child nodes                    & {[}7,8,9{]}            \\
\textbf{LevelIndex}     & Level of child nodes                    & {[}5,10,20{]}          \\
\textbf{DistToParentX}  & Distance to X coordinate of parent node & 0$\sim$50              \\
\textbf{DistToParentY}  & Distance to Y coordinate of parent node & 0$\sim$50              \\
\textbf{ParentIndex}    & Index of parent node                    & {[}0{]}                \\
\textbf{SiblingIndex}   & Index of sibling nodes                  & {[}2,3,4{]}            \\
\textbf{DistToRoot}     & Distance to root node                   & 0$\sim$150             \\ 
\textbf{Index}          & Search order                            & 0$\sim$200             \\ \bottomrule
\end{tabular}}
\end{table}


\subsection{Graph Matching}  \label{sec:matching}

The extracted key points fully determine the representative tree for each dendrite. Therefore, the problem of identification (finding the best match between a sample dendrite pattern and a set of reference patterns in the web-based dataset) translates into evaluating similarity scores between their representative graphs (trees in this case). To assess the similarity between any two trees, we first need to find a one-to-one mapping between their points. We assign a feature vector for each keypoint that mimics local and global morphological information around the keypoint, such as type, distance to the root node (mass center), depth (number of nodes to root), orientation, parent node, sibling nodes, as well as their child node (see Table \ref{tab:keypoint} for details). This list extends the features used in \cite{chi2020consistency}. In contrast to \cite{chi2020consistency}, where each feature contributes equally to the mapping algorithm, we assign weight to features based on their contribution to the ultimate matching accuracy based on the obtained similarity score. This flexible method is driven by the fact that some features might have more essential roles in finding the right match between the nodes of two test and reference trees. Noe that we do not require a dataset for this training purpose, and it can be done with only one sample using data augmentation method.

% For a dendritic pattern, the final results of key points search are suppose to remain the same.

To this end, we design experiments using data augmentation to evaluate the individual robustness of features against different variations. For each reference sample $R_i$, we generate a set of manipulated test samples $\{T_{i1},T_{i2},....T_{iM}\}$ by applying rotation, scaling, and perspective transformation along with noise, as shown in Fig. \ref{fig:data}.

\begin{figure}[h]
  \centering
  \includegraphics[width=0.6\columnwidth]{figure/10.png}
  \caption{Samples for data augmentation, (a) raw image, (b) noisy image with Gaussian noise ($\sigma^2=0.1$), (c) image with perspective transformation (distortion degree = 0.5), (d) rotated image ($\deg = 30$).}
  \label{fig:data}
\end{figure}



Then we apply the proposed key points extraction method to all samples. Now, we generate distance matrices between the nodes of reference and sample trees. If $\mathbf{S}(R_i,T_{ij})=[S_{ij}]$ is the distance matrix between $R_i$ and $T_{ij}$, then $S_{kl}$ is the distance measure between node $n_k$ from tree $R_i$ and node $n_l$ from tree $T_{ij}$. In this paper, we use $d^{(F)}(n_i,n_k)=d_E^{(F)}(n_k,n_l) + d_{cos}^{(F)}(n_k,n_l)$ where $d_E^{(F)}(n_k,n_l)$ and $d_{cos}^{(F)}(n_k,n_l)$ are respectively, the normalized Euclidean and Cosine distance between nodes $n_k$ and $n_l$ using feature $F$. We normalize the measures between 0 and 1 to avoid scaling bias. Then, we apply PCA analysis on average distance metrics obtained using different features to evaluate the contribution of features in generating distance (equivalently similarity) scores. The weights obtained by PCA analysis are used to weigh features before performing the graph matching algorithm. This method provides flexibility in using an arbitrary set of features to represent key points without prior knowledge about the relevance of features. 

% By calculating the square error of features between the reference images and test images, it is confident to determine which features are beneficial \arr{how? are we selecting features or sorting/ranking them, or assignig weights to each one}. The feature loss is calculated as below:
% \begin{align}\label{eq:6}
%     \mathcal{L} = \sum_{i=1}^{n} ||\hat{F_i} - F_i||^2,
% \end{align}
% where $F_i$ represents the features of the reference image, and $\hat{F_i}$ represents the features of the test image. \xc{Can we here say "we prefer the features which are invariant to the transformation"?}

%This feature selection process more relies on experimental results other than empirical or prior knowledge in the previous version. \xc{Is this approach more reliable than previous work? If it is, please state it here.}

%After construct a key points dataset of the reference image and a key points dataset of the test image, we apply both euclidean similarity and cosine similarity to evaluate the consistency between key points. 
%Generally, the euclidean similarity indicates the average distance of features, while the cosine similarity represents the similarity of two vectors in a higher dimension. We combine both two metrics as redundant for robustness. \xc{@Hao: It is better to show the ablation analysis, if only one distance is used.}




\begin{figure}[h]
  \centering
  \includegraphics[width=0.7\columnwidth]{figure/7_2.png}
  \caption{Using SVD to overlap two matched graphs.}
  \label{fig:svd}
\end{figure}





% The experiments including generating test samples by apply rotating and view transformation to the reference samples. By calculating the square error of features between the reference images and test images, it is confident to determine which features are beneficial. The feature loss is calculated as below:
% \begin{align}\label{eq:6}
%     \mathcal{L} = \sum_{i=1}^{n} ||\hat{F_i} - F_i||^2,
% \end{align}
% where $F_i$ represents the features of the reference image, and $\hat{F_i}$ represents the features of the test image. \xc{Can we here say "we prefer the features which are invariant to the transformation"?}

% This feature selection process more relies on experimental results other than empirical or prior knowledge in the previous version. \xc{Is this approach more reliable than previous work? If it is, please state it here.}

% After construct a key points dataset of the reference image and a key points dataset of the test image, we apply both euclidean similarity and cosine similarity to evaluate the consistency between key points. 
% Generally, the euclidean similarity indicates the average distance of features, while the cosine similarity represents the similarity of two vectors in a higher dimension. We combine both two metrics as redundant for robustness. \xc{@Hao: It is better to show the ablation analysis, if only one distance is used.}


Then we apply Kuhn-Munkres algorithm to find the best match between the key points of the reference and test images. An exemplary distance matrix is presented in Fig. \ref{fig:ADM}(i), where pixel intensity (brighter colors) indicates higher similarity between the matched nodes.
A visualization of matching (pairing) nodes between the two graphs is provided in Fig. \ref{fig:ADM}(j). The accuracy of matching is not easily visible. In order to offer a more clear and qualitative visualization of the matching accuracy, we use SVD transformation to align the two graphs based on paired nodes.



Finally, we apply SVD to solve the transform matrix that projects corresponding $N$ key points from the test graph $T$ to the reference graph $R$. We list the sorted order of key points for the two graphs in matrices $P_{2\times N}$ and $Q_{2\times N}$. For instance, the $i^{th}$ column of $P$ is $[P_{1i}, P_{2i}]^T=[x_i-\mu_x,y_i-\mu_y]^T$ is the position of the $i^{th}$ node in the test graph ($n_i=(x_i,y_i) \in T$). 
We subtract the mean (the mass center position) $c=(\mu_x,\mu_y)$ to make it zero-mean. Likewise, the $i^{th}$ column of $Q$ is the zero-mean position of the corresponding point in the reference graph. Then, the cross-covariance matrix can be formed as
\begin{align}\label{eq:1}
    \mathbf{M} = \mathbf{P}*\mathbf{Q^T}.
\end{align} 
We compute the SVD as
\begin{align}\label{eq:2}
    \mathbf{M} = \mathbf{U}*\mathbf{\Sigma}*\mathbf{V^T},
\end{align}
%where the rotation matrix is $\mathbf{R} = \mathbf{V}*\mathbf{U^T}$ and $\Sigma$ is the diagonal scaling matrix. 
It has been shown than the optimal transformation matrix that minimize the squared loss between the reference and projected points, can be obtained as \cite{sorkine2009least}: 
\begin{align}\label{eq:4}
& \mathbf{R} = \mathbf{V} * \left(\begin{array}{lllll}
1 & & & \\
& 1 & & & \\
& & \ddots & \\
& & & 1 & \\
& & & & \operatorname{det}\left(\mathbf{V} \mathbf{U}^T\right)
\end{array}\right) * \mathbf{U}^T\\
%&t=c_R - R *c_P\\
&\mathbf{p_i'} = \mathbf{c_R}+\mathbf{R}*(\mathbf{p_i-c_T}), 
\end{align}
where $X^T$ is the transpose of matrix $X$, $c_R$, and $c_T$ denotes the mass center of graphs $R$, and $T$. We expect that the transformed points of $T$ ($p_i'$) align well with the points of $R$. 
The results in Fig. \ref{fig:svd} shows a nearly-perfect alignment ($92\%$ accuracy) between the two graphs based on paired nodes using the proposed method. % discussed in section \ref{sec:matching}.







% \begin{figure}
%   \centering
%   \fbox{\includegraphics[width=0.5\columnwidth]{figure/mapping.png}}
%   \caption{Key points mapping.}
%   \label{fig:mapping}
% \end{figure}



\begin{comment}
In this paper, we apply SVD to solve the transform matrix and align the key points \cite{ju2016}. Assume that for each key point of a source matrix $\mathbf{P}$, we already know all the corresponding target points in target matrix $\mathbf{Q}$, where point $q_i$ is the corresponding point of $p_i$. Then a cross-covariance matrix can be forming as
\begin{align}\label{eq:1}
    \mathbf{M} = \mathbf{P}*\mathbf{Q^T},
\end{align}
compute the SVD
\begin{align}\label{eq:2}
    \mathbf{M} = \mathbf{U}*\mathbf{W}*\mathbf{V^T},
\end{align}
the rotation matrix can be found as
\begin{align}\label{eq:3}
    \mathbf{R} = \mathbf{V}*\mathbf{U^T}.
\end{align}

Finally, we translate the source matrix to target matrix by
\begin{align}\label{eq:4}
    \mathbf{p_i'} = \mathbf{C_T}+\mathbf{R}*\mathbf{p_i-C_S}
\end{align}
Where $C_S$ denotes the centroid location of the source matrix $P$, and $C_T$ denotes the centroid location of the target matrix $Q$. After the alignment, the source matrix should directly mapping and overlap with target matrix.





% From Section II. part D. we know that the transform matrix $M$ can be obtained by the corresponding points. 
\add{Since we already have the key point pairs, we are able to build the cross-covariance matrix for SVD. The cross-covariance matrix $M$ represents how a matrix is transformed into another matrix. 
}
%\ref{eq:1} \ref{eq:2} \ref{eq:3} \ref{eq:4}
Through Eqs. \ref{eq:1}-\ref{eq:4}, the rotation matrix $R$ and the translation matrix $T$ can be calculated step by step. After applying the transform matrix to the skeleton maps, the skeleton map from the test image will be projected to the reference image, as shown in Fig. \ref{fig:ADM_2} (e). This figure shows how two dendritic patterns are mapped exactly. The matching loss is calculated by the mean distance of paired key points, as show in \ref{eq:6}. The similarity between two patterns can be derived as $(100-\eta)*100\% $.
\end{comment}

% \begin{figure}
%   \centering
%   \fbox{\includegraphics[width=0.5\columnwidth]{figure/SVD.png}}
%   \caption{Feature map alignment using SVD.}
%   \label{fig:SVD}
% \end{figure}


% \begin{table}[hb]
% \centering
% \caption{Key Points Information \xc{Consider to zoom in this table}}
% \label{tab:keypoint}
% \resizebox{1\columnwidth}{!}{
% \blt\begin{tabular}{lll}\toprule
% \textbf{}               & \textbf{Definition}                     & \textbf{Values (Typical/Example)} \\
% \\ \midrule
% \textbf{Index}          & Index of node                           & 0$\sim$200             \\
% \textbf{X}              & Coordinate of X axis on canvas          & 0$\sim$500             \\
% \textbf{Y}              & Coordinate of Y axis on canvas          & 0$\sim$500             \\
% \textbf{Level}          & Level of current node                   & 0$\sim$20              \\
% \textbf{Angle}          & Angle to parent node                    & -360$\sim$360          \\
% \textbf{RelativeLength} & Distance to parent node                 & 0$\sim$100             \\
% \textbf{Type}           & Type of node                            & root, bifurcation, end \\
% \textbf{ChildIndex}     & Index of child nodes                    & {[}7,8,9{]}            \\
% \textbf{LevelIndex}     & Level of child nodes                    & {[}5,10,20{]}          \\
% \textbf{DistToParentX}  & Distance to X coordinate of parent node & 0$\sim$50              \\
% \textbf{DistToParentY}  & Distance to Y coordinate of parent node & 0$\sim$50              \\
% \textbf{ParentIndex}    & Index of parent node                    & {[}0{]}                \\
% \textbf{SiblingIndex}   & Index of sibling nodes                  & {[}2,3,4{]}            \\
% \textbf{DistToRoot}     & Distance to root node                   & 0$\sim$150             \\ 
% \bottomrule
% \end{tabular}}
% \end{table}
% \hw{will fill later}




\section{Results}





% \resizebox{1\columnwidth}{!}{%
% \blt
% \begin{tabular}{|cccccccc|}
% \hline
% \multicolumn{2}{|c}{Rotation}       &                       & \multicolumn{2}{c}{{\color[HTML]{333333} Perspective}} &                       & \multicolumn{2}{c|}{{\color[HTML]{333333} Noise}} \\ \hline
% degree & \multicolumn{1}{c|}{ACC}   & \multicolumn{1}{c|}{} & ratio           & \multicolumn{1}{c|}{ACC}             & \multicolumn{1}{c|}{} &                        & ACC                     \\ \cline{1-2} \cline{4-5} \cline{7-8} 
% 30     & \multicolumn{1}{c|}{99.01} & \multicolumn{1}{c|}{} & 0.1             & \multicolumn{1}{c|}{97.36}           & \multicolumn{1}{c|}{} & 0.01                    & 85.68                   \\
% 70     & \multicolumn{1}{c|}{98.86} & \multicolumn{1}{c|}{} & 0.2             & \multicolumn{1}{c|}{94.79}           & \multicolumn{1}{c|}{} & 0.1                     & 79.24                   \\
% 100    & \multicolumn{1}{c|}{98.26} & \multicolumn{1}{c|}{} & 0.3             & \multicolumn{1}{c|}{89.96}           & \multicolumn{1}{c|}{} & 0.2                     & 77.51                   \\
% 200    & \multicolumn{1}{c|}{98.21} & \multicolumn{1}{c|}{} & 0.4             & \multicolumn{1}{c|}{86.91}           & \multicolumn{1}{c|}{} & 0.5                     & 67.47                   \\
% 300    & \multicolumn{1}{c|}{98.44} & \multicolumn{1}{c|}{} & 0.5             & \multicolumn{1}{c|}{85.85}           & \multicolumn{1}{c|}{} & 1                       & 59.13                   \\ \hline
% \end{tabular}}
\subsection{Benchmark Test}
% \hw{Compare to old version}
We present results in this section to evaluate the proposed method from different perspectives, including the %computing speed, computation complexity of the search algorithm, the number of searched key points, 
computational cost, node matching rate, and ultimate identification accuracy. Table \ref{tab:bench} 
illustrates benchmark results, where the proposed method is compared against commonly used image descriptors as well as the formerly proposed dendrite matching algorithm in \cite{chi2020consistency}. %\sout{Note that only the top 20\% key paired points are used to compute the distance loss.}
The overall matching loss $\mathbf{\eta}$ is defined as:
\begin{align}\label{eq:5}
    \mathbf{\eta} = (\frac{\sum_{i=1}^{N}dist(p_i',q_i)}{N}),  
    % where ~~ \alpha = \begin{cases} 1 & {N_{valid} \geq N_{threshold}}    \\ 
    % 0 & {N_{valid} < N_{threshold}}\\ 
    % \end{cases}
\end{align}
where $dist(p,q)$ is an arbitrary distance metric, (we use Euclidean distance), N is the number of matched key points, $\mathbf{q_i}$ is the position of the $\mathbf{i}$'th key point of the reference graph, and $\mathbf{p_i'}$ is the position of the corresponding point in the test graph after performing SVD projection as detailed in section \ref{sec:matching}. 
% $\mathbf{N_{valid}}$ denotes the valid points, and $\mathbf{N_{threshold}}$ denotes the minimum points to continue the matching process.


\begin{table}[h]
\centering
\caption{COMPARATIVE ANALYSIS OF THE PROPOSED METHOD.}
\label{tab:bench}
\resizebox{0.8\columnwidth}{!}{%
\begin{tabular}{ccccc}\toprule
            & \textbf{Time} & \textbf{Key Points} & \textbf{Accuracy} \\
            \midrule
FAST                       & 0.026          & 1625                                  & 88.38             \\ 
Harris                      & 0.016          & 279                                      & 83.72             \\ 
SIFT                        & 0.091          & 527                                    & 85.58             \\ 
BRISK                       & 0.044          & 670                                      & 84.92             \\ 
ORB                         & 0.020          & 502                                      & 83.13             \\ 
Graph Matching 1 \cite{chi2020consistency}                   & 1.589          & 202                                    & 89.59             \\
\textbf{Proposed}                  & \textbf{0.131}          & \textbf{212}                                     & \textbf{95.90}             \\ \bottomrule
\end{tabular}}
\end{table}




% \begin{table}[h]
% \centering
% \caption{Comparative analysis of the proposed method.}
% \label{tab:evaluation}
% \resizebox{1\columnwidth}{!}{%
% \blt\begin{tabular}{cccccc}\toprule
%             & \textbf{} & \textbf{} & \textbf{Accuracy} & \textbf{} & \textbf{} \\
%             \midrule
% Rotation(30/70/100/200/300)        & 99.01                  & 98.86          & 98.26                   & 98.21                   & 98.44           \\ 
% Perspective(0.1/0.2/0.3/0.4/0.5)      & 97.36                  & 94.79          & 89.96                    & 86.91                    & 85.85             \\ 
% Noise(0.01/0.1/0.2/0.5/1)        & 85.68                  & 79.24          & 77.51                    & 67.47                    & 59.13             \\ 

%        \\ \bottomrule
% \end{tabular}}
% \end{table}





% \begin{align}\label{eq:5}
%     % \mathbf{\eta} = \alpha*(100 - \frac{\sum_{i=1}^{N}dist(p_i'-q_i)}{N})   \\
%     \alpha = \begin{cases} 1 & {N_{valid} \geq N_{threshold}}    \\ 
%     0 & {N_{valid} < N_{threshold}}.\\ 
%     \end{cases}
% \end{align}



% \hw{will fill later}

According to Table \ref{tab:bench}, our method achieves 10-times speed compared to the reference method in \cite{chi2020consistency} due to implementing several new features such as parallel searching with multiple sliding windows, eliminating visited pixels to avoid repeated visits, and handling undesired loop scenarios in the pattern. 
Noting that both methods are slower than highly-optimized standard key descriptor methods. It is justified by the fact that our method (similar to \cite{chi2020consistency}) is customized for dendritic patterns, and includes multiple extra image processing stages to restrict the selected key points to the dendrite pattern skeleton in contrast to standard key descriptors.  


% Please add the following required packages to your document preamble:
% \usepackage[table,xcdraw]{xcolor}
% If you use beamer only pass "xcolor=table" option, i.e. \documentclass[xcolor=table]{beamer}



The proposed method achieves a high accuracy of $95.9\%$, which is significantly higher than all competitor methods by a significant margin. The gain with respect to the closest method is over $6\%$. Note that here accuracy is the rate of successfully matched key points between the high-resolution reference image and the low-resolution and noisy test image of the same dendrite taken with a regular camera. 

Also, it is shown that an excellent matching rate of 96\% is achieved with much fewer key points compared to standard descriptors. For instance, SIFT achieves 85\% accuracy using over 500 key points, while our method achieves $96\%$ using only 212 points. The number of key points is comparable to \cite{chi2020consistency}, but our method includes 10 additional seed points. Achieving high accuracy with fewer key points reduces the computational complexity and makes it robust to imaging artifacts, noise, and scratching.


\begin{table}[h]
\centering
\caption{Comparative analysis of the proposed method under different rotation, perspective, and noise.}
\label{tab:test}
\resizebox{0.8\columnwidth}{!}{%
\begin{tabular}{cc|cc|cc} \toprule
\multicolumn{2}{c}{Rotation} & \multicolumn{2}{c}{Perspective} & \multicolumn{2}{c}{Noise} \\ \midrule
\textbf{degree} & \textbf{ACC} & \textbf{ratio}  & \textbf{ACC}  & $\mathbf{\sigma^2}$ & \textbf{ACC}               \\ \midrule
30              & 99.01      & 0.1            & 97.36          & 0.01        & 85.68       \\
70              & 98.86      & 0.2            & 94.79          & 0.1         & 79.24       \\
100             & 98.26      & 0.3            & 89.96          & 0.2         & 77.51       \\
200             & 98.21      & 0.4            & 86.91          & 0.5         & 67.47       \\
300             & 98.44      & 0.5            & 85.85          & 1           & 59.13      \\ \bottomrule
\end{tabular}%
}
\end{table}


\begin{figure}[h]
  \centering
  \includegraphics[width=0.7\columnwidth]{figure/kp_cos_2.png}
  \caption{Graph matching test using 50 dendrite samples, pixel intensity represents the matching loss, brighter is better}
  \label{fig:kpcos}
\end{figure}


Table \ref{tab:test} demonstrates the robustness of the proposed method to rotation, perspective transformation, and noise. It is seen that the drop in matching rate accuracy is negligible under any rotation which verifies the rotation-invariance property. For perspective transformation the performance degradation remains under 12\% (from 97\% to 85\%), which is acceptable. 

For the dendrite identification test, we use 50 different dendrite samples and then apply the perspective transformation (distortion degree$=0.5$) to generate distorted test samples. In each round, we assess the similarity between the reference and all test samples. We repeat the experiment 20 times and take the average matching rate. Fig. \ref{fig:kpcos} shows the results of cross-matching of 50 different samples. The high similarity scores between the corresponding samples appear as a bright diagonal, which indicates that the proposed method approached high identification accuracy and yields excellent false-reject performance. 


% \begin{figure}[h]
%   \centering
%   \includegraphics[width=1\columnwidth]{figure/8.png}
%   \caption{Application of the proposed method to other graph-structured images.}
%   \label{fig:tree}
% \end{figure}


% \begin{figure}[h]
%   \centering
%   \includegraphics[width=1\columnwidth]{figure/9.png}
%   \caption{Application of the proposed method to other graph-structured images. \arr{remove 0.7 ...}}
%   \label{fig:tree}
% \end{figure}





\begin{figure}[h]
  \centering
  \includegraphics[width=1\columnwidth]{figure/6_2.png}
  \caption{Application of the proposed method to other graph-structured images. Green, blue, and red pixels in the key points map, respectively, represent seed points, bifurcation, and endpoints. (a) is a fingerprint image, (b) is the map of Tokyo Metro.}
  \label{fig:tree}
\end{figure}




% \section{Discussion}
\subsection{Generalizability Test}



In order to demonstrate the generalizability of the proposed method beyond dendrite identification, we test the proposed method using different sample types and achieved promising results as well. 
Fig. \ref{fig:tree} presents the results for two cases, fingerprints and maps. The extracted skeleton and the extracted key points for both cases represent the overall geometry of the original image that preserves maximal information.
In the Tokyo Metro Map, benefited from the feature fusion, the proposed method has divided the subway lines (colored) from other crossed city highways. Since the metro stations and intersections appear as key points, the resulting key points can be used for metro complexity evaluation or transportation analysis \cite{stott2010automatic, chen2022network}.
Also, it is seen that in contrast to \cite{chi2020consistency}, which is applicable only to tree-structured loop-free graphs, our method is applicable to more complex graphs. 



% \begin{figure}[h]
%   \centering
%   \includegraphics[width=1\columnwidth]{figure/Tree_kp_zoom.jpg}
%   \caption{The skeleton and key points extraction from a tree image, (a)key points of the tree image ,(b) A zoom view of selected area.}
%   \label{fig:tree_zoom}
% \end{figure}



% \section{Discussion}





%\subsection{Retinal Vessel Images}

\begin{figure}[h]
  \centering
  \includegraphics[width=1\columnwidth]{figure/4.png}
  \caption{Application of the proposed method to Retinal Vessel images. (a) original image, (b) foreground using feature fusion and kmeans segmentation, (c) extracted skeleton, (d) key points map, green, blue, and red pixels, respectively, represent seed points, bifurcation, and endpoints.}
  \label{fig:retina}
\end{figure}

We have also tested the proposed methods on retinal vessel images. 
Retinal vessel segmentation has remained a hot research topic over the last decade. 
The information regarding age-related macular degeneration and diabetic retinopathy (DR) can be extracted from fundus screening \cite{lyu2022fractal}.
With simple modifications in the modeling parameters (such as using different color spaces and filters in channel fusion and the number of clusters (K) in the segmentation stage), the proposed method is applicable to retina images and yields reasonable performance. 

Recently, the deep learning (DL)-based methods have been applied to image segmentation, skeleton extraction, and image matching tasks \cite{fu2016deepvessel, guo2021mes, lyu2022fractal, nguyen2013effective, orlando2016discriminatively, shah2019unsupervised}. 
Although the overall performance of DL methods is much higher, our method is unsupervised and train-free that still remains advantages in specific tasks. More importantly, our proposed method is fully compatible with their results, which means we can directly utilize their extracted skeleton for key points identification and graph matching. For the application at hand, using dendrites as visual identifiers, the sharing of images may raise security and privacy concerns. Therefore, untrained methods are highly desired. 

% This paper have applied a set of machine learning algorithms to solve a specific problem. However, the object detection-based tasks in machine learning are always based on our prior discrimination and the data analysis. This means when we apply those methods in reality, we still facing some severe problem due to the data measurement bias, noise, and wrong hypothesis. 
% For example, the number of clusters for the K-means segmentation is the most important variable that directly determined the success chance of the body extraction. Meanwhile, when calculate the similarity between vectors, the feature selection and similarity methods (euclidean, cosine, or combined) are the key to improve matching accuracy.
% Overall, the machine learning method is more suitable to solve a specific problem, but can hardly derives a universal solution for all such kinds of problems. Despite the superior performance for solve monotonic problems, data analysis and the initial assumption become more important when improving the robustness and reliability. Finding an efficient way to solve the problem is more important than an extraordinary algorithm.




\section{Conclusion}
We develop a new method for visual identification based on graph matching of tree-structured images. The essence of our method is extracting representative graphs from test and reference images using a channel fusion method along with a fast key points extractor. The resulting key points are used by pairwise point matching with a learnable loss function to assess the structural similarity between the two graphs. 
%,   Our methoid that based on pure image processing and machine learning algorithms to detect key points from dendritic patterns. Meanwhile, a novel graph matching algorithm has been implemented to align and evaluate the overall structure similarity of two different samples. 
The experiments exhibit our proposed method has competitive performance against several benchmarks in terms of both robustness and efficiency while using much fewer key points to assess structural similarity. We improved upon the recently developed algorithm in \cite{chi2020consistency} in terms of complexity and matching rate by introducing a few novel features. Specifically, our method utilizes about 20\% to 80\% fewer key points than standard descriptors such as SIFT, BRISK, ORB, etc. Our method is about 10-times faster with respect to \cite{chi2020consistency}. More importantly, our method handles general graphs with loopy connections, extending the applications beyond tree-structured images. Therefore, it is applicable to fingerprint detection, map analysis, retinal vessel imaging, and similar tasks. Finally, note that our method is unsupervised and does not require a training dataset as opposed to deep learning-based methods, which makes it appropriate for security-restricted and privacy-preserving applications.


%Moreover, this frame work can be applied to all tree-structured images including the retinal vessel images. The modification allows it to be fast deployed to all similar tasks. This solution can be widely applied to many research related with secure authentication and disease tracking.









% \section*{Acknowledgment}

% The preferred spelling of the word ``acknowledgment'' in America is without 
% an ``e'' after the ``g''. Avoid the stilted expression ``one of us (R. B. 
% G.) thanks $\ldots$''. Instead, try ``R. B. G. thanks$\ldots$''. Put sponsor 
% acknowledgments in the unnumbered footnote on the first page.



% \fbox{The format of reference need to correct..}
% \printbibliography %Prints bibliography
\bibliography{ref}
\bibliographystyle{vancouver}


% \section*{References}

% Please number citations consecutively within brackets \cite{b1}. The 
% sentence punctuation follows the bracket \cite{b2}. Refer simply to the reference 
% number, as in \cite{b3}---do not use ``Ref. \cite{b3}'' or ``reference \cite{b3}'' except at 
% the beginning of a sentence: ``Reference \cite{b3} was the first $\ldots$''

% Number footnotes separately in superscripts. Place the actual footnote at 
% the bottom of the column in which it was cited. Do not put footnotes in the 
% abstract or reference list. Use letters for table footnotes.

% Unless there are six authors or more give all authors' names; do not use 
% ``et al.''. Papers that have not been published, even if they have been 
% submitted for publication, should be cited as ``unpublished'' \cite{b4}. Papers 
% that have been accepted for publication should be cited as ``in press'' \cite{b5}. 
% Capitalize only the first word in a paper title, except for proper nouns and 
% element symbols.

% For papers published in translation journals, please give the English 
% citation first, followed by the original foreign-language citation \cite{b6}.

% \begin{thebibliography}{00}
% \bibitem{b1} G. Eason, B. Noble, and I. N. Sneddon, ``On certain integrals of Lipschitz-Hankel type involving products of Bessel functions,'' Phil. Trans. Roy. Soc. London, vol. A247, pp. 529--551, April 1955.
% \bibitem{b2} J. Clerk Maxwell, A Treatise on Electricity and Magnetism, 3rd ed., vol. 2. Oxford: Clarendon, 1892, pp.68--73.
% \bibitem{b3} I. S. Jacobs and C. P. Bean, ``Fine particles, thin films and exchange anisotropy,'' in Magnetism, vol. III, G. T. Rado and H. Suhl, Eds. New York: Academic, 1963, pp. 271--350.
% \bibitem{b4} K. Elissa, ``Title of paper if known,'' unpublished.
% \bibitem{b5} R. Nicole, ``Title of paper with only first word capitalized,'' J. Name Stand. Abbrev., in press.
% \bibitem{b6} Y. Yorozu, M. Hirano, K. Oka, and Y. Tagawa, ``Electron spectroscopy studies on magneto-optical media and plastic substrate interface,'' IEEE Transl. J. Magn. Japan, vol. 2, pp. 740--741, August 1987 [Digests 9th Annual Conf. Magnetics Japan, p. 301, 1982].
% \bibitem{b7} M. Young, The Technical Writer's Handbook. Mill Valley, CA: University Science, 1989.
% \end{thebibliography}
% \vspace{12pt}
% \color{red}
% IEEE conference templates contain guidance text for composing and formatting conference papers. Please ensure that all template text is removed from your conference paper prior to submission to the conference. Failure to remove the template text from your paper may result in your paper not being published.

\end{document}
