\section{Evaluation}
\label{sec:results}

This section contains the results of the conducted experiments.
First, we introduce the used knowledge sources -- semantic similarity graph, WordNet and Wikidata.
Then, we describe the evaluation of KGTN-ens with different combinations of embeddings and compare them with the previous work.
Finally, we provide a detailed analysis and ablation studies.

\subsection{Knowledge sources}
\label{sec:results_knowledge}

In our evaluation, we use three different sources of knowledge, which can be the backbone of KGTMs: \emph{hierarchy}, \emph{glove}, and \emph{wiki}.
The first two have been proposed by \cite{chen2020knowledge}.
The wiki graph is constructed on top of Wikidata, a collaborative knowledge graph connected to Wikipedia \citep{vrandevcic2014wikidata}.
In this subsection, we discuss the preparation of these knowledge sources in detail.

\textbf{Semantic similarity graph (\emph{glove}).}
The first source of knowledge is built from GLoVe word embeddings \citep{pennington2014glove}.
For two words $w_i$ and $w_j$, their semantic distance $d_{i,j}$ is defined as the Euclidean distance between their GLoVe embeddings $\mathbf{f}_i^w$ and $\mathbf{f}_j^w$.
Following \cite{chen2020knowledge}, the final correlation coefficient $a_{i,j}$ is obtained using the following function:
\begin{equation}
    \lambda^{d_{i,j} - \min d_{i,k}, \forall k \neq i},
    \label{eq:monotonic_function}
\end{equation}
where $\lambda=0.4$ and $a_{ii}=1$.

\textbf{WordNet category distance (\emph{hierarchy}).}
This source of knowledge is built from the WordNet hierarchy -- a popular lexical database of English \citep{miller1995wordnet}.
Since ImageNet classes are based on WordNet, the WordNet hierarchy can be used to measure the distance between two classes.
This time the distance $d_{i,j}$ is defined as the number of common ancestors of the two words (categories) $w_i$ and $w_j$.
The output is processed similarly to Equation \eqref{eq:monotonic_function}, except that the $\lambda$ parameter is set to 0.5.

\textbf{Wikidata embeddings (\emph{wiki}).}
The last source of knowledge is built from the Wikidata embeddings.
The mapping between the ImageNet classes and Wikidata is provided by \cite{filipiak2021mapping}.
Having the mapping, the class-corresponding entities from Wikidata can be embedded and used as a class prototypes.
Although there exist some datasets of Wikidata embeddings, they are often incomplete.
Most importantly, they does not contain all the embeddings of ImageNet classes.
Wembedder \citep{nielsen2017wembedder} offers 100-dimensional Wikidata embeddings made using the word2vec algorithm \citep{mikolov2013efficient}, but it bases on an incomplete dump of Wikidata and does not contain all the classes nedded in the ImageNet-FS dataset.
\cite{zhu2019graphvite} proposed Graphvite, a general graph embedding engine.
\textit{Wikidata5m} is a large dataset of 5 million Wikidata entities, which is used to train the embeddings.
The framework comes with embeddings created using numerous popular algorithms, such as TransE, DistMult, ComplEx, SimplE, RotatE, and QuatE.
However, 891 out of 1000 entities used in the ImageNet are embedded, which was not enough for performing the experiment.

We used the pre-trained 200-dimensional embeddings of Wikidata entities from PyTorch BigGraph \citep{lerer2019pytorch}, which are publicly available\footnote{\url{https://torchbiggraph.readthedocs.io/en/latest/pretrained_embeddings.html}}.
The embeddings were prepared using the full Wikidata dump from 2019-03-06.
All but three entities were directly mapped to embeddings to their Wikidata ID.
Three entities (\texttt{Q1295201}, \texttt{Q98957255}, \texttt{Q89579852}) could not be instantly matched -- they were manually matched to \texttt{"grocery store"@en}, \texttt{"cricket"@en}, and \texttt{Q655301} respectively.
Having the mapping, now we create an embedding array, ordered as the mappings in the original KGTN paper (that is, as a $1000 \times 200$ array, where 200 denotes the dimensionality of a single embedding).
The same function from Equation~\eqref{eq:monotonic_function} was used to generate final correlations between the embeddings, although this time $\lambda=0.32$ was used (see Section \ref{sec:details_ablation}).

\subsection{Experiment results}
\label{sec:results_experiment}

In this subsection, we present the results of the conducted experiments.
We describe the evaluation data -- the experiment has been conducted on ImageNet-FS dataset.
The training hyperparameters and the setup is also described.
We also describe the evaluation protocol, as well as the evaluation metrics.
Finally, we present the results of the experiments and compare them with the previous work.

\textbf{Data.}
Similarly to Chen et al., our approach has been evaluated on ImageNet-FS, a popular benchmark for few-shot learning task.
ImageNet-FS contains 1,000 classes from ImageNet Large Scale Visual Recognition Challenge 2012 \citep{ILSVRC15}, of which 389 belongs to the base category and 611 to the novel category.
193 base classes and 300 novel classes are used for training and cross-validation, whereas the test phase is performed on the remaining 196 categories and 311 novel classes.
Base categories consist of around 1280 train and 50 test images per each class.
The authors of KGTN also evaluated their solution against a larger dataset, ImageNet-6K, which contains 6,000 classes (of which 1,00 belongs to the novel category).
Unfortunately, we were unable to test KGTN-ens using this dataset, since it has not been made public nor available to us at the time of writing this paper.

\textbf{Training.}
To enable fair comparison, we used the same 2-step training and evaluation procedures as in KGTN.
Stochastic gradient descent (SGD) was used to train the model with a batch size equal to 256 (divided equally for base and novel classes), a momentum of 0.9, and a weight decay of 0.0005.
The learning rate is initially set at 0.1 and divided by 30 at every 30 epochs.
In general, we used the same hyperparameters as in KGTN unless stated otherwise.

\textbf{Setup.}
All the experiments have been conducted on a single NVIDIA GeForce RTX 2080 Ti GPU.
We used the code released by the authors of KGTN and modified it to support the KGTN-ens approach.
PyTorch \cite{paszke2017automatic} was used to conduct the experiments.
The code will be released after the publication of this article.

\textbf{Evaluation.}
Following previous work in few-shot learning, we report our evaluation results in terms of the top-5 accuracy of novel and all (base + novel) classes in the $k$-shot learning task, where $k \in \{1,2,5, 10\}$ is the number of classes in the novel category.
Following \cite{hariharan2017low} and \cite{chen2020knowledge}, we repeat each experiment five times and report the averaged values of the top-5 accuracy.
Table~\ref{tab:results} shows the classification results compared with some of the recent state-of-the-art benchmarks.
Figure \ref{fig:kgtn-ens_performance} presents the top-5 accuracy of the KGTN-ens model on ImageNet-FS.
Of three possible combinations of the three sources of knowledge, the KGTN-ens model performed best with the combination of \emph{hierarchy} and \emph{glove}.
Notably, it performed better than KGTN with these two sources of knowledge alone.
Compared to KGTN (with inner product similarity and glove embeddings), the KGTN-ens model (inner product, max ens. function, glove and hierarchy embeddings) achieved +0.63, +0.58, +0.43, +0.26 pp. top-5 accuracy on novel classes for $k \in \{ 1, 2, 5, 10\}$ respectively.
The smaller the $k$, the higher the performance gain.
It also beats the more recent graph-based framework proposed by \cite{shen2021model} by +1.73/+1.18/+0.20 pp. top-5 accuracy on novel classes.
For the all classes, the KGTN-ens model achieved +0.26, +0.25, +0.32, --0.04 pp. top-5 accuracy compared to the same KGTN model for $k \in \{ 1, 2, 5, 10\}$ respectively.

\begin{table*}
    \centering
    \caption{Top-5 accuracy on \emph{novel} and \emph{all} subsets on ImageNet-FS. All the methods used ResNet-50 as a feature extractor. Partially based on the data provided by \cite{chen2020knowledge}.}
    \label{tab:results}
    \begin{tabular}{l rrrr rrrr}
    \toprule
     & \multicolumn{4}{c}{novel} & \multicolumn{4}{c}{all} \\ \cmidrule(lr){2-5} \cmidrule(lr){6-9}
     & \multicolumn{1}{c}{1} & \multicolumn{1}{c}{2} & \multicolumn{1}{c}{5} & \multicolumn{1}{c}{10} & \multicolumn{1}{c}{1} & \multicolumn{1}{c}{2} & \multicolumn{1}{c}{5} & \multicolumn{1}{c}{10} \\
    \midrule
    MN \cite{vinyals2016matching} & 53.5 & 63.5 & 72.7 & 77.4 & 64.9 & 71.0 & 77.0 & 80.2 \\
    PN \cite{snell2017prototypical} & 49.6 & 64.0 & 74.4 & 78.1 & 61.4 & 71.4 & 78.0 & 80.0 \\
    SGM \cite{hariharan2017low} & 54.3 & 67.0 & 77.4 & 81.9 & 60.7 & 71.6 & 80.2 &  83.6 \\
    SGM w/ G \cite{hariharan2017low} & 52.9 & 64.9 & 77.3 & 82.0 & 63.9 & 71.9 & 80.2 & 83.6 \\
    AWG \cite{gidaris2018dynamic} & 53.9 & 65.5 & 75.9 & 80.3 & 65.1 & 72.3 & 79.1 & 82.1 \\
    PMN \cite{wang2018low} & 53.3 & 65.2 & 75.9 & 80.1 & 64.8 & 72.1 & 78.8 & 81.7 \\
    PMN w/ G \cite{wang2018low} & 54.7 & 66.8 & 77.4 & 81.4 & 65.7 & 73.5 & 80.2 & 82.8 \\
    LSD \cite{douze2018low} & 57.7 & 66.9 & 73.8 & 77.6 & -- & -- & -- & -- \\
    KTCH \cite{li2019large} & 58.1 & 67.3 & 77.6 & 81.8 & -- & -- & -- & -- \\
    IDeMe-Net \cite{chen2019image} & 60.1 & 69.6 & 77.4 & 80.2 & -- & -- & -- & -- \\
    KGTN-CosSim \cite{chen2020knowledge} & 61.4 & 70.4 & 78.4 & 82.2 &  67.7 & 74.7 & 80.9 & \textbf{83.6} \\
    KGTN-PearsonCorr \cite{chen2020knowledge} & 61.5 & 70.6 & 78.5 & 82.3 & 67.5 & 74.4 & 80.7 & 83.5 \\
    KGTN-InnerProduct \cite{chen2020knowledge} & 62.1 & 70.9 & 78.4 & 82.3 & 68.3 & 75.2 & 80.8 & 83.5 \\
    SGM with graph regularisation \cite{shen2021model} & 61.1 & 70.3 & 78.6 & -- & -- & -- & -- & -- \\
    KGTN-ens (ours) & \textbf{62.73} & \textbf{71.48} & \textbf{78.83} & \textbf{82.56} & \textbf{68.58} & \textbf{75.45} & \textbf{81.12} & 83.46 \\
    \bottomrule
    \end{tabular}    
\end{table*}

\begin{figure}
    \centering    
    \begin{subfigure}[b]{.5\textwidth}
        \centering
        \input{figures/max_novel.tex}
        % \caption{Subset: novell}
        \label{fig:max_novel}
    \end{subfigure}  
    % \hfill
    % \begin{subfigure}[b]{.5\textwidth}
    %     \centering
    %     \input{figures/max_all.tex}
    %     % \caption{Subset: all}
    %     \label{fig:max_all}
    % \end{subfigure}
    \caption{KNGT-ens (blue) performance mean top-5 accuracy with compared to the KGTN (orange) over 5 runs. KGNT-ens uses glove and hierarchy graphs combined with the max ensembling function. Horizontal lines indicate standard deviations. }
    \label{fig:kgtn-ens_performance}
\end{figure}

\subsection{Details and ablation studies}
\label{sec:details_ablation}

This subsection provides more details on the KGTN-ens model and its ablation studies.
We analyse the impact of the following factors on the performance of the KGTN-ens model: adjacency matrices, used embeddings, ensembling method, similarity function, and variance of the results.

\textbf{Adjacency matrix analysis.}
Since glove knowledge graph was the most effective for KGTN, we assume that wiki should roughly resemble it in terms of its distribution.
In order to investigate the similarity between distributions, adjacency matrices have been created using pairwise euclidean distances.
While glove and wiki are normal-like, the distribution for hierarchy is bimodal and most of the distances are the highest ones (Figure \ref{fig:ablation_dist}).
To assess the correlation between adjacency matrices, Mantel tests have been performed (Table \ref{tab:mantel_raw}).
The values marked as processed were run through Equation \eqref{eq:monotonic_function}.
Correlations of the processed matrices are visibly higher compared to raw ones, especially regarding glove and wiki).
The highest correlation has been observed between glove and wiki.

\begin{table}
    \centering
    \caption{Descriptive statistics about the adjacency matrices.}
    \label{tab:ablation_adjacency_stats}
    \begin{tabular}{ll rrrr}
    \toprule
    & KG & Min & Avg & Max & Std \\
    \midrule
    \multirow{3}{*}{Raw} & hierarchy & 0.00 & 9.76 & 10.00 & 1.20 \\
    & glove & 0.00 & 8.52 & 14.31 & 1.29 \\
    & wiki & 0.00 & 5.82 & 12.73 & 1.32 \\ \cmidrule(){2-6}
    \multirow{3}{*}{Processed} & hierachy   & 0.00 & 0.01 & 2.00 & 0.07 \\
    & glove      & 0.00 & 0.05 & 2.00 & 0.11 \\
    & wiki       & 0.00 & 0.08 & 2.00 & 0.14 \\ \bottomrule
\end{tabular}
\end{table}

\begin{figure}%
    % \centering
    \begin{subfigure}{.5\textwidth}
        \centering
        \label{fig:ablation_dist_raw}
        \input{figures/dist_raw.tex}
        \caption{Raw values}
    \end{subfigure}
    % \vskip\baselineskip
    \begin{subfigure}{.5\textwidth}
        \centering
        \label{fig:ablation_dist_processed}
        \input{figures/dist_processed.tex}
        \caption{Processed values}
    \end{subfigure}  
    \caption{Adjacency matrix distributions}
    \label{fig:ablation_dist}
\end{figure}

\begin{table}
    \caption{Mantel test results.}
    \label{tab:mantel_raw}
    \begin{tabular}{lll rr}
        \toprule
        & KG$_{1}$ & KG$_{1}$ & correlation & p-value \\ \midrule
        \multirow{3}{*}{Raw} & hierachy   & glove      & 0.14 & 0.001 \\
        & hierachy   & wiki       & 0.13 & 0.001 \\
        & glove      & wiki       & 0.16 & 0.001 \\ \cmidrule(){2-5}
        \multirow{3}{*}{Processed} & hierachy   & glove & 0.19 & 0.001 \\
        & hierachy   & wiki       & 0.18 & 0.001 \\
        & glove      & wiki       & 0.44 & 0.001 \\ \bottomrule
    \end{tabular}
\end{table}

\textbf{Importance of used knowledge graphs.}
Firstly, we analyse the influence of the used KGs separately (without ensembling) -- that is, with the original KGTN architecture.
Table \ref{tab:ablation_kgtn_graphs_and_functions} shows the results of the ablation studies on the three knowledge graphs.
The hierarchy and glove knowledge graphs are the ones examined by \cite{chen2020knowledge}, whereas the wiki knowledge graph is the one introduced in our experiments.
In order to ensure that the advantage comes from the knowledge encoded in KGs, Chen et al. argue that glove and hierarchy embeddings perform better than uniform (all correlations set to $1/K$) and random (correlations drawn from the uniform distributions) distance matrices.
Similarly, the usage of wiki knowledge graph yielded generally better results (up to +3.44 pp for 1-shot in the novel category) compared to random and uniform cases, which constitutes a noticeable improvement.
However, compared to glove and hierarchy, the wiki knowledge graph yields worse results -- notably for low-shot scenarios.
We hypothesise that the difference in the performance of wiki knowledge graph is due to the low quality of embeddings, as some issues regarding their accuracy were previously reported\footnote{\url{https://datascience.stackexchange.com/q/95007/8949}}.

% Hyperparams
% GGNN coefficient 0.32
% GGNN time step 2

\begin{table*}
    \caption{Classification results for KGTN on different embeddings tested against different similarity functions. Results for glove and hierarchy embeddings are provided by \cite{chen2020knowledge}. Bold results mean the best ones for the wiki graph only.}
    \label{tab:ablation_kgtn_graphs_and_functions}
    \begin{tabular}{ll rrrr rrrr}
        \toprule
        & & \multicolumn{4}{c}{novel} & \multicolumn{4}{c}{all} \\ \cmidrule(lr){3-6} \cmidrule(lr){7-10}
        knowledge graph & similarity function & \multicolumn{1}{c}{1} & \multicolumn{1}{c}{2} & \multicolumn{1}{c}{5} & \multicolumn{1}{c}{10} & \multicolumn{1}{c}{1} & \multicolumn{1}{c}{2} & \multicolumn{1}{c}{5} & \multicolumn{1}{c}{10} \\
        \midrule
        \multirow{3}{*}{wiki} & cosine similarity   &  56.65 &  \textbf{68.21} &  77.31 &  81.88 &  64.59 &  73.32 &  80.03 &  \textbf{83.44} \\
        & inner product &  55.55 &  67.81 &  \textbf{77.99} &  \textbf{82.15} &  \textbf{64.61} &  \textbf{73.28} &  \textbf{80.55} &  83.22 \\
        & Pearson correlation &  \textbf{56.84} &  68.10 &  77.03 &  81.62 &  64.03 &  72.61 &  79.53 &  83.20 \\ \cmidrule(lr){2-10}
        \multirow{3}{*}{glove} & cosine similarity & 61.4 & 70.4 & 78.4 & 82.2 &  67.7 & 74.7 & 80.9 & 83.6 \\
        & inner product & 62.1 & 70.9 & 78.4 & 82.3 & 68.3 & 75.2 & 80.8 & 83.5 \\
        & Pearson correlation & 61.5 & 70.6 & 78.5 & 82.3 & 67.5 & 74.4 & 80.7 & 83.5 \\ \cmidrule(lr){2-10}
        \multirow{1}{*}{hierarchy} & inner product & 60.1 & 69.4 & 78.1 & 82.1 & 67.0 & 74.4 & 80.7 & 83.3\\ \cmidrule(lr){2-10}
        \multirow{1}{*}{(uniform)} & inner product & 53.4 & 67.4 & 78.8 & 81.5 & 63.8 & 73.3 & 80.3 & 82.9 \\ \cmidrule(lr){2-10}
        \multirow{1}{*}{(random)} & inner product & 54.4 & 67.4 & 77.8 & 81.9 & 64.5 & 73.3 & 80.5 & 83.2 \\ 
        %   KGTN\_InnerProduct &  61.96 &  71.08 &  78.53 &  82.48 & 68.34 &  75.27 &  80.92 &  83.41 \\
    \bottomrule
    \end{tabular}
\end{table*}


\textbf{Importance of the ensembling method.}
Table \ref{tab:results_ensembling} presents results for the different ensembling strategies compared to the KGTN baseline, which can be treated as a KGTN-ens model with no ensembling.
Mean ensembling gave mixed results compared to the baseline ($+0.34$, $-0.63$, $+0.36$, $-0.27$ pp. for novel classes and $-1.45$, $-1.41$, $+0.14$, $-0.17$ pp. for all classes, both groups for $k \in \{1, 2, 5, 10\}$ respectively.
However, using the max ensembling strategy has been better in all the cases ($+0.77$, $+0.40$, $+0.29$, $+0.07$ pp. for novel classes and $+0.24$, $+0.18$, $+0.19$, $+0.06$ pp. for all classes).
A possible explanation of this effect might stem from the \emph{winner takes all} nature of the maximum function, which chooses the most similar embedding to the given prototype and rejects other, potentially improper, embeddings.
At the same time, these improper embeddings still contribute to the overall formula for the mean ensembling function.
However, research on a larger number of employed knowledge graphs has to be conducted to validate this hypothesis.

\textbf{Variance of the results.}
Contrary to expectations, adding additional knowledge sources slightly increase the variance of the results in most cases (Table \ref{tab:stddev}).
A possible explanation of these results is the fact that KGTN-ens is not an ensembling technique in the typical sense of this word, but rather a way of choosing the embeddings of the different knowledge sources.
We report results for novel classes only, as the difference in variance is amplified among these (see also Fig. \ref{fig:kgtn-ens_performance}).
No significant differences in the variance of mean and max ensembling have been found.
The variance of the results for baseline KGTN has been obtained using five runs of the original KGTN with glove embeddings.

\begin{table}
    \centering
    \caption{Standard deviations of top-5 accuracy. Abbreviations: h -- hierarchy, g -- glove, w -- wiki.}
    \label{tab:stddev}
    \begin{tabular}{llrrrr}
    \toprule
     & & \multicolumn{4}{c}{novel} \\
    type & KG & \multicolumn{1}{c}{1} & \multicolumn{1}{c}{2} & \multicolumn{1}{c}{5} & \multicolumn{1}{c}{10} \\
    \midrule
    KGTN (baseline) & g & 0.40 & 0.38 & 0.28 & 0.40 \\ \cmidrule(lr){2-6}
    \multirow{4}{*}{KGTN-ens (max)} & h+g & 0.53 & 0.33 & 0.32 & 0.30 \\
          & w+g & 0.59 & 0.16 & 0.30 & 0.34 \\
          & w+h & 0.66 & 0.25 & 0.26 & 0.28 \\
          & w+h+g & 0.45 & 0.27 & 0.34 & 0.30 \\  \cmidrule(lr){2-6}
    \multirow{4}{*}{KGTN-ens (mean)} & h+g & 0.57 & 0.31 & 0.31 & 0.34 \\
        & w+g & 0.24 & 0.28 & 0.33 & 0.38 \\
        & w+h & 0.56 & 0.31 & 0.41 & 0.31 \\
        & w+h+g & 0.53 & 0.37 & 0.33 & 0.42 \\
    \bottomrule
    \end{tabular}
    \end{table}

\textbf{Importance of similarity function.}
Table \ref{tab:ablation_kgtn_graphs_and_functions} includes data for performing ablative studies for KGTN with the three different similarity functions: cosine similarity, inner product and Pearson correlation.
\cite[]{chen2020knowledge} analysed all these for KGTN with glove embeddings.
In general, the inner product showed the best performance.
These conclusions can be extrapolated to the wiki graph, as the inner product usually turned out to be the most effective in terms of the top-5 accuracy.
Interestingly, Pearson correlation displayed the best performance for the 1-shot scenario with novel classes.
Table \ref{tab:results_cosine} presents results for the different similarity functions used in the KGTN-ens.
While the combination of hierarchy and glove embeddings was usually the best for cosine similarity as well, the results are visibly worse compared to the inner product similarity function (e.g. $-3.16$ pp. top-5 accuracy difference for 1-shot scenario among novel classes).
Noticeably, the combination of these two graphs and cosine similarity function performed worse than KGTN solely based on glove embeddings (for example, there is a $-2.39$ pp. difference for top-5 accuracy difference for 1-shot scenario among novel classes).

\begin{table*}
    \centering
    \caption{Knowledge graph ensembling (sum), top-5 accuracy, inner product}
    \label{tab:results_ensembling}
    \begin{tabular}{llrrrr rrrr}
    \toprule
    \multicolumn{2}{l}{} & \multicolumn{4}{c}{novel} & \multicolumn{4}{c}{all} \\ \cmidrule(lr){3-6} \cmidrule(lr){7-10}
    \multicolumn{1}{c}{type} & \multicolumn{1}{c}{knowledge graphs} & \multicolumn{1}{c}{1} & \multicolumn{1}{c}{2} & \multicolumn{1}{c}{5} & \multicolumn{1}{c}{10} & \multicolumn{1}{c}{1} & \multicolumn{1}{c}{2} & \multicolumn{1}{c}{5} & \multicolumn{1}{c}{10} \\
    \midrule
    KGTN (baseline) & glove & 61.96 & 71.08 & 78.53 & 82.48 & 68.34 & 75.27 & 80.92 & 83.40   \\ \cmidrule(lr){2-10}
    \multirow{4}{*}{KGTN-ens (mean)} & hierarchy + glove & 62.30 & 70.45 & \textbf{78.90} & 82.21 & 66.89 & 73.86 & 81.06 & 83.22   \\
       & wiki + glove & 60.41 & 69.41 & 78.81 & 82.10 & 66.07 & 73.30 & 81.01 & 83.15   \\
       & wiki + hierarchy + glove & 58.74 & 67.95 & 78.74 & 81.70 & 63.90 & 71.43 & 80.91 & 82.88   \\
       & wiki + hierarchy & 57.89 & 67.49 & 78.49 & 81.91 & 64.10 & 71.83 & 80.80 & 83.04   \\ \cmidrule(lr){2-10}
       \multirow{4}{*}{KGTN-ens (max)} & hierarchy + glove & \textbf{62.73} & \textbf{71.48} & 78.83 & \textbf{82.56} & \textbf{68.58} & \textbf{75.45} & \textbf{81.12} & \textbf{83.46}  \\
       & wiki + glove & 61.21 & 70.66 & 78.60 & 82.34 & 67.69 & 75.04 & 80.95 & 83.33  \\
       & wiki + hierarchy + glove & 61.32 & 70.77 & 78.70 & 82.38 & 67.85 & 75.06 & 81.06 & 83.35  \\
       & wiki + hierarchy & 58.77 & 69.17 & 78.44 & 82.25 & 66.17 & 74.01 & 80.86 & 83.26  \\
    \bottomrule
    \end{tabular}
\end{table*}

% \begin{figure}
%     \centering
%     \begin{subfigure}[b]{.5\textwidth}
%         \centering
%         \input{figures/mean_all.tex}
%         % \caption{Subset: all}
%         \label{fig:mean_all}
%     \end{subfigure}
%     \hfill
%     \begin{subfigure}[b]{.5\textwidth}
%         \centering
%         \input{figures/mean_novel.tex}
%         % \caption{Subset: novell}
%         \label{fig:mean_novel}
%     \end{subfigure}
%     \caption{KNGT-ens mean}
%     \label{fig:three graphs}
% \end{figure}

\begin{table*}
    \centering
    \caption{Ablation on different similarity functions used in KGTN-ens, top-5 accuracy.}
    \label{tab:results_cosine}
    \begin{tabular}{llrrrrrrrr}
    \toprule
    \multicolumn{2}{l}{} & \multicolumn{4}{c}{novel} & \multicolumn{4}{c}{all} \\ \cmidrule(lr){3-6} \cmidrule(lr){7-10}
    \multicolumn{1}{c}{type} & \multicolumn{1}{c}{knowledge graphs} & \multicolumn{1}{c}{1} & \multicolumn{1}{c}{2} & \multicolumn{1}{c}{5} & \multicolumn{1}{c}{10} & \multicolumn{1}{c}{1} & \multicolumn{1}{c}{2} & \multicolumn{1}{c}{5} & \multicolumn{1}{c}{10} \\
    
    \midrule
       KGTN &                      glove & 61.96 & 71.08 & 78.53 & 82.48 & 68.34 & 75.27 & 80.92 & 83.40 \\\cmidrule(lr){2-10}
       \multirow{4}{*}{KGTN-ens (inner prod.)} & hierarchy + glove & \textbf{62.73} & \textbf{71.48} & \textbf{78.83} & \textbf{82.56} & \textbf{68.58} & \textbf{75.45} & \textbf{81.12} & \textbf{83.46}  \\
       & wiki + glove & 61.21 & 70.66 & 78.60 & 82.34 & 67.69 & 75.04 & 80.95 & 83.33  \\
       & wiki + hierarchy + glove & 61.32 & 70.77 & 78.70 & 82.38 & 67.85 & 75.06 & 81.06 & 83.35  \\
       & wiki + hierarchy & 58.77 & 69.17 & 78.44 & 82.25 & 66.17 & 74.01 & 80.86 & 83.26  \\\cmidrule(lr){2-10}
       \multirow{4}{*}{KGTN-ens (cosine sim.)} &       hierarchy + glove & 59.57 & 69.40 & 77.29 & 81.89 & 64.86 & 73.72 & 80.05 & \textbf{83.46} \\
        &            wiki + glove & 58.34 & 68.75 & 77.24 & 81.84 & 64.43 & 73.44 & 80.01 & 83.38 \\
        &  wiki + hierarchy + glove & 57.75 & 68.44 & 77.20 & 81.90& 63.81 & 73.12 & 79.99 & 83.44 \\
        &        wiki + hierarchy & 57.35 & 68.50 & 77.27 & 81.90  & 63.87 & 73.23 & 80.00 & 83.43 \\

    \bottomrule
    \end{tabular}
    \end{table*}
    