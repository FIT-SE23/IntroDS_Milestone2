% Template for ICASSP-2021 paper; to be used with:
%          spconf.sty  - ICASSP/ICIP LaTeX style file, and
%          IEEEbib.bst - IEEE bibliography style file.
% --------------------------------------------------------------------------
\documentclass{article}
\usepackage{spconf}
\usepackage{amsmath,graphicx}
\usepackage{comment}
\usepackage{amsthm, amssymb, tikz, ifthen, xspace, bm, ulem}
\usepackage[hidelinks]{hyperref}
\usepackage{dsfont}
\usepackage{booktabs}
\usepackage{subfig}
\usepackage[demo,export]{adjustbox}
\usepackage{mathtools}
\usepackage{multirow}

% Example definitions.
% --------------------
\def\x{{\mathbf x}}
\def\L{{\cal L}}
\newcommand\blfootnote[1]{%
  \begingroup
  \renewcommand\thefootnote{}\footnote{#1}%
  \addtocounter{footnote}{-1}%
  \endgroup
}
\newcommand{\N}{\mathbb{N}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\Q}{\mathbb{Q}}
\newcommand{\R}{\mathbb{R}}	
\newcommand{\C}{\mathbb{C}}

\theoremstyle{plain}
\newtheorem{thm}{Theorem}[section]
\newtheorem{cor}[thm]{Corollary}
\newtheorem{lemma}[thm]{Lemma}
\newtheorem{prop}[thm]{Proposition}
\newtheorem*{theorem*}{Theorem}

\theoremstyle{definition}
\newtheorem{defn}[thm]{Definition}
\newtheorem{example}[thm]{Example}
%\frenchspacing

\input{my_symbol.sty}

%\linespread{0.95}

% Title.
% ------
\title{Graph Neural Networks for Community Detection on Sparse Graphs}
%
% Single address.
% ---------------
%\name{Luana Ruiz, Ningyuan (Teresa) Huang, Soledad Villar\thanks{SV is supported by NSF DMS 2044349, EOARD FA9550-18-1-7007, and the NSF-Simons Research Collaboration on the Mathematical and Scientific Foundations of Deep Learning (MoDL) (NSF DMS 2031985). The authors thank Zhengdao Chen for motivating this note and pointing out relevant references; Haoteng Yin for pointing out an error in a previous version of this manuscript; and David W. Hogg for writing advice.}}

%
% For example:
% ------------
%\address{School\\
%	Department\\
%	Address}
%
% Two addresses (uncomment and modify for two-address case).
% ----------------------------------------------------------
\name{Luana Ruiz$^*$, Ningyuan (Teresa) Huang$^\dagger$, Soledad Villar$^\dagger$}
\address{}

\begin{document}
\ninept
\maketitle
\begin{abstract}
Spectral methods provide consistent estimators for community detection in dense graphs. However, their performance deteriorates as the graphs become sparser. In this work we consider a random graph model that can produce graphs at different levels of sparsity, and we show that graph neural networks can outperform spectral methods on sparse graphs. We illustrate the results with numerical examples in both synthetic and real graphs.
\end{abstract}
\begin{keywords}
Graph neural networks, community detection, spectral embedding
\end{keywords}
%
%%%%%%%%%%%%%%%%%
%%%% SECTION %%%%
%%%%%%%%%%%%%%%%%
\section{Introduction} \label{sec:intro}

\blfootnote{
$^*$LR is with the Simons-Berkeley Inst. and is supported by a Simons Research Fellowship.. $^\dagger$NH and SV are with the AMS Dept.\ and MINDS at Johns Hopkins University. SV is partially supported by ONR N00014-22-1-2126, NSF CISE 2212457, an AI2AI Amazon research award, and the NSFâ€“Simons Research Collaboration on the Mathematical and Scientific Foundations of Deep Learning (MoDL) (NSF DMS 2031985).
}Community detection is a fundamental problem in network science and highly relevant to graph signal processing \cite{wai2020blind,navarro2022graphon,ortega2018graph}. Community in a graph refers to a group of nodes that are similar in terms of their connectivity structure and their attributes. Detecting communities reveals important graph structures which can be exploited in a variety of applications, including human neuroimaging \cite{petrovic2020community}, network protocol design \cite{lu2015algorithms}, and social networks \cite{bedi2016community}. %(e.g., understanding sociological behavior, modeling molecular interactions). 
Numerous approaches have been proposed for community detection, including graph neural networks (GNNs) \cite{chen2018supervised}. See, e.g., \cite{schaub2017many, su2022comprehensive} for a survey and references therein. 

In this paper, we focus on supervised community detection with GNNs. We compare GNNs with spectral embeddings, a class of established statistical methods. Spectral embeddings (SEs) have nice theoretical guarantees in random graph models, but can be computationally intensive in large graphs and brittle in sparse graphs \cite{abbe2017community}. On the other hand, GNNs are deep convolutional architectures for graph data \cite{ruiz2020gnns,kipf17-classifgcnn} enjoying desirable mathematical properties such as stability \cite{gama19-stability} and transferability \cite{ruiz2021transferability}, and showing remarkable empirical performance in a variety of problems on large-scale, sparse graphs \cite{tolstaya2020learning,eisen2019optimal,bronstein17-geomdeep}.

Motivated by these observations, we seek out to understand the theoretical underpinnings of the differences in behavior observed between GNNs and SEs in community detection on dense and sparse graphs. To this end, we propose a random graph model that can generate graphs with variable sparsity (Def. \ref{def:graphex}). We then use it to prove that SEs degrade with graph sparsity (Thm. \ref{thm:comm_conc}), and to explain why GNNs perform consistently well in sparse graphs (Thm. \ref{thm:power}). These findings are further demonstrated empirically through numerical experiments on both synthetic and real-world graphs (Sec. \ref{sec:exp}). 

%%%%%%%%%%%%%%%%%
%%%% SECTION %%%%
%%%%%%%%%%%%%%%%%
\section{Preliminary Definitions} \label{sec:prelim}

A graph $\bbG$ is a triplet $\bbG=(\ccalV,\ccalE,\ccalW)$ where $\ccalV=\{1,\ldots,N\}$ is the node set, $\ccalE \subseteq \ccalV \times \ccalV$ the edge set, and $\ccalW: \ccalE \to \reals$ a function assigning edge weights. We focus on unweighted, undirected and connected graphs $\bbG$, so that $\ccalW:\ccalE \to \{0,1\}$, $\ccalW(i,j)=\ccalW(j,i)$ for all $i,j$ and there is a single connected component. We represent the graph $\bbG$ by its adjacency matrix $\bbA \in \reals^{N \times N}$, defined as $[\bbA]_{ij} = \ccalW(i,j)$ if $(i,j) \in \ccalE$ and $0$ otherwise. Since $\bbA$ is symmetric, it can be diagonalized as $\bbA = \bbV\bbLam\bbV^\top$. The diagonal elements of $\bbLam$ are the eigenvalues $\lambda_i \in \R$, $|\lambda_1 | \geq \ldots \geq |\lambda_N|$, and the columns of $\bbV$ the corresponding eigenvectors $\bbv_i$, $1 \leq i \leq N$.  

We assume that the nodes of $\bbG$ can carry data, which is represented in the form of \textit{graph signals} \cite{ortega2018graph,sandryhaila13-dspg}. A graph signal is a vector $\bbx \in \reals^N$ where $[\bbx]_i$ is the value of the signal of the node $i$. More generally, graphs can also carry $D$-dimensional signals $\bbX \in \reals^{N \times D}$, where each column of $\bbX$, denoted $\bbx^d$, is a \textit{node feature}.

Community detection on $\bbG$ consists of clustering nodes $i \in \ccalV$ into $K$ communities. 
The goal of community detection is thus to obtain a graph signal $\bbY \in [0,1]^{N \times K}$ where each row $[\bbY]_{i\cdot}$ represents the \textit{community assignment} of node $i$ (potentially overlapping \cite{xie2013overlapping}). In this paper, we assume non-overlapping communities, so that $[\bbY]_{i\cdot}=\texttt{one-hot}(k)$ (i.e., $[\bbY]_{ij}=1$ for $j=k$ and $0$ for $j \neq k$) implies that node $i$ is in community $k$. 

There are many variants of community detection \cite{abbe2017community}. For example, the number of communities $K$ may or may not be predefined \cite{choi2012stochastic}, and the problem can be solved in an unsupervised or supervised manner \cite{cai2020weighted}. In this paper, we assume that $K$ is given and solve the problem with supervision.
Formally, given a graph $\bbG$ and a signal $\bbX$, and a true community assignment matrix $\bbY$, we fix a training set consisting of a subset $\ccalT = \{i_1, \ldots, i_M\} \subset \ccalV$ of the graph nodes. This training set is used to define a node selection matrix $\bbM_\ccalT \in \{0,1\}^{M\times N}$ where $[\bbM_\ccalT]_{ij}=1$ only for $i=m$, $j=i_m$, and the masked input signal $\bbX_\ccalT \in \reals^{N \times D}$ where $[\bbX_\ccalT]_{i \cdot} = [\bbX]_{i \cdot}$ for $i \in \ccalT$ and $0$ otherwise. We then use $\ccalT$ to solve the following optimization problem
\begin{equation} \label{eqn:erm}
    \min_f \ell(\bbM_\ccalT\bbY,\bbM_\ccalT f(\bbA,\bbX_\ccalT))
\end{equation}
where $\ell:\reals^{M\times K} \times \reals^{M \times K} \to \reals$ is a loss and $f: \reals^{N \times N} \times \reals^{N \times D} \to \reals^{N\times K}$ is a parametric function.

Typically, the function $f$ is parametrized as 
\begin{equation} \label{eqn:parametrization}
    f = c \circ \phi
\end{equation}
where $c$ is a classifier and $\phi$ is an embedding. We will consider the case where the embedding is obtained via SEs in Sec. \ref{sbs:sbm}, and via GNNs in Sec. \ref{sbs:gnns}.

\subsection{Stochastic Block Model and Spectral Embeddings} \label{sbs:sbm}

The canonical statistical model for graphs with communities is the stochastic block model (SBM).

\begin{definition}[Stochastic Block Model] \label{defn:sbm}
A SBM graph with $K$ communities is defined as a graph $\bbG$ with adjacency matrix $\bbA \in \{0,1\}^{N \times N}$ given by
\begin{equation*}
\bbA \sim \mbox{Ber}(\bbP),\ \, 
\bbP = \bbY \bbB \bbY^{\top}
\label{eqn:SBM}
\end{equation*}
where $\bbY \in \{0,1\}^{N \times K}$ is the community assignment matrix $\bbY_{i\cdot} = \texttt{one-hot}(k)$, and $\bbB \in [0,1]^{K \times K}$ is a full-rank matrix representing the block connection probability. %We naturally associate node $i$ from block $k$ with community label $\bbY_{i\cdot} = \texttt{one-hot}(k)$.
\end{definition}

Spectral methods for community detection are inspired by the spectral decomposition of the SBM. Consider for example the case where $K=2$, $\bbB=[p\ q; q\ p]$, $p \neq q$, and the communities are balanced, i.e., $N$ is even and both communities have size $N/2$. Relabeling $\ccalV$ so that the first $N/2$ nodes belong to the first community and the remaining $N/2$ to the second, we see that the eigenvectors of $\mbE\bbA \equiv \bbP$, the expected adjacency, are given by
\begin{equation} \label{eqn:sbm_eig}
    [\bbv_1(\mbE\bbA)]_i = \frac{1}{\sqrt{N}},\ [\bbv_2(\mbE\bbA)]_i = 
    \begin{cases}
    -1/\sqrt{N}, \ i \leq N/2 \\
    +1/\sqrt{N}, \ i > N/2 \text{.}
    \end{cases}
\end{equation}
For a graph $\bbG$ sampled from this model, with sufficiently large $N$ and mild assumptions on $p,q$, we can thus expect the eigenvector $\bbv_2(\bbA)$ to provide a good estimate of its community structure, i.e., $\bbv_k(\bbA) \approx \bbv_k(\mbE\bbA), k \in \{1,2 \}$.  
%In fact, we can estimate the true community assignment matrix $\bbY$ as $\hbY = \bbC [\bbv_1(\bbA)\ \bbv_2(\bbA)]$ where $\bbC = 0.5/\sqrt{N}[1\ 1; -1\ 1]$.

Real-world graphs $\bbG$ have more intricate sparsity patterns than the SBM, but it is reasonable to assume that if the graph $\bbG$ has two balanced communities, for some permutation of the nodes its adjacency matrix $\bbA$ can be approximately written as $\bbA = \bbA_{\tiny \mbox{SBM}} + \bbE$, where $\bbA{\tiny \mbox{SBM}}$ is as in Def. \ref{defn:sbm} and $\bbE$ can be seen as a perturbation satisfying $\|\bbE\|_2 < \|\bbA{\tiny \mbox{SBM}}\|_2$. As such, the first two eigenvectors of $\bbA$ still ``embed'' community information. More generally, in graphs $\bbG$ with $K>2$ balanced communities, the community information is ``embedded'' in the first $K$ eigenvectors. Based on this observation, the order-$K$ \textit{spectral embedding} (SE) of a graph $\bbG$ is defined as
\begin{equation} \label{eqn:spectral_embedding} 
    \phi_{\tiny \mbox{SE}}(\bbA) = [\bbv_1\ \bbv_2\ \ldots\ \bbv_{K-1}\ \bbv_K] = \bbV_K,
\end{equation}
i.e., as the concatenation of the first $K$ eigenvectors of $\bbA$. Variants of SE tailored for sparse graphs propose replacing $\bbA$ with other graph operators, such as the normalized adjacency matrix $\tilde{\bbA}  \coloneqq \bbD^{-0.5} \bbA \bbD^{-0.5}$ where $\bbD$ is the degree matrix \cite{cape2019spectral}, the non-backtracking operator \cite{abbe2017community}, etc. 

Note that $\phi_{\tiny \mbox{SE}}$ is nonparametric; it can be obtained directly from the graph without node label supervision. When we use spectral embeddings in \eqref{eqn:parametrization}, the only parameters that are learned are those of the classifier $c$. E.g., choosing a linear classifier yields a simple parameterization of $f$ as
%\begin{equation} 
$f(\bbA) = c \circ \phi_{\tiny \mbox{SE}}(\bbA) = \mbox{\texttt{softmax}}(\bbV_K\bbC)$
%\end{equation}
where $\bbC \in \reals^{K \times K}$ is learned. More generally, it is possible to use embeddings $\phi_{\tiny \mbox{SE}}(\bbA) = \bbV_{\tilde{K}}$ with $\tilde{K}>K$, i.e., with a larger number of eigenvectors than that of communities, in which case $\bbC \in \reals^{\tilde{K}\times K}$.

An important observation to make is that $ \phi_{\tiny \mbox{SE}}$ (and so $f$) do not need to depend on $\bbX$, but if such node features are available, they can be incorporated into the spectral embedding in different ways, e.g., \cite{yang2013node, binkiewicz2017covariate, arroyo2021inference, mu2022spectral, Mele2022discrete}. We consider an approach similar to \cite{arroyo2021inference}, by first embedding the node feature covariance and concatenating it with the spectral embedding. More precisely, let $\bbV'_{\kappa}$ be the first $\kappa$ eigenvectors of the covariance matrix $\bbX \bbX^{\top}$, then the \textit{feature-aware} spectral embedding is defined as 
\begin{equation}
  \label{eqn:node_sp_emb}
    \phi_{\tiny \mbox{SE}}(\bbA;\bbX) = [\bbV_K\ \ \bbV'_{\kappa}].
\end{equation}


\subsection{Graph Neural Networks} \label{sbs:gnns}

Given a graph $\bbG$ with adjacency matrix $\bbA \in \reals^{N \times N}$ and a graph signal $\bbx \in \reals^{N}$, a graph convolution (or filter) is given by \cite{du2018graph}
\begin{equation} \label{eqn:graph_convolution}
   \bbu = \sum_{k=0}^{K-1} h_k \bbA^k \bbx
\end{equation}
where $h_0, \ldots, h_{K-1}$ are the filter coefficients or taps. More generally, if $\bbX \in \reals^{N \times D}$ and $\bbU \in \reals^{N\times G}$ have $D$ and $G$ features respectively, we write
\begin{equation} \label{eqn:graph_convolution_multifeature}
   \bbU = \sum_{k=0}^{K-1} \bbA^k \bbX \bbH_k
\end{equation} 
where the filter parameters are now collected in the matrices $\bbH_0, \ldots,$ $\bbH_{K-1} \in \reals^{D \times G}$.

Graph neural networks (GNNs) are deep convolutional architectures where each layer composes a graph convolution \eqref{eqn:graph_convolution_multifeature} and a pointwise nonlinearity $[\sigma(\bbU)]_{ij}=\sigma([\bbU]_{ij})$, e.g., the ReLU or the sigmoid. The $\ell$th layer of a GNN can thus be written as
\begin{equation} \label{eqn:gnn}
    \bbX_\ell = \sigma \left( \sum_{k=0}^{K-1} \bbA^k \bbX_{\ell-1} \bbH_{\ell k} \right)
\end{equation}
where $\bbX_{\ell-1} \in \reals^{N \times F_{\ell-1}}$ and $\bbX_{\ell} \in \reals^{N \times F_{\ell}}$ are the input and output to this layer with $F_{\ell-1}$ and $F_\ell$ features each. If the GNN has $L$ layers, its input and output are $\bbX_0 = \bbX \in \reals^{N \times F_0}$ and $\bbX_L \in \reals^{N \times F_L}$.

The GNN in \eqref{eqn:gnn} may be used to parametrize $\phi$ in \eqref{eqn:parametrization}, in which case we define the \textit{GNN embedding}
\begin{equation} \label{eqn:gnn_embedding}
    \phi_{\tiny \mbox{GNN}}(\bbA,\bbX)=\bbX_L \text{.}
\end{equation}
Note that, unlike the spectral embedding \eqref{eqn:spectral_embedding}, \eqref{eqn:gnn_embedding} is parametric on $\{\bbH_{\ell k}\}_{\ell,k}$ and always needs an input signal $\bbX$ (if an input signal is not available, $\bbX$ may be a random signal, for example). A typical parametrization of $f$ for GNN embeddings is
%\begin{equation}
$f(\bbA,\bbX) = c \circ \phi_{\tiny \mbox{GNN}}(\bbA, \bbX) = \mbox{\texttt{softmax}}(\bbX_L\bbC)$
%\end{equation}
where $\bbC \in \reals^{F_L \times K}$ is a linear classifier over $F_L$ node features. This is equivalent to a $L+1$-layer GNN with $K=1$ and softmax nonlinearity in the last layer.

%%%%%%%%%%%%%%%%%
%%%% SECTION %%%%
%%%%%%%%%%%%%%%%%

\section{Main Results} \label{sec:main}

In the following, we introduce a random graph model for both dense and sparse graphs. We use this model to prove a result that helps explain the limitations of spectral embeddings on sparse graphs. We then show that under mild assumptions on both the graph and the input signal, GNNs give access to entire spectrum, and thus can learn embeddings that are more expressive than spectral embeddings.

\subsection{A Graph Model for Dense and Sparse Graphs} \label{sbs:graph_model}

Def. \ref{def:graphex} introduces a random graph model allowing to model graphs with varying levels of sparsity according to a sparsity parameter $\gamma$.

\begin{definition}[Dense-Sparse Graph Model (DSGM)] \label{def:graphex}
A DSGM graph with kernel $\bbW$ and sparsity parameter $\gamma$ is defined as a graph $\bbG$ with adjacency matrix $\bbA \in \{0,1\}^{N \times N}$ given by
\begin{equation*}
    [\bbA]_{ij} = [\bbA]_{ji} \sim \mbox{Ber}(\bbW(u_i,u_j)),\   
    u_{i} = 
    \begin{cases}
    u_{i-1} + \gamma, \ 2 \leq i \leq N \\
    -\lfloor\dfrac{n}{2}\rfloor\gamma + \dfrac{\gamma}{2},\ i=1
    \end{cases} \label{eqn:ui}
\end{equation*}
where $\bbW: \reals^2 \to [0,1]$ is symmetric, $\|\bbW\|_{L^2} < \infty$, and $\gamma > 0$.
\end{definition}

This model allows sampling both dense and sparse graphs because, since $\bbW$ has vanishing tails (or can be mapped to a kernel that does by some measure-preserving transformation), for a fixed $N$ the graph is sparser for larger $\gamma$. 

The kernel $\bbW$ defines a self-adjoint Hilbert Schmidt operator. Hence, it has a real spectrum given by
\begin{equation}
    \int_{-\infty}^{\infty} \bbW(u,v) \varphi_i(u)du = \lambda_i \varphi_i(v)
\end{equation}
where the eigenvalues $\lambda_i$ are countable and the eigenfunctions $\varphi_i$ form an orthonormal basis of $L^2$. 
By convention, the eigenvalues are ordered as $|\lambda_1|\geq|\lambda_2|\geq\ldots$. Moreover, $|\lambda_i| \leq \infty$ for all $i$, and $\lambda_i \to 0$ as $i \to \infty$ with zero being the only accumulation point.

We further introduce the notion of a kernel induced by a graph, which will be useful in future derivations. For $N\geq 2$, the kernel induced by the graph $\bbG_N$ with adjacency $\bbA_N$ and sparsity parameter $\gamma$ is defined as
\begin{equation} \label{eqn:induced}
\bbW_{N}(u,v)=\sum_{i=1}^{N-1}\sum_{j=1}^{N-1} [\bbA_N]_{ij}\mbI(u \in I_i)\mbI(v \in I_j) 
\end{equation}
where $I_i=[u_i,u_{i+1})$ for $1\leq i\leq N-2$, $I_{N-1} = [u_{N-1},u_N]$, and $u_i$ is as in Def. \ref{eqn:ui}.

\subsection{Limitations of Spectral Embeddings} \label{sbs:limitations}

To discuss community detection on graphs sampled from a DSGM (Def. \ref{def:graphex}), we assume that the kernel $\bbW$ exhibits community structure. 
For simplicity, we focus on $2$ communities but the discussions can be easily extended to $K$ communities. 
Inspired by the degree-corrected SBM \cite{karrer2011stochastic, qin2013regularized}, in Def. \ref{eqn:dc_sbm} we introduce the degree-corrected stochastic block kernel (SBK) as the canonical kernel for DSGMs with $2$ balanced communities. This model is suitable to model sparse graphs and well-studied in the spectral embedding literature \cite{qin2013regularized, cape2019spectral}. To ensure that models based on these kernels are valid DSGMs, we restrict attention to finite-energy degree functions $\theta$.

\begin{definition}[Degree-Corrected SBK] \label{eqn:dc_sbm} 
The degree-corrected SBK with $2$ communities is given by
\begin{equation*}
 \bbW(u,v) = 
    \begin{cases}
    \theta(u) \, \theta(v) \, p ,\quad u v \ge 0 \\
     \theta(u) \, \theta(v) \, q,\quad u v < 0 
    \end{cases}
\end{equation*}
where $\theta:\reals\to[0,1]$, $\theta \in L^2$, is the degree function. The true community assignment is $Y(u) =  [1\ 0]\mbI(u \ge 0) + [0\ 1]\mbI(u < 0)$, which is independent of $\theta$.
\end{definition}

It is not difficult to see that the first $2$ eigenfunctions of $\bbW$ in Def. \ref{eqn:dc_sbm} reveal the community structure \footnote{$\varphi_1(u) = \theta(u)/C, \varphi_2(u) =  \big( -\theta(u) \mbI(u < 0) + \theta(u) \mbI(u \ge 0) \big)/C, \\ \text{ where } C \coloneqq \int \theta(u) du$.}. 
%The Degree-corrected SBM also has community structure embedded in the first $K$ eigenfunctions of $\bbW$ in \eqn{},
For graphs $\bbG_N$ sampled as in Def. \ref{def:graphex} from the DSGM with degree-corrected kernel as in Def. \ref{eqn:dc_sbm}, the true community assignment is given by $[\bbY]_{i\cdot} = Y(u_i)$ for $1 \leq i \leq N$. As such, the quality of the estimate of the community assignment given by the first $2$ (or, more generally, the first $K$) eigenvectors of $\bbG_N$ will depend on both (i) how close the eigenvalues $\lambda_{k}(\bbG_N)$ are to the kernel eigenvalues $\lambda_{k}(\bbW)$ (as this can affect their ordering) and (ii) how close the eigenvectors $\bbv_{k}$ are to the eigenfunctions $\varphi_{k}$. These differences are upper bounded by Thm. \ref{thm:comm_conc}.

\begin{theorem}[Eigenvalue and eigenvector concentration] \label{thm:comm_conc}
Let $\bbG_N$ be a graph sampled from the DSGM in Def. \ref{def:graphex}, where $N$ satisfies \cite[Ass. AS4]{ruiz2021transferability}. Let $c \leq \lfloor N/2\rfloor\gamma-\gamma/2$, and assume that:
\begin{enumerate}
    \item $\bbW$ is $A_w$-Lipschitz in $[-c,c] \times [-c,c]$ (see \cite[Ass. AS2]{ruiz2021transferability})
    \item $\int_{|v|\geq c} \int_{|u|\geq c} \bbW(u,v)dudv < \epsilon(c)$.
\end{enumerate}
Then, with probability at least $1-\chi$, the difference between the $k$th eigenvalue of $\bbG_N$ and $\bbW$, $1\leq k\leq K$, is bounded by
\begin{align*}
\begin{split}
    |\lambda_k(\bbW_N)-\lambda_k(\bbW)| &\leq 4 A_w c \gamma + {\beta(\chi,N)}{N^{-1}} + \epsilon(c) \\
    &\leq 2 A_w N \gamma ^2 + {\beta(\chi,N)}{N^{-1}} + \epsilon(c)
\end{split}
\end{align*}
and the difference between their $k$th eigenvectors by
\begin{align*}
\begin{split}
    \|\varphi_k(\bbW_N)-\varphi_k(\bbW)\| 
    \leq \frac{\pi}{2\delta_k} \bigg(4A_w c \gamma
    + {\beta(\chi,N)}{N^{-1}} + \epsilon(c)\bigg)
\end{split}
\end{align*}
where $\bbW_N$ is the kernel induced by $\bbG_N$ \eqref{eqn:induced}\footnote{See \cite[Lemma 2]{ruiz2020graphonsp} for the relationship between $\lambda_k(\bbG_N)$, $\bbv_k(\bbG_N)$ and $\lambda_k(\bbW_N)$, $\varphi_k(\bbW_N)$.}, $\delta_k = \min_i{\{|\lambda_k(\bbW)-}$ ${\lambda_i(\bbW_N)|,|\lambda_k(\bbW_N)-\lambda_i(\bbW)|\}}$ and $\beta(\chi,N)$ is sublinear in $N$ and as in \cite[Def. 7]{ruiz2021transferability}.
\end{theorem}
\begin{proof}
Refer to the extended version in this \href{https://github.com/nhuang37/GNN_community_detection}{repository}.
\end{proof}

\begin{figure}[t]
  \centering
\subfloat[$\bbW$ ]{\includegraphics[height=1.9cm,valign=c]{sampling/graphex.png}}
\qquad
\subfloat[Dense $\bbG_N^d$]{\includegraphics[height=1.9cm,valign=c]{sampling/graph_dense.png}  }
\qquad
\subfloat[Sparse $\bbG_N^s$]{\includegraphics[height=1.9cm,valign=c]{sampling/graph_sparse.png}  }
%
\caption{Kernel $\bbW:\R^2 \to [0,1]$ visualized in $[-2,2]^2$ and sampled graphs with different sparsity levels $\gamma$.}
  \label{fig:graphex_model}
\end{figure}

This theorem shows that the differences between the eigenvalues and eigenvectors of the graph and the underlying random graph model are upper bounded by terms that increase with $\gamma$. Consider a dense graph $\bbG_N^d$ and a sparse graph $\bbG_N^s$ sampled from DSGMs with same kernel $\bbW$ but different sparsity parameters $\gamma_d \ll \gamma_s$. If $N$ and $c$ are large enough for the term depending on $4A_w c \gamma$ to dominate the bound in the dense case, the bound on the difference between eigenvalues and eigenvectors in the sparse case is much larger than in the dense case.
In the context of community detection, this can be interpreted to mean that, since $\varphi_k(\bbW_N^d)$ is close to $\varphi_k(\bbW)$ for dense graphs, some linear combination of the eigenvectors $\bbv_k(\bbG_N^d)$ provides a good estimate of the true community assignment $\bbY$.
This is not true for the eigenvectors $\bbv_k(\bbG_N^s)$ of the sparse graph, since $\varphi_k(\bbW_N^s)$ is further away from $\varphi_k(\bbW)$.
Another way to think about this is that on dense graphs most of the ``community information'' is on the first $K$ eigenvectors. On sparse graphs, it is more spread throughout the spectrum. This implies that while spectral embeddings may be effective for community detection on dense graphs, they are less likely to be effective in sparse graphs. We further demonstrate this empirically in Sec. \ref{sec:exp}.

\subsection{Graph Neural Networks for Community Detection} \label{sbs:gnns_comm}

In sparse graphs, GNN embeddings are a better option than spectral embeddings because, provided that the input signal $\bbX_{\ccalT}$ in \eqref{eqn:erm} is not orthogonal to any of the graph's eigenvectors, GNNs ``have access'' to the entire spectrum. Moreover, if the true community assignment signal is $\bbY$, a GNN can always represent $\bbY$ with $K\leq N$ in \eqref{eqn:gnn}. These claims are formally stated for the simple graph convolution \eqref{eqn:graph_convolution} in Thm. \ref{thm:power}. They can be readily extended to multi-feature graph convolutions \eqref{eqn:graph_convolution_multifeature} and GNNs \eqref{eqn:gnn} where the nonlinearity $\sigma$ preserves the sign (e.g., the hyperbolic tangent).

\begin{theorem}[Expressive power of graph convolution] \label{thm:power}
Let $\bbG$ be a symmetric graph with full-rank adjacency matrix $\bbA \in \reals^{N\times N}$ diagonalizable as $\bbA=\bbV\bbLam\bbV^\top$ where all eigenvalues have multiplicity one. Let $\bbx \in \reals^N$ be an input signal satisfying $[\bbV^\top\bbx]_i \neq 0$ for $1\leq i\leq N$. 
Consider the graph convolution $\hby=\sum_{k=0}^{K-1}h_k \bbA^k \bbx$ \eqref{eqn:graph_convolution}. Then, the following hold:
\begin{enumerate}
    \item For all $K\geq 1$, there exist $h_0, \ldots, h_{K-1} \in \reals$ such that $\hby$ satisfies $[\bbV^\top\hby]_i \neq 0$ for every $i$.
    \item Let $\bby \in \reals^N$ be a target signal. There exist $K\leq N$ coefficients $h_0, \ldots, h_{K-1} \in \reals$ for which $\hby$ satisfies $\hby=\bby$.
\end{enumerate}
\end{theorem}
\begin{proof}
Refer to the extended version in this \href{https://github.com/nhuang37/GNN_community_detection}{repository}.
\end{proof}

Note that the assumptions of Thm. \ref{thm:power} are not too restrictive; most real-world graphs are full rank, and even a random signal $\bbx \in \reals^N$---which may be used as the input in \eqref{eqn:gnn_embedding} when $\bbx$ is not given---satisfies $[\bbV^\top\bbx]_i\neq 0$ with high probability. It is also worth pointing out that while $K \leq N$ is necessary to \textit{exactly represent} $\bby$, in practice small $K$ is often enough to obtain good \textit{approximations} of the true community assignment as illustrated in Sec. \ref{sec:exp}. This is another reason why in practical, large graph settings, GNN embeddings are advantageous w.r.t. spectral embeddings: a small number of matrix-vector multiplications requires less computations than calculating a number of eigenvectors at least as large as the number of communities. 

%%%%%%%%%%%%%%%%%
%%%% SECTION %%%%
%%%%%%%%%%%%%%%%%
\section{Experiments} \label{sec:exp}
 
\begin{figure}[t]
  \centering
\includegraphics[width=0.5\textwidth]{sampling/experiment_comp.png}
  \caption{Test accuracy for different sparsity levels of the sampled graphs. GNNs perform better than SEs in sparse graphs for both operators $\bbA, \tilde{\bbA}$.}
  \label{fig:graphex_exp}
\end{figure}
 
In what follows, we conduct simulations on synthetic graphs sampled from a DSGM (Section \ref{sec:exp-dsgm}) and real-world graphs (Section \ref{sec:exp-real}). For completeness, we consider graph operators $\bbA, \tilde{\bbA}$. Our empirical results validate our theoretical analysis and show that GNNs outperform spectral embedding for community detection in sparse graphs.\footnote{All the simulations and code are available in this \href{https://github.com/nhuang37/GNN_community_detection}{repository}.}

\subsection{Experiments on Synthetic Graphs} \label{sec:exp-dsgm}

\noindent \textbf{Setup.} We consider the following kernel
\begin{equation}
\bbW(u,v) = \begin{cases}
\frac{p}{(|u|+1)^{2}(|v|+1)^{2}} & uv \ge  0\\
\frac{q}{(|u|+1)^{2}(|v|+1)^{2}} & uv < 0. \label{eqn:a_graphex}
\end{cases}
\end{equation}
The graphs $\bbG$ are sampled from the DSGM with kernel $\bbW$ above following Def. \ref{def:graphex}, with $N=1000$ and different choices of density parameter $\gamma_d = 0.002, \gamma_s = 0.01$ as illustrated in Fig. \ref{fig:graphex_model}. The node features $\bbX$ are sampled from a mixture of two Gaussians in $\R^2$ where $\bbmu_0 = - \bbmu_1 = [1, 1], \bbSigma_0 = \bbSigma_1 = \bbI/4$.
For each tuple $(\bbG,\bbX)$, we randomly split the nodes in each community by $50/50$ to create the training and test sets. We compare spectral embeddings with various choices of $K$ against GNNs. 

\noindent \textbf{Results.} Fig. \ref{fig:graphex_exp} shows that spectral embedding with $K=2$ outperforms GNNs in dense graphs while GNNs are more competitive in sparse graphs. Fig. \ref{fig:freq} depicts the frequency response $\hby$ from the trained GNN model using $\tilde{\bbA}$: (a) shows that, in the dense graph, GNNs indeed attend to frequency components other than the first two eigenvectors, which increase the noise/variance of the embedding and thus degrades the downstream classification performance, confirming the discussion in Thm. \ref{thm:power}; (b) shows that, in the sparse graph, GNNs increasingly attend to higher-frequency components, which are useful since they may also encode community information; spectral embeddings exhibit higher variance, and can benefit from choosing suitably larger embedding dimension.

\begin{figure}[t]
  \centering
\subfloat[$\hby$ (linear) for $\bbG_N^d$ ]{\includegraphics[width=0.2\textwidth,height=2cm,valign=c]{sampling/freq_dense.png}}
\qquad
\subfloat[$\hby$ (linear) for $\bbG_N^s$]{\includegraphics[width=0.2\textwidth,height=2cm,valign=c]{sampling/freq_sparse.png}  }
\qquad
\subfloat[$\hby$ (nonlinear) for $\bbG_N^d$ ]{\includegraphics[width=0.2\textwidth,height=2cm,valign=c]{sampling/freq_dense_nonlinear.png}}
\qquad
\subfloat[$\hby$ (nonlinear) for $\bbG_N^s$]{\includegraphics[width=0.2\textwidth,height=2cm,valign=c]{sampling/freq_sparse_nonlinear.png}  }
%
\caption{Frequency responses $\hby$ of GNNs using $\tilde{\bbA}$ on $\bbG_N^d$ and $\bbG_N^s$. In the dense case (left), although the optimal frequency response is a step-function on the first two components, GNNs spread energies on the remaining components, adding noise; In the sparse case (right), the community information spreads widely across the spectrum and thus GNNs outperform spectral embedding. Nonlinear GNNs (bottom) leverage the spectrum more uniformly than linear convolutions (top). Eigenvalues of $\tilde{\bbA}$ (dashed) are sorted in decreasing order.}
  \label{fig:freq}
\end{figure}

\subsection{Experiments on Real-World Graphs} \label{sec:exp-real}

\noindent \textbf{Setup.} We consider the Wikipedia webpage network Chameleon, a heterophilous benchmark graph with $5$ communities introduced in \cite{rozemberczki2021multi}. We treat the original Chameleon network ($|\ccalV|=2277$, average degree $13.8$) as the dense baseline, and randomly drop a fraction of its edges to obtain the sparse(r) graphs. We then evaluate GNNs and spectral embedding in the original and sparsified graphs. For each sparsity level, we randomly generate $10$ sparsified graphs. 

\noindent
\textbf{Results.} Table~\ref{tab:exp_real} shows that GNNs and spectral embeddings both perform well in the original graph. Yet, in the sparsified graphs (``Drop(20)'', ``Drop(70)''), performance degradation in GNNs is smaller than spectral embeddings. %It is interesting to observe that GNNs using $\tilde{\bbA}$ appears to be more robust than using $\bbA$. %GNNs perform consistently well while spectral embeddings degrade moderately when dropping 20\% edges (``Drop(20)'')  and drastically when dropping 50\% edges (``Drop(50)''). 
Moreover, in sparsified graphs, spectral embeddings with large $K$ are numerically unstable and computationally intensive due to the presence of many small eigenvalues. These findings show that GNNs can detect communities more accurately and efficiently than spectral methods in sparse graphs.

\begin{table}[htb!]
\scriptsize
\caption{Test accuracy on Chameleon graphs, reported as \texttt{mean($\pm$stderr)} across 10 data splits and 10 sparsified subgraphs.}
\label{tab:exp_real}
\centering
\resizebox{\columnwidth}{!}{\begin{tabular}{cccccc}
\hline \hline
Graph & Operator	&	SE(150)	&	SE(200)	&	GNN(lin)	&	 GNN(non) 	\\
\hline \hline
\multirow{2}{*}{Original} & $\bbA$ 	&	\textbf{57.29 $\pm$ 0.69}	&	56.97 $\pm$ 0.59	&	56.27 $\pm$ 0.69	&	54.38 $\pm$  0.97	\\
 & $\tilde{\bbA}$		&	52.70 $\pm$ 0.36	&	53.84 $\pm$ 0.43	&	55.60 $\pm$  0.70	&	\textbf{55.90 $\pm$ 0.73  }	\\
  \hline
\multirow{2}{*}{Drop(20)} & $\bbA$ &	53.20 $\pm$ 0.21	&	53.30 $\pm$  0.22	&	\textbf{53.91 $\pm$  0.25}	&	52.69 $\pm$ 0.29	\\
	&  $\tilde{\bbA}$ &	49.42 $\pm$ 0.21	&	51.53 $\pm$  0.19		& 54.45	 $\pm$ 0.21 &	\textbf{54.66 $\pm$ 0.22 }	\\
\hline
\multirow{2}{*}{Drop(70)}  & $\bbA$	&	45.47 $\pm$ 0.20	&	45.12 $\pm$ 0.22	&	\textbf{46.21 $\pm$ 0.23}	&	45.95 $\pm$ 0.24	\\ 
	&$\tilde{\bbA}$ & 41.21 $\pm$ 0.19	&	42.51 $\pm$ 0.27	&	50.10 $\pm$	0.19 &\textbf{50.25 $\pm$ 0.21}\\
\hline \hline
\end{tabular}}
\end{table}

% \begin{table}[t]
% \scriptsize
% \caption{Test accuracy on the Chameleon network and its sparsified graphs, reported as \texttt{mean($\pm$stderr)} across 10 data splits and 10 sparsified subgraphs.}
% \label{tab:exp_real}
% \centering
% \begin{tabular}{ccccc}
% \hline \hline
% Graph	&	SE(150)	&	SE(200)	&	GNN(lin)	&	 GNN(non) 	\\
% \hline \hline
% Original	&	52.70 $\pm$ 0.36	&	53.84 $\pm$ 0.43	&	55.60 $\pm$  0.70	&	\textbf{55.90 $\pm$ 0.73  }	\\
% Drop(20)	&	49.42 $\pm$ 0.21	&	51.53 $\pm$  0.19		& 54.45	 $\pm$ 0.21 &	\textbf{54.66 $\pm$ 0.22 }	\\
% Drop(50)	&	44.69 $\pm$ 0.27	&	45.73 $\pm$ 0.25	&	52.06 $\pm$	0.22 &\textbf{52.15 $\pm$ 0.21}\\ 
% %\textcolor{red}{Work-in-progress} & & & &\\
% \hline \hline
% \end{tabular}
% \end{table}

% \begin{figure}[htb!]
%   \centering
% \includegraphics[width=0.47\textwidth,height=2.2cm]{sampling/experiment2.png}
%   \caption{Performance w.r.t sparsified subgraphs of Amazon Photo. When dropping half of the original edges, GNNs are stable while spectral methods are brittle and highly dependent on the embedding dimension $K$.}
%   \label{fig:photo}
% \end{figure}

\begin{comment}
\clearpage
\section{Degree-corrected SBM}

\begin{definition}[Contextual degree-corrected stochastic block model (CDSBM) \cite{karrer2011stochastic, qin2013regularized}] \label{defn:cdsbm}
A random CDSBM graph with $K$ communities is defined as a tuple $(\bbG,\bbX)$ where:
\begin{enumerate}
    \item The adjacency matrix $\bbA \in \{0,1\}^{N \times N}$ is sampled as %connecting every pair of nodes independently,
    \begin{equation}
    \bbA \sim \mbox{Bernoulli}(\bbP), \, 
    \bbP = \bbTheta \bbZ \bbB \bbZ^{\top} \bbTheta
    \label{eqn:CDSBM}
    \end{equation}
    where $\bbTheta = \operatorname{diag}(\theta_i) \in \R^{N \times N}$ is the diagonal matrix controlling the degree, $\bbZ \in \R^{N \times K}$ is the membership matrix satisfying $\sum_{k=1}^K |Z_{ik}| = 1$, and $\bbB \in [0,1]^{K \times K}$ is a full-rank matrix representing the block connection probability.
    \item The node features $\bbX \in \reals^{N\times D}$ is sampled as 
    \begin{equation}
    [\bbX]_{i\cdot} \sim \ccalN(\bbmu_k,\bbSigma_k), \, 
    \mbox{if } [\bbZ]_{ik} = 1
    \label{eqn:CDSBM-feat}
    \end{equation}
    where $\bbmu_k \in \reals^D$, $\bbSigma_k \in \reals^{D \times D}$ for $1 \leq k \leq K$.
\end{enumerate}
We naturally associate node $i$ from block $k$ with community label $\bbY_{i\cdot} = \texttt{one-hot}(k)$.
\end{definition}


Spectral methods for community detection are inspired by the spectral decomposition of the (C)DSBM. To see this, consider the two-block balanced case where $K=2$, $N$ is even,
\begin{equation}
    Z_{i,\cdot} = \begin{cases}
    [1,0] & \text{if } i \in [N/2]  \\
    [0,1] & \text{otherwise.}
    \end{cases}, \, \,
    B =  \begin{bmatrix}
p  & q\\
q  & p
\end{bmatrix},  \label{eqn:2B-SBM}
\end{equation}
with $p, q \in (0,1), p \ne q$. 
We see that the nonzero eigenvectors of $\mbE\bbA \equiv \bbP$, the rank-$2$ expected adjacency matrix, are given by
\begin{align} \label{eqn:sbm_eig}
    [\bbv_1(\mbE\bbA)]_i &= \theta_i /\sqrt{N} \\
    [\bbv_2(\mbE\bbA)]_i &= 
    \begin{cases}
    -\theta_i /\sqrt{N}, \quad i \leq N/2 \\
    +\theta_i /\sqrt{N}, \quad i > N/2 \text{.}
    \end{cases}
\end{align}
Thus, the sign of the second eigenvector $\bbv_2(\mbE\bbA)$ reveals the community assignment. 

When the degree factors $\theta_i$ are close to each other (e.g., $\theta_i = c, \,  i \in [N]$ reduces to SBM), we can expect the eigenvector $\bbv_2(\bbA)$ provide a good estimate of the community structure. However, real-world graphs are typically sparse (CITE), which implies that $\theta_i$ has large variance and can be very small. To avoid the confounding effect of the node degree in sparse graphs, it is better to use the eigenvector of the normalized adjacency matrix $\tilde{\bbA} \coloneqq \bbD^{-0.5} \bbA \bbD^{-0.5}, \bbD \coloneqq \operatorname{diag}(\bbA \boldsymbol{1}_N)$ (also known as the graph Laplacian) \cite{cape2019spectral}. In this paper, we focus on the sparse setting and thus consider the order-$K$ \textit{Laplacian spectral embedding} (LSE), 
\begin{equation} \label{eqn:spectral_embedding_lap}
    \phi_{\tiny \mbox{LSE}}(\bbA) = [\bbv_1(\tilde{\bbA})\ \ldots\  \bbv_K(\tilde{\bbA})] = \bbV_K,
\end{equation}
i.e., as the row-concatenation of the first $K$ eigenvectors of $\tilde{\bbA}$. 

\begin{definition}[Dense-Sparse Graph Model (DSGM)] \label{def:graphex_degree}
Consider $\bbP$ in eqn \ref{eqn:CDSBM} with $\theta_i = \theta_{N + 1- i}$, $\theta_1 < \ldots <  \theta_{\lfloor N/2 \rfloor}$ and $\theta_N$ decreases with $N$.
Let $\bbW: \reals^2 \to [0,1]$ be a symmetric kernel induced from $\bbP$ in the sense of \eqref{eqn:induce}, with $\|\bbW\|_{L^2} < \infty$. Let $\gamma > 0$. The DSGM $\bbG_N \sim G(N,\bbW,\gamma)$ is defined as
\begin{align}
    (i,j) &\sim \mbox{Bernoulli}(\bbW(u_i,u_j)) \\
    u_{i} &= 
    \begin{cases}
    u_{i-1} + \gamma, \quad 2 \leq i \leq N \\
    -\lfloor\dfrac{n}{2}\rfloor\gamma + \dfrac{\gamma}{2},\quad i=1.
    \end{cases} \label{eqn:ui}
\end{align}
\end{definition}

\color{red}
Question: can we define a continuous (not necessarily piece-wise) $\bbW$? For example, $\bbW$ defined in \eqref{eqn:a_graphex} and illustrated in Fig \ref{fig:graphex_model}?
\color{black}

Observe that $\bbW$ in Defn \ref{def:graphex_degree} is a piece-wise constant kernel; its vanishing tail allows us to sample both dense and sparse graphs.
\end{comment}

% References should be produced using the bibtex program from suitable
% BiBTeX files (here: strings, refs, manuals). The IEEEbib.bst bibliography
% style file from IEEE produces unsorted bibliography list.
% -------------------------------------------------------------------------
%\vfill\pagebreak

%\section{REFERENCES}
\label{sec:refs}
\bibliography{myIEEEabrv,refs,bib_dissertation}
\bibliographystyle{IEEEbib}

\section{Appendix} \label{sec:app}

\subsection{Proof of Theorem \ref{thm:comm_conc}}\label{sec:comm_conc}

The proof of Theorem \ref{thm:comm_conc} relies on slight variations of the Courant-Fisher and Davis-Kahan theorems, stated here as Propositions \ref{prop:eigenvalue_diff} and \ref{thm:davis_kahan}.

\begin{proposition}[Variant of Courant-Fisher] \label{prop:eigenvalue_diff}
Let $\bbW:[0,1]^2\to[0,1]$ and $\bbW':[0,1]^2\to[0,1]$ be two graphons with eigenvalues given by $\{\lambda_i(T_\bbW)\}_{i\in\mbZ\setminus\{0\}}$ and $\{\lambda_i(T_{\bbW'})\}_{i\in\mbZ\setminus\{0\}}$, ordered according to their sign and in decreasing order of absolute value, where $T_\bbW$ denotes the integral linear operator with kernel $\bbW$. Then, for all $i \in \mbZ \setminus \{0\}$, the following inequalities hold
\begin{equation*}
|\lambda_i(T_{\bbW'})-\lambda_i(T_\bbW)| \leq \vertiii{T_{\bbW'-\bbW}} \leq \|\bbW'-\bbW\|\ .
\end{equation*}
\end{proposition}
\begin{proof} \renewcommand{\qedsymbol}{}
%Let $\bbA:=\bbW'-\bbW$ and let $S_k$ denote a $k$-dimensional subspace of $L_2([0,1])$. Using the minimax principle \cite[Chapter 1.6.10]{kato2013perturbation}, we can write
%\begin{align*}
%\lambda_k(T_{\bbW}) = \min_{S_{k-1}} \max_{X \in S^\perp_{k-1}, \|X\|_{L_2}=1} \langle T_\bbW X, X\rangle\ .
%\end{align*}
%Therefore, it holds that
%\begin{align*}
%\lambda_k&(T_{\bbW'}) = \lambda_k(T_{\bbW+\bbA})\\
%&= \min_{S_{k-1}} \max_{X \in S^\perp_{k-1}, \|X\|_{L_2}=1} \langle T_{\bbW+\bbA} X, X\rangle \\
%&= \min_{S_{k-1}} \max_{X \in S^\perp_{k-1}, \|X\|_{L_2}=1} \langle T_{\bbW}+T_{\bbA} X, X\rangle \\
%&=\min_{S_{k-1}} \max_{X \in S^\perp_{k-1}, \|X\|_{L_2}=1} \left(\langle T_{\bbW} X, X\rangle + \langle T_{\bbA} X, X\rangle\right) \\
%&\leq \min_{S_{k-1}} \bigg( \max_{X \in S^\perp_{k-1}, \|X\|_{L_2}=1} \langle T_{\bbW} X, X\rangle \\
%&\ \quad \quad \quad \quad \quad \quad \quad \quad \quad + \max_{X \in S^\perp_{k-1}, \|X\|_{L_2}=1} \langle T_{\bbA} X, X\rangle \bigg) \\
%&\leq 
%\min_{S_{k-1}} \bigg( \max_{X \in S^\perp_{k-1}, \|X\|_{L_2}=1} \langle T_{\bbW} X, X\rangle + \max_\ell\lambda_\ell(T_\bbA)\bigg) \\
%&= \min_{S_{k-1}} \max_{X \in S^\perp_{k-1}, \|X\|_{L_2}=1} \langle T_{\bbW} X, X\rangle + \max_\ell\lambda_\ell(T_\bbA) \\
%&=\lambda_k(T_\bbW) + \max_\ell\lambda_\ell(T_\bbA)\ .
%\end{align*}
%where the first inequality follows from $\max(a+b) \leq \max(a) + \max(b)$ and the second from the fact that $\langle T_{\bbA} X, X\rangle \leq \max_\ell\lambda_\ell(T_\bbA)$ for all unitary $X$. Rearranging terms and using the definition of the operator norm, we get
%\begin{align} \label{eqn:pf_eig_diff}
%\begin{split}
%\lambda_k(T_{\bbW'})-\lambda_k(T_{\bbW}) &\leq \max_\ell\lambda_\ell(T_\bbA) \leq \max_\ell|\lambda_\ell(T_\bbA)| \\
%&= \|T_{\bbA}\| \leq \|A\|_{L_2}.
%\end{split}
%\end{align}
%where we have also used the fact that the Hilbert-Schmidt norm dominates the operator norm. 

%To prove that this inequality holds in absolute value, let $\bbA'=-\bbA$. Following the same reasoning as before, we get
%\begin{align*}
%\begin{split}
%\lambda_k(T_\bbW) &= \lambda_k(T_{\bbW'+A'}) \leq \lambda_k(T_{\bbW'}) + \|T_{\bbA'}\| \\
%&\leq \lambda_k(T_\bbW) + \|A'\|_{L_2}
%\end{split}
%\end{align*}
%and since $\|T_{\bbA'}\|=\|T_{\bbA}\|$ and $\|A'\|_{L_2} = \|A\|_{L_2}$, 
%\begin{equation} \label{eqn:pf_eig_diff2}
%\lambda_k(T_\bbW)- \lambda_k(T_{\bbW'}) \leq \|T_{\bbA}\| \leq \|A\|_{L_2}\ .
%\end{equation}
%Putting \eqref{eqn:pf_eig_diff} and \eqref{eqn:pf_eig_diff2} together completes the proof.
See \cite[Proposition 4]{ruiz20-transf}.
\end{proof}

\begin{proposition}[Variant of Davis-Kahan]\label{thm:davis_kahan}
Let $T$ and $T^\prime$ be two self-adjoint operators on a separable Hilbert space $\ccalH$ whose spectra are partitioned as $\gamma \cup \Gamma$ and $\omega \cup \Omega$ respectively, with $\gamma \cap \Gamma = \emptyset$ and $\omega \cap \Omega = \emptyset$. If there exists $d > 0$ such that $\min_{x \in \gamma,\, y \in \Omega} |{x - y}| \geq d$ and $\min_{x \in \omega,\, y \in \Gamma}|{x - y}| \geq d$, then the spectral projections $E_T(\gamma)$ and $E_{T^\prime}(\omega)$ satisfy
\begin{equation*}\label{eqn:davis_kahan}
	\vertiii{E_T(\gamma) - E_{T^\prime}(\omega)} \leq \frac{\pi}{2} \frac{\vertiii{{T - T^\prime}}}{d}
\end{equation*}
\end{proposition}
\begin{proof} \renewcommand{\qedsymbol}{}
See \cite{seelmann2014notes}.
\end{proof}

We thus only need to bound $\|\bbW-\bbW_N\|$. To do so, define $\overline{\bbW}_N$ as
\begin{equation} \label{eqn:induced}
\overline{\bbW}_{N}(u,v)=\sum_{i=1}^{N-1}\sum_{j=1}^{N-1} \bbW(u_i,\bbu_j)\mbI(u \in I_i)\mbI(v \in I_j) 
\end{equation}
where $I_i=[u_i,u_{i+1})$ for $1\leq i\leq N-2$, $I_{N-1} = [u_{N-1},u_N]$, and $u_i$ is as in \eqref{eqn:ui}.
Using the triangle inequality, we can write
\begin{equation}
    \|\bbW-\bbW_N\| \leq \|\bbW-\overline{\bbW}_N\| + \|\overline{\bbW}_N-\bbW_N\| \text{.}
\end{equation}
The norm difference between $\overline{\bbW}_N$ and $\bbW_N$ is bounded as $N^{-1}\beta(\chi,N)$ by \cite[Proposition 4]{ruiz2021transferability} and by the fact that $\|\bbW_N\|_{L_2}=N^{-1}\|\bbA_N\|_2$ (see \cite[Lemma 2]{ruiz2020graphonsp}). Let us now derive a bound for $\|\bbW-\overline{\bbW}_N\|$.

By definition of the $L^2$ norm,
\begin{align} \label{eqn:integral_term}
\begin{split}
    \|\bbW-\overline{\bbW}_N\| &= \sqrt{\int_{-\infty}^\infty |\bbW(u,v)-\overline{\bbW}_N(u,v)|^2dudv} \\
                               &\leq \sqrt{\int_{|v|< c} \int_{|u|< c} |\bbW(u,v)-\overline{\bbW}_N(u,v)|^2dudv} \\
                               &+ \sqrt{\int_{|v|\geq c} \int_{|u|\geq c} |\bbW(u,v)-\overline{\bbW}_N(u,v)|^2dudv}
\end{split}
\end{align}
The rightmost term is bounded by $\epsilon(c)$, as $\overline{\bbW}_N$ is zero outside of $[-c,c]$. Since $\bbW$ is $A_w$-Lispchitz in the $[-c,c]$ interval, we can write
\begin{align*}
|\bbW(u,v)-\overline{\bbW}_N(u,v)| &\leq A_w\max\left(\left|u-u_i\right|,\left|u_{i+1}-u\right|\right) \\
&+ A_w\max\left(\left|v-u_j\right|,\left|u_{j+1}-v\right|\right) \\
&\leq {A_w}{\gamma} + {A_w}{\gamma} = {2A_w}{\gamma}
\end{align*}
for $u_i \leq u \leq u_{i+1}$, $u_j \leq v \leq u_{j+1}$, where the $u_i$, $u_j$ are as in Definition \ref{def:graphex} for all $1\leq i,j\leq N$.
Therefore, the leftmost term in \eqref{eqn:integral_term} can be upper bounded as $\sqrt{2c \times 2c \times ({2A_w}{\gamma})^2}=4A_w\gamma c$, which completes the proof.

\subsection{Proof of Theorem \ref{thm:power}}
  
Theorem \ref{thm:power}.1 is a direct consequence of the fact that the graph convolution is pointwise in the spectral domain. To see this, substitute $\bbA=\bbV\bbLam\bbV^\top$ in \eqref{eqn:graph_convolution} and left-multiply both sides by $\bbV^\top$. We get
\begin{equation} \label{eqn:linear_system}
    [\bbV^\top\hby]_i = \sum_{k=0}^{K-1}h_k \lambda_i^k [\bbV^\top\bbx]_i \text{.}
\end{equation}
Hence, Theorem \ref{thm:power}.1 holds for any $h_k \neq 0$.

\noindent
To show Theorem \ref{thm:power}.2, we write \eqref{eqn:graph_convolution} in the matrix form
\begin{equation}
 \hby = [\bbx \  \bbA \bbx  \ldots \  \bbA^{K-1} \bbx] \, [h_0 \ h_1 \ \ldots h_{K-1}]^{\top}. \label{eqn: krylov}  
\end{equation}
To show there exists $h_k$ such that $\hby = \bby$, we consider $K=N$, which yields a linear system of $N$ equations (i.e., $\hby_i = \bby_i$ for $i \in [N]$) with $N$ unknowns $h_0, \ldots, h_{N-1}$. Thus, it suffices to show that the vectors $\bbx, \bbA \bbx, \ldots, \bbA^{N-1} \bbx$ are linearly independent. Consider projecting them to the eigen-basis of $\bbA$, i.e., 
\begin{equation}
    \bbV^{\top} [\bbx \  \bbA \bbx \  \ldots \  \bbA^{N-1} \bbx] \equiv  [\tbx\  \bbLam \tbx\  \ldots\  \bbLam^{N-1} \tbx],
\end{equation}
where $\tbx \coloneqq \bbV^{\top} \bbx$. Since $\bbV$ is invertible, it remains to show that  $\tbx, \ldots, \bbLam^{N-1} \tbx$ are linearly independent. Let $\bm{c} \in \R^N, \bbM \in \R^{N \times d}$, and $\bm{c} \odot \bbM$ denote multiplying the $i$-th row of $\bbM$ by the $i$-th component of $\bm{c}$. We write $\tbx = \bm{c} \odot \bm{1}$ where $\bm{1}$ denotes the all-ones vector. Then the matrix $[\tbx\  \bbLam \tbx\  \ldots\  \bbLam^{N-1} \tbx]$ reduces to 
\begin{equation}
\bm{c} \odot \left[\begin{array}{cccc}
1 & \lambda_1 &  \ldots & \lambda_1^{N-1} \\
1 & \lambda_2 &  \ldots & \lambda_2^{N-1}  \\
\vdots & \vdots &  \ddots & \vdots \\
1 & \lambda_N &  \ldots & \lambda_N^{N-1} 
\end{array}\right]. \label{eqn:vandermonde}
\end{equation}
%we can write $\bbx = \sum_{i=1}^N c_i \bbv_i$, where $\bbv_i$ are the eigenvectors of $\bbA$. Thus, 
%\begin{equation}
    %\bbA^k \bbx = \sum_{i=1}^N c_i \lambda_i^k \bbv_i. \label{eqn:power}
%\end{equation}
Observe that \eqref{eqn:vandermonde} is a row-wise scaled Vandermonde matrix, which has determinant $\prod_i [\bm{c}]_i \prod_{i < j} (\lambda_i - \lambda_j)$. By assumption that $[\bbV^{\top} \bbx]_i \neq 0$ for $1 \le i \le N$, all entries $[\bm{c}]_i$ are nonzero. By assumption that the eigenvalues all have multiplicity one, $\lambda_i - \lambda_j \ne 0$ for all $i< j$. Therefore, \eqref{eqn:vandermonde} has nonzero determinant and  linearly independent columns, %This shows that $\bbA^k \bbx$ is linearly independent of $\bbA^j \bbx$ for $k \neq j$, 
which completes the proof. It is also clear from the proof that both assumptions are necessary for the scaled Vandemonde matrix in \eqref{eqn:vandermonde} to have nonzero determinant.

%follows from the Cayley-Hamilton theorem. \red{Expand.}

\subsection{Experiment Details for Sec. \ref{sec:exp-dsgm}} \label{sec:app1}

\noindent \textbf{Data.} Our chosen $\bbW$ in \eqref{eqn:a_graphex} follows the degree-corrected SBM model in Def.\ref{eqn:dc_sbm}, which exhibits block structure via the two parameters $p,q$ and core-periphery pattern via the degree function. It is easy to check that $\bbW$ in \eqref{eqn:a_graphex} satisfies integrability condition in Def. \ref{def:graphex} and the Lipschitz continuity assumption (i) in Thm. \ref{thm:comm_conc}.

%It is well known that in sparse graphs, using $\tilde{\bbA}$ in SE is better than $\bbA$ due to the effects of heterogeneous node degrees \cite{cape2019spectral}. 

\noindent \textbf{Methods.} For a comprehensive investigation, we compare spectral embeddings and GNNs using two graph operators: the graph adjacency matrix $\bbA$  and the normalized adjacency $\tilde{\bbA}$. Since $\bbW$ has $2$ communities, we choose $\phi_{\text{SE}}$ as the top-$K$ eigenvectors of the graph operator where $K \in \{2,6,10,20\}$, combined with the top-$2$ principal components of the nodes features $X$ per \eqref{eqn:node_sp_emb}, and $c_{\text{SE}}$ as a multilayer-perception with 1-hidden layer. We choose $\phi_{\text{GNN}}$ as a degree-2 polynomial graph filter with 2 layers, and $c_{\text{GNN}}$ as a linear layer. All methods are trained with full-batch gradient descent, using learning rate $0.02$ for $200$ epochs (with early stopping if the loss has converged) and dropout probability $0.5$. For GNNs, We use PReLU nonlinearity (i.e., ReLU with a learnable parameter for negative inputs).

\subsection{Experiment Details for Sec. \ref{sec:exp-real}} \label{sec:app2}


\textbf{Data.} The Chameleon webpage network has $2277$ nodes with average node degree $13.8$, where nodes represent webpages and edges are hyperlinks between them. The node features are $2325$-dimensional bag-of-words vectors of the webpages, and node labels are $5$ webpage categories. We use the same data splits (48/32/20 for train/validation/test) from \cite{Pei2020Geom-GCN} released in Pytorch Geometric \cite{Fey2019pytorchgeo}.

\noindent \textbf{Methods.} We use the similar setup as described in Section \ref{sec:app1}, except using SE dimension $K=\kappa \in \{150, 200 \}$, and learning rate $0.01$.


\begin{comment}
\subsection{Comparison of Graph Operators $\bbA, \tilde{\bbA}$}

For completeness, we also conduct the experiments in Section \ref{sec:exp} using the graph adjacency matrix $\bbA$. Fig \ref{fig:compare}, Table \ref{tab:compare} compare the performance of $\bbA, \tilde{\bbA}$ on the DSGM simulation and Amazon Photo experiment respectively: using $\tilde{\bbA}$ as the graph operator yields (slightly) better performance than $\bbA$.

\begin{figure}[t]
  \centering
\includegraphics[width=0.5\textwidth]{sampling/experiment_comp.png}
 \caption{DSGM Simulation: $\tilde{\bbA}$ yields slightly better performance than $\bbA$.}
  \label{fig:compare}
\end{figure}


\begin{table}[htb!]
\scriptsize
\caption{Test accuracy on Chameleon and its sparsified graphs, reported as \texttt{mean($\pm$stderr)} across 10 data splits and 10 sparsified subgraphs.}
\label{tab:compare}
\centering
\begin{tabular}{cccccc}
\hline \hline
Graph & Operator	&	SE(150)	&	SE(200)	&	GNN(lin)	&	 GNN(non) 	\\
\hline \hline
\multirow{2}{*}{Original} & $\bbA$ 	&	\textbf{57.29 $\pm$ 0.69}	&	56.97 $\pm$ 0.59	&	56.27 $\pm$ 0.69	&	54.38 $\pm$  0.97	\\
 & $\tilde{\bbA}$		&	52.70 $\pm$ 0.36	&	53.84 $\pm$ 0.43	&	55.60 $\pm$  0.70	&	\textbf{55.90 $\pm$ 0.73  }	\\

 \hline
\multirow{2}{*}{Drop(20)} & $\bbA$ &	53.20 $\pm$ 0.21	&	53.30 $\pm$  0.22	&	\textbf{53.91 $\pm$  0.25}	&	52.69 $\pm$ 0.29	\\
	&  $\tilde{\bbA}$ &	49.42 $\pm$ 0.21	&	51.53 $\pm$  0.19		& 54.45	 $\pm$ 0.21 &	\textbf{54.66 $\pm$ 0.22 }	\\

\hline
\multirow{2}{*}{Drop(50)}& $\bbA$	&	48.48 $\pm$ 0.22	&	48.22 $\pm$ 0.21	&	\textbf{48.90 $\pm$ 0.25}	&	48.28 $\pm$ 0.25	\\ 
&$\tilde{\bbA}$ &44.69 $\pm$ 0.27	&	45.73 $\pm$ 0.25	&	52.06 $\pm$	0.22 &\textbf{52.15 $\pm$ 0.21}\\  
\hline
\multirow{2}{*}{Drop(70)}  & $\bbA$	&	45.47 $\pm$ 0.20	&	45.12 $\pm$ 0.22	&	\textbf{46.21 $\pm$ 0.23}	&	45.95 $\pm$ 0.24	\\ 
	&$\tilde{\bbA}$ & 41.21 $\pm$ 0.19	&	42.51 $\pm$ 0.27	&	50.10 $\pm$	0.19 &\textbf{50.25 $\pm$ 0.21}\\
\hline \hline
\end{tabular}
\end{table}



\subsection{Stability: Graph Density}

We investigate the stability of GNNs versus SE with respect to the changes in graph density. We vary the block matrix $B$ in \eqref{eqn:block} as follows:
dense $p=1/4, q=1/6$; moderately sparse $p=1/10, q=1/15$; sparse $p=1/40, q=1/60$. We set $n=1000$ and draw $10$ random graphs for each density setting. Fig. \ref{fig:sparsity} illustrates that GNNs are stable with respect to graph density change, and outperform SE in sparse graphs.

\begin{figure}[htb!]
\begin{minipage}[b]{1.0\linewidth}
  \centering
  \centerline{\includegraphics[width=6cm]{experiment/sparsity.png}}
%  \vspace{2.0cm}
\end{minipage}
%
 \caption{GNNs are stable w.r.t graph density change, and outperform SE in sparse graphs.}
  \label{fig:sparsity}
\end{figure}

\subsection{Transferability: Graph Perturbation}

\textbf{Robust inference problem.} Equipped with the inference problem set-up, we now consider its robust version where $f$ is trained from the original clean graph $G$, but tested on the perturbed graphs $G'$. Examples of robust inference include white-box adversarial attacks to the graph structure (without modifying $f$), etc (?!). Equivalently, we can view this as examining the transferability of $f$ (from clean graphs to perturbed graphs).

\noindent
\textbf{Methods.} Once GNNs learn $f_{\text{GNN}}$ that takes in $G=(A,X)$, the same function can be effortlessly used for perturbed graph $G'=(A',X)$ (here we only restrict to perturbation on graph topology structure, without modifying node features). However, SE only learns $c_{\text{SE}}$, and thus when given a new graph $G'$ (assuming the vertices in $G,G'$ are matched a priori), it requires two additional steps:
\begin{enumerate}
    \item Embed the new graph $G'$ to obtain $h'_{\text{SE}}$;
    \item Align $h'_{\text{SE}}, h_{\text{SE}}$ via Procrustes analysis to obtain $h'_{\text{SP\_align}}$.
\end{enumerate}
Step 2 is necessary due to the non-identifiability of the eigen-basis (up to orthogonal transform). Then we can use the original classifier $c_{\text{SE}}$ on the aligned SE $h'_{\text{SP\_align}}$.

Fig. \ref{fig:perturb} shows that GNNs are transferable, and more stable than SE against relative large perturbation (See (a), XL). On the other hand, SEs can also transfer from original to perturbed graphs, albeit suffer from higher computational costs and lower stability (i.e., increased variance and faster degradation of performance).

Remark 1: If the new graph has the same vertex set with the original graph (and they are aligned), then we can essentially ignore the new graph while keep using the original $f_{\text{SE}} = c_{\text{SE}} \circ h_{\text{SE}}$, as the SE $h_{\text{SE}}$ is fixed and can be viewed as trained weights.

Remark 2: If the new graph is drawn from the same graphon but with different sizes than the original graph, then we must re-embed $G'$ and perform the alignment step 2 (which actually includes both a vertex alignment component and a procrutes analysis). This can be carried out using Optimal Transport Procrustes analysis \cite{chung2022valid}.

\begin{figure}[htb!]
\begin{minipage}[b]{1.0\linewidth}
  \centerline{\includegraphics[width=6cm]{experiment/add_triangel_remove_edge.png}}
%  \vspace{2.0cm}
  \centerline{(a) AddTriangle + DropEdge}\medskip
\end{minipage}
%
\begin{minipage}[b]{.48\linewidth}
  \centering
  \centerline{\includegraphics[width=4cm]{experiment/add_triangle.png}}
%  \vspace{1.5cm}
  \centerline{(b) AddTriangle}\medskip
\end{minipage}
\hfill
\begin{minipage}[b]{0.48\linewidth}
  \centering
  \centerline{\includegraphics[width=4cm]{experiment/remove_edge.png}}
%  \vspace{1.5cm}
  \centerline{(c) DropEdge}\medskip
%
\end{minipage}
%
 \caption{GNNs are stable w.r.t graph perturbation, and outperform SE when perturbation is large.}
  \label{fig:perturb}
\end{figure}


%\begin{comment}
These guidelines include complete descriptions of the fonts, spacing, and
related information for producing your proceedings manuscripts. Please follow
them and if you have any questions, direct them to Conference Management
Services, Inc.: Phone +1-979-846-6800 or email
to \\\texttt{papers@2021.ieeeicassp.org}.

\section{Formatting your paper}
\label{sec:format}

All printed material, including text, illustrations, and charts, must be kept
within a print area of 7 inches (178 mm) wide by 9 inches (229 mm) high. Do
not write or print anything outside the print area. The top margin must be 1
inch (25 mm), except for the title page, and the left margin must be 0.75 inch
(19 mm).  All {\it text} must be in a two-column format. Columns are to be 3.39
inches (86 mm) wide, with a 0.24 inch (6 mm) space between them. Text must be
fully justified.

\section{PAGE TITLE SECTION}
\label{sec:pagestyle}

The paper title (on the first page) should begin 1.38 inches (35 mm) from the
top edge of the page, centered, completely capitalized, and in Times 14-point,
boldface type.  The authors' name(s) and affiliation(s) appear below the title
in capital and lower case letters.  Papers with multiple authors and
affiliations may require two or more lines for this information. Please note
that papers should not be submitted blind; include the authors' names on the
PDF.

\section{TYPE-STYLE AND FONTS}
\label{sec:typestyle}

To achieve the best rendering both in printed proceedings and electronic proceedings, we
strongly encourage you to use Times-Roman font.  In addition, this will give
the proceedings a more uniform look.  Use a font that is no smaller than nine
point type throughout the paper, including figure captions.

In nine point type font, capital letters are 2 mm high.  {\bf If you use the
smallest point size, there should be no more than 3.2 lines/cm (8 lines/inch)
vertically.}  This is a minimum spacing; 2.75 lines/cm (7 lines/inch) will make
the paper much more readable.  Larger type sizes require correspondingly larger
vertical spacing.  Please do not double-space your paper.  TrueType or
Postscript Type 1 fonts are preferred.

The first paragraph in each section should not be indented, but all the
following paragraphs within the section should be indented as these paragraphs
demonstrate.

\section{MAJOR HEADINGS}
\label{sec:majhead}

Major headings, for example, "1. Introduction", should appear in all capital
letters, bold face if possible, centered in the column, with one blank line
before, and one blank line after. Use a period (".") after the heading number,
not a colon.

\subsection{Subheadings}
\label{ssec:subhead}

Subheadings should appear in lower case (initial word capitalized) in
boldface.  They should start at the left margin on a separate line.
 
\subsubsection{Sub-subheadings}
\label{sssec:subsubhead}

Sub-subheadings, as in this paragraph, are discouraged. However, if you
must use them, they should appear in lower case (initial word
capitalized) and start at the left margin on a separate line, with paragraph
text beginning on the following line.  They should be in italics.

\section{PRINTING YOUR PAPER}
\label{sec:print}

Print your properly formatted text on high-quality, 8.5 x 11-inch white printer
paper. A4 paper is also acceptable, but please leave the extra 0.5 inch (12 mm)
empty at the BOTTOM of the page and follow the top and left margins as
specified.  If the last page of your paper is only partially filled, arrange
the columns so that they are evenly balanced if possible, rather than having
one long column.

In LaTeX, to start a new column (but not a new page) and help balance the
last-page column lengths, you can use the command ``$\backslash$pagebreak'' as
demonstrated on this page (see the LaTeX source below).

\section{PAGE NUMBERING}
\label{sec:page}

Please do {\bf not} paginate your paper.  Page numbers, session numbers, and
conference identification will be inserted when the paper is included in the
proceedings.

\section{ILLUSTRATIONS, GRAPHS, AND PHOTOGRAPHS}
\label{sec:illust}

Illustrations must appear within the designated margins.  They may span the two
columns.  If possible, position illustrations at the top of columns, rather
than in the middle or at the bottom.  Caption and number every illustration.
All halftone illustrations must be clear black and white prints.  Colors may be
used, but they should be selected so as to be readable when printed on a
black-only printer.

Since there are many ways, often incompatible, of including images (e.g., with
experimental results) in a LaTeX document, below is an example of how to do
this \cite{Lamp86}.

\section{FOOTNOTES}
\label{sec:foot}

Use footnotes sparingly (or not at all!) and place them at the bottom of the
column on the page on which they are referenced. Use Times 9-point type,
single-spaced. To help your readers, avoid using footnotes altogether and
include necessary peripheral observations in the text (within parentheses, if
you prefer, as in this sentence).

% Below is an example of how to insert images. Delete the ``\vspace'' line,
% uncomment the preceding line ``\centerline...'' and replace ``imageX.ps''
% with a suitable PostScript file name.
% -------------------------------------------------------------------------
\begin{figure}[htb]

\begin{minipage}[b]{1.0\linewidth}
  \centering
  \centerline{\includegraphics[width=8.5cm]{image1}}
%  \vspace{2.0cm}
  \centerline{(a) Result 1}\medskip
\end{minipage}
%
\begin{minipage}[b]{.48\linewidth}
  \centering
  \centerline{\includegraphics[width=4.0cm]{image3}}
%  \vspace{1.5cm}
  \centerline{(b) Results 3}\medskip
\end{minipage}
\hfill
\begin{minipage}[b]{0.48\linewidth}
  \centering
  \centerline{\includegraphics[width=4.0cm]{image4}}
%  \vspace{1.5cm}
  \centerline{(c) Result 4}\medskip
\end{minipage}
%
\caption{Example of placing a figure with experimental results.}
\label{fig:res}
%
\end{figure}


% To start a new column (but not a new page) and help balance the last-page
% column length use \vfill\pagebreak.
% -------------------------------------------------------------------------
%\vfill
%\pagebreak

\section{COPYRIGHT FORMS}
\label{sec:copyright}

You must submit your fully completed, signed IEEE electronic copyright release
form when you submit your paper. We {\bf must} have this form before your paper
can be published in the proceedings.

\section{RELATION TO PRIOR WORK}
\label{sec:prior}

The text of the paper should contain discussions on how the paper's
contributions are related to prior work in the field. It is important
to put new work in  context, to give credit to foundational work, and
to provide details associated with the previous work that have appeared
in the literature. This discussion may be a separate, numbered section
or it may appear elsewhere in the body of the manuscript, but it must
be present.

You should differentiate what is new and how your work expands on
or takes a different path from the prior studies. An example might
read something to the effect: "The work presented here has focused
on the formulation of the ABC algorithm, which takes advantage of
non-uniform time-frequency domain analysis of data. The work by
Smith and Cohen \cite{Lamp86} considers only fixed time-domain analysis and
the work by Jones et al \cite{C2} takes a different approach based on
fixed frequency partitioning. While the present study is related
to recent approaches in time-frequency analysis [3-5], it capitalizes
on a new feature space, which was not considered in these earlier
studies."


List and number all bibliographical references at the end of the
paper. The references can be numbered in alphabetic order or in
order of appearance in the document. When referring to them in
the text, type the corresponding reference number in square
brackets as shown at the end of this sentence \cite{C2}. An
additional final page (the fifth page, in most cases) is
allowed, but must contain only references to the prior
literature.
\end{comment}


\end{document}
