
\subsection{Robustness to linguistic bias}

Linguistic biases are prevalent in natural language, which can subtly change the emotions and stances of the text~\citep{field2018framing, ziems2021protect}. Pre-trained language models have also been found to be affected by subtle linguistic perturbations~\citep{kojima2022large} and hints~\citep{patel2021stated}.
% Given that most modern natural language processing models are built on top of these pre-trained language models, they are likely to be susceptible to linguistic biases. 

In this section we investigate how different models are affected by these linguistic biases in inputs. To this end, we experiment on 3 categories of linguistic biases. \textit{Punctuated}: inspired by discussions about linguistic hints in~\cite{patel2021stated}, we append punctuation such as ``?'' and ``...'' to the input in order to change its underlying tone. \textit{Hinted}: we change the joining character from ``\texttt{|}'' to phrases with doubting hints such as ``is claimed to be''. \textit{Verbose}: Transformer-based models are found to attend on a local window of words~\citep{child2019generating}, so we append a long verbose sentence ($\approx$ 30 words) to the input sentence to perturb the attention mechanism.

Results are presented in Figure~\ref{fig:robustness}. Compared with the original scores without linguistic biases (the horizontal lines), \model{}'s performance is not significantly affected. But ExEnt appears to be susceptible to these biases with a large drop in performance. This result demonstrates that ExEnt also inherits the sensitivity to these linguistic biases from its PLM backbone. By contrast, \model{} is encouraged to explicitly parse explanations into its logical structure and conduct compositional logical reasoning. This provides better inductive bias for classification, and regulates the model from leveraging subtle linguistic patterns .