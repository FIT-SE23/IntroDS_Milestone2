\subsection{Datasets}

\paragraph{CUB-Explanations} We build a CUB-Explanations dataset based on image classification dataset CUB-200-2011 dataset~\citep{wah2011caltech}. 
CUB-200-2011 originally includes $\sim$ 12k images involving 200 categories of birds, 150 categories are used for training and other 50 categories are left for zero-shot image classification.
In this work, we focus on the setting of zero-shot classification using natural language explanations.
Natural language explanations are more efficient to collect than the crowd-sourced attribute annotations. They are also similar to human learning process, and would be more challenging for models to utilize.
To this end, we collect natural language explanations of each bird category from Wikipedia. These explanations come from the short description part\footnote{As defined in \url{ https://en.wikipedia.org/wiki/Wikipedia:Short_description}} and the \textit{Description}, \textit{Morphology} or \textit{Identification} sections in the Wikipedia pages.
We mainly focus on the sentences that describe visual attributes that can be recognized in images (e.g. body parts, visual patterns and colors).
Finally we get 1$\sim$8 explanation sentences for each category with a total of 991 explanations.
%CUB-200-2011 originally includes $\sim$ 12k images involving 200 categories of birds, and evaluates models' performance on image classification. In zero-shot classification setting, data of 150 classes are provided for training and other 50 classes are left for evaluation.
% \heng{\sout{so you actually enriched this dataset? add it into contributions}}

% To get input features $X$ for \model{}, we use a pretrained visual encoder (we experiment with both ResNet-101~\citep{he2016deep} and CLIP~\citep{radford2021learning}) to obtain image patch representation vectors. These vectors are then flattened as a sequence and used as visual input $X$.
For evaluation, we adopt the three metrics commonly used for generalized zero-shot learning: $ACC_U$ denotes accuracy on unseen categories, $ACC_S$ denotes accuracy on seen categories, and their harmonic average $ACC_\textbf{H}=\frac{2ACC_UACC_S}{ACC_U + ACC_S}$.

\paragraph{ECtHR-Explanations} In legal domain the demand for transparent and robust classification is higher ~\citep{branting2021scalable}. This is also a representative domain where humans learn concepts by reading language definitions.
So we build a variant dataset ECtHR-Explanations based on ECtHR dataset ~\citep{chalkidis2021paragraph}, which is a recent dataset containing allegations of states violating the European Convention of Human Rights (ECHR). 
The dataset contains 11k cases of allegations in total. Each case consists of multiple natural language paragraphs describing the facts in the case, and is mapped to a list of allegedly violated ECHR articles. We use the main text under each ECHR article \heng{\sout{unclear what is language contents, reword it}} \footnote{\url{https://www.echr.coe.int/documents/convention_eng.pdf}} as the explanation sentences, thus the name ECtHR-Explanations.

For evaluation, we adopt a ``leave-one-out'' evaluation method: at each time, one category is left out  for zero-shot evaluation while the rest categories are used for training. Finally we use the averages of F1 scores as evaluation metric.

% Each input is a long document containing multiple paragraphs, so we follow~\cite{chalkidis-etal-2022-lexglue} to use paragraph embeddings as the input features $X$.
%because ROC-AUC score is more stable and does not result in lots of zero scores as F1 metric do.