
\subsection{Interpretability}

% One advantage logical reasoning is that it provides better interpretable intermediate results~\citep{mao2019neuro, yi2018neural}. 
\Model{} is interpretable in two senses: 1) it parses logical structures to explain how the explanations are interpreted, and 2) the logical reasoning   evidence serves as decision making rationales.
To demonstrate the interpretability of \model{}, in Table \ref{tab:interpretability} we present an example of the parsed logical structure and reasoning process. We compare \model{}'s behavior with ExEnt baseline. More interpretability examples and error analysis are listed in Appendix~\ref{appsec:interpretability}.

The upper part of Figure \ref{tab:interpretability} shows that \model{} selects ``\textit{with higher safety}'' and ``\textit{and capacity}'' as concepts candidates, and uses an \texttt{AND} operator over the concepts.
The middle part of the figure visualizes the logical reasoning evidence.
In the example, both two concepts matches with the text span \texttt{safety|high[SEP]person capacity|4[SEP]buying cost|med}, and yield scores of 0.65 and 0.58, respectively. Then the \textit{And} operation outputs classification score of 0.58 ($>$ 0.5, indicating a positive prediction). This example is correctly classified by our model, but mis-classified by the ExEnt baseline. This examplifies the efficacy of \model{} on compositional reasoning on explanations.
To quantitatively evaluate the learned concepts, we manually annotate keyword spans for 100 out of 344 explanations. These spans describe the key attributes for making the explanation.
% Each keyword span is represented as a pair (start position, end position), which are start and end token positions numbers in the explanation sentences\footnote{by using the \texttt{BERT-base-uncased} tokenizer}.
Then we plot the histogram of the relative position between top-attention tokens and annotated keyword spans in Figure~\ref{fig:argument_position}. When there are multiple concepts detected, we select the one closest to the keyword span. From the figure we can see that the majority of top-attention tokens (52\%) fall within the range of annotated keyword spans. 
The ratio increases to 81\% within distance of 5 tokens from the keyword span, and 95\% within distance of 10 tokens.
