\clearpage
\newpage

% \chapter{Appendix}

\section{Appendix}

\subsection{Configuration and Experiment Setting}
\label{appsec:configuration}
We train \model{} for 30 epochs in all experiments. In the image classification task on CUB-Explanations, we adopt a two-phase training paradigm: in the first phase we fix both visual encoders and Explanation encoders in $E_\Phi$, and in the second phase we finetune all parameters in \model{}.

Across experiments in this work we use the AdamW~\citep{loshchilov2017decoupled} optimizer widely adopted for optimizing NLP tasks. For hyper-parameters in most experiments we follow the common practice of learning rate$=3e-5$, $\beta_1=0.9, \beta_2=0.999$, $\epsilon=1e-8$ and weight decay$=0.01$. An exception is the first phase in image classification where, as we fix the input encoder, the learnable parameters become much less. Therefore we use the default learning rate$=1e-3$ in AdamW. For randomness control, we use random seed of 1 across all experiments.

In Figure~\ref{fig:effect_of_complexity}, there are multiple data points at $x$-value of 0. Therefore, the data variance on data at $x=0$ is intrinsic in data, and is unsolvable theoretical for \textit{any} function fitting the data series. This causes the problem when calculating $R^2$ value, as $R^2$ measures the extent to which the data variance are ``explained'' by the fitting function. So $R^2$ can be upper bounded by:
$
    R^2 \leq 1 - \frac{Var_{intrinsic}}{Var_{total}}
$. To deal with this problem when measuring $R^2$ metric, we removed the intrinsic variance in data point set $D$ by replacing data points $(0, y_i)\sim D$ with $(0, \frac{1}{n}\sum_{(0,y_i)\sim D}y_i)$ in both series in Figure~\ref{fig:effect_of_complexity} before calculating $R^2$ value.

\subsection{Logical Structure Templates}
\label{appsec:templates}
As the number of valid logical structure templates grows exponentially with maximal concept numbers $T$, we limit $T$ to a small value, typically 3. We list the logical structure templates in Table~\ref{tab:all_program_templates}.
\input{tables/all_program_templates}

\subsection{Effect of Logical Reasoning Complexity}
\label{appsec:ablation}
We also explores the effect of logical reasoning on model performance. Figure ~\ref{fig:program_length} plots the performance regarding the maximum number of concepts $T$. Generally speaking, when $T$ is larger, \model{} can model more complex logical reasoning process. When $T=1$, the model reduces to a simple similarity-based model without logical reasoning. The figure shows that when $T$ is 2$\sim$3, the model generally achieves the highest performance, which also aligns with our intuition in the section~\ref{sec:approach}. We hypothesize that a maximum logical structure length up to 4 provides insufficient regularization, and \model{} is more likely to overfit the data.

\input{figures/program_length}


\subsection{Resources}

We use one Tesla V100 GPU with 16GB memory to carry out all the experiments. The training time is 1 hour for tabular data classification on CLUES, 2 hours for image classification on CUB-Explanations, and 4.5 hours for text classification on ECtHR-Explanations dataset.

\subsection{Interpretability Results}
\label{appsec:interpretability}

We add more interpretability examples in Table~\ref{tab:program_examples} and ~\ref{tab:execution_evidence}.
\input{tables/program_examples.tex}
\input{tables/execution_evidence}
From the table we can see that this interpretation is readable with some interesting phenomena.
Some parsed logical structures also uses the label names (e.g., \textit{with higher safety} and \textit{with higher safety and capacity}) as concepts. This reflects that the Explanations may not provide complete guidance for classification, so \model{} also partly relies on semantic information from label names.
In the third example, only part of the complete word \textit{WPSL} is used as an concept. We attribute this to that in contextual word embeddings, a token also often includes information from neighbouring words. The contextual embedding of the single token ``\textit{W}'' may contain information of the whole word \textit{WPSL}. The similar phenomenon happens in the 5th example, where \model{} takes \textit{Dermos} as concept from the the word \textit{Dermosan}.

In logical reasoning evidence examples in Table~\ref{tab:execution_evidence}, the detected concepts generally contain the relevant information to $a_l$. For instance, in the first row the concept \texttt{group above 40} selects the text span containing the key information $age | 65$. An interesting phenomenon is when the model selects the label name instead of more specific attribute information as concept. In the first row, the second concept is \texttt{ensures liver} which is directly related to the label \texttt{indian-liver-patient}. In this example, the concept (\texttt{ensures liver}) selects the same text span as the first concept, which is probably due to the model learning the implicit semantic correspondence between this text span and the label.

\input{figures/quantifiers.tex}
\input{EACL_body/4_4_quantifier_learning.tex}