
\label{sec:intro}

\input{figures/teaser}
Humans are capable of understanding new concepts by reasoning on natural language explanations~\citep{chopra2019first, tomasello2009cultural}.
For example, in Figure \ref{fig:teaser}, we can describe that common ravens are ``a kind of large birds with black feathers''. Then when we view a real common raven the first time, we can match its visual appearance with previously learned concepts ``large birds'' and ``black feathers'', and then logically combine these results to recognize it. % This requires learners to logically understand, parse and apply the explanations.
This ability is applicable to both visual objects and abstract concepts~\citep{tomasello2009cultural}. Compared to learning only through examples, using language information enables humans to acquire higher accuracy in less learning time~\citep{chopra2019first}.

One important advantage of learning with natural language explanations is that explanations are often logical and compositional. That is, we can logically combine previously seen concepts such as ``large birds'' and ``black feathers'' to form an explanation for the new category. This enables us to reuse knowledge acquired elsewhere and avoid a ``trial-and-error'' learning process.
Furthermore, learning with explanations provides better interpretability which makes results more trust-worthy. This property is of vital importance in transparency-sensitive domains, such as legal judgement, where some sort of ``rationales'' are preferred by human observers~\citep{branting2021scalable}.

\input{figures/approach}

Recently, there have been research efforts on using language information for task-level generalization. Types of language information include human-annotated explanations or task-level instructions \citep{menon2022clues, sanh2022multitask, mishra2022cross}. However, auxiliary language information are often treated merely as additional text sequences to be fed into pre-trained language models. This approach does not fully leverage the compositional nature of natural language, and does not provide sufficient interpretable rationales for its decisions on the unseen tasks.


Inspired by these observations, in this work we explore classifying unseen categories by logically reasoning on their language explanations. To this end, we propose the framework of Classification by LOgical Reasoning on Explanations (\model{}). 
% \hengzhi{\sout{In the experiment section, I think you use \textit{explanation} more than \textit{explanation}, which may needs to be unified}}\hanchi{\sout{done}}
\model{} works in two stages: it first parses an explanation into a logical structure, and then reasons along this logical structure. Figure~\ref{fig:approach} illustrates an example of classifying common ravens in this way. We first get the logical structure of explanation ((b) $\rightarrow$ (d)). Then we detect if the input matches both concepts ``large birds'' and ``black feathers'' ((c,d)$\rightarrow$(e)). Finally we gather the matching scores along the logical structure to output the overall classification score, which in this case is an AND operator over two concepts.
The whole model is end-to-end trainable with respect to learnable parameters, and can be optimized with classification supervision.


% This is done as sequence-to-sequence generation, where the BERT~\citep{kenton2019bert} embeddings of the language explanation are taken as input for another recurrent neural network (GRU, to be specific) to generate a sequence of program functions. \heng{how is this rnn trained?} 
% An example of the working paradigm is shown in Figure~\ref{fig:approach}.
% The language explanation ``a kind of large birds with black feathers'' can be converted to the program in :
% \heng{\sout{also need to explain/summarize how this executor is being learned?}}
% \begin{equation*}
% \begin{split}
%     & \text{function}_1(X) = \textit{Exist}(\texttt{black feathers}, X) \\
%     & \text{function}_2(X) = \textit{Exist}(\texttt{large}, X) \\
%     & \text{function}_3(X) = \textit{And}(\text{function}_1, \text{function}_2; X) \\
%     & \textbf{return  } \text{function}_3(X)
% \end{split}
% \end{equation*}
% \xd{very repetitive (since you put the same program example in fig 1, 2 and sec approach), you can rm it here and refer to Figure 1 program (e.g., the grey box...) or refer to fig 2 program which is larger.}
%The function$_3$ takes logical AND of function$_1$, function$_2$ as its own function output. The output of the final one function is used as the classification score $p^{i,j}$ for this sub-program. 
% The final function$_3$ takes logical AND of function$_1$, function$_2$ and outputs the classification score $p^{i,j}$ for this sub-program. 
% The weighted average of sub-program scores is then scaled by $c_{certainty}$ and used as the final classification score for $\Lambda$.
% \chihan{The only learnable parameters are in the \textit{Exist} function. It learns to project inputs $x_i$ and argument vectors into a shared space, and use dot-product to detect if there is a match.}
% \heng{\sout{again AND is applied to two functions instead of two scores}}

We conduct a thorough set of analysis on the latest benchmark for zero-shot classifier learning with explanations, CLUES. 
Our analysis shows that \model{} works better than baselines on tasks requiring higher level of compositional reasoning, which validates our model design. \model{} also demonstrates better interpretability and robustness against linguistic biases. Furthermore, as a test on generalizability of the proposed approach on other modalities, we built two new benchmarks on zero-shot classification with explanations: CUB-Explanations and ECtHR-Explanations. They are built upon the image dataset CUB-200-2011~\citep{wah2011caltech} and legal-text dataset ECtHR~\citep{chalkidis2021paragraph}, and we associate each category with a set of language explanations. \model{} consistently outperforms baseline models in zero-shot classification across modalities.

% The rest of the paper is organized as follows:
% Section~\ref{sec:related} will list most related research areas to our work. Then we will describe our proposed approach in Section~\ref{sec:approach}. Finally in Section~\ref{sec:experiments} we will explain experiment settings and give result analysis.
To sum up, our contributions are as follows:
\begin{itemize}
    \item We propose a novel zero-shot classification framework by logically parsing and reasoning over explanations.
    % Empirical results demonstrate its superior performance than baseline models, especially on tasks that require more compositional reasoning.
    
    \item We demonstrate our model's superior performance and explainability, and empirically show that \model{} is more robust to linguistic biases and reasoning complexity than black-box baselines.
    
    \item We demonstrate the universality of the proposed approach by building two new benchmarks, CUB-Explanations and ECtHR-Explanations. They are derived from CUB-200-2011~\citep{wah2011caltech} and ECtHR~\citep{chalkidis2021paragraph} by collecting natural language explanations for each category.
    
\end{itemize}