
\label{sec:conclusions}

In this work, we propose a multi-modal zero-shot classification framework by logical parsing and reasoning on natural language explanations. Our method consistently outperforms baselines across modalities.
% on CUB-200 (image classification), CLUES (structured data classification) and Lex-GLUE and Natural Instructions (text classification).
We also demonstrate that, besides being interpretable, \model{} also benefits more from tasks that require more compositional reasoning, and is more robust against linguistic biases.

There are several future directions to be explored. The most intriguing one is how to utilize pre-trained generative language models for explicit logical reasoning, as pre-trained language models have been shown capable of planning~\citep{DBLP:conf/icml/HuangAPM22}. Another direction is to incorporate semantic reasoning ability in our approach, such as reasoning on entity relations or event roles.