

\subsection{Experiment Setting and Baselines}
% In all experiments, we process the inputs into feature vector sequences $X=(x_1, x_2, \cdots, x_k)$ to provide a uniform representation across modalities.
% On CLUES dataset, this is done by following the data processing in~\citet{menon2022clues} and convert each input into a text sequence. The text sequence is in the form of ``\texttt{odor | pungent [SEP] ... [SEP] ring-type | pendant}'', where ``\texttt{odor}'' is the attribute type name, and ``\texttt{pungent}'' is the attribute value for this input, so on and so forth. Then we encode the sentence with BERT~\citep{kenton2019bert} and use the word embeddings as input features $X$. To set the maximum length of programs, we manually inspect explanations in CLUES dataset, and observe that a maximum length of 5 covers most cases.
On CUB-Explanations dataset, we use a pretrained visual encoder to obtain image patch representation vectors. These vectors are then flattened as a sequence and used as visual input $X$. We use ResNet~\citep{he2016deep} as visual backbone for \model{}.
For baselines, we make comparisons in two groups. The first group of models does not use parameters from pre-trained vision-language models (VLPMs). We adapt TF-VAEGAN~\citep{narayan2020latent}, a state-of-the-art model on the CUB-200 zero-shot classification task, to use RoBERTa-encoded explanations as auxiliary information. This results in the baseline TF-VAEGAN$_{expl}$. The second group of models are those using pre-trained VLPMs. The main baseline we compare with is CLIP~\citep{radford2021learning}, which is a well-performed pretrained VLPM. We build two of its variants: CLIP$_{linear}$, which only fine-tunes the final linear layer and CLIP$_{finetuned}$, which fine-tunes all parameters on the task. For fairer compasion, in this group wealso  replace the visual encoder with CLIP encoder in our model and get \model{}$_{CLIP}$.


On the ECtHR dataset, for baselines, we use Legal-BERT and hierarchical Legal-BERT to encode inputs, which has reported SotA performance on legal-domain datasets \citep{chalkidis-etal-2022-lexglue}. To utilize language explanations, we use another Legal-BERT~\citep{chalkidis2020legal} to encode the explanations, and compute the dot-product between the input embeddings and and explanation embeddings. The maximal dot-product across explanation sentences is used as classification score for each class.
For our model, we follow~\citet{chalkidis-etal-2022-lexglue} and adopt a hierarchical model structure. First the pre-trained language encoder encodes each paragraph as one vector. Then these vectors are concatenated into a vector sequence and used as $X$. To adapt our model to this setting, we also replace the text encoder with Legal-BERT and get \model$_{Legal-BERT}$.