\begin{table}[t!]
\resizebox{\columnwidth}{!}{
\centering
% \small
\linespread{1}

\setlength{\tabcolsep}{1mm}{
\begin{tabular}{c|c|ccc}

\toprule
\textbf{CUB-Explanations} & \textbf{Model} & $ACC_U$ & $ACC_S$ & $ACC_\textbf{H}$ \\
\midrule

\multirow{2}{30mm}{w/o VLPMs}
& TF-VAEGAN$_{\mathit{expl}}$ & 4.7 & 39.1 & 8.3 \\
% & Similarity & 4.7 & \textbf{61.2} & 8.8 \\
& \Model{} (ours) & \textbf{6.6} & \textbf{51.1} & \textbf{11.7} \\

\midrule

\multirow{3}{30mm}{w/ VLPMs}
% & CLIP$_{original}$ & 41.5 & 45.9 & 43.6 \\
& CLIP$_{linear}$ & 34.3 & 41.2 & 37.4 \\
& CLIP$_{finetuned}$ & 29.9 & \textbf{66.9} & 41.3 \\
& \Model{}$_{CLIP}$ (ours) & \textbf{39.1} & 65.8 & \textbf{49.1} \\

% \textbf{Model} & \textbf{\Model{}} & \textbf{Similarity} & ZSL\textunderscore TF-VAEGAN & \textbf{\Model{}} & CLIP \\

\bottomrule

\end{tabular}
}
}

\caption{Generalized zero-shot classification results (in \%) on the new CUB-Explanations dataset.
% $ACC_U$ denotes accuracy on unseen categories, $ACC_S$ denotes accuracy on seen categories, and $ACC_\textbf{H}$ is the harmonic average of the them. 
% The lower and upper parts show models with and without parameters from the Vision Language Pretrained Models (VLPMs).
% TF-VAEGAN$_{def}$ is our re-implementation to adapt to
% \heng{\sout{what do you mean by 'out adaptation'?}}
% CUB-Definitions.
% \Model{}$_{CLIP}$ uses visual and text encoders from CLIP as model backbone.
}
\vspace{-5mm}
\label{tab:cub_main_results}
\end{table}