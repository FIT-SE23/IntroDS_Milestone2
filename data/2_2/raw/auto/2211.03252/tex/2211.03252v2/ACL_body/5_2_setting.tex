

\subsection{Experiment Setting and Baselines}
% In all experiments, we process the inputs into feature vector sequences $X=(x_1, x_2, \cdots, x_k)$ to provide a uniform representation across modalities.
% On CLUES dataset, this is done by following the data processing in~\citet{menon2022clues} and convert each input into a text sequence. The text sequence is in the form of ``\texttt{odor | pungent [SEP] ... [SEP] ring-type | pendant}'', where ``\texttt{odor}'' is the attribute type name, and ``\texttt{pungent}'' is the attribute value for this input, so on and so forth. Then we encode the sentence with BERT~\citep{kenton2019bert} and use the word embeddings as input features $X$. To set the maximum length of programs, we manually inspect explanations in CLUES dataset, and observe that a maximum length of 5 covers most cases.
On CUB-Explanations dataset, we use a pretrained visual encoder to obtain image patch representation vectors. These vectors are then flattened as a sequence and used as visual input $X$. We use ResNet~\citep{he2016deep} as visual backbone for \model{}.
For baselines, we make comparisons in two groups. The first group of models does not use parameters from pre-trained vision-language models (VLPMs). We adapt TF-VAEGAN~\citep{narayan2020latent}\footnote{\url{https://github.com/akshitac8/tfvaegan}}, a state-of-the-art model on the CUB-200 zero-shot classification task, to use RoBERTa-encoded explanations as auxiliary information. This results in the baseline TF-VAEGAN$_{expl}$. The second group of models are those using pre-trained VLPMs. The main baseline we compare with is CLIP~\citep{radford2021learning}\footnote{\url{https://github.com/openai/CLIP}}, which is a well-performed pretrained VLPM. We build two of its variants: CLIP$_{linear}$, which only fine-tunes the final linear layer and CLIP$_{finetuned}$, which fine-tunes all parameters on the task. For fairer compasion, in this group we also replace the visual encoder with CLIP encoder in our model and get \model{}$_{CLIP}$.


% On the ECtHR-Explanations dataset, for baselines, we use Legal-BERT and hierarchical Legal-BERT as main baselines. They have reported SotA performance on legal-domain datasets \citep{chalkidis-etal-2022-lexglue}. We use two Legal-BERT encoders to encode the explanations and inputs individually, and compute the dot-product between them. The maximal dot-product across explanation sentences is used as classification score for each class. As classification based on explanations shares similarity with natural language inference (NLI), we also add two NLI baselines. BART-large-MNLI~\citep{lewis2020bart} is a powerful model pretrained on NLI tasks, and we fine-tune it on ECtHR-Explanations. T0~\citep{sanh2021multitask} is a strong large pretrained language model capable of a series of tasks, and is used as a zero-shot baseline here without further training due to its large size.
% For \model{}, we follow~\citet{chalkidis-etal-2022-lexglue} and adopt a hierarchical model structure. First the pre-trained language encoder encodes each paragraph as one vector. Then these vectors are concatenated into a vector sequence and used as $X$. To adapt our model to this setting, we also replace the text encoder with Legal-BERT and get \model$_{Legal-BERT}$.