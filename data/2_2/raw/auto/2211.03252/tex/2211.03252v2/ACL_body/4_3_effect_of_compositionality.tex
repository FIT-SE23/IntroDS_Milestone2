

\subsection{Effect of Explanation Compositionality}
% \hanchi{\sout{Heng: add a table to show complex/simple examples}}
What causes the difference in performance between \model{} and baselines? To answer this question, we investigate into how the models' performance varies with the compositionality of each task on CLUES.
Table~\ref{tab:compositional_example} provides a pair of examples. An explanations is called ``simple explanation'' if it only describes one attribute, e.g., ``If safety is high, then the car will not be unacceptable.''. Other explanations describe multiple attributes to define a class, e.g., ``Cars with higher safety and medium luggage boot size
are highly acceptable for resale.''. 
We define the latter type as ``compositional explanation''.
% We hypothesize that tasks with higher ratio of compositional explanations require more complex reasoning, 
% \heng{\sout{this point is very good, and needs to be mentioned in the intro}}
% which causes the performance difference between \model{} and ExEnt.
In Figure~\ref{fig:effect_of_complexity} we plot the classification accuracy against the proportion of compositional explanations in each subtask's explanation set.
% \hengzhi{\sout{Could it be a problem that \model{} performance is just stable but not increase as the ratio of compositional explanations increases?}}
% \hanchi{\sout{Good question. How about this explanation?}}
Intuitively, with more compositional explanations, the difficulty of the task increases, so generally we should expect a drop in performance.
Results show that, on tasks with only simple explanations (x-value = 0), both models perform similarly. However, with higher ratio of compositional explanations, \Model{}'s performance generally remains stable, but ExEnt's performance degrades. This validates our hypothesis that \model{}'s performance gain mainly benefits from its better compositional reasoning power.



% \subsection{Effect of Logical Reasoning Complexity}
% \label{appsec:ablation}
To further explore the effect of logical reasoning on model performance. Figure ~\ref{fig:program_length} plots the performance regarding the maximum number of attributes $T$. Generally speaking, when $T$ is larger, \model{} can model more complex logical reasoning process. When $T=1$, the model reduces to a simple similarity-based model without logical reasoning. The figure shows that when $T$ is 2$\sim$3, the model generally achieves the highest performance, which also aligns with our intuition in the section~\ref{sec:approach}. We hypothesize that a maximum logical structure length up to 4 provides insufficient regularization, and \model{} is more likely to overfit the data.