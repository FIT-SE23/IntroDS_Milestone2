
\subsection{CLUES benchmark}
CLUES is designed as a cross-task generalization benchmark on structured data classification. It consists of 36 real-world and 144 synthetic multi-class classification tasks, respectively.
% \heng{what is the data modality for this data set initially?}\hanchi{This dataset is on structured data upon construction. Do you see some problem in adopting this setting?}
The model is given a set of tasks for learning, and then evaluated on a set of unseen tasks.
The inputs in each task constitute a structured table.
Each column represents an attribute type, and each row is one input datum. In each task, for each class, CLUES provides a set of natural language explanations.
% For example, in the ``Mushroom'' task, the columns  are ``odor'', ``stalk-surface-above-ring'', ``gill-color'' and ``ring-type''. The explanations are sentences including \textit{Foul smelling are Poisonous. 7 of 7 rows,with no deviation.}
% We follow the data pre-processing in~\cite{menon2022clues} and convert each input into a text sequence. The text sequence is in the form of ``\texttt{odor | pungent [SEP] ... [SEP] ring-type | pendant}'', where ``\texttt{odor}'' is the attribute type name, and ``\texttt{pungent}'' is the attribute value for this input, and so on.
% Then we encode the sentence with BERT~\citep{kenton2019bert} as inputs $X$.

We follow the data processing in~\citet{menon2022clues} and convert each input into a text sequence. The text sequence is in the form of ``\texttt{odor | pungent [SEP] ... [SEP] ring-type | pendant}'', where ``\texttt{odor}'' is the attribute type name, and ``\texttt{pungent}'' is the attribute value for this input, so on and so forth. For \model{}, we encode the sentence with RoBERTa~\citep{liu2019roberta}\footnote{\url{https://huggingface.co/roberta-base}} and use the word embeddings as input features $X$. More implementation details can be found in Appendix~\ref{appsec:configuration}. We use ExEnt as a baseline, which is an text entailment model introduced in the CLUES paper. ExEnt uses pre-trained RoBERTa as backbone. It works by encoding concatenated explanations and inputs, and then computing an entailment score. We also introduce a similarity-based baseline, RoBERTa-sim, which uses cosine between RoBERTa-encoded inputs and explanations as classification scores. Finally, we compare with \model{}-plain as an ablation study, which ignores the logical structure in \model{} and plainly addes all attribute scores as the overall classification scofre.
\heng{\sout{make it clear you are using ExEnt as baseline}}