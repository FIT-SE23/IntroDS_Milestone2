Humans can classify data of an unseen category by reasoning on its language explanations. This ability is owing to the compositional nature of language: we can combine previously seen attributes to describe the new category. For example, we might describe a sage thrasher
% \zhenhailong{ravens? (to be consistent with the intro)}
as "it has a slim straight relatively short bill, yellow
eyes and a long tail", so that others can use their knowledge of attributes ``slim straight relatively short bill'', ``yellow eyes'' and ``long tail'' to recognize a sage thrasher.
% \heng{\sout{you can consider revising it to be consistent with the example in Figure 1}}
Inspired by this observation, in this work we tackle zero-shot classification task by logically parsing and reasoning on natural language explanations. To this end, we propose the framework \model{} (Classification by LOgical Reasoning on Explanations). While previous methods usually regard textual information as implicit features, \model{} parses explanations into logical structures and then explicitly reasons along thess structures on the input to produce a classification score.
Experimental results on explanation-based zero-shot classification benchmarks demonstrate that \model{} is superior to baselines,
which we further show mainly comes from higher scores on tasks requiring more logical reasoning.
We also demonstrate that our framework can be extended to zero-shot classification on visual modality.
Alongside classification decisions, \model{} can provide the logical parsing and reasoning process as a clear form of rationale.
Through empirical analysis we demonstrate that \model{} is also less affected by linguistic biases than baselines.
% \heng{do you have results for this?} \hanchi{We do but the numbers are small, like 2\% for structured data, 3-8\% for text data and 1-4\% for visual data. Should we add them?}
% \yi{Need to finish converting template from EACL to ACL}
% \yi{\sout{is multimedia capability worthile to mention in abstract?}}