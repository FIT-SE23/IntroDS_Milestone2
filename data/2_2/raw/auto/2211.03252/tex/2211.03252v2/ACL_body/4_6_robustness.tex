
\subsection{Robustness to linguistic bias}

Linguistic biases are prevalent in natural language, which can subtly change the emotions and stances of the text~\citep{field2018framing, ziems2021protect}. Pre-trained language models have also been found to be affected by subtle linguistic perturbations~\citep{kojima2022large} and hints~\citep{patel2021stated}.
% Given that most modern natural language processing models are built on top of these pre-trained language models, they are likely to be susceptible to linguistic biases. 

In this section we investigate how different models are affected by these linguistic biases in inputs. To this end, we experiment on 3 categories of linguistic biases. \textit{Punctuated}: inspired by discussions about linguistic hints in~\cite{patel2021stated}, we append punctuation such as ``?'' and ``...'' to the input in order to change its underlying tone. \textit{Hinted}: we change the joining character from ``\texttt{|}'' to phrases with doubting hints such as ``is claimed to be''. \textit{Verbose}: Transformer-based models are found to attend on a local window of words~\citep{child2019generating}, so we append a long verbose sentence ($\approx$ 30 words) to the input sentence to perturb the attention mechanism. These changes are automatically applied.
% \yi{Are these automatically or manually done?}

Results are presented in Figure~\ref{fig:robustness}. Compared with the original scores without linguistic biases (the horizontal lines), \model{}'s performance is not significantly affected. But ExEnt appears to be susceptible to these biases with a large drop in performance. This result demonstrates that ExEnt also inherits the sensitivity to these linguistic biases from its PLM backbone. By contrast, \model{} is encouraged to explicitly parse explanations into its logical structure and conduct compositional logical reasoning. This provides better inductive bias for classification, and regulates the model from leveraging subtle linguistic patterns.




\subsection{Linguistic Quantifier Understanding} Linguistic quantifiers is a topic to understand the degree of certainty in natural language~\citep{srivastava2018zero, yildirim2013linguistic}. For example, humans are more certain when saying something \textit{usually} happens, but less certain when using words like \textit{sometimes}.
We observe that the certainty coefficient $c_{certainty}$ that \model{} learns can naturally serve the purpose the of modelling quantifiers.
% \hengzhi{\sout{do we have a explanation for certainty coefficient $c_{certainty}$ before?}}\chihan{\sout{On I forgot. I added this one in Section~\ref{sec:approach}.}}
We first detect the existence of linguistic quantifiers like \textit{often} and \textit{usually} by simply word matching. % listed in Table~\ref{tab:quantifier_extraction}.
Then we take the average of $c_{certainty}$ on the matched explanations. We plot these values against expert-annotated ``quantifier probabilities'' in~\citep{srivastava2018zero}
% , which indicates how ``probable'' the sentence is supposed to be true with a certain quantifier.
in Figure~\ref{fig:quantifiers}. Results show that $c_{certainty}$ correlates positively with ``quantifier probabilities'' with Pearson correlation coefficient value of 0.271. In cases where they disagree, our quantifier coefficients also make some sense, such as assigning \textit{often} a relatively higher value but giving \textit{likely} a lower value.
% \heng{\sout{this paragraph is very hard to follow, and unclear what the point is}}
\hengzhi{and do we need a comparison for this part using the baseline?}
\chihan{I can only think of a most naive way to do this. Let me try it in the next days}