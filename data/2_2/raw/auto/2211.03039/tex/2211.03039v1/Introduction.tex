\section{Introduction}
% BERT+CRF需要重新训练全连接层，不管是在source domain还是target domain
% 基于BART生成打分的方法需要计算每个token的概率且只生成non-entity spans的负例
% 我们的方法还可以利用MNLI的数据进行迁移，尤其当source也没有很多标注样本时
Recently, Pre-trained Language Models (PLMs) have achieved promising improvement on several NLP tasks~\cite{bert,roberta,lan2019albert}. Nevertheless, fine-tuning language models still needs a moderate number of labeled data for downstream tasks. When difficulties result in limited labeled data available, the trained model shows large variance in downstream performance under full fine-tuning~\cite{DBLP:conf/iclr/MosbachAK21,huggface_prompt}. For example, labeling technical and professional terms can be time-consuming and labor-intensive in medical scenarios. Moreover, crowd-sourced annotation is also limited by the reality of existing samples (e.g., when online health assistants are applied to rare diseases).

To address learning challenges in these low-resource scenarios, researchers find that PLMs can learn well by prompt-based learning~\cite{schick2020exploiting,schick2020s,ADAPET}. Prompt-based learning models the probability of text directly; it does not need an extra fully-connected layer usually used by fine-tuning. The main idea is to reformulate NLP tasks as cloze-style question answering for better using the knowledge in PLMs. The model predicts the word probability of masked positions and then derives the final output via mapping relations between words and labels. Previous works have shown the ability of prompt-based learning under low-resource settings~\cite{schick2020exploiting,schick2020s,schick-etal-2020-automatically,Lester}. For example,
% For more details, we refer the reader to survey~\cite{pengfei,DBLP:journals/corr/abs-2111-01243}.
some prompt-based works have explored in classification and generation tasks where it is relatively easy to reformulate into cloze-style tasks (cf. Section~\ref{sec:related}). Nevertheless, the application to Named Entity Recognition (NER) still poses challenges for current methods. Unlike text classification and text generation, NER is the task of identifying named entities (e.g., \textit{person name}, \textit{location}) in a given sentence, and each unit of the input needs to be predicted. If we directly use Masked Language Modeling (MLM) head to predict each unit label, the lexical and semantic coherence are ignored as there exists latent relationships between the tokens~\cite{lample2016neural,elmo,TENER}.  


In this work, we propose \textbf{Prompt-based Text Entailment} (\texttt{PTE}) for low-resource NER. Firstly, we reformulate NER as a \textit{text entailment} task.
Textual Entailment (TE) is the task of studying the relation of two sentences, Premise (P) and Hypothesis (H): whether H is true given P~\cite{snli}. Specifically, we treat the original sentence as premise and entity type-specific prompt as a hypothesis. Given an entity type, the P and H are fed into PLMs to get entailment scores for each candidate. Then, the entailment score is the probability of a specific token at the mask position of the prompt. After that, the entity type with the top entailment score is selected as the final label.  
During inference, we enumerate all possible text spans or words in the input sentence as named entity candidates~\cite{templatener}. The reformulation provides a unified entailment framework for NER tasks where annotations are insufficient, as the model shares the same inference pattern across different domains. As such, we can also leverage generic text entailment datasets such SNLI~\cite{snli} and MNLI~\cite{mnli} to pre-train models, which transfer knowledge from the general domain and get better performance in new domains. Our method can be a step forward towards the development of a solution for the low-resource NER because any new domain does not typically have extensive annotated data in the real world, whereas it is feasible to obtain a couple of examples (e.g., online assistant). Moreover, considering the existence of noisy annotations, \textit{TE only needs to specify the labels of certain entities for training rather than the complete annotations of the entire sequence}.
Experimental results demonstrate that the proposed method \texttt{PTE} achieves competitive F1 score on the CoNLL03 dataset~\cite{conll03}, and better than fine-tuned counterparts by a large margin on the MIT Movie~\cite{mit-dataset} and Few-NERD datasets~\cite{fewnerd} in low-resource settings.

