
\begin{table}[t!]
\centering
    \resizebox{\linewidth}{!}{
\begin{tabular}{llccccc}
\toprule
\textbf{Datasets} & \textbf{Domain} & \textbf{\# Type} &  \textbf{\# Tokens} & \textbf{\# Train} & \textbf{\# Dev} & \textbf{\# Test} \\
\midrule
CoNLL03 & Reuters news stories & 4 & 21.0k & 14041 & 3250 & 3453 \\
MIT Movie & Movie reviews & 12 & 6.0k & 7820 & 1955 & 2443\\
Few-NERD & Wikipedia & 8 & 4601.2k & 131767 & 18824 & 37648 \\
\bottomrule

\end{tabular}}
\caption{Statistics of our datasets. We count the number of sentences in the training/development/test set, the number of tokens and the number of tags in datasets.}
\label{tab:dataset}
\end{table}

\begin{table}[t!]
\small
    \centering
    \resizebox{\linewidth}{!}{
    \begin{tabular}{l|l}
    \hline
        \textbf{Type} & \textbf{Templates} \\ 
    \hline
     Positive (Y) & <candidate> is the part of a <entity\_type> entity. \\
     \hline
     False positive (N) & <candidate> is the part of a <another\_entity\_type> entity. \\
     \hline
     Non-entity (N) &  <others> is the part of a <entity\_type> entity. \\
     \hline
      \multirow{2}{*}{Null label (Y/N)} & <others> is not a name entity. \\
      & <candidate> is not a name entity.\\
     \hline
 
    \end{tabular}
    }
    \caption{The discrete manually-crafted templates.}
    \label{tab:template}
\end{table}

\begin{table}[t!]
\small
    \centering
    \resizebox{\linewidth}{!}{
    \begin{tabular}{l|l}
    \hline
      \textbf{Number}  & \textbf{Patterns} \\ 
    \hline
     Pattern\#1 & [HYPOTHESIS] ? </s></s> [MASK], [PREMISE] </s> \\
     \hline
      Pattern\#2 & `` [HYPOTHESIS] '' ? </s></s> [MASK], `` [PREMISE] '' </s> \\
     \hline
      Pattern\#3 & [HYPOTHESIS] ? </s></s> [MASK]. [PREMISE] </s> \\ 
     \hline
       Pattern\#4 & `` [HYPOTHESIS] '' ? </s></s> [MASK]. `` [PREMISE] '' </s> \\
     \hline
 
    \end{tabular}
    }
    \caption{We list the patterns used by our method where <s> and </s> are start token and separated token.}
    % sep_token='</s>', cls_token='<s>',
    \label{tab:pattern}
\end{table}
\begin{figure}[t!]
\centering
\includegraphics[width=0.8\linewidth]{pattern.pdf}
  \caption{The performance of different modes after up to 7000 training batches. The patterns we use are from RTE task of \citet{ADAPET}. }
  \vspace{-4mm}
   \label{figpatt}

\end{figure}
\appendix
\label{sec:supplemental}




\section{Templates}
\label{sec:templ}

We use the naive random sampling method and the positive-negative ratio is 1:1.5 in the low-resource scenario after sampling. 
As shown in Table~\ref{tab:template}, we list our templates for each example type used by \texttt{PTE} (discrete). Our soft prompt is to add different special tokens before the [MASK] to form a template and tune the embeddings of these tokens directly following~\citet{typing} used by \texttt{PTE} (soft). We leave it for future work to examine whether the NER performance further improves with a more well-designed soft prompt.


\section{Dataset Statistics}
\label{sec:dataset}
We use the following datasets where data statistics are displayed in Table~\ref{tab:dataset}: (1) The CoNLL03 dataset~\cite{conll03} is from the English Reuters News and consists of 4 entity types. We use the previous split in~\citet{templatener} for our experiments. The entity types are \textit{person}, \textit{location}, \textit{organization}, and \textit{miscellaneous entities}. The sampled training dataset in \S \ref{cross_type} includes 1500 organization entities, 1500 person entities, 150 location entities and 150 miscellaneous entities  (2) The MIT Movie dataset~\cite{mit-dataset} is from queries related to movie information. The entity types are \textit{actor}, \textit{character}, \textit{director}, \textit{genre}, \textit{plot}, \textit{year}, \textit{soundtrack}, \textit{opinion}, \textit{award}, \textit{origin}, \textit{quote}, and \textit{relationship}. (3) The Few-NERD dataset~\cite{fewnerd} is a low-resource NER dataset with a hierarchy of 8 coarse-grained and 66 fine-grained entity types. We use the coarse-grained entity in our experiments. The entity types are \textit{location}, \textit{event}, \textit{building}, \textit{art}, \textit{product}, \textit{person}, \textit{organization}, and \textit{miscellaneous entities}.

\section{Experimental Settings}
\label{sec:exp_settings}
We use the pre-trained models and codes provided by ADAPET and follow their default hyperparameter settings unless noted otherwise. The pre-trained language model of our method is BERT that is pre-trained in the MNLI datasets. We use AdamW optimizer and grid search batch size of \{$8$,$16$,$32$\} for model training.  We use grid search for learning rate from $[1\text{e-}5, 2\text{e-}5, 3\text{e-}5, 4\text{e-}5, 5\text{e-}5]$. And we grid search the optimal weight decay weight from $[0.1, 0.01, 0.005, 0.001]$. The maximum sequence length, the dropout rate, the gradient accumulation steps, the maximum training steps and the warm-up ratio are set to $256$, $0.1$, $16$, $7000$, $0.06$ respectively. Early stopping is also applied based on model performance on the development set. Our models are trained with NVIDIA Tesla V100s. The verbalizer words are [``yes'', ``no''] and [``true'', ``false'']. The $\tau$ of transition probability in decoding is selected by searching with $0.05$ step from $0$ to $1$. For sequence labeling BERT fine-tuning, we train BERT with a softmax classifier following \citet{bert}, updating parameters using Adam with an initial learning rate of $1\text{e-}5$, and a batch size of $32$. 
% Source code will be available upon publication.

\section{Pattern Engineering}
\label{sec:pattern}

After designing templates of entity-specific hypothesis, we follow \citet{ADAPET} to define the TE patterns in Table~\ref{tab:pattern} and report results across all patterns for all datasets in Figure~\ref{figpatt}. We find that the subtle difference of the prompts impacts performance, while Pattern\#4 outperforms others across datasets and settings.

