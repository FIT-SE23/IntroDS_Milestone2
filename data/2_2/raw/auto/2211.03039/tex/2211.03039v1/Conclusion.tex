\section{Related Work}
\label{sec:related}
Previous works have shown the ability of prompt-based learning under low-resource settings\cite{schick2020exploiting,schick2020s,schick-etal-2020-automatically,Lester}. \citet{schick2020exploiting} address low-resource text classification by manually designing templates as prompt-based learning in a iterative training manner. \citet{gao2020making} improve low-resource performance with well-designed templates with demonstrations. \citet{ptuning} apply continuous prompts for low-resource learning. Recently, some works~\cite{fewnerd,tong-etal-2021-learning,tempfree,LightNER} also focus on low-resource NER.  In contrast, we propose to use prompt-tuning to treat NER as the TE task. Unlike traditional NER methods, we use prompt-based learning without an additional linear layer for fine-tuning. By defining different prompts, the model is able to perform well in low-resource settings, which adapts to new domains with few labeled data. In contrast to recent work which also adopts prompt-based fine-tuning for NER ~\cite{DBLP:journals/corr/abs-2109-13532}, we show that the effectiveness of the text entailment reformulation for named entity recognition using PLMs.
\section{Conclusion}
In this paper, we apply prompt-based learning to low-resource named entity recognition. For token classification of NER, we reformulate it into a text entailment task. Our method transfers knowledge in different NLP tasks and domains, and performs better in low-resource scenarios. Future work includes how to apply \texttt{PTE} to other NLP tasks.
