

\section{Experiments}
We compare our methods with several baselines on both rich-resource settings and low-resource settings. We use the CoNLL2003 \cite{conll03} as the rich-resource dataset, and MIT Movie \cite{mit-dataset}, Few-NERD \cite{fewnerd} as the cross-domain low-resource datasets. And we conduct experiments on the CoNLL03 dataset in both full and low-resource settings. The dataset statistics and experimental settings are included in Appendix~\ref{sec:dataset} and~\ref{sec:exp_settings}. The standard precision, recall, and F1 score are used for model evaluation. 



\subsection{Rich-Resource NER Results}

We first use the whole training set of the CoNLL03 to train the model and evaluate its performance on the test set. 
Table~\ref{table:mp_conll} shows the performance of the comparison method and our model on the test set. We can find that although the potential applications of \texttt{PTE} is low-resource named entity recognition, it can also achieve competitive performance in rich-resource domain data sets. Compared with BERT fine-tuning reported in the previous work, the \texttt{PTE} model using discrete manual design reduces the F1 by 0.32, while the \texttt{PTE} model using the soft prompt method design mode~\cite{liu2021ptuning,ptuning} increases the F1 by 0.5. It shows that our method effectively recognizes named entities, and soft prompts can improve performance compared with manually designed prompts. More experimental results about TE patterns (\S\ref{sec:pet}) are in the Appendix~\ref{sec:pattern}.


\subsection{Cross Entity Type NER Results} 
\label{cross_type}

Following~\citet{templatener}, we sample the number of examples corresponding to different types of entities on the CoNLL03 data training set as new training set while keep test set unchanged. Among them, ``PER'' and ``ORG'' are rich-resource entity types, and ``LOC'' and ``MISC'' are low-resource entity types. The experimental results are shown in Table~\ref{table:conll_few}. The results show that our method achieves better results than baselines on the low-resource entity types, thus improving overall performance. On the other hand, our method is better than fine-tuning in both cases.

\begin{table}[t!]
\centering
\small
\resizebox{\linewidth}{!}{
\begin{tabular}{l|c|c|c}
     \hline
    {\bf Method} & {\bf Precision} & {\bf Recall} & {\bf F1} \\
    \hline
     % \multicolumn{4}{c}{Traditional Models} \\
    % \hline
    \citet{label-agnostic} & - & - & 89.94 \\
    \citet{yang-etal-2018-design} & - & - & 90.77 \\
    \citet{lstm-cnn-crf} & - & - & 91.21 \\
     BERT~\cite{templatener} & 91.93 & 91.54 & 91.73 \\
    \citet{DBLP:conf/emnlp/YamadaASTM20} & - & - & \bf{94.30} 
     \\
     \hline
     Template BART~\cite{templatener} & 90.51 & 93.34 & 91.90 \\
     
     \texttt{PTE} (discrete) & 91.27 & 91.56 & 91.41 \\
     \texttt{PTE} (soft) & 92.01 & 92.45 & \bf{92.23} \\
     \hline
\end{tabular}
}
\caption{Model performance on the CoNLL03 test set.\label{table:mp_conll}}
\end{table}

\begin{table}[t]
\centering
\small
\resizebox{\linewidth}{!}{

\begin{tabular}{l|c|c|c|c|c}
     \hline
   \textbf{Method} & {\bf PER} & {\bf ORG} & {\bf LOC} & {\bf MISC} & {\bf Overall} \\
    \hline
    BERT & 75.71 & 77.59 & 60.72 & 60.39 & 69.62 \\
    Template BART & 84.49 & 72.61 & 71.98 & 73.37 & 75.59 \\
    \texttt{PTE} (BERT) & 85.34 & 72.89 & 73.01 & 74.32 & \bf{76.40} \\
     \hline
\end{tabular}
}
\caption{Cross entity type results on the CoNLL03. LOC and MISC are low-resource entity types, where PER and ORG are rich-resource entity types. \label{table:conll_few}}
% \vspace{-2mm}
\end{table}
\subsection{Domain Transfer for Low-Resource NER}




\begin{table}[t!]
\centering
\small

\vspace{-0.2cm}
\resizebox{\linewidth}{!}{
\begin{tabular}{l|c|c|c|c|c|c}\hline
\multicolumn{7}{c}{\textit{MIT Movie} (12)}\\
\hline

     \textbf{Method} & {\bf K=10} & {\bf K=20} & {\bf K=50} & {\bf K=100} & {\bf K=200} & {\bf K=500} \\
\hline
    \citet{label-agnostic} &\ 3.1 &\ 4.5 &\ 4.1 &\ 5.3 &\ 5.4 &\ 8.6 \\
    \citet{example-ner} & 40.1 & 39.5 & 40.2 & 40.0 & 40.0 & 39.5 \\
    Sequence Labeling BERT & 28.3 & 45.2 & 50.0 & 52.4 & 60.7 & 76.8 \\
    \citet{DBLP:conf/emnlp/YamadaASTM20} & 35.6 & 49.2 & 61.8 & 72.4 & 78.7 & 82.8 \\
    % & 
    Template BART~\cite{templatener} & 42.4 & 54.2 & 59.6 & 65.3 & 69.6 & 80.3 \\ \hline
     \texttt{PTE} (discrete) & 46.9$\dagger$ & 59.2$\dagger$ & 66.9$\dagger$ & 74.9$\dagger$ & 79.9$\dagger$ & 83.6\\
     \texttt{PTE} (soft) & \textbf{47.8}$\dagger$ & \textbf{60.8}$\dagger$ & \textbf{68.1}$\dagger$ & \textbf{76.5}$\dagger$ & \textbf{83.6}$\dagger$ & \textbf{86.4}$\dagger$ \\
    \hline \hline
    \multicolumn{7}{c}{\textit{Few-NERD} (8)}\\\hline 

     \textbf{Method} & {\bf K=10} & {\bf K=20} & {\bf K=50} & {\bf K=100} & {\bf K=200} & {\bf K=500} \\
\hline
%     Source & Methods & 10 & 20 & 50 & 100 & 200 & 500 \\
     \citet{label-agnostic} &\ 5.2 &\ 4.1 &\ 4.7 &\ 7.8 &\ 12.3 &\ 10.1 \\
     \citet{example-ner} & 35.4 & 48.3 & 51.2 & 51.8 & 53.6 & 55.7 \\
     Sequence Labeling BERT & 50.6 & 59.3 & 61.3 & 61.4 & 62.5 & 66.4 \\ 
     \citet{DBLP:conf/emnlp/YamadaASTM20} & 51.7 & 60.1 & 62.3 & 61.0 & 62.5 & 66.8 \\ 
     \hline
     \texttt{PTE} (discrete) & 51.8 & 59.7 & 60.5 & 61.3 & 61.8 & 63.4\\
     \texttt{PTE} (soft) & \textbf{54.2} & \textbf{61.4} & \textbf{62.3} & \textbf{62.5} & \textbf{63.6} & \textbf{67.4} \\
 

\hline
\end{tabular}
}
\caption{\label{tab:fewshot}F1 comparison of two low-resource NER datasets. We set 6 sample size $K$ for different low-resource settings. $\dagger$ means a significant difference compared to Template BART ($p < .05$).}
\vspace{-0.2cm}
% \vspace{-2mm}
\end{table}


We do not use $N$-way $K$-shot setting~\cite{proto2,fewnerd} which samples $N$ categories and $K$ examples for training in each episode because a sentence in the NER task may contain multiple entities from different types. Thus, we randomly sample training data from the MIT Movie and Few-NERD datasets to simulate low-resource scenarios and use CoNLL03 as the rich-resource dataset. As such, we have only $K$ examples for each type of training. We choose $K\in\{10,20,50,100,200,500\}$ for experiments to evaluate the ability of the model on training data of different sizes. The experimental results are in Table~\ref{tab:fewshot}. The results show that when the $K$ value is relatively small, our \texttt{PTE} method can be better than the fine-tuning method, and this trend decreases with the increase of $K$. In addition, the soft mode is also better than the discrete mode in the case of a small number of samples. Overall, our method achieves the best results on both data sets in the low-resource scenario.


\begin{figure}[t!]
\centering
% \small
\includegraphics[width=1.0\linewidth]{ablation.pdf}
  \caption{F1 scores with different experimental settings and model variants.}
  \vspace{-4mm}
   \label{figablition}
\end{figure}
\subsection{Ablation Study}

We conduct ablation experiments and the results are shown in Figure~\ref{figablition}. The results show that (1) the selection of negative examples has a great impact on the performance of the model, especially the negative examples of the null label type. However, in rich-resource scenario, the gap between full setting and decreased setting is not as much as the low-resource scenario; (2) the low-resource scenario is a challenge to the model, and the results of some variants are not inconsistent where prompt-based learning may not be as good as fine-tuning; (3) label conditioning and soft mode have a consistent effect on the model. These findings highlight that it still has room left to use prompt for effectively transferring knowledge in the case of low-resource scenario.