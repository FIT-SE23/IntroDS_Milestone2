\vspace{-0.1cm}
\section{Introduction}
\label{sec:intro}
\vspace{-0.1cm}
Recent advances in neural generative models have challenged the way we create and consume digital content. From image and audio generations~\cite{karras2019style, oord2016wavenet} to the recently proposed textually guided generative methods~\cite{dalle2, nichol2021glide,gafni2022make}, these models have shown remarkable results.

Large-scale datasets of text-image pairs automatically obtained from the internet~\cite{schuhmann2022laion} were one of the main factors enabling recent breakthroughs in such models~\cite{dalle2, nichol2021glide}. However, replicating this success for audio is limited, as a similarly sized text-audio pairs dataset cannot be easily collected. For comparison, DALL-E 2 text-to-image model was trained on $\sim$650M text-image pairs~\cite{dalle2}, while the audio equivalent, \textsc{AudioGen} model~\cite{kreuk2022audiogen} was trained on $\sim$3M text-audio pairs. Contrary to text-audio pairs, videos that can be easily obtained from the web naturally contain image-audio pairs~\cite{Vggsound}. This makes the use of video data appealing for designing a conditional audio generation model.


Generating open-domain visually guided audio is a challenging task. Most prior attempts to solve this task have used a class-aware approach. Chen et al.~\cite{chen2018visually} proposed learning the delta from a per-class average spectrogram representation to an audio instance given input images. Next, the authors in~\cite{zhou2018visual, chen2020regnet} proposed training a model for each class independently. Although these methods provide high-quality generations, they are limited in their generalization ability to unseen classes and require labeled data. Lastly, the current state-of-the-art is the SpecVQGAN model proposed by~\cite{SpecVQGAN_Iashin_2021}. SpecVQGAN is based on a single model capable of generating a diverse set of sounds conditioned on visual inputs from multiple classes without a pre-determined class set. It is conditioned on image representations obtained from a pre-trained image classifier and generates a mel-spectrogram. Then, the generated mel-spectrogram is converted to the time domain using a neural vocoder~\cite{kumar2019melgan}.

\begin{figure}[t!]
\centering
\escapeus{\includegraphics[width=\columnwidth]{figs/Transformer.pdf}}
\caption{A high-level description of the \imwav{} architecture. Given an image sequence, CLIP features are extracted from each image and used as a condition for an autoregressive audio tokens generation model. The \textsc{Low} level tokens are then upsampled to higher resolution \textsc{Up} level tokens using an additional autoregressive model. Finally, both token sequences are decoded to a time-domain audio signal.\label{fig:Transformerarch}}
\vspace{-0.2cm}
\end{figure}

In this work, we follow such a label-free approach of generating general audio from natural images. Inspired by ~\cite{kreuk2022audiogen, dhariwal2020jukebox}, we propose \imwav{}, a Transformer-based audio Language Model (LM) conditioned on image representation. Given an input image sequence, \imwav{} generates an audio sample that highly correlates with the appeared objects in the image sequence. \imwav{} consists of two main stages. The first encodes raw audio to a discrete sequence of tokens using a hierarchical VQ-VAE model. In the second stage, we optimize an autoregressive Transformer language model that operates on the discrete audio tokens obtained from the first stage. The language model is conditioned on visual representations obtained from a pre-trained CLIP model~\cite{CLIPradford2021learning}. Our proposed model can be conditioned on either a single image or a sequence of images (i.e., video). Additionally, we apply the classifier-free guidance method~\cite{ho2021classifier} to better achieve image adherence in the generation process. 

We empirically show that the proposed method significantly outperforms the evaluated baselines across a diverse set of metrics. We additionally provide a label distribution analysis of the generated audio, together with an ablation study, to better assess the effect of each component of the proposed system. A visual description of the proposed system can be seen in Figure~\ref{fig:Transformerarch}.  
