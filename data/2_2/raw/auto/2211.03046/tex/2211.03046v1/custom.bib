
@inproceedings{abbasi2022role,
  title = {Role of {{Human Intuition}} in {{AI Aided Managerial Decision Making}}: {{A Review}}},
  shorttitle = {Role of {{Human Intuition}} in {{AI Aided Managerial Decision Making}}},
  booktitle = {2022 {{International Conference}} on {{Decision Aid Sciences}} and {{Applications}} ({{DASA}})},
  author = {Abbasi, Merium Fazal and Bilal, Muhammad and Rasheed, Kumeel},
  year = {2022},
  month = mar,
  pages = {713--718},
  publisher = {{IEEE}},
  address = {{Chiangrai, Thailand}},
  doi = {10.1109/DASA54658.2022.9765153},
  isbn = {978-1-66549-501-1},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\storage\\Role of Human Intuition in AI Aided Managerial Decision Making _Abbasi et al 2022.pdf}
}

@misc{abid2022meaningfully,
  title = {Meaningfully {{Debugging Model Mistakes}} Using {{Conceptual Counterfactual Explanations}}},
  author = {Abid, Abubakar and Yuksekgonul, Mert and Zou, James},
  year = {2022},
  month = jun,
  number = {arXiv:2106.12723},
  eprint = {2106.12723},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  abstract = {Understanding and explaining the mistakes made by trained models is critical to many machine learning objectives, such as improving robustness, addressing concept drift, and mitigating biases. However, this is often an ad hoc process that involves manually looking at the model's mistakes on many test samples and guessing at the underlying reasons for those incorrect predictions. In this paper, we propose a systematic approach, conceptual counterfactual explanations (CCE), that explains why a classifier makes a mistake on a particular test sample(s) in terms of human-understandable concepts (e.g. this zebra is misclassified as a dog because of faint stripes). We base CCE on two prior ideas: counterfactual explanations and concept activation vectors, and validate our approach on wellknown pretrained models, showing that it explains the models' mistakes meaningfully. In addition, for new models trained on data with spurious correlations, CCE accurately identifies the spurious correlation as the cause of model mistakes from a single misclassified test sample. On two challenging medical applications, CCE generated useful insights, confirmed by clinicians, into biases and mistakes the model makes in real-world settings. The code for CCE is publicly available at https://github.com/ mertyg/debug-mistakes-cce.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning},
  file = {C\:\\Users\\Hytn\\Zotero\\storage\\E4F8F449\\Abid et al. - 2022 - Meaningfully Debugging Model Mistakes using Concep.pdf}
}

@article{adadi2018peeking,
  title = {Peeking {{Inside}} the {{Black-Box}}: {{A Survey}} on {{Explainable Artificial Intelligence}} ({{XAI}})},
  shorttitle = {Peeking {{Inside}} the {{Black-Box}}},
  author = {Adadi, Amina and Berrada, Mohammed},
  year = {2018},
  journal = {IEEE Access},
  volume = {6},
  pages = {52138--52160},
  issn = {2169-3536},
  doi = {10.1109/ACCESS.2018.2870052},
  abstract = {At the dawn of the fourth industrial revolution, we are witnessing a fast and widespread adoption of artificial intelligence (AI) in our daily life, which contributes to accelerating the shift towards a more algorithmic society. However, even with such unprecedented advancements, a key impediment to the use of AI-based systems is that they often lack transparency. Indeed, the black-box nature of these systems allows powerful predictions, but it cannot be directly explained. This issue has triggered a new debate on explainable AI (XAI). A research field holds substantial promise for improving trust and transparency of AI-based systems. It is recognized as the sine qua non for AI to continue making steady progress without disruption. This survey provides an entry point for interested researchers and practitioners to learn key aspects of the young and rapidly growing body of research related to XAI. Through the lens of the literature, we review the existing approaches regarding the topic, discuss trends surrounding its sphere, and present major research trajectories.},
  langid = {english},
  file = {C\:\\Users\\Hytn\\Zotero\\storage\\96EH3D3P\\Adadi and Berrada - 2018 - Peeking Inside the Black-Box A Survey on Explaina.pdf}
}

@inproceedings{agrawal2016analyzing,
  title = {Analyzing the Behavior of Visual Question Answering Models},
  booktitle = {Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing},
  author = {Agrawal, Aishwarya and Batra, Dhruv and Parikh, Devi},
  year = {2016},
  pages = {1955--1960},
  doi = {10.18653/v1/D16-1203},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\storage\\Analyzing the behavior of visual question answering models _Agrawal et al 2016.pdf}
}

@inproceedings{ahuja2021empirical,
  title = {Empirical or Invariant Risk Minimization? {{A}} Sample Complexity Perspective},
  booktitle = {International Conference on Learning Representations},
  author = {Ahuja, Kartik and Wang, Jun and Dhurandhar, Amit and Shanmugam, Karthikeyan and Varshney, Kush R.},
  year = {2021},
  keywords = {⛔ No DOI found},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\storage\\Empirical or invariant risk minimization _Ahuja et al 2021.pdf}
}

@inproceedings{ahuja2022aspectnews,
  title = {{{ASPECTNEWS}}: {{Aspect-Oriented Summarization}} of {{News Documents}}},
  shorttitle = {{{ASPECTNEWS}}},
  booktitle = {Proceedings of the 60th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}} ({{Volume}} 1: {{Long Papers}})},
  author = {Ahuja, Ojas and Xu, Jiacheng and Gupta, Akshay and Horecka, Kevin and Durrett, Greg},
  year = {2022},
  pages = {6494--6506},
  publisher = {{Association for Computational Linguistics}},
  address = {{Dublin, Ireland}},
  doi = {10.18653/v1/2022.acl-long.449},
  langid = {english},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\storage\\ASPECTNEWS _Ahuja et al 2022.pdf}
}

@article{aletras2016predicting,
  title = {Predicting Judicial Decisions of the {{European Court}} of {{Human Rights}}: A {{Natural Language Processing}} Perspective},
  shorttitle = {Predicting Judicial Decisions of the {{European Court}} of {{Human Rights}}},
  author = {Aletras, Nikolaos and Tsarapatsanis, Dimitrios and {Preo{\c t}iuc-Pietro}, Daniel and Lampos, Vasileios},
  year = {2016},
  month = oct,
  journal = {PeerJ Computer Science},
  volume = {2},
  pages = {e93},
  issn = {2376-5992},
  doi = {10.7717/peerj-cs.93},
  abstract = {Recent advances in Natural Language Processing and Machine Learning provide us with the tools to build predictive models that can be used to unveil patterns driving judicial decisions. This can be useful, for both lawyers and judges, as an assisting tool to rapidly identify cases and extract patterns which lead to certain decisions. This paper presents the first systematic study on predicting the outcome of cases tried by the European Court of Human Rights based solely on textual content. We formulate a binary classification task where the input of our classifiers is the textual content extracted from a case and the target output is the actual judgment as to whether there has been a violation of an article of the convention of human rights. Textual information is represented using contiguous word sequences, i.e.,~N-grams, and topics. Our models can predict the court's decisions with a strong accuracy (79\% on average). Our empirical analysis indicates that the formal facts of a case are the most important predictive factor. This is consistent with the theory of legal realism suggesting that judicial decision-making is significantly affected by the stimulus of the facts. We also observe that the topical content of a case is another important feature in this classification task and explore this relationship further by conducting a qualitative analysis.},
  langid = {english},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\storage\\Predicting judicial decisions of the European Court of Human Rights _Aletras et al 2016.pdf}
}

@inproceedings{alt2019improving,
  title = {Improving {{Relation Extraction}} by {{Pre-trained Language Representations}}},
  booktitle = {{{AKBC}} 2019 : 1st {{Conference}} on {{Automated Knowledge Base Construction}}},
  author = {Alt, Christoph and H{\~A}{$\frac{1}{4}$}bner, Marc and Hennig, Leonhard},
  year = {2019},
  keywords = {⛔ No DOI found}
}

@inproceedings{alvarez-melis2017causal,
  title = {A Causal Framework for Explaining the Predictions of Black-Box Sequence-to-Sequence Models},
  booktitle = {Proceedings of the 2017 {{Conference}} on {{Empirical Methods}} in {{Natural}}           {{Language Processing}}},
  author = {{Alvarez-Melis}, David and Jaakkola, Tommi},
  year = {2017},
  pages = {412--421},
  publisher = {{Association for Computational Linguistics}},
  address = {{Copenhagen, Denmark}},
  doi = {10.18653/v1/D17-1042},
  langid = {english},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\storage\\A causal framework for explaining the predictions of black-box _Alvarez-Melis_Jaakkola 2017.pdf}
}

@inproceedings{alzantot2018generating,
  title = {Generating {{Natural Language Adversarial Examples}}},
  booktitle = {Proceedings of the 2018 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}}},
  author = {Alzantot, Moustafa and Sharma, Yash and Elgohary, Ahmed and Ho, Bo-Jhang and Srivastava, Mani and Chang, Kai-Wei},
  year = {2018},
  pages = {2890--2896},
  publisher = {{Association for Computational Linguistics}},
  address = {{Brussels, Belgium}},
  doi = {10.18653/v1/D18-1316},
  langid = {english},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\storage\\Generating Natural Language Adversarial Examples _Alzantot et al 2018.pdf}
}

@article{angelidis2021extractive,
  title = {Extractive {{Opinion Summarization}} in {{Quantized Transformer Spaces}}},
  author = {Angelidis, Stefanos and Amplayo, Reinald Kim and Suhara, Yoshihiko and Wang, Xiaolan and Lapata, Mirella},
  year = {2021},
  month = mar,
  journal = {Transactions of the Association for Computational Linguistics},
  volume = {9},
  pages = {277--293},
  issn = {2307-387X},
  doi = {10.1162/tacl_a_00366},
  abstract = {Abstract             We present the Quantized Transformer (QT), an unsupervised system for extractive opinion summarization. QT is inspired by Vector- Quantized Variational Autoencoders, which we repurpose for popularity-driven summarization. It uses a clustering interpretation of the quantized space and a novel extraction algorithm to discover popular opinions among hundreds of reviews, a significant step towards opinion summarization of practical scope. In addition, QT enables controllable summarization without further training, by utilizing properties of the quantized space to extract aspect-specific summaries. We also make publicly available Space, a large-scale evaluation benchmark for opinion summarizers, comprising general and aspect-specific summaries for 50 hotels. Experiments demonstrate the promise of our approach, which is validated by human studies where judges showed clear preference for our method over competitive baselines.},
  langid = {english},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\storage\\Extractive Opinion Summarization in Quantized Transformer Spaces _Angelidis et al 2021.pdf}
}

@misc{arjovsky2020invariant,
  title = {Invariant {{Risk Minimization}}},
  author = {Arjovsky, Martin and Bottou, L{\'e}on and Gulrajani, Ishaan and {Lopez-Paz}, David},
  year = {2020},
  month = mar,
  number = {arXiv:1907.02893},
  eprint = {1907.02893},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  abstract = {We introduce Invariant Risk Minimization (IRM), a learning paradigm to estimate invariant correlations across multiple training distributions. To achieve this goal, IRM learns a data representation such that the optimal classifier, on top of that data representation, matches for all training distributions. Through theory and experiments, we show how the invariances learned by IRM relate to the causal structures governing the data and enable out-of-distribution generalization.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C\:\\Users\\Hytn\\Zotero\\storage\\ENYU7YT2\\Arjovsky et al. - 2020 - Invariant Risk Minimization.pdf}
}

@inproceedings{athiwaratkun2020augmented,
  title = {Augmented {{Natural Language}} for {{Generative Sequence Labeling}}},
  booktitle = {Proceedings of the 2020 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}} ({{EMNLP}})},
  author = {Athiwaratkun, Ben and {Nogueira dos Santos}, Cicero and Krone, Jason and Xiang, Bing},
  year = {2020},
  pages = {375--385},
  publisher = {{Association for Computational Linguistics}},
  address = {{Online}},
  doi = {10.18653/v1/2020.emnlp-main.27},
  langid = {english},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\storage\\Augmented Natural Language for Generative Sequence Labeling _Athiwaratkun et al 2020.pdf}
}

@article{bach2015pixelwise,
  title = {On {{Pixel-Wise Explanations}} for {{Non-Linear Classifier Decisions}} by {{Layer-Wise Relevance Propagation}}},
  author = {Bach, Sebastian and Binder, Alexander and Montavon, Gr{\'e}goire and Klauschen, Frederick and M{\"u}ller, Klaus-Robert and Samek, Wojciech},
  editor = {Suarez, Oscar Deniz},
  year = {2015},
  month = jul,
  journal = {PLOS ONE},
  volume = {10},
  number = {7},
  pages = {e0130140},
  issn = {1932-6203},
  doi = {10.1371/journal.pone.0130140},
  langid = {english},
  keywords = {LRP CV},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\storage\\On Pixel-Wise Explanations for Non-Linear Classifier Decisions by Layer-Wise _Bach et al 2015.pdf}
}

@article{baehrens2010explain,
  title = {How to Explain Individual Classification Decisions},
  author = {Baehrens, David and Schroeter, Timon and Harmeling, Stefan and Kawanabe, Motoaki and Hansen, Katja and M{\"u}ller, Klaus-Robert},
  year = {2010},
  journal = {The Journal of Machine Learning Research},
  volume = {11},
  pages = {1803--1831},
  publisher = {{JMLR. org}},
  keywords = {⛔ No DOI found},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\storage\\How to explain individual classification decisions _Baehrens et al 2010.pdf}
}

@article{bag2019efficient,
  title = {An Efficient Recommendation Generation Using Relevant {{Jaccard}} Similarity},
  author = {Bag, Sujoy and Kumar, Sri Krishna and Tiwari, Manoj Kumar},
  year = {2019},
  month = may,
  journal = {Information Sciences},
  volume = {483},
  pages = {53--64},
  issn = {00200255},
  doi = {10.1016/j.ins.2019.01.023},
  langid = {english}
}

@inproceedings{barba2022extend,
  title = {{{ExtEnD}}: {{Extractive}} Entity Disambiguation},
  booktitle = {Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: {{Long}} Papers)},
  author = {Barba, Edoardo and Procopio, Luigi and Navigli, Roberto},
  year = {2022},
  month = may,
  pages = {2478--2488},
  publisher = {{Association for Computational Linguistics}},
  address = {{Dublin, Ireland}},
  abstract = {Local models for Entity Disambiguation (ED) have today become extremely powerful, in most part thanks to the advent of large pre-trained language models. However, despite their significant performance achievements, most of these approaches frame ED through classification formulations that have intrinsic limitations, both computationally and from a modeling perspective. In contrast with this trend, here we propose ExtEnD, a novel local formulation for ED where we frame this task as a text extraction problem, and present two Transformer-based architectures that implement it. Based on experiments in and out of domain, and training over two different data regimes, we find our approach surpasses all its competitors in terms of both data efficiency and raw performance. ExtEnD outperforms its alternatives by as few as 6 F1 points on the more constrained of the two data regimes and, when moving to the other higher-resourced regime, sets a new state of the art on 4 out of 4 benchmarks under consideration, with average improvements of 0.7 F1 points overall and 1.1 F1 points out of domain. In addition, to gain better insights from our results, we also perform a fine-grained evaluation of our performances on different classes of label frequency, along with an ablation study of our architectural choices and an error analysis. We release our code and models for research purposes at https://github.com/SapienzaNLP/extend.},
  file = {C\:\\Users\\Hytn\\Zotero\\storage\\VTPIBNYY\\ExtEnD_Barba et al 2022.pdf}
}

@article{bareinboim2014recovering,
  title = {Recovering from {{Selection Bias}} in {{Causal}} and {{Statistical Inference}}},
  author = {Bareinboim, Elias and Tian, Jin and Pearl, Judea},
  year = {2014},
  month = jun,
  journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
  volume = {28},
  number = {1},
  issn = {2374-3468, 2159-5399},
  doi = {10.1609/aaai.v28i1.9074},
  abstract = {Selection bias is caused by preferential exclusion of units from the samples and represents a major obstacle to valid causal and statistical inferences; it cannot be removed by randomized experiments and can rarely be detected in either experimental or observational studies. In this paper, we provide complete graphical and algorithmic conditions for recovering conditional probabilities from selection biased data. We also provide graphical conditions for recoverability when unbiased data is available over a subset of the variables. Finally, we provide a graphical condition that generalizes the backdoor criterion and serves to recover causal effects when the data is collected under preferential selection.},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\storage\\Recovering from Selection Bias in Causal and Statistical Inference _Bareinboim et al 2014.pdf}
}

@article{bareinboim2015recovering,
  title = {Recovering {{Causal Effects}} from {{Selection Bias}}},
  author = {Bareinboim, Elias and Tian, Jin},
  year = {2015},
  month = mar,
  journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
  volume = {29},
  number = {1},
  issn = {2374-3468, 2159-5399},
  doi = {10.1609/aaai.v29i1.9679},
  abstract = {Controlling for selection and confounding biases are two of the most challenging problems that appear in data analysis in the empirical sciences as well as in artificial intelligence tasks. The combination of previously studied methods for each of these biases in isolation is not directly applicable to certain non-trivial cases in which selection and confounding biases are simultaneously present. In this paper, we tackle these instances non-parametrically and in full generality. We provide graphical and algorithmic conditions for recoverability of interventional distributions for when selection and confounding biases are both present. Our treatment completely characterizes the class of causal effects that are recoverable in Markovian models, and is suffi- cient for Semi-Markovian models.},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\storage\\Recovering Causal Effects from Selection Bias _Bareinboim_Tian 2015.pdf}
}

@article{barredoarrieta2020explainable,
  title = {Explainable {{Artificial Intelligence}} ({{XAI}}): {{Concepts}}, Taxonomies, Opportunities and Challenges toward Responsible {{AI}}},
  shorttitle = {Explainable {{Artificial Intelligence}} ({{XAI}})},
  author = {Barredo Arrieta, Alejandro and {D{\'i}az-Rodr{\'i}guez}, Natalia and Del Ser, Javier and Bennetot, Adrien and Tabik, Siham and Barbado, Alberto and Garcia, Salvador and {Gil-Lopez}, Sergio and Molina, Daniel and Benjamins, Richard and Chatila, Raja and Herrera, Francisco},
  year = {2020},
  month = jun,
  journal = {Information Fusion},
  volume = {58},
  pages = {82--115},
  issn = {15662535},
  doi = {10.1016/j.inffus.2019.12.012},
  langid = {english},
  file = {C\:\\Users\\Hytn\\Zotero\\storage\\TEGRCMSM\\Barredo Arrieta et al. - 2020 - Explainable Artificial Intelligence (XAI) Concept.pdf}
}

@article{bastani2018interpretability,
  title = {Interpretability via {{Model Extraction}}},
  author = {Bastani, Osbert and Kim, Carolyn and Bastani, Hamsa},
  year = {2018},
  month = mar,
  journal = {arXiv:1706.09773 [cs, stat]},
  eprint = {1706.09773},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {The ability to interpret machine learning models has become increasingly important now that machine learning is used to inform consequential decisions. We propose an approach called model extraction for interpreting complex, blackbox models. Our approach approximates the complex model using a much more interpretable model; as long as the approximation quality is good, then statistical properties of the complex model are reflected in the interpretable model. We show how model extraction can be used to understand and debug random forests and neural nets trained on several datasets from the UCI Machine Learning Repository, as well as control policies learned for several classical reinforcement learning problems.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {⛔ No DOI found,Computer Science - Computers and Society,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C\:\\Users\\Hytn\\Zotero\\storage\\UNMEQUCS\\Bastani et al. - 2018 - Interpretability via Model Extraction.pdf}
}

@inproceedings{bastings2020elephant,
  title = {The Elephant in the Interpretability Room: {{Why}} Use Attention as Explanation When We Have Saliency Methods?},
  shorttitle = {The Elephant in the Interpretability Room},
  booktitle = {Proceedings of the {{Third BlackboxNLP Workshop}} on {{Analyzing}} and {{Interpreting Neural Networks}} for {{NLP}}},
  author = {Bastings, Jasmijn and Filippova, Katja},
  year = {2020},
  pages = {149--155},
  publisher = {{Association for Computational Linguistics}},
  address = {{Online}},
  doi = {10.18653/v1/2020.blackboxnlp-1.14},
  langid = {english},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\storage\\The elephant in the interpretability room _Bastings_Filippova 2020.pdf}
}

@article{bates1995models,
  title = {Models of Natural Language Understanding.},
  author = {Bates, M.},
  year = {1995},
  month = oct,
  journal = {Proceedings of the National Academy of Sciences},
  volume = {92},
  number = {22},
  pages = {9977--9982},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.92.22.9977},
  langid = {english},
  file = {C\:\\Users\\Hytn\\Zotero\\storage\\Y8Q3F5VR\\Bates_1995_Models of natural language understanding.pdf}
}

@inproceedings{beck2018graphtosequence,
  title = {Graph-to-{{Sequence Learning}} Using {{Gated Graph Neural Networks}}},
  booktitle = {Proceedings of the 56th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}} ({{Volume}} 1: {{Long Papers}})},
  author = {Beck, Daniel and Haffari, Gholamreza and Cohn, Trevor},
  year = {2018},
  pages = {273--283},
  publisher = {{Association for Computational Linguistics}},
  address = {{Melbourne, Australia}},
  doi = {10.18653/v1/P18-1026},
  langid = {english},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\storage\\Graph-to-Sequence Learning using Gated Graph Neural Networks _Beck et al 2018.pdf}
}

@inproceedings{belinkov2018synthetic,
  title = {Synthetic and Natural Noise Both Break Neural Machine Translation},
  booktitle = {International Conference on Learning Representations},
  author = {Belinkov, Yonatan and Bisk, Yonatan},
  year = {2018},
  keywords = {⛔ No DOI found},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\storage\\Synthetic and natural noise both break neural machine translation _Belinkov_Bisk 2018.pdf}
}

@article{beltagy2020longformer,
  title = {Longformer: {{The Long-Document Transformer}}},
  shorttitle = {Longformer},
  author = {Beltagy, Iz and Peters, Matthew E. and Cohan, Arman},
  year = {2020},
  publisher = {{arXiv}},
  doi = {10.48550/ARXIV.2004.05150},
  abstract = {Transformer-based models are unable to process long sequences due to their self-attention operation, which scales quadratically with the sequence length. To address this limitation, we introduce the Longformer with an attention mechanism that scales linearly with sequence length, making it easy to process documents of thousands of tokens or longer. Longformer's attention mechanism is a drop-in replacement for the standard self-attention and combines a local windowed attention with a task motivated global attention. Following prior work on long-sequence transformers, we evaluate Longformer on character-level language modeling and achieve state-of-the-art results on text8 and enwik8. In contrast to most prior work, we also pretrain Longformer and finetune it on a variety of downstream tasks. Our pretrained Longformer consistently outperforms RoBERTa on long document tasks and sets new state-of-the-art results on WikiHop and TriviaQA. We finally introduce the Longformer-Encoder-Decoder (LED), a Longformer variant for supporting long document generative sequence-to-sequence tasks, and demonstrate its effectiveness on the arXiv summarization dataset.},
  copyright = {arXiv.org perpetual, non-exclusive license},
  keywords = {Computation and Language (cs.CL),FOS: Computer and information sciences}
}

@article{besserve2020counterfactuals,
  title = {{{COUNTERFACTUALS UNCOVER THE MODULAR STRUCTURE OF DEEP GENERATIVE MODELS}}},
  author = {Besserve, Michel and Mehrjou, Arash and Sun, Remy and Scholkopf, Bernhard},
  year = {2020},
  pages = {26},
  abstract = {Deep generative models can emulate the perceptual properties of complex image datasets, providing a latent representation of the data. However, manipulating such representation to perform meaningful and controllable transformations in the data space remains challenging without some form of supervision. While previous work has focused on exploiting statistical independence to disentangle latent factors, we argue that such requirement can be advantageously relaxed and propose instead a non-statistical framework that relies on identifying a modular organization of the network, based on counterfactual manipulations. Our experiments support that modularity between groups of channels is achieved to a certain degree on a variety of generative models. This allowed the design of targeted interventions on complex image datasets, opening the way to applications such as computationally efficient style transfer and the automated assessment of robustness to contextual changes in pattern recognition systems.},
  langid = {english},
  keywords = {⛔ No DOI found},
  file = {C\:\\Users\\Hytn\\Zotero\\storage\\5FDTG94R\\Besserve et al. - 2020 - COUNTERFACTUALS UNCOVER THE MODULAR STRUCTURE OF D.pdf}
}

@inproceedings{bevilacqua2020generationary,
  title = {Generationary or ``{{How We Went}} beyond {{Word Sense Inventories}} and {{Learned}} to {{Gloss}}''},
  booktitle = {Proceedings of the 2020 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}} ({{EMNLP}})},
  author = {Bevilacqua, Michele and Maru, Marco and Navigli, Roberto},
  year = {2020},
  pages = {7207--7221},
  publisher = {{Association for Computational Linguistics}},
  address = {{Online}},
  doi = {10.18653/v1/2020.emnlp-main.585},
  langid = {english},
  keywords = {seq2seq WSD},
  file = {C\:\\Users\\Hytn\\Zotero\\storage\\NHALL9U6\\Generationary or “How We Went beyond Word Sense Inventories and Learned to_Bevilacqua et al 2020.pdf}
}

@article{bevilacqua2021recent,
  title = {Recent Trends in Word Sense Disambiguation: {{A}} Survey},
  author = {Bevilacqua, Michele and Pasini, Tommaso and Raganato, Alessandro and Navigli, Roberto},
  doi = {10.24963/ijcai.2021/593},
  file = {C\:\\Users\\Hytn\\Zotero\\storage\\IW52K4FL\\Recent trends in word sense disambiguation_Bevilacqua et al.pdf}
}

@inproceedings{blevins2021fews,
  title = {{{FEWS}}: {{Large-Scale}}, {{Low-Shot Word Sense Disambiguation}} with the {{Dictionary}}},
  shorttitle = {{{FEWS}}},
  booktitle = {Proceedings of the 16th {{Conference}} of the {{European Chapter}} of the {{Association}} for {{Computational Linguistics}}: {{Main Volume}}},
  author = {Blevins, Terra and Joshi, Mandar and Zettlemoyer, Luke},
  year = {2021},
  pages = {455--465},
  publisher = {{Association for Computational Linguistics}},
  address = {{Online}},
  doi = {10.18653/v1/2021.eacl-main.36},
  langid = {english},
  file = {C\:\\Users\\Hytn\\Zotero\\storage\\IPKSXZQ5\\FEWS_Blevins et al 2021.pdf}
}

@inproceedings{boggust2022shared,
  title = {Shared {{Interest}}: {{Measuring Human-AI Alignment}} to {{Identify Recurring Patterns}} in {{Model Behavior}}},
  shorttitle = {Shared {{Interest}}},
  booktitle = {{{CHI Conference}} on {{Human Factors}} in {{Computing Systems}}},
  author = {Boggust, Angie and Hoover, Benjamin and Satyanarayan, Arvind and Strobelt, Hendrik},
  year = {2022},
  month = apr,
  pages = {1--17},
  publisher = {{ACM}},
  address = {{New Orleans LA USA}},
  doi = {10.1145/3491102.3501965},
  isbn = {978-1-4503-9157-3},
  langid = {english},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\storage\\Shared Interest _Boggust et al 2022.pdf}
}

@article{bommasaniopportunities,
  title = {On the {{Opportunities}} and {{Risks}} of {{Foundation Models}}},
  author = {Bommasani, Rishi and Hudson, Drew A and Altman, Ehsan Adeli Russ and Arora, Simran},
  pages = {212},
  langid = {english},
  keywords = {⛔ No DOI found},
  file = {C\:\\Users\\Hytn\\Zotero\\storage\\VAXTUEWM\\Bommasani et al. - On the Opportunities and Risks of Foundation Model.pdf}
}

@article{brown1992class,
  title = {Class-Based n-Gram Models of Natural Language},
  author = {Brown, Peter F and Della Pietra, Vincent J and Desouza, Peter V and Lai, Jennifer C and Mercer, Robert L},
  year = {1992},
  journal = {Computational linguistics},
  volume = {18},
  number = {4},
  pages = {467--480},
  keywords = {⛔ No DOI found},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\storage\\Class-based n-gram models of natural language _Brown et al 1992.pdf}
}

@article{buhlmann2020invariance,
  title = {Invariance, {{Causality}} and {{Robustness}}},
  author = {B{\"u}hlmann, Peter},
  year = {2020},
  month = aug,
  journal = {Statistical Science},
  volume = {35},
  number = {3},
  issn = {0883-4237},
  doi = {10.1214/19-STS721},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\storage\\Invariance, Causality and Robustness _Bühlmann 2020.pdf}
}

@inproceedings{cai2020amr,
  title = {{{AMR Parsing}} via {{Graph-Sequence Iterative Inference}}},
  booktitle = {Proceedings of the 58th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}}},
  author = {Cai, Deng and Lam, Wai},
  year = {2020},
  pages = {1290--1301},
  publisher = {{Association for Computational Linguistics}},
  address = {{Online}},
  doi = {10.18653/v1/2020.acl-main.119},
  langid = {english},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\storage\\AMR Parsing via Graph-Sequence Iterative Inference _Cai_Lam 2020.pdf}
}

@article{cai2020graph,
  title = {Graph {{Transformer}} for {{Graph-to-Sequence Learning}}},
  author = {Cai, Deng and Lam, Wai},
  year = {2020},
  month = apr,
  journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
  volume = {34},
  number = {05},
  pages = {7464--7471},
  issn = {2374-3468, 2159-5399},
  doi = {10.1609/aaai.v34i05.6243},
  abstract = {The dominant graph-to-sequence transduction models employ graph neural networks for graph representation learning, where the structural information is reflected by the receptive field of neurons. Unlike graph neural networks that restrict the information exchange between immediate neighborhood, we propose a new model, known as Graph Transformer, that uses explicit relation encoding and allows direct communication between two distant nodes. It provides a more efficient way for global graph structure modeling. Experiments on the applications of text generation from Abstract Meaning Representation (AMR) and syntax-based neural machine translation show the superiority of our proposed model. Specifically, our model achieves 27.4 BLEU on LDC2015E86 and 29.7 BLEU on LDC2017T10 for AMR-to-text generation, outperforming the state-of-the-art results by up to 2.2 points. On the syntax-based translation tasks, our model establishes new single-model state-of-the-art BLEU scores, 21.3 for English-to-German and 14.1 for English-to-Czech, improving over the existing best results, including ensembles, by over 1 BLEU.},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\storage\\Graph Transformer for Graph-to-Sequence Learning _Cai_Lam 2020.pdf}
}

@article{caliskan2017semantics,
  title = {Semantics Derived Automatically from Language Corpora Contain Human-like Biases},
  author = {Caliskan, Aylin and Bryson, Joanna J. and Narayanan, Arvind},
  year = {2017},
  month = apr,
  journal = {Science},
  volume = {356},
  number = {6334},
  pages = {183--186},
  issn = {0036-8075, 1095-9203},
  doi = {10.1126/science.aal4230},
  abstract = {Machines learn what people know implicitly                            AlphaGo has demonstrated that a machine can learn how to do things that people spend many years of concentrated study learning, and it can rapidly learn how to do them better than any human can. Caliskan               et al.               now show that machines can learn word associations from written texts and that these associations mirror those learned by humans, as measured by the Implicit Association Test (IAT) (see the Perspective by Greenwald). Why does this matter? Because the IAT has predictive value in uncovering the association between concepts, such as pleasantness and flowers or unpleasantness and insects. It can also tease out attitudes and beliefs\textemdash for example, associations between female names and family or male names and career. Such biases may not be expressed explicitly, yet they can prove influential in behavior.                                         Science               , this issue p.               183               ; see also p.               133                        ,              Computers can learn which words go together more or less often and can thus mimic human performance on a test of implicit bias.           ,              Machine learning is a means to derive artificial intelligence by discovering patterns in existing data. Here, we show that applying machine learning to ordinary human language results in human-like semantic biases. We replicated a spectrum of known biases, as measured by the Implicit Association Test, using a widely used, purely statistical machine-learning model trained on a standard corpus of text from the World Wide Web. Our results indicate that text corpora contain recoverable and accurate imprints of our historic biases, whether morally neutral as toward insects or flowers, problematic as toward race or gender, or even simply veridical, reflecting the status quo distribution of gender with respect to careers or first names. Our methods hold promise for identifying and addressing sources of bias in culture, including technology.},
  langid = {english},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\storage\\Semantics derived automatically from language corpora contain human-like biases _Caliskan et al 2017.pdf}
}

@article{camburuesnli,
  title = {E-{{SNLI}}: {{Natural Language Inference}} with {{Natural Language Explanations}}},
  author = {Camburu, Oana-Maria and Rockt{\"a}schel, Tim and Lukasiewicz, Thomas and Blunsom, Phil},
  pages = {11},
  abstract = {In order for machine learning to garner widespread public adoption, models must be able to provide interpretable and robust explanations for their decisions, as well as learn from human-provided explanations at train time. In this work, we extend the Stanford Natural Language Inference dataset with an additional layer of human-annotated natural language explanations of the entailment relations. We further implement models that incorporate these explanations into their training process and output them at test time. We show how our corpus of explanations, which we call e-SNLI, can be used for various goals, such as obtaining full sentence justifications of a model's decisions, improving universal sentence representations and transferring to out-of-domain NLI datasets. Our dataset1 thus opens up a range of research directions for using natural language explanations, both for improving models and for asserting their trust.},
  langid = {english},
  keywords = {⛔ No DOI found},
  file = {C\:\\Users\\Hytn\\Zotero\\storage\\XMESM86X\\Camburu et al. - e-SNLI Natural Language Inference with Natural La.pdf}
}

@inproceedings{cao2019question,
  title = {Question {{Answering}} by {{Reasoning Across Documents}} with {{Graph Convolutional Networks}}},
  booktitle = {Proceedings of the 2019 {{Conference}} of the {{North American Chapter}} of the {{Association}} for {{Computational Linguistics}}: {{Human Language Technologies}}, {{Volume}} 1 ({{Long}} and {{Short Papers}})},
  author = {Cao, Nicola De and Aziz, Wilker and Titov, Ivan},
  year = {2019},
  pages = {2306--2317},
  keywords = {⛔ No DOI found}
}

@inproceedings{cao2021autoregressive,
  title = {Autoregressive Entity Retrieval},
  booktitle = {International Conference on Learning Representations},
  author = {Cao, Nicola De and Izacard, Gautier and Riedel, Sebastian and Petroni, Fabio},
  year = {2021},
  keywords = {⛔ No DOI found},
  file = {C\:\\Users\\Hytn\\Zotero\\storage\\E8Q4QAG2\\Autoregressive entity retrieval_Cao et al 2021.pdf}
}

@inproceedings{carlsson2022finegrained,
  title = {Fine-{{Grained Controllable Text Generation Using Non-Residual Prompting}}},
  booktitle = {Proceedings of the 60th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}} ({{Volume}} 1: {{Long Papers}})},
  author = {Carlsson, Fredrik and {\"O}hman, Joey and Liu, Fangyu and Verlinden, Severine and Nivre, Joakim and Sahlgren, Magnus},
  year = {2022},
  pages = {6837--6857},
  publisher = {{Association for Computational Linguistics}},
  address = {{Dublin, Ireland}},
  doi = {10.18653/v1/2022.acl-long.471},
  langid = {english}
}

@inproceedings{caruana2015intelligible,
  title = {Intelligible {{Models}} for {{HealthCare}}: {{Predicting Pneumonia Risk}} and {{Hospital}} 30-Day {{Readmission}}},
  shorttitle = {Intelligible {{Models}} for {{HealthCare}}},
  booktitle = {Proceedings of the 21th {{ACM SIGKDD International Conference}} on {{Knowledge Discovery}} and {{Data Mining}}},
  author = {Caruana, Rich and Lou, Yin and Gehrke, Johannes and Koch, Paul and Sturm, Marc and Elhadad, Noemie},
  year = {2015},
  month = aug,
  pages = {1721--1730},
  publisher = {{ACM}},
  address = {{Sydney NSW Australia}},
  doi = {10.1145/2783258.2788613},
  isbn = {978-1-4503-3664-2},
  langid = {english},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\Intelligible Models for HealthCare _Caruana et al 2015.pdf}
}

@inproceedings{chalkidis2019neural,
  title = {Neural {{Legal Judgment Prediction}} in {{English}}},
  booktitle = {Proceedings of the 57th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}}},
  author = {Chalkidis, Ilias and Androutsopoulos, Ion and Aletras, Nikolaos},
  year = {2019},
  pages = {4317--4323},
  publisher = {{Association for Computational Linguistics}},
  address = {{Florence, Italy}},
  doi = {10.18653/v1/P19-1424},
  langid = {english},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\storage\\Neural Legal Judgment Prediction in English _Chalkidis et al 2019.pdf;C\:\\Users\\Hytn\\Zotero\\storage\\3TK5C5NG\\Chalkidis et al. - 2019 - Neural Legal Judgment Prediction in English.pdf}
}

@inproceedings{chalkidis2020empirical,
  title = {An {{Empirical Study}} on {{Large-Scale Multi-Label Text Classification Including Few}} and {{Zero-Shot Labels}}},
  booktitle = {Proceedings of the 2020 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}} ({{EMNLP}})},
  author = {Chalkidis, Ilias and Fergadiotis, Manos and Kotitsas, Sotiris and Malakasiotis, Prodromos and Aletras, Nikolaos and Androutsopoulos, Ion},
  year = {2020},
  pages = {7503--7515},
  publisher = {{Association for Computational Linguistics}},
  address = {{Online}},
  doi = {10.18653/v1/2020.emnlp-main.607},
  langid = {english},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\storage\\An Empirical Study on Large-Scale Multi-Label Text Classification Including Few _Chalkidis et al 2020.pdf}
}

@inproceedings{chalkidis2020legalbert,
  title = {{{LEGAL-BERT}}: {{The Muppets}} Straight out of {{Law School}}},
  shorttitle = {{{LEGAL-BERT}}},
  booktitle = {Findings of the {{Association}} for {{Computational Linguistics}}: {{EMNLP}} 2020},
  author = {Chalkidis, Ilias and Fergadiotis, Manos and Malakasiotis, Prodromos and Aletras, Nikolaos and Androutsopoulos, Ion},
  year = {2020},
  pages = {2898--2904},
  publisher = {{Association for Computational Linguistics}},
  address = {{Online}},
  doi = {10.18653/v1/2020.findings-emnlp.261},
  langid = {english},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\storage\\LEGAL-BERT _Chalkidis et al 2020.pdf}
}

@inproceedings{chalkidis2022fairlex,
  title = {{{FairLex}}: {{A Multilingual Benchmark}} for {{Evaluating Fairness}} in {{Legal Text Processing}}},
  shorttitle = {{{FairLex}}},
  booktitle = {Proceedings of the 60th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}} ({{Volume}} 1: {{Long Papers}})},
  author = {Chalkidis, Ilias and Pasini, Tommaso and Zhang, Sheng and Tomada, Letizia and Schwemer, Sebastian and S{\o}gaard, Anders},
  year = {2022},
  pages = {4389--4406},
  publisher = {{Association for Computational Linguistics}},
  address = {{Dublin, Ireland}},
  doi = {10.18653/v1/2022.acl-long.301},
  langid = {english},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\storage\\FairLex _Chalkidis et al 2022.pdf}
}

@inproceedings{chalkidis2022lexglue,
  title = {{{LexGLUE}}: {{A Benchmark Dataset}} for {{Legal Language Understanding}} in {{English}}},
  shorttitle = {{{LexGLUE}}},
  booktitle = {Proceedings of the 60th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}} ({{Volume}} 1: {{Long Papers}})},
  author = {Chalkidis, Ilias and Jana, Abhik and Hartung, Dirk and Bommarito, Michael and Androutsopoulos, Ion and Katz, Daniel and Aletras, Nikolaos},
  year = {2022},
  pages = {4310--4330},
  publisher = {{Association for Computational Linguistics}},
  address = {{Dublin, Ireland}},
  doi = {10.18653/v1/2022.acl-long.297},
  langid = {english},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\storage\\LexGLUE _Chalkidis et al 2022.pdf}
}

@inproceedings{chalkidis2022lexgluea,
  title = {{{LexGLUE}}: {{A Benchmark Dataset}} for {{Legal Language Understanding}} in {{English}}},
  shorttitle = {{{LexGLUE}}},
  booktitle = {Proceedings of the 60th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}} ({{Volume}} 1: {{Long Papers}})},
  author = {Chalkidis, Ilias and Jana, Abhik and Hartung, Dirk and Bommarito, Michael and Androutsopoulos, Ion and Katz, Daniel and Aletras, Nikolaos},
  year = {2022},
  pages = {4310--4330},
  publisher = {{Association for Computational Linguistics}},
  address = {{Dublin, Ireland}},
  doi = {10.18653/v1/2022.acl-long.297},
  langid = {english},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\storage\\LexGLUE _Chalkidis et al 22.pdf}
}

@inproceedings{chan2011exploiting,
  title = {Exploiting Syntactico-Semantic Structures for Relation Extraction},
  booktitle = {Proceedings of the 49th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}}: {{Human Language Technologies}}},
  author = {Chan, Yee Seng and Roth, Dan},
  year = {2011},
  pages = {551--560},
  keywords = {⛔ No DOI found},
  file = {C\:\\Users\\Hytn\\Zotero\\storage\\IYPEY7Q6\\P11-1056.pdf}
}

@article{chatterjeegeneralization,
  title = {{{ON THE GENERALIZATION MYSTERY IN DEEP LEARNING}}},
  author = {Chatterjee, Satrajit and Zielinski, Piotr},
  pages = {82},
  abstract = {The generalization mystery in deep learning is the following: Why do over-parameterized neural networks trained with gradient descent (GD) generalize well on real datasets even though they are capable of fitting random datasets of comparable size? Furthermore, from among all solutions that fit the training data, how does GD find one that generalizes well (when such a well-generalizing solution exists)? We argue that the answer to both questions lies in the interaction of the gradients of different examples during training. Intuitively, if the per-example gradients are well-aligned, that is, if they are coherent, then one may expect GD to be (algorithmically) stable, and hence generalize well. We formalize this argument with an easy to compute and interpretable metric for coherence, and show that the metric takes on very different values on real and random datasets for several common vision networks. The theory also explains a number of other phenomena in deep learning, such as why some examples are reliably learned earlier than others, why early stopping works, and why it is possible to learn from noisy labels. Moreover, since the theory provides a causal explanation of how GD finds a well-generalizing solution when one exists, it motivates a class of simple modifications to GD that attenuate memorization and improve generalization.},
  langid = {english},
  keywords = {⛔ No DOI found},
  file = {C\:\\Users\\Hytn\\Zotero\\storage\\I577NNP8\\Chatterjee and Zielinski - ON THE GENERALIZATION MYSTERY IN DEEP LEARNING.pdf}
}

@inproceedings{chen2019codah,
  title = {{{CODAH}}: {{An}} Adversarially-Authored Question Answering Dataset for Common Sense},
  booktitle = {Proceedings of the 3rd Workshop on Evaluating Vector Space Representations for {{NLP}}},
  author = {Chen, Michael and D'Arcy, Mike and Liu, Alisa and Fernandez, Jared and Downey, Doug},
  year = {2019},
  month = jun,
  pages = {63--69},
  publisher = {{Association for Computational Linguistics}},
  address = {{Minneapolis, USA}},
  doi = {10.18653/v1/W19-2008},
  abstract = {Commonsense reasoning is a critical AI capability, but it is difficult to construct challenging datasets that test common sense. Recent neural question answering systems, based on large pre-trained models of language, have already achieved near-human-level performance on commonsense knowledge benchmarks. These systems do not possess human-level common sense, but are able to exploit limitations of the datasets to achieve human-level scores. We introduce the CODAH dataset, an adversarially-constructed evaluation dataset for testing common sense. CODAH forms a challenging extension to the recently-proposed SWAG dataset, which tests commonsense knowledge using sentence-completion questions that describe situations observed in video. To produce a more difficult dataset, we introduce a novel procedure for question acquisition in which workers author questions designed to target weaknesses of state-of-the-art neural question answering systems. Workers are rewarded for submissions that models fail to answer correctly both before and after fine-tuning (in cross-validation). We create 2.8k questions via this procedure and evaluate the performance of multiple state-of-the-art question answering systems on our dataset. We observe a significant gap between human performance, which is 95.3\%, and the performance of the best baseline accuracy of 65.3\% by the OpenAI GPT model.}
}

@inproceedings{chen2019no,
  title = {[{{No}} Title Found]},
  booktitle = {Proceedings of the 3rd {{Workshop}} on {{Evaluating Vector Space Representations}} For},
  author = {Chen, Michael and D'Arcy, Mike and Liu, Alisa and Fernandez, Jared and Downey, Doug},
  year = {2019},
  pages = {63--69},
  publisher = {{Association for Computational Linguistics}},
  address = {{Minneapolis, USA}},
  doi = {10.18653/v1/W19-2008},
  langid = {english}
}

@inproceedings{chen2020generating,
  title = {Generating {{Hierarchical Explanations}} on {{Text Classification}} via {{Feature Interaction Detection}}},
  booktitle = {Proceedings of the 58th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}}},
  author = {Chen, Hanjie and Zheng, Guangtao and Ji, Yangfeng},
  year = {2020},
  pages = {5578--5593},
  publisher = {{Association for Computational Linguistics}},
  address = {{Online}},
  doi = {10.18653/v1/2020.acl-main.494},
  langid = {english},
  keywords = {permutation-based},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\storage\\Generating Hierarchical Explanations on Text Classification via Feature _Chen et al 2020.pdf}
}

@misc{chen2021lightner,
  title = {{{LightNER}}: {{A Lightweight Generative Framework}} with {{Prompt-guided Attention}} for {{Low-resource NER}}},
  shorttitle = {{{LightNER}}},
  author = {Chen, Xiang and Zhang, Ningyu and Li, Lei and Xie, Xin and Deng, Shumin and Tan, Chuanqi and Huang, Fei and Si, Luo and Chen, Huajun},
  year = {2021},
  month = sep,
  number = {arXiv:2109.00720},
  eprint = {2109.00720},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  abstract = {Most existing NER methods rely on extensive labeled data for model training, which struggles in the low-resource scenarios with limited training data. Recently, prompt-tuning methods for pre-trained language models have achieved remarkable performance in few-shot learning by exploiting prompts as task guidance to reduce the gap between training progress and downstream tuning. Inspired by prompt learning, we propose a novel lightweight generative framework with promptguided attention for low-resource NER (LightNER). Specifically, we construct the semantic-aware answer space of entity categories for prompt learning to generate the entity span sequence and entity categories without any label-specific classifiers. We further propose prompt-guided attention by incorporating continuous prompts into the self-attention layer to re-modulate the attention and adapt pre-trained weights. Note that we only tune those continuous prompts with the whole parameter of the pre-trained language model fixed, thus, making our approach lightweight and flexible for low-resource scenarios and can better transfer knowledge across domains. Experimental results show that LightNER can obtain comparable performance in the standard supervised setting and outperform strong baselines in low-resource settings by tuning only a small part of the parameters.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computation and Language},
  file = {C\:\\Users\\Hytn\\Zotero\\storage\\NPN9DWKR\\Chen 等。 - 2021 - LightNER A Lightweight Generative Framework with .pdf}
}

@inproceedings{chia2022relationprompt,
  title = {{{RelationPrompt}}: {{Leveraging Prompts}} to {{Generate Synthetic Data}} for {{Zero-Shot Relation Triplet Extraction}}},
  shorttitle = {{{RelationPrompt}}},
  booktitle = {Findings of the {{Association}} for {{Computational Linguistics}}: {{ACL}} 2022},
  author = {Chia, Yew Ken and Bing, Lidong and Poria, Soujanya and Si, Luo},
  year = {2022},
  pages = {45--57},
  publisher = {{Association for Computational Linguistics}},
  address = {{Dublin, Ireland}},
  doi = {10.18653/v1/2022.findings-acl.5},
  langid = {english},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\storage\\RelationPrompt _Chia et al 2022.pdf}
}

@inproceedings{chia2022relationprompta,
  title = {{{RelationPrompt}}: {{Leveraging Prompts}} to {{Generate Synthetic Data}} for {{Zero-Shot Relation Triplet Extraction}}},
  shorttitle = {{{RelationPrompt}}},
  booktitle = {Findings of the {{Association}} for {{Computational Linguistics}}: {{ACL}} 2022},
  author = {Chia, Yew Ken and Bing, Lidong and Poria, Soujanya and Si, Luo},
  year = {2022},
  pages = {45--57},
  publisher = {{Association for Computational Linguistics}},
  address = {{Dublin, Ireland}},
  doi = {10.18653/v1/2022.findings-acl.5},
  langid = {english},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\storage\\RelationPrompt _Chia et al 22.pdf}
}

@inproceedings{christopoulou2019connecting,
  title = {Connecting the {{Dots}}: {{Document-level Neural Relation Extraction}} with {{Edge-oriented Graphs}}},
  booktitle = {Proceedings of the 2019 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}} and the 9th {{International Joint Conference}} on {{Natural Language Processing}} ({{EMNLP-IJCNLP}})},
  author = {Christopoulou, Fenia and Miwa, Makoto and Ananiadou, Sophia},
  year = {2019},
  pages = {4924--4935},
  doi = {10.18653/v1/D19-1498},
  file = {C\:\\Users\\Hytn\\Zotero\\storage\\PAA3IDDX\\Christopoulou et al_2019_Connecting the Dots.pdf}
}

@article{church2022emerging,
  title = {Emerging {{Trends}}: {{SOTA-Chasing}}},
  shorttitle = {Emerging {{Trends}}},
  author = {Church, Kenneth Ward and Kordoni, Valia},
  year = {2022},
  month = mar,
  journal = {Natural Language Engineering},
  volume = {28},
  number = {2},
  pages = {249--269},
  issn = {1351-3249, 1469-8110},
  doi = {10.1017/S1351324922000043},
  abstract = {Abstract             Many papers are chasing state-of-the-art (SOTA) numbers, and more will do so in the future. SOTA-chasing comes with many costs. SOTA-chasing squeezes out more promising opportunities such as coopetition and interdisciplinary collaboration. In addition, there is a risk that too much SOTA-chasing could lead to claims of superhuman performance, unrealistic expectations, and the next AI winter. Two root causes for SOTA-chasing will be discussed: (1) lack of leadership and (2) iffy reviewing processes. SOTA-chasing may be similar to the replication crisis in the scientific literature. The replication crisis is yet another example, like evaluation, of over-confidence in accepted practices and the scientific method, even when such practices lead to absurd consequences.},
  langid = {english},
  file = {C\:\\Users\\Hytn\\Zotero\\storage\\I9WVAHNK\\Emerging Trends_Church_Kordoni 2022.pdf}
}

@inproceedings{clark2016deep,
  title = {Deep {{Reinforcement Learning}} for {{Mention-Ranking Coreference Models}}},
  booktitle = {Proceedings of the 2016 {{Conference}} on {{Empirical Methods}} in {{Natural}}           {{Language Processing}}},
  author = {Clark, Kevin and Manning, Christopher D.},
  year = {2016},
  pages = {2256--2262},
  publisher = {{Association for Computational Linguistics}},
  address = {{Austin, Texas}},
  doi = {10.18653/v1/D16-1245},
  langid = {english},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\storage\\Deep Reinforcement Learning for Mention-Ranking Coreference Models _Clark_Manning 2016.pdf}
}

@inproceedings{clark2020learning,
  title = {Learning to {{Model}} and {{Ignore Dataset Bias}} with {{Mixed Capacity Ensembles}}},
  booktitle = {Findings of the {{Association}} for {{Computational Linguistics}}: {{EMNLP}} 2020},
  author = {Clark, Christopher and Yatskar, Mark and Zettlemoyer, Luke},
  year = {2020},
  pages = {3031--3045},
  publisher = {{Association for Computational Linguistics}},
  address = {{Online}},
  doi = {10.18653/v1/2020.findings-emnlp.272},
  langid = {english},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\storage\\Learning to Model and Ignore Dataset Bias with Mixed Capacity Ensembles _Clark et al 2020.pdf}
}

@article{journals/ml/CortesV95,
  title = {Support-Vector Networks.},
  author = {Cortes, Corinna and Vapnik, Vladimir},
  year = {1995},
  journal = {Machine Learning},
  volume = {20},
  number = {3},
  pages = {273--297},
  added-at = {2011-06-29T16:50:31.000+0200},
  biburl = {https://www.bibsonomy.org/bibtex/287a1b1cb4913bafb37c82d66b0bc088a/oliver\textsubscript{a}wm},
  ee = {http://dx.doi.org/10.1007/BF00994018},
  interhash = {c223c465141618ad63aac5a6132280f7},
  intrahash = {87a1b1cb4913bafb37c82d66b0bc088a},
  keywords = {❓ Multiple DOI,awm2011 svm},
  timestamp = {2011-06-29T16:50:31.000+0200}
}

@article{cui2022stable,
  title = {Stable Learning Establishes Some Common Ground between Causal Inference and Machine Learning},
  author = {Cui, Peng and Athey, Susan},
  year = {2022},
  month = feb,
  journal = {Nature Machine Intelligence},
  volume = {4},
  number = {2},
  pages = {110--115},
  issn = {2522-5839},
  doi = {10.1038/s42256-022-00445-z},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\storage\\Stable learning establishes some common ground between causal inference and _Cui_Athey 2022.pdf}
}

@misc{cui2022survey,
  title = {A {{Survey}} on {{Legal Judgment Prediction}}: {{Datasets}}, {{Metrics}}, {{Models}} and {{Challenges}}},
  shorttitle = {A {{Survey}} on {{Legal Judgment Prediction}}},
  author = {Cui, Junyun and Shen, Xiaoyu and Nie, Feiping and Wang, Zheng and Wang, Jinglong and Chen, Yulong},
  year = {2022},
  month = apr,
  number = {arXiv:2204.04859},
  eprint = {2204.04859},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  abstract = {Legal judgment prediction (LJP) applies Natural Language Processing (NLP) techniques to predict judgment results based on fact descriptions automatically. Recently, large-scale public datasets and advances in NLP research have led to increasing interest in LJP. Despite a clear gap between machine and human performance, impressive results have been achieved in various benchmark datasets. In this paper, to address the current lack of comprehensive survey of existing LJP tasks, datasets, models and evaluations, (1) we analyze 31 LJP datasets in 6 languages, present their construction process and define a classification method of LJP with 3 different attributes; (2) we summarize 14 evaluation metrics under four categories for different outputs of LJP tasks; (3) we review 12 legal-domain pretrained models in 3 languages and highlight 3 major research directions for LJP; (4) we show the state-of-art results for 8 representative datasets from different court cases and discuss the open challenges. This paper can provide up-to-date and comprehensive reviews to help readers understand the status of LJP. We hope to facilitate both NLP researchers and legal professionals for further joint efforts in this problem.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {C\:\\Users\\Hytn\\Zotero\\storage\\JQSK2UMH\\Cui et al. - 2022 - A Survey on Legal Judgment Prediction Datasets, M.pdf}
}

@article{cui2022teaching,
  title = {Teaching {{Machines}} to {{Read}}, {{Answer}} and {{Explain}}},
  author = {Cui, Yiming and Liu, Ting and Che, Wanxiang and Chen, Zhigang and Wang, Shijin},
  year = {2022},
  journal = {IEEE/ACM Transactions on Audio, Speech, and Language Processing},
  volume = {30},
  pages = {1483--1492},
  issn = {2329-9290, 2329-9304},
  doi = {10.1109/TASLP.2022.3156789},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\storage\\Teaching Machines to Read, Answer and Explain _Cui et al 2022.pdf}
}

@inproceedings{dai2019joint,
  title = {Joint Extraction of Entities and Overlapping Relations Using Position-Attentive Sequence Labeling},
  booktitle = {Proceedings of the {{AAAI Conference}} on {{Artificial Intelligence}}},
  author = {Dai, Dai and Xiao, Xinyan and Lyu, Yajuan and Dou, Shan and She, Qiaoqiao and Wang, Haifeng},
  year = {2019},
  volume = {33},
  pages = {6300--6308},
  keywords = {⛔ No DOI found}
}

@inproceedings{das2022automatic,
  title = {Automatic Error Analysis for Document-Level Information Extraction},
  booktitle = {Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: {{Long}} Papers)},
  author = {Das, Aliva and Du, Xinya and Wang, Barry and Shi, Kejian and Gu, Jiayuan and Porter, Thomas and Cardie, Claire},
  year = {2022},
  month = may,
  pages = {3960--3975},
  publisher = {{Association for Computational Linguistics}},
  address = {{Dublin, Ireland}},
  abstract = {Document-level information extraction (IE) tasks have recently begun to be revisited in earnest using the end-to-end neural network techniques that have been successful on their sentence-level IE counterparts. Evaluation of the approaches, however, has been limited in a number of dimensions. In particular, the precision/recall/F1 scores typically reported provide few insights on the range of errors the models make. We build on the work of Kummerfeld and Klein (2013) to propose a transformation-based framework for automating error analysis in document-level event and (N-ary) relation extraction. We employ our framework to compare two state-of-the-art document-level template-filling approaches on datasets from three domains; and then, to gauge progress in IE since its inception 30 years ago, vs. four systems from the MUC-4 (1992) evaluation.},
  file = {C\:\\Users\\Hytn\\Zotero\\storage\\I3HSJJRB\\Das 等。 - 2022 - Automatic error analysis for document-level inform.pdf}
}

@article{DBLP:journals/corr/abs-2004-14546,
  title = {{{WT5}}?! {{Training}} Text-to-Text Models to Explain Their Predictions.},
  author = {Narang, Sharan and Raffel, Colin and Lee, Katherine and Roberts, Adam and Fiedel, Noah and Malkan, Karishma},
  year = {2020},
  journal = {CoRR},
  volume = {abs/2004.14546},
  cdate = {1577836800000},
  publtype = {informal},
  keywords = {⛔ No DOI found},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\storage\\WT5 _Narang et al 2020.pdf}
}

@inproceedings{devlin2018bert,
  title = {{{BERT}}: {{Pre-training}} of {{Deep Bidirectional Transformers}} for {{Language Understanding}}},
  booktitle = {Proceedings of the 2019 {{Conference}} of the {{North American Chapter}} of the {{Association}} for {{Computational Linguistics}}: {{Human Language Technologies}}, {{Volume}} 1 ({{Long}} and {{Short Papers}})},
  author = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina N.},
  year = {2018},
  pages = {4171--4186},
  keywords = {⛔ No DOI found},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\storage\\BERT _Devlin et al 2018.pdf}
}

@inproceedings{devlin2019bert,
  title = {{{BERT}}: {{Pre-training}} of {{Deep Bidirectional Transformers}} for {{Language Understanding}}},
  booktitle = {Proceedings of the 2019 {{Conference}} of the {{North}}},
  author = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  year = {2019},
  pages = {4171--4186},
  publisher = {{Association for Computational Linguistics}},
  address = {{Minneapolis, Minnesota}},
  doi = {10.18653/v1/N19-1423},
  langid = {english},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\BERT _Devlin et al 2019.pdf}
}

@misc{dias2022state,
  title = {State of the {{Art}} in {{Artificial Intelligence}} Applied to the {{Legal Domain}}},
  author = {Dias, Jo{\~a}o and Santos, Pedro A. and Cordeiro, Nuno and Antunes, Ana and Martins, Bruno and Baptista, Jorge and Gon{\c c}alves, Carlos},
  year = {2022},
  month = mar,
  number = {arXiv:2204.07047},
  eprint = {2204.07047},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  abstract = {While Artificial Intelligence applied to the legal domain is a topic with origins in the last century, recent advances in Artificial Intelligence are posed to revolutionize it. This work presents an overview and contextualizes the main advances on the field of Natural Language Processing and how these advances have been used to further the state of the art in legal text analysis.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {C\:\\Users\\Hytn\\Zotero\\storage\\24ZRXG9H\\Dias et al. - 2022 - State of the Art in Artificial Intelligence applie.pdf}
}

@inproceedings{ding2019cognitive,
  title = {Cognitive {{Graph}} for {{Multi-Hop Reading Comprehension}} at {{Scale}}},
  booktitle = {Proceedings of the 57th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}}},
  author = {Ding, Ming and Zhou, Chang and Chen, Qibin and Yang, Hongxia and Tang, Jie},
  year = {2019},
  pages = {2694--2703},
  doi = {10.18653/v1/P19-1259},
  file = {C\:\\Users\\Hytn\\Zotero\\storage\\PA5R8DWW\\Ding et al_2019_Cognitive Graph for Multi-Hop Reading Comprehension at Scale.pdf}
}

@inproceedings{du2019attribution,
  title = {On {{Attribution}} of {{Recurrent Neural Network Predictions}} via {{Additive Decomposition}}},
  booktitle = {The {{World Wide Web Conference}} on - {{WWW}} '19},
  author = {Du, Mengnan and Liu, Ninghao and Yang, Fan and Ji, Shuiwang and Hu, Xia},
  year = {2019},
  pages = {383--393},
  publisher = {{ACM Press}},
  address = {{San Francisco, CA, USA}},
  doi = {10.1145/3308558.3313545},
  isbn = {978-1-4503-6674-8},
  langid = {english},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\On Attribution of Recurrent Neural Network Predictions via Additive _Du et al 2019.pdf}
}

@article{du2019techniques,
  title = {Techniques for Interpretable Machine Learning},
  author = {Du, Mengnan and Liu, Ninghao and Hu, Xia},
  year = {2019},
  month = dec,
  journal = {Communications of the ACM},
  volume = {63},
  number = {1},
  pages = {68--77},
  issn = {0001-0782, 1557-7317},
  doi = {10.1145/3359786},
  abstract = {Uncovering the mysterious ways machine learning models make decisions.},
  langid = {english},
  file = {C\:\\Users\\Hytn\\Zotero\\storage\\9BUEI6A8\\Du et al. - 2019 - Techniques for interpretable machine learning.pdf}
}

@inproceedings{du2021interpreting,
  title = {Towards {{Interpreting}} and {{Mitigating Shortcut Learning Behavior}} of {{NLU}} Models},
  booktitle = {Proceedings of the 2021 {{Conference}} of the {{North American Chapter}} of the {{Association}} for {{Computational Linguistics}}: {{Human Language Technologies}}},
  author = {Du, Mengnan and Manjunatha, Varun and Jain, Rajiv and Deshpande, Ruchi and Dernoncourt, Franck and Gu, Jiuxiang and Sun, Tong and Hu, Xia},
  year = {2021},
  pages = {915--929},
  publisher = {{Association for Computational Linguistics}},
  address = {{Online}},
  doi = {10.18653/v1/2021.naacl-main.71},
  langid = {english},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\storage\\Towards Interpreting and Mitigating Shortcut Learning Behavior of NLU models _Du et al 2021.pdf}
}

@inproceedings{eberts2020spanbased,
  title = {Span-{{Based Joint Entity}} and {{Relation Extraction}} with {{Transformer Pre-Training}}.},
  booktitle = {{{ECAI}}},
  author = {Eberts, Markus and Ulges, Adrian},
  year = {2020},
  pages = {2006--2013},
  keywords = {⛔ No DOI found}
}

@inproceedings{fader2014open,
  title = {Open Question Answering over Curated and Extracted Knowledge Bases},
  booktitle = {Proceedings of the 20th {{ACM SIGKDD}} International Conference on {{Knowledge}} Discovery and Data Mining},
  author = {Fader, Anthony and Zettlemoyer, Luke and Etzioni, Oren},
  year = {2014},
  month = aug,
  pages = {1156--1165},
  publisher = {{ACM}},
  address = {{New York New York USA}},
  doi = {10.1145/2623330.2623677},
  isbn = {978-1-4503-2956-9},
  langid = {english},
  file = {C\:\\Users\\Hytn\\Zotero\\storage\\5XYFM7R2\\Fader et al_2014_Open question answering over curated and extracted knowledge bases.pdf}
}

@inproceedings{fan2019using,
  title = {Using {{Local Knowledge Graph Construction}} to {{Scale Seq2Seq Models}} to {{Multi-Document Inputs}}},
  booktitle = {Proceedings of the 2019 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}} and the 9th {{International Joint Conference}} on {{Natural Language Processing}} ({{EMNLP-IJCNLP}})},
  author = {Fan, Angela and Gardent, Claire and Braud, Chlo{\'e} and Bordes, Antoine},
  year = {2019},
  pages = {4184--4194},
  publisher = {{Association for Computational Linguistics}},
  address = {{Hong Kong, China}},
  doi = {10.18653/v1/D19-1428},
  langid = {english},
  keywords = {graph linearization},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\storage\\Using Local Knowledge Graph Construction to Scale Seq2Seq Models to _Fan et al 2019.pdf}
}

@inproceedings{feng2018pathologies,
  title = {Pathologies of {{Neural Models Make Interpretations Difficult}}},
  booktitle = {Proceedings of the 2018 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}}},
  author = {Feng, Shi and Wallace, Eric and Grissom II, Alvin and Iyyer, Mohit and Rodriguez, Pedro and {Boyd-Graber}, Jordan},
  year = {2018},
  pages = {3719--3728},
  publisher = {{Association for Computational Linguistics}},
  address = {{Brussels, Belgium}},
  doi = {10.18653/v1/D18-1407},
  langid = {english},
  keywords = {permutation-based},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\storage\\Pathologies of Neural Models Make Interpretations Difficult _Feng et al 2018.pdf}
}

@inproceedings{ferracane2019evaluating,
  title = {Evaluating {{Discourse}} in {{Structured Text Representations}}},
  booktitle = {Proceedings of the 57th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}}},
  author = {Ferracane, Elisa and Durrett, Greg and Li, Junyi Jessy and Erk, Katrin},
  year = {2019},
  pages = {646--653},
  doi = {10.18653/v1/P19-1062},
  file = {C\:\\Users\\Hytn\\Zotero\\storage\\BVZTPHXK\\Ferracane et al_2019_Evaluating Discourse in Structured Text Representations.pdf}
}

@article{fort2019stiffness,
  title = {Stiffness: {{A New Perspective}} on {{Generalization}} in {{Neural Networks}}},
  author = {Fort, Stanislav and Nowak, Pawe{\l} Krzysztof and Jastrzebski, Stanislaw and Narayanan, Srini},
  year = {2019},
  month = jan,
  abstract = {In this paper we develop a new perspective on generalization of neural networks by proposing and investigating the concept of a neural network stiffness. We measure how stiff a network is by looking at how a small gradient step in the network's parameters on one example affects the loss on another example. Higher stiffness suggests that a network is learning features that generalize. In particular, we study how stiffness depends on 1) class membership, 2) distance between data points in the input space, 3) training iteration, and 4) learning rate. We present experiments on MNIST, FASHION MNIST, and CIFAR-10/100 using fully-connected and convolutional neural networks, as well as on a transformer-based NLP model. We demonstrate the connection between stiffness and generalization, and observe its dependence on learning rate. When training on CIFAR-100, the stiffness matrix exhibits a coarse-grained behavior indicative of the model's awareness of super-class membership. In addition, we measure how stiffness between two data points depends on their mutual input-space distance, and establish the concept of a dynamical critical length \textendash{} a distance below which a parameter update based on a data point influences its neighbors.},
  keywords = {⛔ No DOI found},
  file = {C\:\\Users\\Hytn\\Zotero\\storage\\CPUHLVA2\\Fort et al_2019_Stiffness.pdf}
}

@article{freitas2014comprehensible,
  title = {Comprehensible Classification Models: A Position Paper},
  shorttitle = {Comprehensible Classification Models},
  author = {Freitas, Alex A.},
  year = {2014},
  month = mar,
  journal = {ACM SIGKDD Explorations Newsletter},
  volume = {15},
  number = {1},
  pages = {1--10},
  issn = {1931-0145, 1931-0153},
  doi = {10.1145/2594473.2594475},
  abstract = {The vast majority of the literature evaluates the performance of classification models using only the criterion of predictive accuracy. This paper reviews the case for considering also the comprehensibility (interpretability) of classification models, and discusses the interpretability of five types of classification models, namely decision trees, classification rules, decision tables, nearest neighbors and Bayesian network classifiers. We discuss both interpretability issues which are specific to each of those model types and more generic interpretability issues, namely the drawbacks of using model size as the only criterion to evaluate the comprehensibility of a model, and the use of monotonicity constraints to improve the comprehensibility and acceptance of classification models by users.},
  langid = {english},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\Comprehensible classification models _Freitas 2014.pdf}
}

@inproceedings{fu2019graphrel,
  title = {{{GraphRel}}: {{Modeling}} Text as Relational Graphs for Joint Entity and Relation Extraction},
  booktitle = {Proceedings of the 57th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}}},
  author = {Fu, Tsu-Jui and Li, Peng-Hsuan and Ma, Wei-Yun},
  year = {2019},
  pages = {1409--1418},
  doi = {10.18653/v1/P19-1136},
  file = {C\:\\Users\\Hytn\\Zotero\\storage\\AMP8EN3R\\Fu et al_2019_GraphRel.pdf}
}

@article{fu2020rethinking,
  title = {Rethinking {{Generalization}} of {{Neural Models}}: {{A Named Entity Recognition Case Study}}},
  author = {Fu, Jinlan and Liu, Pengfei and Zhang, Qi},
  year = {2020},
  month = apr,
  journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
  volume = {34},
  number = {05},
  pages = {7732--7739},
  issn = {2374-3468},
  doi = {10.1609/aaai.v34i05.6276},
  abstract = {{$<$}p{$>$}While neural network-based models have achieved impressive performance on a large body of NLP tasks, the generalization behavior of different models remains poorly understood: Does this excellent performance imply a perfect generalization model, or are there still some limitations? In this paper, we take the NER task as a testbed to analyze the generalization behavior of existing models from different perspectives and characterize the differences of their generalization abilities through the lens of our proposed measures, which guides us to better design models and training methods. Experiments with in-depth analyses diagnose the bottleneck of existing neural NER models in terms of breakdown performance analysis, annotation errors, dataset bias, and category relationships, which suggest directions for improvement. We have released the datasets: (ReCoNLL, PLONER) for the future research at our project page: http://pfliu.com/InterpretNER/.{$<$}/p{$>$}},
  file = {C\:\\Users\\Hytn\\Zotero\\storage\\5XZZC3CG\\Fu et al_2020_Rethinking Generalization of Neural Models.pdf}
}

@inproceedings{gao2021simcse,
  title = {{{SimCSE}}: {{Simple Contrastive Learning}} of {{Sentence Embeddings}}},
  shorttitle = {{{SimCSE}}},
  booktitle = {Proceedings of the 2021 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}}},
  author = {Gao, Tianyu and Yao, Xingcheng and Chen, Danqi},
  year = {2021},
  pages = {6894--6910},
  publisher = {{Association for Computational Linguistics}},
  address = {{Online and Punta Cana, Dominican Republic}},
  doi = {10.18653/v1/2021.emnlp-main.552},
  langid = {english},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\SimCSE _Gao et al 2021.pdf}
}

@inproceedings{gao2021simcsea,
  title = {{{SimCSE}}: {{Simple Contrastive Learning}} of {{Sentence Embeddings}}},
  shorttitle = {{{SimCSE}}},
  booktitle = {Proceedings of the 2021 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}}},
  author = {Gao, Tianyu and Yao, Xingcheng and Chen, Danqi},
  year = {2021},
  pages = {6894--6910},
  publisher = {{Association for Computational Linguistics}},
  address = {{Online and Punta Cana, Dominican Republic}},
  doi = {10.18653/v1/2021.emnlp-main.552},
  langid = {english},
  file = {C\:\\Users\\Hytn\\Zotero\\storage\\JUWCBHJR\\SimCSE_Gao et al 2021.pdf}
}

@inproceedings{gaonkar2020modeling,
  title = {Modeling {{Label Semantics}} for {{Predicting Emotional Reactions}}},
  booktitle = {Proceedings of the 58th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}}},
  author = {Gaonkar, Radhika and Kwon, Heeyoung and Bastan, Mohaddeseh and Balasubramanian, Niranjan and Chambers, Nathanael},
  year = {2020},
  pages = {4687--4692},
  publisher = {{Association for Computational Linguistics}},
  address = {{Online}},
  doi = {10.18653/v1/2020.acl-main.426},
  langid = {english},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\storage\\Modeling Label Semantics for Predicting Emotional Reactions _Gaonkar et al 2020.pdf}
}

@inproceedings{gardent2017creating,
  title = {Creating Training Corpora for Nlg Micro-Planning},
  booktitle = {55th Annual Meeting of the {{Association}} for {{Computational Linguistics}} ({{ACL}})},
  author = {Gardent, Claire and Shimorina, Anastasia and Narayan, Shashi and {Perez-Beltrachini}, Laura},
  year = {2017},
  keywords = {⛔ No DOI found}
}

@inproceedings{gardent2017creatinga,
  title = {Creating {{Training Corpora}} for {{NLG Micro-Planners}}},
  booktitle = {Proceedings of the 55th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}} ({{Volume}} 1: {{Long Papers}})},
  author = {Gardent, Claire and Shimorina, Anastasia and Narayan, Shashi and {Perez-Beltrachini}, Laura},
  year = {2017},
  month = jul,
  pages = {179--188},
  publisher = {{Association for Computational Linguistics}},
  address = {{Vancouver, Canada}},
  doi = {10.18653/v1/P17-1017},
  abstract = {In this paper, we present a novel framework for semi-automatically creating linguistically challenging micro-planning data-to-text corpora from existing Knowledge Bases. Because our method pairs data of varying size and shape with texts ranging from simple clauses to short texts, a dataset created using this framework provides a challenging benchmark for microplanning. Another feature of this framework is that it can be applied to any large scale knowledge base and can therefore be used to train and learn KB verbalisers. We apply our framework to DBpedia data and compare the resulting dataset with Wen et al. 2016's. We show that while Wen et al.'s dataset is more than twice larger than ours, it is less diverse both in terms of input and in terms of text. We thus propose our corpus generation framework as a novel method for creating challenging data sets from which NLG models can be learned which are capable of handling the complex interactions occurring during in micro-planning between lexicalisation, aggregation, surface realisation, referring expression generation and sentence segmentation. To encourage researchers to take up this challenge, we made available a dataset of 21,855 data/text pairs created using this framework in the context of the WebNLG shared task.},
  keywords = {WebNLG},
  file = {C\:\\Users\\Hytn\\Zotero\\storage\\EZRGDHER\\Gardent et al_2017_Creating Training Corpora for NLG Micro-Planners.pdf}
}

@inproceedings{garg2019counterfactual,
  title = {Counterfactual {{Fairness}} in {{Text Classification}} through {{Robustness}}},
  booktitle = {Proceedings of the 2019 {{AAAI}}/{{ACM Conference}} on {{AI}}, {{Ethics}}, and {{Society}}},
  author = {Garg, Sahaj and Perot, Vincent and Limtiaco, Nicole and Taly, Ankur and Chi, Ed H. and Beutel, Alex},
  year = {2019},
  month = jan,
  pages = {219--226},
  publisher = {{ACM}},
  address = {{Honolulu HI USA}},
  doi = {10.1145/3306618.3317950},
  abstract = {In this paper, we study counterfactual fairness in text classification, which asks the question: How would the prediction change if the sensitive attribute referenced in the example were different? Toxicity classifiers demonstrate a counterfactual fairness issue by predicting that ``Some people are gay'' is toxic while ``Some people are straight'' is nontoxic. We offer a metric, counterfactual token fairness (CTF), for measuring this particular form of fairness in text classifiers, and describe its relationship with group fairness. Further, we offer three approaches, blindness, counterfactual augmentation, and counterfactual logit pairing (CLP), for optimizing counterfactual token fairness during training, bridging the robustness and fairness literature. Empirically, we find that blindness and CLP address counterfactual token fairness. The methods do not harm classifier performance, and have varying tradeoffs with group fairness. These approaches, both for measurement and optimization, provide a new path forward for addressing fairness concerns in text classification.},
  isbn = {978-1-4503-6324-2},
  langid = {english},
  file = {C\:\\Users\\Hytn\\Zotero\\storage\\86QRR268\\Garg et al. - 2019 - Counterfactual Fairness in Text Classification thr.pdf}
}

@article{geirhos2020shortcut,
  title = {Shortcut Learning in Deep Neural Networks},
  author = {Geirhos, Robert and Jacobsen, J{\"o}rn-Henrik and Michaelis, Claudio and Zemel, Richard and Brendel, Wieland and Bethge, Matthias and Wichmann, Felix A.},
  year = {2020},
  month = nov,
  journal = {Nature Machine Intelligence},
  volume = {2},
  number = {11},
  pages = {665--673},
  issn = {2522-5839},
  doi = {10.1038/s42256-020-00257-z},
  langid = {english},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\storage\\Shortcut learning in deep neural networks _Geirhos et al 2020.pdf;C\:\\Users\\Hytn\\Zotero\\storage\\DK8A2Y4X\\Geirhos et al. - 2020 - Shortcut learning in deep neural networks.pdf}
}

@inproceedings{glockner2018breaking,
  title = {Breaking {{NLI Systems}} with {{Sentences}} That {{Require Simple Lexical Inferences}}},
  booktitle = {Proceedings of the 56th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}} ({{Volume}} 2: {{Short Papers}})},
  author = {Glockner, Max and Shwartz, Vered and Goldberg, Yoav},
  year = {2018},
  pages = {650--655},
  publisher = {{Association for Computational Linguistics}},
  address = {{Melbourne, Australia}},
  doi = {10.18653/v1/P18-2103},
  langid = {english},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\storage\\Breaking NLI Systems with Sentences that Require Simple Lexical Inferences _Glockner et al 2018.pdf}
}

@book{glymour2016causal,
  title = {Causal Inference in Statistics: {{A}} Primer},
  author = {Glymour, Madelyn and Pearl, Judea and Jewell, Nicholas P},
  year = {2016},
  publisher = {{John Wiley \& Sons}}
}

@article{gormley2015improved,
  title = {Improved Relation Extraction with Feature-Rich Compositional Embedding Models},
  author = {Gormley, Matthew R and Yu, Mo and Dredze, Mark},
  year = {2015},
  journal = {arXiv preprint arXiv:1505.02419},
  eprint = {1505.02419},
  eprinttype = {arxiv},
  archiveprefix = {arXiv},
  keywords = {⛔ No DOI found}
}

@article{gu2017chemicalinduced,
  title = {Chemical-Induced Disease Relation Extraction via Convolutional Neural Network.},
  author = {Gu, Jinghang and Sun, Fuqing and Qian, Longhua and Zhou, Guodong},
  year = {2017},
  journal = {Database},
  volume = {2017},
  number = {1},
  keywords = {⛔ No DOI found}
}

@article{guidotti2019survey,
  title = {A {{Survey}} of {{Methods}} for {{Explaining Black Box Models}}},
  author = {Guidotti, Riccardo and Monreale, Anna and Ruggieri, Salvatore and Turini, Franco and Giannotti, Fosca and Pedreschi, Dino},
  year = {2019},
  month = sep,
  journal = {ACM Computing Surveys},
  volume = {51},
  number = {5},
  pages = {1--42},
  issn = {0360-0300, 1557-7341},
  doi = {10.1145/3236009},
  abstract = {In recent years, many accurate decision support systems have been constructed as black boxes, that is as systems that hide their internal logic to the user. This lack of explanation constitutes both a practical and an ethical issue. The literature reports many approaches aimed at overcoming this crucial weakness, sometimes at the cost of sacrificing accuracy for interpretability. The applications in which black box decision systems can be used are various, and each approach is typically developed to provide a solution for a specific problem and, as a consequence, it explicitly or implicitly delineates its own definition of interpretability and explanation. The aim of this article is to provide a classification of the main problems addressed in the literature with respect to the notion of explanation and the type of black box system. Given a problem definition, a black box type, and a desired explanation, this survey should help the researcher to find the proposals more useful for his own work. The proposed classification of approaches to open black box models should also be useful for putting the many research open questions in perspective.},
  langid = {english},
  file = {C\:\\Users\\Hytn\\Zotero\\storage\\6GFBMFG5\\Guidotti et al. - 2019 - A Survey of Methods for Explaining Black Box Model.pdf}
}

@inproceedings{guo2019attention,
  title = {Attention {{Guided Graph Convolutional Networks}} for {{Relation Extraction}}},
  booktitle = {Proceedings of the 57th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}}},
  author = {Guo, Zhijiang and Zhang, Yan and Lu, Wei},
  year = {2019},
  pages = {241--251},
  doi = {10.18653/v1/P19-1024},
  file = {C\:\\Users\\Hytn\\Zotero\\storage\\AUJ4VQ4Q\\Guo et al_2019_Attention Guided Graph Convolutional Networks for Relation Extraction.pdf}
}

@inproceedings{guo2022autodebias,
  title = {Auto-{{Debias}}: {{Debiasing Masked Language Models}} with {{Automated Biased Prompts}}},
  shorttitle = {Auto-{{Debias}}},
  booktitle = {Proceedings of the 60th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}} ({{Volume}} 1: {{Long Papers}})},
  author = {Guo, Yue and Yang, Yi and Abbasi, Ahmed},
  year = {2022},
  pages = {1012--1023},
  publisher = {{Association for Computational Linguistics}},
  address = {{Dublin, Ireland}},
  doi = {10.18653/v1/2022.acl-long.72},
  langid = {english},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\storage\\Auto-Debias _Guo et al 2022.pdf}
}

@article{gupta2010survey,
  title = {A {{Survey}} of {{Text Summarization Extractive Techniques}}},
  author = {Gupta, Vishal and Lehal, Gurpreet Singh},
  year = {2010},
  month = aug,
  journal = {Journal of Emerging Technologies in Web Intelligence},
  volume = {2},
  number = {3},
  pages = {258--268},
  issn = {1798-0461},
  doi = {10.4304/jetwi.2.3.258-268},
  file = {C\:\\Users\\Hytn\\Zotero\\storage\\KKIAFY5H\\Gupta_Lehal_2010_A Survey of Text Summarization Extractive Techniques.pdf}
}

@inproceedings{gupta2016table,
  title = {Table Filling Multi-Task Recurrent Neural Network for Joint Entity and Relation Extraction},
  booktitle = {Proceedings of {{COLING}} 2016, the 26th {{International Conference}} on {{Computational Linguistics}}: {{Technical Papers}}},
  author = {Gupta, Pankaj and Sch{\"u}tze, Hinrich and Andrassy, Bernt},
  year = {2016},
  pages = {2537--2547},
  keywords = {⛔ No DOI found}
}

@inproceedings{gupta2019neural,
  title = {Neural Relation Extraction within and across Sentence Boundaries},
  booktitle = {Proceedings of the {{AAAI Conference}} on {{Artificial Intelligence}}},
  author = {Gupta, Pankaj and Rajaram, Subburam and Sch{\"u}tze, Hinrich and Runkler, Thomas},
  year = {2019},
  volume = {33},
  pages = {6513--6520},
  keywords = {⛔ No DOI found}
}

@misc{gurel2022knowledge,
  title = {Knowledge {{Enhanced Machine Learning Pipeline}} against {{Diverse Adversarial Attacks}}},
  author = {G{\"u}rel, Nezihe Merve and Qi, Xiangyu and Rimanic, Luka and Zhang, Ce and Li, Bo},
  year = {2022},
  month = mar,
  number = {arXiv:2106.06235},
  eprint = {2106.06235},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  abstract = {Despite the great successes achieved by deep neural networks (DNNs), recent studies show that they are vulnerable against adversarial examples, which aim to mislead DNNs by adding small adversarial perturbations. Several defenses have been proposed against such attacks, while many of them have been adaptively attacked. In this work, we aim to enhance the ML robustness from a different perspective by leveraging domain knowledge: We propose a Knowledge Enhanced Machine Learning Pipeline (KEMLP) to integrate domain knowledge (i.e., logic relationships among different predictions) into a probabilistic graphical model via first-order logic rules. In particular, we develop KEMLP by integrating a diverse set of weak auxiliary models based on their logical relationships to the main DNN model that performs the target task. Theoretically, we provide convergence results and prove that, under mild conditions, the prediction of KEMLP is more robust than that of the main DNN model. Empirically, we take road sign recognition as an example and leverage the relationships between road signs and their shapes and contents as domain knowledge. We show that compared with adversarial training and other baselines, KEMLP achieves higher robustness against physical attacks, Lp bounded attacks, unforeseen attacks, and natural corruptions under both whitebox and blackbox settings, while still maintaining high clean accuracy.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning},
  file = {C\:\\Users\\Hytn\\Zotero\\storage\\JG7EBDDB\\Gürel et al. - 2022 - Knowledge Enhanced Machine Learning Pipeline again.pdf}
}

@inproceedings{gururangan2018annotation,
  title = {Annotation {{Artifacts}} in {{Natural Language Inference Data}}},
  booktitle = {Proceedings of the 2018 {{Conference}} of the {{North American Chapter}} of           the {{Association}} for {{Computational Linguistics}}: {{Human Language}}           {{Technologies}}, {{Volume}} 2 ({{Short Papers}})},
  author = {Gururangan, Suchin and Swayamdipta, Swabha and Levy, Omer and Schwartz, Roy and Bowman, Samuel and Smith, Noah A.},
  year = {2018},
  pages = {107--112},
  publisher = {{Association for Computational Linguistics}},
  address = {{New Orleans, Louisiana}},
  doi = {10.18653/v1/N18-2017},
  langid = {english},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\storage\\Annotation Artifacts in Natural Language Inference Data _Gururangan et al 2018.pdf}
}

@incollection{haidar2019textkdgan,
  title = {{{TextKD-GAN}}: {{Text Generation Using Knowledge Distillation}} and {{Generative Adversarial Networks}}},
  shorttitle = {{{TextKD-GAN}}},
  booktitle = {Advances in {{Artificial Intelligence}}},
  author = {Haidar, Md. Akmal and Rezagholizadeh, Mehdi},
  editor = {Meurs, Marie-Jean and Rudzicz, Frank},
  year = {2019},
  volume = {11489},
  pages = {107--118},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-030-18305-9_9},
  abstract = {Text generation is of particular interest in many NLP applications such as machine translation, language modeling, and text summarization. Generative adversarial networks (GANs) achieved a remarkable success in high quality image generation in computer vision, and recently, GANs have gained lots of interest from the NLP community as well. However, achieving similar success in NLP would be more challenging due to the discrete nature of text. In this work, we introduce a method using knowledge distillation to effectively exploit GAN setup for text generation. We demonstrate how autoencoders (AEs) can be used for providing a continuous representation of sentences, which is a smooth representation that assign non-zero probabilities to more than one word. We distill this representation to train the generator to synthesize similar smooth representations. We perform a number of experiments to validate our idea using different datasets and show that our proposed approach yields better performance in terms of the BLEU score and Jensen-Shannon distance (JSD) measure compared to traditional GANbased text generation approaches without pre-training.},
  isbn = {978-3-030-18304-2 978-3-030-18305-9},
  langid = {english},
  file = {C\:\\Users\\Hytn\\Zotero\\storage\\57F5ZGEI\\Haidar 和 Rezagholizadeh - 2019 - TextKD-GAN Text Generation Using Knowledge Distil.pdf}
}

@article{hamilton2017inductive,
  title = {Inductive Representation Learning on Large Graphs},
  author = {Hamilton, Will and Ying, Zhitao and Leskovec, Jure},
  year = {2017},
  journal = {Advances in neural information processing systems},
  volume = {30},
  keywords = {⛔ No DOI found}
}

@inproceedings{hao2021self,
  title = {Self-Attention Attribution: {{Interpreting}} Information Interactions inside Transformer},
  booktitle = {Proceedings of the {{AAAI}} Conference on Artificial Intelligence},
  author = {Hao, Yaru and Dong, Li and Wei, Furu and Xu, Ke},
  year = {2021},
  volume = {35},
  pages = {12963--12971},
  keywords = {⛔ No DOI found},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\storage\\Self-attention attribution _Hao et al 2021.pdf}
}

@inproceedings{hao2021self,
  title = {Self-Attention Attribution: {{Interpreting}} Information Interactions inside Transformer},
  booktitle = {Proceedings of the {{AAAI}} Conference on Artificial Intelligence},
  author = {Hao, Yaru and Dong, Li and Wei, Furu and Xu, Ke},
  year = {2021},
  volume = {35},
  pages = {12963--12971},
  keywords = {⛔ No DOI found},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\storage\\Self-attention attribution _Hao et al 22.pdf}
}

@article{hayashi2021wikiasp,
  title = {{{WikiAsp}}: {{A Dataset}} for {{Multi-domain Aspect-based Summarization}}},
  shorttitle = {{{WikiAsp}}},
  author = {Hayashi, Hiroaki and Budania, Prashant and Wang, Peng and Ackerson, Chris and Neervannan, Raj and Neubig, Graham},
  year = {2021},
  month = mar,
  journal = {Transactions of the Association for Computational Linguistics},
  volume = {9},
  pages = {211--225},
  issn = {2307-387X},
  doi = {10.1162/tacl_a_00362},
  abstract = {Abstract             Aspect-based summarization is the task of generating focused summaries based on specific points of interest. Such summaries aid efficient analysis of text, such as quickly understanding reviews or opinions from different angles. However, due to large differences in the type of aspects for different domains (e.g., sentiment, product features), the development of previous models has tended to be domain-specific. In this paper, we propose WikiAsp,1 a large-scale dataset for multi-domain aspect- based summarization that attempts to spur research in the direction of open-domain aspect-based summarization. Specifically, we build the dataset using Wikipedia articles from 20 different domains, using the section titles and boundaries of each article as a proxy for aspect annotation. We propose several straightforward baseline models for this task and conduct experiments on the dataset. Results highlight key challenges that existing summarization models face in this setting, such as proper pronoun handling of quoted sources and consistent explanation of time-sensitive events.},
  langid = {english},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\storage\\WikiAsp _Hayashi et al 2021.pdf}
}

@inproceedings{he2021deberta,
  title = {{{DEBERTA}}: {{DECODING-ENHANCED BERT WITH DISENTANGLED ATTENTION}}},
  booktitle = {International Conference on Learning Representations},
  author = {He, Pengcheng and Liu, Xiaodong and Gao, Jianfeng and Chen, Weizhu},
  year = {2021},
  keywords = {⛔ No DOI found}
}

@inproceedings{he2022causpref,
  title = {{{CausPref}}: {{Causal Preference Learning}} for {{Out-of-Distribution Recommendation}}},
  shorttitle = {{{CausPref}}},
  booktitle = {Proceedings of the {{ACM Web Conference}} 2022},
  author = {He, Yue and Wang, Zimu and Cui, Peng and Zou, Hao and Zhang, Yafeng and Cui, Qiang and Jiang, Yong},
  year = {2022},
  month = apr,
  pages = {410--421},
  publisher = {{ACM}},
  address = {{Virtual Event, Lyon France}},
  doi = {10.1145/3485447.3511969},
  isbn = {978-1-4503-9096-5},
  langid = {english},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\storage\\CausPref _He et al 2022.pdf}
}

@inproceedings{hendricks2018grounding,
  title = {Grounding {{Visual Explanations}}},
  booktitle = {Proceedings of the {{European Conference}} on {{Computer Vision}} ({{ECCV}})},
  author = {Hendricks, Lisa Anne and Hu, Ronghang and Darrell, Trevor and Akata, Zeynep},
  year = {2018},
  month = sep
}

@inproceedings{hendricks2018women,
  title = {Women Also Snowboard: {{Overcoming}} Bias in Captioning Models},
  booktitle = {Proceedings of the European Conference on Computer Vision ({{ECCV}})},
  author = {Hendricks, Lisa Anne and Burns, Kaylee and Saenko, Kate and Darrell, Trevor and Rohrbach, Anna},
  year = {2018},
  month = sep,
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\storage\\Women also snowboard _Hendricks et al 2018.pdf}
}

@inproceedings{hendrickx2010semeval,
  title = {{{SemEval-2010}} Task 8: {{Multi-way}} Classification of Semantic Relations between Pairs of Nominals},
  booktitle = {Proceedings of the 5th International Workshop on Semantic Evaluation},
  author = {Hendrickx, Iris and Kim, Su Nam and Kozareva, Zornitsa and Nakov, Preslav and S{\'e}aghdha, Diarmuid {\'O} and Pad{\'o}, Sebastian and Pennacchiotti, Marco and Romano, Lorenza and Szpakowicz, Stan},
  year = {2010},
  pages = {33--38},
  keywords = {⛔ No DOI found},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\storage\\SemEval-2010 task 8 _Hendrickx et al 2010.pdf}
}

@article{higgins2018definition,
  title = {Towards a {{Definition}} of {{Disentangled Representations}}},
  author = {Higgins, Irina and Amos, David and Pfau, David and Racaniere, Sebastien and Matthey, Loic and Rezende, Danilo and Lerchner, Alexander},
  year = {2018},
  month = dec,
  journal = {arXiv:1812.02230 [cs, stat]},
  eprint = {1812.02230},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {How can intelligent agents solve a diverse set of tasks in a data-efficient manner? The disentangled representation learning approach posits that such an agent would benefit from separating out (disentangling) the underlying structure of the world into disjoint parts of its representation. However, there is no generally agreed-upon definition of disentangling, not least because it is unclear how to formalise the notion of world structure beyond toy datasets with a known ground truth generative process. Here we propose that a principled solution to characterising disentangled representations can be found by focusing on the transformation properties of the world. In particular, we suggest that those transformations that change only some properties of the underlying world state, while leaving all other properties invariant, are what gives exploitable structure to any kind of data. Similar ideas have already been successfully applied in physics, where the study of symmetry transformations has revolutionised the understanding of the world structure. By connecting symmetry transformations to vector representations using the formalism of group and representation theory we arrive at the first formal definition of disentangled representations. Our new definition is in agreement with many of the current intuitions about disentangling, while also providing principled resolutions to a number of previous points of contention. While this work focuses on formally defining disentangling - as opposed to solving the learning problem - we believe that the shift in perspective to studying data transformations can stimulate the development of better representation learning algorithms.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {⛔ No DOI found,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C\:\\Users\\Hytn\\Zotero\\storage\\86LQN3CK\\Higgins et al. - 2018 - Towards a Definition of Disentangled Representatio.pdf}
}

@article{hochreiter1997long,
  title = {Long Short-Term Memory},
  author = {Hochreiter, Sepp and Schmidhuber, J{\"u}rgen},
  year = {1997},
  journal = {Neural computation},
  volume = {9},
  number = {8},
  pages = {1735--1780},
  publisher = {{MIT Press}},
  doi = {10.1162/neco.1997.9.8.1735},
  file = {C\:\\Users\\Hytn\\Zotero\\storage\\E8WTQ5KH\\Hochreiter_Schmidhuber_1997_Long short-term memory.pdf}
}

@inproceedings{huang2022does,
  title = {Does {{Recommend-Revise Produce Reliable Annotations}}? {{An Analysis}} on {{Missing Instances}} in {{DocRED}}},
  shorttitle = {Does {{Recommend-Revise Produce Reliable Annotations}}?},
  booktitle = {Proceedings of the 60th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}} ({{Volume}} 1: {{Long Papers}})},
  author = {Huang, Quzhe and Hao, Shibo and Ye, Yuan and Zhu, Shengqi and Feng, Yansong and Zhao, Dongyan},
  year = {2022},
  pages = {6241--6252},
  publisher = {{Association for Computational Linguistics}},
  address = {{Dublin, Ireland}},
  doi = {10.18653/v1/2022.acl-long.432},
  langid = {english},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\storage\\Does Recommend-Revise Produce Reliable Annotations _Huang et al 2022.pdf}
}

@inproceedings{huang2022open,
  title = {Open {{Relation Modeling}}: {{Learning}} to {{Define Relations}} between {{Entities}}},
  shorttitle = {Open {{Relation Modeling}}},
  booktitle = {Findings of the {{Association}} for {{Computational Linguistics}}: {{ACL}} 2022},
  author = {Huang, Jie and Chang, Kevin and Xiong, Jinjun and Hwu, Wen-mei},
  year = {2022},
  pages = {297--308},
  publisher = {{Association for Computational Linguistics}},
  address = {{Dublin, Ireland}},
  doi = {10.18653/v1/2022.findings-acl.26},
  langid = {english},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\storage\\Open Relation Modeling _Huang et al 22.pdf}
}

@inproceedings{huang2022opena,
  title = {Open {{Relation Modeling}}: {{Learning}} to {{Define Relations}} between {{Entities}}},
  shorttitle = {Open {{Relation Modeling}}},
  booktitle = {Findings of the {{Association}} for {{Computational Linguistics}}: {{ACL}} 2022},
  author = {Huang, Jie and Chang, Kevin and Xiong, Jinjun and Hwu, Wen-mei},
  year = {2022},
  pages = {297--308},
  publisher = {{Association for Computational Linguistics}},
  address = {{Dublin, Ireland}},
  doi = {10.18653/v1/2022.findings-acl.26},
  langid = {english},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\storage\\Open Relation Modeling _Huang et al 2022.pdf}
}

@inproceedings{huguetcabot2021rebel,
  title = {{{REBEL}}: {{Relation Extraction By End-to-end Language}} Generation},
  shorttitle = {{{REBEL}}},
  booktitle = {Findings of the {{Association}} for {{Computational Linguistics}}: {{EMNLP}} 2021},
  author = {Huguet Cabot, Pere-Llu{\'i}s and Navigli, Roberto},
  year = {2021},
  pages = {2370--2381},
  publisher = {{Association for Computational Linguistics}},
  address = {{Punta Cana, Dominican Republic}},
  doi = {10.18653/v1/2021.findings-emnlp.204},
  langid = {english},
  file = {C\:\\Users\\Hytn\\Zotero\\storage\\HPMKZDPB\\Huguet Cabot and Navigli - 2021 - REBEL Relation Extraction By End-to-end Language .pdf}
}

@inproceedings{ian2014generative,
  title = {Generative Adversarial Nets},
  booktitle = {Advances in Neural Information Processing Systems},
  author = {Goodfellow, Ian and {Pouget-Abadie}, Jean and Mirza, Mehdi and Xu, Bing and {Warde-Farley}, David and Ozair, Sherjil and Courville, Aaron and Bengio, Yoshua},
  editor = {Ghahramani, Z. and Welling, M. and Cortes, C. and Lawrence, N. and Weinberger, K.Q.},
  year = {2014},
  volume = {27},
  publisher = {{Curran Associates, Inc.}},
  keywords = {⛔ No DOI found},
  file = {C\:\\Users\\Hytn\\Zotero\\storage\\VQFX6T98\\Generative adversarial nets_Goodfellow et al 2014.pdf}
}

@article{jaccard1912distribution,
  title = {{{THE DISTRIBUTION OF THE FLORA IN THE ALPINE ZONE}}.1},
  author = {Jaccard, Paul},
  year = {1912},
  month = feb,
  journal = {New Phytologist},
  volume = {11},
  number = {2},
  pages = {37--50},
  issn = {0028-646X, 1469-8137},
  doi = {10.1111/j.1469-8137.1912.tb05611.x},
  langid = {english},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\storage\\THE DISTRIBUTION OF THE FLORA IN THE ALPINE ZONE _Jaccard 1912.pdf}
}

@inproceedings{jacovi2021contrastive,
  title = {Contrastive {{Explanations}} for {{Model Interpretability}}},
  booktitle = {Proceedings of the 2021 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}}},
  author = {Jacovi, Alon and Swayamdipta, Swabha and Ravfogel, Shauli and Elazar, Yanai and Choi, Yejin and Goldberg, Yoav},
  year = {2021},
  pages = {1597--1611},
  publisher = {{Association for Computational Linguistics}},
  address = {{Online and Punta Cana, Dominican Republic}},
  doi = {10.18653/v1/2021.emnlp-main.120},
  langid = {english},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\storage\\Contrastive Explanations for Model Interpretability _Jacovi et al 2021.pdf}
}

@inproceedings{jeon2022entitybased,
  title = {Entity-Based {{Neural Local Coherence Modeling}}},
  booktitle = {Proceedings of the 60th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}} ({{Volume}} 1: {{Long Papers}})},
  author = {Jeon, Sungho and Strube, Michael},
  year = {2022},
  pages = {7787--7805},
  publisher = {{Association for Computational Linguistics}},
  address = {{Dublin, Ireland}},
  doi = {10.18653/v1/2022.acl-long.537},
  langid = {english},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\storage\\Entity-based Neural Local Coherence Modeling _Jeon_Strube 2022.pdf}
}

@inproceedings{jia2017adversariala,
  title = {Adversarial {{Examples}} for {{Evaluating Reading Comprehension Systems}}},
  booktitle = {Proceedings of the 2017 {{Conference}} on {{Empirical Methods}} in {{Natural}}           {{Language Processing}}},
  author = {Jia, Robin and Liang, Percy},
  year = {2017},
  pages = {2021--2031},
  publisher = {{Association for Computational Linguistics}},
  address = {{Copenhagen, Denmark}},
  doi = {10.18653/v1/D17-1215},
  langid = {english},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\storage\\Adversarial Examples for Evaluating Reading Comprehension Systems _Jia_Liang 2017.pdf}
}

@inproceedings{jia2019arnor,
  title = {{{ARNOR}}: {{Attention}} Regularization Based Noise Reduction for Distant Supervision Relation Classification},
  booktitle = {Proceedings of the 57th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}}},
  author = {Jia, Wei and Dai, Dai and Xiao, Xinyan and Wu, Hua},
  year = {2019},
  pages = {1399--1408},
  doi = {10.18653/v1/P19-1135},
  file = {C\:\\Users\\Hytn\\Zotero\\storage\\F76GSEIX\\Jia et al_2019_ARNOR.pdf}
}

@inproceedings{jia2019documentlevel,
  title = {Document-{{Level N-ary Relation Extraction}} with {{Multiscale Representation Learning}}.},
  booktitle = {Proceedings of the 2019 {{Conference}} of the {{North American Chapter}} of the {{Association}} for {{Computational Linguistics}}: {{Human Language Technologies}}, {{Volume}} 1 ({{Long}} and {{Short Papers}})},
  author = {Jia, Robin and Wong, Cliff and Poon, Hoifung},
  year = {2019},
  pages = {3693--3704},
  keywords = {⛔ No DOI found}
}

@inproceedings{jiang-etal-2018-interpretable,
  title = {Interpretable Rationale Augmented Charge Prediction System},
  booktitle = {Proceedings of the 27th International Conference on Computational Linguistics: {{System}} Demonstrations},
  author = {Jiang, Xin and Ye, Hai and Luo, Zhunchen and Chao, WenHan and Ma, Wenjia},
  year = {2018},
  month = aug,
  pages = {146--151},
  publisher = {{Association for Computational Linguistics}},
  address = {{Santa Fe, New Mexico}},
  abstract = {This paper proposes a neural based system to solve the essential interpretability problem existing in text classification, especially in charge prediction task. First, we use a deep reinforcement learning method to extract rationales which mean short, readable and decisive snippets from input text. Then a rationale augmented classification model is proposed to elevate the prediction accuracy. Naturally, the extracted rationales serve as the introspection explanation for the prediction result of the model, enhancing the transparency of the model. Experimental results demonstrate that our system is able to extract readable rationales in a high consistency with manual annotation and is comparable with the attention model in prediction accuracy.}
}

@article{jin2020bert,
  title = {Is {{BERT Really Robust}}? {{A Strong Baseline}} for {{Natural Language Attack}} on {{Text Classification}} and {{Entailment}}},
  shorttitle = {Is {{BERT Really Robust}}?},
  author = {Jin, Di and Jin, Zhijing and Zhou, Joey Tianyi and Szolovits, Peter},
  year = {2020},
  month = apr,
  journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
  volume = {34},
  number = {05},
  pages = {8018--8025},
  issn = {2374-3468, 2159-5399},
  doi = {10.1609/aaai.v34i05.6311},
  abstract = {Machine learning algorithms are often vulnerable to adversarial examples that have imperceptible alterations from the original counterparts but can fool the state-of-the-art models. It is helpful to evaluate or even improve the robustness of these models by exposing the maliciously crafted adversarial examples. In this paper, we present TextFooler, a simple but strong baseline to generate adversarial text. By applying it to two fundamental natural language tasks, text classification and textual entailment, we successfully attacked three target models, including the powerful pre-trained BERT, and the widely used convolutional and recurrent neural networks. We demonstrate three advantages of this framework: (1) effective\textemdash it outperforms previous attacks by success rate and perturbation rate, (2) utility-preserving\textemdash it preserves semantic content, grammaticality, and correct types classified by humans, and (3) efficient\textemdash it generates adversarial text with computational complexity linear to the text length.1},
  keywords = {data augmentation},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\storage\\Is BERT Really Robust _Jin et al 2020.pdf}
}

@inproceedings{jin2020multigranularity,
  title = {Multi-{{Granularity Interaction Network}} for {{Extractive}} and {{Abstractive Multi-Document Summarization}}},
  booktitle = {Proceedings of the 58th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}}},
  author = {Jin, Hanqi and Wang, Tianming and Wan, Xiaojun},
  year = {2020},
  pages = {6244--6254},
  publisher = {{Association for Computational Linguistics}},
  address = {{Online}},
  doi = {10.18653/v1/2020.acl-main.556},
  langid = {english},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\storage\\Multi-Granularity Interaction Network for Extractive and Abstractive _Jin et al 2020.pdf}
}

@inproceedings{ju2022logic,
  title = {Logic {{Traps}} in {{Evaluating Attribution Scores}}},
  booktitle = {Proceedings of the 60th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}} ({{Volume}} 1: {{Long Papers}})},
  author = {Ju, Yiming and Zhang, Yuanzhe and Yang, Zhao and Jiang, Zhongtao and Liu, Kang and Zhao, Jun},
  year = {2022},
  pages = {5911--5922},
  publisher = {{Association for Computational Linguistics}},
  address = {{Dublin, Ireland}},
  doi = {10.18653/v1/2022.acl-long.407},
  langid = {english},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\storage\\Logic Traps in Evaluating Attribution Scores _Ju et al 2022.pdf}
}

@inproceedings{kamaleddine2022frugalscorea,
  title = {{{FrugalScore}}: {{Learning Cheaper}}, {{Lighter}} and {{Faster Evaluation Metrics}} for {{Automatic Text Generation}}},
  shorttitle = {{{FrugalScore}}},
  booktitle = {Proceedings of the 60th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}} ({{Volume}} 1: {{Long Papers}})},
  author = {Kamal Eddine, Moussa and Shang, Guokan and Tixier, Antoine and Vazirgiannis, Michalis},
  year = {2022},
  pages = {1305--1318},
  publisher = {{Association for Computational Linguistics}},
  address = {{Dublin, Ireland}},
  doi = {10.18653/v1/2022.acl-long.93},
  langid = {english},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\storage\\FrugalScore _Kamal Eddine et al 22.pdf}
}

@inproceedings{karras2018progressive,
  title = {Progressive Growing of {{GANs}} for Improved Quality, Stability, and Variation},
  booktitle = {International Conference on Learning Representations},
  author = {Karras, Tero and Aila, Timo and Laine, Samuli and Lehtinen, Jaakko},
  year = {2018},
  keywords = {⛔ No DOI found},
  file = {C\:\\Users\\Hytn\\Zotero\\storage\\MUFK9P78\\Progressive growing of GANs for improved quality, stability, and variation_Karras et al 2018.pdf}
}

@inproceedings{katiyar2017going,
  title = {Going out on a Limb: {{Joint}} Extraction of Entity Mentions and Relations without Dependency Trees},
  booktitle = {Proceedings of the 55th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}} ({{Volume}} 1: {{Long Papers}})},
  author = {Katiyar, Arzoo and Cardie, Claire},
  year = {2017},
  pages = {917--928},
  doi = {10.18653/v1/P17-1085},
  file = {C\:\\Users\\Hytn\\Zotero\\storage\\B8YR9E2Z\\Katiyar_Cardie_2017_Going out on a limb.pdf}
}

@article{katz2012quantitative,
  title = {Quantitative Legal Prediction-or-How i Learned to Stop Worrying and Start Preparing for the Data-Driven Future of the Legal Services Industry},
  author = {Katz, Daniel Martin},
  year = {2012},
  journal = {Emory LJ},
  volume = {62},
  pages = {909},
  publisher = {{HeinOnline}},
  keywords = {⛔ No DOI found}
}

@inproceedings{kavumba2019when,
  title = {When {{Choosing Plausible Alternatives}}, {{Clever Hans}} Can Be {{Clever}}},
  booktitle = {Proceedings of the {{First Workshop}} on {{Commonsense Inference}} in {{Natural Language Processing}}},
  author = {Kavumba, Pride and Inoue, Naoya and Heinzerling, Benjamin and Singh, Keshav and Reisert, Paul and Inui, Kentaro},
  year = {2019},
  pages = {33--42},
  publisher = {{Association for Computational Linguistics}},
  address = {{Hong Kong, China}},
  doi = {10.18653/v1/D19-6004},
  langid = {english},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\storage\\When Choosing Plausible Alternatives, Clever Hans can be Clever _Kavumba et al 2019.pdf}
}

@article{keown1980mathematical,
  title = {Mathematical Models for Legal Prediction},
  author = {Keown, R},
  year = {1980},
  journal = {Computer/lj},
  volume = {2},
  pages = {829},
  publisher = {{HeinOnline}},
  keywords = {⛔ No DOI found}
}

@inproceedings{khani2021removing,
  title = {Removing {{Spurious Features}} Can {{Hurt Accuracy}} and {{Affect Groups Disproportionately}}},
  booktitle = {Proceedings of the 2021 {{ACM Conference}} on {{Fairness}}, {{Accountability}}, and {{Transparency}}},
  author = {Khani, Fereshte and Liang, Percy},
  year = {2021},
  month = mar,
  pages = {196--205},
  publisher = {{ACM}},
  address = {{Virtual Event Canada}},
  doi = {10.1145/3442188.3445883},
  isbn = {978-1-4503-8309-7},
  langid = {english},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\storage\\Removing Spurious Features can Hurt Accuracy and Affect Groups _Khani_Liang 2021.pdf}
}

@inproceedings{kim2017structured,
  title = {Structured {{Attention Networks}}},
  booktitle = {{{ICLR}} ({{Poster}})},
  author = {Kim, Yoon and Denton, Carl and Hoang, Luong and Rush, Alexander M.},
  year = {2017},
  keywords = {⛔ No DOI found}
}

@inproceedings{kindermans2018learning,
  title = {Learning How to Explain Neural Networks: {{Patternnet}} and {{Patternattribution}}},
  booktitle = {6th {{International Conference}} on {{Learning Representations}}},
  author = {Kindermans, Pieter Jan and Sch{\"u}tt, Kristof T. and Alber, Maximilian and M{\"u}ller, Klaus Robert and Erhan, Dumitru and Kim, Been and D{\"a}hne, Sven},
  year = {2018},
  month = jan,
  abstract = {DeConvNet, Guided BackProp, LRP, were invented to better understand deep neural networks. We show that these methods do not produce the theoretically correct explanation for a linear model. Yet they are used on multi-layer networks with millions of parameters. This is a cause for concern since linear models are simple neural networks. We argue that explanation methods for neural nets should work reliably in the limit of simplicity, the linear models. Based on our analysis of linear models we propose a generalization that yields two explanation techniques (PatternNet and PatternAttribution) that are theoretically sound for linear models and produce improved explanations for deep networks.},
  langid = {english},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\storage\\Learning how to explain neural networks _Kindermans et al 2018.pdf}
}

@article{kingma2014adam,
  title = {Adam: {{A}} Method for Stochastic Optimization},
  author = {Kingma, Diederik P and Ba, Jimmy},
  year = {2014},
  journal = {arXiv preprint arXiv:1412.6980},
  eprint = {1412.6980},
  eprinttype = {arxiv},
  archiveprefix = {arXiv},
  keywords = {⛔ No DOI found}
}

@article{kipf2016semisupervised,
  title = {Semi-Supervised Classification with Graph Convolutional Networks},
  author = {Kipf, Thomas N and Welling, Max},
  year = {2016},
  journal = {arXiv preprint arXiv:1609.02907},
  eprint = {1609.02907},
  eprinttype = {arxiv},
  archiveprefix = {arXiv},
  keywords = {⛔ No DOI found}
}

@inproceedings{koo2007structured,
  title = {Structured Prediction Models via the Matrix-Tree Theorem},
  booktitle = {Proceedings of the 2007 {{Joint Conference}} on {{Empirical Methods}} in {{Natural Language Processing}} and {{Computational Natural Language Learning}} ({{EMNLP-CoNLL}})},
  author = {Koo, Terry and Globerson, Amir and Carreras, Xavier and Collins, Michael},
  year = {2007},
  pages = {141--150},
  keywords = {⛔ No DOI found}
}

@inproceedings{kuang2018stable,
  title = {Stable {{Prediction}} across {{Unknown Environments}}},
  booktitle = {Proceedings of the 24th {{ACM SIGKDD International Conference}} on {{Knowledge Discovery}} \& {{Data Mining}}},
  author = {Kuang, Kun and Cui, Peng and Athey, Susan and Xiong, Ruoxuan and Li, Bo},
  year = {2018},
  month = jul,
  pages = {1617--1626},
  publisher = {{ACM}},
  doi = {10.1145/3219819.3220082},
  isbn = {978-1-4503-5552-0},
  file = {C\:\\Users\\Hytn\\Zotero\\storage\\35EZMCVL\\Kuang et al_2018_Stable Prediction across Unknown Environments.pdf}
}

@article{lawlor1963computers,
  title = {What Computers Can Do: {{Analysis}} and Prediction of Judicial Decisions},
  author = {Lawlor, Reed C},
  year = {1963},
  journal = {American Bar Association Journal},
  pages = {337--344},
  publisher = {{JSTOR}},
  keywords = {⛔ No DOI found}
}

@article{lecun2015deep,
  title = {Deep Learning},
  author = {LeCun, Yann and Bengio, Yoshua and Hinton, Geoffrey},
  year = {2015},
  journal = {nature},
  volume = {521},
  number = {7553},
  pages = {436--444},
  publisher = {{Nature Publishing Group}},
  doi = {10.1038/nature14539},
  file = {C\:\\Users\\Hytn\\Zotero\\storage\\EX3T2CYN\\LeCun et al_2015_Deep learning.pdf}
}

@inproceedings{levy2017zeroshot,
  title = {Zero-{{Shot Relation Extraction}} via {{Reading Comprehension}}},
  booktitle = {Proceedings of the 21st {{Conference}} on {{Computational Natural Language}}           {{Learning}} ({{CoNLL}} 2017)},
  author = {Levy, Omer and Seo, Minjoon and Choi, Eunsol and Zettlemoyer, Luke},
  year = {2017},
  pages = {333--342},
  publisher = {{Association for Computational Linguistics}},
  address = {{Vancouver, Canada}},
  doi = {10.18653/v1/K17-1034},
  langid = {english},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\storage\\Zero-Shot Relation Extraction via Reading Comprehension _Levy et al 2017.pdf}
}

@inproceedings{lewis2020bart,
  title = {{{BART}}: {{Denoising}} Sequence-to-Sequence Pre-Training for Natural Language Generation, Translation, and Comprehension},
  booktitle = {Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics},
  author = {Lewis, Mike and Liu, Yinhan and Goyal, Naman and Ghazvininejad, Marjan and Mohamed, Abdelrahman and Levy, Omer and Stoyanov, Veselin and Zettlemoyer, Luke},
  year = {2020},
  pages = {7871--7880},
  doi = {10.18653/v1/2020.acl-main.703},
  file = {C\:\\Users\\Hytn\\Zotero\\storage\\WZBJVG8K\\BART_Lewis et al 2020.pdf}
}

@inproceedings{li2014incremental,
  title = {Incremental Joint Extraction of Entity Mentions and Relations},
  booktitle = {Proceedings of the 52nd {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}} ({{Volume}} 1: {{Long Papers}})},
  author = {Li, Qi and Ji, Heng},
  year = {2014},
  pages = {402--412},
  doi = {10.3115/v1/P14-1038},
  file = {C\:\\Users\\Hytn\\Zotero\\storage\\Y346AF7J\\Li_Ji_2014_Incremental joint extraction of entity mentions and relations.pdf}
}

@article{li2016biocreative,
  title = {{{BioCreative V CDR}} Task Corpus: A Resource for Chemical Disease Relation Extraction},
  author = {Li, Jiao and Sun, Yueping and Johnson, Robin J. and Sciaky, Daniela and Wei, Chih-Hsuan and Leaman, Robert and Davis, Allan Peter and Mattingly, Carolyn J. and Wiegers, Thomas C. and Lu, Zhiyong},
  year = {2016},
  journal = {Database},
  volume = {2016},
  publisher = {{Oxford Academic}},
  keywords = {❓ Multiple DOI}
}

@inproceedings{li2016visualizing,
  title = {Visualizing and Understanding Neural Models in {{NLP}}},
  booktitle = {Proceedings of {{NAACL-HLT}}},
  author = {Li, Jiwei and Chen, Xinlei and Hovy, Eduard and Jurafsky, Dan},
  year = {2016},
  pages = {681--691},
  keywords = {⛔ No DOI found}
}

@inproceedings{li2016visualizinga,
  title = {Visualizing and {{Understanding Neural Models}} in {{NLP}}},
  booktitle = {Proceedings of the 2016 {{Conference}} of the {{North American Chapter}} of the {{Association}} for {{Computational Linguistics}}: {{Human Language Technologies}}},
  author = {Li, Jiwei and Chen, Xinlei and Hovy, Eduard and Jurafsky, Dan},
  year = {2016},
  pages = {681--691},
  publisher = {{Association for Computational Linguistics}},
  address = {{San Diego, California}},
  doi = {10.18653/v1/N16-1082},
  abstract = {While neural networks have been successfully applied to many NLP tasks the resulting vectorbased models are very difficult to interpret. For example it's not clear how they achieve compositionality, building sentence meaning from the meanings of words and phrases. In this paper we describe strategies for visualizing compositionality in neural models for NLP, inspired by similar work in computer vision. We first plot unit values to visualize compositionality of negation, intensification, and concessive clauses, allowing us to see wellknown markedness asymmetries in negation. We then introduce methods for visualizing a unit's salience, the amount that it contributes to the final composed meaning from first-order derivatives. Our general-purpose methods may have wide applications for understanding compositionality and other semantic properties of deep networks.},
  langid = {english},
  file = {C\:\\Users\\Hytn\\Zotero\\storage\\7BL9UQ5F\\Li et al. - 2016 - Visualizing and Understanding Neural Models in NLP.pdf}
}

@misc{li2017understanding,
  title = {Understanding {{Neural Networks}} through {{Representation Erasure}}},
  author = {Li, Jiwei and Monroe, Will and Jurafsky, Dan},
  year = {2017},
  month = jan,
  number = {arXiv:1612.08220},
  eprint = {1612.08220},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  abstract = {While neural networks have been successfully applied to many natural language processing tasks, they come at the cost of interpretability. In this paper, we propose a general methodology to analyze and interpret decisions from a neural model by observing the effects on the model of erasing various parts of the representation, such as input word-vector dimensions, intermediate hidden units, or input words. We present several approaches to analyzing the effects of such erasure, from computing its impact on evaluation metrics, to using reinforcement learning to erase the minimum set of input words in order to flip a neural model's decision. In a comprehensive analysis of multiple NLP tasks from lexical (word shape, morphology) to sentence-level (sentiment) to document level (sentiment aspect), we show that the proposed methodology not only offers clear explanations about neural model decisions, but also provides a way to conduct error analysis on neural models.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computation and Language},
  file = {C\:\\Users\\Hytn\\Zotero\\storage\\IXG9D6U6\\Li et al. - 2017 - Understanding Neural Networks through Representati.pdf}
}

@inproceedings{li2019entityrelation,
  title = {Entity-{{Relation Extraction}} as {{Multi-Turn Question Answering}}},
  booktitle = {Proceedings of the 57th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}}},
  author = {Li, Xiaoya and Yin, Fan and Sun, Zijun and Li, Xiayu and Yuan, Arianna and Chai, Duo and Zhou, Mingxin and Li, Jiwei},
  year = {2019},
  pages = {1340--1350},
  doi = {10.18653/v1/P19-1129},
  file = {C\:\\Users\\Hytn\\Zotero\\storage\\56KEY6UR\\Li et al_2019_Entity-Relation Extraction as Multi-Turn Question Answering.pdf}
}

@inproceedings{li2020bertattack,
  title = {{{BERT-ATTACK}}: {{Adversarial Attack Against BERT Using BERT}}},
  shorttitle = {{{BERT-ATTACK}}},
  booktitle = {Proceedings of the 2020 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}} ({{EMNLP}})},
  author = {Li, Linyang and Ma, Ruotian and Guo, Qipeng and Xue, Xiangyang and Qiu, Xipeng},
  year = {2020},
  pages = {6193--6202},
  publisher = {{Association for Computational Linguistics}},
  address = {{Online}},
  doi = {10.18653/v1/2020.emnlp-main.500},
  langid = {english},
  file = {C\:\\Users\\Hytn\\Zotero\\storage\\XLZRAHFL\\BERT-ATTACK_Li et al 2020.pdf}
}

@inproceedings{li2021robustness,
  title = {On {{Robustness}} and {{Bias Analysis}} of {{BERT-Based Relation Extraction}}},
  booktitle = {Communications in {{Computer}} and {{Information Science}}},
  author = {Li, Luoqiu and Chen, Xiang and Ye, Hongbin and Bi, Zhen and Deng, Shumin and Zhang, Ningyu and Chen, Huajun},
  year = {2021},
  volume = {1466 CCIS},
  pages = {43--59},
  publisher = {{Springer Science and Business Media Deutschland GmbH}},
  issn = {18650937},
  doi = {10.1007/978-981-16-6471-7_4},
  abstract = {Fine-tuning pre-trained models have achieved impressive performance on standard natural language processing benchmarks. However, the resultant model generalizability remains poorly understood. We do not know, for example, how excellent performance can lead to the perfection of generalization models. In this study, we analyze a fine-tuned BERT model from different perspectives using relation extraction. We also characterize the differences in generalization techniques according to our proposed improvements. From empirical experimentation, we find that BERT suffers a bottleneck in terms of robustness by way of randomizations, adversarial and counterfactual tests, and biases (i.e., selection and semantic). These findings highlight opportunities for future improvements. Our open-sourced testbed DiagnoseRE is available in https://github.com/zjunlp/DiagnoseRE.},
  isbn = {9789811664700},
  file = {C\:\\Users\\Hytn\\Zotero\\storage\\6RU8KLTF\\Li et al_2021_On Robustness and Bias Analysis of BERT-Based Relation Extraction.pdf}
}

@inproceedings{li2021tdeer,
  title = {{{TDEER}}: {{An Efficient Translating Decoding Schema}} for {{Joint Extraction}} of {{Entities}} and {{Relations}}},
  shorttitle = {{{TDEER}}},
  booktitle = {Proceedings of the 2021 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}}},
  author = {Li, Xianming and Luo, Xiaotian and Dong, Chenghao and Yang, Daichuan and Luan, Beidi and He, Zhen},
  year = {2021},
  pages = {8055--8064},
  publisher = {{Association for Computational Linguistics}},
  address = {{Online and Punta Cana, Dominican Republic}},
  doi = {10.18653/v1/2021.emnlp-main.635},
  langid = {english},
  file = {C\:\\Users\\Hytn\\Zotero\\storage\\XTCSY93N\\Li et al. - 2021 - TDEER An Efficient Translating Decoding Schema fo.pdf}
}

@misc{li2022diffusionlm,
  title = {Diffusion-{{LM Improves Controllable Text Generation}}},
  author = {Li, Xiang Lisa and Thickstun, John and Gulrajani, Ishaan and Liang, Percy and Hashimoto, Tatsunori B.},
  year = {2022},
  month = may,
  number = {arXiv:2205.14217},
  eprint = {2205.14217},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  abstract = {Controlling the behavior of language models (LMs) without re-training is a major open problem in natural language generation. While recent works have demonstrated successes on controlling simple sentence attributes (e.g., sentiment), there has been little progress on complex, fine-grained controls (e.g., syntactic structure). To address this challenge, we develop a new non-autoregressive language model based on continuous diffusions that we call Diffusion-LM. Building upon the recent successes of diffusion models in continuous domains, Diffusion-LM iteratively denoises a sequence of Gaussian vectors into word vectors, yielding a sequence of intermediate latent variables. The continuous, hierarchical nature of these intermediate variables enables a simple gradient-based algorithm to perform complex, controllable generation tasks. We demonstrate successful control of Diffusion-LM for six challenging fine-grained control tasks, significantly outperforming prior work.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {C\:\\Users\\Hytn\\Zotero\\storage\\X7BXSJ7Y\\Li et al. - 2022 - Diffusion-LM Improves Controllable Text Generation.pdf}
}

@inproceedings{li2022hiclre,
  title = {{{HiCLRE}}: {{A Hierarchical Contrastive Learning Framework}} for {{Distantly Supervised Relation Extraction}}},
  shorttitle = {{{HiCLRE}}},
  booktitle = {Findings of the {{Association}} for {{Computational Linguistics}}: {{ACL}} 2022},
  author = {Li, Dongyang and Zhang, Taolin and Hu, Nan and Wang, Chengyu and He, Xiaofeng},
  year = {2022},
  pages = {2567--2578},
  publisher = {{Association for Computational Linguistics}},
  address = {{Dublin, Ireland}},
  doi = {10.18653/v1/2022.findings-acl.202},
  langid = {english},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\storage\\HiCLRE _Li et al 2022.pdf}
}

@inproceedings{li2022keywords,
  title = {Keywords and Instances: {{A}} Hierarchical Contrastive Learning Framework Unifying Hybrid Granularities for Text Generation},
  booktitle = {Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: {{Long}} Papers)},
  author = {Li, Mingzhe and Lin, XieXiong and Chen, Xiuying and Chang, Jinxiong and Zhang, Qishen and Wang, Feng and Wang, Taifeng and Liu, Zhongyi and Chu, Wei and Zhao, Dongyan and Yan, Rui},
  year = {2022},
  month = may,
  pages = {4432--4441},
  publisher = {{Association for Computational Linguistics}},
  address = {{Dublin, Ireland}},
  abstract = {Contrastive learning has achieved impressive success in generation tasks to militate the ``exposure bias'' problem and discriminatively exploit the different quality of references. Existing works mostly focus on contrastive learning on the instance-level without discriminating the contribution of each word, while keywords are the gist of the text and dominant the constrained mapping relationships. Hence, in this work, we propose a hierarchical contrastive learning mechanism, which can unify hybrid granularities semantic meaning in the input text. Concretely, we first propose a keyword graph via contrastive correlations of positive-negative pairs to iteratively polish the keyword representations. Then, we construct intra-contrasts within instance-level and keyword-level, where we assume words are sampled nodes from a sentence distribution. Finally, to bridge the gap between independent contrast levels and tackle the common contrast vanishing problem, we propose an inter-contrast mechanism that measures the discrepancy between contrastive keyword nodes respectively to the instance distribution. Experiments demonstrate that our model outperforms competitive baselines on paraphrasing, dialogue generation, and storytelling tasks.},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\storage\\Keywords and instances _Li et al 2022.pdf}
}

@misc{li2022sok,
  title = {{{SoK}}: {{Certified Robustness}} for {{Deep Neural Networks}}},
  shorttitle = {{{SoK}}},
  author = {Li, Linyi and Xie, Tao and Li, Bo},
  year = {2022},
  month = feb,
  number = {arXiv:2009.04131},
  eprint = {2009.04131},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  abstract = {Great advances in deep neural networks (DNNs) have led to state-of-the-art performance on a wide range of tasks. However, recent studies have shown that DNNs are vulnerable to adversarial attacks, which have brought great concerns when deploying these models to safety-critical applications such as autonomous driving. Different defense approaches have been proposed against adversarial attacks, including: a) empirical defenses, which usually can be adaptively attacked again without providing robustness certification; and b) certifiably robust approaches, which consist of robustness verification providing the lower bound of robust accuracy against any attacks under certain conditions and corresponding robust training approaches. In this paper, we systematize the certifiably robust approaches and related practical and theoretical implications and findings. We also provide the first comprehensive benchmark on existing robustness verification and training approaches on different datasets. In particular, we 1) provide a taxonomy for the robustness verification and training approaches, as well as summarize the methodologies for representative algorithms, 2) reveal the characteristics, strengths, limitations, and fundamental connections among these approaches, 3) discuss current research progresses, theoretical barriers, main challenges, and future directions for certifiably robust approaches for DNNs, and 4) provide an open-sourced unified platform to evaluate over 20 representative certifiably robust approaches for a wide range of DNNs.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Cryptography and Security,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C\:\\Users\\Hytn\\Zotero\\storage\\DTXAZSM6\\Li et al. - 2022 - SoK Certified Robustness for Deep Neural Networks.pdf}
}

@inproceedings{limsopatham2021effectively,
  title = {Effectively {{Leveraging BERT}} for {{Legal Document Classification}}},
  booktitle = {Proceedings of the {{Natural Legal Language Processing Workshop}} 2021},
  author = {Limsopatham, Nut},
  year = {2021},
  pages = {210--216},
  publisher = {{Association for Computational Linguistics}},
  address = {{Punta Cana, Dominican Republic}},
  doi = {10.18653/v1/2021.nllp-1.22},
  langid = {english},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\storage\\Effectively Leveraging BERT for Legal Document Classification _Limsopatham 2021.pdf}
}

@inproceedings{lin2016neural,
  title = {Neural Relation Extraction with Selective Attention over Instances},
  booktitle = {Proceedings of the 54th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}} ({{Volume}} 1: {{Long Papers}})},
  author = {Lin, Yankai and Shen, Shiqi and Liu, Zhiyuan and Luan, Huanbo and Sun, Maosong},
  year = {2016},
  pages = {2124--2133},
  doi = {10.18653/v1/P16-1200},
  file = {C\:\\Users\\Hytn\\Zotero\\storage\\UFH4L72L\\Lin et al_2016_Neural relation extraction with selective attention over instances.pdf}
}

@inproceedings{lin2020rigorous,
  title = {A {{Rigorous Study}} on {{Named Entity Recognition}}: {{Can Fine-tuning Pretrained Model Lead}} to the {{Promised Land}}?},
  shorttitle = {A {{Rigorous Study}} on {{Named Entity Recognition}}},
  booktitle = {Proceedings of the 2020 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}} ({{EMNLP}})},
  author = {Lin, Hongyu and Lu, Yaojie and Tang, Jialong and Han, Xianpei and Sun, Le and Wei, Zhicheng and Yuan, Nicholas Jing},
  year = {2020},
  pages = {7291--7300},
  publisher = {{Association for Computational Linguistics}},
  address = {{Online}},
  doi = {10.18653/v1/2020.emnlp-main.592},
  langid = {english},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\storage\\A Rigorous Study on Named Entity Recognition _Lin et al 2020.pdf}
}

@article{liu2018learning,
  title = {Learning Structured Text Representations},
  author = {Liu, Yang and Lapata, Mirella},
  year = {2018},
  journal = {Transactions of the Association for Computational Linguistics},
  volume = {6},
  pages = {63--75},
  publisher = {{MIT Press}},
  doi = {10.1162/tacl_a_00005},
  file = {C\:\\Users\\Hytn\\Zotero\\storage\\3PVNP755\\Liu_Lapata_2018_Learning structured text representations.pdf}
}

@inproceedings{liu2019incorporating,
  title = {Incorporating {{Priors}} with {{Feature Attribution}} on {{Text Classification}}},
  booktitle = {Proceedings of the 57th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}}},
  author = {Liu, Frederick and Avci, Besim},
  year = {2019},
  pages = {6274--6283},
  publisher = {{Association for Computational Linguistics}},
  address = {{Florence, Italy}},
  doi = {10.18653/v1/P19-1631},
  langid = {english},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\storage\\Incorporating Priors with Feature Attribution on Text Classification _Liu_Avci 2019.pdf}
}

@article{liu2019roberta,
  title = {Roberta: {{A}} Robustly Optimized Bert Pretraining Approach},
  author = {Liu, Yinhan and Ott, Myle and Goyal, Naman and Du, Jingfei and Joshi, Mandar and Chen, Danqi and Levy, Omer and Lewis, Mike and Zettlemoyer, Luke and Stoyanov, Veselin},
  year = {2019},
  journal = {arXiv preprint arXiv:1907.11692},
  eprint = {1907.11692},
  eprinttype = {arxiv},
  archiveprefix = {arXiv},
  keywords = {⛔ No DOI found}
}

@inproceedings{liu2019single,
  title = {Single Document Summarization as Tree Induction},
  booktitle = {Proceedings of the 2019 {{Conference}} of the {{North American Chapter}} of the {{Association}} for {{Computational Linguistics}}: {{Human Language Technologies}}, {{Volume}} 1 ({{Long}} and {{Short Papers}})},
  author = {Liu, Yang and Titov, Ivan and Lapata, Mirella},
  year = {2019},
  pages = {1745--1755},
  keywords = {⛔ No DOI found}
}

@inproceedings{liu2019text,
  title = {Text {{Summarization}} with {{Pretrained Encoders}}},
  booktitle = {Proceedings of the 2019 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}} and the 9th {{International Joint Conference}} on {{Natural Language Processing}} ({{EMNLP-IJCNLP}})},
  author = {Liu, Yang and Lapata, Mirella},
  year = {2019},
  pages = {3728--3738},
  publisher = {{Association for Computational Linguistics}},
  address = {{Hong Kong, China}},
  doi = {10.18653/v1/D19-1387},
  langid = {english},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\storage\\Text Summarization with Pretrained Encoders _Liu_Lapata 2019.pdf}
}

@inproceedings{liu2021element,
  title = {Element {{Intervention}} for {{Open Relation Extraction}}},
  booktitle = {Proceedings of the 59th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}} and the 11th {{International Joint Conference}} on {{Natural Language Processing}} ({{Volume}} 1: {{Long Papers}})},
  author = {Liu, Fangchao and Yan, Lingyong and Lin, Hongyu and Han, Xianpei and Sun, Le},
  year = {2021},
  pages = {4683--4693},
  publisher = {{Association for Computational Linguistics}},
  address = {{Online}},
  doi = {10.18653/v1/2021.acl-long.361},
  langid = {english},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\storage\\Element Intervention for Open Relation Extraction _Liu et al 2021.pdf}
}

@misc{liu2021everything,
  title = {Everything {{Has}} a {{Cause}}: {{Leveraging Causal Inference}} in {{Legal Text Analysis}}},
  shorttitle = {Everything {{Has}} a {{Cause}}},
  author = {Liu, Xiao and Yin, Da and Feng, Yansong and Wu, Yuting and Zhao, Dongyan},
  year = {2021},
  month = apr,
  number = {arXiv:2104.09420},
  eprint = {2104.09420},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  abstract = {Causal inference is the process of capturing cause-effect relationship among variables. Most existing works focus on dealing with structured data, while mining causal relationship among factors from unstructured data, like text, has been less examined, but is of great importance, especially in the legal domain. In this paper, we propose a novel Graph-based Causal Inference (GCI) framework, which builds causal graphs from fact descriptions without much human involvement and enables causal inference to facilitate legal practitioners to make proper decisions. We evaluate the framework on a challenging similar charge disambiguation task. Experimental results show that GCI can capture the nuance from fact descriptions among multiple confusing charges and provide explainable discrimination, especially in few-shot settings. We also observe that the causal knowledge contained in GCI can be effectively injected into powerful neural networks for better performance and interpretability. Code and data are available at https://github.com/xxxiaol/GCI/.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {C\:\\Users\\Hytn\\Zotero\\storage\\TSCHIXRX\\Liu et al. - 2021 - Everything Has a Cause Leveraging Causal Inferenc.pdf}
}

@inproceedings{liu2022floodingx,
  title = {Flooding-{{X}}: {{Improving BERT}}'s {{Resistance}} to {{Adversarial Attacks}} via {{Loss-Restricted Fine-Tuning}}},
  shorttitle = {Flooding-{{X}}},
  booktitle = {Proceedings of the 60th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}} ({{Volume}} 1: {{Long Papers}})},
  author = {Liu, Qin and Zheng, Rui and Rong, Bao and Liu, Jingyi and Liu, ZhiHua and Cheng, Zhanzhan and Qiao, Liang and Gui, Tao and Zhang, Qi and Huang, Xuanjing},
  year = {2022},
  pages = {5634--5644},
  publisher = {{Association for Computational Linguistics}},
  address = {{Dublin, Ireland}},
  doi = {10.18653/v1/2022.acl-long.386},
  langid = {english},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\storage\\Flooding-X _Liu et al 2022.pdf}
}

@misc{liu2022generated,
  title = {Generated {{Knowledge Prompting}} for {{Commonsense Reasoning}}},
  author = {Liu, Jiacheng and Liu, Alisa and Lu, Ximing and Welleck, Sean and West, Peter and Bras, Ronan Le and Choi, Yejin and Hajishirzi, Hannaneh},
  year = {2022},
  month = mar,
  number = {arXiv:2110.08387},
  eprint = {2110.08387},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  abstract = {It remains an open question whether incorporating external knowledge benefits commonsense reasoning while maintaining the flexibility of pretrained sequence models. To investigate this question, we develop generated knowledge prompting, which consists of generating knowledge from a language model, then providing the knowledge as additional input when answering a question. Our method does not require task-specific supervision for knowledge integration, or access to a structured knowledge base, yet it improves performance of large-scale, state-of-the-art models on four commonsense reasoning tasks, achieving state-of-the-art results on numerical commonsense (NumerSense), general commonsense (CommonsenseQA 2.0), and scientific commonsense (QASC) benchmarks. Generated knowledge prompting highlights large-scale language models as flexible sources of external knowledge for improving commonsense reasoning. Our code is available at \textbackslash url\{github.com/liujch1998/GKP\}},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computation and Language},
  file = {C\:\\Users\\Hytn\\Zotero\\storage\\H235Z9PW\\Liu et al. - 2022 - Generated Knowledge Prompting for Commonsense Reas.pdf}
}

@inproceedings{liu2022saliency,
  title = {Saliency as {{Evidence}}: {{Event Detection}} with {{Trigger Saliency Attribution}}},
  shorttitle = {Saliency as {{Evidence}}},
  booktitle = {Proceedings of the 60th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}} ({{Volume}} 1: {{Long Papers}})},
  author = {Liu, Jian and Chen, Yufeng and Xu, Jinan},
  year = {2022},
  pages = {4573--4585},
  publisher = {{Association for Computational Linguistics}},
  address = {{Dublin, Ireland}},
  doi = {10.18653/v1/2022.acl-long.313},
  langid = {english},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\storage\\Saliency as Evidence _Liu et al 2022.pdf}
}

@inproceedings{liu2022simple,
  title = {A {{Simple}} yet {{Effective Relation Information Guided Approach}} for {{Few-Shot Relation Extraction}}},
  booktitle = {Findings of the {{Association}} for {{Computational Linguistics}}: {{ACL}} 2022},
  author = {Liu, Yang and Hu, Jinpeng and Wan, Xiang and Chang, Tsung-Hui},
  year = {2022},
  pages = {757--763},
  publisher = {{Association for Computational Linguistics}},
  address = {{Dublin, Ireland}},
  doi = {10.18653/v1/2022.findings-acl.62},
  langid = {english},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\storage\\A Simple yet Effective Relation Information Guided Approach for Few-Shot _Liu et al 2022.pdf}
}

@inproceedings{liu2022simplea,
  title = {A {{Simple}} yet {{Effective Relation Information Guided Approach}} for {{Few-Shot Relation Extraction}}},
  booktitle = {Findings of the {{Association}} for {{Computational Linguistics}}: {{ACL}} 2022},
  author = {Liu, Yang and Hu, Jinpeng and Wan, Xiang and Chang, Tsung-Hui},
  year = {2022},
  pages = {757--763},
  publisher = {{Association for Computational Linguistics}},
  address = {{Dublin, Ireland}},
  doi = {10.18653/v1/2022.findings-acl.62},
  langid = {english},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\storage\\A Simple yet Effective Relation Information Guided Approach for Few-Shot _Liu et al 22.pdf}
}

@article{loshchilov2017decoupled,
  title = {Decoupled Weight Decay Regularization},
  author = {Loshchilov, Ilya and Hutter, Frank},
  year = {2017},
  journal = {arXiv preprint arXiv:1711.05101},
  eprint = {1711.05101},
  eprinttype = {arxiv},
  archiveprefix = {arXiv},
  keywords = {⛔ No DOI found}
}

@inproceedings{loshchilov2019decoupled,
  title = {Decoupled {{Weight Decay Regularization}}},
  booktitle = {{{ICLR}}},
  author = {Loshchilov, I. and Hutter, F.},
  year = {2019},
  keywords = {⛔ No DOI found}
}

@article{loureiro2022lmms,
  title = {{{LMMS}} Reloaded: {{Transformer-based}} Sense Embeddings for Disambiguation and Beyond},
  shorttitle = {{{LMMS}} Reloaded},
  author = {Loureiro, Daniel and M{\'a}rio Jorge, Al{\'i}pio and {Camacho-Collados}, Jose},
  year = {2022},
  month = apr,
  journal = {Artificial Intelligence},
  volume = {305},
  pages = {103661},
  issn = {00043702},
  doi = {10.1016/j.artint.2022.103661},
  langid = {english},
  file = {C\:\\Users\\Hytn\\Zotero\\storage\\KB9YJ6LR\\LMMS reloaded_Loureiro et al 2022.pdf}
}

@inproceedings{lu2021text2event,
  title = {{{Text2Event}}: {{Controllable Sequence-to-Structure Generation}} for {{End-to-end Event Extraction}}},
  booktitle = {Proceedings of the 59th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}} and the 11th {{International Joint Conference}} on {{Natural Language Processing}} ({{Volume}} 1: {{Long Papers}})},
  author = {Lu, Yaojie and Lin, Hongyu and Xu, Jin and Han, Xianpei and Tang, Jialong and Li, Annan and Sun, Le and Liao, Meng and Chen, Shaoyi},
  year = {2021},
  pages = {2795--2806},
  publisher = {{Association for Computational Linguistics}},
  doi = {10.18653/v1/2021.acl-long.217},
  file = {C\:\\Users\\Hytn\\Zotero\\storage\\PER94USI\\Lu et al_2021_Text2Event.pdf}
}

@inproceedings{lu2022unified,
  title = {Unified Structure Generation for Universal Information Extraction},
  booktitle = {Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: {{Long}} Papers)},
  author = {Lu, Yaojie and Liu, Qing and Dai, Dai and Xiao, Xinyan and Lin, Hongyu and Han, Xianpei and Sun, Le and Wu, Hua},
  year = {2022},
  month = may,
  pages = {5755--5772},
  publisher = {{Association for Computational Linguistics}},
  address = {{Dublin, Ireland}},
  abstract = {Information extraction suffers from its varying targets, heterogeneous structures, and demand-specific schemas. In this paper, we propose a unified text-to-structure generation framework, namely UIE, which can universally model different IE tasks, adaptively generate targeted structures, and collaboratively learn general IE abilities from different knowledge sources. Specifically, UIE uniformly encodes different extraction structures via a structured extraction language, adaptively generates target extractions via a schema-based prompt mechanism \textendash{} structural schema instructor, and captures the common IE abilities via a large-scale pretrained text-to-structure model. Experiments show that UIE achieved the state-of-the-art performance on 4 IE tasks, 13 datasets, and on all supervised, low-resource, and few-shot settings for a wide range of entity, relation, event and sentiment extraction tasks and their unification. These results verified the effectiveness, universality, and transferability of UIE.},
  file = {C\:\\Users\\Hytn\\Zotero\\storage\\LGRKQBFA\\Unified structure generation for universal information extraction_Lu et al 2022.pdf}
}

@inproceedings{luong2015effective,
  title = {Effective {{Approaches}} to {{Attention-based Neural Machine Translation}}},
  booktitle = {Proceedings of the 2015 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}}},
  author = {Luong, Minh-Thang and Pham, Hieu and Manning, Christopher D.},
  year = {2015},
  pages = {1412--1421},
  doi = {10.18653/v1/D15-1166},
  file = {C\:\\Users\\Hytn\\Zotero\\storage\\K9Q7QWL6\\Luong et al_2015_Effective Approaches to Attention-based Neural Machine Translation.pdf}
}

@inproceedings{ma2016user,
  title = {User {{Fatigue}} in {{Online News Recommendation}}},
  booktitle = {Proceedings of the 25th {{International Conference}} on {{World Wide Web}}},
  author = {Ma, Hao and Liu, Xueqing and Shen, Zhihong},
  year = {2016},
  month = apr,
  pages = {1363--1372},
  publisher = {{International World Wide Web Conferences Steering Committee}},
  address = {{Montr\'eal Qu\'ebec Canada}},
  doi = {10.1145/2872427.2874813},
  isbn = {978-1-4503-4143-1},
  langid = {english}
}

@article{ma2021effective,
  title = {Effective {{Cascade Dual-Decoder Model}} for {{Joint Entity}} and {{Relation Extraction}}},
  author = {Ma, Lianbo and Ren, Huimin and Zhang, Xiliang},
  year = {2021},
  publisher = {{arXiv}},
  doi = {10.48550/ARXIV.2106.14163},
  abstract = {Extracting relational triples from texts is a fundamental task in knowledge graph construction. The popular way of existing methods is to jointly extract entities and relations using a single model, which often suffers from the overlapping triple problem. That is, there are multiple relational triples that share the same entities within one sentence. In this work, we propose an effective cascade dual-decoder approach to extract overlapping relational triples, which includes a text-specific relation decoder and a relation-corresponded entity decoder. Our approach is straightforward: the text-specific relation decoder detects relations from a sentence according to its text semantics and treats them as extra features to guide the entity extraction; for each extracted relation, which is with trainable embedding, the relation-corresponded entity decoder detects the corresponding head and tail entities using a span-based tagging scheme. In this way, the overlapping triple problem is tackled naturally. Experiments on two public datasets demonstrate that our proposed approach outperforms state-of-the-art methods and achieves better F1 scores under the strict evaluation metric. Our implementation is available at https://github.com/prastunlp/DualDec.},
  copyright = {arXiv.org perpetual, non-exclusive license},
  keywords = {Artificial Intelligence (cs.AI),Computation and Language (cs.CL),FOS: Computer and information sciences},
  file = {C\:\\Users\\Hytn\\Zotero\\storage\\S2Q4Y867\\Effective Cascade Dual-Decoder Model for Joint Entity and Relation Extraction_Ma et al 2021.pdf}
}

@inproceedings{maddela2022entsum,
  title = {{{EntSUM}}: {{A Data Set}} for {{Entity-Centric Extractive Summarization}}},
  shorttitle = {{{EntSUM}}},
  booktitle = {Proceedings of the 60th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}} ({{Volume}} 1: {{Long Papers}})},
  author = {Maddela, Mounica and Kulkarni, Mayank and {Preotiuc-Pietro}, Daniel},
  year = {2022},
  pages = {3355--3366},
  publisher = {{Association for Computational Linguistics}},
  address = {{Dublin, Ireland}},
  doi = {10.18653/v1/2022.acl-long.237},
  langid = {english},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\storage\\EntSUM _Maddela et al 2022.pdf}
}

@misc{mao2022dyle,
  title = {{{DYLE}}: {{Dynamic Latent Extraction}} for {{Abstractive Long-Input Summarization}}},
  shorttitle = {{{DYLE}}},
  author = {Mao, Ziming and Wu, Chen Henry and Ni, Ansong and Zhang, Yusen and Zhang, Rui and Yu, Tao and Deb, Budhaditya and Zhu, Chenguang and Awadallah, Ahmed H. and Radev, Dragomir},
  year = {2022},
  month = apr,
  number = {arXiv:2110.08168},
  eprint = {2110.08168},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  abstract = {Transformer-based models have achieved state-of-the-art performance on short-input summarization. However, they still struggle with summarizing longer text. In this paper, we present DYLE, a novel dynamic latent extraction approach for abstractive long-input summarization. DYLE jointly trains an extractor and a generator and treats the extracted text snippets as the latent variable, allowing dynamic snippet-level attention weights during decoding. To provide adequate supervision, we propose simple yet effective heuristics for oracle extraction as well as a consistency loss term, which encourages the extractor to approximate the averaged dynamic weights predicted by the generator. We evaluate our method on different long-document and long-dialogue summarization tasks: GovReport, QMSum, and arXiv. Experiment results show that DYLE outperforms all existing methods on GovReport and QMSum, with gains up to 6.1 ROUGE, while yielding strong results on arXiv. Further analysis shows that the proposed dynamic weights provide interpretability of our generation process.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computation and Language},
  file = {C\:\\Users\\Hytn\\Zotero\\storage\\PWNW8HSD\\Mao et al. - 2022 - DYLE Dynamic Latent Extraction for Abstractive Lo.pdf}
}

@inproceedings{marcheggiani2017encoding,
  title = {Encoding {{Sentences}} with {{Graph Convolutional Networks}} for {{Semantic Role Labeling}}},
  booktitle = {Proceedings of the 2017 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}}},
  author = {Marcheggiani, Diego and Titov, Ivan},
  year = {2017},
  pages = {1506--1515},
  keywords = {⛔ No DOI found}
}

@inproceedings{martinezlorenzo2022fullysemantic,
  title = {Fully-{{Semantic Parsing}} and {{Generation}}: The {{BabelNet Meaning Representation}}},
  shorttitle = {Fully-{{Semantic Parsing}} and {{Generation}}},
  booktitle = {Proceedings of the 60th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}} ({{Volume}} 1: {{Long Papers}})},
  author = {Mart{\'i}nez Lorenzo, Abelardo Carlos and Maru, Marco and Navigli, Roberto},
  year = {2022},
  pages = {1727--1741},
  publisher = {{Association for Computational Linguistics}},
  address = {{Dublin, Ireland}},
  doi = {10.18653/v1/2022.acl-long.121},
  langid = {english},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\storage\\Fully-Semantic Parsing and Generation _Martínez Lorenzo et al 2022.pdf}
}

@inproceedings{matthias20automatic,
  title = {Automatic Shortcut Removal for Self-Supervised Representation Learning},
  booktitle = {Proceedings of the 37th International Conference on Machine Learning},
  author = {Minderer, Matthias and Bachem, Olivier and Houlsby, Neil and Tschannen, Michael},
  editor = {III, Hal Daum{\'e} and Singh, Aarti},
  year = {2020},
  month = jul,
  series = {Proceedings of Machine Learning Research},
  volume = {119},
  pages = {6927--6937},
  publisher = {{PMLR}},
  abstract = {In self-supervised visual representation learning, a feature extractor is trained on a "pretext task" for which labels can be generated cheaply, without human annotation. A central challenge in this approach is that the feature extractor quickly learns to exploit low-level visual features such as color aberrations or watermarks and then fails to learn useful semantic representations. Much work has gone into identifying such "shortcut" features and hand-designing schemes to reduce their effect. Here, we propose a general framework for mitigating the effect shortcut features. Our key assumption is that those features which are the first to be exploited for solving the pretext task may also be the most vulnerable to an adversary trained to make the task harder. We show that this assumption holds across common pretext tasks and datasets by training a "lens" network to make small image changes that maximally reduce performance in the pretext task. Representations learned with the modified images outperform those learned without in all tested cases. Additionally, the modifications made by the lens reveal how the choice of pretext task and dataset affects the features learned by self-supervision.},
  pdf = {http://proceedings.mlr.press/v119/minderer20a/minderer20a.pdf},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\storage\\Automatic shortcut removal for self-supervised representation learning _Minderer et al 2020.pdf}
}

@inproceedings{mccoy2019right,
  title = {Right for the {{Wrong Reasons}}: {{Diagnosing Syntactic Heuristics}} in {{Natural Language Inference}}},
  shorttitle = {Right for the {{Wrong Reasons}}},
  booktitle = {Proceedings of the 57th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}}},
  author = {McCoy, Tom and Pavlick, Ellie and Linzen, Tal},
  year = {2019},
  pages = {3428--3448},
  publisher = {{Association for Computational Linguistics}},
  address = {{Florence, Italy}},
  doi = {10.18653/v1/P19-1334},
  langid = {english},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\storage\\Right for the Wrong Reasons _McCoy et al 2019.pdf}
}

@article{medvedeva2022rethinking,
  title = {Rethinking the Field of Automatic Prediction of Court Decisions},
  author = {Medvedeva, Masha and Wieling, Martijn and Vols, Michel},
  year = {2022},
  month = jan,
  journal = {Artificial Intelligence and Law},
  issn = {0924-8463, 1572-8382},
  doi = {10.1007/s10506-021-09306-3},
  abstract = {In this paper, we discuss previous research in automatic prediction of court decisions. We define the difference between outcome identification, outcome-based judgement categorisation and outcome forecasting, and review how various studies fall into these categories. We discuss how important it is to understand the legal data that one works with in order to determine which task can be performed. Finally, we reflect on the needs of the legal discipline regarding the analysis of court judgements.},
  langid = {english},
  file = {C\:\\Users\\Hytn\\Zotero\\storage\\5IWYVLQL\\Medvedeva et al. - 2022 - Rethinking the field of automatic prediction of co.pdf}
}

@article{merrill2019generalized,
  title = {Generalized {{Integrated Gradients}}: {{A}} Practical Method for Explaining Diverse Ensembles},
  shorttitle = {Generalized {{Integrated Gradients}}},
  author = {Merrill, John and Ward, Geoff and Kamkar, Sean and Budzik, Jay and Merrill, Douglas},
  year = {2019},
  publisher = {{arXiv}},
  doi = {10.48550/ARXIV.1909.01869},
  abstract = {We introduce Generalized Integrated Gradients (GIG), a formal extension of the Integrated Gradients (IG) (Sundararajan et al., 2017) method for attributing credit to the input variables of a predictive model. GIG improves IG by explaining a broader variety of functions that arise from practical applications of ML in domains like financial services. GIG is constructed to overcome limitations of Shapley (1953) and Aumann-Shapley (1974), and has desirable properties when compared to other approaches. We prove GIG is the only correct method, under a small set of reasonable axioms, for providing explanations for mixed-type models or games. We describe the implementation, and present results of experiments on several datasets and systems of models.},
  copyright = {arXiv.org perpetual, non-exclusive license},
  keywords = {FOS: Computer and information sciences,GIG,I.2.6; H.1.2; K.4,Machine Learning (cs.LG),Machine Learning (stat.ML)},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\storage\\Generalized Integrated Gradients _Merrill et al 2019.pdf}
}

@inproceedings{micikevicius2017mixed,
  title = {Mixed {{Precision Training}}},
  booktitle = {International {{Conference}} on {{Learning Representations}}},
  author = {Micikevicius, Paulius and Narang, Sharan and Alben, Jonah and Diamos, Gregory F. and Elsen, Erich and Garc{\'i}a, David and Ginsburg, Boris and Houston, Michael and Kuchaiev, Oleksii and Venkatesh, Ganesh and Wu, Hao},
  year = {2017},
  keywords = {⛔ No DOI found}
}

@article{mikolov2013efficient,
  title = {Efficient {{Estimation}} of {{Word Representations}} in {{Vector Space}}},
  author = {Mikolov, Tomas and Chen, Kai and Corrado, Greg and Dean, Jeffrey},
  year = {2013},
  publisher = {{arXiv}},
  doi = {10.48550/ARXIV.1301.3781},
  abstract = {We propose two novel model architectures for computing continuous vector representations of words from very large data sets. The quality of these representations is measured in a word similarity task, and the results are compared to the previously best performing techniques based on different types of neural networks. We observe large improvements in accuracy at much lower computational cost, i.e. it takes less than a day to learn high quality word vectors from a 1.6 billion words data set. Furthermore, we show that these vectors provide state-of-the-art performance on our test set for measuring syntactic and semantic word similarities.},
  copyright = {arXiv.org perpetual, non-exclusive license},
  keywords = {Computation and Language (cs.CL),FOS: Computer and information sciences},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\storage\\Efficient Estimation of Word Representations in Vector Space _Mikolov et al 2013.pdf}
}

@article{miller1995wordnet,
  title = {{{WordNet}}: A Lexical Database for {{English}}},
  shorttitle = {{{WordNet}}},
  author = {Miller, George A.},
  year = {1995},
  month = nov,
  journal = {Communications of the ACM},
  volume = {38},
  number = {11},
  pages = {39--41},
  issn = {0001-0782, 1557-7317},
  doi = {10.1145/219717.219748},
  abstract = {Because meaningful sentences are composed of meaningful words, any system that hopes to process natural languages as people do must have information about words and their meanings. This information is traditionally provided through dictionaries, and machine-readable dictionaries are now widely available. But dictionary entries evolved for the convenience of human readers, not for machines. WordNet               1               provides a more effective combination of traditional lexicographic information and modern computing. WordNet is an online lexical database designed for use under program control. English nouns, verbs, adjectives, and adverbs are organized into sets of synonyms, each representing a lexicalized concept. Semantic relations link the synonym sets [4].},
  langid = {english},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\storage\\WordNet _Miller 1995.pdf}
}

@inproceedings{mintz2009distant,
  title = {Distant Supervision for Relation Extraction without Labeled Data},
  booktitle = {Proceedings of the {{Joint Conference}} of the 47th {{Annual Meeting}} of the {{ACL}} and the 4th {{International Joint Conference}} on {{Natural Language Processing}} of the {{AFNLP}}},
  author = {Mintz, Mike and Bills, Steven and Snow, Rion and Jurafsky, Dan},
  year = {2009},
  pages = {1003--1011},
  keywords = {⛔ No DOI found}
}

@inproceedings{miwa2014modeling,
  title = {Modeling Joint Entity and Relation Extraction with Table Representation},
  booktitle = {Proceedings of the 2014 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}} ({{EMNLP}})},
  author = {Miwa, Makoto and Sasaki, Yutaka},
  year = {2014},
  pages = {1858--1869},
  doi = {10.3115/v1/D14-1200},
  file = {C\:\\Users\\Hytn\\Zotero\\storage\\NWVQ9PSB\\Miwa_Sasaki_2014_Modeling joint entity and relation extraction with table representation.pdf}
}

@article{miwa2016endtoend,
  title = {End-to-End Relation Extraction Using Lstms on Sequences and Tree Structures},
  author = {Miwa, Makoto and Bansal, Mohit},
  year = {2016},
  journal = {arXiv preprint arXiv:1601.00770},
  eprint = {1601.00770},
  eprinttype = {arxiv},
  archiveprefix = {arXiv},
  keywords = {⛔ No DOI found}
}

@book{molnar2020interpretable,
  title = {Interpretable Machine Learning},
  author = {Molnar, Christoph},
  year = {2020},
  publisher = {{Lulu. com}}
}

@article{montavon2017explaining,
  title = {Explaining Nonlinear Classification Decisions with Deep {{Taylor}} Decomposition},
  author = {Montavon, Gr{\'e}goire and Lapuschkin, Sebastian and Binder, Alexander and Samek, Wojciech and M{\"u}ller, Klaus-Robert},
  year = {2017},
  month = may,
  journal = {Pattern Recognition},
  volume = {65},
  pages = {211--222},
  issn = {00313203},
  doi = {10.1016/j.patcog.2016.11.008},
  langid = {english},
  keywords = {DTD},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\storage\\Explaining nonlinear classification decisions with deep Taylor decomposition _Montavon et al 2017.pdf}
}

@article{montavon2018methods,
  title = {Methods for Interpreting and Understanding Deep Neural Networks},
  author = {Montavon, Gr{\'e}goire and Samek, Wojciech and M{\"u}ller, Klaus-Robert},
  year = {2018},
  month = feb,
  journal = {Digital Signal Processing},
  volume = {73},
  pages = {1--15},
  issn = {10512004},
  doi = {10.1016/j.dsp.2017.10.011},
  langid = {english},
  file = {C\:\\Users\\Hytn\\Zotero\\storage\\FUR3EAKL\\Montavon et al. - 2018 - Methods for interpreting and understanding deep ne.pdf}
}

@inproceedings{moro2022discriminative,
  title = {Discriminative {{Marginalized Probabilistic Neural Method}} for {{Multi-Document Summarization}} of {{Medical Literature}}},
  booktitle = {Proceedings of the 60th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}} ({{Volume}} 1: {{Long Papers}})},
  author = {Moro, Gianluca and Ragazzi, Luca and Valgimigli, Lorenzo and Freddi, Davide},
  year = {2022},
  pages = {180--189},
  publisher = {{Association for Computational Linguistics}},
  address = {{Dublin, Ireland}},
  doi = {10.18653/v1/2022.acl-long.15},
  langid = {english},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\storage\\Discriminative Marginalized Probabilistic Neural Method for Multi-Document _Moro et al 2022.pdf}
}

@inproceedings{mudrakarta2018did,
  title = {Did the {{Model Understand}} the {{Question}}?},
  booktitle = {Proceedings of the 56th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}} ({{Volume}} 1: {{Long Papers}})},
  author = {Mudrakarta, Pramod Kaushik and Taly, Ankur and Sundararajan, Mukund and Dhamdhere, Kedar},
  year = {2018},
  pages = {1896--1906},
  publisher = {{Association for Computational Linguistics}},
  doi = {10.18653/v1/P18-1176},
  file = {C\:\\Users\\Hytn\\Zotero\\storage\\VNYB8FR9\\Mudrakarta et al_2018_Did the Model Understand the Question.pdf}
}

@inproceedings{mudrakarta2018dida,
  title = {Did the {{Model Understand}} the {{Question}}?},
  booktitle = {Proceedings of the 56th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}} ({{Volume}} 1: {{Long Papers}})},
  author = {Mudrakarta, Pramod Kaushik and Taly, Ankur and Sundararajan, Mukund and Dhamdhere, Kedar},
  year = {2018},
  pages = {1896--1906},
  publisher = {{Association for Computational Linguistics}},
  address = {{Melbourne, Australia}},
  doi = {10.18653/v1/P18-1176},
  langid = {english},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\storage\\Did the Model Understand the Question _Mudrakarta et al 2018.pdf}
}

@inproceedings{mueller2022label,
  title = {Label {{Semantic Aware Pre-training}} for {{Few-shot Text Classification}}},
  booktitle = {Proceedings of the 60th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}} ({{Volume}} 1: {{Long Papers}})},
  author = {Mueller, Aaron and Krone, Jason and Romeo, Salvatore and Mansour, Saab and Mansimov, Elman and Zhang, Yi and Roth, Dan},
  year = {2022},
  pages = {8318--8334},
  publisher = {{Association for Computational Linguistics}},
  address = {{Dublin, Ireland}},
  doi = {10.18653/v1/2022.acl-long.570},
  langid = {english},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\storage\\Label Semantic Aware Pre-training for Few-shot Text Classification _Mueller et al 2022.pdf}
}

@inproceedings{mullenbach2018explainable,
  title = {Explainable {{Prediction}} of {{Medical Codes}} from {{Clinical Text}}},
  booktitle = {Proceedings of the 2018 {{Conference}} of the {{North American Chapter}} of           the {{Association}} for {{Computational Linguistics}}: {{Human Language}}           {{Technologies}}, {{Volume}} 1 ({{Long Papers}})},
  author = {Mullenbach, James and Wiegreffe, Sarah and Duke, Jon and Sun, Jimeng and Eisenstein, Jacob},
  year = {2018},
  pages = {1101--1111},
  publisher = {{Association for Computational Linguistics}},
  address = {{New Orleans, Louisiana}},
  doi = {10.18653/v1/N18-1100},
  langid = {english},
  keywords = {LWAN},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\storage\\Explainable Prediction of Medical Codes from Clinical Text _Mullenbach et al 2018.pdf}
}

@article{nagel1960weighting,
  title = {{{WEIGHTING VARIABLES IN JUDICIAL PREDICTION}}},
  author = {Nagel, Stuart},
  year = {1960},
  journal = {MULL: Modern Uses of Logic in Law},
  volume = {2},
  number = {3},
  pages = {93--97},
  publisher = {{American Bar Association}},
  issn = {21589240},
  keywords = {⛔ No DOI found}
}

@inproceedings{nan2020reasoning,
  title = {Reasoning with {{Latent Structure Refinement}} for {{Document-Level Relation Extraction}}},
  booktitle = {Proceedings of the 58th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}}},
  author = {Nan, Guoshun and Guo, Zhijiang and Sekulic, Ivan and Lu, Wei},
  year = {2020},
  pages = {1546--1557},
  doi = {10.18653/v1/2020.acl-main.141},
  file = {C\:\\Users\\Hytn\\Zotero\\storage\\QWG9IL6L\\Nan et al_2020_Reasoning with Latent Structure Refinement for Document-Level Relation.pdf}
}

@inproceedings{nayak2020effective,
  title = {Effective Modeling of Encoder-Decoder Architecture for Joint Entity and Relation Extraction},
  booktitle = {Proceedings of the {{AAAI Conference}} on {{Artificial Intelligence}}},
  author = {Nayak, Tapas and Ng, Hwee Tou},
  year = {2020},
  volume = {34},
  pages = {8528--8535},
  keywords = {⛔ No DOI found}
}

@article{neuberg2003causality,
  title = {{{CAUSALITY}}: {{MODELS}}, {{REASONING}}, {{AND INFERENCE}}, by {{Judea Pearl}}, {{Cambridge University Press}}, 2000},
  author = {Neuberg, Leland Gerson},
  year = {2003},
  month = aug,
  journal = {Econometric Theory},
  volume = {19},
  number = {04},
  issn = {0266-4666},
  doi = {10.1017/S0266466603004109},
  file = {C\:\\Users\\Hytn\\Zotero\\storage\\9NV4UYUX\\Neuberg_2003_CAUSALITY.pdf}
}

@article{nguyen2016multifaceted,
  title = {Multifaceted {{Feature Visualization}}: {{Uncovering}} the {{Different Types}} of {{Features Learned By Each Neuron}} in {{Deep Neural Networks}}},
  shorttitle = {Multifaceted {{Feature Visualization}}},
  author = {Nguyen, Anh and Yosinski, Jason and Clune, Jeff},
  year = {2016},
  month = may,
  journal = {arXiv:1602.03616 [cs]},
  eprint = {1602.03616},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {We can better understand deep neural networks by identifying which features each of their neurons have learned to detect. To do so, researchers have created Deep Visualization techniques including activation maximization, which synthetically generates inputs (e.g. images) that maximally activate each neuron. A limitation of current techniques is that they assume each neuron detects only one type of feature, but we know that neurons can be multifaceted, in that they fire in response to many different types of features: for example, a grocery store class neuron must activate either for rows of produce or for a storefront. Previous activation maximization techniques constructed images without regard for the multiple different facets of a neuron, creating inappropriate mixes of colors, parts of objects, scales, orientations, etc. Here, we introduce an algorithm that explicitly uncovers the multiple facets of each neuron by producing a synthetic visualization of each of the types of images that activate a neuron. We also introduce regularization methods that produce state-of-the-art results in terms of the interpretability of images obtained by activation maximization. By separately synthesizing each type of image a neuron fires in response to, the visualizations have more appropriate colors and coherent global structure. Multifaceted feature visualization thus provides a clearer and more comprehensive description of the role of each neuron.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {⛔ No DOI found,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Neural and Evolutionary Computing},
  file = {C\:\\Users\\Hytn\\Zotero\\storage\\SFB4T2VI\\Nguyen et al. - 2016 - Multifaceted Feature Visualization Uncovering the.pdf}
}

@inproceedings{nguyen2016synthesizing,
  title = {Synthesizing the Preferred Inputs for Neurons in Neural Networks via Deep Generator Networks},
  booktitle = {Proceedings of the 30th International Conference on Neural Information Processing Systems},
  author = {Nguyen, Anh and Dosovitskiy, Alexey and Yosinski, Jason and Brox, Thomas and Clune, Jeff},
  year = {2016},
  series = {{{NIPS}}'16},
  pages = {3395--3403},
  publisher = {{Curran Associates Inc.}},
  address = {{Red Hook, NY, USA}},
  abstract = {Deep neural networks (DNNs) have demonstrated state-of-the-art results on many pattern recognition tasks, especially vision classification problems. Understanding the inner workings of such computational brains is both fascinating basic science that is interesting in its own right\textemdash similar to why we study the human brain\textemdash and will enable researchers to further improve DNNs. One path to understanding how a neural network functions internally is to study what each of its neurons has learned to detect. One such method is called activation maximization (AM), which synthesizes an input (e.g. an image) that highly activates a neuron. Here we dramatically improve the qualitative state of the art of activation maximization by harnessing a powerful, learned prior: a deep generator network (DGN). The algorithm (1) generates qualitatively state-of-the-art synthetic images that look almost real, (2) reveals the features learned by each neuron in an interpretable way, (3) generalizes well to new datasets and somewhat well to different network architectures without requiring the prior to be relearned, and (4) can be considered as a high-quality generative method (in this case, by generating novel, creative, interesting, recognizable images).},
  isbn = {978-1-5108-3881-9},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\Synthesizing the preferred inputs for neurons in neural networks via deep _Nguyen et al 2016.pdf}
}

@inproceedings{nguyen2018convolutional,
  title = {Convolutional Neural Networks for Chemical-Disease Relation Extraction Are Improved with Character-Based Word Embeddings},
  booktitle = {Proceedings of the {{BioNLP}} 2018 Workshop},
  author = {Nguyen, Dat Quoc and Verspoor, Karin},
  year = {2018},
  pages = {129--136},
  doi = {10.18653/v1/W18-2314},
  file = {C\:\\Users\\Hytn\\Zotero\\storage\\J3CJS5J3\\Nguyen_Verspoor_2018_Convolutional neural networks for chemical-disease relation extraction are.pdf}
}

@inproceedings{NIPS2013_9aa42b31,
  title = {Distributed Representations of Words and Phrases and Their Compositionality},
  booktitle = {Advances in Neural Information Processing Systems},
  author = {Mikolov, Tomas and Sutskever, Ilya and Chen, Kai and Corrado, Greg S and Dean, Jeff},
  editor = {Burges, C.J. and Bottou, L. and Welling, M. and Ghahramani, Z. and Weinberger, K.Q.},
  year = {2013},
  volume = {26},
  publisher = {{Curran Associates, Inc.}},
  keywords = {⛔ No DOI found},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\storage\\Distributed representations of words and phrases and their compositionality _Mikolov et al 2013.pdf}
}

@inproceedings{NIPS2016_a486cd07,
  title = {Man Is to Computer Programmer as Woman Is to Homemaker? {{Debiasing}} Word Embeddings},
  booktitle = {Advances in Neural Information Processing Systems},
  author = {Bolukbasi, Tolga and Chang, Kai-Wei and Zou, James Y and Saligrama, Venkatesh and Kalai, Adam T},
  editor = {Lee, D. and Sugiyama, M. and Luxburg, U. and Guyon, I. and Garnett, R.},
  year = {2016},
  volume = {29},
  publisher = {{Curran Associates, Inc.}},
  keywords = {⛔ No DOI found},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\storage\\Man is to computer programmer as woman is to homemaker _Bolukbasi et al 2016.pdf}
}

@inproceedings{niven2019probing,
  title = {Probing {{Neural Network Comprehension}} of {{Natural Language Arguments}}},
  booktitle = {Proceedings of the 57th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}}},
  author = {Niven, Timothy and Kao, Hung-Yu},
  year = {2019},
  pages = {4658--4664},
  publisher = {{Association for Computational Linguistics}},
  address = {{Florence, Italy}},
  doi = {10.18653/v1/P19-1459},
  langid = {english},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\storage\\Probing Neural Network Comprehension of Natural Language Arguments _Niven_Kao 2019.pdf}
}

@article{paolini2021structured,
  title = {Structured {{Prediction}} as {{Translation}} between {{Augmented Natural Languages}}},
  author = {Paolini, Giovanni and Athiwaratkun, Ben and Krone, Jason and Ma, Jie and Achille, Alessandro and Anubhai, Rishita and dos Santos, Cicero Nogueira and Xiang, Bing and Soatto, Stefano},
  year = {2021},
  publisher = {{arXiv}},
  doi = {10.48550/ARXIV.2101.05779},
  abstract = {We propose a new framework, Translation between Augmented Natural Languages (TANL), to solve many structured prediction language tasks including joint entity and relation extraction, nested named entity recognition, relation classification, semantic role labeling, event extraction, coreference resolution, and dialogue state tracking. Instead of tackling the problem by training task-specific discriminative classifiers, we frame it as a translation task between augmented natural languages, from which the task-relevant information can be easily extracted. Our approach can match or outperform task-specific models on all tasks, and in particular, achieves new state-of-the-art results on joint entity and relation extraction (CoNLL04, ADE, NYT, and ACE2005 datasets), relation classification (FewRel and TACRED), and semantic role labeling (CoNLL-2005 and CoNLL-2012). We accomplish this while using the same architecture and hyperparameters for all tasks and even when training a single model to solve all tasks at the same time (multi-task learning). Finally, we show that our framework can also significantly improve the performance in a low-resource regime, thanks to better use of label semantics.},
  copyright = {arXiv.org perpetual, non-exclusive license},
  keywords = {Computation and Language (cs.CL),FOS: Computer and information sciences,Machine Learning (cs.LG)},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\storage\\Structured Prediction as Translation between Augmented Natural Languages _Paolini et al 2021.pdf}
}

@article{parascandololearning,
  title = {Learning {{Independent Causal Mechanisms}}},
  author = {Parascandolo, Giambattista and Kilbertus, Niki and {Rojas-Carulla}, Mateo and Scholkopf, Bernhard},
  pages = {9},
  langid = {english},
  keywords = {⛔ No DOI found},
  file = {C\:\\Users\\Hytn\\Zotero\\storage\\IQADWTHZ\\Parascandolo et al. - Learning Independent Causal Mechanisms.pdf}
}

@inproceedings{park2018reducing,
  title = {Reducing {{Gender Bias}} in {{Abusive Language Detection}}},
  booktitle = {Proceedings of the 2018 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}}},
  author = {Park, Ji Ho and Shin, Jamin and Fung, Pascale},
  year = {2018},
  pages = {2799--2804},
  publisher = {{Association for Computational Linguistics}},
  address = {{Brussels, Belgium}},
  doi = {10.18653/v1/D18-1302},
  langid = {english},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\storage\\Reducing Gender Bias in Abusive Language Detection _Park et al 2018.pdf}
}

@inproceedings{pasunuru2021efficiently,
  title = {Efficiently {{Summarizing Text}} and {{Graph Encodings}} of {{Multi-Document Clusters}}},
  booktitle = {Proceedings of the 2021 {{Conference}} of the {{North American Chapter}} of the {{Association}} for {{Computational Linguistics}}: {{Human Language Technologies}}},
  author = {Pasunuru, Ramakanth and Liu, Mengwen and Bansal, Mohit and Ravi, Sujith and Dreyer, Markus},
  year = {2021},
  pages = {4768--4779},
  publisher = {{Association for Computational Linguistics}},
  address = {{Online}},
  doi = {10.18653/v1/2021.naacl-main.380},
  langid = {english},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\storage\\Efficiently Summarizing Text and Graph Encodings of Multi-Document Clusters _Pasunuru et al 2021.pdf}
}

@inproceedings{pasunuru2021efficientlya,
  title = {Efficiently {{Summarizing Text}} and {{Graph Encodings}} of {{Multi-Document Clusters}}},
  booktitle = {Proceedings of the 2021 {{Conference}} of the {{North American Chapter}} of the {{Association}} for {{Computational Linguistics}}: {{Human Language Technologies}}},
  author = {Pasunuru, Ramakanth and Liu, Mengwen and Bansal, Mohit and Ravi, Sujith and Dreyer, Markus},
  year = {2021},
  pages = {4768--4779},
  publisher = {{Association for Computational Linguistics}},
  address = {{Online}},
  doi = {10.18653/v1/2021.naacl-main.380},
  langid = {english},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\storage\\Efficiently Summarizing Text and Graph Encodings of Multi-Document Clusters _Pasunuru et al 22.pdf}
}

@article{paszke2017automatic,
  title = {Automatic Differentiation in Pytorch},
  author = {Paszke, Adam and Gross, Sam and Chintala, Soumith and Chanan, Gregory and Yang, Edward and DeVito, Zachary and Lin, Zeming and Desmaison, Alban and Antiga, Luca and Lerer, Adam},
  year = {2017},
  keywords = {⛔ No DOI found}
}

@article{pearl2009causal,
  title = {Causal Inference in Statistics: {{An}} Overview},
  shorttitle = {Causal Inference in Statistics},
  author = {Pearl, Judea},
  year = {2009},
  month = jan,
  journal = {Statistics Surveys},
  volume = {3},
  number = {none},
  issn = {1935-7516},
  doi = {10.1214/09-SS057},
  file = {C\:\\Users\\Hytn\\Zotero\\storage\\FWVJBJEW\\Causal inference in statistics_Pearl 2009.pdf}
}

@book{pearl2009causality,
  title = {Causality},
  author = {Pearl, Judea},
  year = {2009},
  publisher = {{Cambridge university press}}
}

@article{pearl2014external,
  title = {External {{Validity}}: {{From Do-Calculus}} to {{Transportability Across Populations}}},
  author = {Pearl, Judea and Bareinboim, Elias},
  year = {2014},
  month = nov,
  journal = {Statistical Science},
  volume = {29},
  number = {4},
  issn = {0883-4237},
  doi = {10.1214/14-STS486},
  file = {C\:\\Users\\Hytn\\Zotero\\storage\\EEL2MLD6\\Pearl_Bareinboim_2014_External Validity.pdf}
}

@article{peng2017crosssentence,
  title = {Cross-Sentence n-Ary Relation Extraction with Graph Lstms},
  author = {Peng, Nanyun and Poon, Hoifung and Quirk, Chris and Toutanova, Kristina and Yih, Wen-tau},
  year = {2017},
  journal = {Transactions of the Association for Computational Linguistics},
  volume = {5},
  pages = {101--115},
  publisher = {{MIT Press}},
  doi = {10.1162/tacl_a_00049},
  file = {C\:\\Users\\Hytn\\Zotero\\storage\\J8H4V7E2\\Peng et al_2017_Cross-sentence n-ary relation extraction with graph lstms.pdf}
}

@article{peng2017crosssentencea,
  title = {Cross-{{Sentence}} {{{\emph{N}}}} -Ary {{Relation Extraction}} with {{Graph LSTMs}}},
  author = {Peng, Nanyun and Poon, Hoifung and Quirk, Chris and Toutanova, Kristina and Yih, Wen-tau},
  year = {2017},
  month = dec,
  journal = {Transactions of the Association for Computational Linguistics},
  volume = {5},
  pages = {101--115},
  issn = {2307-387X},
  doi = {10.1162/tacl_a_00049},
  abstract = {Past work in relation extraction has focused on binary relations in single sentences. Recent NLP inroads in high-value domains have sparked interest in the more general setting of extracting n-ary relations that span multiple sentences. In this paper, we explore a general relation extraction framework based on graph long short-term memory networks (graph LSTMs) that can be easily extended to cross-sentence n-ary relation extraction. The graph formulation provides a unified way of exploring different LSTM approaches and incorporating various intra-sentential and intersentential dependencies, such as sequential, syntactic, and discourse relations. A robust contextual representation is learned for the entities, which serves as input to the relation classifier. This simplifies handling of relations with arbitrary arity, and enables multi-task learning with related relations. We evaluate this framework in two important precision medicine settings, demonstrating its effectiveness with both conventional supervised learning and distant supervision. Cross-sentence extraction produced larger knowledge bases. and multi-task learning significantly improved extraction accuracy. A thorough analysis of various LSTM approaches yielded useful insight the impact of linguistic analysis on extraction accuracy.},
  langid = {english},
  file = {C\:\\Users\\Hytn\\Zotero\\storage\\6EX28N39\\Peng et al. - 2017 - Cross-Sentence N -ary Relation Extraction w.pdf}
}

@inproceedings{pennington2014glove,
  title = {Glove: {{Global}} Vectors for Word Representation},
  booktitle = {Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing ({{EMNLP}})},
  author = {Pennington, Jeffrey and Socher, Richard and Manning, Christopher D},
  year = {2014},
  pages = {1532--1543},
  doi = {10.3115/v1/D14-1162},
  file = {C\:\\Users\\Hytn\\Zotero\\storage\\RJ3DURFN\\Pennington et al_2014_Glove.pdf}
}

@inproceedings{pmlr-v119-bras20a,
  title = {Adversarial Filters of Dataset Biases},
  booktitle = {Proceedings of the 37th International Conference on Machine Learning},
  author = {Bras, Ronan Le and Swayamdipta, Swabha and Bhagavatula, Chandra and Zellers, Rowan and Peters, Matthew and Sabharwal, Ashish and Choi, Yejin},
  editor = {III, Hal Daum{\'e} and Singh, Aarti},
  year = {2020},
  month = jul,
  series = {Proceedings of Machine Learning Research},
  volume = {119},
  pages = {1078--1088},
  publisher = {{PMLR}},
  abstract = {Large neural models have demonstrated human-level performance on language and vision benchmarks, while their performance degrades considerably on adversarial or out-of-distribution samples. This raises the question of whether these models have learned to solve a dataset rather than the underlying task by overfitting to spurious dataset biases. We investigate one recently proposed approach, AFLITE, which adversarially filters such dataset biases, as a means to mitigate the prevalent overestimation of machine performance. We provide a theoretical understanding for AFLITE, by situating it in the generalized framework for optimum bias reduction. We present extensive supporting evidence that AFLITE is broadly applicable for reduction of measurable dataset biases, and that models trained on the filtered datasets yield better generalization to out-of-distribution tasks. Finally, filtering results in a large drop in model performance (e.g., from 92\% to 62\% for SNLI), while human performance still remains high. Our work thus shows that such filtered datasets can pose new research challenges for robust generalization by serving as upgraded benchmarks.},
  pdf = {http://proceedings.mlr.press/v119/bras20a/bras20a.pdf},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\storage\\Adversarial filters of dataset biases _Bras et al 2020.pdf}
}

@inproceedings{pmlr-v119-chang20c,
  title = {Invariant Rationalization},
  booktitle = {Proceedings of the 37th International Conference on Machine Learning},
  author = {Chang, Shiyu and Zhang, Yang and Yu, Mo and Jaakkola, Tommi},
  editor = {III, Hal Daum{\'e} and Singh, Aarti},
  year = {2020},
  month = jul,
  series = {Proceedings of Machine Learning Research},
  volume = {119},
  pages = {1448--1458},
  publisher = {{PMLR}},
  abstract = {Selective rationalization improves neural network interpretability by identifying a small subset of input features \textemdash{} the rationale \textemdash{} that best explains or supports the prediction. A typical rationalization criterion, i.e. maximum mutual information (MMI), finds the rationale that maximizes the prediction performance based only on the rationale. However, MMI can be problematic because it picks up spurious correlations between the input features and the output. Instead, we introduce a game-theoretic invariant rationalization criterion where the rationales are constrained to enable the same predictor to be optimal across different environments. We show both theoretically and empirically that the proposed rationales can rule out spurious correlations and generalize better to different test scenarios. The resulting explanations also align better with human judgments. Our implementations are publicly available at https://github.com/code-terminator/invariant\textsubscript{r}ationalization.},
  pdf = {http://proceedings.mlr.press/v119/chang20c/chang20c.pdf},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\storage\\Invariant rationalization _Chang et al 2020.pdf}
}

@inproceedings{pmlr-v119-sagawa20a,
  title = {An Investigation of Why Overparameterization Exacerbates Spurious Correlations},
  booktitle = {Proceedings of the 37th International Conference on Machine Learning},
  author = {Sagawa, Shiori and Raghunathan, Aditi and Koh, Pang Wei and Liang, Percy},
  editor = {III, Hal Daum{\'e} and Singh, Aarti},
  year = {2020},
  month = jul,
  series = {Proceedings of Machine Learning Research},
  volume = {119},
  pages = {8346--8356},
  publisher = {{PMLR}},
  abstract = {We study why overparameterization\textemdash increasing model size well beyond the point of zero training error\textemdash can hurt test error on minority groups despite improving average test error when there are spurious correlations in the data. Through simulations and experiments on two image datasets, we identify two key properties of the training data that drive this behavior: the proportions of majority versus minority groups, and the signal-to-noise ratio of the spurious correlations. We then analyze a linear setting and theoretically show how the inductive bias of models towards ``memorizing'' fewer examples can cause overparameterization to hurt. Our analysis leads to a counterintuitive approach of subsampling the majority group, which empirically achieves low minority error in the overparameterized regime, even though the standard approach of upweighting the minority fails. Overall, our results suggest a tension between using overparameterized models versus using all the training data for achieving low worst-group error.},
  pdf = {http://proceedings.mlr.press/v119/sagawa20a/sagawa20a.pdf},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\storage\\An investigation of why overparameterization exacerbates spurious correlations _Sagawa et al 2020.pdf}
}

@inproceedings{pmlr-v119-srivastava20a,
  title = {Robustness to Spurious Correlations via Human Annotations},
  booktitle = {Proceedings of the 37th International Conference on Machine Learning},
  author = {Srivastava, Megha and Hashimoto, Tatsunori and Liang, Percy},
  editor = {III, Hal Daum{\'e} and Singh, Aarti},
  year = {2020},
  month = jul,
  series = {Proceedings of Machine Learning Research},
  volume = {119},
  pages = {9109--9119},
  publisher = {{PMLR}},
  abstract = {The reliability of machine learning systems critically assumes that the associations between features and labels remain similar between training and test distributions. However, unmeasured variables, such as confounders, break this assumption\textemdash useful correlations between features and labels at training time can become useless or even harmful at test time. For example, high obesity is generally predictive for heart disease, but this relation may not hold for smokers who generally have lower rates of obesity and higher rates of heart disease. We present a framework for making models robust to spurious correlations by leveraging humans' common sense knowledge of causality. Specifically, we use human annotation to augment each training example with a potential unmeasured variable (i.e. an underweight patient with heart disease may be a smoker), reducing the problem to a covariate shift problem. We then introduce a new distributionally robust optimization objective over unmeasured variables (UV-DRO) to control the worst-case loss over possible test- time shifts. Empirically, we show improvements of 5\textendash 10\% on a digit recognition task confounded by rotation, and 1.5\textendash 5\% on the task of analyzing NYPD Police Stops confounded by location.},
  pdf = {http://proceedings.mlr.press/v119/srivastava20a/srivastava20a.pdf},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\storage\\Robustness to spurious correlations via human annotations _Srivastava et al 2020.pdf}
}

@inproceedings{pmlr-v130-kamath21a,
  title = {Does Invariant Risk Minimization Capture Invariance?},
  booktitle = {Proceedings of the 24th International Conference on Artificial Intelligence and Statistics},
  author = {Kamath, Pritish and Tangella, Akilesh and Sutherland, Danica and Srebro, Nathan},
  editor = {Banerjee, Arindam and Fukumizu, Kenji},
  year = {2021},
  month = apr,
  series = {Proceedings of Machine Learning Research},
  volume = {130},
  pages = {4069--4077},
  publisher = {{PMLR}},
  abstract = {We show that the Invariant Risk Minimization (IRM) formulation of Arjovsky et al. (2019) can fail to capture "natural" invariances, at least when used in its practical "linear" form, and even on very simple problems which directly follow the motivating examples for IRM. This can lead to worse generalization on new environments, even when compared to unconstrained ERM. The issue stems from a significant gap between the linear variant (as in their concrete method IRMv1) and the full non-linear IRM formulation. Additionally, even when capturing the "right" invariances, we show that it is possible for IRM to learn a sub-optimal predictor, due to the loss function not being invariant across environments. The issues arise even when measuring invariance on the population distributions, but are exacerbated by the fact that IRM is extremely fragile to sampling.},
  pdf = {http://proceedings.mlr.press/v130/kamath21a/kamath21a.pdf},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\storage\\Does invariant risk minimization capture invariance _Kamath et al 2021.pdf}
}

@inproceedings{pmlr-v139-creager21a,
  title = {Environment Inference for Invariant Learning},
  booktitle = {Proceedings of the 38th International Conference on Machine Learning},
  author = {Creager, Elliot and Jacobsen, Joern-Henrik and Zemel, Richard},
  editor = {Meila, Marina and Zhang, Tong},
  year = {2021},
  month = jul,
  series = {Proceedings of Machine Learning Research},
  volume = {139},
  pages = {2189--2200},
  publisher = {{PMLR}},
  abstract = {Learning models that gracefully handle distribution shifts is central to research on domain generalization, robust optimization, and fairness. A promising formulation is domain-invariant learning, which identifies the key issue of learning which features are domain-specific versus domain-invariant. An important assumption in this area is that the training examples are partitioned into ``domains'' or ``environments''. Our focus is on the more common setting where such partitions are not provided. We propose EIIL, a general framework for domain-invariant learning that incorporates Environment Inference to directly infer partitions that are maximally informative for downstream Invariant Learning. We show that EIIL outperforms invariant learning methods on the CMNIST benchmark without using environment labels, and significantly outperforms ERM on worst-group performance in the Waterbirds dataset. Finally, we establish connections between EIIL and algorithmic fairness, which enables EIIL to improve accuracy and calibration in a fair prediction problem.},
  pdf = {http://proceedings.mlr.press/v139/creager21a/creager21a.pdf},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\storage\\Environment inference for invariant learning _Creager et al 2021.pdf}
}

@inproceedings{pmlr-v139-krueger21a,
  title = {Out-of-Distribution Generalization via Risk Extrapolation ({{REx}})},
  booktitle = {Proceedings of the 38th International Conference on Machine Learning},
  author = {Krueger, David and Caballero, Ethan and Jacobsen, Joern-Henrik and Zhang, Amy and Binas, Jonathan and Zhang, Dinghuai and Priol, Remi Le and Courville, Aaron},
  editor = {Meila, Marina and Zhang, Tong},
  year = {2021},
  month = jul,
  series = {Proceedings of Machine Learning Research},
  volume = {139},
  pages = {5815--5826},
  publisher = {{PMLR}},
  abstract = {Distributional shift is one of the major obstacles when transferring machine learning prediction systems from the lab to the real world. To tackle this problem, we assume that variation across training domains is representative of the variation we might encounter at test time, but also that shifts at test time may be more extreme in magnitude. In particular, we show that reducing differences in risk across training domains can reduce a model's sensitivity to a wide range of extreme distributional shifts, including the challenging setting where the input contains both causal and anti-causal elements. We motivate this approach, Risk Extrapolation (REx), as a form of robust optimization over a perturbation set of extrapolated domains (MM-REx), and propose a penalty on the variance of training risks (V-REx) as a simpler variant. We prove that variants of REx can recover the causal mechanisms of the targets, while also providing robustness to changes in the input distribution (``covariate shift''). By appropriately trading-off robustness to causally induced distributional shifts and covariate shift, REx is able to outperform alternative methods such as Invariant Risk Minimization in situations where these types of shift co-occur.},
  pdf = {http://proceedings.mlr.press/v139/krueger21a/krueger21a.pdf},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\storage\\Out-of-distribution generalization via risk extrapolation (REx) _Krueger et al 2021.pdf}
}

@inproceedings{pmlr-v162-miao22a,
  title = {Interpretable and Generalizable Graph Learning via Stochastic Attention Mechanism},
  booktitle = {Proceedings of the 39th International Conference on Machine Learning},
  author = {Miao, Siqi and Liu, Mia and Li, Pan},
  editor = {Chaudhuri, Kamalika and Jegelka, Stefanie and Song, Le and Szepesvari, Csaba and Niu, Gang and Sabato, Sivan},
  year = {2022},
  month = jul,
  series = {Proceedings of Machine Learning Research},
  volume = {162},
  pages = {15524--15543},
  publisher = {{PMLR}},
  abstract = {Interpretable graph learning is in need as many scientific applications depend on learning models to collect insights from graph-structured data. Previous works mostly focused on using post-hoc approaches to interpret pre-trained models (graph neural networks in particular). They argue against inherently interpretable models because the good interpretability of these models is often at the cost of their prediction accuracy. However, those post-hoc methods often fail to provide stable interpretation and may extract features that are spuriously correlated with the task. In this work, we address these issues by proposing Graph Stochastic Attention (GSAT). Derived from the information bottleneck principle, GSAT injects stochasticity to the attention weights to block the information from task-irrelevant graph components while learning stochasticity-reduced attention to select task-relevant subgraphs for interpretation. The selected subgraphs provably do not contain patterns that are spuriously correlated with the task under some assumptions. Extensive experiments on eight datasets show that GSAT outperforms the state-of-the-art methods by up to 20\% in interpretation AUC and 5\% in prediction accuracy. Our code is available at https://github.com/Graph-COM/GSAT.},
  pdf = {https://proceedings.mlr.press/v162/miao22a/miao22a.pdf},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\storage\\Interpretable and generalizable graph learning via stochastic attention _Miao et al 2022.pdf}
}

@inproceedings{pmlr-v162-zhou22d,
  title = {Model Agnostic Sample Reweighting for Out-of-Distribution Learning},
  booktitle = {Proceedings of the 39th International Conference on Machine Learning},
  author = {Zhou, Xiao and Lin, Yong and Pi, Renjie and Zhang, Weizhong and Xu, Renzhe and Cui, Peng and Zhang, Tong},
  editor = {Chaudhuri, Kamalika and Jegelka, Stefanie and Song, Le and Szepesvari, Csaba and Niu, Gang and Sabato, Sivan},
  year = {2022},
  month = jul,
  series = {Proceedings of Machine Learning Research},
  volume = {162},
  pages = {27203--27221},
  publisher = {{PMLR}},
  abstract = {Distributionally robust optimization (DRO) and invariant risk minimization (IRM) are two popular methods proposed to improve out-of-distribution (OOD) generalization performance of machine learning models. While effective for small models, it has been observed that these methods can be vulnerable to overfitting with large overparameterized models. This work proposes a principled method, Model Agnostic samPLe rEweighting (MAPLE), to effectively address OOD problem, especially in overparameterized scenarios. Our key idea is to find an effective reweighting of the training samples so that the standard empirical risk minimization training of a large model on the weighted training data leads to superior OOD generalization performance. The overfitting issue is addressed by considering a bilevel formulation to search for the sample reweighting, in which the generalization complexity depends on the search space of sample weights instead of the model size. We present theoretical analysis in linear case to prove the insensitivity of MAPLE to model size, and empirically verify its superiority in surpassing state-of-the-art methods by a large margin.},
  pdf = {https://proceedings.mlr.press/v162/zhou22d/zhou22d.pdf},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\storage\\Model agnostic sample reweighting for out-of-distribution learning _Zhou et al 2022.pdf}
}

@inproceedings{pmlr-v80-chen18j,
  title = {Learning to Explain: {{An}} Information-Theoretic Perspective on Model Interpretation},
  booktitle = {Proceedings of the 35th International Conference on Machine Learning},
  author = {Chen, Jianbo and Song, Le and Wainwright, Martin and Jordan, Michael},
  editor = {Dy, Jennifer and Krause, Andreas},
  year = {2018},
  month = jul,
  series = {Proceedings of Machine Learning Research},
  volume = {80},
  pages = {883--892},
  publisher = {{PMLR}},
  abstract = {We introduce instancewise feature selection as a methodology for model interpretation. Our method is based on learning a function to extract a subset of features that are most informative for each given example. This feature selector is trained to maximize the mutual information between selected features and the response variable, where the conditional distribution of the response variable given the input is the model to be explained. We develop an efficient variational approximation to the mutual information, and show the effectiveness of our method on a variety of synthetic and real data sets using both quantitative metrics and human evaluation.},
  pdf = {http://proceedings.mlr.press/v80/chen18j/chen18j.pdf},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\storage\\Learning to explain _Chen et al 2018.pdf}
}

@inproceedings{poliak2018hypothesis,
  title = {Hypothesis {{Only Baselines}} in {{Natural Language Inference}}},
  booktitle = {Proceedings of the {{Seventh Joint Conference}} on {{Lexical}} and           {{Computational Semantics}}},
  author = {Poliak, Adam and Naradowsky, Jason and Haldar, Aparajita and Rudinger, Rachel and Van Durme, Benjamin},
  year = {2018},
  pages = {180--191},
  publisher = {{Association for Computational Linguistics}},
  address = {{New Orleans, Louisiana}},
  doi = {10.18653/v1/S18-2023},
  langid = {english},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\storage\\Hypothesis Only Baselines in Natural Language Inference _Poliak et al 2018.pdf}
}

@inproceedings{qin2022comparison,
  title = {A {{Comparison Study}} of {{Pre-trained Language Models}} for {{Chinese Legal Document Classification}}},
  booktitle = {2022 5th {{International Conference}} on {{Artificial Intelligence}} and {{Big Data}} ({{ICAIBD}})},
  author = {Qin, Ruyu and Huang, Min and Luo, Yutong},
  year = {2022},
  month = may,
  pages = {444--449},
  publisher = {{IEEE}},
  address = {{Chengdu, China}},
  doi = {10.1109/ICAIBD55127.2022.9820466},
  abstract = {Legal artificial intelligence (LegalAI), aiming to benefit the legal domain using artificial intelligence technologies, is the hot topic of the moment. As the basis for various LegalAI tasks such as judgment prediction and similar case matching, the classification of legal documents is an issue that has to be addressed. The majority of current approaches focus on the legal systems of native English-speaking countries. However, both Chinese language and legal system differ significantly from that of English. Given the success of pre-trained Language Models (PLMs) and outperformance compared with feature-engineering-based machine learning models as well as traditional deep neural network models such as CNNs and RNNs in NLP, their effectiveness in specific domains needs to be further investigated, especially in legal domain. Moreover, few studies have made comparisons of these PLMs for specific legal tasks. Therefore, in this paper we train several strong PLMs which differ in pretraining corpus on three datasets of Chinese legal documents. Experimental results show that the model pre-trained on the legal corpus demonstrates its high efficiency on all datasets.},
  isbn = {978-1-66549-913-2},
  langid = {english},
  file = {C\:\\Users\\Hytn\\Zotero\\storage\\NDW2YR5R\\Qin et al. - 2022 - A Comparison Study of Pre-trained Language Models .pdf}
}

@inproceedings{qiu2019dynamically,
  title = {Dynamically Fused Graph Network for Multi-Hop Reasoning},
  booktitle = {Proceedings of the 57th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}}},
  author = {Qiu, Lin and Xiao, Yunxuan and Qu, Yanru and Zhou, Hao and Li, Lei and Zhang, Weinan and Yu, Yong},
  year = {2019},
  pages = {6140--6150},
  doi = {10.18653/v1/P19-1617},
  file = {C\:\\Users\\Hytn\\Zotero\\storage\\LXF96P58\\Qiu et al_2019_Dynamically fused graph network for multi-hop reasoning.pdf}
}

@article{quessard2020learning,
  title = {Learning {{Group Structure}} and {{Disentangled Representations}} of {{Dynamical Environments}}},
  author = {Quessard, Robin and Barrett, Thomas D. and Clements, William R.},
  year = {2020},
  month = oct,
  journal = {arXiv:2002.06991 [cs, stat]},
  eprint = {2002.06991},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {Learning disentangled representations is a key step towards effectively discovering and modelling the underlying structure of environments. In the natural sciences, physics has found great success by describing the universe in terms of symmetry preserving transformations. Inspired by this formalism, we propose a framework, built upon the theory of group representation, for learning representations of a dynamical environment structured around the transformations that generate its evolution. Experimentally, we learn the structure of explicitly symmetric environments without supervision from observational data generated by sequential interactions. We further introduce an intuitive disentanglement regularisation to ensure the interpretability of the learnt representations. We show that our method enables accurate long-horizon predictions, and demonstrate a correlation between the quality of predictions and disentanglement in the latent space.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {⛔ No DOI found,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C\:\\Users\\Hytn\\Zotero\\storage\\EFZBJLXK\\Quessard et al. - 2020 - Learning Group Structure and Disentangled Represen.pdf}
}

@inproceedings{quirk2017distant,
  title = {Distant {{Supervision}} for {{Relation Extraction}} beyond the {{Sentence Boundary}}},
  booktitle = {Proceedings of the 15th {{Conference}} of the {{European Chapter}} of the {{Association}} for {{Computational Linguistics}}: {{Volume}} 1, {{Long Papers}}},
  author = {Quirk, Chris and Poon, Hoifung},
  year = {2017},
  pages = {1171--1182},
  doi = {10.18653/v1/E17-1110},
  file = {C\:\\Users\\Hytn\\Zotero\\storage\\5CCQ6KS5\\Quirk_Poon_2017_Distant Supervision for Relation Extraction beyond the Sentence Boundary.pdf}
}

@inproceedings{redmon2016you,
  title = {You Only Look Once: {{Unified}}, Real-Time Object Detection},
  booktitle = {Proceedings of the {{IEEE}} Conference on Computer Vision and Pattern Recognition ({{CVPR}})},
  author = {Redmon, Joseph and Divvala, Santosh and Girshick, Ross and Farhadi, Ali},
  year = {2016},
  month = jun,
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\storage\\You only look once _Redmon et al 2016.pdf}
}

@article{reed2001pareto,
  title = {The {{Pareto}}, {{Zipf}} and Other Power Laws},
  author = {Reed, William J.},
  year = {2001},
  month = dec,
  journal = {Economics Letters},
  volume = {74},
  number = {1},
  pages = {15--19},
  publisher = {{North-Holland}},
  issn = {01651765},
  doi = {10.1016/S0165-1765(01)00524-9},
  abstract = {Many empirical size distributions in economics and elsewhere exhibit power-law behaviour in the upper tail. This article contains a simple explanation for this. It also predicts lower-tail power-law behaviour, which is verified empirically for income and city-size data. \textcopyright{} Elsevier Science B.V.},
  keywords = {C49,City-size distribution,D31,Gibrat's law,Income distribution,Power law,R12,Tail behaviour},
  file = {C\:\\Users\\Hytn\\Zotero\\storage\\VHUHAHYM\\Reed_2001_The Pareto, Zipf and other power laws.pdf}
}

@article{reed2001paretoa,
  title = {The {{Pareto}}, {{Zipf}} and Other Power Laws},
  author = {Reed, William J},
  year = {2001},
  month = dec,
  journal = {Economics Letters},
  volume = {74},
  number = {1},
  pages = {15--19},
  issn = {01651765},
  doi = {10.1016/S0165-1765(01)00524-9},
  langid = {english},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\storage\\The Pareto, Zipf and other power laws _Reed 2001.pdf}
}

@inproceedings{ren2015fasterrcnn,
  title = {Faster {{R-CNN}}: {{Towards}} Real-Time Object Detection with Region Proposal Networks},
  booktitle = {Advances in Neural Information Processing Systems},
  author = {Ren, Shaoqing and He, Kaiming and Girshick, Ross and Sun, Jian},
  editor = {Cortes, C. and Lawrence, N. and Lee, D. and Sugiyama, M. and Garnett, R.},
  year = {2015},
  volume = {28},
  publisher = {{Curran Associates, Inc.}},
  keywords = {⛔ No DOI found},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\storage\\Faster R-CNN _Ren et al 2015.pdf}
}

@inproceedings{ren2017cotype,
  title = {{{CoType}}: {{Joint Extraction}} of {{Typed Entities}} and {{Relations}} with {{Knowledge Bases}}},
  shorttitle = {{{CoType}}},
  booktitle = {Proceedings of the 26th {{International Conference}} on {{World Wide Web}}},
  author = {Ren, Xiang and Wu, Zeqiu and He, Wenqi and Qu, Meng and Voss, Clare R. and Ji, Heng and Abdelzaher, Tarek F. and Han, Jiawei},
  year = {2017},
  month = apr,
  pages = {1015--1024},
  publisher = {{International World Wide Web Conferences Steering Committee}},
  address = {{Perth Australia}},
  doi = {10.1145/3038912.3052708},
  isbn = {978-1-4503-4913-0},
  langid = {english},
  file = {C\:\\Users\\Hytn\\Zotero\\storage\\CU8LYRVV\\Ren et al_2017_CoType.pdf}
}

@inproceedings{rethmeier2022longtail,
  title = {Long-{{Tail Zero}} and {{Few-Shot Learning}} via {{Contrastive Pretraining}} on and for {{Small Data}}},
  booktitle = {{{AAAI Workshop}} on {{Artificial Intelligence}} with {{Biased}} or {{Scarce Data}} ({{AIBSD}})},
  author = {Rethmeier, Nils and Augenstein, Isabelle},
  year = {2022},
  month = may,
  pages = {10},
  publisher = {{MDPI}},
  doi = {10.3390/cmsf2022003010},
  langid = {english},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\storage\\Long-Tail Zero and Few-Shot Learning via Contrastive Pretraining on and for _Rethmeier_Augenstein 2022.pdf}
}

@inproceedings{ribeiro2016why,
  title = {"{{Why Should I Trust You}}?": {{Explaining}} the {{Predictions}} of {{Any Classifier}}},
  shorttitle = {"{{Why Should I Trust You}}?},
  booktitle = {Proceedings of the 22nd {{ACM SIGKDD International Conference}} on {{Knowledge Discovery}} and {{Data Mining}}},
  author = {Ribeiro, Marco Tulio and Singh, Sameer and Guestrin, Carlos},
  year = {2016},
  month = aug,
  pages = {1135--1144},
  publisher = {{ACM}},
  address = {{San Francisco California USA}},
  doi = {10.1145/2939672.2939778},
  isbn = {978-1-4503-4232-2},
  langid = {english},
  keywords = {LIME,reference-based},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\storage\\Why Should I Trust You _Ribeiro et al 2016.pdf}
}

@inproceedings{ribeiro2018anchors,
  title = {Anchors: {{High-precision}} Model-Agnostic Explanations},
  booktitle = {Proceedings of the {{AAAI}} Conference on Artificial Intelligence},
  author = {Ribeiro, Marco Tulio and Singh, Sameer and Guestrin, Carlos},
  year = {2018},
  volume = {32},
  keywords = {⛔ No DOI found},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\Anchors _Ribeiro et al 2018.pdf}
}

@article{ribeiro2018anchorsa,
  title = {Anchors: {{High-Precision Model-Agnostic Explanations}}},
  shorttitle = {Anchors},
  author = {Ribeiro, Marco Tulio and Singh, Sameer and Guestrin, Carlos},
  year = {2018},
  month = apr,
  journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
  volume = {32},
  number = {1},
  issn = {2374-3468, 2159-5399},
  doi = {10.1609/aaai.v32i1.11491},
  abstract = {We introduce a novel model-agnostic system that explains the behavior of complex models with high-precision rules called anchors, representing local, "sufficient" conditions for predictions. We propose an algorithm to efficiently compute these explanations for any black-box model with high-probability guarantees. We demonstrate the flexibility of anchors by explaining a myriad of different models for different domains and tasks. In a user study, we show that anchors enable users to predict how a model would behave on unseen instances with less effort and higher precision, as compared to existing linear explanations or no explanations.},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\storage\\Anchors _Ribeiro et al 2018.pdf}
}

@inproceedings{ribeiro2018semantically,
  title = {Semantically {{Equivalent Adversarial Rules}} for {{Debugging NLP}} Models},
  booktitle = {Proceedings of the 56th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}} ({{Volume}} 1: {{Long Papers}})},
  author = {Ribeiro, Marco Tulio and Singh, Sameer and Guestrin, Carlos},
  year = {2018},
  pages = {856--865},
  publisher = {{Association for Computational Linguistics}},
  address = {{Melbourne, Australia}},
  doi = {10.18653/v1/P18-1079},
  langid = {english},
  keywords = {attack,paraphrasing},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\storage\\Semantically Equivalent Adversarial Rules for Debugging NLP models _Ribeiro et al 2018.pdf}
}

@inproceedings{ribeiro2020accuracy,
  title = {Beyond {{Accuracy}}: {{Behavioral Testing}} of {{NLP Models}} with {{CheckList}}},
  shorttitle = {Beyond {{Accuracy}}},
  booktitle = {Proceedings of the 58th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}}},
  author = {Ribeiro, Marco Tulio and Wu, Tongshuang and Guestrin, Carlos and Singh, Sameer},
  year = {2020},
  pages = {4902--4912},
  publisher = {{Association for Computational Linguistics}},
  address = {{Online}},
  doi = {10.18653/v1/2020.acl-main.442},
  langid = {english},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\storage\\Beyond Accuracy _Ribeiro et al 2020.pdf}
}

@inproceedings{riedel2010modeling,
  title = {Modeling Relations and Their Mentions without Labeled Text},
  booktitle = {Joint {{European Conference}} on {{Machine Learning}} and {{Knowledge Discovery}} in {{Databases}}},
  author = {Riedel, Sebastian and Yao, Limin and McCallum, Andrew},
  year = {2010},
  pages = {148--163},
  publisher = {{Springer}},
  doi = {10.1007/978-3-642-15939-8_10},
  keywords = {NYT},
  file = {C\:\\Users\\Hytn\\Zotero\\storage\\SZITGF8I\\Riedel et al_2010_Modeling relations and their mentions without labeled text.pdf}
}

@inproceedings{riedel2013relation,
  title = {Relation {{Extraction}} with {{Matrix Factorization}} and {{Universal Schemas}}},
  booktitle = {Proceedings of the 2013 {{Conference}} of the {{North American Chapter}} of the {{Association}} for {{Computational Linguistics}}: {{Human Language Technologies}}},
  author = {Riedel, Sebastian and Yao, Limin and McCallum, Andrew and Marlin, Benjamin M.},
  year = {2013},
  month = jun,
  pages = {74--84},
  publisher = {{Association for Computational Linguistics}},
  address = {{Atlanta, Georgia}}
}

@inproceedings{rink2010utd,
  title = {Utd: {{Classifying}} Semantic Relations by Combining Lexical and Semantic Resources},
  booktitle = {Proceedings of the 5th International Workshop on Semantic Evaluation},
  author = {Rink, Bryan and Harabagiu, Sanda},
  year = {2010},
  pages = {256--259},
  keywords = {⛔ No DOI found}
}

@inproceedings{rongali2020don,
  title = {Don't {{Parse}}, {{Generate}}! {{A Sequence}} to {{Sequence Architecture}} for {{Task-Oriented Semantic Parsing}}},
  booktitle = {Proceedings of {{The Web Conference}} 2020},
  author = {Rongali, Subendhu and Soldaini, Luca and Monti, Emilio and Hamza, Wael},
  year = {2020},
  month = apr,
  pages = {2962--2968},
  publisher = {{ACM}},
  address = {{Taipei Taiwan}},
  doi = {10.1145/3366423.3380064},
  isbn = {978-1-4503-7023-3},
  langid = {english},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\storage\\Don’t Parse, Generate _Rongali et al 2020.pdf}
}

@inproceedings{rony2022rome,
  title = {{{RoMe}}: {{A Robust Metric}} for {{Evaluating Natural Language Generation}}},
  shorttitle = {{{RoMe}}},
  booktitle = {Proceedings of the 60th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}} ({{Volume}} 1: {{Long Papers}})},
  author = {Rony, Md Rashad Al Hasan and Kovriguina, Liubov and Chaudhuri, Debanjan and Usbeck, Ricardo and Lehmann, Jens},
  year = {2022},
  pages = {5645--5657},
  publisher = {{Association for Computational Linguistics}},
  address = {{Dublin, Ireland}},
  doi = {10.18653/v1/2022.acl-long.387},
  langid = {english},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\storage\\RoMe _Rony et al 2022.pdf}
}

@inproceedings{rosenfeld2021the,
  title = {The Risks of Invariant Risk Minimization},
  booktitle = {International Conference on Learning Representations},
  author = {Rosenfeld, Elan and Ravikumar, Pradeep Kumar and Risteski, Andrej},
  year = {2021},
  keywords = {⛔ No DOI found},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\storage\\The risks of invariant risk minimization _Rosenfeld et al 2021.pdf}
}

@article{rubin2005causal,
  title = {Causal {{Inference Using Potential Outcomes}}},
  author = {Rubin, Donald B.},
  year = {2005},
  month = mar,
  journal = {Journal of the American Statistical Association},
  volume = {100},
  number = {469},
  pages = {322--331},
  issn = {0162-1459},
  doi = {10.1198/016214504000001880},
  file = {C\:\\Users\\Hytn\\Zotero\\storage\\XKJHLTBG\\Rubin_2005_Causal Inference Using Potential Outcomes.pdf}
}

@inproceedings{saha2022explanation,
  title = {Explanation {{Graph Generation}} via {{Pre-trained Language Models}}: {{An Empirical Study}} with {{Contrastive Learning}}},
  shorttitle = {Explanation {{Graph Generation}} via {{Pre-trained Language Models}}},
  booktitle = {Proceedings of the 60th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}} ({{Volume}} 1: {{Long Papers}})},
  author = {Saha, Swarnadeep and Yadav, Prateek and Bansal, Mohit},
  year = {2022},
  pages = {1190--1208},
  publisher = {{Association for Computational Linguistics}},
  address = {{Dublin, Ireland}},
  doi = {10.18653/v1/2022.acl-long.85},
  langid = {english},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\storage\\Explanation Graph Generation via Pre-trained Language Models _Saha et al 2022.pdf}
}

@inproceedings{saha2022explanationa,
  title = {Explanation {{Graph Generation}} via {{Pre-trained Language Models}}: {{An Empirical Study}} with {{Contrastive Learning}}},
  shorttitle = {Explanation {{Graph Generation}} via {{Pre-trained Language Models}}},
  booktitle = {Proceedings of the 60th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}} ({{Volume}} 1: {{Long Papers}})},
  author = {Saha, Swarnadeep and Yadav, Prateek and Bansal, Mohit},
  year = {2022},
  pages = {1190--1208},
  publisher = {{Association for Computational Linguistics}},
  address = {{Dublin, Ireland}},
  doi = {10.18653/v1/2022.acl-long.85},
  langid = {english},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\storage\\Explanation Graph Generation via Pre-trained Language Models _Saha et al 22.pdf}
}

@inproceedings{sahu2019intersentence,
  title = {Inter-Sentence {{Relation Extraction}} with {{Document-level Graph Convolutional Neural Network}}},
  booktitle = {Proceedings of the 57th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}}},
  author = {Sahu, Sunil Kumar and Christopoulou, Fenia and Miwa, Makoto and Ananiadou, Sophia},
  year = {2019},
  pages = {4309--4316},
  doi = {10.18653/v1/P19-1423},
  file = {C\:\\Users\\Hytn\\Zotero\\storage\\8KLUDIRP\\Sahu et al_2019_Inter-sentence Relation Extraction with Document-level Graph Convolutional.pdf}
}

@article{santos2015classifying,
  title = {Classifying Relations by Ranking with Convolutional Neural Networks},
  author = {dos Santos, Cicero Nogueira and Xiang, Bing and Zhou, Bowen},
  year = {2015},
  journal = {arXiv preprint arXiv:1504.06580},
  eprint = {1504.06580},
  eprinttype = {arxiv},
  archiveprefix = {arXiv},
  keywords = {⛔ No DOI found}
}

@incollection{scholkopf2022causality,
  title = {Causality for {{Machine Learning}}},
  booktitle = {Probabilistic and {{Causal Inference}}},
  author = {Sch{\"o}lkopf, Bernhard},
  editor = {Geffner, Hector and Dechter, Rina and Halpern, Joseph Y.},
  year = {2022},
  month = feb,
  edition = {First},
  pages = {765--804},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/3501714.3501755},
  isbn = {978-1-4503-9586-1},
  langid = {english},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\storage\\Causality for Machine Learning _Schölkopf 2022.pdf}
}

@article{schuster1997bidirectional,
  title = {Bidirectional Recurrent Neural Networks},
  author = {Schuster, Mike and Paliwal, Kuldip K.},
  year = {1997},
  journal = {IEEE transactions on Signal Processing},
  volume = {45},
  number = {11},
  pages = {2673--2681},
  publisher = {{Ieee}},
  doi = {10.1109/78.650093},
  file = {C\:\\Users\\Hytn\\Zotero\\storage\\P5BQ4Z5Y\\Schuster_Paliwal_1997_Bidirectional recurrent neural networks.pdf}
}

@inproceedings{see2017get,
  title = {Get to the Point: {{Summarization}} with Pointer-Generator Networks},
  booktitle = {Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: {{Long}} Papers)},
  author = {See, Abigail and Liu, Peter J and Manning, Christopher D},
  year = {2017},
  pages = {1073--1083},
  doi = {10.18653/v1/P17-1099},
  file = {C\:\\Users\\Hytn\\Zotero\\storage\\TMWE4RRZ\\Get to the point_See et al 2017.pdf}
}

@article{sevim2022gender,
  title = {Gender Bias in Legal Corpora and Debiasing It},
  author = {Sevim, Nurullah and {\c S}ahinu{\c c}, Furkan and Ko{\c c}, Aykut},
  year = {2022},
  month = mar,
  journal = {Natural Language Engineering},
  pages = {1--34},
  issn = {1351-3249, 1469-8110},
  doi = {10.1017/S1351324922000122},
  abstract = {Abstract             Word embeddings have become important building blocks that are used profoundly in natural language processing (NLP). Despite their several advantages, word embeddings can unintentionally accommodate some gender- and ethnicity-based biases that are present within the corpora they are trained on. Therefore, ethical concerns have been raised since word embeddings are extensively used in several high-level algorithms. Studying such biases and debiasing them have recently become an important research endeavor. Various studies have been conducted to measure the extent of bias that word embeddings capture and to eradicate them. Concurrently, as another subfield that has started to gain traction recently, the applications of NLP in the field of law have started to increase and develop rapidly. As law has a direct and utmost effect on people's lives, the issues of bias for NLP applications in legal domain are certainly important. However, to the best of our knowledge, bias issues have not yet been studied in the context of legal corpora. In this article, we approach the gender bias problem from the scope of legal text processing domain. Word embedding models that are trained on corpora composed by legal documents and legislation from different countries have been utilized to measure and eliminate gender bias in legal documents. Several methods have been employed to reveal the degree of gender bias and observe its variations over countries. Moreover, a debiasing method has been used to neutralize unwanted bias. The preservation of semantic coherence of the debiased vector space has also been demonstrated by using high-level tasks. Finally, overall results and their implications have been discussed in the scope of NLP in legal domain.},
  langid = {english}
}

@inproceedings{sha2021learning,
  title = {Learning from the Best: {{Rationalizing}} Predictions by Adversarial Information Calibration.},
  booktitle = {{{AAAI}}},
  author = {Sha, Lei and Camburu, Oana-Maria and Lukasiewicz, Thomas},
  year = {2021},
  pages = {13771--13779},
  keywords = {⛔ No DOI found},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\storage\\Learning from the best _Sha et al 2021.pdf}
}

@article{shang2022onerel,
  title = {{{OneRel}}:{{Joint Entity}} and {{Relation Extraction}} with {{One Module}} in {{One Step}}},
  shorttitle = {{{OneRel}}},
  author = {Shang, Yu-Ming and Huang, Heyan and Mao, Xian-Ling},
  year = {2022},
  month = mar,
  journal = {arXiv:2203.05412 [cs]},
  eprint = {2203.05412},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {Joint entity and relation extraction is an essential task in natural language processing and knowledge graph construction. Existing approaches usually decompose the joint extraction task into several basic modules or processing steps to make it easy to conduct. However, such a paradigm ignores the fact that the three elements of a triple are interdependent and indivisible. Therefore, previous joint methods suffer from the problems of cascading errors and redundant information. To address these issues, in this paper, we propose a novel joint entity and relation extraction model, named OneRel, which casts joint extraction as a fine-grained triple classification problem. Specifically, our model consists of a scoring-based classifier and a relation-specific horns tagging strategy. The former evaluates whether a token pair and a relation belong to a factual triple. The latter ensures a simple but effective decoding process. Extensive experimental results on two widely used datasets demonstrate that the proposed method performs better than the state-of-the-art baselines, and delivers consistent performance gain on complex scenarios of various overlapping patterns and multiple triples.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {⛔ No DOI found,Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {C\:\\Users\\Hytn\\Zotero\\storage\\DVN5TB9U\\Shang et al. - 2022 - OneRelJoint Entity and Relation Extraction with O.pdf}
}

@inproceedings{shao2020graph,
  title = {Is {{Graph Structure Necessary}} for {{Multi-hop Question Answering}}?},
  booktitle = {Proceedings of the 2020 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}} ({{EMNLP}})},
  author = {Shao, Nan and Cui, Yiming and Liu, Ting and Wang, Shijin and Hu, Guoping},
  year = {2020},
  pages = {7187--7192},
  doi = {10.18653/v1/2020.emnlp-main.583},
  file = {C\:\\Users\\Hytn\\Zotero\\storage\\IMJ3CDHK\\Shao et al_2020_Is Graph Structure Necessary for Multi-hop Question Answering.pdf}
}

@inproceedings{shen2022mred,
  title = {{{MReD}}: {{A Meta-Review Dataset}} for {{Structure-Controllable Text Generation}}},
  shorttitle = {{{MReD}}},
  booktitle = {Findings of the {{Association}} for {{Computational Linguistics}}: {{ACL}} 2022},
  author = {Shen, Chenhui and Cheng, Liying and Zhou, Ran and Bing, Lidong and You, Yang and Si, Luo},
  year = {2022},
  pages = {2521--2535},
  publisher = {{Association for Computational Linguistics}},
  address = {{Dublin, Ireland}},
  doi = {10.18653/v1/2022.findings-acl.198},
  langid = {english},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\storage\\MReD _Shen et al 2022.pdf}
}

@article{shi2019simple,
  title = {Simple Bert Models for Relation Extraction and Semantic Role Labeling},
  author = {Shi, Peng and Lin, Jimmy},
  year = {2019},
  journal = {arXiv preprint arXiv:1904.05255},
  eprint = {1904.05255},
  eprinttype = {arxiv},
  archiveprefix = {arXiv},
  keywords = {⛔ No DOI found}
}

@inproceedings{shrikumar2017learning,
  title = {Learning Important Features through Propagating Activation Differences},
  booktitle = {Proceedings of the 34th International Conference on Machine Learning},
  author = {Shrikumar, Avanti and Greenside, Peyton and Kundaje, Anshul},
  editor = {Precup, Doina and Teh, Yee Whye},
  year = {2017},
  month = aug,
  series = {Proceedings of Machine Learning Research},
  volume = {70},
  pages = {3145--3153},
  publisher = {{PMLR}},
  abstract = {The purported ``black box'' nature of neural networks is a barrier to adoption in applications where interpretability is essential. Here we present DeepLIFT (Deep Learning Important FeaTures), a method for decomposing the output prediction of a neural network on a specific input by backpropagating the contributions of all neurons in the network to every feature of the input. DeepLIFT compares the activation of each neuron to its `reference activation' and assigns contribution scores according to the difference. By optionally giving separate consideration to positive and negative contributions, DeepLIFT can also reveal dependencies which are missed by other approaches. Scores can be computed efficiently in a single backward pass. We apply DeepLIFT to models trained on MNIST and simulated genomic data, and show significant advantages over gradient-based methods. Video tutorial: http://goo.gl/qKb7pL code: http://goo.gl/RM8jvH},
  pdf = {http://proceedings.mlr.press/v70/shrikumar17a/shrikumar17a.pdf},
  keywords = {DeepLIFT,reference-based},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\storage\\Learning important features through propagating activation differences _Shrikumar et al 2017.pdf}
}

@inproceedings{sikdar2021integrated,
  title = {Integrated {{Directional Gradients}}: {{Feature Interaction Attribution}} for {{Neural NLP Models}}},
  shorttitle = {Integrated {{Directional Gradients}}},
  booktitle = {Proceedings of the 59th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}} and the 11th {{International Joint Conference}} on {{Natural Language Processing}} ({{Volume}} 1: {{Long Papers}})},
  author = {Sikdar, Sandipan and Bhattacharya, Parantapa and Heese, Kieran},
  year = {2021},
  pages = {865--878},
  publisher = {{Association for Computational Linguistics}},
  address = {{Online}},
  doi = {10.18653/v1/2021.acl-long.71},
  abstract = {In this paper, we introduce Integrated Directional Gradients (IDG), a method for attributing importance scores to groups of features, indicating their relevance to the output of a neural network model for a given input. The success of Deep Neural Networks has been attributed to their ability to capture higher level feature interactions. Hence, in the last few years capturing the importance of these feature interactions has received increased prominence in ML interpretability literature. In this paper, we formally define the feature group attribution problem and outline a set of axioms that any intuitive feature group attribution method should satisfy. Earlier, cooperative game theory inspired axiomatic methods only borrowed axioms from solution concepts (such as Shapley value) for individual feature attributions and introduced their own extensions to model interactions. In contrast, our formulation is inspired by axioms satisfied by characteristic functions as well as solution concepts in cooperative game theory literature. We believe that characteristic functions are much better suited to model importance of groups compared to just solution concepts. We demonstrate that our proposed method, IDG, satisfies all the axioms. Using IDG we analyze two state-of-the-art text classifiers on three benchmark datasets for sentiment analysis. Our experiments show that IDG is able to effectively capture semantic interactions in linguistic models via negations and conjunctions.},
  langid = {english},
  file = {C\:\\Users\\Hytn\\Zotero\\storage\\78IISYB3\\Sikdar et al. - 2021 - Integrated Directional Gradients Feature Interact.pdf}
}

@inproceedings{simonyan2019deep,
  title = {Deep inside Convolutional Networks: Visualising Image Classification Models and Saliency Maps},
  author = {Simonyan, K and Vedaldi, A and Zisserman, A},
  year = {2014},
  series = {Proceedings of the {{International Conference}} on {{Learning Representations}} ({{ICLR}})},
  pages = {1--8},
  publisher = {{ICLR}},
  keywords = {⛔ No DOI found},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\storage\\Deep inside convolutional networks _Simonyan et al 2014.pdf}
}

@inproceedings{soares2019matching,
  title = {Matching the {{Blanks}}: {{Distributional Similarity}} for {{Relation Learning}}},
  booktitle = {Proceedings of the 57th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}}},
  author = {Soares, Livio Baldini and FitzGerald, Nicholas Arthur and Ling, Jeffrey and Kwiatkowski, Tom},
  year = {2019},
  pages = {2895--2905},
  doi = {10.18653/v1/P19-1279},
  file = {C\:\\Users\\Hytn\\Zotero\\storage\\4EJKCF6Z\\Soares et al_2019_Matching the Blanks.pdf}
}

@inproceedings{song2018nary,
  title = {N-Ary {{Relation Extraction}} Using {{Graph-State LSTM}}},
  booktitle = {Proceedings of the 2018 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}}},
  author = {Song, Linfeng and Zhang, Yue and Wang, Zhiguo and Gildea, Daniel},
  year = {2018},
  pages = {2226--2235},
  doi = {10.18653/v1/D18-1246},
  file = {C\:\\Users\\Hytn\\Zotero\\storage\\9K5HFUNW\\Song et al_2018_N-ary Relation Extraction using Graph-State LSTM.pdf}
}

@inproceedings{springenberg2015striving,
  title = {Striving for Simplicity: {{The}} All Convolutional Net},
  booktitle = {{{ICLR}} (Workshop Track)},
  author = {Springenberg, J.T. and Dosovitskiy, A. and Brox, T. and Riedmiller, M.},
  year = {2015},
  keywords = {⛔ No DOI found},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\storage\\Striving for simplicity _Springenberg et al 2015.pdf}
}

@inproceedings{stanovsky2018supervised,
  title = {Supervised {{Open Information Extraction}}},
  booktitle = {Proceedings of the 2018 {{Conference}} of the {{North American Chapter}} of           the {{Association}} for {{Computational Linguistics}}: {{Human Language}}           {{Technologies}}, {{Volume}} 1 ({{Long Papers}})},
  author = {Stanovsky, Gabriel and Michael, Julian and Zettlemoyer, Luke and Dagan, Ido},
  year = {2018},
  pages = {885--895},
  publisher = {{Association for Computational Linguistics}},
  address = {{New Orleans, Louisiana}},
  doi = {10.18653/v1/N18-1081},
  langid = {english},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\storage\\Supervised Open Information Extraction _Stanovsky et al 2018.pdf}
}

@article{Stoica_Platanios_Poczos_2021,
  title = {Re-{{TACRED}}: {{Addressing}} Shortcomings of the {{TACRED}} Dataset},
  author = {Stoica, George and Platanios, Emmanouil Antonios and Poczos, Barnabas},
  year = {2021},
  month = may,
  journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
  volume = {35},
  number = {15},
  pages = {13843--13850},
  keywords = {⛔ No DOI found},
  annotation = {Abstract note: TACRED is one of the largest and most widely used sentence-level relation extraction datasets. Proposed models that are evaluated using this dataset consistently set new state-of-the-art performance. However, they still exhibit large error rates despite leveraging external knowledge and unsupervised pretraining on large text corpora. A recent study suggested that this may be due to poor dataset quality. The study observed that over 50\% of the most challenging sentences from the development and test sets are incorrectly labeled and account for an average drop of 8\% f1-score in model performance. However, this study was limited to a small biased sample of 5k (out of a total of 106k) sentences, substantially restricting the generalizability and broader implications of its findings. In this paper, we address these shortcomings by: (i) performing a comprehensive study over the whole TACRED dataset, (ii) proposing an improved crowdsourcing strategy and deploying it to re-annotate the whole dataset, and (iii) performing a thorough analysis to understand how correcting the TACRED annotations affects previously published results. After verification, we observed that 23.9\% of TACRED labels are incorrect. Moreover, evaluating several models on our revised dataset yields an average f1-score improvement of 14.3\% and helps uncover significant relationships between the different models (rather than simply offsetting or scaling their scores by a constant factor). Finally, aside from our analysis we also release Re-TACRED, a new completely re-annotated version of the TACRED dataset that can be used to perform reliable evaluation of relation extraction models.},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\storage\\Re-TACRED _Stoica et al 2021.pdf}
}

@inproceedings{strickson2020legal,
  title = {Legal {{Judgement Prediction}} for {{UK Courts}}},
  booktitle = {Proceedings of the 2020 {{The}} 3rd {{International Conference}} on {{Information Science}} and {{System}}},
  author = {Strickson, Benjamin and De La Iglesia, Beatriz},
  year = {2020},
  month = mar,
  pages = {204--209},
  publisher = {{ACM}},
  address = {{Cambridge United Kingdom}},
  doi = {10.1145/3388176.3388183},
  isbn = {978-1-4503-7725-6},
  langid = {english},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\storage\\Legal Judgement Prediction for UK Courts _Strickson_De La Iglesia 2020.pdf}
}

@article{sui2020joint,
  title = {Joint {{Entity}} and {{Relation Extraction}} with {{Set Prediction Networks}}},
  author = {Sui, Dianbo and Chen, Yubo and Liu, Kang and Zhao, Jun and Zeng, Xiangrong and Liu, Shengping},
  year = {2020},
  publisher = {{arXiv}},
  doi = {10.48550/ARXIV.2011.01675},
  abstract = {The joint entity and relation extraction task aims to extract all relational triples from a sentence. In essence, the relational triples contained in a sentence are unordered. However, previous seq2seq based models require to convert the set of triples into a sequence in the training phase. To break this bottleneck, we treat joint entity and relation extraction as a direct set prediction problem, so that the extraction model can get rid of the burden of predicting the order of multiple triples. To solve this set prediction problem, we propose networks featured by transformers with non-autoregressive parallel decoding. Unlike autoregressive approaches that generate triples one by one in a certain order, the proposed networks directly output the final set of triples in one shot. Furthermore, we also design a set-based loss that forces unique predictions via bipartite matching. Compared with cross-entropy loss that highly penalizes small shifts in triple order, the proposed bipartite matching loss is invariant to any permutation of predictions; thus, it can provide the proposed networks with a more accurate training signal by ignoring triple order and focusing on relation types and entities. Experiments on two benchmark datasets show that our proposed model significantly outperforms current state-of-the-art methods. Training code and trained models will be available at http://github.com/DianboWork/SPN4RE.},
  copyright = {arXiv.org perpetual, non-exclusive license},
  keywords = {Computation and Language (cs.CL),FOS: Computer and information sciences},
  file = {C\:\\Users\\Hytn\\Zotero\\storage\\ZIUUNN6N\\Joint Entity and Relation Extraction with Set Prediction Networks_Sui et al 2020.pdf}
}

@inproceedings{sundararajan2017axiomatic,
  title = {Axiomatic Attribution for Deep Networks},
  booktitle = {Proceedings of the 34th International Conference on Machine Learning},
  author = {Sundararajan, Mukund and Taly, Ankur and Yan, Qiqi},
  editor = {Precup, Doina and Teh, Yee Whye},
  year = {2017},
  month = aug,
  series = {Proceedings of Machine Learning Research},
  volume = {70},
  pages = {3319--3328},
  publisher = {{PMLR}},
  abstract = {We study the problem of attributing the prediction of a deep network to its input features, a problem previously studied by several other works. We identify two fundamental axioms\textemdash Sensitivity and Implementation Invariance that attribution methods ought to satisfy. We show that they are not satisfied by most known attribution methods, which we consider to be a fundamental weakness of those methods. We use the axioms to guide the design of a new attribution method called Integrated Gradients. Our method requires no modification to the original network and is extremely simple to implement; it just needs a few calls to the standard gradient operator. We apply this method to a couple of image models, a couple of text models and a chemistry model, demonstrating its ability to debug networks, to extract rules from a network, and to enable users to engage with models better.},
  pdf = {http://proceedings.mlr.press/v70/sundararajan17a/sundararajan17a.pdf},
  keywords = {IG,reference-based},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\storage\\Axiomatic attribution for deep networks _Sundararajan et al 2017.pdf}
}

@article{suterrobustly,
  title = {Robustly {{Disentangled Causal Mechanisms}}:  {{Validating Deep Representations}} for {{Interventional Robustness}}},
  author = {Suter, Raphael and Miladinovic, {\DH}ore and Sch{\"o}lkopf, Bernhard and Bauer, Stefan},
  pages = {10},
  abstract = {The ability to learn disentangled representations that split underlying sources of variation in high dimensional, unstructured data is important for data efficient and robust use of neural networks. While various approaches aiming towards this goal have been proposed in recent times, a commonly accepted definition and validation procedure is missing. We provide a causal perspective on representation learning which covers disentanglement and domain shift robustness as special cases. Our causal framework allows us to introduce a new metric for the quantitative evaluation of deep latent variable models. We show how this metric can be estimated from labeled observational data and further provide an efficient estimation algorithm that scales linearly in the dataset size.},
  langid = {english},
  keywords = {⛔ No DOI found},
  file = {C\:\\Users\\Hytn\\Zotero\\storage\\2MSGJI4U\\Suter et al. - Robustly Disentangled Causal Mechanisms  Validati.pdf}
}

@article{tan2021reliability,
  title = {Reliability {{Testing}} for {{Natural Language Processing Systems}}},
  author = {Tan, Samson and Joty, Shafiq and Baxter, Kathy and Taeihagh, Araz and Bennett, Gregory A. and Kan, Min-Yen},
  year = {2021},
  month = may,
  journal = {arXiv:2105.02590 [cs]},
  eprint = {2105.02590},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {Questions of fairness, robustness, and transparency are paramount to address before deploying NLP systems. Central to these concerns is the question of reliability: Can NLP systems reliably treat different demographics fairly and function correctly in diverse and noisy environments? To address this, we argue for the need for reliability testing and contextualize it among existing work on improving accountability. We show how adversarial attacks can be reframed for this goal, via a framework for developing reliability tests. We argue that reliability testing \textemdash{} with an emphasis on interdisciplinary collaboration \textemdash{} will enable rigorous and targeted testing, and aid in the enactment and enforcement of industry standards.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {⛔ No DOI found,Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Computers and Society,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing},
  file = {C\:\\Users\\Hytn\\Zotero\\storage\\PQPLMDQB\\Tan et al. - 2021 - Reliability Testing for Natural Language Processin.pdf}
}

@article{tan2022documentlevel,
  title = {Document-{{Level Relation Extraction}} with {{Adaptive Focal Loss}} and {{Knowledge Distillation}}},
  author = {Tan, Qingyu and He, Ruidan and Bing, Lidong and Ng, Hwee Tou},
  year = {2022},
  month = mar,
  journal = {arXiv:2203.10900 [cs]},
  eprint = {2203.10900},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {Document-level Relation Extraction (DocRE) is a more challenging task compared to its sentence-level counterpart. It aims to extract relations from multiple sentences at once. In this paper, we propose a semi-supervised framework for DocRE with three novel components. Firstly, we use an axial attention module for learning the interdependency among entity-pairs, which improves the performance on two-hop relations. Secondly, we propose an adaptive focal loss to tackle the class imbalance problem of DocRE. Lastly, we use knowledge distillation to overcome the differences between human annotated data and distantly supervised data. We conducted experiments on two DocRE datasets. Our model consistently outperforms strong baselines and its performance exceeds the previous SOTA by 1.36 F1 and 1.46 Ign\_F1 score on the DocRED leaderboard. Our code and data will be released at https://github.com/tonytan48/KD-DocRE.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {⛔ No DOI found,Computer Science - Computation and Language},
  file = {C\:\\Users\\Hytn\\Zotero\\storage\\8WG7CN73\\Tan 等。 - 2022 - Document-Level Relation Extraction with Adaptive F.pdf}
}

@inproceedings{tan2022documentlevela,
  title = {Document-{{Level Relation Extraction}} with {{Adaptive Focal Loss}} and {{Knowledge Distillation}}},
  booktitle = {Findings of the {{Association}} for {{Computational Linguistics}}: {{ACL}} 2022},
  author = {Tan, Qingyu and He, Ruidan and Bing, Lidong and Ng, Hwee Tou},
  year = {2022},
  pages = {1672--1681},
  publisher = {{Association for Computational Linguistics}},
  address = {{Dublin, Ireland}},
  doi = {10.18653/v1/2022.findings-acl.132},
  langid = {english},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\storage\\Document-Level Relation Extraction with Adaptive Focal Loss and Knowledge _Tan et al 2022.pdf}
}

@inproceedings{tang2020hin,
  title = {Hin: {{Hierarchical}} Inference Network for Document-Level Relation Extraction},
  booktitle = {Pacific-{{Asia Conference}} on {{Knowledge Discovery}} and {{Data Mining}}},
  author = {Tang, Hengzhu and Cao, Yanan and Zhang, Zhenyu and Cao, Jiangxia and Fang, Fang and Wang, Shi and Yin, Pengfei},
  year = {2020},
  pages = {197--209},
  doi = {10.1007/978-3-030-47426-3_16},
  file = {C\:\\Users\\Hytn\\Zotero\\storage\\T6PW3EFK\\Tang et al_2020_Hin.pdf}
}

@inproceedings{tang2020unbiased,
  title = {Unbiased {{Scene Graph Generation From Biased Training}}},
  booktitle = {2020 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Tang, Kaihua and Niu, Yulei and Huang, Jianqiang and Shi, Jiaxin and Zhang, Hanwang},
  year = {2020},
  month = jun,
  pages = {3713--3722},
  publisher = {{IEEE}},
  doi = {10.1109/CVPR42600.2020.00377},
  isbn = {978-1-72817-168-5},
  file = {C\:\\Users\\Hytn\\Zotero\\storage\\S3MG5DUN\\Tang et al_2020_Unbiased Scene Graph Generation From Biased Training.pdf}
}

@inproceedings{Teney_2021_ICCV,
  title = {Unshuffling Data for Improved Generalization in Visual Question Answering},
  booktitle = {Proceedings of the {{IEEE}}/{{CVF}} International Conference on Computer Vision ({{ICCV}})},
  author = {Teney, Damien and Abbasnejad, Ehsan and {van den Hengel}, Anton},
  year = {2021},
  month = oct,
  pages = {1417--1427},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\storage\\Unshuffling data for improved generalization in visual question answering _Teney et al 2021.pdf}
}

@inproceedings{tu2019multihop,
  title = {Multi-Hop {{Reading Comprehension}} across {{Multiple Documents}} by {{Reasoning}} over {{Heterogeneous Graphs}}.},
  booktitle = {Proceedings of the 57th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}}},
  author = {Tu, Ming and Wang, Guangtao and Huang, Jing and Tang, Yun and He, Xiaodong and Zhou, Bowen},
  year = {2019},
  pages = {2704--2713},
  doi = {10.18653/v1/P19-1260},
  file = {C\:\\Users\\Hytn\\Zotero\\storage\\IMD3FB53\\Tu et al_2019_Multi-hop Reading Comprehension across Multiple Documents by Reasoning over.pdf}
}

@article{tu2020empirical,
  title = {An {{Empirical Study}} on {{Robustness}} to {{Spurious Correlations}} Using {{Pre-trained Language Models}}},
  author = {Tu, Lifu and Lalwani, Garima and Gella, Spandana and He, He},
  year = {2020},
  month = dec,
  journal = {Transactions of the Association for Computational Linguistics},
  volume = {8},
  pages = {621--633},
  issn = {2307-387X},
  doi = {10.1162/tacl_a_00335},
  abstract = {Recent work has shown that pre-trained language models such as BERT improve robustness to spurious correlations in the dataset. Intrigued by these results, we find that the key to their success is generalization from a small amount of counterexamples where the spurious correlations do not hold. When such minority examples are scarce, pre-trained models perform as poorly as models trained from scratch. In the case of extreme minority, we propose to use multi-task learning (MTL) to improve generalization. Our experiments on natural language inference and paraphrase identification show that MTL with the right auxiliary tasks significantly improves performance on challenging examples without hurting the in-distribution performance. Further, we show that the gain from MTL mainly comes from improved generalization from the minority examples. Our results highlight the importance of data diversity for overcoming spurious correlations.               1},
  langid = {english},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\storage\\An Empirical Study on Robustness to Spurious Correlations using Pre-trained _Tu et al 2020.pdf}
}

@inproceedings{tuggener2020ledgar,
  title = {{{LEDGAR}}: {{A}} Large-Scale Multi-Label Corpus for Text Classification of Legal Provisions in Contracts},
  booktitle = {Proceedings of the 12th Language Resources and Evaluation Conference},
  author = {Tuggener, Don and {von D{\"a}niken}, Pius and Peetz, Thomas and Cieliebak, Mark},
  year = {2020},
  month = may,
  pages = {1235--1241},
  publisher = {{European Language Resources Association}},
  address = {{Marseille, France}},
  abstract = {We present LEDGAR, a multilabel corpus of legal provisions in contracts. The corpus was crawled and scraped from the public domain (SEC filings) and is, to the best of our knowledge, the first freely available corpus of its kind. Since the corpus was constructed semi-automatically, we apply and discuss various approaches to noise removal. Due to the rather large labelset of over 12'000 labels annotated in almost 100'000 provisions in over 60'000 contracts, we believe the corpus to be of interest for research in the field of Legal NLP, (large-scale or extreme) text classification, as well as for legal studies. We discuss several methods to sample subcopora from the corpus and implement and evaluate different automatic classification approaches. Finally, we perform transfer experiments to evaluate how well the classifiers perform on contracts stemming from outside the corpus.},
  isbn = {979-10-95546-34-4},
  langid = {english},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\storage\\LEDGAR _Tuggener et al 2020.pdf}
}

@inproceedings{vaswani2017attention,
  title = {Attention Is {{All}} You {{Need}}},
  booktitle = {Proceedings of the 31st {{International Conference}} on {{Neural Information Processing Systems}}},
  author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, Lukasz and Polosukhin, Illia},
  year = {2017},
  volume = {30},
  pages = {5998--6008},
  keywords = {⛔ No DOI found},
  file = {C\:\\Users\\Hytn\\Zotero\\storage\\G7RI6V7B\\Vaswani et al_2017_Attention is All you Need.pdf}
}

@article{veitch2021counterfactual,
  title = {Counterfactual Invariance to Spurious Correlations in Text Classification},
  author = {Veitch, Victor and D'Amour, Alexander and Yadlowsky, Steve and Eisenstein, Jacob},
  year = {2021},
  journal = {Advances in Neural Information Processing Systems},
  volume = {34},
  keywords = {⛔ No DOI found},
  file = {C\:\\Users\\Hytn\\Zotero\\storage\\WIY8SF6Y\\Veitch et al_2021_Counterfactual invariance to spurious correlations in text classification.pdf}
}

@article{velickovic2017graph,
  title = {Graph {{Attention Networks}}},
  author = {Veli{\v c}kovi{\'c}, Petar and Cucurull, Guillem and Casanova, Arantxa and Romero, Adriana and Li{\`o}, Pietro and Bengio, Yoshua},
  year = {2017},
  publisher = {{arXiv}},
  doi = {10.48550/ARXIV.1710.10903},
  abstract = {We present graph attention networks (GATs), novel neural network architectures that operate on graph-structured data, leveraging masked self-attentional layers to address the shortcomings of prior methods based on graph convolutions or their approximations. By stacking layers in which nodes are able to attend over their neighborhoods' features, we enable (implicitly) specifying different weights to different nodes in a neighborhood, without requiring any kind of costly matrix operation (such as inversion) or depending on knowing the graph structure upfront. In this way, we address several key challenges of spectral-based graph neural networks simultaneously, and make our model readily applicable to inductive as well as transductive problems. Our GAT models have achieved or matched state-of-the-art results across four established transductive and inductive graph benchmarks: the Cora, Citeseer and Pubmed citation network datasets, as well as a protein-protein interaction dataset (wherein test graphs remain unseen during training).},
  copyright = {arXiv.org perpetual, non-exclusive license},
  keywords = {Artificial Intelligence (cs.AI),FOS: Computer and information sciences,Machine Learning (cs.LG),Machine Learning (stat.ML),Social and Information Networks (cs.SI)},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\Graph Attention Networks _Veličković et al 2017.pdf}
}

@inproceedings{velickovic2018graph,
  title = {Graph {{Attention Networks}}},
  booktitle = {International {{Conference}} on {{Learning Representations}}},
  author = {Veli{\v c}kovi{\'c}, Petar and Cucurull, Guillem and Casanova, Arantxa and Romero, Adriana and Li{\`o}, Pietro and Bengio, Yoshua},
  year = {2018},
  keywords = {⛔ No DOI found}
}

@inproceedings{verga2018simultaneously,
  title = {Simultaneously {{Self-Attending}} to {{All Mentions}} for {{Full-Abstract Biological Relation Extraction}}},
  booktitle = {Proceedings of the 2018 {{Conference}} of the {{North American Chapter}} of the {{Association}} for {{Computational Linguistics}}: {{Human Language Technologies}}, {{Volume}} 1 ({{Long Papers}})},
  author = {Verga, Patrick and Strubell, Emma and McCallum, Andrew},
  year = {2018},
  volume = {1},
  pages = {872--884},
  doi = {10.18653/v1/N18-1080},
  file = {C\:\\Users\\Hytn\\Zotero\\storage\\SLQFPRE5\\Verga et al_2018_Simultaneously Self-Attending to All Mentions for Full-Abstract Biological.pdf}
}

@article{vig2020investigating,
  title = {Investigating Gender Bias in Language Models Using Causal Mediation Analysis},
  author = {Vig, Jesse and Gehrmann, Sebastian and Belinkov, Yonatan and Qian, Sharon and Nevo, Daniel and Singer, Yaron and Shieber, Stuart},
  year = {2020},
  journal = {Advances in Neural Information Processing Systems},
  volume = {33},
  pages = {12388--12401},
  keywords = {⛔ No DOI found},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\storage\\Investigating gender bias in language models using causal mediation analysis _Vig et al 2020.pdf}
}

@inproceedings{wallace2019allennlp,
  title = {{{AllenNLP Interpret}}: {{A Framework}} for {{Explaining Predictions}} of {{NLP Models}}},
  shorttitle = {{{AllenNLP Interpret}}},
  booktitle = {Proceedings of the 2019 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}} and the 9th {{International Joint Conference}} on {{Natural Language Processing}} ({{EMNLP-IJCNLP}}): {{System Demonstrations}}},
  author = {Wallace, Eric and Tuyls, Jens and Wang, Junlin and Subramanian, Sanjay and Gardner, Matt and Singh, Sameer},
  year = {2019},
  pages = {7--12},
  publisher = {{Association for Computational Linguistics}},
  address = {{Hong Kong, China}},
  doi = {10.18653/v1/D19-3002},
  langid = {english},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\storage\\AllenNLP Interpret _Wallace et al 2019.pdf}
}

@inproceedings{Wang_2021_ICCV,
  title = {Causal Attention for Unbiased Visual Recognition},
  booktitle = {Proceedings of the {{IEEE}}/{{CVF}} International Conference on Computer Vision ({{ICCV}})},
  author = {Wang, Tan and Zhou, Chang and Sun, Qianru and Zhang, Hanwang},
  year = {2021},
  month = oct,
  pages = {3091--3100},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\storage\\Causal attention for unbiased visual recognition _Wang et al 2021.pdf}
}

@inproceedings{wang2016relation,
  title = {Relation Classification via Multi-Level Attention Cnns},
  booktitle = {Proceedings of the 54th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}} ({{Volume}} 1: {{Long Papers}})},
  author = {Wang, Linlin and Cao, Zhu and De Melo, Gerard and Liu, Zhiyuan},
  year = {2016},
  pages = {1298--1307},
  doi = {10.18653/v1/P16-1123},
  file = {C\:\\Users\\Hytn\\Zotero\\storage\\K7RZUQSG\\Wang et al_2016_Relation classification via multi-level attention cnns.pdf}
}

@inproceedings{wang2018modeling,
  title = {Modeling {{Dynamic Pairwise Attention}} for {{Crime Classification}} over {{Legal Articles}}},
  booktitle = {The 41st {{International ACM SIGIR Conference}} on {{Research}} \& {{Development}} in {{Information Retrieval}}},
  author = {Wang, Pengfei and Yang, Ze and Niu, Shuzi and Zhang, Yongfeng and Zhang, Lei and Niu, ShaoZhang},
  year = {2018},
  month = jun,
  pages = {485--494},
  publisher = {{ACM}},
  address = {{Ann Arbor MI USA}},
  doi = {10.1145/3209978.3210057},
  isbn = {978-1-4503-5657-2},
  langid = {english},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\storage\\Modeling Dynamic Pairwise Attention for Crime Classification over Legal Articles _Wang et al 2018.pdf;C\:\\Users\\Hytn\\Zotero\\storage\\J4XI9P9E\\Wang et al. - 2018 - Modeling Dynamic Pairwise Attention for Crime Clas.pdf}
}

@inproceedings{wang2019extracting,
  title = {Extracting {{Multiple-Relations}} in {{One-Pass}} with {{Pre-Trained Transformers}}},
  booktitle = {Proceedings of the 57th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}}},
  author = {Wang, Haoyu and Tan, Ming and Yu, Mo and Chang, Shiyu and Wang, Dakuo and Xu, Kun and Guo, Xiaoxiao and Potdar, Saloni},
  year = {2019},
  pages = {1371--1377},
  doi = {10.18653/v1/P19-1132},
  file = {C\:\\Users\\Hytn\\Zotero\\storage\\UX2MD3D4\\Wang et al_2019_Extracting Multiple-Relations in One-Pass with Pre-Trained Transformers.pdf}
}

@article{wang2019finetune,
  title = {Fine-Tune {{Bert}} for {{DocRED}} with {{Two-step Process}}.},
  author = {Wang, Hong and Focke, Christfried and Sylvester, Rob and Mishra, Nilesh and Wang, William Yang},
  year = {2019},
  journal = {arXiv preprint arXiv:1909.11898},
  eprint = {1909.11898},
  eprinttype = {arxiv},
  archiveprefix = {arXiv},
  keywords = {⛔ No DOI found}
}

@article{wang2019improving,
  title = {Improving {{Natural Language Inference Using External Knowledge}} in the {{Science Questions Domain}}},
  author = {Wang, Xiaoyan and Kapanipathi, Pavan and Musa, Ryan and Yu, Mo and Talamadupula, Kartik and Abdelaziz, Ibrahim and Chang, Maria and Fokoue, Achille and Makni, Bassem and Mattei, Nicholas and Witbrock, Michael},
  year = {2019},
  month = jul,
  journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
  volume = {33},
  pages = {7208--7215},
  issn = {2374-3468, 2159-5399},
  doi = {10.1609/aaai.v33i01.33017208},
  abstract = {Natural Language Inference (NLI) is fundamental to many Natural Language Processing (NLP) applications including semantic search and question answering. The NLI problem has gained significant attention due to the release of large scale, challenging datasets. Present approaches to the problem largely focus on learning-based methods that use only textual information in order to classify whether a given premise entails, contradicts, or is neutral with respect to a given hypothesis. Surprisingly, the use of methods based on structured knowledge \textendash{} a central topic in artificial intelligence \textendash{} has not received much attention vis-a-vis the NLI problem. While there are many open knowledge bases that contain various types of reasoning information, their use for NLI has not been well explored. To address this, we present a combination of techniques that harness external knowledge to improve performance on the NLI problem in the science questions domain. We present the results of applying our techniques on text, graph, and text-and-graph based models; and discuss the implications of using external knowledge to solve the NLI problem. Our model achieves close to state-of-the-art performance for NLI on the SciTail science questions dataset.},
  file = {C\:\\Users\\Hytn\\Zotero\\storage\\PGJXVYAP\\Wang et al_2019_Improving Natural Language Inference Using External Knowledge in the Science.pdf}
}

@inproceedings{wang2020identifying,
  title = {Identifying {{Spurious Correlations}} for {{Robust Text Classification}}},
  booktitle = {Findings of the {{Association}} for {{Computational Linguistics}}: {{EMNLP}} 2020},
  author = {Wang, Zhao and Culotta, Aron},
  year = {2020},
  pages = {3431--3440},
  publisher = {{Association for Computational Linguistics}},
  address = {{Online}},
  doi = {10.18653/v1/2020.findings-emnlp.308},
  langid = {english},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\storage\\Identifying Spurious Correlations for Robust Text Classification _Wang_Culotta 2020.pdf}
}

@inproceedings{wang2020tplinker,
  title = {{{TPLinker}}: {{Single-stage Joint Extraction}} of {{Entities}} and {{Relations Through Token Pair Linking}}},
  booktitle = {Proceedings of the 28th {{International Conference}} on {{Computational Linguistics}}},
  author = {Wang, Yucheng and Yu, Bowen and Zhang, Yueyang and Liu, Tingwen and Zhu, Hongsong and Sun, Limin},
  year = {2020},
  pages = {1572--1582},
  doi = {10.18653/v1/2020.coling-main.138},
  file = {C\:\\Users\\Hytn\\Zotero\\storage\\3ZFLL7UR\\Wang et al_2020_TPLinker.pdf;C\:\\Users\\Hytn\\Zotero\\storage\\F9L7NQ9I\\Wang et al_2020_TPLinker.pdf}
}

@inproceedings{wang2020visual,
  title = {Visual {{Commonsense R-CNN}}},
  booktitle = {2020 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Wang, Tan and Huang, Jianqiang and Zhang, Hanwang and Sun, Qianru},
  year = {2020},
  month = jun,
  pages = {10757--10767},
  publisher = {{IEEE}},
  doi = {10.1109/CVPR42600.2020.01077},
  isbn = {978-1-72817-168-5},
  file = {C\:\\Users\\Hytn\\Zotero\\storage\\42UDIJWC\\Wang et al_2020_Visual Commonsense R-CNN.pdf}
}

@misc{wang2021equality,
  title = {Equality before the {{Law}}: {{Legal Judgment Consistency Analysis}} for {{Fairness}}},
  shorttitle = {Equality before the {{Law}}},
  author = {Wang, Yuzhong and Xiao, Chaojun and Ma, Shirong and Zhong, Haoxi and Tu, Cunchao and Zhang, Tianyang and Liu, Zhiyuan and Sun, Maosong},
  year = {2021},
  month = mar,
  number = {arXiv:2103.13868},
  eprint = {2103.13868},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  abstract = {In a legal system, judgment consistency is regarded as one of the most important manifestations of fairness. However, due to the complexity of factual elements that impact sentencing in real-world scenarios, few works have been done on quantitatively measuring judgment consistency towards real-world data. In this paper, we propose an evaluation metric for judgment inconsistency, Legal Inconsistency Coefficient (LInCo), which aims to evaluate inconsistency between data groups divided by specific features (e.g., gender, region, race). We propose to simulate judges from different groups with legal judgment prediction (LJP) models and measure the judicial inconsistency with the disagreement of the judgment results given by LJP models trained on different groups. Experimental results on the synthetic data verify the effectiveness of LInCo. We further employ LInCo to explore the inconsistency in real cases and come to the following observations: (1) Both regional and gender inconsistency exist in the legal system, but gender inconsistency is much less than regional inconsistency; (2) The level of regional inconsistency varies little across different time periods; (3) In general, judicial inconsistency is negatively correlated with the severity of the criminal charges. Besides, we use LInCo to evaluate the performance of several de-bias methods, such as adversarial learning, and find that these mechanisms can effectively help LJP models to avoid suffering from data bias.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {C\:\\Users\\Hytn\\Zotero\\storage\\JRYWZA8A\\Wang et al. - 2021 - Equality before the Law Legal Judgment Consistenc.pdf}
}

@inproceedings{wang2021ilg,
  title = {{{ILG}}:{{Inference}} Model Based on {{Line Graphs}} for Document-Level Relation Extraction},
  shorttitle = {{{ILG}}},
  booktitle = {2021 4th {{International Conference}} on {{Algorithms}}, {{Computing}} and {{Artificial Intelligence}}},
  author = {Wang, Caihong and Xu, Xinyue and Zhao, Sicong and Cao, Yang and Lyu, Naibing and Jia, Shuainan and Peng, Yuan and Li, Dongxue},
  year = {2021},
  month = dec,
  pages = {1--6},
  publisher = {{ACM}},
  address = {{Sanya China}},
  doi = {10.1145/3508546.3508633},
  isbn = {978-1-4503-8505-3},
  langid = {english},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\storage\\ILG _Wang et al 2021.pdf}
}

@article{wang2021kepler,
  title = {{{KEPLER}}: {{A Unified Model}} for {{Knowledge Embedding}} and {{Pre-trained Language Representation}}},
  author = {Wang, Xiaozhi and Gao, Tianyu and Zhu, Zhaocheng and Zhang, Zhengyan and Liu, Zhiyuan and Li, Juanzi and Tang, Jian},
  year = {2021},
  journal = {Transactions of the Association for Computational Linguistics},
  volume = {9},
  pages = {176--194},
  doi = {10.1162/tacl_a_00360},
  file = {C\:\\Users\\Hytn\\Zotero\\storage\\XY8AQBNB\\Wang et al_2021_KEPLER.pdf}
}

@inproceedings{wang2022causal,
  title = {Causal {{Representation Learning}} for {{Out-of-Distribution Recommendation}}},
  booktitle = {Proceedings of the {{ACM Web Conference}} 2022},
  author = {Wang, Wenjie and Lin, Xinyu and Feng, Fuli and He, Xiangnan and Lin, Min and Chua, Tat-Seng},
  year = {2022},
  month = apr,
  pages = {3562--3571},
  publisher = {{ACM}},
  address = {{Virtual Event, Lyon France}},
  doi = {10.1145/3485447.3512251},
  isbn = {978-1-4503-9096-5},
  langid = {english},
  file = {C\:\\Users\\Hytn\\Zotero\\storage\\I82YPJIE\\Causal Representation Learning for Out-of-Distribution Recommendation_Wang et al 2022.pdf}
}

@inproceedings{wang2022identifying,
  title = {Identifying and Mitigating Spurious Correlations for Improving Robustness in {{NLP}} Models},
  booktitle = {Findings of the Association for Computational Linguistics: {{NAACL}} 2022},
  author = {Wang, Tianlu and Sridhar, Rohit and Yang, Diyi and Wang, Xuezhi},
  year = {2022},
  month = jul,
  pages = {1719--1729},
  publisher = {{Association for Computational Linguistics}},
  address = {{Seattle, United States}},
  abstract = {Recently, NLP models have achieved remarkable progress across a variety of tasks; however, they have also been criticized for being not robust. Many robustness problems can be attributed to models exploiting ``spurious correlations'', or ``shortcuts'' between the training data and the task labels. Most existing work identifies a limited set of task-specific shortcuts via human priors or error analyses, which requires extensive expertise and efforts. In this paper, we aim to automatically identify such spurious correlations in NLP models at scale. We first leverage existing interpretability methods to extract tokens that significantly affect model's decision process from the input text. We then distinguish ``genuine'' tokens and ``spurious'' tokens by analyzing model predictions across multiple corpora and further verify them through knowledge-aware perturbations. We show that our proposed method can effectively and efficiently identify a scalable set of ``shortcuts'', and mitigating these leads to more robust models in multiple applications.},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\storage\\Identifying and mitigating spurious correlations for improving robustness in _Wang et al 2022.pdf}
}

@article{wang2022interpretable,
  title = {Interpretable Prison Term Prediction with Reinforce Learning and Attention},
  author = {Wang, Peipeng and Zhang, Xiuguo and Yu, Han and Cao, Zhiying},
  year = {2022},
  month = apr,
  journal = {Applied Intelligence},
  issn = {0924-669X, 1573-7497},
  doi = {10.1007/s10489-022-03675-1},
  abstract = {The task of prison term prediction is to predict the term of penalty based on the charge and the seriousness of the sentencing plot. Most existing methods focus on improving prediction accuracy but disregard interpretability, which yields unreliable judgment results. To address this problem, we propose an interpretable prison term prediction method. First, the prison term is divided into intervals according to the charge and sentencing plot. Second, we propose a reinforcement learning principle representation model combined with an attention mechanism for regression prediction (PRRP), which extracts phrase-level principles representation as the explanatory basis of prediction results, uses the principle in conjunction with the charge semantics to predict the interval value, and extracts the interval keywords as the sentencing plot. Third, we design a novel multiangle attention mechanism to capture the distinguishing features of cases from different aspects, and a feature fusion network is employed to more effectively stitch multiple pieces of information to learn the feature-enhanced fact representation. Last, the feature-enhanced fact representation is used to predict the prison term. Experimental results on real-work datasets show the interpretability and effectiveness of our method.},
  langid = {english},
  keywords = {CAIL},
  file = {C\:\\Users\\Hytn\\Zotero\\storage\\9K7URIWU\\Wang et al. - 2022 - Interpretable prison term prediction with reinforc.pdf}
}

@inproceedings{wang2022training,
  title = {Training {{Data}} Is {{More Valuable}} than {{You Think}}: {{A Simple}} and {{Effective Method}} by {{Retrieving}} from {{Training Data}}},
  shorttitle = {Training {{Data}} Is {{More Valuable}} than {{You Think}}},
  booktitle = {Proceedings of the 60th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}} ({{Volume}} 1: {{Long Papers}})},
  author = {Wang, Shuohang and Xu, Yichong and Fang, Yuwei and Liu, Yang and Sun, Siqi and Xu, Ruochen and Zhu, Chenguang and Zeng, Michael},
  year = {2022},
  pages = {3170--3179},
  publisher = {{Association for Computational Linguistics}},
  address = {{Dublin, Ireland}},
  doi = {10.18653/v1/2022.acl-long.226},
  langid = {english},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\storage\\Training Data is More Valuable than You Think _Wang et al 2022.pdf}
}

@misc{weber2022certifying,
  title = {Certifying {{Out-of-Domain Generalization}} for {{Blackbox Functions}}},
  author = {Weber, Maurice and Li, Linyi and Wang, Boxin and Zhao, Zhikuan and Li, Bo and Zhang, Ce},
  year = {2022},
  month = feb,
  number = {arXiv:2202.01679},
  eprint = {2202.01679},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  abstract = {Certifying the robustness of model performance under bounded data distribution shifts has recently attracted intensive interests under the umbrella of distributional robustness. However, existing techniques either make strong assumptions on the model class and loss functions that can be certified, such as smoothness expressed via Lipschitz continuity of gradients, or require to solve complex optimization problems. As a result, the wider application of these techniques is currently limited by its scalability and flexibility \textemdash{} these techniques often do not scale to large-scale datasets with modern deep neural networks or cannot handle loss functions which may be non-smooth, such as the 0-1 loss. In this paper, we focus on the problem of certifying distributional robustness for black box models and bounded losses, without other assumptions. We propose a novel certification framework given bounded distance of mean and variance of two distributions. Our certification technique scales to ImageNet-scale datasets, complex models, and a diverse range of loss functions. We then focus on one specific application enabled by such scalability and flexibility, i.e., certifying out-of-domain generalization for large neural networks and loss functions such as accuracy and AUC. We experimentally validate our certification method on a number of datasets, ranging from ImageNet, where we provide the first non-vacuous certified out-of-domain generalization, to smaller classification tasks where we are able to compare with the state-of-the-art and show that our method performs considerably better.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning},
  file = {C\:\\Users\\Hytn\\Zotero\\storage\\H7WJ48WA\\Weber et al. - 2022 - Certifying Out-of-Domain Generalization for Blackb.pdf}
}

@inproceedings{wei2020novel,
  title = {A {{Novel Cascade Binary Tagging Framework}} for {{Relational Triple Extraction}}},
  booktitle = {Proceedings of the 58th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}}},
  author = {Wei, Zhepei and Su, Jianlin and Wang, Yue and Tian, Yuan and Chang, Yi},
  year = {2020},
  pages = {1476--1488},
  doi = {10.18653/v1/2020.acl-main.136},
  file = {C\:\\Users\\Hytn\\Zotero\\storage\\9KQFQA56\\Wei et al_2020_A Novel Cascade Binary Tagging Framework for Relational Triple Extraction.pdf;C\:\\Users\\Hytn\\Zotero\\storage\\Z8IRCVPN\\Wei et al_2020_A Novel Cascade Binary Tagging Framework for Relational Triple Extraction.pdf}
}

@article{wei2021graphtosequence,
  title = {A {{Graph-to-Sequence Learning Framework}} for {{Summarizing Opinionated Texts}}},
  author = {Wei, Penghui and Zhao, Jiahao and Mao, Wenji},
  year = {2021},
  journal = {IEEE/ACM Transactions on Audio, Speech, and Language Processing},
  volume = {29},
  pages = {1650--1660},
  issn = {2329-9290, 2329-9304},
  doi = {10.1109/TASLP.2021.3071667},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\storage\\A Graph-to-Sequence Learning Framework for Summarizing Opinionated Texts _Wei et al 2021.pdf}
}

@inproceedings{wolf2020transformers,
  title = {Transformers: {{State-of-the-Art Natural Language Processing}}},
  booktitle = {Proceedings of the 2020 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}}: {{System Demonstrations}}},
  author = {Wolf, Thomas and Debut, Lysandre and Sanh, Victor and Chaumond, Julien and Delangue, Clement and Moi, Anthony and Cistac, Pierric and Rault, Tim and Louf, R{\'e}mi and Funtowicz, Morgan and Davison, Joe and Shleifer, Sam and von Platen, Patrick and Ma, Clara and Jernite, Yacine and Plu, Julien and Xu, Canwen and Scao, Teven Le and Gugger, Sylvain and Drame, Mariama and Lhoest, Quentin and Rush, Alexander M.},
  year = {2020},
  month = oct,
  pages = {38--45},
  publisher = {{Association for Computational Linguistics}},
  file = {C\:\\Users\\Hytn\\Zotero\\storage\\M7XSFVX5\\Wolf et al_2020_Transformers.pdf}
}

@inproceedings{wolf2020transformersa,
  title = {Transformers: {{State-of-the-Art Natural Language Processing}}},
  shorttitle = {Transformers},
  booktitle = {Proceedings of the 2020 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}}: {{System Demonstrations}}},
  author = {Wolf, Thomas and Debut, Lysandre and Sanh, Victor and Chaumond, Julien and Delangue, Clement and Moi, Anthony and Cistac, Pierric and Rault, Tim and Louf, Remi and Funtowicz, Morgan and Davison, Joe and Shleifer, Sam and {von Platen}, Patrick and Ma, Clara and Jernite, Yacine and Plu, Julien and Xu, Canwen and Le Scao, Teven and Gugger, Sylvain and Drame, Mariama and Lhoest, Quentin and Rush, Alexander},
  year = {2020},
  pages = {38--45},
  publisher = {{Association for Computational Linguistics}},
  address = {{Online}},
  doi = {10.18653/v1/2020.emnlp-demos.6},
  langid = {english},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\storage\\Transformers _Wolf et al 2020.pdf}
}

@inproceedings{wu2019renet,
  title = {Renet: {{A}} Deep Learning Approach for Extracting Gene-Disease Associations from Literature},
  booktitle = {International {{Conference}} on {{Research}} in {{Computational Molecular Biology}}},
  author = {Wu, Ye and Luo, Ruibang and Leung, Henry C. M. and Ting, Hing-Fung and Lam, Tak-Wah},
  year = {2019},
  pages = {272--284},
  keywords = {⛔ No DOI found}
}

@inproceedings{wu2022discovering,
  title = {Discovering Invariant Rationales for Graph Neural Networks},
  booktitle = {International Conference on Learning Representations},
  author = {Wu, Yingxin and Wang, Xiang and Zhang, An and He, Xiangnan and Chua, Tat-Seng},
  year = {2022},
  keywords = {⛔ No DOI found},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\storage\\Discovering invariant rationales for graph neural networks _Wu et al 2022.pdf}
}

@inproceedings{wu2022generating,
  title = {Generating {{Data}} to {{Mitigate Spurious Correlations}} in {{Natural Language Inference Datasets}}},
  booktitle = {Proceedings of the 60th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}} ({{Volume}} 1: {{Long Papers}})},
  author = {Wu, Yuxiang and Gardner, Matt and Stenetorp, Pontus and Dasigi, Pradeep},
  year = {2022},
  pages = {2660--2676},
  publisher = {{Association for Computational Linguistics}},
  address = {{Dublin, Ireland}},
  doi = {10.18653/v1/2022.acl-long.190},
  langid = {english},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\storage\\Generating Data to Mitigate Spurious Correlations in Natural Language Inference _Wu et al 2022.pdf}
}

@article{wu2022learning,
  title = {Learning {{Decomposed Representations}} for {{Treatment Effect Estimation}}},
  author = {Wu, Anpeng and Yuan, Junkun and Kuang, Kun and Li, Bo and Wu, Runze and Zhu, Qiang and Zhuang, Yue Ting and Wu, Fei},
  year = {2022},
  journal = {IEEE Transactions on Knowledge and Data Engineering},
  pages = {1--1},
  issn = {1041-4347, 1558-2191, 2326-3865},
  doi = {10.1109/TKDE.2022.3150807},
  keywords = {待读},
  file = {C\:\\Users\\Hytn\\Zotero\\storage\\2VM9K3HU\\Learning Decomposed Representations for Treatment Effect Estimation_Wu et al 2022.pdf}
}

@inproceedings{xiao2016semantic,
  title = {Semantic Relation Classification via Hierarchical Recurrent Neural Network with Attention},
  booktitle = {Proceedings of {{COLING}} 2016, the 26th {{International Conference}} on {{Computational Linguistics}}: {{Technical Papers}}},
  author = {Xiao, Minguang and Liu, Cong},
  year = {2016},
  pages = {1254--1263},
  keywords = {⛔ No DOI found}
}

@misc{xiao2018cail2018,
  title = {{{CAIL2018}}: {{A Large-Scale Legal Dataset}} for {{Judgment Prediction}}},
  shorttitle = {{{CAIL2018}}},
  author = {Xiao, Chaojun and Zhong, Haoxi and Guo, Zhipeng and Tu, Cunchao and Liu, Zhiyuan and Sun, Maosong and Feng, Yansong and Han, Xianpei and Hu, Zhen and Wang, Heng and Xu, Jianfeng},
  year = {2018},
  month = jul,
  number = {arXiv:1807.02478},
  eprint = {1807.02478},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  abstract = {In this paper, we introduce the Chinese AI and Law challenge dataset (CAIL2018), the first large-scale Chinese legal dataset for judgment prediction. CAIL2018 contains more than 2.6 million criminal cases published by the Supreme People's Court of China, which are several times larger than other datasets in existing works on judgment prediction. Moreover, the annotations of judgment results are more detailed and rich. It consists of applicable law articles, charges, and prison terms, which are expected to be inferred according to the fact descriptions of cases. For comparison, we implement several conventional text classification baselines for judgment prediction and experimental results show that it is still a challenge for current models to predict the judgment results of legal cases, especially on prison terms. To help the researchers make improvements on legal judgment prediction, both CAIL2018 and baselines will be released after the CAIL competition1.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computation and Language},
  file = {C\:\\Users\\Hytn\\Zotero\\storage\\G4GUPHXH\\Xiao et al. - 2018 - CAIL2018 A Large-Scale Legal Dataset for Judgment.pdf}
}

@inproceedings{xiao2022sais,
  title = {{{SAIS}}: {{Supervising}} and Augmenting Intermediate Steps for Document-Level Relation Extraction},
  booktitle = {Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: {{Human}} Language Technologies},
  author = {Xiao, Yuxin and Zhang, Zecheng and Mao, Yuning and Yang, Carl and Han, Jiawei},
  year = {2022},
  month = jul,
  pages = {2395--2409},
  publisher = {{Association for Computational Linguistics}},
  address = {{Seattle, United States}},
  abstract = {Stepping from sentence-level to document-level, the research on relation extraction (RE) confronts increasing text length and more complicated entity interactions. Consequently, it is more challenging to encode the key information sources\textemdash relevant contexts and entity types. However, existing methods only implicitly learn to model these critical information sources while being trained for RE. As a result, they suffer the problems of ineffective supervision and uninterpretable model predictions. In contrast, we propose to explicitly teach the model to capture relevant contexts and entity types by supervising and augmenting intermediate steps (SAIS) for RE. Based on a broad spectrum of carefully designed tasks, our proposed SAIS method not only extracts relations of better quality due to more effective supervision, but also retrieves the corresponding supporting evidence more accurately so as to enhance interpretability. By assessing model uncertainty, SAIS further boosts the performance via evidence-based data augmentation and ensemble inference while reducing the computational cost. Eventually, SAIS delivers state-of-the-art RE results on three benchmarks (DocRED, CDR, and GDA) and outperforms the runner-up by 5.04\% relatively in F1 score in evidence retrieval on DocRED.},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\storage\\SAIS _Xiao et al 2022.pdf}
}

@inproceedings{xie2022eider,
  title = {Eider: {{Empowering Document-level Relation Extraction}} with {{Efficient Evidence Extraction}} and {{Inference-stage Fusion}}},
  shorttitle = {Eider},
  booktitle = {Findings of the {{Association}} for {{Computational Linguistics}}: {{ACL}} 2022},
  author = {Xie, Yiqing and Shen, Jiaming and Li, Sha and Mao, Yuning and Han, Jiawei},
  year = {2022},
  pages = {257--268},
  publisher = {{Association for Computational Linguistics}},
  address = {{Dublin, Ireland}},
  doi = {10.18653/v1/2022.findings-acl.23},
  langid = {english},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\storage\\Eider _Xie et al 2022.pdf}
}

@inproceedings{xu2015classifying,
  title = {Classifying Relations via Long Short Term Memory Networks along Shortest Dependency Paths},
  booktitle = {Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing},
  author = {Xu, Yan and Mou, Lili and Li, Ge and Chen, Yunchuan and Peng, Hao and Jin, Zhi},
  year = {2015},
  pages = {1785--1794},
  doi = {10.18653/v1/D15-1206},
  file = {C\:\\Users\\Hytn\\Zotero\\storage\\G7UW8L37\\Xu et al_2015_Classifying relations via long short term memory networks along shortest.pdf}
}

@inproceedings{xu2018sequence,
  title = {Sequence {{Generative Adversarial Network}} for {{Long Text Summarization}}},
  booktitle = {2018 {{IEEE}} 30th {{International Conference}} on {{Tools}} with {{Artificial Intelligence}} ({{ICTAI}})},
  author = {Xu, Hao and Cao, Yanan and Jia, Ruipeng and Liu, Yanbing and Tan, Jianlong},
  year = {2018},
  month = nov,
  pages = {242--248},
  publisher = {{IEEE}},
  address = {{Volos, Greece}},
  doi = {10.1109/ICTAI.2018.00045},
  isbn = {978-1-5386-7449-9},
  file = {C\:\\Users\\Hytn\\Zotero\\storage\\AGGWGTHC\\Sequence Generative Adversarial Network for Long Text Summarization_Xu et al 2018.pdf}
}

@article{xu2021dissecting,
  title = {Dissecting {{Generation Modes}} for {{Abstractive Summarization Models}} via {{Ablation}} and {{Attribution}}},
  author = {Xu, Jiacheng and Durrett, Greg},
  year = {2021},
  month = jun,
  journal = {arXiv:2106.01518 [cs]},
  eprint = {2106.01518},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {Despite the prominence of neural abstractive summarization models, we know little about how they actually form summaries and how to understand where their decisions come from. We propose a two-step method to interpret summarization model decisions. We first analyze the model's behavior by ablating the full model to categorize each decoder decision into one of several generation modes: roughly, is the model behaving like a language model, is it relying heavily on the input, or is it somewhere in between? After isolating decisions that do depend on the input, we explore interpreting these decisions using several different attribution methods. We compare these techniques based on their ability to select content and reconstruct the model's predicted token from perturbations of the input, thus revealing whether highlighted attributions are truly important for the generation of the next token. While this machinery can be broadly useful even beyond summarization, we specifically demonstrate its capability to identify phrases the summarization model has memorized and determine where in the training pipeline this memorization happened, as well as study complex generation phenomena like sentence fusion on a per-instance basis.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {⛔ No DOI found,Computer Science - Computation and Language},
  file = {C\:\\Users\\Hytn\\Zotero\\storage\\623LGRLB\\Xu and Durrett - 2021 - Dissecting Generation Modes for Abstractive Summar.pdf}
}

@inproceedings{xu2021entity,
  title = {Entity {{Structure Within}} and {{Throughout}}: {{Modeling Mention Dependencies}} for {{Document-Level Relation Extraction}}.},
  booktitle = {{{AAAI}}},
  author = {Xu, Benfeng and Wang, Quan and Lyu, Yajuan and Zhu, Yong and Mao, Zhendong},
  year = {2021},
  pages = {14149--14157},
  keywords = {⛔ No DOI found},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\storage\\Entity Structure Within and Throughout _Xu et al 2021.pdf}
}

@article{yan2021partition,
  title = {A {{Partition Filter Network}} for {{Joint Entity}} and {{Relation Extraction}}},
  author = {Yan, Zhiheng and Zhang, Chong and Fu, Jinlan and Zhang, Qi and Wei, Zhongyu},
  year = {2021},
  month = sep,
  journal = {arXiv:2108.12202 [cs]},
  eprint = {2108.12202},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {In joint entity and relation extraction, existing work either sequentially encode task-specific features, leading to an imbalance in inter-task feature interaction where features extracted later have no direct contact with those that come first. Or they encode entity features and relation features in a parallel manner, meaning that feature representation learning for each task is largely independent of each other except for input sharing. We propose a partition filter network to model two-way interaction between tasks properly, where feature encoding is decomposed into two steps: partition and filter. In our encoder, we leverage two gates: entity and relation gate, to segment neurons into two task partitions and one shared partition. The shared partition represents inter-task information valuable to both tasks and is evenly shared across two tasks to ensure proper two-way interaction. The task partitions represent intra-task information and are formed through concerted efforts of both gates, making sure that encoding of taskspecific features is dependent upon each other. Experiment results on six public datasets show that our model performs significantly better than previous approaches. In addition, contrary to what previous work has claimed, our auxiliary experiments suggest that relation prediction is contributory to named entity prediction in a non-negligible way. The source code can be found at https://github.com/ Coopercoppers/PFN.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {⛔ No DOI found,Computer Science - Computation and Language},
  file = {C\:\\Users\\Hytn\\Zotero\\storage\\XLGNDE4X\\Yan et al. - 2021 - A Partition Filter Network for Joint Entity and Re.pdf}
}

@inproceedings{yang2019contextaware,
  title = {Context-Aware Self-Attention Networks},
  booktitle = {Proceedings of the {{AAAI Conference}} on {{Artificial Intelligence}}},
  author = {Yang, Baosong and Li, Jian and Wong, Derek F. and Chao, Lidia S. and Wang, Xing and Tu, Zhaopeng},
  year = {2019},
  volume = {33},
  pages = {387--394},
  keywords = {⛔ No DOI found}
}

@article{yang2019xlnet,
  title = {Xlnet: {{Generalized}} Autoregressive Pretraining for Language Understanding},
  author = {Yang, Zhilin and Dai, Zihang and Yang, Yiming and Carbonell, Jaime and Salakhutdinov, Russ R and Le, Quoc V},
  year = {2019},
  journal = {Advances in neural information processing systems},
  volume = {32},
  keywords = {⛔ No DOI found}
}

@inproceedings{yang2021causal,
  title = {Causal {{Attention}} for {{Vision-Language Tasks}}},
  booktitle = {2021 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Yang, Xu and Zhang, Hanwang and Qi, Guojun and Cai, Jianfei},
  year = {2021},
  month = jun,
  pages = {9842--9852},
  publisher = {{IEEE}},
  doi = {10.1109/CVPR46437.2021.00972},
  isbn = {978-1-66544-509-2},
  file = {C\:\\Users\\Hytn\\Zotero\\storage\\DG3UPPH9\\Yang et al_2021_Causal Attention for Vision-Language Tasks.pdf}
}

@misc{yang2021endtoend,
  title = {End-to-End {{Robustness}} for {{Sensing-Reasoning Machine Learning Pipelines}}},
  author = {Yang, Zhuolin and Zhao, Zhikuan and Pei, Hengzhi and Wang, Boxin and Karlas, Bojan and Liu, Ji and Guo, Heng and Li, Bo and Zhang, Ce},
  year = {2021},
  month = aug,
  number = {arXiv:2003.00120},
  eprint = {2003.00120},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  abstract = {As machine learning (ML) has been widely applied to safety-critical scenarios, certifying ML model robustness becomes increasingly important. Many previous studies focus on the robustness of independent ML and ensemble models, and can only certify a relatively small magnitude of p norm bounded adversarial perturbation. In this paper, we take a different viewpoint and aim to improve ML robustness by going beyond independent ML and ensemble models. We propose a generic Sensing-Reasoning machine learning pipeline which contains both the sensing (e.g. deep neural networks) and reasoning (e.g. Markov logic networks (MLN)) components enriched with domain knowledge, and ask questions: Can domain knowledge help improve learning robustness? Can we formally certify the end-to-end robustness of such an ML pipeline? Towards the end-to-end certified robustness of Sensing-Reasoning Pipeline, there are already existing studies on certifying the robustness for deep neural networks, which can be applied to certifying the sensing component. Thus, in this paper we will focus on the certified robustness for the reasoning component. In particular, we first theoretically analyze the computational complexity of certifying the robustness for the reasoning component. We then derive the certified robustness bound for several structures of reasoning components, such as Markov logic networks and Bayesian networks. We show that for reasoning components such as MLN and a specific family of Bayesian networks it is possible to certify the robustness of the whole pipeline even against a large magnitude of perturbation. We also prove that the certified robustness bound for Bayesian networks is tight. Finally, we conduct extensive experiments on both image and natural language datasets to evaluate the certified robustness of the Sensing-Reasoning ML pipeline. We show that based on the hierarchical knowledge of the Primate family (i.e. PrimateNet), the certified robustness of the SensingReasoning Pipeline can achieve over 50\%, even when 50\% sensors are attacked under adversarial perturbation with L2 norm bounded by 0.5. We also evaluate Sensing-Reasoning Pipeline on a relation extraction task with Stock News dataset, which shows that even when all sensors are attacked, under adversarial attack with confidence bounded by 0.5, the certified robustness can reach almost 100\%.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Cryptography and Security,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C\:\\Users\\Hytn\\Zotero\\storage\\B4N8ZS7V\\Yang et al. - 2021 - End-to-end Robustness for Sensing-Reasoning Machin.pdf}
}

@inproceedings{yao2019docred,
  title = {{{DocRED}}: {{A Large-Scale Document-Level Relation Extraction Dataset}}.},
  booktitle = {Proceedings of the 57th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}}},
  author = {Yao, Yuan and Ye, Deming and Li, Peng and Han, Xu and Lin, Yankai and Liu, Zhenghao and Liu, Zhiyuan and Huang, Lixin and Zhou, Jie and Sun, Maosong},
  year = {2019},
  pages = {764--777},
  doi = {10.18653/v1/P19-1074},
  file = {C\:\\Users\\Hytn\\Zotero\\storage\\KDIJBNJA\\Yao et al_2019_DocRED.pdf}
}

@inproceedings{yao2020heterogeneous,
  title = {Heterogeneous {{Graph Transformer}} for {{Graph-to-Sequence Learning}}},
  booktitle = {Proceedings of the 58th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}}},
  author = {Yao, Shaowei and Wang, Tianming and Wan, Xiaojun},
  year = {2020},
  pages = {7145--7154},
  publisher = {{Association for Computational Linguistics}},
  address = {{Online}},
  doi = {10.18653/v1/2020.acl-main.640},
  langid = {english},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\storage\\Heterogeneous Graph Transformer for Graph-to-Sequence Learning _Yao et al 2020.pdf}
}

@inproceedings{yao2021codred,
  title = {{{CodRED}}: {{A Cross-Document Relation Extraction Dataset}} for {{Acquiring Knowledge}} in the {{Wild}}},
  shorttitle = {{{CodRED}}},
  booktitle = {Proceedings of the 2021 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}}},
  author = {Yao, Yuan and Du, Jiaju and Lin, Yankai and Li, Peng and Liu, Zhiyuan and Zhou, Jie and Sun, Maosong},
  year = {2021},
  pages = {4452--4472},
  publisher = {{Association for Computational Linguistics}},
  address = {{Online and Punta Cana, Dominican Republic}},
  doi = {10.18653/v1/2021.emnlp-main.366},
  langid = {english},
  file = {C\:\\Users\\Hytn\\Zotero\\storage\\WT2JAJBV\\CodRED_Yao et al 2021.pdf}
}

@inproceedings{ye2020coreferential,
  title = {Coreferential {{Reasoning Learning}} for {{Language Representation}}},
  booktitle = {Proceedings of the 2020 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}} ({{EMNLP}})},
  author = {Ye, Deming and Lin, Yankai and Du, Jiaju and Liu, Zhenghao and Li, Peng and Sun, Maosong and Liu, Zhiyuan},
  year = {2020},
  pages = {7170--7186},
  doi = {10.18653/v1/2020.emnlp-main.582},
  file = {C\:\\Users\\Hytn\\Zotero\\storage\\38CLTEKQ\\Ye et al_2020_Coreferential Reasoning Learning for Language Representation.pdf}
}

@inproceedings{ye2021contrastive,
  title = {Contrastive Triple Extraction with Generative Transformer},
  booktitle = {Proceedings of the {{AAAI}} Conference on Artificial Intelligence},
  author = {Ye, Hongbin and Zhang, Ningyu and Deng, Shumin and Chen, Mosha and Tan, Chuanqi and Huang, Fei and Chen, Huajun},
  year = {2021},
  volume = {35},
  pages = {14257--14265},
  keywords = {⛔ No DOI found},
  file = {C\:\\Users\\Hytn\\Zotero\\storage\\VUUGYFUT\\Ye 等。 - Contrastive Triple Extraction with Generative Tran.pdf}
}

@article{ye2021more,
  title = {Towards {{More Fine-grained}} and {{Reliable NLP Performance Prediction}}},
  author = {Ye, Zihuiwen and Liu, Pengfei and Fu, Jinlan and Neubig, Graham},
  year = {2021},
  month = feb,
  abstract = {Performance prediction, the task of estimating a system's performance without performing experiments, allows us to reduce the experimental burden caused by the combinatorial explosion of different datasets, languages, tasks, and models. In this paper, we make two contributions to improving performance prediction for NLP tasks. First, we examine performance predictors not only for holistic measures of accuracy like F1 or BLEU but also fine-grained performance measures such as accuracy over individual classes of examples. Second, we propose methods to understand the reliability of a performance prediction model from two angles: confidence intervals and calibration. We perform an analysis of four types of NLP tasks, and both demonstrate the feasibility of fine-grained performance prediction and the necessity to perform reliability analysis for performance prediction methods in the future. We make our code publicly available: \{https://github.com/neulab/Reliable-NLPPP\}},
  keywords = {⛔ No DOI found},
  file = {C\:\\Users\\Hytn\\Zotero\\storage\\TU6GKNQC\\Ye et al_2021_Towards More Fine-grained and Reliable NLP Performance Prediction.pdf}
}

@inproceedings{ye2022packed,
  title = {Packed {{Levitated Marker}} for {{Entity}} and {{Relation Extraction}}},
  booktitle = {Proceedings of the 60th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}} ({{Volume}} 1: {{Long Papers}})},
  author = {Ye, Deming and Lin, Yankai and Li, Peng and Sun, Maosong},
  year = {2022},
  pages = {4904--4917},
  publisher = {{Association for Computational Linguistics}},
  address = {{Dublin, Ireland}},
  doi = {10.18653/v1/2022.acl-long.337},
  langid = {english},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\storage\\Packed Levitated Marker for Entity and Relation Extraction _Ye et al 2022.pdf}
}

@book{yin2019towards,
  title = {Towards More Scalable and Robust Machine Learning},
  author = {Yin, Dong},
  year = {2019},
  publisher = {{University of California, Berkeley}},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\storage\\Towards more scalable and robust machine learning _Yin 2019.pdf}
}

@inproceedings{yin2022sensitivity,
  title = {On the {{Sensitivity}} and {{Stability}} of {{Model Interpretations}} in {{NLP}}},
  booktitle = {Proceedings of the 60th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}} ({{Volume}} 1: {{Long Papers}})},
  author = {Yin, Fan and Shi, Zhouxing and Hsieh, Cho-Jui and Chang, Kai-Wei},
  year = {2022},
  pages = {2631--2647},
  publisher = {{Association for Computational Linguistics}},
  address = {{Dublin, Ireland}},
  doi = {10.18653/v1/2022.acl-long.188},
  langid = {english},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\storage\\On the Sensitivity and Stability of Model Interpretations in NLP _Yin et al 2022.pdf}
}

@inproceedings{yu2010jointly,
  title = {Jointly Identifying Entities and Extracting Relations in Encyclopedia Text via a Graphical Model Approach},
  booktitle = {Coling 2010: {{Posters}}},
  author = {Yu, Xiaofeng and Lam, Wai},
  year = {2010},
  pages = {1399--1407},
  keywords = {⛔ No DOI found}
}

@article{yu2017seqgan,
  title = {{{SeqGAN}}: {{Sequence Generative Adversarial Nets}} with {{Policy Gradient}}},
  shorttitle = {{{SeqGAN}}},
  author = {Yu, Lantao and Zhang, Weinan and Wang, Jun and Yu, Yong},
  year = {2017},
  month = feb,
  journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
  volume = {31},
  number = {1},
  issn = {2374-3468, 2159-5399},
  doi = {10.1609/aaai.v31i1.10804},
  abstract = {As a new way of training generative models, Generative Adversarial Net (GAN) that uses a discriminative model to guide the training of the generative model has enjoyed considerable success in generating real-valued data. However, it has limitations when the goal is for generating sequences of discrete tokens. A major reason lies in that the discrete outputs from the generative model make it difficult to pass the gradient update from the discriminative model to the generative model. Also, the discriminative model can only assess a complete sequence, while for a partially generated sequence, it is non-trivial to balance its current score and the future one once the entire sequence has been generated. In this paper, we propose a sequence generation framework, called SeqGAN, to solve the problems. Modeling the data generator as a stochastic policy in reinforcement learning (RL), SeqGAN bypasses the generator differentiation problem by directly performing gradient policy update. The RL reward signal comes from the GAN discriminator judged on a complete sequence, and is passed back to the intermediate state-action steps using Monte Carlo search. Extensive experiments on synthetic data and real-world tasks demonstrate significant improvements over strong baselines.},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\storage\\SeqGAN _Yu et al 2017.pdf}
}

@article{yu2019joint,
  title = {Joint Extraction of Entities and Relations Based on a Novel Decomposition Strategy},
  author = {Yu, Bowen and Zhang, Zhenyu and Su, Jianlin},
  year = {2019},
  journal = {arXiv preprint arXiv:1909.04273},
  eprint = {1909.04273},
  eprinttype = {arxiv},
  archiveprefix = {arXiv},
  keywords = {⛔ No DOI found}
}

@article{yuan2022auto,
  title = {Auto {{IV}}: {{Counterfactual Prediction}} via {{Automatic Instrumental Variable Decomposition}}},
  shorttitle = {Auto {{IV}}},
  author = {Yuan, Junkun and Wu, Anpeng and Kuang, Kun and Li, Bo and Wu, Runze and Wu, Fei and Lin, Lanfen},
  year = {2022},
  month = aug,
  journal = {ACM Transactions on Knowledge Discovery from Data},
  volume = {16},
  number = {4},
  pages = {1--20},
  issn = {1556-4681, 1556-472X},
  doi = {10.1145/3494568},
  abstract = {Instrumental variables (IVs), sources of treatment randomization that are conditionally independent of the outcome, play an important role in causal inference with unobserved confounders. However, the existing IV-based counterfactual prediction methods need well-predefined IVs, while it's an art rather than science to find valid IVs in many real-world scenes. Moreover, the predefined hand-made IVs could be weak or erroneous by violating the conditions of valid IVs. These thorny facts hinder the application of the IV-based counterfactual prediction methods. In this article, we propose a novel Automatic Instrumental Variable decomposition (AutoIV) algorithm to automatically generate representations serving the role of IVs from observed variables (IV candidates). Specifically, we let the learned IV representations satisfy the relevance condition with the treatment and exclusion condition with the outcome via mutual information maximization and minimization constraints, respectively. We also learn confounder representations by encouraging them to be relevant to both the treatment and the outcome. The IV and confounder representations compete for the information with their constraints in an adversarial game, which allows us to get valid IV representations for IV-based counterfactual prediction. Extensive experiments demonstrate that our method generates valid IV representations for accurate IV-based counterfactual prediction.},
  langid = {english},
  file = {C\:\\Users\\Hytn\\Zotero\\storage\\AV83MB8G\\Auto IV_Yuan et al 2022.pdf}
}

@inproceedings{zaheer2020big,
  title = {Big Bird: {{Transformers}} for Longer Sequences},
  booktitle = {Advances in Neural Information Processing Systems},
  author = {Zaheer, Manzil and Guruganesh, Guru and Dubey, Kumar Avinava and Ainslie, Joshua and Alberti, Chris and Ontanon, Santiago and Pham, Philip and Ravula, Anirudh and Wang, Qifan and Yang, Li and Ahmed, Amr},
  editor = {Larochelle, H. and Ranzato, M. and Hadsell, R. and Balcan, M.F. and Lin, H.},
  year = {2020},
  volume = {33},
  pages = {17283--17297},
  publisher = {{Curran Associates, Inc.}},
  keywords = {⛔ No DOI found}
}

@inproceedings{zeiler2014visualizing,
  title = {Visualizing and Understanding Convolutional Networks},
  booktitle = {European Conference on Computer Vision},
  author = {Zeiler, Matthew D and Fergus, Rob},
  year = {2014},
  pages = {818--833},
  organization = {{Springer}},
  keywords = {⛔ No DOI found}
}

@incollection{zeiler2014visualizinga,
  title = {Visualizing and {{Understanding Convolutional Networks}}},
  booktitle = {Computer {{Vision}} \textendash{} {{ECCV}} 2014},
  author = {Zeiler, Matthew D. and Fergus, Rob},
  editor = {Fleet, David and Pajdla, Tomas and Schiele, Bernt and Tuytelaars, Tinne},
  year = {2014},
  volume = {8689},
  pages = {818--833},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-319-10590-1_53},
  abstract = {Large Convolutional Network models have recently demonstrated impressive classification performance on the ImageNet benchmark Krizhevsky et al. [18]. However there is no clear understanding of why they perform so well, or how they might be improved. In this paper we explore both issues. We introduce a novel visualization technique that gives insight into the function of intermediate feature layers and the operation of the classifier. Used in a diagnostic role, these visualizations allow us to find model architectures that outperform Krizhevsky et al. on the ImageNet classification benchmark. We also perform an ablation study to discover the performance contribution from different model layers. We show our ImageNet model generalizes well to other datasets: when the softmax classifier is retrained, it convincingly beats the current state-ofthe-art results on Caltech-101 and Caltech-256 datasets.},
  isbn = {978-3-319-10589-5 978-3-319-10590-1},
  langid = {english},
  file = {C\:\\Users\\Hytn\\Zotero\\storage\\H7NMQIK2\\Zeiler and Fergus - 2014 - Visualizing and Understanding Convolutional Networ.pdf}
}

@inproceedings{zelenko2002kernel,
  title = {Kernel Methods for Relation Extraction},
  booktitle = {Proceedings of the {{ACL-02}} Conference on {{Empirical}} Methods in Natural Language Processing  - {{EMNLP}} '02},
  author = {Zelenko, Dmitry and Aone, Chinatsu and Richardella, Anthony},
  year = {2002},
  volume = {10},
  pages = {71--78},
  publisher = {{Association for Computational Linguistics}},
  address = {{Not Known}},
  doi = {10.3115/1118693.1118703},
  langid = {english},
  file = {C\:\\Users\\Hytn\\Zotero\\storage\\RC7I4F9Z\\Zelenko et al_2002_Kernel methods for relation extraction.pdf}
}

@article{zelenko2003kernel,
  title = {Kernel Methods for Relation Extraction},
  author = {Zelenko, Dmitry and Aone, Chinatsu and Richardella, Anthony},
  year = {2003},
  journal = {Journal of machine learning research},
  volume = {3},
  number = {Feb},
  pages = {1083--1106},
  keywords = {⛔ No DOI found}
}

@inproceedings{zeng2014relation,
  title = {Relation Classification via Convolutional Deep Neural Network},
  booktitle = {Proceedings of {{COLING}} 2014, the 25th International Conference on Computational Linguistics: Technical Papers},
  author = {Zeng, Daojian and Liu, Kang and Lai, Siwei and Zhou, Guangyou and Zhao, Jun},
  year = {2014},
  pages = {2335--2344},
  keywords = {⛔ No DOI found}
}

@inproceedings{zeng2015distant,
  title = {Distant Supervision for Relation Extraction via Piecewise Convolutional Neural Networks},
  booktitle = {Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing},
  author = {Zeng, Daojian and Liu, Kang and Chen, Yubo and Zhao, Jun},
  year = {2015},
  pages = {1753--1762},
  doi = {10.18653/v1/D15-1203},
  file = {C\:\\Users\\Hytn\\Zotero\\storage\\6BZ3F4JQ\\Zeng et al_2015_Distant supervision for relation extraction via piecewise convolutional neural.pdf}
}

@inproceedings{zeng2018extracting,
  title = {Extracting {{Relational Facts}} by an {{End-to-End Neural Model}} with {{Copy Mechanism}}},
  booktitle = {Proceedings of the 56th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}} ({{Volume}} 1: {{Long Papers}})},
  author = {Zeng, Xiangrong and Zeng, Daojian and He, Shizhu and Liu, Kang and Zhao, Jun},
  year = {2018},
  pages = {506--514},
  publisher = {{Association for Computational Linguistics}},
  address = {{Melbourne, Australia}},
  doi = {10.18653/v1/P18-1047},
  langid = {english},
  file = {C\:\\Users\\Hytn\\Zotero\\storage\\MBLNS2BZ\\Zeng et al_2018_Extracting Relational Facts by an End-to-End Neural Model with Copy Mechanism.pdf}
}

@inproceedings{zeng2019learning,
  title = {Learning the Extraction Order of Multiple Relational Facts in a Sentence with Reinforcement Learning},
  booktitle = {Proceedings of the 2019 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}} and the 9th {{International Joint Conference}} on {{Natural Language Processing}} ({{EMNLP-IJCNLP}})},
  author = {Zeng, Xiangrong and He, Shizhu and Zeng, Daojian and Liu, Kang and Liu, Shengping and Zhao, Jun},
  year = {2019},
  pages = {367--377},
  doi = {10.18653/v1/D19-1035},
  file = {C\:\\Users\\Hytn\\Zotero\\storage\\4X3L3RGW\\Zeng et al_2019_Learning the extraction order of multiple relational facts in a sentence with.pdf}
}

@inproceedings{zeng2020copymtl,
  title = {Copymtl: {{Copy}} Mechanism for Joint Extraction of Entities and Relations with Multi-Task Learning},
  booktitle = {Proceedings of the {{AAAI Conference}} on {{Artificial Intelligence}}},
  author = {Zeng, Daojian and Zhang, Haoran and Liu, Qianying},
  year = {2020},
  volume = {34},
  pages = {9507--9514},
  doi = {10.1609/aaai.v34i05.6495},
  keywords = {⛔ No DOI found},
  file = {C\:\\Users\\Hytn\\Zotero\\storage\\L8N4RETP\\Zeng et al_2020_Copymtl.pdf}
}

@inproceedings{zeng2020double,
  title = {Double {{Graph Based Reasoning}} for {{Document-level Relation Extraction}}},
  booktitle = {Proceedings of the 2020 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}} ({{EMNLP}})},
  author = {Zeng, Shuang and Xu, Runxin and Chang, Baobao and Li, Lei},
  year = {2020},
  pages = {1630--1640},
  doi = {10.18653/v1/2020.emnlp-main.127},
  file = {C\:\\Users\\Hytn\\Zotero\\storage\\TK3535TX\\Zeng et al_2020_Double Graph Based Reasoning for Document-level Relation Extraction.pdf}
}

@inproceedings{zeng2021sire,
  title = {{{SIRE}}: {{Separate}} Intra-and Inter-Sentential Reasoning for Document-Level Relation Extraction},
  booktitle = {Findings of the Association for Computational Linguistics: {{ACL-IJCNLP}} 2021},
  author = {Zeng, Shuang and Wu, Yuting and Chang, Baobao},
  year = {2021},
  pages = {524--534},
  doi = {10.18653/v1/2021.findings-acl.47},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\storage\\SIRE _Zeng et al 2021.pdf}
}

@inproceedings{zhang2017endtoend,
  title = {End-to-End Neural Relation Extraction with Global Optimization},
  booktitle = {Proceedings of the 2017 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}}},
  author = {Zhang, Meishan and Zhang, Yue and Fu, Guohong},
  year = {2017},
  pages = {1730--1740},
  keywords = {⛔ No DOI found}
}

@inproceedings{zhang2017positionaware,
  title = {Position-Aware Attention and Supervised Data Improve Slot Filling},
  booktitle = {Proceedings of the 2017 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}}},
  author = {Zhang, Yuhao and Zhong, Victor and Chen, Danqi and Angeli, Gabor and Manning, Christopher D.},
  year = {2017},
  pages = {35--45},
  keywords = {⛔ No DOI found}
}

@inproceedings{zhang2018graph,
  title = {Graph {{Convolution}} over {{Pruned Dependency Trees Improves Relation Extraction}}},
  booktitle = {Proceedings of the 2018 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}}},
  author = {Zhang, Yuhao and Qi, Peng and Manning, Christopher D.},
  year = {2018},
  pages = {2205--2215},
  publisher = {{Association for Computational Linguistics}},
  doi = {10.18653/v1/D18-1244},
  file = {C\:\\Users\\Hytn\\Zotero\\storage\\WNVMA2QD\\Zhang et al_2018_Graph Convolution over Pruned Dependency Trees Improves Relation Extraction.pdf}
}

@inproceedings{zhang2018interpretable,
  title = {Interpretable {{Convolutional Neural Networks}}},
  booktitle = {2018 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  author = {Zhang, Quanshi and Wu, Ying Nian and Zhu, Song-Chun},
  year = {2018},
  month = jun,
  pages = {8827--8836},
  publisher = {{IEEE}},
  address = {{Salt Lake City, UT}},
  doi = {10.1109/CVPR.2018.00920},
  abstract = {This paper proposes a method to modify a traditional convolutional neural network (CNN) into an interpretable CNN, in order to clarify knowledge representations in high conv-layers of the CNN. In an interpretable CNN, each filter in a high conv-layer represents a specific object part. Our interpretable CNNs use the same training data as ordinary CNNs without a need for any annotations of object parts or textures for supervision. The interpretable CNN automatically assigns each filter in a high conv-layer with an object part during the learning process. We can apply our method to different types of CNNs with various structures. The explicit knowledge representation in an interpretable CNN can help people understand the logic inside a CNN, i.e. what patterns are memorized by the CNN for prediction. Experiments have shown that filters in an interpretable CNN are more semantically meaningful than those in a traditional CNN. The code is available at https: //github.com/zqs1022/interpretableCNN .},
  isbn = {978-1-5386-6420-9},
  langid = {english},
  file = {C\:\\Users\\Hytn\\Zotero\\storage\\JC932J6A\\Zhang et al. - 2018 - Interpretable Convolutional Neural Networks.pdf}
}

@inproceedings{zhang2019ernie,
  title = {{{ERNIE}}: {{Enhanced Language Representation}} with {{Informative Entities}}},
  booktitle = {Proceedings of the 57th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}}},
  author = {Zhang, Zhengyan and Han, Xu and Liu, Zhiyuan and Jiang, Xin and Sun, Maosong and Liu, Qun},
  year = {2019},
  pages = {1441--1451},
  doi = {10.18653/v1/P19-1139},
  file = {C\:\\Users\\Hytn\\Zotero\\storage\\VP77LKYM\\Zhang et al_2019_ERNIE.pdf}
}

@article{zhang2020robust,
  title = {Towards {{Robust Pattern Recognition}}: {{A Review}}},
  shorttitle = {Towards {{Robust Pattern Recognition}}},
  author = {Zhang, Xu-Yao and Liu, Cheng-Lin and Suen, Ching Y.},
  year = {2020},
  month = jun,
  journal = {Proceedings of the IEEE},
  volume = {108},
  number = {6},
  pages = {894--922},
  issn = {0018-9219, 1558-2256},
  doi = {10.1109/JPROC.2020.2989782},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\storage\\Towards Robust Pattern Recognition _Zhang et al 2020.pdf}
}

@inproceedings{zhang2021deep,
  title = {Deep {{Stable Learning}} for {{Out-Of-Distribution Generalization}}},
  booktitle = {2021 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Zhang, Xingxuan and Cui, Peng and Xu, Renzhe and Zhou, Linjun and He, Yue and Shen, Zheyan},
  year = {2021},
  month = jun,
  pages = {5368--5378},
  publisher = {{IEEE}},
  address = {{Nashville, TN, USA}},
  doi = {10.1109/CVPR46437.2021.00533},
  isbn = {978-1-66544-509-2},
  langid = {english},
  file = {C\:\\Users\\Hytn\\Zotero\\storage\\NCZNILGC\\Zhang et al. - 2021 - Deep Stable Learning for Out-Of-Distribution Gener.pdf}
}

@inproceedings{zhang2021document,
  title = {Document-Level Relation Extraction as Semantic Segmentation},
  booktitle = {{{IJCAI}}},
  author = {Zhang, Ningyu and Chen, Xiang and Xie, Xin and Deng, Shumin and Tan, Chuanqi and Chen, Mosha and Huang, Fei and Si, Luo and Chen, Huajun},
  year = {2021},
  doi = {10.24963/ijcai.2021/551},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\storage\\Document-level relation extraction as semantic segmentation _Zhang et al 2021.pdf}
}

@techreport{zhang2021how,
  type = {Preprint},
  title = {How {{Using Machine Learning Classification}} as a {{Variable}} in {{Regression Leads}} to {{Attenuation Bias}} and {{What}} to {{Do About It}}},
  author = {Zhang, Han},
  year = {2021},
  month = may,
  institution = {{SocArXiv}},
  doi = {10.31235/osf.io/453jk},
  abstract = {Social scientists have increasingly been applying machine learning algorithms to  big data  to measure theoretical concepts they cannot easily measure before, and then been using these machine-predicted variables in a regression. This article  rst demonstrates that directly inserting binary predictions (i.e., classi cation) without regard for prediction error will generally lead to attenuation biases of either slope coe cients or marginal e ect estimates. We then propose four estimators to obtain consistent estimates of coe cients. The estimators require validation data, of which researchers have both machine prediction and true values. Monte Carlo simulations demonstrate the e ectiveness and robustness of the proposed estimators. We summarize the usage pattern of machine learning predictions in 18 recent publications in top social science journals, apply our proposed estimators to four of them, and o er some practical recommendations. We develop an R package CCER to help researchers use the proposed estimators.},
  langid = {english},
  file = {C\:\\Users\\Hytn\\Zotero\\storage\\NPH5JPRB\\Zhang - 2021 - How Using Machine Learning Classification as a Var.pdf}
}

@inproceedings{zhang2021supporting,
  title = {Supporting {{Clustering}} with {{Contrastive Learning}}},
  booktitle = {Proceedings of the 2021 {{Conference}} of the {{North American Chapter}} of the {{Association}} for {{Computational Linguistics}}: {{Human Language Technologies}}},
  author = {Zhang, Dejiao and Nan, Feng and Wei, Xiaokai and Li, Shang-Wen and Zhu, Henghui and McKeown, Kathleen and Nallapati, Ramesh and Arnold, Andrew O. and Xiang, Bing},
  year = {2021},
  pages = {5419--5430},
  publisher = {{Association for Computational Linguistics}},
  address = {{Online}},
  doi = {10.18653/v1/2021.naacl-main.427},
  langid = {english},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\storage\\Supporting Clustering with Contrastive Learning _Zhang et al 2021.pdf}
}

@inproceedings{zhao2018adversarially,
  title = {Adversarially Regularized Autoencoders},
  booktitle = {Proceedings of the 35th International Conference on Machine Learning},
  author = {Zhao, Junbo and Kim, Yoon and Zhang, Kelly and Rush, Alexander and LeCun, Yann},
  editor = {Dy, Jennifer and Krause, Andreas},
  year = {2018},
  month = jul,
  series = {Proceedings of Machine Learning Research},
  volume = {80},
  pages = {5902--5911},
  publisher = {{PMLR}},
  abstract = {Deep latent variable models, trained using variational autoencoders or generative adversarial networks, are now a key technique for representation learning of continuous structures. However, applying similar methods to discrete structures, such as text sequences or discretized images, has proven to be more challenging. In this work, we propose a more flexible method for training deep latent variable models of discrete structures. Our approach is based on the recently proposed Wasserstein Autoencoder (WAE) which formalizes adversarial autoencoders as an optimal transport problem. We first extend this framework to model discrete sequences, and then further explore different learned priors targeting a controllable representation. Unlike many other latent variable generative models for text, this adversarially regularized autoencoder (ARAE) allows us to generate fluent textual outputs as well as perform manipulations in the latent space to induce change in the output space. Finally we show that the latent representation can be trained to perform unaligned textual style transfer, giving improvements both in automatic measures and human evaluation.},
  pdf = {http://proceedings.mlr.press/v80/zhao18b/zhao18b.pdf},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\storage\\Adversarially regularized autoencoders _Zhao et al 2018.pdf}
}

@article{zhao2021causal,
  title = {Causal {{Interpretations}} of {{Black-Box Models}}},
  author = {Zhao, Qingyuan and Hastie, Trevor},
  year = {2021},
  month = jan,
  journal = {Journal of Business \& Economic Statistics},
  volume = {39},
  number = {1},
  pages = {272--281},
  issn = {0735-0015, 1537-2707},
  doi = {10.1080/07350015.2019.1624293},
  langid = {english},
  file = {C\:\\Users\\Hytn\\Zotero\\storage\\GW9F6UQ6\\Causal Interpretations of Black-Box Models_Zhao_Hastie 2021.pdf}
}

@inproceedings{zhao2021relationoriented,
  title = {A {{Relation-Oriented Clustering Method}} for {{Open Relation Extraction}}},
  booktitle = {Proceedings of the 2021 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}}},
  author = {Zhao, Jun and Gui, Tao and Zhang, Qi and Zhou, Yaqian},
  year = {2021},
  pages = {9707--9718},
  publisher = {{Association for Computational Linguistics}},
  address = {{Online and Punta Cana, Dominican Republic}},
  doi = {10.18653/v1/2021.emnlp-main.765},
  langid = {english},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\storage\\A Relation-Oriented Clustering Method for Open Relation Extraction _Zhao et al 2021.pdf}
}

@inproceedings{zhao2021unified,
  title = {A Unified Multi-Task Learning Framework for Joint Extraction of Entities and Relations},
  booktitle = {Proceedings of the {{AAAI}} Conference on Artificial Intelligence},
  author = {Zhao, Tianyang and Yan, Zhao and Cao, Yunbo and Li, Zhoujun},
  year = {2021},
  volume = {35},
  pages = {14524--14531},
  keywords = {⛔ No DOI found},
  file = {C\:\\Users\\Hytn\\Zotero\\storage\\MGRYMUCG\\Zhao 等。 - A Unified Multi-Task Learning Framework for Joint .pdf}
}

@inproceedings{zhao2022fine,
  title = {Fine- and {{Coarse-Granularity Hybrid Self-Attention}} for {{Efficient BERT}}},
  booktitle = {Proceedings of the 60th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}} ({{Volume}} 1: {{Long Papers}})},
  author = {Zhao, Jing and Wang, Yifan and Bao, Junwei and Wu, Youzheng and He, Xiaodong},
  year = {2022},
  pages = {4811--4820},
  publisher = {{Association for Computational Linguistics}},
  address = {{Dublin, Ireland}},
  doi = {10.18653/v1/2022.acl-long.330},
  langid = {english},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\storage\\Fine- and Coarse-Granularity Hybrid Self-Attention for Efficient BERT _Zhao et al 2022.pdf}
}

@article{zhao2022legal,
  title = {Legal {{Judgment Prediction}} via {{Heterogeneous Graphs}} and {{Knowledge}} of {{Law Articles}}},
  author = {Zhao, Qihui and Gao, Tianhan and Zhou, Song and Li, Dapeng and Wen, Yingyou},
  year = {2022},
  month = feb,
  journal = {Applied Sciences},
  volume = {12},
  number = {5},
  pages = {2531},
  issn = {2076-3417},
  doi = {10.3390/app12052531},
  abstract = {Legal judgment prediction (LJP) is a crucial task in legal intelligence to predict charges, law articles and terms of penalties based on case fact description texts. Although existing methods perform well, they still have many shortcomings. First, the existing methods have significant limitations in understanding long documents, especially those based on RNNs and BERT. Secondly, the existing methods are not good at solving the problem of similar charges and do not fully and effectively integrate the information of law articles. To address the above problems, we propose a novel LJP method. Firstly, we improve the model's comprehension of the whole document based on a graph neural network approach. Then, we design a graph attention network-based law article distinction extractor to distinguish similar law articles. Finally, we design a graph fusion method to fuse heterogeneous graphs of text and external knowledge (law article group distinction information). The experiments show that the method could effectively improve LJP performance. The experimental metrics are superior to the existing state of the art.},
  langid = {english},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\storage\\Legal Judgment Prediction via Heterogeneous Graphs and Knowledge of Law Articles _Zhao et al 2022.pdf}
}

@article{zhao2022research,
  title = {Research on a {{Decision Prediction Method Based}} on {{Causal Inference}} and a {{Multi-Expert FTOPJUDGE Mechanism}}},
  author = {Zhao, Qiang and Guo, Rundong and Feng, Xiaowei and Hu, Weifeng and Zhao, Siwen and Wang, Zihan and Li, Yujun and Cao, Yewen},
  year = {2022},
  month = jun,
  journal = {Mathematics},
  volume = {10},
  number = {13},
  pages = {2281},
  issn = {2227-7390},
  doi = {10.3390/math10132281},
  abstract = {Legal judgement prediction (LJP) is a crucial part of legal AI, and its goal is to predict the outcome of a case based on the information in the description of criminal facts. This paper proposes a decision prediction method based on causal inference and a multi-expert FTOPJUDGE mechanism. First, a causal inference algorithm was adopted to process unstructured text. This process did not require very much manual intervention to better mine the information in the text. Then, a neural network dedicated to each task was set up, and a neural network that simultaneously served multiple tasks was also set up. Finally, the pre-trained language model Lawformer was used to provide knowledge for downstream tasks. By using the public data set CAIL2018 and comparing it with current mainstream decision prediction models, it was shown that the model significantly improved the performance of downstream tasks and achieved great improvements in multiple indicators. Through ablation experiments, the effectiveness and rationality of each module of the proposed model were verified. The method proposed in this study achieved reasonably good performance in legal judgment prediction, which provides a promising solution for legal judgment prediction.},
  langid = {english},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\storage\\Research on a Decision Prediction Method Based on Causal Inference and a _Zhao et al 2022.pdf}
}

@inproceedings{zheng2017joint,
  title = {Joint {{Extraction}} of {{Entities}} and {{Relations Based}} on a {{Novel Tagging Scheme}}},
  booktitle = {Proceedings of the 55th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}} ({{Volume}} 1: {{Long Papers}})},
  author = {Zheng, Suncong and Wang, Feng and Bao, Hongyun and Hao, Yuexing and Zhou, Peng and Xu, Bo},
  year = {2017},
  month = jul,
  pages = {1227--1236},
  publisher = {{Association for Computational Linguistics}},
  address = {{Vancouver, Canada}},
  doi = {10.18653/v1/P17-1113},
  abstract = {Joint extraction of entities and relations is an important task in information extraction. To tackle this problem, we firstly propose a novel tagging scheme that can convert the joint extraction task to a tagging problem.. Then, based on our tagging scheme, we study different end-to-end models to extract entities and their relations directly, without identifying entities and relations separately. We conduct experiments on a public dataset produced by distant supervision method and the experimental results show that the tagging based methods are better than most of the existing pipelined and joint learning methods. What's more, the end-to-end model proposed in this paper, achieves the best results on the public dataset.},
  file = {C\:\\Users\\Hytn\\Zotero\\storage\\JIXQSEJB\\Zheng et al_2017_Joint Extraction of Entities and Relations Based on a Novel Tagging Scheme.pdf}
}

@inproceedings{zheng2021prgc,
  title = {{{PRGC}}: {{Potential Relation}} and {{Global Correspondence Based Joint Relational Triple Extraction}}},
  shorttitle = {{{PRGC}}},
  booktitle = {Proceedings of the 59th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}} and the 11th {{International Joint Conference}} on {{Natural Language Processing}} ({{Volume}} 1: {{Long Papers}})},
  author = {Zheng, Hengyi and Wen, Rui and Chen, Xi and Yang, Yifan and Zhang, Yunyan and Zhang, Ziheng and Zhang, Ningyu and Qin, Bin and Ming, Xu and Zheng, Yefeng},
  year = {2021},
  pages = {6225--6235},
  publisher = {{Association for Computational Linguistics}},
  address = {{Online}},
  doi = {10.18653/v1/2021.acl-long.486},
  langid = {english},
  file = {C\:\\Users\\Hytn\\Zotero\\storage\\Q7U5E5I2\\PRGC_Zheng et al 2021.pdf}
}

@inproceedings{zheng2021when,
  title = {When Does Pretraining Help?: Assessing Self-Supervised Learning for Law and the {{CaseHOLD}} Dataset of 53,000+ Legal Holdings},
  shorttitle = {When Does Pretraining Help?},
  booktitle = {Proceedings of the {{Eighteenth International Conference}} on {{Artificial Intelligence}} and {{Law}}},
  author = {Zheng, Lucia and Guha, Neel and Anderson, Brandon R. and Henderson, Peter and Ho, Daniel E.},
  year = {2021},
  month = jun,
  pages = {159--168},
  publisher = {{ACM}},
  address = {{S\~ao Paulo Brazil}},
  doi = {10.1145/3462757.3466088},
  isbn = {978-1-4503-8526-8},
  langid = {english},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\storage\\When does pretraining help _Zheng et al 2021.pdf}
}

@inproceedings{zhou2005exploring,
  title = {Exploring Various Knowledge in Relation Extraction},
  booktitle = {Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics (Acl'05)},
  author = {Zhou, GuoDong and Su, Jian and Zhang, Jie and Zhang, Min},
  year = {2005},
  pages = {427--434},
  keywords = {⛔ No DOI found}
}

@article{zhou2015object,
  title = {Object {{Detectors Emerge}} in {{Deep Scene CNNs}}},
  author = {Zhou, Bolei and Khosla, Aditya and Lapedriza, Agata and Oliva, Aude and Torralba, Antonio},
  year = {2015},
  month = apr,
  journal = {arXiv:1412.6856 [cs]},
  eprint = {1412.6856},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {With the success of new computational architectures for visual processing, such as convolutional neural networks (CNN) and access to image databases with millions of labeled examples (e.g., ImageNet, Places), the state of the art in computer vision is advancing rapidly. One important factor for continued progress is to understand the representations that are learned by the inner layers of these deep architectures. Here we show that object detectors emerge from training CNNs to perform scene classification. As scenes are composed of objects, the CNN for scene classification automatically discovers meaningful objects detectors, representative of the learned scene categories. With object detectors emerging as a result of learning to recognize scenes, our work demonstrates that the same network can perform both scene recognition and object localization in a single forward-pass, without ever having been explicitly taught the notion of objects.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {⛔ No DOI found,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Neural and Evolutionary Computing},
  file = {C\:\\Users\\Hytn\\Zotero\\storage\\CMZA9FJ5\\Zhou et al. - 2015 - Object Detectors Emerge in Deep Scene CNNs.pdf}
}

@inproceedings{zhou2020documentlevel,
  title = {Document-{{Level Relation Extraction}} with {{Adaptive Thresholding}} and {{Localized Context Pooling}}.},
  booktitle = {{{AAAI}}},
  author = {Zhou, Wenxuan and Huang, Kevin and Ma, Tengyu and Huang, Jing},
  year = {2020},
  pages = {14612--14620},
  keywords = {⛔ No DOI found},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\storage\\Document-Level Relation Extraction with Adaptive Thresholding and Localized _Zhou et al 2020.pdf}
}

@inproceedings{zhou2020robustifying,
  title = {Towards {{Robustifying NLI Models Against Lexical Dataset Biases}}},
  booktitle = {Proceedings of the 58th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}}},
  author = {Zhou, Xiang and Bansal, Mohit},
  year = {2020},
  pages = {8759--8771},
  publisher = {{Association for Computational Linguistics}},
  address = {{Online}},
  doi = {10.18653/v1/2020.acl-main.773},
  langid = {english},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\storage\\Towards Robustifying NLI Models Against Lexical Dataset Biases _Zhou_Bansal 2020.pdf}
}

@article{zhu2004recall,
  title = {Recall, Precision and Average Precision},
  author = {Zhu, Mu},
  year = {2004},
  journal = {Department of Statistics and Actuarial Science, University of Waterloo, Waterloo},
  volume = {2},
  number = {30},
  pages = {6},
  keywords = {⛔ No DOI found},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\storage\\Recall, precision and average precision _Zhu 2004.pdf}
}

@inproceedings{zhu2021twag,
  title = {{{TWAG}}: {{A Topic-Guided Wikipedia Abstract Generator}}},
  shorttitle = {{{TWAG}}},
  booktitle = {Proceedings of the 59th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}} and the 11th {{International Joint Conference}} on {{Natural Language Processing}} ({{Volume}} 1: {{Long Papers}})},
  author = {Zhu, Fangwei and Tu, Shangqing and Shi, Jiaxin and Li, Juanzi and Hou, Lei and Cui, Tong},
  year = {2021},
  pages = {4623--4635},
  publisher = {{Association for Computational Linguistics}},
  address = {{Online}},
  doi = {10.18653/v1/2021.acl-long.356},
  langid = {english},
  file = {C\:\\Users\\Hytn\\Zotero\\storage\\9EXD2F72\\TWAG_Zhu et al 2021.pdf}
}

@inproceedings{zhuang2022longrange,
  title = {Long-Range {{Sequence Modeling}} with {{Predictable Sparse Attention}}},
  booktitle = {Proceedings of the 60th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}} ({{Volume}} 1: {{Long Papers}})},
  author = {Zhuang, Yimeng and Zhang, Jing and Tu, Mei},
  year = {2022},
  pages = {234--243},
  publisher = {{Association for Computational Linguistics}},
  address = {{Dublin, Ireland}},
  doi = {10.18653/v1/2022.acl-long.19},
  langid = {english},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\storage\\Long-range Sequence Modeling with Predictable Sparse Attention _Zhuang et al 2022.pdf}
}

@incollection{ziyu2021estimating,
  title = {Estimating {{Treatment Effect}} via {{Differentiated Confounder Matching}}},
  booktitle = {Artificial {{Intelligence}}},
  author = {Ziyu, Zhao and Kuang, Kun and Wu, Fei},
  editor = {Fang, Lu and Chen, Yiran and Zhai, Guangtao and Wang, Jane and Wang, Ruiping and Dong, Weisheng},
  year = {2021},
  volume = {13069},
  pages = {689--699},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-030-93046-2_58},
  isbn = {978-3-030-93045-5 978-3-030-93046-2},
  langid = {english},
  file = {C\:\\Users\\Hytn\\Zotero\\storage\\4XUCWJLS\\Estimating Treatment Effect via Differentiated Confounder Matching_Ziyu et al 2021.pdf}
}


