@article{10.5555/1005332.1044710,
  title = {Variance Reduction Techniques for Gradient Estimates in Reinforcement Learning},
  author = {Greensmith, Evan and Bartlett, Peter L. and Baxter, Jonathan},
  year = {2004},
  month = dec,
  journal = {Journal of Machine Learning Research},
  volume = {5},
  pages = {1471--1530},
  publisher = {{JMLR.org}},
  issn = {1532-4435},
  abstract = {Policy gradient methods for reinforcement learning avoid some of the undesirable properties of the value function approaches, such as policy degradation (Baxter and Bartlett, 2001). However, the variance of the performance gradient estimates obtained from the simulation is sometimes excessive. In this paper, we consider variance reduction methods that were developed for Monte Carlo estimates of integrals. We study two commonly used policy gradient techniques, the baseline and actor-critic methods, from this perspective. Both can be interpreted as additive control variate variance reduction methods. We consider the expected average reward performance measure, and we focus on the GPOMDP algorithm for estimating performance gradients in partially observable Markov decision processes controlled by stochastic reactive policies. We give bounds for the estimation error of the gradient estimates for both baseline and actor-critic algorithms, in terms of the sample size and mixing properties of the controlled system. For the baseline technique, we compute the optimal baseline, and show that the popular approach of using the average reward to define the baseline can be suboptimal. For actor-critic algorithms, we show that using the true value function as the critic can be suboptimal. We also discuss algorithms for estimating the optimal baseline and approximate value function.},
  issue_date = {12/1/2004},
  keywords = {⛔ No DOI found},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\Variance reduction techniques for gradient estimates in reinforcement learning _Greensmith et al 2004.pdf}
}

@article{10.5555/2567709.2502622,
  title = {Stochastic Variational Inference},
  author = {Hoffman, Matthew D. and Blei, David M. and Wang, Chong and Paisley, John},
  year = {2013},
  month = may,
  journal = {Journal of Machine Learning Research},
  volume = {14},
  number = {1},
  pages = {1303--1347},
  publisher = {{JMLR.org}},
  issn = {1532-4435},
  abstract = {We develop stochastic variational inference, a scalable algorithm for approximating posterior distributions. We develop this technique for a large class of probabilistic models and we demonstrate it with two probabilistic topic models, latent Dirichlet allocation and the hierarchical Dirichlet process topic model. Using stochastic variational inference, we analyze several large collections of documents: 300K articles from Nature, 1.8M articles from The New York Times, and 3.8M articles from Wikipedia. Stochastic inference can easily handle data sets of this size and outperforms traditional variational inference, which can only handle a smaller subset. (We also show that the Bayesian nonparametric topic model outperforms its parametric counterpart.) Stochastic variational inference lets us apply complex Bayesian models to massive data sets.},
  issue_date = {January 2013},
  keywords = {Bayesian inference,Bayesian nonparametrics,stochastic inference,stochastic optimization,topic models,variational inference},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\Stochastic variational inference _Hoffman et al 2013.pdf}
}

@inproceedings{abbasi2022role,
  title = {Role of {{Human Intuition}} in {{AI Aided Managerial Decision Making}}: {{A Review}}},
  shorttitle = {Role of {{Human Intuition}} in {{AI Aided Managerial Decision Making}}},
  booktitle = {2022 {{International Conference}} on {{Decision Aid Sciences}} and {{Applications}} ({{DASA}})},
  author = {Abbasi, Merium Fazal and Bilal, Muhammad and Rasheed, Kumeel},
  year = {2022},
  month = mar,
  pages = {713--718},
  publisher = {{IEEE}},
  address = {{Chiangrai, Thailand}},
  doi = {10.1109/DASA54658.2022.9765153},
  urldate = {2022-07-21},
  isbn = {978-1-66549-501-1},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\Role of Human Intuition in AI Aided Managerial Decision Making _Abbasi et al 22.pdf}
}

@misc{abid2022meaningfully,
  title = {Meaningfully {{Debugging Model Mistakes}} Using {{Conceptual Counterfactual Explanations}}},
  author = {Abid, Abubakar and Yuksekgonul, Mert and Zou, James},
  year = {2022},
  month = jun,
  number = {arXiv:2106.12723},
  eprint = {arXiv:2106.12723},
  publisher = {{arXiv}},
  urldate = {2022-07-04},
  abstract = {Understanding and explaining the mistakes made by trained models is critical to many machine learning objectives, such as improving robustness, addressing concept drift, and mitigating biases. However, this is often an ad hoc process that involves manually looking at the model's mistakes on many test samples and guessing at the underlying reasons for those incorrect predictions. In this paper, we propose a systematic approach, conceptual counterfactual explanations (CCE), that explains why a classifier makes a mistake on a particular test sample(s) in terms of human-understandable concepts (e.g. this zebra is misclassified as a dog because of faint stripes). We base CCE on two prior ideas: counterfactual explanations and concept activation vectors, and validate our approach on wellknown pretrained models, showing that it explains the models' mistakes meaningfully. In addition, for new models trained on data with spurious correlations, CCE accurately identifies the spurious correlation as the cause of model mistakes from a single misclassified test sample. On two challenging medical applications, CCE generated useful insights, confirmed by clinicians, into biases and mistakes the model makes in real-world settings. The code for CCE is publicly available at https://github.com/ mertyg/debug-mistakes-cce.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\Meaningfully Debugging Model Mistakes using Conceptual Counterfactual _Abid et al 22.pdf}
}

@inproceedings{abu2022docinfer,
  title = {{{DocInfer}}: {{Document-level}} Natural Language Inference Using Optimal Evidence Selection},
  booktitle = {Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing},
  author = {Mathur, Puneet and Kunapuli, Gautam and Bhat, Riyaz and Shrivastava, Manish and Manocha, Dinesh and Singh, Maneesh},
  year = {2022},
  month = dec,
  pages = {809--824},
  publisher = {{Association for Computational Linguistics}},
  address = {{Abu Dhabi, United Arab Emirates}},
  abstract = {We present DocInfer - a novel, end-to-end Document-level Natural Language Inference model that builds a hierarchical document graph enriched through inter-sentence relations (topical, entity-based, concept-based), performs paragraph pruning using the novel SubGraph Pooling layer, followed by optimal evidence selection based on REINFORCE algorithm to identify the most important context sentences for a given hypothesis. Our evidence selection mechanism allows it to transcend the input length limitation of modern BERT-like Transformer models while presenting the entire evidence together for inferential reasoning. We show this is an important property needed to reason on large documents where the evidence may be fragmented and located arbitrarily far from each other. Extensive experiments on popular corpora - DocNLI, ContractNLI, and ConTRoL datasets, and our new proposed dataset called CaseHoldNLI on the task of legal judicial reasoning, demonstrate significant performance gains of 8-12\% over SOTA methods. Our ablation studies validate the impact of our model. Performance improvement of 3-6\% on annotation-scarce downstream tasks of fact verification, multiple-choice QA, and contract clause retrieval demonstrates the usefulness of DocInfer beyond primary NLI tasks.},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\DocInfer _Mathur et al 2022.pdf}
}

@article{adadi2018peeking,
  title = {Peeking {{Inside}} the {{Black-Box}}: {{A Survey}} on {{Explainable Artificial Intelligence}} ({{XAI}})},
  shorttitle = {Peeking {{Inside}} the {{Black-Box}}},
  author = {Adadi, Amina and Berrada, Mohammed},
  year = {2018},
  journal = {IEEE Access},
  volume = {6},
  pages = {52138--52160},
  issn = {2169-3536},
  doi = {10.1109/ACCESS.2018.2870052},
  urldate = {2022-03-13},
  abstract = {At the dawn of the fourth industrial revolution, we are witnessing a fast and widespread adoption of artificial intelligence (AI) in our daily life, which contributes to accelerating the shift towards a more algorithmic society. However, even with such unprecedented advancements, a key impediment to the use of AI-based systems is that they often lack transparency. Indeed, the black-box nature of these systems allows powerful predictions, but it cannot be directly explained. This issue has triggered a new debate on explainable AI (XAI). A research field holds substantial promise for improving trust and transparency of AI-based systems. It is recognized as the sine qua non for AI to continue making steady progress without disruption. This survey provides an entry point for interested researchers and practitioners to learn key aspects of the young and rapidly growing body of research related to XAI. Through the lens of the literature, we review the existing approaches regarding the topic, discuss trends surrounding its sphere, and present major research trajectories.},
  langid = {english},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\Peeking Inside the Black-Box _Adadi_Berrada 22.pdf}
}

@inproceedings{agrawal2016analyzing,
  title = {Analyzing the Behavior of Visual Question Answering Models},
  booktitle = {Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing},
  author = {Agrawal, Aishwarya and Batra, Dhruv and Parikh, Devi},
  year = {2016},
  pages = {1955--1960},
  doi = {10.18653/v1/D16-1203},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\Analyzing the behavior of visual question answering models _Agrawal et al 22.pdf}
}

@inproceedings{ahuja2021empirical,
  title = {Empirical or Invariant Risk Minimization? {{A}} Sample Complexity Perspective},
  booktitle = {International Conference on Learning Representations},
  author = {Ahuja, Kartik and Wang, Jun and Dhurandhar, Amit and Shanmugam, Karthikeyan and Varshney, Kush R.},
  year = {2021},
  keywords = {⛔ No DOI found},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\Empirical or invariant risk minimization _Ahuja et al 22.pdf}
}

@inproceedings{ahuja2022aspectnews,
  title = {{{ASPECTNEWS}}: {{Aspect-Oriented Summarization}} of {{News Documents}}},
  shorttitle = {{{ASPECTNEWS}}},
  booktitle = {Proceedings of the 60th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}} ({{Volume}} 1: {{Long Papers}})},
  author = {Ahuja, Ojas and Xu, Jiacheng and Gupta, Akshay and Horecka, Kevin and Durrett, Greg},
  year = {2022},
  pages = {6494--6506},
  publisher = {{Association for Computational Linguistics}},
  address = {{Dublin, Ireland}},
  doi = {10.18653/v1/2022.acl-long.449},
  urldate = {2022-06-07},
  langid = {english},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\ASPECTNEWS _Ahuja et al 22.pdf}
}

@misc{akkus2023multimodal,
  title = {Multimodal {{Deep Learning}}},
  author = {Akkus, Cem and Chu, Luyang and Djakovic, Vladana and {Jauch-Walser}, Steffen and Koch, Philipp and Loss, Giacomo and Marquardt, Christopher and Moldovan, Marco and Sauter, Nadja and Schneider, Maximilian and Schulte, Rickmer and Urbanczyk, Karol and Goschenhofer, Jann and Heumann, Christian and Hvingelby, Rasmus and Schalk, Daniel and A{\ss}enmacher, Matthias},
  year = {2023},
  month = jan,
  number = {arXiv:2301.04856},
  eprint = {arXiv:2301.04856},
  publisher = {{arXiv}},
  urldate = {2023-02-07},
  abstract = {This book is the result of a seminar in which we reviewed multimodal approaches and attempted to create a solid overview of the field, starting with the current state-of-the-art approaches in the two subfields of Deep Learning individually. Further, modeling frameworks are discussed where one modality is transformed into the other, as well as models in which one modality is utilized to enhance representation learning for the other. To conclude the second part, architectures with a focus on handling both modalities simultaneously are introduced. Finally, we also cover other modalities as well as general-purpose multi-modal models, which are able to handle different tasks on different modalities within one unified architecture. One interesting application (Generative Art) eventually caps off this booklet.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C\:\\Users\\Hytn\\Zotero\\storage\\YIH8CAXS\\Akkus et al. - 2023 - Multimodal Deep Learning.pdf}
}

@misc{akyurek2022tracing,
  title = {Towards {{Tracing Factual Knowledge}} in {{Language Models Back}} to the {{Training Data}}},
  author = {Aky{\"u}rek, Ekin and Bolukbasi, Tolga and Liu, Frederick and Xiong, Binbin and Tenney, Ian and Andreas, Jacob and Guu, Kelvin},
  year = {2022},
  month = oct,
  number = {arXiv:2205.11482},
  eprint = {arXiv:2205.11482},
  publisher = {{arXiv}},
  urldate = {2023-04-10},
  abstract = {Language models (LMs) have been shown to memorize a great deal of factual knowledge contained in their training data. But when an LM generates an assertion, it is often difficult to determine where it learned this information and whether it is true. In this paper, we propose the problem of fact tracing: identifying which training examples taught an LM to generate a particular factual assertion. Prior work on training data attribution (TDA) may offer effective tools for identifying such examples, known as "proponents". We present the first quantitative benchmark to evaluate this. We compare two popular families of TDA methods -- gradient-based and embedding-based -- and find that much headroom remains. For example, both methods have lower proponent-retrieval precision than an information retrieval baseline (BM25) that does not have access to the LM at all. We identify key challenges that may be necessary for further improvement such as overcoming the problem of gradient saturation, and also show how several nuanced implementation details of existing neural TDA methods can significantly improve overall fact tracing performance.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Computation and Language,Computer Science - Information Retrieval},
  file = {C\:\\Users\\Hytn\\Zotero\\storage\\XVXL76D7\\Akyürek et al. - 2022 - Towards Tracing Factual Knowledge in Language Mode.pdf}
}

@misc{albuquerque2021generalizing,
  title = {Generalizing to Unseen Domains via Distribution Matching},
  author = {Albuquerque, Isabela and Monteiro, Jo{\~a}o and Darvishi, Mohammad and Falk, Tiago H. and Mitliagkas, Ioannis},
  year = {2021},
  month = sep,
  number = {arXiv:1911.00804},
  eprint = {arXiv:1911.00804},
  publisher = {{arXiv}},
  urldate = {2022-09-19},
  abstract = {In many applications of machine learning, the training and test set data come from different distributions, or domains. A number of domain generalization strategies have been introduced with the goal of achieving good performance on out-of-distribution data. In this paper, we propose an adversarial approach to the problem. We propose a process that enforces pair-wise domain invariance while training a feature extractor over a diverse set of domains. We show that this process ensures invariance to any distribution that can be expressed as a mixture of the training domains. Following this insight, we then introduce an adversarial approach in which pair-wise divergences are estimated and minimized. Experiments on two domain generalization benchmarks for object recognition (i.e., PACS and VLCS) show that the proposed method yields higher average accuracy on the target domains in comparison to previously introduced adversarial strategies, as well as recently proposed methods based on learning invariant representations.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C\:\\Users\\Hytn\\Zotero\\storage\\LFII5UQ5\\Albuquerque et al. - 2021 - Generalizing to unseen domains via distribution ma.pdf}
}

@article{aletras2016predicting,
  title = {Predicting Judicial Decisions of the {{European Court}} of {{Human Rights}}: A {{Natural Language Processing}} Perspective},
  shorttitle = {Predicting Judicial Decisions of the {{European Court}} of {{Human Rights}}},
  author = {Aletras, Nikolaos and Tsarapatsanis, Dimitrios and {Preo{\c t}iuc-Pietro}, Daniel and Lampos, Vasileios},
  year = {2016},
  month = oct,
  journal = {PeerJ Computer Science},
  volume = {2},
  pages = {e93},
  issn = {2376-5992},
  doi = {10.7717/peerj-cs.93},
  urldate = {2022-08-10},
  abstract = {Recent advances in Natural Language Processing and Machine Learning provide us with the tools to build predictive models that can be used to unveil patterns driving judicial decisions. This can be useful, for both lawyers and judges, as an assisting tool to rapidly identify cases and extract patterns which lead to certain decisions. This paper presents the first systematic study on predicting the outcome of cases tried by the European Court of Human Rights based solely on textual content. We formulate a binary classification task where the input of our classifiers is the textual content extracted from a case and the target output is the actual judgment as to whether there has been a violation of an article of the convention of human rights. Textual information is represented using contiguous word sequences, i.e.,~N-grams, and topics. Our models can predict the court's decisions with a strong accuracy (79\% on average). Our empirical analysis indicates that the formal facts of a case are the most important predictive factor. This is consistent with the theory of legal realism suggesting that judicial decision-making is significantly affected by the stimulus of the facts. We also observe that the topical content of a case is another important feature in this classification task and explore this relationship further by conducting a qualitative analysis.},
  langid = {english},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\Predicting judicial decisions of the European Court of Human Rights _Aletras et al 22.pdf}
}

@article{alomari2022deep,
  title = {Deep Reinforcement and Transfer Learning for Abstractive Text Summarization: {{A}} Review},
  shorttitle = {Deep Reinforcement and Transfer Learning for Abstractive Text Summarization},
  author = {Alomari, Ayham and Idris, Norisma and Sabri, Aznul Qalid Md and Alsmadi, Izzat},
  year = {2022},
  month = jan,
  journal = {Computer Speech \& Language},
  volume = {71},
  pages = {101276},
  issn = {08852308},
  doi = {10.1016/j.csl.2021.101276},
  urldate = {2023-02-20},
  langid = {english},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\Deep reinforcement and transfer learning for abstractive text summarization _Alomari et al 2022.pdf}
}

@inproceedings{alt2019improving,
  title = {Improving {{Relation Extraction}} by {{Pre-trained Language Representations}}},
  booktitle = {{{AKBC}} 2019 : 1st {{Conference}} on {{Automated Knowledge Base Construction}}},
  author = {Alt, Christoph and H{\~A}{$\frac{1}{4}$}bner, Marc and Hennig, Leonhard},
  year = {2019},
  keywords = {⛔ No DOI found}
}

@inproceedings{alvarez-melis2017causal,
  title = {A Causal Framework for Explaining the Predictions of Black-Box Sequence-to-Sequence Models},
  booktitle = {Proceedings of the 2017 {{Conference}} on {{Empirical Methods}} in {{Natural}}           {{Language Processing}}},
  author = {{Alvarez-Melis}, David and Jaakkola, Tommi},
  year = {2017},
  pages = {412--421},
  publisher = {{Association for Computational Linguistics}},
  address = {{Copenhagen, Denmark}},
  doi = {10.18653/v1/D17-1042},
  urldate = {2022-07-16},
  langid = {english},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\A causal framework for explaining the predictions of black-box _Alvarez-Melis_Jaakkola 22.pdf}
}

@inproceedings{alzantot2018generating,
  title = {Generating {{Natural Language Adversarial Examples}}},
  booktitle = {Proceedings of the 2018 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}}},
  author = {Alzantot, Moustafa and Sharma, Yash and Elgohary, Ahmed and Ho, Bo-Jhang and Srivastava, Mani and Chang, Kai-Wei},
  year = {2018},
  pages = {2890--2896},
  publisher = {{Association for Computational Linguistics}},
  address = {{Brussels, Belgium}},
  doi = {10.18653/v1/D18-1316},
  urldate = {2022-07-29},
  langid = {english},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\Generating Natural Language Adversarial Examples _Alzantot et al 22.pdf}
}

@inproceedings{amplayo2021informative,
  title = {Informative and {{Controllable Opinion Summarization}}},
  booktitle = {Proceedings of the 16th {{Conference}} of the {{European Chapter}} of the {{Association}} for {{Computational Linguistics}}: {{Main Volume}}},
  author = {Amplayo, Reinald Kim and Lapata, Mirella},
  year = {2021},
  pages = {2662--2672},
  publisher = {{Association for Computational Linguistics}},
  address = {{Online}},
  doi = {10.18653/v1/2021.eacl-main.229},
  urldate = {2022-10-07},
  langid = {english},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\Informative and Controllable Opinion Summarization _Amplayo_Lapata 2021.pdf}
}

@inproceedings{angeli2014combining,
  title = {Combining Distant and Partial Supervision for Relation Extraction},
  booktitle = {Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing ({{EMNLP}})},
  author = {Angeli, Gabor and Tibshirani, Julie and Wu, Jean and Manning, Christopher D},
  year = {2014},
  pages = {1556--1567},
  doi = {10.3115/v1/D14-1164},
  keywords = {MIML-RE},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\Combining distant and partial supervision for relation extraction _Angeli et al 2014.pdf}
}

@inproceedings{angelidis2018summarizing,
  title = {Summarizing {{Opinions}}: {{Aspect Extraction Meets Sentiment Prediction}} and {{They Are Both Weakly Supervised}}},
  shorttitle = {Summarizing {{Opinions}}},
  booktitle = {Proceedings of the 2018 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}}},
  author = {Angelidis, Stefanos and Lapata, Mirella},
  year = {2018},
  pages = {3675--3686},
  publisher = {{Association for Computational Linguistics}},
  address = {{Brussels, Belgium}},
  doi = {10.18653/v1/D18-1403},
  urldate = {2022-08-30},
  langid = {english},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\Summarizing Opinions _Angelidis_Lapata 2018.pdf}
}

@article{angelidis2021extractive,
  title = {Extractive {{Opinion Summarization}} in {{Quantized Transformer Spaces}}},
  author = {Angelidis, Stefanos and Amplayo, Reinald Kim and Suhara, Yoshihiko and Wang, Xiaolan and Lapata, Mirella},
  year = {2021},
  month = mar,
  journal = {Transactions of the Association for Computational Linguistics},
  volume = {9},
  pages = {277--293},
  issn = {2307-387X},
  doi = {10.1162/tacl-a-00366},
  urldate = {2022-06-12},
  abstract = {Abstract             We present the Quantized Transformer (QT), an unsupervised system for extractive opinion summarization. QT is inspired by Vector- Quantized Variational Autoencoders, which we repurpose for popularity-driven summarization. It uses a clustering interpretation of the quantized space and a novel extraction algorithm to discover popular opinions among hundreds of reviews, a significant step towards opinion summarization of practical scope. In addition, QT enables controllable summarization without further training, by utilizing properties of the quantized space to extract aspect-specific summaries. We also make publicly available Space, a large-scale evaluation benchmark for opinion summarizers, comprising general and aspect-specific summaries for 50 hotels. Experiments demonstrate the promise of our approach, which is validated by human studies where judges showed clear preference for our method over competitive baselines.},
  langid = {english},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\Extractive Opinion Summarization in Quantized Transformer Spaces _Angelidis et al 22.pdf}
}

@misc{arjovsky2020invariant,
  title = {Invariant {{Risk Minimization}}},
  author = {Arjovsky, Martin and Bottou, L{\'e}on and Gulrajani, Ishaan and {Lopez-Paz}, David},
  year = {2020},
  month = mar,
  number = {arXiv:1907.02893},
  eprint = {arXiv:1907.02893},
  publisher = {{arXiv}},
  urldate = {2022-07-16},
  abstract = {We introduce Invariant Risk Minimization (IRM), a learning paradigm to estimate invariant correlations across multiple training distributions. To achieve this goal, IRM learns a data representation such that the optimal classifier, on top of that data representation, matches for all training distributions. Through theory and experiments, we show how the invariances learned by IRM relate to the causal structures governing the data and enable out-of-distribution generalization.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\Invariant Risk Minimization _Arjovsky et al 22.pdf}
}

@inproceedings{arora2008latent,
  title = {Latent {{Dirichlet Allocation}} and {{Singular Value Decomposition Based Multi-document Summarization}}},
  booktitle = {2008 {{Eighth IEEE International Conference}} on {{Data Mining}}},
  author = {Arora, Rachit and Ravindran, Balaraman},
  year = {2008},
  month = dec,
  pages = {713--718},
  publisher = {{IEEE}},
  address = {{Pisa, Italy}},
  doi = {10.1109/ICDM.2008.55},
  urldate = {2022-08-27},
  isbn = {978-0-7695-3502-9}
}

@inproceedings{athiwaratkun2020augmented,
  title = {Augmented {{Natural Language}} for {{Generative Sequence Labeling}}},
  booktitle = {Proceedings of the 2020 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}} ({{EMNLP}})},
  author = {Athiwaratkun, Ben and {Nogueira dos Santos}, Cicero and Krone, Jason and Xiang, Bing},
  year = {2020},
  pages = {375--385},
  publisher = {{Association for Computational Linguistics}},
  address = {{Online}},
  doi = {10.18653/v1/2020.emnlp-main.27},
  urldate = {2022-06-20},
  langid = {english},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\Augmented Natural Language for Generative Sequence Labeling _Athiwaratkun et al 22.pdf}
}

@misc{aubin2021linear,
  title = {Linear Unit-Tests for Invariance Discovery},
  author = {Aubin, Benjamin and S{\l}owik, Agnieszka and Arjovsky, Martin and Bottou, Leon and {Lopez-Paz}, David},
  year = {2021},
  month = feb,
  number = {arXiv:2102.10867},
  eprint = {arXiv:2102.10867},
  publisher = {{arXiv}},
  urldate = {2022-09-20},
  abstract = {There is an increasing interest in algorithms to learn invariant correlations across training environments. A big share of the current proposals find theoretical support in the causality literature but, how useful are they in practice? The purpose of this note is to propose six linear low-dimensional problems \textemdash ``unit tests''\textemdash to evaluate different types of out-of-distribution generalization in a precise manner. Following initial experiments, none of the three recently proposed alternatives passes all tests. By providing the code to automatically replicate all the results in this manuscript (https://www.github.com/facebookresearch/ InvarianceUnitTests), we hope that our unit tests become a standard stepping stone for researchers in out-of-distribution generalization.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  file = {C\:\\Users\\Hytn\\Zotero\\storage\\4K8UEYKH\\Aubin et al. - 2021 - Linear unit-tests for invariance discovery.pdf}
}

@book{aumann2015values,
  title = {Values of Non-Atomic Games},
  author = {Aumann, Robert J and Shapley, Lloyd S},
  year = {2015},
  publisher = {{Princeton University Press}}
}

@article{bach2015pixelwise,
  title = {On {{Pixel-Wise Explanations}} for {{Non-Linear Classifier Decisions}} by {{Layer-Wise Relevance Propagation}}},
  author = {Bach, Sebastian and Binder, Alexander and Montavon, Gr{\'e}goire and Klauschen, Frederick and M{\"u}ller, Klaus-Robert and Samek, Wojciech},
  editor = {Suarez, Oscar Deniz},
  year = {2015},
  month = jul,
  journal = {PLOS ONE},
  volume = {10},
  number = {7},
  pages = {e0130140},
  issn = {1932-6203},
  doi = {10.1371/journal.pone.0130140},
  urldate = {2022-07-11},
  langid = {english},
  keywords = {LRP CV},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\On Pixel-Wise Explanations for Non-Linear Classifier Decisions by Layer-Wise _Bach et al 22.pdf}
}

@article{baehrens2010explain,
  title = {How to Explain Individual Classification Decisions},
  author = {Baehrens, David and Schroeter, Timon and Harmeling, Stefan and Kawanabe, Motoaki and Hansen, Katja and M{\"u}ller, Klaus-Robert},
  year = {2010},
  journal = {The Journal of Machine Learning Research},
  volume = {11},
  pages = {1803--1831},
  publisher = {{JMLR. org}},
  keywords = {⛔ No DOI found},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\How to explain individual classification decisions _Baehrens et al 22.pdf}
}

@article{bag2019efficient,
  title = {An Efficient Recommendation Generation Using Relevant {{Jaccard}} Similarity},
  author = {Bag, Sujoy and Kumar, Sri Krishna and Tiwari, Manoj Kumar},
  year = {2019},
  month = may,
  journal = {Information Sciences},
  volume = {483},
  pages = {53--64},
  issn = {00200255},
  doi = {10.1016/j.ins.2019.01.023},
  urldate = {2022-08-12},
  langid = {english},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\An efficient recommendation generation using relevant Jaccard similarity _Bag et al 22.pdf}
}

@inproceedings{bahrainian2022newts,
  title = {{{NEWTS}}: {{A Corpus}} for {{News Topic-Focused Summarization}}},
  shorttitle = {{{NEWTS}}},
  booktitle = {Findings of the {{Association}} for {{Computational Linguistics}}: {{ACL}} 2022},
  author = {Bahrainian, Seyed Ali and Feucht, Sheridan and Eickhoff, Carsten},
  year = {2022},
  pages = {493--503},
  publisher = {{Association for Computational Linguistics}},
  address = {{Dublin, Ireland}},
  doi = {10.18653/v1/2022.findings-acl.42},
  urldate = {2022-09-19},
  langid = {english},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\NEWTS _Bahrainian et al 2022.pdf}
}

@inproceedings{bahri2022sharpnessaware,
  title = {Sharpness-{{Aware Minimization Improves Language Model Generalization}}},
  booktitle = {Proceedings of the 60th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}} ({{Volume}} 1: {{Long Papers}})},
  author = {Bahri, Dara and Mobahi, Hossein and Tay, Yi},
  year = {2022},
  pages = {7360--7371},
  publisher = {{Association for Computational Linguistics}},
  address = {{Dublin, Ireland}},
  doi = {10.18653/v1/2022.acl-long.508},
  urldate = {2022-11-01},
  langid = {english},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\Sharpness-Aware Minimization Improves Language Model Generalization _Bahri et al 2022.pdf}
}

@inproceedings{bajaj2021long,
  title = {Long {{Document Summarization}} in a {{Low Resource Setting}} Using {{Pretrained Language Models}}},
  booktitle = {Proceedings of the 59th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}} and the 11th {{International Joint Conference}} on {{Natural Language Processing}}: {{Student Research Workshop}}},
  author = {Bajaj, Ahsaas and Dangati, Pavitra and Krishna, Kalpesh and Ashok Kumar, Pradhiksha and Uppaal, Rheeya and Windsor, Bradford and Brenner, Eliot and Dotterrer, Dominic and Das, Rajarshi and McCallum, Andrew},
  year = {2021},
  pages = {71--80},
  publisher = {{Association for Computational Linguistics}},
  address = {{Online}},
  doi = {10.18653/v1/2021.acl-srw.7},
  urldate = {2022-08-20},
  langid = {english},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\Long Document Summarization in a Low Resource Setting using Pretrained Language _Bajaj et al 2021.pdf}
}

@inproceedings{baldinisoares2019matching,
  title = {Matching the {{Blanks}}: {{Distributional Similarity}} for {{Relation Learning}}},
  shorttitle = {Matching the {{Blanks}}},
  booktitle = {Proceedings of the 57th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}}},
  author = {Baldini Soares, Livio and FitzGerald, Nicholas and Ling, Jeffrey and Kwiatkowski, Tom},
  year = {2019},
  pages = {2895--2905},
  publisher = {{Association for Computational Linguistics}},
  address = {{Florence, Italy}},
  doi = {10.18653/v1/P19-1279},
  urldate = {2022-11-05},
  langid = {english},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\Matching the Blanks _Baldini Soares et al 2019.pdf}
}

@misc{bansal2018chauffeurnet,
  title = {{{ChauffeurNet}}: {{Learning}} to {{Drive}} by {{Imitating}} the {{Best}} and {{Synthesizing}} the {{Worst}}},
  shorttitle = {{{ChauffeurNet}}},
  author = {Bansal, Mayank and Krizhevsky, Alex and Ogale, Abhijit},
  year = {2018},
  month = dec,
  number = {arXiv:1812.03079},
  eprint = {arXiv:1812.03079},
  publisher = {{arXiv}},
  urldate = {2022-11-23},
  abstract = {Our goal is to train a policy for autonomous driving via imitation learning that is robust enough to drive a real vehicle. We find that standard behavior cloning is insufficient for handling complex driving scenarios, even when we leverage a perception system for preprocessing the input and a controller for executing the output on the car: 30 million examples are still not enough. We propose exposing the learner to synthesized data in the form of perturbations to the expert's driving, which creates interesting situations such as collisions and/or going off the road. Rather than purely imitating all data, we augment the imitation loss with additional losses that penalize undesirable events and encourage progress \textendash{} the perturbations then provide an important signal for these losses and lead to robustness of the learned model. We show that the ChauffeurNet model can handle complex situations in simulation, and present ablation experiments that emphasize the importance of each of our proposed changes and show that the model is responding to the appropriate causal factors. Finally, we demonstrate the model driving a car in the real world.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Robotics},
  file = {C\:\\Users\\Hytn\\Zotero\\storage\\8Y9W4UF3\\Bansal et al. - 2018 - ChauffeurNet Learning to Drive by Imitating the B.pdf}
}

@inproceedings{baralis2012multidocument,
  title = {Multi-Document Summarization Exploiting Frequent Itemsets},
  booktitle = {Proceedings of the 27th {{Annual ACM Symposium}} on {{Applied Computing}} - {{SAC}} '12},
  author = {Baralis, Elena and Cagliero, Luca and Jabeen, Saima and Fiori, Alessandro},
  year = {2012},
  pages = {782},
  publisher = {{ACM Press}},
  address = {{Trento, Italy}},
  doi = {10.1145/2245276.2245427},
  urldate = {2022-08-30},
  isbn = {978-1-4503-0857-1},
  langid = {english},
  keywords = {TFIDF},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\Multi-document summarization exploiting frequent itemsets _Baralis et al 2012.pdf}
}

@inproceedings{barba2022extend,
  title = {{{ExtEnD}}: {{Extractive}} Entity Disambiguation},
  booktitle = {Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: {{Long}} Papers)},
  author = {Barba, Edoardo and Procopio, Luigi and Navigli, Roberto},
  year = {2022},
  month = may,
  pages = {2478--2488},
  publisher = {{Association for Computational Linguistics}},
  address = {{Dublin, Ireland}},
  abstract = {Local models for Entity Disambiguation (ED) have today become extremely powerful, in most part thanks to the advent of large pre-trained language models. However, despite their significant performance achievements, most of these approaches frame ED through classification formulations that have intrinsic limitations, both computationally and from a modeling perspective. In contrast with this trend, here we propose ExtEnD, a novel local formulation for ED where we frame this task as a text extraction problem, and present two Transformer-based architectures that implement it. Based on experiments in and out of domain, and training over two different data regimes, we find our approach surpasses all its competitors in terms of both data efficiency and raw performance. ExtEnD outperforms its alternatives by as few as 6 F1 points on the more constrained of the two data regimes and, when moving to the other higher-resourced regime, sets a new state of the art on 4 out of 4 benchmarks under consideration, with average improvements of 0.7 F1 points overall and 1.1 F1 points out of domain. In addition, to gain better insights from our results, we also perform a fine-grained evaluation of our performances on different classes of label frequency, along with an ablation study of our architectural choices and an error analysis. We release our code and models for research purposes at https://github.com/SapienzaNLP/extend.},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\ExtEnD _Barba et al 22.pdf}
}

@article{bareinboim2014recovering,
  title = {Recovering from {{Selection Bias}} in {{Causal}} and {{Statistical Inference}}},
  author = {Bareinboim, Elias and Tian, Jin and Pearl, Judea},
  year = {2014},
  month = jun,
  journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
  volume = {28},
  number = {1},
  issn = {2374-3468, 2159-5399},
  doi = {10.1609/aaai.v28i1.9074},
  urldate = {2022-08-09},
  abstract = {Selection bias is caused by preferential exclusion of units from the samples and represents a major obstacle to valid causal and statistical inferences; it cannot be removed by randomized experiments and can rarely be detected in either experimental or observational studies. In this paper, we provide complete graphical and algorithmic conditions for recovering conditional probabilities from selection biased data. We also provide graphical conditions for recoverability when unbiased data is available over a subset of the variables. Finally, we provide a graphical condition that generalizes the backdoor criterion and serves to recover causal effects when the data is collected under preferential selection.},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\Recovering from Selection Bias in Causal and Statistical Inference _Bareinboim et al 22.pdf}
}

@article{bareinboim2015recovering,
  title = {Recovering {{Causal Effects}} from {{Selection Bias}}},
  author = {Bareinboim, Elias and Tian, Jin},
  year = {2015},
  month = mar,
  journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
  volume = {29},
  number = {1},
  issn = {2374-3468, 2159-5399},
  doi = {10.1609/aaai.v29i1.9679},
  urldate = {2022-08-12},
  abstract = {Controlling for selection and confounding biases are two of the most challenging problems that appear in data analysis in the empirical sciences as well as in artificial intelligence tasks. The combination of previously studied methods for each of these biases in isolation is not directly applicable to certain non-trivial cases in which selection and confounding biases are simultaneously present. In this paper, we tackle these instances non-parametrically and in full generality. We provide graphical and algorithmic conditions for recoverability of interventional distributions for when selection and confounding biases are both present. Our treatment completely characterizes the class of causal effects that are recoverable in Markovian models, and is suffi- cient for Semi-Markovian models.},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\Recovering Causal Effects from Selection Bias _Bareinboim_Tian 22.pdf}
}

@article{barredoarrieta2020explainable,
  title = {Explainable {{Artificial Intelligence}} ({{XAI}}): {{Concepts}}, Taxonomies, Opportunities and Challenges toward Responsible {{AI}}},
  shorttitle = {Explainable {{Artificial Intelligence}} ({{XAI}})},
  author = {Barredo Arrieta, Alejandro and {D{\'i}az-Rodr{\'i}guez}, Natalia and Del Ser, Javier and Bennetot, Adrien and Tabik, Siham and Barbado, Alberto and Garcia, Salvador and {Gil-Lopez}, Sergio and Molina, Daniel and Benjamins, Richard and Chatila, Raja and Herrera, Francisco},
  year = {2020},
  month = jun,
  journal = {Information Fusion},
  volume = {58},
  pages = {82--115},
  issn = {15662535},
  doi = {10.1016/j.inffus.2019.12.012},
  urldate = {2022-03-13},
  langid = {english},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\Explainable Artificial Intelligence (XAI) _Barredo Arrieta et al 22.pdf}
}

@article{bastani2018interpretability,
  title = {Interpretability via {{Model Extraction}}},
  author = {Bastani, Osbert and Kim, Carolyn and Bastani, Hamsa},
  year = {2018},
  month = mar,
  journal = {arXiv:1706.09773 [cs, stat]},
  eprint = {1706.09773},
  primaryclass = {cs, stat},
  urldate = {2022-03-20},
  abstract = {The ability to interpret machine learning models has become increasingly important now that machine learning is used to inform consequential decisions. We propose an approach called model extraction for interpreting complex, blackbox models. Our approach approximates the complex model using a much more interpretable model; as long as the approximation quality is good, then statistical properties of the complex model are reflected in the interpretable model. We show how model extraction can be used to understand and debug random forests and neural nets trained on several datasets from the UCI Machine Learning Repository, as well as control policies learned for several classical reinforcement learning problems.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {⛔ No DOI found,Computer Science - Computers and Society,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\Interpretability via Model Extraction _Bastani et al 22.pdf}
}

@inproceedings{bastings2019interpretable,
  title = {Interpretable {{Neural Predictions}} with {{Differentiable Binary Variables}}},
  booktitle = {Proceedings of the 57th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}}},
  author = {Bastings, Joost and Aziz, Wilker and Titov, Ivan},
  year = {2019},
  pages = {2963--2977},
  publisher = {{Association for Computational Linguistics}},
  address = {{Florence, Italy}},
  doi = {10.18653/v1/P19-1284},
  urldate = {2022-11-01},
  langid = {english},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\Interpretable Neural Predictions with Differentiable Binary Variables _Bastings et al 2019.pdf}
}

@inproceedings{bastings2020elephant,
  title = {The Elephant in the Interpretability Room: {{Why}} Use Attention as Explanation When We Have Saliency Methods?},
  shorttitle = {The Elephant in the Interpretability Room},
  booktitle = {Proceedings of the {{Third BlackboxNLP Workshop}} on {{Analyzing}} and {{Interpreting Neural Networks}} for {{NLP}}},
  author = {Bastings, Jasmijn and Filippova, Katja},
  year = {2020},
  pages = {149--155},
  publisher = {{Association for Computational Linguistics}},
  address = {{Online}},
  doi = {10.18653/v1/2020.blackboxnlp-1.14},
  urldate = {2022-07-14},
  langid = {english},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\The elephant in the interpretability room _Bastings_Filippova 22.pdf}
}

@article{bates1995models,
  title = {Models of Natural Language Understanding.},
  author = {Bates, M.},
  year = {1995},
  month = oct,
  journal = {Proceedings of the National Academy of Sciences},
  volume = {92},
  number = {22},
  pages = {9977--9982},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.92.22.9977},
  urldate = {2022-03-07},
  langid = {english},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\Models of natural language understanding _Bates 12.pdf}
}

@article{baxendale1958machinemade,
  title = {Machine-{{Made Index}} for {{Technical Literature}}\textemdash{{An Experiment}}},
  author = {Baxendale, P. B.},
  year = {1958},
  month = oct,
  journal = {IBM Journal of Research and Development},
  volume = {2},
  number = {4},
  pages = {354--361},
  issn = {0018-8646, 0018-8646},
  doi = {10.1147/rd.24.0354},
  urldate = {2022-08-27},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\Machine-Made Index for Technical Literature—An Experiment _Baxendale 1958.pdf}
}

@inproceedings{beck2018graphtosequence,
  title = {Graph-to-{{Sequence Learning}} Using {{Gated Graph Neural Networks}}},
  booktitle = {Proceedings of the 56th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}} ({{Volume}} 1: {{Long Papers}})},
  author = {Beck, Daniel and Haffari, Gholamreza and Cohn, Trevor},
  year = {2018},
  pages = {273--283},
  publisher = {{Association for Computational Linguistics}},
  address = {{Melbourne, Australia}},
  doi = {10.18653/v1/P18-1026},
  urldate = {2022-06-10},
  langid = {english},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\Graph-to-Sequence Learning using Gated Graph Neural Networks _Beck et al 22.pdf}
}

@inproceedings{belinkov2018synthetic,
  title = {Synthetic and Natural Noise Both Break Neural Machine Translation},
  booktitle = {International Conference on Learning Representations},
  author = {Belinkov, Yonatan and Bisk, Yonatan},
  year = {2018},
  keywords = {⛔ No DOI found},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\Synthetic and natural noise both break neural machine translation _Belinkov_Bisk 22.pdf}
}

@inproceedings{beltagy2019scibert,
  title = {{{SciBERT}}: {{A Pretrained Language Model}} for {{Scientific Text}}},
  shorttitle = {{{SciBERT}}},
  booktitle = {Proceedings of the 2019 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}} and the 9th {{International Joint Conference}} on {{Natural Language Processing}} ({{EMNLP-IJCNLP}})},
  author = {Beltagy, Iz and Lo, Kyle and Cohan, Arman},
  year = {2019},
  pages = {3613--3618},
  publisher = {{Association for Computational Linguistics}},
  address = {{Hong Kong, China}},
  doi = {10.18653/v1/D19-1371},
  urldate = {2022-11-04},
  langid = {english},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\SciBERT _Beltagy et al 2019.pdf}
}

@article{beltagy2020longformer,
  title = {Longformer: {{The Long-Document Transformer}}},
  shorttitle = {Longformer},
  author = {Beltagy, Iz and Peters, Matthew E. and Cohan, Arman},
  year = {2020},
  journal = {ArXiv},
  publisher = {{arXiv}},
  doi = {10.48550/ARXIV.2004.05150},
  urldate = {2022-08-16},
  abstract = {Transformer-based models are unable to process long sequences due to their self-attention operation, which scales quadratically with the sequence length. To address this limitation, we introduce the Longformer with an attention mechanism that scales linearly with sequence length, making it easy to process documents of thousands of tokens or longer. Longformer's attention mechanism is a drop-in replacement for the standard self-attention and combines a local windowed attention with a task motivated global attention. Following prior work on long-sequence transformers, we evaluate Longformer on character-level language modeling and achieve state-of-the-art results on text8 and enwik8. In contrast to most prior work, we also pretrain Longformer and finetune it on a variety of downstream tasks. Our pretrained Longformer consistently outperforms RoBERTa on long document tasks and sets new state-of-the-art results on WikiHop and TriviaQA. We finally introduce the Longformer-Encoder-Decoder (LED), a Longformer variant for supporting long document generative sequence-to-sequence tasks, and demonstrate its effectiveness on the arXiv summarization dataset.},
  copyright = {arXiv.org perpetual, non-exclusive license},
  keywords = {Computation and Language (cs.CL),FOS: Computer and information sciences},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\Longformer _Beltagy et al 2020.pdf}
}

@article{ben2010atheory,
  title = {A Theory of Learning from Different Domains},
  author = {{Ben-David}, Shai and Blitzer, John and Crammer, Koby and Kulesza, Alex and Pereira, Fernando and Vaughan, Jennifer Wortman},
  year = {2010},
  month = may,
  journal = {Machine Learning},
  volume = {79},
  number = {1-2},
  pages = {151--175},
  issn = {0885-6125, 1573-0565},
  doi = {10.1007/s10994-009-5152-4},
  urldate = {2022-09-19},
  langid = {english},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\A theory of learning from different domains _Ben-David et al 2010.pdf}
}

@article{besserve2020counterfactuals,
  title = {{{COUNTERFACTUALS UNCOVER THE MODULAR STRUCTURE OF DEEP GENERATIVE MODELS}}},
  author = {Besserve, Michel and Mehrjou, Arash and Sun, Remy and Scholkopf, Bernhard},
  year = {2020},
  pages = {26},
  abstract = {Deep generative models can emulate the perceptual properties of complex image datasets, providing a latent representation of the data. However, manipulating such representation to perform meaningful and controllable transformations in the data space remains challenging without some form of supervision. While previous work has focused on exploiting statistical independence to disentangle latent factors, we argue that such requirement can be advantageously relaxed and propose instead a non-statistical framework that relies on identifying a modular organization of the network, based on counterfactual manipulations. Our experiments support that modularity between groups of channels is achieved to a certain degree on a variety of generative models. This allowed the design of targeted interventions on complex image datasets, opening the way to applications such as computationally efficient style transfer and the automated assessment of robustness to contextual changes in pattern recognition systems.},
  langid = {english},
  keywords = {⛔ No DOI found},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\COUNTERFACTUALS UNCOVER THE MODULAR STRUCTURE OF DEEP GENERATIVE MODELS _Besserve et al 22.pdf}
}

@inproceedings{bevilacqua2020generationary,
  title = {Generationary or ``{{How We Went}} beyond {{Word Sense Inventories}} and {{Learned}} to {{Gloss}}''},
  booktitle = {Proceedings of the 2020 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}} ({{EMNLP}})},
  author = {Bevilacqua, Michele and Maru, Marco and Navigli, Roberto},
  year = {2020},
  pages = {7207--7221},
  publisher = {{Association for Computational Linguistics}},
  address = {{Online}},
  doi = {10.18653/v1/2020.emnlp-main.585},
  urldate = {2022-05-24},
  langid = {english},
  keywords = {seq2seq WSD},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\Generationary or “How We Went beyond Word Sense Inventories and Learned to _Bevilacqua et al 22.pdf}
}

@article{bevilacqua2021recent,
  title = {Recent Trends in Word Sense Disambiguation: {{A}} Survey},
  author = {Bevilacqua, Michele and Pasini, Tommaso and Raganato, Alessandro and Navigli, Roberto},
  doi = {10.24963/ijcai.2021/593},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\Recent trends in word sense disambiguation _Bevilacqua et al2.pdf}
}

@inproceedings{blevins2021fews,
  title = {{{FEWS}}: {{Large-Scale}}, {{Low-Shot Word Sense Disambiguation}} with the {{Dictionary}}},
  shorttitle = {{{FEWS}}},
  booktitle = {Proceedings of the 16th {{Conference}} of the {{European Chapter}} of the {{Association}} for {{Computational Linguistics}}: {{Main Volume}}},
  author = {Blevins, Terra and Joshi, Mandar and Zettlemoyer, Luke},
  year = {2021},
  pages = {455--465},
  publisher = {{Association for Computational Linguistics}},
  address = {{Online}},
  doi = {10.18653/v1/2021.eacl-main.36},
  urldate = {2022-05-02},
  langid = {english},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\FEWS _Blevins et al 22.pdf}
}

@inproceedings{bloedorn1997multidocument,
  title = {Multi-Document Summarization by Graph Search and Matching},
  booktitle = {Proceedings of the Fourteenth National Conference on Artificial Intelligence and Ninth Conference on Innovative Applications of Artificial Intelligence},
  author = {Mani, Inderjeet and Bloedorn, Eric},
  year = {1997},
  series = {{{AAAI}}'97/{{IAAI}}'97},
  pages = {622--628},
  publisher = {{AAAI Press}},
  address = {{Providence, Rhode Island}},
  abstract = {We describe a new method for summarizing similarities and differences in a pair of related documents using a graph representation for text. Concepts denoted by words, phrases, and proper names in the document are represented positionally as nodes in the graph along with edges corresponding to semantic relations between items. Given a perspective in terms of which the pair of documents is to be summarized, the algorithm first uses a spreading activation technique to discover, in each document, nodes semantically related to the topic. The activated graphs of each document are then matched to yield a graph corresponding to similarities and differences between the pair, which is rendered in natural language. An evaluation of these techniques has been carried out.},
  isbn = {0-262-51095-2}
}

@article{blondel2008fast,
  title = {Fast Unfolding of Communities in Large Networks},
  author = {Blondel, Vincent D and Guillaume, Jean-Loup and Lambiotte, Renaud and Lefebvre, Etienne},
  year = {2008},
  month = oct,
  journal = {Journal of Statistical Mechanics: Theory and Experiment},
  volume = {2008},
  number = {10},
  pages = {P10008},
  issn = {1742-5468},
  doi = {2008101003130400},
  urldate = {2022-09-09},
  keywords = {⚠️ Invalid DOI},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\Fast unfolding of communities in large networks _Blondel et al 2008.pdf}
}

@inproceedings{boggust2022shared,
  title = {Shared {{Interest}}: {{Measuring Human-AI Alignment}} to {{Identify Recurring Patterns}} in {{Model Behavior}}},
  shorttitle = {Shared {{Interest}}},
  booktitle = {{{CHI Conference}} on {{Human Factors}} in {{Computing Systems}}},
  author = {Boggust, Angie and Hoover, Benjamin and Satyanarayan, Arvind and Strobelt, Hendrik},
  year = {2022},
  month = apr,
  pages = {1--17},
  publisher = {{ACM}},
  address = {{New Orleans LA USA}},
  doi = {10.1145/3491102.3501965},
  urldate = {2022-06-03},
  isbn = {978-1-4503-9157-3},
  langid = {english},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\Shared Interest _Boggust et al 22.pdf}
}

@article{bommasaniopportunities,
  title = {On the {{Opportunities}} and {{Risks}} of {{Foundation Models}}},
  author = {Bommasani, Rishi and Hudson, Drew A and Altman, Ehsan Adeli Russ and Arora, Simran},
  pages = {212},
  langid = {english},
  keywords = {⛔ No DOI found},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\On the Opportunities and Risks of Foundation Models _Bommasani et al2.pdf}
}

@inproceedings{borkan2019nuanced,
  title = {Nuanced {{Metrics}} for {{Measuring Unintended Bias}} with {{Real Data}} for {{Text Classification}}},
  booktitle = {Companion {{Proceedings}} of {{The}} 2019 {{World Wide Web Conference}}},
  author = {Borkan, Daniel and Dixon, Lucas and Sorensen, Jeffrey and Thain, Nithum and Vasserman, Lucy},
  year = {2019},
  month = may,
  pages = {491--500},
  publisher = {{ACM}},
  address = {{San Francisco USA}},
  doi = {10.1145/3308560.3317593},
  urldate = {2022-11-09},
  isbn = {978-1-4503-6675-5},
  langid = {english},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\Nuanced Metrics for Measuring Unintended Bias with Real Data for Text _Borkan et al 2019.pdf}
}

@inproceedings{brazinskas2021learning,
  title = {Learning {{Opinion Summarizers}} by {{Selecting Informative Reviews}}},
  booktitle = {Proceedings of the 2021 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}}},
  author = {Bra{\v z}inskas, Arthur and Lapata, Mirella and Titov, Ivan},
  year = {2021},
  pages = {9424--9442},
  publisher = {{Association for Computational Linguistics}},
  address = {{Online and Punta Cana, Dominican Republic}},
  doi = {10.18653/v1/2021.emnlp-main.743},
  urldate = {2022-11-07},
  langid = {english},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\Learning Opinion Summarizers by Selecting Informative Reviews _Bražinskas et al 2021.pdf}
}

@inproceedings{brazinskas2021learninga,
  title = {Learning {{Opinion Summarizers}} by {{Selecting Informative Reviews}}},
  booktitle = {Proceedings of the 2021 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}}},
  author = {Bra{\v z}inskas, Arthur and Lapata, Mirella and Titov, Ivan},
  year = {2021},
  pages = {9424--9442},
  publisher = {{Association for Computational Linguistics}},
  address = {{Online and Punta Cana, Dominican Republic}},
  doi = {10.18653/v1/2021.emnlp-main.743},
  urldate = {2023-02-20},
  langid = {english},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\Learning Opinion Summarizers by Selecting Informative Reviews _Bražinskas et al 22.pdf}
}

@article{brown1992class,
  title = {Class-Based n-Gram Models of Natural Language},
  author = {Brown, Peter F and Della Pietra, Vincent J and Desouza, Peter V and Lai, Jennifer C and Mercer, Robert L},
  year = {1992},
  journal = {Computational linguistics},
  volume = {18},
  number = {4},
  pages = {467--480},
  keywords = {⛔ No DOI found},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\Class-based n-gram models of natural language _Brown et al 12.pdf}
}

@article{buhlmann2020invariance,
  title = {Invariance, {{Causality}} and {{Robustness}}},
  author = {B{\"u}hlmann, Peter},
  year = {2020},
  month = aug,
  journal = {Statistical Science},
  volume = {35},
  number = {3},
  issn = {0883-4237},
  doi = {10.1214/19-STS721},
  urldate = {2022-07-16},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\Invariance, Causality and Robustness _Bühlmann 22.pdf}
}

@article{cabrera2022what,
  title = {What {{Did My AI Learn}}? {{How Data Scientists Make Sense}} of {{Model Behavior}}},
  shorttitle = {What {{Did My AI Learn}}?},
  author = {Cabrera, {\'A}ngel Alexander and Ribeiro, Marco Tulio and Lee, Bongshin and DeLine, Rob and Perer, Adam and Drucker, Steven M.},
  year = {2022},
  month = jun,
  journal = {ACM Transactions on Computer-Human Interaction},
  pages = {3542921},
  issn = {1073-0516, 1557-7325},
  doi = {10.1145/3542921},
  urldate = {2022-11-23},
  abstract = {Data scientists require rich mental models of how AI systems behave to effectively train, debug, and work with them. Despite the prevalence of AI analysis tools, there is no general theory describing how people make sense of what their models have learned. We frame this process as a form of sensemaking and derive a framework describing how data scientists develop mental models of AI behavior. To evaluate the framework, we show how existing AI analysis tools fit into this sensemaking process and use it to design               AIFinnity               , a system for analyzing image-and-text models. Lastly, we explored how data scientists use a tool developed with the framework through a think-aloud study with 10 data scientists tasked with using               AIFinnity               to pick an image captioning model. We found that               AIFinnity               's sensemaking workflow reflected participants' mental processes and enabled them to discover and validate diverse AI behaviors.},
  langid = {english},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\What Did My AI Learn _Cabrera et al 2022.pdf}
}

@inproceedings{cai2020amr,
  title = {{{AMR Parsing}} via {{Graph-Sequence Iterative Inference}}},
  booktitle = {Proceedings of the 58th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}}},
  author = {Cai, Deng and Lam, Wai},
  year = {2020},
  pages = {1290--1301},
  publisher = {{Association for Computational Linguistics}},
  address = {{Online}},
  doi = {10.18653/v1/2020.acl-main.119},
  urldate = {2022-06-09},
  langid = {english},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\AMR Parsing via Graph-Sequence Iterative Inference _Cai_Lam 22.pdf}
}

@article{cai2020graph,
  title = {Graph {{Transformer}} for {{Graph-to-Sequence Learning}}},
  author = {Cai, Deng and Lam, Wai},
  year = {2020},
  month = apr,
  journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
  volume = {34},
  number = {05},
  pages = {7464--7471},
  issn = {2374-3468, 2159-5399},
  doi = {10.1609/aaai.v34i05.6243},
  urldate = {2022-06-10},
  abstract = {The dominant graph-to-sequence transduction models employ graph neural networks for graph representation learning, where the structural information is reflected by the receptive field of neurons. Unlike graph neural networks that restrict the information exchange between immediate neighborhood, we propose a new model, known as Graph Transformer, that uses explicit relation encoding and allows direct communication between two distant nodes. It provides a more efficient way for global graph structure modeling. Experiments on the applications of text generation from Abstract Meaning Representation (AMR) and syntax-based neural machine translation show the superiority of our proposed model. Specifically, our model achieves 27.4 BLEU on LDC2015E86 and 29.7 BLEU on LDC2017T10 for AMR-to-text generation, outperforming the state-of-the-art results by up to 2.2 points. On the syntax-based translation tasks, our model establishes new single-model state-of-the-art BLEU scores, 21.3 for English-to-German and 14.1 for English-to-Czech, improving over the existing best results, including ensembles, by over 1 BLEU.},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\Graph Transformer for Graph-to-Sequence Learning _Cai_Lam 22.pdf}
}

@article{cai2022survey,
  title = {A {{Survey}} on {{Deep Reinforcement Learning}} for {{Data Processing}} and {{Analytics}}},
  author = {Cai, Qingpeng and Cui, Can and Xiong, Yiyuan and Wang, Wei and Xie, Zhongle and Zhang, Meihui},
  year = {2022},
  journal = {IEEE Transactions on Knowledge and Data Engineering},
  pages = {1--1},
  issn = {1041-4347, 1558-2191, 2326-3865},
  doi = {10.1109/TKDE.2022.3155196},
  urldate = {2023-02-20},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\A Survey on Deep Reinforcement Learning for Data Processing and Analytics _Cai et al 2022.pdf}
}

@article{caliskan2017semantics,
  title = {Semantics Derived Automatically from Language Corpora Contain Human-like Biases},
  author = {Caliskan, Aylin and Bryson, Joanna J. and Narayanan, Arvind},
  year = {2017},
  month = apr,
  journal = {Science},
  volume = {356},
  number = {6334},
  pages = {183--186},
  issn = {0036-8075, 1095-9203},
  doi = {10.1126/science.aal4230},
  urldate = {2022-07-12},
  abstract = {Machines learn what people know implicitly                            AlphaGo has demonstrated that a machine can learn how to do things that people spend many years of concentrated study learning, and it can rapidly learn how to do them better than any human can. Caliskan               et al.               now show that machines can learn word associations from written texts and that these associations mirror those learned by humans, as measured by the Implicit Association Test (IAT) (see the Perspective by Greenwald). Why does this matter? Because the IAT has predictive value in uncovering the association between concepts, such as pleasantness and flowers or unpleasantness and insects. It can also tease out attitudes and beliefs\textemdash for example, associations between female names and family or male names and career. Such biases may not be expressed explicitly, yet they can prove influential in behavior.                                         Science               , this issue p.               183               ; see also p.               133                        ,              Computers can learn which words go together more or less often and can thus mimic human performance on a test of implicit bias.           ,              Machine learning is a means to derive artificial intelligence by discovering patterns in existing data. Here, we show that applying machine learning to ordinary human language results in human-like semantic biases. We replicated a spectrum of known biases, as measured by the Implicit Association Test, using a widely used, purely statistical machine-learning model trained on a standard corpus of text from the World Wide Web. Our results indicate that text corpora contain recoverable and accurate imprints of our historic biases, whether morally neutral as toward insects or flowers, problematic as toward race or gender, or even simply veridical, reflecting the status quo distribution of gender with respect to careers or first names. Our methods hold promise for identifying and addressing sources of bias in culture, including technology.},
  langid = {english},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\Semantics derived automatically from language corpora contain human-like biases _Caliskan et al 22.pdf}
}

@article{camburuesnli,
  title = {E-{{SNLI}}: {{Natural Language Inference}} with {{Natural Language Explanations}}},
  author = {Camburu, Oana-Maria and Rockt{\"a}schel, Tim and Lukasiewicz, Thomas and Blunsom, Phil},
  pages = {11},
  abstract = {In order for machine learning to garner widespread public adoption, models must be able to provide interpretable and robust explanations for their decisions, as well as learn from human-provided explanations at train time. In this work, we extend the Stanford Natural Language Inference dataset with an additional layer of human-annotated natural language explanations of the entailment relations. We further implement models that incorporate these explanations into their training process and output them at test time. We show how our corpus of explanations, which we call e-SNLI, can be used for various goals, such as obtaining full sentence justifications of a model's decisions, improving universal sentence representations and transferring to out-of-domain NLI datasets. Our dataset1 thus opens up a range of research directions for using natural language explanations, both for improving models and for asserting their trust.},
  langid = {english},
  keywords = {⛔ No DOI found},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\e-SNLI _Camburu et al2.pdf}
}

@inproceedings{campolungo2022dibimt,
  title = {{{DiBiMT}}: {{A Novel Benchmark}} for {{Measuring Word Sense Disambiguation Biases}} in {{Machine Translation}}},
  shorttitle = {{{DiBiMT}}},
  booktitle = {Proceedings of the 60th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}} ({{Volume}} 1: {{Long Papers}})},
  author = {Campolungo, Niccol{\`o} and Martelli, Federico and Saina, Francesco and Navigli, Roberto},
  year = {2022},
  pages = {4331--4352},
  publisher = {{Association for Computational Linguistics}},
  address = {{Dublin, Ireland}},
  doi = {10.18653/v1/2022.acl-long.298},
  urldate = {2022-11-04},
  langid = {english},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\DiBiMT _Campolungo et al 2022.pdf}
}

@inproceedings{cao2019question,
  title = {Question {{Answering}} by {{Reasoning Across Documents}} with {{Graph Convolutional Networks}}},
  booktitle = {Proceedings of the 2019 {{Conference}} of the {{North American Chapter}} of the {{Association}} for {{Computational Linguistics}}: {{Human Language Technologies}}, {{Volume}} 1 ({{Long}} and {{Short Papers}})},
  author = {Cao, Nicola De and Aziz, Wilker and Titov, Ivan},
  year = {2019},
  pages = {2306--2317},
  doi = {10.18653/v1/N19-1240},
  keywords = {⛔ No DOI found},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\Question Answering by Reasoning Across Documents with Graph Convolutional _Cao et al 2019.pdf}
}

@inproceedings{cao2021autoregressive,
  title = {Autoregressive Entity Retrieval},
  booktitle = {International Conference on Learning Representations},
  author = {Cao, Nicola De and Izacard, Gautier and Riedel, Sebastian and Petroni, Fabio},
  year = {2021},
  keywords = {⛔ No DOI found},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\Autoregressive entity retrieval _Cao et al 22.pdf}
}

@inproceedings{carlsson2022finegrained,
  title = {Fine-{{Grained Controllable Text Generation Using Non-Residual Prompting}}},
  booktitle = {Proceedings of the 60th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}} ({{Volume}} 1: {{Long Papers}})},
  author = {Carlsson, Fredrik and {\"O}hman, Joey and Liu, Fangyu and Verlinden, Severine and Nivre, Joakim and Sahlgren, Magnus},
  year = {2022},
  pages = {6837--6857},
  publisher = {{Association for Computational Linguistics}},
  address = {{Dublin, Ireland}},
  doi = {10.18653/v1/2022.acl-long.471},
  urldate = {2022-06-09},
  langid = {english},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\Fine-Grained Controllable Text Generation Using Non-Residual Prompting _Carlsson et al 2022.pdf}
}

@inproceedings{caruana2015intelligible,
  title = {Intelligible {{Models}} for {{HealthCare}}: {{Predicting Pneumonia Risk}} and {{Hospital}} 30-Day {{Readmission}}},
  shorttitle = {Intelligible {{Models}} for {{HealthCare}}},
  booktitle = {Proceedings of the 21th {{ACM SIGKDD International Conference}} on {{Knowledge Discovery}} and {{Data Mining}}},
  author = {Caruana, Rich and Lou, Yin and Gehrke, Johannes and Koch, Paul and Sturm, Marc and Elhadad, Noemie},
  year = {2015},
  month = aug,
  pages = {1721--1730},
  publisher = {{ACM}},
  address = {{Sydney NSW Australia}},
  doi = {10.1145/2783258.2788613},
  urldate = {2022-03-20},
  isbn = {978-1-4503-3664-2},
  langid = {english},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\Intelligible Models for HealthCare _Caruana et al 22.pdf}
}

@inproceedings{chalkidis2019neural,
  title = {Neural {{Legal Judgment Prediction}} in {{English}}},
  booktitle = {Proceedings of the 57th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}}},
  author = {Chalkidis, Ilias and Androutsopoulos, Ion and Aletras, Nikolaos},
  year = {2019},
  pages = {4317--4323},
  publisher = {{Association for Computational Linguistics}},
  address = {{Florence, Italy}},
  doi = {10.18653/v1/P19-1424},
  urldate = {2022-08-15},
  langid = {english},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\Neural Legal Judgment Prediction in English _Chalkidis et al 22.pdf}
}

@inproceedings{chalkidis2020empirical,
  title = {An {{Empirical Study}} on {{Large-Scale Multi-Label Text Classification Including Few}} and {{Zero-Shot Labels}}},
  booktitle = {Proceedings of the 2020 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}} ({{EMNLP}})},
  author = {Chalkidis, Ilias and Fergadiotis, Manos and Kotitsas, Sotiris and Malakasiotis, Prodromos and Aletras, Nikolaos and Androutsopoulos, Ion},
  year = {2020},
  pages = {7503--7515},
  publisher = {{Association for Computational Linguistics}},
  address = {{Online}},
  doi = {10.18653/v1/2020.emnlp-main.607},
  urldate = {2022-06-20},
  langid = {english},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\An Empirical Study on Large-Scale Multi-Label Text Classification Including Few _Chalkidis et al 22.pdf}
}

@inproceedings{chalkidis2020legalbert,
  title = {{{LEGAL-BERT}}: {{The Muppets}} Straight out of {{Law School}}},
  shorttitle = {{{LEGAL-BERT}}},
  booktitle = {Findings of the {{Association}} for {{Computational Linguistics}}: {{EMNLP}} 2020},
  author = {Chalkidis, Ilias and Fergadiotis, Manos and Malakasiotis, Prodromos and Aletras, Nikolaos and Androutsopoulos, Ion},
  year = {2020},
  pages = {2898--2904},
  publisher = {{Association for Computational Linguistics}},
  address = {{Online}},
  doi = {10.18653/v1/2020.findings-emnlp.261},
  urldate = {2022-08-10},
  langid = {english},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\LEGAL-BERT _Chalkidis et al 22.pdf}
}

@inproceedings{chalkidis2022fairlex,
  title = {{{FairLex}}: {{A Multilingual Benchmark}} for {{Evaluating Fairness}} in {{Legal Text Processing}}},
  shorttitle = {{{FairLex}}},
  booktitle = {Proceedings of the 60th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}} ({{Volume}} 1: {{Long Papers}})},
  author = {Chalkidis, Ilias and Pasini, Tommaso and Zhang, Sheng and Tomada, Letizia and Schwemer, Sebastian and S{\o}gaard, Anders},
  year = {2022},
  pages = {4389--4406},
  publisher = {{Association for Computational Linguistics}},
  address = {{Dublin, Ireland}},
  doi = {10.18653/v1/2022.acl-long.301},
  urldate = {2022-07-21},
  langid = {english},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\FairLex _Chalkidis et al 22.pdf}
}

@inproceedings{chalkidis2022lexglue,
  title = {{{LexGLUE}}: {{A Benchmark Dataset}} for {{Legal Language Understanding}} in {{English}}},
  shorttitle = {{{LexGLUE}}},
  booktitle = {Proceedings of the 60th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}} ({{Volume}} 1: {{Long Papers}})},
  author = {Chalkidis, Ilias and Jana, Abhik and Hartung, Dirk and Bommarito, Michael and Androutsopoulos, Ion and Katz, Daniel and Aletras, Nikolaos},
  year = {2022},
  pages = {4310--4330},
  publisher = {{Association for Computational Linguistics}},
  address = {{Dublin, Ireland}},
  doi = {10.18653/v1/2022.acl-long.297},
  urldate = {2022-06-15},
  langid = {english},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\LexGLUE _Chalkidis et al 4.pdf}
}

@inproceedings{chalkidis2022lexgluea,
  title = {{{LexGLUE}}: {{A Benchmark Dataset}} for {{Legal Language Understanding}} in {{English}}},
  shorttitle = {{{LexGLUE}}},
  booktitle = {Proceedings of the 60th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}} ({{Volume}} 1: {{Long Papers}})},
  author = {Chalkidis, Ilias and Jana, Abhik and Hartung, Dirk and Bommarito, Michael and Androutsopoulos, Ion and Katz, Daniel and Aletras, Nikolaos},
  year = {2022},
  pages = {4310--4330},
  publisher = {{Association for Computational Linguistics}},
  address = {{Dublin, Ireland}},
  doi = {10.18653/v1/2022.acl-long.297},
  urldate = {2022-07-21},
  langid = {english},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\LexGLUE _Chalkidis et al 3.pdf}
}

@inproceedings{chan2011exploiting,
  title = {Exploiting Syntactico-Semantic Structures for Relation Extraction},
  booktitle = {Proceedings of the 49th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}}: {{Human Language Technologies}}},
  author = {Chan, Yee Seng and Roth, Dan},
  year = {2011},
  pages = {551--560},
  keywords = {⛔ No DOI found},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\Exploiting syntactico-semantic structures for relation extraction _Chan_Roth 22.pdf}
}

@article{chatterjeegeneralization,
  title = {{{ON THE GENERALIZATION MYSTERY IN DEEP LEARNING}}},
  author = {Chatterjee, Satrajit and Zielinski, Piotr},
  pages = {82},
  abstract = {The generalization mystery in deep learning is the following: Why do over-parameterized neural networks trained with gradient descent (GD) generalize well on real datasets even though they are capable of fitting random datasets of comparable size? Furthermore, from among all solutions that fit the training data, how does GD find one that generalizes well (when such a well-generalizing solution exists)? We argue that the answer to both questions lies in the interaction of the gradients of different examples during training. Intuitively, if the per-example gradients are well-aligned, that is, if they are coherent, then one may expect GD to be (algorithmically) stable, and hence generalize well. We formalize this argument with an easy to compute and interpretable metric for coherence, and show that the metric takes on very different values on real and random datasets for several common vision networks. The theory also explains a number of other phenomena in deep learning, such as why some examples are reliably learned earlier than others, why early stopping works, and why it is possible to learn from noisy labels. Moreover, since the theory provides a causal explanation of how GD finds a well-generalizing solution when one exists, it motivates a class of simple modifications to GD that attenuate memorization and improve generalization.},
  langid = {english},
  keywords = {⛔ No DOI found},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\ON THE GENERALIZATION MYSTERY IN DEEP LEARNING _Chatterjee_Zielinski2.pdf}
}

@inproceedings{chen2018fast,
  title = {Fast {{Abstractive Summarization}} with {{Reinforce-Selected Sentence Rewriting}}},
  booktitle = {Proceedings of the 56th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}} ({{Volume}} 1: {{Long Papers}})},
  author = {Chen, Yen-Chun and Bansal, Mohit},
  year = {2018},
  pages = {675--686},
  publisher = {{Association for Computational Linguistics}},
  address = {{Melbourne, Australia}},
  doi = {10.18653/v1/P18-1063},
  urldate = {2022-08-20},
  langid = {english},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\Fast Abstractive Summarization with Reinforce-Selected Sentence Rewriting _Chen_Bansal 2018.pdf}
}

@inproceedings{chen2019codah,
  title = {{{CODAH}}: {{An}} Adversarially-Authored Question Answering Dataset for Common Sense},
  booktitle = {Proceedings of the 3rd Workshop on Evaluating Vector Space Representations for {{NLP}}},
  author = {Chen, Michael and D'Arcy, Mike and Liu, Alisa and Fernandez, Jared and Downey, Doug},
  year = {2019},
  month = jun,
  pages = {63--69},
  publisher = {{Association for Computational Linguistics}},
  address = {{Minneapolis, USA}},
  doi = {10.18653/v1/W19-2008},
  abstract = {Commonsense reasoning is a critical AI capability, but it is difficult to construct challenging datasets that test common sense. Recent neural question answering systems, based on large pre-trained models of language, have already achieved near-human-level performance on commonsense knowledge benchmarks. These systems do not possess human-level common sense, but are able to exploit limitations of the datasets to achieve human-level scores. We introduce the CODAH dataset, an adversarially-constructed evaluation dataset for testing common sense. CODAH forms a challenging extension to the recently-proposed SWAG dataset, which tests commonsense knowledge using sentence-completion questions that describe situations observed in video. To produce a more difficult dataset, we introduce a novel procedure for question acquisition in which workers author questions designed to target weaknesses of state-of-the-art neural question answering systems. Workers are rewarded for submissions that models fail to answer correctly both before and after fine-tuning (in cross-validation). We create 2.8k questions via this procedure and evaluate the performance of multiple state-of-the-art question answering systems on our dataset. We observe a significant gap between human performance, which is 95.3\%, and the performance of the best baseline accuracy of 65.3\% by the OpenAI GPT model.},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\CODAH _Chen et al 2019.pdf}
}

@inproceedings{chen2019no,
  title = {[{{No}} Title Found]},
  booktitle = {Proceedings of the 3rd {{Workshop}} on {{Evaluating Vector Space Representations}} For},
  author = {Chen, Michael and D'Arcy, Mike and Liu, Alisa and Fernandez, Jared and Downey, Doug},
  year = {2019},
  pages = {63--69},
  publisher = {{Association for Computational Linguistics}},
  address = {{Minneapolis, USA}},
  doi = {10.18653/v1/W19-2008},
  urldate = {2022-07-13},
  langid = {english},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\[No title found] _Chen et al 2019.pdf}
}

@inproceedings{chen2020generating,
  title = {Generating {{Hierarchical Explanations}} on {{Text Classification}} via {{Feature Interaction Detection}}},
  booktitle = {Proceedings of the 58th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}}},
  author = {Chen, Hanjie and Zheng, Guangtao and Ji, Yangfeng},
  year = {2020},
  pages = {5578--5593},
  publisher = {{Association for Computational Linguistics}},
  address = {{Online}},
  doi = {10.18653/v1/2020.acl-main.494},
  urldate = {2022-07-13},
  langid = {english},
  keywords = {permutation-based},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\Generating Hierarchical Explanations on Text Classification via Feature _Chen et al 22.pdf}
}

@misc{chen2021lightner,
  title = {{{LightNER}}: {{A Lightweight Generative Framework}} with {{Prompt-guided Attention}} for {{Low-resource NER}}},
  shorttitle = {{{LightNER}}},
  author = {Chen, Xiang and Zhang, Ningyu and Li, Lei and Xie, Xin and Deng, Shumin and Tan, Chuanqi and Huang, Fei and Si, Luo and Chen, Huajun},
  year = {2021},
  month = sep,
  number = {arXiv:2109.00720},
  eprint = {arXiv:2109.00720},
  publisher = {{arXiv}},
  urldate = {2022-05-24},
  abstract = {Most existing NER methods rely on extensive labeled data for model training, which struggles in the low-resource scenarios with limited training data. Recently, prompt-tuning methods for pre-trained language models have achieved remarkable performance in few-shot learning by exploiting prompts as task guidance to reduce the gap between training progress and downstream tuning. Inspired by prompt learning, we propose a novel lightweight generative framework with promptguided attention for low-resource NER (LightNER). Specifically, we construct the semantic-aware answer space of entity categories for prompt learning to generate the entity span sequence and entity categories without any label-specific classifiers. We further propose prompt-guided attention by incorporating continuous prompts into the self-attention layer to re-modulate the attention and adapt pre-trained weights. Note that we only tune those continuous prompts with the whole parameter of the pre-trained language model fixed, thus, making our approach lightweight and flexible for low-resource scenarios and can better transfer knowledge across domains. Experimental results show that LightNER can obtain comparable performance in the standard supervised setting and outperform strong baselines in low-resource settings by tuning only a small part of the parameters.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Computation and Language},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\LightNER _Chen et al 22.pdf}
}

@article{chernozhukov2018double,
  title = {Double/Debiased Machine Learning for Treatment and Structural Parameters},
  author = {Chernozhukov, Victor and Chetverikov, Denis and Demirer, Mert and Duflo, Esther and Hansen, Christian and Newey, Whitney and Robins, James},
  year = {2018},
  month = feb,
  journal = {The Econometrics Journal},
  volume = {21},
  number = {1},
  pages = {C1-C68},
  issn = {1368-4221, 1368-423X},
  doi = {10.1111/ectj.12097},
  urldate = {2022-11-07},
  langid = {english},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\Double-debiased machine learning for treatment and structural parameters _Chernozhukov et al 2018.pdf}
}

@inproceedings{chia2022relationprompt,
  title = {{{RelationPrompt}}: {{Leveraging Prompts}} to {{Generate Synthetic Data}} for {{Zero-Shot Relation Triplet Extraction}}},
  shorttitle = {{{RelationPrompt}}},
  booktitle = {Findings of the {{Association}} for {{Computational Linguistics}}: {{ACL}} 2022},
  author = {Chia, Yew Ken and Bing, Lidong and Poria, Soujanya and Si, Luo},
  year = {2022},
  pages = {45--57},
  publisher = {{Association for Computational Linguistics}},
  address = {{Dublin, Ireland}},
  doi = {10.18653/v1/2022.findings-acl.5},
  urldate = {2022-06-13},
  langid = {english},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\RelationPrompt _Chia et al 4.pdf}
}

@inproceedings{chia2022relationprompta,
  title = {{{RelationPrompt}}: {{Leveraging Prompts}} to {{Generate Synthetic Data}} for {{Zero-Shot Relation Triplet Extraction}}},
  shorttitle = {{{RelationPrompt}}},
  booktitle = {Findings of the {{Association}} for {{Computational Linguistics}}: {{ACL}} 2022},
  author = {Chia, Yew Ken and Bing, Lidong and Poria, Soujanya and Si, Luo},
  year = {2022},
  pages = {45--57},
  publisher = {{Association for Computational Linguistics}},
  address = {{Dublin, Ireland}},
  doi = {10.18653/v1/2022.findings-acl.5},
  urldate = {2022-06-25},
  langid = {english},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\RelationPrompt _Chia et al 3.pdf}
}

@article{child2019generating,
  title = {Generating Long Sequences with Sparse Transformers},
  author = {Child, Rewon and Gray, Scott and Radford, Alec and Sutskever, Ilya},
  year = {2019},
  journal = {arXiv preprint arXiv:1904.10509},
  eprint = {1904.10509},
  archiveprefix = {arxiv},
  keywords = {⛔ No DOI found},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\Generating long sequences with sparse transformers _Child et al 2019.pdf}
}

@article{chow1960tests,
  title = {Tests of {{Equality Between Sets}} of {{Coefficients}} in {{Two Linear Regressions}}},
  author = {Chow, Gregory C.},
  year = {1960},
  month = jul,
  journal = {Econometrica},
  volume = {28},
  number = {3},
  eprint = {1910133},
  eprinttype = {jstor},
  pages = {591},
  issn = {00129682},
  doi = {10.2307/1910133},
  urldate = {2022-09-19},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\Tests of Equality Between Sets of Coefficients in Two Linear Regressions _Chow 1960.pdf}
}

@inproceedings{christopoulou2019connecting,
  title = {Connecting the {{Dots}}: {{Document-level Neural Relation Extraction}} with {{Edge-oriented Graphs}}},
  booktitle = {Proceedings of the 2019 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}} and the 9th {{International Joint Conference}} on {{Natural Language Processing}} ({{EMNLP-IJCNLP}})},
  author = {Christopoulou, Fenia and Miwa, Makoto and Ananiadou, Sophia},
  year = {2019},
  pages = {4924--4935},
  doi = {10.18653/v1/D19-1498},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\Connecting the Dots _Christopoulou et al 22.pdf}
}

@inproceedings{chrysostomou2022empirical,
  title = {An {{Empirical Study}} on {{Explanations}} in {{Out-of-Domain Settings}}},
  booktitle = {Proceedings of the 60th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}} ({{Volume}} 1: {{Long Papers}})},
  author = {Chrysostomou, George and Aletras, Nikolaos},
  year = {2022},
  pages = {6920--6938},
  publisher = {{Association for Computational Linguistics}},
  address = {{Dublin, Ireland}},
  doi = {10.18653/v1/2022.acl-long.477},
  urldate = {2022-10-18},
  langid = {english},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\An Empirical Study on Explanations in Out-of-Domain Settings _Chrysostomou_Aletras 2022.pdf}
}

@article{church2022emerging,
  title = {Emerging {{Trends}}: {{SOTA-Chasing}}},
  shorttitle = {Emerging {{Trends}}},
  author = {Church, Kenneth Ward and Kordoni, Valia},
  year = {2022},
  month = mar,
  journal = {Natural Language Engineering},
  volume = {28},
  number = {2},
  pages = {249--269},
  issn = {1351-3249, 1469-8110},
  doi = {10.1017/S1351324922000043},
  urldate = {2022-05-04},
  abstract = {Abstract             Many papers are chasing state-of-the-art (SOTA) numbers, and more will do so in the future. SOTA-chasing comes with many costs. SOTA-chasing squeezes out more promising opportunities such as coopetition and interdisciplinary collaboration. In addition, there is a risk that too much SOTA-chasing could lead to claims of superhuman performance, unrealistic expectations, and the next AI winter. Two root causes for SOTA-chasing will be discussed: (1) lack of leadership and (2) iffy reviewing processes. SOTA-chasing may be similar to the replication crisis in the scientific literature. The replication crisis is yet another example, like evaluation, of over-confidence in accepted practices and the scientific method, even when such practices lead to absurd consequences.},
  langid = {english},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\Emerging Trends _Church_Kordoni 22.pdf}
}

@inproceedings{clark2016deep,
  title = {Deep {{Reinforcement Learning}} for {{Mention-Ranking Coreference Models}}},
  booktitle = {Proceedings of the 2016 {{Conference}} on {{Empirical Methods}} in {{Natural}}           {{Language Processing}}},
  author = {Clark, Kevin and Manning, Christopher D.},
  year = {2016},
  pages = {2256--2262},
  publisher = {{Association for Computational Linguistics}},
  address = {{Austin, Texas}},
  doi = {10.18653/v1/D16-1245},
  urldate = {2022-08-08},
  langid = {english},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\Deep Reinforcement Learning for Mention-Ranking Coreference Models _Clark_Manning 22.pdf}
}

@inproceedings{clark2019what,
  title = {What {{Does BERT Look}} at? {{An Analysis}} of {{BERT}}'s {{Attention}}},
  shorttitle = {What {{Does BERT Look}} At?},
  booktitle = {Proceedings of the 2019 {{ACL Workshop BlackboxNLP}}: {{Analyzing}} and {{Interpreting Neural Networks}} for {{NLP}}},
  author = {Clark, Kevin and Khandelwal, Urvashi and Levy, Omer and Manning, Christopher D.},
  year = {2019},
  pages = {276--286},
  publisher = {{Association for Computational Linguistics}},
  address = {{Florence, Italy}},
  doi = {10.18653/v1/W19-4828},
  urldate = {2022-10-02},
  langid = {english},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\What Does BERT Look at _Clark et al 2019.pdf}
}

@inproceedings{clark2020learning,
  title = {Learning to {{Model}} and {{Ignore Dataset Bias}} with {{Mixed Capacity Ensembles}}},
  booktitle = {Findings of the {{Association}} for {{Computational Linguistics}}: {{EMNLP}} 2020},
  author = {Clark, Christopher and Yatskar, Mark and Zettlemoyer, Luke},
  year = {2020},
  pages = {3031--3045},
  publisher = {{Association for Computational Linguistics}},
  address = {{Online}},
  doi = {10.18653/v1/2020.findings-emnlp.272},
  urldate = {2022-07-29},
  langid = {english},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\Learning to Model and Ignore Dataset Bias with Mixed Capacity Ensembles _Clark et al 22.pdf}
}

@inproceedings{cohan2018discourseaware,
  title = {A {{Discourse-Aware Attention Model}} for {{Abstractive Summarization}} of {{Long Documents}}},
  booktitle = {Proceedings of the 2018 {{Conference}} of the {{North American Chapter}} of           the {{Association}} for {{Computational Linguistics}}: {{Human Language}}           {{Technologies}}, {{Volume}} 2 ({{Short Papers}})},
  author = {Cohan, Arman and Dernoncourt, Franck and Kim, Doo Soon and Bui, Trung and Kim, Seokhwan and Chang, Walter and Goharian, Nazli},
  year = {2018},
  pages = {615--621},
  publisher = {{Association for Computational Linguistics}},
  address = {{New Orleans, Louisiana}},
  doi = {10.18653/v1/N18-2097},
  urldate = {2022-08-31},
  langid = {english},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\A Discourse-Aware Attention Model for Abstractive Summarization of Long _Cohan et al 2018.pdf}
}

@article{comaniciu2002mean,
  title = {Mean Shift: A Robust Approach toward Feature Space Analysis},
  shorttitle = {Mean Shift},
  author = {Comaniciu, D. and Meer, P.},
  year = {2002},
  month = may,
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  volume = {24},
  number = {5},
  pages = {603--619},
  issn = {01628828},
  doi = {10.1109/34.1000236},
  urldate = {2022-09-09},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\Mean shift _Comaniciu_Meer 2002.pdf}
}

@article{cortes1995support,
  title = {Support-Vector Networks.},
  author = {Cortes, Corinna and Vapnik, Vladimir},
  year = {1995},
  journal = {Machine Learning},
  volume = {20},
  number = {3},
  pages = {273--297},
  added-at = {2011-06-29T16:50:31.000+0200},
  biburl = {https://www.bibsonomy.org/bibtex/287a1b1cb4913bafb37c82d66b0bc088a/oliver\textsubscript{a}wm},
  ee = {http://dx.doi.org/10.1007/BF00994018},
  interhash = {c223c465141618ad63aac5a6132280f7},
  intrahash = {87a1b1cb4913bafb37c82d66b0bc088a},
  keywords = {❓ Multiple DOI,awm2011 svm},
  timestamp = {2011-06-29T16:50:31.000+0200}
}

@incollection{cote2019textworld,
  title = {{{TextWorld}}: {{A Learning Environment}} for {{Text-Based Games}}},
  shorttitle = {{{TextWorld}}},
  booktitle = {Computer {{Games}}},
  author = {C{\^o}t{\'e}, Marc-Alexandre and K{\'a}d{\'a}r, {\'A}kos and Yuan, Xingdi and Kybartas, Ben and Barnes, Tavian and Fine, Emery and Moore, James and Hausknecht, Matthew and El Asri, Layla and Adada, Mahmoud and Tay, Wendy and Trischler, Adam},
  editor = {Cazenave, Tristan and Saffidine, Abdallah and Sturtevant, Nathan},
  year = {2019},
  volume = {1017},
  pages = {41--75},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-030-24337-1_3},
  urldate = {2023-02-04},
  isbn = {978-3-030-24336-4 978-3-030-24337-1},
  langid = {english},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\TextWorld _Côté et al 2019.pdf}
}

@inproceedings{creager2021environment,
  title = {Environment Inference for Invariant Learning},
  booktitle = {Proceedings of the 38th International Conference on Machine Learning},
  author = {Creager, Elliot and Jacobsen, Joern-Henrik and Zemel, Richard},
  editor = {Meila, Marina and Zhang, Tong},
  year = {2021},
  month = jul,
  series = {Proceedings of Machine Learning Research},
  volume = {139},
  pages = {2189--2200},
  publisher = {{PMLR}},
  abstract = {Learning models that gracefully handle distribution shifts is central to research on domain generalization, robust optimization, and fairness. A promising formulation is domain-invariant learning, which identifies the key issue of learning which features are domain-specific versus domain-invariant. An important assumption in this area is that the training examples are partitioned into ``domains'' or ``environments''. Our focus is on the more common setting where such partitions are not provided. We propose EIIL, a general framework for domain-invariant learning that incorporates Environment Inference to directly infer partitions that are maximally informative for downstream Invariant Learning. We show that EIIL outperforms invariant learning methods on the CMNIST benchmark without using environment labels, and significantly outperforms ERM on worst-group performance in the Waterbirds dataset. Finally, we establish connections between EIIL and algorithmic fairness, which enables EIIL to improve accuracy and calibration in a fair prediction problem.},
  pdf = {http://proceedings.mlr.press/v139/creager21a/creager21a.pdf},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\Environment inference for invariant learning _Creager et al 22.pdf}
}

@article{crockford2022how,
  title = {How to Regulate Face Recognition Technology},
  author = {Crockford, Kade},
  year = {2022},
  month = apr,
  journal = {Nature Human Behaviour},
  volume = {6},
  number = {4},
  pages = {476--476},
  issn = {2397-3374},
  doi = {10.1038/s41562-022-01329-3},
  urldate = {2022-10-30},
  langid = {english}
}

@inproceedings{cui2021topicguided,
  title = {Topic-{{Guided Abstractive Multi-Document Summarization}}},
  booktitle = {Findings of the {{Association}} for {{Computational Linguistics}}: {{EMNLP}} 2021},
  author = {Cui, Peng and Hu, Le},
  year = {2021},
  pages = {1463--1472},
  publisher = {{Association for Computational Linguistics}},
  address = {{Punta Cana, Dominican Republic}},
  doi = {10.18653/v1/2021.findings-emnlp.126},
  urldate = {2023-04-02},
  langid = {english},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\Topic-Guided Abstractive Multi-Document Summarization _Cui_Hu 2021.pdf;C\:\\Users\\Hytn\\Zotero\\storage\\84J9KYKX\\Cui and Hu - 2021 - Topic-Guided Abstractive Multi-Document Summarizat.pdf}
}

@article{cui2022stable,
  title = {Stable Learning Establishes Some Common Ground between Causal Inference and Machine Learning},
  author = {Cui, Peng and Athey, Susan},
  year = {2022},
  month = feb,
  journal = {Nature Machine Intelligence},
  volume = {4},
  number = {2},
  pages = {110--115},
  issn = {2522-5839},
  doi = {10.1038/s42256-022-00445-z},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\Stable learning establishes some common ground between causal inference and _Cui_Athey 22.pdf}
}

@misc{cui2022survey,
  title = {A {{Survey}} on {{Legal Judgment Prediction}}: {{Datasets}}, {{Metrics}}, {{Models}} and {{Challenges}}},
  shorttitle = {A {{Survey}} on {{Legal Judgment Prediction}}},
  author = {Cui, Junyun and Shen, Xiaoyu and Nie, Feiping and Wang, Zheng and Wang, Jinglong and Chen, Yulong},
  year = {2022},
  month = apr,
  number = {arXiv:2204.04859},
  eprint = {arXiv:2204.04859},
  publisher = {{arXiv}},
  urldate = {2022-07-21},
  abstract = {Legal judgment prediction (LJP) applies Natural Language Processing (NLP) techniques to predict judgment results based on fact descriptions automatically. Recently, large-scale public datasets and advances in NLP research have led to increasing interest in LJP. Despite a clear gap between machine and human performance, impressive results have been achieved in various benchmark datasets. In this paper, to address the current lack of comprehensive survey of existing LJP tasks, datasets, models and evaluations, (1) we analyze 31 LJP datasets in 6 languages, present their construction process and define a classification method of LJP with 3 different attributes; (2) we summarize 14 evaluation metrics under four categories for different outputs of LJP tasks; (3) we review 12 legal-domain pretrained models in 3 languages and highlight 3 major research directions for LJP; (4) we show the state-of-art results for 8 representative datasets from different court cases and discuss the open challenges. This paper can provide up-to-date and comprehensive reviews to help readers understand the status of LJP. We hope to facilitate both NLP researchers and legal professionals for further joint efforts in this problem.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\A Survey on Legal Judgment Prediction _Cui et al 22.pdf}
}

@article{cui2022teaching,
  title = {Teaching {{Machines}} to {{Read}}, {{Answer}} and {{Explain}}},
  author = {Cui, Yiming and Liu, Ting and Che, Wanxiang and Chen, Zhigang and Wang, Shijin},
  year = {2022},
  journal = {IEEE/ACM Transactions on Audio, Speech, and Language Processing},
  volume = {30},
  pages = {1483--1492},
  issn = {2329-9290, 2329-9304},
  doi = {10.1109/TASLP.2022.3156789},
  urldate = {2022-07-09},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\Teaching Machines to Read, Answer and Explain _Cui et al 22.pdf}
}

@inproceedings{dai2019joint,
  title = {Joint Extraction of Entities and Overlapping Relations Using Position-Attentive Sequence Labeling},
  booktitle = {Proceedings of the {{AAAI Conference}} on {{Artificial Intelligence}}},
  author = {Dai, Dai and Xiao, Xinyan and Lyu, Yajuan and Dou, Shan and She, Qiaoqiao and Wang, Haifeng},
  year = {2019},
  volume = {33},
  pages = {6300--6308},
  keywords = {⛔ No DOI found}
}

@inproceedings{das2022automatic,
  title = {Automatic Error Analysis for Document-Level Information Extraction},
  booktitle = {Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: {{Long}} Papers)},
  author = {Das, Aliva and Du, Xinya and Wang, Barry and Shi, Kejian and Gu, Jiayuan and Porter, Thomas and Cardie, Claire},
  year = {2022},
  month = may,
  pages = {3960--3975},
  publisher = {{Association for Computational Linguistics}},
  address = {{Dublin, Ireland}},
  abstract = {Document-level information extraction (IE) tasks have recently begun to be revisited in earnest using the end-to-end neural network techniques that have been successful on their sentence-level IE counterparts. Evaluation of the approaches, however, has been limited in a number of dimensions. In particular, the precision/recall/F1 scores typically reported provide few insights on the range of errors the models make. We build on the work of Kummerfeld and Klein (2013) to propose a transformation-based framework for automating error analysis in document-level event and (N-ary) relation extraction. We employ our framework to compare two state-of-the-art document-level template-filling approaches on datasets from three domains; and then, to gauge progress in IE since its inception 30 years ago, vs. four systems from the MUC-4 (1992) evaluation.},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\Automatic error analysis for document-level information extraction _Das et al 22.pdf}
}

@article{DBLP:journals/corr/abs-2004-14546,
  title = {{{WT5}}?! {{Training}} Text-to-Text Models to Explain Their Predictions.},
  author = {Narang, Sharan and Raffel, Colin and Lee, Katherine and Roberts, Adam and Fiedel, Noah and Malkan, Karishma},
  year = {2020},
  journal = {CoRR},
  volume = {abs/2004.14546},
  cdate = {1577836800000},
  publtype = {informal},
  keywords = {⛔ No DOI found},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\WT5 _Narang et al 22.pdf}
}

@inproceedings{devlin2018bert,
  title = {{{BERT}}: {{Pre-training}} of {{Deep Bidirectional Transformers}} for {{Language Understanding}}},
  booktitle = {Proceedings of the 2019 {{Conference}} of the {{North American Chapter}} of the {{Association}} for {{Computational Linguistics}}: {{Human Language Technologies}}, {{Volume}} 1 ({{Long}} and {{Short Papers}})},
  author = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina N.},
  year = {2018},
  pages = {4171--4186},
  keywords = {⛔ No DOI found},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\BERT _Devlin et al 3.pdf}
}

@inproceedings{devlin2019bert,
  title = {{{BERT}}: {{Pre-training}} of {{Deep Bidirectional Transformers}} for {{Language Understanding}}},
  booktitle = {Proceedings of the 2019 {{Conference}} of the {{North}}},
  author = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  year = {2019},
  pages = {4171--4186},
  publisher = {{Association for Computational Linguistics}},
  address = {{Minneapolis, Minnesota}},
  doi = {10.18653/v1/N19-1423},
  urldate = {2022-03-28},
  langid = {english},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\BERT _Devlin et al 22.pdf}
}

@misc{dias2022state,
  title = {State of the {{Art}} in {{Artificial Intelligence}} Applied to the {{Legal Domain}}},
  author = {Dias, Jo{\~a}o and Santos, Pedro A. and Cordeiro, Nuno and Antunes, Ana and Martins, Bruno and Baptista, Jorge and Gon{\c c}alves, Carlos},
  year = {2022},
  month = mar,
  number = {arXiv:2204.07047},
  eprint = {arXiv:2204.07047},
  publisher = {{arXiv}},
  urldate = {2022-07-21},
  abstract = {While Artificial Intelligence applied to the legal domain is a topic with origins in the last century, recent advances in Artificial Intelligence are posed to revolutionize it. This work presents an overview and contextualizes the main advances on the field of Natural Language Processing and how these advances have been used to further the state of the art in legal text analysis.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\State of the Art in Artificial Intelligence applied to the Legal Domain _Dias et al 22.pdf}
}

@inproceedings{ding2019cognitive,
  title = {Cognitive {{Graph}} for {{Multi-Hop Reading Comprehension}} at {{Scale}}},
  booktitle = {Proceedings of the 57th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}}},
  author = {Ding, Ming and Zhou, Chang and Chen, Qibin and Yang, Hongxia and Tang, Jie},
  year = {2019},
  pages = {2694--2703},
  doi = {10.18653/v1/P19-1259},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\Cognitive Graph for Multi-Hop Reading Comprehension at Scale _Ding et al 22.pdf}
}

@inproceedings{dou2021gsum,
  title = {{{GSum}}: {{A General Framework}} for {{Guided Neural Abstractive Summarization}}},
  shorttitle = {{{GSum}}},
  booktitle = {Proceedings of the 2021 {{Conference}} of the {{North American Chapter}} of the {{Association}} for {{Computational Linguistics}}: {{Human Language Technologies}}},
  author = {Dou, Zi-Yi and Liu, Pengfei and Hayashi, Hiroaki and Jiang, Zhengbao and Neubig, Graham},
  year = {2021},
  pages = {4830--4842},
  publisher = {{Association for Computational Linguistics}},
  address = {{Online}},
  doi = {10.18653/v1/2021.naacl-main.384},
  urldate = {2022-08-20},
  langid = {english},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\GSum _Dou et al 2021.pdf}
}

@inproceedings{du2019attribution,
  title = {On {{Attribution}} of {{Recurrent Neural Network Predictions}} via {{Additive Decomposition}}},
  booktitle = {The {{World Wide Web Conference}} on - {{WWW}} '19},
  author = {Du, Mengnan and Liu, Ninghao and Yang, Fan and Ji, Shuiwang and Hu, Xia},
  year = {2019},
  pages = {383--393},
  publisher = {{ACM Press}},
  address = {{San Francisco, CA, USA}},
  doi = {10.1145/3308558.3313545},
  urldate = {2022-03-20},
  isbn = {978-1-4503-6674-8},
  langid = {english},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\On Attribution of Recurrent Neural Network Predictions via Additive _Du et al 22.pdf}
}

@article{du2019techniques,
  title = {Techniques for Interpretable Machine Learning},
  author = {Du, Mengnan and Liu, Ninghao and Hu, Xia},
  year = {2019},
  month = dec,
  journal = {Communications of the ACM},
  volume = {63},
  number = {1},
  pages = {68--77},
  issn = {0001-0782, 1557-7317},
  doi = {10.1145/3359786},
  urldate = {2022-03-13},
  abstract = {Uncovering the mysterious ways machine learning models make decisions.},
  langid = {english},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\Techniques for interpretable machine learning _Du et al 22.pdf}
}

@inproceedings{du2021interpreting,
  title = {Towards {{Interpreting}} and {{Mitigating Shortcut Learning Behavior}} of {{NLU}} Models},
  booktitle = {Proceedings of the 2021 {{Conference}} of the {{North American Chapter}} of the {{Association}} for {{Computational Linguistics}}: {{Human Language Technologies}}},
  author = {Du, Mengnan and Manjunatha, Varun and Jain, Rajiv and Deshpande, Ruchi and Dernoncourt, Franck and Gu, Jiuxiang and Sun, Tong and Hu, Xia},
  year = {2021},
  pages = {915--929},
  publisher = {{Association for Computational Linguistics}},
  address = {{Online}},
  doi = {10.18653/v1/2021.naacl-main.71},
  urldate = {2022-07-29},
  langid = {english},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\Towards Interpreting and Mitigating Shortcut Learning Behavior of NLU models _Du et al 22.pdf}
}

@misc{du2022shortcut,
  title = {Shortcut {{Learning}} of {{Large Language Models}} in {{Natural Language Understanding}}: {{A Survey}}},
  shorttitle = {Shortcut {{Learning}} of {{Large Language Models}} in {{Natural Language Understanding}}},
  author = {Du, Mengnan and He, Fengxiang and Zou, Na and Tao, Dacheng and Hu, Xia},
  year = {2022},
  month = aug,
  number = {arXiv:2208.11857},
  eprint = {arXiv:2208.11857},
  publisher = {{arXiv}},
  urldate = {2022-08-29},
  abstract = {Large language models (LLMs) have achieved state-of-the-art performance on a series of natural language understanding tasks. However, these LLMs might rely on dataset bias and artifacts as shortcuts for prediction. This has significantly hurt their Out-of-Distribution (OOD) generalization and adversarial robustness. In this paper, we provide a review of recent developments that address the robustness challenge of LLMs. We first introduce the concepts and robustness challenge of LLMs. We then introduce methods to identify shortcut learning behavior in LLMs, characterize the reasons for shortcut learning, as well as introduce mitigation solutions. Finally, we identify key challenges and introduce the connections of this line of research to other directions.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\Shortcut Learning of Large Language Models in Natural Language Understanding _Du et al 2022.pdf}
}

@inproceedings{eberts2020spanbased,
  title = {Span-{{Based Joint Entity}} and {{Relation Extraction}} with {{Transformer Pre-Training}}.},
  booktitle = {{{ECAI}}},
  author = {Eberts, Markus and Ulges, Adrian},
  year = {2020},
  pages = {2006--2013},
  keywords = {⛔ No DOI found}
}

@article{el-kassas2021automatic,
  title = {Automatic Text Summarization: {{A}} Comprehensive Survey},
  shorttitle = {Automatic Text Summarization},
  author = {{El-Kassas}, Wafaa S. and Salama, Cherif R. and Rafea, Ahmed A. and Mohamed, Hoda K.},
  year = {2021},
  month = mar,
  journal = {Expert Systems with Applications},
  volume = {165},
  pages = {113679},
  issn = {09574174},
  doi = {10.1016/j.eswa.2020.113679},
  urldate = {2022-08-25},
  langid = {english},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\Automatic text summarization _El-Kassas et al 2021.pdf}
}

@incollection{elsahar2017unsupervised,
  title = {Unsupervised {{Open Relation Extraction}}},
  booktitle = {The {{Semantic Web}}: {{ESWC}} 2017 {{Satellite Events}}},
  author = {Elsahar, Hady and Demidova, Elena and Gottschalk, Simon and Gravier, Christophe and Laforest, Frederique},
  editor = {Blomqvist, Eva and Hose, Katja and Paulheim, Heiko and {\L}awrynowicz, Agnieszka and Ciravegna, Fabio and Hartig, Olaf},
  year = {2017},
  volume = {10577},
  pages = {12--16},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-319-70407-4_3},
  urldate = {2022-09-09},
  isbn = {978-3-319-70406-7 978-3-319-70407-4},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\Unsupervised Open Relation Extraction _Elsahar et al 2017.pdf}
}

@article{erkan2004lexrank,
  title = {{{LexRank}}: {{Graph-based Lexical Centrality}} as {{Salience}} in {{Text Summarization}}},
  shorttitle = {{{LexRank}}},
  author = {Erkan, G. and Radev, D. R.},
  year = {2004},
  month = dec,
  journal = {Journal of Artificial Intelligence Research},
  volume = {22},
  pages = {457--479},
  issn = {1076-9757},
  doi = {10.1613/jair.1523},
  urldate = {2022-08-27},
  abstract = {We introduce a stochastic graph-based method for computing relative importance of textual units for Natural Language Processing. We test the technique on the problem of Text Summarization (TS). Extractive TS relies on the concept of sentence salience to identify the most important sentences in a document or set of documents. Salience is typically defined in terms of the presence of particular important words or in terms of similarity to a centroid pseudo-sentence. We consider a new approach, LexRank, for computing sentence importance based on the concept of eigenvector centrality in a graph representation of sentences. In this model, a connectivity matrix based on intra-sentence cosine similarity is used as the adjacency matrix of the graph representation of sentences. Our system, based on LexRank ranked in first place in more than one task in the recent DUC 2004 evaluation. In this paper we present a detailed analysis of our approach and apply it to a larger data set including data from earlier DUC evaluations. We discuss several methods to compute centrality using the similarity graph. The results show that degree-based methods (including LexRank) outperform both centroid-based methods and other systems participating in DUC in most of the cases. Furthermore, the LexRank with threshold method outperforms the other degree-based techniques including continuous LexRank. We also show that our approach is quite insensitive to the noise in the data that may result from an imperfect topical clustering of documents.},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\LexRank _Erkan_Radev 2004.pdf}
}

@incollection{erxleben2014introducing,
  title = {Introducing {{Wikidata}} to the {{Linked Data Web}}},
  booktitle = {The {{Semantic Web}} \textendash{} {{ISWC}} 2014},
  author = {Erxleben, Fredo and G{\"u}nther, Michael and Kr{\"o}tzsch, Markus and Mendez, Julian and Vrande{\v c}i{\'c}, Denny},
  editor = {Mika, Peter and Tudorache, Tania and Bernstein, Abraham and Welty, Chris and Knoblock, Craig and Vrande{\v c}i{\'c}, Denny and Groth, Paul and Noy, Natasha and Janowicz, Krzysztof and Goble, Carole},
  year = {2014},
  volume = {8796},
  pages = {50--65},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-319-11964-9_4},
  urldate = {2022-10-07},
  isbn = {978-3-319-11963-2 978-3-319-11964-9},
  langid = {english},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\Introducing Wikidata to the Linked Data Web _Erxleben et al 2014.pdf}
}

@inproceedings{fabbri2019multinews,
  title = {Multi-{{News}}: {{A Large-Scale Multi-Document Summarization Dataset}} and {{Abstractive Hierarchical Model}}},
  shorttitle = {Multi-{{News}}},
  booktitle = {Proceedings of the 57th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}}},
  author = {Fabbri, Alexander and Li, Irene and She, Tianwei and Li, Suyi and Radev, Dragomir},
  year = {2019},
  pages = {1074--1084},
  publisher = {{Association for Computational Linguistics}},
  address = {{Florence, Italy}},
  doi = {10.18653/v1/P19-1102},
  urldate = {2022-08-31},
  langid = {english},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\Multi-News _Fabbri et al 2019.pdf}
}

@inproceedings{fader2014open,
  title = {Open Question Answering over Curated and Extracted Knowledge Bases},
  booktitle = {Proceedings of the 20th {{ACM SIGKDD}} International Conference on {{Knowledge}} Discovery and Data Mining},
  author = {Fader, Anthony and Zettlemoyer, Luke and Etzioni, Oren},
  year = {2014},
  month = aug,
  pages = {1156--1165},
  publisher = {{ACM}},
  address = {{New York New York USA}},
  doi = {10.1145/2623330.2623677},
  urldate = {2022-03-07},
  isbn = {978-1-4503-2956-9},
  langid = {english},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\Open question answering over curated and extracted knowledge bases _Fader et al 22.pdf}
}

@inproceedings{fan2018controllable,
  title = {Controllable {{Abstractive Summarization}}},
  booktitle = {Proceedings of the 2nd {{Workshop}} on {{Neural Machine Translation}} and {{Generation}}},
  author = {Fan, Angela and Grangier, David and Auli, Michael},
  year = {2018},
  pages = {45--54},
  publisher = {{Association for Computational Linguistics}},
  address = {{Melbourne, Australia}},
  doi = {10.18653/v1/W18-2706},
  urldate = {2022-10-07},
  langid = {english},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\Controllable Abstractive Summarization _Fan et al 2018.pdf}
}

@inproceedings{fan2019using,
  title = {Using {{Local Knowledge Graph Construction}} to {{Scale Seq2Seq Models}} to {{Multi-Document Inputs}}},
  booktitle = {Proceedings of the 2019 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}} and the 9th {{International Joint Conference}} on {{Natural Language Processing}} ({{EMNLP-IJCNLP}})},
  author = {Fan, Angela and Gardent, Claire and Braud, Chlo{\'e} and Bordes, Antoine},
  year = {2019},
  pages = {4184--4194},
  publisher = {{Association for Computational Linguistics}},
  address = {{Hong Kong, China}},
  doi = {10.18653/v1/D19-1428},
  urldate = {2022-08-08},
  langid = {english},
  keywords = {graph linearization},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\Using Local Knowledge Graph Construction to Scale Seq2Seq Models to _Fan et al 22.pdf}
}

@misc{felizardo2022reinforcement,
  title = {Reinforcement {{Learning Applied}} to {{Trading Systems}}: {{A Survey}}},
  shorttitle = {Reinforcement {{Learning Applied}} to {{Trading Systems}}},
  author = {Felizardo, Leonardo Kanashiro and Paiva, Francisco Caio Lima and Costa, Anna Helena Reali and {Del-Moral-Hernandez}, Emilio},
  year = {2022},
  month = nov,
  number = {arXiv:2212.06064},
  eprint = {arXiv:2212.06064},
  publisher = {{arXiv}},
  urldate = {2023-02-14},
  abstract = {Financial domain tasks, such as trading in market exchanges, are challenging and have long attracted researchers. The recent achievements and the consequent notoriety of Reinforcement Learning (RL) have also increased its adoption in trading tasks. RL uses a framework with well-established formal concepts, which raises its attractiveness in learning profitable trading strategies. However, RL use without due attention in the financial area can prevent new researchers from following standards or failing to adopt relevant conceptual guidelines. In this work, we embrace the seminal RL technical fundamentals, concepts, and recommendations to perform a unified, theoreticallygrounded examination and comparison of previous research that could serve as a structuring guide for the field of study. A selection of twenty-nine articles was reviewed under our classification that considers RL's most common formulations and design patterns from a large volume of available studies. This classification allowed for precise inspection of the most relevant aspects regarding data input, preprocessing, state and action composition, adopted RL techniques, evaluation setups, and overall results. Our analysis approach organized around fundamental RL concepts allowed for a clear identification of current system design best practices, gaps that require further investigation, and promising research opportunities. Finally, this review attempts to promote the development of this field of study by facilitating researchers' commitment to standards adherence and helping them to avoid straying away from the RL constructs' firm ground.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence},
  file = {C\:\\Users\\Hytn\\Zotero\\storage\\Z48VDP7L\\Felizardo et al. - 2022 - Reinforcement Learning Applied to Trading Systems.pdf}
}

@inproceedings{feng2018pathologies,
  title = {Pathologies of {{Neural Models Make Interpretations Difficult}}},
  booktitle = {Proceedings of the 2018 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}}},
  author = {Feng, Shi and Wallace, Eric and Grissom II, Alvin and Iyyer, Mohit and Rodriguez, Pedro and {Boyd-Graber}, Jordan},
  year = {2018},
  pages = {3719--3728},
  publisher = {{Association for Computational Linguistics}},
  address = {{Brussels, Belgium}},
  doi = {10.18653/v1/D18-1407},
  urldate = {2022-07-13},
  langid = {english},
  keywords = {permutation-based},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\Pathologies of Neural Models Make Interpretations Difficult _Feng et al 22.pdf}
}

@article{feng2018reinforcement,
  title = {Reinforcement {{Learning}} for {{Relation Classification From Noisy Data}}},
  author = {Feng, Jun and Huang, Minlie and Zhao, Li and Yang, Yang and Zhu, Xiaoyan},
  year = {2018},
  month = apr,
  journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
  volume = {32},
  number = {1},
  issn = {2374-3468, 2159-5399},
  doi = {10.1609/aaai.v32i1.12063},
  urldate = {2023-02-20},
  abstract = {Existing relation classification methods that rely on distant supervision assume that a bag of sentences mentioning an entity pair are all describing a relation for the entity pair. Such methods, performing classification at the bag level, cannot identify the mapping between a relation and a sentence, and largely suffers from the noisy labeling problem. In this paper, we propose a novel model for relation classification at the sentence level from noisy data. The model has two modules: an instance selector and a relation classifier. The instance selector chooses high-quality sentences with reinforcement learning and feeds the selected sentences into the relation classifier, and the relation classifier makes sentence-level prediction and provides rewards to the instance selector. The two modules are trained jointly to optimize the instance selection and relation classification processes.Experiment results show that our model can deal with the noise of data effectively and obtains better performance for relation classification at the sentence level.},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\Reinforcement Learning for Relation Classification From Noisy Data _Feng et al 2018.pdf}
}

@inproceedings{feng2022legal,
  title = {Legal {{Judgment Prediction}}: {{A Survey}} of the {{State}} of the {{Art}}},
  shorttitle = {Legal {{Judgment Prediction}}},
  booktitle = {Proceedings of the {{Thirty-First International Joint Conference}} on {{Artificial Intelligence}}},
  author = {Feng, Yi and Li, Chuanyi and Ng, Vincent},
  year = {2022},
  month = jul,
  pages = {5461--5469},
  publisher = {{International Joint Conferences on Artificial Intelligence Organization}},
  address = {{Vienna, Austria}},
  doi = {10.24963/ijcai.2022/765},
  urldate = {2022-08-31},
  abstract = {Automatic legal judgment prediction (LJP) has recently received increasing attention in the natural language processing community in part because of its practical values as well as the associated research challenges. We present an overview of the major milestones made in LJP research covering multiple jurisdictions and multiple languages, and conclude with promising future research directions.},
  isbn = {978-1-956792-00-3},
  langid = {english},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\Legal Judgment Prediction _Feng et al 2022.pdf}
}

@inproceedings{ferracane2019evaluating,
  title = {Evaluating {{Discourse}} in {{Structured Text Representations}}},
  booktitle = {Proceedings of the 57th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}}},
  author = {Ferracane, Elisa and Durrett, Greg and Li, Junyi Jessy and Erk, Katrin},
  year = {2019},
  pages = {646--653},
  doi = {10.18653/v1/P19-1062},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\Evaluating Discourse in Structured Text Representations _Ferracane et al 22.pdf}
}

@inproceedings{fisas-etal-2016-multi,
  title = {A Multi-Layered Annotated Corpus of Scientific Papers},
  booktitle = {Proceedings of the Tenth International Conference on Language Resources and Evaluation ({{LREC}}'16)},
  author = {Fisas, Beatriz and Ronzano, Francesco and Saggion, Horacio},
  year = {2016},
  month = may,
  pages = {3081--3088},
  publisher = {{European Language Resources Association (ELRA)}},
  address = {{Portoro\v{z}, Slovenia}},
  abstract = {Scientific literature records the research process with a standardized structure and provides the clues to track the progress in a scientific field. Understanding its internal structure and content is of paramount importance for natural language processing (NLP) technologies. To meet this requirement, we have developed a multi-layered annotated corpus of scientific papers in the domain of Computer Graphics. Sentences are annotated with respect to their role in the argumentative structure of the discourse. The purpose of each citation is specified. Special features of the scientific discourse such as advantages and disadvantages are identified. In addition, a grade is allocated to each sentence according to its relevance for being included in a summary.To the best of our knowledge, this complex, multi-layered collection of annotations and metadata characterizing a set of research papers had never been grouped together before in one corpus and therefore constitutes a newer, richer resource with respect to those currently available in the field.},
  file = {C\:\\Users\\Hytn\\Zotero\\storage\\YQXXMLNX\\A multi-layered annotated corpus of scientific papers_Fisas et al 2016.pdf}
}

@article{fort2019stiffness,
  title = {Stiffness: {{A New Perspective}} on {{Generalization}} in {{Neural Networks}}},
  author = {Fort, Stanislav and Nowak, Pawe{\l} Krzysztof and Jastrzebski, Stanislaw and Narayanan, Srini},
  year = {2019},
  month = jan,
  abstract = {In this paper we develop a new perspective on generalization of neural networks by proposing and investigating the concept of a neural network stiffness. We measure how stiff a network is by looking at how a small gradient step in the network's parameters on one example affects the loss on another example. Higher stiffness suggests that a network is learning features that generalize. In particular, we study how stiffness depends on 1) class membership, 2) distance between data points in the input space, 3) training iteration, and 4) learning rate. We present experiments on MNIST, FASHION MNIST, and CIFAR-10/100 using fully-connected and convolutional neural networks, as well as on a transformer-based NLP model. We demonstrate the connection between stiffness and generalization, and observe its dependence on learning rate. When training on CIFAR-100, the stiffness matrix exhibits a coarse-grained behavior indicative of the model's awareness of super-class membership. In addition, we measure how stiffness between two data points depends on their mutual input-space distance, and establish the concept of a dynamical critical length \textendash{} a distance below which a parameter update based on a data point influences its neighbors.},
  keywords = {⛔ No DOI found},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\Stiffness _Fort et al 22.pdf}
}

@article{freitas2014comprehensible,
  title = {Comprehensible Classification Models: A Position Paper},
  shorttitle = {Comprehensible Classification Models},
  author = {Freitas, Alex A.},
  year = {2014},
  month = mar,
  journal = {ACM SIGKDD Explorations Newsletter},
  volume = {15},
  number = {1},
  pages = {1--10},
  issn = {1931-0145, 1931-0153},
  doi = {10.1145/2594473.2594475},
  urldate = {2022-03-20},
  abstract = {The vast majority of the literature evaluates the performance of classification models using only the criterion of predictive accuracy. This paper reviews the case for considering also the comprehensibility (interpretability) of classification models, and discusses the interpretability of five types of classification models, namely decision trees, classification rules, decision tables, nearest neighbors and Bayesian network classifiers. We discuss both interpretability issues which are specific to each of those model types and more generic interpretability issues, namely the drawbacks of using model size as the only criterion to evaluate the comprehensibility of a model, and the use of monotonicity constraints to improve the comprehensibility and acceptance of classification models by users.},
  langid = {english},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\Comprehensible classification models _Freitas 22.pdf}
}

@inproceedings{frermann2019inducing,
  title = {Inducing {{Document Structure}} for {{Aspect-based Summarization}}},
  booktitle = {Proceedings of the 57th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}}},
  author = {Frermann, Lea and Klementiev, Alexandre},
  year = {2019},
  pages = {6263--6273},
  publisher = {{Association for Computational Linguistics}},
  address = {{Florence, Italy}},
  doi = {10.18653/v1/P19-1630},
  urldate = {2022-08-21},
  langid = {english},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\Inducing Document Structure for Aspect-based Summarization _Frermann_Klementiev 2019.pdf}
}

@inproceedings{fu2019graphrel,
  title = {{{GraphRel}}: {{Modeling}} Text as Relational Graphs for Joint Entity and Relation Extraction},
  booktitle = {Proceedings of the 57th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}}},
  author = {Fu, Tsu-Jui and Li, Peng-Hsuan and Ma, Wei-Yun},
  year = {2019},
  pages = {1409--1418},
  doi = {10.18653/v1/P19-1136},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\GraphRel _Fu et al 22.pdf}
}

@article{fu2020rethinking,
  title = {Rethinking {{Generalization}} of {{Neural Models}}: {{A Named Entity Recognition Case Study}}},
  author = {Fu, Jinlan and Liu, Pengfei and Zhang, Qi},
  year = {2020},
  month = apr,
  journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
  volume = {34},
  number = {05},
  pages = {7732--7739},
  issn = {2374-3468},
  doi = {10.1609/aaai.v34i05.6276},
  abstract = {{$<$}p{$>$}While neural network-based models have achieved impressive performance on a large body of NLP tasks, the generalization behavior of different models remains poorly understood: Does this excellent performance imply a perfect generalization model, or are there still some limitations? In this paper, we take the NER task as a testbed to analyze the generalization behavior of existing models from different perspectives and characterize the differences of their generalization abilities through the lens of our proposed measures, which guides us to better design models and training methods. Experiments with in-depth analyses diagnose the bottleneck of existing neural NER models in terms of breakdown performance analysis, annotation errors, dataset bias, and category relationships, which suggest directions for improvement. We have released the datasets: (ReCoNLL, PLONER) for the future research at our project page: http://pfliu.com/InterpretNER/.{$<$}/p{$>$}},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\Rethinking Generalization of Neural Models _Fu et al 22.pdf}
}

@inproceedings{galke2022bagofwords,
  title = {Bag-of-{{Words}} vs. {{Graph}} vs. {{Sequence}} in {{Text Classification}}: {{Questioning}} the {{Necessity}} of {{Text-Graphs}} and the {{Surprising Strength}} of a {{Wide MLP}}},
  shorttitle = {Bag-of-{{Words}} vs. {{Graph}} vs. {{Sequence}} in {{Text Classification}}},
  booktitle = {Proceedings of the 60th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}} ({{Volume}} 1: {{Long Papers}})},
  author = {Galke, Lukas and Scherp, Ansgar},
  year = {2022},
  pages = {4038--4051},
  publisher = {{Association for Computational Linguistics}},
  address = {{Dublin, Ireland}},
  doi = {10.18653/v1/2022.acl-long.279},
  urldate = {2022-10-18},
  langid = {english},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\Bag-of-Words vs _Galke_Scherp 2022.pdf}
}

@inproceedings{gan2021judgment,
  title = {Judgment Prediction via Injecting Legal Knowledge into Neural Networks},
  booktitle = {Proceedings of the {{AAAI}} Conference on Artificial Intelligence},
  author = {Gan, Leilei and Kuang, Kun and Yang, Yi and Wu, Fei},
  year = {2021},
  volume = {35},
  pages = {12866--12874},
  keywords = {⛔ No DOI found},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\Judgment prediction via injecting legal knowledge into neural networks _Gan et al 2021.pdf}
}

@misc{ganguli2022red,
  title = {Red {{Teaming Language Models}} to {{Reduce Harms}}: {{Methods}}, {{Scaling Behaviors}}, and {{Lessons Learned}}},
  shorttitle = {Red {{Teaming Language Models}} to {{Reduce Harms}}},
  author = {Ganguli, Deep and Lovitt, Liane and Kernion, Jackson and Askell, Amanda and Bai, Yuntao and Kadavath, Saurav and Mann, Ben and Perez, Ethan and Schiefer, Nicholas and Ndousse, Kamal and Jones, Andy and Bowman, Sam and Chen, Anna and Conerly, Tom and DasSarma, Nova and Drain, Dawn and Elhage, Nelson and {El-Showk}, Sheer and Fort, Stanislav and Dodds, Zac Hatfield and Henighan, Tom and Hernandez, Danny and Hume, Tristan and Jacobson, Josh and Johnston, Scott and Kravec, Shauna and Olsson, Catherine and Ringer, Sam and {Tran-Johnson}, Eli and Amodei, Dario and Brown, Tom and Joseph, Nicholas and McCandlish, Sam and Olah, Chris and Kaplan, Jared and Clark, Jack},
  year = {2022},
  month = aug,
  number = {arXiv:2209.07858},
  eprint = {arXiv:2209.07858},
  publisher = {{arXiv}},
  urldate = {2022-09-23},
  abstract = {We describe our early efforts to red team language models in order to simultaneously discover, measure, and attempt to reduce their potentially harmful outputs. We make three main contributions. First, we investigate scaling behaviors for red teaming across 3 model sizes (2.7B, 13B, and 52B parameters) and 4 model types: a plain language model (LM); an LM prompted to be helpful, honest, and harmless; an LM with rejection sampling; and a model trained to be helpful and harmless using reinforcement learning from human feedback (RLHF). We find that the RLHF models are increasingly difficult to red team as they scale, and we find a flat trend with scale for the other model types. Second, we release our dataset of 38,961 red team attacks for others to analyze and learn from. We provide our own analysis of the data and find a variety of harmful outputs, which range from offensive language to more subtly harmful non-violent unethical outputs. Third, we exhaustively describe our instructions, processes, statistical methodologies, and uncertainty about red teaming. We hope that this transparency accelerates our ability to work together as a community in order to develop shared norms, practices, and technical standards for how to red team language models. Warning: this paper contains examples that may be offensive or upsetting.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Computers and Society,urgent},
  file = {C\:\\Users\\Hytn\\Zotero\\storage\\ZWUM6ZIT\\Ganguli et al. - 2022 - Red Teaming Language Models to Reduce Harms Metho.pdf}
}

@inproceedings{gao2021simcse,
  title = {{{SimCSE}}: {{Simple Contrastive Learning}} of {{Sentence Embeddings}}},
  shorttitle = {{{SimCSE}}},
  booktitle = {Proceedings of the 2021 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}}},
  author = {Gao, Tianyu and Yao, Xingcheng and Chen, Danqi},
  year = {2021},
  pages = {6894--6910},
  publisher = {{Association for Computational Linguistics}},
  address = {{Online and Punta Cana, Dominican Republic}},
  doi = {10.18653/v1/2021.emnlp-main.552},
  urldate = {2022-03-22},
  langid = {english},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\SimCSE _Gao et al 4.pdf}
}

@inproceedings{gao2021simcsea,
  title = {{{SimCSE}}: {{Simple Contrastive Learning}} of {{Sentence Embeddings}}},
  shorttitle = {{{SimCSE}}},
  booktitle = {Proceedings of the 2021 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}}},
  author = {Gao, Tianyu and Yao, Xingcheng and Chen, Danqi},
  year = {2021},
  pages = {6894--6910},
  publisher = {{Association for Computational Linguistics}},
  address = {{Online and Punta Cana, Dominican Republic}},
  doi = {10.18653/v1/2021.emnlp-main.552},
  urldate = {2022-05-08},
  langid = {english},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\SimCSE _Gao et al 3.pdf}
}

@misc{gao2023outofdomain,
  title = {Out-of-{{Domain Robustness}} via {{Targeted Augmentations}}},
  author = {Gao, Irena and Sagawa, Shiori and Koh, Pang Wei and Hashimoto, Tatsunori and Liang, Percy},
  year = {2023},
  month = feb,
  number = {arXiv:2302.11861},
  eprint = {arXiv:2302.11861},
  publisher = {{arXiv}},
  urldate = {2023-04-11},
  abstract = {Models trained on one set of domains often suffer performance drops on unseen domains, e.g., when wildlife monitoring models are deployed in new camera locations. In this work, we study principles for designing data augmentations for out-of-domain (OOD) generalization. In particular, we focus on real-world scenarios in which some domain-dependent features are robust, i.e., some features that vary across domains are predictive OOD. For example, in the wildlife monitoring application above, image backgrounds vary across camera locations but indicate habitat type, which helps predict the species of photographed animals. Motivated by theoretical analysis on a linear setting, we propose targeted augmentations, which selectively randomize spurious domain-dependent features while preserving robust ones. We prove that targeted augmentations improve OOD performance, allowing models to generalize better with fewer domains. In contrast, existing approaches such as generic augmentations, which fail to randomize domain-dependent features, and domain-invariant augmentations, which randomize all domain-dependent features, both perform poorly OOD. In experiments on three realworld datasets, we show that targeted augmentations set new states-of-the-art for OOD performance by 3.2\textendash 15.2\%.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {C\:\\Users\\Hytn\\Zotero\\storage\\BUDEAR9K\\Gao et al. - 2023 - Out-of-Domain Robustness via Targeted Augmentation.pdf}
}

@inproceedings{gaonkar2020modeling,
  title = {Modeling {{Label Semantics}} for {{Predicting Emotional Reactions}}},
  booktitle = {Proceedings of the 58th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}}},
  author = {Gaonkar, Radhika and Kwon, Heeyoung and Bastan, Mohaddeseh and Balasubramanian, Niranjan and Chambers, Nathanael},
  year = {2020},
  pages = {4687--4692},
  publisher = {{Association for Computational Linguistics}},
  address = {{Online}},
  doi = {10.18653/v1/2020.acl-main.426},
  urldate = {2022-06-20},
  langid = {english},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\Modeling Label Semantics for Predicting Emotional Reactions _Gaonkar et al 22.pdf}
}

@inproceedings{gardent2017creating,
  title = {Creating Training Corpora for Nlg Micro-Planning},
  booktitle = {55th Annual Meeting of the {{Association}} for {{Computational Linguistics}} ({{ACL}})},
  author = {Gardent, Claire and Shimorina, Anastasia and Narayan, Shashi and {Perez-Beltrachini}, Laura},
  year = {2017},
  keywords = {⛔ No DOI found}
}

@inproceedings{gardent2017creatinga,
  title = {Creating {{Training Corpora}} for {{NLG Micro-Planners}}},
  booktitle = {Proceedings of the 55th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}} ({{Volume}} 1: {{Long Papers}})},
  author = {Gardent, Claire and Shimorina, Anastasia and Narayan, Shashi and {Perez-Beltrachini}, Laura},
  year = {2017},
  month = jul,
  pages = {179--188},
  publisher = {{Association for Computational Linguistics}},
  address = {{Vancouver, Canada}},
  doi = {10.18653/v1/P17-1017},
  abstract = {In this paper, we present a novel framework for semi-automatically creating linguistically challenging micro-planning data-to-text corpora from existing Knowledge Bases. Because our method pairs data of varying size and shape with texts ranging from simple clauses to short texts, a dataset created using this framework provides a challenging benchmark for microplanning. Another feature of this framework is that it can be applied to any large scale knowledge base and can therefore be used to train and learn KB verbalisers. We apply our framework to DBpedia data and compare the resulting dataset with Wen et al. 2016's. We show that while Wen et al.'s dataset is more than twice larger than ours, it is less diverse both in terms of input and in terms of text. We thus propose our corpus generation framework as a novel method for creating challenging data sets from which NLG models can be learned which are capable of handling the complex interactions occurring during in micro-planning between lexicalisation, aggregation, surface realisation, referring expression generation and sentence segmentation. To encourage researchers to take up this challenge, we made available a dataset of 21,855 data/text pairs created using this framework in the context of the WebNLG shared task.},
  keywords = {WebNLG},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\Creating Training Corpora for NLG Micro-Planners _Gardent et al 22.pdf}
}

@inproceedings{garg2019counterfactual,
  title = {Counterfactual {{Fairness}} in {{Text Classification}} through {{Robustness}}},
  booktitle = {Proceedings of the 2019 {{AAAI}}/{{ACM Conference}} on {{AI}}, {{Ethics}}, and {{Society}}},
  author = {Garg, Sahaj and Perot, Vincent and Limtiaco, Nicole and Taly, Ankur and Chi, Ed H. and Beutel, Alex},
  year = {2019},
  month = jan,
  pages = {219--226},
  publisher = {{ACM}},
  address = {{Honolulu HI USA}},
  doi = {10.1145/3306618.3317950},
  urldate = {2022-03-13},
  abstract = {In this paper, we study counterfactual fairness in text classification, which asks the question: How would the prediction change if the sensitive attribute referenced in the example were different? Toxicity classifiers demonstrate a counterfactual fairness issue by predicting that ``Some people are gay'' is toxic while ``Some people are straight'' is nontoxic. We offer a metric, counterfactual token fairness (CTF), for measuring this particular form of fairness in text classifiers, and describe its relationship with group fairness. Further, we offer three approaches, blindness, counterfactual augmentation, and counterfactual logit pairing (CLP), for optimizing counterfactual token fairness during training, bridging the robustness and fairness literature. Empirically, we find that blindness and CLP address counterfactual token fairness. The methods do not harm classifier performance, and have varying tradeoffs with group fairness. These approaches, both for measurement and optimization, provide a new path forward for addressing fairness concerns in text classification.},
  isbn = {978-1-4503-6324-2},
  langid = {english},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\Counterfactual Fairness in Text Classification through Robustness _Garg et al 22.pdf}
}

@article{geirhos2020shortcut,
  title = {Shortcut Learning in Deep Neural Networks},
  author = {Geirhos, Robert and Jacobsen, J{\"o}rn-Henrik and Michaelis, Claudio and Zemel, Richard and Brendel, Wieland and Bethge, Matthias and Wichmann, Felix A.},
  year = {2020},
  month = nov,
  journal = {Nature Machine Intelligence},
  volume = {2},
  number = {11},
  pages = {665--673},
  issn = {2522-5839},
  doi = {10.1038/s42256-020-00257-z},
  urldate = {2022-07-01},
  langid = {english},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\Shortcut learning in deep neural networks _Geirhos et al 3.pdf;C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\Shortcut learning in deep neural networks _Geirhos et al 4.pdf;C\:\\Users\\Hytn\\Zotero\\storage\\ICCA5XTV\\s42256-020-00257-z.pdf}
}

@article{gelman2021what,
  title = {What Are the {{Most Important Statistical Ideas}} of the {{Past}} 50 {{Years}}?},
  author = {Gelman, Andrew and Vehtari, Aki},
  year = {2021},
  month = oct,
  journal = {Journal of the American Statistical Association},
  volume = {116},
  number = {536},
  pages = {2087--2097},
  issn = {0162-1459, 1537-274X},
  doi = {10.1080/01621459.2021.1938081},
  urldate = {2022-10-25},
  langid = {english},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\What are the Most Important Statistical Ideas of the Past 50 Years _Gelman_Vehtari 2021.pdf}
}

@inproceedings{ghalandari2022efficient,
  title = {Efficient {{Unsupervised Sentence Compression}} by {{Fine-tuning Transformers}} with {{Reinforcement Learning}}},
  booktitle = {Proceedings of the 60th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}} ({{Volume}} 1: {{Long Papers}})},
  author = {Ghalandari, Demian and Hokamp, Chris and Ifrim, Georgiana},
  year = {2022},
  pages = {1267--1280},
  publisher = {{Association for Computational Linguistics}},
  address = {{Dublin, Ireland}},
  doi = {10.18653/v1/2022.acl-long.90},
  urldate = {2023-01-29},
  langid = {english},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\Efficient Unsupervised Sentence Compression by Fine-tuning Transformers with _Ghalandari et al 2022.pdf}
}

@article{gidiotis2020divideandconquer,
  title = {A {{Divide-and-Conquer Approach}} to the {{Summarization}} of {{Long Documents}}},
  author = {Gidiotis, Alexios and Tsoumakas, Grigorios},
  year = {2020},
  journal = {IEEE/ACM Transactions on Audio, Speech, and Language Processing},
  volume = {28},
  pages = {3029--3040},
  issn = {2329-9290, 2329-9304},
  doi = {10.1109/TASLP.2020.3037401},
  urldate = {2022-08-20},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\A Divide-and-Conquer Approach to the Summarization of Long Documents _Gidiotis_Tsoumakas 2020.pdf}
}

@inproceedings{glockner2018breaking,
  title = {Breaking {{NLI Systems}} with {{Sentences}} That {{Require Simple Lexical Inferences}}},
  booktitle = {Proceedings of the 56th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}} ({{Volume}} 2: {{Short Papers}})},
  author = {Glockner, Max and Shwartz, Vered and Goldberg, Yoav},
  year = {2018},
  pages = {650--655},
  publisher = {{Association for Computational Linguistics}},
  address = {{Melbourne, Australia}},
  doi = {10.18653/v1/P18-2103},
  urldate = {2022-07-11},
  langid = {english},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\Breaking NLI Systems with Sentences that Require Simple Lexical Inferences _Glockner et al 22.pdf}
}

@book{glymour2016causal,
  title = {Causal Inference in Statistics: {{A}} Primer},
  author = {Glymour, Madelyn and Pearl, Judea and Jewell, Nicholas P},
  year = {2016},
  publisher = {{John Wiley \& Sons}}
}

@inproceedings{gokhale2022generalized,
  title = {Generalized but Not {{Robust}}? {{Comparing}} the {{Effects}} of {{Data Modification Methods}} on {{Out-of-Domain Generalization}} and {{Adversarial Robustness}}},
  shorttitle = {Generalized but Not {{Robust}}?},
  booktitle = {Findings of the {{Association}} for {{Computational Linguistics}}: {{ACL}} 2022},
  author = {Gokhale, Tejas and Mishra, Swaroop and Luo, Man and Sachdeva, Bhavdeep and Baral, Chitta},
  year = {2022},
  pages = {2705--2718},
  publisher = {{Association for Computational Linguistics}},
  address = {{Dublin, Ireland}},
  doi = {10.18653/v1/2022.findings-acl.213},
  urldate = {2022-10-18},
  langid = {english},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\Generalized but not Robust _Gokhale et al 2022.pdf}
}

@inproceedings{gokhale2022generalizeda,
  title = {Generalized but Not {{Robust}}? {{Comparing}} the {{Effects}} of {{Data Modification Methods}} on {{Out-of-Domain Generalization}} and {{Adversarial Robustness}}},
  shorttitle = {Generalized but Not {{Robust}}?},
  booktitle = {Findings of the {{Association}} for {{Computational Linguistics}}: {{ACL}} 2022},
  author = {Gokhale, Tejas and Mishra, Swaroop and Luo, Man and Sachdeva, Bhavdeep and Baral, Chitta},
  year = {2022},
  pages = {2705--2718},
  publisher = {{Association for Computational Linguistics}},
  address = {{Dublin, Ireland}},
  doi = {10.18653/v1/2022.findings-acl.213},
  urldate = {2022-11-01},
  langid = {english},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\Generalized but not Robust _Gokhale et al 22.pdf}
}

@inproceedings{goldstein2000multidocument,
  title = {Multi-Document Summarization by Sentence Extraction},
  booktitle = {{{NAACL-ANLP}} 2000 {{Workshop}} on {{Automatic}} Summarization  -},
  author = {Goldstein, Jade and Mittal, Vibhu and Carbonell, Jaime and Kantrowitz, Mark},
  year = {2000},
  volume = {4},
  pages = {40--48},
  publisher = {{Association for Computational Linguistics}},
  address = {{Seattle, Washington}},
  doi = {10.3115/1117575.1117580},
  urldate = {2022-08-30},
  langid = {english},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\Multi-document summarization by sentence extraction _Goldstein et al 2000.pdf}
}

@article{gormley2015improved,
  title = {Improved Relation Extraction with Feature-Rich Compositional Embedding Models},
  author = {Gormley, Matthew R and Yu, Mo and Dredze, Mark},
  year = {2015},
  journal = {arXiv preprint arXiv:1505.02419},
  eprint = {1505.02419},
  archiveprefix = {arxiv},
  keywords = {⛔ No DOI found}
}

@inproceedings{grail2021globalizing,
  title = {Globalizing {{BERT-based Transformer Architectures}} for {{Long Document Summarization}}},
  booktitle = {Proceedings of the 16th {{Conference}} of the {{European Chapter}} of the {{Association}} for {{Computational Linguistics}}: {{Main Volume}}},
  author = {Grail, Quentin and Perez, Julien and Gaussier, Eric},
  year = {2021},
  pages = {1792--1810},
  publisher = {{Association for Computational Linguistics}},
  address = {{Online}},
  doi = {10.18653/v1/2021.eacl-main.154},
  urldate = {2022-08-20},
  langid = {english},
  keywords = {divide and conquer},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\Globalizing BERT-based Transformer Architectures for Long Document Summarization _Grail et al 2021.pdf}
}

@article{gu2017chemicalinduced,
  title = {Chemical-Induced Disease Relation Extraction via Convolutional Neural Network.},
  author = {Gu, Jinghang and Sun, Fuqing and Qian, Longhua and Zhou, Guodong},
  year = {2017},
  journal = {Database},
  volume = {2017},
  number = {1},
  keywords = {⛔ No DOI found}
}

@inproceedings{guerreiro2021spectra,
  title = {{{SPECTRA}}: {{Sparse Structured Text Rationalization}}},
  shorttitle = {{{SPECTRA}}},
  booktitle = {Proceedings of the 2021 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}}},
  author = {Guerreiro, Nuno M. and Martins, Andr{\'e} F. T.},
  year = {2021},
  pages = {6534--6550},
  publisher = {{Association for Computational Linguistics}},
  address = {{Online and Punta Cana, Dominican Republic}},
  doi = {10.18653/v1/2021.emnlp-main.525},
  urldate = {2022-11-01},
  langid = {english},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\SPECTRA _Guerreiro_Martins 2021.pdf}
}

@article{guidotti2019survey,
  title = {A {{Survey}} of {{Methods}} for {{Explaining Black Box Models}}},
  author = {Guidotti, Riccardo and Monreale, Anna and Ruggieri, Salvatore and Turini, Franco and Giannotti, Fosca and Pedreschi, Dino},
  year = {2019},
  month = sep,
  journal = {ACM Computing Surveys},
  volume = {51},
  number = {5},
  pages = {1--42},
  issn = {0360-0300, 1557-7341},
  doi = {10.1145/3236009},
  urldate = {2022-03-13},
  abstract = {In recent years, many accurate decision support systems have been constructed as black boxes, that is as systems that hide their internal logic to the user. This lack of explanation constitutes both a practical and an ethical issue. The literature reports many approaches aimed at overcoming this crucial weakness, sometimes at the cost of sacrificing accuracy for interpretability. The applications in which black box decision systems can be used are various, and each approach is typically developed to provide a solution for a specific problem and, as a consequence, it explicitly or implicitly delineates its own definition of interpretability and explanation. The aim of this article is to provide a classification of the main problems addressed in the literature with respect to the notion of explanation and the type of black box system. Given a problem definition, a black box type, and a desired explanation, this survey should help the researcher to find the proposals more useful for his own work. The proposed classification of approaches to open black box models should also be useful for putting the many research open questions in perspective.},
  langid = {english},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\A Survey of Methods for Explaining Black Box Models _Guidotti et al 22.pdf}
}

@inproceedings{guo2019attention,
  title = {Attention {{Guided Graph Convolutional Networks}} for {{Relation Extraction}}},
  booktitle = {Proceedings of the 57th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}}},
  author = {Guo, Zhijiang and Zhang, Yan and Lu, Wei},
  year = {2019},
  pages = {241--251},
  doi = {10.18653/v1/P19-1024},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\Attention Guided Graph Convolutional Networks for Relation Extraction _Guo et al 22.pdf}
}

@inproceedings{guo2019attentiona,
  title = {Attention {{Guided Graph Convolutional Networks}} for {{Relation Extraction}}},
  booktitle = {Proceedings of the 57th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}}},
  author = {Guo, Zhijiang and Zhang, Yan and Lu, Wei},
  year = {2019},
  pages = {241--251},
  publisher = {{Association for Computational Linguistics}},
  address = {{Florence, Italy}},
  doi = {10.18653/v1/P19-1024},
  urldate = {2022-11-10},
  langid = {english},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\Attention Guided Graph Convolutional Networks for Relation Extraction _Guo et al 3.pdf}
}

@inproceedings{guo2022autodebias,
  title = {Auto-{{Debias}}: {{Debiasing Masked Language Models}} with {{Automated Biased Prompts}}},
  shorttitle = {Auto-{{Debias}}},
  booktitle = {Proceedings of the 60th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}} ({{Volume}} 1: {{Long Papers}})},
  author = {Guo, Yue and Yang, Yi and Abbasi, Ahmed},
  year = {2022},
  pages = {1012--1023},
  publisher = {{Association for Computational Linguistics}},
  address = {{Dublin, Ireland}},
  doi = {10.18653/v1/2022.acl-long.72},
  urldate = {2022-06-11},
  langid = {english},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\Auto-Debias _Guo et al 22.pdf}
}

@article{gupta2010survey,
  title = {A {{Survey}} of {{Text Summarization Extractive Techniques}}},
  author = {Gupta, Vishal and Lehal, Gurpreet Singh},
  year = {2010},
  month = aug,
  journal = {Journal of Emerging Technologies in Web Intelligence},
  volume = {2},
  number = {3},
  pages = {258--268},
  issn = {1798-0461},
  doi = {10.4304/jetwi.2.3.258-268},
  urldate = {2022-03-07},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\A Survey of Text Summarization Extractive Techniques _Gupta_Lehal 22.pdf}
}

@inproceedings{gupta2016table,
  title = {Table Filling Multi-Task Recurrent Neural Network for Joint Entity and Relation Extraction},
  booktitle = {Proceedings of {{COLING}} 2016, the 26th {{International Conference}} on {{Computational Linguistics}}: {{Technical Papers}}},
  author = {Gupta, Pankaj and Sch{\"u}tze, Hinrich and Andrassy, Bernt},
  year = {2016},
  pages = {2537--2547},
  keywords = {⛔ No DOI found}
}

@inproceedings{gupta2019neural,
  title = {Neural Relation Extraction within and across Sentence Boundaries},
  booktitle = {Proceedings of the {{AAAI Conference}} on {{Artificial Intelligence}}},
  author = {Gupta, Pankaj and Rajaram, Subburam and Sch{\"u}tze, Hinrich and Runkler, Thomas},
  year = {2019},
  volume = {33},
  pages = {6513--6520},
  keywords = {⛔ No DOI found}
}

@misc{gurel2022knowledge,
  title = {Knowledge {{Enhanced Machine Learning Pipeline}} against {{Diverse Adversarial Attacks}}},
  author = {G{\"u}rel, Nezihe Merve and Qi, Xiangyu and Rimanic, Luka and Zhang, Ce and Li, Bo},
  year = {2022},
  month = mar,
  number = {arXiv:2106.06235},
  eprint = {arXiv:2106.06235},
  publisher = {{arXiv}},
  urldate = {2022-06-30},
  abstract = {Despite the great successes achieved by deep neural networks (DNNs), recent studies show that they are vulnerable against adversarial examples, which aim to mislead DNNs by adding small adversarial perturbations. Several defenses have been proposed against such attacks, while many of them have been adaptively attacked. In this work, we aim to enhance the ML robustness from a different perspective by leveraging domain knowledge: We propose a Knowledge Enhanced Machine Learning Pipeline (KEMLP) to integrate domain knowledge (i.e., logic relationships among different predictions) into a probabilistic graphical model via first-order logic rules. In particular, we develop KEMLP by integrating a diverse set of weak auxiliary models based on their logical relationships to the main DNN model that performs the target task. Theoretically, we provide convergence results and prove that, under mild conditions, the prediction of KEMLP is more robust than that of the main DNN model. Empirically, we take road sign recognition as an example and leverage the relationships between road signs and their shapes and contents as domain knowledge. We show that compared with adversarial training and other baselines, KEMLP achieves higher robustness against physical attacks, Lp bounded attacks, unforeseen attacks, and natural corruptions under both whitebox and blackbox settings, while still maintaining high clean accuracy.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\Knowledge Enhanced Machine Learning Pipeline against Diverse Adversarial Attacks _Gürel et al 22.pdf}
}

@inproceedings{gururangan2018annotation,
  title = {Annotation {{Artifacts}} in {{Natural Language Inference Data}}},
  booktitle = {Proceedings of the 2018 {{Conference}} of the {{North American Chapter}} of           the {{Association}} for {{Computational Linguistics}}: {{Human Language}}           {{Technologies}}, {{Volume}} 2 ({{Short Papers}})},
  author = {Gururangan, Suchin and Swayamdipta, Swabha and Levy, Omer and Schwartz, Roy and Bowman, Samuel and Smith, Noah A.},
  year = {2018},
  pages = {107--112},
  publisher = {{Association for Computational Linguistics}},
  address = {{New Orleans, Louisiana}},
  doi = {10.18653/v1/N18-2017},
  urldate = {2022-07-11},
  langid = {english},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\Annotation Artifacts in Natural Language Inference Data _Gururangan et al 22.pdf}
}

@inproceedings{haghighi2009exploring,
  title = {Exploring Content Models for Multi-Document Summarization},
  booktitle = {Proceedings of Human Language Technologies: {{The}} 2009 Annual Conference of the {{North American Chapter}} of the {{Association}} for {{Computational Linguistics}}},
  author = {Haghighi, Aria and Vanderwende, Lucy},
  year = {2009},
  pages = {362--370},
  keywords = {⛔ No DOI found}
}

@incollection{haidar2019textkdgan,
  title = {{{TextKD-GAN}}: {{Text Generation Using Knowledge Distillation}} and {{Generative Adversarial Networks}}},
  shorttitle = {{{TextKD-GAN}}},
  booktitle = {Advances in {{Artificial Intelligence}}},
  author = {Haidar, Md. Akmal and Rezagholizadeh, Mehdi},
  editor = {Meurs, Marie-Jean and Rudzicz, Frank},
  year = {2019},
  volume = {11489},
  pages = {107--118},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-030-18305-9_9},
  urldate = {2022-05-23},
  abstract = {Text generation is of particular interest in many NLP applications such as machine translation, language modeling, and text summarization. Generative adversarial networks (GANs) achieved a remarkable success in high quality image generation in computer vision, and recently, GANs have gained lots of interest from the NLP community as well. However, achieving similar success in NLP would be more challenging due to the discrete nature of text. In this work, we introduce a method using knowledge distillation to effectively exploit GAN setup for text generation. We demonstrate how autoencoders (AEs) can be used for providing a continuous representation of sentences, which is a smooth representation that assign non-zero probabilities to more than one word. We distill this representation to train the generator to synthesize similar smooth representations. We perform a number of experiments to validate our idea using different datasets and show that our proposed approach yields better performance in terms of the BLEU score and Jensen-Shannon distance (JSD) measure compared to traditional GANbased text generation approaches without pre-training.},
  isbn = {978-3-030-18304-2 978-3-030-18305-9},
  langid = {english},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\TextKD-GAN _Haidar_Rezagholizadeh 22.pdf}
}

@article{hamilton2017inductive,
  title = {Inductive Representation Learning on Large Graphs},
  author = {Hamilton, Will and Ying, Zhitao and Leskovec, Jure},
  year = {2017},
  journal = {Advances in neural information processing systems},
  volume = {30},
  keywords = {⛔ No DOI found}
}

@inproceedings{han2018fewrel,
  title = {{{FewRel}}: {{A Large-Scale Supervised Few-Shot Relation Classification Dataset}} with {{State-of-the-Art Evaluation}}},
  shorttitle = {{{FewRel}}},
  booktitle = {Proceedings of the 2018 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}}},
  author = {Han, Xu and Zhu, Hao and Yu, Pengfei and Wang, Ziyun and Yao, Yuan and Liu, Zhiyuan and Sun, Maosong},
  year = {2018},
  pages = {4803--4809},
  publisher = {{Association for Computational Linguistics}},
  address = {{Brussels, Belgium}},
  doi = {10.18653/v1/D18-1514},
  urldate = {2022-11-04},
  langid = {english},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\FewRel _Han et al 2018.pdf}
}

@misc{han2022orca,
  title = {{{ORCA}}: {{Interpreting Prompted Language Models}} via {{Locating Supporting Data Evidence}} in the {{Ocean}} of {{Pretraining Data}}},
  shorttitle = {{{ORCA}}},
  author = {Han, Xiaochuang and Tsvetkov, Yulia},
  year = {2022},
  month = may,
  number = {arXiv:2205.12600},
  eprint = {arXiv:2205.12600},
  publisher = {{arXiv}},
  urldate = {2023-04-10},
  abstract = {Large pretrained language models have been performing increasingly well in a variety of downstream tasks via prompting. However, it remains unclear from where the model learns the task-specific knowledge, especially in a zero-shot setup. In this work, we want to find evidence of the model's task-specific competence from pretraining and are specifically interested in locating a very small subset of pretraining data that directly supports the model in the task. We call such a subset supporting data evidence and propose a novel method ORCA to effectively identify it, by iteratively using gradient information related to the downstream task. This supporting data evidence offers interesting insights about the prompted language models: in the tasks of sentiment analysis and textual entailment, BERT shows a substantial reliance on BookCorpus, the smaller corpus of BERT's two pretraining corpora, as well as on pretraining examples that mask out synonyms to the task verbalizers.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {C\:\\Users\\Hytn\\Zotero\\storage\\9LARPQV9\\Han and Tsvetkov - 2022 - ORCA Interpreting Prompted Language Models via Lo.pdf}
}

@inproceedings{hao2021self,
  title = {Self-Attention Attribution: {{Interpreting}} Information Interactions inside Transformer},
  booktitle = {Proceedings of the {{AAAI}} Conference on Artificial Intelligence},
  author = {Hao, Yaru and Dong, Li and Wei, Furu and Xu, Ke},
  year = {2021},
  volume = {35},
  pages = {12963--12971},
  keywords = {⛔ No DOI found},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\Self-attention attribution _Hao et al 3.pdf}
}

@inproceedings{hao2021self,
  title = {Self-Attention Attribution: {{Interpreting}} Information Interactions inside Transformer},
  booktitle = {Proceedings of the {{AAAI}} Conference on Artificial Intelligence},
  author = {Hao, Yaru and Dong, Li and Wei, Furu and Xu, Ke},
  year = {2021},
  volume = {35},
  pages = {12963--12971},
  keywords = {⛔ No DOI found},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\Self-attention attribution _Hao et al 4.pdf}
}

@article{hayashi2021wikiasp,
  title = {{{WikiAsp}}: {{A Dataset}} for {{Multi-domain Aspect-based Summarization}}},
  shorttitle = {{{WikiAsp}}},
  author = {Hayashi, Hiroaki and Budania, Prashant and Wang, Peng and Ackerson, Chris and Neervannan, Raj and Neubig, Graham},
  year = {2021},
  month = mar,
  journal = {Transactions of the Association for Computational Linguistics},
  volume = {9},
  pages = {211--225},
  issn = {2307-387X},
  doi = {10.1162/tacl-a-00362},
  urldate = {2022-06-13},
  abstract = {Abstract             Aspect-based summarization is the task of generating focused summaries based on specific points of interest. Such summaries aid efficient analysis of text, such as quickly understanding reviews or opinions from different angles. However, due to large differences in the type of aspects for different domains (e.g., sentiment, product features), the development of previous models has tended to be domain-specific. In this paper, we propose WikiAsp,1 a large-scale dataset for multi-domain aspect- based summarization that attempts to spur research in the direction of open-domain aspect-based summarization. Specifically, we build the dataset using Wikipedia articles from 20 different domains, using the section titles and boundaries of each article as a proxy for aspect annotation. We propose several straightforward baseline models for this task and conduct experiments on the dataset. Results highlight key challenges that existing summarization models face in this setting, such as proper pronoun handling of quoted sources and consistent explanation of time-sensitive events.},
  langid = {english},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\WikiAsp _Hayashi et al 22.pdf}
}

@inproceedings{he2017unsupervised,
  title = {An {{Unsupervised Neural Attention Model}} for {{Aspect Extraction}}},
  booktitle = {Proceedings of the 55th {{Annual Meeting}} of the {{Association}} for           {{Computational Linguistics}} ({{Volume}} 1: {{Long Papers}})},
  author = {He, Ruidan and Lee, Wee Sun and Ng, Hwee Tou and Dahlmeier, Daniel},
  year = {2017},
  pages = {388--397},
  publisher = {{Association for Computational Linguistics}},
  address = {{Vancouver, Canada}},
  doi = {10.18653/v1/P17-1036},
  urldate = {2022-08-31},
  langid = {english},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\An Unsupervised Neural Attention Model for Aspect Extraction _He et al 2017.pdf}
}

@article{he2020ctrlsum,
  title = {{{CTRLsum}}: {{Towards Generic Controllable Text Summarization}}},
  shorttitle = {{{CTRLsum}}},
  author = {He, Junxian and Kry{\'s}ci{\'n}ski, Wojciech and McCann, Bryan and Rajani, Nazneen and Xiong, Caiming},
  year = {2020},
  publisher = {{arXiv}},
  doi = {10.48550/ARXIV.2012.04281},
  urldate = {2022-10-07},
  abstract = {Current summarization systems yield generic summaries that are disconnected from users' preferences and expectations. To address this limitation, we present CTRLsum, a novel framework for controllable summarization. Our approach enables users to control multiple aspects of generated summaries by interacting with the summarization system through textual input in the form of a set of keywords or descriptive prompts. Using a single unified model, CTRLsum is able to achieve a broad scope of summary manipulation at inference time without requiring additional human annotations or pre-defining a set of control aspects during training. We quantitatively demonstrate the effectiveness of our approach on three domains of summarization datasets and five control aspects: 1) entity-centric and 2) length-controllable summarization, 3) contribution summarization on scientific papers, 4) invention purpose summarization on patent filings, and 5) question-guided summarization on news articles in a reading comprehension setting. Moreover, when used in a standard, uncontrolled summarization setting, CTRLsum achieves state-of-the-art results on the CNN/DailyMail dataset. Code and model checkpoints are available at https://github.com/salesforce/ctrl-sum},
  copyright = {arXiv.org perpetual, non-exclusive license},
  keywords = {Computation and Language (cs.CL),FOS: Computer and information sciences},
  file = {C\:\\Users\\Hytn\\Zotero\\storage\\TAQNQS76\\2012.04281.pdf}
}

@inproceedings{he2021deberta,
  title = {{{DEBERTA}}: {{DECODING-ENHANCED BERT WITH DISENTANGLED ATTENTION}}},
  booktitle = {International Conference on Learning Representations},
  author = {He, Pengcheng and Liu, Xiaodong and Gao, Jianfeng and Chen, Weizhu},
  year = {2021},
  keywords = {⛔ No DOI found},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\DEBERTA _He et al 2021.pdf}
}

@inproceedings{he2022causpref,
  title = {{{CausPref}}: {{Causal Preference Learning}} for {{Out-of-Distribution Recommendation}}},
  shorttitle = {{{CausPref}}},
  booktitle = {Proceedings of the {{ACM Web Conference}} 2022},
  author = {He, Yue and Wang, Zimu and Cui, Peng and Zou, Hao and Zhang, Yafeng and Cui, Qiang and Jiang, Yong},
  year = {2022},
  month = apr,
  pages = {410--421},
  publisher = {{ACM}},
  address = {{Virtual Event, Lyon France}},
  doi = {10.1145/3485447.3511969},
  urldate = {2022-07-23},
  isbn = {978-1-4503-9096-5},
  langid = {english},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\CausPref _He et al 22.pdf}
}

@inproceedings{hendricks2018grounding,
  title = {Grounding {{Visual Explanations}}},
  booktitle = {Proceedings of the {{European Conference}} on {{Computer Vision}} ({{ECCV}})},
  author = {Hendricks, Lisa Anne and Hu, Ronghang and Darrell, Trevor and Akata, Zeynep},
  year = {2018},
  month = sep
}

@inproceedings{hendricks2018women,
  title = {Women Also Snowboard: {{Overcoming}} Bias in Captioning Models},
  booktitle = {Proceedings of the European Conference on Computer Vision ({{ECCV}})},
  author = {Hendricks, Lisa Anne and Burns, Kaylee and Saenko, Kate and Darrell, Trevor and Rohrbach, Anna},
  year = {2018},
  month = sep,
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\Women also snowboard _Hendricks et al 22.pdf}
}

@inproceedings{hendrickx2010semeval,
  title = {{{SemEval-2010}} Task 8: {{Multi-way}} Classification of Semantic Relations between Pairs of Nominals},
  booktitle = {Proceedings of the 5th International Workshop on Semantic Evaluation},
  author = {Hendrickx, Iris and Kim, Su Nam and Kozareva, Zornitsa and Nakov, Preslav and S{\'e}aghdha, Diarmuid {\'O} and Pad{\'o}, Sebastian and Pennacchiotti, Marco and Romano, Lorenza and Szpakowicz, Stan},
  year = {2010},
  pages = {33--38},
  keywords = {⛔ No DOI found},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\SemEval-2010 task 8 _Hendrickx et al 22.pdf}
}

@inproceedings{hendrycks2020pretrained,
  title = {Pretrained {{Transformers Improve Out-of-Distribution Robustness}}},
  booktitle = {Proceedings of the 58th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}}},
  author = {Hendrycks, Dan and Liu, Xiaoyuan and Wallace, Eric and Dziedzic, Adam and Krishnan, Rishabh and Song, Dawn},
  year = {2020},
  pages = {2744--2751},
  publisher = {{Association for Computational Linguistics}},
  address = {{Online}},
  doi = {10.18653/v1/2020.acl-main.244},
  urldate = {2022-11-01},
  langid = {english},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\Pretrained Transformers Improve Out-of-Distribution Robustness _Hendrycks et al 2020.pdf}
}

@article{hessel2018rainbow,
  title = {Rainbow: {{Combining Improvements}} in {{Deep Reinforcement Learning}}},
  shorttitle = {Rainbow},
  author = {Hessel, Matteo and Modayil, Joseph and Van Hasselt, Hado and Schaul, Tom and Ostrovski, Georg and Dabney, Will and Horgan, Dan and Piot, Bilal and Azar, Mohammad and Silver, David},
  year = {2018},
  month = apr,
  journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
  volume = {32},
  number = {1},
  issn = {2374-3468, 2159-5399},
  doi = {10.1609/aaai.v32i1.11796},
  urldate = {2023-02-13},
  abstract = {The deep reinforcement learning community has made several independent improvements to the DQN algorithm. However, it is unclear which of these extensions are complementary and can be fruitfully combined. This paper examines six extensions to the DQN algorithm and empirically studies their combination. Our experiments show that the combination provides state-of-the-art performance on the Atari 2600 benchmark, both in terms of data efficiency and final performance. We also provide results from a detailed ablation study that shows the contribution of each component to overall performance.},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\Rainbow _Hessel et al 2018.pdf}
}

@article{higgins2018definition,
  title = {Towards a {{Definition}} of {{Disentangled Representations}}},
  author = {Higgins, Irina and Amos, David and Pfau, David and Racaniere, Sebastien and Matthey, Loic and Rezende, Danilo and Lerchner, Alexander},
  year = {2018},
  month = dec,
  journal = {arXiv:1812.02230 [cs, stat]},
  eprint = {1812.02230},
  primaryclass = {cs, stat},
  urldate = {2022-03-21},
  abstract = {How can intelligent agents solve a diverse set of tasks in a data-efficient manner? The disentangled representation learning approach posits that such an agent would benefit from separating out (disentangling) the underlying structure of the world into disjoint parts of its representation. However, there is no generally agreed-upon definition of disentangling, not least because it is unclear how to formalise the notion of world structure beyond toy datasets with a known ground truth generative process. Here we propose that a principled solution to characterising disentangled representations can be found by focusing on the transformation properties of the world. In particular, we suggest that those transformations that change only some properties of the underlying world state, while leaving all other properties invariant, are what gives exploitable structure to any kind of data. Similar ideas have already been successfully applied in physics, where the study of symmetry transformations has revolutionised the understanding of the world structure. By connecting symmetry transformations to vector representations using the formalism of group and representation theory we arrive at the first formal definition of disentangled representations. Our new definition is in agreement with many of the current intuitions about disentangling, while also providing principled resolutions to a number of previous points of contention. While this work focuses on formally defining disentangling - as opposed to solving the learning problem - we believe that the shift in perspective to studying data transformations can stimulate the development of better representation learning algorithms.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {⛔ No DOI found,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\Towards a Definition of Disentangled Representations _Higgins et al 22.pdf}
}

@inproceedings{hirsch2021ifacetsum,
  title = {{{iFacetSum}}: {{Coreference-based Interactive Faceted Summarization}} for {{Multi-Document Exploration}}},
  shorttitle = {{{iFacetSum}}},
  booktitle = {Proceedings of the 2021 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}}: {{System Demonstrations}}},
  author = {Hirsch, Eran and Eirew, Alon and Shapira, Ori and Caciularu, Avi and Cattan, Arie and Ernst, Ori and Pasunuru, Ramakanth and Ronen, Hadar and Bansal, Mohit and Dagan, Ido},
  year = {2021},
  pages = {283--297},
  publisher = {{Association for Computational Linguistics}},
  address = {{Online and Punta Cana, Dominican Republic}},
  doi = {10.18653/v1/2021.emnlp-demo.33},
  urldate = {2023-04-02},
  langid = {english},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\iFacetSum _Hirsch et al 2021.pdf}
}

@article{hochreiter1997long,
  title = {Long Short-Term Memory},
  author = {Hochreiter, Sepp and Schmidhuber, J{\"u}rgen},
  year = {1997},
  journal = {Neural computation},
  volume = {9},
  number = {8},
  pages = {1735--1780},
  publisher = {{MIT Press}},
  doi = {10.1162/neco.1997.9.8.1735},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\Long short-term memory _Hochreiter_Schmidhuber 12.pdf}
}

@inproceedings{Holtzman2020The,
  title = {The Curious Case of Neural Text Degeneration},
  booktitle = {International Conference on Learning Representations},
  author = {Holtzman, Ari and Buys, Jan and Du, Li and Forbes, Maxwell and Choi, Yejin},
  year = {2020},
  keywords = {⛔ No DOI found},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\The curious case of neural text degeneration _Holtzman et al 2020.pdf}
}

@inproceedings{hu2020selfore,
  title = {{{SelfORE}}: {{Self-supervised Relational Feature Learning}} for {{Open Relation Extraction}}},
  shorttitle = {{{SelfORE}}},
  booktitle = {Proceedings of the 2020 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}} ({{EMNLP}})},
  author = {Hu, Xuming and Wen, Lijie and Xu, Yusong and Zhang, Chenwei and Yu, Philip},
  year = {2020},
  pages = {3673--3682},
  publisher = {{Association for Computational Linguistics}},
  address = {{Online}},
  doi = {10.18653/v1/2020.emnlp-main.299},
  urldate = {2022-09-10},
  langid = {english},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\SelfORE _Hu et al 2020.pdf}
}

@inproceedings{hu2021gradient,
  title = {Gradient {{Imitation Reinforcement Learning}} for {{Low Resource Relation Extraction}}},
  booktitle = {Proceedings of the 2021 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}}},
  author = {Hu, Xuming and Zhang, Chenwei and Yang, Yawen and Li, Xiaohe and Lin, Li and Wen, Lijie and Yu, Philip S.},
  year = {2021},
  pages = {2737--2746},
  publisher = {{Association for Computational Linguistics}},
  address = {{Online and Punta Cana, Dominican Republic}},
  doi = {10.18653/v1/2021.emnlp-main.216},
  urldate = {2023-02-20},
  langid = {english},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\Gradient Imitation Reinforcement Learning for Low Resource Relation Extraction _Hu et al 2021.pdf}
}

@inproceedings{huang-etal-2022-document,
  title = {Document-Level Relation Extraction via Pair-Aware and Entity-Enhanced Representation Learning},
  booktitle = {Proceedings of the 29th International Conference on Computational Linguistics},
  author = {Huang, Xiusheng and Yang, Hang and Chen, Yubo and Zhao, Jun and Liu, Kang and Sun, Weijian and Zhao, Zuyu},
  year = {2022},
  month = oct,
  pages = {2418--2428},
  publisher = {{International Committee on Computational Linguistics}},
  address = {{Gyeongju, Republic of Korea}},
  abstract = {Document-level relation extraction aims to recognize relations among multiple entity pairs from a whole piece of article. Recent methods achieve considerable performance but still suffer from two challenges: a) the relational entity pairs are sparse, b) the representation of entity pairs is insufficient. In this paper, we propose Pair-Aware and Entity-Enhanced(PAEE) model to solve the aforementioned two challenges. For the first challenge, we design a Pair-Aware Representation module to predict potential relational entity pairs, which constrains the relation extraction to the predicted entity pairs subset rather than all pairs; For the second, we introduce a Entity-Enhanced Representation module to assemble directional entity pairs and obtain a holistic understanding of the entire document. Experimental results show that our approach can obtain state-of-the-art performance on four benchmark datasets DocRED, DWIE, CDR and GDA.},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\Document-level relation extraction via pair-aware and entity-enhanced _Huang et al 2022.pdf}
}

@misc{huang2021dependency,
  title = {Dependency {{Learning}} for {{Legal Judgment Prediction}} with a {{Unified Text-to-Text Transformer}}},
  author = {Huang, Yunyun and Shen, Xiaoyu and Li, Chuanyi and Ge, Jidong and Luo, Bin},
  year = {2021},
  month = dec,
  number = {arXiv:2112.06370},
  eprint = {arXiv:2112.06370},
  publisher = {{arXiv}},
  urldate = {2022-08-31},
  abstract = {Given the fact of a case, Legal Judgment Prediction (LJP) involves a series of sub-tasks such as predicting violated law articles, charges and term of penalty. We propose leveraging a unified text-to-text Transformer for LJP, where the dependencies among sub-tasks can be naturally established within the auto-regressive decoder. Compared with previous works, it has three advantages: (1) it fits in the pretraining pattern of masked language models, and thereby can benefit from the semantic prompts of each sub-task rather than treating them as atomic labels, (2) it utilizes a single unified architecture, enabling full parameter sharing across all sub-tasks, and (3) it can incorporate both classification and generative sub-tasks. We show that this unified transformer, albeit pretrained on general-domain text, outperforms pretrained models tailored specifically for the legal domain. Through an extensive set of experiments, we find that the best order to capture dependencies is different from human intuitions, and the most reasonable logical order for humans can be sub-optimal for the model. We further include two more auxiliary tasks: court view generation and article content prediction, showing they can not only improve the prediction accuracy, but also provide interpretable explanations for model outputs even when an error is made. With the best configuration, our model outperforms both previous SOTA and a single-tasked version of the unified transformer by a large margin. Code and dataset are available at https://github.com/oli-yun/Dependency-LJP.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\Dependency Learning for Legal Judgment Prediction with a Unified Text-to-Text _Huang et al 2021.pdf}
}

@inproceedings{huang2021efficient,
  title = {Efficient {{Attentions}} for {{Long Document Summarization}}},
  booktitle = {Proceedings of the 2021 {{Conference}} of the {{North American Chapter}} of the {{Association}} for {{Computational Linguistics}}: {{Human Language Technologies}}},
  author = {Huang, Luyang and Cao, Shuyang and Parulian, Nikolaus and Ji, Heng and Wang, Lu},
  year = {2021},
  pages = {1419--1436},
  publisher = {{Association for Computational Linguistics}},
  address = {{Online}},
  doi = {10.18653/v1/2021.naacl-main.112},
  urldate = {2022-08-20},
  langid = {english},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\Efficient Attentions for Long Document Summarization _Huang et al 2021.pdf}
}

@inproceedings{huang2022does,
  title = {Does {{Recommend-Revise Produce Reliable Annotations}}? {{An Analysis}} on {{Missing Instances}} in {{DocRED}}},
  shorttitle = {Does {{Recommend-Revise Produce Reliable Annotations}}?},
  booktitle = {Proceedings of the 60th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}} ({{Volume}} 1: {{Long Papers}})},
  author = {Huang, Quzhe and Hao, Shibo and Ye, Yuan and Zhu, Shengqi and Feng, Yansong and Zhao, Dongyan},
  year = {2022},
  pages = {6241--6252},
  publisher = {{Association for Computational Linguistics}},
  address = {{Dublin, Ireland}},
  doi = {10.18653/v1/2022.acl-long.432},
  urldate = {2022-07-01},
  langid = {english},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\Does Recommend-Revise Produce Reliable Annotations _Huang et al 22.pdf}
}

@inproceedings{huang2022open,
  title = {Open {{Relation Modeling}}: {{Learning}} to {{Define Relations}} between {{Entities}}},
  shorttitle = {Open {{Relation Modeling}}},
  booktitle = {Findings of the {{Association}} for {{Computational Linguistics}}: {{ACL}} 2022},
  author = {Huang, Jie and Chang, Kevin and Xiong, Jinjun and Hwu, Wen-mei},
  year = {2022},
  pages = {297--308},
  publisher = {{Association for Computational Linguistics}},
  address = {{Dublin, Ireland}},
  doi = {10.18653/v1/2022.findings-acl.26},
  urldate = {2022-07-11},
  langid = {english},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\Open Relation Modeling _Huang et al 3.pdf}
}

@inproceedings{huang2022opena,
  title = {Open {{Relation Modeling}}: {{Learning}} to {{Define Relations}} between {{Entities}}},
  shorttitle = {Open {{Relation Modeling}}},
  booktitle = {Findings of the {{Association}} for {{Computational Linguistics}}: {{ACL}} 2022},
  author = {Huang, Jie and Chang, Kevin and Xiong, Jinjun and Hwu, Wen-mei},
  year = {2022},
  pages = {297--308},
  publisher = {{Association for Computational Linguistics}},
  address = {{Dublin, Ireland}},
  doi = {10.18653/v1/2022.findings-acl.26},
  urldate = {2022-06-18},
  langid = {english},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\Open Relation Modeling _Huang et al 4.pdf}
}

@inproceedings{huguetcabot2021rebel,
  title = {{{REBEL}}: {{Relation Extraction By End-to-end Language}} Generation},
  shorttitle = {{{REBEL}}},
  booktitle = {Findings of the {{Association}} for {{Computational Linguistics}}: {{EMNLP}} 2021},
  author = {Huguet Cabot, Pere-Llu{\'i}s and Navigli, Roberto},
  year = {2021},
  pages = {2370--2381},
  publisher = {{Association for Computational Linguistics}},
  address = {{Punta Cana, Dominican Republic}},
  doi = {10.18653/v1/2021.findings-emnlp.204},
  urldate = {2022-03-07},
  langid = {english},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\REBEL _Huguet Cabot_Navigli 22.pdf}
}

@misc{hupkes2022stateoftheart,
  title = {State-of-the-Art Generalisation Research in {{NLP}}: A Taxonomy and Review},
  shorttitle = {State-of-the-Art Generalisation Research in {{NLP}}},
  author = {Hupkes, Dieuwke and Giulianelli, Mario and Dankers, Verna and Artetxe, Mikel and Elazar, Yanai and Pimentel, Tiago and Christodoulopoulos, Christos and Lasri, Karim and Saphra, Naomi and Sinclair, Arabella and Ulmer, Dennis and Schottmann, Florian and Batsuren, Khuyagbaatar and Sun, Kaiser and Sinha, Koustuv and Khalatbari, Leila and Ryskina, Maria and Frieske, Rita and Cotterell, Ryan and Jin, Zhijing},
  year = {2022},
  month = oct,
  number = {arXiv:2210.03050},
  eprint = {arXiv:2210.03050},
  publisher = {{arXiv}},
  urldate = {2022-11-23},
  abstract = {The ability to generalise well is one of the primary desiderata of natural language processing (NLP). Yet, what `good generalisation' entails and how it should be evaluated is not well understood, nor are there any common standards to evaluate it. In this paper, we aim to lay the groundwork to improve both of these issues. We present a taxonomy for characterising and understanding generalisation research in NLP, we use that taxonomy to present a comprehensive map of published generalisation studies, and we make recommendations for which areas might deserve attention in the future. Our taxonomy is based on an extensive literature review of generalisation research, and contains five axes along which studies can differ: their main motivation, the type of generalisation they aim to solve, the type of data shift they consider, the source by which this data shift is obtained, and the locus of the shift within the modelling pipeline. We use our taxonomy to classify over 400 previous papers that test generalisation, for a total of more than 600 individual experiments. Considering the results of this review, we present an in-depth analysis of the current state of generalisation research in NLP, and make recommendations for the future. Along with this paper, we release a webpage where the results of our review can be dynamically explored, and which we intend to update as new NLP generalisation studies are published. With this work, we aim to make steps towards making state-of-the-art generalisation testing the new status quo in NLP.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {C\:\\Users\\Hytn\\Zotero\\storage\\94D3AXN4\\Hupkes et al. - 2022 - State-of-the-art generalisation research in NLP a.pdf}
}

@inproceedings{ian2014generative,
  title = {Generative Adversarial Nets},
  booktitle = {Advances in Neural Information Processing Systems},
  author = {Goodfellow, Ian and {Pouget-Abadie}, Jean and Mirza, Mehdi and Xu, Bing and {Warde-Farley}, David and Ozair, Sherjil and Courville, Aaron and Bengio, Yoshua},
  editor = {Ghahramani, Z. and Welling, M. and Cortes, C. and Lawrence, N. and Weinberger, K.Q.},
  year = {2014},
  volume = {27},
  publisher = {{Curran Associates, Inc.}},
  keywords = {⛔ No DOI found},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\Generative adversarial nets _Goodfellow et al 22.pdf}
}

@article{jaccard1912distribution,
  title = {{{THE DISTRIBUTION OF THE FLORA IN THE ALPINE ZONE}}.1},
  author = {Jaccard, Paul},
  year = {1912},
  month = feb,
  journal = {New Phytologist},
  volume = {11},
  number = {2},
  pages = {37--50},
  issn = {0028-646X, 1469-8137},
  doi = {10.1111/j.1469-8137.1912.tb05611.x},
  urldate = {2022-07-28},
  langid = {english},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\THE DISTRIBUTION OF THE FLORA IN THE ALPINE ZONE _Jaccard 12.pdf}
}

@inproceedings{jacovi2021contrastive,
  title = {Contrastive {{Explanations}} for {{Model Interpretability}}},
  booktitle = {Proceedings of the 2021 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}}},
  author = {Jacovi, Alon and Swayamdipta, Swabha and Ravfogel, Shauli and Elazar, Yanai and Choi, Yejin and Goldberg, Yoav},
  year = {2021},
  pages = {1597--1611},
  publisher = {{Association for Computational Linguistics}},
  address = {{Online and Punta Cana, Dominican Republic}},
  doi = {10.18653/v1/2021.emnlp-main.120},
  urldate = {2022-07-29},
  langid = {english},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\Contrastive Explanations for Model Interpretability _Jacovi et al 22.pdf}
}

@inproceedings{jain2019attention,
  title = {Attention Is Not {{Explanation}}},
  booktitle = {Proceedings of the 2019 {{Conference}} of the {{North}}},
  author = {Jain, Sarthak and Wallace, Byron C.},
  year = {2019},
  pages = {3543--3556},
  publisher = {{Association for Computational Linguistics}},
  address = {{Minneapolis, Minnesota}},
  doi = {10.18653/v1/N19-1357},
  urldate = {2023-02-22},
  langid = {english},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\Attention is not Explanation _Jain_Wallace 2019.pdf}
}

@inproceedings{jain2020learning,
  title = {Learning to {{Faithfully Rationalize}} by {{Construction}}},
  booktitle = {Proceedings of the 58th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}}},
  author = {Jain, Sarthak and Wiegreffe, Sarah and Pinter, Yuval and Wallace, Byron C.},
  year = {2020},
  pages = {4459--4473},
  publisher = {{Association for Computational Linguistics}},
  address = {{Online}},
  doi = {10.18653/v1/2020.acl-main.409},
  urldate = {2022-11-01},
  langid = {english},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\Learning to Faithfully Rationalize by Construction _Jain et al 2020.pdf}
}

@inproceedings{jang2017categorical,
  title = {Categorical Reparameterization with Gumbel-Softmax},
  booktitle = {International Conference on Learning Representations},
  author = {Jang, Eric and Gu, Shixiang and Poole, Ben},
  year = {2017},
  keywords = {Gumbel-Softmax},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\Categorical reparameterization with gumbel-softmax _Jang et al 2017.pdf}
}

@inproceedings{jeon2022entitybased,
  title = {Entity-Based {{Neural Local Coherence Modeling}}},
  booktitle = {Proceedings of the 60th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}} ({{Volume}} 1: {{Long Papers}})},
  author = {Jeon, Sungho and Strube, Michael},
  year = {2022},
  pages = {7787--7805},
  publisher = {{Association for Computational Linguistics}},
  address = {{Dublin, Ireland}},
  doi = {10.18653/v1/2022.acl-long.537},
  urldate = {2022-06-05},
  langid = {english},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\Entity-based Neural Local Coherence Modeling _Jeon_Strube 22.pdf}
}

@inproceedings{ji2021discodvt,
  title = {{{DiscoDVT}}: {{Generating Long Text}} with {{Discourse-Aware Discrete Variational Transformer}}},
  shorttitle = {{{DiscoDVT}}},
  booktitle = {Proceedings of the 2021 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}}},
  author = {Ji, Haozhe and Huang, Minlie},
  year = {2021},
  pages = {4208--4224},
  publisher = {{Association for Computational Linguistics}},
  address = {{Online and Punta Cana, Dominican Republic}},
  doi = {10.18653/v1/2021.emnlp-main.347},
  urldate = {2022-11-29},
  langid = {english},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\DiscoDVT _Ji_Huang 2021.pdf}
}

@inproceedings{jia2017adversariala,
  title = {Adversarial {{Examples}} for {{Evaluating Reading Comprehension Systems}}},
  booktitle = {Proceedings of the 2017 {{Conference}} on {{Empirical Methods}} in {{Natural}}           {{Language Processing}}},
  author = {Jia, Robin and Liang, Percy},
  year = {2017},
  pages = {2021--2031},
  publisher = {{Association for Computational Linguistics}},
  address = {{Copenhagen, Denmark}},
  doi = {10.18653/v1/D17-1215},
  urldate = {2022-07-06},
  langid = {english},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\Adversarial Examples for Evaluating Reading Comprehension Systems _Jia_Liang 2017.pdf}
}

@inproceedings{jia2019arnor,
  title = {{{ARNOR}}: {{Attention}} Regularization Based Noise Reduction for Distant Supervision Relation Classification},
  booktitle = {Proceedings of the 57th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}}},
  author = {Jia, Wei and Dai, Dai and Xiao, Xinyan and Wu, Hua},
  year = {2019},
  pages = {1399--1408},
  doi = {10.18653/v1/P19-1135},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\ARNOR _Jia et al 22.pdf}
}

@inproceedings{jia2019documentlevel,
  title = {Document-{{Level N-ary Relation Extraction}} with {{Multiscale Representation Learning}}.},
  booktitle = {Proceedings of the 2019 {{Conference}} of the {{North American Chapter}} of the {{Association}} for {{Computational Linguistics}}: {{Human Language Technologies}}, {{Volume}} 1 ({{Long}} and {{Short Papers}})},
  author = {Jia, Robin and Wong, Cliff and Poon, Hoifung},
  year = {2019},
  pages = {3693--3704},
  keywords = {⛔ No DOI found},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\Document-Level N-ary Relation Extraction with Multiscale Representation Learning _Jia et al 2019.pdf}
}

@inproceedings{jiang-etal-2018-interpretable,
  title = {Interpretable Rationale Augmented Charge Prediction System},
  booktitle = {Proceedings of the 27th International Conference on Computational Linguistics: {{System}} Demonstrations},
  author = {Jiang, Xin and Ye, Hai and Luo, Zhunchen and Chao, WenHan and Ma, Wenjia},
  year = {2018},
  month = aug,
  pages = {146--151},
  publisher = {{Association for Computational Linguistics}},
  address = {{Santa Fe, New Mexico}},
  abstract = {This paper proposes a neural based system to solve the essential interpretability problem existing in text classification, especially in charge prediction task. First, we use a deep reinforcement learning method to extract rationales which mean short, readable and decisive snippets from input text. Then a rationale augmented classification model is proposed to elevate the prediction accuracy. Naturally, the extracted rationales serve as the introspection explanation for the prediction result of the model, enhancing the transparency of the model. Experimental results demonstrate that our system is able to extract readable rationales in a high consistency with manual annotation and is comparable with the attention model in prediction accuracy.},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\Interpretable rationale augmented charge prediction system _Jiang et al 2018.pdf}
}

@article{jiang2020re,
  title = {({{Re-}}){{Imag}}(in)Ing {{Price Trends}}},
  author = {Jiang, Jingwen and Kelly, Bryan T. and Xiu, Dacheng},
  year = {2020},
  journal = {SSRN Electronic Journal},
  issn = {1556-5068},
  doi = {10.2139/ssrn.3756587},
  urldate = {2022-11-29},
  langid = {english},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\(Re-)Imag(in)ing Price Trends _Jiang et al 2020.pdf}
}

@article{jin2020bert,
  title = {Is {{BERT Really Robust}}? {{A Strong Baseline}} for {{Natural Language Attack}} on {{Text Classification}} and {{Entailment}}},
  shorttitle = {Is {{BERT Really Robust}}?},
  author = {Jin, Di and Jin, Zhijing and Zhou, Joey Tianyi and Szolovits, Peter},
  year = {2020},
  month = apr,
  journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
  volume = {34},
  number = {05},
  pages = {8018--8025},
  issn = {2374-3468, 2159-5399},
  doi = {10.1609/aaai.v34i05.6311},
  urldate = {2022-07-29},
  abstract = {Machine learning algorithms are often vulnerable to adversarial examples that have imperceptible alterations from the original counterparts but can fool the state-of-the-art models. It is helpful to evaluate or even improve the robustness of these models by exposing the maliciously crafted adversarial examples. In this paper, we present TextFooler, a simple but strong baseline to generate adversarial text. By applying it to two fundamental natural language tasks, text classification and textual entailment, we successfully attacked three target models, including the powerful pre-trained BERT, and the widely used convolutional and recurrent neural networks. We demonstrate three advantages of this framework: (1) effective\textemdash it outperforms previous attacks by success rate and perturbation rate, (2) utility-preserving\textemdash it preserves semantic content, grammaticality, and correct types classified by humans, and (3) efficient\textemdash it generates adversarial text with computational complexity linear to the text length.1},
  keywords = {data augmentation,textfooler},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\Is BERT Really Robust _Jin et al 22.pdf}
}

@inproceedings{jin2020multigranularity,
  title = {Multi-{{Granularity Interaction Network}} for {{Extractive}} and {{Abstractive Multi-Document Summarization}}},
  booktitle = {Proceedings of the 58th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}}},
  author = {Jin, Hanqi and Wang, Tianming and Wan, Xiaojun},
  year = {2020},
  pages = {6244--6254},
  publisher = {{Association for Computational Linguistics}},
  address = {{Online}},
  doi = {10.18653/v1/2020.acl-main.556},
  urldate = {2022-06-04},
  langid = {english},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\Multi-Granularity Interaction Network for Extractive and Abstractive _Jin et al 3.pdf}
}

@inproceedings{jin2020multigranularitya,
  title = {Multi-{{Granularity Interaction Network}} for {{Extractive}} and {{Abstractive Multi-Document Summarization}}},
  booktitle = {Proceedings of the 58th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}}},
  author = {Jin, Hanqi and Wang, Tianming and Wan, Xiaojun},
  year = {2020},
  pages = {6244--6254},
  publisher = {{Association for Computational Linguistics}},
  address = {{Online}},
  doi = {10.18653/v1/2020.acl-main.556},
  urldate = {2022-08-26},
  langid = {english},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\Multi-Granularity Interaction Network for Extractive and Abstractive _Jin et al 22.pdf}
}

@misc{jones2022capturing,
  title = {Capturing {{Failures}} of {{Large Language Models}} via {{Human Cognitive Biases}}},
  author = {Jones, Erik and Steinhardt, Jacob},
  year = {2022},
  month = feb,
  number = {arXiv:2202.12299},
  eprint = {arXiv:2202.12299},
  publisher = {{arXiv}},
  urldate = {2022-09-23},
  abstract = {Large language models generate complex, openended outputs: instead of outputting a single class, they can write summaries, generate dialogue, and produce working code. In order to study the reliability of these open-ended systems, we must understand not just when they fail, but also how they fail. To approach this, we draw inspiration from human cognitive biases\textemdash systematic patterns of deviation from rational judgement. Specifically, we use cognitive biases to (i) identify inputs that models are likely to err on, and (ii) develop tests to qualitatively characterize their errors on these inputs. Using code generation as a case study, we find that OpenAI's Codex errs predictably based on how the input prompt is framed, adjusts outputs towards anchors, and is biased towards outputs that mimic frequent training examples. We then use our framework to uncover high-impact errors such as incorrectly deleting files. Our experiments suggest that cognitive science can be a useful jumping-off point to better understand how contemporary machine learning systems behave.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning,urgent},
  file = {C\:\\Users\\Hytn\\Zotero\\storage\\837YVLGU\\Jones and Steinhardt - 2022 - Capturing Failures of Large Language Models via Hu.pdf}
}

@inproceedings{joshi2022investigation,
  title = {An {{Investigation}} of the ({{In}})Effectiveness of {{Counterfactually Augmented Data}}},
  booktitle = {Proceedings of the 60th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}} ({{Volume}} 1: {{Long Papers}})},
  author = {Joshi, Nitish and He, He},
  year = {2022},
  pages = {3668--3681},
  publisher = {{Association for Computational Linguistics}},
  address = {{Dublin, Ireland}},
  doi = {10.18653/v1/2022.acl-long.256},
  urldate = {2022-10-18},
  langid = {english},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\An Investigation of the (In)effectiveness of Counterfactually Augmented Data _Joshi_He 2022.pdf}
}

@inproceedings{ju2022logic,
  title = {Logic {{Traps}} in {{Evaluating Attribution Scores}}},
  booktitle = {Proceedings of the 60th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}} ({{Volume}} 1: {{Long Papers}})},
  author = {Ju, Yiming and Zhang, Yuanzhe and Yang, Zhao and Jiang, Zhongtao and Liu, Kang and Zhao, Jun},
  year = {2022},
  pages = {5911--5922},
  publisher = {{Association for Computational Linguistics}},
  address = {{Dublin, Ireland}},
  doi = {10.18653/v1/2022.acl-long.407},
  urldate = {2022-06-15},
  langid = {english},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\Logic Traps in Evaluating Attribution Scores _Ju et al 22.pdf}
}

@inproceedings{jwalapuram2022rethinking,
  title = {Rethinking {{Self-Supervision Objectives}} for {{Generalizable Coherence Modeling}}},
  booktitle = {Proceedings of the 60th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}} ({{Volume}} 1: {{Long Papers}})},
  author = {Jwalapuram, Prathyusha and Joty, Shafiq and Lin, Xiang},
  year = {2022},
  pages = {6044--6059},
  publisher = {{Association for Computational Linguistics}},
  address = {{Dublin, Ireland}},
  doi = {10.18653/v1/2022.acl-long.418},
  urldate = {2022-11-01},
  langid = {english},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\Rethinking Self-Supervision Objectives for Generalizable Coherence Modeling _Jwalapuram et al 2022.pdf}
}

@inproceedings{kamaleddine2022frugalscorea,
  title = {{{FrugalScore}}: {{Learning Cheaper}}, {{Lighter}} and {{Faster Evaluation Metrics}} for {{Automatic Text Generation}}},
  shorttitle = {{{FrugalScore}}},
  booktitle = {Proceedings of the 60th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}} ({{Volume}} 1: {{Long Papers}})},
  author = {Kamal Eddine, Moussa and Shang, Guokan and Tixier, Antoine and Vazirgiannis, Michalis},
  year = {2022},
  pages = {1305--1318},
  publisher = {{Association for Computational Linguistics}},
  address = {{Dublin, Ireland}},
  doi = {10.18653/v1/2022.acl-long.93},
  urldate = {2022-06-09},
  langid = {english},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\FrugalScore _Kamal Eddine et al 22.pdf}
}

@inproceedings{kantharaj2022charttotext,
  title = {Chart-to-{{Text}}: {{A Large-Scale Benchmark}} for {{Chart Summarization}}},
  shorttitle = {Chart-to-{{Text}}},
  booktitle = {Proceedings of the 60th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}} ({{Volume}} 1: {{Long Papers}})},
  author = {Kantharaj, Shankar and Leong, Rixie Tiffany and Lin, Xiang and Masry, Ahmed and Thakkar, Megh and Hoque, Enamul and Joty, Shafiq},
  year = {2022},
  pages = {4005--4023},
  publisher = {{Association for Computational Linguistics}},
  address = {{Dublin, Ireland}},
  doi = {10.18653/v1/2022.acl-long.277},
  urldate = {2022-11-04},
  langid = {english},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\Chart-to-Text _Kantharaj et al 2022.pdf}
}

@inproceedings{karras2018progressive,
  title = {Progressive Growing of {{GANs}} for Improved Quality, Stability, and Variation},
  booktitle = {International Conference on Learning Representations},
  author = {Karras, Tero and Aila, Timo and Laine, Samuli and Lehtinen, Jaakko},
  year = {2018},
  keywords = {⛔ No DOI found},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\Progressive growing of GANs for improved quality, stability, and variation _Karras et al 22.pdf}
}

@inproceedings{katiyar2017going,
  title = {Going out on a Limb: {{Joint}} Extraction of Entity Mentions and Relations without Dependency Trees},
  booktitle = {Proceedings of the 55th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}} ({{Volume}} 1: {{Long Papers}})},
  author = {Katiyar, Arzoo and Cardie, Claire},
  year = {2017},
  pages = {917--928},
  doi = {10.18653/v1/P17-1085},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\Going out on a limb _Katiyar_Cardie 22.pdf}
}

@article{katz2012quantitative,
  title = {Quantitative Legal Prediction-or-How i Learned to Stop Worrying and Start Preparing for the Data-Driven Future of the Legal Services Industry},
  author = {Katz, Daniel Martin},
  year = {2012},
  journal = {Emory LJ},
  volume = {62},
  pages = {909},
  publisher = {{HeinOnline}},
  keywords = {⛔ No DOI found}
}

@inproceedings{kavumba2019when,
  title = {When {{Choosing Plausible Alternatives}}, {{Clever Hans}} Can Be {{Clever}}},
  booktitle = {Proceedings of the {{First Workshop}} on {{Commonsense Inference}} in {{Natural Language Processing}}},
  author = {Kavumba, Pride and Inoue, Naoya and Heinzerling, Benjamin and Singh, Keshav and Reisert, Paul and Inui, Kentaro},
  year = {2019},
  pages = {33--42},
  publisher = {{Association for Computational Linguistics}},
  address = {{Hong Kong, China}},
  doi = {10.18653/v1/D19-6004},
  urldate = {2022-07-13},
  langid = {english},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\When Choosing Plausible Alternatives, Clever Hans can be Clever _Kavumba et al 22.pdf}
}

@inproceedings{kedzie2018content,
  title = {Content {{Selection}} in {{Deep Learning Models}} of {{Summarization}}},
  booktitle = {Proceedings of the 2018 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}}},
  author = {Kedzie, Chris and McKeown, Kathleen and Daum{\'e} III, Hal},
  year = {2018},
  pages = {1818--1828},
  publisher = {{Association for Computational Linguistics}},
  address = {{Brussels, Belgium}},
  doi = {10.18653/v1/D18-1208},
  urldate = {2022-10-07},
  langid = {english},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\Content Selection in Deep Learning Models of Summarization _Kedzie et al 2018.pdf}
}

@article{keneshloo2019deep,
  title = {Deep {{Reinforcement Learning}} for {{Sequence-to-Sequence Models}}},
  author = {Keneshloo, Yaser and Shi, Tian and Ramakrishnan, Naren and Reddy, Chandan K.},
  year = {2019},
  journal = {IEEE Transactions on Neural Networks and Learning Systems},
  pages = {1--21},
  issn = {2162-237X, 2162-2388},
  doi = {10.1109/TNNLS.2019.2929141},
  urldate = {2023-02-20},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\Deep Reinforcement Learning for Sequence-to-Sequence Models _Keneshloo et al 2019.pdf}
}

@article{keown1980mathematical,
  title = {Mathematical Models for Legal Prediction},
  author = {Keown, R},
  year = {1980},
  journal = {Computer/lj},
  volume = {2},
  pages = {829},
  publisher = {{HeinOnline}},
  keywords = {⛔ No DOI found}
}

@inproceedings{khani2021removing,
  title = {Removing {{Spurious Features}} Can {{Hurt Accuracy}} and {{Affect Groups Disproportionately}}},
  booktitle = {Proceedings of the 2021 {{ACM Conference}} on {{Fairness}}, {{Accountability}}, and {{Transparency}}},
  author = {Khani, Fereshte and Liang, Percy},
  year = {2021},
  month = mar,
  pages = {196--205},
  publisher = {{ACM}},
  address = {{Virtual Event Canada}},
  doi = {10.1145/3442188.3445883},
  urldate = {2022-07-29},
  isbn = {978-1-4503-8309-7},
  langid = {english},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\Removing Spurious Features can Hurt Accuracy and Affect Groups _Khani_Liang 22.pdf}
}

@inproceedings{kim2017structured,
  title = {Structured {{Attention Networks}}},
  booktitle = {{{ICLR}} ({{Poster}})},
  author = {Kim, Yoon and Denton, Carl and Hoang, Luong and Rush, Alexander M.},
  year = {2017},
  keywords = {⛔ No DOI found}
}

@inproceedings{kim2018disentangling,
  title = {Disentangling by Factorising},
  booktitle = {Proceedings of the 35th International Conference on Machine Learning},
  author = {Kim, Hyunjik and Mnih, Andriy},
  editor = {Dy, Jennifer and Krause, Andreas},
  year = {2018},
  month = jul,
  series = {Proceedings of Machine Learning Research},
  volume = {80},
  pages = {2649--2658},
  publisher = {{PMLR}},
  abstract = {We define and address the problem of unsupervised learning of disentangled representations on data generated from independent factors of variation. We propose FactorVAE, a method that disentangles by encouraging the distribution of representations to be factorial and hence independent across the dimensions. We show that it improves upon beta-VAE by providing a better trade-off between disentanglement and reconstruction quality and being more robust to the number of training iterations. Moreover, we highlight the problems of a commonly used disentanglement metric and introduce a new metric that does not suffer from them.},
  pdf = {http://proceedings.mlr.press/v80/kim18b/kim18b.pdf},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\Disentangling by factorising _Kim_Mnih 2018.pdf}
}

@inproceedings{kindermans2018learning,
  title = {Learning How to Explain Neural Networks: {{Patternnet}} and {{Patternattribution}}},
  booktitle = {6th {{International Conference}} on {{Learning Representations}}},
  author = {Kindermans, Pieter Jan and Sch{\"u}tt, Kristof T. and Alber, Maximilian and M{\"u}ller, Klaus Robert and Erhan, Dumitru and Kim, Been and D{\"a}hne, Sven},
  year = {2018},
  month = jan,
  abstract = {DeConvNet, Guided BackProp, LRP, were invented to better understand deep neural networks. We show that these methods do not produce the theoretically correct explanation for a linear model. Yet they are used on multi-layer networks with millions of parameters. This is a cause for concern since linear models are simple neural networks. We argue that explanation methods for neural nets should work reliably in the limit of simplicity, the linear models. Based on our analysis of linear models we propose a generalization that yields two explanation techniques (PatternNet and PatternAttribution) that are theoretically sound for linear models and produce improved explanations for deep networks.},
  langid = {english},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\Learning how to explain neural networks _Kindermans et al 22.pdf}
}

@article{king1967stepwise,
  title = {Step-{{Wise Clustering Procedures}}},
  author = {King, Benjamin},
  year = {1967},
  month = mar,
  journal = {Journal of the American Statistical Association},
  volume = {62},
  number = {317},
  pages = {86--101},
  issn = {0162-1459, 1537-274X},
  doi = {10.1080/01621459.1967.10482890},
  urldate = {2022-09-09},
  langid = {english},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\Step-Wise Clustering Procedures _King 1967.pdf}
}

@article{kingma2013autoencoding,
  title = {Auto-{{Encoding Variational Bayes}}},
  author = {Kingma, Diederik P and Welling, Max},
  year = {2013},
  publisher = {{arXiv}},
  doi = {10.48550/ARXIV.1312.6114},
  urldate = {2023-03-01},
  abstract = {How can we perform efficient inference and learning in directed probabilistic models, in the presence of continuous latent variables with intractable posterior distributions, and large datasets? We introduce a stochastic variational inference and learning algorithm that scales to large datasets and, under some mild differentiability conditions, even works in the intractable case. Our contributions are two-fold. First, we show that a reparameterization of the variational lower bound yields a lower bound estimator that can be straightforwardly optimized using standard stochastic gradient methods. Second, we show that for i.i.d. datasets with continuous latent variables per datapoint, posterior inference can be made especially efficient by fitting an approximate inference model (also called a recognition model) to the intractable posterior using the proposed lower bound estimator. Theoretical advantages are reflected in experimental results.},
  copyright = {arXiv.org perpetual, non-exclusive license},
  keywords = {FOS: Computer and information sciences,Machine Learning (cs.LG),Machine Learning (stat.ML)},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\Auto-Encoding Variational Bayes _Kingma_Welling 2013.pdf}
}

@article{kingma2014adam,
  title = {Adam: {{A}} Method for Stochastic Optimization},
  author = {Kingma, Diederik P and Ba, Jimmy},
  year = {2014},
  journal = {arXiv preprint arXiv:1412.6980},
  eprint = {1412.6980},
  archiveprefix = {arxiv},
  keywords = {⛔ No DOI found}
}

@article{kipf2016semisupervised,
  title = {Semi-Supervised Classification with Graph Convolutional Networks},
  author = {Kipf, Thomas N and Welling, Max},
  year = {2016},
  journal = {arXiv preprint arXiv:1609.02907},
  eprint = {1609.02907},
  archiveprefix = {arxiv},
  keywords = {⛔ No DOI found}
}

@inproceedings{Kitaev2020Reformer:,
  title = {Reformer: {{The}} Efficient Transformer},
  booktitle = {International Conference on Learning Representations},
  author = {Kitaev, Nikita and Kaiser, Lukasz and Levskaya, Anselm},
  year = {2020},
  keywords = {⛔ No DOI found},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\Reformer _Kitaev et al 2020.pdf}
}

@article{kiyani2017survey,
  title = {A Survey Automatic Text Summarization},
  author = {Kiyani, Farzad and Tas, Oguzhan},
  year = {2017},
  month = jun,
  journal = {Pressacademia},
  volume = {5},
  number = {1},
  pages = {205--213},
  issn = {2146-7943},
  doi = {10.17261/Pressacademia.2017.591},
  urldate = {2022-08-26},
  langid = {english},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\A survey automatic text summarization _Kiyani_Tas 2017.pdf}
}

@article{kiyoumarsi2015evaluation,
  title = {Evaluation of {{Automatic Text Summarizations}} Based on {{Human Summaries}}},
  author = {Kiyoumarsi, Farshad},
  year = {2015},
  month = jun,
  journal = {Procedia - Social and Behavioral Sciences},
  volume = {192},
  pages = {83--91},
  issn = {18770428},
  doi = {10.1016/j.sbspro.2015.06.013},
  urldate = {2022-08-27},
  langid = {english},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\Evaluation of Automatic Text Summarizations based on Human Summaries _Kiyoumarsi 2015.pdf}
}

@inproceedings{10.5555/3305381.3305576,
  title = {Understanding Black-Box Predictions via Influence Functions},
  booktitle = {Proceedings of the 34th International Conference on Machine Learning - Volume 70},
  author = {Koh, Pang Wei and Liang, Percy},
  year = {2017},
  series = {{{ICML}}'17},
  pages = {1885--1894},
  publisher = {{JMLR.org}},
  address = {{Sydney, NSW, Australia}},
  abstract = {How can we explain the predictions of a black-box model? In this paper, we use influence functions \textemdash{} a classic technique from robust statistics \textemdash{} to trace a model's prediction through the learning algorithm and back to its training data, thereby identifying training points most responsible for a given prediction. To scale up influence functions to modern machine learning settings, we develop a simple, efficient implementation that requires only oracle access to gradients and Hessian-vector products. We show that even on non-convex and non-differentiable models where the theory breaks down, approximations to influence functions can still provide valuable information. On linear models and convolutional neural networks, we demonstrate that influence functions are useful for multiple purposes: understanding model behavior, debugging models, detecting dataset errors, and even creating visually-indistinguishable training-set attacks.},
  keywords = {⛔ No DOI found},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\Understanding black-box predictions via influence functions _Koh_Liang 2017.pdf}
}

@inproceedings{koo2007structured,
  title = {Structured Prediction Models via the Matrix-Tree Theorem},
  booktitle = {Proceedings of the 2007 {{Joint Conference}} on {{Empirical Methods}} in {{Natural Language Processing}} and {{Computational Natural Language Learning}} ({{EMNLP-CoNLL}})},
  author = {Koo, Terry and Globerson, Amir and Carreras, Xavier and Collins, Michael},
  year = {2007},
  pages = {141--150},
  keywords = {⛔ No DOI found}
}

@inproceedings{kovaleva2019revealing,
  title = {Revealing the {{Dark Secrets}} of {{BERT}}},
  booktitle = {Proceedings of the 2019 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}} and the 9th {{International Joint Conference}} on {{Natural Language Processing}} ({{EMNLP-IJCNLP}})},
  author = {Kovaleva, Olga and Romanov, Alexey and Rogers, Anna and Rumshisky, Anna},
  year = {2019},
  pages = {4364--4373},
  publisher = {{Association for Computational Linguistics}},
  address = {{Hong Kong, China}},
  doi = {10.18653/v1/D19-1445},
  urldate = {2022-10-02},
  langid = {english},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\Revealing the Dark Secrets of BERT _Kovaleva et al 2019.pdf}
}

@inproceedings{krishna2018generating,
  title = {Generating {{Topic-Oriented Summaries Using Neural Attention}}},
  booktitle = {Proceedings of the 2018 {{Conference}} of the {{North American Chapter}} of           the {{Association}} for {{Computational Linguistics}}: {{Human Language}}           {{Technologies}}, {{Volume}} 1 ({{Long Papers}})},
  author = {Krishna, Kundan and Srinivasan, Balaji Vasan},
  year = {2018},
  pages = {1697--1705},
  publisher = {{Association for Computational Linguistics}},
  address = {{New Orleans, Louisiana}},
  doi = {10.18653/v1/N18-1153},
  urldate = {2022-08-31},
  langid = {english},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\Generating Topic-Oriented Summaries Using Neural Attention _Krishna_Srinivasan 2018.pdf}
}

@inproceedings{krueger2021out,
  title = {Out-of-Distribution Generalization via Risk Extrapolation ({{REx}})},
  booktitle = {Proceedings of the 38th International Conference on Machine Learning},
  author = {Krueger, David and Caballero, Ethan and Jacobsen, Joern-Henrik and Zhang, Amy and Binas, Jonathan and Zhang, Dinghuai and Priol, Remi Le and Courville, Aaron},
  editor = {Meila, Marina and Zhang, Tong},
  year = {2021},
  month = jul,
  series = {Proceedings of Machine Learning Research},
  volume = {139},
  pages = {5815--5826},
  publisher = {{PMLR}},
  abstract = {Distributional shift is one of the major obstacles when transferring machine learning prediction systems from the lab to the real world. To tackle this problem, we assume that variation across training domains is representative of the variation we might encounter at test time, but also that shifts at test time may be more extreme in magnitude. In particular, we show that reducing differences in risk across training domains can reduce a model's sensitivity to a wide range of extreme distributional shifts, including the challenging setting where the input contains both causal and anti-causal elements. We motivate this approach, Risk Extrapolation (REx), as a form of robust optimization over a perturbation set of extrapolated domains (MM-REx), and propose a penalty on the variance of training risks (V-REx) as a simpler variant. We prove that variants of REx can recover the causal mechanisms of the targets, while also providing robustness to changes in the input distribution (``covariate shift''). By appropriately trading-off robustness to causally induced distributional shifts and covariate shift, REx is able to outperform alternative methods such as Invariant Risk Minimization in situations where these types of shift co-occur.},
  pdf = {http://proceedings.mlr.press/v139/krueger21a/krueger21a.pdf},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\Out-of-distribution generalization via risk extrapolation (REx) _Krueger et al 3.pdf}
}

@inproceedings{kuang2018stable,
  title = {Stable {{Prediction}} across {{Unknown Environments}}},
  booktitle = {Proceedings of the 24th {{ACM SIGKDD International Conference}} on {{Knowledge Discovery}} \& {{Data Mining}}},
  author = {Kuang, Kun and Cui, Peng and Athey, Susan and Xiong, Ruoxuan and Li, Bo},
  year = {2018},
  month = jul,
  pages = {1617--1626},
  publisher = {{ACM}},
  doi = {10.1145/3219819.3220082},
  isbn = {978-1-4503-5552-0},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\Stable Prediction across Unknown Environments _Kuang et al 22.pdf}
}

@article{kuang2020stable,
  title = {Stable {{Prediction}} with {{Model Misspecification}} and {{Agnostic Distribution Shift}}},
  author = {Kuang, Kun and Xiong, Ruoxuan and Cui, Peng and Athey, Susan and Li, Bo},
  year = {2020},
  month = apr,
  journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
  volume = {34},
  number = {04},
  pages = {4485--4492},
  issn = {2374-3468, 2159-5399},
  doi = {10.1609/aaai.v34i04.5876},
  urldate = {2022-11-09},
  abstract = {For many machine learning algorithms, two main assumptions are required to guarantee performance. One is that the test data are drawn from the same distribution as the training data, and the other is that the model is correctly specified. In real applications, however, we often have little prior knowledge on the test data and on the underlying true model. Under model misspecification, agnostic distribution shift between training and test data leads to inaccuracy of parameter estimation and instability of prediction across unknown test data. To address these problems, we propose a novel Decorrelated Weighting Regression (DWR) algorithm which jointly optimizes a variable decorrelation regularizer and a weighted regression model. The variable decorrelation regularizer estimates a weight for each sample such that variables are decorrelated on the weighted training data. Then, these weights are used in the weighted regression to improve the accuracy of estimation on the effect of each variable, thus help to improve the stability of prediction across unknown test data. Extensive experiments clearly demonstrate that our DWR algorithm can significantly improve the accuracy of parameter estimation and stability of prediction with model misspecification and agnostic distribution shift.},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\Stable Prediction with Model Misspecification and Agnostic Distribution Shift _Kuang et al 2020.pdf}
}

@article{kung2023performance,
  title = {Performance of {{ChatGPT}} on {{USMLE}}: {{Potential}} for {{AI-assisted}} Medical Education Using Large Language Models},
  shorttitle = {Performance of {{ChatGPT}} on {{USMLE}}},
  author = {Kung, Tiffany H. and Cheatham, Morgan and Medenilla, Arielle and Sillos, Czarina and De Leon, Lorie and Elepa{\~n}o, Camille and Madriaga, Maria and Aggabao, Rimel and {Diaz-Candido}, Giezel and Maningo, James and Tseng, Victor},
  editor = {Dagan, Alon},
  year = {2023},
  month = feb,
  journal = {PLOS Digital Health},
  volume = {2},
  number = {2},
  pages = {e0000198},
  issn = {2767-3170},
  doi = {10.1371/journal.pdig.0000198},
  urldate = {2023-02-15},
  abstract = {We evaluated the performance of a large language model called ChatGPT on the United States Medical Licensing Exam (USMLE), which consists of three exams: Step 1, Step 2CK, and Step 3. ChatGPT performed at or near the passing threshold for all three exams without any specialized training or reinforcement. Additionally, ChatGPT demonstrated a high level of concordance and insight in its explanations. These results suggest that large language models may have the potential to assist with medical education, and potentially, clinical decision-making.},
  langid = {english},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\Performance of ChatGPT on USMLE _Kung et al 2023.pdf}
}

@inproceedings{laban2020summary,
  title = {The {{Summary Loop}}: {{Learning}} to {{Write Abstractive Summaries Without Examples}}},
  shorttitle = {The {{Summary Loop}}},
  booktitle = {Proceedings of the 58th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}}},
  author = {Laban, Philippe and Hsi, Andrew and Canny, John and Hearst, Marti A.},
  year = {2020},
  pages = {5135--5150},
  publisher = {{Association for Computational Linguistics}},
  address = {{Online}},
  doi = {10.18653/v1/2020.acl-main.460},
  urldate = {2023-02-15},
  langid = {english},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\The Summary Loop _Laban et al 2020.pdf}
}

@article{lawlor1963computers,
  title = {What Computers Can Do: {{Analysis}} and Prediction of Judicial Decisions},
  author = {Lawlor, Reed C},
  year = {1963},
  journal = {American Bar Association Journal},
  pages = {337--344},
  publisher = {{JSTOR}},
  keywords = {⛔ No DOI found}
}

@inproceedings{le2022shield,
  title = {{{SHIELD}}: {{Defending Textual Neural Networks}} against {{Multiple Black-Box Adversarial Attacks}} with {{Stochastic Multi-Expert Patcher}}},
  shorttitle = {{{SHIELD}}},
  booktitle = {Proceedings of the 60th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}} ({{Volume}} 1: {{Long Papers}})},
  author = {Le, Thai and Park, Noseong and Lee, Dongwon},
  year = {2022},
  pages = {6661--6674},
  publisher = {{Association for Computational Linguistics}},
  address = {{Dublin, Ireland}},
  doi = {10.18653/v1/2022.acl-long.459},
  urldate = {2022-11-01},
  langid = {english},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\SHIELD _Le et al 2022.pdf}
}

@article{lecun2015deep,
  title = {Deep Learning},
  author = {LeCun, Yann and Bengio, Yoshua and Hinton, Geoffrey},
  year = {2015},
  journal = {nature},
  volume = {521},
  number = {7553},
  pages = {436--444},
  publisher = {{Nature Publishing Group}},
  doi = {10.1038/nature14539},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\Deep learning _LeCun et al 22.pdf}
}

@inproceedings{lei2016rationalizing,
  title = {Rationalizing {{Neural Predictions}}},
  booktitle = {Proceedings of the 2016 {{Conference}} on {{Empirical Methods}} in {{Natural}}           {{Language Processing}}},
  author = {Lei, Tao and Barzilay, Regina and Jaakkola, Tommi},
  year = {2016},
  pages = {107--117},
  publisher = {{Association for Computational Linguistics}},
  address = {{Austin, Texas}},
  doi = {10.18653/v1/D16-1011},
  urldate = {2022-11-01},
  langid = {english},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\Rationalizing Neural Predictions _Lei et al 2016.pdf}
}

@inproceedings{levy2017zeroshot,
  title = {Zero-{{Shot Relation Extraction}} via {{Reading Comprehension}}},
  booktitle = {Proceedings of the 21st {{Conference}} on {{Computational Natural Language}}           {{Learning}} ({{CoNLL}} 2017)},
  author = {Levy, Omer and Seo, Minjoon and Choi, Eunsol and Zettlemoyer, Luke},
  year = {2017},
  pages = {333--342},
  publisher = {{Association for Computational Linguistics}},
  address = {{Vancouver, Canada}},
  doi = {10.18653/v1/K17-1034},
  urldate = {2022-07-15},
  langid = {english},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\Zero-Shot Relation Extraction via Reading Comprehension _Levy et al 22.pdf}
}

@inproceedings{lewis2020bart,
  title = {{{BART}}: {{Denoising}} Sequence-to-Sequence Pre-Training for Natural Language Generation, Translation, and Comprehension},
  booktitle = {Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics},
  author = {Lewis, Mike and Liu, Yinhan and Goyal, Naman and Ghazvininejad, Marjan and Mohamed, Abdelrahman and Levy, Omer and Stoyanov, Veselin and Zettlemoyer, Luke},
  year = {2020},
  pages = {7871--7880},
  doi = {10.18653/v1/2020.acl-main.703},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\BART _Lewis et al 22.pdf}
}

@inproceedings{li2014incremental,
  title = {Incremental Joint Extraction of Entity Mentions and Relations},
  booktitle = {Proceedings of the 52nd {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}} ({{Volume}} 1: {{Long Papers}})},
  author = {Li, Qi and Ji, Heng},
  year = {2014},
  pages = {402--412},
  doi = {10.3115/v1/P14-1038},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\Incremental joint extraction of entity mentions and relations _Li_Ji 22.pdf}
}

@article{li2016biocreative,
  title = {{{BioCreative V CDR}} Task Corpus: A Resource for Chemical Disease Relation Extraction},
  author = {Li, Jiao and Sun, Yueping and Johnson, Robin J. and Sciaky, Daniela and Wei, Chih-Hsuan and Leaman, Robert and Davis, Allan Peter and Mattingly, Carolyn J. and Wiegers, Thomas C. and Lu, Zhiyong},
  year = {2016},
  journal = {Database},
  volume = {2016},
  publisher = {{Oxford Academic}},
  keywords = {❓ Multiple DOI}
}

@inproceedings{li2016visualizing,
  title = {Visualizing and Understanding Neural Models in {{NLP}}},
  booktitle = {Proceedings of {{NAACL-HLT}}},
  author = {Li, Jiwei and Chen, Xinlei and Hovy, Eduard and Jurafsky, Dan},
  year = {2016},
  pages = {681--691},
  keywords = {⛔ No DOI found}
}

@inproceedings{li2016visualizinga,
  title = {Visualizing and {{Understanding Neural Models}} in {{NLP}}},
  booktitle = {Proceedings of the 2016 {{Conference}} of the {{North American Chapter}} of the {{Association}} for {{Computational Linguistics}}: {{Human Language Technologies}}},
  author = {Li, Jiwei and Chen, Xinlei and Hovy, Eduard and Jurafsky, Dan},
  year = {2016},
  pages = {681--691},
  publisher = {{Association for Computational Linguistics}},
  address = {{San Diego, California}},
  doi = {10.18653/v1/N16-1082},
  urldate = {2022-07-12},
  abstract = {While neural networks have been successfully applied to many NLP tasks the resulting vectorbased models are very difficult to interpret. For example it's not clear how they achieve compositionality, building sentence meaning from the meanings of words and phrases. In this paper we describe strategies for visualizing compositionality in neural models for NLP, inspired by similar work in computer vision. We first plot unit values to visualize compositionality of negation, intensification, and concessive clauses, allowing us to see wellknown markedness asymmetries in negation. We then introduce methods for visualizing a unit's salience, the amount that it contributes to the final composed meaning from first-order derivatives. Our general-purpose methods may have wide applications for understanding compositionality and other semantic properties of deep networks.},
  langid = {english},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\Visualizing and Understanding Neural Models in NLP _Li et al 22.pdf}
}

@misc{li2017understanding,
  title = {Understanding {{Neural Networks}} through {{Representation Erasure}}},
  author = {Li, Jiwei and Monroe, Will and Jurafsky, Dan},
  year = {2017},
  month = jan,
  number = {arXiv:1612.08220},
  eprint = {arXiv:1612.08220},
  publisher = {{arXiv}},
  urldate = {2022-07-13},
  abstract = {While neural networks have been successfully applied to many natural language processing tasks, they come at the cost of interpretability. In this paper, we propose a general methodology to analyze and interpret decisions from a neural model by observing the effects on the model of erasing various parts of the representation, such as input word-vector dimensions, intermediate hidden units, or input words. We present several approaches to analyzing the effects of such erasure, from computing its impact on evaluation metrics, to using reinforcement learning to erase the minimum set of input words in order to flip a neural model's decision. In a comprehensive analysis of multiple NLP tasks from lexical (word shape, morphology) to sentence-level (sentiment) to document level (sentiment aspect), we show that the proposed methodology not only offers clear explanations about neural model decisions, but also provides a way to conduct error analysis on neural models.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Computation and Language},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\Understanding Neural Networks through Representation Erasure _Li et al 22.pdf}
}

@inproceedings{li2018domain,
  title = {Domain {{Generalization}} with {{Adversarial Feature Learning}}},
  booktitle = {2018 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  author = {Li, Haoliang and Pan, Sinno Jialin and Wang, Shiqi and Kot, Alex C.},
  year = {2018},
  month = jun,
  pages = {5400--5409},
  publisher = {{IEEE}},
  address = {{Salt Lake City, UT}},
  doi = {10.1109/CVPR.2018.00566},
  urldate = {2022-11-09},
  abstract = {In this paper, we tackle the problem of domain generalization: how to learn a generalized feature representation for an ``unseen'' target domain by taking the advantage of multiple seen source-domain data. We present a novel framework based on adversarial autoencoders to learn a generalized latent feature representation across domains for domain generalization. To be specific, we extend adversarial autoencoders by imposing the Maximum Mean Discrepancy (MMD) measure to align the distributions among different domains, and matching the aligned distribution to an arbitrary prior distribution via adversarial feature learning. In this way, the learned feature representation is supposed to be universal to the seen source domains because of the MMD regularization, and is expected to generalize well on the target domain because of the introduction of the prior distribution. We proposed an algorithm to jointly train different components of our proposed framework. Extensive experiments on various vision tasks demonstrate that our proposed framework can learn better generalized features for the unseen target domain compared with state-of-the-art domain generalization methods.},
  isbn = {978-1-5386-6420-9},
  langid = {english},
  file = {C\:\\Users\\Hytn\\Zotero\\storage\\E98L4CUR\\Li et al. - 2018 - Domain Generalization with Adversarial Feature Lea.pdf}
}

@inproceedings{li2018paraphrase,
  title = {Paraphrase {{Generation}} with {{Deep Reinforcement Learning}}},
  booktitle = {Proceedings of the 2018 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}}},
  author = {Li, Zichao and Jiang, Xin and Shang, Lifeng and Li, Hang},
  year = {2018},
  pages = {3865--3878},
  publisher = {{Association for Computational Linguistics}},
  address = {{Brussels, Belgium}},
  doi = {10.18653/v1/D18-1421},
  urldate = {2023-02-20},
  langid = {english},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\Paraphrase Generation with Deep Reinforcement Learning _Li et al 2018.pdf}
}

@inproceedings{li2019entityrelation,
  title = {Entity-{{Relation Extraction}} as {{Multi-Turn Question Answering}}},
  booktitle = {Proceedings of the 57th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}}},
  author = {Li, Xiaoya and Yin, Fan and Sun, Zijun and Li, Xiayu and Yuan, Arianna and Chai, Duo and Zhou, Mingxin and Li, Jiwei},
  year = {2019},
  pages = {1340--1350},
  doi = {10.18653/v1/P19-1129},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\Entity-Relation Extraction as Multi-Turn Question Answering _Li et al 22.pdf}
}

@inproceedings{li2020bertattack,
  title = {{{BERT-ATTACK}}: {{Adversarial Attack Against BERT Using BERT}}},
  shorttitle = {{{BERT-ATTACK}}},
  booktitle = {Proceedings of the 2020 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}} ({{EMNLP}})},
  author = {Li, Linyang and Ma, Ruotian and Guo, Qipeng and Xue, Xiangyang and Qiu, Xipeng},
  year = {2020},
  pages = {6193--6202},
  publisher = {{Association for Computational Linguistics}},
  address = {{Online}},
  doi = {10.18653/v1/2020.emnlp-main.500},
  urldate = {2022-05-06},
  langid = {english},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\BERT-ATTACK _Li et al 22.pdf}
}

@inproceedings{li2021normal,
  title = {Normal vs. {{Adversarial}}: {{Salience-based Analysis}} of {{Adversarial Samples}} for {{Relation Extraction}}},
  shorttitle = {Normal vs. {{Adversarial}}},
  booktitle = {The 10th {{International Joint Conference}} on {{Knowledge Graphs}}},
  author = {Li, Luoqiu and Chen, Xiang and Bi, Zhen and Xie, Xin and Deng, Shumin and Zhang, Ningyu and Tan, Chuanqi and Chen, Mosha and Chen, Huajun},
  year = {2021},
  month = dec,
  pages = {115--120},
  publisher = {{ACM}},
  address = {{Virtual Event Thailand}},
  doi = {10.1145/3502223.3502237},
  urldate = {2022-09-03},
  isbn = {978-1-4503-9565-6},
  langid = {english},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\Normal vs _Li et al 2021.pdf}
}

@inproceedings{li2021robustness,
  title = {On {{Robustness}} and {{Bias Analysis}} of {{BERT-Based Relation Extraction}}},
  booktitle = {Communications in {{Computer}} and {{Information Science}}},
  author = {Li, Luoqiu and Chen, Xiang and Ye, Hongbin and Bi, Zhen and Deng, Shumin and Zhang, Ningyu and Chen, Huajun},
  year = {2021},
  volume = {1466 CCIS},
  pages = {43--59},
  publisher = {{Springer Science and Business Media Deutschland GmbH}},
  issn = {18650937},
  doi = {10.1007/978-981-16-6471-7_4},
  abstract = {Fine-tuning pre-trained models have achieved impressive performance on standard natural language processing benchmarks. However, the resultant model generalizability remains poorly understood. We do not know, for example, how excellent performance can lead to the perfection of generalization models. In this study, we analyze a fine-tuned BERT model from different perspectives using relation extraction. We also characterize the differences in generalization techniques according to our proposed improvements. From empirical experimentation, we find that BERT suffers a bottleneck in terms of robustness by way of randomizations, adversarial and counterfactual tests, and biases (i.e., selection and semantic). These findings highlight opportunities for future improvements. Our open-sourced testbed DiagnoseRE is available in https://github.com/zjunlp/DiagnoseRE.},
  isbn = {9789811664700},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\On Robustness and Bias Analysis of BERT-Based Relation Extraction _Li et al 22.pdf}
}

@inproceedings{li2021tdeer,
  title = {{{TDEER}}: {{An Efficient Translating Decoding Schema}} for {{Joint Extraction}} of {{Entities}} and {{Relations}}},
  shorttitle = {{{TDEER}}},
  booktitle = {Proceedings of the 2021 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}}},
  author = {Li, Xianming and Luo, Xiaotian and Dong, Chenghao and Yang, Daichuan and Luan, Beidi and He, Zhen},
  year = {2021},
  pages = {8055--8064},
  publisher = {{Association for Computational Linguistics}},
  address = {{Online and Punta Cana, Dominican Republic}},
  doi = {10.18653/v1/2021.emnlp-main.635},
  urldate = {2022-03-02},
  langid = {english},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\TDEER _Li et al 22.pdf}
}

@article{li2022charge,
  title = {Charge Prediction Modeling with Interpretation Enhancement Driven by Double-Layer Criminal System},
  author = {Li, Lin and Zhao, Lingyun and Nai, Peiran and Tao, Xiaohui},
  year = {2022},
  month = jan,
  journal = {World Wide Web},
  volume = {25},
  number = {1},
  pages = {381--400},
  issn = {1386-145X, 1573-1413},
  doi = {10.1007/s11280-021-00873-8},
  urldate = {2022-08-31},
  langid = {english},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\Charge prediction modeling with interpretation enhancement driven by _Li et al 2022.pdf}
}

@misc{li2022diffusionlm,
  title = {Diffusion-{{LM Improves Controllable Text Generation}}},
  author = {Li, Xiang Lisa and Thickstun, John and Gulrajani, Ishaan and Liang, Percy and Hashimoto, Tatsunori B.},
  year = {2022},
  month = may,
  number = {arXiv:2205.14217},
  eprint = {arXiv:2205.14217},
  publisher = {{arXiv}},
  urldate = {2022-06-22},
  abstract = {Controlling the behavior of language models (LMs) without re-training is a major open problem in natural language generation. While recent works have demonstrated successes on controlling simple sentence attributes (e.g., sentiment), there has been little progress on complex, fine-grained controls (e.g., syntactic structure). To address this challenge, we develop a new non-autoregressive language model based on continuous diffusions that we call Diffusion-LM. Building upon the recent successes of diffusion models in continuous domains, Diffusion-LM iteratively denoises a sequence of Gaussian vectors into word vectors, yielding a sequence of intermediate latent variables. The continuous, hierarchical nature of these intermediate variables enables a simple gradient-based algorithm to perform complex, controllable generation tasks. We demonstrate successful control of Diffusion-LM for six challenging fine-grained control tasks, significantly outperforming prior work.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\Diffusion-LM Improves Controllable Text Generation _Li et al 22.pdf}
}

@misc{li2022diffusionlma,
  title = {Diffusion-{{LM Improves Controllable Text Generation}}},
  author = {Li, Xiang Lisa and Thickstun, John and Gulrajani, Ishaan and Liang, Percy and Hashimoto, Tatsunori B.},
  year = {2022},
  month = may,
  number = {arXiv:2205.14217},
  eprint = {arXiv:2205.14217},
  publisher = {{arXiv}},
  urldate = {2022-12-09},
  abstract = {Controlling the behavior of language models (LMs) without re-training is a major open problem in natural language generation. While recent works have demonstrated successes on controlling simple sentence attributes (e.g., sentiment), there has been little progress on complex, fine-grained controls (e.g., syntactic structure). To address this challenge, we develop a new non-autoregressive language model based on continuous diffusions that we call Diffusion-LM. Building upon the recent successes of diffusion models in continuous domains, Diffusion-LM iteratively denoises a sequence of Gaussian vectors into word vectors, yielding a sequence of intermediate latent variables. The continuous, hierarchical nature of these intermediate variables enables a simple gradient-based algorithm to perform complex, controllable generation tasks. We demonstrate successful control of Diffusion-LM for six challenging fine-grained control tasks, significantly outperforming prior work.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {C\:\\Users\\Hytn\\Zotero\\storage\\K6L2BTCC\\Li et al. - 2022 - Diffusion-LM Improves Controllable Text Generation.pdf}
}

@article{li2022evidence,
  title = {Evidence {{Mining}} for {{Interpretable Charge Prediction}} via {{Prompt Learning}}},
  author = {Li, Lin and Liu, Dan and Zhao, Lingyun and Zhang, Jianwei and Liu, Jinhang},
  year = {2022},
  journal = {IEEE Transactions on Computational Social Systems},
  pages = {1--11},
  issn = {2329-924X, 2373-7476},
  doi = {10.1109/TCSS.2022.3178551},
  urldate = {2022-08-31},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\Evidence Mining for Interpretable Charge Prediction via Prompt Learning _Li et al 2022.pdf}
}

@inproceedings{li2022hiclre,
  title = {{{HiCLRE}}: {{A Hierarchical Contrastive Learning Framework}} for {{Distantly Supervised Relation Extraction}}},
  shorttitle = {{{HiCLRE}}},
  booktitle = {Findings of the {{Association}} for {{Computational Linguistics}}: {{ACL}} 2022},
  author = {Li, Dongyang and Zhang, Taolin and Hu, Nan and Wang, Chengyu and He, Xiaofeng},
  year = {2022},
  pages = {2567--2578},
  publisher = {{Association for Computational Linguistics}},
  address = {{Dublin, Ireland}},
  doi = {10.18653/v1/2022.findings-acl.202},
  urldate = {2022-06-13},
  langid = {english},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\HiCLRE _Li et al 22.pdf}
}

@inproceedings{li2022keywords,
  title = {Keywords and Instances: {{A}} Hierarchical Contrastive Learning Framework Unifying Hybrid Granularities for Text Generation},
  booktitle = {Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: {{Long}} Papers)},
  author = {Li, Mingzhe and Lin, XieXiong and Chen, Xiuying and Chang, Jinxiong and Zhang, Qishen and Wang, Feng and Wang, Taifeng and Liu, Zhongyi and Chu, Wei and Zhao, Dongyan and Yan, Rui},
  year = {2022},
  month = may,
  pages = {4432--4441},
  publisher = {{Association for Computational Linguistics}},
  address = {{Dublin, Ireland}},
  abstract = {Contrastive learning has achieved impressive success in generation tasks to militate the ``exposure bias'' problem and discriminatively exploit the different quality of references. Existing works mostly focus on contrastive learning on the instance-level without discriminating the contribution of each word, while keywords are the gist of the text and dominant the constrained mapping relationships. Hence, in this work, we propose a hierarchical contrastive learning mechanism, which can unify hybrid granularities semantic meaning in the input text. Concretely, we first propose a keyword graph via contrastive correlations of positive-negative pairs to iteratively polish the keyword representations. Then, we construct intra-contrasts within instance-level and keyword-level, where we assume words are sampled nodes from a sentence distribution. Finally, to bridge the gap between independent contrast levels and tackle the common contrast vanishing problem, we propose an inter-contrast mechanism that measures the discrepancy between contrastive keyword nodes respectively to the instance distribution. Experiments demonstrate that our model outperforms competitive baselines on paraphrasing, dialogue generation, and storytelling tasks.},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\Keywords and instances _Li et al 22.pdf}
}

@misc{li2022sok,
  title = {{{SoK}}: {{Certified Robustness}} for {{Deep Neural Networks}}},
  shorttitle = {{{SoK}}},
  author = {Li, Linyi and Xie, Tao and Li, Bo},
  year = {2022},
  month = feb,
  number = {arXiv:2009.04131},
  eprint = {arXiv:2009.04131},
  publisher = {{arXiv}},
  urldate = {2022-06-30},
  abstract = {Great advances in deep neural networks (DNNs) have led to state-of-the-art performance on a wide range of tasks. However, recent studies have shown that DNNs are vulnerable to adversarial attacks, which have brought great concerns when deploying these models to safety-critical applications such as autonomous driving. Different defense approaches have been proposed against adversarial attacks, including: a) empirical defenses, which usually can be adaptively attacked again without providing robustness certification; and b) certifiably robust approaches, which consist of robustness verification providing the lower bound of robust accuracy against any attacks under certain conditions and corresponding robust training approaches. In this paper, we systematize the certifiably robust approaches and related practical and theoretical implications and findings. We also provide the first comprehensive benchmark on existing robustness verification and training approaches on different datasets. In particular, we 1) provide a taxonomy for the robustness verification and training approaches, as well as summarize the methodologies for representative algorithms, 2) reveal the characteristics, strengths, limitations, and fundamental connections among these approaches, 3) discuss current research progresses, theoretical barriers, main challenges, and future directions for certifiably robust approaches for DNNs, and 4) provide an open-sourced unified platform to evaluate over 20 representative certifiably robust approaches for a wide range of DNNs.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Cryptography and Security,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\SoK _Li et al 22.pdf}
}

@article{li2022unifying,
  title = {Unifying {{Model Explainability}} and {{Robustness}} for {{Joint Text Classification}} and {{Rationale Extraction}}},
  author = {Li, Dongfang and Hu, Baotian and Chen, Qingcai and Xu, Tujie and Tao, Jingcong and Zhang, Yunan},
  year = {2022},
  month = jun,
  journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
  volume = {36},
  number = {10},
  pages = {10947--10955},
  issn = {2374-3468, 2159-5399},
  doi = {10.1609/aaai.v36i10.21342},
  urldate = {2023-04-03},
  abstract = {Recent works have shown explainability and robustness are two crucial ingredients of trustworthy and reliable text classification. However, previous works usually address one of two aspects: i) how to extract accurate rationales for explainability while being beneficial to prediction; ii) how to make the predictive model robust to different types of adversarial attacks. Intuitively, a model that produces helpful explanations should be more robust against adversarial attacks, because we cannot trust the model that outputs explanations but changes its prediction under small perturbations. To this end, we propose a joint classification and rationale extraction model named AT-BMC. It includes two key mechanisms: mixed Adversarial Training (AT) is designed to use various perturbations in discrete and embedding space to improve the model's robustness, and Boundary Match Constraint (BMC) helps to locate rationales more precisely with the guidance of boundary information. Performances on benchmark datasets demonstrate that the proposed AT-BMC outperforms baselines on both classification and rationale extraction by a large margin. Robustness analysis shows that the proposed AT-BMC decreases the attack success rate effectively by up to 69\%. The results indicate that there are connections between robust models and better explanations.},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\Unifying Model Explainability and Robustness for Joint Text Classification and _Li et al 2022.pdf}
}

@misc{li2023survey,
  title = {A {{Survey}} on {{Transformers}} in {{Reinforcement Learning}}},
  author = {Li, Wenzhe and Luo, Hao and Lin, Zichuan and Zhang, Chongjie and Lu, Zongqing and Ye, Deheng},
  year = {2023},
  month = jan,
  number = {arXiv:2301.03044},
  eprint = {arXiv:2301.03044},
  publisher = {{arXiv}},
  urldate = {2023-02-14},
  abstract = {Transformer has been considered the dominating neural architecture in NLP and CV, mostly under a supervised setting. Recently, a similar surge of using Transformers has appeared in the domain of reinforcement learning (RL), but it is faced with unique design choices and challenges brought by the nature of RL. However, the evolution of Transformers in RL has not yet been well unraveled. Hence, in this paper, we seek to systematically review motivations and progress on using Transformers in RL, provide a taxonomy on existing works, discuss each sub-field, and summarize future prospects.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  file = {C\:\\Users\\Hytn\\Zotero\\storage\\7AY5NWHQ\\Li et al. - 2023 - A Survey on Transformers in Reinforcement Learning.pdf}
}

@inproceedings{liang2021improving,
  title = {Improving {{Unsupervised Extractive Summarization}} with {{Facet-Aware Modeling}}},
  booktitle = {Findings of the {{Association}} for {{Computational Linguistics}}: {{ACL-IJCNLP}} 2021},
  author = {Liang, Xinnian and Wu, Shuangzhi and Li, Mu and Li, Zhoujun},
  year = {2021},
  pages = {1685--1697},
  publisher = {{Association for Computational Linguistics}},
  address = {{Online}},
  doi = {10.18653/v1/2021.findings-acl.147},
  urldate = {2023-04-02},
  langid = {english},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\Improving Unsupervised Extractive Summarization with Facet-Aware Modeling _Liang et al 2021.pdf}
}

@article{liang2022improving,
  title = {Improving {{Unsupervised Extractive Summarization}} by {{Jointly Modeling Facet}} and {{Redundancy}}},
  author = {Liang, Xinnian and Li, Jing and Wu, Shuangzhi and {li}, Mu and Li, Zhoujun},
  year = {2022},
  journal = {IEEE/ACM Transactions on Audio, Speech, and Language Processing},
  volume = {30},
  pages = {1546--1557},
  issn = {2329-9290, 2329-9304},
  doi = {10.1109/TASLP.2021.3138673},
  urldate = {2023-04-02},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\Improving Unsupervised Extractive Summarization by Jointly Modeling Facet and _Liang et al 2022.pdf}
}

@inproceedings{limsopatham2021effectively,
  title = {Effectively {{Leveraging BERT}} for {{Legal Document Classification}}},
  booktitle = {Proceedings of the {{Natural Legal Language Processing Workshop}} 2021},
  author = {Limsopatham, Nut},
  year = {2021},
  pages = {210--216},
  publisher = {{Association for Computational Linguistics}},
  address = {{Punta Cana, Dominican Republic}},
  doi = {10.18653/v1/2021.nllp-1.22},
  urldate = {2022-07-21},
  langid = {english},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\Effectively Leveraging BERT for Legal Document Classification _Limsopatham 22.pdf}
}

@inproceedings{lin2003automatic,
  title = {Automatic Evaluation of Summaries Using {{N-gram}} Co-Occurrence Statistics},
  booktitle = {Proceedings of the 2003 Human Language Technology Conference of the North {{American}} Chapter of the Association for Computational Linguistics},
  author = {Lin, Chin-Yew and Hovy, Eduard},
  year = {2003},
  pages = {150--157},
  keywords = {⛔ No DOI found},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\Automatic evaluation of summaries using N-gram co-occurrence statistics _Lin_Hovy 2003.pdf}
}

@inproceedings{lin2004rouge,
  title = {{{ROUGE}}: {{A}} Package for Automatic Evaluation of Summaries},
  booktitle = {Text Summarization Branches Out},
  author = {Lin, Chin-Yew},
  year = {2004},
  month = jul,
  pages = {74--81},
  publisher = {{Association for Computational Linguistics}},
  address = {{Barcelona, Spain}},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\ROUGE _Lin 2004.pdf}
}

@inproceedings{lin2016neural,
  title = {Neural Relation Extraction with Selective Attention over Instances},
  booktitle = {Proceedings of the 54th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}} ({{Volume}} 1: {{Long Papers}})},
  author = {Lin, Yankai and Shen, Shiqi and Liu, Zhiyuan and Luan, Huanbo and Sun, Maosong},
  year = {2016},
  pages = {2124--2133},
  doi = {10.18653/v1/P16-1200},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\Neural relation extraction with selective attention over instances _Lin et al 22.pdf}
}

@inproceedings{lin2020rigorous,
  title = {A {{Rigorous Study}} on {{Named Entity Recognition}}: {{Can Fine-tuning Pretrained Model Lead}} to the {{Promised Land}}?},
  shorttitle = {A {{Rigorous Study}} on {{Named Entity Recognition}}},
  booktitle = {Proceedings of the 2020 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}} ({{EMNLP}})},
  author = {Lin, Hongyu and Lu, Yaojie and Tang, Jialong and Han, Xianpei and Sun, Le and Wei, Zhicheng and Yuan, Nicholas Jing},
  year = {2020},
  pages = {7291--7300},
  publisher = {{Association for Computational Linguistics}},
  address = {{Online}},
  doi = {10.18653/v1/2020.emnlp-main.592},
  urldate = {2022-07-09},
  langid = {english},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\A Rigorous Study on Named Entity Recognition _Lin et al 22.pdf}
}

@misc{lin2022reinforcement,
  title = {Reinforcement {{Learning}} and {{Bandits}} for {{Speech}} and {{Language Processing}}: {{Tutorial}}, {{Review}} and {{Outlook}}},
  shorttitle = {Reinforcement {{Learning}} and {{Bandits}} for {{Speech}} and {{Language Processing}}},
  author = {Lin, Baihan},
  year = {2022},
  month = oct,
  number = {arXiv:2210.13623},
  eprint = {arXiv:2210.13623},
  publisher = {{arXiv}},
  urldate = {2023-02-14},
  abstract = {In recent years, reinforcement learning and bandits have transformed a wide range of real-world applications including healthcare, finance, recommendation systems, robotics, and last but not least, the speech and natural language processing. While most speech and language applications of reinforcement learning algorithms are centered around improving the training of deep neural networks with its flexible optimization properties, there are still many grounds to explore to utilize the benefits of reinforcement learning, such as its reward-driven adaptability, state representations, temporal structures and generalizability. In this survey, we present an overview of recent advancements of reinforcement learning and bandits, and discuss how they can be effectively employed to solve speech and natural language processing problems with models that are adaptive, interactive and scalable.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {C\:\\Users\\Hytn\\Zotero\\storage\\LCCFFZE6\\Lin - 2022 - Reinforcement Learning and Bandits for Speech and .pdf}
}

@inproceedings{liu-lapata-2019-hierarchical,
  title = {Hierarchical Transformers for Multi-Document Summarization},
  booktitle = {Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics},
  author = {Liu, Yang and Lapata, Mirella},
  year = {2019},
  month = jul,
  pages = {5070--5081},
  publisher = {{Association for Computational Linguistics}},
  address = {{Florence, Italy}},
  doi = {10.18653/v1/P19-1500},
  abstract = {In this paper, we develop a neural summarization model which can effectively process multiple input documents and distill Transformer architecture with the ability to encode documents in a hierarchical manner. We represent cross-document relationships via an attention mechanism which allows to share information as opposed to simply concatenating text spans and processing them as a flat sequence. Our model learns latent dependencies among textual units, but can also take advantage of explicit graph representations focusing on similarity or discourse relations. Empirical results on the WikiSum dataset demonstrate that the proposed architecture brings substantial improvements over several strong baselines.}
}

@inproceedings{liu2018generating,
  title = {Generating Wikipedia by Summarizing Long Sequences},
  booktitle = {International Conference on Learning Representations},
  author = {Liu*, Peter J. and Saleh*, Mohammad and Pot, Etienne and Goodrich, Ben and Sepassi, Ryan and Kaiser, Lukasz and Shazeer, Noam},
  year = {2018},
  keywords = {⛔ No DOI found},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\Generating wikipedia by summarizing long sequences _Liu et al 2018.pdf}
}

@article{liu2018learning,
  title = {Learning Structured Text Representations},
  author = {Liu, Yang and Lapata, Mirella},
  year = {2018},
  journal = {Transactions of the Association for Computational Linguistics},
  volume = {6},
  pages = {63--75},
  publisher = {{MIT Press}},
  doi = {10.1162/tacl-a-00005},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\Learning structured text representations _Liu_Lapata 22.pdf}
}

@inproceedings{liu2019hierarchical,
  title = {Hierarchical {{Transformers}} for {{Multi-Document Summarization}}},
  booktitle = {Proceedings of the 57th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}}},
  author = {Liu, Yang and Lapata, Mirella},
  year = {2019},
  pages = {5070--5081},
  publisher = {{Association for Computational Linguistics}},
  address = {{Florence, Italy}},
  doi = {10.18653/v1/P19-1500},
  urldate = {2022-08-27},
  langid = {english},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\Hierarchical Transformers for Multi-Document Summarization _Liu_Lapata 2019.pdf}
}

@inproceedings{liu2019incorporating,
  title = {Incorporating {{Priors}} with {{Feature Attribution}} on {{Text Classification}}},
  booktitle = {Proceedings of the 57th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}}},
  author = {Liu, Frederick and Avci, Besim},
  year = {2019},
  pages = {6274--6283},
  publisher = {{Association for Computational Linguistics}},
  address = {{Florence, Italy}},
  doi = {10.18653/v1/P19-1631},
  urldate = {2022-07-14},
  langid = {english},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\Incorporating Priors with Feature Attribution on Text Classification _Liu_Avci 22.pdf}
}

@article{liu2019roberta,
  title = {Roberta: {{A}} Robustly Optimized Bert Pretraining Approach},
  author = {Liu, Yinhan and Ott, Myle and Goyal, Naman and Du, Jingfei and Joshi, Mandar and Chen, Danqi and Levy, Omer and Lewis, Mike and Zettlemoyer, Luke and Stoyanov, Veselin},
  year = {2019},
  journal = {arXiv preprint arXiv:1907.11692},
  eprint = {1907.11692},
  archiveprefix = {arxiv},
  keywords = {⛔ No DOI found}
}

@inproceedings{liu2019single,
  title = {Single Document Summarization as Tree Induction},
  booktitle = {Proceedings of the 2019 {{Conference}} of the {{North American Chapter}} of the {{Association}} for {{Computational Linguistics}}: {{Human Language Technologies}}, {{Volume}} 1 ({{Long}} and {{Short Papers}})},
  author = {Liu, Yang and Titov, Ivan and Lapata, Mirella},
  year = {2019},
  pages = {1745--1755},
  keywords = {⛔ No DOI found}
}

@inproceedings{liu2019text,
  title = {Text {{Summarization}} with {{Pretrained Encoders}}},
  booktitle = {Proceedings of the 2019 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}} and the 9th {{International Joint Conference}} on {{Natural Language Processing}} ({{EMNLP-IJCNLP}})},
  author = {Liu, Yang and Lapata, Mirella},
  year = {2019},
  pages = {3728--3738},
  publisher = {{Association for Computational Linguistics}},
  address = {{Hong Kong, China}},
  doi = {10.18653/v1/D19-1387},
  urldate = {2022-07-26},
  langid = {english},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\Text Summarization with Pretrained Encoders _Liu_Lapata 22.pdf}
}

@inproceedings{liu2021element,
  title = {Element {{Intervention}} for {{Open Relation Extraction}}},
  booktitle = {Proceedings of the 59th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}} and the 11th {{International Joint Conference}} on {{Natural Language Processing}} ({{Volume}} 1: {{Long Papers}})},
  author = {Liu, Fangchao and Yan, Lingyong and Lin, Hongyu and Han, Xianpei and Sun, Le},
  year = {2021},
  pages = {4683--4693},
  publisher = {{Association for Computational Linguistics}},
  address = {{Online}},
  doi = {10.18653/v1/2021.acl-long.361},
  urldate = {2022-07-11},
  langid = {english},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\Element Intervention for Open Relation Extraction _Liu et al 22.pdf}
}

@misc{liu2021everything,
  title = {Everything {{Has}} a {{Cause}}: {{Leveraging Causal Inference}} in {{Legal Text Analysis}}},
  shorttitle = {Everything {{Has}} a {{Cause}}},
  author = {Liu, Xiao and Yin, Da and Feng, Yansong and Wu, Yuting and Zhao, Dongyan},
  year = {2021},
  month = apr,
  number = {arXiv:2104.09420},
  eprint = {arXiv:2104.09420},
  publisher = {{arXiv}},
  urldate = {2022-07-21},
  abstract = {Causal inference is the process of capturing cause-effect relationship among variables. Most existing works focus on dealing with structured data, while mining causal relationship among factors from unstructured data, like text, has been less examined, but is of great importance, especially in the legal domain. In this paper, we propose a novel Graph-based Causal Inference (GCI) framework, which builds causal graphs from fact descriptions without much human involvement and enables causal inference to facilitate legal practitioners to make proper decisions. We evaluate the framework on a challenging similar charge disambiguation task. Experimental results show that GCI can capture the nuance from fact descriptions among multiple confusing charges and provide explainable discrimination, especially in few-shot settings. We also observe that the causal knowledge contained in GCI can be effectively injected into powerful neural networks for better performance and interpretability. Code and data are available at https://github.com/xxxiaol/GCI/.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\Everything Has a Cause _Liu et al 22.pdf}
}

@inproceedings{liu2021hetero,
  title = {Heterogeneous Risk Minimization},
  booktitle = {Proceedings of the 38th International Conference on Machine Learning},
  author = {Liu, Jiashuo and Hu, Zheyuan and Cui, Peng and Li, Bo and Shen, Zheyan},
  editor = {Meila, Marina and Zhang, Tong},
  year = {2021},
  month = jul,
  series = {Proceedings of Machine Learning Research},
  volume = {139},
  pages = {6804--6814},
  publisher = {{PMLR}},
  abstract = {Machine learning algorithms with empirical risk minimization usually suffer from poor generalization performance due to the greedy exploitation of correlations among the training data, which are not stable under distributional shifts. Recently, some invariant learning methods for out-of-distribution (OOD) generalization have been proposed by leveraging multiple training environments to find invariant relationships. However, modern datasets are frequently assembled by merging data from multiple sources without explicit source labels. The resultant unobserved heterogeneity renders many invariant learning methods inapplicable. In this paper, we propose Heterogeneous Risk Minimization (HRM) framework to achieve joint learning of latent heterogeneity among the data and invariant relationship, which leads to stable prediction despite distributional shifts. We theoretically characterize the roles of the environment labels in invariant learning and justify our newly proposed HRM framework. Extensive experimental results validate the effectiveness of our HRM framework.},
  pdf = {http://proceedings.mlr.press/v139/liu21h/liu21h.pdf},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\Heterogeneous risk minimization _Liu et al 2021.pdf}
}

@inproceedings{liu2021learning,
  title = {Learning Causal Semantic Representation for Out-of-Distribution Prediction},
  booktitle = {Advances in Neural Information Processing Systems},
  author = {Liu, Chang and Sun, Xinwei and Wang, Jindong and Tang, Haoyue and Li, Tao and Qin, Tao and Chen, Wei and Liu, Tie-Yan},
  editor = {Ranzato, M. and Beygelzimer, A. and Dauphin, Y. and Liang, P.S. and Vaughan, J. Wortman},
  year = {2021},
  volume = {34},
  pages = {6155--6170},
  publisher = {{Curran Associates, Inc.}},
  keywords = {⛔ No DOI found},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\Learning causal semantic representation for out-of-distribution prediction _Liu et al 2021.pdf}
}

@inproceedings{liu2022floodingx,
  title = {Flooding-{{X}}: {{Improving BERT}}'s {{Resistance}} to {{Adversarial Attacks}} via {{Loss-Restricted Fine-Tuning}}},
  shorttitle = {Flooding-{{X}}},
  booktitle = {Proceedings of the 60th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}} ({{Volume}} 1: {{Long Papers}})},
  author = {Liu, Qin and Zheng, Rui and Rong, Bao and Liu, Jingyi and Liu, ZhiHua and Cheng, Zhanzhan and Qiao, Liang and Gui, Tao and Zhang, Qi and Huang, Xuanjing},
  year = {2022},
  pages = {5634--5644},
  publisher = {{Association for Computational Linguistics}},
  address = {{Dublin, Ireland}},
  doi = {10.18653/v1/2022.acl-long.386},
  urldate = {2022-06-09},
  langid = {english},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\Flooding-X _Liu et al 22.pdf}
}

@misc{liu2022generated,
  title = {Generated {{Knowledge Prompting}} for {{Commonsense Reasoning}}},
  author = {Liu, Jiacheng and Liu, Alisa and Lu, Ximing and Welleck, Sean and West, Peter and Bras, Ronan Le and Choi, Yejin and Hajishirzi, Hannaneh},
  year = {2022},
  month = mar,
  number = {arXiv:2110.08387},
  eprint = {arXiv:2110.08387},
  publisher = {{arXiv}},
  urldate = {2022-06-10},
  abstract = {It remains an open question whether incorporating external knowledge benefits commonsense reasoning while maintaining the flexibility of pretrained sequence models. To investigate this question, we develop generated knowledge prompting, which consists of generating knowledge from a language model, then providing the knowledge as additional input when answering a question. Our method does not require task-specific supervision for knowledge integration, or access to a structured knowledge base, yet it improves performance of large-scale, state-of-the-art models on four commonsense reasoning tasks, achieving state-of-the-art results on numerical commonsense (NumerSense), general commonsense (CommonsenseQA 2.0), and scientific commonsense (QASC) benchmarks. Generated knowledge prompting highlights large-scale language models as flexible sources of external knowledge for improving commonsense reasoning. Our code is available at \textbackslash url\{github.com/liujch1998/GKP\}},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Computation and Language},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\Generated Knowledge Prompting for Commonsense Reasoning _Liu et al 22.pdf}
}

@inproceedings{liu2022hiure,
  title = {{{HiURE}}: {{Hierarchical Exemplar Contrastive Learning}} for {{Unsupervised Relation Extraction}}},
  shorttitle = {{{HiURE}}},
  booktitle = {Proceedings of the 2022 {{Conference}} of the {{North American Chapter}} of the {{Association}} for {{Computational Linguistics}}: {{Human Language Technologies}}},
  author = {Liu, Shuliang and Hu, Xuming and Zhang, Chenwei and Li, Shu'ang and Wen, Lijie and Yu, Philip},
  year = {2022},
  pages = {5970--5980},
  publisher = {{Association for Computational Linguistics}},
  address = {{Seattle, United States}},
  doi = {10.18653/v1/2022.naacl-main.437},
  urldate = {2022-10-04},
  langid = {english},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\HiURE _Liu et al 2022.pdf}
}

@inproceedings{liu2022saliency,
  title = {Saliency as {{Evidence}}: {{Event Detection}} with {{Trigger Saliency Attribution}}},
  shorttitle = {Saliency as {{Evidence}}},
  booktitle = {Proceedings of the 60th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}} ({{Volume}} 1: {{Long Papers}})},
  author = {Liu, Jian and Chen, Yufeng and Xu, Jinan},
  year = {2022},
  pages = {4573--4585},
  publisher = {{Association for Computational Linguistics}},
  address = {{Dublin, Ireland}},
  doi = {10.18653/v1/2022.acl-long.313},
  urldate = {2022-06-29},
  langid = {english},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\Saliency as Evidence _Liu et al 22.pdf}
}

@inproceedings{liu2022simple,
  title = {A {{Simple}} yet {{Effective Relation Information Guided Approach}} for {{Few-Shot Relation Extraction}}},
  booktitle = {Findings of the {{Association}} for {{Computational Linguistics}}: {{ACL}} 2022},
  author = {Liu, Yang and Hu, Jinpeng and Wan, Xiang and Chang, Tsung-Hui},
  year = {2022},
  pages = {757--763},
  publisher = {{Association for Computational Linguistics}},
  address = {{Dublin, Ireland}},
  doi = {10.18653/v1/2022.findings-acl.62},
  urldate = {2022-06-13},
  langid = {english},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\A Simple yet Effective Relation Information Guided Approach for Few-Shot _Liu et al 4.pdf}
}

@inproceedings{liu2022simplea,
  title = {A {{Simple}} yet {{Effective Relation Information Guided Approach}} for {{Few-Shot Relation Extraction}}},
  booktitle = {Findings of the {{Association}} for {{Computational Linguistics}}: {{ACL}} 2022},
  author = {Liu, Yang and Hu, Jinpeng and Wan, Xiang and Chang, Tsung-Hui},
  year = {2022},
  pages = {757--763},
  publisher = {{Association for Computational Linguistics}},
  address = {{Dublin, Ireland}},
  doi = {10.18653/v1/2022.findings-acl.62},
  urldate = {2022-06-17},
  langid = {english},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\A Simple yet Effective Relation Information Guided Approach for Few-Shot _Liu et al 3.pdf}
}

@inproceedings{liu2022tokenlevel,
  title = {A {{Token-level Reference-free Hallucination Detection Benchmark}} for {{Free-form Text Generation}}},
  booktitle = {Proceedings of the 60th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}} ({{Volume}} 1: {{Long Papers}})},
  author = {Liu, Tianyu and Zhang, Yizhe and Brockett, Chris and Mao, Yi and Sui, Zhifang and Chen, Weizhu and Dolan, Bill},
  year = {2022},
  pages = {6723--6737},
  publisher = {{Association for Computational Linguistics}},
  address = {{Dublin, Ireland}},
  doi = {10.18653/v1/2022.acl-long.464},
  urldate = {2022-11-04},
  langid = {english},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\A Token-level Reference-free Hallucination Detection Benchmark for Free-form _Liu et al 2022.pdf}
}

@article{liu2023computational,
  title = {Computational {{Language Acquisition}} with {{Theory}} of {{Mind}}},
  author = {Liu, Andy and Zhu, Hao and Liu, Emmy and Bisk, Yonatan and Neubig, Graham},
  year = {2023},
  abstract = {Unlike current state-of-the-art language models, young children actively acquire language through interactions with their surrounding environment and caretakers. One mechanism that has been argued to be critical to language learning is the ability to infer the mental states of other agents in social environments, coined Theory of Mind (ToM) by Premack \& Woodruff (1978). Drawing inspiration from the modern operationalized versions of ToM implemented in Rabinowitz et al. (2018) and Zhu et al. (2021), we build language-learning agents equipped with ToM, and measure its effects on the learning process. We model ToM by giving the speaker agent an internal listener model that is trained alongside the speaker and used to rerank potential utterances. We experiment with varying task difficulty, hypothesizing that models will acquire more complex language to adapt to stronger environmental pressures. We find that training speakers with a highly weighted ToM listener component leads to performance gains in our image referential game setting. We also find some evidence that increasing task difficulty in the training process results in more fluent and precise utterances in evaluation. This suggests the potential utility of further incorporating ToM, as well as other insights from child language acquisition, into computational models of language acquisition1.},
  langid = {english},
  keywords = {⛔ No DOI found},
  file = {C\:\\Users\\Hytn\\Zotero\\storage\\6V6R64F5\\Liu et al. - 2023 - COMPUTATIONAL LANGUAGE ACQUISITION WITH THEORY OF .pdf}
}

@inproceedings{Liu2023DocumentlevelRE,
  title = {Document-Level Relation Extraction with Cross-Sentence Reasoning Graph},
  author = {Liu, Hongfei and Kang, Zhao and Zhang, Lizong and Tian, Ling and Hua, Fujun},
  year = {2023},
  keywords = {⛔ No DOI found},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\Document-level relation extraction with cross-sentence reasoning graph _Liu et al 2023.pdf}
}

@article{loshchilov2017decoupled,
  title = {Decoupled Weight Decay Regularization},
  author = {Loshchilov, Ilya and Hutter, Frank},
  year = {2017},
  journal = {arXiv preprint arXiv:1711.05101},
  eprint = {1711.05101},
  archiveprefix = {arxiv},
  keywords = {⛔ No DOI found}
}

@inproceedings{loshchilov2019decoupled,
  title = {Decoupled {{Weight Decay Regularization}}},
  booktitle = {{{ICLR}}},
  author = {Loshchilov, I. and Hutter, F.},
  year = {2019},
  keywords = {⛔ No DOI found}
}

@article{loureiro2022lmms,
  title = {{{LMMS}} Reloaded: {{Transformer-based}} Sense Embeddings for Disambiguation and Beyond},
  shorttitle = {{{LMMS}} Reloaded},
  author = {Loureiro, Daniel and M{\'a}rio Jorge, Al{\'i}pio and {Camacho-Collados}, Jose},
  year = {2022},
  month = apr,
  journal = {Artificial Intelligence},
  volume = {305},
  pages = {103661},
  issn = {00043702},
  doi = {10.1016/j.artint.2022.103661},
  urldate = {2022-05-02},
  langid = {english},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\LMMS reloaded _Loureiro et al 22.pdf}
}

@inproceedings{lu2021text2event,
  title = {{{Text2Event}}: {{Controllable Sequence-to-Structure Generation}} for {{End-to-end Event Extraction}}},
  booktitle = {Proceedings of the 59th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}} and the 11th {{International Joint Conference}} on {{Natural Language Processing}} ({{Volume}} 1: {{Long Papers}})},
  author = {Lu, Yaojie and Lin, Hongyu and Xu, Jin and Han, Xianpei and Tang, Jialong and Li, Annan and Sun, Le and Liao, Meng and Chen, Shaoyi},
  year = {2021},
  pages = {2795--2806},
  publisher = {{Association for Computational Linguistics}},
  doi = {10.18653/v1/2021.acl-long.217},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\Text2Event _Lu et al 22.pdf}
}

@inproceedings{lu2022unified,
  title = {Unified Structure Generation for Universal Information Extraction},
  booktitle = {Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: {{Long}} Papers)},
  author = {Lu, Yaojie and Liu, Qing and Dai, Dai and Xiao, Xinyan and Lin, Hongyu and Han, Xianpei and Sun, Le and Wu, Hua},
  year = {2022},
  month = may,
  pages = {5755--5772},
  publisher = {{Association for Computational Linguistics}},
  address = {{Dublin, Ireland}},
  abstract = {Information extraction suffers from its varying targets, heterogeneous structures, and demand-specific schemas. In this paper, we propose a unified text-to-structure generation framework, namely UIE, which can universally model different IE tasks, adaptively generate targeted structures, and collaboratively learn general IE abilities from different knowledge sources. Specifically, UIE uniformly encodes different extraction structures via a structured extraction language, adaptively generates target extractions via a schema-based prompt mechanism \textendash{} structural schema instructor, and captures the common IE abilities via a large-scale pretrained text-to-structure model. Experiments show that UIE achieved the state-of-the-art performance on 4 IE tasks, 13 datasets, and on all supervised, low-resource, and few-shot settings for a wide range of entity, relation, event and sentiment extraction tasks and their unification. These results verified the effectiveness, universality, and transferability of UIE.},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\Unified structure generation for universal information extraction _Lu et al 22.pdf}
}

@inproceedings{luan2018multitask,
  title = {Multi-{{Task Identification}} of {{Entities}}, {{Relations}}, and {{Coreference}} for {{Scientific Knowledge Graph Construction}}},
  booktitle = {Proceedings of the 2018 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}}},
  author = {Luan, Yi and He, Luheng and Ostendorf, Mari and Hajishirzi, Hannaneh},
  year = {2018},
  pages = {3219--3232},
  publisher = {{Association for Computational Linguistics}},
  address = {{Brussels, Belgium}},
  doi = {10.18653/v1/D18-1360},
  urldate = {2022-11-04},
  langid = {english},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\Multi-Task Identification of Entities, Relations, and Coreference for _Luan et al 2018.pdf}
}

@misc{luo2022understanding,
  title = {Understanding {{Diffusion Models}}: {{A Unified Perspective}}},
  shorttitle = {Understanding {{Diffusion Models}}},
  author = {Luo, Calvin},
  year = {2022},
  month = aug,
  number = {arXiv:2208.11970},
  eprint = {arXiv:2208.11970},
  publisher = {{arXiv}},
  urldate = {2022-12-04},
  abstract = {Diffusion models have shown incredible capabilities as generative models; indeed, they power the current state-of-the-art models on text-conditioned image generation such as Imagen and DALL-E 2. In this work we review, demystify, and unify the understanding of diffusion models across both variational and score-based perspectives. We first derive Variational Diffusion Models (VDM) as a special case of a Markovian Hierarchical Variational Autoencoder, where three key assumptions enable tractable computation and scalable optimization of the ELBO. We then prove that optimizing a VDM boils down to learning a neural network to predict one of three potential objectives: the original source input from any arbitrary noisification of it, the original source noise from any arbitrarily noisified input, or the score function of a noisified input at any arbitrary noise level. We then dive deeper into what it means to learn the score function, and connect the variational perspective of a diffusion model explicitly with the Score-based Generative Modeling perspective through Tweedie's Formula. Lastly, we cover how to learn a conditional distribution using diffusion models via guidance.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {C\:\\Users\\Hytn\\Zotero\\storage\\28L8RU6K\\Luo - 2022 - Understanding Diffusion Models A Unified Perspect.pdf}
}

@inproceedings{luong2015effective,
  title = {Effective {{Approaches}} to {{Attention-based Neural Machine Translation}}},
  booktitle = {Proceedings of the 2015 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}}},
  author = {Luong, Minh-Thang and Pham, Hieu and Manning, Christopher D.},
  year = {2015},
  pages = {1412--1421},
  doi = {10.18653/v1/D15-1166},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\Effective Approaches to Attention-based Neural Machine Translation _Luong et al 22.pdf}
}

@inproceedings{ma2016user,
  title = {User {{Fatigue}} in {{Online News Recommendation}}},
  booktitle = {Proceedings of the 25th {{International Conference}} on {{World Wide Web}}},
  author = {Ma, Hao and Liu, Xueqing and Shen, Zhihong},
  year = {2016},
  month = apr,
  pages = {1363--1372},
  publisher = {{International World Wide Web Conferences Steering Committee}},
  address = {{Montr\'eal Qu\'ebec Canada}},
  doi = {10.1145/2872427.2874813},
  urldate = {2022-08-12},
  isbn = {978-1-4503-4143-1},
  langid = {english},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\User Fatigue in Online News Recommendation _Ma et al 22.pdf}
}

@inproceedings{ma2018deepgauge,
  title = {{{DeepGauge}}: Multi-Granularity Testing Criteria for Deep Learning Systems},
  shorttitle = {{{DeepGauge}}},
  booktitle = {Proceedings of the 33rd {{ACM}}/{{IEEE International Conference}} on {{Automated Software Engineering}}},
  author = {Ma, Lei and {Juefei-Xu}, Felix and Zhang, Fuyuan and Sun, Jiyuan and Xue, Minhui and Li, Bo and Chen, Chunyang and Su, Ting and Li, Li and Liu, Yang and Zhao, Jianjun and Wang, Yadong},
  year = {2018},
  month = sep,
  pages = {120--131},
  publisher = {{ACM}},
  address = {{Montpellier France}},
  doi = {10.1145/3238147.3238202},
  urldate = {2022-11-23},
  isbn = {978-1-4503-5937-5},
  langid = {english},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\DeepGauge _Ma et al 2018.pdf}
}

@article{ma2021effective,
  title = {Effective {{Cascade Dual-Decoder Model}} for {{Joint Entity}} and {{Relation Extraction}}},
  author = {Ma, Lianbo and Ren, Huimin and Zhang, Xiliang},
  year = {2021},
  publisher = {{arXiv}},
  doi = {10.48550/ARXIV.2106.14163},
  urldate = {2022-04-04},
  abstract = {Extracting relational triples from texts is a fundamental task in knowledge graph construction. The popular way of existing methods is to jointly extract entities and relations using a single model, which often suffers from the overlapping triple problem. That is, there are multiple relational triples that share the same entities within one sentence. In this work, we propose an effective cascade dual-decoder approach to extract overlapping relational triples, which includes a text-specific relation decoder and a relation-corresponded entity decoder. Our approach is straightforward: the text-specific relation decoder detects relations from a sentence according to its text semantics and treats them as extra features to guide the entity extraction; for each extracted relation, which is with trainable embedding, the relation-corresponded entity decoder detects the corresponding head and tail entities using a span-based tagging scheme. In this way, the overlapping triple problem is tackled naturally. Experiments on two public datasets demonstrate that our proposed approach outperforms state-of-the-art methods and achieves better F1 scores under the strict evaluation metric. Our implementation is available at https://github.com/prastunlp/DualDec.},
  copyright = {arXiv.org perpetual, non-exclusive license},
  keywords = {Artificial Intelligence (cs.AI),Computation and Language (cs.CL),FOS: Computer and information sciences},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\Effective Cascade Dual-Decoder Model for Joint Entity and Relation Extraction _Ma et al 22.pdf}
}

@article{ma2022multidocument,
  title = {Multi-Document {{Summarization}} via {{Deep Learning Techniques}}: {{A Survey}}},
  shorttitle = {Multi-Document {{Summarization}} via {{Deep Learning Techniques}}},
  author = {Ma, Congbo and Zhang, Wei Emma and Guo, Mingyu and Wang, Hu and Sheng, Quan Z.},
  year = {2022},
  month = apr,
  journal = {ACM Computing Surveys},
  pages = {3529754},
  issn = {0360-0300, 1557-7341},
  doi = {10.1145/3529754},
  urldate = {2022-08-25},
  abstract = {Multi-document summarization (MDS) is an effective tool for information aggregation that generates an informative and concise summary from a cluster of topic-related documents. Our survey, the first of its kind, systematically overviews the recent deep learning based MDS models. We propose a novel taxonomy to summarize the design strategies of neural networks and conduct a comprehensive summary of the state-of-the-art. We highlight the differences between various objective functions that are rarely discussed in the existing literature. Finally, we propose several future directions pertaining to this new and exciting field.},
  langid = {english},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\Multi-document Summarization via Deep Learning Techniques _Ma et al 2022.pdf}
}

@article{Ma2023DREEAMGA,
  title = {{{DREEAM}}: {{Guiding}} Attention with Evidence for Improving Document-Level Relation Extraction},
  author = {Ma, Youmi and Wang, An and Okazaki, Naoaki},
  year = {2023},
  journal = {ArXiv},
  volume = {abs/2302.08675},
  keywords = {⛔ No DOI found},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\DREEAM _Ma et al 2023.pdf}
}

@inproceedings{maddela2022entsum,
  title = {{{EntSUM}}: {{A Data Set}} for {{Entity-Centric Extractive Summarization}}},
  shorttitle = {{{EntSUM}}},
  booktitle = {Proceedings of the 60th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}} ({{Volume}} 1: {{Long Papers}})},
  author = {Maddela, Mounica and Kulkarni, Mayank and {Preotiuc-Pietro}, Daniel},
  year = {2022},
  pages = {3355--3366},
  publisher = {{Association for Computational Linguistics}},
  address = {{Dublin, Ireland}},
  doi = {10.18653/v1/2022.acl-long.237},
  urldate = {2022-06-04},
  langid = {english},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\EntSUM _Maddela et al 22.pdf}
}

@misc{mao2022dyle,
  title = {{{DYLE}}: {{Dynamic Latent Extraction}} for {{Abstractive Long-Input Summarization}}},
  shorttitle = {{{DYLE}}},
  author = {Mao, Ziming and Wu, Chen Henry and Ni, Ansong and Zhang, Yusen and Zhang, Rui and Yu, Tao and Deb, Budhaditya and Zhu, Chenguang and Awadallah, Ahmed H. and Radev, Dragomir},
  year = {2022},
  month = apr,
  number = {arXiv:2110.08168},
  eprint = {arXiv:2110.08168},
  publisher = {{arXiv}},
  urldate = {2022-06-04},
  abstract = {Transformer-based models have achieved state-of-the-art performance on short-input summarization. However, they still struggle with summarizing longer text. In this paper, we present DYLE, a novel dynamic latent extraction approach for abstractive long-input summarization. DYLE jointly trains an extractor and a generator and treats the extracted text snippets as the latent variable, allowing dynamic snippet-level attention weights during decoding. To provide adequate supervision, we propose simple yet effective heuristics for oracle extraction as well as a consistency loss term, which encourages the extractor to approximate the averaged dynamic weights predicted by the generator. We evaluate our method on different long-document and long-dialogue summarization tasks: GovReport, QMSum, and arXiv. Experiment results show that DYLE outperforms all existing methods on GovReport and QMSum, with gains up to 6.1 ROUGE, while yielding strong results on arXiv. Further analysis shows that the proposed dynamic weights provide interpretability of our generation process.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Computation and Language},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\DYLE _Mao et al 22.pdf}
}

@inproceedings{mao2022dylea,
  title = {{{DYLE}}: {{Dynamic Latent Extraction}} for {{Abstractive Long-Input Summarization}}},
  shorttitle = {{{DYLE}}},
  booktitle = {Proceedings of the 60th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}} ({{Volume}} 1: {{Long Papers}})},
  author = {Mao, Ziming and Wu, Chen Henry and Ni, Ansong and Zhang, Yusen and Zhang, Rui and Yu, Tao and Deb, Budhaditya and Zhu, Chenguang and Awadallah, Ahmed and Radev, Dragomir},
  year = {2022},
  pages = {1687--1698},
  publisher = {{Association for Computational Linguistics}},
  address = {{Dublin, Ireland}},
  doi = {10.18653/v1/2022.acl-long.118},
  urldate = {2022-08-20},
  langid = {english},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\DYLE _Mao et al 2022.pdf}
}

@inproceedings{marcheggiani2017encoding,
  title = {Encoding {{Sentences}} with {{Graph Convolutional Networks}} for {{Semantic Role Labeling}}},
  booktitle = {Proceedings of the 2017 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}}},
  author = {Marcheggiani, Diego and Titov, Ivan},
  year = {2017},
  pages = {1506--1515},
  doi = {10.18653/v1/D17-1159},
  keywords = {⛔ No DOI found},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\Encoding Sentences with Graph Convolutional Networks for Semantic Role Labeling _Marcheggiani_Titov 2017.pdf}
}

@inproceedings{martin1996dbscan,
  title = {A Density-Based Algorithm for Discovering Clusters in Large Spatial Databases with Noise},
  booktitle = {Proceedings of the Second International Conference on Knowledge Discovery and Data Mining},
  author = {Ester, Martin and Kriegel, Hans-Peter and Sander, J{\"o}rg and Xu, Xiaowei},
  year = {1996},
  series = {{{KDD}}'96},
  pages = {226--231},
  publisher = {{AAAI Press}},
  address = {{Portland, Oregon}},
  abstract = {Clustering algorithms are attractive for the task of class identification in spatial databases. However, the application to large spatial databases rises the following requirements for clustering algorithms: minimal requirements of domain knowledge to determine the input parameters, discovery of clusters with arbitrary shape and good efficiency on large databases. The well-known clustering algorithms offer no solution to the combination of these requirements. In this paper, we present the new clustering algorithm DBSCAN relying on a density-based notion of clusters which is designed to discover clusters of arbitrary shape. DBSCAN requires only one input parameter and supports the user in determining an appropriate value for it. We performed an experimental evaluation of the effectiveness and efficiency of DBSCAN using synthetic data and real data of the SEQUOIA 2000 benchmark. The results of our experiments demonstrate that (1) DBSCAN is significantly more effective in discovering clusters of arbitrary shape than the well-known algorithm CLAR-ANS, and that (2) DBSCAN outperforms CLARANS by a factor of more than 100 in terms of efficiency.},
  keywords = {⛔ No DOI found,arbitrary shape of clusters,clustering algorithms,efficiency on large spatial databases,handling nlj4-275oise},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\A density-based algorithm for discovering clusters in large spatial databases _Ester et al 1996.pdf}
}

@inproceedings{martinezlorenzo2022fullysemantic,
  title = {Fully-{{Semantic Parsing}} and {{Generation}}: The {{BabelNet Meaning Representation}}},
  shorttitle = {Fully-{{Semantic Parsing}} and {{Generation}}},
  booktitle = {Proceedings of the 60th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}} ({{Volume}} 1: {{Long Papers}})},
  author = {Mart{\'i}nez Lorenzo, Abelardo Carlos and Maru, Marco and Navigli, Roberto},
  year = {2022},
  pages = {1727--1741},
  publisher = {{Association for Computational Linguistics}},
  address = {{Dublin, Ireland}},
  doi = {10.18653/v1/2022.acl-long.121},
  urldate = {2022-06-09},
  langid = {english},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\Fully-Semantic Parsing and Generation _Martínez Lorenzo et al 22.pdf}
}

@inproceedings{matthias20automatic,
  title = {Automatic Shortcut Removal for Self-Supervised Representation Learning},
  booktitle = {Proceedings of the 37th International Conference on Machine Learning},
  author = {Minderer, Matthias and Bachem, Olivier and Houlsby, Neil and Tschannen, Michael},
  editor = {III, Hal Daum{\'e} and Singh, Aarti},
  year = {2020},
  month = jul,
  series = {Proceedings of Machine Learning Research},
  volume = {119},
  pages = {6927--6937},
  publisher = {{PMLR}},
  abstract = {In self-supervised visual representation learning, a feature extractor is trained on a "pretext task" for which labels can be generated cheaply, without human annotation. A central challenge in this approach is that the feature extractor quickly learns to exploit low-level visual features such as color aberrations or watermarks and then fails to learn useful semantic representations. Much work has gone into identifying such "shortcut" features and hand-designing schemes to reduce their effect. Here, we propose a general framework for mitigating the effect shortcut features. Our key assumption is that those features which are the first to be exploited for solving the pretext task may also be the most vulnerable to an adversary trained to make the task harder. We show that this assumption holds across common pretext tasks and datasets by training a "lens" network to make small image changes that maximally reduce performance in the pretext task. Representations learned with the modified images outperform those learned without in all tested cases. Additionally, the modifications made by the lens reveal how the choice of pretext task and dataset affects the features learned by self-supervision.},
  pdf = {http://proceedings.mlr.press/v119/minderer20a/minderer20a.pdf},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\Automatic shortcut removal for self-supervised representation learning _Minderer et al 22.pdf}
}

@inproceedings{mccoy2019right,
  title = {Right for the {{Wrong Reasons}}: {{Diagnosing Syntactic Heuristics}} in {{Natural Language Inference}}},
  shorttitle = {Right for the {{Wrong Reasons}}},
  booktitle = {Proceedings of the 57th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}}},
  author = {McCoy, Tom and Pavlick, Ellie and Linzen, Tal},
  year = {2019},
  pages = {3428--3448},
  publisher = {{Association for Computational Linguistics}},
  address = {{Florence, Italy}},
  doi = {10.18653/v1/P19-1334},
  urldate = {2022-07-13},
  langid = {english},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\Right for the Wrong Reasons _McCoy et al 22.pdf}
}

@article{mcgrath2022acquisition,
  title = {Acquisition of Chess Knowledge in {{AlphaZero}}},
  author = {McGrath, Thomas and Kapishnikov, Andrei and Toma{\v s}ev, Nenad and Pearce, Adam and Wattenberg, Martin and Hassabis, Demis and Kim, Been and Paquet, Ulrich and Kramnik, Vladimir},
  year = {2022},
  month = nov,
  journal = {Proceedings of the National Academy of Sciences},
  volume = {119},
  number = {47},
  pages = {e2206625119},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.2206625119},
  urldate = {2023-03-21},
  abstract = {We analyze the knowledge acquired by AlphaZero, a neural network engine that learns chess solely by playing against itself yet becomes capable of outperforming human chess players. Although the system trains without access to human games or guidance, it appears to learn concepts analogous to those used by human chess players. We provide two lines of evidence. Linear probes applied to AlphaZero's internal state enable us to quantify when and where such concepts are represented in the network. We also describe a behavioral analysis of opening play, including qualitative commentary by a former world chess champion.},
  langid = {english},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\Acquisition of chess knowledge in AlphaZero _McGrath et al 2022.pdf}
}

@inproceedings{mcmilin2022selection,
  title = {Selection Bias Induced Spurious Correlations in Large Language Models},
  booktitle = {{{ICML}} 2022: {{Workshop}} on Spurious Correlations, Invariance and Stability},
  author = {McMilin, Emily},
  year = {2022},
  keywords = {⛔ No DOI found},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\Selection bias induced spurious correlations in large language models _McMilin 2022.pdf}
}

@article{medvedeva2022rethinking,
  title = {Rethinking the Field of Automatic Prediction of Court Decisions},
  author = {Medvedeva, Masha and Wieling, Martijn and Vols, Michel},
  year = {2022},
  month = jan,
  journal = {Artificial Intelligence and Law},
  issn = {0924-8463, 1572-8382},
  doi = {10.1007/s10506-021-09306-3},
  urldate = {2022-07-21},
  abstract = {In this paper, we discuss previous research in automatic prediction of court decisions. We define the difference between outcome identification, outcome-based judgement categorisation and outcome forecasting, and review how various studies fall into these categories. We discuss how important it is to understand the legal data that one works with in order to determine which task can be performed. Finally, we reflect on the needs of the legal discipline regarding the analysis of court judgements.},
  langid = {english},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\Rethinking the field of automatic prediction of court decisions _Medvedeva et al 22.pdf}
}

@inproceedings{mendelson2021debiasing,
  title = {Debiasing {{Methods}} in {{Natural Language Understanding Make Bias More Accessible}}},
  booktitle = {Proceedings of the 2021 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}}},
  author = {Mendelson, Michael and Belinkov, Yonatan},
  year = {2021},
  pages = {1545--1557},
  publisher = {{Association for Computational Linguistics}},
  address = {{Online and Punta Cana, Dominican Republic}},
  doi = {10.18653/v1/2021.emnlp-main.116},
  urldate = {2022-09-02},
  langid = {english},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\Debiasing Methods in Natural Language Understanding Make Bias More Accessible _Mendelson_Belinkov 2021.pdf}
}

@article{merrill2019generalized,
  title = {Generalized {{Integrated Gradients}}: {{A}} Practical Method for Explaining Diverse Ensembles},
  shorttitle = {Generalized {{Integrated Gradients}}},
  author = {Merrill, John and Ward, Geoff and Kamkar, Sean and Budzik, Jay and Merrill, Douglas},
  year = {2019},
  publisher = {{arXiv}},
  doi = {10.48550/ARXIV.1909.01869},
  urldate = {2022-07-11},
  abstract = {We introduce Generalized Integrated Gradients (GIG), a formal extension of the Integrated Gradients (IG) (Sundararajan et al., 2017) method for attributing credit to the input variables of a predictive model. GIG improves IG by explaining a broader variety of functions that arise from practical applications of ML in domains like financial services. GIG is constructed to overcome limitations of Shapley (1953) and Aumann-Shapley (1974), and has desirable properties when compared to other approaches. We prove GIG is the only correct method, under a small set of reasonable axioms, for providing explanations for mixed-type models or games. We describe the implementation, and present results of experiments on several datasets and systems of models.},
  copyright = {arXiv.org perpetual, non-exclusive license},
  keywords = {FOS: Computer and information sciences,GIG,I.2.6; H.1.2; K.4,Machine Learning (cs.LG),Machine Learning (stat.ML)},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\Generalized Integrated Gradients _Merrill et al 22.pdf}
}

@inproceedings{micikevicius2017mixed,
  title = {Mixed {{Precision Training}}},
  booktitle = {International {{Conference}} on {{Learning Representations}}},
  author = {Micikevicius, Paulius and Narang, Sharan and Alben, Jonah and Diamos, Gregory F. and Elsen, Erich and Garc{\'i}a, David and Ginsburg, Boris and Houston, Michael and Kuchaiev, Oleksii and Venkatesh, Ganesh and Wu, Hao},
  year = {2017},
  keywords = {⛔ No DOI found}
}

@inproceedings{mihalcea2004textrank,
  title = {Textrank: {{Bringing}} Order into Text},
  booktitle = {Proceedings of the 2004 Conference on Empirical Methods in Natural Language Processing},
  author = {Mihalcea, Rada and Tarau, Paul},
  year = {2004},
  pages = {404--411},
  keywords = {⛔ No DOI found},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\Textrank _Mihalcea_Tarau 2004.pdf}
}

@article{mikolov2013efficient,
  title = {Efficient {{Estimation}} of {{Word Representations}} in {{Vector Space}}},
  author = {Mikolov, Tomas and Chen, Kai and Corrado, Greg and Dean, Jeffrey},
  year = {2013},
  publisher = {{arXiv}},
  doi = {10.48550/ARXIV.1301.3781},
  urldate = {2022-07-01},
  abstract = {We propose two novel model architectures for computing continuous vector representations of words from very large data sets. The quality of these representations is measured in a word similarity task, and the results are compared to the previously best performing techniques based on different types of neural networks. We observe large improvements in accuracy at much lower computational cost, i.e. it takes less than a day to learn high quality word vectors from a 1.6 billion words data set. Furthermore, we show that these vectors provide state-of-the-art performance on our test set for measuring syntactic and semantic word similarities.},
  copyright = {arXiv.org perpetual, non-exclusive license},
  keywords = {Computation and Language (cs.CL),FOS: Computer and information sciences},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\Efficient Estimation of Word Representations in Vector Space _Mikolov et al 22.pdf}
}

@article{miller1995wordnet,
  title = {{{WordNet}}: A Lexical Database for {{English}}},
  shorttitle = {{{WordNet}}},
  author = {Miller, George A.},
  year = {1995},
  month = nov,
  journal = {Communications of the ACM},
  volume = {38},
  number = {11},
  pages = {39--41},
  issn = {0001-0782, 1557-7317},
  doi = {10.1145/219717.219748},
  urldate = {2022-08-07},
  abstract = {Because meaningful sentences are composed of meaningful words, any system that hopes to process natural languages as people do must have information about words and their meanings. This information is traditionally provided through dictionaries, and machine-readable dictionaries are now widely available. But dictionary entries evolved for the convenience of human readers, not for machines. WordNet               1               provides a more effective combination of traditional lexicographic information and modern computing. WordNet is an online lexical database designed for use under program control. English nouns, verbs, adjectives, and adverbs are organized into sets of synonyms, each representing a lexicalized concept. Semantic relations link the synonym sets [4].},
  langid = {english},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\WordNet _Miller 12.pdf}
}

@inproceedings{mintz2009distant,
  title = {Distant Supervision for Relation Extraction without Labeled Data},
  booktitle = {Proceedings of the {{Joint Conference}} of the 47th {{Annual Meeting}} of the {{ACL}} and the 4th {{International Joint Conference}} on {{Natural Language Processing}} of the {{AFNLP}}},
  author = {Mintz, Mike and Bills, Steven and Snow, Rion and Jurafsky, Dan},
  year = {2009},
  pages = {1003--1011},
  keywords = {⛔ No DOI found}
}

@inproceedings{mishra2022crosstask,
  title = {Cross-{{Task Generalization}} via {{Natural Language Crowdsourcing Instructions}}},
  booktitle = {Proceedings of the 60th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}} ({{Volume}} 1: {{Long Papers}})},
  author = {Mishra, Swaroop and Khashabi, Daniel and Baral, Chitta and Hajishirzi, Hannaneh},
  year = {2022},
  pages = {3470--3487},
  publisher = {{Association for Computational Linguistics}},
  address = {{Dublin, Ireland}},
  doi = {10.18653/v1/2022.acl-long.244},
  urldate = {2022-11-01},
  langid = {english},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\Cross-Task Generalization via Natural Language Crowdsourcing Instructions _Mishra et al 2022.pdf}
}

@article{mitchell2023debate,
  title = {The Debate over Understanding in {{AI}}'s Large Language Models},
  author = {Mitchell, Melanie and Krakauer, David C.},
  year = {2023},
  month = mar,
  journal = {Proceedings of the National Academy of Sciences},
  volume = {120},
  number = {13},
  pages = {e2215907120},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.2215907120},
  urldate = {2023-04-03},
  abstract = {We survey a current, heated debate in the artificial intelligence (AI) research community on whether large pretrained language models can be said to understand language\textemdash and the physical and social situations language encodes\textemdash in any humanlike sense. We describe arguments that have been made for and against such understanding and key questions for the broader sciences of intelligence that have arisen in light of these arguments. We contend that an extended science of intelligence can be developed that will provide insight into distinct modes of understanding, their strengths and limitations, and the challenge of integrating diverse forms of cognition.},
  langid = {english},
  file = {C\:\\Users\\Hytn\\Zotero\\storage\\MD5GN2WB\\Mitchell and Krakauer - 2023 - The debate over understanding in AI’s large langua.pdf}
}

@inproceedings{miwa2014modeling,
  title = {Modeling Joint Entity and Relation Extraction with Table Representation},
  booktitle = {Proceedings of the 2014 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}} ({{EMNLP}})},
  author = {Miwa, Makoto and Sasaki, Yutaka},
  year = {2014},
  pages = {1858--1869},
  doi = {10.3115/v1/D14-1200},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\Modeling joint entity and relation extraction with table representation _Miwa_Sasaki 22.pdf}
}

@article{miwa2016endtoend,
  title = {End-to-End Relation Extraction Using Lstms on Sequences and Tree Structures},
  author = {Miwa, Makoto and Bansal, Mohit},
  year = {2016},
  journal = {arXiv preprint arXiv:1601.00770},
  eprint = {1601.00770},
  archiveprefix = {arxiv},
  keywords = {⛔ No DOI found}
}

@misc{mollo2023vector,
  title = {The {{Vector Grounding Problem}}},
  author = {Mollo, Dimitri Coelho and Milli{\`e}re, Rapha{\"e}l},
  year = {2023},
  month = apr,
  number = {arXiv:2304.01481},
  eprint = {arXiv:2304.01481},
  publisher = {{arXiv}},
  urldate = {2023-04-09},
  abstract = {The remarkable performance of large language models (LLMs) on complex linguistic tasks has sparked a lively debate on the nature of their capabilities. Unlike humans, these models learn language exclusively from textual data, without direct interaction with the real world. Nevertheless, they can generate seemingly meaningful text about a wide range of topics. This impressive accomplishment has rekindled interest in the classical 'Symbol Grounding Problem,' which questioned whether the internal representations and outputs of classical symbolic AI systems could possess intrinsic meaning. Unlike these systems, modern LLMs are artificial neural networks that compute over vectors rather than symbols. However, an analogous problem arises for such systems, which we dub the Vector Grounding Problem. This paper has two primary objectives. First, we differentiate various ways in which internal representations can be grounded in biological or artificial systems, identifying five distinct notions discussed in the literature: referential, sensorimotor, relational, communicative, and epistemic grounding. Unfortunately, these notions of grounding are often conflated. We clarify the differences between them, and argue that referential grounding is the one that lies at the heart of the Vector Grounding Problem. Second, drawing on theories of representational content in philosophy and cognitive science, we propose that certain LLMs, particularly those fine-tuned with Reinforcement Learning from Human Feedback (RLHF), possess the necessary features to overcome the Vector Grounding Problem, as they stand in the requisite causal-historical relations to the world that underpin intrinsic meaning. We also argue that, perhaps unexpectedly, multimodality and embodiment are neither necessary nor sufficient conditions for referential grounding in artificial systems.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Computation and Language},
  file = {C\:\\Users\\Hytn\\Zotero\\storage\\TP7KDCNA\\Mollo and Millière - 2023 - The Vector Grounding Problem.pdf}
}

@book{molnar2020interpretable,
  title = {Interpretable Machine Learning},
  author = {Molnar, Christoph},
  year = {2020},
  publisher = {{Lulu. com}}
}

@article{montavon2017explaining,
  title = {Explaining Nonlinear Classification Decisions with Deep {{Taylor}} Decomposition},
  author = {Montavon, Gr{\'e}goire and Lapuschkin, Sebastian and Binder, Alexander and Samek, Wojciech and M{\"u}ller, Klaus-Robert},
  year = {2017},
  month = may,
  journal = {Pattern Recognition},
  volume = {65},
  pages = {211--222},
  issn = {00313203},
  doi = {10.1016/j.patcog.2016.11.008},
  urldate = {2022-07-11},
  langid = {english},
  keywords = {DTD},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\Explaining nonlinear classification decisions with deep Taylor decomposition _Montavon et al 22.pdf}
}

@article{montavon2018methods,
  title = {Methods for Interpreting and Understanding Deep Neural Networks},
  author = {Montavon, Gr{\'e}goire and Samek, Wojciech and M{\"u}ller, Klaus-Robert},
  year = {2018},
  month = feb,
  journal = {Digital Signal Processing},
  volume = {73},
  pages = {1--15},
  issn = {10512004},
  doi = {10.1016/j.dsp.2017.10.011},
  urldate = {2022-03-13},
  langid = {english},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\Methods for interpreting and understanding deep neural networks _Montavon et al 22.pdf}
}

@inproceedings{moro2022discriminative,
  title = {Discriminative {{Marginalized Probabilistic Neural Method}} for {{Multi-Document Summarization}} of {{Medical Literature}}},
  booktitle = {Proceedings of the 60th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}} ({{Volume}} 1: {{Long Papers}})},
  author = {Moro, Gianluca and Ragazzi, Luca and Valgimigli, Lorenzo and Freddi, Davide},
  year = {2022},
  pages = {180--189},
  publisher = {{Association for Computational Linguistics}},
  address = {{Dublin, Ireland}},
  doi = {10.18653/v1/2022.acl-long.15},
  urldate = {2022-06-04},
  langid = {english},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\Discriminative Marginalized Probabilistic Neural Method for Multi-Document _Moro et al 22.pdf}
}

@article{mozesgradientbased,
  title = {Gradient-{{Based Automated Iterative Recovery}} for {{Parameter-Efficient Tuning}}},
  author = {Mozes, Maximilian and Bolukbasi, Tolga and Yuan, Ann and Thain, Nithum and Dixon, Lucas and Liu, Frederick},
  abstract = {Pretrained large language models (LLMs) are able to solve a wide variety of tasks through transfer learning. Various explainability methods have been developed to investigate their decision making process. TracIn (Pruthi et al., 2020) is one such gradient-based method which explains model inferences based on the influence of training examples. In this paper, we explore the use of TracIn to improve model performance in the parameter-efficient tuning (PET) setting. We develop conversational safety classifiers via the prompt-tuning PET method and show how the unique characteristics of the PET regime enable TracIn to identify the cause for certain misclassifications by LLMs. We develop a new methodology for using gradient-based explainability techniques to improve model performance, GBAIR: gradient-based automated iterative recovery. We show that G-BAIR can recover LLM performance on benchmarks after manually corrupting training labels. This suggests that influence methods like TracIn can be used to automatically perform data cleaning, and introduces the potential for interactive debugging and relabeling for PET-based transfer learning methods.},
  langid = {english},
  keywords = {⛔ No DOI found},
  file = {C\:\\Users\\Hytn\\Zotero\\storage\\DSZTZG3Z\\Mozes et al. - Gradient-Based Automated Iterative Recovery for Pa.pdf}
}

@inproceedings{muandet2013domain,
  title = {Domain Generalization via Invariant Feature Representation},
  booktitle = {Proceedings of the 30th International Conference on Machine Learning},
  author = {Muandet, Krikamol and Balduzzi, David and Sch{\"o}lkopf, Bernhard},
  editor = {Dasgupta, Sanjoy and McAllester, David},
  year = {2013},
  month = jun,
  series = {Proceedings of Machine Learning Research},
  volume = {28},
  pages = {10--18},
  publisher = {{PMLR}},
  address = {{Atlanta, Georgia, USA}},
  abstract = {This paper investigates domain generalization: How to take knowledge acquired from an arbitrary number of related domains and apply it to previously unseen domains? We propose Domain-Invariant Component Analysis (DICA), a kernel-based optimization algorithm that learns an invariant transformation by minimizing the dissimilarity across domains, whilst preserving the functional relationship between input and output variables. A learning-theoretic analysis shows that reducing dissimilarity improves the expected generalization ability of classifiers on new domains, motivating the proposed algorithm. Experimental results on synthetic and real-world datasets demonstrate that DICA successfully learns invariant features and improves classifier performance in practice.},
  pdf = {http://proceedings.mlr.press/v28/muandet13.pdf},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\Domain generalization via invariant feature representation _Muandet et al 2013.pdf}
}

@inproceedings{mudrakarta2018did,
  title = {Did the {{Model Understand}} the {{Question}}?},
  booktitle = {Proceedings of the 56th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}} ({{Volume}} 1: {{Long Papers}})},
  author = {Mudrakarta, Pramod Kaushik and Taly, Ankur and Sundararajan, Mukund and Dhamdhere, Kedar},
  year = {2018},
  pages = {1896--1906},
  publisher = {{Association for Computational Linguistics}},
  doi = {10.18653/v1/P18-1176},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\Did the Model Understand the Question _Mudrakarta et al 4.pdf}
}

@inproceedings{mudrakarta2018dida,
  title = {Did the {{Model Understand}} the {{Question}}?},
  booktitle = {Proceedings of the 56th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}} ({{Volume}} 1: {{Long Papers}})},
  author = {Mudrakarta, Pramod Kaushik and Taly, Ankur and Sundararajan, Mukund and Dhamdhere, Kedar},
  year = {2018},
  pages = {1896--1906},
  publisher = {{Association for Computational Linguistics}},
  address = {{Melbourne, Australia}},
  doi = {10.18653/v1/P18-1176},
  urldate = {2022-07-06},
  langid = {english},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\Did the Model Understand the Question _Mudrakarta et al 3.pdf}
}

@inproceedings{mueller2022label,
  title = {Label {{Semantic Aware Pre-training}} for {{Few-shot Text Classification}}},
  booktitle = {Proceedings of the 60th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}} ({{Volume}} 1: {{Long Papers}})},
  author = {Mueller, Aaron and Krone, Jason and Romeo, Salvatore and Mansour, Saab and Mansimov, Elman and Zhang, Yi and Roth, Dan},
  year = {2022},
  pages = {8318--8334},
  publisher = {{Association for Computational Linguistics}},
  address = {{Dublin, Ireland}},
  doi = {10.18653/v1/2022.acl-long.570},
  urldate = {2022-06-19},
  langid = {english},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\Label Semantic Aware Pre-training for Few-shot Text Classification _Mueller et al 22.pdf}
}

@inproceedings{mullenbach2018explainable,
  title = {Explainable {{Prediction}} of {{Medical Codes}} from {{Clinical Text}}},
  booktitle = {Proceedings of the 2018 {{Conference}} of the {{North American Chapter}} of           the {{Association}} for {{Computational Linguistics}}: {{Human Language}}           {{Technologies}}, {{Volume}} 1 ({{Long Papers}})},
  author = {Mullenbach, James and Wiegreffe, Sarah and Duke, Jon and Sun, Jimeng and Eisenstein, Jacob},
  year = {2018},
  pages = {1101--1111},
  publisher = {{Association for Computational Linguistics}},
  address = {{New Orleans, Louisiana}},
  doi = {10.18653/v1/N18-1100},
  urldate = {2022-06-20},
  langid = {english},
  keywords = {LWAN},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\Explainable Prediction of Medical Codes from Clinical Text _Mullenbach et al 22.pdf}
}

@article{nagel1960weighting,
  title = {{{WEIGHTING VARIABLES IN JUDICIAL PREDICTION}}},
  author = {Nagel, Stuart},
  year = {1960},
  journal = {MULL: Modern Uses of Logic in Law},
  volume = {2},
  number = {3},
  eprint = {29760840},
  eprinttype = {jstor},
  pages = {93--97},
  publisher = {{American Bar Association}},
  issn = {21589240},
  urldate = {2022-08-10},
  keywords = {⛔ No DOI found}
}

@misc{nahian2021training,
  title = {Training {{Value-Aligned Reinforcement Learning Agents Using}} a {{Normative Prior}}},
  author = {Nahian, Md Sultan Al and Frazier, Spencer and Harrison, Brent and Riedl, Mark},
  year = {2021},
  month = apr,
  number = {arXiv:2104.09469},
  eprint = {arXiv:2104.09469},
  publisher = {{arXiv}},
  urldate = {2023-02-04},
  abstract = {As more machine learning agents interact with humans, it is increasingly a prospect that an agent trained to perform a task optimally - using only a measure of task performance as feedback - can violate societal norms for acceptable behavior or cause harm. Value alignment is a property of intelligent agents wherein they solely pursue non-harmful behaviors or human-beneficial goals. We introduce an approach to valuealigned reinforcement learning, in which we train an agent with two reward signals: a standard task performance reward, plus a normative behavior reward. The normative behavior reward is derived from a value-aligned prior model previously shown to classify text as normative or non-normative. We show how variations on a policy shaping technique can balance these two sources of reward and produce policies that are both effective and perceived as being more normative. We test our value-alignment technique on three interactive textbased worlds; each world is designed specifically to challenge agents with a task as well as provide opportunities to deviate from the task to engage in normative and/or altruistic behavior.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Human-Computer Interaction,Computer Science - Machine Learning},
  file = {C\:\\Users\\Hytn\\Zotero\\storage\\ET4PN3NX\\Nahian et al. - 2021 - Training Value-Aligned Reinforcement Learning Agen.pdf}
}

@inproceedings{nan2020reasoning,
  title = {Reasoning with {{Latent Structure Refinement}} for {{Document-Level Relation Extraction}}},
  booktitle = {Proceedings of the 58th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}}},
  author = {Nan, Guoshun and Guo, Zhijiang and Sekulic, Ivan and Lu, Wei},
  year = {2020},
  pages = {1546--1557},
  doi = {10.18653/v1/2020.acl-main.141},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\Reasoning with Latent Structure Refinement for Document-Level Relation _Nan et al 22.pdf}
}

@inproceedings{nan2021uncovering,
  title = {Uncovering {{Main Causalities}} for {{Long-tailed Information Extraction}}},
  booktitle = {Proceedings of the 2021 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}}},
  author = {Nan, Guoshun and Zeng, Jiaqi and Qiao, Rui and Guo, Zhijiang and Lu, Wei},
  year = {2021},
  pages = {9683--9695},
  publisher = {{Association for Computational Linguistics}},
  address = {{Online and Punta Cana, Dominican Republic}},
  doi = {10.18653/v1/2021.emnlp-main.763},
  urldate = {2023-03-21},
  langid = {english},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\Uncovering Main Causalities for Long-tailed Information Extraction _Nan et al 2021.pdf}
}

@inproceedings{narasimhan2016improving,
  title = {Improving {{Information Extraction}} by {{Acquiring External Evidence}} with {{Reinforcement Learning}}},
  booktitle = {Proceedings of the 2016 {{Conference}} on {{Empirical Methods}} in {{Natural}}           {{Language Processing}}},
  author = {Narasimhan, Karthik and Yala, Adam and Barzilay, Regina},
  year = {2016},
  pages = {2355--2365},
  publisher = {{Association for Computational Linguistics}},
  address = {{Austin, Texas}},
  doi = {10.18653/v1/D16-1261},
  urldate = {2023-02-20},
  langid = {english},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\Improving Information Extraction by Acquiring External Evidence with _Narasimhan et al 2016.pdf}
}

@inproceedings{narayan2018don,
  title = {Don't {{Give Me}} the {{Details}}, {{Just}} the {{Summary}}! {{Topic-Aware Convolutional Neural Networks}} for {{Extreme Summarization}}},
  booktitle = {Proceedings of the 2018 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}}},
  author = {Narayan, Shashi and Cohen, Shay B. and Lapata, Mirella},
  year = {2018},
  pages = {1797--1807},
  publisher = {{Association for Computational Linguistics}},
  address = {{Brussels, Belgium}},
  doi = {10.18653/v1/D18-1206},
  urldate = {2023-04-02},
  langid = {english},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\Don’t Give Me the Details, Just the Summary _Narayan et al 2018.pdf}
}

@inproceedings{nayak2020effective,
  title = {Effective Modeling of Encoder-Decoder Architecture for Joint Entity and Relation Extraction},
  booktitle = {Proceedings of the {{AAAI Conference}} on {{Artificial Intelligence}}},
  author = {Nayak, Tapas and Ng, Hwee Tou},
  year = {2020},
  volume = {34},
  pages = {8528--8535},
  keywords = {⛔ No DOI found}
}

@article{neuberg2003causality,
  title = {{{CAUSALITY}}: {{MODELS}}, {{REASONING}}, {{AND INFERENCE}}, by {{Judea Pearl}}, {{Cambridge University Press}}, 2000},
  author = {Neuberg, Leland Gerson},
  year = {2003},
  month = aug,
  journal = {Econometric Theory},
  volume = {19},
  number = {04},
  issn = {0266-4666},
  doi = {10.1017/S0266466603004109},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\CAUSALITY _Neuberg 22.pdf}
}

@inproceedings{NEURIPS2018_1819932f,
  title = {Amortized Inference Regularization},
  booktitle = {Advances in Neural Information Processing Systems},
  author = {Shu, Rui and Bui, Hung H and Zhao, Shengjia and Kochenderfer, Mykel J and Ermon, Stefano},
  editor = {Bengio, S. and Wallach, H. and Larochelle, H. and Grauman, K. and {Cesa-Bianchi}, N. and Garnett, R.},
  year = {2018},
  volume = {31},
  publisher = {{Curran Associates, Inc.}},
  keywords = {⛔ No DOI found},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\Amortized inference regularization _Shu et al 2018.pdf}
}

@inproceedings{NEURIPS2018_b691334c,
  title = {Latent Alignment and Variational Attention},
  booktitle = {Advances in Neural Information Processing Systems},
  author = {Deng, Yuntian and Kim, Yoon and Chiu, Justin and Guo, Demi and Rush, Alexander},
  editor = {Bengio, S. and Wallach, H. and Larochelle, H. and Grauman, K. and {Cesa-Bianchi}, N. and Garnett, R.},
  year = {2018},
  volume = {31},
  publisher = {{Curran Associates, Inc.}},
  keywords = {⛔ No DOI found},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\Latent alignment and variational attention _Deng et al 2018.pdf}
}

@inproceedings{NEURIPS2020_e6385d39,
  title = {Estimating Training Data Influence by Tracing Gradient Descent},
  booktitle = {Advances in Neural Information Processing Systems},
  author = {Pruthi, Garima and Liu, Frederick and Kale, Satyen and Sundararajan, Mukund},
  editor = {Larochelle, H. and Ranzato, M. and Hadsell, R. and Balcan, M.F. and Lin, H.},
  year = {2020},
  volume = {33},
  pages = {19920--19930},
  publisher = {{Curran Associates, Inc.}},
  keywords = {⛔ No DOI found},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\Estimating training data influence by tracing gradient descent _Pruthi et al 2020.pdf}
}

@inproceedings{NEURIPS2022_d0702278,
  title = {First Is Better than Last for Language Data Influence},
  booktitle = {Advances in Neural Information Processing Systems},
  author = {Yeh, Chih-Kuan and Taly, Ankur and Sundararajan, Mukund and Liu, Frederick and Ravikumar, Pradeep},
  editor = {Koyejo, S. and Mohamed, S. and Agarwal, A. and Belgrave, D. and Cho, K. and Oh, A.},
  year = {2022},
  volume = {35},
  pages = {32285--32298},
  publisher = {{Curran Associates, Inc.}},
  keywords = {⛔ No DOI found},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\First is better than last for language data influence _Yeh et al 2022.pdf}
}

@article{nguyen2016multifaceted,
  title = {Multifaceted {{Feature Visualization}}: {{Uncovering}} the {{Different Types}} of {{Features Learned By Each Neuron}} in {{Deep Neural Networks}}},
  shorttitle = {Multifaceted {{Feature Visualization}}},
  author = {Nguyen, Anh and Yosinski, Jason and Clune, Jeff},
  year = {2016},
  month = may,
  journal = {arXiv:1602.03616 [cs]},
  eprint = {1602.03616},
  primaryclass = {cs},
  urldate = {2022-03-20},
  abstract = {We can better understand deep neural networks by identifying which features each of their neurons have learned to detect. To do so, researchers have created Deep Visualization techniques including activation maximization, which synthetically generates inputs (e.g. images) that maximally activate each neuron. A limitation of current techniques is that they assume each neuron detects only one type of feature, but we know that neurons can be multifaceted, in that they fire in response to many different types of features: for example, a grocery store class neuron must activate either for rows of produce or for a storefront. Previous activation maximization techniques constructed images without regard for the multiple different facets of a neuron, creating inappropriate mixes of colors, parts of objects, scales, orientations, etc. Here, we introduce an algorithm that explicitly uncovers the multiple facets of each neuron by producing a synthetic visualization of each of the types of images that activate a neuron. We also introduce regularization methods that produce state-of-the-art results in terms of the interpretability of images obtained by activation maximization. By separately synthesizing each type of image a neuron fires in response to, the visualizations have more appropriate colors and coherent global structure. Multifaceted feature visualization thus provides a clearer and more comprehensive description of the role of each neuron.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {⛔ No DOI found,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Neural and Evolutionary Computing},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\Multifaceted Feature Visualization _Nguyen et al 22.pdf}
}

@inproceedings{nguyen2016synthesizing,
  title = {Synthesizing the Preferred Inputs for Neurons in Neural Networks via Deep Generator Networks},
  booktitle = {Proceedings of the 30th International Conference on Neural Information Processing Systems},
  author = {Nguyen, Anh and Dosovitskiy, Alexey and Yosinski, Jason and Brox, Thomas and Clune, Jeff},
  year = {2016},
  series = {{{NIPS}}'16},
  pages = {3395--3403},
  publisher = {{Curran Associates Inc.}},
  address = {{Red Hook, NY, USA}},
  abstract = {Deep neural networks (DNNs) have demonstrated state-of-the-art results on many pattern recognition tasks, especially vision classification problems. Understanding the inner workings of such computational brains is both fascinating basic science that is interesting in its own right\textemdash similar to why we study the human brain\textemdash and will enable researchers to further improve DNNs. One path to understanding how a neural network functions internally is to study what each of its neurons has learned to detect. One such method is called activation maximization (AM), which synthesizes an input (e.g. an image) that highly activates a neuron. Here we dramatically improve the qualitative state of the art of activation maximization by harnessing a powerful, learned prior: a deep generator network (DGN). The algorithm (1) generates qualitatively state-of-the-art synthetic images that look almost real, (2) reveals the features learned by each neuron in an interpretable way, (3) generalizes well to new datasets and somewhat well to different network architectures without requiring the prior to be relearned, and (4) can be considered as a high-quality generative method (in this case, by generating novel, creative, interesting, recognizable images).},
  isbn = {978-1-5108-3881-9},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\Synthesizing the preferred inputs for neurons in neural networks via deep _Nguyen et al 22.pdf}
}

@inproceedings{nguyen2018convolutional,
  title = {Convolutional Neural Networks for Chemical-Disease Relation Extraction Are Improved with Character-Based Word Embeddings},
  booktitle = {Proceedings of the {{BioNLP}} 2018 Workshop},
  author = {Nguyen, Dat Quoc and Verspoor, Karin},
  year = {2018},
  pages = {129--136},
  doi = {10.18653/v1/W18-2314},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\Convolutional neural networks for chemical-disease relation extraction are _Nguyen_Verspoor 22.pdf}
}

@inproceedings{ni2019justifying,
  title = {Justifying {{Recommendations}} Using {{Distantly-Labeled Reviews}} and {{Fine-Grained Aspects}}},
  booktitle = {Proceedings of the 2019 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}} and the 9th {{International Joint Conference}} on {{Natural Language Processing}} ({{EMNLP-IJCNLP}})},
  author = {Ni, Jianmo and Li, Jiacheng and McAuley, Julian},
  year = {2019},
  pages = {188--197},
  publisher = {{Association for Computational Linguistics}},
  address = {{Hong Kong, China}},
  doi = {10.18653/v1/D19-1018},
  urldate = {2022-11-09},
  langid = {english},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\Justifying Recommendations using Distantly-Labeled Reviews and Fine-Grained _Ni et al 2019.pdf}
}

@inproceedings{NIPS1999_464d828b,
  title = {Policy Gradient Methods for Reinforcement Learning with Function Approximation},
  booktitle = {Advances in Neural Information Processing Systems},
  author = {Sutton, Richard S and McAllester, David and Singh, Satinder and Mansour, Yishay},
  editor = {Solla, S. and Leen, T. and M{\"u}ller, K.},
  year = {1999},
  volume = {12},
  publisher = {{MIT Press}},
  keywords = {⛔ No DOI found},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\Policy gradient methods for reinforcement learning with function approximation _Sutton et al 1999.pdf}
}

@inproceedings{NIPS2013_9aa42b31,
  title = {Distributed Representations of Words and Phrases and Their Compositionality},
  booktitle = {Advances in Neural Information Processing Systems},
  author = {Mikolov, Tomas and Sutskever, Ilya and Chen, Kai and Corrado, Greg S and Dean, Jeff},
  editor = {Burges, C.J. and Bottou, L. and Welling, M. and Ghahramani, Z. and Weinberger, K.Q.},
  year = {2013},
  volume = {26},
  publisher = {{Curran Associates, Inc.}},
  keywords = {⛔ No DOI found},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\Distributed representations of words and phrases and their compositionality _Mikolov et al 22.pdf}
}

@inproceedings{NIPS2016_a486cd07,
  title = {Man Is to Computer Programmer as Woman Is to Homemaker? {{Debiasing}} Word Embeddings},
  booktitle = {Advances in Neural Information Processing Systems},
  author = {Bolukbasi, Tolga and Chang, Kai-Wei and Zou, James Y and Saligrama, Venkatesh and Kalai, Adam T},
  editor = {Lee, D. and Sugiyama, M. and Luxburg, U. and Guyon, I. and Garnett, R.},
  year = {2016},
  volume = {29},
  publisher = {{Curran Associates, Inc.}},
  keywords = {⛔ No DOI found},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\Man is to computer programmer as woman is to homemaker _Bolukbasi et al 22.pdf}
}

@inproceedings{niven2019probing,
  title = {Probing {{Neural Network Comprehension}} of {{Natural Language Arguments}}},
  booktitle = {Proceedings of the 57th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}}},
  author = {Niven, Timothy and Kao, Hung-Yu},
  year = {2019},
  pages = {4658--4664},
  publisher = {{Association for Computational Linguistics}},
  address = {{Florence, Italy}},
  doi = {10.18653/v1/P19-1459},
  urldate = {2022-07-10},
  langid = {english},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\Probing Neural Network Comprehension of Natural Language Arguments _Niven_Kao 22.pdf}
}

@misc{ouyang2022training,
  title = {Training Language Models to Follow Instructions with Human Feedback},
  author = {Ouyang, Long and Wu, Jeff and Jiang, Xu and Almeida, Diogo and Wainwright, Carroll L. and Mishkin, Pamela and Zhang, Chong and Agarwal, Sandhini and Slama, Katarina and Ray, Alex and Schulman, John and Hilton, Jacob and Kelton, Fraser and Miller, Luke and Simens, Maddie and Askell, Amanda and Welinder, Peter and Christiano, Paul and Leike, Jan and Lowe, Ryan},
  year = {2022},
  month = mar,
  number = {arXiv:2203.02155},
  eprint = {arXiv:2203.02155},
  publisher = {{arXiv}},
  urldate = {2023-02-04},
  abstract = {Making language models bigger does not inherently make them better at following a user's intent. For example, large language models can generate outputs that are untruthful, toxic, or simply not helpful to the user. In other words, these models are not aligned with their users. In this paper, we show an avenue for aligning language models with user intent on a wide range of tasks by fine-tuning with human feedback. Starting with a set of labeler-written prompts and prompts submitted through the OpenAI API, we collect a dataset of labeler demonstrations of the desired model behavior, which we use to fine-tune GPT-3 using supervised learning. We then collect a dataset of rankings of model outputs, which we use to further fine-tune this supervised model using reinforcement learning from human feedback. We call the resulting models InstructGPT. In human evaluations on our prompt distribution, outputs from the 1.3B parameter InstructGPT model are preferred to outputs from the 175B GPT-3, despite having 100x fewer parameters. Moreover, InstructGPT models show improvements in truthfulness and reductions in toxic output generation while having minimal performance regressions on public NLP datasets. Even though InstructGPT still makes simple mistakes, our results show that fine-tuning with human feedback is a promising direction for aligning language models with human intent.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {C\:\\Users\\Hytn\\Zotero\\storage\\TAXCNB7R\\Ouyang et al. - 2022 - Training language models to follow instructions wi.pdf}
}

@article{over2004introduction,
  title = {An Introduction to {{DUC-2004}}},
  author = {Over, Paul and Yen, James},
  year = {2004},
  journal = {National Institute of Standards and Technology},
  keywords = {⛔ No DOI found}
}

@inproceedings{owczarzak2011overview,
  title = {Overview of the {{TAC}} 2011 Summarization Track: {{Guided}} Task and {{AESOP}} Task},
  booktitle = {Proceedings of the Text Analysis Conference ({{TAC}} 2011), Gaithersburg, Maryland, {{USA}}, November},
  author = {Owczarzak, Karolina and Dang, Hoa Trang},
  year = {2011},
  keywords = {⛔ No DOI found}
}

@article{paolini2021structured,
  title = {Structured {{Prediction}} as {{Translation}} between {{Augmented Natural Languages}}},
  author = {Paolini, Giovanni and Athiwaratkun, Ben and Krone, Jason and Ma, Jie and Achille, Alessandro and Anubhai, Rishita and dos Santos, Cicero Nogueira and Xiang, Bing and Soatto, Stefano},
  year = {2021},
  publisher = {{arXiv}},
  doi = {10.48550/ARXIV.2101.05779},
  urldate = {2022-06-20},
  abstract = {We propose a new framework, Translation between Augmented Natural Languages (TANL), to solve many structured prediction language tasks including joint entity and relation extraction, nested named entity recognition, relation classification, semantic role labeling, event extraction, coreference resolution, and dialogue state tracking. Instead of tackling the problem by training task-specific discriminative classifiers, we frame it as a translation task between augmented natural languages, from which the task-relevant information can be easily extracted. Our approach can match or outperform task-specific models on all tasks, and in particular, achieves new state-of-the-art results on joint entity and relation extraction (CoNLL04, ADE, NYT, and ACE2005 datasets), relation classification (FewRel and TACRED), and semantic role labeling (CoNLL-2005 and CoNLL-2012). We accomplish this while using the same architecture and hyperparameters for all tasks and even when training a single model to solve all tasks at the same time (multi-task learning). Finally, we show that our framework can also significantly improve the performance in a low-resource regime, thanks to better use of label semantics.},
  copyright = {arXiv.org perpetual, non-exclusive license},
  keywords = {Computation and Language (cs.CL),FOS: Computer and information sciences,Machine Learning (cs.LG)},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\Structured Prediction as Translation between Augmented Natural Languages _Paolini et al 22.pdf}
}

@article{parascandololearning,
  title = {Learning {{Independent Causal Mechanisms}}},
  author = {Parascandolo, Giambattista and Kilbertus, Niki and {Rojas-Carulla}, Mateo and Scholkopf, Bernhard},
  pages = {9},
  langid = {english},
  keywords = {⛔ No DOI found},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\Learning Independent Causal Mechanisms _Parascandolo et al2.pdf}
}

@inproceedings{park2018reducing,
  title = {Reducing {{Gender Bias}} in {{Abusive Language Detection}}},
  booktitle = {Proceedings of the 2018 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}}},
  author = {Park, Ji Ho and Shin, Jamin and Fung, Pascale},
  year = {2018},
  pages = {2799--2804},
  publisher = {{Association for Computational Linguistics}},
  address = {{Brussels, Belgium}},
  doi = {10.18653/v1/D18-1302},
  urldate = {2022-07-16},
  langid = {english},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\Reducing Gender Bias in Abusive Language Detection _Park et al 22.pdf}
}

@inproceedings{pasunuru2021efficiently,
  title = {Efficiently {{Summarizing Text}} and {{Graph Encodings}} of {{Multi-Document Clusters}}},
  booktitle = {Proceedings of the 2021 {{Conference}} of the {{North American Chapter}} of the {{Association}} for {{Computational Linguistics}}: {{Human Language Technologies}}},
  author = {Pasunuru, Ramakanth and Liu, Mengwen and Bansal, Mohit and Ravi, Sujith and Dreyer, Markus},
  year = {2021},
  pages = {4768--4779},
  publisher = {{Association for Computational Linguistics}},
  address = {{Online}},
  doi = {10.18653/v1/2021.naacl-main.380},
  urldate = {2022-06-04},
  langid = {english},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\Efficiently Summarizing Text and Graph Encodings of Multi-Document Clusters _Pasunuru et al 4.pdf}
}

@inproceedings{pasunuru2021efficientlya,
  title = {Efficiently {{Summarizing Text}} and {{Graph Encodings}} of {{Multi-Document Clusters}}},
  booktitle = {Proceedings of the 2021 {{Conference}} of the {{North American Chapter}} of the {{Association}} for {{Computational Linguistics}}: {{Human Language Technologies}}},
  author = {Pasunuru, Ramakanth and Liu, Mengwen and Bansal, Mohit and Ravi, Sujith and Dreyer, Markus},
  year = {2021},
  pages = {4768--4779},
  publisher = {{Association for Computational Linguistics}},
  address = {{Online}},
  doi = {10.18653/v1/2021.naacl-main.380},
  urldate = {2022-08-08},
  langid = {english},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\Efficiently Summarizing Text and Graph Encodings of Multi-Document Clusters _Pasunuru et al 3.pdf}
}

@article{paszke2017automatic,
  title = {Automatic Differentiation in Pytorch},
  author = {Paszke, Adam and Gross, Sam and Chintala, Soumith and Chanan, Gregory and Yang, Edward and DeVito, Zachary and Lin, Zeming and Desmaison, Alban and Antiga, Luca and Lerer, Adam},
  year = {2017},
  keywords = {⛔ No DOI found}
}

@incollection{paszke2019pytorch,
  title = {{{PyTorch}}: {{An Imperative Style}}, {{High-Performance Deep Learning Library}}},
  booktitle = {Advances in {{Neural Information Processing Systems}} 32},
  author = {Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and Desmaison, Alban and Kopf, Andreas and Yang, Edward and DeVito, Zachary and Raison, Martin and Tejani, Alykhan and Chilamkurthy, Sasank and Steiner, Benoit and Fang, Lu and Bai, Junjie and Chintala, Soumith},
  year = {2019},
  pages = {8024--8035},
  publisher = {{Curran Associates, Inc.}}
}

@inproceedings{paulus2018a,
  title = {A Deep Reinforced Model for Abstractive Summarization},
  booktitle = {International Conference on Learning Representations},
  author = {Paulus, Romain and Xiong, Caiming and Socher, Richard},
  year = {2018},
  keywords = {⛔ No DOI found},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\A deep reinforced model for abstractive summarization _Paulus et al 2018.pdf}
}

@article{pearl2009causal,
  title = {Causal Inference in Statistics: {{An}} Overview},
  shorttitle = {Causal Inference in Statistics},
  author = {Pearl, Judea},
  year = {2009},
  month = jan,
  journal = {Statistics Surveys},
  volume = {3},
  number = {none},
  issn = {1935-7516},
  doi = {10.1214/09-SS057},
  urldate = {2022-05-14},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\Causal inference in statistics _Pearl 22.pdf}
}

@book{pearl2009causality,
  title = {Causality},
  author = {Pearl, Judea},
  year = {2009},
  publisher = {{Cambridge university press}}
}

@article{pearl2014external,
  title = {External {{Validity}}: {{From Do-Calculus}} to {{Transportability Across Populations}}},
  author = {Pearl, Judea and Bareinboim, Elias},
  year = {2014},
  month = nov,
  journal = {Statistical Science},
  volume = {29},
  number = {4},
  issn = {0883-4237},
  doi = {10.1214/14-STS486},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\External Validity _Pearl_Bareinboim 22.pdf}
}

@inproceedings{pei2017deepxplore,
  title = {{{DeepXplore}}: {{Automated Whitebox Testing}} of {{Deep Learning Systems}}},
  shorttitle = {{{DeepXplore}}},
  booktitle = {Proceedings of the 26th {{Symposium}} on {{Operating Systems Principles}}},
  author = {Pei, Kexin and Cao, Yinzhi and Yang, Junfeng and Jana, Suman},
  year = {2017},
  month = oct,
  pages = {1--18},
  publisher = {{ACM}},
  address = {{Shanghai China}},
  doi = {10.1145/3132747.3132785},
  urldate = {2022-11-23},
  isbn = {978-1-4503-5085-3},
  langid = {english},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\DeepXplore _Pei et al 2017.pdf}
}

@article{peng2017crosssentence,
  title = {Cross-Sentence n-Ary Relation Extraction with Graph Lstms},
  author = {Peng, Nanyun and Poon, Hoifung and Quirk, Chris and Toutanova, Kristina and Yih, Wen-tau},
  year = {2017},
  journal = {Transactions of the Association for Computational Linguistics},
  volume = {5},
  pages = {101--115},
  publisher = {{MIT Press}},
  doi = {10.1162/tacl-a-00049},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\Cross-sentence n-ary relation extraction with graph lstms _Peng et al 22.pdf}
}

@article{peng2017crosssentencea,
  title = {Cross-{{Sentence}} {{{\emph{N}}}} -Ary {{Relation Extraction}} with {{Graph LSTMs}}},
  author = {Peng, Nanyun and Poon, Hoifung and Quirk, Chris and Toutanova, Kristina and Yih, Wen-tau},
  year = {2017},
  month = dec,
  journal = {Transactions of the Association for Computational Linguistics},
  volume = {5},
  pages = {101--115},
  issn = {2307-387X},
  doi = {10.1162/tacl-a-00049},
  urldate = {2022-07-15},
  abstract = {Past work in relation extraction has focused on binary relations in single sentences. Recent NLP inroads in high-value domains have sparked interest in the more general setting of extracting n-ary relations that span multiple sentences. In this paper, we explore a general relation extraction framework based on graph long short-term memory networks (graph LSTMs) that can be easily extended to cross-sentence n-ary relation extraction. The graph formulation provides a unified way of exploring different LSTM approaches and incorporating various intra-sentential and intersentential dependencies, such as sequential, syntactic, and discourse relations. A robust contextual representation is learned for the entities, which serves as input to the relation classifier. This simplifies handling of relations with arbitrary arity, and enables multi-task learning with related relations. We evaluate this framework in two important precision medicine settings, demonstrating its effectiveness with both conventional supervised learning and distant supervision. Cross-sentence extraction produced larger knowledge bases. and multi-task learning significantly improved extraction accuracy. A thorough analysis of various LSTM approaches yielded useful insight the impact of linguistic analysis on extraction accuracy.},
  langid = {english},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\Cross-Sentence iN-i -ary Relation Extraction with Graph LSTMs _Peng et al 22.pdf}
}

@inproceedings{peng2020learning,
  title = {Learning from {{Context}} or {{Names}}? {{An Empirical Study}} on {{Neural Relation Extraction}}},
  shorttitle = {Learning from {{Context}} or {{Names}}?},
  booktitle = {Proceedings of the 2020 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}} ({{EMNLP}})},
  author = {Peng, Hao and Gao, Tianyu and Han, Xu and Lin, Yankai and Li, Peng and Liu, Zhiyuan and Sun, Maosong and Zhou, Jie},
  year = {2020},
  pages = {3661--3672},
  publisher = {{Association for Computational Linguistics}},
  address = {{Online}},
  doi = {10.18653/v1/2020.emnlp-main.298},
  urldate = {2022-09-27},
  langid = {english},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\Learning from Context or Names _Peng et al 2020.pdf}
}

@inproceedings{peng2020learninga,
  title = {Learning from {{Context}} or {{Names}}? {{An Empirical Study}} on {{Neural Relation Extraction}}},
  shorttitle = {Learning from {{Context}} or {{Names}}?},
  booktitle = {Proceedings of the 2020 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}} ({{EMNLP}})},
  author = {Peng, Hao and Gao, Tianyu and Han, Xu and Lin, Yankai and Li, Peng and Liu, Zhiyuan and Sun, Maosong and Zhou, Jie},
  year = {2020},
  pages = {3661--3672},
  publisher = {{Association for Computational Linguistics}},
  address = {{Online}},
  doi = {10.18653/v1/2020.emnlp-main.298},
  urldate = {2022-09-27},
  langid = {english},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\Learning from Context or Names _Peng et al 22.pdf}
}

@inproceedings{peng2020learningb,
  title = {Learning from {{Context}} or {{Names}}? {{An Empirical Study}} on {{Neural Relation Extraction}}},
  shorttitle = {Learning from {{Context}} or {{Names}}?},
  booktitle = {Proceedings of the 2020 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}} ({{EMNLP}})},
  author = {Peng, Hao and Gao, Tianyu and Han, Xu and Lin, Yankai and Li, Peng and Liu, Zhiyuan and Sun, Maosong and Zhou, Jie},
  year = {2020},
  pages = {3661--3672},
  publisher = {{Association for Computational Linguistics}},
  address = {{Online}},
  doi = {10.18653/v1/2020.emnlp-main.298},
  urldate = {2022-09-27},
  langid = {english},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\Learning from Context or Names _Peng et al 3.pdf}
}

@inproceedings{pennington2014glove,
  title = {Glove: {{Global}} Vectors for Word Representation},
  booktitle = {Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing ({{EMNLP}})},
  author = {Pennington, Jeffrey and Socher, Richard and Manning, Christopher D},
  year = {2014},
  pages = {1532--1543},
  doi = {10.3115/v1/D14-1162},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\Glove _Pennington et al 22.pdf}
}

@inproceedings{perez2019finding,
  title = {Finding {{Generalizable Evidence}} by {{Learning}} to {{Convince Q}}\&{{A Models}}},
  booktitle = {Proceedings of the 2019 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}} and the 9th {{International Joint Conference}} on {{Natural Language Processing}} ({{EMNLP-IJCNLP}})},
  author = {Perez, Ethan and Karamcheti, Siddharth and Fergus, Rob and Weston, Jason and Kiela, Douwe and Cho, Kyunghyun},
  year = {2019},
  pages = {2402--2411},
  publisher = {{Association for Computational Linguistics}},
  address = {{Hong Kong, China}},
  doi = {10.18653/v1/D19-1244},
  urldate = {2023-02-04},
  langid = {english},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\Finding Generalizable Evidence by Learning to Convince Q&A Models _Perez et al 2019.pdf}
}

@article{peters2016causal,
  title = {Causal Inference by Using Invariant Prediction: Identification and Confidence Intervals},
  shorttitle = {Causal Inference by Using Invariant Prediction},
  author = {Peters, Jonas and B{\"u}hlmann, Peter and Meinshausen, Nicolai},
  year = {2016},
  month = nov,
  journal = {Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
  volume = {78},
  number = {5},
  pages = {947--1012},
  issn = {13697412},
  doi = {10.1111/rssb.12167},
  urldate = {2022-09-19},
  langid = {english},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\Causal inference by using invariant prediction _Peters et al 2016.pdf}
}

@inproceedings{pmlr-v119-bras20a,
  title = {Adversarial Filters of Dataset Biases},
  booktitle = {Proceedings of the 37th International Conference on Machine Learning},
  author = {Bras, Ronan Le and Swayamdipta, Swabha and Bhagavatula, Chandra and Zellers, Rowan and Peters, Matthew and Sabharwal, Ashish and Choi, Yejin},
  editor = {III, Hal Daum{\'e} and Singh, Aarti},
  year = {2020},
  month = jul,
  series = {Proceedings of Machine Learning Research},
  volume = {119},
  pages = {1078--1088},
  publisher = {{PMLR}},
  abstract = {Large neural models have demonstrated human-level performance on language and vision benchmarks, while their performance degrades considerably on adversarial or out-of-distribution samples. This raises the question of whether these models have learned to solve a dataset rather than the underlying task by overfitting to spurious dataset biases. We investigate one recently proposed approach, AFLITE, which adversarially filters such dataset biases, as a means to mitigate the prevalent overestimation of machine performance. We provide a theoretical understanding for AFLITE, by situating it in the generalized framework for optimum bias reduction. We present extensive supporting evidence that AFLITE is broadly applicable for reduction of measurable dataset biases, and that models trained on the filtered datasets yield better generalization to out-of-distribution tasks. Finally, filtering results in a large drop in model performance (e.g., from 92\% to 62\% for SNLI), while human performance still remains high. Our work thus shows that such filtered datasets can pose new research challenges for robust generalization by serving as upgraded benchmarks.},
  pdf = {http://proceedings.mlr.press/v119/bras20a/bras20a.pdf},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\Adversarial filters of dataset biases _Bras et al 22.pdf}
}

@inproceedings{pmlr-v119-chang20c,
  title = {Invariant Rationalization},
  booktitle = {Proceedings of the 37th International Conference on Machine Learning},
  author = {Chang, Shiyu and Zhang, Yang and Yu, Mo and Jaakkola, Tommi},
  editor = {III, Hal Daum{\'e} and Singh, Aarti},
  year = {2020},
  month = jul,
  series = {Proceedings of Machine Learning Research},
  volume = {119},
  pages = {1448--1458},
  publisher = {{PMLR}},
  abstract = {Selective rationalization improves neural network interpretability by identifying a small subset of input features \textemdash{} the rationale \textemdash{} that best explains or supports the prediction. A typical rationalization criterion, i.e. maximum mutual information (MMI), finds the rationale that maximizes the prediction performance based only on the rationale. However, MMI can be problematic because it picks up spurious correlations between the input features and the output. Instead, we introduce a game-theoretic invariant rationalization criterion where the rationales are constrained to enable the same predictor to be optimal across different environments. We show both theoretically and empirically that the proposed rationales can rule out spurious correlations and generalize better to different test scenarios. The resulting explanations also align better with human judgments. Our implementations are publicly available at https://github.com/code-terminator/invariant\textsubscript{r}ationalization.},
  pdf = {http://proceedings.mlr.press/v119/chang20c/chang20c.pdf},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\Invariant rationalization _Chang et al 22.pdf}
}

@inproceedings{pmlr-v119-sagawa20a,
  title = {An Investigation of Why Overparameterization Exacerbates Spurious Correlations},
  booktitle = {Proceedings of the 37th International Conference on Machine Learning},
  author = {Sagawa, Shiori and Raghunathan, Aditi and Koh, Pang Wei and Liang, Percy},
  editor = {III, Hal Daum{\'e} and Singh, Aarti},
  year = {2020},
  month = jul,
  series = {Proceedings of Machine Learning Research},
  volume = {119},
  pages = {8346--8356},
  publisher = {{PMLR}},
  abstract = {We study why overparameterization\textemdash increasing model size well beyond the point of zero training error\textemdash can hurt test error on minority groups despite improving average test error when there are spurious correlations in the data. Through simulations and experiments on two image datasets, we identify two key properties of the training data that drive this behavior: the proportions of majority versus minority groups, and the signal-to-noise ratio of the spurious correlations. We then analyze a linear setting and theoretically show how the inductive bias of models towards ``memorizing'' fewer examples can cause overparameterization to hurt. Our analysis leads to a counterintuitive approach of subsampling the majority group, which empirically achieves low minority error in the overparameterized regime, even though the standard approach of upweighting the minority fails. Overall, our results suggest a tension between using overparameterized models versus using all the training data for achieving low worst-group error.},
  pdf = {http://proceedings.mlr.press/v119/sagawa20a/sagawa20a.pdf},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\An investigation of why overparameterization exacerbates spurious correlations _Sagawa et al 22.pdf}
}

@inproceedings{pmlr-v119-srivastava20a,
  title = {Robustness to Spurious Correlations via Human Annotations},
  booktitle = {Proceedings of the 37th International Conference on Machine Learning},
  author = {Srivastava, Megha and Hashimoto, Tatsunori and Liang, Percy},
  editor = {III, Hal Daum{\'e} and Singh, Aarti},
  year = {2020},
  month = jul,
  series = {Proceedings of Machine Learning Research},
  volume = {119},
  pages = {9109--9119},
  publisher = {{PMLR}},
  abstract = {The reliability of machine learning systems critically assumes that the associations between features and labels remain similar between training and test distributions. However, unmeasured variables, such as confounders, break this assumption\textemdash useful correlations between features and labels at training time can become useless or even harmful at test time. For example, high obesity is generally predictive for heart disease, but this relation may not hold for smokers who generally have lower rates of obesity and higher rates of heart disease. We present a framework for making models robust to spurious correlations by leveraging humans' common sense knowledge of causality. Specifically, we use human annotation to augment each training example with a potential unmeasured variable (i.e. an underweight patient with heart disease may be a smoker), reducing the problem to a covariate shift problem. We then introduce a new distributionally robust optimization objective over unmeasured variables (UV-DRO) to control the worst-case loss over possible test- time shifts. Empirically, we show improvements of 5\textendash 10\% on a digit recognition task confounded by rotation, and 1.5\textendash 5\% on the task of analyzing NYPD Police Stops confounded by location.},
  pdf = {http://proceedings.mlr.press/v119/srivastava20a/srivastava20a.pdf},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\Robustness to spurious correlations via human annotations _Srivastava et al 22.pdf}
}

@inproceedings{pmlr-v130-kamath21a,
  title = {Does Invariant Risk Minimization Capture Invariance?},
  booktitle = {Proceedings of the 24th International Conference on Artificial Intelligence and Statistics},
  author = {Kamath, Pritish and Tangella, Akilesh and Sutherland, Danica and Srebro, Nathan},
  editor = {Banerjee, Arindam and Fukumizu, Kenji},
  year = {2021},
  month = apr,
  series = {Proceedings of Machine Learning Research},
  volume = {130},
  pages = {4069--4077},
  publisher = {{PMLR}},
  abstract = {We show that the Invariant Risk Minimization (IRM) formulation of Arjovsky et al. (2019) can fail to capture "natural" invariances, at least when used in its practical "linear" form, and even on very simple problems which directly follow the motivating examples for IRM. This can lead to worse generalization on new environments, even when compared to unconstrained ERM. The issue stems from a significant gap between the linear variant (as in their concrete method IRMv1) and the full non-linear IRM formulation. Additionally, even when capturing the "right" invariances, we show that it is possible for IRM to learn a sub-optimal predictor, due to the loss function not being invariant across environments. The issues arise even when measuring invariance on the population distributions, but are exacerbated by the fact that IRM is extremely fragile to sampling.},
  pdf = {http://proceedings.mlr.press/v130/kamath21a/kamath21a.pdf},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\Does invariant risk minimization capture invariance _Kamath et al 22.pdf}
}

@inproceedings{pmlr-v139-krueger21a,
  title = {Out-of-Distribution Generalization via Risk Extrapolation ({{REx}})},
  booktitle = {Proceedings of the 38th International Conference on Machine Learning},
  author = {Krueger, David and Caballero, Ethan and Jacobsen, Joern-Henrik and Zhang, Amy and Binas, Jonathan and Zhang, Dinghuai and Priol, Remi Le and Courville, Aaron},
  editor = {Meila, Marina and Zhang, Tong},
  year = {2021},
  month = jul,
  series = {Proceedings of Machine Learning Research},
  volume = {139},
  pages = {5815--5826},
  publisher = {{PMLR}},
  abstract = {Distributional shift is one of the major obstacles when transferring machine learning prediction systems from the lab to the real world. To tackle this problem, we assume that variation across training domains is representative of the variation we might encounter at test time, but also that shifts at test time may be more extreme in magnitude. In particular, we show that reducing differences in risk across training domains can reduce a model's sensitivity to a wide range of extreme distributional shifts, including the challenging setting where the input contains both causal and anti-causal elements. We motivate this approach, Risk Extrapolation (REx), as a form of robust optimization over a perturbation set of extrapolated domains (MM-REx), and propose a penalty on the variance of training risks (V-REx) as a simpler variant. We prove that variants of REx can recover the causal mechanisms of the targets, while also providing robustness to changes in the input distribution (``covariate shift''). By appropriately trading-off robustness to causally induced distributional shifts and covariate shift, REx is able to outperform alternative methods such as Invariant Risk Minimization in situations where these types of shift co-occur.},
  pdf = {http://proceedings.mlr.press/v139/krueger21a/krueger21a.pdf},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\Out-of-distribution generalization via risk extrapolation (REx) _Krueger et al 22.pdf}
}

@inproceedings{pmlr-v15-larochelle11a,
  title = {The Neural Autoregressive Distribution Estimator},
  booktitle = {Proceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics},
  author = {Larochelle, Hugo and Murray, Iain},
  editor = {Gordon, Geoffrey and Dunson, David and Dud{\'i}k, Miroslav},
  year = {2011},
  month = apr,
  series = {Proceedings of Machine Learning Research},
  volume = {15},
  pages = {29--37},
  publisher = {{PMLR}},
  address = {{Fort Lauderdale, FL, USA}},
  abstract = {We describe a new approach for modeling the distribution of high-dimensional vectors of discrete variables. This model is inspired by the restricted Boltzmann machine (RBM), which has been shown to be a powerful model of such distributions. However, an RBM typically does not provide a tractable distribution estimator, since evaluating the probability it assigns to some given observation requires the computation of the so-called partition function, which itself is intractable for RBMs of even moderate size. Our model circumvents this difficulty by decomposing the joint distribution of observations into tractable conditional distributions and modeling each conditional using a non-linear function similar to a conditional of an RBM. Our model can also be interpreted as an autoencoder wired such that its output can be used to assign valid probabilities to observations. We show that this new model outperforms other multivariate binary distribution estimators on several datasets and performs similarly to a large (but intractable) RBM.},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\The neural autoregressive distribution estimator _Larochelle_Murray 2011.pdf}
}

@inproceedings{pmlr-v162-miao22a,
  title = {Interpretable and Generalizable Graph Learning via Stochastic Attention Mechanism},
  booktitle = {Proceedings of the 39th International Conference on Machine Learning},
  author = {Miao, Siqi and Liu, Mia and Li, Pan},
  editor = {Chaudhuri, Kamalika and Jegelka, Stefanie and Song, Le and Szepesvari, Csaba and Niu, Gang and Sabato, Sivan},
  year = {2022},
  month = jul,
  series = {Proceedings of Machine Learning Research},
  volume = {162},
  pages = {15524--15543},
  publisher = {{PMLR}},
  abstract = {Interpretable graph learning is in need as many scientific applications depend on learning models to collect insights from graph-structured data. Previous works mostly focused on using post-hoc approaches to interpret pre-trained models (graph neural networks in particular). They argue against inherently interpretable models because the good interpretability of these models is often at the cost of their prediction accuracy. However, those post-hoc methods often fail to provide stable interpretation and may extract features that are spuriously correlated with the task. In this work, we address these issues by proposing Graph Stochastic Attention (GSAT). Derived from the information bottleneck principle, GSAT injects stochasticity to the attention weights to block the information from task-irrelevant graph components while learning stochasticity-reduced attention to select task-relevant subgraphs for interpretation. The selected subgraphs provably do not contain patterns that are spuriously correlated with the task under some assumptions. Extensive experiments on eight datasets show that GSAT outperforms the state-of-the-art methods by up to 20\% in interpretation AUC and 5\% in prediction accuracy. Our code is available at https://github.com/Graph-COM/GSAT.},
  pdf = {https://proceedings.mlr.press/v162/miao22a/miao22a.pdf},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\Interpretable and generalizable graph learning via stochastic attention _Miao et al 22.pdf}
}

@inproceedings{pmlr-v162-zhou22d,
  title = {Model Agnostic Sample Reweighting for Out-of-Distribution Learning},
  booktitle = {Proceedings of the 39th International Conference on Machine Learning},
  author = {Zhou, Xiao and Lin, Yong and Pi, Renjie and Zhang, Weizhong and Xu, Renzhe and Cui, Peng and Zhang, Tong},
  editor = {Chaudhuri, Kamalika and Jegelka, Stefanie and Song, Le and Szepesvari, Csaba and Niu, Gang and Sabato, Sivan},
  year = {2022},
  month = jul,
  series = {Proceedings of Machine Learning Research},
  volume = {162},
  pages = {27203--27221},
  publisher = {{PMLR}},
  abstract = {Distributionally robust optimization (DRO) and invariant risk minimization (IRM) are two popular methods proposed to improve out-of-distribution (OOD) generalization performance of machine learning models. While effective for small models, it has been observed that these methods can be vulnerable to overfitting with large overparameterized models. This work proposes a principled method, Model Agnostic samPLe rEweighting (MAPLE), to effectively address OOD problem, especially in overparameterized scenarios. Our key idea is to find an effective reweighting of the training samples so that the standard empirical risk minimization training of a large model on the weighted training data leads to superior OOD generalization performance. The overfitting issue is addressed by considering a bilevel formulation to search for the sample reweighting, in which the generalization complexity depends on the search space of sample weights instead of the model size. We present theoretical analysis in linear case to prove the insensitivity of MAPLE to model size, and empirically verify its superiority in surpassing state-of-the-art methods by a large margin.},
  pdf = {https://proceedings.mlr.press/v162/zhou22d/zhou22d.pdf},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\Model agnostic sample reweighting for out-of-distribution learning _Zhou et al 22.pdf}
}

@inproceedings{pmlr-v80-chen18j,
  title = {Learning to Explain: {{An}} Information-Theoretic Perspective on Model Interpretation},
  booktitle = {Proceedings of the 35th International Conference on Machine Learning},
  author = {Chen, Jianbo and Song, Le and Wainwright, Martin and Jordan, Michael},
  editor = {Dy, Jennifer and Krause, Andreas},
  year = {2018},
  month = jul,
  series = {Proceedings of Machine Learning Research},
  volume = {80},
  pages = {883--892},
  publisher = {{PMLR}},
  abstract = {We introduce instancewise feature selection as a methodology for model interpretation. Our method is based on learning a function to extract a subset of features that are most informative for each given example. This feature selector is trained to maximize the mutual information between selected features and the response variable, where the conditional distribution of the response variable given the input is the model to be explained. We develop an efficient variational approximation to the mutual information, and show the effectiveness of our method on a variety of synthetic and real data sets using both quantitative metrics and human evaluation.},
  pdf = {http://proceedings.mlr.press/v80/chen18j/chen18j.pdf},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\Learning to explain _Chen et al 22.pdf}
}

@inproceedings{poliak2018hypothesis,
  title = {Hypothesis {{Only Baselines}} in {{Natural Language Inference}}},
  booktitle = {Proceedings of the {{Seventh Joint Conference}} on {{Lexical}} and           {{Computational Semantics}}},
  author = {Poliak, Adam and Naradowsky, Jason and Haldar, Aparajita and Rudinger, Rachel and Van Durme, Benjamin},
  year = {2018},
  pages = {180--191},
  publisher = {{Association for Computational Linguistics}},
  address = {{New Orleans, Louisiana}},
  doi = {10.18653/v1/S18-2023},
  urldate = {2022-07-11},
  langid = {english},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\Hypothesis Only Baselines in Natural Language Inference _Poliak et al 22.pdf}
}

@misc{qi2022class,
  title = {Class {{Is Invariant}} to {{Context}} and {{Vice Versa}}: {{On Learning Invariance}} for {{Out-Of-Distribution Generalization}}},
  shorttitle = {Class {{Is Invariant}} to {{Context}} and {{Vice Versa}}},
  author = {Qi, Jiaxin and Tang, Kaihua and Sun, Qianru and Hua, Xian-Sheng and Zhang, Hanwang},
  year = {2022},
  month = aug,
  number = {arXiv:2208.03462},
  eprint = {arXiv:2208.03462},
  publisher = {{arXiv}},
  urldate = {2022-09-03},
  abstract = {Out-Of-Distribution generalization (OOD) is all about learning invariance against environmental changes. If the context1 in every class is evenly distributed, OOD would be trivial because the context can be easily removed due to an underlying principle: class is invariant to context. However, collecting such a balanced dataset is impractical. Learning on imbalanced data makes the model bias to context and thus hurts OOD. Therefore, the key to OOD is context balance. We argue that the widely adopted assumption in prior work\textemdash the context bias can be directly annotated or estimated from biased class prediction\textemdash renders the context incomplete or even incorrect. In contrast, we point out the everoverlooked other side of the above principle: context is also invariant to class, which motivates us to consider the classes (which are already labeled) as the varying environments2 to resolve context bias (without context labels). We implement this idea by minimizing the contrastive loss of intra-class sample similarity while assuring this similarity to be invariant across all classes. On benchmarks with various context biases and domain gaps, we show that a simple re-weighting based classifier equipped with our context estimation achieves state-of-the-art performance. We provide the theoretical justifications in Appendix and codes on Github3.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\Class Is Invariant to Context and Vice Versa _Qi et al 2022.pdf}
}

@inproceedings{qin2022comparison,
  title = {A {{Comparison Study}} of {{Pre-trained Language Models}} for {{Chinese Legal Document Classification}}},
  booktitle = {2022 5th {{International Conference}} on {{Artificial Intelligence}} and {{Big Data}} ({{ICAIBD}})},
  author = {Qin, Ruyu and Huang, Min and Luo, Yutong},
  year = {2022},
  month = may,
  pages = {444--449},
  publisher = {{IEEE}},
  address = {{Chengdu, China}},
  doi = {10.1109/ICAIBD55127.2022.9820466},
  urldate = {2022-07-21},
  abstract = {Legal artificial intelligence (LegalAI), aiming to benefit the legal domain using artificial intelligence technologies, is the hot topic of the moment. As the basis for various LegalAI tasks such as judgment prediction and similar case matching, the classification of legal documents is an issue that has to be addressed. The majority of current approaches focus on the legal systems of native English-speaking countries. However, both Chinese language and legal system differ significantly from that of English. Given the success of pre-trained Language Models (PLMs) and outperformance compared with feature-engineering-based machine learning models as well as traditional deep neural network models such as CNNs and RNNs in NLP, their effectiveness in specific domains needs to be further investigated, especially in legal domain. Moreover, few studies have made comparisons of these PLMs for specific legal tasks. Therefore, in this paper we train several strong PLMs which differ in pretraining corpus on three datasets of Chinese legal documents. Experimental results show that the model pre-trained on the legal corpus demonstrates its high efficiency on all datasets.},
  isbn = {978-1-66549-913-2},
  langid = {english},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\A Comparison Study of Pre-trained Language Models for Chinese Legal Document _Qin et al 22.pdf}
}

@inproceedings{qin2022continual,
  title = {Continual {{Few-shot Relation Learning}} via {{Embedding Space Regularization}} and {{Data Augmentation}}},
  booktitle = {Proceedings of the 60th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}} ({{Volume}} 1: {{Long Papers}})},
  author = {Qin, Chengwei and Joty, Shafiq},
  year = {2022},
  pages = {2776--2789},
  publisher = {{Association for Computational Linguistics}},
  address = {{Dublin, Ireland}},
  doi = {10.18653/v1/2022.acl-long.198},
  urldate = {2022-10-18},
  langid = {english},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\Continual Few-shot Relation Learning via Embedding Space Regularization and _Qin_Joty 2022.pdf}
}

@inproceedings{qiu2019dynamically,
  title = {Dynamically Fused Graph Network for Multi-Hop Reasoning},
  booktitle = {Proceedings of the 57th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}}},
  author = {Qiu, Lin and Xiao, Yunxuan and Qu, Yanru and Zhou, Hao and Li, Lei and Zhang, Weinan and Yu, Yong},
  year = {2019},
  pages = {6140--6150},
  doi = {10.18653/v1/P19-1617},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\Dynamically fused graph network for multi-hop reasoning _Qiu et al 22.pdf}
}

@article{quessard2020learning,
  title = {Learning {{Group Structure}} and {{Disentangled Representations}} of {{Dynamical Environments}}},
  author = {Quessard, Robin and Barrett, Thomas D. and Clements, William R.},
  year = {2020},
  month = oct,
  journal = {arXiv:2002.06991 [cs, stat]},
  eprint = {2002.06991},
  primaryclass = {cs, stat},
  urldate = {2022-03-21},
  abstract = {Learning disentangled representations is a key step towards effectively discovering and modelling the underlying structure of environments. In the natural sciences, physics has found great success by describing the universe in terms of symmetry preserving transformations. Inspired by this formalism, we propose a framework, built upon the theory of group representation, for learning representations of a dynamical environment structured around the transformations that generate its evolution. Experimentally, we learn the structure of explicitly symmetric environments without supervision from observational data generated by sequential interactions. We further introduce an intuitive disentanglement regularisation to ensure the interpretability of the learnt representations. We show that our method enables accurate long-horizon predictions, and demonstrate a correlation between the quality of predictions and disentanglement in the latent space.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {⛔ No DOI found,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\Learning Group Structure and Disentangled Representations of Dynamical _Quessard et al 22.pdf}
}

@book{quinonero2008dataset,
  title = {Dataset Shift in Machine Learning},
  author = {{Quinonero-Candela}, Joaquin and Sugiyama, Masashi and Schwaighofer, Anton and Lawrence, Neil D},
  year = {2008},
  publisher = {{Mit Press}}
}

@inproceedings{quirk2017distant,
  title = {Distant {{Supervision}} for {{Relation Extraction}} beyond the {{Sentence Boundary}}},
  booktitle = {Proceedings of the 15th {{Conference}} of the {{European Chapter}} of the {{Association}} for {{Computational Linguistics}}: {{Volume}} 1, {{Long Papers}}},
  author = {Quirk, Chris and Poon, Hoifung},
  year = {2017},
  pages = {1171--1182},
  doi = {10.18653/v1/E17-1110},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\Distant Supervision for Relation Extraction beyond the Sentence Boundary _Quirk_Poon 22.pdf}
}

@inproceedings{radev2000common,
  title = {A Common Theory of Information Fusion from Multiple Text Sources Step One: Cross-Document Structure},
  booktitle = {1st {{SIGdial}} Workshop on {{Discourse}} and Dialogue},
  author = {Radev, Dragomir},
  year = {2000},
  pages = {74--83},
  doi = {10.3115/1117736.1117745},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\A common theory of information fusion from multiple text sources step one _Radev 2000.pdf}
}

@article{radev2004centroidbased,
  title = {Centroid-Based Summarization of Multiple Documents},
  author = {Radev, Dragomir R. and Jing, Hongyan and Sty{\'s}, Ma{\l}gorzata and Tam, Daniel},
  year = {2004},
  month = nov,
  journal = {Information Processing \& Management},
  volume = {40},
  number = {6},
  pages = {919--938},
  issn = {03064573},
  doi = {10.1016/j.ipm.2003.10.006},
  urldate = {2022-08-30},
  langid = {english},
  keywords = {TFIDF}
}

@article{raffel2020exploring,
  title = {Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer.},
  author = {Raffel, Colin and Shazeer, Noam and Roberts, Adam and Lee, Katherine and Narang, Sharan and Matena, Michael and Zhou, Yanqi and Li, Wei and Liu, Peter J and others},
  year = {2020},
  journal = {Journal of Machine Learning Research},
  volume = {21},
  number = {140},
  pages = {1--67},
  keywords = {⛔ No DOI found},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\Exploring the limits of transfer learning with a unified text-to-text _Raffel et al 2020.pdf}
}

@inproceedings{redmon2016you,
  title = {You Only Look Once: {{Unified}}, Real-Time Object Detection},
  booktitle = {Proceedings of the {{IEEE}} Conference on Computer Vision and Pattern Recognition ({{CVPR}})},
  author = {Redmon, Joseph and Divvala, Santosh and Girshick, Ross and Farhadi, Ali},
  year = {2016},
  month = jun,
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\You only look once _Redmon et al 22.pdf}
}

@article{reed2001pareto,
  title = {The {{Pareto}}, {{Zipf}} and Other Power Laws},
  author = {Reed, William J.},
  year = {2001},
  month = dec,
  journal = {Economics Letters},
  volume = {74},
  number = {1},
  pages = {15--19},
  publisher = {{North-Holland}},
  issn = {01651765},
  doi = {10.1016/S0165-1765(01)00524-9},
  abstract = {Many empirical size distributions in economics and elsewhere exhibit power-law behaviour in the upper tail. This article contains a simple explanation for this. It also predicts lower-tail power-law behaviour, which is verified empirically for income and city-size data. \textcopyright{} Elsevier Science B.V.},
  keywords = {C49,City-size distribution,D31,Gibrat's law,Income distribution,Power law,R12,Tail behaviour},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\The Pareto, Zipf and other power laws _Reed 3.pdf}
}

@article{reed2001paretoa,
  title = {The {{Pareto}}, {{Zipf}} and Other Power Laws},
  author = {Reed, William J},
  year = {2001},
  month = dec,
  journal = {Economics Letters},
  volume = {74},
  number = {1},
  pages = {15--19},
  issn = {01651765},
  doi = {10.1016/S0165-1765(01)00524-9},
  urldate = {2022-08-10},
  langid = {english},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\The Pareto, Zipf and other power laws _Reed 4.pdf}
}

@inproceedings{ren2015fasterrcnn,
  title = {Faster {{R-CNN}}: {{Towards}} Real-Time Object Detection with Region Proposal Networks},
  booktitle = {Advances in Neural Information Processing Systems},
  author = {Ren, Shaoqing and He, Kaiming and Girshick, Ross and Sun, Jian},
  editor = {Cortes, C. and Lawrence, N. and Lee, D. and Sugiyama, M. and Garnett, R.},
  year = {2015},
  volume = {28},
  publisher = {{Curran Associates, Inc.}},
  keywords = {⛔ No DOI found},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\Faster R-CNN _Ren et al 22.pdf}
}

@inproceedings{ren2017cotype,
  title = {{{CoType}}: {{Joint Extraction}} of {{Typed Entities}} and {{Relations}} with {{Knowledge Bases}}},
  shorttitle = {{{CoType}}},
  booktitle = {Proceedings of the 26th {{International Conference}} on {{World Wide Web}}},
  author = {Ren, Xiang and Wu, Zeqiu and He, Wenqi and Qu, Meng and Voss, Clare R. and Ji, Heng and Abdelzaher, Tarek F. and Han, Jiawei},
  year = {2017},
  month = apr,
  pages = {1015--1024},
  publisher = {{International World Wide Web Conferences Steering Committee}},
  address = {{Perth Australia}},
  doi = {10.1145/3038912.3052708},
  urldate = {2022-03-10},
  isbn = {978-1-4503-4913-0},
  langid = {english},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\CoType _Ren et al 22.pdf}
}

@inproceedings{ren2017cotypea,
  title = {{{CoType}}: {{Joint Extraction}} of {{Typed Entities}} and {{Relations}} with {{Knowledge Bases}}},
  shorttitle = {{{CoType}}},
  booktitle = {Proceedings of the 26th {{International Conference}} on {{World Wide Web}}},
  author = {Ren, Xiang and Wu, Zeqiu and He, Wenqi and Qu, Meng and Voss, Clare R. and Ji, Heng and Abdelzaher, Tarek F. and Han, Jiawei},
  year = {2017},
  month = apr,
  pages = {1015--1024},
  publisher = {{International World Wide Web Conferences Steering Committee}},
  address = {{Perth Australia}},
  doi = {10.1145/3038912.3052708},
  urldate = {2022-11-04},
  isbn = {978-1-4503-4913-0},
  langid = {english},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\CoType _Ren et al 3.pdf}
}

@inproceedings{rethmeier2022longtail,
  title = {Long-{{Tail Zero}} and {{Few-Shot Learning}} via {{Contrastive Pretraining}} on and for {{Small Data}}},
  booktitle = {{{AAAI Workshop}} on {{Artificial Intelligence}} with {{Biased}} or {{Scarce Data}} ({{AIBSD}})},
  author = {Rethmeier, Nils and Augenstein, Isabelle},
  year = {2022},
  month = may,
  pages = {10},
  publisher = {{MDPI}},
  doi = {10.3390/cmsf2022003010},
  urldate = {2022-06-20},
  langid = {english},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\Long-Tail Zero and Few-Shot Learning via Contrastive Pretraining on and for _Rethmeier_Augenstein 22.pdf}
}

@inproceedings{ribeiro2016why,
  title = {"{{Why Should I Trust You}}?": {{Explaining}} the {{Predictions}} of {{Any Classifier}}},
  shorttitle = {"{{Why Should I Trust You}}?},
  booktitle = {Proceedings of the 22nd {{ACM SIGKDD International Conference}} on {{Knowledge Discovery}} and {{Data Mining}}},
  author = {Ribeiro, Marco Tulio and Singh, Sameer and Guestrin, Carlos},
  year = {2016},
  month = aug,
  pages = {1135--1144},
  publisher = {{ACM}},
  address = {{San Francisco California USA}},
  doi = {10.1145/2939672.2939778},
  urldate = {2022-07-11},
  isbn = {978-1-4503-4232-2},
  langid = {english},
  keywords = {LIME,reference-based},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\Why Should I Trust You _Ribeiro et al 22.pdf}
}

@inproceedings{ribeiro2018anchors,
  title = {Anchors: {{High-precision}} Model-Agnostic Explanations},
  booktitle = {Proceedings of the {{AAAI}} Conference on Artificial Intelligence},
  author = {Ribeiro, Marco Tulio and Singh, Sameer and Guestrin, Carlos},
  year = {2018},
  volume = {32},
  keywords = {⛔ No DOI found},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\Anchors _Ribeiro et al 4.pdf}
}

@article{ribeiro2018anchorsa,
  title = {Anchors: {{High-Precision Model-Agnostic Explanations}}},
  shorttitle = {Anchors},
  author = {Ribeiro, Marco Tulio and Singh, Sameer and Guestrin, Carlos},
  year = {2018},
  month = apr,
  journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
  volume = {32},
  number = {1},
  issn = {2374-3468, 2159-5399},
  doi = {10.1609/aaai.v32i1.11491},
  urldate = {2022-07-11},
  abstract = {We introduce a novel model-agnostic system that explains the behavior of complex models with high-precision rules called anchors, representing local, "sufficient" conditions for predictions. We propose an algorithm to efficiently compute these explanations for any black-box model with high-probability guarantees. We demonstrate the flexibility of anchors by explaining a myriad of different models for different domains and tasks. In a user study, we show that anchors enable users to predict how a model would behave on unseen instances with less effort and higher precision, as compared to existing linear explanations or no explanations.},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\Anchors _Ribeiro et al 3.pdf}
}

@inproceedings{ribeiro2018semantically,
  title = {Semantically {{Equivalent Adversarial Rules}} for {{Debugging NLP}} Models},
  booktitle = {Proceedings of the 56th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}} ({{Volume}} 1: {{Long Papers}})},
  author = {Ribeiro, Marco Tulio and Singh, Sameer and Guestrin, Carlos},
  year = {2018},
  pages = {856--865},
  publisher = {{Association for Computational Linguistics}},
  address = {{Melbourne, Australia}},
  doi = {10.18653/v1/P18-1079},
  urldate = {2022-07-11},
  langid = {english},
  keywords = {attack,paraphrasing},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\Semantically Equivalent Adversarial Rules for Debugging NLP models _Ribeiro et al 22.pdf}
}

@inproceedings{ribeiro2020accuracy,
  title = {Beyond {{Accuracy}}: {{Behavioral Testing}} of {{NLP Models}} with {{CheckList}}},
  shorttitle = {Beyond {{Accuracy}}},
  booktitle = {Proceedings of the 58th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}}},
  author = {Ribeiro, Marco Tulio and Wu, Tongshuang and Guestrin, Carlos and Singh, Sameer},
  year = {2020},
  pages = {4902--4912},
  publisher = {{Association for Computational Linguistics}},
  address = {{Online}},
  doi = {10.18653/v1/2020.acl-main.442},
  urldate = {2022-07-29},
  langid = {english},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\Beyond Accuracy _Ribeiro et al 22.pdf}
}

@inproceedings{riedel2010modeling,
  title = {Modeling Relations and Their Mentions without Labeled Text},
  booktitle = {Joint {{European Conference}} on {{Machine Learning}} and {{Knowledge Discovery}} in {{Databases}}},
  author = {Riedel, Sebastian and Yao, Limin and McCallum, Andrew},
  year = {2010},
  pages = {148--163},
  publisher = {{Springer}},
  doi = {10.1007/978-3-642-15939-8_10},
  keywords = {NYT},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\Modeling relations and their mentions without labeled text _Riedel et al 22.pdf}
}

@inproceedings{riedel2013relation,
  title = {Relation {{Extraction}} with {{Matrix Factorization}} and {{Universal Schemas}}},
  booktitle = {Proceedings of the 2013 {{Conference}} of the {{North American Chapter}} of the {{Association}} for {{Computational Linguistics}}: {{Human Language Technologies}}},
  author = {Riedel, Sebastian and Yao, Limin and McCallum, Andrew and Marlin, Benjamin M.},
  year = {2013},
  month = jun,
  pages = {74--84},
  publisher = {{Association for Computational Linguistics}},
  address = {{Atlanta, Georgia}},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\Relation Extraction with Matrix Factorization and Universal Schemas _Riedel et al 2013.pdf}
}

@inproceedings{rink2010utd,
  title = {Utd: {{Classifying}} Semantic Relations by Combining Lexical and Semantic Resources},
  booktitle = {Proceedings of the 5th International Workshop on Semantic Evaluation},
  author = {Rink, Bryan and Harabagiu, Sanda},
  year = {2010},
  pages = {256--259},
  keywords = {⛔ No DOI found}
}

@misc{rohde2021hierarchical,
  title = {Hierarchical {{Learning}} for {{Generation}} with {{Long Source Sequences}}},
  author = {Rohde, Tobias and Wu, Xiaoxia and Liu, Yinhan},
  year = {2021},
  month = sep,
  number = {arXiv:2104.07545},
  eprint = {arXiv:2104.07545},
  publisher = {{arXiv}},
  urldate = {2022-08-20},
  abstract = {One of the challenges for current sequence to sequence (seq2seq) models is processing long sequences, such as those in summarization and document level machine translation tasks. These tasks require the model to reason at the token level as well as the sentence and paragraph level. We design and study a new Hierarchical Attention Transformer-based architecture (HAT) that outperforms standard Transformers on several sequence to sequence tasks. Furthermore, our model achieves state-of-theart ROUGE scores on four summarization tasks, including PubMed, arXiv, CNN/DM, SAMSum, and AMI. Our model outperforms document-level machine translation baseline on the WMT20 English to German translation task. We investigate what the hierarchical layers learn by visualizing the hierarchical encoder-decoder attention. Finally, we study hierarchical learning on encoder-only pre-training and analyze its performance on classification tasks.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Computation and Language},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\Hierarchical Learning for Generation with Long Source Sequences _Rohde et al 2021.pdf}
}

@inproceedings{rongali2020don,
  title = {Don't {{Parse}}, {{Generate}}! {{A Sequence}} to {{Sequence Architecture}} for {{Task-Oriented Semantic Parsing}}},
  booktitle = {Proceedings of {{The Web Conference}} 2020},
  author = {Rongali, Subendhu and Soldaini, Luca and Monti, Emilio and Hamza, Wael},
  year = {2020},
  month = apr,
  pages = {2962--2968},
  publisher = {{ACM}},
  address = {{Taipei Taiwan}},
  doi = {10.1145/3366423.3380064},
  urldate = {2022-06-20},
  isbn = {978-1-4503-7023-3},
  langid = {english},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\Don’t Parse, Generate _Rongali et al 22.pdf}
}

@inproceedings{rony2022rome,
  title = {{{RoMe}}: {{A Robust Metric}} for {{Evaluating Natural Language Generation}}},
  shorttitle = {{{RoMe}}},
  booktitle = {Proceedings of the 60th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}} ({{Volume}} 1: {{Long Papers}})},
  author = {Rony, Md Rashad Al Hasan and Kovriguina, Liubov and Chaudhuri, Debanjan and Usbeck, Ricardo and Lehmann, Jens},
  year = {2022},
  pages = {5645--5657},
  publisher = {{Association for Computational Linguistics}},
  address = {{Dublin, Ireland}},
  doi = {10.18653/v1/2022.acl-long.387},
  urldate = {2022-06-16},
  langid = {english},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\RoMe _Rony et al 22.pdf}
}

@inproceedings{rosenfeld2021the,
  title = {The Risks of Invariant Risk Minimization},
  booktitle = {International Conference on Learning Representations},
  author = {Rosenfeld, Elan and Ravikumar, Pradeep Kumar and Risteski, Andrej},
  year = {2021},
  keywords = {⛔ No DOI found},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\The risks of invariant risk minimization _Rosenfeld et al 22.pdf}
}

@misc{rosenfeld2022domainadjusted,
  title = {Domain-{{Adjusted Regression}} or: {{ERM May Already Learn Features Sufficient}} for {{Out-of-Distribution Generalization}}},
  shorttitle = {Domain-{{Adjusted Regression}} Or},
  author = {Rosenfeld, Elan and Ravikumar, Pradeep and Risteski, Andrej},
  year = {2022},
  month = feb,
  number = {arXiv:2202.06856},
  eprint = {arXiv:2202.06856},
  publisher = {{arXiv}},
  urldate = {2022-09-02},
  abstract = {A common explanation for the failure of deep networks to generalize out-of-distribution is that they fail to recover the ``correct'' features. Focusing on the domain generalization setting, we challenge this notion with a simple experiment which suggests that ERM already learns sufficient features and that the current bottleneck is not feature learning, but robust regression. We therefore argue that devising simpler methods for learning predictors on existing features is a promising direction for future research. Towards this end, we introduce Domain-Adjusted Regression (DARE), a convex objective for learning a linear predictor that is provably robust under a new model of distribution shift. Rather than learning one function, DARE performs a domain-specific adjustment to unify the domains in a canonical latent space and learns to predict in this space. Under a natural model, we prove that the DARE solution is the minimax-optimal predictor for a constrained set of test distributions. Further, we provide the first finite-environment convergence guarantee to the minimax risk, improving over existing results which show a ``threshold effect''. Evaluated on finetuned features, we find that DARE compares favorably to prior methods, consistently achieving equal or better performance.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\Domain-Adjusted Regression or _Rosenfeld et al 2022.pdf}
}

@inproceedings{roth2004linear,
  title = {A Linear Programming Formulation for Global Inference in Natural Language Tasks},
  booktitle = {Proceedings of the Eighth Conference on Computational Natural Language Learning ({{CoNLL-2004}}) at {{HLT-NAACL}} 2004},
  author = {Roth, Dan and Yih, Wen-tau},
  year = {05 6 - 05 7 2004},
  pages = {1--8},
  publisher = {{Association for Computational Linguistics}},
  address = {{Boston, Massachusetts, USA}},
  keywords = {⛔ No DOI found,conll04},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\A linear programming formulation for global inference in natural language tasks _Roth_Yih 2004.pdf}
}

@article{rubin2005causal,
  title = {Causal {{Inference Using Potential Outcomes}}},
  author = {Rubin, Donald B.},
  year = {2005},
  month = mar,
  journal = {Journal of the American Statistical Association},
  volume = {100},
  number = {469},
  pages = {322--331},
  issn = {0162-1459},
  doi = {10.1198/016214504000001880},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\Causal Inference Using Potential Outcomes _Rubin 22.pdf}
}

@inproceedings{saha2022explanation,
  title = {Explanation {{Graph Generation}} via {{Pre-trained Language Models}}: {{An Empirical Study}} with {{Contrastive Learning}}},
  shorttitle = {Explanation {{Graph Generation}} via {{Pre-trained Language Models}}},
  booktitle = {Proceedings of the 60th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}} ({{Volume}} 1: {{Long Papers}})},
  author = {Saha, Swarnadeep and Yadav, Prateek and Bansal, Mohit},
  year = {2022},
  pages = {1190--1208},
  publisher = {{Association for Computational Linguistics}},
  address = {{Dublin, Ireland}},
  doi = {10.18653/v1/2022.acl-long.85},
  urldate = {2022-06-05},
  langid = {english},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\Explanation Graph Generation via Pre-trained Language Models _Saha et al 4.pdf}
}

@inproceedings{saha2022explanationa,
  title = {Explanation {{Graph Generation}} via {{Pre-trained Language Models}}: {{An Empirical Study}} with {{Contrastive Learning}}},
  shorttitle = {Explanation {{Graph Generation}} via {{Pre-trained Language Models}}},
  booktitle = {Proceedings of the 60th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}} ({{Volume}} 1: {{Long Papers}})},
  author = {Saha, Swarnadeep and Yadav, Prateek and Bansal, Mohit},
  year = {2022},
  pages = {1190--1208},
  publisher = {{Association for Computational Linguistics}},
  address = {{Dublin, Ireland}},
  doi = {10.18653/v1/2022.acl-long.85},
  urldate = {2022-06-30},
  langid = {english},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\Explanation Graph Generation via Pre-trained Language Models _Saha et al 3.pdf}
}

@inproceedings{sahu2019intersentence,
  title = {Inter-Sentence {{Relation Extraction}} with {{Document-level Graph Convolutional Neural Network}}},
  booktitle = {Proceedings of the 57th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}}},
  author = {Sahu, Sunil Kumar and Christopoulou, Fenia and Miwa, Makoto and Ananiadou, Sophia},
  year = {2019},
  pages = {4309--4316},
  doi = {10.18653/v1/P19-1423},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\Inter-sentence Relation Extraction with Document-level Graph Convolutional _Sahu et al 22.pdf}
}

@article{sahu2023overview,
  title = {An {{Overview}} of {{Machine Learning}}, {{Deep Learning}}, and {{Reinforcement Learning-Based Techniques}} in {{Quantitative Finance}}: {{Recent Progress}} and {{Challenges}}},
  shorttitle = {An {{Overview}} of {{Machine Learning}}, {{Deep Learning}}, and {{Reinforcement Learning-Based Techniques}} in {{Quantitative Finance}}},
  author = {Sahu, Santosh Kumar and Mokhade, Anil and Bokde, Neeraj Dhanraj},
  year = {2023},
  month = feb,
  journal = {Applied Sciences},
  volume = {13},
  number = {3},
  pages = {1956},
  issn = {2076-3417},
  doi = {10.3390/app13031956},
  urldate = {2023-02-14},
  abstract = {Forecasting the behavior of the stock market is a classic but difficult topic, one that has attracted the interest of both economists and computer scientists. Over the course of the last couple of decades, researchers have investigated linear models as well as models that are based on machine learning (ML), deep learning (DL), reinforcement learning (RL), and deep reinforcement learning (DRL) in order to create an accurate predictive model. Machine learning algorithms can now extract high-level financial market data patterns. Investors are using deep learning models to anticipate and evaluate stock and foreign exchange markets due to the advantage of artificial intelligence. Recent years have seen a proliferation of the deep reinforcement learning algorithm's application in algorithmic trading. DRL agents, which combine price prediction and trading signal production, have been used to construct several completely automated trading systems or strategies. Our objective is to enable interested researchers to stay current and easily imitate earlier findings. In this paper, we have worked to explain the utility of Machine Learning, Deep Learning, Reinforcement Learning, and Deep Reinforcement Learning in Quantitative Finance (QF) and the Stock Market. We also outline potential future study paths in this area based on the overview that was presented before.},
  langid = {english},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\An Overview of Machine Learning, Deep Learning, and Reinforcement _Sahu et al 2023.pdf}
}

@article{sandhaus2008new,
  title = {The New York Times Annotated Corpus},
  author = {Sandhaus, Evan},
  year = {2008},
  journal = {Linguistic Data Consortium, Philadelphia},
  volume = {6},
  number = {12},
  pages = {e26752},
  keywords = {⛔ No DOI found}
}

@article{santos2015classifying,
  title = {Classifying Relations by Ranking with Convolutional Neural Networks},
  author = {dos Santos, Cicero Nogueira and Xiang, Bing and Zhou, Bowen},
  year = {2015},
  journal = {arXiv preprint arXiv:1504.06580},
  eprint = {1504.06580},
  archiveprefix = {arxiv},
  keywords = {⛔ No DOI found}
}

@article{sargent202210,
  title = {10 Tips on How We Write Papers},
  author = {Sargent, Edward H.},
  year = {2022},
  month = nov,
  journal = {Matter},
  volume = {5},
  number = {11},
  pages = {3562--3564},
  issn = {25902385},
  doi = {10.1016/j.matt.2022.09.025},
  urldate = {2022-11-29},
  langid = {english}
}

@inproceedings{sawhney2021quantitative,
  title = {Quantitative {{Day Trading}} from {{Natural Language}} Using {{Reinforcement Learning}}},
  booktitle = {Proceedings of the 2021 {{Conference}} of the {{North American Chapter}} of the {{Association}} for {{Computational Linguistics}}: {{Human Language Technologies}}},
  author = {Sawhney, Ramit and Wadhwa, Arnav and Agarwal, Shivam and Shah, Rajiv Ratn},
  year = {2021},
  pages = {4018--4030},
  publisher = {{Association for Computational Linguistics}},
  address = {{Online}},
  doi = {10.18653/v1/2021.naacl-main.316},
  urldate = {2023-02-20},
  langid = {english},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\Quantitative Day Trading from Natural Language using Reinforcement Learning _Sawhney et al 2021.pdf}
}

@incollection{scholkopf2022causality,
  title = {Causality for {{Machine Learning}}},
  booktitle = {Probabilistic and {{Causal Inference}}},
  author = {Sch{\"o}lkopf, Bernhard},
  editor = {Geffner, Hector and Dechter, Rina and Halpern, Joseph Y.},
  year = {2022},
  month = feb,
  edition = {First},
  pages = {765--804},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/3501714.3501755},
  urldate = {2022-07-16},
  isbn = {978-1-4503-9586-1},
  langid = {english},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\Causality for Machine Learning _Schölkopf 22.pdf}
}

@inproceedings{schumann2022analyzing,
  title = {Analyzing {{Generalization}} of {{Vision}} and {{Language Navigation}} to {{Unseen Outdoor Areas}}},
  booktitle = {Proceedings of the 60th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}} ({{Volume}} 1: {{Long Papers}})},
  author = {Schumann, Raphael and Riezler, Stefan},
  year = {2022},
  pages = {7519--7532},
  publisher = {{Association for Computational Linguistics}},
  address = {{Dublin, Ireland}},
  doi = {10.18653/v1/2022.acl-long.518},
  urldate = {2022-11-01},
  langid = {english},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\Analyzing Generalization of Vision and Language Navigation to Unseen Outdoor _Schumann_Riezler 2022.pdf}
}

@article{schuster1997bidirectional,
  title = {Bidirectional Recurrent Neural Networks},
  author = {Schuster, Mike and Paliwal, Kuldip K.},
  year = {1997},
  journal = {IEEE transactions on Signal Processing},
  volume = {45},
  number = {11},
  pages = {2673--2681},
  publisher = {{Ieee}},
  doi = {10.1109/78.650093},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\Bidirectional recurrent neural networks _Schuster_Paliwal 12.pdf}
}

@inproceedings{see2017get,
  title = {Get to the Point: {{Summarization}} with Pointer-Generator Networks},
  booktitle = {Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: {{Long}} Papers)},
  author = {See, Abigail and Liu, Peter J and Manning, Christopher D},
  year = {2017},
  pages = {1073--1083},
  doi = {10.18653/v1/P17-1099},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\Get to the point _See et al 22.pdf}
}

@inproceedings{sekhon2022whitebox,
  title = {White-Box {{Testing}} of {{NLP}} Models with {{Mask Neuron Coverage}}},
  booktitle = {Findings of the {{Association}} for {{Computational Linguistics}}: {{NAACL}} 2022},
  author = {Sekhon, Arshdeep and Ji, Yangfeng and Dwyer, Matthew and Qi, Yanjun},
  year = {2022},
  pages = {1547--1558},
  publisher = {{Association for Computational Linguistics}},
  address = {{Seattle, United States}},
  doi = {10.18653/v1/2022.findings-naacl.116},
  urldate = {2022-11-22},
  langid = {english},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\White-box Testing of NLP models with Mask Neuron Coverage _Sekhon et al 2022.pdf}
}

@article{sevim2022gender,
  title = {Gender Bias in Legal Corpora and Debiasing It},
  author = {Sevim, Nurullah and {\c S}ahinu{\c c}, Furkan and Ko{\c c}, Aykut},
  year = {2022},
  month = mar,
  journal = {Natural Language Engineering},
  pages = {1--34},
  issn = {1351-3249, 1469-8110},
  doi = {10.1017/S1351324922000122},
  urldate = {2022-08-06},
  abstract = {Abstract             Word embeddings have become important building blocks that are used profoundly in natural language processing (NLP). Despite their several advantages, word embeddings can unintentionally accommodate some gender- and ethnicity-based biases that are present within the corpora they are trained on. Therefore, ethical concerns have been raised since word embeddings are extensively used in several high-level algorithms. Studying such biases and debiasing them have recently become an important research endeavor. Various studies have been conducted to measure the extent of bias that word embeddings capture and to eradicate them. Concurrently, as another subfield that has started to gain traction recently, the applications of NLP in the field of law have started to increase and develop rapidly. As law has a direct and utmost effect on people's lives, the issues of bias for NLP applications in legal domain are certainly important. However, to the best of our knowledge, bias issues have not yet been studied in the context of legal corpora. In this article, we approach the gender bias problem from the scope of legal text processing domain. Word embedding models that are trained on corpora composed by legal documents and legislation from different countries have been utilized to measure and eliminate gender bias in legal documents. Several methods have been employed to reveal the degree of gender bias and observe its variations over countries. Moreover, a debiasing method has been used to neutralize unwanted bias. The preservation of semantic coherence of the debiased vector space has also been demonstrated by using high-level tasks. Finally, overall results and their implications have been discussed in the scope of NLP in legal domain.},
  langid = {english},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\Gender bias in legal corpora and debiasing it _Sevim et al 2022.pdf}
}

@inproceedings{sha2021learning,
  title = {Learning from the Best: {{Rationalizing}} Predictions by Adversarial Information Calibration.},
  booktitle = {{{AAAI}}},
  author = {Sha, Lei and Camburu, Oana-Maria and Lukasiewicz, Thomas},
  year = {2021},
  pages = {13771--13779},
  keywords = {⛔ No DOI found},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\Learning from the best _Sha et al 22.pdf}
}

@article{shang2022onerel,
  title = {{{OneRel}}:{{Joint Entity}} and {{Relation Extraction}} with {{One Module}} in {{One Step}}},
  shorttitle = {{{OneRel}}},
  author = {Shang, Yu-Ming and Huang, Heyan and Mao, Xian-Ling},
  year = {2022},
  month = mar,
  journal = {arXiv:2203.05412 [cs]},
  eprint = {2203.05412},
  primaryclass = {cs},
  urldate = {2022-03-12},
  abstract = {Joint entity and relation extraction is an essential task in natural language processing and knowledge graph construction. Existing approaches usually decompose the joint extraction task into several basic modules or processing steps to make it easy to conduct. However, such a paradigm ignores the fact that the three elements of a triple are interdependent and indivisible. Therefore, previous joint methods suffer from the problems of cascading errors and redundant information. To address these issues, in this paper, we propose a novel joint entity and relation extraction model, named OneRel, which casts joint extraction as a fine-grained triple classification problem. Specifically, our model consists of a scoring-based classifier and a relation-specific horns tagging strategy. The former evaluates whether a token pair and a relation belong to a factual triple. The latter ensures a simple but effective decoding process. Extensive experimental results on two widely used datasets demonstrate that the proposed method performs better than the state-of-the-art baselines, and delivers consistent performance gain on complex scenarios of various overlapping patterns and multiple triples.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {⛔ No DOI found,Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\OneRel _Shang et al 22.pdf}
}

@inproceedings{shao2020graph,
  title = {Is {{Graph Structure Necessary}} for {{Multi-hop Question Answering}}?},
  booktitle = {Proceedings of the 2020 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}} ({{EMNLP}})},
  author = {Shao, Nan and Cui, Yiming and Liu, Ting and Wang, Shijin and Hu, Guoping},
  year = {2020},
  pages = {7187--7192},
  doi = {10.18653/v1/2020.emnlp-main.583},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\Is Graph Structure Necessary for Multi-hop Question Answering _Shao et al 22.pdf}
}

@article{shapley1971assignment,
  title = {The Assignment Game {{I}}: {{The}} Core},
  shorttitle = {The Assignment Game {{I}}},
  author = {Shapley, L. S. and Shubik, M.},
  year = {1971},
  month = dec,
  journal = {International Journal of Game Theory},
  volume = {1},
  number = {1},
  pages = {111--130},
  issn = {0020-7276, 1432-1270},
  doi = {10.1007/BF01753437},
  urldate = {2022-11-22},
  langid = {english},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\The assignment game I _Shapley_Shubik 1971.pdf}
}

@inproceedings{sharma2019bigpatent,
  title = {{{BIGPATENT}}: {{A Large-Scale Dataset}} for {{Abstractive}} and {{Coherent Summarization}}},
  shorttitle = {{{BIGPATENT}}},
  booktitle = {Proceedings of the 57th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}}},
  author = {Sharma, Eva and Li, Chen and Wang, Lu},
  year = {2019},
  pages = {2204--2213},
  publisher = {{Association for Computational Linguistics}},
  address = {{Florence, Italy}},
  doi = {10.18653/v1/P19-1212},
  urldate = {2022-08-31},
  langid = {english},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\BIGPATENT _Sharma et al 2019.pdf}
}

@article{shen2020stable,
  title = {Stable {{Learning}} via {{Sample Reweighting}}},
  author = {Shen, Zheyan and Cui, Peng and Zhang, Tong and Kunag, Kun},
  year = {2020},
  month = apr,
  journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
  volume = {34},
  number = {04},
  pages = {5692--5699},
  issn = {2374-3468, 2159-5399},
  doi = {10.1609/aaai.v34i04.6024},
  urldate = {2022-11-05},
  abstract = {We consider the problem of learning linear prediction models with model misspecification bias. In such case, the collinearity among input variables may inflate the error of parameter estimation, resulting in instability of prediction results when training and test distributions do not match. In this paper we theoretically analyze this fundamental problem and propose a sample reweighting method that reduces collinearity among input variables. Our method can be seen as a pretreatment of data to improve the condition of design matrix, and it can then be combined with any standard learning method for parameter estimation and variable selection. Empirical studies on both simulation and real datasets demonstrate the effectiveness of our method in terms of more stable performance across different distributed data.},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\Stable Learning via Sample Reweighting _Shen et al 2020.pdf}
}

@misc{shen2021outofdistribution,
  title = {Towards {{Out-Of-Distribution Generalization}}: {{A Survey}}},
  shorttitle = {Towards {{Out-Of-Distribution Generalization}}},
  author = {Shen, Zheyan and Liu, Jiashuo and He, Yue and Zhang, Xingxuan and Xu, Renzhe and Yu, Han and Cui, Peng},
  year = {2021},
  month = aug,
  number = {arXiv:2108.13624},
  eprint = {arXiv:2108.13624},
  publisher = {{arXiv}},
  urldate = {2022-09-03},
  abstract = {Classic machine learning methods are built on the i.i.d. assumption that training and testing data are independent and identically distributed. However, in real scenarios, the i.i.d. assumption can hardly be satisfied, rendering the sharp drop of classic machine learning algorithms' performances under distributional shifts, which indicates the significance of investigating the Out-of-Distribution generalization problem. Out-of-Distribution (OOD) generalization problem addresses the challenging setting where the testing distribution is unknown and different from the training. This paper serves as the first effort to systematically and comprehensively discuss the OOD generalization problem, from the definition, methodology, evaluation to the implications and future directions. Firstly, we provide the formal definition of the OOD generalization problem. Secondly, existing methods are categorized into three parts based on their positions in the whole learning pipeline, namely unsupervised representation learning, supervised model learning and optimization, and typical methods for each category are discussed in detail. We then demonstrate the theoretical connections of different categories, and introduce the commonly used datasets and evaluation metrics. Finally, we summarize the whole literature and raise some future directions for OOD generalization problem. The summary of OOD generalization methods reviewed in this survey can be found at http://out-of-distribution-generalization.com.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\Towards Out-Of-Distribution Generalization _Shen et al 2021.pdf}
}

@inproceedings{shen2022mred,
  title = {{{MReD}}: {{A Meta-Review Dataset}} for {{Structure-Controllable Text Generation}}},
  shorttitle = {{{MReD}}},
  booktitle = {Findings of the {{Association}} for {{Computational Linguistics}}: {{ACL}} 2022},
  author = {Shen, Chenhui and Cheng, Liying and Zhou, Ran and Bing, Lidong and You, Yang and Si, Luo},
  year = {2022},
  pages = {2521--2535},
  publisher = {{Association for Computational Linguistics}},
  address = {{Dublin, Ireland}},
  doi = {10.18653/v1/2022.findings-acl.198},
  urldate = {2022-06-07},
  langid = {english},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\MReD _Shen et al 22.pdf}
}

@article{shi2019simple,
  title = {Simple Bert Models for Relation Extraction and Semantic Role Labeling},
  author = {Shi, Peng and Lin, Jimmy},
  year = {2019},
  journal = {arXiv preprint arXiv:1904.05255},
  eprint = {1904.05255},
  archiveprefix = {arxiv},
  keywords = {⛔ No DOI found}
}

@misc{shi2022gradient,
  title = {Gradient {{Estimation}} with {{Discrete Stein Operators}}},
  author = {Shi, Jiaxin and Zhou, Yuhao and Hwang, Jessica and Titsias, Michalis K. and Mackey, Lester},
  year = {2022},
  month = nov,
  number = {arXiv:2202.09497},
  eprint = {arXiv:2202.09497},
  publisher = {{arXiv}},
  urldate = {2022-11-29},
  abstract = {Gradient estimation\textemdash approximating the gradient of an expectation with respect to the parameters of a distribution\textemdash is central to the solution of many machine learning problems. However, when the distribution is discrete, most common gradient estimators suffer from excessive variance. To improve the quality of gradient estimation, we introduce a variance reduction technique based on Stein operators for discrete distributions. We then use this technique to build flexible control variates for the REINFORCE leave-one-out estimator. Our control variates can be adapted online to minimize variance and do not require extra evaluations of the target function. In benchmark generative modeling tasks such as training binary variational autoencoders, our gradient estimator achieves substantially lower variance than state-of-the-art estimators with the same number of function evaluations.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C\:\\Users\\Hytn\\Zotero\\storage\\DSESSX5D\\Shi et al. - 2022 - Gradient Estimation with Discrete Stein Operators.pdf}
}

@inproceedings{shrikumar2017learning,
  title = {Learning Important Features through Propagating Activation Differences},
  booktitle = {Proceedings of the 34th International Conference on Machine Learning},
  author = {Shrikumar, Avanti and Greenside, Peyton and Kundaje, Anshul},
  editor = {Precup, Doina and Teh, Yee Whye},
  year = {2017},
  month = aug,
  series = {Proceedings of Machine Learning Research},
  volume = {70},
  pages = {3145--3153},
  publisher = {{PMLR}},
  abstract = {The purported ``black box'' nature of neural networks is a barrier to adoption in applications where interpretability is essential. Here we present DeepLIFT (Deep Learning Important FeaTures), a method for decomposing the output prediction of a neural network on a specific input by backpropagating the contributions of all neurons in the network to every feature of the input. DeepLIFT compares the activation of each neuron to its `reference activation' and assigns contribution scores according to the difference. By optionally giving separate consideration to positive and negative contributions, DeepLIFT can also reveal dependencies which are missed by other approaches. Scores can be computed efficiently in a single backward pass. We apply DeepLIFT to models trained on MNIST and simulated genomic data, and show significant advantages over gradient-based methods. Video tutorial: http://goo.gl/qKb7pL code: http://goo.gl/RM8jvH},
  pdf = {http://proceedings.mlr.press/v70/shrikumar17a/shrikumar17a.pdf},
  keywords = {DeepLIFT,reference-based},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\Learning important features through propagating activation differences _Shrikumar et al 22.pdf}
}

@incollection{shukla2022text,
  title = {Text {{Summarization}} of {{Legal Documents Using Reinforcement Learning}}: {{A Study}}},
  shorttitle = {Text {{Summarization}} of {{Legal Documents Using Reinforcement Learning}}},
  booktitle = {Intelligent {{Sustainable Systems}}},
  author = {Shukla, Bharti and Gupta, Sonam and Yadav, Arun Kumar and Yadav, Divakar},
  editor = {Raj, Jennifer S. and Shi, Yong and Pelusi, Danilo and Balas, Valentina Emilia},
  year = {2022},
  volume = {458},
  pages = {403--414},
  publisher = {{Springer Nature Singapore}},
  address = {{Singapore}},
  doi = {10.1007/978-981-19-2894-9_30},
  urldate = {2023-02-20},
  isbn = {978-981-19289-3-2 978-981-19289-4-9},
  langid = {english},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\Text Summarization of Legal Documents Using Reinforcement Learning _Shukla et al 2022.pdf}
}

@inproceedings{sikdar2021integrated,
  title = {Integrated {{Directional Gradients}}: {{Feature Interaction Attribution}} for {{Neural NLP Models}}},
  shorttitle = {Integrated {{Directional Gradients}}},
  booktitle = {Proceedings of the 59th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}} and the 11th {{International Joint Conference}} on {{Natural Language Processing}} ({{Volume}} 1: {{Long Papers}})},
  author = {Sikdar, Sandipan and Bhattacharya, Parantapa and Heese, Kieran},
  year = {2021},
  pages = {865--878},
  publisher = {{Association for Computational Linguistics}},
  address = {{Online}},
  doi = {10.18653/v1/2021.acl-long.71},
  urldate = {2022-03-13},
  abstract = {In this paper, we introduce Integrated Directional Gradients (IDG), a method for attributing importance scores to groups of features, indicating their relevance to the output of a neural network model for a given input. The success of Deep Neural Networks has been attributed to their ability to capture higher level feature interactions. Hence, in the last few years capturing the importance of these feature interactions has received increased prominence in ML interpretability literature. In this paper, we formally define the feature group attribution problem and outline a set of axioms that any intuitive feature group attribution method should satisfy. Earlier, cooperative game theory inspired axiomatic methods only borrowed axioms from solution concepts (such as Shapley value) for individual feature attributions and introduced their own extensions to model interactions. In contrast, our formulation is inspired by axioms satisfied by characteristic functions as well as solution concepts in cooperative game theory literature. We believe that characteristic functions are much better suited to model importance of groups compared to just solution concepts. We demonstrate that our proposed method, IDG, satisfies all the axioms. Using IDG we analyze two state-of-the-art text classifiers on three benchmark datasets for sentiment analysis. Our experiments show that IDG is able to effectively capture semantic interactions in linguistic models via negations and conjunctions.},
  langid = {english},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\Integrated Directional Gradients _Sikdar et al 22.pdf}
}

@article{silver2021reward,
  title = {Reward Is Enough},
  author = {Silver, David and Singh, Satinder and Precup, Doina and Sutton, Richard S.},
  year = {2021},
  month = oct,
  journal = {Artificial Intelligence},
  volume = {299},
  pages = {103535},
  issn = {00043702},
  doi = {10.1016/j.artint.2021.103535},
  urldate = {2022-12-05},
  langid = {english},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\Reward is enough _Silver et al 2021.pdf}
}

@inproceedings{simon2019unsupervised,
  title = {Unsupervised {{Information Extraction}}: {{Regularizing Discriminative Approaches}} with {{Relation Distribution Losses}}},
  shorttitle = {Unsupervised {{Information Extraction}}},
  booktitle = {Proceedings of the 57th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}}},
  author = {Simon, {\'E}tienne and Guigue, Vincent and Piwowarski, Benjamin},
  year = {2019},
  pages = {1378--1387},
  publisher = {{Association for Computational Linguistics}},
  address = {{Florence, Italy}},
  doi = {10.18653/v1/P19-1133},
  urldate = {2022-09-10},
  langid = {english},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\Unsupervised Information Extraction _Simon et al 2019.pdf}
}

@inproceedings{simonyan2019deep,
  title = {Deep inside Convolutional Networks: Visualising Image Classification Models and Saliency Maps},
  author = {Simonyan, K and Vedaldi, A and Zisserman, A},
  year = {2014},
  series = {Proceedings of the {{International Conference}} on {{Learning Representations}} ({{ICLR}})},
  pages = {1--8},
  publisher = {{ICLR}},
  keywords = {⛔ No DOI found},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\Deep inside convolutional networks _Simonyan et al 22.pdf}
}

@inproceedings{soares2019matching,
  title = {Matching the {{Blanks}}: {{Distributional Similarity}} for {{Relation Learning}}},
  booktitle = {Proceedings of the 57th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}}},
  author = {Soares, Livio Baldini and FitzGerald, Nicholas Arthur and Ling, Jeffrey and Kwiatkowski, Tom},
  year = {2019},
  pages = {2895--2905},
  doi = {10.18653/v1/P19-1279},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\Matching the Blanks _Soares et al 22.pdf}
}

@inproceedings{song2018nary,
  title = {N-Ary {{Relation Extraction}} Using {{Graph-State LSTM}}},
  booktitle = {Proceedings of the 2018 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}}},
  author = {Song, Linfeng and Zhang, Yue and Wang, Zhiguo and Gildea, Daniel},
  year = {2018},
  pages = {2226--2235},
  doi = {10.18653/v1/D18-1246},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\N-ary Relation Extraction using Graph-State LSTM _Song et al 22.pdf}
}

@article{song2022effectiveness,
  title = {On the {{Effectiveness}} of {{Pre-Trained Language Models}} for {{Legal Natural Language Processing}}: {{An Empirical Study}}},
  shorttitle = {On the {{Effectiveness}} of {{Pre-Trained Language Models}} for {{Legal Natural Language Processing}}},
  author = {Song, Dezhao and Gao, Sally and He, Baosheng and Schilder, Frank},
  year = {2022},
  journal = {IEEE Access},
  volume = {10},
  pages = {75835--75858},
  issn = {2169-3536},
  doi = {10.1109/ACCESS.2022.3190408},
  urldate = {2022-08-31},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\On the Effectiveness of Pre-Trained Language Models for Legal Natural Language _Song et al 2022.pdf}
}

@inproceedings{song2022improving,
  title = {Improving {{Multi-Document Summarization}} through {{Referenced Flexible Extraction}} with {{Credit-Awareness}}},
  booktitle = {Proceedings of the 2022 {{Conference}} of the {{North American Chapter}} of the {{Association}} for {{Computational Linguistics}}: {{Human Language Technologies}}},
  author = {Song, Yun-Zhu and Chen, Yi-Syuan and Shuai, Hong-Han},
  year = {2022},
  pages = {1667--1681},
  publisher = {{Association for Computational Linguistics}},
  address = {{Seattle, United States}},
  doi = {10.18653/v1/2022.naacl-main.120},
  urldate = {2023-02-25},
  langid = {english},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\Improving Multi-Document Summarization through Referenced Flexible Extraction _Song et al 2022.pdf}
}

@article{song2022multilabel,
  title = {Multi-Label Legal Document Classification: {{A}} Deep Learning-Based Approach with Label-Attention and Domain-Specific Pre-Training},
  shorttitle = {Multi-Label Legal Document Classification},
  author = {Song, Dezhao and Vold, Andrew and Madan, Kanika and Schilder, Frank},
  year = {2022},
  month = may,
  journal = {Information Systems},
  volume = {106},
  pages = {101718},
  issn = {03064379},
  doi = {10.1016/j.is.2021.101718},
  urldate = {2022-08-31},
  langid = {english},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\Multi-label legal document classification _Song et al 2022.pdf}
}

@article{song2022pandemic,
  title = {Pandemic Policy Assessment by Artificial Intelligence},
  author = {Song, Sirui and Liu, Xue and Li, Yong and Yu, Yang},
  year = {2022},
  month = aug,
  journal = {Scientific Reports},
  volume = {12},
  number = {1},
  pages = {13843},
  issn = {2045-2322},
  doi = {10.1038/s41598-022-17892-8},
  urldate = {2022-08-25},
  abstract = {Abstract             Mobility-control policy is a controversial nonpharmacological approach to pandemic control due to its restriction on people's liberty and economic impacts. Due to the computational complexity of mobility control, it is challenging to assess or compare alternative policies. Here, we develop a pandemic policy assessment system that employs artificial intelligence (AI) to evaluate and analyze mobility-control policies. The system includes three components: (1) a general simulation framework that models different policies to comparable network-flow control problems; (2) a reinforcement-learning (RL) oracle to explore the upper-bound execution results of policies; and (3) comprehensive protocols for converting the RL results to policy-assessment measures, including execution complexity, effectiveness, cost and benefit, and risk. We applied the system to real-world metropolitan data and evaluated three popular policies: city lockdown, community quarantine, and route management. For each policy, we generated mobility-pandemic trade-off frontiers. The results manifest that the smartest policies, such as route management, have high execution complexity but limited additional gain from mobility retention. In contrast, a moderate-level intelligent policy such as community quarantine has acceptable execution complexity but can effectively suppress infections and largely mitigate mobility interventions. The frontiers also show one or two turning points, reflecting the safe threshold of mobility retention when considering policy-execution errors. In addition, we simulated different policy environments and found inspirations for the current policy debates on the zero-COVID policy, vaccination policy, and relaxing restrictions.},
  langid = {english},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\Pandemic policy assessment by artificial intelligence _Song et al 2022.pdf}
}

@article{song2022pandemica,
  title = {Pandemic Policy Assessment by Artificial Intelligence},
  author = {Song, Sirui and Liu, Xue and Li, Yong and Yu, Yang},
  year = {2022},
  month = aug,
  journal = {Scientific Reports},
  volume = {12},
  number = {1},
  pages = {13843},
  issn = {2045-2322},
  doi = {10.1038/s41598-022-17892-8},
  urldate = {2022-10-02},
  abstract = {Abstract             Mobility-control policy is a controversial nonpharmacological approach to pandemic control due to its restriction on people's liberty and economic impacts. Due to the computational complexity of mobility control, it is challenging to assess or compare alternative policies. Here, we develop a pandemic policy assessment system that employs artificial intelligence (AI) to evaluate and analyze mobility-control policies. The system includes three components: (1) a general simulation framework that models different policies to comparable network-flow control problems; (2) a reinforcement-learning (RL) oracle to explore the upper-bound execution results of policies; and (3) comprehensive protocols for converting the RL results to policy-assessment measures, including execution complexity, effectiveness, cost and benefit, and risk. We applied the system to real-world metropolitan data and evaluated three popular policies: city lockdown, community quarantine, and route management. For each policy, we generated mobility-pandemic trade-off frontiers. The results manifest that the smartest policies, such as route management, have high execution complexity but limited additional gain from mobility retention. In contrast, a moderate-level intelligent policy such as community quarantine has acceptable execution complexity but can effectively suppress infections and largely mitigate mobility interventions. The frontiers also show one or two turning points, reflecting the safe threshold of mobility retention when considering policy-execution errors. In addition, we simulated different policy environments and found inspirations for the current policy debates on the zero-COVID policy, vaccination policy, and relaxing restrictions.},
  langid = {english},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\Pandemic policy assessment by artificial intelligence _Song et al 22.pdf}
}

@inproceedings{springenberg2015striving,
  title = {Striving for Simplicity: {{The}} All Convolutional Net},
  booktitle = {{{ICLR}} (Workshop Track)},
  author = {Springenberg, J.T. and Dosovitskiy, A. and Brox, T. and Riedmiller, M.},
  year = {2015},
  keywords = {⛔ No DOI found},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\Striving for simplicity _Springenberg et al 22.pdf}
}

@inproceedings{stanovsky2018supervised,
  title = {Supervised {{Open Information Extraction}}},
  booktitle = {Proceedings of the 2018 {{Conference}} of the {{North American Chapter}} of           the {{Association}} for {{Computational Linguistics}}: {{Human Language}}           {{Technologies}}, {{Volume}} 1 ({{Long Papers}})},
  author = {Stanovsky, Gabriel and Michael, Julian and Zettlemoyer, Luke and Dagan, Ido},
  year = {2018},
  pages = {885--895},
  publisher = {{Association for Computational Linguistics}},
  address = {{New Orleans, Louisiana}},
  doi = {10.18653/v1/N18-1081},
  urldate = {2022-08-08},
  langid = {english},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\Supervised Open Information Extraction _Stanovsky et al 22.pdf}
}

@article{Stoica_Platanios_Poczos_2021,
  title = {Re-{{TACRED}}: {{Addressing}} Shortcomings of the {{TACRED}} Dataset},
  author = {Stoica, George and Platanios, Emmanouil Antonios and Poczos, Barnabas},
  year = {2021},
  month = may,
  journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
  volume = {35},
  number = {15},
  pages = {13843--13850},
  keywords = {⛔ No DOI found},
  annotation = {Abstract note: TACRED is one of the largest and most widely used sentence-level relation extraction datasets. Proposed models that are evaluated using this dataset consistently set new state-of-the-art performance. However, they still exhibit large error rates despite leveraging external knowledge and unsupervised pretraining on large text corpora. A recent study suggested that this may be due to poor dataset quality. The study observed that over 50\% of the most challenging sentences from the development and test sets are incorrectly labeled and account for an average drop of 8\% f1-score in model performance. However, this study was limited to a small biased sample of 5k (out of a total of 106k) sentences, substantially restricting the generalizability and broader implications of its findings. In this paper, we address these shortcomings by: (i) performing a comprehensive study over the whole TACRED dataset, (ii) proposing an improved crowdsourcing strategy and deploying it to re-annotate the whole dataset, and (iii) performing a thorough analysis to understand how correcting the TACRED annotations affects previously published results. After verification, we observed that 23.9\% of TACRED labels are incorrect. Moreover, evaluating several models on our revised dataset yields an average f1-score improvement of 14.3\% and helps uncover significant relationships between the different models (rather than simply offsetting or scaling their scores by a constant factor). Finally, aside from our analysis we also release Re-TACRED, a new completely re-annotated version of the TACRED dataset that can be used to perform reliable evaluation of relation extraction models.},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\Re-TACRED _Stoica et al 22.pdf}
}

@inproceedings{strickson2020legal,
  title = {Legal {{Judgement Prediction}} for {{UK Courts}}},
  booktitle = {Proceedings of the 2020 {{The}} 3rd {{International Conference}} on {{Information Science}} and {{System}}},
  author = {Strickson, Benjamin and De La Iglesia, Beatriz},
  year = {2020},
  month = mar,
  pages = {204--209},
  publisher = {{ACM}},
  address = {{Cambridge United Kingdom}},
  doi = {10.1145/3388176.3388183},
  urldate = {2022-07-21},
  isbn = {978-1-4503-7725-6},
  langid = {english},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\Legal Judgement Prediction for UK Courts _Strickson_De La Iglesia 22.pdf}
}

@inproceedings{su2022comparison,
  title = {A {{Comparison}} of {{Strategies}} for {{Source-Free Domain Adaptation}}},
  booktitle = {Proceedings of the 60th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}} ({{Volume}} 1: {{Long Papers}})},
  author = {Su, Xin and Zhao, Yiyun and Bethard, Steven},
  year = {2022},
  pages = {8352--8367},
  publisher = {{Association for Computational Linguistics}},
  address = {{Dublin, Ireland}},
  doi = {10.18653/v1/2022.acl-long.572},
  urldate = {2022-10-18},
  langid = {english},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\A Comparison of Strategies for Source-Free Domain Adaptation _Su et al 2022.pdf}
}

@article{sui2020joint,
  title = {Joint {{Entity}} and {{Relation Extraction}} with {{Set Prediction Networks}}},
  author = {Sui, Dianbo and Chen, Yubo and Liu, Kang and Zhao, Jun and Zeng, Xiangrong and Liu, Shengping},
  year = {2020},
  publisher = {{arXiv}},
  doi = {10.48550/ARXIV.2011.01675},
  urldate = {2022-04-04},
  abstract = {The joint entity and relation extraction task aims to extract all relational triples from a sentence. In essence, the relational triples contained in a sentence are unordered. However, previous seq2seq based models require to convert the set of triples into a sequence in the training phase. To break this bottleneck, we treat joint entity and relation extraction as a direct set prediction problem, so that the extraction model can get rid of the burden of predicting the order of multiple triples. To solve this set prediction problem, we propose networks featured by transformers with non-autoregressive parallel decoding. Unlike autoregressive approaches that generate triples one by one in a certain order, the proposed networks directly output the final set of triples in one shot. Furthermore, we also design a set-based loss that forces unique predictions via bipartite matching. Compared with cross-entropy loss that highly penalizes small shifts in triple order, the proposed bipartite matching loss is invariant to any permutation of predictions; thus, it can provide the proposed networks with a more accurate training signal by ignoring triple order and focusing on relation types and entities. Experiments on two benchmark datasets show that our proposed model significantly outperforms current state-of-the-art methods. Training code and trained models will be available at http://github.com/DianboWork/SPN4RE.},
  copyright = {arXiv.org perpetual, non-exclusive license},
  keywords = {Computation and Language (cs.CL),FOS: Computer and information sciences},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\Joint Entity and Relation Extraction with Set Prediction Networks _Sui et al 22.pdf}
}

@article{sun2020summarize,
  title = {Summarize, Outline, and Elaborate: {{Long-text}} Generation via Hierarchical Supervision from Extractive Summaries},
  author = {Sun, Xiaofei and Fan, Chun and Sun, Zijun and Meng, Yuxian and Wu, Fei and Li, Jiwei},
  year = {2020},
  journal = {arXiv preprint arXiv:2010.07074},
  eprint = {2010.07074},
  archiveprefix = {arxiv},
  keywords = {⛔ No DOI found}
}

@misc{sun2020testtime,
  title = {Test-Time Training for out-of-Distribution Generalization},
  author = {Sun, Yu and Wang, Xiaolong and Liu, Zhuang and Miller, John and Efros, Alexei A. and Hardt, Moritz},
  year = {2020},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\Test-time training for out-of-distribution generalization _Sun et al 2020.pdf}
}

@article{sun2023documentlevel,
  title = {Document-Level Relation Extraction with Two-Stage Dynamic Graph Attention Networks},
  author = {Sun, Qi and Zhang, Kun and Huang, Kun and Xu, Tiancheng and Li, Xun and Liu, Yaodi},
  year = {2023},
  month = may,
  journal = {Knowledge-Based Systems},
  volume = {267},
  pages = {110428},
  issn = {09507051},
  doi = {10.1016/j.knosys.2023.110428},
  urldate = {2023-03-16},
  langid = {english},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\Document-level relation extraction with two-stage dynamic graph attention _Sun et al 2023.pdf}
}

@inproceedings{sundararajan2017axiomatic,
  title = {Axiomatic Attribution for Deep Networks},
  booktitle = {Proceedings of the 34th International Conference on Machine Learning},
  author = {Sundararajan, Mukund and Taly, Ankur and Yan, Qiqi},
  editor = {Precup, Doina and Teh, Yee Whye},
  year = {2017},
  month = aug,
  series = {Proceedings of Machine Learning Research},
  volume = {70},
  pages = {3319--3328},
  publisher = {{PMLR}},
  abstract = {We study the problem of attributing the prediction of a deep network to its input features, a problem previously studied by several other works. We identify two fundamental axioms\textemdash Sensitivity and Implementation Invariance that attribution methods ought to satisfy. We show that they are not satisfied by most known attribution methods, which we consider to be a fundamental weakness of those methods. We use the axioms to guide the design of a new attribution method called Integrated Gradients. Our method requires no modification to the original network and is extremely simple to implement; it just needs a few calls to the standard gradient operator. We apply this method to a couple of image models, a couple of text models and a chemistry model, demonstrating its ability to debug networks, to extract rules from a network, and to enable users to engage with models better.},
  pdf = {http://proceedings.mlr.press/v70/sundararajan17a/sundararajan17a.pdf},
  keywords = {IG,reference-based},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\Axiomatic attribution for deep networks _Sundararajan et al 22.pdf}
}

@article{suterrobustly,
  title = {Robustly {{Disentangled Causal Mechanisms}}:  {{Validating Deep Representations}} for {{Interventional Robustness}}},
  author = {Suter, Raphael and Miladinovic, {\DH}ore and Sch{\"o}lkopf, Bernhard and Bauer, Stefan},
  pages = {10},
  abstract = {The ability to learn disentangled representations that split underlying sources of variation in high dimensional, unstructured data is important for data efficient and robust use of neural networks. While various approaches aiming towards this goal have been proposed in recent times, a commonly accepted definition and validation procedure is missing. We provide a causal perspective on representation learning which covers disentanglement and domain shift robustness as special cases. Our causal framework allows us to introduce a new metric for the quantitative evaluation of deep latent variable models. We show how this metric can be estimated from labeled observational data and further provide an efficient estimation algorithm that scales linearly in the dataset size.},
  langid = {english},
  keywords = {⛔ No DOI found},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\Robustly Disentangled Causal Mechanisms _Suter et al2.pdf}
}

@article{takanobu2019hierarchical,
  title = {A {{Hierarchical Framework}} for {{Relation Extraction}} with {{Reinforcement Learning}}},
  author = {Takanobu, Ryuichi and Zhang, Tianyang and Liu, Jiexi and Huang, Minlie},
  year = {2019},
  month = jul,
  journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
  volume = {33},
  number = {01},
  pages = {7072--7079},
  issn = {2374-3468, 2159-5399},
  doi = {10.1609/aaai.v33i01.33017072},
  urldate = {2023-02-20},
  abstract = {Most existing methods determine relation types only after all the entities have been recognized, thus the interaction between relation types and entity mentions is not fully modeled. This paper presents a novel paradigm to deal with relation extraction by regarding the related entities as the arguments of a relation. We apply a hierarchical reinforcement learning (HRL) framework in this paradigm to enhance the interaction between entity mentions and relation types. The whole extraction process is decomposed into a hierarchy of two-level RL policies for relation detection and entity extraction respectively, so that it is more feasible and natural to deal with overlapping relations. Our model was evaluated on public datasets collected via distant supervision, and results show that it gains better performance than existing methods and is more powerful for extracting overlapping relations1.},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\A Hierarchical Framework for Relation Extraction with Reinforcement Learning _Takanobu et al 2019.pdf}
}

@article{tan2021reliability,
  title = {Reliability {{Testing}} for {{Natural Language Processing Systems}}},
  author = {Tan, Samson and Joty, Shafiq and Baxter, Kathy and Taeihagh, Araz and Bennett, Gregory A. and Kan, Min-Yen},
  year = {2021},
  month = may,
  journal = {arXiv:2105.02590 [cs]},
  eprint = {2105.02590},
  primaryclass = {cs},
  urldate = {2022-03-13},
  abstract = {Questions of fairness, robustness, and transparency are paramount to address before deploying NLP systems. Central to these concerns is the question of reliability: Can NLP systems reliably treat different demographics fairly and function correctly in diverse and noisy environments? To address this, we argue for the need for reliability testing and contextualize it among existing work on improving accountability. We show how adversarial attacks can be reframed for this goal, via a framework for developing reliability tests. We argue that reliability testing \textemdash{} with an emphasis on interdisciplinary collaboration \textemdash{} will enable rigorous and targeted testing, and aid in the enactment and enforcement of industry standards.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {⛔ No DOI found,Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Computers and Society,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\Reliability Testing for Natural Language Processing Systems _Tan et al 22.pdf}
}

@article{tan2022documentlevel,
  title = {Document-{{Level Relation Extraction}} with {{Adaptive Focal Loss}} and {{Knowledge Distillation}}},
  author = {Tan, Qingyu and He, Ruidan and Bing, Lidong and Ng, Hwee Tou},
  year = {2022},
  month = mar,
  journal = {arXiv:2203.10900 [cs]},
  eprint = {2203.10900},
  primaryclass = {cs},
  urldate = {2022-03-31},
  abstract = {Document-level Relation Extraction (DocRE) is a more challenging task compared to its sentence-level counterpart. It aims to extract relations from multiple sentences at once. In this paper, we propose a semi-supervised framework for DocRE with three novel components. Firstly, we use an axial attention module for learning the interdependency among entity-pairs, which improves the performance on two-hop relations. Secondly, we propose an adaptive focal loss to tackle the class imbalance problem of DocRE. Lastly, we use knowledge distillation to overcome the differences between human annotated data and distantly supervised data. We conducted experiments on two DocRE datasets. Our model consistently outperforms strong baselines and its performance exceeds the previous SOTA by 1.36 F1 and 1.46 Ign\_F1 score on the DocRED leaderboard. Our code and data will be released at https://github.com/tonytan48/KD-DocRE.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {⛔ No DOI found,Computer Science - Computation and Language},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\Document-Level Relation Extraction with Adaptive Focal Loss and Knowledge _Tan et al 4.pdf}
}

@inproceedings{tan2022documentlevela,
  title = {Document-{{Level Relation Extraction}} with {{Adaptive Focal Loss}} and {{Knowledge Distillation}}},
  booktitle = {Findings of the {{Association}} for {{Computational Linguistics}}: {{ACL}} 2022},
  author = {Tan, Qingyu and He, Ruidan and Bing, Lidong and Ng, Hwee Tou},
  year = {2022},
  pages = {1672--1681},
  publisher = {{Association for Computational Linguistics}},
  address = {{Dublin, Ireland}},
  doi = {10.18653/v1/2022.findings-acl.132},
  urldate = {2022-07-14},
  langid = {english},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\Document-Level Relation Extraction with Adaptive Focal Loss and Knowledge _Tan et al 3.pdf}
}

@inproceedings{tan2022revisiting,
  title = {Revisiting {{DocRED}} - {{Addressing}} the {{False Negative Problem}} in {{Relation Extraction}}},
  booktitle = {Proceedings of the 2022 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}}},
  author = {Tan, Qingyu and Xu, Lu and Bing, Lidong and Ng, Hwee Tou and Aljunied, Sharifah Mahani},
  year = {2022},
  month = dec,
  pages = {8472--8487},
  publisher = {{Association for Computational Linguistics}},
  address = {{Abu Dhabi, United Arab Emirates}},
  abstract = {The DocRED dataset is one of the most popular and widely used benchmarks for document-level relation extraction (RE). It adopts a recommend-revise annotation scheme so as to have a large-scale annotated dataset. However, we find that the annotation of DocRED is incomplete, i.e., false negative samples are prevalent. We analyze the causes and effects of the overwhelming false negative problem in the DocRED dataset. To address the shortcoming, we re-annotate 4,053 documents in the DocRED dataset by adding the missed relation triples back to the original DocRED. We name our revised DocRED dataset Re-DocRED. We conduct extensive experiments with state-of-the-art neural models on both datasets, and the experimental results show that the models trained and evaluated on our Re-DocRED achieve performance improvements of around 13 F1 points. Moreover, we conduct a comprehensive analysis to identify the potential areas for further improvement.},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\Revisiting DocRED - Addressing the False Negative Problem in Relation Extraction _Tan et al 2022.pdf}
}

@inproceedings{tang2020hin,
  title = {Hin: {{Hierarchical}} Inference Network for Document-Level Relation Extraction},
  booktitle = {Pacific-{{Asia Conference}} on {{Knowledge Discovery}} and {{Data Mining}}},
  author = {Tang, Hengzhu and Cao, Yanan and Zhang, Zhenyu and Cao, Jiangxia and Fang, Fang and Wang, Shi and Yin, Pengfei},
  year = {2020},
  pages = {197--209},
  doi = {10.1007/978-3-030-47426-3_16},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\Hin _Tang et al 22.pdf}
}

@inproceedings{tang2020unbiased,
  title = {Unbiased {{Scene Graph Generation From Biased Training}}},
  booktitle = {2020 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Tang, Kaihua and Niu, Yulei and Huang, Jianqiang and Shi, Jiaxin and Zhang, Hanwang},
  year = {2020},
  month = jun,
  pages = {3713--3722},
  publisher = {{IEEE}},
  doi = {10.1109/CVPR42600.2020.00377},
  isbn = {978-1-72817-168-5},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\Unbiased Scene Graph Generation From Biased Training _Tang et al 22.pdf}
}

@article{tanginterpretable,
  title = {{{INTERPRETABLE NATURAL LANGUAGE PROCESSING WITH APPLICATIONS TO INFORMATION EXTRACTION}}},
  author = {Tang, Zheng},
  langid = {english},
  keywords = {⛔ No DOI found},
  file = {C\:\\Users\\Hytn\\Zotero\\storage\\5BX3GH9P\\Tang - INTERPRETABLE NATURAL LANGUAGE PROCESSING WITH APP.pdf}
}

@inproceedings{tanzer2022memorisation,
  title = {Memorisation versus {{Generalisation}} in {{Pre-trained Language Models}}},
  booktitle = {Proceedings of the 60th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}} ({{Volume}} 1: {{Long Papers}})},
  author = {T{\"a}nzer, Michael and Ruder, Sebastian and Rei, Marek},
  year = {2022},
  pages = {7564--7578},
  publisher = {{Association for Computational Linguistics}},
  address = {{Dublin, Ireland}},
  doi = {10.18653/v1/2022.acl-long.521},
  urldate = {2022-11-01},
  langid = {english},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\Memorisation versus Generalisation in Pre-trained Language Models _Tänzer et al 2022.pdf}
}

@inproceedings{tay2021long,
  title = {Long Range Arena : {{A}} Benchmark for Efficient Transformers},
  booktitle = {International Conference on Learning Representations},
  author = {Tay, Yi and Dehghani, Mostafa and Abnar, Samira and Shen, Yikang and Bahri, Dara and Pham, Philip and Rao, Jinfeng and Yang, Liu and Ruder, Sebastian and Metzler, Donald},
  year = {2021},
  keywords = {⛔ No DOI found},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\Long range arena _Tay et al 2021.pdf}
}

@inproceedings{Teney_2021_ICCV,
  title = {Unshuffling Data for Improved Generalization in Visual Question Answering},
  booktitle = {Proceedings of the {{IEEE}}/{{CVF}} International Conference on Computer Vision ({{ICCV}})},
  author = {Teney, Damien and Abbasnejad, Ehsan and {van den Hengel}, Anton},
  year = {2021},
  month = oct,
  pages = {1417--1427},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\Unshuffling data for improved generalization in visual question answering _Teney et al 22.pdf}
}

@inproceedings{tian2018deeptest,
  title = {{{DeepTest}}: Automated Testing of Deep-Neural-Network-Driven Autonomous Cars},
  shorttitle = {{{DeepTest}}},
  booktitle = {Proceedings of the 40th {{International Conference}} on {{Software Engineering}}},
  author = {Tian, Yuchi and Pei, Kexin and Jana, Suman and Ray, Baishakhi},
  year = {2018},
  month = may,
  pages = {303--314},
  publisher = {{ACM}},
  address = {{Gothenburg Sweden}},
  doi = {10.1145/3180155.3180220},
  urldate = {2022-11-23},
  isbn = {978-1-4503-5638-1},
  langid = {english},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\DeepTest _Tian et al 2018.pdf}
}

@misc{to2023better,
  title = {Better {{Language Models}} of {{Code}} through {{Self-Improvement}}},
  author = {To, Hung Quoc and Bui, Nghi D. Q. and Guo, Jin and Nguyen, Tien N.},
  year = {2023},
  month = apr,
  number = {arXiv:2304.01228},
  eprint = {arXiv:2304.01228},
  publisher = {{arXiv}},
  urldate = {2023-04-09},
  abstract = {Pre-trained language models for code (PLMCs) have gained attention in recent research. These models are pre-trained on large-scale datasets using multi-modal objectives. However, fine-tuning them requires extensive supervision and is limited by the size of the dataset provided. We aim to improve this issue by proposing a data augmentation framework using knowledge distillation. Our framework utilizes knowledge gained during the pre-training and fine-tuning stage to generate pseudo data, which is then used as training data for the next step. We incorporate this framework into the state-of-the-art language models, such as CodeT5, CodeBERT, and UnixCoder. The results show that our framework significantly improves PLMCs' performance in sequence-generation tasks, such as code summarization and code generation in the CodeXGLUE benchmark.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {C\:\\Users\\Hytn\\Zotero\\storage\\6NM4G98J\\To et al. - 2023 - Better Language Models of Code through Self-Improv.pdf}
}

@inproceedings{treviso2020explanation,
  title = {The {{Explanation Game}}: {{Towards Prediction Explainability}} through {{Sparse Communication}}},
  shorttitle = {The {{Explanation Game}}},
  booktitle = {Proceedings of the {{Third BlackboxNLP Workshop}} on {{Analyzing}} and {{Interpreting Neural Networks}} for {{NLP}}},
  author = {Treviso, Marcos and Martins, Andr{\'e} F. T.},
  year = {2020},
  pages = {107--118},
  publisher = {{Association for Computational Linguistics}},
  address = {{Online}},
  doi = {10.18653/v1/2020.blackboxnlp-1.10},
  urldate = {2022-11-01},
  langid = {english},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\The Explanation Game _Treviso_Martins 2020.pdf}
}

@inproceedings{tsai2019multimodal,
  title = {Multimodal {{Transformer}} for {{Unaligned Multimodal Language Sequences}}},
  booktitle = {Proceedings of the 57th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}}},
  author = {Tsai, Yao-Hung Hubert and Bai, Shaojie and Liang, Paul Pu and Kolter, J. Zico and Morency, Louis-Philippe and Salakhutdinov, Ruslan},
  year = {2019},
  pages = {6558--6569},
  publisher = {{Association for Computational Linguistics}},
  address = {{Florence, Italy}},
  doi = {10.18653/v1/P19-1656},
  urldate = {2022-10-03},
  langid = {english},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\Multimodal Transformer for Unaligned Multimodal Language Sequences _Tsai et al 2019.pdf}
}

@inproceedings{tu2019multihop,
  title = {Multi-Hop {{Reading Comprehension}} across {{Multiple Documents}} by {{Reasoning}} over {{Heterogeneous Graphs}}.},
  booktitle = {Proceedings of the 57th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}}},
  author = {Tu, Ming and Wang, Guangtao and Huang, Jing and Tang, Yun and He, Xiaodong and Zhou, Bowen},
  year = {2019},
  pages = {2704--2713},
  doi = {10.18653/v1/P19-1260},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\Multi-hop Reading Comprehension across Multiple Documents by Reasoning over _Tu et al 22.pdf}
}

@article{tu2020empirical,
  title = {An {{Empirical Study}} on {{Robustness}} to {{Spurious Correlations}} Using {{Pre-trained Language Models}}},
  author = {Tu, Lifu and Lalwani, Garima and Gella, Spandana and He, He},
  year = {2020},
  month = dec,
  journal = {Transactions of the Association for Computational Linguistics},
  volume = {8},
  pages = {621--633},
  issn = {2307-387X},
  doi = {10.1162/tacl-a-00335},
  urldate = {2022-07-29},
  abstract = {Recent work has shown that pre-trained language models such as BERT improve robustness to spurious correlations in the dataset. Intrigued by these results, we find that the key to their success is generalization from a small amount of counterexamples where the spurious correlations do not hold. When such minority examples are scarce, pre-trained models perform as poorly as models trained from scratch. In the case of extreme minority, we propose to use multi-task learning (MTL) to improve generalization. Our experiments on natural language inference and paraphrase identification show that MTL with the right auxiliary tasks significantly improves performance on challenging examples without hurting the in-distribution performance. Further, we show that the gain from MTL mainly comes from improved generalization from the minority examples. Our results highlight the importance of data diversity for overcoming spurious correlations.               1},
  langid = {english},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\An Empirical Study on Robustness to Spurious Correlations using Pre-trained _Tu et al 22.pdf}
}

@inproceedings{tuggener2020ledgar,
  title = {{{LEDGAR}}: {{A}} Large-Scale Multi-Label Corpus for Text Classification of Legal Provisions in Contracts},
  booktitle = {Proceedings of the 12th Language Resources and Evaluation Conference},
  author = {Tuggener, Don and {von D{\"a}niken}, Pius and Peetz, Thomas and Cieliebak, Mark},
  year = {2020},
  month = may,
  pages = {1235--1241},
  publisher = {{European Language Resources Association}},
  address = {{Marseille, France}},
  abstract = {We present LEDGAR, a multilabel corpus of legal provisions in contracts. The corpus was crawled and scraped from the public domain (SEC filings) and is, to the best of our knowledge, the first freely available corpus of its kind. Since the corpus was constructed semi-automatically, we apply and discuss various approaches to noise removal. Due to the rather large labelset of over 12'000 labels annotated in almost 100'000 provisions in over 60'000 contracts, we believe the corpus to be of interest for research in the field of Legal NLP, (large-scale or extreme) text classification, as well as for legal studies. We discuss several methods to sample subcopora from the corpus and implement and evaluate different automatic classification approaches. Finally, we perform transfer experiments to evaluate how well the classifiers perform on contracts stemming from outside the corpus.},
  isbn = {979-10-95546-34-4},
  langid = {english},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\LEDGAR _Tuggener et al 22.pdf}
}

@article{uc-cetina2023survey,
  title = {Survey on Reinforcement Learning for Language Processing},
  author = {{Uc-Cetina}, V{\'i}ctor and {Navarro-Guerrero}, Nicol{\'a}s and {Martin-Gonzalez}, Anabel and Weber, Cornelius and Wermter, Stefan},
  year = {2023},
  month = feb,
  journal = {Artificial Intelligence Review},
  volume = {56},
  number = {2},
  pages = {1543--1575},
  issn = {0269-2821, 1573-7462},
  doi = {10.1007/s10462-022-10205-5},
  urldate = {2023-02-14},
  abstract = {Abstract             In recent years some researchers have explored the use of reinforcement learning (RL) algorithms as key components in the solution of various natural language processing (NLP) tasks. For instance, some of these algorithms leveraging deep neural learning have found their way into conversational systems. This paper reviews the state of the art of RL methods for their possible use for different problems of NLP, focusing primarily on conversational systems, mainly due to their growing relevance. We provide detailed descriptions of the problems as well as discussions of why RL is well-suited to solve them. Also, we analyze the advantages and limitations of these methods. Finally, we elaborate on promising research directions in NLP that might benefit from RL.},
  langid = {english},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\Survey on reinforcement learning for language processing _Uc-Cetina et al 2023.pdf}
}

@inproceedings{vapnik1991principles,
  title = {Principles of Risk Minimization for Learning Theory},
  booktitle = {Advances in Neural Information Processing Systems},
  author = {Vapnik, V.},
  editor = {Moody, J. and Hanson, S. and Lippmann, R.P.},
  year = {1991},
  volume = {4},
  publisher = {{Morgan-Kaufmann}},
  keywords = {⛔ No DOI found},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\Principles of risk minimization for learning theory _Vapnik 1991.pdf}
}

@article{vapnik1999overview,
  title = {An Overview of Statistical Learning Theory},
  author = {Vapnik, V.N.},
  year = {Sept./1999},
  journal = {IEEE Transactions on Neural Networks},
  volume = {10},
  number = {5},
  pages = {988--999},
  issn = {10459227},
  doi = {10.1109/72.788640},
  urldate = {2022-09-19},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\An overview of statistical learning theory _Vapnik 1999.pdf}
}

@inproceedings{varshney2022investigating,
  title = {Investigating {{Selective Prediction Approaches Across Several Tasks}} in {{IID}}, {{OOD}}, and {{Adversarial Settings}}},
  booktitle = {Findings of the {{Association}} for {{Computational Linguistics}}: {{ACL}} 2022},
  author = {Varshney, Neeraj and Mishra, Swaroop and Baral, Chitta},
  year = {2022},
  pages = {1995--2002},
  publisher = {{Association for Computational Linguistics}},
  address = {{Dublin, Ireland}},
  doi = {10.18653/v1/2022.findings-acl.158},
  urldate = {2022-10-18},
  langid = {english},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\Investigating Selective Prediction Approaches Across Several Tasks in IID, OOD, _Varshney et al 2022.pdf}
}

@inproceedings{vaswani2017attention,
  title = {Attention Is {{All}} You {{Need}}},
  booktitle = {Proceedings of the 31st {{International Conference}} on {{Neural Information Processing Systems}}},
  author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, Lukasz and Polosukhin, Illia},
  year = {2017},
  volume = {30},
  pages = {5998--6008},
  keywords = {⛔ No DOI found},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\Attention is All you Need _Vaswani et al 22.pdf}
}

@article{veitch2021counterfactual,
  title = {Counterfactual Invariance to Spurious Correlations in Text Classification},
  author = {Veitch, Victor and D'Amour, Alexander and Yadlowsky, Steve and Eisenstein, Jacob},
  year = {2021},
  journal = {Advances in Neural Information Processing Systems},
  volume = {34},
  keywords = {⛔ No DOI found},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\Counterfactual invariance to spurious correlations in text classification _Veitch et al 22.pdf}
}

@article{velickovic2017graph,
  title = {Graph {{Attention Networks}}},
  author = {Veli{\v c}kovi{\'c}, Petar and Cucurull, Guillem and Casanova, Arantxa and Romero, Adriana and Li{\`o}, Pietro and Bengio, Yoshua},
  year = {2017},
  publisher = {{arXiv}},
  doi = {10.48550/ARXIV.1710.10903},
  urldate = {2022-03-24},
  abstract = {We present graph attention networks (GATs), novel neural network architectures that operate on graph-structured data, leveraging masked self-attentional layers to address the shortcomings of prior methods based on graph convolutions or their approximations. By stacking layers in which nodes are able to attend over their neighborhoods' features, we enable (implicitly) specifying different weights to different nodes in a neighborhood, without requiring any kind of costly matrix operation (such as inversion) or depending on knowing the graph structure upfront. In this way, we address several key challenges of spectral-based graph neural networks simultaneously, and make our model readily applicable to inductive as well as transductive problems. Our GAT models have achieved or matched state-of-the-art results across four established transductive and inductive graph benchmarks: the Cora, Citeseer and Pubmed citation network datasets, as well as a protein-protein interaction dataset (wherein test graphs remain unseen during training).},
  copyright = {arXiv.org perpetual, non-exclusive license},
  keywords = {Artificial Intelligence (cs.AI),FOS: Computer and information sciences,Machine Learning (cs.LG),Machine Learning (stat.ML),Social and Information Networks (cs.SI)},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\Graph Attention Networks _Veličković et al 22.pdf}
}

@inproceedings{velickovic2018graph,
  title = {Graph {{Attention Networks}}},
  booktitle = {International {{Conference}} on {{Learning Representations}}},
  author = {Veli{\v c}kovi{\'c}, Petar and Cucurull, Guillem and Casanova, Arantxa and Romero, Adriana and Li{\`o}, Pietro and Bengio, Yoshua},
  year = {2018},
  keywords = {⛔ No DOI found},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\Graph Attention Networks _Veličković et al 2018.pdf}
}

@inproceedings{verga2018simultaneously,
  title = {Simultaneously {{Self-Attending}} to {{All Mentions}} for {{Full-Abstract Biological Relation Extraction}}},
  booktitle = {Proceedings of the 2018 {{Conference}} of the {{North American Chapter}} of the {{Association}} for {{Computational Linguistics}}: {{Human Language Technologies}}, {{Volume}} 1 ({{Long Papers}})},
  author = {Verga, Patrick and Strubell, Emma and McCallum, Andrew},
  year = {2018},
  volume = {1},
  pages = {872--884},
  doi = {10.18653/v1/N18-1080},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\Simultaneously Self-Attending to All Mentions for Full-Abstract Biological _Verga et al 22.pdf}
}

@article{vig2020investigating,
  title = {Investigating Gender Bias in Language Models Using Causal Mediation Analysis},
  author = {Vig, Jesse and Gehrmann, Sebastian and Belinkov, Yonatan and Qian, Sharon and Nevo, Daniel and Singer, Yaron and Shieber, Stuart},
  year = {2020},
  journal = {Advances in Neural Information Processing Systems},
  volume = {33},
  pages = {12388--12401},
  keywords = {⛔ No DOI found},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\Investigating gender bias in language models using causal mediation analysis _Vig et al 22.pdf}
}

@article{vrandecic2014wikidata,
  title = {Wikidata: A Free Collaborative Knowledgebase},
  shorttitle = {Wikidata},
  author = {Vrande{\v c}i{\'c}, Denny and Kr{\"o}tzsch, Markus},
  year = {2014},
  month = sep,
  journal = {Communications of the ACM},
  volume = {57},
  number = {10},
  pages = {78--85},
  issn = {0001-0782, 1557-7317},
  doi = {10.1145/2629489},
  urldate = {2022-10-07},
  abstract = {This collaboratively edited knowledgebase provides a common source of data for Wikipedia, and everyone else.},
  langid = {english},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\Wikidata _Vrandečić_Krötzsch 2014.pdf}
}

@article{walker2006ace,
  title = {{{ACE}} 2005 Multilingual Training Corpus},
  author = {Walker, Christopher and Strassel, Stephanie and Medero, Julie and Maeda, Kazuaki},
  year = {2006},
  journal = {Linguistic Data Consortium, Philadelphia},
  volume = {57},
  pages = {45},
  keywords = {⛔ No DOI found}
}

@inproceedings{wallace2019allennlp,
  title = {{{AllenNLP Interpret}}: {{A Framework}} for {{Explaining Predictions}} of {{NLP Models}}},
  shorttitle = {{{AllenNLP Interpret}}},
  booktitle = {Proceedings of the 2019 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}} and the 9th {{International Joint Conference}} on {{Natural Language Processing}} ({{EMNLP-IJCNLP}}): {{System Demonstrations}}},
  author = {Wallace, Eric and Tuyls, Jens and Wang, Junlin and Subramanian, Sanjay and Gardner, Matt and Singh, Sameer},
  year = {2019},
  pages = {7--12},
  publisher = {{Association for Computational Linguistics}},
  address = {{Hong Kong, China}},
  doi = {10.18653/v1/D19-3002},
  urldate = {2022-07-13},
  langid = {english},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\AllenNLP Interpret _Wallace et al 22.pdf}
}

@inproceedings{wan2006improved,
  title = {Improved Affinity Graph Based Multi-Document Summarization},
  booktitle = {Proceedings of the Human Language Technology Conference of the {{NAACL}}, {{Companion}} Volume: {{Short}} Papers},
  author = {Wan, Xiaojun and Yang, Jianwu},
  year = {2006},
  pages = {181--184},
  keywords = {⛔ No DOI found}
}

@inproceedings{wan2008multidocument,
  title = {Multi-Document Summarization Using Cluster-Based Link Analysis},
  booktitle = {Proceedings of the 31st Annual International {{ACM SIGIR}} Conference on {{Research}} and Development in Information Retrieval - {{SIGIR}} '08},
  author = {Wan, Xiaojun and Yang, Jianwu},
  year = {2008},
  pages = {299},
  publisher = {{ACM Press}},
  address = {{Singapore, Singapore}},
  doi = {10.1145/1390334.1390386},
  urldate = {2022-08-30},
  isbn = {978-1-60558-164-4},
  langid = {english},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\Multi-document summarization using cluster-based link analysis _Wan_Yang 2008.pdf}
}

@inproceedings{Wang_2021_ICCV,
  title = {Causal Attention for Unbiased Visual Recognition},
  booktitle = {Proceedings of the {{IEEE}}/{{CVF}} International Conference on Computer Vision ({{ICCV}})},
  author = {Wang, Tan and Zhou, Chang and Sun, Qianru and Zhang, Hanwang},
  year = {2021},
  month = oct,
  pages = {3091--3100},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\Causal attention for unbiased visual recognition _Wang et al 22.pdf}
}

@inproceedings{wang-etal-2022-entity,
  title = {Entity-Centered Cross-Document Relation Extraction},
  booktitle = {Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing},
  author = {Wang, Fengqi and Li, Fei and Fei, Hao and Li, Jingye and Wu, Shengqiong and Su, Fangfang and Shi, Wenxuan and Ji, Donghong and Cai, Bo},
  year = {2022},
  month = dec,
  pages = {9871--9881},
  publisher = {{Association for Computational Linguistics}},
  address = {{Abu Dhabi, United Arab Emirates}},
  abstract = {Relation Extraction (RE) is a fundamental task of information extraction, which has attracted a large amount of research attention. Previous studies focus on extracting the relations within a sentence or document, while currently researchers begin to explore cross-document RE. However, current cross-document RE methods directly utilize text snippets surrounding target entities in multiple given documents, which brings considerable noisy and non-relevant sentences. Moreover, they utilize all the text paths in a document bag in a coarse-grained way, without considering the connections between these text paths.In this paper, we aim to address both of these shortages and push the state-of-the-art for cross-document RE. First, we focus on input construction for our RE model and propose an entity-based document-context filter to retain useful information in the given documents by using the bridge entities in the text paths. Second, we propose a cross-document RE model based on cross-path entity relation attention, which allow the entity relations across text paths to interact with each other. We compare our cross-document RE method with the state-of-the-art methods in the dataset CodRED. Our method outperforms them by at least 10\% in F1, thus demonstrating its effectiveness.},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\Entity-centered cross-document relation extraction _Wang et al 2022.pdf}
}

@inproceedings{wang2016recursive,
  title = {Recursive Neural Conditional Random Fields for Aspect-Based Sentiment Analysis},
  booktitle = {Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing},
  author = {Wang, Wenya and Pan, Sinno Jialin and Dahlmeier, Daniel and Xiao, Xiaokui},
  year = {2016},
  pages = {616--626},
  doi = {10.18653/v1/D16-1059},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\Recursive neural conditional random fields for aspect-based sentiment analysis _Wang et al 2016.pdf}
}

@inproceedings{wang2016relation,
  title = {Relation Classification via Multi-Level Attention Cnns},
  booktitle = {Proceedings of the 54th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}} ({{Volume}} 1: {{Long Papers}})},
  author = {Wang, Linlin and Cao, Zhu and De Melo, Gerard and Liu, Zhiyuan},
  year = {2016},
  pages = {1298--1307},
  doi = {10.18653/v1/P16-1123},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\Relation classification via multi-level attention cnns _Wang et al 22.pdf}
}

@inproceedings{wang2018glue,
  title = {{{GLUE}}: {{A Multi-Task Benchmark}} and {{Analysis Platform}} for {{Natural Language Understanding}}},
  shorttitle = {{{GLUE}}},
  booktitle = {Proceedings of the 2018 {{EMNLP Workshop BlackboxNLP}}: {{Analyzing}} and {{Interpreting Neural Networks}} for {{NLP}}},
  author = {Wang, Alex and Singh, Amanpreet and Michael, Julian and Hill, Felix and Levy, Omer and Bowman, Samuel},
  year = {2018},
  pages = {353--355},
  publisher = {{Association for Computational Linguistics}},
  address = {{Brussels, Belgium}},
  doi = {10.18653/v1/W18-5446},
  urldate = {2022-11-01},
  langid = {english},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\GLUE _Wang et al 2018.pdf}
}

@inproceedings{wang2018modeling,
  title = {Modeling {{Dynamic Pairwise Attention}} for {{Crime Classification}} over {{Legal Articles}}},
  booktitle = {The 41st {{International ACM SIGIR Conference}} on {{Research}} \& {{Development}} in {{Information Retrieval}}},
  author = {Wang, Pengfei and Yang, Ze and Niu, Shuzi and Zhang, Yongfeng and Zhang, Lei and Niu, ShaoZhang},
  year = {2018},
  month = jun,
  pages = {485--494},
  publisher = {{ACM}},
  address = {{Ann Arbor MI USA}},
  doi = {10.1145/3209978.3210057},
  urldate = {2022-08-12},
  isbn = {978-1-4503-5657-2},
  langid = {english},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\Modeling Dynamic Pairwise Attention for Crime Classification over Legal Articles _Wang et al 22.pdf}
}

@inproceedings{wang2019extracting,
  title = {Extracting {{Multiple-Relations}} in {{One-Pass}} with {{Pre-Trained Transformers}}},
  booktitle = {Proceedings of the 57th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}}},
  author = {Wang, Haoyu and Tan, Ming and Yu, Mo and Chang, Shiyu and Wang, Dakuo and Xu, Kun and Guo, Xiaoxiao and Potdar, Saloni},
  year = {2019},
  pages = {1371--1377},
  doi = {10.18653/v1/P19-1132},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\Extracting Multiple-Relations in One-Pass with Pre-Trained Transformers _Wang et al 22.pdf}
}

@article{wang2019finetune,
  title = {Fine-Tune {{Bert}} for {{DocRED}} with {{Two-step Process}}.},
  author = {Wang, Hong and Focke, Christfried and Sylvester, Rob and Mishra, Nilesh and Wang, William Yang},
  year = {2019},
  journal = {arXiv preprint arXiv:1909.11898},
  eprint = {1909.11898},
  archiveprefix = {arxiv},
  keywords = {⛔ No DOI found}
}

@article{wang2019improving,
  title = {Improving {{Natural Language Inference Using External Knowledge}} in the {{Science Questions Domain}}},
  author = {Wang, Xiaoyan and Kapanipathi, Pavan and Musa, Ryan and Yu, Mo and Talamadupula, Kartik and Abdelaziz, Ibrahim and Chang, Maria and Fokoue, Achille and Makni, Bassem and Mattei, Nicholas and Witbrock, Michael},
  year = {2019},
  month = jul,
  journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
  volume = {33},
  pages = {7208--7215},
  issn = {2374-3468, 2159-5399},
  doi = {10.1609/aaai.v33i01.33017208},
  urldate = {2022-03-07},
  abstract = {Natural Language Inference (NLI) is fundamental to many Natural Language Processing (NLP) applications including semantic search and question answering. The NLI problem has gained significant attention due to the release of large scale, challenging datasets. Present approaches to the problem largely focus on learning-based methods that use only textual information in order to classify whether a given premise entails, contradicts, or is neutral with respect to a given hypothesis. Surprisingly, the use of methods based on structured knowledge \textendash{} a central topic in artificial intelligence \textendash{} has not received much attention vis-a-vis the NLI problem. While there are many open knowledge bases that contain various types of reasoning information, their use for NLI has not been well explored. To address this, we present a combination of techniques that harness external knowledge to improve performance on the NLI problem in the science questions domain. We present the results of applying our techniques on text, graph, and text-and-graph based models; and discuss the implications of using external knowledge to solve the NLI problem. Our model achieves close to state-of-the-art performance for NLI on the SciTail science questions dataset.},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\Improving Natural Language Inference Using External Knowledge in the Science _Wang et al 22.pdf}
}

@inproceedings{wang2020heterogeneous,
  title = {Heterogeneous {{Graph Neural Networks}} for {{Extractive Document Summarization}}},
  booktitle = {Proceedings of the 58th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}}},
  author = {Wang, Danqing and Liu, Pengfei and Zheng, Yining and Qiu, Xipeng and Huang, Xuanjing},
  year = {2020},
  pages = {6209--6219},
  publisher = {{Association for Computational Linguistics}},
  address = {{Online}},
  doi = {10.18653/v1/2020.acl-main.553},
  urldate = {2023-04-02},
  langid = {english},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\Heterogeneous Graph Neural Networks for Extractive Document Summarization _Wang et al 2020.pdf}
}

@inproceedings{wang2020identifying,
  title = {Identifying {{Spurious Correlations}} for {{Robust Text Classification}}},
  booktitle = {Findings of the {{Association}} for {{Computational Linguistics}}: {{EMNLP}} 2020},
  author = {Wang, Zhao and Culotta, Aron},
  year = {2020},
  pages = {3431--3440},
  publisher = {{Association for Computational Linguistics}},
  address = {{Online}},
  doi = {10.18653/v1/2020.findings-emnlp.308},
  urldate = {2022-07-29},
  langid = {english},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\Identifying Spurious Correlations for Robust Text Classification _Wang_Culotta 22.pdf}
}

@inproceedings{wang2020tplinker,
  title = {{{TPLinker}}: {{Single-stage Joint Extraction}} of {{Entities}} and {{Relations Through Token Pair Linking}}},
  booktitle = {Proceedings of the 28th {{International Conference}} on {{Computational Linguistics}}},
  author = {Wang, Yucheng and Yu, Bowen and Zhang, Yueyang and Liu, Tingwen and Zhu, Hongsong and Sun, Limin},
  year = {2020},
  pages = {1572--1582},
  doi = {10.18653/v1/2020.coling-main.138},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\TPLinker _Wang et al 3.pdf;C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\TPLinker _Wang et al 4.pdf}
}

@inproceedings{wang2020visual,
  title = {Visual {{Commonsense R-CNN}}},
  booktitle = {2020 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Wang, Tan and Huang, Jianqiang and Zhang, Hanwang and Sun, Qianru},
  year = {2020},
  month = jun,
  pages = {10757--10767},
  publisher = {{IEEE}},
  doi = {10.1109/CVPR42600.2020.01077},
  isbn = {978-1-72817-168-5},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\Visual Commonsense R-CNN _Wang et al 22.pdf}
}

@misc{wang2021equality,
  title = {Equality before the {{Law}}: {{Legal Judgment Consistency Analysis}} for {{Fairness}}},
  shorttitle = {Equality before the {{Law}}},
  author = {Wang, Yuzhong and Xiao, Chaojun and Ma, Shirong and Zhong, Haoxi and Tu, Cunchao and Zhang, Tianyang and Liu, Zhiyuan and Sun, Maosong},
  year = {2021},
  month = mar,
  number = {arXiv:2103.13868},
  eprint = {arXiv:2103.13868},
  publisher = {{arXiv}},
  urldate = {2022-07-21},
  abstract = {In a legal system, judgment consistency is regarded as one of the most important manifestations of fairness. However, due to the complexity of factual elements that impact sentencing in real-world scenarios, few works have been done on quantitatively measuring judgment consistency towards real-world data. In this paper, we propose an evaluation metric for judgment inconsistency, Legal Inconsistency Coefficient (LInCo), which aims to evaluate inconsistency between data groups divided by specific features (e.g., gender, region, race). We propose to simulate judges from different groups with legal judgment prediction (LJP) models and measure the judicial inconsistency with the disagreement of the judgment results given by LJP models trained on different groups. Experimental results on the synthetic data verify the effectiveness of LInCo. We further employ LInCo to explore the inconsistency in real cases and come to the following observations: (1) Both regional and gender inconsistency exist in the legal system, but gender inconsistency is much less than regional inconsistency; (2) The level of regional inconsistency varies little across different time periods; (3) In general, judicial inconsistency is negatively correlated with the severity of the criminal charges. Besides, we use LInCo to evaluate the performance of several de-bias methods, such as adversarial learning, and find that these mechanisms can effectively help LJP models to avoid suffering from data bias.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\Equality before the Law _Wang et al 22.pdf}
}

@inproceedings{wang2021ilg,
  title = {{{ILG}}:{{Inference}} Model Based on {{Line Graphs}} for Document-Level Relation Extraction},
  shorttitle = {{{ILG}}},
  booktitle = {2021 4th {{International Conference}} on {{Algorithms}}, {{Computing}} and {{Artificial Intelligence}}},
  author = {Wang, Caihong and Xu, Xinyue and Zhao, Sicong and Cao, Yang and Lyu, Naibing and Jia, Shuainan and Peng, Yuan and Li, Dongxue},
  year = {2021},
  month = dec,
  pages = {1--6},
  publisher = {{ACM}},
  address = {{Sanya China}},
  doi = {10.1145/3508546.3508633},
  urldate = {2022-07-14},
  isbn = {978-1-4503-8505-3},
  langid = {english},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\ILG _Wang et al 22.pdf}
}

@article{wang2021kepler,
  title = {{{KEPLER}}: {{A Unified Model}} for {{Knowledge Embedding}} and {{Pre-trained Language Representation}}},
  author = {Wang, Xiaozhi and Gao, Tianyu and Zhu, Zhaocheng and Zhang, Zhengyan and Liu, Zhiyuan and Li, Juanzi and Tang, Jian},
  year = {2021},
  journal = {Transactions of the Association for Computational Linguistics},
  volume = {9},
  pages = {176--194},
  doi = {10.1162/tacl-a-00360},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\KEPLER _Wang et al 22.pdf}
}

@inproceedings{wang2022causal,
  title = {Causal {{Representation Learning}} for {{Out-of-Distribution Recommendation}}},
  booktitle = {Proceedings of the {{ACM Web Conference}} 2022},
  author = {Wang, Wenjie and Lin, Xinyu and Feng, Fuli and He, Xiangnan and Lin, Min and Chua, Tat-Seng},
  year = {2022},
  month = apr,
  pages = {3562--3571},
  publisher = {{ACM}},
  address = {{Virtual Event, Lyon France}},
  doi = {10.1145/3485447.3512251},
  urldate = {2022-05-14},
  isbn = {978-1-4503-9096-5},
  langid = {english},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\Causal Representation Learning for Out-of-Distribution Recommendation _Wang et al 22.pdf}
}

@article{wang2022deep,
  title = {Deep {{Reinforcement Learning}}: {{A Survey}}},
  shorttitle = {Deep {{Reinforcement Learning}}},
  author = {Wang, Xu and Wang, Sen and Liang, Xingxing and Zhao, Dawei and Huang, Jincai and Xu, Xin and Dai, Bin and Miao, Qiguang},
  year = {2022},
  journal = {IEEE Transactions on Neural Networks and Learning Systems},
  pages = {1--15},
  issn = {2162-237X, 2162-2388},
  doi = {10.1109/TNNLS.2022.3207346},
  urldate = {2023-02-14}
}

@inproceedings{wang2022deepstruct,
  title = {{{DeepStruct}}: {{Pretraining}} of {{Language Models}} for {{Structure Prediction}}},
  shorttitle = {{{DeepStruct}}},
  booktitle = {Findings of the {{Association}} for {{Computational Linguistics}}: {{ACL}} 2022},
  author = {Wang, Chenguang and Liu, Xiao and Chen, Zui and Hong, Haoyun and Tang, Jie and Song, Dawn},
  year = {2022},
  pages = {803--823},
  publisher = {{Association for Computational Linguistics}},
  address = {{Dublin, Ireland}},
  doi = {10.18653/v1/2022.findings-acl.67},
  urldate = {2022-11-04},
  langid = {english},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\DeepStruct _Wang et al 2022.pdf}
}

@article{wang2022diaa,
  title = {{{DI-AA}}: {{An}} Interpretable White-Box Attack for Fooling Deep Neural Networks},
  shorttitle = {{{DI-AA}}},
  author = {Wang, Yixiang and Liu, Jiqiang and Chang, Xiaolin and Rodr{\'i}guez, Ricardo J. and Wang, Jianhua},
  year = {2022},
  month = sep,
  journal = {Information Sciences},
  volume = {610},
  pages = {14--32},
  issn = {00200255},
  doi = {10.1016/j.ins.2022.07.157},
  urldate = {2022-11-22},
  langid = {english},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\DI-AA _Wang et al 2022.pdf}
}

@inproceedings{wang2022identifying,
  title = {Identifying and Mitigating Spurious Correlations for Improving Robustness in {{NLP}} Models},
  booktitle = {Findings of the Association for Computational Linguistics: {{NAACL}} 2022},
  author = {Wang, Tianlu and Sridhar, Rohit and Yang, Diyi and Wang, Xuezhi},
  year = {2022},
  month = jul,
  pages = {1719--1729},
  publisher = {{Association for Computational Linguistics}},
  address = {{Seattle, United States}},
  abstract = {Recently, NLP models have achieved remarkable progress across a variety of tasks; however, they have also been criticized for being not robust. Many robustness problems can be attributed to models exploiting ``spurious correlations'', or ``shortcuts'' between the training data and the task labels. Most existing work identifies a limited set of task-specific shortcuts via human priors or error analyses, which requires extensive expertise and efforts. In this paper, we aim to automatically identify such spurious correlations in NLP models at scale. We first leverage existing interpretability methods to extract tokens that significantly affect model's decision process from the input text. We then distinguish ``genuine'' tokens and ``spurious'' tokens by analyzing model predictions across multiple corpora and further verify them through knowledge-aware perturbations. We show that our proposed method can effectively and efficiently identify a scalable set of ``shortcuts'', and mitigating these leads to more robust models in multiple applications.},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\Identifying and mitigating spurious correlations for improving robustness in _Wang et al 22.pdf}
}

@article{wang2022interpretable,
  title = {Interpretable Prison Term Prediction with Reinforce Learning and Attention},
  author = {Wang, Peipeng and Zhang, Xiuguo and Yu, Han and Cao, Zhiying},
  year = {2022},
  month = apr,
  journal = {Applied Intelligence},
  issn = {0924-669X, 1573-7497},
  doi = {10.1007/s10489-022-03675-1},
  urldate = {2022-07-21},
  abstract = {The task of prison term prediction is to predict the term of penalty based on the charge and the seriousness of the sentencing plot. Most existing methods focus on improving prediction accuracy but disregard interpretability, which yields unreliable judgment results. To address this problem, we propose an interpretable prison term prediction method. First, the prison term is divided into intervals according to the charge and sentencing plot. Second, we propose a reinforcement learning principle representation model combined with an attention mechanism for regression prediction (PRRP), which extracts phrase-level principles representation as the explanatory basis of prediction results, uses the principle in conjunction with the charge semantics to predict the interval value, and extracts the interval keywords as the sentencing plot. Third, we design a novel multiangle attention mechanism to capture the distinguishing features of cases from different aspects, and a feature fusion network is employed to more effectively stitch multiple pieces of information to learn the feature-enhanced fact representation. Last, the feature-enhanced fact representation is used to predict the prison term. Experimental results on real-work datasets show the interpretability and effectiveness of our method.},
  langid = {english},
  keywords = {CAIL},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\Interpretable prison term prediction with reinforce learning and attention _Wang et al 22.pdf}
}

@article{wang2022iwa,
  title = {{{IWA}}: {{Integrated}} Gradient-based White-box Attacks for Fooling Deep Neural Networks},
  shorttitle = {{{IWA}}},
  author = {Wang, Yixiang and Liu, Jiqiang and Chang, Xiaolin and Mi{\v s}i{\'c}, Jelena and Mi{\v s}i{\'c}, Vojislav B.},
  year = {2022},
  month = jul,
  journal = {International Journal of Intelligent Systems},
  volume = {37},
  number = {7},
  pages = {4253--4276},
  issn = {0884-8173, 1098-111X},
  doi = {10.1002/int.22720},
  urldate = {2022-11-22},
  langid = {english},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\IWA _Wang et al 2022.pdf}
}

@inproceedings{wang2022learning,
  title = {Toward {{Learning Robust}} and {{Invariant Representations}} with {{Alignment Regularization}} and {{Data Augmentation}}},
  booktitle = {Proceedings of the 28th {{ACM SIGKDD Conference}} on {{Knowledge Discovery}} and {{Data Mining}}},
  author = {Wang, Haohan and Huang, Zeyi and Wu, Xindi and Xing, Eric},
  year = {2022},
  month = aug,
  pages = {1846--1856},
  publisher = {{ACM}},
  address = {{Washington DC USA}},
  doi = {10.1145/3534678.3539438},
  urldate = {2023-03-16},
  isbn = {978-1-4503-9385-0},
  langid = {english},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\Toward Learning Robust and Invariant Representations with Alignment _Wang et al 2022.pdf}
}

@inproceedings{wang2022should,
  title = {Should {{We Rely}} on {{Entity Mentions}} for {{Relation Extraction}}? {{Debiasing Relation Extraction}} with {{Counterfactual Analysis}}},
  shorttitle = {Should {{We Rely}} on {{Entity Mentions}} for {{Relation Extraction}}?},
  booktitle = {Proceedings of the 2022 {{Conference}} of the {{North American Chapter}} of the {{Association}} for {{Computational Linguistics}}: {{Human Language Technologies}}},
  author = {Wang, Yiwei and Chen, Muhao and Zhou, Wenxuan and Cai, Yujun and Liang, Yuxuan and Liu, Dayiheng and Yang, Baosong and Liu, Juncheng and Hooi, Bryan},
  year = {2022},
  pages = {3071--3081},
  publisher = {{Association for Computational Linguistics}},
  address = {{Seattle, United States}},
  doi = {10.18653/v1/2022.naacl-main.224},
  urldate = {2022-10-21},
  langid = {english},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\Should We Rely on Entity Mentions for Relation Extraction _Wang et al 2022.pdf}
}

@inproceedings{wang2022training,
  title = {Training {{Data}} Is {{More Valuable}} than {{You Think}}: {{A Simple}} and {{Effective Method}} by {{Retrieving}} from {{Training Data}}},
  shorttitle = {Training {{Data}} Is {{More Valuable}} than {{You Think}}},
  booktitle = {Proceedings of the 60th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}} ({{Volume}} 1: {{Long Papers}})},
  author = {Wang, Shuohang and Xu, Yichong and Fang, Yuwei and Liu, Yang and Sun, Siqi and Xu, Ruochen and Zhu, Chenguang and Zeng, Michael},
  year = {2022},
  pages = {3170--3179},
  publisher = {{Association for Computational Linguistics}},
  address = {{Dublin, Ireland}},
  doi = {10.18653/v1/2022.acl-long.226},
  urldate = {2022-06-29},
  langid = {english},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\Training Data is More Valuable than You Think _Wang et al 22.pdf}
}

@article{wang2023denoising,
  title = {Denoising {{Graph Inference Network}} for {{Document-Level Relation Extraction}}},
  author = {Wang, Hailin and Qin, Ke and Duan, Guiduo and Luo, Guangchun},
  year = {2023},
  month = jun,
  journal = {Big Data Mining and Analytics},
  volume = {6},
  number = {2},
  pages = {248--262},
  issn = {2096-0654},
  doi = {10.26599/BDMA.2022.9020051},
  urldate = {2023-03-16},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\Denoising Graph Inference Network for Document-Level Relation Extraction _Wang et al 2023.pdf}
}

@misc{weber2022certifying,
  title = {Certifying {{Out-of-Domain Generalization}} for {{Blackbox Functions}}},
  author = {Weber, Maurice and Li, Linyi and Wang, Boxin and Zhao, Zhikuan and Li, Bo and Zhang, Ce},
  year = {2022},
  month = feb,
  number = {arXiv:2202.01679},
  eprint = {arXiv:2202.01679},
  publisher = {{arXiv}},
  urldate = {2022-07-02},
  abstract = {Certifying the robustness of model performance under bounded data distribution shifts has recently attracted intensive interests under the umbrella of distributional robustness. However, existing techniques either make strong assumptions on the model class and loss functions that can be certified, such as smoothness expressed via Lipschitz continuity of gradients, or require to solve complex optimization problems. As a result, the wider application of these techniques is currently limited by its scalability and flexibility \textemdash{} these techniques often do not scale to large-scale datasets with modern deep neural networks or cannot handle loss functions which may be non-smooth, such as the 0-1 loss. In this paper, we focus on the problem of certifying distributional robustness for black box models and bounded losses, without other assumptions. We propose a novel certification framework given bounded distance of mean and variance of two distributions. Our certification technique scales to ImageNet-scale datasets, complex models, and a diverse range of loss functions. We then focus on one specific application enabled by such scalability and flexibility, i.e., certifying out-of-domain generalization for large neural networks and loss functions such as accuracy and AUC. We experimentally validate our certification method on a number of datasets, ranging from ImageNet, where we provide the first non-vacuous certified out-of-domain generalization, to smaller classification tasks where we are able to compare with the state-of-the-art and show that our method performs considerably better.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\Certifying Out-of-Domain Generalization for Blackbox Functions _Weber et al 22.pdf}
}

@inproceedings{wei2012document,
  title = {Document Summarization Method Based on Heterogeneous Graph},
  booktitle = {2012 9th {{International Conference}} on {{Fuzzy Systems}} and {{Knowledge Discovery}}},
  author = {Wei, Yang},
  year = {2012},
  month = may,
  pages = {1285--1289},
  publisher = {{IEEE}},
  address = {{Chongqing, Sichuan, China}},
  doi = {10.1109/FSKD.2012.6234047},
  urldate = {2023-04-02},
  isbn = {978-1-4673-0024-7 978-1-4673-0025-4 978-1-4673-0023-0},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\Document summarization method based on heterogeneous graph _Wei 2012.pdf;C\:\\Users\\Hytn\\Zotero\\storage\\5ZLQ3Q7E\\Document_summarization_method_based_on_heterogeneous_graph.pdf}
}

@inproceedings{wei2020novel,
  title = {A {{Novel Cascade Binary Tagging Framework}} for {{Relational Triple Extraction}}},
  booktitle = {Proceedings of the 58th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}}},
  author = {Wei, Zhepei and Su, Jianlin and Wang, Yue and Tian, Yuan and Chang, Yi},
  year = {2020},
  pages = {1476--1488},
  doi = {10.18653/v1/2020.acl-main.136},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\A Novel Cascade Binary Tagging Framework for Relational Triple Extraction _Wei et al 3.pdf;C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\A Novel Cascade Binary Tagging Framework for Relational Triple Extraction _Wei et al 4.pdf}
}

@article{wei2021graphtosequence,
  title = {A {{Graph-to-Sequence Learning Framework}} for {{Summarizing Opinionated Texts}}},
  author = {Wei, Penghui and Zhao, Jiahao and Mao, Wenji},
  year = {2021},
  journal = {IEEE/ACM Transactions on Audio, Speech, and Language Processing},
  volume = {29},
  pages = {1650--1660},
  issn = {2329-9290, 2329-9304},
  doi = {10.1109/TASLP.2021.3071667},
  urldate = {2022-06-10},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\A Graph-to-Sequence Learning Framework for Summarizing Opinionated Texts _Wei et al 22.pdf}
}

@inproceedings{wiles2022a,
  title = {A Fine-Grained Analysis on Distribution Shift},
  booktitle = {International Conference on Learning Representations},
  author = {Wiles, Olivia and Gowal, Sven and Stimberg, Florian and Rebuffi, Sylvestre-Alvise and Ktena, Ira and Dvijotham, Krishnamurthy Dj and Cemgil, Ali Taylan},
  year = {2022},
  keywords = {⛔ No DOI found},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\A fine-grained analysis on distribution shift _Wiles et al 2022.pdf}
}

@incollection{williams1992simple,
  title = {Simple {{Statistical Gradient-Following Algorithms}} for {{Connectionist Reinforcement Learning}}},
  booktitle = {Reinforcement {{Learning}}},
  author = {Williams, Ronald J.},
  editor = {Sutton, Richard S.},
  year = {1992},
  pages = {5--32},
  publisher = {{Springer US}},
  address = {{Boston, MA}},
  doi = {10.1007/978-1-4615-3618-5_2},
  urldate = {2023-03-01},
  isbn = {978-1-4613-6608-9 978-1-4615-3618-5},
  keywords = {REINFORCE},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\Simple Statistical Gradient-Following Algorithms for Connectionist _Williams 1992.pdf}
}

@inproceedings{wolf2020transformers,
  title = {Transformers: {{State-of-the-Art Natural Language Processing}}},
  booktitle = {Proceedings of the 2020 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}}: {{System Demonstrations}}},
  author = {Wolf, Thomas and Debut, Lysandre and Sanh, Victor and Chaumond, Julien and Delangue, Clement and Moi, Anthony and Cistac, Pierric and Rault, Tim and Louf, R{\'e}mi and Funtowicz, Morgan and Davison, Joe and Shleifer, Sam and von Platen, Patrick and Ma, Clara and Jernite, Yacine and Plu, Julien and Xu, Canwen and Scao, Teven Le and Gugger, Sylvain and Drame, Mariama and Lhoest, Quentin and Rush, Alexander M.},
  year = {2020},
  month = oct,
  pages = {38--45},
  publisher = {{Association for Computational Linguistics}},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\Transformers _Wolf et al 4.pdf}
}

@inproceedings{wolf2020transformersa,
  title = {Transformers: {{State-of-the-Art Natural Language Processing}}},
  shorttitle = {Transformers},
  booktitle = {Proceedings of the 2020 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}}: {{System Demonstrations}}},
  author = {Wolf, Thomas and Debut, Lysandre and Sanh, Victor and Chaumond, Julien and Delangue, Clement and Moi, Anthony and Cistac, Pierric and Rault, Tim and Louf, Remi and Funtowicz, Morgan and Davison, Joe and Shleifer, Sam and {von Platen}, Patrick and Ma, Clara and Jernite, Yacine and Plu, Julien and Xu, Canwen and Le Scao, Teven and Gugger, Sylvain and Drame, Mariama and Lhoest, Quentin and Rush, Alexander},
  year = {2020},
  pages = {38--45},
  publisher = {{Association for Computational Linguistics}},
  address = {{Online}},
  doi = {10.18653/v1/2020.emnlp-demos.6},
  urldate = {2022-08-16},
  langid = {english},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\Transformers _Wolf et al 3.pdf}
}

@inproceedings{wu2019renet,
  title = {Renet: {{A}} Deep Learning Approach for Extracting Gene-Disease Associations from Literature},
  booktitle = {International {{Conference}} on {{Research}} in {{Computational Molecular Biology}}},
  author = {Wu, Ye and Luo, Ruibang and Leung, Henry C. M. and Ting, Hing-Fung and Lam, Tak-Wah},
  year = {2019},
  pages = {272--284},
  keywords = {⛔ No DOI found}
}

@inproceedings{wu2022discovering,
  title = {Discovering Invariant Rationales for Graph Neural Networks},
  booktitle = {International Conference on Learning Representations},
  author = {Wu, Yingxin and Wang, Xiang and Zhang, An and He, Xiangnan and Chua, Tat-Seng},
  year = {2022},
  keywords = {⛔ No DOI found},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\Discovering invariant rationales for graph neural networks _Wu et al 22.pdf}
}

@inproceedings{wu2022generating,
  title = {Generating {{Data}} to {{Mitigate Spurious Correlations}} in {{Natural Language Inference Datasets}}},
  booktitle = {Proceedings of the 60th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}} ({{Volume}} 1: {{Long Papers}})},
  author = {Wu, Yuxiang and Gardner, Matt and Stenetorp, Pontus and Dasigi, Pradeep},
  year = {2022},
  pages = {2660--2676},
  publisher = {{Association for Computational Linguistics}},
  address = {{Dublin, Ireland}},
  doi = {10.18653/v1/2022.acl-long.190},
  urldate = {2022-06-10},
  langid = {english},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\Generating Data to Mitigate Spurious Correlations in Natural Language Inference _Wu et al 22.pdf}
}

@article{wu2022learning,
  title = {Learning {{Decomposed Representations}} for {{Treatment Effect Estimation}}},
  author = {Wu, Anpeng and Yuan, Junkun and Kuang, Kun and Li, Bo and Wu, Runze and Zhu, Qiang and Zhuang, Yue Ting and Wu, Fei},
  year = {2022},
  journal = {IEEE Transactions on Knowledge and Data Engineering},
  pages = {1--1},
  issn = {1041-4347, 1558-2191, 2326-3865},
  doi = {10.1109/TKDE.2022.3150807},
  urldate = {2022-05-14},
  keywords = {待读},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\Learning Decomposed Representations for Treatment Effect Estimation _Wu et al 22.pdf}
}

@inproceedings{xiao2016semantic,
  title = {Semantic Relation Classification via Hierarchical Recurrent Neural Network with Attention},
  booktitle = {Proceedings of {{COLING}} 2016, the 26th {{International Conference}} on {{Computational Linguistics}}: {{Technical Papers}}},
  author = {Xiao, Minguang and Liu, Cong},
  year = {2016},
  pages = {1254--1263},
  keywords = {⛔ No DOI found}
}

@misc{xiao2018cail2018,
  title = {{{CAIL2018}}: {{A Large-Scale Legal Dataset}} for {{Judgment Prediction}}},
  shorttitle = {{{CAIL2018}}},
  author = {Xiao, Chaojun and Zhong, Haoxi and Guo, Zhipeng and Tu, Cunchao and Liu, Zhiyuan and Sun, Maosong and Feng, Yansong and Han, Xianpei and Hu, Zhen and Wang, Heng and Xu, Jianfeng},
  year = {2018},
  month = jul,
  number = {arXiv:1807.02478},
  eprint = {arXiv:1807.02478},
  publisher = {{arXiv}},
  urldate = {2022-07-21},
  abstract = {In this paper, we introduce the Chinese AI and Law challenge dataset (CAIL2018), the first large-scale Chinese legal dataset for judgment prediction. CAIL2018 contains more than 2.6 million criminal cases published by the Supreme People's Court of China, which are several times larger than other datasets in existing works on judgment prediction. Moreover, the annotations of judgment results are more detailed and rich. It consists of applicable law articles, charges, and prison terms, which are expected to be inferred according to the fact descriptions of cases. For comparison, we implement several conventional text classification baselines for judgment prediction and experimental results show that it is still a challenge for current models to predict the judgment results of legal cases, especially on prison terms. To help the researchers make improvements on legal judgment prediction, both CAIL2018 and baselines will be released after the CAIL competition1.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Computation and Language},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\CAIL2018 _Xiao et al 22.pdf}
}

@article{xiao2021lawformer,
  title = {Lawformer: {{A}} Pre-Trained Language Model for {{Chinese}} Legal Long Documents},
  shorttitle = {Lawformer},
  author = {Xiao, Chaojun and Hu, Xueyu and Liu, Zhiyuan and Tu, Cunchao and Sun, Maosong},
  year = {2021},
  journal = {AI Open},
  volume = {2},
  pages = {79--84},
  issn = {26666510},
  doi = {10.1016/j.aiopen.2021.06.003},
  urldate = {2022-08-31},
  langid = {english},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\Lawformer _Xiao et al 2021.pdf}
}

@article{xiao2022fusionsum,
  title = {{{FusionSum}}: {{Abstractive}} Summarization with Sentence Fusion and Cooperative Reinforcement Learning},
  shorttitle = {{{FusionSum}}},
  author = {Xiao, Liqiang and He, Hao and Jin, Yaohui},
  year = {2022},
  month = may,
  journal = {Knowledge-Based Systems},
  volume = {243},
  pages = {108483},
  issn = {09507051},
  doi = {10.1016/j.knosys.2022.108483},
  urldate = {2023-02-15},
  langid = {english},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\FusionSum _Xiao et al 2022.pdf}
}

@inproceedings{xiao2022primera,
  title = {{{PRIMERA}}: {{Pyramid-based Masked Sentence Pre-training}} for {{Multi-document Summarization}}},
  shorttitle = {{{PRIMERA}}},
  booktitle = {Proceedings of the 60th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}} ({{Volume}} 1: {{Long Papers}})},
  author = {Xiao, Wen and Beltagy, Iz and Carenini, Giuseppe and Cohan, Arman},
  year = {2022},
  pages = {5245--5263},
  publisher = {{Association for Computational Linguistics}},
  address = {{Dublin, Ireland}},
  doi = {10.18653/v1/2022.acl-long.360},
  urldate = {2023-04-02},
  langid = {english},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\PRIMERA _Xiao et al 2022.pdf}
}

@inproceedings{xiao2022sais,
  title = {{{SAIS}}: {{Supervising}} and Augmenting Intermediate Steps for Document-Level Relation Extraction},
  booktitle = {Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: {{Human}} Language Technologies},
  author = {Xiao, Yuxin and Zhang, Zecheng and Mao, Yuning and Yang, Carl and Han, Jiawei},
  year = {2022},
  month = jul,
  pages = {2395--2409},
  publisher = {{Association for Computational Linguistics}},
  address = {{Seattle, United States}},
  abstract = {Stepping from sentence-level to document-level, the research on relation extraction (RE) confronts increasing text length and more complicated entity interactions. Consequently, it is more challenging to encode the key information sources\textemdash relevant contexts and entity types. However, existing methods only implicitly learn to model these critical information sources while being trained for RE. As a result, they suffer the problems of ineffective supervision and uninterpretable model predictions. In contrast, we propose to explicitly teach the model to capture relevant contexts and entity types by supervising and augmenting intermediate steps (SAIS) for RE. Based on a broad spectrum of carefully designed tasks, our proposed SAIS method not only extracts relations of better quality due to more effective supervision, but also retrieves the corresponding supporting evidence more accurately so as to enhance interpretability. By assessing model uncertainty, SAIS further boosts the performance via evidence-based data augmentation and ensemble inference while reducing the computational cost. Eventually, SAIS delivers state-of-the-art RE results on three benchmarks (DocRED, CDR, and GDA) and outperforms the runner-up by 5.04\% relatively in F1 score in evidence retrieval on DocRED.},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\SAIS _Xiao et al 22.pdf}
}

@inproceedings{xie2022eider,
  title = {Eider: {{Empowering Document-level Relation Extraction}} with {{Efficient Evidence Extraction}} and {{Inference-stage Fusion}}},
  shorttitle = {Eider},
  booktitle = {Findings of the {{Association}} for {{Computational Linguistics}}: {{ACL}} 2022},
  author = {Xie, Yiqing and Shen, Jiaming and Li, Sha and Mao, Yuning and Han, Jiawei},
  year = {2022},
  pages = {257--268},
  publisher = {{Association for Computational Linguistics}},
  address = {{Dublin, Ireland}},
  doi = {10.18653/v1/2022.findings-acl.23},
  urldate = {2022-06-14},
  langid = {english},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\Eider _Xie et al 22.pdf}
}

@inproceedings{xu2015classifying,
  title = {Classifying Relations via Long Short Term Memory Networks along Shortest Dependency Paths},
  booktitle = {Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing},
  author = {Xu, Yan and Mou, Lili and Li, Ge and Chen, Yunchuan and Peng, Hao and Jin, Zhi},
  year = {2015},
  pages = {1785--1794},
  doi = {10.18653/v1/D15-1206},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\Classifying relations via long short term memory networks along shortest _Xu et al 22.pdf}
}

@inproceedings{xu2018sequence,
  title = {Sequence {{Generative Adversarial Network}} for {{Long Text Summarization}}},
  booktitle = {2018 {{IEEE}} 30th {{International Conference}} on {{Tools}} with {{Artificial Intelligence}} ({{ICTAI}})},
  author = {Xu, Hao and Cao, Yanan and Jia, Ruipeng and Liu, Yanbing and Tan, Jianlong},
  year = {2018},
  month = nov,
  pages = {242--248},
  publisher = {{IEEE}},
  address = {{Volos, Greece}},
  doi = {10.1109/ICTAI.2018.00045},
  urldate = {2022-05-23},
  isbn = {978-1-5386-7449-9},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\Sequence Generative Adversarial Network for Long Text Summarization _Xu et al 22.pdf}
}

@inproceedings{xu2019neural,
  title = {Neural {{Extractive Text Summarization}} with {{Syntactic Compression}}},
  booktitle = {Proceedings of the 2019 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}} and the 9th {{International Joint Conference}} on {{Natural Language Processing}} ({{EMNLP-IJCNLP}})},
  author = {Xu, Jiacheng and Durrett, Greg},
  year = {2019},
  pages = {3290--3301},
  publisher = {{Association for Computational Linguistics}},
  address = {{Hong Kong, China}},
  doi = {10.18653/v1/D19-1324},
  urldate = {2022-08-20},
  langid = {english},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\Neural Extractive Text Summarization with Syntactic Compression _Xu_Durrett 2019.pdf}
}

@article{xu2021dissecting,
  title = {Dissecting {{Generation Modes}} for {{Abstractive Summarization Models}} via {{Ablation}} and {{Attribution}}},
  author = {Xu, Jiacheng and Durrett, Greg},
  year = {2021},
  month = jun,
  journal = {arXiv:2106.01518 [cs]},
  eprint = {2106.01518},
  primaryclass = {cs},
  urldate = {2022-03-13},
  abstract = {Despite the prominence of neural abstractive summarization models, we know little about how they actually form summaries and how to understand where their decisions come from. We propose a two-step method to interpret summarization model decisions. We first analyze the model's behavior by ablating the full model to categorize each decoder decision into one of several generation modes: roughly, is the model behaving like a language model, is it relying heavily on the input, or is it somewhere in between? After isolating decisions that do depend on the input, we explore interpreting these decisions using several different attribution methods. We compare these techniques based on their ability to select content and reconstruct the model's predicted token from perturbations of the input, thus revealing whether highlighted attributions are truly important for the generation of the next token. While this machinery can be broadly useful even beyond summarization, we specifically demonstrate its capability to identify phrases the summarization model has memorized and determine where in the training pipeline this memorization happened, as well as study complex generation phenomena like sentence fusion on a per-instance basis.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {⛔ No DOI found,Computer Science - Computation and Language},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\Dissecting Generation Modes for Abstractive Summarization Models via Ablation _Xu_Durrett 22.pdf}
}

@inproceedings{xu2021entity,
  title = {Entity {{Structure Within}} and {{Throughout}}: {{Modeling Mention Dependencies}} for {{Document-Level Relation Extraction}}.},
  booktitle = {{{AAAI}}},
  author = {Xu, Benfeng and Wang, Quan and Lyu, Yajuan and Zhu, Yong and Mao, Zhendong},
  year = {2021},
  pages = {14149--14157},
  keywords = {⛔ No DOI found},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\Entity Structure Within and Throughout _Xu et al 22.pdf}
}

@inproceedings{xu2022evidenceaware,
  title = {Evidence-Aware {{Document-level Relation Extraction}}},
  booktitle = {Proceedings of the 31st {{ACM International Conference}} on {{Information}} \& {{Knowledge Management}}},
  author = {Xu, Tianyu and Hua, Wen and Qu, Jianfeng and Li, Zhixu and Xu, Jiajie and Liu, An and Zhao, Lei},
  year = {2022},
  month = oct,
  pages = {2311--2320},
  publisher = {{ACM}},
  address = {{Atlanta GA USA}},
  doi = {10.1145/3511808.3557313},
  urldate = {2023-02-20},
  isbn = {978-1-4503-9236-5},
  langid = {english},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\Evidence-aware Document-level Relation Extraction _Xu et al 2022.pdf}
}

@article{yan2021partition,
  title = {A {{Partition Filter Network}} for {{Joint Entity}} and {{Relation Extraction}}},
  author = {Yan, Zhiheng and Zhang, Chong and Fu, Jinlan and Zhang, Qi and Wei, Zhongyu},
  year = {2021},
  month = sep,
  journal = {arXiv:2108.12202 [cs]},
  eprint = {2108.12202},
  primaryclass = {cs},
  urldate = {2022-03-03},
  abstract = {In joint entity and relation extraction, existing work either sequentially encode task-specific features, leading to an imbalance in inter-task feature interaction where features extracted later have no direct contact with those that come first. Or they encode entity features and relation features in a parallel manner, meaning that feature representation learning for each task is largely independent of each other except for input sharing. We propose a partition filter network to model two-way interaction between tasks properly, where feature encoding is decomposed into two steps: partition and filter. In our encoder, we leverage two gates: entity and relation gate, to segment neurons into two task partitions and one shared partition. The shared partition represents inter-task information valuable to both tasks and is evenly shared across two tasks to ensure proper two-way interaction. The task partitions represent intra-task information and are formed through concerted efforts of both gates, making sure that encoding of taskspecific features is dependent upon each other. Experiment results on six public datasets show that our model performs significantly better than previous approaches. In addition, contrary to what previous work has claimed, our auxiliary experiments suggest that relation prediction is contributory to named entity prediction in a non-negligible way. The source code can be found at https://github.com/ Coopercoppers/PFN.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {⛔ No DOI found,Computer Science - Computation and Language},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\A Partition Filter Network for Joint Entity and Relation Extraction _Yan et al 22.pdf}
}

@inproceedings{yang2019contextaware,
  title = {Context-Aware Self-Attention Networks},
  booktitle = {Proceedings of the {{AAAI Conference}} on {{Artificial Intelligence}}},
  author = {Yang, Baosong and Li, Jian and Wong, Derek F. and Chao, Lidia S. and Wang, Xing and Tu, Zhaopeng},
  year = {2019},
  volume = {33},
  pages = {387--394},
  keywords = {⛔ No DOI found}
}

@article{yang2019xlnet,
  title = {Xlnet: {{Generalized}} Autoregressive Pretraining for Language Understanding},
  author = {Yang, Zhilin and Dai, Zihang and Yang, Yiming and Carbonell, Jaime and Salakhutdinov, Russ R and Le, Quoc V},
  year = {2019},
  journal = {Advances in neural information processing systems},
  volume = {32},
  keywords = {⛔ No DOI found}
}

@inproceedings{yang2021causal,
  title = {Causal {{Attention}} for {{Vision-Language Tasks}}},
  booktitle = {2021 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Yang, Xu and Zhang, Hanwang and Qi, Guojun and Cai, Jianfei},
  year = {2021},
  month = jun,
  pages = {9842--9852},
  publisher = {{IEEE}},
  doi = {10.1109/CVPR46437.2021.00972},
  isbn = {978-1-66544-509-2},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\Causal Attention for Vision-Language Tasks _Yang et al 22.pdf}
}

@misc{yang2021endtoend,
  title = {End-to-End {{Robustness}} for {{Sensing-Reasoning Machine Learning Pipelines}}},
  author = {Yang, Zhuolin and Zhao, Zhikuan and Pei, Hengzhi and Wang, Boxin and Karlas, Bojan and Liu, Ji and Guo, Heng and Li, Bo and Zhang, Ce},
  year = {2021},
  month = aug,
  number = {arXiv:2003.00120},
  eprint = {arXiv:2003.00120},
  publisher = {{arXiv}},
  urldate = {2022-07-02},
  abstract = {As machine learning (ML) has been widely applied to safety-critical scenarios, certifying ML model robustness becomes increasingly important. Many previous studies focus on the robustness of independent ML and ensemble models, and can only certify a relatively small magnitude of p norm bounded adversarial perturbation. In this paper, we take a different viewpoint and aim to improve ML robustness by going beyond independent ML and ensemble models. We propose a generic Sensing-Reasoning machine learning pipeline which contains both the sensing (e.g. deep neural networks) and reasoning (e.g. Markov logic networks (MLN)) components enriched with domain knowledge, and ask questions: Can domain knowledge help improve learning robustness? Can we formally certify the end-to-end robustness of such an ML pipeline? Towards the end-to-end certified robustness of Sensing-Reasoning Pipeline, there are already existing studies on certifying the robustness for deep neural networks, which can be applied to certifying the sensing component. Thus, in this paper we will focus on the certified robustness for the reasoning component. In particular, we first theoretically analyze the computational complexity of certifying the robustness for the reasoning component. We then derive the certified robustness bound for several structures of reasoning components, such as Markov logic networks and Bayesian networks. We show that for reasoning components such as MLN and a specific family of Bayesian networks it is possible to certify the robustness of the whole pipeline even against a large magnitude of perturbation. We also prove that the certified robustness bound for Bayesian networks is tight. Finally, we conduct extensive experiments on both image and natural language datasets to evaluate the certified robustness of the Sensing-Reasoning ML pipeline. We show that based on the hierarchical knowledge of the Primate family (i.e. PrimateNet), the certified robustness of the SensingReasoning Pipeline can achieve over 50\%, even when 50\% sensors are attacked under adversarial perturbation with L2 norm bounded by 0.5. We also evaluate Sensing-Reasoning Pipeline on a relation extraction task with Stock News dataset, which shows that even when all sensors are attacked, under adversarial attack with confidence bounded by 0.5, the certified robustness can reach almost 100\%.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Cryptography and Security,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\End-to-end Robustness for Sensing-Reasoning Machine Learning Pipelines _Yang et al 22.pdf}
}

@misc{yang2022generalized,
  title = {Generalized {{Out-of-Distribution Detection}}: {{A Survey}}},
  shorttitle = {Generalized {{Out-of-Distribution Detection}}},
  author = {Yang, Jingkang and Zhou, Kaiyang and Li, Yixuan and Liu, Ziwei},
  year = {2022},
  month = aug,
  number = {arXiv:2110.11334},
  eprint = {arXiv:2110.11334},
  publisher = {{arXiv}},
  urldate = {2022-09-03},
  abstract = {Out-of-distribution (OOD) detection is critical to ensuring the reliability and safety of machine learning systems. For instance, in autonomous driving, we would like the driving system to issue an alert and hand over the control to humans when it detects unusual scenes or objects that it has never seen during training time and cannot make a safe decision. The term, OOD detection, first emerged in 2017 and since then has received increasing attention from the research community, leading to a plethora of methods developed, ranging from classification-based to density-based to distance-based ones. Meanwhile, several other problems, including anomaly detection (AD), novelty detection (ND), open set recognition (OSR), and outlier detection (OD), are closely related to OOD detection in terms of motivation and methodology. Despite common goals, these topics develop in isolation, and their subtle differences in definition and problem setting often confuse readers and practitioners. In this survey, we first present a unified framework called generalized OOD detection, which encompasses the five aforementioned problems, i.e., AD, ND, OSR, OOD detection, and OD. Under our framework, these five problems can be seen as special cases or sub-tasks, and are easier to distinguish. We then review each of these five areas by summarizing their recent technical developments, with a special focus on OOD detection methodologies. We conclude this survey with open challenges and potential research directions.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\Generalized Out-of-Distribution Detection _Yang et al 2022.pdf}
}

@inproceedings{yao2011structured,
  title = {Structured Relation Discovery Using Generative Models},
  booktitle = {Proceedings of the Conference on Empirical Methods in Natural Language Processing},
  author = {Yao, Limin and Haghighi, Aria and Riedel, Sebastian and McCallum, Andrew},
  year = {2011},
  series = {{{EMNLP}} '11},
  pages = {1456--1466},
  publisher = {{Association for Computational Linguistics}},
  address = {{USA}},
  abstract = {We explore unsupervised approaches to relation extraction between two named entities; for instance, the semantic bornIn relation between a person and location entity. Concretely, we propose a series of generative probabilistic models, broadly similar to topic models, each which generates a corpus of observed triples of entity mention pairs and the surface syntactic dependency path between them. The output of each model is a clustering of observed relation tuples and their associated textual expressions to underlying semantic relation types. Our proposed models exploit entity type constraints within a relation as well as features on the dependency path between entity mentions. We examine effectiveness of our approach via multiple evaluations and demonstrate 12\% error reduction in precision over a state-of-the-art weakly supervised baseline.},
  isbn = {978-1-937284-11-4},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\Structured relation discovery using generative models _Yao et al 2011.pdf}
}

@inproceedings{yao2019docred,
  title = {{{DocRED}}: {{A Large-Scale Document-Level Relation Extraction Dataset}}.},
  booktitle = {Proceedings of the 57th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}}},
  author = {Yao, Yuan and Ye, Deming and Li, Peng and Han, Xu and Lin, Yankai and Liu, Zhenghao and Liu, Zhiyuan and Huang, Lixin and Zhou, Jie and Sun, Maosong},
  year = {2019},
  pages = {764--777},
  doi = {10.18653/v1/P19-1074},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\DocRED _Yao et al 22.pdf}
}

@inproceedings{yao2020heterogeneous,
  title = {Heterogeneous {{Graph Transformer}} for {{Graph-to-Sequence Learning}}},
  booktitle = {Proceedings of the 58th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}}},
  author = {Yao, Shaowei and Wang, Tianming and Wan, Xiaojun},
  year = {2020},
  pages = {7145--7154},
  publisher = {{Association for Computational Linguistics}},
  address = {{Online}},
  doi = {10.18653/v1/2020.acl-main.640},
  urldate = {2022-06-10},
  langid = {english},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\Heterogeneous Graph Transformer for Graph-to-Sequence Learning _Yao et al 22.pdf}
}

@inproceedings{yao2021codred,
  title = {{{CodRED}}: {{A Cross-Document Relation Extraction Dataset}} for {{Acquiring Knowledge}} in the {{Wild}}},
  shorttitle = {{{CodRED}}},
  booktitle = {Proceedings of the 2021 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}}},
  author = {Yao, Yuan and Du, Jiaju and Lin, Yankai and Li, Peng and Liu, Zhiyuan and Zhou, Jie and Sun, Maosong},
  year = {2021},
  pages = {4452--4472},
  publisher = {{Association for Computational Linguistics}},
  address = {{Online and Punta Cana, Dominican Republic}},
  doi = {10.18653/v1/2021.emnlp-main.366},
  urldate = {2022-06-02},
  langid = {english},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\CodRED _Yao et al 22.pdf}
}

@inproceedings{yasunaga2017graphbased,
  title = {Graph-Based {{Neural Multi-Document Summarization}}},
  booktitle = {Proceedings of the 21st {{Conference}} on {{Computational Natural Language}}           {{Learning}} ({{CoNLL}} 2017)},
  author = {Yasunaga, Michihiro and Zhang, Rui and Meelu, Kshitijh and Pareek, Ayush and Srinivasan, Krishnan and Radev, Dragomir},
  year = {2017},
  pages = {452--462},
  publisher = {{Association for Computational Linguistics}},
  address = {{Vancouver, Canada}},
  doi = {10.18653/v1/K17-1045},
  urldate = {2022-08-27},
  langid = {english},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\Graph-based Neural Multi-Document Summarization _Yasunaga et al 2017.pdf}
}

@inproceedings{ye2020coreferential,
  title = {Coreferential {{Reasoning Learning}} for {{Language Representation}}},
  booktitle = {Proceedings of the 2020 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}} ({{EMNLP}})},
  author = {Ye, Deming and Lin, Yankai and Du, Jiaju and Liu, Zhenghao and Li, Peng and Sun, Maosong and Liu, Zhiyuan},
  year = {2020},
  pages = {7170--7186},
  doi = {10.18653/v1/2020.emnlp-main.582},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\Coreferential Reasoning Learning for Language Representation _Ye et al 22.pdf}
}

@inproceedings{ye2021contrastive,
  title = {Contrastive Triple Extraction with Generative Transformer},
  booktitle = {Proceedings of the {{AAAI}} Conference on Artificial Intelligence},
  author = {Ye, Hongbin and Zhang, Ningyu and Deng, Shumin and Chen, Mosha and Tan, Chuanqi and Huang, Fei and Chen, Huajun},
  year = {2021},
  volume = {35},
  pages = {14257--14265},
  keywords = {⛔ No DOI found},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\Contrastive triple extraction with generative transformer _Ye et al 22.pdf}
}

@article{ye2021more,
  title = {Towards {{More Fine-grained}} and {{Reliable NLP Performance Prediction}}},
  author = {Ye, Zihuiwen and Liu, Pengfei and Fu, Jinlan and Neubig, Graham},
  year = {2021},
  month = feb,
  abstract = {Performance prediction, the task of estimating a system's performance without performing experiments, allows us to reduce the experimental burden caused by the combinatorial explosion of different datasets, languages, tasks, and models. In this paper, we make two contributions to improving performance prediction for NLP tasks. First, we examine performance predictors not only for holistic measures of accuracy like F1 or BLEU but also fine-grained performance measures such as accuracy over individual classes of examples. Second, we propose methods to understand the reliability of a performance prediction model from two angles: confidence intervals and calibration. We perform an analysis of four types of NLP tasks, and both demonstrate the feasibility of fine-grained performance prediction and the necessity to perform reliability analysis for performance prediction methods in the future. We make our code publicly available: \{https://github.com/neulab/Reliable-NLPPP\}},
  keywords = {⛔ No DOI found},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\Towards More Fine-grained and Reliable NLP Performance Prediction _Ye et al 22.pdf}
}

@inproceedings{ye2021towards,
  title = {Towards a Theoretical Framework of Out-of-Distribution Generalization},
  booktitle = {Advances in Neural Information Processing Systems},
  author = {Ye, Haotian and Xie, Chuanlong and Cai, Tianle and Li, Ruichen and Li, Zhenguo and Wang, Liwei},
  editor = {Ranzato, M. and Beygelzimer, A. and Dauphin, Y. and Liang, P.S. and Vaughan, J. Wortman},
  year = {2021},
  volume = {34},
  pages = {23519--23531},
  publisher = {{Curran Associates, Inc.}},
  keywords = {⛔ No DOI found},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\Towards a theoretical framework of out-of-distribution generalization _Ye et al 2021.pdf}
}

@inproceedings{ye2022packed,
  title = {Packed {{Levitated Marker}} for {{Entity}} and {{Relation Extraction}}},
  booktitle = {Proceedings of the 60th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}} ({{Volume}} 1: {{Long Papers}})},
  author = {Ye, Deming and Lin, Yankai and Li, Peng and Sun, Maosong},
  year = {2022},
  pages = {4904--4917},
  publisher = {{Association for Computational Linguistics}},
  address = {{Dublin, Ireland}},
  doi = {10.18653/v1/2022.acl-long.337},
  urldate = {2022-06-13},
  langid = {english},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\Packed Levitated Marker for Entity and Relation Extraction _Ye et al 22.pdf}
}

@article{yeoodbench,
  title = {{{OoD-Bench}}: {{Quantifying}} and {{Understanding Two Dimensions}} of {{Out-of-Distribution Generalization}}},
  author = {Ye, Nanyang and Li, Kaican and Bai, Haoyue and Yu, Runpeng and Hong, Lanqing and Zhou, Fengwei and Li, Zhenguo and Zhu, Jun},
  pages = {21},
  abstract = {Deep learning has achieved tremendous success with independent and identically distributed (i.i.d.) data. However, the performance of neural networks often degenerates drastically when encountering out-of-distribution (OoD) data, i.e., when training and test data are sampled from different distributions. While a plethora of algorithms have been proposed for OoD generalization, our understanding of the data used to train and evaluate these algorithms remains stagnant. In this work, we first identify and measure two distinct kinds of distribution shifts that are ubiquitous in various datasets. Next, through extensive experiments, we compare OoD generalization algorithms across two groups of benchmarks, each dominated by one of the distribution shifts, revealing their strengths on one shift as well as limitations on the other shift. Overall, we position existing datasets and algorithms from different research areas seemingly unconnected into the same coherent picture. It may serve as a foothold that can be resorted to by future OoD generalization research. Our code is available at https://github.com/ynysjtu/ood\_bench.},
  langid = {english},
  keywords = {⛔ No DOI found},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\OoD-Bench _Ye et al.pdf}
}

@book{yin2019towards,
  title = {Towards More Scalable and Robust Machine Learning},
  author = {Yin, Dong},
  year = {2019},
  publisher = {{University of California, Berkeley}},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\Towards more scalable and robust machine learning _Yin 22.pdf}
}

@inproceedings{yin2022sensitivity,
  title = {On the {{Sensitivity}} and {{Stability}} of {{Model Interpretations}} in {{NLP}}},
  booktitle = {Proceedings of the 60th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}} ({{Volume}} 1: {{Long Papers}})},
  author = {Yin, Fan and Shi, Zhouxing and Hsieh, Cho-Jui and Chang, Kai-Wei},
  year = {2022},
  pages = {2631--2647},
  publisher = {{Association for Computational Linguistics}},
  address = {{Dublin, Ireland}},
  doi = {10.18653/v1/2022.acl-long.188},
  urldate = {2022-06-30},
  langid = {english},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\On the Sensitivity and Stability of Model Interpretations in NLP _Yin et al 22.pdf}
}

@inproceedings{ying2021transformers,
  title = {Do {{Transformers Really Perform Badly}} for {{Graph Representation}}?},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Ying, Chengxuan and Cai, Tianle and Luo, Shengjie and Zheng, Shuxin and Ke, Guolin and He, Di and Shen, Yanming and Liu, Tie-Yan},
  editor = {Ranzato, M. and Beygelzimer, A. and Dauphin, Y. and Liang, P. S. and Vaughan, J. Wortman},
  year = {2021},
  volume = {34},
  pages = {28877--28888},
  publisher = {{Curran Associates, Inc.}},
  keywords = {⛔ No DOI found,Graphormer},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\Do Transformers Really Perform Badly for Graph Representation _Ying et al 2021.pdf}
}

@inproceedings{yu2010jointly,
  title = {Jointly Identifying Entities and Extracting Relations in Encyclopedia Text via a Graphical Model Approach},
  booktitle = {Coling 2010: {{Posters}}},
  author = {Yu, Xiaofeng and Lam, Wai},
  year = {2010},
  pages = {1399--1407},
  keywords = {⛔ No DOI found}
}

@article{yu2017seqgan,
  title = {{{SeqGAN}}: {{Sequence Generative Adversarial Nets}} with {{Policy Gradient}}},
  shorttitle = {{{SeqGAN}}},
  author = {Yu, Lantao and Zhang, Weinan and Wang, Jun and Yu, Yong},
  year = {2017},
  month = feb,
  journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
  volume = {31},
  number = {1},
  issn = {2374-3468, 2159-5399},
  doi = {10.1609/aaai.v31i1.10804},
  urldate = {2022-07-01},
  abstract = {As a new way of training generative models, Generative Adversarial Net (GAN) that uses a discriminative model to guide the training of the generative model has enjoyed considerable success in generating real-valued data. However, it has limitations when the goal is for generating sequences of discrete tokens. A major reason lies in that the discrete outputs from the generative model make it difficult to pass the gradient update from the discriminative model to the generative model. Also, the discriminative model can only assess a complete sequence, while for a partially generated sequence, it is non-trivial to balance its current score and the future one once the entire sequence has been generated. In this paper, we propose a sequence generation framework, called SeqGAN, to solve the problems. Modeling the data generator as a stochastic policy in reinforcement learning (RL), SeqGAN bypasses the generator differentiation problem by directly performing gradient policy update. The RL reward signal comes from the GAN discriminator judged on a complete sequence, and is passed back to the intermediate state-action steps using Monte Carlo search. Extensive experiments on synthetic data and real-world tasks demonstrate significant improvements over strong baselines.},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\SeqGAN _Yu et al 22.pdf}
}

@article{yu2017seqgana,
  title = {{{SeqGAN}}: {{Sequence Generative Adversarial Nets}} with {{Policy Gradient}}},
  shorttitle = {{{SeqGAN}}},
  author = {Yu, Lantao and Zhang, Weinan and Wang, Jun and Yu, Yong},
  year = {2017},
  month = feb,
  journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
  volume = {31},
  number = {1},
  issn = {2374-3468, 2159-5399},
  doi = {10.1609/aaai.v31i1.10804},
  urldate = {2023-02-20},
  abstract = {As a new way of training generative models, Generative Adversarial Net (GAN) that uses a discriminative model to guide the training of the generative model has enjoyed considerable success in generating real-valued data. However, it has limitations when the goal is for generating sequences of discrete tokens. A major reason lies in that the discrete outputs from the generative model make it difficult to pass the gradient update from the discriminative model to the generative model. Also, the discriminative model can only assess a complete sequence, while for a partially generated sequence, it is non-trivial to balance its current score and the future one once the entire sequence has been generated. In this paper, we propose a sequence generation framework, called SeqGAN, to solve the problems. Modeling the data generator as a stochastic policy in reinforcement learning (RL), SeqGAN bypasses the generator differentiation problem by directly performing gradient policy update. The RL reward signal comes from the GAN discriminator judged on a complete sequence, and is passed back to the intermediate state-action steps using Monte Carlo search. Extensive experiments on synthetic data and real-world tasks demonstrate significant improvements over strong baselines.},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\SeqGAN _Yu et al 3.pdf}
}

@article{yu2019joint,
  title = {Joint Extraction of Entities and Relations Based on a Novel Decomposition Strategy},
  author = {Yu, Bowen and Zhang, Zhenyu and Su, Jianlin},
  year = {2019},
  journal = {arXiv preprint arXiv:1909.04273},
  eprint = {1909.04273},
  archiveprefix = {arxiv},
  keywords = {⛔ No DOI found}
}

@inproceedings{yu2022relationspecific,
  title = {Relation-Specific Attentions over Entity Mentions for Enhanced Document-Level Relation Extraction},
  booktitle = {North American Chapter of the Association for Computational Linguistics},
  author = {Yu, Jiaxin and Yang, Deqing and Tian, Shuyu},
  year = {2022},
  doi = {10.18653/v1/2022.naacl-main.109},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\Relation-specific attentions over entity mentions for enhanced document-level _Yu et al 2022.pdf}
}

@article{yu2022survey,
  title = {A {{Survey}} of {{Knowledge-Enhanced Text Generation}}},
  author = {Yu, Wenhao and Zhu, Chenguang and Li, Zaitang and Hu, Zhiting and Wang, Qingyun and Ji, Heng and Jiang, Meng},
  year = {2022},
  month = mar,
  journal = {ACM Computing Surveys},
  pages = {3512467},
  issn = {0360-0300, 1557-7341},
  doi = {10.1145/3512467},
  urldate = {2022-08-25},
  abstract = {The goal of text-to-text generation is to make machines express like a human in many applications such as conversation, summarization, and translation. It is one of the most important yet challenging tasks in natural language processing (NLP). Various neural encoder-decoder models have been proposed to achieve the goal by learning to map input text to output text. However, the input text alone often provides limited knowledge to generate the desired output, so the performance of text generation is still far from satisfaction in many real-world scenarios. To address this issue, researchers have considered incorporating (i) internal knowledge embedded in the input text and (ii) external knowledge from outside sources such as knowledge base and knowledge graph into the text generation system. This research topic is known as               knowledge-enhanced text generation               . In this survey, we present a comprehensive review of the research on this topic over the past five years. The main content includes two parts: (i) general methods and architectures for integrating knowledge into text generation; (ii) specific techniques and applications according to different forms of knowledge data. This survey can have broad audiences, researchers and practitioners, in academia and industry.},
  langid = {english},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\A Survey of Knowledge-Enhanced Text Generation _Yu et al 2022.pdf}
}

@article{yu2022whitebox,
  title = {A {{White-Box Testing}} for {{Deep Neural Networks Based}} on {{Neuron Coverage}}},
  author = {Yu, Jing and Duan, Shukai and Ye, Xiaojun},
  year = {2022},
  journal = {IEEE Transactions on Neural Networks and Learning Systems},
  pages = {1--13},
  issn = {2162-237X, 2162-2388},
  doi = {10.1109/TNNLS.2022.3156620},
  urldate = {2022-11-22},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\A White-Box Testing for Deep Neural Networks Based on Neuron Coverage _Yu et al 2022.pdf}
}

@inproceedings{yuan2021physics,
  title = {Physics {{Interpretable Shallow-Deep Neural Networks}} for {{Physical System Identification}} with {{Unobservability}}},
  booktitle = {2021 {{IEEE International Conference}} on {{Data Mining}} ({{ICDM}})},
  author = {Yuan, Jingyi and Weng, Yang},
  year = {2021},
  month = dec,
  pages = {847--856},
  publisher = {{IEEE}},
  address = {{Auckland, New Zealand}},
  doi = {10.1109/ICDM51629.2021.00096},
  urldate = {2022-09-01},
  isbn = {978-1-66542-398-4},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\Physics Interpretable Shallow-Deep Neural Networks for Physical System _Yuan_Weng 2021.pdf}
}

@article{yuan2022auto,
  title = {Auto {{IV}}: {{Counterfactual Prediction}} via {{Automatic Instrumental Variable Decomposition}}},
  shorttitle = {Auto {{IV}}},
  author = {Yuan, Junkun and Wu, Anpeng and Kuang, Kun and Li, Bo and Wu, Runze and Wu, Fei and Lin, Lanfen},
  year = {2022},
  month = aug,
  journal = {ACM Transactions on Knowledge Discovery from Data},
  volume = {16},
  number = {4},
  pages = {1--20},
  issn = {1556-4681, 1556-472X},
  doi = {10.1145/3494568},
  urldate = {2022-05-14},
  abstract = {Instrumental variables (IVs), sources of treatment randomization that are conditionally independent of the outcome, play an important role in causal inference with unobserved confounders. However, the existing IV-based counterfactual prediction methods need well-predefined IVs, while it's an art rather than science to find valid IVs in many real-world scenes. Moreover, the predefined hand-made IVs could be weak or erroneous by violating the conditions of valid IVs. These thorny facts hinder the application of the IV-based counterfactual prediction methods. In this article, we propose a novel Automatic Instrumental Variable decomposition (AutoIV) algorithm to automatically generate representations serving the role of IVs from observed variables (IV candidates). Specifically, we let the learned IV representations satisfy the relevance condition with the treatment and exclusion condition with the outcome via mutual information maximization and minimization constraints, respectively. We also learn confounder representations by encouraging them to be relevant to both the treatment and the outcome. The IV and confounder representations compete for the information with their constraints in an adversarial game, which allows us to get valid IV representations for IV-based counterfactual prediction. Extensive experiments demonstrate that our method generates valid IV representations for accurate IV-based counterfactual prediction.},
  langid = {english},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\Auto IV _Yuan et al 22.pdf}
}

@article{yue2022ts2vec,
  title = {{{TS2Vec}}: {{Towards Universal Representation}} of {{Time Series}}},
  shorttitle = {{{TS2Vec}}},
  author = {Yue, Zhihan and Wang, Yujing and Duan, Juanyong and Yang, Tianmeng and Huang, Congrui and Tong, Yunhai and Xu, Bixiong},
  year = {2022},
  month = jun,
  journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
  volume = {36},
  number = {8},
  pages = {8980--8987},
  issn = {2374-3468, 2159-5399},
  doi = {10.1609/aaai.v36i8.20881},
  urldate = {2022-12-01},
  abstract = {This paper presents TS2Vec, a universal framework for learning representations of time series in an arbitrary semantic level. Unlike existing methods, TS2Vec performs contrastive learning in a hierarchical way over augmented context views, which enables a robust contextual representation for each timestamp. Furthermore, to obtain the representation of an arbitrary sub-sequence in the time series, we can apply a simple aggregation over the representations of corresponding timestamps. We conduct extensive experiments on time series classification tasks to evaluate the quality of time series representations. As a result, TS2Vec achieves significant improvement over existing SOTAs of unsupervised time series representation on 125 UCR datasets and 29 UEA datasets. The learned timestamp-level representations also achieve superior results in time series forecasting and anomaly detection tasks. A linear regression trained on top of the learned representations outperforms previous SOTAs of time series forecasting. Furthermore, we present a simple way to apply the learned representations for unsupervised anomaly detection, which establishes SOTA results in the literature. The source code is publicly available at https://github.com/yuezhihan/ts2vec.},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\TS2Vec _Yue et al 2022.pdf}
}

@inproceedings{zaheer2020big,
  title = {Big Bird: {{Transformers}} for Longer Sequences},
  booktitle = {Advances in Neural Information Processing Systems},
  author = {Zaheer, Manzil and Guruganesh, Guru and Dubey, Kumar Avinava and Ainslie, Joshua and Alberti, Chris and Ontanon, Santiago and Pham, Philip and Ravula, Anirudh and Wang, Qifan and Yang, Li and Ahmed, Amr},
  editor = {Larochelle, H. and Ranzato, M. and Hadsell, R. and Balcan, M.F. and Lin, H.},
  year = {2020},
  volume = {33},
  pages = {17283--17297},
  publisher = {{Curran Associates, Inc.}},
  keywords = {⛔ No DOI found},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\Big bird _Zaheer et al 2020.pdf}
}

@inproceedings{zeiler2014visualizing,
  title = {Visualizing and Understanding Convolutional Networks},
  booktitle = {European Conference on Computer Vision},
  author = {Zeiler, Matthew D and Fergus, Rob},
  year = {2014},
  pages = {818--833},
  organization = {{Springer}},
  keywords = {⛔ No DOI found}
}

@incollection{zeiler2014visualizinga,
  title = {Visualizing and {{Understanding Convolutional Networks}}},
  booktitle = {Computer {{Vision}} \textendash{} {{ECCV}} 2014},
  author = {Zeiler, Matthew D. and Fergus, Rob},
  editor = {Fleet, David and Pajdla, Tomas and Schiele, Bernt and Tuytelaars, Tinne},
  year = {2014},
  volume = {8689},
  pages = {818--833},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-319-10590-1_53},
  urldate = {2022-07-11},
  abstract = {Large Convolutional Network models have recently demonstrated impressive classification performance on the ImageNet benchmark Krizhevsky et al. [18]. However there is no clear understanding of why they perform so well, or how they might be improved. In this paper we explore both issues. We introduce a novel visualization technique that gives insight into the function of intermediate feature layers and the operation of the classifier. Used in a diagnostic role, these visualizations allow us to find model architectures that outperform Krizhevsky et al. on the ImageNet classification benchmark. We also perform an ablation study to discover the performance contribution from different model layers. We show our ImageNet model generalizes well to other datasets: when the softmax classifier is retrained, it convincingly beats the current state-ofthe-art results on Caltech-101 and Caltech-256 datasets.},
  isbn = {978-3-319-10589-5 978-3-319-10590-1},
  langid = {english},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\Visualizing and Understanding Convolutional Networks _Zeiler_Fergus 22.pdf}
}

@inproceedings{zelenko2002kernel,
  title = {Kernel Methods for Relation Extraction},
  booktitle = {Proceedings of the {{ACL-02}} Conference on {{Empirical}} Methods in Natural Language Processing  - {{EMNLP}} '02},
  author = {Zelenko, Dmitry and Aone, Chinatsu and Richardella, Anthony},
  year = {2002},
  volume = {10},
  pages = {71--78},
  publisher = {{Association for Computational Linguistics}},
  address = {{Not Known}},
  doi = {10.3115/1118693.1118703},
  urldate = {2022-03-10},
  langid = {english},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\Kernel methods for relation extraction _Zelenko et al 22.pdf}
}

@article{zelenko2003kernel,
  title = {Kernel Methods for Relation Extraction},
  author = {Zelenko, Dmitry and Aone, Chinatsu and Richardella, Anthony},
  year = {2003},
  journal = {Journal of machine learning research},
  volume = {3},
  number = {Feb},
  pages = {1083--1106},
  keywords = {⛔ No DOI found}
}

@inproceedings{zeng2014relation,
  title = {Relation Classification via Convolutional Deep Neural Network},
  booktitle = {Proceedings of {{COLING}} 2014, the 25th International Conference on Computational Linguistics: Technical Papers},
  author = {Zeng, Daojian and Liu, Kang and Lai, Siwei and Zhou, Guangyou and Zhao, Jun},
  year = {2014},
  pages = {2335--2344},
  keywords = {⛔ No DOI found}
}

@inproceedings{zeng2015distant,
  title = {Distant Supervision for Relation Extraction via Piecewise Convolutional Neural Networks},
  booktitle = {Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing},
  author = {Zeng, Daojian and Liu, Kang and Chen, Yubo and Zhao, Jun},
  year = {2015},
  pages = {1753--1762},
  doi = {10.18653/v1/D15-1203},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\Distant supervision for relation extraction via piecewise convolutional neural _Zeng et al 22.pdf}
}

@inproceedings{zeng2018extracting,
  title = {Extracting {{Relational Facts}} by an {{End-to-End Neural Model}} with {{Copy Mechanism}}},
  booktitle = {Proceedings of the 56th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}} ({{Volume}} 1: {{Long Papers}})},
  author = {Zeng, Xiangrong and Zeng, Daojian and He, Shizhu and Liu, Kang and Zhao, Jun},
  year = {2018},
  pages = {506--514},
  publisher = {{Association for Computational Linguistics}},
  address = {{Melbourne, Australia}},
  doi = {10.18653/v1/P18-1047},
  urldate = {2022-03-07},
  langid = {english},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\Extracting Relational Facts by an End-to-End Neural Model with Copy Mechanism _Zeng et al 22.pdf}
}

@article{zeng2018large,
  title = {Large {{Scaled Relation Extraction With Reinforcement Learning}}},
  author = {Zeng, Xiangrong and He, Shizhu and Liu, Kang and Zhao, Jun},
  year = {2018},
  month = apr,
  journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
  volume = {32},
  number = {1},
  issn = {2374-3468, 2159-5399},
  doi = {10.1609/aaai.v32i1.11950},
  urldate = {2023-02-20},
  abstract = {Sentence relation extraction aims to extract relational facts from sentences, which is an important task in natural language processing field. Previous models rely on the manually labeled supervised dataset. However, the human annotation is costly and limits to the number of relation and data size, which is difficult to scale to large domains. In order to conduct largely scaled relation extraction, we utilize an existing knowledge base to heuristically align with texts, which not rely on human annotation and easy to scale. However, using distant supervised data for relation extraction is facing a new challenge: sentences in the distant supervised dataset are not directly labeled and not all sentences that mentioned an entity pair can represent the relation between them. To solve this problem, we propose a novel model with reinforcement learning. The relation of the entity pair is used as distant supervision and guide the training of relation extractor with the help of reinforcement learning method. We conduct two types of experiments on a publicly released dataset. Experiment results demonstrate the effectiveness of the proposed method compared with baseline models, which achieves 13.36\textbackslash\% improvement.},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\Large Scaled Relation Extraction With Reinforcement Learning _Zeng et al 2018.pdf}
}

@inproceedings{zeng2019learning,
  title = {Learning the Extraction Order of Multiple Relational Facts in a Sentence with Reinforcement Learning},
  booktitle = {Proceedings of the 2019 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}} and the 9th {{International Joint Conference}} on {{Natural Language Processing}} ({{EMNLP-IJCNLP}})},
  author = {Zeng, Xiangrong and He, Shizhu and Zeng, Daojian and Liu, Kang and Liu, Shengping and Zhao, Jun},
  year = {2019},
  pages = {367--377},
  doi = {10.18653/v1/D19-1035},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\Learning the extraction order of multiple relational facts in a sentence with _Zeng et al 22.pdf}
}

@inproceedings{zeng2020copymtl,
  title = {Copymtl: {{Copy}} Mechanism for Joint Extraction of Entities and Relations with Multi-Task Learning},
  booktitle = {Proceedings of the {{AAAI Conference}} on {{Artificial Intelligence}}},
  author = {Zeng, Daojian and Zhang, Haoran and Liu, Qianying},
  year = {2020},
  volume = {34},
  pages = {9507--9514},
  doi = {10.1609/aaai.v34i05.6495},
  keywords = {⛔ No DOI found},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\Copymtl _Zeng et al 22.pdf}
}

@inproceedings{zeng2020double,
  title = {Double {{Graph Based Reasoning}} for {{Document-level Relation Extraction}}},
  booktitle = {Proceedings of the 2020 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}} ({{EMNLP}})},
  author = {Zeng, Shuang and Xu, Runxin and Chang, Baobao and Li, Lei},
  year = {2020},
  pages = {1630--1640},
  doi = {10.18653/v1/2020.emnlp-main.127},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\Double Graph Based Reasoning for Document-level Relation Extraction _Zeng et al 22.pdf}
}

@inproceedings{zeng2021sire,
  title = {{{SIRE}}: {{Separate}} Intra-and Inter-Sentential Reasoning for Document-Level Relation Extraction},
  booktitle = {Findings of the Association for Computational Linguistics: {{ACL-IJCNLP}} 2021},
  author = {Zeng, Shuang and Wu, Yuting and Chang, Baobao},
  year = {2021},
  pages = {524--534},
  doi = {10.18653/v1/2021.findings-acl.47},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\SIRE _Zeng et al 22.pdf}
}

@misc{zeng2023survey,
  title = {A {{Survey}} on {{Causal Reinforcement Learning}}},
  author = {Zeng, Yan and Cai, Ruichu and Sun, Fuchun and Huang, Libo and Hao, Zhifeng},
  year = {2023},
  month = feb,
  number = {arXiv:2302.05209},
  eprint = {arXiv:2302.05209},
  publisher = {{arXiv}},
  urldate = {2023-02-15},
  abstract = {While Reinforcement Learning (RL) achieves tremendous success in sequential decision-making problems of many domains, it still faces key challenges of data inefficiency and the lack of interpretability. Interestingly, many researchers have leveraged insights from the causality literature recently, bringing forth flourishing works to unify the merits of causality and address well the challenges from RL. As such, it is of great necessity and significance to collate these Causal Reinforcement Learning (CRL) works, offer a review of CRL methods, and investigate the potential functionality from causality toward RL. In particular, we divide existing CRL approaches into two categories according to whether their causality-based information is given in advance or not. We further analyze each category in terms of the formalization of different models, ranging from the Markov Decision Process (MDP), Partially Observed Markov Decision Process (POMDP), Multi-Arm Bandits (MAB), and Dynamic Treatment Regime (DTR). Moreover, we summarize the evaluation matrices and open sources, while we discuss emerging applications, along with promising prospects for the future development of CRL.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  file = {C\:\\Users\\Hytn\\Zotero\\storage\\VN4ICGEW\\Zeng et al. - 2023 - A Survey on Causal Reinforcement Learning.pdf}
}

@article{zhang2015relation,
  title = {Relation Classification via Recurrent Neural Network},
  author = {Zhang, Dongxu and Wang, Dong},
  year = {2015},
  journal = {arXiv preprint arXiv:1508.01006},
  eprint = {1508.01006},
  archiveprefix = {arxiv},
  keywords = {⛔ No DOI found,KBP-37},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\Relation classification via recurrent neural network _Zhang_Wang 2015.pdf}
}

@inproceedings{zhang2017endtoend,
  title = {End-to-End Neural Relation Extraction with Global Optimization},
  booktitle = {Proceedings of the 2017 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}}},
  author = {Zhang, Meishan and Zhang, Yue and Fu, Guohong},
  year = {2017},
  pages = {1730--1740},
  keywords = {⛔ No DOI found}
}

@inproceedings{zhang2017positionaware,
  title = {Position-Aware Attention and Supervised Data Improve Slot Filling},
  booktitle = {Proceedings of the 2017 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}}},
  author = {Zhang, Yuhao and Zhong, Victor and Chen, Danqi and Angeli, Gabor and Manning, Christopher D.},
  year = {2017},
  pages = {35--45},
  keywords = {⛔ No DOI found}
}

@inproceedings{zhang2017positionawarea,
  title = {Position-Aware {{Attention}} and {{Supervised Data Improve Slot Filling}}},
  booktitle = {Proceedings of the 2017 {{Conference}} on {{Empirical Methods}} in {{Natural}}           {{Language Processing}}},
  author = {Zhang, Yuhao and Zhong, Victor and Chen, Danqi and Angeli, Gabor and Manning, Christopher D.},
  year = {2017},
  pages = {35--45},
  publisher = {{Association for Computational Linguistics}},
  address = {{Copenhagen, Denmark}},
  doi = {10.18653/v1/D17-1004},
  urldate = {2022-11-04},
  langid = {english},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\Position-aware Attention and Supervised Data Improve Slot Filling _Zhang et al 2017.pdf}
}

@inproceedings{zhang2017sentence,
  title = {Sentence {{Simplification}} with {{Deep Reinforcement Learning}}},
  booktitle = {Proceedings of the 2017 {{Conference}} on {{Empirical Methods}} in {{Natural}}           {{Language Processing}}},
  author = {Zhang, Xingxing and Lapata, Mirella},
  year = {2017},
  pages = {584--594},
  publisher = {{Association for Computational Linguistics}},
  address = {{Copenhagen, Denmark}},
  doi = {10.18653/v1/D17-1062},
  urldate = {2023-02-20},
  langid = {english},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\Sentence Simplification with Deep Reinforcement Learning _Zhang_Lapata 2017.pdf}
}

@inproceedings{zhang2018graph,
  title = {Graph {{Convolution}} over {{Pruned Dependency Trees Improves Relation Extraction}}},
  booktitle = {Proceedings of the 2018 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}}},
  author = {Zhang, Yuhao and Qi, Peng and Manning, Christopher D.},
  year = {2018},
  pages = {2205--2215},
  publisher = {{Association for Computational Linguistics}},
  doi = {10.18653/v1/D18-1244},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\Graph Convolution over Pruned Dependency Trees Improves Relation Extraction _Zhang et al 22.pdf}
}

@inproceedings{zhang2018interpretable,
  title = {Interpretable {{Convolutional Neural Networks}}},
  booktitle = {2018 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  author = {Zhang, Quanshi and Wu, Ying Nian and Zhu, Song-Chun},
  year = {2018},
  month = jun,
  pages = {8827--8836},
  publisher = {{IEEE}},
  address = {{Salt Lake City, UT}},
  doi = {10.1109/CVPR.2018.00920},
  urldate = {2022-03-20},
  abstract = {This paper proposes a method to modify a traditional convolutional neural network (CNN) into an interpretable CNN, in order to clarify knowledge representations in high conv-layers of the CNN. In an interpretable CNN, each filter in a high conv-layer represents a specific object part. Our interpretable CNNs use the same training data as ordinary CNNs without a need for any annotations of object parts or textures for supervision. The interpretable CNN automatically assigns each filter in a high conv-layer with an object part during the learning process. We can apply our method to different types of CNNs with various structures. The explicit knowledge representation in an interpretable CNN can help people understand the logic inside a CNN, i.e. what patterns are memorized by the CNN for prediction. Experiments have shown that filters in an interpretable CNN are more semantically meaningful than those in a traditional CNN. The code is available at https: //github.com/zqs1022/interpretableCNN .},
  isbn = {978-1-5386-6420-9},
  langid = {english},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\Interpretable Convolutional Neural Networks _Zhang et al 22.pdf}
}

@inproceedings{zhang2019ernie,
  title = {{{ERNIE}}: {{Enhanced Language Representation}} with {{Informative Entities}}},
  booktitle = {Proceedings of the 57th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}}},
  author = {Zhang, Zhengyan and Han, Xu and Liu, Zhiyuan and Jiang, Xin and Sun, Maosong and Liu, Qun},
  year = {2019},
  pages = {1441--1451},
  doi = {10.18653/v1/P19-1139},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\ERNIE _Zhang et al 22.pdf}
}

@article{zhang2020robust,
  title = {Towards {{Robust Pattern Recognition}}: {{A Review}}},
  shorttitle = {Towards {{Robust Pattern Recognition}}},
  author = {Zhang, Xu-Yao and Liu, Cheng-Lin and Suen, Ching Y.},
  year = {2020},
  month = jun,
  journal = {Proceedings of the IEEE},
  volume = {108},
  number = {6},
  pages = {894--922},
  issn = {0018-9219, 1558-2256},
  doi = {10.1109/JPROC.2020.2989782},
  urldate = {2022-07-11},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\Towards Robust Pattern Recognition _Zhang et al 22.pdf}
}

@inproceedings{zhang2021deep,
  title = {Deep {{Stable Learning}} for {{Out-Of-Distribution Generalization}}},
  booktitle = {2021 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Zhang, Xingxuan and Cui, Peng and Xu, Renzhe and Zhou, Linjun and He, Yue and Shen, Zheyan},
  year = {2021},
  month = jun,
  pages = {5368--5378},
  publisher = {{IEEE}},
  address = {{Nashville, TN, USA}},
  doi = {10.1109/CVPR46437.2021.00533},
  urldate = {2022-03-25},
  isbn = {978-1-66544-509-2},
  langid = {english},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\Deep Stable Learning for Out-Of-Distribution Generalization _Zhang et al 22.pdf}
}

@inproceedings{zhang2021document,
  title = {Document-Level Relation Extraction as Semantic Segmentation},
  booktitle = {{{IJCAI}}},
  author = {Zhang, Ningyu and Chen, Xiang and Xie, Xin and Deng, Shumin and Tan, Chuanqi and Chen, Mosha and Huang, Fei and Si, Luo and Chen, Huajun},
  year = {2021},
  doi = {10.24963/ijcai.2021/551},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\Document-level relation extraction as semantic segmentation _Zhang et al 22.pdf}
}

@inproceedings{zhang2021exploratory,
  title = {An {{Exploratory Study}} on {{Long Dialogue Summarization}}: {{What Works}} and {{What}}'s {{Next}}},
  shorttitle = {An {{Exploratory Study}} on {{Long Dialogue Summarization}}},
  booktitle = {Findings of the {{Association}} for {{Computational Linguistics}}: {{EMNLP}} 2021},
  author = {Zhang, Yusen and Ni, Ansong and Yu, Tao and Zhang, Rui and Zhu, Chenguang and Deb, Budhaditya and Celikyilmaz, Asli and Awadallah, Ahmed Hassan and Radev, Dragomir},
  year = {2021},
  pages = {4426--4433},
  publisher = {{Association for Computational Linguistics}},
  address = {{Punta Cana, Dominican Republic}},
  doi = {10.18653/v1/2021.findings-emnlp.377},
  urldate = {2022-08-20},
  langid = {english},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\An Exploratory Study on Long Dialogue Summarization _Zhang et al 2021.pdf}
}

@article{zhang2021farass,
  title = {{{FAR-ASS}}: {{Fact-aware}} Reinforced Abstractive Sentence Summarization},
  shorttitle = {{{FAR-ASS}}},
  author = {Zhang, Mengli and Zhou, Gang and Yu, Wanting and Liu, Wenfen},
  year = {2021},
  month = may,
  journal = {Information Processing \& Management},
  volume = {58},
  number = {3},
  pages = {102478},
  issn = {03064573},
  doi = {10.1016/j.ipm.2020.102478},
  urldate = {2023-02-20},
  langid = {english},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\FAR-ASS _Zhang et al 2021.pdf}
}

@techreport{zhang2021how,
  type = {Preprint},
  title = {How {{Using Machine Learning Classification}} as a {{Variable}} in {{Regression Leads}} to {{Attenuation Bias}} and {{What}} to {{Do About It}}},
  author = {Zhang, Han},
  year = {2021},
  month = may,
  institution = {{SocArXiv}},
  doi = {10.31235/osf.io/453jk},
  urldate = {2022-05-13},
  abstract = {Social scientists have increasingly been applying machine learning algorithms to  big data  to measure theoretical concepts they cannot easily measure before, and then been using these machine-predicted variables in a regression. This article  rst demonstrates that directly inserting binary predictions (i.e., classi cation) without regard for prediction error will generally lead to attenuation biases of either slope coe cients or marginal e ect estimates. We then propose four estimators to obtain consistent estimates of coe cients. The estimators require validation data, of which researchers have both machine prediction and true values. Monte Carlo simulations demonstrate the e ectiveness and robustness of the proposed estimators. We summarize the usage pattern of machine learning predictions in 18 recent publications in top social science journals, apply our proposed estimators to four of them, and o er some practical recommendations. We develop an R package CCER to help researchers use the proposed estimators.},
  langid = {english},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\How Using Machine Learning Classification as a Variable in Regression Leads to _Zhang 22.pdf}
}

@inproceedings{zhang2021open,
  title = {Open {{Hierarchical Relation Extraction}}},
  booktitle = {Proceedings of the 2021 {{Conference}} of the {{North American Chapter}} of the {{Association}} for {{Computational Linguistics}}: {{Human Language Technologies}}},
  author = {Zhang, Kai and Yao, Yuan and Xie, Ruobing and Han, Xu and Liu, Zhiyuan and Lin, Fen and Lin, Leyu and Sun, Maosong},
  year = {2021},
  pages = {5682--5693},
  publisher = {{Association for Computational Linguistics}},
  address = {{Online}},
  doi = {10.18653/v1/2021.naacl-main.452},
  urldate = {2022-09-09},
  langid = {english},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\Open Hierarchical Relation Extraction _Zhang et al 2021.pdf}
}

@inproceedings{zhang2021supporting,
  title = {Supporting {{Clustering}} with {{Contrastive Learning}}},
  booktitle = {Proceedings of the 2021 {{Conference}} of the {{North American Chapter}} of the {{Association}} for {{Computational Linguistics}}: {{Human Language Technologies}}},
  author = {Zhang, Dejiao and Nan, Feng and Wei, Xiaokai and Li, Shang-Wen and Zhu, Henghui and McKeown, Kathleen and Nallapati, Ramesh and Arnold, Andrew O. and Xiang, Bing},
  year = {2021},
  pages = {5419--5430},
  publisher = {{Association for Computational Linguistics}},
  address = {{Online}},
  doi = {10.18653/v1/2021.naacl-main.427},
  urldate = {2022-07-11},
  langid = {english},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\Supporting Clustering with Contrastive Learning _Zhang et al 22.pdf}
}

@inproceedings{zhang2022cblue,
  title = {{{CBLUE}}: {{A Chinese Biomedical Language Understanding Evaluation Benchmark}}},
  shorttitle = {{{CBLUE}}},
  booktitle = {Proceedings of the 60th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}} ({{Volume}} 1: {{Long Papers}})},
  author = {Zhang, Ningyu and Chen, Mosha and Bi, Zhen and Liang, Xiaozhuan and Li, Lei and Shang, Xin and Yin, Kangping and Tan, Chuanqi and Xu, Jian and Huang, Fei and Si, Luo and Ni, Yuan and Xie, Guotong and Sui, Zhifang and Chang, Baobao and Zong, Hui and Yuan, Zheng and Li, Linfeng and Yan, Jun and Zan, Hongying and Zhang, Kunli and Tang, Buzhou and Chen, Qingcai},
  year = {2022},
  pages = {7888--7915},
  publisher = {{Association for Computational Linguistics}},
  address = {{Dublin, Ireland}},
  doi = {10.18653/v1/2022.acl-long.544},
  urldate = {2022-11-04},
  langid = {english},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\CBLUE _Zhang et al 2022.pdf}
}

@inproceedings{zhang2022learn,
  title = {Learn to {{Adapt}} for {{Generalized Zero-Shot Text Classification}}},
  booktitle = {Proceedings of the 60th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}} ({{Volume}} 1: {{Long Papers}})},
  author = {Zhang, Yiwen and Yuan, Caixia and Wang, Xiaojie and Bai, Ziwei and Liu, Yongbin},
  year = {2022},
  pages = {517--527},
  publisher = {{Association for Computational Linguistics}},
  address = {{Dublin, Ireland}},
  doi = {10.18653/v1/2022.acl-long.39},
  urldate = {2022-11-01},
  langid = {english},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\Learn to Adapt for Generalized Zero-Shot Text Classification _Zhang et al 2022.pdf}
}

@misc{zhang2023why,
  title = {"{{Why}} Did the {{Model Fail}}?": {{Attributing Model Performance Changes}} to {{Distribution Shifts}}},
  shorttitle = {"{{Why}} Did the {{Model Fail}}?},
  author = {Zhang, Haoran and Singh, Harvineet and Ghassemi, Marzyeh and Joshi, Shalmali},
  year = {2023},
  month = feb,
  number = {arXiv:2210.10769},
  eprint = {arXiv:2210.10769},
  publisher = {{arXiv}},
  urldate = {2023-03-17},
  abstract = {Performance of machine learning models may differ between training and deployment for many reasons. For instance, model performance can change between environments due to changes in data quality, observing a different population than the one in training, or changes in the relationship between labels and features. These changes result in distribution shifts across environments. Attributing model performance changes to specific shifts is critical for identifying sources of model failures, and for taking mitigating actions that ensure robust models. In this work, we introduce the problem of attributing performance differences between environments to distribution shifts in the underlying data generating mechanisms. We formulate the problem as a cooperative game where the players are distributions. We define the value of a set of distributions to be the change in model performance when only this set of distributions has changed between environments, and derive an importance weighting method for computing the value of an arbitrary set of distributions. The contribution of each distribution to the total performance change is then quantified as its Shapley value. We demonstrate the correctness and utility of our method on synthetic, semi-synthetic, and real-world case studies, showing its effectiveness in attributing performance changes to a wide range of distribution shifts.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C\:\\Users\\Hytn\\Zotero\\storage\\YE46ZT5C\\Zhang et al. - 2023 - Why did the Model Fail Attributing Model Perfo.pdf}
}

@article{zhao2005hierarchical,
  title = {Hierarchical {{Clustering Algorithms}} for {{Document Datasets}}},
  author = {Zhao, Ying and Karypis, George and Fayyad, Usama},
  year = {2005},
  month = mar,
  journal = {Data Mining and Knowledge Discovery},
  volume = {10},
  number = {2},
  pages = {141--168},
  issn = {1384-5810, 1573-756X},
  doi = {10.1007/s10618-005-0361-3},
  urldate = {2022-09-09},
  langid = {english},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\Hierarchical Clustering Algorithms for Document Datasets _Zhao et al 2005.pdf}
}

@inproceedings{zhao2018adversarially,
  title = {Adversarially Regularized Autoencoders},
  booktitle = {Proceedings of the 35th International Conference on Machine Learning},
  author = {Zhao, Junbo and Kim, Yoon and Zhang, Kelly and Rush, Alexander and LeCun, Yann},
  editor = {Dy, Jennifer and Krause, Andreas},
  year = {2018},
  month = jul,
  series = {Proceedings of Machine Learning Research},
  volume = {80},
  pages = {5902--5911},
  publisher = {{PMLR}},
  abstract = {Deep latent variable models, trained using variational autoencoders or generative adversarial networks, are now a key technique for representation learning of continuous structures. However, applying similar methods to discrete structures, such as text sequences or discretized images, has proven to be more challenging. In this work, we propose a more flexible method for training deep latent variable models of discrete structures. Our approach is based on the recently proposed Wasserstein Autoencoder (WAE) which formalizes adversarial autoencoders as an optimal transport problem. We first extend this framework to model discrete sequences, and then further explore different learned priors targeting a controllable representation. Unlike many other latent variable generative models for text, this adversarially regularized autoencoder (ARAE) allows us to generate fluent textual outputs as well as perform manipulations in the latent space to induce change in the output space. Finally we show that the latent representation can be trained to perform unaligned textual style transfer, giving improvements both in automatic measures and human evaluation.},
  pdf = {http://proceedings.mlr.press/v80/zhao18b/zhao18b.pdf},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\Adversarially regularized autoencoders _Zhao et al 22.pdf}
}

@article{zhao2021causal,
  title = {Causal {{Interpretations}} of {{Black-Box Models}}},
  author = {Zhao, Qingyuan and Hastie, Trevor},
  year = {2021},
  month = jan,
  journal = {Journal of Business \& Economic Statistics},
  volume = {39},
  number = {1},
  pages = {272--281},
  issn = {0735-0015, 1537-2707},
  doi = {10.1080/07350015.2019.1624293},
  urldate = {2022-05-30},
  langid = {english},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\Causal Interpretations of Black-Box Models _Zhao_Hastie 22.pdf}
}

@inproceedings{zhao2021relationoriented,
  title = {A {{Relation-Oriented Clustering Method}} for {{Open Relation Extraction}}},
  booktitle = {Proceedings of the 2021 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}}},
  author = {Zhao, Jun and Gui, Tao and Zhang, Qi and Zhou, Yaqian},
  year = {2021},
  pages = {9707--9718},
  publisher = {{Association for Computational Linguistics}},
  address = {{Online and Punta Cana, Dominican Republic}},
  doi = {10.18653/v1/2021.emnlp-main.765},
  urldate = {2022-07-11},
  langid = {english},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\A Relation-Oriented Clustering Method for Open Relation Extraction _Zhao et al 22.pdf}
}

@inproceedings{zhao2021unified,
  title = {A Unified Multi-Task Learning Framework for Joint Extraction of Entities and Relations},
  booktitle = {Proceedings of the {{AAAI}} Conference on Artificial Intelligence},
  author = {Zhao, Tianyang and Yan, Zhao and Cao, Yunbo and Li, Zhoujun},
  year = {2021},
  volume = {35},
  pages = {14524--14531},
  keywords = {⛔ No DOI found},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\A unified multi-task learning framework for joint extraction of entities and _Zhao et al 22.pdf}
}

@inproceedings{zhao2022fine,
  title = {Fine- and {{Coarse-Granularity Hybrid Self-Attention}} for {{Efficient BERT}}},
  booktitle = {Proceedings of the 60th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}} ({{Volume}} 1: {{Long Papers}})},
  author = {Zhao, Jing and Wang, Yifan and Bao, Junwei and Wu, Youzheng and He, Xiaodong},
  year = {2022},
  pages = {4811--4820},
  publisher = {{Association for Computational Linguistics}},
  address = {{Dublin, Ireland}},
  doi = {10.18653/v1/2022.acl-long.330},
  urldate = {2022-06-09},
  langid = {english},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\Fine- and Coarse-Granularity Hybrid Self-Attention for Efficient BERT _Zhao et al 22.pdf}
}

@article{zhao2022legal,
  title = {Legal {{Judgment Prediction}} via {{Heterogeneous Graphs}} and {{Knowledge}} of {{Law Articles}}},
  author = {Zhao, Qihui and Gao, Tianhan and Zhou, Song and Li, Dapeng and Wen, Yingyou},
  year = {2022},
  month = feb,
  journal = {Applied Sciences},
  volume = {12},
  number = {5},
  pages = {2531},
  issn = {2076-3417},
  doi = {10.3390/app12052531},
  urldate = {2022-08-06},
  abstract = {Legal judgment prediction (LJP) is a crucial task in legal intelligence to predict charges, law articles and terms of penalties based on case fact description texts. Although existing methods perform well, they still have many shortcomings. First, the existing methods have significant limitations in understanding long documents, especially those based on RNNs and BERT. Secondly, the existing methods are not good at solving the problem of similar charges and do not fully and effectively integrate the information of law articles. To address the above problems, we propose a novel LJP method. Firstly, we improve the model's comprehension of the whole document based on a graph neural network approach. Then, we design a graph attention network-based law article distinction extractor to distinguish similar law articles. Finally, we design a graph fusion method to fuse heterogeneous graphs of text and external knowledge (law article group distinction information). The experiments show that the method could effectively improve LJP performance. The experimental metrics are superior to the existing state of the art.},
  langid = {english},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\Legal Judgment Prediction via Heterogeneous Graphs and Knowledge of Law Articles _Zhao et al 22.pdf}
}

@article{zhao2022research,
  title = {Research on a {{Decision Prediction Method Based}} on {{Causal Inference}} and a {{Multi-Expert FTOPJUDGE Mechanism}}},
  author = {Zhao, Qiang and Guo, Rundong and Feng, Xiaowei and Hu, Weifeng and Zhao, Siwen and Wang, Zihan and Li, Yujun and Cao, Yewen},
  year = {2022},
  month = jun,
  journal = {Mathematics},
  volume = {10},
  number = {13},
  pages = {2281},
  issn = {2227-7390},
  doi = {10.3390/math10132281},
  urldate = {2022-07-21},
  abstract = {Legal judgement prediction (LJP) is a crucial part of legal AI, and its goal is to predict the outcome of a case based on the information in the description of criminal facts. This paper proposes a decision prediction method based on causal inference and a multi-expert FTOPJUDGE mechanism. First, a causal inference algorithm was adopted to process unstructured text. This process did not require very much manual intervention to better mine the information in the text. Then, a neural network dedicated to each task was set up, and a neural network that simultaneously served multiple tasks was also set up. Finally, the pre-trained language model Lawformer was used to provide knowledge for downstream tasks. By using the public data set CAIL2018 and comparing it with current mainstream decision prediction models, it was shown that the model significantly improved the performance of downstream tasks and achieved great improvements in multiple indicators. Through ablation experiments, the effectiveness and rationality of each module of the proposed model were verified. The method proposed in this study achieved reasonably good performance in legal judgment prediction, which provides a promising solution for legal judgment prediction.},
  langid = {english},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\Research on a Decision Prediction Method Based on Causal Inference and a _Zhao et al 22.pdf}
}

@inproceedings{zheng2017joint,
  title = {Joint {{Extraction}} of {{Entities}} and {{Relations Based}} on a {{Novel Tagging Scheme}}},
  booktitle = {Proceedings of the 55th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}} ({{Volume}} 1: {{Long Papers}})},
  author = {Zheng, Suncong and Wang, Feng and Bao, Hongyun and Hao, Yuexing and Zhou, Peng and Xu, Bo},
  year = {2017},
  month = jul,
  pages = {1227--1236},
  publisher = {{Association for Computational Linguistics}},
  address = {{Vancouver, Canada}},
  doi = {10.18653/v1/P17-1113},
  abstract = {Joint extraction of entities and relations is an important task in information extraction. To tackle this problem, we firstly propose a novel tagging scheme that can convert the joint extraction task to a tagging problem.. Then, based on our tagging scheme, we study different end-to-end models to extract entities and their relations directly, without identifying entities and relations separately. We conduct experiments on a public dataset produced by distant supervision method and the experimental results show that the tagging based methods are better than most of the existing pipelined and joint learning methods. What's more, the end-to-end model proposed in this paper, achieves the best results on the public dataset.},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\Joint Extraction of Entities and Relations Based on a Novel Tagging Scheme _Zheng et al 22.pdf}
}

@inproceedings{zheng2021prgc,
  title = {{{PRGC}}: {{Potential Relation}} and {{Global Correspondence Based Joint Relational Triple Extraction}}},
  shorttitle = {{{PRGC}}},
  booktitle = {Proceedings of the 59th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}} and the 11th {{International Joint Conference}} on {{Natural Language Processing}} ({{Volume}} 1: {{Long Papers}})},
  author = {Zheng, Hengyi and Wen, Rui and Chen, Xi and Yang, Yifan and Zhang, Yunyan and Zhang, Ziheng and Zhang, Ningyu and Qin, Bin and Ming, Xu and Zheng, Yefeng},
  year = {2021},
  pages = {6225--6235},
  publisher = {{Association for Computational Linguistics}},
  address = {{Online}},
  doi = {10.18653/v1/2021.acl-long.486},
  urldate = {2022-04-04},
  langid = {english},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\PRGC _Zheng et al 22.pdf}
}

@inproceedings{zheng2021when,
  title = {When Does Pretraining Help?: Assessing Self-Supervised Learning for Law and the {{CaseHOLD}} Dataset of 53,000+ Legal Holdings},
  shorttitle = {When Does Pretraining Help?},
  booktitle = {Proceedings of the {{Eighteenth International Conference}} on {{Artificial Intelligence}} and {{Law}}},
  author = {Zheng, Lucia and Guha, Neel and Anderson, Brandon R. and Henderson, Peter and Ho, Daniel E.},
  year = {2021},
  month = jun,
  pages = {159--168},
  publisher = {{ACM}},
  address = {{S\~ao Paulo Brazil}},
  doi = {10.1145/3462757.3466088},
  urldate = {2022-08-10},
  isbn = {978-1-4503-8526-8},
  langid = {english},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\When does pretraining help _Zheng et al 22.pdf}
}

@article{zheng2022ai,
  title = {The {{AI Economist}}: {{Taxation}} Policy Design via Two-Level Deep Multiagent Reinforcement Learning},
  shorttitle = {The {{AI Economist}}},
  author = {Zheng, Stephan and Trott, Alexander and Srinivasa, Sunil and Parkes, David C. and Socher, Richard},
  year = {2022},
  month = may,
  journal = {Science Advances},
  volume = {8},
  number = {18},
  pages = {eabk2607},
  issn = {2375-2548},
  doi = {10.1126/sciadv.abk2607},
  urldate = {2023-03-09},
  abstract = {Artificial intelligence (AI) and reinforcement learning (RL) have improved many areas but are not yet widely adopted in economic policy design, mechanism design, or economics at large. The AI Economist is a two-level, deep RL framework for policy design in which agents and a social planner coadapt. In particular, the AI Economist uses structured curriculum learning to stabilize the challenging two-level, coadaptive learning problem. We validate this framework in the domain of taxation. In one-step economies, the AI Economist recovers the optimal tax policy of economic theory. In spatiotemporal economies, the AI Economist substantially improves both utilitarian social welfare and the trade-off between equality and productivity over baselines. It does so despite emergent tax-gaming strategies while accounting for emergent labor specialization, agent interactions, and behavioral change. These results demonstrate that two-level, deep RL complements economic theory and unlocks an AI-based approach to designing and understanding economic policy.           ,              The AI Economist finds tax policies that yield higher social welfare compared to progressive, regressive, or no taxes.},
  langid = {english},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\The AI Economist _Zheng et al 2022.pdf}
}

@inproceedings{zheng2022fewnlu,
  title = {{{FewNLU}}: {{Benchmarking State-of-the-Art Methods}} for {{Few-Shot Natural Language Understanding}}},
  shorttitle = {{{FewNLU}}},
  booktitle = {Proceedings of the 60th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}} ({{Volume}} 1: {{Long Papers}})},
  author = {Zheng, Yanan and Zhou, Jing and Qian, Yujie and Ding, Ming and Liao, Chonghua and Jian, Li and Salakhutdinov, Ruslan and Tang, Jie and Ruder, Sebastian and Yang, Zhilin},
  year = {2022},
  pages = {501--516},
  publisher = {{Association for Computational Linguistics}},
  address = {{Dublin, Ireland}},
  doi = {10.18653/v1/2022.acl-long.38},
  urldate = {2022-11-04},
  langid = {english},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\FewNLU _Zheng et al 2022.pdf}
}

@inproceedings{zhou2005exploring,
  title = {Exploring Various Knowledge in Relation Extraction},
  booktitle = {Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics (Acl'05)},
  author = {Zhou, GuoDong and Su, Jian and Zhang, Jie and Zhang, Min},
  year = {2005},
  pages = {427--434},
  keywords = {⛔ No DOI found}
}

@article{zhou2015object,
  title = {Object {{Detectors Emerge}} in {{Deep Scene CNNs}}},
  author = {Zhou, Bolei and Khosla, Aditya and Lapedriza, Agata and Oliva, Aude and Torralba, Antonio},
  year = {2015},
  month = apr,
  journal = {arXiv:1412.6856 [cs]},
  eprint = {1412.6856},
  primaryclass = {cs},
  urldate = {2022-03-20},
  abstract = {With the success of new computational architectures for visual processing, such as convolutional neural networks (CNN) and access to image databases with millions of labeled examples (e.g., ImageNet, Places), the state of the art in computer vision is advancing rapidly. One important factor for continued progress is to understand the representations that are learned by the inner layers of these deep architectures. Here we show that object detectors emerge from training CNNs to perform scene classification. As scenes are composed of objects, the CNN for scene classification automatically discovers meaningful objects detectors, representative of the learned scene categories. With object detectors emerging as a result of learning to recognize scenes, our work demonstrates that the same network can perform both scene recognition and object localization in a single forward-pass, without ever having been explicitly taught the notion of objects.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {⛔ No DOI found,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Neural and Evolutionary Computing},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\Object Detectors Emerge in Deep Scene CNNs _Zhou et al 22.pdf}
}

@inproceedings{zhou2020documentlevel,
  title = {Document-{{Level Relation Extraction}} with {{Adaptive Thresholding}} and {{Localized Context Pooling}}.},
  booktitle = {{{AAAI}}},
  author = {Zhou, Wenxuan and Huang, Kevin and Ma, Tengyu and Huang, Jing},
  year = {2020},
  pages = {14612--14620},
  keywords = {⛔ No DOI found},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\Document-Level Relation Extraction with Adaptive Thresholding and Localized _Zhou et al 22.pdf}
}

@inproceedings{zhou2020robustifying,
  title = {Towards {{Robustifying NLI Models Against Lexical Dataset Biases}}},
  booktitle = {Proceedings of the 58th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}}},
  author = {Zhou, Xiang and Bansal, Mohit},
  year = {2020},
  pages = {8759--8771},
  publisher = {{Association for Computational Linguistics}},
  address = {{Online}},
  doi = {10.18653/v1/2020.acl-main.773},
  urldate = {2022-07-29},
  langid = {english},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\Towards Robustifying NLI Models Against Lexical Dataset Biases _Zhou_Bansal 22.pdf}
}

@inproceedings{zhou2022closer,
  title = {A {{Closer Look}} at {{How Fine-tuning Changes BERT}}},
  booktitle = {Proceedings of the 60th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}} ({{Volume}} 1: {{Long Papers}})},
  author = {Zhou, Yichu and Srikumar, Vivek},
  year = {2022},
  pages = {1046--1061},
  publisher = {{Association for Computational Linguistics}},
  address = {{Dublin, Ireland}},
  doi = {10.18653/v1/2022.acl-long.75},
  urldate = {2022-10-18},
  langid = {english},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\A Closer Look at How Fine-tuning Changes BERT _Zhou_Srikumar 2022.pdf}
}

@article{zhou2022domain,
  title = {Domain {{Generalization}}: {{A Survey}}},
  shorttitle = {Domain {{Generalization}}},
  author = {Zhou, Kaiyang and Liu, Ziwei and Qiao, Yu and Xiang, Tao and Loy, Chen Change},
  year = {2022},
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  pages = {1--20},
  issn = {0162-8828, 2160-9292, 1939-3539},
  doi = {10.1109/TPAMI.2022.3195549},
  urldate = {2022-09-16},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\Domain Generalization _Zhou et al 2022.pdf}
}

@inproceedings{zhou2022flipda,
  title = {{{FlipDA}}: {{Effective}} and {{Robust Data Augmentation}} for {{Few-Shot Learning}}},
  shorttitle = {{{FlipDA}}},
  booktitle = {Proceedings of the 60th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}} ({{Volume}} 1: {{Long Papers}})},
  author = {Zhou, Jing and Zheng, Yanan and Tang, Jie and Jian, Li and Yang, Zhilin},
  year = {2022},
  pages = {8646--8665},
  publisher = {{Association for Computational Linguistics}},
  address = {{Dublin, Ireland}},
  doi = {10.18653/v1/2022.acl-long.592},
  urldate = {2022-10-18},
  langid = {english},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\FlipDA _Zhou et al 2022.pdf}
}

@article{zhu2004recall,
  title = {Recall, Precision and Average Precision},
  author = {Zhu, Mu},
  year = {2004},
  journal = {Department of Statistics and Actuarial Science, University of Waterloo, Waterloo},
  volume = {2},
  number = {30},
  pages = {6},
  keywords = {⛔ No DOI found},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\Recall, precision and average precision _Zhu 22.pdf}
}

@inproceedings{zhu2020hierarchical,
  title = {A {{Hierarchical Network}} for {{Abstractive Meeting Summarization}} with {{Cross-Domain Pretraining}}},
  booktitle = {Findings of the {{Association}} for {{Computational Linguistics}}: {{EMNLP}} 2020},
  author = {Zhu, Chenguang and Xu, Ruochen and Zeng, Michael and Huang, Xuedong},
  year = {2020},
  pages = {194--203},
  publisher = {{Association for Computational Linguistics}},
  address = {{Online}},
  doi = {10.18653/v1/2020.findings-emnlp.19},
  urldate = {2022-08-20},
  langid = {english},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\A Hierarchical Network for Abstractive Meeting Summarization with Cross-Domain _Zhu et al 2020.pdf}
}

@inproceedings{zhu2021twag,
  title = {{{TWAG}}: {{A Topic-Guided Wikipedia Abstract Generator}}},
  shorttitle = {{{TWAG}}},
  booktitle = {Proceedings of the 59th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}} and the 11th {{International Joint Conference}} on {{Natural Language Processing}} ({{Volume}} 1: {{Long Papers}})},
  author = {Zhu, Fangwei and Tu, Shangqing and Shi, Jiaxin and Li, Juanzi and Hou, Lei and Cui, Tong},
  year = {2021},
  pages = {4623--4635},
  publisher = {{Association for Computational Linguistics}},
  address = {{Online}},
  doi = {10.18653/v1/2021.acl-long.356},
  urldate = {2022-05-23},
  langid = {english},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\TWAG _Zhu et al 22.pdf}
}

@inproceedings{zhuang2022longrange,
  title = {Long-Range {{Sequence Modeling}} with {{Predictable Sparse Attention}}},
  booktitle = {Proceedings of the 60th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}} ({{Volume}} 1: {{Long Papers}})},
  author = {Zhuang, Yimeng and Zhang, Jing and Tu, Mei},
  year = {2022},
  pages = {234--243},
  publisher = {{Association for Computational Linguistics}},
  address = {{Dublin, Ireland}},
  doi = {10.18653/v1/2022.acl-long.19},
  urldate = {2022-06-15},
  langid = {english},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\Long-range Sequence Modeling with Predictable Sparse Attention _Zhuang et al 22.pdf}
}

@incollection{ziyu2021estimating,
  title = {Estimating {{Treatment Effect}} via {{Differentiated Confounder Matching}}},
  booktitle = {Artificial {{Intelligence}}},
  author = {Ziyu, Zhao and Kuang, Kun and Wu, Fei},
  editor = {Fang, Lu and Chen, Yiran and Zhai, Guangtao and Wang, Jane and Wang, Ruiping and Dong, Weisheng},
  year = {2021},
  volume = {13069},
  pages = {689--699},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-030-93046-2_58},
  urldate = {2022-05-14},
  isbn = {978-3-030-93045-5 978-3-030-93046-2},
  langid = {english},
  file = {C\:\\Users\\Hytn\\Desktop\\BaiduNetdiskWorkspace\\zotero\\Estimating Treatment Effect via Differentiated Confounder Matching _Ziyu et al 22.pdf}
}
