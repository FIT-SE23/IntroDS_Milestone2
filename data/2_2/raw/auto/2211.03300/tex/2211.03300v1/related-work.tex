\begin{figure*}
\centering
\includegraphics[width=.9\textwidth]{pic/hfedms-framework.pdf}
\caption{The overview of our HFedMS framework.
}
\label{fig:framework}
\end{figure*}

\section{Related Work}
\textbf{Industrial Metaverse and Enabling Technologies.}
The emerging concept of Metaverse\cite{xu2022full} allows for immersive interaction and collaboration between virtual and real worlds. This paradigm is most likely to be adopted by the industrial manufacturing field first, as \textit{Industrial Metaverse}\cite{li2022internet} has a great opportunity to break through the bottlenecks encountered by manufacturers, and is expected to increase productivity and provide better services in the future Internet of Everything. As its four pillar technologies (Figure \ref{fig:fl-assisted-metaverse}), \textit{Digital Twins}\cite{el2018digital,han2022dynamic} collects real-time data from the real-world industrial environment and creates digital avatars of machines and managers in the virtual world. \textit{AI} (especially FL and Blockchain)\cite{kang2022blockchain,jeon2022blockchain,yang2022fusing,chang20226g} learn from these digital data and assist managers in making decisions or executing automated controls, where FL and Blockchain provide privacy and security guarantees respectively. In addition, actions performed in the virtual world will also be synchronized with the real world in real time. \textit{5G/6G High-Speed Communication and Scheduling} and \textit{Mobile Edge Computing}\cite{chang20226g,wang2022mobile,du2022exploring,du2022attention,liu2022slicing4meta} infrastructures guarantee low latency and high reliability for real-time data stream synchronization, aiming to provide users with seamless and immersive interactive experiences.

\textbf{Federated Learning and Heterogeneity.}
Federated Learning (FL), emerged as an advanced distributed ML paradigm that addresses data privacy breaches when working with a large number of smart edge devices, has become an indispensable underlying foundation for Metaverse. A lot of work has been done in FL in terms of efficiency\cite{li2020esync}, privacy\cite{yin2021comprehensive}, security\cite{li2021byzantine,xie2022securing}, etc., but statistical heterogeneity still remains an open problem. In \cite{zhao2018federated}, the authors found that the drop in FL accuracy is caused by the divergence in class distributions between clients (i.e., non-i.i.d.), which can reach a 55\% drop in the worst case. It motivated some efforts\cite{zhao2018federated,jeong2018communication,duan2019astraea} to balance local class distribution by sharing and augmenting data. In \cite{li2020federated}, a proximal penalty term was added to the local loss function to constrain local models. Feature fusion\cite{yao2018two,yao2019towards}, weighted aggregation\cite{yeganeh2020inverse}, client selection \cite{wang2020optimizing}, and adaptive optimizers\cite{reddi2020adaptive} are also promising directions in promoting convergence on non-i.i.d. data. Moreover, the authors in \cite{duan2019astraea,zeng2022heterogeneous,li2022data,li2022fedgs} proposed clustering-based approaches to remove heterogeneity between groups, with best-fit grouping strategies\cite{duan2019astraea,zeng2022heterogeneous} and unique client selection\cite{li2022data,li2022fedgs}.

\textbf{Online and Continual Learning.}
Online learning\cite{hoi2021online} is an ML technology that continuously learns new knowledge from a continuous stream of data (e.g., industrial sensing data), rather than requiring the entire dataset to be prepared in advance as in traditional offline learning. It is a natural trend to combine online learning and FL such as in \cite{han2020adaptive,zhou2019privacy,chen2020asynchronous}, however, the original purpose of online learning did not take into account catastrophic forgetting, where ML models may forget previously learned knowledge in later learning. This phenomenon was first discovered in the study of connectionist networks\cite{french1999catastrophic} and has received increasing attention in recent years\cite{rebuffi2017icarl,pellegrini2020latent,van2020brain,iscen2020memory}. To distinguish it from online learning, continual learning\cite{de2021continual} (also incremental learning) was proposed as a subset of online learning that deals with catastrophic forgetting. For brevity, here we only discuss one of the mainstream directions, that is, data replay. A naive idea is to store the raw HD data and reuse it in later training\cite{rebuffi2017icarl}, but due to the small storage capacity of sensors, doing so will limit the learning ability and put high storage pressure on industrial devices. To this end, lighter and lower-dimensional information should be stored and used for replay. The authors in \cite{pellegrini2020latent} stored the activation map at some intermediate layer, and in \cite{van2020brain} used a standard variational autoencoder to recover past data from stored features. Nonetheless, as training progresses, the stored features may become outdated, but they did not incorporate compensation module to bridge this gap. In \cite{iscen2020memory}, an additional multi-layer perception was used to compensate for outdated features, but this approach increases model complexity and places more burden on devices. 

\begin{algorithm*}
\normalem
\caption{Heterogeneous Federated Learning with Memorable Semantics (\NAME)}\label{alg:hfedms}
Server initializes the global model parameters $\omega^{0}$; \\
Server creates a training scheduler: $\text{STP}\gets\Call{\textbf{Sequential-to-Parallel-Scheduler}}{ }$; \\
\For{each round $r=1,\cdots,R$}{
\% Full synchronization round. \\
\If{every $T$ rounds}{
$\Call{STP.enable\_grouping\_func}{\textsc{\textbf{Inter-Cluster-Grouping}}}$; \\
$\Call{STP.set\_sync\_mode}{\texttt{FULL\_SYNC}}$;  \% \textbf{LASP}: Synchronize the full model parameters. \\
}
\% Calibration round. \\
\Else{
$\Call{STP.enable\_calibration\_func}{\textsc{\textbf{Semantic-Compression-and-Compensation}}}$; \\
$\Call{STP.set\_sync\_mode}{\texttt{PART\_SYNC}}$;  \% \textbf{LASP}: Synchronize only classifier parameters.
}
$\omega^{r}\gets\Call{STP.run}{r}$;
}
\Return $\omega^{r}$;
\end{algorithm*}

In this work, we adopt \cite{li2020federated,yao2018two,yao2019towards,yeganeh2020inverse,reddi2020adaptive} as comparison benchmarks and use ablation experiments to explain why \cite{duan2019astraea,zeng2022heterogeneous} is not applicable for our highly dynamic case. Then, we propose a novel replay mechanism that stores lightweight semantic features and leverages a \textit{low-footprint} compensation function to calibrate classifier parameters. As another important feature, our approach \textit{reduces considerable traffic without any loss of information}. These features are invaluable and superior to commonly used compression methods that can compromise model accuracy (e.g., sparsification\cite{lin2017deep} and quantization\cite{seide20141}) or slow down training (e.g., low-rank decomposition\cite{liu2015l_}).