\section{Experiments and Results}\label{section:experiment}
\subsection{Experimental Setup}
We built the experimental platform on $K=368$ simulated devices and used the most common FEMNIST dataset\cite{caldas2018leaf} as the benchmark to evaluate the performance. FEMNIST is a non-i.i.d. dataset designed for FL, which contains 3,550 clients with 805,263 character samples divided according to a non-i.i.d. and non-uniform distribution. 

For resource-constrained industrial devices, we used a lightweight CNN network composed of two convolutional layers and two fully connected layers with a total of $\mathcal{M}=6.68\times 10^6$ parameters, where the classifier has about $\mathcal{M}_c=6.3\times 10^3$ parameters (see Table \ref{table:params-statistics}). 

The simulated devices use the standard mini-batch SGD to train their local models. Detailed hyperparameter settings are summarized in Table \ref{table:hyperparams}. The values of $T,f,\alpha,\beta$ will be further tuned in experiments to explore their impact on performance.

\begin{table}[t]
  \caption{Summary of main hyperparameter settings.}
  \small
  \label{table:hyperparams}
  \centering
  \renewcommand{\arraystretch}{1.2}
  \begin{tabular}{c|c|c}
    \hline
    \textbf{Definition} & \textbf{Symbol} & \textbf{Value} \\
    \hline\hline
    Number of devices & $K$ & 368 \\
    \hline
    Local learning rate & $\eta$ & 0.01 \\
    \hline
    Batch size & $b$ & 5 \\
    \hline
    Local epoch & $e$ & 1 \\
    \hline
    Maximum training rounds & $R$ & 500 \\
    \hline
    Interval of rounds & $T$ & 5 \\
    \hline
    Storage capacity of semantic database & $Q$ & 200 \\
    \hline
    Group sampling rate & $\kappa$ & 0.3 \\
    \hline
    Group growth function & $f(\cdot)$ & \texttt{LOG} \\
    \hline
    Coefficients for $f(\cdot)$ & $\alpha;\beta$ & 2; 10 \\
    \hline
    Size of streaming data batches & $n$ & 50 \\
    \hline
    Num of parameters for the full model & $\mathcal{M}$ & $6.68\times 10^6$ \\
    \hline
    Num of parameters for the classifier & $\mathcal{M}_c$ & $6.3\times 10^3$ \\
    \hline
  \end{tabular}
\end{table}

The experiments not only evaluate the performance of \NAME~on streaming non-i.i.d. data, but also the performance on traditional static non-i.i.d. data. 
Below we briefly introduce the difference between these two settings:
\begin{itemize}
\item \textbf{\textit{HFedMS-S}}: This setting enables only two features, that is, STP$+$ICG, and is suitable for processing static non-i.i.d. data. In this setup, the local data is fixed and the same dataset is used every round.
\item \textbf{\textit{HFedMS-D}}: This setting has all features enabled, that is, STP$+$ICG$+$SCC$+$LASP, and is suitable for processing dynamic streaming non-i.i.d. data. To generate streaming data, the client draws $n=50$ pieces of data from its local dataset each round and converts them into new samples using augmentation techniques.
\end{itemize}

Please note that the scheme in our previous work\cite{zeng2022heterogeneous} is the same as \NAME-S. The code implementation of this work is open available at \url{https://github.com/slz-ai/hfedms}.

\subsection{Results and Discussion}
\vspace{1mm}
\textbf{(a) Performance comparison with benchmarks.} First, we evaluate the overall performance (i.e., test accuracy and loss) and efficiency (i.e., training time and communication load) of our \NAME~under both streaming and static data settings, and compare it with several benchmarks to highlight its superiority. The benchmarks include FedAvg\cite{mcmahan2017communication}, FedProx\cite{li2020federated}, FedMMD\cite{yao2018two}, FedFusion\cite{yao2019towards}, IDA\cite{yeganeh2020inverse}, FedAdagrad, FedAdam, and FedYogi\cite{reddi2020adaptive}. The first is the vanilla algorithm for FL, and the rest are some state-of-the-art solutions proposed for non-i.i.d. data. All the benchmarks use the maximum training rounds $R=500$. For a fair comparison, our \NAME-D and \NAME-S also use $R=500$, that is, 100 full synchronization rounds and 400 calibration rounds in \NAME-D, and 500 full synchronization rounds in \NAME-S.

\begin{table}[t]
  \caption{Performance comparison with several benchmarks.}
  \small
  \label{table:hfedms-vs-others}
  \centering
  \renewcommand{\arraystretch}{1.2}
  \begin{tabular}{c|c|c|c|c} 
    \hline
    \multirow{2}{*}{\textbf{Algorithm}} & \multicolumn{2}{c|}{\textbf{Stream non-i.i.d. data}} & \multicolumn{2}{c}{\textbf{Static non-i.i.d. data}}  \\ 
    \cline{2-5}
    & \textbf{Acc} & \textbf{Loss} & \textbf{Acc} & \textbf{Loss} \\ 
    \hline\hline
    FedAvg & 70.1\% & 1.037 & 80.1\% & 0.602 \\
    FedProx & 70.3\% & 1.031 & 78.7\% & 0.633 \\
    FedMMD & 75.6\% & 0.821 & 81.7\% & 0.587 \\
    FedFusion & 72.6\% & 0.951 & 82.4\% & 0.554 \\
    IDA & 72.1\% & 1.003 & 82.0\% & 0.567 \\
    FedAdagrad & 77.4\% & 0.769 & 81.9\% & 0.582 \\
    FedAdam & 77.8\% & 0.730 & 82.1\% & 0.566 \\
    FedYogi & 75.8\% & 0.803 & 83.2\% & 0.543 \\ 
    \hline\hline
    \textbf{Algorithm} & \multicolumn{2}{c|}{\textbf{\NAME-D}} & \multicolumn{2}{c}{\textbf{\NAME-S}} \\
    \hline
    500 Rounds & \textbf{84.2\%} & \textbf{0.494} & \textbf{85.4\%} & \textbf{0.453} \\
    2141 Rounds & \textbf{85.9\%} & \textbf{0.401} & $\times$ & $\times$ \\
    \hline
  \end{tabular}
\end{table}

\begin{table}[t]
  \caption{Comparison of transfer bytes and training time.}
  \small
  \label{table:hfedms-efficiency}
  \centering
  \renewcommand{\arraystretch}{1.2}
  \begin{tabular}{c|c|c|c|c} 
    \hline
    \textbf{Algorithm} & \textbf{$\mathbf{T}$} & \textbf{$\mathbf{R}$} & \textbf{Transfer Bytes} & \textbf{Runtime} \\ 
    \hline\hline
    FedAvg & $\times$ & 490 & 2.629TB & 1262h \\ 
    \hline
    \NAME-S & 1 & 32 & 0.172TB & 82h \\
    \hline
    \multirow{4}{*}{\textbf{\NAME-D}} & 3 & 36 & 0.097TB & 42h \\
    & \textbf{5} & 34 & \textbf{0.056TB} & \textbf{25h} \\
    & 7 & 67 & 0.081TB & 35h \\
    & 9 & 81 & 0.073TB & 32h \\
    \hline
  \end{tabular}
\end{table}

Table \ref{table:hfedms-vs-others} gives the numerical results. It can be seen that \NAME~achieves significant performance improvements under both the stream and static non-i.i.d. data settings. For the static non-i.i.d. data, \NAME-S improves the benchmark accuracy by 2.2\%$\sim$6.7\%. Nevertheless, this is not all of \NAME, as \NAME-S does not have SCC and LASP enabled. The more advanced \NAME-D achieves a more significant breakthrough underlying streaming non-i.i.d. data. In this setting, \NAME-D outperforms the benchmarks by 6.4\%$\sim$14.1\% and maintains a satisfactory accuracy of 84.2\%, while the benchmark accuracy drops by 4.3\%$\sim$10\%. Nonetheless, \NAME-D has not converged to the optimum, and if we continue to train \NAME-D until convergence, we can obtain a better result of 85.9\%. These results demonstrate that \NAME-S is strongly robust to data heterogeneity, and its advanced version \NAME-D is also very suitable for processing dynamic streaming data.

Table \ref{table:hfedms-efficiency} compares the communication load and runtime efficiency when FedAvg and \NAME~reach the accuracy of 70\%. For \NAME, we take the interval $T=\{1,3,5,7,9\}$. It should be noted that \NAME-D with $T=1$ is actually \NAME-S. Eqs. \eqref{eq:hfedms-s-traffic}-\eqref{eq:hfedms-d-traffic} are used to count the total traffic,
\begin{align}
\text{\NAME-S:~}& 8\kappa K\mathcal{M}R, \label{eq:hfedms-s-traffic} \\
\text{\NAME-D:~}& 4\kappa K(3\mathcal{M}\lceil\frac{R}{T}\rceil+2(R-\lceil\frac{R}{T}\rceil)\mathcal{M}_c). \label{eq:hfedms-d-traffic}
\end{align}
Please note that Eqs. \eqref{eq:hfedms-s-traffic}-\eqref{eq:hfedms-d-traffic} do not take ICG into consideration since its traffic $4K\mathcal{F}\lceil R/T \rceil$ only accounts for a few hundred kilobytes and can be ignored. For the time cost, the runtime mainly consists of computational time and communication time, of which communication accounts for the majority due to the bandwidth bottleneck of LPWAN, while computation only takes a few minutes and can be ignored. The communication time was simulated using LTE Cat M2 with peak data rates of 4Mbps and 7Mbps for uplink and downlink, respectively. The results show that, regardless of the value of $T$, our \NAME~transfers far fewer bytes and runs in far less time than FedAvg, especially \NAME-D. The traffic was reduced by 90\%$\sim$98\% and the runtime was reduced by 93\%$\sim$98\%. This feature benefits from a cliff-like reduction in the training round $R$, about 83\%$\sim$93\% fewer.

To sum up, our \NAME~has four advantages: \textit{higher accuracy}, \textit{faster convergence}, \textit{less runtime}, and \textit{lower communication costs}, which makes it highly competitive in harsh industrial applications.
In follow-up experiments, we performed ablation experiments on \NAME~to help understand why it achieves such performance.

\vspace{1mm}
\textbf{(b) Effects of ICG and STP.}
We first define a benchmark algorithm for ablation experiments. This benchmark also divides clients into multiple groups and uses sequential training within each group and parallel training between groups. The difference is that the number and members of groups are fixed, like Astraea\cite{duan2019astraea}. Then, we apply ICG to this static benchmark for proper and fast grouping and refer it as ICG. Finally, STP is applied to make it dynamic, and it is now \NAME-S.

Figure \ref{fig:icg} compares the class probability distances (CPDs) of FedAvg, Benchmark and ICG, where CPD is defined as the kernel two-sample estimation with Gaussian radial basis kernel $\mathcal{K}$,
\begin{align}
\nonumber
&\mathrm{CPD}(m_1,m_2)=\mathrm{MMD}^2(\mathcal{X}, \mathcal{Y}) \\
\nonumber
&= \mathbb{E}_{x,x'\sim\mathcal{X}}\left[\mathcal{K}(x,x')\right]-2\mathbb{E}_{x\sim\mathcal{X},y\sim\mathcal{Y}}\left[\mathcal{K}(x,y)\right]\\
\nonumber
&+\mathbb{E}_{y,y'\sim\mathcal{Y}}\left[\mathcal{K}(y,y')\right],
\end{align}
and $\mathcal{X}=\mathrm{norm}(\mathcal{V}_{m_1}),\mathcal{Y}=\mathrm{norm}(\mathcal{V}_{m_2})$ are normalized class probability distributions of two groups $\mathcal{G}_{m_1}, \mathcal{G}_{m_2}$. CPD quantifies the difference in the class probability distribution between two clients (or groups). In general, the smaller the CPD, the less heterogeneity between two clients (or groups), and thus the better the grouping strategy. In Figure \ref{fig:icg}, the CPD between every pair of clients (or groups) is counted, and the results show a remarkable reduction in CPD for our ICG, with the median value (with orange line) about 1/4 that of FedAvg and 2/5 that of Benchmark.

\begin{figure}[t]
\centering
\includegraphics[width=0.25\textwidth]{pic/class-prob-dist1.pdf}
\caption{Comparison of class probability distance.}
\label{fig:icg}
\end{figure}

\begin{figure}[t]
\centering
\subfloat[]{\includegraphics[width=.245\textwidth]{pic/acc-curve-compare.pdf}
\label{fig:acc-curve}}
\subfloat[]{\includegraphics[width=0.255\textwidth]{pic/loss-curve-compare.pdf}
\label{fig:loss-curve}}
\caption{Comparison of performance curves on static non-i.i.d. data.}
\label{fig:curve-on-static}
\end{figure}

\begin{figure*}
\centering
\subfloat[]{\includegraphics[width=.24\textwidth]{pic/heatmap-linear.pdf}
\label{fig:hfedms-s-heatmap-linear}}
\subfloat[]{\includegraphics[width=.24\textwidth]{pic/heatmap-log.pdf}
\label{fig:hfedms-s-heatmap-log}}
\subfloat[]{\includegraphics[width=.24\textwidth]{pic/heatmap-exp.pdf}
\label{fig:hfedms-s-heatmap-exp}}
\subfloat[]{\includegraphics[width=.26\textwidth]{pic/Function.pdf}
\label{fig:hfedms-s-group-curve}}
\caption{Test loss heatmaps on (a) linear, (b) logarithmic, (c) exponential growth functions, and (d) curves of group status. The green area indicates the recommended range of $\alpha,\beta$ values.}
\label{fig:heatmap}
\end{figure*}

Figure \ref{fig:acc-curve} shows that native Benchmark converges significantly faster than vanilla FedAvg, while ICG further improves accuracy. This proves that constructing groups with homogeneous data distribution can indeed resist the negative effects of data heterogeneity. But surprisingly, Benchmark and ICG show diametrically opposite performances on the loss metric, that is, overfitting, as shown in Figure \ref{fig:loss-curve}. This is reasonable because of the static nature of Benchmark (i.e., the number and members of groups are fixed), which makes it possible for the FL model to learn some interfering information, such as the order in which clients perform sequential training. Therefore, \NAME-S was proposed to solve this problem, by introducing STP to make it dynamic. For example, regrouping clients, increasing the number of groups, and shuffling clients. The blue curves in Figures \ref{fig:acc-curve} and \ref{fig:loss-curve} perfectly overcome overfitting and converge to a higher accuracy of 85.4\%.

\vspace{1mm}
\textbf{(c) Effects of growth function and its coefficients.} 
The growth function $f(\cdot)$ and its coefficients $\alpha,\beta$ play an important role in STP. To investigate their effects, we conduct a grid search on $f=\{\texttt{LINEAR},\texttt{LOG},\texttt{EXP}\}$ and $\alpha,\beta$. Figures \ref{fig:hfedms-s-heatmap-linear}-\ref{fig:hfedms-s-heatmap-exp} gives the test loss heatmaps on \NAME-S and shows that the logarithmic growth function is most favored, where the loss value reaches the minimum 0.453 when $\alpha=2,\beta=10$. In fact, it is not good for the number of groups $M$ to grow too fast or too slowly. A slow increase in $M$ causes sequential training to dominate and leads to overfitting. Conversely, a rapid increase in $M$ causes \NAME-S to prematurely degenerate into FedAvg and suffer from data heterogeneity. Therefore, we recommend $\alpha\beta$ to be a moderate value, such as the green area in Figure \ref{fig:hfedms-s-heatmap-log}. In Figure \ref{fig:hfedms-s-group-curve}, we plot the actual curves of the number of groups and the number of clients within each group. An interesting finding is that, \NAME~prefers small groups with only 2$\sim$10 clients.

\begin{figure}[t]
\centering
\subfloat[]{\includegraphics[width=.255\textwidth]{pic/Memory.pdf}
\label{fig:scc-gain}}
\subfloat[]{\includegraphics[width=.245\textwidth]{pic/SCC.pdf}
\label{fig:scc-compensate}}
\caption{(a) The gain in accuracy and (b) accuracy curves before and after applying SCC.}
\label{fig:scc-curves}
\end{figure}

\vspace{1mm}
\textbf{(d) Effects of SCC.}
Although \NAME-S has shown exciting performance, it is still not enough to handle streaming non-i.i.d. data. In Table \ref{table:hfedms-efficiency}, \NAME-S and \NAME-D(T=5) achieve 70\% accuracy after similar rounds, however, this is an unfair comparison for \NAME-D as it runs only 7 full synchronization rounds, while the remaining 27 rounds of calibration only update the classifier. For a fair comparison, we use the total number of updated parameters as the x-axis to compare the convergence performance. From Figure \ref{fig:lasp-curves}, we can see that \NAME-D converges much faster on streaming non-i.i.d. data. This is expected as \NAME-S is not optimized for the learning forgetting problem, which is also our motivation to introduce SCC and propose a more advanced \NAME-D.

Figure \ref{fig:scc-gain} illustrates the phenomenon of learning forgetting. When we learn a batch of data in a certain round, in the subsequent rounds, the accuracy curve of \NAME-S on this data batch drops rapidly since these data no longer appear, this phenomenon is called learning forgetting. Instead, \NAME-D with SCC enabled maintains the accuracy well, and the accuracy gain achieved by resisting forgetting becomes more pronounced as the round progresses.

\begin{figure}[t]
\centering
\subfloat[]{\includegraphics[width=0.25\textwidth]{pic/Pcompare.pdf}
\label{fig:lasp-acc}}
\subfloat[]{\includegraphics[width=0.25\textwidth]{pic/Pcompare-loss.pdf}
\label{fig:lasp-loss}}
\caption{Comparison of (a) accuracy and (b) loss curves for different round intervals.}
\label{fig:lasp-curves}
\end{figure}

Figure \ref{fig:scc-compensate} also shows the effectiveness of SCC in improving overall performance. As expected, \NAME-D with SCC enabled converges faster and better than \NAME-S with SCC disabled, and it is also closest to the upper bound (in green line), which can store all the raw data and compensate the historical semantics accurately. Besides, we have an interesting finding that if we directly calibrate classifier with stored historical semantics without compensation (in red line), we would obtain worse result than that of \NAME-S with SCC disabled. This finding demonstrates the necessity and effectiveness of our compensation module.

\vspace{1mm}
\textbf{(e) Effects of LASP.} It is time-consuming to synchronize all parameters in all rounds in resource-constrained industrial networks. For example, in Table \ref{table:hfedms-efficiency}, FedAvg takes 1262 hours to achieve 70\% accuracy, and even \NAME-S takes 82 hours. To alleviate the communication bottleneck, LASP blocks the transmission of redundant parameters and saves 999\textperthousand~of transfer bytes in calibration rounds (see Table \ref{table:params-statistics}). For overall performance, as shown in Table \ref{table:hfedms-efficiency}, LASP-enabled \NAME-D achieves up to 67\% savings in transfer bytes compared to \NAME-S, and thus reduces runtime by nearly 70\%. For the setting of the round interval $T$, the overall performance reaches the best when $T=5$, at this time, the FL system has the highest accuracy, the shortest runtime, the fewest rounds, and the lowest communication cost. 

\vspace{1mm}
\textbf{(f) \NAME-S vs \NAME-D.} Figure \ref{fig:lasp-curves} shows the state-of-the-art of \NAME-D in terms of convergence performance when dealing with streaming non-i.i.d. data. For \NAME-D, the curve contains and alternates sharp- and slow-rising curves. The slow-rising curves indicate that the algorithm runs in full synchronization rounds, and the sharp-rising curves run in calibration rounds. In general, \NAME-D works much better than \NAME-S, with significantly faster convergence and higher accuracy. This is intuitively due to the jump in accuracy in calibration rounds (the sharp-rising curves), and it also proves that SCC plays a pivotal role in improving performance.