\section{Results and Discussion}

In this section, we provide and discuss the detailed benchmark results on the efficiency of DGL and PyG.

\begin{figure}[t]
	\vspace{0mm}
	\centering
	\includegraphics[width=0.95\linewidth, trim=2mm 1mm 2mm 1mm, clip]{fig/workflow}
	\vspace{0mm}
	\caption{Workflow of sampling-based GNN training.}
	\vspace{0mm}
	\label{fig:workflow}
\end{figure}

\subsection{Functional Testing}

Figure~\ref{fig:workflow} illustrates the end-to-end workflow of training a sampling-based GNN with mini-batch training. It can be divided into the following three main processes: data loading, graph sampling, and model training. We thus conduct `functional testing' on each main process to evaluate the performance of DGL and PyG. Note that for the entire training process, data loading is a one-time operation while the other two process, i.e., graph sampling and model training, are performed repeatedly and periodically for each training batch. Note also that we do not consider the inference of each model in this paper. We repeat the experiments for each functional test for ten times and report the average values. In addition, for the functional tests, we do not include the power/energy consumption results since the runtime of some functions are too small, e.g., a few milliseconds, which can lead to incorrect power/energy measurement.

\vspace{1mm}
\noindent \textbf{Data loader.} We first compare the data loader of DGL and PyG, which is used to load the input graph and its associated node features from storage and to create a library-specific graph object for the next process of graph sampling and model training. We present the runtime results in Figure~\ref{fig:loader}.

\vspace{1mm}

\noindent \textbf{Observation 1:} \textit{PyG's data loader is more efficient and user-friendly than DGLâ€™s data loader.}

\vspace{1mm}

There are two main reasons. First, while both frameworks provide an easy-to-use interface to create and process the datasets, PyG integrates more datasets (around 80) into its library as compared with DGL (around 40). Specifically, five out of six datasets used in this work can be directly accessed from PyG's `dataset' module while three datasets are already included in DGL. Note that, for the datasets that are not included in the libraries, we follow the official instructions to process the raw datasets and to create their corresponding graph objects. Second, DGL uses a graph-centric programming abstraction, which makes rich information of the input graph accessible and enables full control of manipulating the input graph. As a consequence, the workload of creating a `DGLGraph' object is relatively higher than its counterpart in PyG.

\begin{figure}[t]
	\vspace{0mm}
	\centering
	\includegraphics[width=0.7\linewidth, trim=2mm 1mm 2mm 1mm, clip]{fig/loader}
	\vspace{0mm}
	\caption{Runtime of data loader.}
	\vspace{0mm}
	\label{fig:loader}
	\vspace{-3mm}
\end{figure}

\begin{figure}[t]
	\captionsetup[subfloat]{captionskip=1pt}
	\centering
	\subfloat[Neighborhood sampler]{%
		\includegraphics[width=0.47\linewidth, trim=0cm 0cm 0cm 0cm, clip]{fig/neighbor-sampler}
	}
	\hspace{0mm}
	\subfloat[GraphSAINT sampler]{%
		\includegraphics[width=0.47\linewidth, trim=0cm 0cm 0cm 0cm, clip]{fig/saint-sampler}
	}
	\vspace{0mm}
		\subfloat[ClusterGCN sampler: METIS]{%
		\includegraphics[width=0.47\linewidth, trim=0cm 0cm 0cm 0cm, clip]{fig/cluster-sampler-METIS}
	}
	\hspace{0mm}
	\subfloat[ClusterGCN sampler: Combining]{%
		\includegraphics[width=0.47\linewidth, trim=0cm 0cm 0cm 0cm, clip]{fig/cluster-sampler}
	}
	\vspace{0mm}
	\caption{Runtime comparison of graph samplers. Note that the range of y-axis is different across different figures.}
	\label{fig:sampler}
	\vspace{-3mm}
\end{figure}

\begin{figure*}[t]
	\vspace{0mm}
	\captionsetup[subfloat]{captionskip=1pt}
	\centering
	\subfloat[GCNConv-CPU]{%
		\includegraphics[width=0.23\linewidth, trim=0cm 0cm 0cm 0cm, clip]{fig/GCNConv-CPU}
	}
	\hspace{0mm}
	\subfloat[GCNConv-GPU]{%
		\includegraphics[width=0.23\linewidth, trim=0cm 0cm 0cm 0cm, clip]{fig/GCNConv-GPU}
	}
	\hspace{0mm}
	\subfloat[GCN2Conv-CPU]{%
		\includegraphics[width=0.23\linewidth, trim=0cm 0cm 0cm 0cm, clip]{fig/GCN2Conv-CPU}
	}
	\hspace{0mm}
	\subfloat[GCN2Conv-GPU]{%
		\includegraphics[width=0.23\linewidth, trim=0cm 0cm 0cm 0cm, clip]{fig/GCN2Conv-GPU}
	}
	\vspace{-3mm}
	\subfloat[GATConv-CPU]{%
		\includegraphics[width=0.23\linewidth, trim=0cm 0cm 0cm 0cm, clip]{fig/GATConv-CPU}
	}
	\hspace{0mm}
	\subfloat[GATConv-GPU]{%
		\includegraphics[width=0.23\linewidth, trim=0cm 0cm 0cm 0cm, clip]{fig/GATConv-GPU}
	}
	\hspace{0mm}
	\subfloat[GATv2Conv-CPU]{%
		\includegraphics[width=0.23\linewidth, trim=0cm 0cm 0cm 0cm, clip]{fig/GATv2Conv-CPU}
	}
	\hspace{0mm}
	\subfloat[GATv2Conv-GPU]{%
		\includegraphics[width=0.23\linewidth, trim=0cm 0cm 0cm 0cm, clip]{fig/GATv2Conv-GPU}
	}
	\vspace{-3mm}
	\subfloat[SAGEConv-CPU]{%
		\includegraphics[width=0.23\linewidth, trim=0cm 0cm 0cm 0cm, clip]{fig/SAGEConv-CPU}
	}
	\hspace{0mm}
	\subfloat[SAGEConv-GPU]{%
		\includegraphics[width=0.23\linewidth, trim=0cm 0cm 0cm 0cm, clip]{fig/SAGEConv-GPU}
	}
	\hspace{0mm}
	\centering
	\subfloat[ChebConv-CPU]{%
		\includegraphics[width=0.23\linewidth, trim=0cm 0cm 0cm 0cm, clip]{fig/ChebConv-CPU}
	}
	\hspace{0mm}
	\subfloat[ChebConv-GPU]{%
		\includegraphics[width=0.23\linewidth, trim=0cm 0cm 0cm 0cm, clip]{fig/ChebConv-GPU}
	}
	\vspace{-3mm}
	\centering
	\subfloat[TAGConv-CPU]{%
		\includegraphics[width=0.23\linewidth, trim=0cm 0cm 0cm 0cm, clip]{fig/TAGConv-CPU}
	}
	\hspace{0mm}
	\subfloat[TAGConv-GPU]{%
		\includegraphics[width=0.23\linewidth, trim=0cm 0cm 0cm 0cm, clip]{fig/TAGConv-GPU}
	}
	\hspace{0mm}
	\subfloat[SGConv-CPU]{%
		\includegraphics[width=0.23\linewidth, trim=0cm 0cm 0cm 0cm, clip]{fig/SGConv-CPU}
	}
	\hspace{0mm}
	\subfloat[SGConv-GPU]{%
		\includegraphics[width=0.23\linewidth, trim=0cm 0cm 0cm 0cm, clip]{fig/SGConv-GPU}
	}
	\vspace{0mm}
	\caption{Runtime of eight Conv layers. Note that the range of y-axis is different for CPU and GPU cases.}
	\label{fig:Conv}
	\vspace{-3mm}
\end{figure*}

\vspace{1mm}
\noindent \textbf{Sampler.} We then compare the performance of three different graph samplers provided by DGL and PyG, namely neighborhood sampler in GraphSAGE~\cite{hamilton2017inductive}, graph clustering-based sampler in ClusterGCN~\cite{chiang2019cluster}, and random walk-based sampler in GraphSAINT~\cite{zeng2019graphsaint}. 

For GraphSAGE sampler, we follow the settings in~\cite{hamilton2017inductive}, which sample 25 and 10 neighbors per node in its first-hop and second-hop neighborhoods, respectively, with a batch size of 512. Note that each mini-batch is composed of 512 subgraphs. For ClusterCGN sampler, there are two steps, which are (1) graph partitioning with METIS algorithm and (2) cluster aggregation. The former partitions the input graph into a given number of small clusters with METIS algorithm, while the latter is to randomly select a few of them to form a subgraph for a training batch. Note that the former is done only once, but the latter is repeated to obtain different mini-batches. In this experiment, we partition the input graph into 2000 clusters and combine 50 of them for each mini-batch. For GraphSAINT sampler, we use the random walk sampling method with 3000 roots and a walk length of two steps to construct subgraphs from the input graph for mini-batch training. While there are two other sampling methods, namely node sampling and edge sampling, in GraphSAINT, we here do not consider them as they are shown to be inferior to the random walk sampling~\cite{zeng2019graphsaint}. We measure the runtime of each sampler for one training epoch, i.e., one pass over the entire graph, and report the results in Figure~\ref{fig:sampler}.

\vspace{1mm}

\noindent \textbf{Observation 2:} \textit{All three samplers provided by DGL are more efficient than the ones in PyG. The performance gap is relatively small for GraphSAINT sampler since it is computationally cheaper than the other two samplers.}

\vspace{1mm}

We observe that DGL implements its samplers in C++ with OpenMP, thus leading to superior performance to the ones of PyG, which are developed in Python. In addition, although the choices of hyperparameters can affect the sampling performance, GraphSAINT sampler is generally faster than GraphSAGE's neighborhood sampler and ClusterGCN sampler. It is also worth noting that the neighborhood sampler can lead to a very large computational graph for each node, while the ClusterGCN sampler can lead to information loss and data imbalance. Thus, we expect that the GraphSAINT sampler is a preferable choice in practice. Furthermore, we observe that PyG requires data format conversion to the compressed sparse column (CSC) format, e.g., if it was in the compressed sparse row (CSR) format, which turns out to be quite slow on large datasets. Finally, while all three samplers in both DGL and PyG run on CPU, DGL also provides GPU support and CUDA-Unified Virtual Addressing (UVA) support for GraphSAGE, but not for other GNN models. We shall discuss them in Section~\ref{sec:case-study}.

\vspace{0mm}
\noindent \textbf{Graph convolutional layer.} A convolutional (Conv) layer is a key and dominant component of GNNs, and its runtime performance can often reflect the overall performance. We thus conduct functional testing on a collection of Conv layers available in DGL and PyG. Both frameworks provide an `nn' module that contains the implementations of popular Conv layers.  We notice that PyG covers more than 50 Conv layers and DGL has about 30 of them. We here select eight commonly used Conv layers for functional testing. They are GCNConv~\cite{kipf2016semi}, GCN2Conv~\cite{chen2020simple}, ChebConv~\cite{defferrard2016convolutional}, SAGEConv~\cite{hamilton2017inductive}, GATConv~\cite{velivckovic2017graph}, GATv2Conv~\cite{brody2021attentive}, TAGConv~\cite{du2017topology}, and SGConv~\cite{wu2019simplifying}.


\begin{figure}[t]
	\captionsetup[subfloat]{captionskip=1pt}
	\centering
	\subfloat[DGL-CPU]{%
		\includegraphics[width=0.47\linewidth, trim=0cm 0cm 0cm 0cm, clip]{fig/graphsage-breakdown-DGL-CPU}
	}
	\vspace{0mm}
	\subfloat[DGL-CPUGPU]{%
		\includegraphics[width=0.47\linewidth, trim=0cm 0cm 0cm 0cm, clip]{fig/graphsage-breakdown-DGL-CPUGPU}
	}
	\hspace{1mm}
	\subfloat[PyG-CPU]{%
		\includegraphics[width=0.47\linewidth, trim=0cm 0cm 0cm 0cm, clip]{fig/graphsage-breakdown-PyG-CPU}
	}
	\vspace{0mm}
	\subfloat[PyG-CPUGPU]{%
		\includegraphics[width=0.47\linewidth, trim=0cm 0cm 0cm 0cm, clip]{fig/graphsage-breakdown-PyG-CPUGPU}
	}
	\vspace{0mm}
	\caption{Runtime breakdown of GraphSAGE.}
	\label{fig:sage-breakdown}
	\vspace{-2mm}
\end{figure}


We measure the runtime of executing each Conv layer on CPU and GPU. In other words, the reported runtime is equivalent to the time of running \emph{one forward propagation} over a single Conv layer with the entire input graph. We manually set the hyperparameters to be the same across the frameworks for each Conv layer. The output dimension is fixed to be 256 for all test cases. The results are presented in Figure~\ref{fig:Conv}.

\vspace{1mm}

\noindent \textbf{Observation 3:} \textit{All eight Conv layers in DGL run faster than the ones of PyG on CPU. The ones in DGL also run faster than their PyG counterparts on GPU in most cases, while PyG only outperforms DGL for few cases with small graphs. Furthermore, graph convolutional operations on GPU show up to 70x speedup over them on CPU.}

\vspace{1mm}


The main reason for the performance on CPU is that DGL adopts an improved CPU message passing kernel developed by~\cite{md2021distgnn} to boost the performance, while PyG relies on the CPU kernels included in its own PyTorch Sparse and PyTorch Scatter, where some `scatter' operations are not well optimized on CPU. As for the performance on GPU, it is worth noting that our observation does not conflict but match with the observation in \cite{wu2021performance}, which shows that PyG is more efficient than DGL, yet for small graphs. Our observation also confirms the claim in~\cite{wang2019deep}. Although DGL is a bit slower on small graphs due to its framework overhead, it is generally more efficient than PyG, especially on large graphs, thanks to its highly tuned kernels. We also find that SAGEConv is relatively computationally cheaper than the other Conv layers, due to its simple aggregation operation.


In addition, we observe that both frameworks provide \emph{fused} kernels to improve their efficiency and scalability, where two separate message-passing and aggregation operations are merged as a single message aggregation operation. DGL uses `g.update\_all()' function to invoke its g-SpMM and g-SDDMM kernels, while PyG simply calls `matmul()' function in PyTorch Sparse. It is worth noting that PyG does not provide such fused kernel support for ChebConv, GATConv, and GATv2Conv layers. As a result, all three layers of PyG suffer from an out-of-memory issue on large graphs.

\begin{figure}[t]
	\captionsetup[subfloat]{captionskip=1pt}
	\centering
	\subfloat{%
		\includegraphics[width=0.47\linewidth, trim=0cm 0cm 0cm 0cm, clip]{fig/graphsage-total-1}
	}
	\hspace{0mm}
	\subfloat{%
		\includegraphics[width=0.47\linewidth, trim=0cm 0cm 0cm 0cm, clip]{fig/graphsage-total-2}
	}
	\vspace{-2mm}
	\caption{Total runtime of GraphSAGE.}
	\label{fig:sage-runtime}
	\vspace{-3mm}
\end{figure}

\begin{figure}[t!]
	\captionsetup[subfloat]{captionskip=1pt}
	\centering
	\subfloat{%
		\includegraphics[width=0.47\linewidth, trim=0cm 0cm 0cm 0cm, clip]{fig/graphsage-power-1}
	}
	\vspace{0mm}
	\subfloat{%
		\includegraphics[width=0.47\linewidth, trim=0cm 0cm 0cm 0cm, clip]{fig/graphsage-power-2}
	}
	\vspace{-2mm}
	\caption{Average power consumption of GraphSAGE.}
	\label{fig:sage-power}
	\vspace{-2mm}
\end{figure}


\begin{figure}[t!]
	\captionsetup[subfloat]{captionskip=1pt}
	\centering
	\subfloat{%
		\includegraphics[width=0.47\linewidth, trim=0cm 0cm 0cm 0cm, clip]{fig/graphsage-energy-1}
	}
	\vspace{0mm}
	\subfloat{%
		\includegraphics[width=0.47\linewidth, trim=0cm 0cm 0cm 0cm, clip]{fig/graphsage-energy-2}
	}
	\vspace{-2mm}
	\caption{Energy consumption of GraphSAGE.}
	\label{fig:sage-energy}
	\vspace{-3mm}
\end{figure}


\subsection{Performance Evaluation of GNNs}\label{sec:performance}

We evaluate three representative sampling-based GNNs, namely GraphSAGE, ClusterGCN, and GraphSAINT on CPU and GPU separately. We use `DGL-CPU' and `PyG-CPU' to indicate when both sampling and training are done on CPU and use `DGL-CPUGPU' and `PyG-CPUGPU' to indicate when sampling is done on CPU while training is done on GPU. We present their runtime breakdown, total runtime, average power consumption, and energy consumption in Figures~\ref{fig:sage-breakdown}--\ref{fig:saint-energy}. Note that, for all three GNNs, we use the same hyperparameters of their samplers as used in the above functional testing. We use two convolutional layers for all three models and the hyperparameters of each GNN model are set to be the same across DGL and PyG for a fair comparison. The reported results are based on the models trained by 10 epochs. We repeated the same experiments multiple times and observed more or less the same results.  

\begin{figure}[t!]
	\captionsetup[subfloat]{captionskip=1pt}
	\centering
	\subfloat[DGL-CPU]{%
		\includegraphics[width=0.47\linewidth, trim=0cm 0cm 0cm 0cm, clip]{fig/clustergcn-breakdown-DGL-CPU}
	}
	\hspace{0mm}
	\subfloat[DGL-CPUGPU]{%
		\includegraphics[width=0.47\linewidth, trim=0cm 0cm 0cm 0cm, clip]{fig/clustergcn-breakdown-DGL-CPUGPU}
	}
	\vspace{-2mm}
	\subfloat[PyG-CPU]{%
		\includegraphics[width=0.47\linewidth, trim=0cm 0cm 0cm 0cm, clip]{fig/clustergcn-breakdown-PyG-CPU}
	}
	\vspace{0mm}
	\subfloat[PyG-CPUGPU]{%
		\includegraphics[width=0.47\linewidth, trim=0cm 0cm 0cm 0cm, clip]{fig/clustergcn-breakdown-PyG-CPUGPU}
	}
	\vspace{0mm}
	\caption{Runtime breakdown of ClusterGCN.}
	\label{fig:cluster-breakdown}
	\vspace{-4mm}
\end{figure}


\begin{figure}[t!]
	\captionsetup[subfloat]{captionskip=1pt}
	\centering
	\subfloat{%
		\includegraphics[width=0.47\linewidth, trim=0cm 0cm 0cm 0cm, clip]{fig/clustergcn-total-1}
	}
	\hspace{0mm}
	\subfloat{%
		\includegraphics[width=0.47\linewidth, trim=0cm 0cm 0cm 0cm, clip]{fig/clustergcn-total-2}
	}
	\vspace{0mm}
	\caption{Total runtime of ClusterGCN.}
	\label{fig:clustergcn-runtime}
	\vspace{-4mm}
\end{figure}


\begin{figure}[t!]
	\captionsetup[subfloat]{captionskip=1pt}
	\centering
	\subfloat{%
		\includegraphics[width=0.47\linewidth, trim=0cm 0cm 0cm 0cm, clip]{fig/clustergcn-power-1}
	}
	\vspace{0mm}
	\subfloat{%
		\includegraphics[width=0.47\linewidth, trim=0cm 0cm 0cm 0cm, clip]{fig/clustergcn-power-2}
	}
	\vspace{0mm}
	\caption{Average power consumption of ClusterGCN.}
	\label{fig:cluster-power}
	\vspace{-4mm}
\end{figure}


\begin{figure}[t!]
	\captionsetup[subfloat]{captionskip=1pt}
	\centering
	\subfloat{%
		\includegraphics[width=0.47\linewidth, trim=0cm 0cm 0cm 0cm, clip]{fig/clustergcn-energy-1}
	}
	\vspace{0mm}
	\subfloat{%
		\includegraphics[width=0.47\linewidth, trim=0cm 0cm 0cm 0cm, clip]{fig/clustergcn-energy-2}
	}
	\vspace{0mm}
	\caption{Energy consumption of ClusterGCN.}
	\label{fig:cluster-energy}
	\vspace{-4mm}
\end{figure}


\begin{figure}[t!]
	\captionsetup[subfloat]{captionskip=1pt}
	\centering
	\subfloat[DGL-CPU]{%
		\includegraphics[width=0.47\linewidth, trim=0cm 0cm 0cm 0cm, clip]{fig/graphsaint-breakdown-DGL-CPU}
	}
	\hspace{0mm}
	\subfloat[DGL-CPUGPU]{%
		\includegraphics[width=0.47\linewidth, trim=0cm 0cm 0cm 0cm, clip]{fig/graphsaint-breakdown-DGL-CPUGPU}
	}
	\vspace{-2mm}
	\subfloat[PyG-CPU]{%
		\includegraphics[width=0.47\linewidth, trim=0cm 0cm 0cm 0cm, clip]{fig/graphsaint-breakdown-PyG-CPU}
	}
	\vspace{0mm}
	\subfloat[PyG-CPUGPU]{%
		\includegraphics[width=0.47\linewidth, trim=0cm 0cm 0cm 0cm, clip]{fig/graphsaint-breakdown-PyG-CPUGPU}
	}
	\vspace{0mm}
	\caption{Runtime breakdown of GraphSAINT.}
	\label{fig:saint-breakdown}
	\vspace{-3.5mm}
\end{figure}


\begin{figure}[t!]
	\captionsetup[subfloat]{captionskip=1pt}
	\centering
	\subfloat{%
		\includegraphics[width=0.47\linewidth, trim=0cm 0cm 0cm 0cm, clip]{fig/graphsaint-total-1}
	}
	\hspace{0mm}
	\subfloat{%
		\includegraphics[width=0.47\linewidth, trim=0cm 0cm 0cm 0cm, clip]{fig/graphsaint-total-2}
	}
	\vspace{0mm}
	\caption{Total runtime of GraphSAINT.}
	\label{fig:saint-runtime}
	\vspace{-4mm}
\end{figure}



\begin{figure}[t!]
	\captionsetup[subfloat]{captionskip=1pt}
	\centering
	\subfloat{%
		\includegraphics[width=0.47\linewidth, trim=0cm 0cm 0cm 0cm, clip]{fig/graphsaint-power-1}
	}
	\vspace{0mm}
	\subfloat{%
		\includegraphics[width=0.47\linewidth, trim=0cm 0cm 0cm 0cm, clip]{fig/graphsaint-power-2}
	}
	\vspace{0mm}
	\caption{Average power consumption of GraphSAINT.}
	\label{fig:saint-power}
	\vspace{-4mm}
\end{figure}


\begin{figure}[t!]
	\captionsetup[subfloat]{captionskip=1pt}
	\centering
	\subfloat{%
		\includegraphics[width=0.47\linewidth, trim=0cm 0cm 0cm 0cm, clip]{fig/graphsaint-energy-1}
	}
	\vspace{0mm}
	\subfloat{%
		\includegraphics[width=0.47\linewidth, trim=0cm 0cm 0cm 0cm, clip]{fig/graphsaint-energy-2}
	}
	\vspace{0mm}
	\caption{Energy consumption of GraphSAINT.}
	\label{fig:saint-energy}
	\vspace{-4mm}
\end{figure}


As shown in Figure~\ref{fig:sage-breakdown}, Figure~\ref{fig:cluster-breakdown}, and Figure~\ref{fig:saint-breakdown}, we break the runtime of each GNN into four parts, which are data loading, sampling, data movement, and model training. Data loading is done by `data loader' to load the input graph and its associated node features from storage to CPU memory. Sampling is done by `sampler' to extract subgraphs and fetch the node features of the sampled subgraphs from the entire feature matrix for mini-batch training. Data movement is to copy the initial weight matrices of a GNN model, each subgraph matrix, and its corresponding node features from CPU to GPU. Note that there is no data movement (from CPU to GPU) for DGL-CPU and PyG-CPU. Model training includes forward propagation, backward propagation, and update of model weights. Note that as the number of training epochs increases, the fraction of data loading in total runtime will decrease since it is a one-time operation. However, sampling, data movement, and model training are performed repeatedly for different mini-batches.


\vspace{1mm}

\noindent \textbf{Observation 4:} \textit{Sampling is slow for all three GNNs and can take up to 90\% of total runtime.}

\vspace{1mm}

This observation indicates that there is a need to optimize sampling and its associated operations. In particular, for PyG, its CPU kernel could be improved for not only sampling but also model training on CPU. In addition, we observe that data movement can also take a large portion of total runtime in both frameworks. As shall be shown in Section~\ref{sec:case-study}, data pre-loading in the frameworks can be used to mitigate this issue.


\vspace{1mm}

\noindent \textbf{Observation 5:} \textit{DGL is generally more efficient than PyG on both CPU and GPU in terms of runtime and energy consumption, especially for large graphs.}

\vspace{1mm}

We observe that PyG is more efficient than DGL for small graphs when CPU is used for sampling and GPU is used for training, while DGL is generally more efficient for the other cases. In particular, PyG-CPUGPU is generally more efficient than DGL-CPUGPU for GraphSAINT. This behavior can be explained as follows. With mini-batch training, a GNN model is trained based on sampled subgraphs, which are much smaller than the input graph. We observe that each sampled subgraph (corresponding to a mini-batch) by GraphSAINT sampler is relatively smaller than the ones by GraphSAGE's neighborhood sampler and ClusterGCN sampler. Here the one with GraphSAGE's neighborhood sampler has multiple subgraphs for a mini-batch. Also, recall that the performance gap of the GraphSAINT sampler between DGL and PyG is insignificant, as shown in Figure~\ref{fig:sampler}. Since PyG is more efficient in model training with small graphs, PyG becomes more efficient even for medium-size graphs with GraphSAINT, as shown in Figure~\ref{fig:saint-runtime}.

In addition, we find that there is no clear winner between DGL and PyG regarding average power consumption, which indicates that energy consumption mainly depends on overall runtime. We observe that GraphSAINT is more efficient in both runtime and energy consumption compared with the GraphSAGE and ClusterGCN, thanks to its light-weight sampling and GNN operations. Note that they are trained for the same number of epochs in our experiments. Nonetheless, we emphasize that different choices of the hyperparameters for each GNN in optimizing its accuracy would affect the efficiency in runtime and energy consumption differently.
