\section{Background and Related Work}

\subsection{Graph Neural Networks}

GNNs have emerged as an effective means for learning on graph-structured data. They commonly rely on a `feature aggregation' mechanism, which can be written as
\begin{equation*}
\setlength{\abovedisplayskip}{5pt}
\setlength{\belowdisplayskip}{5pt}
\mathbf{H}^{(l+1)} = f(\mathbf{G}\mathbf{H}^{(l)}\mathbf{W}^{(l)}),
\end{equation*}
where $f(\cdot)$ is a non-linear activation function that is applied element-wise, $\mathbf{G}$ is a graph matrix representing the graph structure, e.g., the adjacency or (normalized) Laplacian marix of the input graph, and $\mathbf{H}^{(l)}$ and $\mathbf{W}^{(l)}$ are the node feature/embedding matrix and the weight matrix of neural networks at $l$-th layer, respectively. In other words, it is the neighborhood aggregation or message passing as each node in the graph updates its current feature vector by aggregating the feature vectors (messages) from its neighbors.

\begin{figure}[t!]
	\captionsetup[subfloat]{captionskip=1pt}
	\centering
	\subfloat[Full-batch training]{%
		\includegraphics[width=0.85\linewidth, trim=0cm 0cm 0cm 0cm, clip]{fig/full}
	}
	\vspace{-3mm}
	\subfloat[Mini-batch training]{%
		\includegraphics[width=0.85\linewidth, trim=0cm 0cm 0cm 0cm, clip]{fig/minibatch}
	}
	\hspace{0mm}
	\caption{Two training methods for GNNs.}
	\label{fig:gnn}
	\vspace{-3mm}
\end{figure}

Due to the feature aggregation mechanism or the interdependence of the nodes (samples), traditional GNNs such as GCN~\cite{kipf2016semi} and GAT~\cite{velivckovic2017graph} were trained using the \emph{full-batch} gradient descent, as shown in Figure~\ref{fig:gnn}(a). In other words, they require the entire graph and node features to be maintained in memory, leading to a \emph{scalability} issue with large graphs. To cope with the scalability issue, recent GNNs have then adopted `sampling' techniques to construct \emph{mini-batches} based on the graph structure to train GNNs on large graphs, as mini-batch gradient descent is used for deep neural networks. See Figure~\ref{fig:gnn}(b) for illustration.


Hamilton \textit{et al.}~\cite{hamilton2017inductive} proposed GraphSAGE, which is the first work that introduces the use of sampling in training GNNs to improve the scalability. It combines neighborhood sampling, which samples $k$-hop neighbors with a fixed sampling size for feature aggregation, with mini-batch training. However, its resulting computation graph can be still explosive and thus cause an out-of-memory issue for large graphs. To alleviate this issue, Chen \textit{et al.}~\cite{chen2018fastgcn} developed FastGCN, which samples a fixed number of nodes in each GNN layer independently based on a pre-computed probability distribution. Nonetheless, it can generate isolated nodes, thereby leading to an accuracy drop. Zou \textit{et al.}~\cite{zou2019layer} proposed a layer-dependent importance sampling algorithm called LADIES to resolve the sparsity issue in FastGCN, while it introduces additional computational cost and non-negligible overhead in the sampling process.



\begin{table*}[t]
	\renewcommand{\arraystretch}{1.2}
	\caption{Dataset statistics}
	\label{table:dataset}
	\centering
	\scriptsize
	\begin{adjustbox}{width=1.8\columnwidth,center}
		\begin{tabular}{|c|c|c|c|c|c|c|}
			\hline
			Dataset & Description & \# Nodes & \# Edges & \# Features & \# Classes & Train / Val / Test \\
			\hline
			\hline
			PPI & Protein-Protein Interactions & 14,755 & 225,270 & 50 & 121 & 0.66 / 0.12 / 0.22 \\
			\hline
			Flickr & Images Sharing Common Properties & 89,250 & 899,756 & 500 & 7 & 0.50 / 0.25 / 0.25 \\
			\hline
			ogbn-Arxiv & Citation Network of arXiv CS papers & 169,343 & 1,166,243 & 128 & 40 & 0.54 / 0.29 / 0.17 \\
			\hline
			Reddit & Online Communities & 232,965 & 114,615,892 & 602 & 41 & 0.66 / 0.10 / 0.24\\	
			\hline
			Yelp & Businesses and Reviews & 716,847 & 13,954,819 & 300 & 100 & 0.75 / 0.10 / 0.15 \\
			\hline
			ogbn-Products & Amazon Product Co-purchasing Network & 2,449,029 & 61,859,140 & 100 & 47 & 0.08 / 0.02 / 0.90 \\
			\hline
		\end{tabular}
	\end{adjustbox}
\end{table*}


In addition, Chiang \textit{et al.}~\cite{chiang2019cluster} proposed ClusterGCN, which partitions the input graph into many small clusters, some of which are then randomly selected to form a subgraph -- or, more precisely, mini-batch, during training. It highly improves the scalability of GNNs, although it can lead to data imbalance and information loss issues. Zeng \textit{et al.}~\cite{zeng2019graphsaint} proposed GraphSAINT, which constructs training batches by sampling subgraphs of the input graph. They leveraged graph sampling techniques, such as node sampling, edge sampling, and random walk-based sampling, to obtain subgraphs.


\subsection{Related Work}

There are a few GNN benchmark studies in the literature. Dwivedi \textit{et al.}~\cite{dwivedi2020benchmarking} introduced a benchmark framework along with a set of medium-scale graph datasets for a large collection of GNN models. They developed the benchmark framework on top of PyG and DGL and presented the results in accuracy and training time, \emph{yet} without any detailed component analysis. Duan \textit{et al.}~\cite{duan2018benchmarking} used a greedy \emph{hyperparameter} search method to tune up the performance of several GNN models and reported the resulting accuracy of each model and its corresponding time and space complexity. Zhang \textit{et al.}~\cite{zhang2020architectural} provided a detailed workload analysis on the \emph{inference} of GNNs. Lin \textit{et al.}~\cite{lin2022characterizing} focused on the \emph{distributed training} benchmark of three GNN models implemented in PyG. The studies in \cite{baruah2021gnnmark,mernyei2020wiki,hu2020open} presented new datasets for GNN benchmarking.

The work by Wu \textit{et al.}~\cite{wu2021performance} is most relevant to our work as it is also concerned about the performance analysis of DGL and PyG. It was, however, based only on five datasets of \emph{small-size} graphs with six GNN models, which are mostly traditional ones. Three datasets are for `graph' classification as a downstream task. The smallest one has 600 graphs, each with about 30 nodes and 60 edges on average, while the largest one has 80K graphs, each with about 70 nodes and 500 edges on average. The other datasets are two small graphs (the larger one has about 20K nodes and 40K edges) for `node' classification. For this downstream task, they focused on the \emph{full-batch} training, not to mention lack of any detailed component analysis.

We can summarize the \emph{differences} between our work and the GNN benchmark literature as follows. First, we provide a detailed and comprehensive analysis of DGL and PyG not only at the level of the efficiency (total training time) but also at the level of the runtime of each key component of GNN models. Second, our benchmark of the frameworks is done based on a wide range of graphs, having the largest one with about 2.4M nodes and 61M edges, and three representative GNNs that support mini-batch training for scalability. Finally, we present the energy and power efficiency of GNN models and frameworks.
