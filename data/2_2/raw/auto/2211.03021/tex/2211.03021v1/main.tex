\documentclass[10pt,letterpaper,compsoc,conference]{iiswc22}

\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage[dvipsnames]{xcolor}
\usepackage[final]{microtype}
\usepackage[italic]{mathastext}
\usepackage{libertine}
\usepackage[T1]{fontenc}
\usepackage{textcomp}
\usepackage[varqu,varl]{zi4}
\usepackage[all]{nowidow}
\usepackage[auth-lg,affil-it]{authblk}
\usepackage[keeplastbox]{flushend}
\usepackage{url}
\usepackage{adjustbox}
\usepackage{multirow}
\usepackage{enumitem}
\usepackage[algo2e, linesnumbered]{algorithm2e}
\usepackage{algorithm}
\usepackage{bm}
\usepackage{balance}

\newlength\mylen
\newcommand\myinput[1]{%
	\settowidth\mylen{\KwIn{}}%
	\setlength\hangindent{\mylen}%
	\hspace*{\mylen}#1\\}

\usepackage[caption=false]{subfig}


\begin{document}


\title{Characterizing the Efficiency of Graph Neural Network Frameworks with a Magnifying Glass}


\renewcommand\Authsep{\qquad}
\renewcommand\Authand{\qquad}
\renewcommand\Authands{\qquad}


\author[1]{Xin Huang}
\author[2]{Jongryool Kim}
\author[3]{Bradley Rees}
\author[1]{Chul-Ho Lee}
\affil[1]{Texas State University}
\affil[2]{SK hynix America}
\affil[3]{NVIDIA}



\maketitle


\begin{abstract}
Graph neural networks (GNNs) have received great attention due to their success in various graph-related learning tasks. Several GNN frameworks have then been developed for fast and easy implementation of GNN models. Despite their popularity, they are not well documented, and their implementations and system performance have not been well understood. In particular, unlike the traditional GNNs that are trained based on the entire graph in a \emph{full-batch} manner, recent GNNs have been developed with different graph sampling techniques for \emph{mini-batch} training of GNNs on large graphs. While they improve the scalability, their training times still depend on the implementations in the frameworks as sampling and its associated operations can introduce non-negligible overhead and computational cost. In addition, it is unknown how much the frameworks are `eco-friendly' from a green computing perspective. In this paper, we provide an in-depth study of two mainstream GNN frameworks along with three state-of-the-art GNNs to analyze their performance in terms of runtime and power/energy consumption. We conduct extensive benchmark experiments at several different levels and present detailed analysis results and observations, which could be helpful for further improvement and optimization.
\end{abstract}


\input{intro}
\input{related}
\input{methodology}
\input{result1}
\input{result2}
\input{conclusion}


\section*{Acknowledgments}
This work was supported in part by a grant from SK hynix America and an equipment gift from NVIDIA. This work was also supported in part by the National Science Foundation under Grant IIS-2209921.



\bibliographystyle{IEEEtranS}
\bibliography{ref}


\end{document} 