\section{Introduction}

Graphs are everywhere, from social networks to transportation networks to biological networks. It is of vital importance to mining graph-structured data~\cite{li2021efficient,li2021estimating} and learning on graphs~\cite{hamilton2020graph,ma2021deep} since they contain rich underlying information and can be used for a wide range of applications. In particular, graph neural networks (GNNs) have attracted a lot of attention in recent years. Unlike the conventional machine learning (ML) algorithms, which assume data samples are independent and identically distributed, GNNs take graph-structured data as input for downstream tasks and capture the correlation between data samples (nodes in the graph) according to their connections (edges in the graph). GNNs have been shown to be effective for many tasks, such as representation learning, node classification, and link prediction.

Early GNN studies mainly reply on general deep learning (DL) frameworks, such as TensorFlow~\cite{tensorflow2015-whitepaper} and PyTorch~\cite{NEURIPS2019_9015}. It is, however, non-trivial to implement a GNN model using the DL frameworks. While they are designed and optimized for regular yet often dense data, real-world graphs often exhibit irregularity and sparsity, thereby making them inefficient for GNNs. Thus motivated, several GNN frameworks have been developed to speed up the computation and to simplify GNN implementation. The examples include Graph Nets~\cite{battaglia2018relational}, Deep Graph Library (DGL)~\cite{wang2019deep}, PyTorch Geometric (PyG)~\cite{FeyLenssen2019}, StellarGraph~\cite{StellarGraph}, Spektral~\cite{grattarola2021graph}, TF-Geometric~\cite{hu2021efficient}, and CogDL~\cite{cen2021cogdl}.

DGL and PyG are two most popular ones among them, thanks to their user-friendly designs, rich functionalities, and easy-to-follow tutorials. Inspired by NetworkX~\cite{hagberg2008exploring}, DGL uses a graph-centric programming abstraction, making it easy for NetworkX users to use. It defines a `DGLGraph' object as its key data structure for computations with graph-structured data and GNN operations. DGL also realizes the message passing operations of GNNs with generalized sparse-dense matrix multiplication (g-SpMM) and generalized sampled dense-dense matrix multiplication (g-SDDMM). Furthermore, it develops highly tuned CPU and GPU kernels for GNN operations and supports a wide range of applications for general-purpose graph learning. In addition, PyG is an extension library of PyTorch for deep learning on graph-structured data. It provides a simple `MessagePassing' interface for the message passing operations based on a gather-and-scatter paradigm, which is built on top of its own PyTorch Scatter\footnote{\url{https://github.com/rusty1s/pytorch_scatter}.} and PyTorch Sparse\footnote{\url{https://github.com/rusty1s/pytorch_sparse}.} that provide dedicated kernels for relevant computations. It also provides a large number of off-the-shelf examples along with a lot of commonly used benchmark datasets for users to easily use and test. Both DGL and PyG have been updated and optimized significantly compared with their initial versions. However, their current implementations and system performance are not well understood.

On the other hand, `\emph{sustainability}' becomes an important factor in both industry and academia due to climate change. Energy and power consumption ought to be critical metrics in ML/DL since training advanced models are often energy and resource hungry. Early studies in ML/DL have, however, mainly focused on improving their model accuracy to achieve state-of-the-art performance. Schwartz \textit{et al.}~\cite{schwartz2020green} recently urge researchers to provide not only the accuracy, but also the efficiency in terms of carbon emission, energy consumption, runtime, to name a few. Strubell \textit{et al.}~\cite{strubell2019energy} bring power and energy concerns in ML/DL research by estimating the financial and environmental costs of building well-trained state-of-the-art natural language processing models. There is then a movement, albeit slowly, in recent studies that take power and energy consumption into consideration~\cite{schwartz2020green}. Nonetheless, there is no prior work to quantify the power and energy consumption of GNN models and frameworks.

In this paper, we study the two mainstream GNN frameworks -- DGL and PyG, by evaluating their efficiency in terms of runtime (not only at the level of each key function but also at the level of the entire model), and power and energy consumption.\footnote{Here we do not report the accuracy results of GNN models as they mainly depend on their underlying GNN methods, not the software frameworks. It has also been shown that there is no clear difference between two frameworks when it comes to the accuracy of each GNN model~\cite{wang2019deep, NEURIPS2019_9015, wu2021performance}.} We benchmark their performance via functional testing on each main component of GNNs and three state-of-the-art GNNs, namely GraphSAGE, ClusterGCN, and GraphSAINT, which adopt graph sampling for mini-batch training. We further provide case studies on different implementation strategies, GPU-based sampling, and full-batch training. We provide detailed and comprehensive analysis to fully understand their performance and find opportunities for further improvement and optimization. We summarize our contributions as follows:
\vspace{0mm}
\begin{itemize}[itemsep=2pt,leftmargin=1.1em]
\item First, we present the results of functional testing on each key component of building a GNN model in DGL and PyG, including data loader, sampler, and graph convolutional layers. We find that DGL is more efficient for sampling and GNN operations, especially when it comes to large graphs.

\item Second, we evaluate the efficiency of sampling and GNN operations on different hardware devices (CPU vs. GPU).


\item Third, we provide runtime breakdown of three state-of-the-art GNNs in both frameworks. Our results indicate that there is still a room for further improvement, especially for sampling and data movement.


\item Finally, we quantify the power and energy consumption of GNN models and frameworks. To the best of our knowledge, we are the first to analyze the GNN performance from such a green computing perspective.
\end{itemize}
