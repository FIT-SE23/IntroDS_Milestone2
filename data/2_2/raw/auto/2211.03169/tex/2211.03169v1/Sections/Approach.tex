\section{Learning Riemannian Stable Dynamical Systems}
\label{sec:approach}
\begin{wrapfigure}[13]{r}{0.46\linewidth}
    \centering
    \includegraphics[trim={0.0cm 0.0cm 0.0cm .5cm}, width=0.44\textwidth, ]{Images/Approach/diffeomorphsim_sphere.pdf}
    \caption{Architecture of a diffeomorphism-based stable vector field on the Riemannian manifold $\SphereManifold^2$.}
    \label{fig:diffeomorphism_based_svf}
\end{wrapfigure}

We here introduce our approach for learning stable dynamical systems on Riemannian manifolds from demonstrated point-to-point motions.
First, let us consider that the recorded demonstrations follow a dynamical system $\Dot{\bm{x}} = f(\bm{x})$, where the state $\bm{x}$ evolves on a Riemannian manifold $\mathcal{M}$ with velocity $\dot{\bm{x}} \in \tangentspace{\bm{x}}$.
This dynamical system is equivalent, under a change of coordinates, to another system defined on a latent Riemannian manifold $\mathcal{N}$.
Under the diffeomorphism $\psi_{\bm{\theta}}: \bm{x} \mapsto \bm{y} \in \mathcal{N}$, parameterized by $\bm{\theta}$, we map the observed states $\bm{x}$ onto $\mathcal{N}$.
Then, we evaluate the canonical stable vector field $g_{\bm{\gamma}}(\bm{y})$ to obtain the velocity $\dot{\bm{y}} \in \mathcal{T}_{\bm{y}} \mathcal{N}$.
Finally, we leverage the \emph{pullback operator} $D_{\bm{y}}\psi_{\bm{\theta}}^\star$ to project $\dot{\bm{y}}$ back to the tangent space $\tangentspace{\bm{x}}$.
The whole procedure can be expressed as follows, 
\begin{equation}
    \dot{\bm{x}} = (D_{\bm{y}}\psi_{\bm{\theta}}^\star \circ g_{\bm{\gamma}} \circ \psi_{\bm{\theta}})(\bm{x}) \\ =  D_{\bm{y}}\psi_{\bm{\theta}}^\star (\dot{\bm{y}}) ,
    \label{eq:diffeomorphism_based_svf_equation}
\end{equation}
which is illustrated in Fig.~\ref{fig:diffeomorphism_based_svf} (a proof is given in App.~\ref{app:stabilityAnalysis}).
In the sequel, we describe how we design a Lyapunov-stable vector field $g_{\bm{\gamma}}$ on $\mathcal{N}$ to provide stability guarantees on the learned dynamical system.
Later, we explain how to compute the diffeomorphism between the target and latent manifolds.
Finally, we introduce two different methods to compute the pullback operator $D_{\bm{y}}\psi_{\bm{\theta}}^\star$.
\vspace{-0.15cm}

\subsection{Lyapunov-stable Geodesic Vector Fields}
\label{subsec:GeodesicVF}
To design a stable vector field on the Riemannian manifold $\mathcal{N}$, we enforce the canonical dynamical system to follow geodesic curves that converge to a single equilibrium.
Such a vector field can be designed via the logarithmic map.
Specifically, given an equilibrium point $\bm{y}^* \in \mathcal{N}$, the corresponding velocity vector $\dot{\bm{y}} \in \mathcal{T}_{\bm{y}}\mathcal{N}$ can be computed as $\dot{\bm{y}} = g_{\bm{\gamma}}(\bm{y}) = k_{\bm{\gamma}}(\bm{y}) g_n(\bm{y})$ with the normalized geodesics vector field $ g_n(\bm{y}) := \frac{\operatorname{Log}_{\bm{y}}(\bm{y}^*)}{\lVert \operatorname{Log}_{\bm{y}}(\bm{y}^*) \rVert_2}$.
This implies that the direction of tangent vectors is fully specified by $\operatorname{Log}_{\bm{y}}(\bm{y}^*)$, while their magnitude depends on the scaling factor $k_{\bm{\gamma}}: \mathbb{R}^n \supset \mathcal{N} \rightarrow \mathbb{R}_{\geq 0}$.
We can prove the stability of this geodesic vector field by choosing the Lyapunov function $V(\bm{y}):=\langle F, F \rangle_{\bm{y}^*}$ with $F = \logmap{\bm{y}^*}{\bm{y}}$, and applying Theorem~\ref{th:stability_geodesics_vf} for Lyapunov stability on Riemannian manifolds, as detailed in App.~\ref{app:stabilityAnalysis}.
Given that our geodesic vector field is Lyapunov stable, we can easily prove that the desired dynamical system is also globally asymptotically stable by defining a new valid Lyapunov function $\tilde{V}(\bm{x}) := V(\psi_{\bm{\theta}}(\bm{x}))$ via the diffeomorphism $\psi_{\bm{\theta}}$, with a single equilibrium point $\bm{x}^* = \psi_{\bm{\theta}}^{-1}(\bm{y}^*) \in \mathcal{M}$.
As $\psi_{\bm{\theta}}$ preserves the topological properties of $\mathcal{N}$, the equilibrium point $\bm{x}^*$ is also globally asymptotically stable on $\mathcal{M}$ (see App.~\ref{app:stabilityAnalysis} for the proof).
Note that for certain Riemannian manifolds, it is only possible to guarantee \emph{quasi-global} stability guarantees due to the Poincar√©-Hopf theorem (see App.~\ref{app:stabilityAnalysis} for details).

Note that we separate the parameterization for the magnitude and direction of vector fields to improve the expressiveness of our framework.
By relocating the scaling factor $k_{\bm{\gamma}}$ and normalizing the vector fields governed by~\eqref{eq:diffeomorphism_based_svf_equation}, we can obtain our final RSDS learning framework
\begin{equation}
    \dot{\bm{x}} = \hat{k}_{\bm{\gamma}}(\bm{x}) \frac{(D_{\bm{y}}\psi_{\bm{\theta}}^\star \circ  g_n \circ \psi_{\bm{\theta}})(\bm{x})}{\lVert (D_{\bm{y}}\psi_{\bm{\theta}}^\star \circ  g_n \circ \psi_{\bm{\theta}})(\bm{x}) \rVert_2} ,
    \label{eq:final_diffeomorphism_based_vf_equation}
\end{equation}
where $\hat{k}_{\bm{\gamma}}(\bm{x})$ is the new positive scaling factor that fully determines the magnitude of the learned vector fields. 
In App.~\ref{app:finalFramework}, we prove that the models~\eqref{eq:diffeomorphism_based_svf_equation} and \eqref{eq:final_diffeomorphism_based_vf_equation} are equivalent.

\subsection{Diffeomorphisms on Riemannian Manifolds}
\label{subsec:DiffeomorphRM}
Given the final RSDS in~\eqref{eq:final_diffeomorphism_based_vf_equation} and $M$ demonstrations, the goal of learning stable dynamics on a Riemannian manifold reduces to learning $\psi_{\bm{\theta}}$, computing its pullback operator $D_{\bm{y}}\psi_{\bm{\theta}}^\star$, and subsequently estimating $\hat{k}_{\bm{\gamma}}(\bm{x})$. 
However, due to the geometric constraints arising from $\mathcal{M}$, learning a diffeomorphism and calculating the corresponding pullback operator are non-trivial problems. 
To address them, we leverage Neural MODEs~\citep{lou2020:NeuralMODE} to build the diffeomorphism $\psi_{\bm{\theta}}$. 
Unlike~\citep{lou2020:NeuralMODE}, we propose a novel approach to compute the pullback operator by reversing the time interval of the ODE integration (see \S~\ref{subsec:DiffInvDiffeomorph}), avoiding to explicitly compute the corresponding inverse. 
We also propose a method to design Lyapunov-stable geodesic vector fields on a Riemannian manifold, which are leveraged to provide stability guarantees on the learned dynamical system, as explained in \S~\ref{subsec:GeodesicVF}.

According to Theorem~\ref{th:vector_flows} in App.~\ref{app:NeuralMODEs}, the dynamics $f_{\bm{\theta}}$ of Neural MODEs only has to be a $\mathcal{C}^1$ function.
To compute the diffeomorphism with a parametric Neural MODE, we solve an integration problem based on the local parameterization $\bm{w}(t)= \varphi(\bm{z}(t))$ (described in App.~\ref{app:integrators_manifolds}).
Using this method requires the selection of coordinate charts, which can be created via the exponential map $\varphi_i^{-1} = \operatorname{Exp}_{\bm{z}_i}$ and logarithmic map $\varphi_i = \operatorname{Log}_{\bm{z}_i}$, similarly to~\citep{lou2020:NeuralMODE}.
Under this choice of coordinate mapping and given a fixed number of charts $k$, the diffeomorphism $\psi_{\bm{\theta}}: \bm{x} = \bm{z}_0 \mapsto \bm{z}_k = \bm{y}$, obtained via integration on the manifold can be then viewed as the composition of blocks of solving Neural ODEs and chart transitions defined as,
\begin{equation}
    \begin{split}
    \psi_{\bm{\theta}} = \operatorname{Exp}_{\bm{z}_{k-1}} \circ \hat{\psi}_{\bm{\theta},k-1} \circ \operatorname{Log}_{\bm{z}_{k-1}} \circ \ldots \circ \operatorname{Exp}_{\bm{z}_0} \circ \hat{\psi}_{\bm{\theta},0} \circ \operatorname{Log}_{\bm{z}_0} , \quad \textrm{with} \\
    \hat{\psi}_{\bm{\theta}, i}(\bm{w}_i(t_{i, s})) = \bm{w}_i(t_{i, s}) + \int_{\tau=t_{i, s}}^{t_{i, e}} D_{\varphi_i^{-1}(\bm{w}_i(\tau))}\varphi_i \circ f_{\boldsymbol{\theta}}(\varphi_i^{-1}(\bm{w}_i(\tau)), \tau) d\tau ,
    \end{split}
    \label{eq:dynamic_chart_composition}
\end{equation}
where $i$ is the chart index, $t_{i, s}$ and $t_{i, e}$ are the starting and end time for $i^{th}$ chart. 
$\hat{\psi}_{\bm{\theta}, i}$ defines a diffeomorphism computed by the classical ODE solver on the tangent space (i.e. Euclidean space) and provides the solution of the IVP of the equivalent ODE~\eqref{eq:equivalentode}. 

\subsection{Differential of the Inverse Diffeomorphism}
\label{subsec:DiffInvDiffeomorph}
We are now left with the problem of computing the pullback operator $D_{\bm{y}}\psi_{\bm{\theta}}^\star$ in~\eqref{eq:diffeomorphism_based_svf_equation}, which maps the latent velocity $\dot{\bm{y}}$ back to the original tangent space $\tangentspace{\bm{x}}$.
This operator can be considered as the inverse mapping of the differential $D_{\bm{x}} \psi_{\bm{\theta}}:  \tangentspace{\bm{x}} \rightarrow \mathcal{T}_{\psi_{\bm{\theta}}(\bm{x})} \mathcal{N}$.
As we already have the diffeomorphism $\psi_{\bm{\theta}}$, the straightforward solution is to compute its derivatives and then obtain the required inverse.
Nevertheless, under the Riemannian setting, particularly for $d$-dimensional submanifolds $\manifold^d$ embedded in $\mathbb{R}^n$, computing the inverse directly becomes problematic due to the geometric constraints arising from $\manifold$. 
Next, we provide two methods to deal with this problem.

\paragraph{Pullback operator via constrained optimization:}
\label{subsubsec:pullbackConstrainedOpt}
Instead of naively differentiating through the ODE solver of $\psi_{\bm{\theta}}$, we can use the adjoint method to calculate the differential of a diffeomorphism constructed by a Neural MODE.
Assuming that we have the differential $D_{\bm{x}} \psi_{\bm{\theta}}$ (as computed in Algorithm~\ref{alg:differential_diffeomorphism} in App.~\ref{app:adjoint_method}), the connection between tangent vectors $\dot{\bm{x}}$ and $\dot{\bm{y}}$ can be written as $D_{\bm{x}} \psi_{\bm{\theta}}(\bm{x}) \dot{\bm{x}} = \dot{\bm{y}}$.
In the Euclidean case, we can directly compute $\dot{\bm{x}} = (D_{\bm{x}} \psi_{\bm{\theta}}(\bm{x}))^{-1}\dot{\bm{y}}$.
However, under the Riemannian setting, computing the inverse $(D_{\bm{x}} \psi_{\bm{\theta}}(\bm{x}))^{-1}$ often leads to a loss of rank in the matrix representation of $D_{\bm{x}} \psi_{\bm{\theta}}(\bm{x})$ for an embedded submanifold $\mathcal{M}^d$ due to the intrinsic geometric constraints of $\bm{x}$.
We address this problem by introducing geometric constraints that allow us to compute $\dot{\bm{x}}$ on $\tangentspace{\bm{x}}$.
For example, for manifold $\SphereManifold^d$, the tangent vector $\dot{\bm{x}}$ is orthogonal to $\bm{x}$, that is $\bm{x}^\trsp \dot{\bm{x}}=0$.
Hence, we can find a solution by solving a constrained optimization problem, from which the pullback operator $D_{\bm{y}} \psi_{\bm{\theta}}^\star$ is obtained as,
\begin{equation}
    D_{\bm{y}} \psi_{\bm{\theta}}^\star = \left[ D_{\bm{x}} \psi_{\bm{\theta}}(\bm{x})^\trsp D_{\bm{x}} \psi_{\bm{\theta}}(\bm{x}) + \bm{x} \bm{x}^\trsp \right]^{-1} D_{\bm{x}} \psi_{\bm{\theta}}(\bm{x})^\trsp .
    \label{eq:pullback_operator_sphere_contrained_problem}
\end{equation}
The full derivation and discussions can be found in App.~\ref{app:PullbackConstrainedOpt}.
Note that $D_{\bm{y}} \psi_{\bm{\theta}}^\star$ in~\eqref{eq:pullback_operator_sphere_contrained_problem} is specific to hypersphere manifolds due to the choice of constraints. 
Thus, this constrained optimization approach does not easily scale to compute the pullback operator for arbitrary Riemannian manifolds. 

\paragraph{Pullback operator via the adjoint method:}
\label{subsubsec:pullbackAdjoint}
To generalize the computation of the pullback operator for arbitrary Riemannian manifolds, we introduce a new approach based on a modified version of the adjoint method.
By reversing the integration time interval (i.e. from $[t_s, t_e]$ to $[t_e, t_s$]), we can determine the inverse diffeomorphism $\psi_{\bm{\theta}}^{-1}$, which is a distinct benefit of Neural MODEs.
Thus, the pullback operator $D_{\bm{y}} \psi_{\bm{\theta}}^\star$ can be viewed as the differential of the inverse diffeomorphism $D_{\bm{y}} (\psi_{\bm{\theta}}^{-1})$.
Furthermore, we leverage the adjoint method to compute the differential of $\psi_{\bm{\theta}}^{-1}$
using the adjoint ODE
$\dot{\bm{A}}^*(t) = - \bm{A}^*(t) D_{\bm{z}(t)}  f_{\bm{\theta}}(\bm{z}(t), t)$, with $\bm{A}^*(t):=D_{\bm{z}(t)} (\psi_{\bm{\theta}}^{-1})$.
Due to the availability of starting states $\bm{z}(t_s) = \bm{x}$ and $\bm{A}^*(t_s) = \bm{I}_n$, we can integrate both the Neural MODE~\eqref{eq:neuralode} and adjoint ODE to get the $D_{\bm{y}} (\psi_{\bm{\theta}}^{-1})$.
For clarification, we provide Algorithm~\ref{alg:differential_inv_diffeomorphism} in App.~\ref{app:PullbackAdjoint} for computing $D_{\bm{y}} (\psi_{\bm{\theta}}^{-1})$.
Although, we can use dynamic charts method to solve the Neural MODE, dealing with the adjoint ODE dynamics is still not straightforward. 
The main challenge is to compute the differential of the vector fields on the Riemannian manifold $D_{\bm{z}(t)}  f_{\bm{\theta}}(\bm{z}(t), t)$, despite it is nothing but partial derivatives in the Euclidean case.
To avoid directly computing the differential of vector fields on $\manifold$, we adopt an approach similar to~\eqref{eq:dynamic_chart_composition}, such that a component $\bm{z}_i = (\operatorname{Exp}_{\bm{z}_i} \circ \hat{\psi}_{\bm{\theta},i}^{-1} \circ \operatorname{Log}_{\bm{z}_i}) \bm{z}_{i+1}$ for computing the inverse diffeomorphism has its differential as, 
\begin{equation}
    D_{\bm{z}_{i+1}} \bm{z}_i = D_{\bm{w}_i(t_{i, s})} \operatorname{Exp}_{\bm{z}_i} \circ D_{\bm{w}_i(t_{i, e})} \hat{\psi}_{\bm{\theta},i}^{-1} \circ D_{\bm{z}_{i+1}} \operatorname{Log}_{\bm{z}_i} ,
    \label{eq:inv_block_eq}
\end{equation}
where $D_{\bm{w}_i(t_{i,e})} \hat{\psi}_{\bm{\theta},i}^{-1}$ boils down to partial derivatives (the proof of~\eqref{eq:inv_block_eq} is provided in App.~\ref{app:PullbackAdjoint}).