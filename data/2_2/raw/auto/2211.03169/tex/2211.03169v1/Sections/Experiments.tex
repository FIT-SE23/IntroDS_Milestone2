\section{Experiments}
\label{sec:result}
We evaluate our method in two settings: reproducing trajectories based on the LASA dataset~\citep{Lee18:RiemannManifold} projected on $\SphereManifold^2$; and reproducing real dynamic motions learned from demonstrations. 
To show the importance of incorporating geometry, we compare against a baseline method similar to Euclideanizing flows~\citep{Rana2020:EuclideanizingFlows}, that is implemented using CNFs with Neural ODEs in $\euclideanspace^n$ for illustrative experiments on a modified LASA dataset. We refer to this baseline as \textit{EuclideanFlow} and our model as \textit{RSDS}.

\begin{figure}[t!]
    \includegraphics[width =.24\linewidth]{Images/Experiments/Sphere/riemannianflows/P_Shape_0.png}
    \includegraphics[width = .24\linewidth]{Images/Experiments/Sphere/riemannianflows/W_Shape_0.png}
    \includegraphics[width =.24\linewidth]{Images/Experiments/Sphere/euclideanflows/Normal/2/P-Shape-5.png}
    \includegraphics[width = .24\linewidth]{Images/Experiments/Sphere/euclideanflows/Normal/2/W_Shape_5_2_cut.png}
    \caption{Experiments on LASA dataset on $\SphereManifold^2$ ($\mathsf{P}$ and $\mathsf{W}$ letters): Demonstrations (white), learned vector fields, and reproductions (black and blue). Blue trajectories start at the same initial points as the demonstrations, while the black ones depart from randomly-sampled points around the initial points of the demonstrations. The first two plots show the results for our RSDS approach, and the last two display the \textit{EuclideanFlow} results.}           
    \label{fig:lasa}
\end{figure}

\subsection{LASA dataset on $\SphereManifold^2$}
\label{subsec:ExpSphere}
\paragraph{Architectures:}
For \textit{EuclideanFlow}, we use a fully-connected neural network with an input vector on $\euclideanspace^{4}$ (i.e., the $3$-dimensional state $\bm{x}$ and time), and $3$ hidden layers each, with $32$ hidden units for $\SphereManifold^2$.
We use $\operatorname{tanh}$ as activation function to guarantee a $\mathcal{C}^1$-bounded mapping for modeling the Neural ODE. 
The RSDS architecture has an additional projection operator $\proju$ on the head of the network to impose the output on the tangent space of manifolds.
The scaling factor $\hat{k}_{\bm{\gamma}}$ is generated using a network composed of an RBF layer and a linear layer without bias. 
The network architectures are depicted in App.~\ref{app:NetworkArchiecture} (Fig.~\ref{fig:net_structures_framework}). 
For all the experiments, we use an Euler ODE solver with step size of $1/32$, and $4$ coordinate charts for the integration. 
For each dataset, one trajectory is used for testing and the remaining ones as training set. 
All models are trained using the ADAM optimizer~\citep{kingma2014:ADAM} with learning rate of $10^{-3}$ (decaying by factor $0.1$ after $1000$ epochs) over $2000$ epochs. 

To illustratively show how RSDS and \emph{EuclideanFlow} perform, we use a modified version of the LASA dataset of hand-written letters~\citep{Lemme2015:LasaDataset}, whose trajectories are projected on $\SphereManifold^2$.
The corresponding vector field can be easily computed from the projected trajectories by using the logarithmic map. 
As a result, we have a new dataset $\{\{\bm{x}_{m,t}, \Dot{\bm{x}}_{m,t}\}_{t=1}^{T_m}\}_{m=1}^M$ of $M$ demonstrations for each letter, with positions $\bm{x} \in \SphereManifold^2$ and velocities $\Dot{\bm{x}} \in \mathcal{T}_{\bm{x}}\SphereManifold^2$, from which we learn stable dynamical systems. 
Figure~\ref{fig:lasa} shows the resulting demonstrations as white curves for datasets $\mathsf{P}$ and $\mathsf{W}$, the corresponding learned vector fields and the reproduced trajectories.
Note that we provide more results in App.~\ref{app:extended_results} and comparisons to an alternative method to \textit{EuclideanFlow}, which projects trajectories onto the manifold after computing the integration in Euclidean space.
Concerning the reproductions, the blue and black trajectories are rollouts starting from the same initial position as the demonstrations and from randomly-sampled points around them, respectively.
Regarding our RSDS approach (first two plots in Fig.~\ref{fig:lasa}), it is evident that all blue and black rollouts closely match the demonstrations and converge to the equilibrium. 
In contrast, the \textit{EuclideanFlow} reproductions  constantly leave the manifold since there are no mechanisms accounting for the inherent geometric constraints of the data (see last two plots in Fig.~\ref{fig:lasa}). 
The bald regions on the manifold where the velocity vectors point inwards towards the sphere's center are also evidence of this phenomenon.
We also provide quantitative metrics for accuracy comparisons.
Figure~\ref{fig:metrics}-\emph{right}, and -\emph{middle} show the \textit{dynamic time warping distance} (DTWD) as a measure of reproduced position trajectory accuracy, and the \textit{mean squared error} (MSE) of the velocities reproduction. 
These metrics show that RSDS outperforms \textit{EuclideanFlow} for the most complex trajectories, e.g. the $\mathsf{W}$ dataset.
Although Fig.~\ref{fig:metrics} shows that both models seem to perform well on the $\mathsf{P}$, $\mathsf{G}$, and $\mathsf{Multi Models}$ datasets, as pointed out before, Fig.~\ref{fig:lasa} displays that the \textit{EuclideanFlow} trajectories do not obey the data geometry. 

Secondly, we evaluate the stability of both approaches. For a fair comparison, we first project the vector fields onto $\SphereManifold^2$ and compute the integration trajectories for \textit{EuclideanFlow}. 
To quantitatively assess this, we measure the stability of the learned vector fields (i.e. convergence to the equilibrium), by uniformly sampling $1000$ initial points on $\SphereManifold^2$ and counting the number of trajectories that successfully converge.
This procedure is repeated for $7$ different trained models, with the average success rate computed over the initial points. 
Using one of the trained models, Fig.~\ref{fig:stability} shows green and red curves representing successful and failed trajectories, respectively. 
It is evident that a large number of the \textit{EuclideanFlows} trajectories failed to converge despite the projection, however all the \textit{RSDS} trajectories succeeded.
This result is supported by the success rate metric displayed in Fig.~\ref{fig:metrics}--\emph{left}.
\begin{figure}[t!]
    \includegraphics[width =.24\linewidth]{Images/Experiments/Sphere/riemannianflows/Stability/G_Shape_S_1_5.png}
    \includegraphics[width =.24\linewidth]{Images/Experiments/Sphere/riemannianflows/Stability/M_Shape_S_1_5.png}
    \includegraphics[width =.24\linewidth]{Images/Experiments/Sphere/euclideanflows/Stability/G_Shape_S_0_5.png}
    \includegraphics[width =.24\linewidth]{Images/Experiments/Sphere/euclideanflows/Stability/M_Shape_S_0_5.png}
    \caption{Stability of reproductions on $\SphereManifold^2$ for $\mathsf{G}$ and $\mathsf{Multi Models}$: $1000$ trajectories starting from uniformly-sampled points.
    The successful and failed trajectories are depicted in green and red, respectively.
    The first two spheres from the left correspond to RSDS reproductions while the other two relate to \textit{EuclideanFlow} results. For \textit{EuclideanFlow}, we first project the vector fields onto $\SphereManifold^2$ and then compute the integration trajectories.} 
    \label{fig:stability}
\end{figure}
These results show that accounting for the data geometry, as in our RSDS approach, is crucial to provide stability guarantees of the learned dynamical system.
\begin{figure}[t!]
    \includegraphics[width =.33\linewidth]{Images/Experiments/Sphere/metrics/stability_comparison_bar_h.pdf}
    \includegraphics[width =.33\linewidth]{Images/Experiments/Sphere/metrics/MSE_comparison_bar_h.pdf}
    \includegraphics[width =.33\linewidth]{Images/Experiments/Sphere/metrics/DTWD_comparison_bar_h.pdf}
    \caption{\textit{Left}: Average success rate of RSDS and \emph{EuclideanFlow} over randomly-sampled initial points on $\SphereManifold^2$ using $7$ different trained models indicated as points. \textit{Middle:} Average mean square error (MSE) between observed and predicted velocity over data points in the test trajectories indicated as points. \textit{Right:} Dynamic time warping distance (DTWD) between demonstrations and reproductions.} 
    \label{fig:metrics}
\end{figure}
Additionally, we also compare the learning efficiency between the two methods, where RSDS generally requires fewer training epochs than \textit{EuclideanFlow}. 
Nevertheless, each training epoch of \textit{RSDS} is more computationally expensive due to manifold operations. 
The details of these results are provided in App.~\ref{app:extended_results} (Fig.~\ref{fig:loss_comparison_appendix}). 

\subsection{Real robot experiments on $\RtimeS$}
We evaluated two different manipulation tasks using the $7$-DOF Franka-Emika Panda robot: \emph{(1)} a $\mathsf{GraspingTask}$ with $90$ degrees rotation, and \emph{(2)} a $\mathsf{V}$-shape $\mathsf{DrawingTask}$. 
For both experiments, we collected $10$ kinesthetic demonstrations of the robot end-effector motion as position-velocity trajectories at a frequency of \SI[group-digits=false]{10}{\hertz}.
Here, we show that RSDS can indeed be used in real-world applications. 
We train our model on the $\RtimeS$ manifold accounting for position and orientation of robot end-effector with the same network architecture as our illustrative experiments, except that we use $16$ hidden units for faster computations.
As shown in Fig.~\ref{fig:robot_exp}, while following the $\mathsf{V}$-shape curve in the $\mathsf{DrawingTask}$, the end-effector always faces the moving direction; similarly, when approaching the object in the $\mathsf{GraspingTask}$, the gripper rotates $90$ degrees.
These motion patterns require synchronized position and orientation trajectories, which is only attainable by training a model with a state variable on a product manifold, i.e., $\bm{x} \in \RtimeS$. 
To deploy the reproduced motion on the robot, we numerically integrated the desired velocity vector $\hat{\bm{x}} \in \tangentspace{\Dot{\bm{x}}}$ online, and used it as reference for a Cartesian impedance controller.

As observed in Fig.~\ref{fig:robot_exp}, the reproductions governed by the vector fields learned with our RSDS model accurately imitate the demonstrated motion patterns and converge to the goal position. 
Furthermore, these experiments incorporated some target shifts to test if our model could cope with them without retraining. 
We further evaluated the stability of the learned vector fields by perturbing the robot during the task reproduction. 
As Fig.~\ref{fig:robot_exp} shows, after perturbing the robot, the end-effector still follows an alternative trajectory computed from the learned vector field (see black and orange trajectories for $\mathsf{DrawingTask}$ and $\mathsf{GraspingTask}$, respectively).

\label{subsec:ExpReal}
\begin{figure}[t!]
    \includegraphics[width =.245\linewidth]{Images/Experiments/Robot/V/Reproduction_V.pdf}
    \includegraphics[width =.245\linewidth]{Images/Experiments/Robot/V/Perturbation_V.pdf}
    \includegraphics[width =.245\linewidth]{Images/Experiments/Robot/Grasp/Reproduction_Grasping_2.pdf}
    \includegraphics[width =.245\linewidth]{Images/Experiments/Robot/Grasp/Perturbation_Grasping_2.pdf}
    \caption{$\mathsf{DrawingTask}$ and $\mathsf{GraspingTask}$: Time evolution of the reproductions is depicted by superimposed images from different time frames. The transparent robots depict the trace of the motion trajectory. The second plot for each task displays the task reproduction under perturbation, where the unperturbed reproduction is depicted as a green trajectory for reference.} 
    \label{fig:robot_exp}
\end{figure}
