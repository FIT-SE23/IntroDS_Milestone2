\section{Background}
\label{sec:background}

\subsection{Dynamical Systems and Lyapunov Stability}
\label{subsec:DSlyapunov}
We here give a short review of Lyapunov stability in the Euclidean setting.
Let us assume an autonomous dynamical system $\dot{\bm{x}} = f(\bm{x})$, with a single equilibrium point $\bm{x}^*$, where $\bm{x} \in \mathbb{R}^n$ is the state variable.
Consider a potential function $V(\bm{x}(t))$ describing the energy of such a system.
If this system loses energy over time and the energy is never restored, the system must eventually reach some final resting state.
This idea is formally described as (see~\citep{Slotine91:AppliedNonlinearCtrl} for details):
\begin{theorem}[Lyapunov Stability]
\label{th:lyapunov_stability}
A dynamical system $\dot{\bm{x}} = f(\bm{x})$ is globally asymptotically stable at $\bm{x}^*$ if there exists a continuously differentiable Lyapunov function $V(\bm{x}): \mathbb{R}^n \rightarrow \mathbb{R}$ such that 
\begin{equation}
        V(\bm{x}^*) = 0 , \quad \dot{V}(\bm{x}^*) = 0 , \quad V(\bm{x}) > 0 ,\  \forall \ \bm{x} \neq \bm{x}^* , \quad  \dot{V}(\bm{x}) < 0 , \ \forall \  \bm{x} \neq \bm{x}^* .
    \label{eq:lyapunov_conditions}
\end{equation}
\end{theorem}
From Theorem~\ref{th:lyapunov_stability} we know that we can always find a Lyapunov function that fulfills these four conditions in Eq.~\ref{eq:lyapunov_conditions} for a globally asymptotically stable dynamical system.
\vspace{.5cm}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Riemannian Manifolds}
\label{subsec:RiemanianManif}
A smooth manifold $\manifold$ can be seen as a set of points that locally, but not globally, resemble the Euclidean space $\euclideanspace^d$~\citep{DoCarmo92:RiemannManifold,Lee18:RiemannManifold}. 
An abstract definition of a manifold specifies the topological, differential and geometric structure by using \emph{charts}, which are maps between parts of $\manifold$ to $\euclideanspace^d$.
The collection of these charts (a.k.a. local parameterizations) is called \emph{atlas}.
More formally, a chart on a smooth manifold $\manifold$ is a diffeomorphic mapping (i.e. a bijective and differentiable function) $\varphi: U \to \tilde{U}$ from an open set $U \subset \manifold$ to an open set $\tilde{U} \subseteq \euclideanspace^d$ (see Fig.~\ref{fig:coordinate_chart} in App.~\ref{app:RiemannianManif}).
Moreover, the transition map between two intersecting sets $U_1$ and $U_2$, given by $\varphi_1 \circ \varphi_2^{-1}$ or $\varphi_2 \circ \varphi_1^{-1}: \mathbb{R}^d \rightarrow \mathbb{R}^d$ is also a diffeomorphism.
The smooth structure of $\manifold$ makes it possible to take derivatives of curves on the manifold, leading to tangent vectors in $\euclideanspace^d$.
The set of tangent vectors of all curves at $\bm{x} \in \manifold$ spans a $d$-dimensional affine subspace of $\euclideanspace^d$, which is known as the \emph{tangent space} $\tangentspace{\bm{x}}$ of $\manifold$ at $\bm{x}$.
The collection of all tangent spaces of $\manifold$ is the \emph{tangent bundle} $\tangentspace{} = \bigsqcup_{\bm{x} \in \manifold} \tangentspace{x}$.
Therefore, a velocity vector $\dot{\bm{x}}$ at $\bm{x} \in \manifold$ lies on $\tangentspace{x}$, and consequently a vector field on $\manifold$ lies on $\tangentspace{}$.

The above definitions do not provide the mechanisms to measure how curved $\manifold$ is, or to compute distances on $\manifold$. 
To do so, we can endow $\manifold$ with a \emph{Riemannian metric}, which is a family of inner products $g_{\bm{x}}: \tangentspace{\bm{x}} \times  \tangentspace{\bm{x}} \rightarrow \euclideanspace$ associated to each point $\bm{x} \in \manifold$.
As a result, a \emph{Riemannian manifold} $(\manifold, g)$ is a smooth manifold endowed with a Riemannian metric~\citep{Lee18:RiemannManifold}. 
To operate with Riemannian manifolds, it is common practice to exploit the Euclidean tangent spaces. 
To do so, we resort to mappings back and forth between $\tangentspace{\bm{x}}$ and $\manifold$ using the exponential and logarithmic maps.
The exponential map $\expmap{\bm{x}}{\bm{u}}: \tangentspace{\bm{x}} \to \manifold$ maps a point $\bm{u}$ in the tangent space of $\bm{x}$ to a point $\bm{y}$ on the manifold, so that it lies on the geodesic starting at $\bm{x}$ in the direction $\bm{u}$, and such that the geodesic distance $d_{\manifold}(\bm{x}, \bm{y}) = d_{\euclideanspace}(\bm{x}, \bm{u})$. 
The inverse operation is the logarithmic map $\logmap{\bm{x}}{\bm{y}}: \manifold \to \tangentspace{\bm{x}}$.
We provide all the necessary operations in App.~\ref{app:RiemannianManif}.
\vspace{.5cm}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Diffeomorphism}
\label{subsec:Diffeomorphism}
A diffeomorphism $\diffeomorphism: \manifold \rightarrow \mathcal{N}$ is a smooth bijective mapping between two smooth manifolds which preserves the topological properties of $\manifold$, and whose inverse $\diffeomorphism^{-1}$ is also smooth.
When learning stable dynamical systems, diffeomorphisms can be exploited to impose Lyapunov stability guarantees by transferring a manually-designed stable dynamical system on $\mathcal{N}$ to the desired manifold $\manifold$. 
We focus on constructing learnable diffeomorphisms that resemble continuous normalizing flows (CNFs)~\citep{chen2018:NeuralODE,Grathwohl19:FFJORD,Finlay2020:JacobianRegularization}, which are bijective and bidirectionally differentiable mappings, and have been recently exploited on density estimation problems~\citep{Rezende2015:VIwithNF,Papamakarios21:NormalizingFlows,Kobyzev2021:IntroductionNFs}.
We here exploit them to learn diffeomorphic mappings between Riemannian manifolds. 

Most CNFs are constructed from \emph{neural ordinary differential equation} (Neural ODEs) in Euclidean space~\citep{chen2018:NeuralODE,Grathwohl19:FFJORD,Finlay2020:JacobianRegularization}, with the exception of \emph{neural manifold ordinary differential equations} (Neural MODEs) on Riemannian manifolds~\citep{lou2020:NeuralMODE, mathieu2020:RCNormalizingFlows}.
Generally, Neural ODEs parametrize the dynamics of a hidden variable using a continuous-time ODE represented by a neural network, as follows,
\begin{equation}
    \dot{\bm{z}}(t) = f_{\bm{\theta}}(\bm{z}(t), t) ,
    \label{eq:neuralode}
\end{equation}
where $\bm{z} \in \euclideanspace^d$ is the state variable and $f_{\bm{\theta}}: \euclideanspace^d \times \euclideanspace \rightarrow \euclideanspace^d$ is a neural network.
According to~\citet{mathieu2020:RCNormalizingFlows} (see Theorem~\ref{th:vector_flows} in App.~\ref{app:NeuralMODEs}), we can extend CNFs to the Riemannian setting, where the state variable $\bm{z} \in \mathcal{M}$ and the vector field $f_{\bm{\theta}}: \mathcal{M} \times \mathbb{R} \rightarrow \tangentspace{}$.
As a result, we can use~\eqref{eq:neuralode} as a manifold ODE, whose initial value problem (IVP) solution results in a diffeormorphic mapping $\psi_{\bm{\theta}}: \manifold \rightarrow \mathcal{N}, \ \bm{x} = \bm{z}(t_s) \in \manifold$ and $\bm{y}=\bm{z}(t_e) \in \mathcal{N}$.
i.e. $\bm{y} = \psi_{\bm{\theta}}(\bm{x}) = \bm{x} + \int_{\tau=t_s}^{t_e} f_{\bm{\theta}}(\bm{z}(\tau), \tau) d\tau$.
To solve the IVP on $\manifold$, we leverage integrators on Riemannian manifolds based on the local representation via coordinate charts~\citep{hairer2011:ODEManifolds}.
This method uses a local representation of $\mathcal{M}$ defined by a coordinate map $\varphi: \mathcal{M} \supseteq U \rightarrow \tilde{U} \subseteq \mathbb{R}^d$ with coordinates $\bm{w}(t) = \varphi(\bm{z}(t))$. 
Computing integrators on $\manifold$ can be approximated by solving an equivalent ODE in $\euclideanspace^d$
\begin{equation}
    \dot{\bm{w}}(t) = D_{\varphi^{-1}(\bm{w}(t))}\varphi \circ f_{\bm{\theta}}(\varphi^{-1}(\bm{w}(t)), t) ,
    \label{eq:equivalentode}
\end{equation}
where $D_{\varphi^{-1}(\bm{w}(t))}\varphi$ represents the differential of $\varphi$ at $\varphi^{-1}(\bm{w}(t))$
(see App.~\ref{app:NeuralMODEs} and~\ref{app:integrators_manifolds} for details).
Additionally, we use the adjoint method~\citep{chen2018:NeuralODE} on Riemannian manifolds~\citep{lou2020:NeuralMODE} to compute gradients, which can also be used for calculating differentials. Consider a loss function $\mathcal{L} : \mathcal{M} \rightarrow \mathbb{R}$, in order to compute the derivative of $\mathcal{L}$ with respect to an intermediate variable $\boldsymbol{z}(t)$ of the manifold ODE, we can solve $\dot{\bm{a}}(t)^\trsp = - \bm{a}(t)^\trsp D_{\bm{z}(t)}  f_{\bm{\theta}}(\bm{z}(t), t)$, where $\bm{a}(t)^\trsp := D_{\bm{z}(t)} \mathcal{L}$  (as detailed in App.~ \ref{app:adjoint_method}).
\vspace{2cm}

