
@misc{Authors20,
 author = {Authors},
 title = {The frobnicatable foo filter},
 note = {Face and Gesture  submission ID 324. Supplied as additional material {\tt fg324.pdf}},
 year = 2020
}

@inproceedings{lucaskanade,
author = {Lucas, Bruce D. and Kanade, Takeo},
title = {An Iterative Image Registration Technique with an Application to Stereo Vision},
year = {1981},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
abstract = {Image registration finds a variety of applications in computer vision. Unfortunately, traditional image registration techniques tend to be costly. We present a new image registration technique that makes use of the spatial intensity gradient of the images to find a good match using a type of Newton-Raphson iteration. Our technique is taster because it examines far fewer potential matches between the images than existing techniques Furthermore, this registration technique can be generalized to handle rotation, scaling and shearing. We show how our technique can be adapted tor use in a stereo vision system.},
booktitle = {Proceedings of the 7th International Joint Conference on Artificial Intelligence - Volume 2},
pages = {674–679},
numpages = {6},
location = {Vancouver, BC, Canada},
series = {IJCAI'81}
}



@article{flownet,
  author    = {Philipp Fischer and
               Alexey Dosovitskiy and
               Eddy Ilg and
               Philip H{\"{a}}usser and
               Caner Hazirbas and
               Vladimir Golkov and
               Patrick van der Smagt and
               Daniel Cremers and
               Thomas Brox},
  title     = {FlowNet: Learning Optical Flow with Convolutional Networks},
  journal   = {CoRR},
  volume    = {abs/1504.06852},
  year      = {2015},
  url       = {http://arxiv.org/abs/1504.06852},
  eprinttype = {arXiv},
  eprint    = {1504.06852},
  timestamp = {Mon, 13 Aug 2018 16:49:12 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/FischerDIHHGSCB15.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{opticalflowearly3,
title = "A Duality Based Approach for Realtime TV-L1 Optical Flow",
author = "Christopher Zach and Thomas Pock and Horst Bischof",
year = "2007",
language = "English",
pages = "214--223",
booktitle = "Proceedings 29th DAGM Symposium {"}Pattern Recognition{"}",
publisher = "Springer",
note = "29th DAGM Symposium on Pattern Recognition : DAGM 2007 ; Conference date: 12-09-2007 Through 14-09-2007",
}

@inproceedings{opticalflowearly2,
  title = {A framework for the robust estimation of optical flow},
  author = {Black, M. J. and Anandan, P.},
  booktitle = {Fourth International Conf. on Computer Vision, ICCV-93},
  pages = {231-236},
  address = {Berlin, Germany},
  month = may,
  year = {1993},
  doi = {},
  month_numeric = {5}
}

@article{opticalflowearly1,
author = {Horn, Berthold K. P. and Schunck, Brian G.},
title = {Determining Optical Flow},
year = {1981},
issue_date = {August 1981},
publisher = {Elsevier Science Publishers Ltd.},
address = {GBR},
volume = {17},
number = {1–3},
issn = {0004-3702},
url = {https://doi.org/10.1016/0004-3702(81)90024-2},
doi = {10.1016/0004-3702(81)90024-2},
abstract = {Optical flow cannot be computed locally, since only one independent measurement is available from the image sequence at a point, while the flow velocity has two components. A second constraint is needed. A method for finding the optical flow pattern is presented which assumes that the apparent velocity of the brightness pattern varies smoothly almost everywhere in the image. An iterative implementation is shown which successfully computes the optical flow for a number of synthetic image sequences. The algorithm is robust in that it can handle image sequences that are quantized rather coarsely in space and time. It is also insensitive to quantization of brightness levels and additive noise. Examples are included where the assumption of smoothness is violated at singular points or along lines in the image.},
journal = {Artif. Intell.},
month = {aug},
pages = {185–203},
numpages = {19}
}

@misc{Authors20b,
 author = {Authors},
 title = {Frobnication tutorial},
 note = {Supplied as additional material {\tt tr.pdf}},
 year = 2020
}

@article{Alpher02,
author = {A. Alpher},
title = {Frobnication},
journal = {Journal of Foo},
volume = 12, 
number = 1, 
pages = {234--778}, 
year = 2002
}

@article{Alpher03,
author = {A. Alpher and  J.~P.~N. Fotheringham-Smythe},
title = {Frobnication revisited},
journal = {Journal of Foo},
volume = 13, 
number = 1, 
pages = {234--778}, 
year = 2003
}

@article{Alpher04,
author = {A. Alpher and J.~P.~N. Fotheringham-Smythe and G. Gamow},
title = {Can a machine frobnicate?},
journal = {Journal of Foo},
volume = 14, 
number = 1, 
pages = {234--778}, 
year = 2004
}

@article{hershey1999audio,
  title={Audio vision: Using audio-visual synchrony to locate sounds},
  author={Hershey, John and Movellan, Javier},
  journal={Advances in neural information processing systems},
  volume={12},
  year={1999}
}
@article{fisher2000learning,
  title={Learning joint statistical models for audio-visual fusion and segregation},
  author={Fisher III, John W and Darrell, Trevor and Freeman, William and Viola, Paul},
  journal={Advances in neural information processing systems},
  volume={13},
  year={2000}
}

@article{izadinia2012multimodal,
  title={Multimodal analysis for identification and segmentation of moving-sounding objects},
  author={Izadinia, Hamid and Saleemi, Imran and Shah, Mubarak},
  journal={IEEE Transactions on Multimedia},
  volume={15},
  number={2},
  pages={378--390},
  year={2012},
  publisher={IEEE}
}
@inproceedings{hu2019deep,
  title={Deep multimodal clustering for unsupervised audiovisual learning},
  author={Hu, Di and Nie, Feiping and Li, Xuelong},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={9248--9257},
  year={2019}
}
@inproceedings{zhao2019sound,
  title={The sound of motions},
  author={Zhao, Hang and Gan, Chuang and Ma, Wei-Chiu and Torralba, Antonio},
  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},
  pages={1735--1744},
  year={2019}
}
@inproceedings{afouras2020self,
  title={Self-supervised learning of audio-visual objects from video},
  author={Afouras, Triantafyllos and Owens, Andrew and Chung, Joon Son and Zisserman, Andrew},
  booktitle={European Conference on Computer Vision},
  pages={208--224},
  year={2020},
  organization={Springer}
}

@inproceedings{aytar2016soundnet,
  title={Soundnet: Learning sound representations from unlabeled video},
  author={Aytar, Yusuf and Vondrick, Carl and Torralba, Antonio},
  booktitle={Advances in Neural Information Processing Systems},
  year={2016}
}
@inproceedings{qian2020multiple,
  title={Multiple sound sources localization from coarse to fine},
  author={Qian, Rui and Hu, Di and Dinkel, Heinrich and Wu, Mengyue and Xu, Ning and Lin, Weiyao},
  booktitle={European Conference on Computer Vision},
  pages={292--308},
  year={2020},
  organization={Springer}
}

@inproceedings{zhou2016learning,
  title={Learning deep features for discriminative localization},
  author={Zhou, Bolei and Khosla, Aditya and Lapedriza, Agata and Oliva, Aude and Torralba, Antonio},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={2921--2929},
  year={2016}
}
@article{selvaraju2016grad,
  title={Grad-CAM: Why did you say that?},
  author={Selvaraju, Ramprasaath R and Das, Abhishek and Vedantam, Ramakrishna and Cogswell, Michael and Parikh, Devi and Batra, Dhruv},
  journal={arXiv preprint arXiv:1611.07450},
  year={2016}
}
@inproceedings{owens2018audio,
  title={Audio-visual scene analysis with self-supervised multisensory features},
  author={Owens, Andrew and Efros, Alexei A},
  booktitle={Proceedings of the European Conference on Computer Vision (ECCV)},
  pages={631--648},
  year={2018}
}

@inproceedings{senocak2018learning,
  title={Learning to localize sound source in visual scenes},
  author={Senocak, Arda and Oh, Tae-Hyun and Kim, Junsik and Yang, Ming-Hsuan and Kweon, In So},
  booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  pages={4358--4366},
  year={2018}
}

@inproceedings{chen2021localizing,
  title={Localizing visual sounds the hard way},
  author={Chen, Honglie and Xie, Weidi and Afouras, Triantafyllos and Nagrani, Arsha and Vedaldi, Andrea and Zisserman, Andrew},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={16867--16876},
  year={2021}
}




@inproceedings{song2022self,
  title={Self-Supervised Predictive Learning: A Negative-Free Method for Sound Source Localization in Visual Scenes},
  author={Song, Zengjie and Wang, Yuxi and Fan, Junsong and Tan, Tieniu and Zhang, Zhaoxiang},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={3222--3231},
  year={2022}
}

@inproceedings{senocak2022learning,
  title={Learning sound localization better from semantically similar samples},
  author={Senocak, Arda and Ryu, Hyeonggon and Kim, Junsik and Kweon, In So},
  booktitle={ICASSP 2022-2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  pages={4863--4867},
  year={2022},
  organization={IEEE}
}


@article{mo2022localizing,
  title={Localizing Visual Sounds the Easy Way},
  author={Mo, Shentong and Morgado, Pedro},
  journal={arXiv preprint arXiv:2203.09324},
  year={2022}
}
@inproceedings{girshick2015fast,
  title={Fast r-cnn},
  author={Girshick, Ross},
  booktitle={Proceedings of the IEEE international conference on computer vision},
  pages={1440--1448},
  year={2015}
}
@inproceedings{girshick2014rich,
  title={Rich feature hierarchies for accurate object detection and semantic segmentation},
  author={Girshick, Ross and Donahue, Jeff and Darrell, Trevor and Malik, Jitendra},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={580--587},
  year={2014}
}
@article{ren2015faster,
  title={Faster r-cnn: Towards real-time object detection with region proposal networks},
  author={Ren, Shaoqing and He, Kaiming and Girshick, Ross and Sun, Jian},
  journal={Advances in neural information processing systems},
  volume={28},
  year={2015}
}

@inproceedings{farneback2003two,
  title={Two-frame motion estimation based on polynomial expansion},
  author={Farneb{\"a}ck, Gunnar},
  booktitle={Scandinavian conference on Image analysis},
  pages={363--370},
  year={2003},
  organization={Springer}
}

@inproceedings{chen2020vggsound,
  title={Vggsound: A large-scale audio-visual dataset},
  author={Chen, Honglie and Xie, Weidi and Vedaldi, Andrea and Zisserman, Andrew},
  booktitle={ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  pages={721--725},
  year={2020},
  organization={IEEE}
}
@inproceedings{gao2020listen,
  title={Listen to look: Action recognition by previewing audio},
  author={Gao, Ruohan and Oh, Tae-Hyun and Grauman, Kristen and Torresani, Lorenzo},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={10457--10467},
  year={2020}
}
@inproceedings{zhou2021joint,
  title={Joint audio-visual deepfake detection},
  author={Zhou, Yipin and Lim, Ser-Nam},
  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},
  pages={14800--14809},
  year={2021}
}
@inproceedings{chen2020simple,
  title={A simple framework for contrastive learning of visual representations},
  author={Chen, Ting and Kornblith, Simon and Norouzi, Mohammad and Hinton, Geoffrey},
  booktitle={International conference on machine learning},
  pages={1597--1607},
  year={2020},
  organization={PMLR}
}

@article{grill2020bootstrap,
  title={Bootstrap your own latent-a new approach to self-supervised learning},
  author={Grill, Jean-Bastien and Strub, Florian and Altch{\'e}, Florent and Tallec, Corentin and Richemond, Pierre and Buchatskaya, Elena and Doersch, Carl and Avila Pires, Bernardo and Guo, Zhaohan and Gheshlaghi Azar, Mohammad and others},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={21271--21284},
  year={2020}
}
@article{devlin2018bert,
  title={Bert: Pre-training of deep bidirectional transformers for language understanding},
  author={Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  journal={arXiv preprint arXiv:1810.04805},
  year={2018}
}
@article{raffel2020exploring,
  title={Exploring the limits of transfer learning with a unified text-to-text transformer.},
  author={Raffel, Colin and Shazeer, Noam and Roberts, Adam and Lee, Katherine and Narang, Sharan and Matena, Michael and Zhou, Yanqi and Li, Wei and Liu, Peter J and others},
  journal={J. Mach. Learn. Res.},
  volume={21},
  number={140},
  pages={1--67},
  year={2020}
}
@article{akbari2021vatt,
  title={Vatt: Transformers for multimodal self-supervised learning from raw video, audio and text},
  author={Akbari, Hassan and Yuan, Liangzhe and Qian, Rui and Chuang, Wei-Hong and Chang, Shih-Fu and Cui, Yin and Gong, Boqing},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={24206--24221},
  year={2021}
}
@article{mu2021slip,
  title={Slip: Self-supervision meets language-image pre-training},
  author={Mu, Norman and Kirillov, Alexander and Wagner, David and Xie, Saining},
  journal={arXiv:2112.12750},
  year={2021}
}
@article{oord2018representation,
  title={Representation learning with contrastive predictive coding},
  author={Oord, Aaron van den and Li, Yazhe and Vinyals, Oriol},
  journal={arXiv:1807.03748},
  year={2018}
}
@inproceedings{arandjelovic2018objects,
  title={Objects that sound},
  author={Arandjelovic, Relja and Zisserman, Andrew},
  booktitle={Proceedings of the European conference on computer vision (ECCV)},
  pages={435--451},
  year={2018}
}

