\documentclass{article}

% \usepackage{corl_2022} % Use this for the initial submission.
% \usepackage[final]{corl_2022} % Uncomment for the camera-ready ``final'' version.
\usepackage[preprint]{corl_2022} % Uncomment for pre-prints (e.g., arxiv); This is like ``final'', but will remove the CORL footnote.
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subcaption}
\usepackage{booktabs} % for professional tables

\usepackage{multicol}
\usepackage{multirow}
\usepackage{color,colortbl}
% \usepackage{subfigure}

\usepackage{tabularx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}
\usepackage{enumitem}
\usepackage{graphicx}
\usepackage{hyperref}
\definecolor{Gray}{gray}{0.9}
\usepackage[export]{adjustbox}[2011/08/13]

\setlength{\belowcaptionskip}{-6pt}

% \title{Exploring Parameter-Efficient Tuning for Few-Shot Motion Forecasting}
% \title{Exploring Low-Rank Adaptation Techniques for Few-Shot Motion Adaptation}
% \title{Style updates have a low rank: Few-Shot Adaptation for Motion Forecasting}
% \title{Style Shift is Low Rank: Few-Shot Adaptation for Deep Motion Forecasting}
\title{Motion Style Transfer: Modular Low-Rank Adaptation for Deep Motion Forecasting}
% \title{Efficient Motion Style Transfer for Deep Motion Forecasting}
% \title{Modular Low-Rank Adaptation for Deep Motion Forecasting}
% The \author macro works with any number of authors. There are two
% commands used to separate the names and addresses of multiple
% authors: \And and \AND.
%
% Using \And between authors leaves it to LaTeX to determine where to
% break the lines. Using \AND forces a line break at that point. So,
% if LaTeX puts 3 of 4 authors names on the first line, and the last
% on the second line, try using \AND instead of \And before the third
% author name.

% NOTE: authors will be visible only in the camera-ready and preprint versions (i.e., when using the option 'final' or 'preprint'). 
% 	For the initial submission the authors will be anonymized.

% \; Danya Li \;  Yuejiang Liu \; Alexandre Alahi
\author{
  Parth Kothari \; Danya Li \;  Yuejiang Liu \; Alexandre Alahi \\
  VITA Lab, EPFL\\
%   United States\\
%   \texttt{janedoe@berkeley.edu} \\
  %% examples of more authors
  %% \And
  %% Coauthor \\
  %% Affiliation \\
  %% Address \\
  %% \texttt{email} \\
  %% \AND
  %% Coauthor \\
  %% Affiliation \\
  %% Address \\
  %% \texttt{email} \\
  %% \And
  %% Coauthor \\
  %% Affiliation \\
  %% Address \\
  %% \texttt{email} \\
  %% \And
  %% Coauthor \\
  %% Affiliation \\
  %% Address \\
  %% \texttt{email} \\
}


\begin{document}
\maketitle

%===============================================================================

% \begin{abstract} Deep motion forecasting models have achieved great success when trained on a massive amount of data. However, the performance suffers when the training data is limited. It is well-known that the knowledge learned by neural networks pre-trained on large data is transferrable, it can be reused across related tasks. In this paper, we propose a transfer learning technique for adapting a pre-trained motion prediction model to new domains such as unseen agent types and scene contexts, in a sample-efficient manner.  We hypothesize that one only needs to model the underlying style shift across domains that often reside in a low-dimensional space. We formulate this intuition in the form of a low-rank style-injection module, which is trained to account for the style shifts while keeping the pre-trained parameters frozen. Empirically, our method yields superior results over previous fine-tuning paradigms in low-resource settings. While updating less than 5\% extra parameters, our method achieves about 16\% and 20\% relative improvement compared to fully fine-tuned models on Stanford Drone and Lyft Level 5 respectively.

% % on two popular datasets, Stanford Drone and Lyft Level 5,

% % Deep motion forecasting models struggle to generalize to novel scenarios not observed in the training set. In this paper, we propose to address this challenge by efficiently adapting a pre-trained model to new domains, such as unseen agent types and scene contexts. We hypothesize that most of the knowledge captured by recent models can be reused across domains, and we only need to model the underlying style shift which is sparse. We formulate this intuition into a low-rank style-injection module, which is trained to account for the style shifts while keeping the pre-trained parameters frozen. Empirically, our method yields superior results over existing fine-tuning methods on two popular datasets, Stanford Drone and Lyft Level 5, in few-shot settings.
% \end{abstract}

% % Parth 0615
% \begin{abstract} Deep motion forecasting models have achieved great success when trained on a massive amount of data. However, the performance suffers when the training data is limited. We propose a transfer learning technique for adapting a pre-trained motion prediction model to new domains such as unseen agent types and scene contexts, in a sample-efficient manner. We hypothesize that one only needs to model the underlying style shift across domains, that often resides in a low-dimensional space. We formulate this intuition into our motion style adapter (MoSA) design, that is trained to infer and update the style factors of variation in target domain while keeping the pre-trained parameters frozen. Additionally, we present a modularized style transfer strategy that updates only a subset of model layers given the nature of style transfer problem. We empirically demonstrate that MoSA improves the generalization performance by $25\%$ using just 30 samples on Stanford Drone and Intersection Drone dataset. Moreover, while updating less than $5\%$ extra parameters, our method achieves $20\%$ relative improvement compared to fully fine-tuned models on Lyft Level 5 dataset. 
% % Modularization further boosts performance in these low-resource settings.
% \end{abstract}


% Yuejiang at 0615
\begin{abstract} 
Deep motion forecasting models have achieved great success when trained on a massive amount of data. Yet, they often perform poorly when training data is limited. 
To address this challenge, we propose a transfer learning approach for efficiently adapting pre-trained forecasting models to new domains, such as unseen agent types and scene contexts.
Unlike the conventional fine-tuning approach that updates the whole encoder, our main idea is to reduce the amount of tunable parameters that can precisely account for the target domain-specific motion style.
To this end, we introduce two components that exploit our prior knowledge of motion style shifts:
(i) a low-rank motion style adapter that projects and adjusts the style features at a low-dimensional bottleneck; and
(ii) a modular adapter strategy that disentangles the features of scene context and motion history to facilitate a fine-grained choice of adaptation layers.
Through extensive experimentation, we show that our proposed adapter design, coined MoSA, outperforms prior methods on several forecasting benchmarks. Code available at \url{https://github.com/vita-epfl/motion-style-transfer}

% Experiment results show that our method improves the performance of modern forecasting models by $25\%$ using only 30 samples from the target domain on Stanford Drone and Intersection Drone dataset. Moreover, while updating less than $5\%$ extra parameters, our method achieves $20\%$ relative improvement compared to fully fine-tuned models on Lyft Level 5 dataset. 
\end{abstract}




% Two or three meaningful keywords should be added here
\keywords{Motion Forecasting, Distribution Shifts, Transfer Learning} 

%===============================================================================

\section{Introduction}
Motion forecasting is an essential pillar for the successful deployment of autonomous systems in environments comprising various heterogeneous agents. It presents the challenges of modeling (i) universal etiquette (\textit{e.g.}, goal-directed behaviors, avoiding collisions) that govern general motion dynamics of all agents; and (ii) social norms (\textit{e.g.}, the minimum separation distance, preferred speed) that influence the navigation styles of different agents across different locations. Owing to the success of deep neural networks on large-scale datasets, learning prediction models in a data-driven manner has become a de-facto approach for motion forecasting and has shown impressive results \citep{Alahi2016SocialLH, Mangalam2021FromGW, Kothari2020HumanTF, Salzmann2020TrajectronDT}. 

However, existing deep forecasting models suffer from inferior performance when they encounter novel scenarios \cite{wang2022transferable, xu2022adaptive,saadatnejad2022socially,bahari2022vehicle}. For instance, a network trained with large-scale data for pedestrian forecasting struggles to directly generalize to cyclists. Some recent methods propose to incorporate strong priors robust to the underlying distribution shifts  \cite{liang2020simaug, liu2021social, bhattacharyya2022ssl}. Yet, these priors often make strong assumptions on the distribution shifts, which may not hold in practice. This shortcoming motivates the following transfer learning paradigm: adapting a forecasting model pretrained on one domain with sufficient data to new domains such as unseen agent types and scene contexts \textit{as efficiently as possible}.

One common transfer learning approach is fine-tuning a pretrained model on the data collected from target domain. However, directly updating the model is often sample inefficient, as it fails to exploit the inherent structure of the distributional shifts in the motion context. In the forecasting setup, the physical laws behind motion dynamics are generally invariant across geographical locations and agent types: all agents move towards their goal and avoid collisions. As a result, the distribution shift can be largely attributed to the changes in the motion style, defined as the way an agent interacts with its surroundings. Given this decoupling of motion dynamics, it can be efficient for an adaptation algorithm to only account for the updates in the target motion style.

% we can leverage the fact that 

% , for e.g., pedestrians prefer to move on sidewalks while cyclists prefer road lanes
% more desirable / suitable / advantageous


In this work, we efficiently adapt a deep forecasting model from one motion style to another. We refer to this task as \textit{motion style transfer}. We retain the domain-invariant dynamics by freezing the pre-trained network weights. To learn the underlying shifts in style during adaptation, we introduce motion style adapters (MoSA), which are new modules inserted in parallel to the encoder layers. The style shift learned by MoSA is injected into the frozen pre-trained model. We hypothesize that the style shifts across forecasting domains often reside in a low-dimensional space. To formulate this intuition, we design MoSA as a low-dimensional bottleneck, inspired by recent works in language \cite{hu2021lora, Mahabadi2021CompacterEL}. Specifically, MoSA comprises two trainable matrices with a low rank. The first matrix is responsible for extracting the style factors to be updated, while the second enforces the updates. MoSA learns the style updates by adding and updating less than $2\%$ of the parameters in \textit{each layer}. 

% OLD: Morning 12th July
% In this work, we aim to effectively adapt a deep forecasting model from one motion style to another with limited data. We refer to this task as \textit{motion style transfer}. To preserve the domain-invariant dynamics, we freeze the weights of the pretrained network. On the other hand, we introduce motion style adapters (MoSA), which are new modules in parallel to the encoder layers, to learn the underlying shifts in style during adaptation. The style shift learned by MoSA is subsequently injected into the frozen pretrained model. We hypothesize that the style shifts across forecasting domains often reside in a low-dimensional space. To formulate this intuition, we design MoSA as a low-dimensional bottleneck, inspired by recent works in language \cite{hu2021lora, Mahabadi2021CompacterEL}. Spcifically, MoSA comprises two trainable matrices with low rank where the first matrix is responsible for extracting the style factors to be updated, while the second enforces the updates. MoSA learns the style updates by adding and updating less than $2\%$ of the parameters in \textit{each layer}. 
%%%%%%%%%%%%%%%%%%%%%%%%


% MoSA learns the style updates by adding and updating less than $2\%$ of the parameters in \textit{each layer}. We formulate this intuition in the form of Motion Style Adapter (MoSA), designed as a low-dimensional bottleneck, inspired by recent works in language \cite{hu2021lora, Mahabadi2021CompacterEL}.

In low-resource settings, it can be difficult for MoSA to distinguish the relevant encoder layers updates from the irrelevant ones, resulting in sub-optimal performance. To facilitate an informed choice of adaptation layers, we propose a modularized adaptation strategy. Specifically, we consider forecasting architectures that disentangle the fine-grained scene context and past agent motion using two independent low-level encoders. This design allows the flexible injection of MoSA to one encoder while leaving the other unchanged. Given the style transfer setup, our modular adaptation strategy yields substantial performance gains in the low-data regime.

% leading to overfitting on statistically spurious correlations.
% To promote a targetted choice of adaptation layers,

% Motion style can be decoupled into (i) scene-specific style that influences the motion dynamics due to physical structures like roads and sidewalks, and (ii) the agent-specific style that captures the navigation preferences of different agents like preferred speed. To model these style components separately,

% Our modular adaptation strategy, based upon the type of style transfer setup \textit{known a-priori}, yields substantial performance gains in the low-resource setting.

We empirically demonstrate the efficiency of MoSA on the state-of-the-art model Y-Net \citep{Mangalam2021FromGW} on the heterogenous SDD \citep{Robicquet2016LearningSE} and inD \cite{Bock2020TheID} datasets in various style transfer setups. Next, we highlight the potential of our modularized adaptation strategy on two setups: Agent Motion Style Transfer and Scene Style Transfer. Finally, to showcase the generalizability of MoSA in self-driving applications, we adapt a large-scale model trained on one part of the city to an unseen part, on the Level 5 Dataset \citep{Houston2020OneTA}. Through extensive experimentation, we quantitatively and qualitatively show that given just 10-30 samples in the new domain, MoSA improves the generalization error by $25\%$ on SDD and inD. Moreover, our design outperforms standard fine-tuning techniques by $20\%$ on the Level 5 dataset.


\section{Related Work}

\textbf{Motion forecasting.} Classical models described the interactions between various agents based on domain knowledge but often failed to model complex social interactions in crowds \citep{Helbing1995SocialFM, Coscia2018LongtermPP, Antonini2006DiscreteCM}. Following the success of Social LSTM \citep{Alahi2016SocialLH}, various data-driven forecasting models have been proposed to capture social interactions directly from observed data \citep{Kothari2020HumanTF, Vemula2018SocialAM, Mohamed2020SocialSTGCNNAS, Salzmann2020TrajectronDT, Kothari2021InterpretableSA, Kothari2022SafetycompliantGA}. These methods heavily rely on a large and diverse set of training data, which may not be readily available for novel agents and locations. In this work, we efficiently adapt a pretrained model to unseen target domains.

% Instead of training models from scratch, we explore a novel technique that aims to efficiently adapt pre-trained forecasting models with limited data.

% However, there has been limited efforts into developing efficient adaptation techniques to different agent types, which is the objective of this work. 

% However, we will empirically demonstrate that the current state-of-the-art architectures do not generalize well ac
% Another line of work propose data-driven architectures that consider different inputs like rasterized maps [] and vectorized representations [] to train on large-scale autonomous vehicle datasets.


% \textbf{Fine-tuning.} fine-tuning an entire or a part of a pretrained model is the most popular alternative for adapting a pretrained model on a new task. However, full model fine-tuning can suffer from overfitting in few-shot scenario. On the other hand, naive fine-tuning the last few layers suffers from inferior performance in motion context (see Sec.). We propose to introduce few extra parameters at various layers that can help inject the style of the new domain without altering the learned weights of the pretrained model.

% , thereby promoting high parameter-sharing.
% Apart from the gain in performance, an advantage of fine-tuning is that it does not require task-specific model design.

%  Features in deep neural networks have been observed to transition from general to task-specific from the initial to the final layers \citep{Yosinski2014HowTA}.

% One ambitious approach is developing domain generalization techniques that aim to learn models that directly function well in unseen test domains \citep{Gulrajani2021InSO, Blanchard2011GeneralizingFS}.
\textbf{Distribution shifts.} The primary challenge in adapting to new domains lies in tackling the underlying distributional shifts. In the motion context, negative data augmentation techniques have been applied in a limited scope to reduce collisions \cite{liu2021social} and off-road predictions \citep{zhu2022motion} on new domains. Closely related to our work, Liu \textit{et al.} \citep{Liu2021TowardsRA} proposed to reuse the majority of pre-trained parameters for efficient adaptation. However, there exist key differences in the methodology: (1) we does not require access to multiple training domains with varying styles in order to perform adaptation (2) we introduce low-rank adapters instead of finetuning existing parameters, to model domain shifts. Domain adaptation is another paradigm that allows a learning algorithm to observe unlabelled test samples. While this approach is effective for supervised visual tasks \cite{Csurka2020DeepVD, Wang2018DeepVD, Zhao2020MultisourceDA}, it is not ideal for motion forecasting where the crucial challenge is sample efficiency as labels (future trajectories) are fairly easy to acquire. Therefore, we propose to perform transfer learning using limited data.

% Additionally, we demonstrate that decoupling motion style into scene-style and agent-style favors efficient adaptation, which can improve the performance of RA.

% the application of distributionally robust optimization to forecasting has been explored albeit in an artificial setup \citep{Liu2021TowardsRA}.



% Recent works proposed to address this challenge by incorporating strong priors into learning objectives \citep{zhu2022motion, liu2021social} or architecture designs \citep{Liu2021TowardsRA}. These priors often make strong assumptions, thereby, constraining the extent of distribution shifts. In this work, we take an alternate approach and adapt a pretrained forecasting model to unseen target domains (novel agent types and scenes) as efficiently as possible.

%  We are interested in the inductive transfer learning setting for deep motion forecasting \citep{Pan2010ASO}.
\textbf{Transfer learning.} The standard approach of fine-tuning the entire or part of the network \citep{Howard2018UniversalLM, Radford2018ImprovingLU} has been shown to outperform feature-based transfer strategy \citep{Cer2018UniversalSE, Mikolov2013DistributedRO}. In the motion context, transfer learning given limited data often requires special architecture designs like external memory \citep{Zang2020FewshotHM} and meta-learning objectives \citep{finn2017model, Gui2018FewShotHM} that require access to multiple training environments. Wang \textit{et al.} \cite{Wang2022TransferableAA} performed online adaptation across different scenarios for vehicle prediction domains. Recently, there has been a growing interest in developing parameter-efficient fine-tuning (PET) methods in both language and vision, as they yield a compact model \citep{Houlsby2019ParameterEfficientTL, hu2021lora, Mahabadi2021CompacterEL} and show promising results in outperforming fine-tuning in low-resource settings \citep{Mahabadi2021CompacterEL, Liu2022FewShotPF}. Similar in spirit to PET methods, we introduce additional parameters in our network that account for the updates in target style.

% large-scale forecasting model pretraining captures many facets of human motion such as long-term dependencies, collision avoidance. Additionally,

%  The core idea of PET is to learn only a small number of additional parameters on top of the pretrained model that can be tuned separately for each downstream task without revisiting the base model.

\textbf{Motion Style.} the popular work of \citet{Robicquet2016LearningSE} defined \textit{navigation style} as the way different agents interact with their surroundings. It introduced social sensitivity as two handcrafted descriptions of agent style and provided them as input to the social force model \citep{Helbing1995SocialFM}. In this work, we model style as a latent variable that is learned in a data-driven manner. Furthermore, we decouple motion style into scene-style components and agent-style components to favour efficient adaptation.

% In vision, style transfer refers to the task of transforming an input image into a particular artistic style while preserving the notion of content \citep{Dumoulin2017ALR, Johnson2016PerceptualLF}. In motion, 

% Closely related to ours, \citet{Liu2021TowardsRA} decoupled domain-invariant laws and domain-specific style inside their causal forecasting framework. However, their method imposes the strong constraint of requiring access to multiple environments of varying style during training. Furthermore, we decouple motion style into scene-style components and agent-style components to favour efficient adaptation.

% In vision, style transfer refers to the task of transforming an input image into a particular artistic style while preserving the notion of content \citep{Dumoulin2017ALR, Johnson2016PerceptualLF}. 

\begin{figure*}
    \centering
    \includegraphics[width=\textwidth]{figures/Pull2.pdf}
    \caption{We present an efficient transfer learning technique that adapts a forecasting model trained with sufficient labeled data (\textit{e.g.}, pedestrians), to novel domains exhibiting different motion styles (\textit{e.g.}, cyclists). We freeze the pretrained model and only tune a few additional parameters that aim to learn the underlying style shifts (left). We hypothesize that the style updates across domains lie in a low-dimensional space. Therefore, we propose motion style adapters with a low-rank decomposition ($r \ll d$), designed to infer and update the few style factors that vary in the target domain (right).}
    \label{fig:pull}
\end{figure*}

\section{Method}

% Tuning with style-injection modules involves adding a small number of new parameters to a model, which are trained on the new domains.
Deep neural networks have shown remarkable performance in motion forecasting thanks to the availability of large-scale datasets. However, these models often struggle to generalize when unseen scenarios are encountered in the real world due to underlying differences in motion style. In this section, we first formally introduce the problem setting of motion style transfer. Subsequently, we introduce the design of our style adapter modules that help to effectively tackle motion style transfer. Finally, we describe the application of style adapters to a modularized architecture in order to perform style transfer in a more efficient manner.

% In this section, we describe style-injection modules, built upon the idea of low-rank adaptation, that perform a more general parameter-efficient modification to the pretrained network. We now explain the motivation of low-rank adaptation and the working of these modules in detail.
% : one that has the ability to change the distribution of activations at every network layer while tuning $\sim <1\%$ of the network parameters

\subsection{Motion Style Transfer}

\textbf{Motion style.} Modelling agent motion behavior involves learning the social norms (\textit{e.g.}, minimum separation distance to others, preferred speed, valid areas of traversal) that dictate the motion of the agent in its surroundings. These norms differ across agents as well as locations. For instance, the preferred speed of pedestrians differs from that of cyclists; the separation distance between pedestrians in parks differs from that in train stations. To describe these agent-specific (or scene-specific) elements that govern underlying motion behavior, we define the notion of ``motion style". Motion style is the collective umbrella that models the social norms of an agent given its surroundings.  

\textbf{Problem statement.} Consider the inductive transfer learning setting for deep motion forecasting across different motion styles. Specifically, we are provided a forecasting model trained on large quantities of data comprising a particular set of style(s) and our goal is to adapt the model to the idiosyncrasies of a target style as efficiently as possible. We denote the model input and ground-truth future trajectory of an agent $i$ using $x_{i}$ and $y_{i}$ respectively. The input $x_{i}$ comprises the past trajectory of the agent, surrounding neighbors, and the surrounding context map. The context can take various forms like RGB images or rasterized maps. We assume that the data corresponding to an agent type is generated by an underlying distribution $\mathcal{P}_{X, Y}( \cdot; s)$ parameterized by $s$, the style of the agent. As mentioned earlier, the style is dictated by both the agent type and its surroundings.

% Under this assumption, we can transfer motion characteristics of one of agent to another by injecting the underlying style updates.

%%% OLD %%% %%% %%% %%% %%% %%% %%% %%% %%% %%% 

%%%% \paragraph{Motion Style:} Each agent has their own way of interactin with other agents and environment.
% Let us denote the inherent style distribution of the training dataset by $S$. $S$ can correspond to motion style of different agents within the dataset like pedestrian and cyclists, or style of different locations within the dataset like parks and train stations. We are provided data in near-unlimited quantities for a particular agent style(s) and we want to adapt to the idiosyncrasies of the target agent style.

% Style can correspond to motion style of different agents within the dataset like pedestrian and cyclists, or style of different locations within the dataset like parks and train stations. We are provided data in
% near-unlimited quantities for a particular agent style(s) and we want to adapt to the idiosyncrasies of the target agent style.

% We assume that the data corresponding to an agent is generated by an underlying distribution $\mathcal{P}_{X, Y}( \cdot; s)$ parameterized by $s$, the style of the agent. Thus, we can transfer data of one of agent to another by injecting their style.

% Note that $s$ is not unobservable.


% We denote the past trajectory and ground-truth future trajectory using $x_{i}$ and $y_{i}$ of an agent $i$. The surrounding context map for a given sample is denoted by $m_{i}$. The context can take various forms like RGB images or rasterized maps. Each dataset has an inherent style distribution $S$. For instance, $S$ can correspond to motion style of different agents comprising the dataset, or style of different locations  $i$ has an inherent motion style denoted by $s_{i}$ like The training dataset can comprise homogenous or heterogeneous agents. Let us denote the style of each agent by $s_{i}$ and the set of styles observed during training by $S$. The motion forecasting model has an encoder-decoder architecture (see Fig.~\ref{fig:pull}) denoted by $e$ and $d$ respectively. 

\textbf{Training.} The forecasting model has an encoder-decoder architecture (see Fig.~\ref{fig:pull}) with weights $W_{enc}$ and $W_{dec}$ respectively. The training dataset, $D_S$ of size $N$ is given by $\cup_{s \in S} D_s = {(x_i, y_i)}_{i \in \{1,...,N\}}$, where $S$ is a collection of motion styles observed within the dataset. The model is trained to minimize a loss objective $\mathcal{L}$, such as the negative log likelihood (NLL) \cite{Alahi2016SocialLH, Kothari2020HumanTF} loss or variety loss \cite{Gupta2018SocialGS}:
\begin{equation}
    % \resizebox{0.42\textwidth}{!}{%
    % $\mathcal{L}_{train} (D_{S}; W_{enc}, W_{dec}) = \frac{1}{N} \sum\limits_{i=1}^{N} \mathcal{L}(x_i, y_i; W_{enc}, W_{dec})$%
    % }
    \mathcal{L}_{train} (D_{S}; W_{enc}, W_{dec}) = \frac{1}{N} \sum\limits_{i=1}^{N} \mathcal{L}(x_i, y_i; W_{enc}, W_{dec}).
\end{equation}
% corresponding to each heterogeneous agent and scene

% UNCOMMENT ?
% While this stage is the most expensive, it only needs to be performed once. We would like to re-iterate that in many applications revisiting the pre-training data can be impractical due to increasing privacy concerns, inflating sizes of datasets as well as many other real-world constraints.


% No matter how diverse the general-domain data used for pretraining is, the data of the target task will likely come from a different distribution.
\textbf{Adaptation.}  When a novel scenario with style $s'$ ($s' \notin S$) is encountered, it leads to a distribution shift and the learned model often struggles to directly generalize to the corresponding dataset $D_{s'} = (x'_i, y'_i)_{i \in \{1,...,N_{target}\}}$ of size $N_{target}$. The common approach to tackling such shifts is to fine-tune the entire or part of the pretrained model. Fine-tuning optimizes an objective similar to training, but on the new dataset:
\begin{equation}
    \mathcal{L}_{adapt} (D_{s'};  W_{enc}, W_{dec}) = \frac{1}{N_{target}} \sum_{i=1}^{N_{target}} \mathcal{L}(x'_i, y'_i; W_{enc}, W_{dec}).
\end{equation}

In this work, we aim to develop an adaptation strategy for efficient motion style transfer, \textit{i.e.}, cases where $N_{target}$ is small ($N_{target} \ll N$). 
Often, motion behaviors do not change drastically across domains. Instead, most of the behavioral dynamics are governed by universal physical laws (\textit{e.g.}, influence of inertia, collision-avoidance). We therefore propose to freeze the weights of the pretrained forecasting model and introduce motion style adapters, termed \textit{MoSA}, to capture the target  motion style. As shown Fig.~\ref{fig:pull}, we adapt a pre-trained forecasting model by fine-tuning $W_{MoSA}$ with the following objective:
% Therefore, as shown in Fig.~\ref{fig:pull}, we propose to freeze the weights of the pretrained model and introduce motion style adapters, parametrized by $ W_{MoSA}$, for learning the target style updates. Therefore, during adaptation, the model minimizes:
% We observe that often when unseen domains are encountered, only the corresponding motion style parameters need to be updated.
% Therefore, as shown in Fig.~\ref{fig:pull}, we propose to freeze the weights of the pretrained model and introduce motion style adapters, parametrized by $ W_{MoSA}$, for learning the target style updates. Therefore, during adaptation, the model minimizes:
\begin{equation}
    \mathcal{L}_{adapt} (D_{s'}; W_{MoSA}) = \frac{1}{N_{target}} \sum_{i=1}^{N_{target}} \mathcal{L}(x'_i, y'_i; W_{MoSA}).
\end{equation}

    % \resizebox{0.33\textwidth}{!}{%
    % $\mathcal{L}_{adapt} (D_{S'}; \Delta W) = \frac{1}{N'} \sum\limits_{i=1}^{N'} \mathcal{L}(x_i, y_i; \Delta W)$%
    % }
% We only require to learn the style differences and develop an effective way to inject them into the pretrained model. 

% Next, we describe the simple yet elegant design of our style-injection modules.

% Our adaptation strategy learns to forecast the motion of new individuals by combining limited supervised information $D'$ with knowledge learned by the pretrained model. 


% We are given as input the past trajectories of agents denoted by \textbf{X} for $t_{obs}$ seconds and the surrounding physical scene denoted by \textbf{S} (top-view RGB images or rasterized maps). The forecasting task entails predicting the future trajectory of the agent of interest for the next $t_{obs}$ seconds.

% We refer to the input dimension size at any encoder layer as $d_{model}$. $W$ or $W_0$ refers to a pretrained weight matrix and $\Delta W$ its accumulated gradient update during adaptation. We use $W_q$, $W_k$, $W_v$, and $W_o$ to refer to the
% query/key/value/output projection matrices in the self-attention module. Similarly, $W_{mlp}$ and $W_{conv}$ refers to the weight matrics of MLP and convolutional layers. We use $r$ to denote the rank of style-injection module.


% We propose Motion Style-Adapters for effective transfer learning, which we introduce next.


\subsection{Motion Style Adapters} 
% An objective function’s intrinsic dimension measures the minimum number of parameters needed to reach satisfactory solutions to the respective objective \citep{Li2018MeasuringTI}. Recently, \citet{Aghajanyan2021IntrinsicDE} showed that large-scale pre-training helps to reduce the intrinsic dimension of the downstream tasks (even without accessing them!). In the motion context, this finding indicates that large-scale training on heterogeneous agents can help lower the intrinsic dimension of the objective of forecasting unseen novel agents.

Our main intuition is that the style shifts across forecasting domains are usually localized – they are often due to the changes in only a few variables of the underlying motion generation process. Therefore, during style transfer, we only need to adapt the distribution of this small portion of latent factors, while keeping the rest of the factors constant. These updates would correspond to the changes in motion style ($s \rightarrow s'$) in the target domain, as the general principles of motion dynamics (\textit{e.g.}, avoid collisions, move towards goal) remain the same across domains for all agents. We design motion style adapters, referred to as MoSA, to carry out these updates.

Our proposed MoSA design comprises a small number of extra parameters added to the model during adaptation (see Fig.~\ref{fig:pull}). Each module comprises two trainable weight matrices of low rank, denoted by $A$ and $B$. The first matrix $A$ is responsible for inferring the style factors that are required to be updated to match the target style, while the second matrix $B$ performs the desired update. The low rank $r$ realizes our intuition that style updates reside in a low-dimensional space, by restricting the number of style factors that get updated ($r \ll d$ where $d$ is the dimension size of an encoder layer). Therefore, during adaptation, the weight updates of the encoder are constrained with our low-rank decomposition $W_{MoSA} = BA$. The pretrained model is frozen and only $A$ and $B$ are trained. 

% Given $W_{enc}$, we constrain the weight updates of the encoder by representing the gradient update $\Delta W$ with a low-rank decomposition $\Delta W = BA$, where $B$ and $A$ are low-rank matrices with rank $r$ ($r << d$ where $d$ is the dimension size of an encoder layer). During adaptation, $W_{enc}$ is frozen and only $A$ and $B$ are trained. 

For brevity, let us consider the adaptation of encoder layer $l$ with input $h^l$ and output $h^{l+1}$. As shown in Fig.~\ref{fig:pull}, $W_{enc}^{l}$ and $W_{MoSA}^{l}$ are multiplied with the same input $h^{l}$, and their respective output vectors are summed coordinate-wise as shown below:
% \small
\begin{align}
    \text{(Train)} & \; \; h^{l+1} = W^{l}_{enc} h^{l},  \\
    \text{(Adapt)} & \; \; h^{l+1} = W^{l}_{enc} h^{l} + W_{MoSA}^{l} h^{l} =  W^{l}_{enc} h^{l} + B^{l}A^{l}h^{l}.
\end{align}
% \normalsize

It has been shown that initialization plays a crucial role in parameter-efficient transfer learning \citep{Houlsby2019ParameterEfficientTL, hu2021lora}. Therefore, following common practices, matrices $A$ and $B$ are initialized with a near-zero function \citep{hu2021lora}, so that the original network is unaffected when training starts. Furthermore, the initialization provides flexibility to these modules to ignore certain layers during motion style updates. Despite this flexibility, the total number of extra parameters is significant and can be inconducive to efficient style transfer. Therefore, to further boost sample efficiency, we present a modular adaptation strategy which we describe next.

\begin{figure*}
    \centering
    \includegraphics[width=0.9\textwidth]{figures/encodr_mod_new.pdf}
    \caption{Our modular style transfer strategy updates only a subset of the encoder to account for the underlying style shifts. For instance, we adapt the scene encoder only to model scene style shifts (top right). While for the underlying agent motion shift, we only update the agent motion encoder (bottom right). This strategy boosts performance in low-resource settings.}
    \label{fig:modular}
\end{figure*}

\subsection{Decoupling Motion Style Adapters} 

Motion style can be decoupled into scene-specific style and agent-specific style. Scene-specific style dictates the changes in motion due to physical scene structures. For instance, cyclists prefer to stay on the lanes while pedestrians move along sidewalks. The agent-specific style captures the underlying navigation preferences of different agents like distance to others and preferred speed. Modularizing the encoder into two parts that account for the physical scene and agent's past motion independently can help to decouple the functionality of our motion style adapters and further improve performance.
% factorized according to the graph structure,
% each distribution module can be adapted separately

% One important decision in the motion style transfer setup is determining the model layers to be updated using our style-injection modules. It is indeed difficult to determine which layers of the encoder are responsible for modelling the different aspects of motion style. Using the domain knowledge of motion forecasting task [][], motion style can be decoupled into scene-specific style and agent-specific style. 

% For instance, cyclists prefer to stay on the center of lanes on student campus while they steer towards the lane edges on roads with cars.

% Scene-specific style dictates the changes in motion due to physical scene structures. For instance, cyclists prefer to stay on the lanes while pedestrians move along sidewalks. Agent-specific style captures the underlying navigation preferences of different agents like distance to others and preferred speed. In this work, we explore the application of style adapters in such modularized setups to garner further insights on their performance.

 


% When applied naively across a generic encoder, motion style adapters are required to model both the underlying scene and agent styles.
% This design not only increases the number of extra parameters but also the additional task to disambiguate the two styles leads to suboptimal performance.

Consider the modularized motion encoder designs shown in Fig.~\ref{fig:modular}. The modularized encoder models the input scene and agent's past history independently. The fusion encoder then fuses the two representations together. This design has the advantage to decouple the task of the style adapters into scene-specific updates and agent-specific updates. Given the modularized setup, the nature of the underlying distribution shifts can help guide which modules within the model are required to be updated to the target style. As demonstrated in Section.~\ref{sec:decouple}, given different categories of style transfer setups, decoupling style adapters can improve the adaptation performance while significantly reducing the number of updated parameters.

% The scene encoder $enc_{scene}$ learns the input scene representation, while agent encoder $enc_{motion}$ models the agent preference.

% Our STUD design is generic, the efficiency of style transfer can be further improved by decoupling motion style into scene-specific style and agent-specific style. The former dictates the changes in motion due to physical scene structures like sidewalks and lanes while the latter governs the underlying agent preferences like distance to others and preferred speed. Such modularized representations are garnering interest lately and have been shown to ....[][]  Therefore, given the nature of the target domain, one can further reduce the number of adapted parameters without compromising performance. 


% Low-rank adaptation has shown great empirical results when applied to the self-attention modules of Transformer-based architectures \cite{hu2021lora}.
% Our style-injection module design is generic and can be naturally applied to different architectures. We demonstrate that our method outperforms fine-tuning in low-resource settings when applied to both convolutional-based \citep{Mangalam2021FromGW} and transformer-based  \citep{dosovitskiy2020image} forecasting models. We further show that they outperform other parallel module designs corroborating our hypothesis that style updates indeed have an intrinsic low rank.

\section{Experiments}

We evaluate our method on a total of three datasets to study the performance of motion style adapters: Stanford Drone Dataset (SDD) \citep{Robicquet2016LearningSE}, the Intersection Drone Dataset (InD) \citep{Bock2020TheID}, and Level 5 Dataset (L5) \citep{Houston2020OneTA}. We evaluate each method over five experiments with different random seeds. More implementation details and ablations are summarized in the Appendix.

\textbf{Baselines.} We use the state-of-the-art Y-Net model \cite{mangalam2021goals} on SDD and inD, and the Vision Transformer (ViT) \cite{dosovitskiy2020image} on L5 across all methods. We compare the following:

\begin{itemize}[leftmargin=*]
\itemsep 0em
\item Full Model Finetuning (FT) \citep{Howard2018UniversalLM}: we update the weights of the entire model.
\item Partial Model Finetuning (ET) \citep{Liu2021TowardsRA}: we update the weights of the Y-Net encoder for SDD and inD , and the last two layers of ViT for Level 5.
\item Parallel Adapters (PA) \citep{rebuffi2018efficient}: we insert a convolutional layer with a fixed filter size in parallel to each encoder layer and update the weights of these added layers. This baseline does not incorporate the low-rank constraint.
\item Adaptive Layer Normalization \cite{li2017revisiting, de2017modulating}: we update the weights and biases of the layer normalization.
\item Motion Style Adapters (MoSA) [Ours]: we insert our motion style adapters in parallel to each encoder layer in SDD and inD, and in parallel to query and value matrices of multi-headed attention in Level 5. During modularized adaptation, we add our modules only across the specified encoders.
\end{itemize}

\textbf{Metrics.} We use the established Average Displacement Error (ADE) and Final Displacement Error (FDE) metrics for measuring the performance of model predictions. ADE is calculated as the $l_2$ error between the predicted future and the ground truth averaged over the entire trajectory while FDE is the $l_2$ error between the predicted future and ground truth for the final predicted point \cite{Alahi2016SocialLH}. For multiple predictions, the final error is reported as the \texttt{min} error over all predictions \cite{Gupta2018SocialGS}. Additionally, we define the generalization error as the error of the pretrained model on the target domain. The more the dissimilarity between the source domain and target domain, the higher the generalization error.


\begin{table*}[]
    \begin{center}
    \caption{Evaluation of adaptation methods for motion style transfer (pedestrians to cyclists) on SDD and scene style transfer on InD using few samples $N_{target} = \{ 10, 20, 30 \}$. Error reported is Top-20 FDE in pixels. The generalization error on SDD is 58 pixels and on inD is 33 pixels. Our proposed motion style adapters (MoSA) outperform competitive baselines and improve upon the generalization error by $>25\%$ in both setups. Mean and standard deviation were calculated over 5 runs.}
    \label{tab:sdd_ind_result}
    \resizebox{\textwidth}{!}{%
    \begin{tabular}{l|ccc||ccc}
        \toprule[0.35ex]
        \midrule[0.25ex]
        \multicolumn{1}{|c}{ } & \multicolumn{3}{|c||}{Stanford Drone Dataset} & \multicolumn{3}{c|}{Intersection Drone Dataset} \\[0.25ex]
        \midrule[0.25ex]
        $N_{target}$ & 10 & 20 & 30 & 10 & 20 & 30 \\[0.25ex]
        \midrule
        % OOD & 58.0 & 58.0 & 58.0 & 33.0 & 33.0 & 33.0 \\[0.25ex]
        FT & 57.28 $\pm$ 1.21  & 52.61 $\pm$ 0.87 & 46.31 $\pm$ 1.79 & 27.92 $\pm$ 1.99 & 25.15 $\pm$ 1.08 & 23.18 $\pm$ 0.64 \\[0.25ex]
        ET \citep{Liu2021TowardsRA} & 51.88 $\pm$ 1.32 & 46.78 $\pm$ 1.78 & 43.13 $\pm$ 1.03 & 28.06 $\pm$ 0.68 & 23.19 $\pm$ 1.39 & 21.13 $\pm$ 1.00 \\[0.25ex]
        PA \cite{rebuffi2018efficient} & 52.77 $\pm$ 0.85 & 47.75 $\pm$ 1.83 & 44.70 $\pm$ 1.28 & 28.71 $\pm$ 1.50 & 26.10 $\pm$ 0.74 & 25.00 $\pm$ 1.08 \\[0.25ex]
        MoSA (ours) & \textbf{49.98 $\pm$ 1.05} & \textbf{45.55 $\pm$ 0.77} & \textbf{41.69 $\pm$ 0.88} & \textbf{25.18 $\pm$ 0.72} &\textbf{ 21.70 $\pm$ 0.84} & \textbf{20.35 $\pm$ 1.18} \\[0.25ex]
        \midrule[0.35ex]
    \end{tabular}
    }
    \end{center}
\end{table*}


% \begin{table*}[]
%     \centering
%     \caption{Evaluation of few-shot adaptation techniques on SDD. Our proposed low-rank style-injection module outperforms fine-tuning and parallel adapter while adding and updating only $0.5\%$ of total network parameters. Error reported are Top-20 ADE/FDE in pixels.}
%     \label{tab:sdd_result}
%     \resizebox{\textwidth}{!}{%
%     \begin{tabular}{lccccccc}
%     \toprule 
%     \multirow{2}{*}{Method} & \%params & \multicolumn{2}{c}{\#sample=20} & \multicolumn{2}{c}{\#sample=40} & \multicolumn{2}{c}{\#sample=80} \\
%     \cline{3-8}
%      & tuned  & ADE & FDE  & ADE & FDE  & ADE & FDE \\
%     \hline 
%     % Training from scratch & 1.6M & 100\% & 268.84 & 744.87 & 265.71 & 1008.24\\
%     \rowcolor{Gray}
%     Generalization & 0\% & 34.0 & 58.0  & 34.0 & 58.0  & 34.0 & 58.0\\
%     Full fine-tuning & 100\% & 30.1 & 52.4  &  28.9 & 51.0  & 26.1 & 46.6 \\
%     \rowcolor{Gray}
%     Modular fine-tuning \cite{Liu2021TowardsRA} & 13.7\% & 28.8 & 46.9  & 26.8 & 44.0  & 24.9 & 40.3 \\
%     Parallel Adapters \cite{rebuffi2018efficient} & 6.1\% & 27.9 & 47.3  & 26.2 & 44.4  & 23.9	& 39.7 \\
%     \hline 
%     \rowcolor{Gray}
%     Low-rank style injection (ours) & \textbf{0.5\%} & \textbf{26.9} & \textbf{43.6} & \textbf{25.6} & \textbf{43.0}  & \textbf{23.7} & \textbf{39.0}\\
%     \bottomrule
%     \end{tabular}}
% \end{table*}




% In the target domain we consider different sample sizes with $N_{target} = \{ 10, 20, 30\}$ for the SDD and inD experiment, and $N_{target} = \{ TODO \}$ for the Level 5 experiment.

% We demonstrate the effectiveness of our proposed style injection modules on two different style shift scenarios: (1) Agent Style Transfer and (2) Scene Style Transfer. First, we adapt the state-of-the-art Y-Net \cite{mangalam2021goals} model trained on pedestrian data only to cyclists on SDD. On the Level 5 dataset, we adapt a Vision Transformer \cite{dosovitskiy2020image} model trained on one part of the city to another part of the city. The detailed implementation details are provided in the Appendix.
% Detailed implementation details are provided in the Appendix.







\subsection{Motion Style Transfer across Agents on Stanford Drone Dataset}

We perform short-term prediction, where the trajectory is predicted for the next 4.8 seconds, given 3.2 seconds of observation. The Y-Net model is trained on pedestrian data across \textit{all} scenes and adapted to the cyclists' data in \textit{deathCircle\_0}, as there exists a clear distinction between the motion style of pedestrians and cyclists (see Fig.~\ref{fig:DC0_heatmap}). We adapt the model using $N_{target} = \{ 10, 20, 30 \}$ samples.
% SDD comprises many scenes with limited samples of rare agents like skateboarders and buses. Here, w


% We compare three baselines \citep{Liu2021TowardsRA, rebuffi2018efficient} against our style-injection modules, the details of which are provided in the supplementary.

% The SDD comprises 20 top-down scenes on the Stanford campus. We choose the \textit{deathCircle} scene for adaptation as there exists a clear distinction between the motion style of pedestrians and cyclists (see Fig.~\ref{fig:DC0_heatmap}). We consider two parameter-efficient adaptation designs on top of the Y-Net encoder: the parallel adapter \citep{rebuffi2018efficient} and our low-rank style-injection module (details in Appendix).

% Secondly, we utilize the LoRA module \cite{hu2021lora} to learn the style-updates when transitioning from pedestrians to cyclists. The architecture details and hyper-parameters are provided in Appendix.

Tab.~\ref{tab:sdd_ind_result} quantifies the performance of various adaptation techniques. The model trained on pedestrians does not generalize to cyclists as evidenced by the high generalization error of $58$ pixels. Our MoSA design reduces this error by $\sim 30\%$ using only $30$ samples. Moreover, MoSA outperforms the baselines while keeping the pretrained model frozen and updating only $2\%$ additional parameters. Fig.~\ref{fig:DC0_viz} illustrates the updates in the Y-Net goal decoder output (red means increase in focus and blue means decrease in focus) on model adaptation using MoSA. Adapted Y-Net successfully learns the style differences between the behavior of pedestrians and cyclists: 1) it correctly infers valid areas of traversal, 2) effectively captures the multimodality of cyclists, and 3) updates the motion style parameters as the new cyclist goal positions (red) are farther from the end of the observation position, compared to the un-adapted goal positions (blue). 

% the performance of both adapter methods outperforms the full model and partial fine-tuning \citep{Liu2021TowardsRA}.  Moreover, our MoSA design with the low-rank constraint outperforms all methods and improves the generalization error by $25\%$ using only $20$ samples, corroborating our hypothesis that style updates reside in a low-dimensional space.


% The two parallel designs, namely parallel adapters \citet{rebuffi2018efficient} and MoSA (ours) empirically demonstrate the efficacy of parallel style-injection.




% Ynet is a pure convolution-based model and outputs prediction maps for sampling. 
% We conduct our experiments on two dominating agents, pedestrians and cyclists. For each agent, the dataset has 14860 and 5152 unique trajectories, covering 64\% and 22\% of the data, respectively. 

% The dataset has 20 top-down scenes in campus. As a first step, a single scene, deathCircle\_0, is selected for adaptation in order to better understand what is happening behind. This scene has a speciality that there is clear distinction between the trajectories of pedestrians and cyclists. We provide a few samples in cyclist domain for adaptation, i.e., 20 and 40 trajectories, and evaluate the performance on 500 leftout test trajectories. 

% Three baseline methods are training from scratch, direct generalization of the pretrained model, fine-tuning all the parameters of pretrained model, and fine-tuning only the encoder of the pretrained model. 
% Our implemented methods include 1) inserting two parallel adapters with filter size 3x3 and 5x5 around two consecutive convolutional layers for the whole encoder and 2) applying LoRA of rank 1 to each sub-layer of the encoder, into all the layers of encoder. We summarize model performance in Tab.~\ref{tab:sdd_result},  the superiority of adapter modules. 


% We qualitatively analyze the Y-Net goal decoder outputs after model adaptation using MoSA. Fig.~\ref{fig:DC0_viz} illustrates the updates in the goal decoder output (red means increase in focus and blue means decrease in focus). Adapted Y-Net successfully learns the style differences between the behavior of pedestrians and cyclists: 1) it correctly infers valid areas of traversal, 2) effectively captures the multimodality of cyclists, and 3) updates the motion style parameters as the new cyclist goal positions (red) are farther from the end of the observation position, compared to the un-adapted goal positions (blue). 

% \begin{figure*}
% \begin{center}
%  \begin{subfigure}{0.24\textwidth}
%     \includegraphics[width=\linewidth]{bp_distribution_light.png}
%     \caption{Distribution of pedestrian (blue) and cyclist (red) in deathCircle}
%     \label{fig:DC0}
%  \end{subfigure}
%  \begin{subfigure}{0.24\textwidth}
%     \includegraphics[width=\linewidth]{5358__lora_1_0_1_2_3_4_20__goal_decoder.predictor_output__diff_single__0.15.png}
%     \caption{Example Id = 5358}
%     \label{fig:DC0_5358}
%  \end{subfigure}
%  \begin{subfigure}{0.24\textwidth}
%     \includegraphics[width=\linewidth]{5883__lora_1_0_1_2_3_4_20__goal_decoder.predictor_output__diff_single__0.15.png}
%     \caption{Example Id = 5883}
%     \label{fig:DC0_5883}
%  \end{subfigure}
%  \begin{subfigure}{0.24\textwidth}
%     \includegraphics[width=\linewidth]{5982__lora_1_0_1_2_3_4_20__goal_decoder.predictor_output__diff_single__0.15.png}
%     \caption{Example Id = 5982}
%     \label{fig:DC0_5982}
% \end{subfigure}
% \end{center}
% \caption{Caption}
% \label{fig:DC0_viz}
% \end{figure*}





\subsection{Motion Style Transfer across Scenes on Intersection Drone Dataset}

We perform long-term prediction, where trajectory in the next 30 seconds is predicted, given 5 seconds of observation. The Y-Net model is trained on pedestrians in $\{ scene2, scene3, scene4\}$ and tested on unseen scene $scene1$. We adapt the model using $N_{target} = \{ 10, 20, 30 \}$ samples.
% Here, we generalize across different scenes.

Despite the long-term prediction setup, the generalization error is 33 pixels which is lower compared to SDD, as the target domain is more similar to the source domain. Tab.~\ref{tab:sdd_ind_result} quantifies the performance of scene style transfer across all methods. Using just $30$ samples, MoSA improves the generalization error by $\sim 40\%$ and outperforms its counterparts. The superior performance in comparison to PA justifies the importance of the low-rank constraint.

% It is interesting to note that its style-injection counterpart PA does not perform as well in the scene generalization setup. 
% One possible reason is that the underlying style shifts are limited compared to the SDD setup, making this setup more susceptible to overfitting.

% Further, in low-resource settings, the performance of parameter-efficient methods outperforms full model and modular fine-tuning \citep{Liu2021TowardsRA}. The two parallel designs, namely parallel adapters \citet{rebuffi2018efficient} and MoSA (ours) empirically demonstrate the efficacy of parallel style-injection. Finally, our low-rank design using only rank 1 outperforms all methods corroborating our hypothesis that style update is a low-rank feature.



% \begin{figure*}[t!]
% \begin{minipage}{.24\textwidth}
%   \centering
%   \includegraphics[width=0.97\linewidth]{figures/bp_distribution_0.4.pdf}
%   \vspace{-2pt}
%   \captionof{figure}{Distribution of trajectories of pedestrians (blue) and cyclists (red) on \textit{deathCircle}.}
%   \label{fig:DC0_heatmap}
% \end{minipage}
% \quad
% \begin{minipage}{0.71\textwidth}
%   \begin{subfigure}{0.325\textwidth}
%     \includegraphics[width=\linewidth]{figures/5358__lora_1_0_1_2_3_4_20__goal_decoder.predictor_output__diff_single__0.15.pdf}
%     % \caption{Sample Id = 5358}
%     % \label{fig:DC0_5358}
%  \end{subfigure}
%  \begin{subfigure}{0.325\textwidth}
%     \includegraphics[width=\linewidth]{figures/5883__lora_1_0_1_2_3_4_20__goal_decoder.predictor_output__diff_single__0.15.pdf}
%     % \caption{Sample Id = 5883}
%     % \label{fig:DC0_5883}
%  \end{subfigure}
%  \begin{subfigure}{0.325\textwidth}
%     \includegraphics[width=\linewidth]{figures/5982__lora_1_0_1_2_3_4_20__goal_decoder.predictor_output__diff_single__0.15.pdf}
%     % \caption{Sample Id = 5982}
%     % \label{fig:DC0_5982}
% \end{subfigure}
% \caption{Illustration of the difference in goal decoder output of Y-Net on the adaptation of pedestrian-trained model using our proposed style-injection modules (rank=1) (Red is positive, blue is negative). During adaptation, Y-Net learns to focus on the road lanes for cyclist forecasting.}
% \label{fig:DC0_viz}
% \end{minipage}
% % \vspace{-0.6cm}
% \end{figure*}

\begin{figure*}[t!]
\begin{minipage}{.22\textwidth}
  \centering
  \includegraphics[width=0.97\linewidth]{figures/bp_distribution_0.4.pdf}
  \vspace{-2pt}
  \captionof{figure}{Heatmap of pedestrians motion (in blue) and cyclists motion (in red) on SDD \textit{deathCircle} location.}
  \label{fig:DC0_heatmap}
\end{minipage}
\quad
\begin{minipage}{0.76\textwidth}
  \begin{subfigure}{0.3\textwidth}
    \includegraphics[width=\linewidth]{figures/5358__lora_1_0_1_2_3_4_20__goal_decoder.predictor_output__diff_single__0.15.pdf}
    % \caption{Sample Id = 5358}
    % \label{fig:DC0_5358}
 \end{subfigure}
 \begin{subfigure}{0.3\textwidth}
    \includegraphics[width=\linewidth]{figures/5883__lora_1_0_1_2_3_4_20__goal_decoder.predictor_output__diff_single__0.15.pdf}
    % \caption{Sample Id = 5883}
    % \label{fig:DC0_5883}
 \end{subfigure}
 \begin{subfigure}{0.3\textwidth}
    \includegraphics[width=\linewidth]{figures/5982__lora_1_0_1_2_3_4_20__goal_decoder.predictor_output__diff_single__0.15.pdf}
    % \caption{Sample Id = 5982}
    % \label{fig:DC0_5982}
 \end{subfigure}
 \begin{subfigure}{0.076\textwidth}
    \includegraphics[width=\linewidth]{figures/legend.pdf}
 \end{subfigure}
\caption{Illustration of the difference in goal decoder output of Y-Net, pre-trained on pedestrians, after model adaptation on cyclist data using our motion style adapters. Using only 20 samples, Y-Net learns to focus on the road lanes (red) instead of sidewalks (blue) for cyclist forecasting.}
\label{fig:DC0_viz}
\end{minipage}
% \vspace{-0.6cm}
\end{figure*}


\subsection{Motion Style Transfer across Scenes on Lyft Level 5 Dataset}

We divide the L5 dataset into two splits based on the data collection locations thereby, constructing a scene style shift scenario. We train the ViT-Tiny model on the majority route and adapt it to the smaller route not seen during training. To simulate low-resource settings, we provide the frames, sampled at different rates, that cover the unseen route only once. 

Fig.~\ref{fig:level5_quant} quantitatively evaluates the performance of various adaptation strategies. MoSA performs superior in comparison to different baselines while adding and updating only $~5\%$ of the full model parameters. It is apparent that our low-rank design is a smarter way of adapting models in the motion context as compared to fine-tuning the model. Fig.~\ref{fig:level5_datasize} empirically validates that a bigger pre-training dataset size results in better adaptation performance. Finally, Fig.~\ref{fig:attention-maps} qualitatively illustrates the improvement of the attention heatmaps of the last layer of ViT post model adaptation using MoSA. MoSA helps to better focus on the different possible future routes and the vehicles in front. Additional visualizations are provided in the Appendix.

% In Fig.~\ref{fig:level5_qual} qualitatively demonstrates the attention heatmaps from the last layer of ViT before and after adapting the model in the new domain. Post adaptation, the ego vehicle is able to reason better about the different road structures and traffic light status. Further, compared to full model fine-tuning, low-rank adaptation better handles the future uncertainties (turning left or going straight) and attends to far-away elements (attention on the right side of the image).

\begin{figure}
\centering
\begin{minipage}{.9\textwidth}
     \begin{subfigure}[b]{0.32\textwidth}
         \centering
         \includegraphics[width=0.99\textwidth]{figures/sample.pdf}
         \caption{Input sample}
         \label{subfig:sample}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{0.32\textwidth}
         \centering
         \includegraphics[width=\textwidth]{figures/pre-adapt.pdf}
         \caption{Before adaptation}
         \label{subfig:pre-adapt}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{0.32\textwidth}
         \centering
         \includegraphics[width=\textwidth]{figures/post-adapt.pdf}
         \caption{After adaptation}
         \label{subfig:post-adapt}
     \end{subfigure}
     \vspace*{2mm}
        \caption{Attention heatmaps of the last layer of ViT before and after model adaptation on unseen route in Level 5. After adaptation, the attention maps are more refined. The ego (in green) better focuses on the different possible future routes and the vehicles in front.}
        \label{fig:attention-maps}
\end{minipage}
\end{figure}


\begin{figure*}[t]
\begin{minipage}{.49\textwidth}
  \centering
  \includegraphics[width=\linewidth]{figures/fde_plot.pdf}
  \vspace{-2pt}
  \captionof{figure}{Evaluation of adaptation techniques for long-term motion prediction (25 secs) on Level 5. Error in meters for 5 seeds.}
%  {Evaluation of adaptation techniques for long-term motion prediction (25 secs) on Level 5. During adaptation, the model traverses the unseen route \textit{only once}. Error in meters.}
%   at different sampling rates (0.5 Hz to 5 Hz)
  \label{fig:level5_quant}
\end{minipage}
\quad
\begin{minipage}{0.49\textwidth}
  \centering
  \includegraphics[width=\linewidth]{figures/CoRLGraph.pdf}
  \vspace{-2pt}
  \captionof{figure}{Impact of the pre-trained dataset size (relative size shown on x-axis) on motion adaptation on L5. Target dataset size fixed to 15 batches.}
%  {Evaluation of adaptation techniques for long-term motion prediction (25 secs) on Level 5. During adaptation, the model traverses the unseen route \textit{only once}. Error in meters.}
%   at different sampling rates (0.5 Hz to 5 Hz)
  \label{fig:level5_datasize}
\end{minipage}
\end{figure*}






\begin{figure*}
\centering
\begin{minipage}{.42\textwidth}
  \centering
  \includegraphics[width=1.15\linewidth,center]{figures/agent_transfer.pdf}
  \vspace{-2pt}
  \captionof{figure}{Agent Motion Transfer from \textit{cars} to \textit{trucks} on InD with $N_{target} = 20$ samples using different MoSA configurations. [A] performs best while [S] worsens performance. Error is Top-20 FDE in pix.}
  \label{fig:agent_transfer}
\end{minipage}
\quad
\begin{minipage}{.42\textwidth}
  \centering
  \includegraphics[width=1.15\linewidth,center]{figures/scene_transfer.pdf}
  \vspace{-2pt}
  \captionof{figure}{Scene Transfer on InD \textit{pedestrians} with $N_{target} = 20$ samples using different MoSA configurations.[S+F] performs best while [A] worsens performance. Error is Top-20 FDE in pix. }
  \label{fig:scene_transfer}
\end{minipage}
\end{figure*}

\subsection{Modular Motion Style Adapters} \label{sec:decouple}

Now, we demonstrate the effectiveness of applying our adapters on top of a modularized architecture on two setups: Scene generalization and Agent Motion generalization. As shown in Fig.~\ref{fig:modular}, we modularize the Y-Net architecture. We treat scene and agent motion independently for the first two layers of the encoder before fusing the learned representations. We refer to this design as Y-Net-Mod. Given Y-Net-Mod, we consider five cases based on modules on which MoSA is applied: (1) scene only [S], (2) agent motion encoder only [A], (3) scene and fusion encoder [S+F], (4) agent motion and fusion encoder [A+F], and (5) scene, agent motion and fusion encoders together [S+A+F].
% The base Y-Net fuses the scene context information and agent history heatmaps at the input directly.


% We adapt the Y-Net-Mod architecture on $scene1$ of inD. Fig~\ref{} shows cars and trucks share the same scene context but differ in their velocity distribution (static trajectories have been filtered out) with cars being the majority class.

%  (see Fig.~\ref{fig:car_vel})
\textbf{Agent motion generalization:} In $scene1$ of inD, cars and trucks share the same scene context differing only in their velocity distribution. Fig.~\ref{fig:agent_transfer} represents the performance of style transfer from cars to trucks on 20 samples under five different adaptation cases. It is interesting to note that adapting the agent motion encoder alone [A] performs the best while including the scene encoder for adaptation deteriorates performance ([S] worse than [A], [S+A+F] worse than [A+F]). 

% Without modularization, the default MoSA [S+A+F] outperforms fine-tuning. Moreover, the performance can be further improved using the MoSA [A] configuration that updates only the motion encoder.

% Now, we demonstrate that scene generalization of performance of MoSA can be further improved within the decoupled setup.
\textbf{Scene generalization:}  We train the Y-Net-Mod model on pedestrian data on scene ids = $\{2, 3, 4\}$ and adapt it on $scene1$ of inD. Fig.~\ref{fig:scene_transfer} represents the performance of scene style transfer on 20 samples in the five cases. Contrary to the previous setup, adapting the scene encoder [S] is clearly more important than the agent motion encoder [A]. Further, adapting the agent encoder deteriorates performance ([S+A+F] worse than [S+F]). It is clear that modularization helps to boost the performance of MoSA.

% Again, without modularization, the default MoSA [S+A+F] outperforms fine-tuning. The performance can be improved by adapting only the scene and fusion encoder. 

% Thus, it is apparent that having modularized adaptations based on the motion style transfer setup helps to improve the style transfer performance while reducing the number of parameters to be updated.
% \vspace{0.5em}
% \textbf{Additional experiments and implementation details.} Please refer to supplementary. 
% % for additional ablation results and implementation details.
% \vspace{-0.5em}

\section{Limitations}

% 1. Human-in-the-loop selection of adapters. Automatic selection of which adapters to utilise
% 2. Additional HP: Rank number. In theory, rank number affects fine-tuning, in a principled manner. 
% 3. Training a modular architecture, no training signal enforces the model learning.

We demonstrated the effectiveness of decoupling our proposed motion style adapters using Y-Net-Mod. However, during training, we do not enforce any constraints on the learning objective to favor effective modularization that can result in more efficient adaptation. Developing training strategies that allow quick adaptation using MoSA is a potential line of future work. Another limitation is the requirement of human intervention in determining the modules to be adapted given the nature of the style shift. A future direction is automating the selection of layers to be adapted for target domains.


% What's more, a critical hyperparameter is the rank of style modules that hypothesizes the number of factors of variations exhibited in the target domains, which means the rank value can be an indication of problem complexity to some extent. As a result, we can possibly theoretically decide the rank to use depending on the essence of the problem in the future.

% The convolutional weights of the style transfer network encode transformations that
% represent “elements of style”. The scaling and shifting factors would then provide a way for each
% style to inhibit or enhance the expression of various elements of style to form a global identity of
% style. While this work does not attempt to verify this hypothesis, we think that this would constitute a very promising direction of research in understanding the computation behind style transfer
% networks as well as the representation of images in general.






% \begin{figure}
%     \centering
%     \includegraphics[width=0.5\textwidth]{figures/fde_plot.pdf}
%     \caption{Evaluation of adaptation techniques in low-resources settings for long-term motion prediction (25 secs) on Lyft Level 5. During adaptation, the model traverses the unseen route \textit{only once} at different sampling rates (0.5 Hz to 5 Hz). Error in meters.}
%     \label{fig:level5_quant}
% \end{figure}

% \begin{figure*}
%     \centering
%     \includegraphics[width=0.8\textwidth]{figures/l5_route.png}
%     \caption{Training-Adaptation split for Level 5 prediction dataset. Forecasting model has been trained on the route in green and adapted to the route in blue. The distribution shift arises from differing styles in different scene locations during training and adaptation while the inherent motion style parameters of the agent remain same (ego vehicle in both cases). Background taken from \cite{Houston2020OneTA}.}
%     \label{fig:level5_split}
% \end{figure*}


% \begin{figure*}
%     \centering
%     \includegraphics[width=0.9\textwidth]{figures/viz.pdf}
%     \caption{Illustration of the attention heatmaps of the final layer of ViT before and after model adaptation on the unseen route in the Level 5 dataset (ego in green, surrounding agents in blue). Post adaptation, the model starts attending to the essential elements of the rasterized map (turns and green traffic lights). Furthermore, style-injection modules better model the uncertainty (equal attention on going straight and turning) as well as long-term dependencies (cars and lanes on the right-side of raster maps) in comparison to full model fine-tuning.}
%     \label{fig:level5_qual}
% \end{figure*}



% \begin{figure*}
% \begin{minipage}{.48\textwidth}
%   \centering
%   \includegraphics[width=0.97\linewidth]{figures/inD_scene1_trajectory_cropped.pdf}
%   \vspace{-2pt}
%   \captionof{figure}{Car and truck and bus trajectory distribution.}
%   \label{fig:inD_scene1_trajectory}
% \end{minipage}
% \quad
% \begin{minipage}{.48\textwidth}
%   \centering
%   \includegraphics[width=0.97\linewidth]{figures/car_truck_velocity_distribution.pdf}
%   \vspace{-2pt}
%   \captionof{figure}{Velocity distribution of car, and truck and bus in inD dataset of scene1.}
%   \label{fig:inD_scene1_vel}
% \end{minipage}
% \end{figure*}


% \section{Conclusion}


% % In the unusual situation where you want a paper to appear in the
% % references without citing it in the main text, use \nocite
% \nocite{langley00}

\section{Conclusion}

We tackle the task of efficient motion style transfer wherein we adapt a pre-trained forecasting model using limited samples from an unseen target domain. We hypothesize that the underlying shift across domains often resides in a low-dimensional space. We formulated this intuition into our motion style adapter (MoSA) design, which is trained to infer and update the style factors of variation in the target domain while keeping the pre-trained parameters frozen. Additionally, we present a modularized adaptation strategy that updates only a subset of the model given the style transfer setup. Extensive experimentation on three real-world datasets demonstrates the effectiveness of our approach.

% In this work, we performed efficient motion style transfer where we adapt a pretrained motion forecasting model to unseen data-limited domains with different styles. We introduced a low-rank motion style adapter that generally captures the low-dimensional underlying style variations in target domains. We also proposed a modularized style transfer strategy, in which the general motion style concept is decoupled into independent scene-specific and agent-specific sub-modules, with only the sub-module corresponding to the new style shift being updated. Experiments on various real-world datasets reveal that inserting low-rank style adapters and modularizing the motion style category is beneficial towards both sample-efficient and parameter-efficient transfer in trajectory forecasting. 

% We perform efficient motion style transfer wherein we adapt a trained forecasting model on a target domain comprising limited samples of unseen target style. To leverage learned knowledge, we freeze pretrained model parameters and develop low-rank style-injection modules that inject the target style into the motion encoder. We demonstrate the efficacy of our method on popular motion forecasting datasets.

%===============================================================================
\input{appendix}
% \clearpage
% The acknowledgments are automatically included only in the final and preprint versions of the paper.
\acknowledgments{This work was supported by Honda R\&D Co. Ltd, the Swiss National Science Foundation under the Grant 2OOO21-L92326, and EPFL. We also thank VITA members and reviewers for their valuable comments.}

%===============================================================================

% no \bibliographystyle is required, since the corl style is automatically used.
\bibliography{main}  % .bib

% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% % APPENDIX
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\end{document}
