\newpage
\appendix
% \twocolumn

% \section*{Appendices}
\normalsize
\appendix



% The appendices are organized as follows: First, we provide additional experiments and visualizations to demonstrate the effectiveness of our proposed method. Next, we describe the implementation details of all experiments. 

\section{Additional Experiments}

Training a residual has been shown effective in other domains like robotics \cite{Johannink2019ResidualRL}. Yet, itâ€™s not clear what kind of residual is most effective for adaptation, and where they should be introduced. To address these open questions, we introduced two key components specific to motion forecasting: (i) we model the residual as a low-dimensional bottleneck, (ii) we propose a modular adaptation strategy that facilitates a targeted choice of adaptation layers. 

\begin{figure*}
\centering
\begin{minipage}{.50\textwidth}
    \captionof{table}{Importance of the low-rank constraint.}
    \label{tab:ff}
    \resizebox{\textwidth}{!}{%
    \begin{tabular}{|l|ccc||}
        \toprule[0.35ex]
        \midrule[0.25ex]
        \multicolumn{1}{|c}{ } & \multicolumn{3}{|c||}{Lyft Level 5} \\[0.25ex]
        \midrule[0.25ex]
        $N_{target}$ (batches) & 8 & 15 & 36\\[0.25ex]
        \midrule
        Finetuning & 41.76 $\pm$ 0.45  & 33.90 $\pm$ 0.89 & 31.83 $\pm$ 1.89 \\[0.25ex]
        No-Rank Constraint & 42.37 $\pm$ 0.72  & 34.12 $\pm$ 0.51 & 30.37 $\pm$ 2.19 \\[0.25ex]
        Rank Constraint & \textbf{29.43 $\pm$ 0.70} & \textbf{25.96 $\pm$ 0.47} & \textbf{25.06 $\pm$ 1.26} \\[0.25ex]
        \midrule[0.35ex]
    \end{tabular}
    }
\end{minipage}
\quad
\begin{minipage}{.42\textwidth}
    \captionof{table}{Varying the rank $r$ of MoSA.}
    \label{tab:rank}
    \resizebox{\textwidth}{!}{%
    \begin{tabular}{|l|ccc||}
        \toprule[0.35ex]
        \midrule[0.25ex]
        \multicolumn{1}{|c}{ } & \multicolumn{3}{|c||}{Stanford Drone} \\[0.25ex]
        \midrule[0.25ex]
        $N_{target}$ & 10 & 20 & 30\\[0.25ex]
        \midrule
        Rank 1 & 51.84  $\pm$ 1.41  & 46.68 $\pm$ 1.32 & 42.53 $\pm$ 1.19 \\[0.25ex]
        Rank 3  & \textbf{49.98 $\pm$ 1.05} & \textbf{45.55 $\pm$ 0.77} & \textbf{41.69 $\pm$ 0.88} \\[0.25ex]
        Rank 10  & 51.44 $\pm$ 0.66 & 46.48 $\pm$ 0.84 & 42.44 $\pm$ 0.72  \\[0.25ex]
        \midrule[0.35ex]
    \end{tabular}
    }
\end{minipage}
\end{figure*}

\begin{figure*}
\centering
\begin{minipage}{.45\textwidth}
  \centering
  \includegraphics[width=1.15\linewidth,center]{figures/bikers_low2high_fde.pdf}
  \vspace{-2pt}
  \captionof{figure}{Agent Motion Transfer from \textit{slow} cyclists to \textit{fast} cyclists on \textit{death\_circle} SDD scene with $N_{target} = 20$ samples using different MoSA configurations. [A] performs best while [S] worsens performance.}
  \label{fig:biker_l2h}
\end{minipage}
\quad
\begin{minipage}{.45\textwidth}
  \centering
  \includegraphics[width=1.15\linewidth,center]{figures/modular_scene_supp.pdf}
  \vspace{-2pt}
  \captionof{figure}{Scene Transfer from \textit{scene1, scene3, scene4} to \textit{scene2} on InD \textit{pedestrians} with $N_{target} = 20$ samples using different MoSA configurations. [S+F] performs best while [A] worsens performance.}
  \label{fig:scene_transfer_supp}
\end{minipage}
\end{figure*}

\subsection{Importance of Low-Rank constraint}

Tab.~\ref{tab:ff} illustrates the importance of the low-rank constraint in the residual design. Further, Tab.~\ref{tab:rank} provides the performance on varying the rank $r$ of the MoSA modules on SDD. Very low rank limits the number of style factors that can be updated leading to sub-optimal performance. On the other hand, a high rank increases the number of trainable parameters, leading to overfitting in the low-data regime. 

Sec.~\ref{sec:decouple} demonstrated the effectiveness of our modularized adaptation strategy. Next, we provide additional experiments that further validates our proposed scheme.

\subsection{Modularized Adaptation across Agent Motion on SDD}

We demonstrate the efficacy of modularized adaptation scheme on the SDD dataset. We utilize the cyclists data on \textit{deathCircle\_0} scene. The training and adaptation data are constructed based on the average speed of the cyclist trajectories. The training set contains low-speed trajectories with the speed in the range of 0.5 to 3.5 pixel per second. The adaptation set has cyclist trajectories with an average speed in the range of 4 to 8 pixels per second. We use $N_{target}=\{20\}$ trajectories for adaptation. Given our setup, the dominant underlying style factor that changes across domains is the agent speed distribution (the scene context and the type of agent are fixed). We benchmark the performance of Y-Net-Mod given the five modularization strategies described in the main text. Rank of MoSA is set to 2.

Fig.~\ref{fig:biker_l2h} illustrates the performance of various modular adaptation strategies. The generalization error is very high as the error accumulates quickly for fast-moving agents. Using just 20 samples, updating only the agent motion module [A] leads to the best performance, while including the scene context module [S] during adaptation worsens performance.






\subsection{Modularized Adaptation across Scenes on inD}

In the main text, we demonstrated the efficacy of modularized adaptation strategy by training on pedestrian data from $\{ scene2, scene3, scene4\}$ and testing on unseen scene $scene1$. Different scene combinations can help to corroborate the efficacy of our modular adaptation strategy. Therefore, as shown in Fig.~\ref{fig:scene_transfer_supp}, we provide the results of our strategy for a different setup of scene generalization. We train on $\{ scene1, scene3, scene4\}$ and test on unseen scene $scene2$. We observe the same trend as Fig.~\ref{fig:scene_transfer}: updating the scene module [S] is more effective than updating all the modules [S+A+F], and adapting of agent motion module [A] deteriorates performance.



\section{Additional visualizations}

\paragraph{Motion Style Transfer across Scenes on inD.} In this experiment, the Y-Net model is trained on pedestrians in $\{ scene2, scene3, scene4\}$ and tested on unseen scene $scene1$. We qualitatively show the goal decoder output difference of three examples in Fig.~\ref{fig:ped2ped_viz}. We can observe that the adapted model learns to cross at a particular road location, even though this behavior is not present in the training data. 

\paragraph{Motion Style Transfer across Scenes on Level 5.} In this experiment, we divide the L5 dataset into two splits based on the data collection locations thereby, constructing a scene style shift scenario (see Fig.\ref{fig:level5_split}). In addition to the examples provided in the main text, Fig.~\ref{fig:attention-maps-supp} qualitatively illustrates the improvement of the attention heatmaps of the last layer of ViT post model adaptation using MoSA. MoSA helps to better focus on the different possible future routes and the vehicles in front.




\begin{figure*}[t!]
\begin{subfigure}{0.33\textwidth}
    \centering
    \includegraphics[width=\linewidth]{figures/ped2ped/scene1__337/mosa_1_0_1_2_3_4_30__goal_decoder.predictor_sigmoid__diff_single__0.1.pdf}
    \caption{Sample 1}
\end{subfigure}
\begin{subfigure}{0.33\textwidth}
    \centering
    \includegraphics[width=\linewidth]{figures/ped2ped/scene1__339/mosa_1_0_1_2_3_4_30__goal_decoder.predictor_sigmoid__diff_single__0.1.pdf}
    \caption{Sample 2}
\end{subfigure}
\begin{subfigure}{0.33\textwidth}
    \centering
    \includegraphics[width=\linewidth]{figures/ped2ped/scene1__81/mosa_1_0_1_2_3_4_30__goal_decoder.predictor_sigmoid__diff_single__0.1.pdf}
    \caption{Sample 3}
\end{subfigure}
\caption{Illustration of the difference in goal decoder output of Y-Net on the scene style transfer on InD using our motion style adapters (Red is positive, blue is negative). Observe that in the adapted scene, pedestrians tend to cross at a particular segment of the road. This behavior did not occur during the pre-training. MoSA learns this novel behavior using just 30 samples of adaptation.}
\label{fig:ped2ped_viz}
\end{figure*}

\begin{figure}[t!]
\centering
\begin{minipage}{.9\textwidth}
     \begin{subfigure}[b]{0.32\textwidth}
         \centering
         \includegraphics[width=0.99\textwidth]{figures/sample_supp.pdf}
         \caption{Input sample}
         \label{subfig:sample_supp}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{0.32\textwidth}
         \centering
         \includegraphics[width=\textwidth]{figures/pre-adapt_supp.pdf}
         \caption{Pre-adaptation}
         \label{subfig:pre-adapt_supp}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{0.32\textwidth}
         \centering
         \includegraphics[width=\textwidth]{figures/post-adapt_supp.pdf}
         \caption{Post-adaptation}
         \label{subfig:post-adapt_supp}
     \end{subfigure}
     \vspace*{2mm}
        \caption{Attention heatmaps of the last layer of ViT before and after model adaptation on unseen route in Level 5. Post adaptation, the attention maps are more refined. The ego (in green) better focuses on the different possible future routes and the vehicles in front.}
        \label{fig:attention-maps-supp}
\end{minipage}
\end{figure}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%





% \begin{figure}
%     \centering
%     \includegraphics[width=\linewidth]{figures/l5_route_comp.png}
%     \caption{Training-Adaptation split for Level 5 prediction dataset \cite{Houston2020OneTA}. Model is trained on the green route and adapted on the blue route. The distribution shift arises from the different scene locations. The inherent agent motion style parameters remain constant (ego vehicle in both cases).}
%     \label{fig:level5_split}
% \end{figure}

%  Background taken from \cite{Houston2020OneTA}.
 



\section{Dataset Preparation}

We use a total of three datasets to study the performance of motion style adapters: Stanford Drone Dataset (SDD) \citep{Robicquet2016LearningSE}, the Intersection Drone Dataset (InD) \citep{Bock2020TheID}, and Level 5 Dataset (L5) \citep{Houston2020OneTA}. 

\subsection{Stanford Drone Dataset (SDD)}
SDD comprises 20 top-down scenes on the Stanford campus with various agent types (i.e., pedestrians, bicyclists, car, skateboarders, buses, golf carts). We perform short-term prediction where we give 3.2 seconds trajectories and output the future 4.8 seconds. Following the same pre-processing procedure in \citet{Mangalam2021FromGW}, we filter out short trajectories below 8 seconds in duration, split temporally discontinued trajectories, and then use a sliding window approach without overlap to split the cleaned trajectories. After those steps, the dataset contains 14860 pedestrian trajectories and 5152 bicyclist trajectories. The semantic segmentation has 6 classes, namely pavement, terrain, structure, tree, road, and others \cite{caesar2018coco}.

\subsection{Intersection Drone Dataset (inD)}
The inD Dataset comprises four distinct road intersections, namely \textit{scene1}, \textit{scene2}, \textit{scene3}, and \textit{scene4}, with various agent types, i.e., cars, pedestrians, bicyclists, trucks and buses. We perform both short-term and long-term predictions. The short-term prediction outputs 4.8 seconds trajectories given 3.2 seconds observation, while the long-term prediction generates 30 seconds future trajectories given 5 seconds of observed ones. We use similar pre-processing steps as for SDD. Additionally, we convert the data from the real-world coordinates to pixel coordinates using the provided scaling factors \citep{Bock2020TheID}. After the pre-processing steps, inD contains 1396 long-term pedestrian trajectories, 1508 short-term car trajectories, and 157 short-term trucks trajectories. 

\begin{figure*}[t!]
\centering
\begin{minipage}{.46\textwidth}
  \centering
  \includegraphics[width=\textwidth]{figures/scene1_cropped_comp.pdf}
  \vspace{-2pt}
   \captionof{figure}{Car and truck trajectory distribution in inD \textit{scene1}. The two agents share the same scene context, differing in the motion style.}
    \label{fig:inD_scene1_trajectory} 
\end{minipage}
\quad
\begin{minipage}{.46\textwidth}
  \centering
  \includegraphics[width=\textwidth]{figures/car_truck_velocity_distribution.pdf}
  \vspace{-2pt}
    \captionof{figure}{Velocity distribution of \textit{car} and \textit{truck} in inD \textit{scene1}. Static samples removed.}
    \label{fig:car_vel}
\end{minipage}
\end{figure*}

\subsection{Lyft Level 5 Dataset (L5)}
Lyft Level 5 Prediction is a self-driving dataset, containing over 1,000 hours of real-world vehicle data, where each scene lasts 25 seconds. We perform long-term motion forecasting where the ego vehicle moves in a closed loop for the entirety of the scene for 25 seconds, while the surrounding agents follow log-replay \citep{Houston2020OneTA}. In the real-world, ego vehicles can come across novel scene contexts, for example, road constructions. 

In summary, SDD and inD with different heterogeneous agents provide the ideal setup to validate motion style transfer techniques. L5 dataset provides long sequences of ego vehicle to study the effects of style transfer on long-term self-driving settings. 



\section{Pre-trained Models}

We utilize the state-of-the-art model Y-Net \citep{Mangalam2021FromGW} for experiments on SDD and inD. We further propose an alternate design of Y-Net termed Y-Net-Mod to demonstrate the efficacy of our modular adaptation strategy. Finally, we utilize the ViT-Tiny \citep{dosovitskiy2020image} architecture for experiments on L5.

\subsection{Y-Net}

Y-net \cite{Mangalam2021FromGW} comprises three sub-networks: the scene heatmap encoder, the waypoint heatmap decoder, and the trajectory heatmap decoder. Specifically, the encoder is designed as a U-net encoder which consists of one center convolutional layer, four intermediate blocks where each uses max pooling and two convolutional layers, and one final max pooling layer. It takes as input the concatenation of the scene semantic map and past trajectory heatmap.


% \begin{table}[t]
%     \centering
%     \caption{Generalization performance of Y-Net and Y-Net-Mod on two modularization setups. Errors reported are Top-20 ADE/FDE in pixels. Y-Net-Mod performs at par with the original Y-Net.}
%     \vspace*{2mm}
%     \label{tab:ynet_ynetmod_performance}
%     \resizebox{0.48\linewidth}{!}{
%     \begin{tabular}{l|c|c}
%         \toprule[0.35ex]
%         \midrule[0.25ex]
%         Experiment & Y-Net & Y-Net-Mod \\
%         \midrule[0.25ex]
%         Scene transfer on inD & 6.44 / 10.72 & 6.60 / 11.17 \\ 
%         Agent motion transfer on inD & 24.48 / 33.54 & 24.56 / 29.07 \\
%         \midrule[0.35ex]
%     \end{tabular}}
% \end{table}
% Y-Net & 6.44 / 10.72 & 22.23 / 23.33 \\ 


\subsection{Y-Net-Mod}

We construct Y-Net-Mod on top of the original Y-Net architecture. The modification treats the scene context and agent motion \textit{independently} before fusing their representations together. As shown in Fig.~\ref{fig:modular}, the first three layers of the original encoder are decoupled into scene context and past agent motion modules in order to learn their representations independently. Subsequently, the representations are fused together using the fusion encoder, that is similar in design to the last two layers of Y-Net. The original number of channels in each encoder layer of Y-Net are evenly divided between each module in Y-Net-Mod encoder so that the latter is compatible with the Y-Net decoders. 


\paragraph{Benchmarking Y-Net and Y-Net-Mod:} To illustrate that our modification to Y-Net does not result in severe drop in performance, we benchmark the performance of Y-Net and Y-Net-Mod on inD for the modularization setups presented in main draft: 1) scene transfer of pedestrians, 2) agent motion transfer from cars to trucks. Table.~\ref{tab:ynet_ynetmod_performance} illustrates that modularization of the Y-Net encoder does not lead to a significant drop in the performance.

\subsection{Vision Transformer}
We utilize the official ViT-Tiny architecture \citep{dosovitskiy2020image} for the Level 5 dataset. We only modify the last layer to output the forecasting predictions in the form of $x, y$ coordinates for $T_{pred}$ time-steps.


% \begin{figure}[t]
%     \begin{subfigure}{0.47\linewidth}
%     \centering
%     \includegraphics[width=\linewidth]{figures/style-injection_PA.pdf}
%     \caption{Parallel residual adapters \citep{rebuffi2018efficient}.}
%     \label{fig:style-injection_pa}
%     \end{subfigure}
%     \hfill
%     \begin{subfigure}{0.47\linewidth}
%     \centering
%     \includegraphics[width=\linewidth]{figures/style-injection_MoSA.pdf}
%     \caption{Modular Style Adapters (ours).}
%     \label{fig:style-injection_mosa}
%     \end{subfigure}
%     \caption{Illustration of different adapter designs applied to the Y-Net encoder layers.}
%     \label{fig:style-injection}
% \end{figure}



\begin{figure*}[t!]
\centering
\begin{minipage}{.42\textwidth}
  \centering
  \includegraphics[width=\linewidth]{figures/l5_route_comp.png}
  \vspace{-2pt}
  \captionof{figure}{Train-adaptation split for Level 5 dataset \cite{Houston2020OneTA}. Model is trained on the green route and adapted on the blue route.}
  \label{fig:level5_split}
\end{minipage}
\quad
\begin{minipage}{.54\textwidth}
  \centering
  \captionof{table}{Generalization performance of Y-Net and Y-Net-Mod on the modularization setups of inD. Errors reported are Top-20 ADE/FDE in pixels. Y-Net-Mod does not degrade performance in comparison to Y-Net.}
    % \vspace*{2mm}
  \label{tab:ynet_ynetmod_performance}
  \resizebox{\linewidth}{!}{
    \begin{tabular}{l|c|c}
        \toprule[0.35ex]
        \midrule[0.25ex]
        Experiment & Y-Net & Y-Net-Mod \\
        \midrule[0.25ex]
        Scene transfer & 6.44 / 10.72 & 6.60 / 11.17 \\ 
        Agent motion transfer & 24.48 / 33.54 & 24.56 / 29.07 \\
        \midrule[0.35ex]
    \end{tabular}}
\end{minipage}
\end{figure*}

\section{Implementations Details}

We present implementation details and hyperparameters for each method and model training. For each experiment, the best model is chosen based on the performance on the validation set. All model pretraining follows the training, validation and test split of 7:1:2. During adaptation, all experiments utilize Adam optimizer and a batch size of 10, unless mentioned otherwise. Learning rate for FT is $5\text{e-}5$,  $5\text{e-}4$ for ET, $5\text{e-}5$ for PA, $1\text{e-}4$ for BN, and $5\text{e-}3$ for MoSA, unless mentioned otherwise. The details for our designed experiments are listed as below.

\paragraph{Motion Style Transfer across Agents on SDD.} We pretrain Y-Net network for 100 epochs and learning rate of $5\text{e-}5$. Rest of the hyper-parameters are kept the same as \cite{Mangalam2021FromGW}. We adapt the pretrained model using $N_{target}=\{10,20,30\}$ trajectories and utilize 80 trajectories for validation. We adapt the pretrained model for 100 epochs with an early stop value of 30 epochs. For MoSA, the rank value is set to 3.

\paragraph{Motion Style Transfer across Scenes on inD.} In this experiment, we utilize the pretrained model provided by Y-Net paper \cite{Mangalam2021FromGW}. We adapt this model using $N_{target}=\{20\}$ trajectories and use 40 trajectories for validation. The pretrained model is 100 epochs. Fig.~\ref{fig:ped2ped_viz} illustrates the adaptation performance of MoSA using 30 samples. MoSA learns the unseen behavior of pedestrians crossing at a particular segment of the road, that was unobserved in the training scenes of inD.

\paragraph{Motion Style Transfer across Scenes on L5.} 
To simulate a scene context transfer scenario, we split the dataset as shown in Fig.~\ref{fig:level5_split}. The ViT-Tiny model is trained on the majority route shown in green and adapted to the blue route that was not seen during training. We follow the training strategy provided in \citet{Houston2020OneTA}: specifically, the network is trained to accept BEV rasters of size $224 \times 224$ pixels (centered around the SDV) to predict future $(x,y)$ positions over a 1.2 second horizon. The hyper-parameters of the ViT-Tiny architecture are kept the same as in \cite{dosovitskiy2020image}. We train the model on the training data corresponding to the majority route for 15 epochs using batch size of 64.

To simulate low-resource settings during adaptation, the network is shown the unseen route only once, sampled at different rates. As a result, we adapt for $N_{batches}=\{4, 8, 15, 24, 36\}$ with a batch size of 64. We benchmark the following four cases: 1) Full model fine-tuning with learning rate of $1\text{e-}4$, 2) Final layer fine-tuning with learning rate of $3\text{e-}4$, 3) Adaptive layer normalization with a learning rate of $1\text{e-}4$, 4) MoSA (ours) with rank value of 8 and learning rate of $3\text{e-}3$. We apply MoSA across the query and value matrices of each attention layer. We observe that applying MoSA across the feed-forward layers deteriorated performance. We adapt all the methods for 250 epochs using a one-cycle learning rate scheduler.

\paragraph{Motion Style Transfer across Agents on inD with Modular Adaptation.} We perform style transfer from cars to trucks in \textit{scene1} of inD. Cars and trucks data have different speed distribution (see Fig.~\ref{fig:car_vel}) but share the same context as shown in Fig.~\ref{fig:inD_scene1_trajectory}. Trajectories with an average speed less than 0.2 pixels per second are filtered out. We train a Y-Net-Mod model on cars and adapt the model to trucks in which $N_{target}=\{20\}$ trajectories are used for adaptation and 40 trajectories for validation. 

\paragraph{Motion Style Transfer across Scenes on inD with Modular Adaptation.} We pretrained Y-Net-Mod model using pedestrian data from scene ids = $\{2, 3, 4\}$ and transferred to pedestrian data from \textit{scene1} following the setup in \citep{Mangalam2021FromGW}. The adaptation uses $N_{target}=\{20\}$ for fine-tuning and 20 trajectories for validation.

\paragraph{Motion Style Transfer across Agent Motion on SDD with Modular Adaptation.} We pretrain a Y-Net-Mod model using slow cyclists from \textit{deathCircle\_0} scene and adapt to fast cyclists from the same scene. The pretraining set has 1213 trajectories. The adaptation set has 381 trajectories where 50 trajectories are used for validation. 

\subsection{Initialization of Motion Style Adapters}
Matrices $A$ and $B$ are initialized such that the original network is not affected when training starts. Specifically, we use a random Gaussian initialization for $A$ and zero for $B$. This initialization scheme allows these modules to be ignored at certain layers if there is no need for a change in activation distribution.
