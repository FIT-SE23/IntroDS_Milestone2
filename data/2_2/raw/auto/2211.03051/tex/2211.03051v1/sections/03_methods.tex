\section{Methods}
\label{sec:methods}

\subsection{Data Collection}
Zebrafish larvae with wildtype and mutant \textit{dnajc6} genotypes were generated by heterozygous incross of two parent pairs. Two families were generated on subsequent days. Larvae were placed in 24-well plates in 1 mL of E3 embryo medium \cite{westerfield_zebrafish_2000} at 4 days old, before being subjected to behavioural testing at 5 days old. Video footage was collected of larvae swimming in the 24-well plates for 15 minutes using the DanioVision (Noldus). A sudden light to dark transition was used to provoke a stress response in the larvae once 5 minutes had elapsed, as we believed that this stimulation would emphasise any genotypic effects on behaviour (specifically locomotor deficits). Each tray of larvae was subjected to 4 separate behaviour trials throughout the course of the day to augment the data available for the network. Fish were genotyped after behaviour testing.

Tracking of the zebrafish and calculation of behavioural variables was done using the EthoVisionXT software (RRID: SCR\_000441). The ``centrepoint" (trunk) of each larva was tracked by the software from live 2D video footage at 15 frames/second. Table 1 shows the statistics calculated for each 1 minute time bin. Note that there is no correspondence across one minute windows.

\newcolumntype{P}[1]{>{\arraybackslash}p{#1}}
\begin{table}
    \caption{Behavioural variables used to train the network. Features were calculated based on the tracked centre-point of each larva for one minute time bins. For each variable, one or more statistics were calculated for a total of 25 variables.}
    \small
    \begin{tabular}{P{83pt} P{35pt} P{83pt}}
        \toprule
        \textbf{Variable}&\textbf{Units}&\textbf{Statistic}\\
        \midrule
        Distance travelled  & mm        & Total, mean, variance   \\
        Velocity	        & mm/s	    & Mean, variance          \\
        Time moving	        & s	        & Total                   \\
        Frequency moving	& count	    & Total                   \\
        Acceleration	    & mm/s$^2$  & Max, min, variance      \\
        Frequency in middle & count     & Total                   \\
        Time in middle	    & s	        & Total, mean, variance   \\
        Distance to middle	& mm	    & Total, mean, variance   \\
        Mobility	        & \%	    & Total, mean, variance   \\
        Meander	            & deg/mm	& Total, mean, variance   \\
        Heading	            & deg	    & Mean, variance          \\
        \bottomrule
    \end{tabular}
\label{tab:features}
\end{table}

\subsection{Pre-processing}
Pre-processing of the raw data was performed in R. Behavioural data from both families (spawned and tested on different days) was joined to the fish metadata (genotype, tray number, position in tray, trial number). Two fish were omitted, one due to a deformity that prevented movement and the other due to issues with detection of the fish by the behaviour tracking software.

\subsection*{Outlier removal}
The absolute z-score (distance from the mean in standard deviations) was calculated for each data point based on the mean value of that statistic within a genotype group. This was done to prevent the removal of attributes that were due to differences between genotypes. Any values with a z-score $>$4 (4 standard deviations from the mean) were removed. This was done separately for each family before merging all data.

\subsection*{Data summarisation}
The values of each behavioural statistic were averaged for each fish across three time bins from the first light period (bins 3, 4, and 5), and the dark period (bins 6, 7, and 8). These summarised values were used as behavioural features (eg. distance travelled in the light, distance travelled in the dark). A total of 41 features were used here. For the dark period, all statistics in Table 1 were used and for the light period, all statistics other than the distance to middle (variance), meander (all), and heading (all).

\subsection{Model Training}
% For simplicity, binary classification of wildtype vs. mutant was performed. Due to the heterogeneity of behaviour observed in heterozygous fish, they were omitted so the model was only trained on wildtype or homozygous mutant fish.

Prior to training, the data was standardised to have a mean of zero and a standard deviation of one to encourage faster convergence. A random 85/15 train/validation split was done to set aside data for testing the generalisation ability of the models. Each model was trained on the validation data for 500 epochs and the model was evaluated after each epoch. The model with the highest validation accuracy during this time was chosen as the most fit model and was used for calculating integrated gradients.

% Mention data normalisation
% \subsection{Neural Network}
% We designed a lightweight neural network for classifying wildtype and mutant genotypes. An important design choice was for the NN to be able to be trained using a laptop. In our case, trained on an M1 Macbook Pro. This was accomplished by maintaining a hidden layer depth of three. A sufficiently wide neural network with just a single hidden layer can approximate any (reasonable) function given sufficient training data. However, increasing the width of a NN too much will come with the adverse affect of memorisation. Our hypothesis space is sufficiently small where three layers are adequate. It's worth noting that increasing the hypothesis space requires an exponential increase in width to approximate the new space \cite{eldan_power_2016}.

% The output of the final layer in the NN is passed through a sigmoid function to produce a probability distribution. Cross entropy loss is then used to calculate the error between the predicted distribution and the true distribution. The loss is backpropagated via stochastic gradient descent (SGD). Paired with the non-linear activation function ReLU, we are able to find the global minima of the training loss \cite{zou_stochastic_2018}. A learning rate of $5e^{-4}$ in combination with a momentum of $0.99$ proved to be near optimal for training. The momentum was important for not getting stuck in local minima \cite{sutskever_importance_nodate}.

% Initialisation and generalisation of the network was of utmost importance and proved a difficult challenge. We initialise our network using \textit{He initialisation} as it is a great choice when paired with the ReLU activation function, instead of the popular \textit{Xavier initialisation}, as theoretically proven by \cite{kumar_weight_2017}. We found that a batch size of $10$ produced the best generalisation, as higher batch sizes accelerated overfitting and a lower size made it difficult for the model to converge due to "jumpiness" in the loss caused by significant gradient changes \cite{kandel_effect_2020}. Dropout proved to be essential in reducing overfitting in our model by evading the model's simplicity bias. Instead of being able to learn the easiest way (memorisation) to reduce the loss of the training data, the model is forced to learn more difficult functions \cite{ying_overview_2019}. We found that inciting dropout ($p=0.5$) after the first and second layers was ideal.

% \subsection*{Attribution}
% Understanding the flow of information through NNs is a challenging problem that has recently gained increased attention. Being able to analyse the impact of input features on outputs allow further analysis and selection of features for obtaining better results (cite Towards better understanding of gradient-based). A simple manner of evaluating the importance of features given by a network can be by analysing the input gradients, that is, the gradients between the input layer and the first hidden layer. But this has a clear limitation of only representing a small portion of the network, akin to looking at it through a peep hole. 

% (Moved to model architecture)Instead, we make use of a method that satisfies two fundamental axoims of attribution - \textit{Sensitivity} and \textit{Implementation Invariance}. A lack of sensitivity causes gradients to focus on irrelevant features, and attributions should always be the same for two networks that produce the same output from the same input. Attribution can then be defined as assigning the blame (or credit) of an output to the input features \cite{sundararajan_axiomatic_2017}. In this way, we go on to assign blame to our input features to inform us of the usefulness of said features. We go into our analysis of the assigned blames in our next section.
% I need to read on this again