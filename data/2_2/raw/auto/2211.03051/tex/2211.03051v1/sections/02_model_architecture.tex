\section{Model Architecture}
\label{sec:model_architecture}
Our proposed network architecture has an input layer of 41 dimensions, three hidden layers of $n$ dimensions, and an output layer of two dimensions. We chose an output size of two, even though we are solving a binary classification problem, to allow for the simple addition of another genotype for classification (heterozygous). The optimal dimension of the hidden layers is decided after evaluating our experiments as discussed in the next section. The architecture is visualised in Figure \ref{fig:architecture}.

\begin{figure}[t]
    \begin{center}
    % \fbox{\rule{0pt}{2in} \rule{0.9\linewidth}{0pt}}
        \includegraphics[width=1\linewidth]{architecture.png}
    \end{center}
        \caption{A visualisation of the network architecture used. The input layer has a perceptron for each behaviour feature (41). Each hidden layer is followed by a ReLU activation function. The first two hidden layers are inhibited by dropout. The output layer has two perceptrons, one for each genotype (wildtype and homozygous), and is passed to a sigmoid activation function.}
    \label{fig:architecture}
    \end{figure}

\subsection{Initialisation}
Random initialisation of a neural network has been shown to lead to slower convergence and toward poorer local minima \cite{glorot_understanding_2010}. Xavier initialisation is a popular arrow in the quiver of deep neural network (NN) researchers as it decreases the probability of encountering the vanishing/exploding gradients problem during training \cite{boulila_weight_2021}. However, this method is based on the assumption that are linear \cite{kumar_weight_2017}. This assumption is invalid for the ReLU activation function used in our proposed architecture. He \textit{et al}. proposed a new initialisation strategy for ReLU-activated networks \cite{he_delving_2015}. \textit{He initialisation} sets the network weights randomly from a normal distribution, as in Xavier initialisation, but with a variance of: \begin{equation} v^{2} = 2/N \end{equation} in order to reduce the probability of the dying neuron problem.


\subsection{Generalisation}
Batch size is an important contributor to the generalisation ability of a network. When employing stochastic gradient descent as a learning strategy, it is necessary to ensure that the batch size is not too large so as to not reduce the gradient noise \cite{kandel_effect_2020, he_control_2019}. Fluctuation in gradient is sometimes required to escape a poor local minima, thus reduction of this ability can be detrimental. A batch size of $10$ proved helpful for our network to reach good minima in an adequate amount of epochs.

A significant contribution to the field in terms of improving the generalisation of a network is Dropout \cite{srivastava_dropout_nodate}. The proposed idea is to randomly drop perceptrons and their connections from the network during training. Although simple it is extremely effective and has seen massive adoption to network architectures \cite{labach_survey_2019}. By randomly inhibiting the network it prevents units from co-adapting too much. Metaphorically speaking, it prevents the network from memorising the answers to the test since it is easier than studying the concepts \cite{ying_overview_2019}. We found that inciting a dropout ($p=0.5$) after the first and second layers of our network was ideal for achieving greater generalisation.


\subsection{Integrated Gradients}
Understanding the flow of information through NNs is a challenging problem that has recently gained increased attention. Being able to analyse the impact of input features on outputs allow further analysis and selection of features for obtaining better results \cite{ancona_towards_2018}. A simple manner of evaluating the importance of features given by a network can be by analysing the input gradients, that is, the gradients between the input layer and the first hidden layer. But this has a clear limitation of only representing a small portion of the network, akin to looking at it through a peep hole. 

We make use of a method that satisfies two fundamental axoims of attribution - \textit{Sensitivity} and \textit{Implementation Invariance}. In simpler words, a lack of sensitivity causes gradients to focus on irrelevant features and networks that produce the same outputs, given the same input data, should always have the same attributions. Sundararajan et al. show that most methods do not satisfy these axioms and proposed a new method \cite{sundararajan_axiomatic_2017}. Integrated gradients postulates an additional axiom called \textit{completeness}, stating that, for an input $x$, the attributions should add up to the difference between the output of a network and a baseline. It is suggested that the baseline be chosen such that its prediction is near zero. The proposed technique can be applied to a range of network architectures, including ours.

\subsection{Hyper-Parameters}
We designed a lightweight neural network for classifying wildtype and mutant genotypes. An important design choice was for the network to be able to be trained using a portable laptop, in our case, an M1 Macbook Pro. This was accomplished by only having a hidden depth of three layers that are fully connected. A sufficiently wide neural network with just a single hidden layer can approximate any (reasonable) function given sufficient training data, although increasing the hypothesis space requires an exponential increase in width \cite{eldan_power_2016}. Our hypothesis space is sufficiently small where three layers is adequate to approximate our ideal function.

The output of the final layer in the NN is passes through a sigmoid function to produce a probability distribution. A logistic loss (binary cross entropy) with one-hot encoded labels is used to calculate the error between the predicted distribution and the true distribution. The loss is backpropagated via stochastic gradient descent (SGD) \cite{robbins_stochastic_1951} where, paired with the non-linear activation function ReLU, we are able to reliably find the global minima of the training loss \cite{zou_stochastic_2018}. Adding momentum to SGD has also been shown to be helpful in traversing sub-optimal local minima \cite{sutskever_importance_2013}. A learning rate of $5e^{-4}$ was chosen in combination with a momentum of $0.99$ and proved to be near optimal for our network training.
