
@article{hosseini_efficient_2019,
	title = {Efficient phenotypic sex classification of zebrafish using machine learning methods},
	volume = {9},
	issn = {2045-7758},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/ece3.5788},
	doi = {10.1002/ece3.5788},
	abstract = {Sex determination in zebrafish by manual approaches according to current guidelines relies on human observation. These guidelines for sex recognition have proven to be subjective and highly labor-intensive. To address this problem, we present a methodology to automatically classify the phenotypic sex using two machine learning methods: Deep Convolutional Neural Networks (DCNNs) based on the whole fish appearance and Support Vector Machine (SVM) based on caudal fin coloration. Machine learning techniques in sex classification provide potential efficiency with the advantage of automatization and robustness in the prediction process. Furthermore, since developmental plasticity can be influenced by environmental conditions, we have investigated the impact of elevated water temperature during embryogenesis on sex and sex-related differences in color intensity of adult zebrafish. The estimated color intensity based on SVM was then applied to detect the association between coloration and body weight and length. Phenotypic sex classifications using machine learning methods resulted in a high degree of association with the real sex in nontreated animals. In temperature-induced animals, DCNNs reached a performance of 100\%, whereas 20\% of males were misclassified using SVM due to a lower color intensity. Furthermore, a positive association between color intensity and body weight and length was observed in males. Our study demonstrates that high ambient temperature leads to a lower color intensity in male animals and a positive association of male caudal fin coloration with body weight and length, which appears to play a significant role in sexual attraction. The software developed for sex classification in this study is readily applicable to other species with sex-linked visible phenotypic differences.},
	language = {en},
	number = {23},
	urldate = {2022-10-08},
	journal = {Ecology and Evolution},
	author = {Hosseini, Shahrbanou and Simianer, Henner and Tetens, Jens and Brenig, Bertram and Herzog, Sebastian and Sharifi, Ahmad Reza},
	year = {2019},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/ece3.5788},
	keywords = {zebrafish, color, machine learning, sex classification, temperature},
	pages = {13332--13343},
	file = {Full Text PDF:C\:\\Users\\Angel\\Zotero\\storage\\IC6LNXBI\\Hosseini et al. - 2019 - Efficient phenotypic sex classification of zebrafi.pdf:application/pdf},
}

@article{yang_zebrafish_2021,
	title = {Zebrafish behavior feature recognition using three-dimensional tracking and machine learning},
	volume = {11},
	copyright = {2021 The Author(s)},
	issn = {2045-2322},
	url = {https://www.nature.com/articles/s41598-021-92854-0},
	doi = {10.1038/s41598-021-92854-0},
	abstract = {In this work, we aim to construct a new behavior analysis method by using machine learning. We used two cameras to capture three-dimensional (3D) tracking data of zebrafish, which were analyzed using fuzzy adaptive resonance theory (FuzzyART), a type of machine learning algorithm, to identify specific behavioral features. The method was tested based on an experiment in which electric shocks were delivered to zebrafish and zebrafish swimming was tracked in 3D simultaneously to find electric shock-associated behaviors. By processing the obtained data with FuzzyART, we discovered that distinguishing behaviors were statistically linked to the electric shock based on the machine learning algorithm. Moreover, our system could accept user-supplied data for detection and quantitative analysis of the behavior features, such as the behavior features defined by the 3D tracking analysis above. This system could be applied to discover new distinct behavior features in mutant zebrafish and used for drug administration screening and cognitive ability tests of zebrafish in the future.},
	language = {en},
	number = {1},
	urldate = {2022-10-08},
	journal = {Scientific Reports},
	author = {Yang, Peng and Takahashi, Hiro and Murase, Masataka and Itoh, Motoyuki},
	month = jun,
	year = {2021},
	note = {Number: 1
Publisher: Nature Publishing Group},
	keywords = {Behavioural methods, Cognitive neuroscience},
	pages = {13492},
	file = {Full Text PDF:C\:\\Users\\Angel\\Zotero\\storage\\B6DLAJQI\\Yang et al. - 2021 - Zebrafish behavior feature recognition using three.pdf:application/pdf},
}

@article{hughes_machine_2020,
	title = {Machine learning discriminates a movement disorder in a zebrafish model of {Parkinson}'s disease},
	volume = {13},
	issn = {1754-8403},
	url = {https://doi.org/10.1242/dmm.045815},
	doi = {10.1242/dmm.045815},
	abstract = {Animal models of human disease provide an in vivo system that can reveal molecular mechanisms by which mutations cause pathology, and, moreover, have the potential to provide a valuable tool for drug development. Here, we have developed a zebrafish model of Parkinson's disease (PD) together with a novel method to screen for movement disorders in adult fish, pioneering a more efficient drug-testing route. Mutation of the PARK7 gene (which encodes DJ-1) is known to cause monogenic autosomal recessive PD in humans, and, using CRISPR/Cas9 gene editing, we generated a Dj-1 loss-of-function zebrafish with molecular hallmarks of PD. To establish whether there is a human-relevant parkinsonian phenotype in our model, we adapted proven tools used to diagnose PD in clinics and developed a novel and unbiased computational method to classify movement disorders in adult zebrafish. Using high-resolution video capture and machine learning, we extracted novel features of movement from continuous data streams and used an evolutionary algorithm to classify parkinsonian fish. This method will be widely applicable for assessing zebrafish models of human motor diseases and provide a valuable asset for the therapeutics pipeline. In addition, interrogation of RNA-seq data indicate metabolic reprogramming of brains in the absence of Dj-1, adding to growing evidence that disruption of bioenergetics is a key feature of neurodegeneration.This article has an associated First Person interview with the first author of the paper.},
	number = {10},
	urldate = {2022-10-08},
	journal = {Disease Models \& Mechanisms},
	author = {Hughes, Gideon L. and Lones, Michael A. and Bedder, Matthew and Currie, Peter D. and Smith, Stephen L. and Pownall, Mary Elizabeth},
	month = oct,
	year = {2020},
	pages = {dmm045815},
	file = {Full Text PDF:C\:\\Users\\Angel\\Zotero\\storage\\4SIQCCWI\\Hughes et al. - 2020 - Machine learning discriminates a movement disorder.pdf:application/pdf},
}

@article{tharwat_towards_2015,
	series = {International {Conference} on {Communications}, management, and {Information} technology ({ICCMIT}'2015)},
	title = {Towards an {Automated} {Zebrafish}-based {Toxicity} {Test} {Model} {Using} {Machine} {Learning}},
	volume = {65},
	issn = {1877-0509},
	url = {https://www.sciencedirect.com/science/article/pii/S1877050915028380},
	doi = {10.1016/j.procs.2015.09.008},
	abstract = {Zebrafish animal is considered as one of the most suitable animals to test toxicity of compounds due many features such as transparency and a large number of embryos produced in each mating. The main problem of the zebrafish-based toxicity test is the manual inspection of thousands of animals images in different phases and this is not feasible enough for the analysis, i.e. it is slow and may be inaccurate process. To help addressing this problem, in this paper, an automated classification of alive (healthy) and coagulant (died because of toxic compounds) zebrafish embryos are proposed. The embryos’ images are used to extract some features using the Segmentation-based Fractal Texture Analysis (SFTA) technique. The Rotation Forest classifier is then used to match between testing and training features (i.e. to classify alive and coagulant embryos). The experiments have proved that choosing threshold value of SFTA technique and the size of the rotation forest classifier have a great impact on the classification accuracy. With accuracy around 99.98\%, the experimental results have showed that the proposed model is a very promising step toward a fully automated toxicity test during drug discovery.},
	language = {en},
	urldate = {2022-10-08},
	journal = {Procedia Computer Science},
	author = {Tharwat, Alaa and Gaber, Tarek and Fouad, Mohamed Mostaf and Snasel, Vaclav and Hassanien, Aboul Ella},
	month = jan,
	year = {2015},
	keywords = {Zebrafish, Classification, Fish embryo toxicity test, Rotation Forest Classifier, Segmentation-based Fractal Texture Analysis (SFTA)},
	pages = {643--651},
	file = {ScienceDirect Full Text PDF:C\:\\Users\\Angel\\Zotero\\storage\\8FPHUKVN\\Tharwat et al. - 2015 - Towards an Automated Zebrafish-based Toxicity Test.pdf:application/pdf;ScienceDirect Snapshot:C\:\\Users\\Angel\\Zotero\\storage\\Q7A54PAH\\S1877050915028380.html:text/html},
}

@misc{teney_evading_2022,
	title = {Evading the {Simplicity} {Bias}: {Training} a {Diverse} {Set} of {Models} {Discovers} {Solutions} with {Superior} {OOD} {Generalization}},
	shorttitle = {Evading the {Simplicity} {Bias}},
	url = {http://arxiv.org/abs/2105.05612},
	doi = {10.48550/arXiv.2105.05612},
	abstract = {Neural networks trained with SGD were recently shown to rely preferentially on linearly-predictive features and can ignore complex, equally-predictive ones. This simplicity bias can explain their lack of robustness out of distribution (OOD). The more complex the task to learn, the more likely it is that statistical artifacts (i.e. selection biases, spurious correlations) are simpler than the mechanisms to learn. We demonstrate that the simplicity bias can be mitigated and OOD generalization improved. We train a set of similar models to fit the data in different ways using a penalty on the alignment of their input gradients. We show theoretically and empirically that this induces the learning of more complex predictive patterns. OOD generalization fundamentally requires information beyond i.i.d. examples, such as multiple training environments, counterfactual examples, or other side information. Our approach shows that we can defer this requirement to an independent model selection stage. We obtain SOTA results in visual recognition on biased data and generalization across visual domains. The method - the first to evade the simplicity bias - highlights the need for a better understanding and control of inductive biases in deep learning.},
	urldate = {2022-10-07},
	publisher = {arXiv},
	author = {Teney, Damien and Abbasnejad, Ehsan and Lucey, Simon and Hengel, Anton van den},
	month = sep,
	year = {2022},
	note = {arXiv:2105.05612 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Computer Vision and Pattern Recognition},
}

@misc{teney_evading_2022-1,
	title = {Evading the {Simplicity} {Bias}: {Training} a {Diverse} {Set} of {Models} {Discovers} {Solutions} with {Superior} {OOD} {Generalization}},
	shorttitle = {Evading the {Simplicity} {Bias}},
	url = {http://arxiv.org/abs/2105.05612},
	doi = {10.48550/arXiv.2105.05612},
	abstract = {Neural networks trained with SGD were recently shown to rely preferentially on linearly-predictive features and can ignore complex, equally-predictive ones. This simplicity bias can explain their lack of robustness out of distribution (OOD). The more complex the task to learn, the more likely it is that statistical artifacts (i.e. selection biases, spurious correlations) are simpler than the mechanisms to learn. We demonstrate that the simplicity bias can be mitigated and OOD generalization improved. We train a set of similar models to fit the data in different ways using a penalty on the alignment of their input gradients. We show theoretically and empirically that this induces the learning of more complex predictive patterns. OOD generalization fundamentally requires information beyond i.i.d. examples, such as multiple training environments, counterfactual examples, or other side information. Our approach shows that we can defer this requirement to an independent model selection stage. We obtain SOTA results in visual recognition on biased data and generalization across visual domains. The method - the first to evade the simplicity bias - highlights the need for a better understanding and control of inductive biases in deep learning.},
	urldate = {2022-10-07},
	publisher = {arXiv},
	author = {Teney, Damien and Abbasnejad, Ehsan and Lucey, Simon and Hengel, Anton van den},
	month = sep,
	year = {2022},
	note = {arXiv:2105.05612 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Computer Vision and Pattern Recognition},
}

@misc{sundararajan_axiomatic_2017,
	title = {Axiomatic {Attribution} for {Deep} {Networks}},
	url = {http://arxiv.org/abs/1703.01365},
	abstract = {We study the problem of attributing the prediction of a deep network to its input features, a problem previously studied by several other works. We identify two fundamental axioms---Sensitivity and Implementation Invariance that attribution methods ought to satisfy. We show that they are not satisfied by most known attribution methods, which we consider to be a fundamental weakness of those methods. We use the axioms to guide the design of a new attribution method called Integrated Gradients. Our method requires no modification to the original network and is extremely simple to implement; it just needs a few calls to the standard gradient operator. We apply this method to a couple of image models, a couple of text models and a chemistry model, demonstrating its ability to debug networks, to extract rules from a network, and to enable users to engage with models better.},
	urldate = {2022-10-07},
	publisher = {arXiv},
	author = {Sundararajan, Mukund and Taly, Ankur and Yan, Qiqi},
	month = jun,
	year = {2017},
	note = {arXiv:1703.01365 [cs]},
	keywords = {Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:C\:\\Users\\Angel\\Zotero\\storage\\ZADQBWXR\\Sundararajan et al. - 2017 - Axiomatic Attribution for Deep Networks.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Angel\\Zotero\\storage\\NRERSQ4C\\1703.html:text/html},
}

@misc{kumar_weight_2017,
	title = {On weight initialization in deep neural networks},
	url = {http://arxiv.org/abs/1704.08863},
	abstract = {A proper initialization of the weights in a neural network is critical to its convergence. Current insights into weight initialization come primarily from linear activation functions. In this paper, I develop a theory for weight initializations with non-linear activations. First, I derive a general weight initialization strategy for any neural network using activation functions differentiable at 0. Next, I derive the weight initialization strategy for the Rectified Linear Unit (RELU), and provide theoretical insights into why the Xavier initialization is a poor choice with RELU activations. My analysis provides a clear demonstration of the role of non-linearities in determining the proper weight initializations.},
	urldate = {2022-10-07},
	publisher = {arXiv},
	author = {Kumar, Siddharth Krishna},
	month = may,
	year = {2017},
	note = {arXiv:1704.08863 [cs]},
	keywords = {Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:C\:\\Users\\Angel\\Zotero\\storage\\TYMPS7PS\\Kumar - 2017 - On weight initialization in deep neural networks.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Angel\\Zotero\\storage\\53876GA7\\1704.html:text/html},
}

@article{marques_structure_2018,
	title = {Structure of the {Zebrafish} {Locomotor} {Repertoire} {Revealed} with {Unsupervised} {Behavioral} {Clustering}},
	volume = {28},
	issn = {0960-9822},
	url = {https://www.cell.com/current-biology/abstract/S0960-9822(17)31604-4},
	doi = {10.1016/j.cub.2017.12.002},
	language = {English},
	number = {2},
	urldate = {2022-10-10},
	journal = {Current Biology},
	author = {Marques, João C. and Lackner, Simone and Félix, Rita and Orger, Michael B.},
	month = jan,
	year = {2018},
	pmid = {29307558},
	note = {Publisher: Elsevier},
	keywords = {zebrafish, behavior, behavioral motifs, cluster analysis, clusterdv, locomotion, motor control, sequences, unsupervised machine learning, visual behavior},
	pages = {181--195.e5},
	file = {Full Text PDF:C\:\\Users\\Angel\\Zotero\\storage\\7CNBZIFX\\Marques et al. - 2018 - Structure of the Zebrafish Locomotor Repertoire Re.pdf:application/pdf;Snapshot:C\:\\Users\\Angel\\Zotero\\storage\\5GFU8ZC7\\S0960-9822(17)31604-4.html:text/html},
}

@article{clift_high-throughput_2014,
	title = {High-{Throughput} {Analysis} of {Behavior} in {Zebrafish} {Larvae}: {Effects} of {Feeding}},
	volume = {11},
	issn = {1545-8547},
	shorttitle = {High-{Throughput} {Analysis} of {Behavior} in {Zebrafish} {Larvae}},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4172468/},
	doi = {10.1089/zeb.2014.0989},
	abstract = {Early brain development can be influenced by numerous genetic and environmental factors, with long-lasting effects on brain function and behavior. Identification of these factors is facilitated by high-throughput analyses of behavior in zebrafish larvae, which can be imaged in multiwell or multilane plates. However, the nutritional needs of zebrafish larvae during the behavioral experiments are not fully understood. Zebrafish larvae begin feeding between 4 and 5 days postfertilization (dpf), but can live solely on nutrients derived from the yolk until at least 7 dpf. To examine whether feeding affects behavior, we measured a broad range of behaviors with and without feeding at 5, 6, and 7 dpf. We found that feeding did not have a significant effect on behavior in 5-day-old larvae. In contrast, fed 6- and 7-day-old larvae displayed increased avoidance responses to visual stimuli, increased swim speeds, and decreased resting in comparison to unfed larvae. In addition, the fed 7-day-old larvae displayed a decrease in thigmotaxis and a decrease in the distance between larvae in the presence of visual stimuli. Thus, feeding affects a range of behaviors in 6- and 7-day-old larvae. We conclude that 5-day-old larvae are well-suited for high-throughput analyses of behavior, since effects of feeding can be avoided at this time. For high-throughput analyses of behavior in older larvae, standard feeding protocols need to be developed.},
	number = {5},
	urldate = {2022-10-10},
	journal = {Zebrafish},
	author = {Clift, Danielle and Richendrfer, Holly and Thorn, Robert J. and Colwill, Ruth M. and Creton, Robbert},
	month = oct,
	year = {2014},
	pmid = {25153037},
	pmcid = {PMC4172468},
	pages = {455--461},
	file = {PubMed Central Full Text PDF:C\:\\Users\\Angel\\Zotero\\storage\\L3A9WKMM\\Clift et al. - 2014 - High-Throughput Analysis of Behavior in Zebrafish .pdf:application/pdf},
}

@article{zhang_alternative_2018,
	title = {Alternative empirical {Bayes} models for adjusting for batch effects in genomic studies},
	volume = {19},
	issn = {1471-2105},
	url = {https://doi.org/10.1186/s12859-018-2263-6},
	doi = {10.1186/s12859-018-2263-6},
	abstract = {Combining genomic data sets from multiple studies is advantageous to increase statistical power in studies where logistical considerations restrict sample size or require the sequential generation of data. However, significant technical heterogeneity is commonly observed across multiple batches of data that are generated from different processing or reagent batches, experimenters, protocols, or profiling platforms. These so-called batch effects often confound true biological relationships in the data, reducing the power benefits of combining multiple batches, and may even lead to spurious results in some combined studies. Therefore there is significant need for effective methods and software tools that account for batch effects in high-throughput genomic studies.},
	number = {1},
	urldate = {2022-10-12},
	journal = {BMC Bioinformatics},
	author = {Zhang, Yuqing and Jenkins, David F. and Manimaran, Solaiappan and Johnson, W. Evan},
	month = jul,
	year = {2018},
	keywords = {Batch effects, Biomarker development, Data integration, Empirical Bayes models},
	pages = {262},
	file = {Full Text PDF:C\:\\Users\\Angel\\Zotero\\storage\\B4IZIKCW\\Zhang et al. - 2018 - Alternative empirical Bayes models for adjusting f.pdf:application/pdf;Snapshot:C\:\\Users\\Angel\\Zotero\\storage\\DXMEDMUM\\s12859-018-2263-6.html:text/html},
}

@article{yang_zebrafish_2021-1,
	title = {Zebrafish behavior feature recognition using three-dimensional tracking and machine learning},
	volume = {11},
	issn = {2045-2322},
	doi = {10.1038/s41598-021-92854-0},
	abstract = {In this work, we aim to construct a new behavior analysis method by using machine learning. We used two cameras to capture three-dimensional (3D) tracking data of zebrafish, which were analyzed using fuzzy adaptive resonance theory (FuzzyART), a type of machine learning algorithm, to identify specific behavioral features. The method was tested based on an experiment in which electric shocks were delivered to zebrafish and zebrafish swimming was tracked in 3D simultaneously to find electric shock-associated behaviors. By processing the obtained data with FuzzyART, we discovered that distinguishing behaviors were statistically linked to the electric shock based on the machine learning algorithm. Moreover, our system could accept user-supplied data for detection and quantitative analysis of the behavior features, such as the behavior features defined by the 3D tracking analysis above. This system could be applied to discover new distinct behavior features in mutant zebrafish and used for drug administration screening and cognitive ability tests of zebrafish in the future.},
	language = {eng},
	number = {1},
	journal = {Scientific Reports},
	author = {Yang, Peng and Takahashi, Hiro and Murase, Masataka and Itoh, Motoyuki},
	month = jun,
	year = {2021},
	pmid = {34188116},
	pmcid = {PMC8242018},
	keywords = {Animals, Zebrafish, Behavior, Animal, Machine Learning, Video Recording},
	pages = {13492},
	file = {Full Text:C\:\\Users\\Angel\\Zotero\\storage\\2RUX9E2Z\\Yang et al. - 2021 - Zebrafish behavior feature recognition using three.pdf:application/pdf},
}

@article{liu_statistical_2017,
	title = {Statistical {Analysis} of {Zebrafish} {Locomotor} {Behaviour} by {Generalized} {Linear} {Mixed} {Models}},
	volume = {7},
	copyright = {2017 The Author(s)},
	issn = {2045-2322},
	url = {https://www.nature.com/articles/s41598-017-02822-w},
	doi = {10.1038/s41598-017-02822-w},
	abstract = {Upon a drastic change in environmental illumination, zebrafish larvae display a rapid locomotor response. This response can be simultaneously tracked from larvae arranged in multi-well plates. The resulting data have provided new insights into neuro-behaviour. The features of these data, however, present a challenge to traditional statistical tests. For example, many larvae display little or no movement. Thus, the larval responses have many zero values and are imbalanced. These responses are also measured repeatedly from the same well, which results in correlated observations. These analytical issues were addressed in this study by the generalized linear mixed model (GLMM). This approach deals with binary responses and characterizes the correlation of observations in the same group. It was used to analyze a previously reported dataset. Before applying the GLMM, the activity values were transformed to binary responses (movement vs. no movement) to reduce data imbalance. Moreover, the GLMM estimated the variations among the effects of different well locations, which would eliminate the location effects when two biological groups or conditions were compared. By addressing the data-imbalance and location-correlation issues, the GLMM effectively quantified true biological effects on zebrafish locomotor response.},
	language = {en},
	number = {1},
	urldate = {2022-10-21},
	journal = {Scientific Reports},
	author = {Liu, Yiwen and Ma, Ping and Cassidy, Paige A. and Carmer, Robert and Zhang, Gaonan and Venkatraman, Prahatha and Brown, Skye A. and Pang, Chi Pui and Zhong, Wenxuan and Zhang, Mingzhi and Leung, Yuk Fai},
	month = jun,
	year = {2017},
	note = {Number: 1
Publisher: Nature Publishing Group},
	keywords = {Retina, Visual system},
	pages = {2937},
	file = {Full Text PDF:C\:\\Users\\Angel\\Zotero\\storage\\FUJCHW3R\\Liu et al. - 2017 - Statistical Analysis of Zebrafish Locomotor Behavi.pdf:application/pdf;Snapshot:C\:\\Users\\Angel\\Zotero\\storage\\JITY39YP\\s41598-017-02822-w.html:text/html},
}

@article{ying_overview_2019,
	title = {An {Overview} of {Overfitting} and its {Solutions}},
	volume = {1168},
	issn = {1742-6596},
	url = {https://dx.doi.org/10.1088/1742-6596/1168/2/022022},
	doi = {10.1088/1742-6596/1168/2/022022},
	abstract = {Overfitting is a fundamental issue in supervised machine learning which prevents us from perfectly generalizing the models to well fit observed data on training data, as well as unseen data on testing set. Because of the presence of noise, the limited size of training set, and the complexity of classifiers, overfitting happens. This paper is going to talk about overfitting from the perspectives of causes and solutions. To reduce the effects of overfitting, various strategies are proposed to address to these causes: 1) “early-stopping” strategy is introduced to prevent overfitting by stopping training before the performance stops optimize; 2) “network-reduction” strategy is used to exclude the noises in training set; 3) “data-expansion” strategy is proposed for complicated models to fine-tune the hyper-parameters sets with a great amount of data; and 4) “regularization” strategy is proposed to guarantee models performance to a great extent while dealing with real world issues by feature-selection, and by distinguishing more useful and less useful features.},
	language = {en},
	number = {2},
	urldate = {2022-10-23},
	journal = {Journal of Physics: Conference Series},
	author = {Ying, Xue},
	month = feb,
	year = {2019},
	note = {Publisher: IOP Publishing},
	pages = {022022},
	file = {IOP Full Text PDF:C\:\\Users\\Angel\\Zotero\\storage\\DPYFTQ7A\\Ying - 2019 - An Overview of Overfitting and its Solutions.pdf:application/pdf},
}

@article{xia_aquatic_2018,
	title = {Aquatic {Toxic} {Analysis} by {Monitoring} {Fish} {Behavior} {Using} {Computer} {Vision}: {A} {Recent} {Progress}},
	volume = {2018},
	issn = {1687-8191},
	shorttitle = {Aquatic {Toxic} {Analysis} by {Monitoring} {Fish} {Behavior} {Using} {Computer} {Vision}},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5903295/},
	doi = {10.1155/2018/2591924},
	abstract = {Video tracking based biological early warning system achieved a great progress with advanced computer vision and machine learning methods. Ability of video tracking of multiple biological organisms has been largely improved in recent years. Video based behavioral monitoring has become a common tool for acquiring quantified behavioral data for aquatic risk assessment. Investigation of behavioral responses under chemical and environmental stress has been boosted by rapidly developed machine learning and artificial intelligence. In this paper, we introduce the fundamental of video tracking and present the pioneer works in precise tracking of a group of individuals in 2D and 3D space. Technical and practical issues suffered in video tracking are explained. Subsequently, the toxic analysis based on fish behavioral data is summarized. Frequently used computational methods and machine learning are explained with their applications in aquatic toxicity detection and abnormal pattern analysis. Finally, advantages of recent developed deep learning approach in toxic prediction are presented.},
	urldate = {2022-10-29},
	journal = {Journal of Toxicology},
	author = {Xia, Chunlei and Fu, Longwen and Liu, Zuoyi and Liu, Hui and Chen, Lingxin and Liu, Yuedan},
	month = apr,
	year = {2018},
	pmid = {29849612},
	pmcid = {PMC5903295},
	pages = {2591924},
	file = {PubMed Central Full Text PDF:C\:\\Users\\Angel\\Zotero\\storage\\QHFJV6QN\\Xia et al. - 2018 - Aquatic Toxic Analysis by Monitoring Fish Behavior.pdf:application/pdf},
}

@misc{eldan_power_2016,
	title = {The {Power} of {Depth} for {Feedforward} {Neural} {Networks}},
	url = {http://arxiv.org/abs/1512.03965},
	abstract = {We show that there is a simple (approximately radial) function on \${\textbackslash}reals{\textasciicircum}d\$, expressible by a small 3-layer feedforward neural networks, which cannot be approximated by any 2-layer network, to more than a certain constant accuracy, unless its width is exponential in the dimension. The result holds for virtually all known activation functions, including rectified linear units, sigmoids and thresholds, and formally demonstrates that depth -- even if increased by 1 -- can be exponentially more valuable than width for standard feedforward neural networks. Moreover, compared to related results in the context of Boolean functions, our result requires fewer assumptions, and the proof techniques and construction are very different.},
	urldate = {2022-10-30},
	publisher = {arXiv},
	author = {Eldan, Ronen and Shamir, Ohad},
	month = may,
	year = {2016},
	note = {arXiv:1512.03965 [cs, stat]
version: 4},
	keywords = {Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:C\:\\Users\\Angel\\Zotero\\storage\\4QFH7MGU\\Eldan and Shamir - 2016 - The Power of Depth for Feedforward Neural Networks.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Angel\\Zotero\\storage\\SEPLUP4H\\1512.html:text/html},
}

@misc{zou_stochastic_2018,
	title = {Stochastic {Gradient} {Descent} {Optimizes} {Over}-parameterized {Deep} {ReLU} {Networks}},
	url = {http://arxiv.org/abs/1811.08888},
	doi = {10.48550/arXiv.1811.08888},
	abstract = {We study the problem of training deep neural networks with Rectified Linear Unit (ReLU) activation function using gradient descent and stochastic gradient descent. In particular, we study the binary classification problem and show that for a broad family of loss functions, with proper random weight initialization, both gradient descent and stochastic gradient descent can find the global minima of the training loss for an over-parameterized deep ReLU network, under mild assumption on the training data. The key idea of our proof is that Gaussian random initialization followed by (stochastic) gradient descent produces a sequence of iterates that stay inside a small perturbation region centering around the initial weights, in which the empirical loss function of deep ReLU networks enjoys nice local curvature properties that ensure the global convergence of (stochastic) gradient descent. Our theoretical results shed light on understanding the optimization for deep learning, and pave the way for studying the optimization dynamics of training modern deep neural networks.},
	urldate = {2022-10-30},
	publisher = {arXiv},
	author = {Zou, Difan and Cao, Yuan and Zhou, Dongruo and Gu, Quanquan},
	month = dec,
	year = {2018},
	note = {arXiv:1811.08888 [cs, math, stat]},
	keywords = {Computer Science - Machine Learning, Mathematics - Optimization and Control, Statistics - Machine Learning, Computer Science - Artificial Intelligence},
	file = {arXiv Fulltext PDF:C\:\\Users\\Angel\\Zotero\\storage\\K5BDYXLB\\Zou et al. - 2018 - Stochastic Gradient Descent Optimizes Over-paramet.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Angel\\Zotero\\storage\\FAUFYK5H\\1811.html:text/html},
}

@misc{liu_accelerating_2019,
	title = {Accelerating {SGD} with momentum for over-parameterized learning},
	url = {http://arxiv.org/abs/1810.13395},
	doi = {10.48550/arXiv.1810.13395},
	abstract = {Nesterov SGD is widely used for training modern neural networks and other machine learning models. Yet, its advantages over SGD have not been theoretically clarified. Indeed, as we show in our paper, both theoretically and empirically, Nesterov SGD with any parameter selection does not in general provide acceleration over ordinary SGD. Furthermore, Nesterov SGD may diverge for step sizes that ensure convergence of ordinary SGD. This is in contrast to the classical results in the deterministic scenario, where the same step size ensures accelerated convergence of the Nesterov's method over optimal gradient descent. To address the non-acceleration issue, we introduce a compensation term to Nesterov SGD. The resulting algorithm, which we call MaSS, converges for same step sizes as SGD. We prove that MaSS obtains an accelerated convergence rates over SGD for any mini-batch size in the linear setting. For full batch, the convergence rate of MaSS matches the well-known accelerated rate of the Nesterov's method. We also analyze the practically important question of the dependence of the convergence rate and optimal hyper-parameters on the mini-batch size, demonstrating three distinct regimes: linear scaling, diminishing returns and saturation. Experimental evaluation of MaSS for several standard architectures of deep networks, including ResNet and convolutional networks, shows improved performance over SGD, Nesterov SGD and Adam.},
	urldate = {2022-10-30},
	publisher = {arXiv},
	author = {Liu, Chaoyue and Belkin, Mikhail},
	month = sep,
	year = {2019},
	note = {arXiv:1810.13395 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:C\:\\Users\\Angel\\Zotero\\storage\\PHW6KXDB\\Liu and Belkin - 2019 - Accelerating SGD with momentum for over-parameteri.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Angel\\Zotero\\storage\\KT5HC62T\\1810.html:text/html},
}

@article{kandel_effect_2020,
	title = {The effect of batch size on the generalizability of the convolutional neural networks on a histopathology dataset},
	volume = {6},
	issn = {2405-9595},
	url = {https://www.sciencedirect.com/science/article/pii/S2405959519303455},
	doi = {10.1016/j.icte.2020.04.010},
	abstract = {Many hyperparameters have to be tuned to have a robust convolutional neural network that will be able to accurately classify images. One of the most important hyperparameters is the batch size, which is the number of images used to train a single forward and backward pass. In this study, the effect of batch size on the performance of convolutional neural networks and the impact of learning rates will be studied for image classification, specifically for medical images. To train the network faster, a VGG16 network with ImageNet weights was used in this experiment. Our results concluded that a higher batch size does not usually achieve high accuracy, and the learning rate and the optimizer used will have a significant impact as well. Lowering the learning rate and decreasing the batch size will allow the network to train better, especially in the case of fine-tuning.},
	language = {en},
	number = {4},
	urldate = {2022-10-30},
	journal = {ICT Express},
	author = {Kandel, Ibrahem and Castelli, Mauro},
	month = dec,
	year = {2020},
	keywords = {Batch size, Convolutional neural networks, Deep learning, Image classification, Medical images},
	pages = {312--315},
	file = {ScienceDirect Full Text PDF:C\:\\Users\\Angel\\Zotero\\storage\\87GSJV9P\\Kandel and Castelli - 2020 - The effect of batch size on the generalizability o.pdf:application/pdf;ScienceDirect Snapshot:C\:\\Users\\Angel\\Zotero\\storage\\P7GQQSRB\\S2405959519303455.html:text/html},
}

@inproceedings{sutskever_importance_2013,
	title = {On the importance of initialization and momentum in deep learning},
	url = {https://proceedings.mlr.press/v28/sutskever13.html},
	abstract = {Deep and recurrent neural networks (DNNs and RNNs respectively) are powerful models that were considered to be almost impossible to train using stochastic gradient descent with momentum. In this paper, we show that when stochastic gradient descent with momentum uses a well-designed random initialization and a particular type of slowly increasing schedule for the momentum parameter, it can train both DNNs and RNNs (on datasets with long-term dependencies) to levels of performance that were previously achievable only with Hessian-Free optimization. We find that both the initialization and the momentum are crucial since poorly initialized networks cannot be trained with momentum and well-initialized networks perform markedly worse when the momentum is absent or poorly tuned.     Our success training these models suggests that previous attempts to train deep and recurrent neural networks from random initializations have likely failed due to poor initialization schemes. Furthermore, carefully tuned momentum methods suffice for dealing with the curvature issues in deep and recurrent network training objectives without the need for sophisticated second-order methods.},
	language = {en},
	urldate = {2022-10-30},
	booktitle = {Proceedings of the 30th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Sutskever, Ilya and Martens, James and Dahl, George and Hinton, Geoffrey},
	month = may,
	year = {2013},
	note = {ISSN: 1938-7228},
	pages = {1139--1147},
	file = {Full Text PDF:C\:\\Users\\Angel\\Zotero\\storage\\F6GLZBPP\\Sutskever et al. - 2013 - On the importance of initialization and momentum i.pdf:application/pdf},
}

@misc{ancona_towards_2018,
	title = {Towards better understanding of gradient-based attribution methods for {Deep} {Neural} {Networks}},
	url = {http://arxiv.org/abs/1711.06104},
	abstract = {Understanding the flow of information in Deep Neural Networks (DNNs) is a challenging problem that has gain increasing attention over the last few years. While several methods have been proposed to explain network predictions, there have been only a few attempts to compare them from a theoretical perspective. What is more, no exhaustive empirical comparison has been performed in the past. In this work, we analyze four gradient-based attribution methods and formally prove conditions of equivalence and approximation between them. By reformulating two of these methods, we construct a unified framework which enables a direct comparison, as well as an easier implementation. Finally, we propose a novel evaluation metric, called Sensitivity-n and test the gradient-based attribution methods alongside with a simple perturbation-based attribution method on several datasets in the domains of image and text classification, using various network architectures.},
	urldate = {2022-10-30},
	publisher = {arXiv},
	author = {Ancona, Marco and Ceolini, Enea and Öztireli, Cengiz and Gross, Markus},
	month = mar,
	year = {2018},
	note = {arXiv:1711.06104 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:C\:\\Users\\Angel\\Zotero\\storage\\IH2BU9VR\\Ancona et al. - 2018 - Towards better understanding of gradient-based att.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Angel\\Zotero\\storage\\6ARXBZJB\\1711.html:text/html},
}

@article{yang_artificial_2022,
	title = {Artificial intelligence-enabled detection and assessment of {Parkinson}’s disease using nocturnal breathing signals},
	volume = {28},
	copyright = {2022 The Author(s)},
	issn = {1546-170X},
	url = {https://www.nature.com/articles/s41591-022-01932-x},
	doi = {10.1038/s41591-022-01932-x},
	abstract = {There are currently no effective biomarkers for diagnosing Parkinson’s disease (PD) or tracking its progression. Here, we developed an artificial intelligence (AI) model to detect PD and track its progression from nocturnal breathing signals. The model was evaluated on a large dataset comprising 7,671 individuals, using data from several hospitals in the United States, as well as multiple public datasets. The AI model can detect PD with an area-under-the-curve of 0.90 and 0.85 on held-out and external test sets, respectively. The AI model can also estimate PD severity and progression in accordance with the Movement Disorder Society Unified Parkinson’s Disease Rating Scale (R = 0.94, P = 3.6 × 10–25). The AI model uses an attention layer that allows for interpreting its predictions with respect to sleep and electroencephalogram. Moreover, the model can assess PD in the home setting in a touchless manner, by extracting breathing from radio waves that bounce off a person’s body during sleep. Our study demonstrates the feasibility of objective, noninvasive, at-home assessment of PD, and also provides initial evidence that this AI model may be useful for risk assessment before clinical diagnosis.},
	language = {en},
	number = {10},
	urldate = {2022-10-30},
	journal = {Nature Medicine},
	author = {Yang, Yuzhe and Yuan, Yuan and Zhang, Guo and Wang, Hao and Chen, Ying-Cong and Liu, Yingcheng and Tarolli, Christopher G. and Crepeau, Daniel and Bukartyk, Jan and Junna, Mithri R. and Videnovic, Aleksandar and Ellis, Terry D. and Lipford, Melissa C. and Dorsey, Ray and Katabi, Dina},
	month = oct,
	year = {2022},
	note = {Number: 10
Publisher: Nature Publishing Group},
	keywords = {Parkinson's disease, Biomarkers},
	pages = {2207--2215},
	file = {Full Text PDF:C\:\\Users\\Angel\\Zotero\\storage\\49GQ4NFP\\Yang et al. - 2022 - Artificial intelligence-enabled detection and asse.pdf:application/pdf;Snapshot:C\:\\Users\\Angel\\Zotero\\storage\\JQM7J3NR\\s41591-022-01932-x.html:text/html},
}

@inproceedings{glorot_understanding_2010,
	title = {Understanding the difficulty of training deep feedforward neural networks},
	url = {https://proceedings.mlr.press/v9/glorot10a.html},
	abstract = {Whereas before 2006 it appears that deep multi-layer neural networks were not successfully trained, since then several algorithms have been shown to successfully train them, with experimental results showing the superiority of deeper vs less deep architectures. All these experimental results were obtained with new initialization or training mechanisms. Our objective here is to understand better why standard gradient descent from random initialization is doing so poorly with deep neural networks, to better understand these recent relative successes and help design better algorithms in the future.  We first observe the influence of the non-linear activations functions. We find that the logistic sigmoid activation is unsuited for deep networks with random initialization because of its mean value, which can drive especially the top hidden layer into saturation. Surprisingly, we find that saturated units can move out of saturation by themselves, albeit slowly, and explaining the plateaus sometimes seen when training neural networks. We find that a new non-linearity that saturates less can often be beneficial. Finally, we study how activations and gradients vary across layers and during training, with the idea that training may be more difficult when the singular values of the Jacobian associated with each layer are far from 1.  Based on these considerations, we propose a new initialization scheme that brings substantially faster convergence.},
	language = {en},
	urldate = {2022-10-30},
	booktitle = {Proceedings of the {Thirteenth} {International} {Conference} on {Artificial} {Intelligence} and {Statistics}},
	publisher = {JMLR Workshop and Conference Proceedings},
	author = {Glorot, Xavier and Bengio, Yoshua},
	month = mar,
	year = {2010},
	note = {ISSN: 1938-7228},
	pages = {249--256},
	file = {Full Text PDF:C\:\\Users\\Angel\\Zotero\\storage\\4546ET5P\\Glorot and Bengio - 2010 - Understanding the difficulty of training deep feed.pdf:application/pdf},
}

@misc{boulila_weight_2021,
	title = {Weight {Initialization} {Techniques} for {Deep} {Learning} {Algorithms} in {Remote} {Sensing}: {Recent} {Trends} and {Future} {Perspectives}},
	shorttitle = {Weight {Initialization} {Techniques} for {Deep} {Learning} {Algorithms} in {Remote} {Sensing}},
	url = {http://arxiv.org/abs/2102.07004},
	abstract = {During the last decade, several research works have focused on providing novel deep learning methods in many application fields. However, few of them have investigated the weight initialization process for deep learning, although its importance is revealed in improving deep learning performance. This can be justified by the technical difficulties in proposing new techniques for this promising research field. In this paper, a survey related to weight initialization techniques for deep algorithms in remote sensing is conducted. This survey will help practitioners to drive further research in this promising field. To the best of our knowledge, this paper constitutes the first survey focusing on weight initialization for deep learning models.},
	urldate = {2022-10-30},
	publisher = {arXiv},
	author = {Boulila, Wadii and Driss, Maha and Al-Sarem, Mohamed and Saeed, Faisal and Krichen, Moez},
	month = feb,
	year = {2021},
	note = {arXiv:2102.07004 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence},
	file = {arXiv Fulltext PDF:C\:\\Users\\Angel\\Zotero\\storage\\DF7UIQMA\\Boulila et al. - 2021 - Weight Initialization Techniques for Deep Learning.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Angel\\Zotero\\storage\\IGVIKCRF\\2102.html:text/html},
}

@inproceedings{he_control_2019,
	title = {Control {Batch} {Size} and {Learning} {Rate} to {Generalize} {Well}: {Theoretical} and {Empirical} {Evidence}},
	volume = {32},
	shorttitle = {Control {Batch} {Size} and {Learning} {Rate} to {Generalize} {Well}},
	url = {https://proceedings.neurips.cc/paper/2019/hash/dc6a70712a252123c40d2adba6a11d84-Abstract.html},
	urldate = {2022-10-30},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {He, Fengxiang and Liu, Tongliang and Tao, Dacheng},
	year = {2019},
	file = {Full Text PDF:C\:\\Users\\Angel\\Zotero\\storage\\RP9DENPB\\He et al. - 2019 - Control Batch Size and Learning Rate to Generalize.pdf:application/pdf},
}

@article{srivastava_dropout_nodate,
	title = {Dropout: {A} {Simple} {Way} to {Prevent} {Neural} {Networks} from {Overﬁtting}},
	abstract = {Deep neural nets with a large number of parameters are very powerful machine learning systems. However, overﬁtting is a serious problem in such networks. Large networks are also slow to use, making it diﬃcult to deal with overﬁtting by combining the predictions of many diﬀerent large neural nets at test time. Dropout is a technique for addressing this problem. The key idea is to randomly drop units (along with their connections) from the neural network during training. This prevents units from co-adapting too much. During training, dropout samples from an exponential number of diﬀerent “thinned” networks. At test time, it is easy to approximate the eﬀect of averaging the predictions of all these thinned networks by simply using a single unthinned network that has smaller weights. This signiﬁcantly reduces overﬁtting and gives major improvements over other regularization methods. We show that dropout improves the performance of neural networks on supervised learning tasks in vision, speech recognition, document classiﬁcation and computational biology, obtaining state-of-the-art results on many benchmark data sets.},
	language = {en},
	author = {Srivastava, Nitish and Hinton, Geoﬀrey and Krizhevsky, Alex and Sutskever, Ilya and Salakhutdinov, Ruslan},
	pages = {30},
	file = {Srivastava et al. - Dropout A Simple Way to Prevent Neural Networks f.pdf:C\:\\Users\\Angel\\Zotero\\storage\\X5Y8LXWK\\Srivastava et al. - Dropout A Simple Way to Prevent Neural Networks f.pdf:application/pdf},
}

@misc{labach_survey_2019,
	title = {Survey of {Dropout} {Methods} for {Deep} {Neural} {Networks}},
	url = {http://arxiv.org/abs/1904.13310},
	doi = {10.48550/arXiv.1904.13310},
	abstract = {Dropout methods are a family of stochastic techniques used in neural network training or inference that have generated significant research interest and are widely used in practice. They have been successfully applied in neural network regularization, model compression, and in measuring the uncertainty of neural network outputs. While original formulated for dense neural network layers, recent advances have made dropout methods also applicable to convolutional and recurrent neural network layers. This paper summarizes the history of dropout methods, their various applications, and current areas of research interest. Important proposed methods are described in additional detail.},
	urldate = {2022-10-30},
	publisher = {arXiv},
	author = {Labach, Alex and Salehinejad, Hojjat and Valaee, Shahrokh},
	month = oct,
	year = {2019},
	note = {arXiv:1904.13310 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Computer Science - Artificial Intelligence},
	file = {arXiv Fulltext PDF:C\:\\Users\\Angel\\Zotero\\storage\\7XZKHRBY\\Labach et al. - 2019 - Survey of Dropout Methods for Deep Neural Networks.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Angel\\Zotero\\storage\\IKW3TEKN\\1904.html:text/html},
}

@article{robbins_stochastic_1951,
	title = {A {Stochastic} {Approximation} {Method}},
	volume = {22},
	issn = {0003-4851, 2168-8990},
	url = {https://projecteuclid.org/journals/annals-of-mathematical-statistics/volume-22/issue-3/A-Stochastic-Approximation-Method/10.1214/aoms/1177729586.full},
	doi = {10.1214/aoms/1177729586},
	abstract = {Let \$M(x)\$ denote the expected value at level \$x\$ of the response to a certain experiment. \$M(x)\$ is assumed to be a monotone function of \$x\$ but is unknown to the experimenter, and it is desired to find the solution \$x = {\textbackslash}theta\$ of the equation \$M(x) = {\textbackslash}alpha\$, where \${\textbackslash}alpha\$ is a given constant. We give a method for making successive experiments at levels \$x\_1,x\_2,{\textbackslash}cdots\$ in such a way that \$x\_n\$ will tend to \${\textbackslash}theta\$ in probability.},
	number = {3},
	urldate = {2022-10-30},
	journal = {The Annals of Mathematical Statistics},
	author = {Robbins, Herbert and Monro, Sutton},
	month = sep,
	year = {1951},
	note = {Publisher: Institute of Mathematical Statistics},
	pages = {400--407},
	file = {Full Text PDF:C\:\\Users\\Angel\\Zotero\\storage\\YDQ4PYL7\\Robbins and Monro - 1951 - A Stochastic Approximation Method.pdf:application/pdf;Snapshot:C\:\\Users\\Angel\\Zotero\\storage\\5A4XBPK5\\1177729586.html:text/html},
}

@misc{he_delving_2015,
	title = {Delving {Deep} into {Rectifiers}: {Surpassing} {Human}-{Level} {Performance} on {ImageNet} {Classification}},
	shorttitle = {Delving {Deep} into {Rectifiers}},
	url = {http://arxiv.org/abs/1502.01852},
	doi = {10.48550/arXiv.1502.01852},
	abstract = {Rectified activation units (rectifiers) are essential for state-of-the-art neural networks. In this work, we study rectifier neural networks for image classification from two aspects. First, we propose a Parametric Rectified Linear Unit (PReLU) that generalizes the traditional rectified unit. PReLU improves model fitting with nearly zero extra computational cost and little overfitting risk. Second, we derive a robust initialization method that particularly considers the rectifier nonlinearities. This method enables us to train extremely deep rectified models directly from scratch and to investigate deeper or wider network architectures. Based on our PReLU networks (PReLU-nets), we achieve 4.94\% top-5 test error on the ImageNet 2012 classification dataset. This is a 26\% relative improvement over the ILSVRC 2014 winner (GoogLeNet, 6.66\%). To our knowledge, our result is the first to surpass human-level performance (5.1\%, Russakovsky et al.) on this visual recognition challenge.},
	urldate = {2022-11-01},
	publisher = {arXiv},
	author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
	month = feb,
	year = {2015},
	note = {arXiv:1502.01852 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Artificial Intelligence},
	file = {arXiv Fulltext PDF:C\:\\Users\\Angel\\Zotero\\storage\\L73UC64T\\He et al. - 2015 - Delving Deep into Rectifiers Surpassing Human-Lev.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Angel\\Zotero\\storage\\SQKNLKZF\\1502.html:text/html},
}

@article{newman_using_2014,
	title = {Using the zebrafish model for {Alzheimer}’s disease research},
	volume = {5},
	issn = {1664-8021},
	url = {https://www.frontiersin.org/articles/10.3389/fgene.2014.00189},
	abstract = {Rodent models have been extensively used to investigate the cause and mechanisms behind Alzheimer’s disease. Despite many years of intensive research using these models we still lack a detailed understanding of the molecular events that lead to neurodegeneration. Although zebrafish lack the complexity of advanced cognitive behaviors evident in rodent models they have proven to be a very informative model for the study of human diseases. In this review we give an overview of how the zebrafish has been used to study Alzheimer’s disease. Zebrafish possess genes orthologous to those mutated in familial Alzheimer’s disease and research using zebrafish has revealed unique characteristics of these genes that have been difficult to observe in rodent models. The zebrafish is becoming an increasingly popular model for the investigation of Alzheimer’s disease and will complement studies using other models to help complete our understanding of this disease.},
	urldate = {2022-10-29},
	journal = {Frontiers in Genetics},
	author = {Newman, Morgan and Ebrahimie, Esmaeil and Lardelli, Michael},
	year = {2014},
	file = {Full Text PDF:C\:\\Users\\Angel\\Zotero\\storage\\XZYYE22E\\Newman et al. - 2014 - Using the zebrafish model for Alzheimer’s disease .pdf:application/pdf},
}

@book{westerfield_zebrafish_2000,
	address = {Eugene},
	edition = {4},
	title = {The zebrafish book. {A} guide for the laboratory use of zebrafish ({Danio} rerio)},
	url = {https://zfin.org/zf_info/zfbook/zfbk.html},
	urldate = {2022-11-01},
	publisher = {University of Oregon Press},
	author = {Westerfield, Monte},
	year = {2000},
	file = {ZFIN\: Zebrafish Book\: Contents:C\:\\Users\\Angel\\Zotero\\storage\\C68S3TKU\\zfbk.html:text/html},
}
