\section{Results}
\label{sec:results}
Larvae showed an overall increase in movement following the light to dark transition, shown in Figure \ref{fig:raw_data}. There was a visible difference in the group mean for the total distance travelled during both the light and dark periods between wildtype and mutant larvae, indicating possible locomotor deficits in the mutants. 
% For the frequency moving (how many times a larva initiated movement, higher values indicate more stop/starts) there were differences between genotype in the light (normal conditions), but not in the dark (following a stress stimulus).

\begin{figure}[t]
    \begin{center}
    % \fbox{\rule{0pt}{2in} \rule{0.9\linewidth}{0pt}}
        \includegraphics[width=1\linewidth]{raw_data.png}
    \end{center}
        \caption{Total distance travelled (in mm) by larvae for each one minute time bin (total 15 minutes). The coloured panels indicate the light state, with a sudden light to dark transition at the 5 minute mark. Lines indicate the mean for each genotype over time.}
    \label{fig:raw_data}
    \end{figure}

During pre-processing of the data, $673$ values were considered outliers and omitted. The majority 

\subsection{Training and Inference}
Ten experiments were run with differing numbers of perceptrons in the hidden layers to allow comparison in terms of overfitting and selection of the best model. We performed each experiment on the same $85/15$ train/validation split so that results were easier to compare. Each experiment trained ten models of the same architecture and were evaluated on the validation set. Early stopping was used to select the best version of each model. 

The average train accuracy and validation accuracy of each experiment for analysis, shown in Table \ref{tab:results}. The architecture with 70 perceptrons in the hidden layers produced the best train accuracy. The 80-perceptron architecture had the best accuracy on the validation set and came a close second on the train set. We found that architectures with more than 30 perceptrons in their hidden layers were more prone to loss divergence but still achieved high train and validation accuracies. In comparison, the architectures with less than 30 perceptrons had a lower frequency of divergence but could not achieve the same accuracy. We theorise that this is due to the networks not having a sufficient number of parameters to fit the data well. In comparison, the losses of  architectures with more perceptrons were more likely to diverge, but were also more likely to achieve better generalisation.

% \begin{table}
%     \centering
%     \begin{tabular}{ |P{2cm}|P{2cm}|P{2cm}|  }
%         \hline
%         \multicolumn{3}{|c|}{\textbf{Experimental Results}} \\
%         \hline
%         \textbf{Perceptrons}&\textbf{Train \%}&\textbf{Validation \%}\\
%         \hline
%         10          &       82.24          &        78.86           \\
%         20          &       81.80          &        80.00           \\
%         30          &       85.30          &        82.72           \\
%         40          &       86.77          &        83.64           \\
%         50          &       87.18          &        82.95           \\
%         60          &       87.02          &        83.18           \\
%         70          &       \textbf{88.45} &        82.95           \\
%         80          &       88.41          &        \textbf{84.09}  \\
%         90          &       87.18          &        82.72           \\
%         100         &       87.67          &        83.41           \\
%         \hline
%     \end{tabular}\par
% \caption{Results of ten different experiments, each with a different number of perceptrons in each of their hidden layers. Each experiment trained ten models, where the average train \& validation accuracy of the models were taken and presented in the table.}
% \label{tab:results}
% \end{table}

\begin{table}[h]\centering
    \begin{tabular}{ccccc}
        \toprule
        \multirow{2}[3]{*}{\textbf{Perceptrons}} & \multicolumn{2}{c}{\textbf{Train (\%)}} & \multicolumn{2}{c}{\textbf{Validation (\%)}} \\
        \cmidrule(lr){2-3} \cmidrule(lr){4-5}
        & Mean & Stdev & Mean & Stdev \\
        \midrule
        10  & 84.0 & 4.40 & 81.1 & 2.96 \\
        20  & 81.8 & 4.55 & 80.0 & 4.39 \\
        30  & 85.3 & 2.50 & 82.7 & 3.42 \\
        40  & 86.8 & 3.28 & 83.6 & 1.44 \\
        50  & 87.2 & 2.86 & 83.0 & 1.93 \\
        60  & 87.0 & 3.03 & 83.2 & 2.44 \\
        70  & 88.4 & 2.29 & 83.0 & 2.45 \\
        80  & 88.4 & 2.00 & 84.1 & 1.52 \\
        90  & 87.2 & 2.84 & 82.7 & 2.20 \\
        100 & 87.7 & 2.12 & 83.4 & 1.53 \\
        \bottomrule
    \end{tabular}
    \caption{Train and validation accuracies (\%) for ten different experiments with increasing numbers of perceptrons in their hidden layers. Each experiment trained ten models, for which the mean and standard deviation of the train and validation accuracies were calculated.}
    \label{tab:results}
\end{table}

\subsection{Integrated Gradients}
The average integrated gradients from the ten models were calculated for each experiment. Each behavioural feature was given an attribution value representative of the impact it had on the classification outcome across the entire dataset. Positive attributions indicate that the feature increases the probability of the genotype being chosen, and vice versa. The average integrated gradients for all models with hidden layers of 80 perceptrons are visualised in Figure~\ref{fig:gradients}.

The mean distance travelled, mean velocity, velocity variance, time in middle variance, and total time moving have very small attribution scores indicating that the model is generally unsure how to make use of these features. Whereas we can see that the other features have larger attribution scores, indicating that the model depends on them for the label decision.

Overall, the behavioural features from the dark period (after a stressful stimulus) show higher attribution scores than those from the light period. This may be due to the overall increased movement seen in larvae following the light to dark transition (Figure~\ref{fig:raw_data}) providing more information with regards to their movement.

The velocity and time moving in the light have very small attribution scores indicating that the model is generally unsure how to make use of these features. This is also seen in the variance for the time spend in the middle in the dark. Many other features have larger attribution scores, indicating that the model depends on them for the label decision.

\begin{figure}[ht]
\begin{center}
% \fbox{\rule{0pt}{2in} \rule{0.9\linewidth}{0pt}}
    \includegraphics[width=1\linewidth]{gradients.png}
\end{center}
    \caption{Integrated gradients averaged over ten models with 80-perceptron hidden layers. Large attribution scores (in either direction) indicate a large influence of a feature on the output. Positive/negative attributions indicate that a feature increases/decreases the probability of a certain genotype being chosen. Standard deviation shown as error bars.}
\label{fig:gradients}
\end{figure}