\def\year{2022}\relax
%File: formatting-instructions-latex-2022.tex
%release 2022.1
\documentclass[letterpaper]{article} % DO NOT CHANGE THIS
\usepackage{aaai22}  % DO NOT CHANGE THIS
\nocopyright
\usepackage{times}  % DO NOT CHANGE THIS
\usepackage{helvet}  % DO NOT CHANGE THIS
\usepackage{courier}  % DO NOT CHANGE THIS
\usepackage[hyphens]{url}  % DO NOT CHANGE THIS
\usepackage{graphicx} % DO NOT CHANGE THIS
\urlstyle{rm} % DO NOT CHANGE THIS
\def\UrlFont{\rm}  % DO NOT CHANGE THIS
\usepackage{natbib}  % DO NOT CHANGE THIS AND DO NOT ADD ANY OPTIONS TO IT
\usepackage{caption} % DO NOT CHANGE THIS AND DO NOT ADD ANY OPTIONS TO IT
\DeclareCaptionStyle{ruled}{labelfont=normalfont,labelsep=colon,strut=off} % DO NOT CHANGE THIS
\frenchspacing  % DO NOT CHANGE THIS
\setlength{\pdfpagewidth}{8.5in}  % DO NOT CHANGE THIS
\setlength{\pdfpageheight}{11in}  % DO NOT CHANGE THIS
\usepackage{multirow}
\newcommand\x{x}
\newcommand\X{X}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{makecell} 
\usepackage{newfloat}
\usepackage{listings}
\lstset{%
	basicstyle={\footnotesize\ttfamily},% footnotesize acceptable for monospace
	numbers=left,numberstyle=\footnotesize,xleftmargin=2em,% show line numbers, remove this entire line if you don't want the numbers.
	aboveskip=0pt,belowskip=0pt,%
	showstringspaces=false,tabsize=2,breaklines=true}
\floatstyle{ruled}
\newfloat{listing}{tb}{lst}{}
\floatname{listing}{Listing}

\pdfinfo{
/Title (ViT-CX: Causal Explanation of Vision Transformers)
}



\setcounter{secnumdepth}{2} 


\title{ViT-CX: Causal Explanation of Vision Transformers}


\author{Weiyan Xie\textsuperscript{\rm 1}, Xiao-Hui Li\textsuperscript{\rm 2}, Caleb Chen Cao\textsuperscript{\rm 2}, Nevin L. Zhang\textsuperscript{\rm 1}}

\affiliations {
    \textsuperscript{\rm 1}  The Hong Kong University of Science and Technology \ 
    \textsuperscript{\rm 2} Huawei Technologies Co., Ltd \\
    \textsuperscript{\rm 1} \{wxieai,lzhang\}@cse.ust.hk  \ \textsuperscript{\rm 2} \{lixiaohui33,caleb.cao\}@huawei.com
}


\usepackage{bibentry}


\begin{document}

\maketitle




\begin{figure*}[h!]
	\centering
	\begin{tabular}{cccccc|cccccc}
		{\scriptsize Input} & & {\scriptsize CGW1} & {\scriptsize CGW2} & {\scriptsize TAM} & {\scriptsize ViT-CX}	& {\scriptsize Input} & & {\scriptsize CGW1} & {\scriptsize CGW2} & {\scriptsize TAM} & {\scriptsize ViT-CX}	\\
		\includegraphics[height=1.1cm,width=1.1cm]{goldfish2222/ILSVRC2012_val_00028713.JPEG} & &
		\includegraphics[height=1.cm,width=1.cm]{goldfish33/h_1.jpg} &
		\includegraphics[height=1.cm,width=1.cm]{goldfish33/h_2.jpg} &
		\includegraphics[height=1.cm,width=1.cm]{goldfish33/tam.jpg} &
		\includegraphics[height=1.cm,width=1.cm]{goldfish33/sc.jpg}&
		\includegraphics[height=1.1cm,width=1.1cm]{sc/dogsled.jpg} & &
		\includegraphics[height=1.cm,width=1.cm]{sc/hc_1_dogsled.jpg} &
		\includegraphics[height=1.cm,width=1.cm]{sc/hc_2_dogsled.jpg} &
		\includegraphics[height=1.cm,width=1.cm]{sc/tam_dogsled.jpg} &
		\includegraphics[height=1.cm,width=1.cm]{sc/sc_dogsled.jpg} \\
		{\scriptsize {\tt  goldfish}}	&	{\scriptsize Del$\downarrow$} &{\scriptsize 0.258} &{\scriptsize 0.355}&{\scriptsize 0.271}&{\scriptsize \textbf{0.202}}&{\scriptsize {\tt  dogsled}}&{\scriptsize Del$\downarrow$}&{\scriptsize 0.097}&{\scriptsize 0.147}&{\scriptsize 0.498}&{\scriptsize \textbf{0.078}}\\
		{\scriptsize $P=0.999$}	&	{\scriptsize Ins$\uparrow$} &{\scriptsize 0.829} &{\scriptsize 0.833}&{\scriptsize 0.866}&{\scriptsize \textbf{0.879}}&{\scriptsize  $P=0.991$}&{\scriptsize Ins $\uparrow$}&{\scriptsize 0.827}&{\scriptsize 0.820}&{\scriptsize 0.692}&{\scriptsize \textbf{0.884}}\\
		
	\includegraphics[height=1.1cm,width=1.1cm]{vine_snake/ILSVRC2012_val_00040048.JPEG}& &
		\includegraphics[height=1.cm,width=1.cm]{vine_snake/hc_1.jpg} &
		\includegraphics[height=1.cm,width=1.cm]{vine_snake/hc_2.jpg} &
		\includegraphics[height=1.cm,width=1.cm]{vine_snake/tam.jpg}&
		\includegraphics[height=1.cm,width=1.cm]{vine_snake/sc.jpg}&
		\includegraphics[height=1.1cm,width=1.1cm]{head_cabbage/head_cabbage.JPEG}& &
		\includegraphics[height=1.cm,width=1.cm]{head_cabbage/hc_1.jpg} &
		\includegraphics[height=1.cm,width=1.cm]{head_cabbage/hc_2.jpg} &
		\includegraphics[height=1.cm,width=1.cm]{head_cabbage/tam.jpg}&
		\includegraphics[height=1.cm,width=1.cm]{head_cabbage/sc5.jpg} \\
		{\scriptsize {\tt  Vine Snake}}	&	{\scriptsize Del$\downarrow$} &{\scriptsize 0.164} &{\scriptsize 0.122}&{\scriptsize 0.114}&{\scriptsize \textbf{0.106} }&{\scriptsize {\tt \scriptsize {Head Cabbage}}}&{\scriptsize Del$\downarrow$}&{\scriptsize 0.506}&{\scriptsize 0.598}&{\scriptsize 0.373}&{\scriptsize  \textbf{0.351}}\\
		{\scriptsize $P=0.976$}&	{\scriptsize Ins$\uparrow$} &{\scriptsize 0.410} &{\scriptsize 0.544}&{\scriptsize 0.337}&{\scriptsize \textbf{0.603}}&{\scriptsize  $P=0.999$}&{\scriptsize Ins$\uparrow$}&{\scriptsize 0.798}&{\scriptsize 0.780}&{\scriptsize 0.801}&{\scriptsize \textbf{0.848}}\\
	\end{tabular}
	\caption{Explaining the predictions of ViT-B/16 on four images: The saliency maps produced by  ViT-CX are clearly more meaningful than those  by previous methods, as they highlight all the regions that are apparently important to predictions.  They are also more faithful to the model as measured by the deletion (Del) and insertion (Ins) AUC metrics.}
	\label{score_cam_vit11}
	%\label{fig.pcb.rise}
\end{figure*}



\begin{abstract}


Despite the popularity of Vision Transformers (ViTs) and eXplainable AI (XAI), only a few explanation methods have been proposed for ViTs thus far. They use attention weights of the classification token on patch embeddings and often produce unsatisfactory saliency maps. In this paper, we propose a novel method for explaining ViTs called  {\em ViT-CX}. It is  based on patch embeddings, rather than attentions paid to them, and their causal impacts on the model output. ViT-CX can be used to explain different ViT models. Empirical results show that, in comparison with previous methods, ViT-CX produces more meaningful  saliency maps  and does a better job at  revealing all the important evidence for prediction. It is also significantly more faithful to the model as measured by deletion AUC and insertion AUC. %Our code is available on GitHub [URL withheld temporarily for anonymity]. 



\end{abstract}




\section{Introduction}
%-------------------------------------------------------------


\begin{figure}[t!]
	\centering
	%\includegraphics[height=1.7cm]{fig000.png}
	\begin{tabular}{cc}
		
		\includegraphics[height=1cm,width=1cm]{goldfish2222/ILSVRC2012_val_00028713.JPEG}
			\begin{tabular}{ccccc}
			\includegraphics[height=0.95cm]{goldfish2222/3.png}&
		\includegraphics[height=0.95cm]{goldfish2222/0.png}&
		\includegraphics[height=0.95cm]{goldfish2222/11.png}&
		\includegraphics[height=0.95cm]{goldfish2222/6.png}&
		\includegraphics[height=0.95cm]{goldfish2222/5.png}\\
		%\includegraphics[height=0.95cm]{goldfish2222/9.png} \\
		{\scriptsize (a.1)} & {\scriptsize (a.2)} &	{\scriptsize (a.3)} & {\scriptsize (a.4)} & {\scriptsize (a.5)}  \\
		\includegraphics[height=0.95cm]{goldfish33/437_f1.jpg}&
		\includegraphics[height=0.95cm]{goldfish33/83_f1.jpg}&
		\includegraphics[height=0.95cm]{goldfish33/671_f1.jpg} &
		\includegraphics[height=0.95cm]{goldfish33/740_f1.jpg} &
		\includegraphics[height=0.95cm]{goldfish33/247_f1.jpg}  \\
		{\scriptsize (b.1)} & {\scriptsize (b.2)} & {\scriptsize (b.3)} & {\scriptsize (b.4)} & 	{\scriptsize (b.5)} \\
		\includegraphics[height=0.95cm]{goldfish33/437_m1.jpg}&
		\includegraphics[height=0.95cm]{goldfish33/83_m1.jpg}&
		\includegraphics[height=0.95cm]{goldfish33/671_m1.jpg} &
		\includegraphics[height=0.95cm]{goldfish33/740_m1.jpg} &
		\includegraphics[height=0.95cm]{goldfish33/247_m1.jpg}\\
	  	
			{\scriptsize (c.1)} & {\scriptsize (c.2)} & {\scriptsize (c.3)} & {\scriptsize (c.4)} &	{\scriptsize (c.5)}
		\end{tabular}
	\end{tabular}

	\caption{ViT feature maps (b.1 - b.5) are frontal slides of a 3D tensor made up of patch embedding vectors (as fibers). They are generally more
meaningful than attention weight maps (a.1 - a.5), and they are used in ViT-CX as masks to create masked images (c.1 - c.5).
}
	\label{vit1}
	\label{graph3}
	%\label{fig.pcb.rise}
\end{figure}



The necessity of explaining the predictions by deep neural networks (DNNs) is now widely recognized \cite{miller2019explanation,gunning2019darpa,rebuffi2020there}. To practitioners, DNNs are black boxes whose inner workings are difficult to comprehend. Explaining why DNNs make specific predictions can help build user trust and ensure fairness.

Vision Transformers (ViTs) are a new class of deep learning models that rival or even surpass the performance of convolutional neural networks (CNNs) on various vision tasks \cite{dosovitskiy2020image,carion2020end,liu2021swin}. A few methods for explaining ViTs have been proposed,
e.g., CGW1 \cite{chefer2021transformer}, CGW2 \cite{chefer2021generic} and TAM \cite{yuan2021explaining}. Those methods are based on attention weights of the classification token ($[CLS]$) on patch embeddings, or a combination of attention weights and class gradients. The use of attention weights for explaining NLP models has been extensively debated and the general conclusion seems to point to the negative side \cite{jain2019attention,serrano2019attention,pruthi2020learning,bastings2020elephant}. In ViT models, {\em  attention weights are concerned with the importance of path embeddings to the $[CLS]$ token, but not the semantic contents of the embeddings}. Using attention weights for explanation is analogous to understanding a doctor's diagnosis by listing the body parts he/she examined (patches) rather than considering the information he/she got from the examinations (patch embeddings). 


In this paper, we propose a novel explanation method for ViTs that is based on  patch embeddings and their causal impacts on the output \footnote{Note that here we talk about causality in the inference process rather than in the  data generation process.}. A quick comparison of our method, termed {\em ViT-CX}, with several previous methods is shown in Figure  \ref{score_cam_vit11}. The task here is to explain ViT-B/16 on several example images. We see that the explanations given by ViT-CX are visually more meaningful than those by previous methods. They are also more faithful to the model according to the deletion AUC and insertion AUC metrics \cite{petsiuk2018rise}.


More specifically, all the patch embeddings at a self-attention layer can be arranged into a 3D tensor, with the $(x,y)$-coordinates indicating their spatial information and the $z$-coordinate representing their semantic contents. A frontal slice (with a fixed $z$ value) of the tensor can be upsampled to the size of the input image and the resulting heatmap is known as a {\em ViT feature map}. Figure \ref{vit1} (b.1 - b.5) show some example ViT feature maps. They are clearly more meaningful than the attention weight maps shown in (a.1 - 1.5). In general, ViT feature maps tend to be semantically meaningful even at early transformer blocks \cite{yuan2021tokens,raghu2021vision}.
In ViT-CX, we treat the ViT feature maps from different stages of a ViT model as masks ({\em ViT masks}), apply them to the input image to obtain masked images (Figure \ref{vit1} (c.1 - c.5)),  and combine the masks using the class scores on the masked images  (i.e., causal impacts on output) to produce a saliency map.



The first important technical issue of this paper concerns is what we call the {\em  pixel coverage bias (PCB)}. It refers to the fact that some pixels are included in more masks than others and hence get unustifiably higher saliency values.   This  can lead to nonsensical explanations in the case of causal overdetermination, where the correct prediction can be made by any of many possible small patches, as often happens in ViTs \cite{naseer2021intriguing,paul2022vision}. We analyze the phenomenon theoretically and propose a simple and elegant solution.
The second issue is that making inferences on a large number of masked images is costly. We alleviate this problem by clustering the ViT masks.
Mask clustering does not lead to severe loss of information since they tend to overlap substantially (Figure \ref{vit1} (b.1 - b.5)).
The third issue is that applying a mask to an image might cause unintended artifacts \cite{fong2017interpretable,fong2019understanding}.  We resolve this issue by randomizing the values of the masked-out pixels.

The contributions of this work are listed below: 
\begin{itemize}
	\item We propose a new method for explaining ViT, and shows that it significantly outperforms previous methods (CGW1,CGW2,TAM) for the same task as evaluated by conventional protocols.
	\item Our ViT-CX is built upon on several previous ideas, namely mask-based explanation, creation of masks from feature maps and pixel coverage bias correction. We apply these ideas novelly to an important problem - ViT explanation.
	\item ViT-CX uses masks created from patch embeddings, whereas previous ViT explanation methods are based on attention weights paid to the path embeddings. 
	\item We provide a detailed analysis of the impact of PCB in mask-based ViT explanation and theoretically show how it can be alleviated by the correction technique. 
	\item An alternative way to correct PCB is to use a large number of random masks. We show that ViT-CX is computationally much more efficient than this brute-force method (384 vs. 5000 masks, a few seconds vs. 1 minute per image).

\end{itemize}


\section{Related Work}
%\subsection{Explainability for Vision Transformers}
\subsubsection{Explanation Methods for Vision Transformers:} 

\begin{figure*}[t]
	\centering
	%\includegraphics[height=6.7cm]{grpah_0308.eps}
	\includegraphics[height=8.7cm]{graph100.eps}
	\caption{Overview of ViT-CX. (a) Mask Generation: A set of semantic masks is generated from the patch embeddings; (b) Mask Aggregation: A saliency map is created by combining the masks using the class scores of the masked images. Pixel coverage frequencies are used in the second step to correct for pixel coverage bias (PCB).}
	\label{procedure1}
\end{figure*}


The earliest methods for explaining ViT models are based on attention weights. All attention weights at an attention head can be arranged into an attention map, which can be upsampled to the input size to form a saliency map. Rollout \cite{abnar2020quantifying} considers all heads from multiple layers, and combines the corresponding attention maps to form one saliency map. Partial LRP \cite{voita2019analyzing} is similar to Rollout, except that it assigns different  weights to different heads, which are computed using Layer-wise Relevance Propagation (LRP).  The saliency maps produced by Rollout and Partial LRP  are not class-specific because the attention weights are class-agnostic. As such, those methods cannot be used to explain the reasons for particular output classes.


There are methods that aim to explain a particular output class.
CGW1 \cite{chefer2021transformer} is similar to Partial LRP, except that
the gradients of the class score  with respect to the heads are also considered, alongside LRP weights, when combining attention maps from different heads. In  CGW2 \cite{chefer2021generic}, the LRP weights are removed since they are found to be unnecessary.  Transition Attention Map (TAM) \cite{yuan2021explaining} is similar to CGW2 except that simple gradients are replaced by integrated gradients \cite{sundararajan2017axiomatic}.  Figure \ref{score_cam_vit11} shows several saliency maps produced by CGW1, CGW2 and TAM.  They are apparently less satisfactory than those by ViT-CX. Moreover, attention weight-based methods cannot be used to explain the ViT models \cite{liu2021swin,chu2021twins,zhang2022nested} without $[CLS]$ token as they utilize the attention maps between the  $[CLS]$ token and patch tokens. In contrast, ViT-CX does not rely on the $[CLS]$ token and can be adopted by more ViT variants. 



\subsubsection{Mask-based Explanation Methods:} 
While there are only a few methods for explaining ViT models, a large number of methods have been proposed to explain CNN models.  Mask-based explanation methods are one  subclass.  They determine saliency values of pixels using a collection of masks
$\mathbb{M}=\{\mathbf{M}_1, \cdots, \mathbf{M}_K\}$. Each mask $\mathbf{M}_i$ is of the same size as the input image $\mathbf{X}$, and its pixel values are between 0 and 1. It is combined with the input
via element-wise product $\odot$ to yield
a  masked image $\mathbf{X} \odot \mathbf{M}_i$. Intuitively the pixels `inside' the mask (with high values in $\mathbf{M}_i$)  are kept and those `outside' are erased. Each masked image is fed to the target model  $f(\cdot)$ to get a score $f(\mathbf{X}\odot \mathbf{M}_i)$ for the target class, i.e., the output class being explained.  Finally,
a saliency map is created by combining the masks  with the class scores:
%Intuitively, the pixels `inside' the mask are kept and the pixels `outside' are erased.   
\begin{gather}
	S(x) =  \sum_{i=1}^{K}f(\mathbf{X}\odot \mathbf{M}_i)\mathbf{M}_i(x). \label{eq.score} 
	%	s(X,M_i)=f(X\odot M_i)\ . \label{ip.score}
\end{gather}
For visualization, the saliency values are normalized to the interval $[0, 1]$ using $(S(x)- \min_x S(x))/(\max_x S(x)-\min_x S(x)$. Occlusion Map \cite{zeiler2014visualizing}, Leave-One-Out \cite{li2016understanding} and RISE \cite{petsiuk2018rise} are typical mask-based explanation methods. They differ in the way to generate masks. %representative

Mask-based explanation is considered a causal approach because it perturbs the input image using masks, observes how the class score changes, and then builds a saliency map accordingly. Intuitively, we can think of a mask $\mathbf{M}_i$ as a `team' of pixels, and the class score
$f(\mathbf{X}\odot \mathbf{M}_i)$ as the {\em causal impact} of the `team' on the output class.\
\footnote{Strictly speaking, the causal impact score (causal effect) should be the difference between the class score of a masked image and that of the blank image. However, the second term is usually ignored as it is the same for all masks.}
The saliency value of a pixel is simply an aggregation of the causal impact scores of
the `teams' of which it is a member of.
ViT-CX is a mask-based explanation method for ViT models. It has its own way of generating masks and calculating causal impact scores of masks.  Being a causal method, ViT-CX is sensitive to the changes in model parameters.


Besides masked-based explanation, there are other
causal explanation methods, e.g., LIME \cite{ribeiro2016should}, Kernel SHAP \cite{lundberg2017unified}. They are regression methods based on super-pixels.  As such, they produce coarse-grained explanations. 

\subsubsection{Gradient-based Explanation Methods:}

Another branch of explanation methods for CNN models is gradient-based. Those methods can also
be used to explain ViT models. As baselines to be compared with ViT-CX, we choose three popular methods, namely Grad-CAM \cite{selvaraju2017grad}, Integrated-Grad \cite{sundararajan2017axiomatic} and Smooth-Grad \cite{smilkov2017smoothgrad}. Gradient-based methods are not causal and have known drawbacks. For instance, they can suffer from  gradient saturation, which can lead to poor explanation results  \cite{sundararajan2017axiomatic,shrikumar2017learning}.
%Moreover, some gradient-based method fail sanity checks \cite{adebayo2018sanity}
%in the sense that their results are insensitive to the changes in model parameters.
%Being a causal method, ViT-CX is sensitive to the changes in model parameters.





\section{Methodology}


\subsection{Preliminaries}


In ViT models, an image $\mathbf{X} \in \mathbb{R}^{H\times W\times C}$ is split into $N=HW/p^2$ patches,  with the $i$-th patch represented by a 2D vector $\mathbf{x}_i\in \mathbb{R}^{(p\times p) \times C}$, where $(H, W, C)$ are  the height, width, and  the number of channels of the image, and $(p, p)$ is the spatial resolution of each patch. The patches are mapped to embeddings with $D$ dimensions via linear projection. In the vanilla ViT architecture \cite{dosovitskiy2020image}, the embeddings are fed into $L$ transformer blocks. Each block includes two modules: A {\em Multi-Head Self-Attention (MHSA) module}  and a {\em Multi-Layer Perceptron (MLP) module}. They yield new embeddings of the patches. In the process, the number and size of the patches remain the same and the dimension of the embeddings is kept constant. 

In more recent hierarchical ViT architectures \cite{wang2021pyramid,chu2021twins,liu2021swin}, the patches are gradually merged as the computation proceeds from layer to layer. As a consequence, the number of patches is gradually reduced, and the dimension of the embeddings is increased.  


Hierarchical ViTs are divided into multiple (say $B$) stages. In the original ViT paper, there is no notion of the stages. Following \citet{zheng2021rethinking}, we divide the vanilla ViT into $B$ stages for consistency. We denote the embedding output at the last attention module of stage $i$ as $\mathbf{E}^{(i)} \in \mathbb{R}^{N_i\times D_i}$, where $N_i$ is the number of patch tokens and $D_i$ is the feature dimension.



\subsection{Overview of ViT-CX}

An overview of ViT-CX is shown in Figure  \ref{procedure1}. There are two phases. In the first phase, a collection of masks is generated from the patch embeddings in the target ViT model (Section \ref{mask_generation.section}). In the second phase, the masks are combined linearly to yield a saliency map.  The weight for each mask is determined by the causal impact of the pixels `inside' the mask on the class score (Section \ref{artifact.section}).  Pixel coverage frequencies are used to correct the pixel coverage bias (Section \ref{pcb.section}). 




\subsection{Mask Generation}	
\label{mask_generation.section}



Different mask-based methods have their own ways of generating masks. Occlusion Map \cite{zeiler2014visualizing} uses a small sliding window and generates a binary mask at each location by setting the values of the pixels inside the window to 0 and the values of the pixels outside the window to 1. Leave-One-Out \cite{li2016understanding} uses a special case of this strategy where the sliding window size is $1 \times 1$. RISE \cite{petsiuk2018rise} uses a random process to generate masks. Those methods lead to many masks, which implies high online computation costs. %In addition, the masks are not related to the target model, which hurts the model faithfulness of explanations.  


Score-CAM  \cite{wang2020score}  uses feature maps from a target CNN model as masks. The number of feature maps is relatively small, and they capture semantic information from the target model. We follow this strategy in ViT-CX to generate masks for a target ViT model from its patch embeddings. 


We first create a collection of masks from the embedding output $\mathbf{E}^{(i)}$ at the last attention module of stage $i$. The embeddings are first reshaped into a 3D tensor of size $\sqrt{N_i} \times \sqrt{N_i} \times D_i$. Each fiber in the tensor corresponds to the embedding of a patch, and 
the $(x, y)$-coordinates of the fiber correspond to the spatial location of the patch in the input image.
The frontal slices of the tensor are upsampled to the size of the input image, and the results are called {\em ViT feature maps}. \footnote{If the value matrix in self-attention is the identity matrix, a frontal slice of the tensor plays a similar role as a feature map in a CNN model, in the sense that the `pixel values' on it are aggregated using self-attention weights to compute activations for the next layer.} 
Several example ViT feature maps are shown in
Figure \ref{vit1} (b.1 - b.5). The feature maps are subsequently normalized to the interval $[0, 1]$ to get {\em ViT masks}.   


The number of ViT masks created at stage $i$ is $D_i$. We apply 
the K-means clustering algorithm to partition them into $k$ clusters, where
$k$ is a hyperparameter,  and then use the cluster centroids as the final masks.  
As the initial masks tend to overlap significantly (Figure \ref{vit1}: b.1 - b.5), clustering would not lead to much loss in information. However, it does improve the efficiency of online explanation. 
We denote the final set of masks for stage $i$ as $\mathbb{ M}^{(i)}=\{\mathbf{M}^{(i)}_1,\mathbf{M}^{(i)}_2,\cdots,\mathbf{M}^{(i)}_k\}$. 

%$\cal{ M}^{(i)}=\{M^{(i)}_1,M^{(i)}_2,\cdots,M^{(i)}_k\}$.




Feature maps from different stages of the model are different representations
of the input image.  To utilize the diversity of information, we use all the masks
from the last few stages in ViT-CX:
$$\mathbb{M}_{cx}=\bigcup_{i=b}^{B} \mathbb{M}^{(i)},$$
\noindent where  $b$ is a hyperparameter, and the total number of masks $K$ used in the explanation is $k\times(B-b+1)$. 

\subsection{Causal Impact Score Revisited}


\begin{figure}[t]
	\centering
	\begin{tabular}{cc|cc|cc}
		
%		\includegraphics[height=1.1cm,width=1.1cm]{goldfish2222/ILSVRC2012_val_00028713.JPEG}  &
		\includegraphics[height=0.91cm]{goldfish33/1_1_f1.jpg} &
		\includegraphics[height=0.91cm]{goldfish33/1_2_f1.jpg}  &
		\includegraphics[height=0.91cm]{goldfish33/1_1_m1.jpg} &
		\includegraphics[height=0.91cm]{goldfish33/1_2_m1.jpg} &
		\includegraphics[height=0.91cm]{rm1.png} &
		\includegraphics[height=0.91cm]{rm2.png}  \\  	
		%{\scriptsize Input} & 
		\multicolumn{2}{c|}{\scriptsize (a) ViT feature maps}  & \multicolumn{2}{c|}{\scriptsize Masked images}& \multicolumn{2}{c}{\scriptsize Masked images with noise} \\
		
			\includegraphics[height=0.91cm]{goldfish33/f11.png} &
	\includegraphics[height=0.91cm]{goldfish33/f22.png}  &
	\includegraphics[height=0.91cm]{goldfish33/m11.png} &
	\includegraphics[height=0.91cm]{goldfish33/m22.png} &
	\includegraphics[height=0.91cm]{goldfish33/n11.png} &
	\includegraphics[height=0.91cm]{goldfish33/n22.png}  \\  	
	%{\scriptsize Input} & 
	\multicolumn{2}{c|}{\scriptsize (b) ViT feature maps}  & \multicolumn{2}{c|}{\scriptsize Masked images}& \multicolumn{2}{c}{\scriptsize Masked images with noise}
	\end{tabular}
	
	\caption{(a) Unintended artifacts of masking: The masked images are classified
		as {\tt goldfish} with 94.8\%  and 95.1\%  though the fishes are masked out. After the random noises added, the prediction probabilities drop to 0.44\% and 0.39\%; (b) Examples for showing that the added random noises will not affect the preserved regions of the masked images.}
	\label{vit2}
	
\end{figure}

\label{artifact.section}
In previous methods, masks are combined via Equation (\ref{eq.score}), where the $f(\mathbf{X}\odot \mathbf{M}_i)$ is the score of the target class on the masked image $\mathbf{X}\odot \mathbf{M}_i$. One issue here is that masking can lead to unintended artifacts, which gives misleading scores.

Consider the two examples in the Figure \ref{vit2} (a). The ViT-feature maps focus on the background rather than the foreground, where the foreground pixels (the pixels
of the goldfish) share the lowest values. When their corresponding masks are combined with the image, the foreground pixels are assigned nearly identical `zero' pixel values. That `erases' the detailed feature information of the goldfish, such as texture and color, but clearly leaves the shape of the goldfish in the masked images.

The two masked images with the shape of goldfish left are correctly classified as {\tt goldfish} with high probabilities (94.8\% and 95.1\%). Evidently, those probabilities are not a good measure of the causal impact of the pixels `inside' the mask. To reduce the influence of such artifacts, we replace the term $f(\mathbf{X}\odot \mathbf{M}_i)$ in  Equation (\ref{eq.score}) with the new definition of {\em causal impact score}:
\begin{eqnarray}
	s(\mathbf{X},\mathbf{M}_i)=f(\mathbf{X}\odot \mathbf{M}_i+\mathbf{Rd} \odot (1-\mathbf{M}_i))\ ,
	\label{eq.score.artifact}
\end{eqnarray}
$\mathbf{Rd}\in \mathbb{R}^{H\times W \times C}$ is a matrix of random numbers. Since we use soft masks with mask values in $M_i$ between $[0,1]$, most noises are added to masked-out pixels (pixels with mask values 0 in $M_i$), and noises are more or less added to other pixels based on their mask values. With the added noises, the preserved shape in the masked images is corrupted. In the examples of Figure \ref{vit2} (a), the score of {\tt goldfish} drops to 0.44\% and 0.39\% after the introduction of random noises. At the same time, the masked images with important regions to the prediction preserved will not be affected by the added noises as examples shown in Figure \ref{vit2} (b). That is because fewest noises are added to the preserved pixels (with mask values closing to 1 in $M_i$) of the masked images.



\begin{figure*}
	\centering
	\begin{tabular}{cccc|cccc}
		
		\includegraphics[height=1.1cm,width=1.1cm]{pcb1/f.jpg} &
		\includegraphics[height=1.1cm]{pcb1/f_cf.jpg} &
		\includegraphics[height=1.1cm]{pcb1/f_no_pcb.jpg}&
		\includegraphics[height=1.1cm]{pcb1/f_pcb.jpg} &\includegraphics[height=1.1cm,width=1.1cm]{pcb1/h.jpg} &
		\includegraphics[height=1.1cm]{pcb1/h_cf.jpg} &
		\includegraphics[height=1.1cm]{pcb1/h_no_pcb.jpg}&
		\includegraphics[height=1.1cm]{pcb1/h_pcb.jpg} \\
		{\scriptsize (a) } &{\scriptsize (a.1)}&{\scriptsize (a.2)}&{\scriptsize (a.3)} &{\scriptsize (b) }& {\scriptsize (b.1) }&{\scriptsize (b.2)}&{\scriptsize (b.3)} \\
		%		\includegraphics[height=1.2cm,width=1.2cm]{pcb1/h.jpg} &
		%		\includegraphics[height=1.2cm]{pcb1/h_no_pcb.jpg}&
		%		\includegraphics[height=1.2cm]{pcb1/h_cf.jpg} &
		%		\includegraphics[height=1.2cm]{pcb1/h_pcb.jpg} \\
		%		
		%		{\scriptsize (b) }& {\scriptsize (b.1) }&{\scriptsize (b.2)}&{\scriptsize (b.3)}\\			
		%		\includegraphics[height=1.2cm,width=1.2cm]{pcb1/g.jpg} &
		%		\includegraphics[height=1.2cm]{pcb1/g_no_pcb.jpg}&
		%		\includegraphics[height=1.2cm]{pcb1/g_cf.jpg} &
		%		\includegraphics[height=1.2cm]{pcb1/g_pcb.jpg}\\
		%		
		%		{\scriptsize (c) } &{\scriptsize (c.1) }&{\scriptsize (c.2) }&{\scriptsize (c.3)} \\			
	\end{tabular}
	\caption{Impact of PCB: The saliency maps (a.2, b.2) closely resemble the coverage frequency  maps  (a.1, b.1), and hence offer no meaningful explanations for the target classes ({\tt goldfish} and {\tt dogsled}). After correcting for PCB, the saliency maps (a.3, b.3) become more meaningful. }
	%	\caption{Examples for the PCB issue and correction (model: ViT-B). For image(a,b), the probabilities for their masked images are all close to 1. The mask-based explanations without PCB correction (a.1,b.1) are identical to their mask coverage frequency maps (a.2,b.2), leading to nonsensical results. With our correction, the results locate the target class objects well (a.3,b.3)}
	%	$\phi(x)=\sum_{i=1}^{N} M_i(\x)$
	\label{overdetermined}
	\label{fig.pcb11}
\end{figure*}
\subsection{Pixel Coverage Bias}
\label{pcb.section}


Given a set of masks $\mathbb{M}=\{\mathbf{M}_1, \mathbf{M}_2, \cdots, \mathbf{M}_K\}$, the {\em coverage frequency} of a pixel $x$ is defined as: $$	\rho(x) = \frac{1}{K} \sum_{i=1}^K \mathbf{M}_i(x).$$
\noindent  
Pixel coverage bias (PCB) refers to the phenomenon that different pixels might have different coverage frequencies. It is a severe issue for mask-based explanation, and it has not received any attention.  According to  Equation (\ref{eq.score}),
the saliency value of a pixel, before normalized to $[0, 1]$, is the sum of the causal impact scores of the `teams' (masks) of which it is a member.  Consequently, the more `teams' a pixel in, the higher its saliency value. This is clearly not justified.



\subsubsection{Adverse Effects of PCB:}  PCB can lead to nonsensical explanations.
Figure \ref{overdetermined} show two examples.
For each example, a collection of masks is generated according to the procedure
described in Section \ref{mask_generation.section}. The pixel coverage frequencies are shown in  (a.1) and (b.1).  The saliency maps, as computed using Equation (\ref{eq.score}), are given in  (a.2) and (b.2). We see that
they closely resemble the coverage frequency maps, and 
offer no meaningful explanations  for the 
output labels. %({\tt goldfish} and {\tt dogsled}).  

\subsubsection{Analysis:}
To understand why PCB causes the nonsensical explanations, we first
note that, in both examples,  the correct prediction can be made from various small patches of the input image.  This phenomenon is  called {\em causal overdetermination} \cite{white2021contrastive}. It is analogous to committee voting where any memberâ€™s vote kills a proposal. Causal overdetermination is common with ViT models. In fact, it has been observed that the class scores in ViTs  are more robust
to deletions of small patches from the input image than many popular CNNs \cite{naseer2021intriguing}.



Let
$\mu=\frac1K \sum_{i=1}^K s(\mathbf{X},\mathbf{M}_i)$  and $\beta_i=s(\mathbf{X},\mathbf{M}_i)-\mu$. We divide the saliency score
$S(x)$ of a pixel into two parts:
\begin{eqnarray}
	S(x) & =&  \sum_{i=1}^{K} \beta_i \mathbf{M}_i(x)+ \sum_{i=1}^{K} \mu \mathbf{M}_i(x) \label{eq.score.rewrite0} \\ 
	&=&  \sum_{i=1}^{K} \beta_i \mathbf{M}_i(x)+ \mu N \rho(x).
	\label{eq.score.rewrite}
\end{eqnarray}
\noindent   In the case of causal overdetermination, the impact score $s(\mathbf{X}, \mathbf{M}_i)$ of most masks are close to 1. 
Consequently, $\mu$ is also close to 1, but $\beta_i$ is small for most $i$'s. Thus the second term is much larger than the first term.  When normalized to the interval $[0, 1]$, the first term basically vanishes.  This explains why 
in Figure \ref{overdetermined} the saliency maps closely resemble the coverage frequency maps.


\subsubsection{Correction for PCB: } 
A simple way to correct for PCB is to divide the saliency value $S(x)$ by the coverage frequency $\rho(x)$. This 
results in the {\em corrected saliency value}:
\begin{eqnarray}
	S^c(\x) = \frac{S(x)}{\rho(x)}=  \sum_{i=1}^{K} s(\mathbf{X},\mathbf{M}_i)\frac{\mathbf{M}_i(\x)}{\rho(x)}\ , \label{eq.sc.1}
\end{eqnarray}
where $S^c(x)=0$ by definition when $\rho(x)=0$. Intuitively, 
the corrected saliency value of a pixel is the sum of the causal impact scores of the `teams' of which it is a member, divided by the number of `teams' it participates in. Similar to Equation (\ref{eq.score.rewrite}), we decompose 	$S^c(\x)$ into two parts:
\begin{eqnarray}
	S^c(x) = \sum_{i=1}^{K} \beta_i \frac{\mathbf{M}_i(\x)}{\rho(x)}+ \mu N .
	\label{eq.score.corrected}
\end{eqnarray}

\noindent The second term is still much larger than the first term. However, it is a constant and does not depend on the 
pixel.  When the saliency values $S^c(x)$ are normalized to the interval $[0, 1]$ for visualization, the influence of the second term is completely eliminated.\footnote{Recall that the normalization is done using $(S^c(x) - \min_x S^c(x))/(\max_x S^c(x) - \min_x S^c(x))$.} This is why meaningful saliency maps emerge in 
Figure \ref{overdetermined} after correcting for PCB.  



\begin{table*}[t]
	\centering
	
	\begin{tabular}{c|ccc|ccc|ccc}
	%	\hline
		\Xhline{1.2pt}	
		\multicolumn{1}{c}{}	& \multicolumn{3}{c|}{ViT-B} & \multicolumn{3}{c|}{DeiT-B}   & \multicolumn{3}{c}{Swin-B}                \\ \hline
		\multicolumn{1}{c|}{}	& Del $\downarrow$  &    Ins $\uparrow$ &PG Acc $\uparrow$&	  Del $\downarrow$  &    Ins $\uparrow$ &PG Acc $\uparrow$ &	  Del $\downarrow$ &    Ins $\uparrow$ &PG Acc $\uparrow$ \\ \hline
		{\bf \em ViT-CX} &\textbf{0.154} &\textbf{0.607}&\textbf{86.76\%}&\textbf{0.209}&\textbf{ 0.806} &\textbf{87.51\%} &\textbf{0.255} &\textbf{0.769} & \textbf{91.26\%}\\ %\hline
	%	\addtolength{\tabcolsep}{-5.pt}
		\Xhline{1.2pt}
		\multicolumn{10}{c}{(a) Performance of Baselines}\\ 	%\Xhline{1.0pt}
		 %\hline
		Rollout     &0.251  &0.517  & 60.91\%	 &0.406	 & 0.642& 35.70\% &--- &--- &--- \\
		Partial LRP   &0.239&0.499&66.52\% &0.349& 0.655 &61.25\%&--- &--- &--- \\
		CGW1   &0.201 & 0.542    &77.14\%   &0.286 &0.717& 70.54\%  &--- &--- &---    \\
		CGW2  & 0.209 & 0.549  & 70.94\% &0.271 &0.736 &70.54\% &--- &--- &---\\
		TAM &0.180&0.556&  \underline{77.87\%} &0.240& 0.747& 75.47\%&--- &--- &--- \\\hline 	%\multicolumn{1}{c|}{}  &	\multicolumn{3}{c|}{}&	\multicolumn{3}{c|}{}   \\
			Grad-CAM  &0.212&0.456&50.45\%  &0.250 &0.743 & \underline{79.24 \%} & \underline{0.356} &0.693 & \underline{ 88.46\%} \\
		{Integrated-Grad}       &  0.184  & 0.263  &10.61\%  & 0.259 & 0.362 & 10.74\% &0.420 &0.483 &7.69\%\\
		{Smooth-Grad}       &  \underline{0.174}  &0.438 & 16.96\%   & \underline{0.231} &0.528 &31.05\%   &0.369 &0.505&14.52\%\\ 	 \hline
		
			{LIME}       & 0.207  &   0.572 & 64.78\% &0.312 &0.768 &59.80\% &0.388 &0.692 &63.53\%\\
		{Occlusion}       & 0.291  & 0.571 & 64.75\% &	0.380 & \underline{ 0.801} & 59.51\% &0.448 & \underline{ 0.752} & 69.65\%\\ 
			{RISE}  &0.234&\underline{0.581}&73.30\%&0.366 & 0.759& 71.84\%  &0.416 &0.727 &75.07\%\\ 
		
		\Xhline{1.2pt}
		\multicolumn{10}{c}{(b) Performance of RISE$^c$}\\ 	%\Xhline{1.0pt}
	 %\hline%\multicolumn{1}{c|}{}  &	\multicolumn{3}{c|}{}&	\multicolumn{3}{c|}{}   \\	% \vspace{-0.2cm}  \\
		%{MP}       &  0.280  & 0.451 & 58.29\% &0.363 &0.698& 51.23\% &0.372 & 0.665 & 61.24\%\\ \hline
	
		%	{\bf \em ViT-CX - ${M}_{cx}(K)$}  &\textbf{0.166} &\textbf{0.581}&\textbf{83.56\%}&\textbf{0.212}&0.798 &\textbf{85.51\%} &\textbf{0.253} &\textbf{0.769} & \textbf{91.26\%}\\
	%	$\mathbb{M}_{rd}(384)$   &0.281&0.508&61.14\% &0.387 &0.742& 67.37\%  &0.475 &0.689 & 57.54\% \\
		%$\mathbb{M}_{rd}(5,000)$   &0.234&0.581&73.30\%&0.366 & 0.759& 71.84\%  &0.416 &0.727 &75.07\%\\ 	
		
		RISE$^c$  &0.185&0.589&79.62\% & 0.279 & 0.797 &80.61\% & 0.352&0.734&83.41\%\\ \Xhline{1.2pt}
		
		%\hline
	\end{tabular}
	\caption{(a) Main results: \textbf{Boldface} and \underline{underline} indicate best and second best performance respectively, and `---' means not applicable.  ViT-CX significantly outperforms the baselines in terms of the faithfulness metrics deletion AUC (Del) and insertion AUC (Ins), and in terms  of the interpretability metric Pointing Game Accuracy (PG Acc). (b): Performance of RISE$^c$. Under our mask aggregation approaches, the explanation quality of RISE$^c$ is improved over RISE but still falls short of ViT-CX.}
	\label{robust_result1}
\end{table*}


\begin{figure*}[t]
	\centering
	\begin{tabular}{c|ccccc||c||cccc}
	&	\multicolumn{5}{c||}{(a) Comparison with Baseline Methods} & &	\multicolumn{4}{c}{(b) Comparison with Random Masks} \\
		{\scriptsize Input}  & {\scriptsize Rollout}  & {\scriptsize Partial LRP} & {\scriptsize CGW1} & {\scriptsize CGW2} & {\scriptsize TAM} & {\scriptsize ViT-CX} & {\tiny RISE(N=384)} &  {\tiny RISE$^c$(N=384)} &  {\tiny RISE(N=5000)} &  {\tiny RISE$^c$(N=5000)} \\
		%				\includegraphics[height=1.35cm]{sc/1.JPEG}&
		%				\includegraphics[height=1.35cm]{sc/rollout1.jpg}&
		%				\includegraphics[height=1.35cm]{sc/lrp1.jpg}&
		%				\includegraphics[height=1.35cm]{sc/hc1_1.jpg} &
		%				\includegraphics[height=1.35cm]{sc/hc2_1.jpg}&
		%				\includegraphics[height=1.35cm]{sc/tam1.jpg} &
		%				\includegraphics[height=1.35cm]{sc/sc1.jpg}
		%				\\  	
		\includegraphics[height=1.1cm,width=1.1cm]{sc/2.JPEG}&
	\includegraphics[height=1.1cm]{sc/rollout2.jpg}&
	\includegraphics[height=1.1cm]{sc/lrp2.jpg}&
	%	\includegraphics[height=1.1cm]{gd1.jpg}&
	\includegraphics[height=1.1cm]{sc/hc1_2.jpg} &
	\includegraphics[height=1.1cm]{sc/hc2_2.jpg}&
	\includegraphics[height=1.1cm]{sc/tam2.jpg} &
	\includegraphics[height=1.1cm]{sc/sc2.jpg}  &
	\includegraphics[height=1.1cm]{RISE/1_384_r.png}  &
	\includegraphics[height=1.1cm]{RISE/1_384_c.png}  &
	\includegraphics[height=1.1cm]{RISE/1_5000_r.png}  &
	\includegraphics[height=1.1cm]{RISE/1_5000_c.png}  \\
	\includegraphics[height=1.1cm,width=1.1cm]{pill_bottle/ILSVRC2012_val_00041459.JPEG}&
	\includegraphics[height=1.1cm]{pill_bottle/rollout.jpg}&
	\includegraphics[height=1.1cm]{pill_bottle/lrp.jpg}&
	%\includegraphics[height=1.1cm]{gd2.jpg}&
	\includegraphics[height=1.1cm]{pill_bottle/hc_1.jpg} &
	\includegraphics[height=1.1cm]{pill_bottle/hc_2.jpg}&
	\includegraphics[height=1.1cm]{pill_bottle/tam.jpg} &
	\includegraphics[height=1.1cm]{pill_bottle/sc5.jpg}&
	\includegraphics[height=1.1cm]{RISE/2_384_r.png}  &
	\includegraphics[height=1.1cm]{RISE/2_384_c.png}  &
	\includegraphics[height=1.1cm]{RISE/2_5000_r.png}  &
	\includegraphics[height=1.1cm]{RISE/2_5000_c.png}  \\
	\includegraphics[height=1.1cm,width=1.1cm]{sc/sofa.JPEG}&
	\includegraphics[height=1.1cm]{sc/sofa_rollout.jpg}&
	\includegraphics[height=1.1cm]{sc/sofa_lrp.jpg}&
	%\includegraphics[height=1.1cm]{gd3.jpg}&
	\includegraphics[height=1.1cm]{sc/sofa_hc_1.jpg} &
	\includegraphics[height=1.1cm]{sc/sofa_hc_2.jpg}&
	\includegraphics[height=1.1cm]{sc/sofa_tam.jpg} &
	\includegraphics[height=1.1cm]{sc/sofa_sc.jpg}&
		\includegraphics[height=1.1cm]{RISE/3_384_r.png}  &
	\includegraphics[height=1.1cm]{RISE/3_384_c.png}  &
	\includegraphics[height=1.1cm]{RISE/3_5000_r.png}  &
	\includegraphics[height=1.1cm]{RISE/3_5000_c.png}  \\
\end{tabular}
\caption{(a) Visual comparisons of ViT-CX and several baselines on explaining the predictions of ViT-B/16 on three examples.
Clearly, the regions highlighted by ViT-CX better match what human considers as evidence for the classes; (b) Visual comparison of ViT-CX under ViT masks and RISE(without PCB correction)/RISE$^c$(with PCB correction) under random masks. It can be seen that even with 5,000 random masks, the explanation quality is still poorer than that of our ViT-CX with only 384 masks (Explained Labels: {\tt Grile}, {\tt Pill Bottle}, {\tt Sofa}).
}
%The visual examples are all from ViT-ori.
\label{score_cam_vit1}
%\label{fig.pcb.rise}

\begin{tabular}{cc}
\hspace{-0.2cm}
\begin{tabular}{c}
	\ \\
	\includegraphics[width=1.3cm, height=1.3cm]{ILSVRC2012_val_00018235/ILSVRC2012_val_00018235.JPEG} \\
	{\scriptsize  Input }
\end{tabular}
&
\hspace{-0.2cm}
\begin{tabular}{c}
	\begin{tabular}{ccccccc}
		{\scriptsize Output Class} &	{\scriptsize Rollout}  & {\scriptsize  Partial LRP} & {\scriptsize  CGW1} & {\scriptsize  CGW2} & {\scriptsize  TAM} & {\scriptsize  ViT-CX}\\
		%\includegraphics[width=2.8cm, height=2.6cm]{Fig/fig1/guitar-cello.JPG}&
		%{\scriptsize }	& {\scriptsize \bf rollout}  & {\scriptsize \bf partial LRP} & {\scriptsize \bf CGW1} & {\scriptsize \bf CGW2} & {\scriptsize \bf TAM} & {\scriptsize \bf ViT-CX}\\ \\
		
		{\scriptsize Top-1: \tt Screen} &	\includegraphics[width=1.1cm,height=1.1cm]{ILSVRC2012_val_00018235/rollout.jpg}&
		\includegraphics[width=1.1cm,height=1.1cm]{ILSVRC2012_val_00018235/lrp.jpg}&
		\includegraphics[width=1.1cm,height=1.1cm]{ILSVRC2012_val_00018235/hc_1_screen.jpg}&
		\includegraphics[width=1.1cm,height=1.1cm]{ILSVRC2012_val_00018235/hc_2_screen.jpg}&
		\includegraphics[width=1.1cm,height=1.1cm]{ILSVRC2012_val_00018235/tam_screen.jpg}&
		\includegraphics[width=1.1cm,height=1.1cm]{ILSVRC2012_val_00018235/sc_screen.jpg}  \\				
		&{\scriptsize (a)} & {\scriptsize (b)} & {\scriptsize (c)} & {\scriptsize (d)} & 	{\scriptsize (e)}& 	{\scriptsize (f)} \\
		%\includegraphics[width=2.8cm, height=2.6cm]{Fig/fig1/guitar-cello.JPG}&
		{\scriptsize Ground-Truth: \tt Radiator} &	\includegraphics[width=1.1cm,height=1.1cm]{ILSVRC2012_val_00018235/rollout.jpg}&
		\includegraphics[width=1.1cm,height=1.1cm]{ILSVRC2012_val_00018235/lrp.jpg}&
		\includegraphics[width=1.1cm,height=1.1cm]{ILSVRC2012_val_00018235/hc_1_radiator.jpg}&
		\includegraphics[width=1.1cm,height=1.1cm]{ILSVRC2012_val_00018235/hc_2_radiator.jpg}&
		\includegraphics[width=1.1cm,height=1.1cm]{ILSVRC2012_val_00018235/tam_radiator.jpg}&
		\includegraphics[width=1.1cm,height=1.1cm]{ILSVRC2012_val_00018235/sc_radiator.jpg}  \\		
		&{\scriptsize (a')} & {\scriptsize (b')} & {\scriptsize (c')} & {\scriptsize (d')} & 	{\scriptsize (e')}& 	{\scriptsize (f')}
	\end{tabular}
\end{tabular}
\end{tabular}

\caption {Visual comparisons of ViT-CX and several baselines on revealing what ViT-B/16 considers as the evidence for: (a-f) the predicted label {\tt screen} (9.8\%); (a'-f') the ground-truth label  {\tt radiator} (4.5\%).}
\label{score_cam_vit_a}

\end{figure*}





\section{Experiments}


\subsection{Evaluation Metrics} 
We evaluate ViT-CX following a protocol similar to how previous ViT explanation methods (CGW1, CGW2, TAM) are evaluated, i.e., using deletion AUC, insertion AUC \cite{petsiuk2018rise}, the Pointing Game \cite{zhang2018top} and  visual examples. This scheme is also commonly used to evaluate explanation methods for CNNs.


\subsubsection{Deletion AUC and Insertion AUC:}  
These two metrics are about the {\em faithfulness} of an explanation (saliency map) to the target model, i.e., whether pixels with high saliency values are really important to the prediction of the model \cite{petsiuk2018rise}.  Deletion AUC measures how fast the score of the target class drops when pixels are deleted from the input image in descending order of the saliency values. Insertion AUC measures how fast the score increases when pixels are inserted into an empty canvas in that order. Smaller deletion AUC and larger insertion AUC indicate a more faithful explanation.
Let there be $n$ steps of deletion or insertion, and $\mathbf{X}_r$ be the $r$-th image in the process.  The metrics are defined as: $AUC=\frac1n \sum _{r=1}^nf(\mathbf{X}_r)$.


\subsubsection{Pointing Game:}  
This metric is about the {\em interpretability} of an explanation, i.e., whether it provides qualitative understanding between input and output \cite{ribeiro2016should,doshi2017towards}. In the Pointing Game, saliency maps from an explanation method are compared with human-annotated bounding boxes, which reflex how well the explanations match the qualitative understanding pre-defined by humans. 
For each pair of saliency map and  bounding box, if the pixel with the highest saliency value falls inside the box, it is considered a hit. Otherwise it is considered a miss. The Pointing Game Accuracy is defined  as: $Acc={\#Hits}/({\#Hits+\#Misses})$. 
\footnote{Following  \citet{wang2020score}, we use only those images where there is only a bounding box and it occupies no more than 50\% of the image.}
As a supplement to quantitative metrics, visual examples are often used to demonstrate the interpretability of explanations.


\subsection{Experiment Settings}

\subsubsection{Models and Dataset:} Three ViT variants are used in our experiments:  (1) ViT-B/16  \cite{dosovitskiy2020image}, the vanilla ViT;  (2) DeiT-B/16-Distill \cite{touvron2021training}, a improved version of the vanilla ViT with a distillation token added; (3) Swin-B \cite{liu2021swin}, a hierarchical ViT. We use 5,000 images randomly selected from the ILSVRC2012 validation set \cite{deng2009imagenet}. {All experiments are run on  an Intel Xeon E5-2620 CPU and an NVIDIA 2080 Ti GPU.}

\subsubsection{Hyper-parameters Setting:} Swin-B is a hierarchical model with $B=4$ stages. ViT-B and DeiT-B are also divided into 4 stages. Masks are generated from stage $b=2$ to stage 4 and the number of mask for each stage is $k=128$. The total number of masks is $K=384$.
The impact of $b$ is investigated in \textbf{Appendix A}.



\subsubsection{Baselines:} We compare ViT-CX with three groups of baselines: (a) five attention-based methods, namely Rollout, Partial LRP, CGW1, CGW2 and TAM; (b) three gradient-based explanation methods, namely Grad-CAM, Integrated-Grad and Smooth-Grad; (c) Three other causal explanation methods, namely LIME, Occlusion Map and RISE, in which Occlusion map and RISE are also mask-based explanation methods.




\subsection{Results}

The main results are in Table \ref{robust_result1} (a).
\subsubsection{Faithfulness:}
ViT-CX has the lowest deletion AUC values across the board,  being 10\% lower than the next best. ViT-CX also enjoys the highest insertion AUC values in all cases.
Those indicate that ViT-CX is more faithful to the target models than the baselines.



\subsubsection{Interpretability:} ViT-CX enjoys significantly higher accuracy in the Pointing Game than the baselines in all cases. This implies that the explanation results of ViT-CX are more consistent with the human-annotated bounding boxes.


To supplement the quantitative comparisons, Figures \ref{score_cam_vit11} and \ref{score_cam_vit1} show several examples to demonstrate the interpretability of the saliency maps for ViT-B by ViT-CX in comparison with those by the baselines.
More examples for other ViT models are given in \textbf{Appendix B}.
In all cases, the regions highlighted by ViT-CX better match human's qualitative understanding of what information should be considered when labeling an image with a particular class.




Figure  \ref{score_cam_vit_a} shows an example that is misclassified. ViT-CX and other methods are used to explain the predicted and the ground-truth classes. Clearly, ViT-CX does a much better job than others in helping users understand why the model makes a mistake. Note that
Rollout and Partial LRP yield identical saliency maps in the two cases. The reason is that those two methods are class-agnostic. 


\subsubsection{Sanity Check:}
As a causal method, ViT-CX is sensitive to the changes in model parameters and 
passes the sanity check \cite{adebayo2018sanity}. See \textbf{Appendix C} for details.

\subsection{Ablation Study}

ViT-CX consists of three key components: (1) A way to generate masks, (2) a method for avoiding unintended artifacts of masking, and (3) a correction for PCB during mask aggregation.  In the following, we report ablation studies on the importance of each component.



\subsubsection{Mask Generation:}
\label{evaluation.section.masks}


ViT-CX uses a set of masks $\mathbb{M}_{cx}$ (with number of masks $K=384$) from patch embeddings. To compare the explanations under $\mathbb{M}_{cx}$ with those under the randomly generated masks $\mathbb{M}_{rd}$, we have a variant of RISE, termed RISE$^c$. RISE$^c$ follows the setting of RISE to generate 5,000 masks randomly but uses our proposed Equation (2) to compute the causal impact scores and use Equation (5) to aggregate the masks with PCB correction. Thus, the only difference between ViT-CX and RISE$^c$ is in the mask used - $\mathbb{M}_{cx} (K=384)$ versus $\mathbb{M}_{rd} (K=5000)$. 

Table \ref{robust_result1} (b) shows the scores of the explanations by RISE$^c$ under various quality metrics, and Table \ref{ablation} shows the average time needed for ViT-CX and  RISE$^c$ to explain one image. We see that the masks generated by ViT-CX yield significantly better explanation quality in terms of interpretability and faithfulness. ViT-CX is also much more computationally efficient, taking only a few seconds to explain one image, while RISE$^c$ takes more than 1 minute. 

Figure \ref{score_cam_vit1} (b) also provide visual comparison between explanations under ViT masks and those under the random masks, which indicates that ViT-CX can yield explanations in higher visual quality with much smaller amount of masks (384 ViT masks vs. 5,000 random masks).


\begin{table}[t]
	\centering	
	
	\begin{tabular}{ccccc}
		\hline
		\multicolumn{1}{c}{}	&	\multicolumn{1}{c}{Masks}	&	\multicolumn{1}{c}{ ViT-B} & \multicolumn{1}{c}{ DeiT-B}   & \multicolumn{1}{c}{ Swin-B}       \\ \hline
 ViT-CX&	{\scriptsize $\mathbb{M}_{cx} (K=384)$}   &6.54&7.71 &6.29 \\
 	 RISE$^c$ & {\scriptsize $\mathbb{M}_{rd} (K=5,000)$} &56.32&65.30&72.23 \\\hline
	\end{tabular}		
	\centering
	\caption{Average  time (secs) for  explaining one image.} %($K=128$ for ViT and DeiT, and $K=384$ for SwinT)}
	\label{ablation}
	
	\
	
	\begin{tabular}{c|ccc}
		\hline
		%	\multicolumn{1}{c}{}	& \multicolumn{3}{c}{ViT-B/16}       \\ \hline
		
		\multicolumn{1}{c|}{}	& Del $\downarrow$  &    Ins $\uparrow$ &PG Acc $\uparrow$ \\ \hline
		{\bf \em ViT-CX}  &\textbf{0.154} &\textbf{0.607}&\textbf{86.76\%}\\
		{\em w/o P}  & 0.169 &0.578&80.90\% \\
		{\em 	w/o A} &0.196&0.547& 74.65\%     \\
		{\em 	w/o A\&P}   & 0.211 &0.531& 69.17\%  \\
		\hline
	\end{tabular}	
	\caption{Ablation study on the mask aggregation of ViT-CX.}
	\label{ablation.2}
\end{table}


\subsubsection{Mask Aggregation:}
\label{evaluation.section.ablation}

Table \ref{ablation.2} compares different ablations of ViT-CX on ViT-B/16. In the names of the ablations, {\em w/o P} means without correcting for PCB, and {\em w/o A} means without accounting for an unintended artifact of masking. 
It is clear that both PCB correction and artifact avoidance are important for ViT-CX.  Several visual examples are given in \textbf{Appendix D} to support this point further.  

Besides, the results of RISE and RISE$^c$ in Table 1 indicate that our mask aggregation approaches can also improve the explanation quality of RISE. In \textbf{Appendix E}, we show the importance of PCB correction when explaining CNNs.




\section{Conclusion}

A novel method for explaining ViT models named ViT-CX is proposed. While previous methods use attention weights on patch embeddings and class gradients, ViT-CX is based on patch embeddings themselves. It creates masks from the patch embeddings, determines their causal impact scores on a target output class, and combines the masks using those scores. Empirical evaluations show that ViT-CX does a noticeably better job highlighting the pixels that are important for model output.  It also provides a better qualitative understanding of the relationship between input and output.  ViT-CX can be used to explain different flavors of ViT models. It can be deployed with ViT model to boost user trust in them.

%\newpage

\bibliography{aaai22.bib}


\end{document}
