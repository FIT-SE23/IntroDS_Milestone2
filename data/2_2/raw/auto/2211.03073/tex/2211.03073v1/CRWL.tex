\def\year{2022}\relax
%File: formatting-instructions-latex-2022.tex
%release 2022.1
\documentclass[letterpaper]{article} % DO NOT CHANGE THIS
\usepackage{aaai22}  % DO NOT CHANGE THIS
\usepackage{times}  % DO NOT CHANGE THIS
\usepackage{helvet}  % DO NOT CHANGE THIS
\usepackage{courier}  % DO NOT CHANGE THIS
\usepackage[hyphens]{url}  % DO NOT CHANGE THIS
\usepackage{graphicx} % DO NOT CHANGE THIS
\urlstyle{rm} % DO NOT CHANGE THIS
\def\UrlFont{\rm}  % DO NOT CHANGE THIS
% \usepackage{natbib}  % DO NOT CHANGE THIS AND DO NOT ADD ANY OPTIONS TO IT
\usepackage{caption} % DO NOT CHANGE THIS AND DO NOT ADD ANY OPTIONS TO IT
\DeclareCaptionStyle{ruled}{labelfont=normalfont,labelsep=colon,strut=off} % DO NOT CHANGE THIS
\frenchspacing  % DO NOT CHANGE THIS
\setlength{\pdfpagewidth}{8.5in}  % DO NOT CHANGE THIS
\setlength{\pdfpageheight}{11in}  % DO NOT CHANGE THIS
%
% These are recommended to typeset algorithms but not required. See the subsubsection on algorithms. Remove them if you don't have algorithms in your paper.
\usepackage{algorithm}
\usepackage{algorithmic}

%
% These are are recommended to typeset listings but not required. See the subsubsection on listing. Remove this block if you don't have listings in your paper.
\usepackage{newfloat}
\usepackage{listings}

\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{hyperref}
\usepackage{bbm}

\lstset{%
	basicstyle={\footnotesize\ttfamily},% footnotesize acceptable for monospace
	numbers=left,numberstyle=\footnotesize,xleftmargin=2em,% show line numbers, remove this entire line if you don't want the numbers.
	aboveskip=0pt,belowskip=0pt,%
	showstringspaces=false,tabsize=2,breaklines=true}
\floatstyle{ruled}
\newfloat{listing}{tb}{lst}{}
\floatname{listing}{Listing}
%
%\nocopyright
%
% PDF Info Is REQUIRED.
% For /Title, write your title in Mixed Case.
% Don't use accents or commands. Retain the parentheses.
% For /Author, add all authors within the parentheses,
% separated by commas. No accents, special characters
% or commands are allowed.
% Keep the /TemplateVersion tag as is
\pdfinfo{
/Title (Contrastive Weighted Learning for Near-Infrared Gaze Estimation)
/Author (AAAI Press Staff, Pater Patel Schneider, Sunil Issar, J. Scott Penberthy, George Ferguson, Hans Guesgen, Francisco Cruz, Marc Pujol-Gonzalez)
/TemplateVersion (2022.1)
}

% DISALLOWED PACKAGES
% \usepackage{authblk} -- This package is specifically forbidden
% \usepackage{balance} -- This package is specifically forbidden
% \usepackage{color (if used in text)
% \usepackage{CJK} -- This package is specifically forbidden
% \usepackage{float} -- This package is specifically forbidden
% \usepackage{flushend} -- This package is specifically forbidden
% \usepackage{fontenc} -- This package is specifically forbidden
% \usepackage{fullpage} -- This package is specifically forbidden
% \usepackage{geometry} -- This package is specifically forbidden
% \usepackage{grffile} -- This package is specifically forbidden
% \usepackage{hyperref} -- This package is specifically forbidden
% \usepackage{navigator} -- This package is specifically forbidden
% (or any other package that embeds links such as navigator or hyperref)
% \indentfirst} -- This package is specifically forbidden
% \layout} -- This package is specifically forbidden
% \multicol} -- This package is specifically forbidden
% \nameref} -- This package is specifically forbidden
% \usepackage{savetrees} -- This package is specifically forbidden
% \usepackage{setspace} -- This package is specifically forbidden
% \usepackage{stfloats} -- This package is specifically forbidden
% \usepackage{tabu} -- This package is specifically forbidden
% \usepackage{titlesec} -- This package is specifically forbidden
% \usepackage{tocbibind} -- This package is specifically forbidden
% \usepackage{ulem} -- This package is specifically forbidden
% \usepackage{wrapfig} -- This package is specifically forbidden
% DISALLOWED COMMANDS
% \nocopyright -- Your paper will not be published if you use this command
% \addtolength -- This command may not be used
% \balance -- This command may not be used
% \baselinestretch -- Your paper will not be published if you use this command
% \clearpage -- No page breaks of any kind may be used for the final version of your paper
% \columnsep -- This command may not be used
% \newpage -- No page breaks of any kind may be used for the final version of your paper
% \pagebreak -- No page breaks of any kind may be used for the final version of your paperr
% \pagestyle -- This command may not be used
% \tiny -- This is not an acceptable font size.
% \vspace{- -- No negative value may be used in proximity of a caption, figure, table, section, subsection, subsubsection, or reference
% \vskip{- -- No negative value may be used to alter spacing above or below a caption, figure, table, section, subsection, subsubsection, or reference

\setcounter{secnumdepth}{0} %May be changed to 1 or 2 if section numbers are desired.

% The file aaai22.sty is the style file for AAAI Press
% proceedings, working notes, and technical reports.
%

% Title

% Your title must be in mixed case, not sentence case.
% That means all verbs (including short verbs like be, is, using,and go),
% nouns, adverbs, adjectives should be capitalized, including both words in hyphenated terms, while
% articles, conjunctions, and prepositions are lower case unless they
% directly follow a colon or long dash
\title{Contrastive Weighted Learning for Near-Infrared Gaze Estimation}
\author{
    %Authors
    % All authors must be in the same font size and format.
    Adam Lee
}
\affiliations{
    %Afiliations
    Georgia Institute of Technology\\
    % If you have multiple authors and multiple affiliations
    % use superscripts in text and roman font to identify them.
    % For example,

    % Sunil Issar, \textsuperscript{\rm 2}
    % J. Scott Penberthy, \textsuperscript{\rm 3}
    % George Ferguson,\textsuperscript{\rm 4}
    % Hans Guesgen, \textsuperscript{\rm 5}.
    % Note that the comma should be placed BEFORE the superscript for optimum readability

    % email address must be in roman text type, not monospace or sans serif
    alee489@gatech.edu
%
% See more examples next
}

%Example, Single Author, ->> remove \iffalse,\fi and place them surrounding AAAI title to use it
\iffalse
\title{My Publication Title --- Single Author}
\author {
    Author Name
}
\affiliations{
    Affiliation\\
    Affiliation Line 2\\
    name@example.com
}
\fi

\iffalse
%Example, Multiple Authors, ->> remove \iffalse,\fi and place them surrounding AAAI title to use it
\title{My Publication Title --- Multiple Authors}
\author {
    % Authors
    First Author Name,\textsuperscript{\rm 1}
    Second Author Name, \textsuperscript{\rm 2}
    Third Author Name \textsuperscript{\rm 1}
}
\affiliations {
    % Affiliations
    \textsuperscript{\rm 1} Affiliation 1\\
    \textsuperscript{\rm 2} Affiliation 2\\
    firstAuthor@affiliation1.com, secondAuthor@affilation2.com, thirdAuthor@affiliation1.com
}
\fi


% REMOVE THIS: bibentry
% This is only needed to show inline citations in the guidelines document. You should not need it and can safely delete it.
\usepackage{bibentry}
% END REMOVE bibentry

\begin{document}

\maketitle

\begin{abstract}
Appearance-based gaze estimation has been very successful with the use of deep learning. Many following works improved domain generalization for gaze estimation. However, even though there has been much progress in domain generalization for gaze estimation, most of the recent work have been focused on cross-dataset performance-- accounting for different distributions in illuminations, head pose, and lighting. Although improving gaze estimation in different distributions of RGB images is important, near-infrared image based gaze estimation is also critical for gaze estimation in dark settings. Also there are inherent limitations relying solely on supervised learning for regression tasks. This paper contributes to solving these problems and proposes GazeCWL, a novel framework for gaze estimation with near-infrared images using contrastive learning. This leverages adversarial attack techniques for data augmentation and a novel contrastive loss function specifically for regression tasks that effectively clusters the features of different samples in the latent space. Our model outperforms previous domain generalization models in infrared image based gaze estimation and outperforms the baseline by 45.6\% while improving the state-of-the-art by 8.6\%, we demonstrate the efficacy of our method. 
\end{abstract}

\section{1. Introduction}
Gaze estimation techniques have many critical applications in various domains including human-computer interaction[2], virtual reality[3] and medical analysis[4]. Appearance based gaze estimation leveraging deep learning and convolutional neural networks have gained great traction over the past several years[12,13,14]. There have been especially notable progress in appearance based gaze estimation via supervised learning and domain adaptation for cross-dataset performance that accounts for different lighting, illuminations, and head poses. But with broader usage of gaze estimation in edge devices, there is increasing need for gaze estimation in the dark. And infrared images are much more reliable in these settings. 

Even though domain generalization using more datasets have made improvements in performance, gaze annotations are difficult to obtain. Thus it is very challenging to create large datasets that are representative of real world distributions[16]. 

Also training with a diverse group of large datasets alone could be insufficient since each dataset has different distribution in illuminations and poses, but does not represent many of the real world scenarios that can be encountered with edge devices[21].

Self-supervised learning (SSL) has been a successful alternative that gained great traction recently, reducing the reliance on labeled samples[17,18,19]. 

Contrastive learning has been used in many SSL training frameworks. However, the general contrastive loss is most suited for classification tasks[10]. With regression, the slightest changes in the latent vector representation of features could hinder the performance drastically. Thus using the similarity between gaze labels can be exploited as a proxy for the distance between samples in the feature space. 

To the best of our knowledge, we are the first to improve gaze estimation with near-infrared images using appearance based deep learning and contrastive learning. We propose the following:  
\begin{enumerate}
\item GazeCWL, a novel framework for gaze estimation with near-infrared images. GazeCWL works as a teacher-student model and leverages two novel techniques along with the AI Hub near infrared dataset[15] for knowledge distillation. 
\item Data augmentation methods for gaze estimation in near-infrared images. Using both adversarial attack and data augmentation suited for near-infrared images for more effective adaptation. 
\item CRWL, a novel contrastive loss function for better clustering of features in the latent space. 
\end{enumerate}

Our model achieves significant performance improvements compared to existing domain generalization methods, outperforming the baseline by 45.6\% and improving the state-of-the-art by 8.6\%. 

\section{2. Related work}
\subsection{2.1. Appearance based Gaze Estimation}
Appearance based gaze estimation has recently gained great attention as it does not require dedicated devices[6]. Appearance based methods directly map images to the gaze vector and perform better than model methods which rely on eye-geometry[12,24]. 
\subsection{2.2. Domain adaptation}
Several datasets are frequently used for training gaze estimation models, including ETH-XGaze[8], Gaze Capture[5], and MPII[7]. However as each of these datasets have different distributions in lighting, illumination, and pose, several methods have been previously proposed to increase domain generalization and thus improve cross-dataset performance. 

Many works have employed adversarial training for better domain generalization. And others have used data augmentation techniques, and better feature alignment techniques. 
\subsection{2.3. Contrastive learning}
Contrastive learning is a form of unsupervised learning where the goal is to increase the distance between the anchor sample and negative samples and decrease the distance between the anchor sample and positive samples. 

The most notable version of contrastive loss is NT-Xent, highlighted by Chen et al.[9]. Previously contrastive loss has been used in the context of classification tasks and is thus not best suited for regression tasks[10]. Wang et al.[10] proposed a contrastive loss by using the distribution of the labels as the weights with KL-divergence. Jindal et al.[20] recently proposed a GazeCRL, a contrastive learning framework for gaze estimation. 

\section{3. Methodology}
Our objective is to improve the performance of gaze estimation on the target domain of near-infrared images. Since gaze is a very subtle feature from external appearance, we employ data augmentation and contrastive learning to better cluster features in the latent space which yields better performance in domain adaptation. 
\begin{equation}
\min_{f}E_{x^{\tau},y^{\tau}}[L(f(x^{\tau}),g^{\tau})] 
\end{equation}
\subsection{3.1. Overview}
We used the backbone of ITracker[5] for our gaze estimation model pretrained on two RGB gaze labeled datasets: MPII and XGaze. ITracker takes three normalized images as inputs: the left eye, the right eye, and the face. The output of the model is the gaze estimation on the 2-dimensional camera plane on the z-axis. 

Preliminary data augmentation is done on both near-infrared and RGB images. $g$ denotes transformations to grayscale and $i$ denotes adversarial attack elaborated in \hyperref[sec:adversarial]{Sec 3.2}.
\begin{equation}
x' = 
\left\{\begin{matrix}
        i(g(x^{rgb})) & RGB\\
        i(x^{nir}) & NIR
    \end{matrix}\right.
\end{equation}

Labels contain information regarding the relationship between features of different samples. Thus our proposed contrastive loss weights each negative sample by the similarity between the gaze label of itself and the anchor. This is amalgamated with supervised learning using the outputs of the teacher model as the pseudo label for the student model. Backpropagation is done to train the feature extractor with the CRWL. Then the predictor is trained in comparison to the pseudo label. 

The results are optimized for validation with l2 distance on the 2-dimensional camera plane. Again, using the outputs of the teacher model as the pseudo label. 
\begin{equation}
\|\hat{y}_{t} - \hat{y}_{s}\| 
\end{equation}
\subsection{3.2. Adversarial Attack}
\label{sec:adversarial}
Adversarial attack has been an integral technique for domain adaptation as it allows the generation of diverse positive and negative samples which can be used for contrastive learning for domain adaptation. Minimizing the distance between feature maps of positive samples and the anchor while maximizing the distance between those of negative samples and the anchor lead to better clusterization of features in the latent space. Since the outer appearance between samples with drastically different gaze directions can be very similar, better clustering of features in the latent space improves the performance of gaze estimation. 

We apply two types of data augmentation. One on the source model by converting the RGB image into grayscale by only taking the red intensity. And then applying the High-Frequency Component[21] adversarial attack on the target domain on both the source domain and paired target domain images. 

The High-Frequency Component adversarial attack leverages two techniques. The first one is fast gradient sign method (FGSM), a one step scheme[22]  
\begin{equation}
x' = x + \epsilon \cdot sign(\triangledown_{x}L(f(x),y))
\end{equation}
Where $\epsilon$ is the magnitude of perturbation. The second is a multi-step variant of the former, the projected gradient descent (PGD)[23].
\begin{equation}
x^{t+1} = \Pi_{x+S}(x^{t} + \epsilon \cdot sign(\triangledown_{x}L(f(x),y))) \end{equation}

High-Frequency Component, utilizes both adversarial techniques as follows.  
\begin{equation}
x^{\prime s,t}_{i} = 
    \left\{\begin{matrix}
        FGSM(x^{s,t}_{i},y^{s,t}_{i},L_{gaze}) & 0.5\\
        PGD(x^{s,t}_{i},y^{s,t}_{i},L_{gaze}) & 0.5
    \end{matrix}\right.
\end{equation}


\subsection{3.3. Contrastive regression loss for gaze estimation}
Contrastive loss has proved to be a powerful method for unsupervised learning in classification tasks. 
We propose CRWL, a novel contrastive regression loss function for gaze estimation. 
Contrary to classification tasks, the distancing method has to be scaled relative to the sample's proximity to the anchor sample inferred by the labels[10]. Also it should be scaled accordingly since the similarities between samples are relatively similar between gaze samples. Here is the proposed contrastive loss for regression tasks.  
\begin{equation}
CRWL = -\log\frac{\sum_{j} exp(sim(z_{i}, z_{j})/\tau)}{\sum_{k}\mathbbm{1}_{k \ne i}| S_{i, k} | \cdot exp(sim(z_{i}, z_{k})/\tau)}
\end{equation}
Contrast to the NT-Xent, we scale the negative samples by the similarity between the two labels since subtle distances in features are crucial signals in regression tasks. Cosine similarity is used to measure the difference between features in the latent space which is defined as the cosine of the angle between latent vectors implemented as follows:
\begin{equation}
sim(x,y) = \frac{x \cdot y}{\| x \| \| y \|}
\end{equation}
The similarity function for weighing the negative samples is the following:
\begin{equation}
S = -log \frac{sim(g_{i}, g_{k})}{cos(\pi/60)}
\end{equation}
Since the similarity between the gaze labels of the anchor and negative sample can be used to infer the ideal distance between the two in the latent space of feaures, the loss function encourages negative samples that are more different in labels to be more different in the feature space which facilitates clustering of relevant features in the latent space. The additional division has been added to account for the kappa angle - the difference between visual and pupillary axis.


\begin{algorithm}[tb]
\caption{GazeCWL}
\label{alg:algorithm}
\textbf{Input}: RGB Data $D_{s} = \{x^{s}_{n}\}$, NIR Data $D_{\tau} = \{x^{\tau}_{n}\}_{1:N_{\tau}}$, Pretrained Network M, Feature Extractor f, Projection Head h, Contrastive Regression Weighted Loss CRWL\\
\textbf{Output}: Trained Network $M$
\begin{algorithmic}[1] %[1] enables line numbers
\WHILE{not converged do}
\STATE Get batch data $x^{s}$ and $x^{\tau}$ from $D_{s}$ and $D_{\tau}$ respectively
\STATE Conduct augmentation $I^{s} = i(g(x^{s})), I^{\tau} = g(x^{\tau})$ 
\STATE Generate pseudo label $g'$ by feeding $I^{s}$ to teacher model $M$
\STATE Feed $I^{\tau}$ into the student model $M$
\STATE Calculate CRWL from the output of the feature extractor $f(I^{\tau})$
\STATE Calculate Huber Loss $L_{\delta}(h(f(I^{\tau})), g')$
\STATE Backpropagate and update the student model $M$, updating by $M(\theta)$ with the final loss of $CRWL + \gamma L_{\delta}$ 
\ENDWHILE
\STATE \textbf{return} $M$
\end{algorithmic}
\end{algorithm}

\subsection{3.4. GazeCWL}
Huber loss is used for the supervised learning between the output of the student model and the pseudo label.
\begin{equation}
L_{\delta} = 
    \left \{ \begin{matrix}
        \frac{1}{2}(y - \hat{y})^{2} & if \left | (y - \hat{y})  \right | < \delta\\
        \delta ((y - \hat{y}) - \frac1 2 \delta) & otherwise
    \end{matrix}  \right.
\end{equation}
The psdeu label $g'$ is generated by the output of the teacher model. The final loss is the following:  
\begin{equation}
L_{f} = CRWL + \gamma L_{\delta}
\end{equation}

\section{4. Experiments}
\subsection{4.1. Setup}
We pretrain the teacher with six datasets: Eth-XGaze, Gaze-360, GazeCapture, RT-Gene, EyeDiap, and MPIIGaze. We then test other methods against the AI Hub near-infrared image dataset[15] to evaluate and compare the performance of each method on infrared images. We also employ techniques[25,32] to normalize the gaze datasets by standardizing the head pose by rotating the virtual camera. Additional training on the student model was done with the AI Hub near-infrared dataset.
\subsection{4.2. Training Details}
We perform our experiments on NVIDIA RTX 3080. The resolution for input images in all the experiments is set as 224x224, following the conventions of [5], while different from [25], which employs 448x448as the resolution of input for training on Gaze360. We take ResNet-50 [26] as the backbone to extract features for all experiments  if  without  extra  annotation,  a  3-layers  MLP  as the predictor to generate 32-dim latent vectors of the feature, and an FC layer to regress a 2-dim gaze vector head pose angles.   For domain generalization tasks on source domain RGB, we follow [27], set the batch size as 16, use the Adam optimizer with a learning rate of 1x10-3 and train for 100 epochs using a decay factor 0.1 every 10 epochs. For domain adaptation tasks, the hyperparameter setting keeps the same as that in domain generalization tasks in source domain RGB.
\subsection{4.3. Domain adaptation}
We fine-tune models pretrained on RGB gaze images with near-infrared images using different domain adaptation methods. Using the vanilla domain adaptation, the source domain is available and we then perform CWRL on the target domain.  The CWRL module outperforms other models by a wide margin. 

\begin{table}
\caption{}
\begin{center}
\begin{tabular}{|c|c|c|}
\hline
\textbf{Method}& \textbf{$Source_{RGB}$} & \textbf{$Target_{NIR}$} \\
% \textbf{Table}&\multicolumn{3}{|c|}{\textbf{Table Column Head}} \\
\cline{1-3} 
Baseline[27] & 2.43 & 3.35 \\

GazeAdv[29] & 2.67 & 2.589 \\

PureGaze[31] & 2.67 & 2.312 \\

PnP-GA[25] & 2.33 & 2.038 \\

CDG[10] & 2.58 & 2.24 \\

\textbf{CRWL} & \textbf{2.32} & \textbf{1.82} \\

\multicolumn{2}{l}{$^{\mathrm{a}}$ L2 Distance (cm) is used as the evaluation metric.}
\end{tabular}
\label{tab1}
\end{center}
\end{table}

First we use $ CWRL $ with $ \tau $ as the annealed temperature. Then we used the pseudo labels from the teacher model to perform several epochs of self-training. Results in tab. 1 demonstrates the efficacy of our method as it outperforms the baseline method by 45.6\%. Out method also performs better on near-infrared based gaze estimation compared to other state-of-the-art domain adaptation methods in gaze estimation, improving the state-of-the-art by 8.6\%. 

\section{5. Conclusion}
In this paper we introduced GazeCWL, a novel contrastive learning framework for gaze estimation with near infrared images. Our framework employs data augmentation techniques specifically for near-infrared images and utilize a novel contrastive loss function effective for regression tasks. Furthermore, we showed that GazeCWL is effective at clustering relevant features closer together in the latent space.GazeCWL can be used as a general framework for training with near-infrared images on regression tasks, thus can be explored in the future for applications.

\begin{thebibliography}{99}
\bibitem{1}
Philip Bachman, R Devon Hjelm, and William Buchwalter.Learning representations by maximizing mutual information across views.Advances in Neural Information Processing Systems, 32:15535–15545, 2019.  
\bibitem{2}
X. Zhang, Y. Sugano, and A. Bulling, “Evaluation of appearance-based   methods and implications for gaze-based applications,” in Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems, ser. CHI ’19. New York, NY,  SA: Association for Computing Machinery, 2019.  
\bibitem{3}
Alisa Burova, John Makela, Jaakko Hakulinen, Tuuli Keski-nen, Hanna Heinonen, Sanni Siltanen, and Markku Turunen.Utilizing vr and gaze tracking to develop ar solutions for in-dustrial maintenance. In Proceedings of the 2020 CHI Con-ference on Human Factors in Computing Systems, pages 1–13, 2020.

\bibitem{4}
Nora Castner, Thomas C Kuebler, Katharina Scheiter, Ju-liane Richter, Therese Eder, Fabian Huttig, Constanze Keu-tel, and Enkelejda Kasneci. Deep semantic gaze embedding and scan path comparison for expertise classification duringopt viewing. InACM Symposium on Eye Tracking Research and Applications, pages 1–10, 2020
\bibitem{5}
Krafka, Kyle, et al. “Eye tracking for everyone.” Proceedings of the IEEE conference on computer vision and pattern recognition. 2016.
\bibitem{6}
Cheng, Yihua, et al. “Appearance-based gaze estimation with deep learning: A review and benchmark.” arXiv preprint arXiv:2104.12668 (2021).
\bibitem{7}
Zhang, Xucong, et al. “Mpiigaze: Real-world dataset and deep appearance-based gaze estimation.” IEEE transactions on pattern analysis and machine intelligence 41.1 (2017): 162-175.
\bibitem{8}
Zhang, Xucong, et al. “Eth-xgaze: A large scale dataset for gaze estimation under extreme head pose and gaze variation.” European Conference on Computer Vision. Springer, Cham, 2020.
\bibitem{9}
Chen, Ting, et al. “A simple framework for contrastive learning of visual representations.” International conference on machine learning. PMLR, 2020.
\bibitem{10}
Y. Wang et al., “Contrastive Regression for Domain Adaptation on Gaze Estimation,” 2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2022, pp. 19354-19363, doi: 10.1109/CVPR52688.2022.01877.
\bibitem{11}
Xucong Zhang, Yusuke Sugano, and Andreas Bulling. Revisiting data normalization for appearance-based gaze estimation. In Proceedings of the 2018 ACM Symposium on EyeTracking Research and Applications, pages 1–9, 2018
\bibitem{12}
X. Zhang, Y. Sugano, M. Fritz, and A. Bulling, “Appearance-based gaze estimation in the wild,” in Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 4511–4520, 2015.
\bibitem{13}
X. Zhang, Y. Sugano, M. Fritz, and A. Bulling, “It’s written all over your face: Full-face appearance-based gaze estimation,” in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops, pp. 51–60, 2017.
\bibitem{14}
K. Krafka, A. Khosla, P. Kellnhofer, H. Kannan, S. Bhandarkar, W. Matusik, and A. Torralba, “Eyetracking for everyone,” in Proceedings of the IEEE conference on computer vision and pattern recognition,pp. 2176–2184, 2016. 
\bibitem{15}
Song. “Eye Movement Video Data” AIHub. 2022. Web1. 1 Sep 2022. https://aihub.or.kr/aihubdata/data/view.do?currMenu=115
\bibitem{16}
S. Ghosh, A. Dhall, M. Hayat, J. Knibbe, and Q. Ji, “Automatic gaze analysis: A survey of deep learningbased approaches,”arXiv preprint arXiv:2108.05479, 2021.
\bibitem{17}
K. He, H. Fan, Y. Wu, S. Xie, and R. Girshick, “Momentum contrast for unsupervised visual representationlearning,” inProceedings of the IEEE/CVF conference on computer vision and pattern recognition,pp. 9729–9738, 2020.
\bibitem{18}
T. Chen, S. Kornblith, M. Norouzi, and G. Hinton, “A simple framework for contrastive learning of visualrepresentations,” inInternational conference on machine learning, pp. 1597–1607, PMLR, 2020.
\bibitem{19}
J.-B. Grill, F. Strub, F. Altché, C. Tallec, P. Richemond, E. Buchatskaya, C. Doersch, B. Avila Pires,Z. Guo, M. Gheshlaghi Azar,et al., “Bootstrap your own latent-a new approach to self-supervised learning,”Advances in Neural Information Processing Systems, vol. 33, pp. 21271–21284, 2020.
\bibitem{20}
Jindal, Swati, and Roberto Manduchi. “Contrastive Representation Learning for Gaze Estimation.” arXiv preprint arXiv:2210.13404 (2022).
\bibitem{21}
Liu, Ruicong, et al. “Jitter Does Matter: Adapting Gaze Estimation to New Domains.” arXiv preprint arXiv:2210.02082 (2022).
\bibitem{22}
Goodfellow, I. J.; Shlens, J.; and Szegedy, C. 2014. Explain-ing and harnessing adversarial examples.arXiv preprintarXiv:1412.6572.
\bibitem{23}
Madry, A.; Makelov, A.; Schmidt, L.; Tsipras, D.; andVladu, A. 2017a. Towards deep learning models resistantto adversarial attacks.arXiv preprint arXiv:1706.06083.
\bibitem{24}
D. W. Hansen and Q. Ji, “In the eye of the beholder: A survey of models for eyes and gaze,”IEEEtransactions on pattern analysis and machine intelligence, vol. 32, no. 3, pp. 478–500, 2009.
\bibitem{25}
Yunfei Liu, Ruicong Liu, Haofei Wang, and Feng Lu. Gen-eralizing gaze estimation with outlier-guided collaborativeadaptation. InProceedings of the IEEE/CVF InternationalConference on Computer Vision, pages 3835–3844, 2021.
\bibitem{26}
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.Deep residual learning for image recognition. InProceed-ings of the IEEE conference on computer vision and patternrecognition, pages 770–778, 2016
\bibitem{27}
Xucong Zhang, Seonwook Park, Thabo Beeler, DerekBradley, Siyu Tang, and Otmar Hilliges. Eth-xgaze: A largescale dataset for gaze estimation under extreme head poseand gaze variation. InEuropean Conference on ComputerVision, pages 365–381. Springer, 2020.
\bibitem{28}
Petr Kellnhofer, Adria Recasens, Simon Stent, Wojciech Ma-tusik, and Antonio Torralba. Gaze360: Physically uncon-strained gaze estimation in the wild. InProceedings of theIEEE/CVF International Conference on Computer Vision,pages 6912–6921, 2019.
\bibitem{29}
Kang Wang, Rui Zhao, Hui Su, and Qiang Ji. Generalizingeye tracking with bayesian adversarial learning. InProceed-ings of the IEEE/CVF Conference on Computer Vision andPattern Recognition, pages 11907–11916, 2019.
\bibitem{30}
Petr Kellnhofer, Adria Recasens, Simon Stent, Wojciech Ma-tusik, and Antonio Torralba. Gaze360: Physically uncon-strained gaze estimation in the wild. InProceedings of theIEEE/CVF International Conference on Computer Vision,pages 6912–6921, 2019
\bibitem{31}
Yihua Cheng, Yiwei Bao, and Feng Lu. Puregaze: Puri-fying gaze feature for generalizable gaze estimation.arXivpreprint arXiv:2103.13173, 2021.
\bibitem{32}
Yihua Cheng, Haofei Wang, Yiwei Bao, and Feng Lu.Appearance-based gaze estimation with deep learning: Areview and benchmark.arXiv preprint arXiv:2104.12668,2021.
\end{thebibliography}
\end{document}
