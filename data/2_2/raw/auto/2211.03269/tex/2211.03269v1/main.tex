\documentclass[11pt]{article}

\textheight 8.5truein
\parskip 0.1in
\topmargin 0.25in
\headheight 0in
\headsep 0in
\textwidth 6.5truein
\oddsidemargin  0in
\evensidemargin 0in
\renewcommand{\baselinestretch}{1.2}
\parindent 6pt

%\def\proof{\par\noindent{\em Proof.}}
%\def\endproof{\hfill $\Box$ \vskip 0.4cm}

%\documentclass[11pt, oneside]{article}      % use "amsart" instead of "article" for AMSLaTeX format
%\usepackage{fullpage}
%\usepackage[centering]{geometry}
%\geometry{letterpaper}                          % ... or a4paper or a5paper or ...

%\textwidth   6.5in
%\textheight  8.5 in
%\topmargin   0pt
%\marginparwidth 0pt \oddsidemargin  0pt \evensidemargin  0pt
%\marginparsep 0pt
%
%\geometry{landscape}                       % Activate for for rotated page geometry
%\usepackage[parfill]{parskip}          % Activate to begin paragraphs with an empty line rather than an indent

%\usepackage{natbib}
\usepackage[style=numeric,maxbibnames=99]{biblatex}
\addbibresource{ref.bib}
\usepackage{amsmath,amssymb,amsthm,color}
\usepackage{hyperref,url}
\usepackage{fancybox,graphicx} 
\usepackage[title]{appendix}
% Use pdf, png, jpg, or eps§ with pdflatex; use eps in DVI mode. TeX will automatically convert eps --> pdf in pdflatex
%\usepackage{natbib} % This part is for bibliography.

% \bibpunct[, ]{(}{)}{,}{a}{}{,}%
% \def\bibfont{\small}%
% \def\bibsep{\smallskipamount}%
% \def\bibhang{24pt}%

 \def\newblock{\ }%
 \def\BIBand{and}%

% New environment part


\newtheorem{fact}{Fact}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\newtheorem{claim}[theorem]{Claim}
\newtheorem{remark}[theorem]{Remark}
\newtheorem{example}{Example}
% Need the environment Proof ? Please use the following:
% {\noindent\bf Proof.}  ...........................  $\hfill \square$\\

% Bold type part.
\newcommand{\ba}{\mbox{\boldmath $a$}}
\newcommand{\bb}{\mbox{\boldmath $b$}}
\newcommand{\bc}{\mbox{\boldmath $c$}}
\newcommand{\bd}{\mbox{\boldmath $d$}}
\newcommand{\be}{\mbox{\boldmath $e$}}
\newcommand{\bff}{\mbox{\boldmath $f$}} %NOTICE!
\newcommand{\bg}{\mbox{\boldmath $g$}}
\newcommand{\bh}{\mbox{\boldmath $h$}}
\newcommand{\bp}{\mbox{\boldmath $p$}}
\newcommand{\bq}{\mbox{\boldmath $q$}}
\newcommand{\br}{\mbox{\boldmath $r$}}
\newcommand{\bs}{\mbox{\boldmath $s$}}
\newcommand{\bt}{\mbox{\boldmath $t$}}
\newcommand{\bu}{\mbox{\boldmath $u$}}
\newcommand{\bv}{\mbox{\boldmath $v$}}
\newcommand{\bw}{\mbox{\boldmath $w$}}
\newcommand{\bx}{\mbox{\boldmath $x$}}
\newcommand{\by}{\mbox{\boldmath $y$}}
\newcommand{\bz}{\mbox{\boldmath $z$}}
\newcommand{\bzero}{\mbox{\boldmath $0$}}

\newcommand{\bA}{\mbox{\boldmath $A$}}
\newcommand{\bB}{\mbox{\boldmath $B$}}
\newcommand{\bG}{\mbox{\boldmath $G$}}
\newcommand{\bC}{\mbox{\boldmath $C$}}
\newcommand{\bD}{\mbox{\boldmath $D$}}
\newcommand{\bI}{\mbox{\boldmath $I$}}
\newcommand{\bT}{\mbox{\boldmath $T$}}
\newcommand{\bM}{\mbox{\boldmath $M$}}
\newcommand{\bU}{\mbox{\boldmath $U$}}
\newcommand{\bV}{\mbox{\boldmath $V$}}
\newcommand{\bX}{\mbox{\boldmath $X$}}
\newcommand{\bY}{\mbox{\boldmath $Y$}}
\newcommand{\bZ}{\mbox{\boldmath $Z$}}


\newcommand{\balpha}{\mbox{\boldmath $\alpha$}}
\newcommand{\bbeta}{\mbox{\boldmath $\beta$}}
\newcommand{\btheta}{\mbox{\boldmath $\theta$}}
\newcommand{\bgamma}{\mbox{\boldmath $\gamma$}}
\newcommand{\bdelta}{\mbox{\boldmath $\delta$}}
\newcommand{\bmu}{\mbox{\boldmath $\mu$}}
\newcommand{\bepsilon}{\mbox{\boldmath $\epsilon$}}
\newcommand{\blambda}{\mbox{\boldmath $\lambda$}}
\newcommand{\btau}{\mbox{\boldmath $\tau$}}
\newcommand{\bsigma}{\mbox{\boldmath $\sigma$}}

\newcommand{\bSigma}{\mbox{\boldmath $\Sigma$}}

\newcommand{\real}[1]{\mbox{$\mathbb{R}^{#1}$}}
\newcommand{\recom}[1]{\mbox{$\mathbb{B}^{#1}$}}
\newcommand{\compl}[1]{\mbox{$\mathbb{C}^{#1}$}}
\newcommand{\realN}[1]{\mbox{$\real{I_1 \times I_2 \times \dots \times I_{#1}}$}}
\newcommand{\recomN}[1]{\mbox{$\recom{I_1 \times I_2 \times \dots \times I_{#1}}$}}
\newcommand{\complN}[1]{\mbox{$\compl{I_1 \times I_2 \times \dots \times I_{#1}}$}}
\newcommand{\complR}[1]{\mbox{$\compl{R_1 \times R_2 \times \dots \times R_{#1}}$}}
\newcommand{\prob}{\mbox{$\mathbb{P}$}}
\newcommand{\expect}{\mbox{$\mathbb{E}$}}
\newcommand{\var}{\mbox{$\mathbb{V}$}}
\newcommand{\rank}{\textnormal{rank}}
\newcommand{\half}{\frac{1}{2}}


% Enumerate part
\newcommand{\V}[1]{\mbox{\boldmath $#1$}}
\newcommand{\RV}[1]{\mbox{\boldmath $\tilde{#1}$}}
\newcommand{\rv}[1]{\tilde{#1}}
\newcommand{\OPT}{{\mbox{OPT}}}
\newcommand{\SP}{{\tt{SP}}}
\newcommand{\CVX}{{\tt{CVX}}}
\newcommand{\LP}{{\tt{LP}}}
\newcommand{\D}{{\tt{D}}}
\newcommand{\ALG}{\tt{ALG}}
\newcommand{\cT}{{\cal T}}
\newcommand{\XX}{{\cal X}}
\newcommand{\YY}{{\cal Y}}
\newcommand{\ZZ}{{\cal Z}}

\newcommand{\LL}{{\cal L}}
\newcommand{\FF}{{\cal F}}
\newcommand{\CC}{{\cal C}}
\newcommand{\KK}{{\cal K}}


%\newcommand{\arg}{\mbox{\rm arg}\,}
\newcommand{\A}{{\cal A}}
\newcommand{\RR}{\mathbb R}
\newcommand{\EE}{\mathbb E}
\newcommand{\Diag}{{\rm Diag}}
\newcommand{\tr}{{\rm tr \, }}


%Added on Feb 23, 2015: Roman numbers, chapters, DEFINE format
\newcommand{\rmnum}[1]{\romannumeral #1} % Roman small letter
\newcommand{\Rmnum}[1]{\uppercase\expandafter{\romannumeral #1}} % Roman capital letter
\usepackage[raggedright]{titlesec}% 可换为 raggedleft  raggedright
\titleformat{\chapter}{\centering\Huge\bfseries}{Chapter \Rmnum{\thechapter} }{1em}{} % in book format
\newcommand{\df}{\em } %{\bfseries \em}  % DEFINE format for text

%Added on April 30th, 2015:
\usepackage{mathrsfs} %math-script format, then use \mathscr{F}
\usepackage{enumerate} % change the symbols in enumerate
\graphicspath{{figures/}}

\newcommand{\tensorA}{\mbox{\boldmath $\mathcal{A}$}}
\newcommand{\tensorD}{\mbox{\boldmath $\mathcal{D}$}}
\newcommand{\tensorE}{\mbox{\boldmath $\mathcal{E}$}}
\newcommand{\tensorF}{\mbox{\boldmath $\mathcal{F}$}}
\newcommand{\tensorG}{\mbox{\boldmath $\mathcal{G}$}}
\newcommand{\tensorI}{\mbox{\boldmath $\mathcal{I}$}}
\newcommand{\tensorT}{\mbox{\boldmath $\mathcal{T}$}}
\newcommand{\tensorU}{\mbox{\boldmath $\mathcal{U}$}}
\newcommand{\tensorV}{\mbox{\boldmath $\mathcal{V}$}}
\newcommand{\tensorX}{\mbox{\boldmath $\mathcal{X}$}}
\newcommand{\tensorY}{\mbox{\boldmath $\mathcal{Y}$}}
\newcommand{\VI}{\mbox{VI}}

\usepackage{algorithm}
\usepackage{algorithmicx}
\usepackage{multirow}
\usepackage{amsmath}
\usepackage{stmaryrd}
\usepackage{float}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{xcolor}
\usepackage{algpseudocode}
\DeclareMathOperator*{\argmin}{argmin}
%\floatname{algorithm}{算法}
%\renewcommand{\algorithmicrequire}{\textbf{输入：}}
%\renewcommand{\algorithmicensure}{\textbf{输出：}}

%\newcommand{\natural}{\mbox{$\mathbb{N}$}}
\usepackage{color}
\newcommand{\tbw}{\bigskip \mbox{\color{red} {\df (TBW) }}\bigskip}


\begin{document}
%\title{Finite Sum VI and Optimization}
\title{ An Accelerated Variance Reduced Extra-Point Approach to \\ Finite-Sum VI and Optimization } 

\author{
Kevin Huang\thanks{Department of Industrial and System Engineering, University of Minnesota, huan1741@umn.edu}
\hspace{1cm}
Nuozhou Wang\thanks{Department of Industrial and System Engineering, University of Minnesota, wang9886@umn.edu}
\hspace{1cm}
Shuzhong Zhang\thanks{Department of Industrial and System Engineering, University of Minnesota, zhangs@umn.edu}
}

\date{\today}
\maketitle

\begin{abstract}
    In this paper, we develop stochastic variance reduced algorithms for solving a class of {\it finite-sum} monotone VI, where the operator consists of the sum of finitely many monotone VI mappings and the sum of finitely many monotone gradient mappings. We study the gradient complexities of the proposed algorithms under the settings when the sum of VI mappings is either strongly monotone or merely monotone. Furthermore, we consider the case when each of the VI mapping and gradient mapping is only accessible via noisy stochastic estimators and establish the sample gradient complexity. We demonstrate the application of the proposed algorithms for solving finite-sum convex optimization with finite-sum inequality constraints and develop a zeroth-order approach when only noisy and biased samples of objective/constraint function values are available.
    
    \vspace{3mm}

    \noindent\textbf{Keywords:} finite-sum optimization, stochastic gradient method, variational inequality, stochastic zeroth-order method.
\end{abstract}

\section{Introduction}
In machine learning research, a common optimization problem is the so-called finite-sum optimization:
\begin{eqnarray}
    \min\limits_{x\in\XX}\quad g(x):=\sum\limits_{i=1}^mg_i(x),\label{finite-sum-opt}
\end{eqnarray}
where the objective is the sum of finitely many (convex) loss functions. When the total number of functions is large, it can be costly for a deterministic gradient method to evaluate the gradients of all the functions in each iteration. A conventional way for solving the finite-sum model \eqref{finite-sum-opt} is through stochastic gradient descent (SGD), where in each iteration only one or a mini-batch of functions are randomly chosen and the corresponding gradients are estimated. While SGD may improve the overall gradient complexity over the deterministic methods, the iteration complexity to obtain an $\epsilon$-solution is only $\mathcal{O}\left(\frac{1}{\epsilon}\right)$ even if each of the function $g_i(x)$ is strongly convex and smooth. In order to further improve the gradient and iteration complexity, {\it variance reduced}\/ algorithms such as SAG \cite{schmidt2017minimizing}, SAGA \cite{defazio2014saga}, SVRG \cite{johnson2013accelerating} have been developed to achieve the gradient complexity $\mathcal{O}\left(\left(m+\frac{L}{\mu}\right)\log\frac{1}{\epsilon}\right)$, assuming each function $g_i(x)$ is strongly monotone with modulus $\mu>0$ and gradient Lipschitz continuous with constant $L\ge\mu$. Recently, {\it accelerated}\/ variance reduced algorithms such as Katyusha \cite{allen2017katyusha} and SSNM \cite{zhou2019direct} are proposed to achieve an even better gradient complexity $\mathcal{O}\left(\left(m+\sqrt \frac{mL}{\mu}\right)\log\frac{1}{\epsilon}\right)$, which matches the lower bound established in \cite{lan2018optimal}, hence optimal.

A specific branch in machine learning which has received much attention in recent years is training {Generative Adversarial Network} (GAN) \cite{goodfellow2014generative}. Different from an optimization model \eqref{finite-sum-opt}, training a GAN can be formulated as a minimax saddle point problem:
\begin{eqnarray}
\min\limits_{x\in\XX}\max\limits_{y\in\YY}\quad f(x,y).\label{saddle-prob}
\end{eqnarray}
When $f(\cdot,y)$ is convex for fixed $y\in\YY$ and $f(x,\cdot)$ is concave for fixed $x\in\XX$ and $\XX,\YY$ are closed convex sets, \eqref{saddle-prob} can be reformulated into a more general variational inequalities (VI) model:
\begin{eqnarray}
\mbox{find $x^*$ s.t.}\quad \langle F(x^*),x-x^*\rangle\ge0,\quad\forall x\in\ZZ,\label{VI-prob}
\end{eqnarray}
where $F(\cdot)$ is a general monotone vector mapping. Since GAN is known to be very difficult to train and the conventional (stochastic) gradient methods applied for deep learning do not perform well in practice, there has been a surge of interest in developing efficient gradient methods in the context of either saddle point problem or VI \cite{daskalakis2017training, mertikopoulos2018optimistic, liang2019interaction, gidel2018variational, huang2021new}. It is also natural to consider the finite-sum VI where $F(x)=\sum\limits_{i=1}^m F_i(x)$ and develop variance reduced algorithms applying techniques from finite-sum optimization. The authors in \cite{alacaoglu2022stochastic} incorporated such variance reduced techniques into various VI algorithms and established the gradient complexity $\mathcal{O}\left(m+\frac{\sqrt{m}L}{\epsilon}\right)$ for monotone VI and  $\mathcal{O}\left(\left(m+\frac{\sqrt{m}L}{\mu}\right)\log\frac{1}{\epsilon}\right)$ for strongly monotone VI, where {each} operator $F_i(x)$ is (strongly) monotone with modulus $\mu\, (>)\ge0$ and Lipschitz continuous with $L\ge\mu$. On the other hand, a lower gradient complexity bound has also been established in \cite{xie2020lower} with $\Omega\left(m+\frac{L}{\epsilon}\right)$ for convex-concave saddle point problem and $\Omega\left(\left(m+\frac{L}{\mu}\right)\log\frac{1}{\epsilon}\right)$ for strongly-convex-strongly-concave saddle point problem. Unlike the accelerated variance reduced algorithms in optimization \cite{allen2017katyusha, zhou2019direct} which have been proven to be optimal, 
%closing the gap 
there is still a gap between the upper and lower gradient complexity bounds for finite-sum VI. It remains an open problem to determine where the optimal gradient complexity bound actually lands. 

In this paper, we consider an extended class of monotone VI \eqref{VI-prob} in the finite-sum form, where the operator consists of the sum of finitely many general vector mapping $H_i(x)$ and the sum of finitely many gradient mapping $\nabla g_i(x)$:
\begin{eqnarray}
F(x) = H(x)+\nabla g(x):= \sum\limits_{i=1}^{m_1}H_i(x)+\sum\limits_{i=1}^{m_2}\nabla g_i(x),\label{finite-sum-VI-opt-prob}
\end{eqnarray}
where {each} $H_i(\cdot)$ is Lipschitz continuous with $L_{h(i)}$ and $H(\cdot)$ is (strongly) monotone with $\mu\, (>)\ge0$, and each $\nabla g_i(\cdot)$ is Lipschitz continuous with $L_{g(i)}$ and $\nabla g(\cdot)$ is monotone. The pioneering work considering such extended class of monotone VI \eqref{finite-sum-VI-opt-prob} without the finite-sum structure (i.e.\ $m_1=m_2=1$) is \cite{chen2017accelerated}, where the authors propose a stochastic accelerated mirror-prox method with iteration (sample) complexity
\[
\mathcal{O}\left(\sqrt\frac{L_g}{\epsilon}+\frac{L_h}{\epsilon}+\frac{\sigma^2}{\epsilon^2}\right)
\]
for monotone $H(\cdot)$ and $\nabla g(\cdot)$. Note that the subscript $i$ indicating the index in the finite-sum is omitted since $m_1=m_2=1$. In addition, the authors \cite{chen2017accelerated} consider the stochastic setting where both $H(\cdot)$ and $\nabla g(\cdot)$ can only be estimated via an unbiased oracle with bounded variance $\sigma^2$. In this paper, we continue along this line of research on the extended class of monotone VI \eqref{finite-sum-VI-opt-prob} with general $m_1,m_2$ and apply variance reduced techniques to establish accelerated gradient complexity bound. We assume each $H_i(\cdot)$ and $\nabla g_i(\cdot)$ can only be estimated via a stochastic oracle with bounded variance and bias and give the corresponding sample gradient complexity. We show that the proposed algorithms can be applied to solving finite-sum convex optimization with finite-sum inequality constraints \cite{lin2018level} with an improved gradient complexity. Furthermore, the general stochastic setting in this paper makes it possible to apply zeroth-order approach \cite{larson2019derivative} to solve the aforementioned problem with our algorithm, when only biased samples of objective/constraint function values are accessible. 

The rest of the paper is organized as follows. In Section \ref{sec:strong-monotone}, we propose a stochastic variance reduced algorithm for the extended class of VI \eqref{finite-sum-VI-opt-prob} when $H_i(\cdot)$ is strongly monotone. In Section \ref{sec:monotone}, we provide an alternative variance reduced method to solve the case when $H_i(\cdot)$ is only monotone. In Section \ref{sec:finite-sum-constrained}, we demonstrate the application to solving finite-sum convex optimization with finite-sum inequality constraints. We further extend this application to a more general black-box setting and demonstrate the implementation of our proposed algorithm via a zeroth-order approach. We present numerical results in Section \ref{sec:numerical} and conclude the paper in Section \ref{sec:conclusion}.



% \section{Literature Review}

% Variance reduced finite sum optimization: \cite{allen2017katyusha, zhou2019direct,kovalev2020don}.

% \bigskip
% Variance reduced finite sum VI: \cite{alacaoglu2022stochastic}.

% \bigskip
% Finite sum minimax lower bound:
% \cite{xie2020lower}.

% \bigskip
% Finite sum constrained finite sum optimization: \cite{lin2018level}.

\section{Variance Reduced Scheme for Finite-Sum Strongly Monotone VI 
and %with 
Finite-Sum Monotone Gradients}
\label{sec:strong-monotone}

In this section, we present our first variance reduced scheme for solving VI \eqref{VI-prob}, where the operator $F(\cdot)$ takes the combined VI/gradient mapping form with finite-sum structure respectively \eqref{finite-sum-VI-opt-prob}. We assume the constraint set $\ZZ$ is closed and convex, and the problem is summarized below:
\begin{eqnarray}
\left\{
\begin{array}{ll}
     \mbox{find $x^*$ s.t.}\quad \langle F(x^*),x-x^*\rangle\ge0,\quad\forall x\in\ZZ, \\
    % \\
     F(x) = H(x)+\nabla g(x):= \sum\limits_{i=1}^{m_1}H_i(x)+\sum\limits_{i=1}^{m_2}\nabla g_i(x).
\end{array}
\right.\label{finite-sum-vi-opt-prob-summarize}
\end{eqnarray}

We specifically consider the combined finite-sum operator $F(\cdot)$ being strongly monotone in this section, and we shall propose an alternative approach for $F(\cdot)$ being merely monotone in the next section. In particular, we assume $H(\cdot)$ to be strongly monotone with modulus $\mu_h>0$, and $\nabla g(\cdot)$ to be monotone. Furthermore, we denote $H(\cdot)=\sum\limits_{i=1}^{m_1}H_i(\cdot)$, where each $H_i(\cdot)$ is Lipschitz continuous with constant $L_{h(i)}$, and denote $g(x)=\sum\limits_{i=1}^{m_2}g_i(x)$ (therefore $\nabla g(x)=\sum\limits_{i=1}^{m_2}\nabla g_i(x)$) where each $\nabla g_i(x)$ is Lipschitz continuous with constant $L_{g(i)}$. Let us also define the sum of the Lipschitz constants $L_h:=\sum\limits_{i=1}^{m_1}L_{h(i)}$ and $L_g:=\sum\limits_{i=1}^{m_2}L_{g(i)}$. 
        
Consider the following update for iteration count $k$:
\begin{equation}
\left\{
\begin{array}{lcl}
     \Bar{x}^k&=& (1-p_1)x^k+p_1w^k\\
     %&&\\
     {y^k} &=& (1-\alpha-\beta)v^k+\alpha x^k+\beta\bar w^k\\
     %&&\\
     x^{k+0.5}&=&\arg\min\limits_{x\in\mathcal{Z}}\quad\gamma\langle {H'}(w^k)+{\tilde\nabla {g'}(y^k)},x-\Bar{x}^k\rangle+\frac{1}{2}\|x-\Bar{x}^k\|^2\\
     %&&\\
     x^{k+1} &=& \arg\min\limits_{x\in\mathcal{Z}}\quad\gamma\langle \hat {H'}(x^{k+0.5})+{\tilde\nabla {g'}(y^k)},x-\Bar{x}^k\rangle+\frac{1}{2}\|x-\Bar{x}^k\|^2\\
     %&&\\
    %  \bar v^{k+1} &=& (1-p_2)v^{k}+p_2\bar w^{k}\\
    %  &&\\
     { v^{k+1}} &=& {(1-\alpha-\beta) v^{k}+\alpha x^{k+0.5}+\beta\bar w^k}\\
     %&&\\
     w^{k+1}&=&\left\{
     \begin{array}{ll}
          x^{k+1},& \mbox{with prob. $p_1$} \\
          w^k,    & \mbox{with prob. $1-p_1$}
    \end{array}\right.\\
    % &&\\
     \bar w^{k+1}&=&\left\{
     \begin{array}{ll}
          v^{k+1},  & \mbox{with prob. $p_2$} \\
          \bar w^k, & \mbox{with prob. $1-p_2$}.
       
     \end{array}
     \right.
\end{array}
\right.\label{VR-VI-Opt-Strong-Update}
\end{equation}

Method \eqref{VR-VI-Opt-Strong-Update} is a general {\it stochastic variance reduced} scheme for solving \eqref{finite-sum-vi-opt-prob-summarize}, and in the rest of the paper we refer to it as {\bf S}tochastic {\bf A}ccelerated {\bf V}ariance {\bf R}educed {\bf E}xtra {\bf P}oint method (SAVREP). We shall make the following remarks. First, the variance reduction techniques are applied to {\it both} the general VI operator $H(\cdot)$ and the gradient mapping $\nabla g(\cdot)$, and the resulting update procedure will require using the variance reduced gradient estimator \cite{allen2017katyusha, alacaoglu2022stochastic}, denote by $\hat H(\cdot)$ and $\tilde\nabla g(\cdot)$, respectively. Second, although the construction of the variance reduced gradient estimator $\hat H(\cdot)$ ($\tilde\nabla g(\cdot)$) involves sampling from the $m_1$ ($m_2$) individual operators $H_i(\cdot)$ ($\nabla g_i(\cdot)$) as we shall see later, we use the term ``stochastic'' to specifically refer to the fact that the update of SAVREP \eqref{VR-VI-Opt-Strong-Update} only accesses the {\it noisy} estimations of the individual operators, denote by $H'_i(\cdot)$ and $\nabla g'_i(\cdot)$, respectively. This allows the application of zeroth-order approach \cite{larson2019derivative} to problems when the gradients are unavailable and only the function values can be sampled, such as black-box optimization \cite{nesterov2017random, shalev2011online, duchi2015optimal, shamir2017optimal} and saddle-point problem \cite{wang2020zeroth,xu2020gradient,liu2019min,roy2019online,menickelly2020derivative}. We shall exemplify such application in Section \ref{sec:finite-sum-constrained}. Finally, the multiple-sequence structure of \eqref{VR-VI-Opt-Strong-Update} is the key to achieve the overall accelerated variance reduced gradient complexity in terms of both $L_h$ and $L_g$. While the sequences $\{x^{k+0.5}\},\{x^k\}$ in general take the extra-gradient form of update, the sequences $\{w^k\},\{\bar w^k\}$ help maintain the single-loop structure for variance reduction \cite{alacaoglu2022stochastic}; the sequences $\{y^k\},\{v^k\}$ help improve the constants related to the gradient mapping, and the sequence $\{\bar x^k\}$ facilitates the variance reduction for the VI operator $H(\cdot)$. The derivations of gradient complexity and sample complexity involving the analysis of each of these sequences are discussed in Section \ref{sec:grad-comp-strong}, following Section \ref{sec:pre-strong} where the detailed formulations of the (stochastic) variance reduced gradient estimators and the corresponding assumptions are presented.

\subsection{Preliminaries}
\label{sec:pre-strong}

We first state the assumptions for the stochastic estimators $H'_i(\cdot)$ $(\nabla g'_i(\cdot))$ of the individual operators $H_i(\cdot)$ $(\nabla g_i(\cdot))$ for $i=1,2,...,m_1\,(m_2)$. Denote $\EE'[\cdot]$ as the expectation taken for these samples and consider the following bias and variance upper bounds:
%Let $H'_i(x)$ (respectively ${\nabla g_i'(x)}$) be a stochastic estimation of the $i$-th operator $H_i(x)$ (respectively ${\nabla g_i(x)}$) at $x$, assume:
\begin{eqnarray}
&&\left\|H_i(x)-\EE'[H'_i(x)]\right\|\le\delta_h,\quad \EE'\left[\left\|H_i'(x)-\EE'\left[H'_i(x)\right]\right\|^2\right]\le \sigma_h^2,\label{sto-bd-H-1}\\
&& {\left\|\nabla g_i(x)-\EE'\left[\nabla g_i'(x)\right]\right\|\le \delta_g,}\quad {\EE'\left[\left\|\nabla g_i'(x)-\EE'\left[\nabla g_i'(x)\right]\right\|^2\right]\le\sigma_g^2},\label{sto-bd-g-1}
\end{eqnarray}
for some $\delta_h,\sigma_h^2,{\delta_g,\sigma_g^2}\ge0$. In other words, we assume the variance and the bias of these samples are upper bounded by some non-negative constants (therefore they are not necessarily unbiased estimators). Denote $H'(x):=\sum\limits_{i=1}^{m_1}H'_i(x)$ (respectively ${\nabla g'(x):=\sum\limits_{i=1}^{m_2}\nabla g_i'(x)}$) as the sum of $m_1$ (respectively ${m_2}$) such independent stochastic oracles. The below bounds follow straightforwardly:
\begin{eqnarray}
&&\left\|H(x)-\EE'[H'(x)]\right\|\le m_1\delta_h,\quad \EE'\left[\left\|H(x)-H'(x)\right\|^2\right]\le 2m_1\sigma_h^2+2m_1^2\delta_h^2,\label{sto-bd-H-2}\\
&&{\left\|\nabla g(x)-\EE'[\nabla g'(x)]\right\|\le m_2\delta_g},\quad {\EE'\left[\left\|\nabla g(x)-\nabla g'(x)\right\|^2\right]\le 2m_2\sigma_g^2+2m_2^2\delta_g^2}.\label{sto-bd-g-2}
\end{eqnarray}

We next give explicit expressions for the (noiseless) variance reduced gradient estimators at the corresponding iterates given in \eqref{VR-VI-Opt-Strong-Update}:
\begin{eqnarray}
&&\hat{{H}}(x^{k+0.5}):={H}(w^k)+{H}_{\xi_k}(x^{k+0.5})-{H}_{\xi_k}(w^k)\label{VR-grad-H}\\
&& \tilde\nabla {g}(y^k):=\nabla {g}(\bar w^k)+\nabla {g_{\zeta_k}}(y^k)-\nabla {g_{\zeta_k}}(\bar w^k).\label{VR-grad-g}
\end{eqnarray}
The above forms follow from the well-established variance reduction literature \cite{allen2017katyusha, alacaoglu2022stochastic}, and the random variables $\xi\,(\zeta)$ take samples from the $m_1\,(m_2)$ individual operators $H_i(\cdot)\,(\nabla g_i(\cdot))$ with probability distribution taking respective Lipschitz constants $L_{h(i)}\,(L_{g(i)})$ into account. In particular, we have
\[
\mbox{Pr}\{\xi=i\}=\frac{L_{h(i)}}{L_h} \, := q_i,\,\, i=1,2,...,m_1, \quad \mbox{Pr}\{\zeta=i\}=\frac{L_{g(i)}}{L_g}\, := \pi_i,\,\, i=1,2,...,m_2.
\]
% and
% \[
% \mbox{Pr}\{\zeta=i\}=\frac{L_{g(i)}}{L_g}\, := \pi_i,\,\, i=1,2,...,m_2.
% \]
The stochastic oracles are then given by $H_{\xi}(\cdot):=\frac{1}{q_i}H_i(\cdot)$ and $\nabla g_{\zeta}(\cdot)=\frac{1}{\pi_i}\nabla g_i(\cdot)$.

However, note that in the update \eqref{VR-VI-Opt-Strong-Update}, only the noisy variance reduced gradient operators $\hat H'(x^{k+0.5})$ $(\tilde\nabla g'(y^k))$ are accessed, which are defined by:
\begin{eqnarray}
&&\hat{{H'}}(x^{k+0.5}):={H'}(w^k)+{H}_{\xi_k}'(x^{k+0.5})-{H}_{\xi_k}'(w^k)\nonumber\\
&& \tilde\nabla {g'}(y^k):=\nabla {g'}(\bar w^k)+\nabla {g_{\zeta_k}'}(y^k)-\nabla {g_{\zeta_k}'}(\bar w^k),\nonumber
\end{eqnarray}
where $H_{\xi}'(\cdot):=\frac{1}{q_i}H_i'(\cdot)$ and $\nabla g_{\zeta}'(\cdot)=\frac{1}{\pi_i}\nabla g_i'(\cdot)$. To save the computational costs, we can reuse the noisy samples estimated at the same iterate within each iteration. For example, after sampling $H'(w^k)$ in the update of $x^{k+0.5}$, we could reuse the oracles for $H_{\xi_k}'(w^k)$ and $H'(w^k)$ in constructing $\hat H'(x^{k+0.5})$.

% \begin{eqnarray}
% \EE_{\xi}\left[\|H_{\xi}(x)-H_{\xi}(y)\|^2\right]&=&\sum\limits_{i=1}^{m_1}\frac{1}{q_i}\|H_i(x)-H_i(y)\|^2\nonumber\\
% &\le&\sum\limits_{i=1}^{m_1}\frac{L_{h(i)}^2}{q_i}\|x-y\|^2=\left(\sum\limits_{i=1}^{m_1}L_{h(i)}\right)^2\|x-y\|^2=L_h^2\|x-y\|^2.\nonumber
% \end{eqnarray}
%{\color{blue}(Note that if we choose $\xi$ to be uniform distributed, the Lipschitz constant of the stochastic oracle will become $\sqrt{m_1\sum\limits_{i=1}^{m_1}L^2_{h(i)}}$.)}

Finally, to simplify the notations in the following analysis, denote the expressions of conditional expectations taken for different random variables:
\begin{eqnarray}
&&\EE_{k_1}[\cdot]:=\EE_{\xi_k}[\cdot|x^k,w^k],\quad \EE_{k_2}[\cdot]:=\EE_{\zeta_k}[\cdot|x^k,\bar w^k,v^k],\label{expectations-1}\\
&& \EE_{k_1+}[\cdot]:=\EE_{\xi_k}[\cdot|x^{k+1},w^k],\quad \EE_{k_2+}[\cdot]:=\EE_{\zeta_k}[\cdot|\bar w^k,v^{k+1}]. \label{expectations-2}
\end{eqnarray}



\subsection{Gradient complexity analysis}
\label{sec:grad-comp-strong}

The overall analysis for gradient complexity of the proposed SAVREP \eqref{VR-VI-Opt-Strong-Update} can be largely divided into three parts. In the first part, the stochastic gradient mapping $\tilde\nabla g'(y^k)$ is viewed as a constant vector mapping, and we establish the relation among the sequences $\{\bar x^k\}$, $\{x^{k+0.5}\}$, $\{x^k\}$, $\{w^k\}$, which are related to the general VI mapping $H(\cdot)$. In the second part, we turn to focus on the sequences $\{y^k\}$, $\{v^k\}$, $\{\bar w^k\}$, and establish their relation in terms of the function value $g(\cdot)$. Finally, the results in the previous two parts are combined to show the per-iteration convergence in terms of a potential function. By selecting the parameters carefully, we derive the resulting gradient complexity for obtaining an $\epsilon$-solution $\EE\left[\|x^k-x^*\|^2\right]\le\epsilon$, together with the corresponding stochastic errors.
The lemma below summarizes the results from the first part of the analysis.

\begin{lemma}
\label{lem:VI-relation-1}
For the iterates generated by \eqref{VR-VI-Opt-Strong-Update}, define the following stochastic error terms:
\[
\varepsilon_x:=\|H_{\xi}(x)-H_{\xi}'(x)\|,\quad \bar \varepsilon_x:= \|H(x)-H'(x)\|.
\]
Then, the following inequality holds for any $x\in\ZZ$ and $k=0,1,2,...$
\begin{eqnarray}
&&\EE_{k_1}\left[\gamma\langle H(x)+\tilde\nabla {g'}(y^k),x^{k+0.5}-x\rangle\right]\nonumber\\
&\le& \frac{1}{2}\EE_{k_1}\left[(1-p_1-{\frac{1}{2}}\gamma\mu_h)\|x^k-x\|^2+p_1\|w^k-x\|^2-\|x^{k+1}-x\|^2\right]\nonumber\\
&&-\frac{1}{2}\left(p_1-{2}\gamma^2L_h^2\right)\EE_{k_1}\left[\|x^{k+0.5}-w^k\|^2\right]-\frac{1}{2}\left(1-p_1-\gamma\mu_h\right)\EE_{k_1}\left[\|x^{k+0.5}-x^k\|^2\right]\nonumber\\
&&{+\frac{\gamma \bar\varepsilon_{x^{k+0.5}}^2}{2\mu_h}+\gamma^2\EE_{k_1}\left[(\varepsilon_{x^{k+0.5}}+\varepsilon_{w^k})^2\right]}.\nonumber
\end{eqnarray}

\begin{proof}
See Appendix \ref{proof:VI-relation-1}.
\end{proof}
\end{lemma}
% \begin{proof}
% See Appendix \ref{proof:VI-relation-1}.
% \end{proof}
In Lemma \ref{lem:VI-relation-1}, we define two stochastic error terms, $\varepsilon_x$ and $\bar\varepsilon_x$, which are due to the noisy samples of $H_i(\cdot)$. We can first bound the squared stochastic error $\bar\varepsilon_x^2$ for the total operator $H(\cdot)$ with our assumption in \eqref{sto-bd-H-2}:
\[
\EE'[\bar\varepsilon_x^2]\le 2m_1\sigma_h^2+2m_1^2\delta_h^2.
\]
On the other hand, the error $\varepsilon_x$ involves a random variable $\xi$ sampled from $i=1,...,m_1$. Since
\begin{eqnarray}
\EE_{\xi}\left[\varepsilon_x^2\right]= \sum\limits_{i=1}^{m_1}q_i\cdot\frac{1}{q_i^2}\|H_i'(x)-H_i(x)\|^2=\sum\limits_{i=1}^{m_1}\frac{1}{q_i}\|H_i'(x)-H_i(x)\|^2, \nonumber
\end{eqnarray}
we have
\begin{eqnarray}
\EE'\left[\varepsilon_x^2\right]=\EE'\left[\EE_{\xi}\left[\varepsilon_x^2\right]\right]&\le& \sum\limits_{i=1}^{m_1}\frac{2}{q_i}\EE'\left[\left\|H_i'(x)-\EE'\left[H_i'(x)\right]\right\|^2+\left\|\EE'\left[H_i'(x)\right]-H_i(x)\right\|^2\right]\nonumber\\
&\le&2(\sigma_h^2+\delta_h^2)\cdot\sum\limits_{i=1}^{m_1}\frac{1}{q_i}=2L_h\cdot(\sigma_h^2+\delta_h^2)\cdot\sum\limits_{i=1}^{m_1}\frac{1}{L_{h(i)}}:=\tilde\sigma_h^2. \label{sto-epsilon-x-bd}
\end{eqnarray}

Now we shall proceed to present the results in the second part of the analysis, summarized in the next lemma.
\begin{lemma}
\label{lem:function-relation-1}
For the iterates generated by \eqref{VR-VI-Opt-Strong-Update}, define the following stochastic error terms:
\[
\rho_x:=\|\nabla g_{\zeta}(x)-\nabla g_{\zeta}'(x)\|,\quad \bar \rho_x:=\|\nabla g(x)-\nabla g'(x)\|.
\]
With the condition $1-\alpha-\beta\ge0$, the following inequality holds for any $x\in\ZZ$ and $k=0,1,2,...$
\begin{eqnarray}
\EE_{k_2}\left[g(v^{k+1})-g(x)\right]&\le& \EE_{k_2}\left[(1-\alpha-\beta)\left(g(v^k)-g(x)\right)+\beta \left(g(\bar w^k)-g(x)\right)\right]\nonumber\\
&&+\EE_{k_2}\left[\alpha\langle\tilde\nabla g'(y^k),x^{k+0.5}-x\rangle\right]+{\left(\frac{\alpha^2L_g}{2}+\frac{\alpha^2L_g}{2\beta}+\frac{\alpha\mu_h}{8}\right)}\EE_{k_2}\left[\|x^{k+0.5}-x^k\|^2\right]\nonumber\\
&&+{\frac{\alpha\mu_h}{8}\EE_{k_2}\left[\|x^k-x\|^2\right]+\frac{8\alpha}{\mu_h}\EE_{k_2}[\bar\rho_{\bar w^k}^2]+\frac{8\alpha}{\mu_h}\EE_{k_2}\left[(\rho_{\bar w^k}+\rho_{y^k})^2\right]}.\nonumber
\end{eqnarray}

\begin{proof}
See Appendix \ref{proof:function-relation-1}.
\end{proof}
\end{lemma}
% \begin{proof}
% See Appendix \ref{proof:function-relation-1}.
% \end{proof}

Similarly, we define the two stochastic error terms in Lemma \ref{lem:function-relation-1}, $\rho_x$ and $\bar\rho_x$, which are due to the noisy samples of the gradient mapping $\nabla g_i(\cdot)$. The bound for $\bar\rho_x^2$ follows directly from \eqref{sto-bd-g-2}:
\begin{eqnarray}
\EE'[\bar\rho_x^2]\le 2m_2\sigma_g^2+2m_2^2\delta_g^2,\nonumber
\end{eqnarray}
whereas the bound for $\rho_x^2$ can be derived as follows
\begin{eqnarray}
&&\EE'\left[\EE_{\zeta}[\rho_x^2]\right]\nonumber\\
&=&\EE'\left[\EE_{\zeta}\left[\|\nabla g_{\zeta}(x)-\nabla g_{\zeta}'(x)\|^2\right]\right]\nonumber\\
&=& \EE'\left[\sum\limits_{i=1}^{m_2}\pi_i\cdot\frac{1}{\pi_i^2}\|\nabla g_i(x)-\nabla g'_i(x)\|^2\right]\nonumber\\
&\le& \sum\limits_{i=1}^{m_2}\frac{2}{\pi_i}\EE'\left[\left\|\nabla {g}_{i}( x)-\EE'\left[\nabla {g}_{i}'(x)\right]\right\|^2+\left\|\EE'\left[\nabla {g}_{i}'( x)\right]-\nabla {g}_{i}'( x)\right\|^2\right]\nonumber\\
&\le&2(\sigma_g^2+\delta_g^2)\cdot\sum\limits_{i=1}^{m_2}\frac{1}{\pi_i}=2L_g\cdot(\sigma_g^2+\delta_g^2)\cdot\sum\limits_{i=1}^{m_2}\frac{1}{L_{g(i)}}:=\tilde\sigma_g^2.\label{sto-rho-x-bd}
\end{eqnarray}

The last part of the analysis will combine the results from Lemma \ref{lem:VI-relation-1} and Lemma \ref{lem:function-relation-1} and establish the overall per-iteration convergence in terms of a potential function. Let us first define the following function, which serves as an important component in our potential function:
\begin{eqnarray}
Q(x';x):= \langle H(x),x'-x\rangle+g(x')-g(x).\nonumber
\end{eqnarray}
In particular, we will use the function $Q(x';x^*)$ with $x'$ being the iterates generated by SAVREP \eqref{VR-VI-Opt-Strong-Update}. The following properties show that $Q(x';x^*)$ is nonnegative for any $x'\in\ZZ$ and is upper-bounded in terms of $x'$:
\begin{eqnarray}
Q(x';x^*)=\langle H(x^*),x'-x^*\rangle+g(x')-g(x^*)\ge \langle H(x^*)+\nabla g(x^*),x'-x^*\rangle\ge0.\label{positive-Q-star}
\end{eqnarray}
and
\begin{eqnarray}
Q(x';x^*)&=& \langle H(x^*),x'-x^*\rangle+g(x')-g(x^*)\nonumber\\
&\le& \langle H(x'),x'-x^*\rangle-\mu_h\|x'-x^*\|^2+g(x')-g(x^*)\nonumber\\
&\le& \langle H(x')+\nabla g(x'),x'-x^*\rangle-\mu_h\|x'-x^*\|^2\le \frac{1}{4\mu_h}\left\|H(x')+\nabla g(x')\right\|^2.\nonumber
\end{eqnarray}
Now we are ready to show the per-iteration convergence for \eqref{VR-VI-Opt-Strong-Update}:
\begin{theorem}
\label{thm:per-iter-conv-1}
For the iterates generated by \eqref{VR-VI-Opt-Strong-Update}, define the following constants:
\begin{eqnarray}
\Delta_h:=\frac{\alpha }{\mu_h}(m_1\sigma_h^2+m_1^2\delta_h^2)+2\alpha\gamma\tilde\sigma_h^2,\quad\Delta_g:=\frac{16\alpha}{\mu_h}(m_2\sigma_g^2+m_2^2\delta_g^2)+\frac{16\alpha}{\mu_h}\tilde\sigma_g^2.\nonumber
\end{eqnarray}
Then, the following inequality holds for $k=0,1,2,...$
\begin{eqnarray}
&&\EE\left[(1-\phi p_2)Q(v^{k+1};x^*)+\phi Q(\bar w^{k+1};x^*)\right]+\frac{\alpha}{2\gamma}\EE\left[(1-p_1)\|x^{k+1}-x^*\|^2+\|w^{k+1}-x^*\|^2\right]\nonumber\\
&\le& \EE\left[(1-\alpha-\beta)Q(v^k;x^*)+(\beta+\phi(1-p_2)) Q(\bar w^k;x^*)\right]\nonumber\\
&&+\left(1-\frac{\gamma\mu_h}{12}\right)\frac{\alpha}{2\gamma}\EE\left[\left(1-p_1\right)\|x^k-x^*\|^2+\|w^k-x^*\|^2\right]+\Delta_h+\Delta_g.\label{potential-reduce-sto-error-2}
\end{eqnarray}

\begin{proof}
See Appendix \ref{proof:per-iter-conv-1}.
\end{proof}
\end{theorem}
% \begin{proof}
% See Appendix \ref{proof:per-iter-conv-1}.
% \end{proof}
Theorem \ref{thm:per-iter-conv-1} establishes the relation for the iterates generated by \eqref{VR-VI-Opt-Strong-Update}, with additional stochastic errors $\Delta_h$, $\Delta_g$ due to the noisy samples taken for $H_i(\cdot)$ and $\nabla g_i(\cdot)$. To further derive the gradient complexity and the overall stochastic errors, we are left with specifying the parameters $\alpha,\beta,\gamma,\phi,p_1,p_2$. Note that in deriving \eqref{potential-reduce-sto-error-2}, we have imposed the constraints \eqref{const-2} on some of the parameters (see Appendix \ref{proof:per-iter-conv-1}), together with the condition $1-\alpha-\beta\ge0$ in Lemma \ref{lem:function-relation-1}, which should be honored during the parameter selection process. We summarize the gradient complexity results in the next proposition:
\begin{proposition}
\label{prop:grad-complexity-1}
In view of Theorem \ref{thm:per-iter-conv-1}, by specifying the following parameters:
\begin{eqnarray}
\gamma=\frac{1}{4}\min\left(\frac{\sqrt{p_1}}{L_h},\sqrt\frac{p_2}{L_g\mu_h},\frac{p_1}{\mu_h}\right),\quad \alpha=\frac{1}{12}\min\left(\sqrt{\frac{\mu_h}{L_gp_2}},1\right),\quad \beta=\frac{1}{2},\nonumber
\end{eqnarray}
and
\[
\phi = \frac{(1+\alpha)m_2}{2},\quad p_1=\frac{1}{m_1},\quad p_2 = \frac{1}{m_2},
\]
the gradient complexity for reducing the deterministic errors to some $\epsilon>0$ is %given by
\begin{eqnarray}
\mathcal{O}
\left(\left(m_1+m_2+\sqrt{\frac{L_gm_2}{\mu_h}}+\frac{L_h\sqrt{m_1}}{\mu_h}\right)\log\frac{d_0}{\epsilon}\right)\label{grad-complexity-strong},
\end{eqnarray}
where
\[
d_0:=\frac{\gamma}{\alpha\mu_h}\left\|H(x^0)+\nabla g(x^0)\right\|^2+2\|x^0-x^*\|^2.
\]
In addition, the overall stochastic error after reducing the deterministic error to $\epsilon$ is of the order %will have the order
\begin{eqnarray}
\mathcal{O}\left(\left(m_1+m_2+\sqrt{\frac{L_gm_2}{\mu_h}}+\frac{L_h\sqrt{m_1}}{\mu_h}\right)\cdot\frac{\gamma}{\alpha}\cdot(\Delta_h{+\Delta_g})\right).\label{sto-error-order-strong}
\end{eqnarray}

\begin{proof}
See Appendix \ref{proof:grad-complexity-1}.
\end{proof}
\end{proposition}
% \begin{proof}
% See Appendix \ref{proof:grad-complexity-1}.
% \end{proof}

A few remarks are in order to interpret the results in Proposition \ref{prop:grad-complexity-1}.

\begin{remark}
Under the noiseless case where $H_i(\cdot)$ and $\nabla g_i(\cdot)$ can be computed exactly ($\delta_h=\sigma_h=\delta_g=\sigma_g=0$), \eqref{grad-complexity-strong} gives the iteration complexity before reaching either $\|x^k-x^*\|^2\le\epsilon$ or $\|w^k-w^*\|^2\le\epsilon$. Since in each iteration the full operator $H(\cdot)$ ($\nabla g(\cdot)$) is estimated at $w^k/\bar w^k$, which in expectation only updates every $m_1$ ($m_2$) iterations, the expected cost for estimating an individual operator $H_i(\cdot)$ ($\nabla g_i(\cdot)$) is constant. Therefore, \eqref{grad-complexity-strong} is also the gradient complexity for obtaining the $\epsilon$-solution.
\end{remark}

For a general strongly monotone VI, \cite{alacaoglu2022stochastic} has established the $\mathcal{O}\left(\left(m_1+\frac{L_h\sqrt{m_1}}{\mu_h}\right)\log\frac{1}{\epsilon}\right)$ gradient complexity, while for strongly convex optimization \cite{allen2017katyusha, zhou2019direct} the gradient complexity \\ $\mathcal{O}\left(\left(m_2+\sqrt{\frac{L_gm_2}{\mu_g}}\right)\log\frac{1}{\epsilon}\right)$ has been established. While the former gradient complexity has not been shown tight for VI, Proposition \ref{prop:grad-complexity-1} implies that it is indeed possible to improve upon the previous results and reflect the accelerated complexity from optimization, when the VI is of the specific form \eqref{finite-sum-vi-opt-prob-summarize}.

\begin{remark}
Under the noisy case when the operators can only be estimated inexactly, the stochastic error $\Delta_h+\Delta_g$ will be carried throughout the iterations. Provided that the total number of iterations is in the order \eqref{grad-complexity-strong}, the overall error is given by $\epsilon+\Delta_{T}$, where we refer to $\epsilon$ as the ``deterministic error''. The order of the overall ``stochastic error'' $\Delta_T$, is then given by \eqref{sto-error-order-strong}. Through standard techniques such as increasing the sample size for $H'_i(\cdot)$ ($\nabla g'_i(\cdot)$), the overall stochastic error $\Delta_T$ can be further reduced to $\mathcal{O}(\epsilon)$.
\end{remark}
% \begin{remark}
% \begin{enumerate}
%     \item Under the noiseless case where $H_i(\cdot)$ and $\nabla g_i(\cdot)$ can be computed exactly ($\delta_h=\sigma_h=\delta_g=\sigma_g=0$), \eqref{grad-complexity-strong} gives the iteration complexity before reaching either $\|x^k-x^*\|^2\le\epsilon$ or $\|w^k-w^*\|^2\le\epsilon$. Since in each iteration the full operator $H(\cdot)/\nabla g(\cdot)$ is estimated at $w^k/\bar w^k$, which in expectation only updates every $m_1/m_2$ iterations, the expected cost for estimating an individual operator $H_i(\cdot)/\nabla g_i(\cdot)$ is constant. Therefore, \eqref{grad-complexity-strong} is also the gradient complexity for obtaining the $\epsilon$-solution. For a general strongly monotone VI, \cite{alacaoglu2022stochastic} has established the $\mathcal{O}\left(\left(m_1+\frac{L_h\sqrt{m_1}}{\mu_h}\right)\log\frac{1}{\epsilon}\right)$ gradient complexity, while for strongly convex optimization \cite{allen2017katyusha, zhou2019direct} the gradient complexity $\mathcal{O}\left(\left(m_2+\sqrt{\frac{L_gm_2}{\mu_g}}\right)\log\frac{1}{\epsilon}\right)$ has been established. While the former gradient complexity hasn't been shown tight for VI, Proposition \ref{prop:grad-complexity-1} implies that it is indeed possible to improve upon the previous results and reflect the accelerated complexity from optimization, when the VI is of the specific form \eqref{finite-sum-vi-opt-prob-summarize}.
%     \item On the other hand, when the operators can only be estimated inexactly, the stochastic error $\Delta_h+\Delta_g$ will be carried throughout the iterations. Provided that the total number of iterations is in the order \eqref{grad-complexity-strong}, the overall error is given by $\epsilon+\Delta_{T}$, where we refer to $\epsilon$ as the ``deterministic error''. The order of the overall ``stochastic error'' $\Delta_T$, is then given by \eqref{sto-error-order-strong}. Through standard techniques such as increasing the sample size for $H'_i(\cdot)/\nabla g'_i(\cdot)$, the overall stochastic error $\Delta_T$ can be further reduced to $\mathcal{O}(\epsilon)$.
% \end{enumerate}
% \end{remark}

\section{Variance Reduced Scheme for Finite-Sum Monotone VI 
and %with 
Finite-Sum Monotone Gradients}
\label{sec:monotone}

%The original variance reduced method has a design of a double loop; see~\cite{johnson2013accelerating,allen2017katyusha}. At the beginning of the outer loop, the gradient is calculated exactly. The gradient is used to help get the gradient estimator in the inner loop. In \cite{kovalev2020don} the loopless variants of both SVRG and Katyusha were developed to simplify the convergence analysis. However, to the best of our knowledge, the loopless technique has not been used in the non-strongly convex finite sum optimization. While the loopless technique has been used in the previous section, we apply the double loop design in this section.


In this section, we develop a new algorithm for the same finite-sum monotone VI in the form \eqref{finite-sum-vi-opt-prob-summarize}, but now we only assume $H(\cdot)$ to be monotone instead of strongly monotone, i.e.\ $\mu_h=0$ (the monotone assumption for $\nabla g(\cdot)$ remains). The loss of strong monotonicity assumption therefore requires a different design of update procedure and analysis from the previous section, as we shall present shortly later.  Same as in %Identical with 
the previous section, we define $H(\cdot)=\sum\limits_{i=1}^{m_1}H_i(\cdot)$ where each $H_i(\cdot)$ is Lipschitz continuous with constant $L_{h(i)}$, and $g(x)=\sum\limits_{i=1}^{m_2}g_i(x)$ is sum of Lipschitz continuous gradient mappings, each with Lipschitz constant $L_{g(i)}$. The rest of the setups in Section \ref{sec:pre-strong} also apply, and we shall only supplement with some specific changes in the analysis that follows.




%{\color{brown}We can extend to result to the monotone VI operator by adding small perturbation. We assume the radius of the domain is $R$. Then consider the perturbed problem with $H^{\tau}(x)=H(x)+\tau x$, where $\tau=\frac{\epsilon}{R^2}$. We have the following iteration complexity assurance
%\begin{corollary}
%By applying the algorithm, then we can reach $Q(x,x^*)\le \epsilon$ in no more than
%\begin{eqnarray}
%\mathcal{O}
%\left(\left(m_1+m_2+R\sqrt{\frac{L_gm_2}{\epsilon}}+\frac{R^2 L_h\sqrt{m_1}}{\epsilon}\right)\ln\frac{1}{\epsilon}\right).\label{grad-comp-mono}
%\end{eqnarray}
%sample complexity.
%\end{corollary}

%}



% {\color{blue}
% Kevin: I don't know if this makes sense. How about setting
% \[
% \bar w^{k+1}=\left\{
% \begin{array}{ll}
%      v^{k+1},& \mbox{with $p_2$}; \\
%      \bar w^k,& \mbox{with $1-p_2$}.
% \end{array}
% \right.
% \]
% In addition, set $\beta^k=p_2$. It looks like we might end up with 
% \[
% \Gamma_{k+1}=\left(1-\frac{\alpha_k}{1-p_2}\right)\Gamma_k.
% \]
% Compared to Lan's original work \cite{chen2017accelerated} where $\Gamma_{k+1}=(1-\alpha_k)\Gamma_k$, the form seems to make sense and might actually solve what I was struggling with in the loopless case. Haven't gone through Nuozhou's derivations in detailed so not sure if this will work out in Nuozhou's derivations.
% }


Consider the following update for iteration count $k$: %$k\ge0$:

\begin{equation}
\left\{
\begin{array}{lcl}
     \Bar{x}^k&=& (1-p_1)x^k+p_1w^k\\
     %&&\\
     {y^k} &=& (1-\alpha_k-\beta_k)v^k+\alpha_k x^k+\beta_k\bar w^k\\
     %&&\\
     x^{k+0.5}&=&\arg\min\limits_{x\in\mathcal{Z}}\quad\gamma_k\langle H'(w^k)+{\tilde\nabla g'(y^k)},x-\Bar{x}^k\rangle+\frac{1}{2}\|x-\Bar{x}^k\|^2\\
     %&&\\
     x^{k+1} &=& \arg\min\limits_{x\in\mathcal{Z}}\quad\gamma_k\langle \hat H'(x^{k+0.5})+{\tilde\nabla g'(y^k)},x-\Bar{x}^k\rangle+\frac{1}{2}\|x-\Bar{x}^k\|^2\\
    % &&\\
    %  \bar v^{k+1} &=& (1-p_2)v^{k}+p_2\bar w^{k}\\
    %  &&\\
     { v^{k+1}} &=& {(1-\alpha_k-\beta_k) v^{k}+\alpha_k x^{k+0.5}+\beta_k\bar w^k}\\
     %&&\\
     w^{k+1}&=&\left\{
     \begin{array}{ll}
          x^{k+1},& \mbox{with prob. $p_1$} \\
          w^k,    & \mbox{with prob. $1-p_1$}
       
     \end{array}\right.\\
     %&&\\
     \bar w^{k+1}&=&\left\{
     \begin{array}{ll}
        \frac{1}{m_2}\sum_{i=k+2-m_2}^{k+1} v^{i}, & \mbox{$m_2|(k+1)$}\\
          \bar w^k, & \mbox{otherwise}.
     \end{array}
     \right.
\end{array}
\right.\label{savrep-m-Update}
\end{equation}

There are two main differences between the update \eqref{savrep-m-Update} presented above and the update \eqref{VR-VI-Opt-Strong-Update} in the previous section. First, while \eqref{VR-VI-Opt-Strong-Update} simply updates $\bar w^k$ with probability $p_2=\frac{1}{m_2}$ in each iteration, \eqref{savrep-m-Update} has a double-loop structure, which updates $\bar w^{k}$ once every $m_2$ iterations. In other words, the full gradient $\nabla g(\bar w^k)$ is only estimated at the beginning of each outer-loop, and such gradient is used to obtain the variance reduced gradient $\tilde\nabla g(y^k)$ within each inner-loop. Although in \cite{kovalev2020don} single-loop variants of Katyusha and SVRG are developed for strongly convex finite-sum optimization, there are no single-loop variants yet for the convex case as far as our knowledge goes. Therefore, this double-loop structure given in \eqref{savrep-m-Update} also turns out to be critical in the monotone VI setting. Second, instead of using constant parameters as in \eqref{VR-VI-Opt-Strong-Update}, the update \eqref{savrep-m-Update} uses parameters $\alpha_k,\beta_k,\gamma_k$ that depend on iteration number $k$. This change is again reasonable to make given our (non-strongly) monotone assumption in this section, and it is also consistent with the literature of finite-sum algorithms under the non-strongly convex setting. For example, Katyusha also has parameters depending on epochs. We shall refer to the update \eqref{savrep-m-Update} as SAVREP-m (SAVREP for monotone VI) in the rest of the paper.


\subsection{Gradient complexity analysis}
\label{sec:grad-comp-monotone}

In order to establish a theoretical guarantee for the gradient complexity, we make two additional assumptions compared to the analysis of SAVREP. In particular, we assume that the stochastic estimators $H'_i(x)$ and $\nabla g'_i(x)$ are unbiased, and the constraint set $\ZZ$ is bounded, as summarized below.

\begin{assumption}
The stochastic estimators $H_i'(x)$ and $\nabla g_i'(x)$ are both unbiased, i.e.\ $\delta_h=\delta_g=0$ in \eqref{sto-bd-H-1}-\eqref{sto-bd-g-1}.
\end{assumption}

\begin{assumption}
\label{ass:monotone-const-bd}
The diameter of the constraint set $\ZZ$ is $\Omega_\ZZ$, i.e.
\begin{eqnarray}
\sup_{x,y\in \ZZ}\|x-y\|=\Omega_\ZZ.\label{const-diameter}
\end{eqnarray}
\end{assumption}

The gradient complexity analysis of SAVREP-m \eqref{savrep-m-Update} consists of two major steps. The first step is to derive the per-iteration relation among iterates and establish a result similar to Theorem \ref{thm:per-iter-conv-1}. In the first step, we only consider the iterations from $k$ to $k+1$, which is within a single inner-loop in the update \eqref{savrep-m-Update} with $\bar w^k$ remaining unchanged. In the second step, we derive the relation among iterates after one outer-loop, where the iterations proceed from $sm_2$ to $(s+1)m_2$. This step specifically establishes an inequality relating $\bar w^{(s+1)m_2}$ and $\bar w^{sm_2}$, which eventually guarantees the convergence of the iterate $\bar w^k$ as long as the parameters are chosen to satisfy certain conditions.

The results derived from the first step is presented in the next lemma:
\begin{lemma}
\label{lem:savrep-m-inner-relation}
For the iterates generated by \eqref{savrep-m-Update}, assume the following condition holds for all $k\ge0$:
\begin{equation}
    \label{const-3}
    \left\{
    \begin{array}{ll}
        p_1-2\gamma_k^2L_h^2\ge0, \\
        q-p_1-\alpha_k\gamma_k L_g-\frac{\alpha_k \gamma_k L_g}{\beta_k}\ge0,\\
        {1-\alpha_k-\beta_k\ge 0}
    \end{array}
    \right.
\end{equation}
where $0<q<1$ is a constant independent of the problem and algorithm parameters. Then we have:
\begin{eqnarray}
&&\EE\left[Q(v^{k+1};x)\right]+\frac{\alpha_k}{2\gamma_k}\EE\left[(1-p_1)\|x^{k+1}-x\|^2+\|w^{k+1}-x\|^2\right]\label{mu-equal-zero-bd} \\
&\le&\EE\left[(1-\alpha_k-\beta_k)Q(v^k;x)+\beta_k Q(\bar w^k;x)\right]+\frac{\alpha_k}{2\gamma_k}\EE\left[\left(1-p_1\right)\|x^k-x\|^2+\|w^k-x\|^2\right]
%\nonumber\\
%&&
+\alpha_k\gamma_k\Delta \nonumber
\end{eqnarray}
where $\Delta$ is the stochastic error defined as:
\begin{eqnarray}
\Delta=2\tilde\sigma_h^2+\frac{1}{(1-q)}(2m_2\sigma_g^2+4\tilde\sigma_g^2)=O\left(\sigma_h^2 L_h\sum\limits_{i=1}^{m_1}\frac{1}{L_{h(i)}}+\sigma_g^2 L_g\sum\limits_{i=1}^{m_2}\frac{1}{L_{g(i)}}\right).\nonumber
\end{eqnarray}

\begin{proof}
See Appendix \ref{proof:savrep-m-inner-relation}.
\end{proof}
\end{lemma}
% \begin{proof}
% See Appendix \ref{proof:savrep-m-inner-relation}.
% \end{proof}

Note that while Lemma \ref{lem:savrep-m-inner-relation} establishes the relation of iterates between iteration $k$ and $k+1$, $\bar w^k$ remains unchanged (unless $m_2|k+1$). Since $\bar w^k$ plays the central role in the convergence under the monotone case, we have to extend the result in \eqref{mu-equal-zero-bd} to iterations between $sm_2$ and $(s+1)m_2$, where $s$ denotes the number of outer-loops (or {\it epochs}). In particular, we assume that the parameters $\alpha_k,\beta_k,\gamma_k$ are also unchanged within each interval of updating $\bar w^k$, i.e.\, $\alpha_{sm_2}=\alpha_{sm_2+1}=\cdots=\alpha_{(s+1)m_2-1}$,  $\beta_{sm_2}=\beta_{sm_2+1}=\cdots=\beta_{(s+1)m_2-1}$, and $\gamma_{sm_2}=\gamma_{sm_2+1}=\cdots=\gamma_{(s+1)m_2-1}$. Then, by summing up inequality \eqref{mu-equal-zero-bd} from $k=sm_2$ to $k=(s+1)m_2-1$, we get

\begin{eqnarray}\label{eq-non-25}
&&\mathbb{E}\left[Q(v^{(s+1)m_2};x)+(\alpha_{sm_2}+\beta_{sm_2})\sum_{k=sm_2+1}^{(s+1)m_2-1}Q(v^{k};x)\right]\nonumber\\
&&+\frac{\alpha_{sm_2}}{2\gamma_{sm_2}}\mathbb{E}\left[(1-p_1)\|x^{(s+1)m_2}-x\|^2+\|w^{(s+1)m_2}-x\|^2\right]\nonumber\\
&\le&
(1-\alpha_{sm_2}-\beta_{sm_2} )\EE[Q(v^{sm_2};x)]+\beta_{sm_2}m_2\EE[Q(\bar{w}^{sm_2};x)]\nonumber\\
&&+\frac{\alpha_{sm_2}}{2\gamma_{sm_2}}\mathbb{E}\left[(1-p_1)\|x^{sm_2}-x\|^2+\|w^{sm_2}-x\|^2\right]+m_2\alpha_{sm_2}\gamma_{sm_2}\Delta. 
\end{eqnarray}
Since $Q(x';x):= \langle H(x),x'-x\rangle+g(x')-g(x)$ and $g$ is convex, $Q(\cdot;x)$ is convex. By using the definition $\bar{w}^{sm_2}=\frac{1}{m_2}\sum_{i=(s-1)m_2+1}^{sm_2} v^{i}$ and the convexity of $Q(\cdot;x)$, we have $\sum_{k=(s-1)m_2+1}^{sm_2}Q(v^{k};x)\ge m_2 Q(\bar{w}^{sm_2};x)$. Then,
\begin{eqnarray}\label{eq-non-3}
&&\mathbb{E}\left[Q(v^{(s+1)m_2};x)+(\alpha_{sm_2}+\beta_{sm_2})\sum_{k=sm_2+1}^{(s+1)m_2-1}Q(v^{k};x)\right]\nonumber\\
&&
+\frac{\alpha_{sm_2}}{2\gamma_{sm_2}}\mathbb{E}[(1-p_1)\|x^{(s+1)m_2}-x\|^2+\|w^{(s+1)m_2}-x\|^2]\nonumber\\
&\le&
(1-\alpha_{sm_2})\EE[Q(v^{sm_2};x)]+\beta_{sm_2} \EE\left[\sum_{k=(s-1)m_2+1}^{sm_2-1}Q(v^{k};x)\right]\nonumber\\
&&+\frac{\alpha_{sm_2}}{2\gamma_{sm_2}}\mathbb{E}[(1-p_1)\|x^{sm_2}-x\|^2+\|w^{sm_2}-x\|^2]+m_2\alpha_{sm_2}\gamma_{sm_2}\Delta. 
\end{eqnarray}

Let us define
\[\Gamma_{s}=\left\{\begin{array}{lr}
1, & \text { when } s=0 \\
(1-\alpha_{(s-1)m_2}) \Gamma_{s-1}, & \text { when } s>0
\end{array}\right.\]
and
\[
V(x',w;x)=(1-p_1)\|x'-x\|^2+\|w-x\|^2.
\]
Then, by dividing $\Gamma_{s+1}$ on both sides of \ref{eq-non-3}, we have
\begin{eqnarray}\label{eq-non-4}
&&\mathbb{E}\left[\frac{1}{\Gamma_{s+1}}Q(v^{(s+1)m_2};x)+\frac{\alpha_{sm_2}+\beta_{sm_2}}{\Gamma_{s+1}}\sum_{k=sm_2+1}^{(s+1)m_2-1}Q(v^{k};x)\right]\nonumber\\
&\le&
\frac{1}{\Gamma_{s}}\EE[Q(v^{sm_2};x)]+\frac{\beta_{sm_2}}{\Gamma_{s+1}} \EE\left[\sum_{k=(s-1)m_2+1}^{sm_2-1}Q(v^{k};x)\right]\nonumber\\
&&+\frac{\alpha_{sm_2}}{2\gamma_{sm_2}\Gamma_{s+1}}\mathbb{E}\left[V(x^{sm_2},w^{sm_2};x)-V(x^{(s+1)m_2},w^{(s+1)m_2};x)\right]\nonumber\\
&&+\frac{m_2\alpha_{sm_2}\gamma_{sm_2}}{\Gamma_{s+1}}\Delta.\end{eqnarray}

Since for any solution $x^*$ we have $Q(x;x^*)\ge0$ for all $x\in\ZZ$ (c.f.\ \eqref{positive-Q-star}), by taking $x=x^*$ in \eqref{eq-non-4} with the condition on the parameters:
\begin{eqnarray}
\frac{\beta_{sm_2}}{\Gamma_{s+1}}\le \frac{\alpha_{(s-1)m_2}+\beta_{(s-1)m_2}}{\Gamma_{s}},\label{const-4}
\end{eqnarray}
we can rewrite \eqref{eq-non-4} into:
\begin{eqnarray}\label{eq-non-4.5}
&&\mathbb{E}\left[\frac{1}{\Gamma_{s+1}}Q(v^{(s+1)m_2};x^*)+\frac{\alpha_{sm_2}+\beta_{sm_2}}{\Gamma_{s+1}}\sum_{k=sm_2+1}^{(s+1)m_2-1}Q(v^{k};x^*)\right]\nonumber\\
&\le&
\frac{1}{\Gamma_{s}}\EE[Q(v^{sm_2};x^*)]+\frac{\alpha_{(s-1)m_2}+\beta_{(s-1)m_2}}{\Gamma_{s}}\EE\left[\sum_{k=(s-1)m_2+1}^{sm_2-1}Q(v^{k};x^*)\right]\nonumber\\
&&+\frac{\alpha_{sm_2}}{2\gamma_{sm_2}\Gamma_{s+1}}\mathbb{E}\left[V(x^{sm_2},w^{sm_2};x^*)-V(x^{(s+1)m_2},w^{(s+1)m_2};x^*)\right]\nonumber\\
&&+\frac{m_2\alpha_{sm_2}\gamma_{sm_2}}{\Gamma_{s+1}}\Delta.\end{eqnarray}
Now, define
\begin{eqnarray}
B_s=\frac{\alpha_{sm_2}}{2\gamma_{sm_2}\Gamma_{s+1}}\nonumber
\end{eqnarray}
and assume the next condition holds for $s=1,...,S-1$:
\begin{eqnarray}
B_{s-1}\le B_{s}.\label{const-5}
\end{eqnarray}
Then we can obtain the next inequalities by summing up \eqref{eq-non-4.5} for $s=1,...,S-1$:


\begin{eqnarray}&&\frac{m_2\beta_{(S-1)m_2}}{\Gamma_{S}}\mathbb{E}\left[Q(\bar{w}^{Sm_2};x^*)\right]\nonumber\\
&\le&\mathbb{E}\left[\frac{1}{\Gamma_{S}}Q(v^{Sm_2};x^*)+\frac{\alpha_{(S-1)m_2}+\beta_{(S-1)m_2}}{\Gamma_{S}}\sum_{k=(S-1)m_2+1}^{Sm_2-1}Q(v^{k};x^*)\right]\nonumber\\
&\le&
\frac{1}{\Gamma_{1}}\EE[Q(v^{m_2};x^*)]+\frac{\alpha_{0}+\beta_{0}}{\Gamma_{1}}\EE\left[\sum_{k=1}^{m_2-1}Q(v^{k};x^*)\right]
\nonumber\\
&&+\sum_{s=1}^{S-1}B_s\mathbb{E}\left[V(x^{sm_2},w^{sm_2};x^*)-V(x^{(s+1)m_2},w^{(s+1)m_2};x^*)\right]+\sum_{s=1}^{S-1}\frac{m_2\alpha_{sm_2}\gamma_{sm_2}}{\Gamma_{s+1}}\Delta\nonumber\\
&\le&\frac{(1-\alpha_{0}-\beta_{0} )}{\Gamma_{1}}Q(v^{0};x^*)+\frac{\beta_0 m_2}{\Gamma_{1}}Q(\bar{w}^{0};x^*)+\sum_{s=0}^{S-1}B_s\mathbb{E}\left[V(x^{sm_2},w^{sm_2};x^*)-V(x^{(s+1)m_2},w^{(s+1)m_2};x^*)\right]\nonumber \\
&&+\sum_{s=0}^{S-1}\frac{m_2\alpha_{sm_2}\gamma_{sm_2}}{\Gamma_{s+1}}\Delta\nonumber\\
&\le&\frac{(1-\alpha_{0}+(m_2-1)\beta_{0} )}{\Gamma_{1}}Q(w^{0};x^*)+B_0\mathbb{E}\left[V(x^{0},w^{0};x^*)\right]+\sum_{s=1}^{S-1}(B_s-B_{s-1})\mathbb{E}\left[V(x^{sm_2},w^{sm_2};x^*)\right]\nonumber\\
&&+\sum_{s=0}^{S-1}\frac{m_2\alpha_{sm_2}\gamma_{sm_2}}{\Gamma_{s+1}}\Delta\nonumber\\
&\overset{\eqref{const-diameter},\eqref{const-5}}{\le}&\frac{(1-\alpha_{0}+(m_2-1)\beta_{0} )}{\Gamma_{1}}Q(w^{0};x^*)+B_0 \Omega_Z^2+\sum_{s=1}^{S-1}(B_s-B_{s-1})\Omega_Z^2+\sum_{s=0}^{S-1}\frac{m_2\alpha_{sm_2}\gamma_{sm_2}}{\Gamma_{s+1}}\Delta\nonumber\\
&\le&\frac{(1-\alpha_{0}+(m_2-1)\beta_{0} )}{\Gamma_{1}}Q(w^{0};x^*)+\frac{\alpha_{(S-1)m_2}}{2\gamma_{(S-1)m_2}\Gamma_{S}} \Omega_Z^2+\sum_{s=0}^{S-1}\frac{m_2\alpha_{sm_2}\gamma_{sm_2}}{\Gamma_{s+1}}\Delta,\nonumber
\end{eqnarray}
where in the third inequality we apply \eqref{eq-non-25} with $s=0$ and $x=x^*$, and
in the forth inequality we simply remove the nonpositive term $-B_{S-1}\EE\left[V(x^{Sm_2},w^{Sm_2};x^*)\right]$, together with the definition $v^0:=\bar w^0:=w^0=x^0$. 

We summarize the above results together with the required conditions on the parameters $\eqref{const-3},\eqref{const-4},\eqref{const-5}$ in the next theorem:
\begin{theorem}\label{theo-non}
Suppose the following conditions hold for $k\ge0$ and $s=1,...,S-1$:
\begin{eqnarray}
\left\{
\begin{array}{lcl}
   p_1-2\gamma_k^{2}L_h^2& \ge& 0 \\
   %&&\\
    q-p_1-\alpha_k\gamma_k L_g-\frac{\alpha_k \gamma_k L_g}{\beta_k}&\ge&0 \\
    %&&\\
    1-\alpha_k-\beta_k&\ge&0\\
    \frac{\alpha_{(s-1)m_2}}{\gamma_{(s-1)m_2}\Gamma_s}&\le& \frac{\alpha_{sm_2}}{\gamma_{sm_2}\Gamma_{s+1}}\\
    %&&\\
    \frac{\beta_{sm_2}}{1-\alpha_{sm_2}}&\le& \alpha_{(s-1)m_2}+\beta_{(s-1)m_2} 
\end{array}
\right.\label{paramter-conditions}
\end{eqnarray}
where $0<q<1$ is a constant, and $\alpha_k$, $\beta_k$, $\gamma_k$ are constants within each interval of updating $\bar{w}$, i.e.\, $\alpha_{sm_2}=\alpha_{sm_2+1}=\cdots=\alpha_{(s+1)m_2-1}$,  $\beta_{sm_2}=\beta_{sm_2+1}=\cdots=\beta_{(s+1)m_2-1}$, and $\gamma_{sm_2}=\gamma_{sm_2+1}=\cdots=\gamma_{(s+1)m_2-1}$. Then,
\begin{eqnarray}\mathbb{E}\left[Q(\bar{w}^{Sm_2};x^*)\right]&\le&\frac{1}{m_2\beta_{(S-1)m_2}} \frac{(1-\alpha_{0}+(m_2-1)\beta_{0} )\Gamma_S}{\Gamma_{1}}Q(w^{0};x^*)+\frac{\alpha_{(S-1)m_2}}{2m_2\gamma_{(S-1)m_2}\beta_{(S-1)m_2}} \Omega_Z^2\nonumber \\
&&+\frac{\Gamma_S}{\beta_{(S-1)m_2}}\sum_{s=0}^{S-1}\frac{\alpha_{sm_2}\gamma_{sm_2}}{\Gamma_{s+1}}\Delta
\nonumber\end{eqnarray}
for any $x^*\in \ZZ^*$, where $\ZZ^*\subseteq\ZZ$ is the solution set and $\Delta=O\left(\sigma_h^2 L_h\sum\limits_{i=1}^{m_1}\frac{1}{L_{h(i)}}+\sigma_g^2 L_g\sum\limits_{i=1}^{m_2}\frac{1}{L_{g(i)}}\right)$.
\end{theorem}

We shall specify a set of parameters that satisfy the conditions in \eqref{paramter-conditions} and the corresponding gradient complexities in the next corollary.
\begin{corollary} \label{cor-parameter-complexity}
If we choose 
\begin{eqnarray}
&&q=\frac 3 4,\quad p_1=\frac{1}{m_1}\le \frac{1}{2},\quad \alpha_k=\frac{2}{s+4}\le \frac 1 2,\quad\beta_k=\frac 1 2,\nonumber\\
&&\gamma_k=\frac{s+3}{24(L_g+(s+1)L_h\sqrt{m_1})+(s+1)\sqrt{(s+1)\Delta m_2}/\Omega_\ZZ},\nonumber
\end{eqnarray}
where $s=\left\lfloor\frac{k}{m_2}\right\rfloor$, then when $m_2|k$, 
\begin{eqnarray}
\mathbb{E}[Q(\bar{w}^{k},x^*)]&\le& \frac{24}{S^2}Q(w^0,x^*)+\frac{48}{m_2S^2}L_g\Omega_\ZZ^2+\frac{48}{m_2S}L_h\sqrt{m_1}\Omega_\ZZ^2+\frac{26\Omega_\ZZ\sqrt{\Delta}}{\sqrt{S m_2}}\nonumber\\
    &=&\frac{24m_2^2}{k^2}Q(w^0,x^*)+\frac{48m_2}{k^2}L_g\Omega_\ZZ^2+\frac{48}{k}L_h\sqrt{m_1}\Omega_\ZZ^2+\frac{26\Omega_\ZZ\sqrt{\Delta}}{\sqrt{k}}\label{monotone-convergence-rate}
\end{eqnarray}
where $S=k/m_2$. The gradient complexity for reducing $\mathbb{E}[Q(\bar{w}^{k},x^*)]$ to some $\epsilon>0$ is given by
\begin{eqnarray}
\mathcal{O}
\left(\sqrt{\frac{Q(w^0,x^*)}{\epsilon}}m_2+\sqrt{\frac{L_gm_2}{\epsilon}}\Omega_\ZZ+\frac{L_h\sqrt{m_1}\Omega_{\ZZ}^2}{\epsilon}+\frac{\Delta\Omega_\ZZ^2}{\epsilon^2}\right).\label{monotone-grad-complexity}
\end{eqnarray}
\end{corollary}

\begin{proof}
We first verify the conditions \eqref{paramter-conditions} are satisfied by the specific choices of the parameters. Note that $\Gamma_s=\frac{6}{(s+2)(s+3)}$, and the following inequalities holds:
\[\gamma_k^{2}L_h^2\leq \left(\frac{s+3}{24(s+1)\sqrt{m_1}}\right)^2\le \frac{1}{2m_1}=\frac{p_1}{2},\]

\[p_1+\alpha_k\gamma_k L_g+\frac{\alpha_k \gamma_k L_g}{\beta_k}=p_1+3\alpha_k\gamma_k L_g\le \frac{1}{2}+\frac{6}{s+4}\cdot\frac{s+3}{24}\le \frac 3 4,\]

\[\frac{\alpha_{sm_2}}{\gamma_{sm_2}\Gamma_{s+1}}=8(L_g+(s+1)L_h\sqrt{m_1})+\sqrt{(s+1)\Delta m_2}/\Omega_\ZZ\]
which is non-decreasing in $s=0,1,...,S-1$, and

\[
\frac{s+4}{2(s+2)}=\frac{\beta_{sm_2}}{1-\alpha_{sm_2}}\le \alpha_{(s-1)m_2}+\beta_{(s-1)m_2}=\frac{s+7}{2(s+3).}
\]
Therefore, the conditions in \eqref{paramter-conditions} are indeed satisfied. 

The convergence rate \eqref{monotone-convergence-rate} can be derived by noticing the next few inequalities:

\[\frac{1}{m_2\beta_{(S-1)m_2}} \frac{(1-\alpha_{0}+(m_2-1)\beta_{0} )\Gamma_S}{\Gamma_{1}}\le 4\Gamma_{S}\le \frac{24}{S^2},\]

\begin{eqnarray}
\frac{\alpha_{(S-1)m_2}}{2m_2\gamma_{(S-1)m_2}\beta_{(S-1)m_2}}&\le&\frac{2}{m_2S^2}\left(24(L_g+SL_h\sqrt{m_1})+S\sqrt{S\Delta m_2}/\Omega_\ZZ\right)\nonumber\\
&\le& \frac{48}{m_2S^2}L_g+\frac{48}{m_2S}L_h\sqrt{m_1}+\frac{2\sqrt{\Delta}}{\Omega_\ZZ\sqrt{S m_2}},\nonumber
\end{eqnarray}
and
\[\frac{\alpha_{sm_2}\gamma_{sm_2}}{\Gamma_{s+1}}\le \frac{s+3}{3}\gamma_{sm_2} \le  \frac{(s+3)^2}{3(s+1)\sqrt{(s+1)\Delta m_2}/\Omega_\ZZ}\le 3\sqrt{s+1}\frac{\Omega_\ZZ}{\sqrt{\Delta m_2}},\]
which results in the following bound since $\sum_{s=0}^{S-1} \sqrt{s+1}\le \int_{s=0}^{S+1} \sqrt{s}ds=\frac{2}{3} (S+1)^{3/2}$:
\[\frac{\Gamma_S}{\beta_{(S-1)m_2}}\sum_{s=0}^{S-1}\frac{\alpha_{sm_2}\gamma_{sm_2}}{\Gamma_{s+1}}\le 24\frac{\Omega_\ZZ}{\sqrt{S \Delta m_2}}.\]

Therefore, we have
\begin{eqnarray}
\mathbb{E}[Q(\bar{w}^{k},x^*)]&\le& \frac{24}{S^2}Q(w^0,x^*)+\frac{48}{m_2S^2}L_g\Omega_\ZZ^2+\frac{48}{m_2S}L_h\sqrt{m_1}\Omega_\ZZ^2+\frac{26\Omega_Z\sqrt{\Delta}}{\sqrt{S m_2}}\nonumber\\
    &=&\frac{24m_2^2}{k^2}Q(w^0,x^*)+\frac{48m_2}{k^2}L_g\Omega_\ZZ^2+\frac{48}{k}L_h\sqrt{m_1}\Omega_\ZZ^2+\frac{26\Omega_\ZZ\sqrt{\Delta}}{\sqrt{k}}\nonumber.
\end{eqnarray}
\end{proof}





% \begin{eqnarray}
% &&\mathbb{E}\left[Q(v^{(s+1)m_2};x)+(\alpha_{sm_2}+\beta_{sm_2})\sum_{k=sm_2+1}^{(s+1)m_2}Q(v^{k};x)\right]+\frac{\alpha_{sm_2}}{2\gamma_{sm_2}}\mathbb{E}\left[(1-p_1)\|x^{(s+1)m_2}-x\|^2+\|w^{(s+1)m_2}-x\|^2\right]\nonumber\\
% &\le&
% (1-\alpha_{sm_2}-\beta_{sm_2} )\EE[Q(v^{sm_2};x)]+\beta_{sm_2}m_2\EE[Q(\bar{w}^{sm_2};x)]\nonumber\\
% &&+\frac{\alpha_{sm_2}}{2\gamma_{sm_2}}\mathbb{E}\left[(1-p_1)\|x^{sm_2}-x\|^2+\|w^{sm_2}-x\|^2\right]+m_2\alpha_{sm_2}\gamma_{sm_2}\Delta. \nonumber
% \end{eqnarray}

% Since $Q(x';x):= \langle H(x),x'-x\rangle+g(x')-g(x)$ and $g$ is convex, $Q(\cdot;x)$ is convex. By using the definition $\bar{w}^{(s+1)m_2}=\frac{1}{m_2}\sum_{i=sm_2+1}^{(s+1)m_2} v^{i}$ and the convexity of $Q(\cdot;x)$, we have $\sum_{k=sm_2+1}^{(s+1)m_2}Q(v^{k};x)\ge m_2 Q(\bar{w}^{(s+1)m_2};x)$. Then,
% \begin{eqnarray}\label{eq-non-3}
% &&\mathbb{E}[Q(v^{(s+1)m_2};x)+(\alpha_{sm_2}+\beta_{sm_2})m_2Q(\bar{w}^{(s+1)m_2};x)]
% %\nonumber\\
% %&&
% +\frac{\alpha_{sm_2}}{2\gamma_{sm_2}}\mathbb{E}[(1-p_1)\|x^{(s+1)m_2}-x\|^2+\|w^{(s+1)m_2}-x\|^2]\nonumber\\
% &\le&
% (1-\alpha_{sm_2}-\beta_{sm_2} )\EE[Q(v^{sm_2};x)]+\beta_{sm_2}m_2 \EE[Q(\bar{w}^{sm_2};x)]\nonumber\\
% &&+\frac{\alpha_{sm_2}}{2\gamma_{sm_2}}\mathbb{E}[(1-p_1)\|x^{sm_2}-x\|^2+\|w^{sm_2}-x\|^2]+m_2\alpha_{sm_2}\gamma_{sm_2}\Delta. 
% \end{eqnarray}


% Let us define
% \[\Gamma_{s}=\left\{\begin{array}{lr}
% 1, & \text { when } s=0 \\
% \frac{\beta_{(s-1)m_2}}{\alpha_{(s-1)m_2}+\beta_{(s-1)m_2}} \Gamma_{s-1}, & \text { when } s>0
% \end{array}\right.\]
% and
% \[
% V(x',w;x)=(1-p_1)\|x'-x\|^2+\|w-x\|^2.
% \]
% Then, by dividing $\Gamma_{s+1}(\alpha_{sm_2}+\beta_{sm_2})m_2$ on both sides of \ref{eq-non-3}, we have
% \begin{eqnarray}\label{eq-non-4}
% &&\mathbb{E}\left[\frac{1}{\Gamma_{s+1}(\alpha_{sm_2}+\beta_{sm_2})m_2}Q(v^{(s+1)m_2};x)+\frac{1}{\Gamma_{s+1}}Q(\bar{w}^{(s+1)m_2};x)\right]\nonumber\\
% &\le&
% \frac{(1-\alpha_{sm_2}-\beta_{sm_2} )}{\Gamma_{s+1}(\alpha_{sm_2}+\beta_{sm_2})m_2}\EE[Q(v^{sm_2};x)]+\frac{1}{\Gamma_{s}}\EE[Q(\bar{w}^{sm_2};x)]\nonumber\\
% &&+\frac{\alpha_{sm_2}}{2\gamma_{sm_2}\Gamma_{s+1}(\alpha_{sm_2}+\beta_{sm_2})m_2}\mathbb{E}\left[V(x^{sm_2},w^{sm_2};x)-V(x^{(s+1)m_2},w^{(s+1)m_2};x)\right]\nonumber\\
% &&+\frac{\alpha_{sm_2}\gamma_{sm_2}}{\Gamma_{s+1}(\alpha_{sm_2}+\beta_{sm_2})}\Delta.\end{eqnarray}

% Since for any solution $x^*$ we have $Q(x;x^*)\ge0$ for all $x\in\ZZ$ (c.f.\ \eqref{positive-Q-star}), by taking $x=x^*$ in \eqref{eq-non-4} with the condition on the parameters:
% \begin{eqnarray}
% \frac{1-\alpha_{sm_2}-\beta_{sm_2} }{\Gamma_{s+1}(\alpha_{sm_2}+\beta_{sm_2})}\le \frac{1}{\Gamma_{s}(\alpha_{(s-1)m_2}+\beta_{(s-1)m_2})},\label{const-4}
% \end{eqnarray}
% we can rewrite \eqref{eq-non-4} into:
% \begin{eqnarray}\label{eq-non-4.5}
% &&\mathbb{E}\left[\frac{1}{\Gamma_{s+1}(\alpha_{sm_2}+\beta_{sm_2})m_2}Q(v^{(s+1)m_2};x^*)+\frac{1}{\Gamma_{s+1}}Q(\bar{w}^{(s+1)m_2};x^*)\right]\nonumber\\
% &\le&
% \frac{1}{\Gamma_{s}(\alpha_{(s-1)m_2}+\beta_{(s-1)m_2})m_2}\EE[Q(v^{sm_2};x^*)]+\frac{1}{\Gamma_{s}}\EE[Q(\bar{w}^{sm_2};x^*)]\nonumber\\
% &&+\frac{\alpha_{sm_2}}{2\gamma_{sm_2}\Gamma_{s+1}(\alpha_{sm_2}+\beta_{sm_2})m_2}\mathbb{E}\left[V(x^{sm_2},w^{sm_2};x^*)-V(x^{(s+1)m_2},w^{(s+1)m_2};x^*)\right]\nonumber\\
% &&+\frac{\alpha_{sm_2}\gamma_{sm_2}}{\Gamma_{s+1}(\alpha_{sm_2}+\beta_{sm_2})}\Delta.\end{eqnarray}
% Now, define
% \begin{eqnarray}
% B_s=\frac{\alpha_{sm_2}}{2\gamma_{sm_2}\Gamma_{s+1}(\alpha_{sm_2}+\beta_{sm_2})m_2}\nonumber
% \end{eqnarray}
% and assume the next condition holds for $s=0,...,S-1$:
% \begin{eqnarray}
% B_s\le B_{s+1}.\label{const-5}
% \end{eqnarray}
% Then we can obtain the next inequalities by summing up \eqref{eq-non-4.5} for $s=0,...,S-1$:

% \begin{eqnarray}&&\mathbb{E}\left[\frac{1}{\Gamma_{S}}Q(\bar{w}^{Sm_2};x^*)\right]\nonumber\\
% &\le&
% \frac{(1-\alpha_{0}-\beta_{0} )}{\Gamma_{0}(\alpha_{0}+\beta_{0})m_2}Q(v^{0};x^*)+\frac{1}{\Gamma_{0}}Q(\bar{w}^{0};x^*)\nonumber\\
% &&+\sum_{s=0}^{S-1}\frac{\alpha_{sm_2}}{2\gamma_{sm_2}\Gamma_{s+1}(\alpha_{sm_2}+\beta_{sm_2})m_2}\mathbb{E}\left[V(x^{sm_2},w^{sm_2};x^*)-V(x^{(s+1)m_2},w^{(s+1)m_2};x^*)\right]\nonumber\\
% &&+\sum_{s=0}^{S-1}\frac{\alpha_{sm_2}\gamma_{sm_2}}{\Gamma_{s+1}(\alpha_{sm_2}+\beta_{sm_2})}\Delta\nonumber\\
% &\le&\frac{(1-\alpha_{0}-\beta_{0} )}{\Gamma_{0}(\alpha_{0}+\beta_{0})m_2}Q(v^{0};x^*)+\frac{1}{\Gamma_{0}}Q(\bar{w}^{0};x^*)+B_0V(r^{0};x^*)+\sum_{s=1}^{S-1}(B_s-B_{s-1})\mathbb{E}\left[V(x^{sm_2},w^{sm_2};x^*)\right]\nonumber \\
% &&+\sum_{s=0}^{S-1}\frac{\alpha_{sm_2}\gamma_{sm_2}}{\Gamma_{s+1}(\alpha_{sm_2}+\beta_{sm_2})}\Delta\nonumber\\
% &\overset{\eqref{const-diameter},\eqref{const-5}}{\le}&\frac{(1-\alpha_{0}-\beta_{0} )}{\Gamma_{0}(\alpha_{0}+\beta_{0})m_2}Q(w^{0};x^*)+\frac{1}{\Gamma_{0}}Q(w^{0};x^*)+B_0 \Omega_Z^2+\sum_{s=1}^{S-1}(B_s-B_{s-1})\Omega_Z^2\nonumber\\
% &&+\sum_{s=0}^{S-1}\frac{\alpha_{sm_2}\gamma_{sm_2}}{\Gamma_{s+1}(\alpha_{sm_2}+\beta_{sm_2})}\Delta\nonumber\\
% &\le&\frac{1}{(\alpha_{0}+\beta_{0})m_2}Q(w^{0};x^*)+Q(w^{0};x^*)+\frac{\alpha_{(S-1)m_2}}{\gamma_{(S-1)m_2}\Gamma_{S}(\alpha_{(S-1)m_2}+\beta_{(S-1)m_2})m_2} \Omega_Z^2\nonumber\\
% &&+\sum_{s=0}^{S-1}\frac{\alpha_{sm_2}\gamma_{sm_2}}{\Gamma_{s+1}(\alpha_{sm_2}+\beta_{sm_2})}\Delta\nonumber
% \end{eqnarray}
% where in the second inequality we simply remove the non-positive term $-B_{S-1}\EE\left[V(x^{Sm_2},w^{Sm_2};x^*)\right]$. 

% We summarize the above results together with the required conditions on the parameters $\eqref{const-3},\eqref{const-4},\eqref{const-5}$ in the next theorem:
% \begin{theorem}\label{theo-non}
% Suppose the following conditions hold for $k\ge0$ and $s=0,...,S-1$:
% \begin{eqnarray}
% \left\{
% \begin{array}{lcl}
%   p_1-2\gamma_k^{2}L_h^2& \ge& 0 \\
%   %&&\\
%     q-p_1-\alpha_k\gamma_k L_g-\frac{\alpha_k \gamma_k L_g}{\beta_k}&\ge&0 \\
%     %&&\\
%     \frac{\alpha_{(s-1)m_2}}{\gamma_{(s-1)m_2}\Gamma_{s}(\alpha_{(s-1)m_2}+\beta_{(s-1)m_2})}&\le& \frac{\alpha_{sm_2}}{\gamma_{sm_2}\Gamma_{s+1}(\alpha_{sm_2}+\beta_{sm_2})}\\
%     %&&\\
%     \frac{1-\alpha_{sm_2}-\beta_{sm_2} }{\Gamma_{s+1}(\alpha_{sm_2}+\beta_{sm_2})}&\le& \frac{1}{\Gamma_{s}(\alpha_{(s-1)m_2}+\beta_{(s-1)m_2})}  
% \end{array}
% \right.\label{paramter-conditions}
% \end{eqnarray}
% where $0<q<1$ is a constant, and $\alpha_k$, $\beta_k$, $\gamma_k$ are constants within each interval of updating $\bar{w}$, i.e.\, $\alpha_{sm_2}=\alpha_{sm_2+1}=\cdots=\alpha_{(s+1)m_2-1}$,  $\beta_{sm_2}=\beta_{sm_2+1}=\cdots=\beta_{(s+1)m_2-1}$, and $\gamma_{sm_2}=\gamma_{sm_2+1}=\cdots=\gamma_{(s+1)m_2-1}$, and $\alpha_k+\beta_k\leq 1, \forall k\ge0$. Then,
% \begin{eqnarray}\mathbb{E}\left[Q(\bar{w}^{Sm_2};x^*)\right]&\le& \left(\frac{\Gamma_{S}}{(\alpha_{0}+\beta_{0})m_2}+\Gamma_{S}\right)Q(w^{0};x^*)+\frac{\alpha_{(S-1)m_2}}{\gamma_{(S-1)m_2}(\alpha_{(S-1)m_2}+\beta_{(S-1)m_2})m_2} \Omega_Z^2\nonumber\\
% &&+\Gamma_S\sum_{s=0}^{S-1}\frac{\alpha_{sm_2}\gamma_{sm_2}}{\Gamma_{s+1}(\alpha_{sm_2}+\beta_{sm_2})}\Delta,\nonumber\end{eqnarray}
% for any $x^*\in \ZZ^*$, where $\ZZ^*\subset\ZZ$ is the solution set and $\Delta=O\left(\sigma_h^2 L_h\sum\limits_{i=1}^{m_1}\frac{1}{L_{h(i)}}+\sigma_g^2 L_g\sum\limits_{i=1}^{m_2}\frac{1}{L_{g(i)}}\right)$.
% \end{theorem}

% We shall specify a set of parameters that satisfy the conditions in \eqref{paramter-conditions} and the corresponding gradient complexities in the next corollary.

% \begin{corollary} \label{cor-parameter-complexity}
% If we choose 
% \begin{eqnarray}
% &&q=\frac 3 4,\quad p_1=\frac{1}{m_1}\le \frac{1}{2},\quad \alpha_k=\frac{1}{s+2},\quad\beta_k=\frac 1 2,\nonumber\\
% &&\gamma_k=\frac{s+3}{24(L_g+(s+1)L_h\sqrt{m_1})+(s+1)\sqrt{(s+1)\Delta m_2}/\Omega_\ZZ},\nonumber
% \end{eqnarray}
% where $s=\left\lfloor\frac{k}{m_2}\right\rfloor$, then when $m_2|k$, 
% \begin{eqnarray}
% \mathbb{E}[Q(\bar{w}^{k},x^*)]&\le& \frac{18}{S^2}Q(w^0,x^*)+\frac{48}{m_2S^2}L_g\Omega_\ZZ^2+\frac{48}{m_2S}L_h\sqrt{m_1}\Omega_\ZZ^2+\frac{20\Omega_\ZZ\sqrt{\Delta}}{\sqrt{S m_2}}\nonumber\\
%     &=&\frac{18m_2^2}{k^2}Q(w^0,x^*)+\frac{48m_2}{k^2}L_g\Omega_\ZZ^2+\frac{48}{k}L_h\sqrt{m_1}\Omega_\ZZ^2+\frac{20\Omega_\ZZ\sqrt{\Delta}}{\sqrt{k}}\label{monotone-convergence-rate}
% \end{eqnarray}
% where $S=k/m_2$. The gradient complexity for reducing $\mathbb{E}[Q(\bar{w}^{k},x^*)]$ to some $\epsilon>0$ is given by
% \begin{eqnarray}
% \mathcal{O}
% \left(\sqrt{\frac{Q(w^0,x^*)}{\epsilon}}m_2+\sqrt{\frac{L_gm_2}{\epsilon}}\Omega_\ZZ+\frac{L_h\sqrt{m_1}}{\epsilon}+\frac{\Delta\Omega_\ZZ^2}{\epsilon^2}\right).\label{monotone-grad-complexity}
% \end{eqnarray}
% \end{corollary}

% \proof{Proof.}
% We first verify the conditions \eqref{paramter-conditions} are satisfied by the specific choices of the parameters. Note that $\Gamma_s=\frac{6}{(s+2)(s+3)}$, and the following inequalities holds:
% \[\gamma_k^{2}L_h^2\leq \left(\frac{s+3}{24(s+1)\sqrt{m_1}}\right)^2\le \frac{1}{2m_1}=\frac{p_1}{2},\]

% \[p_1+\alpha_k\gamma_k L_g+\frac{\alpha_k \gamma_k L_g}{\beta_k}=p_1+3\alpha_k\gamma_k L_g\le \frac{1}{2}+\frac{3}{s+2}\cdot\frac{s+3}{24}\le \frac 3 4, \]

% \[\frac{\alpha_{sm_2}}{\gamma_{sm_2}\Gamma_{s+1}(\alpha_{sm_2}+\beta_{sm_2})}=24(L_g+(s+1)L_h\sqrt{m_1})+\sqrt{(s+1)\Delta m_2}/\Omega_\ZZ\]
% which is non-decreasing in $s$, and

% \[\frac{1-\alpha_{sm_2}-\beta_{sm_2} }{\Gamma_{s+1}(\alpha_{sm_2}+\beta_{sm_2})}\le \frac{\frac{1}{2}}{\Gamma_{s+1}(\alpha_{sm_2}+\beta_{sm_2})}=
% \frac{1}{\Gamma_s}\le \frac{1}{\Gamma_{s}(\alpha_{(s-1)m_2}+\beta_{(s-1)m_2})}.\]
% Therefore, the conditions in \eqref{paramter-conditions} are indeed satisfied. 

% The convergence rate \eqref{monotone-convergence-rate} can be derived by noticing the next few inequalities:

% \[\frac{\Gamma_{S}}{(\alpha_{0}+\beta_{0})m_2}+\Gamma_{S}\le 3\Gamma_{S}\le \frac{18}{S^2},\]

% \begin{eqnarray}
% \frac{\alpha_{(S-1)m_2}}{\gamma_{(S-1)m_2}(\alpha_{(S-1)m_2}+\beta_{(S-1)m_2})m_2}&\le& \frac{1}{S}\cdot\frac{24(L_g+S L_h\sqrt{m_1})+S\sqrt{S\Delta m_2}/\Omega_\ZZ}{S}\cdot\frac{2}{m_2}\nonumber\\
% &\le& \frac{48}{m_2S^2}L_g+\frac{48}{m_2S}L_h\sqrt{m_1}+\frac{2\sqrt{\Delta}}{\Omega_\ZZ\sqrt{S m_2}},\nonumber
% \end{eqnarray}
% and
% \[\frac{\alpha_{sm_2}\gamma_{sm_2}}{\Gamma_{s+1}(\alpha_{sm_2}+\beta_{sm_2})}\le \frac{(s+3)^2}{3(s+1)\sqrt{(s+1)\Delta m_2}/\Omega_\ZZ}\le 3\sqrt{s+1}\frac{\Omega_\ZZ}{\sqrt{\Delta m_2}},\]
% which results in the following bound since $\sum_{s=0}^{S-1} \sqrt{s+1}\le \int_{s=0}^{S+1} \sqrt{s}ds=\frac{2}{3} (S+1)^{3/2}$:
% \[\Gamma_S\sum_{s=0}^{S-1}\frac{\alpha_{sm_2}\gamma_{sm_2}}{\Gamma_{s+1}(\alpha_{sm_2}+\beta_{sm_2})}\le 18\frac{\Omega_\ZZ}{\sqrt{S \Delta m_2}}.\]

% Therefore, we have
% \begin{eqnarray}
% \mathbb{E}[Q(\bar{w}^{k},x^*)]&\le& \frac{18}{S^2}Q(w^0,x^*)+\frac{48}{m_2S^2}L_g\Omega_\ZZ^2+\frac{48}{m_2S}L_h\sqrt{m_1}\Omega_\ZZ^2+\frac{20\Omega_Z\sqrt{\Delta}}{\sqrt{S m_2}}\nonumber\\
%     &=&\frac{18m_2^2}{k^2}Q(w^0,x^*)+\frac{48m_2}{k^2}L_g\Omega_\ZZ^2+\frac{48}{k}L_h\sqrt{m_1}\Omega_\ZZ^2+\frac{20\Omega_\ZZ\sqrt{\Delta}}{\sqrt{k}}\nonumber.\Halmos
% \end{eqnarray}
% \endproof

\begin{remark}
In case of $H(x)=0$ and $\Delta=0$, the only difference between the above complexity and the counterpart of Katyusha \cite{allen2017katyusha} is we replace $\|w^0-x^*\|$ with $\Omega_\ZZ$. In addition, when $H(x)\neq 0$, the complexity improves the result in \cite{alacaoglu2022stochastic} in terms of $L_g$. In the case of $g(x)=0$, the complexity matches the results in \cite{alacaoglu2022stochastic}, with the gap between the current lower bound \cite{xie2020lower} remaining to be filled.
\end{remark}



Note that in general $Q(\bar{w}^{k},x^*)$ converging to $0$ does not necessarily guarantee that $\bar{w}^{k}$ converges to a solution $x^*$. Under additional condition, such as when $g(x)$ is strictly convex, the convergence to $x^*$ can be shown. 

%\begin{assumption}
%If $\lim_{k\rightarrow 0}\sup_{x^*\in X}Q(z^k,x^*)=0$, where $X$ is the solution set, then $\lim_{k\rightarrow  \infty} z^k=x^*$.
%\end{assumption}


\begin{corollary}
If $g(x)$ is strictly convex, then $\lim_{k\rightarrow \infty} \EE[\|\bar{w}^{k}-x^*\|]=0$ in the setting of Corollary \ref{cor-parameter-complexity}.
\end{corollary}

\begin{proof}
Let $M(r):=\min_{\|x'-x^*\|=r}g(x')-g(x^*)-\langle \nabla g(x^*),x'-x^*\rangle$. 
%According to maximum theorem, $M(r)$ is attainable. 
Since $g(x)$ is strictly convex, $M(r)>0$ as $r>0$. Notice that for any $z'\in \ZZ$,
\begin{eqnarray}
Q(x';x^*)&=&\langle H(x^*),x'-x^*\rangle+g(x')-g(x^*)\nonumber\\
&=& \langle H(x^*)+\nabla g(x^*),x-x^*\rangle +g(x')-g(x^*)-\langle \nabla g(x^*),x'-x^*\rangle \nonumber\\
&\ge& g(x')-g(x^*)-\langle \nabla g(x^*),x'-x^*\rangle. \nonumber
\end{eqnarray}
With Corollary \ref{cor-parameter-complexity}, we get $\lim_{k\rightarrow \infty}\EE[M(\|\bar{w}^{k}-x^*\|)]=0$. Note $\frac{M(r)}{r}=\min_{\|\theta\|=1}\frac{g(x^*+r\theta)-g(x^*)-\langle \nabla g(x^*),r\theta\rangle}{r}$ is increasing with respect to $r$. So, given any $\epsilon>0$, $M(r)\ge r \frac{M(\epsilon)}{\epsilon}$ for any $r>\epsilon$, which implies
\begin{eqnarray}
\EE[M(\|\bar{w}^{k}-x^*\|)]\ge \EE[M(\|\bar{w}^{k}-x^*\|)\textbf{1}_{\|\bar{w}^{k}-x^*\|\ge \epsilon} ]\ge \frac{M(\epsilon)}{\epsilon}\EE[\|\bar{w}^{k}-x^*\|\textbf{1}_{\|\bar{w}^{k}-x^*\|\ge \epsilon} ]. \nonumber
\end{eqnarray}
Therefore, $\lim_{k\rightarrow \infty}\EE[\|\bar{w}^{k}-x^*\|\textbf{1}_{\|\bar{w}^{k}-x^*\|\ge \epsilon}]=0$ and
\begin{eqnarray}
\overline{\lim_{k\rightarrow \infty}}\EE[\|\bar{w}^{k}-x^*\|]= \lim_{k\rightarrow \infty}\EE[\|\bar{w}^{k}-x^*\|\textbf{1}_{\|\bar{w}^{k}-x^*\|\ge \epsilon}]+\overline{\lim_{k\rightarrow \infty}}\EE[\|\bar{w}^{k}-x^*\|\textbf{1}_{\|\bar{w}^{k}-x^*\|< \epsilon}]\le \epsilon.  \nonumber
\end{eqnarray}
Since $\epsilon>0$ can be chosen arbitrarily, we have $\lim_{k\rightarrow \infty} \EE[\|\bar{w}^{k}-x^*\|]=0$.
\end{proof}



\section{Finite-Sum Constrained Finite-Sum Optimization}
\label{sec:finite-sum-constrained}

In this section, we introduce an application for which the proposed SAVREP and SAVREP-m can be applied to. Consider the following problem:
\begin{eqnarray}
\begin{array}{lll}
(P) & \min & \sum_{i=1}^{m_2} g_i(x) \\
& \mbox{s.t.} & \sum_{j=1}^{m_1} h_j(x) \le 0 \\
& & x \in \XX.
\end{array}\label{finite-sum-cons-prob}
\end{eqnarray}
While it is not uncommon to formulate the objective function as finite-sum in machine learning research, the specific finite-sum structure of inequality constraints given in \eqref{finite-sum-cons-prob}  is also found in applications such as empirical risk minimization and Neyman-Pearson classification \cite{tong2016survey}. Previous research \cite{aravkin2019level, lin2018levelconvex, lin2018level} has developed level-set methods for solving \eqref{finite-sum-cons-prob}. In particular, \cite{lin2018level} proposed to reformulate the level-set subproblem into saddle-point problem and solve it with variance-reduced method \cite{palaniappan2016stochastic}. 

\subsection{A noise-free VI reformulation}

In this paper, we propose to solve \eqref{finite-sum-cons-prob} through its Lagrangian dual formulation, which is equivalently a saddle point problem with a special structure that is suitable for applying the accelerated variance reduced method SAVREP-m. In our discussion, we assume $g_i(x)$ is convex for all $i=1,...,m_2$, $h_j(x)=(h_{j,1}(x),\cdots, h_{j,\ell}(x))^\top \in \RR^{\ell}$ and $h_{j,s}(x)$ is convex in $x$ for all $j=1,...,m_1$ and $s=1,...,\ell$, and $\XX\subseteq \RR^n$ is a closed convex set. The corresponding saddle point reformulation of \eqref{finite-sum-cons-prob} solves the following:
\begin{eqnarray}
\min\limits_{x\in\XX}\max\limits_{y\in\mathbb{R}^\ell_+}\quad L(x;y) :=\sum_{i=1}^{m_2} g_i(x) + \sum_{j=1}^{m_1} y^\top h_j(x),\label{finite-sum-SPP-reform}
\end{eqnarray}
where $L(x;y)$ defines the Lagrangian function of $(P)$. The partial gradients of the Lagrangian function are given by:
\[
\left\{
\begin{array}{lcl}
\nabla_x L(x;y) &=& \sum_{i=1}^{m_2} \nabla g_i(x) + \sum_{j=1}^{m_1} \left( J h_j(x) \right)^\top  y \\
%                &=& \sum_{i=1}^{m_2} \nabla f_i(x) + \sum_{j=1}^{m_1}  \sum_{s=1}^\ell  y_s  \nabla g_{j,s} (x) \\
\nabla_y L(x;y) &=& \sum_{j=1}^{m_1} h_j(x) .
\end{array}
\right.
\]
Denote $\YY:=\RR^\ell_+$, then the optimality condition for \eqref{finite-sum-SPP-reform} is the following VI problem:
\begin{quote}
Find $(x^*,y^*) \in \XX \times \YY$ such that
\begin{eqnarray}
\label{finite-sum-const-VI-prob}
\left( \begin{array}{c} \nabla_x L(x^*;y^*) \\ - \nabla_y L(x^*;y^*) \end{array}
\right)^\top \left( \begin{array}{c} x-x^* \\ y - y^* \end{array}
\right) \ge 0,\,\, \mbox{ for all } (x,y) \in \XX\times \YY.
\end{eqnarray}
\end{quote}
Or simply:
\begin{quote}
    Find $z^*\in\ZZ$ such that
    \[
    \langle F(z^*),z-z^*\rangle\ge0,\,\,\mbox{for all } z\in\ZZ,
    \]
\end{quote}
where we let $z:=(x;y)$, $\ZZ:=\XX\times\YY$, and
\begin{eqnarray}
F(z)&:=&  \sum\limits_{j=1}^{m_1}\begin{pmatrix}(Jh_j(x))^\top y\\-h_j(x)\end{pmatrix}+\sum_{i=1}^{m_2} \left( \begin{array}{c} \nabla g_i(x) \\ 0 \end{array} \right) =\left( \begin{array}{c} \nabla_x L(x;y) \\ - \nabla_y L(x;y) \end{array}
\right)\label{finite-sum-const-VI-op}\\
&=& \sum\limits_{j=1}^{m_1}H_j(z)+\sum\limits_{i=1}^{m_2}\nabla g_i(z).\nonumber
\end{eqnarray}
Note that we have transformed the original finite-sum constrained finite-sum optimization problem \eqref{finite-sum-cons-prob} into solving a VI problem with the operator defined in \eqref{finite-sum-const-VI-op}, and such $F(z)$ indeed takes the form of \eqref{finite-sum-VI-opt-prob}, which consists of a finite-sum general VI mappings $\sum\limits_{j=1}^{m_1}\begin{pmatrix}(Jh_j(x))^\top y\\-h_j(x)\end{pmatrix}$ and a finite-sum gradient mappings $\sum\limits_{i=1}^{m_2} \left( \begin{array}{c} \nabla g_i(x) \\ 0 \end{array} \right)$. 
We caution that the {\it variables} $x,y$ used in these expressions should not be confused with the {\it sequences} $\{x^k\},\{y^k\}$ presented in the update \eqref{VR-VI-Opt-Strong-Update} or \eqref{savrep-m-Update}. The former corresponds to the (dual) variables in the original optimization problem, while the latter is general VI variables. We use $z$ as the variable in the VI reformulation \eqref{finite-sum-const-VI-op} to distinguish between the two.
 
Note that the Jacobian of $ \left( \begin{array}{c} \nabla g_i(x) \\ 0 \end{array} \right)$ is
$
\left[  \begin{array}{cc} \nabla^2 g_i(x) & 0 \\ 0 & 0 \end{array} \right],
$
which is positive semidefinite since each $f_i(x)$ is assumed to be convex. On the other hand, the Jacobian of $\begin{pmatrix}(Jh_j(x))^\top y\\-h_j(x)\end{pmatrix}$ is
\[
\begin{pmatrix}\sum\limits_{s=1}^\ell y_s  \nabla^2 h_{j,s} (x)& (Jh_j(x))^\top\\-Jh_j(x)&0_{\ell \times \ell} \end{pmatrix} .
\]
Since $y_s \ge 0$ and $h_{j,s}(x)$ is convex, the above Jacobian matrix is also positive semidefinite. Therefore, we can conclude that the operator $F(z)$ in the VI reformulation is indeed monotone. 

While the efficiency of the variance reduced algorithms for optimization is now commonly recognized when the total number $m_2$ of functions $g_i(x)$ in the summation is large, it is also reasonable to apply similar variance reduced techniques for estimating the constraint functions $h_j(x)$ when the total number $m_1$ in the summation is large, as it can be costly to evaluate all these constraint functions (or their Jacobians) in each iteration.  Problem $(P)$ in \eqref{finite-sum-cons-prob} describes exactly such a situation, and by reformulating the original problem into a finite-sum VI with the special structure \eqref{finite-sum-const-VI-op}, the proposed SAVREP-m in Section \ref{sec:monotone} can be applied.  It incorporates variance reduction into the update process for both finite-sum gradient mappings and finite-sum VI mappings, where the latter is attributed to the (Jacobians) of the constraints $h_i(x)$ and the corresponding dual variable $y$. 
Note that since the dual variable $y$ is constrained to be non-negative, Assumption \ref{ass:monotone-const-bd} in general does not hold in our VI reformulation with the operator \eqref{finite-sum-const-VI-op}, where the constraint is given by $\ZZ:=\XX\times \mathbb{R}^\ell_+$. However, it is merely a convenient assumption for deriving the gradient complexity guarantee in our analysis, while in practice it makes sense to set a large enough diameter constant that contains the optimal dual variable $y^*$ and perform projections onto the bounded constrained set instead. As we will show in Section \ref{sec:numerical}, the improved gradient complexities due to applying variance reduction respectively to the general VI mapping and gradient mapping are indeed observed regardless of the boundedness of the constraint sets in our most general VI reformulation \eqref{finite-sum-const-VI-prob}.

Alternatively, one can also apply SAVREP proposed in Section \ref{sec:strong-monotone}, which solves a strongly monotone VI instead. While the operator \eqref{finite-sum-const-VI-op} in our VI reformulation is merely monotone, it can be easily transformed to a strongly monotone VI by considering the following {\it approximated} VI problem with the perturbed operator:
\begin{eqnarray}
F_{\mu}(z):=F(z)+\mu z,\label{finite-sum-const-perturb-map}
\end{eqnarray}
which is strongly monotone with $\mu>0$ with $F(z)$ defined in \eqref{finite-sum-const-VI-op}. Note that SAVREP only requires $H(z)=\sum\limits_{j=1}^{m_1}H_j(z)$ to be strongly monotone, so the perturbation term $\mu z$ can be associated to $H_j(z)$ for arbitrary $j=1,2,..,m_1$. In particular, we can construct the variance reduced gradient estimators in \eqref{VR-grad-H} as:
\begin{eqnarray}
\hat H(z^{k+0.5}):=H(w^k)+H_{\xi_k}(z^{k+0.5})-H_{\xi_k}(w^k)+\mu z^{k+0.5},\nonumber
\end{eqnarray}
where $\xi_k$ randomly samples from $j=1,2,..,m_1$ and $H_j(\cdot)$ is defined in \eqref{finite-sum-const-VI-op}. The counterpart for $\tilde\nabla g(z^k)$ remains unchanged from \eqref{VR-grad-g}.

To ensure that the solution obtained from the VI associated with $F_{\mu}(z)$ serves as a good approximated solution to the original VI when $\mu$ is small, let us also introduce the following {\it error bound} assumption:
\begin{assumption}[Error Bound]
\label{ass:error-bound}
Let $F(z)$ be monotone and $\mu>0$. Denote $z^*(\mu)$ as the solution to the VI problem with operator $F_{\mu}(z)$, namely:
\[
\langle F_\mu(z^*(\mu)),z-z^*(\mu)\rangle\ge0,\quad\forall z\in\ZZ,
\]
and let $z^*$ be a solution to the VI with operator $F(z)$. There exist constants $\theta\in(0,1],\, c_1,\, c_2>0$ such that for all $0<\mu<c_1$, the following holds:
\begin{eqnarray}
\|z^*(\mu)-z^*\|\le c_2\cdot\mu^\theta.\nonumber
\end{eqnarray}
\end{assumption}
Assumption \ref{ass:error-bound} ensures that by solving an approximated solution to the strongly monotone VI with operator $F_\mu(z)$, we are able to use the exact same solution as the approximated solution to the original monotone VI with $F(z)$, which in turn solves \eqref{finite-sum-cons-prob}. A similar error bound assumption for convex-concave saddle point problem is also discussed in Assumption 5.1 in \cite{huang2022cubic}, where they further showed that the assumption holds with $\theta=1$ for quadratic saddle point functions with bilinear coupling. In theory, taking a perturbation parameter
$\mu=O(\epsilon^{\frac{1}{\theta}})$
%$\mu=\frac{\epsilon}{2c_2}$ 
while applying SAVREP to obtain a $\frac{\epsilon}{2}$-solution $z^k$ will guarantee the same $z^k$ to be an $\epsilon$-solution to the operator $F(z)$. In practice, the single-loop structure of SAVREP makes it easier to implement compared to its monotone variant SAVREP-m.

% Such assumption may seem restrictive at first glance, but it turns out that there are important problem classes that satisfy Assumption \ref{ass:error-bound}. In particular, a special case of $(P)$:
% \begin{eqnarray}
% \begin{array}{lll}
% (P') & \min & \sum_{i=1}^{m_2} x^\top Q_ix \\
% & \mbox{s.t.} & \sum_{j=1}^{m_1} A_jx \le 0 \\
% & & x \in \XX
% \end{array}\nonumber
% \end{eqnarray}
% with $Q_i\succeq0$ for $i=1,...,m_2$ and $A_j\in\mathbb{R}^{\ell\times m}$ for $j=1,...,m_1$, will induce a saddle point problem formulation described earlier that satisfy Assumption \ref{ass:error-bound}. The proof can be extended from results in Lemma 5.1 and Proposition 5.1 in \cite{huang2022cubic} and the proofs therein, and we omit the detailed derivations here.

% \begin{theorem}
% Let $x^*$ be a solution to \eqref{finite-sum-cons-prob} and $y^*$ be the corresponding Lagrangian multiplier. If we apply ASVREP to the VI problem with $F_\mu(z)$ defined in \eqref{finite-sum-const-perturb-map} with $\mu=\frac{\epsilon}{2c_2}$ and $F(z)$ defined in \eqref{finite-sum-const-VI-op} and obtain a $\frac{\epsilon}{2}$-solution $z^k$ such that
% \[
% \|z^k-z^*(\mu)\|\le \frac{\epsilon}{2}.
% \]
% Then $z^k$ is an $\epsilon$-solution to the problem \eqref{finite-sum-cons-prob} such that
% \[
% \|z^k-z^*\|\le\epsilon,
% \]
% with $z^*:=(x^*;y^*)$. The total gradient complexity is given by
% \begin{eqnarray}
% \mathcal{O}
% \left(\left(m_1+m_2+\sqrt{\frac{c_2L_gm_2}{\epsilon}}+\frac{c_2L_h\sqrt{m_1}}{\epsilon}\right)\log\frac{d_0}{\epsilon}\right)\nonumber.
% \end{eqnarray}
% \end{theorem}

\subsection{A stochastic zeroth-order approach}

In this section, we consider the same problem $(P)$ in \eqref{finite-sum-cons-prob} but assume that only the unbiased noisy estimations of each of the function values $g_i(x)$ and constraint function values $h_{j,s}(x)$ are accessible, denoted by $g'_i(x)$ and $h'_{j,s}(x)$ respectively. Under this assumption, the gradients of the functions $\nabla g_i(x)$,$\nabla h_{j,s}(x)$ are not directly available, and the methods applied to problems of this type are often referred to as {\it derivation-free} or {\it zeroth-order} methods. There have been developments in the context of optimization \cite{nesterov2017random, shalev2011online, duchi2015optimal, shamir2017optimal, larson2019derivative}, as well as in the context of saddle point problems \cite{wang2020zeroth,xu2020gradient,liu2019min,roy2019online,menickelly2020derivative, huang2021new}. 

In the following discussion, we present a zeroth-order approach based on the saddle point reformulation \eqref{finite-sum-SPP-reform}. By applying the randomized smoothing approach \cite{nesterov2017random}, we can replace the gradients $\nabla g_i(x)$, $\nabla h_{j,s}(x)$ required in the VI operator \eqref{finite-sum-const-VI-op} with the {\it stochastic zeroth-order gradients}. The resulting VI operator then serves as the stochastic estimators used in the update of SAVREP \eqref{VR-VI-Opt-Strong-Update}, and we shall derive the stochastic bounds in \eqref{sto-bd-H-1}-\eqref{sto-bd-g-1} in terms of the parameters involved in the construction of these stochastic zeroth-order gradients.

Let us first state the assumptions for the stochastic oracles $g'_i(x)$, $h'_{j,s}(x)$:
\begin{eqnarray}
\left\{
\begin{array}{ll}
     \EE'\left[g'_i(x)\right]=g_i(x), \\\smallskip
     \EE'\left[\nabla g'_i(x)\right]=\nabla g_i(x),\\\smallskip
     \EE'\left[\left\|\nabla g'_i(x)-\nabla g_i(x)\right\|^2\right]\le \varsigma_g^2,
\end{array}
\right.\quad
\left\{
\begin{array}{ll}
     \EE'[h'_{j,s}(x)]=h_{j,s}(x),\quad \EE'\left[|h'_{j,s}(x)-h_{j,s}(x)|^2\right]\le\varpi^2, \\\smallskip
     \EE'[\nabla h'_{j,s}(x)]=\nabla h_{j,s}(x),\\\smallskip
     \EE'\left[\left\|\nabla h'_{j,s}(x)-\nabla h_{j,s}(x)\right\|^2\right]\le \varsigma_h^2.
\end{array}
\right.\label{noisy-func-bd}
\end{eqnarray}
Note that we have used $\EE'[\cdot]$ as the expectation taken for the stochastic oracle, suppressing the notation of random variable for simplicity. {In addition, we assume that $g_i(x)$, $\nabla g_i(x)$, $h_{j,s}(x)$, $\nabla h_{j,s}(x)$ are Lipschitz continuous with constants $M_{i,g},L_{i,g},M_{j,s,h},L_{j,s,h}$ respectively.}

Given a function $g(x)$, the corresponding smoothing function with parameter $\varphi$ can be obtained by taking the expectation of random samples taken from the uniform distribution $U_b$ on a Euclidean ball $B$ in $\mathbb{R}^n$, defined as following:
\begin{eqnarray}
g_{\varphi}(x):=\EE_{u\sim U_b}\left[g(x+\varphi u)\right]=\frac{1}{\alpha(n)}\int_Bg(x+\varphi u)du,\nonumber
\end{eqnarray}
where $\alpha(n)$ is the volume of the unit ball $B$. The above smoothing function $g_\varphi(x)$ is then continuously differentiable regardless of the continuity of the original function $g(x)$. We summarize some properties of the smoothing function and its gradient in the next lemma, which can also be found in the literature of zeroth-order methods.

\begin{lemma}
\label{lem:smooth-property}
The smoothing function $g_{\varphi}(x)$ is continuously differentiable. Denote $U_{S_p}$ as the uniform distribution on the unit sphere $S_p$ in $\mathbb{R}^n$. The gradient $\nabla g_{\varphi}(x)$ can be expressed as the following:
\begin{eqnarray}
\nabla g_{\varphi}(x):=\EE_{u\sim U_{S_p}}\left[\frac{n}{\varphi}g(x+\varphi u)u\right]=\EE_{u\sim U_{S_p}}\left[\frac{n}{\varphi}\left(g(x+\varphi u)-g(x)\right)u\right].\nonumber
\end{eqnarray}
Furthermore, if $g(x)$ is also differentiable, then the following bounds hold:
\begin{eqnarray}
\left\|\nabla g_{\varphi}(x)-\nabla g(x)\right\|\le \frac{\varphi nL}{2},\label{smooth-grad-and-grad-bd}
\end{eqnarray}
\begin{eqnarray}
\EE_{u\sim U_{S_p}}\left[\left\|\frac{n}{\varphi}\left(g(x+\varphi u)-g(x)\right)u\right\|^2\right]\le 2n\left\|\nabla g(x)\right\|^2+\frac{\varphi^2n^2L^2}{2},\label{smooth-grad-bd}
\end{eqnarray}
where $L$ is the Lipschitz constant of $\nabla g(x)$.
\end{lemma}
The proof of Lemma \ref{lem:smooth-property} can be found in the literature, and we refer the interested readers to \cite{shalev2011online} (Lemma 4.4) and \cite{gao2018low} (Propositions 2.7.5 and 2.7.6) for the details.


Based on the properties in Lemma \ref{lem:smooth-property}, we now define the {\it stochastic zeroth-order gradient} as the following:
\begin{eqnarray}
G'_{i,\varphi}(x,u)&:=&\frac{n}{\varphi}\left(g'_{i}(x+\varphi u)-g'_{i}(x)\right)u,\nonumber\\
H'_{j,s,\varphi}(x,u)&:=&\frac{n}{\varphi}\left(h'_{j,s}(x+\varphi u)-h'_{j,s}(x)\right)u,\nonumber
\end{eqnarray}
where $u\sim U_{S_p}$, and we have replaced the function values $g(x)$ with the corresponding stochastic oracles $g'_i(x)$ and $h'_{j,s}(x)$ for each of the function. Note that when evaluating the stochastic zeroth-order gradient $G'_{i,\varphi}(x,u)$ ($H'_{j,s,\varphi}(x,u)$), we use the {\it same} random variable $\xi_g$ ($\xi_h$) to evaluate the stochastic function estimator $g'_i(\cdot)$ ($h'_{j,s}(\cdot)$) at $x+\varphi u$ and $x$ respectively. The dependency on the random variables is suppressed for simplicity. The next corollary states that the stochastic zeroth-order gradient is an unbiased estimator of the gradient of the smoothing function $\nabla g_{\varphi}(x)$ with bounded variance.

\begin{corollary}
\label{coro:sto-zero-grad-bds}
The stochastic zeroth-order gradients are unbiased with respect to the gradient of the smoothing function with bounded variance:
\begin{eqnarray}
\EE'_u\left[G'_{i,\varphi}(x,u)\right]=\nabla g_{i,\varphi}(x),\quad \EE'_u\left[H'_{j,s,\varphi}(x,u)\right]=\nabla h_{j,s,\varphi}(x),\nonumber
\end{eqnarray}
and
\begin{eqnarray}
&&\EE'_u\left[\left\|G'_{i,\varphi}(x,u)-\nabla g_{i,\varphi}(x)\right\|^2\right]\le \tilde\varsigma_g^2:=2n\left(M_{i,g}^2+\varsigma_g^2\right)+\frac{\varphi^2n^2L_{i,g}^2}{2},\label{sto-zero-grad-var-g}\\
&&\EE'_u\left[\left\|H'_{j,s,\varphi}(x,u)-\nabla h_{j,s,\varphi}(x)\right\|^2\right]\le \tilde\varsigma_h^2:=2n\left(M_{j,s,h}^2+\varsigma_h^2\right)+\frac{\varphi^2n^2L_{j,s,h}^2}{2}.\label{sto-zero-grad-var-h}
\end{eqnarray}

\begin{proof}
See Appendix \ref{proof:sto-zero-grad-bds}.
\end{proof}
\end{corollary}
% \begin{proof}
% See Appendix \ref{proof:sto-zero-grad-bds}.
% \end{proof}

Now we can replace the gradient mappings in the operator $F(z)$ of the VI reformulation \eqref{finite-sum-const-VI-op} with the stochastic zeroth-order gradients:
\begin{eqnarray}
F'(z):=  \sum\limits_{j=1}^{m_1}\begin{pmatrix}H'_{j,\varphi}(x,u) y\\-h'_j(x)\end{pmatrix}+\sum_{i=1}^{m_2} \left( \begin{array}{c} G'_{i,\varphi}(x,u) \\ 0 \end{array} \right):=\sum\limits_{j=1}^{m_1}H'_j(z)+\sum\limits_{i=1}^{m_2}\nabla g'_i(z),\label{sto-zero-operator}
\end{eqnarray}
where $H'_{j,\varphi}(x,u):=(H'_{j,1,\varphi}(x,u),H'_{j,2,\varphi}(x,u),...,H'_{j,\ell,\varphi}(x,u))$ is a matrix with column vectors being the stochastic zeroth-order gradient $H'_{j,s,\varphi}(x,u)$ for $s=1,2,...,\ell$, and $h'_{j}(x):=(h'_{j,1}(x),...,h'_{j,\ell}(x))^\top$. By constructing the stochastic zeroth-order operator $F'(z)$ as in \eqref{sto-zero-operator}, the proposed SAVREP \eqref{VR-VI-Opt-Strong-Update} is readily applicable to the VI reformulation of problem $(P)$ when the function value estimations are noisy. We conclude this section by summarizing the corresponding stochastic bounds in the forms of \eqref{sto-bd-H-1}-\eqref{sto-bd-g-1}.
\begin{corollary}
\label{coro:sto-bds}
Let $H_j(z),\nabla g_i(z)$ be defined in \eqref{finite-sum-const-VI-op} and $H'_j(z),\nabla g'_i(z)$ be defined in \eqref{sto-zero-operator}. Furthermore, denote $K$ as the total number of iterations performed by SAVREP and define $D_y:=\max\limits_{0\le k\le K}\|y\|$, then we have the following stochastic bounds hold for all $z\in\{z^k\}_{0\le k\le K}$:
\begin{eqnarray}
&\left\|H_j(z)-\EE'[H'_j(z)]\right\|\le\delta_h:=\frac{\varphi nD_y}{2}\sqrt{\sum\limits_{s=1}^{\ell}L^2_{j,s,h}},\quad&\EE'\left[\left\|H_j'(z)-\EE'\left[H'_j(z)\right]\right\|^2\right]\le \sigma_h^2:=\ell\left(\tilde \varsigma_h^2D_y^2+\varpi^2\right),\nonumber\\
&\left\|\nabla g_i(z)-\EE'\left[\nabla g_i'(z)\right]\right\|\le \delta_g:=\frac{\varphi nL_{i,g}}{2},\quad&\EE'\left[\left\|\nabla g_i'(z)-\EE'\left[\nabla g_i'(z)\right]\right\|^2\right]\le\sigma_g^2:=\tilde\varsigma_g^2.\nonumber
\end{eqnarray}

\begin{proof}
See Appendix \ref{proof:sto-bds}.
\end{proof}
\end{corollary}
% \begin{proof}
% See Appendix \ref{proof:sto-bds}.
% \end{proof}




% According to \cite{lin2018level}, a special case of $(P)$ (with some affine linear composition form) would require an oracle complexity of
% \[
% O\left( n(m_1+m_2 \ell) + \frac{n+m_1+m_2 \ell}{\epsilon^2} \right)
% \]
% or
% \[
% O\left(\frac{n(m_1+m_2 \ell)}{\epsilon} \right).
% \]

% According to Kevin's notes, if all the details can be worked out well, and if the strongly monotone case can be extended to the weakly monotone case, then the {\it gradient complexity}\/ for solving $(P)$ would likely be
% \[
% O\left( m_1+m_2 \ell + \sqrt{\frac{m_1}{\epsilon}} + \frac{\sqrt{m_2 \ell}}{\epsilon} \right).
% \]

% I think the oracle complexity discussed in \cite{lin2018level} is more expensive than the gradient complexity that we were discussing in our meetings. So, in any account, it is likely that we could have an improved upper bound for a more general form of expectation-constrained finite-sum optimization model $(P)$. Also, in this note it is purely for convenience to assume the number of terms in each constraint is constant.


\section{Numerical Experiments}
\label{sec:numerical}
In this section, we evaluate the numerical performance of SAVREP and SAVREP-m 
by using the same example as in \cite{lin2018level}, which is a Neyman-Pearson
classification problem \cite{tong2016survey} formulated as
\begin{eqnarray}
\min _{\|\mathbf{x}\|_2 \leq \lambda} \frac{1}{n_0} \sum_{j=1}^{n_0} \phi\left(\mathbf{x}^{\top} \xi_{0 j}\right), \text { s.t. } \frac{1}{n_1} \sum_{j=1}^{n_1} \phi\left(-\mathbf{x}^{\top} \xi_{1 j}\right) \leq r_1,\label{NP-classification}
\end{eqnarray}
where $\phi$ is the loss function, defined as smoothed hinge loss function in the experiment for SAVREP and logistic loss function in the experiment for SAVREP-m.
The dataset is the rcv1 training data set from LIBSVM library with $20,242$ data points with $n_0 = 10, 491$ and $n_1 = 9, 751$ and a dimension of $47,236$.  

\subsection{SAVREP}
In this experiment, the loss function is defined as
\begin{eqnarray}
\phi(t)=
\left\{
\begin{array}{ll}
    \frac{1}{2}-t,&t\le 0,\\
    \frac 1 2 (1-t)^2,& 0<t\le 1,\\
    0,& t>1,
\end{array}\right.\nonumber
\end{eqnarray}
% \[\phi(t)=\begin{cases}
% \frac{1}{2}-t,\quad t\le 0,\\
% \frac 1 2 (1-t)^2,\quad 0<t\le 1,\\
% 0,\quad t>1,
% \end{cases}\]
and we focus on the perturbed problem \ref{finite-sum-const-perturb-map}. The parameters are set as $\lambda = 5$ and $r_1 = 0.1$, and the perturbation is set as $\mu=10^{-5}, 10^{-10}$ respectively. We compare the performance of SAVREP with extragradient with variance reduction (EVR) \cite{alacaoglu2022stochastic}. Both of the methods use the mini-batch with a batch size of $100$ to get the stochastic gradient estimators. We tune $\tau$ for EVR methods and $\alpha$ and $\gamma$ for SAVREP. To give a fair comparison, for all the parameters we tune, we select learning rates from the set $\left\{10^{-k}, 2 \times 10^{-k}, 4 \times 10^{-k}, 8\times 10^{-k}:k \in \mathbb{Z}\right\}$ times the parameter settings for theoretical analysis in their corresponding paper. We use both the distance from the iterates to the optimal solution (solved by CVX mosek) and the norm of $H(x)+\nabla g(x)$ as the performance measure. The results are shown in Figure \ref{fig:savrep-10e-5} ($\mu=10^{-5}$) and Figure \ref{fig:savrep-10e-10} ($\mu=10^{-10}$), with left plots showing distance to the optimal solution and right plots showing the norm of $H(x)+\nabla g(x)$. In these experiments, SAVREP shows faster convergence than EVR. Furthermore, the distance to the optimal solution is shorter for smaller perturbation $\mu=10^{-10}$, which is in line with expectation.

\begin{figure}[htbp]
  \begin{minipage}[t]{0.48\linewidth}
    \centering
    \includegraphics[scale=0.5]{strong_d_com.png}
    %\caption{Distance to the optimal solution}
    %\label{fig01}
  \end{minipage}%
  \begin{minipage}[t]{0.48\linewidth}
    \centering
    \includegraphics[scale=0.5]{strong_g_com.png}
    %\caption{Norm of $H+g$}
    %\label{fig02}
  \end{minipage}
  \caption{Convergence of SAVREP under perturbation $\mu=10^{-5}$}
  \label{fig:savrep-10e-5}
\end{figure}

\begin{figure}[htbp]
  \begin{minipage}[t]{0.48\linewidth}
    \centering
    \includegraphics[scale=0.5]{strong_d_com_2.png}
    %\caption{Distance to the optimal solution}
    %\label{fig01}
  \end{minipage}%
  \begin{minipage}[t]{0.48\linewidth}
    \centering
    \includegraphics[scale=0.5]{strong_g_com_2.png}
    %\caption{Norm of $H+g$}
    %\label{fig02}
  \end{minipage}
  \caption{Convergence of SAVREP under perturbation $\mu=10^{-10}$}
  \label{fig:savrep-10e-10}
\end{figure}

\subsection{SAVREP-m}
In this experiment, we test SAVREP-m on the same problem \eqref{NP-classification}, using the VI formulation without perturbation \eqref{finite-sum-const-VI-op}. The parameter tuning is similar to the previous experiment, while the loss function is defined as the logistic loss function, i.e.\,$\phi(t)=\log(1+\exp(-t))$, with $\lambda=5$ and $r_1=0.4$. The convergence in terms of distance to optimal solution and norm of $H(x)+\nabla g(x)$ are shown in Figure \ref{fig:savrep-m-conv-1}. In addition, the constraint violation and objective function gap are shown in Figure \ref{fig:savrep-m-conv-2}. These results demonstrate the sublinear convergence rate of SAVREP-m, which is consistent with the theoretical guarantee derived in Section \ref{sec:grad-comp-monotone}. On the other hand, EVR in this experiment shows a linear convergence rate similar to that in the previous experiment with perturbation. Note that in the second experiment the problem may still be (locally) strongly  monotone after reformulation even without perturbation. While EVR does not require the strongly monotone modulus in its algorithm and reflects linear convergence automatically, we note that SAVREP-m {\it explicitly} assumes the problem to be merely monotone by using diminishing step sizes, as shown in Corollary \ref{cor-parameter-complexity}. The specific design is necessary for SAVREP-m to guarantee a faster sublinear rate given the composite VI structure \eqref{finite-sum-vi-opt-prob-summarize}, at the cost of not being able to converge linearly when the problem is actually strongly monotone. We remark that the purpose of this experiment is to demonstrate the convergence behavior of SAVREP-m, and in practice it makes sense to apply SAVREP instead with estimated strong monotonicity modulus.

%Even though EVR method shows the linear convergence rate, it might be because of the local good property of the problem.

\begin{figure}[htbp]
  \begin{minipage}[t]{0.48\linewidth}
    \centering
    \includegraphics[scale=0.5]{mono_dis_com_log.png}
    %\caption{Distance to the optimal solution}
    %\label{fig1}
  \end{minipage}%
  \begin{minipage}[t]{0.48\linewidth}
    \centering
    \includegraphics[scale=0.5]{mono_g_com_log.png}
    %\caption{Norm of $H+g$}
    %\label{fig2}
  \end{minipage}
  \caption{Convergence of SAVREP-m: distance to optimal solution (left) and norm of $H+\nabla g$ (right)}
  \label{fig:savrep-m-conv-1}
\end{figure}

\begin{figure}[htbp]
  \begin{minipage}[t]{0.48\linewidth}
    \centering
    \includegraphics[scale=0.5]{mono_c_com_log.png}
    %\caption{Constraint Violation}
    %\label{fig3}
  \end{minipage}%
  \begin{minipage}[t]{0.48\linewidth}
    \centering
    \includegraphics[scale=0.5]{mono_opt_com_log.png}
    %\caption{Objective value minus Optimal value}
    %\label{fig4}
  \end{minipage}
  \caption{Convergence of SAVREP-m: constraint violation (left) and objective function gap (right)}
  \label{fig:savrep-m-conv-2}
\end{figure}


\section{Conclusions}
\label{sec:conclusion}
In this paper, we propose two stochastic variance reduced schemes, SAVREP and SAVREP-m, for solving an extended class of finite-sum VI with strongly monotone operator and monotone operator respectively. The operator consists of the sum of a general VI mapping and a gradient mapping, both with finite-sum structure. By exploiting this special structure and applying variance reduction techniques developed in the literature, we show that both schemes admit improved gradient complexities, compared to existing variance reduction algorithms proposed for general finite-sum VI. In addition, we consider a more general stochastic setup in both proposed schemes, where the stochastic oracles of noisy mappings are adopted in the updates, and derive the corresponding stochastic bounds in the results of our complexity analysis. We show that an application of finite-sum optimization with finite-sum inequality constraints can be reformulated into the finite-sum VI of the special structure discussed in this paper, where the proposed schemes can be readily applied to. Finally, we note that while the established gradient complexity results match the optimal complexities in terms of problem constants in optimization (i.e.\ the constants related to the gradient mapping), the gap between the current lower bound established for finite-sum VI remains unfilled. It is still an open question whether the upper bound or the lower bound can be improved to match the other, and we leave it to future works.



\printbibliography

\begin{appendices}
\section{Proofs for Technical Results}
\subsection{Proof of Lemma \ref{lem:VI-relation-1}}
\label{proof:VI-relation-1}
The optimality condition at $x^{k+0.5}$ yields
\begin{eqnarray}
\langle\gamma\left({H'}(w^k)+\tilde\nabla {g'}(y^k)\right)+x^{k+0.5}-\Bar{x}^k,x-x^{k+0.5}\rangle\ge0,\quad\forall x\in\mathcal{Z}, \label{VI-opt-1}
\end{eqnarray}
and the optimality condition at $x^{k+1}$ yields
\begin{eqnarray}
\langle\gamma\left(\hat {H'}(x^{k+0.5})+\tilde\nabla {g'}(y^k)\right)+x^{k+1}-\Bar{x}^k,x-x^{k+1}\rangle\ge0,\quad\forall x\in\mathcal{Z}.\label{VI-opt-2}
\end{eqnarray}
From \eqref{VI-opt-2}, use the expression of $\Bar{x}^k$:
\begin{eqnarray}
&&\frac{1}{2}\left(\|x^{k+1}-x\|^2+(1-p_1)\|x^{k+1}-x^k\|^2-(1-p_1)\|x^k-x\|^2+p_1\|x^{k+1}-w^k\|^2-p_1\|w^k-x\|^2\right)\nonumber\\
&=&(1-p_1)\langle x^{k+1}-x^k,x^{k+1}-x\rangle+p_1\langle x^{k+1}-w^k,x^{k+1}-x\rangle\le\gamma\langle\hat{{H'}}(x^{k+0.5})+\tilde\nabla {g'}(y^k),x-x^{k+1}\rangle,\nonumber\\
&&\label{VI-bd-0}
\end{eqnarray}
where
\begin{eqnarray}
&&\gamma\langle\hat{{H'}}(x^{k+0.5})+\tilde\nabla {g'}(y^k),x-x^{k+1}\rangle\nonumber\\
&=& \gamma\langle\hat{{H'}}(x^{k+0.5})+\tilde\nabla {g'}(y^k),x-x^{k+0.5}\rangle+\gamma\langle\hat{{H'}}(x^{k+0.5})+\tilde\nabla {g'}(y^k),x^{k+0.5}-x^{k+1}\rangle\nonumber\\
&=& \gamma\langle H(x^{k+0.5})+\tilde\nabla {g'}(y^k),x-x^{k+0.5}\rangle+\gamma\langle\hat{{H'}}(x^{k+0.5})-H(x^{k+0.5}),x-x^{k+0.5}\rangle\nonumber\\
&&+\gamma\langle {H'}(w^k)+\tilde\nabla {g'}(y^k),x^{k+0.5}-x^{k+1}\rangle+\gamma\langle\hat{{H'}}(x^{k+0.5})-{H'}(w^k),x^{k+0.5}-x^{k+1}\rangle.\label{VI-bd-1}
\end{eqnarray}
The third term in the above inequality can be bounded by using \eqref{VI-opt-1} with $x=x^{k+1}$:
\begin{eqnarray}
&&\gamma\langle {H'}(w^k)+\tilde\nabla {g'}(y^k),x^{k+0.5}-x^{k+1}\rangle\le \langle x^{k+0.5}-\Bar{x}^k,x^{k+1}-x^{k+0.5}\rangle\nonumber\\
&=& (1-p_1)\langle x^{k+0.5}-x^k,x^{k+1}-x^{k+0.5}\rangle+p_1\langle x^{k+0.5}-w^k,x^{k+1}-x^{k+0.5}\rangle\nonumber\\
&=& \frac{1}{2}\left(-\|x^{k+1}-x^{k+0.5}\|^2+(1-p_1)\|x^{k+1}-x^k\|^2-(1-p_1)\|x^{k+0.5}-x^k\|^2\right.\nonumber\\
&&\left.+p_1\|x^{k+1}-w^k\|^2-p_1\|x^{k+0.5}-w^k\|^2\right),\nonumber
\end{eqnarray}
while the fourth term can be bounded by:
\begin{eqnarray}
&&\gamma\langle\hat{{H'}}(x^{k+0.5})-{H'}(w^k),x^{k+0.5}-x^{k+1}\rangle=\gamma\langle {H}_{\xi_k}'(x^{k+0.5})-{H}_{\xi_k}'(w^k),x^{k+0.5}-x^{k+1}\rangle\nonumber\\
&\le& \frac{\gamma^2}{2}\|{H}_{\xi_k}'(x^{k+0.5})-{H}_{\xi_k}'(w^k)\|^2+\frac{1}{2}\|x^{k+0.5}-x^{k+1}\|^2.\nonumber
\end{eqnarray}

Combine the above two inequalities with \eqref{VI-bd-1} and use $\EE_{k_1}[\cdot]:=\EE_{\xi_k}[\cdot|x^k,w^k]$:
\begin{eqnarray}
&&\EE_{k_1}\left[\gamma\langle\hat{{H'}}(x^{k+0.5})+\tilde\nabla {g'}(y^k),x-x^{k+1}\rangle\nonumber\right]\nonumber\\
&\le& \EE_{k_1}\left[\gamma\langle H(x^{k+0.5})+\tilde\nabla {g'}(y^k),x-x^{k+0.5}\rangle\right]+\EE_{k_1}\left[\gamma\langle\hat {H'}(x^{k+0.5})-H(x^{k+0.5}),x-x^{k+0.5}\rangle\right]\nonumber\\
&&+\frac{1}{2}\EE_{k_1}\left[\gamma^2\|{H}_{\xi_k}'(x^{k+0.5})-{H}_{\xi_k}'(w^k)\|^2+(1-p_1)\|x^{k+1}-x^k\|^2\right.\nonumber\\
&&\left.-(1-p_1)\|x^{k+0.5}-x^k\|^2+p_1\|x^{k+1}-w^k\|^2-p_1\|x^{k+0.5}-w^k\|^2\right].\label{ineq-e-k1}
\end{eqnarray}

% {\color{blue}
% Note $\EE_{k_1}\left[\gamma\langle\hat H(x^{k+0.5})-H(x^{k+0.5}),x-x^{k+0.5}\rangle\right]=0$ since $x^{k+0.5}$ is deterministic and $\EE_{k_1}\left[\hat H(x^{k+0.5})\right]=H(x^{k+0.5})$.
% }


Let us bound the term $\EE_{k_1}\left[\gamma\langle\hat H'(x^{k+0.5})-H(x^{k+0.5}),x-x^{k+0.5}\rangle\right]$. Note that with $\EE_{k_1}[\cdot]$, $x^{k+0.5}$ is deterministic and $\EE_{k_1}\left[\hat H'(x^{k+0.5})\right]=H'(x^{k+0.5})$. Therefore, 
\begin{eqnarray}
&& \EE_{k_1}\left[\gamma\langle\hat H'(x^{k+0.5})-H(x^{k+0.5}),x-x^{k+0.5}\rangle\right]= \gamma\langle H'(x^{k+0.5})-H(x^{k+0.5}),x-x^{k+0.5}\rangle\nonumber\\
&\le& \frac{\gamma}{2\mu_h}\left\|H'(x^{k+0.5})-H(x^{k+0.5})\right\|^2+\frac{\gamma\mu_h}{2}\|x-x^{k+0.5}\|^2.\label{sto-bias-bd-VI}
\end{eqnarray}
% Define 
% \[
% \bar \varepsilon_x:= \|H'(x)-H(x)\|
% \]
% and note that
% \[
% \EE'[\bar\varepsilon_x^2]\le 2m_1\sigma_h^2+2m_1^2\delta_h^2,
% \]
% which will be used in the later analysis.
%

% {\color{red}
% Note
% \begin{eqnarray}
% &&\EE_{k_1}\left[\gamma\langle\hat H'(x^{k+0.5})-H(x^{k+0.5}),x-x^{k+0.5}\rangle\right]\nonumber\\
% %&=& \EE_{k_1}\left[\gamma\langle\hat H'(x^{k+0.5})-\hat H(x^{k+0.5}),x-x^{k+0.5}\rangle\right]+\EE_{k_1}\left[\gamma\langle\hat H(x^{k+0.5})-H(x^{k+0.5}),x-x^{k+0.5}\rangle\right]\nonumber\\
% %&=& \EE_{k_1}\left[\gamma\langle\hat H'(x^{k+0.5})-\hat H(x^{k+0.5}),x-x^{k+0.5}\rangle\right]\nonumber\\
% &=& \EE_{k_1}\left[\gamma\langle\EE\left[\hat H'(x^{k+0.5})\right]- H(x^{k+0.5}),x-x^{k+0.5}\rangle\right]\nonumber\\
% &=& \EE\left[\gamma\langle\EE_{k_1}\left[\hat H'(x^{k+0.5})\right]- H(x^{k+0.5}),x-x^{k+0.5}\rangle\right]\nonumber\\
% &=& \EE\left[\gamma\langle H'(x^{k+0.5})- H(x^{k+0.5}),x-x^{k+0.5}\rangle\right]\nonumber\\
% &\le& \gamma\left\|\EE\left[\hat H'(x^{k+0.5})\right]- H(x^{k+0.5})\right\|\cdot\|x-x^{k+0.5}\|\nonumber\\
% &\le& \gamma m_1\delta\cdot\|x-x^{k+0.5}\|\nonumber\\
% &\le& \frac{\gamma m_1^2\delta^2}{2\mu_h}+\frac{1}{2}\gamma\mu_h\|x^{k+0.5}-x\|^2,\nonumber
% \end{eqnarray}
% where in the above inequalities $x^{k+0.5}$ is deterministic and in the third equality we use $\EE_{k_1}\left[\hat H(x^{k+0.5})\right]=H(x^{k+0.5})$. In the second equality, the tower property (law of total expectation) is used in view of taking overall expectation in the final step in the following analysis.
% }

% {\color{blue}To Kevin: I guess these inequalities are

% \begin{eqnarray}&&\EE\left[\gamma\langle H'(x^{k+0.5})- H(x^{k+0.5}),x-x^{k+0.5}\rangle\right]\nonumber\\
% &\le& \gamma\EE\left[\left\|H'(x^{k+0.5})- H(x^{k+0.5})\right\|\cdot\|x-x^{k+0.5}\|\right]\nonumber\\
% &\le& \frac{\gamma}{2\mu_h}\EE\left[\left\|H'(x^{k+0.5})- H(x^{k+0.5})\right\|^2\right]+\frac{1}{2}\gamma\mu_h\|x-x^{k+0.5}\|^2\nonumber\\
% &\le& \frac{\gamma}{\mu_h}(m_1\sigma^2+m_1^2\delta^2)+\frac{1}{2}\gamma\mu_h\|x-x^{k+0.5}\|^2\nonumber.
% \end{eqnarray}}
% {\color{red}
% Kevin: In the first step, I guess you have used $\EE_{k_1}\left[\hat H'(x^{k+0.5})\right]\rightarrow H'(x^{k+0.5})$, is that right? The outer expectation is then actually $\EE'[\cdot]$? If that's the case, I think $\EE'[\cdot]$ can only come after we combine everything and take the overall expectation (then use the law of total expectation), if I understand it correctly. At the current stage, we only take $\EE_{k_1}$, and then we take $\EE_{k_2}$ after combining with the optimization part, then finally the overall expectation. That's my understanding. OK.
% }



%where we have used the fact that $x^{k+0.5}$ is deterministic given $x^k,w^k$ and that $\EE_k\left[\hat{H}(x^{k+0.5})\right]=H(x^{k+0.5})$ on the second term of \eqref{VI-bd-1}.
% Let us bound the second term with the following:
% \begin{eqnarray}
% &&\EE_{k_1}\left[\gamma\langle\hat H(x^{k+0.5})-H(x^{k+0.5}),x-x^{k+0.5}\rangle\right]\nonumber\\
% &=& {\color{blue}\EE_{k_1}\left[\gamma\langle\hat H(x^{k+0.5})-H(x^{k+0.5}),x^k-x^{k+0.5}\rangle\right]}\nonumber\\
% &\le& \EE_{k_1}\left[\gamma^2\|\hat H(x^{k+0.5})-H(x^{k+0.5})\|^2\right]+\frac{1}{4}\EE_{k_1}\left[\|x^{k+0.5}-x^k\|^2\right]\nonumber\\
% &\le& \EE_{k_1}\left[\gamma^2\left\|H_{\xi_k}(x^{k+0.5})-H_{\xi_k}(w^k)\right\|^2\right]+\frac{1}{4}\EE_{k_1}\left[\|x^{k+0.5}-x^k\|^2\right]\nonumber
% %&\le& \frac{\gamma^2\bar L_h^2}{2}\|x^{k+0.5}-w^k\|^2+\EE_{k_1}\left[\|x^{k+0.5}-x^k\|^2\right],\nonumber
% \end{eqnarray}
% where in the last inequality we use the fact:
% \begin{eqnarray}
% &&\EE_{k_1}\left[\|\hat H(x^{k+0.5})-H(x^{k+0.5})\|^2\right]\nonumber\\
% &=& \EE_{k_1}\left[\|H(w^k)+H_{\xi_k}(x^{k+0.5})-H_{\xi_k}(w^k)-H(x^{k+0.5})\|^2\right]\nonumber\\
% &=& \EE_{k_1}\left[\left\|H_{\xi_k}(x^{k+0.5})-H_{\xi_k}(w^k)-\EE_{k_1}\left[H_{\xi_k}(x^{k+0.5})-H_{\xi_k}(w^k)\right]\right\|^2\right]\nonumber\\
% &\le& \EE_{k_1}\left[\left\|H_{\xi_k}(x^{k+0.5})-H_{\xi_k}(w^k)\right\|^2\right].\nonumber
% \end{eqnarray}

Continue with \eqref{ineq-e-k1}:
\begin{eqnarray}
&&\EE_{k_1}\left[\gamma\langle\hat{{H'}}(x^{k+0.5})+\tilde\nabla {g'}(y^k),x-x^{k+1}\rangle\nonumber\right]\nonumber\\
&\le& \EE_{k_1}\left[\gamma\langle H(x^{k+0.5})+\tilde\nabla {g'}(y^k),x-x^{k+0.5}\rangle\right]+\frac{1}{2}\EE_{k_1}\left[\gamma^2\|{H}_{\xi_k}'(x^{k+0.5})-{H}_{\xi_k}'(w^k)\|^2\right]\nonumber\\
&&+\frac{1}{2}\EE_{k_1}\left[(1-p_1)\|x^{k+1}-x^k\|^2-\left({1}-p_1\right)\|x^{k+0.5}-x^k\|^2+p_1\|x^{k+1}-w^k\|^2-p_1\|x^{k+0.5}-w^k\|^2\right]\nonumber\\
&&{+\frac{\gamma \bar\varepsilon_{x^{k+0.5}}^2}{2\mu_h}+\frac{1}{2}\EE_{k_1}\left[\gamma\mu_h\|x^{k+0.5}-x\|^2\right]}.\nonumber
\end{eqnarray}

Combine with \eqref{VI-bd-0}:
\begin{eqnarray}
&&\frac{1}{2}\EE_{k_1}\left[\|x^{k+1}-x\|^2-(1-p_1)\|x^k-x\|^2-p_1\|w^k-x\|^2+\left(1-p_1\right)\|x^{k+0.5}-x^k\|^2\right]\nonumber\\
&&+\EE_{k_1}\left[\gamma\langle H(x^{k+0.5})+\tilde\nabla {g'}(y^k),x^{k+0.5}-x\rangle\right]\nonumber\\
&\le& \frac{1}{2}\EE_{k_1}\left[\gamma^2\|{H}_{\xi_k}'(x^{k+0.5})-{H}_{\xi_k}'(w^k)\|^2-p_1\|x^{k+0.5}-w^k\|^2\right]{+\frac{\gamma \bar\varepsilon_{x^{k+0.5}}^2}{2\mu_h}+\frac{1}{2}\EE_{k_1}\left[\gamma\mu_h\|x^{k+0.5}-x\|^2\right]}\nonumber\\
&\le& \frac{1}{2}({2}\gamma^2L_h^2-p_1)\|x^{k+0.5}-w^k\|^2{+\frac{\gamma \bar\varepsilon_{x^{k+0.5}}^2}{2\mu_h}+\frac{1}{2}\EE_{k_1}\left[\gamma\mu_h\|x^{k+0.5}-x\|^2\right]+\gamma^2\EE_{k_1}\left[(\varepsilon_{x^{k+0.5}}+\varepsilon_{w^k})^2\right]}\nonumber.
%&\le& \frac{1}{2}({\color{blue}2}\gamma^2L_h^2-p_1)\|x^{k+0.5}-w^k\|^2{\color{blue}+\frac{\gamma m_1^2\delta^2}{2\mu_h}+\frac{1}{2}\EE_{k_1}\left[\gamma\mu_h\|x^{k+0.5}-x\|^2\right]+4\gamma^2\tilde\sigma_h^2}.\nonumber
\end{eqnarray}

The last inequality is due to the following relation:
\begin{eqnarray}
&&\EE_{k_1}\left[\|H_{\xi_k}'(x^{k+0.5})-H_{\xi_k}'(w^k)\|^2\right]\nonumber\\
&\le& \EE_{k_1}\left[2\|H_{\xi_k}(x^{k+0.5})-H_{\xi_k}(w^k)\|^2\right]+\EE_{k_1}\left[2\left(\|H_{\xi_k}'(x^{k+0.5})-H_{\xi_k}(x^{k+0.5})\|+\|H_{\xi_k}'(w^k)-H_{\xi_k}(w^k)\|\right)^2\right]\nonumber\\
&\le& 2L_h^2\|x^{k+0.5}-w^k\|^2+2\EE_{k_1}\left[\left(\varepsilon_{x^{k+0.5}}+\varepsilon_{w^k}\right)^2\right].\label{Lipschitz-H-sample}
\end{eqnarray}
% where
% \[
% \varepsilon_x:=\|H_{\xi}'(x)-H_{\xi}(x)\|.
% \]
% Since
% \begin{eqnarray}
% \EE_{\xi}\left[\varepsilon_x^2\right]= \sum\limits_{i=1}^{m_1}q_i\cdot\frac{1}{q_i^2}\|H_i'(x)-H_i(x)\|^2=\sum\limits_{i=1}^{m_1}\frac{1}{q_i}\|H_i'(x)-H_i(x)\|^2,\nonumber
% \end{eqnarray}
% we have
% \begin{eqnarray}
% \EE'\left[\varepsilon_x^2\right]=\EE'\left[\EE_{\xi}\left[\varepsilon_x^2\right]\right]&\le& \sum\limits_{i=1}^{m_1}\frac{2}{q_i}\EE'\left[\left\|H_i'(x)-\EE'\left[H_i'(x)\right]\right\|^2+\left\|\EE'\left[H_i'(x)\right]-H_i(x)\right\|^2\right]\nonumber\\
% &\le&2(\sigma_h^2+\delta_h^2)\cdot\sum\limits_{i=1}^{m_1}\frac{1}{q_i}=2L_h\cdot(\sigma_h^2+\delta_h^2)\cdot\sum\limits_{i=1}^{m_1}\frac{1}{L_{h(i)}}:=\tilde\sigma_h^2,\nonumber
% \end{eqnarray}
% which will be used in the later analysis.
%

Rearrange the terms with the strong monotonicity of $H(\cdot)$:
\begin{eqnarray}
&&\EE_{k_1}\left[\gamma\langle H(x)+\tilde\nabla {g'}(y^k),x^{k+0.5}-x\rangle+\gamma\mu_h\|x^{k+0.5}-x\|^2+\frac{1}{2}(p_1-{2}\gamma^2L^2_h)\|x^{k+0.5}-w^k\|^2\right]\nonumber\\
&\le& \EE_{k_1}\left[\gamma\langle H(x^{k+0.5})+\tilde\nabla {g'}(y^k),x^{k+0.5}-x\rangle+\frac{1}{2}(p_1-{2}\gamma^2L^2_h)\|x^{k+0.5}-w^k\|^2\right]\nonumber\\
&\le& \frac{1}{2}\EE_{k_1}\left[-\|x^{k+1}-x\|^2+(1-p_1)\|x^k-x\|^2+p_1\|w^k-x\|^2-\left(1-p_1\right)\|x^{k+0.5}-x^k\|^2\right]\nonumber\\
&&{+\frac{\gamma \bar\varepsilon_{x^{k+0.5}}^2}{2\mu_h}+\frac{1}{2}\EE_{k_1}\left[\gamma\mu_h\|x^{k+0.5}-x\|^2\right]+\gamma^2\EE_{k_1}\left[(\varepsilon_{x^{k+0.5}}+\varepsilon_{w^k})^2\right]}\nonumber,
\end{eqnarray}

% By the analysis of the VI updates $x^{k+0.5},x^{k+1}$, we can obtain the following relation:

% \begin{eqnarray}
% &&\gamma_k\langle H(x)+\nabla g(y^k),x^{k+0.5}-x\rangle+\frac{1}{2}(1-\gamma_k^2M^2)\|x^{k+0.5}-x^k\|^2+\gamma_k{\color{red}\mu_h}\|x^{k+0.5}-x\|^2\nonumber\\
% &\le& \gamma_k\langle H(x^{k+0.5})+\nabla g(y^k),x^{k+0.5}-x\rangle+\frac{1}{2}(1-\gamma_k^2M^2)\|x^{k+0.5}-x^k\|^2\nonumber\\
% &\le& \frac{1}{2}\left(\|x^k-x\|^2-\|x^{k+1}-x\|^2\right),\nonumber
% \end{eqnarray}
which gives us
\begin{eqnarray}
&&\EE_{k_1}\left[\gamma\langle H(x)+\tilde\nabla {g'}(y^k),x^{k+0.5}-x\rangle\right]\nonumber\\
&\le& \frac{1}{2}\EE_{k_1}\left[(1-p_1)\|x^k-x\|^2+p_1\|w^k-x\|^2-\|x^{k+1}-x\|^2\right]\nonumber\\
&&-\frac{1}{2}\EE_{k_1}\left[(p_1-{2}\gamma^2L_h^2)\|x^{k+0.5}-w^k\|^2\right]-{\frac{1}{2}}\EE_{k_1}\left[\gamma{\mu_h}\|x^{k+0.5}-x\|^2\right]-\frac{1}{2}\left(1-p_1\right)\|x^{k+0.5}-x^k\|^2\nonumber\\
&&{+\frac{\gamma \bar\varepsilon_{x^{k+0.5}}^2}{2\mu_h}+\gamma^2\EE_{k_1}\left[(\varepsilon_{x^{k+0.5}}+\varepsilon_{w^k})^2\right]}\nonumber\\
&\le& \frac{1}{2}\EE_{k_1}\left[(1-p_1)\|x^k-x\|^2+p_1\|w^k-x\|^2-\|x^{k+1}-x\|^2\right]\nonumber\\
&&-\frac{1}{2}\EE_{k_1}\left[(p_1-{2}\gamma^2L_h^2)\|x^{k+0.5}-w^k\|^2\right]-\EE_{k_1}\left[\frac{1}{{4}}\gamma{\mu_h}\|x^{k}-x\|^2-{\frac{1}{2}}\gamma\mu_h\|x^{k+0.5}-x^k\|^2\right]\nonumber\\
&&-\frac{1}{2}\left(1-p_1\right)\EE_{k_1}\left[\|x^{k+0.5}-x^k\|^2\right]{+\frac{\gamma \bar\varepsilon_{x^{k+0.5}}^2}{2\mu_h}+\gamma^2\EE_{k_1}\left[(\varepsilon_{x^{k+0.5}}+\varepsilon_{w^k})^2\right]}\nonumber\\\nonumber\\
&\le& \frac{1}{2}\EE_{k_1}\left[(1-p_1-{\frac{1}{2}}\gamma\mu_h)\|x^k-x\|^2+p_1\|w^k-x\|^2-\|x^{k+1}-x\|^2\right]\nonumber\\
&&-\frac{1}{2}\left(p_1-{2}\gamma^2L_h^2\right)\EE_{k_1}\left[\|x^{k+0.5}-w^k\|^2\right]-\frac{1}{2}\left(1-p_1-\gamma\mu_h\right)\EE_{k_1}\left[\|x^{k+0.5}-x^k\|^2\right]\nonumber\\
&&{+\frac{\gamma \bar\varepsilon_{x^{k+0.5}}^2}{2\mu_h}+\gamma^2\EE_{k_1}\left[(\varepsilon_{x^{k+0.5}}+\varepsilon_{w^k})^2\right]},\label{strong-telescope}
\end{eqnarray}
completing the proof.

\subsection{Proof of Lemma \ref{lem:function-relation-1}}
\label{proof:function-relation-1}
Using the Lipchistz continuity of $g(\cdot)$:
\begin{eqnarray}
g(v^{k+1})&\le& g(y^k)+\langle \nabla g(y^k),v^{k+1}-y^k\rangle+\frac{L_g}{2}\|v^{k+1}-y^k\|^2\nonumber\\
&=& g(y^k)+\langle \nabla g(y^k),(1-\alpha-\beta)v^k+\alpha x^{k+0.5}+\beta \bar w^k-y^k\rangle+\frac{L_g\alpha^2}{2}\|x^{k+0.5}-x^k\|^2\nonumber\\
&=& (1-\alpha-\beta)\left(g(y^k)+\langle \nabla g(y^k),v^k-y^k\rangle\right)+\alpha \left(g(y^k)+\langle \nabla g(y^k),x^{k+0.5}-y^k\rangle\right)\nonumber\\
&&+\beta\left(g(y^k)+\langle \nabla g(y^k),\bar w^k-y^k\rangle\right)+\frac{L_g\alpha^2}{2}\|x^{k+0.5}-x^k\|^2\nonumber\\
&\le& (1-\alpha-\beta)g(v^k)+\alpha \left(g(x)+\langle \nabla g(y^k),x^{k+0.5}-x\rangle\right)\nonumber\\
&&+\beta\left(g(y^k)+\langle \nabla g(y^k),\bar w^k-y^k\rangle\right)+\frac{L_g\alpha^2}{2}\|x^{k+0.5}-x^k\|^2\nonumber\\
&=& (1-\alpha-\beta)g(v^k)+\beta\left(g(y^k)+\langle \nabla g(y^k),\bar w^k-y^k\rangle\right)+\frac{L_g\alpha^2}{2}\|x^{k+0.5}-x^k\|^2\nonumber\\
&&+\alpha \left(g(x)+\langle \tilde\nabla {g'}(y^k),x^{k+0.5}-x\rangle+\langle \nabla g(y^k)-\tilde\nabla g(y^k),x^{k+0.5}-x\rangle\right)\nonumber\\
&&+\alpha\langle \tilde\nabla g(y^k)-\tilde\nabla {g'}(y^k),x^{k+0.5}-x\rangle\label{g-ineq-1}.
\end{eqnarray}

Let us first bound the last term $\alpha\langle \tilde\nabla g(y^k)-\tilde\nabla {g'}(y^k),x^{k+0.5}-x\rangle$ in the above inequality \eqref{g-ineq-1}. By taking the expectation $\EE_{k_2}[\cdot]$:
\begin{eqnarray}
&& \alpha\EE_{k_2}\left[\langle \tilde\nabla g(y^k)-\tilde\nabla {g'}(y^k),x^{k+0.5}-x\rangle\right]\nonumber\\
&\le& \EE_{k_2}\left[\frac{4\alpha}{\mu_h}\left\|\tilde\nabla g(y^k)-\tilde\nabla {g'}(y^k)\right\|^2\right]+\EE_{k_2}\left[\frac{\alpha\mu_h}{16}\|x^{k+0.5}-x\|^2\right]\nonumber\\
&\le& \EE_{k_2}\left[\frac{4\alpha}{\mu_h}\left\|\tilde\nabla g(y^k)-\tilde\nabla {g'}(y^k)\right\|^2\right]+\EE_{k_2}\left[\frac{\alpha\mu_h}{8}\|x^{k+0.5}-x^k\|^2\right]+\EE_{k_2}\left[\frac{\alpha\mu_h}{8}\|x^{k}-x\|^2\right].\nonumber\\
&&\label{sto-bias-bd-opt}
\end{eqnarray}
Note that
\begin{eqnarray}
&&\EE_{k_2}\left[\left\|\tilde\nabla g(y^k)-\tilde\nabla {g'}(y^k)\right\|^2\right]\nonumber\\
&\le& \EE_{k_2}\left[\left(\|\nabla g(\bar w^k)-\nabla g'(\bar w^k)\|+\|\nabla g_{\zeta_k}(\bar w^k)-\nabla g_{\zeta_k}'(\bar w^k)\|+\|\nabla g_{\zeta_k}(y^k)-\nabla g_{\zeta_k}'(y^k)\|\right)^2\right]\nonumber\\
&\le& 2\bar \rho_{\bar w^k}^2+2\EE_{k_2}\left[(\rho_{\bar w^k}+\rho_{y^k})^2\right].\nonumber
\end{eqnarray}
% where
% \[
% \bar \rho_x:=\|\nabla g(x)-\nabla g'(x)\|,\quad \rho_x:=\|\nabla g_{\zeta}(x)-\nabla g_{\zeta}'(x)\|.
% \]
% We have the following bounds:
% \begin{eqnarray}
% &&\EE'\left[\EE_{\zeta}[\rho_x^2]\right]\nonumber\\
% &=&\EE'\left[\EE_{\zeta}\left[\|\nabla g_{\zeta}(x)-\nabla g_{\zeta}'(x)\|^2\right]\right]\nonumber\\
% &=& \EE'\left[\sum\limits_{i=1}^{m_2}\pi_i\cdot\frac{1}{\pi_i^2}\|\nabla g_i(x)-\nabla g'_i(x)\|^2\right]\nonumber\\
% &\le& \sum\limits_{i=1}^{m_2}\frac{2}{\pi_i}\EE'\left[\left\|\nabla {g}_{i}( x)-\EE'\left[\nabla {g}_{i}'(x)\right]\right\|^2+\left\|\EE'\left[\nabla {g}_{i}'( x)\right]-\nabla {g}_{i}'( x)\right\|^2\right]\nonumber\\
% &\le&2(\sigma_g^2+\delta_g^2)\cdot\sum\limits_{i=1}^{m_2}\frac{1}{\pi_i}=2L_g\cdot(\sigma_g^2+\delta_g^2)\cdot\sum\limits_{i=1}^{m_2}\frac{1}{L_{g(i)}}:=\tilde\sigma_g^2,\nonumber
% \end{eqnarray}
% and
% \begin{eqnarray}
% \EE'[\bar\rho_x^2]\le 2m_2\sigma_g^2+2m_2^2\delta_g^2,\nonumber
% \end{eqnarray}
% which will both be used in the following analysis.
%


%{\color{red}
% First note that the last term can be bounded by
% \begin{eqnarray}
% &&\alpha\EE_{k_2}\left[\langle \tilde\nabla g(y^k)-\tilde\nabla {\color{red}g'}(y^k),x^{k+0.5}-x\rangle\right]\nonumber\\
% &=& \alpha\EE_{k_2}\left[\langle \tilde\nabla g(y^k)-\tilde\nabla {\color{red}g'}(y^k),x^{k+0.5}-x^k\rangle\right]\nonumber\\
% &=& \alpha\EE_{k_2}\left[\langle \tilde\nabla g(y^k)-\EE\left[\tilde\nabla {\color{red}g'}(y^k)\right],x^{k+0.5}-x^k\rangle\right]\nonumber\\
% &\le& \EE_{k_2}\left[\frac{1}{2L_g}\left\|\tilde\nabla g(y^k)-\EE\left[\tilde\nabla {\color{red}g'}(y^k)\right]\right\|^2\right]+\EE_{k_2}\left[\frac{\alpha^2L_g}{2}\|x^{k+0.5}-x^k\|^2\right]\nonumber\\
% &\le& \EE_{k_2}\left[\frac{\delta_g^2}{L_g}\left(m_2^2+\frac{2}{\pi_i^2}\right)\mid \zeta=i\right]+\EE_{k_2}\left[\frac{\alpha^2L_g}{2}\|x^{k+0.5}-x^k\|^2\right]\nonumber\\
% &=& \frac{\delta_g^2}{L_g}\left(m_2^2+\sum\limits_{i=1}^{m_2}\frac{2}{\pi_i}\right)+\EE_{k_2}\left[\frac{\alpha^2L_g}{2}\|x^{k+0.5}-x^k\|^2\right].\label{sto-bias-bd-opt}
% \end{eqnarray}


%{\color{brown}
%\begin{eqnarray}
%&&\alpha\EE_{k_2}\left[\langle \tilde\nabla g(y^k)-\tilde\nabla {\color{red}g'}(y^k),x^{k+0.5}-x\rangle\right]\nonumber\\
%&=& \alpha\EE_{k_2}\left[\langle \tilde\nabla g(y^k)-\tilde\nabla {\color{red}g'}(y^k),x^{k+0.5}-x^k\rangle\right]+\alpha\EE_{k_2}\left[\langle \tilde\nabla g(y^k)-\tilde\nabla {\color{red}g'}(y^k),x^k-x\rangle\right]\nonumber\\
%&\le& \EE_{k_2}\left[\frac{1}{2L_g}\left\|\tilde\nabla g(y^k)-\tilde\nabla {\color{red}g'}(y^k)\right\|^2\right]+\EE_{k_2}\left[\frac{\alpha^2L_g}{2}\|x^{k+0.5}-x^k\|^2\right]+\alpha\EE_{k_2}\left[\langle \nabla g(y^k)-\nabla {\color{red}g'}(y^k),x^k-x\rangle\right]\nonumber\\
%&\le &\EE_{k_2}\left[\frac{1}{2L_g}\left\|\tilde\nabla g(y^k)-\tilde\nabla {\color{red}g'}(y^k)\right\|^2\right]+\EE_{k_2}\left[\frac{\alpha^2L_g}{2}\|x^{k+0.5}-x^k\|^2\right]\nonumber\\
%&&+\frac{\alpha}{2\mu_h}\EE_{k_2}\left[\| \nabla g(y^k)-\nabla {\color{red}g'}(y^k)\|^2\right]+\frac{\alpha\mu_h}{2}\EE_{k_2}\left[\|x^k-x\|^2\right]\nonumber
%\end{eqnarray}}
% {\color{red}
% Kevin: Perhaps we could do the following:
% \begin{eqnarray}
% && \alpha\EE_{k_2}\left[\langle \tilde\nabla g(y^k)-\tilde\nabla {\color{red}g'}(y^k),x^{k+0.5}-x\rangle\right]\nonumber\\
% &\le& \EE_{k_2}\left[\frac{4\alpha}{\mu_h}\left\|\tilde\nabla g(y^k)-\tilde\nabla {\color{red}g'}(y^k)\right\|^2\right]+\EE_{k_2}\left[\frac{\alpha\mu_h}{16}\|x^{k+0.5}-x\|^2\right]\nonumber\\
% &\le& \EE_{k_2}\left[\frac{4\alpha}{\mu_h}\left\|\tilde\nabla g(y^k)-\tilde\nabla {\color{red}g'}(y^k)\right\|^2\right]+\EE_{k_2}\left[\frac{\alpha\mu_h}{8}\|x^{k+0.5}-x^k\|^2\right]+\EE_{k_2}\left[\frac{\alpha\mu_h}{8}\|x^{k}-x\|^2\right].\nonumber
% \end{eqnarray}
% The last two terms should be able to be absorbed into \eqref{strong-telescope}, and the final result would not change by adjusting the constant order of $\gamma$. The first term will relate to the stochastic error anyway.
% }


% {\color{blue}You are right. The constant $\alpha\mu$ seems to be the one we're using here. The stochastic bound might be:
% \begin{eqnarray}
% && \EE'\left[\left\|\tilde\nabla g(y^k)-\tilde\nabla {g'}(y^k)\right\|^2\right]\nonumber\\
% &\le& \EE'\left[\left(\left\|\nabla g(\bar w^k)-\nabla {g'}(\bar w^k)\right\|+\left\|\nabla g_{\zeta_k}( y^k)-\nabla {g}_{\zeta_k}'( y^k)\right\|+\left\|\nabla g_{\zeta_k}(\bar w^k)-\nabla {g}_{\zeta_k}'(\bar w^k)\right\|\right)^2\right]\nonumber\\
% &\le& 2\left(m_2\delta_g+\frac{2}{\pi_{\zeta_k}}\delta_g\right)^2\nonumber\\
% &&+2\EE'\left[\left(\left\|\nabla g'(\bar w^k)-\EE'\left[\nabla {g'}(\bar w^k)\right]\right\|+\left\|\nabla g_{\zeta_k}'( y^k)-\EE'\left[\nabla {g}_{\zeta_k}'( y^k)\right]\right\|+\left\|\nabla g_{\zeta_k}'(\bar w^k)-\EE'\left[\nabla {g}_{\zeta_k}'(\bar w^k)\right]\right\|\right)^2\right]\nonumber\\
% &\le& 2\left(m_2\delta_g+\frac{2}{\pi_{\zeta_k}}\delta_g\right)^2\nonumber\\
% &&+6\EE'\left[\left\|\nabla g'(\bar w^k)-\EE'\left[\nabla {g'}(\bar w^k)\right]\right\|^2+\left\|\nabla g_{\zeta_k}'( y^k)-\EE'\left[\nabla {g}_{\zeta_k}'( y^k)\right]\right\|^2+\left\|\nabla g_{\zeta_k}'(\bar w^k)-\EE'\left[\nabla {g}_{\zeta_k}'(\bar w^k)\right]\right\|^2\right]\nonumber\\
% &\le& 2\left(m_2+\frac{2}{\pi_{\zeta_k}}\right)^2\delta_g^2+6\left(m_2\sigma_g^2+\frac{2}{\pi_{\zeta_k}^2}\sigma_g^2\right).\nonumber
% \end{eqnarray}

% }

% {\color{blue} To Kevin: Maybe we can use
% \[\EE'\left[\left\|\nabla g(x)-\nabla g'(x)\right\|^2\right]\le 2m_2\sigma_g^2+2m_2^2\delta_g^2\]
% Same with the VI part, we have
% \begin{eqnarray}
% \EE'\left[\left\|\nabla g_{\zeta_k}( y^k)-\nabla {g}_{\zeta_k}'( y^k)\right\|^2\right]&=&\sum\limits_{i=1}^{m_2}\frac{1}{\pi_i}\left\|\nabla g_{i}( y^k)-\nabla {g}_{i}'( y^k)\right\|^2\nonumber\\
% &\le& \sum\limits_{i=1}^{m_1}\frac{2}{\pi_i}\left(\|\nabla {g}_{i}'( y^k)-\EE'\left[\nabla {g}_{i}'(y^k)\right]\|^2+\|\EE'\left[\nabla {g}_{i}'( y^k)\right]-\nabla {g}_{i}'( y^k)\|^2\right)\nonumber\\
% &\le&2(\sigma_g^2+\delta_g^2)\cdot\sum\limits_{i=1}^{m_1}\frac{1}{\pi_i}=2L_g\cdot(\sigma_g^2+\delta_g^2)\cdot\sum\limits_{i=1}^{m_1}\frac{1}{L_{g(i)}}.\nonumber
% \end{eqnarray}
% }




%{\color{green} The second equation may not holds since $\tilde\nabla {\color{red}g'}(y^k)$ may not be independent with $x^{k+0.5}-x^k$.}
%{\color{brown}
%\begin{eqnarray}
%&&\alpha\EE_{k_2}\left[\langle \tilde\nabla g(y^k)-\tilde\nabla {\color{red}g'}(y^k),x^{k+0.5}-x^k\rangle\right]\nonumber\\
%&\le& \EE_{k_2}\left[\frac{1}{2L_g}\left\|\tilde\nabla g(y^k)-\tilde\nabla {\color{red}g'}(y^k)\right\|^2\right]+\EE_{k_2}\left[\frac{\alpha^2L_g}{2}\|x^{k+0.5}-x^k\|^2\right]\nonumber.
%\end{eqnarray}
%}

% In the second inequality, we use the following relation:
% \begin{eqnarray}
% && \left\|\tilde\nabla g(y^k)-\EE\left[\tilde\nabla {g'}(y^k)\right]\right\|^2\nonumber\\
% &\le& \left(\left\|\nabla g(\bar w^k)-\EE\left[\nabla {g'}(\bar w^k)\right]\right\|+\left\|\nabla g_{\zeta_k}( y^k)-\EE\left[\nabla {g}_{\zeta_k}'( y^k)\right]\right\|+\left\|\nabla g_{\zeta_k}(\bar w^k)-\EE\left[\nabla {g}_{\zeta_k}'(\bar w^k)\right]\right\|\right)^2\nonumber\\
% &\le& \left(m_2\delta_g+\frac{2}{\pi_{\zeta_k}}\delta_g\right)^2\nonumber\\
% &\le& 2\left(m_2+\frac{2}{\pi_{\zeta_k}}\right)^2\delta_g^2.\nonumber
% \end{eqnarray}
%}






{Also note that:}
\begin{eqnarray}
&&\EE_{k_2}\left[\langle \nabla g(y^k)-\tilde\nabla g(y^k),x^{k+0.5}-x\rangle\right]= \EE_{k_2}\left[\langle \nabla g(y^k)-\tilde\nabla g(y^k),x^{k+0.5}-x^k\rangle\right]\nonumber\\
&\le& \EE_{k_2}\left[\frac{\beta}{2\alpha L_g}\|\nabla g(y^k)-\tilde\nabla g(y^k)\|^2\right]+\EE_{k_2}\left[\frac{\alpha L_g}{2\beta}\|x^{k+0.5}-x^k\|^2\right]\nonumber\\
&\le& \frac{\beta}{\alpha}\left(g(\bar w^k)-g(y^k)-\langle\nabla g(y^k),\bar w^k-y^k\rangle\right)+\EE_{k_2}\left[\frac{\alpha L_g}{2\beta}\|x^{k+0.5}-x^k\|^2\right]\label{g-ineq-2}.
\end{eqnarray}


In the second inequality, we use the following relation:
\begin{eqnarray}
&&\EE_{k_2}\left[\|\nabla g(y^k)-\tilde\nabla g(y^k)\|^2\right]= \EE_{k_2}\left[\|\nabla g_{\zeta_k}(\bar w^k)-\nabla g_{\zeta_k}(y^k)-\left(\nabla g(\bar w^k)-\nabla g(y^k)\right)\|^2\right]\nonumber\\
&\le& \EE_{k_2}\left[\|\nabla g_{\zeta_k}(\bar w^k)-\nabla g_{\zeta_k}(y^k)\|^2\right]= \sum\limits_{i=1}^{m_2}\frac{1}{\pi_i}\|\nabla g_{i}(\bar w^k)-\nabla g_{i}(y^k)\|^2\nonumber\\
&\leq & \sum\limits_{i=1}^{m_2}\frac{2L_{g(i)}}{\pi_i}\left(g_{i}(\bar w^k)-g_{i}(y^k)-\langle\nabla g_i(y^k),\bar w^k-y^k\rangle\right)=  2L_g\sum\limits_{i=1}^{m_2}\left(g_{i}(\bar w^k)-g_{i}(y^k)-\langle\nabla g_i(y^k),\bar w^k-y^k\rangle\right)\nonumber\\
&=& 2L_g\left(g(\bar w^k)-g(y^k)-\langle\nabla g(y^k),\bar w^k-y^k\rangle\right),\label{ineqopt}
\end{eqnarray}
where the first inequality is from $\mathbb{E}\|\zeta-\mathbb{E} \zeta\|^{2}=\mathbb{E}\|\zeta\|^{2}-\|\mathbb{E} \zeta\|^{2}\leq \mathbb{E}\|\zeta\|^{2}$ and the second inequality is from Theorem 2.1.5 in \cite{nesterov2003introductory}


Combine \eqref{g-ineq-1}, \eqref{sto-bias-bd-opt} and \eqref{g-ineq-2}:
\begin{eqnarray}
\EE_{k_2}\left[g(v^{k+1})\right]&\le& \EE_{k_2}\left[(1-\alpha-\beta)g(v^k)+\alpha g(x)+\beta g(\bar w^k)\right]\nonumber\\
&&+\EE_{k_2}\left[\alpha\langle\tilde \nabla g'(y^k),x^{k+0.5}-x\rangle\right]+{\left(\frac{\alpha^2L_g}{2}+\frac{\alpha^2L_g}{2\beta}+\frac{\alpha\mu_h}{8}\right)}\EE_{k_2}\left[\|x^{k+0.5}-x^k\|^2\right]\nonumber\\
&&+{\frac{\alpha\mu_h}{8}\EE_{k_2}\left[\|x^k-x\|^2\right]+\frac{8\alpha}{\mu_h}\EE_{k_2}[\bar\rho_{\bar w^k}^2]+\frac{8\alpha}{\mu_h}\EE_{k_2}\left[(\rho_{\bar w^k}+\rho_{y^k})^2\right]},\nonumber
\end{eqnarray}
implying
\begin{eqnarray}
\EE_{k_2}\left[g(v^{k+1})-g(x)\right]&\le& \EE_{k_2}\left[(1-\alpha-\beta)\left(g(v^k)-g(x)\right)+\beta \left(g(\bar w^k)-g(x)\right)\right]\nonumber\\
&&+\EE_{k_2}\left[\alpha\langle\tilde\nabla g'(y^k),x^{k+0.5}-x\rangle\right]+{\left(\frac{\alpha^2L_g}{2}+\frac{\alpha^2L_g}{2\beta}+\frac{\alpha\mu_h}{8}\right)}\EE_{k_2}\left[\|x^{k+0.5}-x^k\|^2\right]\nonumber\\
&&+{\frac{\alpha\mu_h}{8}\EE_{k_2}\left[\|x^k-x\|^2\right]+\frac{8\alpha}{\mu_h}\EE_{k_2}[\bar\rho_{\bar w^k}^2]+\frac{8\alpha}{\mu_h}\EE_{k_2}\left[(\rho_{\bar w^k}+\rho_{y^k})^2\right]},\nonumber
\end{eqnarray}
completing the proof.

% Then we have
% \begin{eqnarray}
% g(v^{k+1})-g(x)&\le& (1-\theta)\left(g(v^k)-g(x)\right)-\theta\langle \nabla g(y^k)-\tilde\nabla g(y^k),x^{k+0.5}-x\rangle\nonumber\\
% &&+\theta\langle \tilde\nabla g(y^k),x^{k+0.5}-x\rangle+\frac{L_g\theta^2}{2}\|x^{k+0.5}-\Bar{x}^k\|^2\label{function-value-bd}
% \end{eqnarray}
\subsection{Proof of Theorem \ref{thm:per-iter-conv-1}}
\label{proof:per-iter-conv-1}
In view of Lemma \ref{lem:VI-relation-1} and Lemma \ref{lem:function-relation-1}, we can establish the following inequality
\begin{eqnarray}
& & \EE_{k_2}\left[Q(v^{k+1};x)\right] \nonumber \\
&=&\EE_{k_2}\left[\langle H(x),v^{k+1}-x\rangle+g(v^{k+1})-g(x)\right]\nonumber\\
&=& \EE_{k_2}\left[(1-\alpha-\beta)\langle H(x),v^k-x\rangle+\alpha\langle H(x),x^{k+0.5}-x\rangle+\beta\langle H(x),\bar w^k-x\rangle\right]
%\nonumber\\
%&&
+\EE_{k_2}\left[g(v^{k+1})-g(x)\nonumber\right]\\
&\le& \EE_{k_2}\left[(1-\alpha-\beta)\langle H(x),v^k-x\rangle+\alpha\langle H(x),x^{k+0.5}-x\rangle+\beta\langle H(x),\bar w^k-x\rangle\right]\nonumber\\
&&+ \EE_{k_2}\left[(1-\alpha-\beta)\left(g(v^k)-g(x)\right)+\beta \left(g(\bar w^k)-g(x)\right)\right]\nonumber\\
&&+\EE_{k_2}\left[\alpha\langle\tilde \nabla g'(y^k),x^{k+0.5}-x\rangle\right]+{\left(\frac{\alpha^2L_g}{2}+\frac{\alpha^2L_g}{2\beta}+\frac{\alpha\mu_h}{8}\right)}\EE_{k_2}\left[\|x^{k+0.5}-x^k\|^2\right]\nonumber\\
&&{+\frac{\alpha\mu_h}{8}\EE_{k_2}\left[\|x^k-x\|^2\right]+\frac{8\alpha}{\mu_h}\EE_{k_2}[\bar\rho_{\bar w^k}^2]+\frac{8\alpha}{\mu_h}\EE_{k_2}\left[(\rho_{\bar w^k}+\rho_{y^k})^2\right]}\nonumber\\
&=& (1-\alpha-\beta)\EE_{k_2}\left[\langle H(x),v^k-x\rangle+g(v^k)-g(x)\right]
%\nonumber\\
%&&
+\beta\EE_{k_2}\left[\langle H(x),\bar w^k-x\rangle+g(\bar w^k)-g(x)\right]\nonumber\\
&&+\alpha\EE_{k_2}\left[\langle H(x)+\tilde\nabla g'(y^k),x^{k+0.5}-x\rangle+{\left(\frac{\alpha L_g}{2}+\frac{\alpha L_g}{2\beta}+\frac{\mu_h}{8}\right)}\|x^{k+0.5}-\Bar{x}^k\|^2\right]\nonumber\\
&&+{\frac{\alpha\mu_h}{8}\EE_{k_2}\left[\|x^k-x\|^2\right]+\frac{8\alpha}{\mu_h}\EE_{k_2}[\bar\rho_{\bar w^k}^2]+\frac{8\alpha}{\mu_h}\EE_{k_2}\left[(\rho_{\bar w^k}+\rho_{y^k})^2\right]}\nonumber\\
&\le& (1-\alpha-\beta)\EE_{k_2}\left[Q(v^k;x)\right]+\beta\EE_{k_2}\left[ Q(\bar w^k;x)\right]\nonumber\\
&&+\frac{\alpha}{2\gamma}\EE_{k_2}\left[\left(1-p_1-{\frac{1}{2}}\gamma\mu_h+{\frac{\gamma\mu_h}{4}}\right)\|x^k-x\|^2+p_1\|w^k-x\|^2-\|x^{k+1}-x\|^2\right]\nonumber\\
&&
-\frac{\alpha}{2\gamma}\left(p_1-{2}\gamma^2L_h^2\right)\EE_{k_2}\left[\|x^{k+0.5}-w^k\|^2\right]
%\nonumber\\
%&&
-\frac{\alpha}{2\gamma}\left(1-p_1-\gamma\mu_h-\alpha\gamma L_g-\frac{\alpha \gamma L_g}{\beta}-{\frac{\gamma\mu_h}{4}}\right)\EE_{k_2}\left[\|x^{k+0.5}-x^k\|^2\right]\nonumber\\
&&{+\frac{\alpha\EE_{k_2}\left[ \bar\varepsilon_{x^{k+0.5}}^2\right]}{2\mu_h}+\alpha\gamma\EE_{k_2}\left[(\varepsilon_{x^{k+0.5}}+\varepsilon_{w^k})^2\right]}+{\frac{8\alpha}{\mu_h}\EE_{k_2}[\bar\rho_{\bar w^k}^2]+\frac{8\alpha}{\mu_h}\EE_{k_2}\left[(\rho_{\bar w^k}+\rho_{y^k})^2\right]},\nonumber
\end{eqnarray}
where the last inequality is due to Lemma \ref{lem:VI-relation-1}.

Our goal now is to construct a proper potential function while keeping the coefficients of $\|x^{k+0.5}-w^k\|^2$ and $\|x^{k+0.5}-x^k\|^2$ non-positive. To this end, let us introduce the following bound while noting the expectation $\EE_{k_1+}[\cdot]:=\EE[\cdot|w^k,x^{k+1}]$:
\begin{eqnarray}
\EE_{k_2}\left[\|w^{k+1}-x\|^2\right]&=&\EE_{k_2}\left[\EE_{k_1+}\left[\|w^{k+1}-x\|^2\right]\right]=p_1\EE_{k_2}\left[\|x^{k+1}-x\|^2\right]+(1-p_1)\EE_{k_2}\left[\|w^k-x\|^2\right]\nonumber\\
&=& \EE_{k_2}\left[p_1\|x^{k+1}-x\|^2+(1-p_1-c)\|w^k-x\|^2+c\|w^k-x\|^2\right]\nonumber\\
&\le& \EE_{k_2}\left[p_1\|x^{k+1}-x\|^2+(1-p_1-c)\|w^k-x\|^2\right]\nonumber\\
&&+\EE_{k_2}\left[2c\|x^k-x\|^2+4c\|x^k-x^{k+0.5}\|^2+4c\|x^{k+0.5}-w^k\|^2\right],\nonumber
\end{eqnarray}
where $c>0$ is a parameter that needs to satisfy certain constraints to be determined later. Combine the above inequality with the previous inequality on $Q(v^{k+1};x)$, we have:

\begin{eqnarray}
&&\EE_{k_2}\left[Q(v^{k+1};x)\right]+\frac{\alpha}{2\gamma}\EE_{k_2}\left[(1-p_1)\|x^{k+1}-x\|^2+\|w^{k+1}-x\|^2\right]\nonumber\\
&\le& (1-\alpha-\beta)\EE_{k_2}\left[Q(v^k;x)\right]+\beta\EE_{k_2}\left[ Q(\bar w^k;x)\right]\nonumber\\
&&+\frac{\alpha}{2\gamma}\EE_{k_2}\left[(1-p_1-{\frac{1}{4}}\gamma\mu_h+2c)\|x^k-x\|^2+(1-c)\|w^k-x\|^2\right]\nonumber\\
&&-\frac{\alpha}{2\gamma}\left(p_1-{2}\gamma^2L_h^2-4c\right)\EE_{k_2}\left[\|x^{k+0.5}-w^k\|^2\right]\nonumber\\
&&-\frac{\alpha}{2\gamma}\left(1-p_1-{\frac{5}{4}}\gamma\mu_h-\alpha\gamma L_g-\frac{\alpha \gamma L_g}{\beta}-4c\right)\EE_{k_2}\left[\|x^{k+0.5}-x^k\|^2\right]\nonumber\\
&&{+\frac{\alpha \EE_{k_2}\left[ \bar\varepsilon_{x^{k+0.5}}^2\right]}{2\mu_h}+\alpha\gamma\EE_{k_2}\left[(\varepsilon_{x^{k+0.5}}+\varepsilon_{w^k})^2\right]}+{\frac{8\alpha}{\mu_h}\EE_{k_2}[\bar\rho_{\bar w^k}^2]+\frac{8\alpha}{\mu_h}\EE_{k_2}\left[(\rho_{\bar w^k}+\rho_{y^k})^2\right]}.\label{Q-bd-c}
\end{eqnarray}

By imposing the following constraints on $c$:
\begin{eqnarray}
\left\{
\begin{array}{ll}
     1-{\frac{1}{4}}\gamma\mu_h+2c\le 1, \\
     1-{\frac{1}{4}}\gamma\mu_h+2c\ge1-c
\end{array}
\right.\Longleftrightarrow
\frac{\gamma\mu_h}{12}\le c \le \frac{\gamma\mu_h}{{8}},\nonumber
\end{eqnarray}
let us first take $c=\frac{\gamma\mu_h}{12}$ and impose another set of constraints on $\gamma$ and $\alpha$:
\begin{equation}
    \label{const-2}
    \left\{
    \begin{array}{ll}
        p_1-{2}\gamma^2L_h^2-\frac{\gamma\mu_h}{3}\ge0,\\
        1-p_1-{\frac{19\gamma\mu_h}{12}}-\alpha\gamma L_g-\frac{\alpha \gamma L_g}{\beta}\ge0,
    \end{array}
    \right.
\end{equation}
then \eqref{Q-bd-c} can be reduced to:
\begin{eqnarray}
&& \EE_{k_2}\left[Q(v^{k+1};x)\right]+\frac{\alpha}{2\gamma}\EE_{k_2}\left[(1-p_1)\|x^{k+1}-x\|^2+\|w^{k+1}-x\|^2\right]\nonumber\\
&\le& (1-\alpha-\beta)\EE_{k_2}\left[Q(v^k;x)\right]+\beta\EE_{k_2}\left[ Q(\bar w^k;x)\right]{+\frac{\alpha E_{k_2}\left[ \bar\varepsilon_{x^{k+0.5}}^2\right]}{2\mu_h}+\alpha\gamma\EE_{k_2}\left[(\varepsilon_{x^{k+0.5}}+\varepsilon_{w^k})^2\right]}\nonumber\\
&&+\frac{\alpha}{2\gamma}\EE_{k_2}\left[\left(1-p_1-\frac{\gamma\mu_h}{12}\right)\|x^k-x\|^2+\left(1-\frac{\gamma\mu_h}{12}\right)\|w^k-x\|^2\right]+{\frac{8\alpha}{\mu_h}\EE_{k_2}[\bar\rho_{\bar w^k}^2]+\frac{8\alpha}{\mu_h}\EE_{k_2}\left[(\rho_{\bar w^k}+\rho_{y^k})^2\right]}\nonumber\\
&\le& (1-\alpha-\beta)\EE_{k_2}\left[Q(v^k;x)\right]+\beta\EE_{k_2}\left[ Q(\bar w^k;x)\right]{+\frac{\alpha \EE_{k_2}\left[ \bar\varepsilon_{x^{k+0.5}}^2\right]}{2\mu_h}+\alpha\gamma\EE_{k_2}\left[(\varepsilon_{x^{k+0.5}}+\varepsilon_{w^k})^2\right]}\nonumber\\
&&+\left(1-\frac{\gamma\mu_h}{12}\right)\frac{\alpha}{2\gamma}\EE_{k_2}\left[\left(1-p_1\right)\|x^k-x\|^2+\|w^k-x\|^2\right]+{\frac{8\alpha}{\mu_h}\EE_{k_2}[\bar\rho_{\bar w^k}^2]+\frac{8\alpha}{\mu_h}\EE_{k_2}\left[(\rho_{\bar w^k}+\rho_{y^k})^2\right]}\nonumber.
\end{eqnarray}

Now we only need to add the term $Q(\bar w^{k+1};x)$ to the LHS, by noting:
\begin{eqnarray}
\phi\EE_{k_2}\left[Q(\bar w^{k+1};x)\right]&=&\phi\EE_{k_2}\left[\EE_{k_2+}\left[Q(\bar w^{k+1};x)\right]\right]\nonumber\\
&=& \phi p_2\EE_{k_2}\left[Q(v^{k+1};x)\right]+\phi(1-p_2)\EE_{k_2}\left[Q(\bar w^k;x)\right],\nonumber
\end{eqnarray}
for any $\phi>0$.

Add the above identity to the previous inequality and take the total expectation, we have:
\begin{eqnarray}
&&\EE\left[(1-\phi p_2)Q(v^{k+1};x)+\phi Q(\bar w^{k+1};x)\right]+\frac{\alpha}{2\gamma}\EE\left[(1-p_1)\|x^{k+1}-x\|^2+\|w^{k+1}-x\|^2\right]\nonumber\\
&\le& \EE\left[(1-\alpha-\beta)Q(v^k;x)+(\beta+\phi(1-p_2)) Q(\bar w^k;x)\right]+\left(1-\frac{\gamma\mu_h}{12}\right)\frac{\alpha}{2\gamma}\EE\left[\left(1-p_1\right)\|x^k-x\|^2+\|w^k-x\|^2\right]\nonumber\\
&&+\EE\left[{\frac{\alpha \EE_{k_2}\left[ \bar\varepsilon_{x^{k+0.5}}^2\right]}{2\mu_h}+\alpha\gamma\EE_{k_2}\left[(\varepsilon_{x^{k+0.5}}+\varepsilon_{w^k})^2\right]}+{\frac{8\alpha}{\mu_h}\EE_{k_2}[\bar\rho_{\bar w^k}^2]+\frac{8\alpha}{\mu_h}\EE_{k_2}\left[(\rho_{\bar w^k}+\rho_{y^k})^2\right]}\right]\label{potential-reduce-sto-error},
\end{eqnarray}
where
{
\begin{eqnarray}
&&\left\{
\begin{array}{ll}
     \EE\left[\EE_{k_2}\left[ \bar\varepsilon_{x^{k+0.5}}^2\right]\right]=\EE\left[\EE'\left[\bar\varepsilon_{x^{k+0.5}}^2\right]\right]\le 2m_1\sigma_h^2+2m_1^2\delta_h^2\\
%     \\
     \EE\left[\EE_{k_2}\left[(\varepsilon_{x^{k+0.5}}+\varepsilon_{w^k})^2\right]\right]=\EE\left[\EE'\left[(\varepsilon_{x^{k+0.5}}+\varepsilon_{w^k})^2\right]\right]\le 2\cdot\left(2L_h\cdot(\sigma_h^2+\delta_h^2)\cdot\sum\limits_{i=1}^{m_1}\frac{1}{L_{h(i)}}\right)=2\tilde\sigma_h^2.
\end{array}
\right.\nonumber\\
&&\left\{
\begin{array}{ll}
      \EE\left[\EE_{k_2}[\bar\rho_{\bar w^k}^2]\right]=\EE\left[\EE'[\bar\rho_{\bar w^k}^2]\right]\le 2m_2\sigma_g^2+2m_2^2\delta_g^2\\
%      \\
     \EE\left[\EE_{k_2}\left[(\rho_{\bar w^k}+\rho_{y^k})^2\right]\right]=\EE\left[\EE'\left[(\rho_{\bar w^k}+\rho_{y^k})^2\right]\right]\le 2\cdot\left(2L_g\cdot(\sigma_g^2+\delta_g^2)\cdot\sum\limits_{i=1}^{m_2}\frac{1}{L_{g(i)}}\right)=2\tilde\sigma_g^2.
\end{array}
\right.\nonumber
\end{eqnarray}
}

By taking $x=x^*$ in \eqref{potential-reduce-sto-error} together with the expression of $\Delta_h$ and $\Delta_g$, we obtain \eqref{potential-reduce-sto-error-2}, thus complete the proof.
% \begin{eqnarray}
% &&\EE\left[(1-\phi p_2)Q(v^{k+1};x^*)+\phi Q(\bar w^{k+1};x^*)\right]+\frac{\alpha}{2\gamma}\EE\left[(1-p_1)\|x^{k+1}-x^*\|^2+\|w^{k+1}-x^*\|^2\right]\nonumber\\
% &\le& \EE\left[(1-\alpha-\beta)Q(v^k;x^*)+(\beta+\phi(1-p_2)) Q(\bar w^k;x^*)\right]\nonumber\\
% &&+\left(1-\frac{\gamma\mu_h}{12}\right)\frac{\alpha}{2\gamma}\EE\left[\left(1-p_1\right)\|x^k-x^*\|^2+\|w^k-x^*\|^2\right]+\Delta_h+\Delta_g.\label{potential-reduce-sto-error-2}
% \end{eqnarray}
\subsection{Proof of Proposition \ref{prop:grad-complexity-1}}
\label{proof:grad-complexity-1}
We first show that the parameters specified in Proposition \ref{prop:grad-complexity-1} satisfy the constraint \eqref{const-2}. Indeed, the constraints will be reduced to the following:
\[
p_1-\frac{p_1}{8}-\frac{p_1}{12}\ge0,\quad \frac{15}{16}-\frac{67p_1}{48}\ge0,
\]
where the second inequality holds because $p_1=\frac{1}{m_1}$ and we assume trivially that $m_1\ge2$.

Next, inequality \eqref{potential-reduce-sto-error-2} in Theorem \ref{thm:per-iter-conv-1} implies that the reduction rate is given by:
\begin{eqnarray}
C_{\scriptsize\mbox{red}1}:=\max\left\{\frac{1-\alpha-\beta}{1-\phi p_2},\frac{\beta+\phi(1-p_2)}{\phi},1-\frac{\gamma\mu_h}{12}\right\}.\nonumber
\end{eqnarray}
With the choice of $\phi$ and $p_2$, the following bounds hold:
\begin{eqnarray}
\frac{1-\alpha-\beta}{1-\phi p_2}=\frac{1-2\alpha}{1-\alpha}\le 1-\alpha,\nonumber
\end{eqnarray}
and 
\begin{eqnarray}
\frac{\beta+\phi(1-p_2)}{\phi}=1-\frac{1}{m_2}+\frac{1}{(1+\alpha)m_2}=1-\frac{\alpha}{(1+\alpha)m_2}\le 1-\frac{\alpha}{2m_2}.\nonumber
\end{eqnarray}
Therefore, the reduction rate is can be further expressed as
\begin{eqnarray}
&&\max\left\{\frac{1-\alpha-\beta}{1-\phi p_2},\frac{\beta+\phi(1-p_2)}{\phi},1-\frac{\gamma\mu_h}{12}\right\}\le\max\left\{1-\alpha,1-
\frac{\alpha}{2m_2},1-\frac{\gamma\mu_h}{12}\right\}\nonumber\\
&=&\max\left\{1-
\frac{\alpha}{2m_2},1-\frac{\gamma\mu_h}{12}\right\}\nonumber\\
%&\le& \max\left\{1-\frac{\sqrt{\mu_h}}{24\sqrt{L_gm_2}},1-\frac{1}{24m_2},1-\frac{\mu_h}{L_h\sqrt{m_1}},1-\frac{\sqrt{\mu_h}}{12\sqrt{L_gm_2}},1-\frac{1}{12(m_1+5)}\right\} \nonumber
&=&{\max\left\{\max\left(1-\frac{\sqrt{\mu_h}}{24\sqrt{L_gm_2}},1-\frac{1}{24m_2}\right),\max\left(1-\frac{\mu_h}{48L_h\sqrt{m_1}},1-\frac{\sqrt{\mu_h}}{48\sqrt{L_gm_2}},1-\frac{1}{48m_1}\right)\right\}} \nonumber\\
&{=}& \max\left\{1-\frac{\sqrt{\mu_h}}{24\sqrt{L_gm_2}},1-\frac{1}{24m_2},1-\frac{\mu_h}{48L_h\sqrt{m_1}},1-\frac{\sqrt{\mu_h}}{48\sqrt{L_gm_2}},1-\frac{1}{48m_1}\right\}{:=C_{\scriptsize\mbox{red}2}},\nonumber
\end{eqnarray}
{and \eqref{potential-reduce-sto-error-2} becomes
\begin{eqnarray}
&&\EE\left[(1-\phi p_2)Q(v^{k+1};x^*)+\phi Q(\bar w^{k+1};x^*)\right]+\frac{\alpha}{2\gamma}\EE\left[(1-p_1)\|x^{k+1}-x^*\|^2+\|w^{k+1}-x^*\|^2\right]\nonumber\\
&\le& \EE\left[(1-\alpha-\beta)Q(v^k;x^*)+(\beta+\phi(1-p_2)) Q(\bar w^k;x^*)\right]\nonumber\\
&&+\left(1-\frac{\gamma\mu_h}{12}\right)\frac{\alpha}{2\gamma}\EE\left[\left(1-p_1\right)\|x^k-x^*\|^2+\|w^k-x^*\|^2\right]+\Delta_h+\Delta_g\nonumber\\
&\le& C_{\scriptsize\mbox{red}1}\cdot\left(\EE\left[(1-\phi p_2)Q(v^k;x^*)+\phi Q(\bar w^k;x^*)\right]+\frac{\alpha}{2\gamma}\EE\left[\left(1-p_1\right)\|x^k-x^*\|^2+\|w^k-x^*\|^2\right]\right)+\Delta_h+\Delta_g\nonumber\\
&\le& C_{\scriptsize\mbox{red}2}\cdot\left(\EE\left[(1-\phi p_2)Q(v^k;x^*)+\phi Q(\bar w^k;x^*)\right]+\frac{\alpha}{2\gamma}\EE\left[\left(1-p_1\right)\|x^k-x^*\|^2+\|w^k-x^*\|^2\right]\right)+\Delta_h+\Delta_g\nonumber\\
&\le& C_{\scriptsize\mbox{red}2}^{k+1}\cdot\left(\EE\left[(1-\phi p_2)Q(v^0;x^*)+\phi Q(\bar w^0;x^*)\right]+\frac{\alpha}{2\gamma}\EE\left[\left(1-p_1\right)\|x^0-x^*\|^2+\|w^0-x^*\|^2\right]\right)\nonumber\\
&&+\sum\limits_{i=0}^kC_{\scriptsize\mbox{red}2}^{i}\left(\Delta_h+\Delta_g\right)\nonumber.
\end{eqnarray}
Note $v^0:=\bar w^0:=w^0=x^0$. Therefore,
\begin{eqnarray}
&&\EE\left[(1-p_1)\|x^{k+1}-x^*\|^2+\|w^{k+1}-x^*\|^2\right]\nonumber\\
&\le& \frac{2\gamma}{\alpha}\cdot C_{\scriptsize\mbox{red}2}^{k+1}\cdot\left(\EE\left[(1-\phi p_2)Q(v^0;x^*)+\phi Q(\bar w^0;x^*)\right]+\frac{\alpha}{2\gamma}\EE\left[\left(1-p_1\right)\|x^0-x^*\|^2+\|w^0-x^*\|^2\right]\right)\nonumber\\
&&+\frac{2\gamma}{\alpha}\sum\limits_{i=0}^kC_{\scriptsize\mbox{red}2}^{i}\left(\Delta_h+\Delta_g\right)\nonumber\\
&\le& C_{\scriptsize\mbox{red}2}^{k+1}\cdot\left(\frac{4\gamma}{\alpha}Q(x^0;x^*)+2\|x^0-x^*\|^2\right)+\frac{2\gamma}{\alpha}\sum\limits_{i=0}^kC_{\scriptsize\mbox{red}2}^{i}\left(\Delta_h+\Delta_g\right)\nonumber\\
&\le& C_{\scriptsize\mbox{red}2}^{k+1}\cdot\left(\frac{\gamma}{\alpha\mu_h}\left\|H(x^0)+\nabla g(x^0)\right\|^2+2\|x^0-x^*\|^2\right)+\frac{2\gamma}{\alpha}\sum\limits_{i=0}^kC_{\scriptsize\mbox{red}2}^{i}\left(\Delta_h+\Delta_g\right)\nonumber
\end{eqnarray}

% {\color{blue}
% I'm actually a little confused. Let $\alpha=\min(a,b)$, then $1-\alpha\ge 1-a$ and $1-\alpha\ge 1-b$, meaning $1-\alpha\ge\max(1-a,1-b)$. Since the parameters are chosen to be the minimum, the convergence rate is only guaranteed to be the {\it worse} of the two(three), so the convergence rate given by the parameters is larger(worse).
% }


By using the expression of $C_{\scriptsize\mbox{red}2}$, the above rate guarantees the iteration complexity for reducing the deterministic error to $\epsilon$ is
\begin{eqnarray}
\mathcal{O}\left(\frac{1}{1-C_{\scriptsize\mbox{red}2}}\log\frac{d_0}{\epsilon}\right)=\mathcal{O}
\left(\left(m_1+m_2+\sqrt{\frac{L_gm_2}{\mu_h}}+\frac{L_h\sqrt{m_1}}{\mu_h}\right)\ln\frac{d_0}{\epsilon}\right)
\label{grad-comp-1}
\end{eqnarray}
where the expected per iteration gradient cost is
$
\mathcal{O}(p_1m_1+p_2m_2+4)=\mathcal{O}(1).
$
}

{
The additional stochastic error (per iteration) has the order:
\begin{eqnarray}
&&\Delta_h= \mathcal{O}\left(\frac{\sqrt{m_2}(m_1\sigma_h^2+m_1^2\delta_h^2)}{\sqrt{L_g\mu_h}}+\frac{L_h}{L_g}\cdot(\sigma_h^2+\delta_h^2)\cdot\sum\limits_{i=1}^{m_1}\frac{1}{L_{h(i)}}\right),\nonumber\\
&& {\Delta_g=\mathcal{O}\left(\left(\frac{\sqrt{m_2}}{\sqrt{L_g\mu_h}}\right)\left(m_2\sigma_g^2+m_2^2\delta_g^2+L_g\cdot(\sigma_g^2+\delta_g^2)\cdot\sum\limits_{i=1}^{m_2}\frac{1}{L_{g(i)}}\right)\right)}\nonumber.
\end{eqnarray}
{The total stochastic error after reducing the deterministic error to $\epsilon$ is then multiplied by the factor
\[
\sum\limits_{i=0}^kC_{\scriptsize\mbox{red}2}^i=\mathcal{O}\left(\frac{1}{1-C_{\scriptsize\mbox{red}2}}\right),
\]
and is summarized as
\[
\mathcal{O}\left(\left(m_1+m_2+\sqrt{\frac{L_gm_2}{\mu_h}}+\frac{L_h\sqrt{m_1}}{\mu_h}\right)\cdot\frac{\gamma}{\alpha}\cdot(\Delta_h{+\Delta_g})\right).
\]
}
}

\subsection{Proof of Lemma \ref{lem:savrep-m-inner-relation}}
\label{proof:savrep-m-inner-relation}
The proof of this lemma follows the similar logic to the proof of SAVREP in Section \ref{sec:strong-monotone}. We first consider the sequences related to the VI mapping: $\{\bar x^k\}$, $\{x^{k+0.5}\}$, $\{x^k\}$, $\{w^k\}$. It is immediate that we reach the following inequality:
\begin{eqnarray}
&&\EE_{k_1}\left[\gamma_k\langle\hat{{H'}}(x^{k+0.5})+\tilde\nabla {g'}(y^k),x-x^{k+1}\rangle\nonumber\right]\nonumber\\
&\le& \EE_{k_1}\left[\gamma_k\langle H(x^{k+0.5})+\tilde\nabla {g'}(y^k),x-x^{k+0.5}\rangle\right]+\EE_{k_1}\left[\gamma_k\langle\hat {H'}(x^{k+0.5})-H(x^{k+0.5}),x-x^{k+0.5}\rangle\right]\nonumber\\
&&+\frac{1}{2}\EE_{k_1}\left[\gamma_k^2\|{H}_{\xi_k}'(x^{k+0.5})-{H}_{\xi_k}'(w^k)\|^2+(1-p_1)\|x^{k+1}-x^k\|^2\right.\nonumber\\
&&\left.-(1-p_1)\|x^{k+0.5}-x^k\|^2+p_1\|x^{k+1}-w^k\|^2-p_1\|x^{k+0.5}-w^k\|^2\right],\label{ineq-e-k1-monotone}
\end{eqnarray}
which is the same as \eqref{ineq-e-k1} in the proof of Lemma \ref{lem:VI-relation-1}, except that the parameter $\gamma_k$ now depends on the iteration $k$.

Combining with the bound in \eqref{VI-bd-0}, we have:
\begin{eqnarray}
&&\frac{1}{2}\EE_{k_1}\left[\|x^{k+1}-x\|^2-(1-p_1)\|x^k-x\|^2-p_1\|w^k-x\|^2+\left(1-p_1\right)\|x^{k+0.5}-x^k\|^2\right]\nonumber\\
&&+\EE_{k_1}\left[\gamma_k\langle H(x^{k+0.5})+\tilde\nabla {g'}(y^k),x^{k+0.5}-x\rangle\right]\nonumber\\
&\le& \frac{1}{2}\EE_{k_1}\left[\gamma_k^2\|{H}_{\xi_k}'(x^{k+0.5})-{H}_{\xi_k}'(w^k)\|^2-p_1\|x^{k+0.5}-w^k\|^2\right]+\EE_{k_1}\left[\gamma_k\langle\hat {H'}(x^{k+0.5})-H(x^{k+0.5}),x-x^{k+0.5}\rangle\right]\nonumber\\
&\le& \frac{1}{2}({2}\gamma_k^2L_h^2-p_1)\|x^{k+0.5}-w^k\|^2+\gamma_k^2\EE_{k_1}\left[(\varepsilon_{x^{k+0.5}}+\varepsilon_{w^k})^2\right]+\EE_{k_1}\left[\gamma_k\langle\hat {H'}(x^{k+0.5})-H(x^{k+0.5}),x-x^{k+0.5}\rangle\right],\nonumber
\end{eqnarray}
where the last inequality is due to the bound \eqref{Lipschitz-H-sample}. The monotonicity of $H(\cdot)$ implies:
\begin{eqnarray}
&&\EE_{k_1}\left[\gamma_k\langle H(x)+\tilde\nabla {g'}(y^k),x^{k+0.5}-x\rangle+\frac{1}{2}(p_1-{2}\gamma_k^2L^2_h)\|x^{k+0.5}-w^k\|^2\right]\nonumber\\
&\le& \EE_{k_1}\left[\gamma_k\langle H(x^{k+0.5})+\tilde\nabla {g'}(y^k),x^{k+0.5}-x\rangle+\frac{1}{2}(p_1-{2}\gamma_k^2L^2_h)\|x^{k+0.5}-w^k\|^2\right]\nonumber\\
&\le& \frac{1}{2}\EE_{k_1}\left[-\|x^{k+1}-x\|^2+(1-p_1)\|x^k-x\|^2+p_1\|w^k-x\|^2-\left(1-p_1\right)\|x^{k+0.5}-x^k\|^2\right]\nonumber\\
&&+\gamma_k^2\EE_{k_1}\left[(\varepsilon_{x^{k+0.5}}+\varepsilon_{w^k})^2\right]+\EE_{k_1}\left[\gamma_k\langle\hat {H'}(x^{k+0.5})-H(x^{k+0.5}),x-x^{k+0.5}\rangle\right]\nonumber.
\end{eqnarray}
Rearranging the terms, we get:
\begin{eqnarray}
&&\EE_{k_1}\left[\gamma_k\langle H(x)+\tilde\nabla {g'}(y^k),x^{k+0.5}-x\rangle\right]\nonumber\\
&\le& \frac{1}{2}\EE_{k_1}\left[(1-p_1)\|x^k-x\|^2+p_1\|w^k-x\|^2-\|x^{k+1}-x\|^2\right]\nonumber\\
&&-\EE_{k_1}\left[\frac{1}{2}(p_1-{2}\gamma_k^2L^2_h)\|x^{k+0.5}-w^k\|^2\right]-\frac{1}{2}\left(1-p_1\right)\EE_{k_1}\left[\|x^{k+0.5}-x^k\|^2\right]\nonumber\\
&&+\gamma_k^2\EE_{k_1}\left[(\varepsilon_{x^{k+0.5}}+\varepsilon_{w^k})^2\right]+\EE_{k_1}\left[\gamma_k\langle\hat {H'}(x^{k+0.5})-H(x^{k+0.5}),x-x^{k+0.5}\rangle\right].\label{SAVREP-m-VI-relation}
\end{eqnarray}

The next part of the analysis follows the similar logic to the proof of Lemma \ref{lem:function-relation-1}, which establishes the relation among the sequences $\{y^k\}$, $\{v^k\}$, $\{\bar w^k\}$. It is immediate that we get an inequality similar to \eqref{g-ineq-1}:
\begin{eqnarray}
g(v^{k+1})&\le& (1-\alpha_k-\beta_k)g(v^k)+\beta_k\left(g(y^k)+\langle \nabla g(y^k),\bar w^k-y^k\rangle\right)+\frac{L_g\alpha_k^2}{2}\|x^{k+0.5}-x^k\|^2\nonumber\\
&&+\alpha_k \left(g(x)+\langle \tilde\nabla {g'}(y^k),x^{k+0.5}-x\rangle+\langle \nabla g(y^k)-\tilde\nabla g(y^k),x^{k+0.5}-x\rangle\right)\nonumber\\
&&+\alpha_k\langle \tilde\nabla g(y^k)-\tilde\nabla {g'}(y^k),x^{k+0.5}-x\rangle\nonumber
\end{eqnarray}
with parameters $\alpha_k,\beta_k$ depending on the iterations $k$. Combined with \eqref{g-ineq-2}, we get:
\begin{eqnarray}
\EE_{k_2}\left[g(v^{k+1})-g(x)\right]&\le& \EE_{k_2}\left[(1-\alpha_k-\beta_k)\left(g(v^k)-g(x)\right)+\beta_k \left(g(\bar w^k)-g(x)\right)\right]\nonumber\\
&&+\alpha_k\EE_{k_2}\left[\langle\tilde\nabla g'(y^k),x^{k+0.5}-x\rangle\right]+{\left(\frac{\alpha_k^2L_g}{2}+\frac{\alpha_k^2L_g}{2\beta_k}\right)}\EE_{k_2}\left[\|x^{k+0.5}-x^k\|^2\right]\nonumber\\
&&+\alpha_k\EE_{k_2}\left[\langle \tilde\nabla g(y^k)-\tilde\nabla g'(y^k),x^{k+0.5}-x\rangle\right].\nonumber
\end{eqnarray}
The next steps follow similarly the proof of Theorem \ref{thm:per-iter-conv-1}, by noticing:
\begin{eqnarray}
& & \EE_{k_2}\left[Q(v^{k+1};x)\right] \nonumber \\
&=&\EE_{k_2}\left[\langle H(x),v^{k+1}-x\rangle+g(v^{k+1})-g(x)\right]\nonumber\\
&=& \EE_{k_2}\left[(1-\alpha_k-\beta_k)\langle H(x),v^k-x\rangle+\alpha_k\langle H(x),x^{k+0.5}-x\rangle+\beta_k\langle H(x),\bar w^k-x\rangle\right]
%\nonumber\\
%&&
+\EE_{k_2}\left[g(v^{k+1})-g(x)\nonumber\right]\\
&\le& \EE_{k_2}\left[(1-\alpha_k-\beta_k)\langle H(x),v^k-x\rangle+\alpha_k\langle H(x),x^{k+0.5}-x\rangle+\beta_k\langle H(x),\bar w^k-x\rangle\right]\nonumber\\
&&+ \EE_{k_2}\left[(1-\alpha_k-\beta_k)\left(g(v^k)-g(x)\right)+\beta_k \left(g(\bar w^k)-g(x)\right)\right]\nonumber\\
&&+\alpha_k\EE_{k_2}\left[\langle\tilde \nabla g'(y^k),x^{k+0.5}-x\rangle\right]+{\left(\frac{\alpha_k^2L_g}{2}+\frac{\alpha_k^2L_g}{2\beta_k}\right)}\EE_{k_2}\left[\|x^{k+0.5}-x^k\|^2\right]\nonumber\\
&&+\alpha_k\EE_{k_2}\left[\langle \tilde\nabla g(y^k)-\tilde\nabla g'(y^k),x^{k+0.5}-x\rangle\right]\nonumber\\
&=& (1-\alpha_k-\beta_k)\EE_{k_2}\left[\langle H(x),v^k-x\rangle+g(v^k)-g(x)\right]
%\nonumber\\
%&&
+\beta_k\EE_{k_2}\left[\langle H(x),\bar w^k-x\rangle+g(\bar w^k)-g(x)\right]
\nonumber\\
&&
+\alpha_k\EE_{k_2}\left[\langle H(x)+\tilde\nabla g'(y^k),x^{k+0.5}-x\rangle+{\left(\frac{\alpha_k L_g}{2}+\frac{\alpha_k L_g}{2\beta_k}\right)}\|x^{k+0.5}-\Bar{x}^k\|^2\right]\nonumber\\
&&+\alpha_k\EE_{k_2}\left[\langle \tilde\nabla g(y^k)-\tilde\nabla g'(y^k),x^{k+0.5}-x\rangle\right]\nonumber\\
&\overset{\eqref{SAVREP-m-VI-relation}}{\le}& (1-\alpha_k-\beta_k)\EE_{k_2}\left[Q(v^k;x)\right]+\beta_k\EE_{k_2}\left[ Q(\bar w^k;x)\right]\nonumber\\
&&+\frac{\alpha_k}{2\gamma_k}\EE_{k_2}\left[\left(1-p_1\right)\|x^k-x\|^2+p_1\|w^k-x\|^2-\|x^{k+1}-x\|^2\right]\nonumber\\
&&-\frac{\alpha_k}{2\gamma_k}\left(p_1-{2}\gamma_k^2L_h^2\right)\EE_{k_2}\left[\|x^{k+0.5}-w^k\|^2\right]
%\nonumber\\
%&&
-\frac{\alpha_k}{2\gamma_k}\left(1-p_1-\alpha_k\gamma_k L_g-\frac{\alpha_k \gamma_k L_g}{\beta_k}\right)\EE_{k_2}\left[\|x^{k+0.5}-x^k\|^2\right]\nonumber\\
&&+\alpha_k\EE_{k_2}\left[\langle\hat {H'}(x^{k+0.5})-H(x^{k+0.5}),x-x^{k+0.5}\rangle\right]+\alpha_k\gamma_k\EE_{k_2}\left[(\varepsilon_{x^{k+0.5}}+\varepsilon_{w^k})^2\right]\nonumber\\
&&+\alpha_k\EE_{k_2}\left[\langle \tilde\nabla g(y^k)-\tilde\nabla g'(y^k),x^{k+0.5}-x\rangle\right].\nonumber
\end{eqnarray}
Using the relation \begin{eqnarray}
\EE_{k_2}\left[\|w^{k+1}-x\|^2\right]&=&\EE_{k_2}\left[\EE_{k_1+}\left[\|w^{k+1}-x\|^2\right]\right]=p_1\EE_{k_2}\left[\|x^{k+1}-x\|^2\right]+(1-p_1)\EE_{k_2}\left[\|w^k-x\|^2\right]\nonumber
\end{eqnarray}
in the above inequality and rearranging terms, we get the next inequality,
\begin{eqnarray}\label{eq-non}
&&\EE_{k_2}\left[Q(v^{k+1};x)\right]+\frac{\alpha_k}{2\gamma_k}\EE_{k_2}\left[(1-p_1)\|x^{k+1}-x\|^2+\|w^{k+1}-x\|^2\right]\nonumber\\
&\le& (1-\alpha_k-\beta_k)\EE_{k_2}\left[Q(v^k;x)\right]+\beta_k\EE_{k_2}\left[ Q(\bar w^k;x)\right]
%\nonumber\\
%&&
+\frac{\alpha_k}{2\gamma_k}\EE_{k_2}\left[(1-p_1)\|x^k-x\|^2+\|w^k-x\|^2\right]\nonumber\\
&&-\frac{\alpha_k}{2\gamma_k}\left(p_1-{2}\gamma_k^{2}L_h^2\right)\EE_{k_2}\left[\|x^{k+0.5}-w^k\|^2\right]
%\nonumber\\
%&&
-\frac{\alpha_k}{2\gamma_k}\left(1-p_1-\alpha_k\gamma_k L_g-\frac{\alpha_k \gamma_k L_g}{\beta_k}\right)\EE_{k_2}\left[\|x^{k+0.5}-x^k\|^2\right]\nonumber\\
&&+\alpha_k\EE_{k_2}\left[\langle\hat H'(x^{k+0.5})-H(x^{k+0.5}),x-x^{k+0.5}\rangle\right]+\alpha_k\gamma_k\EE_{k_2}\left[(\varepsilon_{x^{k+0.5}}+\varepsilon_{w^k})^2\right]\nonumber\\
&&+\alpha_k\EE_{k_2}\left[\langle \tilde\nabla g(y^k)-\tilde\nabla g'(y^k),x^{k+0.5}-x\rangle\right].
\end{eqnarray}

Note that with $\EE_{k_1}[\cdot]$, $x^{k+0.5}$ is deterministic and $\EE'\left[\EE_{k_1}\left[\hat H'(x^{k+0.5})\right]\right]=\EE'\left[H'(x^{k+0.5})\right]=H(x^{k+0.5})$. Therefore, 
\begin{eqnarray} \label{eq-non-1}
&& \EE\left[\langle\hat H'(x^{k+0.5})-H(x^{k+0.5}),x-x^{k+0.5}\rangle\right]\nonumber\\
&=& \EE\left[\EE'\left[\EE_{k_1}\left[\langle \hat H'(x^{k+0.5})-H(x^{k+0.5}),x-x^{k+0.5}\rangle\right]\right]\right]= 0.
\end{eqnarray}

Similarly, we apply the above argument to $\EE_{k_2}[\cdot]$ and have
\begin{eqnarray}
&& \EE_{k_2}\left[\langle \tilde\nabla g(y^k)-\tilde\nabla g'(y^k),x^{k}-x\rangle\right]= \langle \nabla g(y^k)-\nabla g'(y^k),x^{k}-x\rangle\nonumber
\end{eqnarray}
which results in
\begin{eqnarray}\label{eq-non-2}
&& \EE\left[\langle \nabla g(y^k)-\nabla g'(y^k),x^{k}-x\rangle\right]= \EE\left[\EE'\left[\langle \nabla g(y^k)-\nabla g'(y^k),x^{k}-x\rangle\right]\right]= 0.
\end{eqnarray}

Combine inequalities \ref{eq-non} \ref{eq-non-1} \ref{eq-non-2} together with the condition \eqref{const-3},
% \begin{equation}
%     \label{const-3}
%     \left\{
%     \begin{array}{ll}
%         p_1-2\gamma^{k2}L_h^2\ge0, \\
%         \\
%         q-p_1-\alpha^k\gamma^k L_g-\frac{\alpha^k \gamma^k L_g}{\beta^k}\ge0,
%     \end{array}
%     \right.
% \end{equation}
we have 
\begin{eqnarray}
&&\EE\left[Q(v^{k+1};x)\right]+\frac{\alpha^k}{2\gamma^k}\EE\left[(1-p_1)\|x^{k+1}-x\|^2+\|w^{k+1}-x\|^2\right]\nonumber\\
&\le& \EE\left[(1-\alpha^k-\beta^k)Q(v^k;x)+\beta^k Q(\bar w^k;x)\right]+\frac{\alpha^k}{2\gamma^k}\EE\left[\left(1-p_1\right)\|x^k-x\|^2+\|w^k-x\|^2\right]\nonumber\\
&&+\alpha^k\gamma^k\EE_{k}\left[(\varepsilon_{x^{k+0.5}}+\varepsilon_{w^k})^2\right]+\alpha^k\EE_{k}\left[\langle \tilde\nabla g(y^k)-\tilde\nabla g'(y^k),x^{k+0.5}-x^k\rangle\right]\nonumber\\
&&-\frac{\alpha^k}{2\gamma^k}\left(1-q\right)\EE_{k_2}\left[\|x^{k+0.5}-x^k\|^2\right]\nonumber\\
&\le&\EE\left[(1-\alpha^k-\beta^k)Q(v^k;x)+\beta^k Q(\bar w^k;x)\right]+\frac{\alpha^k}{2\gamma^k}\EE\left[\left(1-p_1\right)\|x^k-x\|^2+\|w^k-x\|^2\right]\nonumber\\
&&+\alpha^k\gamma^k\EE\left[(\varepsilon_{x^{k+0.5}}+\varepsilon_{w^k})^2\right]+\frac{\alpha^k\gamma^k}{2(1-q)}\EE\left[\| \tilde\nabla g(y^k)-\tilde\nabla g'(y^k)\|^2\right]\nonumber\\
&\le&\EE\left[(1-\alpha^k-\beta^k)Q(v^k;x)+\beta^k Q(\bar w^k;x)\right]+\frac{\alpha^k}{2\gamma^k}\EE\left[\left(1-p_1\right)\|x^k-x\|^2+\|w^k-x\|^2\right]\nonumber\\
&&+\alpha^k\gamma^k\Delta,\nonumber
\end{eqnarray}
for a constant $0<q<1$ to be determined later. Note that since $\EE\left[(\varepsilon_{x^{k+0.5}}+\varepsilon_{w^k})^2\right]\le 2\tilde{\sigma}_h^2$ by the bound \eqref{sto-epsilon-x-bd}, and 
\begin{eqnarray}
\EE\left[\| \tilde\nabla g(y^k)-\tilde\nabla g'(y^k)\|^2\right]&\le& 2\EE\left[\left\|\nabla g(\bar w^k)-\nabla g'(\bar w^k)\right\|^2\right]\nonumber\\
&&+4\EE\left[\left\|\nabla g_{\zeta_k}(y^k)-\nabla g'_{\zeta_k}(y^k)\right\|^2\right]+4\EE\left[\left\|\nabla g_{\zeta_k}(\bar w^k)-\nabla g'_{\zeta_k}(\bar w^k)\right\|^2\right]\nonumber\\
&\overset{\eqref{sto-bd-g-2},\eqref{sto-rho-x-bd}}{\le}& 4m_2\sigma_g^2+8\tilde{\sigma}_g^2,\nonumber
\end{eqnarray}
we have $\Delta=2\tilde\sigma_h^2+\frac{1}{(1-q)}(2m_2\sigma_g^2+4\tilde\sigma_g^2)=O\left(\sigma_h^2 L_h\sum\limits_{i=1}^{m_1}\frac{1}{L_{h(i)}}+\sigma_g^2 L_g\sum\limits_{i=1}^{m_2}\frac{1}{L_{g(i)}}\right)$.


\subsection{Proof of Corollary \ref{coro:sto-zero-grad-bds}}
\label{proof:sto-zero-grad-bds}
The unbiasedness of $G'_{i,\varphi}(x,u)$ and $H'_{j,s,\varphi}(x,u)$ follow immediately from Lemma \ref{lem:smooth-property}. We shall show the variance bound for $G'_{i,\varphi}(x,u)$, and the proof for $H'_{j,s,\varphi}(x,u)$ follows similarly. Note that:
\begin{eqnarray}
&&\EE'_{u}\left[\left\|G'_{i,\varphi}(x,u)\right\|^2\right]=\EE'\left[\EE_u\left[\left\|G'_{i,\varphi}(x,u)\right\|^2\right]\right]=\EE'\left[\EE_u\left[\left\|\frac{n}{\varphi}(g'_i(x+\varphi u)-g'_i(x))u\right\|^2\right]\right]\nonumber\\
&\overset{\eqref{smooth-grad-bd}}{\le}& \EE'\left[2n\left\|\nabla g'_i(x)\right\|^2\right]+\frac{\varphi^2n^2L_{i,g}^2}{2}\nonumber\\
&=& 2n\EE'\left[\left\|\nabla g_i(x)\right\|^2+2\nabla g_i(x)^\top\left(\nabla g'_i(x)-\nabla g_i(x)\right)+\left\|\nabla g'_i(x)-\nabla g_i(x)\right\|^2\right]+\frac{\varphi^2n^2L_{i,g}^2}{2}\nonumber\\
&\overset{\eqref{noisy-func-bd}}{\le}& 2n\left(M_{i,g}^2+\varsigma_g^2\right)+\frac{\varphi^2n^2L_{i,g}^2}{2}.\nonumber
\end{eqnarray}
In the first inequality, we apply the bound in \eqref{smooth-grad-bd} on the function $g'_i(\cdot)$, which is the stochastic function estimator of $g_i(\cdot)$. Note that we use the same random variable to estimate the function at the two points $x+\varphi u$ and $x$ when calculating the stochastic zeroth-order gradient $G'_{i,\varphi}(x,u)$. Now, since $\EE'_u\left[G'_{i,\varphi}(x,u)\right]=\nabla g_{i,\varphi}(x)$, we have:
\begin{eqnarray}
&&\EE'_u\left[\left\|G'_{i,\varphi}(x,u)-\nabla g_{i,\varphi}(x)\right\|^2\right]=\EE'_u\left[\left\|G'_{i,\varphi}(x,u)\right\|^2-\left\|\nabla g_{i,\varphi}(x)\right\|^2\right]\nonumber\\
&\le& \EE'_u\left[\left\|G'_{i,\varphi}(x,u)\right\|^2\right]\le 2n\left(M_{i,g}^2+\varsigma_g^2\right)+\frac{\varphi^2n^2L_{i,g}^2}{2}.\nonumber
\end{eqnarray}

\subsection{Proof of Corollary \ref{coro:sto-bds}}
\label{proof:sto-bds}
We derive the bounds corresponding to $H'_j(z)$. The bounds corresponding to $\nabla g'_i(z)$ can be derived similarly with simpler analysis.

With expressions in \eqref{finite-sum-const-VI-op} and \eqref{sto-zero-operator}, we have:
\begin{eqnarray}
&&\left\|H_j(z)-\EE'\left[H'_j(z)\right]\right\|^2= \left\|Jh_j(x)^\top y-\EE'\left[H'_{j,\varphi}(x,u)y\right]\right\|^2+\left\|h_j(x)-\EE'\left[h'_j(x)\right]\right\|^2\nonumber\\
&=&\left\|\sum\limits_{s=1}^{\ell}y_s\left(\nabla h_{j,s}(x)-\EE'_u\left[H'_{j,s,\varphi}(x,u)\right]\right)\right\|^2= \left\|\sum\limits_{s=1}^{\ell}y_s\left(\nabla h_{j,s}(x)-\nabla h_{j,s,\varphi}(x)\right)\right\|^2\nonumber\\
&\le& \left(\sum\limits_{s=1}^\ell y_s\left\|\nabla h_{j,s}(x)-\nabla h_{j,s,\varphi}(x)\right\|\right)^2\overset{\ref{smooth-grad-and-grad-bd}}{\le} \left(\frac{\varphi n}{2}\sum\limits_{s=1}^\ell y_sL_{j,s,h}\right)^2\nonumber,
\end{eqnarray}
where the second equality is due to $\EE'\left[h'_{j,s}(x)\right]=h_{j,s}(x)$ in our assumption \eqref{noisy-func-bd}. Therefore, by denoting $L_{j,h}:=(L_{j,1,h},L_{j,2,h},...,L_{j,s,h})^\top$,
\begin{eqnarray}
\left\|H_j(z)-\EE'\left[H'_j(z)\right]\right\|\le \frac{\varphi n}{2}\sum\limits_{s=1}^\ell y_sL_{j,s,h}\le \frac{\varphi n}{2}\|y\|\cdot\|L_{j,h}\|\le \frac{\varphi nD_y}{2}\sqrt{\sum\limits_{s=1}^{\ell}L^2_{j,s,h}}.\nonumber
\end{eqnarray}

The second bound can be derived by the following:
\begin{eqnarray}
&&\EE'\left[\left\|H_j'(z)-\EE'\left[H'_j(z)\right]\right\|^2\right]= \EE'\left[\left\|H'_{j,\varphi}(x,u)y-\EE'_u\left[H'_{j,\varphi}(x,u)y\right]\right\|^2+\left\|h'_j(x)-\EE'\left[h'_j(x)\right]\right\|^2\right]\nonumber\\
&\overset{\eqref{noisy-func-bd}}{\le}& \EE'\left[\left\|H'_{j,\varphi}(x,u)y-\EE'_u\left[H'_{j,\varphi}(x,u)y\right]\right\|^2\right]+\ell\varpi^2= \EE'\left[\left\|\sum\limits_{s=1}^\ell y_s\left(H'_{j,s,\varphi}(x,u)-\nabla h_{j,s,\varphi}(x,u)\right)\right\|^2\right]+\ell\varpi^2\nonumber\\
&\le& \EE'\left[\ell\cdot\sum\limits_{s=1}^\ell y_s^2\left\|H'_{j,s,\varphi}(x,u)-\nabla h_{j,s,\varphi}(x,u)\right\|^2\right]+\ell\varpi^2\overset{\eqref{sto-zero-grad-var-h}}{\le} \ell\cdot\tilde{\varsigma}_h^2\cdot\sum\limits_{s=1}^\ell y_s^2+\varpi^2\le \ell\tilde \varsigma_h^2D_y^2+\ell\varpi^2.\nonumber
\end{eqnarray}


\end{appendices}

\end{document}
