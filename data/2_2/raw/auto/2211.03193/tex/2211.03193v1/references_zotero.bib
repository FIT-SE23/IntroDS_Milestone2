
@article{sohl-dickstein_deep_nodate,
	title = {Deep {Unsupervised} {Learning} using {Nonequilibrium} {Thermodynamics}},
	abstract = {A central problem in machine learning involves modeling complex data-sets using highly ﬂexible families of probability distributions in which learning, sampling, inference, and evaluation are still analytically or computationally tractable. Here, we develop an approach that simultaneously achieves both ﬂexibility and tractability. The essential idea, inspired by non-equilibrium statistical physics, is to systematically and slowly destroy structure in a data distribution through an iterative forward diffusion process. We then learn a reverse diffusion process that restores structure in data, yielding a highly ﬂexible and tractable generative model of the data. This approach allows us to rapidly learn, sample from, and evaluate probabilities in deep generative models with thousands of layers or time steps, as well as to compute conditional and posterior probabilities under the learned model. We additionally release an open source reference implementation of the algorithm.},
	language = {en},
	author = {Sohl-Dickstein, Jascha and Weiss, Eric A and Maheswaranathan, Niru and Ganguli, Surya},
	pages = {18},
}

@article{luo_understanding_nodate,
	title = {Understanding {Diffusion} {Models}: {A} {Unified} {Perspective}},
	language = {en},
	author = {Luo, Calvin},
	pages = {23},
}

@article{talebi_estimation_2018,
	title = {Estimation of effective connectivity using multi-layer perceptron artificial neural network},
	volume = {12},
	issn = {1871-4080},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5801276/},
	doi = {10.1007/s11571-017-9453-1},
	abstract = {Studies on interactions between brain regions estimate effective connectivity, (usually) based on the causality inferences made on the basis of temporal precedence. In this study, the causal relationship is modeled by a multi-layer perceptron feed-forward artificial neural network, because of the ANN’s ability to generate appropriate input–output mapping and to learn from training examples without the need of detailed knowledge of the underlying system. At any time instant, the past samples of data are placed in the network input, and the subsequent values are predicted at its output. To estimate the strength of interactions, the measure of “Causality coefficient” is defined based on the network structure, the connecting weights and the parameters of hidden layer activation function. Simulation analysis demonstrates that the method, called “CREANN” (Causal Relationship Estimation by Artificial Neural Network), can estimate time-invariant and time-varying effective connectivity in terms of MVAR coefficients. The method shows robustness with respect to noise level of data. Furthermore, the estimations are not significantly influenced by the model order (considered time-lag), and the different initial conditions (initial random weights and parameters of the network). CREANN is also applied to EEG data collected during a memory recognition task. The results implicate that it can show changes in the information flow between brain regions, involving in the episodic memory retrieval process. These convincing results emphasize that CREANN can be used as an appropriate method to estimate the causal relationship among brain signals.},
	number = {1},
	urldate = {2022-10-31},
	journal = {Cognitive Neurodynamics},
	author = {Talebi, Nasibeh and Nasrabadi, Ali Motie and Mohammad-Rezazadeh, Iman},
	month = feb,
	year = {2018},
	pmid = {29435085},
	pmcid = {PMC5801276},
	note = {21 citations (Semantic Scholar/DOI) [2022-10-31]},
	pages = {21--42},
}

@article{li_novel_2019,
	title = {Novel {Effective} {Connectivity} {Inference} {Using} {Ultra}-{Group} {Constrained} {Orthogonal} {Forward} {Regression} and {Elastic} {Multilayer} {Perceptron} {Classifier} for {MCI} {Identification}},
	volume = {38},
	issn = {1558-254X},
	doi = {10.1109/TMI.2018.2882189},
	abstract = {Mild cognitive impairment (MCI) detection is important, such that appropriate interventions can be imposed to delay or prevent its progression to severe stages, including Alzheimer’s disease (AD). Brain connectivity network inferred from the functional magnetic resonance imaging data has been prevalently used to identify the individuals with MCI/AD from the normal controls. The capability to detect the causal or effective connectivity is highly desirable for understanding directed functional interactions between brain regions and further helping the detection of MCI. In this paper, we proposed a novel sparse constrained effective connectivity inference method and an elastic multilayer perceptron classifier for MCI identification. Specifically, a ultra-group constrained structure detection algorithm is first designed to identify the parsimonious topology of the effective connectivity network, in which the weak derivatives of the observable data are considered. Second, based on the identified topology structure, an effective connectivity network is then constructed by using an ultra-orthogonal forward regression algorithm to minimize the shrinking effect of the group constraint-based method. Finally, the effective connectivity network is validated in MCI identification using an elastic multilayer perceptron classifier, which extracts lower to higher level information from initial input features and hence improves the classification performance. Relatively high classification accuracy is achieved by the proposed method when compared with the state-of-the-art classification methods. Furthermore, the network analysis results demonstrate that MCI patients suffer a rich club effect loss and have decreased connectivity among several brain regions. These findings suggest that the proposed method not only improves the classification performance but also successfully discovers critical disease-related neuroimaging biomarkers.},
	number = {5},
	journal = {IEEE Transactions on Medical Imaging},
	author = {Li, Yang and Yang, Hao and Lei, Baiying and Liu, Jingyu and Wee, Chong-Yaw},
	month = may,
	year = {2019},
	note = {23 citations (Semantic Scholar/DOI) [2022-10-30]
Conference Name: IEEE Transactions on Medical Imaging},
	keywords = {Classification algorithms, Data mining, Diseases, Feature extraction, Functional imaging, Functional magnetic resonance imaging, Multilayer perceptrons, Time series analysis, brain, computer-aided detection and diagnosis, connectivity analysis, machine learning},
	pages = {1227--1239},
}

@article{desai_anatomization_2021,
	title = {An anatomization on breast cancer detection and diagnosis employing multi-layer perceptron neural network ({MLP}) and {Convolutional} neural network ({CNN})},
	volume = {4},
	issn = {2588-9141},
	url = {https://www.sciencedirect.com/science/article/pii/S2588914120300125},
	doi = {10.1016/j.ceh.2020.11.002},
	abstract = {This paper aims to review Artificial neural networks, Multi-Layer Perceptron Neural network (MLP) and Convolutional Neural network (CNN) employed to detect breast malignancies for early diagnosis of breast cancer based on their accuracy in order to identify which method is better for the diagnosis of breast cell malignancies. Deep comparison of functioning of each network and its designing is performed and then analysis is done based on the accuracy of diagnosis and classification of breast malignancy by the network to decide which network outperforms the other. CNN is found to give slightly higher accuracy than MLP for diagnosis and detection of breast cancer. There still is the need to carefully analyse and perform a thorough research that uses both these methods on the same data set under same conditions in order identify the architecture that gives better accuracy.},
	language = {en},
	urldate = {2022-10-30},
	journal = {Clinical eHealth},
	author = {Desai, Meha and Shah, Manan},
	month = jan,
	year = {2021},
	note = {54 citations (Semantic Scholar/DOI) [2022-10-30]},
	keywords = {Artificial neural network, Breast cancer, Detection, Diagnosis},
	pages = {1--11},
}

@article{bou_assi_bispectrum_2018,
	title = {Bispectrum {Features} and {Multilayer} {Perceptron} {Classifier} to {Enhance} {Seizure} {Prediction}},
	volume = {8},
	copyright = {2018 The Author(s)},
	issn = {2045-2322},
	url = {https://www.nature.com/articles/s41598-018-33969-9},
	doi = {10.1038/s41598-018-33969-9},
	abstract = {The ability to accurately forecast seizures could significantly improve the quality of life of patients with drug-refractory epilepsy. Prediction capabilities rely on the adequate identification of seizure activity precursors from electroencephalography recordings. Although a long list of features has been proposed, none of these is able to independently characterize the brain states during transition to a seizure. This work assessed the feasibility of using the bispectrum, an advanced signal processing technique based on higher order statistics, as a precursor of seizure activity. Quantitative features were extracted from the bispectrum and passed through two statistical tests to check for significant differences between preictal and interictal recordings. Results showed statistically significant differences (p {\textless} 0.05) between preictal and interictal states using all bispectrum-extracted features. We used normalized bispectral entropy, normalized bispectral squared entropy, and mean of magnitude as inputs to a 5-layer multilayer perceptron classifier and achieved respective held-out test accuracies of 78.11\%, 72.64\%, and 73.26\%.},
	language = {en},
	number = {1},
	urldate = {2022-10-30},
	journal = {Scientific Reports},
	author = {Bou Assi, Elie and Gagliano, Laura and Rihana, Sandy and Nguyen, Dang K. and Sawan, Mohamad},
	month = oct,
	year = {2018},
	note = {37 citations (Semantic Scholar/DOI) [2022-10-30]
Number: 1
Publisher: Nature Publishing Group},
	keywords = {Epilepsy, Learning algorithms},
	pages = {15491},
}

@inproceedings{fick_analog_2022,
	address = {San Francisco, CA, USA},
	title = {Analog {Matrix} {Processor} for {Edge} {AI} {Real}-{Time} {Video} {Analytics}},
	isbn = {978-1-66542-800-2},
	url = {https://ieeexplore.ieee.org/document/9731773/},
	doi = {10.1109/ISSCC42614.2022.9731773},
	language = {en},
	urldate = {2022-10-28},
	booktitle = {2022 {IEEE} {International} {Solid}- {State} {Circuits} {Conference} ({ISSCC})},
	publisher = {IEEE},
	author = {Fick, Laura and Skrzyniarz, Skylar and Parikh, Malav and Henry, Michael B. and Fick, David},
	month = feb,
	year = {2022},
	note = {4 citations (Semantic Scholar/DOI) [2022-10-28]},
	pages = {260--262},
}

@article{gao_i-_2020,
	title = {i- flow : {High}-dimensional integration and sampling with normalizing flows},
	volume = {1},
	issn = {2632-2153},
	shorttitle = {i- flow},
	url = {https://iopscience.iop.org/article/10.1088/2632-2153/abab62},
	doi = {10.1088/2632-2153/abab62},
	language = {en},
	number = {4},
	urldate = {2022-10-28},
	journal = {Machine Learning: Science and Technology},
	author = {Gao, Christina and Isaacson, Joshua and Krause, Claudius},
	month = nov,
	year = {2020},
	note = {50 citations (Semantic Scholar/DOI) [2022-10-28]},
	pages = {045023},
}

@article{fan_neural_2017,
	title = {{NEURAL} {DATA} {FILTER} {FOR} {BOOTSTRAPPING} {STOCHASTIC} {GRADIENT} {DESCENT}},
	abstract = {Mini-batch based Stochastic Gradient Descent(SGD) has been widely used to train deep neural networks efﬁciently. In this paper, we design a general framework to automatically and adaptively select training data for SGD. The framework is based on neural networks and we call it Neural Data Filter (NDF). In Neural Data Filter, the whole training process of the original neural network is monitored and supervised by a deep reinforcement network, which controls whether to ﬁlter some data in sequentially arrived mini-batches so as to maximize future accumulative reward (e.g., validation accuracy). The SGD process accompanied with NDF is able to use less data and converge faster while achieving comparable accuracy as the standard SGD trained on the full dataset. Our experiments show that NDF bootstraps SGD training for different neural network models including Multi Layer Perceptron Network and Recurrent Neural Network trained on various types of tasks including image classiﬁcation and text understanding.},
	language = {en},
	author = {Fan, Yang and Tian, Fei and Qin, Tao and Liu, Tie-Yan},
	year = {2017},
	pages = {13},
}

@misc{kairouz_advances_2021,
	title = {Advances and {Open} {Problems} in {Federated} {Learning}},
	url = {http://arxiv.org/abs/1912.04977},
	abstract = {Federated learning (FL) is a machine learning setting where many clients (e.g. mobile devices or whole organizations) collaboratively train a model under the orchestration of a central server (e.g. service provider), while keeping the training data decentralized. FL embodies the principles of focused data collection and minimization, and can mitigate many of the systemic privacy risks and costs resulting from traditional, centralized machine learning and data science approaches. Motivated by the explosive growth in FL research, this paper discusses recent advances and presents an extensive collection of open problems and challenges.},
	urldate = {2022-10-25},
	publisher = {arXiv},
	author = {Kairouz, Peter and McMahan, H. Brendan and Avent, Brendan and Bellet, Aurélien and Bennis, Mehdi and Bhagoji, Arjun Nitin and Bonawitz, Kallista and Charles, Zachary and Cormode, Graham and Cummings, Rachel and D'Oliveira, Rafael G. L. and Eichner, Hubert and Rouayheb, Salim El and Evans, David and Gardner, Josh and Garrett, Zachary and Gascón, Adrià and Ghazi, Badih and Gibbons, Phillip B. and Gruteser, Marco and Harchaoui, Zaid and He, Chaoyang and He, Lie and Huo, Zhouyuan and Hutchinson, Ben and Hsu, Justin and Jaggi, Martin and Javidi, Tara and Joshi, Gauri and Khodak, Mikhail and Konečný, Jakub and Korolova, Aleksandra and Koushanfar, Farinaz and Koyejo, Sanmi and Lepoint, Tancrède and Liu, Yang and Mittal, Prateek and Mohri, Mehryar and Nock, Richard and Özgür, Ayfer and Pagh, Rasmus and Raykova, Mariana and Qi, Hang and Ramage, Daniel and Raskar, Ramesh and Song, Dawn and Song, Weikang and Stich, Sebastian U. and Sun, Ziteng and Suresh, Ananda Theertha and Tramèr, Florian and Vepakomma, Praneeth and Wang, Jianyu and Xiong, Li and Xu, Zheng and Yang, Qiang and Yu, Felix X. and Yu, Han and Zhao, Sen},
	month = mar,
	year = {2021},
	note = {2111 citations (Semantic Scholar/arXiv) [2022-10-25]
arXiv:1912.04977 [cs, stat]},
	keywords = {Computer Science - Cryptography and Security, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@misc{chadha_bagging_2020,
	title = {Bagging, {Boosting}, and {Gradient} {Boosting}},
	url = {https://towardsdatascience.com/bagging-boosting-and-gradient-boosting-1a8f135a5f4e},
	abstract = {An Easy-to-Understand Explanation},
	language = {en},
	urldate = {2022-10-25},
	journal = {Medium},
	author = {Chadha, Chirag},
	month = jan,
	year = {2020},
}

@misc{reed_training_2015,
	title = {Training {Deep} {Neural} {Networks} on {Noisy} {Labels} with {Bootstrapping}},
	url = {http://arxiv.org/abs/1412.6596},
	doi = {10.48550/arXiv.1412.6596},
	abstract = {Current state-of-the-art deep learning systems for visual object recognition and detection use purely supervised training with regularization such as dropout to avoid overfitting. The performance depends critically on the amount of labeled examples, and in current practice the labels are assumed to be unambiguous and accurate. However, this assumption often does not hold; e.g. in recognition, class labels may be missing; in detection, objects in the image may not be localized; and in general, the labeling may be subjective. In this work we propose a generic way to handle noisy and incomplete labeling by augmenting the prediction objective with a notion of consistency. We consider a prediction consistent if the same prediction is made given similar percepts, where the notion of similarity is between deep network features computed from the input data. In experiments we demonstrate that our approach yields substantial robustness to label noise on several datasets. On MNIST handwritten digits, we show that our model is robust to label corruption. On the Toronto Face Database, we show that our model handles well the case of subjective labels in emotion recognition, achieving state-of-the- art results, and can also benefit from unlabeled face images with no modification to our method. On the ILSVRC2014 detection challenge data, we show that our approach extends to very deep networks, high resolution images and structured outputs, and results in improved scalable detection.},
	urldate = {2022-10-25},
	publisher = {arXiv},
	author = {Reed, Scott and Lee, Honglak and Anguelov, Dragomir and Szegedy, Christian and Erhan, Dumitru and Rabinovich, Andrew},
	month = apr,
	year = {2015},
	note = {735 citations (Semantic Scholar/arXiv) [2022-10-25]
arXiv:1412.6596 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing},
}

@article{fang_online_nodate,
	title = {Online {Bootstrap} {Conﬁdence} {Intervals} for the {Stochastic} {Gradient} {Descent} {Estimator}},
	abstract = {In many applications involving large dataset or online learning, stochastic gradient descent (SGD) is a scalable algorithm to compute parameter estimates and has gained increasing popularity due to its numerical convenience and memory eﬃciency. While the asymptotic properties of SGD-based estimators have been well established, statistical inference such as interval estimation remains much unexplored. The classical bootstrap is not directly applicable if the data are not stored in memory. The plug-in method is not applicable when there is no explicit formula for the covariance matrix of the estimator. In this paper, we propose an online bootstrap procedure for the estimation of conﬁdence intervals, which, upon the arrival of each observation, updates the SGD estimate as well as a number of randomly perturbed SGD estimates. The proposed method is easy to implement in practice. We establish its theoretical properties for a general class of models that includes linear regressions, generalized linear models, M-estimators and quantile regressions as special cases. The ﬁnite-sample performance and numerical utility is evaluated by simulation studies and real data applications.},
	language = {en},
	author = {Fang, Yixin and Xu, Jinfeng and Yang, Lei},
	pages = {21},
}

@misc{salimans_evolution_2017,
	title = {Evolution {Strategies} as a {Scalable} {Alternative} to {Reinforcement} {Learning}},
	url = {http://arxiv.org/abs/1703.03864},
	doi = {10.48550/arXiv.1703.03864},
	abstract = {We explore the use of Evolution Strategies (ES), a class of black box optimization algorithms, as an alternative to popular MDP-based RL techniques such as Q-learning and Policy Gradients. Experiments on MuJoCo and Atari show that ES is a viable solution strategy that scales extremely well with the number of CPUs available: By using a novel communication strategy based on common random numbers, our ES implementation only needs to communicate scalars, making it possible to scale to over a thousand parallel workers. This allows us to solve 3D humanoid walking in 10 minutes and obtain competitive results on most Atari games after one hour of training. In addition, we highlight several advantages of ES as a black box optimization technique: it is invariant to action frequency and delayed rewards, tolerant of extremely long horizons, and does not need temporal discounting or value function approximation.},
	urldate = {2022-10-21},
	publisher = {arXiv},
	author = {Salimans, Tim and Ho, Jonathan and Chen, Xi and Sidor, Szymon and Sutskever, Ilya},
	month = sep,
	year = {2017},
	note = {1082 citations (Semantic Scholar/arXiv) [2022-10-25]
arXiv:1703.03864 [cs, stat]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Statistics - Machine Learning},
}

@misc{whitelam_training_2022,
	title = {Training neural networks using {Metropolis} {Monte} {Carlo} and an adaptive variant},
	url = {http://arxiv.org/abs/2205.07408},
	abstract = {We examine the zero-temperature Metropolis Monte Carlo algorithm as a tool for training a neural network by minimizing a loss function. We find that, as expected on theoretical grounds and shown empirically by other authors, Metropolis Monte Carlo can train a neural net with an accuracy comparable to that of gradient descent, if not necessarily as quickly. The Metropolis algorithm does not fail automatically when the number of parameters of a neural network is large. It can fail when a neural network's structure or neuron activations are strongly heterogenous, and we introduce an adaptive Monte Carlo algorithm, aMC, to overcome these limitations. The intrinsic stochasticity and numerical stability of the Monte Carlo method allow aMC to train deep neural networks and recurrent neural networks in which the gradient is too small or too large to allow training by gradient descent. Monte Carlo methods offer a complement to gradient-based methods for training neural networks, allowing access to a distinct set of network architectures and principles.},
	urldate = {2022-10-21},
	publisher = {arXiv},
	author = {Whitelam, Stephen and Selin, Viktor and Benlolo, Ian and Casert, Corneel and Tamblyn, Isaac},
	month = aug,
	year = {2022},
	note = {0 citations (Semantic Scholar/arXiv) [2022-10-25]
arXiv:2205.07408 [cond-mat]},
	keywords = {Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Condensed Matter - Statistical Mechanics},
}

@article{shen_self-learning_2018,
	title = {Self-learning {Monte} {Carlo} with deep neural networks},
	volume = {97},
	issn = {2469-9950, 2469-9969},
	url = {https://link.aps.org/doi/10.1103/PhysRevB.97.205140},
	doi = {10.1103/PhysRevB.97.205140},
	language = {en},
	number = {20},
	urldate = {2022-10-21},
	journal = {Physical Review B},
	author = {Shen, Huitao and Liu, Junwei and Fu, Liang},
	month = may,
	year = {2018},
	note = {55 citations (Semantic Scholar/DOI) [2022-10-25]},
	pages = {205140},
}

@article{hills_modern_2019,
	title = {Modern microprocessor built from complementary carbon nanotube transistors},
	volume = {572},
	copyright = {2019 The Author(s), under exclusive licence to Springer Nature Limited},
	issn = {1476-4687},
	url = {https://www.nature.com/articles/s41586-019-1493-8},
	doi = {10.1038/s41586-019-1493-8},
	abstract = {Electronics is approaching a major paradigm shift because silicon transistor scaling no longer yields historical energy-efficiency benefits, spurring research towards beyond-silicon nanotechnologies. In particular, carbon nanotube field-effect transistor (CNFET)-based digital circuits promise substantial energy-efficiency benefits, but the inability to perfectly control intrinsic nanoscale defects and variability in carbon nanotubes has precluded the realization of very-large-scale integrated systems. Here we overcome these challenges to demonstrate a beyond-silicon microprocessor built entirely from CNFETs. This 16-bit microprocessor is based on the RISC-V instruction set, runs standard 32-bit instructions on 16-bit data and addresses, comprises more than 14,000 complementary metal–oxide–semiconductor CNFETs and is designed and fabricated using industry-standard design flows and processes. We propose a manufacturing methodology for carbon nanotubes, a set of combined processing and design techniques for overcoming nanoscale imperfections at macroscopic scales across full wafer substrates. This work experimentally validates a promising path towards practical beyond-silicon electronic systems.},
	language = {en},
	number = {7771},
	urldate = {2022-10-05},
	journal = {Nature},
	author = {Hills, Gage and Lau, Christian and Wright, Andrew and Fuller, Samuel and Bishop, Mindy D. and Srimani, Tathagata and Kanhaiya, Pritpal and Ho, Rebecca and Amer, Aya and Stein, Yosi and Murphy, Denis and Arvind and Chandrakasan, Anantha and Shulaker, Max M.},
	month = aug,
	year = {2019},
	note = {286 citations (Semantic Scholar/DOI) [2022-10-05]
Number: 7771
Publisher: Nature Publishing Group},
	keywords = {Carbon nanotubes and fullerenes, Electrical and electronic engineering, Electronic devices},
	pages = {595--602},
}

@misc{hill_stochastic_2021,
	title = {Stochastic {Security}: {Adversarial} {Defense} {Using} {Long}-{Run} {Dynamics} of {Energy}-{Based} {Models}},
	shorttitle = {Stochastic {Security}},
	url = {http://arxiv.org/abs/2005.13525},
	abstract = {The vulnerability of deep networks to adversarial attacks is a central problem for deep learning from the perspective of both cognition and security. The current most successful defense method is to train a classifier using adversarial images created during learning. Another defense approach involves transformation or purification of the original input to remove adversarial signals before the image is classified. We focus on defending naturally-trained classifiers using Markov Chain Monte Carlo (MCMC) sampling with an Energy-Based Model (EBM) for adversarial purification. In contrast to adversarial training, our approach is intended to secure pre-existing and highly vulnerable classifiers. The memoryless behavior of long-run MCMC sampling will eventually remove adversarial signals, while metastable behavior preserves consistent appearance of MCMC samples after many steps to allow accurate long-run prediction. Balancing these factors can lead to effective purification and robust classification. We evaluate adversarial defense with an EBM using the strongest known attacks against purification. Our contributions are 1) an improved method for training EBM's with realistic long-run MCMC samples, 2) an Expectation-Over-Transformation (EOT) defense that resolves theoretical ambiguities for stochastic defenses and from which the EOT attack naturally follows, and 3) state-of-the-art adversarial defense for naturally-trained classifiers and competitive defense compared to adversarially-trained classifiers on Cifar-10, SVHN, and Cifar-100. Code and pre-trained models are available at https://github.com/point0bar1/ebm-defense.},
	urldate = {2022-10-05},
	publisher = {arXiv},
	author = {Hill, Mitch and Mitchell, Jonathan and Zhu, Song-Chun},
	month = mar,
	year = {2021},
	note = {17 citations (Semantic Scholar/arXiv) [2022-10-05]
arXiv:2005.13525 [cs, stat]},
	keywords = {Computer Science - Cryptography and Security, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@misc{kuzina_defending_2022,
	title = {Defending {Variational} {Autoencoders} from {Adversarial} {Attacks} with {MCMC}},
	url = {http://arxiv.org/abs/2203.09940},
	abstract = {Variational autoencoders (VAEs) are deep generative models used in various domains. VAEs can generate complex objects and provide meaningful latent representations, which can be further used in downstream tasks such as classification. As previous work has shown, one can easily fool VAEs to produce unexpected latent representations and reconstructions for a visually slightly modified input. Here, we examine several objective functions for adversarial attacks construction, suggest metrics assess the model robustness, and propose a solution to alleviate the effect of an attack. Our method utilizes the Markov Chain Monte Carlo (MCMC) technique in the inference step and is motivated by our theoretical analysis. Thus, we do not incorporate any additional costs during training or we do not decrease the performance on non-attacked inputs. We validate our approach on a variety of datasets (MNIST, Fashion MNIST, Color MNIST, CelebA) and VAE configurations (\${\textbackslash}beta\$-VAE, NVAE, TC-VAE) and show that it consistently improves the model robustness to adversarial attacks.},
	urldate = {2022-10-05},
	publisher = {arXiv},
	author = {Kuzina, Anna and Welling, Max and Tomczak, Jakub M.},
	month = mar,
	year = {2022},
	note = {0 citations (Semantic Scholar/arXiv) [2022-10-05]
arXiv:2203.09940 [cs]},
	keywords = {Computer Science - Machine Learning},
}

@article{jastrzebski_three_2022,
	title = {Three factors influencing minima in {SGD}},
	url = {https://openreview.net/forum?id=rJma2bZCW},
	abstract = {Three factors (batch size, learning rate, gradient noise) change in predictable way the properties (e.g. sharpness) of minima found by SGD.},
	language = {en},
	urldate = {2022-09-30},
	author = {Jastrzębski, Stanisław and Kenton, Zac and Arpit, Devansh and Ballas, Nicolas and Fischer, Asja and Storkey, Amos and Bengio, Yoshua},
	month = feb,
	year = {2022},
}

@misc{ilyas_black-box_2018,
	title = {Black-box {Adversarial} {Attacks} with {Limited} {Queries} and {Information}},
	url = {http://arxiv.org/abs/1804.08598},
	abstract = {Current neural network-based classifiers are susceptible to adversarial examples even in the black-box setting, where the attacker only has query access to the model. In practice, the threat model for real-world systems is often more restrictive than the typical black-box model where the adversary can observe the full output of the network on arbitrarily many chosen inputs. We define three realistic threat models that more accurately characterize many real-world classifiers: the query-limited setting, the partial-information setting, and the label-only setting. We develop new attacks that fool classifiers under these more restrictive threat models, where previous methods would be impractical or ineffective. We demonstrate that our methods are effective against an ImageNet classifier under our proposed threat models. We also demonstrate a targeted black-box attack against a commercial classifier, overcoming the challenges of limited query access, partial information, and other practical issues to break the Google Cloud Vision API.},
	urldate = {2022-09-30},
	publisher = {arXiv},
	author = {Ilyas, Andrew and Engstrom, Logan and Athalye, Anish and Lin, Jessy},
	month = jul,
	year = {2018},
	note = {685 citations (Semantic Scholar/arXiv) [2022-09-30]
arXiv:1804.08598 [cs, stat]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Cryptography and Security, Statistics - Machine Learning},
}

@article{zhang_overview_2018,
	title = {An overview on probability undirected graphs and their applications in image processing},
	volume = {321},
	issn = {0925-2312},
	url = {https://www.sciencedirect.com/science/article/pii/S0925231218310580},
	doi = {10.1016/j.neucom.2018.07.078},
	abstract = {This review aims to report recent developments about deep learning algorithms based on Restricted Boltzmann Machines (RBMs) and Conditional Random Fields (CRFs). Firstly, we give an overview of the general RBMs and CRFs, which are powerful methods for representing dependency of input data, and they can be treated as the basic blocks of deep neural nets as well. Secondly, this review introduces RBM variants and the deep learning models. Apart from the Deep Belief Networks (DBNs) and the Deep Boltzmann Machines (DBMs), the RBMs can be combined with the Convolutional Neural Nets (CNNs), which perform well in image recognition and image reconstruction. Thirdly, this review discusses CRFs and their applications in image annotation and scene recognition. Lastly, this review describes the developments and existing problems in neural nets and lists some experiments.},
	language = {en},
	urldate = {2022-09-28},
	journal = {Neurocomputing},
	author = {Zhang, Jian and Ding, Shifei and Zhang, Nan},
	month = dec,
	year = {2018},
	keywords = {Classification, Conditional random field, Deep neural nets, Restricted Boltzmann machine},
	pages = {156--168},
}

@inproceedings{wang_algorithm_2021,
	title = {Algorithm and {Hardware} {Co}-{Design} for {FPGA} {Acceleration} of {Hamiltonian} {Monte} {Carlo} {Based} {No}-{U}-{Turn} {Sampler}},
	doi = {10.1109/ASAP52443.2021.00009},
	abstract = {Monte Carlo (MC) methods are widely used in many research areas such as physical simulation, statistical analysis, and machine learning. Application of MC methods requires drawing fast mixing samples from a given probability distribution. Among existing sampling methods, the Hamiltonian Monte Carlo (HMC) utilizes gradient information during Hamiltonian simulation and can produce fast mixing samples at the highest efficiency. However, without carefully chosen simulation parameters for a specific problem, HMC generally suffers from simulation locality and computation waste. As a result, the No-U-Turn Sampler (NUTS) has been proposed to automatically tune these parameters during simulation and is the current state-of-the-art sampling algorithm. However, application of NUTS requires frequent gradient calculation of a given distribution and high-volume vector processing, especially for large-scale problems, leading to drawing an expensively large number of samples and a desire of hardware acceleration. While some hardware acceleration works have been proposed for traditional Markov Chain Monte Carlo (MCMC) and HMC methods, there is no existing work targeting hardware acceleration of the NUTS algorithm. In this paper, we present the first NUTS accelerator on FPGA while addressing the high complexity of this state-of-the-art algorithm. Our hardware and algorithm co-optimizations include an incremental resampling technique which leads to a more memory efficient architecture and pipeline optimization for multi-chain sampling to maximize the throughput. We also explore three levels of parallelism in the NUTS accelerator to further boost performance. Compared with optimized C++ NUTS package: RSTAN, our NUTS accelerator can reach a maximum speedup of 50.6X and an energy improvement of 189.7X.},
	booktitle = {2021 {IEEE} 32nd {International} {Conference} on {Application}-specific {Systems}, {Architectures} and {Processors} ({ASAP})},
	author = {Wang, Yu and Li, Peng},
	month = jul,
	year = {2021},
	note = {1 citations (Semantic Scholar/DOI) [2022-09-28]
ISSN: 2160-052X},
	keywords = {C++ languages, Computational modeling, FPGA, Hamiltonian Monte Carlo, Hardware Acceleration, Memory management, Monte Carlo methods, No-U-Turn Sampler, Parallel processing, Pipelines, Systems architecture},
	pages = {9--16},
}

@article{civitcioglu_machine_nodate,
	title = {Machine {Learning} {Methods} on {2D} {Ising} {Model}},
	abstract = {This report explores the study of Convolutional Neural Networks (CNN) prediction success on recognising the temperature by having the image of the conﬁguration as an input for the Ising Model in 2D. Several studies have been done on transfer learning in various physical problems with promising results[14]. In the Ising Model case, we train a CNN on a square lattice and test it on a triangular lattice and vice versa; in order to conclude if the transfer learning methods work for the Ising model. The conﬁgurations are generated using Monte-Carlo Methods and Metropolis Algorithm and then Neural Networks are trained on square lattices of size 100 × 100 and the trained networks are tested on triangular lattices of the same size. Understanding the transfer learning for Ising model can help us discover if the transfer learning can be applied to any problem or not, moreover even if the transfer learning does not work, we can explore more on what the behaviour is when we try to do a transfer learning.},
	language = {en},
	author = {Civitcioglu, Burak and Universite, CY},
	pages = {45},
}

@inproceedings{kakade_generalization_2008,
	title = {On the {Generalization} {Ability} of {Online} {Strongly} {Convex} {Programming} {Algorithms}},
	volume = {21},
	url = {https://papers.nips.cc/paper/2008/hash/f90f2aca5c640289d0a29417bcb63a37-Abstract.html},
	abstract = {This paper examines the generalization properties of online convex programming algorithms when the loss function is Lipschitz and strongly convex. Our main result is a sharp bound, that holds with high probability, on the excess risk of the output of an online algorithm in terms of the average regret. This allows one to use recent algorithms with logarithmic cumulative regret guarantees to achieve fast convergence rates for the excess risk with high probability. The bound also solves an open problem regarding the convergence rate of \{{\textbackslash}pegasos\}, a recently proposed method for solving the SVM optimization problem.},
	urldate = {2022-09-17},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Kakade, Sham M and Tewari, Ambuj},
	year = {2008},
}

@article{kawaguchi_ordered_nodate,
	title = {Ordered {SGD}: {A} {New} {Stochastic} {Optimization} {Framework} for {Empirical} {Risk} {Minimization}},
	abstract = {We propose a new stochastic optimization framework for empirical risk minimization problems such as those that arise in machine learning. The traditional approaches, such as (mini-batch) stochastic gradient descent (SGD), utilize an unbiased gradient estimator of the empirical average loss. In contrast, we develop a computationally efﬁcient method to construct a gradient estimator that is purposely biased toward those observations with higher current losses. On the theory side, we show that the proposed method minimizes a new ordered modiﬁcation of the empirical average loss, and is guaranteed to converge at a sublinear rate to a global optimum for convex loss and to a critical point for weakly convex (non-convex) loss. Furthermore, we prove a new generalization bound for the proposed algorithm. On the empirical side, the numerical experiments show that our proposed method consistently improves the test errors compared with the standard mini-batch SGD in various models including SVM, logistic regression, and deep learning problems.},
	language = {en},
	author = {Kawaguchi, Kenji and Lu, Haihao},
	pages = {10},
}

@article{katharopoulos_not_nodate,
	title = {Not {All} {Samples} {Are} {Created} {Equal}:  {Deep} {Learning} with {Importance} {Sampling}},
	abstract = {Deep neural network training spends most of the computation on examples that are properly handled, and could be ignored. We propose to mitigate this phenomenon with a principled importance sampling scheme that focuses computation on “informative” examples, and reduces the variance of the stochastic gradients during training. Our contribution is twofold: ﬁrst, we derive a tractable upper bound to the per-sample gradient norm, and second we derive an estimator of the variance reduction achieved with importance sampling, which enables us to switch it on when it will result in an actual speedup. The resulting scheme can be used by changing a few lines of code in a standard SGD procedure, and we demonstrate experimentally, on image classiﬁcation, CNN ﬁne-tuning, and RNN training, that for a ﬁxed wall-clock time budget, it provides a reduction of the train losses of up to an order of magnitude and a relative improvement of test errors between 5\% and 17\%.},
	language = {en},
	author = {Katharopoulos, Angelos and Fleuret, François},
	pages = {10},
}

@inproceedings{kawaguchi_ordered_2020,
	title = {Ordered {SGD}: {A} {New} {Stochastic} {Optimization} {Framework} for {Empirical} {Risk} {Minimization}},
	shorttitle = {Ordered {SGD}},
	url = {https://proceedings.mlr.press/v108/kawaguchi20a.html},
	abstract = {We propose a new stochastic optimization framework for empirical risk minimization problems such as those that arise in machine learning. The traditional approaches, such as (mini-batch) stochastic gradient descent (SGD), utilize an unbiased gradient estimator of the empirical average loss. In contrast, we develop a computationally efficient method to construct a gradient estimator that is purposely biased toward those observations with higher current losses. On the theory side, we show that the proposed method minimizes a new ordered modification of the empirical average loss, and is guaranteed to converge at a sublinear rate to a global optimum for convex loss and to a critical point for weakly convex (non-convex) loss. Furthermore, we prove a new generalization bound for the proposed algorithm. On the empirical side, the numerical experiments show that our proposed method consistently improves the test errors compared with the standard mini-batch SGD in various models including SVM, logistic regression, and deep learning problems.},
	language = {en},
	urldate = {2022-09-11},
	booktitle = {Proceedings of the {Twenty} {Third} {International} {Conference} on {Artificial} {Intelligence} and {Statistics}},
	publisher = {PMLR},
	author = {Kawaguchi, Kenji and Lu, Haihao},
	month = jun,
	year = {2020},
	note = {ISSN: 2640-3498},
	pages = {669--679},
}

@inproceedings{oneil_rethinking_2015,
	address = {San Francisco CA USA},
	title = {Rethinking the parallelization of random-restart hill climbing: a case study in optimizing a 2-opt {TSP} solver for {GPU} execution},
	isbn = {978-1-4503-3407-5},
	shorttitle = {Rethinking the parallelization of random-restart hill climbing},
	url = {https://dl.acm.org/doi/10.1145/2716282.2716287},
	doi = {10.1145/2716282.2716287},
	abstract = {Random-restart hill climbing is a common approach to combinatorial optimization problems such as the traveling salesman problem (TSP). We present and evaluate an implementation of random-restart hill climbing with 2-opt local search applied to TSP. Our implementation is capable of addressing large problem sizes at high throughput. It is based on the key insight that the GPU’s hierarchical hardware parallelism is best exploited with a hierarchical implementation strategy, where independent climbs are parallelized between blocks and the 2-opt evaluations are parallelized across the threads within a block. We analyze the performance impact of this and other optimizations on our heuristic TSP solver and compare its performance to existing GPU-based 2-opt TSP solvers as well as a parallel CPU implementation. Our code outperforms the existing implementations by up to 3X, evaluating up to 60 billion 2-opt moves per second on a single K40 GPU. It also outperforms an OpenMP implementation run on 20 CPU cores by up to 8X.},
	language = {en},
	urldate = {2022-08-31},
	booktitle = {Proceedings of the 8th {Workshop} on {General} {Purpose} {Processing} using {GPUs}},
	publisher = {ACM},
	author = {O'Neil, Molly A. and Burtscher, Martin},
	month = feb,
	year = {2015},
	note = {17 citations (Semantic Scholar/DOI) [2022-08-31]},
	pages = {99--108},
}

@article{oneil_parallel_nodate,
	title = {A {Parallel} {GPU} {Version} of the {Traveling} {Salesman} {Problem}},
	abstract = {This paper describes and evaluates an implementation of iterative hill climbing with random restart for determining high-quality solutions to the traveling salesman problem. With 100,000 restarts, this algorithm finds the optimal solution for four out of five 100-city TSPLIB inputs and yields a tour that is only 0.07\% longer than the optimum on the fifth input. The presented implementation is highly parallel and optimized for GPU-based execution. Running on a single GPU, it evaluates over 20 billion tour modifications per second. It takes 32 CPUs with 8 cores each (256 cores total) to match this performance.},
	language = {en},
	author = {O’Neil, Molly A and Tamir, Dan and Burtscher, Martin},
	pages = {6},
}

@article{lee_opencl_nodate,
	title = {{OpenCL} on {FPGAs} for {GPU} {Programmers}},
	language = {en},
	author = {Lee, Gilbert},
	pages = {10},
}

@article{cadenelli_considerations_2019,
	title = {Considerations in using {OpenCL} on {GPUs} and {FPGAs} for throughput-oriented genomics workloads},
	volume = {94},
	issn = {0167739X},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0167739X18314183},
	doi = {10.1016/j.future.2018.11.028},
	abstract = {The recent upsurge in the available amount of health data and the advances in next-generation sequencing are setting the ground for the long-awaited precision medicine. To process this deluge of data, bioinformatics workloads are becoming more complex and more computationally demanding. For this reasons they have been extended to support different computing architectures, such as GPUs and FPGAs, to leverage the form of parallelism typical of each of such architectures.},
	language = {en},
	urldate = {2022-08-14},
	journal = {Future Generation Computer Systems},
	author = {Cadenelli, Nicola and Jaks̆ić, Zoran and Polo, Jordà and Carrera, David},
	month = may,
	year = {2019},
	note = {18 citations (Semantic Scholar/DOI) [2022-08-14]},
	pages = {148--159},
}

@article{rebentrost_quantum_2018,
	title = {Quantum computational finance: {Monte} {Carlo} pricing of financial derivatives},
	volume = {98},
	issn = {2469-9926, 2469-9934},
	shorttitle = {Quantum computational finance},
	url = {https://link.aps.org/doi/10.1103/PhysRevA.98.022321},
	doi = {10.1103/PhysRevA.98.022321},
	language = {en},
	number = {2},
	urldate = {2022-08-03},
	journal = {Physical Review A},
	author = {Rebentrost, Patrick and Gupt, Brajesh and Bromley, Thomas R.},
	month = aug,
	year = {2018},
	note = {106 citations (Semantic Scholar/DOI) [2022-08-03]},
	pages = {022321},
}

@article{grinko_iterative_2021,
	title = {Iterative quantum amplitude estimation},
	volume = {7},
	copyright = {2021 The Author(s)},
	issn = {2056-6387},
	url = {https://www.nature.com/articles/s41534-021-00379-1},
	doi = {10.1038/s41534-021-00379-1},
	abstract = {We introduce a variant of Quantum Amplitude Estimation (QAE), called Iterative QAE (IQAE), which does not rely on Quantum Phase Estimation (QPE) but is only based on Grover’s Algorithm, which reduces the required number of qubits and gates. We provide a rigorous analysis of IQAE and prove that it achieves a quadratic speedup up to a double-logarithmic factor compared to classical Monte Carlo simulation with provably small constant overhead. Furthermore, we show with an empirical study that our algorithm outperforms other known QAE variants without QPE, some even by orders of magnitude, i.e., our algorithm requires significantly fewer samples to achieve the same estimation accuracy and confidence level.},
	language = {en},
	number = {1},
	urldate = {2022-08-03},
	journal = {npj Quantum Information},
	author = {Grinko, Dmitry and Gacon, Julien and Zoufal, Christa and Woerner, Stefan},
	month = mar,
	year = {2021},
	note = {68 citations (Semantic Scholar/DOI) [2022-08-03]
Number: 1
Publisher: Nature Publishing Group},
	keywords = {Computational science, Quantum information},
	pages = {1--6},
}

@article{camsari_p-bits_2019,
	title = {p-bits for probabilistic spin logic},
	volume = {6},
	issn = {1931-9401},
	url = {http://aip.scitation.org/doi/10.1063/1.5055860},
	doi = {10.1063/1.5055860},
	abstract = {We introduce the concept of a probabilistic or p-bit, intermediate between the standard bits of digital electronics and the emerging q-bits of quantum computing. We show that low barrier magnets or LBMs provide a natural physical representation for p-bits and can be built either from perpendicular magnets designed to be close to the in-plane transition or from circular in-plane magnets. Magnetic tunnel junctions (MTJs) built using LBMs as free layers can be combined with standard NMOS transistors to provide three-terminal building blocks for large scale probabilistic circuits that can be designed to perform useful functions. Interestingly, this three-terminal unit looks just like the 1T/MTJ device used in embedded magnetic random access memory technology, with only one difference: the use of an LBM for the MTJ free layer. We hope that the concept of p-bits and p-circuits will help open up new application spaces for this emerging technology. However, a p-bit need not involve an MTJ; any ﬂuctuating resistor could be combined with a transistor to implement it, while completely digital implementations using conventional CMOS technology are also possible. The p-bit also provides a conceptual bridge between two active but disjoint ﬁelds of research, namely, stochastic machine learning and quantum computing. First, there are the applications that are based on the similarity of a p-bit to the binary stochastic neuron (BSN), a well-known concept in machine learning. Three-terminal p-bits could provide an efﬁcient hardware accelerator for the BSN. Second, there are the applications that are based on the p-bit being like a poor man’s q-bit. Initial demonstrations based on full SPICE simulations show that several optimization problems, including quantum annealing are amenable to p-bit implementations which can be scaled up at room temperature using existing technology.},
	language = {en},
	number = {1},
	urldate = {2022-08-02},
	journal = {Applied Physics Reviews},
	author = {Camsari, Kerem Y. and Sutton, Brian M. and Datta, Supriyo},
	month = mar,
	year = {2019},
	note = {55 citations (Semantic Scholar/DOI) [2022-08-02]},
	pages = {011305},
}

@article{camsari_stochastic_2017,
	title = {Stochastic p -{Bits} for {Invertible} {Logic}},
	volume = {7},
	issn = {2160-3308},
	url = {http://link.aps.org/doi/10.1103/PhysRevX.7.031014},
	doi = {10.1103/PhysRevX.7.031014},
	language = {en},
	number = {3},
	urldate = {2022-08-02},
	journal = {Physical Review X},
	author = {Camsari, Kerem Yunus and Faria, Rafatul and Sutton, Brian M. and Datta, Supriyo},
	month = jul,
	year = {2017},
	note = {156 citations (Semantic Scholar/DOI) [2022-08-02]},
	pages = {031014},
}

@article{loeffler_teaching_2019,
	title = {Teaching an {Old} {Dog} {New} {Tricks}: {Machine} {Learning} an {Improved} {TIP3P} {Potential} {Model} for {Liquid}–{Vapor} {Phase} {Phenomena}},
	volume = {123},
	issn = {1932-7447, 1932-7455},
	shorttitle = {Teaching an {Old} {Dog} {New} {Tricks}},
	url = {https://pubs.acs.org/doi/10.1021/acs.jpcc.9b06348},
	doi = {10.1021/acs.jpcc.9b06348},
	abstract = {Water is ubiquitous yet displays a rich variety of thermodynamic properties and anomalies. An understanding of liquid−vapor phenomena in water is of broad importance to everyday processes such as evaporation, condensation, and cavitation, as well as energy technologies such as steam turbines. An accurate description of the vapor−liquid phenomena is quite challenging owing to the signiﬁcant diﬀerences between how water behaves in small, sparsely distributed clusters and how it behaves in a dense bulk liquid. It is not surprising that there exist a myriad of diﬀerent water models, which have attempted to describe water behavior with varying degrees of success. In general, water models have evolved from simple three-point transferable interatomic potentials (TIP3P) to more complex four-point and ﬁve-point TIP models to more recent polarizable models. The natural evolution from TIP3P to TIP4P families of models was, in part, due to the belief that we have perhaps reached the limit of what the simple three-point models are capable of achieving. The advent of big data analytics and everincreasing supercomputing resources has brought to the forefront powerful machine learning techniques for materials design. Here, we take advantage of machine learning techniques such as hierarchical objective genetic algorithms to demonstrate that simple computationally eﬃcient models developed decades ago can be retrained to perform signiﬁcantly better than their original counterparts. In a departure from typical practice, we train our model against an elaborate temperature-dependent data obtained from molecular dynamics trajectories to cluster properties using extensive conﬁgurational sampling and on-the-ﬂy Monte Carlo simulations. To demonstrate the power of our machine learning approach, we choose the popular TIP3P model that, however, is widely acknowledged to perform poorly in describing vapor−liquid properties. We retrain this TIP3P model to dramatically improve its performance over the original model. Our new ML-TIP3P performs on par or, in some respects, better than even the current best performing nonpolarizable model (TIP4P/2005) for vapor−liquid properties. To exemplify the suitability of our approach, we apply our newly developed model to study a highly nonequilibrium vapor−liquid phenomenon, laser-induced heterogeneous cavitation in a gold−water system. Overall, our study highlights a general strategy for atomistic model development that can be potentially used to retrain existing potential models and help them attain their best possible performance.},
	language = {en},
	number = {36},
	urldate = {2022-07-08},
	journal = {The Journal of Physical Chemistry C},
	author = {Loeffler, Troy D. and Chan, Henry and Sasikumar, Kiran and Narayanan, Badri and Cherukara, Mathew J. and Gray, Stephen and Sankaranarayanan, Subramanian K. R. S.},
	month = sep,
	year = {2019},
	note = {5 citations (Semantic Scholar/DOI) [2022-07-08]},
	pages = {22643--22655},
}

@article{alford_rosetta_2017,
	title = {The {Rosetta} {All}-{Atom} {Energy} {Function} for {Macromolecular} {Modeling} and {Design}},
	volume = {13},
	issn = {1549-9618, 1549-9626},
	url = {https://pubs.acs.org/doi/10.1021/acs.jctc.7b00125},
	doi = {10.1021/acs.jctc.7b00125},
	abstract = {Over the past decade, the Rosetta biomolecular modeling suite has informed diverse biological questions and engineering challenges ranging from interpretation of low-resolution structural data to design of nanomaterials, protein therapeutics, and vaccines. Central to Rosetta’s success is the energy function: a model parametrized from small-molecule and X-ray crystal structure data used to approximate the energy associated with each biomolecule conformation. This paper describes the mathematical models and physical concepts that underlie the latest Rosetta energy function, called the Rosetta Energy Function 2015 (REF15). Applying these concepts, we explain how to use Rosetta energies to identify and analyze the features of biomolecular models. Finally, we discuss the latest advances in the energy function that extend its capabilities from soluble proteins to also include membrane proteins, peptides containing noncanonical amino acids, small molecules, carbohydrates, nucleic acids, and other macromolecules.},
	language = {en},
	number = {6},
	urldate = {2022-07-08},
	journal = {Journal of Chemical Theory and Computation},
	author = {Alford, Rebecca F. and Leaver-Fay, Andrew and Jeliazkov, Jeliazko R. and O’Meara, Matthew J. and DiMaio, Frank P. and Park, Hahnbeom and Shapovalov, Maxim V. and Renfrew, P. Douglas and Mulligan, Vikram K. and Kappel, Kalli and Labonte, Jason W. and Pacella, Michael S. and Bonneau, Richard and Bradley, Philip and Dunbrack, Roland L. and Das, Rhiju and Baker, David and Kuhlman, Brian and Kortemme, Tanja and Gray, Jeffrey J.},
	month = jun,
	year = {2017},
	pages = {3031--3048},
}

@article{kaiser_subnanosecond_2019,
	title = {Subnanosecond {Fluctuations} in {Low}-{Barrier} {Nanomagnets}},
	volume = {12},
	issn = {2331-7019},
	url = {https://link.aps.org/doi/10.1103/PhysRevApplied.12.054056},
	doi = {10.1103/PhysRevApplied.12.054056},
	language = {en},
	number = {5},
	urldate = {2022-07-08},
	journal = {Physical Review Applied},
	author = {Kaiser, J. and Rustagi, A. and Camsari, K. Y. and Sun, J. Z. and Datta, S. and Upadhyaya, P.},
	month = nov,
	year = {2019},
	note = {15 citations (Semantic Scholar/DOI) [2022-07-08]},
	pages = {054056},
}

@article{romano_t_kroemer_structure-based_2007,
	title = {Structure-{Based} {Drug} {Design}: {Docking} and {Scoring}},
	volume = {8},
	issn = {13892037},
	shorttitle = {Structure-{Based} {Drug} {Design}},
	url = {http://www.eurekaselect.com/openurl/content.php?genre=article&issn=1389-2037&volume=8&issue=4&spage=312},
	doi = {10.2174/138920307781369382},
	abstract = {This review gives an introduction into ligand – receptor docking and illustrates the basic underlying concepts. An overview of different approaches and algorithms is provided. Although the application of docking and scoring has led to some remarkable successes, there are still some major challenges ahead, which are outlined here as well. Approaches to address some of these challenges and the latest developments in the area are presented. Some aspects of the assessment of docking program performance are discussed. A number of successful applications of structure-based virtual screening are described.},
	language = {en},
	number = {4},
	urldate = {2022-07-08},
	journal = {Current Protein \& Peptide Science},
	author = {{Romano T. Kroemer}},
	month = aug,
	year = {2007},
	note = {287 citations (Semantic Scholar/DOI) [2022-07-08]},
	pages = {312--328},
}

@inproceedings{abubeker_serial_2013,
	address = {Kanjirapally, India},
	title = {Serial and parallel implementation of {CORDIC} architecture: {A} comparative approach},
	isbn = {978-1-4673-5149-2 978-1-4673-5150-8 978-1-4673-5148-5},
	shorttitle = {Serial and parallel implementation of {CORDIC} architecture},
	url = {http://ieeexplore.ieee.org/document/6575931/},
	doi = {10.1109/AICERA-ICMiCR.2013.6575931},
	language = {en},
	urldate = {2022-07-08},
	booktitle = {2013 {Annual} {International} {Conference} on {Emerging} {Research} {Areas} and 2013 {International} {Conference} on {Microelectronics}, {Communications} and {Renewable} {Energy}},
	publisher = {IEEE},
	author = {Abubeker, K. M. and Backer, Sabana and Varghese, Abey Mathew},
	month = jun,
	year = {2013},
	note = {2 citations (Semantic Scholar/DOI) [2022-07-08]},
	pages = {1--6},
}

@article{heilmann_sampling_2020,
	title = {Sampling of the conformational landscape of small proteins with {Monte} {Carlo} methods},
	volume = {10},
	issn = {2045-2322},
	url = {https://www.nature.com/articles/s41598-020-75239-7},
	doi = {10.1038/s41598-020-75239-7},
	abstract = {Abstract
            Computer simulation provides an increasingly realistic picture of large-scale conformational change of proteins, but investigations remain fundamentally constrained by the femtosecond timestep of molecular dynamics simulations. For this reason, many biologically interesting questions cannot be addressed using accessible state-of-the-art computational resources. Here, we report the development of an all-atom Monte Carlo approach that permits the modelling of the large-scale conformational change of proteins using standard off-the-shelf computational hardware and standard all-atom force fields. We demonstrate extensive thermodynamic characterization of the folding process of the α-helical Trp-cage, the Villin headpiece and the β-sheet WW-domain. We fully characterize the free energy landscape, transition states, energy barriers between different states, and the per-residue stability of individual amino acids over a wide temperature range. We demonstrate that a state-of-the-art intramolecular force field can be combined with an implicit solvent model to obtain a high quality of the folded structures and also discuss limitations that still remain.},
	language = {en},
	number = {1},
	urldate = {2022-07-08},
	journal = {Scientific Reports},
	author = {Heilmann, Nana and Wolf, Moritz and Kozlowska, Mariana and Sedghamiz, Elaheh and Setzler, Julia and Brieg, Martin and Wenzel, Wolfgang},
	month = dec,
	year = {2020},
	note = {5 citations (Semantic Scholar/DOI) [2022-07-08]},
	pages = {18211},
}

@article{li_respre_2019,
	title = {{ResPRE}: high-accuracy protein contact prediction by coupling precision matrix with deep residual neural networks},
	volume = {35},
	issn = {1367-4803, 1460-2059},
	shorttitle = {{ResPRE}},
	url = {https://academic.oup.com/bioinformatics/article/35/22/4647/5487385},
	doi = {10.1093/bioinformatics/btz291},
	abstract = {Motivation: Contact-map of a protein sequence dictates the global topology of structural fold. Accurate prediction of the contact-map is thus essential to protein 3D structure prediction, which is particularly useful for the protein sequences that do not have close homology templates in the Protein Data Bank.},
	language = {en},
	number = {22},
	urldate = {2022-07-08},
	journal = {Bioinformatics},
	author = {Li, Yang and Hu, Jun and Zhang, Chengxin and Yu, Dong-Jun and Zhang, Yang},
	editor = {Valencia, Alfonso},
	month = nov,
	year = {2019},
	note = {107 citations (Semantic Scholar/DOI) [2022-07-08]},
	pages = {4647--4655},
}

@book{sutton_reinforcement_2018,
	address = {Cambridge, Massachusetts},
	edition = {Second edition},
	series = {Adaptive computation and machine learning series},
	title = {Reinforcement learning: an introduction},
	isbn = {978-0-262-03924-6},
	shorttitle = {Reinforcement learning},
	abstract = {"Reinforcement learning, one of the most active research areas in artificial intelligence, is a computational approach to learning whereby an agent tries to maximize the total amount of reward it receives while interacting with a complex, uncertain environment. In Reinforcement Learning, Richard Sutton and Andrew Barto provide a clear and simple account of the field's key ideas and algorithms."--},
	language = {en},
	publisher = {The MIT Press},
	author = {Sutton, Richard S. and Barto, Andrew G.},
	year = {2018},
	keywords = {Reinforcement learning},
}

@article{chaudhury_pyrosetta_2010,
	title = {{PyRosetta}: a script-based interface for implementing molecular modeling algorithms using {Rosetta}},
	volume = {26},
	issn = {1367-4803, 1460-2059},
	shorttitle = {{PyRosetta}},
	url = {https://academic.oup.com/bioinformatics/article-lookup/doi/10.1093/bioinformatics/btq007},
	doi = {10.1093/bioinformatics/btq007},
	abstract = {Summary: PyRosetta is a stand-alone Python-based implementation of the Rosetta molecular modeling package that allows users to write custom structure prediction and design algorithms using the major Rosetta sampling and scoring functions. PyRosetta contains Python bindings to libraries that deﬁne Rosetta functions including those for accessing and manipulating protein structure, calculating energies and running Monte Carlo-based simulations. PyRosetta can be used in two ways: (i) interactively, using iPython and (ii) script-based, using Python scripting. Interactive mode contains a number of help features and is ideal for beginners while script-mode is best suited for algorithm development. PyRosetta has similar computational performance to Rosetta, can be easily scaled up for cluster applications and has been implemented for algorithms demonstrating protein docking, protein folding, loop modeling and design.},
	language = {en},
	number = {5},
	urldate = {2022-07-08},
	journal = {Bioinformatics},
	author = {Chaudhury, S. and Lyskov, S. and Gray, J. J.},
	month = mar,
	year = {2010},
	note = {436 citations (Semantic Scholar/DOI) [2022-07-08]},
	pages = {689--691},
}

@article{drineas_randnla_2016,
	title = {{RandNLA}: randomized numerical linear algebra},
	volume = {59},
	issn = {0001-0782, 1557-7317},
	shorttitle = {{RandNLA}},
	url = {https://dl.acm.org/doi/10.1145/2842602},
	doi = {10.1145/2842602},
	abstract = {Randomization offers new benefits for large-scale linear algebra computations.},
	language = {en},
	number = {6},
	urldate = {2022-07-08},
	journal = {Communications of the ACM},
	author = {Drineas, Petros and Mahoney, Michael W.},
	month = may,
	year = {2016},
	note = {37 citations (Semantic Scholar/DOI) [2022-07-08]},
	pages = {80--90},
}

@incollection{rohl_protein_2004,
	title = {Protein {Structure} {Prediction} {Using} {Rosetta}},
	volume = {383},
	isbn = {978-0-12-182788-5},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0076687904830040},
	language = {en},
	urldate = {2022-07-08},
	booktitle = {Methods in {Enzymology}},
	publisher = {Elsevier},
	author = {Rohl, Carol A. and Strauss, Charlie E.M. and Misura, Kira M.S. and Baker, David},
	year = {2004},
	doi = {10.1016/S0076-6879(04)83004-0},
	pages = {66--93},
}

@article{senior_protein_2019,
	title = {Protein structure prediction using multiple deep neural networks in the 13th {Critical} {Assessment} of {Protein} {Structure} {Prediction} ({CASP13})},
	volume = {87},
	issn = {0887-3585, 1097-0134},
	url = {https://onlinelibrary.wiley.com/doi/10.1002/prot.25834},
	doi = {10.1002/prot.25834},
	abstract = {We describe AlphaFold, the protein structure prediction system that was entered by the group A7D in CASP13. Submissions were made by three free-modeling (FM) methods which combine the predictions of three neural networks. All three systems were guided by predictions of distances between pairs of residues produced by a neural network. Two systems assembled fragments produced by a generative neural network, one using scores from a network trained to regress GDT\_TS. The third system shows that simple gradient descent on a properly constructed potential is able to perform on par with more expensive traditional search techniques and without requiring domain segmentation. In the CASP13 FM assessors' ranking by summed z-scores, this system scored highest with 68.3 vs 48.2 for the next closest group (an average GDT\_TS of 61.4). The system produced highaccuracy structures (with GDT\_TS scores of 70 or higher) for 11 out of 43 FM domains. Despite not explicitly using template information, the results in the template category were comparable to the best performing template-based methods.},
	language = {en},
	number = {12},
	urldate = {2022-07-08},
	journal = {Proteins: Structure, Function, and Bioinformatics},
	author = {Senior, Andrew W. and Evans, Richard and Jumper, John and Kirkpatrick, James and Sifre, Laurent and Green, Tim and Qin, Chongli and Žídek, Augustin and Nelson, Alexander W. R. and Bridgland, Alex and Penedones, Hugo and Petersen, Stig and Simonyan, Karen and Crossan, Steve and Kohli, Pushmeet and Jones, David T. and Silver, David and Kavukcuoglu, Koray and Hassabis, Demis},
	month = dec,
	year = {2019},
	note = {148 citations (Semantic Scholar/DOI) [2022-07-08]},
	pages = {1141--1148},
}

@article{marks_protein_2011,
	title = {Protein {3D} {Structure} {Computed} from {Evolutionary} {Sequence} {Variation}},
	volume = {6},
	issn = {1932-6203},
	url = {https://dx.plos.org/10.1371/journal.pone.0028766},
	doi = {10.1371/journal.pone.0028766},
	abstract = {The evolutionary trajectory of a protein through sequence space is constrained by its function. Collections of sequence homologs record the outcomes of millions of evolutionary experiments in which the protein evolves according to these constraints. Deciphering the evolutionary record held in these sequences and exploiting it for predictive and engineering purposes presents a formidable challenge. The potential benefit of solving this challenge is amplified by the advent of inexpensive high-throughput genomic sequencing. In this paper we ask whether we can infer evolutionary constraints from a set of sequence homologs of a protein. The challenge is to distinguish true co-evolution couplings from the noisy set of observed correlations. We address this challenge using a maximum entropy model of the protein sequence, constrained by the statistics of the multiple sequence alignment, to infer residue pair couplings. Surprisingly, we find that the strength of these inferred couplings is an excellent predictor of residue-residue proximity in folded structures. Indeed, the top-scoring residue couplings are sufficiently accurate and well-distributed to define the 3D protein fold with remarkable accuracy. We quantify this observation by computing, from sequence alone, all-atom 3D structures of fifteen test proteins from different fold classes, ranging in size from 50 to 260 residues., including a G-protein coupled receptor. These blinded inferences are de novo, i.e., they do not use homology modeling or sequence-similar fragments from known structures. The co-evolution signals provide sufficient information to determine accurate 3D protein structure to 2.7–4.8 A˚ Ca-RMSD error relative to the observed structure, over at least two-thirds of the protein (method called EVfold, details at http://EVfold.org). This discovery provides insight into essential interactions constraining protein evolution and will facilitate a comprehensive survey of the universe of protein structures, new strategies in protein and drug design, and the identification of functional genetic variants in normal and disease genomes.},
	language = {en},
	number = {12},
	urldate = {2022-07-08},
	journal = {PLoS ONE},
	author = {Marks, Debora S. and Colwell, Lucy J. and Sheridan, Robert and Hopf, Thomas A. and Pagnani, Andrea and Zecchina, Riccardo and Sander, Chris},
	editor = {Sali, Andrej},
	month = dec,
	year = {2011},
	note = {935 citations (Semantic Scholar/DOI) [2022-07-08]},
	pages = {e28766},
}

@misc{chermoshentsev_polynomial_2021,
	title = {Polynomial unconstrained binary optimisation inspired by optical simulation},
	url = {http://arxiv.org/abs/2106.13167},
	abstract = {We propose an algorithm inspired by optical coherent Ising machines to solve the problem of polynomial unconstrained binary optimisation (PUBO). We benchmark the proposed algorithm against existing PUBO algorithms on the extended Sherrington-Kirkpatrick model and random third-degree polynomial pseudo-Boolean functions, and observe its superior performance. We also address instances of practically relevant computational problems such as protein folding and electronic structure calculations with problem sizes not accessible to existing quantum annealing devices. In particular, we successfully find the lowest-energy conformation of lattice protein molecules containing up to eleven amino-acids. The application of our algorithm to quantum chemistry sheds light on the shortcomings of approximating the electronic structure problem by a PUBO problem, which, in turn, puts into question the applicability of quantum annealers in this context.},
	language = {en},
	urldate = {2022-07-08},
	publisher = {arXiv},
	author = {Chermoshentsev, Dmitry A. and Malyshev, Aleksei O. and Tiunov, Egor S. and Mendoza, Douglas and Aspuru-Guzik, Alán and Fedorov, Aleksey K. and Lvovsky, Alexander I.},
	month = jun,
	year = {2021},
	note = {1 citations (Semantic Scholar/arXiv) [2022-07-08]
arXiv:2106.13167 [nlin, physics:quant-ph]},
	keywords = {Nonlinear Sciences - Chaotic Dynamics, Quantum Physics},
}

@misc{plappert_parameter_2018,
	title = {Parameter {Space} {Noise} for {Exploration}},
	url = {http://arxiv.org/abs/1706.01905},
	abstract = {Deep reinforcement learning (RL) methods generally engage in exploratory behavior through noise injection in the action space. An alternative is to add noise directly to the agent’s parameters, which can lead to more consistent exploration and a richer set of behaviors. Methods such as evolutionary strategies use parameter perturbations, but discard all temporal structure in the process and require significantly more samples. Combining parameter noise with traditional RL methods allows to combine the best of both worlds. We demonstrate that both off- and on-policy methods beneﬁt from this approach through experimental comparison of DQN, DDPG, and TRPO on high-dimensional discrete action environments as well as continuous control tasks.},
	language = {en},
	urldate = {2022-07-08},
	publisher = {arXiv},
	author = {Plappert, Matthias and Houthooft, Rein and Dhariwal, Prafulla and Sidor, Szymon and Chen, Richard Y. and Chen, Xi and Asfour, Tamim and Abbeel, Pieter and Andrychowicz, Marcin},
	month = jan,
	year = {2018},
	note = {406 citations (Semantic Scholar/arXiv) [2022-07-08]
arXiv:1706.01905 [cs, stat]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Computer Science - Robotics, Statistics - Machine Learning},
}

@article{sullivan_optimization_2020,
	title = {Optimization of {Molecular} {Dynamics} {Simulations} of c-{MYC1}-88—{An} {Intrinsically} {Disordered} {System}},
	volume = {10},
	issn = {2075-1729},
	url = {https://www.mdpi.com/2075-1729/10/7/109},
	doi = {10.3390/life10070109},
	abstract = {Many of the proteins involved in key cellular regulatory events contain extensive intrinsically disordered regions that are not readily amenable to conventional structure/function dissection. The oncoprotein c-MYC plays a key role in controlling cell proliferation and apoptosis and more than 70\% of the primary sequence is disordered. Computational approaches that shed light on the range of secondary and tertiary structural conformations therefore provide the only realistic chance to study such proteins. Here, we describe the results of several tests of force ﬁelds and water models employed in molecular dynamics simulations for the N-terminal 88 amino acids of c-MYC. Comparisons of the simulation data with experimental secondary structure assignments obtained by NMR establish a particular implicit solvation approach as highly congruent. The results provide insights into the structural dynamics of c-MYC1-88, which will be useful for guiding future experimental approaches. The protocols for trajectory analysis described here will be applicable for the analysis of a variety of computational simulations of intrinsically disordered proteins.},
	language = {en},
	number = {7},
	urldate = {2022-07-08},
	journal = {Life},
	author = {Sullivan, Sandra S. and Weinzierl, Robert O.J.},
	month = jul,
	year = {2020},
	note = {6 citations (Semantic Scholar/DOI) [2022-07-08]},
	pages = {109},
}

@article{khalil_learning_nodate,
	title = {Learning {Combinatorial} {Optimization} {Algorithms} over {Graphs}},
	abstract = {The design of good heuristics or approximation algorithms for NP-hard combinatorial optimization problems often requires signiﬁcant specialized knowledge and trial-and-error. Can we automate this challenging, tedious process, and learn the algorithms instead? In many real-world applications, it is typically the case that the same optimization problem is solved again and again on a regular basis, maintaining the same problem structure but differing in the data. This provides an opportunity for learning heuristic algorithms that exploit the structure of such recurring problems. In this paper, we propose a unique combination of reinforcement learning and graph embedding to address this challenge. The learned greedy policy behaves like a meta-algorithm that incrementally constructs a solution, and the action is determined by the output of a graph embedding network capturing the current state of the solution. We show that our framework can be applied to a diverse range of optimization problems over graphs, and learns effective algorithms for the Minimum Vertex Cover, Maximum Cut and Traveling Salesman problems.},
	language = {en},
	author = {Khalil, Elias and Dai, Hanjun and Zhang, Yuyu and Dilkina, Bistra and Song, Le},
	pages = {11},
}

@article{mortuza_improving_2021,
	title = {Improving fragment-based ab initio protein structure assembly using low-accuracy contact-map predictions},
	volume = {12},
	issn = {2041-1723},
	url = {https://www.nature.com/articles/s41467-021-25316-w},
	doi = {10.1038/s41467-021-25316-w},
	abstract = {Abstract
            Sequence-based contact prediction has shown considerable promise in assisting non-homologous structure modeling, but it often requires many homologous sequences and a sufficient number of correct contacts to achieve correct folds. Here, we developed a method, C-QUARK, that integrates multiple deep-learning and coevolution-based contact-maps to guide the replica-exchange Monte Carlo fragment assembly simulations. The method was tested on 247 non-redundant proteins, where C-QUARK could fold 75\% of the cases with TM-scores (template-modeling scores) ≥0.5, which was 2.6 times more than that achieved by QUARK. For the 59 cases that had either low contact accuracy or few homologous sequences, C-QUARK correctly folded 6 times more proteins than other contact-based folding methods. C-QUARK was also tested on 64 free-modeling targets from the 13th CASP (critical assessment of protein structure prediction) experiment and had an average GDT\_TS (global distance test) score that was 5\% higher than the best CASP predictors. These data demonstrate, in a robust manner, the progress in modeling non-homologous protein structures using low-accuracy and sparse contact-map predictions.},
	language = {en},
	number = {1},
	urldate = {2022-07-08},
	journal = {Nature Communications},
	author = {Mortuza, S. M. and Zheng, Wei and Zhang, Chengxin and Li, Yang and Pearce, Robin and Zhang, Yang},
	month = dec,
	year = {2021},
	note = {10 citations (Semantic Scholar/DOI) [2022-07-08]},
	pages = {5011},
}

@article{mariani_lddt_2013,
	title = {{lDDT}: a local superposition-free score for comparing protein structures and models using distance difference tests},
	volume = {29},
	issn = {1367-4803, 1460-2059},
	shorttitle = {{lDDT}},
	url = {https://academic.oup.com/bioinformatics/article-lookup/doi/10.1093/bioinformatics/btt473},
	doi = {10.1093/bioinformatics/btt473},
	abstract = {Motivation: The assessment of protein structure prediction techniques requires objective criteria to measure the similarity between a computational model and the experimentally determined reference structure. Conventional similarity measures based on a global superposition of carbon   atoms are strongly influenced by domain motions and do not assess the accuracy of local atomic details in the model. Results: The Local Distance Difference Test (lDDT) is a superpositionfree score that evaluates local distance differences of all atoms in a model, including validation of stereochemical plausibility. The reference can be a single structure, or an ensemble of equivalent structures. We demonstrate that lDDT is well suited to assess local model quality, even in the presence of domain movements, while maintaining good correlation with global measures. These properties make lDDT a robust tool for the automated assessment of structure prediction servers without manual intervention.},
	language = {en},
	number = {21},
	urldate = {2022-07-08},
	journal = {Bioinformatics},
	author = {Mariani, Valerio and Biasini, Marco and Barbato, Alessandro and Schwede, Torsten},
	month = nov,
	year = {2013},
	note = {247 citations (Semantic Scholar/DOI) [2022-07-08]},
	pages = {2722--2728},
}

@article{kandathil_improved_2018,
	title = {Improved fragment-based protein structure prediction by redesign of search heuristics},
	volume = {8},
	issn = {2045-2322},
	url = {http://www.nature.com/articles/s41598-018-31891-8},
	doi = {10.1038/s41598-018-31891-8},
	language = {en},
	number = {1},
	urldate = {2022-07-08},
	journal = {Scientific Reports},
	author = {Kandathil, Shaun M. and Garza-Fabre, Mario and Handl, Julia and Lovell, Simon C.},
	month = dec,
	year = {2018},
	note = {9 citations (Semantic Scholar/DOI) [2022-07-08]},
	pages = {13694},
}

@article{radway_illusion_2021,
	title = {Illusion of large on-chip memory by networked computing chips for neural network inference},
	volume = {4},
	issn = {2520-1131},
	url = {http://www.nature.com/articles/s41928-020-00515-3},
	doi = {10.1038/s41928-020-00515-3},
	language = {en},
	number = {1},
	urldate = {2022-07-08},
	journal = {Nature Electronics},
	author = {Radway, Robert M. and Bartolo, Andrew and Jolly, Paul C. and Khan, Zainab F. and Le, Binh Q. and Tandon, Pulkit and Wu, Tony F. and Xin, Yunfeng and Vianello, Elisa and Vivet, Pascal and Nowak, Etienne and Wong, H.-S. Philip and Aly, Mohamed M. Sabry and Beigne, Edith and Wootters, Mary and Mitra, Subhasish},
	month = jan,
	year = {2021},
	note = {4 citations (Semantic Scholar/DOI) [2022-07-08]},
	pages = {71--80},
}

@article{gianni_fuzziness_2021,
	title = {Fuzziness and {Frustration} in the {Energy} {Landscape} of {Protein} {Folding}, {Function}, and {Assembly}},
	volume = {54},
	issn = {0001-4842, 1520-4898},
	url = {https://pubs.acs.org/doi/10.1021/acs.accounts.0c00813},
	doi = {10.1021/acs.accounts.0c00813},
	language = {en},
	number = {5},
	urldate = {2022-07-08},
	journal = {Accounts of Chemical Research},
	author = {Gianni, Stefano and Freiberger, María Inés and Jemth, Per and Ferreiro, Diego U. and Wolynes, Peter G. and Fuxreiter, Monika},
	month = mar,
	year = {2021},
	note = {37 citations (Semantic Scholar/DOI) [2022-07-08]},
	pages = {1251--1259},
}

@inproceedings{tropea_fpga_2007,
	address = {Mar del Plata, Argentina},
	title = {{FPGA} {Implementation} of {Base}-{N} {Logarithm}},
	isbn = {978-1-4244-0606-7},
	url = {http://ieeexplore.ieee.org/document/4234316/},
	doi = {10.1109/SPL.2007.371719},
	abstract = {In this work, we present an area optimized FPGA implementation of an IP core to compute the base-N logarithm. Nevertheless, we also discuss the area, speed and precision trade-offs. We selected an algorithm that could be implemented on any FPGA avoiding vendor specific features like block RAMs, embedded multipliers, etc. We report the implementation results of a fixed point version of the algorithm using various common configurations on Xilinx and Actel devices. This implementation achieved the required area goals providing a very good speed-area ratio.},
	language = {en},
	urldate = {2022-07-08},
	booktitle = {2007 3rd {Southern} {Conference} on {Programmable} {Logic}},
	publisher = {IEEE},
	author = {Tropea, Salvador E.},
	month = feb,
	year = {2007},
	note = {8 citations (Semantic Scholar/DOI) [2022-07-08]},
	pages = {27--32},
}

@article{perdomo-ortiz_finding_2012,
	title = {Finding low-energy conformations of lattice protein models by quantum annealing},
	volume = {2},
	issn = {2045-2322},
	url = {http://www.nature.com/articles/srep00571},
	doi = {10.1038/srep00571},
	language = {en},
	number = {1},
	urldate = {2022-07-08},
	journal = {Scientific Reports},
	author = {Perdomo-Ortiz, Alejandro and Dickson, Neil and Drew-Brook, Marshall and Rose, Geordie and Aspuru-Guzik, Alán},
	month = dec,
	year = {2012},
	note = {248 citations (Semantic Scholar/DOI) [2022-07-08]},
	pages = {571},
}

@article{askar_evaluation_2021,
	title = {Evaluation of {Pseudo}-{Random} {Number} {Generation} on {GPU} {Cards}},
	volume = {9},
	issn = {2079-3197},
	url = {https://www.mdpi.com/2079-3197/9/12/142},
	doi = {10.3390/computation9120142},
	abstract = {Monte Carlo methods rely on sequences of random numbers to obtain solutions to many problems in science and engineering. In this work, we evaluate the performance of different pseudorandom number generators (PRNGs) of the Curand library on a number of modern Nvidia GPU cards. As a numerical test, we generate pseudo-random number (PRN) sequences and obtain non-uniform distributions using the acceptance-rejection method. We consider GPU, CPU, and hybrid CPU/GPU implementations. For the GPU, we additionally consider two different implementations using the host and device application programming interfaces (API). We study how the performance depends on implementation parameters, including the number of threads per block and the number of blocks per streaming multiprocessor. To achieve the fastest performance, one has to minimize the time consumed by PRNG seed setup and state update. The duration of seed setup time increases with the number of threads, while PRNG state update decreases. Hence, the fastest performance is achieved by the optimal balance of these opposing effects.},
	language = {en},
	number = {12},
	urldate = {2022-07-08},
	journal = {Computation},
	author = {Askar, Tair and Shukirgaliyev, Bekdaulet and Lukac, Martin and Abdikamalov, Ernazar},
	month = dec,
	year = {2021},
	note = {1 citations (Semantic Scholar/DOI) [2022-07-08]},
	pages = {142},
}

@article{lemke_encodermap_2019,
	title = {{EncoderMap}: {Dimensionality} {Reduction} and {Generation} of {Molecule} {Conformations}},
	volume = {15},
	issn = {1549-9618, 1549-9626},
	shorttitle = {{EncoderMap}},
	url = {https://pubs.acs.org/doi/10.1021/acs.jctc.8b00975},
	doi = {10.1021/acs.jctc.8b00975},
	abstract = {Molecular simulation is one example where large amounts of high-dimensional (high-d) data are generated. To extract useful information, e.g., about relevant states and important conformational transitions, a form of dimensionality reduction is required. Dimensionality reduction algorithms diﬀer in their ability to eﬃciently project large amounts of data to an informative low-dimensional (low-d) representation and the way the low and high-d representations are linked. We propose a dimensionality reduction algorithm called EncoderMap that is based on a neural network autoencoder in combination with a nonlinear distance metric. A key advantage of this method is that it establishes a functional link from the high-d to the low-d representation and vice versa. This allows us not only to eﬃciently project data points to the low-d representation but also to generate high-d representatives for any point in the low-d map. The potential of the algorithm is demonstrated for molecular simulation data of a small, highly ﬂexible peptide as well as for folding simulations of the 20-residue Trp-cage protein. We demonstrate that the algorithm is able to eﬃciently project the ensemble of high-d structures to a low-d map where major states can be identiﬁed and important conformational transitions are revealed. We also show that molecular conformations can be generated for any point or any connecting line between points on the low-d map. This ability of inverse mapping from the low-d to the high-d representation is particularly relevant for the use in algorithms that enhance the exploration of conformational space or the sampling of transitions between conformational states.},
	language = {en},
	number = {2},
	urldate = {2022-07-08},
	journal = {Journal of Chemical Theory and Computation},
	author = {Lemke, Tobias and Peter, Christine},
	month = feb,
	year = {2019},
	note = {50 citations (Semantic Scholar/DOI) [2022-07-08]},
	pages = {1209--1215},
}

@article{zheng_detecting_2019,
	title = {Detecting distant-homology protein structures by aligning deep neural-network based contact maps},
	volume = {15},
	issn = {1553-7358},
	url = {https://dx.plos.org/10.1371/journal.pcbi.1007411},
	doi = {10.1371/journal.pcbi.1007411},
	language = {en},
	number = {10},
	urldate = {2022-07-08},
	journal = {PLOS Computational Biology},
	author = {Zheng, Wei and Wuyun, Qiqige and Li, Yang and Mortuza, S. M. and Zhang, Chengxin and Pearce, Robin and Ruan, Jishou and Zhang, Yang},
	editor = {Deane, Charlotte M},
	month = oct,
	year = {2019},
	note = {31 citations (Semantic Scholar/DOI) [2022-07-08]},
	pages = {e1007411},
}

@article{di_lena_deep_2012,
	title = {Deep architectures for protein contact map prediction},
	volume = {28},
	issn = {1460-2059, 1367-4803},
	url = {https://academic.oup.com/bioinformatics/article-lookup/doi/10.1093/bioinformatics/bts475},
	doi = {10.1093/bioinformatics/bts475},
	abstract = {Motivation: Residue–residue contact prediction is important for protein structure prediction and other applications. However, the accuracy of current contact predictors often barely exceeds 20\% on long-range contacts, falling short of the level required for ab initio structure prediction.},
	language = {en},
	number = {19},
	urldate = {2022-07-08},
	journal = {Bioinformatics},
	author = {Di Lena, Pietro and Nagata, Ken and Baldi, Pierre},
	month = oct,
	year = {2012},
	note = {263 citations (Semantic Scholar/DOI) [2022-07-08]},
	pages = {2449--2457},
}

@article{braun_combining_2015,
	title = {Combining {Evolutionary} {Information} and an {Iterative} {Sampling} {Strategy} for {Accurate} {Protein} {Structure} {Prediction}},
	volume = {11},
	issn = {1553-7358},
	url = {https://dx.plos.org/10.1371/journal.pcbi.1004661},
	doi = {10.1371/journal.pcbi.1004661},
	language = {en},
	number = {12},
	urldate = {2022-07-08},
	journal = {PLOS Computational Biology},
	author = {Braun, Tatjana and Koehler Leman, Julia and Lange, Oliver F.},
	editor = {Marks, Debora S},
	month = dec,
	year = {2015},
	note = {19 citations (Semantic Scholar/DOI) [2022-07-08]},
	pages = {e1004661},
}

@article{gao_ant_2008,
	title = {An ant colony algorithm for solving {Max}-cut problem},
	volume = {18},
	issn = {10020071},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S1002007108002219},
	doi = {10.1016/j.pnsc.2008.04.006},
	abstract = {Max-cut problem is an NP-complete and classical combinatorial optimization problem that has a wide range of applications in different domains, such as bioinformatics, network optimization, statistical physics, and very large scale integration design. In this paper we investigate the capabilities of the ant colony optimization (ACO) heuristic for solving the Max-cut problem and present an AntCut algorithm. A large number of simulation experiments show that the algorithm can solve the Max-cut problem more eﬃciently and eﬀectively. Ó 2008 National Natural Science Foundation of China and Chinese Academy of Sciences. Published by Elsevier Limited and Science in China Press. All rights reserved.},
	language = {en},
	number = {9},
	urldate = {2022-07-08},
	journal = {Progress in Natural Science},
	author = {Gao, Lin and Zeng, Yan and Dong, Anguo},
	month = sep,
	year = {2008},
	note = {15 citations (Semantic Scholar/DOI) [2022-07-08]},
	pages = {1173--1178},
}

@article{kuhlman_advances_2019,
	title = {Advances in protein structure prediction and design},
	volume = {20},
	issn = {1471-0072, 1471-0080},
	url = {http://www.nature.com/articles/s41580-019-0163-x},
	doi = {10.1038/s41580-019-0163-x},
	abstract = {The prediction of protein three-dimensional structure from amino acid sequence has been a grand challenge problem in computational biophysics for decades, owing to its intrinsic scientific interest and also to the many potential applications for robust protein structure prediction algorithms, from genome interpretation to protein function prediction. More recently , the inverse problem — designing an amino acid sequence that will fold into a specified three-dimensional structure — has attracted growing attention as a potential route to the rational engineering of proteins with functions useful in biotechnology and medicine. Methods for the prediction and design of protein structures have advanced dramatically in the past decade. Increases in computing power and the rapid growth in protein sequence and structure databases have fuelled the development of new data-intensive and computationally demanding approaches for structure prediction. New algorithms for designing protein folds and protein–protein interfaces have been used to engineer novel high-order assemblies and to design from scratch fluorescent proteins with novel or enhanced properties, as well as signalling proteins with therapeutic potential. In this Review , we describe current approaches for protein structure prediction and design and highlight a selection of the successful applications they have enabled.},
	language = {en},
	number = {11},
	urldate = {2022-07-08},
	journal = {Nature Reviews Molecular Cell Biology},
	author = {Kuhlman, Brian and Bradley, Philip},
	month = nov,
	year = {2019},
	note = {263 citations (Semantic Scholar/DOI) [2022-07-08]},
	pages = {681--697},
}

@book{rice_advances_2014,
	address = {Hoboken, NJ},
	title = {Advances in chemical physics. 155},
	isbn = {978-1-118-75577-8},
	language = {en},
	publisher = {Wiley},
	editor = {Rice, Stuart Alan and Dinner, Aaron R.},
	year = {2014},
	note = {ISSN: 0065-2385},
}

@article{baek_accurate_2021,
	title = {Accurate prediction of protein structures and interactions using a three-track neural network},
	volume = {373},
	issn = {0036-8075, 1095-9203},
	url = {https://www.science.org/doi/10.1126/science.abj8754},
	doi = {10.1126/science.abj8754},
	abstract = {Deep learning takes on protein folding
            
              In 1972, Anfinsen won a Nobel prize for demonstrating a connection between a protein’s amino acid sequence and its three-dimensional structure. Since 1994, scientists have competed in the biannual Critical Assessment of Structure Prediction (CASP) protein-folding challenge. Deep learning methods took center stage at CASP14, with DeepMind’s Alphafold2 achieving remarkable accuracy. Baek
              et al
              . explored network architectures based on the DeepMind framework. They used a three-track network to process sequence, distance, and coordinate information simultaneously and achieved accuracies approaching those of DeepMind. The method, RoseTTA fold, can solve challenging x-ray crystallography and cryo–electron microscopy modeling problems and generate accurate models of protein-protein complexes. —VV
            
          , 
            Protein structure modeling enables the rapid solution of protein structures and provides insights into function.
          , 
            DeepMind presented notably accurate predictions at the recent 14th Critical Assessment of Structure Prediction (CASP14) conference. We explored network architectures that incorporate related ideas and obtained the best performance with a three-track network in which information at the one-dimensional (1D) sequence level, the 2D distance map level, and the 3D coordinate level is successively transformed and integrated. The three-track network produces structure predictions with accuracies approaching those of DeepMind in CASP14, enables the rapid solution of challenging x-ray crystallography and cryo–electron microscopy structure modeling problems, and provides insights into the functions of proteins of currently unknown structure. The network also enables rapid generation of accurate protein-protein complex models from sequence information alone, short-circuiting traditional approaches that require modeling of individual subunits followed by docking. We make the method available to the scientific community to speed biological research.},
	language = {en},
	number = {6557},
	urldate = {2022-07-08},
	journal = {Science},
	author = {Baek, Minkyung and DiMaio, Frank and Anishchenko, Ivan and Dauparas, Justas and Ovchinnikov, Sergey and Lee, Gyu Rie and Wang, Jue and Cong, Qian and Kinch, Lisa N. and Schaeffer, R. Dustin and Millán, Claudia and Park, Hahnbeom and Adams, Carson and Glassman, Caleb R. and DeGiovanni, Andy and Pereira, Jose H. and Rodrigues, Andria V. and van Dijk, Alberdina A. and Ebrecht, Ana C. and Opperman, Diederik J. and Sagmeister, Theo and Buhlheller, Christoph and Pavkov-Keller, Tea and Rathinaswamy, Manoj K. and Dalwadi, Udit and Yip, Calvin K. and Burke, John E. and Garcia, K. Christopher and Grishin, Nick V. and Adams, Paul D. and Read, Randy J. and Baker, David},
	month = aug,
	year = {2021},
	note = {505 citations (Semantic Scholar/DOI) [2022-07-08]},
	pages = {871--876},
}

@article{wang_accurate_2017,
	title = {Accurate {De} {Novo} {Prediction} of {Protein} {Contact} {Map} by {Ultra}-{Deep} {Learning} {Model}},
	volume = {13},
	issn = {1553-7358},
	url = {https://dx.plos.org/10.1371/journal.pcbi.1005324},
	doi = {10.1371/journal.pcbi.1005324},
	abstract = {Motivation Protein contacts contain key information for the understanding of protein structure and function and thus, contact prediction from sequence is an important problem. Recently exciting progress has been made on this problem, but the predicted contacts for proteins without many sequence homologs is still of low quality and not very useful for de novo structure prediction.},
	language = {en},
	number = {1},
	urldate = {2022-07-08},
	journal = {PLOS Computational Biology},
	author = {Wang, Sheng and Sun, Siqi and Li, Zhen and Zhang, Renyu and Xu, Jinbo},
	editor = {Schlessinger, Avner},
	month = jan,
	year = {2017},
	note = {591 citations (Semantic Scholar/DOI) [2022-07-08]},
	pages = {e1005324},
}

@misc{ghosh_3339uw_2022,
	title = {A 333.{9uW} 0.158mm\${\textasciicircum}2\$ {Saber} {Learning} with {Rounding} based {Post}-{Quantum} {Crypto} {Accelerator}},
	url = {http://arxiv.org/abs/2201.07375},
	abstract = {National Institute of Standard \& Technology (NIST) is currently running a multi-year-long standardization procedure to select quantum-safe or post-quantum cryptographic schemes to be used in the future. Saber is the only LWR based algorithm to be in the final of Round 3. This work presents a Saber ASIC which provides 1.37X power-efficient, 1.75x lower area, and 4x less memory implementation w.r.t. other SoA PQC ASIC. The energy-hungry multiplier block is 1.5x energyefficient than SoA.},
	language = {en},
	urldate = {2022-07-08},
	publisher = {arXiv},
	author = {Ghosh, Archisman and Mera, J. M. B. and Karmakar, Angshuman and Das, Debayan and Ghosh, Santosh and Verbauwhede, Ingrid and Sen, Shreyas},
	month = jul,
	year = {2022},
	note = {0 citations (Semantic Scholar/arXiv) [2022-07-08]
arXiv:2201.07375 [cs]},
	keywords = {Computer Science - Cryptography and Security, Computer Science - Hardware Architecture},
}

@article{wang_development_2004,
	title = {Development and testing of a general amber force field},
	volume = {25},
	issn = {0192-8651, 1096-987X},
	url = {https://onlinelibrary.wiley.com/doi/10.1002/jcc.20035},
	doi = {10.1002/jcc.20035},
	abstract = {We describe here a general Amber force ﬁeld (GAFF) for organic molecules. GAFF is designed to be compatible with existing Amber force ﬁelds for proteins and nucleic acids, and has parameters for most organic and pharmaceutical molecules that are composed of H, C, N, O, S, P, and halogens. It uses a simple functional form and a limited number of atom types, but incorporates both empirical and heuristic models to estimate force constants and partial atomic charges. The performance of GAFF in test cases is encouraging. In test I, 74 crystallographic structures were compared to GAFF minimized structures, with a root-mean-square displacement of 0.26 Å, which is comparable to that of the Tripos 5.2 force ﬁeld (0.25 Å) and better than those of MMFF 94 and CHARMm (0.47 and 0.44 Å, respectively). In test II, gas phase minimizations were performed on 22 nucleic acid base pairs, and the minimized structures and intermolecular energies were compared to MP2/6-31G* results. The RMS of displacements and relative energies were 0.25 Å and 1.2 kcal/mol, respectively. These data are comparable to results from Parm99/RESP (0.16 Å and 1.18 kcal/mol, respectively), which were parameterized to these base pairs. Test III looked at the relative energies of 71 conformational pairs that were used in development of the Parm99 force ﬁeld. The RMS error in relative energies (compared to experiment) is about 0.5 kcal/mol. GAFF can be applied to wide range of molecules in an automatic fashion, making it suitable for rational drug design and database searching.},
	language = {en},
	number = {9},
	urldate = {2022-07-08},
	journal = {Journal of Computational Chemistry},
	author = {Wang, Junmei and Wolf, Romain M. and Caldwell, James W. and Kollman, Peter A. and Case, David A.},
	month = jul,
	year = {2004},
	note = {9996 citations (Semantic Scholar/DOI) [2022-07-08]},
	pages = {1157--1174},
}

@article{senior_improved_2020,
	title = {Improved protein structure prediction using potentials from deep learning},
	volume = {577},
	issn = {0028-0836, 1476-4687},
	url = {http://www.nature.com/articles/s41586-019-1923-7},
	doi = {10.1038/s41586-019-1923-7},
	language = {en},
	number = {7792},
	urldate = {2022-07-08},
	journal = {Nature},
	author = {Senior, Andrew W. and Evans, Richard and Jumper, John and Kirkpatrick, James and Sifre, Laurent and Green, Tim and Qin, Chongli and Žídek, Augustin and Nelson, Alexander W. R. and Bridgland, Alex and Penedones, Hugo and Petersen, Stig and Simonyan, Karen and Crossan, Steve and Kohli, Pushmeet and Jones, David T. and Silver, David and Kavukcuoglu, Koray and Hassabis, Demis},
	month = jan,
	year = {2020},
	note = {1362 citations (Semantic Scholar/DOI) [2022-07-08]},
	pages = {706--710},
}

@article{cai_power-efficient_2020,
	title = {Power-efficient combinatorial optimization using intrinsic noise in memristor {Hopfield} neural networks},
	volume = {3},
	issn = {2520-1131},
	url = {http://www.nature.com/articles/s41928-020-0436-6},
	doi = {10.1038/s41928-020-0436-6},
	language = {en},
	number = {7},
	urldate = {2022-07-08},
	journal = {Nature Electronics},
	author = {Cai, Fuxi and Kumar, Suhas and Van Vaerenbergh, Thomas and Sheng, Xia and Liu, Rui and Li, Can and Liu, Zhan and Foltin, Martin and Yu, Shimeng and Xia, Qiangfei and Yang, J. Joshua and Beausoleil, Raymond and Lu, Wei D. and Strachan, John Paul},
	month = jul,
	year = {2020},
	note = {97 citations (Semantic Scholar/DOI) [2022-07-08]},
	pages = {409--418},
}

@article{russ_evolution-based_2020,
	title = {An evolution-based model for designing chorismate mutase enzymes},
	volume = {369},
	issn = {0036-8075, 1095-9203},
	url = {https://www.sciencemag.org/lookup/doi/10.1126/science.aba3304},
	doi = {10.1126/science.aba3304},
	abstract = {The rational design of enzymes is an important goal for both fundamental and practical reasons. Here, we describe a process to learn the constraints for specifying proteins purely from evolutionary sequence data, design and build libraries of synthetic genes, and test them for activity in vivo using a quantitative complementation assay. For chorismate mutase, a key enzyme in the biosynthesis of aromatic amino acids, we demonstrate the design of natural-like catalytic function with substantial sequence diversity. Further optimization focuses the generative model toward function in a specific genomic context. The data show that sequence-based statistical models suffice to specify proteins and provide access to an enormous space of functional sequences. This result provides a foundation for a general process for evolution-based design of artificial proteins.},
	language = {en},
	number = {6502},
	urldate = {2022-07-08},
	journal = {Science},
	author = {Russ, William P. and Figliuzzi, Matteo and Stocker, Christian and Barrat-Charlaix, Pierre and Socolich, Michael and Kast, Peter and Hilvert, Donald and Monasson, Remi and Cocco, Simona and Weigt, Martin and Ranganathan, Rama},
	month = jul,
	year = {2020},
	note = {92 citations (Semantic Scholar/DOI) [2022-07-08]},
	pages = {440--445},
}

@article{ronnow_defining_2014,
	title = {Defining and detecting quantum speedup},
	volume = {345},
	issn = {0036-8075, 1095-9203},
	url = {http://arxiv.org/abs/1401.2910},
	doi = {10.1126/science.1252319},
	abstract = {The development of small-scale digital and analog quantum devices raises the question of how to fairly assess and compare the computational power of classical and quantum devices, and of how to detect quantum speedup. Here we show how to define and measure quantum speedup in various scenarios, and how to avoid pitfalls that might mask or fake quantum speedup. We illustrate our discussion with data from a randomized benchmark test on a D-Wave Two device with up to 503 qubits. Comparing the performance of the device on random spin glass instances with limited precision to simulated classical and quantum annealers, we find no evidence of quantum speedup when the entire data set is considered, and obtain inconclusive results when comparing subsets of instances on an instance-by-instance basis. Our results for one particular benchmark do not rule out the possibility of speedup for other classes of problems and illustrate that quantum speedup is elusive and can depend on the question posed.},
	language = {en},
	number = {6195},
	urldate = {2022-07-08},
	journal = {Science},
	author = {Rønnow, Troels F. and Wang, Zhihui and Job, Joshua and Boixo, Sergio and Isakov, Sergei V. and Wecker, David and Martinis, John M. and Lidar, Daniel A. and Troyer, Matthias},
	month = jul,
	year = {2014},
	note = {417 citations (Semantic Scholar/arXiv) [2022-07-08]
417 citations (Semantic Scholar/DOI) [2022-07-08]
arXiv:1401.2910 [quant-ph]},
	keywords = {Quantum Physics},
	pages = {420--424},
}

@article{roussel_barriers_2021,
	title = {Barriers and {Dynamical} {Paths} in {Alternating} {Gibbs} {Sampling} of {Restricted} {Boltzmann} {Machines}},
	volume = {104},
	issn = {2470-0045, 2470-0053},
	url = {http://arxiv.org/abs/2107.06013},
	doi = {10.1103/PhysRevE.104.034109},
	abstract = {Restricted Boltzmann Machines (RBM) are bi-layer neural networks used for the unsupervised learning of model distributions from data. The bipartite architecture of RBM naturally defines an elegant sampling procedure, called Alternating Gibbs Sampling (AGS), where the configurations of the latent-variable layer are sampled conditional to the data-variable layer, and vice versa. We study here the performance of AGS on several analytically tractable models borrowed from statistical mechanics. We show that standard AGS is not more efficient than classical Metropolis-Hastings (MH) sampling of the effective energy landscape defined on the data layer. However, RBM can identify meaningful representations of training data in their latent space. Furthermore, using these representations and combining Gibbs sampling with the MH algorithm in the latent space can enhance the sampling performance of the RBM when the hidden units encode weakly dependent features of the data. We illustrate our findings on three datasets: Bars and Stripes and MNIST, well known in machine learning, and the so-called Lattice Proteins, introduced in theoretical biology to study the sequence-to-structure mapping in proteins.},
	language = {en},
	number = {3},
	urldate = {2022-07-08},
	journal = {Physical Review E},
	author = {Roussel, Clément and Cocco, Simona and Monasson, Rémi},
	month = sep,
	year = {2021},
	note = {5 citations (Semantic Scholar/arXiv) [2022-07-08]
5 citations (Semantic Scholar/DOI) [2022-07-08]
arXiv:2107.06013 [cond-mat]},
	keywords = {Computer Science - Machine Learning, Condensed Matter - Disordered Systems and Neural Networks, Condensed Matter - Statistical Mechanics},
	pages = {034109},
}

@techreport{dutta_ising_2020,
	type = {preprint},
	title = {An {Ising} {Hamiltonian} {Solver} using {Stochastic} {Phase}-{Transition} {Nano}-{Oscillators}},
	url = {https://www.researchsquare.com/article/rs-93438/v1},
	abstract = {Abstract
          Computationally hard problems, including combinatorial optimization, can be mapped into the problem of finding the ground-state of an Ising Hamiltonian. Building physical systems with collective computational ability and distributed parallel processing capability can accelerate the ground-state search. Here, we present a continuous-time dynamical system (CTDS) approach where the ground-state solution appears as stable points or attractor states of the CTDS. We harness the emergent dynamics of a network of phase-transition nano-oscillators (PTNO) to build an Ising Hamiltonian solver. The hardware fabric comprises of electrically coupled injection-locked stochastic PTNOs with bi-stable phases emulating artificial Ising spins. We demonstrate the ability of the stochastic PTNO-CTDS to progressively find more optimal solution by increasing the strength of the injection-locking signal – akin to performing classical annealing. We demonstrate in silico that the PTNO-CTDS prototype solves a benchmark non-deterministic polynomial time (NP)-hard Max-Cut problem with high probability of success. Using experimentally calibrated numerical simulations, we investigate the performance of the hardware with increasing problem size. We show the best-in-class energy-efficiency of 3.26x107 solutions/sec/Watt which translates to over five orders of magnitude improvement when compared with digital CMOS, superconducting qubit and photonic Ising solver approaches. We also demonstrate an order of magnitude improvement over a discrete-time memristor-based Hopfield network approach. Such an energy efficient CTDS hardware exhibiting high solution-throughput/Watt can find application in industrial planning and manufacturing, defense and cyber-security, bioinformatics and drug discovery.},
	language = {en},
	urldate = {2022-07-08},
	institution = {In Review},
	author = {Dutta, Sourav and Khanna, Abhishek and Paik, Hanjong and Schlom, Darrell and Raychowdhury, Arijit and Toroczkai, Zoltan and Datta, Suman},
	month = nov,
	year = {2020},
	doi = {10.21203/rs.3.rs-93438/v1},
}

@article{hopf_sequence_2014,
	title = {Sequence co-evolution gives {3D} contacts and structures of protein complexes},
	volume = {3},
	issn = {2050-084X},
	url = {https://elifesciences.org/articles/03430},
	doi = {10.7554/eLife.03430},
	abstract = {Protein–protein interactions are fundamental to many biological processes. Experimental screens have identified tens of thousands of interactions, and structural biology has provided detailed functional insight for select 3D protein complexes. An alternative rich source of information about protein interactions is the evolutionary sequence record. Building on earlier work, we show that analysis of correlated evolutionary sequence changes across proteins identifies residues that are close in space with sufficient accuracy to determine the three-dimensional structure of the protein complexes. We evaluate prediction performance in blinded tests on 76 complexes of known 3D structure, predict protein–protein contacts in 32 complexes of unknown structure, and demonstrate how evolutionary couplings can be used to distinguish between interacting and non-interacting protein pairs in a large complex. With the current growth of sequences, we expect that the method can be generalized to genome-wide elucidation of protein–protein interaction networks and used for interaction predictions at residue resolution.},
	language = {en},
	urldate = {2022-07-08},
	journal = {eLife},
	author = {Hopf, Thomas A and Schärfe, Charlotta P I and Rodrigues, João P G L M and Green, Anna G and Kohlbacher, Oliver and Sander, Chris and Bonvin, Alexandre M J J and Marks, Debora S},
	month = sep,
	year = {2014},
	note = {394 citations (Semantic Scholar/DOI) [2022-07-08]},
	pages = {e03430},
}

@article{ma_sampling_2019,
	title = {Sampling can be faster than optimization},
	volume = {116},
	issn = {0027-8424, 1091-6490},
	url = {https://pnas.org/doi/full/10.1073/pnas.1820003116},
	doi = {10.1073/pnas.1820003116},
	abstract = {Significance
            Modern large-scale data analysis and machine learning applications rely critically on computationally efficient algorithms. There are 2 main classes of algorithms used in this setting—those based on optimization and those based on Monte Carlo sampling. The folk wisdom is that sampling is necessarily slower than optimization and is only warranted in situations where estimates of uncertainty are needed. We show that this folk wisdom is not correct in general—there is a natural class of nonconvex problems for which the computational complexity of sampling algorithms scales linearly with the model dimension while that of optimization algorithms scales exponentially.
          , 
            Optimization algorithms and Monte Carlo sampling algorithms have provided the computational foundations for the rapid growth in applications of statistical machine learning in recent years. There is, however, limited theoretical understanding of the relationships between these 2 kinds of methodology, and limited understanding of relative strengths and weaknesses. Moreover, existing results have been obtained primarily in the setting of convex functions (for optimization) and log-concave functions (for sampling). In this setting, where local properties determine global properties, optimization algorithms are unsurprisingly more efficient computationally than sampling algorithms. We instead examine a class of nonconvex objective functions that arise in mixture modeling and multistable systems. In this nonconvex setting, we find that the computational complexity of sampling algorithms scales linearly with the model dimension while that of optimization algorithms scales exponentially.},
	language = {en},
	number = {42},
	urldate = {2022-07-08},
	journal = {Proceedings of the National Academy of Sciences},
	author = {Ma, Yi-An and Chen, Yuansi and Jin, Chi and Flammarion, Nicolas and Jordan, Michael I.},
	month = oct,
	year = {2019},
	note = {82 citations (Semantic Scholar/DOI) [2022-07-08]},
	pages = {20881--20885},
}

@article{jumper_highly_2021,
	title = {Highly accurate protein structure prediction with {AlphaFold}},
	volume = {596},
	issn = {0028-0836, 1476-4687},
	url = {https://www.nature.com/articles/s41586-021-03819-2},
	doi = {10.1038/s41586-021-03819-2},
	abstract = {Abstract
            
              Proteins are essential to life, and understanding their structure can facilitate a mechanistic understanding of their function. Through an enormous experimental effort
              1–4
              , the structures of around 100,000 unique proteins have been determined
              5
              , but this represents a small fraction of the billions of known protein sequences
              6,7
              . Structural coverage is bottlenecked by the months to years of painstaking effort required to determine a single protein structure. Accurate computational approaches are needed to address this gap and to enable large-scale structural bioinformatics. Predicting the three-dimensional structure that a protein will adopt based solely on its amino acid sequence—the structure prediction component of the ‘protein folding problem’
              8
              —has been an important open research problem for more than 50 years
              9
              . Despite recent progress
              10–14
              , existing methods fall far short of atomic accuracy, especially when no homologous structure is available. Here we provide the first computational method that can regularly predict protein structures with atomic accuracy even in cases in which no similar structure is known. We validated an entirely redesigned version of our neural network-based model, AlphaFold, in the challenging 14th Critical Assessment of protein Structure Prediction (CASP14)
              15
              , demonstrating accuracy competitive with experimental structures in a majority of cases and greatly outperforming other methods. Underpinning the latest version of AlphaFold is a novel machine learning approach that incorporates physical and biological knowledge about protein structure, leveraging multi-sequence alignments, into the design of the deep learning algorithm.},
	language = {en},
	number = {7873},
	urldate = {2022-07-08},
	journal = {Nature},
	author = {Jumper, John and Evans, Richard and Pritzel, Alexander and Green, Tim and Figurnov, Michael and Ronneberger, Olaf and Tunyasuvunakool, Kathryn and Bates, Russ and Žídek, Augustin and Potapenko, Anna and Bridgland, Alex and Meyer, Clemens and Kohl, Simon A. A. and Ballard, Andrew J. and Cowie, Andrew and Romera-Paredes, Bernardino and Nikolov, Stanislav and Jain, Rishub and Adler, Jonas and Back, Trevor and Petersen, Stig and Reiman, David and Clancy, Ellen and Zielinski, Michal and Steinegger, Martin and Pacholska, Michalina and Berghammer, Tamas and Bodenstein, Sebastian and Silver, David and Vinyals, Oriol and Senior, Andrew W. and Kavukcuoglu, Koray and Kohli, Pushmeet and Hassabis, Demis},
	month = aug,
	year = {2021},
	note = {3193 citations (Semantic Scholar/DOI) [2022-07-08]},
	pages = {583--589},
}

@misc{christiansen_simulating_2020,
	title = {Simulating {Met}-{Enkephalin} {With} {Population} {Annealing} {Molecular} {Dynamics}},
	url = {http://arxiv.org/abs/2005.00810},
	abstract = {Met-enkephalin, one of the smallest opiate peptides and an important neurotransmitter, is a widely used benchmarking problem in the ﬁeld of molecular simulation. Through its range of possible low-temperature conformations separated by free-energy barriers it was previously found to be hard to thermalize using straight canonical molecular dynamics simulations. Here, we demonstrate how one can use the recently proposed population annealing molecular dynamics scheme to overcome these diﬃculties. We show how the use of multihistogram reweighting allows one to accurately estimate the density of states of the system and hence derive estimates such as the potential energy as quasi continuous functions of temperature. We further investigate the free-energy surface as a function of end-to-end distance and radiusof-gyration and observe two distinct basins of attraction.},
	language = {en},
	urldate = {2022-07-08},
	publisher = {arXiv},
	author = {Christiansen, Henrik and Weigel, Martin and Janke, Wolfhard},
	month = may,
	year = {2020},
	note = {0 citations (Semantic Scholar/arXiv) [2022-07-08]
arXiv:2005.00810 [cond-mat, physics:physics]},
	keywords = {Condensed Matter - Soft Condensed Matter, Condensed Matter - Statistical Mechanics, Physics - Computational Physics},
}

@article{barrat-charlaix_understanding_nodate,
	title = {Understanding and improving statistical models of protein sequences},
	language = {en},
	author = {Barrat-Charlaix, Pierre},
	pages = {152},
}

@article{tiana_use_2007,
	title = {Use of the {Metropolis} algorithm to simulate the dynamics of protein chains},
	volume = {380},
	issn = {03784371},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0378437107001926},
	doi = {10.1016/j.physa.2007.02.044},
	abstract = {The Metropolis implementation of the Monte Carlo algorithm has been developed to study the equilibrium thermodynamics of many-body systems. Choosing small trial moves, the trajectories obtained applying this algorithm agree with those obtained by Langevin’s dynamics. Applying this procedure to a simpliﬁed protein model, it is possible to show that setting a threshold of 1  on the movement of the dihedrals of the protein backbone in a single Monte Carlo step, the mean quantities associated with the off-equilibrium dynamics (e.g., energy, RMSD, etc.) are well reproduced, while the good description of higher moments requires smaller moves. An important result is that the time duration of a Monte Carlo step depends linearly on the temperature, something which should be accounted for when doing simulations at different temperatures.},
	language = {en},
	urldate = {2022-07-08},
	journal = {Physica A: Statistical Mechanics and its Applications},
	author = {Tiana, G. and Sutto, L. and Broglia, R.A.},
	month = jul,
	year = {2007},
	note = {49 citations (Semantic Scholar/DOI) [2022-07-08]},
	pages = {241--249},
}

@article{zhang_scoring_2004,
	title = {Scoring function for automated assessment of protein structure template quality},
	volume = {57},
	issn = {0887-3585, 1097-0134},
	url = {https://onlinelibrary.wiley.com/doi/10.1002/prot.20264},
	doi = {10.1002/prot.20264},
	abstract = {We have developed a new scoring function, the template modeling score (TM-score), to assess the quality of protein structure templates and predicted full-length models by extending the approaches used in Global Distance Test (GDT)1 and MaxSub.2 First, a protein size-dependent scale is exploited to eliminate the inherent protein size dependence of the previous scores and appropriately account for random protein structure pairs. Second, rather than setting speciﬁc distance cutoffs and calculating only the fractions with errors below the cutoff, all residue pairs in alignment/modeling are evaluated in the proposed score. For comparison of various scoring functions, we have constructed a large-scale benchmark set of structure templates for 1489 small to medium size proteins using the threading program PROSPECTOR\_3 and built the full-length models using MODELLER and TASSER. The TM-score of the initial threading alignments, compared to the GDT and MaxSub scoring functions, shows a much stronger correlation to the quality of the ﬁnal full-length models. The TM-score is further exploited as an assessment of all ‘new fold’ targets in the recent CASP5 experiment and shows a close coincidence with the results of human-expert visual assessment. These data suggest that the TMscore is a useful complement to the fully automated assessment of protein structure predictions. The executable program of TM-score is freely downloadable at http://bioinformatics.buffalo.edu/TM-score. Proteins 2004;57:702–710. © 2004 Wiley-Liss, Inc.},
	language = {en},
	number = {4},
	urldate = {2022-07-08},
	journal = {Proteins: Structure, Function, and Bioinformatics},
	author = {Zhang, Yang and Skolnick, Jeffrey},
	month = dec,
	year = {2004},
	note = {1512 citations (Semantic Scholar/DOI) [2022-07-08]},
	pages = {702--710},
}

@article{greenberg_understanding_nodate,
	title = {Understanding the {Metropolis}-{Hastings} {Algorithm}},
	language = {en},
	author = {Greenberg, Siddhartha CHIBand Edward},
	pages = {9},
}

@article{liang_evolutionary_2001,
	title = {Evolutionary {Monte} {Carlo} for protein folding simulations},
	volume = {115},
	issn = {0021-9606, 1089-7690},
	url = {http://aip.scitation.org/doi/10.1063/1.1387478},
	doi = {10.1063/1.1387478},
	language = {en},
	number = {7},
	urldate = {2022-07-08},
	journal = {The Journal of Chemical Physics},
	author = {Liang, Faming and Wong, Wing Hung},
	month = aug,
	year = {2001},
	note = {187 citations (Semantic Scholar/DOI) [2022-07-08]},
	pages = {3374--3380},
}

@article{hansmann_new_1999,
	title = {New {Monte} {Carlo} algorithms for protein folding},
	volume = {9},
	issn = {0959440X},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0959440X99800256},
	doi = {10.1016/S0959-440X(99)80025-6},
	language = {en},
	number = {2},
	urldate = {2022-07-08},
	journal = {Current Opinion in Structural Biology},
	author = {Hansmann, Ulrich H.E. and Okamoto, Yuko},
	month = apr,
	year = {1999},
	note = {271 citations (Semantic Scholar/DOI) [2022-07-08]},
	pages = {177--183},
}

@article{wong_exploring_2018,
	title = {Exploring the conformational space for protein folding with sequential {Monte} {Carlo}},
	volume = {12},
	issn = {1932-6157},
	url = {https://projecteuclid.org/journals/annals-of-applied-statistics/volume-12/issue-3/Exploring-the-conformational-space-for-protein-folding-with-sequential-Monte/10.1214/17-AOAS1124.full},
	doi = {10.1214/17-AOAS1124},
	language = {en},
	number = {3},
	urldate = {2022-07-08},
	journal = {The Annals of Applied Statistics},
	author = {Wong, Samuel W. K. and Liu, Jun S. and Kou, S. C.},
	month = sep,
	year = {2018},
	note = {5 citations (Semantic Scholar/DOI) [2022-07-08]},
}

@article{kaiser_probabilistic_2021,
	title = {Probabilistic computing with p-bits},
	volume = {119},
	issn = {0003-6951, 1077-3118},
	url = {https://aip.scitation.org/doi/10.1063/5.0067927},
	doi = {10.1063/5.0067927},
	abstract = {Digital computers store information in the form of bits that can take on one of two values 0 and 1, while quantum computers are based on qubits that are described by a complex wavefunction, whose squared magnitude gives the probability of measuring either 0 or 1. Here, we make the case for a probabilistic computer based on p-bits, which take on values 0 and 1 with controlled probabilities and can be implemented with specialized compact energy-efﬁcient hardware. We propose a generic architecture for such p-computers and emulate systems with thousands of p-bits to show that they can signiﬁcantly accelerate randomized algorithms used in a wide variety of applications including but not limited to Bayesian networks, optimization, Ising models, and quantum Monte Carlo.},
	language = {en},
	number = {15},
	urldate = {2022-07-08},
	journal = {Applied Physics Letters},
	author = {Kaiser, Jan and Datta, Supriyo},
	month = oct,
	year = {2021},
	note = {0 citations (Semantic Scholar/DOI) [2022-07-08]},
	pages = {150503},
}

@article{yang_improved_2020,
	title = {Improved protein structure prediction using predicted interresidue orientations},
	volume = {117},
	issn = {0027-8424, 1091-6490},
	url = {https://pnas.org/doi/full/10.1073/pnas.1914677117},
	doi = {10.1073/pnas.1914677117},
	abstract = {The prediction of interresidue contacts and distances from coevolutionary data using deep learning has considerably advanced protein structure prediction. Here, we build on these advances by developing a deep residual network for predicting interresidue orientations, in addition to distances, and a Rosetta-constrained energy-minimization protocol for rapidly and accurately generating structure models guided by these restraints. In benchmark tests on 13th Community-Wide Experiment on the Critical Assessment of Techniques for Protein Structure Prediction (CASP13)- and Continuous Automated Model Evaluation (CAMEO)-derived sets, the method outperforms all previously described structure-prediction methods. Although trained entirely on native proteins, the network consistently assigns higher probability to de novo-designed proteins, identifying the key fold-determining residues and providing an independent quantitative measure of the “ideality” of a protein structure. The method promises to be useful for a broad range of protein structure prediction and design problems.},
	language = {en},
	number = {3},
	urldate = {2022-07-08},
	journal = {Proceedings of the National Academy of Sciences},
	author = {Yang, Jianyi and Anishchenko, Ivan and Park, Hahnbeom and Peng, Zhenling and Ovchinnikov, Sergey and Baker, David},
	month = jan,
	year = {2020},
	note = {635 citations (Semantic Scholar/DOI) [2022-07-08]},
	pages = {1496--1503},
}

@misc{kaiser_benchmarking_2021,
	title = {Benchmarking a {Probabilistic} {Coprocessor}},
	url = {http://arxiv.org/abs/2109.14801},
	abstract = {Computation in the past decades has been driven by deterministic computers based on classical deterministic bits. Recently, alternative computing paradigms and domain-based computing like quantum computing and probabilistic computing have gained traction. While quantum computers based on q-bits utilize quantum effects to advance computation, probabilistic computers based on probabilistic (p-)bits are naturally suited to solve problems that require large amount of random numbers utilized in Monte Carlo and Markov Chain Monte Carlo algorithms. These Monte Carlo techniques are used to solve important problems in the fields of optimization, numerical integration or sampling from probability distributions. However, to efficiently implement Monte Carlo algorithms the generation of random numbers is crucial. In this paper, we present and benchmark a probabilistic coprocessor based on p-bits that are naturally suited to solve these problems. We present multiple examples and project that a nanomagnetic implementation of our probabilistic coprocessor can outperform classical CPU and GPU implementations by multiple orders of magnitude.},
	language = {en},
	urldate = {2022-07-08},
	publisher = {arXiv},
	author = {Kaiser, Jan and Jaiswal, Risi and Behin-Aein, Behtash and Datta, Supriyo},
	month = sep,
	year = {2021},
	note = {3 citations (Semantic Scholar/arXiv) [2022-07-08]
arXiv:2109.14801 [cond-mat]},
	keywords = {Computer Science - Emerging Technologies, Condensed Matter - Disordered Systems and Neural Networks, Condensed Matter - Mesoscale and Nanoscale Physics},
}

@article{park_protein_2018,
	title = {Protein homology model refinement by large-scale energy optimization},
	volume = {115},
	issn = {0027-8424, 1091-6490},
	url = {https://pnas.org/doi/full/10.1073/pnas.1719115115},
	doi = {10.1073/pnas.1719115115},
	abstract = {Significance
            Protein structure refinement by direct global energy optimization has been a longstanding challenge in computational structural biology due to limitations in both energy function accuracy and conformational sampling. This manuscript demonstrates that with recent advances in both areas, refinement can significantly improve protein comparative models based on structures of distant homologues.
          , 
            Proteins fold to their lowest free-energy structures, and hence the most straightforward way to increase the accuracy of a partially incorrect protein structure model is to search for the lowest-energy nearby structure. This direct approach has met with little success for two reasons: first, energy function inaccuracies can lead to false energy minima, resulting in model degradation rather than improvement; and second, even with an accurate energy function, the search problem is formidable because the energy only drops considerably in the immediate vicinity of the global minimum, and there are a very large number of degrees of freedom. Here we describe a large-scale energy optimization-based refinement method that incorporates advances in both search and energy function accuracy that can substantially improve the accuracy of low-resolution homology models. The method refined low-resolution homology models into correct folds for 50 of 84 diverse protein families and generated improved models in recent blind structure prediction experiments. Analyses of the basis for these improvements reveal contributions from both the improvements in conformational sampling techniques and the energy function.},
	language = {en},
	number = {12},
	urldate = {2022-07-08},
	journal = {Proceedings of the National Academy of Sciences},
	author = {Park, Hahnbeom and Ovchinnikov, Sergey and Kim, David E. and DiMaio, Frank and Baker, David},
	month = mar,
	year = {2018},
	note = {52 citations (Semantic Scholar/DOI) [2022-07-08]},
	pages = {3054--3059},
}

@article{li_monte_1987,
	title = {Monte {Carlo}-minimization approach to the multiple-minima problem in protein folding.},
	volume = {84},
	issn = {0027-8424, 1091-6490},
	url = {https://pnas.org/doi/full/10.1073/pnas.84.19.6611},
	doi = {10.1073/pnas.84.19.6611},
	abstract = {A Monte Carlo-minimization method has been developed to overcome the multiple-minima problem. The Metropolis Monte Carlo sampling, assisted by energy minimization, surmounts intervening barriers in moving through successive discrete local minima in the multidimensional energy surface. The method has located the lowest-energy minimum thus far reported for the brain pentapeptide [Met5]enkephalin in the absence of water. Presumably it is the global minimumenergy structure. This supports the concept that protein folding may be a Markov process. In the presence of water, the molecules appear to exist as an ensemble of different conformations.},
	language = {en},
	number = {19},
	urldate = {2022-07-08},
	journal = {Proceedings of the National Academy of Sciences},
	author = {Li, Z and Scheraga, H A},
	month = oct,
	year = {1987},
	note = {1157 citations (Semantic Scholar/DOI) [2022-07-08]},
	pages = {6611--6615},
}

@article{alquraishi_alphafold_2019,
	title = {{AlphaFold} at {CASP13}},
	volume = {35},
	issn = {1367-4803, 1460-2059},
	url = {https://academic.oup.com/bioinformatics/article/35/22/4862/5497249},
	doi = {10.1093/bioinformatics/btz422},
	abstract = {Summary: Computational prediction of protein structure from sequence is broadly viewed as a foundational problem of biochemistry and one of the most difﬁcult challenges in bioinformatics. Once every two years the Critical Assessment of protein Structure Prediction (CASP) experiments are held to assess the state of the art in the ﬁeld in a blind fashion, by presenting predictor groups with protein sequences whose structures have been solved but have not yet been made publicly available. The ﬁrst CASP was organized in 1994, and the latest, CASP13, took place last December, when for the ﬁrst time the industrial laboratory DeepMind entered the competition. DeepMind’s entry, AlphaFold, placed ﬁrst in the Free Modeling (FM) category, which assesses methods on their ability to predict novel protein folds (the Zhang group placed ﬁrst in the Template-Based Modeling (TBM) category, which assess methods on predicting proteins whose folds are related to ones already in the Protein Data Bank.) DeepMind’s success generated signiﬁcant public interest. Their approach builds on two ideas developed in the academic community during the preceding decade: (i) the use of co-evolutionary analysis to map residue co-variation in protein sequence to physical contact in protein structure, and (ii) the application of deep neural networks to robustly identify patterns in protein sequence and co-evolutionary couplings and convert them into contact maps. In this Letter, we contextualize the signiﬁcance of DeepMind’s entry within the broader history of CASP, relate AlphaFold’s methodological advances to prior work, and speculate on the future of this important problem.},
	language = {en},
	number = {22},
	urldate = {2022-07-08},
	journal = {Bioinformatics},
	author = {AlQuraishi, Mohammed},
	editor = {Valencia, Alfonso},
	month = nov,
	year = {2019},
	note = {138 citations (Semantic Scholar/DOI) [2022-07-08]},
	pages = {4862--4865},
}

@article{cabeza_de_vaca_enhanced_2018,
	title = {Enhanced {Monte} {Carlo} {Methods} for {Modeling} {Proteins} {Including} {Computation} of {Absolute} {Free} {Energies} of {Binding}},
	volume = {14},
	issn = {1549-9618, 1549-9626},
	url = {https://pubs.acs.org/doi/10.1021/acs.jctc.8b00031},
	doi = {10.1021/acs.jctc.8b00031},
	abstract = {The generation of a complete ensemble of geometrical conﬁgurations is required to obtain reliable estimations of absolute binding free energies by alchemical free energy methods. Molecular dynamics (MD) is the most popular sampling method, but the representation of large biomolecular systems may be incomplete owing to energetic barriers that impede eﬃcient sampling of the conﬁgurational space. Monte Carlo (MC) methods can possibly overcome this issue by adapting the attempted movement sizes to facilitate transitions between alternative local-energy minima. In this study, we present an MC statistical mechanics algorithm to explore the protein-ligand conformational space with emphasis on the motions of the protein backbone and side chains. The parameters for each MC move type were optimized to better reproduce conformational distributions of 18 dipeptides and the well-studied T4-lysozyme L99A protein. Next, the performance of the improved MC algorithms was evaluated by computing absolute free energies of binding for L99A lysozyme with benzene and seven analogs. Results for benzene with L99A lysozyme from MD and the optimized MC protocol were found to agree within 0.6 kcal/mol, while a mean unsigned error of 1.2 kcal/mol between MC results and experiment was obtained for the seven benzene analogs. Signiﬁcant advantages in computation speed are also reported with MC over MD for similar extents of conﬁgurational sampling.},
	language = {en},
	number = {6},
	urldate = {2022-07-08},
	journal = {Journal of Chemical Theory and Computation},
	author = {Cabeza de Vaca, Israel and Qian, Yue and Vilseck, Jonah Z. and Tirado-Rives, Julian and Jorgensen, William L.},
	month = jun,
	year = {2018},
	note = {21 citations (Semantic Scholar/DOI) [2022-07-08]},
	pages = {3279--3288},
}

@article{manohar_comparing_2015,
	title = {Comparing {Stochastic} and {Deterministic} {Computing}},
	volume = {14},
	issn = {1556-6056},
	url = {http://ieeexplore.ieee.org/document/7059235/},
	doi = {10.1109/LCA.2015.2412553},
	abstract = {Technology scaling has raised the specter of myriads of cheap, but unreliable and/or stochastic devices that must be creatively combined to create a reliable computing system. This has renewed the interest in computing that exploits stochasticity—embracing, not combating the device physics. If a stochastic representation is used to implement a programmable general-purpose architecture akin to CPUs, GPUs, or FPGAs, the preponderance of evidence indicates that most of the system energy will be expended in communication and storage as opposed to computation. This paper presents an analytical treatment of the beneﬁts and drawbacks of adopting a stochastic approach by examining the cost of representing a value. We show both scaling laws and costs for low precision representations. We also analyze the cost of multiplication implemented using stochastic versus deterministic approaches, since multiplication is the prototypical inexpensive stochastic operation. We show that the deterministic approach compares favorably to the stochastic approach when holding precision and reliability constant.},
	language = {en},
	number = {2},
	urldate = {2022-07-08},
	journal = {IEEE Computer Architecture Letters},
	author = {Manohar, Rajit},
	month = jul,
	year = {2015},
	note = {17 citations (Semantic Scholar/DOI) [2022-07-08]},
	pages = {119--122},
}

@inproceedings{bhattacherjee_network_2019,
	address = {Orlando Florida},
	title = {Network topology design at 27,000 km/hour},
	isbn = {978-1-4503-6998-5},
	url = {https://dl.acm.org/doi/10.1145/3359989.3365407},
	doi = {10.1145/3359989.3365407},
	abstract = {Upstart space companies are actively developing massive constellations of low-flying satellites to provide global Internet service. We examine the problem of designing the inter-satellite network for low latency and high capacity. We posit that the high density of these new constellations and the high-velocity nature of such systems render traditional approaches for network design ineffective, motivating new methods specialized for this problem setting.},
	language = {en},
	urldate = {2022-07-08},
	booktitle = {Proceedings of the 15th {International} {Conference} on {Emerging} {Networking} {Experiments} {And} {Technologies}},
	publisher = {ACM},
	author = {Bhattacherjee, Debopam and Singla, Ankit},
	month = dec,
	year = {2019},
	note = {39 citations (Semantic Scholar/DOI) [2022-07-08]},
	pages = {341--354},
}

@article{favrin_monte_2001,
	title = {Monte {Carlo} update for chain molecules: {Biased} {Gaussian} steps in torsional space},
	volume = {114},
	issn = {0021-9606, 1089-7690},
	shorttitle = {Monte {Carlo} update for chain molecules},
	url = {http://aip.scitation.org/doi/10.1063/1.1364637},
	doi = {10.1063/1.1364637},
	language = {en},
	number = {18},
	urldate = {2022-07-08},
	journal = {The Journal of Chemical Physics},
	author = {Favrin, Giorgio and Irbäck, Anders and Sjunnesson, Fredrik},
	month = may,
	year = {2001},
	note = {104 citations (Semantic Scholar/DOI) [2022-07-08]},
	pages = {8154--8158},
}

@article{goemans_improved_1995,
	title = {Improved approximation algorithms for maximum cut and satisfiability problems using semidefinite programming},
	volume = {42},
	issn = {0004-5411, 1557-735X},
	url = {https://dl.acm.org/doi/10.1145/227683.227684},
	doi = {10.1145/227683.227684},
	abstract = {We present randomized approximation algorithms for the maximum cut (MAX CUT) and maximum 2-satisfiability (MAX 2SAT) problems that always deliver solutions of expected value at least .87856 times the optimal value. These algorithms use a simple and elegant technique that randomly rounds the solution to a nonlinear programming relaxation. This relaxation can be interpreted both as a semidefinite program and as an eigenvalue minimization problem. The best previously known approximation algorithms for these problems had perfc{\textasciitilde}rmance guarantees of {\textasciitilde} for MAX CUT and {\textasciitilde} for MAX 2SAT. Slight extensions of our analysis lead to a .79607-approximation algorithm for the maximum directed cut problem (MAX DICUT) and a .758-approximation algorithm for MAX SAT, where the best previously known approxim ation algorithms had performance guarantees of {\textasciitilde} and {\textasciitilde}, respectively. Our algorithm gives the first substantial progress in approximating MAX CUT in nearly twenty years, and represents the first use of :semidefinite programming in the design of approximation algorithms.},
	language = {en},
	number = {6},
	urldate = {2022-07-08},
	journal = {Journal of the ACM},
	author = {Goemans, Michel X. and Williamson, David P.},
	month = nov,
	year = {1995},
	note = {3399 citations (Semantic Scholar/DOI) [2022-07-08]},
	pages = {1115--1145},
}

@article{liu_limited_1989,
	title = {On the limited memory {BFGS} method for large scale optimization},
	volume = {45},
	issn = {0025-5610, 1436-4646},
	url = {http://link.springer.com/10.1007/BF01589116},
	doi = {10.1007/BF01589116},
	abstract = {We study the numerical performance of a limited memory quasi-Newton method for large scale optimization, which we call the L-BFGS method. We compare its performance with that of the method developed by Buckley and LeNir (1985), which combines cycles of BFGS steps and conjugate direction steps. Our numerical tests indicate that the L-BFGS method is faster than the method of Buckleyand LeNir, and is better able to use additional storage to accelerate convergence. We show that the L-BFGS method can be greatly accelerated by means of a simple scaling. We then compare the L-BFGS method with the partitioned quasi-Newton method of Griewank and Toint (1982a). The results show that, for some problems, the partitioned quasi-Newton method is clearly superior to the L-BFGS method. However we find that for other problems the L-BFGS method is very competitive due to its low iteration cost. We also study the convergence properties of the L-BFGS method, and prove global convergence On uniformly convex problems.},
	language = {en},
	number = {1-3},
	urldate = {2022-07-08},
	journal = {Mathematical Programming},
	author = {Liu, Dong C. and Nocedal, Jorge},
	month = aug,
	year = {1989},
	note = {5871 citations (Semantic Scholar/DOI) [2022-07-08]},
	pages = {503--528},
}

@article{dill_protein_2008,
	title = {The {Protein} {Folding} {Problem}},
	abstract = {The “protein folding problem” consists of three closely related puzzles: (a) What is the folding code? (b) What is the folding mechanism? (c) Can we predict the native structure of a protein from its amino acid sequence? Once regarded as a grand challenge, protein folding has seen great progress in recent years. Now, foldable proteins and nonbiological polymers are being designed routinely and moving toward successful applications. The structures of small proteins are now often well predicted by computer methods. And, there is now a testable explanation for how a protein can fold so quickly: A protein solves its large global optimization problem as a series of smaller local optimization problems, growing and assembling the native structure from peptide fragments, local structures first.},
	language = {en},
	author = {Dill, Ken A and Ozkan, S Banu and Shell, M Scott and Weikl, Thomas R},
	year = {2008},
	pages = {32},
}

@article{keedy_role_2012,
	title = {The {Role} of {Local} {Backrub} {Motions} in {Evolved} and {Designed} {Mutations}},
	volume = {8},
	issn = {1553-7358},
	url = {https://dx.plos.org/10.1371/journal.pcbi.1002629},
	doi = {10.1371/journal.pcbi.1002629},
	abstract = {Amino acid substitutions in protein structures often require subtle backbone adjustments that are difficult to model in atomic detail. An improved ability to predict realistic backbone changes in response to engineered mutations would be of great utility for the blossoming field of rational protein design. One model that has recently grown in acceptance is the backrub motion, a low-energy dipeptide rotation with single-peptide counter-rotations, that is coupled to dynamic twostate sidechain rotamer jumps, as evidenced by alternate conformations in very high-resolution crystal structures. It has been speculated that backrubs may facilitate sequence changes equally well as rotamer changes. However, backrubinduced shifts and experimental uncertainty are of similar magnitude for backbone atoms in even high-resolution structures, so comparison of wildtype-vs.-mutant crystal structure pairs is not sufficient to directly link backrubs to mutations. In this study, we use two alternative approaches that bypass this limitation. First, we use a quality-filtered structure database to aggregate many examples for precisely defined motifs with single amino acid differences, and find that the effectively amplified backbone differences closely resemble backrubs. Second, we directly apply a provablyaccurate, backrub-enabled protein design algorithm to idealized versions of these motifs, and discover that the lowestenergy computed models match the average-coordinate experimental structures. These results support the hypothesis that backrubs participate in natural protein evolution and validate their continued use for design of synthetic proteins.},
	language = {en},
	number = {8},
	urldate = {2022-07-08},
	journal = {PLoS Computational Biology},
	author = {Keedy, Daniel A. and Georgiev, Ivelin and Triplett, Edward B. and Donald, Bruce R. and Richardson, David C. and Richardson, Jane S.},
	editor = {Ofran, Yanay},
	month = aug,
	year = {2012},
	note = {31 citations (Semantic Scholar/DOI) [2022-07-08]},
	pages = {e1002629},
}

@article{li_visualizing_nodate,
	title = {Visualizing the {Loss} {Landscape} of {Neural} {Nets}},
	abstract = {Neural network training relies on our ability to ﬁnd “good” minimizers of highly non-convex loss functions. It is well-known that certain network architecture designs (e.g., skip connections) produce loss functions that train easier, and wellchosen training parameters (batch size, learning rate, optimizer) produce minimizers that generalize better. However, the reasons for these differences, and their effect on the underlying loss landscape, are not well understood. In this paper, we explore the structure of neural loss functions, and the effect of loss landscapes on generalization, using a range of visualization methods. First, we introduce a simple “ﬁlter normalization” method that helps us visualize loss function curvature and make meaningful side-by-side comparisons between loss functions. Then, using a variety of visualizations, we explore how network architecture affects the loss landscape, and how training parameters affect the shape of minimizers.},
	language = {en},
	author = {Li, Hao and Xu, Zheng and Taylor, Gavin and Studer, Christoph and Goldstein, Tom},
	pages = {11},
}

@article{paquet_molecular_2015,
	title = {Molecular {Dynamics}, {Monte} {Carlo} {Simulations}, and {Langevin} {Dynamics}: {A} {Computational} {Review}},
	volume = {2015},
	issn = {2314-6133, 2314-6141},
	shorttitle = {Molecular {Dynamics}, {Monte} {Carlo} {Simulations}, and {Langevin} {Dynamics}},
	url = {http://www.hindawi.com/journals/bmri/2015/183918/},
	doi = {10.1155/2015/183918},
	abstract = {Macromolecular structures, such as neuraminidases, hemagglutinins, and monoclonal antibodies, are not rigid entities. Rather, they are characterised by their flexibility, which is the result of the interaction and collective motion of their constituent atoms. This conformational diversity has a significant impact on their physicochemical and biological properties. Among these are their structural stability, the transport of ions through the M2 channel, drug resistance, macromolecular docking, binding energy, and rational epitope design. To assess these properties and to calculate the associated thermodynamical observables, the conformational space must be efficiently sampled and the dynamic of the constituent atoms must be simulated. This paper presents algorithms and techniques that address the abovementioned issues. To this end, a computational review of molecular dynamics, Monte Carlo simulations, Langevin dynamics, and free energy calculation is presented. The exposition is made from first principles to promote a better understanding of the potentialities, limitations, applications, and interrelations of these computational methods.},
	language = {en},
	urldate = {2022-07-08},
	journal = {BioMed Research International},
	author = {Paquet, Eric and Viktor, Herna L.},
	year = {2015},
	pages = {1--18},
}

@article{mccammon_dynamics_1977,
	title = {Dynamics of folded proteins},
	volume = {267},
	issn = {0028-0836, 1476-4687},
	url = {http://www.nature.com/articles/267585a0},
	doi = {10.1038/267585a0},
	language = {en},
	number = {5612},
	urldate = {2022-07-08},
	journal = {Nature},
	author = {McCammon, J. Andrew and Gelin, Bruce R. and Karplus, Martin},
	month = jun,
	year = {1977},
	pages = {585--590},
}

@article{pervaiz_weighted_2019,
	title = {Weighted \$p\$ -{Bits} for {FPGA} {Implementation} of {Probabilistic} {Circuits}},
	volume = {30},
	issn = {2162-237X, 2162-2388},
	url = {https://ieeexplore.ieee.org/document/8515266/},
	doi = {10.1109/TNNLS.2018.2874565},
	abstract = {Probabilistic spin logic is a recently proposed computing paradigm based on unstable stochastic units called probabilistic bits ( p-bits) that can be correlated to form probabilistic circuits (p-circuits). These p-circuits can be used to solve the problems of optimization, inference, and implement precise Boolean functions in an “inverted” mode, where a given Boolean circuit can operate in reverse to ﬁnd the input combinations that are consistent with a given output. In this brief, we present a scalable ﬁeld-programmable gate array implementation of such invertible p-circuits. We implement a “weighted” p-bit that combines stochastic units with localized memory structures. We also present a generalized tile of weighted p-bits to which a large class of problems beyond invertible Boolean logic can be mapped and how invertibility can be applied to interesting problems such as the NP-complete subset sum problem by solving a small instance of this problem in hardware.},
	language = {en},
	number = {6},
	urldate = {2022-07-08},
	journal = {IEEE Transactions on Neural Networks and Learning Systems},
	author = {Pervaiz, Ahmed Zeeshan and Sutton, Brian M. and Ghantasala, Lakshmi Anirudh and Camsari, Kerem Y.},
	month = jun,
	year = {2019},
	note = {25 citations (Semantic Scholar/DOI) [2022-07-08]},
	pages = {1920--1926},
}

@article{biswas_simulated_1986,
	title = {Simulated annealing of silicon atom clusters in {Langevin} molecular dynamics},
	volume = {34},
	issn = {0163-1829},
	url = {https://link.aps.org/doi/10.1103/PhysRevB.34.895},
	doi = {10.1103/PhysRevB.34.895},
	language = {en},
	number = {2},
	urldate = {2022-07-08},
	journal = {Physical Review B},
	author = {Biswas, R. and Hamann, D. R.},
	month = jul,
	year = {1986},
	note = {101 citations (Semantic Scholar/DOI) [2022-07-08]},
	pages = {895--901},
}

@article{karplus_molecular_2002,
	title = {Molecular {Dynamics} {Simulations} of {Biomolecules}},
	volume = {35},
	issn = {0001-4842, 1520-4898},
	url = {https://pubs.acs.org/doi/10.1021/ar020082r},
	doi = {10.1021/ar020082r},
	language = {en},
	number = {6},
	urldate = {2022-07-08},
	journal = {Accounts of Chemical Research},
	author = {Karplus, Martin},
	month = jun,
	year = {2002},
	note = {9 citations (Semantic Scholar/DOI) [2022-07-08]},
	pages = {321--323},
}

@article{maantay_mapping_2002,
	title = {Mapping environmental injustices: pitfalls and potential of geographic information systems in assessing environmental health and equity.},
	volume = {110},
	issn = {0091-6765, 1552-9924},
	shorttitle = {Mapping environmental injustices},
	url = {https://ehp.niehs.nih.gov/doi/10.1289/ehp.02110s2161},
	doi = {10.1289/ehp.02110s2161},
	language = {en},
	number = {suppl 2},
	urldate = {2022-07-08},
	journal = {Environmental Health Perspectives},
	author = {Maantay, Juliana},
	month = apr,
	year = {2002},
	note = {272 citations (Semantic Scholar/DOI) [2022-07-08]},
	pages = {161--171},
}

@inproceedings{he_deep_2016,
	address = {Las Vegas, NV, USA},
	title = {Deep {Residual} {Learning} for {Image} {Recognition}},
	isbn = {978-1-4673-8851-1},
	url = {http://ieeexplore.ieee.org/document/7780459/},
	doi = {10.1109/CVPR.2016.90},
	abstract = {Deeper neural networks are more difﬁcult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers—8× deeper than VGG nets [40] but still having lower complexity. An ensemble of these residual nets achieves 3.57\% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classiﬁcation task. We also present analysis on CIFAR-10 with 100 and 1000 layers.},
	language = {en},
	urldate = {2022-07-08},
	booktitle = {2016 {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	publisher = {IEEE},
	author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
	month = jun,
	year = {2016},
	note = {9999 citations (Semantic Scholar/DOI) [2022-07-08]},
	pages = {770--778},
}

@article{kong_dietcam_2012,
	title = {{DietCam}: {Automatic} dietary assessment with mobile camera phones},
	volume = {8},
	issn = {15741192},
	shorttitle = {{DietCam}},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S1574119211001131},
	doi = {10.1016/j.pmcj.2011.07.003},
	abstract = {Obesity has become a severe health problem in developed countries, and a healthy food intake has been recognized as the key factor for obesity prevention. This paper presents a mobile phone based system, DietCam, to help assess food intakes with few human interventions. DietCam only requires users to take three images or a short video around the meal, then it will do the rest. The experiments of DietCam in real restaurants verify the possibility of food recognition with vision techniques.},
	language = {en},
	number = {1},
	urldate = {2022-07-08},
	journal = {Pervasive and Mobile Computing},
	author = {Kong, Fanyu and Tan, Jindong},
	month = feb,
	year = {2012},
	note = {198 citations (Semantic Scholar/DOI) [2022-07-08]},
	pages = {147--163},
}

@article{biamonte_quantum_2017,
	title = {Quantum machine learning},
	volume = {549},
	issn = {0028-0836, 1476-4687},
	url = {http://www.nature.com/articles/nature23474},
	doi = {10.1038/nature23474},
	language = {en},
	number = {7671},
	urldate = {2022-07-08},
	journal = {Nature},
	author = {Biamonte, Jacob and Wittek, Peter and Pancotti, Nicola and Rebentrost, Patrick and Wiebe, Nathan and Lloyd, Seth},
	month = sep,
	year = {2017},
	note = {923 citations (Semantic Scholar/DOI) [2022-07-08]},
	pages = {195--202},
}

@book{koller_probabilistic_2009,
	address = {Cambridge, MA},
	series = {Adaptive computation and machine learning},
	title = {Probabilistic graphical models: principles and techniques},
	isbn = {978-0-262-01319-2},
	shorttitle = {Probabilistic graphical models},
	language = {en},
	publisher = {MIT Press},
	author = {Koller, Daphne and Friedman, Nir},
	year = {2009},
	keywords = {Bayesian statistical decision theory, Graphic methods, Graphical modeling (Statistics)},
}

@inproceedings{ortega-zamorano_high_2014,
	address = {Orlando, FL, USA},
	title = {High precision {FPGA} implementation of neural network activation functions},
	isbn = {978-1-4799-4485-9},
	url = {http://ieeexplore.ieee.org/document/7008986/},
	doi = {10.1109/INTELES.2014.7008986},
	abstract = {The efﬁcient implementation of artiﬁcial neural networks in FPGA boards requires tackling several issues that strongly affect the ﬁnal result. One of these issues is the computation of the neuron’s activation function. In this work, a detailed analysis of the FPGA implementations of the Sigmoid and Exponential functions is carried out, in a approach combining a lookup table with a linear interpolation procedure. Further, to optimize board resources utilization, a time division multiplexing of the multiplier attached to the neurons was used. The results are evaluated in terms of the absolute and relative errors obtained and also through measuring a quality factor and the resource utilization, showing a clear improvement in relationship to previously published works.},
	language = {en},
	urldate = {2022-07-08},
	booktitle = {2014 {IEEE} {Symposium} on {Intelligent} {Embedded} {Systems} ({IES})},
	publisher = {IEEE},
	author = {Ortega-Zamorano, Francisco and Jerez, Jose M. and Juarez, Gustavo and Perez, Jorge O. and Franco, Leonardo},
	month = dec,
	year = {2014},
	note = {23 citations (Semantic Scholar/DOI) [2022-07-08]},
	pages = {55--60},
}

@article{fang_end--end_2019,
	title = {An {End}-to-{End} {Image}-{Based} {Automatic} {Food} {Energy} {Estimation} {Technique} {Based} on {Learned} {Energy} {Distribution} {Images}: {Protocol} and {Methodology}},
	volume = {11},
	issn = {2072-6643},
	shorttitle = {An {End}-to-{End} {Image}-{Based} {Automatic} {Food} {Energy} {Estimation} {Technique} {Based} on {Learned} {Energy} {Distribution} {Images}},
	url = {https://www.mdpi.com/2072-6643/11/4/877},
	doi = {10.3390/nu11040877},
	abstract = {Obtaining accurate food portion estimation automatically is challenging since the processes of food preparation and consumption impose large variations on food shapes and appearances. The aim of this paper was to estimate the food energy numeric value from eating occasion images captured using the mobile food record. To model the characteristics of food energy distribution in an eating scene, a new concept of “food energy distribution” was introduced. The mapping of a food image to its energy distribution was learned using Generative Adversarial Network (GAN) architecture. Food energy was estimated from the image based on the energy distribution image predicted by GAN. The proposed method was validated on a set of food images collected from a 7-day dietary study among 45 community-dwelling men and women between 21–65 years. The ground truth food energy was obtained from pre-weighed foods provided to the participants. The predicted food energy values using our end-to-end energy estimation system was compared to the ground truth food energy values. The average error in the estimated energy was 209 kcal per eating occasion. These results show promise for improving accuracy of image-based dietary assessment.},
	language = {en},
	number = {4},
	urldate = {2022-07-08},
	journal = {Nutrients},
	author = {{Fang} and {Shao} and {Kerr} and {Boushey} and {Zhu}},
	month = apr,
	year = {2019},
	note = {27 citations (Semantic Scholar/DOI) [2022-07-08]},
	pages = {877},
}

@article{nikonov_benchmarking_2019,
	title = {Benchmarking {Delay} and {Energy} of {Neural} {Inference} {Circuits}},
	volume = {5},
	issn = {2329-9231},
	url = {https://ieeexplore.ieee.org/document/8915808/},
	doi = {10.1109/JXCDC.2019.2956112},
	abstract = {Neural network circuits and architectures are currently under active research for applications to artiﬁcial intelligence and machine learning. Their physical performance metrics (area, time, and energy) are estimated. Various types of neural networks (artiﬁcial, cellular, spiking, and oscillator) are implemented with multiple CMOS and beyond-CMOS (spintronic, ferroelectric, and resistive memory) devices. A consistent and transparent methodology is proposed and used to benchmark this comprehensive set of options across several application cases. Promising architecture/device combinations are identiﬁed.},
	language = {en},
	number = {2},
	urldate = {2022-07-08},
	journal = {IEEE Journal on Exploratory Solid-State Computational Devices and Circuits},
	author = {Nikonov, Dmitri E. and Young, Ian A.},
	month = dec,
	year = {2019},
	note = {10 citations (Semantic Scholar/DOI) [2022-07-08]},
	pages = {75--84},
}

@article{berg_metropolis_2003,
	title = {Metropolis {Importance} {Sampling} for {Rugged} {Dynamical} {Variables}},
	volume = {90},
	issn = {0031-9007, 1079-7114},
	url = {https://link.aps.org/doi/10.1103/PhysRevLett.90.180601},
	doi = {10.1103/PhysRevLett.90.180601},
	language = {en},
	number = {18},
	urldate = {2022-07-08},
	journal = {Physical Review Letters},
	author = {Berg, Bernd A.},
	month = may,
	year = {2003},
	note = {12 citations (Semantic Scholar/DOI) [2022-07-08]},
	pages = {180601},
}

@article{bennett_mass_1975,
	title = {Mass tensor molecular dynamics},
	volume = {19},
	issn = {00219991},
	url = {https://linkinghub.elsevier.com/retrieve/pii/0021999175900777},
	doi = {10.1016/0021-9991(75)90077-7},
	language = {en},
	number = {3},
	urldate = {2022-07-08},
	journal = {Journal of Computational Physics},
	author = {Bennett, Charles H.},
	month = nov,
	year = {1975},
	note = {92 citations (Semantic Scholar/DOI) [2022-07-08]},
	pages = {267--279},
}

@article{bernardi_enhanced_2015,
	title = {Enhanced sampling techniques in molecular dynamics simulations of biological systems},
	volume = {1850},
	issn = {03044165},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0304416514003559},
	doi = {10.1016/j.bbagen.2014.10.019},
	abstract = {Background: Molecular dynamics has emerged as an important research methodology covering systems to the level of millions of atoms. However, insufﬁcient sampling often limits its application. The limitation is due to rough energy landscapes, with many local minima separated by high-energy barriers, which govern the biomolecular motion.},
	language = {en},
	number = {5},
	urldate = {2022-07-08},
	journal = {Biochimica et Biophysica Acta (BBA) - General Subjects},
	author = {Bernardi, Rafael C. and Melo, Marcelo C.R. and Schulten, Klaus},
	month = may,
	year = {2015},
	note = {406 citations (Semantic Scholar/DOI) [2022-07-08]},
	pages = {872--877},
}

@article{sakaguchi_boltzmann_2016,
	title = {Boltzmann {Sampling} by {Degenerate} {Optical} {Parametric} {Oscillator} {Network} for {Structure}-{Based} {Virtual} {Screening}},
	volume = {18},
	issn = {1099-4300},
	url = {http://www.mdpi.com/1099-4300/18/10/365},
	doi = {10.3390/e18100365},
	abstract = {A structure-based lead optimization procedure is an essential step to ﬁnding appropriate ligand molecules binding to a target protein structure in order to identify drug candidates. This procedure takes a known structure of a protein-ligand complex as input, and structurally similar compounds with the query ligand are designed in consideration with all possible combinations of atomic species. This task is, however, computationally hard since such combinatorial optimization problems belong to the non-deterministic nonpolynomial-time hard (NP-hard) class. In this paper, we propose the structure-based lead generation and optimization procedures by a degenerate optical parametric oscillator (DOPO) network. Results of numerical simulation demonstrate that the DOPO network efﬁciently identiﬁes a set of appropriate ligand molecules according to the Boltzmann sampling law.},
	language = {en},
	number = {10},
	urldate = {2022-07-08},
	journal = {Entropy},
	author = {Sakaguchi, Hiromasa and Ogata, Koji and Isomura, Tetsu and Utsunomiya, Shoko and Yamamoto, Yoshihisa and Aihara, Kazuyuki},
	month = oct,
	year = {2016},
	note = {21 citations (Semantic Scholar/DOI) [2022-07-08]},
	pages = {365},
}

@article{boomsma_phaistos_2013,
	title = {{PHAISTOS}: {A} framework for {Markov} chain {Monte} {Carlo} simulation and inference of protein structure: {Software} {News} and {Updates}},
	volume = {34},
	issn = {01928651},
	shorttitle = {{PHAISTOS}},
	url = {https://onlinelibrary.wiley.com/doi/10.1002/jcc.23292},
	doi = {10.1002/jcc.23292},
	language = {en},
	number = {19},
	urldate = {2022-07-08},
	journal = {Journal of Computational Chemistry},
	author = {Boomsma, Wouter and Frellsen, Jes and Harder, Tim and Bottaro, Sandro and Johansson, Kristoffer E. and Tian, Pengfei and Stovgaard, Kasper and Andreetta, Christian and Olsson, Simon and Valentin, Jan B. and Antonov, Lubomir D. and Christensen, Anders S. and Borg, Mikael and Jensen, Jan H. and Lindorff-Larsen, Kresten and Ferkinghoff-Borg, Jesper and Hamelryck, Thomas},
	month = jul,
	year = {2013},
	note = {43 citations (Semantic Scholar/DOI) [2022-07-08]},
	pages = {1697--1705},
}

@article{buonfiglio_protein_2015,
	title = {Protein {Flexibility} in {Drug} {Discovery}: {From} {Theory} to {Computation}},
	volume = {10},
	issn = {18607179},
	shorttitle = {Protein {Flexibility} in {Drug} {Discovery}},
	url = {https://onlinelibrary.wiley.com/doi/10.1002/cmdc.201500086},
	doi = {10.1002/cmdc.201500086},
	language = {en},
	number = {7},
	urldate = {2022-07-08},
	journal = {ChemMedChem},
	author = {Buonfiglio, Rosa and Recanatini, Maurizio and Masetti, Matteo},
	month = jul,
	year = {2015},
	note = {44 citations (Semantic Scholar/DOI) [2022-07-08]},
	pages = {1141--1148},
}

@article{botu_adaptive_2015,
	title = {Adaptive machine learning framework to accelerate \textit{ab initio} molecular dynamics},
	volume = {115},
	issn = {00207608},
	url = {https://onlinelibrary.wiley.com/doi/10.1002/qua.24836},
	doi = {10.1002/qua.24836},
	language = {en},
	number = {16},
	urldate = {2022-07-08},
	journal = {International Journal of Quantum Chemistry},
	author = {Botu, Venkatesh and Ramprasad, Rampi},
	month = aug,
	year = {2015},
	note = {274 citations (Semantic Scholar/DOI) [2022-07-08]},
	pages = {1074--1083},
}

@incollection{espina_structure-based_2012,
	address = {Totowa, NJ},
	title = {Structure-{Based} {Functional} {Design} of {Drugs}: {From} {Target} to {Lead} {Compound}},
	volume = {823},
	isbn = {978-1-60327-215-5 978-1-60327-216-2},
	shorttitle = {Structure-{Based} {Functional} {Design} of {Drugs}},
	url = {http://link.springer.com/10.1007/978-1-60327-216-2_23},
	abstract = {Proteomic and genomic discoveries have identified vast numbers of new drug targets for investigation. In the quest to discover drugs that modulate the function of these targets, identification of small-molecule drug leads is one of the earliest steps. Structure-based drug design has emerged as a valuable, inexpensive, and rapid computational resource that identifies lead compounds that are complementary to the structure of the target. Leads identified through this process are biologically evaluated and “hit compounds” with affinity and activity are further optimized. This chapter introduces the process of structure-based drug design, including preparation of the ligand database, preparation of the target structure, docking and scoring, and evaluation.},
	language = {en},
	urldate = {2022-07-08},
	booktitle = {Molecular {Profiling}},
	publisher = {Humana Press},
	author = {Anderson, Amy C.},
	editor = {Espina, Virginia and Liotta, Lance A.},
	year = {2012},
	doi = {10.1007/978-1-60327-216-2_23},
	note = {Series Title: Methods in Molecular Biology},
	pages = {359--366},
}

@article{li_molecular_2015,
	title = {Molecular {Dynamics} with {On}-the-{Fly} {Machine} {Learning} of {Quantum}-{Mechanical} {Forces}},
	volume = {114},
	issn = {0031-9007, 1079-7114},
	url = {https://link.aps.org/doi/10.1103/PhysRevLett.114.096405},
	doi = {10.1103/PhysRevLett.114.096405},
	language = {en},
	number = {9},
	urldate = {2022-07-08},
	journal = {Physical Review Letters},
	author = {Li, Zhenwei and Kermode, James R. and De Vita, Alessandro},
	month = mar,
	year = {2015},
	note = {360 citations (Semantic Scholar/DOI) [2022-07-08]},
	pages = {096405},
}

@article{schafer-nolte_tracking_2014,
	title = {Tracking {Temperature}-{Dependent} {Relaxation} {Times} of {Ferritin} {Nanomagnets} with a {Wideband} {Quantum} {Spectrometer}},
	volume = {113},
	issn = {0031-9007, 1079-7114},
	url = {https://link.aps.org/doi/10.1103/PhysRevLett.113.217204},
	doi = {10.1103/PhysRevLett.113.217204},
	language = {en},
	number = {21},
	urldate = {2022-07-08},
	journal = {Physical Review Letters},
	author = {Schäfer-Nolte, Eike and Schlipf, Lukas and Ternes, Markus and Reinhard, Friedemann and Kern, Klaus and Wrachtrup, Jörg},
	month = nov,
	year = {2014},
	note = {40 citations (Semantic Scholar/DOI) [2022-07-08]},
	pages = {217204},
}

@article{liu_biological_2018,
	title = {Biological and functional relevance of {CASP} predictions},
	volume = {86},
	issn = {08873585},
	url = {https://onlinelibrary.wiley.com/doi/10.1002/prot.25396},
	doi = {10.1002/prot.25396},
	abstract = {Our goal is to answer the question: compared with experimental structures, how useful are predicted models for functional annotation? We assessed the functional utility of predicted models by comparing the performances of a suite of methods for functional characterization on the predictions and the experimental structures. We identified 28 sites in 25 protein targets to perform functional assessment. These 28 sites included nine sites with known ligand binding (holo-sites), nine sites that are expected or suggested by experimental authors for small molecule binding (aposites), and Ten sites containing important motifs, loops, or key residues with important diseaseassociated mutations. We evaluated the utility of the predictions by comparing their microenvironments to the experimental structures. Overall structural quality correlates with functional utility. However, the best-ranked predictions (global) may not have the best functional quality (local). Our assessment provides an ability to discriminate between predictions with high structural quality. When assessing ligand-binding sites, most prediction methods have higher performance on aposites than holo-sites. Some servers show consistently high performance for certain types of functional sites. Finally, many functional sites are associated with protein-protein interaction. We also analyzed biologically relevant features from the protein assemblies of two targets where the active site spanned the protein-protein interface. For the assembly targets, we find that the features in the models are mainly determined by the choice of template.},
	language = {en},
	urldate = {2022-07-08},
	journal = {Proteins: Structure, Function, and Bioinformatics},
	author = {Liu, Tianyun and Ish-Shalom, Shirbi and Torng, Wen and Lafita, Aleix and Bock, Christian and Mort, Matthew and Cooper, David N and Bliven, Spencer and Capitani, Guido and Mooney, Sean D. and Altman, Russ B.},
	month = mar,
	year = {2018},
	note = {7 citations (Semantic Scholar/DOI) [2022-07-08]},
	pages = {374--386},
}

@article{russakovsky_imagenet_2015,
	title = {{ImageNet} {Large} {Scale} {Visual} {Recognition} {Challenge}},
	volume = {115},
	issn = {0920-5691, 1573-1405},
	url = {http://link.springer.com/10.1007/s11263-015-0816-y},
	doi = {10.1007/s11263-015-0816-y},
	abstract = {The ImageNet Large Scale Visual Recognition Challenge is a benchmark in object category classiﬁcation and detection on hundreds of object categories and millions of images. The challenge has been run annually from 2010 to present, attracting participation from more than ﬁfty institutions. This paper describes the creation of this benchmark dataset and the advances in object recognition that have been possible as a result. We discuss the challenges of collecting large-scale ground truth annotation, highlight key breakthroughs in categorical object recognition, provide a detailed analysis of the current state of the ﬁeld of large-scale image classiﬁcation and object detection, and compare the state-ofthe-art computer vision accuracy with human accuracy. We conclude with lessons learned in the 5 years of the challenge, and propose future directions and improvements.},
	language = {en},
	number = {3},
	urldate = {2022-07-08},
	journal = {International Journal of Computer Vision},
	author = {Russakovsky, Olga and Deng, Jia and Su, Hao and Krause, Jonathan and Satheesh, Sanjeev and Ma, Sean and Huang, Zhiheng and Karpathy, Andrej and Khosla, Aditya and Bernstein, Michael and Berg, Alexander C. and Fei-Fei, Li},
	month = dec,
	year = {2015},
	note = {9999 citations (Semantic Scholar/DOI) [2022-07-08]},
	pages = {211--252},
}

@article{hamerly_experimental_2019,
	title = {Experimental investigation of performance differences between coherent {Ising} machines and a quantum annealer},
	volume = {5},
	issn = {2375-2548},
	url = {https://www.science.org/doi/10.1126/sciadv.aau0823},
	doi = {10.1126/sciadv.aau0823},
	abstract = {Benchmarking the coherent Ising machine and the D-Wave quantum annealer sheds light on the importance of connectivity.
          , 
            
              Physical annealing systems provide heuristic approaches to solving combinatorial optimization problems. Here, we benchmark two types of annealing machines—a quantum annealer built by D-Wave Systems and measurement-feedback coherent Ising machines (CIMs) based on optical parametric oscillators—on two problem classes, the Sherrington-Kirkpatrick (SK) model and MAX-CUT. The D-Wave quantum annealer outperforms the CIMs on MAX-CUT on cubic graphs. On denser problems, however, we observe an exponential penalty for the quantum annealer [exp(–α
              DW
              N
              2
              )] relative to CIMs [exp(–α
              CIM
              N
              )] for fixed anneal times, both on the SK model and on 50\% edge density MAX-CUT. This leads to a several orders of magnitude time-to-solution difference for instances with over 50 vertices. An optimal–annealing time analysis is also consistent with a substantial projected performance difference. The difference in performance between the sparsely connected D-Wave machine and the fully-connected CIMs provides strong experimental support for efforts to increase the connectivity of quantum annealers.},
	language = {en},
	number = {5},
	urldate = {2022-07-08},
	journal = {Science Advances},
	author = {Hamerly, Ryan and Inagaki, Takahiro and McMahon, Peter L. and Venturelli, Davide and Marandi, Alireza and Onodera, Tatsuhiro and Ng, Edwin and Langrock, Carsten and Inaba, Kensuke and Honjo, Toshimori and Enbutsu, Koji and Umeki, Takeshi and Kasahara, Ryoichi and Utsunomiya, Shoko and Kako, Satoshi and Kawarabayashi, Ken-ichi and Byer, Robert L. and Fejer, Martin M. and Mabuchi, Hideo and Englund, Dirk and Rieffel, Eleanor and Takesue, Hiroki and Yamamoto, Yoshihisa},
	month = may,
	year = {2019},
	note = {146 citations (Semantic Scholar/DOI) [2022-07-08]},
	pages = {eaau0823},
}

@article{spiegel_autogrow4_2020,
	title = {{AutoGrow4}: an open-source genetic algorithm for de novo drug design and lead optimization},
	volume = {12},
	issn = {1758-2946},
	shorttitle = {{AutoGrow4}},
	url = {https://jcheminf.biomedcentral.com/articles/10.1186/s13321-020-00429-4},
	doi = {10.1186/s13321-020-00429-4},
	abstract = {We here present AutoGrow4, an open-source program for semi-automated computer-aided drug discovery. AutoGrow4 uses a genetic algorithm to evolve predicted ligands on demand and so is not limited to a virtual library of pre-enumerated compounds. It is a useful tool for generating entirely novel drug-like molecules and for optimizing preexisting ligands. By leveraging recent computational and cheminformatics advancements, AutoGrow4 is faster, more stable, and more modular than previous versions. It implements new docking-program compatibility, chemical filters, multithreading options, and selection methods to support a wide range of user needs. To illustrate both de novo design and lead optimization, we here apply AutoGrow4 to the catalytic domain of poly(ADP-ribose) polymerase 1 (PARP-1), a well characterized DNA-damage-recognition protein. AutoGrow4 produces drug-like compounds with better predicted binding affinities than FDA-approved PARP-1 inhibitors (positive controls). The predicted binding modes of the AutoGrow4 compounds mimic those of the known inhibitors, even when AutoGrow4 is seeded with random small molecules. AutoGrow4 is available under the terms of the Apache License, Version 2.0. A copy can be downloaded free of charge from http://durrantlab.com/autogrow4.},
	language = {en},
	number = {1},
	urldate = {2022-07-08},
	journal = {Journal of Cheminformatics},
	author = {Spiegel, Jacob O. and Durrant, Jacob D.},
	month = dec,
	year = {2020},
	note = {36 citations (Semantic Scholar/DOI) [2022-07-08]},
	pages = {25},
}

@article{stephenson_survey_2019,
	title = {Survey of {Machine} {Learning} {Techniques} in {Drug} {Discovery}},
	volume = {20},
	issn = {13892002},
	url = {http://www.eurekaselect.com/164758/article},
	doi = {10.2174/1389200219666180820112457},
	abstract = {Methods: We did a large-scale literature search on existing scientific websites (e.g, ScienceDirect, Arxiv) and startup companies to understand current status of machine learning techniques in drug discovery.
Results: Our experiments demonstrated that there are different patterns in machine learning fields and drug discovery fields. For example, keywords like prediction, brain, discovery, and treatment are usually in drug discovery fields. Also, the total number of papers published in drug discovery fields with machine learning techniques is increasing every year.
Conclusion: The main focus of this survey is to understand the current status of machine learning techniques in the drug discovery field within both academic and industrial settings, and discuss its potential future applications. Several interesting patterns for machine learning techniques in drug discovery fields are discussed in this survey.},
	language = {en},
	number = {3},
	urldate = {2022-07-08},
	journal = {Current Drug Metabolism},
	author = {Stephenson, Natalie and Shane, Emily and Chase, Jessica and Rowland, Jason and Ries, David and Justice, Nicola and Zhang, Jie and Chan, Leong and Cao, Renzhi},
	month = may,
	year = {2019},
	note = {118 citations (Semantic Scholar/DOI) [2022-07-08]},
	pages = {185--193},
}

@article{aadit_massively_2022,
	title = {Massively {Parallel} {Probabilistic} {Computing} with {Sparse} {Ising} {Machines}},
	issn = {2520-1131},
	url = {http://arxiv.org/abs/2110.02481},
	doi = {10.1038/s41928-022-00774-2},
	abstract = {Inspired by the developments in quantum computing, building domain-specific classical hardware to solve computationally hard problems has received increasing attention. Here, by introducing systematic sparsification techniques, we demonstrate a massively parallel architecture: the sparse Ising Machine (sIM). Exploiting sparsity, sIM achieves ideal parallelism: its key figure of merit - flips per second - scales linearly with the number of probabilistic bits (p-bit) in the system. This makes sIM up to 6 orders of magnitude faster than a CPU implementing standard Gibbs sampling. Compared to optimized implementations in TPUs and GPUs, sIM delivers 5-18x speedup in sampling. In benchmark problems such as integer factorization, sIM can reliably factor semiprimes up to 32-bits, far larger than previous attempts from D-Wave and other probabilistic solvers. Strikingly, sIM beats competition-winning SAT solvers (by 4-700x in runtime to reach 95\% accuracy) in solving 3SAT problems. Even when sampling is made inexact using faster clocks, sIM can find the correct ground state with further speedup. The problem encoding and sparsification techniques we introduce can be applied to other Ising Machines (classical and quantum) and the architecture we present can be used for scaling the demonstrated 5,000-10,000 p-bits to 1,000,000 or more through analog CMOS or nanodevices.},
	urldate = {2022-07-08},
	journal = {Nature Electronics},
	author = {Aadit, Navid Anjum and Grimaldi, Andrea and Carpentieri, Mario and Theogarajan, Luke and Martinis, John M. and Finocchio, Giovanni and Camsari, Kerem Y.},
	month = jun,
	year = {2022},
	note = {6 citations (Semantic Scholar/arXiv) [2022-07-08]
6 citations (Semantic Scholar/DOI) [2022-07-08]
arXiv:2110.02481 [cond-mat]},
	keywords = {Computer Science - Distributed, Parallel, and Cluster Computing, Computer Science - Emerging Technologies, Condensed Matter - Disordered Systems and Neural Networks},
}

@article{pervaiz_hardware_2017,
	title = {Hardware emulation of stochastic p-bits for invertible logic},
	volume = {7},
	issn = {2045-2322},
	url = {http://www.nature.com/articles/s41598-017-11011-8},
	doi = {10.1038/s41598-017-11011-8},
	language = {en},
	number = {1},
	urldate = {2022-07-08},
	journal = {Scientific Reports},
	author = {Pervaiz, Ahmed Zeeshan and Ghantasala, Lakshmi Anirudh and Camsari, Kerem Yunus and Datta, Supriyo},
	month = dec,
	year = {2017},
	note = {35 citations (Semantic Scholar/DOI) [2022-07-08]},
	pages = {10994},
}

@article{deka_markov_nodate,
	title = {Markov {Chain} {Algorithms}: {A} {Template} for {Building} {Future} {Robust} {Low} {Power} {Systems}},
	abstract = {Although computational systems are looking towards post CMOS devices in the pursuit of lower power, the inherent unreliability of such devices makes it difﬁcult to design robust systems without additional power overheads for guaranteeing robustness. As such, algorithmic structures with inherent ability to tolerate computational errors are of signiﬁcant interest. We propose to cast applications as stochastic algorithms based on Markov chains as such algorithms are both sufﬁciently general and tolerant to transition errors. We show with four example applications - boolean satisﬁability (SAT), sorting, LDPC decoding and clustering - how applications can be cast as Markov Chain algorithms. Using algorithmic fault injection techniques, we demonstrate the robustness of these implementations to transition errors with high error rates. Based on these results, we make a case for using Markov Chains as an algorithmic template for future robust low power systems.},
	language = {en},
	author = {Deka, Biplab and Birklykke, Alex A and Duwe, Henry and Mansinghka, Vikash K and Kumar, Rakesh},
	pages = {8},
}

@article{johnson_markov_nodate,
	title = {Markov {Chain} {Monte} {Carlo} {Policy} {Optimization}},
	abstract = {Discovering approximately optimal policies in domains is crucial to applying reinforcement learning (RL) in many real-world scenarios, which is termed as policy optimization. By viewing the policy optimization from the perspective of variational inference, the representation power of policy network allows us to obtain the approximate posterior of actions conditioned on the states, with the entropy or KL regularization. However, in practice the policy optimization may lead to suboptimal policy estimates due to amortization gap. Inspired by the Markov Chain Monte Carlo (MCMC) techniques, instead of optimizing policy parameters or policy distributions directly, we propose a new policy optimization method, incorporating gradient-based feedback in various ways. The empirical evaluation veriﬁes the performance improvement of the proposed method in many continuous control benchmarks.},
	language = {en},
	author = {Johnson, David S},
	pages = {10},
}

@article{zhou_novel_2019,
	title = {A {Novel} {Square} {Root} {Algorithm} and its {FPGA} {Simulation}},
	volume = {1314},
	issn = {1742-6588, 1742-6596},
	url = {https://iopscience.iop.org/article/10.1088/1742-6596/1314/1/012008},
	doi = {10.1088/1742-6596/1314/1/012008},
	abstract = {As a basic operation in elementary mathematics, square root operation is widely used in numerical calculation and digital signal processing. The square root operation is a nonlinear operation and cannot be solved directly on FPGA. Many previous square root algorithms have been proposed. This paper introduces several previous square root algorithms, then presents a novel square root algorithm which uses 16-bit integer input and 16-bit integer output. Using the idea of normalization, the square number is mapped to the range of 0-1, and the square root is approximated by the sum of numerical sequence. We discuss the proposed square root algorithm, error analysis. The proposed algorithm was programmed by the Verilog in Xilinx's Vivado 17.2 software environment, and the simulation was verified on the Virtex-7 series chip. Compared with other square root algorithms, the algorithm only uses shifter, adders and sub-tractor. It does not involve complex operations, consumes less resources, and it’s suitable for implementation on FPGA.},
	language = {en},
	number = {1},
	urldate = {2022-07-08},
	journal = {Journal of Physics: Conference Series},
	author = {Zhou, Zhongcheng and Hu, Jingchun},
	month = oct,
	year = {2019},
	note = {1 citations (Semantic Scholar/DOI) [2022-07-08]},
	pages = {012008},
}

@article{cooke_statistical_2008,
	title = {Statistical {Prediction} and {Molecular} {Dynamics} {Simulation}},
	volume = {95},
	issn = {00063495},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0006349508785916},
	doi = {10.1529/biophysj.108.131623},
	abstract = {We describe a statistical approach to the validation and improvement of molecular dynamics simulations of macromolecules. We emphasize the use of molecular dynamics simulations to calculate thermodynamic quantities that may be compared to experimental measurements, and the use of a common set of energetic parameters across multiple distinct molecules. We brieﬂy review relevant results from the theory of stochastic processes and discuss the monitoring of convergence to equilibrium, the obtaining of conﬁdence intervals for summary statistics corresponding to measured quantities, and an approach to validation and improvement of simulations based on out-of-sample prediction. We apply these methods to replica exchange molecular dynamics simulations of a set of eight helical peptides under the AMBER potential using implicit solvent. We evaluate the ability of these simulations to quantitatively reproduce experimental helicity measurements obtained by circular dichroism. In addition, we introduce notions of statistical predictive estimation for force-ﬁeld parameter reﬁnement. We perform a sensitivity analysis to identify key parameters of the potential, and introduce Bayesian updating of these parameters. We demonstrate the effect of parameter updating applied to the internal dielectric constant parameter on the out-of-sample prediction accuracy as measured by cross-validation.},
	language = {en},
	number = {10},
	urldate = {2022-07-08},
	journal = {Biophysical Journal},
	author = {Cooke, Ben and Schmidler, Scott C.},
	month = nov,
	year = {2008},
	note = {24 citations (Semantic Scholar/DOI) [2022-07-08]},
	pages = {4497--4511},
}

@article{kulmanov_semantic_2021,
	title = {Semantic similarity and machine learning with ontologies},
	volume = {22},
	issn = {1467-5463, 1477-4054},
	url = {https://academic.oup.com/bib/article/doi/10.1093/bib/bbaa199/5922325},
	doi = {10.1093/bib/bbaa199},
	abstract = {Ontologies have long been employed in the life sciences to formally represent and reason over domain knowledge and they are employed in almost every major biological database. Recently, ontologies are increasingly being used to provide background knowledge in similarity-based analysis and machine learning models. The methods employed to combine ontologies and machine learning are still novel and actively being developed. We provide an overview over the methods that use ontologies to compute similarity and incorporate them in machine learning methods; in particular, we outline how semantic similarity measures and ontology embeddings can exploit the background knowledge in ontologies and how ontologies can provide constraints that improve machine learning models. The methods and experiments we describe are available as a set of executable notebooks, and we also provide a set of slides and additional resources at https://github.com/ bio-ontology-research-group/machine-learning-with-ontologies.},
	language = {en},
	number = {4},
	urldate = {2022-07-08},
	journal = {Briefings in Bioinformatics},
	author = {Kulmanov, Maxat and Smaili, Fatima Zohra and Gao, Xin and Hoehndorf, Robert},
	month = jul,
	year = {2021},
	note = {38 citations (Semantic Scholar/DOI) [2022-07-08]},
	pages = {bbaa199},
}

@article{mello_epidemics_2021,
	title = {Epidemics, the {Ising}-model and percolation theory: a comprehensive review focussed on {Covid}-19},
	volume = {573},
	issn = {03784371},
	shorttitle = {Epidemics, the {Ising}-model and percolation theory},
	url = {http://arxiv.org/abs/2003.11860},
	doi = {10.1016/j.physa.2021.125963},
	abstract = {We revisit well-established concepts of epidemiology, the Ising-model, and percolation theory. Also, we employ a spin \$S\$ = 1/2 Ising-like model and a (logistic) Fermi-Dirac-like function to describe the spread of Covid-19. Our analysis reinforces well-established literature results, namely: {\textbackslash}emph\{i\}) that the epidemic curves can be described by a Gaussian-type function; {\textbackslash}emph\{ii\}) that the temporal evolution of the accumulative number of infections and fatalities follow a logistic function, which has some resemblance with a distorted Fermi-Dirac-like function; {\textbackslash}emph\{iii\}) the key role played by the quarantine to block the spread of Covid-19 in terms of an {\textbackslash}emph\{interacting\} parameter, which emulates the contact between infected and non-infected people. Furthermore, in the frame of elementary percolation theory, we show that: {\textbackslash}emph\{i\}) the percolation probability can be associated with the probability of a person being infected with Covid-19; {\textbackslash}emph\{ii\}) the concepts of blocked and non-blocked connections can be associated, respectively, with a person respecting or not the social distancing, impacting thus in the probability of an infected person to infect other people. Increasing the number of infected people leads to an increase in the number of net connections, giving rise thus to a higher probability of new infections (percolation). We demonstrate the importance of social distancing in preventing the spread of Covid-19 in a pedagogical way. Given the impossibility of making a precise forecast of the disease spread, we highlight the importance of taking into account additional factors, such as climate changes and urbanization, in the mathematical description of epidemics. Yet, we make a connection between the standard mathematical models employed in epidemics and well-established concepts in condensed matter Physics, such as the Fermi gas and the Landau Fermi-liquid picture.},
	language = {en},
	urldate = {2022-07-08},
	journal = {Physica A: Statistical Mechanics and its Applications},
	author = {Mello, Isys F. and Squillante, Lucas and Gomes, Gabriel O. and Seridonio, Antonio C. and de Souza, M.},
	month = jul,
	year = {2021},
	note = {8 citations (Semantic Scholar/arXiv) [2022-07-08]
14 citations (Semantic Scholar/DOI) [2022-07-08]
arXiv:2003.11860 [cond-mat, physics:physics, q-bio]},
	keywords = {Condensed Matter - Statistical Mechanics, Physics - Physics and Society, Quantitative Biology - Populations and Evolution},
	pages = {125963},
}

@article{kim_statistical-temperature_2006,
	title = {Statistical-{Temperature} {Monte} {Carlo} and {Molecular} {Dynamics} {Algorithms}},
	volume = {97},
	issn = {0031-9007, 1079-7114},
	url = {https://link.aps.org/doi/10.1103/PhysRevLett.97.050601},
	doi = {10.1103/PhysRevLett.97.050601},
	language = {en},
	number = {5},
	urldate = {2022-07-08},
	journal = {Physical Review Letters},
	author = {Kim, Jaegil and Straub, John E. and Keyes, Thomas},
	month = aug,
	year = {2006},
	note = {103 citations (Semantic Scholar/DOI) [2022-07-08]},
	pages = {050601},
}

@misc{raiko_techniques_2015,
	title = {Techniques for {Learning} {Binary} {Stochastic} {Feedforward} {Neural} {Networks}},
	url = {http://arxiv.org/abs/1406.2989},
	abstract = {Stochastic binary hidden units in a multi-layer perceptron (MLP) network give at least three potential beneﬁts when compared to deterministic MLP networks. (1) They allow to learn one-to-many type of mappings. (2) They can be used in structured prediction problems, where modeling the internal structure of the output is important. (3) Stochasticity has been shown to be an excellent regularizer, which makes generalization performance potentially better in general. However, training stochastic networks is considerably more difﬁcult. We study training using M samples of hidden activations per input. We show that the case M = 1 leads to a fundamentally different behavior where the network tries to avoid stochasticity. We propose two new estimators for the training gradient and propose benchmark tests for comparing training algorithms. Our experiments conﬁrm that training stochastic networks is difﬁcult and show that the proposed two estimators perform favorably among all the ﬁve known estimators.},
	language = {en},
	urldate = {2022-07-08},
	publisher = {arXiv},
	author = {Raiko, Tapani and Berglund, Mathias and Alain, Guillaume and Dinh, Laurent},
	month = apr,
	year = {2015},
	note = {117 citations (Semantic Scholar/arXiv) [2022-07-08]
arXiv:1406.2989 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Statistics - Machine Learning},
}

@article{hansson_molecular_nodate,
	title = {Molecular dynamics simulations},
	language = {en},
	author = {Hansson, Tomas and Oostenbrink, Chris and van Gunsteren, Wilfred F},
	pages = {7},
}

@article{camsari_scalable_2019,
	title = {Scalable {Emulation} of {Sign}-{Problem}\$-\${Free} {Hamiltonians} with {Room} {Temperature} p-bits},
	volume = {12},
	issn = {2331-7019},
	url = {http://arxiv.org/abs/1810.07144},
	doi = {10.1103/PhysRevApplied.12.034061},
	abstract = {The growing field of quantum computing is based on the concept of a q-bit which is a delicate superposition of 0 and 1, requiring cryogenic temperatures for its physical realization along with challenging coherent coupling techniques for entangling them. By contrast, a probabilistic bit or a p-bit is a robust classical entity that fluctuates between 0 and 1, and can be implemented at room temperature using present-day technology. Here, we show that a probabilistic coprocessor built out of room temperature p-bits can be used to accelerate simulations of a special class of quantum many-body systems that are sign-problem\$-\$free or stoquastic, leveraging the well-known Suzuki-Trotter decomposition that maps a \$d\$-dimensional quantum many body Hamiltonian to a \$d\$+1-dimensional classical Hamiltonian. This mapping allows an efficient emulation of a quantum system by classical computers and is commonly used in software to perform Quantum Monte Carlo (QMC) algorithms. By contrast, we show that a compact, embedded MTJ-based coprocessor can serve as a highly efficient hardware-accelerator for such QMC algorithms providing several orders of magnitude improvement in speed compared to optimized CPU implementations. Using realistic device-level SPICE simulations we demonstrate that the correct quantum correlations can be obtained using a classical p-circuit built with existing technology and operating at room temperature. The proposed coprocessor can serve as a tool to study stoquastic quantum many-body systems, overcoming challenges associated with physical quantum annealers.},
	language = {en},
	number = {3},
	urldate = {2022-07-08},
	journal = {Physical Review Applied},
	author = {Camsari, Kerem Y. and Chowdhury, Shuvro and Datta, Supriyo},
	month = sep,
	year = {2019},
	note = {8 citations (Semantic Scholar/arXiv) [2022-07-08]
13 citations (Semantic Scholar/DOI) [2022-07-08]
arXiv:1810.07144 [quant-ph]},
	keywords = {Quantum Physics},
	pages = {034061},
}

@article{degiacomi_coupling_2019,
	title = {Coupling {Molecular} {Dynamics} and {Deep} {Learning} to {Mine} {Protein} {Conformational} {Space}},
	volume = {27},
	issn = {09692126},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0969212619301145},
	doi = {10.1016/j.str.2019.03.018},
	abstract = {Flexibility is often a key determinant of protein function. To elucidate the link between their molecular structure and role in an organism, computational techniques such as molecular dynamics can be leveraged to characterize their conformational space. Extensive sampling is, however, required to obtain reliable results, useful to rationalize experimental data or predict outcomes before experiments are carried out. We demonstrate that a generative neural network trained on protein structures produced by molecular simulation can be used to obtain new, plausible conformations complementing preexisting ones. To demonstrate this, we show that a trained neural network can be exploited in a protein-protein docking scenario to account for broad hinge motions taking place upon binding. Overall, this work shows that neural networks can be used as an exploratory tool for the study of molecular conformational space.},
	language = {en},
	number = {6},
	urldate = {2022-07-08},
	journal = {Structure},
	author = {Degiacomi, Matteo T.},
	month = jun,
	year = {2019},
	note = {47 citations (Semantic Scholar/DOI) [2022-07-08]},
	pages = {1034--1040.e3},
}

@article{chmiela_machine_2017,
	title = {Machine learning of accurate energy-conserving molecular force fields},
	volume = {3},
	issn = {2375-2548},
	url = {https://www.science.org/doi/10.1126/sciadv.1603015},
	doi = {10.1126/sciadv.1603015},
	abstract = {The law of energy conservation is used to develop an efficient machine learning approach to construct accurate force fields.
          , 
            
              Using conservation of energy—a fundamental property of closed classical and quantum mechanical systems—we develop an efficient gradient-domain machine learning (GDML) approach to construct accurate molecular force fields using a restricted number of samples from ab initio molecular dynamics (AIMD) trajectories. The GDML implementation is able to reproduce global potential energy surfaces of intermediate-sized molecules with an accuracy of 0.3 kcal mol
              −1
              for energies and 1 kcal mol
              −1
              Å̊
              −1
              for atomic forces using only 1000 conformational geometries for training. We demonstrate this accuracy for AIMD trajectories of molecules, including benzene, toluene, naphthalene, ethanol, uracil, and aspirin. The challenge of constructing conservative force fields is accomplished in our work by learning in a Hilbert space of vector-valued functions that obey the law of energy conservation. The GDML approach enables quantitative molecular dynamics simulations for molecules at a fraction of cost of explicit AIMD calculations, thereby allowing the construction of efficient force fields with the accuracy and transferability of high-level ab initio methods.},
	language = {en},
	number = {5},
	urldate = {2022-07-08},
	journal = {Science Advances},
	author = {Chmiela, Stefan and Tkatchenko, Alexandre and Sauceda, Huziel E. and Poltavsky, Igor and Schütt, Kristof T. and Müller, Klaus-Robert},
	month = may,
	year = {2017},
	note = {571 citations (Semantic Scholar/DOI) [2022-07-08]},
	pages = {e1603015},
}

@article{galvelis_neural_2017,
	title = {Neural {Network} and {Nearest} {Neighbor} {Algorithms} for {Enhancing} {Sampling} of {Molecular} {Dynamics}},
	volume = {13},
	issn = {1549-9618, 1549-9626},
	url = {https://pubs.acs.org/doi/10.1021/acs.jctc.7b00188},
	doi = {10.1021/acs.jctc.7b00188},
	language = {en},
	number = {6},
	urldate = {2022-07-08},
	journal = {Journal of Chemical Theory and Computation},
	author = {Galvelis, Raimondas and Sugita, Yuji},
	month = jun,
	year = {2017},
	note = {44 citations (Semantic Scholar/DOI) [2022-07-08]},
	pages = {2489--2500},
}

@article{batool_structure-based_2019,
	title = {A {Structure}-{Based} {Drug} {Discovery} {Paradigm}},
	volume = {20},
	issn = {1422-0067},
	url = {https://www.mdpi.com/1422-0067/20/11/2783},
	doi = {10.3390/ijms20112783},
	abstract = {Structure-based drug design is becoming an essential tool for faster and more cost-eﬃcient lead discovery relative to the traditional method. Genomic, proteomic, and structural studies have provided hundreds of new targets and opportunities for future drug discovery. This situation poses a major problem: the necessity to handle the “big data” generated by combinatorial chemistry. Artiﬁcial intelligence (AI) and deep learning play a pivotal role in the analysis and systemization of larger data sets by statistical machine learning methods. Advanced AI-based sophisticated machine learning tools have a signiﬁcant impact on the drug discovery process including medicinal chemistry. In this review, we focus on the currently available methods and algorithms for structure-based drug design including virtual screening and de novo drug design, with a special emphasis on AI- and deep-learning-based methods used for drug discovery.},
	language = {en},
	number = {11},
	urldate = {2022-07-08},
	journal = {International Journal of Molecular Sciences},
	author = {Batool, Maria and Ahmad, Bilal and Choi, Sangdun},
	month = jun,
	year = {2019},
	note = {164 citations (Semantic Scholar/DOI) [2022-07-08]},
	pages = {2783},
}

@article{lazim_advances_2020,
	title = {Advances in {Molecular} {Dynamics} {Simulations} and {Enhanced} {Sampling} {Methods} for the {Study} of {Protein} {Systems}},
	volume = {21},
	issn = {1422-0067},
	url = {https://www.mdpi.com/1422-0067/21/17/6339},
	doi = {10.3390/ijms21176339},
	abstract = {Molecular dynamics (MD) simulation is a rigorous theoretical tool that when used eﬃciently could provide reliable answers to questions pertaining to the structure-function relationship of proteins. Data collated from protein dynamics can be translated into useful statistics that can be exploited to sieve thermodynamics and kinetics crucial for the elucidation of mechanisms responsible for the modulation of biological processes such as protein-ligand binding and protein-protein association. Continuous modernization of simulation tools enables accurate prediction and characterization of the aforementioned mechanisms and these qualities are highly beneﬁcial for the expedition of drug development when eﬀectively applied to structure-based drug design (SBDD). In this review, current all-atom MD simulation methods, with focus on enhanced sampling techniques, utilized to examine protein structure, dynamics, and functions are discussed. This review will pivot around computer calculations of protein-ligand and protein-protein systems with applications to SBDD. In addition, we will also be highlighting limitations faced by current simulation tools as well as the improvements that have been made to ameliorate their eﬃciency.},
	language = {en},
	number = {17},
	urldate = {2022-07-08},
	journal = {International Journal of Molecular Sciences},
	author = {Lazim, Raudah and Suh, Donghyuk and Choi, Sun},
	month = sep,
	year = {2020},
	note = {29 citations (Semantic Scholar/DOI) [2022-07-08]},
	pages = {6339},
}

@article{kleesiek_deep_2016,
	title = {Deep {MRI} brain extraction: {A} {3D} convolutional neural network for skull stripping},
	volume = {129},
	issn = {10538119},
	shorttitle = {Deep {MRI} brain extraction},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S1053811916000306},
	doi = {10.1016/j.neuroimage.2016.01.024},
	abstract = {Brain extraction from magnetic resonance imaging (MRI) is crucial for many neuroimaging workflows. Current methods demonstrate good results on non-enhanced T1-weighted images, but struggle when confronted with other modalities and pathologically altered tissue. In this paper we present a 3D convolutional deep learning architecture to address these shortcomings. In contrast to existing methods, we are not limited to non-enhanced T1w images. When trained appropriately, our approach handles an arbitrary number of modalities including contrast-enhanced scans. Its applicability to MRI data, comprising four channels: non-enhanced and contrast-enhanced T1w, T2w and FLAIR contrasts, is demonstrated on a challenging clinical data set containing brain tumors (N = 53), where our approach outperforms six commonly used tools with a Dice score of 95.19. Further, the proposed method at least matches state-of-the-art performance as demonstrated on three publicly available data sets: IBSR, LPBA40 and OASIS, totaling N = 135 volumes. For the IBSR (96.32), LPBA40 (96.96) and the tumor data set (95.19) the convolutional neuronal network (CNN) obtains the highest average Dice scores, albeit not being significantly different from the second best performing method. For the OASIS data the second best Dice (95.02) results are achieved, with no statistical difference in comparison to the best performing tool. For all data sets the highest average specificity measures are evaluated, whereas the sensitivity displays about average results. Adjusting the cut-off threshold for generating the binary masks from the CNN’s probability output can be used to increase the sensitivity of the method. Of course, this comes at the cost of a decreased specificity and has to be decided application specific. Using an optimized GPU implementation predictions can be achieved in less than one minute. The proposed method may prove useful for large-scale studies and clinical trials.},
	language = {en},
	urldate = {2022-07-08},
	journal = {NeuroImage},
	author = {Kleesiek, Jens and Urban, Gregor and Hubert, Alexander and Schwarz, Daniel and Maier-Hein, Klaus and Bendszus, Martin and Biller, Armin},
	month = apr,
	year = {2016},
	note = {332 citations (Semantic Scholar/DOI) [2022-07-08]},
	pages = {460--469},
}

@article{laughton_study_1994,
	title = {A study of simulated annealing protocols for use with molecular dynamics in protein structure prediction},
	volume = {7},
	issn = {1741-0126, 1741-0134},
	url = {https://academic.oup.com/peds/article-lookup/doi/10.1093/protein/7.2.235},
	doi = {10.1093/protein/7.2.235},
	language = {en},
	number = {2},
	urldate = {2022-07-08},
	journal = {"Protein Engineering, Design and Selection"},
	author = {Laughton, C. A.},
	year = {1994},
	note = {18 citations (Semantic Scholar/DOI) [2022-07-08]},
	pages = {235--241},
}

@article{lyne_structure-based_2002,
	title = {Structure-based virtual screening: an overview},
	volume = {7},
	issn = {13596446},
	shorttitle = {Structure-based virtual screening},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S1359644602024832},
	doi = {10.1016/S1359-6446(02)02483-2},
	language = {en},
	number = {20},
	urldate = {2022-07-08},
	journal = {Drug Discovery Today},
	author = {Lyne, Paul D},
	month = oct,
	year = {2002},
	note = {532 citations (Semantic Scholar/DOI) [2022-07-08]},
	pages = {1047--1055},
}

@article{metropolis_equation_nodate,
	title = {Equation of {State} {Calculations} by {Fast} {Computing} {Machines}},
	language = {en},
	author = {Metropolis, Nicholas and Rosenbluth, Arianna W and Rosenbluth, Marshall N and Teller, Augusta H},
	pages = {7},
}

@article{figliuzzi_how_2018,
	title = {How {Pairwise} {Coevolutionary} {Models} {Capture} the {Collective} {Residue} {Variability} in {Proteins}?},
	volume = {35},
	issn = {0737-4038, 1537-1719},
	url = {https://academic.oup.com/mbe/article/35/4/1018/4815777},
	doi = {10.1093/molbev/msy007},
	language = {en},
	number = {4},
	urldate = {2022-07-08},
	journal = {Molecular Biology and Evolution},
	author = {Figliuzzi, Matteo and Barrat-Charlaix, Pierre and Weigt, Martin},
	month = apr,
	year = {2018},
	note = {63 citations (Semantic Scholar/DOI) [2022-07-08]},
	pages = {1018--1027},
}

@misc{noauthor_no_nodate,
	title = {[{No} title found]},
	language = {en},
}

@book{nielsen_quantum_2010,
	address = {Cambridge ; New York},
	edition = {10th anniversary ed},
	title = {Quantum computation and quantum information},
	isbn = {978-1-107-00217-3},
	language = {en},
	publisher = {Cambridge University Press},
	author = {Nielsen, Michael A. and Chuang, Isaac L.},
	year = {2010},
	keywords = {Quantum computers},
}

@article{arute_quantum_2019,
	title = {Quantum supremacy using a programmable superconducting processor},
	volume = {574},
	issn = {0028-0836, 1476-4687},
	url = {http://www.nature.com/articles/s41586-019-1666-5},
	doi = {10.1038/s41586-019-1666-5},
	language = {en},
	number = {7779},
	urldate = {2022-07-08},
	journal = {Nature},
	author = {Arute, Frank and Arya, Kunal and Babbush, Ryan and Bacon, Dave and Bardin, Joseph C. and Barends, Rami and Biswas, Rupak and Boixo, Sergio and Brandao, Fernando G. S. L. and Buell, David A. and Burkett, Brian and Chen, Yu and Chen, Zijun and Chiaro, Ben and Collins, Roberto and Courtney, William and Dunsworth, Andrew and Farhi, Edward and Foxen, Brooks and Fowler, Austin and Gidney, Craig and Giustina, Marissa and Graff, Rob and Guerin, Keith and Habegger, Steve and Harrigan, Matthew P. and Hartmann, Michael J. and Ho, Alan and Hoffmann, Markus and Huang, Trent and Humble, Travis S. and Isakov, Sergei V. and Jeffrey, Evan and Jiang, Zhang and Kafri, Dvir and Kechedzhi, Kostyantyn and Kelly, Julian and Klimov, Paul V. and Knysh, Sergey and Korotkov, Alexander and Kostritsa, Fedor and Landhuis, David and Lindmark, Mike and Lucero, Erik and Lyakh, Dmitry and Mandrà, Salvatore and McClean, Jarrod R. and McEwen, Matthew and Megrant, Anthony and Mi, Xiao and Michielsen, Kristel and Mohseni, Masoud and Mutus, Josh and Naaman, Ofer and Neeley, Matthew and Neill, Charles and Niu, Murphy Yuezhen and Ostby, Eric and Petukhov, Andre and Platt, John C. and Quintana, Chris and Rieffel, Eleanor G. and Roushan, Pedram and Rubin, Nicholas C. and Sank, Daniel and Satzinger, Kevin J. and Smelyanskiy, Vadim and Sung, Kevin J. and Trevithick, Matthew D. and Vainsencher, Amit and Villalonga, Benjamin and White, Theodore and Yao, Z. Jamie and Yeh, Ping and Zalcman, Adam and Neven, Hartmut and Martinis, John M.},
	month = oct,
	year = {2019},
	note = {3019 citations (Semantic Scholar/DOI) [2022-07-08]},
	pages = {505--510},
}

@article{borders_integer_2019,
	title = {Integer factorization using stochastic magnetic tunnel junctions},
	volume = {573},
	issn = {0028-0836, 1476-4687},
	url = {http://www.nature.com/articles/s41586-019-1557-9},
	doi = {10.1038/s41586-019-1557-9},
	language = {en},
	number = {7774},
	urldate = {2022-07-08},
	journal = {Nature},
	author = {Borders, William A. and Pervaiz, Ahmed Z. and Fukami, Shunsuke and Camsari, Kerem Y. and Ohno, Hideo and Datta, Supriyo},
	month = sep,
	year = {2019},
	note = {158 citations (Semantic Scholar/DOI) [2022-07-08]},
	pages = {390--393},
}

@article{gothandaraman_fpga_2008,
	title = {{FPGA} acceleration of a quantum {Monte} {Carlo} application},
	volume = {34},
	issn = {01678191},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0167819108000227},
	doi = {10.1016/j.parco.2008.01.009},
	abstract = {Quantum Monte Carlo methods enable us to determine the ground-state properties of atomic or molecular clusters. Here, we present a reconﬁgurable computing architecture using Field Programmable Gate Arrays (FPGAs) to accelerate two computationally intensive kernels of a Quantum Monte Carlo (QMC) application applied to N-body systems. We focus on two key kernels of the QMC application: acceleration of potential energy and wave function calculations. We compare the performance of our application on two reconﬁgurable platforms. Firstly, we use a dual-processor 2.4 GHz Intel Xeon augmented with two reconﬁgurable development boards consisting of Xilinx Virtex-II Pro FPGAs. Using this platform, we achieve a speedup of 3Â over a software-only implementation. Following this, the chemistry application is ported to the Cray XD1 supercomputer equipped with Xilinx Virtex-II Pro and Virtex-4 FPGAs. The hardware-accelerated application on one node of the high performance system equipped with a single Virtex-4 FPGA yields a speedup of approximately 25Â over the serial reference code running on one node of the dual-processor dual-core 2.2 GHz AMD Opteron. This speedup is mainly attributed to the use of pipelining, the use of ﬁxedpoint arithmetic for all calculations and the ﬁne-grained parallelism using FPGAs. We can further enhance the performance by operating multiple instances of our design in parallel. Ó 2008 Elsevier B.V. All rights reserved.},
	language = {en},
	number = {4-5},
	urldate = {2022-07-08},
	journal = {Parallel Computing},
	author = {Gothandaraman, Akila and Peterson, Gregory D. and Warren, G.L. and Hinde, Robert J. and Harrison, Robert J.},
	month = may,
	year = {2008},
	note = {44 citations (Semantic Scholar/DOI) [2022-07-08]},
	pages = {278--291},
}

@article{radziuk_ultrasonically_2016,
	title = {Ultrasonically treated liquid interfaces for progress in cleaning and separation processes},
	volume = {18},
	issn = {1463-9076, 1463-9084},
	url = {http://xlink.rsc.org/?DOI=C5CP05142H},
	doi = {10.1039/C5CP05142H},
	abstract = {Cleaning and separation processes of liquids can be advanced by acoustic cavitation through bubbles with unique physico-chemical properties.
          , 
            
              Ultrasound and acoustic cavitation enable ergonomic and eco-friendly treatment of complex liquids with outstanding performance in cleaning, separation and recycling of resources. A key element of ultrasonic-based technology is the high speed of mixing by streams, flows and jets (or shock waves), which is accompanied by sonochemical reactions. Mass transfer across the phase boundary with a great variety of catalytic processes is substantially enhanced through acoustic emulsification. Encapsulation, separation and recovery of liquids are fast with high production yield if applied by ultrasound. Here we discuss the state of knowledge of these processes by ultrasound and acoustic cavitation from a perspective of a physico-chemical model in order to predict and control the outcome. We focus on the physical interpretation and quantification of ultrasonic parameters and properties of liquids to understand the chemistry of liquid/liquid interfaces in acoustic fields. The roles of thermodynamic enthalpy and entropy (incl. Laplace and osmotic pressure) in the context of sonochemical reactions (separation, catalysis, degradation, cross-linking, ion exchange and phase transfer) are outlined. The synergy of ultrasound and electric fields or continuous flow chemistry for cleaning and separation
              via
              emulsification is highlighted by specific strategies involving polymers and ultrasonic membranes.},
	language = {en},
	number = {1},
	urldate = {2022-07-08},
	journal = {Physical Chemistry Chemical Physics},
	author = {Radziuk, Darya and Möhwald, Helmuth},
	year = {2016},
	note = {43 citations (Semantic Scholar/DOI) [2022-07-08]},
	pages = {21--46},
}

@article{anderson_hoomd-blue_2020,
	title = {{HOOMD}-blue: {A} {Python} package for high-performance molecular dynamics and hard particle {Monte} {Carlo} simulations},
	volume = {173},
	issn = {09270256},
	shorttitle = {{HOOMD}-blue},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0927025619306627},
	doi = {10.1016/j.commatsci.2019.109363},
	abstract = {HOOMD-blue is a particle simulation engine designed for nano- and colloidal-scale molecular dynamics and hard particle Monte Carlo simulations. It has been actively developed since March 2007 and available open source since August 2008. HOOMD-blue is a Python package with a high performance C++/CUDA backend that we built from the ground up for GPU acceleration. The Python interface allows users to combine HOOMD-blue with other packages in the Python ecosystem to create simulation and analysis workflows. We employ software engineering practices to develop, test, maintain, and expand the code.},
	language = {en},
	urldate = {2022-07-08},
	journal = {Computational Materials Science},
	author = {Anderson, Joshua A. and Glaser, Jens and Glotzer, Sharon C.},
	month = feb,
	year = {2020},
	note = {119 citations (Semantic Scholar/DOI) [2022-07-08]},
	pages = {109363},
}

@article{hamilton_accelerating_2020,
	title = {Accelerating {Scientific} {Computing} in the {Post}-{Moore}’s {Era}},
	volume = {7},
	issn = {2329-4949, 2329-4957},
	url = {https://dl.acm.org/doi/10.1145/3380940},
	doi = {10.1145/3380940},
	abstract = {Novel uses of graphical processing units for accelerated computation revolutionized the field of high-performance scientific computing by providing specialized workflows tailored to algorithmic requirements. As the era of Moore’s law draws to a close, many new non–von Neumann processors are emerging as potential computational accelerators, including those based on the principles of neuromorphic computing, tensor algebra, and quantum information. While development of these new processors is continuing to mature, the potential impact on accelerated computing is anticipated to be profound. We discuss how different processing models can advance computing in key scientific paradigms: machine learning and constraint satisfaction. Significantly, each of these new processor types utilizes a fundamentally different model of computation, and this raises questions about how to best use such processors in the design and implementation of applications. While many processors are being developed with a specific domain target, the ubiquity of spin-glass models and neural networks provides an avenue for multi-functional applications. This also hints at the infrastructure needed to integrate next-generation processing units into future high-performance computing systems.},
	language = {en},
	number = {1},
	urldate = {2022-07-08},
	journal = {ACM Transactions on Parallel Computing},
	author = {Hamilton, Kathleen E. and Schuman, Catherine D. and Young, Steven R. and Bennink, Ryan S. and Imam, Neena and Humble, Travis S.},
	month = apr,
	year = {2020},
	note = {11 citations (Semantic Scholar/DOI) [2022-07-08]},
	pages = {1--31},
}

@misc{simonyan_very_2015,
	title = {Very {Deep} {Convolutional} {Networks} for {Large}-{Scale} {Image} {Recognition}},
	url = {http://arxiv.org/abs/1409.1556},
	abstract = {In this work we investigate the effect of the convolutional network depth on its accuracy in the large-scale image recognition setting. Our main contribution is a thorough evaluation of networks of increasing depth using an architecture with very small (3 × 3) convolution ﬁlters, which shows that a signiﬁcant improvement on the prior-art conﬁgurations can be achieved by pushing the depth to 16–19 weight layers. These ﬁndings were the basis of our ImageNet Challenge 2014 submission, where our team secured the ﬁrst and the second places in the localisation and classiﬁcation tracks respectively. We also show that our representations generalise well to other datasets, where they achieve state-of-the-art results. We have made our two best-performing ConvNet models publicly available to facilitate further research on the use of deep visual representations in computer vision.},
	language = {en},
	urldate = {2022-07-08},
	publisher = {arXiv},
	author = {Simonyan, Karen and Zisserman, Andrew},
	month = apr,
	year = {2015},
	note = {9998 citations (Semantic Scholar/arXiv) [2022-07-08]
arXiv:1409.1556 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@article{sutton_autonomous_2020,
	title = {Autonomous {Probabilistic} {Coprocessing} {With} {Petaflips} per {Second}},
	volume = {8},
	issn = {2169-3536},
	url = {https://ieeexplore.ieee.org/document/9173656/},
	doi = {10.1109/ACCESS.2020.3018682},
	language = {en},
	urldate = {2022-07-08},
	journal = {IEEE Access},
	author = {Sutton, Brian and Faria, Rafatul and Ghantasala, Lakshmi Anirudh and Jaiswal, Risi and Camsari, Kerem Yunus and Datta, Supriyo},
	year = {2020},
	note = {17 citations (Semantic Scholar/DOI) [2022-07-08]},
	pages = {157238--157252},
}

@article{barahona_application_1988,
	title = {An {Application} of {Combinatorial} {Optimization} to {Statistical} {Physics} and {Circuit} {Layout} {Design}},
	volume = {36},
	issn = {0030-364X, 1526-5463},
	url = {http://pubsonline.informs.org/doi/10.1287/opre.36.3.493},
	doi = {10.1287/opre.36.3.493},
	abstract = {We study the problem of finding ground states of spin glasses with exterior magnetic field, and the problem of minimizing the number of vias (holes on a printed circuit board, or contacts on a chip) subject to pin preassignments and layer preferences. The former problem comes up in solid-state physics, and the latter in very-large-scale-integrated (VLSI) circuit design and in printed circuit board design. Both problems can be reduced to the max-cut problem in graphs. Based on a partial characterization of the cut polytope, we design a cutting plane algorithm and report on computational experience with it. Our method has been used to solve max-cut problems on graphs with up to 1,600 nodes.},
	language = {en},
	number = {3},
	urldate = {2022-07-08},
	journal = {Operations Research},
	author = {Barahona, Francisco and Grötschel, Martin and Jünger, Michael and Reinelt, Gerhard},
	month = jun,
	year = {1988},
	note = {421 citations (Semantic Scholar/DOI) [2022-07-08]},
	pages = {493--513},
}

@inproceedings{hitko_circuit_2005,
	address = {Palm Springs, CA, USA},
	title = {Circuit design considerations for 100 {GHz} clock rates},
	isbn = {978-0-7803-9250-2},
	url = {http://ieeexplore.ieee.org/document/1531831/},
	doi = {10.1109/CSICS.2005.1531831},
	abstract = {Recent developments in highly scaled SiGe and InP IC technologies have yielded devices with fT and fmax figures-of-merit well beyond 300GHz, bringing with them the potential to operate circuits of considerable complexity at clock rates exceeding 100GHz. An initial emphasis upon digital applications helps to put the technologies on a roadmap where further development can be fueled to open the door to other mixed-signal possibilities. Designing circuits at these frequencies requires a balance of thermal and electromagnetic concerns along with the traditional IC design issues, but measured results are demonstrating that these concerns are manageable.},
	language = {en},
	urldate = {2022-07-08},
	booktitle = {{IEEE} {Compound} {Semiconductor} {Integrated} {Circuit} {Symposium}, 2005. {CSIC} '05.},
	publisher = {IEEE},
	author = {Hitko, D.A. and Hussain, T. and Li, J.C. and Fields, C.H. and Elliott, K.},
	year = {2005},
	note = {1 citations (Semantic Scholar/DOI) [2022-07-08]},
	pages = {4 pp.},
}

@article{wiegele_biq_nodate,
	title = {Biq {Mac} {Library} - {A} collection of {Max}-{Cut} and quadratic 0-1 programming instances of medium size},
	abstract = {This is a collection of some Max-Cut and quadratic 0-1 programming instances of medium size (n = 20 up to n = 500, most of the instances having size n = 100).},
	language = {en},
	author = {Wiegele, Angelika},
	pages = {16},
}

@inproceedings{jin_opencl_2019,
	address = {San Diego, CA, USA},
	title = {{OpenCL} {Kernel} {Vectorization} on the {CPU}, {GPU}, and {FPGA}: {A} {Case} {Study} with {Frequent} {Pattern} {Compression}},
	isbn = {978-1-72811-131-5},
	shorttitle = {{OpenCL} {Kernel} {Vectorization} on the {CPU}, {GPU}, and {FPGA}},
	url = {https://ieeexplore.ieee.org/document/8735499/},
	doi = {10.1109/FCCM.2019.00071},
	abstract = {OpenCL promotes code portability, and natively supports vectorized data types, which allows developers to potentially take advantage of the single-instruction-multiple-data instructions on CPUs, GPUs, and FPGAs. FPGAs are becoming a promising heterogeneous computing component. In our study, we choose a kernel used in frequent pattern compression as a case study of OpenCL kernel vectorizations on the three computing platforms. We describe different pattern matching approaches for the kernel, and manually vectorize the OpenCL kernel by a factor ranging from 2 to 16. We evaluate the kernel on an Intel Xeon 16-core CPU, an NVIDIA P100 GPU, and a Nallatech 385A FPGA card featuring an Intel Arria 10 GX1150 FPGA. Compared to the optimized kernel that is not vectorized, our vectorization can improve the kernel performance by a factor of 16 on the FPGA. The performance improvement ranges from 1 to 11.4 on the CPU, and from 1.02 to 9.3 on the GPU. The effectiveness of kernel vectorization depends on the work-group size.},
	language = {en},
	urldate = {2022-07-08},
	booktitle = {2019 {IEEE} 27th {Annual} {International} {Symposium} on {Field}-{Programmable} {Custom} {Computing} {Machines} ({FCCM})},
	publisher = {IEEE},
	author = {Jin, Zheming and Finkel, Hal},
	month = apr,
	year = {2019},
	note = {0 citations (Semantic Scholar/DOI) [2022-07-08]},
	pages = {330--330},
}

@article{gu_deep_2020,
	title = {A {Deep} {Learning} {Algorithm} for the {Max}-{Cut} {Problem} {Based} on {Pointer} {Network} {Structure} with {Supervised} {Learning} and {Reinforcement} {Learning} {Strategies}},
	volume = {8},
	issn = {2227-7390},
	url = {https://www.mdpi.com/2227-7390/8/2/298},
	doi = {10.3390/math8020298},
	abstract = {The Max-cut problem is a well-known combinatorial optimization problem, which has many real-world applications. However, the problem has been proven to be non-deterministic polynomial-hard (NP-hard), which means that exact solution algorithms are not suitable for large-scale situations, as it is too time-consuming to obtain a solution. Therefore, designing heuristic algorithms is a promising but challenging direction to effectively solve large-scale Max-cut problems. For this reason, we propose a unique method which combines a pointer network and two deep learning strategies (supervised learning and reinforcement learning) in this paper, in order to address this challenge. A pointer network is a sequence-to-sequence deep neural network, which can extract data features in a purely data-driven way to discover the hidden laws behind data. Combining the characteristics of the Max-cut problem, we designed the input and output mechanisms of the pointer network model, and we used supervised learning and reinforcement learning to train the model to evaluate the model performance. Through experiments, we illustrated that our model can be well applied to solve large-scale Max-cut problems. Our experimental results also revealed that the new method will further encourage broader exploration of deep neural network for large-scale combinatorial optimization problems.},
	language = {en},
	number = {2},
	urldate = {2022-07-08},
	journal = {Mathematics},
	author = {Gu, Shenshen and Yang, Yue},
	month = feb,
	year = {2020},
	note = {7 citations (Semantic Scholar/DOI) [2022-07-08]},
	pages = {298},
}

@article{mazyavkina_reinforcement_2021,
	title = {Reinforcement learning for combinatorial optimization: {A} survey},
	volume = {134},
	issn = {03050548},
	shorttitle = {Reinforcement learning for combinatorial optimization},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0305054821001660},
	doi = {10.1016/j.cor.2021.105400},
	abstract = {Many traditional algorithms for solving combinatorial optimization problems involve using hand-crafted heuristics that sequentially construct a solution. Such heuristics are designed by domain experts and may often be suboptimal due to the hard nature of the problems. Reinforcement learning (RL) proposes a good alternative to automate the search of these heuristics by training an agent in a supervised or self-supervised manner. In this survey, we explore the recent advancements of applying RL frameworks to hard combinatorial problems. Our survey provides the necessary background for operations research and machine learning communities and showcases the works that are moving the field forward. We juxtapose recently proposed RL methods, laying out the timeline of the improvements for each problem, as well as we make a comparison with traditional algorithms, indicating that RL models can become a promising direction for solving combinatorial problems.},
	language = {en},
	urldate = {2022-07-08},
	journal = {Computers \& Operations Research},
	author = {Mazyavkina, Nina and Sviridov, Sergey and Ivanov, Sergei and Burnaev, Evgeny},
	month = oct,
	year = {2021},
	note = {98 citations (Semantic Scholar/DOI) [2022-07-08]},
	pages = {105400},
}

@article{bubeck_universal_nodate,
	title = {A {Universal} {Law} of {Robustness} via {Isoperimetry}},
	abstract = {Classically, data interpolation with a parametrized model class is possible as long as the number of parameters is larger than the number of equations to be satisﬁed. A puzzling phenomenon in deep learning is that models are trained with many more parameters than what this classical theory would suggest. We propose a theoretical explanation for this phenomenon. We prove that for a broad class of data distributions and model classes, overparametrization is necessary if one wants to interpolate the data smoothly. Namely we show that smooth interpolation requires d times more parameters than mere interpolation, where d is the ambient data dimension. We prove this universal law of robustness for any smoothly parametrized function class with polynomial size weights, and any covariate distribution verifying isoperimetry (or a mixture thereof). In the case of two-layer neural networks and Gaussian covariates, this law was conjectured in prior work by Bubeck, Li and Nagaraj. We also give an interpretation of our result as an improved generalization bound for model classes consisting of smooth functions.},
	language = {en},
	author = {Bubeck, Sebastien and Sellke, Mark},
	pages = {12},
}

@misc{patel_ising_2020,
	title = {Ising {Model} {Optimization} {Problems} on a {FPGA} {Accelerated} {Restricted} {Boltzmann} {Machine}},
	url = {http://arxiv.org/abs/2008.04436},
	abstract = {Optimization problems, particularly NP-Hard Combinatorial Optimization problems, are some of the hardest computing problems with no known polynomial time algorithm existing. Recently there has been interest in using dedicated hardware to accelerate the solution to these problems, with physical annealers and quantum adiabatic computers being some of the state of the art. In this work we demonstrate usage of the Restricted Boltzmann Machine (RBM) as a stochastic neural network capable of solving these problems efficiently. We show that by mapping the RBM onto a reconfigurable Field Programmable Gate Array (FPGA), we can effectively hardware accelerate the RBM's stochastic sampling algorithm. We benchmark the RBM against the DWave 2000Q Quantum Adiabatic Computer and the Optical Coherent Ising Machine on two such optimization problems: the MAX-CUT problem and finding the ground state of a Sherrington-Kirkpatrick (SK) spin glass. On these problems, the hardware accelerated RBM shows best in class performance compared to these other accelerators, with an empirical scaling performance of \${\textbackslash}mathcal\{O\}(e{\textasciicircum}\{-N\})\$ for probability of reaching the ground state compared to a similar empirical \${\textbackslash}mathcal\{O\}(e{\textasciicircum}\{-N\})\$ for the CIM (with the RBM showing a constant factor of improvement over the CIM) and empirical \${\textbackslash}mathcal\{O\}(e{\textasciicircum}\{-N{\textasciicircum}2\})\$ for the DWave Annealer. The results show up to \$10{\textasciicircum}7\$x and \$10{\textasciicircum}5\$x time to solution improvement compared to the DWave 2000Q on the MAX-CUT and SK problems respectively, along with a \$150\$x and \$1000\$x performance increase compared to the Coherent Ising Machine annealer on those problems. By using commodity hardware running at room temperature for acceleration, the RBM also has greater potential for immediate and scalable use.},
	urldate = {2022-07-30},
	publisher = {arXiv},
	author = {Patel, Saavan and Chen, Lili and Canoza, Philip and Salahuddin, Sayeef},
	month = oct,
	year = {2020},
	note = {14 citations (Semantic Scholar/arXiv) [2022-07-30]
arXiv:2008.04436 [physics]},
	keywords = {Computer Science - Hardware Architecture, Physics - Computational Physics},
}

@article{grenander_representations_1994,
	title = {Representations of {Knowledge} in {Complex} {Systems}},
	volume = {56},
	issn = {0035-9246},
	url = {http://www.jstor.org/stable/2346184},
	abstract = {Modern sensor technologies, especially in biomedicine, produce increasingly detailed and informative image ensembles, many extremely complex. It will be argued that pattern theory can supply mathematical representations of subject-matter knowledge that can be used as a basis for algorithmic `understanding' of such pictures. After a brief survey of the basic principles of pattern theory we shall illustrate them by an application to a concrete situation: high magnification (greater than 15 000 ×) electron micrographs of cardiac muscle cells. The aim is to build algorithms for automatic hypothesis formation concerning the number, location, orientation and shape of mitochondria and membranes. For this we construct a pattern theoretic model in the form of a prior probability measure on the space of configurations describing these hypotheses. This measure is synthesized by solving sequentially a jump-diffusion equation of generalized Langevin form. The jumps occur for the creation-annihilation of hypotheses, corresponding to a jump from one continuum to another in configuration (hypothesis) space. These continua (subhypotheses) are expressed in terms of products of low dimensional Lie groups acting on the generators of a template. We use a modified Bayes approach to obtain the hypothesis formation, also organized by solving a generalized Langevin equation. To justify this it is shown that the resulting jump-diffusion process is ergodic so that the solution converges to the desired probability measure. To speed up the convergence we reduce the computation of the drift term in the stochastic differential equation analytically to a curvilinear integral, with the random term computed almost instantaneously. The algorithms thus obtained are implemented, both for mitochondria and membranes, on a 4000 processor parallel machine. Photographs of the graphics illustrate how automatic hypothesis formation is achieved. This approach is applied to deformable neuroanatomical atlases and tracking recognition from narrow band and high resolution sensor arrays.},
	number = {4},
	urldate = {2022-07-30},
	journal = {Journal of the Royal Statistical Society. Series B (Methodological)},
	author = {Grenander, Ulf and Miller, Michael I.},
	year = {1994},
	note = {Publisher: [Royal Statistical Society, Wiley]},
	pages = {549--603},
}

@inproceedings{stoltzfus_data_2018,
	address = {Dallas TX USA},
	title = {Data {Placement} {Optimization} in {GPU} {Memory} {Hierarchy} using {Predictive} {Modeling}},
	isbn = {978-1-4503-6113-2},
	url = {https://dl.acm.org/doi/10.1145/3286475.3286482},
	doi = {10.1145/3286475.3286482},
	abstract = {Modern supercomputers often use Graphic Processing Units (or GPUs) to meet the ever-growing demands for high performance computing. GPUs typically have a complex memory architecture with various types of memories and caches, such as global memory, shared memory, constant memory, and texture memory. The placement of data on these memories has a tremendous impact on the performance of the HPC applications and identifying the optimal placement location is non-trivial.},
	language = {en},
	urldate = {2022-07-30},
	booktitle = {Proceedings of the {Workshop} on {Memory} {Centric} {High} {Performance} {Computing}},
	publisher = {ACM},
	author = {Stoltzfus, Larisa and Emani, Murali and Lin, Pei-Hung and Liao, Chunhua},
	month = nov,
	year = {2018},
	note = {3 citations (Semantic Scholar/DOI) [2022-07-29]},
	pages = {45--49},
}

@article{liang_evolutionary_2000,
	title = {{EVOLUTIONARY} {MONTE} {CARLO}: {APPLICATIONS} {TO} {C} p {MODEL} {SAMPLING} {AND} {CHANGE} {POINT} {PROBLEM}},
	volume = {10},
	issn = {1017-0405},
	shorttitle = {{EVOLUTIONARY} {MONTE} {CARLO}},
	url = {http://www.jstor.org/stable/24306722},
	abstract = {Motivated by the success of genetic algorithms and simulated annealing in hard optimization problems, the authors propose a new Markov chain Monte Carlo (MCMC) algorithm called an evolutionary Monte Carlo algorithm. This algorithm has incorporated several attractive features of genetic algorithms and simulated annealing into the framework of MCMC. It works by simulating a population of Markov chains in parallel, where a different temperature is attached to each chain. The population is updated by mutation (Metropolis update), crossover (partial state swapping) and exchange operators (full state swapping). The algorithm is illustrated through examples of Cp-based model selection and change-point identification. The numerical results and the extensive comparisons show that evolutionary Monte Carlo is a promising approach for simulation and optimization.},
	number = {2},
	urldate = {2022-07-29},
	journal = {Statistica Sinica},
	author = {Liang, Faming and Wong, Wing Hung},
	year = {2000},
	note = {Publisher: Institute of Statistical Science, Academia Sinica},
	pages = {317--342},
}

@article{gilks_language_1994,
	title = {A {Language} and {Program} for {Complex} {Bayesian} {Modelling}},
	volume = {43},
	issn = {00390526},
	url = {https://www.jstor.org/stable/10.2307/2348941?origin=crossref},
	doi = {10.2307/2348941},
	abstract = {Gibbs samplinghas enormouspotentialforanalysingcomplexdata sets.However,routineuse ofGibbs sampling has beenhamperedby thelack ofgeneralpurposesoftwareforitsimplementationU.ntilnow all applicationshave involvedwritingone-offcomputercode in low or intermediatleevellanguagessuch as C or Fortran.We describe somegeneralpurposesoftwarethatwe are currentlydevelopingforimplementinGgibbs sampling:BUGS (Bayesian inferenceusing Gibbs sampling).The BUGS systemcomprisesthree components:first,a natural language for specifyingcomplex models; second, an 'expert system'for deciding appropriatemethods for obtaining samplesrequiredby the Gibbs sampler;third,a samplingmodule containingnumericalroutinesto performthe sampling.S objectsare used fordata inputand output.BUGS is writtenin Modula-2 and runsunderboth DOS and UNIX.},
	language = {en},
	number = {1},
	urldate = {2022-07-29},
	journal = {The Statistician},
	author = {Gilks, W. R. and Thomas, A. and Spiegelhalter, D. J.},
	year = {1994},
	note = {722 citations (Semantic Scholar/DOI) [2022-07-29]},
	pages = {169},
}

@article{geyer_markov_nodate,
	title = {Markov {Chain} {Monte} {Carlo} {Maximum} {Likelihood}},
	abstract = {Markov chain Monte Carlo (e. g., the Metropolis algorithm and Gibbs sampler) is a general tool for simulation of complex stochastic processes useful in many types of statistical inference. The basics of Markov chain Monte Carlo are reviewed, including choice of algorithms and variance estimation, and some new methods are introduced. The use of Markov chain Monte Carlo for maximum likelihood estimation is explained, and its performance is compared with maximum pseudo likelihood estimation.},
	language = {en},
	author = {Geyer, Charles J},
	pages = {8},
}

@article{lee_utility_2010,
	title = {On the utility of graphics cards to perform massively parallel simulation of advanced {Monte} {Carlo} methods},
	volume = {19},
	issn = {1061-8600},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3191530/},
	abstract = {We present a case-study on the utility of graphics cards to perform massively parallel simulation of advanced Monte Carlo methods. Graphics cards, containing multiple Graphics Processing Units (GPUs), are self-contained parallel computational devices that can be housed in conventional desktop and laptop computers and can be thought of as prototypes of the next generation of many-core processors. For certain classes of population-based Monte Carlo algorithms they offer massively parallel simulation, with the added advantage over conventional distributed multi-core processors that they are cheap, easily accessible, easy to maintain, easy to code, dedicated local devices with low power consumption. On a canonical set of stochastic simulation examples including population-based Markov chain Monte Carlo methods and Sequential Monte Carlo methods, we nd speedups from 35 to 500 fold over conventional single-threaded computer code. Our findings suggest that GPUs have the potential to facilitate the growth of statistical modelling into complex data rich domains through the availability of cheap and accessible many-core computation. We believe the speedup we observe should motivate wider use of parallelizable simulation methods and greater methodological attention to their design.},
	number = {4},
	urldate = {2022-07-29},
	journal = {Journal of computational and graphical statistics : a joint publication of American Statistical Association, Institute of Mathematical Statistics, Interface Foundation of North America},
	author = {Lee, Anthony and Yau, Christopher and Giles, Michael B. and Doucet, Arnaud and Holmes, Christopher C.},
	month = dec,
	year = {2010},
	pmid = {22003276},
	pmcid = {PMC3191530},
	pages = {769--789},
}

@article{hansmann_parallel_1997,
	title = {Parallel tempering algorithm for conformational studies of biological molecules},
	volume = {281},
	issn = {0009-2614},
	url = {https://www.sciencedirect.com/science/article/pii/S0009261497011986},
	doi = {10.1016/S0009-2614(97)01198-6},
	abstract = {The effectiveness of a new algorithm, parallel tempering, is studied for numerical simulations of biological molecules. These molecules suffer from a rough energy landscape. The resulting slowing down in numerical simulations is overcome by the new method. This is demonstrated by performing simulations with high statistics for one of the simplest peptides, Met-enkephalin. The numerical effectiveness of the new technique was found to be much better than traditional methods and is comparable to sophisticated methods like generalized ensemble techniques.},
	language = {en},
	number = {1},
	urldate = {2022-07-29},
	journal = {Chemical Physics Letters},
	author = {Hansmann, Ulrich H. E.},
	month = dec,
	year = {1997},
	note = {903 citations (Semantic Scholar/DOI) [2022-07-29]},
	pages = {140--150},
}

@article{liu_multiple-try_2000,
	title = {The {Multiple}-{Try} {Method} and {Local} {Optimization} in {Metropolis} {Sampling}},
	volume = {95},
	issn = {0162-1459},
	url = {http://www.jstor.org/stable/2669532},
	doi = {10.2307/2669532},
	abstract = {This article describes a new Metropolis-like transition rule, the multiple-try Metropolis, for Markov chain Monte Carlo (MCMC) simulations. By using this transition rule together with adaptive direction sampling, we propose a novel method for incorporating local optimization steps into a MCMC sampler in continuous state-space. Numerical studies show that the new method performs significantly better than the traditional Metropolis-Hastings (M-H) sampler. With minor tailoring in using the rule, the multiple-try method can also be exploited to achieve the effect of a griddy Gibbs sampler without having to bear with griddy approximations, and the effect of a hit-and-run algorithm without having to figure out the required conditional distribution in a random direction.},
	number = {449},
	urldate = {2022-07-29},
	journal = {Journal of the American Statistical Association},
	author = {Liu, Jun S. and Liang, Faming and Wong, Wing Hung},
	year = {2000},
	note = {Publisher: [American Statistical Association, Taylor \& Francis, Ltd.]},
	pages = {121--134},
}

@article{craiu_learn_2009,
	title = {Learn {From} {Thy} {Neighbor}: {Parallel}-{Chain} and {Regional} {Adaptive} {MCMC}},
	volume = {104},
	issn = {0162-1459},
	shorttitle = {Learn {From} {Thy} {Neighbor}},
	url = {http://www.jstor.org/stable/40592353},
	abstract = {Starting with the seminal paper of Haario, Saksman, and Tamminen (Haario, Saksman, and Tamminen 2001), a substantial amount of work has been done to validate adaptive Markov chain Monte Carlo algorithms. In this paper we focus on two practical aspects of adaptive Metropolis samplers. First, we draw attention to the deficient performance of standard adaptation when the target distribution is multimodal. We propose a parallel chain adaptation strategy that incorporates multiple Markov chains which are run in parallel. Second, we note that the current adaptive MCMC paradigm implicitly assumes that the adaptation is uniformly efficient on all regions of the state space. However, in many practical instances, different "optimal" kernels are needed in different regions of the state space. We propose here a regional adaptation algorithm in which we account for possible errors made in defining the adaptation regions. This corresponds to the more realistic case in which one does not know exactly the optimal regions for adaptation. The methods focus on the random walk Metropolis sampling algorithm but their scope is much wider. We provide theoretical justification for the two adaptive approaches using the existent theory build for adaptive Markov chain Monte Carlo. We illustrate the performance of the methods using simulations and analyze a mixture model for real data using an algorithm that combines the two approaches.},
	number = {488},
	urldate = {2022-07-29},
	journal = {Journal of the American Statistical Association},
	author = {Craiu, Radu V. and Rosenthal, Jeffrey and Yang, Chao},
	year = {2009},
	note = {Publisher: [American Statistical Association, Taylor \& Francis, Ltd.]},
	pages = {1454--1466},
}

@article{calderhead_general_2014,
	title = {A general construction for parallelizing {Metropolis}−{Hastings} algorithms},
	volume = {111},
	url = {https://www.pnas.org/doi/10.1073/pnas.1408184111},
	doi = {10.1073/pnas.1408184111},
	number = {49},
	urldate = {2022-07-29},
	journal = {Proceedings of the National Academy of Sciences},
	author = {Calderhead, Ben},
	month = dec,
	year = {2014},
	note = {111 citations (Semantic Scholar/DOI) [2022-07-29]
Publisher: Proceedings of the National Academy of Sciences},
	pages = {17408--17413},
}

@article{haldane_mi3-gpu_2021,
	title = {Mi3-{GPU}: {MCMC}-based inverse {Ising} inference on {GPUs} for protein covariation analysis},
	volume = {260},
	issn = {00104655},
	shorttitle = {Mi3-{GPU}},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0010465520301193},
	doi = {10.1016/j.cpc.2020.107312},
	abstract = {Inverse Ising inference is a method for inferring the coupling parameters of a Potts/Ising model based on observed site-covariation, which has found important applications in protein physics for detecting interactions between residues in protein families. We introduce Mi3-GPU (“mee-three”, for MCMC Inverse Ising Inference) software for solving the inverse Ising problem for protein-sequence datasets with few analytic approximations, by parallel Markov-Chain Monte-Carlo sampling on GPUs. We also provide tools for analysis and preparation of protein-family Multiple Sequence Alignments (MSAs) to account for ﬁnite-sampling issues, which are a major source of error or bias in inverse Ising inference. Our method is “generative” in the sense that the inferred model can be used to generate synthetic MSAs whose mutational statistics (marginals) can be veriﬁed to match the dataset MSA statistics up to the limits imposed by the eﬀects of ﬁnite sampling. Our GPU implementation enables the construction of models which reproduce the covariation patterns of the observed MSA with a precision that is not possible with more approximate methods. The main components of our method are a GPU-optimized algorithm to greatly accelerate MCMC sampling, combined with a multi-step Quasi-Newton parameter-update scheme using a “Zwanzig reweighting” technique. We demonstrate the ability of this software to produce generative models on typical protein family datasets for sequence lengths L ∼ 300 with 21 residue types with tens of millions of inferred parameters in short running times.},
	language = {en},
	urldate = {2022-07-24},
	journal = {Computer Physics Communications},
	author = {Haldane, Allan and Levy, Ronald M.},
	month = mar,
	year = {2021},
	note = {7 citations (Semantic Scholar/DOI) [2022-07-29]},
	pages = {107312},
}

@article{ahookhosh_efficiency_2017,
	title = {On efficiency of nonmonotone {Armijo}-type line searches},
	volume = {43},
	issn = {0307-904X},
	url = {https://www.sciencedirect.com/science/article/pii/S0307904X16305704},
	doi = {10.1016/j.apm.2016.10.055},
	abstract = {Monotonicity and nonmonotonicity play a key role in studying the global convergence and the efficiency of iterative schemes employed in the field of nonlinear optimization, where globally convergent and computationally efficient schemes are explored. This paper addresses some features of descent schemes and the motivation behind nonmonotone strategies and investigates the efficiency of an Armijo-type line search equipped with some popular nonmonotone terms. More specifically, we propose two novel nonmonotone terms, combine them into Armijo’s rule and establish the global convergence of sequences generated by these schemes. Furthermore, we report extensive numerical results and comparisons indicating the performance of the nonmonotone Armijo-type line searches using the most popular search directions on the CUTEst test collection of unconstrained problems. We finally apply the considered nonmonotone schemes to a deblurring problem to recover a blurred/noisy image.},
	language = {en},
	urldate = {2022-07-22},
	journal = {Applied Mathematical Modelling},
	author = {Ahookhosh, Masoud and Ghaderi, Susan},
	month = mar,
	year = {2017},
	note = {21 citations (Semantic Scholar/DOI) [2022-07-22]},
	keywords = {Armijo-type line search, Computational efficiency, First- and second-order black-box information, Global convergence, Nonmonotone strategy, Unconstrained optimization},
	pages = {170--190},
}

@misc{noauthor_anidude_nodate,
	title = {anidude {\textbar} {RosettaCommons}},
	url = {https://www.rosettacommons.org/user/11894/edit?pass-reset-token=SMhYCuRlsBV4p_xb8voPFucZ7qzNEm5mXuq8zHExRu4},
	urldate = {2022-07-22},
}

@article{anfinsen_principles_1973,
	title = {Principles that {Govern} the {Folding} of {Protein} {Chains}},
	volume = {181},
	issn = {0036-8075, 1095-9203},
	url = {https://www.science.org/doi/10.1126/science.181.4096.223},
	doi = {10.1126/science.181.4096.223},
	language = {en},
	number = {4096},
	urldate = {2022-07-21},
	journal = {Science},
	author = {Anfinsen, Christian B.},
	month = jul,
	year = {1973},
	pages = {223--230},
}

@article{sanyal_neuro-ising_2022,
	title = {Neuro-{Ising}: {Accelerating} {Large} {Scale} {Travelling} {Salesman} {Problems} via {Graph} {Neural} {Network} guided localized {Ising} {Solvers}},
	issn = {0278-0070, 1937-4151},
	shorttitle = {Neuro-{Ising}},
	url = {https://ieeexplore.ieee.org/document/9747927/},
	doi = {10.1109/TCAD.2022.3164330},
	abstract = {One of the most extensively studied combinatorial optimization problems is the Travelling Salesman Problem (TSP). Considerable research efforts in the past have resulted in exact solvers. However, the runtime of such hand-crafted solutions increases exponentially with problem size. Ising model based solvers have also gained prominence due to their abilities to ﬁnd fast and approximate solutions for combinatorial optimization problems. However, such Ising based heuristics also suffer from scalability as the solution quality becomes increasingly suboptimal with increase in problem size. In this work, we propose Neuro-Ising – a machine learning framework which uses Ising models to ﬁnd clusters of near-optimal partial solutions of large scale TSPs and combines those solutions by employing a supervised data driven mechanism, which we model as a Graph Neural Network (GNN). The GNN is trained from solution instances obtained through exact solvers and hence, the proposed approach generalizes to unseen problems while avoiding the run-time complexity otherwise required, if the solution is built from scratch. Using standard computing resources, our proposed framework rapidly converges to near-optimal solutions for 15 TSPs (upto ∼ 5k cities) from the TSPLib benchmark suite. We report ∼ 10.66× speedup over Tabu Search for 8 problems. Furthermore, compared to two state-of-the-art clustering-based TSP solvers, Neuro-Ising achieves ∼ 38× faster convergence along with ∼ 8.9\% better quality of solution, on average.},
	language = {en},
	urldate = {2022-07-21},
	journal = {IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems},
	author = {Sanyal, Sourav and Roy, Kaushik},
	year = {2022},
	pages = {1--1},
}

@article{glorot_understanding_nodate,
	title = {Understanding the difﬁculty of training deep feedforward neural networks},
	abstract = {Whereas before 2006 it appears that deep multilayer neural networks were not successfully trained, since then several algorithms have been shown to successfully train them, with experimental results showing the superiority of deeper vs less deep architectures. All these experimental results were obtained with new initialization or training mechanisms. Our objective here is to understand better why standard gradient descent from random initialization is doing so poorly with deep neural networks, to better understand these recent relative successes and help design better algorithms in the future. We ﬁrst observe the inﬂuence of the non-linear activations functions. We ﬁnd that the logistic sigmoid activation is unsuited for deep networks with random initialization because of its mean value, which can drive especially the top hidden layer into saturation. Surprisingly, we ﬁnd that saturated units can move out of saturation by themselves, albeit slowly, and explaining the plateaus sometimes seen when training neural networks. We ﬁnd that a new non-linearity that saturates less can often be beneﬁcial. Finally, we study how activations and gradients vary across layers and during training, with the idea that training may be more difﬁcult when the singular values of the Jacobian associated with each layer are far from 1. Based on these considerations, we propose a new initialization scheme that brings substantially faster convergence.},
	language = {en},
	author = {Glorot, Xavier and Bengio, Yoshua},
	pages = {8},
}
