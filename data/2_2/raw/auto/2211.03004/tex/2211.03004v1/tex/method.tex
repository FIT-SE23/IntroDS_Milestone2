\section{Bringing FPAR in the wild}

%As we anticipated in the previous sections, egocentric vision demonstrated to be a promising solution for human-centered embedded AI, thanks to the portability of sensing (e.g. glasses with cameras), to the intrinsic attention mechanism driven by the wearer gaze, and the potential that may have for human-robot cooperation tasks.
%However, despite the efforts of the community, most of the solutions provided so far consider very limited scenarios, such as off-line working modes and trimmed data sources. In addition, wearable devices such as smart glasses are only used as passive data sources, while all the computation is performed remotely (with no resources constraint). 
To really enable the deployment of egocentric vision models, it is fundamental to consider a variety of constraints in terms of energetic, memory and temporal budget. The first (and foremost) of these is the amount of resources required to perform the task, namely the memory size to store model parameters and input data, and the number of operations (e.g. MACs) required to perform inference. The first is a constraint imposed by the flash memory of the device, while the second is related to the micro-controller velocity in inference, and to the frame-rate required by the task. 
%Very often, models for %egocentric action recognition
%FPAR do not pose constraints on architecture footprint, keeping the focus toward a mere increase of model accuracy. %However, recently several works contributed toward the development of trade-off between model accuracy and resources requirements in a variety of different tasks, such as image \cite{cavagnero2022freerea} and video classification \cite{feichtenhofer2020x3d, kondratyuk2021movinets}. 

The input specification is another important feature to consider for real-world applications. In this regard, the goal is to find a good trade-off between: i) the amount of information needed as input to properly encode the temporal information; ii) the corresponding memory increase for storing input data on the device; and iii) the critical fact that, unlike the spatial dimension, the temporal dimension %does not have a finite length but 
is presented as a continuous stream, which prevents an efficient sub-sampling and requires online processing. Another important aspect to consider when posing real-time constrains is that, in the context of egocentric vision, many techniques attain notable results only by leveraging non-real-time secondary modalities such as the optical flow. Although this modality is highly successful, it has a high computational cost \cite{crasto2019mars, plizzari2022e2}, which prevents its use in real-time applications, and increases the size of the model.

It is also worth reporting that, because the sensor is worn by the user - usually at the head level - it records data with a high degree of variation produced by rapid changes in background, environment, perspective, and illumination as in Fig \ref{img:unseen}. Their effects, typically referred to as \emph{environmental bias} or \emph{domain shift} from the training (seen) to the test domains (unseen), cause changes in data distribution that can have a negative impact on the model's performance. Studying the network capability to generalize across domains provides clues on how the model will perform in a real scenario (where domain shifts are present). 

The last point of interest for a real deployment of egocentric technologies in the wild lies on how data input are structured. Indeed, the vast majority of works of action recognition assume that the input clips are  ``trimmed''  around the action of interest, which clearly represent an invasive form of supervision not available in realistic settings. Therefore, we argue that, despite recent progress in the area, trimmed action recognition has limited relevance in real-world scenarios, while continuous video flows with no previous knowledge on action location in time are the primary input source to be considered. 

Model size, online recognition, robustness across domains, and untrimmed data source represent the constraints that realistic usecases pose, and - to the best of our knowledge - no work in the literature investigates general solutions appropriate for this setting. In this paper, beyond proposing a new line of research, we investigate a solution to bring existing  FPAR models to perform online action recognition without introducing further training, thereby promoting the repurposing of existing models.
 

% As a result, there is a far greater degree of variation in illumination, perspective, and surroundings than with a fixed third person camera. Despite multiple papers in the subject, one key issue in egocentric action identification remains unresolved, known as "environmental bias." This issue originates from the network's dependency on the context in which the activities are recorded, which limits the network's ability to recognize actions performed in unfamiliar (unseen) surroundings. Analyzing this element is critical because it allows us to understand how a network performs in a real-world context where training and test data do not have the same distribution. A secondary result of this type of analysis, it is that it reveals the network's ability to focus on the motion contained in the video rather than the background cues and object texture, which are often uncorrelated with the action being performed and hence vary greatly in different environments.

%\begin{enumerate}

%    \item \textbf{Limited model size} \checkmark
%        \begin{itemize}
%            \item Networks used in the context of egocentric action recognition are often "large" and are not designed to run on devices in terms of size, memory, and power.
%        \end{itemize}
        
%    \item \textbf{Offline vs Streaming vs Online} \checkmark
        %\begin{itemize}
            %\item  The sampling strategy is another characteristic to analyze for real-world application. Due to the redundancy in time as well as the restricted computing budget in practice, it is infeasible  and unnecessary to feed the entire video into the model. It is also important to consider that online applications should work with continuous input and, as result, they should provide a continuous output, not just one at the end of the video. How sample a small subset of frames is very important for developing a practical video recognition system. In the literature, the two most widely used sampling strategies are uniform and dense sampling. The first consists of dividing a video into equal-length segments and then randomly picking one frame from each segment. Although this method appears in a large number of studies, it is based on the strong assumption that the correct length of the samples is known in advance. Furthermore, this solution limits the possibility properly understand the velocity of the action itself. For this reasons, this technique is unsuitable for an online scenario in which the desired output should be highly responsive and continuous and the proper duration of the samples is unknown. The second technique, dense sampling, consists to take a set of continuous frames as input. Even if the amount of frames used is an important parameter that considerably determines computational cost and performance, this technique is more adaptive to the online environment.
          
         %   \item Another important factor to consider when addressing "real time" is that, in the context of egocentric vision, many techniques attain outstanding performance only by leveraging non-real time secondary modality such as Optical Flow. Although this modality is highly successful, it has a high computing cost, which restricts its use in real-time applications, and it also increases the size of the model to process many inputs.
        %\end{itemize}
        
%    \item \textbf{Cross Domains Analysis} \checkmark
        %\begin{itemize}
        %    \item 
        %     In egocentric vision, the recording equipment is worn by the observer and it moves around with her. As a result, there is a far greater degree of variation in illumination, perspective, and surroundings than with a fixed third person camera. Despite multiple papers in the subject, one key issue in egocentric action identification remains unresolved, known as "environmental bias." This issue originates from the network's dependency on the context in which the activities are recorded, which limits the network's ability to recognize actions performed in unfamiliar (unseen) surroundings. Analyzing this element is critical because it allows us to understand how a network performs in a real-world context where training and test data do not have the same distribution. A secondary result of this type of analysis, it is that it reveals the network's ability to focus on the motion contained in the video rather than the background cues and object texture, which are often uncorrelated with the action being performed and hence vary greatly in different environments.
        %    \end{itemize}
%    \item \textbf{Trimmed vs Untrimmed } \checkmark
         %\begin{itemize}
         %   \item The conventional action recognition protocols assume that the input videos be "trimmed," which means that a short video sequence is sampled from beginning to end of the activity. We suggest that, despite recent progress in the area, trimmed action recognition has limited relevance in real-world scenarios where dealing with "untrimmed" video inputs is necessary and it cannot be assumed that the exact times in which the action would begin and stop are known at test time. A further step in this direction is taken by investigating the tasks of "activity detection" and "action segmentation," which deal with extended untrimmed video and attempt to individualize the action's starting point. This problem is frequently investigated offline or without regard for any model size constraints, without regard for a wearable device as the target machine.
        %\end{itemize}       
    
%\end{enumerate}

%\begin{figure*}[!htb]
%\minipage{0.32\textwidth}
%  \includegraphics[width=\textwidth]%\endminipage\hfill
%\minipage{0.32\textwidth}
%  \includegraphics[width=\textwidth]{img/D2.jpg}
%\endminipage\hfill
%\minipage{0.32\textwidth}%
%  \includegraphics[width=\textwidth]{img/D3.jpg}
%\endminipage
%\caption{In the three images above it is shown how the same action, i.e. mix, can be perceived in different manners by the model due to the presence of different kitchens in the dataset. In this context a model trained on a kitchen is likely to perform better in the domain observed at train time (also mentioned as \textit{seen}) and worse on the others (\textit{unseen})}
%\label{img:unseen}
%\end{figure*%}






%\begin{figure*}[!htb]
%    \centering
%    \label{fig:overview}
%    \includegraphics[width=1\linewidth]{img/overview_bozza.pd%f}
%    \caption{\textbf{placeholder} a) \textbf{Offline}: Uniform (U) and Dense(D). b) \textbf{Streaming}:Temporal Windows of $T_{s}$ frames ($TW_{t_{s}}$), features extractor (F), classifier (C), aggregator (A), reset aggregator (R). c) \textbf{Online}: }
%\end{figure*}



\subsection{Benchmarking FPAR with real-world constraints.} 
\par
%To build a model suitable for real world applications, we move from the state-of-the-art in egocentric action recognition, which we extensively test for model footprint and usability on-line. For fair comparison and to assess the generalization capabilities, we test all the models in seen and unseen condition. All the constraints are included incrementally, following a complexity criterion (Offline, Streaming, and Online).
%
As a first step, we tackle the model footprint issue and assess the impact of the model reduction by comparing it with popular action recognition networks, testing their generalization capabilities in seen and unseen settings (domain shift). Then, following an increasing complexity order, all real-world restrictions are added sequentially (Streaming, Online, and Untrimmed).

%The purpose of this paper is to provide an overview of how existing action recognition systems respond to the challenges of real-world application. We are especially interested in the model complexity study of various action recognition systems. 
%All studies are carried out in order to demonstrate performance in both seen and unseen conditions. 
%In order to thoroughly investigate the impact of the realistic scenario of continuous input data fed to the model, we chose to investigate several settings of increasing complexity, namely Offline, Streaming, and Online. 
%Finally, we test the models on the untrimmed videos directly in the last section of our benchmark, analysing also the impact of performances in terms of latency, power consumption and other important characteristics fundamental for the a designed device.
%Then, we move to the more challenging untrimmed scenario, where data input does not come with actions temporal bounds. For all the conditions, we report latency, power consumption, and model accuracy.
%Finally, in the last section of our benchmark, we directly test the models on the untrimmed videos, measuring the performance in terms of latency, power consumption, and other essential elements to take into consideration for a wearable target device.
%description of our analysis: backbone used / Setting (seen vs unseen) / Sampling + Real time  / Sensitivity (Dependency) to the boundary of the action / energy and memory requirements 

\begin{figure}[t]
    \centering
    \includegraphics[width=0.9\linewidth]{img/streaming.pdf}
    \vspace{-0.6cm}
    \caption{
    An Illustration of the Streaming scenario. $TW_{T_{s}}$ represents a temporal window sliding along the video with stride 1, taking at each time step a clip of $T_{s}$ contiguous frames and feeding it to the network. The model is composed of a feature extractor $F$ and a classifier $C$ with $n$ classes ($C_{1}$, $C_{2}$, ... ,$C_{n}$). $A$ represents the aggregator that - at each step - updates the output of the network, taking into consideration the current output and the previous ones. R stands for aggregator cleaning, triggered by the last frame of the sample.
    }
    \label{fig:overview}
    \vspace{-0.5cm}
\end{figure}

\textbf{Backbone. }
To assess the effects of model footprint on task accuracy, we considered several 
%state-of-the-art 
2D-CNN and 3D-CNN models for action recognition, which are often used in the context of egocentric vision, including I3D \cite{carreira2017quo}, TSN \cite{wang2018temporal}, TSM \cite{lin2019tsm} and TRN \cite{zhou2018temporal}. These typical action recognition architectures are compared with two families of NAS-based models which optimize model efficiency: \cite{feichtenhofer2020x3d} and \cite{ kondratyuk2021movinets}. From these, we considered for our purposes the smallest versions, named X3D-XS and MoViNet-A0 (with and without buffer) respectively. %This is the first comparison we performed in order to understand how much the size of the model utilized affects the performance in the context of egocentric vision.
%
%\textbf{Seen $\rightleftharpoons$ Unseen. } 
In our experiments, we tested the backbones on both seen and unseen data distributions (e.g. different environments). Albeit this is very often omitted, testing across domains is fundamental to assess the generalization capabilities of models and can highlight overfitting occurrence on specific data distributions. 
%In our analysis, we address another fundamental aspect of the action recognition architecture that is typically neglected in standard action recognition benchmarks but allows us to understand how one approach works under real-world constraints.
%Indeed, common action recognition methods are often tested by focusing solely on the architecture's performance in unreal scenarios where the data distribution of the training is identical to the test one, i.e., the seen scenario. We feel that this situation does not allow us to properly evaluate the action recognition architecture since we cannot answer the questions, \textit{how well does it generalise to new domains?} %And indirectly, \textit{how well does it extract motion information to determine what action it is?} 
%For this reason, in our analysis, we conduct experiments in both seen and unseen scenarios.

\textbf{Offline, Streaming, and Online. }% Ideally, a real-world application should be able to deal with a continuous flow of information and provide a continuous output with low latency. 
As anticipated before, the standard action recognition protocol usually works in an offline fashion, exploiting the supervision information on the edges of the action (start and end) to take the right input to process. 
To do this, specific sampling strategies are needed to reduce the amount of input data and avoid that a sample $x$ of $T$ frames cause a model to exceed its memory budget. Most works address this issue relying on uniform sampling, i.e picking $T_s$ equidistant frames, with  $T_s < T$. This method is the preferable solution for video understanding, but suffers of two major drawbacks: i) it assumes the knowledge of samples length in advance (which is not the case for continuous streams); and ii) uniform sampling completely filters out information related to the action velocity. Other works, instead, rely on dense sampling, selecting a set of $T_{s}$ contiguous frames. In some cases, this choice penalizes the model due to the fact that its temporal receptive fields may see only a limited portion of the action. Indeed, the final prediction is usually obtained  averaging the predictions of different equidistant clips over the whole video, requiring the sample's length information as for uniform sampling to obtain a video level prediction.

%
% old version
%This offline setting  also aims to bring to light the impact of the sampling strategy in the action recognition frameworks. Most works rely on uniform sampling of the input segment, which produces very accurate results but has two significant drawbacks: i) the user must know the length of the sample in advance (which is not the case for continuous streams); and ii) uniform sampling completely filters out information related to the action velocity. Other works, on the other hand, rely on dense sampling, which entails taking a set of contiguous frames and directly feeding them as model input.Although this choice clearly penalizes the model, which may see only a limited portion of the action, considering the impossibility of processing all input simultaneously due to the limited memory capacity. Usually, to limit this problem, the final prediction is obtained  averaging the predictions of different equidistant clips over the whole video (requiring the sample's lengths information as for the uniform sampling) in order to obtain a video level prediction.
%
%Although the performance obtained with this sampling strategy is usually lower than with uniform sampling, it represents the only valid solution to enable a continuous and online use of the model for our setting.
%
The artificial limits of offline approaches are alleviated in two novel cases. The first, hereinafter named \textit{Streaming}, still assumes the knowledge of action boundaries but enables the processing of a continuous input stream (see Fig. \ref{fig:overview}). Intermediate outputs are continuously collected with an aggregator ($A$ in Fig. \ref{fig:overview}) which is then used to obtain a final prediction. When the action is completed (i.e. at the final frame), the aggregator is flushed to reset the model for novel predictions. 
%
%
%To overcome the limitations of offline methods, a first step is made with what we call a \textit{Streaming} setting. The idea is to continuously feed the video stream and gather the corresponding inference on-the-fly. %To reduce memory requirements, a viable solution could be to rely on a sliding window ($TW_{T_{s}}$) with unitary stride which progressively select $T_{s}$ dense frames, as in Fig \ref{fig:overview}. For each time sample, the model uses the updated input time window to infer the observed action. Then, the continuous output is aggregated to obtain a final prediction.
%The new setting we explore is called \textit{Streaming}; it is used to see how the model handles continuous input, given the difficulties of saving the entire video. This new scenario is characterized by two aspects, a sliding window and an aggregation strategy. The first one consists of moving a temporal window along the video with stride 1 to choose $T_{s}$ dense frames to provide as input to the model, resulting in a continuous output. Then, the continuous output is aggregated to obtain a final prediction.
%
%The Streaming case, however, is not fully applicable because it exploits supervised knowledge on exact start and end frames of each action.
To relax also the assumption on action boundaries (i.e. no prior knowledge on when to reset the aggregator), we introduce the \textit{Online} setting, where the model is asked to identify both actions and their (rough) temporal edges. 
The complexity of this setting requires to deal with untrimmed data when actions are alternated with ``unknown'' clips. In our experiments, we studied the online settings with and without ``unknown'' clips, to verify how their presence affects the final performance. %It is a simulated version of the dataset in which the actions are linked consecutively, one after the other, with no unknown clips or overlap segments.
%
%The last scenario, named \textit{Online}, introduces further difficulties. That is, the loss of the boundary of the action during the evaluation. It means that the model cannot exploit this information to appropriately distinguish the end of one action from the beginning of another one.
%The major motivation underlying the extension to the Online setting relies on the observation that the fine-grained limits of an action in a continuous stream of data are not unique. We can easily recognize action when we see it, but determining the start and end of fine-grained action in a continuous stream is complex and extremely subjective. For this reason, in our experiments we put the focus on the ability to recognize the action when it happens instead of the precise localization of their boundaries. In other words, standard action recognition protocols need a strategy to deal with the loss of boundary supervision and, at the same time, the boundaries identification impacts the performances at test time.

%\textbf{Trimmed $\rightarrow$ Untrimmed. } The final step of our analysis focuses on the relaxation of the assumption of trimmed input data. Specifically, we assess the capability of models (pre-trained on trimmed data) to tackle also untrimmed videos, where actions are interleaved by "unknown" clips. In this case, we also test the robustness of the  \textit{Online} model in the identification of actions also when these are inserted in a more general context.

%considers the untrimmed part of the video to observe how this aspect negatively affects the performance of the original model. The goal is only to understand how the performance is influenced by the untrimmed part. For this kind of analysis, we don’t consider the re-training of the model in order to understand or detect the class "no-action" as in the action detection or segmentation tasks, but we think that this study is an intermediate analysis that will move in the future toward this direction, bringing with it all the constraints discussed in this work.

\section{Methodology}

\subsection{%Managing a continuous input stream
From single clips to continuous data streams}
We extended the offline approach to deal with continuous streaming input by using a sliding window ($TW_{T_{s}}$) with a unitary stride that selects $T_{s}$ dense frames progressively (see Fig. \ref{fig:overview}). For each time sample, the oldest frame (red in Fig. \ref{fig:overview}) is removed and replaced by a new one (blue in Fig. \ref{fig:overview}), and a new inference is performed and accumulated with the previous ones. Then, a continuous output is obtained with an aggregator strategy (aggregator(A) in Fig \ref{fig:overview}).
MoViNet implements its aggregator by replacing 3D convolution with the (2+1)D operation and exposing a stream buffer mechanism to cache feature activations, allowing the temporal receptive field to expand without the need for recomputation. To support frame-by-frame output and exploit the buffer mechanism, it uses Causal Convolutions and Cumulative Global Average Pooling. The first one is used to make the convolutions unidirectional along the temporal dimension. The second one, instead, approximates any global average pooling involving the temporal dimension. For models lacking specific aggregator mechanisms, we implemented a continuous averaging of the corresponding temporal window’s output. Each aggregator is empty at the beginning of each sample and is resetted (R) at the end.
%%We extended the offline approach to deal with continuous streaming input by adding an aggregator (A), which consists of averaging a continuous output generated by moving a temporal window ($TW_{T_{s}}$) of $T_{s}$ frames throughout the whole sample video and forwarding a continuous updating clip at the network's input. The \textit{updating} clip consists of removing the oldest saved frame (red frames of Fig. \ref{fig:overview}) and adding the new frame (blue frames of Fig. \ref{fig:overview}) accumulated with the stored ones. This method was used for architectures that did not have their own aggregator to deal with this scenario. 
%On the other hand, MoViNet implements its aggregator by replacing 3D convolution with the (2+1)D operation and exposing a stream buffer mechanism to cache feature activations, allowing the temporal receptive field to expand without the need for recomputation, as in I3D or X3D. To support frame-by-frame output and exploit the buffer mechanism, it uses the Causal Convolutions, making them unidirectional along the temporal dimension, and Cumulative Global Average Pooling, to approximate any global average pooling involving the temporal dimension.

\subsection{Actions boundaries localization}
\label{eventboundarydetection}
\par 
%For automatic edges detection, we implemented a Dynamic Boundary Localization (DBL) strategy with almost no overhead in terms of model size and latency. More specifically, we rely on the fact that, for models trained with a cross entropy loss, different classes are well-separated in features space \cite{wang2017normface}. Therefore, in principle it is possible to detect action changes in a continuous data flow by checking anomalies of features distribution in time. 

%Of note, this method, implemented with one single aggregator (or buffer), could not only reveal changes between known actions, but also detect the presence of unknown" segments of untrimmed video (e.g. no-action or novel ones). However, it is important to mention that very often subsequent actions may partially overlap (up to 28.1\% of the whole time slot in Epic-Kitchen \cite{damen2021rescaling}). In this case (depicted in Fig. \ref{fig:overview}), performing inference with a single features aggregator results in a preference toward the first action (because more represented in the buffer). 
%
%To address those issues, in this work we propose a two-fold aggregator ($A^2$) approach based on dynamic boundary localization (DBL).
%
%\textbf{Twofold Aggregator ($A^2$).}
%When the starting point of one action differs from the end point of the previous one, we see that the standard  aggregator is ineffective. This aspect is due to the fact that the overlapping part of the video with multiple actions is used to encode only one of the two actions (usually the first one), negatively affecting the encoding of the second one. 
%To solve this problem, we introduced a twofold aggregator strategy ($A^2$) . The two aggregators ($A_1$ and $A_2$ in Fig. \ref{fig:oursolution}) run asynchronously using a mixed boundary detection approach, allowing the next operation to begin encoding before the previous one finishes. The intuition behind our implementation is that, for each frame of the stream, if we detect an anomaly in the features w.r.t. previous inferences we clean only ....\rev{qui dobbiamo dire come funziona il clean dei due buffer. Anche nella figura, dovremmo aggiungere qualche info e ripensare la caption. ci provo domani.}
%
%The final output is obtained as:
%\begin{equation}
%\mathcal{O} = n_{1}A_{1}(x) + n_{2}A_{2}(x)
%\end{equation}
%where $A_{i}(x)$ is the output of the \textit{i-th} aggregator for the input $x$ and $n_{i}$ correspond on the quantity of frame processed by the \textit{i-th} aggregator.

%As anticipated before, action recognition models are trained to classify the action on a single clip. Transferring this capability to continuous video flows comes with the difficulty that the model may be asked to infer on clips that not necessarily contains well-separated complete actions, as in the training dataset. Therefore, the model may suffer of significant differences in features patterns between training and test data, resulting in an overall increasing of prediction uncertainty and instability in time. To solve this issue, we implemented a Dynamic Boundary Localization (DBL) strategy - with almost no overhead in terms of model size and latency - capable to localize the boundary of an action by examining the continuous stream of extracted features and calculating their distance from the normal pattern.
%More specifically, the well-separated actions observed during training allow the network to learn a normal representation \cite{}, as opposed to the overlapping action encoding observed during the continuous stream. At the same time, models trained with cross-entropy loss promote class representations to be well separated in feature space \cite{wang2017normface}, enabling the use of a distance metric to identify the start of a new action as feature variation. As a result, it is in principle possible to identify action changes in a continuous data flow by looking for abnormalities in feature distribution over time while treating all frames of the same action as normal. This method could not only reveal differences between known actions, but also detect the presence of "unknown" segments of video (e.g. background or novel ones).


As anticipated before, action recognition models are trained to classify well-separated actions taken as input. Transferring this capability to continuous video flows comes with the difficulty that the model may be asked to infer from clips that do not necessarily contain separate and complete actions. Therefore, the continual encoding of successive actions results in an overall increase in prediction uncertainty and instability in time. The anomaly detection literature \cite{chandola2009anomaly} describes this behavior as a consequence of the fact that the network processes data with a pattern that does not conform to the defined notion of \textit{normal} data learned during training. Therefore, the presence of concurrent or unknown action can be seen as an anomaly in time. Based on this consideration, we implemented a Dynamic Boundary Localization (DBL) strategy - with almost no overhead in terms of model size and latency - to localize the boundary of an action by examining the continuous stream of extracted features.

More specifically, leveraging on the fact that cross-entropy loss (de facto standard for FPAR) promotes class representations to be well separated in feature space \cite{wang2017normface}, it is possible to use a distance metric (e.g. Mean Square Error, MSE) between the features extracted to measure their variations caused by action changes. Therefore, it is possible to identify action boundaries in a continuous data flow by looking for abnormalities in feature distribution over time while treating all frames of the same action as \textit{normal}. This method could not only reveal differences between known actions but also detect the presence of "unknown" segments of video (e.g., background or novel ones).

However, it is important to note that in case of overlapping segments (which e.g. in the Epic-Kitchen dataset \cite{damen2021rescaling} reaches up to 28.1\% of the total clip time), the detection of the new class could be delayed or anticipated with respect to the current action. In this case, since the aggregator solution can encode only one action at a time, the network's inference will favor one of the two consecutive actions.

In light of the above considerations, we can state that for fine-grained action recognition, the standard aggregator may be ineffective. To solve this problem, we introduced a two-fold aggregator strategy ($A^2$). The two aggregators ($A_1$ and $A_2$ in Fig. \ref{fig:oursolution}) run asynchronously using a mixed boundary detection approach, allowing the encoding of the next action before the previous one finishes. When one aggregator detects an anomaly, it disables its DBL and activates the DBL of the second aggregator. To guarantee asynchrony in the moment of the anomaly's detection, we delay the activation of the second aggregator's DBL by an hyperparameter $\delta$.


The final output is obtained as:
\begin{equation}
\mathcal{O} = n_{1}A_{1}(x) + n_{2}A_{2}(x)
\end{equation}
where $A_{i}(x)$ is the output of the \textit{i-th} aggregator for the input $x$ and $n_{i}$ corresponds on the quantity of frame processed by the \textit{i-th} aggregator.

\begin{figure}[t]
    \centering
    \includegraphics[width=1\linewidth]{img/oursolution.pdf}
     \vspace{-0.55cm}
    \caption{Illustration of the proposed two-fold aggregator ($A^{2}$) method. The two aggregators work asyncronsly, $\delta$ is a parameter used to guarantee the asyncroncity of the two and indicates the frame-delay of the DBL activation of one aggregator when the other one detects an anomaly.}
     \label{fig:oursolution}
    \vspace{-0.6cm}
\end{figure}
%Moreover, this solution should be able to deal with i) the presence of "unknown" segments of untrimmed video, which might include new actions not found during training or "no-action" segments (background) and ii) a high percent of overlap segments (28.1\% \cite{damen2021rescaling}), i.e., the beginning and/or the end of one action intersects the beginning and/or the end of another one. In particular, the direct consequence of the second issue results in a reduction each sample length, affecting the temporal information that the model may encode. 
%
%To address those issues, in this work we propose a two-fold aggregator ($A^2$) approach based on dynamic boundary localization (DBL).
%
%The dynamic boundary localization (DBL) is based on the assumption that if a model can recognize an action, it will be able to locate its boundaries in relation to the new action. 
%The sequence of frames unrelated to that current action will appear as an anomaly. Indeed, when training an model, by minimizing the cross entropy loss to solve the classification task, it promotes well-separated features \cite{wang2017normface}, i.e. frames of different classes have different features spatially separated in the embedding space. 
%
%As a result, measuring the distance between features frames offers the opportunity to detect frames that relate to different actions. Precisely, when the distance is greater than a specific threshold, we may localize a new action event. Because each sample during training is composed of frames from the same action, frames from the new action observed sequentially will be identified as an anomaly by the model.
%
%



%\textbf{Uniform vs Dense sampling}
%Another aspect which must be carefully taken into account is the sampling strategy adopted. 
%\begin{itemize}
%    \item The constraint of real-time evaluation implies that the network should be able to provide output at the frame-or clip-level without any prior knowledge of the exact length of the sample, considering that this knowledge is not present in a continuous stream of data acquired during daily activities. As a result, the model cannot use a uniform sampling approach but must instead use a dense protocol, and we aim to investigate the impact of this choice on performance. Furthermore, the standard dense protocol contains some constraints that should be considered in our context. To use dense sampling, we require a k-frame sliding window that slides frame by frame to provide an output for each frame. Although this approach appears to be suitable for online video analysis, the size of the sliding windows may impose several limits in practice: I) If the temporal window is too big, it suggests storing a large number of frames in memory, increasing the model's memory requirements (without not considering the latency for the first output) II) If the temporal window is too small, it will most likely be unable to capture the activity and will be difficult to use appropriately. 
%\end{itemize}




%\textbf{Sensitivity (Dependency) to the boundary of the action.}

%\begin{itemize}
%    \item In the last part, we concentrate on MoViNet since it possesses the properties that are closest to those ideals for guiding and motivating the research in this context.
%\item We first examine the network's sensitivity to the action boundary, demonstrating how the start and finish of action for fine-grained action recognition is just a soft restriction and not a hard one that must be respected. 
%\item Furthermore, we analyze the impact of the buffer under real-world conditions in which the cleanliness of the buffer must be automatic and not provided by supervision of the start and end of the action itself. 
%\item Finally, we expand the study in the untrimmed direction, assessing the impact of the existence of "no-action" segments in the video. The purpose of this section is to explore how no-action segment video impacts performance, particularly when it is present in testing but not in training.

%\end{itemize}





%\section{Datasets, Training, Evaluation Protocols}
\section{Implementation}

\textbf{Dataset.}
To perform experiments for fine-grained egocentric action recognition in both \textit{seen} and \textit{unseen} scenarios, we use the EPIC-Kitchens-55 dataset \cite{damen2018scaling} and we follow the same experimental protocol of \cite{Munro_2020_CVPR}, where the three kitchens with the most labeled samples are handpicked from the 32 available. We refer to them here as D1, D2, and D3 respectively. The difficulties arise not only from the substantial domain shift intrinsic in different kitchens, but also from an imbalance in class distribution intra- and inter-domain.


\textbf{Input.}
 Experiments with I3D~\cite{carreira2017quo} and X3D~\cite{feichtenhofer2020x3d} are conducted by sampling one random clip from the video during training and 5 equidistant clips spanning across all the video during test, as in~\cite{munro2020multi}. The number of frames composing each clip is 16. %, to maintain a reasonable computational time. 
 For TSN~\cite{wang2018temporal}, TSM~\cite{lin2019tsm} and TRN \cite{zhou2018temporal} architectures, uniform sampling is used, consisting of 5 frames uniformly sampled along the video. During testing, 5 clips per video are adopted, following the experimental protocol proposed in ~\cite{lin2019tsm}. 
 For MoVinet \cite{kondratyuk2021movinets}, dense sampling is adopted, with 4 consecutive clips composed by 8 frames, randomly taken from the video during training as in the original work.
 All the architectures follow the standard video data augmentation as in~\cite{wang2016temporal}, the spatial input resolution has been kept consistent with the pretrained models (182 for X3D, 172 for MoViNet and 224 for the others) while the temporal resolution for all the models has been set to 30 fps.
 
\textbf{Implementation Details.}
 We adopted the original I3D network proposed in~\cite{carreira2017quo} with Inception-V1 as inflated backbone, while we chose to use X3D-XS and MoViNet-A0 to have the most efficient models from the two families. The optimizer is SGD with momentum of 0.9,  weight decay $10^{-7}$ and a starting learning rate $\eta$ of 0.01. I3D has been trained for a total of 5000 iterations, the learning rate decays by 0.1 at step 3000. Instead, MoViNet-A0 and X3D-XS have been trained for 1500 iterations without learning rate decay. For all the experiments we adopted a batch size of 128.

\textbf{Evaluation Protocols. }
In this part, we discuss the evaluation protocol we used for our benchmark.%, taking into account the numerous scenarios investigated.
\\
\textbf{Seen} $\rightleftharpoons$ \textbf{Unseen}. For the seen results we train on kitchen $D_i$ and test on the same ($D_i \rightarrow D_i$), $i \in \{1,2,3\}$. We evaluate performance on unseen test by training on $D_i$ and testing on  $D_j$, with $i \neq j$ and $i, j \in \{1,2,3\}$ ($D_i \rightarrow D_j$). 
\\
\textbf{Offline, Streaming, and Online.} We  refer to \textit{offline} to indicate the standard action recognition protocol, which typically uses as input a sub-sample of the input frames. We perform experiments using the two standard sampling strategies, uniform and dense sampling. 
The term \textit{streaming} refers to experiments where the test is performed using all the frames (to simulate the continuous stream of the data that comes from a wearable device) with the supervision on the action's boundary (start and end) to properly clean the aggregator.
%prediction (or the buffer for MoViNet).
%It allows us to focus on how the architecture performs with the entire video stream and it provides an upper bound for real-time online action recognition. 
The \textit{online} setting, instead, assumes no supervision on action limits, and to effectively deal with this scenario the model should automatically detect both action and their boundaries.

\par \textbf{Trimmed $\rightarrow$ Untrimmed.} %The performance of the trimmed version is tested using the original dataset including only the  region of the video that contains an action, 
%and the accuracy is computed using the predictions at the end of each samples as in the standard action recognition framework. 
%To shift from the trimmed to the untrimmed version, two major challenges are introduced: the first is the presence of "unknown" segments that could contain new action that are never seen during training or "no-action" (background). The second is the multi-class problem caused by the lack of mutual exclusive separation from the action found in the original dataset. It means that there are some segments in which multiple actions are associated with the same portion of video, i.e., the beginning and/or the end of one action intersects the beginning and/or the end of another one. 
%Moving from the trimmed to the untrimmed scenario, the lack of mutually exclusive temporal separation from the actions found in the original dataset makes it difficult to calculate an accuracy per frame.
%For these reasons, the performance is still computed at the end of each action. For simplicity, in these experiments, the "unknown" segments are not used in the evaluation of the performance but only on the detection part. It means that we don't use "unknown"  class during the evaluation of the accuracy, but the network should be able to manage it during the clean phase of the buffer or the logits accumulation.
Moving from the trimmed to the untrimmed scenario, the lack of mutually exclusive temporal separation from the actions found in the original dataset makes it difficult to calculate an accuracy per frame. At the same time, the precise timestamp of start and end in fine-grained action is complex and extremely subjective. For this reason, we use accuracy as a metric to validate our experiments, putting the focus on the ability to recognize the action when it happens instead of the precise localization of its boundaries. For simplicity, the performance is still computed at the end of each action, and the  ``unknown'' segments are not used in the evaluation of the performance but only in the event boundary localization part. In other words, we did not use the "unknown" class during the evaluation of the accuracy, but the network should be able to manage it during the clean phase of the buffer or the logits accumulation.

