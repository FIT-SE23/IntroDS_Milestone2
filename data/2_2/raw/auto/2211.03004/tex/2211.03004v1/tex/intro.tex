\section{Introduction}

Current robotics research demonstrated an increasing interest in the development of technologies to support the physical interaction between humans and machines, ranging from the planning and control \cite{ajoudani2018progress}, up to their social impact \cite{henschel2020social}. However, the deployment of this technology in the real world % e.g. in household or industrial environments, 
requires an extension of the human intention retrieval capabilities of robots, from a mere pose estimation and forecast, to an high level description of the action executed. 
%As an example, considering a companion robot assisting the human in preparing a meal, the feasibility to infer from video the current action performed can enable the prediction of the next steps of the receipt, and eventually assist the cook with proper tools handover. 
To reach this goal, a very promising solution relies on the use of egocentric vision, in which the human activity is recorded by wearable cameras placed on the head of the user \cite{rodin2021predicting}. %In contrast with standard third person Computer Vision (CV) tasks, 
This setting comes with the benefit that source data are characterised by a rich multi-modal information, thanks to the proximity of audio/video sensors to the action scene, and by an intrinsic embedding of an attention mechanisms that stems from the human gaze direction itself.

%\begin{figure}[t]
%    \centering
%    \label{fig:kitchens}
%    \includegraphics[width=1\linewidth]{img/teaser.pdf}
%    \caption{A nice teaser goes here. \lipsum[66] }
%\end{figure}



\begin{figure}
    \centering
    %\resizebox{\linewidth}{!}{\input{tex/Tables/teaser_bubble}}
    \includegraphics[width=1\linewidth]{img/teaser.pdf}
    %\begin{flushleft}
    \begin{flushright}
    \input{tex/Tables/teaser_bubble}
    \end{flushright}
    %\end{flushleft}
    %\resizebox{\linewidth}{!}{\input{tex/Tables/teaser_hist}}
     %\includegraphics[width=1\linewidth]{img/teaser_p2.pdf}
     
    %\vspace{-0.5cm}
%\sffamily
%\renewcommand*{\arraystretch}{1.2}
%\newcommand*{\chart}[2]{%
%  #1 & \ChartBox{1*#1}{#2}%
%}
%\noindent
%\begin{adjustbox}{width={0.45\textwidth},totalheight={0.3\textheight},keepaspectratio}%
%\begin{tabular}{>{\Triangle\,}lS[mode=text,detect-family,table-format=1.1]@{\,M~}l}

%\toprule
%\multicolumn{1}{c}{%
%  \multirow{2}{*}{Model}%
%}&
%  \multicolumn{2}{c}{$Parameters$}\\
%\multicolumn{1}{c}{}& \multicolumn{2}{c}{%
%  \ChartLegend{Offline}
%  \ChartLegend{Online}
%}\\

%\midrule
%\relax EK $1^{st}$ 2022 \function{}  &
%   \chart{338}{Offline} \\
  
%\relax EK $2^{st}$ 2022 \function{} &
%  \chart{508}{Offline} \\
%  
%\relax Most Cited Works  \function{} &
%  \chart{50}{Offline} \\
%  
%\relax \textbf{Our} \function{} &
%  \chart{3}{Online} \\

%\bottomrule
%\end{tabular}
%\end{adjustbox}
%\caption[Caption for LOF]{Top line, a comparison between offline (a) and online (b) processing. Bottom line, Model size comparison between our introduced analysis, the average of models proposed in the most cited works (MCW), and the first and second place winners of the recent Epic KITCHENS challenge\protect\footnotemark. }
\caption[Caption for LOF]{\textbf{Top:} Comparison between offline (a) and online (b) protocol for first person action recognition (FPAR). \textbf{Bottom}: Frames per Second (FPS) processed with the I3D model \cite{carreira2017quo} on different devices. 
The \protect\tikz \protect\node [rectangle, pattern=north east lines, pattern color=MaterialOrange200, draw=MaterialOrange200, opacity=1] at (2.5,-2) {}; areas show traditional action recognition models' difficulty to run online on edge devices, either due to latency or hardware constraints. 
Our goal is to promote research toward models that can work in the \protect\tikz \protect\node [rectangle,draw=none, fill=pal2, opacity=.5] at (2.5,-2) {}; area, allowing egocentric models to run online and on tiny devices.
}
\label{fig:teaser}
\vspace{-0.6cm}
\end{figure}

\renewcommand{\thempfootnote}{\fnsymbol{footnote}}
\renewcommand{\thefootnote}{\fnsymbol{footnote}}

%\footnotetext{https://epic-kitchens.github.io/Reports/EPIC-KITCHENS-Challenges-2022-Report.pdf}


%\input{tex/teaser}

%The research community exploited egocentric vision for a variety of different tasks, such as human-object interaction~\cite{damen2014you,lu2021understanding}, action anticipation~\cite{abu2018will,furnari2020rolling,girdhar2021anticipative,liu2020forecasting}, action recognition~\cite{kazakos2019epic}, and video summarization~\cite{del2016summarization,lee2012discovering,lee2015predicting}. With the advent of novel large-scale datasets~\cite{damen2018scaling,damen2021rescaling}, new tasks are being proposed, such as wearer's pose estimation~\cite{wen2021seeing} and egocentric videos anonymization~\cite{thapar2021anonymizing}. %This trend will grow in the next years thanks to the very recent release of Ego4D~\cite{grauman2021ego4d}, a massive-scale egocentric video dataset offering more than 3,000 hours of daily-life activity videos accompanied by audio, 3D meshes of the environment, eye gaze, stereo, and multi-view videos.

%Yet, most of the research so far focused on improving the test accuracy of neural architectures in solving the task, proposing larger and larger models. This strategy, however, is not congenial to realistic usecases, where machines should be able to perform inference directly on-board to enable quick actions. 

%To overcome this issue, we set in this work the boundaries of realistic usecases for egocentric action recognition (model size, domain adaptation, on-line and untrimmed input data, see also Fig. \ref{fig:teaser}) and propose a model and an usage methodology which satisfies all the constraints. 


%%%%%%% new version Mirco

%Egocentric Vision appears to be a particularly appealing research area for the computer vision community. Indeed, through first person vision we can efficiently study the human behavior in a much more direct manner, opening the opportunity to learn how humans perceive and interact with the world, and transfer this knowledge toward a human-aware robotics, for a more effective human-robot cooperation.

Although many works in the literature have provided solutions to infer knowledge about human activity from egocentric data (a.k.a. First Person Action Recognition, FPAR), this is frequently achieved through very large neural architectures without regard to their computational demand (see Fig. \ref{fig:teaser}, bottom part). As a consequence, although very accurate, most of the models presented in literature %require high-performance hardware (e.g. GPUs), making hard their 
are not suitable for realistic usecases, where real-time inference (Fig. \ref{fig:teaser}, online scenario) should be performed on board of low-power hardware to enable wearability, avoid data transfer and preserve privacy.
%a device should be able to perform real-time inference on-board with low latency (Fig. \ref{fig:teaser}, online scenario)
%, avoiding data transfer to protect user privacy. 
The goal of this paper is to encourage a new line of research based on realistic egocentric vision use cases. We propose a new FPAR benchmark with real-world constraints, which consists of altering current action recognition protocol to follow a set of realistic limitations that we add progressively (model size, cross domains, online, and untrimmed). 

%To address the numerous challenges posed by real-world restrictions, 
In addition, we propose a model-agnostic technique to enable a fast re-purposing of existing architectures in this new context. Our approach consists of two components: an anomaly detection-based solution for action boundary localization, followed by a two-fold aggregator strategy. The first solution is based on the assumption that \textit{if I can recognize an action, I can also localize it}. Considering that traditional training of the action recognition framework is done with trimmed data containing single actions, the embedding that arises from multiple actions will be very different from the standard one, and as a consequence, the network will be able to detect it as an anomaly. The second solution is introduced to cope with the large proportion of overlapping segments in fine-grained action recognition that make it harder to localize concurrent actions.

To summarize, our contributions are the following:

\begin{itemize}
    \item we define a novel setting of Egocentric Action Recognition in the wild, which encourages researchers to develop applications-aware solutions;

    \item we present a new benchmark for real-world application in FPAR;

    \item we designed a solution to achieve efficient yet accurate action recognition under constraints, exploiting an anomaly detection strategy to localize the boundary of the actions and a two-fold aggregator solution to deal with concurrent actions in a continuous stream;

    \item we analyze the feasibility of deploying an egocentric vision model on a budget, opening interesting perspectives for on-board intelligence.
   
\end{itemize}

%\begin{enumerate}
%    \item \mirco{Egocentric Vision branch di ricerca che sta crescendo molto negli ultimi anni ...}
    %Egocentric vision has introduced a variety of new challenges to the computer vision community, such as human-object interaction~\cite{damen2014you,lu2021understanding}, action anticipation~\cite{abu2018will,furnari2020rolling,girdhar2021anticipative,liu2020forecasting}, action recognition~\cite{kazakos2019epic}, and video summarization~\cite{del2016summarization,lee2012discovering,lee2015predicting}. With the advent of novel large-scale datasets~\cite{damen2018scaling,damen2021rescaling}, new tasks are being proposed, such as wearer's pose estimation~\cite{wen2021seeing} and egocentric videos anonymization~\cite{thapar2021anonymizing}. This trend will grow in the next years thanks to the very recent release of Ego4D~\cite{grauman2021ego4d}, a massive-scale egocentric video dataset offering more than 3,000 hours of daily-life activity videos accompanied by audio, 3D meshes of the environment, eye gaze, stereo, and multi-view videos.
    
    %\item Until now, the focus of egocentric vision research has been on two key elements: I) the use of wearable devices to collect new data and manually labeled them to encourage future study in this area.
    %II) novel deep learning approaches for analyzing and learn useful information from first person prospective videos, in order to accomplish new problems.
    
    %\item Despite several publications in this area and the possible applications that the egocentric vision may enable, the wearable device is still treated as a "passive" tool for data collection that it is not able to provide feedback or response to the user actions.
    
    %\item The purpose of this work is to investigate the egocentric vision from the point of view ofÂ real-world application, taking into account the limitations and difficulties that this new situation introduces...
    
    %\textbf{Contributions}. To summarize:
    %\begin{itemize}
     %   \item We provide a novel scenario for Egocentric Action Recognition, which encourages researchers to create new solution tailored for device, unlocking several applications.
    
   % \item We perform the first benchmark of Egocentric Action Recognition under real-world application constrains.
    
   %\item \mirco{old} We enhance the benchmark by adapting existing UDA approaches, highlighting the limits of extending modern UDA video methods in this setting. We believe that this analysis  offers a starting point for future research in this area.  
    %\end{itemize}
   
   
%\end{enumerate}



