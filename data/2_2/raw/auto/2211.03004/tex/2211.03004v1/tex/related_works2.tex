\section{Related Works}
\label{RW}
%\mirco{placeholder}

\textbf{Action Recognition (AR).}
%\paragraph{Action Recognition.}
%The success of 2D CNNs in the context of image recognition~\cite{he2016deep, ioffe2015batch} inspired the first video understanding architectures. Traditional 2D CNNs are often used to process frames individually, eventually fusing optical flow information~\cite{wang2018temporal}, while late fusion mechanisms ranging from average pooling~\cite{wang2016temporal}, multilayer perceptrons \cite{zhou2018temporal}, recurrent aggregation \cite{donahue2015long, li2018videolstm}, and attention~\cite{girdhar2017attentional, sudhakaran2019lsta} are employed to model temporal relations for action understanding.
%The use of 3D convolutions has also been proposed as an alternative~\cite{carreira2017quo, tran2015learning}. 
%However, despite their ability to learn spatial and temporal relations simultaneously, they often introduce more parameters, requiring pre-training on large-scale video datasets~\cite{carreira2017quo}.
%To reduce the model's complexity, other approaches focus on finding more efficient architectures~\cite{tran2019video, sun2015human,feichtenhofer2020x3d, tran2018closer, xie2018rethinking, qiu2017learning}. As an example, a parameter-free channel-wise temporal shift operator has been introduced in the Temporal Shift Module (TSM) network~\cite{lin2019tsm}, resulting in a 2D CNN capable of encoding temporal information. 
%Although all these architectures aim at implicitly modeling motion, most of them still mix video frames with the externally estimated optical flow. While this improves the overall performance, it also requires pre-computing the flow, making these approaches impracticable in online settings. In addition, two-stream approaches come at the cost of increased model complexity and number of parameters. To overcome this issue, a line of research proposes to distill optical flow information to the RGB stream at training time, while avoiding flow computation at test time \cite{crasto2019mars, planamente2021self, plizzari2022e2}. 
%Action recognition 
AR approaches may be classified into two broad categories: 2D convolution -based \cite{10.5555/2968826.2968890, wang2016temporal, ma2016going, lin2019tsm, zhou2018temporal, Kazakos_2019_ICCV, cartas2019seeing, sudhakaran2020gate} and 3D convolution -based\cite{carreira2017quo, singh2016first, Wu_2019_CVPR, kapidis2019multitask, tran2015learning, feichtenhofer2019slowfast, 10.5555/2968826.2968890, Munro_2020_CVPR}. The first group is generally complemented with other modules such as LSTM or its variations \cite{Sudhakaran_2017_ICCV, sudhakaran2018attention, sudhakaran2019lsta, furnari2020rolling, planamente2021self},Temporal Shift Module (TSM) - a parameter-free channel-wise temporal shift operator presented ~\cite{lin2019tsm}, or the Temporal Relation Network module (TRN) introduced in \cite{zhou2018temporal}.
%, with the drawback of the incresed models size. 
The use of 3D convolutions was proposed as an alternative in~\cite{carreira2017quo, tran2015learning} to learn spatial and temporal relations simultaneously, even if  they often introduce more parameters, requiring pre-training on large-scale video datasets~\cite{carreira2017quo}.
To reduce the model's complexity, other approaches focus on finding more efficient architectures using Neural Architecture Search (NAS) to optimize the model efficiency, as in \cite{}. 
%Although all these architectures aim at implicitly modeling motion, most of them still mix video frames with the externally estimated optical flow. While this improves the overall performance, it also requires pre-computing the flow, making these approaches impracticable in online settings. In addition, two-stream approaches come at the cost of increased model complexity and the number of parameters. 

\begin{figure}[t]
 \includegraphics[width=0.49\linewidth]{img/D1.jpg}
 \includegraphics[width=0.49\linewidth]{img/D2.jpg}
%\includegraphics[width=0.32\linewidth]{img/D3.jpg}
%\minipage{0.32\textwidth}%
%  \includegraphics[width=\textwidth]{img/D3.jpg}
%\endminipage
\caption{Examples of two frames taken from different videos sources with the same label ("mix").}
\vspace{-0.5cm}
\label{img:unseen}
\end{figure}

\\[2pt] \textbf{First Person Action Recognition (FPAR).}
%\paragraph{First Person Action Recognition}
%The complex nature of egocentric videos raises a variety of challenges, such as ego-motion~\cite{li2015delving}, partially visible or occluded objects, and environmental bias~\cite{munro2020multi, planamente2021domain, song2021spatio, kim2021learning, sahoo2021contrast}, which limit the performance of traditional, third-person, approaches when used in first person action recognition (FPAR)~\cite{damen2018scaling, damen2021rescaling}. The community's interest has quickly grown~\cite{ek19report, ek2020, ek2021, rodin2021predicting} in recent years, thanks to the possibilities that these data open for the evaluation and understanding of human behavior, leading to the design of novel architectures~\cite{kazakos2019epic, sudhakaran2019lsta, wang2021interactive, kazakos2021little, furnari2020rolling}.
%While the use of optical flow has been the de-facto procedure~\cite{damen2018scaling, damen2021rescaling, ek19report, ek2020, ek2021, grauman2021ego4d} in FPAR, the interest has recently shifted towards more lightweight alternatives, such as gaze~\cite{li2021eye,fathi2012learning, min2021integrating}, audio~\cite{kazakos2019epic,cartas2019seeing, planamente2021domain}, depth~\cite{garcia2018first}, skeleton~\cite{garcia2018first}, and inertial measurements~\cite{grauman2021ego4d}, to enable motion modeling in online settings. These, when combined with traditional modalities, produce encouraging results, but not enough to make them viable alternatives. 
The complex nature of egocentric videos raises a variety of challenges, such as ego-motion~\cite{li2015delving}, partially visible or occluded objects, and environmental bias~\cite{munro2020multi, planamente2021domain, song2021spatio, kim2021learning, sahoo2021contrast}, which limit the performance of traditional, third-person, approaches when used in FPAR %first person action recognition (FPAR)
~\cite{damen2018scaling, damen2021rescaling}. The community's interest has quickly grown~\cite{ek19report, ek2020, ek2021, rodin2021predicting} in recent years, thanks to the possibilities that these data open for the evaluation and understanding of human behavior, leading to the design of novel  and more complex architectures, often based on multi-stream approaches ~\cite{kazakos2019epic, sudhakaran2019lsta, wang2021interactive, kazakos2021little, furnari2020rolling}. %Most of the effort by the egocentric community is made in the direction of "accuracy increasing ", even if it involves models becoming larger and more complex.  This work differs from the previous one because it starts to analyze the realistic scenario of a real-world egocentric application with all the constraints associated with it.

\\[2pt] \textbf{Action segmentation and detection.}
Action segmentation and detection can be intended as the extension of action recognition to the more complex scenario of untrimmed videos, where the task is to assign an action label to each frame, identifying non-overlapping (for segmentation) and overlapping (for detection) action segments.
%More specifically, the goal is to assign an action label to each frame, typically, it requires non-overlapping segments and are thus unsuited for the egocentric fine-grained action. Meanwhile, action detection predicts segments of video that possibly overlap.  %Indeed, in EPIC-Kitchens, there is a high overlapping segment rate (28.1\%). 
Most of these tasks require large and offline models, where the network takes as input the whole video to infer the action. This makes state-of-the-art models unsuitable for our purpose, where on-line processing is fundamental. 
%which need processing the entire video before producing a result and utilizing complex models. 
%Recently, the action detection task has been introduced in the epic kitchen challenge. 
%Unfortunately, the competition solutions are built on much bigger models, making them inappropriate for our project. 

%Differently from previous works, we focus on investigating a solution that detects event boundaries in an online fashion without raising the parameters of the model and that readily allows us to execute online action recognition by promoting the repurposing of existing %action recognition 
%offline methods.