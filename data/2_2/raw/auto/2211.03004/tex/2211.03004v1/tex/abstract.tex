\begin{abstract}

To enable a safe and effective human-robot cooperation, it is crucial to develop models for the identification of human activities. Egocentric vision seems to be a viable solution to solve this problem, and therefore many works provide deep learning solutions to infer human actions from first person videos. However, although very promising, most of these do not consider the major challenges that comes with a realistic deployment, such as the portability of the model, the need for real-time inference, and the robustness with respect to the novel domains (i.e., new spaces, users, tasks). With this paper, we set the boundaries that egocentric vision models should consider for realistic applications, defining a novel setting of egocentric action recognition in the wild, which encourages researchers to develop novel, applications-aware solutions. We also present a new model-agnostic technique that enables the rapid repurposing of existing architectures in this new context, demonstrating the feasibility to deploy a model on a tiny device (Jetson Nano) and to perform the task directly on the edge with very low energy consumption (2.4W on average at 50 fps).

%With the advent of machines capable - in principle - to cooperate with humans in performing tasks of daily living, it becomes crucial the development of methods to quickly and easily infer human actions from easily-accessible data. Egocentric vision is rapidly becoming one of the most promising source of information for this purpose, demonstrating impressing capabilities in pose estimation, action recognition, anticipation, retrieval, and many more.
%However, often these models suppose very controlled scenarios (i.e. trimmed videos), while the major challenges for bringing egocentric vision into the wild are seldom considered overall. With this paper, we set the boundaries that egocentric vision models should consider for realistic applications, namely the dimensionality of the model, the real-time of inference, the potential to easily generalize across conditions (i.e. new spaces, users, tasks), and the capability to work on continuous stream of data. We provide a deep neural architecture for egocentric action recognition which, for the first time, demonstrates competitive performances on continuous video flows and good robustness to domain changes, while keeping quick the inference time and low the computational cost. We deployed our model on a Jetson Nano demonstrating the feasibility of our method to perform the task directly on the edge with very limited energy consumption (2.4wh for 1 hour of continuous inference at 50 fps).



%Egocentric vision is rapidly becoming one of the most promising and challenging tasks in Robotics and Computer Vision. Indeed, videos extracted from the human own perspective automatically focus on the important part of the scene, thus intrinsically implementing an attention mechanism. Robotics could largely exploit such focused source of information to feed human-robot interaction tasks. Going in this direction, the community proposed models for action recognition, anticipation, retrieval, and many more.
%However, albeit the interesting performances demonstrated, most of the methods does not consider the major challenges that egocentric vision should include, namely the portability of the model (to be deployed in the sensing device), the real-time of inference, and the capability to easily generalize to novel domains (i.e. new spaces, users, tasks). 
%This paper describes the boundaries that egocentric vision models should consider for real-world applications, and provides a benchmark of first person action recognition for resource-aware models. Particular attention is given to the comparison of several state-of-the-art methods for unsupervised domain adaptation. \rev{(to be updated later)}


\end{abstract}