\subsection{Challenges of UDA under Real-world Application constraints }

Short introduction of the methods used for the benchmarks (image-based and video-based) and how we adapted them to fit the constraints imposed by the Real-world Application setting.

\subsection{Problem formulation}
Description of UDA settings (somethings like that):
Given one or more source domains $\{\mathcal{S}_1,...,\mathcal{S}_k\}$, where each $\mathcal{S}={\{(x^s_i,y^s_i)\}}^{N_s}_{i=1}$ is composed of $N_s$ source samples with label space $Y^s$ known, the goal of Unsupervised Domain Adaptation (UDA) techniques is to learn a model representation able to perform well on a target domain $\mathcal{T}={\{x^t_i\}}^{N_t}_{i=1}$ of $N_t$ target samples whose label space $Y^t$ is unknown, having access at a training time a set of unlabeled target samples. Our two main assumptions are that the distributions of all the domains are different, \ie $\mathcal{D}_{s,i} \neq \mathcal{D}_t$ $\land$ $\mathcal{D}_{s,i} \neq \mathcal{D}_{s,j}$, with $i \neq j$, $i,j=1,...,k$, and that the label space is shared, $\mathcal{C}_{s,i} = \mathcal{C}_t$, $i=1,...,k$. 


\subsection{UDA algorithms}



\textbf{DANN.} It encourages the feature extractor to generate domain-invariant features using adversarial learning. A domain discriminator is trained to distinguish source from
target samples, while the feature extractor is trained to fool the
discriminator using a gradient reversal layer

\textbf{AdaBN.} Batch Normalisation layers are updated
with target domain statistics

\textbf{MMD} It encourages the final layers of a neural network to generate domain-invariant features by minimizing
the empirical maximum mean discrepancy, a metric that measures the discrepancy between two domain distributions.

\textbf{TA$^3$N} ...
we adapted it to work with buffer..

\textbf{MM-SADA-(RGB)}
... need the flow in training..

\textbf{CoMix}
CoMix adopts supervised learning on source videos, jointly with temporal contrastive learning on both domains to align features. Additional crossdomain contrastive supervision is obtained using background mixing across domains and using target pseudolabels for enhancing discriminability of the latent space...we adapted it to work with buffer..

