\subsection{How to identify a change of action?}
%For this analysis, we need to appropriately introduce a strategy to detect the action change, without raising the model size or the latency. In this work, we investigate two solutions, the first one statically and the second one dynamically. The first one is based on the strong assumption that the length of the actions is quite similar. For these reasons, we periodically "reset" the model, either to clean the logit accumulation (mentioned before) or to clean the buffer for the MoViNet architecture.
%The second strategy is based on the recent work of Cita that shows how, using a measure at the feature level of the frames, it is possible to detect a good policy for online boundary detection. Beginning with this work, we used an MSE distance to calculate the distance between two consecutive feature frames and reasoned that an appropriate threshold would detect when the action changed. Typically, a change in action should be more visible at the feature level, considering it more task-specific w.r.t the input level.
\label{eventboundarydetection}
For this analysis, we need to appropriately introduce a dynamic strategy to detect the action change without raising the model size or the latency. We think that if a model can recognize an action, it will be able to locate its boundaries in relation to the new action. The sequence of frames unrelated to that current action will appear as an anomaly. Indeed, when training an model, by minimizing the cross entropy loss to solve the classification task, it promotes well-separated features \cite{wang2017normface}, i.e. frames of different classes have different features spatially separated in the embedding space. As a result, measuring the distance between features frames offers the opportunity to detect frames that relate to different actions. Precisely, when the distance is greater than a specific threshold, we may localize a new action event. Because each sample during training is composed of frames from the same action, frames from the new action observed sequentially will be evaluated as an anomaly by the model.

\subsection{How to deal with the untrimmed videos?}

%To investigate action recognition in real-world egocentric applications, we chose to conduct experiments using Epic Kitchens, a large collection of egocentric action videos recorded in different environments. Moving from the trimmed annotation supplied by the dataset's authors to a new one that includes the entire recording video, we note that the labeled activities provided are not "mutually exclusive." This indicates that the beginning and conclusion of each activity are not sequential. They may overlap, resulting in a frame linked with many activities. This feature verifies our intuition about how difficult it is to identify and assign the action boundary for fine-grained action recognition, as well as how difficult it is to determine in an objective and unique manner. This is also a result of the fact that fine-grained actions in an egocentric context are actions based on the user's habits. This implies they are very rapid and are frequently not followed in continuous by the user's eyes until the conclusion of the action, and as a result, strongly mutual exclusive labeling risks a length decrease of each sample, increasing the complexity for the action recognition model to recognize action.
When switching from the trimmed to the untrimmed setting, two new obstacles are encountered: the first is the presence of "unknown" segments, which might include new actions not found during training or "no-action" segments (background). The second issue is the presence of a high percent of overlap segments (28.1\%), i.e., the beginning and/or the end of one action intersects the beginning and/or the end of another one. As a result, passing from trimmed to untrimmed, the length of each sample is modified and, in many cases, reduced, affecting the temporal information that the model may encode. Furthermore, this second problem raising the uncertainty of our border detection technique, since distinct actions share sections of videos during training.
\\
\textbf{Our Approach}. In the case when the start point of one action differs from the end point of the preceding action, we see that the conventional method of "aggregation" is ineffective. This aspect is due to the fact that the overlapping part of the video with multiple actions is used to encode only one of the two actions (usually the first one), negatively affecting the encoding of the second one. For these reasons, we introduce a two-fold aggregator strategy in this paper to overcome this issue. The two aggregators run asynchronously using a mixed boundary detection approach, allowing the next operation to begin encoding before the previous one finishes.
The following formula how the final output is obtained:
\begin{equation}
\mathcal{O} = \#A_{1}*A_{1}(x) + \#A_{2}*A_{2}(x) 
\end{equation}
where $A_{x}(x)$ is the output of the aggregator x and \#$A_{x}(x)$ correspond on the quantity of frame processed by it.


