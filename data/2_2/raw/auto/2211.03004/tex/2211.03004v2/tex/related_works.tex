\section{Related Works}
\label{RW}

\begin{figure}[t]
\vspace{2mm}
 \includegraphics[width=0.49\linewidth]{img/D1.jpg}
 \includegraphics[width=0.49\linewidth]{img/D2.jpg}
\caption{Examples of two frames taken from different videos sources with the same label (``mix'').}
\vspace{-0.5cm}
\label{img:unseen}
\end{figure}

\par \textbf{First Person Action Recognition (FPAR).}
The main architectures utilized in this context are generally inherited from third-person literature and may be classified into two broad categories: 2D convolution -based \cite{wang2016temporal, lin2019tsm, zhou2018temporal, cartas2019seeing} and 3D convolution -based \cite{carreira2017quo,singh2016first, tran2015learning, feichtenhofer2019slowfast, Munro_2020_CVPR}. The first group is generally complemented with other modules such as LSTM or its variations \cite{sudhakaran2019lsta, furnari2020rolling, planamente2021self}, Temporal Shift Module (TSM) - a parameter-free channel-wise temporal shift operator presented in \cite{lin2019tsm}, or the Temporal Relation Network module (TRN) \cite{zhou2018temporal}.
%, with the drawback of the incresed models size. 
The use of 3D convolutions was proposed as an alternative in~\cite{carreira2017quo, tran2015learning} to learn spatial and temporal relations simultaneously, even if  they often introduce more parameters, requiring pre-training on large-scale video datasets~\cite{carreira2017quo}.
%To reduce the model's complexity, other approaches focus on finding more efficient architectures using Neural Architecture Search (NAS) to optimize the model efficiency, as in \cite{}. 
%Although all these architectures aim at implicitly modeling motion, most of them still mix video frames with the externally estimated optical flow. While this improves the overall performance, it also requires pre-computing the flow, making these approaches impracticable in online settings. In addition, two-stream approaches come at the cost of increased model complexity and the number of parameters. 


%\paragraph{First Person Action Recognition}
%The complex nature of egocentric videos raises a variety of challenges, such as ego-motion~\cite{li2015delving}, partially visible or occluded objects, and environmental bias~\cite{munro2020multi, planamente2021domain, song2021spatio, kim2021learning, sahoo2021contrast}, which limit the performance of traditional, third-person, approaches when used in first person action recognition (FPAR)~\cite{damen2018scaling, damen2021rescaling}. The community's interest has quickly grown~\cite{ek19report, ek2020, ek2021, rodin2021predicting} in recent years, thanks to the possibilities that these data open for the evaluation and understanding of human behavior, leading to the design of novel architectures~\cite{kazakos2019epic, sudhakaran2019lsta, wang2021interactive, kazakos2021little, furnari2020rolling}.
%While the use of optical flow has been the de-facto procedure~\cite{damen2018scaling, damen2021rescaling, ek19report, ek2020, ek2021, grauman2021ego4d} in FPAR, the interest has recently shifted towards more lightweight alternatives, such as gaze~\cite{li2021eye,fathi2012learning, min2021integrating}, audio~\cite{kazakos2019epic,cartas2019seeing, planamente2021domain}, depth~\cite{garcia2018first}, skeleton~\cite{garcia2018first}, and inertial measurements~\cite{grauman2021ego4d}, to enable motion modeling in online settings. These, when combined with traditional modalities, produce encouraging results, but not enough to make them viable alternatives. 
The complex nature of egocentric videos raises a variety of challenges, such as ego-motion~\cite{li2015delving}, partially visible or occluded objects, and environmental bias~\cite{munro2020multi, planamente2021domain}, \rev{\cite{planamente2023toward}}, which limit the performance of traditional approaches when used in FPAR %first person action recognition (FPAR)
~\cite{damen2018scaling, damen2021rescaling}. 
Those challenges attract the community's interest and motivate the design of novel and more complex architectures, often based on multi-stream approaches such as \cite{sudhakaran2019lsta, wang2021interactive, kazakos2021little, furnari2020rolling}.

%The community's interest has quickly grown~\cite{ek19report, ek2020, ek2021, rodin2021predicting} in recent years, thanks to the possibilities that these data open for the evaluation and understanding of human behavior, leading to the design of novel  and more complex architectures, often based on multi-stream approaches ~\cite{kazakos2019epic, sudhakaran2019lsta, wang2021interactive, kazakos2021little, furnari2020rolling}. %Most of the effort by the egocentric community is made in the direction of "accuracy increasing ", even if it involves models becoming larger and more complex.  This work differs from the previous one because it starts to analyze the realistic scenario of a real-world egocentric application with all the constraints associated with it.

\par \textbf{Action segmentation and detection.}
Action segmentation \cite{huang2016connectionist, wang2020boundary,li2021temporal,khan2022timestamp}, \rev{\cite{aakur2019perceptual, mounir2022spatio}}  and detection \cite{shou2016temporal, shou2017cdc, piergiovanni2019temporal} can be intended as the extension of action recognition to the more complex scenario of untrimmed videos, where the task is to assign an action label to each frame, identifying non-overlapping (for segmentation) and overlapping (for detection) action segments.
%More specifically, the goal is to assign an action label to each frame, typically, it requires non-overlapping segments and are thus unsuited for the egocentric fine-grained action. Meanwhile, action detection predicts segments of video that possibly overlap.  %Indeed, in EPIC-Kitchens, there is a high overlapping segment rate (28.1\%). 
Most of these tasks require large and offline models, especially for the EPIC-KITCHENS challenge solutions\footnote{https://epic-kitchens.github.io/Reports/EPIC-KITCHENS-Challenges-2021-Report.pdf}, in which the network uses the entire video as input to infer the action. This makes state-of-the-art models unsuitable for our purpose, where on-line processing is fundamental. Recently, \cite{du2022fast} developed a novel unsupervised methodology for event boundary localization (detection) that outperforms current approaches while increasing inference time significantly, making it perfect for edge devices.
%which need processing the entire video before producing a result and utilizing complex models. 
%Recently, the action detection task has been introduced in the epic kitchen challenge. 
%Unfortunately, the competition solutions are built on much bigger models, making them inappropriate for our project. 

%Differently from previous works, we focus on investigating a solution that detects event boundaries in an online fashion without raising the parameters of the model and that readily allows us to execute online action recognition by promoting the repurposing of existing %action recognition 
%offline methods.