\documentclass[11pt]{article}
\pdfoutput=1
\usepackage[margin=1in]{geometry} 
\usepackage{amsmath,amsthm,amssymb,amsfonts,titling,bbm, titlesec, bm, physics, enumitem, accents, xcolor, setspace, fancyhdr, natbib, geometry,
	pdflscape}
\usepackage{graphicx}
\usepackage{float}
\usepackage[font=footnotesize,labelfont=bf]{caption}
\usepackage{array}
\usepackage[title]{appendix}
\usepackage{afterpage}
\usepackage{listings}
\usepackage{prodint}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{algorithmicx}
\usepackage[hidelinks]{hyperref}
\usepackage{multirow}
\usepackage{booktabs}

\newlength{\continueindent}
\setlength{\continueindent}{2em}
\usepackage{etoolbox}
\makeatletter
\newcommand*{\ALG@customparshape}{\parshape 2 \leftmargin \linewidth \dimexpr\ALG@tlm+\continueindent\relax \dimexpr\linewidth+\leftmargin-\ALG@tlm-\continueindent\relax}
\apptocmd{\ALG@beginblock}{\ALG@customparshape}{}{\errmessage{failed to patch}}
\makeatother

\algnewcommand\algorithmicstack{\textit{Stack:}}
\algnewcommand\Stack{\item[\algorithmicstack]}
\algnewcommand\algorithmicchoosetimegrid{\textit{Choose time grid:}}
\algnewcommand\Choosetimegrid{\item[\algorithmicchoosetimegrid]}
\algnewcommand\algorithmicchoosetimebasis{\textit{Choose time basis:}}
\algnewcommand\Choosetimebasis{\item[\algorithmicchoosetimebasis]}
\algnewcommand\algorithmicbuilddiscretedata{\textit{Build data at time $t_i$:}}
\algnewcommand\Builddiscretedata{\item[\algorithmicbuilddiscretedata]}
\algnewcommand\algorithmicfit{\textit{Fit:}}
\algnewcommand\Fit{\item[\algorithmicfit]}
\algnewcommand\algorithmicpredict{\textit{Predict:}}
\algnewcommand\Predict{\item[\algorithmicpredict]}

\newcommand\Algphase[1]{%
\vspace*{-.7\baselineskip}\Statex\hspace*{\dimexpr-\algorithmicindent-2pt\relax}\rule{\textwidth}{0.4pt}%
\Statex\hspace*{-\algorithmicindent}\textbf{#1}%
\vspace*{-.7\baselineskip}\Statex\hspace*{\dimexpr-\algorithmicindent-2pt\relax}\rule{\textwidth}{0.4pt}%
}

\setlength{\droptitle}{-5em}
\bibliographystyle{apalike}

\doublespacing

\newcommand{\N}{\mathbb{N}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\Q}{\mathbb{Q}}
\newcommand{\I}{\mathbbm{1}}
\newcommand\tab[1][1cm]{\hspace*{#1}}
\newcommand\independent{\protect\mathpalette{\protect\independenT}{\perp}}
\def\independenT#1#2{\mathrel{\rlap{$#1#2$}\mkern2mu{#1#2}}}
\newcommand{\Cov}{\text{Cov}}
\newcommand{\Var}{\text{Var}}
\newcommand{\E}{\mathbb{E}}
\newcommand{\matr}[1]{\mathbf{#1}}
\newcommand{\prob}{\text{Pr}}
\newcommand{\ut}[1]{\underaccent{\tilde}{#1}}
\renewcommand{\vec}[1]{\ut{#1}}
\newcommand{\argmax}{\text{argmax}}
\newcommand{\argmin}{\text{argmin}}
\newcommand{\cpt}{\stackrel{\text{p}}{\to}}
\newcommand{\haz}{\lambda}
\newcommand{\Haz}{\Lambda}
\newcommand{\midd}{\,|\,}

\newtheorem{theorem}{Theorem}

\theoremstyle{definition}
\newtheorem{exmp}{Example}[section]

\titleformat{\section}{\large\scshape\bfseries}{\thesection.}{1em}{}
\titleformat{\subsection}{\normalfont\bfseries}{\thesubsection.}{1em}{}%\itshape


\begin{document}
	
\begin{center}
    \textbf{A framework for leveraging machine learning tools to estimate}\\
    \textbf{personalized survival curves}
	\\ ~\ 
	
	Charles J. Wolock\textsuperscript{1}, Peter B. Gilbert\textsuperscript{2}, Noah Simon\textsuperscript{1} \& Marco Carone\textsuperscript{1,2,3}
	\\ ~\ 
	
	\textsuperscript{1}Department of Biostatistics, University of Washington\\
	\textsuperscript{2}Vaccine and Infectious Disease Division, Fred Hutchinson  Cancer Research Center\\
	\textsuperscript{3}Department of Statistics, University of Washington

\end{center}
\vspace{0.5cm}
\begin{center}
	\textbf{Abstract}
\end{center}
\begin{quote}
	
	The conditional survival function of a time-to-event outcome subject to censoring and truncation is a common target of estimation in survival analysis. This parameter may be of scientific interest and also often appears as a nuisance in nonparametric and semiparametric problems. In addition to classical parametric and semiparametric methods (e.g., based on the Cox proportional hazards model), flexible machine learning approaches have been developed to estimate the conditional survival function. However, many of these methods are either implicitly or explicitly targeted toward risk stratification rather than overall survival function estimation. Others apply only to discrete-time settings or require inverse probability of censoring weights, which can be as difficult to estimate as the outcome survival function itself. Here, we employ a decomposition of the conditional survival function in terms of observable regression models in which censoring and truncation play no role. This allows application of an array of flexible regression and classification methods rather than only approaches that explicitly handle the complexities inherent to survival data. We outline estimation procedures based on this decomposition, empirically assess their performance, and demonstrate their use on data from an HIV vaccine trial.	
	\\~\
	
	\textit{Keywords:} Survival analysis, machine learning, nonparametric, censoring, truncation
\end{quote}
   

\section{Introduction}\label{sec:introduction}
\subsection{Motivation}\label{subsec:lit review}
In the analysis of time-to-event data, the conditional survival function is a key quantity of interest. Within the biomedical field, the conditional survival function, which describes the distribution of an outcome variable conditional on a set of covariates, is especially relevant for prediction. For example, the survival function of a clinical outcome, such as death or disease recurrence, conditional on baseline characteristics may allow a clinician to better understand a patient's medical prognosis. The conditional survival function also appears as a function-valued nuisance parameter in nonparametric and semiparametric survival analysis problems (see, for example, \citealp{Diaz2019} and \citealp{Westling2021}). 

Typically, the analysis of survival data is complicated by the fact that the data are subject to censoring, truncation, or both, depending on the study design. In prospective studies, participants are sampled from the population of interest and followed over time, ideally until experiencing the event of interest. However, occurrence of competing events, loss to follow-up, or study termination may preclude observation of the event time. Participants who do not experience the event of interest during follow-up are considered right-censored. Additionally, individuals who have already experienced the event at study initiation are not eligible for recruitment. This sampling constraint is referred to as left truncation. Conversely, in retrospective studies, individuals must have already experienced the event in order to be recruited into the study, leading to right truncation. In this design, censoring is generally not a concern. All forms of truncation lead to systematic selection bias.   

\subsection{Classical methods}\label{subsec:classical methods}
Even under right censoring alone, standard regression techniques cannot be directly applied to estimation of the conditional survival function. However, a number of valid survival-specific approaches have been proposed. Parametric methods, such as exponential or Weibull regression, are straightforward to use and automatically yield inference. Since their validity relies on strong distributional assumptions, they are less widely used than semiparametric methods. The most common regression model used to study survival outcomes is the Cox proportional hazards model \citep{Cox1972}. Hazard ratio estimates from the estimated Cox model can be combined with an estimate of the baseline cumulative hazard function (e.g., the \citet{Breslow1972} estimator) to yield a conditional survival function estimate \citep{Lin1994}. A common alternative to the Cox model is the accelerated failure time model \citep{Wei1992}, which is usually implemented in a fully parametric manner. Semiparametric implementations \citep{Buckley1979, Lin2013} exist but are seldom used because they are complicated and can be unstable \citep{Zeng2007}.

Under independent censoring (independence of the event and censoring times), the Kaplan-Meier estimator \citep{Kaplan1958} is the nonparametric maximum likelihood estimator of the marginal survival function. If the covariates of interest are low-dimensional and discrete-valued, a stratified Kaplan-Meier approach may be reasonable. This method breaks down in moderate dimensions, or when the covariates include continuous variables. As such, it is of limited use in most applications. \citet{Beran1981} introduced a conditional Kaplan-Meier estimator using kernel smoothing. However, kernel-based methods tend to perform poorly as the number of covariates grows. Furthermore, the smoothing bandwidth can, in general, be allowed to vary for each covariate; selection of an optimal set of bandwidths can be computationally expensive. 

\subsection{Risk stratification methods}\label{subsec:risk stratification}
Fortunately, machine learning methods offer many strategies for estimating complex functions of moderate or large numbers of covariates in a flexible manner. For this reason, there has been a recent proliferation of machine learning methods in survival analysis --- see \citet{Sonabend2021} for a comprehensive review. This motivates a discussion of the precise objectives of these methods. Estimation of the conditional survival function and risk stratification are distinct tasks that are often conflated. The Cox proportional hazards model (and related machine learning methods) are based on the partial likelihood. Maximization of the partial likelihood is equivalent to maximization of the expected concordance between estimated risk scores and survival times \citep{Tarkhan2022}. Indeed, the partial likelihood has no dependence on actual event times and relies only on the relative ordering of events. Due to this fact, methods based on the Cox proportional hazards model might be best understood as risk stratification techniques. It is often the case that conditional survival function estimates can be derived from the resulting risk stratification algorithm (e.g., combining the Breslow baseline hazard estimate with a fitted Cox model), but ultimately these are a byproduct rather than the core goal of stratification approaches. 

Along these lines, flexible and high-dimensional implementations of the Cox model have become increasingly common. Regularized regression methods such as LASSO, ridge, and elastic net have been implemented in a proportional hazards framework \citep{Tibshirani1997,Simon2011}. Flexible modeling of the proportional hazards risk score has been implemented within the framework of generalized additive models \citep{Hastie1986}. In addition, many deep learning survival analysis methods are built on the proportional hazards model and have largely been used to estimate complex interactions between covariates. Examples of non-linear proportional hazards neural networks include DeepSurv \citep{Katzman2018} and Cox-nnet \citep{Ching2018}.

Tree-based methods are another popular machine learning strategy.  Random survival forests \citep{Ishwaran2008} encompass several previously proposed approaches; they are built similarly as standard random forests, but tree splits are determined based on stratification objective functions that can be evaluated with censored data (e.g., the log-rank test statistic). One difficulty with tree methods is evaluating predictions, which is important for choosing tuning parameters (or more generally, discriminating between candidate learners). \citet{Ishwaran2008} suggest using Harrell's concordance index \citep{Harrell1982} to evaluate out-of-bag prediction error; this statistic measures discrimination rather than predictive accuracy per se. Once the random survival forest is constructed, cumulative hazard estimates for an individual with covariate vector $x$ are obtained by dropping $x$ down each tree, and using the Nelson-Aalen cumulative hazard estimator \citep{Nelson1969, Aalen1978} in the resulting leaf node. Averaging the Nelson-Aalen estimates over all trees yields an overall cumulative hazard estimate, from which a survival function estimate can be obtained. In conditional inference forests \citep{Hothorn2006a}, predictors on which to split are chosen using hypothesis testing rather than exhaustive search, but otherwise a similar splitting principle as in random survival forests is used. Relative risk forests \citep{Ishwaran2004} generalize the random forest algorithm to survival outcomes using Poisson regression under a proportional hazards model. 

\subsection{Methods based on censoring-weighted loss functions}\label{subsec:loss functions}
Stratification objective functions circumvent the hurdle of comparing observed and predicted outcomes when some observations are censored. An alternative strategy is to use inverse probability of censoring weights (IPCW) to connect a full-data loss function, such as squared-error loss, to a loss that can be evaluated using the observed data \citep{VanderLaan2003}. Defining an observed-data loss function allows the use of many established learning methods. \citet{Molinaro2004} use IPCW in the context of regression trees, where inverse-weighted loss functions can be used to both grow trees and evaluate performance. \citet{Hothorn2006} use IPCW to extend ensemble learning techniques such as random forests and boosting to survival data. These methods rely on estimation of the conditional survival function of the censoring variable in order to construct the inverse probability weights. When the censoring variable can be assumed to be independent of covariates (for example, in studies where censoring times are controlled by the investigator), a simple marginal survival function estimator, such as Kaplan-Meier, can be used. But in general, estimation of the conditional survival function of the censoring variable is no easier than estimation of the conditional survival function of the outcome. Furthermore, in the presence of truncation, a candidate loss function must be adapted to account for the induced sampling bias. 

Recently, \citet{Westling2021} framed the conditional event time and censoring survival functions as minimizers of oracle risks, which allows for iterative empirical risk minimization in order to combine multiple candidate estimators in a Super Learner approach \citep{VanDerLaan2007}. This method, termed survival Super Learner, is appealing because unlike usual IPCW loss functions, which are evaluated at a single time-point, it targets the entire survival function and simultaneously provides estimates of the outcome and censoring distributions. However, the candidate survival function estimators comprising the Super Learner are limited to existing survival-specific methods. In addition, in their current implementations, the oracle risk functions do not account for truncation. 

\subsection{Discrete-time methods}\label{subsec:discrete time methods}
Many methods aimed at estimation of the conditional survival function, as opposed to risk stratification, rely on the assumption that events occur in discrete time. For discrete time-to-event variables, the hazard function at a single time is a conditional probability whose estimation can be framed as a binary regression problem in terms of the observed data distribution: among those who have not experienced the event by time $t$, what proportion experience the outcome at that time? Reframing survival function estimation as a binary regression or classification problem allows use of a wider array of machine learning algorithms. Estimation of the survival function at time $t$ involves computing the product of one minus the hazard at each time-point up to and including $t$.

For some approaches, time is discretized and the conditional hazard is estimated at each time based on a separate binary regression. Methods in this category include multitask logistic regression (MTLR), in which the hazards in discrete-time bins are modeled as separate logistic regressions \citep{Yu2011}. Penalized regression enforces smoothness over time, since event status in one time bin depends on what occurred in previous time bins. Censored observations are handled by marginalizing over possible sequences of survival statuses for all remaining times after censoring. In order to better learn nonlinear hazard-predictor relationships, \citet{Fotso2018} proposed a deep neural network adaptation of the MTLR framework. Other discretization-based neural network implementations include RNN-SURV \citep{Giunchiglia2018} and Nnet-survival \citep{Gensheimer2019}, which estimate the hazard in user-specified time bins. In these implementations, guidelines for choosing time bins, as well as handling individuals that are censored within a particular time bin, are provided.  \citet{Friedman1982} proposed a piecewise constant exponential hazards model, where time is treated as continuous, but with a constant hazard in user-specified time bins. 

Perhaps the most flexible discretization method is what has recently been referred to as ``survival stacking" \citep{Craig2021}, and which we hereafter call \emph{local survival stacking}. The approach dates back at least to work by \citet{Polley2011}. \citet{Craig2021} propose to implement the approach by discretizing at each observed event time.  With time discretized, a survival dataset can be transformed into a longitudinal data set, where each individual appears in the data set at each observed event time until exiting the risk set. Time itself is included as a covariate, often as a dummy variable indicating whether an individual is in the risk set at a given time, or as a continuous predictor. Fitting a logistic regression without interactions on this ``stacked" data set (with time treated as a dummy variable) approximates fitting the Cox proportional hazards model using the partial likelihood as the grid of time-points becomes increasingly fine \citep{DAgostino1990}. In contrast to methods discussed above, local survival stacking does not involve estimating separate regressions at each discrete time, but rather estimates a single regression including time as a covariate. The tradeoff is that the size of the stacked data grows at rate $O(n^2)$ for a sample of size $n$, and so computational issues may be a concern. It is unclear how performance depends on the choice of time discretization. Beyond right censoring, left truncation is dealt with naturally in discrete-time models, since each individual appears in the longitudinal data for as many time-points as they remain in the risk set. Left truncation is handled by only including the individual at those time-points after which they have entered the study. 

\subsection{Contribution and organization of the article}\label{subsec:organization}

The emerging theme, as discussed by \citet{Polley2011}, is that survival function estimation is generally based on one of two loss functions: the IPCW loss function for survival function estimation at a single time-point, or the hazard loss function for estimation of a discrete-time hazard function. Risk stratification methods do not employ an estimation-oriented loss, though they may produce distributional estimates as a byproduct. 

In this article, we consider decompositions of the conditional survival function that allow use of standard loss-based estimation of functionals of the observed data distribution. As we will show, this permits use of a wide range of flexible estimation tools neither specially designed to handle censoring nor truncation. Our proposed method, which we call \emph{global survival stacking}, involves estimating a small number of binary regression functions and cumulative probability functions, and it can be applied in both prospective and retrospective settings without assuming a discrete-time process. This approach simultaneously yields estimates of both the event time and censoring time conditional survival functions using the same fitted regressions. Because we propose a general framework, rather than a prescriptive method, the practitioner is free to choose from a variety of learners. Recent work by \citet{Zhou2022} also approaches conditional survival estimation in terms of the observed data distribution but is tied to a specific machine learning architecture based on generative adversarial networks and does not handle truncation. 

The organization of this article is as follows: In Section \ref{sec:data structures and identification}, we describe the data structures emerging from prospective and retrospective survival studies and provide identification results that form the basis for our estimation framework. In Section \ref{sec:estimation}, we propose the global survival stacking procedure for estimating the conditional survival function using machine learning estimators of the constituent regressions, and we give a more detailed comparison of global versus local survival stacking. In Section \ref{sec:simulations}, we evaluate the performance of our class of estimators via numerical experiments and analysis of publicly available time-to-event datasets. In Section \ref{sec:step}, we demonstrate our method on data from the STEP HIV vaccine trial. In Section \ref{sec:discussion}, we provide concluding remarks. 

\section{Data structures and identification}\label{sec:data structures and identification}

\subsection{Ideal data and parameter of interest}\label{subsec:ideal data}

Suppose that $X$ is a vector of baseline covariates taking values in $\mathcal{X} \subset \R^p$, and $T \in (0, \infty)$ is the event time of interest. The ideal data unit is $O^*:= (X, T)$. We use $P^*$ to denote the distribution of $O^*$. In reality, the ideal data unit $O^*$ is observed subject to both censoring and truncation, which are determined by the study design. The observed data consist of $n$ independent and identically distributed observations $O_1, O_2,\dots, O_n$ drawn from $P$, the observed data distribution implied by $P^*$. The relationship between $P^*$ and $P$ is determined by the censoring and sampling mechanisms. 

Our goal is estimation of the conditional survival function of $T$ given $X$, defined as $S(t \midd x) := P^*(T > t \midd X = x)$. Because $T$ is not directly observed, this parameter is not a functional of the observed data distribution. However, with an additional assumption, the conditional hazard function (and through it, the conditional survival function) can be identified. Our proposed method relies on a reformulation of standard identification results in order to write the hazard function in terms of observable regression functions.

Let $\Lambda(t \midd x) := \int_0^t \frac{F(du \midd x)}{1 - F(u^- \midd x)}$ denote the conditional cumulative hazard of $T$ given $X=x$ at time $t$, 
where $F := 1 - S$ is the conditional distribution function of $T$. Identification of $S$ in full generality requires the use of product integrals via the mapping
\begin{align*}
	S(t \midd x) = \Prodi_{u \in (0, t]}\left\{1 - \Lambda(du \midd x)\right\}.
\end{align*}
When the mapping $t \mapsto F(t \midd x)$ is differentiable everywhere, the product integral simplifies to the exponential form $S(t \midd x) = \exp\left\{-\Lambda(t \midd x)\right\}$.

Observational epidemiological studies are often conducted to learn characteristics of the distribution of time from an initiating event (e.g., disease onset) until a terminating event (e.g., death). Here, we treat the time of the initiating event as $t = 0$, and use the event time $T$ to refer to the time between initiating and terminating events. 

\subsection{Prospective sampling}\label{subsec:prospective data structure}
We consider prospective studies in which individuals who have not yet experienced the event of interest are sampled and followed over time. Ideally, every participant is followed until the event has occurred, but right censoring is essentially inevitable in prospective biomedical studies. Participants who do not experience the event during follow-up are considered right-censored. This may be due to loss to follow-up or to the termination of the study.  Let $C \in (0, \infty)$ denote the right censoring time. For each participant in the study, we observe $Y := \min\{T,C\}$, the observed follow-up time, and $\Delta := \I(T \leq C)$, the event indicator. 

Common prospective observational study designs include: (a) the incident cohort --- people who have not experienced the initiating event upon entering the study, and can be followed from the initiating event onward; and (b) the prevalent cohort --- people who experienced the initiating event prior to entering the study. A study sample may also contain both prevalent and incident cases. 

Because prevalent cases have already experienced the initiating event upon study entry, observation of these participants does not begin at $t = 0$. This phenomenon is commonly referred to as delayed entry, and it implies that the event times are observed subject to left truncation. Left truncation induces sampling bias, since individuals with larger event times are more likely to enter the sample. Let $W \in (0,\infty)$ denote the time from the initiating event until entry into the study. Under left truncation, an individual can only enter the study (i.e., be observed) if $W \leq Y$. The observed data for participants in the sample are $O := (X,Y,\Delta,W)$, and the sampling criterion is $W \leq Y$. If a prospective study consists only of incident cases, there is no left truncation. In that special case, $W =0 $ for all participants. 

In order to identify $\Lambda(\cdot \midd x)$ in the prospective setting, we rely on the following assumption: 
\begin{itemize}
	\item[] \textit{Assumption A:} $T$ and $(C,W)$ are conditionally independent given $X$. 
\end{itemize}
Let $F_{\delta}(y \midd x) := P(Y \leq y \midd  \Delta = \delta, X = x,W\leq Y)$ denote the conditional distribution function of $Y$ among observed participants with $\Delta = \delta$. Let $\pi(x) := P(\Delta = 1 \midd X = x, W \leq Y)$ denote the probability of a random observed individual being uncensored. In addition, define $G_{\delta}(y \midd x) := P(W \leq y \midd  \Delta = \delta,X = x,Y \geq y, W \leq Y)$. We note that these regressions are all functionals of the observed data distribution $P$. As detailed in Appendix \ref{sec:calculations}, we then have that $\Lambda(\cdot \midd x)$ can be identified at generic time $t$ by  
\begin{align}
	\Lambda^{\text{obs}}(t \midd x) := \int_0^t\frac{\pi(x)F_{1}(du \midd x)}{G_{1}(u \midd x)\pi(x)\left\{1 - F_{1}(u^- \midd x)\right\} + G_{0}(u \midd x)\left\{1-\pi(x)\right\}\left\{1 - F_{0}(u^- \midd x)\right\}}\ .\label{eq:ID1}
\end{align}
When there is no left truncation, the distribution of $W$ is degenerate at 0, so that $G_{1}(u \midd x) = G_{0}(u \midd x) = 1$ for all $u$. In that case, $\Lambda^{\text{obs}}$ is a function only of the conditional distributions of $Y$ given $(\Delta, X)$ and $\Delta$ given  $X$. 

Assumption A can be considered when $C$ is defined for all individuals in the target population. However, in some settings, censoring may only affect the follow-up time under study for enrolled participants. It may then be more appropriate to consider an alternative assumption wherein (i) $T$ and $W$ are independent given $X$ (conditionally independent truncation), and (ii) $T$ and $C$ are conditionally independent given $W$ and $W \leq T$ (conditionally independent residual censoring) \citep{Qian2014}. In Appendix \ref{sec:quasi}, we show that \eqref{eq:ID1} still holds --- and so, our proposed estimation strategy is still valid --- under this alternative assumption.

\subsection{Retrospective sampling}\label{subsec:retrospective data structure}
In the retrospective setting, we consider studies in which investigators only sample individuals who have experienced the terminating event prior to the end of the sampling period. For example, in autopsy studies where death is the terminating event, an individual who did not die prior to the end of the study could not enter the sample. As above, let $W$ be the time from the initiating event until entry into the study, which we will refer to as the study entry time. We do not consider censoring in the retrospective setting, so the event time $T$ is observed for all participants. For notational consistency, we set $C = 0$ for all participants and define $Y := \max\{T,C\}$ and $\Delta := \I(T \geq C) = 1$. This implies that $Y = T$ for all participants, i.e., the observed follow-up times are equal to the event times. Under right truncation, an individual is sampled if $Y \leq W$. Similarly as in left truncation, right truncation induces sampling bias. The observed data are $O := (X,Y,\Delta,W)$, and the sampling criterion is $W \geq Y$.

Identification of $S(t \midd x)$ in the retrospective setting follows directly from the prospective identification. Let $\tau$ denote a user-specified real number, and  define the random variables $\bar{T} := \tau - T$, $\bar{C} := \tau - C$, $\bar{Y} := \tau - Y$, $\bar{W} := \tau - W$, and $\bar{\Delta} := \I(\bar{T} \leq \bar{C}) = 1$. In practice, we set $\tau$ as the maximum study entry time $W$, so that $\bar{T}$, $\bar{C}$, $\bar{Y}$ and $\bar{W}$ are non-negative. If $T$ has bounded support, the upper bound of that support would be another natural choice for $\tau$. (In principle, $\tau$ could be any real number, including 0, in which case the transformed data could take negative values, but for the sake of applying our prospective results to the retrospective setting, we assume the transformed data are nonnegative.) We again suppose that Assumption A holds. 

We note that $\bar{T}$ is subject to conditionally independent left truncation by $\bar{W}$. Denoting by $\bar{\Lambda}(t \midd x)$ and $\bar{S}(t \midd x)$ the conditional cumulative hazard and survival functions of $\bar{T}$ given $X$ at $t$, we can directly use the prospective setting results to identity $\bar{\Lambda}(\cdot \midd x)$ at generic time point $t$ by
\begin{align*}
     \int_0^{t}\frac{\bar{\pi}(x)\bar{F}_{1}(du \midd x)}{\bar{G}_{1}(u \midd x)\bar{\pi}(x)\left\{1 - \bar{F}_{1}(u^- \midd x)\right\} + \bar{G}_{0}(u \midd x)\left\{1-\bar{\pi}(x)\right\}\left\{1 - \bar{F}_{0}(u^- \midd x)\right\}}\ ,
\end{align*}
where $\bar{F}_1, \bar{F}_0$, $\bar{G}_1$, and $\bar{G}_0$ are defined analogously as in the prospective setting, and $\bar{\pi}(x):= P(\bar{\Delta} = 1 \midd X = x)$. Because there is no censoring in this setting, $\bar{\pi}(x) = 1$, and the above identification simplifies to
\begin{align}
    \bar{\Lambda}^{\text{obs}}(t \midd x) := \int_0^{t}\frac{\bar{F}_{1}(du \midd x)}{\bar{G}_{1}(u \midd x)\left\{1 - \bar{F}_{1}(u^- \midd x)\right\}}\ . \label{eq:ID2}
\end{align}
Finally, we note that
\begin{align*}
    S(t \midd x) \ &=\  P(T > t \midd X = x) \\
    &=\  1 - P(\bar{T} > \tau - t \midd X = x) \ =\  1- \bar{S}(\tau - t \midd x) \ =\  1 - \Prodi_{u \in (0, \tau - t]}\left\{1 - \bar{\Lambda}(du \mid x)\right\},
\end{align*}
and so it suffices to use the above identification of $\bar{\Lambda}(\cdot \midd x)$ in order to estimate $S(\cdot \midd x)$. This result demonstrates that estimating the conditional hazard of $T$ given  $X$ under right truncation can be accomplished by simply estimating the conditional hazard of $\tau - T$ given  $X$ under left truncation. 

\section{Estimation}\label{sec:estimation}

\subsection{Estimation procedure}\label{subsec:estimation procedure}
The results of Section \ref{sec:data structures and identification} suggest that we can construct estimators of $\Lambda(\cdot \midd x)$ and $\bar{\Lambda}(\cdot \midd x)$ by estimating a small number of regression functions based on the observed data. These hazard estimators can then be mapped to survival function estimators via either the product integral or exponential mappings. The regression functions that appear in the identification results constitute either: (i) a conditional probability (specifically, $\pi(x)$), or (ii) a conditional cumulative probability function ($F_1, F_0, G_1$, $G_0$, or their retrospective analogs). These regression functions can be estimated using standard machine learning techniques, without requiring any adaptation for the censoring or sampling mechanisms. 

Let $t_\text{max}$ denote the maximum time at which the survival function is to be estimated, and let $t \in (0, t_{\text{max}}]$ be a generic time-point of interest. Below we outline the steps to estimate $S(t \midd x)$. Our proposed procedure is as follows: 
\begin{enumerate}

    \item Select an approximation grid: Choose a partition $\mathcal{B}:= \{t_0,t_1\dots,t_\text{max}\}$ of the interval $[0, t_{\text{max}}]$ ($t_0 $ will often be 0).
    
	\item Estimate the cumulative hazard: For each $t_j \in \mathcal{B}$, obtain estimators $F_{1,n}(t_j \midd x)$, $F_{0,n}(t_j \midd x)$, $\pi_n(x)$, $G_{1,n}(t_j \midd x)$, and $G_{0,n}(t_j \midd x)$ of $F_{1}(t_j\midd x)$, $F_{0}(t_j \midd x)$, $\pi(x)$, $G_{1}(t_j \midd x)$, and $G_{0}(t_j \midd x)$ respectively. 
	
	\item Approximate a mapping from the hazard to the survival function:  Let $t_k := \max\{t' \in \mathcal{B}: t' \leq t\}$. Define the estimated differential of the cumulative hazard at $t_i$ as
	\begin{align*}
	    \hspace{-0.2cm}M_n(t_i, x) := \frac{\pi_n( x)\left\{F_{1,n}(t_i \midd x)-F_{1,n}(t_{i-1} \midd x)\right\}}{G_{1,n}(t_i \midd x)\pi_n( x)\left\{1-F_{1,n}(t_{i-1}\midd x)\right\} + G_{0,n}(t_i \midd x)\left\{1-\pi_n(x)\right\}\left\{1-F_{0,n}(t_{i-1}\midd x)\right\}},
	\end{align*}
	where $M_n(t_0, x):= 0$.
	\begin{enumerate}
		\item Product integral form: Approximate the product integral using the product
		\begin{align*}
			S_{n,p}(t \midd x):=\prod_{i = 1}^k  \left\{1-M_{n}(t_i,x)\right\}.
		\end{align*}
		\item Exponential form: Approximate the exponentiated negative cumulative hazard using the Riemann sum approximation
		\begin{align*}
		S_{n,e}(t \midd x) := \exp\left\{-\sum_{i=1}^k M_n(t_i,x)\right\}.
		\end{align*}
	\end{enumerate}
\end{enumerate}

In the retrospective setting, estimation proceeds by simply (i) transforming the data to reverse time, taking $\bar{Y}_i = \tau - Y_i$, $\bar{\Delta}_i = 1$, $\bar{W}_i = \tau - W_i$, and $\bar{t}  = \tau - t$; (ii) following Steps 1, 2, and 3 above to produce an estimate $\bar{S}_n(\bar{t} \midd x)$ of $\bar{S}(\bar{t} \midd x)$, with $\bar{S}_n$ being either the product integral or exponential form; and (iii) computing $S_n(t \midd x) = 1 - \bar{S}_{n}(\bar{t} \midd x)$. 

The product integral form of the estimator is the more natural option, since the product integral mapping holds whether $T$ has a discrete, continuous, or mixed distribution. As the basis for the Kaplan-Meier estimator, it is also more familiar to most practitioners. However, in practice, $S_{n,p}$ can have numerical issues. Specifically, when the product integral is discretized, the differential of the cumulative hazard is a probability and must lie in $[0,1]$. Our method may yield an estimated differential that lies outside of $[0,1]$, leading to survival function estimates that are negative, particularly in the tails of the distribution of $Y$. The exponential form protects against this potential issue and is analogous to exponentiating the negative Nelson-Aalen cumulative hazard estimate as an alternative to the Kaplan-Meier estimator \citep{Fleming1984}. We note that in settings without truncation, $S_{n,p}$ naturally respects the $[0,1]$ bounds of the survival function. When the distribution function of $T$ is continuous, we expect minimal differences in performance between the two forms of the survival function estimator. We summarize results from simulations addressing this question in Appendix \ref{sec:additional sims}.

Approximating the product integral and the cumulative hazard requires choosing an approximation partition $\mathcal{B}$ of the interval $[0, t_{\text{max}}]$. A simple option for this is the set of observed follow-up times $\{Y_{(1)}, Y_{(2)}, \dots, Y_{(n)}\}$, where $Y_{(j)}$ denotes the $j$th order statistic. Alternatively, $\mathcal{B}$ could be set to an evenly spaced grid of times between 0 and $t_{\text{max}}$. For computational purposes, in large samples, it may be more practical to use a grid of fixed size rather than including every observed follow-up time.  

Estimating the conditional event probability function $\pi(x)$ is a simple binary regression problem, for which there are numerous flexible methods. In practice, we recommend using a boosted classifier, such as boosted trees \citep{Friedman2001}, or an ensemble regression method such as Super Learner \citep{VanDerLaan2007}. The observed follow-up time distribution functions $F_{1}(\cdot \midd x)$ and $F_0(\cdot \midd x)$ are slightly more complicated to estimate. At any fixed time $t$, these can be viewed as a binary regression on the indicator variable $\I(Y \leq t)$ (prospective) or $\I(\bar{Y} \leq t)$ (retrospective). However, we must estimate these distribution functions on the grid $\mathcal{B}$ of times in order to approximate the product- or sum-integral of the hazard function. Here we outline some possible approaches for estimating these functions. 

\textit{Parametric regression:} Any parametric regression technique for a positive-valued outcome (e.g. exponential regression) automatically yields an estimate of the entire distribution function. 

\textit{Kernel-based methods:} Closed-form kernel-weighted estimators of a conditional distribution function have been described previously; see, for example, the product kernel estimators of \citet{Li2008}, which allow either smoothing in covariates alone, or smoothing in covariates and the outcome variable. Least-squares cross-validation and rule-of-thumb bandwidth selectors have been proposed for kernel distribution function estimators \citep{Li2013}. 

\textit{Pooled binary regression/classification:} A simple and flexible approach is to perform pooled binary regression on a user-specified time grid $\mathcal{C}$. This grid could be the same as the approximation grid $\mathcal{B}$, but in general it need not be. A natural choice for $\mathcal{C}$ would be the observed follow-up times. A coarser grid speeds computation at the cost of increased bias. At each time-point $t$ in the grid, the available data are baseline covariates, an indicator outcome variable $\I(Y \leq t)$ (prospective) or $\I(\bar{Y} \leq t)$ (retrospective), and time $t$. These data are pooled across time into a single dataset, which serves as training data for binary regression. This approach differs from local survival stacking in that the risk set at each time-point consists of all participants, and the outcome is cumulative across times. In our experiments and data analysis, we include time as a continuous variable. To ensure monotonicity in time, we recommend isotonizing the distribution function estimates using isotonic regression (see for example \citealp{Westling2020}). 

The pooled binary regression approach to conditional distribution function estimation is the most flexible, and is therefore the option we explore throughout the remainder of this article.

The conditional entry time regression functions $G_{1}(\cdot \midd x)$ and $G_0(\cdot \midd x)$ are similar to conditional distribution functions, although they are each conditioned on being at-risk for an event at time $t$. Similarly as the conditional distribution functions, these functions can be easily estimated using pooled binary regression. Given a time grid, the data at each time-point $t$ consist of all individuals who remain under follow-up at time $t$, along with covariates, time $t$, and the outcome $\I(W \leq t)$. 

Because the pooled binary regression approach involves ``stacking" datasets across time-points, we refer to the resulting estimation procedure as global survival stacking, which we differentiate from the discrete-time hazard approach of local survival stacking. In Algorithm \ref{alg:cpe}, we outline the steps to construct the global survival stacking estimator. 

% cpe F algo
\begin{algorithm}
	\caption{Global survival stacking}\label{alg:cpe}
	\begin{algorithmic}[1]
	\State Choose grid $\mathcal{B} := \{t_0, t_1, \dots, t_{\text{max}}\}$ for approximation of product- or sum-integral. 
	\State Construct estimator $\pi_n(x)$ of $\pi(x)$ using binary regression. 
	\Algphase{Estimate $F_1$ and $F_0$}
	    \For{$\delta \in \{0,1\}$}
	    \State Choose grid of time-points
	    $\mathcal{C} := \{t^*_1,t^*_2,\dots,t^*_k\}$ on which to discretize $F_\delta$.
	    \State Choose how to include  time in model (continuous, dummy variable, etc.).
	    \For{$t^*_j \in \mathcal{C}$}
		\State Including only participants with $\Delta = \delta$, construct dataset $D_{t^*_j}$ consisting of participant baseline covariates, outcomes $\I(Y \leq t^*_{j})$, and time using chosen basis. 
		\EndFor
		\State Construct full stacked dataset by combining  $\{D_{t_1^*}, D_{t_2^*}, \dots, D_{t_k^*}\}$. 
	    \State Fit binary regression or classification algorithm of choice. 
		\State Generate predictions \{$F_{\delta, n}(t_0 \midd x),F_{\delta, n}(t_1 \midd x),\dots,F_{\delta, n}(t_{\text{max}} \midd x)\}$. 
	\EndFor
	\Algphase{Estimate $G_1$ and $G_0$ (if truncation is present)}
	    \For{$\delta \in \{0,1\}$}
	    \State Choose grid of time-points
	    $\mathcal{C} := \{t^*_1,t^*_2,\dots,t^*_k\}$ on which to discretize $G_\delta$.
	    \State Choose how to include  time in model (continuous, dummy variable, etc.).
	    \For{$t^*_j \in \mathcal{C}$}
	    \State Including only participants with $\Delta = \delta$ and $Y \geq t^*_j$, construct dataset $D_{t^*_j}$ consisting of participant baseline covariates, outcomes $\I(Y \leq t^*_{j})$, and time using chosen basis. 
		\EndFor
		\State Construct full stacked dataset by combining  $\{D_{t_1^*}, D_{t_2^*}, \dots, D_{t_k^*}\}$. 
	    \State Fit binary regression or classification algorithm of choice. 
		\State Generate predictions \{$G_{\delta, n}(t_0 \midd x),G_{\delta, n}(t_1 \midd x),\dots,G_{\delta, n}(t_{\text{max}} \midd x)\}$. 
		\EndFor
	\Algphase{Combine constituent estimators}
	\State Compute $\{M_n(t_0,x),M_n(t_1,x),\dots,M_n(t_{\text{max}},x)\}$, as detailed in Section \ref{subsec:estimation procedure}.
	\State Compute $S_{n,p}(t \midd x)$ or $S_{n,e}(t \midd x)$ as detailed in Section \ref{subsec:estimation procedure}.
	\end{algorithmic}
\end{algorithm}

\subsection{Comparison to local survival stacking}\label{sec:comparison}
Local survival stacking is the most natural alternative to the proposed framework since it allows practitioners to draw upon a wide array of general machine learning techniques. The idea of local survival stacking has appeared several times in the literature (see, for example, \citealp{Polley2011} and \citealp{Craig2021}). When performing a local survival stacking analysis, besides the question of which binary classification or regression tool to use, the user must also choose how to discretize time. Local survival stacking assumes a discrete survival process, so that the conditional hazard takes the form of a conditional probability that can be estimated for each time-point in the grid. The discretization is usually chosen on the basis of the observed event times $\mathcal{R}_n := \{Y_i: \Delta_i = 1, i = 1,2,\dots,n\}$. In an illustrative data analysis, \citet{Polley2011} choose 30 time-points based on quantiles of $\mathcal{R}_n$. \citet{Craig2021} instead define local survival stacking based on discretizing at each time in $\mathcal{R}_n$. In some situations, there may be a convenient choice based on the measurement scheme. For example, in an analysis of a clinical trial studying patients with intraventricular hemorrhage, \citet{Diaz2019} analyze time to death on a discrete time scale of days because this was how the data were recorded. The fineness of the time grid determines the number of events used to estimate the conditional probability of an observed event at each time-point, and we would expect the grid choice to affect performance. 

The fineness of the time grid may also be relevant for global survival stacking if pooled binary regression is used, although we emphasize that the outcome is cumulative over time, meaning that the probability of an outcome at any given time does not shrink as the grid becomes finer. The simulation studies in Section \ref{sec:simulations} explore the performance of these methods under various grid sizes. 

In Algorithm \ref{alg:stack prospective}, we provide an operational description of local survival stacking in the prospective setting. As with global survival stacking, local survival stacking can be adapted to retrospective studies by simply considering them to be analogous to prospective studies with the time scale reversed. 

% stacking algorithm
\begin{algorithm}
	\caption{Local survival stacking}\label{alg:stack prospective}
	\begin{algorithmic}[1]
	    \State Choose grid of time-points
	    $\mathcal{C} := \{t^*_1,t_2^*,\dots,t^*_k\}$ on which to discretize. Set $t^*_{k+1} = \infty$.
	    \State Choose how to include  time in model (continuous, dummy variable, etc.).
	    \For{$t^*_j \in \mathcal{C}$}
		\State Including only participants with $Y \geq t_j^*$ and $W \leq t_j^*$, construct dataset $D_{t^*_j}$ consisting of participant baseline covariates, outcomes $\I(t^*_j \leq Y < t^*_{j+1})$, and time using chosen basis. 
		\EndFor
		\State Construct full stacked dataset by combining $\{D_{t_1^*}, D_{t_2^*}, \dots, D_{t_k^*}\}$. 
	    \State Fit binary regression or classification algorithm of choice. 
	    \State Generate hazard predictions $\{\lambda_n(t^*_1 \midd x), \lambda_n(t^*_2 \midd x), \dots, \lambda_n(t^*_k \midd x) \}$ from fitted model.
	    \State Compute estimate $S_n(t \midd x) = \prod_{t^*_j \in \mathcal{C}: t^*_j \leq t} \{1-\lambda_n(t_j^* \midd x)\}$. 
	\end{algorithmic}
\end{algorithm}

\section{Empirical evaluation}\label{sec:simulations}
\subsection{Simulation studies}
We conducted a series of numerical studies using simulated data to evaluate the performance of our proposed method. In addition to overall estimation performance, i.e., mean squared error, there were specific aspects of global survival stacking that we aimed to interrogate. First, we assessed the sensitivity of global survival stacking to the choice of time grid used for estimating $F$ and $G$ via pooled binary regression. As discussed in Section \ref{sec:comparison}, we expected global survival stacking to be less sensitive to the grid choice compared to local survival stacking due to the fact that the regression outcome is cumulative over time. Second, we assessed the computational burden of global survival stacking. Our method requires fitting several separate learning algorithms, and when using pooled binary regression the resulting data sets can be quite large. We expected global survival stacking to have a large computational burden relative to local survival stacking. 

\subsubsection{Estimator performance under different study designs}\label{sec:simulations prospective}

We performed numerical experiments to compare the performance of several conditional survival function estimators in both prospective and retrospective study designs. The methods compared were:
\begin{enumerate}
	\item Global survival stacking: We estimated $F_1, F_0, G_1$, and $G_0$ using pooled binary regression with Super Learner, as implemented in the \texttt{SuperLearner} software package. The algorithm library consisted of the marginal mean, logistic regression with all pairwise interactions, generalized additive models (GAM), multivariate adaptive regression splines, random forests, and gradient-boosted trees (Table \ref{tab:SL algorithms}). We estimated $\pi(x)$ (where applicable) using the same Super Learner library. We used five-fold cross-validation and the built-in nonnegative least-squares method to determine
    the optimal convex combination of these algorithms. We considered three time grids $\mathcal{C}$ for the pooled regression: a grid made up of every observed follow-up time and grids of 10 or 40 cutpoints evenly spaced on the quantile scale of observed follow-up times. We used the exponential form $S_{n,e}(t \midd x)$. The approximation time grid $\mathcal{B}$ was set to the observed follow-up times. The predictions across times in the approximation grid were isotonized using the pool adjacent violators algorithm, as implemented in the \texttt{Iso} software package.  
	\item Local survival stacking: We used Super Learner as the binary classifier in local survival stacking, using the same algorithm library as for global survival stacking. Tuning was performed in the same manner as described above, and the same time grids were included, based on observed event times $\mathcal{R}_n$.
	
	\item Survival Super Learner: We used the survival Super Learner approach as implemented in the \texttt{survSuperLearner} package. We used the same library of algorithms for both the censoring and event time distributions, including the marginal Kaplan-Meier estimator, the Cox proportional hazards model with Breslow baseline hazard estimator, exponential regression, Weibull regression, log-logistic regression, a generalized additive proportional hazards model, and random survival forest (see Table \ref{tab:survSL algorithms}). We did not evaluate this method in any settings with truncation since it is not designed to handle truncation. 
	\item Cox proportional hazards regression: As a semiparametric comparator, we used a main-terms Cox proportional hazards model with Breslow baseline hazard estimator. The Cox model was not evaluated in the retrospective study design as its standard implementation does not handle right truncation. 
\end{enumerate}

We simulated a covariate vector $X := (X_1, X_2, \dots, X_{10})$ of 10 independent components. These components included continuous covariates $X_1, X_2 \sim \text{Uniform}(-1,1)$, discrete covariates $X_3,X_4 \sim \text{Uniform}(\{-1,1\})$, and continuous covariate $X_5 \sim N(0, 1)$. The five additional covariates were independent standard normal noise, i.e., $(X_6,X_7,\dots, X_{10}) \sim \text{MVN}(0, \matr{I}_5)$. Given covariate vector $X = x$, we simulated the censoring time $C$ from a Weibull distribution with shape 1.5 and scale $\lambda_C = \exp\left\{\beta_{0C} + \tfrac{1}{2}\left(x_1 + x_2\right) + \tfrac{1}{5}\left(x_3 + x_4 + x_5\right)\right\}$, where in each simulation setting $\beta_{0C}$ was chosen to give a censoring rate of 25\%. (The shape and scale follow the parameterization used in the \texttt{rweibull} function.) Given covariate vector $X = x$, we independently simulated the event time $T$ to be distributed as $100Z_1$. In the left-skewed scenario, $Z_1$ was a Beta($a(x) + 2, 2$) random variable with $\log a(x) = x_1 + x_2 + x_3 + x_4 + x_5+ x_1x_2 + x_3x_4 + x_1x_5$. In the right-skewed scenario, $Z_1$ was a Beta($2, a(x) + 2$) random variable. Density plots for $T$ given $X$ for 10 random draws from the covariate distribution are shown in Figure \ref{fig:example densities}. These distributions do not meet the proportional hazards assumption. This data-generating framework was used for convenience so that the conditional distribution of $T$ given $X$ could be identified on the entire support of $T$. 

Given covariate vector $X = x$, the study entry time variable $W$ was distributed as $100Z_2$, where $Z_2$ was a Beta$\left(1 + \tfrac{1}{2}\I(x_1 > 0), 1 + \tfrac{1}{2}\I(x_1 < 0)\right)$ random variable. In the prospective study design, only observations with $Y \geq W$ were sampled, whereas in the retrospective study design, only observations with $Y \leq W$ were sampled. The average truncation rates for all simulation settings are shown in Table \ref{tab:cens trunc}. There was no censoring in the retrospective study design. In the retrospective study design, both global and local survival stacking were implemented with $\tau$ set to the largest study entry time. 
We evaluated performance using Monte Carlo approximations of mean squared error (MSE) at three landmark times and mean integrated squared error (MISE) over the interval $[0,100]$. We computed MSE at landmark times corresponding to the 50\textsuperscript{th}, 75\textsuperscript{th}, and 90\textsuperscript{th} percentiles of observed event times. To calculate the MISE, we computed the MSE at each time on an evenly spaced grid of 1000 points from $t = 0.1$ to $t = 100$, and took a simple average over times. We estimated the performance metrics using a test set of size 1000. For the study designs with truncation, the test data were generated without truncation in order to evaluate performance across the marginal distribution of covariates in the target population. 

Global survival stacking performs well across all settings, with the finer grids generally yielding slight to moderate decreases in MISE and MSE compared to the 10 cutpoint grids (Figures \ref{fig:RC integrated}, \ref{fig:LTRC integrated}, and \ref{fig:RT integrated}). The performance of local survival stacking is much more variable and appears to be more sensitive to grid size choice. Among local stacking implementations, the grid of 40 cutpoints performs the best in general. The 10 cutpoint grid appears too coarse for optimal performance, while the finest grid performs well in the right-skewed settings but poorly in the left-skewed settings. Without truncation, survival Super Learner performs reasonably well, although it is outperformed by global survival stacking in the left-skewed setting and by both global and local survival stacking in the right-skewed setting. The main-terms Cox model is misspecified, and its performance does not improve substantially with sample size. The relative performance of the estimators is similar across the four performance metrics, although at the 90th percentile landmark time, there tends to be less separation between global stacking, local stacking, and survival Super Learner. 

%%%% figure RC integrated
\begin{figure}
	\begin{center}
		\includegraphics[width=\linewidth]{prospective_notrunc.png}
	\end{center}
	\vspace{-0.5cm}
	\caption{Performance of conditional survival estimators with right-censored data. The methods compared were global survival stacking, local survival stacking, survival Super Learner, and the main-terms Cox proportional hazards model with Breslow baseline hazard estimator. Time grids are based on quantiles of observed follow-up times (global stacking) or observed event times (local stacking). From top to bottom, rows correspond to MISE and to MSE at 50th, 75th, and 90th percentiles of observed event times. Each boxplot represents 100 simulation replicates.}
	\label{fig:RC integrated}
\end{figure}

%%%% figure LTRC integrated
\begin{figure}
	\begin{center}
		\includegraphics[width=1\linewidth]{prospective_ltrunc.png}
	\end{center}
	\vspace{-0.5cm}
	\caption{Performance of conditional survival estimators with left-truncated, right-censored data. The methods compared were global survival stacking, local survival stacking, and the main-terms Cox proportional hazards model with Breslow baseline hazard estimator. Time grids are based on quantiles of observed follow-up times (global stacking) or observed event times (local stacking). From top to bottom, rows correspond to MISE and to MSE at 50th, 75th, and 90th percentiles of observed event times. Each boxplot represents 100 simulation replicates.}
	\label{fig:LTRC integrated}
\end{figure}

%%%% figure RT integrated
\begin{figure}
	\begin{center}
		\includegraphics[width=1\linewidth]{retrospective.png}
	\end{center}
	\vspace{-0.5cm}
	\caption{Performance of conditional survival estimators with right-truncated data. The methods compared were global survival stacking and local survival stacking. Time grids are based on quantiles of observed follow-up times (global stacking) or observed event times (local stacking). From top to bottom, rows correspond to MISE and to MSE at 50th, 75th, and 90th percentiles of observed event times. Each boxplot consists of 100 simulation replicates.}
	\label{fig:RT integrated}
\end{figure}

\subsubsection{Computational considerations}\label{subsec:computation}
Both global and local survival stacking can be computationally expensive, particularly when the number of cutpoints in the time grid is allowed to grow with sample size. Global survival stacking requires fitting multiple regressions, and for distribution function estimation the dataset size is generally larger than that of local survival stacking, since individuals do not exit the risk set over time. 
In order to benchmark computational burden, we simulated samples of size 500 in the prospective study design without left truncation under the left-skewed data-generating mechanism. We fit each estimator as described above and generated conditional survival function estimates for a test data set of size 100 on an evenly spaced grid of times from $t = 0.1$ to $t = 100$. The computational benchmarking simulations were run on an Amazon Web Services EC2 \texttt{r6a.large} instance with 2 vCPUs and 16GB memory. There were 100 simulation replicates for each estimator. 

Local survival stacking and survival Super Learner were both substantially faster than global survival stacking in our numerical experiments (Table \ref{tab:computation time}). Unsurprisingly, the computation time required for both global and local survival stacking increases as the grid becomes finer (i.e., as the number of times in $\mathcal{C}$ increases). In practice, of course, the computational resources required for using any ensemble regression method will depend on which algorithms are included in the library. Some machine learning algorithms allow for subsampling or mini-batching, which involves training base learners on subsets of the data. Doing so has the potential to reduce the computational burden of both stacking approaches. 

%%% computation time
\begin{table}%[h]
	\centering
		\begin{tabular}{l c c} \toprule
			Estimator & Mean runtime (s) & Std. dev. runtime (s)\\ \midrule
            Global stacking (all times grid)&998&36.0\\
            Global stacking (40 cutpoint grid)&222 &43.2\\
             Global stacking (10 cutpoint grid)&129&1.1\\
             Local stacking (all times grid)&493&21.7\\
              Local stacking (40 cutpoint grid)&52&0.8\\
            Local stacking (10 cutpoint grid)&20&0.3\\
             survSuperLearner&61&1.8\\
             Cox&0.03&0.002\\ \bottomrule
		\end{tabular}
		\caption{Computation time for conditional survival estimators from numerical experiments.}
		\label{tab:computation time}
\end{table} 

Based on the performance of global survival stacking, there appears to be no harm in using as fine a time grid as computational resources and time allow. If an analysis is only performed once on a dataset of modest size, using a grid of every observed follow-up time may be reasonable, although in our experiments there was little gain, if any, for the computational cost. The grid of 40 cutpoints performs nearly as well as a grid of every observed follow-up time, while the 10 cutpoint grid seems too coarse to perform optimally. 

\subsubsection{Additional simulation results}

In Appendix \ref{sec:additional sims}, we present additional empirical results. First, we evaluated the performance of global survival stacking when the proportional hazards assumption was met. Briefly, we found that the Cox model, when correctly specified, yields moderately better performance than the machine learning comparators. Local survival stacking with 40 cutpoints performs the best of the machine learning methods, while, as in the primary results, local stacking is sensitive to grid size choice. Global stacking shows generally good performance, with relatively small differences between different choices of grid size. While we expect the proportional hazards assumption to rarely hold in practice, predictably, the Cox model would be the preferred method in this situation but with only modest performance loss from more flexible methods.

We also considered a setting in which $Y$ and $W$ were observed on a discrete grid of times, rather than in continuous time. In this setting, we implemented global stacking on a grid of all observed follow-up times and local stacking on a grid of all observed event times. When $Y$ and $W$ are observed on a discrete grid of 10 or 20 times, global and local stacking demonstrate similar performance. When $Y$ and $W$ are observed on a discrete grid of 50 times, global survival stacking performs the best overall and demonstrates similar performance as in the continuous-time setting. The discrete-time experiments show that there is relatively little difference between global and local survival stacking when times are observed on a coarse grid. The advantages of global survival stacking become more pronounced on a finer grid.  

\subsection{Predictive performance on time-to-event datasets}\label{sec:real data performance}

We also evaluated our proposed method on several publicly available datasets with right-censored time-to-event outcomes, which are described in Appendix \ref{sec:data details}. We predicted the survival probability at three landmark times, corresponding to the 50\textsuperscript{th}, 75\textsuperscript{th}, and 90\textsuperscript{th} percentiles of observed event times in each dataset, with performance evaluated using the Brier score. To account for censoring, we used the IPCW Brier score given by \citet{Gerds2006}, with Kaplan-Meier censoring weights. For each of the five datasets, we compared global survival stacking, local survival stacking, and the Cox proportional hazards model with Breslow baseline hazard estimator. Both global and local stacking were implemented with 40 cutpoints using the same algorithm library as in Section \ref{sec:simulations}. In addition, we considered a na\"ive approach in which, for predicting the survival probability at landmark time $t$, the binary outcome $\I(Y > t)$ was regressed on $X$. We used the Super Learner, with the same algorithm library as in global and local stacking, to fit this binary regression. We used five-fold cross-validation to estimate the Brier score of each of the methods under consideration. The performance of each method was evaluated relative to the performance of the marginal model constructed without covariates using the Kaplan-Meier estimator (i.e., using the same prediction for every observation in the test set). 

Global survival stacking performs well in all five datasets (Table \ref{tab:public data results}). The na\"ive model typically has relatively poor performance but does slightly outperform the other methods at two landmark time in the SUPPORT dataset, where the censoring rate is zero. This is unsurprising: for prediction at landmark time $t$, observations censored after $t$ provide the same information as uncensored observations. When the censoring rate is higher --- for example, in the METABRIC dataset --- the na\"ive approach is outperformed by the methods that account for censoring.  

\begin{table}
	\centering
			\begin{tabular}{p{0.11\linewidth}  >{\centering}p{0.11\linewidth} >{\centering}p{0.11\linewidth} >{\centering}p{0.12\linewidth}>{\centering}p{0.12\linewidth} >{\centering}p{0.12\linewidth} >{\centering\arraybackslash}p{0.12\linewidth}} 
			\toprule
			\multirow{3}{*}{Dataset}& \multirow{3}{*}{Quantile}& \multirow{3}{*}{Censoring}&\multicolumn{4}{c}{Performance relative to KM} \\
			\cmidrule{4-7}&&&Global stacking &Local stacking&\multirow{2}{*}{Cox}&\multirow{2}{*}{Na\"ive}\\\midrule
			 FLCHAIN &50\textsuperscript{th}&0.07&\textbf{0.748}&0.752&0.757&0.758\\
			 &75\textsuperscript{th}&0.19&\textbf{0.685}&0.689&0.695&0.695\\
			 &90\textsuperscript{th}&0.31&\textbf{0.646}&0.658&0.660&0.662\\
			 GBSG &50\textsuperscript{th}&0.03&\textbf{0.858}&0.889&0.907&0.895 \\
			 &75\textsuperscript{th}&0.07&\textbf{0.825}&0.850&0.861&0.978\\
			 &90\textsuperscript{th}&0.17&\textbf{0.831}&0.858&0.857&1.119\\
			  METABRIC &50\textsuperscript{th}&0.07&\textbf{0.889}&0.909&0.933&0.913\\
			 &75\textsuperscript{th}&0.19&\textbf{0.885}&0.886&0.894&0.982\\
			 &90\textsuperscript{th}&0.30&0.871&0.868&\textbf{0.867}&1.048\\
			 NWTCO &50\textsuperscript{th}&0.02&0.859&\textbf{0.858}&0.916&0.861 \\
			 &75\textsuperscript{th}&0.04&\textbf{0.866}&0.870&0.899&0.898\\
			 &90\textsuperscript{th}&0.11&\textbf{0.865}&0.866&0.896&0.995\\
			 SUPPORT &50\textsuperscript{th}&0.00&0.930&0.952&0.987&\textbf{0.927}\\ 
			 &75\textsuperscript{th}&0.00&0.909&0.924&0.934&\textbf{0.908}\\
			 &90\textsuperscript{th}&0.08&\textbf{0.879}&0.892&0.899&0.902\\\bottomrule
		\end{tabular}
		\caption{Predictive performance of candidate methods on publicly available survival datasets. The performance metric is the Brier score standardized by the Brier score of the Kaplan-Meier (KM) estimator (i.e., predicting survival probability without using covariate information). The Brier score was evaluated at three landmark times corresponding to the 50\textsuperscript{th}, 75\textsuperscript{th}, and 90\textsuperscript{th} percentiles of observed event times. Lower values are preferred. Boldface font indicates the best performance for each dataset and landmark time. The methods compared were global survival stacking, local survival stacking, the Cox model with Breslow baseline hazard estimator, and a na\"ive binary regression approach ignoring censoring.}
		\label{tab:public data results}
\end{table}

\section{Assessing risk of HIV infection in the STEP trial}\label{sec:step}
    
Between December 2004 and March 2007, 3,000 HIV-negative individuals were enrolled in the STEP study (HVTN 502/Merck 023), a randomized, placebo-controlled phase 2b trial that tested the efficacy of a candidate HIV vaccine to prevent acquisition of HIV-1 infection. The vaccine contains an adenovirus serotype 5 (Ad5) vector that expresses subtype B HIV-1 \textit{gag/pol/nef} proteins. Participants were at high risk of HIV-1 acquisition. Participants were unblinded in October 2007 after the prespecified monitoring boundary for efficacy futility was crossed at the first interim analysis \citep{Buchbinder2008}. Data analyses suggested an increased risk of HIV-1 infection among vaccine recipients versus placebo recipients, particularly among participants who were uncircumcised or had neutralizing antibodies against the Ad5 vector at enrollment (``baseline Ad5 titer).
	
In order to assess the risk of HIV-1 infection conditional on circumcision status and baseline Ad5 titer, we estimated the conditional survival function of the time-to-infection-diagnosis variable within randomized treatment arms at landmark times of one year and two years of follow-up, corresponding to approximately 60\% and 10\% of participants still at-risk in each treatment arm. We limited our analyses to the 1,836 participants with male sex assigned at birth in the modified intention-to-treat cohort, which included all vaccinated participants except those diagnosed as HIV-1 positive on or before the day 1 visit. At one year of follow-up, 41 participants in the vaccine arm (4.6\%) and 27 participants in the placebo arm (3.0\%) had been diagnosed with HIV-1; at two years, 51 participants in the vaccine arm (5.7\%) and 35 participants in the placebo arm (3.9\%) had been diagnosed. We implemented global survival stacking using Super Learner with the same algorithm library as in the numerical simulations, using a grid of 40 cutpoints based on quantiles of observed follow-up times (i.e., the number of days from randomization until the end of follow-up, for each participant in the study) with five-fold cross-validation for tuning. For comparison, we also fit a Cox model including the two-way circumcision/baseline Ad5 titer interaction and estimated the baseline cumulative hazard function using the Breslow estimator. Both models were fit separately in the two treatment arms. Baseline Ad5 titer was log-transformed (using the natural logarithm), and titers under the assay detection limit of 18 were treated as equal to 18 for analysis \citep{Duerr2012}. We calculated the risk difference conditional on circumcision status and baseline Ad5 titer by taking the difference of the estimated conditional survival functions in the two treatment arms at each landmark time. Using global survival stacking, we also computed representative survival curves for individuals in each treatment arm, circumcised and uncircumcised, at log baseline Ad5 titer values of 3, 5, and 7. 

The estimated survival curves (Figure \ref{fig:step curves}) show that, in the vaccine group, the probability of HIV-1 diagnosis through day 730 tends to be higher for individuals with higher baseline Ad5 titers. The probability of HIV-1 diagnosis was higher in the vaccine arm than the placebo arm, as estimated by both global survival stacking and the Cox model, except at low baseline Ad5 titers among circumcised participants (Figure \ref{fig:step}). The estimated excess risk in the vaccine arm tends to increase with baseline Ad5 titer and is generally higher among uncircumcised participants, although the Cox model fit suggests that circumcised participants may have slightly larger excess risk at high baseline Ad5 titers. Overall, these results agree with the original analysis in \citet{Duerr2012}, which did not explicitly account for right censoring.

	\begin{figure}
		\begin{center}
			\includegraphics[width=1\linewidth]{step_curves.png}
		\end{center}
		\vspace{-0.5cm}
		\caption{Estimated survival curves for time to HIV-1 diagnosis in the STEP study. The curves were estimated separately in each treatment arm, conditional on baseline Ad5 titer and circumcision status.}
		\label{fig:step curves}
	\end{figure}
	
\begin{figure}
		\begin{center}
			\includegraphics[width=1\linewidth]{step_analysis.png}
		\end{center}
		\vspace{-0.5cm}
		\caption{Estimated risk difference (vaccine - placebo) of HIV-1 infection diagnosis in the STEP study conditional on baseline Ad5 titer and circumcision status at one year and two years of follow-up. The estimators compared were the Cox model with first-order interaction, and global survival stacking.}
		\label{fig:step}
	\end{figure}
	
\section{Discussion}\label{sec:discussion}
In this article, we proposed a framework for estimating a conditional survival function in both prospective and retrospective settings using flexible machine learning tools. This framework, which we refer to as global survival stacking, relies on an identification of the hazard function in terms of observable regressions that can be estimated using standard methods for binary outcomes, without the need to explicitly account for censoring or truncation. Similarly as with local survival stacking, our approach recasts conditional survival function estimation as a statistical learning task that does not require specially tailored survival analysis tools. These methods not only enable practitioners to take advantage of the myriad machine learning methods currently available, but also to harness the improved performance of new methods as they are developed. 

Numerical experiments show that global survival stacking works well across a variety of settings, performing on par with or better than competing methods when the proportional hazards assumption fails to hold. Global survival stacking is relatively insensitive to the choice of grid size in estimating the constituent regressions via pooled binary regression. While our proposed method can be computationally expensive when using a time grid that grows finer with increasing sample size, it suffers virtually no decrease in performance using a relatively coarse grid of fixed size.  

Because our method involves estimating regression functions within strata defined by the event indicator, we can use the same procedure to obtain an estimate of the conditional censoring distribution, simply replacing $\pi_n(x)$ with $1 - \pi_n(x)$ and $F_{1,n}(t \midd x)$ with $F_{0,n}(t \midd x)$ in the numerator of $M_n(t,x)$ in Section \ref{subsec:estimation procedure}. While any conditional survival function estimation algorithm can be repurposed by reversing the roles of $T$ and $C$, our proposed approach requires no refitting, resulting in greater computational efficiency when both distributions are desired.

Unlike right censoring, interval censoring is a common type of data coarsening that remains unaddressed by many survival function estimators. Interval-censored event times are known only to lie in a particular interval, rather than being observed exactly. Data that are truly subject to interval censoring (e.g. data from biomedical studies with periodic follow-up) are often treated as subject only to right censoring. Whether or not the flexible machine learning methods presented here can be adapted to handle interval censored data remains an open question.

\section{Software and code}
We have implemented both global and local survival stacking in the \texttt{R}
package \texttt{survML} (\url{https://github.com/cwolock/survML}). Code to reproduce all results is available online at \url{https://github.com/cwolock/stack_supplementary}.

\section{Acknowledgments}
The authors thank the study participants and investigators of the STEP HVTN 502/Merck 023 trial conducted by the HIV Vaccine Trials Network.  Research reported in this publication was supported by National Institute Of Allergy And Infectious Diseases grants UM1-AI068635 and R37-AI029168,  and by National Heart, Lung, and Blood Institute grant R01-HL137808. This work was also supported by the National Science Foundation Graduate Research Fellowship Program under Grant No. DGE-2140004. The content is solely the responsibility of the authors and does not necessarily represent the official views of the funding agencies.

\newpage

\bibliography{global_stacking} 

\newpage

%%% Appendix
\titleformat{\section}{\normalfont\scshape\bfseries}{Appendix~\Alph{section}.}{1em}{}
\renewcommand{\thefigure}{A\arabic{figure}}
\setcounter{figure}{0}
\renewcommand{\thetable}{A\arabic{table}}
\setcounter{table}{0}

\begin{appendices}

\section{Details of identification result}\label{sec:calculations}
Let $F_{T, C, W}$ and $F_{C,W}$ denote the conditional distribution functions of $(T,C,W)$ given $X$ and $(C,W)$ given  $X$, respectively. We begin by using standard probability rules to write
\begin{align*}
	\pi(x)F_1(u \midd x) \ &=\  P(\Delta = 1\midd X = x, W \leq Y)P(Y \leq u\midd \Delta = 1, X = x, W \leq Y)\\
	&=\  \frac{P(\Delta =1, Y \leq u, W \leq Y \midd X = x)}{P(W \leq Y \midd X = x)}\\
	&=\  \frac{P(T \leq C, T \leq u, W \leq T \midd X = x)}{P(W \leq Y \midd X = x)}\\
	&=\  \frac{\iiint \I(t \leq u, c \geq t, w \leq t)F_{T,C,W}(dt, dc, dw \midd x)}{P(W \leq Y \midd X = x)}\\
	&\stackrel{\text{(a)}}{=}\  \frac{ \iiint\I(t \leq u, c \geq t, w \leq t)F_{C,W}(dc, dw \midd x)F(dt\midd x)}{P(W \leq Y \midd X = x)}\\
	&=\  \frac{ \int_0^uP(C \geq t, W \leq t\midd X = x)F(dt \midd x)}{P(W \leq Y \midd X = x)}\  ,
\end{align*}
where (a) follows from Assumption A. The differential of this function with respect to $u$ is 
\begin{align*}
	\pi(x)F_1(du \midd x) = \frac{P(W \leq u, C \geq u  \midd X = x)F(du \midd x)}{P(W \leq Y \midd X = x)}\ .
\end{align*}
The denominator of $\Lambda^{\text{obs}}$ is
\begin{align*}
    &G_1(u \midd x)\pi(x)\left\{1 - F_1(u^- \midd x)\right\} +G_0(u \midd x)\left\{1-\pi(x)\right\}\left\{1 - F_0(u^- \midd x)\right\} \\
    % &= P(W \leq u \midd Y \geq u, W \leq Y, X = x, \Delta = 1)P(\Delta = 1 \midd X = x, W \leq Y)P(Y \leq u \midd W \leq Y, X = x, \Delta = 1) \\
    % & + P(W \leq u \midd Y \geq u, W \leq Y, X = x, \Delta = 0)P(\Delta = 0 \midd X = x, W \leq Y)P(Y \leq u \midd W \leq Y, X = x, \Delta = 0)\\
    &=\ P(W \leq u, Y \geq u \midd W \leq Y, X = x)\ ,
\end{align*}
where we have applied the law of total probability. Continuing from this expression we have
\begin{align*}
	P(W \leq u, Y \geq u \midd W \leq Y, X = x) \ &=\  \frac{P(W \leq u \leq Y, W \leq Y \midd X = x)}{P(W \leq Y \midd X = x)}\\
	&=\  \frac{P(W \leq u \leq Y\midd X = x)}{P(W \leq Y \midd X = x)}\\
	&=\  \frac{P(W \leq u, C \geq u, T \geq u \midd  X = x)}{P(W \leq Y \midd X = x)}\\
	&\stackrel{\text{(b)}}{=}\  \frac{P(W \leq u, C \geq u \midd X = x)S(u^{-} \midd  x)}{P(W \leq Y \midd X = x)}\ , 
\end{align*}
where (b) follows from Assumption A. Combining this with the numerator, we have
\begin{align*}
	&\Lambda^{\text{obs}}(t \midd x) \ =\  \int_0^t\frac{\pi(x)F_{1}(du \midd x)}{G_{1}(u \midd x)\pi(x)\left\{1 - F_1(u^- \midd x)\right\} +G_{0}(u \midd x)\left\{1-\pi(x)\right\}\left\{1 - F_0(u^- \midd x)\right\}}\\
	&\hspace{0.5cm}=\  \int_0^t \left. \left(\frac{P(W \leq u, C \geq u  \midd X = x)}{P(W \leq Y \midd X = x)}\right) \middle/ \left( \frac{P(W \leq u, C \geq u \midd X = x)S(u^{-} \midd x)}{P(W \leq Y \midd X = x)}\right)F(du \midd x)\right.\\
	&\hspace{0.5cm}=\  \int_0^t\frac{F(du\midd x)}{S(u^{-} \midd x)}\\
	&\hspace{0.5cm}=\  \Lambda(t \midd x) \ .
\end{align*}

%%% residual censoring
\section{Identification under alternative assumption}\label{sec:quasi}
In this section, we consider an alternative identifying assumption for use in contexts in which censoring can only occur in individuals who satisfy the sampling criterion. Assumption B is given in three parts as:
\begin{itemize}
	\item[] \textit{Assumption B1:} $W < C$ almost surely;
	\item[] \textit{Assumption B2:} $T$ and $W$ are conditionally independent given $X$;
	\item[] \textit{Assumption B3:} $T$ and $C$ are conditionally independent given $(X,W)$ and $W\leq T$.
\end{itemize}
Let $F_W$ denote the conditional distribution function of $W$ given $X$. Let $H_{T,C,W}$ and $H_W$ denote respectively the conditional distribution functions of $(T,C,W)$ and $W$ given both $X$ and $W \leq T$. Let  $H_{T,C \midd W}$, $H_{C\midd W}$, and $H_{T \midd W}$ denote respectively the conditional distribution functions of $(T,C)$, $C$, and $T$ given both $( W, X)$ and $W \leq T$.

We note that Assumption (B2) allows us to write
\begin{align}
    \label{eq:eq3}
    H_{T \midd W}(t \midd w,x) \ &=\ P(T \leq t \midd X = x, W = w, W \leq T)\nonumber\\
    &=\ P(T \leq t \midd X = x, W = w, T \geq w)\nonumber\\
    &=\ \frac{P(T \leq t, T \geq w \midd X = x, W = w)}{P(T \geq w \midd X = x, W = w)}\nonumber\\
    &=\ \frac{\I(w \leq t)P(w \leq T \leq t \midd X = x, W = w)}{P(T \geq w \midd X = x, W = w)}\nonumber\\
    &\stackrel{\text{(a)}}{=}\ \frac{\I(w \leq t)P(w \leq T \leq t \midd X = x)}{P(T \geq w \midd X = x)}\ ,
\end{align}
where (a) follows from Assumption B2. We then use standard probability rules to write
\begin{align*}
    \pi(x)F_1(u \midd x) \ &=\  P(\Delta = 1\midd X = x, W \leq Y)P(Y \leq u\midd \Delta = 1, X = x, W \leq Y)\\
	&=\  P(\Delta =1, Y \leq u \midd W \leq Y, X = x)\\
	&\stackrel{\text{(b)}}{=}\ P(\Delta =1, Y \leq u \midd W \leq T, X = x)\\
	&=\  P(T \leq C, T \leq u \midd W \leq T, X = x)\\
	&=\  \iiint \I(t \leq u, c \geq t)H_{T,C,W}(dt, dc, dw \midd x)\\
	&=\ \iiint \I(t \leq u, c \geq t)H_{T,C|W}(dt, dc \midd w, x)H_W(dw \midd x)\\
	&\stackrel{\text{(c)}}{=}\ \iiint \I(t \leq u, c \geq t)H_{C|W}(dc \midd w, x)H_{T|W}(dt \midd w, x)H_W(dw \midd x)\\
	&\stackrel{\text{(d)}}{=}\ \iiint \frac{\I(t \leq u, c \geq t, w \leq t)H_{C|W}(dc \midd w, x)F(dt \midd x) H_W(dw \midd x)}{P(T \geq w \midd X = x)}\ ,
\end{align*}
where (b) follows from Assumption B1, (c) from Assumption B3, and (d) from equation \eqref{eq:eq3}. The differential of this function with respect to $u$ is 
\begin{align*}
    \pi(x)F_1(du \midd x) = \iint \frac{\I(c \geq u, w \leq u)H_{C|W}(dc \midd w, x) H_W(dw \midd x)F(du \midd x)}{P(T \geq w \midd X = x)}\ .
\end{align*}
Let $R(u,x) := \iint \frac{\I(c \geq u, w \leq u)H_{C|W}(dc \midd w, x) H_W(dw \midd x)}{P(T \geq w \midd X = x)}$. As in Appendix \ref{sec:calculations}, the denominator of $\Lambda^{\text{obs}}$ can be written as
\begin{align*}
    P(W \leq u, &Y \geq u \midd W \leq Y, X = x) \ \stackrel{\text{(e)}}{=}\ P(W \leq u, Y \geq u \midd W \leq T, X = x)\\
    &=\ P(W \leq u, C \geq u, T \geq u \midd W \leq T, X = x)\\
    &= \iiint \I(t \geq u, c \geq u, w \leq u)H_{T,C,W}(dt, dc, dw \midd x)\\
    &=\ \iiint \I(t \geq u, c \geq u, w \leq u)H_{T,C|W}(dt, dc \midd w, x)H_W(dw \midd x)\\
	&\stackrel{\text{(f)}}{=}\ \iiint \I(t \geq u, c \geq u, w \leq u)H_{C|W}(dc \midd w, x)H_{T|W}(dt \midd w, x)H_W(dw \midd x)\\
	&\stackrel{\text{(g)}}{=}\ \iiint \frac{\I(t \geq u, c \geq u, w \leq u, w \leq t)H_{C|W}(dc \midd w, x)F_T(dt \midd x) H_W(dw \midd x)}{P(T \geq w \midd X = x)}\\
	&=\ \iiint \frac{\I(t \geq u, c \geq u, w \leq u)H_{C|W}(dc \midd w, x)F_T(dt \midd x) H_W(dw \midd x)}{P(T \geq w \midd X = x)}\\
	&=\ S(u^- \midd x)R(u,x)\ ,
\end{align*}
where (e) follows from Assumption B1, (f) from Assumption B3, and (g) from equation \eqref{eq:eq3}. Combining this with the numerator, we find, as claimed, that
\begin{align*}
	\Lambda^{\text{obs}}(t \midd x) \ &=\  \int_0^t\frac{\pi(x)F_{1}(du \midd x)}{G_{1}(u \midd x)\pi(x)\left\{1 - F_1(u^- \midd x)\right\} +G_{0}(u \midd x)\left\{1-\pi(x)\right\}\left\{1 - F_0(u^- \midd x)\right\}}\\
	&=\  \int_0^t \frac{R(u,x)}{R(u,x)S(u^{-} \midd x)} F(du \midd x)\ =\   \int_0^t\frac{F(du\midd x)}{S(u^{-} \midd x)} \ =\  \Lambda(t \midd x) \ .
\end{align*}

%%% sim details
\section{Simulation details}\label{sec:sim details}

\subsection{Additional details on estimators and data-generating mechanism}

Below, we include additional information on the Super Learner libraries used for the conditional survival estimators compared via simulation in  Section \ref{sec:simulations} of the main text, as well as censoring and truncation rates for the simulation data-generating mechanisms. 

\begin{itemize}
    \item[-] \textbf{Table \ref{tab:SL algorithms}:} Super Learner library used for estimating binary regressions in global and local survival stacking. 
    
    \item[-] \textbf{Table \ref{tab:survSL algorithms}:} Super Learner library used for \texttt{survSuperLearner}.
    
    \item[-] \textbf{Figure \ref{fig:example densities}:} Example densities for the time-to-event variable $T$ under the two data-generating mechanisms used in simulations.
    
    \item[-] \textbf{Table \ref{tab:cens trunc}:} Average truncation rates in numerical experiments. 
\end{itemize}

%%% Super Learner algorithms
\begin{table}%[h]
	\centering 
		\begin{tabular}{l ll }\toprule 
			Algorithm name & Algorithm description & Tuning parameters\\ \midrule 
			\texttt{SL.mean} & Marginal mean&NA\\\cmidrule{1-3}
			\texttt{SL.glm.interaction} & Logistic regression with pairwise interactions&NA\\ \cmidrule{1-3}
			\texttt{SL.gam} & Generalized additive model&default\\\cmidrule{1-3}
			\texttt{SL.earth} & Multivariate adaptive regression splines&default\\\cmidrule{1-3}
			\texttt{SL.ranger} & Random forest&default\\\cmidrule{1-3}
			\texttt{SL.xgboost} & Gradient-boosted trees&\texttt{ntrees} $\in \{250, 500, 1000\}$\\
			&&\texttt{max\_depth} $\in \{1, 2\}$\\\bottomrule
		\end{tabular}
		\caption{Algorithms included in the Super Learner for global and local survival stacking. All tuning parameters besides those for \texttt{SL.xgboost} were set to default values. In particular, \texttt{gam} was implemented with \texttt{degree = 2}; \texttt{earth} with \texttt{degree = 2, penalty = 3, nk =} number of predictors plus 1, \texttt{endspan = 0}, \texttt{minspan = 0}; and \texttt{ranger} with \texttt{num.trees = 500, mtry =} the square root of the number of predictors, \texttt{min.node.size = 1, sample.fraction = 1} with replacement. For \texttt{SL.xgboost}, \texttt{shrinkage} was set to 0.01, \texttt{minobspernode} was set to 1, and each combination of \texttt{ntrees} and \texttt{max\_depth} was included in the Super Learner library.}
		\label{tab:SL algorithms}
\end{table} 

%%% survSL algorithms
\begin{table}%[h]
	\centering
		\begin{tabular}{p{0.25\linewidth}  p{0.65\linewidth}}\toprule
			Algorithm name & Algorithm description \\ \midrule
		\texttt{survSL.km} & Kaplan-Meier estimator\\	\cmidrule{1-2}
			\texttt{survSL.expreg} & Survival regression assuming event and censoring times follow an exponential distribution conditional on covariates\\ 	\cmidrule{1-2}
			\texttt{survSL.weibreg} & Survival regression assuming event and censoring times follow a Weibull distribution conditional on covariates\\	\cmidrule{1-2}
			\texttt{survSL.loglogreg} & Survival regression assuming event and censoring times follow a log-logistic distribution conditional on covariates\\	\cmidrule{1-2}
			\texttt{survSL.gam} & Main-terms generalized additive Cox proportional hazards estimator as implemented in the \texttt{mgcv} package\\	\cmidrule{1-2}
			\texttt{survSL.coxph} & Main-terms Cox proportional hazards estimator with Breslow baseline cumulative hazard\\	\cmidrule{1-2}
			\texttt{survSL.rfsrc} & Random survival forest as implemented in the \texttt{randomForestSRC} package\\\bottomrule
		\end{tabular}
		\caption{Algorithms included in the survival Super Learner. All tuning parameters were set to default values. In particular, \texttt{gam} was implemented with \texttt{degree = 1}; and \texttt{rfsrc} with \texttt{ntree = 500}, \texttt{mtry =} the square root of the number of predictors, \texttt{nodesize = 15}, \texttt{splitrule = "logrank"}, \texttt{sampsize = 1} with replacement.}
		\label{tab:survSL algorithms}
\end{table} 

%%%% figure example densities
\begin{figure}%[h]
	\begin{center}
		\includegraphics[width=\linewidth]{examp_densities.png}
	\end{center}
	\vspace{-0.5cm}
	\caption{Example densities for the time-to-event variable $T$ under the two data-generating mechanisms used in simulations. Each plot shows the conditional density of $T$ given $X$ for ten random draws from the distribution of $X$.}
	\label{fig:example densities}
\end{figure}

%%% Censoring and truncation
\begin{table}%[h]
	\centering 
		\begin{tabular}{l c c   c} 
		\toprule 
			Study design & Skew &  Setting& Truncation rate\\ \midrule 
            Prospective & Right & Non-proportional hazards &70\%\\
             && Proportional hazards &66\%\\
             && Discrete & 70\% \\
            &Left & Non-proportional hazards&46\%\\
            && Proportional hazards &51\%\\
            && Discrete & 46\% \\
            Retrospective&Right&Non-proportional hazards &35\%\\
            &Left & Non-proportional hazards&65\%\\\bottomrule
		\end{tabular}
		\caption{Average truncation rates across simulations.}
		\label{tab:cens trunc}
\end{table} 

%%% additional sims
\section{Additional numerical results}\label{sec:additional sims}

\subsection{Estimator performance under proportional hazards}

In addition to the primary empirical results presented in the main text, we evaluated the performance of global and local survival stacking when the data satisfied the proportional hazards assumption. The covariate vector $X$, censoring variable $C$, and study entry variable $W$ were generated in the same manner as in the primary numerical experiments described in Section \ref{sec:simulations}. Given covariate vector $X = x$, we simulated the event time $T$ to be distributed as $100c(x)Z_3$, where $c(x) = \exp\left\{\tfrac{1}{2}\left(x_1 + x_2 + x_3 + x_4 + x_5\right)\right\}$ was the hazard ratio. In the right-skewed setting, $Z_3$ was a Beta(2,3) random variable, and in the left-skewed setting $Z_3$ was a Beta(3,2) random variable. We considered the prospective setting with left truncation and 25\% censoring rate. We evaluated performance in the same manner as described in Section \ref{sec:simulations}. We compared global survival stacking, local survival stacking, and the Cox model with Breslow baseline hazard estimator. 

We display the results for the proportional hazards simulation in Figure \ref{fig:ph ltrunc}. The Cox model, which in this case was correctly specified, yields the best overall performance across all metrics. Among the machine learning approaches, local stacking on a 40 cutpoint grid performs the best by a modest margin, and global survival stacking demonstrates good performance as well.  As in the primary empirical results in Section \ref{sec:simulations}, local stacking is more sensitive to grid size choice. Local stacking on a grid of all observed event times tends to show increasing estimation error beyond a sample size of 500. 

	\begin{figure}
	\centering
			\includegraphics[width=0.98\linewidth]{ph_ltrunc.png}
		\vspace{-0.3cm}
		\caption{Performance of conditional survival estimators with right-censored, left-truncated data generated under a proportional hazards model. The methods compared were global survival stacking, local survival stacking, and the main-terms Cox proportional hazards model with Breslow baseline hazard estimator. Time grids are based on quantiles of observed follow-up times (global stacking) or observed event times (local stacking). From top to bottom, rows correspond to MISE and to MSE at 50th, 75th, and 90th percentiles of observed event times. Each boxplot consists of 100 simulation replicates.} 
		\label{fig:ph ltrunc}
	\end{figure}

\subsection{Estimator performance when events are observed at discrete times}

For the discrete-time numerical experiments, we generated $X$, $T$, $C$, and $W$ in the same manner as in the primary numerical experiments described in Section \ref{sec:simulations}. For $m$ the desired number of times in the discrete-time grid, we divided the interval $[0,100]$ into $m$ equally sized intervals $I_1, I_2, \dots, I_m$. For all $Y$ falling in $I_j$, we set $\tilde{Y}$ equal to the right endpoint of interval $I_j$ and used $\tilde{Y}$ as the observed follow-up time. In this way, while the distribution of $T$ was continuous, $\tilde{Y}$ was observed on a discrete time scale. Likewise, for all $W$ falling in $I_j$, we set $\tilde{W}$ equal to the left endpoint of interval $I_j$. We considered the prospective setting with left truncation and 25\% censoring rate. We evaluated performance in the same manner as described in Section \ref{sec:simulations} and compared the performance of global and local stacking on grids of all observed follow-up and event times, respectively. We used the product integral form for global stacking. We included the main-terms Cox model as a comparator. 

We display the results for the discrete-time experiment with 10 intervals in Figure \ref{fig:discrete ltrunc 10}, with 20 intervals in Figure \ref{fig:discrete ltrunc 20}, and with 50 intervals in Figure \ref{fig:discrete ltrunc 50}. With 10 and 20 intervals, the overall performance of global and local stacking are similar. With 50 intervals, global survival stacking generally outperforms local survival stacking in the left-skewed setting, and the two perform similarly in the right-skewed setting. The MSE and MISE for global stacking are similar in the 50 interval setting as in the continuous-time setting. 

\begin{figure}%[h]
	\centering
			\includegraphics[width=1\linewidth]{discrete_ltrunc_bin10.png}
		\vspace{-1cm}
		\caption{Performance of conditional survival estimators with right-censored, left-truncated data observed on a discrete grid of 10 time-points. The methods compared were global survival stacking, local survival stacking, and the main-terms Cox proportional hazards model with Breslow baseline hazard estimator. Global and local survival stacking were implemented using a grid of every observed follow-up time (global) or every observed event time (local). From top to bottom, rows correspond to MISE and to MSE at 50th, 75th, and 90th percentiles of observed event times. Each boxplot consists of 100 simulation replicates.} 
		\label{fig:discrete ltrunc 10}
\end{figure}

\begin{figure}%[h]
	\centering
			\includegraphics[width=1\linewidth]{discrete_ltrunc_bin20.png}
		\vspace{-1cm}
		\caption{Performance of conditional survival estimators with right-censored, left-truncated data observed on a discrete grid of 20 time-points. The methods compared were global survival stacking, local survival stacking, and the main-terms Cox proportional hazards model with Breslow baseline hazard estimator. Global and local survival stacking were implemented using a grid of every observed follow-up time (global) or every observed event time (local). From top to bottom, rows correspond to MISE and to MSE at 50th, 75th, and 90th percentiles of observed event times. Each boxplot consists of 100 simulation replicates.} 
		\label{fig:discrete ltrunc 20}
\end{figure}

 \begin{figure}%[h]
		\centering
			\includegraphics[width=1\linewidth]{discrete_ltrunc_bin50.png}
		\vspace{-1cm}
		\caption{Performance of conditional survival estimators with right-censored, left-truncated data observed on a discrete grid of 50 time-points. The methods compared were global survival stacking, local survival stacking, and the main-terms Cox proportional hazards model with Breslow baseline hazard estimator.  Global and local survival stacking were implemented using a grid of every observed follow-up time (global) or every observed event time (local). From top to bottom, rows correspond to MISE and to MSE at 50th, 75th, and 90th percentiles of observed event times. Each boxplot consists of 100 simulation replicates.} 
		\label{fig:discrete ltrunc 50}
\end{figure}

\subsection{Comparison of survival function mappings in global survival stacking}

As discussed in depth in Section \ref{sec:estimation}, the product integral form of the global survival stacking estimator can, in practice, fall outside the interval $[0,1]$. The exponential form, obtained by exponentiating the negative estimated cumulative hazard function, cannot fall below 0. However, the exponential mapping from hazard to survival function only holds mathematically if $T$ has a continuous distribution, so it is not clear if it should perform as well as the product integral form when the hazard is evaluated on a grid of times. 

We performed a simulation study to compare the two forms (product integral and exponential) of our estimator in the prospective setting with left truncation and right censoring. Both estimators used a grid of 40 cutpoints evenly spaced on the quantile scale. Data were generated as in the other prospective settings, and performance was again evaluated using MISE and MSE at three landmark times. In addition to assessing performance, we also recorded the proportion of estimated survival probabilities in the test data that fell outside the interval $[0,1]$.

The overall performance of global survival stacking appears insensitive to the choice of survival function mapping (Figure \ref{fig:formcompare}). For the product integral form, between 0.6\% and 1.5\% of estimated survival probabilities fell outside the unit interval, depending on the training data sample size (Table \ref{tab:incompat}). For the exponential form, none of the survival function estimates fell outside the unit interval. When the distribution of $T$ is continuous, we recommend using the exponential form to protect again potential issues arising in a particular sample.

	\begin{figure}%[h]
		\centering
			\includegraphics[width=1\linewidth]{prospective_ltrunc_formcompare.png}
		\vspace{-0.5cm}
		\caption{Performance of different forms of the global survival stacking estimator in the prospective study design with left truncation and right censoring. The two forms are based on the mappings from hazard to survival function (product integral and exponential). Each boxplot represents 100 simulation replicates.} 
		\label{fig:formcompare}
	\end{figure}
	
	%%% incompatibility
\begin{table}%[h]
	\centering
		\begin{tabular}{l c c} \toprule 
			Estimator & Training sample size & Percent of estimates outside $[0,1]$\\ \midrule 
            Exponential & 250 & 0 \\
            & 500 & 0\\
            & 750 & 0\\
            & 1000 & 0\\
            Product integral & 250 & 0.6\%\\
            & 500& 1.3\%\\
            & 750 & 1.5\%\\
            & 1000 & 1.4\%\\\bottomrule
		\end{tabular}
		\caption{Percentage of estimated survival probabilities falling outside $[0,1]$ using two forms of the global survival stacking estimator in the prospective study design with left truncation and right censoring.}
		\label{tab:incompat}
\end{table} 

\section{Details on publicly available datasets}\label{sec:data details}

We describe the publicly available survival datasets analyzed in Section \ref{sec:real data performance}. 

\noindent{\textbf{FLCHAIN:}} The Assay of Serum-Free Light Chain study investigated the relationship between serum-free light chain and mortality in residents of Olmstead County \citep{Kyle2006}. We used eight features for prediction: age, sex, calendar year of sample collection, serum-free light chain kappa portion, serum-free light chain lambda portion, free light chain group, serum creatinine, and an indicator of monoclonal gammopathy diagnosis. After removal of individuals with missing data, the dataset consisted of 6542 individuals. This dataset is available in the \texttt{survival} package \citep{survival-package}. 

\noindent{\textbf{GBSG:}} The German Breast Cancer Study Group data is derived from a 1984-1989 trial of patients with node-positive breast cancer \citep{Schumacher1994}. The outcome of interest was recurrence-free survival time, with seven features of interest: hormone therapy, age, menopausal status, tumor size, tumor grade, number of positive nodes, progesterone receptor positivity, and estrogen receptor positivity. We used dummy variables for tumor grade, which consists of three categories. After removal of individuals with missing data, the dataset consisted of 684 individuals. It is available in the \texttt{survival} package \citep{survival-package}. 

\noindent{\textbf{METABRIC:}} This dataset was produced by the Molecular Taxonomy of Breast Cancer International Consortium \citep{Curtis2012}. The outcome of interest was mortality, and the features of interest included expression of different genes (MKI67, EGFR, PGR, and ERBB2), as well as five clinical features (hormone treatment, radiotherapy, chemotherapy, estrogen receptor positivity, and age at diagnosis). This dataset consisted of 1904 individuals, after removal of individuals with missing data. It is available in the \texttt{DeepSurv} software package \citep{Katzman2018}. 

\noindent{\textbf{NWTCO:}} The National Wilms' Tumor Study investigated the relationship between time to tumor relapse and several prognostic variables, including two types of histology \citep{Dangio1976}. We included five features: local histology, central histology, age, disease stage, and an indicator of whether the individual was a participant in NWTS 3 or 4. We used dummy variables for disease stage, which consists of four categories. This dataset consisted of 4028 individuals and is available in the \texttt{survival} package \citep{survival-package}.  

\noindent{\textbf{SUPPORT:}} The Study to Understand Prognoses and Preferences for Outcomes and Risks of Treatments investigated the relationship between clinical outcomes among seriously ill hospitalized adults \citep{Knaus1995}. For our analysis, the outcome of interest was mortality, with 14 features of interest: sex, age, race, number of comorbidities, blood pressure, heart rate, respiration, white blood cell count, temperature, serum creatinine, serum sodium, dementia diagnosis, diabetes diagnosis, and cancer diagnosis. Dummy variables were used for race (five categories) and cancer (three categories). After removal of individuals with missing data, the dataset consisted of 8873 individuals. This dataset is available on the Vanderbilt Biostatistics website. 

\end{appendices}

\end{document} 