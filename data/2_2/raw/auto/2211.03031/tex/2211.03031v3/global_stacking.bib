@article{Knaus1995,
  title={The SUPPORT prognostic model: Objective estimates of survival for seriously ill hospitalized adults},
  author={Knaus, William A and Harrell, Frank E and Lynn, Joanne and Goldman, Lee and Phillips, Russell S and Connors, Alfred F and Dawson, Neal V and Fulkerson, William J and Califf, Robert M and Desbiens, Norman and others},
  journal={Annals of internal medicine},
  volume={122},
  number={3},
  pages={191--203},
  year={1995},
  publisher={American College of Physicians}
}

@article{Dangio1976,
   author = {Giulio J. D'Angio and Audrey E. Evans and Norman Breslow and Bruce Beckwith and Harry Bishop and Polly Feigl and Willard Goodwin and Lucian L. Leape and Lucius F. Sinks and Wataru Sutow and Melvin Tefft and James Wolff},
   doi = {10.1002/1097-0142(197608)38:2<633::AID-CNCR2820380203>3.0.CO;2-S},
   issn = {10970142},
   issue = {2},
   journal = {Cancer},
   pages = {633-646},
   pmid = {184912},
   title = {The treatment of Wilms' tumor. Results of the national Wilms' tumor study},
   volume = {38},
   year = {1976},
}

@article{Schumacher1994,
  title={Randomized 2 x 2 trial evaluating hormonal treatment and the duration of chemotherapy in node-positive breast cancer patients. German Breast Cancer Study Group.},
  author={Schumacher, M and Bastert, G and Bojar, H and H{\"u}bner, K and Olschewski, M and Sauerbrei, W and Schmoor, C and Beyerle, C and Neumann, RL and Rauschecker, HF},
  journal={Journal of Clinical Oncology},
  volume={12},
  number={10},
  pages={2086--2093},
  year={1994}
}

@Manual{survival-package,
    title = {A Package for Survival Analysis in R},
    author = {Terry M Therneau},
    year = {2022},
    note = {R package version 3.3-1},
    url = {https://CRAN.R-project.org/package=survival},
  }

@article{Kyle2006,
  title={Prevalence of monoclonal gammopathy of undetermined significance},
  author={Kyle, Robert A and Therneau, Terry M and Rajkumar, S Vincent and Larson, Dirk R and Plevak, Matthew F and Offord, Janice R and Dispenzieri, Angela and Katzmann, Jerry A and Melton III, L Joseph},
  journal={New England Journal of Medicine},
  volume={354},
  number={13},
  pages={1362--1369},
  year={2006},
  publisher={Mass Medical Soc}
}

@article{Curtis2012,
  title={The genomic and transcriptomic architecture of 2,000 breast tumours reveals novel subgroups},
  author={Curtis, Christina and Shah, Sohrab P and Chin, Suet-Feung and Turashvili, Gulisa and Rueda, Oscar M and Dunning, Mark J and Speed, Doug and Lynch, Andy G and Samarajiwa, Shamith and Yuan, Yinyin and others},
  journal={Nature},
  volume={486},
  number={7403},
  pages={346--352},
  year={2012},
  publisher={Nature Publishing Group UK London}
}


@article{Gerds2006,
   abstract = {In survival analysis with censored data the mean squared error of prediction can be estimated by weighted averages of time-dependent residuals. Graf et al. (1999) suggested a robust weighting scheme based on the assumption that the censoring mechanism is independent of the covariates. We show consistency of the estimator. Furthermore, we show that a modified version of this estimator is consistent even when censoring and event times are only conditionally independent given the covariates. The modified estimators are derived on the basis of regression models for the censoring distribution. A simulation study and a real data example illustrate the results. © 2006 WILEY-VCH Verlag GmbH & Co. KGaA.},
   author = {Thomas A. Gerds and Martin Schumacher},
   doi = {10.1002/bimj.200610301},
   issn = {03233847},
   issue = {6},
   journal = {Biometrical Journal},
   keywords = {Brier score,Censoring bias,Inverse of probability of censoring weighting,Model validation,Survival analysis},
   month = {12},
   pages = {1029-1040},
   pmid = {17240660},
   title = {Consistent estimation of the expected {B}rier score in general survival models with right-censored event times},
   volume = {48},
   year = {2006},
}

@article{Aalen1978,
   author = {Odd Aalen},
   issue = {4},
   journal = {The Annals of Statistics},
   pages = {701-726},
   title = {Nonparametric inference for a family of counting processes},
   volume = {6},
   year = {1978},
}
@techreport{Beran1981,
   abstract = {The paper deals with a class of non-parametric regression estimates introduced by Beran (1981) to estimate conditional survival functions in the presence of right censoring. Weak convergence results are established for kernel and nearest neighbour estimates of the conditional cumulative hazard and survival functions as well as the quantile and L-type regression functionals.},
   author = {Rudolf Beran},
   issn = {0303-6898},
   keywords = {Censored sample,Echantillon censuré,Estimación no paramétrica,Estimation non paramétrique,Kernel method,Muestra censurada,Méthode noyau,Método kernel,Nearest neighbour,Non parametric estimation,Plus proche voisin,Supervivencia,Survie,Survival,Vecino más cercano},
   title = {Non-parametric regression with censored survival time data},
   year = {1981},
   institution = {University of California, Berkeley},
}
@article{Breslow1972,
   author = {Norman E. Breslow},
   issue = {2},
   journal = {Journal of the Royal Statistical Society. Series B (Statistical Methodology)},
   pages = {216-217},
   title = {Discussion of the paper by {D}.{R}. {Cox}.},
   volume = {34},
   year = {1972},
}
@article{Buchbinder2008,
   abstract = {Background: Observational data and non-human primate challenge studies suggest that cell-mediated immune responses might provide control of HIV replication. The Step Study directly assessed the efficacy of a cell-mediated immunity vaccine to protect against HIV-1 infection or change in early plasma HIV-1 levels. Methods: We undertook a double-blind, phase II, test-of-concept study at 34 sites in North America, the Caribbean, South America, and Australia. We randomly assigned 3000 HIV-1-seronegative participants by computer-generated assignments to receive three injections of MRKAd5 HIV-1 gag/pol/nef vaccine (n=1494) or placebo (n=1506). Randomisation was prestratified by sex, adenovirus type 5 (Ad5) antibody titre at baseline, and study site. Primary objective was a reduction in HIV-1 acquisition rates (tested every 6 months) or a decrease in HIV-1 viral-load setpoint (early plasma HIV-1 RNA measured 3 months after HIV-1 diagnosis). Analyses were per protocol and modified intention to treat. The study was stopped early because it unexpectedly met the prespecified futility boundaries at the first interim analysis. This study is registered with ClinicalTrials.gov, number NCT00095576. Findings: In a prespecified interim analysis in participants with baseline Ad5 antibody titre 200 or less, 24 (3%) of 741 vaccine recipients became HIV-1 infected versus 21 (3%) of 762 placebo recipients (hazard ratio [HR] 1·2 [95% CI 0·6-2·2]). All but one infection occurred in men. The corresponding geometric mean plasma HIV-1 RNA was comparable in infected male vaccine and placebo recipients (4·61 vs 4·41 log10 copies per mL, one tailed p value for potential benefit 0·66). The vaccine elicited interferon-γ ELISPOT responses in 75% (267) of the 25% random sample of all vaccine recipients (including both low and high Ad5 antibody titres) on whose specimens this testing was done (n=354). In exploratory analyses of all study volunteers, irrespective of baseline Ad5 antibody titre, the HR of HIV-1 infection between vaccine and placebo recipients was higher in Ad5 seropositive men (HR 2·3 [95% CI 1·2-4·3]) and uncircumcised men (3·8 [1·5-9·3]), but was not increased in Ad5 seronegative (1·0 [0·5-1·9]) or circumcised (1·0 [0·6-1·7]) men. Interpretation: This cell-mediated immunity vaccine did not prevent HIV-1 infection or reduce early viral level. Mechanisms for insufficient efficacy of the vaccine and the increased HIV-1 infection rates in subgroups of vaccine recipients are being explored. Funding: Merck Research Laboratories; the Division of AIDS, National Institute of Allergy and Infectious Diseases, in the US National Institutes of Health (NIH); and the NIH-sponsored HIV Vaccine Trials Network (HVTN). © 2008 Elsevier Ltd. All rights reserved.},
   author = {Susan P. Buchbinder and Devan V. Mehrotra and Ann Duerr and Daniel W. Fitzgerald and Robin Mogg and David Li and Peter B. Gilbert and Javier R. Lama and Michael Marmor and Carlos del Rio and M. Juliana McElrath and Danilo R. Casimiro and Keith M. Gottesdiener and Jeffrey A. Chodakewitz and Lawrence Corey and Michael N. Robertson},
   doi = {10.1016/S0140-6736(08)61591-3},
   issn = {01406736},
   journal = {The Lancet},
   pages = {1881-1893},
   pmid = {19012954},
   title = {Efficacy assessment of a cell-mediated immunity {H}{I}{V}-1 vaccine (the {S}tep Study): a double-blind, randomised, placebo-controlled, test-of-concept trial},
   volume = {372},
   year = {2008},
}
@article{Buckley1979,
   author = {Jonathan Buckley and Ian James},
   issue = {3},
   journal = {Biometrika},
   pages = {429-436},
   title = {Linear Regression with Censored Data},
   volume = {66},
   year = {1979},
}
@article{Ching2018,
   abstract = {Artificial neural networks (ANN) are computing architectures with many interconnections of simple neural-inspired computing elements, and have been applied to biomedical fields such as imaging analysis and diagnosis. We have developed a new ANN framework called Cox-nnet to predict patient prognosis from high throughput transcriptomics data. In 10 TCGA RNA-Seq data sets, Cox-nnet achieves the same or better predictive accuracy compared to other methods, including Cox-proportional hazards regression (with LASSO, ridge, and mimimax concave penalty), Random Forests Survival and CoxBoost. Cox-nnet also reveals richer biological information, at both the pathway and gene levels. The outputs from the hidden layer node provide an alternative approach for survival-sensitive dimension reduction. In summary, we have developed a new method for accurate and efficient prognosis prediction on high throughput data, with functional biological insights. The source code is freely available at https://github.com/lanagarmire/cox-nnet.},
   author = {Travers Ching and Xun Zhu and Lana Garmire},
   doi = {10.1101/093021},
   isbn = {1111111111},
   issue = {4},
   journal = {PLoS Computational Biology},
   keywords = {Artificial intelligence,Artificial neural network,Bioinformatics,Computer science,Data set,Dimensionality reduction,Machine learning,Massively parallel,Proportional hazards model,Random forest,Throughput},
   pages = {e1006076},
   title = {Cox-nnet: an artificial neural network method for prognosis prediction on high-throughput omics data},
   volume = {14},
   year = {2018},
}
@article{Cox1972,
   abstract = {The analysis of censored failure times is considered. It is assumed that on each individual are available values of one or more explanatory variables. The hazard function (age-specific failure rate) is taken to be a function of the explanatory variables and unknown regression coefficients multiplied by an arbitrary and unknown function of time. A conditional likelihood is obtained, leading to inferences about the unknown regression coefficients. Some generalizations are outlined.},
   author = {D. R. Cox},
   isbn = {00359246},
   issn = {00359246},
   issue = {2},
   journal = {Journal of the Royal Statistical Society. Series B (Methodological)},
   keywords = {age-specific failure rate,hazard function,life table,product},
   pages = {187-220},
   pmid = {2985181},
   title = {Regression Models and Life-Tables},
   volume = {34},
   year = {1972},
}
@article{Craig2021,
   abstract = {While there are many well-developed data science methods for classification and regression, there are relatively few methods for working with right-censored data. Here, we present "survival stacking": a method for casting survival analysis problems as classification problems, thereby allowing the use of general classification methods and software in a survival setting. Inspired by the Cox partial likelihood, survival stacking collects features and outcomes of survival data in a large data frame with a binary outcome. We show that survival stacking with logistic regression is approximately equivalent to the Cox proportional hazards model. We further recommend methods for evaluating model performance in the survival stacked setting, and we illustrate survival stacking on real and simulated data. By reframing survival problems as classification problems, we make it possible for data scientists to use well-known learning algorithms (including random forests, gradient boosting machines and neural networks) in a survival setting, and lower the barrier for flexible survival modeling.},
   author = {Erin Craig and Chenyang Zhong and Robert Tibshirani},
   journal = {arXiv:2107.13480},
   title = {Survival stacking: casting survival analysis as a classification problem},
   url = {http://arxiv.org/abs/2107.13480},
   year = {2021},
}
@article{DAgostino1990,
   author = {Ralph B. D'Agostino and Mei-Ling Lee and Albert J. Belanger and L. Adrienne Cupples and Keaven Anderson and William B. Kannel},
   issue = {12},
   journal = {Statistics in Medicine},
   pages = {1501-1515},
   title = {Relation of pooled logistic regression to time dependent {C}ox regression analysis: The Framingham Heart Study},
   volume = {9},
   year = {1990},
}
@article{Diaz2019,
   abstract = {The consistency of doubly robust estimators relies on the consistent estimation of at least one of two nuisance regression parameters. In moderate-to-large dimensions, the use of flexible data-adaptive regression estimators may aid in achieving this consistency. However, n1/2-consistency of doubly robust estimators is not guaranteed if one of the nuisance estimators is inconsistent. In this paper, we present a doubly robust estimator for survival analysis with the novel property that it converges to a Gaussian variable at an n1/2-rate for a large class of data-adaptive estimators of the nuisance parameters, under the only assumption that at least one of them is consistently estimated at an n1/4-rate. This result is achieved through the adaptation of recent ideas in semiparametric inference, which amount to (i) Gaussianizing (ie, making asymptotically linear) a drift term that arises in the asymptotic analysis of the doubly robust estimator and (ii) using cross-fitting to avoid entropy conditions on the nuisance estimators. We present the formula of the asymptotic variance of the estimator, which allows for the computation of doubly robust confidence intervals and p values. We illustrate the finite-sample properties of the estimator in simulation studies and demonstrate its use in a phase III clinical trial for estimating the effect of a novel therapy for the treatment of human epidermal growth factor receptor 2 (HER2)–positive breast cancer.},
   author = {Iván Díaz},
   doi = {10.1002/sim.8156},
   issn = {10970258},
   issue = {15},
   journal = {Statistics in Medicine},
   keywords = {CAN,cross-fitting,double robustness,targeted minimum loss–based estimation},
   month = {7},
   pages = {2735-2748},
   pmid = {30950107},
   publisher = {John Wiley and Sons Ltd},
   title = {Statistical inference for data-adaptive doubly robust estimators with survival outcomes},
   volume = {38},
   year = {2019},
}
@article{Duerr2012,
   abstract = {Background. The Step Study tested whether an adenovirus serotype 5 (Ad5)-vectored human immunodeficiency virus (HIV) vaccine could prevent HIV acquisition and/or reduce viral load set-point after infection. At the first interim analysis, nonefficacy criteria were met. Vaccinations were halted; participants were unblinded. In post hoc analyses, more HIV infections occurred in vaccinees vs placebo recipients in men who had Ad5-neutralizing antibodies and/or were uncircumcised. Follow-up was extended to assess relative risk of HIV acquisition in vaccinees vs placebo recipients over time.Methods.We used Cox proportional hazard models for analyses of vaccine effect on HIV acquisition and vaccine effect modifiers, and nonparametric and semiparametric methods for analysis of constancy of relative risk over time.Results.One hundred seventy-two of 1836 men were infected. The adjusted vaccinees vs placebo recipients hazard ratio (HR) for all follow-up time was 1.40 (95% confidence interval [CI], 1.03-1.92; P =. 03). Vaccine effect differed by baseline Ad5 or circumcision status during first 18 months, but neither was significant for all follow-up time. The HR among uncircumcised and/or Ad5-seropositive men waned with time since vaccination. No significant vaccine-associated risk was seen among circumcised, Ad5-negative men (HR, 0.97; P = 1.0) over all follow-up time.Conclusions.The vaccine-associated risk seen in interim analysis was confirmed but waned with time from vaccination. © 2012 The Author.},
   author = {Ann Duerr and Yunda Huang and Susan Buchbinder and Robert W. Coombs and Jorge Sanchez and Carlos Del Rio and Martin Casapia and Steven Santiago and Peter Gilbert and Lawrence Corey and Michael N. Robertson},
   doi = {10.1093/infdis/jis342},
   issn = {00221899},
   issue = {2},
   journal = {Journal of Infectious Diseases},
   pages = {258-266},
   pmid = {22561365},
   title = {Extended follow-up confirms early vaccine-enhanced risk of {H}{I}{V} acquisition and demonstrates waning effect over time among participants in a randomized trial of recombinant adenovirus HIV vaccine ({S}tep Study)},
   volume = {206},
   year = {2012},
}
@article{Fleming1984,
   author = {Thomas R Fleming and David P Harrington},
   issue = {20},
   journal = {Communications in Statistics: Theory and Methods},
   pages = {2469-2486},
   title = {Nonparametric estimation of the survival distribution in censored data},
   volume = {13},
   year = {1984},
}
@article{Fotso2018,
   author = {Stephane Fotso},
   journal = {arXiv:1801.05512},
   title = {Deep neural networks for survival analysis based on a multi-task framework},
   year = {2018},
}
@article{Friedman1982,
   author = {Michael Friedman},
   issue = {1},
   journal = {The Annals of Statistics},
   pages = {101-113},
   title = {Piecewise Exponential Models for Survival Data with Covariates},
   volume = {10},
   year = {1982},
}
@article{Gensheimer2019,
   abstract = {There is currently great interest in applying neural networks to prediction tasks in medicine. It is important for predictive models to be able to use survival data, where each patient has a known follow-up time and event/censoring indicator. This avoids information loss when training the model and enables generation of predicted survival curves. In this paper, we describe a discrete-time survival model that is designed to be used with neural networks, which we refer to as Nnet-survival. The model is trained with the maximum likelihood method using mini-batch stochastic gradient descent (SGD). The use of SGD enables rapid convergence and application to large datasets that do not fit in memory. The model is flexible, so that the baseline hazard rate and the effect of the input data on hazard probability can vary with follow-up time. It has been implemented in the Keras deep learning framework, and source code for the model and several examples is available online. We demonstrate the performance of the model on both simulated and real data and compare it to existing models Cox-nnet and Deepsurv.},
   author = {Michael F. Gensheimer and Balasubramanian Narasimhan},
   doi = {10.7717/peerj.6257},
   issn = {21678359},
   journal = {PeerJ},
   keywords = {Machine learning,Neural networks,Survival analysis},
   pages = {1-19},
   title = {A scalable discrete-time survival model for neural networks},
   year = {2019},
}
@article{Gill1990,
   author = {Richard D. Gill and Soren Johansen},
   issue = {4},
   journal = {The Annals of Statistics},
   pages = {1501-1555},
   title = {A Survey of Product-Integration with a View Toward Application in Survival Analysis},
   volume = {18},
   year = {1990},
}
@inproceedings{Giunchiglia2018,
   abstract = {Current medical practice is driven by clinical guidelines which are designed for the “average” patient. Deep learning is enabling medicine to become personalized to the patient at hand. In this paper we present a new recurrent neural network model for personalized survival analysis called rnn-surv. Our model is able to exploit censored data to compute both the risk score and the survival function of each patient. At each time step, the network takes as input the features characterizing the patient and the identifier of the time step, creates an embedding, and outputs the value of the survival function in that time step. Finally, the values of the survival function are linearly combined to compute the unique risk score. Thanks to the model structure and the training designed to exploit two loss functions, our model gets better concordance index (C-index) than the state of the art approaches.},
   author = {Eleonora Giunchiglia and Anton Nemchenko and Mihaela van der Schaar},
   doi = {10.1007/978-3-030-01424-7_3},
   isbn = {9783030014230},
   issn = {16113349},
   journal = {International Conference on Artificial Neural Networks},
   pages = {23-32},
   publisher = {Springer},
   title = {RNN-SURV: A deep recurrent model for survival analysis},
   booktitle = {Artificial Neural Networks and Machine Learning – ICANN 2018},
   year = {2018},
}
@article{Harrell1982,
   abstract = {A method is presented for evaluating the amount of information a medical test provides about individual patients. Emphasis is placed on the role of a test in the evaluation of patients with a chronic disease. In this context, the yield of a test is best interpreted by analyzing the prognostic information it furnishes. Information from the history, physical examination, and routine procedures should be used in assessing the yield of a new test. As an example, the method is applied to the use of the treadmill exercise test in evaluating the prognosis of patients with suspected coronary artery disease. The treadmill test is shown to provide surprisingly little prognostic information beyond that obtained from basic clinical measurements. © 1982, American Medical Association. All rights reserved.},
   author = {Frank E. Harrell and Robert M. Califf and David B. Pryor and Kerry L. Lee and Robert A. Rosati},
   doi = {10.1001/jama.1982.03320430047030},
   issn = {15383598},
   issue = {18},
   journal = {JAMA: The Journal of the American Medical Association},
   pages = {2543-2546},
   pmid = {7069920},
   title = {Evaluating the Yield of Medical Tests},
   volume = {247},
   year = {1982},
}
@article{Hastie1986,
   author = {Trevor Hastie and Robert Tibshirani},
   issue = {3},
   journal = {Statistical Science},
   keywords = {.},
   pages = {297-318},
   title = {Generalized Additive Models},
   volume = {1},
   year = {1986},
}
@article{Hothorn2006a,
   abstract = {Recursive binary partitioning is a popular tool for regression analysis. Two fundamental problems of exhaustive search procedures usually applied to fit such models have been known for a long time: overfitting and a selection bias towards covariates with many possible splits or missing values. While pruning procedures are able to solve the overfitting problem, the variable selection bias still seriously affects the interpretability of tree-structured regression models. For some special cases unbiased procedures have been suggested, however lacking a common theoretical foundation. We propose a unified framework for recursive partitioning which embeds tree-structured regression models into a well defined theory of conditional inference procedures. Stopping criteria based on multiple test procedures are implemented and it is shown that the predictive performance of the resulting trees is as good as the performance of established exhaustive search procedures. It turns out that the partitions and therefore the models induced by both approaches are structurally different, confirming the need for an unbiased variable selection. Moreover, it is shown thai the prediction accuracy of trees with early stopping is equivalent to the prediction accuracy of pruned trees with unbiased variable selection. The methodology presented here is applicable to all kinds of regression problems, including nominal, ordinal, numeric, censored as well as multivariate response variables and arbitrary measurement scales of the covariates. Data from studies on glaucoma classification, node positive breast cancer survival and mammography experience are re-analyzed. © 2006 American Statistical Association, Institute of Mathematical Statistics, and Interface Foundation of North America.},
   author = {Torsten Hothorn and Kurt Hornik and Achim Zeileis},
   doi = {10.1198/106186006X133933},
   isbn = {106186006X},
   issn = {10618600},
   issue = {3},
   journal = {Journal of Computational and Graphical Statistics},
   keywords = {Multiple testing,Multivariate regression trees,Ordinal regression trees,Permutation tests,Variable selection},
   pages = {651-674},
   title = {Unbiased recursive partitioning: A conditional inference framework},
   volume = {15},
   year = {2006},
}
@article{Hothorn2006,
   abstract = {We propose a unified and flexible framework for ensemble learning in the presence of censoring. For right-censored data, we introduce a random forest algorithm and a generic gradient boosting algorithm for the construction of prognostic and diagnostic models. The methodology is utilized for predicting the survival time of patients suffering from acute myeloid leukemia based on clinical and genetic covariates. Furthermore, we compare the diagnostic capabilities of the proposed censored data random forest and boosting methods, applied to the recurrence-free survival time of node-positive breast cancer patients, with previously published findings. © The Author 2005. Published by Oxford University Press. All rights reserved.},
   author = {Hothorn, T. and Bühlmann, P. and Dudoit, S. and van der Laan, M. J.},
   doi = {10.1093/biostatistics/kxj011},
   issn = {14654644},
   issue = {3},
   journal = {Biostatistics},
   keywords = {Censoring,Cross-validation,Ensemble methods,IPC weights,Loss function,Prediction,Prognostic factors,Survival analysis},
   pages = {355-373},
   pmid = {16344280},
   title = {Survival ensembles},
   volume = {7},
   year = {2006},
}
@article{Huang2009,
   abstract = {Early risk-prediction is essential to prevent cardiac al-lograft vasculopathy (CAV) and graft failure in heart transplant patients. We developed multivariate models to identify patients likely to experience CAV, severe CAV, and failure due to CAV, at 1, 5 and 10 years. A cohort of 172 patients was followed prospectively for 6.7 ± 3.9 years. Logistic regression models were developed and cross-validated using bootstrap resampling. Predictive markers of atherothrombosis (myocardial fibrin deposition, and loss of vascular antithrombin and tissue plasminogen activator) and arterial endothelial activation (intercellular adhesion molecule-1 expression) were measured in serial biopsies obtained within 3 months posttransplant. Most markers were univari-ately associated with outcome. Multivariate models showed that loss of tissue plasminogen activator was the dominant and, in most cases, only predictor of long-term CAV (p < 0.001), severe CAV (p < 0.001), and graft failure due to CAV (p < 0.001). The models discriminated patients having adverse outcomes, had particularly high negative predictive values (graft failure due to CAV: 99%, 99% and 95% at 1, 5 and 10 years) and predicted event incidence and time to event. Early absence of atherothrombotic risk identifies a patient subgroup that rarely develops CAV or graft failure, implying that this low-risk subgroup could possibly be followed with fewer invasive procedures.},
   author = {Ying Huang and Peter B Gilbert and Holly Janes},
   doi = {10.1111/j},
   issue = {3},
   journal = {Biometrics},
   keywords = {classification accuracy,constrained maximum likelihood,monotone treatment effect,potent},
   pages = {423-436},
   title = {Assessing Treatment-Selection Markers using a Potential Outcomes Framework},
   volume = {68},
   year = {2009},
}
@article{Ishwaran2008,
   abstract = {We introduce random survival forests, a random forests method for the analysis of right-censored survival data. New survival splitting rules for growing survival trees are introduced, as is a new missing data algorithm for imputing missing data. A conservation-of-events principle for survival forests is introduced and used to define ensemble mortality, a simple interpretable measure of mortality that can be used as a predicted outcome. Several illustrative examples are given, including a case study of the prognostic implications of body mass for individuals with coronary artery disease. Computations for all examples were implemented using the freely available R-software package, random Survival Forest. © Institute of Mathematical Statistics.},
   author = {Hemant Ishwaran and Udaya B. Kogalur and Eugene H. Blackstone and Michael S. Lauer},
   doi = {10.1214/08-AOAS169},
   issn = {19326157},
   issue = {3},
   journal = {Annals of Applied Statistics},
   keywords = {Conservation of events,Cumulative hazard function,Ensemble,Out-of-bag,Prediction error,Survival tree},
   pages = {841-860},
   title = {Random survival forests},
   volume = {2},
   year = {2008},
}
@article{Ishwaran2004,
   abstract = {Recent studies have confirmed heart rate fall after treadmill exercise testing, or heart rate recovery, as a powerful predictor of mortality from heart disease. Heart rate recovery depends on central reactivation of vagal tone and decreased vagal activity is a risk factor for death. If heart rate recovery is defined as the fall in heart rate after 1 minute following peak exercise, then a heart rate recovery value of 12 beats per minute (bpm) or lower has been shown to be a good prognostic threshold for identifying patients at high risk. Although this finding establishes a simple, useful relationship between heart recovery and mortality, a working understanding of how heart rate recovery interacts with other characteristics of a patient in determining risk of death is still largely unexplored. Such knowledge, addressed in this article, could improve the prognostic value of the exercise test. Our analysis is based on over 23,000 patients who underwent exercise testing. A rich assortment of data was collected on these patients, including clinical and physiological information, heart rate recovery, and other exercise test performance measures. Our approach was to grow relative risk forests, a novel method that combines random forest methodology with survival trees grown using Poisson likelihoods. Our analysis reveals a complex relationship between peak heart rate, age, level of fitness, heart rate recovery, and risk of death.},
   author = {Hemant Ishwaran and Eugene H. Blackstone and Claire E. Pothier and Michael S. Lauer},
   doi = {10.1198/016214504000000638},
   issn = {01621459},
   issue = {467},
   journal = {Journal of the American Statistical Association},
   keywords = {Cox regression,MARS,Proportional hazards,Random forests,Relative risk trees,Stochastic variable selection},
   pages = {591-600},
   title = {Relative risk forests for exercise heart rate recovery as a predictor of mortality},
   volume = {99},
   year = {2004},
}
@article{Kaplan1958,
   abstract = {Previous studies of target-cancellation performance in visuospatial neglect patients have reported lateral (left-right) and radial (near-far) gradients of attentional ability. The purpose of the present study was to replicate the reported attentional gradients in peripersonal space (within arms reach) and to examine whether lateral gradients of detection also appear in extrapersonal space (beyond arms reach), using equivalent tasks with no manual requirement. The relationship between radial gradients in peripersonal space and neglect severity (degree of lateral gradient) in extrapersonal space was also of interest. Right-hemisphere stroke subjects, with and without neglect, and healthy control subjects named visual targets on scanning sheets placed in peripersonal and extrapersonal space. The neglect group showed lateral gradients of increasing target detection from left to right in both peripersonal and extrapersonal space, which were not evident in the performance of either of the control groups. Double dissociations of neglect severity in peripersonal and extrapersonal space were also found in analyses of individual performance. Lesion analyses showed that peripersonal neglect was related to dorsal stream damage and extrapersonal neglect was related to ventral stream damage. Group analyses showed no significant radial gradients in peripersonal space in the three groups. In addition, while analyses of some individuals found significant near-far and far-near radial gradients, there was no correlation between radial gradients in peripersonal space and neglect severity in extrapersonal space. These results are discussed in terms of theorised hemispheric mechanisms of spatial attention and the relationship of neglect in the two co-ordinate spaces to the extent and location of damaged neurons in the right hemisphere.},
   author = {E.L. Kaplan and Paul Meier},
   isbn = {01621459},
   issn = {01621459},
   issue = {282},
   journal = {Journal of the American Statistical Association},
   pages = {457-481},
   pmid = {14670573},
   title = {Nonparametric Estimation from Incomplete Observations},
   volume = {53},
   year = {1958},
}
@article{Katzman2018,
   abstract = {Background: Medical practitioners use survival models to explore and understand the relationships between patients' covariates (e.g. clinical and genetic features) and the effectiveness of various treatment options. Standard survival models like the linear Cox proportional hazards model require extensive feature engineering or prior medical knowledge to model treatment interaction at an individual level. While nonlinear survival methods, such as neural networks and survival forests, can inherently model these high-level interaction terms, they have yet to be shown as effective treatment recommender systems. Methods: We introduce DeepSurv, a Cox proportional hazards deep neural network and state-of-the-art survival method for modeling interactions between a patient's covariates and treatment effectiveness in order to provide personalized treatment recommendations. Results: We perform a number of experiments training DeepSurv on simulated and real survival data. We demonstrate that DeepSurv performs as well as or better than other state-of-the-art survival models and validate that DeepSurv successfully models increasingly complex relationships between a patient's covariates and their risk of failure. We then show how DeepSurv models the relationship between a patient's features and effectiveness of different treatment options to show how DeepSurv can be used to provide individual treatment recommendations. Finally, we train DeepSurv on real clinical studies to demonstrate how it's personalized treatment recommendations would increase the survival time of a set of patients. Conclusions: The predictive and modeling capabilities of DeepSurv will enable medical researchers to use deep neural networks as a tool in their exploration, understanding, and prediction of the effects of a patient's characteristics on their risk of failure.},
   author = {Jared L. Katzman and Uri Shaham and Alexander Cloninger and Jonathan Bates and Tingting Jiang and Yuval Kluger},
   doi = {10.1186/s12874-018-0482-1},
   issn = {14712288},
   issue = {24},
   journal = {BMC Medical Research Methodology},
   keywords = {Deep learning,Survival analysis,Treatment recommendations},
   pages = {1-12},
   pmid = {29482517},
   title = {DeepSurv: Personalized treatment recommender system using a {C}ox proportional hazards deep neural network},
   volume = {18},
   year = {2018},
}
@article{Li2013,
   abstract = {We propose a data-driven least-square cross-validation method to optimally select smoothing parameters for the nonparametric estimation of conditional cumulative distribution functions and conditional quantile functions. We allow for general multivariate covariates that can be continuous, categorical, or a mix of either. We provide asymptotic analysis, examine finite-sample properties via Monte Carlo simulation, and consider an application involving testing for first-order stochastic dominance of children's health conditional on parental education and income. This article has supplementary materials online. © 2013 American Statistical Association.},
   author = {Qi Li and Juan Lin and Jeffrey S. Racine},
   doi = {10.1080/07350015.2012.738955},
   issn = {07350015},
   issue = {1},
   journal = {Journal of Business and Economic Statistics},
   keywords = {Cross-validation,Data-driven,Kernel smoothing},
   pages = {57-65},
   title = {Optimal bandwidth selection for nonparametric conditional distribution and quantile functions},
   volume = {31},
   year = {2013},
}
@article{Li2008,
   abstract = {We propose a new nonparametric conditional cumulative distribution function kernel estimator that admits a mix of discrete and categorical data along with an associated nonparametric conditional quantile estimator. Bandwidth selection for kernel quantile regression remains an open topic of research. We employ a conditional probability density function-based bandwidth selector proposed by Hall, Racine, and Li that can automatically remove irrelevant variables and has impressive performance in this setting. We provide theoretical underpinnings including rates of convergence and limiting distributions. Simulations demonstrate that this approach performs quite well relative to its peers; two illustrative examples serve to underscore its value in applied settings. © 2008 American Statistical Association.},
   author = {Qi Li and Jeffrey S. Racine},
   doi = {10.1198/073500107000000250},
   issn = {07350015},
   issue = {4},
   journal = {Journal of Business and Economic Statistics},
   keywords = {Conditional quantiles,Density estimation,Kernel smoothing},
   pages = {423-434},
   title = {Nonparametric estimation of conditional CDF and quantile functions with mixed categorical and continuous data},
   volume = {26},
   year = {2008},
}
@article{Molinaro2004,
   abstract = {We propose a unified strategy for estimator construction, selection, and performance assessment in the presence of censoring. This approach is entirely driven by the choice of a loss function for the full (uncensored) data structure and can be stated in terms of the following three main steps. (1) First, define the parameter of interest as the minimizer of the expected loss, or risk, for a full data loss function chosen to represent the desired measure of performance. Map the full data loss function into an observed (censored) data loss function having the same expected value and leading to an efficient estimator of this risk. (2) Next, construct candidate estimators based on the loss function for the observed data. (3) Then, apply cross-validation to estimate risk based on the observed data loss function and to select an optimal estimator among the candidates. A number of common estimation procedures follow this approach in the full data situation, but depart from it when faced with the obstacle of evaluating the loss function for censored observations. Here, we argue that one can, and should, also adhere to this estimation road map in censored data situations.Tree-based methods, where the candidate estimators in Step 2 are generated by recursive binary partitioning of a suitably defined covariate space, provide a striking example of the chasm between estimation procedures for full data and censored data (e.g., regression trees as in CART for uncensored data and adaptations to censored data). Common approaches for regression trees bypass the risk estimation problem for censored outcomes by altering the node splitting and tree pruning criteria in manners that are specific to right-censored data. This article describes an application of our unified methodology to tree-based estimation with censored data. The approach encompasses univariate outcome prediction, multivariate outcome prediction, and density estimation, simply by defining a suitable loss function for each of these problems. The proposed method for tree-based estimation with censoring is evaluated using a simulation study and the analysis of CGH copy number and survival data from breast cancer patients. © 2004 Elsevier Inc. All rights reserved.},
   author = {Molinaro, A. M. and Dudoit, S and van der Laan, M. J.},
   doi = {10.1016/j.jmva.2004.02.003},
   issn = {10957243},
   issue = {1 SPEC. ISS.},
   journal = {Journal of Multivariate Analysis},
   keywords = {CART,Censored data,Comparative genomic hybridization,Cross-validation,Density estimation,Loss function,Microarray,Model selection,Multivariate outcome,Prediction,Regression tree,Risk estimation,Survival analysis},
   pages = {154-177},
   title = {Tree-based multivariate regression and density estimation with right-censored data},
   volume = {90},
   year = {2004},
}
@article{Nelson1969,
   author = {Wayne Nelson},
   issue = {1},
   journal = {Journal of Quality Technology},
   pages = {27-52},
   title = {Hazard Plotting for Incomplete Failure Data},
   volume = {1},
   year = {1969},
}

@inbook{Polley2011,
    title={Super Learning for Right-Censored Data},
   author = {Eric C. Polley and Mark J. van der Laan},
   pages = {249-258},
   publisher = {Springer},
   booktitle = {Targeted Learning: Causal Inference for Observational Data},
   year = {2011},
   city = {New York},
}
%   editor = {Mark J. van der Laan and Sherri Rose},
%   journal = {Super Learning for Right-Censored Data},

@book{Robertson1988,
   author = {Tim Robertson and FT Wright and RL Dykstra},
   city = {New York},
   publisher = {John Wiley and Sons},
   title = {Order Restricted Statistical Inference},
   year = {1988},
}
@article{Simon2011,
   author = {Noah Simon and Jerome Friedman and Trevor Hastie and Robert Tibshirani},
   doi = {10.18637/jss.v039.i05.Regularization},
   isbn = {3902264330},
   issue = {5},
   journal = {Journal of Statistical Software},
   pages = {1-13},
   title = {Regularization Paths for {C}ox's Proportional Hazards Model via Coordinate Descent},
   volume = {39},
   year = {2011},
}
@phdthesis{Sonabend2021,
    school = {University College London},
   author = {Raphael Edward Benjamin Sonabend},
   title = {A Theoretical and Methodological Framework for Machine Learning in Survival Analysis},
   year = {2021},
}
@article{Tarkhan2020,
   author = {Aliasghar Tarkhan and Noah Simon},
   journal = {arXiv:2003.00116},
   title = {{Big}{Surv}{S}{G}{D}: Big Survival Data Analysis via Stochastic Gradient Descent},
   year = {2020},
}
@article{Tarkhan2022,
  title={An online framework for survival analysis: reframing Cox proportional hazards model for large data sets and neural networks},
  author={Tarkhan, Aliasghar and Simon, Noah},
  journal={Biostatistics},
  year={2022}
}
@article{Tibshirani1997,
   abstract = {I propose a new method for variable selection and shrinkage in Cox's proportional hazards model. My proposal minimizes the log partial likelihood subject to the sum of the absolute values of the parameters being bounded by a constant. Because of the nature of this constraint, it shrinks coefficients and produces some coefficients that are exactly zero. As a result it reduces the estimation variance while providing an interpretable final model. The method is a variation of the 'lasso' proposal of Tibshirani, designed for the linear regression context. Simulations indicate that the lasso can be more accurate than stepwise selection in this setting.},
   author = {Robert Tibshirani},
   doi = {10.1002/(SICI)1097-0258(19970228)16:4<385::AID-SIM380>3.0.CO;2-3},
   issn = {02776715},
   issue = {4},
   journal = {Statistics in Medicine},
   pages = {385-395},
   pmid = {9044528},
   title = {The lasso method for variable selection in the {C}ox model},
   volume = {16},
   year = {1997},
}
@book{VanderLaan2011,
   author = {Mark J. van der Laan and Sherri Rose},
   doi = {10.100/978-1-4419-9782-1},
   isbn = {978-1-4419-9781-4},
   publisher = {Springer},
   title = {Targeted Learning: Causal Inference for Observational Data},
   year = {2011},
   city = {New York},
}
@article{vanderlaan2007,
  title={Super learner},
  author={van der Laan, MJ and Polley, EC and Hubbard, AE},
  journal={Statistical Applications in Genetics and Molecular Biology},
  volume={6},
  number={1},
  pages={Online Article 25},
  year={2007}
}
@book{VanderLaan2003,
   author = {Mark J. van der Laan and James M. Robins},
   city = {New York},
   publisher = {Springer},
   title = {Unified Methods for Censored Longitudinal Data and Causality},
   year = {2003},
}
@inproceedings{Wang2021,
   abstract = {Being able to accurately predict the time to event of interest, commonly known as survival analysis, is extremely beneficial in many real-world applications. Traditional commonly used statistical survival analysis methods, e.g., Cox proportional hazards model and parametric censored regressions, are based on strong and sometimes impractical assumptions and can only handle linearity relationship between features and target. Recently, deep learning based formulations have been proposed for survival analysis to handle non-linearity. However, these existing deep learning methods either inherit strong assumptions from their corresponding base models or tailor discrete-time survival analysis. To overcome the limitations within these existing models in the literature, we propose an objective function to guide the training of a deep learning model for continuous-time survival analysis. The objective function combines both ranking based and point-wise regression based losses. The ranking based loss measures the goodness of the orders of the predicted survival time for all instances. The point-wise based loss measures the difference between the predicted survival time and the true survival time for the right censored time-to-event data. More specifically, we derive two versions of the ranking based loss from the smoothed concordance index, and two versions of point-wise based loss based on the normalized mean squared error (MSE) and mean absolute error (MAE). Thus, the proposed formulation is capable of dealing with the continuous-time survival analysis from both global and local perspectives. We conduct experimental analysis over several large-scale real-world time-to-event datasets, and the results demonstrate that our model outperforms the state-of-the-art survival analysis methods. The codes and data used in the experiments are available in the link 1.1https://github.com/yanlirock/local_global_survival},
   author = {Lu Wang and Yan Li and Mark Chignell},
   doi = {10.1109/ICDM51629.2021.00080},
   isbn = {9781665423984},
   issn = {15504786},
   journal = {Proceedings - IEEE International Conference on Data Mining, ICDM},
   keywords = {Censored Regression,Concordance Index,Deep Learning,Right Censoring,Survival Analysis},
   pages = {689-698},
   publisher = {Institute of Electrical and Electronics Engineers Inc.},
   title = {Combining Ranking and Point-wise Losses for Training Deep Survival Analysis Models},
   volume = {2021-December},
   year = {2021},
}
@article{Wei1992,
   author = {L. J. Wei},
   issue = {14-15},
   journal = {Statistics in Medicine},
   pages = {1871-1879},
   title = {The accelerated failure time model: A useful alternative to the {C}ox regression model in survival analysis.},
   volume = {11},
   year = {1992},
}
@article{Westling2021,
   author = {Ted Westling and Alex Luedtke and Peter B. Gilbert and Marco Carone},
   journal = {arXiv:2106.06602},
   title = {Inference for treatment-specific survival curves using machine learning},
   year = {2021},
}
@article{Yu2011,
   abstract = {An accurate model of patient survival time can help in the treatment and care of cancer patients. The common practice of providing survival time estimates based only on population averages for the site and stage of cancer ignores many important individual differences among patients. In this paper, we propose a local regression method for learning patient-specific survival time distribution based on patient attributes such as blood tests and clinical assessments. When tested on a cohort of more than 2000 cancer patients, our method gives survival time predictions that are much more accurate than popular survival analysis models such as the Cox and Aalen regression models. Our results also show that using patient-specific attributes can reduce the prediction error on survival time by as much as 20% when compared to using cancer site and stage only.},
   author = {Chun Nam Yu and Russell Greiner and Hsiu Chin Lin and Vickie Baracos},
   isbn = {9781618395993},
   journal = {Advances in Neural Information Processing Systems},
   pages = {1845-1853},
   title = {Learning patient-specific cancer survival distributions as a sequence of dependent regressors},
   volume = {24},
   year = {2011},
}
@article{Zeng2007,
   abstract = {The accelerated failure time model provides a natural formulation of the effects of covariates on potentially censored response variable. The existing semiparametric estimators are computationally intractable and statistically inefficient. In this article we propose an approximate nonparametric maximum likelihood method for the accelerated failure time model with possibly time-dependent covariates. We estimate the regression parameters by maximizing a kernel-smoothed profile likelihood function. The maximization can be achieved through conventional gradient-based search algorithms. The resulting estimators are consistent and asymptotically normal. The limiting covariance matrix attains the semiparametric efficiency bound and can be consistently estimated. We also provide a consistent estimator for the error distribution. Extensive simulation studies demonstrate that the asymptotic approximations are accurate in practical situations and the new estimators are considerably more efficient than the existing ones. Illustrations with clinical and epidemiologic studies are provided. © 2007 American Statistical Association.},
   author = {Donglin Zeng and D. Y. Lin},
   doi = {10.1198/016214507000001085},
   issn = {01621459},
   issue = {480},
   journal = {Journal of the American Statistical Association},
   keywords = {Censoring,Kernel smoothing,Linear regression,Profile likelihood,Semiparametric efficiency,Survival data},
   month = {12},
   pages = {1387-1396},
   title = {Efficient estimation for the accelerated failure time model},
   volume = {102},
   year = {2007},
}
@article{Zhou2022,
   abstract = {We propose a deep generative approach to nonparametric estimation of conditional survival and hazard functions with right-censored data. The key idea of the proposed method is to first learn a conditional generator for the joint conditional distribution of the observed time and censoring indicator given the covariates, and then construct the Kaplan-Meier and Nelson-Aalen estimators based on this conditional generator for the conditional hazard and survival functions. Our method combines ideas from the recently developed deep generative learning and classical nonparametric estimation in survival analysis. We analyze the convergence properties of the proposed method and establish the consistency of the generative nonparametric estimators of the conditional survival and hazard functions. Our numerical experiments validate the proposed method and demonstrate its superior performance in a range of simulated models. We also illustrate the applications of the proposed method in constructing prediction intervals for survival times with the PBC (Primary Biliary Cholangitis) and SUPPORT (Study to Understand Prognoses and Preferences for Outcomes and Risks of Treatments) datasets.},
   author = {Xingyu Zhou and Wen Su and Changyu Liu and Yuling Jiao and Xingqiu Zhao and Jian Huang},
   journal = {arXiv:2205.09633},
   month = {5},
   title = {Deep Generative Survival Analysis: Nonparametric Estimation of Conditional Survival Function},
   url = {http://arxiv.org/abs/2205.09633},
   year = {2022},
}



@article{Friedman2001,
   author = {Jerome H Friedman},
   issue = {5},
   journal = {The Annals of Statistics},
   pages = {1189-1232},
   title = {Greedy Function Approximation: A Gradient Boosting Machine},
   volume = {29},
   year = {2001},
}

@article{Westling2020,
   abstract = {In many problems, a sensible estimator of a possibly multivariate monotone function may fail to be monotone. We study the correction of such an estimator obtained via projection onto the space of functions monotone over a finite grid in the domain. We demonstrate that this corrected estimator has no worse supremal estimation error than the initial estimator, and that analogously corrected confidence bands contain the true function whenever the initial bands do, at no loss to band width. Additionally, we demonstrate that the corrected estimator is asymptotically equivalent to the initial estimator if the initial estimator satisfies a stochastic equicontinuity condition and the true function is Lipschitz and strictly monotone. We provide simple sufficient conditions in the special case that the initial estimator is asymptotically linear, and illustrate the use of these results for estimation of a G-computed distribution function. Our stochastic equicontinuity condition is weaker than standard uniform stochastic equicontinuity, which has been required for alternative correction procedures. This allows us to apply our results to the bivariate correction of the local linear estimator of a conditional distribution function known to be monotone in its conditioning argument. Our experiments suggest that the projection step can yield significant practical improvements.},
   author = {Ted Westling and Mark J. van der Laan and Marco Carone},
   doi = {10.1214/20-EJS1740},
   issn = {19357524},
   issue = {2},
   journal = {Electronic Journal of Statistics},
   keywords = {Asymptotic linearity,Confidence band,Kernel smoothing,Projection,Shape constraint,Stochastic equicontinuity},
   pages = {3032-3069},
   publisher = {Institute of Mathematical Statistics},
   title = {Correcting an estimator of a multivariate monotone function with isotonic regression},
   volume = {14},
   year = {2020},
}

@article{Lin1994,
  title = {Confidence bands for survival curves under the proportional hazards model},
  author = {Lin, DY and Fleming, TR and Wei, LJ},
  journal = {Biometrika},
  pages = {73-81},
  year = {1994},
  volume = {81},
  issue = {1},
  publisher={JSTOR}
}

@article{Lin2013,
   author = {Yuanyuan Lin and Kani Chen},
   doi = {10.1093/biomet/ass073},
   issn = {00063444},
   issue = {2},
   journal = {Biometrika},
   month = {6},
   pages = {525-530},
   title = {Efficient estimation of the censored linear regression model},
   volume = {100},
   year = {2013},
}

@article{Qian2014,
   abstract = {Clinical studies using complex sampling often involve both truncation and censoring, where there are options for the assumptions of independence of censoring and event and for the relationship between censoring and truncation. In this paper, we clarify these choices, show certain equivalences, and provide examples. © 2013 Elsevier B.V.},
   author = {Jing Qian and Rebecca A. Betensky},
   doi = {10.1016/j.spl.2013.12.016},
   issn = {01677152},
   issue = {1},
   journal = {Statistics and Probability Letters},
   keywords = {Censoring,Cross-sectional sampling,Quasi-independence,Truncation},
   month = {4},
   pages = {12-17},
   title = {Assumptions regarding right censoring in the presence of left truncation},
   volume = {87},
   year = {2014},
}
