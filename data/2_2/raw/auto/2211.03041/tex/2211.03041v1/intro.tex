\section{Introduction}


Accurate estimates of posterior probabilities are crucial for neural networks in various Natural Language Processing (NLP) tasks~\cite{icml17,DBLP:conf/nips/Lakshminarayanan17}. For example, it would be helpful for humans if the models deployed in practice abstain or interact when they cannot make a decision with high confidence~\cite{DBLP:journals/jamia/JiangOKO12}. While Pre-trained Language Models (PLMs) have improved the performance of many NLP tasks~\cite{bert,roberta}, how to better avoid miscalibration is still an open research problem ~\cite{calibration_emnlp20,dan_roth_emnlp21}. 
\begin{table}[t!]
    \centering
    \begin{tabular}{l|p{0.65\columnwidth}}
    \hline

    %  Example 1: & It is \hlc[cyan!10]{a} \hlc[red!40]{warm} \hlc[red!60]{funny} \hlc[red!40]{engaging} \hlc[cyan!20]{film} . \\ \hline
     Positive & a fast \hlc[green!10]{funny} \hlc[green!40]{highly} \hlc[green!80]{enjoyable} movie.\\ \hline
    %  like a south of the border melrose place
     
     Negative & It's about \hlc[red!5]{following} your \hlc[green!10]{dreams} \hlc[red!10]{no} matter \hlc[red!5]{what} your \hlc[green!5]{parents} think.\\
    \hline
  \end{tabular}
    \caption{Two motivating examples with highlight explanations~\cite{SST}. The saturation of the colors signifies the magnitude. The confidence of the model should be easily recognized by looking at token attributions.}
    % \vspace{-4mm}
    \label{tab:example-m}
\end{table}
In this paper, we investigate if and how model explanations can help calibrate the model. 

Explanation methods have attracted considerable research interest in recent years for revealing the internal reasoning processes behind models~\cite{IG,Uncertainty_Aware_Attention,deeplift}. Token attribution scores generated by explanation methods represent the contribution to the prediction~\cite{diagnostic}. Intuitively, one can draw some insight for analyzing and debugging neural models from these scores if they are correctly attributed, as shown in Table~\ref{tab:example-m}. For example, when the model identifies a highly indicative pattern, the tokens involved would have high attribution scores for the predicted label and low attribution scores for other labels. Similarly, if the model has difficulty recognizing the inductive information of any class (i.e., the attribution scores are not high for any label), the model should not be highly confident. As such, the computed explanation of an instance could indicate the confidence of the model in its prediction to some extent.
 
Inspired by this, we propose a simple and effective method named \textbf{CME} that can be applied at training time and improve the performance of the confidence estimates. The estimated confidence measures how confident the model is for a specific example. Ideally, reasonable confidence estimates should have higher confidence for correctly classified examples resulting in higher attributions than incorrect ones. Hence, given an example pair during training with an inverse classification relationship, we regularize the classifier by comparing the wrong example's attribution magnitude and the correct example's attribution magnitude.

Our work is related to recent works on incorporating explanations into learning. Different from previous studies that leverage explanations to help users predict model decisions~\cite{DBLP:journals/corr/abs-2102-02201} or improve the accuracy~\cite{DBLP:conf/icml/RiegerSMY20}, we focus on answering the following question: \textit{are these explanations of black-box models useful for calibration?} If so, how should we exploit the predictive power of these explanations? Considering the model may be uninterpretable due to the nature of neural networks and limitations of explanation method~\cite{Fragile,DBLP:conf/nips/YehHSIR19}, a calibrated model by CME at least can output the unbiased confidence. Moreover, we exploit intrinsic explanation during training, which does not require designing heuristics~\cite{xiye1} and additional data augmentation~\cite{mixup21acl}.
% Are these explanations useful for calibrating the model?

We conduct extensive experiments using BERT~\cite{bert} and RoBERTa~\cite{roberta} to show the efficacy of our approach on three natural language understanding tasks (i.e., natural language inference, paraphrase detection, and commonsense reasoning) under In-Domain (ID) and Out-of-Domain (OD) settings. CME achieves the lowest expected calibration error without accuracy drops compared with strong SOTA methods, e.g.,~\citet{mixup21acl}. When combined with Temperature Scaling (TS)~\cite{icml17}, the expected calibration errors are further reduced as better calibrated posterior estimates under these two settings.

