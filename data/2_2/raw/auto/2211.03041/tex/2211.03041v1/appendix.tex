\appendix
\label{sec:supplemental}
\renewcommand{\arraystretch}{1.2}

\begin{table}[!t]
    \centering
    \small
    \resizebox{\linewidth}{!}{
    \begin{tabular}{l|c|c}
    \hline
        \textbf{Dataset} & \textbf{Class} & \textbf{Train/Dev/Test}\\ \hline
        SNLI~\cite{bowman-etal-2015-large} & 3 & 549367 / 4921 / 4921  \\
        MNLI~\cite{williams-etal-2018-broad} & 3 & 391176 / 4772 / 4907 \\ \hline
        QQP~\cite{iyer-2017-quora} & 2 & 363178 / 20207 / 20215 \\
        TPPDB~\cite{lan-etal-2017-continuously} & 2 & 42200 / 4685 / 4649  \\ \hline
        SWAG~\cite{zellers-etal-2018-swag} & 4 & 73546 / 10003 / 10003 \\
        HellaSWAG~\cite{zellers-etal-2019-hellaswag} & 4 & 39905 / 5021 / 5021  \\ \hline
    \end{tabular}}
    \caption{Dataset statistics with number of classes and pre-processed train/development/test splits.}
    \label{tab:data_characteristic}
\end{table}
\setlength\tabcolsep{2pt}
\begin{table}[t!]
    \centering
    \small
    \begin{tabular}{c|c|c|c|c} \hline
    \textbf{Dataset} &            \textbf{Statistics}           &   \textbf{Train} &    \textbf{Dev} &   \textbf{Test} \\ \hline 
    \multirow{4}{*}{SNLI} &  Avg. Seq. Length &      27.27 &     28.62 &     28.54 \\
         &       Num. of class-0 &    183416 &    1680 &    1649 \\
         &       Num. of class-1 &    183187 &    1627 &    1651 \\ 
         &       Num. of class-2 &    182764 &    1614 &    1621 \\
         \hline
       \multirow{4}{*}{MNLI} &  Avg. Seq. Length &     40.50 &    39.65 &    40.01 \\
         &       Num. of class-0 &    130416 &   1736  &   1695  \\
         &       Num. of class-1 &    130381 &    1535 &    1631 \\ 
         &       Num. of class-2 &    130379 &    1501 &    1581 \\ \hline
       \multirow{3}{*}{QQP} &  Avg. Seq. Length &     31.00 &    30.92 &    31.06 \\
        &       Num. of class-0 &  229037 &  12772 &  12768 \\
        &       Num. of class-1 &  134141 &  7435 &  7447 \\\hline
 \multirow{3}{*}{TPPDB} & Avg. Seq. Length &      38.65 &     40.76 &     40.51 \\
  &       Num. of class-0 &    31033 &    3744 &    3769 \\
  &       Num. of class-1 &    11167 &    941 &   880 \\ \hline
 \multirow{5}{*}{SWAG} &  Avg. Seq. Length &   124.65    &   127.84   &  128.35   \\
  &       Num. of class-0 &   18414  &   2453  &  2480  \\
  &       Num. of class-1 &   18334  &  2500  & 2529   \\
  &       Num. of class-2 &   18340 & 2546  & 2492 \\ 
  &       Num. of class-3 &  18458  & 2504  &  2502 \\
  \hline
  \multirow{5}{*}{HellaSWAG} &  Avg. Seq. Length &  338.84    &   347.64   &  347.64   \\
   &       Num. of class-0 &  9986  &  1244  &  1271  \\
   &       Num. of class-1 &  10031  & 1257   &  1228  \\
   &       Num. of class-2 & 9867  &  1295 & 1289 \\ 
   &       Num. of class-3 & 10021   & 1225  &  1233 \\
   \hline
\end{tabular}
    \caption{Average sequence lengths after tokenization and label distributions of datasets.}
    \label{tab:extednded_data_characteristics}
\end{table}




\section{Dataset Statistics}
\label{sec:dataset:sta}
Table~\ref{tab:data_characteristic} and Table~\ref{tab:extednded_data_characteristics} present the characteristics of all datasets. The information across the three data splits includes the average sequence length and the number of examples under each label. Then we briefly introduce the datasets:

\paragraph{Natural Language Inference}
The in-domain dataset is the Stanford Natural Language Inference (SNLI) dataset~\cite{bowman-etal-2015-large}. It is used to predict if the relationship between the hypothesis and the premise (i.e., \textit{neutral}, \textit{entailment} and \textit{contradiction,} ) for natural language inference task. The out-of-domain dataset is the Multi-Genre Natural Language Inference (MNLI)~\cite{williams-etal-2018-broad}, which covers more diverse domains compared with SNLI. 

\paragraph{Paraphrase Detection} 
The in-domain dataset is the Quora Question Pairs (QQP) dataset~\cite{iyer-2017-quora}. It is proposed to test if two questions are semantically equivalent as a paraphrase detection task. The out-of-domain dataset is the Twitter news URL Paraphrase
Database (TPPDB) dataset~\cite{lan-etal-2017-continuously}. It is used to determine whether Twitter sentence pairs have similar semantics when they share URL and we set the label less than 3 as the first class, and the others as the second class following previous works.

\paragraph{Commonsense Reasoning} 
The in-domain dataset is the Situations With Adversarial Generations (SWAG) dataset~\cite{zellers-etal-2018-swag}. It is a popular benchmark for commonsense reasoning task where the objective is to pick the most logical continuation of a statement from a list of four options. 
The out-of-domain dataset is the HellaSWAG dataset~\cite{zellers-etal-2019-hellaswag}. It is generated by adversarial filtering and is more challenging for out-of-domain generalization.


\section{Experimental Settings}
\label{sec:exp_setting}
For all experiments, we report the average performance results of five random seed initializations for a maximum of 3 epochs. For a fair comparison, we follow most of the hyperparameters of~\citet{calibration_emnlp20} unless reported below.
For BERT, the batch size is 32 (SNLI/QQP) or 8 (SWAG) and the learning rate is $1\times10^{-5}$, the  weight of gradient clip is 1.0, and we exclude weight decay mechanism. 
For RoBERTa, the batch size is 16 (SNLI/QQP) or 8 (SWAG) and the learning rate is $2\times10^{-5}$, the weight of gradient clip is 1.0, and the weight decay is 0.1. The maximum sequence length is set to 256. The optimal weights of $\lambda$ in Eqn.~\ref{loss_function} are 0.05 and 1.0 for BERT and RoBERTa, respectively. We search the weight with respect to ECEs on the development sets from $[0.05, 0.1, 0.5, 1.0]$. The hyperparameter of label smoothing $\sigma$ is 0.1. All experiments are conducted on NVIDIA Tesla A100 40G GPUs. We perform the temperature scaling searches in the range of [0.01,10.0] with a granularity of 0.01 using development sets.  The search processes are fast as we use cached predicted logits of each dataset. The training time is moderate compared to the baseline. For example, on a A100 GPU, training the model with BERT and RoBERTa takes around 3-4 hours for QQP dataset. For the experiments in this paper, we use $K=10$.

\begin{figure}[!]
\centering
\resizebox{\linewidth}{!}{
% 
\includegraphics[scale=1.0]{misc/ablation-CME.pdf}
}
% where an .eps filename suffix will be assumed under latex,
% and a .pdf suffix will be assumed for pdflatex; or what has been declared
% via \DeclareGraphicsExtensions.
% \vspace{-2mm}
\caption{Reliability diagram of ablation study. We train each BERT-based variant, and adopt QQP as the ID dataset which is relatively unbalanced in labels. }
% \vspace{-2mm}
\label{fig:ablation}
\end{figure}
\section{Ablation Study}
As shown in Figure~\ref{fig:ablation}, we find that using only confidence in Eqn.~\ref{eqn:atten3} generally yields higher ECE than other variants. Also using attention instead of scaled attention brings an increase in errors.

\section{Case Study}
\label{sec:case_study}
As shown in the Table~\ref{tab:example}, we list randomly-selected examples of BERT-base models with MLE and CME. If models correctly predict the true label, the model confidence should be greater than 50\%. For example, in the second case of out-of-domain SNLI dataset, the model confidence of true label falls slightly below the borderline probability which results in an incorrect prediction (Probabilities: 30.54\%, 30.83\%, 38.63\% vs. 67.87\%, 14.66\%, 17.47\%). In contrast, CME leverages model explanation during training that helps calibrate the model confidence and predicts correctly.

\section{Standard Deviations}
\label{sec:sd}
Table~\ref{tb:ece_result_std} lists the standard deviations of each methods. We report the results across five runs with random seeds.
% \section{Related Work}



\begin{table*}[ht!]
\centering
%\begin{tabularx}{\linewidth}{@{} p{8.2cm} @{\hskip 0.2cm} p{3.8cm} @{\hskip 0.2cm}c @{\hskip 0.2cm} c@{\hskip 0.2cm}c@{}}
%\toprule
\small
\begin{tabularx}{1.0\textwidth}{@{} P{0.5cm}@{\hskip 0.2cm} p{11.0cm}@{\hskip 0.2cm} P{1.7cm}@{\hskip 0.2cm} P{0.8cm}@{\hskip 0.2cm} P{1.2cm}@{}}
\toprule
\textbf{Data} &\multicolumn{1}{c}{\textbf{Input}}&
\textbf{True Label} &
\textbf{MLE} & \textbf{CME}\\
\midrule
\parbox[t]{2mm}{\multirow{7}{*}{\rotatebox[origin=c]{90}{{\bf{SNLI}}}}}&
\multirow{4}{*}{\parbox{11cm}{\textbf{Premise:} The shadow silhouette of a woman standing near the water looking at a large attraction on the other side.
\newline
\textbf{Hypothesis:} She is in the water.}}
&contradiction	& 21.91 & 81.64 \\
% &&&\multicolumn{2}{c}{\scriptsize{wrong} $\rightarrow$ correct}\\
& & Test\#1544 & \textcolor{red}{wrong} & \textcolor{green}{correct} \\
\\ \\
\cmidrule{2-5}
&\multirow{3}{*}{\parbox{11cm}{\textbf{Premise:} A child is jumping off a platform into a pool. 
\newline
\textbf{Hypothesis:} The child is swimming.}}
&entailment & 30.54 & 67.87\\
% &&&\multicolumn{2}{c}{\scriptsize\textcolor{blue}{INCOR} $\rightarrow$ \textcolor{blue}{COR}}\\
& & Test\#1841 & \textcolor{red}{wrong} & \textcolor{green}{correct} \\
\\

\end{tabularx}
\hspace{\fill}
\begin{tabularx}{1.0\textwidth}{@{} P{0.5cm}@{\hskip 0.2cm} p{11.0cm}@{\hskip 0.2cm} P{1.7cm}@{\hskip 0.2cm} P{0.8cm}@{\hskip 0.2cm} P{1.2cm}@{}}
\toprule
\parbox[t]{2mm}{\multirow{7}{*}{\rotatebox[origin=c]{90}{{\bf{MNLI}}}}}&
\multirow{4}{*}{\parbox{11cm}{\textbf{Premise:} There are no means of destroying it; and he dare not keep it.
\newline
\textbf{Hypothesis:} He should keep it with him.}}
&contradiction	& 18.33 & 80.78 \\
% &&&\multicolumn{2}{c}{\scriptsize{wrong} $\rightarrow$ correct}\\
& & Test\#560 & \textcolor{red}{wrong} & \textcolor{green}{correct} \\
\\ \\
\cmidrule{2-5}
&\multirow{3}{*}{\parbox{11cm}{\textbf{Premise:} yeah that's that's a big step yeah
\newline
\textbf{Hypothesis:} Yes, you have to be committed to make that big step.}}
&neutral & 49.43 & 69.18\\
% &&&\multicolumn{2}{c}{\scriptsize\textcolor{blue}{INCOR} $\rightarrow$ \textcolor{blue}{COR}}\\
& & Test\#1016 & \textcolor{red}{wrong} & \textcolor{green}{correct} \\
\\\bottomrule
\end{tabularx}
\hspace{\fill}

\begin{tabularx}{1.0\textwidth}{@{} P{0.5cm}@{\hskip 0.2cm} p{11.0cm}@{\hskip 0.2cm} P{1.7cm}@{\hskip 0.2cm} P{0.8cm}@{\hskip 0.2cm} P{1.2cm}@{}}
\parbox[t]{2mm}{\multirow{7}{*}{\rotatebox[origin=c]{90}{{\bf{SWAG}}}}}&
\multirow{4}{*}{\parbox{11cm}{\textbf{Prompt:} Among them, someone embraces someone and someone. Someone
\newline
\textbf{Options:} (A). is brought back to the trunk beside him. (B). waits for someone someone and the clerk at the dance wall. (C). scoops up someone and hugs someone. (D). looks at her, utterly miserable.}}
& C	& 47.29 & 59.84 \\
% &&&\multicolumn{2}{c}{\scriptsize{wrong} $\rightarrow$ correct}\\
& & Test\#940 & \textcolor{red}{wrong} & \textcolor{green}{correct} \\
\\ \\
\cmidrule{2-5}
\\
&\multirow{3}{*}{\parbox{11cm}{\textbf{Prompt:} He is holding a violin and string in his hands. He
\newline 
\textbf{Options:} (A). is playing an accordian on the stage. (B). talks about how to play it and a harmonica. (C). picks up a small curling tool and begins talking. (D). continues to play the guitar too.}}
& B & 32.32 & 45.00\\
% &&&\multicolumn{2}{c}{\scriptsize\textcolor{blue}{INCOR} $\rightarrow$ \textcolor{blue}{COR}}\\
& & Test\#30 & \textcolor{green}{correct} & \textcolor{red}{wrong} \\
\\\\\bottomrule
\end{tabularx}

\caption{\label{tab:example} Examples with model confidence of true label by \textbf{MLE} and \textbf{CME} in the SNLI/MNLI and SWAG dataset. The labels of SNLI/MNLI are entailment, contradiction and neutral. We list most cases with probabilities of true label that CME predicts correctly and the predictions of MLE are wrong.
} 
\end{table*}

\begin{table*}[t]
% \resizebox{\textwidth}{!}{%
\small
\centering

\begin{tabular}{l|c|c|c|c|c|c|c|c|c|c|c|c}
\hline
                       \multirow{3}{*}{\textbf{Methods}}  & \multicolumn{6}{c|}{In-Domain}                                                                                                                                            & \multicolumn{6}{c}{Out-of-Domain}                                                                                                                                        \\ \cline{2-13}
                      & \multicolumn{2}{c|}{SNLI}                               & \multicolumn{2}{c|}{QQP}                                & \multicolumn{2}{c|}{SWAG}                               & \multicolumn{2}{c|}{MNLI}                               & \multicolumn{2}{c|}{TPPDB}                        & \multicolumn{2}{c}{HellaSWAG}                          \\ \cline{2-13}
                      & \multicolumn{1}{c|}{{OOTB}} & \multicolumn{1}{c|}{{TS}} & \multicolumn{1}{c|}{{OOTB}} & \multicolumn{1}{c|}{{TS}} & \multicolumn{1}{c|}{{OOTB}} & \multicolumn{1}{c|}{{TS}} & \multicolumn{1}{c|}{{OOTB}} & \multicolumn{1}{c|}{{TS}} & \multicolumn{1}{c|}{{OOTB}} & \multicolumn{1}{c|}{{TS}} & \multicolumn{1}{c|}{{OOTB}} & \multicolumn{1}{c}{{TS}} \\ \hline
BERT                  & $0.8$                    & $1.0$ & $0.5$          & $0.1$     & $1.8$       & $0.4$          & $2.1$          & $1.7$        & $0.6$           & $0.9$           & $2.8$      & $2.1$        \\
BERT+LS              & $0.3$ & $0.5$         & $0.4$          & $0.7$         & $1.0$         & $1.1$        & $1.4$         & $0.9$          & $0.8$         & $0.7$          & $0.6$ & $0.9$         \\ 
Manifold-mixup       & $0.8$        & $0.3$          & $1.2$          & $1.1$          & $0.6$          & $0.4$          & $2.6$         &  $1.9$           & $2.3$         &  $2.6$         & $1.2$          & $0.9$              \\
Manifold-mixup+LS & $0.4$                     & $0.7$          &  $0.2$  &  $0.7$  &  $0.5$  &  $0.2$  & $1.3$  &  $0.9$  & $1.1$    &  $1.7$   &  $0.7$           &  $0.6$  \\ 
 \citet{mixup21acl} & {${0.4}$}      &{${0.7}$}         & {$0.6$} & {$0.6$} & {${0.4}$} & {$0.2$} & {$2.5$} & {$0.6$} & ${0.7}$  & ${1.2}$  & ${1.9}$         & ${1.5}$  \\
\citet{mixup21acl}+LS & {$0.3$}                     &{$1.0$ }         & $0.9$ & $0.1$ & $0.7$ & ${0.3}$ & ${{1.0}}$ & ${0.5}$ & ${1.0}$  & {$1.1$}  & ${{0.8}}$         & ${0.7}$ \\ \hline
 CME (Ours) & {${0.3}$}      &$0.2$         & {${0.5}$} & {${0.1}$} & {${0.2}$} & {${0.2}$} & {$0.3$} & {${0.6}$} & ${0.8}$  & ${0.2}$  & ${1.8}$         & ${{0.4}}$ \\
CME+LS (Ours) & {$0.3$}                     &$0.2$         & ${0.1}$ & ${0.3}$ & {$1.5$} & ${0.2}$ & ${0.7}$ & ${1.0}$ & ${{1.5}}$  & {$0.6$}  & ${1.8}$         & ${0.7}$ \\ \hline
%\toprule


\hline
                       \multirow{3}{*}{\textbf{Methods}} & \multicolumn{6}{c|}{In-Domain}                                                                                                                                            & \multicolumn{6}{c}{Out-of-Domain}                                                                                                                                        \\ \cline{2-13}
                      & \multicolumn{2}{c|}{SNLI}                               & \multicolumn{2}{c|}{QQP}                                & \multicolumn{2}{c|}{SWAG}                               & \multicolumn{2}{c|}{MNLI}                               & \multicolumn{2}{c|}{TPPDB}                        & \multicolumn{2}{c}{HellaSWAG}                          \\ \cline{2-13}
                    & \multicolumn{1}{c|}{{OOTB}} & \multicolumn{1}{c|}{{TS}} & \multicolumn{1}{c|}{{OOTB}} & \multicolumn{1}{c|}{{TS}} & \multicolumn{1}{c|}{{OOTB}} & \multicolumn{1}{c|}{{TS}} & \multicolumn{1}{c|}{{OOTB}} & \multicolumn{1}{c|}{{TS}} & \multicolumn{1}{c|}{{OOTB}} & \multicolumn{1}{c|}{{TS}} & \multicolumn{1}{c|}{{OOTB}} & \multicolumn{1}{c}{{TS}} \\ \hline
RoBERTa               & $0.5$                      & $0.8$              & $0.1$                      & $0.6$                       & $1.0$                      & ${0.7}$                       & $3.2$                      & {$2.5$}                       & $0.6$                      & $0.5$                       & $3.2$                     & $2.9$                      \\
RoBERTa+LS           & $0.6$                      & $1.0$                       & $0.3$                      & $0.6$                       & $0.3$                      & $0.6$                      & $1.4$                      & $1.9$                       & $0.3$                      & $0.7$                       & $1.4$                      & $1.1$                       \\ 
Manifold-mixup       & $0.8$         & $0.4$          & $0.5$          & $0.6$          & $1.2$          & $0.3$          & $3.1$         & $1.3$           & $1.8$          &  $2.1$         & $2.8$          & $1.5$          \\
Manifold-mixup+LS & $1.0$                     & $0.9$          &  $0.7$  &  $0.6$  &  $1.5$  &  $0.4$  &  
$1.6$  &  $1.0$  &  $0.9$   &  $1.1$   &  $0.6$           &  $1.6$  \\ 
\citet{mixup21acl} & {$0.7$}                     & ${0.5}$          & {$0.6$}  &{$0.2$}  & $0.1$   & $0.2$  &{$1.9$}  & ${1.4}$ &${0.9}$ & ${1.2}$  & $1.8$        & $1.5$ \\ 
\citet{mixup21acl}+LS & ${0.6}$                     & $0.6$          & {${0.7}$}  &{${0.4}$}  &{${0.4}$}  &$0.1$  &${1.7}$  & {$1.3$} &${1.6}$ & ${1.8}$  & ${0.9}$         & ${1.2}$
 \\ \hline
CME (Ours) & {$0.6$}                     & ${0.2}$          & {$0.5$}  &{$0.1$}  & $1.0$   & $0.2$  &{$0.8$}  & ${0.3}$ &${0.5}$ & ${0.8}$  & $1.8$        & $0.6$ \\ 
CME+LS (Ours) & ${0.4}$                     & $0.3$          & {${0.4}$}  &{${0.2}$}  &{${0.9}$}  &$0.2$  &${0.6}$  & {$1.0$} &${0.6}$ & ${0.4}$  & ${1.3}$         & ${1.6}$
 \\ \hline
\end{tabular}
% }
\caption{{The standard deviations of BERT-based and RoBERTa-based models.}} 
\vspace{-4mm}
\label{tb:ece_result_std}
\end{table*}



