\section{Related Work}

Our work is primarily motivated by previous analyses of posterior calibration on modern neural networks. Guo et al. (2017) pointed out that in some cases, as the classification performance of neural networks improves, its posterior output becomes poorly calibrated. There are a few attempts to investigate the effect of posterior calibration on natural language processing (NLP) tasks: Nguyen and O’Connor (2015) empirically tested how classifiers on NLP tasks (e.g., sequence tagging) are calibrated. For instance, compared to the Naive Bayes classifier, logistic regression outputs wellcalibrated posteriors in sentiment classification task. Card and Smith (2018) also mentioned the importance of calibration when generating a training corpus for NLP tasks.

As noted above, numerous post-processing calibration techniques have been developed: traditional binning methods (Zadrozny and Elkan, 2001, 2002) set up bins based on the predicted posterior ˆ p, recalculate calibrated posteriors ˆ q per each bin on a validation set, and then update every ˆ p with ˆ q if ˆ p falls into the certain bin. On the other hand, scaling methods (Platt et al., 1999; Guo et al., 2017; Kull et al., 2019) re-scale the predicted posterior ˆ p from the softmax layer trained on a validation set. Recently, Kumar et al. (2019) pointed out that such re-scaling methods do not actually produce well-calibrated probabilities as reported since the true posterior probability distribution can not be captured with the often low number of samples in the validation set2 . To address the issue, the authors proposed a scaling-binning calibrator, but still rely on the validation set.