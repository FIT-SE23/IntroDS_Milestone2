\section{Related Work}
As accurate estimates are required for many difficult or sensitive prediction tasks~\cite{Platt99probabilisticoutputs}, probability calibration is an important uncertainty estimation task for NLP. Unlike other uncertainty estimation task (e.g., out-of-domain detection, selective inference), calibration focuses on aleatoric uncertainty measured by the probability of the prediction and adjusts the overall model confidence level~\cite{DBLP:conf/iclr/HendrycksG17,LB1,icml17,DBLP:conf/nips/QinWBC21}. For example,~\citet{DBLP:conf/icml/GalG16} propose to adopt multiple predictions with different dropout masks and then combine them to get the confidence estimate. Recently, several works focus on the calibration of PLMs models for NLP tasks~\cite{pt_improve_uncertainy,calibration_emnlp20,poscal,DBLP:conf/eacl/HeMXH21,mixup21acl,DBLP:conf/acl/BoseAIF22}.~\citet{dan_roth_emnlp21} investigate the calibration properties of different transformer architectures and sizes of BERT. In line with recent work~\cite{xiye1}, our work focuses on how explanations can help calibration in three NLP tasks. However, we do not need to learn a calibrator by using model interpretations with heuristics, and also do not compare due to its intensive computation cost when generating attributions. In contrast, we explore whether model explanations are useful for calibrating black-box models during training.

\section{Conclusion}


We propose a method that leverages model attributions to address calibration estimates of PLMs-based models. Considering model attributions as facts about model behaviors, we show that CME achieves the lowest ECEs under most settings for two popular PLMs. 


\section{Limitations}
Calibrated confidence is essential in many high-stakes applications where incorrect predictions are highly problematic (e.g., self-driving cars, medical diagnoses). Though improving the performance on the calibration of pre-trained language models and achieving the comparable task performance, our explanation-based calibration method is still limited by the reliability and fidelity of interpretable methods. We adopt the scaled attention weight as the calculation method of attributions because (i) it has been shown to be more faithful in previous work~\cite{previous_work}, and (ii) the interpretation of the model is that the internal parameters of the model participate in the calculation and are derivable. Despite the above limitations, it does not undermine the main contribution of this paper, as involving explanations when training helps calibrate black-box models. Our approach can apply to most NLP models, incurs no additional overhead when testing, and is modularly pluggable. Another promising research direction is to explore using free-text explanations to help calibrate the model.


\section*{Acknowledgements}
We thank the anonymous reviewers for their insightful comments and suggestions. This work is jointly supported by grants: National Key R\&D Program of China (No. 2021ZD0113301), Natural Science Foundation of China (No. 62006061).