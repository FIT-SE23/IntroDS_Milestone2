\section{Experiment}
% 1. In-box / out-of-the-box / posthoc
% 2. Ablation study
% 3. Analysis by visualization or decision boundary case study
% 4. Dataset statistic
% 5. Implement Details
% 6. Task performance
% 7. Limitations

\begin{table*}[t]
\resizebox{\textwidth}{!}{%
\centering

\begin{tabular}{l|c|c|c|c|c|c|c|c|c|c|c|c}
\hline
                       \multirow{3}{*}{\textbf{Methods}}  & \multicolumn{6}{c|}{In-Domain}                                                                                                                                            & \multicolumn{6}{c}{Out-of-Domain}                                                                                                                                        \\ \cline{2-13}
                      & \multicolumn{2}{c|}{SNLI}                               & \multicolumn{2}{c|}{QQP}                                & \multicolumn{2}{c|}{SWAG}                               & \multicolumn{2}{c|}{MNLI}                               & \multicolumn{2}{c|}{TPPDB}                        & \multicolumn{2}{c}{HellaSWAG}                          \\ \cline{2-13}
                      & \multicolumn{1}{c|}{{OOTB}} & \multicolumn{1}{c|}{{TS}} & \multicolumn{1}{c|}{{OOTB}} & \multicolumn{1}{c|}{{TS}} & \multicolumn{1}{c|}{{OOTB}} & \multicolumn{1}{c|}{{TS}} & \multicolumn{1}{c|}{{OOTB}} & \multicolumn{1}{c|}{{TS}} & \multicolumn{1}{c|}{{OOTB}} & \multicolumn{1}{c|}{{TS}} & \multicolumn{1}{c|}{{OOTB}} & \multicolumn{1}{c}{{TS}} \\ \hline
BERT                  & $2.54$                    & $1.14$ & $2.71$          & $0.97$     & $2.49$       & $0.85$          & $7.09$          & $3.61 $        & $8.51$           & $7.15$           & $12.62$      & $12.83 $        \\
BERT+LS              & $7.12$ & $8.37$         & $6.33$          & $8.16$         & $10.01$         & $10.89$        & $3.74$         & $4.05$          & $6.30$         & $5.78$          & $5.73$ & $5.34$         \\ 
Manifold-mixup       & $3.17$        & $1.77$          & $8.55$          & $6.11$          & $5.18$          & $1.09$          & $12.92$         &  $2.34$           & $12.10$         &  $7.98$         & $9.82$          & $5.12$              \\
Manifold-mixup+LS & $3.40$                     & $5.14$          &  $3.49$  &  $3.71$  &  $5.24$  &  $1.26$  & $16.76$  &  $4.57$  & $6.29$    &  $6.54$   &  $8.32$           &  $3.64$  \\ 
 \citet{mixup21acl} & {${1.29}$}      &{${0.77}$}         & {$2.05$} & {$1.02$} & {${2.01}$} & {$0.81$} & {$2.73$} & {$3.50$} & ${5.69}$  & ${3.16}$  & ${5.49}$         & ${4.11}$  \\
\citet{mixup21acl}+LS & {$1.85$}                     &{$1.05$ }         & $1.70$ & $0.95$ & $2.09$ & ${0.79}$ & ${\bf{2.26}}$ & ${1.70}$ & ${5.37}$  & {$3.54$}  & ${\bf{4.26}}$         & ${3.28}$ \\ \hline
 CME (Ours) & {$\bf{1.11}$}      &{$\bf{0.64}$}         & {$\bf{1.66}$} & {$\bf{0.70}$} & {$\bf{1.16}$} & {$\bf{0.69}$} & {$2.65$} & {$\bf{1.59}$} & ${7.77}$  & $\bf{1.59}$  & ${11.64}$         & ${\bf{2.11}}$ \\
CME+LS (Ours) & {$6.92$}                     &{$2.16$ }         & ${6.53}$ & ${2.73}$ & {$8.83$} & ${0.71}$ & ${4.32}$ & ${3.34}$ & ${\bf{4.21}}$  & {$3.83$}  & ${6.40}$         & ${2.91}$ \\ \hline

\hline
                       \multirow{3}{*}{\textbf{Methods}} & \multicolumn{6}{c|}{In-Domain}                                                                                                                                            & \multicolumn{6}{c}{Out-of-Domain}                                                                                                                                        \\ \cline{2-13}
                      & \multicolumn{2}{c|}{SNLI}                               & \multicolumn{2}{c|}{QQP}                                & \multicolumn{2}{c|}{SWAG}                               & \multicolumn{2}{c|}{MNLI}                               & \multicolumn{2}{c|}{TPPDB}                        & \multicolumn{2}{c}{HellaSWAG}                          \\ \cline{2-13}
                    & \multicolumn{1}{c|}{{OOTB}} & \multicolumn{1}{c|}{{TS}} & \multicolumn{1}{c|}{{OOTB}} & \multicolumn{1}{c|}{{TS}} & \multicolumn{1}{c|}{{OOTB}} & \multicolumn{1}{c|}{{TS}} & \multicolumn{1}{c|}{{OOTB}} & \multicolumn{1}{c|}{{TS}} & \multicolumn{1}{c|}{{OOTB}} & \multicolumn{1}{c|}{{TS}} & \multicolumn{1}{c|}{{OOTB}} & \multicolumn{1}{c}{{TS}} \\ \hline
RoBERTa               & $1.93$                      & $0.84$              & $2.33$                      & $0.88$                       & $1.76$                      & ${0.76}$                       & $3.62$                      & {${1.46}$}                       & $9.55$                      & $7.86$                       & $11.93$                     & $11.22$                      \\
RoBERTa+LS           & $6.38$                      & $8.70$                       & $6.11$                      & $8.69$                       & $8.81$                      & $11.40$                      & $4.50$                      & $5.93$                       & $8.91$                      & $5.31$                       & $2.14$                      & $2.23$                       \\ 
Manifold-mixup       & $7.32$         & $4.56$          & $3.54$          & $5.05$          & $1.68$          & $0.96$          & $19.78$         & $7.65$           & $7.18$          &  $8.76$         & $5.63$          & $3.43$          \\
Manifold-mixup+LS & $3.51$                     & $3.00$          &  $2.82$  &  $3.03$  &  $1.83$  &  $0.94$  &  
$8.23$  &  $5.08$  &  $6.17$   &  $6.91$   &  $4.27$           &  $2.88$  \\ 
\citet{mixup21acl} & {$1.34$}                     & ${{{0.63}}}$          & {$2.47$}  &{$1.41$}  & $1.24$   & $1.03$  &{$1.41$}  & ${1.18}$ &${\bf{3.94}}$ & ${1.89}$  & $2.40$        & ${2.08}$ \\ 
\citet{mixup21acl}+LS & ${{1.28}}$                     & $1.02$          & {${2.18}$}  &{$\bf{0.84}$}  &{${{\bf{1.12}}}$}  &$0.81$  &${\bf{1.37}}$  & {$1.60$} &${3.96}$ & ${2.67}$  & ${\bf{1.86}}$         & ${1.70}$
 \\ \hline
CME (Ours) & {${\bf{0.84}}$}                     & $\bf{0.61}$          & {${\bf{1.33}}$}  &{${1.16}$}  & $1.24$   & ${\bf{0.69}}$  &{$1.57$}  & $\bf{1.01}$ &${9.26}$ & $\bf{1.71}$  & $9.01$        & $\bf{1.44}$ \\ 
CME+LS (Ours) & $6.83$                     & $1.26$          & {$6.88$}  &{$2.77$}  &{${8.01}$}  &$0.97$  &${3.98}$  & {$2.84$} &${7.77}$ & ${7.14}$  & ${3.80}$     & ${2.32}$
 \\ \hline
\end{tabular}
}
\caption{{Expected calibration errors ($\downarrow$) of BERT-based (Top) and RoBERTa-based (Bottom) models. We report the average results with five different random seeds. The standard deviations are in the Appendix~\ref{sec:sd}. The baselines are vanilla BERT~\cite{calibration_emnlp20}}, Manifold-mixup~\cite{DBLP:conf/icml/VermaLBNMLB19} and~\citet{mixup21acl}.} 
% \vspace{-4mm}
\label{tb:ece_result}
\end{table*}

\subsection{Dataset}

% implying that MLE training yields scores that are fundamentally good but just need some minor rescaling.
We conduct the experiments in three natural language understanding tasks under the in-domain/out-of-the-domain settings: SNLI~\cite{bowman-etal-2015-large}/MNLI~\cite{williams-etal-2018-broad} (natural language inference), QQP~\cite{iyer-2017-quora}/TPPDB~\cite{lan-etal-2017-continuously} (paraphrase detection), and SWAG~\cite{zellers-etal-2018-swag}/HellaSWAG~\cite{zellers-etal-2019-hellaswag} (commonsense reasoning). We describe all datasets in details in Appendix~\ref{sec:dataset:sta}. 
\subsection{Results}
Following~\citet{calibration_emnlp20}, we consider two settings: out-of-the-box (OOTB) calibration (i.e., we directly evaluate off-the-shelf trained models) and post-hoc calibration - temperature scaling (TS) (i.e., we rescale logit vectors with a single temperature for all classes). And we also experiment with Label Smoothing (LS)~\cite{LB1,LB2} compared to traditional MLE training. The models are trained on the ID training set for each task, and the performance is evaluated on the ID and OD test sets. Additionally, we present implementation details and case studies in the Appendix~\ref{sec:exp_setting} and~\ref{sec:case_study}. 

Table~\ref{tb:ece_result} shows the comparison of experimental results (ECEs) on BERT and RoBERTa. First, for OOTB calibration, we find that CME achieves the lowest calibration errors in the ID datasets except for RoBERTa in SWAG. At the same time, training with LS (i.e., CME+LS) exhibits more improvements in the calibration compared with original models in the TPPDB and HellaSWAG datasets. However, in most cases, LS models largely increase calibration errors for ID datasets. We conjecture that LS may affect the smoothness of the gradient and thus produces poor calibrated results.
Secondly, for post-hoc calibration, we observe that TS always fails to correct miscalibrations of models with LS (e.g., CME-TS 0.64 vs. CME+LS-TS 2.16 in SNLI) under ID and OD settings. Nevertheless, TS reduces the ECEs in the OD setting by a large margin (e.g., HellaSWAG BERT 11.64 $\to$ 2.11).
Compared to baselines, CME consistently improves over different tasks on calibration reduction of BERT-based models. While we apply CME to a relatively larger model, models with TS may perform better. It indicates that our method can be complementary to these post-hoc calibration techniques. 

\begin{table}[t!]
% \setlength{\tabcolsep}{4pt}
\small
\centering
\resizebox{\linewidth}{!}{
\begin{tabular}{lcccc}
\toprule
\multirow{2}{*}{\textbf{Model}} & \multicolumn{2}{c}{\textbf{Dev Acc.}} & \multicolumn{2}{c}{\textbf{Test Acc.}} \\
\cmidrule(l){2-3} \cmidrule(l){4-5}
 & ID & OD & ID & OD \\
\midrule
\multicolumn{5}{l}{\textbf{Natural Language Inference (SNLI/MNLI)}} \\
\midrule
BERT-MLE & 90.18 & 74.04 & 90.04 & 73.52 \\
RoBERTa-MLE & 91.20 & 79.17 & 91.23 & 78.79 \\
BERT-CME & 90.22$\pm0.20$ & 74.17$\pm0.86$ & 90.22$\pm0.22$ & 73.81$\pm0.73$ \\
RoBERTa-CME & 91.62$\pm0.14$ & 79.61$\pm0.31$ & 91.37$\pm0.41$ &  79.45$\pm0.27$ \\
\midrule
\multicolumn{5}{l}{\textbf{Paraphrase Detection (QQP/TPPDB)}} \\
\midrule
BERT-MLE & 90.22 & 86.02 & 90.27 & 87.63\\
RoBERTa-MLE & 89.97 & 86.17 & 91.11 & 86.72 \\
BERT-CME & 90.08$\pm0.45$ & 86.22$\pm0.09$ & 90.52$\pm0.39$ & 87.46$\pm0.26$ \\
RoBERTa-CME & 90.23$\pm0.30$ & 86.38$\pm0.76$ & 91.05$\pm0.27$ &  86.44$\pm0.67$ \\
\midrule
\multicolumn{5}{l}{\textbf{Commonsense Reasoning (SWAG/HellaSWAG)}} \\
\midrule
BERT-MLE & 78.82 & 38.01 & 79.40 & 34.48 \\
RoBERTa-MLE & 81.85 & 59.03 & 82.45 & 41.68\\
BERT-CME & 77.57$\pm0.54$ & 33.22$\pm0.73$ & 78.94$\pm0.29$ & 34.75$\pm0.69$ \\
RoBERTa-CME & 80.13$\pm0.25$ & 42.01$\pm0.51$ & 82.47$\pm0.35$ &  41.92$\pm0.38$ \\
\bottomrule
\end{tabular}
}
\caption{Average accuracy of development set and test set results for ID and OD datasets using pre-trained models with five random seeds. The results of BERT and RoBERTa baselines are from Table 2 and Table 6 of~\citet{calibration_emnlp20}.}
%  The results of BERT and RoBERTa baselines are from Table 2 and Table 6 of~\citet{calibration_emnlp20}.
% \vspace{-4mm}
\label{tab:dev-results}
\end{table}
\subsection{Analysis}
\label{sec:task_perfor}
Table~\ref{tab:dev-results} presents the accuracy of BERT and RoBERTa on the development sets and test sets of our datasets. Our models have comparable accuracy (even better) compared to fine-tuned counterparts. For example, RoBERTa-CME has better accuracy than RoBERTa in the test set of the MNLI dataset (79.45 vs. 78.79). Specifically, CME performs poorly on the development set of HellaSWAG but performs comparably to baselines on the test set.

\begin{figure}[t!]
\centering
    \subfloat[BERT]{{\includegraphics[width=3.7cm]{misc/bert_confidence.pdf}}}
    \subfloat[RoBERTa]{{\includegraphics[width=3.7cm]{misc/roberta_confidence.pdf}}}
\caption{Visualization of calibration (OOTB) between different PLMs and competitive methods on QQP.}
% \vspace{-4mm}
\label{fig:ablationL}
\end{figure}

As shown in Figure~\ref{fig:ablationL}, we visualize the alignment between the posterior probability measured by the model confidence and the empirical output measured by the accuracy. Note that a perfectly calibrated model has confidence equals accuracy for each bucket. Our model performs well under both PLMs architectures. We observe that, in general, CME helps calibrate the confidence of cases close to the decision boundary as it does not change most predictions. For example, compared to the baseline, CME optimizes the samples whose predicted probabilities are higher than actual probabilities. Moreover, we find that training with label smoothing technique can make the model underestimates some examples with high predicted probabilities. In addition, we conducted preliminary experiments with different batch sizes, and found that more large sizes did not significantly impact calibration performance. On the other hand, we found that larger LMs usually achieve both higher accuracy and better calibration performance (Table~\ref{tb:ece_result}), which is in line with the observation in question answering~\cite{DBLP:journals/tacl/JiangADN21}.
