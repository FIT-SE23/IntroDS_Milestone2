
\section{Related Work}

\paragraph{Few-Shot Learning with PLMs.}

Few-shot learning has gained much attention recently due to its minimal resource assumption---Without requiring massive annotated data but only leveraging a few training samples (\eg, $16$ per label), few-shot methods can be widely adopted in many practical scenarios where obtaining large-scale annotations is unaffordable.
Standard fine-tuning of PLMs for few-shot learning usually performs poorly because the limited training samples may not be sufficient for optimizing the parameters in the newly introduced classification head.
To reuse the language modeling ability of PLMs without introducing randomly initialized parameters, prompt-based approaches~\citep{Brown2020LanguageMA,gao2021making,Hu2022KnowledgeablePI,logan2021cutting,Min2022NoisyCL,Schick2021ExploitingCF,Schick2021FewShotTG,Schick2021ItsNJ,Tam2021ImprovingAS} formulate training samples as natural language prompt templates so that various downsteam tasks can be solved as a token prediction problem.
They enjoy improved training data efficiency over standard fine-tuning in low-data regimes~\citep{Scao2021HowMD} and achieve remarkable few-shot learning performance. 
Later developments in prompt-based methods replace the manual design of prompt templates with automatic search or learning~\citep{Cui2022PrototypicalVF,Hambardzumyan2021WARPWA,Lester2021ThePO,Liu2021GPTUT,zhang2022differentiable,Zhong2021FactualPI}.
There are also studies focusing on specific issues in prompt-based methods such as densifying the supervision by revising the training objective~\citep{Liu2022FewShotPF,Tam2021ImprovingAS} and calibrating the biased predictions of PLMs before fine-tuning~\citep{Zhao2021CalibrateBU}.
Instead of focusing on fine-tuning methods for few-shot learning, we study how to effectively generate abundant quality training samples by learning from the few-shot samples and use them to improve the generalization of the classification model. 

\paragraph{Data Augmentation.}
Data augmentation methods~\citep{Chen2020MixTextLI,lee2021neural,Miyato2017AdversarialTM,Xie2020UnsupervisedDA} aim to create similar samples to the existing ones so that the enlarged training set can benefit model generalization.
% However, data augmentation usually relies on a large amount of task-specific unlabeled data, which can be hard to obtain for some NLU tasks that have requirements on the input data formats~\citep{Tam2021ImprovingAS}.
% Our \method method is able to synthesize abundant training data based on the few-shot samples and thus does not require additional unlabeled data.
% It is also possible to integrate \method with data augmentation methods (\eg, by applying the idea of consistency training~\citep{Xie2020UnsupervisedDA} to generated samples with the same label).
Early approaches simply use manually designed rules (\eg, swapping or inserting tokens) for word-level alterations over the given samples to create new ones~\citep{Wei2019EDAED}.
Later methods leverage the strong generation power of PLMs to synthesize novel samples from scratch. 
Given a training set, the PLMs can be either fine-tuned on the labeled samples to learn label-conditioned generation probability~\citep{Kumar2020DataAU,lee2021neural,Yang2020GDAugGD} or take the labeled data as demonstrations~\citep{Wang2021TowardsZL,Yoo2021GPT3MixLL} to generate similar samples pertaining to the same label. 
In this work, we study how to effectively tune generators on few-shot training data for creating new data---standard fine-tuning of PLMs on a small set of training data is prone to overfitting, and the resulting model may struggle to generate accurate, diverse and novel training data.
We address this challenge by leveraging prefix-tuning and proposing a new meta-weighted training objective to emphasize label-discriminative tokens for generator tuning.

\paragraph{Controlled Text Generation.}

Generating training samples for different labels can be viewed as a form of controlled text generation~\citep{Hu2017TowardCG}, whose goal is to generate textual contents of desired semantics, styles or attributes.
Such control can be realized through different stages of PLM training and deployment:
During pretraining, control codes~\citep{Keskar2019CTRLAC} can be used as explicit guidance for training the model to generate domain/attribute-specific texts;
fine-tuning PLMs with attribute-specific data can also grant high-level control (\eg, certain topics or sentiments~\citep{Ziegler2019FineTuningLM}), fine-grained control (\eg, specific words or phrases~\citep{Chan2021CoConAS}) or both~\citep{Khalifa2021ADA}; at inference time, control over desired attributes can also be enforced without updating the PLM parameters~\citep{Dathathri2020PlugAP,Krause2021GeDiGD,Kumar2021ControlledTG,Liu2021DExpertsDC,Pascual2021APM,Yang2021FUDGECT}.
More specifically related to the idea of generating training data with language models, early methods in topic classification use bag-of-words or LSTM-based language models~\citep{Meng2018WeaklySupervisedNT,Meng2019WeaklySupervisedHT} to generate class-conditioned texts as training data. Recently, a few studies explore fine-tuning autoregressive PLMs ~\citep{AnabyTavor2020DoNH,Yang2020GDAugGD} with the standard language modeling objective on the training set or using label-specific prompts~\citep{Meng2022GeneratingTD,Schick2021GeneratingDW,Wang2021TowardsZL,Ye2022ZeroGenEZ} to steer text generation towards the desired label.
% In this work, we analyze issues with directly tuning PLMs on few-shot samples with the standard maximum likelihood objective and propose a weighted variant of the objective that encourages the PLM to focus on label-discriminative tokens.
% The weights are automatically learned via a meta-learning discriminative objective.

\paragraph{Meta-Learning for Sample Weighting.}
The idea of weighting training samples in the loss calculation originates from the class imbalance~\citep{Wang2017LearningTM} and noisy label~\citep{Hendrycks2018UsingTD} learning scenarios---By assigning higher weights to the samples from minority classes or lower weights to the noisy samples, the learning process is less impacted by the imbalance/label noise issues.
Meta-learning~\citep{Andrychowicz2016LearningTL,Finn2017ModelAgnosticMF,Franceschi2018BilevelPF,Wu2018LearningTT} is one way to automatically learn the weight for each sample. 
Specifically, a meta objective, usually defined as the loss on a clean unbiased validation set~\citep{Ren2018LearningTR,Shu2019MetaWeightNetLA}, can be used to learn the sample weights which become hyperparameters that control the optimization of model parameters.
Our work has a different motivation and formulation of the meta objective for token-wise weighted training:
Not all tokens in a training sample are equally label-discriminative.
We thus design a meta objective to emphasize distinction across different labels (instead of using the validation loss as the meta objective) for learning the token weights.
