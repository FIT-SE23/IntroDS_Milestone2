\section{Evaluation}
\input{Tables/main_res}
\subsection{Main Results}
We present the results of \method and baselines in Table~\ref{tab:main_res}.
\method achieves overall better performance across the GLUE tasks, on average $5+$ points higher than the previous best few-shot method without augmentation, and $3+$ points better than GPT3Mix\footnote{The CoLA results reported in the original GPT3Mix paper use accuracy as the metric instead of Matthews correlation; our reimplemented GPT3Mix achieves ${79.4}_{0.6}$ on CoLA if measured by accuracy.}~\citep{Yoo2021GPT3MixLL} which uses a $100$ times larger  generator model ($175$B) than \method.
The promising results confirm the effectiveness of our proposed \method method in generating quality training data and leveraging them in combination with the few-shot training set for fine-tuning the classification model.
% The improved model performance is in accordance with our intuition that training the classification model with more data benefits generalization. 
\paragraph{Comparison with Back Translation.} Using back translation to paraphrase the few-shot samples does not improve the results, even with prompt-based fine-tuning to train the classifier -- this is probably because it does not produce samples that are sufficiently different from the few-shot training set.
The success of UDA~\citep{Xie2020UnsupervisedDA} is grounded in the augmentations from abundant unlabeled data that improve the classifier generalization.
However, under the strict few-shot learning setup, there is no access to additional task-specific unlabeled data~\citep{gao2021making}, making it challenging for paraphrase-based methods to create sufficiently diverse training samples only based on the small few-shot set. The new training samples produced by our \method method are not limited to the paraphrases of the few-shot samples, as the generator is trained via prefix-tuning to preserve the PLM's pretraining knowledge, based on which novel training samples can be synthesized.
\paragraph{Comparison with GPT3Mix.} The gigantic size of GPT3 makes it challenging for tuning on few-shot samples. Therefore, GPT3Mix~\citep{Yoo2021GPT3MixLL} uses few-shot samples as demonstrations for creating the augmentations. Such an approach suffers from two limitations: (1) Without any parameter update to the PLM, its learning ability is not fully leveraged to adapt to the few-shot training set. (2) The PLM can only use a small subset of the few-shot samples at a time for creating each augmentation, as the number of demonstrations received by the model is bounded by its maximum input sequence length. This makes the quality of the created augmentations more sensitive to the  randomly drawn training samples. Our \method method, on the other hand, can use the entire few-shot set for tuning the PLM and achieves overall even better classification results with a much smaller PLM ($<1\%$ the size of the GPT3 model) which can be deployed much more easily in practice.

\subsection{Ablation Studies}
\input{Tables/ablation}
We further analyze the effectiveness of each important component in \method. 
Specifically, we compare \method with the following ablations:
(1) Using the standard $\mathcal{L}_{\text{gen}}$ in Eq.~\eqref{eq:gen} instead of our proposed  $\mathcal{L}_{\text{w-gen}}$ in Eq.~\eqref{eq:weigh_gen} for generator tuning (w. $\mathcal{L}_{\text{gen}}$);
(2) using the directly combined $\mathcal{L}_{\text{gen}}$ and $\mathcal{L}_{\text{disc}}$ for generator tuning (w. $\mathcal{L}_{\text{gen}}+\mathcal{L}_{\text{disc}}$);
(3) without applying label smoothing in Eq.~\eqref{eq:finetune} ($-$ label smooth);
(4) without applying temporal ensembling in Eq.~\eqref{eq:finetune} ($-$ temporal ensemble).
% (4) directly fine-tuning the classification model on $\mathcal{D}_{\text{gen}}$ instead of first on $\mathcal{D}_{\text{train}}$ and then on $\mathcal{D}_{\text{gen}}$ (- fine-tune on $\mathcal{D}_{\text{train}}$).
% and (4) using RoBERTa$_{\text{Large}}$~\citep{liu2019roberta} instead of COCO-LM$_{\text{Large}}$ as the classification model (w. RoBERTa).
As shown in Table~\ref{tab:ablation}, 
(1) \& (2) using the standard maximum likelihood loss or the combination of generation and discrimination losses to tune the generator both yield lower-quality training data and lead to degraded classification performance; 
(3) \& (4) not applying regularization techniques for fine-tuning the classifier is more prone to label noise in the generated samples.
% (4) not leveraging the in-domain data $\mathcal{D}_{\text{train}}$ for training the classifier results in worse model initialization for training on $\mathcal{D}_{\text{gen}}$, and the eventual classifier will not perform very well.
% (4) the model performance is not very sensitive to the choice of classifier PLMs. 
% \input{Figs/num_data}
To study the impact of the amount of generated training samples on the model performance, we plot the MNLI-m accuracy (mean and standard deviation) with different sizes of $\mathcal{D}_{\text{gen}}$ in Fig.~\ref{fig:num_data}. 
Both the average model performance and stability improve with more generated samples.

\input{Figs/plots}


\subsection{Analyses of Loss Functions for Generator Tuning}
As shown in Table~\ref{tab:ablation}, the choice of generator loss has a significant impact on the synthesized data quality and thus the final model performance.
We conduct further analyses to compare the training processes of the generator under the following three loss functions and the resulting generated samples:
(1) $\mathcal{L}_{\text{gen}}$ which is the standard language modeling loss;
(2) $\mathcal{L}_{\text{gen}}+\mathcal{L}_{\text{disc}}$ which directly adds the discriminative loss to generator training;
and (3) $\mathcal{L}_{\text{w-gen}}$ which is our meta-weighted objective.
Fig.~\ref{fig:loss_funcs} shows the discriminative loss $\mathcal{L}_{\text{disc}}$ and the standard language modeling loss on the held-out development set throughout training.
Although using $\mathcal{L}_{\text{gen}}+\mathcal{L}_{\text{disc}}$ helps reduce the discriminative loss, it comes at the cost of hindering language modeling---the generator loss on the development set is high.
Using our meta-weighted objective $\mathcal{L}_{\text{w-gen}}$ for tuning the generator not only encourages discriminativeness but also mitigates overfitting, yielding the lowest validation set loss.
This is probably because the model receives contrastive information from other labels which facilitates more accurate modeling of the texts with the target label.
We also showcase concrete generation results for the three labels of MNLI by models trained with the three different loss functions in Table~\ref{tab:case_studies}.
The model trained with $\mathcal{L}_{\text{gen}}$ produces fluent and coherent sentences, but the generated sentences do not accurately pertain to the desired label (\ie, the ``entailment'' and ``contradiction'' generation results are in fact neutral with respect to the given sentence), lacking label discriminativeness.
When $\mathcal{L}_{\text{gen}}+\mathcal{L}_{\text{disc}}$ is used, the generated samples of different labels are more distinctive, but also become less natural and coherent due to the model's language modeling ability being hampered.
The generator tuned with $\mathcal{L}_{\text{w-gen}}$ produces both coherent and label-discriminative samples which can serve as quality training data.
We also visualize the token weights $\bs{w}$ automatically learned and used in $\mathcal{L}_{\text{w-gen}}$ in Appendix~\ref{app:vis}.
\input{Tables/case_study}

% \input{Figs/visualize}
% \subsection{Visualization of Token Weight Learning}
% To gain intuitive understanding of what tokens are assigned more weight during generator tuning, we visualize the learned weights in Fig.~\ref{fig:vis}.
% The tokens with higher weights (\eg, ``weak'' in the first example and ``hates'' in the second example) are learned to be important tokens that decide the relation of the second sentence to the first sentence (\ie, the label of the training sample).
% With such tokens emphasized during training, the generator is encouraged to capture label-discriminative information that facilitates the generation of unambiguous training samples.
