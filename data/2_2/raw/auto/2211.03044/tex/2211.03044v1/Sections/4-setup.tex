
\section{Experimental Setup}\label{sec:setup}

\paragraph{Downstream Tasks and Metrics.} 
We conduct evaluation on all tasks of the GLUE benchmark~\citep{wang2018glue} (more details in Appendix~\ref{app:glue}) except STS-B which is a regression task.
We follow the same data split and evaluation protocol as \cite{gao2021making}: 
Both $\mathcal{D}_{\text{train}}$ and $\mathcal{D}_{\text{dev}}$ contain $16$ samples per label and are sampled from the original training set with $5$ different random seeds.
The original development sets are used for testing.
For all reported results, we include the average and standard deviation over the $5$ different $\mathcal{D}_{\text{train}}$/$\mathcal{D}_{\text{dev}}$ splits.
F1 score is used as the metric for QQP and MRPC, Matthews correlation for CoLA, and accuracy for the remaining tasks.

\paragraph{Models, Training Settings and Hyperparameters.} 
\method is a training data generation method and can be used with any fine-tuning method on any classification model.
We use moderate-sized PLMs to ensure our results are reproducible on typical research hardware:
CTRL ($1.6$B parameters)~\citep{Keskar2019CTRLAC} as the generator $G_{\theta}$ and RoBERTa$_{\text{Large}}$ ($356$M parameters)~\citep{liu2019roberta} as the classifier $C_{\phi}$. 
% We also show the results using similar-sized PLMs (GPT-2~\citep{radford2019language}/RoBERTa~\citep{liu2019roberta}) as the generator/classifier in Section~\ref{sec:plms}.
We use prefix-tuning for training $G_{\theta}$ and prompt-based fine-tuning for training $C_{\phi}$.
For simplicity, we use the most basic manual prompt version of LM-BFF~\citep{gao2021making}.
The only exception is CoLA for which we use the standard fine-tuning since the input data might be out of the distribution of $C_{\phi}$~\citep{gao2021making}.
The hyperparameter tuning is performed on $\mathcal{D}_{\text{dev}}$. More details are in Appendix~\ref{app:impl_details}.

\paragraph{Compared Methods.} 
No-augmentation baselines include zero-shot prompting, standard fine-tuning, in-context learning, and the following strong few-shot learning methods: Four versions of LM-BFF~\citep{gao2021making}, P-Tuning~\citep{Liu2021GPTUT} and DART~\citep{zhang2022differentiable}.
We also compare \method with data augmentation methods for few-shot learning: Using back translation systems to generate paraphrases (UDA-style~\citep{Xie2020UnsupervisedDA} augmentation), GPT3Mix~\citep{Yoo2021GPT3MixLL} and standard fine-tuning of generator on the few-shot samples with prompts. All augmentation methods use LM-BFF (Man.) for fine-tuning the RoBERTa$_{\text{Large}}$ classifier.
More details about data augmentation baselines can be found in Appendix~\ref{app:aug_baselines}.
% We also include the results of fine-tuning using the entire training set. 

% \paragraph{Implementation Details.}
% For single-sequence tasks (SST-2 and CoLA), we tune $G_{\theta}$ on the entire few-shot sample and then use it to generate training data from scratch; for sequence-pair tasks (MNLI, QQP, QNLI, RTE and MRPC), we tune $G_{\theta}$ on the second sequence of the few-shot sample given the first sequence and then use it to generate training data by sampling the first sequence from the pretraining corpus (\eg, Wikipedia).
% The experiments are conducted on NVIDIA GeForce RTX 3090 and A6000 GPUs.
% More details can be found in Appendix~\ref{app:impl_details}.
