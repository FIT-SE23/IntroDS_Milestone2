
% \input{Figs/overview}

\section{Introduction}
Recent research has demonstrated the appealing few-shot learning potential of pretrained language models (PLMs)~\citep{Brown2020LanguageMA,clark2020electra,devlin2019bert,he2020deberta,liu2019roberta,meng2021coco,Meng2022PretrainingTE} on natural language understanding (NLU) tasks~\citep{Wang2019SuperGLUEAS,wang2018glue}:
Instead of relying on abundant task-specific annotations, PLMs can effectively leverage a small set of training samples to quickly learn a new task.
Such training data efficiency is usually achieved by formulating downstream tasks as prompts~\citep{Brown2020LanguageMA,gao2021making,Scao2021HowMD,Schick2021ExploitingCF,Schick2021ItsNJ} which allow the PLM to adapt its language modeling ability acquired through pretraining to new downstream tasks.

The success of prompt-based methods has stimulated numerous explorations along the line of effective few-shot learning with PLMs: 
The training samples converted to natural language prompts can be used to directly fine-tune PLMs~\citep{gao2021making,Schick2021ExploitingCF} or as in-context demonstrations to facilitate better inference~\citep{Brown2020LanguageMA,Liu2022WhatMG}.
More recent approaches aim to automate the design of prompts by gradient-based searching~\citep{Shin2020ElicitingKF} or parameterizing prompts as continuous learnable embeddings~\citep{Lester2021ThePO,Liu2021GPTUT,zhang2022differentiable,Zhong2021FactualPI}.
Other studies investigate and address specific issues in prompt-based few-shot learning~\citep{Liu2022FewShotPF,Tam2021ImprovingAS,Zhao2021CalibrateBU}.
While remarkable, the model performance still has a nontrivial gap from fully supervised models trained on massive labeled data.
Indeed, training deep models is inherently data demanding---model generalization usually benefits from more training samples~\citep{baum1988size}.

In this work, we study few-shot learning with PLMs from a different perspective:
Instead of proposing new methods for fine-tuning on few-shot samples, we focus on the generation of quality training data based on few-shot samples and using these synthesized training samples to fine-tune the classification models.
Motivated by the strong text generation power of autoregressive PLMs~\citep{Brown2020LanguageMA,Keskar2019CTRLAC,raffel2019t5}, 
previous data augmentation methods enlarge the training set by synthesizing new samples based on the few-shot samples. 
They either fine-tune the generator on the training set with the standard maximum likelihood objective~\citep{AnabyTavor2020DoNH,Kumar2020DataAU} or use the training samples as demonstrations~\citep{Yoo2021GPT3MixLL}.
However, these methods do not explicitly model the distinction across different labels and may struggle to generate accurate training samples pertaining to the desired labels for challenging NLU tasks.
% we aim to use them as generators for creating novel training data after tuning on few-shot samples.

In this paper, we study how to use few-shot samples to effectively tune PLMs to generate high quality label-discriminative training samples.
% We first analyze the 
% To ensure the samples created by the generator PLM are label-discriminative, we propose a weighted maximum likelihood objective, where the weight for each token is automatically learned from a discriminative meta-learning objective. 
% The synthesized samples by the generator can be combined with the few-shot samples for training any classification PLM with any fine-tuning method.
Our contributions are as follows: (1) We analyze the issues of using standard maximum likelihood for tuning the generator and propose a meta-weighted maximum likelihood objective for generator tuning by automatically learning token weights that emphasize label discriminativeness.
(2) We propose a simple and effective training procedure for fine-tuning classification PLMs on generated data by mitigating label noise.
(3) Under the same few-shot learning setting, our method \method outperforms existing methods by $3+$ average points on seven classification tasks of the GLUE benchmark~\citep{wang2018glue}. Ablation studies demonstrate the effectiveness of our proposed meta-weighted training objective and classifier fine-tuning method.
