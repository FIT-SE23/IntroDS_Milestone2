
\section{Discussions and Conclusions}\label{sec:concl}
\paragraph{Ethical Considerations.}
% While PLMs have demonstrated remarkable text generation and understanding capability, they can come with potential risks or harms~\citep{Bender2020ClimbingTN,Bender2021OnTD,Brown2020LanguageMA} such as generating misinformation~\citep{Pagnoni2021UnderstandingFI} or amplifying harmful biases~\citep{Prabhumoye2018StyleTT}. The focus of our work is on utilizing existing PLMs to generate training data for NLU tasks instead of developing new PLMs or generation methods.
% Therefore, our method can be used in company with any bias reduction and correction techniques~\citep{Gehman2020RealToxicityPromptsEN,Ma2020PowerTransformerUC} to mitigate the risks of PLMs.
Despite the impressive text generation and representation power of PLMs, they can also come with the risk~\citep{Bender2021OnTD,Bender2020ClimbingTN,Brown2020LanguageMA} of generating disinformation~\citep{Pagnoni2021UnderstandingFI} or exacerbating biases~\citep{Prabhumoye2018StyleTT}. Instead of improving upon PLM architectures or generation techniques, our work focuses on using existing PLMs to create training data for NLU tasks. Therefore, our method can be combined with any bias reduction and correction strategies~\citep{Gehman2020RealToxicityPromptsEN,Ma2020PowerTransformerUC} in practice to reduce the adverse effects of PLMs.

\paragraph{Limitations.}

Compared to few-shot learning methods that directly train classification models on the small training set, \method requires tuning a generator PLM and using it to synthesize novel training samples, resulting in higher computation costs and longer running time.
Still, we believe that our method may bring more good than harm---when the small training data size becomes the performance bottleneck for NLU tasks, a simple yet costly solution is to obtain more human annotations.
Our method may replace or reduce the human efforts in such training data creation processes.

\paragraph{Conclusions.}
In this work, we propose \method, 
which leverages few-shot training samples to tune a generator PLM for synthesizing novel training data.
The generated data can be then used in combination with few-shot samples to fine-tune a classification model for better generalization.
To emphasize label-discriminative information during generator tuning, we propose a weighted maximum likelihood objective where the token weights are automatically learned via a discriminative meta objective.
Since the generated samples may contain label noise, we propose a simple training procedure that first trains classifiers on the few-shot training set and then on the generated set by applying temporal ensembling for noise-robustness.
Across seven classification tasks from the GLUE benchmark, \method significantly outperforms existing approaches under the same few-shot learning setting.
The effectiveness of each important component in \method is validated via ablation studies.
Future work directions may include: Using larger PLMs as the generator and the classifier, jointly training both models with each other's high-confident predictions, and developing systematic metrics for evaluating the quality of generated training samples.
