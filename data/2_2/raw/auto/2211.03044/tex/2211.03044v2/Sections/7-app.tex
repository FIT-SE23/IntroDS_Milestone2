
\section{Details of Weighting Network Implementation}\label{app:weight_net}
Since the token weights $\bs{w}$ used in Eq.~\eqref{eq:meta_disc} need to characterize the discriminativeness of each token, we use the value of discriminative objective at each token  $\mathcal{L}_{\text{disc}}^j$ as the input to the weighting network, and we use softmax to normalize the weights:
% $$
% w_j(\bs{\omega}) = g_{\bs{\omega}}(\mathcal{L}_{\text{disc}}^j).
% % \quad
% % \mathcal{L}_{\text{disc}}^j = -\log p_{\bs{\theta}_{p}}(y_l|\bs{x}_{\le j}).
% $$
% We further normalize 
$$
w_j(\bs{\omega}) =  \frac{\exp\left(g_{\bs{\omega}}(\mathcal{L}_{\text{disc}}^j)\right)}{\sum_{{j'}=1}^n \exp\left(g_{\bs{\omega}}(\mathcal{L}_{\text{disc}}^{j'})\right)}.
$$

Following \cite{Shu2019MetaWeightNetLA}, we instantiate $g_{\bs{\omega}}$ to be a feedforward network (FFN) with only one $100$-dimension hidden layer by default.
% We explore an alternative instantiation that adds one self-attention layer on top of the generator PLM's output hidden states. The meta weights are finally obtained by projecting the outputs of the self-attention layer using another linear layer.
% We evaluate the resulting generator quality via the same two metrics as in Table~\ref{tab:gen_eval}.
% Table~\ref{tab:weight_net} shows that using more complicated architectures (\eg, adding another self-attention layer) does not result in a better generator compared to using a simple FFN for meta weight learning. 
% This is probably because the generator PLM's output representations are sufficiently contextualized and contain the information necessary for learning the token weights, thus a simple FFN as the weighting network will be enough.
% Using more complicated networks, on the other hand, will introduce more randomly initialized new parameters which may not be learned well using the limited amount of few-shot training data.

\section{Implementation Details}
\label{app:impl_details}
\input{Tables/full_prompts}

\paragraph{Details of Initialization Prompts Used for Generator Tuning on Different Tasks.}

For generator tuning, we find it beneficial to initialize the prefix vectors with task-descriptive prompts, similar to the observations in \cite{Li2021PrefixTuningOC}.
The prefix lengths (\ie, number of trained prefix token positions) are equal to the number of tokens in the prompts.
We present details about the prompts used for initializing the prefix vectors for different tasks in Table~\ref{tab:full_prompts}.
For sequence-pair tasks, an additional infix prompt is used between the two sequences,
and we also tune the embeddings of the infix (\ie, prompt-tuning~\citep{Lester2021ThePO}) for generator training.



\paragraph{Details of Generator Tuning.}
% \input{Figs/num_data.tex}
% In Algorithm~\ref{alg:meta}, we use SGD with $2e-2$ as the learning rate for the first gradient update (\ie, optimizing $\hat{\theta}_{p}^{(t)}\left(\omega^{(t)}\right)$); we use SGD with $1e-2$ as the learning rate for the second gradient update (\ie, optimizing $\omega^{(t+1)}$); we use Adam~\citep{Kingma2015AdamAM} with $5e-3$ as the learning rate for the third gradient update (\ie, optimizing $\theta_{p}^{(t+1)}$).
The meta-weighted generator tuning procedure (Algorithm~\ref{alg:meta}) involves three forward and backward passes, and thus its time complexity is approximately $3$ times of standard generator training without meta learning.
However, since the few-shot training sets have a small amount of training data, the extra time cost is usually affordable.
In practice, our generator tuning with meta weight learning takes $10$ minutes to train on each task (the standard generator training time without meta-learning is $3.5$ minutes).
We use a fixed set of hyperparamters for all tasks without task-specific hyperparamter tuning: In Algorithm~\ref{alg:meta}, we set the batch size to be $2$, the learning rate for optimizing $\hat{\bs{\theta}}_{p}$ to be $2e-2$, the learning rate for optimizing $\bs{\omega}$ to be $1e-2$, the learning rate for optimizing $\bs{\theta}_{p}$ to be $5e-3$, and training epoch to be $20$.
We also experiment with larger batch sizes (\eg, $16$/$32$) and/or training for more epochs, but they result in worse language modeling quality than the default hyperparamters.

\paragraph{Details of Generating Training Data.}


Following~\cite{Meng2022GeneratingTD}, for sequence-pair tasks (MNLI, QQP, QNLI, RTE and MRPC), we randomly sample the first sequence from the pretraining corpus (\eg, Wikipedia) and use greedy sampling for generating the second sequence.
For single-sequence tasks (SST-2 and CoLA), we use top-$k$ sampling with temperature to generate training data from scratch where $k=10$.
For all tasks, we generate $5,000$ samples per label.
% To study the impact of the amount of generated training samples on the model performance, we plot the MNLI-m accuracy (mean and standard deviation) with different sizes of $\mathcal{D}_{\text{gen}}$ in Fig.~\ref{fig:num_data}. 
% Both the average model performance and stability improve with more generated samples.

For SST-2, we use one of the following tokens to start generation: ``a'', ``one'', ``the'', ``this'', ``that'', ``i'', ``you'', ``it'', ``what''. 
For CoLA, we use a random stop word to start generation.

\input{Tables/gen_hyperpara}

We apply repetition penalty~\citep{Keskar2019CTRLAC} to the logits of tokens that have already appeared in the sequence.
Overall, the token probability distribution is post-processed as follows before conducting sampling:
\begin{align*}
\label{eq:penalty}
p_{\theta}(x_i|\bs{x}_{<i}) &= \frac{\exp(\bs{e}_i^\top \bs{h}_i/\omega)}{\sum_{j=1}^{|V|}\exp(\bs{e}_j^\top \bs{h}_i/\omega)}, \\  \omega &= \begin{cases}
\tau \alpha & x_i \in \bs{x}_{<i} \\
\tau & \text{else}
\end{cases},    
\end{align*}
where $\tau$ is the temperature hyperparameter, and $\alpha$ is the repetition penalty hyperparameter.
For labels that favor token repetitions between the first and the second sequences (\eg, paraphrase or entailment), we set $\alpha$ to be a smaller value (\eg, $1.0$), and vice versa.

The hyperparameter values for training data generation on all tasks can be found in Table~\ref{tab:gen_hyperpara}.


\paragraph{Hyperparameters for Fine-Tuning Classifier PLMs.}
% \input{Tables/finetune_hyperpara}
% Table~\ref{tab:finetune_hyperpara} lists the hyperparameters used in the fine-tuning stage.
For fine-tuning on the few-shot training samples $\mathcal{D}_{\text{train}}$, we search among the following hyperparameter ranges based on development set ($\mathcal{D}_{\text{dev}}$) model performance and pick the best performing model for futher fine-tuning on synthesized data:
Learning rate in $[1e-5, 2e-5]$ and batch size in $[4, 8]$.
The number of training steps is fixed to be $1000$. We also find it beneficial to apply label smoothing (smoothing weight set to $0.15$) for fine-tuning on the few-shot training set.

For fine-tuning on the synthesized training samples $\mathcal{D}_{\text{gen}}$,
we use the following hyperparameters:
$5e-6$ as the learning rate; $16$ as the batch size; label smoothing weight $\epsilon = 0.15$ ; temporal ensemble momentum $\gamma = 0.9$; temporal ensemble loss weight $\lambda = 20$; training steps $T = 6,000$.

\paragraph{Details of Temporal Ensembling for Fine-Tuning Classifier PLMs on Synthetic Data.}

We update ensembled predictions $\bar{\bs{z}}$ as follows where $\bs{p}_{\phi}$ is the current model prediction, $\gamma$ is the momentum parameter, $\hat{\bs{z}}$ is the accumulated model prediction before bias correction, $\bar{\bs{z}}$ is the accumulated model prediction after bias correction, and $t$ is the number of updates $\bar{\bs{z}}$ has received:
\begin{equation*}
\label{eq:udpate_ens}
\hat{\bs{z}} \gets \gamma\hat{\bs{z}} + (1-\gamma)\bs{p}_{\phi}, \, \bar{\bs{z}} \gets \hat{\bs{z}}/(1-\gamma^t).
\end{equation*}
The accumulated model prediction $\hat{\bs{z}}$ has a zero initialization;  the division $(1-\gamma^t)$ is for bias correction~\citep{Laine2017TemporalEF}.
After each update of $\hat{\bs{z}}$, it will be compared to a threshold value $\delta$; each synthesized sample $(\tilde{\bs{x}}, \tilde{y})$ will be included in training only if $\bar{z}_{\tilde{y}} > \delta$.

We update the ensembled predictions $\bar{\bs{z}}$ on all samples in $\mathcal{D}_{\text{gen}}$ every $200$ steps, and set the threshold value for sample filtering $\delta = 0.8$.


\paragraph{Computation Environment.}
The experiments are conducted on NVIDIA A100 GPUs.


\section{Derivation of Meta Weight Gradient Update}
\label{app:gradient}


We first write out the gradient update of $\hat{\bs{\theta}}_{p}^{(t)}\left(\bs{\omega}^{(t)}\right)$ and $\bs{\omega}^{(t+1)}$ according to Algorithm~\ref{alg:meta} as follows:

{\small
\begin{equation}
\label{eq:theta_update}
\hat{\bs{\theta}}_{p}^{(t)}\left(\bs{\omega}^{(t)}\right) 
= \bs{\theta}_{p}^{(t)} - \alpha \left . \frac{\partial\mathcal{L}_{\text{w-gen}} \left(\bs{\theta}_{p};\bs{\omega}^{(t)}\right) }{\partial \bs{\theta}_{p}} \right \vert_{\bs{\theta}_{p} = \bs{\theta}_{p}^{(t)}}
= \bs{\theta}_{p}^{(t)} - \alpha \left . \sum_{j=1}^n w_j \left(\bs{\omega}^{(t)} \right) \frac{\partial \mathcal{L}^j_{\text{gen}} (\bs{\theta}_{p}) }{\partial \bs{\theta}_{p}} \right \vert_{\bs{\theta}_{p} = \bs{\theta}_{p}^{(t)}}
\end{equation}

\begin{equation}
\label{eq:omega_update}
\bs{\omega}^{(t+1)} = 
\bs{\omega}^{(t)} - \beta \left . \frac{\partial \mathcal{L}_{\text{disc}}\left(\hat{\bs{\theta}}_{p}^{(t)}\left(\bs{\omega}\right)\right) }{\partial \bs{\omega}} \right \vert_{\bs{\omega} = \bs{\omega}^{(t)}} .
\end{equation}
}where $\alpha$ and $\beta$ are step sizes.

The gradient in Equation~\eqref{eq:omega_update} is calculated as:
{\small
\begin{align*}
& \quad \left . \frac{\partial \mathcal{L}_{\text{disc}} \left(\hat{\bs{\theta}}^{(t)}_{p}\left(\bs{\omega} \right)\right)}{\partial \bs{\omega}} \right \vert_{\bs{\omega} = \bs{\omega}^{(t)}} \\
&= \left . \frac{\partial \mathcal{L}_{\text{disc}} \left(\hat{\bs{\theta}}_{p}\right)}{\partial \hat{\bs{\theta}}_{p}} \right \vert_{\hat{\bs{\theta}}_{p} = \hat{\bs{\theta}}_{p}^{(t)} } \left . \frac{\partial \hat{\bs{\theta}}_{p}\left(\bs{\omega}\right)}{\partial \bs{\omega}} \right \vert_{\bs{\omega} = \bs{\omega}^{(t)}} \\
&= \left . \frac{\partial \mathcal{L}_{\text{disc}} \left(\hat{\bs{\theta}}_{p}\right)}{\partial \hat{\bs{\theta}}_{p}} \right \vert_{\hat{\bs{\theta}}_{p} = \hat{\bs{\theta}}_{p}^{(t)} } \left( -\alpha \sum_{j=1}^n  \left . \frac{\partial \mathcal{L}^j_{\text{gen}} (\bs{\theta}_{p}) }{\partial \bs{\theta}_{p}} \right \vert_{\bs{\theta}_{p} = \bs{\theta}_{p}^{(t)}} ^\top  \left .\frac{\partial w_j \left(\bs{\omega} \right)}{\partial \bs{\omega}}\right \vert_{\bs{\omega} = \bs{\omega}^{(t)}} \right) \tag*{Plugging in Eq.~\eqref{eq:theta_update}} \\
&= -\alpha \sum_{j=1}^n \left( \underbrace{\left . \frac{\partial \mathcal{L}_{\text{disc}} \left(\hat{\bs{\theta}}_{p}\right)}{\partial \hat{\bs{\theta}}_{p}} \right \vert_{\hat{\bs{\theta}}_{p} = \hat{\bs{\theta}}_{p}^{(t)} } \left . \frac{\partial \mathcal{L}^j_{\text{gen}} (\bs{\theta}_{p}) }{\partial \bs{\theta}_{p}} \right \vert_{\bs{\theta}_{p} = \bs{\theta}_{p}^{(t)}} ^\top}_{\triangleq d_j} \left .\frac{\partial w_j \left(\bs{\omega} \right)}{\partial \bs{\omega}}\right \vert_{\bs{\omega} = \bs{\omega}^{(t)}} \right) \\
\end{align*}
}
Therefore, 
{\small
$$
-\left . \frac{\partial \mathcal{L}_{\text{disc}} \left(\hat{\bs{\theta}}^{(t)}_{p}\left(\bs{\omega} \right)\right)}{\partial \bs{\omega}} \right \vert_{\bs{\omega} = \bs{\omega}^{(t)}} \propto \sum_{j=1}^n d_j \left .\frac{\partial w_j \left(\bs{\omega} \right)}{\partial \bs{\omega}}\right \vert_{\bs{\omega} = \bs{\omega}^{(t)}}, \,\,\,\, d_j = \left . \frac{\partial \mathcal{L}_{\text{disc}} \left(\hat{\bs{\theta}}_{p}\right)}{\partial \hat{\bs{\theta}}_{p}} \right \vert_{\hat{\bs{\theta}}_{p} = \hat{\bs{\theta}}_{p}^{(t)} } \left . \frac{\partial \mathcal{L}^j_{\text{gen}} (\bs{\theta}_{p}) }{\partial \bs{\theta}_{p}} \right \vert_{\bs{\theta}_{p} = \bs{\theta}_{p}^{(t)}} ^\top.
$$
}

\section{GLUE Tasks}
\label{app:glue}
We provide the details of the seven classification tasks included in the GLUE benchmark.

\textbf{MNLI:} Multi-genre Natural Language Inference~\citep{MNLI} requires predicting whether a given premise sentence entails, contradicts or neutral with respect to a given hypothesis sentence. 

\textbf{QQP:} Quora Question Pairs~\citep{QQP} requires judging whether a pair of questions asked are semantically equivalent.

\textbf{QNLI:} Question Natural Language Inference requires predicting whether a given sentence contains the answer to a given question sentence.

\textbf{SST-2:} Stanford Sentiment Treebank~\citep{SST-2} requires determining if a movie review has positive or negative sentiment. 

\textbf{CoLA:} Corpus of Linguistic Acceptability~\citep{COLA} requires determining whether a given sentence is linguistically acceptable or not. 

\textbf{RTE:} Recognizing Textual Entailment~\citep{RTE-5,RTE-1,RTE-3,RTE-2} requires predicting whether a given premise sentence entails a given hypothesis sentence or not.

\textbf{MRPC:} Microsoft Research Paraphrase Corpus~\citep{MRPC} requires predicting whether two sentences are semantically equivalent or not.





\section{Data Augmentation Baseline Details}
\label{app:aug_baselines}

\paragraph{Details About MixText~\citep{Chen2020MixTextLI}.}
We use the TMix version of MixText to perform data interpolation on the few-shot labeled dataset (since there is no access to unlabeled task-specific data under the strict few-shot learning setting~\cite{gao2021making}). 
% Note that under the strict few-shot learning setting~\cite{gao2021making}, there is no access to unlabeled task-specific data, so .
We adapt the label mix-up operation to fit prompt-based fine-tuning by interpolating the label words instead of categorical labels; we observe that this results in better few-shot performance than the original TMix, probably analogous to why prompt-based fine-tuning outperforms standard fine-tuning for few-shot learning.
We train the classifier with supervised loss combined with consistency loss over the interpolated samples as in the original paper.
We follow the default hyperparameters in MixText.


\paragraph{Details About Back Translation.}
We use two trained Marian~\citep{mariannmt} models to perform data augmentation via back translation. 
We translate our labeled examples from English to French, and then back to English. As in UDA~\citep{Xie2020UnsupervisedDA}, we 
employ random sampling with a tunable temperature to generate a diverse set of derivative examples. We generate $32$ 
examples from each few-shot training example and let the synthesized samples share the same label with the original few-shot training sample. 
After combining with the original examples,
we fine-tune the classifier and observe performance.


\paragraph{Details About GPT3Mix~\citep{Yoo2021GPT3MixLL}.}
\input{Tables/gpt3mix_prompt}
We use the $175$B GPT3 model for generating the augmentations. 
For creating each augmentation, we randomly sample $k=4$ (the optimal setting according to GPT3Mix) examples from the few-shot training set as demonstrations.
The prompts follow the suggested format proposed in the original paper~\citep{Yoo2021GPT3MixLL} and are shown in Table~\ref{tab:gpt3mix_prompt}.
We create $5,000$ augmented samples per label to make the resulting training set size equal to that of \method. After obtaining the augmented examples and their pseudo labels (the probability predictions over all labels by GPT3), we use them along with the real few-shot samples for fine-tuning the classifier, following the setting in GPT3Mix~\citep{Yoo2021GPT3MixLL}.

\paragraph{Details About Standard Generator Fine-Tuning.}
We fine-tune the same $1.6$B CTRL~\citep{Keskar2019CTRLAC} model as used in \method with the standard maximum likelihood objective. Different from previous studies~\citep{AnabyTavor2020DoNH,Kumar2020DataAU} that prepend categorical labels to the training samples, we enhance the generator fine-tuning with label-descriptive prompts (shown in Table~\ref{tab:full_prompts}) used in \method.
We create $5,000$ augmented samples per label to make the resulting training set size equal to that of \method.

% \input{Tables/weight_net_study}




% \section{Qualitative Analyses of Generator Training Objectives}\label{app:eval_gen}


% which can serve as quality training data.




\section{Concrete Generation Results}\label{app:gen_result}

We present some concrete generation results (from $\mathcal{D}_{\text{gen}}$) for all tasks in Tables~\ref{tab:case_sst2}, \ref{tab:case_cola}, \ref{tab:case_qqp}, \ref{tab:case_mnli}, \ref{tab:case_qnli}, \ref{tab:case_rte}, and \ref{tab:case_mrpc}.
To compare $\mathcal{D}_{\text{gen}}$ with $\mathcal{D}_{\text{train}}$, we also show the few-shot training samples ($\mathcal{D}_{\text{train}}$) of SST-2 in Table~\ref{tab:sst2_fewshot},

Comparing Tables~\ref{tab:case_sst2} with \ref{tab:sst2_fewshot}, it can be seen that the synthetic samples are accurate and quite different from the given training samples to serve as effective augmentations. 
For sequence-pair tasks, because we randomly sample the first sequence from the pretraining corpus and let the generator create the second sequence given certain labels, the resulting generated samples will be certainly different from the given training samples.

\input{Tables/app_case_study}

\input{Tables/app_sst2_kshot}