% \newcommand{\acc}{Acc. (\uparrow)}
% \newcommand{\ppl}{PPL (\downarrow)}

\begin{wraptable}[11]{wr}{0.5\textwidth}
\small
\centering
\caption{Study of weighting network instantiation. The default architecture is a feedforward network (FFN) with one hidden layer. We also explore adding a self-attention layer on top of the generator PLM's output hidden states (Self-attention). We use the same two metrics with Table~\ref{tab:gen_eval} for evaluation.}
\resizebox{0.5\textwidth}{!}{
\begin{tabular}{l*{4}{c}}
\toprule 
\multirow{2}{*}{\textbf{Architecture}} & \multicolumn{2}{c}{\textbf{MNLI}} & \multicolumn{2}{c}{\textbf{SST-2}} \\
& \acc & \ppl & \acc & \ppl \\
\midrule
FFN & $\textbf{72.3}$ & $\textbf{11.9}$ & $\textbf{93.2}$ & $\textbf{43.5}$ \\
Self-attention & $70.3$ & ${12.9}$ & ${92.3}$ & ${44.2}$ \\
\bottomrule
\end{tabular}
}
\label{tab:weight_net}
\end{wraptable}
