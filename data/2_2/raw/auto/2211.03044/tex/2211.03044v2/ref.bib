
@inproceedings{vaswani2017attention,
  title={Attention is all you need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  booktitle={NeurIPS},
  year={2017}
}

@inproceedings{devlin2019bert,
  title={{BERT}: Pre-training of Deep Bidirectional Transformers for Language Understanding},
  author={Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  booktitle={NAACL-HLT},
  year={2019}
}

@article{liu2019roberta,
  title={Ro{BERT}a: {A} Robustly Optimized {BERT} Pretraining Approach},
  author={Liu, Yinhan and Ott, Myle and Goyal, Naman and Du, Jingfei and Joshi, Mandar and Chen, Danqi and Levy, Omer and Lewis, Mike and Zettlemoyer, Luke and Stoyanov, Veselin},
  journal={arXiv preprint arXiv:1907.11692},
  year={2019}
}

@inproceedings{clark2020electra,
  title={{ELECTRA}: Pre-training Text Encoders as Discriminators Rather Than Generators},
  author={Clark, Kevin and Luong, Minh-Thang and Le, Quoc V and Manning, Christopher D},
  booktitle={ICLR},
  year={2020}
}

@inproceedings{meng2021coco,
  title={{COCO-LM}: Correcting and contrasting text sequences for language model pretraining},
  author={Meng, Yu and Xiong, Chenyan and Bajaj, Payal and Tiwary, Saurabh and Bennett, Paul and Han, Jiawei and Song, Xia},
  booktitle={NeurIPS},
  year={2021}
}

@inproceedings{Meng2022PretrainingTE,
  title={Pretraining Text Encoders with Adversarial Mixture of Training Signal Generators},
  author={Yu Meng and Chenyan Xiong and Payal Bajaj and Saurabh Tiwary and Paul Bennett and Jiawei Han and Xia Song},
  booktitle={ICLR},
  year={2022}
}

@inproceedings{Gao2022SelfGuidedND,
  title={Self-Guided Noise-Free Data Generation for Efficient Zero-Shot Learning},
  author={Jiahui Gao and Renjie Pi and Yong Lin and Hang Xu and Jiacheng Ye and Zhiyong Wu and Weizhong Zhang and Xiaodan Liang and Zhenguo Li and Lingpeng Kong},
  booktitle={ICLR},
  year={2023}
}

@inproceedings{he2020deberta,
  title={{DeBERTa}: Decoding-enhanced {BERT} with Disentangled Attention},
  author={He, Pengcheng and Liu, Xiaodong and Gao, Jianfeng and Chen, Weizhu},
  booktitle={ICLR},
  year={2021}
}

@article{radford2019language,
  title={Language models are unsupervised multitask learners},
  author={Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya},
  journal={OpenAI blog},
  volume={1},
  number={8},
  pages={9},
  year={2019}
}

@inproceedings{Brown2020LanguageMA,
  title={Language Models are Few-Shot Learners},
  author={Tom B. Brown and Benjamin Mann and Nick Ryder and Melanie Subbiah and Jared Kaplan and Prafulla Dhariwal and Arvind Neelakantan and Pranav Shyam and Girish Sastry and Amanda Askell and Sandhini Agarwal and Ariel Herbert-Voss and Gretchen Krueger and T. J. Henighan and Rewon Child and Aditya Ramesh and Daniel M. Ziegler and Jeff Wu and Clemens Winter and Christopher Hesse and Mark Chen and Eric Sigler and Mateusz Litwin and Scott Gray and Benjamin Chess and Jack Clark and Christopher Berner and Sam McCandlish and Alec Radford and Ilya Sutskever and Dario Amodei},
  booktitle={NeurIPS},
  year={2020}
}

@inproceedings{yang2019xlnet,
  title={{XLNet}: Generalized Autoregressive Pretraining for Language Understanding},
  author = {Yang, Zhilin and Dai, Zihang and Yang, Yiming and Carbonell, Jaime and Salakhutdinov, Russ R and Le, Quoc V},
  booktitle = {NeurIPS},
  year = {2019}
}

@article{Keskar2019CTRLAC,
  title={{CTRL}: A Conditional Transformer Language Model for Controllable Generation},
  author={Nitish Shirish Keskar and Bryan McCann and Lav R. Varshney and Caiming Xiong and Richard Socher},
  journal={ArXiv},
  year={2019},
  volume={abs/1909.05858}
}

@inproceedings{Holtzman2020TheCC,
  title={The Curious Case of Neural Text Degeneration},
  author={Ari Holtzman and Jan Buys and Maxwell Forbes and Yejin Choi},
  booktitle={ICLR},
  year={2020}
}

@inproceedings{Welleck2020NeuralTG,
  title={Neural Text Generation with Unlikelihood Training},
  author={Sean Welleck and Ilia Kulikov and Stephen Roller and Emily Dinan and Kyunghyun Cho and Jason Weston},
  booktitle={ICLR},
  year={2020}
}

@inproceedings{Chen2020MixTextLI,
  title={{MixText}: Linguistically-Informed Interpolation of Hidden Space for Semi-Supervised Text Classification},
  author={Jiaao Chen and Zichao Yang and Diyi Yang},
  booktitle={ACL},
  year={2020}
}

@inproceedings{Xie2020UnsupervisedDA,
  title={Unsupervised Data Augmentation for Consistency Training},
  author={Qizhe Xie and Zihang Dai and Eduard H. Hovy and Minh-Thang Luong and Quoc V. Le},
  booktitle={NeurIPS},
  year={2020}
}

@inproceedings{Miyato2017AdversarialTM,
  title={Adversarial Training Methods for Semi-Supervised Text Classification},
  author={Takeru Miyato and Andrew M. Dai and Ian J. Goodfellow},
  booktitle={ICLR},
  year={2017}
}

@inproceedings{gao2021making,
   title={Making Pre-trained Language Models Better Few-shot Learners},
   author={Gao, Tianyu and Fisch, Adam and Chen, Danqi},
   booktitle={ACL},
   year={2021}
}

@inproceedings{zhang2022differentiable,
    title={Differentiable Prompt Makes Pre-trained Language Models Better Few-shot Learners},
    author={Ningyu Zhang and Luoqiu Li and Xiang Chen and Shumin Deng and Zhen Bi and Chuanqi Tan and Fei Huang and Huajun Chen},
    booktitle={ICLR},
    year={2022}
}

@inproceedings{Gao2021SimCSESC,
  title={{SimCSE}: Simple Contrastive Learning of Sentence Embeddings},
  author={Tianyu Gao and Xingcheng Yao and Danqi Chen},
  booktitle={EMNLP},
  year={2021}
}

@inproceedings{Zhou2022FlipDAEA,
  title={{FlipDA}: Effective and Robust Data Augmentation for Few-Shot Learning},
  author={Jing Zhou and Yanan Zheng and Jie Tang and Jian Li and Zhilin Yang},
  booktitle={ACL},
  year={2022}
}

@inproceedings{Reimers2019SentenceBERTSE,
  title={{Sentence-BERT}: Sentence Embeddings using Siamese {BERT}-Networks},
  author={Nils Reimers and Iryna Gurevych},
  booktitle={EMNLP},
  year={2019}
}

@inproceedings{Schick2021ExploitingCF,
  title={Exploiting Cloze-Questions for Few-Shot Text Classification and Natural Language Inference},
  author={Timo Schick and Hinrich Sch{\"u}tze},
  booktitle={EACL},
  year={2021}
}

@inproceedings{Tam2021ImprovingAS,
  title={Improving and Simplifying Pattern Exploiting Training},
  author={Derek Tam and Rakesh R Menon and Mohit Bansal and Shashank Srivastava and Colin Raffel},
  booktitle={EMNLP},
  year={2021}
}

@inproceedings{Schick2021ItsNJ,
  title={It’s Not Just Size That Matters: Small Language Models Are Also Few-Shot Learners},
  author={Timo Schick and Hinrich Sch{\"u}tze},
  booktitle={NAACL},
  year={2021}
}

@inproceedings{Scao2021HowMD,
  title={How many data points is a prompt worth?},
  author={Teven Le Scao and Alexander M. Rush},
  booktitle={NAACL},
  year={2021}
}

@article{Liu2021GPTUT,
  title={{GPT} Understands, Too},
  author={Xiao Liu and Yanan Zheng and Zhengxiao Du and Ming Ding and Yujie Qian and Zhilin Yang and Jie Tang},
  journal={ArXiv},
  year={2021},
  volume={abs/2103.10385}
}


@article{logan2021cutting,
  title={Cutting down on prompts and parameters: Simple few-shot learning with language models},
  author={Logan IV, Robert L and Bala{\v{z}}evi{\'c}, Ivana and Wallace, Eric and Petroni, Fabio and Singh, Sameer and Riedel, Sebastian},
  journal={arXiv preprint arXiv:2106.13353},
  year={2021}
}


@article{raffel2019t5,
  title={Exploring the limits of transfer learning with a unified text-to-text transformer},
  author={Raffel, Colin and Shazeer, Noam and Roberts, Adam and Lee, Katherine and Narang, Sharan and Matena, Michael and Zhou, Yanqi and Li, Wei and Liu, Peter J},
  journal={Journal of Machine Learning Research},
  year={2019}
}

@inproceedings{Lewis2020BARTDS,
  title={{BART}: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension},
  author={Mike Lewis and Yinhan Liu and Naman Goyal and Marjan Ghazvininejad and Abdelrahman Mohamed and Omer Levy and Veselin Stoyanov and Luke Zettlemoyer},
  booktitle={ACL},
  year={2020}
}

@inproceedings{Schick2021FewShotTG,
  title={Few-Shot Text Generation with Natural Language Instructions},
  author={Timo Schick and Hinrich Sch{\"u}tze},
  booktitle={EMNLP},
  year={2021}
}

@inproceedings{Wei2022FinetunedLM,
  title={Finetuned Language Models Are Zero-Shot Learners},
  author={Jason Wei and Maarten Bosma and Vincent Zhao and Kelvin Guu and Adams Wei Yu and Brian Lester and Nan Du and Andrew M. Dai and Quoc V. Le},
  booktitle={ICLR},
  year={2022},
}

@article{Puri2019ZeroshotTC,
  title={Zero-shot Text Classification With Generative Language Models},
  author={Raul Puri and Bryan Catanzaro},
  journal={ArXiv},
  year={2019},
  volume={abs/1912.10165}
}

@article{mishra2021cross,
  title={Cross-task generalization via natural language crowdsourcing instructions},
  author={Mishra, Swaroop and Khashabi, Daniel and Baral, Chitta and Hajishirzi, Hannaneh},
  journal={arXiv preprint arXiv:2104.08773},
  year={2021}
}

@inproceedings{Yin2020UniversalNL,
  title={Universal Natural Language Processing with Limited Annotations: Try Few-shot Textual Entailment as a Start},
  author={Wenpeng Yin and Nazneen Rajani and Dragomir Radev and Richard Socher and Caiming Xiong},
  booktitle={EMNLP},
  year={2020}
}

@inproceedings{Yin2019BenchmarkingZT,
  title={Benchmarking Zero-shot Text Classification: Datasets, Evaluation and Entailment Approach},
  author={Wenpeng Yin and Jamaal Hay and Dan Roth},
  booktitle={EMNLP},
  year={2019}
}

@inproceedings{Ye2021CrossFitAF,
  title={CrossFit: A Few-shot Learning Challenge for Cross-task Generalization in NLP},
  author={Qinyuan Ye and Bill Yuchen Lin and Xiang Ren},
  booktitle={EMNLP},
  year={2021}
}

@inproceedings{Zhong2021AdaptingLM,
  title={Adapting Language Models for Zero-shot Learning by Meta-tuning on Dataset and Prompt Collections},
  author={Ruiqi Zhong and Kristy Lee and Zheng Zhang and Dan Klein},
  booktitle={EMNLP Findings},
  year={2021}
}

@inproceedings{wang2018glue,
  title={{GLUE}: A multi-task benchmark and analysis platform for natural language understanding},
  author={Wang, Alex and Singh, Amanpreet and Michael, Julian and Hill, Felix and Levy, Omer and Bowman, Samuel R},
  booktitle={EMNLP Workshop BlackboxNLP},
  year={2018}
}

@inproceedings{Wang2019SuperGLUEAS,
  title={{SuperGLUE}: A Stickier Benchmark for General-Purpose Language Understanding Systems},
  author={Alex Wang and Yada Pruksachatkun and Nikita Nangia and Amanpreet Singh and Julian Michael and Felix Hill and Omer Levy and Samuel R. Bowman},
  booktitle={NeurIPS},
  year={2019}
}

@inproceedings{Perez2021TrueFL,
  title={True Few-Shot Learning with Language Models},
  author={Ethan Perez and Douwe Kiela and Kyunghyun Cho},
  booktitle={NeurIPS},
  year={2021}
}

@incollection{torrey2010transfer,
  title={Transfer learning},
  author={Torrey, Lisa and Shavlik, Jude},
  booktitle={Handbook of research on machine learning applications and trends: algorithms, methods, and techniques},
  year={2010},
}

@article{zhuang2020comprehensive,
  title={A comprehensive survey on transfer learning},
  author={Zhuang, Fuzhen and Qi, Zhiyuan and Duan, Keyu and Xi, Dongbo and Zhu, Yongchun and Zhu, Hengshu and Xiong, Hui and He, Qing},
  journal={Proceedings of the IEEE},
  year={2020},
  publisher={IEEE}
}

@inproceedings{Szegedy2016RethinkingTI,
  title={Rethinking the Inception Architecture for Computer Vision},
  author={Christian Szegedy and Vincent Vanhoucke and Sergey Ioffe and Jonathon Shlens and Zbigniew Wojna},
  booktitle={CVPR},
  year={2016}
}

@inproceedings{Mller2019WhenDL,
  title={When Does Label Smoothing Help?},
  author={Rafael M{\"u}ller and Simon Kornblith and Geoffrey E. Hinton},
  booktitle={NeurIPS},
  year={2019}
}

@inproceedings{Lukasik2020DoesLS,
  title={Does label smoothing mitigate label noise?},
  author={Michal Lukasik and Srinadh Bhojanapalli and Aditya Krishna Menon and Surinder Kumar},
  booktitle={ICML},
  year={2020}
}

@inproceedings{Nguyen2020SELFLT,
  title={{SELF}: Learning to Filter Noisy Labels with Self-Ensembling},
  author={Duc Tam Nguyen and Chaithanya Kumar Mummadi and Thi-Phuong-Nhung Ngo and Thi Hoai Phuong Nguyen and Laura Beggel and Thomas Brox},
  booktitle={ICLR},
  year={2020}
}

@inproceedings{Laine2017TemporalEF,
  title={Temporal Ensembling for Semi-Supervised Learning},
  author={Samuli Laine and Timo Aila},
  booktitle={ICLR},
  year={2017}
}

@inproceedings{Zhang2017UnderstandingDL,
  title={Understanding deep learning requires rethinking generalization},
  author={Chiyuan Zhang and Samy Bengio and Moritz Hardt and Benjamin Recht and Oriol Vinyals},
  booktitle={ICLR},
  year={2017}
}

@inproceedings{Yuan2021BARTScoreEG,
  title={{BARTS}core: Evaluating Generated Text as Text Generation},
  author={Weizhe Yuan and Graham Neubig and Pengfei Liu},
  booktitle={NeurIPS},
  year={2021}
}

@inproceedings{Hu2017TowardCG,
  title={Toward Controlled Generation of Text},
  author={Zhiting Hu and Zichao Yang and Xiaodan Liang and Ruslan Salakhutdinov and Eric P. Xing},
  booktitle={ICML},
  year={2017}
}

@inproceedings{Dathathri2020PlugAP,
  title={Plug and Play Language Models: A Simple Approach to Controlled Text Generation},
  author={Sumanth Dathathri and Andrea Madotto and Janice Lan and Jane Hung and Eric Frank and Piero Molino and Jason Yosinski and Rosanne Liu},
  booktitle={ICLR},
  year={2020}
}

@inproceedings{Pascual2021APM,
  title={A Plug-and-Play Method for Controlled Text Generation},
  author={Damian Pascual and B{\'e}ni Egressy and Clara Meister and Ryan Cotterell and Roger Wattenhofer},
  booktitle={EMNLP Findings},
  year={2021}
}

@inproceedings{Liu2021DExpertsDC,
  title={{DExperts}: Decoding-Time Controlled Text Generation with Experts and Anti-Experts},
  author={Alisa Liu and Maarten Sap and Ximing Lu and Swabha Swayamdipta and Chandra Bhagavatula and Noah A. Smith and Yejin Choi},
  booktitle={ACL},
  year={2021}
}

@inproceedings{Yang2021FUDGECT,
  title={{FUDGE}: Controlled Text Generation With Future Discriminators},
  author={Kevin Yang and Dan Klein},
  booktitle={NAACL},
  year={2021}
}

@inproceedings{Kumar2021ControlledTG,
  title={Controlled Text Generation as Continuous Optimization with Multiple Constraints},
  author={Sachin Kumar and Eric Malmi and Aliaksei Severyn and Yulia Tsvetkov},
  booktitle={NeurIPS},
  year={2021}
}

@article{Ziegler2019FineTuningLM,
  title={Fine-Tuning Language Models from Human Preferences},
  author={Daniel M. Ziegler and Nisan Stiennon and Jeff Wu and Tom B. Brown and Alec Radford and Dario Amodei and Paul Christiano and Geoffrey Irving},
  journal={ArXiv},
  year={2019},
  volume={abs/1909.08593}
}

@inproceedings{Khalifa2021ADA,
  title={A Distributional Approach to Controlled Text Generation},
  author={Muhammad Khalifa and Hady ElSahar and Marc Dymetman},
  booktitle={ICLR},
  year={2021}
}

@inproceedings{Krause2021GeDiGD,
  title={{GeDi}: Generative Discriminator Guided Sequence Generation},
  author={Ben Krause and Akhilesh Deepak Gotmare and Bryan McCann and Nitish Shirish Keskar and Shafiq R. Joty and Richard Socher and Nazneen Rajani},
  booktitle={EMNLP},
  year={2021}
}

@inproceedings{Chan2021CoConAS,
  title={{CoCon}: A Self-Supervised Approach for Controlled Text Generation},
  author={Alvin Chan and Y. Ong and Bill Tuck Weng Pung and Aston Zhang and Jie Fu},
  booktitle={ICLR},
  year={2021}
}

@inproceedings{Li2021PrefixTuningOC,
  title={Prefix-Tuning: Optimizing Continuous Prompts for Generation},
  author={Xiang Lisa Li and Percy Liang},
  booktitle={ACL},
  year={2021}
}

@article{Xu2022ZeroPromptSP,
  title={{ZeroPrompt}: Scaling Prompt-Based Pretraining to 1, 000 Tasks Improves Zero-Shot Generalization},
  author={Hanwei Xu and Yujun Chen and Yulun Du and Nan Shao and Yanggang Wang and Haiyu Li and Zhilin Yang},
  journal={ArXiv},
  year={2022},
  volume={abs/2201.06910}
}

@article{Sanh2021MultitaskPT,
  title={Multitask Prompted Training Enables Zero-Shot Task Generalization},
  author={Victor Sanh and Albert Webson and Colin Raffel and Stephen H. Bach and Lintang A. Sutawika and Zaid Alyafeai and Antoine Chaffin and Arnaud Stiegler and Teven Le Scao and Arun Raja and Manan Dey and M SAIFUL BARI and Canwen Xu and Urmish Thakker and Shanya Sharma Sharma and Eliza Szczechla and Taewoon Kim and Gunjan Chhablani and Nihal V. Nayak and Debajyoti Datta and Jonathan Chang and Mike Tian-Jian Jiang and Han Wang and Matteo Manica and Sheng Shen and Zheng Xin Yong and Harshit Pandey and Rachel Bawden and Thomas Wang and Trishala Neeraj and Jos Rozen and Abheesht Sharma and Andrea Santilli and Thibault F{\'e}vry and Jason Alan Fries and Ryan Teehan and Stella Rose Biderman and Leo Gao and T. G. Owe Bers and Thomas Wolf and Alexander M. Rush},
  journal={ArXiv},
  year={2021},
  volume={abs/2110.08207}
}

@inproceedings{Meng2020TextCU,
  title={Text Classification Using Label Names Only: A Language Model Self-Training Approach},
  author={Yu Meng and Yunyi Zhang and Jiaxin Huang and Chenyan Xiong and Heng Ji and Chao Zhang and Jiawei Han},
  booktitle={EMNLP},
  year={2020}
}

@article{Hinton2015DistillingTK,
  title={Distilling the Knowledge in a Neural Network},
  author={Geoffrey E. Hinton and Oriol Vinyals and Jeffrey Dean},
  journal={ArXiv},
  year={2015},
  volume={abs/1503.02531}
}

@inproceedings{Bender2020ClimbingTN,
  title={Climbing towards {NLU}: On Meaning, Form, and Understanding in the Age of Data},
  author={Emily M. Bender and Alexander Koller},
  booktitle={ACL},
  year={2020}
}

@inproceedings{Bender2021OnTD,
  title={On the Dangers of Stochastic Parrots: Can Language Models Be Too Big?},
  author={Emily M. Bender and Timnit Gebru and Angelina McMillan-Major and Shmargaret Shmitchell},
  booktitle={ACM Conference on Fairness, Accountability, and Transparency},
  year={2021}
}


@inproceedings{Pagnoni2021UnderstandingFI,
  title={Understanding Factuality in Abstractive Summarization with {FRANK}: A Benchmark for Factuality Metrics},
  author={Artidoro Pagnoni and Vidhisha Balachandran and Yulia Tsvetkov},
  booktitle={NAACL},
  year={2021}
}

@inproceedings{Gehman2020RealToxicityPromptsEN,
  title={{RealToxicityPrompts}: Evaluating Neural Toxic Degeneration in Language Models},
  author={Samuel Gehman and Suchin Gururangan and Maarten Sap and Yejin Choi and Noah A. Smith},
  booktitle={EMNLP Findings},
  year={2020}
}

@inproceedings{Min2022RethinkingTR,
  title={Rethinking the Role of Demonstrations: What Makes In-Context Learning Work?},
  author={Sewon Min and Xinxi Lyu and Ari Holtzman and Mikel Artetxe and Mike Lewis and Hannaneh Hajishirzi and Luke Zettlemoyer},
  booktitle={EMNLP},
  year={2022},
}

@inproceedings{Prabhumoye2018StyleTT,
  title={Style Transfer Through Back-Translation},
  author={Shrimai Prabhumoye and Yulia Tsvetkov and Ruslan Salakhutdinov and Alan W. Black},
  booktitle={ACL},
  year={2018}
}

@inproceedings{Ma2020PowerTransformerUC,
  title={{PowerTransformer}: Unsupervised Controllable Revision for Biased Language Correction},
  author={Xinyao Ma and Maarten Sap and Hannah Rashkin and Yejin Choi},
  booktitle={EMNLP},
  year={2020}
}

@inproceedings{MNLI,
  title={A Broad-Coverage Challenge Corpus for Sentence Understanding through Inference},
  author={Williams, Adina and Nangia, Nikita and Bowman, Samuel},
  booktitle={NAACL-HLT},
  year={2018}
}

@misc{QQP,
    title={First {Quora} Dataset Release: Question Pairs},
    author={Iyer Shankar and Dandekar Nikhil and Csernai  Kornél},
    year={2017},
    URL={https://www.quora.com/q/quoradata/First-Quora-Dataset-Release-Question-Pairs},
}

@inproceedings{rajpurkar2016squad,
  title={{SQuAD}: 100,000+ Questions for Machine Comprehension of Text},
  author={Rajpurkar, Pranav and Zhang, Jian and Lopyrev, Konstantin and Liang, Percy},
  booktitle={EMNLP},
  year={2016}
}

@inproceedings{SST-2,
  title={Recursive deep models for semantic compositionality over a sentiment treebank},
  author={Socher, Richard and Perelygin, Alex and Wu, Jean and Chuang, Jason and Manning, Christopher D and Ng, Andrew Y and Potts, Christopher},
  booktitle={EMNLP},
  year={2013}
}

@inproceedings{COLA,
  title={Neural network acceptability judgments},
  author={Warstadt, Alex and Singh, Amanpreet and Bowman, Samuel R},
  booktitle={TACL},
  year={2019},
}

@inproceedings{RTE-1,
  title={The PASCAL recognising textual entailment challenge},
  author={Dagan, Ido and Glickman, Oren and Magnini, Bernardo},
  booktitle={Machine Learning Challenges Workshop},
  year={2005},
}

@inproceedings{RTE-2,
  title={The second pascal recognising textual entailment challenge},
  author={Haim, R Bar and Dagan, Ido and Dolan, Bill and Ferro, Lisa and Giampiccolo, Danilo and Magnini, Bernardo and Szpektor, Idan},
  booktitle={PASCAL Challenges Workshop on Recognising Textual Entailment},
  year={2006}
}

@inproceedings{RTE-3,
  title={The third pascal recognizing textual entailment challenge},
  author={Giampiccolo, Danilo and Magnini, Bernardo and Dagan, Ido and Dolan, Bill},
  booktitle={ACL-PASCAL workshop on textual entailment and paraphrasing},
  year={2007}
}
@inproceedings{RTE-5,
  title={The Fifth PASCAL Recognizing Textual Entailment Challenge},
  author={Bentivogli, Luisa and Clark, Peter and Dagan, Ido and Giampiccolo, Danilo},
  booktitle={TAC},
  year={2009}
}

@inproceedings{MRPC,
  title={Automatically constructing a corpus of sentential paraphrases},
  author={Dolan, William B and Brockett, Chris},
  booktitle={International Workshop on Paraphrasing (IWP)},
  year={2005}
}

@misc{Gokaslan2019OpenWeb,  
	title={{OpenWebText} Corpus},
	author={Aaron Gokaslan and Vanya Cohen},
	howpublished={\url{http://Skylion007.github.io/OpenWebTextCorpus}}, 
	year={2019}
}

@inproceedings{Meng2018WeaklySupervisedNT,
  title={Weakly-Supervised Neural Text Classification},
  author={Yu Meng and Jiaming Shen and Chao Zhang and Jiawei Han},
  booktitle={CIKM},
  year={2018}
}

@inproceedings{Meng2019WeaklySupervisedHT,
  title={Weakly-Supervised Hierarchical Text Classification},
  author={Yu Meng and Jiaming Shen and Chao Zhang and Jiawei Han},
  booktitle={AAAI},
  year={2019}
}

@inproceedings{Lester2021ThePO,
  title={The Power of Scale for Parameter-Efficient Prompt Tuning},
  author={Brian Lester and Rami Al-Rfou and Noah Constant},
  booktitle={EMNLP},
  year={2021}
}

@inproceedings{Shin2020ElicitingKF,
  title={Eliciting Knowledge from Language Models Using Automatically Generated Prompts},
  author={Taylor Shin and Yasaman Razeghi and Robert L Logan IV and Eric Wallace and Sameer Singh},
  booktitle={EMNLP},
  year={2020}
}

@inproceedings{Zhong2021FactualPI,
  title={Factual Probing Is [MASK]: Learning vs. Learning to Recall},
  author={Zexuan Zhong and Dan Friedman and Danqi Chen},
  booktitle={NAACL},
  year={2021}
}

@inproceedings{Shu2019MetaWeightNetLA,
  title={Meta-Weight-Net: Learning an Explicit Mapping For Sample Weighting},
  author={Jun Shu and Qi Xie and Lixuan Yi and Qian Zhao and Sanping Zhou and Zongben Xu and Deyu Meng},
  booktitle={NeurIPS},
  year={2019}
}

@article{kirkpatrick2017overcoming,
  title={Overcoming catastrophic forgetting in neural networks},
  author={Kirkpatrick, James and Pascanu, Razvan and Rabinowitz, Neil and Veness, Joel and Desjardins, Guillaume and Rusu, Andrei A and Milan, Kieran and Quan, John and Ramalho, Tiago and Grabska-Barwinska, Agnieszka and others},
  journal={Proceedings of the national academy of sciences},
  year={2017}
}

@inproceedings{Zhao2021CalibrateBU,
  title={Calibrate Before Use: Improving Few-Shot Performance of Language Models},
  author={Tony Zhao and Eric Wallace and Shi Feng and Dan Klein and Sameer Singh},
  booktitle={ICML},
  year={2021}
}

@inproceedings{baum1988size,
  title={What size net gives valid generalization?},
  author={Baum, Eric and Haussler, David},
  booktitle={NIPS},
  year={1988}
}

@inproceedings{Cui2022PrototypicalVF,
  title={Prototypical Verbalizer for Prompt-based Few-shot Tuning},
  author={Ganqu Cui and Shengding Hu and Ning Ding and Longtao Huang and Zhiyuan Liu},
  booktitle={ACL},
  year={2022}
}

@article{Wang2021TowardsZL,
  title={Towards Zero-Label Language Learning},
  author={Zirui Wang and Adams Wei Yu and Orhan Firat and Yuan Cao},
  journal={ArXiv},
  year={2021},
  volume={abs/2109.09193}
}

@inproceedings{Schick2021GeneratingDW,
  title={Generating Datasets with Pretrained Language Models},
  author={Timo Schick and Hinrich Sch{\"u}tze},
  booktitle={EMNLP},
  year={2021}
}

@inproceedings{Yang2020GDAugGD,
  title={G-DAug: Generative Data Augmentation for Commonsense Reasoning},
  author={Yiben Yang and Chaitanya Malaviya and Jared Fernandez and Swabha Swayamdipta and Ronan Le Bras and Ji-ping Wang and Chandra Bhagavatula and Yejin Choi and Doug Downey},
  booktitle={EMNLP Findings},
  year={2020}
}

@inproceedings{AnabyTavor2020DoNH,
  title={Do Not Have Enough Data? Deep Learning to the Rescue!},
  author={Ateret Anaby-Tavor and Boaz Carmeli and Esther Goldbraich and Amir Kantor and George Kour and Segev Shlomov and N. Tepper and Naama Zwerdling},
  booktitle={AAAI},
  year={2020}
}

@inproceedings{Meng2022GeneratingTD,
  title={Generating Training Data with Language Models: Towards Zero-Shot Language Understanding},
  author={Yu Meng and Jiaxin Huang and Yu Zhang and Jiawei Han},
  booktitle={NeurIPS},
  year={2022},
}

@inproceedings{Meng2021DistantlySupervisedNE,
  title={Distantly-Supervised Named Entity Recognition with Noise-Robust Learning and Language Model Augmented Self-Training},
  author={Yu Meng and Yunyi Zhang and Jiaxin Huang and Xuan Wang and Yu Zhang and Heng Ji and Jiawei Han},
  booktitle={EMNLP},
  year={2021}
}

@article{Huang2022LargeLM,
  title={Large Language Models Can Self-Improve},
  author={Jiaxin Huang and Shixiang Shane Gu and Le Hou and Yuexin Wu and Xuezhi Wang and Hongkun Yu and Jiawei Han},
  journal={ArXiv},
  year={2022},
  volume={abs/2210.11610}
}

@inproceedings{Ye2022ZeroGenEZ,
  title={{ZeroGen}: Efficient Zero-shot Learning via Dataset Generation},
  author={Jiacheng Ye and Jiahui Gao and Qintong Li and Hang Xu and Jiangtao Feng and Zhiyong Wu and Tao Yu and Lingpeng Kong},
  booktitle={EMNLP},
  year={2022}
}

@inproceedings{Wang2017LearningTM,
  title={Learning to Model the Tail},
  author={Yu-Xiong Wang and Deva Ramanan and Martial Hebert},
  booktitle={NIPS},
  year={2017}
}

@inproceedings{Hendrycks2018UsingTD,
  title={Using Trusted Data to Train Deep Networks on Labels Corrupted by Severe Noise},
  author={Dan Hendrycks and Mantas Mazeika and Duncan Wilson and Kevin Gimpel},
  booktitle={NeurIPS},
  year={2018}
}

@inproceedings{Finn2017ModelAgnosticMF,
  title={Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks},
  author={Chelsea Finn and P. Abbeel and Sergey Levine},
  booktitle={ICML},
  year={2017}
}

@inproceedings{Ren2018LearningTR,
  title={Learning to Reweight Examples for Robust Deep Learning},
  author={Mengye Ren and Wenyuan Zeng and Binh Yang and Raquel Urtasun},
  booktitle={ICML},
  year={2018}
}

@inproceedings{Liu2022WhatMG,
  title={What Makes Good In-Context Examples for {GPT-3}?},
  author={Jiachang Liu and Dinghan Shen and Yizhe Zhang and Bill Dolan and Lawrence Carin and Weizhu Chen},
  booktitle={Proceedings of Deep Learning Inside Out},
  year={2022}
}

@inproceedings{Andrychowicz2016LearningTL,
  title={Learning to learn by gradient descent by gradient descent},
  author={Marcin Andrychowicz and Misha Denil and Sergio Gomez Colmenarejo and Matthew W. Hoffman and David Pfau and Tom Schaul and Nando de Freitas},
  booktitle={NIPS},
  year={2016}
}

@inproceedings{Franceschi2018BilevelPF,
  title={Bilevel Programming for Hyperparameter Optimization and Meta-Learning},
  author={Luca Franceschi and Paolo Frasconi and Saverio Salzo and Riccardo Grazzi and Massimiliano Pontil},
  booktitle={ICML},
  year={2018}
}

@inproceedings{Wu2018LearningTT,
  title={Learning to Teach with Dynamic Loss Functions},
  author={Lijun Wu and Fei Tian and Yingce Xia and Yang Fan and Tao Qin and Jianhuang Lai and Tie-Yan Liu},
  booktitle={NeurIPS},
  year={2018}
}


@inproceedings{Min2022NoisyCL,
  title={Noisy Channel Language Model Prompting for Few-Shot Text Classification},
  author={Sewon Min and Michael Lewis and Hannaneh Hajishirzi and Luke Zettlemoyer},
  booktitle={ACL},
  year={2022},
}

@inproceedings{Hambardzumyan2021WARPWA,
  title={{WARP}: Word-level Adversarial ReProgramming},
  author={Karen Hambardzumyan and H. Khachatrian and Jonathan May},
  booktitle={ACL},
  year={2021}
}

@inproceedings{Hu2022KnowledgeablePI,
  title={Knowledgeable Prompt-tuning: Incorporating Knowledge into Prompt Verbalizer for Text Classification},
  author={Shengding Hu and Ning Ding and Huadong Wang and Zhiyuan Liu and Juan-Zi Li and Maosong Sun},
  booktitle={ACL},
  year={2022},
}

@inproceedings{Yoo2021GPT3MixLL,
  title={{GPT3Mix}: Leveraging Large-scale Language Models for Text Augmentation},
  author={Kang Min Yoo and Do-Hyoung Park and Jaewoo Kang and Sang-Woo Lee and Woomyeong Park},
  booktitle={EMNLP Findings},
  year={2021}
}

@inproceedings{Kumar2020DataAU,
  title={Data Augmentation using Pre-trained Transformer Models},
  author={Varun Kumar and Ashutosh Choudhary and Eunah Cho},
  booktitle={Workshop on Life-long Learning for Spoken Language Systems},
  year={2020}
}

@article{lee2021neural,
  title={Neural data augmentation via example extrapolation},
  author={Lee, Kenton and Guu, Kelvin and He, Luheng and Dozat, Tim and Chung, Hyung Won},
  journal={arXiv preprint arXiv:2102.01335},
  year={2021}
}

@inproceedings{Wei2019EDAED,
  title={{EDA}: Easy Data Augmentation Techniques for Boosting Performance on Text Classification Tasks},
  author={Jason Wei and Kai Zou},
  booktitle={EMNLP},
  year={2019}
}

@inproceedings{mariannmt,
  title={Marian: Fast Neural Machine Translation in {C++}},
  author={Marcin Junczys-Dowmunt and Roman Grundkiewicz and Tomasz Dwojak and Hieu T. Hoang and Kenneth Heafield and Tom Neckermann and Frank Seide and Ulrich Germann and Alham Fikri Aji and Nikolay Bogoychev and Andr{\'e} F. T. Martins and Alexandra Birch},
  booktitle={ACL System Demo},
  year={2018}
}

@inproceedings{Liu2022FewShotPF,
  title={Few-Shot Parameter-Efficient Fine-Tuning is Better and Cheaper than In-Context Learning},
  author={Haokun Liu and Derek Tam and Mohammed Muqeeth and Jay Mohta and Tenghao Huang and Mohit Bansal and Colin Raffel},
  booktitle={NeurIPS},
  year={2022}
}

@inproceedings{Kingma2015AdamAM,
  title={Adam: A Method for Stochastic Optimization},
  author={Diederik P. Kingma and Jimmy Ba},
  booktitle={ICLR},
  year={2015}
}

@inproceedings{Standley2019WhichTS,
  title={Which Tasks Should Be Learned Together in Multi-task Learning?},
  author={Trevor Scott Standley and Amir Roshan Zamir and Dawn Chen and Leonidas J. Guibas and Jitendra Malik and Silvio Savarese},
  booktitle={ICML},
  year={2019}
}
