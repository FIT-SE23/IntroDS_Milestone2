\pdfoutput=1
\documentclass[11pt]{article}
\usepackage[]{emnlp2021}

\usepackage{times}
\usepackage{latexsym}
\usepackage{helvet}
\usepackage{courier}
\frenchspacing
\setlength{\pdfpagewidth}{8.5in}
\setlength{\pdfpageheight}{11in}
\pdfinfo{
/Title (Technical Report on Web-based Visual Corpus for Visual Document Understanding)
/Author (Donghyun Kim, Teakgyu Hong, Moonbin Yim, Yoonsik Kim, Geewook Kim)}

\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}

\usepackage{url}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{booktabs}
\usepackage{caption}
\usepackage{subcaption,blindtext}
\usepackage{multirow}
\usepackage{graphics}
\usepackage{pifont}
\usepackage{color,colortbl}
\usepackage{enumitem}
\usepackage{adjustbox}
\usepackage{placeins}
\input{math_commands}
\usepackage{cleveref}
\usepackage{algorithm}
\usepackage{algcompatible}
\usepackage{amsmath}


\newcommand\blfootnote[1]{%
  \begingroup
  \renewcommand\thefootnote{}\footnote{\\ \quad #1}%
  \addtocounter{footnote}{-1}%
  \endgroup
}

\title{Technical Report on Web-based Visual Corpus Construction\\for Visual Document Understanding}

\author{Donghyun Kim$^1$, Teakgyu Hong$^2$\thanks{\ \ This work is done at NAVER CLOVA.}, Moonbin Yim$^1$, Yoonsik Kim$^1$, Geewook Kim$^{1}$\thanks{\ \ Correspondence to \url{gwkim.rsrch@gmail.com}}\vspace{1.0em} \\
$^1$NAVER CLOVA \:\: $^2$Upstage AI\\
\tt\small\{artit.anthony, new.tghong, moonbings, terryoo528, gwkim.rsrch\}@gmail.com
}
\begin{document}
\maketitle
\begin{abstract}
We present a dataset generator engine named \textit{Web-based Visual Corpus Builder} (Webvicob).
Webvicob can readily construct a large-scale visual corpus (i.e., images with text annotations) from a raw Wikipedia HTML dump.
In this report, we validate that Webvicob-generated data can cover a wide range of context and knowledge
and helps practitioners to build a powerful Visual Document Understanding (VDU) backbone.
The proposed engine is publicly available at \url{https://github.com/clovaai/webvicob}.
\end{abstract}

\section{Introduction}
Language modeling has been a long-standing fundamental task in natural language processing (NLP). The trained language models (LMs) are utilized in a range of downstream NLP applications, such as information extraction (IE)~\cite{hwang2019pot} and question answering (QA)~\cite{devlinBERT2018}. To build a powerful LM, recent methods utilize large-scale text corpus at the pretraining phase.
The text corpus is generally constructed with a specialized engine or software.
For example, WikiExtractor~\cite{Wikiextractor2015} extracts texts from Wikipedia HTML dumps and builds a clean text corpus.


\begin{figure}[ht!]
    \centering
    \begin{minipage}{\linewidth}
        \centering
        \begin{subfigure}{0.4\textwidth}
            \captionsetup{oneside,margin={0cm,0cm},font=tiny}
            \includegraphics[width=\textwidth]{figure/sample_Webvicob.jpg}
            \subcaption*{Webvicob}
        \end{subfigure}\hspace{7mm}
        \begin{subfigure}{0.4\textwidth}
            \captionsetup{oneside,margin={0cm,0cm},font=tiny}
            \includegraphics[width=\textwidth]{figure/sample_iit.jpg}
            \subcaption*{IIT-CDIP}
        \end{subfigure} \\[3mm]
        \begin{subfigure}{0.4\textwidth}
            \captionsetup{oneside,margin={0cm,0cm},font=tiny}
            \includegraphics[width=\textwidth]{figure/sample_docvqa_t1.png}
            \subcaption*{DocVQA Task 1}
        \end{subfigure}\hspace{7mm}
        \begin{subfigure}{0.4\textwidth}
            \captionsetup{oneside,margin={0cm,0cm},font=tiny}
            \includegraphics[width=\textwidth]{figure/sample_docvqa_t3.png}
            \subcaption*{DocVQA Task 3}
        \end{subfigure}

        \subcaption{Document image samples.}
        \label{fig:1a}
    \end{minipage}
    \begin{minipage}{\linewidth}
        \centering
        \begin{subfigure}{0.24\textwidth}
            \captionsetup{oneside,margin={0.4cm,0cm},font=tiny}
            \includegraphics[width=\textwidth]{figure/pca/4_5_Webvicob.png}
            \subcaption*{Webvicob}
        \end{subfigure}
        \begin{subfigure}{0.24\textwidth}
            \captionsetup{oneside,margin={0.4cm,0cm},font=tiny}
            \includegraphics[width=\textwidth]{figure/pca/4_5_iit.png}
            \subcaption*{IIT-CDIP}
        \end{subfigure}
        \begin{subfigure}{0.24\textwidth}
            \captionsetup{oneside,margin={0.4cm,0cm},font=tiny}
            \includegraphics[width=\textwidth]{figure/pca/4_5_vqa_t1.png}
            \subcaption*{DocVQA Task 1}
        \end{subfigure}
        \begin{subfigure}{0.24\textwidth}
            \captionsetup{oneside,margin={0.4cm,0cm},font=tiny}
            \includegraphics[width=\textwidth]{figure/pca/4_5_vqa_t3.png}
            \subcaption*{DocVQA Task 3}
        \end{subfigure}

        \subcaption{PCA visualization.}
        \label{fig:1b}
    \end{minipage}
    \begin{minipage}{\linewidth}
        \centering
        \includegraphics[width=0.49\linewidth]{figure/fig1_docvqa_t1_pp.png}
        \includegraphics[width=0.49\linewidth]{figure/fig1_docvqa_t3_pp.png}
        \subcaption{Document VQA benchmarks.}
        \label{fig:1c}
    \end{minipage}
    \caption{\textbf{Image Samples and Visualization of Main Results.} (a) Samples of Webvicob, IIT-CDIP~\protect\cite{iitcdip}, DocVQA Task 1~\protect\cite{icdar21docvqa}, and Task 3~\protect\cite{mathew2022infographicvqa} are shown, respectively. (b) Visualization of principal component analysis (PCA) with bag of words (BoW) representations. (c) Results on two benchmarks with pretrained BROS models are shown. As can be seen in (a) and (b), DocVQA Task 1 has a similar distribution to IIT-CDIP, while Task 3 does not. This also affects the final score.}
    \label{fig:1}
\end{figure}
\begin{figure*}[ht!]
    \centering
    \includegraphics[width=0.95\textwidth]{figure/fig2_2.png}
    \caption{\textbf{A sample of Webvicob dataset.} The dataset contains large-scale web document images with hierarchical text annotations (i.e., character, word, line, and paragraph-level annotations). If box contains LaTeX~\citep{latex} (i.e., math formula), we set ``{is\_latex}'' value as True.}
    \label{fig:2}
\end{figure*}



Visual Document Understanding (VDU)~\cite{xu2019_layoutLM,hong2021bros,kim2022donut} has been developed to conduct a wide range of real-world tasks on document images.
For example, Document Parsing~\citep{xu2019_layoutLM,kim2022donut} aims to extract some key texts from a document image~\cite{hwang2019pot,hwang2021costeffective}.


Most recent VDU backbones can be regarded as an extension of LM.

Inspired by recent advances in LMs~\cite{vaswani2017transformer,devlinBERT2018},
recent VDU methods share a similar approach that (1) it first collects large-scale real document images, (2) conducts OCR on the images to extract texts, and (3) trains a BERT-like LM backbone on the extracted texts~\cite{xu2019_layoutLM,xu2021layoutlmv2,hong2021bros,huang2022layoutlmv3}.



Although conventional VDU methods have shown promising results, several practical challenges exist, especially in the training dataset preparation phase.
Most recent works~\cite{xu2019_layoutLM,hong2021bros} rely on a large-scale real image dataset. For example, IIT-CDIP dataset~\cite{iitcdip} consists of 11M industrial document images and is utilized in a range of VDU works.
However, in most low-resourced languages, there is no public dataset like IIT-CDIP. 
In addition, off-the-shelf OCR engines (e.g., CLOVA OCR API,\footnote{\scriptsize\label{clova_ocr_note}\url{https://clova.ai/ocr}} MS Read API,\footnote{\scriptsize\url{https://docs.microsoft.com/en-us/azure/cognitive-services/computer-vision}} Amazon Textract\footnote{\scriptsize\url{https://aws.amazon.com/ko/textract}}) are required to extract texts in the pre-processing.
This often requires enormous costs.



Moreover, the use of OCR raises other negative ramifications; the constructed data is strongly tied to the OCR engine, and OCR errors are propagated throughout the process. This problem becomes severe, especially in some non-Latin languages such as Korean and Japanese, where OCR is known to be complicated.




In this work, we propose an engine for building a web-based visual corpus. The proposed Web-based Visual Corpus Builder (Webvicob) renders a web page into an image file and generates rich text annotations (See Figure~\ref{fig:2}). With expressive Document Object Model (DOM) APIs, Webvicob produces accurate bounding boxes for all characters.


As can be seen in Figure~\ref{fig:1}, Webvicob covers a wide range of word contexts.
More visualization results are also available in Appendix~\ref{appendix:C}.
Compared to the traditional model trained on IIT-CDIP, the Webvicob-trained model shows a comparable result on DocVQA Task 1 and a higher score\footnote{\scriptsize\url{https://rrc.cvc.uab.es/?ch=17&com=evaluation&task=3}} on Task 3, showing the effectiveness of the de-biased Webvicob-based visual corpora.



The proposed method is simple yet effective. We show that the Webvicob-generated corpus can be critical in building a powerful VDU backbone.
Through extensive experiments and analysis, we show the effectiveness of Webvicob.
The contributions of this work can be summarized as follows:
\begin{enumerate}
    \item We propose Webvicob, which can be used for pretraining the visual document understanding models. 
    \item We conduct extensive experiments to verify the effectiveness of the proposed engine and dataset.
    \item The source code of our engine is publicly available for reproducibility and enhancement.\footnote{\scriptsize\label{Webvicob_github}\url{https://github.com/clovaai/webvicob}}
\end{enumerate}


\begin{table*}[t]
\centering
\resizebox{1.0\textwidth}{!}{%
    \begin{tabular}{crrrrrrrrl}
    \toprule
    \multirow{2}{*}{Dataset} & \multicolumn{3}{c}{\#Image} & \multicolumn{1}{c}{} & \multicolumn{4}{c}{Annotation Level} & \multirow{2}{*}{Language Supports} \\ \cline{2-9}
     & \multicolumn{1}{c}{Train} & \multicolumn{1}{c}{Val} & \multicolumn{1}{c}{Test} & \multicolumn{1}{c}{} & \multicolumn{1}{c}{Char} & \multicolumn{1}{c}{Word} & \multicolumn{1}{c}{Line} & \multicolumn{1}{c}{Para} &  \\ \toprule
    MSRA-TD500~\shortcite{msratd500} & 300 & 0 & 200 & &  &  & \ding{51} &  & EN \\
    IC15~\shortcite{ic15} & 1,000 & 0 & 500 & &  & \ding{51} &  &  & EN \\
    CTW-1500~\shortcite{ctw1500} & 1,000 & 0 & 500 & &  &  & \ding{51} &  & EN \\
    IC17 MLT~\shortcite{ic17mlt} & 7,200 & 1,800 & 9,000 & &  & \ding{51} &  &  & EN ZH JA KO BN AR \\

    IC19 MLT~\shortcite{ic19mlt} & 10,000 & 0 & 10,000 & &  & \ding{51} &  &  & EN ZH JA KO BN AR HI \\

    IC19 LSVT~\shortcite{ic19lsvt} & 30,000 & 0 & 20,000 & &  &  & \ding{51} &  & EN ZH \\
    IC19 ArT~\shortcite{ic19art} & 5,603 & 0 & 4,563 & &  & \ding{51} &  &  & EN ZH \\
    Total-Text~\shortcite{totaltext} & 1,255 & 0 & 300 & &  & \ding{51} &  &  & EN ZH \\
    TextOCR~\shortcite{textocr} & 21,778 & 3,124 & 3,232 & &  & \ding{51} &  &  & EN \\
    IntelOCR~\shortcite{intelocr} & 191,059 & 16,731 & 0 & &  & \ding{51} &  &  & EN \\
    HierText~\shortcite{hiertext} & 8,281 & 1,724 & 1,634 & &  & \ding{51} & \ding{51} & \ding{51} & EN \\ \hline
    SynthText~\shortcite{synthtext} & 858,750 & 0 & 0 & & \ding{51} & \ding{51} &  &  & EN \\ \hline
    IIT-CDIP~\shortcite{iitcdip} & 11,434,146 & 0 & 0 & &  &  &  &  & EN \\
    OCR-IDL~\shortcite{OCR_IDL} & 26,600,964 & 0 & 0 & &  & \ding{51} & \ding{51} &  & EN \\ \hline
    Webvicob$^\dagger$ & 18,584,173 & 4000 & 4000 & & \ding{51} & \ding{51} & \ding{51} & \ding{51} & EN ZH JA ES FR PT IT DE \\

    Webvicob$^\ddagger$ & \multicolumn{3}{c}{59,102,975} & & \ding{51} & \ding{51} & \ding{51} & \ding{51} & 270+ languages \\ \bottomrule
    \end{tabular}%
}
\caption{\textbf{Statistics of Webvicob and conventional datasets.} ISO 639-1 language code is used for Language Support. Webvicob$^\dagger$ is used in our multilingual experiments (Section~\ref{sec:multi}). Webvicob$^\ddagger$ is a virtual dataset that can be constructed with Webvicob. It contains more than 59M samples with rich annotations and 270+ language supports. The total number of Wikipedia articles (59M) can be found on the Wikipedia statistic webpage.\protect\footnotemark}
\label{table1}
\end{table*}

\section{Background and Related Work}
In this section, we introduce a traditional VDU pipeline and datasets. Most of the current methods share a similar approach of (1) collecting large-scale real images, (2) conducting OCR on the images, and (3) training a BERT-like backbone on the extracted texts~\cite{xu2019_layoutLM,hong2021bros}.

\subsection{VDU Backbones}
Inspired by BERT~\cite{devlinBERT2018} and the recent advancements in language modeling,
a range of BERT-like Transformer-based VDU backbones has been proposed~\citep{xu2019_layoutLM,hong2021bros,xu2021layoutlmv2,huang2022layoutlmv3,DOM_LM}.
For handling layout information of document images, 
spatial coordinates of OCR text boxes are fed to the VDU backbone~\cite{xu2019_layoutLM,hong2021bros,layoutxlm}.
Using visual encoders like ResNet~\citep{HeZRS16}, visual features of an input image are also being incorporated into the recent VDU backbones~\cite{xu2021layoutlmv2,huang2022layoutlmv3}. More recently, with the advances in Vision Transformer (ViT)~\citep{dosovitskiy2020vit}, training a Transformer encoder-decoder VDU backbone without OCR has also been attempted~\cite{kim2022donut,Dessurt,pix2struct}.
Our engine can be used together in various VDU backbones. In this paper, we verified the performance of the Webvicob engine by pretraining BROS and LayoutXLM.

\footnotetext{\scriptsize\url{https://en.wikipedia.org/wiki/List_of_Wikipedias}}  %

\subsection{Visual Corpus Construction for VDU}
Most existing OCR annotated datasets have small sizes, leading to difficulties in training VDU backbones.
To construct a rich corpus, in the traditional pipeline, large-scale real-world document images (e.g., IIT-CDIP) and an OCR engine (e.g., CLOVA OCR API~\textsuperscript{\ref{clova_ocr_note}}), are used.

The quality of the OCR engine significantly affects the downstream processes~\cite{kim2022donut, Dessurt}.
Hence, there have been difficulties in training and testing the VDU backbone. For example, since BROS~\citep{hong2021bros} and LayoutLM~\citep{layoutxlm} use different in-house OCRs, it has been difficult to make a fair comparison.


LayoutXLM~\cite{layoutxlm} collects large-scale digital-born PDF data from the world wide web and extracts text annotations from the PDF via an open-source PDF renderer.\footnote{\scriptsize\label{pymupdf}\url{https://github.com/pymupdf/PyMuPDF}}
Although this showed another promising direction, it is not easy to follow the pipeline in practice. A practitioner has to collect the PDF data as there is no publicly available dataset (\citet{layoutxlm} did not open-source the dataset).

Moreover, since pdf files cannot easily be editable, augmentation is limited.
Unlike the existing approach, Webvicob can efficiently modify and augment data (i.e., layout, background image) with javascript as Webvicob renders HTML directly.
Moreover, Webvicob can easily be incorporated with the HTML dumps (e.g., Wikipedia dumps\footnote{\scriptsize\url{https://dumps.wikimedia.org}}), which are easily accessible and have been widely used in building powerful NLP backbones~\citep{devlinBERT2018}.

\section{Web-based Visual Corpus Builder}
Webvicob uses HTML dumps and modifies Document Object Model (DOM) to generate data for pretraining VDU backbones with rich corpora.
As seen in Figure~\ref{fig:2}, Webvicob provides box annotations of characters, words, LaTeX~\citep{latex}, images, lines, and paragraphs.

In this section, we explain the generation procedures (Algorithm~\ref{alg:ProcessHTML}) in detail.



\subsection{Annotation}
\subsubsection{Adding Spans}
\label{alg:add_span}
We can access each Element using the DOM and find out the bounding box (bbox) of the Element\footnote{\scriptsize\url{https://developer.mozilla.org/en-US/docs/Web/API/Element}} through the \texttt{getBoundingClientRect()}\footnote{\scriptsize\url{https://developer.mozilla.org/en-US/docs/Web/API/Element/getBoundingClientRect}} function. Webvicob modifies the HTML so that all characters can be bounded with a <span> tag to get the bbox of each character (i.e., from $\scriptstyle \textbf{<p>abc</p>}$ to $\scriptstyle \textbf{<p><span>a</span><span>b</span><span>c</span></p>}$).


\subsubsection{Construct Annotations} 
\label{alg:annot}
We construct word annotations and line annotations by calculating the spaces between the character boxes and the line boxes. We define LaTex Elements with ``mwe-math-fallback-image-inline'' className, and define image Elements with \{image, canvas, SVG, video\} tags. As MarkupLM~\cite{MarkupLM} did, paragraph annotations are extracted using a well-ordered tree structure. Elements with the same depth are grouped into one paragraph.



\renewcommand{\algorithmicrequire}{\textbf{Input:}}
\renewcommand{\algorithmicensure}{\textbf{Output:}}
\algnewcommand\algorithmicreturn{\textbf{return}}
\algnewcommand\RETURN{\algorithmicreturn}
\algnewcommand\algorithmicprocedure{\textbf{procedure}}
\algnewcommand\PROCEDURE{\item[\algorithmicprocedure]}%
\algnewcommand\algorithmicendprocedure{\textbf{end procedure}}
\algnewcommand\ENDPROCEDURE{\item[\algorithmicendprocedure]}%
\algnewcommand{\algvar}[1]{{\text{\ttfamily\detokenize{#1}}}}
\algnewcommand{\algstart}[1]{{\text{\ttfamily\detokenize{#1}}}}
\algnewcommand{\algarg}[1]{{\text{\ttfamily\itshape\detokenize{#1}}}}
\algnewcommand{\algproc}[1]{{\text{\color{blue}\ttfamily\detokenize{#1}}}}
\algnewcommand{\algassign}{\leftarrow}

\begin{algorithm}[t]
\footnotesize
\caption{Process HTML}
\label{alg:ProcessHTML}
\begin{algorithmic}[1]
\REQUIRE \algvar{html}
\ENSURE \algvar{image, annotations}
\Statex
\PROCEDURE \algstart{process_html}(\algvar{html})
  \STATE $\algvar{html} \algassign \algproc{add_spans}(\algvar{html})$ \Comment{\ref{alg:add_span}}
  \Statex // From $\scriptstyle \textbf{<p>ab</p>}$ to $\scriptstyle \textbf{<p><span>a</span><span>b</span></p>}$
  \STATE $\algvar{driver} \algassign \algproc{get_selenium_driver}(\algvar{html})$
  \STATE $\algproc{remove_invisible_elements}(\algvar{driver})$ \Comment{\ref{alg:preprocess}}
  \STATE $\algproc{change_paragraph_fonts}(\algvar{driver})$ \Comment{\ref{alg:para}}
  \STATE $\algvar{image} \algassign \algproc{capture}(\algvar{driver})$
  \STATE $\algvar{boxes} \algassign \algproc{get_glyph_box}(\algvar{driver})$ \Comment{\ref{alg:bbox}}
  \STATE $\algvar{annotations} \algassign \algproc{get_annots}(\algvar{boxes})$ \Comment{\ref{alg:annot}}
  
  \Statex \RETURN{} \algvar{image, annotations}
\end{algorithmic}
\end{algorithm}



\subsection{Rendering with Various Fonts} 
\subsubsection{Remove Invisible Elements} 
\label{alg:preprocess}
There are various ways to hide a specific Element in a web document. Even if the Element is disabled, \texttt{getBoundingClientRect()} returns results since it occupies space. To solve this, various preprocessing should be performed (e.g., removing pseudo-elements). For details, please refer to our open-source engine code.\textsuperscript{\ref{Webvicob_github}}


\subsubsection{Random Paragraph Fonts} 
\label{alg:para}

In general, ample visual diversity in the VDU backbone training phase often leads to performance improvement~\citep{mjsynth,synthtext,synthtiger,kim2022donut}.

For diversity, Webvicob renders HTML with various fonts for each paragraph (See Figure~\ref{fig:3(a)} and \ref{fig:3(b)}). We randomly select fonts from 3567 GoogleFonts\footnote{\scriptsize\url{https://fonts.google.com}} in our experiments and analyses.



\subsubsection{Precise Bounding Boxes}
\label{alg:bbox}

The actual bounding box of the glyph and the result of \texttt{getBoundingClientRect()} are different. As shown in Figure~\ref{fig:3(a)} and \ref{fig:3(b)}, the result of \texttt{getBoundingClientRect()} has a large margin. We extract a ratio of the actual glyph box to the bounding box via rendering vector images in a font file with a Pygame FreeType handler.\footnote{\scriptsize\url{https://www.pygame.org/docs/ref/freetype.html}} Using the ratio, the final tight glyph box can be obtained (Figure~\ref{fig:3(c)}).



\begin{figure}[t!]
    \centering
    \begin{subfigure}{0.7\columnwidth}
        \includegraphics[width=\columnwidth]{figure/fig3_3.png}
        \caption{}
        \label{fig:3(a)}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.7\columnwidth}
        \includegraphics[width=\columnwidth]{figure/fig3_4.png}
        \caption{}
        \label{fig:3(b)}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.7\columnwidth}
        \includegraphics[width=\columnwidth]{figure/fig3_0.png}
        \caption{}
        \label{fig:3(c)}
    \end{subfigure}
    
    \caption{\textbf{Visualization of rendered images with various fonts.} (a) Original font with \texttt{getBoundingClientRect()} results. (b) Random font with \texttt{getBoundingClientRect()} results. (c) Random font with actual glyph boxes.}
\end{figure}

\begin{table}[t]
\begin{center}
\tabcolsep=0.1cm
\begin{tabular}{cccc}
\toprule
\textbf{IIT-CDIP} & \textbf{Webvicob} & \textbf{Task 1}  &  \textbf{Task 3}  \\
\toprule
- & - & 62.98 &  25.22 \\ \midrule
500K & - & 65.01 & 25.12 \\
- & 500K & 64.62 & 26.20 \\
250K & 250K & 65.56 & 26.56 \\ \midrule
1M & - & 65.27 & 26.31 \\
- & 1M & 64.92 & 28.09 \\
500K & 500K & 65.67 & 26.51 \\ \midrule
11M & - & 68.07 & 24.76 \\
- & 6.4M & 65.63 & 27.85 \\
\bottomrule
\end{tabular}%

\caption{\textbf{Quantitative comparison in DocVQA tasks} with BROS$_{\text{BASE}}$ according to various settings of pretraining corpus. For an apples-to-apples comparison, we control the amount of pretraining data in experiments. The score is measured with Average Normalized Levenshtein Similarity (ANLS). }
\label{table:docvqa_results}
\end{center}
\end{table}

\section{Experiment and Analysis}
\subsection{Setup}
Our experiments are conducted on NSML~\cite{nsml} with four NVIDIA A100 80G GPUs. We use mixed precision training technique~\cite{MixedPrecision}. 


\subsubsection{Comparison Methods}


We set BROS~\cite{hong2021bros} as a baseline model.
BROS was proposed with an effective pretraining method (i.e., area masking) and a relative positional encoding trick. To validate the effectiveness of Webvicob-generated data, we pretrain BROS and measure performance on DocVQA Task 1~\cite{icdar21docvqa} and Task 3~\cite{mathew2022infographicvqa}.

LayoutXLM~\cite{layoutxlm} is a widely-used multilingual VDU backbone. We re-implement and pretrain LayoutXLM for eight languages to validate Webvicob in a multilingual scenario.

\begin{table*}[ht!]
\resizebox{1.0\textwidth}{!}{%
\begin{tabular}{@{}lrrrrrrrrr@{}}

\toprule
\textbf{Model} & \multicolumn{1}{c}{\textbf{FUNSD}} & \multicolumn{1}{c}{\textbf{ZH}} & \multicolumn{1}{c}{\textbf{JA}} & \multicolumn{1}{c}{\textbf{ES}} & \multicolumn{1}{c}{\textbf{FR}} & \multicolumn{1}{c}{\textbf{IT}} & \multicolumn{1}{c}{\textbf{DE}} & \multicolumn{1}{c}{\textbf{PT}} & \multicolumn{1}{c}{\textbf{Avg.}} \\
\toprule
XLM-RoBERTa$_{\text{BASE}}$~\shortcite{XLM_R} & 0.663 & 0.883 & 0.779 & 0.622 & 0.704 & 0.681 & 0.715 & 0.673 & 0.715 \\
InfoXLM$_{\text{BASE}}$~\shortcite{InfoXLM} & 0.654 & 0.874 & 0.786 & 0.598 & 0.706 & 0.683 & 0.706 & 0.680 & 0.711 \\
LayoutXLM$_{\text{BASE}}$ (PDF 22M + IIT 8M) & 0.792 & 0.897 & 0.796 & 0.780 & 0.817 & 0.821 & 0.832 & 0.824 & 0.820 \\
LayoutXLM$_{\text{BASE}}^*$ (PDF 22M + IIT 8M) & 0.785 & 0.897 & 0.787 & 0.744 & 0.791 & 0.811 & 0.832 & 0.824 & 0.785 \\
LayoutXLM$_{\text{BASE}}^*$ (Webvicob 18.6M) & 0.727 & 0.893 & 0.794 & 0.686 & 0.749 & 0.765 & 0.760 & 0.764 & 0.767 \\ \bottomrule

\end{tabular}%
}
\caption{\textbf{Multitask Fine-tuning Accuracy (F1) for XFUND Semantic Entity Recognition (SER).} We perform multitask-finetuning with eight languages. The results with our finetuning schedule are marked with $*$. Other scores are taken from the LayoutXLM paper.}
\label{table:XfundMultitaskFinetune}
\end{table*}

\begin{table}[ht!]
\centering
\resizebox{1.0\columnwidth}{!}{%
\begin{tabular}{crrr} \toprule
\textbf{Language (Code)} & \multicolumn{1}{c}{\textbf{\#Train}} & \multicolumn{1}{l}{\textbf{\#Val}} & \multicolumn{1}{l}{\textbf{\#Test}} \\ \midrule\midrule
English (EN) & 6,403,095 & 500 & 500 \\
Chinese (ZH) & 1,248,720 & 500 & 500 \\
Japanese (JA) & 1,293,628 & 500 & 500 \\
Spanish (ES) & 1,712,046 & 500 & 500 \\
French (FR) & 2,409,552 & 500 & 500 \\
Portuguese (PT) & 1,087,824 & 500 & 500 \\
Italian (IT) & 1,747,412 & 500 & 500 \\
German (DE) & 2,681,896 & 500 & 500 \\ \midrule\midrule
\textbf{Total} & 18,584,173 & 4,000 & 4,000 \\ \bottomrule
\end{tabular}
}
\caption{\textbf{Number of samples per language} in our multilingual data. ISO 639-1 language code is denoted as ``Code''.}
\label{table:NumSamplesPerLang}
\end{table}

 

\subsubsection{Pretraining}
We pretrain BROS$_{\text{BASE}}$ for five epochs with 6.4M Webvicob-generated English samples. As performed in LayoutLMv2~\cite{xu2021layoutlmv2}, 512 consecutive tokens in each document were randomly selected. Except for this, we use the same training hyperparameters of BROS$_{\text{BASE}}$.



We pretrain LayoutXLM$_{\text{BASE}}$ with 18.6M multilingual webvicob generated data composed of eight languages (English, Chinese, Japanese, Spanish, French, Portuguese, Italian, and German). The model has been pretrained for five epochs with 64 batch-size following LayoutXLM.



We follow the \textit{multilingual sampling strategy}~\citep{XLM,InfoXLM,layoutxlm}, which means that each batch is sampled with the probability $p_l \propto (\frac{N_l}{N})^{\alpha}$ where language $l$, the total number of data $N$, and the number of data per language $N_l$. The $\alpha$ is set as 0.7 to sample batches from imbalanced datasets. 

\subsubsection{Finetuning}
We finetune BROS with DocVQA datasets for 16K iterations. 64 batch-size, Adam~\cite{Adamoptim} optimizer, 5e-5 learning rate, and cosine annealing learning rate scheduler~\cite{CosineLR} are adapted. We adopt some heuristics to predictions as LayoutLMv2~\cite{xu2021layoutlmv2} did.

Since the exact finetuning schedule has not been disclosed, we set up a ``rough'' finetuning schedule and compared results. For XFUND Semantic Entity Recognition (SER), we finetune LayoutXLM with 10K iterations, 64 batch-size, AdamW~\cite{AdamWoptim} optimizer, 1e-4 learning rate, linear decay learning rate, and warmup~\cite{warmup} learning rate for 10\% of total iterations. We use unilm library\footnote{\scriptsize\url{https://github.com/microsoft/unilm}} for LayoutXLM finetuning experiments.

\subsection{Experimental Results}
\subsubsection{DocVQA Task 1 \& Task 3}
The domains of DocVQA Task 1 and Task 3 are very different. As shown in Appendix~\ref{appendix:C}, data generated by Webvicob has a variety of corpus compared to IIT-CDIP. As seen in Table~\ref{table:docvqa_results}, pretrained BROS using Webvicob data performs much better in Task 3. Surprisingly, the performance of Task 3 using Webvicob 1M has better performance than BROS using IIT-CDIP 11M.


\subsubsection{FUNSD \& XFUND}\label{sec:multi}
We report the f1 scores of Semantic Entity Recognition (SER) for the XFUND dataset. Table~\ref{table:XfundMultitaskFinetune} shows f1 scores of multitask-finetuning with eight languages. Under the same finetuning setup, Webvicob data shows comparable performance. Please note that the total number of pretraining iterations of \textit{LayoutXLM (Webvicob)} is $\sim$1.45M, much smaller than the iteration of \textit{LayoutXLM (PDF 22M + IIT 8M)} which is $\sim$2.34M. 


\section{Discussion}

Our initial goal was to handle tons of HTML data, such as Common Crawl dataset.\footnote{\scriptsize\url{https://commoncrawl.github.io/cc-crawl-statistics/plots/crawlsize}}

However, the massive variety in the HTML format made it difficult to design an integrated solution.
Currently, Webvicob is only specialized in a Wikipedia format.
Expanding the scope could be future work.  

We expect Webvicob to easily be integrated with any augmentation project, such as Augraphy~\citep{augraphy}, to boost the performance further. See Appendix~\ref{appendix:augraphy} for the integrated examples.



\section{Conclusion}
In this work, we propose an engine, Webvicob, that builds visual corpora from web resources.
The constructed visual corpora can be utilized in building VDU backbones.
In our experiments and analyses, we observe that the Webvicob-generated data helps the VDU backbone robust on various document formats and domains.
The source code of Webvicob is available at GitHub.\textsuperscript{\ref{Webvicob_github}}

\section{Acknowledgements}
The authors thank NAVER CLOVA Text Vision Team and Information Extraction Team.  

\bibliographystyle{aaai}
\bibliography{custom}

\appendix
\newpage
\onecolumn
\section{Appendix}
\subsection{Visualization of Webvicob annotations.}
\label{appendix:A}
Example data of Webvicob produced are shown in Figure~\ref{fig:viz_annot1} and Figure~\ref{fig:viz_annot2} to aid the reader's understanding.


\begin{figure}[ht]
    \centering
    \begin{subfigure}{0.47\textwidth}
        \includegraphics[width=\textwidth]{figure/A_char.jpg}
        \caption{Character annotations.}
        \label{fig:4(a)}
    \end{subfigure}
    \begin{subfigure}{0.47\textwidth}
        \includegraphics[width=\textwidth]{figure/A_word.jpg}
        \caption{Word annotations.}
        \label{fig:4(b)}
    \end{subfigure}

    \caption{\textbf{Visualization of Webvicob character and word annotations.} We use a colormap to show the order of box annotation.}
    \label{fig:viz_annot1}
\end{figure}

\newpage

\begin{figure}[h]
    \centering
    \begin{subfigure}{0.47\textwidth}
        \includegraphics[width=\textwidth]{figure/A_line.jpg}
        \caption{Line annotations.}
        \label{fig:4(c)}
    \end{subfigure}
    \begin{subfigure}{0.47\textwidth}
        \includegraphics[width=\textwidth]{figure/A_paragraph.jpg}
        \caption{Paragraph annotations.}
        \label{fig:4(d)}
    \end{subfigure}

    \caption{\textbf{Visualization of Webvicob line and paragraph annotations.} We use a colormap to show the order of box annotation.}
    \label{fig:viz_annot2}
\end{figure}


\newpage
\subsection{Webvicob with Augraphy}
\label{appendix:augraphy}
Combining Webvicob and Augraphy can generate various synthetic documents with rich visual effects.
\begin{figure}[ht]
    \centering
    \includegraphics[scale=0.25]{figure/sample_webvicob.jpg}
    \caption{\textbf{Original Webvicob data.}}
    \label{fig:augraphy1}
\end{figure}

\begin{figure}[ht]
    \captionsetup[subfigure]{oneside,margin={0.7cm,0cm}}
    
    \begin{subfigure}{0.244\textwidth}
        \includegraphics[width=\textwidth]{figure/augraphy/aug0.png}
    \end{subfigure}
    \begin{subfigure}{0.244\textwidth}
        \includegraphics[width=\textwidth]{figure/augraphy/aug1.png}
    \end{subfigure}
    \begin{subfigure}{0.244\textwidth}
        \includegraphics[width=\textwidth]{figure/augraphy/aug16.png}
    \end{subfigure}
    \begin{subfigure}{0.244\textwidth}
        \includegraphics[width=\textwidth]{figure/augraphy/aug7.png}
    \end{subfigure}
    \hfill
    
    \begin{subfigure}{0.245\textwidth}
        \includegraphics[width=\textwidth]{figure/augraphy/aug12.png}
    \end{subfigure}
    \begin{subfigure}{0.245\textwidth}
        \includegraphics[width=\textwidth]{figure/augraphy/aug5.png}
    \end{subfigure}
    \begin{subfigure}{0.245\textwidth}
        \includegraphics[width=\textwidth]{figure/augraphy/aug14.png}
    \end{subfigure}
    \begin{subfigure}{0.245\textwidth}
        \includegraphics[width=\textwidth]{figure/augraphy/aug19.png}
    \end{subfigure}
    \hfill
    \caption{\textbf{Webvicob with Augraphy.} The results of Augraphy using Figure~\ref{fig:augraphy1}.}
    \label{fig:augraphy2}
\end{figure}


\newpage
\subsection{PCA analysis.}
\label{appendix:C}
We sampled 3,626 data from each of the four datasets (Webvicob, IIT-CDIP, DocVQA Task 1, DocVQA Task 3) and made 14504 vectors for PCA.
We construct a 100,000 word vocabulary using Wikipedia 1M data and use Bag Of Words (BOW) to get representations. Stop words\footnote{~\url{https://www.nltk.org/index.html}} are excluded.
The figures [\ref{fig:5}, \ref{fig:6}, \ref{fig:7}] in the $i^{th}$ row show the analysis between the $i^{th}$ and $(i+1)^{th}$ principal components. For example, the $0^{th}$ row is a plot using the $0^{th}$ and $1^{st}$ principal components.

We performed visualization using a total of 10 principal components. IIT-CDIP and DocVQA Task 1 have very similar BOW vectors, and Webvicob contains all of the various BOW vectors.

\begin{figure}[ht]
    \captionsetup[subfigure]{oneside,margin={0.7cm,0cm}}
    
    \begin{subfigure}{0.244\textwidth}
        \includegraphics[width=\textwidth]{figure/pca/0_1_Webvicob.png}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.244\textwidth}
        \includegraphics[width=\textwidth]{figure/pca/0_1_iit.png}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.244\textwidth}
        \includegraphics[width=\textwidth]{figure/pca/0_1_vqa_t1.png}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.244\textwidth}
        \includegraphics[width=\textwidth]{figure/pca/0_1_vqa_t3.png}
    \end{subfigure}
    \hfill
    
    \begin{subfigure}{0.245\textwidth}
        \includegraphics[width=\textwidth]{figure/pca/1_2_Webvicob.png}
    \end{subfigure}
    \begin{subfigure}{0.245\textwidth}
        \includegraphics[width=\textwidth]{figure/pca/1_2_iit.png}
    \end{subfigure}
    \begin{subfigure}{0.245\textwidth}
        \includegraphics[width=\textwidth]{figure/pca/1_2_vqa_t1.png}
    \end{subfigure}
    \begin{subfigure}{0.245\textwidth}
        \includegraphics[width=\textwidth]{figure/pca/1_2_vqa_t3.png}
    \end{subfigure}
    \hfill
    
    \begin{subfigure}{0.245\textwidth}
        \includegraphics[width=\textwidth]{figure/pca/2_3_Webvicob.png}
        \caption{Webvicob.}
    \end{subfigure}
    \begin{subfigure}{0.245\textwidth}
        \includegraphics[width=\textwidth]{figure/pca/2_3_iit.png}
        \caption{IIT-CDIP.}
    \end{subfigure}
    \begin{subfigure}{0.245\textwidth}
        \includegraphics[width=\textwidth]{figure/pca/2_3_vqa_t1.png}
        \caption{DocVQA Task 1.}
    \end{subfigure}
    \begin{subfigure}{0.245\textwidth}
        \includegraphics[width=\textwidth]{figure/pca/2_3_vqa_t3.png}
        \caption{DocVQA Task 3.}
    \end{subfigure}
    
    \caption{\textbf{Visualization of PCA Results.} Components 0, 1, 2, 3.}
    \label{fig:5}
\end{figure}

\begin{figure}[ht]
    \captionsetup[subfigure]{oneside,margin={0.7cm,0cm}}
    
    \begin{subfigure}{0.245\textwidth}
        \includegraphics[width=\textwidth]{figure/pca/3_4_Webvicob.png}
    \end{subfigure}
    \begin{subfigure}{0.245\textwidth}
        \includegraphics[width=\textwidth]{figure/pca/3_4_iit.png}
    \end{subfigure}
    \begin{subfigure}{0.245\textwidth}
        \includegraphics[width=\textwidth]{figure/pca/3_4_vqa_t1.png}
    \end{subfigure}
    \begin{subfigure}{0.245\textwidth}
        \includegraphics[width=\textwidth]{figure/pca/3_4_vqa_t3.png}
    \end{subfigure}
    
    \hfill
    \begin{subfigure}{0.245\textwidth}
        \includegraphics[width=\textwidth]{figure/pca/4_5_Webvicob.png}
    \end{subfigure}
    \begin{subfigure}{0.245\textwidth}
        \includegraphics[width=\textwidth]{figure/pca/4_5_iit.png}
    \end{subfigure}
    \begin{subfigure}{0.245\textwidth}
        \includegraphics[width=\textwidth]{figure/pca/4_5_vqa_t1.png}
    \end{subfigure}
    \begin{subfigure}{0.245\textwidth}
        \includegraphics[width=\textwidth]{figure/pca/4_5_vqa_t3.png}
    \end{subfigure}
    
    \hfill
    \begin{subfigure}{0.245\textwidth}
        \includegraphics[width=\textwidth]{figure/pca/5_6_Webvicob.png}
        \caption{Webvicob.}
    \end{subfigure}
    \begin{subfigure}{0.245\textwidth}
        \includegraphics[width=\textwidth]{figure/pca/5_6_iit.png}
        \caption{IIT-CDIP.}
    \end{subfigure}
    \begin{subfigure}{0.245\textwidth}
        \includegraphics[width=\textwidth]{figure/pca/5_6_vqa_t1.png}
        \caption{DocVQA Task 1.}
    \end{subfigure}
    \begin{subfigure}{0.245\textwidth}
        \includegraphics[width=\textwidth]{figure/pca/5_6_vqa_t3.png}
        \caption{DocVQA Task 3.}
    \end{subfigure}
    
    \caption{\textbf{Visualization of PCA Results.} Components 3, 4, 5, 6.}
    \label{fig:6}
\end{figure}

\begin{figure}[ht]
    \captionsetup[subfigure]{oneside,margin={0.7cm,0cm}}
    
    \begin{subfigure}{0.245\textwidth}
        \includegraphics[width=\textwidth]{figure/pca/6_7_Webvicob.png}
    \end{subfigure}
    \begin{subfigure}{0.245\textwidth}
        \includegraphics[width=\textwidth]{figure/pca/6_7_iit.png}
    \end{subfigure}
    \begin{subfigure}{0.245\textwidth}
        \includegraphics[width=\textwidth]{figure/pca/6_7_vqa_t1.png}
    \end{subfigure}
    \begin{subfigure}{0.245\textwidth}
        \includegraphics[width=\textwidth]{figure/pca/6_7_vqa_t3.png}
    \end{subfigure}
    
    \hfill
    
    \begin{subfigure}{0.245\textwidth}
        \includegraphics[width=\textwidth]{figure/pca/7_8_Webvicob.png}
    \end{subfigure}
    \begin{subfigure}{0.245\textwidth}
        \includegraphics[width=\textwidth]{figure/pca/7_8_iit.png}
    \end{subfigure}
    \begin{subfigure}{0.245\textwidth}
        \includegraphics[width=\textwidth]{figure/pca/7_8_vqa_t1.png}
    \end{subfigure}
    \begin{subfigure}{0.245\textwidth}
        \includegraphics[width=\textwidth]{figure/pca/7_8_vqa_t3.png}
    \end{subfigure}
    
    \hfill
    
    \begin{subfigure}{0.245\textwidth}
        \includegraphics[width=\textwidth]{figure/pca/8_9_Webvicob.png}
        \caption{Webvicob.}
    \end{subfigure}
    \begin{subfigure}{0.245\textwidth}
        \includegraphics[width=\textwidth]{figure/pca/8_9_iit.png}
        \caption{IIT-CDIP.}
    \end{subfigure}
    \begin{subfigure}{0.245\textwidth}
        \includegraphics[width=\textwidth]{figure/pca/8_9_vqa_t1.png}
        \caption{DocVQA Task 1.}
    \end{subfigure}
    \begin{subfigure}{0.245\textwidth}
        \includegraphics[width=\textwidth]{figure/pca/8_9_vqa_t3.png}
        \caption{DocVQA Task 3.}
    \end{subfigure}
    \hfill
    
    
    \caption{\textbf{Visualization of PCA Results.} Components 6, 7, 8, 9.}
    \label{fig:7}
\end{figure}
\end{document}
