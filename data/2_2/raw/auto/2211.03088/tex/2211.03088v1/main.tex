%\documentclass[10pt, conference, letterpaper]{IEEEtran}

\documentclass[10pt,english,journal]{IEEEtran}
% \documentclass[transmag,12pt,a4paper,draftclsnofoot,peerreview,onecolumn]{IEEEtran}
% \documentclass[12pt, draftclsnofoot, onecolumn]{IEEEtran}
\pagenumbering{gobble}
\usepackage[T1]{fontenc}
\usepackage[latin9]{inputenc}
\usepackage{geometry}
\geometry{verbose,tmargin=0.75in,bmargin=1in,lmargin=0.625in,rmargin=0.625in}
\usepackage{amsmath}
\usepackage{xpatch}
\usepackage{amssymb}
\usepackage{esint}
\usepackage{mathtools}
\usepackage{amsthm}
\usepackage{nicefrac}
%\usepackage{gensymb}
\usepackage{bigints}
%\usepackage{kbordermatrix}
\usepackage{mathtools}
\usepackage{blkarray, bigstrut}
\usepackage{physics}
\usepackage{calligra}
\usepackage{graphicx}
\usepackage{epstopdf}
\usepackage{dsfont}
\usepackage{array,ragged2e}
\usepackage{enumitem}
\usepackage{etoolbox}
\usepackage{babel}
\usepackage[nopar]{lipsum}
\usepackage{psfrag}
\usepackage[ruled,lined,linesnumbered]{algorithm2e}
\usepackage{algorithmic}
\usepackage{algorithm2e}
\usepackage{balance}
\usepackage{float}
\usepackage{hyperref}

%\usepackage[caption = false]{subfig}
% \usepackage{subfigure}
\usepackage{subcaption}

\SetKw{KwBy}{by}

\makeatletter
\newcommand{\removelatexerror}{\let\@latex@error\@gobble}
\makeatother

\graphicspath{ {Figures/} }
\newcommand{\tbd}[1]{{\color{red}{[TBD] #1}}}
\newcommand{\lz}[1]{{\color{blue}{#1}}}
\newcommand{\fd}[1]{{\color{blue}{#1}}}
\newcommand{\fr}[1]{{\color{blue}{#1}}}

%%%%%%%%%%%% Theorem style %%%%%%%%%%%%%

\newtheorem{theorem}{Theorem}
\newtheorem{corollary}{Corollary}
\newtheorem{defn}{Definition}
\newtheorem{assum}{Assumption}
\newtheorem{remark}{Remark}
%%%%%%%%%%%% Proof style %%%%%%%%%%%%%%
\renewcommand{\qedsymbol}{$\blacksquare$}
\xpatchcmd{\proof}{\hskip\labelsep}{\hskip5\labelsep}{}{}  %% change 5 here as you wish
\makeatletter
%\xpatchcmd{\proof}{\@addpunct{.}}{\normalfont\,\@addpunct{:}}{}{}
\xpatchcmd{\proof}{\@addpunct{.}}{\@addpunct{:}}{}{}
\makeatother

%%%%%%%%%%%%%% Underbrace/Overbrace redefinition (to keep text size) %%%%%%%%%%%%%
\newcommand*{\KeepStyleUnderBrace}[1]{%
  \mathop{%
    \mathchoice
    {\underbrace{\displaystyle#1}}%
    {\underbrace{\textstyle#1}}%
    {\underbrace{\scriptstyle#1}}%
    {\underbrace{\scriptscriptstyle#1}}%
  }\limits
}

\newcommand*{\KeepStyleOverBrace}[1]{%
  \mathop{%
    \mathchoice
    {\overbrace{\displaystyle#1}}%
    {\overbrace{\textstyle#1}}%
    {\overbrace{\scriptstyle#1}}%
    {\overbrace{\scriptscriptstyle#1}}%
  }\limits
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% LyX specific LaTeX commands.
\newcommand{\noun}[1]{\textsc{#1}}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% User specified LaTeX commands.
\renewcommand\[{\begin{equation}}
\renewcommand\]{\end{equation}} 
\pagestyle{empty}
\newcounter{MYtempeqncnt}
\usepackage{listings}
\usepackage{fancyvrb}
\usepackage{framed}

%\usepackage[margin=0.75in]{geometry}
\usepackage{courier}
%\usepackage{color}
\usepackage[usenames,dvipsnames,table]{xcolor}

\definecolor{dkgreen}{rgb}{0,0.3,0}
\definecolor{gray}{rgb}{0.5,0.5,0.5}

%%%%% Modify space after authors %%%%%
%\makeatletter
%\patchcmd{\@maketitle}
% {\addvspace{0.5\baselineskip}\egroup}
% {\addvspace{-1.5\baselineskip}\egroup}
% {}
%{}
%\author{Anonymous Authors}
%\makeatother

% SubItem
\newcommand{\SubItem}[1]{
    {\setlength\itemindent{15pt} \item[--] #1}
}

% Ceil and Floor
\DeclarePairedDelimiter\ceil{\lceil}{\rceil}
\DeclarePairedDelimiter\floor{\lfloor}{\rfloor}

%\newcommand{\norm}[1]{\left\lVert#1\right\rVert}

\makeatletter
\newcommand*{\rom}[1]{\expandafter\@slowromancap\romannumeral #1@}
\makeatother


\usepackage{siunitx}
\usepackage{tabu}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{capt-of}
\usepackage{array}
\usepackage{arydshln}
\setlength\dashlinedash{0.2pt}
\setlength\dashlinegap{1.5pt}
\setlength\arrayrulewidth{0.3pt}

%Widows & Orphans & Penalties

\DeclareMathOperator*{\argmax}{arg\,max}
\newcommand{\diag}{\mathop{\mathrm{diag}}}
\newcommand{\comment}[1]{}
\newcommand{\change}[1]{{\color{black} {#1}}}
%\usepackage[sort&compress, numbers]{natbib}


\begin{document}

%\title{A Federated Deep Reinforcement Learning Approach for Distributed Network Slicing Orchestration}
%\title{Scalable Statistical Federated DRL for Orchestration of Distributed SLA-Constrained RAN Slicing}
\title{
% Semantic-Aware Federated DRL for Scalable Distributed 6G RAN Slicing Orchestration 
%A Dynamic Federated DRL Approach for\\ Distributed RAN Slicing Orchestration
On the Specialization of FDRL Agents for Scalable and Distributed 6G RAN Slicing Orchestration 
%\change{Specialized Federated DRL for Scalable Distributed 6G RAN Slicing}
}

%\author{\IEEEauthorblockN{
%Farhad Rezazadeh\IEEEauthorrefmark{1}\IEEEauthorrefmark{2},
%Lanfranco Zanzi\IEEEauthorrefmark{3}\IEEEauthorrefmark{4},
%Francesco Devoti\IEEEauthorrefmark{3},
%Hatim Chergui\IEEEauthorrefmark{1},\\
%Xavier Costa-P\'erez\IEEEauthorrefmark{3}\IEEEauthorrefmark{5},
%and Christos Verikoukis\IEEEauthorrefmark{6}
%}
%\\
%\IEEEauthorrefmark{1}Centre Tecnol\'ogic de Telecomunicacions de Catalunya (CTTC), Barcelona, Spain\\
%\IEEEauthorrefmark{2}Universitat Polit\'ecnica de Catalunya (UPC), Barcelona, Spain\\
%\IEEEauthorrefmark{3}NEC Laboratories Europe, Heidelberg, Germany\\
%\IEEEauthorrefmark{4}Technische Universit\"at Kaiserslautern, Kaiserslautern, Germany\\
%\IEEEauthorrefmark{5}i2CAT Foundation and ICREA, Barcelona, Spain\\
%\IEEEauthorrefmark{6}University of Patras, Patras, Greece
%}


\author{
Farhad~Rezazadeh,~\IEEEmembership{Student~Member,~IEEE}, Lanfranco~Zanzi,~\IEEEmembership{Member,~IEEE}, Francesco~Devoti,~\IEEEmembership{Member,~IEEE} Hatim~Chergui,~\IEEEmembership{Senior~Member,~IEEE}, Xavier~Costa-P\'erez,~\IEEEmembership{Senior~Member,~IEEE}, and~Christos~Verikoukis,~\IEEEmembership{Senior~Member,~IEEE}



\IEEEcompsocitemizethanks{
Copyright (c) 2015 IEEE. Personal use of 
this material is permitted. However, permission to use this material for any other purposes must be 
obtained from the IEEE by sending a request to pubs-permissions@ieee.org.}
\IEEEcompsocitemizethanks{This work was partially funded by the Spanish Government (initially by MICCIN and since November 2021 by the Next Generation EU program) under Grant PCI2020-112049 and by the Electronic Components and Systems for European Leadership Joint Undertaking (JU) under grant agreement No. 876868. This JU receives support from the EU`s H2020 research and innovation programme and Germany, Slovakia, Netherlands, Spain, Italy, and in part by the EU H2020 projects MonB5G (871780), 5GMediaHUB (101016714), and OPTIMIST
(872866).
}
\IEEEcompsocitemizethanks{\IEEEcompsocthanksitem F. Rezazadeh is with the Telecommunications Technological Center of Catalonia (CTTC) and  Technical University of Catalonia (UPC), Barcelona, Spain (e-mail: frezazadeh@cttc.es).}
\IEEEcompsocitemizethanks{\IEEEcompsocthanksitem L. Zanzi and F. Devoti are with the NEC Laboratories Europe, Heidelberg, Germany (e-mails: lanfranco.zanzi@neclab.eu, francesco.devoti@neclab.eu).}
\IEEEcompsocitemizethanks{\IEEEcompsocthanksitem H. Chergui is with the Telecommunications Technological Center of Catalonia (CTTC), Barcelona, Spain (e-mail: hchergui@cttc.es).}
\IEEEcompsocitemizethanks{\IEEEcompsocthanksitem Xavier Costa-P\'erez is with NEC Laboratories Europe, i2CAT Foundation, and ICREA, Barcelona, Spain (e-mail: xavier.costa@neclab.eu).}
\IEEEcompsocitemizethanks{\IEEEcompsocthanksitem C. Verikoukis is with the University of Patras, ATHENA/ISI, and IQUADRAT Informatica, Barcelona, Spain (e-mail: cveri@ceid.upatras.gr).}
%\vspace*{-.3cm}
}

\maketitle

\begin{abstract}
%Xavi The network slicing technology enables multiple virtual networks to be instantiated and customized to meet heterogeneous use case requirements over 5G network deployments. However, most of the solutions available today suffer poor scalability performances when considering a multitude of slices, mainly due to centralized controllers that require a holistic view of the resource availability and consumption over different networking domains. To fill this gap, in this paper we investigate a hierarchical architecture to manage network slices in a distributed manner. Enabled by the rapid evolution of deep reinforcement learning (DRL) schemes, we envision a set of local decision agents (DAs) being dynamically deployed in the radio access network (RAN) domain as virtualized instances. Such decision entities are able to learn, in a federated way, the underlying traffic dynamics and tailor the resource allocation task. Aided by local double deep Q-networks (DDQNs) and traffic-aware dynamic agent selection algorithm, the proposed approach allows to quickly react at the end-user mobility patterns while reducing costly interaction with a centralized controller, thus providing higher resource efficiency during the operational phase.
Network slicing enables multiple virtual networks to be instantiated and customized to meet heterogeneous use case requirements over 5G and beyond network deployments. However, most of the solutions available today face scalability issues when considering many slices, due to centralized controllers requiring a holistic view of the resource availability and consumption over different networking domains. In order to tackle this challenge, we design a \emph{hierarchical architecture to manage network slices resources in a federated manner}. 
Driven by the rapid evolution of deep reinforcement learning (DRL) schemes and the Open RAN (O-RAN) paradigm, we propose a set of traffic-aware local decision agents (DAs) dynamically placed in the radio access network (RAN). These federated decision entities tailor their resource allocation policy according to the long-term dynamics of the underlying traffic, defining \emph{specialized} clusters that enable faster training and communication overhead reduction.
Indeed, aided by a traffic-aware agent selection algorithm, our proposed \emph{Federated DRL} approach provides higher resource efficiency than benchmark solutions by quickly reacting to end-user mobility patterns and reducing costly interactions with centralized controllers.
\end{abstract}

\begin{IEEEkeywords}
B5G/6G, Network Slicing, AI, Federated Learning, Deep Reinforcement Learning, Distributed Management
\end{IEEEkeywords}

\section{Introduction}

 \IEEEPARstart{V}{ehicle-to-everything} \change{(V2X) communication, Internet of things (IoT), augmented/virtual reality (AR/VR), are just some examples of emerging use-cases in 5G/6G verticals that need to co-exist over a common physical infrastructure.
However, the highly heterogeneous performance requirements in terms of bandwidth, latency, and reliability, exacerbate the need for orchestration solutions able to accommodate such services in a resource and cost-efficient manner.}
% \IEEEPARstart{E}{merging} use-cases in beyond 5G (B5G) and 6G vertical market, such as internet of things (IoT), augmented/virtual reality (AR/VR), vehicle-to-everything (V2X) communication, demand for highly heterogeneous performance requirements in terms of bandwidth, latency, and reliability, which exacerbate the need for orchestration solutions able to accommodate such services in a resource and cost-efficient manner.
% Enabled by software-defined networking (SDN) and network functions virtualization (NFV)~\cite{TVT_3}, 
Network slicing represents a promising technology able to address such a challenging scenario, by enabling the setup of multiple logical and virtualized network instances, namely \emph{slices}, on top of a common physical mobile network infrastructure~\cite{TVT_2}. Given the cloud nature of these resources, the networking resources associated to each slice can be dynamically orchestrated and tailored to meet the performance requirements of running services.
%
In this context, temporal variations of the traffic demand deeply complicate resource planning and allocation tasks, especially in the radio access network (RAN) domain where resource allocation \cite{Zhou_sli} decisions, e.g., in terms of bandwidth, must cope with the additional variability inherent of the wireless channel and end-user's mobility.
Traditional RAN slicing solutions envision a centralized controller with a holistic and real-time view of the network, especially about resource utilization, availability, and real-time wireless channel statistics, as depicted in Fig.~\ref{fig:problem}. However, similar approaches suffer from scalability issues in real deployments, where the amount of monitoring information to be exchanged, together with the \change{large} number of base stations (BSs), make it practically impossible to devise optimal resource allocation schemes in a timely and resource-efficient manner~\cite{f-centralized}. 
\begin{figure}[t!]
\centering
% \vspace{3mm}
\includegraphics[clip, trim = 6cm 5.5cm 13cm 3cm, width=0.85\columnwidth]{problem_infocom.pdf}
\caption{ \small RAN resource allocation in network slicing.}
\label{fig:problem}
\vspace{-0.6cm}
\end{figure}
Motivated by the need for a more cost-effective and agile RAN, the Open RAN (O-RAN) Alliance recently presented a vendor-neutral alternative way of building mobile networks~\cite{ORAN_White_paper}, based on disaggregated hardware and interoperable interfaces that allow secure network sharing by means of virtualization. 
\change{Despite the revolutionary approach, it is still not clear how to efficiently support slicing scenarios~\cite{masssive-slicing} characterized by a large number of vertical services.} Therefore, we take on this challenge and propose a hierarchical architecture for network slice resource orchestration. In particular, given the variable spatio-temporal distribution of mobile traffic demands~\cite{AztecInfocom20}, we envision the dynamic setup of a network of local decision agents (DAs) as virtual software instances co-located within the Near-Real Time RAN Intelligent Controller (Near-RT RIC) premises, able to access local RAN monitoring information and extract local knowledge without the need of a centralized entity performing decisions on aggregated information. Our framework leverages a dynamic agent selection mechanism based on local traffic conditions similarity, which enables more efficient information exchange and collaboration among groups of local DAs, while specializing their decision policy.
The benefit coming from our approach are several: $i$) it enables resource allocation at the edge of the network, thus accounting for more timely and accurate information, $ii$) the amount of control information that needs to cross the network to reach the central controller dramatically decreases, thus reducing overhead towards the core network, $iii$) by allowing information exchange among local DAs, we enable the provisioning of federated learning schemes to further enrich the capabilities of the DAs.
In fact, DAs will not only learn from a local observation space, but also leverage information coming from other (statistically different) RAN nodes, thus improving the generalization of the learning procedure.

The main contributions of our paper can be summarized as follows:
\begin{itemize}
    \item We cast the RAN resource allocation problem as an optimization problem, focusing on minimizing the traffic exceeding the service level agreement (SLA) and assessing its complexity.
    \item We propose a distributed architecture for RAN slice resource orchestration based on deep reinforcement learning (DRL), composed of multiple artificial intelligence (AI)-enabled decision agents that perform local radio allocation decisions without the need for a centralized control entity.
    \item We design a federated learning (FL) scheme composed of multiple parallel layers, one for each slice, to enhance the capabilities of the local decision-making process,  following the recent development of the Open RAN architecture.
    \item We further improve the decision process by dynamically defining the subset of decision agents to be involved in the federation process, based on long-term slice traffic demands variations and their temporal similarities.
    \item We validate our hierarchical architecture and assess its capabilities in realistic scenarios by means of an exhaustive simulation campaign, accounting for a wide geographical area and thousands of end-users.
\end{itemize}

The remainder of this paper is as follows:
Sec.~\ref{sec:related} provides an overview of the related works in the field.
Sec.~\ref{sec:scenario} formulates our problem and describes the considered scenario.
Sec.~\ref{sec:architecture} presents the main building blocks of our solution, describing the interaction among the different entities.
Sec.~\ref{sec:ORAN} highlights the compliance of our solution with respect to the O-RAN architecture.
Sec.~\ref{sec:perf_eval} validates the design principles of our solution through a comprehensive simulation campaign.
Finally, Sec.~\ref{sec:conclusion} provides the final remarks and concludes this paper.

\section{Related Work}
\label{sec:related}

AI-driven approaches applied to mobile networks have recently gained momentum in distributed resource control and management tasks. In this context, DRL \cite{mano-far2s,ans2-far} and federated DRL (FDRL) \cite{Zhang_slice} stand out among a multitude of different approaches and are at the center of a strong research interest, especially in the field of automated resource orchestration. 
\change{ The authors of \cite{rec1} consider the sum power minimization problem based on jointly optimizing resource allocation, user association, and power control in a multi-access edge computing (MEC) system. In this intent, they propose a multi-agent federated reinforcement learning algorithm to solve centralized method limitations and privacy concerns. The simulation results shown that the proposed approach provides lower maximal latency, lower maximal computation capacity,
higher CPU cycles for the tasks, and higher data rate. Due to enhancing spectrum utilization in new generation wireless communication technologies, the authors in \cite{rec2} invoke an FDRL approach to accelerate learning convergence in edge nodes. 
% In their proposed method, the multi-channel dynamic spectrum is formulated as a Markovian decision process (MDP) based on its characteristics. The numerical results demonstrated the efficiency of the proposed framework.
In \cite{rec3}, the authors investigate the decentralized joint optimization of channel selection and power control for V2V communication, proposing a federated multi-agent DRL (Fed-MARL) approach to satisfy the reliability and latency requirements of V2V communication, and maximize the transmit rates of cellular links. The results have shown how the federation of local DRL models coming from different V2V agents can tackle the limitations of partial observability of the entire network, resulting in superior perfomances over baseline approaches in terms of communication rate and packet delivery rate. 
%The problem of static orchestrations of service function chains (SFCs) is addressed by \cite{rec5}. The authors propose a novel Scalable SFC Orchestration (SSCO) based on the FDRL method to dynamically fulfill the ever-increasing requirements of different network applications. Simulation results have shown that both placement-error-rate-based and reward-based federated weighed strategies for local agent exhibits better performance over different network scenarios in terms of convergence, average reward, and average resource consumption compared to other reference policies. 
Targeting at implementing the open RAN (O-RAN) with virtualized network components, the authors of~\cite{rec6} proposed an FDRL-based with a global model server installed in the intelligent controller (RIC) to update the deep Q-networks parameters. This approach can mitigate load balancing and frequent handovers in the massive base station deployment. Following O-RAN standardization, the numerical results have demonstrated the proposed method enables UE to effectively maximizes the long-term throughput and avoids frequent handovers.}
The authors of~\cite{FMA_HUY} develop a DRL algorithm for the resource allocation in a mobility-aware FL network, optimizing the number of successful transmissions while minimizing energy and channel costs. In~\cite{Fdrl_seif}, the authors propose a federated network slicing scheme based on DRL techniques for channels and bandwidth allocation in the context of industrial IoT (IIoT), highlighting significant performance improvements when compared against centralized strategies.
%
In~\cite{DeepRL_Infocom} the authors model the network utility maximization problem and exploit DRL techniques such as deep Q-learning to solve the decision-making task. Their work highlights significant improvements over key performance indicators (KPIs) and networking metrics, such as throughput and latency. This solution however exploits a centralized approach that aims to solve a global optimization, therefore limiting the individual network slices in the management of their own resources.
%
A similar problem has been addressed by~\cite{EdgeSlice}, which however proposes a decentralized resource orchestration system to automate dynamic end-to-end network slicing resource management in wireless edge computing networks. The proposed architecture makes use of a central performance coordinator entity and multiple orchestration agents. This work however provides limited details on inter-agent information exchange aspects.
Also~\cite{Scalable_Orchestration} proposes a DRL approach for the orchestration of service function chains in NFV-enabled networks, addressing both placement-error-rate-based and reward-based federated weighed strategies, showing significant convergence performance, higher average reward, and smaller average resource consumption in a variety of networking scenarios.

From the viewpoint of real-time inter-slice resource management and yield an intelligent strategy, the authors of~\cite{TVT_5} design a graph attention multi-agent Reinforcement Learning to cope with frequent BS handover. The simulation results have demonstrated that the proposed approach is effective to enhance the cooperation for the multi-BS system in RAN while satisfying the strict SLA requirements. In~\cite{TVT_4}, the authors propose a multi-agent reinforcement learning approach for RAN capacity sharing, showcasing better scalability and faster learning in comparison to single-agent approaches. More recently, the authors of~\cite{NODL} develop an FL framework in the context of fog computing, focusing on the distribution of training tasks. The numerical results show that the proposed network-aware scheme significantly improves network resource utilization while achieving comparable accuracy.
%
%In~\cite{LACO} the authors propose a multi-armed-bandit based algorithm for RAN resource allocation with latency guarantees. Their approach however suffers scalability issues for an increasing number of slices.



Following this state-of-the-art overview, the key novelty of our approach relies on the exploitation of distributed RAN information to design a new class of specialized agents that collaborate in homogeneous clusters via a federation layer,  which leads to scalable and stable decision under highly dynamic traffic conditions and then proposed framework is also mapped to O-RAN. To the best of our knowledge, this is the first work to propose an FDRL framework in the context of distributed radio resource management, by adopting dynamic agent selection to improve specialization of agents and reduce communication overhead.
%The key novelty of our approach relies on the exploitation of distributed RAN information coming from the underlying system structure by means of multiple and independent federation layers. 


\begin{table}[!t]
\caption{\change{Notation Table}}
\label{tab:model_var_and_par}
%\ra{1.3}
%\scriptsize
\centering
\begin{tabular}{@{}lc@{}}\toprule

\textbf{Notation} & \textbf{Description} \\ \midrule
$\mathcal{B}$ & Set of BSs\\ \hdashline
$\mathcal{I}$ & Set of slices\\ \hdashline
$C_b$ & Capacity of BS $b\in\mathcal{B}$\\ \hdashline
$\Lambda_i$ & Latency requirement of slice $i\in\mathcal{I}$\\ \hdashline
$\lambda_i$ & Throughput requirement of slice $i\in\mathcal{I}$\\ \hdashline
$\mathcal{T}$ & Set of decision intervals\\ \hdashline
$\epsilon$ & Duration of decision intervals $t\in\mathcal{T}$\\ \hdashline
$a_{i,b}^{(t)}$ & PRB allocation for slice $i\in\mathcal{I}$ at BS $b\in\mathcal{B}$\\
 & taken at time interval $t\in\mathcal{T}$\\ \hdashline
$\sigma_{i,b}^{(t)}$ & Average SNR experienced by the users of\\
& slice $i\in\mathcal{I}$ at BS $b\in\mathcal{B}$ in time interval $t\in\mathcal{T}$\\ \hdashline
%$\lambda_{i,b}^{(t)}$ & Aggregated traffic demand of the users of\\
%& slice $i\in\mathcal{I}$ at BS $b\in\mathcal{B}$ in time interval $t\in\mathcal{T}$\\ \hdashline
$\varphi_{i,b}^{(t)}$ & Instantaneous traffic demand of the users of\\
& slice $i\in\mathcal{I}$ at BS $b\in\mathcal{B}$ in time interval $t\in\mathcal{T}$\\ \hdashline
$d_{i,b}^{(t)}$ & Traffic of slice $i\in\mathcal{I}$ at BS $b\in\mathcal{B}$\\
& dropped in time interval $t\in\mathcal{T}$\\ \hdashline
%$\mathcal{S}$ & State Space \\ \hdashline
%$\mathcal{A}$ & Action space \\ \hdashline
%$\mathcal{R}$ & Reward \\ \hdashline
$\iota$ & Minimum PRB allocation\\ 
\hdashline

\change{$E_{i,b}^{(t)}$} & \change{Expected transmission latency}\\ 
\hdashline
\change{$\nu_i^{(t)}$} & \change{Amount of available capacity left by}\\& \change{previous decisions of other agents} \\ 
\hdashline
\change{$\alpha_{i}^{(t)}$} & \change{Allocation gap}\\ 
\hdashline

\change{$\rho^{(t)}_{\text{up}}$} & \change{Upper boundary of allocation gap}\\ 
\hdashline

\change{$\rho^{(t)}_{\text{lower}}$} & \change{Lower boundary of allocation gap}\\ 
\hdashline
\change{$r_i^{(t)}$} & \change{Instantaneous reward of the $i$-th agent}\\ 
\hdashline

\change{$P_i^{(t)}$} & \change{Penalty of $i$-th agent}\\ 
\hdashline

\change{$\eta_i$} & \change{Penalty coefficient}\\ 
\hdashline

\change{$Q_{\pi}$} & \change{Action-value function under a given policy $\pi$}\\ 
\hdashline


\change{$\gamma$} & \change{Discount factor}\\ 
\hdashline
\change{$\xi$} & \change{Learning rate}\\ 
\hdashline

\change{$\beta_{i}$} & \change{Experience buffer}\\ 
\hdashline

\change{$\theta_{i}^{(t)}$} & \change{Online network parameter}\\ 
\hdashline

\change{$\tilde{\theta}_{i}^{(t)}$} & \change{Target network parameter}\\ 
\hdashline

\change{$\Omega_{i}^{(t+1)}$} & \change{Global updated model}\\ 
\hdashline

\change{$\Psi$} & \change{Set of clusters}\\ 

\bottomrule
\end{tabular}
\end{table}


\section{Framework Overview}
\label{sec:scenario}

% \tbd{
% Figure 1 presents our E2E network slicing scheme where it encompasses the radio access network (RAN) slicing and the core network slicing with control plane (CP) and user plane (UP) network functions within a public land mobile network (PLMN). A network slice spans across the network domains running on shared physical infrastructure resources. We consider slice-enabling RAN (gNB) with 3GPP central unit (CU) and distributed unit (DU) functional split (CU-DU).
% \begin{figure*}[h!]
% \centering
% \includegraphics[scale=0.43]{E2E.eps}
% \caption{The E2E network slicing scheme.}
% %\vspace{3mm}
% \end{figure*}
% A total of $N$ access points (NR-RU/DU) are covering $M$ single-antenna users in a downlink setup and connected to CU hosting control agents running a set of VNF. The network slicing architecture consist of $L \in \mathbb{N}$ slice instances where each slice accommodates $M_l$ users with $\sum_{l=1}^{L}M_{l} = M$. 

% \begin{assum} 
% The network slice identification is done through the single network slice selection assistance information (S-NSSAI) where NSSAI signaling message is allowed to send a collection up to eight S-NSSAIs between the user and network \cite{refA18}. Indeed, each user $M$ sends NSSAI to assist the network in selecting a particular network slice where it may be served by a maximum of eight network slices simultaneously. We suppose each user just request one type of service and thereby one slice instance at each decision time step.  
% \end{assum}

% The S-NSSAI is comprised of a Slice/Service Type (SST) and an optional slice differentiator (SD) where SST refers to expected specific features and services which includes eMBB (enhanced Mobile Broadband), URLLC (Ultra-Reliable Low Latency Communications), and mMTC (massive machine-type communications) and other SSTs are still work in progress. Moreover, the SD is defined as an additional differentiator for multiple network slices with the same SST value. Indeed, standardized S-NSSAI has only SST value. Without loss of generality, we define 3 different slice classes as C1, C2, and C3 in considered network slicing architecture.

% The user/user equipment (UE) sends configured or allowed NSSAI to the radio resource control layer (RRC) and non-access stratum (NAS) signaling as part of registration. In fact,  NSSAIs are managed at the tracking area level (RAN) and registration area level (core). According to NSSAI, the CU-CP selects specific access and mobility management function (AMF), and also specific CU-UP is given by the AMF during the protocol data unit (PDU) setup procedure. Notice that UE can use multiple different network slices and therefore AMF can be shared between slices. For an eMBB user (S1), the AMF selects an S1-specific session management function (SMF), and thereby SMF chooses a specific user plane function(s) (UPF(s)) concerning location and load information. The UPF connects mobile infrastructure to the data network (DN) or application server. The common core is a cost-efficient approach for 4G to 5G migration where LTE and 5G new radio (NR), and non-3GPP accesses (e.g.Wi-Fi and fixed broadband) are integrated via a common interface to underpin multi-radio access technology (RAT).

% These NFs can be implemented as VNFs. In this context, B5G RAN functionality most likely to be implemented as VNF(s) in gNB-CU except NR lower layers. Indeed, these VNFs are chained together with a specific order in network slices forming an E2E service function chain (SFC) to complies the 3GPP standards. The mobile network operator (MNO) embeds the receiving E2E slice requests onto the substrate network and guarantees slice isolation and requirements. Note that the full virtualized gNB CU-DU and NR protocol stack can be a plausible scenario in the future. The NFV incorporates three entities, namely VNFs, NFV infrastructure (NFVI), and NFV-MANO \cite{refA18}. The network service provisioning and NFV resource allocation consist of three main phases that have a vital role in network performance and costs: 1) VNF Forwarding Graphs (VNFFG) is defined as a chain of VNFs or SFC to instantiate a network service, 2) The VNF placement or embedding problem refers to choosing a set of optimal locations and mapping VNFs concerning nodes, links, and resources, 3) The scheduling allows to run a set of VNFs to be hosted in a virtual machine VNF under given constraints. The optimization of these stages will result in a dramatic reduction in network costs while improving the experienced QoS.

% \begin{assum}
% In this paper, we consider scheduling issues where the main aim is to minimize VNF instantiation costs for each slice instance. Moreover, it is assumed that the VNFs are located in predetermined data centers (DCs) called RAN-DC and core-DC where we have resources constraint in RAN-DC and more resources/VNFs are hosted in core-DC. It can be noticed that the network slicing states (network configurations and parameters) change rapidly in the radio domain,
% while this change is relatively slow in the core part.
% \end{assum}

% The RAN-DC and core-DC maintain and deploy a set of VNFs to serve the users of distributed APs and also hosts agents to facilitate the training process to learn best policies and actions for scaling vertically the computing resources and consequently scale horizontally for VNFs instantiation according to system states. For the sake of simplicity and without loss of generality, we assume specific type of VNFs per slice. We consider resource allocation (in NFV) tasks where the MNO collects the free and unused resources from the tenants and allocate them to the slices in need. It is done either periodically to avoid over-heading or based on requests of tenants. 
% }

Our solution builds on the concept of slicing-enabled mobile networks~\cite{net-Slicing}, wherein multiple network tenants are sharing a portion, namely a \emph{slice}, of a common mobile network infrastructure, each one with predefined and dedicated networking resources to satisfy an SLA. Within the context of our paper, we focus on the RAN domain and consider the SLAs to be expressed in terms of maximum slice throughput and transmission latency. We define transmission latency as the average time the traffic belonging to a certain slice needs to wait within the base station transmission buffers before being served due to inter-slice scheduling procedures.
%
In the following, we overview the main system building blocks and model assumptions, finally introducing the mathematical formulation of the RAN resource allocation problem. We summarize parameters and variables describing the system model in Table~\ref{tab:model_var_and_par}.

Let us introduce a mobile network infrastructure composed of a set $\mathcal{B}$ of base stations (BSs), wherein a set of slices $\mathcal{I}$ is deployed. Each BS $b \in \mathcal{B}$ is characterized by a capacity $C_b$, expressed in terms of a discrete number of physical resource blocks (PRBs) of a fixed bandwidth. 
This resource availability must be divided into subsets of PRBs, and dynamically assigned to each network slice according to their real-time traffic demand and SLA requirements. 
\change{As part of the SLA between the network operator and the slice owner, we assume each network slice to come with predefined latency and throughput requirements defined by the variables $\Lambda_i$ and $\lambda_i$, respectively. In the context of our work, we focus on the RAN domain, and therefore consider as latency the queueing delay time experienced by the traffic while flowing through the scheduling processes of each base station.}
Let us consider a time-slotted system where time is divided into \emph{decision intervals} $t \in \mathcal{T} = \{ 1,2,\dots,T\}$.
The PRB allocation decisions can be taken only at the beginning of each decision interval, whose duration $\epsilon$ may be decided according to the infrastructure provider policies, ranging from few seconds up to several minutes.




We assume the presence of a preliminary admission and control mechanism, e.g., the one presented in~\cite{RL-NSB}, to verify the admissibility of the current network slice setup within the available networking capacity, and focus our effort on meeting the resource allocation for the downlink traffic.
We envision the allocation of radio resources towards the end-users as a two-step process~\cite{foukas_orion}. Initially, once network slices are admitted into the system, the infrastructure provider schedules the assignment of slots of radio resources for each of the tenants. Then, based on the slice resource availability, each tenant may decide to enforce proprietary scheduling solutions towards its end-users, depending on use-case or business requirements~\cite{net-Slicing}. 
%Given the plethora of user to base station association algorithms~\cite{Spatial_Loads}, as well as scheduling algorithms addressing the end-user resource allocation task, we do not address the intra-slice end-user scheduling issue, but rather focus on the correct and fair dimensioning of the inter-slice PRB allocation.
\change{Given the plethora of user to base station association and scheduling algorithms addressing the end-user resource allocation task~\cite{Spatial_Loads}, we do not address the intra-slice scheduling issue, but rather focus on the correct and fair dimensioning of the inter-slice PRB allocation.}
To this aim, we denote with the variable $a_{i,b}^{(t)}$ the PRB allocation decision for the $i$-th slice under the $b$-th BS taken at $t$-th decision time interval, and with $\sigma_{i,b}^{(t)}$ the signal-to-noise ratio (SNR) value expressing the wireless channel quality, averaged over the duration of a decision time interval $\epsilon$, and over the end-users of the $i$-th slice attached to the $b$-th BS. Similarly, we introduce $\varphi_{i,b}^{(t)}$ as the aggregated downlink traffic demand generated by the users of the $i$-th slice under the coverage area of the $b$-th BS within the $t$-th time interval. 
%\fr{To ensure slice isolation, we assume that $a_{i}$ is the PRB share for slice $i\in\mathcal{I}$. Nonetheless, for performance improvement considerations at traffic congestion time, each slice is allowed to use the spare PRBs of the other slices as long as the BS available PRB resources $C_b$ are not exceeded by the total allocated PRBs by all slices; the situation in which slices using more than their share are penalized, and their allocations are ceiled to their respective shares.}
All together, we can formalize our problem as: \\
% \vspace{1mm}
\noindent \textbf{Problem}~\texttt{RAN Resource Allocation}:
\label{prob:RAN_Slicing}
\begin{flalign}
\text{min} &  \lim\limits_{{T\rightarrow\infty}} \sum\limits_{t = 1}^T \mathbb{E}\left [\sum\limits_{i\in\mathcal{I}} d_{i,b}^{(t)} \right ] \hspace{-3cm}& &\\
\noindent\text{subject to:} \hspace{-1cm}& & &\nonumber\\
& E_{i,b}^{(t)}  \leq \Lambda_i, \hspace{-0.7cm}& \forall t \in \mathcal{T}, \forall i\in\mathcal{I}, \forall b\in\mathcal{B};&\\
& \sum\limits_{i\in\mathcal{I}} a_{i,b}^{(t)} \leq C_b, &\forall t \in \mathcal{T}, \forall b \in \mathcal{B};&\\
% & \textcolor{blue}{\mathds{1}\left(\sum\limits_{i\in\mathcal{I}} a_{i,b}^{(t)} > C_b\right) \times \left(a_{i,b}^{(t)} < a_i\right)} \hspace{-3cm}& & \\
& a_{i,b}^{(t)}\in\mathbb{Z}_+, d_{i,b}^{(t)}\in\mathbb{R}_+, &\forall t \in \mathcal{T}, \forall i\in\mathcal{I},\forall b\in\mathcal{B};&
\end{flalign}
where $E_{i,b}^{(t)} = \mathbb{E}\left [ \frac{\varphi_{i,b}^{(t)}}{\Gamma \left(a_{i,b}^{(t)},\sigma_{i,b}^{(t)} \right)+d_{i,b}^{(t)}}\right ]$
defines the expected transmission latency, and $\Gamma(a, \sigma)$ is a function that translates the PRB allocation $a$ in the equivalent transmission capacity, given the experienced channel quality $\sigma$. %$\varphi_{i,b}^{(t)}$ is the instantaneous aggregated traffic demand
The traffic demand generated within a decision interval might not be fully satisfied due to erroneous PRB allocation estimations, incurring in additional transmission \change{latency} due to traffic queuing at the base station. Therefore, we introduce %in our formulation
the variable $d_{i,b}^{(t)}$ as a deficit value indicating the volume of traffic not served within the agreed slice latency tolerance $\Lambda_i$, and that is therefore dropped.
\change{
Due to fast traffic variations, slice resource allocation \cite{melike_sli} decisions at the RAN domain should be taken in a dynamic, proactive, and flexible way to avoid service and performance degradation. While advanced admission and control mechanisms could select the set of slices to be admitted to the system, and provide static resource allocation boundaries to satisfy the available capacity, the dynamic nature of the slice's traffic load and wireless channel statistics may lead to suboptimal performances.

Additionally, the optimization problem underlying RAN resource allocation, that is, fitting the requests of the slices maximizing the overall utilization by considering the limited resource availability of a BS, has been proven to be NP-Hard~\cite{RL-NSB}. 
In fact, this problem can be easily mapped into a knapsack problem instance, wherein the sum of allocated resources is bounded by the capacity of the radio interface, and the experienced latency, i.e., the cost, is minimized. This family of problems is well-known to be NP-Hard~\cite{Knapsack_NP_Complete}, resulting in a time complexity of $O(IC_b)$ in our scenario, where $I$ is the cardinality of the set $\mathcal{I}$, and $C_b$ is the base station resource availability in number of PRBs. 
In order to obtain a solution for the overall RAN deployment, the same problem should be solved for all the nodes in the network, therefore introducing scalability issues. 
%The resulting complexity should be multiplied by the number of base stations in the network.
Moreover, the centralization of all the necessary up-to-date monitoring information further exacerbates the complexity of this problem, which becomes impractical in real mobile networks characterized by thousands of RAN nodes~\cite{pi_ROAD}.
}

%\fd{Moreover, in order to minimize resource over provisioning, we introduce the variable $d_{i,b}^{(t)}$ which indicates the traffic volume corresponding to the excess PRBs, thus accounting for lost resources.}
%All the variables and parameters used along this article are summarized in Table~\ref{tab:model_var_and_par}.
%Hereafter, whenever is evident from context, \tbd{we will drop the superscript $(t)$ to reduce the clutter in the formulation.}

% \tbd{A typical latency SLA between slice $\mathcal{I}$ tenant and the mobile network operator (MNO) consists on imposing a long-term statistical constraint on the distribution of latency values. In this regard, we invoke the $Q$-th percentile metric $f_{Q}^{(t)}$ that captures, at each step $t$, the actual latency value below which $Q\%$ of latency samples over the measurement interval ---- are located. We then enforce slice-specific upper bound $\Lambda_i$ on it. To calculate $f_{Q}^{(t)}(\cdot)$ over set $\mathcal{Y}_{l}=\left\{D_{i,b}^{(1)}, ..., D_{i,b}^{(T)}\right\}$, the elements thereof are sorted in the ascending order, i.e., $\mathcal{Y}_{l}= \{z_1,\ldots,z_{(t)}\mid z_i < z_{i+1}\},$and then the $Q$-th percentile is derived as,
% \begin{equation}
% f_{Q}\left(D_{i,b}^{(1)}, ..., D_{i,b}^{(t)}\right)= z_j,\,j = \floor*{\frac{Q \times\left(t+1\right)}{100}}.
% \end{equation}
% }


\section{A multi-agent architecture for RAN resource allocation}
\label{sec:architecture}

\begin{figure}[t!]
\centering
\includegraphics[clip,width=\columnwidth]{architecture_infocom.jpg}
\caption{\small Generic Federated DRL architecture for RAN slicing.}
\vspace{-5mm}
\label{fig:architecure}
\end{figure}

\change{In this paper,} we advocate for the adoption of an FDRL-based architecture to address the RAN slicing scenario. In particular, we rely on local DAs running as software instances within the premises of each BS, as shown in Fig.~\ref{fig:architecure}. Each agent is in charge of performing slice PRB allocation decisions based on local monitoring information coming from the underlying network monitoring system, or BS \emph{context}. We provide the details of our local decision algorithm later in Sec.~\ref{sec:DQN}.
Nevertheless, the distributed nature of RAN deployments, as well as the varying spatio-temporal behavior of mobile traffic traces~\cite{FurnoTMC2017}, make it difficult for an agent trained exclusively on complex and multi-variate monitoring metrics to address unknown statistical distributions of its base station context. %Moreover, general RL approaches suffer a slow learning rate when dealing with complex multi-variate scenarios, as the one considered in our work.

To concurrently address the above issues, we introduce an FL layer that allows inter-agent information exchange, and expedites the learning procedure  local knowledge sharing. We provide the details of our FL approach in Sec.~\ref{sec:federated}.


\subsection{Local RAN Slicing via DDQN Agent}
\label{sec:DQN}

Deep Q-network (DQN) is a popular reinforcement learning~\cite{TVT_1} algorithm that evolves from the well-known concepts of Q-learning and neural network function approximation.
%with the difference that the value of an action in a particular state, also known as Q-value, is not calculated by state action pairs, but rather obtained by exploiting the approximation capabilities of a neural network.
DQN represents a model-free approach. It stores the trajectory of experiences for each interaction with the environment in a replay buffer, as to update the network parameters without prior knowledge of the underlying environment statistics.
In the following, we will use the $i$ index interchangeably while referring to slices and DAs, assuming a one-to-one mapping of each DA with the corresponding network slice.
With focus on a single BS and a single decision interval %(we therefore drop the indexes $b$ and $t$ to reduce the clutter)
the design choices of our DQN model are as follows:\\
\textbf{State Space $\mathcal{S}$} We define the state of the $i$-th agent associated to the $b$-th BS as a tuple of local monitoring information $s_i^{(t)} = \{ ( \sigma_i^{(t)}, \lambda_i^{(t)}, \nu_i^{(t)} ) \mid \forall i \in \mathcal{I} \},$ where $\sigma_i^{(t)}$ is the SNR value, averaged over the duration of a decision time interval experienced by the users of the $i$-th slice, $\lambda_i^{(t)}$ is the aggregated traffic volume generated by the $i$-th slice over the time decision duration $\epsilon$,
and $\nu_i^{(t)}$ is the amount of available capacity left by the previous decisions of other agents.
\\%
\textbf{Action Space $\mathcal{A}$} Without loss of generality, we define $\iota$ as the minimum PRB allocation step, or \emph{chunk size}, and assume that the PRB allocation decision of the $i$-th agent can only take values that are an integer multiple of $\iota$.
It results that $\mathcal{A} = \{ \iota \cdot k \mid k = \{0,1, \dots, \floor{\frac{C}{\iota}} \} \}$.
Such discrete action space allows controlling the dimensionality of the action space and positively influences the learning process~\cite{LACO}.
\\%
\textbf{Reward $\mathcal{R}$} We adopt an iterative reward-penalty approach to guide the agent learning procedure, which translates into maximizing a reward function. An accurate PRB allocation should concurrently guarantee the satisfaction of transmission latency $\Lambda_i$ and the traffic requirements $ \lambda_i^{(t)}$, while avoiding both under-provisioning and over-provisioning of resources. Given the instantaneous slice traffic volume $\varphi_i^{(t)}$, and the corresponding allocation decision $a_{i}^{(t)} \in \mathcal{A}$, we can identify an \emph{allocation gap} $\alpha_{i}^{(t)} = \Gamma(a_{i}^{(t)},\sigma_i^{(t)}) - \varphi_i^{(t)}$. To measure the goodness of the action, we therefore introduce two variables, namely $\rho^{(t)}_{\text{up}}$ and $\rho^{(t)}_{\text{lower}}$, which characterize the upper and lower boundaries of the allocation gap as $\rho^{(t)}_{\text{up}} = 2\cdot\Gamma(\iota^{(t)},\sigma_i^{(t)}) $ and $\rho^{(t)}_{\text{lower}} = -\Gamma(\iota^{(t)},\sigma_i^{(t)})$. Accordingly, we define the instantaneous reward $r^{(t)}_i \in \mathcal{R}$ of the $i$-th agent as: %the following case function:
\[
    r_i^{(t)} =\begin{cases}
    \alpha^{(t)}_{i} -4 \rho^{(t)}_{\text{lower}}  & \text{if} \qquad \alpha^{(t)}_{i} < \rho^{(t)}_{\text{lower}}, \\
    (1- \frac{\alpha^{(t)}_{i}}{\rho^{(t)}_{\text{up}}}) \frac{\alpha^{(t)}_{i}}{\rho^{(t)}_{\text{up}}}  & \text{if} \qquad \rho^{(t)}_{\text{lower}} \leq \alpha^{(t)}_{i} \leq \rho^{(t)}_{\text{up}},\\
    -(\alpha^{(t)}_{i} - \rho^{(t)}_{\text{up}})                  & \text{if} \qquad \alpha^{(t)}_{i} > \rho^{(t)}_{\text{up}}.
\end{cases}
\label{eq:reward}
\]
Notably, the first case linearly penalizes the occurrence of under provisioning decisions, while the third case acts in a similar way on the over-provisioning cases. The middle case is the target scenario, which assumes correct PRB allocation decisions in response to the instantaneous slice traffic request.
We envision the multi-agent RAN slicing problem as a sequential procedure, where at the beginning of each decision interval $t$, the different agents perform local decisions according to a priority value $\mu_i$. Nevertheless, multiple and independent agents may perform inaccurate decisions and leave the subsequent agents with no spare resources, specially in the initial training phase. Therefore, at the end of each training period, we calculate a penalty
%\begin{equation}
%P_i^{(t)}=-\eta_i\mathds{1}\left(\sum\limits_{i\in\mathcal{I}} a_{i,b}^{(t)} > C_b | i \right), 
\begin{equation}
P_i^{(t)}=-\eta_i\mathds{1}\left(a_{i}^{(t)}  > \nu_{i}^{(t)} \right), 
\end{equation}
%that overrides the instantaneous agent reward $r_i$, where $\eta_i$ is the penalty coefficient of the $i$-th slice, and $\mathds{1}$ denotes the logical operator.
where $\eta_i$ is the penalty coefficient of the $i$-th slice, and $\mathds{1}$ denotes the logical operator. This penalty overrides the instantaneous agent reward $r_i$ if the decision $a_{i}^{(t)}$ is greater than the amount of spare resources left by the previous decisions of the other agents, that in turn prevents the agents to exceed the available resources at the base station.
This design choice is justified by the results provided in Sec.\ref{single_bs_scenario}.
%common (i.e., for all the agents deployed on the same base station $b$)

% \tbd{ Indeed, the DRL-based approaches can dynamically forecast and control the changed resource requirements and complex behaviour of network slices.It is still difficult for the traditional methods to cope with dynamics and complex behaviour of network where they only enable to consider low-dimensional state spaces and temporary actions with the same rules. Moreover, the formulated problem can suffer from the curse of dimensionality due to the inor-dinately large size of the continuous state and action spaces with increasing the number of service demands for different slices, so it cannot be solved with traditional methods. As we discuss in Seq. II, the model-free DRL approach does not need to exploit the transition probabilities governing the underlying MDP and thereby directly learns the policy and action-value functions concerning trajectories of experiences.In the subsequent section, we will describe the working flow and problem-solving in detail.}

\begin{figure}[t!]
\centering
\includegraphics[width=.7\columnwidth, trim={1cm 0 2cm 0},clip]{DDQN.new.pdf}
\caption{ \small An illustration of DDQN workflow.}
\vspace{-6mm}
\label{fig:DDQN}
\end{figure}

\textbf{Training of Agents}
The training of the local agent implies the characterization of the action-value function $Q \colon \mathcal{S} \to \mathcal{A}$.
Let us define the policy $\pi$ as a probabilistic function mapping states to actions. The agent makes decisions and selects the corresponding actions based on $\pi$, determining the best action for each state. Under a given policy $\pi$, the action-value function can be defined as, $Q_{\pi}(s^{(t)}, a^{(t)}) = \mathbb{E}_{\pi}\left[\sum_{n=0}^{\infty}\left(\gamma^{n} r^{(t+n+1)} | (s^{(t)}, a^{(t)}) \right)\right]$, where $\gamma \in [0,1]$ is a discount factor that %indicates the importance
weights the short-sighted and far-sighted reward, and $n$ is the temporal index. According to Bellman's equation~\cite{Bellman}, the optimal state-action value function can be expressed as ${Q}^{\star}(s^{(t)},a^{(t)}) = \mathbb{E}\left[r^{(t)}+\gamma\underset{a^{(t+1)}}{\max}{Q}^{\star}{(s^{(t+1)},a^{(t+1)} | s^{(t)}, a^{(t)}}) \right]$,
%where $a^{\prime}$ denotes the action that agent takes in the next state $s^{(t+1)}$
and thereby the Q-learning update rule based on temporal difference (TD)~\cite{TD-error} is given by, 
\begin{multline}
Q(s^{(t)},a^{(t)}) \leftarrow
 Q(s^{(t)},a^{(t)}) +\\ \xi[r^{(t)}+\gamma \underset{a^{(t+1)}}{\max}Q(s^{(t+1)}, a^{(t+1)})
- Q(s^{(t)},a^{(t)})],
\end{multline}
where $\xi$ is the learning rate. DQN adopts deep neural network (DNN) to approximate the state-action value and surmount the curse of dimensionality concerning inordinate large state spaces. To limit the catastrophic interference problem~\cite{catastrophic_g}, which is the tendency of a neural network to forget about previously learned information upon learning new ones, we adopt an experience replay strategy. 
% Let $s_{i,b}^{(t)}$, $a_{i,b}^{(t)}$ and $r_{i,b}^{(t)}$ denote the state, action and reward of local agent in the $i$-th slice and $b$-th BS at time $t$, and then introduce $\beta_{i,b}$ as the experience buffer.
%
In particular, let us introduce $\beta_{i}$ as the experience buffer.
As depicted in Fig.~\ref{fig:DDQN}, in every training interval, we store the tuple ${({s}_{i}^{(t)},{a}_{i}^{(t)},{r}_{i}^{(t)},{s}_{i}^{(t+1)})}$ describing the instantaneous experience generated by the agent while interacting with the environment, and sample from $\beta_{i}$ a random batch of past experiences to regularize the training.

%
Additionally, DQNs are well known to provide an overoptimistic value estimation. We alleviate this problem by leveraging an additional DQN network, in the form of DDQN~\cite{DoubleDQN}-\cite{DDQN_VNF_OPT}. With a slight abuse of notation, let us introduce $Q(s_{i}^{(t)}, a_{i}^{(t)}; \theta_{i}^{(t)})$ and $Q(s_{i}^{(t)}, a_{i}^{(t)}; \tilde{\theta}_{i}^{(t)})$ as the online network and target network respectively, where $\theta_{i}^{(t)}$ and $\tilde{\theta}_{i}^{(t)}$ denote the model parameters. To optimize the parameter set $\theta_{i}^{(t)}$ and approximate the optimal action-value function ${Q}^{\star}(s_{i}^{(t)},a_{i}^{(t)})$, we use the following loss function,
\begin{equation}
    L(\theta_{i}^{(t)}) = \mathbb{E}[y_{i}^{(t)} - Q(s_{i}^{(t)}, a_{i}^{(t)}; \theta_{i}^{(t)})]^2,
\end{equation}
where $y_{i}^{(t)} = r_{i}^{(t)} + \gamma \underset{a^{(t+1)}_{i}}{\max}Q(s_{i}^{(t+1)}, a^{(t+1)}_{i}; \tilde{\theta}_{i}^{(t)})$ and $\tilde{\theta}_{i}^{(t)}$ is copied from $\theta_{i}^{(t)}$ at the end of each episode.
Finally, the objective function of the DDQN model can be written as,
\begin{equation}
    y_{i}^{(t)} = r_{i}^{(t)} + \gamma Q(s_{i}^{(t+1)}, \underset{a^{(t+1)}_{i}}{\argmax}Q(s_{i}^{(t+1)}, a^{(t+1)}_{i}; \theta_{i}^{(t)}); \tilde{\theta}_{i}^{(t)})),
\end{equation}
where $\theta_{i}^{(t)}$ is a local training model used for selecting actions, and $\tilde{\theta}_{i}^{(t)}$ is used to evaluate their values according to a different policy, thus mitigating over-estimations issues and improving the decision agents' performances~\cite{DoubleDQN}. The loss function estimates the difference between true action-value and target action-value. As the overall training procedure aims at minimizing this loss function, we adopt stochastic gradient descent (SGD) approach~\cite{Stochastic-gd} to pursue this goal.
The local agent training procedure is summarized in Algorithm~\ref{algo:DRL}. The overall local process is aided by a federation scheme (lines 1-6) described in details in the following subsection.

\begin{algorithm}[!t]
%\scriptsize
\small
\SetKwInOut{Input}{Input}
\SetKwInOut{Output}{Output}
\SetKwInOut{Return}{return}
\SetKwInOut{Initialize}{Initialize}
\Input{  $t, T, \hat{T}, i \in \mathcal{I}, \theta_{i,b}^{(t)}, \Omega_{k}^{(t)}$ \;}
\Output{Improved DDQN model $\theta_{i,b}^{(t+1)}$ \;}
\Initialize{ $\theta_{i,b}^{(0)}, \forall b \in \mathcal{B}, t = 0 $\; }
%  \While{ $t < T$}{
      \If{$mod(t,\hat{T})==0 \land t > 0$}{ 
        Upload $\theta_{i,b}^{(t)}$ \;
        Wait for Algorithm~\ref{algo:FDRL}\;
        \emph{ \#Get FL model and update the local one}\;  
        $\theta_{i,b}^{(t+1)} \leftarrow \Omega_{k}^{(t)}$\;
   } %if
    \For{ $b \in \mathcal{B}$, in parallel}{
        $\nabla L(\theta_{i,b}^{(t)})  \leftarrow $ Local model training\;
        $\theta_{i,b}^{(t+1)} \leftarrow \theta_{i,b}^{(t)} + \nabla L(\theta_{i,b}^{(t)})$\; % 
    } %For
%  }%While
 %\Return{$\Omega_{i}^{(t+1)}, \forall i \in \mathcal{I}$}
\caption{DRL RAN resource allocation for the $i$-th slice}
\label{algo:DRL}
\end{algorithm}
\setlength{\textfloatsep}{0.5pt}% Reduce space after algo

% The Q-value is  in Q-learning as a model-free approach~\cite{JSAC_Puj,Globe_far} is given by
% \begin{multline}
% Q(s^{(t)},a^{(t)})=
% Q(s^{(t)},a^{(t)}) +\\ \alpha [r^{(t+1)}+\gamma \underset{a}{\max}Q(s^{(t+1)}, a)
% - Q(s^{(t)},a^{(t)})].
% \end{multline}
% where $\alpha$ is learning rate. %$r^{(t)} \in R^{(t)}(s^{(t)}, a^{(t)})$. 

%nlike the tabular and non-parametric approach, Q-network is assisted with deep neural network (DNN) in deep Q-network (DQN) to surmount the curse of dimensionality concerning inordinate large state spaces. DQN represents a model-free approach. It stores the trajectory of experiences for each interaction with the environment (slices) in replay buffer and then selects data from the latter to update the network parameters without prior knowledge of the underlying environment statistics. 

%
%We take a random batch $\hat{\beta}_{i,b}$ for all transitions ${({s}_{\hat{\beta}_{i,b}}^{(t)},{a}_{\hat{\beta}_{i,b}}^{(t)},{r}_{\hat{\beta}_{i,b}}^{(t)},{s}_{\hat{\beta}_{i,b}}^{(t+1)})}$ of $\beta_{i,b}$.
%
%\tbd{
%More into details, one DQN model learns during the experience replay (just like standard DQN model does), while the second one is a copy of the first model at the time of the last training episode. Standard DQN approaches use the same values both to select and to evaluate an action. 
%To solve problem of large state and action spaces, we parametrize the value function $Q(s_{i,b}^{(t)}, a_{i,b}^{(t)}; \theta_{i,b}^{(t)})$ where $\theta_{i,b}^{(t)}$ are the parameters of the DNN.
%

%Indeed, $\theta_{i,b}^{(t)}$ is the local training model concerning currents state $s_{i,b}^{(t)}$ and the triggered action $a_{i,b}^{(t)}$ 

%}


\subsection{Federated DRL for RAN Slicing}
\label{sec:federated}
FL allows training machine learning models across multiple decentralized entities which have access to a limited set of the overall data available. Conversely to multi-agent reinforcement learning, which defines a set of autonomous agents that observe a global state (or partial state) of the system, select individual actions and receive individual rewards, FL allows to collaboratively learn a shared prediction model by iteratively aggregating multiple model updates, thus decoupling the learning procedure from the need of centralized data sources. A refined version of the original models, combination of multiple local models according to specific federation strategies, is then shared to the agents allowing to significantly improve the learning rate, ensure privacy~\cite{not_just_privacy} and provide better generalization~\cite{fl_Aled}.

As depicted in Fig.~\ref{fig:architecure}, within the context of our FDRL-based framework each agent trains a local DDQN model $\theta_{i,b}^{(t)}$ and shares its experience, under the form of model hyperparameters, to those entities belonging to the corresponding federation layer. This iterative training approach enables each federation layer to aggregate the collected knowledge of single agents into a global updated model $\Omega_{i}^{(t+1)}$, usually stored into a cloud platform or a nearby edge platform to allow faster feedbacks.
In order to enhance efficiency and avoid communication overhead, we allow the federation layer to collect the local models (and share the updated ones) only every $\hat{T}$ decision intervals, defining this time period as \emph{federation episode}.
Different strategies can be adopted to derive the global federated model, each one implementing a predefined federation strategy function $f_{\textit{strategy}}(\cdot)$.

In \emph{Average} federation strategy, dubbed as \emph{FDRL} in the following of this work, the collective federation model for the next time interval $\Omega_{i}^{(t+1)}$ is derived as the simple average of the incoming model weights belonging to all the agents, as
\begin{equation}
    \Omega_{i}^{(t+1)} = \frac{1}{B}\sum\limits_{b \in \mathcal{B}}\theta_{i,b}^{(t)}.
    \label{eq:averaging}
\end{equation}
%
Aggregated mobile traffic demands follow repetitive spatio-temporal trends due to human activities~\cite{Spatio-LTM}. %such as vehicular commuting and day-night shifts~\cite{pi_ROAD}.
In this context, it is expected that a good characterization of such processes would allow more accurate forecasting of the network utilization and, in turn, enable an efficient and even proactive planning of the resource allocation.
%
However, as highlighted in~\cite{DeepCogInfocom19}, it is not enough to leverage the geographical locations and related spatial proximity of the BS to obtain a comprehensive view of traffic demands, as the land usage of the slice resources may differ even within base stations belonging to the same geographical areas. This introduces an additional issue in our framework, as not all the federated agents should exchange knowledge with each other, nor this should be restricted to only nearby entities. To address this fundamental issue, in the following we propose a clustering algorithm to guide DA subsets definition, based on network monitoring traces and their similarity.

\begin{figure}[h]
\centering
\includegraphics[clip, trim = 0cm 0cm 0cm 2cm, width=\columnwidth]{Example_DTW.pdf}
\caption{\small Example comparison of Euclidean distance against Dynamic Time Warping distance over traffic demand time series.}
% \vspace{-5mm}
\label{fig:DTW}
\end{figure}

\subsection{Dynamic Traffic-Aware Agent Selection}
\label{subsec:dynamic_clustering}
% \tbd{Discuss agent priority $\mu_i(\theta_i,\mathcal{U}_i)$}
Given the rapid spatio-temporal variation of the traffic demand due to end-user mobility, we advocate for the setup of a clustering algorithm to derive the subset of slice agents that should exchange their local knowledge, while considering both mobility and traffic demand variations.
Let us introduce $\tau_{i,b}$ as the time series describing the downlink traffic demand of the $i$-th slice instantiated over base station $b$.
Then, for each pair $j,z\in \mathcal{B}$, we can compute the similarity of the recorded monitoring information as $DTW^{(j,z)}=f_{DTW}(\tau_{i,j},\tau_{i,z})$, where $f_{DTW} (\cdot)$ is the Dynamic Time Warping distance~\cite{DTW}, a state-of-the-art distance metric for time series analysis\footnote{We refer the reader to~\cite{DTW} for an exhaustive explanation.}.
DTW is particularly suitable in our scenario as it allows, conversely to standard distance metrics, e.g., Euclidean distance, to calculate accurate similarity value even in presence of differently sized sequences, and independently of their time shift. An example of DTW distance calculation is depicted in Fig.~\ref{fig:DTW}, where it can be noticed how maximum and minimum values of the traces are correctly mapped to each other. The pairwise distances are then collected into the distance matrix $\mathbf{D}= (DTW^{(j,z)}) \in \mathbb{R}^{|\mathcal{B}| \times |\mathcal{B}|}$, and provided as input of our clustering algorithm.
DTW has linear space complexity, but quadratic time complexity. To reduce the latter, a number of options are available. In our case, we limit the maximal shift by setting a fixed time a window of few hours, thus reducing the complexity even in case of long sequences. Nevertheless, recent work from~\cite{DTW_Barrier} proposed a novel efficient implementation which breaks the quadratic time complexity to $O(n^2 \log{n})$, where $n$ is the length of the sequence. 
%
To perform the final classification, we rely on an extended version of the Density-based spatial clustering of applications with noise (DBSCAN) algorithm, introduced in~\cite{DBSCAN}. DBSCAN is a non-parametric density-based clustering algorithm that allows finding the most representative points within a dataset (also known as \emph{core samples}) based on their density in a multi-dimensional space, and expands clusters from them. It expects two inputs: $\epsilon_d$, representing the maximum distance between two samples for one to be considered as in the neighborhood of the other, and $n_{min}$, which defines the minimum number of samples in a neighborhood of a point to be considered as a core sample.

\begin{algorithm}[!t]
%\scriptsize
\small
\SetKwInOut{Input}{Input}
\SetKwInOut{Output}{Output}
\SetKwInOut{Return}{return}
\Input{$t, T, \theta_{i,b}^{(t)}$ $\forall b \in \mathcal{B}$, $\tau_{i,b}^{(t)}$ $\forall b \in \mathcal{B}$, $\epsilon_d$, $n_{min}$}
\Output{Improved federation models $\Omega_{i,k}^{(t+1)}, \forall \Psi_k \in \Psi$}
\emph{\#Define clusters and send initial/updated model}\;
        Collect $\tau_{i,b}^{(t)}$, $\forall b \in \mathcal{B}$ \;
        Compute $\mathbf{D} = (DTW^{(j,z)}), \forall j,z \in \mathcal{B}$\; 
        $\Psi_k \in \Psi \leftarrow DBSCAN( \mathbf{D},\epsilon_d,n_{min})$\;
 \While{ $t < T$}{
  \If{$mod(t,\hat{T})==0 \land t > 0$}{
          \For{ each $\Psi_k \in \Psi$, in parallel}{
                Collect $\theta_{i,k}^{(t)}, \forall b \in \Psi_k$\;
                \emph{ \#Derive FL models based on Fed. strategy}\;  
                $\Omega_{i,k}^{(t+1)} \leftarrow f_{\textit{strategy}}(\theta_{i,b}^{(t)}, \forall b \in \Psi_k)$\;
                $\theta_{i,b}^{(t+1)} \leftarrow \Omega_{i,k}^{(t+1)}$ $\forall b \in \Psi_k$\;
         }
        \emph{ \#Return updated local models}\;  \
        \Return{$\theta_{i,b}^{(t+1)}, \forall b \in \mathcal{B}$}
    }
    Run Algorithm~\ref{algo:DRL}\;
 }
 %\Return{$\Omega_{i}^{(t+1)}, \forall i \in \mathcal{I}$}
\caption{RAN resource orchestration for the $i$-th federation layer}
\label{algo:FDRL}
\end{algorithm}
\setlength{\textfloatsep}{0.5pt}% Reduce space after algo

Given the above, at the end of each federation episode, we can derive in a dynamic way (and based on updated mobile monitoring information) the clusters $\Psi_k \in \Psi$, $k=\{1,\dots,|\mathcal{I}|\}$, where $\Psi$ is the cluster set. Each cluster includes the set of base station $b \in \Psi_k$ that should be involved in the following model update procedure. Therefore, the framework spawns multiple federation models $\Omega_k$, one for each detected cluster $k$, which evolve in parallel till the next federation episode. The pseudocode of our FDRL-based approach for RAN slicing resource orchestration is listed in Algorithm~\ref{algo:FDRL}.
We remark that in our framework multiple instances of Algorithm ~\ref{algo:FDRL}, i.e., one for each slice $i\in\mathcal{I}$, are deployed to build the corresponding FL domain for a given federation strategy $f_\textit{strategy}(\cdot)$. It follows that the updated federation model, combination of the information coming from the elements of the cluster $\Psi_k$ (described in line 10), can be derived following the \texttt{Full-Cluster} strategy, namely $f_{\textit{FC}}(\cdot)$, as:
\begin{equation}
    \Omega_{i,k}^{(t+1)} = \frac{1}{|\psi_k|}\sum\limits_{b \in \psi_k}\omega_{i,b}^{(t)}\theta_{i,b}^{(t)}, \qquad \forall \psi_k \in \psi
\end{equation}
where $|\psi_k|$ is the cardinality of $\psi_k$ , and $\omega_{i,b}^{(t)} = \frac{\hat{r}_{i,b}^{(t)}}{\sum\limits_{b \in \psi_k} \hat{r}_{i,b}^{(t)}}$ is a weight parameter.
It should be noted that within these settings, the federation step will occur among models with high degree of similarity, thus favoring the \emph{specialization} of the agents towards the most common traffic statistics.

Other complementary approaches can be defined to guide the agent selection and subsequent federation model update. In particular, upon the definition of the cluster set $\Psi$, we introduce \texttt{Random Representative} strategy $f_{\textit{RR}}(\cdot)$ as a baseline approach, which randomly selects a representative from each cluster:
\begin{equation}
    \psi_{\textit{random}} = \{x | x = \textit{rand}(\psi_k) , \qquad \forall \psi_k \in \psi\}
\end{equation}
and consequently defines the updated federated model as:
\begin{equation}
    \Omega_{i}^{(t+1)} = \frac{1}{|\psi_{\textit{random}}|}\sum\limits_{b \in \psi_{\textit{random}}}\omega_{i,b}^{(t)}\theta_{i,b}^{(t)}.
\end{equation}
Similarly, let us introduce the \texttt{Best Representative} strategy, as a method that derives the updated federation model by selecting a representative agent from each cluster as follows:
\begin{equation}
    \psi_{\textit{best}} = \{x | x = \underset{k}{\argmax R_k} , \qquad \forall \psi_k \in \psi\}
\end{equation}
where $R_k$ is the cumulative reward within the past federation episode.
Thus, the model update strategy \texttt{Best Representative} $f_{\textit{BR}}(\cdot)$, can be defined as:
\begin{equation}
    \Omega_{i}^{(t+1)} = \frac{1}{|\psi_{\textit{best}}|}\sum\limits_{b \in \psi_{\textit{best}}}\omega_{i,b}^{(t)}\theta_{i,b}^{(t)}.
\end{equation}
By combining single models derived from each cluster, we can pursue higher \emph{generalization} of performances, i.e., aim at a federated model able to deal with heterogeneous traffic statistics.

\begin{figure}[t]
\centering
\includegraphics[width=0.7\columnwidth]{Figures/oran_arch.pdf}
%\includegraphics[clip, trim = 1cm 0cm 10cm 0cm,width=0.8\columnwidth]{Figures/oran_arch_v2}
\caption{\small O-RAN compliant system architecture.}
% %% \vspace{-3mm}
\label{fig:ORAN}
\end{figure}
\section{O-RAN Compliance}
\label{sec:ORAN}

The design of our solution closely follows the O-RAN framework~\cite{ORAN_architecture}.
O-RAN represents a worldwide effort to reach new levels of openness in next-generation virtualized radio access networks (vRANs). Driven by major carriers, it aims at disrupting the vRAN ecosystem traditionally dominated by a small set of player by breaking vendors' lock-in and opening the business market~\cite{ORAN_NEC}.
The most important functional components introduced by O-RAN are the non-real-time (non-RT) radio intelligent controller (RIC) and the near-RT RIC~\cite{ORAN-infocom}.
%
The main functionality provided by the Non-RT RIC it to support RAN optimization over relatively large time scales (e.g., seconds or minutes). This often implies machine learning (ML) model training and subsequent control policy definition, to be enforced via the A1 interface towards the distributed Near-RT RICs.
%
The Near-RT RIC is a logical function that enables near-real-time optimization and control, as well as data monitoring of O-RAN central unit (O-CU) and O-RAN distributed unit (O-DU) nodes (which support eNBs/gNBs deployment as virtualized network functions (VNFs)) in near-RT timescales (between 10 ms and 1 s).
%
Fig.~\ref{fig:ORAN} depicts a high-level view of the O-RAN architecture, highlighting the synergies with respect to our proposed approach. In particular, we envision our federated learning and dynamic agent selection module as co-located with Non-RT RIC, which handles the A1's Policy Management Service to enforce radio policies.
On the other side, local agents co-located with the Near-RT RICs collect this information, perform local decisions, and exploit the E2 interface to forward resulting radio policies to the base station. The same E2 interface would allow the local agent to gather base station KPIs for the purpose of model training and monitoring.

\change{As depicted in Fig.~\ref{fig:sw_architecture}, we implement our framework in Python programming language, exploiting OpenAI Gym library~\cite{Gym_Broc} and interfacing DRL agents with a custom base station simulator environment, which includes virtual transmission queues and main PHY/MAC/RLC functionalities, together with O-RAN E2 interface to allow gathering the slice networking statistics from each distributed unit (O-DU), and to enforce PRB policy decisions in the BS slice scheduler based on defined state space and action space in Sec.~\ref{sec:DQN}. Finally, as described in Sec.~\ref{sec:federated}, a federation layer connects the DRL agents of the $i$-th slice to enable inter-agent information exchange and expedite the overall learning procedure. The procedure is summarized in Algorithm~\ref{algo:DRL} and~\ref{algo:FDRL}. }
%\begin{figure*}[!t]
	%\centering
	  % \begin{subfigure}[t]{.33\textwidth}
	%	\centering
       % \includegraphics[trim = 0cm 0cm 0cm 0cm, clip, height=5.5cm,keepaspectratio]{Simulation_Environment.png} %width=\columnwidth
        %\subcaption{\small Software architecture overview.}
        %\label{fig:sw_architecture}
   % \end{subfigure}%
   % \hfill
    %\begin{subfigure}[t]{.33\textwidth}
    %	\centering
       % \includegraphics[trim = 0cm 0cm 0cm 0cm, clip, width=\columnwidth ]{Average_Reward_Non-FED_final.pdf}
       % \subcaption{\small Performance comparison of different \\local decision algorithms.
        %}
    %\label{fig:Local_Model_Perf}
   % \end{subfigure}%
    %\hfill
    %\begin{subfigure}[t]{.33\textwidth}
    	%\centering
        %\includegraphics[trim = 0cm 0cm 0cm 0cm, clip, width=\columnwidth]{Average_Reward_FDRL_vs_DRL_final.pdf}
       % \subcaption{\small Comparison of average reward for DDQN model in DRL and FDRL settings.}
        %\label{fig:DRL_vs_FDRL}
    %\end{subfigure}%
    %\caption{\small System architecture and performance comparison for a single base station scenario. The
%curves are smoothed for visual clarity with respect to confidence bands.}
    % Comparison of the reward obtained with different local agent strategies with and without federation. {\color{red}Obtained averaging the reward of $100$ BSs??}, single base station perspective. \lz{$\hat{T}$=5, 15k  users}} \tbd{explain D-DDPG in text and update description} \tbd{shrink the images (b) and (c) fix the max value of the reward to $0$ - Add per-slice view as it was before
    %\vspace{-5mm}
%\end{figure*}%

\begin{figure}[t]
\centering
\includegraphics[clip, trim = 0cm 0cm 0cm 0cm,width=6cm]{Figures/Simulation_Environment.png}
\caption{\small Software architecture and protocol stack overview.}
\label{fig:sw_architecture}
\end{figure}

\section{Performance Evaluation}
\label{sec:perf_eval}
In this section, we evaluate our proposed architecture  numerical simulations on a dedicated server, equipped with two Intel(R) Xeon(R) Gold 5218 CPUs @ 2.30GHz and two NVIDIA GeForce RTX 2080 Ti GPUs. Moreover, the DNNs are implemented based on TensorFlow-gpu version 2.5.0. In neural network architecture, we use two fully connected layers with $24$ neurons activated by ReLU function for each layer where the target network is updated per episode and each episode consists of $5$ decision intervals, or epochs. Each decision interval has a duration of $\epsilon = 60$ seconds, during which local monitoring information is collected to build the local agent state. Online and Target networks are characterized by the same DNN structure. 
The hyperparameter tuning depends highly on capability, scenario, and technology used~\cite{Globe_far}. The network parameters are updated using the Adam optimizer~\cite{ADAM}. The discount factor $\gamma$ and learning rate $\xi$ are set to be $0.99$ and $0.001$ respectively. The replay buffer size of each agent $\beta_{i,b}$ is set to $20000$ samples, out of which a batch of $32$ samples is extracted for each training interval. Without loss of generality, We set $\eta_i=100$ as penalty value for all the slices. In order to provide a comprehensive overview, we first evaluate single base station settings, focusing on the capabilities of single agents to deal with RAN resource allocation. Then, we address a more realistic scenario considering a multi-slice deployment over several RAN nodes, accounting for end-user mobility and variable traffic demands.


\subsection{Local Agent Performance Assessment}
\label{single_bs_scenario}
In our proposed framework, DRL agents optimally allocate radio resources to each slice, while a federation layer enables a periodical exchange of the DRL's parameter values to improve the learning process across multiple agents of the same slice.%\tbd{As depicted in Fig.~\ref{fig:sw_architecture}, we implement our framework  in Python programming language, exploiting OpenAI Gym library~\cite{Gym_Broc} and interfacing DRL agents with a custom base station simulator environment, which includes virtual transmission queues and main PHY/MAC/RLC functionalities, together with O-RAN E2 interface to allow gathering the slice networking statistics from each Distributed Unit (O-DU), and to enforce PRB policy decisions in the BS slice scheduler. Finally, as described in Sec.~\ref{sec:federated}, a federation layer connects the DRL agents of the $i$-th slice to enable inter-agent information exchange and expedite the overall learning procedure.}
\begin{figure}[t]
\centering
\includegraphics[clip, trim = 0cm 0cm 0cm 0cm,width=\columnwidth]{Figures/Average_Reward_single_bs_final.pdf}
\caption{\small The convergence performance of different local decision algorithms and an FDRL approach for a single decision agent.} %base station. The curves are smoothed for visual clarity.
% \vspace{-1mm}
\label{fig:Local_Model_Perf}
\end{figure}
% \begin{table}[h!]
% \caption{Comparison of hyperparameters tuning in simulation.}
% %\ra{1.3}
% \scriptsize
% \centering
% \begin{tabular}{@{}lccccccc@{}}\toprule
% \textbf{Architecture} & \textbf{DDPG} &  \textbf{SAC} & \textbf{TD3} \\ \midrule
% \textbf{Method} & Actor-Critic  & Actor-Critic  & Actor-Critic \\ \hdashline
% \textbf{Model Type} & MLP  & MLP  & MLP\\ \hdashline
% \textbf{Policy Type} & Deterministic &  Stochastic &  Deterministic\\ \hdashline
% \textbf{Policy Evaluation} & TD learning &  Double Q-learning & CDQ \\ \hdashline
% \textbf{No. of DNNs}& 4  & 4  & 6\\ \hdashline
% \textbf{No. of Policy DNNs}& 1  & 1  & 1  \\ \hdashline
% \textbf{No. of Value DNNs}& 1  & 2  & 2 \\ \hdashline
% \textbf{No. of Target DNNs}& 2  & 1  & 3 \\ \hdashline
% \textbf{No. of hidden layers}& 2  & 2  & 2 \\ \hdashline
% \textbf{No. of hidden units/layer}& 200  & 256 &  400/300 \\ \hdashline
% \textbf{No. of Time Steps}& $2e6$ &  $2e6$ &  $2e6$  \\ \hdashline
% \textbf{Batch Size}& 64  & 256  & 100\\ \hdashline
% \textbf{Optimizer}& ADAM &  ADAM &  ADAM \\ \hdashline
% \textbf{ADAM ($\beta_1, \beta_2$)}& (0.9, 0.999) &  (0.9, 0.999)  &  (0.9, 0.999) \\ \hdashline
% \textbf{Nonlinearity}& ReLU  & ReLU & ReLU \\ \hdashline
% \textbf{Target Smoothing $(\tau)$}& 0.001 &  0.005  & 0.005 \\ \hdashline
% \textbf{Exploration Noise}& $\theta,\sigma=0.15,0.2$  & None &  $\mathcal{N}(0,0.1)$  \\ \hdashline
% \textbf{Update Interval}& None & None  & 2\\ \hdashline
% \textbf{Policy Smoothing}& None  & None  & - \\ \hdashline
% \textbf{Expected Entropy$(\mathcal{H})$}& None & -dim(Action) & None   \\ \hdashline
% \textbf{Actor Learning Rate}& 0.0001 &  0.0001  & 0.001  \\ \hdashline
% \textbf{Critic Learning Rate}& 0.001 & 0.0001 &  0.001\\ \hdashline
% \textbf{Reward Scaling}& 1.0  & 0.2  & 1.0 \\ \hdashline
% \textbf{Discount Factor}& 0.99  & 0.99  & 0.99  \\ \hdashline
% \textbf{Replay Buffer Type}& Simple &  Simple &  Simple  \\ \hdashline
% \textbf{Replay Buffer Size}& $1e6$ &  $1e6$ &  $1e6$\\ \hdashline
% \textbf{Max Episode Length}& $50$ &  $50$ &  $50$\\
% \hdashline
% \textbf{Seed}& System time &  System time &  System time  \\
% \bottomrule
% \end{tabular}
% \end{table}
%
\begin{figure*}[!t]
\centering
\includegraphics[clip, trim = 0cm 0cm 0cm 0cm, width=0.85\textwidth]{Mobility_Simulation_medium-crop.pdf}
\caption{\small Mobility Statistics for different network slices. (Left) CDF of end-users radius of gyration, (Center) Average Spatial distribution of slice users over 24h time span, and corresponding DTW distance matrix. (Right) Example of resulting clustering for the URLLC slice case, each color defines a different cluster.}
\label{fig:Mobility}
%\vspace{-1mm}
\end{figure*}
First, we compare the performances of different RL algorithms when dealing with radio resource allocation, without involving federated learning. To this aim, we consider a base station scenario including $3$ network slices, i.e., an ultra reliable low latency communication (URLLC) kind of slice, an enhanced mobile broadband (eMBB), and one dedicated to massive machine-type communications (mMTC) traffic, each one characterized by the SLA latency values of $\Lambda_i = [10, 40, 20]$ ms, respectively~\cite{GAN_Powered}.
Regarding the throughput requirements, we do not assume any fixed value as it would depend on the random mobility pattern of the end users and their generated traffic. Instead, we enforce (with a slight abuse of notation), $\lambda_{i,b}^{(t)} = \varphi_{i,b}^{(t)}$ for every decision interval to let the agents adapt their decisions to the instantaneous traffic volumes.
% focus on generating an overall traffic load that fits within the capacity region of the base stations, and 
% networking requirements \tbd{as SLA throughput $\lambda_i = [10, 30, 1]$ Mbps},
We instantiate a DA in every base station for each slice. We model the instantaneous traffic demand of each slice as the realization of a Poisson distribution with mean value $\lambda_i$, and emulate the SNR variability extracting its instantaneous values from a Rayleigh distribution with the average value set to $25$ dB. Moreover, we set $\iota = 10$ PRBs as the minimum resource allocation step.
Fig.~\ref{fig:Local_Model_Perf} depicts the training procedure for the eMBB slice, comparing different local decision algorithms. In particular, we consider the single DQN approach, which implements standard Q-Learning procedures, discretized Deep Deterministic Policy Gradient (d-DDPG) a popular reinforcement learning algorithm~\cite{DDPG}, and our DDQN scheme. 

We let the scenario run for $800$ federation episodes, and depict the results in terms of cumulative reward, as defined in \change{Eq.~\eqref{eq:reward}}.
The variability of the network slicing environment leads to experience learning curves with high fluctuations. For visual clarity, results are averaged over $10$ simulations. As expected, the DQN approach hardly copes with the definition of suitable PRB allocation policies, providing lower performances both in terms of cumulative reward and convergence time. Similarly, d-DDPG suffers the temporal periodicity of the traffic demand, resulting in a steep learning curve that soon saturates to suboptimal performances. 
%We highlight that such behaviour may be mitigated  hyperparameter optimizations and longer %training, as for example showed by~\cite{Globe_far}, which is however out of the scope of this work. 
Conversely, after an initial exploration phase, the DDQN approach is able to allocate in a more consistent way correct amount of PRBs to each slice according to the corresponding real-time traffic and latency demands. It is worth highlighting that in terms of convergence time, in general, FDRL schemes do not necessarily provide better performances when compared against standard DRL approaches. In fact, one of the main features of FL is that it allows local DRL agents to indirectly gain knowledge on a wider state space, extending the local experience with that coming from other decision entities deployed within the same environment. This enables the DAs to provide more robust performances when deployed in realistic environments. Nevertheless, the same Fig.~\ref{fig:Local_Model_Perf} provides an overview of the local model training procedure, with and without the adoption of FL schemes. In our considered scenario, it can be noticed how DRL curves (dashed lines in the plot) present slower convergence time and higher fluctuations when compared against Federated DDQN approach (solid line in the plot).
%Note that curves are averaged for visual clarity over the set of agents, according to their respective slice type.
Additionally, DRL curves present lower cumulative reward after $400$ episodes, suggesting a lower capability of the DAs to adapt their decisions at the fast-changing network slicing environment considered in our work.


\subsection{System-level Simulations}
\label{multi_bs_scenario}

\subsubsection{Mobility and Traffic Demand Characterization}
In order to validate our framework in realistic settings, we consider the city of Milan, Italy, as scenario of study. We collect city-wide RAN deployment information including more than $50$ BSs from publicly available sources\footnote{https://opencellid.org/}, and simulate realistic human mobility patterns leveraging the work of~\cite{Mobility_model}.
%
The density exploration and preferential return (d-EPR) algorithm allows capturing mobility patterns by specifying as input the geographical position of the base stations together with several probabilistic parameters. We let the model evolve adopting the default parameters described in~\cite{dEPR}. By defining the location relevance on the mobility space, we can influence the next-hop selection of each end-user, therefore emulating a higher concentration of mobile devices in specific areas of the city over time, e.g., the daily commuting over the city center during working days.
Fig.~\ref{fig:Mobility} (Left) depicts the CDF of the resulting radius of gyration per slice, aggregating the results over $15000$ end-users equally distributed among the different slices. 
Without loss of generality, we consider the set of BSs characterized by the same radio capacity $C_b=100$ PRBs, and assume the same $3$ slices introduced above simultaneously running over all the BSs. 
%
Fig.~\ref{fig:Mobility} (Center) depicts the resulting spatial distribution of the end-users, accounting for a temporal time span of a full day.
From the picture, it can be noticed how the spatial distribution of slice users is actually similar along with the slice set, and influenced in specific areas of the city by the high density of RAN nodes. This is due to the d-EPR algorithm, which favors the next-hop destination of each user to happen towards a nearby point of interest, or, in our settings, the closest base station location.
The instantaneous traffic demand of each end-user is derived starting from the values reported in Sec.\ref{single_bs_scenario}%Table~\ref{tab:simulation_parameters}
, weighted by a temporal factor to account for the traffic demand fluctuations typical of mobile network scenarios, as those presented for example by~\cite{pi_ROAD} and~\cite{Wang_18}.
%
In the lower part of Fig.~\ref{fig:Mobility}, we depict the resulting distance matrix $\mathbf{D}$ of each, i.e., per slice, downlink traffic demand, calculated at the beginning of every federation episode for each base station pair over the past $24$ hours. As detailed in Sec.~\ref{subsec:dynamic_clustering}, this matrix is used as input to an instance of the DBSCAN algorithm to derive the set of DAs (belonging to the $i$-th slice) which should be involved in the next federation episode and model exchange. Fig.~\ref{fig:Mobility} (Right) shows the resulting output clustering for the URLLC slice case, using $\epsilon_d =0.06$ and $n_{min}=2$ as parameters. Such values have been empirically selected following the sensitivity analysis depicted in Fig.~\ref{fig:clustering_hyperparameter}, which certifies that along the evaluation timeline and across the different running slices, the selection algorithm identified on average $3$ clusters populated by $15$ agents each. The resulting behavior of DAs is heavily affected by the entities participating in the federation process. Therefore, such kind of characterization is fundamental to ensure performances.

%\begin{figure*}[!t]
	%\centering
   % \begin{subfigure}[t]{.49\textwidth}
        %\centering
        %\includegraphics[trim = 0cm 0cm 0cm 0cm, clip, width=\columnwidth]{Average_reward_final.pdf}
        %\subcaption{\small Comparison of avg. reward for different ap, \lz{[15k, $\hat{T}=5$, $\iota=10$]\\The curves are smoothed for
%visual clarity with respect to confidence bands.}}
        %\label{fig:Comparison}       
    %\end{subfigure}%
    % \hfill
    %\begin{subfigure}[t]{.49\textwidth}
        %\centering
         %\includegraphics[clip, trim = 0cm 0cm 0cm 0cm,width=\columnwidth]{Figures/federation_overhead_box.jpg}
        %\subcaption{\small Overhead + Performance for different federation strategies \tbd{uniform style}}
        % \vspace{-3mm}
        %\label{fig:Overhead}
    %\end{subfigure}
   % \caption{ \small Performance and resulting communication overhead comparison for different federation strategies.}
%\end{figure*}%

\begin{figure}[t]
\centering
\includegraphics[clip, trim = 0cm 0cm 0cm 0cm, width=.95\linewidth]{Figures/clustering.pdf}
%\vspace{-2mm}
\caption{\small Sensitivity analysis performed on the clustering parameters and generated traffic traces.}
\label{fig:clustering_hyperparameter}
\end{figure}

\begin{figure}[h]
\centering
\includegraphics[clip, trim = 0cm 0.2cm 0cm 0cm,width=\columnwidth]{Average_Reward_Chunk_final.pdf}
\caption{\small Learning curve for different action spaces.}
% \vspace{-3mm}
\label{fig:PRB_Action_Space}
\end{figure}

\subsubsection{Effects of Different PRB Action space}

The size of the action space is well-known to affect the learning curve of any reinforcement learning algorithm. In Fig.~\ref{fig:PRB_Action_Space}, we investigate this aspect by varying the minimum PRB chunk size $\iota = {2, 5, 10}$ of the URLLC slice, while fixing $\hat{T}=5$ decision epochs per federation episode and adopting the full-cluster federation strategy. The plot shows how increasing the PRB chunk size, i.e., adopting smaller action spaces, actually influences the reward of the URLLC slice type and its stringent SLA requirements, with larger PRB chunk values achieving satisfactory performances in a faster way, with about 25\% performance gap with respect to $\iota = 2$. Nevertheless, a too broad PRB chunk allocation may result in resource wastage, with portions of the radio resources being under-utilized by the running slices. Such a trade-off should be carefully investigated according to both slice and system requirements. In the following, we will adopt $\iota=10$ PRBs whenever not specified otherwise.




% \setcounter{figure}{11}

\begin{figure}[h]
\centering
\includegraphics[clip, trim = 0cm 0.2cm 0cm 0cm,width=\columnwidth]{Average_reward_final.pdf}
\caption{\small Comparison of global performances for different dynamic and non-dynamic federation approaches. } 
% \vspace{-3mm}
\label{fig:Comparison}
%\lz{[15k, $\hat{T}=5$, $\iota=10$]}
\end{figure}





\subsubsection{Comparison of Different Federation Strategies}
Given the particular nature of the network slicing scenario, in this paper we advocate for a dynamic agent selection method based on the time similarity of traffic demands, dubbed as \emph{Dynamic Clustering} (DC). As discussed in Sec.~\ref{subsec:dynamic_clustering}, several strategies can be adopted to combine local models into federated ones, pursuing generalization and performance improvements.
In this paper we consider three DC aided approaches, namely \emph{Full-Cluster} (FC), \emph{Best-Representative} (BR) and \emph{Random-Representative} (RR), and compare their performances against a standard strategy which simply derives a new federated model accounting for all the available local models, without adopting any dynamic agent selection scheme, dubbed as \emph{FDRL}. The benchmark \emph{FDRL} approach exploits all the local trained models and the respective knowledge from the agents, and would theoretically allow for the best generalization of performances~\cite{FDRL_Generalization}. Interestingly instead, from our experiments it turns out that aggregation of widely heterogeneous local models actually limits the capability of the global federated model to converge to a one-fits-all unified model, motivating our dynamic agent selection approach which favors the specialization of federated agents working under similar RAN and mobility contexts.
%
Fig.~\ref{fig:Comparison} provides a comparison of learning performances for different federation strategies in terms of average reward and for $\hat{T}=5$. The agent's action selection follows a greedy approach which balances exploration of new actions and exploitation of already known decision policies.
We gradually limit the exploration capabilities in favour of the adoption of the learned policies, such that around half of the overall simulated time span, the possibility that the agent will explore new actions given a known instantaneous context is in the order of 2\%.
From the figure, we can observe how \emph{Full-Cluster} approach achieves better generalization of the learning policies, resulting in stable performances. Conversely, \emph{Best-Representative}, \emph{Random-Representative} and \emph{FDRL} suffer the dynamic behavior of the underlying traffic conditions, presenting inconsistent reward traces.

% On the other side, simple \emph{DRL} does not involve federation, and, as discussed before, suffers poor performances when dealing with highly variable scenarios characterized by wide state and action spaces.

% \tbd{COMPLEXITY TEXT TO BE REUSED Nevertheless, we can highlight some interesting findings when observing the federation processing time for a variable number of local models, as depicted in Fig.~\ref{fig:Overhead}. The federation processing time accounts for the time to collect the local agent models, compute the federated model according to the specific strategy, and send it back to the agents. These processes are assumed to happen within an edge platform located within the city.The bar chart reveals a linear relationship between the number of local models and processing time due to time-consuming I/O reading and writing operations.
% In this regard, and in light of the results shown in Fig.~\ref{fig:FL_CDFlatency}, it is safe to assert that the proposed clustering approach enhances the efficiency of the federation learning, limiting the overall processing time due to a smaller set of models to be processed in parallel, while providing comparable performances in realistic scenarios.
% }


% \begin{figure}[h]
% \centering
% \includegraphics[clip, trim = 0cm 0cm 0cm 0cm,width=\columnwidth]{Figures/5vs20steps-final.pdf}
% \caption{\small steps TO BE CHANGED }
% % \vspace{-3mm}
% \label{fig:5vs20steps}
% \end{figure}


% \tbd{
% % \subsubsection{Resource Allocation}
% In the following, we provide an aggregated view of the system, considering one representative cluster for each slice kind.
% In particular, Fig.~\ref{fig:training_testing_cluster} analyzes the system performances considering both machine learning (Fig.~\ref{fig:training_reward}) and networking statistics during local model training.
% %
% The FDRL algorithm follows an iterative approach which let DAs interact with the environment to collect the corresponding reward. The learning curves depicted in Fig.~\ref{fig:training_reward} attest how the DAs are able to quickly learn how to interact with different network configurations and traffic demands. Our results show that dynamic clustering allow DAs to increase their cumulative reward during the considered time span of $200$ episodes, thereby decreasing networking costs in terms of latency and dropped traffic.

% In complex scenarios characterized by a wide state space it is possible that even after several federation episodes, agents may encounter an unknown state, therefore resulting in significant fluctuations over the control metrics. This is the case of Fig.~\ref{fig:training_latency} and Fig.~\ref{fig:training_dropped}, which depict latency and corresponding dropped traffic (normalized over the instantaneous traffic demand) metrics for the different kind of slices during the training phase. It can be noticed how these traces are characterized by high variability before episode $100$, which steadily reduces for an increasing number of training intervals, i.e., when the state space gets explored by more and more agents. This behaviour is particularly evident for the \emph{eMBB} slice, which is characterized by greater traffic volumes and is therefore particularly affected by erroneous PRB allocation decisions performed during the initial training steps.

% % it clearly appears how adopting \emph{Average} and \emph{Reward-Based} approaches as federation strategies provides sub-optimal performances when compared against \emph{Dynamic Clustering}, specially in presence of slices with more stringent requirements, such as the \emph{URLLC} one. }

% % \tbd{Fig.\ref{fig:time_FDRL} illustrates the federation processing time for URLLC slices where the DDQN agents send the local trained model to the federated central server and corresponding federation layer (URLLC) for global model training and scalable deployment. The processing time consists of the local models collection for a scalable feature extracting to create a global model based on different high-dimensional states and network configurations, actions, and also rewards, update the global model based on the federation strategy and thereby send back this updated model to BSs and related agents involved in FDRL procedure. The bar chart results reveal that there is a linear relation between the number of local models and processing time. With reference to Fig.\ref{fig:FL_CDFlatency}, the proposed clustering approach helps to accelerate performance improvement while makes the overall processing time more efficient in federation layer. }
% }
\begin{figure}[h!]
\centering
\includegraphics[clip, trim = 0cm 0.2cm 0cm 0cm,width=\columnwidth]{delay_sta.pdf}
\caption{\small \change{CCDF of transmission latency for URLLC and eMBB slices.}} 
% \vspace{-3mm}
\label{fig:Delay_sta}
%\lz{[15k, $\hat{T}=5$, $\iota=10$]}
\end{figure}
\subsubsection{\change{Latency Analysis for Different Federation Strategies}}
\change{
%To better understand the latency behavior, we check the tail distribution. In this regard, Fig.~\ref{fig:Delay_sta} depicts the complementary cumulative distribution function (CCDF) \cite{ccdf_ref, ccdf_ref2} of the eMBB latency for various federation strategies. As we mentioned in ~\ref{sec:scenario}, the transmission latency reflects the average waiting time that the traffic belonging to a specific slice experiences within the base station transmission buffers before being scheduled. This latency is directly related to the traffic as well as the degree of contention in the system which increases in the case of FDRL where all BSs are involved in the learning  falling in either competitive or too cooperative situations, which delays the decision. In contrast, Full-Cluster presents a trade-off in terms of collaboration as well as an efficient utilization of the shared PRB resource pool to minimize the contention and lead to lower latency as depicted in Fig.~\ref{fig:Delay_sta}. Therein we remark also that RR and BR federation strategies achieve slightly better performance than FDRL method. However, these three federation approaches suffer the dynamic behavior of the underlying traffic conditions where agents can not completely learn the optimal and proper radio allocation decisions in different network states.

We continue our performance evaluation by considering the experienced transmission latency. We recall that as mentioned in Sec.~\ref{sec:scenario}, we define latency as the time spent by the slice traffic within transmission buffer of the base station. Fig.~\ref{fig:Delay_sta} depicts the complementary cumulative distribution function (CCDF) \cite{ccdf_ref}~\cite{ccdf_ref2} of the latency experienced by the URLLC and eMBB slices, resulting by different federation strategies.
\begin{figure*}[t!]
\centering
\includegraphics[clip, trim = 0cm 0.2cm 0cm 0cm,width=2\columnwidth]{15-20-25-dropped_traffic.pdf}
\caption{\small Performance evaluation for different network loads derived by an increasing number of end-users in Full-Cluster settings.} %\lz{[$\hat{T}=5$, $\iota=10$, Full-Cluster]}
%\vspace{-3mm}
\label{fig:network_load}
\end{figure*}
For the both slices, this latency is directly proportional to the traffic demand and the degree of contention of resources among the different slices, as well as to the resource allocation decisions taken by the agents. From the results, it can be noticed that the FDRL strategy leads to the worst performances, as having all the BSs involved in the learning process results in a slow adaptation of the decisions of the agents to the local traffic conditions, therefore leading to sub-optimal resource allocation and higher latency. 
In contrast, Full-Cluster presents a good trade-off in terms of collaboration among agents and specialization to the local traffic conditions, resulting in a more efficient PRB allocation and lower perceived latency. 
Finally, RR and BR federation strategies achieve performances comparable with the FDRL method, resulting from the limited cooperation in learning that leads these federation approaches to suffer more from the dynamic behavior of the underlying traffic conditions.}



\subsubsection{Effects of Different Network Loads and Mobility}
We continue our analysis investigating the performances of the \emph{Full-Cluster} method in heterogeneous traffic conditions. To this aim, we generate traffic and mobility dataset for an increasing number of end-users, namely 15k, 20k, and 25k.
%
As highlighted in~\cite{ARENA}, a non-linear relationship characterizes end-user mobility and throughput performances in crowded scenarios. Clearly, this also affects the communication latency, as a higher number of users will be simultaneously active under the same radio access node. In the context of RAN slicing resource allocation, this translates to finding the best DA logic to efficiently address such variability. In Fig.~\ref{fig:network_load}, we focus our analysis on the dropped traffic, i.e., the volume of traffic that did not meet the latency requirements due to wrong PRB allocation decisions, measured in percentage of the offered traffic volume of each federation episode.
From the picture, we can notice how during the initial exploration phase inexperienced PRB allocation decisions performed by the DAs heavily affect the latency requirements of all network slices, with peaks of dropped traffic that increase with the growing number of end-users. Nevertheless, this trend improves over time as the agents gain knowledge over the underlying scenario and get trained, finally converging after policy switch, i.e. after episode 400, towards values in the order of 2\% for the eMBB slice, and 0,32\% for the URLLC slice.
\begin{figure}[t!]
\centering
\includegraphics[clip, trim = 0cm 0.2cm 0cm 0cm,width=0.95\columnwidth]{Figures/federation_overhead_all_eps0.0316_nmin4.pdf}
\caption{\small Communication overhead per federation episode for different federation strategies (top-part) and for different number of BSs deployed (bottom-part). RR and BR federation strategies are referred as Representative.}
%\vspace{-3mm}
\label{fig:Overhead}
\end{figure}
\subsubsection{\change{Communication Overhead Comparison for Different Federation Strategies}}
% \setcounter{figure}{12}
Federated Learning aims at building global knowledge from the exchange of multiple locally trained models towards a centralized entity.
Such a frequent model exchange however introduces significant communication overhead and synchronization issues, specially in wide scenarios as those considered in our work.
Fig.~\ref{fig:Overhead} compares the model exchange overhead per federation episode resulting from our experiments for a different number of base stations while running the same 3 slices.
In the upper part we focus on the overhead statistical distribution. The benchmark \emph{FDRL} approach assumes the exchange of all the locally trained model weights to derive the federated ones, which implies the highest communication overhead. 
The BR and RR approaches (depicted in the center of the image, and referred to as \emph{Representative}) allow reducing the uplink information exchange by selecting a single representative of each cluster, regardless of the dimensions of the group itself, thus minimizing the communication overhead in each federation episode. It results in less than $800$ kBytes in our settings.
Finally, the \emph{Full-Cluster} approach is characterized by an intermediate average value but higher variance. This is due to the variable size of the DAs clusters, which follows the real-time traffic variations, and the need to exchange the local model weights of each element of the cluster, saving communication resources from those base stations that presented peculiar traffic traces and remain unclustered.
% This is characterized by a linear dependency with respect to the number of BSs involved.

On the lower part of the picture, we differentiate between uplink and downlink model exchange overhead. 
The FDRL approach presents a symmetric behavior matching the model exchange of all the running DAs, in both directions. Conversely, the RR/BR approaches show an asymmetric behavior that favors the upload communication with respect to the downlink one, as only a single DA per cluster shares its local model during the federation process, resulting in a logarithmic trend (with respect to the number of BSs) characterizing the overhead in uplink. This would guarantee better scalability, at the expense of suboptimal performances, as shown in our evaluation.
Finally, the FC approach shows a sublinear trend, with a slower growth rate than the benchmark FDRL, but with significant better performances thanks to the specialization of the DAs. It is safe to assert that the proposed dynamic clustering approach enhances the efficiency of the federated learning scheme, limiting the overall communication overhead with respect to traditional approaches, while providing better performances.
\begin{figure}[h!]
\centering
\includegraphics[clip, trim = 0cm 0.2cm 0cm 0cm,width=0.9\columnwidth]{Power-usage.pdf}
\caption{\small \change{CPU and GPU power consumption for different federation strategies during agent training.}}
%\vspace{-3mm}
\label{fig:ENERGY}
\end{figure}

\subsubsection{Power Consumption Comparison} %\change{On the other hand, we compare the power consumption of the different DRL strategies. Given that their CPU usages present similar cumulative density functions (CDF) as shown on the left hand side of Fig. 14, we resort to the evaluation of the GPU power distribution. In this respect, we use \texttt{nvidia-smi} which is a command line utility based on top of the NVIDIA management library (NVML)\footnote{https://developer.nvidia.com/nvidia-management-library-nvml} enabling the management and monitoring of NVIDIA GPU devices. The obtained power CDFs are distinct as depicted on the right hand side of Fig. 14, where the RR scheme presents the lowest consumption compared to FC and FDRL. Besides being in line with the overhead variation in Fig. 13, this power consumption behavior emanates also from the reduced computation complexity of RR and FC that involve only a subset of DAs through the clustering.
%}


%%%%%%%%%%%%%%%%%% PREVIOUS TEXT
%\change{Energy consumption is an important factor in federeated learning schemes. In Fig.~\ref{fig:ENERGY}, we compare the power consumption of the different DRL strategies when adopting a GPU for training.  We use \texttt{nvidia-smi} command line utility \footnote{Part of the NVIDIA management library (NVML). Online available at \url{https://developer.nvidia.com/nvidia-management-library-nvml}} to retrieve in real-time the energy consumption of the device for different federated learning strategies. The obtained CDFs show that RR/BR schemes present lower consumption compared to FC and FDRL. Besides being positively influenced by the communication overhead variation depicted in Fig.~\ref{fig:Overhead}, such reduced power consumption also results from the limited number of RR/BR agents involved in the federation process (selected through accurate clustering procedures), when compared against baseline approaches.

%\tbd{Given that their CPU usages present similar cumulative density functions (CDF) as shown on the left hand side of Fig.~\ref{fig:ENERGY}, we resort to the evaluation of the GPU power distribution.} 



\change{Energy consumption is an important factor in federeated learning schemes. In Fig.~\ref{fig:ENERGY}, we compare the power consumption of the different DRL strategies during training both in terms of CPU (left-hand side) and GPU (right-hand side), assuming they use the same computational platform as specified at the beginning of Sec.~\ref{sec:perf_eval}.
We use \texttt{nvidia-smi} command line utility\footnote{Part of the NVIDIA management library (NVML). Online available at \url{https://developer.nvidia.com/nvidia-management-library-nvml}}
to retrieve in real-time the energy consumption of the device, whereas for the CPU consumption we monitor the CPU utilization during the training, and consider a proportional fraction of the absorbed power at full computational load as declared by the vendor\footnote {https://ark.intel.com/content/www/us/en/ark/products/192437/intel-xeon-gold-6230-processor-27-5m-cache-2-10-ghz.html}.
As we leverage the GPU hardware to train the models, the different federation strategies exhibit a similar impact on the power consumption of the CPU. Therefore, we focus on the GPU power consumption to better highlight their behavior. The obtained CDFs show that RR/BR schemes present lower consumption compared to FC and FDRL. Besides being positively influenced by the communication overhead variation depicted in Fig.~\ref{fig:Overhead}, such reduced power consumption also results from the limited number of RR/BR agents involved in the federation process (selected through accurate clustering procedures, as shown in Sec.~\ref{subsec:dynamic_clustering}), when compared against baseline approaches.

%\tbd{Given that their CPU usages present similar cumulative density functions (CDF) as shown on the left hand side of Fig.~\ref{fig:ENERGY}, we resort to the evaluation of the GPU power distribution.} 

}



% Finally, the \emph{Full-Cluster} approach provides comparable values characterized by an intermediate average value but higher variance. This is due to the variable size of the DAs clusters, which follows the real-time traffic variations, and the need to exchange the local model weights of each element of the cluster, saving communication resources from those base stations that presented peculiar traffic traces and remain unclustered. Also in this case the overhead shows a linear dependency on the number of BSs deployed but with a slower growth rate than the benchmark. 
% It is safe to assert that the proposed dynamic clustering approach enhances the efficiency of the federated learning scheme, limiting the overall communication overhead with respect to traditional schemes, while providing better performances as showcased in the previous subsection.

%\begin{figure*}[t]
%\centering
%\includegraphics[clip, trim = 0cm 0cm 0cm 0cm,width=0.8\textwidth]{Figures/federation_overhead_all_eps0.0316_nmin4.pdf}
%\caption{\small Communication overhead per federation episode for different federation strategies and for different number of BSs deployed. Random Representative and Best Representative strategies are referred as Representative.}
% \vspace{-3mm}
%\label{fig:Overhead}
%\end{figure*}
%\begin{figure}[h]
%\centering
%\includegraphics[clip, trim = 0cm 0cm 0cm 0cm,width=\columnwidth]{Figures/federation_overhead_box_eps0.0316_nmin4.pdf}
%\caption{\small Communication overhead per federation episode for different federation strategies.}
% \vspace{-3mm}
%\label{fig:Overhead}
%\end{figure}


% We compare the effects of a different number of end-users and federation strategies on the transmission latency experienced by the slice traffic.

% \subsubsection{Effects of Different Federation Episode Duration}
% \lz{It is well-known that the duration of each federation episode affects the learning procedure and, consequently, its convergence time~\cite{LACO}. Accounting for the traffic and mobility patterns generated by 15K users, in the following we investigate the effects of a different federation episode duration. Fig.~\ref{fig:fed_episode_duration} depicts the eMBB slice dropped traffic due to wrong resource allocation decisions and latency violations. It can be noticed  }

% \tbd{ FIG 9 WHAT TO HIGHLIGHT -> t=20 very similar performances but way longer time is needed, T=5 fed clearly has better performances. V2 has noisy behaviour due to? }


%\begin{figure}[!t]
%\centering
%\includegraphics[clip, trim = 0cm 0cm 0cm 0cm,width=0.8\columnwidth]{Dropped_Traffic_eMBB.pdf}
%\caption{\small Effects of different federation episode duration \lz{15k, $\hat{T}$=20 vs $\hat{T}=5$, eMBB} }
% \vspace{-3mm}
%\label{fig:fed_episode_duration}
%\end{figure}

\section{Conclusions}
\label{sec:conclusion}
Major research efforts in the \emph{network slicing orchestration} area focus on designing solutions able to \emph{concurrently and efficiently deal with both spatial and temporal aspects of users' traffic demand}. Due to the distributed nature of the RANs domain, centralized approaches are doomed to provide suboptimal performance and introduce significant communication overhead towards holistic resource controllers.
Tackling such challenging scenario, in this paper we proposed an \emph{FDRL-based architecture for network slice resource orchestration}, where \emph{clusters} of decision agents are dynamically instantiated as virtualized instances with control over base stations radio resources. Enabled by the latest developments in federated learning, our approach allows building specialized knowledge from traffic and mobility patterns by exploiting similarity metrics. Our results show that the proposed  \emph{FDRL-based architecture} poses a trade-off involving the minimization of the communication overhead and the specialization of the decision agents, which in turn affects their accuracy along the resource allocation process.


%Major efforts in design of network slicing orchestration pivots over the definition of solutions able to concurrently and efficiently deal with both spatial and temporal aspects of the traffic demand. Due to the distributed nature of the RAN domain, traditionally centralized approaches provide sub-optimal performances and introduce significant communication overhead towards holistic resource controllers.
%Tackling such challenging scenario, in this paper we investigate a distributed architecture for network slice resource orchestration, where DAs can be dynamically instantiated as virtualized instances with control over local base stations radio resources. Enabled by latest development in Federated Learning, our approach allows to build local knowledge from traffic and mobility patterns through RAN-limited communication exchange, finally enabling faster and more accurate resource allocation decisions even in extended scenarios.
\comment{
\section*{Acknowledgment}
The research leading to these results has been supported by the H$2020$ MonB5G Project (grant agreement no. $871780$).
}

% \tbd{ Show that at the end agents do not overprovision even with the whole action space available\\}

\begin{thebibliography}{1}
\bibitem{TVT_2}
W.~K. {Seah}, C.~{Lee}, Y.~{Lin}, and Y.~{Lai}, ``{Combined Communication and
  Computing Resource Scheduling in Sliced 5G Multi-Access Edge Computing
  Systems},'' \emph{IEEE Trans. Veh. Technol.}, vol.~71, no.~3, pp. 3144--3154,
  Mar. 2022.

\bibitem{Zhou_sli}
H.~Zhou, M.~Erol-Kantarci, and V.~Poor, ``{Learning from Peers: Deep Transfer
  Reinforcement Learning for Joint Radio and Cache Resource Allocation in 5G
  RAN Slicing},'' \emph{IEEE Trans. Cogn. Commun. Netw}, Sep. 2022.

\bibitem{f-centralized}
H.~{Xiang}, W.~{Zhou}, M.~{Daneshmand}, and M.~{Peng}, ``{Network Slicing in
  Fog Radio Access Networks: Issues and Challenges},'' \emph{IEEE Commun.
  Mag.}, vol.~55, no.~12, pp. 110--116, Dec. 2017.

\bibitem{ORAN_White_paper}
{O-RAN Alliance}, ``{O-RAN: Towards an Open and Smart RAN},'' Tech. Rep., Oct.
  2018.

\bibitem{masssive-slicing}
F.~{Rezazadeh}, H.~{Chergui}, L.~{Blanco}, L.~{Alonso}, and C.~{Verikoukis },
  ``{A Collaborative Statistical Actor-Critic Learning Approach for 6G Network
  Slicing Control},'' in \emph{Proc. IEEE Glob. Commun. Conf. (GLOBECOM)}, Feb.
  2021.

\bibitem{AztecInfocom20}
D.~{Bega}, M.~{Gramaglia}, M.~{Fiore}, A.~{Banchs}, and X.~{Costa-P\'erez},
  ``{AZTEC: Anticipatory Capacity Allocation for Zero-Touch Network Slicing},''
  in \emph{Proc. IEEE Conf. Compu. Commun.}, Jul. 2020.

\bibitem{mano-far2s}
F.~Rezazadeh, H.~Chergui, L.~Christofi, and C.~Verikoukis,
  ``{Actor-Critic-Based Learning for Zero-touch Joint Resource and Energy
  Control in Network Slicing},'' in \emph{Proc. IEEE Int. Conf. Commun. (ICC)},
  Jun. 2021.

\bibitem{ans2-far}
A.~Dalgkitsis, L.~A. Garrido, F.~Rezazadeh, H.~Chergui, K.~Ramantas, J.~S.
  Vardakas, and C.~Verikoukis, ``{SCHE2MA: Scalable, Energy-Aware, Multidomain
  Orchestration for Beyond-5G URLLC Services},'' \emph{IEEE Trans. Intel.
  Transp. Syst.}, pp. 1--11, Sep. 2022.

\bibitem{Zhang_slice}
H.~Zhang, H.~Zhou, and M.~Erol-Kantarci, ``{Federated Deep Reinforcement
  Learning for Resource Allocation in O-RAN Slicing},'' in \emph{Proc. IEEE
  Glob. Commun. Conf.}, Dec. 2022.

\bibitem{rec1}
Y.~Nie, J.~Zhao, F.~Gao, and Y.~F. Richard, ``{Semi-Distributed Resource
  Management in UAV-Aided MEC Systems: A Multi-Agent Federated Reinforcement
  Learning Approach},'' \emph{IEEE Trans. Veh. Technol.}, vol.~70, no.~12, pp.
  13\,162--13\,173, Dec. 2021.

\bibitem{rec2}
F.~Li, B.~Shen, J.~Guo, K.-Y. Lam, G.~Wei, and L.~Wang, ``{Dynamic Spectrum
  Access for Internet-of-Things Based on Federated Deep Reinforcement
  Learning},'' \emph{IEEE Trans. Veh. Technol.}, vol.~71, no.~7, pp.
  7952--7956, Jul. 2022.

\bibitem{rec3}
X.~Li, L.~Lu, W.~Ni, A.~Jamalipour, D.~Zhang, and H.~Du, ``{Federated
  Multi-Agent Deep Reinforcement Learning for Resource Allocation of
  Vehicle-to-Vehicle Communications },'' \emph{IEEE Trans. Veh. Technol.},
  vol.~71, no.~8, pp. 8810--8824, Aug. 2022.

\bibitem{rec6}
Y.~Cao, S.-Y. Lien, Y.-C. Liang, K.-C. Chen, and X.~Shen, ``{User Access
  Control in Open Radio Access Networks: A Federated Deep Reinforcement
  Learning Approach},'' \emph{IEEE Trans. on Wirel. Commun.}, vol.~21, no.~6,
  pp. 3721--3736, Jun. 2022.

\bibitem{FMA_HUY}
H.~T. Nguyen, N.~Cong~Luong, J.~Zhao, C.~Yuen, and D.~Niyato, ``{Resource
  Allocation in Mobility-Aware Federated Learning Networks: A Deep
  Reinforcement Learning Approach},'' in \emph{Proc. IEEE World Forum Inte.
  Things (WF-IoT)}, Jun. 2020.

\bibitem{Fdrl_seif}
S.~Messaoud, A.~Bradai, O.~B. Ahmed, P.~T.~A. ~, Quang, M.~Atri, and M.~S.
  Hossain, ``{Deep Federated Q-Learning-based Network Slicing for Industrial
  IoT},'' \emph{{IEEE Trans. Indust. Informat.}}, vol.~17, no.~8, pp.
  5572--5582, Aug. 2021.

\bibitem{DeepRL_Infocom}
Z.~Xu, J.~Tang, J.~Meng, W.~Zhang, Y.~Wang, C.~H. Liu, and D.~Yang,
  ``{Experience-driven Networking: A Deep Reinforcement Learning based
  Approach},'' in \emph{{Proc. IEEE Conf. Comput. Commun.}}, Apr. 2018.

\bibitem{EdgeSlice}
Q.~{Liu}, T.~{Han}, and E.~{Moges}, ``{EdgeSlice: Slicing Wireless Edge
  Computing Network with Decentralized Deep Reinforcement Learning},'' in
  \emph{Proc. IEEE Int. Conf. Distr. Compu. Syst. (ICDCS)}, Dec. 2020.

\bibitem{Scalable_Orchestration}
H.~Huang, C.~Zeng, Y.~Zhao, G.~Min, Y.~Zhu, W.~Miao, and J.~Hu, ``{Scalable
  Orchestration of Service Function Chains in NFV-enabled Networks: A Federated
  Reinforcement Learning Approach},'' \emph{IEEE Jour. Selec. Areas Commun.},
  vol.~39, no.~8, pp. 2558--2571, Aug. 2021.

\bibitem{TVT_5}
Y.~{Shao}, R.~{Li}, B.~{Hu}, Y.~{Wu}, Z.~{Zhao}, and H.~{Zhang}, ``{Graph
  Attention Network-Based Multi-Agent Reinforcement Learning for Slicing
  Resource Management in Dense Cellular Network},'' \emph{IEEE Trans. Veh.
  Technol.}, vol.~70, no.~10, pp. 10\,792--10\,803, Oct. 2021.

\bibitem{TVT_4}
I.~{Vila}, J.~{Perez-Romero}, O.~{Sallent}, and A.~{Umbert}, ``{A Multi-Agent
  Reinforcement Learning Approach for Capacity Sharing in Multi-Tenant
  Scenarios},'' \emph{IEEE Trans. Veh. Technol.}, vol.~70, no.~9, pp.
  9450--9465, Sep. 2021.

\bibitem{NODL}
Y.~{Tu}, Y.~{Ruan}, S.~{Wagle}, C.~G. {Brinton}, and C.~{Joe-Wong},
  ``{Network-Aware Optimization of Distributed Learning for Fog Computing},''
  in \emph{Proc. IEEE Conf. Compu. Commun.}, Jul. 2020.

\bibitem{net-Slicing}
Y.~K. {Tun}, N.~H. {Tran}, D.~T. {Ngo}, S.~R. {Pandey}, Z.~{Han}, and C.~S.
  {Hong}, ``{Wireless Network Slicing: Generalized Kelly Mechanism-Based
  Resource Allocation},'' \emph{IEEE Jour. Selec. Areas Commun.}, vol.~37,
  no.~8, pp. 1794--1807, Aug. 2019.

\bibitem{RL-NSB}
V.~Sciancalepore, X.~Costa-P\'erez, and A.~Banchs, ``{RL-NSB: Reinforcement
  Learning-Based 5G Network Slice Broker},'' \emph{IEEE/ACM Trans. Netw.},
  vol.~27, no.~4, pp. 1543--1557, Aug. 2019.

\bibitem{foukas_orion}
X.~Foukas, M.~K. Marina, and K.~Kontovasilis, ``{Orion: RAN Slicing for a
  Flexible and Cost-Effective Multi-Service Mobile Network Architecture},'' in
  \emph{Proc. ACM Int. Conf. Mobile Comput. Netw.}, Oct. 2017.

\bibitem{Spatial_Loads}
P.~Caballero, A.~Banchs, G.~de~Veciana, and X.~Costa-P\'erez, ``{Multi-Tenant
  Radio Access Network Slicing: Statistical Multiplexing of Spatial Loads},''
  \emph{IEEE/ACM Trans. Netw.}, vol.~25, no.~5, pp. 3044--3058, Oct. 2017.

\bibitem{melike_sli}
H.~Zhou and M.~Erol-Kantarci, ``{Knowledge Transfer based Radio and Computation
  Resource Allocation for 5G RAN Slicing},'' in \emph{Proc. IEEE 19th Annual
  Consu. Commun. Netw. Conf. (CCNC)}, Jan. 2022.

\bibitem{Knapsack_NP_Complete}
C.~H. Papadimitriou, \emph{Computational complexity}.\hskip 1em plus 0.5em
  minus 0.4em\relax Addison-Wesley, 1994.

\bibitem{pi_ROAD}
A.~{Okic}, L.~{Zanzi}, V.~{Sciancalepore}, A.~{Redondi}, and
  X.~{Costa-P\'erez}, ``{$\pi$-ROAD: a Learn-as-You-Go Framework for On-Demand
  Emergency Slices in V2X Scenarios},'' in \emph{{Proc. IEEE Conf. Compu.
  Commun.}}, Jul. 2021.

\bibitem{FurnoTMC2017}
A.~{Furno}, M.~{Fiore}, R.~{Stanica}, C.~{Ziemlicki}, and Z.~{Smoreda}, ``{A
  Tale of Ten Cities: Characterizing Signatures of Mobile Traffic in Urban
  Areas},'' \emph{IEEE Trans. on Mobile Comput.}, vol.~16, no.~10, pp.
  2682--2696, Oct. 2017.

\bibitem{TVT_1}
Y.~{Azimi}, S.~{Yousefi}, H.~{Kalbkhani}, and T.~{Kunz}, ``{Energy-Efficient
  Deep Reinforcement Learning Assisted Resource Allocation for 5G-RAN
  Slicing},'' \emph{IEEE Trans. Veh. Technol.}, vol.~71, no.~1, pp. 856--871,
  Jan. 2022.

\bibitem{LACO}
L.~{Zanzi}, V.~{Sciancalepore}, A.~{Garcia-Saavedra}, H.~D. {Schotten}, and
  X.~{Costa-P\'erez}, ``{LACO: A Latency-Driven Network Slicing Orchestration
  in Beyond-5G Networks},'' \emph{IEEE Trans. Wirel. Commun.}, vol.~20, no.~1,
  pp. 667--682, Oct. 2021.

\bibitem{Bellman}
R.~S. {Sutton} and A.~G. {Barto}, \emph{Reinforcement learning: An
  introduction}.\hskip 1em plus 0.5em minus 0.4em\relax MIT press, 2018.

\bibitem{TD-error}
R.~S. {Sutton}, ``{Learning to Predict by the Methods of Temporal
  Differences},'' \emph{Machine Learning}, vol.~3, no.~1, pp. 9--44, Aug. 1988.

\bibitem{catastrophic_g}
I.~{Goodfellow}, M.~{Mirza}, D.~{Xiao}, A.~{Courville}, and Y.~{Bengio}, ``{An
  Empirical Investigation of Catastrophic Forgetting in Gradient-based Neural
  Networks},'' in \emph{Proc. Int. Conf. Learning Represent. (ICLR)}, Apr.
  2014.

\bibitem{DoubleDQN}
H.~V. Hasselt, A.~Guez, and D.~Silver, ``{Deep Reinforcement Learning with
  Double Q-Learning},'' in \emph{Proc. Assoc. Advanc. Artifi. Intellig.
  (AAAI)}, Feb. 2016.

\bibitem{DDQN_VNF_OPT}
J.~{Pei}, P.~{Hong}, M.~{Pan}, J.~{Liu}, and J.~{Zhou}, ``{Optimal VNF
  Placement via Deep Reinforcement Learning in SDN/NFV-Enabled Networks},''
  \emph{IEEE Jour. Selec. Areas Commun.}, vol.~38, no.~2, pp. 263--278, Feb.
  2020.

\bibitem{Stochastic-gd}
X.~{Lyu}, C.~{Ren}, W.~{Ni}, H.~{Tian}, R.~{Ping Liu}, and E.~{Dutkiewicz},
  ``{Optimal Online Data Partitioning for Geo-Distributed Machine Learning in
  Edge of Wireless Networks},'' \emph{IEEE Jour. Selec. Areas Commun.},
  vol.~37, no.~10, pp. 2393--2406, Oct. 2019.

\bibitem{not_just_privacy}
J.~Wang, J.~Zhang, W.~Bao, X.~Zhu, B.~Cao, and P.~S. Yu, ``{Not Just Privacy:
  Improving Performance of Private Deep Learning in Mobile Cloud},'' in
  \emph{Proc. ACM SIGKDD Int. Conf. Knowl. Discovery \& Data Min.}, Aug. 2018.

\bibitem{fl_Aled}
M.~Aledhari, R.~Razzak, R.~M.~Parizi, and F.~Saeed, ``{Federated Learning: A
  Survey on Enabling Technologies, Protocols, and Applications},'' \emph{IEEE
  Access}, vol.~8, pp. 140\,699--140\,725, Jul. 2020.

\bibitem{Spatio-LTM}
C.~{Zhang} and P.~{Patras}, ``{Long-Term Mobile Traffic Forecasting Using Deep
  Spatio-Temporal Neural Networks},'' in \emph{Proc. ACM Int. Symp. Mobile Ad
  Hoc Netw. Comput.}, Jun. 2018.

\bibitem{DeepCogInfocom19}
D.~{Bega}, M.~{Gramaglia}, M.~{Fiore}, A.~{Banchs}, and X.~{Costa-P\'erez},
  ``{DeepCog: Cognitive Network Management in Sliced 5G Networks with Deep
  Learning},'' Apr. 2019.

\bibitem{DTW}
W.~Wang, G.~Lyu, Y.~Shi, and X.~Liang, ``{Time Series Clustering Based on
  Dynamic Time Warping},'' in \emph{Proc. IEEE Int. Conf. Soft. Eng. Service
  Sci. (ICSESS)}, Nov. 2018, pp. 487--490.

\bibitem{DTW_Barrier}
O.~Gold and M.~Sharir, ``{Dynamic Time Warping and Geometric Edit Distance:
  Breaking the Quadratic Barrier},'' \emph{ACM Tran. Algorithms}, vol.~14,
  no.~4, Aug. 2018.

\bibitem{DBSCAN}
M.~Ester, H.-P. Kriegel, J.~Sander, and X.~Xu, ``{A Density-Based Algorithm for
  Discovering Clusters in Large Spatial Databases with Noise},'' in \emph{Proc.
  Conf. Knowl. Discovery Data Min.}, 1996, pp. 226--231.

\bibitem{ORAN_architecture}
{O-RAN Alliance}, ``{O-RAN-WG1-O-RAN Architecture Description
  (ORAN.WG1.O-RAN-Architecture-Description-v04.00)},'' {Technical
  Specification}, 2020.

\bibitem{ORAN_NEC}
A.~Garcia-Saavedra and X.~Costa-P\'erez, ``{O-RAN: Disrupting the Virtualized
  RAN Ecosystem},'' \emph{IEEE Commun. Standards Mag.}, pp. 1--8, 2021.

\bibitem{ORAN-infocom}
S.~{D'Oro}, L.~{Bonati}, M.~{Polese}, and T.~{Melodi}, ``{OrchestRAN: Network
  Automation through Orchestrated Intelligence in the Open RAN},'' in
  \emph{Proc. IEEE Conf. Compu. Commun.}, 2022.

\bibitem{Gym_Broc}
G.~Brockman, V.~Cheung, L.~Pettersson, J.~Schneider, J.~Schulman, J.~Tang, and
  W.~Zaremba, ``{OpenAI Gym},'' \emph{arXiv preprint arXiv:1606.01540}, Jun.
  2016.

\bibitem{Globe_far}
F.~Rezazadeh, H.~Chergui, L.~Alonso, and C.~Verikoukis, ``{Continuous
  Multi-objective Zero-touch Network Slicing via Twin Delayed DDPG and OpenAI
  Gym},'' in \emph{Proc. IEEE Glob. Commun. Conf.}, Dec. 2020.

\bibitem{ADAM}
D.~Kingma and J.~Ba, ``{Adam: A method for stochastic optimization},'' in
  \emph{Proc. Int. Conf. Learn. Represent. (ICLR)}, 2015.

\bibitem{GAN_Powered}
Y.~Hua, R.~Li, Z.~Zhao, X.~Chen, and H.~Zhang, ``{GAN-Powered Deep
  Distributional Reinforcement Learning for Resource Management in Network
  Slicing},'' \emph{IEEE Jour. Selec. Areas Commun.}, vol.~38, no.~2, pp.
  334--349, Feb. 2020.

\bibitem{DDPG}
D.~Bega, M.~Gramaglia, A.~Garcia-Saavedra, M.~Fiore, A.~Banchs, and
  X.~Costa-Perez, ``{Network Slicing Meets Artificial Intelligence: An AI-Based
  Framework for Slice Management},'' \emph{IEEE Commun. Mag.}, vol.~58, no.~6,
  pp. 32--38, 2020.

\bibitem{Mobility_model}
L.~Pappalardo and F.~Simini, ``{Data-driven Generation of Spatio-Temporal
  Routines in Human Mobility},'' \emph{Data Min. Knowle. Discovery}, vol.~32,
  pp. 787--829, Dec. 2017.

\bibitem{dEPR}
L.~Pappalardo, F.~Simini, S.~Rinzivillo, D.~Pedreschi, F.~Giannotti, and A.-L.
  Baraba\'si, ``{Returners and Explorers Dichotomy in Human Mobility},''
  \emph{Nature Commun.}, vol.~6, pp. 1--8, Sep. 2015.

\bibitem{Wang_18}
J.~{Wang}, J.~{Tang}, Z.~{Xu}, Y.~{Wang}, G.~{Xue}, X.~{Zhang}, and D.~{Yang},
  ``{Spatiotemporal Modeling and Prediction in Cellular Networks: A Big Data
  Enabled Deep Learning Approach},'' in \emph{Proc. IEEE Conf. Comput.
  Commun.}, May 2017.

\bibitem{FDRL_Generalization}
N.~Bouacida, J.~Hou, H.~Zang, and X.~Liu, ``{Adaptive Federated Dropout:
  Improving Communication Efficiency and Generalization for Federated
  Learning},'' in \emph{Proc. IEEE Conf. on Comput. Commun. Works. (INFOCOM
  WKSHPS)}, May 2021, pp. 1--6.

\bibitem{ccdf_ref}
A.~Ksentini and N.~Nikaein, ``{Toward Enforcing Network Slicing on RAN:
  Flexibility and Resources Abstraction},'' \emph{IEEE Commun. Mag.}, vol.~55,
  no.~6, pp. 102--108, Jun. 2017.

\bibitem{ccdf_ref2}
X.~Wang and M.~C. Gursoy, ``{Uplink Coverage in Heterogeneous mmWave Cellular
  Networks With Clustered Users},'' \emph{IEEE Access}, vol.~9, pp.
  69\,439--69\,455, Apr. 2021.

\bibitem{ARENA}
L.~{Zanzi}, V.~{Sciancalepore}, A.~{Garcia-Saavedra}, X.~{Costa-P\'erez},
  G.~{Agapiou}, and H.~D. {Schotten}, ``{ARENA: A Data-Driven Radio Access
  Networks Analysis of Football Events},'' \emph{IEEE Trans. Netw. Serv.
  Manage.}, vol.~17, no.~4, pp. 2634--2647, Dec. 2020.
\end{thebibliography}
%\bibliographystyle{IEEEtran}
%\bibliography{IEEEabrv,bibliography}

%\section*{Biographies}
%\vspace{-13mm}
\vspace{-10mm}

\begin{IEEEbiography}[{\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{Farhad_Rezazadeh.jpg}}]{Farhad Rezazadeh}(S'19) is currently a Ph.D. candidate at the Technical University of Catalonia (UPC) and Researcher at CTTC. He is involved in some European H2020 projects and he was awarded first patent connected to H2020 5G-SOLUTIONS project. He was a secondee at NEC Lab Europe and had scientific mission at TUM, TUHH, and UdG. He won multiple European, Government, and IEEE Grants. He also serves/served as Reviewer, Organizing, and TPC member in IEEE. His research interests lie in the area of Lifelong ML, B5G/6G, and Network Slicing.
\end{IEEEbiography}
%\vspace{-1.3cm}
\vspace{-15mm}
\begin{IEEEbiography}[{\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{Lanfranco_Zanzi.jpg}}]{Lanfranco Zanzi}(S'17--M'22) received his B.Sc. and M.Sc. in Telecommunication Engineering from Politecnico of Milan (Italy) in 2014 and 2017, respectively, and the Ph.D. degree from the Technical University of Kaiserlautern (Germany) in 2022. He works as senior research scientist at NEC Laboratories Europe. His research interests include network virtualization, machine learning, blockchain, and their applicability to 5G and 6G mobile networks in the context of network slicing.
\end{IEEEbiography}
%\vspace{-1.4cm}
\vspace{-15mm}
\begin{IEEEbiography}[{\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{Francesco_Devoti.jpg}}]{Francesco Devoti} (M'20) received the B.S., and M.S. degrees in Telecommunication Engineering, and the Ph.D. degree in Information Technology from the Politecnico di Milano, in 2013, 2016, and 2020 respectively. He is currently a senior research scientist in the 6G Network group at NEC Laboratories Europe. His research interests include reflective intelligent surfaces, millimeter-wave technologies in 5G and 6G networks, and network slicing.
\end{IEEEbiography}
%\vspace{-1.4cm}
\vspace{-15mm}
\begin{IEEEbiography}[{\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{Hatim_Chergui.jpg}}]{Hatim Chergui} (M'12--SM'22) received the engineering degree in telecommunications from the Institut National des Postes et T\'el\'ecommunications (INPT), Rabat, Morocco, in 2007 and the Ph.D. degree (summa cum laude) in electrical engineering and telecommunications from IMT-Atlantique (T\'el\'ecom-Bretagne), Brest, France, in 2015. He is currently the project manager of the H2020 MonB5G European project and a researcher at CTTC, Spain. He served as a RAN expert at both INWI and Huawei Technologies, Morocco. He was the recipient of the IEEE ComSoc CSIM 2021 Best Journal Paper Award and the IEEE ICC 2020 Best Paper Award. He is an Associate Editor of IEEE Networking Letters.
\end{IEEEbiography}
%\vspace{-1.4cm}
\vspace{-15mm}
\begin{IEEEbiography}[{\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{Xavier_Costa-Perez.jpg}}]{Xavier Costa-P\'erez} (M'06--SM'18) is Head of Beyond 5G Networks R\&D at NEC Laboratories Europe, Scientific Director at the i2Cat R\&D Center and Research Professor at ICREA. His team contributes to products roadmap evolution as well as to European Commission R\&D collaborative projects and received several awards for successful technology transfers. In addition, the team contributes to related standardization bodies: 3GPP, ETSI NFV, ETSI MEC and IETF. Xavier has been a 5GPPP Technology Board member, served on the Program Committee of several conferences (including IEEE Greencom, WCNC, and INFOCOM), published at top research venues and holds several patents. He also serves as Editor of IEEE Transactions on Mobile Computing and Transactions on Communications journals. He received both his M.Sc. and Ph.D. degrees in Telecommunications from the Polytechnic University of Catalonia (UPC) in Barcelona and was the recipient of a national award for his Ph.D. thesis.
\end{IEEEbiography}
\vspace{-125mm}
%\vspace{-1.4cm}
\begin{IEEEbiography}[{\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{Christos_Verekoukis.jpg}}]{Christos Verekoukis} (SM'07) received the Ph.D. degree from Technical University of Catalonia (UPC), Barcelona, Spain, in 2000. He is currently a scientific director in Iquadrat Informatica, and an Associate Proferssor with the Univerisity of Patras. He has authored 151 journal papers and over 200 conference papers. He is also a coauthor of three books, 14 chapters in other books, and two patents. He has participated in more than 40 competitive projects, and has served as a principal investigator of national projects in Greece and Spain. He is currently the IEEE ComSoc GITC vice-chair and the editor-in-chief of the IEEE Networking Letters.
\end{IEEEbiography}







\end{document}
