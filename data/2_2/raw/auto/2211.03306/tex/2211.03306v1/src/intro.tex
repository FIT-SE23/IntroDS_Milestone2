\section{Introduction}\label{sec:introduction}
Network analysis has played a key role in knowledge discovery and data mining. %~\cite{}\memo{empty citation}. 
In many real-world applications in recent years, we are interested in mining \emph{multilayer networks} rather than ordinary (i.e., single-layer) networks, where we have a number of edge sets called \emph{layers}, which encode different types of connections and/or time-dependent connections over the same set of vertices~\cite{Boccaletti+14,DeDomenico+13,Kivela+16}.
For example, in the Twitter network, there are various layers representing different types of connections, e.g., follower--followee relations, retweets, and mentions, among users.
%For example, in the Twitter network, there are various layers representing different types of connections, e.g., follower--followee relations, retweets, mentions, and replies, among users.
Moreover, each of those connections is time-dependent and therefore leads to multiple layers by itself.
%Another example is a brain network arising in neuroscience, where vertices correspond to small regions of a brain~\cite{Friston11}.
As another example, consider brain networks arising in neuroscience, where vertices correspond to small regions of a brain~\cite{Friston11}.
In this network, we can obtain at least two layers representing the structural connectivity and the functional connectivity (e.g., co-activation) among the small pieces of a brain.

Among many network analysis techniques, dense subgraph discovery, aiming to find a dense component in a network, is an essential primitive with a variety of applications in diverse domains~\cite{Gionis_Tsourakakis_15,Lee+10}.
Examples include
detecting communities and spam link farms in Web graphs~\cite{Dourisboure+_07,Gibson+_05},
experts extraction in crowdsourcing systems~\cite{Kawase+19},
real-time story identification in microblogging streams~\cite{Angel+_12},
and extracting molecular complexes in protein--protein interaction networks~\cite{Bader_Hogue_03}.

%\begin{figure}[t]
%    \centering
%    \includegraphics[scale=.28]{img/wildbird_regret.pdf}
%    \caption{An optimal solution to our model with the regret metric for the real-world multilayer network called WILDBIRDS: selecting the red triangle vertices with probability 0.2919 and the union of the triangle and the blue diamond vertices with probability 0.7081. There are six layers, and the color of each edge represents the layer containing the edge.}
%    \label{fig:result_example}
%\end{figure}
\begin{figure}[t]
  \centering
  \begin{minipage}{0.67\linewidth}
    \includegraphics[scale=.18]{img/wildbird_regret.pdf}
  \end{minipage}
  \begin{minipage}{0.32\linewidth}
    \includegraphics[scale=.044]{img/wildbird_regret_0.pdf}
    \includegraphics[scale=.044]{img/wildbird_regret_1.pdf}\\
    \includegraphics[scale=.044]{img/wildbird_regret_2.pdf}
    \includegraphics[scale=.044]{img/wildbird_regret_3.pdf}\\
    \includegraphics[scale=.044]{img/wildbird_regret_4.pdf}
    \includegraphics[scale=.044]{img/wildbird_regret_5.pdf}
  \end{minipage}
  \caption{An optimal solution to our model with the regret metric for the real-world multilayer network called WILDBIRDS: selecting the black vertices with probability 0.29 and the union of the gray and black vertices with probability 0.71. There are six layers, and the color of each edge represents the layer containing the edge.}
  \label{fig:result_example}
\end{figure}

Recently, dense subgraph discovery has been extended from single-layer networks to multilayer networks.
%most existing methods focus only on single-layer networks. 
%Therefore, it is crucial to design novel methods applicable to multilayer networks. 
%However, dense subgraph discovery in multilayer networks has remained largely unexplored; in fact, there are only a few studies available (see the remainder of this section and Section~\ref{sec:related}). 
%It is desired that dense subgraph discovery, as well as the other network analysis primitives, is generalized to that for multilayer networks. 
%Suppose that we analyze the aforementioned Twitter multilayer network. \\
Jethava and Beerenwinkel~\cite{JB2015} introduced the \emph{densest common subgraph problem},
where given a multilayer network, we are asked to find a vertex subset
that maximizes the minimum \emph{degree density} (defined later) over the layers.
They mentioned some concrete applications of this problem in biological networks.
%Indeed, if we have such a primitive, we can obtain a subgraph 
Moreover, we can see that the densest common subgraph problem may appear in the context of robust optimization.
Indeed, multilayer networks can be seen as a model of single-layer networks with uncertain edges with a number of scenarios,
where each layer corresponds to one scenario.
If we can find a subgraph that is reasonably dense for all layers,
the solution is more robust than that obtained by single-layer network analysis,
which may be arbitrarily bad for some layers.
%For example, consider the case where we have no idea which layer is critical in the analysis at hand. 
%In such a case, it is guaranteed that Solutions obtained by multilayer-network analysis perform reasonably well for any layer, although those obtained by single-layer network analysis may be arbitrarily bad for some layers.
%
%which is often desirable, e.g., when there are several scenarios on a network, 
%or when we have no idea which type of connections is critical in the analysis at hand. 
%It is guaranteed that such a solution performs reasonably well for any case. 
%However, dense subgraph discovery in multilayer networks has remained largely unexplored; in fact, there are only a few studies available (see the remainder of this section and Section~\ref{sec:related}). 
%
%We will mention related work in Section~\ref{sec:related}.



\subsection{Our contribution}
In this paper, we introduce a novel optimization model for dense subgraph discovery in multilayer networks.
%Our model aims to find a stochastic solution, i.e., a probability distribution over the family of vertex subsets, while previous work focus on a deterministic solution, i.e., a vertex subset.
Given an edge-weighted multilayer network, our model aims to find a vertex subset that is dense for the layer selected by an adversary.
Specifically, we employ a stochastic solution, i.e., a probability distribution over the family of vertex subsets, while existing work focuses only on a deterministic solution, i.e., a single vertex subset.
%% More precisely, our algorithm stochastically chooses a vertex subset and then, knowing the algorithm's code but not the randomized results of the algorithm (i.e., knowing only the stochastic solution but not a realization), the adversary selects the worst layer for us.%
More precisely, we stochastically choose a vertex subset according to a stochastic solution and then,
knowing only the stochastic solution but not a realization, the adversary selects the worst layer for us.
%\footnote{We can also consider a model in which the adversary also selects a layer in a stochastic way. However, the model is not useful because there always exists a deterministic optimal choice for the adversary.}
Note that the worst layer with respect to each realization of the stochastic solution may be different from that with respect to the stochastic solution.
%Note that such an algorithm can be seen as a randomized algorithm for the densest common subgraph problem.
%rather than a single vertex subset.
% which is more robust for the aforementioned uncertainty arising from the multiplicity of layers. 
%To introduce our model, we employ a well-known concept in game theory, called a zero-sum two-player Stackelberg game~\cite{BO1999}. 
%% \memo{これ以降を少し変えれば良さそう．}
%% In this game, one player called a \emph{leader} decides her strategy first and the other player called a \emph{follower} 
%% decides her strategy while knowing the leader's choice. 
%% In our setting, the leader is our algorithm and the follower is the nature.
%% Our pure strategy is a vertex subset of a given multilayer network, and that of the follower is a layer. 
%% We first choose a vertex subset, and then the follower chooses a layer while knowing our choice. 
%% %Note that we have $2^{|V|}$ pure strategies and the opponent has $k$. 
%% Finally, we receive some payoff of our vertex set, which is defined below.
%% We wish to obtain a subgraph with a high payoff, whereas the follower chooses the layer that minimizes our payoff. 
%% It is known that the follower only needs to choose a pure strategy, while we consider a mixed strategy to compete with the follower. %\memo{cite?}
%% Note that in our game, a mixed strategy is a probability distribution over the family of vertex subsets. 
%% We aim to compute a mixed strategy that maximizes the payoff, i.e., Stackelberg equilibrium. 
It is worth mentioning that our model can be seen as a zero-sum two-player Stackelberg game~\cite{BO1999}, where the leader is our algorithm and the follower is the adversary.



%We define the payoff of a vertex subset using a quality function called the \emph{degree density} (or simply called \emph{density}).
We measure the density of a vertex subset using the quality function called the \emph{degree density}
(or simply \emph{density}) similarly to Jethava and Beerenwinkel~\cite{JB2015}.
The degree density of a vertex subset (in a single-layer network) is defined as half the average degree of the subgraph induced by the subset.
This quality function is often used in the literature of dense subgraph discovery; in fact, this is the objective function of the well-known \emph{densest subgraph problem} (see Section~\ref{sec:related}).

We evaluate the performance of a stochastic solution (or an algorithm)
using the following three metrics for the layer selected by the adversary:
(i) the \emph{density}, i.e., the expected degree density,
(ii) the \emph{robust ratio}, i.e., the ratio of the expected density to the optimal density, and
%(ii) the \emph{robust ratio}, i.e., the multiplicatively normalized version of the density, and
(iii) the \emph{regret}, i.e., the difference between the optimal density and the expected density.
%(iii) the \emph{regret}, i.e., the difference between the expected density and the optimal density. 
%\memo{unclear terms; how do they generalize to multilayer graphs?}
%which represent the worst-case density over all layers, the multiplicatively normalized version of the minimum density, and the worst-case difference between the expected density and the optimal density over all layers, respectively. 
Therefore, we address three optimization problems according to the metrics.
%In particular, the values of the optimal densities would be important to analyze multilayer networks.


%It should be noted that Jethava and Beerenwinkel~\cite{JB2015} introduced an optimization problem called the \emph{densest common subgraph problem}, which is a generalization of the densest subgraph problem. 
%In this problem, given a multilayer network, we are asked to find a vertex subset that maximizes the minimum density over the layers. 
%Therefore, this problem can be seen as our problem with the density metric in which an algorithm deterministically chooses a vertex subset. %we consider only pure strategies. 
%However, Charikar, Naamad, and Wu~\cite{charikar+18} showed strong inapproximability results for the problem, 
%based on some reasonable computational complexity assumptions. 
%%there are only heuristics or approximation algorithms with approximation ratios at least proportional to polynomial in the number of vertices or the number of layers~\cite{charikar+18,JB2015}. 
%Thus, it is very unlikely that the densest common subgraph problem can be approximated well in polynomial time.  

%\subsection*{Our contribution}
% In this study, we further investigate dense subgraph discovery in multilayer networks. 
% Specifically, we present stochastic optimization models, 
% \memo{stochastic vs fractional}
% where we aim to find a probability distribution over the family of vertex subsets $2^V$ that has a reasonably high \emph{expected} performance for all layers. 

%Our main algorithmic contribution is to design a polynomial-time \emph{exact} algorithm for finding a Stackelberg equilibrium.
Our main algorithmic contribution is to design a polynomial-time exact algorithm for our optimization model,
which is applicable to all of the above three metrics.
Specifically, the algorithm first solves an LP, which is a generalization of the LP used for the densest common subgraph problem~\cite{JB2015}
and then computes an optimal probability distribution based on the LP's optimal solution.
%For the model with the minimum density, our algorithm first solves the LP introduced by Jethava and Beerenwinkel~\cite{JB2015} for the densest common subgraph problem and then computes a probability distribution using the information of the optimal solution to the LP. For the other two models, we design our algorithms by introducing slightly different LPs. 
%

We observe that the output of our algorithm has a useful structure;
the family of the vertex subsets with positive probabilities has a hierarchical structure.
This leads to several practical benefits, e.g., the largest size subset contains all the other subsets
and the optimal solution obtained by our algorithm has support size at most the number of vertices (although a solution of our optimization model may have support size exponential in the number of vertices).
Moreover, we can demonstrate that the support size of the solution obtained is upper bounded by the number of layers.
It is desirable to have a small support size
for the understandability of stochastic solutions and for purposes of simple verification and validation.



For practical use of our proposed algorithm,
we wish to speed up our algorithm
so that it is applicable to larger-scale multilayer networks.
%although our exact algorithm runs in polynomial time. 
The bottleneck is the computation cost of the LP, which has a lot of variables and constraints.
To this end, we devise a simple, scalable preprocessing algorithm, which often reduces the size of the input networks significantly and results in a substantial speed-up.
Specifically, the algorithm first computes an approximate solution by solving a much smaller LP than the aforementioned LP
and then removes vertices from the original network using the information of the approximate solution obtained.
%Specifically, we iteratively remove a vertex that has the minimum maximum degree over all layers to obtain a sequence of vertex subsets from $V$ to $\emptyset$ and output a probability distribution that has $1$ for the best subset among the sequence. 
%Then the algorithm removes any vertex whose maximum degree over all layers is less than the objective function value of the approximate solution. 
Note that our preprocessing algorithm is a generalization of that proposed by Balalau et al.~\cite{Balalau+15} for the densest subgraph problem.
However, to verify that the preprocessing algorithm does not harm any optimal solution, we need a more sophisticated analysis, which is totally different from theirs.


Finally, we conduct thorough computational experiments using synthetic graphs and real-world networks to verify the validity of our model and to evaluate the performance of our algorithms.
Figure~\ref{fig:result_example} illustrates an optimal solution to our model with the regret metric for the real-world multilayer network called WILDBIRDS (see Section~\ref{sec:experiments} for its detailed characteristics).
The optimal solution obtained is reasonably dense for all layers, as desired.

It should be remarked that stochastic solutions can also be used for obtaining a single vertex subset.
For example, the following three rules are reasonable to select a vertex subset from a stochastic solution:
(i) stochastically select a subset following the probability distribution,
(ii) select a subset with the highest probability,
and
(iii) select a subset with non-zero probability that optimizes some metric at hand (e.g., the minimum density, minimum robust ratio, and maximum regret, over layers).
In our experiments, we compare the vertex subsets detected using the above rules with those obtained by existing algorithms for the densest common subgraph problem,
in terms of various evaluation metrics.


%\paragraph{Motivation.} 
%Here we mention two motivations for focusing on stochastic solutions rather than deterministic solutions.
%
%One is the computational hardness of deterministic solutions.
%For the densest common subgraph problem, Charikar, Naamad, and Wu~\cite{charikar+18} provided strong inapproximability results based on some reasonable computational complexity assumptions.
%Thus, it is very unlikely that a well-approximate solution can be found in polynomial time.
%However, as mentioned above, we can compute an \emph{optimal} stochastic solution in terms of the above three metrics in polynomial time.
%
%The other motivation is the fact that deterministic solutions tend to be sensitive to the change of edge weights.
%Let us consider a two-layer graph with $n$ vertices ($n>4$).
%One layer has only one edge, say $\{u,v\}$, with weight $M=i(i-1)/2$ for some integer $2<t<n$.
%In the other layer, the $n$ vertices form a clique in which every edge weight is equal to $1$.
%Let us employ the density metric.
%An optimal deterministic solution is to choose $t$ vertices including $u$ and $v$, and its density is $(t-1)/2$.
%On the other hand, an optimal stochastic solution is to choose the $n$ vertices with probability $p=\frac{n(n-1)-2M}{(n-2)(M+n)}$ and $\{u,v\}$ with probability $1-p$ (although we do not verify the optimality here).
%Its density is $\frac{M}{2}\cdot\frac{n+1}{n+M} ~(> (t-1)/2)$.
%When $t$ changes, the optimal deterministic solution will shift to a different vertex subset,
%but in the stochastic one, only the probability $p$ slightly changes.
%As numerical data are often noisy, our stochastic solution may be more desirable in the sense of robustness.
%




\begin{comment}
Our contribution can be summarized as follows:
\begin{itemize}
  \leftskip=-12pt
  \setlength{\itemsep}{1mm}
  \item We introduce a novel optimization model for dense subgraph discovery in multilayer networks; %\memo{ここは変える}
  \item We design an LP-based polynomial-time exact algorithm for the proposed model;
        %\item We demonstrate that an optimal solution obtained by the proposed algorithm has a layered structure, which is a useful property in practice\memo{useful?};
  \item We devise a simple, scalable preprocessing algorithm, enabling us to apply our exact algorithm to large-scale multilayer networks in practice;
  \item We conduct computational experiments to evaluate the performance of the proposed algorithms.
        %We conduct thorough computational experiments to verify the validity of the proposed model and to evaluate the performance of the proposed algorithms. 
\end{itemize}
\end{comment}

