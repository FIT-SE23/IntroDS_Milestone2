\section{Experimental Evaluation}\label{sec:experiments}
In this section, we conduct computational experiments using synthetic graphs and real-world networks
to verify the validity of our proposed model and
to evaluate the performance of our proposed algorithms.
All experiments were conducted on a machine equipped with Intel Xeon W 10-core processor and 64GB RAM.
Algorithms were implemented in Python using Gurobi Optimizer 9.0.2.
%Algorithms were implemented in Python using PuLP library with Gurobi Optimizer 9.0.2.




\subsection{Validity of our model}
Here we aim to verify the validity of our model using synthetic graphs.
To this end, we use a randomly generated multilayer network with a planted clique,
and examine whether an optimal solution detects vertex subsets close to the clique.

We first explain our random procedure for generating multilayer networks.
We produce an (unweighted) random power-law graph as a layer using the Chung--Lu model~\cite{CL2002}, where we first specify an expected degree $d_v$ for each $v\in V$ according to the power-law distribution with exponent $\beta$,
and then connect each pair of vertices $\{u,v\}$ with probability $\frac{d_u\cdot d_v}{\sum_{r\in V}d_r}$.
%It is known that most real-world networks have power-law exponents between $2.2$ and $3.1$~\cite{newman2003}.
Note that the graph becomes sparser as the exponent $\beta$ increases.
In this multilayer network, we randomly select a vertex subset $V_c$ with some size,
and plant a clique on $V_c$ (in some specified layers).

To evaluate the performance of optimal solution $p\in \Delta(2^V)$ to $(\bm{\alpha},\bm{\beta})$-\dens in the above multilayer network,
we introduce the following measure, which we refer to as the
%the measures of 
(expected) F measure:
\begin{align*}
  \text{F measure} \coloneqq \frac{2\cdot \text{precision}\cdot \text{recall}}{\text{precision}+\text{recall}},
\end{align*}
where
%\begin{align*}
$\text{precision}  \coloneqq\mathbb{E}_{S\sim p}\left[\frac{|S\cap V_c|}{|S|}\right]=\sum_{S\subseteq V}p_S\frac{|S\cap V_c|}{|S|}$ and
$\text{recall}     \coloneqq\mathbb{E}_{S\sim p}\left[\frac{|S\cap V_c|}{|V_c|}\right]=\sum_{S\subseteq V}p_S\frac{|S\cap V_c|}{|V_c|}$.
%\end{align*}
The F measure approaches to 1 if $p$ tends to be close to $V_c$.
%and that of recall becomes larger if $p$ often chooses a superset of $V'$. 


We first investigate the case where a clique is planted in all layers.
We generate $k~(=1,2,3,4,5)$ power-law graphs (i.e., layers) with $\beta=2.3$ on $V$ with $|V|=\text{1,000}$.
Then we randomly select a subset $V_c\subseteq V$ consisting of $10$ vertices, and plant a clique on $V_c$ in all layers.
The performance of optimal solutions to $(\bm{\alpha},\bm{\beta})$-\dens is shown in Figure~\ref{subfig:synthetic1-1}.
As can be seen, for any metric, the F measure is reasonably large for $k\geq 2$,
meaning that our algorithm tends to detect $V_c$ using the information of multiple layers.

\begin{figure}[t]
  \begin{minipage}{.5\linewidth}
    \includegraphics[scale=.32]{img/fvalue_all_opt.pdf}
    \subcaption{Clique in all layers.}\label{subfig:synthetic1-1}
  \end{minipage}%
  \begin{minipage}{.5\linewidth}
    \includegraphics[scale=.32]{img/fvalue_one_opt.pdf}
    \subcaption{Clique in only one layer.}\label{subfig:synthetic1-2}
  \end{minipage}
  \caption{Performance of optimal solutions. Each point corresponds to the average value over 100 network realizations.}\label{fig:synthetic1}
\end{figure}


\begin{figure}[t]
  {
    \begin{minipage}{.5\linewidth}
      \includegraphics[scale=.32]{img/fvalue_all_maxprob.pdf}
      \subcaption{Clique in all layers.}\label{subfig:synthetic2-1}
    \end{minipage}%
    \begin{minipage}{.5\linewidth}
      \includegraphics[scale=.32]{img/fvalue_one_maxprob.pdf}
      \subcaption{Clique in only one layer.}\label{subfig:synthetic2-2}
    \end{minipage}%
  }
  \caption{Performance of single vertex subsets obtained by optimal solutions. The same averaging procedure is applied.}\label{fig:synthetic2}
  % 
\end{figure}


%\begin{table}[t] % I like t.
%  \caption{Real-world datasets used in our experiments.}\label{table:data_description}
%  \renewcommand{\arraystretch}{1}
%  \scalebox{.8}{
%    \setlength{\tabcolsep}{4pt}
%    \begin{threeparttable}
%      \begin{tabular}{c|rrrl}\toprule
%        Dataset & $|V|$   & $|E|$   & $k$ & Description                                 \\\midrule
%        WILDBIRDS\tnote{$*$}
%                & 202     & 4,574   & 6   & Animal interaction~\cite{FS2015}            \\
%        %ANT-COLONY1\tnote{$*$}
%        %        & 113     & 6,113   & 41  & Animal interaction~\cite{MCK2013}           \\
%        AS-733\tnote{$\dagger$}
%                & 7,716   & 24,179  & 733 & Autonomous systems~\cite{LKF2005}           \\
%        Oregon1\tnote{$\dagger$}
%                & 11,492  & 26,461  & 9   & Autonomous systems~\cite{LKF2005}           \\
%        MoscowAthletics2013\tnote{$\ddagger$}
%                & 88,804  & 186,846 & 3   & Twitter~\cite{ODMA2015}                     \\
%        SacchCere\tnote{$\ddagger$}
%                & 6,570   & 223,542 & 7   & Genetic interaction~\cite{SBB+006,DNAL2015} \\
%        NYClimateMarch2014\tnote{$\ddagger$}
%                & 102,439 & 329,474 & 3   & Twitter~\cite{ODMA2015}                     \\
%        Cannes2013\tnote{$\ddagger$}
%                & 438,537 & 848,017 & 3   & Twitter~\cite{ODMA2015}                     \\
%        DBLP\tnote{$\mathsection$}
%                & 513,627 & 888,353 & 10  & Co-authorship~\cite{Galimberti+_17}         \\
%        \bottomrule
%      \end{tabular}
%      \begin{tablenotes}[para]
%        \item[$*$] \url{http://networkrepository.com}
%        \item[$\dagger$] \url{http://snap.stanford.edu}
%        \item[$\ddagger$] \url{https://comunelab.fbk.eu/data.php}
%        \item[$\mathsection$] \url{https://goo.gl/8741Gs}
%      \end{tablenotes}
%    \end{threeparttable}
%  }
%\end{table}



\begin{table*}[h!]
  \caption{Performance of our algorithm for real-world datasets.
  OPT and $|\text{supp}|$ denote the optimal value of $(\bm{\alpha},\bm{\beta})$-\dens and the support size of the optimal solution, respectively.
  Note that the objective values for the regret are negated.
  LB indicates the optimal value of LP~\eqref{LP:lower}.
  $|V'|$ and $|E[V']|$, respectively, denote the number of vertices and edges after running Algorithm~\ref{alg:remove}.
  Preprocess time represents the running time of Algorithm~\ref{alg:remove}.
  Total time represents the running time of our proposed algorithm, i.e., Algorithm~\ref{alg:general} together with Algorithm~\ref{alg:remove}.
  For reference, the running time of Algorithm~\ref{alg:general} without Algorithm~\ref{alg:remove} is also presented in the next column.
  The last three columns report the performance of single vertex subsets obtained by optimal solutions.
  For each instance and metric, the best value among the algorithms is in bold.
  }\label{table:computation}
  \setlength{\tabcolsep}{3pt}
  \renewcommand{\arraystretch}{1}
  \scalebox{.78}{
    \begin{threeparttable}
      \begin{tabular}{c|rrr|c|rr|rrrr|r|r|rrr}\toprule
        Dataset                                     & $|V|$   & $|E|$   & $k$ & Metric       & OPT       & $|\text{supp}|$ & LB        & $|V'|$  & $|E[V']|$ & \shortstack{\small Preprocess                                                                                 \\ \small time (s)} & \shortstack{\small Total\\\small time (s)} & \shortstack{\small w/o preprocess\\\small time (s)} & Density  & \shortstack{Robust             \\ratio} & Regret  \\\midrule
                                                    &         &         &     & Density      & 1.1950    & 3               & 1.1313    & 123     & 3,126     & 0.04                          & 0.66     & 0.73     & \textbf{1.1875}  & 0.6579          & -0.6282            \\
        WILDBIRDS                                   & 202     & 4,574   & 6   & Robust ratio & 0.7707    & 4               & 0.7364    & 121     & 2,901     & 0.05                          & 0.77     & 0.79     & 1.0058           & \textbf{0.6970} & \textbf{-0.5065}   \\
        \cite{FS2015}\tnote{$*$}                    &         &         &     & Regret       & -0.4122   & 2               & -0.4473   & 124     & 2,976     & 0.04                          & 0.71     & 0.77     & 0.9129           & 0.6518          & -0.5821            \\\hline
                                                    &         &         &     & Density      & 4.7023    & 2               & 4.4257    & 730     & 6,403     & 1208.79                       & 1304.28  & 1834.88  & \textbf{4.6316}  & 0.6399          & -3.1581            \\
        AS-733                                      & 7,716   & 24,179  & 733 & Robust ratio & 0.7721    & 4               & 0.7295    & 278     & 3,452     & 1165.87                       & 1211.82  & 1906.21  & 4.5789           & 0.7057          & -1.9621            \\
        \cite{LKF2005}\tnote{$\dagger$}             &         &         &     & Regret       & -6.9861   & 4               & -1.8023   & 228     & 3,020     & 1289.15                       & 1329.56  & 1950.33  & 3.6154           & \textbf{0.7317} & \textbf{-1.6316}   \\\hline
                                                    &         &         &     & Density      & 12.0263   & 1               & 12.0263   & 118     & 1,815     & 0.29                          & 0.92     & 8.31     & \textbf{12.0263} & 0.9666          & -0.4382            \\
        Oregon1                                     & 11,492  & 26,461  & 9   & Robust ratio & 0.9808    & 3               & 0.9738    & 110     & 1,702     & 0.48                          & 1.02     & 7.55     & 11.9016          & \textbf{0.9728} & \textbf{-0.3578}   \\
        \cite{LKF2005}\tnote{$\dagger$}             &         &         &     & Regret       & -0.2544   & 3               & -0.3441   & 110     & 1,702     & 0.50                          & 0.95     & 9.72     & 11.8889          & \textbf{0.9728} & \textbf{-0.3578}   \\\hline
                                                    &         &         &     & Density      & 22.2503   & 2               & 21.8228   & 1,130   & 14,166    & 1.90                          & 4.31     & 93.30    & \textbf{18.4000} & 0.1183          & -247.6000          \\
        MoscowAthletics2013                         & 88,804  & 186,846 & 3   & Robust ratio & 0.4709    & 3               & 0.3566    & 105     & 787       & 2.02                          & 2.12     & 112.50   & 13.2000          & \textbf{0.4197} & \textbf{-181.1667} \\
        \cite{ODMA2015}\tnote{$\ddagger$}           &         &         &     & Regret       & -125.4477 & 2               & -130.1752 & 88,804  & 186,846   & 1.40                          & 336.56   & 334.43   & 0.4286           & 0.0186          & -200.8333          \\\hline
                                                    &         &         &     & Density      & 3.5649    & 2               & 3.5077    & 17,551  & 184,659   & 1.98                          & 332.02   & 5466.73  & \textbf{3.5625}  & 0.1768          & -17.0770           \\
        NYClimateMarch2014                          & 102,439 & 329,474 & 3   & Robust ratio & 0.6661    & 3               & 0.5503    & 3,901   & 78,126    & 2.36                          & 147.40   & 6058.12  & 2.3839           & \textbf{0.6652} & -6.8047            \\
        \cite{ODMA2015}\tnote{$\ddagger$}           &         &         &     & Regret       & -2.3052   & 3               & -3.5835   & 102,439 & 329,473   & 1.58                          & 18575.97 & 18909.39 & 1.2785           & 0.3576          & \textbf{-2.3221}   \\\hline
                                                    &         &         &     & Density      & 60.7462   & 2               & 60.7462   & 375     & 4,536     & 11.07                         & 11.72    & 4268.72  & \textbf{52.7143} & 0.0840          & -837.8095          \\
        Cannes2013                                  & 438,537 & 848,017 & 3   & Robust ratio & 0.3633    & 3               & 0.3633    & 246     & 1,617     & 11.53                         & 11.72    & 3981.18  & 42.6000          & \textbf{0.2441} & -365.0667          \\
        \cite{ODMA2015}\tnote{$\ddagger$}           &         &         &     & Regret       & -132.8628 & 2               & -132.8628 & 438,537 & 848,017   & 7.31                          & 4021.19  & 4022.75  & 0.6667           & 0.0073          & \textbf{-155.5000} \\\hline
                                                    &         &         &     & Density      & 1.1891    & 10              & 1.1236    & 191,074 & 559,628   & 18.52                         & 7826.41  & 13787.37 & 0.4146           & 0.0387          & -31.2295           \\
        DBLP                                        & 513,627 & 888,353 & 10  & Robust ratio & 0.1178    & 10              & 0.1125    & 317,231 & 687,335   & 17.01                         & 15323.66 & 22828.17 & \textbf{0.6067}  & \textbf{0.0607} & -20.2759           \\
        \cite{Galimberti+_17}\tnote{$\mathsection$} &         &         &     & Regret       & -11.6944  & 3               & -11.6963  & 513,627 & 888,353   & 14.78                         & 37266.95 & 37085.30 & 0.1045           & 0.0114          & \textbf{-13.4481}  \\
        \bottomrule
      \end{tabular}
      \begin{tablenotes}[para]
        \item[$*$] \url{http://networkrepository.com}
        \item[$\dagger$] \url{http://snap.stanford.edu}
        \item[$\ddagger$] \url{https://comunelab.fbk.eu/data.php}
        \item[$\mathsection$] \url{https://goo.gl/8741Gs}
      \end{tablenotes}
      %The baselines DCS-LP and DCS-Greedy are omitted here because they did not outperform our algorithm. %本文でより詳細に言及
    \end{threeparttable}
  }
\end{table*}


Next we investigate the case where a clique is planted in only one layer.
We generate $k~(=1,2,3,4,5)$ power-law graphs with $\beta=3.0$ on $V$ with $|V|=\text{1,000}$.
Then we randomly select $V_c\subseteq V$ consisting of 20 vertices, but plant a clique on $V_c$ only in one randomly selected layer.
The performance of optimal solutions are described in Figure~\ref{subfig:synthetic1-2}.
As can be seen, the F measure becomes smaller as the number of layers increases.
Among the three metrics, the regret performs particularly well because it concentrates on the layer containing the clique from its definition;
therefore, the regret metric seems most suitable for robust analysis with noisy layers.
The robust ratio performs second best; it also cares about the layer containing the clique.


Finally we conclude this subsection by evaluating vertex subsets that are obtained from optimal solutions $p\in \Delta(2^V)$.
To verify the validity of our model, we select a vertex subset attaining the highest probability in $p\in \Delta(2^V)$.
As our algorithm does not know about $V_c$, it cannot select a vertex subset using the F measure.
For reference, we also run two baseline algorithms, DCS-LP and DCS-Greedy, designed by Jethava and Beerenwinkel~\cite{JB2015}.
Note that DCS-LP can be seen as the algorithm that selects a vertex subset with the largest minimum density value over layers from the support of $p\in \Delta(2^V)$,
where $p\in \Delta(2^V)$ is an optimal solution to our algorithm with the density metric.
%The DCS-LP is an algorithm that outputs the densest common subgraph among the support of the optimal solution to Density.
%The DCS-Greedy is a peeling algorithm that iteratively removes a vertex with the minimum degree in the minimum density layer, and outputs the densest common subgraph over the iterations.
The results are depicted in Figure~\ref{fig:synthetic2}, where we employed the same experimental settings as above and used the usual (deterministic) F measure for evaluation.
As can be seen, the trend of our algorithm is similar to that observed in the above experiments;
all metrics almost detect $V_c$ for the all-layers setting, but only the regret metric is successful for the only-one-layer setting.
As for the baseline methods, DCS-LP is comparable to our algorithm with the density metric, while DCS-Greedy performs quite poorly.




%\begin{table}
%\caption{}\ref{table:subset}
%\centering
%\scalebox{.8}{
%\begin{tabular}{c|c|rrr}\toprule
%  Dataset             & Metric       & Density  & \shortstack{Robust             \\ratio} & Regret \\\midrule
%                      & Density      & \textbf{1.1875}   & 0.6579             & -0.6282            \\
%  WILDBIRDS           & Robust ratio & 1.0058   & \textbf{0.6970}             & \textbf{-0.5065}   \\
%                      & Regret       & 0.9129   & 0.6518             & -0.5821                     \\\hline
%                      & Density      & \textbf{110.1400} & \textbf{0.5756}             & -125.6968 \\
%  ANT-COLONY1         & Robust ratio & 101.2222 & 0.5277             & -98.3542                    \\
%                      & Regret       & 82.0000  & 0.5423             & \textbf{-96.5012}           \\\hline
%                      & Density      & \textbf{4.6316}   & 0.6399             & -3.1581            \\
%  AS-733              & Robust ratio & 4.5789   & 0.7057             & -1.9621                     \\
%                      & Regret       & 3.6154   & \textbf{0.7317}             & \textbf{-1.6316}   \\\hline
%                      & Density      & \textbf{12.0263}  & 0.9666             & -0.4382            \\
%  Oregon1             & Robust ratio & 11.9016  & \textbf{0.9728}             & \textbf{-0.3578}   \\
%                      & Regret       & 11.8889  & \textbf{0.9728}             & \textbf{-0.3578}   \\\hline
%                      & Density      & \textbf{18.4000}  & 0.1183             & -247.6000          \\
%  MoscowAthletics2013 & Robust ratio & 13.2000  & \textbf{0.4197}             & \textbf{-181.1667} \\
%                      & Regret       & 0.4286   & 0.0186             & -200.8333                   \\\hline
%                      & Density      & 0.5890   & 0.1661             & -27.9544                    \\
%  SacchCere           & Robust ratio & \textbf{0.7848}   & \textbf{0.2214}             & -18.6055  \\
%                      & Regret       & 0.3764   & 0.1062             & \textbf{-14.7002}           \\\hline
%                      & Density      & \textbf{3.5625}   & 0.1768             & -17.0770           \\
%  NYClimateMarch2014  & Robust ratio & 2.3839   & \textbf{0.6652}             & -6.8047            \\
%                      & Regret       & 1.2785   & 0.3576             & \textbf{-2.3221}            \\\hline
%                      & Density      & \textbf{52.7143}  & 0.0840             & -837.8095          \\
%  Cannes2013          & Robust ratio & 42.6000  & \textbf{0.2441}             & -365.0667          \\
%                      & Regret       & 0.6667   & 0.0073             & \textbf{-155.5000}          \\\hline
%                      & Density      & 0.4146   & 0.0387             & -31.2295                    \\
%  DBLP                & Robust ratio & \textbf{0.6067}   & \textbf{0.0607}             & -20.2759  \\
%                      & Regret       & 0.1045   & 0.0114             & \textbf{-13.4481}           \\
%  \bottomrule
%\end{tabular}
%}
%\end{table}







\subsection{Performance of our algorithm}
Here we examine the performance of our proposed algorithm using publicly-available real-world multilayer networks.
Table~\ref{table:computation} shows the results together with the summary of the characteristics of the datasets.
Even for large networks,
%Although some networks have a few hundreds of thousands of vertices,
our algorithm obtains an optimal solution in reasonable time.
The preprocessing algorithm often reduces the size of the networks significantly using a reasonably large lower bound computed by LP~\eqref{LP:lower} 
and results in a substantial speed-up.
%This is due to the preprocessing algorithm; it reduces the size of the networks significantly using a reasonably large lower bound computed by LP~\eqref{LP:lower}.
As an extreme example, for Cannes2013 with the density and robust ratio metrics, the lower bound attains the optimal value, and the number of vertices is reduced by more than 99.9\%,
which makes the computation more than $300$ times faster.
% However, it should be noted that when we have the regret metric, the preprocessing is less effective.
% In fact, it removes no vertices for the largest five datasets.
% This may be caused by the large gap of the maximum density values among layers.
% For example, for MoscowAthletics2013,
% the minimum regret is $125.4477$, but the maximum densities for the three layers are 370, 204.5, and 23, respectively. %\memo{Cannes2013の値になっている？} minimum regretの値が間違いでした
% Hence, the layer with the smallest maximum density prevents us to remove vertices
% because any vertex $v^*$ does not satisfy the condition to be removed
% by $\max_{i\in[3]}[d_i(v^*)-w_i(S_i^*)/|S_i^*|]\ge 0-23=-23>-125.4477$.
% In general, our preprocessing cannot remove any vertex if the minimum regret is larger than the maximum density of some layer.
Consistent with our theoretical analysis, $|\text{supp}|$ is at most $k$. 
For AS-733, $k$ is quite large but $|\text{supp}|$ is still small.

Finally we evaluate the single vertex subsets obtained from optimal solutions.
For $p\in \Delta(2^V)$, we select a vertex subset from $\supp(p)$
% a vertex subset 
that optimizes the metric employed in the algorithm.
Note that in this setting, the output of DCS-LP coincides with that obtained by our algorithm with the density metric.
As DCS-Greedy performed quite poorly, it is omitted.
The results are shown in the last three columns of Table~\ref{table:computation}.
Although there are a few exceptions, the algorithm with a metric performs best
in terms of the metric employed.
A critical fact is that depending only on the density metric, we may fail to obtain meaningful structure from networks.
For example, the algorithm with the robust ratio admits a particularly large robust ratio value of $0.6652$ for NYClimateMarch2014,
meaning that the vertex subset obtained achieves an approximation ratio of $0.6652$ for all layers.
Moreover, the algorithm with the regret metric admits a particularly small regret value of 155.5000 for Cannes2013.
For those instances, the algorithm with the density metric (i.e., DCS-LP) performs poorly in terms of those metrics, respectively.
From the above, it seems quite important to select an appropriate metric depending on the practical purpose at hand.

